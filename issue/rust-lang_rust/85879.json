{"url": "https://api.github.com/repos/rust-lang/rust/issues/85879", "repository_url": "https://api.github.com/repos/rust-lang/rust", "labels_url": "https://api.github.com/repos/rust-lang/rust/issues/85879/labels{/name}", "comments_url": "https://api.github.com/repos/rust-lang/rust/issues/85879/comments", "events_url": "https://api.github.com/repos/rust-lang/rust/issues/85879/events", "html_url": "https://github.com/rust-lang/rust/issues/85879", "id": 907835063, "node_id": "MDU6SXNzdWU5MDc4MzUwNjM=", "number": 85879, "title": "Extremely bad codegen of `leading_zeros` on several architectures", "user": {"login": "AaronKutch", "id": 32419308, "node_id": "MDQ6VXNlcjMyNDE5MzA4", "avatar_url": "https://avatars.githubusercontent.com/u/32419308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AaronKutch", "html_url": "https://github.com/AaronKutch", "followers_url": "https://api.github.com/users/AaronKutch/followers", "following_url": "https://api.github.com/users/AaronKutch/following{/other_user}", "gists_url": "https://api.github.com/users/AaronKutch/gists{/gist_id}", "starred_url": "https://api.github.com/users/AaronKutch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AaronKutch/subscriptions", "organizations_url": "https://api.github.com/users/AaronKutch/orgs", "repos_url": "https://api.github.com/users/AaronKutch/repos", "events_url": "https://api.github.com/users/AaronKutch/events{/privacy}", "received_events_url": "https://api.github.com/users/AaronKutch/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 108333, "node_id": "MDU6TGFiZWwxMDgzMzM=", "url": "https://api.github.com/repos/rust-lang/rust/labels/A-LLVM", "name": "A-LLVM", "color": "f7e101", "default": false, "description": "Area: Code generation parts specific to LLVM. Both correctness bugs and optimization-related issues."}, {"id": 113376, "node_id": "MDU6TGFiZWwxMTMzNzY=", "url": "https://api.github.com/repos/rust-lang/rust/labels/I-slow", "name": "I-slow", "color": "e10c02", "default": false, "description": "Problems and improvements with respect to performance of generated code."}, {"id": 234956, "node_id": "MDU6TGFiZWwyMzQ5NTY=", "url": "https://api.github.com/repos/rust-lang/rust/labels/A-codegen", "name": "A-codegen", "color": "f7e101", "default": false, "description": "Area: Code generation"}, {"id": 211668100, "node_id": "MDU6TGFiZWwyMTE2NjgxMDA=", "url": "https://api.github.com/repos/rust-lang/rust/labels/T-compiler", "name": "T-compiler", "color": "bfd4f2", "default": false, "description": "Relevant to the compiler team, which will review and decide on the PR/issue."}, {"id": 650731663, "node_id": "MDU6TGFiZWw2NTA3MzE2NjM=", "url": "https://api.github.com/repos/rust-lang/rust/labels/C-bug", "name": "C-bug", "color": "f5f1fd", "default": false, "description": "Category: This is a bug."}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2021-06-01T03:29:40Z", "updated_at": "2022-01-26T19:43:54Z", "closed_at": null, "author_association": "CONTRIBUTOR", "active_lock_reason": null, "body": "A few months ago, I noticed some problems with the code gen of `leading_zeros` for RISCV and some other architectures, and tried to fix it in a [compiler-builtins PR](https://github.com/rust-lang/compiler-builtins/pull/366). Today, I was looking at the function for another reason and decided to see what the codegen currently looks like. I assumed that LLVM used `__clzsi2` for `usize::leading_zeros` and then used a recursive wrapper to extend it to larger integers. It seems that it is using its own method sometimes, and other times using `__clzsi2`. I double checked that the codegen in `compiler-builtins` is still correct. To see what I am talking about, run this code in a empty library crate with `cargo rustc --release --target=[target triple] -- --emit=asm` and check the assembly file under `target/[target triple]/release/deps/`:\r\n<details>\r\n\r\n```\r\n#![no_std]\r\n\r\npub fn lz_u128(x: u128) -> usize {\r\n    x.leading_zeros() as usize\r\n}\r\n\r\npub fn actual_lz(x: usize) -> usize {\r\n    x.leading_zeros() as usize\r\n}\r\n\r\nfn usize_leading_zeros_riscv(x: usize) -> usize {\r\n    let mut x = x;\r\n    // the number of potential leading zeros\r\n    let mut z = usize::MAX.count_ones() as usize;\r\n    // a temporary\r\n    let mut t: usize;\r\n    #[cfg(target_pointer_width = \"64\")]\r\n    {\r\n        // If the upper 32 bits of `x` are not all 0, `t` is set to `1 << 5`, otherwise\r\n        // `t` is set to 0.\r\n        t = ((x >= (1 << 32)) as usize) << 5;\r\n        // If `t` was set to `1 << 5`, then the upper 32 bits are shifted down for the\r\n        // next step to process.\r\n        x >>= t;\r\n        // If `t` was set to `1 << 5`, then we subtract 32 from the number of potential\r\n        // leading zeros\r\n        z -= t;\r\n    }\r\n    #[cfg(any(target_pointer_width = \"32\", target_pointer_width = \"64\"))]\r\n    {\r\n        t = ((x >= (1 << 16)) as usize) << 4;\r\n        x >>= t;\r\n        z -= t;\r\n    }\r\n    t = ((x >= (1 << 8)) as usize) << 3;\r\n    x >>= t;\r\n    z -= t;\r\n    t = ((x >= (1 << 4)) as usize) << 2;\r\n    x >>= t;\r\n    z -= t;\r\n    t = ((x >= (1 << 2)) as usize) << 1;\r\n    x >>= t;\r\n    z -= t;\r\n    t = (x >= (1 << 1)) as usize;\r\n    x >>= t;\r\n    z -= t;\r\n    // All bits except the LSB are guaranteed to be zero for this final bisection step.\r\n    // If `x != 0` then `x == 1` and subtracts one potential zero from `z`.\r\n    z - x\r\n}\r\n\r\nfn usize_leading_zeros_default(x: usize) -> usize {\r\n    let mut x = x;\r\n    // the number of potential leading zeros\r\n    let mut z = usize::MAX.count_ones() as usize;\r\n    // a temporary\r\n    let mut t: usize;\r\n    #[cfg(target_pointer_width = \"64\")]\r\n    {\r\n        t = x >> 32;\r\n        if t != 0 {\r\n            z -= 32;\r\n            x = t;\r\n        }\r\n    }\r\n    #[cfg(any(target_pointer_width = \"32\", target_pointer_width = \"64\"))]\r\n    {\r\n        t = x >> 16;\r\n        if t != 0 {\r\n            z -= 16;\r\n            x = t;\r\n        }\r\n    }\r\n    t = x >> 8;\r\n    if t != 0 {\r\n        z -= 8;\r\n        x = t;\r\n    }\r\n    t = x >> 4;\r\n    if t != 0 {\r\n        z -= 4;\r\n        x = t;\r\n    }\r\n    t = x >> 2;\r\n    if t != 0 {\r\n        z -= 2;\r\n        x = t;\r\n    }\r\n    // the last two bisections are combined into one conditional\r\n    t = x >> 1;\r\n    if t != 0 {\r\n        z - 2\r\n    } else {\r\n        z - x\r\n    }\r\n}\r\n\r\npub fn expected_lz(x: usize) -> usize {\r\n    if cfg!(any(target_arch = \"riscv32\", target_arch = \"riscv64\")) {\r\n        usize_leading_zeros_riscv(x)\r\n    } else {\r\n        usize_leading_zeros_default(x)\r\n    }\r\n}\r\n```\r\n</details>\r\n\r\nThe `expected_lz` function is equivalent to what `compiler-builtins` does, while `actual_lz` show what a plain `leading_zeros` compiles down to. `lz_u128` demonstrates another issue that I have known for some time, but never got fixed. I think I remember nagisa opening an LLVM issue for it, but can't find the link.\r\n\r\nThe worst behavior is on `riscv32i-unknown-none-elf` where `expected_lz` compiles down to the expected branchless 27 assembly instructions, but `actual_lz` has more assembly instructions and includes a call to `__mulsi3` which probably results in it being about as expensive to compute as a full integer division. `lz_u128` is truly horrifying.\r\n\r\n`riscv64gc-unknown-none-elf` does roughly the same thing, and while the multiplication could be expected to run much faster, `expected_lz` is still much better. Whatever inlining lead to `lz_u128` is not good. The correct `lz_u128` looks like what is produced by:\r\n```\r\npub fn lz_u128_expected(x: u128) -> usize {\r\n    let mut x = x;\r\n    let mut z = 0;\r\n    if ((x >> 64) as u64) != 0 {\r\n        z += 64;\r\n    } else {\r\n        x >>= 64;\r\n    }\r\n    // note: `usize` needs to have bitwidth 64 here\r\n    z + expected_lz(x as usize) as usize\r\n}\r\n```\r\n\r\n`thumbv8m.base-none-eabi` (I decided to use this as a non-riscv example that has no hardware clz) uses calls to `__clzsi2` instead of rolling its own thing. Take a look at the number of `__clzsi2` calls and extra instructions `lz_u128` makes vs. what the `lz_u128_expected` I coded above shows should be possible. I suspect that instead of two separate bugs, there is one bug where LLVM defines larger bitwidth clz's in terms of small clz's in such a way that it generates horrible code when smaller clz's get inlined upwards into larger clz's. Whenever LLVM decides to roll its own code instead of `__clzsi2`, it starts with some small clz on an integer like `u8`, and by the time `u64` is reached the code is extremely garbled.\r\n\r\n`aarch64-pc-windows-msvc` and x86_64 call hardware clz and do the correct extension for `lz_u128`. `expected_lz` can be ignored, since `__clzsi2` is only used when a hardware clz cannot be used.\r\n\r\nIt is important for the embedded architectures to have a better clz, because the performance of that function is the base of many other kinds of arithmetic emulation such as soft floats.\r\n", "closed_by": null, "reactions": {"url": "https://api.github.com/repos/rust-lang/rust/issues/85879/reactions", "total_count": 2, "+1": 2, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/rust-lang/rust/issues/85879/timeline", "performed_via_github_app": null, "state_reason": null}