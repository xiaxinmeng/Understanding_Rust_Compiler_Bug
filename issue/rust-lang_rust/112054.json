{"url": "https://api.github.com/repos/rust-lang/rust/issues/112054", "repository_url": "https://api.github.com/repos/rust-lang/rust", "labels_url": "https://api.github.com/repos/rust-lang/rust/issues/112054/labels{/name}", "comments_url": "https://api.github.com/repos/rust-lang/rust/issues/112054/comments", "events_url": "https://api.github.com/repos/rust-lang/rust/issues/112054/events", "html_url": "https://github.com/rust-lang/rust/issues/112054", "id": 1729857213, "node_id": "I_kwDOAAsO6M5nG4a9", "number": 112054, "title": "ICE: type_op_normalize_ty: forcing query with already existing `DepNode`", "user": {"login": "TimKotowski", "id": 56227372, "node_id": "MDQ6VXNlcjU2MjI3Mzcy", "avatar_url": "https://avatars.githubusercontent.com/u/56227372?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TimKotowski", "html_url": "https://github.com/TimKotowski", "followers_url": "https://api.github.com/users/TimKotowski/followers", "following_url": "https://api.github.com/users/TimKotowski/following{/other_user}", "gists_url": "https://api.github.com/users/TimKotowski/gists{/gist_id}", "starred_url": "https://api.github.com/users/TimKotowski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TimKotowski/subscriptions", "organizations_url": "https://api.github.com/users/TimKotowski/orgs", "repos_url": "https://api.github.com/users/TimKotowski/repos", "events_url": "https://api.github.com/users/TimKotowski/events{/privacy}", "received_events_url": "https://api.github.com/users/TimKotowski/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 9618520, "node_id": "MDU6TGFiZWw5NjE4NTIw", "url": "https://api.github.com/repos/rust-lang/rust/labels/I-ICE", "name": "I-ICE", "color": "e10c02", "default": false, "description": "Issue: The compiler panicked, giving an Internal Compilation Error (ICE) \u2744\ufe0f"}, {"id": 211668100, "node_id": "MDU6TGFiZWwyMTE2NjgxMDA=", "url": "https://api.github.com/repos/rust-lang/rust/labels/T-compiler", "name": "T-compiler", "color": "bfd4f2", "default": false, "description": "Relevant to the compiler team, which will review and decide on the PR/issue."}, {"id": 650731663, "node_id": "MDU6TGFiZWw2NTA3MzE2NjM=", "url": "https://api.github.com/repos/rust-lang/rust/labels/C-bug", "name": "C-bug", "color": "f5f1fd", "default": false, "description": "Category: This is a bug."}, {"id": 1049510487, "node_id": "MDU6TGFiZWwxMDQ5NTEwNDg3", "url": "https://api.github.com/repos/rust-lang/rust/labels/A-async-await", "name": "A-async-await", "color": "f7e101", "default": false, "description": "Area: Async & Await"}, {"id": 1359848690, "node_id": "MDU6TGFiZWwxMzU5ODQ4Njkw", "url": "https://api.github.com/repos/rust-lang/rust/labels/E-needs-mcve", "name": "E-needs-mcve", "color": "02e10c", "default": false, "description": "Call for participation: This issue needs a Minimal Complete and Verifiable Example"}, {"id": 2420759390, "node_id": "MDU6TGFiZWwyNDIwNzU5Mzkw", "url": "https://api.github.com/repos/rust-lang/rust/labels/A-query-system", "name": "A-query-system", "color": "f7e101", "default": false, "description": "Area: The rustc query system (https://rustc-dev-guide.rust-lang.org/query.html)"}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2023-05-29T01:55:51Z", "updated_at": "2023-06-05T16:00:24Z", "closed_at": null, "author_association": "NONE", "active_lock_reason": null, "body": "<!--\r\nThank you for finding an Internal Compiler Error! \ud83e\uddca  If possible, try to provide\r\na minimal verifiable example. You can read \"Rust Bug Minimization Patterns\" for\r\nhow to create smaller examples.\r\nhttp://blog.pnkfx.org/blog/2019/11/18/rust-bug-minimization-patterns/\r\n-->\r\n\r\n### Code\r\n\r\n```Rust\r\nuse anyhow::Error;\r\nuse arrow::array::{Array, ArrayData, ArrayRef, StringArray};\r\nuse arrow::datatypes::*;\r\nuse arrow::record_batch::RecordBatch;\r\nuse csv::ReaderBuilder;\r\nuse datafusion::datasource::MemTable;\r\nuse datafusion::prelude::*;\r\nuse datafusion::scalar::ScalarValue;\r\nuse serde::de::DeserializeOwned;\r\nuse serde::{Deserialize, Serialize};\r\nuse std::collections::HashMap;\r\nuse std::fmt::Debug;\r\nuse std::fs::File;\r\nuse std::io::{BufReader, Cursor, Read};\r\nuse std::ops::Deref;\r\nuse std::sync::Arc;\r\nuse arrow::ipc::{RecordBatchArgs, RecordBatchBuilder};\r\nuse datafusion::row::layout::RowLayout;\r\nuse thiserror::Error;\r\nuse serde_json::{json, Value};\r\n\r\n#[derive(Serialize, Deserialize, Debug)]\r\nstruct SchemaTest {\r\n    #[serde(rename(serialize = \"firstname\"))]\r\n    #[serde(rename(deserialize = \"firstname\"))]\r\n    first_name: String,\r\n\r\n    #[serde(rename(serialize = \"lastname\"))]\r\n    #[serde(rename(deserialize = \"lastname\"))]\r\n    last_name: String,\r\n\r\n    profession: String,\r\n\r\n    id: isize\r\n}\r\n\r\n#[derive(Error, Debug)]\r\npub enum QueryEngineError {\r\n    #[error(\"fields unable to serde\")]\r\n    SerdeIssue(String),\r\n    #[error(\"unknown data store error\")]\r\n    Unknown,\r\n}\r\n\r\n#[tokio::main]\r\nasync fn main() -> Result<(), Error> {\r\n\r\n    // This is not needed in the crate, but needed for testing.\r\n    let file = File::open(\"./src/myFiles.csv\").unwrap();\r\n    let mut buf_reader = BufReader::new(file);\r\n    let mut contents: Vec<u8> = Vec::new();\r\n    buf_reader.read_to_end(&mut contents).unwrap();\r\n\r\n    let config =\r\n        SessionConfig::new().set(\"datafusion.execution.batch_size\", ScalarValue::from(100000));\r\n\r\n    let ctx = SessionContext::with_config(config);\r\n    // Cursor wraps the in memory buffer to be able to implement Read trait, so it can be a Reader.\r\n    let mut cursor = Cursor::new(contents);\r\n    let mut csv_reader = ReaderBuilder::new()\r\n        .has_headers(true)\r\n        .from_reader(&mut cursor);\r\n\r\n    let mut header_tracker: HashMap<usize, Vec<ArrayRef>> = HashMap::new();\r\n    let mut header_tracker_key = 0;\r\n    let headers = csv_reader.headers().unwrap();\r\n    headers.iter().for_each(|_| {\r\n        header_tracker.insert(header_tracker_key, Vec::new());\r\n        header_tracker_key += 1\r\n    });\r\n\r\n    let fields: Vec<Field> = headers\r\n        .iter()\r\n        .map(|header| Field::new(header, DataType::Utf8, false))\r\n        .collect();\r\n    let schema1 = Arc::new(Schema::new(fields));\r\n\r\n    cursor.set_position(0);\r\n    let mut reader = arrow::csv::ReaderBuilder::new(schema1.clone())\r\n        .with_batch_size(4000)\r\n        .has_header(true)\r\n        .build_buffered(cursor)\r\n        .unwrap();\r\n\r\n    let mut batches = Vec::new();\r\n    // RecordBatch should contains Columns for each csv column. Due to the batching size in reader,\r\n    // Must propagte the correect columns toegther then later combine those columnd that match\r\n    // to allow fast index grabbing down the line\r\n\r\n    for batch in reader.into_iter() {\r\n        batches.push(batch.unwrap());\r\n    }\r\n    // reader\r\n    //     .into_iter()\r\n    //     .flat_map(|batch| batch.into_iter())\r\n    //     .for_each(|record_batch| {\r\n    //         println!(\"{:?}\", record_batch);\r\n    //         record_batch.columns().iter().enumerate().for_each(|(index, c)| {\r\n    //             println!(\"{:?}\", c);\r\n    //                 header_tracker.entry(index).or_insert(vec![]).push(c.clone());\r\n    //         });\r\n    //     });\r\n\r\n\r\n    // Register a new MemTable with the context\r\n    let table = MemTable::try_new(schema1.clone(), vec![batches]).unwrap();\r\n\r\n    ctx.register_table(\"example\", Arc::new(table)).unwrap();\r\n    let df = ctx.sql(\"SELECT * FROM example\").await.unwrap();\r\n\r\n    let results: Vec<RecordBatch> = df.collect().await.unwrap();\r\n    if results.is_empty() {\r\n        return Ok(());\r\n    }\r\n\r\n    let all_data = read_and_process_columns::<SchemaTest>(results.first().unwrap()).await;\r\n\r\n    return Ok(());\r\n}\r\n\r\nasync fn read_and_process_columns<S: 'static>(\r\n    record_batch: &RecordBatch,\r\n) -> Result<Vec<S>, QueryEngineError>\r\nwhere\r\n    S: Serialize + DeserializeOwned + Debug\r\n{\r\n    let column_count = record_batch.columns().len();\r\n    let column = record_batch.columns().first().unwrap();\r\n\r\n    // TODO handle large amounts of data. There could be columns with data over 50k +\r\n    // Either shoot off only 4 tokio spawns wait for them and shoot off more to allow fast\r\n    // data parsing. Tokio spawns are light weight but dont want the user to feel that this\r\n    // crate has some resources/memory exhaustion.\r\n    // Or sequentially process in batched start in intervals of 10000.\r\n    let result = process_batch::<S>(column, record_batch, column_count).await?;\r\n    Ok(result)\r\n}\r\n\r\n#[inline]\r\nasync fn process_batch<S: 'static>(\r\n    column: &ArrayRef,\r\n    record_batch: &RecordBatch,\r\n    column_count: usize,\r\n) -> Result<Vec<S>, QueryEngineError>\r\nwhere\r\n    S: Serialize + DeserializeOwned + Debug\r\n{\r\n    let mut newish: Vec<S> = Vec::new();\r\n    let column_data = column.as_any().downcast_ref::<StringArray>().unwrap();\r\n    for (column_index, column_value) in column_data.iter().enumerate() {\r\n        newish.push(\r\n            read_and_process_column::<S>(\r\n                column_index,\r\n                detect_empty_column_value(column_value),\r\n                column_count,\r\n                record_batch,\r\n            )\r\n            .await?,\r\n        )\r\n    }\r\n    Ok(newish)\r\n}\r\n\r\nfn detect_empty_column_value(column_value: Option<&str>) -> String {\r\n    let column_val = match column_value {\r\n        Some(value) => value.to_string(),\r\n        None => \"\".to_string(),\r\n    };\r\n\r\n    column_val\r\n}\r\n\r\n#[inline]\r\nasync fn read_and_process_column<S: 'static>(\r\n    column_index: usize,\r\n    column_value: String,\r\n    column_count: usize,\r\n    record_batch: &RecordBatch,\r\n) -> Result<S, QueryEngineError>\r\nwhere\r\n    S: Serialize + DeserializeOwned + Debug\r\n{\r\n    let headers = get_headers(record_batch.schema());\r\n    let mut data: HashMap<usize, Vec<String>> = HashMap::new();\r\n    data.entry(column_index)\r\n        .or_insert(vec![])\r\n        .push(column_value);\r\n\r\n    // Start at the second Column.\r\n    // Record-batch will contain a Column each with the data of each Column from the csv.\r\n    for i in 1..column_count {\r\n        if let Some(column) = record_batch\r\n            .column(i)\r\n            .as_any()\r\n            .downcast_ref::<StringArray>()\r\n        {\r\n            let val = column.value(column_index);\r\n            data.entry(column_index)\r\n                .or_insert(vec![])\r\n                .push(val.to_string());\r\n        }\r\n    }\r\n\r\n    let finish = data.get(&column_index).unwrap().to_owned();\r\n    let final_data: S = finalize_data_transfer::<S>(headers, finish)?;\r\n\r\n    Ok(final_data)\r\n}\r\n\r\nfn finalize_data_transfer<S: 'static>(\r\n    headers: Vec<String>,\r\n    column_data: Vec<String>,\r\n) -> Result<S, QueryEngineError>\r\nwhere\r\n    S: Serialize + DeserializeOwned + Debug\r\n{\r\n\r\n\r\n    // let serialized_cmn  =\r\n    //     serde_json::to_string(&column_data).map_err(|err| QueryEngineError::SerdeIssue(err.to_string())).unwrap();\r\n\r\n    // let rows: S = serde_json::from_str(&*serialized_cmn)\r\n    //     .map_err(|err| QueryEngineError::SerdeIssue(err.to_string()))?;\r\n\r\n    // println!(\"LASJ:LDSJLD:S {:?}\", rows);\r\n    let daa = headers\r\n        .iter()\r\n        .zip(column_data.deref().iter())\r\n        .map(|(key, value)| (key.to_owned(), json!(value)))\r\n        .collect::<HashMap<String, serde_json::Value>>();\r\n\r\n    let serialized_column: Value =\r\n        serde_json::to_value(&daa).map_err(|err| QueryEngineError::SerdeIssue(err.to_string()))?;\r\n\r\n    let row: S = serde_json::from_value(serialized_column)\r\n        .map_err(|err| QueryEngineError::SerdeIssue(err.to_string()))?;\r\n\r\n    Ok(row)\r\n}\r\n\r\nfn get_headers(schema: SchemaRef) -> Vec<String> {\r\n    let mut headers = Vec::new();\r\n    let fields = schema.fields.as_ref();\r\n    for field in fields.iter() {\r\n        headers.push(field.name().to_string())\r\n    }\r\n\r\n    headers\r\n}\r\n\r\nAdding Debug down the chain of functions is the issue\r\n```\r\n\r\n\r\n### Meta\r\n<!--\r\nIf you're using the stable version of the compiler, you should also check if the\r\nbug also exists in the beta or nightly versions.\r\n-->\r\n\r\n`rustc --version --verbose`:\r\n```\r\n1.69.0\r\n```\r\n\r\n### Error output\r\n\r\n```\r\nthread 'rustc' panicked at 'forcing query with already existing `DepNode`\r\n- query-key: Canonical { max_universe: U0, variables: [CanonicalVarInfo { kind: Region(U0) }, CanonicalVarInfo { kind: Region(U0) }, CanonicalVarInfo { kind: Region(U0) }], value: ParamEnvAnd { param_env: ParamEnv { caller_bounds: [Binder(TraitPredicate(<S as std::fmt::Debug>, polarity:Positive), []), Binder(TraitPredicate(<S as _::_serde::de::DeserializeOwned>, polarity:Positive), []), Binder(TraitPredicate(<S as _::_serde::Deserialize<'de>>, polarity:Positive), [Region(BrNamed(DefId(64:1330 ~ serde[ab07]::de::DeserializeOwned::'de), 'de))]), Binder(TraitPredicate(<S as _::_serde::Serialize>, polarity:Positive), []), Binder(TraitPredicate(<S as std::marker::Sized>, polarity:Positive), []), Binder(OutlivesPredicate(S, ReStatic), [])], reveal: UserFacing, constness: NotConst }, value: Normalize { value: [async fn body@src\\main.rs:147:1: 162:2] } } }\r\n- dep-node: type_op_normalize_ty(d01b879686a5fa54-891de594ba26c99e)', /rustc/84c898d65adf2f39a5a98507f1fe0ce10a2b8dbc\\compiler\\rustc_query_system\\src\\dep_graph\\graph.rs:319:9\r\n```\r\n\r\n<!--\r\nInclude a backtrace in the code block by setting `RUST_BACKTRACE=1` in your\r\nenvironment. E.g. `RUST_BACKTRACE=1 cargo build`.\r\n-->\r\n<details><summary><strong>Backtrace</strong></summary>\r\n<p>\r\n\r\n```\r\nstack backtrace:\r\nnote: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\r\nerror: the compiler unexpectedly panicked. this is a bug.\r\nnote: we would appreciate a bug report: https://github.com/rust-lang/rust/issues/new?labels=C-bug%2C+I-ICE%2C+T-compiler&template=ice.md\r\nnote: rustc 1.69.0 (84c898d65 2023-04-16) running on x86_64-pc-windows-msvc\r\nnote: compiler flags: --crate-type bin -C embed-bitcode=no -C debuginfo=2 -C incremental=[REDACTED]\r\nnote: some of the compiler flags provided by cargo are hidden\r\nquery stack during panic:\r\n#0 [type_op_normalize_ty] normalizing `[async fn body@src\\main.rs:147:1: 162:2]`\r\n#1 [mir_borrowck] borrow-checking `process_batch::{closure#0}`\r\n#2 [mir_borrowck] borrow-checking `process_batch`\r\n#3 [analysis] running analysis passes on this crate\r\nend of query stack\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n", "closed_by": null, "reactions": {"url": "https://api.github.com/repos/rust-lang/rust/issues/112054/reactions", "total_count": 0, "+1": 0, "-1": 0, "laugh": 0, "hooray": 0, "confused": 0, "heart": 0, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/rust-lang/rust/issues/112054/timeline", "performed_via_github_app": null, "state_reason": null}