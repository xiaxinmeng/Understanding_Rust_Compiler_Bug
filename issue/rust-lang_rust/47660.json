{"url": "https://api.github.com/repos/rust-lang/rust/issues/47660", "repository_url": "https://api.github.com/repos/rust-lang/rust", "labels_url": "https://api.github.com/repos/rust-lang/rust/issues/47660/labels{/name}", "comments_url": "https://api.github.com/repos/rust-lang/rust/issues/47660/comments", "events_url": "https://api.github.com/repos/rust-lang/rust/issues/47660/events", "html_url": "https://github.com/rust-lang/rust/issues/47660", "id": 290529809, "node_id": "MDU6SXNzdWUyOTA1Mjk4MDk=", "number": 47660, "title": "Tracking Issue for Incremental Compilation", "user": {"login": "michaelwoerister", "id": 1825894, "node_id": "MDQ6VXNlcjE4MjU4OTQ=", "avatar_url": "https://avatars.githubusercontent.com/u/1825894?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelwoerister", "html_url": "https://github.com/michaelwoerister", "followers_url": "https://api.github.com/users/michaelwoerister/followers", "following_url": "https://api.github.com/users/michaelwoerister/following{/other_user}", "gists_url": "https://api.github.com/users/michaelwoerister/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelwoerister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelwoerister/subscriptions", "organizations_url": "https://api.github.com/users/michaelwoerister/orgs", "repos_url": "https://api.github.com/users/michaelwoerister/repos", "events_url": "https://api.github.com/users/michaelwoerister/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelwoerister/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 211668100, "node_id": "MDU6TGFiZWwyMTE2NjgxMDA=", "url": "https://api.github.com/repos/rust-lang/rust/labels/T-compiler", "name": "T-compiler", "color": "bfd4f2", "default": false, "description": "Relevant to the compiler team, which will review and decide on the PR/issue."}, {"id": 307747675, "node_id": "MDU6TGFiZWwzMDc3NDc2NzU=", "url": "https://api.github.com/repos/rust-lang/rust/labels/A-incr-comp", "name": "A-incr-comp", "color": "f7e101", "default": false, "description": "Area: Incremental compilation"}, {"id": 650846969, "node_id": "MDU6TGFiZWw2NTA4NDY5Njk=", "url": "https://api.github.com/repos/rust-lang/rust/labels/C-tracking-issue", "name": "C-tracking-issue", "color": "f5f1fd", "default": false, "description": "Category: A tracking issue for an RFC or an unstable feature."}, {"id": 3537211959, "node_id": "LA_kwDOAAsO6M7S1ZI3", "url": "https://api.github.com/repos/rust-lang/rust/labels/S-tracking-impl-incomplete", "name": "S-tracking-impl-incomplete", "color": "4682b4", "default": false, "description": ""}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2018-01-22T16:32:18Z", "updated_at": "2022-05-27T14:09:38Z", "closed_at": null, "author_association": "MEMBER", "active_lock_reason": null, "body": "Incremental compilation will soon be available in stable Rust (with version 1.24) but there's still lots of room for improvement. This issue will try to give an overview of specific areas for potential optimization.\r\n\r\n## Compile Time\r\nThe main goal for incremental compilation is to decrease compile times in the common case. Here are some of things that affect it:\r\n\r\n### Caching Efficiency \r\nThe biggest topic for incremental compilation is: How efficiently are we using our caches. It can be further subdivided into the following areas:\r\n\r\n#### Data Structure Stability\r\nCaching efficiency depends on how the data in the cache is represented. If data is likely to change frequently then the cache is likely to be invalidated frequently. One example is source location information: Source location information is likely to change. Add a comment somewhere and the source location of everything below the comment has changed. As a consequence, everything in the cache that contains source location information is likely in need of frequent invalidation. It would be preferable to factor data structures in a way that confines this volatility to only few kinds of cache entries. The following issues track concrete plans to improve the situation here:\r\n\r\n- [ ] Improve caching efficiency by handling spans in a more robust way. #47389\r\n- [x] Turn translation-related attributes into a query. #47320\r\n\r\n#### Object File Granularity and Partitioning\r\nThe problem of object file granularity can be viewed as a variation of \"Data Structure Stability\" but it is so important for compile times that I want to list it separately. At the moment the compiler will put machine code that comes from items in the same source-level module into the same object file. This means that changing one function in a module will lead to all functions in that module to be re-compiled. Obviously this can lead to a large amount of redundant work. For full accuracy we could in theory have one object file per function - which indeed improves re-compile times a lot in many cases - but that would increase disk space requirements to an unacceptable level and makes compilation sessions with few cache hits much more expensive (TODO: insert numbers). And as long as we don't do incremental linking it might also make linking a bottleneck.\r\n\r\nThe main goal here is to re-compile as little unchanged code as possible while keeping overhead small. This is a hard problem and some approaches are:\r\n\r\n- Keep using a fixed partitioning scheme but improve the partitioning algorithm\r\n- Implement an adaptive scheme that reacts to set of changes a user makes\r\n- It might even be a good idea to write a simulator that allows to test different schemes and then feed it with actual data generated by an instrumented compiler. \r\n\r\nAdaptive schemes would require us to re-think part of our testing and benchmarking infrastructure and writing a simulator is a big project of its own, so short term we should look into improved static partitioning schemes:\r\n\r\n- [ ] Take type parameters into account when assigning generic instances to CGUs. (TODO)\r\n- [ ] Do per-MonoItem dependency tracking in order to collect data about granularity fallout. (#48211)\r\n\r\nAnother avenue for improvement of how we handle object files: \r\n\r\n- [ ] Allow for re-using object files that contain unused code. (#48212)\r\n\r\n#### Whole-Cache Invalidation\r\nCurrently commandline arguments are tracked at a very coarse level: Any change to a commandline argument will completely invalidate the cache. The red-green tracking system could take care of making this finer grained but quite a lot of refactoring would need to happen in order to make sure that commandline arguments are *only* accessible via queries.\r\n\r\nNote that this currently completely prevents sharing a cache between `cargo check` and `cargo build`. \r\n\r\n#### Avoid Redundant Work\r\nThe compiler is doing redundant work at the moment. Reducing it will also positively affect incremental compilation:\r\n\r\n- [ ] Linking is not done incrementally although at least Gold and MSVC would support it. (TODO)\r\n- [x] Instances of generic functions are duplicated for every crate that uses them. #47317\r\n- [x] Closures are unnecessarily duplicated for generic instances. #46477\r\n- [ ] Add support for split-debuginfo on platforms that allow it. #34651\r\n\r\n#### Querify More Logic, Cache More Queries\r\nWe can only cache things that are represented as queries, and we can only profit from caching for things that are (transitively) cached. There are some obvious candidates for querification:\r\n\r\n- [x] Cache the specialization_graph query. #48987\r\n- [x] Cache type_of and some other queries. #47455\r\n- [x] Turn translation-related attributes into a query. #47320\r\n- [x] Querify WF-checking so it can be cached. #46753\r\n- [x] Cache check_match and use ensure() for coherence-related queries. #46881\r\n- [x] Enable query result caching for many more queries. #46556\r\n\r\nA more ambitious goal would be to querify name resolution and macro expansion.\r\n\r\n### Framework Overhead\r\nDoing dependency tracking will invariably introduce some overhead. We should strive to keep this overhead low. The main areas of overhead are:\r\n\r\n- Building the dependency graph during compiler execution\r\n- Computing query result hashes and dependency node identifiers\r\n- Loading and storing the dependency graph from/to disk\r\n\r\nAnother thing that falls into this category is:\r\n\r\n- Efficiency of loading and storing cache entries\r\n\r\nThe following issues track individual improvements:\r\n\r\n- [ ] Load query result cache in background or use memory mapped file. (TODO)\r\n- [ ] Investigate sharing more data inside result cache (e.g. ty::Slice). (TODO)\r\n- [ ] Reduce amount of cache sanity checking for non-debug_assertions compilers. (TODO)\r\n- [ ] Investigate making the result cache updateable in-place. #48231\r\n- [x] Use a struct-of-arrays instead of an array-of-structs for SerializedDepGraph. #47326\r\n- [x] Speed up result hashing and DepNode computation by caching certain stable hashes. #47294\r\n- [x] Optimize DepGraph::try_mark_green(). #47293\r\n- [x] Speed up leb128 encoding and decoding for unsigned values. #46919\r\n- [x] Use an array instead of a hashmap for storing result hashes. #46842\r\n- [x] Precompute small hash for filenames to save some work. #46839\r\n- [x] Speed up span hashing by caching expansion context hashes. #46562\r\n- [x] Load dep-graph in the background. #46555\r\n- [x] Don't encode Fingerprint values with leb128. #45875\r\n- [x] Explore delayed read-edge deduplication or getting rid of it entirely. #45873\r\n- [x] Maybe optimize case of dependency-less anonymous queries. #45408\r\n- [x] Implement \"eval-always\" queries. #45238\r\n\r\nWe should continuously profile common use cases to find out where we can further reduce framework overhead.\r\n\r\n## Disk Space Requirements\r\nBuild directories of large C/C++ and Rust code bases can be *massive*, oftentimes many gigabytes. Since incremental compilation has to keep around the previous version of each object file for re-use, plus LLVM IR, plus the dependency graph and query result cache, build directories can up to triple in size when incremental compilation is turned on (depending on which crates are compiled incrementally). The best way to reduce cache size is to reduce the amount of translated code that we need to cache. Solving #47317 and #46477 would help here. MIR-only RLIBs (#38913), which are one way to solve #47317, might also obviate the need to cache LLVM IR at all.\r\n\r\n## Runtime Performance of Incrementally Compiled Code\r\nCurrently delivering good runtime performance for incrementally compiled code is only a side goal. If incrementally compiled code is \"fast enough\" we rather try to improve compile times. However, since ThinLTO also supports an incremental mode, we could provide a middle ground between \"re-compile everything for good performance\" and \"re-compile incrementally and only get 50% performance\". \r\n \r\n- [x] Make ThinLTO compatible with incremental compilation. (https://github.com/rust-lang/rust/pull/53673)\r\n\r\n---------------\r\n\r\nIf you have any further ideas or spot anything that I've missed, let me know in the comments below!", "closed_by": null, "reactions": {"url": "https://api.github.com/repos/rust-lang/rust/issues/47660/reactions", "total_count": 38, "+1": 12, "-1": 0, "laugh": 0, "hooray": 8, "confused": 0, "heart": 18, "rocket": 0, "eyes": 0}, "timeline_url": "https://api.github.com/repos/rust-lang/rust/issues/47660/timeline", "performed_via_github_app": null, "state_reason": null}