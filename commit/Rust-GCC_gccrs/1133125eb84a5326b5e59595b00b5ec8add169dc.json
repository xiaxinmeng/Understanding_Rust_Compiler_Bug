{"sha": "1133125eb84a5326b5e59595b00b5ec8add169dc", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MTEzMzEyNWViODRhNTMyNmI1ZTU5NTk1YjAwYjVlYzhhZGQxNjlkYw==", "commit": {"author": {"name": "Harsha Jagasia", "email": "harsha.jagasia@amd.com", "date": "2010-05-14T17:35:11Z"}, "committer": {"name": "Harsha Jagasia", "email": "hjagasia@gcc.gnu.org", "date": "2010-05-14T17:35:11Z"}, "message": "config.gcc: Add support for --with-cpu option for bdver1.\n\n2010-05-14  Harsha Jagasia  <harsha.jagasia@amd.com>\n\n\t* config.gcc: Add support for --with-cpu option for bdver1.\n\t* config/i386/i386.h (TARGET_BDVER1): New macro.\n\t(ix86_tune_indices): Change SSE_UNALIGNED_MOVE_OPTIMAL\n\tto SSE_UNALIGNED_LOAD_OPTIMAL. Add SSE_UNALIGNED_STORE_OPTIMAL.\n\t(ix86_tune_features) :Change SSE_UNALIGNED_MOVE_OPTIMAL\n\tto SSE_UNALIGNED_LOAD_OPTIMAL. Add SSE_UNALIGNED_STORE_OPTIMAL.\n\tAdd SSE_PACKED_SINGLE_INSN_OPTIMAL.\n\t(TARGET_CPU_DEFAULT_NAMES): Add bdver1.\n\t(processor_type): Add PROCESSOR_BDVER1.\n\t* config/i386/i386.md: Add bdver1 as a new cpu attribute to match\n\tprocessor_type in config/i386/i386.h.\n\tAdd check for TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL to emit\n\tmovaps <reg, reg> instead of movapd <reg, reg> when replacing\n\tmovsd <reg, reg> or movss <reg, reg> for SSE and AVX.\n\tAdd check for  TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\n\tto emit packed xor instead of packed double/packed integer\n\txor for SSE and AVX when moving a zero value.\n\t* config/i386/sse.md: Add check for TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\n\t to emit movaps instead of movapd/movdqa for SSE and AVX.\n\tAdd check for TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL to emit packed single\n\tlogical operations i.e and, or and xor instead of packed double logical\n\toperations for SSE and AVX. \n\t* config/i386/i386-c.c: \n\t(ix86_target_macros_internal): Add PROCESSOR_BDVER1.\n\t* config/i386/driver-i386.c: Turn on -mtune=native for BDVER1.\n\t(has_fma4, has_xop): New.\n\t* config/i386/i386.c (bdver1_cost): New variable.\n\t(m_BDVER1): New macro.\n\t(m_AMD_MULTIPLE): Add m_BDVER1.\n\t(x86_tune_use_leave, x86_tune_push_memory, x86_tune_unroll_strlen,\n\t x86_tune_deep_branch_prediction, x86_tune_use_sahf, x86_tune_movx,\n\t x86_tune_use_simode_fiop, x86_tune_promote_qimode, \n\t x86_tune_add_esp_8, x86_tune_tune_sub_esp_4, x86_tune_sub_esp_8,\n\t x86_tune_integer_dfmode_moves, x86_tune_partial_reg_dependency,\n\t x86_tune_sse_partial_reg_dependency, x86_tune_sse_unaligned_load_optimal,\n\t x86_tune_sse_unaligned_store_optimal, x86_tune_sse_typeless_stores,\n\t x86_tune_memory_mismatch_stall, x86_tune_use_ffreep,\n\t x86_tune_inter_unit_moves, x86_tune_inter_unit_conversions,\n\t x86_tune_use_bt, x86_tune_pad_returns, x86_tune_slow_imul_imm32_mem,\n\t x86_tune_slow_imul_imm8, x86_tune_fuse_cmp_and_branch): \n\tEnable/disable for bdver1.\n\t(processor_target_table): Add bdver1_cost.\n\t(cpu_names): Add bdver1.\n\t(override_options): Set up PROCESSOR_BDVER1 for bdver1 entry in\n\t processor_alias_table.\n\t(ix86_expand_vector_move_misalign): Change \n\t TARGET_SSE_UNALIGNED_MOVE_OPTIMAL to TARGET_SSE_UNALIGNED_LOAD_OPTIMAL.\n\t Check for TARGET_SSE_UNALIGNED_STORE_OPTIMAL.\n\t Check for TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL to emit movups instead\n\t of movupd/movdqu for SSE and AVX.\n\t(ix86_tune_issue_rate): Add PROCESSOR_BDVER1.\n\t(ix86_tune_adjust_cost): Add code for bdver1.\n\t(standard_sse_constant_opcode): Add check for\n\tTARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL to emit packed single xor instead\n\tof packed double xor for SSE and AVX.\n\nFrom-SVN: r159399", "tree": {"sha": "a6abb15b279401560c616e9b447f8a29e3c9704d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/a6abb15b279401560c616e9b447f8a29e3c9704d"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/1133125eb84a5326b5e59595b00b5ec8add169dc", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1133125eb84a5326b5e59595b00b5ec8add169dc", "html_url": "https://github.com/Rust-GCC/gccrs/commit/1133125eb84a5326b5e59595b00b5ec8add169dc", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1133125eb84a5326b5e59595b00b5ec8add169dc/comments", "author": null, "committer": null, "parents": [{"sha": "e972cc7e9a2fd59a6eb3375674c6c35fae4dcaf2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e972cc7e9a2fd59a6eb3375674c6c35fae4dcaf2", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e972cc7e9a2fd59a6eb3375674c6c35fae4dcaf2"}], "stats": {"total": 450, "additions": 381, "deletions": 69}, "files": [{"sha": "d360ebec8c386dc0e09d4a84441e80954112b5b9", "filename": "gcc/ChangeLog", "status": "modified", "additions": 58, "deletions": 0, "changes": 58, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=1133125eb84a5326b5e59595b00b5ec8add169dc", "patch": "@@ -1,3 +1,61 @@\n+2010-05-14  Harsha Jagasia  <harsha.jagasia@amd.com>\n+\n+\t* config.gcc: Add support for --with-cpu option for bdver1.\n+\t* config/i386/i386.h (TARGET_BDVER1): New macro.\n+\t(ix86_tune_indices): Change SSE_UNALIGNED_MOVE_OPTIMAL\n+\tto SSE_UNALIGNED_LOAD_OPTIMAL. Add SSE_UNALIGNED_STORE_OPTIMAL.\n+\t(ix86_tune_features) :Change SSE_UNALIGNED_MOVE_OPTIMAL\n+\tto SSE_UNALIGNED_LOAD_OPTIMAL. Add SSE_UNALIGNED_STORE_OPTIMAL.\n+\tAdd SSE_PACKED_SINGLE_INSN_OPTIMAL.\n+\t(TARGET_CPU_DEFAULT_NAMES): Add bdver1.\n+\t(processor_type): Add PROCESSOR_BDVER1.\n+\t* config/i386/i386.md: Add bdver1 as a new cpu attribute to match\n+\tprocessor_type in config/i386/i386.h.\n+\tAdd check for TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL to emit\n+\tmovaps <reg, reg> instead of movapd <reg, reg> when replacing\n+\tmovsd <reg, reg> or movss <reg, reg> for SSE and AVX.\n+\tAdd check for  TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\n+\tto emit packed xor instead of packed double/packed integer\n+\txor for SSE and AVX when moving a zero value.\n+\t* config/i386/sse.md: Add check for TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\n+\t to emit movaps instead of movapd/movdqa for SSE and AVX.\n+\tAdd check for TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL to emit packed single\n+\tlogical operations i.e and, or and xor instead of packed double logical\n+\toperations for SSE and AVX. \n+\t* config/i386/i386-c.c: \n+\t(ix86_target_macros_internal): Add PROCESSOR_BDVER1.\n+\t* config/i386/driver-i386.c: Turn on -mtune=native for BDVER1.\n+\t(has_fma4, has_xop): New.\n+\t* config/i386/i386.c (bdver1_cost): New variable.\n+\t(m_BDVER1): New macro.\n+\t(m_AMD_MULTIPLE): Add m_BDVER1.\n+\t(x86_tune_use_leave, x86_tune_push_memory, x86_tune_unroll_strlen,\n+\t x86_tune_deep_branch_prediction, x86_tune_use_sahf, x86_tune_movx,\n+\t x86_tune_use_simode_fiop, x86_tune_promote_qimode, \n+\t x86_tune_add_esp_8, x86_tune_tune_sub_esp_4, x86_tune_sub_esp_8,\n+\t x86_tune_integer_dfmode_moves, x86_tune_partial_reg_dependency,\n+\t x86_tune_sse_partial_reg_dependency, x86_tune_sse_unaligned_load_optimal,\n+\t x86_tune_sse_unaligned_store_optimal, x86_tune_sse_typeless_stores,\n+\t x86_tune_memory_mismatch_stall, x86_tune_use_ffreep,\n+\t x86_tune_inter_unit_moves, x86_tune_inter_unit_conversions,\n+\t x86_tune_use_bt, x86_tune_pad_returns, x86_tune_slow_imul_imm32_mem,\n+\t x86_tune_slow_imul_imm8, x86_tune_fuse_cmp_and_branch): \n+\tEnable/disable for bdver1.\n+\t(processor_target_table): Add bdver1_cost.\n+\t(cpu_names): Add bdver1.\n+\t(override_options): Set up PROCESSOR_BDVER1 for bdver1 entry in\n+\t processor_alias_table.\n+\t(ix86_expand_vector_move_misalign): Change \n+\t TARGET_SSE_UNALIGNED_MOVE_OPTIMAL to TARGET_SSE_UNALIGNED_LOAD_OPTIMAL.\n+\t Check for TARGET_SSE_UNALIGNED_STORE_OPTIMAL.\n+\t Check for TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL to emit movups instead\n+\t of movupd/movdqu for SSE and AVX.\n+\t(ix86_tune_issue_rate): Add PROCESSOR_BDVER1.\n+\t(ix86_tune_adjust_cost): Add code for bdver1.\n+\t(standard_sse_constant_opcode): Add check for\n+\tTARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL to emit packed single xor instead\n+\tof packed double xor for SSE and AVX.\n+\n 2010-05-14  Pat Haugen  <pthaugen@us.ibm.com>\n \n \t* tree-ssa-loop.prefetch.c (prune_ref_by_group_reuse): Cast abs()"}, {"sha": "da56fbecc20b8c7932b312a04d00de0850d7b1f9", "filename": "gcc/config.gcc", "status": "modified", "additions": 16, "deletions": 8, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2Fconfig.gcc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2Fconfig.gcc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig.gcc?ref=1133125eb84a5326b5e59595b00b5ec8add169dc", "patch": "@@ -1139,7 +1139,7 @@ i[34567]86-*-linux* | i[34567]86-*-kfreebsd*-gnu | i[34567]86-*-knetbsd*-gnu | i\n \t\t\tneed_64bit_hwint=yes\n \t\t\tneed_64bit_isa=yes\n \t\t\tcase X\"${with_cpu}\" in\n-\t\t\tXgeneric|Xatom|Xcore2|Xnocona|Xx86-64|Xamdfam10|Xbarcelona|Xk8|Xopteron|Xathlon64|Xathlon-fx|Xathlon64-sse3|Xk8-sse3|Xopteron-sse3)\n+\t\t\tXgeneric|Xatom|Xcore2|Xnocona|Xx86-64|Xbdver1|Xamdfam10|Xbarcelona|Xk8|Xopteron|Xathlon64|Xathlon-fx|Xathlon64-sse3|Xk8-sse3|Xopteron-sse3)\n \t\t\t\t;;\n \t\t\tX)\n \t\t\t\tif test x$with_cpu_64 = x; then\n@@ -1148,7 +1148,7 @@ i[34567]86-*-linux* | i[34567]86-*-kfreebsd*-gnu | i[34567]86-*-knetbsd*-gnu | i\n \t\t\t\t;;\n \t\t\t*)\n \t\t\t\techo \"Unsupported CPU used in --with-cpu=$with_cpu, supported values:\" 1>&2\n-\t\t\t\techo \"generic atom core2 nocona x86-64 amdfam10 barcelona k8 opteron athlon64 athlon-fx athlon64-sse3 k8-sse3 opteron-sse3\" 1>&2\n+\t\t\t\techo \"generic atom core2 nocona x86-64 bdver1 amdfam10 barcelona k8 opteron athlon64 athlon-fx athlon64-sse3 k8-sse3 opteron-sse3\" 1>&2\n \t\t\t\texit 1\n \t\t\t\t;;\n \t\t\tesac\n@@ -1266,7 +1266,7 @@ i[34567]86-*-solaris2*)\n \t\tneed_64bit_isa=yes\n \t\tuse_gcc_stdint=wrap\n \t\tcase X\"${with_cpu}\" in\n-\t\tXgeneric|Xatom|Xcore2|Xnocona|Xx86-64|Xamdfam10|Xbarcelona|Xk8|Xopteron|Xathlon64|Xathlon-fx|Xathlon64-sse3|Xk8-sse3|Xopteron-sse3)\n+\t\tXgeneric|Xatom|Xcore2|Xnocona|Xx86-64|Xbdver1|Xamdfam10|Xbarcelona|Xk8|Xopteron|Xathlon64|Xathlon-fx|Xathlon64-sse3|Xk8-sse3|Xopteron-sse3)\n \t\t\t;;\n \t\tX)\n \t\t\tif test x$with_cpu_64 = x; then\n@@ -1275,7 +1275,7 @@ i[34567]86-*-solaris2*)\n \t\t\t;;\n \t\t*)\n \t\t\techo \"Unsupported CPU used in --with-cpu=$with_cpu, supported values:\" 1>&2\n-\t\t\techo \"generic atom core2 nocona x86-64 amdfam10 barcelona k8 opteron athlon64 athlon-fx athlon64-sse3 k8-sse3 opteron-sse3\" 1>&2\n+\t\t\techo \"generic atom core2 nocona x86-64 bdver1 amdfam10 barcelona k8 opteron athlon64 athlon-fx athlon64-sse3 k8-sse3 opteron-sse3\" 1>&2\n \t\t\texit 1\n \t\t\t;;\n \t\tesac\n@@ -1346,7 +1346,7 @@ i[34567]86-*-mingw* | x86_64-*-mingw*)\n \t\t\tif test x$enable_targets = xall; then\n \t\t\t\ttm_defines=\"${tm_defines} TARGET_BI_ARCH=1\"\n \t\t\t\tcase X\"${with_cpu}\" in\n-\t\t\t\tXgeneric|Xatom|Xcore2|Xnocona|Xx86-64|Xamdfam10|Xbarcelona|Xk8|Xopteron|Xathlon64|Xathlon-fx|Xathlon64-sse3|Xk8-sse3|Xopteron-sse3)\n+\t\t\t\tXgeneric|Xatom|Xcore2|Xnocona|Xx86-64|Xbdver1|Xamdfam10|Xbarcelona|Xk8|Xopteron|Xathlon64|Xathlon-fx|Xathlon64-sse3|Xk8-sse3|Xopteron-sse3)\n \t\t\t\t\t;;\n \t\t\t\tX)\n \t\t\t\t\tif test x$with_cpu_64 = x; then\n@@ -1355,7 +1355,7 @@ i[34567]86-*-mingw* | x86_64-*-mingw*)\n \t\t\t\t\t;;\n \t\t\t\t*)\n \t\t\t\t\techo \"Unsupported CPU used in --with-cpu=$with_cpu, supported values:\" 1>&2\n-\t\t\t\t\techo \"generic atom core2 nocona x86-64 amdfam10 barcelona k8 opteron athlon64 athlon-fx athlon64-sse3 k8-sse3 opteron-sse3\" 1>&2\n+\t\t\t\t\techo \"generic atom core2 nocona x86-64 bdver1 amdfam10 barcelona k8 opteron athlon64 athlon-fx athlon64-sse3 k8-sse3 opteron-sse3\" 1>&2\n \t\t\t\t\texit 1\n \t\t\t\t\t;;\n \t\t\t\tesac\n@@ -2626,6 +2626,10 @@ case ${target} in\n     ;;\n   i686-*-* | i786-*-*)\n     case ${target_noncanonical} in\n+      bdver1-*)\n+\tarch=bdver1\n+\tcpu=bdver1\n+\t;;\n       amdfam10-*|barcelona-*)\n \tarch=amdfam10\n \tcpu=amdfam10\n@@ -2703,6 +2707,10 @@ case ${target} in\n     ;;\n   x86_64-*-*)\n     case ${target_noncanonical} in\n+      bdver1-*)\n+\tarch=bdver1\n+\tcpu=bdver1\n+\t;;\n       amdfam10-*|barcelona-*)\n \tarch=amdfam10\n \tcpu=amdfam10\n@@ -3109,8 +3117,8 @@ case \"${target}\" in\n \t\t\t\t;;\n \t\t\t\"\" | x86-64 | generic | native \\\n \t\t\t| k8 | k8-sse3 | athlon64 | athlon64-sse3 | opteron \\\n-\t\t\t| opteron-sse3 | athlon-fx | amdfam10 | barcelona \\\n-\t\t\t| nocona | core2 | atom)\n+\t\t\t| opteron-sse3 | athlon-fx | bdver1 | amdfam10 \\\n+\t\t\t| barcelona | nocona | core2 | atom)\n \t\t\t\t# OK\n \t\t\t\t;;\n \t\t\t*)"}, {"sha": "8a768577c39206890466185d06d50301d7b0a4ea", "filename": "gcc/config/i386/driver-i386.c", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2Fconfig%2Fi386%2Fdriver-i386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2Fconfig%2Fi386%2Fdriver-i386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fdriver-i386.c?ref=1133125eb84a5326b5e59595b00b5ec8add169dc", "patch": "@@ -396,6 +396,7 @@ const char *host_detect_local_cpu (int argc, const char **argv)\n   unsigned int has_movbe = 0, has_sse4_1 = 0, has_sse4_2 = 0;\n   unsigned int has_popcnt = 0, has_aes = 0, has_avx = 0;\n   unsigned int has_pclmul = 0, has_abm = 0, has_lwp = 0;\n+  unsigned int has_fma4 = 0, has_xop = 0;\n \n   bool arch;\n \n@@ -460,6 +461,8 @@ const char *host_detect_local_cpu (int argc, const char **argv)\n       has_sse4a = ecx & bit_SSE4a;\n       has_abm = ecx & bit_ABM;\n       has_lwp = ecx & bit_LWP;\n+      has_fma4 = ecx & bit_FMA4;\n+      has_xop = ecx & bit_XOP;\n \n       has_longmode = edx & bit_LM;\n       has_3dnowp = edx & bit_3DNOWP;\n@@ -490,6 +493,8 @@ const char *host_detect_local_cpu (int argc, const char **argv)\n \n       if (name == SIG_GEODE)\n \tprocessor = PROCESSOR_GEODE;\n+      else if (has_xop)\n+\tprocessor = PROCESSOR_BDVER1;\n       else if (has_sse4a)\n \tprocessor = PROCESSOR_AMDFAM10;\n       else if (has_sse2 || has_longmode)\n@@ -629,6 +634,9 @@ const char *host_detect_local_cpu (int argc, const char **argv)\n     case PROCESSOR_AMDFAM10:\n       cpu = \"amdfam10\";\n       break;\n+    case PROCESSOR_BDVER1:\n+      cpu = \"bdver1\";\n+      break;\n \n     default:\n       /* Use something reasonable.  */\n@@ -674,6 +682,10 @@ const char *host_detect_local_cpu (int argc, const char **argv)\n \toptions = concat (options, \" -mabm\", NULL);\n       if (has_lwp)\n \toptions = concat (options, \" -mlwp\", NULL);\n+      if (has_fma4)\n+\toptions = concat (options, \" -mfma4\", NULL);\n+      if (has_xop)\n+\toptions = concat (options, \" -mxop\", NULL);\n \n       if (has_avx)\n \toptions = concat (options, \" -mavx\", NULL);"}, {"sha": "285f6ef3c930ebcafad4fc2804125c6be3ef880d", "filename": "gcc/config/i386/i386-c.c", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2Fconfig%2Fi386%2Fi386-c.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2Fconfig%2Fi386%2Fi386-c.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-c.c?ref=1133125eb84a5326b5e59595b00b5ec8add169dc", "patch": "@@ -107,6 +107,10 @@ ix86_target_macros_internal (int isa_flag,\n       def_or_undef (parse_in, \"__amdfam10\");\n       def_or_undef (parse_in, \"__amdfam10__\");\n       break;\n+    case PROCESSOR_BDVER1:\n+      def_or_undef (parse_in, \"__bdver1\");\n+      def_or_undef (parse_in, \"__bdver1__\");\n+      break;\n     case PROCESSOR_PENTIUM4:\n       def_or_undef (parse_in, \"__pentium4\");\n       def_or_undef (parse_in, \"__pentium4__\");\n@@ -182,6 +186,9 @@ ix86_target_macros_internal (int isa_flag,\n     case PROCESSOR_AMDFAM10:\n       def_or_undef (parse_in, \"__tune_amdfam10__\");\n       break;\n+    case PROCESSOR_BDVER1:\n+      def_or_undef (parse_in, \"__tune_bdver1__\");\n+      break;\n     case PROCESSOR_PENTIUM4:\n       def_or_undef (parse_in, \"__tune_pentium4__\");\n       break;"}, {"sha": "b99586a52aecb631c7ddf3ea8c8be187b13aded4", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 187, "deletions": 37, "changes": 224, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=1133125eb84a5326b5e59595b00b5ec8add169dc", "patch": "@@ -819,6 +819,93 @@ struct processor_costs amdfam10_cost = {\n   1,                                    /* cond_not_taken_branch_cost.  */\n };\n \n+struct processor_costs bdver1_cost = {\n+  COSTS_N_INSNS (1),                    /* cost of an add instruction */\n+  COSTS_N_INSNS (2),                    /* cost of a lea instruction */\n+  COSTS_N_INSNS (1),                    /* variable shift costs */\n+  COSTS_N_INSNS (1),                    /* constant shift costs */\n+  {COSTS_N_INSNS (3),                   /* cost of starting multiply for QI */\n+   COSTS_N_INSNS (4),                   /*                               HI */\n+   COSTS_N_INSNS (3),                   /*                               SI */\n+   COSTS_N_INSNS (4),                   /*                               DI */\n+   COSTS_N_INSNS (5)},                  /*                               other */\n+  0,                                    /* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (19),                  /* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (35),                  /*                          HI */\n+   COSTS_N_INSNS (51),                  /*                          SI */\n+   COSTS_N_INSNS (83),                  /*                          DI */\n+   COSTS_N_INSNS (83)},                 /*                          other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  9,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t\t/* cost for loading QImode using movzbl */\n+  {3, 4, 3},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {3, 4, 3},\t\t\t\t/* cost of storing integer registers */\n+  4,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {4, 4, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t   \t\t\t   in SFmode, DFmode and XFmode */\n+  {6, 6, 8},\t\t\t\t/* cost of storing fp registers\n+ \t\t   \t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {3, 3},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {4, 4},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {4, 4, 3},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {4, 4, 5},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  3,\t\t\t\t\t/* MMX or SSE register to integer */\n+  \t\t\t\t\t/* On K8\n+  \t\t\t\t\t    MOVD reg64, xmmreg \tDouble\tFSTORE 4\n+\t\t\t\t\t    MOVD reg32, xmmreg \tDouble\tFSTORE 4\n+\t\t\t\t\t   On AMDFAM10\n+\t\t\t\t\t    MOVD reg64, xmmreg \tDouble\tFADD 3\n+                                                                1/1  1/1\n+\t\t\t\t\t    MOVD reg32, xmmreg \tDouble\tFADD 3\n+                                                                1/1  1/1 */\n+  64,\t\t\t\t\t/* size of l1 cache.  */\n+  1024,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  /* New AMD processors never drop prefetches; if they cannot be performed\n+     immediately, they are queued.  We set number of simultaneous prefetches\n+     to a large constant to reflect this (it probably is not a good idea not\n+     to limit number of prefetches at all, as their execution also takes some\n+     time).  */\n+  100,\t\t\t\t\t/* number of parallel prefetches */\n+  2,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (4),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (4),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (19),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (35),\t\t\t/* cost of FSQRT instruction.  */\n+\n+  /*  BDVER1 has optimized REP instruction for medium sized blocks, but for\n+      very small blocks it is better to use loop. For large blocks, libcall can\n+      do nontemporary accesses and beat inline considerably.  */\n+  {{libcall, {{6, loop}, {14, unrolled_loop}, {-1, rep_prefix_4_byte}}},\n+   {libcall, {{16, loop}, {8192, rep_prefix_8_byte}, {-1, libcall}}}},\n+  {{libcall, {{8, loop}, {24, unrolled_loop},\n+\t      {2048, rep_prefix_4_byte}, {-1, libcall}}},\n+   {libcall, {{48, unrolled_loop}, {8192, rep_prefix_8_byte}, {-1, libcall}}}},\n+  4,                                    /* scalar_stmt_cost.  */\n+  2,                                    /* scalar load_cost.  */\n+  2,                                    /* scalar_store_cost.  */\n+  6,                                    /* vec_stmt_cost.  */\n+  0,                                    /* vec_to_scalar_cost.  */\n+  2,                                    /* scalar_to_vec_cost.  */\n+  2,                                    /* vec_align_load_cost.  */\n+  2,                                    /* vec_unalign_load_cost.  */\n+  2,                                    /* vec_store_cost.  */\n+  2,                                    /* cond_taken_branch_cost.  */\n+  1,                                    /* cond_not_taken_branch_cost.  */\n+};\n+\n static const\n struct processor_costs pentium4_cost = {\n   COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n@@ -1276,7 +1363,8 @@ const struct processor_costs *ix86_cost = &pentium_cost;\n #define m_ATHLON  (1<<PROCESSOR_ATHLON)\n #define m_ATHLON_K8  (m_K8 | m_ATHLON)\n #define m_AMDFAM10  (1<<PROCESSOR_AMDFAM10)\n-#define m_AMD_MULTIPLE  (m_K8 | m_ATHLON | m_AMDFAM10)\n+#define m_BDVER1  (1<<PROCESSOR_BDVER1)\n+#define m_AMD_MULTIPLE  (m_K8 | m_ATHLON | m_AMDFAM10 | m_BDVER1)\n \n #define m_GENERIC32 (1<<PROCESSOR_GENERIC32)\n #define m_GENERIC64 (1<<PROCESSOR_GENERIC64)\n@@ -1321,7 +1409,7 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n   ~m_386,\n \n   /* X86_TUNE_USE_SAHF */\n-  m_ATOM | m_PPRO | m_K6_GEODE | m_K8 | m_AMDFAM10 | m_PENT4\n+  m_ATOM | m_PPRO | m_K6_GEODE | m_K8 | m_AMDFAM10 | m_BDVER1 | m_PENT4\n   | m_NOCONA | m_CORE2 | m_GENERIC,\n \n   /* X86_TUNE_MOVX: Enable to zero extend integer registers to avoid\n@@ -1425,10 +1513,16 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n      while enabling it on K8 brings roughly 2.4% regression that can be partly\n      masked by careful scheduling of moves.  */\n   m_ATOM | m_PENT4 | m_NOCONA | m_PPRO | m_CORE2 | m_GENERIC\n-  | m_AMDFAM10,\n+  | m_AMDFAM10 | m_BDVER1,\n \n-  /* X86_TUNE_SSE_UNALIGNED_MOVE_OPTIMAL */\n-  m_AMDFAM10,\n+  /* X86_TUNE_SSE_UNALIGNED_LOAD_OPTIMAL */\n+  m_AMDFAM10 | m_BDVER1,\n+\n+  /* X86_TUNE_SSE_UNALIGNED_STORE_OPTIMAL */\n+  m_BDVER1,\n+\n+  /* X86_TUNE_SSE_PACKED_SINGLE_INSN_OPTIMAL */\n+  m_BDVER1,\n \n   /* X86_TUNE_SSE_SPLIT_REGS: Set for machines where the type and dependencies\n      are resolved on SSE register parts instead of whole registers, so we may\n@@ -1461,7 +1555,7 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n   ~(m_AMD_MULTIPLE | m_GENERIC),\n \n   /* X86_TUNE_INTER_UNIT_CONVERSIONS */\n-  ~(m_AMDFAM10),\n+  ~(m_AMDFAM10 | m_BDVER1),\n \n   /* X86_TUNE_FOUR_JUMP_LIMIT: Some CPU cores are not able to predict more\n      than 4 branch instructions in the 16 byte window.  */\n@@ -1497,11 +1591,11 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n \n   /* X86_TUNE_SLOW_IMUL_IMM32_MEM: Imul of 32-bit constant and memory is\n      vector path on AMD machines.  */\n-  m_K8 | m_GENERIC64 | m_AMDFAM10,\n+  m_K8 | m_GENERIC64 | m_AMDFAM10 | m_BDVER1,\n \n   /* X86_TUNE_SLOW_IMUL_IMM8: Imul of 8-bit constant is vector path on AMD\n      machines.  */\n-  m_K8 | m_GENERIC64 | m_AMDFAM10,\n+  m_K8 | m_GENERIC64 | m_AMDFAM10 | m_BDVER1,\n \n   /* X86_TUNE_MOVE_M1_VIA_OR: On pentiums, it is faster to load -1 via OR\n      than a MOV.  */\n@@ -1527,7 +1621,7 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n   /* X86_TUNE_FUSE_CMP_AND_BRANCH: Fuse a compare or test instruction\n      with a subsequent conditional jump instruction into a single\n      compare-and-branch uop.  */\n-  m_CORE2,\n+  m_CORE2 | m_BDVER1,\n \n   /* X86_TUNE_OPT_AGU: Optimize for Address Generation Unit. This flag\n      will impact LEA instruction selection. */\n@@ -2067,6 +2161,7 @@ static const struct ptt processor_target_table[PROCESSOR_max] =\n   {&generic32_cost, 16, 7, 16, 7, 16},\n   {&generic64_cost, 16, 10, 16, 10, 16},\n   {&amdfam10_cost, 32, 24, 32, 7, 32},\n+  {&bdver1_cost, 32, 24, 32, 7, 32},\n   {&atom_cost, 16, 7, 16, 7, 16}\n };\n \n@@ -2093,7 +2188,8 @@ static const char *const cpu_names[TARGET_CPU_DEFAULT_max] =\n   \"athlon\",\n   \"athlon-4\",\n   \"k8\",\n-  \"amdfam10\"\n+  \"amdfam10\",\n+  \"bdver1\"\n };\n \f\n /* Implement TARGET_HANDLE_OPTION.  */\n@@ -2751,6 +2847,11 @@ override_options (bool main_args_p)\n       {\"barcelona\", PROCESSOR_AMDFAM10, CPU_AMDFAM10,\n \tPTA_64BIT | PTA_MMX | PTA_3DNOW | PTA_3DNOW_A | PTA_SSE\n \t| PTA_SSE2 | PTA_SSE3 | PTA_SSE4A | PTA_CX16 | PTA_ABM},\n+      {\"bdver1\", PROCESSOR_BDVER1, CPU_BDVER1,\n+\tPTA_64BIT | PTA_MMX | PTA_3DNOW | PTA_3DNOW_A | PTA_SSE\n+\t| PTA_SSE2 | PTA_SSE3 | PTA_SSE4A | PTA_CX16 | PTA_ABM\n+\t| PTA_SSSE3 | PTA_SSE4_1 | PTA_SSE4_2 | PTA_AES \n+\t| PTA_PCLMUL | PTA_AVX | PTA_FMA4 | PTA_XOP | PTA_LWP},\n       {\"generic32\", PROCESSOR_GENERIC32, CPU_PENTIUMPRO,\n \t0 /* flags are only used for -march switch.  */ },\n       {\"generic64\", PROCESSOR_GENERIC64, CPU_GENERIC64,\n@@ -7469,15 +7570,27 @@ standard_sse_constant_opcode (rtx insn, rtx x)\n \tcase MODE_V4SF:\n \t  return TARGET_AVX ? \"vxorps\\t%0, %0, %0\" : \"xorps\\t%0, %0\";\n \tcase MODE_V2DF:\n-\t  return TARGET_AVX ? \"vxorpd\\t%0, %0, %0\" : \"xorpd\\t%0, %0\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return TARGET_AVX ? \"vxorps\\t%0, %0, %0\" : \"xorps\\t%0, %0\";\n+\t  else\n+\t    return TARGET_AVX ? \"vxorpd\\t%0, %0, %0\" : \"xorpd\\t%0, %0\";\t    \n \tcase MODE_TI:\n-\t  return TARGET_AVX ? \"vpxor\\t%0, %0, %0\" : \"pxor\\t%0, %0\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return TARGET_AVX ? \"vxorps\\t%0, %0, %0\" : \"xorps\\t%0, %0\";\n+\t  else\n+\t    return TARGET_AVX ? \"vpxor\\t%0, %0, %0\" : \"pxor\\t%0, %0\";\n \tcase MODE_V8SF:\n \t  return \"vxorps\\t%x0, %x0, %x0\";\n \tcase MODE_V4DF:\n-\t  return \"vxorpd\\t%x0, %x0, %x0\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"vxorps\\t%x0, %x0, %x0\";\n+\t  else\n+\t    return \"vxorpd\\t%x0, %x0, %x0\";\n \tcase MODE_OI:\n-\t  return \"vpxor\\t%x0, %x0, %x0\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"vxorps\\t%x0, %x0, %x0\";\n+\t  else\n+\t    return \"vpxor\\t%x0, %x0, %x0\";\n \tdefault:\n \t  break;\n \t}\n@@ -13233,6 +13346,14 @@ ix86_expand_vector_move_misalign (enum machine_mode mode, rtx operands[])\n \t  switch (GET_MODE_SIZE (mode))\n \t    {\n \t    case 16:\n+\t      /*  If we're optimizing for size, movups is the smallest.  */\n+\t      if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t\t{\n+\t\t  op0 = gen_lowpart (V4SFmode, op0);\n+\t\t  op1 = gen_lowpart (V4SFmode, op1);\n+\t\t  emit_insn (gen_avx_movups (op0, op1));\n+\t\t  return;\n+\t\t}\n \t      op0 = gen_lowpart (V16QImode, op0);\n \t      op1 = gen_lowpart (V16QImode, op1);\n \t      emit_insn (gen_avx_movdqu (op0, op1));\n@@ -13259,6 +13380,13 @@ ix86_expand_vector_move_misalign (enum machine_mode mode, rtx operands[])\n \t      emit_insn (gen_avx_movups256 (op0, op1));\n \t      break;\n \t    case V2DFmode:\n+\t      if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t\t{\n+\t\t  op0 = gen_lowpart (V4SFmode, op0);\n+\t\t  op1 = gen_lowpart (V4SFmode, op1);\n+\t\t  emit_insn (gen_avx_movups (op0, op1));\n+\t\t  return;\n+\t\t}\n \t      emit_insn (gen_avx_movupd (op0, op1));\n \t      break;\n \t    case V4DFmode:\n@@ -13279,7 +13407,8 @@ ix86_expand_vector_move_misalign (enum machine_mode mode, rtx operands[])\n   if (MEM_P (op1))\n     {\n       /* If we're optimizing for size, movups is the smallest.  */\n-      if (optimize_insn_for_size_p ())\n+      if (optimize_insn_for_size_p () \n+\t  || TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n \t{\n \t  op0 = gen_lowpart (V4SFmode, op0);\n \t  op1 = gen_lowpart (V4SFmode, op1);\n@@ -13302,13 +13431,13 @@ ix86_expand_vector_move_misalign (enum machine_mode mode, rtx operands[])\n         {\n           rtx zero;\n \n-          if (TARGET_SSE_UNALIGNED_MOVE_OPTIMAL)\n-            {\n-              op0 = gen_lowpart (V2DFmode, op0);\n-              op1 = gen_lowpart (V2DFmode, op1);\n-              emit_insn (gen_sse2_movupd (op0, op1));\n-              return;\n-            }\n+\t  if (TARGET_SSE_UNALIGNED_LOAD_OPTIMAL)\n+\t    {\n+\t      op0 = gen_lowpart (V2DFmode, op0);\n+\t      op1 = gen_lowpart (V2DFmode, op1);\n+\t      emit_insn (gen_sse2_movupd (op0, op1));\n+\t      return;\n+\t    }\n \n \t  /* When SSE registers are split into halves, we can avoid\n \t     writing to the top half twice.  */\n@@ -13337,12 +13466,12 @@ ix86_expand_vector_move_misalign (enum machine_mode mode, rtx operands[])\n \t}\n       else\n         {\n-          if (TARGET_SSE_UNALIGNED_MOVE_OPTIMAL)\n-            {\n-              op0 = gen_lowpart (V4SFmode, op0);\n-              op1 = gen_lowpart (V4SFmode, op1);\n-              emit_insn (gen_sse_movups (op0, op1));\n-              return;\n+\t  if (TARGET_SSE_UNALIGNED_LOAD_OPTIMAL)\n+\t    {\n+\t      op0 = gen_lowpart (V4SFmode, op0);\n+\t      op1 = gen_lowpart (V4SFmode, op1);\n+\t      emit_insn (gen_sse_movups (op0, op1));\n+\t      return;\n             }\n \n \t  if (TARGET_SSE_PARTIAL_REG_DEPENDENCY)\n@@ -13361,7 +13490,8 @@ ix86_expand_vector_move_misalign (enum machine_mode mode, rtx operands[])\n   else if (MEM_P (op0))\n     {\n       /* If we're optimizing for size, movups is the smallest.  */\n-      if (optimize_insn_for_size_p ())\n+      if (optimize_insn_for_size_p ()\n+\t  || TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n \t{\n \t  op0 = gen_lowpart (V4SFmode, op0);\n \t  op1 = gen_lowpart (V4SFmode, op1);\n@@ -13382,19 +13512,37 @@ ix86_expand_vector_move_misalign (enum machine_mode mode, rtx operands[])\n \n       if (TARGET_SSE2 && mode == V2DFmode)\n \t{\n-\t  m = adjust_address (op0, DFmode, 0);\n-\t  emit_insn (gen_sse2_storelpd (m, op1));\n-\t  m = adjust_address (op0, DFmode, 8);\n-\t  emit_insn (gen_sse2_storehpd (m, op1));\n+\t  if (TARGET_SSE_UNALIGNED_STORE_OPTIMAL)\n+\t    {\n+\t      op0 = gen_lowpart (V2DFmode, op0);\n+\t      op1 = gen_lowpart (V2DFmode, op1);\n+\t      emit_insn (gen_sse2_movupd (op0, op1));\t      \n+\t    }\n+\t  else\n+\t    {\n+\t      m = adjust_address (op0, DFmode, 0);\n+\t      emit_insn (gen_sse2_storelpd (m, op1));\n+\t      m = adjust_address (op0, DFmode, 8);\n+\t      emit_insn (gen_sse2_storehpd (m, op1));\n+\t    }\n \t}\n       else\n \t{\n \t  if (mode != V4SFmode)\n \t    op1 = gen_lowpart (V4SFmode, op1);\n-\t  m = adjust_address (op0, V2SFmode, 0);\n-\t  emit_insn (gen_sse_storelps (m, op1));\n-\t  m = adjust_address (op0, V2SFmode, 8);\n-\t  emit_insn (gen_sse_storehps (m, op1));\n+\n+\t  if (TARGET_SSE_UNALIGNED_STORE_OPTIMAL)\n+\t    {\n+\t      op0 = gen_lowpart (V4SFmode, op0);\n+\t      emit_insn (gen_sse_movups (op0, op1));\t      \n+\t    }\n+\t  else\n+\t    {\n+\t      m = adjust_address (op0, V2SFmode, 0);\n+\t      emit_insn (gen_sse_storelps (m, op1));\n+\t      m = adjust_address (op0, V2SFmode, 8);\n+\t      emit_insn (gen_sse_storehps (m, op1));\n+\t    }\n \t}\n     }\n   else\n@@ -19714,6 +19862,7 @@ ix86_issue_rate (void)\n     case PROCESSOR_NOCONA:\n     case PROCESSOR_GENERIC32:\n     case PROCESSOR_GENERIC64:\n+    case PROCESSOR_BDVER1:\n       return 3;\n \n     case PROCESSOR_CORE2:\n@@ -19903,6 +20052,7 @@ ix86_adjust_cost (rtx insn, rtx link, rtx dep_insn, int cost)\n     case PROCESSOR_ATHLON:\n     case PROCESSOR_K8:\n     case PROCESSOR_AMDFAM10:\n+    case PROCESSOR_BDVER1:\n     case PROCESSOR_ATOM:\n     case PROCESSOR_GENERIC32:\n     case PROCESSOR_GENERIC64:"}, {"sha": "3dd6f43b67d13b9b3d70624a4f42969ebf9b11d9", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 12, "deletions": 3, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=1133125eb84a5326b5e59595b00b5ec8add169dc", "patch": "@@ -240,6 +240,7 @@ extern const struct processor_costs ix86_size_cost;\n #define TARGET_GENERIC64 (ix86_tune == PROCESSOR_GENERIC64)\n #define TARGET_GENERIC (TARGET_GENERIC32 || TARGET_GENERIC64)\n #define TARGET_AMDFAM10 (ix86_tune == PROCESSOR_AMDFAM10)\n+#define TARGET_BDVER1 (ix86_tune == PROCESSOR_BDVER1)\n #define TARGET_ATOM (ix86_tune == PROCESSOR_ATOM)\n \n /* Feature tests against the various tunings.  */\n@@ -277,7 +278,9 @@ enum ix86_tune_indices {\n   X86_TUNE_INTEGER_DFMODE_MOVES,\n   X86_TUNE_PARTIAL_REG_DEPENDENCY,\n   X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY,\n-  X86_TUNE_SSE_UNALIGNED_MOVE_OPTIMAL,\n+  X86_TUNE_SSE_UNALIGNED_LOAD_OPTIMAL,\n+  X86_TUNE_SSE_UNALIGNED_STORE_OPTIMAL,\n+  X86_TUNE_SSE_PACKED_SINGLE_INSN_OPTIMAL,\n   X86_TUNE_SSE_SPLIT_REGS,\n   X86_TUNE_SSE_TYPELESS_STORES,\n   X86_TUNE_SSE_LOAD0_BY_PXOR,\n@@ -352,8 +355,12 @@ extern unsigned char ix86_tune_features[X86_TUNE_LAST];\n \tix86_tune_features[X86_TUNE_PARTIAL_REG_DEPENDENCY]\n #define TARGET_SSE_PARTIAL_REG_DEPENDENCY \\\n \tix86_tune_features[X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY]\n-#define TARGET_SSE_UNALIGNED_MOVE_OPTIMAL \\\n-\tix86_tune_features[X86_TUNE_SSE_UNALIGNED_MOVE_OPTIMAL]\n+#define TARGET_SSE_UNALIGNED_LOAD_OPTIMAL \\\n+\tix86_tune_features[X86_TUNE_SSE_UNALIGNED_LOAD_OPTIMAL]\n+#define TARGET_SSE_UNALIGNED_STORE_OPTIMAL \\\n+\tix86_tune_features[X86_TUNE_SSE_UNALIGNED_STORE_OPTIMAL]\n+#define TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL \\\n+\tix86_tune_features[X86_TUNE_SSE_PACKED_SINGLE_INSN_OPTIMAL]\n #define TARGET_SSE_SPLIT_REGS\tix86_tune_features[X86_TUNE_SSE_SPLIT_REGS]\n #define TARGET_SSE_TYPELESS_STORES \\\n \tix86_tune_features[X86_TUNE_SSE_TYPELESS_STORES]\n@@ -591,6 +598,7 @@ enum target_cpu_default\n   TARGET_CPU_DEFAULT_athlon_sse,\n   TARGET_CPU_DEFAULT_k8,\n   TARGET_CPU_DEFAULT_amdfam10,\n+  TARGET_CPU_DEFAULT_bdver1,\n \n   TARGET_CPU_DEFAULT_max\n };\n@@ -2193,6 +2201,7 @@ enum processor_type\n   PROCESSOR_GENERIC32,\n   PROCESSOR_GENERIC64,\n   PROCESSOR_AMDFAM10,\n+  PROCESSOR_BDVER1,\n   PROCESSOR_ATOM,\n   PROCESSOR_max\n };"}, {"sha": "b98bce3ff98c4f21ad2649371cef22d230ef2a29", "filename": "gcc/config/i386/i386.md", "status": "modified", "additions": 49, "deletions": 13, "changes": 62, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2Fconfig%2Fi386%2Fi386.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2Fconfig%2Fi386%2Fi386.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.md?ref=1133125eb84a5326b5e59595b00b5ec8add169dc", "patch": "@@ -343,7 +343,7 @@\n \f\n ;; Processor type.\n (define_attr \"cpu\" \"none,pentium,pentiumpro,geode,k6,athlon,k8,core2,atom,\n-\t\t    generic64,amdfam10\"\n+\t\t    generic64,amdfam10,bdver1\"\n   (const (symbol_ref \"ix86_schedule\")))\n \n ;; A basic instruction type.  Refinements due to arguments to be\n@@ -3113,9 +3113,15 @@\n \tcase MODE_V4SF:\n \t  return \"%vxorps\\t%0, %d0\";\n \tcase MODE_V2DF:\n-\t  return \"%vxorpd\\t%0, %d0\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"%vxorps\\t%0, %d0\";\n+\t  else\n+\t    return \"%vxorpd\\t%0, %d0\";\n \tcase MODE_TI:\n-\t  return \"%vpxor\\t%0, %d0\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"%vxorps\\t%0, %d0\";\n+\t  else\n+\t    return \"%vpxor\\t%0, %d0\";\n \tdefault:\n \t  gcc_unreachable ();\n \t}\n@@ -3127,9 +3133,15 @@\n \tcase MODE_V4SF:\n \t  return \"%vmovaps\\t{%1, %0|%0, %1}\";\n \tcase MODE_V2DF:\n-\t  return \"%vmovapd\\t{%1, %0|%0, %1}\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"%vmovaps\\t{%1, %0|%0, %1}\";\n+\t  else\n+\t    return \"%vmovapd\\t{%1, %0|%0, %1}\";\n \tcase MODE_TI:\n-\t  return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"%vmovaps\\t{%1, %0|%0, %1}\";\n+\t  else\n+\t    return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n \tcase MODE_DI:\n \t  return \"%vmovq\\t{%1, %0|%0, %1}\";\n \tcase MODE_DF:\n@@ -3263,9 +3275,15 @@\n \tcase MODE_V4SF:\n \t  return \"%vxorps\\t%0, %d0\";\n \tcase MODE_V2DF:\n-\t  return \"%vxorpd\\t%0, %d0\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"%vxorps\\t%0, %d0\";\n+\t  else\n+\t    return \"%vxorpd\\t%0, %d0\";\n \tcase MODE_TI:\n-\t  return \"%vpxor\\t%0, %d0\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"%vxorps\\t%0, %d0\";\n+\t  else\n+\t    return \"%vpxor\\t%0, %d0\";\n \tdefault:\n \t  gcc_unreachable ();\n \t}\n@@ -3277,9 +3295,15 @@\n \tcase MODE_V4SF:\n \t  return \"%vmovaps\\t{%1, %0|%0, %1}\";\n \tcase MODE_V2DF:\n-\t  return \"%vmovapd\\t{%1, %0|%0, %1}\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"%vmovaps\\t{%1, %0|%0, %1}\";\n+\t  else\n+\t    return \"%vmovapd\\t{%1, %0|%0, %1}\";\n \tcase MODE_TI:\n-\t  return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"%vmovaps\\t{%1, %0|%0, %1}\";\n+\t  else\n+\t    return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n \tcase MODE_DI:\n \t  return \"%vmovq\\t{%1, %0|%0, %1}\";\n \tcase MODE_DF:\n@@ -3403,9 +3427,15 @@\n \tcase MODE_V4SF:\n \t  return \"xorps\\t%0, %0\";\n \tcase MODE_V2DF:\n-\t  return \"xorpd\\t%0, %0\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"xorps\\t%0, %0\";\n+\t  else\n+\t    return \"xorpd\\t%0, %0\";\n \tcase MODE_TI:\n-\t  return \"pxor\\t%0, %0\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"xorps\\t%0, %0\";\n+\t  else\n+\t    return \"pxor\\t%0, %0\";\n \tdefault:\n \t  gcc_unreachable ();\n \t}\n@@ -3417,9 +3447,15 @@\n \tcase MODE_V4SF:\n \t  return \"movaps\\t{%1, %0|%0, %1}\";\n \tcase MODE_V2DF:\n-\t  return \"movapd\\t{%1, %0|%0, %1}\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"movaps\\t{%1, %0|%0, %1}\";\n+\t  else\n+\t    return \"movapd\\t{%1, %0|%0, %1}\";\n \tcase MODE_TI:\n-\t  return \"movdqa\\t{%1, %0|%0, %1}\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"movaps\\t{%1, %0|%0, %1}\";\n+\t  else\n+\t    return \"movdqa\\t{%1, %0|%0, %1}\";\n \tcase MODE_DI:\n \t  return \"movq\\t{%1, %0|%0, %1}\";\n \tcase MODE_DF:"}, {"sha": "d028bd3940b0d4334b2dc39e7ceafb258a3e06d3", "filename": "gcc/config/i386/sse.md", "status": "modified", "additions": 40, "deletions": 8, "changes": 48, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2Fconfig%2Fi386%2Fsse.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1133125eb84a5326b5e59595b00b5ec8add169dc/gcc%2Fconfig%2Fi386%2Fsse.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fsse.md?ref=1133125eb84a5326b5e59595b00b5ec8add169dc", "patch": "@@ -194,9 +194,15 @@\n \t  return \"vmovaps\\t{%1, %0|%0, %1}\";\n \tcase MODE_V4DF:\n \tcase MODE_V2DF:\n-\t  return \"vmovapd\\t{%1, %0|%0, %1}\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"vmovaps\\t{%1, %0|%0, %1}\";\n+\t  else\n+\t    return \"vmovapd\\t{%1, %0|%0, %1}\";\n \tdefault:\n-\t  return \"vmovdqa\\t{%1, %0|%0, %1}\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"vmovaps\\t{%1, %0|%0, %1}\";\n+\t  else\n+\t    return \"vmovdqa\\t{%1, %0|%0, %1}\";\n \t}\n     default:\n       gcc_unreachable ();\n@@ -236,9 +242,15 @@\n \tcase MODE_V4SF:\n \t  return \"movaps\\t{%1, %0|%0, %1}\";\n \tcase MODE_V2DF:\n-\t  return \"movapd\\t{%1, %0|%0, %1}\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"movaps\\t{%1, %0|%0, %1}\";\n+\t  else\n+\t    return \"movapd\\t{%1, %0|%0, %1}\";\n \tdefault:\n-\t  return \"movdqa\\t{%1, %0|%0, %1}\";\n+\t  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+\t    return \"movaps\\t{%1, %0|%0, %1}\";\n+\t  else\n+\t    return \"movdqa\\t{%1, %0|%0, %1}\";\n \t}\n     default:\n       gcc_unreachable ();\n@@ -1611,7 +1623,12 @@\n \t  (match_operand:AVXMODEF2P 2 \"nonimmediate_operand\" \"xm\")))]\n   \"AVX_VEC_FLOAT_MODE_P (<MODE>mode)\n    && ix86_binary_operator_ok (<CODE>, <MODE>mode, operands)\"\n-  \"v<logic>p<avxmodesuffixf2c>\\t{%2, %1, %0|%0, %1, %2}\"\n+{\n+  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+    return \"v<logic>ps\\t{%2, %1, %0|%0, %1, %2}\";\n+  else\n+    return \"v<logic>p<avxmodesuffixf2c>\\t{%2, %1, %0|%0, %1, %2}\";\n+}\n   [(set_attr \"type\" \"sselog\")\n    (set_attr \"prefix\" \"vex\")\n    (set_attr \"mode\" \"<avxvecmode>\")])\n@@ -1631,7 +1648,12 @@\n \t  (match_operand:SSEMODEF2P 2 \"nonimmediate_operand\" \"xm\")))]\n   \"SSE_VEC_FLOAT_MODE_P (<MODE>mode)\n    && ix86_binary_operator_ok (<CODE>, <MODE>mode, operands)\"\n-  \"<logic>p<ssemodesuffixf2c>\\t{%2, %0|%0, %2}\"\n+{\n+  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+    return \"<logic>ps\\t{%2, %0|%0, %2}\";\n+  else\n+    return \"<logic>p<ssemodesuffixf2c>\\t{%2, %0|%0, %2}\";\n+}\n   [(set_attr \"type\" \"sselog\")\n    (set_attr \"mode\" \"<MODE>\")])\n \n@@ -1687,7 +1709,12 @@\n \t  (match_operand:MODEF 1 \"register_operand\" \"x\")\n \t  (match_operand:MODEF 2 \"register_operand\" \"x\")))]\n   \"AVX_FLOAT_MODE_P (<MODE>mode)\"\n-  \"v<logic>p<ssemodefsuffix>\\t{%2, %1, %0|%0, %1, %2}\"\n+{\n+  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+    return \"v<logic>ps\\t{%2, %1, %0|%0, %1, %2}\";\n+  else\n+    return \"v<logic>p<ssemodefsuffix>\\t{%2, %1, %0|%0, %1, %2}\";\n+}\n   [(set_attr \"type\" \"sselog\")\n    (set_attr \"prefix\" \"vex\")\n    (set_attr \"mode\" \"<ssevecmode>\")])\n@@ -1698,7 +1725,12 @@\n \t  (match_operand:MODEF 1 \"register_operand\" \"0\")\n \t  (match_operand:MODEF 2 \"register_operand\" \"x\")))]\n   \"SSE_FLOAT_MODE_P (<MODE>mode)\"\n-  \"<logic>p<ssemodefsuffix>\\t{%2, %0|%0, %2}\"\n+{\n+  if (TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL)\n+    return \"<logic>ps\\t{%2, %0|%0, %2}\";\n+  else\n+    return \"<logic>p<ssemodefsuffix>\\t{%2, %0|%0, %2}\";\n+}\n   [(set_attr \"type\" \"sselog\")\n    (set_attr \"mode\" \"<ssevecmode>\")])\n "}]}