{"sha": "77e994c9098df0da205b627d75d2282a4b983435", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NzdlOTk0YzkwOThkZjBkYTIwNWI2MjdkNzVkMjI4MmE0Yjk4MzQzNQ==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@linaro.org", "date": "2017-09-05T20:08:17Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2017-09-05T20:08:17Z"}, "message": "[61/77] Use scalar_int_mode in the AArch64 port\n\nThis patch makes the AArch64 port use scalar_int_mode in various places.\nOther ports won't need this kind of change; we only need it for AArch64\nbecause of the variable-sized SVE modes.\n\nThe only change in functionality is in the rtx_costs handling\nof CONST_INT.  If the caller doesn't supply a mode, we now pass\nword_mode rather than VOIDmode to aarch64_internal_mov_immediate.\naarch64_movw_imm will therefore not now truncate large constants\nin this situation.\n\n2017-09-05  Richard Sandiford  <richard.sandiford@linaro.org>\n\t    Alan Hayward  <alan.hayward@arm.com>\n\t    David Sherwood  <david.sherwood@arm.com>\n\ngcc/\n\t* config/aarch64/aarch64-protos.h (aarch64_is_extend_from_extract):\n\tTake a scalar_int_mode instead of a machine_mode.\n\t(aarch64_mask_and_shift_for_ubfiz_p): Likewise.\n\t(aarch64_output_scalar_simd_mov_immediate): Likewise.\n\t(aarch64_simd_scalar_immediate_valid_for_move): Likewise.\n\t(aarch64_simd_attr_length_rglist): Delete.\n\t* config/aarch64/aarch64.c (aarch64_is_extend_from_extract): Take\n\ta scalar_int_mode instead of a machine_mode.\n\t(aarch64_add_offset): Likewise.\n\t(aarch64_internal_mov_immediate): Likewise\n\t(aarch64_add_constant_internal): Likewise.\n\t(aarch64_add_constant): Likewise.\n\t(aarch64_movw_imm): Likewise.\n\t(aarch64_rtx_arith_op_extract_p): Likewise.\n\t(aarch64_mask_and_shift_for_ubfiz_p): Likewise.\n\t(aarch64_simd_scalar_immediate_valid_for_move): Likewise.\n\tRemove assert that the mode isn't a vector.\n\t(aarch64_output_scalar_simd_mov_immediate): Likewise.\n\t(aarch64_expand_mov_immediate): Update calls after above changes.\n\t(aarch64_output_casesi): Use as_a <scalar_int_mode>.\n\t(aarch64_and_bitmask_imm): Check for scalar integer modes.\n\t(aarch64_move_imm): Likewise.\n\t(aarch64_can_const_movi_rtx_p): Likewise.\n\t(aarch64_strip_extend): Likewise.\n\t(aarch64_extr_rtx_p): Likewise.\n\t(aarch64_rtx_costs): Likewise, using wode_mode as the mode of\n\ta CONST_INT when the mode parameter is VOIDmode.\n\t(aarch64_float_const_rtx_p): Use scalar_int_mode for a temporary.\n\nCo-Authored-By: Alan Hayward <alan.hayward@arm.com>\nCo-Authored-By: David Sherwood <david.sherwood@arm.com>\n\nFrom-SVN: r251735", "tree": {"sha": "e1d4036ce21757c05271d2abc630e53ffe714176", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/e1d4036ce21757c05271d2abc630e53ffe714176"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/77e994c9098df0da205b627d75d2282a4b983435", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/77e994c9098df0da205b627d75d2282a4b983435", "html_url": "https://github.com/Rust-GCC/gccrs/commit/77e994c9098df0da205b627d75d2282a4b983435", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/77e994c9098df0da205b627d75d2282a4b983435/comments", "author": null, "committer": null, "parents": [{"sha": "40300fa4545d51bb636d4f3ca4be6d6914cee321", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/40300fa4545d51bb636d4f3ca4be6d6914cee321", "html_url": "https://github.com/Rust-GCC/gccrs/commit/40300fa4545d51bb636d4f3ca4be6d6914cee321"}], "stats": {"total": 168, "additions": 111, "deletions": 57}, "files": [{"sha": "ae3edf516abc795484c53745569422ce785eb815", "filename": "gcc/ChangeLog", "status": "modified", "additions": 33, "deletions": 0, "changes": 33, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/77e994c9098df0da205b627d75d2282a4b983435/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/77e994c9098df0da205b627d75d2282a4b983435/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=77e994c9098df0da205b627d75d2282a4b983435", "patch": "@@ -1,3 +1,36 @@\n+2017-09-05  Richard Sandiford  <richard.sandiford@linaro.org>\n+\t    Alan Hayward  <alan.hayward@arm.com>\n+\t    David Sherwood  <david.sherwood@arm.com>\n+\n+\t* config/aarch64/aarch64-protos.h (aarch64_is_extend_from_extract):\n+\tTake a scalar_int_mode instead of a machine_mode.\n+\t(aarch64_mask_and_shift_for_ubfiz_p): Likewise.\n+\t(aarch64_output_scalar_simd_mov_immediate): Likewise.\n+\t(aarch64_simd_scalar_immediate_valid_for_move): Likewise.\n+\t(aarch64_simd_attr_length_rglist): Delete.\n+\t* config/aarch64/aarch64.c (aarch64_is_extend_from_extract): Take\n+\ta scalar_int_mode instead of a machine_mode.\n+\t(aarch64_add_offset): Likewise.\n+\t(aarch64_internal_mov_immediate): Likewise\n+\t(aarch64_add_constant_internal): Likewise.\n+\t(aarch64_add_constant): Likewise.\n+\t(aarch64_movw_imm): Likewise.\n+\t(aarch64_rtx_arith_op_extract_p): Likewise.\n+\t(aarch64_mask_and_shift_for_ubfiz_p): Likewise.\n+\t(aarch64_simd_scalar_immediate_valid_for_move): Likewise.\n+\tRemove assert that the mode isn't a vector.\n+\t(aarch64_output_scalar_simd_mov_immediate): Likewise.\n+\t(aarch64_expand_mov_immediate): Update calls after above changes.\n+\t(aarch64_output_casesi): Use as_a <scalar_int_mode>.\n+\t(aarch64_and_bitmask_imm): Check for scalar integer modes.\n+\t(aarch64_move_imm): Likewise.\n+\t(aarch64_can_const_movi_rtx_p): Likewise.\n+\t(aarch64_strip_extend): Likewise.\n+\t(aarch64_extr_rtx_p): Likewise.\n+\t(aarch64_rtx_costs): Likewise, using wode_mode as the mode of\n+\ta CONST_INT when the mode parameter is VOIDmode.\n+\t(aarch64_float_const_rtx_p): Use scalar_int_mode for a temporary.\n+\n 2017-09-05  Richard Sandiford  <richard.sandiford@linaro.org>\n \n \t* machmode.h (bitwise_mode_for_mode): Return opt_mode."}, {"sha": "d2164058de4a16277d8048b1cb11c4af371ea395", "filename": "gcc/config/aarch64/aarch64-protos.h", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/77e994c9098df0da205b627d75d2282a4b983435/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/77e994c9098df0da205b627d75d2282a4b983435/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h?ref=77e994c9098df0da205b627d75d2282a4b983435", "patch": "@@ -332,20 +332,19 @@ bool aarch64_function_arg_regno_p (unsigned);\n bool aarch64_fusion_enabled_p (enum aarch64_fusion_pairs);\n bool aarch64_gen_movmemqi (rtx *);\n bool aarch64_gimple_fold_builtin (gimple_stmt_iterator *);\n-bool aarch64_is_extend_from_extract (machine_mode, rtx, rtx);\n+bool aarch64_is_extend_from_extract (scalar_int_mode, rtx, rtx);\n bool aarch64_is_long_call_p (rtx);\n bool aarch64_is_noplt_call_p (rtx);\n bool aarch64_label_mentioned_p (rtx);\n void aarch64_declare_function_name (FILE *, const char*, tree);\n bool aarch64_legitimate_pic_operand_p (rtx);\n-bool aarch64_mask_and_shift_for_ubfiz_p (machine_mode, rtx, rtx);\n+bool aarch64_mask_and_shift_for_ubfiz_p (scalar_int_mode, rtx, rtx);\n bool aarch64_zero_extend_const_eq (machine_mode, rtx, machine_mode, rtx);\n bool aarch64_move_imm (HOST_WIDE_INT, machine_mode);\n bool aarch64_mov_operand_p (rtx, machine_mode);\n-int aarch64_simd_attr_length_rglist (machine_mode);\n rtx aarch64_reverse_mask (machine_mode);\n bool aarch64_offset_7bit_signed_scaled_p (machine_mode, HOST_WIDE_INT);\n-char *aarch64_output_scalar_simd_mov_immediate (rtx, machine_mode);\n+char *aarch64_output_scalar_simd_mov_immediate (rtx, scalar_int_mode);\n char *aarch64_output_simd_mov_immediate (rtx, machine_mode, unsigned);\n bool aarch64_pad_reg_upward (machine_mode, const_tree, bool);\n bool aarch64_regno_ok_for_base_p (int, bool);\n@@ -354,7 +353,7 @@ bool aarch64_reinterpret_float_as_int (rtx value, unsigned HOST_WIDE_INT *fail);\n bool aarch64_simd_check_vect_par_cnst_half (rtx op, machine_mode mode,\n \t\t\t\t\t    bool high);\n bool aarch64_simd_imm_zero_p (rtx, machine_mode);\n-bool aarch64_simd_scalar_immediate_valid_for_move (rtx, machine_mode);\n+bool aarch64_simd_scalar_immediate_valid_for_move (rtx, scalar_int_mode);\n bool aarch64_simd_shift_imm_p (rtx, machine_mode, bool);\n bool aarch64_simd_valid_immediate (rtx, machine_mode, bool,\n \t\t\t\t   struct simd_immediate_info *);"}, {"sha": "3d16a7ad5c3c0687df9ef24de08926532c1b5a3d", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 74, "deletions": 52, "changes": 126, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/77e994c9098df0da205b627d75d2282a4b983435/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/77e994c9098df0da205b627d75d2282a4b983435/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=77e994c9098df0da205b627d75d2282a4b983435", "patch": "@@ -1183,7 +1183,7 @@ aarch64_is_noplt_call_p (rtx sym)\n \n    (extract:MODE (mult (reg) (MULT_IMM)) (EXTRACT_IMM) (const_int 0)).  */\n bool\n-aarch64_is_extend_from_extract (machine_mode mode, rtx mult_imm,\n+aarch64_is_extend_from_extract (scalar_int_mode mode, rtx mult_imm,\n \t\t\t\trtx extract_imm)\n {\n   HOST_WIDE_INT mult_val, extract_val;\n@@ -1809,7 +1809,8 @@ aarch64_force_temporary (machine_mode mode, rtx x, rtx value)\n \n \n static rtx\n-aarch64_add_offset (machine_mode mode, rtx temp, rtx reg, HOST_WIDE_INT offset)\n+aarch64_add_offset (scalar_int_mode mode, rtx temp, rtx reg,\n+\t\t    HOST_WIDE_INT offset)\n {\n   if (!aarch64_plus_immediate (GEN_INT (offset), mode))\n     {\n@@ -1827,7 +1828,7 @@ aarch64_add_offset (machine_mode mode, rtx temp, rtx reg, HOST_WIDE_INT offset)\n \n static int\n aarch64_internal_mov_immediate (rtx dest, rtx imm, bool generate,\n-\t\t\t\tmachine_mode mode)\n+\t\t\t\tscalar_int_mode mode)\n {\n   int i;\n   unsigned HOST_WIDE_INT val, val2, mask;\n@@ -1958,9 +1959,11 @@ aarch64_expand_mov_immediate (rtx dest, rtx imm)\n   gcc_assert (mode == SImode || mode == DImode);\n \n   /* Check on what type of symbol it is.  */\n-  if (GET_CODE (imm) == SYMBOL_REF\n-      || GET_CODE (imm) == LABEL_REF\n-      || GET_CODE (imm) == CONST)\n+  scalar_int_mode int_mode;\n+  if ((GET_CODE (imm) == SYMBOL_REF\n+       || GET_CODE (imm) == LABEL_REF\n+       || GET_CODE (imm) == CONST)\n+      && is_a <scalar_int_mode> (mode, &int_mode))\n     {\n       rtx mem, base, offset;\n       enum aarch64_symbol_type sty;\n@@ -1974,11 +1977,12 @@ aarch64_expand_mov_immediate (rtx dest, rtx imm)\n \t{\n \tcase SYMBOL_FORCE_TO_MEM:\n \t  if (offset != const0_rtx\n-\t      && targetm.cannot_force_const_mem (mode, imm))\n+\t      && targetm.cannot_force_const_mem (int_mode, imm))\n \t    {\n \t      gcc_assert (can_create_pseudo_p ());\n-\t      base = aarch64_force_temporary (mode, dest, base);\n-\t      base = aarch64_add_offset (mode, NULL, base, INTVAL (offset));\n+\t      base = aarch64_force_temporary (int_mode, dest, base);\n+\t      base = aarch64_add_offset (int_mode, NULL, base,\n+\t\t\t\t\t INTVAL (offset));\n \t      aarch64_emit_move (dest, base);\n \t      return;\n \t    }\n@@ -2000,8 +2004,8 @@ aarch64_expand_mov_immediate (rtx dest, rtx imm)\n \t      mem = gen_rtx_MEM (ptr_mode, base);\n \t    }\n \n-\t  if (mode != ptr_mode)\n-\t    mem = gen_rtx_ZERO_EXTEND (mode, mem);\n+\t  if (int_mode != ptr_mode)\n+\t    mem = gen_rtx_ZERO_EXTEND (int_mode, mem);\n \n \t  emit_insn (gen_rtx_SET (dest, mem));\n \n@@ -2017,8 +2021,9 @@ aarch64_expand_mov_immediate (rtx dest, rtx imm)\n \t  if (offset != const0_rtx)\n \t    {\n \t      gcc_assert(can_create_pseudo_p ());\n-\t      base = aarch64_force_temporary (mode, dest, base);\n-\t      base = aarch64_add_offset (mode, NULL, base, INTVAL (offset));\n+\t      base = aarch64_force_temporary (int_mode, dest, base);\n+\t      base = aarch64_add_offset (int_mode, NULL, base,\n+\t\t\t\t\t INTVAL (offset));\n \t      aarch64_emit_move (dest, base);\n \t      return;\n \t    }\n@@ -2052,7 +2057,8 @@ aarch64_expand_mov_immediate (rtx dest, rtx imm)\n       return;\n     }\n \n-  aarch64_internal_mov_immediate (dest, imm, true, GET_MODE (dest));\n+  aarch64_internal_mov_immediate (dest, imm, true,\n+\t\t\t\t  as_a <scalar_int_mode> (mode));\n }\n \n /* Add DELTA to REGNUM in mode MODE.  SCRATCHREG can be used to hold a\n@@ -2068,9 +2074,9 @@ aarch64_expand_mov_immediate (rtx dest, rtx imm)\n    large immediate).  */\n \n static void\n-aarch64_add_constant_internal (machine_mode mode, int regnum, int scratchreg,\n-\t\t\t       HOST_WIDE_INT delta, bool frame_related_p,\n-\t\t\t       bool emit_move_imm)\n+aarch64_add_constant_internal (scalar_int_mode mode, int regnum,\n+\t\t\t       int scratchreg, HOST_WIDE_INT delta,\n+\t\t\t       bool frame_related_p, bool emit_move_imm)\n {\n   HOST_WIDE_INT mdelta = abs_hwi (delta);\n   rtx this_rtx = gen_rtx_REG (mode, regnum);\n@@ -2117,7 +2123,7 @@ aarch64_add_constant_internal (machine_mode mode, int regnum, int scratchreg,\n }\n \n static inline void\n-aarch64_add_constant (machine_mode mode, int regnum, int scratchreg,\n+aarch64_add_constant (scalar_int_mode mode, int regnum, int scratchreg,\n \t\t      HOST_WIDE_INT delta)\n {\n   aarch64_add_constant_internal (mode, regnum, scratchreg, delta, false, true);\n@@ -3985,7 +3991,7 @@ aarch64_uimm12_shift (HOST_WIDE_INT val)\n /* Return true if val is an immediate that can be loaded into a\n    register by a MOVZ instruction.  */\n static bool\n-aarch64_movw_imm (HOST_WIDE_INT val, machine_mode mode)\n+aarch64_movw_imm (HOST_WIDE_INT val, scalar_int_mode mode)\n {\n   if (GET_MODE_SIZE (mode) > 4)\n     {\n@@ -4089,25 +4095,33 @@ aarch64_and_split_imm2 (HOST_WIDE_INT val_in)\n bool\n aarch64_and_bitmask_imm (unsigned HOST_WIDE_INT val_in, machine_mode mode)\n {\n-  if (aarch64_bitmask_imm (val_in, mode))\n+  scalar_int_mode int_mode;\n+  if (!is_a <scalar_int_mode> (mode, &int_mode))\n+    return false;\n+\n+  if (aarch64_bitmask_imm (val_in, int_mode))\n     return false;\n \n-  if (aarch64_move_imm (val_in, mode))\n+  if (aarch64_move_imm (val_in, int_mode))\n     return false;\n \n   unsigned HOST_WIDE_INT imm2 = aarch64_and_split_imm2 (val_in);\n \n-  return aarch64_bitmask_imm (imm2, mode);\n+  return aarch64_bitmask_imm (imm2, int_mode);\n }\n \n /* Return true if val is an immediate that can be loaded into a\n    register in a single instruction.  */\n bool\n aarch64_move_imm (HOST_WIDE_INT val, machine_mode mode)\n {\n-  if (aarch64_movw_imm (val, mode) || aarch64_movw_imm (~val, mode))\n+  scalar_int_mode int_mode;\n+  if (!is_a <scalar_int_mode> (mode, &int_mode))\n+    return false;\n+\n+  if (aarch64_movw_imm (val, int_mode) || aarch64_movw_imm (~val, int_mode))\n     return 1;\n-  return aarch64_bitmask_imm (val, mode);\n+  return aarch64_bitmask_imm (val, int_mode);\n }\n \n static bool\n@@ -4771,9 +4785,9 @@ aarch64_float_const_rtx_p (rtx x)\n       && SCALAR_FLOAT_MODE_P (mode)\n       && aarch64_reinterpret_float_as_int (x, &ival))\n     {\n-      machine_mode imode = (mode == HFmode\n-\t\t\t    ? SImode\n-\t\t\t    : int_mode_for_mode (mode).require ());\n+      scalar_int_mode imode = (mode == HFmode\n+\t\t\t       ? SImode\n+\t\t\t       : int_mode_for_mode (mode).require ());\n       int num_instr = aarch64_internal_mov_immediate\n \t\t\t(NULL_RTX, gen_int_mode (ival, imode), false, imode);\n       return num_instr < 3;\n@@ -4802,7 +4816,8 @@ aarch64_can_const_movi_rtx_p (rtx x, machine_mode mode)\n   if (!TARGET_SIMD)\n      return false;\n \n-  machine_mode vmode, imode;\n+  machine_mode vmode;\n+  scalar_int_mode imode;\n   unsigned HOST_WIDE_INT ival;\n \n   if (GET_CODE (x) == CONST_DOUBLE\n@@ -4818,17 +4833,14 @@ aarch64_can_const_movi_rtx_p (rtx x, machine_mode mode)\n       imode = int_mode_for_mode (mode).require ();\n     }\n   else if (GET_CODE (x) == CONST_INT\n-\t   && SCALAR_INT_MODE_P (mode))\n-    {\n-       imode = mode;\n-       ival = INTVAL (x);\n-    }\n+\t   && is_a <scalar_int_mode> (mode, &imode))\n+    ival = INTVAL (x);\n   else\n     return false;\n \n    /* use a 64 bit mode for everything except for DI/DF mode, where we use\n      a 128 bit vector mode.  */\n-  int width = GET_MODE_BITSIZE (mode) == 64 ? 128 : 64;\n+  int width = GET_MODE_BITSIZE (imode) == 64 ? 128 : 64;\n \n   vmode = aarch64_simd_container_mode (imode, width);\n   rtx v_op = aarch64_simd_gen_const_vector_dup (vmode, ival);\n@@ -6130,7 +6142,8 @@ aarch64_output_casesi (rtx *operands)\n \n   gcc_assert (GET_CODE (diff_vec) == ADDR_DIFF_VEC);\n \n-  index = exact_log2 (GET_MODE_SIZE (GET_MODE (diff_vec)));\n+  scalar_int_mode mode = as_a <scalar_int_mode> (GET_MODE (diff_vec));\n+  index = exact_log2 (GET_MODE_SIZE (mode));\n \n   gcc_assert (index >= 0 && index <= 3);\n \n@@ -6250,13 +6263,17 @@ aarch64_strip_shift (rtx x)\n static rtx\n aarch64_strip_extend (rtx x, bool strip_shift)\n {\n+  scalar_int_mode mode;\n   rtx op = x;\n \n+  if (!is_a <scalar_int_mode> (GET_MODE (op), &mode))\n+    return op;\n+\n   /* Zero and sign extraction of a widened value.  */\n   if ((GET_CODE (op) == ZERO_EXTRACT || GET_CODE (op) == SIGN_EXTRACT)\n       && XEXP (op, 2) == const0_rtx\n       && GET_CODE (XEXP (op, 0)) == MULT\n-      && aarch64_is_extend_from_extract (GET_MODE (op), XEXP (XEXP (op, 0), 1),\n+      && aarch64_is_extend_from_extract (mode, XEXP (XEXP (op, 0), 1),\n \t\t\t\t\t XEXP (op, 1)))\n     return XEXP (XEXP (op, 0), 0);\n \n@@ -6593,7 +6610,7 @@ aarch64_branch_cost (bool speed_p, bool predictable_p)\n /* Return true if the RTX X in mode MODE is a zero or sign extract\n    usable in an ADD or SUB (extended register) instruction.  */\n static bool\n-aarch64_rtx_arith_op_extract_p (rtx x, machine_mode mode)\n+aarch64_rtx_arith_op_extract_p (rtx x, scalar_int_mode mode)\n {\n   /* Catch add with a sign extract.\n      This is add_<optab><mode>_multp2.  */\n@@ -6652,7 +6669,9 @@ static bool\n aarch64_extr_rtx_p (rtx x, rtx *res_op0, rtx *res_op1)\n {\n   rtx op0, op1;\n-  machine_mode mode = GET_MODE (x);\n+  scalar_int_mode mode;\n+  if (!is_a <scalar_int_mode> (GET_MODE (x), &mode))\n+    return false;\n \n   *res_op0 = NULL_RTX;\n   *res_op1 = NULL_RTX;\n@@ -6837,7 +6856,8 @@ aarch64_extend_bitfield_pattern_p (rtx x)\n    mode MODE.  See the *andim_ashift<mode>_bfiz pattern.  */\n \n bool\n-aarch64_mask_and_shift_for_ubfiz_p (machine_mode mode, rtx mask, rtx shft_amnt)\n+aarch64_mask_and_shift_for_ubfiz_p (scalar_int_mode mode, rtx mask,\n+\t\t\t\t    rtx shft_amnt)\n {\n   return CONST_INT_P (mask) && CONST_INT_P (shft_amnt)\n \t && INTVAL (shft_amnt) < GET_MODE_BITSIZE (mode)\n@@ -6929,8 +6949,8 @@ aarch64_rtx_costs (rtx x, machine_mode mode, int outer ATTRIBUTE_UNUSED,\n \t  if ((GET_CODE (op1) == ZERO_EXTEND\n \t       || GET_CODE (op1) == SIGN_EXTEND)\n \t      && CONST_INT_P (XEXP (op0, 1))\n-\t      && (GET_MODE_BITSIZE (GET_MODE (XEXP (op1, 0)))\n-\t\t  >= INTVAL (XEXP (op0, 1))))\n+\t      && is_a <scalar_int_mode> (GET_MODE (XEXP (op1, 0)), &int_mode)\n+\t      && GET_MODE_BITSIZE (int_mode) >= INTVAL (XEXP (op0, 1)))\n \t    op1 = XEXP (op1, 0);\n \n           if (CONST_INT_P (op1))\n@@ -6975,8 +6995,10 @@ aarch64_rtx_costs (rtx x, machine_mode mode, int outer ATTRIBUTE_UNUSED,\n \t     proportionally expensive to the number of instructions\n \t     required to build that constant.  This is true whether we\n \t     are compiling for SPEED or otherwise.  */\n+\t  if (!is_a <scalar_int_mode> (mode, &int_mode))\n+\t    int_mode = word_mode;\n \t  *cost = COSTS_N_INSNS (aarch64_internal_mov_immediate\n-\t\t\t\t (NULL_RTX, x, false, mode));\n+\t\t\t\t (NULL_RTX, x, false, int_mode));\n \t}\n       return true;\n \n@@ -6992,9 +7014,9 @@ aarch64_rtx_costs (rtx x, machine_mode mode, int outer ATTRIBUTE_UNUSED,\n \t  bool succeed = aarch64_reinterpret_float_as_int (x, &ival);\n \t  gcc_assert (succeed);\n \n-\t  machine_mode imode = (mode == HFmode\n-\t\t\t\t? SImode\n-\t\t\t\t: int_mode_for_mode (mode).require ());\n+\t  scalar_int_mode imode = (mode == HFmode\n+\t\t\t\t   ? SImode\n+\t\t\t\t   : int_mode_for_mode (mode).require ());\n \t  int ncost = aarch64_internal_mov_immediate\n \t\t(NULL_RTX, gen_int_mode (ival, imode), false, imode);\n \t  *cost += COSTS_N_INSNS (ncost);\n@@ -7249,7 +7271,8 @@ aarch64_rtx_costs (rtx x, machine_mode mode, int outer ATTRIBUTE_UNUSED,\n \t  }\n \n \t/* Look for SUB (extended register).  */\n-        if (aarch64_rtx_arith_op_extract_p (op1, mode))\n+\tif (is_a <scalar_int_mode> (mode, &int_mode)\n+\t    && aarch64_rtx_arith_op_extract_p (op1, int_mode))\n \t  {\n \t    if (speed)\n \t      *cost += extra_cost->alu.extend_arith;\n@@ -7328,7 +7351,8 @@ aarch64_rtx_costs (rtx x, machine_mode mode, int outer ATTRIBUTE_UNUSED,\n \t*cost += rtx_cost (op1, mode, PLUS, 1, speed);\n \n \t/* Look for ADD (extended register).  */\n-        if (aarch64_rtx_arith_op_extract_p (op0, mode))\n+\tif (is_a <scalar_int_mode> (mode, &int_mode)\n+\t    && aarch64_rtx_arith_op_extract_p (op0, int_mode))\n \t  {\n \t    if (speed)\n \t      *cost += extra_cost->alu.extend_arith;\n@@ -11696,12 +11720,11 @@ aarch64_simd_gen_const_vector_dup (machine_mode mode, HOST_WIDE_INT val)\n /* Check OP is a legal scalar immediate for the MOVI instruction.  */\n \n bool\n-aarch64_simd_scalar_immediate_valid_for_move (rtx op, machine_mode mode)\n+aarch64_simd_scalar_immediate_valid_for_move (rtx op, scalar_int_mode mode)\n {\n   machine_mode vmode;\n \n-  gcc_assert (!VECTOR_MODE_P (mode));\n-  vmode = aarch64_preferred_simd_mode (as_a <scalar_mode> (mode));\n+  vmode = aarch64_preferred_simd_mode (mode);\n   rtx op_v = aarch64_simd_gen_const_vector_dup (vmode, INTVAL (op));\n   return aarch64_simd_valid_immediate (op_v, vmode, false, NULL);\n }\n@@ -13051,7 +13074,7 @@ aarch64_output_simd_mov_immediate (rtx const_vector,\n }\n \n char*\n-aarch64_output_scalar_simd_mov_immediate (rtx immediate,  machine_mode mode)\n+aarch64_output_scalar_simd_mov_immediate (rtx immediate, scalar_int_mode mode)\n {\n \n   /* If a floating point number was passed and we desire to use it in an\n@@ -13069,7 +13092,6 @@ aarch64_output_scalar_simd_mov_immediate (rtx immediate,  machine_mode mode)\n      a 128 bit vector mode.  */\n   int width = GET_MODE_BITSIZE (mode) == 64 ? 128 : 64;\n \n-  gcc_assert (!VECTOR_MODE_P (mode));\n   vmode = aarch64_simd_container_mode (mode, width);\n   rtx v_op = aarch64_simd_gen_const_vector_dup (vmode, INTVAL (immediate));\n   return aarch64_output_simd_mov_immediate (v_op, vmode, width);"}]}