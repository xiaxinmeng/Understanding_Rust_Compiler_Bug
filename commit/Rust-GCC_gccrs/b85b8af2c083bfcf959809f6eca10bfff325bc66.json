{"sha": "b85b8af2c083bfcf959809f6eca10bfff325bc66", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6Yjg1YjhhZjJjMDgzYmZjZjk1OTgwOWY2ZWNhMTBiZmZmMzI1YmM2Ng==", "commit": {"author": {"name": "Jim Wilson", "email": "wilson@gcc.gnu.org", "date": "1992-03-19T18:40:36Z"}, "committer": {"name": "Jim Wilson", "email": "wilson@gcc.gnu.org", "date": "1992-03-19T18:40:36Z"}, "message": "*** empty log message ***\n\nFrom-SVN: r522", "tree": {"sha": "a07042f11edefade99e740d2c5850b3118e31f5b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/a07042f11edefade99e740d2c5850b3118e31f5b"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/b85b8af2c083bfcf959809f6eca10bfff325bc66", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b85b8af2c083bfcf959809f6eca10bfff325bc66", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b85b8af2c083bfcf959809f6eca10bfff325bc66", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b85b8af2c083bfcf959809f6eca10bfff325bc66/comments", "author": null, "committer": null, "parents": [{"sha": "07fa4fcba0f1820fc06dcc3cdafe5c1e172ccfca", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/07fa4fcba0f1820fc06dcc3cdafe5c1e172ccfca", "html_url": "https://github.com/Rust-GCC/gccrs/commit/07fa4fcba0f1820fc06dcc3cdafe5c1e172ccfca"}], "stats": {"total": 333, "additions": 147, "deletions": 186}, "files": [{"sha": "af73760f8b474e330aaeefb7cfd3be136f9d4c75", "filename": "gcc/objc/objc-act.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b85b8af2c083bfcf959809f6eca10bfff325bc66/gcc%2Fobjc%2Fobjc-act.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b85b8af2c083bfcf959809f6eca10bfff325bc66/gcc%2Fobjc%2Fobjc-act.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fobjc%2Fobjc-act.c?ref=b85b8af2c083bfcf959809f6eca10bfff325bc66", "patch": "@@ -1133,7 +1133,7 @@ build_selector_translation_table ()\n \n #ifndef OBJC_NONUNIQUE_SELECTORS\n       sprintf (buf, \"_OBJC_SELECTOR_REFERENCES_%d\", idx);\n-      sc_spec = build_tree_list (NULLT, ridpointers[RID_STATIC]);\n+      sc_spec = build_tree_list (NULLT, ridpointers[(int) RID_STATIC]);\n \n #ifdef OBJC_INT_SELECTORS\n       /* static unsigned int _OBJC_SELECTOR_REFERENCES_n = ...; */"}, {"sha": "a0a97a167871c222ce130d4c7e7593b8cf832ff5", "filename": "gcc/sched.c", "status": "modified", "additions": 146, "deletions": 185, "changes": 331, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b85b8af2c083bfcf959809f6eca10bfff325bc66/gcc%2Fsched.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b85b8af2c083bfcf959809f6eca10bfff325bc66/gcc%2Fsched.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched.c?ref=b85b8af2c083bfcf959809f6eca10bfff325bc66", "patch": "@@ -1180,8 +1180,6 @@ sched_analyze_1 (x, insn)\n   /* Analyze reads.  */\n   if (GET_CODE (x) == SET)\n     sched_analyze_2 (SET_SRC (x), insn);\n-  else if (GET_CODE (x) != CLOBBER)\n-    sched_analyze_2 (dest, insn);\n }\n \n /* Analyze the uses of memory and registers in rtx X in INSN.  */\n@@ -1201,161 +1199,174 @@ sched_analyze_2 (x, insn)\n \n   code = GET_CODE (x);\n \n-  /* Get rid of the easy cases first.  */\n-\n-  /* Ignore constants.  Note that we must handle CONST_DOUBLE here\n-     because it may have a cc0_rtx in its CONST_DOUBLE_CHAIN field, but\n-     this does not mean that this insn is using cc0.  */\n-  if (code == CONST_INT || code == CONST_DOUBLE || code == SYMBOL_REF\n-      || code == CONST || code == LABEL_REF)\n-    return;\n+  switch (code)\n+    {\n+    case CONST_INT:\n+    case CONST_DOUBLE:\n+    case SYMBOL_REF:\n+    case CONST:\n+    case LABEL_REF:\n+      /* Ignore constants.  Note that we must handle CONST_DOUBLE here\n+\t because it may have a cc0_rtx in its CONST_DOUBLE_CHAIN field, but\n+\t this does not mean that this insn is using cc0.  */\n+      return;\n \n #ifdef HAVE_cc0\n-  else if (code == CC0)\n-    {\n-      rtx link;\n+    case CC0:\n+      {\n+\trtx link;\n \n-      /* User of CC0 depends on immediately preceding insn.\n-\t All notes are removed from the list of insns to schedule before we\n-\t reach here, so the previous insn must be the setter of cc0.  */\n-      if (GET_CODE (PREV_INSN (insn)) != INSN)\n-\tabort ();\n-      SCHED_GROUP_P (insn) = 1;\n+\t/* User of CC0 depends on immediately preceding insn.\n+\t   All notes are removed from the list of insns to schedule before we\n+\t   reach here, so the previous insn must be the setter of cc0.  */\n+\tif (GET_CODE (PREV_INSN (insn)) != INSN)\n+\t  abort ();\n+\tSCHED_GROUP_P (insn) = 1;\n \n-      /* Make a copy of all dependencies on PREV_INSN, and add to this insn.\n-\t This is so that all the dependencies will apply to the group.  */\n+\t/* Make a copy of all dependencies on PREV_INSN, and add to this insn.\n+\t   This is so that all the dependencies will apply to the group.  */\n \n-      for (link = LOG_LINKS (PREV_INSN (insn)); link; link = XEXP (link, 1))\n-\tadd_dependence (insn, XEXP (link, 0), GET_MODE (link));\n+\tfor (link = LOG_LINKS (PREV_INSN (insn)); link; link = XEXP (link, 1))\n+\t  add_dependence (insn, XEXP (link, 0), GET_MODE (link));\n \n-      return;\n-    }\n+\treturn;\n+      }\n #endif\n \n-  else if (code == REG)\n-    {\n-      int regno = REGNO (x);\n-      if (regno < FIRST_PSEUDO_REGISTER)\n-\t{\n-\t  int i;\n+    case REG:\n+      {\n+\tint regno = REGNO (x);\n+\tif (regno < FIRST_PSEUDO_REGISTER)\n+\t  {\n+\t    int i;\n \n-\t  i = HARD_REGNO_NREGS (regno, GET_MODE (x));\n-\t  while (--i >= 0)\n-\t    {\n-\t      reg_last_uses[regno + i]\n-\t\t= gen_rtx (INSN_LIST, VOIDmode,\n-\t\t\t   insn, reg_last_uses[regno + i]);\n-\t      if (reg_last_sets[regno + i])\n-\t\tadd_dependence (insn, reg_last_sets[regno + i], 0);\n-\t      if ((call_used_regs[regno + i] || global_regs[regno + i])\n-\t\t  && last_function_call)\n-\t\t/* Function calls clobber all call_used regs.  */\n-\t\tadd_dependence (insn, last_function_call, REG_DEP_ANTI);\n-\t    }\n-\t}\n-      else\n-\t{\n-\t  reg_last_uses[regno]\n-\t    = gen_rtx (INSN_LIST, VOIDmode, insn, reg_last_uses[regno]);\n-\t  if (reg_last_sets[regno])\n-\t    add_dependence (insn, reg_last_sets[regno], 0);\n+\t    i = HARD_REGNO_NREGS (regno, GET_MODE (x));\n+\t    while (--i >= 0)\n+\t      {\n+\t\treg_last_uses[regno + i]\n+\t\t  = gen_rtx (INSN_LIST, VOIDmode,\n+\t\t\t     insn, reg_last_uses[regno + i]);\n+\t\tif (reg_last_sets[regno + i])\n+\t\t  add_dependence (insn, reg_last_sets[regno + i], 0);\n+\t\tif ((call_used_regs[regno + i] || global_regs[regno + i])\n+\t\t    && last_function_call)\n+\t\t  /* Function calls clobber all call_used regs.  */\n+\t\t  add_dependence (insn, last_function_call, REG_DEP_ANTI);\n+\t      }\n+\t  }\n+\telse\n+\t  {\n+\t    reg_last_uses[regno]\n+\t      = gen_rtx (INSN_LIST, VOIDmode, insn, reg_last_uses[regno]);\n+\t    if (reg_last_sets[regno])\n+\t      add_dependence (insn, reg_last_sets[regno], 0);\n+\n+\t    /* If the register does not already cross any calls, then add this\n+\t       insn to the sched_before_next_call list so that it will still\n+\t       not cross calls after scheduling.  */\n+\t    if (reg_n_calls_crossed[regno] == 0)\n+\t      add_dependence (sched_before_next_call, insn, REG_DEP_ANTI);\n+\t  }\n+\treturn;\n+      }\n \n-\t  /* If the register does not already cross any calls, then add this\n-\t     insn to the sched_before_next_call list so that it will still\n-\t     not cross calls after scheduling.  */\n-\t  if (reg_n_calls_crossed[regno] == 0)\n-\t    add_dependence (sched_before_next_call, insn, REG_DEP_ANTI);\n-\t}\n-      return;\n-    }\n+    case MEM:\n+      {\n+\t/* Reading memory.  */\n \n-  /* The interesting case.  */\n-  else if (code == MEM)\n-    {\n-      /* Reading memory.  */\n+\t/* Don't create a dependence for memory references which are known to\n+\t   be unchanging, such as constant pool accesses.  These will never\n+\t   conflict with any other memory access.  */\n+\tif (RTX_UNCHANGING_P (x) == 0)\n+\t  {\n+\t    rtx pending, pending_mem;\n \n-      /* Don't create a dependence for memory references which are known to\n-\t be unchanging, such as constant pool accesses.  These will never\n-\t conflict with any other memory access.  */\n-      if (RTX_UNCHANGING_P (x) == 0)\n-\t{\n-\t  rtx pending, pending_mem;\n+\t    pending = pending_read_insns;\n+\t    pending_mem = pending_read_mems;\n+\t    while (pending)\n+\t      {\n+\t\t/* If a dependency already exists, don't create a new one.  */\n+\t\tif (! find_insn_list (XEXP (pending, 0), LOG_LINKS (insn)))\n+\t\t  if (read_dependence (XEXP (pending_mem, 0), x))\n+\t\t    add_dependence (insn, XEXP (pending, 0), REG_DEP_ANTI);\n \n-\t  pending = pending_read_insns;\n-\t  pending_mem = pending_read_mems;\n-\t  while (pending)\n-\t    {\n-\t      /* If a dependency already exists, don't create a new one.  */\n-\t      if (! find_insn_list (XEXP (pending, 0), LOG_LINKS (insn)))\n-\t\tif (read_dependence (XEXP (pending_mem, 0), x))\n-\t\t  add_dependence (insn, XEXP (pending, 0), REG_DEP_ANTI);\n+\t\tpending = XEXP (pending, 1);\n+\t\tpending_mem = XEXP (pending_mem, 1);\n+\t      }\n \n-\t      pending = XEXP (pending, 1);\n-\t      pending_mem = XEXP (pending_mem, 1);\n-\t    }\n+\t    pending = pending_write_insns;\n+\t    pending_mem = pending_write_mems;\n+\t    while (pending)\n+\t      {\n+\t\t/* If a dependency already exists, don't create a new one.  */\n+\t\tif (! find_insn_list (XEXP (pending, 0), LOG_LINKS (insn)))\n+\t\t  if (true_dependence (XEXP (pending_mem, 0), x))\n+\t\t    add_dependence (insn, XEXP (pending, 0), 0);\n \n-\t  pending = pending_write_insns;\n-\t  pending_mem = pending_write_mems;\n-\t  while (pending)\n-\t    {\n-\t      /* If a dependency already exists, don't create a new one.  */\n-\t      if (! find_insn_list (XEXP (pending, 0), LOG_LINKS (insn)))\n-\t\tif (true_dependence (XEXP (pending_mem, 0), x))\n-\t\t  add_dependence (insn, XEXP (pending, 0), 0);\n+\t\tpending = XEXP (pending, 1);\n+\t\tpending_mem = XEXP (pending_mem, 1);\n+\t      }\n+\t    if (last_pending_memory_flush)\n+\t      add_dependence (insn, last_pending_memory_flush, REG_DEP_ANTI);\n \n-\t      pending = XEXP (pending, 1);\n-\t      pending_mem = XEXP (pending_mem, 1);\n-\t    }\n-\t  if (last_pending_memory_flush)\n-\t    add_dependence (insn, last_pending_memory_flush, REG_DEP_ANTI);\n+\t    /* Always add these dependencies to pending_reads, since\n+\t       this insn may be followed by a write.  */\n+\t    add_insn_mem_dependence (&pending_read_insns, &pending_read_mems,\n+\t\t\t\t     insn, x);\n+\t  }\n+\t/* Take advantage of tail recursion here.  */\n+\tsched_analyze_2 (XEXP (x, 0), insn);\n+\treturn;\n+      }\n \n-\t  /* Always add these dependencies to pending_reads, since\n-\t     this insn may be followed by a write.  */\n-\t  add_insn_mem_dependence (&pending_read_insns, &pending_read_mems,\n-\t\t\t\t   insn, x);\n-\t}\n-      /* Take advantage of tail recursion here.  */\n-      sched_analyze_2 (XEXP (x, 0), insn);\n-      return;\n-    }\n+    case ASM_OPERANDS:\n+    case ASM_INPUT:\n+    case UNSPEC_VOLATILE:\n+      {\n+\trtx u;\n \n-  else if (code == ASM_OPERANDS || code == ASM_INPUT\n-\t   || code == UNSPEC_VOLATILE)\n-    {\n-      rtx u;\n+\t/* Traditional and volatile asm instructions must be considered to use\n+\t   and clobber all hard registers and all of memory.  So must\n+\t   UNSPEC_VOLATILE operations.  */\n+\tif ((code == ASM_OPERANDS && MEM_VOLATILE_P (x)) || code == ASM_INPUT\n+\t    || code == UNSPEC_VOLATILE)\n+\t  {\n+\t    for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+\t      {\n+\t\tfor (u = reg_last_uses[i]; u; u = XEXP (u, 1))\n+\t\t  if (GET_CODE (PATTERN (XEXP (u, 0))) != USE)\n+\t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\t\treg_last_uses[i] = 0;\n+\t\tif (reg_last_sets[i]\n+\t\t    && GET_CODE (PATTERN (reg_last_sets[i])) != USE)\n+\t\t  add_dependence (insn, reg_last_sets[i], 0);\n+\t\treg_last_sets[i] = insn;\n+\t      }\n \n-      /* Traditional and volatile asm instructions must be considered to use\n-\t and clobber all hard registers and all of memory.  So must\n-\t UNSPEC_VOLATILE operations.  */\n-      if ((code == ASM_OPERANDS && MEM_VOLATILE_P (x)) || code == ASM_INPUT\n-\t  || code == UNSPEC_VOLATILE)\n-\t{\n-\t  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n-\t    {\n-\t      for (u = reg_last_uses[i]; u; u = XEXP (u, 1))\n-\t\tif (GET_CODE (PATTERN (XEXP (u, 0))) != USE)\n-\t\t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t      reg_last_uses[i] = 0;\n-\t      if (reg_last_sets[i]\n-\t\t  && GET_CODE (PATTERN (reg_last_sets[i])) != USE)\n-\t\tadd_dependence (insn, reg_last_sets[i], 0);\n-\t      reg_last_sets[i] = insn;\n-\t    }\n+\t    flush_pending_lists (insn);\n+\t  }\n \n-\t  flush_pending_lists (insn);\n-\t}\n+\t/* For all ASM_OPERANDS, we must traverse the vector of input operands.\n+\t   We can not just fall through here since then we would be confused\n+\t   by the ASM_INPUT rtx inside ASM_OPERANDS, which do not indicate\n+\t   traditional asms unlike their normal usage.  */\n \n-      /* For all ASM_OPERANDS, we must traverse the vector of input operands.\n-\t We can not just fall through here since then we would be confused\n-\t by the ASM_INPUT rtx inside ASM_OPERANDS, which do not indicate\n-\t traditional asms unlike their normal usage.  */\n+\tif (code == ASM_OPERANDS)\n+\t  {\n+\t    for (j = 0; j < ASM_OPERANDS_INPUT_LENGTH (x); j++)\n+\t      sched_analyze_2 (ASM_OPERANDS_INPUT (x, j), insn);\n+\t    return;\n+\t  }\n+\tbreak;\n+      }\n \n-      if (code == ASM_OPERANDS)\n-\t{\n-\t  for (j = 0; j < ASM_OPERANDS_INPUT_LENGTH (x); j++)\n-\t    sched_analyze_2 (ASM_OPERANDS_INPUT (x, j), insn);\n-\t  return;\n-\t}\n+    case PRE_DEC:\n+    case POST_DEC:\n+    case PRE_INC:\n+    case POST_INC:\n+      /* These read and modify the result; just consider them writes.  */\n+      sched_analyze_1 (x, insn);\n+      return;\n     }\n \n   /* Other cases: walk the insn.  */\n@@ -1396,56 +1407,6 @@ sched_analyze_insn (x, insn)\n   else\n     sched_analyze_2 (x, insn);\n \n-  for (link = REG_NOTES (insn); link; link = XEXP (link, 1))\n-    {\n-      /* Any REG_INC note is a SET of the register indicated.  */\n-      if (REG_NOTE_KIND (link) == REG_INC)\n-\t{\n-\t  rtx dest = XEXP (link, 0);\n-\t  int regno = REGNO (dest);\n-\t  int i;\n-\n-\t  /* A hard reg in a wide mode may really be multiple registers.\n-\t     If so, mark all of them just like the first.  */\n-\t  if (regno < FIRST_PSEUDO_REGISTER)\n-\t    {\n-\t      i = HARD_REGNO_NREGS (regno, GET_MODE (dest));\n-\t      while (--i >= 0)\n-\t\t{\n-\t\t  rtx u;\n-\t\t  \n-\t\t  for (u = reg_last_uses[regno+i]; u; u = XEXP (u, 1))\n-\t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t\t  reg_last_uses[regno + i] = 0;\n-\t\t  if (reg_last_sets[regno + i])\n-\t\t    add_dependence (insn, reg_last_sets[regno + i],\n-\t\t\t\t    REG_DEP_OUTPUT);\n-\t\t  reg_last_sets[regno + i] = insn;\n-\t\t  if ((call_used_regs[i] || global_regs[i])\n-\t\t      && last_function_call)\n-\t\t    /* Function calls clobber all call_used regs.  */\n-\t\t    add_dependence (insn, last_function_call, REG_DEP_ANTI);\n-\t\t}\n-\t    }\n-\t  else\n-\t    {\n-\t      rtx u;\n-\t      \n-\t      for (u = reg_last_uses[regno]; u; u = XEXP (u, 1))\n-\t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t      reg_last_uses[regno] = 0;\n-\t      if (reg_last_sets[regno])\n-\t\tadd_dependence (insn, reg_last_sets[regno], REG_DEP_OUTPUT);\n-\t      reg_last_sets[regno] = insn;\n-\n-\t      /* Don't let it cross a call after scheduling if it doesn't\n-\t\t already cross one.  */\n-\t      if (reg_n_calls_crossed[regno] == 0 && last_function_call)\n-\t\tadd_dependence (insn, last_function_call, 0);\n-\t    }\n-\t}\n-    }\n-\n   /* Handle function calls.  */\n   if (GET_CODE (insn) == CALL_INSN)\n     {"}]}