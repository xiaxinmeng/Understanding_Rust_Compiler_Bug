{"sha": "1245df60c108112fedca5d76a69f19895a0f4ae4", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MTI0NWRmNjBjMTA4MTEyZmVkY2E1ZDc2YTY5ZjE5ODk1YTBmNGFlNA==", "commit": {"author": {"name": "J\"orn Rennecke", "email": "amylaar@cygnus.co.uk", "date": "1997-11-08T19:54:57Z"}, "committer": {"name": "Joern Rennecke", "email": "amylaar@gcc.gnu.org", "date": "1997-11-08T19:54:57Z"}, "message": "Sync SH port with FSF; enable regmove for SH.\n\nFrom-SVN: r16371", "tree": {"sha": "59f0428e343ff462a9ba43cace6b8429cdaa9063", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/59f0428e343ff462a9ba43cace6b8429cdaa9063"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/1245df60c108112fedca5d76a69f19895a0f4ae4", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1245df60c108112fedca5d76a69f19895a0f4ae4", "html_url": "https://github.com/Rust-GCC/gccrs/commit/1245df60c108112fedca5d76a69f19895a0f4ae4", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1245df60c108112fedca5d76a69f19895a0f4ae4/comments", "author": null, "committer": null, "parents": [{"sha": "c64e3181fe90fed986f489512dfac2ed1a3eeaeb", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c64e3181fe90fed986f489512dfac2ed1a3eeaeb", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c64e3181fe90fed986f489512dfac2ed1a3eeaeb"}], "stats": {"total": 3105, "additions": 2440, "deletions": 665}, "files": [{"sha": "5db233fcf596ca2bcabb68c9ce13d76252c8cbfd", "filename": "gcc/ChangeLog", "status": "modified", "additions": 33, "deletions": 0, "changes": 33, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1245df60c108112fedca5d76a69f19895a0f4ae4/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1245df60c108112fedca5d76a69f19895a0f4ae4/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=1245df60c108112fedca5d76a69f19895a0f4ae4", "patch": "@@ -1,3 +1,36 @@\n+Sat Nov  8 18:20:21 1997  J\"orn Rennecke <amylaar@cygnus.co.uk>\n+\n+\t* sh.h (ENABLE_REGMOVE_PASS): Define.\n+\n+\tBring over from FSF:\n+\n+\tThu Oct 30 12:21:06 1997  J\"orn Rennecke <amylaar@cygnus.co.uk>\n+\n+\t* va-sh.h (__va_arg_sh1): Define.\n+\t(va_arg): Use it.\n+\tSH3E doesn't use any integer registers for subsequent arguments\n+\tonce a non-float value was passed in the stack.\n+\t* sh.c (machine_dependent_reorg): If optimizing, put explicit\n+\talignment in front label for ADDR_DIFF_VEC.\n+\t* sh.h (PASS_IN_REG_P): Fix SH3E case.\n+\t(ADJUST_INSN_LENGTH): If not optimizing, add two extra bytes length.\n+\n+\tTue Oct 28 15:06:44 1997  J\"orn Rennecke <amylaar@cygnus.co.uk>\n+\n+\t* sh/elf.h (PREFERRED_DEBUGGING_TYPE): Undefine before including\n+\tsvr4.h.\n+\n+\tMon Oct 27 16:11:52 1997  J\"orn Rennecke <amylaar@cygnus.co.uk>\n+\n+\t* sh.c (machine_dependent_reorg): When -flag_delayed_branches,\n+\tput an use_sfunc_addr before each sfunc.\n+\t* sh.md (use_sfunc_addr, dummy_jump): New insns.\n+\t(casesi): For TARGET_SH2, emit a dummy_jump after LAB.\n+\n+\tTue Oct 21 07:12:28 1997  J\"orn Rennecke <amylaar@cygnus.co.uk>\n+\n+\t* sh/elf.h (PREFERRED_DEBUGGING_TYPE): Don't redefine.\n+\n Fri Nov  7 10:22:24 1997  Jason Merrill  <jason@yorick.cygnus.com>\n \n \t* frame.c (add_fdes, count_fdes): Go back to checking pc_begin for"}, {"sha": "a56077e544ea7eb8b8b71412bda5f5c65ab58fa0", "filename": "gcc/config/sh/elf.h", "status": "modified", "additions": 33, "deletions": 16, "changes": 49, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1245df60c108112fedca5d76a69f19895a0f4ae4/gcc%2Fconfig%2Fsh%2Felf.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1245df60c108112fedca5d76a69f19895a0f4ae4/gcc%2Fconfig%2Fsh%2Felf.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fsh%2Felf.h?ref=1245df60c108112fedca5d76a69f19895a0f4ae4", "patch": "@@ -25,10 +25,6 @@ Boston, MA 02111-1307, USA.  */\n /* No SDB debugging info.  */\n #undef SDB_DEBUGGING_INFO\n \n-/* Prefer stabs.  */\n-#undef PREFERRED_DEBUGGING_TYPE\n-#define PREFERRED_DEBUGGING_TYPE DBX_DEBUG\n-\n /* Undefine some macros defined in both sh.h and svr4.h.  */\n #undef IDENT_ASM_OP\n #undef ASM_FILE_END\n@@ -40,10 +36,30 @@ Boston, MA 02111-1307, USA.  */\n #undef ASM_OUTPUT_CONSTRUCTOR\n #undef ASM_OUTPUT_DESTRUCTOR\n #undef ASM_DECLARE_FUNCTION_NAME\n+#undef PREFERRED_DEBUGGING_TYPE\n \n /* Be ELF-like.  */\n #include \"svr4.h\"\n \n+/* The prefix to add to user-visible assembler symbols.\n+   Note that svr4.h redefined it from the original value (that we want)\n+   in sh.h */\n+\n+#undef USER_LABEL_PREFIX\n+#define USER_LABEL_PREFIX \"_\"\n+\n+#undef LOCAL_LABEL_PREFIX\n+#define LOCAL_LABEL_PREFIX \".\"\n+\n+#undef ASM_FILE_START\n+#define ASM_FILE_START(FILE) do {\t\t\t\t\\\n+  output_file_directive ((FILE), main_input_filename);\t\t\\\n+  if (TARGET_LITTLE_ENDIAN)\t\t\t\t\t\\\n+    fprintf ((FILE), \"\\t.little\\n\");\t\t\t\t\\\n+} while (0)\n+\n+\n+\n /* Let code know that this is ELF.  */\n #define CPP_PREDEFINES \"-D__sh__ -D__ELF__ -Acpu(sh) -Amachine(sh)\"\n \n@@ -52,7 +68,7 @@ Boston, MA 02111-1307, USA.  */\n #define ASM_SPEC  \"%{ml:-little} %{mrelax:-relax}\"\n \n #undef LINK_SPEC\n-#define LINK_SPEC \"%{ml:-m shl} %{mrelax:-relax}\"\n+#define LINK_SPEC \"%{ml:-m shlelf} %{mrelax:-relax}\"\n \n /* svr4.h undefined DBX_REGISTER_NUMBER, so we need to define it\n    again.  */\n@@ -63,29 +79,26 @@ Boston, MA 02111-1307, USA.  */\n    symbol names.  */\n #undef ASM_OUTPUT_LABELREF\n #define ASM_OUTPUT_LABELREF(STREAM,NAME) \\\n-  fprintf (STREAM, \"_%s\", NAME)\n-\n-/* Because SH ELF uses underscores, we don't put a '.' before local\n-   labels, for easy compatibility with the COFF implementation.  */\n+  asm_fprintf (STREAM, \"%U%s\", NAME)\n \n #undef ASM_GENERATE_INTERNAL_LABEL\n #define ASM_GENERATE_INTERNAL_LABEL(STRING, PREFIX, NUM) \\\n-  sprintf (STRING, \"*%s%d\", PREFIX, NUM)\n+  sprintf ((STRING), \"*%s%s%d\", LOCAL_LABEL_PREFIX, (PREFIX), (NUM))\n \n #undef ASM_OUTPUT_INTERNAL_LABEL\n #define ASM_OUTPUT_INTERNAL_LABEL(FILE,PREFIX,NUM) \\\n-  fprintf (FILE, \"%s%d:\\n\", PREFIX, NUM)\n+  asm_fprintf ((FILE), \"%L%s%d:\\n\", (PREFIX), (NUM))\n \n #undef  ASM_OUTPUT_SOURCE_LINE\n #define ASM_OUTPUT_SOURCE_LINE(file, line)\t\t\t\t\\\n do\t\t\t\t\t\t\t\t\t\\\n   {\t\t\t\t\t\t\t\t\t\\\n     static int sym_lineno = 1;\t\t\t\t\t\t\\\n-    fprintf (file, \".stabn 68,0,%d,LM%d-\",\t\t\t\t\\\n-\t     line, sym_lineno);\t\t\t\t\t\t\\\n-    assemble_name (file,\t\t\t\t\t\t\\\n+    asm_fprintf ((file), \".stabn 68,0,%d,%LLM%d-\",\t\t\t\\\n+\t     (line), sym_lineno);\t\t\t\t\t\\\n+    assemble_name ((file),\t\t\t\t\t\t\\\n \t\t   XSTR (XEXP (DECL_RTL (current_function_decl), 0), 0));\\\n-    fprintf (file, \"\\nLM%d:\\n\", sym_lineno);\t\t\t\t\\\n+    asm_fprintf ((file), \"\\n%LLM%d:\\n\", sym_lineno);\t\t\t\\\n     sym_lineno += 1;\t\t\t\t\t\t\t\\\n   }\t\t\t\t\t\t\t\t\t\\\n while (0)\n@@ -94,7 +107,7 @@ while (0)\n #define DBX_OUTPUT_MAIN_SOURCE_FILE_END(FILE, FILENAME)\t\t\t\\\n do {\t\t\t\t\t\t\t\t\t\\\n   text_section ();\t\t\t\t\t\t\t\\\n-  fprintf (FILE, \"\\t.stabs \\\"\\\",%d,0,0,Letext\\nLetext:\\n\", N_SO);\t\\\n+  fprintf ((FILE), \"\\t.stabs \\\"\\\",%d,0,0,Letext\\nLetext:\\n\", N_SO);\t\\\n } while (0)\n \n /* Arrange to call __main, rather than using crtbegin.o and crtend.o\n@@ -103,3 +116,7 @@ do {\t\t\t\t\t\t\t\t\t\\\n #undef FINI_SECTION_ASM_OP\n #undef STARTFILE_SPEC\n #undef ENDFILE_SPEC\n+\n+/* HANDLE_SYSV_PRAGMA (defined by svr4.h) takes precedence over HANDLE_PRAGMA.\n+   We want to use the HANDLE_PRAGMA from sh.h.  */\n+#undef HANDLE_SYSV_PRAGMA"}, {"sha": "f26b60042c8b5a65bcac8da39cacc95b2d4efbe8", "filename": "gcc/config/sh/sh.c", "status": "modified", "additions": 1501, "deletions": 254, "changes": 1755, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1245df60c108112fedca5d76a69f19895a0f4ae4/gcc%2Fconfig%2Fsh%2Fsh.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1245df60c108112fedca5d76a69f19895a0f4ae4/gcc%2Fconfig%2Fsh%2Fsh.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fsh%2Fsh.c?ref=1245df60c108112fedca5d76a69f19895a0f4ae4", "patch": "@@ -35,6 +35,8 @@ Boston, MA 02111-1307, USA.  */\n #include \"output.h\"\n #include \"insn-attr.h\"\n \n+int code_for_indirect_jump_scratch = CODE_FOR_indirect_jump_scratch;\n+\n #define MSW (TARGET_LITTLE_ENDIAN ? 1 : 0)\n #define LSW (TARGET_LITTLE_ENDIAN ? 0 : 1)\n \n@@ -84,6 +86,10 @@ enum processor_type sh_cpu;\n rtx sh_compare_op0;\n rtx sh_compare_op1;\n \n+enum machine_mode sh_addr_diff_vec_mode;\n+rtx *uid_align;\n+int uid_align_max;\n+\n /* Provides the class number of the smallest class containing\n    reg number.  */\n \n@@ -114,6 +120,8 @@ enum reg_class reg_class_from_letter[] =\n   /* u */ NO_REGS, /* v */ NO_REGS, /* w */ FP0_REGS, /* x */ MAC_REGS,\n   /* y */ FPUL_REGS, /* z */ R0_REGS\n };\n+\n+static void split_branches PROTO ((rtx));\n \f\n /* Print the operand address in x to the stream.  */\n \n@@ -170,6 +178,7 @@ print_operand_address (stream, x)\n    according to modifier code.\n \n    '.'  print a .s if insn needs delay slot\n+   ','  print LOCAL_LABEL_PREFIX\n    '@'  print trap, rte or rts depending upon pragma interruptness\n    '#'  output a nop if there is nothing to put in the delay slot\n    'O'  print a constant without the #\n@@ -188,7 +197,10 @@ print_operand (stream, x, code)\n     case '.':\n       if (final_sequence\n \t  && ! INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0)))\n-\tfprintf (stream, \".s\");\n+\tfprintf (stream, ASSEMBLER_DIALECT ? \"/s\" : \".s\");\n+      break;\n+    case ',':\n+      fprintf (stream, \"%s\", LOCAL_LABEL_PREFIX);\n       break;\n     case '@':\n       if (trap_exit)\n@@ -389,13 +401,12 @@ prepare_scc_operands (code)\n     mode = GET_MODE (sh_compare_op1);\n \n   sh_compare_op0 = force_reg (mode, sh_compare_op0);\n-  if (code != EQ && code != NE\n-      && (sh_compare_op1 != const0_rtx\n-\t  || code == GTU  || code == GEU || code == LTU || code == LEU))\n+  if ((code != EQ && code != NE\n+       && (sh_compare_op1 != const0_rtx\n+\t   || code == GTU  || code == GEU || code == LTU || code == LEU))\n+      || TARGET_SH3E && GET_MODE_CLASS (mode) == MODE_FLOAT)\n     sh_compare_op1 = force_reg (mode, sh_compare_op1);\n \n-  /* ??? This should be `mode' not `SImode' in the compare, but that would\n-     require fixing the branch patterns too.  */\n   emit_insn (gen_rtx (SET, VOIDmode, t_reg,\n \t\t      gen_rtx (code, SImode, sh_compare_op0,\n \t\t\t       sh_compare_op1)));\n@@ -410,20 +421,31 @@ from_compare (operands, code)\n      rtx *operands;\n      int code;\n {\n-  if (code != EQ && code != NE)\n+  enum machine_mode mode = GET_MODE (sh_compare_op0);\n+  rtx insn;\n+  if (mode == VOIDmode)\n+    mode = GET_MODE (sh_compare_op1);\n+  if (code != EQ\n+      || mode == DImode\n+      || (TARGET_SH3E && GET_MODE_CLASS (mode) == MODE_FLOAT))\n     {\n-      enum machine_mode mode = GET_MODE (sh_compare_op0);\n-      if (mode == VOIDmode)\n-\tmode = GET_MODE (sh_compare_op1);\n-\n       /* Force args into regs, since we can't use constants here.  */\n       sh_compare_op0 = force_reg (mode, sh_compare_op0);\n       if (sh_compare_op1 != const0_rtx\n-\t  || code == GTU  || code == GEU || code == LTU || code == LEU)\n+\t  || code == GTU  || code == GEU\n+\t  || (TARGET_SH3E && GET_MODE_CLASS (mode) == MODE_FLOAT))\n \tsh_compare_op1 = force_reg (mode, sh_compare_op1);\n     }\n-  operands[1] = sh_compare_op0;\n-  operands[2] = sh_compare_op1;\n+  if (TARGET_SH3E && GET_MODE_CLASS (mode) == MODE_FLOAT && code == GE)\n+    {\n+      from_compare (operands, GT);\n+      insn = gen_ieee_ccmpeqsf_t (sh_compare_op0, sh_compare_op1);\n+    }\n+  else\n+    insn = gen_rtx (SET, VOIDmode,\n+\t\t    gen_rtx (REG, SImode, 18),\n+\t\t    gen_rtx (code, SImode, sh_compare_op0, sh_compare_op1));\n+  emit_insn (insn);\n }\n \f\n /* Functions to output assembly code.  */\n@@ -520,33 +542,54 @@ print_slot (insn)\n   INSN_DELETED_P (XVECEXP (insn, 0, 1)) = 1;\n }\n \n-/* We can't tell if we need a register as a scratch for the jump\n-   until after branch shortening, and then it's too late to allocate a\n-   register the 'proper' way.  These instruction sequences are rare\n-   anyway, so to avoid always using a reg up from our limited set, we'll\n-   grab one when we need one on output.  */\n-\n-/* ??? Should fix compiler so that using a clobber scratch in jump\n-   instructions works, and then this will be unnecessary.  */\n-\n char *\n output_far_jump (insn, op)\n      rtx insn;\n      rtx op;\n {\n-  rtx thislab = gen_label_rtx ();\n+  struct { rtx lab, reg, op; } this;\n+  char *jump;\n+  int far;\n \n-  /* Output the delay slot insn first if any.  */\n-  if (dbr_sequence_length ())\n-    print_slot (final_sequence);\n+  this.lab = gen_label_rtx ();\n \n-  output_asm_insn (\"mov.l\tr13,@-r15\", 0);\n-  output_asm_insn (\"mov.l\t%O0,r13\", &thislab);\n-  output_asm_insn (\"jmp\t@r13\", 0);\n-  output_asm_insn (\"mov.l\t@r15+,r13\", 0);\n-  output_asm_insn (\".align\t2\", 0);\n-  ASM_OUTPUT_INTERNAL_LABEL (asm_out_file, \"L\", CODE_LABEL_NUMBER (thislab));\n-  output_asm_insn (\".long\t%O0\", &op);\n+  if (braf_branch_p (insn, 0))\n+    {\n+      far = 0;\n+      jump = \"mov.w\t%O0,%1;braf\t%1\";\n+    }\n+  else\n+    {\n+      far = 1;\n+      jump = \"mov.l\t%O0,%1;jmp\t@%1\";\n+    }\n+  /* If we have a scratch register available, use it.  */\n+  if (GET_CODE (PREV_INSN (insn)) == INSN\n+      && INSN_CODE (PREV_INSN (insn)) == CODE_FOR_indirect_jump_scratch)\n+    {\n+      this.reg = SET_DEST (PATTERN (PREV_INSN (insn)));\n+      output_asm_insn (jump, &this.lab);\n+      if (dbr_sequence_length ())\n+\tprint_slot (final_sequence);\n+      else\n+\toutput_asm_insn (\"nop\", 0);\n+    }\n+  else\n+    {\n+      /* Output the delay slot insn first if any.  */\n+      if (dbr_sequence_length ())\n+\tprint_slot (final_sequence);\n+\n+      this.reg = gen_rtx (REG, SImode, 13);\n+      output_asm_insn (\"mov.l\tr13,@-r15\", 0);\n+      output_asm_insn (jump, &this.lab);\n+      output_asm_insn (\"mov.l\t@r15+,r13\", 0);\n+    }\n+  if (far)\n+    output_asm_insn (\".align\t2\", 0);\n+  ASM_OUTPUT_INTERNAL_LABEL (asm_out_file, \"L\", CODE_LABEL_NUMBER (this.lab));\n+  this.op = op;\n+  output_asm_insn (far ? \".long\t%O2\" : \".word %O2-%O0\", &this.lab);\n   return \"\";\n }\n \n@@ -563,97 +606,100 @@ output_branch (logic, insn, operands)\n      rtx insn;\n      rtx *operands;\n {\n-  int label = lf++;\n-  int length = get_attr_length (insn);\n-  int adjusted_length;\n+  int offset\n+    = (insn_addresses[INSN_UID (XEXP (XEXP (SET_SRC (PATTERN (insn)), 1), 0))]\n+       - insn_addresses[INSN_UID (insn)]);\n \n-  /* Undo the effects of ADJUST_INSN_LENGTH, so that we get the real\n-     length.  If NEXT_INSN (PREV_INSN (insn)) != insn, then the insn\n-     is inside a sequence, and ADJUST_INSN_LENGTH was not called on\n-     it.  */\n-  if (PREV_INSN (insn) == NULL\n-      || NEXT_INSN (PREV_INSN (insn)) == insn)\n+  if (offset == 260\n+      && final_sequence\n+      && ! INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0)))\n     {\n-      adjusted_length = length;\n-      ADJUST_INSN_LENGTH (insn, adjusted_length);\n-      length -= (adjusted_length - length);\n+      /* The filling of the delay slot has caused a forward branch to exceed\n+\t its range.\n+         Just emit the insn from the delay slot in front of the branch.  */\n+      /* The call to print_slot will clobber the operands.  */\n+      rtx op0 = operands[0];\n+      print_slot (final_sequence);\n+      operands[0] = op0;\n     }\n-\n-  switch (length)\n+  else if (offset < -252 || offset > 258)\n     {\n-    case 2:\n-      /* A branch with an unfilled delay slot.  */\n-    case 4:\n-      /* Simple branch in range -252..+258 bytes */\n-      return logic ? \"bt%.\t%l0\" : \"bf%.\t%l0\";\n+      /* This can happen when other condbranches hoist delay slot insn\n+\t from their destination, thus leading to code size increase.\n+\t But the branch will still be in the range -4092..+4098 bytes.  */\n \n-    case 6:\n-      /* A branch with an unfilled delay slot.  */\n-    case 8:\n-      /* Branch in range -4092..+4098 bytes.  */\n-      {\n-\t/* The call to print_slot will clobber the operands.  */\n-\trtx op0 = operands[0];\n+      int label = lf++;\n+      /* The call to print_slot will clobber the operands.  */\n+      rtx op0 = operands[0];\n \n-\t/* If the instruction in the delay slot is annulled (true), then\n-\t   there is no delay slot where we can put it now.  The only safe\n-\t   place for it is after the label.  */\n+      /* If the instruction in the delay slot is annulled (true), then\n+\t there is no delay slot where we can put it now.  The only safe\n+\t place for it is after the label.  final will do that by default.  */\n \n-\tif (final_sequence)\n-\t  {\n-\t    fprintf (asm_out_file, \"\\tb%c%s\\tLF%d\\n\", logic ? 'f' : 't',\n-\t\t     INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0))\n-\t\t     ? \"\" : \".s\", label);\n-\t    if (! INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0)))\n-\t      print_slot (final_sequence);\n-\t  }\n-\telse\n-\t  fprintf (asm_out_file, \"\\tb%c\\tLF%d\\n\", logic ? 'f' : 't', label);\n+      if (final_sequence\n+\t  && ! INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0)))\n+\t{\n+\t  asm_fprintf (asm_out_file, \"\\tb%s%ss\\t%LLF%d\\n\", logic ? \"f\" : \"t\",\n+\t\t       ASSEMBLER_DIALECT ? \"/\" : \".\", label);\n+\t  print_slot (final_sequence);\n+\t}\n+      else\n+\tasm_fprintf (asm_out_file, \"\\tb%s\\t%LLF%d\\n\", logic ? \"f\" : \"t\", label);\n \n-\toutput_asm_insn (\"bra\t%l0\", &op0);\n-\tfprintf (asm_out_file, \"\\tnop\\n\");\n-\tfprintf (asm_out_file, \"LF%d:\\n\", label);\n+      output_asm_insn (\"bra\\t%l0\", &op0);\n+      fprintf (asm_out_file, \"\\tnop\\n\");\n+      ASM_OUTPUT_INTERNAL_LABEL(asm_out_file, \"LF\", label);\n \n-\tif (final_sequence\n-\t    && INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0)))\n-\t  print_slot (final_sequence);\n-      }\n       return \"\";\n+    }\n+  return logic ? \"bt%.\\t%l0\" : \"bf%.\\t%l0\";\n+}\n \n-    case 16:\n-      /* A branch with an unfilled delay slot.  */\n-    case 18:\n-      /* Branches a long way away.  */\n-      {\n-\t/* The call to print_slot will clobber the operands.  */\n-\trtx op0 = operands[0];\n-\n-\t/* If the instruction in the delay slot is annulled (true), then\n-\t   there is no delay slot where we can put it now.  The only safe\n-\t   place for it is after the label.  */\n+int branch_offset ();\n \n-\tif (final_sequence)\n-\t  {\n-\t    fprintf (asm_out_file, \"\\tb%c%s\\tLF%d\\n\", logic ? 'f' : 't',\n-\t\t     INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0))\n-\t\t     ? \"\" : \".s\", label);\n-\t    if (! INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0)))\n-\t      print_slot (final_sequence);\n-\t  }\n-\telse\n-\t  fprintf (asm_out_file, \"\\tb%c\\tLF%d\\n\", logic ? 'f' : 't', label);\n-\n-\toutput_far_jump (insn, op0);\n-\tfprintf (asm_out_file, \"LF%d:\\n\", label);\n+char *\n+output_branchy_insn (code, template, insn, operands)\n+     char *template;\n+     enum rtx_code code;\n+     rtx insn;\n+     rtx *operands;\n+{\n+  rtx next_insn = NEXT_INSN (insn);\n+  int label_nr;\n \n-\tif (final_sequence\n-\t    && INSN_ANNULLED_BRANCH_P (XVECEXP (final_sequence, 0, 0)))\n-\t  print_slot (final_sequence);\n-      }\n-      return \"\";\n+  if (next_insn && GET_CODE (next_insn) == JUMP_INSN && condjump_p (next_insn))\n+    {\n+      rtx src = SET_SRC (PATTERN (next_insn));\n+      if (GET_CODE (src) == IF_THEN_ELSE && GET_CODE (XEXP (src, 0)) != code)\n+\t{\n+\t  /* Following branch not taken */\n+\t  operands[9] = gen_label_rtx ();\n+\t  emit_label_after (operands[9], next_insn);\n+\t  return template;\n+\t}\n+      else\n+\t{\n+\t  int offset = branch_offset (next_insn) + 4;\n+\t  if (offset >= -252 && offset <= 256)\n+\t    {\n+\t      if (GET_CODE (src) == IF_THEN_ELSE)\n+\t\t/* branch_true */\n+\t\tsrc = XEXP (src, 1);\n+\t      operands[9] = src;\n+\t      return template;\n+\t    }\n+\t}\n     }\n+  operands[9] = gen_label_rtx ();\n+  emit_label_after (operands[9], insn);\n+  return template;\n+}\n \n-  abort ();\n+char *\n+output_ieee_ccmpeq (insn, operands)\n+     rtx insn, operands;\n+{\n+  output_branchy_insn (NE, \"bt\\t%l9\\\\;fcmp/eq\\t%1,%0\", insn, operands);\n }\n \f\n /* Output to FILE the start of the assembler file.  */\n@@ -751,20 +797,15 @@ shiftcosts (x)\n \n   /* If shift by a non constant, then this will be expensive.  */\n   if (GET_CODE (XEXP (x, 1)) != CONST_INT)\n-    {\n-      if (TARGET_SH3)\n-\treturn 2;\n-      /* If not an sh3 then we don't even have an instruction for it.  */\n-      return 20;\n-    }\n+    return SH_DYNAMIC_SHIFT_COST;\n \n   /* Otherwise, return the true cost in instructions.  */\n   if (GET_CODE (x) == ASHIFTRT)\n     {\n       int cost = ashiftrt_insns[value];\n       /* If SH3, then we put the constant in a reg and use shad.  */\n-      if (TARGET_SH3 && cost > 3)\n-\tcost = 3;\n+      if (cost > 1 + SH_DYNAMIC_SHIFT_COST)\n+\tcost = 1 + SH_DYNAMIC_SHIFT_COST;\n       return cost;\n     }\n   else\n@@ -999,17 +1040,19 @@ expand_ashiftrt (operands)\n \t  emit_insn (gen_ashrsi3_d (operands[0], operands[1], count));\n \t  return 1;\n \t}\n-      else if (ashiftrt_insns[INTVAL (operands[2])] > 3)\n+      else if (ashiftrt_insns[INTVAL (operands[2]) & 31]\n+\t       > 1 + SH_DYNAMIC_SHIFT_COST)\n \t{\n-\t  rtx count = force_reg (SImode, GEN_INT (- INTVAL (operands[2])));\n+\t  rtx count\n+\t    = force_reg (SImode, GEN_INT (- (INTVAL (operands[2]) & 31)));\n \t  emit_insn (gen_ashrsi3_d (operands[0], operands[1], count));\n \t  return 1;\n \t}\n     }\n   if (GET_CODE (operands[2]) != CONST_INT)\n     return 0;\n \n-  value = INTVAL (operands[2]);\n+  value = INTVAL (operands[2]) & 31;\n \n   if (value == 31)\n     {\n@@ -1050,6 +1093,12 @@ expand_ashiftrt (operands)\n   return 1;\n }\n \n+int sh_dynamicalize_shift_p (count)\n+     rtx count;\n+{\n+  return shift_insns[INTVAL (count)] > 1 + SH_DYNAMIC_SHIFT_COST;\n+}\n+\n /* Try to find a good way to implement the combiner pattern\n   [(set (match_operand:SI 0 \"register_operand\" \"r\")\n         (and:SI (ashift:SI (match_operand:SI 1 \"register_operand\" \"r\")\n@@ -1434,7 +1483,7 @@ shl_sext_kind (left_rtx, size_rtx, costp)\n   if (TARGET_SH3)\n     {\n       /* Try to use a dynamic shift.  */\n-      cost = shift_insns[32 - insize] + 3;\n+      cost = shift_insns[32 - insize] + 1 + SH_DYNAMIC_SHIFT_COST;\n       if (cost < best_cost)\n \t{\n \t  kind = 0;\n@@ -1645,6 +1694,8 @@ typedef struct\n static pool_node pool_vector[MAX_POOL_SIZE];\n static int pool_size;\n \n+static int max_uid_before_fixup_addr_diff_vecs;\n+\n /* ??? If we need a constant in HImode which is the truncated value of a\n    constant we need in SImode, we could combine the two entries thus saving\n    two bytes.  Is this common enough to be worth the effort of implementing\n@@ -1735,7 +1786,8 @@ dump_table (scan)\n \t      scan = emit_label_after (gen_label_rtx (), scan);\n \t      scan = emit_insn_after (gen_align_4 (), scan);\n \t    }\n-\t  scan = emit_label_after (p->label, scan);\n+\t  if (p->label)\n+\t    scan = emit_label_after (p->label, scan);\n \t  scan = emit_insn_after (gen_consttable_4 (p->value), scan);\n \t  break;\n \tcase DFmode:\n@@ -1746,7 +1798,8 @@ dump_table (scan)\n \t      scan = emit_label_after (gen_label_rtx (), scan);\n \t      scan = emit_insn_after (gen_align_4 (), scan);\n \t    }\n-\t  scan = emit_label_after (p->label, scan);\n+\t  if (p->label)\n+\t    scan = emit_label_after (p->label, scan);\n \t  scan = emit_insn_after (gen_consttable_8 (p->value), scan);\n \t  break;\n \tdefault:\n@@ -1792,7 +1845,8 @@ broken_move (insn)\n \t     order bits end up as.  */\n \t  && GET_MODE (SET_DEST (pat)) != QImode\n \t  && CONSTANT_P (SET_SRC (pat))\n-\t  && ! (GET_CODE (SET_SRC (pat)) == CONST_DOUBLE\n+\t  && ! (TARGET_SH3E\n+\t\t&& GET_CODE (SET_SRC (pat)) == CONST_DOUBLE\n \t\t&& (fp_zero_operand (SET_SRC (pat))\n \t\t    || fp_one_operand (SET_SRC (pat)))\n \t\t&& GET_CODE (SET_DEST (pat)) == REG\n@@ -1806,28 +1860,49 @@ broken_move (insn)\n   return 0;\n }\n \n+int\n+cache_align_p (insn)\n+     rtx insn;\n+{\n+  rtx pat;\n+\n+  if (! insn)\n+    return 1;\n+\n+  if (GET_CODE (insn) != INSN)\n+    return 0;\n+\n+  pat = PATTERN (insn);\n+  return (GET_CODE (pat) == UNSPEC_VOLATILE\n+\t  && XINT (pat, 1) == 1\n+\t  && INTVAL (XVECEXP (pat, 0, 0)) == CACHE_LOG);\n+}\n+\n+static int\n+mova_p (insn)\n+     rtx insn;\n+{\n+  return (GET_CODE (insn) == INSN\n+\t  && GET_CODE (PATTERN (insn)) == SET\n+\t  && GET_CODE (SET_SRC (PATTERN (insn))) == UNSPEC\n+\t  && XINT (SET_SRC (PATTERN (insn)), 1) == 1);\n+}\n+\n /* Find the last barrier from insn FROM which is close enough to hold the\n    constant pool.  If we can't find one, then create one near the end of\n    the range.  */\n \n-/* ??? It would be good to put constant pool tables between a case jump and\n-   the jump table.  This fails for two reasons.  First, there is no\n-   barrier after the case jump.  This is a bug in the casesi pattern.\n-   Second, inserting the table here may break the mova instruction that\n-   loads the jump table address, by moving the jump table too far away.\n-   We fix that problem by never outputting the constant pool between a mova\n-   and its label.  */\n-\n static rtx\n-find_barrier (from)\n-     rtx from;\n+find_barrier (num_mova, mova, from)\n+     int num_mova;\n+     rtx mova, from;\n {\n   int count_si = 0;\n   int count_hi = 0;\n   int found_hi = 0;\n   int found_si = 0;\n-  rtx found_barrier = 0;\n-  rtx found_mova = 0;\n+  int leading_mova = num_mova;\n+  rtx barrier_before_mova, found_barrier = 0, good_barrier = 0;\n   int si_limit;\n   int hi_limit;\n \n@@ -1843,83 +1918,100 @@ find_barrier (from)\n      before the table, subtract 2 for the instruction that fills the jump\n      delay slot.  This gives 1018.  */\n \n-  /* If not optimizing, then it is possible that the jump instruction we add\n-     won't be shortened, and thus will have a length of 14 instead of 2.\n-     We must adjust the limits downwards to account for this, giving a limit\n-     of 1008 for SImode and 500 for HImode.  */\n+  /* The branch will always be shortened now that the reference address for\n+     forward branches is the sucessor address, thus we need no longer make\n+     adjustments to the [sh]i_limit for -O0.  */\n \n-  if (optimize)\n-    {\n-      si_limit = 1018;\n-      hi_limit = 510;\n-    }\n-  else\n-    {\n-      si_limit = 1008;\n-      hi_limit = 500;\n-    }\n-\n-  /* If not optimizing for space, then the constant pool will be\n-     aligned to a 4 to 16 byte boundary.  We must make room for that\n-     alignment that by reducing the limits.\n-     ??? It would be better to not align the constant pool, but\n-     ASM_OUTPUT_ALIGN_CODE does not make any provision for basing the\n-     alignment on the instruction.  */\n-\n-  if (! TARGET_SMALLCODE)\n-    {\n-      if (TARGET_SH3 || TARGET_SH3E)\n-\t{\n-\t  si_limit -= 14;\n-\t  hi_limit -= 14;\n-\t}\n-      else\n-\t{\n-\t  si_limit -= 2;\n-\t  hi_limit -= 2;\n-\t}\n-    }\n+  si_limit = 1018;\n+  hi_limit = 510;\n \n   while (from && count_si < si_limit && count_hi < hi_limit)\n     {\n-      int inc = get_attr_length (from);\n+      int inc = 0;\n+\n+      /* The instructions created by fixup_addr_diff_vecs have no valid length\n+       info yet.  They should be considered to have zero at this point.  */\n+      if (INSN_UID (from) < max_uid_before_fixup_addr_diff_vecs)\n+\tinc = get_attr_length (from);\n \n       if (GET_CODE (from) == BARRIER)\n-\tfound_barrier = from;\n+\t{\n+\t  found_barrier = from;\n+\t  /* If we are at the end of the function, or in front of an alignemnt\n+\t     instruction, we need not insert an extra alignment.  We prefer\n+\t     this kind of barrier.  */\n+\t\n+\t  if (cache_align_p (next_real_insn (found_barrier)))\n+\t    good_barrier = from;\n+\t}\n \n       if (broken_move (from))\n \t{\n-\t  rtx pat = PATTERN (from);\n-\t  rtx src = SET_SRC (pat);\n-\t  rtx dst = SET_DEST (pat);\n-\t  enum machine_mode mode = GET_MODE (dst);\n+\t  rtx pat, src, dst;\n+\t  enum machine_mode mode;\n+\n+\t  pat = PATTERN (from);\n+\t  if (GET_CODE (pat) == PARALLEL)\n+\t    pat = XVECEXP (pat, 0, 0);\n+\t  src = SET_SRC (pat);\n+\t  dst = SET_DEST (pat);\n+\t  mode = GET_MODE (dst);\n \n \t  /* We must explicitly check the mode, because sometimes the\n \t     front end will generate code to load unsigned constants into\n \t     HImode targets without properly sign extending them.  */\n \t  if (mode == HImode || (mode == SImode && hi_const (src)))\n \t    {\n-\t      found_hi = 1;\n+\t      found_hi += 2;\n \t      /* We put the short constants before the long constants, so\n \t\t we must count the length of short constants in the range\n \t\t for the long constants.  */\n \t      /* ??? This isn't optimal, but is easy to do.  */\n-\t      if (found_si)\n-\t\tcount_si += 2;\n+\t      si_limit -= 2;\n \t    }\n \t  else\n-\t    found_si = 1;\n+\t    {\n+\t      if (found_si > count_si)\n+\t\tcount_si = found_si;\n+\t      found_si += GET_MODE_SIZE (mode);\n+\t      if (num_mova)\n+\t\tsi_limit -= GET_MODE_SIZE (mode);\n+\t    }\n \t}\n \n       if (GET_CODE (from) == INSN\n \t  && GET_CODE (PATTERN (from)) == SET\n \t  && GET_CODE (SET_SRC (PATTERN (from))) == UNSPEC\n \t  && XINT (SET_SRC (PATTERN (from)), 1) == 1)\n-\tfound_mova = from;\n+\t{\n+\t  if (! num_mova++)\n+\t    {\n+\t      leading_mova = 0;\n+\t      mova = from;\n+\t      barrier_before_mova = good_barrier ? good_barrier : found_barrier;\n+\t    }\n+\t  if (found_si > count_si)\n+\t    count_si = found_si;\n+\t}\n       else if (GET_CODE (from) == JUMP_INSN\n \t       && (GET_CODE (PATTERN (from)) == ADDR_VEC\n \t\t   || GET_CODE (PATTERN (from)) == ADDR_DIFF_VEC))\n-\tfound_mova = 0;\n+\t{\n+\t  if (num_mova)\n+\t    num_mova--;\n+\t  if (cache_align_p (NEXT_INSN (next_nonnote_insn (from))))\n+\t    {\n+\t      /* We have just passed the barrier in front front of the\n+\t\t ADDR_DIFF_VEC.  Since the ADDR_DIFF_VEC is accessed\n+\t\t as data, just like our pool constants, this is a good\n+\t\t opportunity to accomodate what we have gathered so far.\n+\t\t If we waited any longer, we could end up at a barrier in\n+\t\t front of code, which gives worse cache usage for separated\n+\t\t instruction / data caches.  */\n+\t      good_barrier = found_barrier;\n+\t      break;\n+\t    }\n+\t}\n \n       if (found_si)\n \tcount_si += inc;\n@@ -1928,12 +2020,42 @@ find_barrier (from)\n       from = NEXT_INSN (from);\n     }\n \n-  /* Insert the constant pool table before the mova instruction, to prevent\n-     the mova label reference from going out of range.  */\n-  if (found_mova)\n-    from = found_mova;\n+  if (num_mova)\n+    if (leading_mova)\n+      {\n+\t/* Try as we might, the leading mova is out of range.  Change\n+\t   it into a load (which will become a pcload) and retry.  */\n+\tSET_SRC (PATTERN (mova)) = XVECEXP (SET_SRC (PATTERN (mova)), 0, 0);\n+\tINSN_CODE (mova) = -1;\n+        return find_barrier (0, 0, mova);\n+      }\n+    else\n+      {\n+\t/* Insert the constant pool table before the mova instruction,\n+\t   to prevent the mova label reference from going out of range.  */\n+\tfrom = mova;\n+\tgood_barrier = found_barrier = barrier_before_mova;\n+      }\n \n-  if (! found_barrier)\n+  if (found_barrier)\n+    {\n+      /* We have before prepared barriers to come in pairs, with an\n+\t alignment instruction in-between.  We want to use the first\n+\t barrier, so that the alignment applies to the code.\n+\t If we are compiling for SH3 or newer, there are some exceptions\n+\t when the second barrier and the alignment doesn't exist yet, so\n+\t we have to add it.  */\n+      if (good_barrier)\n+\tfound_barrier = good_barrier;\n+      else if (! TARGET_SMALLCODE)\n+\t{\n+\t  found_barrier\n+\t    = emit_insn_before (gen_align_log (GEN_INT (CACHE_LOG)),\n+\t\t\t\tfound_barrier);\n+\t  found_barrier = emit_barrier_before (found_barrier);\n+\t}\n+    }\n+  else\n     {\n       /* We didn't find a barrier in time to dump our stuff,\n \t so we'll make one.  */\n@@ -1961,6 +2083,11 @@ find_barrier (from)\n       LABEL_NUSES (label) = 1;\n       found_barrier = emit_barrier_after (from);\n       emit_label_after (label, found_barrier);\n+      if (! TARGET_SMALLCODE)\n+\t{\n+\t  emit_barrier_after (found_barrier);\n+\t  emit_insn_after (gen_align_log (GEN_INT (CACHE_LOG)), found_barrier);\n+\t}\n     }\n \n   return found_barrier;\n@@ -1970,7 +2097,7 @@ find_barrier (from)\n    positively find the register that is used to call the sfunc, and this\n    register is not used anywhere else in this instruction - except as the\n    destination of a set, return this register; else, return 0.  */\n-static rtx\n+rtx\n sfunc_uses_reg (insn)\n      rtx insn;\n {\n@@ -1986,7 +2113,7 @@ sfunc_uses_reg (insn)\n   for (reg_part = 0, i = XVECLEN (pattern, 0) - 1; i >= 1; i--)\n     {\n       part = XVECEXP (pattern, 0, i);\n-      if (GET_CODE (part) == USE)\n+      if (GET_CODE (part) == USE && GET_MODE (XEXP (part, 0)) == SImode)\n \treg_part = part;\n     }\n   if (! reg_part)\n@@ -2092,6 +2219,480 @@ noncall_uses_reg (reg, insn, set)\n   return 0;\n }\n \n+/* Given a X, a pattern of an insn or a part of it, return a mask of used\n+   general registers.  Bits 0..15 mean that the respective registers\n+   are used as inputs in the instruction.  Bits 16..31 mean that the\n+   registers 0..15, respectively, are used as outputs, or are clobbered.\n+   IS_DEST should be set to 16 if X is the destination of a SET, else to 0.  */\n+int\n+regs_used (x, is_dest)\n+     rtx x; int is_dest;\n+{\n+  enum rtx_code code;\n+  char *fmt;\n+  int i, used = 0;\n+\n+  if (! x)\n+    return used;\n+  code = GET_CODE (x);\n+  switch (code)\n+    {\n+    case REG:\n+      if (REGNO (x) < 16)\n+\treturn (((1 << HARD_REGNO_NREGS (0, GET_MODE (x))) - 1)\n+\t\t<< (REGNO (x) + is_dest));\n+      return 0;\n+    case SUBREG:\n+      {\n+\trtx y = SUBREG_REG (x);\n+     \n+\tif (GET_CODE (y) != REG)\n+\t  break;\n+\tif (REGNO (y) < 16)\n+\t  return (((1 << HARD_REGNO_NREGS (0, GET_MODE (x))) - 1)\n+\t\t  << (REGNO (y) + SUBREG_WORD (x) + is_dest));\n+\treturn 0;\n+      }\n+    case SET:\n+      return regs_used (SET_SRC (x), 0) | regs_used (SET_DEST (x), 16);\n+    case RETURN:\n+      /* If there was a return value, it must have been indicated with USE.  */\n+      return 0x00ffff00;\n+    case CLOBBER:\n+      is_dest = 1;\n+      break;\n+    case MEM:\n+      is_dest = 0;\n+      break;\n+    case CALL:\n+      used |= 0x00ff00f0;\n+      break;\n+    }\n+\n+  fmt = GET_RTX_FORMAT (code);\n+\n+  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n+    {\n+      if (fmt[i] == 'E')\n+\t{\n+\t  register int j;\n+\t  for (j = XVECLEN (x, i) - 1; j >= 0; j--)\n+\t    used |= regs_used (XVECEXP (x, i, j), is_dest);\n+\t}\n+      else if (fmt[i] == 'e')\n+\tused |= regs_used (XEXP (x, i), is_dest);\n+    }\n+  return used;\n+}\n+\n+/* Create an instruction that prevents redirection of a conditional branch\n+   to the desitination of the JUMP with address ADDR.\n+   If the branch needs to be implemented as an indirect jump, try to find\n+   a scratch register for it.\n+   If NEED_BLOCK is 0, don't do anything unless we need a scratch register.\n+   If any preceding insn that doesn't fit into a delay slot is good enough,\n+   pass 1.  Pass 2 if a definite blocking insn is needed.\n+   -1 is used internally to avoid deep recursion.\n+   If a blocking instruction is made or recognized, return it.  */\n+   \n+static rtx\n+gen_block_redirect (jump, addr, need_block)\n+     rtx jump;\n+     int addr, need_block;\n+{\n+  int dead = 0;\n+  rtx prev = prev_nonnote_insn (jump);\n+  rtx dest;\n+\n+  /* First, check if we already have an instruction that satisfies our need.  */\n+  if (prev && GET_CODE (prev) == INSN && ! INSN_DELETED_P (prev))\n+    {\n+      if (INSN_CODE (prev) == CODE_FOR_indirect_jump_scratch)\n+\treturn prev;\n+      if (GET_CODE (PATTERN (prev)) == USE\n+\t  || GET_CODE (PATTERN (prev)) == CLOBBER\n+\t  || get_attr_in_delay_slot (prev) == IN_DELAY_SLOT_YES)\n+\tprev = jump;\n+      else if ((need_block &= ~1) < 0)\n+\treturn prev;\n+      else if (recog_memoized (prev) == CODE_FOR_block_branch_redirect)\n+\tneed_block = 0;\n+    }\n+  /* We can't use JUMP_LABEL here because it might be undefined\n+     when not optimizing.  */\n+  dest = XEXP (SET_SRC (PATTERN (jump)), 0);\n+  /* If the branch is out of range, try to find a scratch register for it.  */\n+  if (optimize\n+      && (insn_addresses[INSN_UID (dest)] - addr + 4092U > 4092 + 4098))\n+    {\n+      rtx scan;\n+      /* Don't look for the stack pointer as a scratch register,\n+\t it would cause trouble if an interrupt occured.  */\n+      unsigned try = 0x7fff, used;\n+      int jump_left = flag_expensive_optimizations + 1;\n+    \n+      /* It is likely that the most recent eligible instruction is wanted for\n+\t the delay slot.  Therefore, find out which registers it uses, and\n+\t try to avoid using them.  */\n+\t \n+      for (scan = jump; scan = PREV_INSN (scan); )\n+\t{\n+\t  enum rtx_code code;\n+\n+\t  if (INSN_DELETED_P (scan))\n+\t    continue;\n+\t  code = GET_CODE (scan);\n+\t  if (code == CODE_LABEL || code == JUMP_INSN)\n+\t    break;\n+\t  if (code == INSN\n+\t      && GET_CODE (PATTERN (scan)) != USE\n+\t      && GET_CODE (PATTERN (scan)) != CLOBBER\n+\t      && get_attr_in_delay_slot (scan) == IN_DELAY_SLOT_YES)\n+\t    {\n+\t      try &= ~regs_used (PATTERN (scan), 0);\n+\t      break;\n+\t    }\n+\t}\n+      for (used = dead = 0, scan = JUMP_LABEL (jump); scan = NEXT_INSN (scan); )\n+\t{\n+\t  enum rtx_code code;\n+\n+\t  if (INSN_DELETED_P (scan))\n+\t    continue;\n+\t  code = GET_CODE (scan);\n+\t  if (GET_RTX_CLASS (code) == 'i')\n+\t    {\n+\t      used |= regs_used (PATTERN (scan), 0);\n+\t      if (code == CALL_INSN)\n+\t\tused |= regs_used (CALL_INSN_FUNCTION_USAGE (scan), 0);\n+\t      dead |= (used >> 16) & ~used;\n+\t      if (dead & try)\n+\t\t{\n+\t\t  dead &= try;\n+\t\t  break;\n+\t\t}\n+\t      if (code == JUMP_INSN)\n+\t\tif (jump_left-- && simplejump_p (scan))\n+\t\t  scan = JUMP_LABEL (scan);\n+\t\telse\n+\t\t  break;\n+\t    }\n+\t}\n+      /* Mask out the stack pointer again, in case it was\n+\t the only 'free' register we have found.  */\n+      dead &= 0x7fff;\n+    }\n+  /* If the immediate destination is still in range, check for possible\n+     threading with a jump beyond the delay slot insn.\n+     Don't check if we are called recursively; the jump has been or will be\n+     checked in a different invokation then.  */\n+\t\n+  else if (optimize && need_block >= 0)\n+    {\n+      rtx next = next_active_insn (next_active_insn (dest));\n+      if (next && GET_CODE (next) == JUMP_INSN\n+\t  && GET_CODE (PATTERN (next)) == SET\n+\t  && recog_memoized (next) == CODE_FOR_jump)\n+\t{\n+\t  dest = JUMP_LABEL (next);\n+\t  if (dest\n+\t      && insn_addresses[INSN_UID (dest)] - addr + 4092U > 4092 + 4098)\n+\t    gen_block_redirect (next, insn_addresses[INSN_UID (next)], -1);\n+\t}\n+    }\n+\n+  if (dead)\n+    {\n+      rtx reg = gen_rtx (REG, SImode, exact_log2 (dead & -dead));\n+\n+      /* It would be nice if we could convert the jump into an indirect\n+\t jump / far branch right now, and thus exposing all consitituent\n+\t instructions to further optimization.  However, reorg uses\n+\t simplejump_p to determine if there is an unconditional jump where\n+\t it should try to schedule instructions from the target of the\n+\t branch; simplejump_p fails for indirect jumps even if they have\n+\t a JUMP_LABEL.  */\n+      rtx insn = emit_insn_before (gen_indirect_jump_scratch\n+\t\t\t\t   (reg, GEN_INT (INSN_UID (JUMP_LABEL (jump))))\n+\t\t\t\t   , jump);\n+      INSN_CODE (insn) = CODE_FOR_indirect_jump_scratch;\n+      return insn;\n+    }\n+  else if (need_block)\n+    /* We can't use JUMP_LABEL here because it might be undefined\n+       when not optimizing.  */\n+    return emit_insn_before (gen_block_branch_redirect\n+\t\t      (GEN_INT (INSN_UID (XEXP (SET_SRC (PATTERN (jump)), 0))))\n+\t\t      , jump);\n+  return prev;\n+}\n+\n+#define CONDJUMP_MIN -252\n+#define CONDJUMP_MAX 262\n+struct far_branch\n+{\n+  /* A label (to be placed) in front of the jump\n+     that jumps to our ultimate destination.  */\n+  rtx near_label;\n+  /* Where we are going to insert it if we cannot move the jump any farther,\n+     or the jump itself if we have picked up an existing jump.  */\n+  rtx insert_place;\n+  /* The ultimate destination.  */\n+  rtx far_label;\n+  struct far_branch *prev;\n+  /* If the branch has already been created, its address;\n+     else the address of its first prospective user.  */\n+  int address;\n+};\n+\n+enum mdep_reorg_phase_e mdep_reorg_phase;\n+void\n+gen_far_branch (bp)\n+     struct far_branch *bp;\n+{\n+  rtx insn = bp->insert_place;\n+  rtx jump;\n+  rtx label = gen_label_rtx ();\n+\n+  emit_label_after (label, insn);\n+  if (bp->far_label)\n+    {\n+      jump = emit_jump_insn_after (gen_jump (bp->far_label), insn);\n+      LABEL_NUSES (bp->far_label)++;\n+    }\n+  else\n+    jump = emit_jump_insn_after (gen_return (), insn);\n+  emit_label_after (bp->near_label, insn);\n+  JUMP_LABEL (jump) = bp->far_label;\n+  if (! invert_jump (insn, label))\n+    abort ();\n+  /* Prevent reorg from undoing our splits.  */\n+  gen_block_redirect (jump, bp->address += 2, 2);\n+}\n+\n+static void\n+fixup_aligns ()\n+{\n+  rtx insn = get_last_insn ();\n+  rtx align_tab[MAX_BITS_PER_WORD];\n+  int i;\n+\n+  for (i = CACHE_LOG; i >= 0; i--)\n+    align_tab[i] = insn;\n+  bzero ((char *) uid_align, uid_align_max * sizeof *uid_align);\n+  for (; insn; insn = PREV_INSN (insn))\n+    {\n+      int uid = INSN_UID (insn);\n+      if (uid < uid_align_max)\n+\tuid_align[uid] = align_tab[1];\n+      if (GET_CODE (insn) == INSN)\n+\t{\n+\t  rtx pat = PATTERN (insn);\n+\t  if (GET_CODE (pat) == UNSPEC_VOLATILE && XINT (pat, 1) == 1)\n+\t    {\n+\t      /* Found an alignment instruction.  */\n+\t      int log = INTVAL (XVECEXP (pat, 0, 0));\n+\t      uid_align[uid] = align_tab[log];\n+\t      for (i = log - 1; i >= 0; i--)\n+\t\talign_tab[i] = insn;\n+\t    }\n+\t}\n+      else if (GET_CODE (insn) == JUMP_INSN\n+\t       && GET_CODE (PATTERN (insn)) == SET)\n+\t{\n+\t  rtx dest = SET_SRC (PATTERN (insn));\n+\t  if (GET_CODE (dest) == IF_THEN_ELSE)\n+\t    dest = XEXP (dest, 1);\n+\t  if (GET_CODE (dest) == LABEL_REF)\n+\t    {\n+\t      dest = XEXP (dest, 0);\n+\t      if (! uid_align[INSN_UID (dest)])\n+\t\t/* Mark backward branch.  */\n+\t\tuid_align[uid] = 0;\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* Fix up ADDR_DIFF_VECs.  */\n+void\n+fixup_addr_diff_vecs (first)\n+     rtx first;\n+{\n+  rtx insn;\n+  int max_address;\n+  int need_fixup_aligns = 0;\n+  \n+  if (optimize)\n+    max_address = insn_addresses[INSN_UID (get_last_insn ())] + 2;\n+  for (insn = first; insn; insn = NEXT_INSN (insn))\n+    {\n+      rtx vec_lab, rel_lab, pat, min_lab, max_lab, adj;\n+      int len, i, min, max, size;\n+\n+      if (GET_CODE (insn) != JUMP_INSN\n+\t  || GET_CODE (PATTERN (insn)) != ADDR_DIFF_VEC)\n+\tcontinue;\n+      pat = PATTERN (insn);\n+      rel_lab = vec_lab = XEXP (XEXP (pat, 0), 0);\n+      if (TARGET_SH2)\n+\t{\n+\t  rtx prev, prevpat, x;\n+\n+\t  /* Search the matching casesi_jump_2.  */\n+\t  for (prev = vec_lab; ; prev = PREV_INSN (prev))\n+\t    {\n+\t      if (GET_CODE (prev) != JUMP_INSN)\n+\t\tcontinue;\n+\t      prevpat = PATTERN (prev);\n+\t      if (GET_CODE (prevpat) != PARALLEL || XVECLEN (prevpat, 0) != 2)\n+\t\tcontinue;\n+\t      x = XVECEXP (prevpat, 0, 1);\n+\t      if (GET_CODE (x) != USE)\n+\t\tcontinue;\n+\t      x = XEXP (x, 0);\n+\t      if (GET_CODE (x) == LABEL_REF && XEXP (x, 0) == vec_lab)\n+\t\tbreak;\n+\t    }\n+\t  /* Fix up the ADDR_DIF_VEC to be relative\n+\t     to the reference address of the braf.  */\n+\t  XEXP (XEXP (pat, 0), 0)\n+\t    = rel_lab = XEXP (XEXP (SET_SRC (XVECEXP (prevpat, 0, 0)), 1), 0);\n+\t}\n+      if (! optimize)\n+\tcontinue;\n+      len = XVECLEN (pat, 1);\n+      if (len <= 0)\n+\tabort ();\n+      for (min = max_address, max = 0, i = len - 1; i >= 0; i--)\n+\t{\n+\t  rtx lab = XEXP (XVECEXP (pat, 1, i), 0);\n+\t  int addr = insn_addresses[INSN_UID (lab)];\n+\t  if (addr < min)\n+\t    {\n+\t      min = addr;\n+\t      min_lab = lab;\n+\t    }\n+\t  if (addr > max)\n+\t    {\n+\t      max = addr;\n+\t      max_lab = lab;\n+\t    }\n+\t}\n+      adj\n+\t= emit_insn_before (gen_addr_diff_vec_adjust (min_lab, max_lab, rel_lab,\n+\t\t\t\t\t\t      GEN_INT (len)), vec_lab);\n+      size = (XVECLEN (pat, 1) * GET_MODE_SIZE (GET_MODE (pat))\n+\t      - addr_diff_vec_adjust (adj, 0));\n+      /* If this is a very small table, we want to remove the alignment after\n+\t the table.  */\n+      if (! TARGET_SMALLCODE && size <= 1 << (CACHE_LOG - 2))\n+\t{\n+\t  rtx align = NEXT_INSN (next_nonnote_insn (insn));\n+\t  PUT_CODE (align, NOTE);\n+\t  NOTE_LINE_NUMBER (align) = NOTE_INSN_DELETED;\n+\t  NOTE_SOURCE_FILE (align) = 0;\n+\t  need_fixup_aligns = 1;\n+\t}\n+    }\n+  if (need_fixup_aligns)\n+    fixup_aligns ();\n+}\n+\n+/* Say how much the ADDR_DIFF_VEC following INSN can be shortened.\n+   If FIRST_PASS is nonzero, all addresses and length of following\n+   insns are still uninitialized.  */\n+int\n+addr_diff_vec_adjust (insn, first_pass)\n+     rtx insn;\n+     int first_pass;\n+{\n+  rtx pat = PATTERN (insn);\n+  rtx min_lab = XEXP (XVECEXP (pat, 0, 0), 0);\n+  rtx max_lab = XEXP (XVECEXP (pat, 0, 1), 0);\n+  rtx rel_lab = XEXP (XVECEXP (pat, 0, 2), 0);\n+  int len = INTVAL (XVECEXP (pat, 0, 3));\n+  int addr, min_addr, max_addr, saving, prev_saving = 0, offset;\n+  rtx align_insn = uid_align[INSN_UID (rel_lab)];\n+  int standard_size = TARGET_BIGTABLE ? 4 : 2;\n+  int last_size = GET_MODE_SIZE ( GET_MODE(pat));\n+  int align_fuzz = 0;\n+\n+  if (! insn_addresses)\n+    return 0;\n+  if (first_pass)\n+    /* If optimizing, we may start off with an optimistic guess.  */\n+    return optimize ? len & ~1 : 0;\n+  addr = insn_addresses[INSN_UID (rel_lab)];\n+  min_addr = insn_addresses[INSN_UID (min_lab)];\n+  max_addr = insn_addresses[INSN_UID (max_lab)];\n+  if (! last_size)\n+    last_size = standard_size;\n+  if (TARGET_SH2)\n+    prev_saving = ((standard_size - last_size) * len) & ~1;\n+\n+  /* The savings are linear to the vector length.  However, if we have an\n+     odd saving, we need one byte again to reinstate 16 bit alignment.  */\n+  saving = ((standard_size - 1) * len) & ~1;\n+  offset = prev_saving - saving;\n+\n+  if ((insn_addresses[INSN_UID (align_insn)] < max_addr\n+       || (insn_addresses[INSN_UID (align_insn)] == max_addr\n+           && next_real_insn (max_lab) != align_insn))\n+      && GET_CODE (align_insn) == INSN)\n+    {\n+      int align = 1 << INTVAL (XVECEXP (PATTERN (align_insn), 0, 0));\n+      int align_addr = insn_addresses[INSN_UID (align_insn)];\n+      if (align_addr > insn_addresses[INSN_UID (insn)])\n+\t{\n+\t  int old_offset = offset;\n+\t  offset = (align_addr - 1 & align - 1) + offset & -align;\n+\t  align_addr += old_offset;\n+\t}\n+      align_fuzz += (align_addr - 1)  & (align - 2);\n+      align_insn = uid_align[INSN_UID (align_insn)];\n+      if (insn_addresses[INSN_UID (align_insn)] <= max_addr\n+          && GET_CODE (align_insn) == INSN)\n+        {\n+          int align2 = 1 << INTVAL (XVECEXP (PATTERN (align_insn), 0, 0));\n+          align_addr = insn_addresses[INSN_UID (align_insn)];\n+\t  if (align_addr > insn_addresses[INSN_UID (insn)])\n+\t    {\n+\t      int old_offset = offset;\n+\t      offset = (align_addr - 1 & align2 - 1) + offset & -align2;\n+\t      align_addr += old_offset;\n+\t    }\n+          align_fuzz += (align_addr - 1)  & (align2 - align);\n+        }\n+    }\n+\n+  if (min_addr >= addr\n+      && max_addr + offset - addr + align_fuzz <= 255)\n+    {\n+      PUT_MODE (pat, QImode);\n+      return saving;\n+    }\n+  saving = 2 * len;\n+/* Since alignment might play a role in min_addr if it is smaller than addr,\n+   we may not use it without exact alignment compensation; a 'worst case'\n+   estimate is not good enough, because it won't prevent infinite oscillation\n+   of shorten_branches.\n+   ??? We should fix that eventually, but the code to deal with alignments\n+   should go in a new function.  */\n+#if 0\n+  if (TARGET_BIGTABLE && min_addr - ((1 << CACHE_LOG) - 2) - addr >= -32768\n+#else\n+  if (TARGET_BIGTABLE && (min_addr >= addr || addr <= 32768)\n+#endif\n+      && max_addr - addr <= 32767 + saving - prev_saving)\n+    {\n+      PUT_MODE (pat, HImode);\n+      return saving;\n+    }\n+  PUT_MODE (pat, TARGET_BIGTABLE ? SImode : HImode);\n+  return 0;\n+}\n+\n /* Exported to toplev.c.\n \n    Do a final pass over the function, just before delayed branch\n@@ -2101,14 +2702,18 @@ void\n machine_dependent_reorg (first)\n      rtx first;\n {\n-  rtx insn;\n+  rtx insn, mova;\n+  int num_mova;\n+  rtx r0_rtx = gen_rtx (REG, Pmode, 0);\n+  rtx r0_inc_rtx = gen_rtx (POST_INC, Pmode, r0_rtx);\n \n   /* If relaxing, generate pseudo-ops to associate function calls with\n      the symbols they call.  It does no harm to not generate these\n      pseudo-ops.  However, when we can generate them, it enables to\n      linker to potentially relax the jsr to a bsr, and eliminate the\n      register load and, possibly, the constant pool entry.  */\n \n+  mdep_reorg_phase = SH_INSERT_USES_LABELS;\n   if (TARGET_RELAX)\n     {\n       /* Remove all REG_LABEL notes.  We want to use them for our own\n@@ -2330,21 +2935,178 @@ machine_dependent_reorg (first)\n \t}\n     }\n \n+  /* The following processing passes need length information.\n+     addr_diff_vec_adjust needs to know if insn_addreses is valid.  */\n+  insn_addresses = 0;\n+\n+  /* If not optimizing for space, we want extra alignment for code after\n+     a barrier, so that it starts on a word / cache line boundary.\n+     We used to emit the alignment for the barrier itself and associate the\n+     instruction length with the following instruction, but that had two\n+     problems:\n+     i) A code label that follows directly after a barrier gets too low an\n+     address.  When there is a forward branch to it, the incorrect distance\n+     calculation can lead to out of range branches.  That happened with\n+     compile/920625-2 -O -fomit-frame-pointer in copyQueryResult.\n+     ii) barriers before constant tables get the extra alignment too.\n+     That is just a waste of space.\n+\n+     So what we do now is to insert align_* instructions after the\n+     barriers.  By doing that before literal tables are generated, we\n+     don't have to care about these.  */\n+  /* We also want alignment in front of ADDR_DIFF_VECs; this is done already\n+     by ASM_OUTPUT_CASE_LABEL, but when optimizing, we have to make it\n+     explicit in the RTL in order to correctly shorten branches.  */\n+    \n+  if (optimize)\n+    for (insn = first; insn; insn = NEXT_INSN (insn))\n+      {\n+\trtx addr_diff_vec;\n+\n+\tif (GET_CODE (insn) == BARRIER\n+\t    && (addr_diff_vec = next_real_insn (insn)))\n+\t  if (GET_CODE (PATTERN (addr_diff_vec)) == ADDR_DIFF_VEC)\n+\t    emit_insn_before (gen_align_4 (),\n+\t\t\t      XEXP (XEXP (PATTERN (addr_diff_vec), 0), 0));\n+\t  else if (TARGET_SMALLCODE)\n+\t    continue;\n+\t  else if (TARGET_SH3)\n+\t    {\n+\t      /* We align for an entire cache line.  If there is a immediately\n+\t\t preceding branch to the insn beyond the barrier, it does not\n+\t\t make sense to insert the align, because we are more likely\n+\t\t to discard useful information from the current cache line\n+\t\t when doing the align than to fetch unneeded insns when not.  */\n+\t      rtx prev = prev_real_insn (prev_real_insn (insn));\n+\t      int slot, credit;\n+\n+\t      for (slot = 2, credit = 1 << (CACHE_LOG - 2) + 2;\n+\t\t   credit >= 0 && prev && GET_CODE (prev) == INSN;\n+\t\t   prev = prev_real_insn (prev))\n+\t\t{\n+\t\t  if (GET_CODE (PATTERN (prev)) == USE\n+\t\t      || GET_CODE (PATTERN (prev)) == CLOBBER)\n+\t\t    continue;\n+\t\t  if (slot &&\n+\t\t      get_attr_in_delay_slot (prev) == IN_DELAY_SLOT_YES)\n+\t\t    slot = 0;\n+\t\t  credit -= get_attr_length (prev);\n+\t\t}\n+\t      if (! prev || GET_CODE (prev) != JUMP_INSN\n+\t\t  || (next_real_insn (JUMP_LABEL (prev))\n+\t\t      != next_real_insn (insn))\n+\t\t  || (credit - slot\n+\t\t      < (GET_CODE (SET_SRC (PATTERN (prev))) == PC ? 2 : 0)))\n+\t\t{\n+\t\t  insn = emit_insn_after (gen_align_log (GEN_INT (CACHE_LOG)),\n+\t\t\t\t\t  insn);\n+\t\t  insn = emit_barrier_after (insn);\n+\t\t}\n+\t    }\n+\t  else\n+\t    {\n+\t      insn = emit_insn_after (gen_align_4 (), insn);\n+\t      insn = emit_barrier_after (insn);\n+\t    }\n+\telse if (TARGET_SMALLCODE)\n+\t  continue;\n+\telse if (GET_CODE (insn) == NOTE\n+\t\t && NOTE_LINE_NUMBER (insn) == NOTE_INSN_LOOP_BEG)\n+\t  {\n+\t    rtx next = next_nonnote_insn (insn);\n+            if (next && GET_CODE (next) == CODE_LABEL)\n+\t      emit_insn_after (gen_align_4 (), insn);\n+\t  }\n+      }\n+\n+  /* If TARGET_IEEE, we might have to split some branches before fixup_align.\n+     If optimizing, the double call to shorten_branches will split insns twice,\n+     unless we split now all that is to split and delete the original insn.  */\n+  if (TARGET_IEEE || optimize)\n+    for (insn = NEXT_INSN (first); insn; insn = NEXT_INSN (insn))\n+      if (GET_RTX_CLASS (GET_CODE (insn)) == 'i' && ! INSN_DELETED_P (insn))\n+\t{\n+\t  rtx old = insn;\n+\t  insn = try_split (PATTERN (insn), insn, 1);\n+\t  if (INSN_DELETED_P (old))\n+\t    {\n+\t      PUT_CODE (old, NOTE);\n+\t      NOTE_LINE_NUMBER (old) = NOTE_INSN_DELETED;\n+\t      NOTE_SOURCE_FILE (old) = 0;\n+\t    }\n+\t}\n+\n+  max_uid_before_fixup_addr_diff_vecs = get_max_uid ();\n+\n+  if (optimize)\n+    {\n+      uid_align_max = get_max_uid ();\n+      uid_align = (rtx *) alloca (uid_align_max * sizeof *uid_align);\n+      fixup_aligns ();\n+      mdep_reorg_phase = SH_SHORTEN_BRANCHES0;\n+      shorten_branches (first);\n+    }\n+  fixup_addr_diff_vecs (first);\n   /* Scan the function looking for move instructions which have to be\n      changed to pc-relative loads and insert the literal tables.  */\n \n-  for (insn = first; insn; insn = NEXT_INSN (insn))\n+  mdep_reorg_phase = SH_FIXUP_PCLOAD;\n+  for (insn = first, num_mova = 0; insn; insn = NEXT_INSN (insn))\n     {\n+      if (mova_p (insn))\n+\t{\n+\t  if (! num_mova++)\n+\t    mova = insn;\n+\t}\n+      else if (GET_CODE (insn) == JUMP_INSN\n+\t       && GET_CODE (PATTERN (insn)) == ADDR_DIFF_VEC\n+\t       && num_mova)\n+\t{\n+\t  rtx scan;\n+\t  int total;\n+\n+\t  num_mova--;\n+\n+\t  /* Some code might have been inserted between the mova and\n+\t     its ADDR_DIFF_VEC.  Check if the mova is still in range.  */\n+\t  for (scan = mova, total = 0; scan != insn; scan = NEXT_INSN (scan))\n+\t    if (INSN_UID (scan) < max_uid_before_fixup_addr_diff_vecs)\n+\t      total += get_attr_length (scan);\n+\n+\t  /* range of mova is 1020, add 4 because pc counts from address of\n+\t     second instruction after this one, subtract 2 in case pc is 2\n+\t     byte aligned.  Possible alignment needed for the ADDR_DIFF_VEC\n+\t     chancles out with alignment effects of the mova itself.  */\n+\t  if (total > 1022)\n+\t    {\n+\t      /* Change the mova into a load, and restart scanning\n+\t\t there.  broken_move will then return true for mova.  */\n+\t      SET_SRC (PATTERN (mova))\n+\t\t= XVECEXP (SET_SRC (PATTERN (mova)), 0, 0);\n+\t      INSN_CODE (mova) = -1;\n+\t      insn = mova;\n+\t    }\n+\t}\n       if (broken_move (insn))\n \t{\n \t  rtx scan;\n \t  /* Scan ahead looking for a barrier to stick the constant table\n \t     behind.  */\n-\t  rtx barrier = find_barrier (insn);\n+\t  rtx barrier = find_barrier (num_mova, mova, insn);\n+\t  rtx last_float_move, last_float = 0, *last_float_addr;\n \n+\t  if (num_mova && ! mova_p (mova))\n+\t    {\n+\t      /* find_barrier had to change the first mova into a\n+\t\t pcload; thus, we have to start with this new pcload.  */\n+\t      insn = mova;\n+\t      num_mova = 0;\n+\t    }\n \t  /* Now find all the moves between the points and modify them.  */\n \t  for (scan = insn; scan != barrier; scan = NEXT_INSN (scan))\n \t    {\n+\t      if (GET_CODE (scan) == CODE_LABEL)\n+\t\tlast_float = 0;\n \t      if (broken_move (scan))\n \t\t{\n \t\t  rtx *patp = &PATTERN (scan), pat = *patp;\n@@ -2373,17 +3135,306 @@ machine_dependent_reorg (first)\n \t\t      dst = gen_rtx (REG, HImode, REGNO (dst) + offset);\n \t\t    }\n \n-\t\t  lab = add_constant (src, mode);\n-\t\t  newsrc = gen_rtx (MEM, mode,\n-\t\t\t\t    gen_rtx (LABEL_REF, VOIDmode, lab));\n+\t\t  if (GET_CODE (dst) == REG\n+\t\t      && ((REGNO (dst) >= FIRST_FP_REG\n+\t\t\t   && REGNO (dst) <= LAST_FP_REG)\n+\t\t\t  || REGNO (dst) == FPUL_REG))\n+\t\t    {\n+\t\t      if (last_float\n+\t\t\t  && reg_set_between_p (r0_rtx, last_float_move, scan))\n+\t\t\tlast_float = 0;\n+\t\t      lab = add_constant (src, mode, last_float);\n+\t\t      if (lab)\n+\t\t\temit_insn_before (gen_mova (lab), scan);\n+\t\t      else\n+\t\t\t*last_float_addr = r0_inc_rtx;\n+\t\t      last_float_move = scan;\n+\t\t      last_float = src;\n+\t\t      newsrc = gen_rtx (MEM, mode,\n+\t\t\t\t\t(REGNO (dst) == FPUL_REG\n+\t\t\t\t\t ? r0_inc_rtx\n+\t\t\t\t\t : r0_rtx));\n+\t\t      last_float_addr = &XEXP (newsrc, 0);\n+\t\t    }\n+\t\t  else\n+\t\t    {\n+\t\t      lab = add_constant (src, mode, 0);\n+\t\t      newsrc = gen_rtx (MEM, mode,\n+\t\t\t\t\tgen_rtx (LABEL_REF, VOIDmode, lab));\n+\t\t    }\n \t\t  RTX_UNCHANGING_P (newsrc) = 1;\n \t\t  *patp = gen_rtx (SET, VOIDmode, dst, newsrc);\n \t\t  INSN_CODE (scan) = -1;\n \t\t}\n \t    }\n \t  dump_table (barrier);\n+\t  insn = barrier;\n \t}\n     }\n+\n+  mdep_reorg_phase = SH_SHORTEN_BRANCHES1;\n+  insn_addresses = 0;\n+  split_branches (first);\n+\n+  /* The INSN_REFERENCES_ARE_DELAYED in sh.h is problematic because it\n+     also has an effect on the register that holds the addres of the sfunc.\n+     Insert an extra dummy insn in front of each sfunc that pretends to\n+     use this register.  */\n+  if (flag_delayed_branch)\n+    {\n+      for (insn = first; insn; insn = NEXT_INSN (insn))\n+\t{\n+\t  rtx reg = sfunc_uses_reg (insn);\n+\n+\t  if (! reg)\n+\t    continue;\n+\t  emit_insn_before (gen_use_sfunc_addr (reg), insn);\n+\t}\n+    }\n+  mdep_reorg_phase = SH_AFTER_MDEP_REORG;\n+}\n+\n+int\n+get_dest_uid (label, max_uid)\n+     rtx label;\n+     int max_uid;\n+{\n+  rtx dest = next_real_insn (label);\n+  int dest_uid;\n+  if (! dest)\n+    /* This can happen for an undefined label.  */\n+    return 0;\n+  dest_uid = INSN_UID (dest);\n+  /* If this is a newly created branch redirection blocking instruction,\n+     we cannot index the branch_uid or insn_addresses arrays with its\n+     uid.  But then, we won't need to, because the actual destination is\n+     the following branch.  */\n+  while (dest_uid >= max_uid)\n+    {\n+      dest = NEXT_INSN (dest);\n+      dest_uid = INSN_UID (dest);\n+    }\n+  if (GET_CODE (dest) == JUMP_INSN && GET_CODE (PATTERN (dest)) == RETURN)\n+    return 0;\n+  return dest_uid;\n+}\n+\n+/* Split condbranches that are out of range.  Also add clobbers for\n+   scratch registers that are needed in far jumps.\n+   We do this before delay slot scheduling, so that it can take our\n+   newly created instructions into account.  It also allows us to\n+   find branches with common targets more easily.  */\n+\n+static void\n+split_branches (first)\n+     rtx first;\n+{\n+  rtx insn;\n+  struct far_branch **uid_branch, *far_branch_list = 0;\n+  int max_uid = get_max_uid ();\n+\n+  /* Find out which branches are out of range.  */\n+  uid_align_max = get_max_uid ();\n+  uid_align = (rtx *) alloca (uid_align_max * sizeof *uid_align);\n+  fixup_aligns ();\n+  shorten_branches (first);\n+\n+  uid_branch = (struct far_branch **) alloca (max_uid * sizeof *uid_branch);\n+  bzero ((char *) uid_branch, max_uid * sizeof *uid_branch);\n+\n+  for (insn = first; insn; insn = NEXT_INSN (insn))\n+    if (GET_RTX_CLASS (GET_CODE (insn)) != 'i')\n+      continue;\n+    else if (INSN_DELETED_P (insn))\n+      {\n+\t/* Shorten_branches would split this instruction again,\n+\t   so transform it into a note.  */\n+\tPUT_CODE (insn, NOTE);\n+\tNOTE_LINE_NUMBER (insn) = NOTE_INSN_DELETED;\n+\tNOTE_SOURCE_FILE (insn) = 0;\n+      }\n+    else if (GET_CODE (insn) == JUMP_INSN\n+\t     /* Don't mess with ADDR_DIFF_VEC */\n+\t     && (GET_CODE (PATTERN (insn)) == SET\n+\t\t || GET_CODE (PATTERN (insn)) == RETURN))\n+      {\n+\tenum attr_type type = get_attr_type (insn);\n+\tif (type == TYPE_CBRANCH)\n+\t  {\n+\t    rtx next, beyond;\n+    \n+\t    if (get_attr_length (insn) > 4)\n+\t      {\n+\t\trtx src = SET_SRC (PATTERN (insn));\n+\t\trtx cond = XEXP (src, 0);\n+\t\trtx olabel = XEXP (XEXP (src, 1), 0);\n+\t\trtx jump;\n+\t\tint addr = insn_addresses[INSN_UID (insn)];\n+\t\trtx label = 0;\n+\t\tint dest_uid = get_dest_uid (olabel, max_uid);\n+\t\tstruct far_branch *bp = uid_branch[dest_uid];\n+    \n+\t\t/* redirect_jump needs a valid JUMP_LABEL, and it might delete\n+\t\t   the label if th lABEL_BUSES count drops to zero.  There is\n+\t\t   always a jump_optimize pass that sets these values, but it\n+\t\t   proceeds to delete unreferenced code, and then if not\n+\t\t   optimizeing, to un-delete the deleted instructions, thus\n+\t\t   leaving labels with too low uses counts.  */\n+\t\tif (! optimize)\n+\t\t  {\n+\t\t    JUMP_LABEL (insn) = olabel;\n+\t\t    LABEL_NUSES (olabel)++;\n+\t\t  }\n+\t\tif (! bp)\n+\t\t  {\n+\t\t    bp = (struct far_branch *) alloca (sizeof *bp);\n+\t\t    uid_branch[dest_uid] = bp;\n+\t\t    bp->prev = far_branch_list;\n+\t\t    far_branch_list = bp;\n+\t\t    bp->far_label\n+\t\t      = XEXP (XEXP (SET_SRC (PATTERN (insn)), 1), 0);\n+\t\t    LABEL_NUSES (bp->far_label)++;\n+\t\t  }\n+\t\telse\n+\t\t  {\n+\t\t    label = bp->near_label;\n+\t\t    if (! label && bp->address - addr >= CONDJUMP_MIN)\n+\t\t      {\n+\t\t\trtx block = bp->insert_place;\n+\n+\t\t\tif (GET_CODE (PATTERN (block)) == RETURN)\n+\t\t\t  block = PREV_INSN (block);\n+\t\t\telse\n+\t\t\t  block = gen_block_redirect (block,\n+\t\t\t\t\t\t      bp->address, 2);\n+\t\t\tlabel = emit_label_after (gen_label_rtx (),\n+\t\t\t\t\t\t  PREV_INSN (block));\n+\t\t\tbp->near_label = label;\n+\t\t      }\n+\t\t    else if (label && ! NEXT_INSN (label))\n+\t\t      if (addr + 2 - bp->address <= CONDJUMP_MAX)\n+\t\t\tbp->insert_place = insn;\n+\t\t      else\n+\t\t\tgen_far_branch (bp);\n+\t\t  }\n+\t\tif (! label\n+\t\t    || NEXT_INSN (label) && bp->address - addr < CONDJUMP_MIN)\n+\t\t  {\n+\t\t    bp->near_label = label = gen_label_rtx ();\n+\t\t    bp->insert_place = insn;\n+\t\t    bp->address = addr;\n+\t\t  }\n+\t\tif (! redirect_jump (insn, label))\n+\t\t  abort ();\n+\t      }\n+\t    else\n+\t      {\n+\t\t/* get_attr_length (insn) == 2 */\n+\t\t/* Check if we have a pattern where reorg wants to redirect\n+\t\t   the branch to a label from an unconditional branch that\n+\t\t   is too far away.  */\n+\t\t/* We can't use JUMP_LABEL here because it might be undefined\n+\t\t   when not optimizing.  */\n+\t\tbeyond\n+\t\t  = next_active_insn (XEXP (XEXP (SET_SRC (PATTERN (insn)), 1),\n+\t\t\t\t\t    0));\n+\t\n+\t\tif ((GET_CODE (beyond) == JUMP_INSN\n+\t\t     || (GET_CODE (beyond = next_active_insn (beyond))\n+\t\t\t == JUMP_INSN))\n+\t\t    && GET_CODE (PATTERN (beyond)) == SET\n+\t\t    && recog_memoized (beyond) == CODE_FOR_jump\n+\t\t    && ((insn_addresses[INSN_UID (XEXP (SET_SRC (PATTERN (beyond)), 0))]\n+\t\t\t - insn_addresses[INSN_UID (insn)] + 252U)\n+\t\t\t> 252 + 258 + 2))\n+\t\t  gen_block_redirect (beyond,\n+\t\t\t\t      insn_addresses[INSN_UID (beyond)], 1);\n+\t      }\n+    \n+\t    next = next_active_insn (insn);\n+\n+\t    if ((GET_CODE (next) == JUMP_INSN\n+\t\t || GET_CODE (next = next_active_insn (next)) == JUMP_INSN)\n+\t\t&& GET_CODE (PATTERN (next)) == SET\n+\t\t&& recog_memoized (next) == CODE_FOR_jump\n+\t\t&& ((insn_addresses[INSN_UID (XEXP (SET_SRC (PATTERN (next)), 0))]\n+\t\t     - insn_addresses[INSN_UID (insn)] + 252U)\n+\t\t    > 252 + 258 + 2))\n+\t      gen_block_redirect (next, insn_addresses[INSN_UID (next)], 1);\n+\t  }\n+\telse if (type == TYPE_JUMP || type == TYPE_RETURN)\n+\t  {\n+\t    int addr = insn_addresses[INSN_UID (insn)];\n+\t    rtx far_label = 0;\n+\t    int dest_uid = 0;\n+\t    struct far_branch *bp;\n+\n+\t    if (type == TYPE_JUMP)\n+\t      {\n+\t\tfar_label = XEXP (SET_SRC (PATTERN (insn)), 0);\n+\t\tdest_uid = get_dest_uid (far_label, max_uid);\n+\t\tif (! dest_uid)\n+\t\t  {\n+\t\t    /* Parse errors can lead to labels outside\n+\t\t      the insn stream.  */\n+\t\t    if (! NEXT_INSN (far_label))\n+\t\t      continue;\n+\n+\t\t    if (! optimize)\n+\t\t      {\n+\t\t\tJUMP_LABEL (insn) = far_label;\n+\t\t\tLABEL_NUSES (far_label)++;\n+\t\t      }\n+\t\t    redirect_jump (insn, NULL_RTX);\n+\t\t    far_label = 0;\n+\t\t  }\n+\t      }\n+\t    bp = uid_branch[dest_uid];\n+\t    if (! bp)\n+\t      {\n+\t\tbp = (struct far_branch *) alloca (sizeof *bp);\n+\t\tuid_branch[dest_uid] = bp;\n+\t\tbp->prev = far_branch_list;\n+\t\tfar_branch_list = bp;\n+\t\tbp->near_label = 0;\n+\t\tbp->far_label = far_label;\n+\t\tif (far_label)\n+\t\t  LABEL_NUSES (far_label)++;\n+\t      }\n+\t    else if (bp->near_label && ! NEXT_INSN (bp->near_label))\n+\t      if (addr - bp->address <= CONDJUMP_MAX)\n+\t\temit_label_after (bp->near_label, PREV_INSN (insn));\n+\t      else\n+\t\t{\n+\t\t  gen_far_branch (bp);\n+\t\t  bp->near_label = 0;\n+\t\t}\n+\t    else\n+\t      bp->near_label = 0;\n+\t    bp->address = addr;\n+\t    bp->insert_place = insn;\n+\t    if (! far_label)\n+\t      emit_insn_before (gen_block_branch_redirect (const0_rtx), insn);\n+\t    else\n+\t      gen_block_redirect (insn, addr, bp->near_label ? 2 : 0);\n+\t  }\n+      }\n+  /* Generate all pending far branches,\n+     and free our references to the far labels.  */\n+  while (far_branch_list)\n+    {\n+      if (far_branch_list->near_label\n+\t  && ! NEXT_INSN (far_branch_list->near_label))\n+\tgen_far_branch (far_branch_list);\n+      if (optimize\n+\t  && far_branch_list->far_label\n+\t  && ! --LABEL_NUSES (far_branch_list->far_label))\n+\tdelete_insn (far_branch_list->far_label);\n+      far_branch_list = far_branch_list->prev;\n+    }\n+  uid_align_max = get_max_uid ();\n+  uid_align = (rtx *) oballoc (uid_align_max * sizeof *uid_align);\n+  fixup_aligns ();\n }\n \n /* Dump out instruction addresses, which is useful for debugging the\n@@ -2545,10 +3596,11 @@ push (rn)\n   rtx x;\n   if ((rn >= FIRST_FP_REG && rn <= LAST_FP_REG)\n       || rn == FPUL_REG)\n-    x = emit_insn (gen_push_e (gen_rtx (REG, SFmode, rn)));\n+    x = gen_push_e (gen_rtx (REG, SFmode, rn));\n   else\n-    x = emit_insn (gen_push (gen_rtx (REG, SImode, rn)));\n+    x = gen_push (gen_rtx (REG, SImode, rn));\n \n+  x = emit_insn (x);\n   REG_NOTES (x) = gen_rtx (EXPR_LIST, REG_INC,\n \t\t\t   gen_rtx(REG, SImode, STACK_POINTER_REGNUM), 0);\n }\n@@ -2562,33 +3614,38 @@ pop (rn)\n   rtx x;\n   if ((rn >= FIRST_FP_REG && rn <= LAST_FP_REG)\n       || rn == FPUL_REG)\n-    x = emit_insn (gen_pop_e (gen_rtx (REG, SFmode, rn)));\n+    x = gen_pop_e (gen_rtx (REG, SFmode, rn));\n   else\n-    x = emit_insn (gen_pop (gen_rtx (REG, SImode, rn)));\n+    x = gen_pop (gen_rtx (REG, SImode, rn));\n     \n+  x = emit_insn (x);\n   REG_NOTES (x) = gen_rtx (EXPR_LIST, REG_INC,\n \t\t\t   gen_rtx(REG, SImode, STACK_POINTER_REGNUM), 0);\n }\n \n-/* Generate code to push the regs specified in the mask, and return\n-   the number of bytes the insns take.  */\n+/* Generate code to push the regs specified in the mask.  */\n \n static void\n push_regs (mask, mask2)\n      int mask, mask2;\n {\n   int i;\n \n+  /* Push PR last; this gives better latencies after the prologue, and\n+     candidates for the return delay slot when there are no general\n+     registers pushed.  */\n   for (i = 0; i < 32; i++)\n-    if (mask & (1 << i))\n+    if (mask & (1 << i) && i != PR_REG)\n       push (i);\n   for (i = 32; i < FIRST_PSEUDO_REGISTER; i++)\n     if (mask2 & (1 << (i - 32)))\n       push (i);\n+  if (mask & (1 << PR_REG))\n+    push (PR_REG);\n }\n \n /* Work out the registers which need to be saved, both as a mask and a\n-   count.\n+   count of saved words.\n \n    If doing a pragma interrupt function, then push all regs used by the\n    function, and if we call another function (we can tell by looking at PR),\n@@ -2601,40 +3658,28 @@ calc_live_regs (count_ptr, live_regs_mask2)\n {\n   int reg;\n   int live_regs_mask = 0;\n-  int count = 0;\n+  int count;\n \n   *live_regs_mask2 = 0;\n-  for (reg = 0; reg < FIRST_PSEUDO_REGISTER; reg++)\n+  for (count = 0, reg = FIRST_PSEUDO_REGISTER - 1; reg >= 0; reg--)\n     {\n-      if (pragma_interrupt && ! pragma_trapa)\n+      if ((pragma_interrupt && ! pragma_trapa)\n+\t  ? (/* Need to save all the regs ever live.  */\n+\t     (regs_ever_live[reg]\n+\t      || (call_used_regs[reg]\n+\t\t  && (! fixed_regs[reg] || reg == MACH_REG || reg == MACL_REG)\n+\t\t  && regs_ever_live[PR_REG]))\n+\t     && reg != STACK_POINTER_REGNUM && reg != ARG_POINTER_REGNUM\n+\t     && reg != RETURN_ADDRESS_POINTER_REGNUM\n+\t     && reg != T_REG && reg != GBR_REG)\n+\t  : (/* Only push those regs which are used and need to be saved.  */\n+\t     regs_ever_live[reg] && ! call_used_regs[reg]))\n \t{\n-\t  /* Need to save all the regs ever live.  */\n-\t  if ((regs_ever_live[reg]\n-\t       || (call_used_regs[reg]\n-\t\t   && (! fixed_regs[reg] || reg == MACH_REG || reg == MACL_REG)\n-\t\t   && regs_ever_live[PR_REG]))\n-\t      && reg != STACK_POINTER_REGNUM && reg != ARG_POINTER_REGNUM\n-\t      && reg != RETURN_ADDRESS_POINTER_REGNUM\n-\t      && reg != T_REG && reg != GBR_REG)\n-\t    {\n-\t      if (reg >= 32)\n-\t\t*live_regs_mask2 |= 1 << (reg - 32);\n-\t      else\n-\t\tlive_regs_mask |= 1 << reg;\n-\t      count++;\n-\t    }\n-\t}\n-      else\n-\t{\n-\t  /* Only push those regs which are used and need to be saved.  */\n-\t  if (regs_ever_live[reg] && ! call_used_regs[reg])\n-\t    {\n-\t      if (reg >= 32)\n-\t\t*live_regs_mask2 |= 1 << (reg - 32);\n-\t      else\n-\t\tlive_regs_mask |= (1 << reg);\n-\t      count++;\n-\t    }\n+\t  if (reg >= 32)\n+\t    *live_regs_mask2 |= 1 << (reg - 32);\n+\t  else\n+\t    live_regs_mask |= 1 << reg;\n+\t  count++;\n \t}\n     }\n \n@@ -2650,7 +3695,6 @@ sh_expand_prologue ()\n   int live_regs_mask;\n   int d, i;\n   int live_regs_mask2;\n-  live_regs_mask = calc_live_regs (&d, &live_regs_mask2);\n \n   /* We have pretend args if we had an object sent partially in registers\n      and partially on the stack, e.g. a large structure.  */\n@@ -2667,7 +3711,7 @@ sh_expand_prologue ()\n \n       /* This is not used by the SH3E calling convention  */\n       if (!TARGET_SH3E)\n-        {\n+\t{\n \t  /* Push arg regs as if they'd been provided by caller in stack.  */\n \t  for (i = 0; i < NPARM_REGS(SImode); i++)\n \t    {\n@@ -2679,13 +3723,14 @@ sh_expand_prologue ()\n \t      push (rn);\n \t      extra_push += 4;\n \t    }\n-        }\n+\t}\n     }\n \n   /* If we're supposed to switch stacks at function entry, do so now.  */\n   if (sp_switch)\n     emit_insn (gen_sp_switch_1 ());\n \n+  live_regs_mask = calc_live_regs (&d, &live_regs_mask2);\n   push_regs (live_regs_mask, live_regs_mask2);\n \n   output_stack_adjust (-get_frame_size (), stack_pointer_rtx, 3);\n@@ -2701,7 +3746,6 @@ sh_expand_epilogue ()\n   int d, i;\n \n   int live_regs_mask2;\n-  live_regs_mask = calc_live_regs (&d, &live_regs_mask2);\n \n   if (frame_pointer_needed)\n     {\n@@ -2726,10 +3770,13 @@ sh_expand_epilogue ()\n \n   /* Pop all the registers.  */\n \n+  live_regs_mask = calc_live_regs (&d, &live_regs_mask2);\n+  if (live_regs_mask & (1 << PR_REG))\n+    pop (PR_REG);\n   for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n     {\n       int j = (FIRST_PSEUDO_REGISTER - 1) - i;\n-      if (j < 32 && (live_regs_mask & (1 << j)))\n+      if (j < 32 && (live_regs_mask & (1 << j)) && j != PR_REG)\n \tpop (j);\n       else if (j >= 32 && (live_regs_mask2 & (1 << (j - 32))))\n \tpop (j);\n@@ -2794,17 +3841,19 @@ sh_builtin_saveregs (arglist)\n      named args need not be saved.\n      We explicitly build a pointer to the buffer because it halves the insn\n      count when not optimizing (otherwise the pointer is built for each reg\n-     saved).  */\n+     saved).\n+     We emit the moves in reverse order so that we can use predecrement.  */\n \n   fpregs = gen_reg_rtx (Pmode);\n   emit_move_insn (fpregs, XEXP (regbuf, 0));\n-  for (regno = first_floatreg; regno < NPARM_REGS (SFmode); regno ++)\n-    emit_move_insn (gen_rtx (MEM, SFmode,\n-\t\t\t     plus_constant (fpregs,\n-\t\t\t\t\t    GET_MODE_SIZE (SFmode)\n-\t\t\t\t\t    * (regno - first_floatreg))),\n-\t\t    gen_rtx (REG, SFmode,\n-\t\t\t     BASE_ARG_REG (SFmode) + regno));\n+  emit_insn (gen_addsi3 (fpregs, fpregs,\n+\t\t\t GEN_INT (n_floatregs * UNITS_PER_WORD)));\n+    for (regno = NPARM_REGS (SFmode) - 1; regno >= first_floatreg; regno--)\n+      {\n+\temit_insn (gen_addsi3 (fpregs, fpregs, GEN_INT (- UNITS_PER_WORD)));\n+\temit_move_insn (gen_rtx (MEM, SFmode, fpregs),\n+\t\t\tgen_rtx (REG, SFmode, BASE_ARG_REG (SFmode) + regno));\n+      }\n \n   /* Return the address of the regbuf.  */\n   return XEXP (regbuf, 0);\n@@ -2821,9 +3870,11 @@ initial_elimination_offset (from, to)\n   int regs_saved;\n   int total_saved_regs_space;\n   int total_auto_space = get_frame_size ();\n+  int save_flags = target_flags;\n \n   int live_regs_mask, live_regs_mask2;\n   live_regs_mask = calc_live_regs (&regs_saved, &live_regs_mask2);\n+  target_flags = save_flags;\n \n   total_saved_regs_space = (regs_saved) * 4;\n \n@@ -2840,13 +3891,10 @@ initial_elimination_offset (from, to)\n   if (from == RETURN_ADDRESS_POINTER_REGNUM\n       && (to == FRAME_POINTER_REGNUM || to == STACK_POINTER_REGNUM))\n     {\n-      int i, n = 0;\n-      for (i = PR_REG+1; i < 32; i++)\n+      int i, n = total_saved_regs_space;\n+      for (i = PR_REG-1; i >= 0; i--)\n \tif (live_regs_mask & (1 << i))\n-\t  n += 4;\n-      for (i = 32; i < FIRST_PSEUDO_REGISTER; i++)\n-\tif (live_regs_mask2 & (1 << (i - 32)))\n-\t  n += 4;\n+\t  n -= 4;\n       return n + total_auto_space;\n     }\n \n@@ -3121,6 +4169,205 @@ fp_one_operand (op)\n   REAL_VALUE_FROM_CONST_DOUBLE (r, op);\n   return REAL_VALUES_EQUAL (r, dconst1);\n }\n+\n+int\n+braf_label_ref_operand(op, mode)\n+     rtx op;\n+     enum machine_mode mode;\n+{\n+  rtx prev;\n+\n+  if (GET_CODE (op) != LABEL_REF)\n+    return 0;\n+  prev = prev_real_insn (XEXP (op, 0));\n+  if (GET_CODE (prev) != JUMP_INSN)\n+    return 0;\n+  prev = PATTERN (prev);\n+  if (GET_CODE (prev) != PARALLEL || XVECLEN (prev, 0) != 2)\n+    return 0;\n+  prev = XVECEXP (prev, 0, 0);\n+  if (GET_CODE (prev) != SET)\n+    return 0;\n+  prev = SET_SRC (prev);\n+  if (GET_CODE (prev) != PLUS || XEXP (prev, 1) != op)\n+    return 0;\n+}\n+\f\n+/* Return the offset of a branch.  Offsets for backward branches are\n+   reported relative to the branch instruction, while offsets for forward\n+   branches are reported relative to the following instruction.  */\n+   \n+int\n+branch_offset (branch)\n+     rtx branch;\n+{\n+  rtx dest = SET_SRC (PATTERN (branch)), dest_next;\n+  int branch_uid = INSN_UID (branch);\n+  int dest_uid, dest_addr;\n+  rtx branch_align = uid_align[branch_uid];\n+\n+  if (GET_CODE (dest) == IF_THEN_ELSE)\n+    dest = XEXP (dest, 1);\n+  dest = XEXP (dest, 0);\n+  dest_uid = INSN_UID (dest);\n+  dest_addr = insn_addresses[dest_uid];\n+  if (branch_align)\n+    {\n+      /* Forward branch. */\n+      /* If branch is in a sequence, get the successor of the sequence.  */\n+      rtx next = NEXT_INSN (NEXT_INSN (PREV_INSN (branch)));\n+      int next_addr = insn_addresses[INSN_UID (next)];\n+      int diff;\n+\n+      /* If NEXT has been hoisted in a sequence further on, it address has\n+\t been clobbered in the previous pass.  However, if that is the case,\n+\t we know that it is exactly 2 bytes long (because it fits in a delay\n+\t slot), and that there is a following label (the destination of the\n+\t instruction that filled its delay slot with NEXT).  The address of\n+\t this label is reliable.  */\n+      if (NEXT_INSN (next))\n+\t{\n+\t  int next_next_addr = insn_addresses[INSN_UID (NEXT_INSN (next))];\n+\t  if (next_addr > next_next_addr)\n+\t    next_addr = next_next_addr - 2;\n+\t}\n+      diff = dest_addr - next_addr;\n+      /* If BRANCH_ALIGN has been the last insn, it might be a barrier or\n+\t a note.  */\n+      if ((insn_addresses[INSN_UID (branch_align)] < dest_addr\n+\t   || (insn_addresses[INSN_UID (branch_align)] == dest_addr\n+\t       && next_real_insn (dest) != branch_align))\n+\t  && GET_CODE (branch_align) == INSN)\n+\t{\n+\t  int align = 1 << INTVAL (XVECEXP (PATTERN (branch_align), 0, 0));\n+\t  int align_addr = insn_addresses[INSN_UID (branch_align)];\n+\t  diff += (align_addr - 1)  & (align - 2);\n+\t  branch_align = uid_align[INSN_UID (branch_align)];\n+\t  if (insn_addresses[INSN_UID (branch_align)] <= dest_addr\n+\t      && GET_CODE (branch_align) == INSN)\n+\t    {\n+\t      int align2 = 1 << INTVAL (XVECEXP (PATTERN (branch_align), 0, 0));\n+\t      align_addr = insn_addresses[INSN_UID (branch_align)];\n+\t      diff += (align_addr - 1)  & (align2 - align);\n+\t    }\n+\t}\n+      return diff;\n+    }\n+  else\n+    {\n+      /* Backward branch. */\n+      int branch_addr = insn_addresses[branch_uid];\n+      int diff = dest_addr - branch_addr;\n+      int old_align = 2;\n+\n+      while (dest_uid >= uid_align_max || ! uid_align[dest_uid])\n+\t{\n+\t  /* Label might be outside the insn stream, or even in a separate\n+\t     insn stream, after a syntax errror.  */\n+\t  if (! NEXT_INSN (dest))\n+\t    return 0;\n+\t  dest = NEXT_INSN (dest), dest_uid = INSN_UID (dest);\n+\t}\n+      \n+      /* By searching for a known destination, we might already have\n+\t stumbled on the alignment instruction.  */\n+      if (GET_CODE (dest) == INSN\n+\t  && GET_CODE (PATTERN (dest)) == UNSPEC_VOLATILE\n+\t  && XINT (PATTERN (dest), 1) == 1\n+\t  && INTVAL (XVECEXP (PATTERN (dest), 0, 0)) > 1)\n+\tbranch_align = dest;\n+      else\n+\tbranch_align = uid_align[dest_uid];\n+      while (insn_addresses[INSN_UID (branch_align)] <= branch_addr\n+\t     && GET_CODE (branch_align) == INSN)\n+\t{\n+\t  int align = 1 << INTVAL (XVECEXP (PATTERN (branch_align), 0, 0));\n+\t  int align_addr = insn_addresses[INSN_UID (branch_align)];\n+\t  diff -= (align_addr - 1)  & (align - old_align);\n+\t  old_align = align;\n+\t  branch_align = uid_align[INSN_UID (branch_align)];\n+\t}\n+      return diff;\n+    }\n+}\n+\n+int\n+short_cbranch_p (branch)\n+     rtx branch;\n+{\n+  int offset;\n+\n+  if (! insn_addresses)\n+    return 0;\n+  if (mdep_reorg_phase <= SH_FIXUP_PCLOAD)\n+    return 0;\n+  offset = branch_offset (branch);\n+  return (offset >= -252\n+\t  && offset <= (NEXT_INSN (PREV_INSN (branch)) == branch ? 256 : 254));\n+}\n+\n+/* The maximum range used for SImode constant pool entrys is 1018.  A final\n+   instruction can add 8 bytes while only being 4 bytes in size, thus we\n+   can have a total of 1022 bytes in the pool.  Add 4 bytes for a branch\n+   instruction around the pool table, 2 bytes of alignment before the table,\n+   and 30 bytes of alignment after the table.  That gives a maximum total\n+   pool size of 1058 bytes.\n+   Worst case code/pool content size ratio is 1:2 (using asms).\n+   Thus, in the worst case, there is one instruction in front of a maximum\n+   sized pool, and then there are 1052 bytes of pool for every 508 bytes of\n+   code.  For the last n bytes of code, there are 2n + 36 bytes of pool.\n+   If we have a forward branch, the initial table will be put after the\n+   unconditional branch.\n+\n+   ??? We could do much better by keeping track of the actual pcloads within\n+   the branch range and in the pcload range in front of the branch range.  */\n+\n+int\n+med_branch_p (branch, condlen)\n+     rtx branch;\n+     int condlen;\n+{\n+  int offset;\n+\n+  if (! insn_addresses)\n+    return 0;\n+  offset = branch_offset (branch);\n+  if (mdep_reorg_phase <= SH_FIXUP_PCLOAD)\n+    return offset - condlen >= -990 && offset <= 998;\n+  return offset - condlen >= -4092 && offset <= 4094;\n+}\n+\n+int\n+braf_branch_p (branch, condlen)\n+     rtx branch;\n+     int condlen;\n+{\n+  int offset;\n+\n+  if (! insn_addresses)\n+    return 0;\n+  if (! TARGET_SH2)\n+    return 0;\n+  offset = branch_offset (branch);\n+  if (mdep_reorg_phase <= SH_FIXUP_PCLOAD)\n+    return offset - condlen >= -10330 && offset <= 10330;\n+  return offset -condlen >= -32764 && offset <= 32766;\n+}\n+\n+int\n+align_length (insn)\n+     rtx insn;\n+{\n+  int align = 1 << INTVAL (XVECEXP (PATTERN (insn), 0, 0));\n+  if (! insn_addresses)\n+    if (optimize\n+\t&& (mdep_reorg_phase == SH_SHORTEN_BRANCHES0\n+\t    || mdep_reorg_phase == SH_SHORTEN_BRANCHES1))\n+      return 0;\n+    else\n+      return align - 2;\n+  return align - 2 - ((insn_addresses[INSN_UID (insn)] - 2) & (align - 2));\n+}\n \f\n /* Return non-zero if REG is not used after INSN.\n    We assume REG is a reload reg, and therefore does"}, {"sha": "ef6aef3786ed55651413bf3a5c2a7950b1a6bbb8", "filename": "gcc/config/sh/sh.h", "status": "modified", "additions": 21, "deletions": 6, "changes": 27, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1245df60c108112fedca5d76a69f19895a0f4ae4/gcc%2Fconfig%2Fsh%2Fsh.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1245df60c108112fedca5d76a69f19895a0f4ae4/gcc%2Fconfig%2Fsh%2Fsh.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fsh%2Fsh.h?ref=1245df60c108112fedca5d76a69f19895a0f4ae4", "patch": "@@ -804,9 +804,16 @@ struct sh_args {\n    This macro is only used in this file. */\n \n #define PASS_IN_REG_P(CUM, MODE, TYPE) \\\n-  (ROUND_REG ((CUM), (MODE)) < NPARM_REGS (MODE)\t\t\\\n-   && ((TYPE) == 0 || ! TREE_ADDRESSABLE ((tree)(TYPE)))\t\\\n-   && (! TARGET_SH3E || (ROUND_REG((CUM), (MODE)) + (GET_MODE_SIZE(MODE)/4) <= NPARM_REGS (MODE))))\n+  (((TYPE) == 0 || ! TREE_ADDRESSABLE ((tree)(TYPE))) \\\n+   && (TARGET_SH3E \\\n+       ? ((MODE) == BLKmode \\\n+\t  ? (((CUM).arg_count[(int) SH_ARG_INT] * UNITS_PER_WORD \\\n+\t      + int_size_in_bytes (TYPE)) \\\n+\t     <= NPARM_REGS (SImode) * UNITS_PER_WORD) \\\n+\t  : ((ROUND_REG((CUM), (MODE)) \\\n+\t      + HARD_REGNO_NREGS (BASE_ARG_REG (MODE), (MODE))) \\\n+\t     <= NPARM_REGS (MODE))) \\\n+       : ROUND_REG ((CUM), (MODE)) < NPARM_REGS (MODE)))\n \n /* Define where to put the arguments to a function.\n    Value is zero to push the argument on the stack,\n@@ -1755,9 +1762,14 @@ sh_valid_machine_decl_attribute (DECL, ATTRIBUTES, IDENTIFIER, ARGS)\n     (LENGTH) = align_length (X);\t\t\t\t\\\n   if (GET_CODE (X) == JUMP_INSN\t\t\t\t\t\\\n       && GET_CODE (PATTERN (X)) == ADDR_DIFF_VEC)\t\t\\\n-    /* The code before an ADDR_DIFF_VEC is even aligned, thus\t\\\n-       any odd estimate is wrong.  */\t\t\t\t\\\n-    (LENGTH) &= ~1;\n+    {\t\t\t\t\t\t\t\t\\\n+      /* The code before an ADDR_DIFF_VEC is even aligned,\t\\\n+\t thus any odd estimate is wrong.  */\t\t\t\\\n+      (LENGTH) &= ~1;\t\t\t\t\t\t\\\n+      /* If not optimizing, the alignment is implicit.  */\t\\\n+      if (! optimize)\t\t\t\t\t\t\\\n+\t(LENGTH) += 2;\t\t\t\t\t\t\\\n+    }\n \n /* Enable a bug fix for the shorten_branches pass.  */\n #define SHORTEN_WITH_ADJUST_INSN_LENGTH\n@@ -1859,4 +1871,7 @@ do {\t\t\t\t\t\t\t\t\t\\\n /* For the sake of libgcc2.c, indicate target supports atexit.  */\n #define HAVE_ATEXIT\n \n+/* Enable the register move pass to improve code.  */\n+#define ENABLE_REGMOVE_PASS\n+\n #define SH_DYNAMIC_SHIFT_COST (TARGET_SH3 ? (TARGET_SMALLCODE ? 1 : 2) : 20)"}, {"sha": "1faad9324268470c6975901bfe451a7e4b234caf", "filename": "gcc/config/sh/sh.md", "status": "modified", "additions": 802, "deletions": 315, "changes": 1117, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1245df60c108112fedca5d76a69f19895a0f4ae4/gcc%2Fconfig%2Fsh%2Fsh.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1245df60c108112fedca5d76a69f19895a0f4ae4/gcc%2Fconfig%2Fsh%2Fsh.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fsh%2Fsh.md?ref=1245df60c108112fedca5d76a69f19895a0f4ae4", "patch": "@@ -69,67 +69,86 @@\n \n ;; Target CPU.\n \n-(define_attr \"cpu\" \"sh0,sh1,sh2,sh3,sh3e\"\n+(define_attr \"cpu\"\n+ \"sh1,sh2,sh3,sh3e\"\n   (const (symbol_ref \"sh_cpu_attr\")))\n \n+(define_attr \"endian\" \"big,little\"\n+ (const (if_then_else (symbol_ref \"TARGET_LITTLE_ENDIAN\")\n+\t\t      (const_string \"little\") (const_string \"big\"))))\n+\n ;; cbranch\tconditional branch instructions\n ;; jump\t\tunconditional jumps\n ;; arith\tordinary arithmetic\n+;; arith3\ta compound insn that behaves similarily to a sequence of\n+;;\t\tthree insns of type arith\n+;; arith3b\tlike above, but might end with a redirected branch\n ;; load\t\tfrom memory\n+;; load_si\tLikewise, SImode variant for general register.\n ;; store\tto memory\n ;; move\t\tregister to register\n+;; fmove\tregister to register, floating point\n ;; smpy\t\tword precision integer multiply\n ;; dmpy\t\tlongword or doublelongword precision integer multiply\n ;; return\trts\n ;; pload\tload of pr reg, which can't be put into delay slot of rts\n ;; pstore\tstore of pr reg, which can't be put into delay slot of jsr\n ;; pcload\tpc relative load of constant value\n+;; pcload_si\tLikewise, SImode variant for general register.\n ;; rte\t\treturn from exception\n ;; sfunc\tspecial function call with known used registers\n ;; call\t\tfunction call\n ;; fp\t\tfloating point\n ;; fdiv\t\tfloating point divide (or square root)\n+;; gp_fpul\tmove between general purpose register and fpul\n \n (define_attr \"type\"\n- \"cbranch,jump,arith,other,load,store,move,smpy,dmpy,return,pload,pstore,pcload,rte,sfunc,call,fp,fdiv\"\n+ \"cbranch,jump,jump_ind,arith,arith3,arith3b,dyn_shift,other,load,load_si,store,move,fmove,smpy,dmpy,return,pload,pstore,pcload,pcload_si,rte,sfunc,call,fp,fdiv,gp_fpul\"\n   (const_string \"other\"))\n \n ; If a conditional branch destination is within -252..258 bytes away\n ; from the instruction it can be 2 bytes long.  Something in the\n ; range -4090..4100 bytes can be 6 bytes long.  All other conditional\n-; branches are 16 bytes long.\n+; branches are initially assumed to be 16 bytes long.\n+; In machine_dependent_reorg, we split all branches that are longer than\n+; 2 bytes.\n \n ; An unconditional jump in the range -4092..4098 can be 2 bytes long.\n-; Otherwise, it must be 14 bytes long.\n+; For wider ranges, we need a combination of a code and a data part.\n+; If we can get a scratch register for a long range jump, the code\n+; part can be 4 bytes long; otherwise, it must be 8 bytes long.\n+; If the jump is in the range -32764..32770, the data part can be 2 bytes\n+; long; otherwise, it must be 6 bytes long.\n \n ; All other instructions are two bytes long by default.\n \n-; All positive offsets have an adjustment added, which is the number of bytes\n-; difference between this instruction length and the next larger instruction\n-; length.  This is because shorten_branches starts with the largest\n-; instruction size and then tries to reduce them.\n-\n (define_attr \"length\" \"\"\n   (cond [(eq_attr \"type\" \"cbranch\")\n-\t (if_then_else (and (ge (minus (match_dup 0) (pc))\n-\t\t\t\t(const_int -252))\n-\t\t\t    (le (minus (match_dup 0) (pc))\n-\t\t\t\t(const_int 262)))\n-\t\t       (const_int 2)\n-\t\t       (if_then_else (and (ge (minus (match_dup 0) (pc))\n-\t\t\t\t\t      (const_int -4090))\n-\t\t\t\t\t  (le (minus (match_dup 0) (pc))\n-\t\t\t\t\t      (const_int 4110)))\n-\t\t\t\t     (const_int 6)\n-\t\t\t\t     (const_int 16)))\n-\n+\t (cond [(ne (symbol_ref \"short_cbranch_p (insn)\") (const_int 0))\n+\t\t(const_int 2)\n+\t\t(ne (symbol_ref \"med_branch_p (insn, 2)\") (const_int 0))\n+\t\t(const_int 6)\n+\t\t(ne (symbol_ref \"braf_branch_p (insn, 2)\") (const_int 0))\n+\t\t(const_int 10)\n+\t\t(ne (pc) (pc))\n+\t\t(const_int 12)\n+\t\t] (const_int 16))\n \t (eq_attr \"type\" \"jump\")\n-\t (if_then_else (and (ge (minus (match_dup 0) (pc))\n-\t\t\t\t(const_int -4092))\n-\t\t\t    (le (minus (match_dup 0) (pc))\n-\t\t\t\t(const_int 4110)))\n-\t\t       (const_int 2)\n-\t\t       (const_int 14))\n+\t (cond [(ne (symbol_ref \"med_branch_p (insn, 0)\") (const_int 0))\n+\t\t(const_int 2)\n+\t\t(and (eq (symbol_ref \"GET_CODE (PREV_INSN (insn))\")\n+\t\t\t (symbol_ref \"INSN\"))\n+\t\t     (eq (symbol_ref \"INSN_CODE (PREV_INSN (insn))\")\n+\t\t\t (symbol_ref \"code_for_indirect_jump_scratch\")))\n+\t\t(if_then_else (ne (symbol_ref \"braf_branch_p (insn, 0)\")\n+\t\t\t\t  (const_int 0))\n+\t\t\t      (const_int 6)\n+\t\t\t      (const_int 10))\n+\t\t(ne (symbol_ref \"braf_branch_p (insn, 0)\") (const_int 0))\n+\t\t(const_int 10)\n+\t\t(ne (pc) (pc))\n+\t\t(const_int 12)\n+\t\t] (const_int 14))\n \t ] (const_int 2)))\n \n ;; (define_function_unit {name} {num-units} {n-users} {test}\n@@ -140,16 +159,41 @@\n ;; gcc to separate load and store instructions by one instruction,\n ;; which makes it more likely that the linker will be able to word\n ;; align them when relaxing.\n+\n+;; Loads have a latency of two.\n+;; However, call insn can have ;; a delay slot, so that we want one more\n+;; insn to be scheduled between the load of the function address and the call.\n+;; This is equivalent to a latency of three.\n+;; We cannot use a conflict list for this, because we need to distinguish\n+;; between the actual call address and the function arguments.\n+;; ADJUST_COST can only properly handle reductions of the cost, so we\n+;; use a latency of three here.\n+;; We only do this for SImode loads of general regsiters, to make the work\n+;; for ADJUST_COST easier.\n+(define_function_unit \"memory\" 1 0\n+  (eq_attr \"type\" \"load_si,pcload_si\")\n+  3 2)\n (define_function_unit \"memory\" 1 0\n-  (eq_attr \"type\" \"load,pcload,pload,store,pstore\") 2 2)\n+  (eq_attr \"type\" \"load,pcload,pload,store,pstore\")\n+  2 2)\n+\n+(define_function_unit \"int\"    1 0\n+  (eq_attr \"type\" \"arith3,arith3b\") 3 3)\n+\n+(define_function_unit \"int\"    1 0\n+  (eq_attr \"type\" \"dyn_shift\") 2 2)\n+\n+(define_function_unit \"int\"    1 0\n+  (eq_attr \"type\" \"arith,arith3b,dyn_shift\") 2 2)\n \n ;; ??? These are approximations.\n (define_function_unit \"mpy\"    1 0 (eq_attr \"type\" \"smpy\") 2 2)\n (define_function_unit \"mpy\"    1 0 (eq_attr \"type\" \"dmpy\") 3 3)\n \n-(define_function_unit \"fp\"     1 0 (eq_attr \"type\" \"fp\") 2 1)\n+(define_function_unit \"fp\"     1 0 (eq_attr \"type\" \"fp,fmove\") 2 1)\n (define_function_unit \"fp\"     1 0 (eq_attr \"type\" \"fdiv\") 13 12)\n \n+\n ; Definitions for filling branch delay slots.\n \n (define_attr \"needs_delay_slot\" \"yes,no\" (const_string \"no\"))\n@@ -161,7 +205,7 @@\n \n (define_attr \"in_delay_slot\" \"yes,no\"\n   (cond [(eq_attr \"type\" \"cbranch\") (const_string \"no\")\n-\t (eq_attr \"type\" \"pcload\") (const_string \"no\")\n+\t (eq_attr \"type\" \"pcload,pcload_si\") (const_string \"no\")\n \t (eq_attr \"needs_delay_slot\" \"yes\") (const_string \"no\")\n \t (eq_attr \"length\" \"2\") (const_string \"yes\")\n \t ] (const_string \"no\")))\n@@ -197,27 +241,15 @@\n ;; Say that we have annulled true branches, since this gives smaller and\n ;; faster code when branches are predicted as not taken.\n \n-;; ??? Branches which are out-of-range actually have two delay slots,\n-;; the first is either always executed or else annulled false, and the\n-;; second is always annulled false.  Handling these differently from\n-;; in range branches would give better code.\n-\n (define_delay\n   (and (eq_attr \"type\" \"cbranch\")\n-       (eq_attr \"cpu\" \"sh2,sh3\"))\n+       (ne (symbol_ref \"TARGET_SH2\") (const_int 0)))\n   [(eq_attr \"in_delay_slot\" \"yes\") (eq_attr \"in_delay_slot\" \"yes\") (nil)])\n \f\n ;; -------------------------------------------------------------------------\n ;; SImode signed integer comparisons\n ;; -------------------------------------------------------------------------\n \n-(define_insn \"\"\n-  [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n-\t(eq:SI (reg:SI 18)\n-\t       (const_int 1)))]\n-  \"\"\n-  \"movt\t%0\")\n-\n (define_insn \"\"\n   [(set (reg:SI 18)\n \t(eq:SI (and:SI (match_operand:SI 0 \"arith_reg_operand\" \"z,r\")\n@@ -288,6 +320,93 @@\n }\")\n \f\n ;; -------------------------------------------------------------------------\n+;; DImode signed integer comparisons\n+;; -------------------------------------------------------------------------\n+\n+;; ??? Could get better scheduling by splitting the initial test from the\n+;; rest of the insn after reload.  However, the gain would hardly justify\n+;; the sh.md size increase necessary to do that.\n+\n+(define_insn \"\"\n+  [(set (reg:SI 18)\n+\t(eq:SI (and:DI (match_operand:DI 0 \"arith_reg_operand\" \"r\")\n+\t\t       (match_operand:DI 1 \"arith_operand\" \"r\"))\n+\t       (const_int 0)))]\n+  \"\"\n+  \"* return output_branchy_insn (EQ, \\\"tst\\\\t%S1,%S0\\;bf\\\\t%l9\\;tst\\\\t%R1,%R0\\\",\n+\t\t\t\t insn, operands);\"\n+  [(set_attr \"length\" \"6\")\n+   (set_attr \"type\" \"arith3b\")])\n+\n+(define_insn \"cmpeqdi_t\"\n+  [(set (reg:SI 18) (eq:SI (match_operand:DI 0 \"arith_reg_operand\" \"r,r\")\n+\t\t\t   (match_operand:DI 1 \"arith_reg_or_0_operand\" \"N,r\")))]\n+  \"\"\n+  \"*\n+  return output_branchy_insn\n+   (EQ,\n+    (which_alternative\n+     ? \\\"cmp/eq\\\\t%S1,%S0\\;bf\\\\t%l9\\;cmp/eq\\\\t%R1,%R0\\\"\n+     : \\\"tst\\\\t%S0,%S0\\;bf\\\\t%l9\\;tst\\\\t%R0,%R0\\\"),\n+    insn, operands);\"\n+  [(set_attr \"length\" \"6\")\n+   (set_attr \"type\" \"arith3b\")])\n+\n+(define_insn \"cmpgtdi_t\"\n+  [(set (reg:SI 18) (gt:SI (match_operand:DI 0 \"arith_reg_operand\" \"r,r\")\n+\t\t\t   (match_operand:DI 1 \"arith_reg_or_0_operand\" \"r,N\")))]\n+  \"TARGET_SH2\"\n+  \"@\n+\tcmp/eq\\\\t%S1,%S0\\;bf{.|/}s\\\\t%,Ldi%=\\;cmp/gt\\\\t%S1,%S0\\;cmp/hi\\\\t%R1,%R0\\\\n%,Ldi%=:\n+\ttst\\\\t%S0,%S0\\;bf{.|/}s\\\\t%,Ldi%=\\;cmp/pl\\\\t%S0\\;cmp/hi\\\\t%S0,%R0\\\\n%,Ldi%=:\"\n+  [(set_attr \"length\" \"8\")\n+   (set_attr \"type\" \"arith3\")])\n+\n+(define_insn \"cmpgedi_t\"\n+  [(set (reg:SI 18) (ge:SI (match_operand:DI 0 \"arith_reg_operand\" \"r,r\")\n+\t\t\t   (match_operand:DI 1 \"arith_reg_or_0_operand\" \"r,N\")))]\n+  \"TARGET_SH2\"\n+  \"@\n+\tcmp/eq\\\\t%S1,%S0\\;bf{.|/}s\\\\t%,Ldi%=\\;cmp/ge\\\\t%S1,%S0\\;cmp/hs\\\\t%R1,%R0\\\\n%,Ldi%=:\n+\tcmp/pz\\\\t%S0\"\n+  [(set_attr \"length\" \"8,2\")\n+   (set_attr \"type\" \"arith3,arith\")])\n+\f\n+;; -------------------------------------------------------------------------\n+;; DImode unsigned integer comparisons\n+;; -------------------------------------------------------------------------\n+\n+(define_insn \"cmpgeudi_t\"\n+  [(set (reg:SI 18) (geu:SI (match_operand:DI 0 \"arith_reg_operand\" \"r\")\n+\t\t\t    (match_operand:DI 1 \"arith_reg_operand\" \"r\")))]\n+  \"TARGET_SH2\"\n+  \"cmp/eq\\\\t%S1,%S0\\;bf{.|/}s\\\\t%,Ldi%=\\;cmp/hs\\\\t%S1,%S0\\;cmp/hs\\\\t%R1,%R0\\\\n%,Ldi%=:\"\n+  [(set_attr \"length\" \"8\")\n+   (set_attr \"type\" \"arith3\")])\n+\n+(define_insn \"cmpgtudi_t\"\n+  [(set (reg:SI 18) (gtu:SI (match_operand:DI 0 \"arith_reg_operand\" \"r\")\n+\t\t\t    (match_operand:DI 1 \"arith_reg_operand\" \"r\")))]\n+  \"TARGET_SH2\"\n+  \"cmp/eq\\\\t%S1,%S0\\;bf{.|/}s\\\\t%,Ldi%=\\;cmp/hi\\\\t%S1,%S0\\;cmp/hi\\\\t%R1,%R0\\\\n%,Ldi%=:\"\n+  [(set_attr \"length\" \"8\")\n+   (set_attr \"type\" \"arith3\")])\n+\n+;; We save the compare operands in the cmpxx patterns and use them when\n+;; we generate the branch.\n+\n+(define_expand \"cmpdi\"\n+  [(set (reg:SI 18) (compare (match_operand:DI 0 \"arith_operand\" \"\")\n+\t\t\t     (match_operand:DI 1 \"arith_operand\" \"\")))]\n+  \"TARGET_SH2\"\n+  \"\n+{\n+  sh_compare_op0 = operands[0];\n+  sh_compare_op1 = operands[1];\n+  DONE;\n+}\")\n+\f\n+;; -------------------------------------------------------------------------\n ;; Addition instructions\n ;; -------------------------------------------------------------------------\n \n@@ -299,9 +418,50 @@\n \t\t (match_operand:DI 2 \"arith_reg_operand\" \"r\")))\n    (clobber (reg:SI 18))]\n   \"\"\n-  \"clrt\\;addc\t%R2,%R0\\;addc\t%S2,%S0\"\n+  \"#\"\n   [(set_attr \"length\" \"6\")])\n \n+(define_split\n+  [(set (match_operand:DI 0 \"arith_reg_operand\" \"=r\")\n+\t(plus:DI (match_operand:DI 1 \"arith_reg_operand\" \"%0\")\n+\t\t (match_operand:DI 2 \"arith_reg_operand\" \"r\")))\n+   (clobber (reg:SI 18))]\n+  \"reload_completed\"\n+  [(const_int 0)]\n+  \"\n+{\n+  rtx high0, high2, low0 = gen_lowpart (SImode, operands[0]);\n+  high0 = gen_rtx (REG, SImode,\n+\t\t   true_regnum (operands[0]) + (TARGET_LITTLE_ENDIAN ? 1 : 0));\n+  high2 = gen_rtx (REG, SImode,\n+\t\t   true_regnum (operands[2]) + (TARGET_LITTLE_ENDIAN ? 1 : 0));\n+  emit_insn (gen_clrt ());\n+  emit_insn (gen_addc (low0, low0, gen_lowpart (SImode, operands[2])));\n+  emit_insn (gen_addc1 (high0, high0, high2));\n+  DONE;\n+}\")\n+\n+(define_insn \"addc\"\n+  [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n+\t(plus:SI (plus:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n+\t\t\t  (match_operand:SI 2 \"arith_reg_operand\" \"r\"))\n+\t\t (reg:SI 18)))\n+   (set (reg:SI 18)\n+\t(ltu:SI (plus:SI (match_dup 1) (match_dup 2)) (match_dup 1)))]\n+  \"\"\n+  \"addc\t%2,%0\"\n+  [(set_attr \"type\" \"arith\")])\n+\n+(define_insn \"addc1\"\n+  [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n+\t(plus:SI (plus:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n+\t\t\t  (match_operand:SI 2 \"arith_reg_operand\" \"r\"))\n+\t\t (reg:SI 18)))\n+   (clobber (reg:SI 18))]\n+  \"\"\n+  \"addc\t%2,%0\"\n+  [(set_attr \"type\" \"arith\")])\n+\n (define_insn \"addsi3\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n \t(plus:SI (match_operand:SI 1 \"arith_operand\" \"%0\")\n@@ -322,9 +482,50 @@\n \t\t (match_operand:DI 2 \"arith_reg_operand\" \"r\")))\n    (clobber (reg:SI 18))]\n   \"\"\n-  \"clrt\\;subc\t%R2,%R0\\;subc\t%S2,%S0\"\n+  \"#\"\n   [(set_attr \"length\" \"6\")])\n \n+(define_split\n+  [(set (match_operand:DI 0 \"arith_reg_operand\" \"=r\")\n+\t(minus:DI (match_operand:DI 1 \"arith_reg_operand\" \"0\")\n+\t\t  (match_operand:DI 2 \"arith_reg_operand\" \"r\")))\n+   (clobber (reg:SI 18))]\n+  \"reload_completed\"\n+  [(const_int 0)]\n+  \"\n+{\n+  rtx high0, high2, low0 = gen_lowpart (SImode, operands[0]);\n+  high0 = gen_rtx (REG, SImode,\n+\t\t   true_regnum (operands[0]) + (TARGET_LITTLE_ENDIAN ? 1 : 0));\n+  high2 = gen_rtx (REG, SImode,\n+\t\t   true_regnum (operands[2]) + (TARGET_LITTLE_ENDIAN ? 1 : 0));\n+  emit_insn (gen_clrt ());\n+  emit_insn (gen_subc (low0, low0, gen_lowpart (SImode, operands[2])));\n+  emit_insn (gen_subc1 (high0, high0, high2));\n+  DONE;\n+}\")\n+\n+(define_insn \"subc\"\n+  [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n+\t(minus:SI (minus:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n+\t\t\t    (match_operand:SI 2 \"arith_reg_operand\" \"r\"))\n+\t\t  (reg:SI 18)))\n+   (set (reg:SI 18)\n+\t(gtu:SI (minus:SI (match_dup 1) (match_dup 2)) (match_dup 1)))]\n+  \"\"\n+  \"subc\t%2,%0\"\n+  [(set_attr \"type\" \"arith\")])\n+\n+(define_insn \"subc1\"\n+  [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n+\t(minus:SI (minus:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n+\t\t\t    (match_operand:SI 2 \"arith_reg_operand\" \"r\"))\n+\t\t  (reg:SI 18)))\n+   (clobber (reg:SI 18))]\n+  \"\"\n+  \"subc\t%2,%0\"\n+  [(set_attr \"type\" \"arith\")])\n+\n (define_insn \"*subsi3_internal\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n \t(minus:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n@@ -360,6 +561,17 @@\n ;; We take advantage of the library routines which don't clobber as many\n ;; registers as a normal function call would.\n \n+;; The INSN_REFERENCES_ARE_DELAYED in sh.h is problematic because it\n+;; also has an effect on the register that holds the addres of the sfunc.\n+;; To make this work, we have an extra dummy insns that shows the use\n+;; of this register for reorg.\n+\n+(define_insn \"use_sfunc_addr\"\n+  [(set (reg:SI 17) (unspec [(match_operand:SI 0 \"register_operand\" \"r\")] 5))]\n+  \"\"\n+  \"\"\n+  [(set_attr \"length\" \"0\")])\n+\n ;; We must use a pseudo-reg forced to reg 0 in the SET_DEST rather than\n ;; hard register 0.  If we used hard register 0, then the next instruction\n ;; would be a move from hard register 0 to a pseudo-reg.  If the pseudo-reg\n@@ -369,14 +581,14 @@\n ;; If we let reload allocate r0, then this problem can never happen.\n \n (define_insn \"\"\n-  [(set (match_operand:SI 1 \"register_operand\" \"=z\")\n+  [(set (match_operand:SI 0 \"register_operand\" \"=z\")\n \t(udiv:SI (reg:SI 4) (reg:SI 5)))\n    (clobber (reg:SI 18))\n    (clobber (reg:SI 17))\n    (clobber (reg:SI 4))\n-   (use (match_operand:SI 0 \"arith_reg_operand\" \"r\"))]\n+   (use (match_operand:SI 1 \"arith_reg_operand\" \"r\"))]\n   \"\"\n-  \"jsr\t@%0%#\"\n+  \"jsr\t@%1%#\"\n   [(set_attr \"type\" \"sfunc\")\n    (set_attr \"needs_delay_slot\" \"yes\")])\n \n@@ -395,16 +607,16 @@\n   \"operands[3] = gen_reg_rtx(SImode);\")\n \n (define_insn \"\"\n-  [(set (match_operand:SI 1 \"register_operand\" \"=z\")\n+  [(set (match_operand:SI 0 \"register_operand\" \"=z\")\n \t(div:SI (reg:SI 4) (reg:SI 5)))\n    (clobber (reg:SI 18))\n    (clobber (reg:SI 17))\n    (clobber (reg:SI 1))\n    (clobber (reg:SI 2))\n    (clobber (reg:SI 3))\n-   (use (match_operand:SI 0 \"arith_reg_operand\" \"r\"))]\n+   (use (match_operand:SI 1 \"arith_reg_operand\" \"r\"))]\n   \"\"\n-  \"jsr\t@%0%#\"\n+  \"jsr\t@%1%#\"\n   [(set_attr \"type\" \"sfunc\")\n    (set_attr \"needs_delay_slot\" \"yes\")])\n \n@@ -676,7 +888,8 @@\n \t(ior:SI (match_operand:SI 1 \"arith_reg_operand\" \"%0,0\")\n \t\t(match_operand:SI 2 \"logical_operand\" \"r,L\")))]\n   \"\"\n-  \"or\t%2,%0\")\n+  \"or\t%2,%0\"\n+  [(set_attr \"type\" \"arith\")])\n \n (define_insn \"xorsi3\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"=z,r\")\n@@ -697,22 +910,25 @@\n    (set (reg:SI 18)\n \t(lshiftrt:SI (match_dup 1) (const_int 31)))]\n   \"\"\n-  \"rotl\t%0\")\n+  \"rotl\t%0\"\n+  [(set_attr \"type\" \"arith\")])\n \n (define_insn \"rotlsi3_31\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n \t(rotate:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n \t\t   (const_int 31)))\n    (clobber (reg:SI 18))]\n   \"\"\n-  \"rotr\t%0\")\n+  \"rotr\t%0\"\n+  [(set_attr \"type\" \"arith\")])\n \n-(define_insn \"\"\n+(define_insn \"rotlsi3_16\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n \t(rotate:SI (match_operand:SI 1 \"arith_reg_operand\" \"r\")\n \t\t   (const_int 16)))]\n   \"\"\n-  \"swap.w\t%1,%0\")\n+  \"swap.w\t%1,%0\"\n+  [(set_attr \"type\" \"arith\")])\n \n (define_expand \"rotlsi3\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"\")\n@@ -721,29 +937,62 @@\n   \"\"\n   \"\n {\n+  static char rot_tab[] = {\n+    000, 000, 000, 000, 000, 000, 010, 001,\n+    001, 001, 011, 013, 003, 003, 003, 003,\n+    003, 003, 003, 003, 003, 013, 012, 002,\n+    002, 002, 010, 000, 000, 000, 000, 000,\n+  };\n+\n+  int count, choice;\n+\n   if (GET_CODE (operands[2]) != CONST_INT)\n     FAIL;\n-\n-  if (INTVAL (operands[2]) == 1)\n-    {\n-      emit_insn (gen_rotlsi3_1 (operands[0], operands[1]));\n-      DONE;\n-    }\n-  else if (INTVAL (operands[2]) == 31)\n+  count = INTVAL (operands[2]);\n+  choice = rot_tab[count];\n+  if (choice & 010 && SH_DYNAMIC_SHIFT_COST <= 1)\n+    FAIL;\n+  choice &= 7;\n+  switch (choice)\n     {\n-      emit_insn (gen_rotlsi3_31 (operands[0], operands[1]));\n-      DONE;\n+    case 0:\n+      emit_move_insn (operands[0], operands[1]);\n+      count -= (count & 16) * 2;\n+      break;\n+    case 3:\n+     emit_insn (gen_rotlsi3_16 (operands[0], operands[1]));\n+     count -= 16;\n+     break;\n+    case 1:\n+    case 2:\n+      {\n+\trtx parts[2];\n+\tparts[0] = gen_reg_rtx (SImode);\n+\tparts[1] = gen_reg_rtx (SImode);\n+\temit_insn (gen_rotlsi3_16 (parts[2-choice], operands[1]));\n+\tparts[choice-1] = operands[1];\n+\temit_insn (gen_ashlsi3 (parts[0], parts[0], GEN_INT (8)));\n+\temit_insn (gen_lshrsi3 (parts[1], parts[1], GEN_INT (8)));\n+\temit_insn (gen_iorsi3 (operands[0], parts[0], parts[1]));\n+\tcount = (count & ~16) - 8;\n+      }\n     }\n-  else if (INTVAL (operands[2]) != 16)\n-    FAIL;\n+\n+  for (; count > 0; count--)\n+    emit_insn (gen_rotlsi3_1 (operands[0], operands[0]));\n+  for (; count < 0; count++)\n+    emit_insn (gen_rotlsi3_31 (operands[0], operands[0]));\n+\n+  DONE;\n }\")\n \n-(define_insn \"\"\n+(define_insn \"*rotlhi3_8\"\n   [(set (match_operand:HI 0 \"arith_reg_operand\" \"=r\")\n \t(rotate:HI (match_operand:HI 1 \"arith_reg_operand\" \"r\")\n \t\t   (const_int 8)))]\n   \"\"\n-  \"swap.b\t%1,%0\")\n+  \"swap.b\t%1,%0\"\n+  [(set_attr \"type\" \"arith\")])\n \n (define_expand \"rotlhi3\"\n   [(set (match_operand:HI 0 \"arith_reg_operand\" \"\")\n@@ -764,7 +1013,8 @@\n \t(ashift:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n \t\t   (match_operand:SI 2 \"arith_reg_operand\" \"r\")))]\n   \"TARGET_SH3\"\n-  \"shld\t%2,%0\")\n+  \"shld\t%2,%0\"\n+  [(set_attr \"type\" \"dyn_shift\")])\n \n (define_insn \"ashlsi3_k\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r,r\")\n@@ -773,7 +1023,8 @@\n   \"CONST_OK_FOR_K (INTVAL (operands[2]))\"\n   \"@\n \tadd\t%0,%0\n-\tshll%O2\t%0\")\n+\tshll%O2\t%0\"\n+  [(set_attr \"type\" \"arith\")])\n \n (define_insn \"ashlhi3_k\"\n   [(set (match_operand:HI 0 \"arith_reg_operand\" \"=r,r\")\n@@ -782,14 +1033,15 @@\n   \"CONST_OK_FOR_K (INTVAL (operands[2]))\"\n   \"@\n \tadd\t%0,%0\n-\tshll%O2\t%0\")\n+\tshll%O2\t%0\"\n+  [(set_attr \"type\" \"arith\")])\n \n (define_insn \"ashlsi3_n\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n \t(ashift:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n \t\t   (match_operand:SI 2 \"const_int_operand\" \"n\")))\n    (clobber (reg:SI 18))]\n-  \"\"\n+  \"! sh_dynamicalize_shift_p (operands[2])\"\n   \"#\"\n   [(set (attr \"length\")\n \t(cond [(eq (symbol_ref \"shift_insns_rtx (insn)\") (const_int 1))\n@@ -822,6 +1074,9 @@\n   \"\"\n   \"\n {\n+  if (GET_CODE (operands[2]) == CONST_INT\n+      && sh_dynamicalize_shift_p (operands[2]))\n+    operands[2] = force_reg (SImode, operands[2]);\n   if (TARGET_SH3 && arith_reg_operand (operands[2], GET_MODE (operands[2])))\n     {\n       emit_insn (gen_ashlsi3_d (operands[0], operands[1], operands[2]));\n@@ -888,27 +1143,59 @@\n         (ashiftrt:SI (match_operand:SI 1 \"arith_reg_operand\" \"r\")\n                      (const_int 16)))]\n   \"\"\n-  \"swap.w\t%1,%0\\;exts.w\t%0,%0\"\n+  \"#\"\n   [(set_attr \"length\" \"4\")])\n \n+(define_split\n+  [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n+        (ashiftrt:SI (match_operand:SI 1 \"arith_reg_operand\" \"r\")\n+\t\t     (const_int 16)))]\n+  \"\"\n+  [(set (match_dup 0) (rotate:SI (match_dup 1) (const_int 16)))\n+   (set (match_dup 0) (sign_extend:SI (match_dup 2)))]\n+  \"operands[2] = gen_lowpart (HImode, operands[0]);\")\n+\n ;; ??? This should be a define expand.\n \n (define_insn \"ashrsi2_31\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n-        (ashiftrt:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n-                     (const_int 31)))\n+\t(ashiftrt:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n+\t\t     (const_int 31)))\n    (clobber (reg:SI 18))]\n   \"\"\n-  \"@\n-   shll\t%0\\;subc\t%0,%0\"\n+  \"#\"\n   [(set_attr \"length\" \"4\")])\n \n+(define_split\n+  [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n+\t(ashiftrt:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n+\t\t     (const_int 31)))\n+   (clobber (reg:SI 18))]\n+  \"\"\n+  [(const_int 0)]\n+  \"\n+{\n+  emit_insn (gen_ashlsi_c (operands[0], operands[1], operands[1]));\n+  emit_insn (gen_subc1 (operands[0], operands[0], operands[0]));\n+  DONE;\n+}\")\n+\n+(define_insn \"ashlsi_c\"\n+  [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n+\t(ashift:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\") (const_int 1)))\n+   (set (reg:SI 18) (lt:SI (match_operand:SI 2 \"arith_reg_operand\" \"0\")\n+\t\t\t   (const_int 0)))]\n+  \"\"\n+  \"shll\t%0\"\n+  [(set_attr \"type\" \"arith\")])\n+\n (define_insn \"ashrsi3_d\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n \t(ashiftrt:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n \t\t     (neg:SI (match_operand:SI 2 \"arith_reg_operand\" \"r\"))))]\n   \"TARGET_SH3\"\n-  \"shad\t%2,%0\")\n+  \"shad\t%2,%0\"\n+  [(set_attr \"type\" \"dyn_shift\")])\n \n (define_insn \"ashrsi3_n\"\n   [(set (reg:SI 4)\n@@ -937,7 +1224,8 @@\n \t(lshiftrt:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n \t\t     (neg:SI (match_operand:SI 2 \"arith_reg_operand\" \"r\"))))]\n   \"TARGET_SH3\"\n-  \"shld\t%2,%0\")\n+  \"shld\t%2,%0\"\n+  [(set_attr \"type\" \"dyn_shift\")])\n \n ;;  Only the single bit shift clobbers the T bit.\n \n@@ -947,38 +1235,42 @@\n \t\t     (match_operand:SI 2 \"const_int_operand\" \"M\")))\n    (clobber (reg:SI 18))]\n   \"CONST_OK_FOR_M (INTVAL (operands[2]))\"\n-  \"shlr\t%0\")\n+  \"shlr\t%0\"\n+  [(set_attr \"type\" \"arith\")])\n \n (define_insn \"lshrsi3_k\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n \t(lshiftrt:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n \t\t     (match_operand:SI 2 \"const_int_operand\" \"K\")))]\n   \"CONST_OK_FOR_K (INTVAL (operands[2]))\n    && ! CONST_OK_FOR_M (INTVAL (operands[2]))\"\n-  \"shlr%O2\t%0\")\n+  \"shlr%O2\t%0\"\n+  [(set_attr \"type\" \"arith\")])\n \n (define_insn \"lshrhi3_m\"\n   [(set (match_operand:HI 0 \"arith_reg_operand\" \"=r\")\n \t(lshiftrt:HI (match_operand:HI 1 \"arith_reg_operand\" \"0\")\n \t\t     (match_operand:HI 2 \"const_int_operand\" \"M\")))\n    (clobber (reg:SI 18))]\n   \"CONST_OK_FOR_M (INTVAL (operands[2]))\"\n-  \"shlr\t%0\")\n+  \"shlr\t%0\"\n+  [(set_attr \"type\" \"arith\")])\n \n (define_insn \"lshrhi3_k\"\n   [(set (match_operand:HI 0 \"arith_reg_operand\" \"=r\")\n \t(lshiftrt:HI (match_operand:HI 1 \"arith_reg_operand\" \"0\")\n \t\t     (match_operand:HI 2 \"const_int_operand\" \"K\")))]\n   \"CONST_OK_FOR_K (INTVAL (operands[2]))\n    && ! CONST_OK_FOR_M (INTVAL (operands[2]))\"\n-  \"shlr%O2\t%0\")\n+  \"shlr%O2\t%0\"\n+  [(set_attr \"type\" \"arith\")])\n \n (define_insn \"lshrsi3_n\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n \t(lshiftrt:SI (match_operand:SI 1 \"arith_reg_operand\" \"0\")\n \t\t     (match_operand:SI 2 \"const_int_operand\" \"n\")))\n    (clobber (reg:SI 18))]\n-  \"\"\n+  \"! sh_dynamicalize_shift_p (operands[2])\"\n   \"#\"\n   [(set (attr \"length\")\n \t(cond [(eq (symbol_ref \"shift_insns_rtx (insn)\") (const_int 1))\n@@ -1011,6 +1303,9 @@\n   \"\"\n   \"\n {\n+  if (GET_CODE (operands[2]) == CONST_INT\n+      && sh_dynamicalize_shift_p (operands[2]))\n+    operands[2] = force_reg (SImode, operands[2]);\n   if (TARGET_SH3 && arith_reg_operand (operands[2], GET_MODE (operands[2])))\n     {\n       rtx count = copy_to_mode_reg (SImode, operands[2]);\n@@ -1060,7 +1355,8 @@\n    (clobber (reg:SI 18))]\n   \"\"\n   \"shll\t%R0\\;rotcl\t%S0\"\n-  [(set_attr \"length\" \"4\")])\n+  [(set_attr \"length\" \"4\")\n+   (set_attr \"type\" \"arith\")])\n \n (define_expand \"ashldi3\"\n   [(parallel [(set (match_operand:DI 0 \"arith_reg_operand\" \"\")\n@@ -1080,7 +1376,8 @@\n    (clobber (reg:SI 18))]\n   \"\"\n   \"shlr\t%S0\\;rotcr\t%R0\"\n-  [(set_attr \"length\" \"4\")])\n+  [(set_attr \"length\" \"4\")\n+   (set_attr \"type\" \"arith\")])\n \n (define_expand \"lshrdi3\"\n   [(parallel [(set (match_operand:DI 0 \"arith_reg_operand\" \"\")\n@@ -1100,7 +1397,8 @@\n    (clobber (reg:SI 18))]\n   \"\"\n   \"shar\t%S0\\;rotcr\t%R0\"\n-  [(set_attr \"length\" \"4\")])\n+  [(set_attr \"length\" \"4\")\n+   (set_attr \"type\" \"arith\")])\n \n (define_expand \"ashrdi3\"\n   [(parallel [(set (match_operand:DI 0 \"arith_reg_operand\" \"\")\n@@ -1306,7 +1604,8 @@\n  \t        (lshiftrt:SI (match_operand:SI 2 \"arith_reg_operand\" \"0\")\n \t\t\t     (const_int 16))))]\n   \"\"\n-  \"xtrct\t%1,%0\")\n+  \"xtrct\t%1,%0\"\n+  [(set_attr \"type\" \"arith\")])\n \n (define_insn \"xtrct_right\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n@@ -1315,7 +1614,8 @@\n  \t        (ashift:SI (match_operand:SI 2 \"arith_reg_operand\" \"r\")\n \t\t\t   (const_int 16))))]\n   \"\"\n-  \"xtrct\t%2,%0\")\n+  \"xtrct\t%2,%0\"\n+  [(set_attr \"type\" \"arith\")])\n \f\n ;; -------------------------------------------------------------------------\n ;; Unary arithmetic\n@@ -1400,13 +1700,8 @@\n ;; ??? This should be a define expand.\n ;; ??? Or perhaps it should be dropped?\n \n-(define_insn \"extendsidi2\"\n-  [(set (match_operand:DI 0 \"arith_reg_operand\" \"=r\")\n-\t(sign_extend:DI (match_operand:SI 1 \"arith_reg_operand\" \"r\")))\n-   (clobber (reg:SI 18))]\n-  \"\"\n-  \"mov\t%1,%S0\\;mov\t%1,%R0\\;shll\t%S0\\;subc\t%S0,%S0\"\n-  [(set_attr \"length\" \"8\")])\n+/* There is no point in defining extendsidi2; convert_move generates good\n+   code for that.  */\n \n (define_insn \"extendhisi2\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r,r\")\n@@ -1499,57 +1794,76 @@\n   \"\"\n   \"sett\")\n \n-;; t/z is first, so that it will be preferred over r/r when reloading a move\n+;; t/r is first, so that it will be preferred over r/r when reloading a move\n ;; of a pseudo-reg into the T reg\n (define_insn \"movsi_i\"\n-  [(set (match_operand:SI 0 \"general_movdst_operand\" \"=t,r,r,r,r,r,m,<,xl,xl,r\")\n-\t(match_operand:SI 1 \"general_movsrc_operand\" \"z,Q,rI,m,xl,t,r,xl,r,>,i\"))]\n+  [(set (match_operand:SI 0 \"general_movdst_operand\" \"=t,r,r,r,r,r,m,<,<,xl,x,l,r\")\n+\t(match_operand:SI 1 \"general_movsrc_operand\" \"r,Q,rI,m,xl,t,r,x,l,r,>,>,i\"))]\n   \"\n-   ! TARGET_SH3E &&\n-   (register_operand (operands[0], SImode)\n-    || register_operand (operands[1], SImode))\"\n+   ! TARGET_SH3E\n+   && (register_operand (operands[0], SImode)\n+       || register_operand (operands[1], SImode))\"\n   \"@\n-\ttst\t%1,%1\\;rotcl\t%1\\;xor\t#1,%1\\;rotcr\t%1\n+\tcmp/pl\t%1\n \tmov.l\t%1,%0\n \tmov\t%1,%0\n \tmov.l\t%1,%0\n \tsts\t%1,%0\n \tmovt\t%0\n \tmov.l\t%1,%0\n \tsts.l\t%1,%0\n+\tsts.l\t%1,%0\n \tlds\t%1,%0\n \tlds.l\t%1,%0\n+\tlds.l\t%1,%0\n \tfake\t%1,%0\"\n-  [(set_attr \"type\" \"move,pcload,move,load,move,store,store,move,load,move,pcload\")\n-   (set_attr \"length\" \"8,*,*,*,*,*,*,*,*,*,*\")])\n+  [(set_attr \"type\" \"*,pcload_si,move,load_si,move,move,store,store,pstore,move,load,pload,pcload_si\")\n+   (set_attr \"length\" \"*,*,*,*,*,*,*,*,*,*,*,*,*\")])\n \n-;; t/z is first, so that it will be preferred over r/r when reloading a move\n-;; of a pseudo-reg into the T reg\n+;; t/r must come after r/r, lest reload will try to reload stuff like\n+;; (subreg:SI (reg:SF 38 fr14) 0) into T (compiling stdlib/strtod.c -m3e -O2)\n ;; ??? This allows moves from macl to fpul to be recognized, but these moves\n ;; will require a reload.\n (define_insn \"movsi_ie\"\n-  [(set (match_operand:SI 0 \"general_movdst_operand\" \"=t,r,r,r,r,r,m,<,xl,xl,r,y,r\")\n-\t(match_operand:SI 1 \"general_movsrc_operand\" \"z,Q,rI,m,xl,t,r,xl,r,>,i,r,y\"))]\n+  [(set (match_operand:SI 0 \"general_movdst_operand\" \"=r,r,t,r,r,r,m,<,<,xl,x,l,r,y,r,y\")\n+\t(match_operand:SI 1 \"general_movsrc_operand\" \"Q,rI,r,m,xl,t,r,x,l,r,>,>,i,r,y,y\"))]\n   \"TARGET_SH3E\n    && (register_operand (operands[0], SImode)\n        || register_operand (operands[1], SImode))\"\n   \"@\n-\ttst\t%1,%1\\;rotcl\t%1\\;xor\t#1,%1\\;rotcr\t%1\n \tmov.l\t%1,%0\n \tmov\t%1,%0\n+\tcmp/pl\t%1\n \tmov.l\t%1,%0\n \tsts\t%1,%0\n \tmovt\t%0\n \tmov.l\t%1,%0\n \tsts.l\t%1,%0\n+\tsts.l\t%1,%0\n \tlds\t%1,%0\n \tlds.l\t%1,%0\n+\tlds.l\t%1,%0\n \tfake\t%1,%0\n \tlds\t%1,%0\n-\tsts\t%1,%0\"\n-  [(set_attr \"type\" \"move,pcload,move,load,move,store,store,move,load,move,pcload,move,move\")\n-   (set_attr \"length\" \"8,*,*,*,*,*,*,*,*,*,*,*,*\")])\n-\n+\tsts\t%1,%0\n+\t! move optimized away\"\n+  [(set_attr \"type\" \"pcload_si,move,*,load_si,move,move,store,store,pstore,move,load,pload,pcload_si,gp_fpul,gp_fpul,other\")\n+   (set_attr \"length\" \"*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,0\")])\n+\n+(define_insn \"movsi_i_lowpart\"\n+  [(set (strict_low_part (match_operand:SI 0 \"general_movdst_operand\" \"=r,r,r,r,r,m,r\"))\n+\t(match_operand:SI 1 \"general_movsrc_operand\" \"Q,rI,m,xl,t,r,i\"))]\n+   \"register_operand (operands[0], SImode)\n+    || register_operand (operands[1], SImode)\"\n+  \"@\n+\tmov.l\t%1,%0\n+\tmov\t%1,%0\n+\tmov.l\t%1,%0\n+\tsts\t%1,%0\n+\tmovt\t%0\n+\tmov.l\t%1,%0\n+\tfake\t%1,%0\"\n+  [(set_attr \"type\" \"pcload,move,load,move,move,store,pcload\")])\n (define_expand \"movsi\"\n   [(set (match_operand:SI 0 \"general_movdst_operand\" \"\")\n \t(match_operand:SI 1 \"general_movsrc_operand\" \"\"))]\n@@ -1600,14 +1914,16 @@\n \n ;; ??? This should be a define expand.\n \n+;; x/r can be created by inlining/cse, e.g. for execute/961213-1.c\n+;; compiled with -m2 -ml -O3 -funroll-loops\n (define_insn \"\"\n-  [(set (match_operand:DI 0 \"general_movdst_operand\" \"=r,r,r,m,r,r,r\")\n-\t(match_operand:DI 1 \"general_movsrc_operand\" \"Q,r,m,r,I,i,x\"))]\n+  [(set (match_operand:DI 0 \"general_movdst_operand\" \"=r,r,r,m,r,r,r,*!x\")\n+\t(match_operand:DI 1 \"general_movsrc_operand\" \"Q,r,m,r,I,i,x,r\"))]\n   \"arith_reg_operand (operands[0], DImode)\n    || arith_reg_operand (operands[1], DImode)\"\n   \"* return output_movedouble (insn, operands, DImode);\"\n   [(set_attr \"length\" \"4\")\n-   (set_attr \"type\" \"pcload,move,load,store,move,pcload,move\")])\n+   (set_attr \"type\" \"pcload,move,load,store,move,pcload,move,move\")])\n \n ;; If the output is a register and the input is memory or a register, we have\n ;; to be careful and see which word needs to be loaded first.  \n@@ -1779,15 +2095,19 @@\n   [(set (match_operand:DF 0 \"general_movdst_operand\" \"\")\n \t(match_operand:DF 1 \"general_movsrc_operand\" \"\"))]\n   \"\"\n-  \"{ if (prepare_move_operands (operands, DFmode)) DONE; }\")\n+  \"\n+{\n+  if (prepare_move_operands (operands, DFmode)) DONE;\n+}\")\n+\n \n (define_insn \"movsf_i\"\n   [(set (match_operand:SF 0 \"general_movdst_operand\" \"=r,r,r,r,m,l,r\")\n \t(match_operand:SF 1 \"general_movsrc_operand\"  \"r,I,FQ,m,r,r,l\"))]\n   \"\n-   ! TARGET_SH3E &&\n-   (arith_reg_operand (operands[0], SFmode)\n-    || arith_reg_operand (operands[1], SFmode))\"\n+   ! TARGET_SH3E\n+   && (arith_reg_operand (operands[0], SFmode)\n+       || arith_reg_operand (operands[1], SFmode))\"\n   \"@\n \tmov\t%1,%0\n \tmov\t%1,%0\n@@ -1800,10 +2120,10 @@\n \n (define_insn \"movsf_ie\"\n   [(set (match_operand:SF 0 \"general_movdst_operand\"\n-\t \"=f,r,f,f,?f,f,m,r,r,m,!??r,!??f\")\n+\t \"=f,r,f,f,fy,f,m,r,r,m,f,y,y,rf,ry\")\n \t(match_operand:SF 1 \"general_movsrc_operand\"\n-\t  \"f,r,G,H,FQ,m,f,FQ,m,r,f,r\"))\n-   (clobber (match_scratch:SI 2 \"=X,X,X,X,&z,X,X,X,X,X,X,X\"))]\n+\t  \"f,r,G,H,FQ,m,f,FQ,m,r,y,f,>,fr,yr\"))\n+   (clobber (match_scratch:SI 2 \"=X,X,X,X,&z,X,X,X,X,X,X,X,X,y,X\"))]\n \n   \"TARGET_SH3E\n    && (arith_reg_operand (operands[0], SFmode)\n@@ -1819,29 +2139,39 @@\n \tmov.l\t%1,%0\n \tmov.l\t%1,%0\n \tmov.l\t%1,%0\n-\tflds\t%1,fpul\\;sts\tfpul,%0\n-\tlds\t%1,fpul\\;fsts\tfpul,%0\"\n-  [(set_attr \"type\" \"move,move,fp,fp,pcload,load,store,pcload,load,store,move,fp\")\n-   (set_attr \"length\" \"*,*,*,*,4,*,*,*,*,*,4,4\")])\n+\tfsts\tfpul,%0\n+\tflds\t%1,fpul\n+\tlds.l\t%1,%0\n+\t#\n+\t#\"\n+  [(set_attr \"type\" \"fmove,move,fmove,fmove,pcload,load,store,pcload,load,store,fmove,fmove,load,*,*\")\n+   (set_attr \"length\" \"*,*,*,*,4,*,*,*,*,*,2,2,2,*,*\")])\n \n (define_split\n-  [(set (match_operand:SF 0 \"general_movdst_operand\" \"\")\n-\t(match_operand:SF 1 \"general_movsrc_operand\"  \"\"))\n-   (clobber (reg:SI 0))]\n-  \"GET_CODE (operands[0]) == REG && REGNO (operands[0]) < FIRST_PSEUDO_REGISTER\"\n-  [(parallel [(set (match_dup 0) (match_dup 1))\n-\t      (clobber (scratch:SI))])]\n+  [(set (match_operand:SF 0 \"register_operand\" \"ry\")\n+\t(match_operand:SF 1 \"register_operand\" \"ry\"))\n+   (clobber (match_scratch:SI 2 \"X\"))]\n+  \"reload_completed\n+   && true_regnum (operands[0]) < FIRST_FP_REG\n+   && true_regnum (operands[1]) < FIRST_FP_REG\"\n+  [(set (match_dup 0) (match_dup 1))]\n   \"\n {\n-  if (REGNO (operands[0]) >= FIRST_FP_REG && REGNO (operands[0]) <= LAST_FP_REG)\n-    {\n-      if (GET_CODE (operands[1]) != MEM)\n-\tFAIL;\n-      emit_insn (gen_mova (XEXP (operands[1], 0)));\n-      XEXP (operands[1], 0) = gen_rtx (REG, Pmode, 0);\n-    }\n+  operands[0] = gen_rtx (REG, SImode, true_regnum (operands[0]));\n+  operands[1] = gen_rtx (REG, SImode, true_regnum (operands[1]));\n }\")\n \n+(define_split\n+  [(set (match_operand:SF 0 \"register_operand\" \"\")\n+\t(match_operand:SF 1 \"register_operand\" \"\"))\n+   (clobber (reg:SI 22))]\n+  \"\"\n+  [(parallel [(set (reg:SF 22) (match_dup 1))\n+\t      (clobber (scratch:SI))])\n+   (parallel [(set (match_dup 0) (reg:SF 22))\n+\t      (clobber (scratch:SI))])]\n+  \"\")\n+\n (define_expand \"movsf\"\n   [(set (match_operand:SF 0 \"general_movdst_operand\" \"\")\n         (match_operand:SF 1 \"general_movsrc_operand\" \"\"))]\n@@ -1884,59 +2214,56 @@\n   \"* return output_branch (0, insn, operands);\"\n   [(set_attr \"type\" \"cbranch\")])\n \n-(define_insn \"inverse_branch_true\"\n-  [(set (pc) (if_then_else (ne (reg:SI 18) (const_int 0))\n-\t\t\t   (pc)\n-\t\t\t   (label_ref (match_operand 0 \"\" \"\"))))]\n+;; Patterns to prevent reorg from re-combining a condbranch with a branch\n+;; which destination is too far away.\n+;; The const_int_operand is distinct for each branch target; it avoids\n+;; unwanted matches with redundant_insn.\n+(define_insn \"block_branch_redirect\"\n+  [(set (pc) (unspec [(match_operand 0 \"const_int_operand\" \"\")] 4))]\n   \"\"\n-  \"* return output_branch (0, insn, operands);\"\n-  [(set_attr \"type\" \"cbranch\")])\n+  \"\"\n+  [(set_attr \"length\" \"0\")])\n \n-(define_insn \"inverse_branch_false\"\n-  [(set (pc) (if_then_else (eq (reg:SI 18) (const_int 0))\n-   \t\t\t   (pc)\n-\t\t\t   (label_ref (match_operand 0 \"\" \"\"))))]\n+;; This one has the additional purpose to record a possible scratch register\n+;; for the following branch.\n+(define_insn \"indirect_jump_scratch\"\n+  [(set (match_operand 0 \"register_operand\" \"r\")\n+\t(unspec [(match_operand 1 \"const_int_operand\" \"\")] 4))]\n   \"\"\n-  \"* return output_branch (1, insn, operands);\"\n-  [(set_attr \"type\" \"cbranch\")])\n+  \"\"\n+  [(set_attr \"length\" \"0\")])\n \f\n ;; Conditional branch insns\n \n (define_expand \"beq\"\n-  [(set (reg:SI 18) (eq:SI (match_dup 1) (match_dup 2)))\n-   (set (pc)\n+  [(set (pc)\n \t(if_then_else (ne (reg:SI 18) (const_int 0))\n \t\t      (label_ref (match_operand 0 \"\" \"\"))\n \t\t      (pc)))]\n   \"\"\n   \"from_compare (operands, EQ);\")\n \n-; There is no bne compare, so we reverse the branch arms.\n-\n (define_expand \"bne\"\n-  [(set (reg:SI 18) (eq:SI (match_dup 1) (match_dup 2)))\n-   (set (pc)\n-\t(if_then_else (ne (reg:SI 18) (const_int 0))\n-\t\t      (pc)\n-\t\t      (label_ref (match_operand 0 \"\" \"\"))))]\n+  [(set (pc)\n+\t(if_then_else (eq (reg:SI 18) (const_int 0))\n+\t\t      (label_ref (match_operand 0 \"\" \"\"))\n+\t\t      (pc)))]\n   \"\"\n-  \"from_compare (operands, NE);\")\n+  \"from_compare (operands, EQ);\")\n \n (define_expand \"bgt\"\n-  [(set (reg:SI 18) (gt:SI (match_dup 1) (match_dup 2)))\n-   (set (pc)\n+  [(set (pc)\n \t(if_then_else (ne (reg:SI 18) (const_int 0))\n \t\t      (label_ref (match_operand 0 \"\" \"\"))\n \t\t      (pc)))]\n   \"\"\n   \"from_compare (operands, GT);\")\n \n (define_expand \"blt\"\n-  [(set (reg:SI 18) (ge:SI (match_dup 1) (match_dup 2)))\n-   (set (pc)\n-\t(if_then_else (ne (reg:SI 18) (const_int 0))\n-\t\t      (pc)\n-\t\t      (label_ref (match_operand 0 \"\" \"\"))))]\n+  [(set (pc)\n+\t(if_then_else (eq (reg:SI 18) (const_int 0))\n+\t\t      (label_ref (match_operand 0 \"\" \"\"))\n+\t\t      (pc)))]\n   \"\"\n   \"\n {\n@@ -1948,28 +2275,41 @@\n       emit_insn (gen_bgt (operands[0]));\n       DONE;\n     }\n-  from_compare (operands, LT);\n+  from_compare (operands, GE);\n }\")\n \n (define_expand \"ble\"\n-  [(set (reg:SI 18) (gt:SI (match_dup 1) (match_dup 2)))\n-   (set (pc)\n-\t(if_then_else (ne (reg:SI 18) (const_int 0))\n-\t\t      (pc)\n-\t\t      (label_ref (match_operand 0 \"\" \"\"))))]\n+  [(set (pc)\n+\t(if_then_else (eq (reg:SI 18) (const_int 0))\n+\t\t      (label_ref (match_operand 0 \"\" \"\"))\n+\t\t      (pc)))]\n   \"\"\n-  \"from_compare (operands, LE);\")\n+  \"\n+{\n+  if (TARGET_SH3E\n+      && TARGET_IEEE\n+      && GET_MODE_CLASS (GET_MODE (sh_compare_op0)) == MODE_FLOAT)\n+    {\n+      rtx tmp = sh_compare_op0;\n+      sh_compare_op0 = sh_compare_op1;\n+      sh_compare_op1 = tmp;\n+      emit_insn (gen_bge (operands[0]));\n+      DONE;\n+    }\n+  from_compare (operands, GT);\n+}\")\n \n (define_expand \"bge\"\n-  [(set (reg:SI 18) (ge:SI (match_dup 1) (match_dup 2)))\n-   (set (pc)\n+  [(set (pc)\n \t(if_then_else (ne (reg:SI 18) (const_int 0))\n \t\t      (label_ref (match_operand 0 \"\" \"\"))\n \t\t      (pc)))]\n   \"\"\n   \"\n {\n-  if (GET_MODE (sh_compare_op0) == SFmode)\n+  if (TARGET_SH3E\n+      && ! TARGET_IEEE\n+      && GET_MODE_CLASS (GET_MODE (sh_compare_op0)) == MODE_FLOAT)\n     {\n       rtx tmp = sh_compare_op0;\n       sh_compare_op0 = sh_compare_op1;\n@@ -1981,40 +2321,36 @@\n }\")\n \n (define_expand \"bgtu\"\n-  [(set (reg:SI 18) (gtu:SI (match_dup 1) (match_dup 2)))\n-   (set (pc)\n+  [(set (pc)\n \t(if_then_else (ne (reg:SI 18) (const_int 0))\n \t\t      (label_ref (match_operand 0 \"\" \"\"))\n \t\t      (pc)))]\n   \"\"\n   \"from_compare (operands, GTU); \")\n \n (define_expand \"bltu\"\n-  [(set (reg:SI 18) (geu:SI (match_dup 1) (match_dup 2)))\n-   (set (pc)\n-\t\t  (if_then_else (ne (reg:SI 18) (const_int 0))\n-\t\t\t\t(pc)\n-\t\t\t\t(label_ref (match_operand 0 \"\" \"\"))))]\n+  [(set (pc)\n+\t\t  (if_then_else (eq (reg:SI 18) (const_int 0))\n+\t\t\t\t(label_ref (match_operand 0 \"\" \"\"))\n+\t\t\t\t(pc)))]\n   \"\"\n-  \"from_compare (operands, LTU);\")\n+  \"from_compare (operands, GEU);\")\n \n (define_expand \"bgeu\"\n-  [(set (reg:SI 18) (geu:SI (match_dup 1) (match_dup 2)))\n-   (set (pc)\n+  [(set (pc)\n \t(if_then_else (ne (reg:SI 18) (const_int 0))\n \t\t      (label_ref (match_operand 0 \"\" \"\"))\n \t\t      (pc)))]\n   \"\"\n   \"from_compare (operands, GEU);\")\n \n (define_expand \"bleu\"\n-  [(set (reg:SI 18) (gtu:SI (match_dup 1) (match_dup 2)))\n-   (set (pc)\n-\t(if_then_else (ne (reg:SI 18) (const_int 0))\n-\t\t      (pc)\n-\t\t      (label_ref (match_operand 0 \"\" \"\"))))]\n+  [(set (pc)\n+\t(if_then_else (eq (reg:SI 18) (const_int 0))\n+\t\t      (label_ref (match_operand 0 \"\" \"\"))\n+\t\t      (pc)))]\n   \"\"\n-  \"from_compare (operands, LEU);\")\n+  \"from_compare (operands, GTU);\")\n \f\n ;; ------------------------------------------------------------------------\n ;; Jump and linkage insns\n@@ -2027,7 +2363,7 @@\n   \"*\n {\n   /* The length is 16 if the delay slot is unfilled.  */\n-  if (get_attr_length(insn) >= 14)\n+  if (get_attr_length(insn) > 4)\n     return output_far_jump(insn, operands[0]);\n   else\n     return   \\\"bra\t%l0%#\\\";\n@@ -2074,19 +2410,38 @@\n \t(match_operand:SI 0 \"arith_reg_operand\" \"r\"))]\n   \"\"\n   \"jmp\t@%0%#\"\n-  [(set_attr \"needs_delay_slot\" \"yes\")])\n+  [(set_attr \"needs_delay_slot\" \"yes\")\n+   (set_attr \"type\" \"jump_ind\")])\n \n-;; This might seem redundant, but it helps us distinguish case table jumps\n+;; The use of operand 1 / 2 helps us distinguish case table jumps\n ;; which can be present in structured code from indirect jumps which can not\n ;; be present in structured code.  This allows -fprofile-arcs to work.\n \n-(define_insn \"*casesi_jump\"\n+;; For SH1 processors.\n+(define_insn \"casesi_jump_1\"\n   [(set (pc)\n-\t(match_operand:SI 0 \"arith_reg_operand\" \"r\"))\n+\t(match_operand:SI 0 \"register_operand\" \"r\"))\n    (use (label_ref (match_operand 1 \"\" \"\")))]\n   \"\"\n-  \"jmp\t@%0%#\"\n-  [(set_attr \"needs_delay_slot\" \"yes\")])\n+  \"jmp  @%0%#\"\n+  [(set_attr \"needs_delay_slot\" \"yes\")\n+   (set_attr \"type\" \"jump_ind\")])\n+\n+;; For all later processors.\n+(define_insn \"casesi_jump_2\"\n+  [(set (pc) (plus:SI (match_operand:SI 0 \"register_operand\" \"r\")\n+\t\t      (match_operand 1 \"braf_label_ref_operand\" \"\")))\n+   (use (label_ref (match_operand 2 \"\" \"\")))]\n+  \"\"\n+  \"braf\t%0%#\"\n+  [(set_attr \"needs_delay_slot\" \"yes\")\n+   (set_attr \"type\" \"jump_ind\")])\n+\n+(define_insn \"dummy_jump\"\n+  [(set (pc) (const_int 0))]\n+  \"\"\n+  \"\"\n+  [(set_attr \"length\" \"0\")])\n \n ;; Call subroutine returning any type.\n ;; ??? This probably doesn't work.\n@@ -2127,22 +2482,26 @@\n \t(eq:SI (match_operand:SI 0 \"arith_reg_operand\" \"+r\") (const_int 1)))\n    (set (match_dup 0) (plus:SI (match_dup 0) (const_int -1)))]\n   \"TARGET_SH2\"\n-  \"dt\t%0\")\n+  \"dt\t%0\"\n+  [(set_attr \"type\" \"arith\")])\n \n (define_insn \"nop\"\n   [(const_int 0)]\n   \"\"\n   \"nop\")\n \n-;; Load address of a label. This is only generated by the casesi expand.\n-;; This must use unspec, because this only works immediately before a casesi.\n+;; Load address of a label. This is only generated by the casesi expand,\n+;; and by machine_dependent_reorg (fixing up fp moves).\n+;; This must use unspec, because this only works for labels that are\n+;; within range,\n \n (define_insn \"mova\"\n   [(set (reg:SI 0)\n \t(unspec [(label_ref (match_operand 0 \"\" \"\"))] 1))]\n   \"\"\n   \"mova\t%O0,r0\"\n-  [(set_attr \"in_delay_slot\" \"no\")])\n+  [(set_attr \"in_delay_slot\" \"no\")\n+   (set_attr \"type\" \"arith\")])\n \n ;; case instruction for switch statements.\n \n@@ -2152,56 +2511,157 @@\n ;; operand 3 is CODE_LABEL for the table;\n ;; operand 4 is the CODE_LABEL to go to if index out of range.\n \n-;; ??? There should be a barrier after the jump at the end.\n-\n (define_expand \"casesi\"\n-  [(set (match_dup 5) (match_operand:SI 0 \"arith_reg_operand\" \"\"))\n-   (set (match_dup 5) (minus:SI (match_dup 5)\n+  [(match_operand:SI 0 \"arith_reg_operand\" \"\")\n+   (match_operand:SI 1 \"arith_reg_operand\" \"\")\n+   (match_operand:SI 2 \"arith_reg_operand\" \"\")\n+   (match_operand 3 \"\" \"\") (match_operand 4 \"\" \"\")]\n+  \"\"\n+  \"\n+{\n+  rtx reg = gen_reg_rtx (SImode);\n+  rtx reg2 = gen_reg_rtx (SImode);\n+  operands[1] = copy_to_mode_reg (SImode, operands[1]);\n+  operands[2] = copy_to_mode_reg (SImode, operands[2]);\n+  /* If optimizing, casesi_worker depends on the mode of the instruction\n+     before label it 'uses' - operands[3].  */\n+  emit_insn (gen_casesi_0 (operands[0], operands[1], operands[2], operands[4],\n+\t\t\t   reg));\n+  emit_insn (gen_casesi_worker_0 (reg2, reg, operands[3]));\n+  if (TARGET_SH2)\n+    {\n+      rtx lab = gen_label_rtx ();\n+      emit_jump_insn (gen_casesi_jump_2 (reg2,\n+\t\t\t\t\t gen_rtx (LABEL_REF, VOIDmode, lab),\n+\t\t\t\t\t operands[3]));\n+      emit_label (lab);\n+      /* Put a fake jump after the label, lest some optimization might\n+\t delete the barrier and LAB.  */\n+      emit_jump_insn (gen_dummy_jump ());\n+    }\n+  else\n+    {\n+      emit_jump_insn (gen_casesi_jump_1 (reg2, operands[3]));\n+    }\n+  /* For SH2 and newer, the ADDR_DIFF_VEC is not actually relative to\n+     operands[3], but to lab.  We will fix this up in\n+     machine_dependent_reorg.  */\n+  emit_barrier ();\n+  DONE;\n+}\")\n+\n+(define_expand \"casesi_0\"\n+  [(set (match_operand:SI 4 \"\" \"\") (match_operand:SI 0 \"arith_reg_operand\" \"\"))\n+   (set (match_dup 4) (minus:SI (match_dup 4)\n \t\t\t\t(match_operand:SI 1 \"arith_operand\" \"\")))\n    (set (reg:SI 18)\n-\t(gtu:SI (match_dup 5)\n+\t(gtu:SI (match_dup 4)\n \t\t(match_operand:SI 2 \"arith_reg_operand\" \"\")))\n    (set (pc)\n \t(if_then_else (ne (reg:SI 18)\n \t\t\t  (const_int 0))\n-\t\t      (label_ref (match_operand 4 \"\" \"\"))\n-\t\t      (pc)))\n-   (set (match_dup 6) (match_dup 5))\n-   (set (match_dup 6) (ashift:SI (match_dup 6) (match_dup 7)))\n-   (set (reg:SI 0) (unspec [(label_ref (match_operand 3 \"\" \"\"))] 1))\n-   (parallel [(set (reg:SI 0) (plus:SI (reg:SI 0)\n-\t\t\t\t       (mem:HI (plus:SI (reg:SI 0)\n-\t\t\t\t\t\t\t(match_dup 6)))))\n-\t      (set (match_dup 6) (mem:HI (plus:SI (reg:SI 0) (match_dup 6))))])\n-   (parallel [(set (pc) (reg:SI 0))\n-\t      (use (label_ref (match_dup 3)))])]\n+\t\t      (label_ref (match_operand 3 \"\" \"\"))\n+\t\t      (pc)))]\n   \"\"\n-  \"\n-{\n-  operands[1] = copy_to_mode_reg (SImode, operands[1]);\n-  operands[2] = copy_to_mode_reg (SImode, operands[2]);\n-  operands[5] = gen_reg_rtx (SImode);\n-  operands[6] = gen_reg_rtx (SImode);\n-  operands[7] = GEN_INT (TARGET_BIGTABLE  ? 2 : 1);\n-}\")\n+  \"\")\n \n-(define_insn \"casesi_worker\"\n-  [(set (reg:SI 0)\n-\t(plus:SI (reg:SI 0)\n-\t\t (mem:HI (plus:SI (reg:SI 0)\n-\t\t\t\t  (match_operand:SI 0 \"arith_reg_operand\" \"+r\")))))\n-   (set (match_dup 0) (mem:HI (plus:SI (reg:SI 0)\n-\t\t\t\t       (match_dup 0))))]\n+;; ??? reload might clobber r0 if we use it explicitly in the RTL before\n+;; reload; using a R0_REGS pseudo reg is likely to give poor code.\n+;; So we keep the use of r0 hidden in a R0_REGS clobber until after reload.\n+\n+(define_insn \"casesi_worker_0\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=r,r\")\n+\t(unspec [(match_operand 1 \"register_operand\" \"0,r\")\n+\t\t (label_ref (match_operand 2 \"\" \"\"))] 2))\n+   (clobber (match_scratch:SI 3 \"=X,1\"))\n+   (clobber (match_scratch:SI 4 \"=&z,z\"))]\n+  \"\"\n+  \"#\")\n+\n+(define_split\n+  [(set (match_operand:SI 0 \"register_operand\" \"\")\n+\t(unspec [(match_operand 1 \"register_operand\" \"\")\n+\t\t (label_ref (match_operand 2 \"\" \"\"))] 2))\n+   (clobber (match_scratch:SI 3 \"\"))\n+   (clobber (match_scratch:SI 4 \"\"))]\n+  \"! TARGET_SH2 && reload_completed\"\n+  [(set (reg:SI 0) (unspec [(label_ref (match_dup 2))] 1))\n+   (parallel [(set (match_dup 0)\n+\t      (unspec [(reg:SI 0) (match_dup 1) (label_ref (match_dup 2))] 2))\n+\t      (clobber (match_dup 3))])\n+   (set (match_dup 0) (plus:SI (match_dup 0) (reg:SI 0)))]\n+  \"LABEL_NUSES (operands[2])++;\")\n+\n+(define_split\n+  [(set (match_operand:SI 0 \"register_operand\" \"\")\n+\t(unspec [(match_operand 1 \"register_operand\" \"\")\n+\t\t (label_ref (match_operand 2 \"\" \"\"))] 2))\n+   (clobber (match_scratch:SI 3 \"\"))\n+   (clobber (match_scratch:SI 4 \"\"))]\n+  \"TARGET_SH2 && reload_completed\"\n+  [(set (reg:SI 0) (unspec [(label_ref (match_dup 2))] 1))\n+   (parallel [(set (match_dup 0)\n+\t      (unspec [(reg:SI 0) (match_dup 1) (label_ref (match_dup 2))] 2))\n+\t      (clobber (match_dup 3))])]\n+  \"LABEL_NUSES (operands[2])++;\")\n+\n+(define_insn \"*casesi_worker\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=r,r\")\n+\t(unspec [(reg:SI 0) (match_operand 1 \"register_operand\" \"0,r\")\n+\t\t (label_ref (match_operand 2 \"\" \"\"))] 2))\n+   (clobber (match_scratch:SI 3 \"=X,1\"))]\n   \"\"\n   \"*\n {\n-  if (TARGET_BIGTABLE)\n-    return \\\"mov.l\t@(r0,%0),%0\\;add\t%0,r0\\\";\n-  else\n-    return \\\"mov.w\t@(r0,%0),%0\\;add\t%0,r0\\\";\n+  enum machine_mode mode\n+    = optimize\n+\t? GET_MODE (PATTERN (prev_real_insn (operands[2])))\n+\t: sh_addr_diff_vec_mode;\n+  switch (mode)\n+    {\n+    case SImode:\n+      return \\\"shll2\t%1\\;mov.l\t@(r0,%1),%0\\\";\n+    case HImode:\n+      return \\\"add\t%1,%1\\;mov.w\t@(r0,%1),%0\\\";\n+    case QImode:\n+      {\n+\trtx adj = PATTERN (prev_real_insn (operands[2]));\n+\tif ((insn_addresses[INSN_UID (XEXP ( XVECEXP (adj, 0, 1), 0))]\n+\t     - insn_addresses[INSN_UID (XEXP (XVECEXP (adj, 0, 2), 0))])\n+\t    <= 126)\n+\t  return \\\"mov.b\t@(r0,%1),%0\\\";\n+\treturn \\\"mov.b\t@(r0,%1),%0\\;extu.b\t%0,%0\\\";\n+      }\n+    default:\n+      abort ();\n+    }\n }\"\n   [(set_attr \"length\" \"4\")])\n \n+;; Include ADDR_DIFF_VECS in the shorten_branches pass; we have to\n+;; use a negative-length instruction to actually accomplish this.\n+(define_insn \"addr_diff_vec_adjust\"\n+  [(unspec_volatile [(label_ref (match_operand 0 \"\" \"\"))\n+\t\t     (label_ref (match_operand 1 \"\" \"\"))\n+\t\t     (label_ref (match_operand 2 \"\" \"\"))\n+\t\t     (match_operand 3 \"const_int_operand\" \"\")] 7)]\n+  \"\"\n+  \"*\n+{\n+  /* ??? ASM_OUTPUT_ADDR_DIFF_ELT gets passed no context information, so\n+     we must use a kludge with a global variable.  */\n+  sh_addr_diff_vec_mode = GET_MODE (PATTERN (insn));\n+  return \\\"\\\";\n+}\"\n+;; Need a variable length for this to be processed in each shorten_branch pass.\n+;; The actual work is done in ADJUST_INSN_LENTH, because length attributes\n+;; need to be (a choice of) constants.\n+;; We use the calculated length before ADJUST_INSN_LENGTH to\n+;; determine if the insn_addresses array contents are valid.\n+  [(set (attr \"length\")\n+\t(if_then_else (eq (pc) (const_int -1))\n+\t\t      (const_int 2) (const_int 0)))])\n+\n (define_insn \"return\"\n   [(return)]\n   \"reload_completed\"\n@@ -2233,7 +2693,8 @@\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n \t(eq:SI (reg:SI 18) (const_int 1)))]\n   \"\"\n-  \"movt\t%0\")\n+  \"movt\t%0\"\n+  [(set_attr \"type\" \"arith\")])\n \n (define_expand \"seq\"\n   [(set (match_operand:SI 0 \"arith_reg_operand\" \"\")\n@@ -2248,18 +2709,15 @@\n   \"operands[1] = prepare_scc_operands (LT);\")\n \n (define_expand \"sle\"\n-  [(set (match_operand:SI 0 \"arith_reg_operand\" \"\")\n-\t(match_dup 1))]\n+  [(match_operand:SI 0 \"arith_reg_operand\" \"\")]\n   \"\"\n   \"\n {\n-  if (GET_MODE (sh_compare_op0) == SFmode)\n-    {\n-      emit_insn (gen_sgt (operands[0]));\n-      emit_insn (gen_xorsi3 (operands[0], operands[0], const1_rtx));\n-      DONE;\n-    }\n-  operands[1] = prepare_scc_operands (LE);\n+  rtx tmp = sh_compare_op0;\n+  sh_compare_op0 = sh_compare_op1;\n+  sh_compare_op1 = tmp;\n+  emit_insn (gen_sge (operands[0]));\n+  DONE;\n }\")\n \n (define_expand \"sgt\"\n@@ -2276,8 +2734,22 @@\n {\n   if (GET_MODE (sh_compare_op0) == SFmode)\n     {\n-      emit_insn (gen_slt (operands[0]));\n-      emit_insn (gen_xorsi3 (operands[0], operands[0], const1_rtx));\n+      if (TARGET_IEEE)\n+\t{\n+\t  rtx t_reg = gen_rtx (REG, SImode, T_REG);\n+\t  rtx lab = gen_label_rtx ();\n+\t  emit_insn (gen_rtx (SET, VOIDmode, t_reg,\n+\t\t\t      gen_rtx (EQ, SImode, sh_compare_op0,\n+\t\t\t\t       sh_compare_op1)));\n+\t  emit_jump_insn (gen_branch_true (lab));\n+\t  emit_insn (gen_rtx (SET, VOIDmode, t_reg,\n+\t\t\t      gen_rtx (GT, SImode, sh_compare_op0,\n+\t\t\t\t       sh_compare_op1)));\n+\t  emit_label (lab);\n+\t  emit_insn (gen_movt (operands[0]));\n+\t}\n+      else\n+\temit_insn (gen_movnegt (operands[0], prepare_scc_operands (LT)));\n       DONE;\n     }\n   operands[1] = prepare_scc_operands (GE);\n@@ -2330,6 +2802,18 @@\n    operands[2] = gen_reg_rtx (SImode);\n }\")\n \n+;; Use the same trick for FP sle / sge\n+(define_expand \"movnegt\"\n+  [(set (match_dup 2) (const_int -1))\n+   (parallel [(set (match_operand 0 \"\" \"\")\n+\t\t   (neg:SI (plus:SI (match_dup 1)\n+\t\t\t\t    (match_dup 2))))\n+\t      (set (reg:SI 18)\n+\t\t   (ne:SI (ior:SI (match_operand 1 \"\" \"\") (match_dup 2))\n+\t\t\t  (const_int 0)))])]  \n+  \"\"\n+  \"operands[2] = gen_reg_rtx (SImode);\")\n+\n ;; Recognize mov #-1/negc/neg sequence, and change it to movt/add #-1.\n ;; This prevents a regression that occured when we switched from xor to\n ;; mov/neg for sne.\n@@ -2416,22 +2900,42 @@\n  [(set_attr \"length\" \"8\")\n   (set_attr \"in_delay_slot\" \"no\")])\n \n+;; Alignment is needed for some constant tables; it may also be added for\n+;; Instructions at the start of loops, or after unconditional branches.\n+;; ??? We would get more accurate lengths if we did instruction\n+;; alignment based on the value of INSN_CURRENT_ADDRESS; the approach used\n+;; here is too conservative.\n+\n ; align to a two byte boundary\n \n (define_insn \"align_2\"\n- [(unspec_volatile [(const_int 0)] 10)]\n+ [(unspec_volatile [(const_int 1)] 1)]\n  \"\"\n  \".align 1\"\n  [(set_attr \"length\" \"0\")\n   (set_attr \"in_delay_slot\" \"no\")])\n \n ; align to a four byte boundary\n+;; align_4 and align_log are instructions for the starts of loops, or\n+;; after unconditional branches, which may take up extra room.\n+\n+(define_expand \"align_4\"\n+ [(unspec_volatile [(const_int 2)] 1)]\n+ \"\"\n+ \"\")\n+\n+; align to a cache line boundary\n \n-(define_insn \"align_4\"\n- [(unspec_volatile [(const_int 0)] 5)]\n+(define_insn \"align_log\"\n+ [(unspec_volatile [(match_operand 0 \"const_int_operand\" \"\")] 1)]\n  \"\"\n- \".align 2\"\n- [(set_attr \"in_delay_slot\" \"no\")])\n+ \".align %O0\"\n+;; Need a variable length for this to be processed in each shorten_branch pass.\n+;; The actual work is done in ADJUST_INSN_LENTH, because length attributes\n+;; need to be (a choice of) constants.\n+  [(set (attr \"length\")\n+\t(if_then_else (ne (pc) (pc)) (const_int 2) (const_int 0)))\n+  (set_attr \"in_delay_slot\" \"no\")])\n \n ; emitted at the end of the literal table, used to emit the\n ; 32bit branch labels if needed.\n@@ -2508,7 +3012,7 @@\n (define_insn \"subsf3\"\n   [(set (match_operand:SF 0 \"arith_reg_operand\" \"=f\")\n \t(minus:SF (match_operand:SF 1 \"arith_reg_operand\" \"0\")\n-\t\t  (match_operand:SF 2 \"arith_reg_operand\" \"f\")))]\n+\t\t (match_operand:SF 2 \"arith_reg_operand\" \"f\")))]\n   \"TARGET_SH3E\"\n   \"fsub\t%2,%0\"\n   [(set_attr \"type\" \"fp\")])\n@@ -2533,81 +3037,64 @@\n (define_insn \"divsf3\"\n   [(set (match_operand:SF 0 \"arith_reg_operand\" \"=f\")\n \t(div:SF (match_operand:SF 1 \"arith_reg_operand\" \"0\")\n-\t\t(match_operand:SF 2 \"arith_reg_operand\" \"f\")))]\n+\t\t (match_operand:SF 2 \"arith_reg_operand\" \"f\")))]\n   \"TARGET_SH3E\"\n   \"fdiv\t%2,%0\"\n   [(set_attr \"type\" \"fdiv\")])\n \n-;; ??? This is the right solution, but it fails because the movs[if] patterns\n-;; silently clobber FPUL (r22) for int<->fp moves.  Thus we can not explicitly\n-;; use FPUL here.\n-;;\n-;;(define_expand \"floatsisf2\"\n-;;  [(set (reg:SI 22)\n-;;\t(match_operand:SI 1 \"arith_reg_operand\" \"\"))\n-;;   (set (match_operand:SF 0 \"arith_reg_operand\" \"\")\n-;;        (float:SF (reg:SI 22)))]\n-;;  \"TARGET_SH3E\"\n-;;  \"\")\n-;;\n-;;(define_insn \"*floatsisf\"\n-;;  [(set (match_operand:SF 0 \"arith_reg_operand\" \"=f\")\n-;;\t(float:SF (reg:SI 22)))]\n-;;  \"TARGET_SH3E\"\n-;;  \"float\tfpul,%0\")\n+(define_expand \"floatsisf2\"\n+  [(set (reg:SI 22)\n+\t(match_operand:SI 1 \"arith_reg_operand\" \"\"))\n+   (set (match_operand:SF 0 \"arith_reg_operand\" \"\")\n+        (float:SF (reg:SI 22)))]\n+  \"TARGET_SH3E\"\n+  \"\")\n \n-(define_insn \"floatsisf2\"\n+(define_insn \"*floatsisf2_ie\"\n   [(set (match_operand:SF 0 \"arith_reg_operand\" \"=f\")\n-\t(float:SF (match_operand:SI 1 \"arith_reg_operand\" \"r\")))]\n+\t(float:SF (reg:SI 22)))]\n   \"TARGET_SH3E\"\n-  \"lds\t%1,fpul\\;float\tfpul,%0\"\n-  [(set_attr \"length\" \"4\")\n-   (set_attr \"type\" \"fp\")])\n+  \"float\tfpul,%0\"\n+  [(set_attr \"type\" \"fp\")])\n \n-;; ??? This is the right solution, but it fails because the movs[if] patterns\n-;; silently clobber FPUL (r22) for int<->fp moves.  Thus we can not explicitly\n-;; use FPUL here.\n-;;\n-;;(define_expand \"fix_truncsfsi2\"\n-;;  [(set (reg:SI 22)\n-;;\t(fix:SI (match_operand:SF 1 \"arith_reg_operand\" \"f\")))\n-;;   (set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n-;;\t(reg:SI 22))]\n-;;  \"TARGET_SH3E\"\n-;;  \"\")\n-;;\n-;;(define_insn \"*fixsfsi\"\n-;;  [(set (reg:SI 22)\n-;;\t(fix:SI (match_operand:SF 0 \"arith_reg_operand\" \"f\")))]\n-;;  \"TARGET_SH3E\"\n-;;  \"ftrc\t%0,fpul\")\n+(define_expand \"fix_truncsfsi2\"\n+  [(set (reg:SI 22)\n+\t(fix:SI (match_operand:SF 1 \"arith_reg_operand\" \"f\")))\n+   (set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n+\t(reg:SI 22))]\n+  \"TARGET_SH3E\"\n+  \"\")\n \n-(define_insn \"fix_truncsfsi2\"\n-  [(set (match_operand:SI 0 \"arith_reg_operand\" \"=r\")\n-\t(fix:SI (match_operand:SF 1 \"arith_reg_operand\" \"f\")))]\n+(define_insn \"*fixsfsi\"\n+  [(set (reg:SI 22)\n+\t(fix:SI (match_operand:SF 0 \"arith_reg_operand\" \"f\")))]\n   \"TARGET_SH3E\"\n-  \"ftrc\t%1,fpul\\;sts\tfpul,%0\"\n-  [(set_attr \"length\" \"4\")\n-   (set_attr \"type\" \"move\")])\n+  \"ftrc\t%0,fpul\"\n+  [(set_attr \"type\" \"fp\")])\n \n-;; ??? This should be SFmode not SImode in the compare, but that would\n-;; require fixing the branch patterns too.\n-(define_insn \"*cmpgtsf_t\"\n+(define_insn \"cmpgtsf_t\"\n   [(set (reg:SI 18) (gt:SI (match_operand:SF 0 \"arith_reg_operand\" \"f\")\n \t\t\t   (match_operand:SF 1 \"arith_reg_operand\" \"f\")))]\n   \"TARGET_SH3E\"\n   \"fcmp/gt\t%1,%0\"\n   [(set_attr \"type\" \"fp\")])\n \n-;; ??? This should be SFmode not SImode in the compare, but that would\n-;; require fixing the branch patterns too.\n-(define_insn \"*cmpeqsf_t\"\n+(define_insn \"cmpeqsf_t\"\n   [(set (reg:SI 18) (eq:SI (match_operand:SF 0 \"arith_reg_operand\" \"f\")\n \t\t\t   (match_operand:SF 1 \"arith_reg_operand\" \"f\")))]\n   \"TARGET_SH3E\"\n   \"fcmp/eq\t%1,%0\"\n   [(set_attr \"type\" \"fp\")])\n \n+(define_insn \"ieee_ccmpeqsf_t\"\n+  [(set (reg:SI 18) (ior:SI (reg:SI 18)\n+\t\t\t    (eq:SI (match_operand:SF 0 \"arith_reg_operand\" \"f\")\n+\t\t\t\t   (match_operand:SF 1 \"arith_reg_operand\" \"f\"))))]\n+  \"TARGET_SH3E && TARGET_IEEE\"\n+  \"* return output_ieee_ccmpeq (insn, operands);\"\n+  [(set_attr \"length\" \"4\")])\n+\n+\n (define_expand \"cmpsf\"\n   [(set (reg:SI 18) (compare (match_operand:SF 0 \"arith_operand\" \"\")\n \t\t\t     (match_operand:SF 1 \"arith_operand\" \"\")))]\n@@ -2813,7 +3300,7 @@\n        || (GET_CODE (operands[2]) == SUBREG\n \t   && REGNO (SUBREG_REG (operands[2])) >= FIRST_FP_REG))\n    && reg_unused_after (operands[0], insn)\"\n-  \"fmov.s\t%2,@(%0,%1)\")\n+  \"fmov{.s|}\t%2,@(%0,%1)\")\n \n (define_peephole\n   [(set (match_operand:SI 0 \"register_operand\" \"=r\")\n@@ -2826,7 +3313,7 @@\n        || (GET_CODE (operands[2]) == SUBREG\n \t   && REGNO (SUBREG_REG (operands[2])) >= FIRST_FP_REG))\n    && reg_unused_after (operands[0], insn)\"\n-  \"fmov.s\t@(%0,%1),%2\")\n+  \"fmov{.s|}\t@(%0,%1),%2\")\n \n ;; Switch to a new stack with its address in sp_switch (a SYMBOL_REF).  */\n (define_insn \"sp_switch_1\""}, {"sha": "f1671c7b0b6c0abc146063b895fb6cd4f03383bf", "filename": "gcc/ginclude/va-sh.h", "status": "modified", "additions": 50, "deletions": 74, "changes": 124, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1245df60c108112fedca5d76a69f19895a0f4ae4/gcc%2Fginclude%2Fva-sh.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1245df60c108112fedca5d76a69f19895a0f4ae4/gcc%2Fginclude%2Fva-sh.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fginclude%2Fva-sh.h?ref=1245df60c108112fedca5d76a69f19895a0f4ae4", "patch": "@@ -23,9 +23,6 @@ typedef struct {\n \n typedef void *__gnuc_va_list;\n \n-#define __va_rounded_size(TYPE)  \\\n-  (((sizeof (TYPE) + sizeof (int) - 1) / sizeof (int)) * sizeof (int))\n-\n #endif /* ! SH3E */\n \n #endif /* __GNUC_VA_LIST */\n@@ -116,104 +113,83 @@ enum __va_type_classes {\n #endif\n #define va_end(pvar)\t((void)0)\n \n+#ifdef __LITTLE_ENDIAN__\n+#define __LITTLE_ENDIAN_P 1\n+#else\n+#define __LITTLE_ENDIAN_P 0\n+#endif\n+\n+#define __SCALAR_TYPE(TYPE)\t\t\t\t\t\\\n+  ((TYPE) == __integer_type_class\t\t\t\t\\\n+   || (TYPE) == __char_type_class\t\t\t\t\\\n+   || (TYPE) == __enumeral_type_class)\n+\n /* RECORD_TYPE args passed using the C calling convention are\n    passed by invisible reference.  ??? RECORD_TYPE args passed\n    in the stack are made to be word-aligned; for an aggregate that is\n    not word-aligned, we advance the pointer to the first non-reg slot.  */\n \n+  /* When this is a smaller-than-int integer, using\n+     auto-increment in the promoted (SImode) is fastest;\n+     however, there is no way to express that is C.  Therefore,\n+     we use an asm.\n+     We want the MEM_IN_STRUCT_P bit set in the emitted RTL, therefore we\n+     use unions even when it would otherwise be unnecessary.  */\n+\n+#define __va_arg_sh1(AP, TYPE) __extension__ \t\t\t\t\\\n+__extension__\t\t\t\t\t\t\t\t\\\n+({(sizeof (TYPE) == 1\t\t\t\t\t\t\t\\\n+   ? ({union {TYPE t; char c;} __t;\t\t\t\t\t\\\n+       asm(\"\"\t\t\t\t\t\t\t\t\\\n+\t   : \"=r\" (__t.c)\t\t\t\t\t\t\\\n+\t   : \"0\" ((((union { int i, j; } *) (AP))++)->i));\t\t\\\n+       __t.t;})\t\t\t\t\t\t\t\t\\\n+   : sizeof (TYPE) == 2\t\t\t\t\t\t\t\\\n+   ? ({union {TYPE t; short s;} __t;\t\t\t\t\t\\\n+       asm(\"\"\t\t\t\t\t\t\t\t\\\n+\t   : \"=r\" (__t.s)\t\t\t\t\t\t\\\n+\t   : \"0\" ((((union { int i, j; } *) (AP))++)->i));\t\t\\\n+       __t.t;})\t\t\t\t\t\t\t\t\\\n+   : sizeof (TYPE) >= 4 || __LITTLE_ENDIAN_P\t\t\t\t\\\n+   ? (((union { TYPE t; int i;} *) (AP))++)->t\t\t\t\t\\\n+   : ((union {TYPE t;TYPE u;}*) ((char *)++(int *)(AP) - sizeof (TYPE)))->t);})\n+\n #ifdef __SH3E__\n \n-#ifdef __LITTLE_ENDIAN__\n+#define __PASS_AS_FLOAT(TYPE_CLASS,SIZE) \\\n+  (TYPE_CLASS == __real_type_class && SIZE == 4)\n \n #define va_arg(pvar,TYPE)\t\t\t\t\t\\\n __extension__\t\t\t\t\t\t\t\\\n-(*({int __type = __builtin_classify_type (* (TYPE *) 0);\t\\\n-  void * __result;\t\t\t\t\t\t\\\n-  if (__type == __real_type_class && sizeof(TYPE) == 4)\t\t\\\n-\t\t\t\t\t\t/* float? */\t\\\n+({int __type = __builtin_classify_type (* (TYPE *) 0);\t\t\\\n+  void * __result_p;\t\t\t\t\t\t\\\n+  if (__PASS_AS_FLOAT (__type, sizeof(TYPE)))\t\t\t\\\n     {\t\t\t\t\t\t\t\t\\\n-      __va_freg *__r;\t\t\t\t\t\t\\\n       if (pvar.__va_next_fp < pvar.__va_next_fp_limit)\t\t\\\n-\t__r = (__va_freg *) pvar.__va_next_fp++;\t\t\\\n-      else\t\t\t\t\t\t\t\\\n-\t__r = (__va_freg *) pvar.__va_next_stack++;\t\t\\\n-      __result = (char *) __r;\t\t\t\t\t\\\n-    }\t\t\t\t\t\t\t\t\\\n-  else\t\t\t\t\t\t\t\t\\\n-    {\t\t\t\t\t\t\t\t\\\n-      __va_greg *_r;\t\t\t\t\t\t\\\n-      if (pvar.__va_next_o + ((sizeof (TYPE) + 3) / 4)\t\t\\\n-\t  <= pvar.__va_next_o_limit) \t\t\t\t\\\n \t{\t\t\t\t\t\t\t\\\n-\t  _r = pvar.__va_next_o;\t\t\t\t\\\n-\t  pvar.__va_next_o += (sizeof (TYPE) + 3) / 4;\t\t\\\n+\t  __result_p = &pvar.__va_next_fp;\t\t\t\\\n \t}\t\t\t\t\t\t\t\\\n       else\t\t\t\t\t\t\t\\\n-\t{\t\t\t\t\t\t\t\\\n-\t  _r = pvar.__va_next_stack;\t\t\t\t\\\n-\t  pvar.__va_next_stack += (sizeof (TYPE) + 3) / 4;\t\\\n-\t}\t\t\t\t\t\t\t\\\n-      __result = (char *) _r;\t\t\t\t\t\\\n-    } \t\t\t\t\t\t\t\t\\\n-  (TYPE *) __result;}))\n-\n-#else /* ! __LITTLE_ENDIAN__ */\n-\n-#define va_arg(pvar,TYPE)\t\t\t\t\t\\\n-__extension__\t\t\t\t\t\t\t\\\n-(*({int __type = __builtin_classify_type (* (TYPE *) 0);\t\\\n-  void * __result;\t\t\t\t\t\t\\\n-  if (__type == __real_type_class && sizeof(TYPE) == 4)\t\t\\\n-\t\t\t\t\t\t/* float? */\t\\\n-    {\t\t\t\t\t\t\t\t\\\n-      __va_freg *__r;\t\t\t\t\t\t\\\n-      if (pvar.__va_next_fp < pvar.__va_next_fp_limit)\t\t\\\n-\t__r = (__va_freg *) pvar.__va_next_fp++;\t\t\\\n-      else\t\t\t\t\t\t\t\\\n-\t__r = (__va_freg *) pvar.__va_next_stack++;\t\t\\\n-      __result = (char *) __r;\t\t\t\t\t\\\n+\t__result_p = &pvar.__va_next_stack;\t\t\t\\\n     }\t\t\t\t\t\t\t\t\\\n   else\t\t\t\t\t\t\t\t\\\n     {\t\t\t\t\t\t\t\t\\\n-      __va_greg *_r;\t\t\t\t\t\t\\\n       if (pvar.__va_next_o + ((sizeof (TYPE) + 3) / 4)\t\t\\\n \t  <= pvar.__va_next_o_limit) \t\t\t\t\\\n-\t{\t\t\t\t\t\t\t\\\n-\t  pvar.__va_next_o += (sizeof (TYPE) + 3) / 4;\t\t\\\n-\t  _r = pvar.__va_next_o;\t\t\t\t\\\n-\t}\t\t\t\t\t\t\t\\\n+\t__result_p = &pvar.__va_next_o;\t\t\t\t\\\n       else\t\t\t\t\t\t\t\\\n \t{\t\t\t\t\t\t\t\\\n-\t  pvar.__va_next_stack += (sizeof (TYPE) + 3) / 4;\t\\\n-\t  _r = pvar.__va_next_stack;\t\t\t\t\\\n+\t  if (sizeof (TYPE) > 4)\t\t\t\t\\\n+\t    pvar.__va_next_o = pvar.__va_next_o_limit;\t\t\\\n+\t\t\t\t\t\t\t\t\\\n+\t  __result_p = &pvar.__va_next_stack;\t\t\t\\\n \t}\t\t\t\t\t\t\t\\\n-      __result = ((char *) _r\t\t\t\t\t\\\n-\t\t  - (sizeof (TYPE) < 4 ? sizeof (TYPE)\t\t\\\n-\t\t     : ((sizeof (TYPE) + 3) / 4) * 4));\t\t\\\n     } \t\t\t\t\t\t\t\t\\\n-  (TYPE *) __result;}))\n-\n-#endif /* __LITTLE_ENDIAN__ */\n+  __va_arg_sh1(*(void **)__result_p, TYPE);})\n \n #else /* ! SH3E */\n \n-#ifdef __LITTLE_ENDIAN__\n-\n-/* This is for little-endian machines; small args are padded upward.  */\n-#define va_arg(AP, TYPE)\t\t\t\t\t\t\\\n- (AP = (__gnuc_va_list) ((char *) (AP) + __va_rounded_size (TYPE)),\t\\\n-  *((TYPE *) (void *) ((char *) (AP) - __va_rounded_size (TYPE))))\n-\n-#else /* ! __LITTLE_ENDIAN__ */\n-\n-/* This is for big-endian machines; small args are padded downward.  */\n-#define va_arg(AP, TYPE)\t\t\t\t\t\t\\\n- (AP = (__gnuc_va_list) ((char *) (AP) + __va_rounded_size (TYPE)),\t\\\n-  *((TYPE *) (void *) ((char *) (AP)\t\t\t\t\t\\\n-\t\t       - ((sizeof (TYPE) < __va_rounded_size (char)\t\\\n-\t\t\t   ? sizeof (TYPE) : __va_rounded_size (TYPE))))))\n-\n-#endif /* __LITTLE_ENDIAN__ */\n+#define va_arg(AP, TYPE) __va_arg_sh1((AP), TYPE)\n \n #endif /* SH3E */\n "}]}