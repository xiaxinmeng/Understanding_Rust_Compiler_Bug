{"sha": "e603cd43b145c426468c95cf85b3c12c94daedaa", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZTYwM2NkNDNiMTQ1YzQyNjQ2OGM5NWNmODViM2MxMmM5NGRhZWRhYQ==", "commit": {"author": {"name": "Mihail Ionescu", "email": "mihail.ionescu@arm.com", "date": "2020-02-18T14:29:47Z"}, "committer": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2020-02-25T18:36:53Z"}, "message": "aarch64: Add bfloat16 vldn/vstn intrinsics\n\nThis patch adds the load/store bfloat16 intrinsics to the AArch64 back-end.\nACLE documents are at https://developer.arm.com/docs/101028/latest\nISA documents are at https://developer.arm.com/docs/ddi0596/latest\n\n2020-02-25  Mihail Ionescu  <mihail.ionescu@arm.com>\n\ngcc/\n\t* config/aarch64/aarch64-builtins.c (aarch64_scalar_builtin_types):\n\tAdd simd_bf.\n\t(aarch64_init_simd_builtin_scalar_types): Register simd_bf.\n\t(VAR15, VAR16): New.\n\t* config/aarch64/iterators.md (VALLDIF): Enable for V4BF and V8BF.\n\t(VD): Enable for V4BF.\n\t(VDC): Likewise.\n\t(VQ): Enable for V8BF.\n\t(VQ2): Likewise.\n\t(VQ_NO2E): Likewise.\n\t(VDBL, Vdbl): Add V4BF.\n\t(V_INT_EQUIV, v_int_equiv): Add V4BF and V8BF.\n\t* config/aarch64/arm_neon.h (bfloat16x4x2_t): New typedef.\n\t(bfloat16x8x2_t): Likewise.\n\t(bfloat16x4x3_t): Likewise.\n\t(bfloat16x8x3_t): Likewise.\n\t(bfloat16x4x4_t): Likewise.\n\t(bfloat16x8x4_t): Likewise.\n\t(vcombine_bf16): New.\n\t(vld1_bf16, vld1_bf16_x2): New.\n\t(vld1_bf16_x3, vld1_bf16_x4): New.\n\t(vld1q_bf16, vld1q_bf16_x2): New.\n\t(vld1q_bf16_x3, vld1q_bf16_x4): New.\n\t(vld1_lane_bf16): New.\n\t(vld1q_lane_bf16): New.\n\t(vld1_dup_bf16): New.\n\t(vld1q_dup_bf16): New.\n\t(vld2_bf16): New.\n\t(vld2q_bf16): New.\n\t(vld2_dup_bf16): New.\n\t(vld2q_dup_bf16): New.\n\t(vld3_bf16): New.\n\t(vld3q_bf16): New.\n\t(vld3_dup_bf16): New.\n\t(vld3q_dup_bf16): New.\n\t(vld4_bf16): New.\n\t(vld4q_bf16): New.\n\t(vld4_dup_bf16): New.\n\t(vld4q_dup_bf16): New.\n\t(vst1_bf16, vst1_bf16_x2): New.\n\t(vst1_bf16_x3, vst1_bf16_x4): New.\n\t(vst1q_bf16, vst1q_bf16_x2): New.\n\t(vst1q_bf16_x3, vst1q_bf16_x4): New.\n\t(vst1_lane_bf16): New.\n\t(vst1q_lane_bf16): New.\n\t(vst2_bf16): New.\n\t(vst2q_bf16): New.\n\t(vst3_bf16): New.\n\t(vst3q_bf16): New.\n\t(vst4_bf16): New.\n\t(vst4q_bf16): New.\n\ngcc/testsuite/\n\t* gcc.target/aarch64/advsimd-intrinsics/bf16_vstn.c: New test.\n\t* gcc.target/aarch64/advsimd-intrinsics/bf16_vldn.c: New test.", "tree": {"sha": "1a81b53ca570e287978ff1f06e2aec601047d734", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/1a81b53ca570e287978ff1f06e2aec601047d734"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/e603cd43b145c426468c95cf85b3c12c94daedaa", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e603cd43b145c426468c95cf85b3c12c94daedaa", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e603cd43b145c426468c95cf85b3c12c94daedaa", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e603cd43b145c426468c95cf85b3c12c94daedaa/comments", "author": null, "committer": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "8ea6c1b89a20ef7c675535ba1994355361dac977", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8ea6c1b89a20ef7c675535ba1994355361dac977", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8ea6c1b89a20ef7c675535ba1994355361dac977"}], "stats": {"total": 822, "additions": 814, "deletions": 8}, "files": [{"sha": "653c276d3fbf663ce62e49f7e2f766e175728320", "filename": "gcc/ChangeLog", "status": "modified", "additions": 54, "deletions": 0, "changes": 54, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e603cd43b145c426468c95cf85b3c12c94daedaa/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e603cd43b145c426468c95cf85b3c12c94daedaa/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=e603cd43b145c426468c95cf85b3c12c94daedaa", "patch": "@@ -1,3 +1,57 @@\n+2020-02-25  Mihail Ionescu  <mihail.ionescu@arm.com>\n+\n+\t* config/aarch64/aarch64-builtins.c (aarch64_scalar_builtin_types):\n+\tAdd simd_bf.\n+\t(aarch64_init_simd_builtin_scalar_types): Register simd_bf.\n+\t(VAR15, VAR16): New.\n+\t* config/aarch64/iterators.md (VALLDIF): Enable for V4BF and V8BF.\n+\t(VD): Enable for V4BF.\n+\t(VDC): Likewise.\n+\t(VQ): Enable for V8BF.\n+\t(VQ2): Likewise.\n+\t(VQ_NO2E): Likewise.\n+\t(VDBL, Vdbl): Add V4BF.\n+\t(V_INT_EQUIV, v_int_equiv): Add V4BF and V8BF.\n+\t* config/aarch64/arm_neon.h (bfloat16x4x2_t): New typedef.\n+\t(bfloat16x8x2_t): Likewise.\n+\t(bfloat16x4x3_t): Likewise.\n+\t(bfloat16x8x3_t): Likewise.\n+\t(bfloat16x4x4_t): Likewise.\n+\t(bfloat16x8x4_t): Likewise.\n+\t(vcombine_bf16): New.\n+\t(vld1_bf16, vld1_bf16_x2): New.\n+\t(vld1_bf16_x3, vld1_bf16_x4): New.\n+\t(vld1q_bf16, vld1q_bf16_x2): New.\n+\t(vld1q_bf16_x3, vld1q_bf16_x4): New.\n+\t(vld1_lane_bf16): New.\n+\t(vld1q_lane_bf16): New.\n+\t(vld1_dup_bf16): New.\n+\t(vld1q_dup_bf16): New.\n+\t(vld2_bf16): New.\n+\t(vld2q_bf16): New.\n+\t(vld2_dup_bf16): New.\n+\t(vld2q_dup_bf16): New.\n+\t(vld3_bf16): New.\n+\t(vld3q_bf16): New.\n+\t(vld3_dup_bf16): New.\n+\t(vld3q_dup_bf16): New.\n+\t(vld4_bf16): New.\n+\t(vld4q_bf16): New.\n+\t(vld4_dup_bf16): New.\n+\t(vld4q_dup_bf16): New.\n+\t(vst1_bf16, vst1_bf16_x2): New.\n+\t(vst1_bf16_x3, vst1_bf16_x4): New.\n+\t(vst1q_bf16, vst1q_bf16_x2): New.\n+\t(vst1q_bf16_x3, vst1q_bf16_x4): New.\n+\t(vst1_lane_bf16): New.\n+\t(vst1q_lane_bf16): New.\n+\t(vst2_bf16): New.\n+\t(vst2q_bf16): New.\n+\t(vst3_bf16): New.\n+\t(vst3q_bf16): New.\n+\t(vst4_bf16): New.\n+\t(vst4q_bf16): New.\n+\n 2020-02-25  Mihail Ionescu  <mihail.ionescu@arm.com>\n \n \t* config/aarch64/iterators.md (VDQF_F16) Add V4BF and V8BF."}, {"sha": "9c9c6d86ae29fcbcf42e84408c5e94990fed8348", "filename": "gcc/config/aarch64/aarch64-builtins.c", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e603cd43b145c426468c95cf85b3c12c94daedaa/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e603cd43b145c426468c95cf85b3c12c94daedaa/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c?ref=e603cd43b145c426468c95cf85b3c12c94daedaa", "patch": "@@ -370,6 +370,12 @@ aarch64_types_storestruct_lane_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n #define VAR14(T, X, MAP, A, B, C, D, E, F, G, H, I, J, K, L, M, N) \\\n   VAR13 (T, X, MAP, A, B, C, D, E, F, G, H, I, J, K, L, M) \\\n   VAR1 (T, X, MAP, N)\n+#define VAR15(T, X, MAP, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O) \\\n+  VAR14 (T, X, MAP, A, B, C, D, E, F, G, H, I, J, K, L, M, N) \\\n+  VAR1 (T, X, MAP, O)\n+#define VAR16(T, X, MAP, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P) \\\n+  VAR15 (T, X, MAP, A, B, C, D, E, F, G, H, I, J, K, L, M, N, O) \\\n+  VAR1 (T, X, MAP, P)\n \n #include \"aarch64-builtin-iterators.h\"\n \n@@ -534,6 +540,7 @@ const char *aarch64_scalar_builtin_types[] = {\n   \"__builtin_aarch64_simd_oi\",\n   \"__builtin_aarch64_simd_ci\",\n   \"__builtin_aarch64_simd_xi\",\n+  \"__builtin_aarch64_simd_bf\",\n   NULL\n };\n \n@@ -847,6 +854,8 @@ aarch64_init_simd_builtin_scalar_types (void)\n \t\t\t\t\t     \"__builtin_aarch64_simd_poly128\");\n   (*lang_hooks.types.register_builtin_type) (intTI_type_node,\n \t\t\t\t\t     \"__builtin_aarch64_simd_ti\");\n+  (*lang_hooks.types.register_builtin_type) (aarch64_bf16_type_node,\n+\t\t\t\t\t     \"__builtin_aarch64_simd_bf\");\n   /* Unsigned integer types for various mode sizes.  */\n   (*lang_hooks.types.register_builtin_type) (unsigned_intQI_type_node,\n \t\t\t\t\t     \"__builtin_aarch64_simd_uqi\");"}, {"sha": "b6f42ac630295d9b827e2763cf487ccfb5bfe64b", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 479, "deletions": 0, "changes": 479, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e603cd43b145c426468c95cf85b3c12c94daedaa/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e603cd43b145c426468c95cf85b3c12c94daedaa/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=e603cd43b145c426468c95cf85b3c12c94daedaa", "patch": "@@ -76,6 +76,36 @@ typedef double float64_t;\n typedef __Bfloat16x4_t bfloat16x4_t;\n typedef __Bfloat16x8_t bfloat16x8_t;\n \n+typedef struct bfloat16x4x2_t\n+{\n+  bfloat16x4_t val[2];\n+} bfloat16x4x2_t;\n+\n+typedef struct bfloat16x8x2_t\n+{\n+  bfloat16x8_t val[2];\n+} bfloat16x8x2_t;\n+\n+typedef struct bfloat16x4x3_t\n+{\n+  bfloat16x4_t val[3];\n+} bfloat16x4x3_t;\n+\n+typedef struct bfloat16x8x3_t\n+{\n+  bfloat16x8_t val[3];\n+} bfloat16x8x3_t;\n+\n+typedef struct bfloat16x4x4_t\n+{\n+  bfloat16x4_t val[4];\n+} bfloat16x4x4_t;\n+\n+typedef struct bfloat16x8x4_t\n+{\n+  bfloat16x8_t val[4];\n+} bfloat16x8x4_t;\n+\n typedef struct int8x8x2_t\n {\n   int8x8_t val[2];\n@@ -34589,6 +34619,13 @@ vcreate_bf16 (uint64_t __a)\n   return (bfloat16x4_t) __a;\n }\n \n+__extension__ extern __inline bfloat16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vcombine_bf16 (bfloat16x4_t __a, bfloat16x4_t __b)\n+{\n+  return (bfloat16x8_t)__builtin_aarch64_combinev4bf (__a, __b);\n+}\n+\n /* vdup */\n \n __extension__ extern __inline bfloat16x4_t\n@@ -34647,6 +34684,448 @@ vduph_laneq_bf16 (bfloat16x8_t __a, const int __b)\n   return __aarch64_vget_lane_any (__a, __b);\n }\n \n+/* vld */\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_bf16 (const bfloat16_t *__a)\n+{\n+  return (bfloat16x4_t) __builtin_aarch64_ld1v4bf (__a);\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_bf16 (const bfloat16_t *__a)\n+{\n+  return __builtin_aarch64_ld1v8bf (__a);\n+}\n+\n+__extension__ extern __inline bfloat16x4x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_bf16_x2 (const bfloat16_t *__a)\n+{\n+  bfloat16x4x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld1x2v4bf ((const __builtin_aarch64_simd_bf *) __a);\n+  ret.val[0] = (bfloat16x4_t) __builtin_aarch64_get_dregoiv4bf (__o, 0);\n+  ret.val[1] = (bfloat16x4_t) __builtin_aarch64_get_dregoiv4bf (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ extern __inline bfloat16x8x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_bf16_x2 (const bfloat16_t *__a)\n+{\n+  bfloat16x8x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld1x2v8bf ((const __builtin_aarch64_simd_bf *) __a);\n+  ret.val[0] = (bfloat16x8_t) __builtin_aarch64_get_qregoiv8bf (__o, 0);\n+  ret.val[1] = (bfloat16x8_t) __builtin_aarch64_get_qregoiv8bf (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ extern __inline bfloat16x4x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_bf16_x3 (const bfloat16_t *__a)\n+{\n+  bfloat16x4x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v4bf ((const __builtin_aarch64_simd_bf *) __a);\n+  __i.val[0] = (bfloat16x4_t) __builtin_aarch64_get_dregciv4bf  (__o, 0);\n+  __i.val[1] = (bfloat16x4_t) __builtin_aarch64_get_dregciv4bf  (__o, 1);\n+  __i.val[2] = (bfloat16x4_t) __builtin_aarch64_get_dregciv4bf  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline bfloat16x8x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_bf16_x3 (const bfloat16_t *__a)\n+{\n+  bfloat16x8x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v8bf ((const __builtin_aarch64_simd_bf *) __a);\n+  __i.val[0] = (bfloat16x8_t) __builtin_aarch64_get_qregciv8bf  (__o, 0);\n+  __i.val[1] = (bfloat16x8_t) __builtin_aarch64_get_qregciv8bf  (__o, 1);\n+  __i.val[2] = (bfloat16x8_t) __builtin_aarch64_get_qregciv8bf  (__o, 2);\n+  return __i;\n+}\n+__extension__ extern __inline bfloat16x4x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_bf16_x4 (const bfloat16_t *__a)\n+{\n+  union { bfloat16x4x4_t __i; __builtin_aarch64_simd_xi __o; } __au;\n+  __au.__o\n+    = __builtin_aarch64_ld1x4v4bf ((const __builtin_aarch64_simd_bf *) __a);\n+  return __au.__i;\n+}\n+\n+__extension__ extern __inline bfloat16x8x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_bf16_x4 (const bfloat16_t *__a)\n+{\n+  union { bfloat16x8x4_t __i; __builtin_aarch64_simd_xi __o; } __au;\n+  __au.__o\n+    = __builtin_aarch64_ld1x4v8bf ((const __builtin_aarch64_simd_bf *) __a);\n+  return __au.__i;\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_lane_bf16 (const bfloat16_t *__src, bfloat16x4_t __vec, const int __lane)\n+{\n+  return __aarch64_vset_lane_any (*__src, __vec, __lane);\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_lane_bf16 (const bfloat16_t *__src, bfloat16x8_t __vec, const int __lane)\n+{\n+  return __aarch64_vset_lane_any (*__src, __vec, __lane);\n+}\n+\n+__extension__ extern __inline bfloat16x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_dup_bf16 (const bfloat16_t* __a)\n+{\n+  return vdup_n_bf16 (*__a);\n+}\n+\n+__extension__ extern __inline bfloat16x8_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_dup_bf16 (const bfloat16_t* __a)\n+{\n+  return vdupq_n_bf16 (*__a);\n+}\n+\n+__extension__ extern __inline bfloat16x4x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld2_bf16 (const bfloat16_t * __a)\n+{\n+  bfloat16x4x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2v4bf (__a);\n+  ret.val[0] = (bfloat16x4_t) __builtin_aarch64_get_dregoiv4bf (__o, 0);\n+  ret.val[1] = (bfloat16x4_t) __builtin_aarch64_get_dregoiv4bf (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ extern __inline bfloat16x8x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld2q_bf16 (const bfloat16_t * __a)\n+{\n+  bfloat16x8x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2v8bf ((const __builtin_aarch64_simd_bf *) __a);\n+  ret.val[0] = (bfloat16x8_t) __builtin_aarch64_get_qregoiv8bf (__o, 0);\n+  ret.val[1] = (bfloat16x8_t) __builtin_aarch64_get_qregoiv8bf (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ extern __inline bfloat16x4x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld2_dup_bf16 (const bfloat16_t * __a)\n+{\n+  bfloat16x4x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv4bf ((const __builtin_aarch64_simd_bf *) __a);\n+  ret.val[0] = (bfloat16x4_t) __builtin_aarch64_get_dregoiv4bf (__o, 0);\n+  ret.val[1] = (bfloat16x4_t) __builtin_aarch64_get_dregoiv4bf (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ extern __inline bfloat16x8x2_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld2q_dup_bf16 (const bfloat16_t * __a)\n+{\n+  bfloat16x8x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv8bf ((const __builtin_aarch64_simd_bf *) __a);\n+  ret.val[0] = (bfloat16x8_t) __builtin_aarch64_get_qregoiv8bf (__o, 0);\n+  ret.val[1] = (bfloat16x8_t) __builtin_aarch64_get_qregoiv8bf (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ extern __inline bfloat16x4x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld3_bf16 (const bfloat16_t * __a)\n+{\n+  bfloat16x4x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3v4bf ((const __builtin_aarch64_simd_bf *) __a);\n+  ret.val[0] = (bfloat16x4_t) __builtin_aarch64_get_dregciv4bf (__o, 0);\n+  ret.val[1] = (bfloat16x4_t) __builtin_aarch64_get_dregciv4bf (__o, 1);\n+  ret.val[2] = (bfloat16x4_t) __builtin_aarch64_get_dregciv4bf (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ extern __inline bfloat16x8x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld3q_bf16 (const bfloat16_t * __a)\n+{\n+  bfloat16x8x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3v8bf ((const __builtin_aarch64_simd_bf *) __a);\n+  ret.val[0] = (bfloat16x8_t) __builtin_aarch64_get_qregciv8bf (__o, 0);\n+  ret.val[1] = (bfloat16x8_t) __builtin_aarch64_get_qregciv8bf (__o, 1);\n+  ret.val[2] = (bfloat16x8_t) __builtin_aarch64_get_qregciv8bf (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ extern __inline bfloat16x4x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld3_dup_bf16 (const bfloat16_t * __a)\n+{\n+  bfloat16x4x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv4bf ((const __builtin_aarch64_simd_bf *) __a);\n+  ret.val[0] = (bfloat16x4_t) __builtin_aarch64_get_dregciv4bf (__o, 0);\n+  ret.val[1] = (bfloat16x4_t) __builtin_aarch64_get_dregciv4bf (__o, 1);\n+  ret.val[2] = (bfloat16x4_t) __builtin_aarch64_get_dregciv4bf (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ extern __inline bfloat16x8x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld3q_dup_bf16 (const bfloat16_t * __a)\n+{\n+  bfloat16x8x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv8bf ((const __builtin_aarch64_simd_bf *) __a);\n+  ret.val[0] = (bfloat16x8_t) __builtin_aarch64_get_qregciv8bf (__o, 0);\n+  ret.val[1] = (bfloat16x8_t) __builtin_aarch64_get_qregciv8bf (__o, 1);\n+  ret.val[2] = (bfloat16x8_t) __builtin_aarch64_get_qregciv8bf (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ extern __inline bfloat16x4x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld4_bf16 (const bfloat16_t * __a)\n+{\n+  bfloat16x4x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4v4bf ((const __builtin_aarch64_simd_bf *) __a);\n+  ret.val[0] = (bfloat16x4_t) __builtin_aarch64_get_dregxiv4bf (__o, 0);\n+  ret.val[1] = (bfloat16x4_t) __builtin_aarch64_get_dregxiv4bf (__o, 1);\n+  ret.val[2] = (bfloat16x4_t) __builtin_aarch64_get_dregxiv4bf (__o, 2);\n+  ret.val[3] = (bfloat16x4_t) __builtin_aarch64_get_dregxiv4bf (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline bfloat16x8x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld4q_bf16 (const bfloat16_t * __a)\n+{\n+  bfloat16x8x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4v8bf ((const __builtin_aarch64_simd_bf *) __a);\n+  ret.val[0] = (bfloat16x8_t) __builtin_aarch64_get_qregxiv8bf (__o, 0);\n+  ret.val[1] = (bfloat16x8_t) __builtin_aarch64_get_qregxiv8bf (__o, 1);\n+  ret.val[2] = (bfloat16x8_t) __builtin_aarch64_get_qregxiv8bf (__o, 2);\n+  ret.val[3] = (bfloat16x8_t) __builtin_aarch64_get_qregxiv8bf (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline bfloat16x4x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld4_dup_bf16 (const bfloat16_t * __a)\n+{\n+  bfloat16x4x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv4bf ((const __builtin_aarch64_simd_bf *) __a);\n+  ret.val[0] = (bfloat16x4_t) __builtin_aarch64_get_dregxiv4bf (__o, 0);\n+  ret.val[1] = (bfloat16x4_t) __builtin_aarch64_get_dregxiv4bf (__o, 1);\n+  ret.val[2] = (bfloat16x4_t) __builtin_aarch64_get_dregxiv4bf (__o, 2);\n+  ret.val[3] = (bfloat16x4_t) __builtin_aarch64_get_dregxiv4bf (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ extern __inline bfloat16x8x4_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld4q_dup_bf16 (const bfloat16_t * __a)\n+{\n+  bfloat16x8x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv8bf ((const __builtin_aarch64_simd_bf *) __a);\n+  ret.val[0] = (bfloat16x8_t) __builtin_aarch64_get_qregxiv8bf (__o, 0);\n+  ret.val[1] = (bfloat16x8_t) __builtin_aarch64_get_qregxiv8bf (__o, 1);\n+  ret.val[2] = (bfloat16x8_t) __builtin_aarch64_get_qregxiv8bf (__o, 2);\n+  ret.val[3] = (bfloat16x8_t) __builtin_aarch64_get_qregxiv8bf (__o, 3);\n+  return ret;\n+}\n+\n+/* vst */\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_bf16 (bfloat16_t *__a, bfloat16x4_t __b)\n+{\n+  __builtin_aarch64_st1v4bf (__a, __b);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_bf16_x2 (bfloat16_t * __a, bfloat16x4x2_t __val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  bfloat16x8x2_t __temp;\n+  __temp.val[0] = vcombine_bf16 (__val.val[0], vcreate_bf16 (__AARCH64_UINT64_C (0)));\n+  __temp.val[1] = vcombine_bf16 (__val.val[1], vcreate_bf16 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv8bf (__o, __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv8bf (__o, __temp.val[1], 1);\n+  __builtin_aarch64_st1x2v4bf (__a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_bf16_x2 (bfloat16_t * __a, bfloat16x8x2_t __val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv8bf (__o, __val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv8bf (__o, __val.val[1], 1);\n+  __builtin_aarch64_st1x2v8bf (__a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_bf16_x3 (bfloat16_t * __a, bfloat16x4x3_t __val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  bfloat16x8x3_t __temp;\n+  __temp.val[0] = vcombine_bf16 (__val.val[0], vcreate_bf16 (__AARCH64_UINT64_C (0)));\n+  __temp.val[1] = vcombine_bf16 (__val.val[1], vcreate_bf16 (__AARCH64_UINT64_C (0)));\n+  __temp.val[2] = vcombine_bf16 (__val.val[2], vcreate_bf16 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv8bf (__o, (bfloat16x8_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv8bf (__o, (bfloat16x8_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv8bf (__o, (bfloat16x8_t) __temp.val[2], 2);\n+  __builtin_aarch64_st1x3v4bf ((__builtin_aarch64_simd_bf *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_bf16_x3 (bfloat16_t * __a, bfloat16x8x3_t __val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv8bf (__o, (bfloat16x8_t) __val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv8bf (__o, (bfloat16x8_t) __val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv8bf (__o, (bfloat16x8_t) __val.val[2], 2);\n+  __builtin_aarch64_st1x3v8bf ((__builtin_aarch64_simd_bf *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_bf16_x4 (bfloat16_t * __a, bfloat16x4x4_t val)\n+{\n+  union { bfloat16x4x4_t __i; __builtin_aarch64_simd_xi __o; } __u = { val };\n+  __builtin_aarch64_st1x4v4bf ((__builtin_aarch64_simd_bf *) __a, __u.__o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_bf16_x4 (bfloat16_t * __a, bfloat16x8x4_t val)\n+{\n+  union { bfloat16x8x4_t __i; __builtin_aarch64_simd_xi __o; } __u = { val };\n+  __builtin_aarch64_st1x4v8bf ((__builtin_aarch64_simd_bf *) __a, __u.__o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_bf16 (bfloat16_t *__a, bfloat16x8_t __b)\n+{\n+  __builtin_aarch64_st1v8bf (__a, __b);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_lane_bf16 (bfloat16_t *__a, bfloat16x4_t __b, const int __lane)\n+{\n+  *__a = __aarch64_vget_lane_any (__b, __lane);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_lane_bf16 (bfloat16_t *__a, bfloat16x8_t __b, const int __lane)\n+{\n+  *__a = __aarch64_vget_lane_any (__b, __lane);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst2_bf16 (bfloat16_t * __a, bfloat16x4x2_t __val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  bfloat16x8x2_t __temp;\n+  __temp.val[0] = vcombine_bf16 (__val.val[0], vcreate_bf16 (__AARCH64_UINT64_C (0)));\n+  __temp.val[1] = vcombine_bf16 (__val.val[1], vcreate_bf16 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv8bf (__o, __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv8bf (__o, __temp.val[1], 1);\n+  __builtin_aarch64_st2v4bf (__a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst2q_bf16 (bfloat16_t * __a, bfloat16x8x2_t __val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv8bf (__o, __val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv8bf (__o, __val.val[1], 1);\n+  __builtin_aarch64_st2v8bf (__a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst3_bf16 (bfloat16_t * __a, bfloat16x4x3_t __val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  bfloat16x8x3_t __temp;\n+  __temp.val[0] = vcombine_bf16 (__val.val[0], vcreate_bf16 (__AARCH64_UINT64_C (0)));\n+  __temp.val[1] = vcombine_bf16 (__val.val[1], vcreate_bf16 (__AARCH64_UINT64_C (0)));\n+  __temp.val[2] = vcombine_bf16 (__val.val[2], vcreate_bf16 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv8bf (__o, (bfloat16x8_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv8bf (__o, (bfloat16x8_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv8bf (__o, (bfloat16x8_t) __temp.val[2], 2);\n+  __builtin_aarch64_st3v4bf ((__builtin_aarch64_simd_bf *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst3q_bf16 (bfloat16_t * __a, bfloat16x8x3_t __val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv8bf (__o, (bfloat16x8_t) __val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv8bf (__o, (bfloat16x8_t) __val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv8bf (__o, (bfloat16x8_t) __val.val[2], 2);\n+  __builtin_aarch64_st3v8bf ((__builtin_aarch64_simd_bf *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst4_bf16 (bfloat16_t * __a, bfloat16x4x4_t __val)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  bfloat16x8x4_t __temp;\n+  __temp.val[0] = vcombine_bf16 (__val.val[0], vcreate_bf16 (__AARCH64_UINT64_C (0)));\n+  __temp.val[1] = vcombine_bf16 (__val.val[1], vcreate_bf16 (__AARCH64_UINT64_C (0)));\n+  __temp.val[2] = vcombine_bf16 (__val.val[2], vcreate_bf16 (__AARCH64_UINT64_C (0)));\n+  __temp.val[3] = vcombine_bf16 (__val.val[3], vcreate_bf16 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregxiv8bf (__o, (bfloat16x8_t) __temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv8bf (__o, (bfloat16x8_t) __temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv8bf (__o, (bfloat16x8_t) __temp.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv8bf (__o, (bfloat16x8_t) __temp.val[3], 3);\n+  __builtin_aarch64_st4v4bf ((__builtin_aarch64_simd_bf *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst4q_bf16 (bfloat16_t * __a, bfloat16x8x4_t __val)\n+{\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_set_qregxiv8bf (__o, (bfloat16x8_t) __val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregxiv8bf (__o, (bfloat16x8_t) __val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregxiv8bf (__o, (bfloat16x8_t) __val.val[2], 2);\n+  __o = __builtin_aarch64_set_qregxiv8bf (__o, (bfloat16x8_t) __val.val[3], 3);\n+  __builtin_aarch64_st4v8bf ((__builtin_aarch64_simd_bf *) __a, __o);\n+}\n+\n /* vreinterpret */\n \n __extension__ extern __inline bfloat16x4_t"}, {"sha": "ec1b92c5379f7c33446d0ac3556f6358fb7433d3", "filename": "gcc/config/aarch64/iterators.md", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e603cd43b145c426468c95cf85b3c12c94daedaa/gcc%2Fconfig%2Faarch64%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e603cd43b145c426468c95cf85b3c12c94daedaa/gcc%2Fconfig%2Faarch64%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fiterators.md?ref=e603cd43b145c426468c95cf85b3c12c94daedaa", "patch": "@@ -87,7 +87,7 @@\n (define_mode_iterator VSDQ_I_DI [V8QI V16QI V4HI V8HI V2SI V4SI V2DI DI])\n \n ;; Double vector modes.\n-(define_mode_iterator VD [V8QI V4HI V4HF V2SI V2SF])\n+(define_mode_iterator VD [V8QI V4HI V4HF V2SI V2SF V4BF])\n \n ;; Double vector modes suitable for moving.  Includes BFmode.\n (define_mode_iterator VDMOV [V8QI V4HI V4HF V4BF V2SI V2SF])\n@@ -105,10 +105,10 @@\n (define_mode_iterator VDQ_BHSI [V8QI V16QI V4HI V8HI V2SI V4SI])\n \n ;; Quad vector modes.\n-(define_mode_iterator VQ [V16QI V8HI V4SI V2DI V8HF V4SF V2DF])\n+(define_mode_iterator VQ [V16QI V8HI V4SI V2DI V8HF V4SF V2DF V8BF])\n \n ;; Copy of the above.\n-(define_mode_iterator VQ2 [V16QI V8HI V4SI V2DI V8HF V4SF V2DF])\n+(define_mode_iterator VQ2 [V16QI V8HI V4SI V2DI V8HF V8BF V4SF V2DF])\n \n ;; Quad vector modes suitable for moving.  Includes BFmode.\n (define_mode_iterator VQMOV [V16QI V8HI V4SI V2DI V8HF V8BF V4SF V2DF])\n@@ -120,7 +120,7 @@\n (define_mode_iterator VQ_I [V16QI V8HI V4SI V2DI])\n \n ;; VQ without 2 element modes.\n-(define_mode_iterator VQ_NO2E [V16QI V8HI V4SI V8HF V4SF])\n+(define_mode_iterator VQ_NO2E [V16QI V8HI V4SI V8HF V4SF V8BF])\n \n ;; Quad vector with only 2 element modes.\n (define_mode_iterator VQ_2E [V2DI V2DF])\n@@ -200,7 +200,7 @@\n \t\t\t\t  V4HF V8HF V4BF V8BF V2SF V4SF V2DF DI])\n \n ;; All Advanced SIMD modes, plus DI and DF.\n-(define_mode_iterator VALLDIF [V8QI V16QI V4HI V8HI V2SI V4SI\n+(define_mode_iterator VALLDIF [V8QI V16QI V4HI V8HI V2SI V4SI V4BF V8BF\n \t\t\t       V2DI V4HF V8HF V2SF V4SF V2DF DI DF])\n \n ;; Advanced SIMD modes for Integer reduction across lanes.\n@@ -226,7 +226,7 @@\n (define_mode_iterator VQW [V16QI V8HI V4SI])\n \n ;; Double vector modes for combines.\n-(define_mode_iterator VDC [V8QI V4HI V4HF V2SI V2SF DI DF])\n+(define_mode_iterator VDC [V8QI V4HI V4BF V4HF V2SI V2SF DI DF])\n \n ;; Advanced SIMD modes except double int.\n (define_mode_iterator VDQIF [V8QI V16QI V4HI V8HI V2SI V4SI V2SF V4SF V2DF])\n@@ -1171,7 +1171,7 @@\n \n ;; Double modes of vector modes.\n (define_mode_attr VDBL [(V8QI \"V16QI\") (V4HI \"V8HI\")\n-\t\t\t(V4HF \"V8HF\")\n+\t\t\t(V4HF \"V8HF\")  (V4BF \"V8BF\")\n \t\t\t(V2SI \"V4SI\")  (V2SF \"V4SF\")\n \t\t\t(SI   \"V2SI\")  (DI   \"V2DI\")\n \t\t\t(DF   \"V2DF\")])\n@@ -1181,7 +1181,7 @@\n \n ;; Double modes of vector modes (lower case).\n (define_mode_attr Vdbl [(V8QI \"v16qi\") (V4HI \"v8hi\")\n-\t\t\t(V4HF \"v8hf\")\n+\t\t\t(V4HF \"v8hf\")  (V4BF \"v8bf\")\n \t\t\t(V2SI \"v4si\")  (V2SF \"v4sf\")\n \t\t\t(SI   \"v2si\")  (DI   \"v2di\")\n \t\t\t(DF   \"v2df\")])\n@@ -1314,6 +1314,7 @@\n \t\t\t       (V2SI \"V2SI\") (V4SI  \"V4SI\")\n \t\t\t       (DI   \"DI\")   (V2DI  \"V2DI\")\n \t\t\t       (V4HF \"V4HI\") (V8HF  \"V8HI\")\n+\t\t\t       (V4BF \"V4HI\") (V8BF  \"V8HI\")\n \t\t\t       (V2SF \"V2SI\") (V4SF  \"V4SI\")\n \t\t\t       (DF   \"DI\")   (V2DF  \"V2DI\")\n \t\t\t       (SF   \"SI\")   (SI    \"SI\")\n@@ -1331,6 +1332,7 @@\n \t\t\t       (V2SI \"v2si\") (V4SI  \"v4si\")\n \t\t\t       (DI   \"di\")   (V2DI  \"v2di\")\n \t\t\t       (V4HF \"v4hi\") (V8HF  \"v8hi\")\n+\t\t\t       (V4BF \"v4hi\") (V8BF  \"v8hi\")\n \t\t\t       (V2SF \"v2si\") (V4SF  \"v4si\")\n \t\t\t       (DF   \"di\")   (V2DF  \"v2di\")\n \t\t\t       (SF   \"si\")"}, {"sha": "f34437641967f0240036e2ee905d7ded30f05165", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e603cd43b145c426468c95cf85b3c12c94daedaa/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e603cd43b145c426468c95cf85b3c12c94daedaa/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=e603cd43b145c426468c95cf85b3c12c94daedaa", "patch": "@@ -1,3 +1,8 @@\n+2020-02-25  Mihail Ionescu  <mihail.ionescu@arm.com>\n+\n+\t* gcc.target/aarch64/advsimd-intrinsics/bf16_vstn.c: New test.\n+\t* gcc.target/aarch64/advsimd-intrinsics/bf16_vldn.c: New test.\n+\n 2020-02-25  Mihail Ionescu  <mihail.ionescu@arm.com>\n \n \t* gcc.target/aarch64/advsimd-intrinsics/bf16_dup.c: New test."}, {"sha": "cf245091af658f58659d3c34dfe9f164c392fcd9", "filename": "gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/bf16_vldn.c", "status": "added", "additions": 150, "deletions": 0, "changes": 150, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e603cd43b145c426468c95cf85b3c12c94daedaa/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_vldn.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e603cd43b145c426468c95cf85b3c12c94daedaa/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_vldn.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_vldn.c?ref=e603cd43b145c426468c95cf85b3c12c94daedaa", "patch": "@@ -0,0 +1,150 @@\n+/* { dg-do assemble { target { aarch64*-*-* } } } */\n+/* { dg-skip-if \"\" { *-*-* } { \"-fno-fat-lto-objects\" } } */\n+/* { dg-require-effective-target arm_v8_2a_bf16_neon_ok } */\n+/* { dg-add-options arm_v8_2a_bf16_neon }  */\n+\n+#include <arm_neon.h>\n+\n+bfloat16x4_t\n+test_vld1_dup_bf16 (bfloat16_t * ptr)\n+{\n+  return vld1_dup_bf16 (ptr);\n+}\n+\n+bfloat16x8_t\n+test_vld1q_dup_bf16 (bfloat16_t * ptr)\n+{\n+  return vld1q_dup_bf16 (ptr);\n+}\n+\n+bfloat16x4_t\n+test_vld1_lane_bf16 (bfloat16_t * ptr, bfloat16x4_t src)\n+{\n+  return vld1_lane_bf16 (ptr, src, 3);\n+}\n+\n+bfloat16x8_t\n+test_vld1q_lane_bf16 (bfloat16_t * ptr, bfloat16x8_t src)\n+{\n+  return vld1q_lane_bf16 (ptr, src, 7);\n+}\n+\n+bfloat16x4_t\n+test_vld1_bf16 (bfloat16_t * ptr)\n+{\n+  return vld1_bf16 (ptr);\n+}\n+\n+bfloat16x8_t\n+test_vld1q_bf16 (bfloat16_t * ptr)\n+{\n+  return vld1q_bf16 (ptr);\n+}\n+\n+bfloat16x4x2_t\n+test_vld1_bf16_x2 (bfloat16_t * ptr)\n+{\n+  return vld1_bf16_x2 (ptr);\n+}\n+\n+bfloat16x8x2_t\n+test_vld1q_bf16_x2 (bfloat16_t * ptr)\n+{\n+  return vld1q_bf16_x2 (ptr);\n+}\n+\n+bfloat16x4x3_t\n+test_vld1_bf16_x3 (bfloat16_t * ptr)\n+{\n+  return vld1_bf16_x3 (ptr);\n+}\n+\n+bfloat16x8x3_t\n+test_vld1q_bf16_x3 (bfloat16_t * ptr)\n+{\n+  return vld1q_bf16_x3 (ptr);\n+}\n+\n+bfloat16x4x4_t\n+test_vld1_bf16_x4 (bfloat16_t * ptr)\n+{\n+  return vld1_bf16_x4 (ptr);\n+}\n+\n+bfloat16x8x4_t\n+test_vld1q_bf16_x4 (bfloat16_t * ptr)\n+{\n+  return vld1q_bf16_x4 (ptr);\n+}\n+\n+bfloat16x4x2_t\n+test_vld2_bf16 (bfloat16_t * ptr)\n+{\n+  return vld2_bf16 (ptr);\n+}\n+\n+bfloat16x8x2_t\n+test_vld2q_bf16 (bfloat16_t * ptr)\n+{\n+  return vld2q_bf16 (ptr);\n+}\n+\n+bfloat16x4x2_t\n+test_vld2_dup_bf16 (bfloat16_t * ptr)\n+{\n+  return vld2_dup_bf16 (ptr);\n+}\n+\n+bfloat16x8x2_t\n+test_vld2q_dup_bf16 (bfloat16_t * ptr)\n+{\n+  return vld2q_dup_bf16 (ptr);\n+}\n+\n+bfloat16x4x3_t\n+test_vld3_bf16 (bfloat16_t * ptr)\n+{\n+  return vld3_bf16 (ptr);\n+}\n+\n+bfloat16x8x3_t\n+test_vld3q_bf16 (bfloat16_t * ptr)\n+{\n+  return vld3q_bf16 (ptr);\n+}\n+\n+bfloat16x4x3_t\n+test_vld3_dup_bf16 (bfloat16_t * ptr)\n+{\n+  return vld3_dup_bf16 (ptr);\n+}\n+\n+bfloat16x8x3_t\n+test_vld3q_dup_bf16 (bfloat16_t * ptr)\n+{\n+  return vld3q_dup_bf16 (ptr);\n+}\n+\n+bfloat16x4x4_t\n+test_vld4_bf16 (bfloat16_t * ptr)\n+{\n+ return vld4_bf16 (ptr);\n+}\n+\n+bfloat16x8x4_t\n+test_vld4q_bf16 (bfloat16_t * ptr)\n+{\n+ return vld4q_bf16 (ptr);\n+}\n+\n+bfloat16x4x4_t\n+test_vld4_dup_bf16 (bfloat16_t * ptr)\n+{\n+  return vld4_dup_bf16 (ptr);\n+}\n+\n+bfloat16x8x4_t\n+test_vld4q_dup_bf16 (bfloat16_t * ptr)\n+{\n+  return vld4q_dup_bf16 (ptr);\n+}"}, {"sha": "162b3ee36ddda0a4884340003acc071105b973c1", "filename": "gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/bf16_vstn.c", "status": "added", "additions": 107, "deletions": 0, "changes": 107, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e603cd43b145c426468c95cf85b3c12c94daedaa/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_vstn.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e603cd43b145c426468c95cf85b3c12c94daedaa/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_vstn.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fbf16_vstn.c?ref=e603cd43b145c426468c95cf85b3c12c94daedaa", "patch": "@@ -0,0 +1,107 @@\n+/* { dg-do assemble { target { aarch64*-*-* } } } */\n+/* { dg-skip-if \"\" { *-*-* } { \"-fno-fat-lto-objects\" } } */\n+/* { dg-require-effective-target arm_v8_2a_bf16_neon_ok } */\n+/* { dg-add-options arm_v8_2a_bf16_neon }  */\n+\n+#include <arm_neon.h>\n+\n+void\n+test_vst1_bf16_x2 (bfloat16_t *ptr, bfloat16x4x2_t val)\n+{\n+  vst1_bf16_x2 (ptr, val);\n+}\n+\n+void\n+test_vst1q_bf16_x2 (bfloat16_t *ptr, bfloat16x8x2_t val)\n+{\n+  vst1q_bf16_x2 (ptr, val);\n+}\n+\n+void\n+test_vst1_bf16_x3 (bfloat16_t *ptr, bfloat16x4x3_t val)\n+{\n+  vst1_bf16_x3 (ptr, val);\n+}\n+\n+void\n+test_vst1q_bf16_x3 (bfloat16_t *ptr, bfloat16x8x3_t val)\n+{\n+  vst1q_bf16_x3 (ptr, val);\n+}\n+\n+void\n+test_vst1_bf16_x4 (bfloat16_t *ptr, bfloat16x4x4_t val)\n+{\n+  vst1_bf16_x4 (ptr, val);\n+}\n+\n+void\n+test_vst1q_bf16_x4 (bfloat16_t *ptr, bfloat16x8x4_t val)\n+{\n+  vst1q_bf16_x4 (ptr, val);\n+}\n+\n+void\n+test_vst1_lane_bf16 (bfloat16_t *ptr, bfloat16x4_t val)\n+{\n+  vst1_lane_bf16 (ptr, val, 3);\n+}\n+\n+void\n+test_vst1q_lane_bf16 (bfloat16_t *ptr, bfloat16x8_t val)\n+{\n+  vst1q_lane_bf16 (ptr, val, 7);\n+}\n+\n+void\n+test_vst1_bf16 (bfloat16_t *ptr, bfloat16x4_t val)\n+{\n+  vst1_bf16 (ptr, val);\n+}\n+\n+void\n+test_vst1q_bf16 (bfloat16_t *ptr, bfloat16x8_t val)\n+{\n+  vst1q_bf16 (ptr, val);\n+}\n+\n+void\n+test_vst2_bf16 (bfloat16_t *ptr, bfloat16x4x2_t val)\n+{\n+  vst2_bf16 (ptr, val);\n+}\n+\n+void\n+test_vst2q_bf16 (bfloat16_t *ptr, bfloat16x8x2_t val)\n+{\n+  vst2q_bf16 (ptr, val);\n+}\n+\n+void\n+test_vst3_bf16 (bfloat16_t *ptr, bfloat16x4x3_t val)\n+{\n+  vst3_bf16 (ptr, val);\n+}\n+\n+void\n+test_vst3q_bf16 (bfloat16_t *ptr, bfloat16x8x3_t val)\n+{\n+  vst3q_bf16 (ptr, val);\n+}\n+\n+void\n+test_vst4_bf16 (bfloat16_t *ptr, bfloat16x4x4_t val)\n+{\n+  vst4_bf16 (ptr, val);\n+}\n+\n+void\n+test_vst4q_bf16 (bfloat16_t *ptr, bfloat16x8x4_t val)\n+{\n+  vst4q_bf16 (ptr, val);\n+}\n+\n+int main()\n+{\n+  return 0;\n+}"}]}