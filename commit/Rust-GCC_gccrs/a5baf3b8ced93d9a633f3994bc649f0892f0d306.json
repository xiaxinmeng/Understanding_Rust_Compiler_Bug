{"sha": "a5baf3b8ced93d9a633f3994bc649f0892f0d306", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YTViYWYzYjhjZWQ5M2Q5YTYzM2YzOTk0YmM2NDlmMDg5MmYwZDMwNg==", "commit": {"author": {"name": "Diego Novillo", "email": "dnovillo@google.com", "date": "2011-09-13T20:24:47Z"}, "committer": {"name": "Diego Novillo", "email": "dnovillo@gcc.gnu.org", "date": "2011-09-13T20:24:47Z"}, "message": "testsuite-management: New.\n\n\t* testsuite-management: New.\n\t* testsuite-management/validate_failures.py: New.\n\nFrom-SVN: r178833", "tree": {"sha": "6eb4f1e97b0294516e01bdc66ed46217678d8585", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/6eb4f1e97b0294516e01bdc66ed46217678d8585"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/a5baf3b8ced93d9a633f3994bc649f0892f0d306", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a5baf3b8ced93d9a633f3994bc649f0892f0d306", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a5baf3b8ced93d9a633f3994bc649f0892f0d306", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a5baf3b8ced93d9a633f3994bc649f0892f0d306/comments", "author": {"login": "dnovillo", "id": 7295335, "node_id": "MDQ6VXNlcjcyOTUzMzU=", "avatar_url": "https://avatars.githubusercontent.com/u/7295335?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dnovillo", "html_url": "https://github.com/dnovillo", "followers_url": "https://api.github.com/users/dnovillo/followers", "following_url": "https://api.github.com/users/dnovillo/following{/other_user}", "gists_url": "https://api.github.com/users/dnovillo/gists{/gist_id}", "starred_url": "https://api.github.com/users/dnovillo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dnovillo/subscriptions", "organizations_url": "https://api.github.com/users/dnovillo/orgs", "repos_url": "https://api.github.com/users/dnovillo/repos", "events_url": "https://api.github.com/users/dnovillo/events{/privacy}", "received_events_url": "https://api.github.com/users/dnovillo/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "94c4133ab492acbebe8bb603175b155e7b4ead3a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/94c4133ab492acbebe8bb603175b155e7b4ead3a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/94c4133ab492acbebe8bb603175b155e7b4ead3a"}], "stats": {"total": 342, "additions": 342, "deletions": 0}, "files": [{"sha": "eddf6ecfe58a5192c147c43da070b3b5e730de61", "filename": "contrib/ChangeLog", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a5baf3b8ced93d9a633f3994bc649f0892f0d306/contrib%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a5baf3b8ced93d9a633f3994bc649f0892f0d306/contrib%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2FChangeLog?ref=a5baf3b8ced93d9a633f3994bc649f0892f0d306", "patch": "@@ -1,3 +1,8 @@\n+2011-09-13   Diego Novillo  <dnovillo@google.com>\n+\n+\t* testsuite-management: New.\n+\t* testsuite-management/validate_failures.py: New.\n+\n 2011-08-25  Rainer Orth  <ro@CeBiTec.Uni-Bielefeld.DE>\n \n \t* gcc_update: Determine svn branch from hg convert_revision."}, {"sha": "be2ffcee5c6d512bc72172f4204241eeb6160eea", "filename": "contrib/testsuite-management/validate_failures.py", "status": "added", "additions": 337, "deletions": 0, "changes": 337, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a5baf3b8ced93d9a633f3994bc649f0892f0d306/contrib%2Ftestsuite-management%2Fvalidate_failures.py", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a5baf3b8ced93d9a633f3994bc649f0892f0d306/contrib%2Ftestsuite-management%2Fvalidate_failures.py", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2Ftestsuite-management%2Fvalidate_failures.py?ref=a5baf3b8ced93d9a633f3994bc649f0892f0d306", "patch": "@@ -0,0 +1,337 @@\n+#!/usr/bin/python\n+\n+# Script to compare testsuite failures against a list of known-to-fail\n+# tests.\n+\n+# Contributed by Diego Novillo <dnovillo@google.com>\n+#\n+# Copyright (C) 2011 Free Software Foundation, Inc.\n+#\n+# This file is part of GCC.\n+#\n+# GCC is free software; you can redistribute it and/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation; either version 3, or (at your option)\n+# any later version.\n+#\n+# GCC is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+#\n+# You should have received a copy of the GNU General Public License\n+# along with GCC; see the file COPYING.  If not, write to\n+# the Free Software Foundation, 51 Franklin Street, Fifth Floor,\n+# Boston, MA 02110-1301, USA.\n+\n+\"\"\"This script provides a coarser XFAILing mechanism that requires no\n+detailed DejaGNU markings.  This is useful in a variety of scenarios:\n+\n+- Development branches with many known failures waiting to be fixed.\n+- Release branches with known failures that are not considered\n+  important for the particular release criteria used in that branch.\n+\n+The script must be executed from the toplevel build directory.  When\n+executed it will:\n+\n+1- Determine the target built: TARGET\n+2- Determine the source directory: SRCDIR\n+3- Look for a failure manifest file in\n+   <SRCDIR>/contrib/testsuite-management/<TARGET>.xfail\n+4- Collect all the <tool>.sum files from the build tree.\n+5- Produce a report stating:\n+   a- Failures expected in the manifest but not present in the build.\n+   b- Failures in the build not expected in the manifest.\n+6- If all the build failures are expected in the manifest, it exits\n+   with exit code 0.  Otherwise, it exits with error code 1.\n+\"\"\"\n+\n+import optparse\n+import os\n+import re\n+import sys\n+\n+# Handled test results.\n+_VALID_TEST_RESULTS = [ 'FAIL', 'UNRESOLVED', 'XPASS', 'ERROR' ]\n+\n+# Pattern for naming manifest files.  The first argument should be\n+# the toplevel GCC source directory.  The second argument is the\n+# target triple used during the build.\n+_MANIFEST_PATH_PATTERN = '%s/contrib/testsuite-management/%s.xfail'\n+\n+def Error(msg):\n+  print >>sys.stderr, '\\nerror: %s' % msg\n+  sys.exit(1)\n+\n+\n+class TestResult(object):\n+  \"\"\"Describes a single DejaGNU test result as emitted in .sum files.\n+\n+  We are only interested in representing unsuccessful tests.  So, only\n+  a subset of all the tests are loaded.\n+\n+  The summary line used to build the test result should have this format:\n+\n+  attrlist | XPASS: gcc.dg/unroll_1.c (test for excess errors)\n+  ^^^^^^^^   ^^^^^  ^^^^^^^^^^^^^^^^^ ^^^^^^^^^^^^^^^^^^^^^^^^\n+  optional   state  name              description\n+  attributes\n+\n+  Attributes:\n+    attrlist: A comma separated list of attributes.\n+      Valid values:\n+        flaky            Indicates that this test may not always fail.  These\n+                         tests are reported, but their presence does not affect\n+                         the results.\n+\n+        expire=YYYYMMDD  After this date, this test will produce an error\n+                         whether it is in the manifest or not.\n+\n+    state: One of UNRESOLVED, XPASS or FAIL.\n+    name: File name for the test.\n+    description: String describing the test (flags used, dejagnu message, etc)\n+  \"\"\"\n+\n+  def __init__(self, summary_line):\n+    try:\n+      self.attrs = ''\n+      if '|' in summary_line:\n+        (self.attrs, summary_line) = summary_line.split('|', 1)\n+      (self.state,\n+       self.name,\n+       self.description) = re.match(r' *([A-Z]+): ([^ ]+) (.*)',\n+                                    summary_line).groups()\n+      self.attrs = self.attrs.strip()\n+      self.state = self.state.strip()\n+      self.description = self.description.strip()\n+    except ValueError:\n+      Error('Cannot parse summary line \"%s\"' % summary_line)\n+\n+    if self.state not in _VALID_TEST_RESULTS:\n+      Error('Invalid test result %s in \"%s\" (parsed as \"%s\")' % (\n+            self.state, summary_line, self))\n+\n+  def __lt__(self, other):\n+    return self.name < other.name\n+\n+  def __hash__(self):\n+    return hash(self.state) ^ hash(self.name) ^ hash(self.description)\n+\n+  def __eq__(self, other):\n+    return (self.state == other.state and\n+            self.name == other.name and\n+            self.description == other.description)\n+\n+  def __ne__(self, other):\n+    return not (self == other)\n+\n+  def __str__(self):\n+    attrs = ''\n+    if self.attrs:\n+      attrs = '%s | ' % self.attrs\n+    return '%s%s: %s %s' % (attrs, self.state, self.name, self.description)\n+\n+\n+def GetMakefileValue(makefile_name, value_name):\n+  if os.path.exists(makefile_name):\n+    with open(makefile_name) as makefile:\n+      for line in makefile:\n+        if line.startswith(value_name):\n+          (_, value) = line.split('=', 1)\n+          value = value.strip()\n+          return value\n+  return None\n+\n+\n+def ValidBuildDirectory(builddir, target):\n+  if (not os.path.exists(builddir) or\n+      not os.path.exists('%s/Makefile' % builddir) or\n+      not os.path.exists('%s/build-%s' % (builddir, target))):\n+    return False\n+  return True\n+\n+\n+def IsInterestingResult(line):\n+  \"\"\"Return True if the given line is one of the summary lines we care about.\"\"\"\n+  line = line.strip()\n+  if line.startswith('#'):\n+    return False\n+  if '|' in line:\n+    (_, line) = line.split('|', 1)\n+  line = line.strip()\n+  for result in _VALID_TEST_RESULTS:\n+    if line.startswith(result):\n+      return True\n+  return False\n+\n+\n+def ParseSummary(sum_fname):\n+  \"\"\"Create a set of TestResult instances from the given summary file.\"\"\"\n+  result_set = set()\n+  with open(sum_fname) as sum_file:\n+    for line in sum_file:\n+      if IsInterestingResult(line):\n+        result_set.add(TestResult(line))\n+  return result_set\n+\n+\n+def GetManifest(manifest_name):\n+  \"\"\"Build a set of expected failures from the manifest file.\n+\n+  Each entry in the manifest file should have the format understood\n+  by the TestResult constructor.\n+\n+  If no manifest file exists for this target, it returns an empty\n+  set.\n+  \"\"\"\n+  if os.path.exists(manifest_name):\n+    return ParseSummary(manifest_name)\n+  else:\n+    return set()\n+\n+\n+def GetSumFiles(builddir):\n+  sum_files = []\n+  for root, dirs, files in os.walk(builddir):\n+    if '.svn' in dirs:\n+      dirs.remove('.svn')\n+    for fname in files:\n+      if fname.endswith('.sum'):\n+        sum_files.append(os.path.join(root, fname))\n+  return sum_files\n+\n+\n+def GetResults(builddir):\n+  \"\"\"Collect all the test results from .sum files under the given build\n+  directory.\"\"\"\n+  sum_files = GetSumFiles(builddir)\n+  build_results = set()\n+  for sum_fname in sum_files:\n+    print '\\t%s' % sum_fname\n+    build_results |= ParseSummary(sum_fname)\n+  return build_results\n+\n+\n+def CompareResults(manifest, actual):\n+  \"\"\"Compare sets of results and return two lists:\n+     - List of results present in MANIFEST but missing from ACTUAL.\n+     - List of results present in ACTUAL but missing from MANIFEST.\n+  \"\"\"\n+  # Report all the actual results not present in the manifest.\n+  actual_vs_manifest = set()\n+  for actual_result in actual:\n+    if actual_result not in manifest:\n+      actual_vs_manifest.add(actual_result)\n+\n+  # Simlarly for all the tests in the manifest.\n+  manifest_vs_actual = set()\n+  for expected_result in manifest:\n+    # Ignore tests marked flaky.\n+    if 'flaky' in expected_result.attrs:\n+      continue\n+    if expected_result not in actual:\n+      manifest_vs_actual.add(expected_result)\n+\n+  return actual_vs_manifest, manifest_vs_actual\n+\n+\n+def GetBuildData(options):\n+  target = GetMakefileValue('%s/Makefile' % options.build_dir, 'target=')\n+  srcdir = GetMakefileValue('%s/Makefile' % options.build_dir, 'srcdir =')\n+  if not ValidBuildDirectory(options.build_dir, target):\n+    Error('%s is not a valid GCC top level build directory.' %\n+          options.build_dir)\n+  print 'Source directory: %s' % srcdir\n+  print 'Build target:     %s' % target\n+  return srcdir, target, True\n+\n+\n+def PrintSummary(msg, summary):\n+  print '\\n\\n%s' % msg\n+  for result in sorted(summary):\n+    print result\n+\n+\n+def CheckExpectedResults(options):\n+  (srcdir, target, valid_build) = GetBuildData(options)\n+  if not valid_build:\n+    return False\n+\n+  manifest_name = _MANIFEST_PATH_PATTERN % (srcdir, target)\n+  print 'Manifest:         %s' % manifest_name\n+  manifest = GetManifest(manifest_name)\n+\n+  print 'Getting actual results from build'\n+  actual = GetResults(options.build_dir)\n+\n+  if options.verbosity >= 1:\n+    PrintSummary('Tests expected to fail', manifest)\n+    PrintSummary('\\nActual test results', actual)\n+\n+  actual_vs_manifest, manifest_vs_actual = CompareResults(manifest, actual)\n+\n+  tests_ok = True\n+  if len(actual_vs_manifest) > 0:\n+    PrintSummary('Build results not in the manifest', actual_vs_manifest)\n+    tests_ok = False\n+\n+  if len(manifest_vs_actual) > 0:\n+    PrintSummary('Manifest results not present in the build'\n+                 '\\n\\nNOTE: This is not a failure.  It just means that the '\n+                 'manifest expected\\nthese tests to fail, '\n+                 'but they worked in this configuration.\\n',\n+                 manifest_vs_actual)\n+\n+  if tests_ok:\n+    print '\\nSUCCESS: No unexpected failures.'\n+\n+  return tests_ok\n+\n+\n+def ProduceManifest(options):\n+  (srcdir, target, valid_build) = GetBuildData(options)\n+  if not valid_build:\n+    return False\n+\n+  manifest_name = _MANIFEST_PATH_PATTERN % (srcdir, target)\n+  if os.path.exists(manifest_name) and not options.force:\n+    Error('Manifest file %s already exists.\\nUse --force to overwrite.' %\n+          manifest_name)\n+\n+  actual = GetResults(options.build_dir)\n+  with open(manifest_name, 'w') as manifest_file:\n+    for result in sorted(actual):\n+      print result\n+      manifest_file.write('%s\\n' % result)\n+\n+  return True\n+\n+\n+def Main(argv):\n+  parser = optparse.OptionParser(usage=__doc__)\n+  parser.add_option('--build_dir', action='store', type='string',\n+                    dest='build_dir', default='.',\n+                    help='Build directory to check (default = .)')\n+  parser.add_option('--manifest', action='store_true', dest='manifest',\n+                    default=False, help='Produce the manifest for the current '\n+                    'build (default = False)')\n+  parser.add_option('--force', action='store_true', dest='force',\n+                    default=False, help='When used with --manifest, it will '\n+                    'overwrite an existing manifest file (default = False)')\n+  parser.add_option('--verbosity', action='store', dest='verbosity',\n+                    type='int', default=0, help='Verbosity level (default = 0)')\n+  (options, _) = parser.parse_args(argv[1:])\n+\n+  if options.manifest:\n+    retval = ProduceManifest(options)\n+  else:\n+    retval = CheckExpectedResults(options)\n+\n+  if retval:\n+    return 0\n+  else:\n+    return 1\n+\n+if __name__ == '__main__':\n+  retval = Main(sys.argv)\n+  sys.exit(retval)"}]}