{"sha": "799142bf743cb04f3a86df85c69481a3c44c1ad9", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6Nzk5MTQyYmY3NDNjYjA0ZjNhODZkZjg1YzY5NDgxYTNjNDRjMWFkOQ==", "commit": {"author": {"name": "Torvald Riegel", "email": "triegel@redhat.com", "date": "2011-12-24T01:42:20Z"}, "committer": {"name": "Torvald Riegel", "email": "torvald@gcc.gnu.org", "date": "2011-12-24T01:42:20Z"}, "message": "libitm: Fixed conversion to C++11 atomics.\n\n\tlibitm/\n\t* beginend.cc (GTM::gtm_thread::begin_transaction): Add comment.\n\t(GTM::gtm_thread::try_commit): Changed memory order.\n\t* config/linux/alpha/futex_bits.h (sys_futex0): Take atomic int\n\tas parameter.\n\t* config/linux/x86/futex_bits.h (sys_futex0): Same.\n\t* config/linux/sh/futex_bits.h (sys_futex0): Same.\n\t* config/linux/futex_bits.h (sys_futex0): Same.\n\t* config/linux/futex.cc (futex_wait, futex_wake): Same.\n\t* config/linux/futex.h (futex_wait, futex_wake): Same.\n\t* config/linux/rwlock.h (gtm_rwlock::writers,\n\tgtm_rwlock::writer_readers, gtm_rwlock::readers): Change to atomic\n\tints.\n\t* config/linux/rwlock.cc (gtm_rwlock::read_lock,\n\tgtm_rwlock::write_lock_generic, gtm_rwlock::read_unlock,\n\tgtm_rwlock::write_unlock): Fix memory orders and fences.\n\t* config/posix/rwlock.cc (gtm_rwlock::read_lock,\n\tgtm_rwlock::write_lock_generic, gtm_rwlock::read_unlock,\n\tgtm_rwlock::write_unlock): Same.\n\t* config/linux/rwlock.h (gtm_rwlock::summary): Change to atomic int.\n\t* method-gl.cc (gl_mg::init, gl_wt_dispatch::memtransfer_static,\n\tgl_wt_dispatch::memset_static, gl_wt_dispatch::begin_or_restart):\n\tAdd comments.\n\t(gl_wt_dispatch::pre_write, gl_wt_dispatch::validate,\n\tgl_wt_dispatch::load, gl_wt_dispatch::store,\n\tgl_wt_dispatch::try_commit, gl_wt_dispatch::rollback): Fix memory\n\torders and fences.  Add comments.\n\nFrom-SVN: r182674", "tree": {"sha": "76234a08c818b5da2d7cbe2a47fa07501ef97cb2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/76234a08c818b5da2d7cbe2a47fa07501ef97cb2"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/799142bf743cb04f3a86df85c69481a3c44c1ad9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/799142bf743cb04f3a86df85c69481a3c44c1ad9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/799142bf743cb04f3a86df85c69481a3c44c1ad9", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/799142bf743cb04f3a86df85c69481a3c44c1ad9/comments", "author": {"login": "triegelrh", "id": 62400967, "node_id": "MDQ6VXNlcjYyNDAwOTY3", "avatar_url": "https://avatars.githubusercontent.com/u/62400967?v=4", "gravatar_id": "", "url": "https://api.github.com/users/triegelrh", "html_url": "https://github.com/triegelrh", "followers_url": "https://api.github.com/users/triegelrh/followers", "following_url": "https://api.github.com/users/triegelrh/following{/other_user}", "gists_url": "https://api.github.com/users/triegelrh/gists{/gist_id}", "starred_url": "https://api.github.com/users/triegelrh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/triegelrh/subscriptions", "organizations_url": "https://api.github.com/users/triegelrh/orgs", "repos_url": "https://api.github.com/users/triegelrh/repos", "events_url": "https://api.github.com/users/triegelrh/events{/privacy}", "received_events_url": "https://api.github.com/users/triegelrh/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "8b3bff4ce9864ae2444d0c1e18dc57b32e4d0e3a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8b3bff4ce9864ae2444d0c1e18dc57b32e4d0e3a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8b3bff4ce9864ae2444d0c1e18dc57b32e4d0e3a"}], "stats": {"total": 312, "additions": 222, "deletions": 90}, "files": [{"sha": "9a835e2860b20d296659496cfce5281290b722a5", "filename": "libitm/ChangeLog", "status": "modified", "additions": 29, "deletions": 0, "changes": 29, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2FChangeLog?ref=799142bf743cb04f3a86df85c69481a3c44c1ad9", "patch": "@@ -1,3 +1,32 @@\n+2011-12-24  Torvald Riegel  <triegel@redhat.com>\n+\n+\t* beginend.cc (GTM::gtm_thread::begin_transaction): Add comment.\n+\t(GTM::gtm_thread::try_commit): Changed memory order.\n+\t* config/linux/alpha/futex_bits.h (sys_futex0): Take atomic int\n+\tas parameter.\n+\t* config/linux/x86/futex_bits.h (sys_futex0): Same.\n+\t* config/linux/sh/futex_bits.h (sys_futex0): Same.\n+\t* config/linux/futex_bits.h (sys_futex0): Same.\n+\t* config/linux/futex.cc (futex_wait, futex_wake): Same.\n+\t* config/linux/futex.h (futex_wait, futex_wake): Same.\n+\t* config/linux/rwlock.h (gtm_rwlock::writers,\n+\tgtm_rwlock::writer_readers, gtm_rwlock::readers): Change to atomic\n+\tints.\n+\t* config/linux/rwlock.cc (gtm_rwlock::read_lock,\n+\tgtm_rwlock::write_lock_generic, gtm_rwlock::read_unlock,\n+\tgtm_rwlock::write_unlock): Fix memory orders and fences.\n+\t* config/posix/rwlock.cc (gtm_rwlock::read_lock,\n+\tgtm_rwlock::write_lock_generic, gtm_rwlock::read_unlock,\n+\tgtm_rwlock::write_unlock): Same.\n+\t* config/linux/rwlock.h (gtm_rwlock::summary): Change to atomic int.\n+\t* method-gl.cc (gl_mg::init, gl_wt_dispatch::memtransfer_static,\n+\tgl_wt_dispatch::memset_static, gl_wt_dispatch::begin_or_restart):\n+\tAdd comments.\n+\t(gl_wt_dispatch::pre_write, gl_wt_dispatch::validate,\n+\tgl_wt_dispatch::load, gl_wt_dispatch::store,\n+\tgl_wt_dispatch::try_commit, gl_wt_dispatch::rollback): Fix memory\n+\torders and fences.  Add comments.\n+\n 2011-12-21  Jakub Jelinek  <jakub@redhat.com>\n \n \t* Makefile.am (AM_CXXFLAGS): Put $(XCFLAGS) first."}, {"sha": "d0ad5a7fc2a9901279cbd69b47c13be5c185887a", "filename": "libitm/beginend.cc", "status": "modified", "additions": 19, "deletions": 5, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fbeginend.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fbeginend.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fbeginend.cc?ref=799142bf743cb04f3a86df85c69481a3c44c1ad9", "patch": "@@ -259,6 +259,9 @@ GTM::gtm_thread::begin_transaction (uint32_t prop, const gtm_jmpbuf *jb)\n   else\n     {\n #ifdef HAVE_64BIT_SYNC_BUILTINS\n+      // We don't really care which block of TIDs we get but only that we\n+      // acquire one atomically; therefore, relaxed memory order is\n+      // sufficient.\n       tx->id = global_tid.fetch_add(tid_block_size, memory_order_relaxed);\n       tx->local_tid = tx->id + 1;\n #else\n@@ -471,17 +474,28 @@ GTM::gtm_thread::trycommit ()\n       // Ensure privatization safety, if necessary.\n       if (priv_time)\n \t{\n+          // There must be a seq_cst fence between the following loads of the\n+          // other transactions' shared_state and the dispatch-specific stores\n+          // that signal updates by this transaction (e.g., lock\n+          // acquisitions).  This ensures that if we read prior to other\n+          // reader transactions setting their shared_state to 0, then those\n+          // readers will observe our updates.  We can reuse the seq_cst fence\n+          // in serial_lock.read_unlock() however, so we don't need another\n+          // one here.\n \t  // TODO Don't just spin but also block using cond vars / futexes\n \t  // here. Should probably be integrated with the serial lock code.\n-\t  // TODO For C++0x atomics, the loads of other threads' shared_state\n-\t  // should have acquire semantics (together with releases for the\n-\t  // respective updates). But is this unnecessary overhead because\n-\t  // weaker barriers are sufficient?\n \t  for (gtm_thread *it = gtm_thread::list_of_threads; it != 0;\n \t      it = it->next_thread)\n \t    {\n \t      if (it == this) continue;\n-\t      while (it->shared_state.load(memory_order_relaxed) < priv_time)\n+\t      // We need to load other threads' shared_state using acquire\n+\t      // semantics (matching the release semantics of the respective\n+\t      // updates).  This is necessary to ensure that the other\n+\t      // threads' memory accesses happen before our actions that\n+\t      // assume privatization safety.\n+\t      // TODO Are there any platform-specific optimizations (e.g.,\n+\t      // merging barriers)?\n+\t      while (it->shared_state.load(memory_order_acquire) < priv_time)\n \t\tcpu_relax();\n \t    }\n \t}"}, {"sha": "ef5c10a5c1a66d9bf47d7eda8dcfea0f125c6bb6", "filename": "libitm/config/linux/alpha/futex_bits.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Falpha%2Ffutex_bits.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Falpha%2Ffutex_bits.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Flinux%2Falpha%2Ffutex_bits.h?ref=799142bf743cb04f3a86df85c69481a3c44c1ad9", "patch": "@@ -29,7 +29,7 @@\n #endif\n \n static inline long\n-sys_futex0 (int *addr, long op, long val)\n+sys_futex0 (std::atomic<int> *addr, long op, long val)\n {\n   register long sc_0 __asm__(\"$0\");\n   register long sc_16 __asm__(\"$16\");"}, {"sha": "5707b40c26298e4a46cd860a17da85f0ff99c240", "filename": "libitm/config/linux/futex.cc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Ffutex.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Ffutex.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Flinux%2Ffutex.cc?ref=799142bf743cb04f3a86df85c69481a3c44c1ad9", "patch": "@@ -41,7 +41,7 @@ static long int gtm_futex_wake = FUTEX_WAKE | FUTEX_PRIVATE_FLAG;\n \n \n void\n-futex_wait (int *addr, int val)\n+futex_wait (std::atomic<int> *addr, int val)\n {\n   long res;\n \n@@ -65,7 +65,7 @@ futex_wait (int *addr, int val)\n \n \n long\n-futex_wake (int *addr, int count)\n+futex_wake (std::atomic<int> *addr, int count)\n {\n   long res = sys_futex0 (addr, gtm_futex_wake, count);\n   if (__builtin_expect (res == -ENOSYS, 0))"}, {"sha": "02bf5d0e323a2afcefe1ce7500510397c2d2304f", "filename": "libitm/config/linux/futex.h", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Ffutex.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Ffutex.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Flinux%2Ffutex.h?ref=799142bf743cb04f3a86df85c69481a3c44c1ad9", "patch": "@@ -27,10 +27,12 @@\n #ifndef GTM_FUTEX_H\n #define GTM_FUTEX_H 1\n \n+#include \"local_atomic\"\n+\n namespace GTM HIDDEN {\n \n-extern void futex_wait (int *addr, int val);\n-extern long futex_wake (int *addr, int count);\n+extern void futex_wait (std::atomic<int> *addr, int val);\n+extern long futex_wake (std::atomic<int> *addr, int count);\n \n }\n "}, {"sha": "9f6be72e3c4c1a2cd05e2d2fe6c8062352a402c6", "filename": "libitm/config/linux/futex_bits.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Ffutex_bits.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Ffutex_bits.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Flinux%2Ffutex_bits.h?ref=799142bf743cb04f3a86df85c69481a3c44c1ad9", "patch": "@@ -33,7 +33,7 @@\n #include <sys/syscall.h>\n \n static inline long\n-sys_futex0 (int *addr, long op, long val)\n+sys_futex0 (std::atomic<int> *addr, long op, long val)\n {\n-  return syscall (SYS_futex, addr, op, val, 0);\n+  return syscall (SYS_futex, (int*) addr, op, val, 0);\n }"}, {"sha": "f87be2e880a6ddb4046b56c8bd6367b84e3d09dc", "filename": "libitm/config/linux/rwlock.cc", "status": "modified", "additions": 52, "deletions": 40, "changes": 92, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Frwlock.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Frwlock.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Flinux%2Frwlock.cc?ref=799142bf743cb04f3a86df85c69481a3c44c1ad9", "patch": "@@ -36,10 +36,11 @@ gtm_rwlock::read_lock (gtm_thread *tx)\n   for (;;)\n     {\n       // Fast path: first announce our intent to read, then check for\n-      // conflicting intents to write.  Note that direct assignment to\n-      // an atomic object is memory_order_seq_cst.\n-      tx->shared_state = 0;\n-      if (likely(writers == 0))\n+      // conflicting intents to write.  The fence ensures that this happens\n+      // in exactly this order.\n+      tx->shared_state.store (0, memory_order_relaxed);\n+      atomic_thread_fence (memory_order_seq_cst);\n+      if (likely (writers.load (memory_order_relaxed) == 0))\n \treturn;\n \n       // There seems to be an active, waiting, or confirmed writer, so enter\n@@ -50,27 +51,28 @@ gtm_rwlock::read_lock (gtm_thread *tx)\n       // We need the barrier here for the same reason that we need it in\n       // read_unlock().\n       // TODO Potentially too many wake-ups. See comments in read_unlock().\n-      tx->shared_state = -1;\n-      if (writer_readers > 0)\n+      tx->shared_state.store (-1, memory_order_relaxed);\n+      atomic_thread_fence (memory_order_seq_cst);\n+      if (writer_readers.load (memory_order_relaxed) > 0)\n \t{\n-\t  writer_readers = 0;\n+\t  writer_readers.store (0, memory_order_relaxed);\n \t  futex_wake(&writer_readers, 1);\n \t}\n \n       // Signal that there are waiting readers and wait until there is no\n       // writer anymore.\n       // TODO Spin here on writers for a while. Consider whether we woke\n       // any writers before?\n-      while (writers)\n+      while (writers.load (memory_order_relaxed))\n \t{\n \t  // An active writer. Wait until it has finished. To avoid lost\n \t  // wake-ups, we need to use Dekker-like synchronization.\n \t  // Note that we cannot reset readers to zero when we see that there\n \t  // are no writers anymore after the barrier because this pending\n \t  // store could then lead to lost wake-ups at other readers.\n-\t  readers = 1;\n-\t  atomic_thread_fence(memory_order_acq_rel);\n-\t  if (writers)\n+\t  readers.store (1, memory_order_relaxed);\n+\t  atomic_thread_fence (memory_order_seq_cst);\n+\t  if (writers.load (memory_order_relaxed))\n \t    futex_wait(&readers, 1);\n \t}\n \n@@ -95,28 +97,23 @@ bool\n gtm_rwlock::write_lock_generic (gtm_thread *tx)\n {\n   // Try to acquire the write lock.\n-  unsigned int w;\n-  if (unlikely((w = __sync_val_compare_and_swap(&writers, 0, 1)) != 0))\n+  int w = 0;\n+  if (unlikely (!writers.compare_exchange_strong (w, 1)))\n     {\n       // If this is an upgrade, we must not wait for other writers or\n       // upgrades.\n       if (tx != 0)\n \treturn false;\n \n       // There is already a writer. If there are no other waiting writers,\n-      // switch to contended mode.\n-      // Note that this is actually an atomic exchange, not a TAS. Also,\n-      // it's only guaranteed to have acquire semantics, whereas we need a\n-      // full barrier to make the Dekker-style synchronization work. However,\n-      // we rely on the xchg being a full barrier on the architectures that we\n-      // consider here.\n-      // ??? Use C++0x atomics as soon as they are available.\n+      // switch to contended mode.  We need seq_cst memory order to make the\n+      // Dekker-style synchronization work.\n       if (w != 2)\n-\tw = __sync_lock_test_and_set(&writers, 2);\n+\tw = writers.exchange (2);\n       while (w != 0)\n \t{\n \t  futex_wait(&writers, 2);\n-\t  w = __sync_lock_test_and_set(&writers, 2);\n+\t  w = writers.exchange (2);\n \t}\n     }\n \n@@ -130,26 +127,28 @@ gtm_rwlock::write_lock_generic (gtm_thread *tx)\n   // TODO In the worst case, this requires one wait/wake pair for each\n   // active reader. Reduce this!\n   if (tx != 0)\n-    tx->shared_state = ~(typeof tx->shared_state)0;\n+    tx->shared_state.store (-1, memory_order_relaxed);\n \n   for (gtm_thread *it = gtm_thread::list_of_threads; it != 0;\n       it = it->next_thread)\n     {\n       // Use a loop here to check reader flags again after waiting.\n-      while (it->shared_state != ~(typeof it->shared_state)0)\n+      while (it->shared_state.load (memory_order_relaxed)\n+          != ~(typeof it->shared_state)0)\n \t{\n \t  // An active reader. Wait until it has finished. To avoid lost\n \t  // wake-ups, we need to use Dekker-like synchronization.\n \t  // Note that we can reset writer_readers to zero when we see after\n \t  // the barrier that the reader has finished in the meantime;\n \t  // however, this is only possible because we are the only writer.\n \t  // TODO Spin for a while on this reader flag.\n-\t  writer_readers = 1;\n-\t  __sync_synchronize();\n-\t  if (it->shared_state != ~(typeof it->shared_state)0)\n+\t  writer_readers.store (1, memory_order_relaxed);\n+\t  atomic_thread_fence (memory_order_seq_cst);\n+\t  if (it->shared_state.load (memory_order_relaxed)\n+\t      != ~(typeof it->shared_state)0)\n \t    futex_wait(&writer_readers, 1);\n \t  else\n-\t    writer_readers = 0;\n+\t    writer_readers.store (0, memory_order_relaxed);\n \t}\n     }\n \n@@ -181,19 +180,28 @@ gtm_rwlock::write_upgrade (gtm_thread *tx)\n void\n gtm_rwlock::read_unlock (gtm_thread *tx)\n {\n-  tx->shared_state = ~(typeof tx->shared_state)0;\n-\n-  // If there is a writer waiting for readers, wake it up. We need the barrier\n-  // to avoid lost wake-ups.\n+  // We only need release memory order here because of privatization safety\n+  // (this ensures that marking the transaction as inactive happens after\n+  // any prior data accesses by this transaction, and that neither the\n+  // compiler nor the hardware order this store earlier).\n+  // ??? We might be able to avoid this release here if the compiler can't\n+  // merge the release fence with the subsequent seq_cst fence.\n+  tx->shared_state.store (-1, memory_order_release);\n+\n+  // If there is a writer waiting for readers, wake it up.  We need the fence\n+  // to avoid lost wake-ups.  Furthermore, the privatization safety\n+  // implementation in gtm_thread::try_commit() relies on the existence of\n+  // this seq_cst fence.\n   // ??? We might not be the last active reader, so the wake-up might happen\n   // too early. How do we avoid this without slowing down readers too much?\n   // Each reader could scan the list of txns for other active readers but\n   // this can result in many cache misses. Use combining instead?\n   // TODO Sends out one wake-up for each reader in the worst case.\n-  __sync_synchronize();\n-  if (unlikely(writer_readers > 0))\n+  atomic_thread_fence (memory_order_seq_cst);\n+  if (unlikely (writer_readers.load (memory_order_relaxed) > 0))\n     {\n-      writer_readers = 0;\n+      // No additional barrier needed here (see write_unlock()).\n+      writer_readers.store (0, memory_order_relaxed);\n       futex_wake(&writer_readers, 1);\n     }\n }\n@@ -204,11 +212,11 @@ gtm_rwlock::read_unlock (gtm_thread *tx)\n void\n gtm_rwlock::write_unlock ()\n {\n-  // This is supposed to be a full barrier.\n-  if (__sync_fetch_and_sub(&writers, 1) == 2)\n+  // This needs to have seq_cst memory order.\n+  if (writers.fetch_sub (1) == 2)\n     {\n       // There might be waiting writers, so wake them.\n-      writers = 0;\n+      writers.store (0, memory_order_relaxed);\n       if (futex_wake(&writers, 1) == 0)\n \t{\n \t  // If we did not wake any waiting writers, we might indeed be the\n@@ -223,9 +231,13 @@ gtm_rwlock::write_unlock ()\n   // No waiting writers, so wake up all waiting readers.\n   // Because the fetch_and_sub is a full barrier already, we don't need\n   // another barrier here (as in read_unlock()).\n-  if (readers > 0)\n+  if (readers.load (memory_order_relaxed) > 0)\n     {\n-      readers = 0;\n+      // No additional barrier needed here.  The previous load must be in\n+      // modification order because of the coherency constraints.  Late stores\n+      // by a reader are not a problem because readers do Dekker-style\n+      // synchronization on writers.\n+      readers.store (0, memory_order_relaxed);\n       futex_wake(&readers, INT_MAX);\n     }\n }"}, {"sha": "e5a53c054f6095bdfde3c9346fe50aaef642fe7c", "filename": "libitm/config/linux/rwlock.h", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Frwlock.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Frwlock.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Flinux%2Frwlock.h?ref=799142bf743cb04f3a86df85c69481a3c44c1ad9", "patch": "@@ -25,6 +25,7 @@\n #ifndef GTM_RWLOCK_H\n #define GTM_RWLOCK_H\n \n+#include \"local_atomic\"\n #include \"common.h\"\n \n namespace GTM HIDDEN {\n@@ -42,9 +43,9 @@ struct gtm_thread;\n class gtm_rwlock\n {\n   // TODO Put futexes on different cachelines?\n-  int writers;          // Writers' futex.\n-  int writer_readers;   // A confirmed writer waits here for readers.\n-  int readers;          // Readers wait here for writers (iff true).\n+  std::atomic<int> writers;       // Writers' futex.\n+  std::atomic<int> writer_readers;// A confirmed writer waits here for readers.\n+  std::atomic<int> readers;       // Readers wait here for writers (iff true).\n \n  public:\n   gtm_rwlock() : writers(0), writer_readers(0), readers(0) {};"}, {"sha": "cc85367f72bd4df9a914f0f6700f57cb469d8f1d", "filename": "libitm/config/linux/sh/futex_bits.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Fsh%2Ffutex_bits.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Fsh%2Ffutex_bits.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Flinux%2Fsh%2Ffutex_bits.h?ref=799142bf743cb04f3a86df85c69481a3c44c1ad9", "patch": "@@ -32,7 +32,7 @@\n        trapa #0x14; or r0,r0; or r0,r0; or r0,r0; or r0,r0; or r0,r0\"\n \n static inline long\n-sys_futex0 (int *addr, long op, long val)\n+sys_futex0 (std::atomic<int> *addr, long op, long val)\n {\n   int __status;\n   register long __r3 asm (\"r3\") = SYS_futex;"}, {"sha": "732c4ea96b52a405009fb7ef77aea5cfd85a0037", "filename": "libitm/config/linux/x86/futex_bits.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Fx86%2Ffutex_bits.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Flinux%2Fx86%2Ffutex_bits.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Flinux%2Fx86%2Ffutex_bits.h?ref=799142bf743cb04f3a86df85c69481a3c44c1ad9", "patch": "@@ -28,7 +28,7 @@\n # endif\n \n static inline long\n-sys_futex0 (int *addr, long op, long val)\n+sys_futex0 (std::atomic<int> *addr, long op, long val)\n {\n   register long r10 __asm__(\"%r10\") = 0;\n   long res;\n@@ -49,7 +49,7 @@ sys_futex0 (int *addr, long op, long val)\n # ifdef __PIC__\n \n static inline long\n-sys_futex0 (int *addr, int op, int val)\n+sys_futex0 (std::atomic<int> *addr, int op, int val)\n {\n   long res;\n \n@@ -66,7 +66,7 @@ sys_futex0 (int *addr, int op, int val)\n # else\n \n static inline long\n-sys_futex0 (int *addr, int op, int val)\n+sys_futex0 (std::atomic<int> *addr, int op, int val)\n {\n   long res;\n "}, {"sha": "7ef39982ccf064b0fb0a785490d8fe3cf3789294", "filename": "libitm/config/posix/rwlock.cc", "status": "modified", "additions": 27, "deletions": 16, "changes": 43, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Fposix%2Frwlock.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Fposix%2Frwlock.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fposix%2Frwlock.cc?ref=799142bf743cb04f3a86df85c69481a3c44c1ad9", "patch": "@@ -53,10 +53,11 @@ void\n gtm_rwlock::read_lock (gtm_thread *tx)\n {\n   // Fast path: first announce our intent to read, then check for conflicting\n-  // intents to write.  Note that direct assignment to an atomic object\n-  // is memory_order_seq_cst.\n-  tx->shared_state = 0;\n-  unsigned int sum = this->summary;\n+  // intents to write.  The fence ensure that this happens in exactly this\n+  // order.\n+  tx->shared_state.store (0, memory_order_relaxed);\n+  atomic_thread_fence (memory_order_seq_cst);\n+  unsigned int sum = this->summary.load (memory_order_relaxed);\n   if (likely(!(sum & (a_writer | w_writer))))\n     return;\n \n@@ -74,7 +75,7 @@ gtm_rwlock::read_lock (gtm_thread *tx)\n \n   // Read summary again after acquiring the mutex because it might have\n   // changed during waiting for the mutex to become free.\n-  sum = this->summary;\n+  sum = this->summary.load (memory_order_relaxed);\n \n   // If there is a writer waiting for readers, wake it up. Only do that if we\n   // might be the last reader that could do the wake-up, otherwise skip the\n@@ -91,10 +92,10 @@ gtm_rwlock::read_lock (gtm_thread *tx)\n   // If there is an active or waiting writer, we must wait.\n   while (sum & (a_writer | w_writer))\n     {\n-      this->summary = sum | w_reader;\n+      this->summary.store (sum | w_reader, memory_order_relaxed);\n       this->w_readers++;\n       pthread_cond_wait (&this->c_readers, &this->mutex);\n-      sum = this->summary;\n+      sum = this->summary.load (memory_order_relaxed);\n       if (--this->w_readers == 0)\n \tsum &= ~w_reader;\n     }\n@@ -123,7 +124,7 @@ gtm_rwlock::write_lock_generic (gtm_thread *tx)\n {\n   pthread_mutex_lock (&this->mutex);\n \n-  unsigned int sum = this->summary;\n+  unsigned int sum = this->summary.load (memory_order_relaxed);\n \n   // If there is an active writer, wait.\n   while (sum & a_writer)\n@@ -136,23 +137,23 @@ gtm_rwlock::write_lock_generic (gtm_thread *tx)\n \t  return false;\n \t}\n \n-      this->summary = sum | w_writer;\n+      this->summary.store (sum | w_writer, memory_order_relaxed);\n       this->w_writers++;\n       pthread_cond_wait (&this->c_writers, &this->mutex);\n-      sum = this->summary;\n+      sum = this->summary.load (memory_order_relaxed);\n       if (--this->w_writers == 0)\n \tsum &= ~w_writer;\n     }\n \n   // Otherwise we can acquire the lock for write. As a writer, we have\n   // priority, so we don't need to take this back.\n-  this->summary = sum | a_writer;\n+  this->summary.store (sum | a_writer, memory_order_relaxed);\n \n   // We still need to wait for active readers to finish. The barrier makes\n   // sure that we first set our write intent and check for active readers\n   // after that, in strictly this order (similar to the barrier in the fast\n   // path of read_lock()).\n-  atomic_thread_fence(memory_order_acq_rel);\n+  atomic_thread_fence(memory_order_seq_cst);\n \n   // If this is an upgrade, we are not a reader anymore.\n   if (tx != 0)\n@@ -235,8 +236,18 @@ gtm_rwlock::write_upgrade (gtm_thread *tx)\n void\n gtm_rwlock::read_unlock (gtm_thread *tx)\n {\n-  tx->shared_state = -1;\n-  unsigned int sum = this->summary;\n+  // We only need release memory order here because of privatization safety\n+  // (this ensures that marking the transaction as inactive happens after\n+  // any prior data accesses by this transaction, and that neither the\n+  // compiler nor the hardware order this store earlier).\n+  // ??? We might be able to avoid this release here if the compiler can't\n+  // merge the release fence with the subsequent seq_cst fence.\n+  tx->shared_state.store (-1, memory_order_release);\n+  // We need this seq_cst fence here to avoid lost wake-ups.  Furthermore,\n+  // the privatization safety implementation in gtm_thread::try_commit()\n+  // relies on the existence of this seq_cst fence.\n+  atomic_thread_fence (memory_order_seq_cst);\n+  unsigned int sum = this->summary.load (memory_order_relaxed);\n   if (likely(!(sum & (a_writer | w_writer))))\n     return;\n \n@@ -269,8 +280,8 @@ gtm_rwlock::write_unlock ()\n {\n   pthread_mutex_lock (&this->mutex);\n \n-  unsigned int sum = this->summary;\n-  this->summary = sum & ~a_writer;\n+  unsigned int sum = this->summary.load (memory_order_relaxed);\n+  this->summary.store (sum & ~a_writer, memory_order_relaxed);\n \n   // If there is a waiting writer, wake it.\n   if (unlikely (sum & w_writer))"}, {"sha": "359991ab45b852864e69cbca253291fd3a47c4ce", "filename": "libitm/config/posix/rwlock.h", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Fposix%2Frwlock.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fconfig%2Fposix%2Frwlock.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fconfig%2Fposix%2Frwlock.h?ref=799142bf743cb04f3a86df85c69481a3c44c1ad9", "patch": "@@ -26,6 +26,7 @@\n #define GTM_RWLOCK_H\n \n #include <pthread.h>\n+#include \"local_atomic\"\n \n namespace GTM HIDDEN {\n \n@@ -55,7 +56,7 @@ class gtm_rwlock\n   static const unsigned w_writer  = 2;\t// The w_writers field != 0\n   static const unsigned w_reader  = 4;  // The w_readers field != 0\n \n-  unsigned int summary;\t\t// Bitmask of the above.\n+  std::atomic<unsigned int> summary;\t// Bitmask of the above.\n   unsigned int a_readers;\t// Nr active readers as observed by a writer\n   unsigned int w_readers;\t// Nr waiting readers\n   unsigned int w_writers;\t// Nr waiting writers"}, {"sha": "e678da76b349c64872f03bcf4575ea77ec4fa02b", "filename": "libitm/method-gl.cc", "status": "modified", "additions": 76, "deletions": 14, "changes": 90, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fmethod-gl.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/799142bf743cb04f3a86df85c69481a3c44c1ad9/libitm%2Fmethod-gl.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fmethod-gl.cc?ref=799142bf743cb04f3a86df85c69481a3c44c1ad9", "patch": "@@ -45,11 +45,14 @@ struct gl_mg : public method_group\n \n   virtual void init()\n   {\n+    // This store is only executed while holding the serial lock, so relaxed\n+    // memory order is sufficient here.\n     orec.store(0, memory_order_relaxed);\n   }\n   virtual void fini() { }\n };\n \n+// TODO cacheline padding\n static gl_mg o_gl_mg;\n \n \n@@ -85,23 +88,34 @@ class gl_wt_dispatch : public abi_dispatch\n   static void pre_write(const void *addr, size_t len)\n   {\n     gtm_thread *tx = gtm_thr();\n-    gtm_word v = tx->shared_state.load(memory_order_acquire);\n+    gtm_word v = tx->shared_state.load(memory_order_relaxed);\n     if (unlikely(!gl_mg::is_locked(v)))\n       {\n \t// Check for and handle version number overflow.\n \tif (unlikely(v >= gl_mg::VERSION_MAX))\n \t  tx->restart(RESTART_INIT_METHOD_GROUP);\n \n-\t// CAS global orec from our snapshot time to the locked state.\n \t// This validates that we have a consistent snapshot, which is also\n \t// for making privatization safety work (see the class' comments).\n+\t// Note that this check here will be performed by the subsequent CAS\n+\t// again, so relaxed memory order is fine.\n \tgtm_word now = o_gl_mg.orec.load(memory_order_relaxed);\n \tif (now != v)\n \t  tx->restart(RESTART_VALIDATE_WRITE);\n+\n+\t// CAS global orec from our snapshot time to the locked state.\n+\t// We need acq_rel memory order here to synchronize with other loads\n+\t// and modifications of orec.\n \tif (!o_gl_mg.orec.compare_exchange_strong (now, gl_mg::set_locked(now),\n-\t\t\t\t\t\t   memory_order_acquire))\n+\t\t\t\t\t\t   memory_order_acq_rel))\n \t  tx->restart(RESTART_LOCKED_WRITE);\n \n+\t// We use an explicit fence here to avoid having to use release\n+\t// memory order for all subsequent data stores.  This fence will\n+\t// synchronize with loads of the data with acquire memory order.  See\n+\t// validate() for why this is necessary.\n+\tatomic_thread_fence(memory_order_release);\n+\n \t// Set shared_state to new value.\n \ttx->shared_state.store(gl_mg::set_locked(now), memory_order_release);\n       }\n@@ -112,11 +126,19 @@ class gl_wt_dispatch : public abi_dispatch\n \n   static void validate()\n   {\n-    // Check that snapshot is consistent. The barrier ensures that this\n-    // happens after previous data loads.  Recall that load cannot itself\n-    // have memory_order_release.\n+    // Check that snapshot is consistent.  We expect the previous data load to\n+    // have acquire memory order, or be atomic and followed by an acquire\n+    // fence.\n+    // As a result, the data load will synchronize with the release fence\n+    // issued by the transactions whose data updates the data load has read\n+    // from.  This forces the orec load to read from a visible sequence of side\n+    // effects that starts with the other updating transaction's store that\n+    // acquired the orec and set it to locked.\n+    // We therefore either read a value with the locked bit set (and restart)\n+    // or read an orec value that was written after the data had been written.\n+    // Either will allow us to detect inconsistent reads because it will have\n+    // a higher/different value.\n     gtm_thread *tx = gtm_thr();\n-    atomic_thread_fence(memory_order_release);\n     gtm_word l = o_gl_mg.orec.load(memory_order_relaxed);\n     if (l != tx->shared_state.load(memory_order_relaxed))\n       tx->restart(RESTART_VALIDATE_READ);\n@@ -131,9 +153,28 @@ class gl_wt_dispatch : public abi_dispatch\n \tpre_write(addr, sizeof(V));\n \treturn *addr;\n       }\n+    if (unlikely(mod == RaW))\n+      return *addr;\n+\n+    // We do not have acquired the orec, so we need to load a value and then\n+    // validate that this was consistent.\n+    // This needs to have acquire memory order (see validate()).\n+    // Alternatively, we can put an acquire fence after the data load but this\n+    // is probably less efficient.\n+    // FIXME We would need an atomic load with acquire memory order here but\n+    // we can't just forge an atomic load for nonatomic data because this\n+    // might not work on all implementations of atomics.  However, we need\n+    // the acquire memory order and we can only establish this if we link\n+    // it to the matching release using a reads-from relation between atomic\n+    // loads.  Also, the compiler is allowed to optimize nonatomic accesses\n+    // differently than atomic accesses (e.g., if the load would be moved to\n+    // after the fence, we potentially don't synchronize properly anymore).\n+    // Instead of the following, just use an ordinary load followed by an\n+    // acquire fence, and hope that this is good enough for now:\n+    // V v = atomic_load_explicit((atomic<V>*)addr, memory_order_acquire);\n     V v = *addr;\n-    if (likely(mod != RaW))\n-      validate();\n+    atomic_thread_fence(memory_order_acquire);\n+    validate();\n     return v;\n   }\n \n@@ -142,6 +183,15 @@ class gl_wt_dispatch : public abi_dispatch\n   {\n     if (unlikely(mod != WaW))\n       pre_write(addr, sizeof(V));\n+    // FIXME We would need an atomic store here but we can't just forge an\n+    // atomic load for nonatomic data because this might not work on all\n+    // implementations of atomics.  However, we need this store to link the\n+    // release fence in pre_write() to the acquire operation in load, which\n+    // is only guaranteed if we have a reads-from relation between atomic\n+    // accesses.  Also, the compiler is allowed to optimize nonatomic accesses\n+    // differently than atomic accesses (e.g., if the store would be moved\n+    // to before the release fence in pre_write(), things could go wrong).\n+    // atomic_store_explicit((atomic<V>*)addr, value, memory_order_relaxed);\n     *addr = value;\n   }\n \n@@ -153,6 +203,8 @@ class gl_wt_dispatch : public abi_dispatch\n \t&& (dst_mod != NONTXNAL || src_mod == RfW))\n       pre_write(dst, size);\n \n+    // FIXME We should use atomics here (see store()).  Let's just hope that\n+    // memcpy/memmove are good enough.\n     if (!may_overlap)\n       ::memcpy(dst, src, size);\n     else\n@@ -167,6 +219,8 @@ class gl_wt_dispatch : public abi_dispatch\n   {\n     if (mod != WaW)\n       pre_write(dst, size);\n+    // FIXME We should use atomics here (see store()).  Let's just hope that\n+    // memset is good enough.\n     ::memset(dst, c, size);\n   }\n \n@@ -183,6 +237,11 @@ class gl_wt_dispatch : public abi_dispatch\n     gtm_word v;\n     while (1)\n       {\n+        // We need acquire memory order here so that this load will\n+        // synchronize with the store that releases the orec in trycommit().\n+        // In turn, this makes sure that subsequent data loads will read from\n+        // a visible sequence of side effects that starts with the most recent\n+        // store to the data right before the release of the orec.\n         v = o_gl_mg.orec.load(memory_order_acquire);\n         if (!gl_mg::is_locked(v))\n \t  break;\n@@ -201,15 +260,15 @@ class gl_wt_dispatch : public abi_dispatch\n     // smaller or equal (the serial lock will set shared_state to zero when\n     // marking the transaction as active, and restarts enforce immediate\n     // visibility of a smaller or equal value with a barrier (see\n-    // release_orec()).\n+    // rollback()).\n     tx->shared_state.store(v, memory_order_relaxed);\n     return NO_RESTART;\n   }\n \n   virtual bool trycommit(gtm_word& priv_time)\n   {\n     gtm_thread* tx = gtm_thr();\n-    gtm_word v = tx->shared_state.load(memory_order_acquire);\n+    gtm_word v = tx->shared_state.load(memory_order_relaxed);\n \n     // Special case: If shared_state is ~0, then we have acquired the\n     // serial lock (tx->state is not updated yet). In this case, the previous\n@@ -227,6 +286,7 @@ class gl_wt_dispatch : public abi_dispatch\n     if (gl_mg::is_locked(v))\n       {\n \t// Release the global orec, increasing its version number / timestamp.\n+        // See begin_or_restart() for why we need release memory order here.\n \tv = gl_mg::clear_locked(v) + 1;\n \to_gl_mg.orec.store(v, memory_order_release);\n \n@@ -245,7 +305,7 @@ class gl_wt_dispatch : public abi_dispatch\n       return;\n \n     gtm_thread *tx = gtm_thr();\n-    gtm_word v = tx->shared_state.load(memory_order_acquire);\n+    gtm_word v = tx->shared_state.load(memory_order_relaxed);\n     // Special case: If shared_state is ~0, then we have acquired the\n     // serial lock (tx->state is not updated yet). In this case, the previous\n     // value isn't available anymore, so grab it from the global lock, which\n@@ -262,22 +322,24 @@ class gl_wt_dispatch : public abi_dispatch\n     if (gl_mg::is_locked(v))\n       {\n \t// Release the global orec, increasing its version number / timestamp.\n+        // See begin_or_restart() for why we need release memory order here.\n \tv = gl_mg::clear_locked(v) + 1;\n \to_gl_mg.orec.store(v, memory_order_release);\n \n \t// Also reset the timestamp published via shared_state.\n \t// Special case: Only do this if we are not a serial transaction\n \t// because otherwise, we would interfere with the serial lock.\n \tif (!is_serial)\n-\t  tx->shared_state.store(v, memory_order_relaxed);\n+\t  tx->shared_state.store(v, memory_order_release);\n \n \t// We need a store-load barrier after this store to prevent it\n \t// from becoming visible after later data loads because the\n \t// previous value of shared_state has been higher than the actual\n \t// snapshot time (the lock bit had been set), which could break\n \t// privatization safety. We do not need a barrier before this\n \t// store (see pre_write() for an explanation).\n-\tatomic_thread_fence(memory_order_acq_rel);\n+\t// ??? What is the precise reasoning in the C++11 model?\n+\tatomic_thread_fence(memory_order_seq_cst);\n       }\n \n   }"}]}