{"sha": "0a1c58a25ab5df1a3e4596024774641ebae8be2a", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MGExYzU4YTI1YWI1ZGYxYTNlNDU5NjAyNDc3NDY0MWViYWU4YmUyYQ==", "commit": {"author": {"name": "Jeffrey A Law", "email": "law@cygnus.com", "date": "2000-03-17T22:40:45Z"}, "committer": {"name": "Richard Henderson", "email": "rth@gcc.gnu.org", "date": "2000-03-17T22:40:45Z"}, "message": "Sibling call optimizations.\n\nCo-Authored-By: Richard Henderson <rth@cygnus.com>\n\nFrom-SVN: r32612", "tree": {"sha": "720b4c50b7ea074422601de35cfc7e48ed679e49", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/720b4c50b7ea074422601de35cfc7e48ed679e49"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/0a1c58a25ab5df1a3e4596024774641ebae8be2a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0a1c58a25ab5df1a3e4596024774641ebae8be2a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0a1c58a25ab5df1a3e4596024774641ebae8be2a", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0a1c58a25ab5df1a3e4596024774641ebae8be2a/comments", "author": null, "committer": null, "parents": [{"sha": "f1fd8077fd1260362aa134deefc197948da270f8", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f1fd8077fd1260362aa134deefc197948da270f8", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f1fd8077fd1260362aa134deefc197948da270f8"}], "stats": {"total": 3092, "additions": 2232, "deletions": 860}, "files": [{"sha": "3ac1d635426b08b1c63ff89db6d2206ae03eb6b3", "filename": "gcc/ChangeLog", "status": "modified", "additions": 54, "deletions": 0, "changes": 54, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -1,3 +1,57 @@\n+2000-03-17  Jeff Law  <law@cygnus.com>\n+\t    Richard Henderson  <rth@cygnus.com>\n+\n+\t* Makefile.in (OBJS): Add sibcall.o.\n+\t(sibcall.o): New.\n+\t* sibcall.c: New file.\n+\t* calls.c (FUNCTION_OK_FOR_SIBCALL): Provide default.\n+\t(ECF_IS_CONST, ECF_NOTHROW, ECF_SIBCALL): New.\n+\t(emit_call_1): Replace `is_const' and `nothrow' with `ecf_flags'.\n+\tEmit sibcall patterns when requested.  Update all callers.\n+\t(expand_call): Generate CALL_PLACEHOLDER insns when tail call\n+\telimination seems feasable.\n+\t* final.c (leaf_function_p): Sibling calls don't discount being\n+\ta leaf function.\n+\t* flow.c (HAVE_sibcall_epilogue): Provide default.\n+\t(find_basic_blocks_1): Sibling calls don't throw.\n+\t(make_edges): Make edge from sibling call to EXIT.\n+\t(propagate_block): Don't remove sibcall_epilogue insns.\n+\t* function.c (prologue, epilogue): Turn into varrays.  Update all uses.\n+\t(sibcall_epilogue): New.\n+\t(fixup_var_refs): Scan CALL_PLACEHOLDER sub-sequences.\n+\t(identify_blocks_1): Likewise.  Break out from ...\n+\t(identify_blocks): ... here.\n+\t(reorder_blocks_1): Scan CALL_PLACEHOLDER.  Break out from ...\n+\t(reorder_blocks): ... here.\n+\t(init_function_for_compilation): Zap prologue/epilogue as varrays.\n+\t(record_insns): Extend a varray instead of mallocing new memory.\n+\t(contains): Read a varray not array of ints.\n+\t(sibcall_epilogue_contains): New.\n+\t(thread_prologue_and_epilogue_insns): Emit and record\n+\tsibcall_epilogue patterns.\n+\t(init_function_once): Allocate prologue/epilogue varrays.\n+\t* genflags.c (gen_insn): Treat sibcall patterns as calls.\n+\t* integrate.c (save_parm_insns): Recurse on CALL_PLACEHOLDER patterns.\n+\tBroken out from ...\n+\t(save_for_inline_nocopy): ... here.\n+\t(copy_insn_list): Recurse on CALL_PLACEHOLDER patterns. \n+\tBroken out from ...\n+\t(expand_inline_function): ... here.\n+\t(copy_rtx_and_substitute): Handle NOTE_INSN_DELETED_LABEL.\n+\t(subst_constants): Handle 'n' formats.\n+\t* jump.c (jump_optimize_minimal): New.\n+\t(jump_optimize_1): New arg `minimal'; update callers.  Elide most\n+\toptimizations if it's set.\n+\t* rtl.c (copy_rtx): Do copy jump & call for insns.\n+\t* rtl.h (struct rtx_def): Document use of jump and call for insns.\n+\t(SIBLING_CALL_P): New.\n+\t(sibcall_use_t): New.\n+\t* toplev.c (rest_of_compilation): Do init_EXPR_INSN_LIST_cache earlier.\n+\tInvoke optimize_sibling_and_tail_recursive_calls.\n+\t* tree.c (lang_safe_for_unsave): New.\n+\t(safe_for_unsave): New.\n+\t* tree.h (lang_safe_for_unsave, safe_for_unsave): Declare.\n+\n 2000-03-17  Mark Mitchell  <mark@codesourcery.com>\n \n \t* objc/objc-act.c (encode_method_prototype): Pass types, not"}, {"sha": "51ff35f72626d77d177079538080d7285366beb1", "filename": "gcc/Makefile.in", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -675,7 +675,8 @@ OBJS = diagnostic.o \\\n  insn-opinit.o insn-recog.o insn-extract.o insn-output.o insn-emit.o lcm.o \\\n  profile.o insn-attrtab.o $(out_object_file) $(EXTRA_OBJS) convert.o \\\n  mbchar.o dyn-string.o splay-tree.o graph.o sbitmap.o resource.o hash.o \\\n- predict.o lists.o ggc-common.o $(GGC) simplify-rtx.o ssa.o bb-reorder.o\n+ predict.o lists.o ggc-common.o $(GGC) simplify-rtx.o ssa.o bb-reorder.o \\\n+ sibcall.o\n \n # GEN files are listed separately, so they can be built before doing parallel\n #  makes for cc1 or cc1plus.  Otherwise sequent parallel make attempts to load\n@@ -1562,6 +1563,8 @@ cse.o : cse.c $(CONFIG_H) system.h $(RTL_H) $(REGS_H) hard-reg-set.h flags.h \\\n gcse.o : gcse.c $(CONFIG_H) system.h $(RTL_H) $(REGS_H) hard-reg-set.h \\\n    flags.h real.h insn-config.h $(RECOG_H) $(EXPR_H) $(BASIC_BLOCK_H) \\\n    function.h output.h toplev.h\n+sibcall.o : sibcall.c $(CONFIG_H) system.h $(RTL_H) $(REGS_H) function.h \\\n+   hard-reg-set.h flags.h insn-config.h $(RECOG_H) $(BASIC_BLOCK_H)\n resource.o : resource.c $(CONFIG_H) $(RTL_H) hard-reg-set.h system.h \\\n    $(BASIC_BLOCK_H) $(REGS_H) flags.h output.h resource.h function.h toplev.h \\\n    insn-attr.h"}, {"sha": "21feeed39ac1e3567746e3361de0f1b8406f708f", "filename": "gcc/calls.c", "status": "modified", "additions": 910, "deletions": 589, "changes": 1499, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Fcalls.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Fcalls.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcalls.c?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -32,6 +32,10 @@ Boston, MA 02111-1307, USA.  */\n #include \"output.h\"\n #include \"tm_p.h\"\n \n+#if !defined FUNCTION_OK_FOR_SIBCALL\n+#define FUNCTION_OK_FOR_SIBCALL(DECL) 1\n+#endif\n+\n #if !defined PREFERRED_STACK_BOUNDARY && defined STACK_BOUNDARY\n #define PREFERRED_STACK_BOUNDARY STACK_BOUNDARY\n #endif\n@@ -132,9 +136,13 @@ int stack_arg_under_construction;\n \n static int calls_function\tPARAMS ((tree, int));\n static int calls_function_1\tPARAMS ((tree, int));\n+\n+#define ECF_IS_CONST\t\t1\n+#define ECF_NOTHROW\t\t2\n+#define ECF_SIBCALL\t\t4\n static void emit_call_1\t\tPARAMS ((rtx, tree, tree, HOST_WIDE_INT,\n \t\t\t\t\t HOST_WIDE_INT, HOST_WIDE_INT, rtx,\n-\t\t\t\t\t rtx, int, rtx, int, int));\n+\t\t\t\t\t rtx, int, rtx, int));\n static void precompute_register_parameters\tPARAMS ((int,\n \t\t\t\t\t\t\t struct arg_data *,\n \t\t\t\t\t\t\t int *));\n@@ -384,7 +392,7 @@ prepare_call_address (funexp, fndecl, call_fusage, reg_parm_seen)\n static void\n emit_call_1 (funexp, fndecl, funtype, stack_size, rounded_stack_size,\n \t     struct_value_size, next_arg_reg, valreg, old_inhibit_defer_pop,\n-\t     call_fusage, is_const, nothrow)\n+\t     call_fusage, ecf_flags)\n      rtx funexp;\n      tree fndecl ATTRIBUTE_UNUSED;\n      tree funtype ATTRIBUTE_UNUSED;\n@@ -395,7 +403,7 @@ emit_call_1 (funexp, fndecl, funtype, stack_size, rounded_stack_size,\n      rtx valreg;\n      int old_inhibit_defer_pop;\n      rtx call_fusage;\n-     int is_const, nothrow;\n+     int ecf_flags;\n {\n   rtx rounded_stack_size_rtx = GEN_INT (rounded_stack_size);\n #if defined (HAVE_call) && defined (HAVE_call_value)\n@@ -414,6 +422,33 @@ emit_call_1 (funexp, fndecl, funtype, stack_size, rounded_stack_size,\n     funexp = memory_address (FUNCTION_MODE, funexp);\n \n #ifndef ACCUMULATE_OUTGOING_ARGS\n+#if defined (HAVE_sibcall_pop) && defined (HAVE_sibcall_value_pop)\n+  if ((ecf_flags & ECF_SIBCALL)\n+      && HAVE_sibcall_pop && HAVE_sibcall_value_pop\n+      && (RETURN_POPS_ARGS (fndecl, funtype, stack_size) > 0 \n+          || stack_size == 0))\n+    {\n+      rtx n_pop = GEN_INT (RETURN_POPS_ARGS (fndecl, funtype, stack_size));\n+      rtx pat;\n+\n+      /* If this subroutine pops its own args, record that in the call insn\n+\t if possible, for the sake of frame pointer elimination.  */\n+\n+      if (valreg)\n+\tpat = gen_sibcall_value_pop (valreg,\n+\t\t\t\t     gen_rtx_MEM (FUNCTION_MODE, funexp),\n+\t\t\t\t     rounded_stack_size_rtx, next_arg_reg,\n+\t\t\t\t     n_pop);\n+      else\n+\tpat = gen_sibcall_pop (gen_rtx_MEM (FUNCTION_MODE, funexp),\n+\t\t\t       rounded_stack_size_rtx, next_arg_reg, n_pop);\n+\n+      emit_call_insn (pat);\n+      already_popped = 1;\n+    }\n+  else\n+#endif\n+\n #if defined (HAVE_call_pop) && defined (HAVE_call_value_pop)\n /* If the target has \"call\" or \"call_value\" insns, then prefer them\n    if no arguments are actually popped.  If the target does not have\n@@ -447,6 +482,23 @@ emit_call_1 (funexp, fndecl, funtype, stack_size, rounded_stack_size,\n #endif\n #endif\n \n+#if defined (HAVE_sibcall) && defined (HAVE_sibcall_value)\n+  if ((ecf_flags & ECF_SIBCALL)\n+      && HAVE_sibcall && HAVE_sibcall_value)\n+    {\n+      if (valreg)\n+\temit_call_insn (gen_sibcall_value (valreg,\n+\t\t\t\t\t   gen_rtx_MEM (FUNCTION_MODE, funexp),\n+\t\t\t\t\t   rounded_stack_size_rtx,\n+\t\t\t\t\t   next_arg_reg, NULL_RTX));\n+      else\n+\temit_call_insn (gen_sibcall (gen_rtx_MEM (FUNCTION_MODE, funexp),\n+\t\t\t\t     rounded_stack_size_rtx, next_arg_reg,\n+\t\t\t\t     struct_value_size_rtx));\n+    }\n+  else\n+#endif\n+\n #if defined (HAVE_call) && defined (HAVE_call_value)\n   if (HAVE_call && HAVE_call_value)\n     {\n@@ -489,15 +541,17 @@ emit_call_1 (funexp, fndecl, funtype, stack_size, rounded_stack_size,\n     CALL_INSN_FUNCTION_USAGE (call_insn) = call_fusage;\n \n   /* If this is a const call, then set the insn's unchanging bit.  */\n-  if (is_const)\n+  if (ecf_flags & ECF_IS_CONST)\n     CONST_CALL_P (call_insn) = 1;\n \n   /* If this call can't throw, attach a REG_EH_REGION reg note to that\n      effect.  */\n-  if (nothrow)\n+  if (ecf_flags & ECF_NOTHROW)\n     REG_NOTES (call_insn) = gen_rtx_EXPR_LIST (REG_EH_REGION, const0_rtx,\n \t\t\t\t\t       REG_NOTES (call_insn));\n \n+  SIBLING_CALL_P (call_insn) = ((ecf_flags & ECF_SIBCALL) != 0);\n+\n   /* Restore this now, so that we do defer pops for this call's args\n      if the context of the call as a whole permits.  */\n   inhibit_defer_pop = old_inhibit_defer_pop;\n@@ -527,7 +581,8 @@ emit_call_1 (funexp, fndecl, funtype, stack_size, rounded_stack_size,\n \n   if (rounded_stack_size != 0)\n     {\n-      if (flag_defer_pop && inhibit_defer_pop == 0 && !is_const)\n+      if (flag_defer_pop && inhibit_defer_pop == 0\n+\t  && !(ecf_flags & ECF_IS_CONST))\n \tpending_stack_adjust += rounded_stack_size;\n       else\n \tadjust_stack (rounded_stack_size_rtx);\n@@ -1227,6 +1282,8 @@ compute_argument_block_size (reg_parm_stack_space, args_size,\n     {\n #ifdef PREFERRED_STACK_BOUNDARY\n       preferred_stack_boundary /= BITS_PER_UNIT;\n+      if (preferred_stack_boundary < 1)\n+\tpreferred_stack_boundary = 1;\n       args_size->constant = (((args_size->constant\n \t\t\t       + arg_space_so_far\n \t\t\t       + pending_stack_adjust\n@@ -1602,17 +1659,31 @@ expand_call (exp, target, ignore)\n      rtx target;\n      int ignore;\n {\n+  /* Nonzero if we are currently expanding a call.  */\n+  static int currently_expanding_call = 0;\n+\n   /* List of actual parameters.  */\n   tree actparms = TREE_OPERAND (exp, 1);\n   /* RTX for the function to be called.  */\n   rtx funexp;\n+  /* Sequence of insns to perform a tail recursive \"call\".  */\n+  rtx tail_recursion_insns = NULL_RTX;\n+  /* Sequence of insns to perform a normal \"call\".  */\n+  rtx normal_call_insns = NULL_RTX;\n+  /* Sequence of insns to perform a tail recursive \"call\".  */\n+  rtx tail_call_insns = NULL_RTX;\n   /* Data type of the function.  */\n   tree funtype;\n   /* Declaration of the function being called,\n      or 0 if the function is computed (not known by name).  */\n   tree fndecl = 0;\n   char *name = 0;\n+#ifdef ACCUMULATE_OUTGOING_ARGS\n   rtx before_call;\n+#endif\n+  rtx insn;\n+  int safe_for_reeval;\n+  int pass;\n \n   /* Register in which non-BLKmode value will be returned,\n      or 0 if no value or if value is BLKmode.  */\n@@ -1708,17 +1779,10 @@ expand_call (exp, target, ignore)\n   rtx old_stack_level = 0;\n   int old_pending_adj = 0;\n   int old_inhibit_defer_pop = inhibit_defer_pop;\n-  rtx call_fusage = 0;\n+  rtx call_fusage;\n   register tree p;\n   register int i;\n-#ifdef PREFERRED_STACK_BOUNDARY\n-  int preferred_stack_boundary = PREFERRED_STACK_BOUNDARY;\n-#else\n-  /* In this case preferred_stack_boundary variable is meaningless.\n-     It is used only in order to keep ifdef noise down when calling\n-     compute_argument_block_size.  */\n-  int preferred_stack_boundary = 0;\n-#endif\n+  int preferred_stack_boundary;\n \n   /* The value of the function call can be put in a hard register.  But\n      if -fcheck-memory-usage, code which invokes functions (and thus\n@@ -1947,11 +2011,91 @@ expand_call (exp, target, ignore)\n       mark_addressable (fndecl);\n     }\n \n+  currently_expanding_call++;\n+\n+  /* If we're considering tail recursion optimizations, verify that the\n+     arguments are safe for re-evaluation.  If we can unsave them, wrap\n+     each argument in an UNSAVE_EXPR.  */\n+\n+  safe_for_reeval = 0;\n+  if (optimize >= 2\n+      && currently_expanding_call == 1\n+      && stmt_loop_nest_empty ())\n+    {\n+      /* Verify that each argument is safe for re-evaluation.  */\n+      for (p = actparms; p; p = TREE_CHAIN (p))\n+\tif (! safe_for_unsave (TREE_VALUE (p)))\n+\t  break;\n+\n+      if (p == NULL_TREE)\n+        {\n+\t  tree new_actparms = NULL_TREE, q;\n+\n+\t  for (p = actparms; p ; p = TREE_CHAIN (p))\n+\t    {\n+\t      tree np = build_tree_list (TREE_PURPOSE (p),\n+\t\t\t\t\t unsave_expr (TREE_VALUE (p)));\n+\t      if (new_actparms)\n+\t\tTREE_CHAIN (q) = np;\n+\t      else\n+\t\tnew_actparms = np;\n+\t      q = np;\n+\t    }\n+\n+\t  actparms = new_actparms;\n+\t  safe_for_reeval = 1;\n+\t}\n+    }\n+\n+  /* Generate a tail recursion sequence when calling ourselves.  */\n+\n+  if (safe_for_reeval\n+      && TREE_CODE (TREE_OPERAND (exp, 0)) == ADDR_EXPR\n+      && TREE_OPERAND (TREE_OPERAND (exp, 0), 0) == current_function_decl)\n+    {\n+      /* We want to emit any pending stack adjustments before the tail\n+\t recursion \"call\".  That way we know any adjustment after the tail\n+\t recursion call can be ignored if we indeed use the tail recursion\n+\t call expansion.  */\n+      int save_pending_stack_adjust = pending_stack_adjust;\n+      rtx last;\n+\n+      /* Use a new sequence to hold any RTL we generate.  We do not even\n+\t know if we will use this RTL yet.  The final decision can not be\n+\t made until after RTL generation for the entire function is\n+\t complete.  */\n+      push_to_sequence (0);\n+\n+      /* Emit the pending stack adjustments before we expand any arguments.  */\n+      do_pending_stack_adjust ();\n+\n+      optimize_tail_recursion (exp, get_last_insn ());\n+ \n+      last = get_last_insn ();\n+      tail_recursion_insns = get_insns ();\n+      end_sequence ();\n+\n+      /* If the last insn on the tail recursion sequence is not a\n+\t BARRIER, then tail recursion optimization failed.  */\n+      if (last == NULL_RTX || GET_CODE (last) != BARRIER)\n+\ttail_recursion_insns = NULL_RTX;\n+\n+      /* Restore the original pending stack adjustment for the sibling and\n+\t normal call cases below.  */\n+      pending_stack_adjust = save_pending_stack_adjust;\n+    }\n+\n   function_call_count++;\n \n   if (fndecl && DECL_NAME (fndecl))\n     name = IDENTIFIER_POINTER (DECL_NAME (fndecl));\n \n+#ifdef PREFERRED_STACK_BOUNDARY\n+  preferred_stack_boundary = PREFERRED_STACK_BOUNDARY;\n+#else\n+  preferred_stack_boundary = STACK_BOUNDARY;\n+#endif\n+\n   /* Ensure current function's preferred stack boundary is at least\n      what we need.  We don't have to increase alignment for recursive\n      functions.  */\n@@ -1973,708 +2117,881 @@ expand_call (exp, target, ignore)\n     abort ();\n   funtype = TREE_TYPE (funtype);\n \n-  /* When calling a const function, we must pop the stack args right away,\n-     so that the pop is deleted or moved with the call.  */\n-  if (is_const)\n-    NO_DEFER_POP;\n+  /* We want to make two insn chains; one for a sibling call, the other\n+     for a normal call.  We will select one of the two chains after\n+     initial RTL generation is complete.  */\n+  for (pass = 0; pass < 2; pass++)\n+    {\n+      int sibcall_failure = 0;\n+      /* We want to emit ay pending stack adjustments before the tail\n+\t recursion \"call\".  That way we know any adjustment after the tail\n+\t recursion call can be ignored if we indeed use the tail recursion\n+\t call expansion.  */\n+      int save_pending_stack_adjust;\n+      rtx insns;\n+      rtx before_call;\n \n-  /* Don't let pending stack adjusts add up to too much.\n-     Also, do all pending adjustments now\n-     if there is any chance this might be a call to alloca.  */\n+      if (pass == 0)\n+\t{\n+\t  /* Various reasons we can not use a sibling call.  */\n+\t  if (! safe_for_reeval\n+#ifdef HAVE_sibcall_epilogue\n+\t      || ! HAVE_sibcall_epilogue\n+#else\n+\t      || 1\n+#endif\n+\t      /* The structure value address is used and modified in the\n+\t\t loop below.  It does not seem worth the effort to save and\n+\t\t restore it as a state variable since few optimizable\n+\t\t sibling calls will return a structure.  */\n+\t      || structure_value_addr != NULL_RTX\n+\t      /* If the register holding the address is a callee saved\n+\t\t register, then we lose.  We have no way to prevent that,\n+\t\t so we only allow calls to named functions.  */\n+\t      || fndecl == NULL_TREE\n+\t      || ! FUNCTION_OK_FOR_SIBCALL (fndecl))\n+\t    continue;\n \n-  if (pending_stack_adjust >= 32\n-      || (pending_stack_adjust > 0 && may_be_alloca))\n-    do_pending_stack_adjust ();\n+\t  /* State variables we need to save and restore between\n+\t     iterations.  */\n+\t  save_pending_stack_adjust = pending_stack_adjust;\n+\t}\n \n-  if (profile_arc_flag && fork_or_exec)\n-    {\n-\t/* A fork duplicates the profile information, and an exec discards\n-\t   it.  We can't rely on fork/exec to be paired.  So write out the\n-\t   profile information we have gathered so far, and clear it.  */\n-      emit_library_call (gen_rtx_SYMBOL_REF (Pmode, \"__bb_fork_func\"), 0,\n-\t\t\t VOIDmode, 0);\n-\n-      /* ??? When __clone is called with CLONE_VM set, profiling is\n-         subject to race conditions, just as with multithreaded programs.  */\n-    }\n+      /* Other state variables that we must reinitialize each time\n+\t through the loop (that are not initialized by the loop itself.  */\n+      argblock = 0;\n+      call_fusage = 0;\n \n-  /* Push the temporary stack slot level so that we can free any temporaries\n-     we make.  */\n-  push_temp_slots ();\n+      /* Start a new sequence for the normal call case. \n \n-  /* Start updating where the next arg would go.\n+\t From this point on, if the sibling call fails, we want to set\n+\t sibcall_failure instead of continuing the loop.  */\n+      start_sequence ();\n \n-     On some machines (such as the PA) indirect calls have a different\n-     calling convention than normal calls.  The last argument in\n-     INIT_CUMULATIVE_ARGS tells the backend if this is an indirect call\n-     or not.  */\n-  INIT_CUMULATIVE_ARGS (args_so_far, funtype, NULL_RTX, (fndecl == 0));\n+      /* When calling a const function, we must pop the stack args right away,\n+\t so that the pop is deleted or moved with the call.  */\n+      if (is_const)\n+\tNO_DEFER_POP;\n \n-  /* If struct_value_rtx is 0, it means pass the address\n-     as if it were an extra parameter.  */\n-  if (structure_value_addr && struct_value_rtx == 0)\n-    {\n-      /* If structure_value_addr is a REG other than\n-\t virtual_outgoing_args_rtx, we can use always use it.  If it\n-\t is not a REG, we must always copy it into a register.\n-\t If it is virtual_outgoing_args_rtx, we must copy it to another\n-\t register in some cases.  */\n-      rtx temp = (GET_CODE (structure_value_addr) != REG\n-#ifdef ACCUMULATE_OUTGOING_ARGS\n-\t\t  || (stack_arg_under_construction\n-\t\t      && structure_value_addr == virtual_outgoing_args_rtx)\n-#endif\n-\t\t  ? copy_addr_to_reg (structure_value_addr)\n-\t\t  : structure_value_addr);\n-\n-      actparms\n-\t= tree_cons (error_mark_node,\n-\t\t     make_tree (build_pointer_type (TREE_TYPE (funtype)),\n-\t\t\t\ttemp),\n-\t\t     actparms);\n-      structure_value_addr_parm = 1;\n-    }\n+      /* Don't let pending stack adjusts add up to too much.\n+\t Also, do all pending adjustments now if there is any chance\n+\t this might be a call to alloca or if we are expanding a sibling\n+\t call sequence.  */\n+      if (pending_stack_adjust >= 32\n+\t  || (pending_stack_adjust > 0 && may_be_alloca)\n+\t  || pass == 0)\n+\tdo_pending_stack_adjust ();\n \n-  /* Count the arguments and set NUM_ACTUALS.  */\n-  for (p = actparms, i = 0; p; p = TREE_CHAIN (p)) i++;\n-  num_actuals = i;\n-\n-  /* Compute number of named args.\n-     Normally, don't include the last named arg if anonymous args follow.\n-     We do include the last named arg if STRICT_ARGUMENT_NAMING is nonzero.\n-     (If no anonymous args follow, the result of list_length is actually\n-     one too large.  This is harmless.)\n-\n-     If PRETEND_OUTGOING_VARARGS_NAMED is set and STRICT_ARGUMENT_NAMING is\n-     zero, this machine will be able to place unnamed args that were passed in\n-     registers into the stack.  So treat all args as named.  This allows the\n-     insns emitting for a specific argument list to be independent of the\n-     function declaration.\n-\n-     If PRETEND_OUTGOING_VARARGS_NAMED is not set, we do not have any reliable\n-     way to pass unnamed args in registers, so we must force them into\n-     memory.  */\n-\n-  if ((STRICT_ARGUMENT_NAMING\n-       || ! PRETEND_OUTGOING_VARARGS_NAMED)\n-      && TYPE_ARG_TYPES (funtype) != 0)\n-    n_named_args\n-      = (list_length (TYPE_ARG_TYPES (funtype))\n-\t /* Don't include the last named arg.  */\n-\t - (STRICT_ARGUMENT_NAMING ? 0 : 1)\n-\t /* Count the struct value address, if it is passed as a parm.  */\n-\t + structure_value_addr_parm);\n-  else\n-    /* If we know nothing, treat all args as named.  */\n-    n_named_args = num_actuals;\n+      if (profile_arc_flag && fork_or_exec)\n+\t{\n+\t  /* A fork duplicates the profile information, and an exec discards\n+\t     it.  We can't rely on fork/exec to be paired.  So write out the\n+\t     profile information we have gathered so far, and clear it.  */\n+\t  /* ??? When Linux's __clone is called with CLONE_VM set, profiling\n+\t     is subject to race conditions, just as with multithreaded\n+\t     programs.  */\n+\n+\t  emit_library_call (gen_rtx_SYMBOL_REF (Pmode, \"__bb_fork_func\"), 0,\n+\t\t\t     VOIDmode, 0);\n+\t}\n \n-  /* Make a vector to hold all the information about each arg.  */\n-  args = (struct arg_data *) alloca (num_actuals * sizeof (struct arg_data));\n-  bzero ((char *) args, num_actuals * sizeof (struct arg_data));\n+      /* Push the temporary stack slot level so that we can free any\n+\t temporaries we make.  */\n+      push_temp_slots ();\n+\n+      /* Start updating where the next arg would go.\n+\n+\t On some machines (such as the PA) indirect calls have a different\n+\t calling convention than normal calls.  The last argument in\n+\t INIT_CUMULATIVE_ARGS tells the backend if this is an indirect call\n+\t or not.  */\n+      INIT_CUMULATIVE_ARGS (args_so_far, funtype, NULL_RTX, (fndecl == 0));\n+\n+      /* If struct_value_rtx is 0, it means pass the address\n+\t as if it were an extra parameter.  */\n+      if (structure_value_addr && struct_value_rtx == 0)\n+\t{\n+\t  /* If structure_value_addr is a REG other than\n+\t     virtual_outgoing_args_rtx, we can use always use it.  If it\n+\t     is not a REG, we must always copy it into a register.\n+\t     If it is virtual_outgoing_args_rtx, we must copy it to another\n+\t     register in some cases.  */\n+\t  rtx temp = (GET_CODE (structure_value_addr) != REG\n+#ifdef ACCUMULATE_OUTGOING_ARGS\n+\t\t      || (stack_arg_under_construction\n+\t\t\t  && structure_value_addr == virtual_outgoing_args_rtx)\n+#endif\n+\t\t      ? copy_addr_to_reg (structure_value_addr)\n+\t\t      : structure_value_addr);\n+\n+\t  actparms\n+\t    = tree_cons (error_mark_node,\n+\t\t\t make_tree (build_pointer_type (TREE_TYPE (funtype)),\n+\t\t\t\t    temp),\n+\t\t\t actparms);\n+\t  structure_value_addr_parm = 1;\n+\t}\n \n-  /* Build up entries inthe ARGS array, compute the size of the arguments\n-     into ARGS_SIZE, etc.  */\n-  initialize_argument_information (num_actuals, args, &args_size, n_named_args,\n-\t\t\t\t   actparms, fndecl, &args_so_far,\n-\t\t\t\t   reg_parm_stack_space, &old_stack_level,\n-\t\t\t\t   &old_pending_adj, &must_preallocate,\n-\t\t\t\t   &is_const);\n+      /* Count the arguments and set NUM_ACTUALS.  */\n+      for (p = actparms, i = 0; p; p = TREE_CHAIN (p)) i++;\n+      num_actuals = i;\n+\n+      /* Compute number of named args.\n+\t Normally, don't include the last named arg if anonymous args follow.\n+\t We do include the last named arg if STRICT_ARGUMENT_NAMING is nonzero.\n+\t (If no anonymous args follow, the result of list_length is actually\n+\t one too large.  This is harmless.)\n+\n+\t If PRETEND_OUTGOING_VARARGS_NAMED is set and STRICT_ARGUMENT_NAMING is\n+\t zero, this machine will be able to place unnamed args that were\n+\t passed in registers into the stack.  So treat all args as named.\n+\t This allows the insns emitting for a specific argument list to be\n+\t independent of the function declaration.\n+\n+\t If PRETEND_OUTGOING_VARARGS_NAMED is not set, we do not have any\n+\t reliable way to pass unnamed args in registers, so we must force\n+\t them into memory.  */\n+\n+      if ((STRICT_ARGUMENT_NAMING\n+\t   || ! PRETEND_OUTGOING_VARARGS_NAMED)\n+\t  && TYPE_ARG_TYPES (funtype) != 0)\n+\tn_named_args\n+\t  = (list_length (TYPE_ARG_TYPES (funtype))\n+\t     /* Don't include the last named arg.  */\n+\t     - (STRICT_ARGUMENT_NAMING ? 0 : 1)\n+\t     /* Count the struct value address, if it is passed as a parm.  */\n+\t     + structure_value_addr_parm);\n+      else\n+\t/* If we know nothing, treat all args as named.  */\n+\tn_named_args = num_actuals;\n+\n+      /* Make a vector to hold all the information about each arg.  */\n+      args = (struct arg_data *) alloca (num_actuals\n+\t\t\t\t\t * sizeof (struct arg_data));\n+      bzero ((char *) args, num_actuals * sizeof (struct arg_data));\n+\n+      /* Build up entries inthe ARGS array, compute the size of the arguments\n+\t into ARGS_SIZE, etc.  */\n+      initialize_argument_information (num_actuals, args, &args_size,\n+\t\t\t\t       n_named_args, actparms, fndecl,\n+\t\t\t\t       &args_so_far, reg_parm_stack_space,\n+\t\t\t\t       &old_stack_level, &old_pending_adj,\n+\t\t\t\t       &must_preallocate, &is_const);\n \n #ifdef FINAL_REG_PARM_STACK_SPACE\n-  reg_parm_stack_space = FINAL_REG_PARM_STACK_SPACE (args_size.constant,\n-\t\t\t\t\t\t     args_size.var);\n+      reg_parm_stack_space = FINAL_REG_PARM_STACK_SPACE (args_size.constant,\n+\t\t\t\t\t\t\t args_size.var);\n #endif\n       \n-  if (args_size.var)\n-    {\n-      /* If this function requires a variable-sized argument list, don't try to\n-\t make a cse'able block for this call.  We may be able to do this\n-\t eventually, but it is too complicated to keep track of what insns go\n-\t in the cse'able block and which don't.  */\n+      if (args_size.var)\n+\t{\n+\t  /* If this function requires a variable-sized argument list, don't\n+\t     try to make a cse'able block for this call.  We may be able to\n+\t     do this eventually, but it is too complicated to keep track of\n+\t     what insns go in the cse'able block and which don't. \n \n-      is_const = 0;\n-      must_preallocate = 1;\n-    }\n+\t     Also do not make a sibling call.  */\n \n-  /* Compute the actual size of the argument block required.  The variable\n-     and constant sizes must be combined, the size may have to be rounded,\n-     and there may be a minimum required size.  */\n-  unadjusted_args_size\n-    = compute_argument_block_size (reg_parm_stack_space, &args_size,\n-\t\t\t\t   preferred_stack_boundary);\n-\n-  /* Now make final decision about preallocating stack space.  */\n-  must_preallocate = finalize_must_preallocate (must_preallocate,\n-\t\t\t\t\t\tnum_actuals, args, &args_size);\n-\n-  /* If the structure value address will reference the stack pointer, we must\n-     stabilize it.  We don't need to do this if we know that we are not going\n-     to adjust the stack pointer in processing this call.  */\n-\n-  if (structure_value_addr\n-      && (reg_mentioned_p (virtual_stack_dynamic_rtx, structure_value_addr)\n-       || reg_mentioned_p (virtual_outgoing_args_rtx, structure_value_addr))\n-      && (args_size.var\n+\t  is_const = 0;\n+\t  must_preallocate = 1;\n+\t  sibcall_failure = 1;\n+\t}\n+\n+      /* Compute the actual size of the argument block required.  The variable\n+\t and constant sizes must be combined, the size may have to be rounded,\n+\t and there may be a minimum required size.  When generating a sibcall\n+\t pattern, do not round up, since we'll be re-using whatever space our\n+\t caller provided.  */\n+      unadjusted_args_size\n+\t= compute_argument_block_size (reg_parm_stack_space, &args_size,\n+\t\t\t\t       (pass == 0 ? 0\n+\t\t\t\t\t: preferred_stack_boundary));\n+\n+      /* If the callee pops its own arguments, then it must pop exactly\n+\t the same number of arguments as the current function.  */\n+      if (RETURN_POPS_ARGS (fndecl, funtype, unadjusted_args_size)\n+\t  != RETURN_POPS_ARGS (current_function_decl,\n+\t\t\t       TREE_TYPE (current_function_decl),\n+\t\t\t       current_function_args_size))\n+\tsibcall_failure = 1;\n+\n+      /* Now make final decision about preallocating stack space.  */\n+      must_preallocate = finalize_must_preallocate (must_preallocate,\n+\t\t\t\t\t\t    num_actuals, args,\n+\t\t\t\t\t\t    &args_size);\n+\n+      /* If the structure value address will reference the stack pointer, we\n+\t must stabilize it.  We don't need to do this if we know that we are\n+\t not going to adjust the stack pointer in processing this call.  */\n+\n+      if (structure_value_addr\n+\t  && (reg_mentioned_p (virtual_stack_dynamic_rtx, structure_value_addr)\n+\t      || reg_mentioned_p (virtual_outgoing_args_rtx,\n+\t\t\t\t  structure_value_addr))\n+\t  && (args_size.var\n #ifndef ACCUMULATE_OUTGOING_ARGS\n-\t  || args_size.constant\n+\t      || args_size.constant\n #endif\n-\t  ))\n-    structure_value_addr = copy_to_reg (structure_value_addr);\n+\t      ))\n+\tstructure_value_addr = copy_to_reg (structure_value_addr);\n \n-  /* Precompute any arguments as needed.  */\n-  precompute_arguments (is_const, must_preallocate, num_actuals,\n-                        args, &args_size);\n+      /* Precompute any arguments as needed.  */\n+      precompute_arguments (is_const, must_preallocate, num_actuals,\n+\t\t\t    args, &args_size);\n \n-  /* Now we are about to start emitting insns that can be deleted\n-     if a libcall is deleted.  */\n-  if (is_const || is_malloc)\n-    start_sequence ();\n+      /* Now we are about to start emitting insns that can be deleted\n+\t if a libcall is deleted.  */\n+      if (is_const || is_malloc)\n+\tstart_sequence ();\n \n-  /* If we have no actual push instructions, or shouldn't use them,\n-     make space for all args right now.  */\n+      /* If we have no actual push instructions, or shouldn't use them,\n+\t make space for all args right now.  */\n \n-  if (args_size.var != 0)\n-    {\n-      if (old_stack_level == 0)\n+      if (args_size.var != 0)\n \t{\n-\t  emit_stack_save (SAVE_BLOCK, &old_stack_level, NULL_RTX);\n-\t  old_pending_adj = pending_stack_adjust;\n-\t  pending_stack_adjust = 0;\n+\t  if (old_stack_level == 0)\n+\t    {\n+\t      emit_stack_save (SAVE_BLOCK, &old_stack_level, NULL_RTX);\n+\t      old_pending_adj = pending_stack_adjust;\n+\t      pending_stack_adjust = 0;\n #ifdef ACCUMULATE_OUTGOING_ARGS\n-\t  /* stack_arg_under_construction says whether a stack arg is\n-\t     being constructed at the old stack level.  Pushing the stack\n-\t     gets a clean outgoing argument block.  */\n-\t  old_stack_arg_under_construction = stack_arg_under_construction;\n-\t  stack_arg_under_construction = 0;\n+\t      /* stack_arg_under_construction says whether a stack arg is\n+\t\t being constructed at the old stack level.  Pushing the stack\n+\t\t gets a clean outgoing argument block.  */\n+\t      old_stack_arg_under_construction = stack_arg_under_construction;\n+\t      stack_arg_under_construction = 0;\n #endif\n+\t    }\n+\t  argblock = push_block (ARGS_SIZE_RTX (args_size), 0, 0);\n \t}\n-      argblock = push_block (ARGS_SIZE_RTX (args_size), 0, 0);\n-    }\n-  else\n-    {\n-      /* Note that we must go through the motions of allocating an argument\n-\t block even if the size is zero because we may be storing args\n-\t in the area reserved for register arguments, which may be part of\n-\t the stack frame.  */\n+      else\n+\t{\n+\t  /* Note that we must go through the motions of allocating an argument\n+\t     block even if the size is zero because we may be storing args\n+\t     in the area reserved for register arguments, which may be part of\n+\t     the stack frame.  */\n \n-      int needed = args_size.constant;\n+\t  int needed = args_size.constant;\n \n-      /* Store the maximum argument space used.  It will be pushed by\n-\t the prologue (if ACCUMULATE_OUTGOING_ARGS, or stack overflow\n-\t checking).  */\n+\t  /* Store the maximum argument space used.  It will be pushed by\n+\t     the prologue (if ACCUMULATE_OUTGOING_ARGS, or stack overflow\n+\t     checking).  */\n \n-      if (needed > current_function_outgoing_args_size)\n-\tcurrent_function_outgoing_args_size = needed;\n+\t  if (needed > current_function_outgoing_args_size)\n+\t    current_function_outgoing_args_size = needed;\n \n-      if (must_preallocate)\n-\t{\n+\t  if (must_preallocate)\n+\t    {\n #ifdef ACCUMULATE_OUTGOING_ARGS\n-\t  /* Since the stack pointer will never be pushed, it is possible for\n-\t     the evaluation of a parm to clobber something we have already\n-\t     written to the stack.  Since most function calls on RISC machines\n-\t     do not use the stack, this is uncommon, but must work correctly.\n+\t      /* Since the stack pointer will never be pushed, it is possible\n+\t\t for the evaluation of a parm to clobber something we have\n+\t\t already written to the stack.  Since most function calls on\n+\t\t RISC machines do not use the stack, this is uncommon, but\n+\t\t must work correctly.\n \n-\t     Therefore, we save any area of the stack that was already written\n-\t     and that we are using.  Here we set up to do this by making a new\n-\t     stack usage map from the old one.  The actual save will be done\n-\t     by store_one_arg. \n+\t\t Therefore, we save any area of the stack that was already\n+\t\t written and that we are using.  Here we set up to do this by\n+\t\t making a new stack usage map from the old one.  The actual\n+\t\t save will be done by store_one_arg. \n \n-\t     Another approach might be to try to reorder the argument\n-\t     evaluations to avoid this conflicting stack usage.  */\n+\t\t Another approach might be to try to reorder the argument\n+\t\t evaluations to avoid this conflicting stack usage.  */\n \n #ifndef OUTGOING_REG_PARM_STACK_SPACE\n-\t  /* Since we will be writing into the entire argument area, the\n-\t     map must be allocated for its entire size, not just the part that\n-\t     is the responsibility of the caller.  */\n-\t  needed += reg_parm_stack_space;\n+\t      /* Since we will be writing into the entire argument area, the\n+\t\t map must be allocated for its entire size, not just the part\n+\t\t that is the responsibility of the caller.  */\n+\t      needed += reg_parm_stack_space;\n #endif\n \n #ifdef ARGS_GROW_DOWNWARD\n-\t  highest_outgoing_arg_in_use = MAX (initial_highest_arg_in_use,\n-\t\t\t\t\t     needed + 1);\n+\t      highest_outgoing_arg_in_use = MAX (initial_highest_arg_in_use,\n+\t\t\t\t\t\t needed + 1);\n #else\n-\t  highest_outgoing_arg_in_use = MAX (initial_highest_arg_in_use,\n-\t\t\t\t\t     needed);\n+\t      highest_outgoing_arg_in_use = MAX (initial_highest_arg_in_use,\n+\t\t\t\t\t\t needed);\n #endif\n-\t  stack_usage_map = (char *) alloca (highest_outgoing_arg_in_use);\n+\t      stack_usage_map = (char *) alloca (highest_outgoing_arg_in_use);\n \n-\t  if (initial_highest_arg_in_use)\n-\t    bcopy (initial_stack_usage_map, stack_usage_map,\n-\t\t   initial_highest_arg_in_use);\n+\t      if (initial_highest_arg_in_use)\n+\t\tbcopy (initial_stack_usage_map, stack_usage_map,\n+\t\t       initial_highest_arg_in_use);\n \n-\t  if (initial_highest_arg_in_use != highest_outgoing_arg_in_use)\n-\t    bzero (&stack_usage_map[initial_highest_arg_in_use],\n-\t\t   highest_outgoing_arg_in_use - initial_highest_arg_in_use);\n-\t  needed = 0;\n+\t      if (initial_highest_arg_in_use != highest_outgoing_arg_in_use)\n+\t\tbzero (&stack_usage_map[initial_highest_arg_in_use],\n+\t\t       (highest_outgoing_arg_in_use\n+\t\t\t- initial_highest_arg_in_use));\n+\t      needed = 0;\n \n-\t  /* The address of the outgoing argument list must not be copied to a\n-\t     register here, because argblock would be left pointing to the\n-\t     wrong place after the call to allocate_dynamic_stack_space below.\n-\t     */\n+\t      /* The address of the outgoing argument list must not be copied\n+\t\t to a register here, because argblock would be left pointing\n+\t\t to the wrong place after the call to\n+\t\t allocate_dynamic_stack_space below. */\n \n-\t  argblock = virtual_outgoing_args_rtx;\n+\t      argblock = virtual_outgoing_args_rtx;\n \n #else /* not ACCUMULATE_OUTGOING_ARGS */\n-\t  if (inhibit_defer_pop == 0)\n-\t    {\n-\t      /* Try to reuse some or all of the pending_stack_adjust\n-\t\t to get this space.  Maybe we can avoid any pushing.  */\n-\t      if (needed > pending_stack_adjust)\n+\t      if (inhibit_defer_pop == 0)\n \t\t{\n-\t\t  needed -= pending_stack_adjust;\n-\t\t  pending_stack_adjust = 0;\n+\t\t  /* Try to reuse some or all of the pending_stack_adjust\n+\t\t     to get this space.  Maybe we can avoid any pushing.  */\n+\t\t  if (needed > pending_stack_adjust)\n+\t\t    {\n+\t\t      needed -= pending_stack_adjust;\n+\t\t      pending_stack_adjust = 0;\n+\t\t    }\n+\t\t  else\n+\t\t    {\n+\t\t      pending_stack_adjust -= needed;\n+\t\t      needed = 0;\n+\t\t    }\n \t\t}\n+\t      /* Special case this because overhead of `push_block' in this\n+\t\t case is non-trivial.  */\n+\t      if (needed == 0)\n+\t\targblock = virtual_outgoing_args_rtx;\n \t      else\n-\t\t{\n-\t\t  pending_stack_adjust -= needed;\n-\t\t  needed = 0;\n-\t\t}\n-\t    }\n-\t  /* Special case this because overhead of `push_block' in this\n-\t     case is non-trivial.  */\n-\t  if (needed == 0)\n-\t    argblock = virtual_outgoing_args_rtx;\n-\t  else\n-\t    argblock = push_block (GEN_INT (needed), 0, 0);\n-\n-\t  /* We only really need to call `copy_to_reg' in the case where push\n-\t     insns are going to be used to pass ARGBLOCK to a function\n-\t     call in ARGS.  In that case, the stack pointer changes value\n-\t     from the allocation point to the call point, and hence\n-\t     the value of VIRTUAL_OUTGOING_ARGS_RTX changes as well.\n-\t     But might as well always do it.  */\n-\t  argblock = copy_to_reg (argblock);\n+\t\targblock = push_block (GEN_INT (needed), 0, 0);\n+\n+\t      /* We only really need to call `copy_to_reg' in the case where\n+\t\t push insns are going to be used to pass ARGBLOCK to a function\n+\t\t call in ARGS.  In that case, the stack pointer changes value\n+\t\t from the allocation point to the call point, and hence\n+\t\t the value of VIRTUAL_OUTGOING_ARGS_RTX changes as well.\n+\t\t But might as well always do it.  */\n+\t      argblock = copy_to_reg (argblock);\n #endif /* not ACCUMULATE_OUTGOING_ARGS */\n+\t    }\n+\t}\n+\n+      /* The argument block when performing a sibling call is the\n+\t incoming argument block.  */\n+      if (pass == 0)\n+\t{\n+\t  rtx temp = plus_constant (arg_pointer_rtx,\n+\t\t\t\t    FIRST_PARM_OFFSET (current_function_decl));\n+\t  argblock = force_reg (Pmode, force_operand (temp, NULL_RTX));\n \t}\n-    }\n \n #ifdef ACCUMULATE_OUTGOING_ARGS\n-  /* The save/restore code in store_one_arg handles all cases except one:\n-     a constructor call (including a C function returning a BLKmode struct)\n-     to initialize an argument.  */\n-  if (stack_arg_under_construction)\n-    {\n+      /* The save/restore code in store_one_arg handles all cases except one:\n+\t a constructor call (including a C function returning a BLKmode struct)\n+\t to initialize an argument.  */\n+      if (stack_arg_under_construction)\n+\t{\n #ifndef OUTGOING_REG_PARM_STACK_SPACE\n-      rtx push_size = GEN_INT (reg_parm_stack_space + args_size.constant);\n+\t  rtx push_size = GEN_INT (reg_parm_stack_space + args_size.constant);\n #else\n-      rtx push_size = GEN_INT (args_size.constant);\n+\t  rtx push_size = GEN_INT (args_size.constant);\n #endif\n-      if (old_stack_level == 0)\n-\t{\n-\t  emit_stack_save (SAVE_BLOCK, &old_stack_level, NULL_RTX);\n-\t  old_pending_adj = pending_stack_adjust;\n-\t  pending_stack_adjust = 0;\n-\t  /* stack_arg_under_construction says whether a stack arg is\n-\t     being constructed at the old stack level.  Pushing the stack\n-\t     gets a clean outgoing argument block.  */\n-\t  old_stack_arg_under_construction = stack_arg_under_construction;\n-\t  stack_arg_under_construction = 0;\n-\t  /* Make a new map for the new argument list.  */\n-\t  stack_usage_map = (char *)alloca (highest_outgoing_arg_in_use);\n-\t  bzero (stack_usage_map, highest_outgoing_arg_in_use);\n-\t  highest_outgoing_arg_in_use = 0;\n+\t  if (old_stack_level == 0)\n+\t    {\n+\t      emit_stack_save (SAVE_BLOCK, &old_stack_level, NULL_RTX);\n+\t      old_pending_adj = pending_stack_adjust;\n+\t      pending_stack_adjust = 0;\n+\t      /* stack_arg_under_construction says whether a stack arg is\n+\t\t being constructed at the old stack level.  Pushing the stack\n+\t\t gets a clean outgoing argument block.  */\n+\t      old_stack_arg_under_construction = stack_arg_under_construction;\n+\t      stack_arg_under_construction = 0;\n+\t      /* Make a new map for the new argument list.  */\n+\t      stack_usage_map = (char *)alloca (highest_outgoing_arg_in_use);\n+\t      bzero (stack_usage_map, highest_outgoing_arg_in_use);\n+\t      highest_outgoing_arg_in_use = 0;\n+\t    }\n+\t  allocate_dynamic_stack_space (push_size, NULL_RTX, BITS_PER_UNIT);\n \t}\n-      allocate_dynamic_stack_space (push_size, NULL_RTX, BITS_PER_UNIT);\n-    }\n-  /* If argument evaluation might modify the stack pointer, copy the\n-     address of the argument list to a register.  */\n-  for (i = 0; i < num_actuals; i++)\n-    if (args[i].pass_on_stack)\n-      {\n-\targblock = copy_addr_to_reg (argblock);\n-\tbreak;\n-      }\n+      /* If argument evaluation might modify the stack pointer, copy the\n+\t address of the argument list to a register.  */\n+      for (i = 0; i < num_actuals; i++)\n+\tif (args[i].pass_on_stack)\n+\t  {\n+\t    argblock = copy_addr_to_reg (argblock);\n+\t    break;\n+\t  }\n #endif\n \n-  compute_argument_addresses (args, argblock, num_actuals);\n+      compute_argument_addresses (args, argblock, num_actuals);\n \n #ifdef PUSH_ARGS_REVERSED\n #ifdef PREFERRED_STACK_BOUNDARY\n-  /* If we push args individually in reverse order, perform stack alignment\n-     before the first push (the last arg).  */\n-  if (args_size.constant != unadjusted_args_size)\n-    {\n-      /* When the stack adjustment is pending,\n-\t we get better code by combining the adjustments.  */\n-      if (pending_stack_adjust && !is_const\n-\t  && !inhibit_defer_pop)\n+      /* If we push args individually in reverse order, perform stack alignment\n+\t before the first push (the last arg).  */\n+      if (args_size.constant != unadjusted_args_size)\n \t{\n-\t  args_size.constant = (unadjusted_args_size\n-\t\t\t        + ((pending_stack_adjust + args_size.constant\n-\t\t\t\t    + arg_space_so_far\n-\t\t\t\t    - unadjusted_args_size)\n-\t\t\t           % (preferred_stack_boundary / BITS_PER_UNIT)));\n-\t  pending_stack_adjust -= args_size.constant - unadjusted_args_size;\n-\t  do_pending_stack_adjust ();\n-\t}\n-      else if (argblock == 0)\n-\tanti_adjust_stack (GEN_INT (args_size.constant - unadjusted_args_size));\n-      arg_space_so_far += args_size.constant - unadjusted_args_size;\n+\t  /* When the stack adjustment is pending, we get better code\n+\t     by combining the adjustments.  */\n+\t  if (pending_stack_adjust && ! is_const\n+\t      && ! inhibit_defer_pop)\n+\t    {\n+\t      args_size.constant = (unadjusted_args_size\n+\t\t\t\t    + ((pending_stack_adjust\n+\t\t\t\t\t+ args_size.constant\n+\t\t\t\t\t+ arg_space_so_far\n+\t\t\t\t\t- unadjusted_args_size)\n+\t\t\t\t       % (preferred_stack_boundary\n+\t\t\t\t\t  / BITS_PER_UNIT)));\n+\t      pending_stack_adjust -= (args_size.constant\n+\t\t\t\t       - unadjusted_args_size);\n+\t      do_pending_stack_adjust ();\n+\t    }\n+\t  else if (argblock == 0)\n+\t    anti_adjust_stack (GEN_INT (args_size.constant\n+\t\t\t\t\t- unadjusted_args_size));\n+\t  arg_space_so_far += args_size.constant - unadjusted_args_size;\n \n-      /* Now that the stack is properly aligned, pops can't safely\n-\t be deferred during the evaluation of the arguments.  */\n-      NO_DEFER_POP;\n-    }\n+\t  /* Now that the stack is properly aligned, pops can't safely\n+\t     be deferred during the evaluation of the arguments.  */\n+\t  NO_DEFER_POP;\n+\t}\n #endif\n #endif\n \n-  /* Don't try to defer pops if preallocating, not even from the first arg,\n-     since ARGBLOCK probably refers to the SP.  */\n-  if (argblock)\n-    NO_DEFER_POP;\n+      /* Don't try to defer pops if preallocating, not even from the first arg,\n+\t since ARGBLOCK probably refers to the SP.  */\n+      if (argblock)\n+\tNO_DEFER_POP;\n \n-  funexp = rtx_for_function_call (fndecl, exp);\n+      funexp = rtx_for_function_call (fndecl, exp);\n \n-  /* Figure out the register where the value, if any, will come back.  */\n-  valreg = 0;\n-  if (TYPE_MODE (TREE_TYPE (exp)) != VOIDmode\n-      && ! structure_value_addr)\n-    {\n-      if (pcc_struct_value)\n-\tvalreg = hard_function_value (build_pointer_type (TREE_TYPE (exp)),\n-\t\t\t\t      fndecl, 0);\n-      else\n-\tvalreg = hard_function_value (TREE_TYPE (exp), fndecl, 0);\n-    }\n+      /* Figure out the register where the value, if any, will come back.  */\n+      valreg = 0;\n+      if (TYPE_MODE (TREE_TYPE (exp)) != VOIDmode\n+\t  && ! structure_value_addr)\n+\t{\n+\t  if (pcc_struct_value)\n+\t    valreg = hard_function_value (build_pointer_type (TREE_TYPE (exp)),\n+\t\t\t\t\t  fndecl, 0);\n+\t  else\n+\t    valreg = hard_function_value (TREE_TYPE (exp), fndecl, 0);\n+\t}\n \n-  /* Precompute all register parameters.  It isn't safe to compute anything\n-     once we have started filling any specific hard regs.  */\n-  precompute_register_parameters (num_actuals, args, &reg_parm_seen);\n+      /* Precompute all register parameters.  It isn't safe to compute anything\n+\t once we have started filling any specific hard regs.  */\n+      precompute_register_parameters (num_actuals, args, &reg_parm_seen);\n \n #if defined(ACCUMULATE_OUTGOING_ARGS) && defined(REG_PARM_STACK_SPACE)\n-\n-  /* Save the fixed argument area if it's part of the caller's frame and\n-     is clobbered by argument setup for this call.  */\n-  save_area = save_fixed_argument_area (reg_parm_stack_space, argblock,\n-\t\t\t\t\t&low_to_save, &high_to_save);\n+      /* Save the fixed argument area if it's part of the caller's frame and\n+\t is clobbered by argument setup for this call.  */\n+      save_area = save_fixed_argument_area (reg_parm_stack_space, argblock,\n+\t\t\t\t\t    &low_to_save, &high_to_save);\n #endif\n-\t\t\t\n \n-  /* Now store (and compute if necessary) all non-register parms.\n-     These come before register parms, since they can require block-moves,\n-     which could clobber the registers used for register parms.\n-     Parms which have partial registers are not stored here,\n-     but we do preallocate space here if they want that.  */\n+      /* Now store (and compute if necessary) all non-register parms.\n+\t These come before register parms, since they can require block-moves,\n+\t which could clobber the registers used for register parms.\n+\t Parms which have partial registers are not stored here,\n+\t but we do preallocate space here if they want that.  */\n \n-  for (i = 0; i < num_actuals; i++)\n-    if (args[i].reg == 0 || args[i].pass_on_stack)\n-      store_one_arg (&args[i], argblock, may_be_alloca,\n-\t\t     args_size.var != 0, reg_parm_stack_space);\n-\n-  /* If we have a parm that is passed in registers but not in memory\n-     and whose alignment does not permit a direct copy into registers,\n-     make a group of pseudos that correspond to each register that we\n-     will later fill.  */\n-  if (STRICT_ALIGNMENT)\n-    store_unaligned_arguments_into_pseudos (args, num_actuals);\n-\n-  /* Now store any partially-in-registers parm.\n-     This is the last place a block-move can happen.  */\n-  if (reg_parm_seen)\n-    for (i = 0; i < num_actuals; i++)\n-      if (args[i].partial != 0 && ! args[i].pass_on_stack)\n-\tstore_one_arg (&args[i], argblock, may_be_alloca,\n-\t\t       args_size.var != 0, reg_parm_stack_space);\n+      for (i = 0; i < num_actuals; i++)\n+\tif (args[i].reg == 0 || args[i].pass_on_stack)\n+\t  store_one_arg (&args[i], argblock, may_be_alloca,\n+\t\t\t args_size.var != 0, reg_parm_stack_space);\n+\n+      /* If we have a parm that is passed in registers but not in memory\n+\t and whose alignment does not permit a direct copy into registers,\n+\t make a group of pseudos that correspond to each register that we\n+\t will later fill.  */\n+      if (STRICT_ALIGNMENT)\n+\tstore_unaligned_arguments_into_pseudos (args, num_actuals);\n+\n+      /* Now store any partially-in-registers parm.\n+\t This is the last place a block-move can happen.  */\n+      if (reg_parm_seen)\n+\tfor (i = 0; i < num_actuals; i++)\n+\t  if (args[i].partial != 0 && ! args[i].pass_on_stack)\n+\t    store_one_arg (&args[i], argblock, may_be_alloca,\n+\t\t\t   args_size.var != 0, reg_parm_stack_space);\n \n #ifndef PUSH_ARGS_REVERSED\n #ifdef PREFERRED_STACK_BOUNDARY\n-  /* If we pushed args in forward order, perform stack alignment\n-     after pushing the last arg.  */\n-  if (argblock == 0)\n-    anti_adjust_stack (GEN_INT (args_size.constant - unadjusted_args_size));\n+      /* If we pushed args in forward order, perform stack alignment\n+\t after pushing the last arg.  */\n+      /* ??? Fix for arg_space_so_far.  */\n+      if (argblock == 0)\n+\tanti_adjust_stack (GEN_INT (args_size.constant\n+\t\t\t\t    - unadjusted_args_size));\n #endif\n #endif\n \n-  /* If register arguments require space on the stack and stack space\n-     was not preallocated, allocate stack space here for arguments\n-     passed in registers.  */\n+      /* If register arguments require space on the stack and stack space\n+\t was not preallocated, allocate stack space here for arguments\n+\t passed in registers.  */\n #if ! defined(ACCUMULATE_OUTGOING_ARGS) && defined(OUTGOING_REG_PARM_STACK_SPACE)\n-  if (must_preallocate == 0 && reg_parm_stack_space > 0)\n-    anti_adjust_stack (GEN_INT (reg_parm_stack_space));\n+      if (must_preallocate == 0 && reg_parm_stack_space > 0)\n+\tanti_adjust_stack (GEN_INT (reg_parm_stack_space));\n #endif\n \n-  /* Pass the function the address in which to return a structure value.  */\n-  if (structure_value_addr && ! structure_value_addr_parm)\n-    {\n-      emit_move_insn (struct_value_rtx,\n-\t\t      force_reg (Pmode,\n-\t\t\t\t force_operand (structure_value_addr,\n-\t\t\t\t\t\tNULL_RTX)));\n-\n-      /* Mark the memory for the aggregate as write-only.  */\n-      if (current_function_check_memory_usage)\n-\temit_library_call (chkr_set_right_libfunc, 1,\n-\t\t\t   VOIDmode, 3,\n-\t\t\t   structure_value_addr, Pmode, \n-\t\t\t   GEN_INT (struct_value_size), TYPE_MODE (sizetype),\n-\t\t\t   GEN_INT (MEMORY_USE_WO),\n-\t\t\t   TYPE_MODE (integer_type_node));\n-\n-      if (GET_CODE (struct_value_rtx) == REG)\n-\t  use_reg (&call_fusage, struct_value_rtx);\n-    }\n-\n-  funexp = prepare_call_address (funexp, fndecl, &call_fusage, reg_parm_seen);\n-\n-  load_register_parameters (args, num_actuals, &call_fusage);\n-\n-  /* Perform postincrements before actually calling the function.  */\n-  emit_queue ();\n-\n-  /* Save a pointer to the last insn before the call, so that we can\n-     later safely search backwards to find the CALL_INSN.  */\n-  before_call = get_last_insn ();\n+      /* Pass the function the address in which to return a\n+\t structure value.  */\n+      if (pass != 0 && structure_value_addr && ! structure_value_addr_parm)\n+\t{\n+\t  emit_move_insn (struct_value_rtx,\n+\t\t\t  force_reg (Pmode,\n+\t\t\t\t     force_operand (structure_value_addr,\n+\t\t\t\t\t\t    NULL_RTX)));\n+\n+\t  /* Mark the memory for the aggregate as write-only.  */\n+\t  if (current_function_check_memory_usage)\n+\t    emit_library_call (chkr_set_right_libfunc, 1,\n+\t\t\t       VOIDmode, 3,\n+\t\t\t       structure_value_addr, ptr_mode, \n+\t\t\t       GEN_INT (struct_value_size),\n+\t\t\t       TYPE_MODE (sizetype),\n+\t\t\t       GEN_INT (MEMORY_USE_WO),\n+\t\t\t       TYPE_MODE (integer_type_node));\n+\n+\t  if (GET_CODE (struct_value_rtx) == REG)\n+\t    use_reg (&call_fusage, struct_value_rtx);\n+\t}\n \n-  /* All arguments and registers used for the call must be set up by now!  */\n+      funexp = prepare_call_address (funexp, fndecl, &call_fusage,\n+\t\t\t\t     reg_parm_seen);\n \n-  /* Generate the actual call instruction.  */\n-  emit_call_1 (funexp, fndecl, funtype, unadjusted_args_size,\n-\t       args_size.constant, struct_value_size,\n-\t       FUNCTION_ARG (args_so_far, VOIDmode, void_type_node, 1),\n-\t       valreg, old_inhibit_defer_pop, call_fusage, is_const, nothrow);\n+      load_register_parameters (args, num_actuals, &call_fusage);\n+     \n+      /* Perform postincrements before actually calling the function.  */\n+      emit_queue ();\n \n-  /* If call is cse'able, make appropriate pair of reg-notes around it.\n-     Test valreg so we don't crash; may safely ignore `const'\n-     if return type is void.  Disable for PARALLEL return values, because\n-     we have no way to move such values into a pseudo register.  */\n-  if (is_const && valreg != 0 && GET_CODE (valreg) != PARALLEL)\n-    {\n-      rtx note = 0;\n-      rtx temp = gen_reg_rtx (GET_MODE (valreg));\n-      rtx insns;\n+      /* Save a pointer to the last insn before the call, so that we can\n+\t later safely search backwards to find the CALL_INSN.  */\n+      before_call = get_last_insn ();\n \n-      /* Mark the return value as a pointer if needed.  */\n-      if (TREE_CODE (TREE_TYPE (exp)) == POINTER_TYPE)\n+      /* All arguments and registers used for the call must be set up by\n+\t now!  */\n+\n+      /* Generate the actual call instruction.  */\n+      emit_call_1 (funexp, fndecl, funtype, unadjusted_args_size,\n+\t\t   args_size.constant, struct_value_size,\n+\t\t   FUNCTION_ARG (args_so_far, VOIDmode, void_type_node, 1),\n+\t\t   valreg, old_inhibit_defer_pop, call_fusage,\n+\t\t   ((is_const ? ECF_IS_CONST : 0)\n+\t\t    | (nothrow ? ECF_NOTHROW : 0)\n+\t\t    | (pass == 0 ? ECF_SIBCALL : 0)));\n+\n+      /* If call is cse'able, make appropriate pair of reg-notes around it.\n+\t Test valreg so we don't crash; may safely ignore `const'\n+\t if return type is void.  Disable for PARALLEL return values, because\n+\t we have no way to move such values into a pseudo register.  */\n+      if (is_const && valreg != 0 && GET_CODE (valreg) != PARALLEL)\n \t{\n-\t  tree pointed_to = TREE_TYPE (TREE_TYPE (exp));\n-\t  mark_reg_pointer (temp, TYPE_ALIGN (pointed_to) / BITS_PER_UNIT);\n-\t}\n+\t  rtx note = 0;\n+\t  rtx temp = gen_reg_rtx (GET_MODE (valreg));\n+\t  rtx insns;\n \n-      /* Construct an \"equal form\" for the value which mentions all the\n-\t arguments in order as well as the function name.  */\n+\t  /* Mark the return value as a pointer if needed.  */\n+\t  if (TREE_CODE (TREE_TYPE (exp)) == POINTER_TYPE)\n+\t    {\n+\t      tree pointed_to = TREE_TYPE (TREE_TYPE (exp));\n+\t      mark_reg_pointer (temp, TYPE_ALIGN (pointed_to) / BITS_PER_UNIT);\n+\t    }\n+\n+\t  /* Construct an \"equal form\" for the value which mentions all the\n+\t     arguments in order as well as the function name.  */\n #ifdef PUSH_ARGS_REVERSED\n-      for (i = 0; i < num_actuals; i++)\n-\tnote = gen_rtx_EXPR_LIST (VOIDmode, args[i].initial_value, note);\n+\t  for (i = 0; i < num_actuals; i++)\n+\t    note = gen_rtx_EXPR_LIST (VOIDmode, args[i].initial_value, note);\n #else\n-      for (i = num_actuals - 1; i >= 0; i--)\n-\tnote = gen_rtx_EXPR_LIST (VOIDmode, args[i].initial_value, note);\n+\t  for (i = num_actuals - 1; i >= 0; i--)\n+\t    note = gen_rtx_EXPR_LIST (VOIDmode, args[i].initial_value, note);\n #endif\n-      note = gen_rtx_EXPR_LIST (VOIDmode, funexp, note);\n-\n-      insns = get_insns ();\n-      end_sequence ();\n-\n-      emit_libcall_block (insns, temp, valreg, note);\n+\t  note = gen_rtx_EXPR_LIST (VOIDmode, funexp, note);\n \n-      valreg = temp;\n-    }\n-  else if (is_const)\n-    {\n-      /* Otherwise, just write out the sequence without a note.  */\n-      rtx insns = get_insns ();\n-\n-      end_sequence ();\n-      emit_insns (insns);\n-    }\n-  else if (is_malloc)\n-    {\n-      rtx temp = gen_reg_rtx (GET_MODE (valreg));\n-      rtx last, insns;\n+\t  insns = get_insns ();\n+\t  end_sequence ();\n \n-      /* The return value from a malloc-like function is a pointer. */\n-      if (TREE_CODE (TREE_TYPE (exp)) == POINTER_TYPE)\n-\tmark_reg_pointer (temp, BIGGEST_ALIGNMENT / BITS_PER_UNIT);\n-\n-      emit_move_insn (temp, valreg);\n-\n-      /* The return value from a malloc-like function can not alias\n-\t anything else.  */\n-      last = get_last_insn ();\n-      REG_NOTES (last) = \n-\tgen_rtx_EXPR_LIST (REG_NOALIAS, temp, REG_NOTES (last));\n+\t  emit_libcall_block (insns, temp, valreg, note);\n+  \n+\t  valreg = temp;\n+\t}\n+      else if (is_const)\n+\t{\n+\t  /* Otherwise, just write out the sequence without a note.  */\n+\t  rtx insns = get_insns ();\n \n-      /* Write out the sequence.  */\n-      insns = get_insns ();\n-      end_sequence ();\n-      emit_insns (insns);\n-      valreg = temp;\n-    }\n+\t  end_sequence ();\n+\t  emit_insns (insns);\n+\t}\n+      else if (is_malloc)\n+\t{\n+\t  rtx temp = gen_reg_rtx (GET_MODE (valreg));\n+\t  rtx last, insns;\n+\n+\t  /* The return value from a malloc-like function is a pointer. */\n+\t  if (TREE_CODE (TREE_TYPE (exp)) == POINTER_TYPE)\n+\t    mark_reg_pointer (temp, BIGGEST_ALIGNMENT / BITS_PER_UNIT);\n+\n+\t  emit_move_insn (temp, valreg);\n+\n+\t  /* The return value from a malloc-like function can not alias\n+\t     anything else.  */\n+\t  last = get_last_insn ();\n+\t  REG_NOTES (last) = \n+\t    gen_rtx_EXPR_LIST (REG_NOALIAS, temp, REG_NOTES (last));\n+\n+\t  /* Write out the sequence.  */\n+\t  insns = get_insns ();\n+\t  end_sequence ();\n+\t  emit_insns (insns);\n+\t  valreg = temp;\n+\t}\n \n-  /* For calls to `setjmp', etc., inform flow.c it should complain\n-     if nonvolatile values are live.  */\n+      /* For calls to `setjmp', etc., inform flow.c it should complain\n+\t if nonvolatile values are live.  For functions that cannot return,\n+\t inform flow that control does not fall through.  */\n \n-  if (returns_twice)\n-    {\n-      /* The NOTE_INSN_SETJMP note must be emitted immediately after the\n-\t CALL_INSN.  Some ports emit more than just a CALL_INSN above, so\n-\t we must search for it here.  */\n-      rtx last = get_last_insn ();\n-      while (GET_CODE (last) != CALL_INSN)\n+      if (returns_twice || is_volatile || is_longjmp || pass == 0)\n \t{\n-\t  last = PREV_INSN (last);\n-\t  /* There was no CALL_INSN?  */\n-\t  if (last == before_call)\n-\t    abort ();\n-\t}\n-      emit_note_after (NOTE_INSN_SETJMP, last);\n-      current_function_calls_setjmp = 1;\n-    }\n+\t  /* The barrier or NOTE_INSN_SETJMP note must be emitted\n+\t     immediately after the CALL_INSN.  Some ports emit more\n+\t     than just a CALL_INSN above, so we must search for it here.  */\n \n-  if (is_longjmp)\n-    current_function_calls_longjmp = 1;\n+\t  rtx last = get_last_insn ();\n+\t  while (GET_CODE (last) != CALL_INSN)\n+\t    {\n+\t      last = PREV_INSN (last);\n+\t      /* There was no CALL_INSN?  */\n+\t      if (last == before_call)\n+\t\tabort ();\n+\t    }\n \n-  /* Notice functions that cannot return.\n-     If optimizing, insns emitted below will be dead.\n-     If not optimizing, they will exist, which is useful\n-     if the user uses the `return' command in the debugger.  */\n+\t  if (returns_twice)\n+\t    {\n+\t      emit_note_after (NOTE_INSN_SETJMP, last);\n+\t      current_function_calls_setjmp = 1;\n+\t      sibcall_failure = 1;\n+\t    }\n+\t  else\n+\t    emit_barrier_after (last);\n+\t}\n \n-  if (is_volatile || is_longjmp)\n-    emit_barrier ();\n+      if (is_longjmp)\n+\tcurrent_function_calls_longjmp = 1, sibcall_failure = 1;\n \n-  /* If value type not void, return an rtx for the value.  */\n+      /* If value type not void, return an rtx for the value.  */\n \n-  /* If there are cleanups to be called, don't use a hard reg as target.\n-     We need to double check this and see if it matters anymore.  */\n-  if (any_pending_cleanups (1)\n-      && target && REG_P (target)\n-      && REGNO (target) < FIRST_PSEUDO_REGISTER)\n-    target = 0;\n+      /* If there are cleanups to be called, don't use a hard reg as target.\n+\t We need to double check this and see if it matters anymore.  */\n+      if (any_pending_cleanups (1)\n+\t  && target && REG_P (target)\n+\t  && REGNO (target) < FIRST_PSEUDO_REGISTER)\n+\ttarget = 0, sibcall_failure = 1;\n \n-  if (TYPE_MODE (TREE_TYPE (exp)) == VOIDmode\n-      || ignore)\n-    {\n-      target = const0_rtx;\n-    }\n-  else if (structure_value_addr)\n-    {\n-      if (target == 0 || GET_CODE (target) != MEM)\n+      if (TYPE_MODE (TREE_TYPE (exp)) == VOIDmode\n+\t  || ignore)\n \t{\n-\t  target = gen_rtx_MEM (TYPE_MODE (TREE_TYPE (exp)),\n-\t\t\t\tmemory_address (TYPE_MODE (TREE_TYPE (exp)),\n-\t\t\t\t\t\tstructure_value_addr));\n-\t  MEM_SET_IN_STRUCT_P (target,\n-\t\t\t       AGGREGATE_TYPE_P (TREE_TYPE (exp)));\n+\t  target = const0_rtx;\n \t}\n-    }\n-  else if (pcc_struct_value)\n-    {\n-      /* This is the special C++ case where we need to\n-\t know what the true target was.  We take care to\n-\t never use this value more than once in one expression.  */\n-      target = gen_rtx_MEM (TYPE_MODE (TREE_TYPE (exp)),\n-\t\t\t    copy_to_reg (valreg));\n-      MEM_SET_IN_STRUCT_P (target, AGGREGATE_TYPE_P (TREE_TYPE (exp)));\n-    }\n-  /* Handle calls that return values in multiple non-contiguous locations.\n-     The Irix 6 ABI has examples of this.  */\n-  else if (GET_CODE (valreg) == PARALLEL)\n-    {\n-      int bytes = int_size_in_bytes (TREE_TYPE (exp));\n-\n-      if (target == 0)\n+      else if (structure_value_addr)\n \t{\n-\t  target = assign_stack_temp (TYPE_MODE (TREE_TYPE (exp)), bytes, 0);\n+\t  if (target == 0 || GET_CODE (target) != MEM)\n+\t    {\n+\t      target = gen_rtx_MEM (TYPE_MODE (TREE_TYPE (exp)),\n+\t\t\t\t    memory_address (TYPE_MODE (TREE_TYPE (exp)),\n+\t\t\t\t\t\t    structure_value_addr));\n+\t      MEM_SET_IN_STRUCT_P (target,\n+\t\t\t\t   AGGREGATE_TYPE_P (TREE_TYPE (exp)));\n+\t    }\n+\t}\n+      else if (pcc_struct_value)\n+\t{\n+\t  /* This is the special C++ case where we need to\n+\t     know what the true target was.  We take care to\n+\t     never use this value more than once in one expression.  */\n+\t  target = gen_rtx_MEM (TYPE_MODE (TREE_TYPE (exp)),\n+\t\t\t\tcopy_to_reg (valreg));\n \t  MEM_SET_IN_STRUCT_P (target, AGGREGATE_TYPE_P (TREE_TYPE (exp)));\n-\t  preserve_temp_slots (target);\n \t}\n+      /* Handle calls that return values in multiple non-contiguous locations.\n+\t The Irix 6 ABI has examples of this.  */\n+      else if (GET_CODE (valreg) == PARALLEL)\n+\t{\n+\t  int bytes = int_size_in_bytes (TREE_TYPE (exp));\n \n-      if (! rtx_equal_p (target, valreg))\n-        emit_group_store (target, valreg, bytes,\n-\t\t\t  TYPE_ALIGN (TREE_TYPE (exp)) / BITS_PER_UNIT);\n-    }\n-  else if (target && GET_MODE (target) == TYPE_MODE (TREE_TYPE (exp))\n-\t   && GET_MODE (target) == GET_MODE (valreg))\n-    /* TARGET and VALREG cannot be equal at this point because the latter\n-       would not have REG_FUNCTION_VALUE_P true, while the former would if\n-       it were referring to the same register.\n-\n-       If they refer to the same register, this move will be a no-op, except\n-       when function inlining is being done.  */\n-    emit_move_insn (target, valreg);\n-  else if (TYPE_MODE (TREE_TYPE (exp)) == BLKmode)\n-    target = copy_blkmode_from_reg (target, valreg, TREE_TYPE (exp));\n-  else\n-    target = copy_to_reg (valreg);\n+\t  if (target == 0)\n+\t    {\n+\t      target = assign_stack_temp (TYPE_MODE (TREE_TYPE (exp)),\n+\t\t\t\t\t  bytes, 0);\n+\t      MEM_SET_IN_STRUCT_P (target, AGGREGATE_TYPE_P (TREE_TYPE (exp)));\n+\t      preserve_temp_slots (target);\n+\t    }\n+\n+\t  if (! rtx_equal_p (target, valreg))\n+\t    emit_group_store (target, valreg, bytes,\n+\t\t\t      TYPE_ALIGN (TREE_TYPE (exp)) / BITS_PER_UNIT);\n+\t  /* We can not support sibling calls for this case.  */\n+\t  sibcall_failure = 1;\n+\t}\n+      else if (target\n+\t       && GET_MODE (target) == TYPE_MODE (TREE_TYPE (exp))\n+\t       && GET_MODE (target) == GET_MODE (valreg))\n+\t{\n+\t  /* TARGET and VALREG cannot be equal at this point because the\n+\t     latter would not have REG_FUNCTION_VALUE_P true, while the\n+\t     former would if it were referring to the same register.\n+\n+\t     If they refer to the same register, this move will be a no-op,\n+\t     except when function inlining is being done.  */\n+\t  emit_move_insn (target, valreg);\n+\t}\n+      else if (TYPE_MODE (TREE_TYPE (exp)) == BLKmode)\n+\ttarget = copy_blkmode_from_reg (target, valreg, TREE_TYPE (exp));\n+      else\n+\ttarget = copy_to_reg (valreg);\n \n #ifdef PROMOTE_FUNCTION_RETURN\n-  /* If we promoted this return value, make the proper SUBREG.  TARGET\n-     might be const0_rtx here, so be careful.  */\n-  if (GET_CODE (target) == REG\n-      && TYPE_MODE (TREE_TYPE (exp)) != BLKmode\n-      && GET_MODE (target) != TYPE_MODE (TREE_TYPE (exp)))\n-    {\n-      tree type = TREE_TYPE (exp);\n-      int unsignedp = TREE_UNSIGNED (type);\n+      /* If we promoted this return value, make the proper SUBREG.  TARGET\n+\t might be const0_rtx here, so be careful.  */\n+      if (GET_CODE (target) == REG\n+\t  && TYPE_MODE (TREE_TYPE (exp)) != BLKmode\n+\t  && GET_MODE (target) != TYPE_MODE (TREE_TYPE (exp)))\n+\t{\n+\t  tree type = TREE_TYPE (exp);\n+\t  int unsignedp = TREE_UNSIGNED (type);\n \n-      /* If we don't promote as expected, something is wrong.  */\n-      if (GET_MODE (target)\n-\t  != promote_mode (type, TYPE_MODE (type), &unsignedp, 1))\n-\tabort ();\n+\t  /* If we don't promote as expected, something is wrong.  */\n+\t  if (GET_MODE (target)\n+\t      != promote_mode (type, TYPE_MODE (type), &unsignedp, 1))\n+\t    abort ();\n \n-      target = gen_rtx_SUBREG (TYPE_MODE (type), target, 0);\n-      SUBREG_PROMOTED_VAR_P (target) = 1;\n-      SUBREG_PROMOTED_UNSIGNED_P (target) = unsignedp;\n-    }\n+\t  target = gen_rtx_SUBREG (TYPE_MODE (type), target, 0);\n+\t  SUBREG_PROMOTED_VAR_P (target) = 1;\n+\t  SUBREG_PROMOTED_UNSIGNED_P (target) = unsignedp;\n+\t}\n #endif\n \n-  /* If size of args is variable or this was a constructor call for a stack\n-     argument, restore saved stack-pointer value.  */\n+      /* If size of args is variable or this was a constructor call for a stack\n+\t argument, restore saved stack-pointer value.  */\n \n-  if (old_stack_level)\n-    {\n-      emit_stack_restore (SAVE_BLOCK, old_stack_level, NULL_RTX);\n-      pending_stack_adjust = old_pending_adj;\n+      if (old_stack_level)\n+\t{\n+\t  emit_stack_restore (SAVE_BLOCK, old_stack_level, NULL_RTX);\n+\t  pending_stack_adjust = old_pending_adj;\n #ifdef ACCUMULATE_OUTGOING_ARGS\n-      stack_arg_under_construction = old_stack_arg_under_construction;\n-      highest_outgoing_arg_in_use = initial_highest_arg_in_use;\n-      stack_usage_map = initial_stack_usage_map;\n+\t  stack_arg_under_construction = old_stack_arg_under_construction;\n+\t  highest_outgoing_arg_in_use = initial_highest_arg_in_use;\n+\t  stack_usage_map = initial_stack_usage_map;\n #endif\n-    }\n+\t  sibcall_failure = 1;\n+\t}\n #ifdef ACCUMULATE_OUTGOING_ARGS\n-  else\n-    {\n+      else\n+\t{\n #ifdef REG_PARM_STACK_SPACE\n-      if (save_area)\n-\trestore_fixed_argument_area (save_area, argblock,\n-\t\t\t\t     high_to_save, low_to_save);\n+\t  if (save_area)\n+\t    {\n+\t      restore_fixed_argument_area (save_area, argblock,\n+\t\t\t\t\t   high_to_save, low_to_save);\n+\t      sibcall_failure = 1;\n+\t    }\n #endif\n \n-      /* If we saved any argument areas, restore them.  */\n-      for (i = 0; i < num_actuals; i++)\n-\tif (args[i].save_area)\n-\t  {\n-\t    enum machine_mode save_mode = GET_MODE (args[i].save_area);\n-\t    rtx stack_area\n-\t      = gen_rtx_MEM (save_mode,\n-\t\t\t     memory_address (save_mode,\n-\t\t\t\t\t     XEXP (args[i].stack_slot, 0)));\n-\n-\t    if (save_mode != BLKmode)\n-\t      emit_move_insn (stack_area, args[i].save_area);\n-\t    else\n-\t      emit_block_move (stack_area, validize_mem (args[i].save_area),\n-\t\t\t       GEN_INT (args[i].size.constant),\n-\t\t\t       PARM_BOUNDARY / BITS_PER_UNIT);\n-\t  }\n+\t  /* If we saved any argument areas, restore them.  */\n+\t  for (i = 0; i < num_actuals; i++)\n+\t    if (args[i].save_area)\n+\t      {\n+\t\tenum machine_mode save_mode = GET_MODE (args[i].save_area);\n+\t\trtx stack_area\n+\t\t  = gen_rtx_MEM (save_mode,\n+\t\t\t\t memory_address (save_mode,\n+\t\t\t\t\t\t XEXP (args[i].stack_slot, 0)));\n+\n+\t\tif (save_mode != BLKmode)\n+\t\t  emit_move_insn (stack_area, args[i].save_area);\n+\t\telse\n+\t\t  emit_block_move (stack_area,\n+\t\t\t\t   validize_mem (args[i].save_area),\n+\t\t\t\t   GEN_INT (args[i].size.constant),\n+\t\t\t\t   PARM_BOUNDARY / BITS_PER_UNIT);\n+\t\tsibcall_failure = 1;\n+\t      }\n \n-      highest_outgoing_arg_in_use = initial_highest_arg_in_use;\n-      stack_usage_map = initial_stack_usage_map;\n-    }\n+\t  highest_outgoing_arg_in_use = initial_highest_arg_in_use;\n+\t  stack_usage_map = initial_stack_usage_map;\n+\t}\n #endif\n \n-  /* If this was alloca, record the new stack level for nonlocal gotos.  \n-     Check for the handler slots since we might not have a save area\n-     for non-local gotos.  */\n+      /* If this was alloca, record the new stack level for nonlocal gotos.  \n+\t Check for the handler slots since we might not have a save area\n+\t for non-local gotos.  */\n \n-  if (may_be_alloca && nonlocal_goto_handler_slots != 0)\n-    emit_stack_save (SAVE_NONLOCAL, &nonlocal_goto_stack_level, NULL_RTX);\n+      if (may_be_alloca && nonlocal_goto_handler_slots != 0)\n+\temit_stack_save (SAVE_NONLOCAL, &nonlocal_goto_stack_level, NULL_RTX);\n \n-  pop_temp_slots ();\n+      pop_temp_slots ();\n+\n+      /* Free up storage we no longer need.  */\n+      for (i = 0; i < num_actuals; ++i)\n+\tif (args[i].aligned_regs)\n+\t  free (args[i].aligned_regs);\n+\n+      insns = get_insns ();\n+      end_sequence ();\n+\n+      if (pass == 0)\n+\t{\n+\t  tail_call_insns = insns;\n+\n+\t  /* If the current function's argument block is not large enough\n+\t     to hold the outoing arguments, or we encountered some other\n+\t     situation we couldn't handle, zero out the sequence.  */\n+\t  if (current_function_args_size < args_size.constant\n+\t      || sibcall_failure)\n+\t    tail_call_insns = NULL_RTX;\n+\n+\t  /* Restore the pending stack adjustment now that we have\n+\t     finished generating the sibling call sequence.  */\n+\t  pending_stack_adjust = save_pending_stack_adjust;\n+\t}\n+      else\n+\tnormal_call_insns = insns;\n+    }\n+\n+  /* The function optimize_sibling_and_tail_recursive_calls doesn't\n+     handle CALL_PLACEHOLDERs inside other CALL_PLACEHOLDERs.  This\n+     can happen if the arguments to this function call an inline\n+     function who's expansion contains another CALL_PLACEHOLDER.\n+\n+     If there are any C_Ps in any of these sequences, replace them\n+     with their normal call. */\n+\n+  for (insn = normal_call_insns; insn; insn = NEXT_INSN (insn))\n+    if (GET_CODE (insn) == CALL_INSN\n+\t&& GET_CODE (PATTERN (insn)) == CALL_PLACEHOLDER)\n+      replace_call_placeholder (insn, sibcall_use_normal);\n+\n+  for (insn = tail_call_insns; insn; insn = NEXT_INSN (insn))\n+    if (GET_CODE (insn) == CALL_INSN\n+\t&& GET_CODE (PATTERN (insn)) == CALL_PLACEHOLDER)\n+      replace_call_placeholder (insn, sibcall_use_normal);\n+\n+  for (insn = tail_recursion_insns; insn; insn = NEXT_INSN (insn))\n+    if (GET_CODE (insn) == CALL_INSN\n+\t&& GET_CODE (PATTERN (insn)) == CALL_PLACEHOLDER)\n+      replace_call_placeholder (insn, sibcall_use_normal);\n+\n+  /* If this was a potential tail recursion site, then emit a\n+     CALL_PLACEHOLDER with the normal and the tail recursion streams.\n+     One of them will be selected later.  */\n+  if (tail_recursion_insns || tail_call_insns)\n+    {\n+      /* The tail recursion label must be kept around.  We could expose\n+\t its use in the CALL_PLACEHOLDER, but that creates unwanted edges\n+\t and makes determining true tail recursion sites difficult.\n+\n+\t So we set LABEL_PRESERVE_P here, then clear it when we select\n+\t one of the call sequences after rtl generation is complete.  */\n+      if (tail_recursion_insns)\n+\tLABEL_PRESERVE_P (tail_recursion_label) = 1;\n+      emit_call_insn (gen_rtx_CALL_PLACEHOLDER (VOIDmode, normal_call_insns,\n+\t\t\t\t\t\ttail_call_insns,\n+\t\t\t\t\t\ttail_recursion_insns,\n+\t\t\t\t\t\ttail_recursion_label));\n+    }\n+  else\n+    emit_insns (normal_call_insns);\n \n-  /* Free up storage we no longer need.  */\n-  for (i = 0; i < num_actuals; ++i)\n-    if (args[i].aligned_regs)\n-      free (args[i].aligned_regs);\n+  currently_expanding_call--;\n \n   return target;\n }\n@@ -3171,7 +3488,9 @@ emit_library_call VPARAMS((rtx orgfun, int no_queue, enum machine_mode outmode,\n \t       original_args_size.constant, args_size.constant, 0,\n \t       FUNCTION_ARG (args_so_far, VOIDmode, void_type_node, 1),\n \t       outmode != VOIDmode ? hard_libcall_value (outmode) : NULL_RTX,\n-\t       old_inhibit_defer_pop + 1, call_fusage, no_queue, nothrow);\n+\t       old_inhibit_defer_pop + 1, call_fusage,\n+\t       ((no_queue ? ECF_IS_CONST : 0)\n+\t\t| (nothrow ? ECF_NOTHROW : 0)));\n \n   pop_temp_slots ();\n \n@@ -3772,7 +4091,9 @@ emit_library_call_value VPARAMS((rtx orgfun, rtx value, int no_queue,\n \t       struct_value_size,\n \t       FUNCTION_ARG (args_so_far, VOIDmode, void_type_node, 1),\n \t       mem_value == 0 ? hard_libcall_value (outmode) : NULL_RTX,\n-\t       old_inhibit_defer_pop + 1, call_fusage, is_const, nothrow);\n+\t       old_inhibit_defer_pop + 1, call_fusage,\n+\t       ((is_const ? ECF_IS_CONST : 0)\n+\t\t| (nothrow ? ECF_NOTHROW : 0)));\n \n   /* Now restore inhibit_defer_pop to its actual original value.  */\n   OK_DEFER_POP;"}, {"sha": "90437e0eb020f391a2722e3543efd4e715008068", "filename": "gcc/final.c", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Ffinal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Ffinal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ffinal.c?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -4019,7 +4019,8 @@ leaf_function_p ()\n \treturn 0;\n       if (GET_CODE (insn) == INSN\n \t  && GET_CODE (PATTERN (insn)) == SEQUENCE\n-\t  && GET_CODE (XVECEXP (PATTERN (insn), 0, 0)) == CALL_INSN)\n+\t  && GET_CODE (XVECEXP (PATTERN (insn), 0, 0)) == CALL_INSN\n+\t  && ! SIBLING_CALL_P (XVECEXP (PATTERN (insn), 0, 0)))\n \treturn 0;\n     }\n   for (insn = current_function_epilogue_delay_list; insn; insn = XEXP (insn, 1))\n@@ -4028,7 +4029,8 @@ leaf_function_p ()\n \treturn 0;\n       if (GET_CODE (XEXP (insn, 0)) == INSN\n \t  && GET_CODE (PATTERN (XEXP (insn, 0))) == SEQUENCE\n-\t  && GET_CODE (XVECEXP (PATTERN (XEXP (insn, 0)), 0, 0)) == CALL_INSN)\n+\t  && GET_CODE (XVECEXP (PATTERN (XEXP (insn, 0)), 0, 0)) == CALL_INSN\n+\t  && ! SIBLING_CALL_P (XVECEXP (PATTERN (XEXP (insn, 0)), 0, 0)))\n \treturn 0;\n     }\n "}, {"sha": "14189b9953b760f365bdabd7a65e8b4080a27b3a", "filename": "gcc/flow.c", "status": "modified", "additions": 20, "deletions": 5, "changes": 25, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Fflow.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Fflow.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fflow.c?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -154,10 +154,12 @@ Boston, MA 02111-1307, USA.  */\n #ifndef HAVE_epilogue\n #define HAVE_epilogue 0\n #endif\n-\n #ifndef HAVE_prologue\n #define HAVE_prologue 0\n #endif\n+#ifndef HAVE_sibcall_epilogue\n+#define HAVE_sibcall_epilogue 0\n+#endif\n \n /* The contents of the current function definition are allocated\n    in this obstack, and all are freed at the end of the function.\n@@ -592,7 +594,8 @@ find_basic_blocks_1 (f)\n \t\t does not imply an abnormal edge, it will be a bit before\n \t\t everything can be updated.  So continue to emit a noop at\n \t\t the end of such a block.  */\n-\t      if (GET_CODE (end) == CALL_INSN)\n+\t      if (GET_CODE (end) == CALL_INSN\n+\t\t  && ! SIBLING_CALL_P (end))\n \t\t{\n \t\t  rtx nop = gen_rtx_USE (VOIDmode, const0_rtx);\n \t\t  end = emit_insn_after (nop, end);\n@@ -644,7 +647,8 @@ find_basic_blocks_1 (f)\n \t     imply an abnormal edge, it will be a bit before everything can\n \t     be updated.  So continue to emit a noop at the end of such a\n \t     block.  */\n-\t  if (GET_CODE (end) == CALL_INSN)\n+\t  if (GET_CODE (end) == CALL_INSN\n+\t      && ! SIBLING_CALL_P (end))\n \t    {\n \t      rtx nop = gen_rtx_USE (VOIDmode, const0_rtx);\n \t      end = emit_insn_after (nop, end);\n@@ -973,6 +977,15 @@ make_edges (label_value_list)\n \t    }\n \t}\n \n+      /* If this is a sibling call insn, then this is in effect a \n+\t combined call and return, and so we need an edge to the\n+\t exit block.  No need to worry about EH edges, since we\n+\t wouldn't have created the sibling call in the first place.  */\n+\n+      if (code == CALL_INSN && SIBLING_CALL_P (insn))\n+\tmake_edge (edge_cache, bb, EXIT_BLOCK_PTR, 0);\n+      else\n+\n       /* If this is a CALL_INSN, then mark it as reaching the active EH\n \t handler for this CALL_INSN.  If we're handling asynchronous\n \t exceptions then any insn can reach any of the active handlers.\n@@ -3249,8 +3262,10 @@ propagate_block (bb, old, significant, flags)\n \t     instructions.  Warn about probable compiler losage.  */\n \t  if (insn_is_dead\n \t      && reload_completed\n-\t      && (HAVE_epilogue || HAVE_prologue)\n-\t      && prologue_epilogue_contains (insn))\n+\t      && (((HAVE_epilogue || HAVE_prologue)\n+\t\t   && prologue_epilogue_contains (insn))\n+\t\t  || (HAVE_sibcall_epilogue\n+\t\t      && sibcall_epilogue_contains (insn))))\n \t    {\n \t      if (flags & PROP_KILL_DEAD_CODE)\n \t        { "}, {"sha": "dfd87618396d26e936801db92a7cc5503cd6d651", "filename": "gcc/function.c", "status": "modified", "additions": 293, "deletions": 151, "changes": 444, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Ffunction.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Ffunction.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ffunction.c?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -152,8 +152,12 @@ struct function *cfun = 0;\n struct function *all_functions = 0;\n \n /* These arrays record the INSN_UIDs of the prologue and epilogue insns.  */\n-static int *prologue;\n-static int *epilogue;\n+static varray_type prologue;\n+static varray_type epilogue;\n+\n+/* Array of INSN_UIDs to hold the INSN_UIDs for each sibcall epilogue\n+   in this function.  */\n+static varray_type sibcall_epilogue;\n \f\n /* In order to evaluate some expressions, such as function calls returning\n    structures in memory, we need to temporarily allocate stack locations.\n@@ -271,13 +275,15 @@ static void pad_below\t\tPARAMS ((struct args_size *, enum machine_mode,\n static tree round_down\t\tPARAMS ((tree, int));\n #endif\n static rtx round_trampoline_addr PARAMS ((rtx));\n+static tree *identify_blocks_1\tPARAMS ((rtx, tree *, tree *, tree *));\n+static void reorder_blocks_1\tPARAMS ((rtx, tree, varray_type *));\n static tree blocks_nreverse\tPARAMS ((tree));\n static int all_blocks\t\tPARAMS ((tree, tree *));\n static tree *get_block_vector   PARAMS ((tree, int *));\n /* We always define `record_insns' even if its not used so that we\n    can always export `prologue_epilogue_contains'.  */\n-static int *record_insns\tPARAMS ((rtx)) ATTRIBUTE_UNUSED;\n-static int contains\t\tPARAMS ((rtx, int *));\n+static void record_insns\tPARAMS ((rtx, varray_type *)) ATTRIBUTE_UNUSED;\n+static int contains\t\tPARAMS ((rtx, varray_type));\n #ifdef HAVE_return\n static void emit_return_into_block PARAMS ((basic_block));\n #endif\n@@ -1507,6 +1513,7 @@ fixup_var_refs (var, promoted_mode, unsignedp, ht)\n   rtx first_insn = get_insns ();\n   struct sequence_stack *stack = seq_stack;\n   tree rtl_exps = rtl_expr_chain;\n+  rtx insn;\n \n   /* Must scan all insns for stack-refs that exceed the limit.  */\n   fixup_var_refs_insns (var, promoted_mode, unsignedp, first_insn, \n@@ -1545,6 +1552,31 @@ fixup_var_refs (var, promoted_mode, unsignedp, ht)\n   fixup_var_refs_insns (var, promoted_mode, unsignedp, catch_clauses,\n \t\t\t0, 0);\n   end_sequence ();\n+\n+  /* Scan sequences saved in CALL_PLACEHOLDERS too.  */\n+  for (insn = first_insn; insn; insn = NEXT_INSN (insn))\n+    {\n+      if (GET_CODE (insn) == CALL_INSN\n+\t  && GET_CODE (PATTERN (insn)) == CALL_PLACEHOLDER)\n+\t{\n+\t  int i;\n+\n+\t  /* Look at the Normal call, sibling call and tail recursion\n+\t     sequences attached to the CALL_PLACEHOLDER.  */\n+\t  for (i = 0; i < 3; i++)\n+\t    {\n+\t      rtx seq = XEXP (PATTERN (insn), i);\n+\t      if (seq)\n+\t\t{\n+\t\t  push_to_sequence (seq);\n+\t\t  fixup_var_refs_insns (var, promoted_mode, unsignedp,\n+\t\t\t\t\tseq, 0, 0);\n+\t\t  XEXP (PATTERN (insn), i) = get_insns ();\n+\t\t  end_sequence ();\n+\t\t}\n+\t    }\n+\t}\n+    }\n }\n \f\n /* REPLACEMENTS is a pointer to a list of the struct fixup_replacement and X is\n@@ -5494,11 +5526,8 @@ identify_blocks (block, insns)\n      rtx insns;\n {\n   int n_blocks;\n-  tree *block_vector;\n+  tree *block_vector, *last_block_vector;\n   tree *block_stack;\n-  int depth = 0;\n-  int current_block_number = 1;\n-  rtx insn;\n \n   if (block == 0)\n     return;\n@@ -5508,35 +5537,83 @@ identify_blocks (block, insns)\n   block_vector = get_block_vector (block, &n_blocks);\n   block_stack = (tree *) xmalloc (n_blocks * sizeof (tree));\n \n+  last_block_vector = identify_blocks_1 (insns, block_vector + 1,\n+\t\t\t\t\t block_vector + n_blocks, block_stack);\n+\n+  /* If we didn't use all of the subblocks, we've misplaced block notes.  */\n+  /* ??? This appears to happen all the time.  Latent bugs elsewhere?  */\n+  if (0 && last_block_vector != block_vector + n_blocks)\n+    abort ();\n+\n+  free (block_vector);\n+  free (block_stack);\n+}\n+\n+/* Subroutine of identify_blocks.  Do the block substitution on the\n+   insn chain beginning with INSNS.  Recurse for CALL_PLACEHOLDER chains.\n+\n+   BLOCK_STACK is pushed and popped for each BLOCK_BEGIN/BLOCK_END pair.\n+   BLOCK_VECTOR is incremented for each block seen.  */\n+\n+static tree *\n+identify_blocks_1 (insns, block_vector, end_block_vector, orig_block_stack)\n+     rtx insns;\n+     tree *block_vector;\n+     tree *end_block_vector;\n+     tree *orig_block_stack;\n+{\n+  rtx insn;\n+  tree *block_stack = orig_block_stack;\n+\n   for (insn = insns; insn; insn = NEXT_INSN (insn))\n-    if (GET_CODE (insn) == NOTE)\n-      {\n-\tif (NOTE_LINE_NUMBER (insn) == NOTE_INSN_BLOCK_BEG)\n-\t  {\n-\t    tree b;\n+    {\n+      if (GET_CODE (insn) == NOTE)\n+\t{\n+\t  if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_BLOCK_BEG)\n+\t    {\n+\t      tree b;\n \n-\t    /* If there are more block notes than BLOCKs, something\n-\t       is badly wrong.  */\n-\t    if (current_block_number == n_blocks)\n-\t      abort ();\n+\t      /* If there are more block notes than BLOCKs, something\n+\t\t is badly wrong.  */\n+\t      if (block_vector == end_block_vector)\n+\t\tabort ();\n \n-\t    b = block_vector[current_block_number++];\n-\t    NOTE_BLOCK (insn) = b;\n-\t    block_stack[depth++] = b;\n-\t  }\n-\telse if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_BLOCK_END)\n-\t  {\n-\t    if (depth == 0)\n-\t      /* There are more NOTE_INSN_BLOCK_ENDs that\n-\t\t NOTE_INSN_BLOCK_BEGs.  Something is badly wrong.  */\n-\t      abort ();\n+\t      b = *block_vector++;\n+\t      NOTE_BLOCK (insn) = b;\n+\t      *block_stack++ = b;\n+\t    }\n+\t  else if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_BLOCK_END)\n+\t    {\n+\t      /* If there are more NOTE_INSN_BLOCK_ENDs than\n+\t\t NOTE_INSN_BLOCK_BEGs, something is badly wrong.  */\n+\t      if (block_stack == orig_block_stack)\n+\t\tabort ();\n \n-\t    NOTE_BLOCK (insn) = block_stack[--depth];\n-\t  }\n-      }\n+\t      NOTE_BLOCK (insn) = *--block_stack;\n+\t    }\n+        }\n+      else if (GET_CODE (insn) == CALL_INSN\n+\t       && GET_CODE (PATTERN (insn)) == CALL_PLACEHOLDER)\n+\t{\n+\t  rtx cp = PATTERN (insn);\n+\n+\t  block_vector = identify_blocks_1 (XEXP (cp, 0), block_vector, \n+\t\t\t\t            end_block_vector, block_stack);\n+\t  if (XEXP (cp, 1))\n+\t    block_vector = identify_blocks_1 (XEXP (cp, 1), block_vector,\n+\t\t\t\t\t      end_block_vector, block_stack);\n+\t  if (XEXP (cp, 2))\n+\t    block_vector = identify_blocks_1 (XEXP (cp, 2), block_vector,\n+\t\t\t\t\t      end_block_vector, block_stack);\n+\t}\n+    }\n \n-  free (block_vector);\n-  free (block_stack);\n+  /* If there are more NOTE_INSN_BLOCK_BEGINs than NOTE_INSN_BLOCK_ENDs,\n+     something is badly wrong.  */\n+  if (block_stack != orig_block_stack)\n+    abort ();\n+\n+  return block_vector;\n }\n \n /* Given a revised instruction chain, rebuild the tree structure of\n@@ -5550,7 +5627,6 @@ reorder_blocks (block, insns)\n      rtx insns;\n {\n   tree current_block = block;\n-  rtx insn;\n   varray_type block_stack;\n \n   if (block == NULL_TREE)\n@@ -5562,35 +5638,7 @@ reorder_blocks (block, insns)\n   BLOCK_SUBBLOCKS (current_block) = 0;\n   BLOCK_CHAIN (current_block) = 0;\n \n-  for (insn = insns; insn; insn = NEXT_INSN (insn))\n-    if (GET_CODE (insn) == NOTE)\n-      {\n-\tif (NOTE_LINE_NUMBER (insn) == NOTE_INSN_BLOCK_BEG)\n-\t  {\n-\t    tree block = NOTE_BLOCK (insn);\n-\t    /* If we have seen this block before, copy it.  */\n-\t    if (TREE_ASM_WRITTEN (block))\n-\t      {\n-\t\tblock = copy_node (block);\n-\t\tNOTE_BLOCK (insn) = block;\n-\t      }\n-\t    BLOCK_SUBBLOCKS (block) = 0;\n-\t    TREE_ASM_WRITTEN (block) = 1;\n-\t    BLOCK_SUPERCONTEXT (block) = current_block; \n-\t    BLOCK_CHAIN (block) = BLOCK_SUBBLOCKS (current_block);\n-\t    BLOCK_SUBBLOCKS (current_block) = block;\n-\t    current_block = block;\n-\t    VARRAY_PUSH_TREE (block_stack, block);\n-\t  }\n-\telse if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_BLOCK_END)\n-\t  {\n-\t    NOTE_BLOCK (insn) = VARRAY_TOP_TREE (block_stack);\n-\t    VARRAY_POP (block_stack);\n-\t    BLOCK_SUBBLOCKS (current_block)\n-\t      = blocks_nreverse (BLOCK_SUBBLOCKS (current_block));\n-\t    current_block = BLOCK_SUPERCONTEXT (current_block);\n-\t  }\n-      }\n+  reorder_blocks_1 (insns, current_block, &block_stack);\n \n   BLOCK_SUBBLOCKS (current_block)\n     = blocks_nreverse (BLOCK_SUBBLOCKS (current_block));\n@@ -5600,6 +5648,60 @@ reorder_blocks (block, insns)\n   return current_block;\n }\n \n+/* Helper function for reorder_blocks.  Process the insn chain beginning\n+   at INSNS.  Recurse for CALL_PLACEHOLDER insns.  */\n+\n+static void\n+reorder_blocks_1 (insns, current_block, p_block_stack)\n+     rtx insns;\n+     tree current_block;\n+     varray_type *p_block_stack;\n+{\n+  rtx insn;\n+\n+  for (insn = insns; insn; insn = NEXT_INSN (insn))\n+    {\n+      if (GET_CODE (insn) == NOTE)\n+\t{\n+\t  if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_BLOCK_BEG)\n+\t    {\n+\t      tree block = NOTE_BLOCK (insn);\n+\t      /* If we have seen this block before, copy it.  */\n+\t      if (TREE_ASM_WRITTEN (block))\n+\t\t{\n+\t\t  block = copy_node (block);\n+\t\t  NOTE_BLOCK (insn) = block;\n+\t\t}\n+\t      BLOCK_SUBBLOCKS (block) = 0;\n+\t      TREE_ASM_WRITTEN (block) = 1;\n+\t      BLOCK_SUPERCONTEXT (block) = current_block; \n+\t      BLOCK_CHAIN (block) = BLOCK_SUBBLOCKS (current_block);\n+\t      BLOCK_SUBBLOCKS (current_block) = block;\n+\t      current_block = block;\n+\t      VARRAY_PUSH_TREE (*p_block_stack, block);\n+\t    }\n+\t  else if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_BLOCK_END)\n+\t    {\n+\t      NOTE_BLOCK (insn) = VARRAY_TOP_TREE (*p_block_stack);\n+\t      VARRAY_POP (*p_block_stack);\n+\t      BLOCK_SUBBLOCKS (current_block)\n+\t\t= blocks_nreverse (BLOCK_SUBBLOCKS (current_block));\n+\t      current_block = BLOCK_SUPERCONTEXT (current_block);\n+\t    }\n+\t}\n+      else if (GET_CODE (insn) == CALL_INSN\n+\t       && GET_CODE (PATTERN (insn)) == CALL_PLACEHOLDER)\n+\t{\n+\t  rtx cp = PATTERN (insn);\n+\t  reorder_blocks_1 (XEXP (cp, 0), current_block, p_block_stack);\n+\t  if (XEXP (cp, 1))\n+\t    reorder_blocks_1 (XEXP (cp, 1), current_block, p_block_stack);\n+\t  if (XEXP (cp, 2))\n+\t    reorder_blocks_1 (XEXP (cp, 2), current_block, p_block_stack);\n+\t}\n+    }\n+}\n+\n /* Reverse the order of elements in the chain T of blocks,\n    and return the new head of the chain (old last element).  */\n \n@@ -5757,6 +5859,7 @@ prepare_function_start ()\n   cfun->preferred_stack_boundary = STACK_BOUNDARY;\n #else\n   cfun->stack_alignment_needed = 0;\n+  cfun->preferred_stack_boundary = 0;\n #endif\n \n   /* Set if a call to setjmp is seen.  */\n@@ -5900,8 +6003,11 @@ void\n init_function_for_compilation ()\n {\n   reg_renumber = 0;\n+\n   /* No prologue/epilogue insns yet.  */\n-  prologue = epilogue = 0;\n+  VARRAY_GROW (prologue, 0);\n+  VARRAY_GROW (epilogue, 0);\n+  VARRAY_GROW (sibcall_epilogue, 0);\n }\n \n /* Indicate that the current function uses extra args\n@@ -6586,38 +6692,40 @@ expand_function_end (filename, line, end_bindings)\n   expand_fixups (get_insns ());\n }\n \f\n-/* Create an array that records the INSN_UIDs of INSNS (either a sequence\n-   or a single insn).  */\n+/* Extend a vector that records the INSN_UIDs of INSNS (either a\n+   sequence or a single insn).  */\n \n-static int *\n-record_insns (insns)\n+static void\n+record_insns (insns, vecp)\n      rtx insns;\n+     varray_type *vecp;\n {\n-  int *vec;\n-\n   if (GET_CODE (insns) == SEQUENCE)\n     {\n       int len = XVECLEN (insns, 0);\n-      vec = (int *) oballoc ((len + 1) * sizeof (int));\n-      vec[len] = 0;\n+      int i = VARRAY_SIZE (*vecp);\n+\n+      VARRAY_GROW (*vecp, i + len);\n       while (--len >= 0)\n-\tvec[len] = INSN_UID (XVECEXP (insns, 0, len));\n+\t{\n+\t  VARRAY_INT (*vecp, i) = INSN_UID (XVECEXP (insns, 0, len));\n+\t  ++i;\n+\t}\n     }\n   else\n     {\n-      vec = (int *) oballoc (2 * sizeof (int));\n-      vec[0] = INSN_UID (insns);\n-      vec[1] = 0;\n+      int i = VARRAY_SIZE (*vecp);\n+      VARRAY_GROW (*vecp, i + 1);\n+      VARRAY_INT (*vecp, i) = INSN_UID (insns);\n     }\n-  return vec;\n }\n \n /* Determine how many INSN_UIDs in VEC are part of INSN.  */\n \n static int\n contains (insn, vec)\n      rtx insn;\n-     int *vec;\n+     varray_type vec;\n {\n   register int i, j;\n \n@@ -6626,15 +6734,15 @@ contains (insn, vec)\n     {\n       int count = 0;\n       for (i = XVECLEN (PATTERN (insn), 0) - 1; i >= 0; i--)\n-\tfor (j = 0; vec[j]; j++)\n-\t  if (INSN_UID (XVECEXP (PATTERN (insn), 0, i)) == vec[j])\n+\tfor (j = VARRAY_SIZE (vec) - 1; j >= 0; --j)\n+\t  if (INSN_UID (XVECEXP (PATTERN (insn), 0, i)) == VARRAY_INT (vec, j))\n \t    count++;\n       return count;\n     }\n   else\n     {\n-      for (j = 0; vec[j]; j++)\n-\tif (INSN_UID (insn) == vec[j])\n+      for (j = VARRAY_SIZE (vec) - 1; j >= 0; --j)\n+\tif (INSN_UID (insn) == VARRAY_INT (vec, j))\n \t  return 1;\n     }\n   return 0;\n@@ -6644,13 +6752,22 @@ int\n prologue_epilogue_contains (insn)\n      rtx insn;\n {\n-  if (prologue && contains (insn, prologue))\n+  if (contains (insn, prologue))\n     return 1;\n-  if (epilogue && contains (insn, epilogue))\n+  if (contains (insn, epilogue))\n     return 1;\n   return 0;\n }\n \n+int\n+sibcall_epilogue_contains (insn)\n+      rtx insn;\n+{\n+  if (sibcall_epilogue)\n+    return contains (insn, sibcall_epilogue);\n+  return 0;\n+}\n+\n #ifdef HAVE_return\n /* Insert gen_return at the end of block BB.  This also means updating\n    block_for_insn appropriately.  */\n@@ -6698,7 +6815,7 @@ thread_prologue_and_epilogue_insns (f)\n       /* Retain a map of the prologue insns.  */\n       if (GET_CODE (seq) != SEQUENCE)\n \tseq = get_insns ();\n-      prologue = record_insns (seq);\n+      record_insns (seq, &prologue);\n       emit_note (NULL, NOTE_INSN_PROLOGUE_END);\n \n       /* GDB handles `break f' by setting a breakpoint on the first\n@@ -6875,7 +6992,7 @@ thread_prologue_and_epilogue_insns (f)\n       /* Retain a map of the epilogue insns.  */\n       if (GET_CODE (seq) != SEQUENCE)\n \tseq = get_insns ();\n-      epilogue = record_insns (seq);\n+      record_insns (seq, &epilogue);\n \n       seq = gen_sequence ();\n       end_sequence();\n@@ -6888,6 +7005,35 @@ thread_prologue_and_epilogue_insns (f)\n \n   if (insertted)\n     commit_edge_insertions ();\n+\n+#ifdef HAVE_sibcall_epilogue\n+  /* Emit sibling epilogues before any sibling call sites.  */\n+  for (e = EXIT_BLOCK_PTR->pred; e ; e = e->pred_next)\n+    {\n+      basic_block bb = e->src;\n+      rtx insn = bb->end;\n+      rtx i;\n+\n+      if (GET_CODE (insn) != CALL_INSN\n+\t  || ! SIBLING_CALL_P (insn))\n+\tcontinue;\n+\n+      start_sequence ();\n+      seq = gen_sibcall_epilogue ();\n+      end_sequence ();\n+\n+      i = PREV_INSN (insn);\n+      emit_insn_before (seq, insn);\n+\n+      /* Update the UID to basic block map.  */\n+      for (i = NEXT_INSN (i); i != insn; i = NEXT_INSN (i))\n+\tset_block_for_insn (i, bb);\n+\n+      /* Retain a map of the epilogue insns.  Used in life analysis to\n+\t avoid getting rid of sibcall epilogue insns.  */\n+      record_insns (seq, &sibcall_epilogue);\n+    }\n+#endif\n }\n \n /* Reposition the prologue-end and epilogue-begin notes after instruction\n@@ -6898,90 +7044,82 @@ reposition_prologue_and_epilogue_notes (f)\n      rtx f ATTRIBUTE_UNUSED;\n {\n #if defined (HAVE_prologue) || defined (HAVE_epilogue)\n-  /* Reposition the prologue and epilogue notes.  */\n-  if (n_basic_blocks)\n+  int len;\n+\n+  if ((len = VARRAY_SIZE (prologue)) > 0)\n     {\n-      int len;\n+      register rtx insn, note = 0;\n \n-      if (prologue)\n+      /* Scan from the beginning until we reach the last prologue insn.\n+\t We apparently can't depend on basic_block_{head,end} after\n+\t reorg has run.  */\n+      for (insn = f; len && insn; insn = NEXT_INSN (insn))\n \t{\n-\t  register rtx insn, note = 0;\n-\n-\t  /* Scan from the beginning until we reach the last prologue insn.\n-\t     We apparently can't depend on basic_block_{head,end} after\n-\t     reorg has run.  */\n-\t  for (len = 0; prologue[len]; len++)\n-\t    ;\n-\t  for (insn = f; len && insn; insn = NEXT_INSN (insn))\n+\t  if (GET_CODE (insn) == NOTE)\n \t    {\n-\t      if (GET_CODE (insn) == NOTE)\n+\t      if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_PROLOGUE_END)\n+\t\tnote = insn;\n+\t    }\n+\t  else if ((len -= contains (insn, prologue)) == 0)\n+\t    {\n+\t      rtx next;\n+\t      /* Find the prologue-end note if we haven't already, and\n+\t\t move it to just after the last prologue insn.  */\n+\t      if (note == 0)\n \t\t{\n-\t\t  if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_PROLOGUE_END)\n-\t\t    note = insn;\n+\t\t  for (note = insn; (note = NEXT_INSN (note));)\n+\t\t    if (GET_CODE (note) == NOTE\n+\t\t\t&& NOTE_LINE_NUMBER (note) == NOTE_INSN_PROLOGUE_END)\n+\t\t      break;\n \t\t}\n-\t      else if ((len -= contains (insn, prologue)) == 0)\n-\t\t{\n-\t\t  rtx next;\n-\t\t  /* Find the prologue-end note if we haven't already, and\n-\t\t     move it to just after the last prologue insn.  */\n-\t\t  if (note == 0)\n-\t\t    {\n-\t\t      for (note = insn; (note = NEXT_INSN (note));)\n-\t\t\tif (GET_CODE (note) == NOTE\n-\t\t\t    && NOTE_LINE_NUMBER (note) == NOTE_INSN_PROLOGUE_END)\n-\t\t\t  break;\n-\t\t    }\n \n-\t\t  next = NEXT_INSN (note);\n+\t      next = NEXT_INSN (note);\n \n-\t\t  /* Whether or not we can depend on BLOCK_HEAD, \n-\t\t     attempt to keep it up-to-date.  */\n-\t\t  if (BLOCK_HEAD (0) == note)\n-\t\t    BLOCK_HEAD (0) = next;\n+\t      /* Whether or not we can depend on BLOCK_HEAD, \n+\t\t attempt to keep it up-to-date.  */\n+\t      if (BLOCK_HEAD (0) == note)\n+\t\tBLOCK_HEAD (0) = next;\n \n-\t\t  remove_insn (note);\n-\t\t  add_insn_after (note, insn);\n-\t\t}\n+\t      remove_insn (note);\n+\t      add_insn_after (note, insn);\n \t    }\n \t}\n+    }\n+\n+  if ((len = VARRAY_SIZE (epilogue)) > 0)\n+    {\n+      register rtx insn, note = 0;\n \n-      if (epilogue)\n+      /* Scan from the end until we reach the first epilogue insn.\n+\t We apparently can't depend on basic_block_{head,end} after\n+\t reorg has run.  */\n+      for (insn = get_last_insn (); len && insn; insn = PREV_INSN (insn))\n \t{\n-\t  register rtx insn, note = 0;\n-\n-\t  /* Scan from the end until we reach the first epilogue insn.\n-\t     We apparently can't depend on basic_block_{head,end} after\n-\t     reorg has run.  */\n-\t  for (len = 0; epilogue[len]; len++)\n-\t    ;\n-\t  for (insn = get_last_insn (); len && insn; insn = PREV_INSN (insn))\n+\t  if (GET_CODE (insn) == NOTE)\n \t    {\n-\t      if (GET_CODE (insn) == NOTE)\n+\t      if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_EPILOGUE_BEG)\n+\t\tnote = insn;\n+\t    }\n+\t  else if ((len -= contains (insn, epilogue)) == 0)\n+\t    {\n+\t      /* Find the epilogue-begin note if we haven't already, and\n+\t\t move it to just before the first epilogue insn.  */\n+\t      if (note == 0)\n \t\t{\n-\t\t  if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_EPILOGUE_BEG)\n-\t\t    note = insn;\n+\t\t  for (note = insn; (note = PREV_INSN (note));)\n+\t\t    if (GET_CODE (note) == NOTE\n+\t\t\t&& NOTE_LINE_NUMBER (note) == NOTE_INSN_EPILOGUE_BEG)\n+\t\t      break;\n \t\t}\n-\t      else if ((len -= contains (insn, epilogue)) == 0)\n-\t\t{\n-\t\t  /* Find the epilogue-begin note if we haven't already, and\n-\t\t     move it to just before the first epilogue insn.  */\n-\t\t  if (note == 0)\n-\t\t    {\n-\t\t      for (note = insn; (note = PREV_INSN (note));)\n-\t\t\tif (GET_CODE (note) == NOTE\n-\t\t\t    && NOTE_LINE_NUMBER (note) == NOTE_INSN_EPILOGUE_BEG)\n-\t\t\t  break;\n-\t\t    }\n \n-\t\t  /* Whether or not we can depend on BLOCK_HEAD, \n-\t\t     attempt to keep it up-to-date.  */\n-\t\t  if (n_basic_blocks\n-\t\t      && BLOCK_HEAD (n_basic_blocks-1) == insn)\n-\t\t    BLOCK_HEAD (n_basic_blocks-1) = note;\n+\t      /* Whether or not we can depend on BLOCK_HEAD, \n+\t\t attempt to keep it up-to-date.  */\n+\t      if (n_basic_blocks\n+\t\t  && BLOCK_HEAD (n_basic_blocks-1) == insn)\n+\t\tBLOCK_HEAD (n_basic_blocks-1) = note;\n \n-\t\t  remove_insn (note);\n-\t\t  add_insn_before (note, insn);\n-\t\t}\n+\t      remove_insn (note);\n+\t      add_insn_before (note, insn);\n \t    }\n \t}\n     }\n@@ -7095,4 +7233,8 @@ init_function_once ()\n {\n   ggc_add_root (&all_functions, 1, sizeof all_functions,\n \t\tmark_function_chain);\n+\n+  VARRAY_INT_INIT (prologue, 0, \"prologue\");\n+  VARRAY_INT_INIT (epilogue, 0, \"epilogue\");\n+  VARRAY_INT_INIT (sibcall_epilogue, 0, \"sibcall_epilogue\");\n }"}, {"sha": "39b5354e7dc280c259e9e376c128c982a7d462d8", "filename": "gcc/genflags.c", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Fgenflags.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Fgenflags.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgenflags.c?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -174,11 +174,15 @@ gen_insn (insn)\n      call_value_pop) ignoring the extra arguments that are passed for\n      some machines, so by default, turn off the prototype.  */\n \n-  obstack_ptr = (name[0] == 'c'\n+  obstack_ptr = ((name[0] == 'c' || name[0] == 's')\n \t\t && (!strcmp (name, \"call\")\n \t\t     || !strcmp (name, \"call_value\")\n \t\t     || !strcmp (name, \"call_pop\")\n-\t\t     || !strcmp (name, \"call_value_pop\")))\n+\t\t     || !strcmp (name, \"call_value_pop\")\n+\t\t     || !strcmp (name, \"sibcall\")\n+\t\t     || !strcmp (name, \"sibcall_value\")\n+\t\t     || !strcmp (name, \"sibcall_pop\")\n+\t\t     || !strcmp (name, \"sibcall_value_pop\")))\n     ? &call_obstack : &normal_obstack;\n \n   obstack_grow (obstack_ptr, &insn, sizeof (rtx));"}, {"sha": "e0374e1d9ff2cb655e910d82a008ea23532043ab", "filename": "gcc/integrate.c", "status": "modified", "additions": 206, "deletions": 88, "changes": 294, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Fintegrate.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Fintegrate.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fintegrate.c?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -66,19 +66,22 @@ extern struct obstack *function_maybepermanent_obstack;\n static rtvec initialize_for_inline\tPARAMS ((tree));\n static void note_modified_parmregs\tPARAMS ((rtx, rtx, void *));\n static void integrate_parm_decls\tPARAMS ((tree, struct inline_remap *,\n-\t\t\t\t\t       rtvec));\n+\t\t\t\t\t\t rtvec));\n static tree integrate_decl_tree\t\tPARAMS ((tree,\n-\t\t\t\t\t       struct inline_remap *));\n+\t\t\t\t\t\t struct inline_remap *));\n static void subst_constants\t\tPARAMS ((rtx *, rtx,\n-\t\t\t\t\t       struct inline_remap *, int));\n+\t\t\t\t\t\t struct inline_remap *, int));\n static void set_block_origin_self\tPARAMS ((tree));\n static void set_decl_origin_self\tPARAMS ((tree));\n static void set_block_abstract_flags\tPARAMS ((tree, int));\n static void process_reg_param\t\tPARAMS ((struct inline_remap *, rtx,\n-\t\t\t\t\t       rtx));\n+\t\t\t\t\t\t rtx));\n void set_decl_abstract_flags\t\tPARAMS ((tree, int));\n static rtx expand_inline_function_eh_labelmap PARAMS ((rtx));\n static void mark_stores                 PARAMS ((rtx, rtx, void *));\n+static void save_parm_insns\t\tPARAMS ((rtx, rtx));\n+static void copy_insn_list              PARAMS ((rtx, struct inline_remap *,\n+\t\t\t\t\t\t rtx));\n static int compare_blocks               PARAMS ((const PTR, const PTR));\n static int find_block                   PARAMS ((const PTR, const PTR));\n \n@@ -423,16 +426,7 @@ save_for_inline_nocopy (fndecl)\n      perform constant folding when its incoming value is constant).\n      Otherwise, we have to copy its value into a new register and track\n      the new register's life.  */\n-  in_nonparm_insns = 0;\n-  for (insn = NEXT_INSN (insn); insn; insn = NEXT_INSN (insn))\n-    {\n-      if (insn == first_nonparm_insn)\n-\tin_nonparm_insns = 1;\n-\n-      if (GET_RTX_CLASS (GET_CODE (insn)) == 'i')\n-\t/* Record what interesting things happen to our parameters.  */\n-\tnote_stores (PATTERN (insn), note_modified_parmregs, NULL);\n-    }\n+  save_parm_insns (insn, first_nonparm_insn);\n \n   /* We have now allocated all that needs to be allocated permanently\n      on the rtx obstack.  Set our high-water mark, so that we\n@@ -449,6 +443,48 @@ save_for_inline_nocopy (fndecl)\n   /* Clean up.  */\n   free (parmdecl_map);\n }\n+\n+/* Scan the chain of insns to see what happens to our PARM_DECLs.  If a\n+   PARM_DECL is used but never modified, we can substitute its rtl directly\n+   when expanding inline (and perform constant folding when its incoming\n+   value is constant). Otherwise, we have to copy its value into a new\n+   register and track the new register's life.  */\n+\n+static void\n+save_parm_insns (insn, first_nonparm_insn)\n+    rtx insn;\n+    rtx first_nonparm_insn;\n+{\n+  in_nonparm_insns = 0;\n+\n+  if (insn == NULL_RTX)\n+    return;\n+\n+  for (insn = NEXT_INSN (insn); insn; insn = NEXT_INSN (insn))\n+    {\n+      if (insn == first_nonparm_insn)\n+\tin_nonparm_insns = 1;\n+\n+      if (GET_RTX_CLASS (GET_CODE (insn)) == 'i')\n+\t{\n+\t  /* Record what interesting things happen to our parameters.  */\n+\t  note_stores (PATTERN (insn), note_modified_parmregs, NULL);\n+\n+\t  /* If this is a CALL_PLACEHOLDER insn then we need to look into the\n+\t     three attached sequences: normal call, sibling call and tail\n+\t     recursion. */\n+\t  if (GET_CODE (insn) == CALL_INSN\n+\t      && GET_CODE (PATTERN (insn)) == CALL_PLACEHOLDER)\n+\t    {\n+\t      int i;\n+\n+\t      for (i = 0; i < 3; i++)\n+\t\tsave_parm_insns (XEXP (PATTERN (insn), i),\n+\t\t\t\t first_nonparm_insn);\n+\t    }\n+\t}\n+    }\n+}\n \f\n /* Note whether a parameter is modified or not.  */\n \n@@ -577,13 +613,11 @@ expand_inline_function (fndecl, parms, target, ignore, type,\n \t       : parm_insns);\n   tree *arg_trees;\n   rtx *arg_vals;\n-  rtx insn;\n   int max_regno;\n   register int i;\n   int min_labelno = inl_f->emit->x_first_label_num;\n   int max_labelno = inl_f->inl_max_label_num;\n   int nargs;\n-  rtx local_return_label = 0;\n   rtx loc;\n   rtx stack_save = 0;\n   rtx temp;\n@@ -1089,7 +1123,100 @@ expand_inline_function (fndecl, parms, target, ignore, type,\n   if (inl_f->calls_alloca)\n     emit_stack_save (SAVE_BLOCK, &stack_save, NULL_RTX);\n \n-  /* Now copy the insns one by one.  Do this in two passes, first the insns and\n+  /* Now copy the insns one by one.  */\n+  copy_insn_list (insns, map, static_chain_value);\n+\n+  /* Restore the stack pointer if we saved it above.  */\n+  if (inl_f->calls_alloca)\n+    emit_stack_restore (SAVE_BLOCK, stack_save, NULL_RTX);\n+\n+  if (! cfun->x_whole_function_mode_p)\n+    /* In statement-at-a-time mode, we just tell the front-end to add\n+       this block to the list of blocks at this binding level.  We\n+       can't do it the way it's done for function-at-a-time mode the\n+       superblocks have not been created yet.  */\n+    insert_block (block);\n+  else\n+    {\n+      BLOCK_CHAIN (block) \n+\t= BLOCK_CHAIN (DECL_INITIAL (current_function_decl));\n+      BLOCK_CHAIN (DECL_INITIAL (current_function_decl)) = block;\n+    }\n+\n+  /* End the scope containing the copied formal parameter variables\n+     and copied LABEL_DECLs.  We pass NULL_TREE for the variables list\n+     here so that expand_end_bindings will not check for unused\n+     variables.  That's already been checked for when the inlined\n+     function was defined.  */\n+  expand_end_bindings (NULL_TREE, 1, 1);\n+\n+  /* Must mark the line number note after inlined functions as a repeat, so\n+     that the test coverage code can avoid counting the call twice.  This\n+     just tells the code to ignore the immediately following line note, since\n+     there already exists a copy of this note before the expanded inline call.\n+     This line number note is still needed for debugging though, so we can't\n+     delete it.  */\n+  if (flag_test_coverage)\n+    emit_note (0, NOTE_REPEATED_LINE_NUMBER);\n+\n+  emit_line_note (input_filename, lineno);\n+\n+  /* If the function returns a BLKmode object in a register, copy it\n+     out of the temp register into a BLKmode memory object. */\n+  if (target \n+      && TYPE_MODE (TREE_TYPE (TREE_TYPE (fndecl))) == BLKmode\n+      && ! aggregate_value_p (TREE_TYPE (TREE_TYPE (fndecl))))\n+    target = copy_blkmode_from_reg (0, target, TREE_TYPE (TREE_TYPE (fndecl)));\n+  \n+  if (structure_value_addr)\n+    {\n+      target = gen_rtx_MEM (TYPE_MODE (type),\n+\t\t\t    memory_address (TYPE_MODE (type),\n+\t\t\t\t\t    structure_value_addr));\n+      MEM_SET_IN_STRUCT_P (target, 1);\n+    }\n+\n+  /* Make sure we free the things we explicitly allocated with xmalloc.  */\n+  if (real_label_map)\n+    free (real_label_map);\n+  VARRAY_FREE (map->const_equiv_varray);\n+  free (map->reg_map);\n+  VARRAY_FREE (map->block_map);\n+  free (map->insn_map);\n+  free (map);\n+  free (arg_vals);\n+  free (arg_trees);\n+\n+  inlining = inlining_previous;\n+\n+  return target;\n+}\n+\n+/* Make copies of each insn in the given list using the mapping\n+   computed in expand_inline_function. This function may call itself for\n+   insns containing sequences.\n+   \n+   Copying is done in two passes, first the insns and then their REG_NOTES,\n+   just like save_for_inline.\n+\n+   If static_chain_value is non-zero, it represents the context-pointer\n+   register for the function. */\n+\n+static void\n+copy_insn_list (insns, map, static_chain_value)\n+    rtx insns;\n+    struct inline_remap *map;\n+    rtx static_chain_value;\n+{\n+  register int i;\n+  rtx insn;\n+  rtx temp;\n+  rtx local_return_label = NULL_RTX;\n+#ifdef HAVE_cc0\n+  rtx cc0_insn = 0;\n+#endif\n+\n+  /* Copy the insns one by one.  Do this in two passes, first the insns and\n      then their REG_NOTES, just like save_for_inline.  */\n \n   /* This loop is very similar to the loop in copy_loop_body in unroll.c.  */\n@@ -1283,11 +1410,50 @@ expand_inline_function (fndecl, parms, target, ignore, type,\n \t  break;\n \n \tcase CALL_INSN:\n+\t  /* If this is a CALL_PLACEHOLDER insn then we need to copy the\n+\t     three attached sequences: normal call, sibling call and tail\n+\t     recursion. */\n+\t  if (GET_CODE (PATTERN (insn)) == CALL_PLACEHOLDER)\n+\t    {\n+\t      rtx sequence[3];\n+\t      rtx tail_label;\n+\n+\t      for (i = 0; i < 3; i++)\n+\t\t{\n+\t\t  rtx seq;\n+\t\t  \n+\t\t  sequence[i] = NULL_RTX;\n+\t\t  seq = XEXP (PATTERN (insn), i);\n+\t\t  if (seq)\n+\t\t    {\n+\t\t      start_sequence ();\n+\t\t      copy_insn_list (seq, map, static_chain_value);\n+\t\t      sequence[i] = get_insns ();\n+\t\t      end_sequence ();\n+\t\t    }\n+\t\t}\n+\n+\t      /* Find the new tail recursion label.  \n+\t         It will already be substituted into sequence[2].  */\n+\t      tail_label = copy_rtx_and_substitute (XEXP (PATTERN (insn), 3),\n+\t\t\t\t\t\t    map, 0);\n+\n+\t      copy = emit_call_insn (gen_rtx_CALL_PLACEHOLDER (VOIDmode, \n+\t\t\t\t\t\t\tsequence[0],\n+\t\t\t\t\t\t\tsequence[1],\n+\t\t\t\t\t\t\tsequence[2],\n+\t\t\t\t\t\t\ttail_label));\n+\t      break;\n+\t    }\n+\n \t  pattern = copy_rtx_and_substitute (PATTERN (insn), map, 0);\n \t  copy = emit_call_insn (pattern);\n \n+\t  SIBLING_CALL_P (copy) = SIBLING_CALL_P (insn);\n+\n \t  /* Because the USAGE information potentially contains objects other\n \t     than hard registers, we need to copy it.  */\n+\n \t  CALL_INSN_FUNCTION_USAGE (copy)\n \t    = copy_rtx_and_substitute (CALL_INSN_FUNCTION_USAGE (insn),\n \t\t\t\t       map, 0);\n@@ -1299,7 +1465,7 @@ expand_inline_function (fndecl, parms, target, ignore, type,\n #endif\n \t  try_constants (copy, map);\n \n-\t  /* Be lazy and assume CALL_INSNs clobber all hard registers.  */\n+\t      /* Be lazy and assume CALL_INSNs clobber all hard registers.  */\n \t  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n \t    VARRAY_CONST_EQUIV (map->const_equiv_varray, i).rtx = 0;\n \t  break;\n@@ -1316,14 +1482,23 @@ expand_inline_function (fndecl, parms, target, ignore, type,\n \t  break;\n \n \tcase NOTE:\n-\t  /* It is important to discard function-end and function-beg notes,\n-\t     so we have only one of each in the current function.\n-\t     Also, NOTE_INSN_DELETED notes aren't useful (save_for_inline\n+\t  /* NOTE_INSN_FUNCTION_END and NOTE_INSN_FUNCTION_BEG are \n+\t     discarded because it is important to have only one of \n+\t     each in the current function.\n+\n+\t     NOTE_INSN_DELETED notes aren't useful (save_for_inline\n \t     deleted these in the copy used for continuing compilation,\n-\t     not the copy used for inlining).  */\n+\t     not the copy used for inlining).\n+\n+\t     NOTE_INSN_BASIC_BLOCK is discarded because the saved bb\n+\t     pointer (which will soon be dangling) confuses flow's\n+\t     attempts to preserve bb structures during the compilation\n+\t     of a function.  */\n+\n \t  if (NOTE_LINE_NUMBER (insn) != NOTE_INSN_FUNCTION_END\n \t      && NOTE_LINE_NUMBER (insn) != NOTE_INSN_FUNCTION_BEG\n-\t      && NOTE_LINE_NUMBER (insn) != NOTE_INSN_DELETED)\n+\t      && NOTE_LINE_NUMBER (insn) != NOTE_INSN_DELETED\n+\t      && NOTE_LINE_NUMBER (insn) != NOTE_INSN_BASIC_BLOCK)\n \t    {\n \t      copy = emit_note (NOTE_SOURCE_FILE (insn),\n \t\t\t\tNOTE_LINE_NUMBER (insn));\n@@ -1403,71 +1578,6 @@ expand_inline_function (fndecl, parms, target, ignore, type,\n \n   if (local_return_label)\n     emit_label (local_return_label);\n-\n-  /* Restore the stack pointer if we saved it above.  */\n-  if (inl_f->calls_alloca)\n-    emit_stack_restore (SAVE_BLOCK, stack_save, NULL_RTX);\n-\n-  if (! cfun->x_whole_function_mode_p)\n-    /* In statement-at-a-time mode, we just tell the front-end to add\n-       this block to the list of blocks at this binding level.  We\n-       can't do it the way it's done for function-at-a-time mode the\n-       superblocks have not been created yet.  */\n-    insert_block (block);\n-  else\n-    {\n-      BLOCK_CHAIN (block) \n-\t= BLOCK_CHAIN (DECL_INITIAL (current_function_decl));\n-      BLOCK_CHAIN (DECL_INITIAL (current_function_decl)) = block;\n-    }\n-\n-  /* End the scope containing the copied formal parameter variables\n-     and copied LABEL_DECLs.  We pass NULL_TREE for the variables list\n-     here so that expand_end_bindings will not check for unused\n-     variables.  That's already been checked for when the inlined\n-     function was defined.  */\n-  expand_end_bindings (NULL_TREE, 1, 1);\n-\n-  /* Must mark the line number note after inlined functions as a repeat, so\n-     that the test coverage code can avoid counting the call twice.  This\n-     just tells the code to ignore the immediately following line note, since\n-     there already exists a copy of this note before the expanded inline call.\n-     This line number note is still needed for debugging though, so we can't\n-     delete it.  */\n-  if (flag_test_coverage)\n-    emit_note (0, NOTE_REPEATED_LINE_NUMBER);\n-\n-  emit_line_note (input_filename, lineno);\n-\n-  /* If the function returns a BLKmode object in a register, copy it\n-     out of the temp register into a BLKmode memory object. */\n-  if (target \n-      && TYPE_MODE (TREE_TYPE (TREE_TYPE (fndecl))) == BLKmode\n-      && ! aggregate_value_p (TREE_TYPE (TREE_TYPE (fndecl))))\n-    target = copy_blkmode_from_reg (0, target, TREE_TYPE (TREE_TYPE (fndecl)));\n-  \n-  if (structure_value_addr)\n-    {\n-      target = gen_rtx_MEM (TYPE_MODE (type),\n-\t\t\t    memory_address (TYPE_MODE (type),\n-\t\t\t\t\t    structure_value_addr));\n-      MEM_SET_IN_STRUCT_P (target, 1);\n-    }\n-\n-  /* Make sure we free the things we explicitly allocated with xmalloc.  */\n-  if (real_label_map)\n-    free (real_label_map);\n-  VARRAY_FREE (map->const_equiv_varray);\n-  free (map->reg_map);\n-  VARRAY_FREE (map->block_map);\n-  free (map->insn_map);\n-  free (map);\n-  free (arg_vals);\n-  free (arg_trees);\n-\n-  inlining = inlining_previous;\n-\n-  return target;\n }\n \f\n /* Given a chain of PARM_DECLs, ARGS, copy each decl into a VAR_DECL,\n@@ -1790,6 +1900,13 @@ copy_rtx_and_substitute (orig, map, for_lhs)\n \t= LABEL_PRESERVE_P (orig);\n       return get_label_from_map (map, CODE_LABEL_NUMBER (orig));\n \n+    /* We need to handle \"deleted\" labels that appear in the DECL_RTL\n+       of a LABEL_DECL.  */\n+    case NOTE:\n+      if (NOTE_LINE_NUMBER (orig) == NOTE_INSN_DELETED_LABEL)\n+\treturn map->insn_map[INSN_UID (orig)];\n+      break;\n+\n     case LABEL_REF:\n       copy\n \t= gen_rtx_LABEL_REF\n@@ -2348,6 +2465,7 @@ subst_constants (loc, insn, map, memonly)\n \tcase 'i':\n \tcase 's':\n \tcase 'w':\n+ \tcase 'n':\n \tcase 't':\n \t  break;\n "}, {"sha": "b5352f43542af3245e57c8d22316a8e8b379fa03", "filename": "gcc/jump.c", "status": "modified", "additions": 31, "deletions": 9, "changes": 40, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Fjump.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Fjump.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fjump.c?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -125,13 +125,13 @@ static void delete_from_jump_chain\tPARAMS ((rtx));\n static int delete_labelref_insn\t\tPARAMS ((rtx, rtx, int));\n static void mark_modified_reg\t\tPARAMS ((rtx, rtx, void *));\n static void redirect_tablejump\t\tPARAMS ((rtx, rtx));\n-static void jump_optimize_1\t\tPARAMS ((rtx, int, int, int, int));\n+static void jump_optimize_1\t\tPARAMS ((rtx, int, int, int, int, int));\n #if ! defined(HAVE_cc0) && ! defined(HAVE_conditional_arithmetic)\n static rtx find_insert_position         PARAMS ((rtx, rtx));\n #endif\n static int returnjump_p_1\t        PARAMS ((rtx *, void *));\n static void delete_prior_computation    PARAMS ((rtx, rtx));\n-\n+\f\n /* Main external entry point into the jump optimizer.  See comments before\n    jump_optimize_1 for descriptions of the arguments.  */\n void\n@@ -141,7 +141,7 @@ jump_optimize (f, cross_jump, noop_moves, after_regscan)\n      int noop_moves;\n      int after_regscan;\n {\n-  jump_optimize_1 (f, cross_jump, noop_moves, after_regscan, 0);\n+  jump_optimize_1 (f, cross_jump, noop_moves, after_regscan, 0, 0);\n }\n \n /* Alternate entry into the jump optimizer.  This entry point only rebuilds\n@@ -151,9 +151,16 @@ void\n rebuild_jump_labels (f)\n      rtx f;\n {\n-  jump_optimize_1 (f, 0, 0, 0, 1);\n+  jump_optimize_1 (f, 0, 0, 0, 1, 0);\n }\n \n+/* Alternate entry into the jump optimizer.  Do only trivial optimizations.  */\n+void\n+jump_optimize_minimal (f)\n+     rtx f;\n+{\n+  jump_optimize_1 (f, 0, 0, 0, 0, 1);\n+}\n \f\n /* Delete no-op jumps and optimize jumps to jumps\n    and jumps around jumps.\n@@ -175,15 +182,29 @@ rebuild_jump_labels (f)\n    just determine whether control drops off the end of the function.\n    This case occurs when we have -W and not -O.\n    It works because `delete_insn' checks the value of `optimize'\n-   and refrains from actually deleting when that is 0.  */\n+   and refrains from actually deleting when that is 0.\n+\n+   If MINIMAL is nonzero, then we only perform trivial optimizations:\n+\n+     * Removal of unreachable code after BARRIERs.\n+     * Removal of unreferenced CODE_LABELs.\n+     * Removal of a jump to the next instruction.\n+     * Removal of a conditional jump followed by an unconditional jump\n+       to the same target as the conditional jump.\n+     * Simplify a conditional jump around an unconditional jump.\n+     * Simplify a jump to a jump.\n+     * Delete extraneous line number notes.\n+  */\n \n static void\n-jump_optimize_1 (f, cross_jump, noop_moves, after_regscan, mark_labels_only)\n+jump_optimize_1 (f, cross_jump, noop_moves, after_regscan,\n+\t\t mark_labels_only, minimal)\n      rtx f;\n      int cross_jump;\n      int noop_moves;\n      int after_regscan;\n      int mark_labels_only;\n+     int minimal;\n {\n   register rtx insn, next;\n   int changed;\n@@ -230,7 +251,8 @@ jump_optimize_1 (f, cross_jump, noop_moves, after_regscan, mark_labels_only)\n   if (mark_labels_only)\n     goto end;\n \n-  exception_optimize ();\n+  if (! minimal)\n+    exception_optimize ();\n \n   last_insn = delete_unreferenced_labels (f);\n \n@@ -320,7 +342,7 @@ jump_optimize_1 (f, cross_jump, noop_moves, after_regscan, mark_labels_only)\n \t  if (nlabel != JUMP_LABEL (insn))\n \t    changed |= redirect_jump (insn, nlabel);\n \n-\t  if (! optimize)\n+\t  if (! optimize || ! minimal)\n \t    continue;\n \n \t  /* If a dispatch table always goes to the same place,\n@@ -2135,7 +2157,7 @@ jump_optimize_1 (f, cross_jump, noop_moves, after_regscan, mark_labels_only)\n      not be cleared.  This is especially true for the case where we\n      delete the NOTE_FUNCTION_END note.  CAN_REACH_END is cleared by\n      the front-end before compiling each function.  */\n-  if (calculate_can_reach_end (last_insn, optimize != 0))\n+  if (! minimal && calculate_can_reach_end (last_insn, optimize != 0))\n     can_reach_end = 1;\n \n end:"}, {"sha": "a8b1a9de63ac6e6fb140adfc64989b70600164a1", "filename": "gcc/rtl.c", "status": "modified", "additions": 5, "deletions": 7, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Frtl.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Frtl.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtl.c?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -406,14 +406,12 @@ copy_rtx (orig)\n      walks over the RTL.  */\n   copy->used = 0;\n \n-  /* We do not copy JUMP, CALL, or FRAME_RELATED for INSNs.  */\n+  /* We do not copy FRAME_RELATED for INSNs.  */\n   if (GET_RTX_CLASS (code) == 'i')\n-    {\n-      copy->jump = 0;\n-      copy->call = 0;\n-      copy->frame_related = 0;\n-    }\n-  \n+    copy->frame_related = 0;\n+  copy->jump = orig->jump;\n+  copy->call = orig->call;\n+\n   format_ptr = GET_RTX_FORMAT (GET_CODE (copy));\n \n   for (i = 0; i < GET_RTX_LENGTH (GET_CODE (copy)); i++)"}, {"sha": "570abdc1a971120f860c620659dd5c905e2744d9", "filename": "gcc/rtl.def", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Frtl.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Frtl.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtl.def?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -1,7 +1,7 @@\n /* This file contains the definitions and documentation for the\n    Register Transfer Expressions (rtx's) that make up the\n    Register Transfer Language (rtl) used in the Back End of the GNU compiler.\n-   Copyright (C) 1987, 88, 92, 94, 95, 97, 98, 1999\n+   Copyright (C) 1987, 88, 92, 94, 95, 97, 98, 1999, 2000\n    Free Software Foundation, Inc.\n \n This file is part of GNU CC.\n@@ -880,7 +880,10 @@ DEF_RTL_EXPR(CONSTANT_P_RTX, \"constant_p_rtx\", \"e\", 'x')\n    potential tail recursive calls were found.\n \n    The tail recursion label is needed so that we can clear LABEL_PRESERVE_P\n-   after we select a call method.  */\n+   after we select a call method.\n+\n+   This method of tail-call elimination is intended to be replaced by\n+   tree-based optimizations once front-end conversions are complete.  */\n DEF_RTL_EXPR(CALL_PLACEHOLDER, \"call_placeholder\", \"uuuu\", 'x')\n \n /* The SSA phi operator. "}, {"sha": "46750ac060ddea931712107bda5b677b5c9add76", "filename": "gcc/rtl.h", "status": "modified", "additions": 20, "deletions": 2, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Frtl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Frtl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtl.h?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -119,9 +119,12 @@ typedef struct rtx_def\n #else\n   enum machine_mode mode : 8;\n #endif\n-  /* LINK_COST_ZERO in an INSN_LIST.  */\n+  /* 1 in an INSN if it can alter flow of control\n+     within this function.\n+     LINK_COST_ZERO in an INSN_LIST.  */\n   unsigned int jump : 1;\n-  /* LINK_COST_FREE in an INSN_LIST.  */\n+  /* 1 in an INSN if it can call another function.\n+     LINK_COST_FREE in an INSN_LIST.  */\n   unsigned int call : 1;\n   /* 1 in a MEM or REG if value of this expression will never change\n      during the current function, even though it is not\n@@ -380,6 +383,9 @@ extern void rtvec_check_failed_bounds PARAMS ((rtvec, int,\n /* 1 if insn is a call to a const function.  */\n #define CONST_CALL_P(INSN) ((INSN)->unchanging)\n \n+/* 1 if insn (assumed to be a CALL_INSN) is a sibling call.  */\n+#define SIBLING_CALL_P(INSN) ((INSN)->jump)\n+\n /* 1 if insn is a branch that should not unconditionally execute its\n    delay slots, i.e., it is an annulled branch.   */\n #define INSN_ANNULLED_BRANCH_P(INSN) ((INSN)->unchanging)\n@@ -1416,6 +1422,7 @@ extern int rtx_renumbered_equal_p\tPARAMS ((rtx, rtx));\n extern int true_regnum\t\t\tPARAMS ((rtx));\n extern int redirect_jump\t\tPARAMS ((rtx, rtx));\n extern void jump_optimize\t\tPARAMS ((rtx, int, int, int));\n+extern void jump_optimize_minimal\tPARAMS ((rtx));\n extern void rebuild_jump_labels\t\tPARAMS ((rtx));\n extern void thread_jumps\t\tPARAMS ((rtx, int, int));\n extern int redirect_exp\t\t\tPARAMS ((rtx *, rtx, rtx, rtx));\n@@ -1513,6 +1520,7 @@ extern void record_excess_regs\t\tPARAMS ((rtx, rtx, rtx *));\n extern void reposition_prologue_and_epilogue_notes\tPARAMS ((rtx));\n extern void thread_prologue_and_epilogue_insns\t\tPARAMS ((rtx));\n extern int prologue_epilogue_contains\t\t\tPARAMS ((rtx));\n+extern int sibcall_epilogue_contains\t\t\tPARAMS ((rtx));\n extern HOST_WIDE_INT get_frame_size\t\t\tPARAMS ((void));\n extern void preserve_rtl_expr_result\t\t\tPARAMS ((rtx));\n extern void mark_temp_addr_taken\t\t\tPARAMS ((rtx));\n@@ -1713,6 +1721,16 @@ extern void record_base_value\t\tPARAMS ((int, rtx, int));\n extern void record_alias_subset         PARAMS ((int, int));\n extern rtx addr_side_effect_eval\tPARAMS ((rtx, int, int));\n \n+/* In sibcall.c */\n+typedef enum {\n+  sibcall_use_normal = 1,\n+  sibcall_use_tail_recursion,\n+  sibcall_use_sibcall\n+} sibcall_use_t;\n+\n+extern void optimize_sibling_and_tail_recursive_calls PARAMS ((void));\n+extern void replace_call_placeholder\tPARAMS ((rtx, sibcall_use_t));\n+\n #ifdef STACK_REGS\n extern int stack_regs_mentioned\t\tPARAMS ((rtx insn));\n #endif"}, {"sha": "b399137a0bf02b5faf27ce673de1530f2a6250a5", "filename": "gcc/sibcall.c", "status": "added", "additions": 578, "deletions": 0, "changes": 578, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Fsibcall.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Fsibcall.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsibcall.c?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -0,0 +1,578 @@\n+/* Generic sibling call optimization support\n+   Copyright (C) 1999, 2000 Free Software Foundation, Inc.\n+\n+This file is part of GNU CC.\n+\n+GNU CC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 2, or (at your option)\n+any later version.\n+\n+GNU CC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GNU CC; see the file COPYING.  If not, write to\n+the Free Software Foundation, 59 Temple Place - Suite 330,\n+Boston, MA 02111-1307, USA.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+\n+#include \"rtl.h\"\n+#include \"regs.h\"\n+#include \"function.h\"\n+#include \"hard-reg-set.h\"\n+#include \"flags.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"basic-block.h\"\n+#include \"output.h\"\n+#include \"except.h\"\n+\n+static int identify_call_return_value\tPARAMS ((rtx, rtx *, rtx *));\n+static rtx skip_copy_to_return_value\tPARAMS ((rtx, rtx, rtx));\n+static rtx skip_use_of_return_value\tPARAMS ((rtx, enum rtx_code));\n+static rtx skip_stack_adjustment\tPARAMS ((rtx));\n+static rtx skip_jump_insn\t\tPARAMS ((rtx));\n+static int uses_addressof\t\tPARAMS ((rtx));\n+static int sequence_uses_addressof\tPARAMS ((rtx));\n+static void purge_reg_equiv_notes\tPARAMS ((void));\n+\n+/* Examine a CALL_PLACEHOLDER pattern and determine where the call's\n+   return value is located.  P_HARD_RETURN receives the hard register\n+   that the function used; P_SOFT_RETURN receives the pseudo register\n+   that the sequence used.  Return non-zero if the values were located.  */\n+\n+static int\n+identify_call_return_value (cp, p_hard_return, p_soft_return)\n+     rtx cp;\n+     rtx *p_hard_return, *p_soft_return;\n+{\n+  rtx insn, set, hard, soft;\n+\n+  /* Search forward through the \"normal\" call sequence to the CALL insn.  */\n+  insn = XEXP (cp, 0);\n+  while (GET_CODE (insn) != CALL_INSN)\n+    insn = NEXT_INSN (insn);\n+\n+  /* Assume the pattern is (set (dest) (call ...)), or that the first\n+     member of a parallel is.  This is the hard return register used\n+     by the function.  */\n+  if (GET_CODE (PATTERN (insn)) == SET\n+      && GET_CODE (SET_SRC (PATTERN (insn))) == CALL)\n+    hard = SET_DEST (PATTERN (insn));\n+  else if (GET_CODE (PATTERN (insn)) == PARALLEL\n+\t   && GET_CODE (XVECEXP (PATTERN (insn), 0, 0)) == SET\n+\t   && GET_CODE (SET_SRC (XVECEXP (PATTERN (insn), 0, 0))) == CALL)\n+    hard = SET_DEST (XVECEXP (PATTERN (insn), 0, 0));\n+  else\n+    return 0;\n+\n+  /* If we didn't get a single hard register (e.g. a parallel), give up.  */\n+  if (GET_CODE (hard) != REG)\n+    return 0;\n+    \n+  /* If there's nothing after, there's no soft return value.  */\n+  insn = NEXT_INSN (insn);\n+  if (! insn)\n+    return 0;\n+  \n+  /* We're looking for a source of the hard return register.  */\n+  set = single_set (insn);\n+  if (! set || SET_SRC (set) != hard)\n+    return 0;\n+\n+  soft = SET_DEST (set);\n+  insn = NEXT_INSN (insn);\n+\n+  /* Allow this first destination to be copied to a second register,\n+     as might happen if the first register wasn't the particular pseudo\n+     we'd been expecting.  */\n+  if (insn\n+      && (set = single_set (insn)) != NULL_RTX\n+      && SET_SRC (set) == soft)\n+    {\n+      soft = SET_DEST (set);\n+      insn = NEXT_INSN (insn);\n+    }\n+\n+  /* Don't fool with anything but pseudo registers.  */\n+  if (GET_CODE (soft) != REG || REGNO (soft) < FIRST_PSEUDO_REGISTER)\n+    return 0;\n+\n+  /* This value must not be modified before the end of the sequence.  */\n+  if (reg_set_between_p (soft, insn, NULL_RTX))\n+    return 0;\n+\n+  *p_hard_return = hard;\n+  *p_soft_return = soft;\n+\n+  return 1;\n+}\n+\n+/* If the first real insn after ORIG_INSN copies to this function's\n+   return value from RETVAL, then return the insn which performs the\n+   copy.  Otherwise return ORIG_INSN.  */\n+\n+static rtx\n+skip_copy_to_return_value (orig_insn, hardret, softret)\n+     rtx orig_insn;\n+     rtx hardret, softret;\n+{\n+  rtx insn, set = NULL_RTX;\n+\n+  insn = next_nonnote_insn (orig_insn);\n+  if (! insn)\n+    return orig_insn;\n+\n+  set = single_set (insn);\n+  if (! set)\n+    return orig_insn;\n+\n+  /* The destination must be the same as the called function's return\n+     value to ensure that any return value is put in the same place by the\n+     current function and the function we're calling. \n+\n+     Further, the source must be the same as the pseudo into which the\n+     called function's return value was copied.  Otherwise we're returning\n+     some other value.  */\n+\n+  if (SET_DEST (set) == current_function_return_rtx\n+      && REG_P (SET_DEST (set))\n+      && REGNO (SET_DEST (set)) == REGNO (hardret)\n+      && SET_SRC (set) == softret)\n+    return insn;\n+\n+  /* It did not look like a copy of the return value, so return the\n+     same insn we were passed.  */\n+  return orig_insn;\n+}\n+\n+/* If the first real insn after ORIG_INSN is a CODE of this function's return\n+   value, return insn.  Otherwise return ORIG_INSN.  */\n+\n+static rtx\n+skip_use_of_return_value (orig_insn, code)\n+     rtx orig_insn;\n+     enum rtx_code code;\n+{\n+  rtx insn;\n+\n+  insn = next_nonnote_insn (orig_insn);\n+\n+  if (insn\n+      && GET_CODE (insn) == INSN\n+      && GET_CODE (PATTERN (insn)) == code\n+      && (XEXP (PATTERN (insn), 0) == current_function_return_rtx\n+\t  || XEXP (PATTERN (insn), 0) == const0_rtx))\n+    return insn;\n+\n+  return orig_insn;\n+}\n+\n+/* If the first real insn after ORIG_INSN adjusts the stack pointer\n+   by a constant, return the insn with the stack pointer adjustment.\n+   Otherwise return ORIG_INSN.  */\n+\n+static rtx\n+skip_stack_adjustment (orig_insn)\n+     rtx orig_insn;\n+{\n+  rtx insn, set = NULL_RTX;\n+\n+  insn = next_nonnote_insn (orig_insn);\n+\n+  if (insn)\n+    set = single_set (insn);\n+\n+  /* The source must be the same as the current function's return value to\n+     ensure that any return value is put in the same place by the current\n+     function and the function we're calling.   The destination register\n+     must be a pseudo.  */\n+  if (insn\n+      && set\n+      && GET_CODE (SET_SRC (set)) == PLUS\n+      && XEXP (SET_SRC (set), 0) == stack_pointer_rtx\n+      && GET_CODE (XEXP (SET_SRC (set), 1)) == CONST_INT\n+      && SET_DEST (set) == stack_pointer_rtx)\n+    return insn;\n+\n+  /* It did not look like a copy of the return value, so return the\n+     same insn we were passed.  */\n+  return orig_insn;\n+}\n+\n+/* If the first real insn after ORIG_INSN is a jump, return the JUMP_INSN.\n+   Otherwise return ORIG_INSN.  */\n+\n+static rtx\n+skip_jump_insn (orig_insn)\n+     rtx orig_insn;\n+{\n+  rtx insn;\n+\n+  insn = next_nonnote_insn (orig_insn);\n+\n+  if (insn\n+      && GET_CODE (insn) == JUMP_INSN\n+      && simplejump_p (insn))\n+    return insn;\n+\n+  return orig_insn;\n+}\n+\n+/* Scan the rtx X for an ADDRESSOF expressions.  Return nonzero if an ADDRESSOF\n+   expresion is found, else return zero.  */\n+\n+static int\n+uses_addressof (x)\n+     rtx x;\n+{\n+  RTX_CODE code;\n+  int i, j;\n+  const char *fmt;\n+\n+  if (x == NULL_RTX)\n+    return 0;\n+\n+  code = GET_CODE (x);\n+\n+  if (code == ADDRESSOF)\n+    return 1;\n+\n+  /* Scan all subexpressions. */\n+  fmt = GET_RTX_FORMAT (code);\n+  for (i = 0; i < GET_RTX_LENGTH (code); i++, fmt++)\n+    {\n+      if (*fmt == 'e')\n+\t{\n+\t  if (uses_addressof (XEXP (x, i)))\n+\t    return 1;\n+\t}\n+      else if (*fmt == 'E')\n+\t{\n+\t  for (j = 0; j < XVECLEN (x, i); j++)\n+\t    if (uses_addressof (XVECEXP (x, i, j)))\n+\t      return 1;\n+\t}\n+    }\n+  return 0;\n+}\n+\n+/* Scan the sequence of insns in SEQ to see if any have an ADDRESSOF\n+   rtl expression.  If an ADDRESSOF expression is found, return nonzero,\n+   else return zero.\n+\n+   This function handles CALL_PLACEHOLDERs which contain multiple sequences\n+   of insns.  */\n+\n+static int\n+sequence_uses_addressof (seq)\n+     rtx seq;\n+{\n+  rtx insn;\n+\n+  for (insn = seq; insn; insn = NEXT_INSN (insn))\n+    if (GET_RTX_CLASS (GET_CODE (insn)) == 'i')\n+      {\n+\t/* If this is a CALL_PLACEHOLDER, then recursively call ourselves\n+\t   with each nonempty sequence attached to the CALL_PLACEHOLDER.  */\n+\tif (GET_CODE (insn) == CALL_INSN\n+\t    && GET_CODE (PATTERN (insn)) == CALL_PLACEHOLDER)\n+\t  {\n+\t    if (XEXP (PATTERN (insn), 0) != NULL_RTX\n+\t\t&& sequence_uses_addressof (XEXP (PATTERN (insn), 0)))\n+\t      return 1;\n+\t    if (XEXP (PATTERN (insn), 1) != NULL_RTX\n+\t\t&& sequence_uses_addressof (XEXP (PATTERN (insn), 1)))\n+\t      return 1;\n+\t    if (XEXP (PATTERN (insn), 2) != NULL_RTX\n+\t\t&& sequence_uses_addressof (XEXP (PATTERN (insn), 2)))\n+\t      return 1;\n+\t  }\n+\telse if (uses_addressof (PATTERN (insn))\n+\t\t || (REG_NOTES (insn) && uses_addressof (REG_NOTES (insn))))\n+\t  return 1;\n+      }\n+  return 0;\n+}\n+\n+/* Remove all REG_EQUIV notes found in the insn chain.  */\n+\n+static void\n+purge_reg_equiv_notes ()\n+{\n+  rtx insn;\n+\n+  for (insn = get_insns (); insn; insn = NEXT_INSN (insn))\n+    {\n+      while (1)\n+\t{\n+\t  rtx note = find_reg_note (insn, REG_EQUIV, 0);\n+\t  if (note)\n+\t    {\n+\t      /* Remove the note and keep looking at the notes for\n+\t\t this insn.  */\n+\t      remove_note (insn, note);\n+\t      continue;\n+\t    }\n+\t  break;\n+\t}\n+    }\n+}\n+\n+/* Replace the CALL_PLACEHOLDER with one of its children.  INSN should be\n+   the CALL_PLACEHOLDER insn; USE tells which child to use.  */\n+\n+void\n+replace_call_placeholder (insn, use)\n+     rtx insn;\n+     sibcall_use_t use;\n+{\n+  if (use == sibcall_use_tail_recursion)\n+    emit_insns_before (XEXP (PATTERN (insn), 2), insn);\n+  else if (use == sibcall_use_sibcall)\n+    emit_insns_before (XEXP (PATTERN (insn), 1), insn);\n+  else if (use == sibcall_use_normal)\n+    emit_insns_before (XEXP (PATTERN (insn), 0), insn);\n+  else\n+    abort();\n+\n+  /* Turn off LABEL_PRESERVE_P for the tail recursion label if it\n+     exists.  We only had to set it long enough to keep the jump\n+     pass above from deleting it as unused.  */\n+  if (XEXP (PATTERN (insn), 3))\n+    LABEL_PRESERVE_P (XEXP (PATTERN (insn), 3)) = 0;\n+  \n+  /* \"Delete\" the placeholder insn. */\n+  PUT_CODE (insn, NOTE);\n+  NOTE_SOURCE_FILE (insn) = 0;\n+  NOTE_LINE_NUMBER (insn) = NOTE_INSN_DELETED;\n+}\n+\n+\n+/* Given a (possibly empty) set of potential sibling or tail recursion call\n+   sites, determine if optimization is possible.\n+\n+   Potential sibling or tail recursion calls are marked with CALL_PLACEHOLDER\n+   insns.  The CALL_PLACEHOLDER insn holds chains of insns to implement a\n+   normal call, sibling call or tail recursive call.\n+\n+   Replace the CALL_PLACEHOLDER with an appropriate insn chain.  */\n+\n+void\n+optimize_sibling_and_tail_recursive_calls ()\n+{\n+  rtx insn, insns;\n+  basic_block alternate_exit = EXIT_BLOCK_PTR;\n+  int current_function_uses_addressof;\n+  int successful_sibling_call = 0;\n+  int replaced_call_placeholder = 0;\n+  edge e;\n+\n+  insns = get_insns ();\n+\n+  /* We do not perform these calls when flag_exceptions is true, so this\n+     is probably a NOP at the current time.  However, we may want to support\n+     sibling and tail recursion optimizations in the future, so let's plan\n+     ahead and find all the EH labels.  */\n+  find_exception_handler_labels ();\n+\n+  /* Run a jump optimization pass to clean up the CFG.  We primarily want\n+     this to thread jumps so that it is obvious which blocks jump to the\n+     epilouge.  */\n+  jump_optimize_minimal (insns);\n+\n+  /* We need cfg information to determine which blocks are succeeded\n+     only by the epilogue.  */\n+  find_basic_blocks (insns, max_reg_num (), 0);\n+  cleanup_cfg (insns);\n+\n+  /* If there are no basic blocks, then there is nothing to do.  */\n+  if (n_basic_blocks == 0)\n+    return;\n+\n+  /* Find the exit block.\n+\n+     It is possible that we have blocks which can reach the exit block\n+     directly.  However, most of the time a block will jump (or fall into)\n+     N_BASIC_BLOCKS - 1, which in turn falls into the exit block.  */\n+  for (e = EXIT_BLOCK_PTR->pred;\n+       e && alternate_exit == EXIT_BLOCK_PTR;\n+       e = e->pred_next)\n+    {\n+      rtx insn;\n+\n+      if (e->dest != EXIT_BLOCK_PTR || e->succ_next != NULL)\n+\tcontinue;\n+\n+      /* Walk forwards through the last normal block and see if it\n+\t does nothing except fall into the exit block.  */\n+      for (insn = BLOCK_HEAD (n_basic_blocks - 1);\n+\t   insn;\n+\t   insn = NEXT_INSN (insn))\n+\t{\n+\t  /* This should only happen once, at the start of this block.  */\n+\t  if (GET_CODE (insn) == CODE_LABEL)\n+\t    continue;\n+\n+\t  if (GET_CODE (insn) == NOTE)\n+\t    continue;\n+\n+\t  if (GET_CODE (insn) == INSN\n+\t      && GET_CODE (PATTERN (insn)) == USE)\n+\t    continue;\n+\n+\t  break;\n+\t}\n+\n+      /* If INSN is zero, then the search walked all the way through the\n+\t block without hitting anything interesting.  This block is a\n+\t valid alternate exit block.  */\n+      if (insn == NULL)\n+\talternate_exit = e->src;\n+    }\n+\n+  /* If the function uses ADDRESSOF, we can't (easily) determine\n+     at this point if the value will end up on the stack.  */\n+  current_function_uses_addressof = sequence_uses_addressof (insns);\n+\n+  /* Walk the insn chain and find any CALL_PLACEHOLDER insns.  We need to\n+     select one of the insn sequences attached to each CALL_PLACEHOLDER.\n+\n+     The different sequences represent different ways to implement the call,\n+     ie, tail recursion, sibling call or normal call.\n+\n+     Since we do not create nested CALL_PLACEHOLDERs, the scan\n+     continues with the insn that was after a replaced CALL_PLACEHOLDER;\n+     we don't rescan the replacement insns.  */\n+  for (insn = insns; insn; insn = NEXT_INSN (insn))\n+    {\n+      if (GET_CODE (insn) == CALL_INSN\n+\t  && GET_CODE (PATTERN (insn)) == CALL_PLACEHOLDER)\n+\t{\n+\t  int sibcall = (XEXP (PATTERN (insn), 1) != NULL_RTX);\n+\t  int tailrecursion = (XEXP (PATTERN (insn), 2) != NULL_RTX);\n+\t  basic_block succ_block, call_block;\n+\t  rtx temp, hardret, softret;\n+\n+\t  /* We must be careful with stack slots which are live at\n+\t     potential optimization sites.\n+\n+\t     ?!? This test is overly conservative and will be replaced.  */\n+\t  if (frame_offset)\n+\t    goto failure;\n+\n+\t  /* alloca (until we have stack slot life analysis) inhibits\n+\t     sibling call optimizations, but not tail recursion.\n+\n+\t     Similarly if we have ADDRESSOF expressions.\n+\n+\t     Similarly if we use varargs or stdarg since they implicitly\n+\t     may take the address of an argument.  */\n+ \t  if (current_function_calls_alloca || current_function_uses_addressof\n+\t      || current_function_varargs || current_function_stdarg)\n+\t    sibcall = 0;\n+\n+\t  call_block = BLOCK_FOR_INSN (insn);\n+\n+\t  /* If the block has more than one successor, then we can not\n+\t     perform sibcall or tail recursion optimizations.  */\n+\t  if (call_block->succ == NULL\n+\t      || call_block->succ->succ_next != NULL)\n+\t    goto failure;\n+\n+\t  /* If the single successor is not the exit block, then we can not\n+\t     perform sibcall or tail recursion optimizations. \n+\n+\t     Note that this test combined with the previous is sufficient\n+\t     to prevent tail call optimization in the presense of active\n+\t     exception handlers.  */\n+\t  succ_block = call_block->succ->dest;\n+\t  if (succ_block != EXIT_BLOCK_PTR && succ_block != alternate_exit)\n+\t    goto failure;\n+\n+\t  /* If the call was the end of the block, then we're OK.  */\n+\t  temp = insn;\n+\t  if (temp == call_block->end)\n+\t    goto success;\n+\n+\t  /* Skip over copying from the call's return value pseudo into\n+\t     this function's hard return register.  */\n+\t  if (identify_call_return_value (PATTERN (insn), &hardret, &softret))\n+\t    {\n+\t      temp = skip_copy_to_return_value (temp, hardret, softret);\n+\t      if (temp == call_block->end)\n+\t        goto success;\n+\t    }\n+\n+\t  /* Skip any stack adjustment.  */\n+\t  temp = skip_stack_adjustment (temp);\n+\t  if (temp == call_block->end)\n+\t    goto success;\n+\n+\t  /* Skip over a CLOBBER of the return value (as a hard reg).  */\n+\t  temp = skip_use_of_return_value (temp, CLOBBER);\n+\t  if (temp == call_block->end)\n+\t    goto success;\n+\n+\t  /* Skip over a USE of the return value (as a hard reg).  */\n+\t  temp = skip_use_of_return_value (temp, USE);\n+\t  if (temp == call_block->end)\n+\t    goto success;\n+\n+\t  /* Skip over the JUMP_INSN at the end of the block.  */\n+\t  temp = skip_jump_insn (temp);\n+\t  if (GET_CODE (temp) == NOTE)\n+\t    temp = next_nonnote_insn (temp);\n+\t  if (temp == call_block->end)\n+\t    goto success;\n+\n+\t  /* There are operations at the end of the block which we must\n+\t     execute after returning from the function call.  So this call\n+\t     can not be optimized.  */\n+failure:\n+\t  sibcall = 0, tailrecursion = 0;\n+success:\n+\n+\t  /* Select a set of insns to implement the call and emit them.\n+\t     Tail recursion is the most efficient, so select it over\n+\t     a tail/sibling call.  */\n+  \n+\t  if (sibcall)\n+\t    successful_sibling_call = 1;\n+\t  replaced_call_placeholder = 1;\n+\t  replace_call_placeholder (insn, \n+\t\t\t\t    tailrecursion != 0 \n+\t\t\t\t      ? sibcall_use_tail_recursion\n+\t\t\t\t      : sibcall != 0\n+\t\t\t\t\t ? sibcall_use_sibcall\n+\t\t\t\t\t : sibcall_use_normal);\n+\t}\n+    }\n+\n+  /* A sibling call sequence invalidates any REG_EQUIV notes made for\n+     this function's incoming arguments. \n+\n+     At the start of RTL generation we know the only REG_EQUIV notes\n+     in the rtl chain are those for incoming arguments, so we can safely\n+     flush any REG_EQUIV note. \n+\n+     This is (slight) overkill.  We could keep track of the highest argument\n+     we clobber and be more selective in removing notes, but it does not\n+     seem to be worth the effort.  */\n+  if (successful_sibling_call)\n+    purge_reg_equiv_notes ();\n+\n+  /* There may have been NOTE_INSN_BLOCK_{BEGIN,END} notes in the \n+     CALL_PLACEHOLDER alternatives that we didn't emit.  Rebuild the\n+     lexical block tree to correspond to the notes that still exist.  */\n+  if (replaced_call_placeholder)\n+    unroll_block_trees ();\n+\n+  /* This information will be invalid after inline expansion.  Kill it now.  */\n+  free_basic_block_vars (0);\n+}"}, {"sha": "1e1a650139c59955f84cc617fe80d6303cf4b9ec", "filename": "gcc/toplev.c", "status": "modified", "additions": 9, "deletions": 2, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Ftoplev.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Ftoplev.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftoplev.c?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -2986,6 +2986,15 @@ rest_of_compilation (decl)\n \tgoto exit_rest_of_compilation;\n     }\n \n+  /* We may have potential sibling or tail recursion sites.  Select one\n+     (of possibly multiple) methods of performing the call.  */\n+  init_EXPR_INSN_LIST_cache ();\n+  if (optimize)\n+    optimize_sibling_and_tail_recursive_calls ();\n+  \n+  if (ggc_p)\n+    ggc_collect ();\n+\n   /* Initialize some variables used by the optimizers.  */\n   init_function_for_compilation ();\n \n@@ -3030,8 +3039,6 @@ rest_of_compilation (decl)\n \n   unshare_all_rtl (current_function_decl, insns);\n \n-  init_EXPR_INSN_LIST_cache ();\n-\n #ifdef SETJMP_VIA_SAVE_AREA\n   /* This must be performed before virutal register instantiation.  */\n   if (current_function_calls_alloca)"}, {"sha": "c6045fffffeae117189c53566fd290242a6fa189", "filename": "gcc/tree.c", "status": "modified", "additions": 79, "deletions": 0, "changes": 79, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Ftree.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Ftree.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree.c?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -285,6 +285,9 @@ static void mark_type_hash PARAMS ((void *));\n void (*lang_unsave) PARAMS ((tree *));\n void (*lang_unsave_expr_now) PARAMS ((tree));\n \n+/* If non-null, a language specific version of safe_for_unsave. */\n+int (*lang_safe_for_unsave) PARAMS ((tree));\n+\n /* The string used as a placeholder instead of a source file name for\n    built-in tree nodes.  The variable, which is dynamically allocated,\n    should be used; the macro is only used to initialize it.  */\n@@ -2666,6 +2669,82 @@ unsave_expr_now (expr)\n \n   return expr;\n }\n+\n+/* Return nonzero if it is safe to unsave EXPR, else return zero.\n+   It is not safe to unsave EXPR if it contains any embedded RTL_EXPRs.  */\n+\n+int\n+safe_for_unsave (expr)\n+     tree expr;\n+{\n+  enum tree_code code;\n+  register int i;\n+  int first_rtl;\n+\n+  if (expr == NULL_TREE)\n+    return 1;\n+\n+  code = TREE_CODE (expr);\n+  first_rtl = first_rtl_op (code);\n+  switch (code)\n+    {\n+    case RTL_EXPR:\n+      return 0;\n+\n+    case CALL_EXPR:\n+      if (TREE_OPERAND (expr, 1)\n+\t  && TREE_CODE (TREE_OPERAND (expr, 1)) == TREE_LIST)\n+\t{\n+\t  tree exp = TREE_OPERAND (expr, 1);\n+\t  while (exp)\n+\t    {\n+\t      if (! safe_for_unsave (TREE_VALUE (exp)))\n+\t\treturn 0;\n+\t      exp = TREE_CHAIN (exp);\n+\t    }\n+\t}\n+      break;\n+\n+    default:\n+      if (lang_safe_for_unsave)\n+\tswitch ((*lang_safe_for_unsave) (expr))\n+\t  {\n+\t  case -1:\n+\t    break;\n+\t  case 0:\n+\t    return 0;\n+\t  case 1:\n+\t    return 1;\n+\t  default:\n+\t    abort ();\n+\t  }\n+      break;\n+    }\n+\n+  switch (TREE_CODE_CLASS (code))\n+    {\n+    case 'c':  /* a constant */\n+    case 't':  /* a type node */\n+    case 'x':  /* something random, like an identifier or an ERROR_MARK.  */\n+    case 'd':  /* A decl node */\n+    case 'b':  /* A block node */\n+      return 1;\n+\n+    case 'e':  /* an expression */\n+    case 'r':  /* a reference */\n+    case 's':  /* an expression with side effects */\n+    case '<':  /* a comparison expression */\n+    case '2':  /* a binary arithmetic expression */\n+    case '1':  /* a unary arithmetic expression */\n+      for (i = first_rtl - 1; i >= 0; i--)\n+\tif (! safe_for_unsave (TREE_OPERAND (expr, i)))\n+\t  return 0;\n+      return 1;\n+\n+    default:\n+      return 0;\n+    }\n+}\n \f\n /* Return 1 if EXP contains a PLACEHOLDER_EXPR; i.e., if it represents a size\n    or offset that depends on a field within a record.  */"}, {"sha": "f9503ceca88eda3f43068bcc6e80ad0052775c42", "filename": "gcc/tree.h", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Ftree.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0a1c58a25ab5df1a3e4596024774641ebae8be2a/gcc%2Ftree.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree.h?ref=0a1c58a25ab5df1a3e4596024774641ebae8be2a", "patch": "@@ -1983,6 +1983,11 @@ extern int first_rtl_op\t\t\tPARAMS ((enum tree_code));\n \n extern tree unsave_expr\t\t\tPARAMS ((tree));\n \n+/* safe_for_reeval_p (EXP) returns nonzero if it is possible to\n+   expand EXP multiple times.  */\n+\n+extern int safe_for_reeval_p\t\tPARAMS ((tree));\n+\n /* Reset EXP in place so that it can be expaned again.  Does not\n    recurse into subtrees.  */\n \n@@ -2000,6 +2005,9 @@ extern tree unsave_expr_now\t\tPARAMS ((tree));\n extern void (*lang_unsave)              PARAMS ((tree *));\n extern void (*lang_unsave_expr_now)     PARAMS ((tree));\n \n+/* If non-null, a language specific version of safe_for_unsave. */\n+extern int (*lang_safe_for_unsave)\tPARAMS ((tree));\n+\n /* Return 1 if EXP contains a PLACEHOLDER_EXPR; i.e., if it represents a size\n    or offset that depends on a field within a record.\n "}]}