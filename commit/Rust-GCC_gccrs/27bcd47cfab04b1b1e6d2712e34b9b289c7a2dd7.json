{"sha": "27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MjdiY2Q0N2NmYWIwNGIxYjFlNmQyNzEyZTM0YjliMjg5YzdhMmRkNw==", "commit": {"author": {"name": "Lawrence Crowl", "email": "crowl@google.com", "date": "2012-09-07T00:06:35Z"}, "committer": {"name": "Lawrence Crowl", "email": "crowl@gcc.gnu.org", "date": "2012-09-07T00:06:35Z"}, "message": "Modify gcc/*.[hc] double_int call sites to use the new interface.\n\nThis change entailed adding a few new methods to double_int.\n\nThe change results in a 0.163% time improvement with a 70% confidence.\n\nTested on x86_64.\n\n\nIndex: gcc/ChangeLog\n\n2012-09-06  Lawrence Crowl  <crowl@google.com>\n\n\t* double-int.h (double_int::operator &=): New.\n\t(double_int::operator ^=): New.\n\t(double_int::operator |=): New.\n\t(double_int::mul_with_sign): Modify overflow parameter to bool*.\n\t(double_int::add_with_sign): New.\n\t(double_int::ule): New.\n\t(double_int::sle): New.\n\t(binary double_int::operator *): Remove parameter name.\n\t(binary double_int::operator +): Likewise.\n\t(binary double_int::operator -): Likewise.\n\t(binary double_int::operator &): Likewise.\n\t(double_int::operator |): Likewise.\n\t(double_int::operator ^): Likewise.\n\t(double_int::and_not): Likewise.\n\t(double_int::from_shwi): Tidy formatting.\n\t(double_int::from_uhwi): Likewise.\n\t(double_int::from_uhwi): Likewise.\n\t* double-int.c (double_int::mul_with_sign): Modify overflow parameter\n\tto bool*.\n\t(double_int::add_with_sign): New.\n\t(double_int::ule): New.\n\t(double_int::sle): New.\n\t* builtins.c: Modify to use the new double_int interface.\n\t* cgraph.c: Likewise.\n\t* combine.c: Likewise.\n\t* dwarf2out.c: Likewise.\n\t* emit-rtl.c: Likewise.\n\t* expmed.c: Likewise.\n\t* expr.c: Likewise.\n\t* fixed-value.c: Likewise.\n\t* fold-const.c: Likewise.\n\t* gimple-fold.c: Likewise.\n\t* gimple-ssa-strength-reduction.c: Likewise.\n\t* gimplify-rtx.c: Likewise.\n\t* ipa-prop.c: Likewise.\n\t* loop-iv.c: Likewise.\n\t* optabs.c: Likewise.\n\t* stor-layout.c: Likewise.\n\t* tree-affine.c: Likewise.\n\t* tree-cfg.c: Likewise.\n\t* tree-dfa.c: Likewise.\n\t* tree-flow-inline.h: Likewise.\n\t* tree-object-size.c: Likewise.\n\t* tree-predcom.c: Likewise.\n\t* tree-pretty-print.c: Likewise.\n\t* tree-sra.c: Likewise.\n\t* tree-ssa-address.c: Likewise.\n\t* tree-ssa-alias.c: Likewise.\n\t* tree-ssa-ccp.c: Likewise.\n\t* tree-ssa-forwprop.c: Likewise.\n\t* tree-ssa-loop-ivopts.c: Likewise.\n\t* tree-ssa-loop-niter.c: Likewise.\n\t* tree-ssa-phiopt.c: Likewise.\n\t* tree-ssa-pre.c: Likewise.\n\t* tree-ssa-sccvn: Likewise.\n\t* tree-ssa-structalias.c: Likewise.\n\t* tree-ssa.c: Likewise.\n\t* tree-switch-conversion.c: Likewise.\n\t* tree-vect-loop-manip.c: Likewise.\n\t* tree-vrp.c: Likewise.\n\t* tree.h: Likewise.\n\t* tree.c: Likewise.\n\t* varasm.c: Likewise.\n\nFrom-SVN: r191047", "tree": {"sha": "82231821d6793cd33f15d6b9792a8b82f2ec15d1", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/82231821d6793cd33f15d6b9792a8b82f2ec15d1"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "html_url": "https://github.com/Rust-GCC/gccrs/commit/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/comments", "author": null, "committer": null, "parents": [{"sha": "316b938ed79ef024177ab82057a061a7a4b5af67", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/316b938ed79ef024177ab82057a061a7a4b5af67", "html_url": "https://github.com/Rust-GCC/gccrs/commit/316b938ed79ef024177ab82057a061a7a4b5af67"}], "stats": {"total": 1819, "additions": 893, "deletions": 926}, "files": [{"sha": "c1639906f73a9b9b35ed7d6308f09354e0039f01", "filename": "gcc/ChangeLog", "status": "modified", "additions": 66, "deletions": 0, "changes": 66, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -1,3 +1,69 @@\n+2012-09-06  Lawrence Crowl  <crowl@google.com>\n+\n+\t* double-int.h (double_int::operator &=): New.\n+\t(double_int::operator ^=): New.\n+\t(double_int::operator |=): New.\n+\t(double_int::mul_with_sign): Modify overflow parameter to bool*.\n+\t(double_int::add_with_sign): New.\n+\t(double_int::ule): New.\n+\t(double_int::sle): New.\n+\t(binary double_int::operator *): Remove parameter name.\n+\t(binary double_int::operator +): Likewise.\n+\t(binary double_int::operator -): Likewise.\n+\t(binary double_int::operator &): Likewise.\n+\t(double_int::operator |): Likewise.\n+\t(double_int::operator ^): Likewise.\n+\t(double_int::and_not): Likewise.\n+\t(double_int::from_shwi): Tidy formatting.\n+\t(double_int::from_uhwi): Likewise.\n+\t(double_int::from_uhwi): Likewise.\n+\t* double-int.c (double_int::mul_with_sign): Modify overflow parameter\n+\tto bool*.\n+\t(double_int::add_with_sign): New.\n+\t(double_int::ule): New.\n+\t(double_int::sle): New.\n+\t* builtins.c: Modify to use the new double_int interface.\n+\t* cgraph.c: Likewise.\n+\t* combine.c: Likewise.\n+\t* dwarf2out.c: Likewise.\n+\t* emit-rtl.c: Likewise.\n+\t* expmed.c: Likewise.\n+\t* expr.c: Likewise.\n+\t* fixed-value.c: Likewise.\n+\t* fold-const.c: Likewise.\n+\t* gimple-fold.c: Likewise.\n+\t* gimple-ssa-strength-reduction.c: Likewise.\n+\t* gimplify-rtx.c: Likewise.\n+\t* ipa-prop.c: Likewise.\n+\t* loop-iv.c: Likewise.\n+\t* optabs.c: Likewise.\n+\t* stor-layout.c: Likewise.\n+\t* tree-affine.c: Likewise.\n+\t* tree-cfg.c: Likewise.\n+\t* tree-dfa.c: Likewise.\n+\t* tree-flow-inline.h: Likewise.\n+\t* tree-object-size.c: Likewise.\n+\t* tree-predcom.c: Likewise.\n+\t* tree-pretty-print.c: Likewise.\n+\t* tree-sra.c: Likewise.\n+\t* tree-ssa-address.c: Likewise.\n+\t* tree-ssa-alias.c: Likewise.\n+\t* tree-ssa-ccp.c: Likewise.\n+\t* tree-ssa-forwprop.c: Likewise.\n+\t* tree-ssa-loop-ivopts.c: Likewise.\n+\t* tree-ssa-loop-niter.c: Likewise.\n+\t* tree-ssa-phiopt.c: Likewise.\n+\t* tree-ssa-pre.c: Likewise.\n+\t* tree-ssa-sccvn: Likewise.\n+\t* tree-ssa-structalias.c: Likewise.\n+\t* tree-ssa.c: Likewise.\n+\t* tree-switch-conversion.c: Likewise.\n+\t* tree-vect-loop-manip.c: Likewise.\n+\t* tree-vrp.c: Likewise.\n+\t* tree.h: Likewise.\n+\t* tree.c: Likewise.\n+\t* varasm.c: Likewise.\n+\n 2012-09-06  Uros Bizjak  <ubizjak@gmail.com>\n \n \t* configure.ac (hle prefixes): Remove .code64."}, {"sha": "e6b10ea43b9b2400737e882a1a981a9f3f9dde00", "filename": "gcc/builtins.c", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fbuiltins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fbuiltins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fbuiltins.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -4990,7 +4990,7 @@ expand_builtin_signbit (tree exp, rtx target)\n \n   if (bitpos < GET_MODE_BITSIZE (rmode))\n     {\n-      double_int mask = double_int_setbit (double_int_zero, bitpos);\n+      double_int mask = double_int_zero.set_bit (bitpos);\n \n       if (GET_MODE_SIZE (imode) > GET_MODE_SIZE (rmode))\n \ttemp = gen_lowpart (rmode, temp);\n@@ -8775,14 +8775,14 @@ fold_builtin_memory_op (location_t loc, tree dest, tree src,\n \t\t  if (! operand_equal_p (TREE_OPERAND (src_base, 0),\n \t\t\t\t\t TREE_OPERAND (dest_base, 0), 0))\n \t\t    return NULL_TREE;\n-\t\t  off = double_int_add (mem_ref_offset (src_base),\n-\t\t\t\t\tshwi_to_double_int (src_offset));\n-\t\t  if (!double_int_fits_in_shwi_p (off))\n+\t\t  off = mem_ref_offset (src_base) +\n+\t\t\t\t\tdouble_int::from_shwi (src_offset);\n+\t\t  if (!off.fits_shwi ())\n \t\t    return NULL_TREE;\n \t\t  src_offset = off.low;\n-\t\t  off = double_int_add (mem_ref_offset (dest_base),\n-\t\t\t\t\tshwi_to_double_int (dest_offset));\n-\t\t  if (!double_int_fits_in_shwi_p (off))\n+\t\t  off = mem_ref_offset (dest_base) +\n+\t\t\t\t\tdouble_int::from_shwi (dest_offset);\n+\t\t  if (!off.fits_shwi ())\n \t\t    return NULL_TREE;\n \t\t  dest_offset = off.low;\n \t\t  if (ranges_overlap_p (src_offset, maxsize,\n@@ -12696,7 +12696,7 @@ fold_builtin_object_size (tree ptr, tree ost)\n     {\n       bytes = compute_builtin_object_size (ptr, object_size_type);\n       if (double_int_fits_to_tree_p (size_type_node,\n-\t\t\t\t     uhwi_to_double_int (bytes)))\n+\t\t\t\t     double_int::from_uhwi (bytes)))\n \treturn build_int_cstu (size_type_node, bytes);\n     }\n   else if (TREE_CODE (ptr) == SSA_NAME)\n@@ -12707,7 +12707,7 @@ fold_builtin_object_size (tree ptr, tree ost)\n       bytes = compute_builtin_object_size (ptr, object_size_type);\n       if (bytes != (unsigned HOST_WIDE_INT) (object_size_type < 2 ? -1 : 0)\n           && double_int_fits_to_tree_p (size_type_node,\n-\t\t\t\t\tuhwi_to_double_int (bytes)))\n+\t\t\t\t\tdouble_int::from_uhwi (bytes)))\n \treturn build_int_cstu (size_type_node, bytes);\n     }\n "}, {"sha": "3d4703b4b636b1d7b2abc9a86aeb9ed16c528726", "filename": "gcc/cgraph.c", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fcgraph.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fcgraph.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcgraph.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -484,9 +484,8 @@ cgraph_add_thunk (struct cgraph_node *decl_node ATTRIBUTE_UNUSED,\n   \n   node = cgraph_create_node (alias);\n   gcc_checking_assert (!virtual_offset\n-\t\t       || double_int_equal_p\n-\t\t            (tree_to_double_int (virtual_offset),\n-\t\t\t     shwi_to_double_int (virtual_value)));\n+\t\t       || tree_to_double_int (virtual_offset) ==\n+\t\t\t     double_int::from_shwi (virtual_value));\n   node->thunk.fixed_offset = fixed_offset;\n   node->thunk.this_adjusting = this_adjusting;\n   node->thunk.virtual_value = virtual_value;"}, {"sha": "3284cee1a2ac03f88bcf8b313dfa83967f8492d6", "filename": "gcc/combine.c", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fcombine.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fcombine.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcombine.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -2673,11 +2673,11 @@ try_combine (rtx i3, rtx i2, rtx i1, rtx i0, int *new_direct_jump_p,\n \t  o = rtx_to_double_int (outer);\n \t  i = rtx_to_double_int (inner);\n \n-\t  m = double_int_mask (width);\n-\t  i = double_int_and (i, m);\n-\t  m = double_int_lshift (m, offset, HOST_BITS_PER_DOUBLE_INT, false);\n-\t  i = double_int_lshift (i, offset, HOST_BITS_PER_DOUBLE_INT, false);\n-\t  o = double_int_ior (double_int_and_not (o, m), i);\n+\t  m = double_int::mask (width);\n+\t  i &= m;\n+\t  m = m.llshift (offset, HOST_BITS_PER_DOUBLE_INT);\n+\t  i = i.llshift (offset, HOST_BITS_PER_DOUBLE_INT);\n+\t  o = o.and_not (m) | i;\n \n \t  combine_merges++;\n \t  subst_insn = i3;"}, {"sha": "66d3b04bd035e20e0668ef9a4a7b2b4085688972", "filename": "gcc/dojump.c", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fdojump.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fdojump.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdojump.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -165,8 +165,7 @@ prefer_and_bit_test (enum machine_mode mode, int bitnum)\n \n   /* Fill in the integers.  */\n   XEXP (and_test, 1)\n-    = immed_double_int_const (double_int_setbit (double_int_zero, bitnum),\n-\t\t\t\t\t\t mode);\n+    = immed_double_int_const (double_int_zero.set_bit (bitnum), mode);\n   XEXP (XEXP (shift_test, 0), 1) = GEN_INT (bitnum);\n \n   speed_p = optimize_insn_for_speed_p ();"}, {"sha": "f3d5e8b3dde97067a0af9ce41a854115a133f5b3", "filename": "gcc/double-int.c", "status": "modified", "additions": 42, "deletions": 2, "changes": 44, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fdouble-int.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fdouble-int.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdouble-int.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -606,7 +606,6 @@ div_and_round_double (unsigned code, int uns,\n   return overflow;\n }\n \n-\n /* Returns mask for PREC bits.  */\n \n double_int\n@@ -754,7 +753,7 @@ double_int::operator * (double_int b) const\n    *OVERFLOW is set to nonzero.  */\n \n double_int\n-double_int::mul_with_sign (double_int b, bool unsigned_p, int *overflow) const\n+double_int::mul_with_sign (double_int b, bool unsigned_p, bool *overflow) const\n {\n   const double_int &a = *this;\n   double_int ret;\n@@ -774,6 +773,19 @@ double_int::operator + (double_int b) const\n   return ret;\n }\n \n+/* Returns A + B. If the operation overflows according to UNSIGNED_P,\n+   *OVERFLOW is set to nonzero.  */\n+\n+double_int\n+double_int::add_with_sign (double_int b, bool unsigned_p, bool *overflow) const\n+{\n+  const double_int &a = *this;\n+  double_int ret;\n+  *overflow = add_double_with_sign (a.low, a.high, b.low, b.high,\n+                                    &ret.low, &ret.high, unsigned_p);\n+  return ret;\n+}\n+\n /* Returns A - B.  */\n \n double_int\n@@ -1104,6 +1116,20 @@ double_int::ult (double_int b) const\n   return false;\n }\n \n+/* Compares two unsigned values A and B for less-than or equal-to.  */\n+\n+bool\n+double_int::ule (double_int b) const\n+{\n+  if ((unsigned HOST_WIDE_INT) high < (unsigned HOST_WIDE_INT) b.high)\n+    return true;\n+  if ((unsigned HOST_WIDE_INT) high > (unsigned HOST_WIDE_INT) b.high)\n+    return false;\n+  if (low <= b.low)\n+    return true;\n+  return false;\n+}\n+\n /* Compares two unsigned values A and B for greater-than.  */\n \n bool\n@@ -1132,6 +1158,20 @@ double_int::slt (double_int b) const\n   return false;\n }\n \n+/* Compares two signed values A and B for less-than or equal-to.  */\n+\n+bool\n+double_int::sle (double_int b) const\n+{\n+  if (high < b.high)\n+    return true;\n+  if (high > b.high)\n+    return false;\n+  if (low <= b.low)\n+    return true;\n+  return false;\n+}\n+\n /* Compares two signed values A and B for greater-than.  */\n \n bool"}, {"sha": "bc7aca1896abc5a08dd7405444f2d6a429e6ed6f", "filename": "gcc/double-int.h", "status": "modified", "additions": 44, "deletions": 16, "changes": 60, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fdouble-int.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fdouble-int.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdouble-int.h?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -50,9 +50,8 @@ along with GCC; see the file COPYING3.  If not see\n    numbers with precision higher than HOST_WIDE_INT).  It might be less\n    confusing to have them both signed or both unsigned.  */\n \n-typedef struct double_int\n+struct double_int\n {\n-public:\n   /* Normally, we would define constructors to create instances.\n      Two things prevent us from doing so.\n      First, defining a constructor makes the class non-POD in C++03,\n@@ -78,6 +77,9 @@ typedef struct double_int\n   double_int &operator *= (double_int);\n   double_int &operator += (double_int);\n   double_int &operator -= (double_int);\n+  double_int &operator &= (double_int);\n+  double_int &operator ^= (double_int);\n+  double_int &operator |= (double_int);\n \n   /* The following functions are non-mutating operations.  */\n \n@@ -104,17 +106,18 @@ typedef struct double_int\n   /* Arithmetic operation functions.  */\n \n   double_int set_bit (unsigned) const;\n-  double_int mul_with_sign (double_int, bool, int *) const;\n+  double_int mul_with_sign (double_int, bool unsigned_p, bool *overflow) const;\n+  double_int add_with_sign (double_int, bool unsigned_p, bool *overflow) const;\n \n-  double_int operator * (double_int b) const;\n-  double_int operator + (double_int b) const;\n-  double_int operator - (double_int b) const;\n+  double_int operator * (double_int) const;\n+  double_int operator + (double_int) const;\n+  double_int operator - (double_int) const;\n   double_int operator - () const;\n   double_int operator ~ () const;\n-  double_int operator & (double_int b) const;\n-  double_int operator | (double_int b) const;\n-  double_int operator ^ (double_int b) const;\n-  double_int and_not (double_int b) const;\n+  double_int operator & (double_int) const;\n+  double_int operator | (double_int) const;\n+  double_int operator ^ (double_int) const;\n+  double_int and_not (double_int) const;\n \n   double_int lshift (HOST_WIDE_INT count, unsigned int prec, bool arith) const;\n   double_int rshift (HOST_WIDE_INT count, unsigned int prec, bool arith) const;\n@@ -156,8 +159,10 @@ typedef struct double_int\n   int scmp (double_int b) const;\n \n   bool ult (double_int b) const;\n+  bool ule (double_int b) const;\n   bool ugt (double_int b) const;\n   bool slt (double_int b) const;\n+  bool sle (double_int b) const;\n   bool sgt (double_int b) const;\n \n   double_int max (double_int b, bool uns);\n@@ -176,7 +181,7 @@ typedef struct double_int\n   unsigned HOST_WIDE_INT low;\n   HOST_WIDE_INT high;\n \n-} double_int;\n+};\n \n #define HOST_BITS_PER_DOUBLE_INT (2 * HOST_BITS_PER_WIDE_INT)\n \n@@ -185,8 +190,8 @@ typedef struct double_int\n /* Constructs double_int from integer CST.  The bits over the precision of\n    HOST_WIDE_INT are filled with the sign bit.  */\n \n-inline\n-double_int double_int::from_shwi (HOST_WIDE_INT cst)\n+inline double_int\n+double_int::from_shwi (HOST_WIDE_INT cst)\n {\n   double_int r;\n   r.low = (unsigned HOST_WIDE_INT) cst;\n@@ -215,8 +220,8 @@ shwi_to_double_int (HOST_WIDE_INT cst)\n /* Constructs double_int from unsigned integer CST.  The bits over the\n    precision of HOST_WIDE_INT are filled with zeros.  */\n \n-inline\n-double_int double_int::from_uhwi (unsigned HOST_WIDE_INT cst)\n+inline double_int\n+double_int::from_uhwi (unsigned HOST_WIDE_INT cst)\n {\n   double_int r;\n   r.low = cst;\n@@ -266,6 +271,27 @@ double_int::operator -= (double_int b)\n   return *this;\n }\n \n+inline double_int &\n+double_int::operator &= (double_int b)\n+{\n+  *this = *this & b;\n+  return *this;\n+}\n+\n+inline double_int &\n+double_int::operator ^= (double_int b)\n+{\n+  *this = *this ^ b;\n+  return *this;\n+}\n+\n+inline double_int &\n+double_int::operator |= (double_int b)\n+{\n+  *this = *this | b;\n+  return *this;\n+}\n+\n /* Returns value of CST as a signed number.  CST must satisfy\n    double_int::fits_signed.  */\n \n@@ -346,7 +372,9 @@ inline double_int\n double_int_mul_with_sign (double_int a, double_int b,\n \t\t\t  bool unsigned_p, int *overflow)\n {\n-  return a.mul_with_sign (b, unsigned_p, overflow);\n+  bool ovf;\n+  return a.mul_with_sign (b, unsigned_p, &ovf);\n+  *overflow = ovf;\n }\n \n /* FIXME(crowl): Remove after converting callers.  */"}, {"sha": "9adb07106ba23f307bd7a94b2a47de3340ca9178", "filename": "gcc/dwarf2out.c", "status": "modified", "additions": 17, "deletions": 20, "changes": 37, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fdwarf2out.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fdwarf2out.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdwarf2out.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -9332,13 +9332,13 @@ static inline double_int\n double_int_type_size_in_bits (const_tree type)\n {\n   if (TREE_CODE (type) == ERROR_MARK)\n-    return uhwi_to_double_int (BITS_PER_WORD);\n+    return double_int::from_uhwi (BITS_PER_WORD);\n   else if (TYPE_SIZE (type) == NULL_TREE)\n     return double_int_zero;\n   else if (TREE_CODE (TYPE_SIZE (type)) == INTEGER_CST)\n     return tree_to_double_int (TYPE_SIZE (type));\n   else\n-    return uhwi_to_double_int (TYPE_ALIGN (type));\n+    return double_int::from_uhwi (TYPE_ALIGN (type));\n }\n \n /*  Given a pointer to a tree node for a subrange type, return a pointer\n@@ -11758,7 +11758,7 @@ mem_loc_descriptor (rtx rtl, enum machine_mode mode,\n \t      mem_loc_result->dw_loc_oprnd2.val_class\n \t\t= dw_val_class_const_double;\n \t      mem_loc_result->dw_loc_oprnd2.v.val_double\n-\t\t= shwi_to_double_int (INTVAL (rtl));\n+\t\t= double_int::from_shwi (INTVAL (rtl));\n \t    }\n \t}\n       break;\n@@ -12317,7 +12317,7 @@ loc_descriptor (rtx rtl, enum machine_mode mode,\n \t\t  double_int val = rtx_to_double_int (elt);\n \n \t\t  if (elt_size <= sizeof (HOST_WIDE_INT))\n-\t\t    insert_int (double_int_to_shwi (val), elt_size, p);\n+\t\t    insert_int (val.to_shwi (), elt_size, p);\n \t\t  else\n \t\t    {\n \t\t      gcc_assert (elt_size == 2 * sizeof (HOST_WIDE_INT));\n@@ -13646,11 +13646,11 @@ simple_decl_align_in_bits (const_tree decl)\n static inline double_int\n round_up_to_align (double_int t, unsigned int align)\n {\n-  double_int alignd = uhwi_to_double_int (align);\n-  t = double_int_add (t, alignd);\n-  t = double_int_add (t, double_int_minus_one);\n-  t = double_int_div (t, alignd, true, TRUNC_DIV_EXPR);\n-  t = double_int_mul (t, alignd);\n+  double_int alignd = double_int::from_uhwi (align);\n+  t += alignd;\n+  t += double_int_minus_one;\n+  t = t.div (alignd, true, TRUNC_DIV_EXPR);\n+  t *= alignd;\n   return t;\n }\n \n@@ -13757,23 +13757,21 @@ field_byte_offset (const_tree decl)\n \n       /* Figure out the bit-distance from the start of the structure to\n \t the \"deepest\" bit of the bit-field.  */\n-      deepest_bitpos = double_int_add (bitpos_int, field_size_in_bits);\n+      deepest_bitpos = bitpos_int + field_size_in_bits;\n \n       /* This is the tricky part.  Use some fancy footwork to deduce\n \t where the lowest addressed bit of the containing object must\n \t be.  */\n-      object_offset_in_bits\n-\t= double_int_sub (deepest_bitpos, type_size_in_bits);\n+      object_offset_in_bits = deepest_bitpos - type_size_in_bits;\n \n       /* Round up to type_align by default.  This works best for\n \t bitfields.  */\n       object_offset_in_bits\n \t= round_up_to_align (object_offset_in_bits, type_align_in_bits);\n \n-      if (double_int_ucmp (object_offset_in_bits, bitpos_int) > 0)\n+      if (object_offset_in_bits.ugt (bitpos_int))\n \t{\n-\t  object_offset_in_bits\n-\t    = double_int_sub (deepest_bitpos, type_size_in_bits);\n+\t  object_offset_in_bits = deepest_bitpos - type_size_in_bits;\n \n \t  /* Round up to decl_align instead.  */\n \t  object_offset_in_bits\n@@ -13785,10 +13783,9 @@ field_byte_offset (const_tree decl)\n     object_offset_in_bits = bitpos_int;\n \n   object_offset_in_bytes\n-    = double_int_div (object_offset_in_bits,\n-\t\t      uhwi_to_double_int (BITS_PER_UNIT), true,\n-\t\t      TRUNC_DIV_EXPR);\n-  return double_int_to_shwi (object_offset_in_bytes);\n+    = object_offset_in_bits.div (double_int::from_uhwi (BITS_PER_UNIT),\n+\t\t\t\t true, TRUNC_DIV_EXPR);\n+  return object_offset_in_bytes.to_shwi ();\n }\n \f\n /* The following routines define various Dwarf attributes and any data\n@@ -14064,7 +14061,7 @@ add_const_value_attribute (dw_die_ref die, rtx rtl)\n \t\tdouble_int val = rtx_to_double_int (elt);\n \n \t\tif (elt_size <= sizeof (HOST_WIDE_INT))\n-\t\t  insert_int (double_int_to_shwi (val), elt_size, p);\n+\t\t  insert_int (val.to_shwi (), elt_size, p);\n \t\telse\n \t\t  {\n \t\t    gcc_assert (elt_size == 2 * sizeof (HOST_WIDE_INT));"}, {"sha": "34d85de495fe52eff2bdafa0d4193858977af115", "filename": "gcc/emit-rtl.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Femit-rtl.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Femit-rtl.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Femit-rtl.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -490,7 +490,7 @@ rtx_to_double_int (const_rtx cst)\n   double_int r;\n \n   if (CONST_INT_P (cst))\n-      r = shwi_to_double_int (INTVAL (cst));\n+      r = double_int::from_shwi (INTVAL (cst));\n   else if (CONST_DOUBLE_AS_INT_P (cst))\n     {\n       r.low = CONST_DOUBLE_LOW (cst);"}, {"sha": "24a15770a477c8fbbb346100743a94947e336d43", "filename": "gcc/expmed.c", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fexpmed.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fexpmed.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpmed.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -1985,11 +1985,11 @@ mask_rtx (enum machine_mode mode, int bitpos, int bitsize, int complement)\n {\n   double_int mask;\n \n-  mask = double_int_mask (bitsize);\n-  mask = double_int_lshift (mask, bitpos, HOST_BITS_PER_DOUBLE_INT, false);\n+  mask = double_int::mask (bitsize);\n+  mask = mask.llshift (bitpos, HOST_BITS_PER_DOUBLE_INT);\n \n   if (complement)\n-    mask = double_int_not (mask);\n+    mask = ~mask;\n \n   return immed_double_int_const (mask, mode);\n }\n@@ -2002,8 +2002,8 @@ lshift_value (enum machine_mode mode, rtx value, int bitpos, int bitsize)\n {\n   double_int val;\n   \n-  val = double_int_zext (uhwi_to_double_int (INTVAL (value)), bitsize);\n-  val = double_int_lshift (val, bitpos, HOST_BITS_PER_DOUBLE_INT, false);\n+  val = double_int::from_uhwi (INTVAL (value)).zext (bitsize);\n+  val = val.llshift (bitpos, HOST_BITS_PER_DOUBLE_INT);\n \n   return immed_double_int_const (val, mode);\n }"}, {"sha": "2ed2f960aa63717f86b5910b3bec6daa351b8849", "filename": "gcc/expr.c", "status": "modified", "additions": 26, "deletions": 35, "changes": 61, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fexpr.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fexpr.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpr.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -727,11 +727,11 @@ convert_modes (enum machine_mode mode, enum machine_mode oldmode, rtx x, int uns\n       && GET_MODE_BITSIZE (mode) == HOST_BITS_PER_DOUBLE_INT\n       && CONST_INT_P (x) && INTVAL (x) < 0)\n     {\n-      double_int val = uhwi_to_double_int (INTVAL (x));\n+      double_int val = double_int::from_uhwi (INTVAL (x));\n \n       /* We need to zero extend VAL.  */\n       if (oldmode != VOIDmode)\n-\tval = double_int_zext (val, GET_MODE_BITSIZE (oldmode));\n+\tval = val.zext (GET_MODE_BITSIZE (oldmode));\n \n       return immed_double_int_const (val, mode);\n     }\n@@ -6557,9 +6557,7 @@ get_inner_reference (tree exp, HOST_WIDE_INT *pbitsize,\n       switch (TREE_CODE (exp))\n \t{\n \tcase BIT_FIELD_REF:\n-\t  bit_offset\n-\t    = double_int_add (bit_offset,\n-\t\t\t      tree_to_double_int (TREE_OPERAND (exp, 2)));\n+\t  bit_offset += tree_to_double_int (TREE_OPERAND (exp, 2));\n \t  break;\n \n \tcase COMPONENT_REF:\n@@ -6574,9 +6572,7 @@ get_inner_reference (tree exp, HOST_WIDE_INT *pbitsize,\n \t      break;\n \n \t    offset = size_binop (PLUS_EXPR, offset, this_offset);\n-\t    bit_offset = double_int_add (bit_offset,\n-\t\t\t\t\t tree_to_double_int\n-\t\t\t\t\t   (DECL_FIELD_BIT_OFFSET (field)));\n+\t    bit_offset += tree_to_double_int (DECL_FIELD_BIT_OFFSET (field));\n \n \t    /* ??? Right now we don't do anything with DECL_OFFSET_ALIGN.  */\n \t  }\n@@ -6608,8 +6604,7 @@ get_inner_reference (tree exp, HOST_WIDE_INT *pbitsize,\n \t  break;\n \n \tcase IMAGPART_EXPR:\n-\t  bit_offset = double_int_add (bit_offset,\n-\t\t\t\t       uhwi_to_double_int (*pbitsize));\n+\t  bit_offset += double_int::from_uhwi (*pbitsize);\n \t  break;\n \n \tcase VIEW_CONVERT_EXPR:\n@@ -6631,11 +6626,10 @@ get_inner_reference (tree exp, HOST_WIDE_INT *pbitsize,\n \t      if (!integer_zerop (off))\n \t\t{\n \t\t  double_int boff, coff = mem_ref_offset (exp);\n-\t\t  boff = double_int_lshift (coff,\n-\t\t\t\t\t    BITS_PER_UNIT == 8\n-\t\t\t\t\t    ? 3 : exact_log2 (BITS_PER_UNIT),\n-\t\t\t\t\t    HOST_BITS_PER_DOUBLE_INT, true);\n-\t\t  bit_offset = double_int_add (bit_offset, boff);\n+\t\t  boff = coff.alshift (BITS_PER_UNIT == 8\n+\t\t\t\t       ? 3 : exact_log2 (BITS_PER_UNIT),\n+\t\t\t\t       HOST_BITS_PER_DOUBLE_INT);\n+\t\t  bit_offset += boff;\n \t\t}\n \t      exp = TREE_OPERAND (TREE_OPERAND (exp, 0), 0);\n \t    }\n@@ -6659,15 +6653,13 @@ get_inner_reference (tree exp, HOST_WIDE_INT *pbitsize,\n   if (TREE_CODE (offset) == INTEGER_CST)\n     {\n       double_int tem = tree_to_double_int (offset);\n-      tem = double_int_sext (tem, TYPE_PRECISION (sizetype));\n-      tem = double_int_lshift (tem,\n-\t\t\t       BITS_PER_UNIT == 8\n-\t\t\t       ? 3 : exact_log2 (BITS_PER_UNIT),\n-\t\t\t       HOST_BITS_PER_DOUBLE_INT, true);\n-      tem = double_int_add (tem, bit_offset);\n-      if (double_int_fits_in_shwi_p (tem))\n-\t{\n-\t  *pbitpos = double_int_to_shwi (tem);\n+      tem = tem.sext (TYPE_PRECISION (sizetype));\n+      tem = tem.alshift (BITS_PER_UNIT == 8 ? 3 : exact_log2 (BITS_PER_UNIT),\n+\t\t\t HOST_BITS_PER_DOUBLE_INT);\n+      tem += bit_offset;\n+      if (tem.fits_shwi ())\n+\t{\n+\t  *pbitpos = tem.to_shwi ();\n \t  *poffset = offset = NULL_TREE;\n \t}\n     }\n@@ -6676,24 +6668,23 @@ get_inner_reference (tree exp, HOST_WIDE_INT *pbitsize,\n   if (offset)\n     {\n       /* Avoid returning a negative bitpos as this may wreak havoc later.  */\n-      if (double_int_negative_p (bit_offset))\n+      if (bit_offset.is_negative ())\n         {\n \t  double_int mask\n-\t    = double_int_mask (BITS_PER_UNIT == 8\n+\t    = double_int::mask (BITS_PER_UNIT == 8\n \t\t\t       ? 3 : exact_log2 (BITS_PER_UNIT));\n-\t  double_int tem = double_int_and_not (bit_offset, mask);\n+\t  double_int tem = bit_offset.and_not (mask);\n \t  /* TEM is the bitpos rounded to BITS_PER_UNIT towards -Inf.\n \t     Subtract it to BIT_OFFSET and add it (scaled) to OFFSET.  */\n-\t  bit_offset = double_int_sub (bit_offset, tem);\n-\t  tem = double_int_rshift (tem,\n-\t\t\t\t   BITS_PER_UNIT == 8\n-\t\t\t\t   ? 3 : exact_log2 (BITS_PER_UNIT),\n-\t\t\t\t   HOST_BITS_PER_DOUBLE_INT, true);\n+\t  bit_offset -= tem;\n+\t  tem = tem.arshift (BITS_PER_UNIT == 8\n+\t\t\t     ? 3 : exact_log2 (BITS_PER_UNIT),\n+\t\t\t     HOST_BITS_PER_DOUBLE_INT);\n \t  offset = size_binop (PLUS_EXPR, offset,\n \t\t\t       double_int_to_tree (sizetype, tem));\n \t}\n \n-      *pbitpos = double_int_to_shwi (bit_offset);\n+      *pbitpos = bit_offset.to_shwi ();\n       *poffset = offset;\n     }\n \n@@ -8720,7 +8711,7 @@ expand_expr_real_2 (sepops ops, rtx target, enum machine_mode tmode,\n       if (reduce_bit_field && TYPE_UNSIGNED (type))\n \ttemp = expand_binop (mode, xor_optab, op0,\n \t\t\t     immed_double_int_const\n-\t\t\t       (double_int_mask (TYPE_PRECISION (type)), mode),\n+\t\t\t       (double_int::mask (TYPE_PRECISION (type)), mode),\n \t\t\t     target, 1, OPTAB_LIB_WIDEN);\n       else\n \ttemp = expand_unop (mode, one_cmpl_optab, op0, target, 1);\n@@ -10407,7 +10398,7 @@ reduce_to_bit_field_precision (rtx exp, rtx target, tree type)\n     }\n   else if (TYPE_UNSIGNED (type))\n     {\n-      rtx mask = immed_double_int_const (double_int_mask (prec),\n+      rtx mask = immed_double_int_const (double_int::mask (prec),\n \t\t\t\t\t GET_MODE (exp));\n       return expand_and (GET_MODE (exp), exp, mask, target);\n     }"}, {"sha": "b74a60e29f25d13afa9ebde617f822688fdfaf64", "filename": "gcc/fixed-value.c", "status": "modified", "additions": 12, "deletions": 17, "changes": 29, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ffixed-value.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ffixed-value.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ffixed-value.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -376,9 +376,8 @@ do_fixed_multiply (FIXED_VALUE_TYPE *f, const FIXED_VALUE_TYPE *a,\n   if (GET_MODE_PRECISION (f->mode) <= HOST_BITS_PER_WIDE_INT)\n     {\n       f->data = a->data * b->data;\n-      f->data = f->data.lshift ((-GET_MODE_FBIT (f->mode)),\n-\t\t     HOST_BITS_PER_DOUBLE_INT,\n-\t\t     !unsigned_p);\n+      f->data = f->data.lshift (-GET_MODE_FBIT (f->mode),\n+\t\t\t\tHOST_BITS_PER_DOUBLE_INT, !unsigned_p);\n       overflow_p = fixed_saturate1 (f->mode, f->data, &f->data, sat_p);\n     }\n   else\n@@ -466,9 +465,8 @@ do_fixed_multiply (FIXED_VALUE_TYPE *f, const FIXED_VALUE_TYPE *a,\n \t  f->data.high = f->data.high | s.high;\n \t  s.low = f->data.low;\n \t  s.high = f->data.high;\n-\t  r = r.lshift ((-GET_MODE_FBIT (f->mode)),\n-\t\t\t HOST_BITS_PER_DOUBLE_INT,\n-\t\t\t !unsigned_p);\n+\t  r = r.lshift (-GET_MODE_FBIT (f->mode),\n+\t\t\tHOST_BITS_PER_DOUBLE_INT, !unsigned_p);\n \t}\n \n       overflow_p = fixed_saturate2 (f->mode, r, s, &f->data, sat_p);\n@@ -493,8 +491,7 @@ do_fixed_divide (FIXED_VALUE_TYPE *f, const FIXED_VALUE_TYPE *a,\n   if (GET_MODE_PRECISION (f->mode) <= HOST_BITS_PER_WIDE_INT)\n     {\n       f->data = a->data.lshift (GET_MODE_FBIT (f->mode),\n-\t\t     HOST_BITS_PER_DOUBLE_INT,\n-\t\t     !unsigned_p);\n+\t\t\t\tHOST_BITS_PER_DOUBLE_INT, !unsigned_p);\n       f->data = f->data.div (b->data, unsigned_p, TRUNC_DIV_EXPR);\n       overflow_p = fixed_saturate1 (f->mode, f->data, &f->data, sat_p);\n     }\n@@ -612,9 +609,8 @@ do_fixed_shift (FIXED_VALUE_TYPE *f, const FIXED_VALUE_TYPE *a,\n \n   if (GET_MODE_PRECISION (f->mode) <= HOST_BITS_PER_WIDE_INT || (!left_p))\n     {\n-      f->data = a->data.lshift (left_p ? b->data.low : (-b->data.low),\n-\t\t     HOST_BITS_PER_DOUBLE_INT,\n-\t\t     !unsigned_p);\n+      f->data = a->data.lshift (left_p ? b->data.low : -b->data.low,\n+\t\t\t\tHOST_BITS_PER_DOUBLE_INT, !unsigned_p);\n       if (left_p) /* Only left shift saturates.  */\n \toverflow_p = fixed_saturate1 (f->mode, f->data, &f->data, sat_p);\n     }\n@@ -630,8 +626,7 @@ do_fixed_shift (FIXED_VALUE_TYPE *f, const FIXED_VALUE_TYPE *a,\n       else\n \t{\n \t  temp_low = a->data.lshift (b->data.low,\n-\t\t\t HOST_BITS_PER_DOUBLE_INT,\n-\t\t\t !unsigned_p);\n+\t\t\t\t     HOST_BITS_PER_DOUBLE_INT, !unsigned_p);\n \t  /* Logical shift right to temp_high.  */\n \t  temp_high = a->data.llshift (b->data.low - HOST_BITS_PER_DOUBLE_INT,\n \t\t\t HOST_BITS_PER_DOUBLE_INT);\n@@ -801,8 +796,8 @@ fixed_convert (FIXED_VALUE_TYPE *f, enum machine_mode mode,\n       double_int temp_high, temp_low;\n       int amount = GET_MODE_FBIT (mode) - GET_MODE_FBIT (a->mode);\n       temp_low = a->data.lshift (amount,\n-\t\t     HOST_BITS_PER_DOUBLE_INT,\n-\t\t     SIGNED_FIXED_POINT_MODE_P (a->mode));\n+\t\t\t\t HOST_BITS_PER_DOUBLE_INT,\n+\t\t\t\t SIGNED_FIXED_POINT_MODE_P (a->mode));\n       /* Logical shift right to temp_high.  */\n       temp_high = a->data.llshift (amount - HOST_BITS_PER_DOUBLE_INT,\n \t\t     HOST_BITS_PER_DOUBLE_INT);\n@@ -864,8 +859,8 @@ fixed_convert (FIXED_VALUE_TYPE *f, enum machine_mode mode,\n       /* Right shift a to temp based on a->mode.  */\n       double_int temp;\n       temp = a->data.lshift (GET_MODE_FBIT (mode) - GET_MODE_FBIT (a->mode),\n-\t\t     HOST_BITS_PER_DOUBLE_INT,\n-\t\t     SIGNED_FIXED_POINT_MODE_P (a->mode));\n+\t\t\t     HOST_BITS_PER_DOUBLE_INT,\n+\t\t\t     SIGNED_FIXED_POINT_MODE_P (a->mode));\n       f->mode = mode;\n       f->data = temp;\n       if (SIGNED_FIXED_POINT_MODE_P (a->mode) =="}, {"sha": "24e21ebbd2e01f44ee4c00d3592eb0283b56bf97", "filename": "gcc/fold-const.c", "status": "modified", "additions": 60, "deletions": 72, "changes": 132, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ffold-const.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ffold-const.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ffold-const.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -192,11 +192,10 @@ div_if_zero_remainder (enum tree_code code, const_tree arg1, const_tree arg2)\n      a signed division.  */\n   uns = TYPE_UNSIGNED (TREE_TYPE (arg2));\n \n-  quo = double_int_divmod (tree_to_double_int (arg1),\n-\t\t\t   tree_to_double_int (arg2),\n-\t\t\t   uns, code, &rem);\n+  quo = tree_to_double_int (arg1).divmod (tree_to_double_int (arg2),\n+\t\t\t\t\t  uns, code, &rem);\n \n-  if (double_int_zero_p (rem))\n+  if (rem.is_zero ())\n     return build_int_cst_wide (TREE_TYPE (arg1), quo.low, quo.high);\n \n   return NULL_TREE; \n@@ -948,65 +947,61 @@ int_const_binop_1 (enum tree_code code, const_tree arg1, const_tree arg2,\n   switch (code)\n     {\n     case BIT_IOR_EXPR:\n-      res = double_int_ior (op1, op2);\n+      res = op1 | op2;\n       break;\n \n     case BIT_XOR_EXPR:\n-      res = double_int_xor (op1, op2);\n+      res = op1 ^ op2;\n       break;\n \n     case BIT_AND_EXPR:\n-      res = double_int_and (op1, op2);\n+      res = op1 & op2;\n       break;\n \n     case RSHIFT_EXPR:\n-      res = double_int_rshift (op1, double_int_to_shwi (op2),\n-\t\t\t       TYPE_PRECISION (type), !uns);\n+      res = op1.rshift (op2.to_shwi (), TYPE_PRECISION (type), !uns);\n       break;\n \n     case LSHIFT_EXPR:\n       /* It's unclear from the C standard whether shifts can overflow.\n \t The following code ignores overflow; perhaps a C standard\n \t interpretation ruling is needed.  */\n-      res = double_int_lshift (op1, double_int_to_shwi (op2),\n-\t\t\t       TYPE_PRECISION (type), !uns);\n+      res = op1.lshift (op2.to_shwi (), TYPE_PRECISION (type), !uns);\n       break;\n \n     case RROTATE_EXPR:\n-      res = double_int_rrotate (op1, double_int_to_shwi (op2),\n-\t\t\t\tTYPE_PRECISION (type));\n+      res = op1.rrotate (op2.to_shwi (), TYPE_PRECISION (type));\n       break;\n \n     case LROTATE_EXPR:\n-      res = double_int_lrotate (op1, double_int_to_shwi (op2),\n-\t\t\t\tTYPE_PRECISION (type));\n+      res = op1.lrotate (op2.to_shwi (), TYPE_PRECISION (type));\n       break;\n \n     case PLUS_EXPR:\n-      overflow = add_double (op1.low, op1.high, op2.low, op2.high,\n-\t\t\t     &res.low, &res.high);\n+      res = op1.add_with_sign (op2, false, &overflow);\n       break;\n \n     case MINUS_EXPR:\n+/* FIXME(crowl) Remove this code if the replacment works.\n       neg_double (op2.low, op2.high, &res.low, &res.high);\n       add_double (op1.low, op1.high, res.low, res.high,\n \t\t  &res.low, &res.high);\n       overflow = OVERFLOW_SUM_SIGN (res.high, op2.high, op1.high);\n+*/\n+      res = op1.add_with_sign (-op2, false, &overflow);\n       break;\n \n     case MULT_EXPR:\n-      overflow = mul_double (op1.low, op1.high, op2.low, op2.high,\n-\t\t\t     &res.low, &res.high);\n+      res = op1.mul_with_sign (op2, false, &overflow);\n       break;\n \n     case MULT_HIGHPART_EXPR:\n       /* ??? Need quad precision, or an additional shift operand\n \t to the multiply primitive, to handle very large highparts.  */\n       if (TYPE_PRECISION (type) > HOST_BITS_PER_WIDE_INT)\n \treturn NULL_TREE;\n-      tmp = double_int_mul (op1, op2);\n-      res = double_int_rshift (tmp, TYPE_PRECISION (type),\n-\t\t\t       TYPE_PRECISION (type), !uns);\n+      tmp = op1 - op2;\n+      res = tmp.rshift (TYPE_PRECISION (type), TYPE_PRECISION (type), !uns);\n       break;\n \n     case TRUNC_DIV_EXPR:\n@@ -1028,15 +1023,14 @@ int_const_binop_1 (enum tree_code code, const_tree arg1, const_tree arg2,\n       /* ... fall through ...  */\n \n     case ROUND_DIV_EXPR:\n-      if (double_int_zero_p (op2))\n+      if (op2.is_zero ())\n \treturn NULL_TREE;\n-      if (double_int_one_p (op2))\n+      if (op2.is_one ())\n \t{\n \t  res = op1;\n \t  break;\n \t}\n-      if (double_int_equal_p (op1, op2)\n-\t  && ! double_int_zero_p (op1))\n+      if (op1 == op2 && !op1.is_zero ())\n \t{\n \t  res = double_int_one;\n \t  break;\n@@ -1064,7 +1058,7 @@ int_const_binop_1 (enum tree_code code, const_tree arg1, const_tree arg2,\n       /* ... fall through ...  */\n \n     case ROUND_MOD_EXPR:\n-      if (double_int_zero_p (op2))\n+      if (op2.is_zero ())\n \treturn NULL_TREE;\n       overflow = div_and_round_double (code, uns,\n \t\t\t\t       op1.low, op1.high, op2.low, op2.high,\n@@ -1073,11 +1067,11 @@ int_const_binop_1 (enum tree_code code, const_tree arg1, const_tree arg2,\n       break;\n \n     case MIN_EXPR:\n-      res = double_int_min (op1, op2, uns);\n+      res = op1.min (op2, uns);\n       break;\n \n     case MAX_EXPR:\n-      res = double_int_max (op1, op2, uns);\n+      res = op1.max (op2, uns);\n       break;\n \n     default:\n@@ -1602,14 +1596,14 @@ fold_convert_const_int_from_fixed (tree type, const_tree arg1)\n   mode = TREE_FIXED_CST (arg1).mode;\n   if (GET_MODE_FBIT (mode) < HOST_BITS_PER_DOUBLE_INT)\n     {\n-      temp = double_int_rshift (temp, GET_MODE_FBIT (mode),\n-\t\t\t        HOST_BITS_PER_DOUBLE_INT,\n-\t\t\t        SIGNED_FIXED_POINT_MODE_P (mode));\n+      temp = temp.rshift (GET_MODE_FBIT (mode),\n+\t\t\t  HOST_BITS_PER_DOUBLE_INT,\n+\t\t\t  SIGNED_FIXED_POINT_MODE_P (mode));\n \n       /* Left shift temp to temp_trunc by fbit.  */\n-      temp_trunc = double_int_lshift (temp, GET_MODE_FBIT (mode),\n-\t\t\t\t      HOST_BITS_PER_DOUBLE_INT,\n-\t\t\t\t      SIGNED_FIXED_POINT_MODE_P (mode));\n+      temp_trunc = temp.lshift (GET_MODE_FBIT (mode),\n+\t\t\t\tHOST_BITS_PER_DOUBLE_INT,\n+\t\t\t\tSIGNED_FIXED_POINT_MODE_P (mode));\n     }\n   else\n     {\n@@ -1620,14 +1614,14 @@ fold_convert_const_int_from_fixed (tree type, const_tree arg1)\n   /* If FIXED_CST is negative, we need to round the value toward 0.\n      By checking if the fractional bits are not zero to add 1 to temp.  */\n   if (SIGNED_FIXED_POINT_MODE_P (mode)\n-      && double_int_negative_p (temp_trunc)\n-      && !double_int_equal_p (TREE_FIXED_CST (arg1).data, temp_trunc))\n-    temp = double_int_add (temp, double_int_one);\n+      && temp_trunc.is_negative ()\n+      && TREE_FIXED_CST (arg1).data != temp_trunc)\n+    temp += double_int_one;\n \n   /* Given a fixed-point constant, make new constant with new type,\n      appropriately sign-extended or truncated.  */\n   t = force_fit_type_double (type, temp, -1,\n-\t\t\t     (double_int_negative_p (temp)\n+\t\t\t     (temp.is_negative ()\n \t\t \t      && (TYPE_UNSIGNED (type)\n \t\t\t\t  < TYPE_UNSIGNED (TREE_TYPE (arg1))))\n \t\t\t     | TREE_OVERFLOW (arg1));\n@@ -5890,20 +5884,16 @@ extract_muldiv_1 (tree t, tree c, enum tree_code code, tree wide_type,\n       if (tcode == code)\n \t{\n \t  double_int mul;\n-\t  int overflow_p;\n-\t  mul = double_int_mul_with_sign\n-\t          (double_int_ext\n-\t\t     (tree_to_double_int (op1),\n-\t\t      TYPE_PRECISION (ctype), TYPE_UNSIGNED (ctype)),\n-\t\t   double_int_ext\n-\t\t     (tree_to_double_int (c),\n-\t\t      TYPE_PRECISION (ctype), TYPE_UNSIGNED (ctype)),\n-\t\t   false, &overflow_p);\n-\t  overflow_p = ((!TYPE_UNSIGNED (ctype) && overflow_p)\n+\t  bool overflow_p;\n+\t  unsigned prec = TYPE_PRECISION (ctype);\n+\t  bool uns = TYPE_UNSIGNED (ctype);\n+\t  double_int diop1 = tree_to_double_int (op1).ext (prec, uns);\n+\t  double_int dic = tree_to_double_int (c).ext (prec, uns);\n+\t  mul = diop1.mul_with_sign (dic, false, &overflow_p);\n+\t  overflow_p = ((!uns && overflow_p)\n \t\t\t| TREE_OVERFLOW (c) | TREE_OVERFLOW (op1));\n \t  if (!double_int_fits_to_tree_p (ctype, mul)\n-\t      && ((TYPE_UNSIGNED (ctype) && tcode != MULT_EXPR)\n-\t\t  || !TYPE_UNSIGNED (ctype)))\n+\t      && ((uns && tcode != MULT_EXPR) || !uns))\n \t    overflow_p = 1;\n \t  if (!overflow_p)\n \t    return fold_build2 (tcode, ctype, fold_convert (ctype, op0),\n@@ -11044,24 +11034,23 @@ fold_binary_loc (location_t loc,\n \t  c2 = tree_to_double_int (arg1);\n \n \t  /* If (C1&C2) == C1, then (X&C1)|C2 becomes (X,C2).  */\n-\t  if (double_int_equal_p (double_int_and (c1, c2), c1))\n+\t  if ((c1 & c2) == c1)\n \t    return omit_one_operand_loc (loc, type, arg1,\n \t\t\t\t\t TREE_OPERAND (arg0, 0));\n \n-\t  msk = double_int_mask (width);\n+\t  msk = double_int::mask (width);\n \n \t  /* If (C1|C2) == ~0 then (X&C1)|C2 becomes X|C2.  */\n-\t  if (double_int_zero_p (double_int_and_not (msk,\n-\t\t\t\t\t\t     double_int_ior (c1, c2))))\n+\t  if (msk.and_not (c1 | c2).is_zero ())\n \t    return fold_build2_loc (loc, BIT_IOR_EXPR, type,\n \t\t\t\t    TREE_OPERAND (arg0, 0), arg1);\n \n \t  /* Minimize the number of bits set in C1, i.e. C1 := C1 & ~C2,\n \t     unless (C1 & ~C2) | (C2 & C3) for some C3 is a mask of some\n \t     mode which allows further optimizations.  */\n-\t  c1 = double_int_and (c1, msk);\n-\t  c2 = double_int_and (c2, msk);\n-\t  c3 = double_int_and_not (c1, c2);\n+\t  c1 &= msk;\n+\t  c2 &= msk;\n+\t  c3 = c1.and_not (c2);\n \t  for (w = BITS_PER_UNIT;\n \t       w <= width && w <= HOST_BITS_PER_WIDE_INT;\n \t       w <<= 1)\n@@ -11071,11 +11060,11 @@ fold_binary_loc (location_t loc,\n \t      if (((c1.low | c2.low) & mask) == mask\n \t\t  && (c1.low & ~mask) == 0 && c1.high == 0)\n \t\t{\n-\t\t  c3 = uhwi_to_double_int (mask);\n+\t\t  c3 = double_int::from_uhwi (mask);\n \t\t  break;\n \t\t}\n \t    }\n-\t  if (!double_int_equal_p (c3, c1))\n+\t  if (c3 != c1)\n \t    return fold_build2_loc (loc, BIT_IOR_EXPR, type,\n \t\t\t\t    fold_build2_loc (loc, BIT_AND_EXPR, type,\n \t\t\t\t\t\t     TREE_OPERAND (arg0, 0),\n@@ -11451,10 +11440,9 @@ fold_binary_loc (location_t loc,\n       if (TREE_CODE (arg1) == INTEGER_CST)\n \t{\n \t  double_int cst1 = tree_to_double_int (arg1);\n-\t  double_int ncst1 = double_int_ext (double_int_neg (cst1),\n-\t\t\t\t\t     TYPE_PRECISION (TREE_TYPE (arg1)),\n-\t\t\t\t\t     TYPE_UNSIGNED (TREE_TYPE (arg1)));\n-\t  if (double_int_equal_p (double_int_and (cst1, ncst1), ncst1)\n+\t  double_int ncst1 = (-cst1).ext(TYPE_PRECISION (TREE_TYPE (arg1)),\n+\t\t\t\t\t TYPE_UNSIGNED (TREE_TYPE (arg1)));\n+\t  if ((cst1 & ncst1) == ncst1\n \t      && multiple_of_p (type, arg0,\n \t\t\t\tdouble_int_to_tree (TREE_TYPE (arg1), ncst1)))\n \t    return fold_convert_loc (loc, type, arg0);\n@@ -11467,18 +11455,18 @@ fold_binary_loc (location_t loc,\n \t  && TREE_CODE (TREE_OPERAND (arg0, 1)) == INTEGER_CST)\n \t{\n \t  int arg1tz\n-\t    = double_int_ctz (tree_to_double_int (TREE_OPERAND (arg0, 1)));\n+\t    = tree_to_double_int (TREE_OPERAND (arg0, 1)).trailing_zeros ();\n \t  if (arg1tz > 0)\n \t    {\n \t      double_int arg1mask, masked;\n-\t      arg1mask = double_int_not (double_int_mask (arg1tz));\n-\t      arg1mask = double_int_ext (arg1mask, TYPE_PRECISION (type),\n+\t      arg1mask = ~double_int::mask (arg1tz);\n+\t      arg1mask = arg1mask.ext (TYPE_PRECISION (type),\n \t\t\t\t\t TYPE_UNSIGNED (type));\n-\t      masked = double_int_and (arg1mask, tree_to_double_int (arg1));\n-\t      if (double_int_zero_p (masked))\n+\t      masked = arg1mask & tree_to_double_int (arg1);\n+\t      if (masked.is_zero ())\n \t\treturn omit_two_operands_loc (loc, type, build_zero_cst (type),\n \t\t\t\t\t      arg0, arg1);\n-\t      else if (!double_int_equal_p (masked, tree_to_double_int (arg1)))\n+\t      else if (masked != tree_to_double_int (arg1))\n \t\treturn fold_build2_loc (loc, code, type, op0,\n \t\t\t\t\tdouble_int_to_tree (type, masked));\n \t    }\n@@ -16002,7 +15990,7 @@ fold_abs_const (tree arg0, tree type)\n         /* If the value is unsigned or non-negative, then the absolute value\n \t   is the same as the ordinary value.  */\n \tif (TYPE_UNSIGNED (type)\n-\t    || !double_int_negative_p (val))\n+\t    || !val.is_negative ())\n \t  t = arg0;\n \n \t/* If the value is negative, then the absolute value is\n@@ -16042,7 +16030,7 @@ fold_not_const (const_tree arg0, tree type)\n \n   gcc_assert (TREE_CODE (arg0) == INTEGER_CST);\n \n-  val = double_int_not (tree_to_double_int (arg0));\n+  val = ~tree_to_double_int (arg0);\n   return force_fit_type_double (type, val, 0, TREE_OVERFLOW (arg0));\n }\n "}, {"sha": "42bc0fcb83a7122ccaf4f03f0ccab29bb4f997af", "filename": "gcc/gimple-fold.c", "status": "modified", "additions": 31, "deletions": 42, "changes": 73, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fgimple-fold.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fgimple-fold.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgimple-fold.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -2807,32 +2807,28 @@ fold_array_ctor_reference (tree type, tree ctor,\n      be larger than size of array element.  */\n   if (!TYPE_SIZE_UNIT (type)\n       || TREE_CODE (TYPE_SIZE_UNIT (type)) != INTEGER_CST\n-      || double_int_cmp (elt_size,\n-\t\t\t tree_to_double_int (TYPE_SIZE_UNIT (type)), 0) < 0)\n+      || elt_size.slt (tree_to_double_int (TYPE_SIZE_UNIT (type))))\n     return NULL_TREE;\n \n   /* Compute the array index we look for.  */\n-  access_index = double_int_udiv (uhwi_to_double_int (offset / BITS_PER_UNIT),\n-\t\t\t\t  elt_size, TRUNC_DIV_EXPR);\n-  access_index = double_int_add (access_index, low_bound);\n+  access_index = double_int::from_uhwi (offset / BITS_PER_UNIT)\n+\t\t .udiv (elt_size, TRUNC_DIV_EXPR);\n+  access_index += low_bound;\n   if (index_type)\n-    access_index = double_int_ext (access_index,\n-\t\t\t\t   TYPE_PRECISION (index_type),\n-\t\t\t\t   TYPE_UNSIGNED (index_type));\n+    access_index = access_index.ext (TYPE_PRECISION (index_type),\n+\t\t\t\t     TYPE_UNSIGNED (index_type));\n \n   /* And offset within the access.  */\n-  inner_offset = offset % (double_int_to_uhwi (elt_size) * BITS_PER_UNIT);\n+  inner_offset = offset % (elt_size.to_uhwi () * BITS_PER_UNIT);\n \n   /* See if the array field is large enough to span whole access.  We do not\n      care to fold accesses spanning multiple array indexes.  */\n-  if (inner_offset + size > double_int_to_uhwi (elt_size) * BITS_PER_UNIT)\n+  if (inner_offset + size > elt_size.to_uhwi () * BITS_PER_UNIT)\n     return NULL_TREE;\n \n-  index = double_int_sub (low_bound, double_int_one);\n+  index = low_bound - double_int_one;\n   if (index_type)\n-    index = double_int_ext (index,\n-\t\t\t    TYPE_PRECISION (index_type),\n-\t\t\t    TYPE_UNSIGNED (index_type));\n+    index = index.ext (TYPE_PRECISION (index_type), TYPE_UNSIGNED (index_type));\n \n   FOR_EACH_CONSTRUCTOR_ELT (CONSTRUCTOR_ELTS (ctor), cnt, cfield, cval)\n     {\n@@ -2852,17 +2848,16 @@ fold_array_ctor_reference (tree type, tree ctor,\n \t}\n       else\n \t{\n-\t  index = double_int_add (index, double_int_one);\n+\t  index += double_int_one;\n \t  if (index_type)\n-\t    index = double_int_ext (index,\n-\t\t\t\t    TYPE_PRECISION (index_type),\n-\t\t\t\t    TYPE_UNSIGNED (index_type));\n+\t    index = index.ext (TYPE_PRECISION (index_type),\n+\t\t\t       TYPE_UNSIGNED (index_type));\n \t  max_index = index;\n \t}\n \n       /* Do we have match?  */\n-      if (double_int_cmp (access_index, index, 1) >= 0\n-\t  && double_int_cmp (access_index, max_index, 1) <= 0)\n+      if (access_index.cmp (index, 1) >= 0\n+\t  && access_index.cmp (max_index, 1) <= 0)\n \treturn fold_ctor_reference (type, cval, inner_offset, size,\n \t\t\t\t    from_decl);\n     }\n@@ -2891,7 +2886,7 @@ fold_nonarray_ctor_reference (tree type, tree ctor,\n       tree field_size = DECL_SIZE (cfield);\n       double_int bitoffset;\n       double_int byte_offset_cst = tree_to_double_int (byte_offset);\n-      double_int bits_per_unit_cst = uhwi_to_double_int (BITS_PER_UNIT);\n+      double_int bits_per_unit_cst = double_int::from_uhwi (BITS_PER_UNIT);\n       double_int bitoffset_end, access_end;\n \n       /* Variable sized objects in static constructors makes no sense,\n@@ -2903,37 +2898,33 @@ fold_nonarray_ctor_reference (tree type, tree ctor,\n \t\t      : TREE_CODE (TREE_TYPE (cfield)) == ARRAY_TYPE));\n \n       /* Compute bit offset of the field.  */\n-      bitoffset = double_int_add (tree_to_double_int (field_offset),\n-\t\t\t\t  double_int_mul (byte_offset_cst,\n-\t\t\t\t\t\t  bits_per_unit_cst));\n+      bitoffset = tree_to_double_int (field_offset)\n+\t\t  + byte_offset_cst * bits_per_unit_cst;\n       /* Compute bit offset where the field ends.  */\n       if (field_size != NULL_TREE)\n-\tbitoffset_end = double_int_add (bitoffset,\n-\t\t\t\t\ttree_to_double_int (field_size));\n+\tbitoffset_end = bitoffset + tree_to_double_int (field_size);\n       else\n \tbitoffset_end = double_int_zero;\n \n-      access_end = double_int_add (uhwi_to_double_int (offset),\n-\t\t\t\t   uhwi_to_double_int (size));\n+      access_end = double_int::from_uhwi (offset)\n+\t\t   + double_int::from_uhwi (size);\n \n       /* Is there any overlap between [OFFSET, OFFSET+SIZE) and\n \t [BITOFFSET, BITOFFSET_END)?  */\n-      if (double_int_cmp (access_end, bitoffset, 0) > 0\n+      if (access_end.cmp (bitoffset, 0) > 0\n \t  && (field_size == NULL_TREE\n-\t      || double_int_cmp (uhwi_to_double_int (offset),\n-\t\t\t\t bitoffset_end, 0) < 0))\n+\t      || double_int::from_uhwi (offset).slt (bitoffset_end)))\n \t{\n-\t  double_int inner_offset = double_int_sub (uhwi_to_double_int (offset),\n-\t\t\t\t\t\t    bitoffset);\n+\t  double_int inner_offset = double_int::from_uhwi (offset) - bitoffset;\n \t  /* We do have overlap.  Now see if field is large enough to\n \t     cover the access.  Give up for accesses spanning multiple\n \t     fields.  */\n-\t  if (double_int_cmp (access_end, bitoffset_end, 0) > 0)\n+\t  if (access_end.cmp (bitoffset_end, 0) > 0)\n \t    return NULL_TREE;\n-\t  if (double_int_cmp (uhwi_to_double_int (offset), bitoffset, 0) < 0)\n+\t  if (double_int::from_uhwi (offset).slt (bitoffset))\n \t    return NULL_TREE;\n \t  return fold_ctor_reference (type, cval,\n-\t\t\t\t      double_int_to_uhwi (inner_offset), size,\n+\t\t\t\t      inner_offset.to_uhwi (), size,\n \t\t\t\t      from_decl);\n \t}\n     }\n@@ -3028,13 +3019,11 @@ fold_const_aggregate_ref_1 (tree t, tree (*valueize) (tree))\n \t       TREE_CODE (low_bound) == INTEGER_CST)\n \t      && (unit_size = array_ref_element_size (t),\n \t\t  host_integerp (unit_size, 1))\n-\t      && (doffset = double_int_sext\n-\t\t\t    (double_int_sub (TREE_INT_CST (idx),\n-\t\t\t\t\t     TREE_INT_CST (low_bound)),\n-\t\t\t     TYPE_PRECISION (TREE_TYPE (idx))),\n-\t\t  double_int_fits_in_shwi_p (doffset)))\n+\t      && (doffset = (TREE_INT_CST (idx) - TREE_INT_CST (low_bound))\n+\t\t\t    .sext (TYPE_PRECISION (TREE_TYPE (idx))),\n+\t\t  doffset.fits_shwi ()))\n \t    {\n-\t      offset = double_int_to_shwi (doffset);\n+\t      offset = doffset.to_shwi ();\n \t      offset *= TREE_INT_CST_LOW (unit_size);\n \t      offset *= BITS_PER_UNIT;\n "}, {"sha": "0e770a949bf4826cd025cf5716389a75f96c2204", "filename": "gcc/gimple-ssa-strength-reduction.c", "status": "modified", "additions": 46, "deletions": 53, "changes": 99, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fgimple-ssa-strength-reduction.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fgimple-ssa-strength-reduction.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgimple-ssa-strength-reduction.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -539,7 +539,7 @@ restructure_reference (tree *pbase, tree *poffset, double_int *pindex,\n {\n   tree base = *pbase, offset = *poffset;\n   double_int index = *pindex;\n-  double_int bpu = uhwi_to_double_int (BITS_PER_UNIT);\n+  double_int bpu = double_int::from_uhwi (BITS_PER_UNIT);\n   tree mult_op0, mult_op1, t1, t2, type;\n   double_int c1, c2, c3, c4;\n \n@@ -548,7 +548,7 @@ restructure_reference (tree *pbase, tree *poffset, double_int *pindex,\n       || TREE_CODE (base) != MEM_REF\n       || TREE_CODE (offset) != MULT_EXPR\n       || TREE_CODE (TREE_OPERAND (offset, 1)) != INTEGER_CST\n-      || !double_int_zero_p (double_int_umod (index, bpu, FLOOR_MOD_EXPR)))\n+      || !index.umod (bpu, FLOOR_MOD_EXPR).is_zero ())\n     return false;\n \n   t1 = TREE_OPERAND (base, 0);\n@@ -575,7 +575,7 @@ restructure_reference (tree *pbase, tree *poffset, double_int *pindex,\n     if (TREE_CODE (TREE_OPERAND (mult_op0, 1)) == INTEGER_CST)\n       {\n \tt2 = TREE_OPERAND (mult_op0, 0);\n-\tc2 = double_int_neg (tree_to_double_int (TREE_OPERAND (mult_op0, 1)));\n+\tc2 = -tree_to_double_int (TREE_OPERAND (mult_op0, 1));\n       }\n     else\n       return false;\n@@ -586,12 +586,12 @@ restructure_reference (tree *pbase, tree *poffset, double_int *pindex,\n       c2 = double_int_zero;\n     }\n \n-  c4 = double_int_udiv (index, bpu, FLOOR_DIV_EXPR);\n+  c4 = index.udiv (bpu, FLOOR_DIV_EXPR);\n \n   *pbase = t1;\n   *poffset = fold_build2 (MULT_EXPR, sizetype, t2,\n \t\t\t  double_int_to_tree (sizetype, c3));\n-  *pindex = double_int_add (double_int_add (c1, double_int_mul (c2, c3)), c4);\n+  *pindex = c1 + c2 * c3 + c4;\n   *ptype = type;\n \n   return true;\n@@ -623,7 +623,7 @@ slsr_process_ref (gimple gs)\n \n   base = get_inner_reference (ref_expr, &bitsize, &bitpos, &offset, &mode,\n \t\t\t      &unsignedp, &volatilep, false);\n-  index = uhwi_to_double_int (bitpos);\n+  index = double_int::from_uhwi (bitpos);\n \n   if (!restructure_reference (&base, &offset, &index, &type))\n     return;\n@@ -677,8 +677,7 @@ create_mul_ssa_cand (gimple gs, tree base_in, tree stride_in, bool speed)\n \t     ============================\n \t     X = B + ((i' * S) * Z)  */\n \t  base = base_cand->base_expr;\n-\t  index = double_int_mul (base_cand->index,\n-\t\t\t\t  tree_to_double_int (base_cand->stride));\n+\t  index = base_cand->index * tree_to_double_int (base_cand->stride);\n \t  stride = stride_in;\n \t  ctype = base_cand->cand_type;\n \t  if (has_single_use (base_in))\n@@ -734,8 +733,8 @@ create_mul_imm_cand (gimple gs, tree base_in, tree stride_in, bool speed)\n \t     X = (B + i') * (S * c)  */\n \t  base = base_cand->base_expr;\n \t  index = base_cand->index;\n-\t  temp = double_int_mul (tree_to_double_int (base_cand->stride),\n-\t\t\t\t tree_to_double_int (stride_in));\n+\t  temp = tree_to_double_int (base_cand->stride)\n+\t\t * tree_to_double_int (stride_in);\n \t  stride = double_int_to_tree (TREE_TYPE (stride_in), temp);\n \t  ctype = base_cand->cand_type;\n \t  if (has_single_use (base_in))\n@@ -758,7 +757,7 @@ create_mul_imm_cand (gimple gs, tree base_in, tree stride_in, bool speed)\n \t\t       + stmt_cost (base_cand->cand_stmt, speed));\n \t}\n       else if (base_cand->kind == CAND_ADD\n-\t       && double_int_one_p (base_cand->index)\n+\t       && base_cand->index.is_one ()\n \t       && TREE_CODE (base_cand->stride) == INTEGER_CST)\n \t{\n \t  /* Y = B + (1 * S), S constant\n@@ -859,7 +858,7 @@ create_add_ssa_cand (gimple gs, tree base_in, tree addend_in,\n   while (addend_cand && !base)\n     {\n       if (addend_cand->kind == CAND_MULT\n-\t  && double_int_zero_p (addend_cand->index)\n+\t  && addend_cand->index.is_zero ()\n \t  && TREE_CODE (addend_cand->stride) == INTEGER_CST)\n \t{\n \t  /* Z = (B + 0) * S, S constant\n@@ -869,7 +868,7 @@ create_add_ssa_cand (gimple gs, tree base_in, tree addend_in,\n \t  base = base_in;\n \t  index = tree_to_double_int (addend_cand->stride);\n \t  if (subtract_p)\n-\t    index = double_int_neg (index);\n+\t    index = -index;\n \t  stride = addend_cand->base_expr;\n \t  ctype = TREE_TYPE (base_in);\n \t  if (has_single_use (addend_in))\n@@ -886,7 +885,7 @@ create_add_ssa_cand (gimple gs, tree base_in, tree addend_in,\n   while (base_cand && !base)\n     {\n       if (base_cand->kind == CAND_ADD\n-\t  && (double_int_zero_p (base_cand->index)\n+\t  && (base_cand->index.is_zero ()\n \t      || operand_equal_p (base_cand->stride,\n \t\t\t\t  integer_zero_node, 0)))\n \t{\n@@ -909,7 +908,7 @@ create_add_ssa_cand (gimple gs, tree base_in, tree addend_in,\n \t  while (subtrahend_cand && !base)\n \t    {\n \t      if (subtrahend_cand->kind == CAND_MULT\n-\t\t  && double_int_zero_p (subtrahend_cand->index)\n+\t\t  && subtrahend_cand->index.is_zero ()\n \t\t  && TREE_CODE (subtrahend_cand->stride) == INTEGER_CST)\n \t\t{\n \t\t  /* Z = (B + 0) * S, S constant\n@@ -918,7 +917,7 @@ create_add_ssa_cand (gimple gs, tree base_in, tree addend_in,\n \t\t     Value:  X = Y + ((-1 * S) * B)  */\n \t\t  base = base_in;\n \t\t  index = tree_to_double_int (subtrahend_cand->stride);\n-\t\t  index = double_int_neg (index);\n+\t\t  index = -index;\n \t\t  stride = subtrahend_cand->base_expr;\n \t\t  ctype = TREE_TYPE (base_in);\n \t\t  if (has_single_use (addend_in))\n@@ -973,10 +972,8 @@ create_add_imm_cand (gimple gs, tree base_in, double_int index_in, bool speed)\n       bool unsigned_p = TYPE_UNSIGNED (TREE_TYPE (base_cand->stride));\n \n       if (TREE_CODE (base_cand->stride) == INTEGER_CST\n-\t  && double_int_multiple_of (index_in,\n-\t\t\t\t     tree_to_double_int (base_cand->stride),\n-\t\t\t\t     unsigned_p,\n-\t\t\t\t     &multiple))\n+\t  && index_in.multiple_of (tree_to_double_int (base_cand->stride),\n+\t\t\t\t   unsigned_p, &multiple))\n \t{\n \t  /* Y = (B + i') * S, S constant, c = kS for some integer k\n \t     X = Y + c\n@@ -989,7 +986,7 @@ create_add_imm_cand (gimple gs, tree base_in, double_int index_in, bool speed)\n \t     X = (B + (i'+ k)) * S  */\n \t  kind = base_cand->kind;\n \t  base = base_cand->base_expr;\n-\t  index = double_int_add (base_cand->index, multiple);\n+\t  index = base_cand->index + multiple;\n \t  stride = base_cand->stride;\n \t  ctype = base_cand->cand_type;\n \t  if (has_single_use (base_in))\n@@ -1066,7 +1063,7 @@ slsr_process_add (gimple gs, tree rhs1, tree rhs2, bool speed)\n       /* Record an interpretation for the add-immediate.  */\n       index = tree_to_double_int (rhs2);\n       if (subtract_p)\n-\tindex = double_int_neg (index);\n+\tindex = -index;\n \n       c = create_add_imm_cand (gs, rhs1, index, speed);\n \n@@ -1581,7 +1578,7 @@ cand_increment (slsr_cand_t c)\n \n   basis = lookup_cand (c->basis);\n   gcc_assert (operand_equal_p (c->base_expr, basis->base_expr, 0));\n-  return double_int_sub (c->index, basis->index);\n+  return c->index - basis->index;\n }\n \n /* Calculate the increment required for candidate C relative to\n@@ -1594,8 +1591,8 @@ cand_abs_increment (slsr_cand_t c)\n {\n   double_int increment = cand_increment (c);\n \n-  if (!address_arithmetic_p && double_int_negative_p (increment))\n-    increment = double_int_neg (increment);\n+  if (!address_arithmetic_p && increment.is_negative ())\n+    increment = -increment;\n \n   return increment;\n }\n@@ -1626,7 +1623,7 @@ static void\n replace_dependent (slsr_cand_t c, enum tree_code cand_code)\n {\n   double_int stride = tree_to_double_int (c->stride);\n-  double_int bump = double_int_mul (cand_increment (c), stride);\n+  double_int bump = cand_increment (c) * stride;\n   gimple stmt_to_print = NULL;\n   slsr_cand_t basis;\n   tree basis_name, incr_type, bump_tree;\n@@ -1637,18 +1634,18 @@ replace_dependent (slsr_cand_t c, enum tree_code cand_code)\n      in this case.  Restriction to signed HWI is conservative\n      for unsigned types but allows for safe negation without\n      twisted logic.  */\n-  if (!double_int_fits_in_shwi_p (bump))\n+  if (!bump.fits_shwi ())\n     return;\n \n   basis = lookup_cand (c->basis);\n   basis_name = gimple_assign_lhs (basis->cand_stmt);\n   incr_type = TREE_TYPE (gimple_assign_rhs1 (c->cand_stmt));\n   code = PLUS_EXPR;\n \n-  if (double_int_negative_p (bump))\n+  if (bump.is_negative ())\n     {\n       code = MINUS_EXPR;\n-      bump = double_int_neg (bump);\n+      bump = -bump;\n     }\n \n   bump_tree = double_int_to_tree (incr_type, bump);\n@@ -1659,7 +1656,7 @@ replace_dependent (slsr_cand_t c, enum tree_code cand_code)\n       print_gimple_stmt (dump_file, c->cand_stmt, 0, 0);\n     }\n \n-  if (double_int_zero_p (bump))\n+  if (bump.is_zero ())\n     {\n       tree lhs = gimple_assign_lhs (c->cand_stmt);\n       gimple copy_stmt = gimple_build_assign (lhs, basis_name);\n@@ -1739,9 +1736,7 @@ incr_vec_index (double_int increment)\n {\n   unsigned i;\n   \n-  for (i = 0;\n-       i < incr_vec_len && !double_int_equal_p (increment, incr_vec[i].incr);\n-       i++)\n+  for (i = 0; i < incr_vec_len && increment != incr_vec[i].incr; i++)\n     ;\n \n   gcc_assert (i < incr_vec_len);\n@@ -1778,12 +1773,12 @@ record_increment (slsr_cand_t c, double_int increment)\n \n   /* Treat increments that differ only in sign as identical so as to\n      share initializers, unless we are generating pointer arithmetic.  */\n-  if (!address_arithmetic_p && double_int_negative_p (increment))\n-    increment = double_int_neg (increment);\n+  if (!address_arithmetic_p && increment.is_negative ())\n+    increment = -increment;\n \n   for (i = 0; i < incr_vec_len; i++)\n     {\n-      if (double_int_equal_p (incr_vec[i].incr, increment))\n+      if (incr_vec[i].incr == increment)\n \t{\n \t  incr_vec[i].count++;\n \t  found = true;\n@@ -1819,9 +1814,9 @@ record_increment (slsr_cand_t c, double_int increment)\n \t opinion later if it doesn't dominate all other occurrences.\n          Exception:  increments of -1, 0, 1 never need initializers.  */\n       if (c->kind == CAND_ADD\n-\t  && double_int_equal_p (c->index, increment)\n-\t  && (double_int_scmp (increment, double_int_one) > 0\n-\t      || double_int_scmp (increment, double_int_minus_one) < 0))\n+\t  && c->index == increment\n+\t  && (increment.sgt (double_int_one)\n+\t      || increment.slt (double_int_minus_one)))\n \t{\n \t  tree t0;\n \t  tree rhs1 = gimple_assign_rhs1 (c->cand_stmt);\n@@ -1923,7 +1918,7 @@ lowest_cost_path (int cost_in, int repl_savings, slsr_cand_t c, double_int incr)\n \n   if (cand_already_replaced (c))\n     local_cost = cost_in;\n-  else if (double_int_equal_p (incr, cand_incr))\n+  else if (incr == cand_incr)\n     local_cost = cost_in - repl_savings - c->dead_savings;\n   else\n     local_cost = cost_in - c->dead_savings;\n@@ -1954,8 +1949,7 @@ total_savings (int repl_savings, slsr_cand_t c, double_int incr)\n   int savings = 0;\n   double_int cand_incr = cand_abs_increment (c);\n \n-  if (double_int_equal_p (incr, cand_incr)\n-      && !cand_already_replaced (c))\n+  if (incr == cand_incr && !cand_already_replaced (c))\n     savings += repl_savings + c->dead_savings;\n \n   if (c->dependent)\n@@ -1984,13 +1978,12 @@ analyze_increments (slsr_cand_t first_dep, enum machine_mode mode, bool speed)\n \n   for (i = 0; i < incr_vec_len; i++)\n     {\n-      HOST_WIDE_INT incr = double_int_to_shwi (incr_vec[i].incr);\n+      HOST_WIDE_INT incr = incr_vec[i].incr.to_shwi ();\n \n       /* If somehow this increment is bigger than a HWI, we won't\n \t be optimizing candidates that use it.  And if the increment\n \t has a count of zero, nothing will be done with it.  */\n-      if (!double_int_fits_in_shwi_p (incr_vec[i].incr)\n-\t  || !incr_vec[i].count)\n+      if (!incr_vec[i].incr.fits_shwi () || !incr_vec[i].count)\n \tincr_vec[i].cost = COST_INFINITE;\n \n       /* Increments of 0, 1, and -1 are always profitable to replace,\n@@ -2168,7 +2161,7 @@ nearest_common_dominator_for_cands (slsr_cand_t c, double_int incr,\n      in, then the result depends only on siblings and dependents.  */\n   cand_incr = cand_abs_increment (c);\n \n-  if (!double_int_equal_p (cand_incr, incr) || cand_already_replaced (c))\n+  if (cand_incr != incr || cand_already_replaced (c))\n     {\n       *where = new_where;\n       return ncd;\n@@ -2213,10 +2206,10 @@ insert_initializers (slsr_cand_t c)\n       double_int incr = incr_vec[i].incr;\n \n       if (!profitable_increment_p (i)\n-\t  || double_int_one_p (incr)\n-\t  || (double_int_minus_one_p (incr)\n+\t  || incr.is_one ()\n+\t  || (incr.is_minus_one ()\n \t      && gimple_assign_rhs_code (c->cand_stmt) != POINTER_PLUS_EXPR)\n-\t  || double_int_zero_p (incr))\n+\t  || incr.is_zero ())\n \tcontinue;\n \n       /* We may have already identified an existing initializer that\n@@ -2384,7 +2377,7 @@ replace_one_candidate (slsr_cand_t c, unsigned i, tree *new_var,\n \t\t\t\t\t   incr_vec[i].initializer,\n \t\t\t\t\t   new_var);\n \n-      if (!double_int_equal_p (incr_vec[i].incr, cand_incr))\n+      if (incr_vec[i].incr != cand_incr)\n \t{\n \t  gcc_assert (repl_code == PLUS_EXPR);\n \t  repl_code = MINUS_EXPR;\n@@ -2400,7 +2393,7 @@ replace_one_candidate (slsr_cand_t c, unsigned i, tree *new_var,\n      from the basis name, or an add of the stride to the basis\n      name, respectively.  It may be necessary to introduce a\n      cast (or reuse an existing cast).  */\n-  else if (double_int_one_p (cand_incr))\n+  else if (cand_incr.is_one ())\n     {\n       tree stride_type = TREE_TYPE (c->stride);\n       tree orig_type = TREE_TYPE (orig_rhs2);\n@@ -2415,7 +2408,7 @@ replace_one_candidate (slsr_cand_t c, unsigned i, tree *new_var,\n \t\t\t\t\t      c);\n     }\n \n-  else if (double_int_minus_one_p (cand_incr))\n+  else if (cand_incr.is_minus_one ())\n     {\n       tree stride_type = TREE_TYPE (c->stride);\n       tree orig_type = TREE_TYPE (orig_rhs2);\n@@ -2441,7 +2434,7 @@ replace_one_candidate (slsr_cand_t c, unsigned i, tree *new_var,\n \tfputs (\"  (duplicate, not actually replacing)\\n\", dump_file);\n     }\n \n-  else if (double_int_zero_p (cand_incr))\n+  else if (cand_incr.is_zero ())\n     {\n       tree lhs = gimple_assign_lhs (c->cand_stmt);\n       tree lhs_type = TREE_TYPE (lhs);"}, {"sha": "141d602d6c7bfc68087d38a4d447415d3b26c8b8", "filename": "gcc/ipa-prop.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fipa-prop.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fipa-prop.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fipa-prop.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -2887,8 +2887,8 @@ ipa_modify_call_arguments (struct cgraph_edge *cs, gimple stmt,\n \t      unsigned HOST_WIDE_INT misalign;\n \n \t      get_pointer_alignment_1 (base, &align, &misalign);\n-\t      misalign += (double_int_sext (tree_to_double_int (off),\n-\t\t\t\t\t    TYPE_PRECISION (TREE_TYPE (off))).low\n+\t      misalign += (tree_to_double_int (off)\n+\t\t\t   .sext (TYPE_PRECISION (TREE_TYPE (off))).low\n \t\t\t   * BITS_PER_UNIT);\n \t      misalign = misalign & (align - 1);\n \t      if (misalign != 0)"}, {"sha": "145056e6d417a995cbcb7af9eb6d9a783784dc00", "filename": "gcc/loop-iv.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Floop-iv.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Floop-iv.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Floop-iv.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -2295,7 +2295,7 @@ iv_number_of_iterations (struct loop *loop, rtx insn, rtx condition,\n   desc->niter_expr = NULL_RTX;\n   desc->niter_max = 0;\n   if (loop->any_upper_bound\n-      && double_int_fits_in_uhwi_p (loop->nb_iterations_upper_bound))\n+      && loop->nb_iterations_upper_bound.fits_uhwi ())\n     desc->niter_max = loop->nb_iterations_upper_bound.low;\n \n   cond = GET_CODE (condition);"}, {"sha": "7cae98cf71894465210c5a6c6340fc0008f01c3e", "filename": "gcc/optabs.c", "status": "modified", "additions": 6, "deletions": 8, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Foptabs.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Foptabs.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Foptabs.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -2908,9 +2908,9 @@ expand_absneg_bit (enum rtx_code code, enum machine_mode mode,\n       nwords = (GET_MODE_BITSIZE (mode) + BITS_PER_WORD - 1) / BITS_PER_WORD;\n     }\n \n-  mask = double_int_setbit (double_int_zero, bitpos);\n+  mask = double_int_zero.set_bit (bitpos);\n   if (code == ABS)\n-    mask = double_int_not (mask);\n+    mask = ~mask;\n \n   if (target == 0\n       || target == op0\n@@ -3569,7 +3569,7 @@ expand_copysign_absneg (enum machine_mode mode, rtx op0, rtx op1, rtx target,\n \t  op1 = operand_subword_force (op1, word, mode);\n \t}\n \n-      mask = double_int_setbit (double_int_zero, bitpos);\n+      mask = double_int_zero.set_bit (bitpos);\n \n       sign = expand_binop (imode, and_optab, op1,\n \t\t\t   immed_double_int_const (mask, imode),\n@@ -3640,7 +3640,7 @@ expand_copysign_bit (enum machine_mode mode, rtx op0, rtx op1, rtx target,\n       nwords = (GET_MODE_BITSIZE (mode) + BITS_PER_WORD - 1) / BITS_PER_WORD;\n     }\n \n-  mask = double_int_setbit (double_int_zero, bitpos);\n+  mask = double_int_zero.set_bit (bitpos);\n \n   if (target == 0\n       || target == op0\n@@ -3662,8 +3662,7 @@ expand_copysign_bit (enum machine_mode mode, rtx op0, rtx op1, rtx target,\n \t      if (!op0_is_abs)\n \t\top0_piece\n \t\t  = expand_binop (imode, and_optab, op0_piece,\n-\t\t\t\t  immed_double_int_const (double_int_not (mask),\n-\t\t\t\t\t\t\t  imode),\n+\t\t\t\t  immed_double_int_const (~mask, imode),\n \t\t\t\t  NULL_RTX, 1, OPTAB_LIB_WIDEN);\n \n \t      op1 = expand_binop (imode, and_optab,\n@@ -3694,8 +3693,7 @@ expand_copysign_bit (enum machine_mode mode, rtx op0, rtx op1, rtx target,\n       op0 = gen_lowpart (imode, op0);\n       if (!op0_is_abs)\n \top0 = expand_binop (imode, and_optab, op0,\n-\t\t\t    immed_double_int_const (double_int_not (mask),\n-\t\t\t\t\t\t    imode),\n+\t\t\t    immed_double_int_const (~mask, imode),\n \t\t\t    NULL_RTX, 1, OPTAB_LIB_WIDEN);\n \n       temp = expand_binop (imode, ior_optab, op0, op1,"}, {"sha": "9ed98e671a07fdd4f6c0c13a85e69a6ca3db17b5", "filename": "gcc/simplify-rtx.c", "status": "modified", "additions": 29, "deletions": 34, "changes": 63, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fsimplify-rtx.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fsimplify-rtx.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsimplify-rtx.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -1986,16 +1986,15 @@ simplify_binary_operation_1 (enum rtx_code code, enum machine_mode mode,\n \t  else if (GET_CODE (lhs) == MULT\n \t\t   && CONST_INT_P (XEXP (lhs, 1)))\n \t    {\n-\t      coeff0 = shwi_to_double_int (INTVAL (XEXP (lhs, 1)));\n+\t      coeff0 = double_int::from_shwi (INTVAL (XEXP (lhs, 1)));\n \t      lhs = XEXP (lhs, 0);\n \t    }\n \t  else if (GET_CODE (lhs) == ASHIFT\n \t\t   && CONST_INT_P (XEXP (lhs, 1))\n                    && INTVAL (XEXP (lhs, 1)) >= 0\n \t\t   && INTVAL (XEXP (lhs, 1)) < HOST_BITS_PER_WIDE_INT)\n \t    {\n-\t      coeff0 = double_int_setbit (double_int_zero,\n-\t\t\t\t\t  INTVAL (XEXP (lhs, 1)));\n+\t      coeff0 = double_int_zero.set_bit (INTVAL (XEXP (lhs, 1)));\n \t      lhs = XEXP (lhs, 0);\n \t    }\n \n@@ -2007,16 +2006,15 @@ simplify_binary_operation_1 (enum rtx_code code, enum machine_mode mode,\n \t  else if (GET_CODE (rhs) == MULT\n \t\t   && CONST_INT_P (XEXP (rhs, 1)))\n \t    {\n-\t      coeff1 = shwi_to_double_int (INTVAL (XEXP (rhs, 1)));\n+\t      coeff1 = double_int::from_shwi (INTVAL (XEXP (rhs, 1)));\n \t      rhs = XEXP (rhs, 0);\n \t    }\n \t  else if (GET_CODE (rhs) == ASHIFT\n \t\t   && CONST_INT_P (XEXP (rhs, 1))\n \t\t   && INTVAL (XEXP (rhs, 1)) >= 0\n \t\t   && INTVAL (XEXP (rhs, 1)) < HOST_BITS_PER_WIDE_INT)\n \t    {\n-\t      coeff1 = double_int_setbit (double_int_zero,\n-\t\t\t\t\t  INTVAL (XEXP (rhs, 1)));\n+\t      coeff1 = double_int_zero.set_bit (INTVAL (XEXP (rhs, 1)));\n \t      rhs = XEXP (rhs, 0);\n \t    }\n \n@@ -2027,7 +2025,7 @@ simplify_binary_operation_1 (enum rtx_code code, enum machine_mode mode,\n \t      double_int val;\n \t      bool speed = optimize_function_for_speed_p (cfun);\n \n-\t      val = double_int_add (coeff0, coeff1);\n+\t      val = coeff0 + coeff1;\n \t      coeff = immed_double_int_const (val, mode);\n \n \t      tem = simplify_gen_binary (MULT, mode, lhs, coeff);\n@@ -2165,16 +2163,15 @@ simplify_binary_operation_1 (enum rtx_code code, enum machine_mode mode,\n \t  else if (GET_CODE (lhs) == MULT\n \t\t   && CONST_INT_P (XEXP (lhs, 1)))\n \t    {\n-\t      coeff0 = shwi_to_double_int (INTVAL (XEXP (lhs, 1)));\n+\t      coeff0 = double_int::from_shwi (INTVAL (XEXP (lhs, 1)));\n \t      lhs = XEXP (lhs, 0);\n \t    }\n \t  else if (GET_CODE (lhs) == ASHIFT\n \t\t   && CONST_INT_P (XEXP (lhs, 1))\n \t\t   && INTVAL (XEXP (lhs, 1)) >= 0\n \t\t   && INTVAL (XEXP (lhs, 1)) < HOST_BITS_PER_WIDE_INT)\n \t    {\n-\t      coeff0 = double_int_setbit (double_int_zero,\n-\t\t\t\t\t  INTVAL (XEXP (lhs, 1)));\n+\t      coeff0 = double_int_zero.set_bit (INTVAL (XEXP (lhs, 1)));\n \t      lhs = XEXP (lhs, 0);\n \t    }\n \n@@ -2186,17 +2183,16 @@ simplify_binary_operation_1 (enum rtx_code code, enum machine_mode mode,\n \t  else if (GET_CODE (rhs) == MULT\n \t\t   && CONST_INT_P (XEXP (rhs, 1)))\n \t    {\n-\t      negcoeff1 = shwi_to_double_int (-INTVAL (XEXP (rhs, 1)));\n+\t      negcoeff1 = double_int::from_shwi (-INTVAL (XEXP (rhs, 1)));\n \t      rhs = XEXP (rhs, 0);\n \t    }\n \t  else if (GET_CODE (rhs) == ASHIFT\n \t\t   && CONST_INT_P (XEXP (rhs, 1))\n \t\t   && INTVAL (XEXP (rhs, 1)) >= 0\n \t\t   && INTVAL (XEXP (rhs, 1)) < HOST_BITS_PER_WIDE_INT)\n \t    {\n-\t      negcoeff1 = double_int_setbit (double_int_zero,\n-\t\t\t\t\t     INTVAL (XEXP (rhs, 1)));\n-\t      negcoeff1 = double_int_neg (negcoeff1);\n+\t      negcoeff1 = double_int_zero.set_bit (INTVAL (XEXP (rhs, 1)));\n+\t      negcoeff1 = -negcoeff1;\n \t      rhs = XEXP (rhs, 0);\n \t    }\n \n@@ -2207,7 +2203,7 @@ simplify_binary_operation_1 (enum rtx_code code, enum machine_mode mode,\n \t      double_int val;\n \t      bool speed = optimize_function_for_speed_p (cfun);\n \n-\t      val = double_int_add (coeff0, negcoeff1);\n+\t      val = coeff0 + negcoeff1;\n \t      coeff = immed_double_int_const (val, mode);\n \n \t      tem = simplify_gen_binary (MULT, mode, lhs, coeff);\n@@ -3590,16 +3586,16 @@ simplify_const_binary_operation (enum rtx_code code, enum machine_mode mode,\n \t{\n \tcase MINUS:\n \t  /* A - B == A + (-B).  */\n-\t  o1 = double_int_neg (o1);\n+\t  o1 = -o1;\n \n \t  /* Fall through....  */\n \n \tcase PLUS:\n-\t  res = double_int_add (o0, o1);\n+\t  res = o0 + o1;\n \t  break;\n \n \tcase MULT:\n-\t  res = double_int_mul (o0, o1);\n+\t  res = o0 * o1;\n \t  break;\n \n \tcase DIV:\n@@ -3635,31 +3631,31 @@ simplify_const_binary_operation (enum rtx_code code, enum machine_mode mode,\n \t  break;\n \n \tcase AND:\n-\t  res = double_int_and (o0, o1);\n+\t  res = o0 & o1;\n \t  break;\n \n \tcase IOR:\n-\t  res = double_int_ior (o0, o1);\n+\t  res = o0 | o1;\n \t  break;\n \n \tcase XOR:\n-\t  res = double_int_xor (o0, o1);\n+\t  res = o0 ^ o1;\n \t  break;\n \n \tcase SMIN:\n-\t  res = double_int_smin (o0, o1);\n+\t  res = o0.smin (o1);\n \t  break;\n \n \tcase SMAX:\n-\t  res = double_int_smax (o0, o1);\n+\t  res = o0.smax (o1);\n \t  break;\n \n \tcase UMIN:\n-\t  res = double_int_umin (o0, o1);\n+\t  res = o0.umin (o1);\n \t  break;\n \n \tcase UMAX:\n-\t  res = double_int_umax (o0, o1);\n+\t  res = o0.umax (o1);\n \t  break;\n \n \tcase LSHIFTRT:   case ASHIFTRT:\n@@ -3674,22 +3670,21 @@ simplify_const_binary_operation (enum rtx_code code, enum machine_mode mode,\n \t\to1.low &= GET_MODE_PRECISION (mode) - 1;\n \t      }\n \n-\t    if (!double_int_fits_in_uhwi_p (o1)\n-\t        || double_int_to_uhwi (o1) >= GET_MODE_PRECISION (mode))\n+\t    if (!o1.fits_uhwi ()\n+\t        || o1.to_uhwi () >= GET_MODE_PRECISION (mode))\n \t      return 0;\n \n-\t    cnt = double_int_to_uhwi (o1);\n+\t    cnt = o1.to_uhwi ();\n+\t    unsigned short prec = GET_MODE_PRECISION (mode);\n \n \t    if (code == LSHIFTRT || code == ASHIFTRT)\n-\t      res = double_int_rshift (o0, cnt, GET_MODE_PRECISION (mode),\n-\t\t\t\t       code == ASHIFTRT);\n+\t      res = o0.rshift (cnt, prec, code == ASHIFTRT);\n \t    else if (code == ASHIFT)\n-\t      res = double_int_lshift (o0, cnt, GET_MODE_PRECISION (mode),\n-\t\t\t\t       true);\n+\t      res = o0.alshift (cnt, prec);\n \t    else if (code == ROTATE)\n-\t      res = double_int_lrotate (o0, cnt, GET_MODE_PRECISION (mode));\n+\t      res = o0.lrotate (cnt, prec);\n \t    else /* code == ROTATERT */\n-\t      res = double_int_rrotate (o0, cnt, GET_MODE_PRECISION (mode));\n+\t      res = o0.rrotate (cnt, prec);\n \t  }\n \t  break;\n "}, {"sha": "674f88801bce3a043ba15f990a7106468ecccdca", "filename": "gcc/stor-layout.c", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fstor-layout.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fstor-layout.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fstor-layout.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -2218,14 +2218,13 @@ layout_type (tree type)\n \t\t    && TYPE_UNSIGNED (TREE_TYPE (lb))\n \t\t    && tree_int_cst_lt (ub, lb))\n \t\t  {\n+\t\t    unsigned prec = TYPE_PRECISION (TREE_TYPE (lb));\n \t\t    lb = double_int_to_tree\n \t\t\t   (ssizetype,\n-\t\t\t    double_int_sext (tree_to_double_int (lb),\n-\t\t\t\t\t     TYPE_PRECISION (TREE_TYPE (lb))));\n+\t\t\t    tree_to_double_int (lb).sext (prec));\n \t\t    ub = double_int_to_tree\n \t\t\t   (ssizetype,\n-\t\t\t    double_int_sext (tree_to_double_int (ub),\n-\t\t\t\t\t     TYPE_PRECISION (TREE_TYPE (ub))));\n+\t\t\t    tree_to_double_int (ub).sext (prec));\n \t\t  }\n \t\tlength\n \t\t  = fold_convert (sizetype,"}, {"sha": "456670d01e9a3a21fb8e725f82b84018e346009e", "filename": "gcc/tree-affine.c", "status": "modified", "additions": 35, "deletions": 38, "changes": 73, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-affine.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-affine.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-affine.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -33,7 +33,7 @@ along with GCC; see the file COPYING3.  If not see\n double_int\n double_int_ext_for_comb (double_int cst, aff_tree *comb)\n {\n-  return double_int_sext (cst, TYPE_PRECISION (comb->type));\n+  return cst.sext (TYPE_PRECISION (comb->type));\n }\n \n /* Initializes affine combination COMB so that its value is zero in TYPE.  */\n@@ -76,27 +76,26 @@ aff_combination_scale (aff_tree *comb, double_int scale)\n   unsigned i, j;\n \n   scale = double_int_ext_for_comb (scale, comb);\n-  if (double_int_one_p (scale))\n+  if (scale.is_one ())\n     return;\n \n-  if (double_int_zero_p (scale))\n+  if (scale.is_zero ())\n     {\n       aff_combination_zero (comb, comb->type);\n       return;\n     }\n \n   comb->offset\n-    = double_int_ext_for_comb (double_int_mul (scale, comb->offset), comb);\n+    = double_int_ext_for_comb (scale * comb->offset, comb);\n   for (i = 0, j = 0; i < comb->n; i++)\n     {\n       double_int new_coef;\n \n       new_coef\n-\t= double_int_ext_for_comb (double_int_mul (scale, comb->elts[i].coef),\n-\t\t\t\t   comb);\n+\t= double_int_ext_for_comb (scale * comb->elts[i].coef, comb);\n       /* A coefficient may become zero due to overflow.  Remove the zero\n \t elements.  */\n-      if (double_int_zero_p (new_coef))\n+      if (new_coef.is_zero ())\n \tcontinue;\n       comb->elts[j].coef = new_coef;\n       comb->elts[j].val = comb->elts[i].val;\n@@ -131,17 +130,17 @@ aff_combination_add_elt (aff_tree *comb, tree elt, double_int scale)\n   tree type;\n \n   scale = double_int_ext_for_comb (scale, comb);\n-  if (double_int_zero_p (scale))\n+  if (scale.is_zero ())\n     return;\n \n   for (i = 0; i < comb->n; i++)\n     if (operand_equal_p (comb->elts[i].val, elt, 0))\n       {\n \tdouble_int new_coef;\n \n-\tnew_coef = double_int_add (comb->elts[i].coef, scale);\n+\tnew_coef = comb->elts[i].coef + scale;\n \tnew_coef = double_int_ext_for_comb (new_coef, comb);\n-\tif (!double_int_zero_p (new_coef))\n+\tif (!new_coef.is_zero ())\n \t  {\n \t    comb->elts[i].coef = new_coef;\n \t    return;\n@@ -172,7 +171,7 @@ aff_combination_add_elt (aff_tree *comb, tree elt, double_int scale)\n   if (POINTER_TYPE_P (type))\n     type = sizetype;\n \n-  if (double_int_one_p (scale))\n+  if (scale.is_one ())\n     elt = fold_convert (type, elt);\n   else\n     elt = fold_build2 (MULT_EXPR, type,\n@@ -191,7 +190,7 @@ aff_combination_add_elt (aff_tree *comb, tree elt, double_int scale)\n static void\n aff_combination_add_cst (aff_tree *c, double_int cst)\n {\n-  c->offset = double_int_ext_for_comb (double_int_add (c->offset, cst), c);\n+  c->offset = double_int_ext_for_comb (c->offset + cst, c);\n }\n \n /* Adds COMB2 to COMB1.  */\n@@ -234,7 +233,7 @@ aff_combination_convert (aff_tree *comb, tree type)\n   for (i = j = 0; i < comb->n; i++)\n     {\n       double_int new_coef = double_int_ext_for_comb (comb->elts[i].coef, comb);\n-      if (double_int_zero_p (new_coef))\n+      if (new_coef.is_zero ())\n \tcontinue;\n       comb->elts[j].coef = new_coef;\n       comb->elts[j].val = fold_convert (type, comb->elts[i].val);\n@@ -323,7 +322,7 @@ tree_to_aff_combination (tree expr, tree type, aff_tree *comb)\n       if (bitpos % BITS_PER_UNIT != 0)\n \tbreak;\n       aff_combination_const (comb, type,\n-\t\t\t     uhwi_to_double_int (bitpos / BITS_PER_UNIT));\n+\t\t\t     double_int::from_uhwi (bitpos / BITS_PER_UNIT));\n       core = build_fold_addr_expr (core);\n       if (TREE_CODE (core) == ADDR_EXPR)\n \taff_combination_add_elt (comb, core, double_int_one);\n@@ -380,7 +379,7 @@ add_elt_to_tree (tree expr, tree type, tree elt, double_int scale,\n   scale = double_int_ext_for_comb (scale, comb);\n   elt = fold_convert (type1, elt);\n \n-  if (double_int_one_p (scale))\n+  if (scale.is_one ())\n     {\n       if (!expr)\n \treturn fold_convert (type, elt);\n@@ -390,7 +389,7 @@ add_elt_to_tree (tree expr, tree type, tree elt, double_int scale,\n       return fold_build2 (PLUS_EXPR, type, expr, elt);\n     }\n \n-  if (double_int_minus_one_p (scale))\n+  if (scale.is_minus_one ())\n     {\n       if (!expr)\n \treturn fold_convert (type, fold_build1 (NEGATE_EXPR, type1, elt));\n@@ -408,10 +407,10 @@ add_elt_to_tree (tree expr, tree type, tree elt, double_int scale,\n \t\t\t fold_build2 (MULT_EXPR, type1, elt,\n \t\t\t\t      double_int_to_tree (type1, scale)));\n \n-  if (double_int_negative_p (scale))\n+  if (scale.is_negative ())\n     {\n       code = MINUS_EXPR;\n-      scale = double_int_neg (scale);\n+      scale = -scale;\n     }\n   else\n     code = PLUS_EXPR;\n@@ -451,9 +450,9 @@ aff_combination_to_tree (aff_tree *comb)\n \n   /* Ensure that we get x - 1, not x + (-1) or x + 0xff..f if x is\n      unsigned.  */\n-  if (double_int_negative_p (comb->offset))\n+  if (comb->offset.is_negative ())\n     {\n-      off = double_int_neg (comb->offset);\n+      off = -comb->offset;\n       sgn = double_int_minus_one;\n     }\n   else\n@@ -516,8 +515,7 @@ aff_combination_add_product (aff_tree *c, double_int coef, tree val,\n \t\t\t      fold_convert (type, val));\n \t}\n \n-      aff_combination_add_elt (r, aval,\n-\t\t\t       double_int_mul (coef, c->elts[i].coef));\n+      aff_combination_add_elt (r, aval, coef * c->elts[i].coef);\n     }\n \n   if (c->rest)\n@@ -534,10 +532,9 @@ aff_combination_add_product (aff_tree *c, double_int coef, tree val,\n     }\n \n   if (val)\n-    aff_combination_add_elt (r, val,\n-\t\t\t     double_int_mul (coef, c->offset));\n+    aff_combination_add_elt (r, val, coef * c->offset);\n   else\n-    aff_combination_add_cst (r, double_int_mul (coef, c->offset));\n+    aff_combination_add_cst (r, coef * c->offset);\n }\n \n /* Multiplies C1 by C2, storing the result to R  */\n@@ -685,7 +682,7 @@ aff_combination_expand (aff_tree *comb ATTRIBUTE_UNUSED,\n          it from COMB.  */\n       scale = comb->elts[i].coef;\n       aff_combination_zero (&curre, comb->type);\n-      aff_combination_add_elt (&curre, e, double_int_neg (scale));\n+      aff_combination_add_elt (&curre, e, -scale);\n       aff_combination_scale (&current, scale);\n       aff_combination_add (&to_add, &current);\n       aff_combination_add (&to_add, &curre);\n@@ -751,17 +748,17 @@ double_int_constant_multiple_p (double_int val, double_int div,\n {\n   double_int rem, cst;\n \n-  if (double_int_zero_p (val))\n+  if (val.is_zero ())\n     return true;\n \n-  if (double_int_zero_p (div))\n+  if (div.is_zero ())\n     return false;\n \n-  cst = double_int_sdivmod (val, div, FLOOR_DIV_EXPR, &rem);\n-  if (!double_int_zero_p (rem))\n+  cst = val.sdivmod (div, FLOOR_DIV_EXPR, &rem);\n+  if (!rem.is_zero ())\n     return false;\n \n-  if (*mult_set && !double_int_equal_p (*mult, cst))\n+  if (*mult_set && *mult != cst)\n     return false;\n \n   *mult_set = true;\n@@ -779,7 +776,7 @@ aff_combination_constant_multiple_p (aff_tree *val, aff_tree *div,\n   bool mult_set = false;\n   unsigned i;\n \n-  if (val->n == 0 && double_int_zero_p (val->offset))\n+  if (val->n == 0 && val->offset.is_zero ())\n     {\n       *mult = double_int_zero;\n       return true;\n@@ -880,10 +877,10 @@ get_inner_reference_aff (tree ref, aff_tree *addr, double_int *size)\n     }\n \n   aff_combination_const (&tmp, sizetype,\n-\t\t\t shwi_to_double_int (bitpos / BITS_PER_UNIT));\n+\t\t\t double_int::from_shwi (bitpos / BITS_PER_UNIT));\n   aff_combination_add (addr, &tmp);\n \n-  *size = shwi_to_double_int ((bitsize + BITS_PER_UNIT - 1) / BITS_PER_UNIT);\n+  *size = double_int::from_shwi ((bitsize + BITS_PER_UNIT - 1) / BITS_PER_UNIT);\n }\n \n /* Returns true if a region of size SIZE1 at position 0 and a region of\n@@ -899,17 +896,17 @@ aff_comb_cannot_overlap_p (aff_tree *diff, double_int size1, double_int size2)\n     return false;\n \n   d = diff->offset;\n-  if (double_int_negative_p (d))\n+  if (d.is_negative ())\n     {\n       /* The second object is before the first one, we succeed if the last\n \t element of the second object is before the start of the first one.  */\n-      bound = double_int_add (d, double_int_add (size2, double_int_minus_one));\n-      return double_int_negative_p (bound);\n+      bound = d + size2 + double_int_minus_one;\n+      return bound.is_negative ();\n     }\n   else\n     {\n       /* We succeed if the second object starts after the first one ends.  */\n-      return double_int_scmp (size1, d) <= 0;\n+      return size1.sle (d);\n     }\n }\n "}, {"sha": "0c350a6f2574249e23a9cc5ec96dd8bf9b8bf6ae", "filename": "gcc/tree-cfg.c", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-cfg.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-cfg.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-cfg.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -1371,14 +1371,12 @@ group_case_labels_stmt (gimple stmt)\n \t{\n \t  tree merge_case = gimple_switch_label (stmt, i);\n \t  basic_block merge_bb = label_to_block (CASE_LABEL (merge_case));\n-\t  double_int bhp1 = double_int_add (tree_to_double_int (base_high),\n-\t\t\t\t\t    double_int_one);\n+\t  double_int bhp1 = tree_to_double_int (base_high) + double_int_one;\n \n \t  /* Merge the cases if they jump to the same place,\n \t     and their ranges are consecutive.  */\n \t  if (merge_bb == base_bb\n-\t      && double_int_equal_p (tree_to_double_int (CASE_LOW (merge_case)),\n-\t\t\t\t     bhp1))\n+\t      && tree_to_double_int (CASE_LOW (merge_case)) == bhp1)\n \t    {\n \t      base_high = CASE_HIGH (merge_case) ?\n \t\t  CASE_HIGH (merge_case) : CASE_LOW (merge_case);"}, {"sha": "f8f10a42276530e732194dc80e338d4a54d1444a", "filename": "gcc/tree-dfa.c", "status": "modified", "additions": 32, "deletions": 44, "changes": 76, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-dfa.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-dfa.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-dfa.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -423,9 +423,7 @@ get_ref_base_and_extent (tree exp, HOST_WIDE_INT *poffset,\n       switch (TREE_CODE (exp))\n \t{\n \tcase BIT_FIELD_REF:\n-\t  bit_offset\n-\t    = double_int_add (bit_offset,\n-\t\t\t      tree_to_double_int (TREE_OPERAND (exp, 2)));\n+\t  bit_offset += tree_to_double_int (TREE_OPERAND (exp, 2));\n \t  break;\n \n \tcase COMPONENT_REF:\n@@ -436,14 +434,11 @@ get_ref_base_and_extent (tree exp, HOST_WIDE_INT *poffset,\n \t    if (this_offset && TREE_CODE (this_offset) == INTEGER_CST)\n \t      {\n \t\tdouble_int doffset = tree_to_double_int (this_offset);\n-\t\tdoffset = double_int_lshift (doffset,\n-\t\t\t\t\t     BITS_PER_UNIT == 8\n-\t\t\t\t\t     ? 3 : exact_log2 (BITS_PER_UNIT),\n-\t\t\t\t\t     HOST_BITS_PER_DOUBLE_INT, true);\n-\t\tdoffset = double_int_add (doffset,\n-\t\t\t\t\t  tree_to_double_int\n-\t\t\t\t\t  (DECL_FIELD_BIT_OFFSET (field)));\n-\t\tbit_offset = double_int_add (bit_offset, doffset);\n+\t\tdoffset = doffset.alshift (BITS_PER_UNIT == 8\n+\t\t\t\t\t   ? 3 : exact_log2 (BITS_PER_UNIT),\n+\t\t\t\t\t   HOST_BITS_PER_DOUBLE_INT);\n+\t\tdoffset += tree_to_double_int (DECL_FIELD_BIT_OFFSET (field));\n+\t\tbit_offset = bit_offset + doffset;\n \n \t\t/* If we had seen a variable array ref already and we just\n \t\t   referenced the last field of a struct or a union member\n@@ -462,11 +457,11 @@ get_ref_base_and_extent (tree exp, HOST_WIDE_INT *poffset,\n \t\t\ttree ssize = TYPE_SIZE_UNIT (stype);\n \t\t\tif (host_integerp (fsize, 0)\n \t\t\t    && host_integerp (ssize, 0)\n-\t\t\t    && double_int_fits_in_shwi_p (doffset))\n+\t\t\t    && doffset.fits_shwi ())\n \t\t\t  maxsize += ((TREE_INT_CST_LOW (ssize)\n \t\t\t\t       - TREE_INT_CST_LOW (fsize))\n \t\t\t\t      * BITS_PER_UNIT\n-\t\t\t\t\t- double_int_to_shwi (doffset));\n+\t\t\t\t\t- doffset.to_shwi ());\n \t\t\telse\n \t\t\t  maxsize = -1;\n \t\t      }\n@@ -481,9 +476,9 @@ get_ref_base_and_extent (tree exp, HOST_WIDE_INT *poffset,\n \t\tif (maxsize != -1\n \t\t    && csize\n \t\t    && host_integerp (csize, 1)\n-\t\t    && double_int_fits_in_shwi_p (bit_offset))\n+\t\t    && bit_offset.fits_shwi ())\n \t\t  maxsize = TREE_INT_CST_LOW (csize)\n-\t\t\t    - double_int_to_shwi (bit_offset);\n+\t\t\t    - bit_offset.to_shwi ();\n \t\telse\n \t\t  maxsize = -1;\n \t      }\n@@ -504,17 +499,13 @@ get_ref_base_and_extent (tree exp, HOST_WIDE_INT *poffset,\n \t\t    TREE_CODE (unit_size) == INTEGER_CST))\n \t      {\n \t\tdouble_int doffset\n-\t\t  = double_int_sext\n-\t\t    (double_int_sub (TREE_INT_CST (index),\n-\t\t\t\t     TREE_INT_CST (low_bound)),\n-\t\t     TYPE_PRECISION (TREE_TYPE (index)));\n-\t\tdoffset = double_int_mul (doffset,\n-\t\t\t\t\t  tree_to_double_int (unit_size));\n-\t\tdoffset = double_int_lshift (doffset,\n-\t\t\t\t\t     BITS_PER_UNIT == 8\n-\t\t\t\t\t     ? 3 : exact_log2 (BITS_PER_UNIT),\n-\t\t\t\t\t     HOST_BITS_PER_DOUBLE_INT, true);\n-\t\tbit_offset = double_int_add (bit_offset, doffset);\n+\t\t  = (TREE_INT_CST (index) - TREE_INT_CST (low_bound))\n+\t\t    .sext (TYPE_PRECISION (TREE_TYPE (index)));\n+\t\tdoffset *= tree_to_double_int (unit_size);\n+\t\tdoffset = doffset.alshift (BITS_PER_UNIT == 8\n+\t\t\t\t\t   ? 3 : exact_log2 (BITS_PER_UNIT),\n+\t\t\t\t\t   HOST_BITS_PER_DOUBLE_INT);\n+\t\tbit_offset = bit_offset + doffset;\n \n \t\t/* An array ref with a constant index up in the structure\n \t\t   hierarchy will constrain the size of any variable array ref\n@@ -530,9 +521,9 @@ get_ref_base_and_extent (tree exp, HOST_WIDE_INT *poffset,\n \t\tif (maxsize != -1\n \t\t    && asize\n \t\t    && host_integerp (asize, 1)\n-\t\t    && double_int_fits_in_shwi_p (bit_offset))\n+\t\t    && bit_offset.fits_shwi ())\n \t\t  maxsize = TREE_INT_CST_LOW (asize)\n-\t\t\t    - double_int_to_shwi (bit_offset);\n+\t\t\t    - bit_offset.to_shwi ();\n \t\telse\n \t\t  maxsize = -1;\n \n@@ -547,8 +538,7 @@ get_ref_base_and_extent (tree exp, HOST_WIDE_INT *poffset,\n \t  break;\n \n \tcase IMAGPART_EXPR:\n-\t  bit_offset\n-\t    = double_int_add (bit_offset, uhwi_to_double_int (bitsize));\n+\t  bit_offset += double_int::from_uhwi (bitsize);\n \t  break;\n \n \tcase VIEW_CONVERT_EXPR:\n@@ -563,12 +553,11 @@ get_ref_base_and_extent (tree exp, HOST_WIDE_INT *poffset,\n \t      else\n \t\t{\n \t\t  double_int off = mem_ref_offset (exp);\n-\t\t  off = double_int_lshift (off,\n-\t\t\t\t\t   BITS_PER_UNIT == 8\n-\t\t\t\t\t   ? 3 : exact_log2 (BITS_PER_UNIT),\n-\t\t\t\t\t   HOST_BITS_PER_DOUBLE_INT, true);\n-\t\t  off = double_int_add (off, bit_offset);\n-\t\t  if (double_int_fits_in_shwi_p (off))\n+\t\t  off = off.alshift (BITS_PER_UNIT == 8\n+\t\t\t\t     ? 3 : exact_log2 (BITS_PER_UNIT),\n+\t\t\t\t     HOST_BITS_PER_DOUBLE_INT);\n+\t\t  off = off + bit_offset;\n+\t\t  if (off.fits_shwi ())\n \t\t    {\n \t\t      bit_offset = off;\n \t\t      exp = TREE_OPERAND (TREE_OPERAND (exp, 0), 0);\n@@ -595,12 +584,11 @@ get_ref_base_and_extent (tree exp, HOST_WIDE_INT *poffset,\n \t      else\n \t\t{\n \t\t  double_int off = mem_ref_offset (exp);\n-\t\t  off = double_int_lshift (off,\n-\t\t\t\t\t   BITS_PER_UNIT == 8\n-\t\t\t\t\t   ? 3 : exact_log2 (BITS_PER_UNIT),\n-\t\t\t\t\t   HOST_BITS_PER_DOUBLE_INT, true);\n-\t\t  off = double_int_add (off, bit_offset);\n-\t\t  if (double_int_fits_in_shwi_p (off))\n+\t\t  off = off.alshift (BITS_PER_UNIT == 8\n+\t\t\t\t     ? 3 : exact_log2 (BITS_PER_UNIT),\n+\t\t\t\t     HOST_BITS_PER_DOUBLE_INT);\n+\t\t  off += bit_offset;\n+\t\t  if (off.fits_shwi ())\n \t\t    {\n \t\t      bit_offset = off;\n \t\t      exp = TREE_OPERAND (TMR_BASE (exp), 0);\n@@ -617,7 +605,7 @@ get_ref_base_and_extent (tree exp, HOST_WIDE_INT *poffset,\n     }\n  done:\n \n-  if (!double_int_fits_in_shwi_p (bit_offset))\n+  if (!bit_offset.fits_shwi ())\n     {\n       *poffset = 0;\n       *psize = bitsize;\n@@ -626,7 +614,7 @@ get_ref_base_and_extent (tree exp, HOST_WIDE_INT *poffset,\n       return exp;\n     }\n \n-  hbit_offset = double_int_to_shwi (bit_offset);\n+  hbit_offset = bit_offset.to_shwi ();\n \n   /* We need to deal with variable arrays ending structures such as\n        struct { int length; int a[1]; } x;           x.a[d]"}, {"sha": "6c55da6fae2fba2ed2449761abc4b50faf67da52", "filename": "gcc/tree-flow-inline.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-flow-inline.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-flow-inline.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-flow-inline.h?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -1271,7 +1271,7 @@ get_addr_base_and_unit_offset_1 (tree exp, HOST_WIDE_INT *poffset,\n \t\t  {\n \t\t    double_int off = mem_ref_offset (exp);\n \t\t    gcc_assert (off.high == -1 || off.high == 0);\n-\t\t    byte_offset += double_int_to_shwi (off);\n+\t\t    byte_offset += off.to_shwi ();\n \t\t  }\n \t\texp = TREE_OPERAND (base, 0);\n \t      }\n@@ -1294,7 +1294,7 @@ get_addr_base_and_unit_offset_1 (tree exp, HOST_WIDE_INT *poffset,\n \t\t  {\n \t\t    double_int off = mem_ref_offset (exp);\n \t\t    gcc_assert (off.high == -1 || off.high == 0);\n-\t\t    byte_offset += double_int_to_shwi (off);\n+\t\t    byte_offset += off.to_shwi ();\n \t\t  }\n \t\texp = TREE_OPERAND (base, 0);\n \t      }"}, {"sha": "9a537f1c5fcddbc53290583cbe7adcf0a4367b56", "filename": "gcc/tree-object-size.c", "status": "modified", "additions": 4, "deletions": 5, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-object-size.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-object-size.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-object-size.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -192,12 +192,11 @@ addr_object_size (struct object_size_info *osi, const_tree ptr,\n \t}\n       if (sz != unknown[object_size_type])\n \t{\n-\t  double_int dsz = double_int_sub (uhwi_to_double_int (sz),\n-\t\t\t\t\t   mem_ref_offset (pt_var));\n-\t  if (double_int_negative_p (dsz))\n+\t  double_int dsz = double_int::from_uhwi (sz) - mem_ref_offset (pt_var);\n+\t  if (dsz.is_negative ())\n \t    sz = 0;\n-\t  else if (double_int_fits_in_uhwi_p (dsz))\n-\t    sz = double_int_to_uhwi (dsz);\n+\t  else if (dsz.fits_uhwi ())\n+\t    sz = dsz.to_uhwi ();\n \t  else\n \t    sz = unknown[object_size_type];\n \t}"}, {"sha": "ba61c5b04e0e9ff12cac2d54d0a27a02f2056cbf", "filename": "gcc/tree-predcom.c", "status": "modified", "additions": 8, "deletions": 9, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-predcom.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-predcom.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-predcom.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -901,7 +901,7 @@ order_drefs (const void *a, const void *b)\n {\n   const dref *const da = (const dref *) a;\n   const dref *const db = (const dref *) b;\n-  int offcmp = double_int_scmp ((*da)->offset, (*db)->offset);\n+  int offcmp = (*da)->offset.scmp ((*db)->offset);\n \n   if (offcmp != 0)\n     return offcmp;\n@@ -925,18 +925,18 @@ add_ref_to_chain (chain_p chain, dref ref)\n   dref root = get_chain_root (chain);\n   double_int dist;\n \n-  gcc_assert (double_int_scmp (root->offset, ref->offset) <= 0);\n-  dist = double_int_sub (ref->offset, root->offset);\n-  if (double_int_ucmp (uhwi_to_double_int (MAX_DISTANCE), dist) <= 0)\n+  gcc_assert (root->offset.sle (ref->offset));\n+  dist = ref->offset - root->offset;\n+  if (double_int::from_uhwi (MAX_DISTANCE).ule (dist))\n     {\n       free (ref);\n       return;\n     }\n-  gcc_assert (double_int_fits_in_uhwi_p (dist));\n+  gcc_assert (dist.fits_uhwi ());\n \n   VEC_safe_push (dref, heap, chain->refs, ref);\n \n-  ref->distance = double_int_to_uhwi (dist);\n+  ref->distance = dist.to_uhwi ();\n \n   if (ref->distance >= chain->length)\n     {\n@@ -1055,7 +1055,7 @@ valid_initializer_p (struct data_reference *ref,\n   if (!aff_combination_constant_multiple_p (&diff, &step, &off))\n     return false;\n \n-  if (!double_int_equal_p (off, uhwi_to_double_int (distance)))\n+  if (off != double_int::from_uhwi (distance))\n     return false;\n \n   return true;\n@@ -1198,8 +1198,7 @@ determine_roots_comp (struct loop *loop,\n   FOR_EACH_VEC_ELT (dref, comp->refs, i, a)\n     {\n       if (!chain || DR_IS_WRITE (a->ref)\n-\t  || double_int_ucmp (uhwi_to_double_int (MAX_DISTANCE),\n-\t\t\t      double_int_sub (a->offset, last_ofs)) <= 0)\n+\t  || double_int::from_uhwi (MAX_DISTANCE).ule (a->offset - last_ofs))\n \t{\n \t  if (nontrivial_chain_p (chain))\n \t    {"}, {"sha": "a92b6d0d2537d872e49eab58cecf36b79f1625d6", "filename": "gcc/tree-pretty-print.c", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-pretty-print.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-pretty-print.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-pretty-print.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -1330,8 +1330,7 @@ dump_generic_node (pretty_printer *buffer, tree node, int spc, int flags,\n \t\t  }\n \t\telse if (is_array_init\n \t\t\t && (TREE_CODE (field) != INTEGER_CST\n-\t\t\t     || !double_int_equal_p (tree_to_double_int (field),\n-\t\t\t\t\t\t     curidx)))\n+\t\t\t     || tree_to_double_int (field) != curidx))\n \t\t  {\n \t\t    pp_character (buffer, '[');\n \t\t    if (TREE_CODE (field) == RANGE_EXPR)\n@@ -1352,7 +1351,7 @@ dump_generic_node (pretty_printer *buffer, tree node, int spc, int flags,\n \t\t  }\n \t      }\n             if (is_array_init)\n-\t      curidx = double_int_add (curidx, double_int_one);\n+\t      curidx += double_int_one;\n \t    if (val && TREE_CODE (val) == ADDR_EXPR)\n \t      if (TREE_CODE (TREE_OPERAND (val, 0)) == FUNCTION_DECL)\n \t\tval = TREE_OPERAND (val, 0);"}, {"sha": "aafaa15a805bece41a91ca2d83e314f5feee1906", "filename": "gcc/tree-sra.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-sra.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-sra.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-sra.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -1488,8 +1488,8 @@ build_ref_for_offset (location_t loc, tree base, HOST_WIDE_INT offset,\n \t  || TREE_CODE (prev_base) == TARGET_MEM_REF)\n \talign = TYPE_ALIGN (TREE_TYPE (prev_base));\n     }\n-  misalign += (double_int_sext (tree_to_double_int (off),\n-\t\t\t\tTYPE_PRECISION (TREE_TYPE (off))).low\n+  misalign += (tree_to_double_int (off)\n+\t       .sext (TYPE_PRECISION (TREE_TYPE (off))).low\n \t       * BITS_PER_UNIT);\n   misalign = misalign & (align - 1);\n   if (misalign != 0)"}, {"sha": "caa51be6a5d0f4285e02fe84cd0feab77a20aeee", "filename": "gcc/tree-ssa-address.c", "status": "modified", "additions": 14, "deletions": 14, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-address.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-address.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-address.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -198,8 +198,8 @@ addr_for_mem_ref (struct mem_address *addr, addr_space_t as,\n \n   if (addr->offset && !integer_zerop (addr->offset))\n     off = immed_double_int_const\n-\t    (double_int_sext (tree_to_double_int (addr->offset),\n-\t\t\t      TYPE_PRECISION (TREE_TYPE (addr->offset))),\n+\t    (tree_to_double_int (addr->offset)\n+\t     .sext (TYPE_PRECISION (TREE_TYPE (addr->offset))),\n \t     pointer_mode);\n   else\n     off = NULL_RTX;\n@@ -400,7 +400,7 @@ move_fixed_address_to_symbol (struct mem_address *parts, aff_tree *addr)\n \n   for (i = 0; i < addr->n; i++)\n     {\n-      if (!double_int_one_p (addr->elts[i].coef))\n+      if (!addr->elts[i].coef.is_one ())\n \tcontinue;\n \n       val = addr->elts[i].val;\n@@ -428,7 +428,7 @@ move_hint_to_base (tree type, struct mem_address *parts, tree base_hint,\n \n   for (i = 0; i < addr->n; i++)\n     {\n-      if (!double_int_one_p (addr->elts[i].coef))\n+      if (!addr->elts[i].coef.is_one ())\n \tcontinue;\n \n       val = addr->elts[i].val;\n@@ -460,7 +460,7 @@ move_pointer_to_base (struct mem_address *parts, aff_tree *addr)\n \n   for (i = 0; i < addr->n; i++)\n     {\n-      if (!double_int_one_p (addr->elts[i].coef))\n+      if (!addr->elts[i].coef.is_one ())\n \tcontinue;\n \n       val = addr->elts[i].val;\n@@ -548,10 +548,10 @@ most_expensive_mult_to_index (tree type, struct mem_address *parts,\n   best_mult = double_int_zero;\n   for (i = 0; i < addr->n; i++)\n     {\n-      if (!double_int_fits_in_shwi_p (addr->elts[i].coef))\n+      if (!addr->elts[i].coef.fits_shwi ())\n \tcontinue;\n \n-      coef = double_int_to_shwi (addr->elts[i].coef);\n+      coef = addr->elts[i].coef.to_shwi ();\n       if (coef == 1\n \t  || !multiplier_allowed_in_address_p (coef, TYPE_MODE (type), as))\n \tcontinue;\n@@ -572,11 +572,11 @@ most_expensive_mult_to_index (tree type, struct mem_address *parts,\n   for (i = j = 0; i < addr->n; i++)\n     {\n       amult = addr->elts[i].coef;\n-      amult_neg = double_int_ext_for_comb (double_int_neg (amult), addr);\n+      amult_neg = double_int_ext_for_comb (-amult, addr);\n \n-      if (double_int_equal_p (amult, best_mult))\n+      if (amult == best_mult)\n \top_code = PLUS_EXPR;\n-      else if (double_int_equal_p (amult_neg, best_mult))\n+      else if (amult_neg == best_mult)\n \top_code = MINUS_EXPR;\n       else\n \t{\n@@ -624,7 +624,7 @@ addr_to_parts (tree type, aff_tree *addr, tree iv_cand,\n   parts->index = NULL_TREE;\n   parts->step = NULL_TREE;\n \n-  if (!double_int_zero_p (addr->offset))\n+  if (!addr->offset.is_zero ())\n     parts->offset = double_int_to_tree (sizetype, addr->offset);\n   else\n     parts->offset = NULL_TREE;\n@@ -656,7 +656,7 @@ addr_to_parts (tree type, aff_tree *addr, tree iv_cand,\n   for (i = 0; i < addr->n; i++)\n     {\n       part = fold_convert (sizetype, addr->elts[i].val);\n-      if (!double_int_one_p (addr->elts[i].coef))\n+      if (!addr->elts[i].coef.is_one ())\n \tpart = fold_build2 (MULT_EXPR, sizetype, part,\n \t\t\t    double_int_to_tree (sizetype, addr->elts[i].coef));\n       add_to_parts (parts, part);\n@@ -876,8 +876,8 @@ copy_ref_info (tree new_ref, tree old_ref)\n \t\t\t   && (TREE_INT_CST_LOW (TMR_STEP (new_ref))\n \t\t\t       < align)))))\n \t    {\n-\t      unsigned int inc = double_int_sub (mem_ref_offset (old_ref),\n-\t\t\t\t\t\t mem_ref_offset (new_ref)).low;\n+\t      unsigned int inc = (mem_ref_offset (old_ref)\n+\t\t\t\t  - mem_ref_offset (new_ref)).low;\n \t      adjust_ptr_info_misalignment (new_pi, inc);\n \t    }\n \t  else"}, {"sha": "b045da27eec711f28cb8f3d9a381c9bf11ec15ee", "filename": "gcc/tree-ssa-alias.c", "status": "modified", "additions": 20, "deletions": 24, "changes": 44, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-alias.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-alias.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-alias.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -756,12 +756,11 @@ indirect_ref_may_alias_decl_p (tree ref1 ATTRIBUTE_UNUSED, tree base1,\n   /* The offset embedded in MEM_REFs can be negative.  Bias them\n      so that the resulting offset adjustment is positive.  */\n   moff = mem_ref_offset (base1);\n-  moff = double_int_lshift (moff,\n-\t\t\t    BITS_PER_UNIT == 8\n-\t\t\t    ? 3 : exact_log2 (BITS_PER_UNIT),\n-\t\t\t    HOST_BITS_PER_DOUBLE_INT, true);\n-  if (double_int_negative_p (moff))\n-    offset2p += double_int_neg (moff).low;\n+  moff = moff.alshift (BITS_PER_UNIT == 8\n+\t\t       ? 3 : exact_log2 (BITS_PER_UNIT),\n+\t\t       HOST_BITS_PER_DOUBLE_INT);\n+  if (moff.is_negative ())\n+    offset2p += (-moff).low;\n   else\n     offset1p += moff.low;\n \n@@ -835,12 +834,11 @@ indirect_ref_may_alias_decl_p (tree ref1 ATTRIBUTE_UNUSED, tree base1,\n       || TREE_CODE (dbase2) == TARGET_MEM_REF)\n     {\n       double_int moff = mem_ref_offset (dbase2);\n-      moff = double_int_lshift (moff,\n-\t\t\t\tBITS_PER_UNIT == 8\n-\t\t\t\t? 3 : exact_log2 (BITS_PER_UNIT),\n-\t\t\t\tHOST_BITS_PER_DOUBLE_INT, true);\n-      if (double_int_negative_p (moff))\n-\tdoffset1 -= double_int_neg (moff).low;\n+      moff = moff.alshift (BITS_PER_UNIT == 8\n+\t\t\t   ? 3 : exact_log2 (BITS_PER_UNIT),\n+\t\t\t   HOST_BITS_PER_DOUBLE_INT);\n+      if (moff.is_negative ())\n+\tdoffset1 -= (-moff).low;\n       else\n \tdoffset2 -= moff.low;\n     }\n@@ -932,21 +930,19 @@ indirect_refs_may_alias_p (tree ref1 ATTRIBUTE_UNUSED, tree base1,\n       /* The offset embedded in MEM_REFs can be negative.  Bias them\n \t so that the resulting offset adjustment is positive.  */\n       moff = mem_ref_offset (base1);\n-      moff = double_int_lshift (moff,\n-\t\t\t\tBITS_PER_UNIT == 8\n-\t\t\t\t? 3 : exact_log2 (BITS_PER_UNIT),\n-\t\t\t\tHOST_BITS_PER_DOUBLE_INT, true);\n-      if (double_int_negative_p (moff))\n-\toffset2 += double_int_neg (moff).low;\n+      moff = moff.alshift (BITS_PER_UNIT == 8\n+\t\t\t   ? 3 : exact_log2 (BITS_PER_UNIT),\n+\t\t\t   HOST_BITS_PER_DOUBLE_INT);\n+      if (moff.is_negative ())\n+\toffset2 += (-moff).low;\n       else\n \toffset1 += moff.low;\n       moff = mem_ref_offset (base2);\n-      moff = double_int_lshift (moff,\n-\t\t\t\tBITS_PER_UNIT == 8\n-\t\t\t\t? 3 : exact_log2 (BITS_PER_UNIT),\n-\t\t\t\tHOST_BITS_PER_DOUBLE_INT, true);\n-      if (double_int_negative_p (moff))\n-\toffset1 += double_int_neg (moff).low;\n+      moff = moff.alshift (BITS_PER_UNIT == 8\n+\t\t\t   ? 3 : exact_log2 (BITS_PER_UNIT),\n+\t\t\t   HOST_BITS_PER_DOUBLE_INT);\n+      if (moff.is_negative ())\n+\toffset1 += (-moff).low;\n       else\n \toffset2 += moff.low;\n       return ranges_overlap_p (offset1, max_size1, offset2, max_size2);"}, {"sha": "830f6f334603d9c9f225ca90774c9b242b7a78ce", "filename": "gcc/tree-ssa-ccp.c", "status": "modified", "additions": 67, "deletions": 89, "changes": 156, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-ccp.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-ccp.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-ccp.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -186,12 +186,11 @@ dump_lattice_value (FILE *outf, const char *prefix, prop_value_t val)\n     case CONSTANT:\n       fprintf (outf, \"%sCONSTANT \", prefix);\n       if (TREE_CODE (val.value) != INTEGER_CST\n-\t  || double_int_zero_p (val.mask))\n+\t  || val.mask.is_zero ())\n \tprint_generic_expr (outf, val.value, dump_flags);\n       else\n \t{\n-\t  double_int cval = double_int_and_not (tree_to_double_int (val.value),\n-\t\t\t\t\t\tval.mask);\n+\t  double_int cval = tree_to_double_int (val.value).and_not (val.mask);\n \t  fprintf (outf, \"%sCONSTANT \" HOST_WIDE_INT_PRINT_DOUBLE_HEX,\n \t\t   prefix, cval.high, cval.low);\n \t  fprintf (outf, \" (\" HOST_WIDE_INT_PRINT_DOUBLE_HEX \")\",\n@@ -323,7 +322,7 @@ get_constant_value (tree var)\n   if (val\n       && val->lattice_val == CONSTANT\n       && (TREE_CODE (val->value) != INTEGER_CST\n-\t  || double_int_zero_p (val->mask)))\n+\t  || val->mask.is_zero ()))\n     return val->value;\n   return NULL_TREE;\n }\n@@ -414,11 +413,8 @@ valid_lattice_transition (prop_value_t old_val, prop_value_t new_val)\n   /* Bit-lattices have to agree in the still valid bits.  */\n   if (TREE_CODE (old_val.value) == INTEGER_CST\n       && TREE_CODE (new_val.value) == INTEGER_CST)\n-    return double_int_equal_p\n-\t\t(double_int_and_not (tree_to_double_int (old_val.value),\n-\t\t\t\t     new_val.mask),\n-\t\t double_int_and_not (tree_to_double_int (new_val.value),\n-\t\t\t\t     new_val.mask));\n+    return tree_to_double_int (old_val.value).and_not (new_val.mask)\n+\t   == tree_to_double_int (new_val.value).and_not (new_val.mask);\n \n   /* Otherwise constant values have to agree.  */\n   return operand_equal_p (old_val.value, new_val.value, 0);\n@@ -444,10 +440,9 @@ set_lattice_value (tree var, prop_value_t new_val)\n       && TREE_CODE (old_val->value) == INTEGER_CST)\n     {\n       double_int diff;\n-      diff = double_int_xor (tree_to_double_int (new_val.value),\n-\t\t\t     tree_to_double_int (old_val->value));\n-      new_val.mask = double_int_ior (new_val.mask,\n-\t\t\t\t     double_int_ior (old_val->mask, diff));\n+      diff = tree_to_double_int (new_val.value)\n+\t     ^ tree_to_double_int (old_val->value);\n+      new_val.mask = new_val.mask | old_val->mask | diff;\n     }\n \n   gcc_assert (valid_lattice_transition (*old_val, new_val));\n@@ -458,7 +453,7 @@ set_lattice_value (tree var, prop_value_t new_val)\n       || (new_val.lattice_val == CONSTANT\n \t  && TREE_CODE (new_val.value) == INTEGER_CST\n \t  && (TREE_CODE (old_val->value) != INTEGER_CST\n-\t      || !double_int_equal_p (new_val.mask, old_val->mask))))\n+\t      || new_val.mask != old_val->mask)))\n     {\n       /* ???  We would like to delay creation of INTEGER_CSTs from\n \t partially constants here.  */\n@@ -511,15 +506,15 @@ get_value_from_alignment (tree expr)\n   gcc_assert (TREE_CODE (expr) == ADDR_EXPR);\n \n   get_pointer_alignment_1 (expr, &align, &bitpos);\n-  val.mask\n-    = double_int_and_not (POINTER_TYPE_P (type) || TYPE_UNSIGNED (type)\n-\t\t\t  ? double_int_mask (TYPE_PRECISION (type))\n-\t\t\t  : double_int_minus_one,\n-\t\t\t  uhwi_to_double_int (align / BITS_PER_UNIT - 1));\n-  val.lattice_val = double_int_minus_one_p (val.mask) ? VARYING : CONSTANT;\n+  val.mask = (POINTER_TYPE_P (type) || TYPE_UNSIGNED (type)\n+\t      ? double_int::mask (TYPE_PRECISION (type))\n+\t      : double_int_minus_one)\n+\t     .and_not (double_int::from_uhwi (align / BITS_PER_UNIT - 1));\n+  val.lattice_val = val.mask.is_minus_one () ? VARYING : CONSTANT;\n   if (val.lattice_val == CONSTANT)\n     val.value\n-      = double_int_to_tree (type, uhwi_to_double_int (bitpos / BITS_PER_UNIT));\n+      = double_int_to_tree (type,\n+\t\t\t    double_int::from_uhwi (bitpos / BITS_PER_UNIT));\n   else\n     val.value = NULL_TREE;\n \n@@ -880,12 +875,10 @@ ccp_lattice_meet (prop_value_t *val1, prop_value_t *val2)\n \n          For INTEGER_CSTs mask unequal bits.  If no equal bits remain,\n \t drop to varying.  */\n-      val1->mask\n-\t  = double_int_ior (double_int_ior (val1->mask,\n-\t\t\t\t\t    val2->mask),\n-\t\t\t    double_int_xor (tree_to_double_int (val1->value),\n-\t\t\t\t\t    tree_to_double_int (val2->value)));\n-      if (double_int_minus_one_p (val1->mask))\n+      val1->mask = val1->mask | val2->mask\n+\t\t   | (tree_to_double_int (val1->value)\n+\t\t      ^ tree_to_double_int (val2->value));\n+      if (val1->mask.is_minus_one ())\n \t{\n \t  val1->lattice_val = VARYING;\n \t  val1->value = NULL_TREE;\n@@ -1080,7 +1073,7 @@ bit_value_unop_1 (enum tree_code code, tree type,\n     {\n     case BIT_NOT_EXPR:\n       *mask = rmask;\n-      *val = double_int_not (rval);\n+      *val = ~rval;\n       break;\n \n     case NEGATE_EXPR:\n@@ -1100,13 +1093,13 @@ bit_value_unop_1 (enum tree_code code, tree type,\n \n \t/* First extend mask and value according to the original type.  */\n \tuns = TYPE_UNSIGNED (rtype);\n-\t*mask = double_int_ext (rmask, TYPE_PRECISION (rtype), uns);\n-\t*val = double_int_ext (rval, TYPE_PRECISION (rtype), uns);\n+\t*mask = rmask.ext (TYPE_PRECISION (rtype), uns);\n+\t*val = rval.ext (TYPE_PRECISION (rtype), uns);\n \n \t/* Then extend mask and value according to the target type.  */\n \tuns = TYPE_UNSIGNED (type);\n-\t*mask = double_int_ext (*mask, TYPE_PRECISION (type), uns);\n-\t*val = double_int_ext (*val, TYPE_PRECISION (type), uns);\n+\t*mask = (*mask).ext (TYPE_PRECISION (type), uns);\n+\t*val = (*val).ext (TYPE_PRECISION (type), uns);\n \tbreak;\n       }\n \n@@ -1135,37 +1128,33 @@ bit_value_binop_1 (enum tree_code code, tree type,\n     case BIT_AND_EXPR:\n       /* The mask is constant where there is a known not\n \t set bit, (m1 | m2) & ((v1 | m1) & (v2 | m2)) */\n-      *mask = double_int_and (double_int_ior (r1mask, r2mask),\n-\t\t\t      double_int_and (double_int_ior (r1val, r1mask),\n-\t\t\t\t\t      double_int_ior (r2val, r2mask)));\n-      *val = double_int_and (r1val, r2val);\n+      *mask = (r1mask | r2mask) & (r1val | r1mask) & (r2val | r2mask);\n+      *val = r1val & r2val;\n       break;\n \n     case BIT_IOR_EXPR:\n       /* The mask is constant where there is a known\n \t set bit, (m1 | m2) & ~((v1 & ~m1) | (v2 & ~m2)).  */\n-      *mask = double_int_and_not\n-\t  \t(double_int_ior (r1mask, r2mask),\n-\t\t double_int_ior (double_int_and_not (r1val, r1mask),\n-\t\t\t\t double_int_and_not (r2val, r2mask)));\n-      *val = double_int_ior (r1val, r2val);\n+      *mask = (r1mask | r2mask)\n+\t      .and_not (r1val.and_not (r1mask) | r2val.and_not (r2mask));\n+      *val = r1val | r2val;\n       break;\n \n     case BIT_XOR_EXPR:\n       /* m1 | m2  */\n-      *mask = double_int_ior (r1mask, r2mask);\n-      *val = double_int_xor (r1val, r2val);\n+      *mask = r1mask | r2mask;\n+      *val = r1val ^ r2val;\n       break;\n \n     case LROTATE_EXPR:\n     case RROTATE_EXPR:\n-      if (double_int_zero_p (r2mask))\n+      if (r2mask.is_zero ())\n \t{\n \t  HOST_WIDE_INT shift = r2val.low;\n \t  if (code == RROTATE_EXPR)\n \t    shift = -shift;\n-\t  *mask = double_int_lrotate (r1mask, shift, TYPE_PRECISION (type));\n-\t  *val = double_int_lrotate (r1val, shift, TYPE_PRECISION (type));\n+\t  *mask = r1mask.lrotate (shift, TYPE_PRECISION (type));\n+\t  *val = r1val.lrotate (shift, TYPE_PRECISION (type));\n \t}\n       break;\n \n@@ -1174,7 +1163,7 @@ bit_value_binop_1 (enum tree_code code, tree type,\n       /* ???  We can handle partially known shift counts if we know\n \t its sign.  That way we can tell that (x << (y | 8)) & 255\n \t is zero.  */\n-      if (double_int_zero_p (r2mask))\n+      if (r2mask.is_zero ())\n \t{\n \t  HOST_WIDE_INT shift = r2val.low;\n \t  if (code == RSHIFT_EXPR)\n@@ -1186,18 +1175,14 @@ bit_value_binop_1 (enum tree_code code, tree type,\n \t     the sign bit was varying.  */\n \t  if (shift > 0)\n \t    {\n-\t      *mask = double_int_lshift (r1mask, shift,\n-\t\t\t\t\t TYPE_PRECISION (type), false);\n-\t      *val = double_int_lshift (r1val, shift,\n-\t\t\t\t\tTYPE_PRECISION (type), false);\n+\t      *mask = r1mask.llshift (shift, TYPE_PRECISION (type));\n+\t      *val = r1val.llshift (shift, TYPE_PRECISION (type));\n \t    }\n \t  else if (shift < 0)\n \t    {\n \t      shift = -shift;\n-\t      *mask = double_int_rshift (r1mask, shift,\n-\t\t\t\t\t TYPE_PRECISION (type), !uns);\n-\t      *val = double_int_rshift (r1val, shift,\n-\t\t\t\t\tTYPE_PRECISION (type), !uns);\n+\t      *mask = r1mask.rshift (shift, TYPE_PRECISION (type), !uns);\n+\t      *val = r1val.rshift (shift, TYPE_PRECISION (type), !uns);\n \t    }\n \t  else\n \t    {\n@@ -1213,21 +1198,18 @@ bit_value_binop_1 (enum tree_code code, tree type,\n \tdouble_int lo, hi;\n \t/* Do the addition with unknown bits set to zero, to give carry-ins of\n \t   zero wherever possible.  */\n-\tlo = double_int_add (double_int_and_not (r1val, r1mask),\n-\t\t\t     double_int_and_not (r2val, r2mask));\n-\tlo = double_int_ext (lo, TYPE_PRECISION (type), uns);\n+\tlo = r1val.and_not (r1mask) + r2val.and_not (r2mask);\n+\tlo = lo.ext (TYPE_PRECISION (type), uns);\n \t/* Do the addition with unknown bits set to one, to give carry-ins of\n \t   one wherever possible.  */\n-\thi = double_int_add (double_int_ior (r1val, r1mask),\n-\t\t\t     double_int_ior (r2val, r2mask));\n-\thi = double_int_ext (hi, TYPE_PRECISION (type), uns);\n+\thi = (r1val | r1mask) + (r2val | r2mask);\n+\thi = hi.ext (TYPE_PRECISION (type), uns);\n \t/* Each bit in the result is known if (a) the corresponding bits in\n \t   both inputs are known, and (b) the carry-in to that bit position\n \t   is known.  We can check condition (b) by seeing if we got the same\n \t   result with minimised carries as with maximised carries.  */\n-\t*mask = double_int_ior (double_int_ior (r1mask, r2mask),\n-\t\t\t\tdouble_int_xor (lo, hi));\n-\t*mask = double_int_ext (*mask, TYPE_PRECISION (type), uns);\n+\t*mask = r1mask | r2mask | (lo ^ hi);\n+\t*mask = (*mask).ext (TYPE_PRECISION (type), uns);\n \t/* It shouldn't matter whether we choose lo or hi here.  */\n \t*val = lo;\n \tbreak;\n@@ -1248,17 +1230,17 @@ bit_value_binop_1 (enum tree_code code, tree type,\n       {\n \t/* Just track trailing zeros in both operands and transfer\n \t   them to the other.  */\n-\tint r1tz = double_int_ctz (double_int_ior (r1val, r1mask));\n-\tint r2tz = double_int_ctz (double_int_ior (r2val, r2mask));\n+\tint r1tz = (r1val | r1mask).trailing_zeros ();\n+\tint r2tz = (r2val | r2mask).trailing_zeros ();\n \tif (r1tz + r2tz >= HOST_BITS_PER_DOUBLE_INT)\n \t  {\n \t    *mask = double_int_zero;\n \t    *val = double_int_zero;\n \t  }\n \telse if (r1tz + r2tz > 0)\n \t  {\n-\t    *mask = double_int_not (double_int_mask (r1tz + r2tz));\n-\t    *mask = double_int_ext (*mask, TYPE_PRECISION (type), uns);\n+\t    *mask = ~double_int::mask (r1tz + r2tz);\n+\t    *mask = (*mask).ext (TYPE_PRECISION (type), uns);\n \t    *val = double_int_zero;\n \t  }\n \tbreak;\n@@ -1267,9 +1249,8 @@ bit_value_binop_1 (enum tree_code code, tree type,\n     case EQ_EXPR:\n     case NE_EXPR:\n       {\n-\tdouble_int m = double_int_ior (r1mask, r2mask);\n-\tif (!double_int_equal_p (double_int_and_not (r1val, m),\n-\t\t\t\t double_int_and_not (r2val, m)))\n+\tdouble_int m = r1mask | r2mask;\n+\tif (r1val.and_not (m) != r2val.and_not (m))\n \t  {\n \t    *mask = double_int_zero;\n \t    *val = ((code == EQ_EXPR) ? double_int_zero : double_int_one);\n@@ -1300,7 +1281,7 @@ bit_value_binop_1 (enum tree_code code, tree type,\n       {\n \tint minmax, maxmin;\n \t/* If the most significant bits are not known we know nothing.  */\n-\tif (double_int_negative_p (r1mask) || double_int_negative_p (r2mask))\n+\tif (r1mask.is_negative () || r2mask.is_negative ())\n \t  break;\n \n \t/* For comparisons the signedness is in the comparison operands.  */\n@@ -1309,10 +1290,8 @@ bit_value_binop_1 (enum tree_code code, tree type,\n \t/* If we know the most significant bits we know the values\n \t   value ranges by means of treating varying bits as zero\n \t   or one.  Do a cross comparison of the max/min pairs.  */\n-\tmaxmin = double_int_cmp (double_int_ior (r1val, r1mask),\n-\t\t\t\t double_int_and_not (r2val, r2mask), uns);\n-\tminmax = double_int_cmp (double_int_and_not (r1val, r1mask),\n-\t\t\t\t double_int_ior (r2val, r2mask), uns);\n+\tmaxmin = (r1val | r1mask).cmp (r2val.and_not (r2mask), uns);\n+\tminmax = r1val.and_not (r1mask).cmp (r2val | r2mask, uns);\n \tif (maxmin < 0)  /* r1 is less than r2.  */\n \t  {\n \t    *mask = double_int_zero;\n@@ -1358,10 +1337,10 @@ bit_value_unop (enum tree_code code, tree type, tree rhs)\n \n   gcc_assert ((rval.lattice_val == CONSTANT\n \t       && TREE_CODE (rval.value) == INTEGER_CST)\n-\t      || double_int_minus_one_p (rval.mask));\n+\t      || rval.mask.is_minus_one ());\n   bit_value_unop_1 (code, type, &value, &mask,\n \t\t    TREE_TYPE (rhs), value_to_double_int (rval), rval.mask);\n-  if (!double_int_minus_one_p (mask))\n+  if (!mask.is_minus_one ())\n     {\n       val.lattice_val = CONSTANT;\n       val.mask = mask;\n@@ -1399,14 +1378,14 @@ bit_value_binop (enum tree_code code, tree type, tree rhs1, tree rhs2)\n \n   gcc_assert ((r1val.lattice_val == CONSTANT\n \t       && TREE_CODE (r1val.value) == INTEGER_CST)\n-\t      || double_int_minus_one_p (r1val.mask));\n+\t      || r1val.mask.is_minus_one ());\n   gcc_assert ((r2val.lattice_val == CONSTANT\n \t       && TREE_CODE (r2val.value) == INTEGER_CST)\n-\t      || double_int_minus_one_p (r2val.mask));\n+\t      || r2val.mask.is_minus_one ());\n   bit_value_binop_1 (code, type, &value, &mask,\n \t\t     TREE_TYPE (rhs1), value_to_double_int (r1val), r1val.mask,\n \t\t     TREE_TYPE (rhs2), value_to_double_int (r2val), r2val.mask);\n-  if (!double_int_minus_one_p (mask))\n+  if (!mask.is_minus_one ())\n     {\n       val.lattice_val = CONSTANT;\n       val.mask = mask;\n@@ -1439,7 +1418,7 @@ bit_value_assume_aligned (gimple stmt)\n     return ptrval;\n   gcc_assert ((ptrval.lattice_val == CONSTANT\n \t       && TREE_CODE (ptrval.value) == INTEGER_CST)\n-\t      || double_int_minus_one_p (ptrval.mask));\n+\t      || ptrval.mask.is_minus_one ());\n   align = gimple_call_arg (stmt, 1);\n   if (!host_integerp (align, 1))\n     return ptrval;\n@@ -1461,7 +1440,7 @@ bit_value_assume_aligned (gimple stmt)\n   bit_value_binop_1 (BIT_AND_EXPR, type, &value, &mask,\n \t\t     type, value_to_double_int (ptrval), ptrval.mask,\n \t\t     type, value_to_double_int (alignval), alignval.mask);\n-  if (!double_int_minus_one_p (mask))\n+  if (!mask.is_minus_one ())\n     {\n       val.lattice_val = CONSTANT;\n       val.mask = mask;\n@@ -1625,7 +1604,7 @@ evaluate_stmt (gimple stmt)\n \t    case BUILT_IN_STRNDUP:\n \t      val.lattice_val = CONSTANT;\n \t      val.value = build_int_cst (TREE_TYPE (gimple_get_lhs (stmt)), 0);\n-\t      val.mask = shwi_to_double_int\n+\t      val.mask = double_int::from_shwi\n \t\t  \t   (~(((HOST_WIDE_INT) MALLOC_ABI_ALIGNMENT)\n \t\t\t      / BITS_PER_UNIT - 1));\n \t      break;\n@@ -1637,9 +1616,8 @@ evaluate_stmt (gimple stmt)\n \t\t       : BIGGEST_ALIGNMENT);\n \t      val.lattice_val = CONSTANT;\n \t      val.value = build_int_cst (TREE_TYPE (gimple_get_lhs (stmt)), 0);\n-\t      val.mask = shwi_to_double_int\n-\t\t  \t   (~(((HOST_WIDE_INT) align)\n-\t\t\t      / BITS_PER_UNIT - 1));\n+\t      val.mask = double_int::from_shwi (~(((HOST_WIDE_INT) align)\n+\t\t\t\t\t\t  / BITS_PER_UNIT - 1));\n \t      break;\n \n \t    /* These builtins return their first argument, unmodified.  */\n@@ -1857,7 +1835,7 @@ ccp_fold_stmt (gimple_stmt_iterator *gsi)\n \t   fold more conditionals here.  */\n \tval = evaluate_stmt (stmt);\n \tif (val.lattice_val != CONSTANT\n-\t    || !double_int_zero_p (val.mask))\n+\t    || !val.mask.is_zero ())\n \t  return false;\n \n \tif (dump_file)\n@@ -2037,7 +2015,7 @@ visit_cond_stmt (gimple stmt, edge *taken_edge_p)\n   block = gimple_bb (stmt);\n   val = evaluate_stmt (stmt);\n   if (val.lattice_val != CONSTANT\n-      || !double_int_zero_p (val.mask))\n+      || !val.mask.is_zero ())\n     return SSA_PROP_VARYING;\n \n   /* Find which edge out of the conditional block will be taken and add it"}, {"sha": "3f5b2f99f5aa81994544af8a10dbfce2d89d6175", "filename": "gcc/tree-ssa-forwprop.c", "status": "modified", "additions": 6, "deletions": 9, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-forwprop.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-forwprop.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-forwprop.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -813,11 +813,10 @@ forward_propagate_addr_expr_1 (tree name, tree def_rhs,\n \t{\n \t  double_int off = mem_ref_offset (lhs);\n \t  tree new_ptr;\n-\t  off = double_int_add (off,\n-\t\t\t\tshwi_to_double_int (def_rhs_offset));\n+\t  off += double_int::from_shwi (def_rhs_offset);\n \t  if (TREE_CODE (def_rhs_base) == MEM_REF)\n \t    {\n-\t      off = double_int_add (off, mem_ref_offset (def_rhs_base));\n+\t      off += mem_ref_offset (def_rhs_base);\n \t      new_ptr = TREE_OPERAND (def_rhs_base, 0);\n \t    }\n \t  else\n@@ -898,11 +897,10 @@ forward_propagate_addr_expr_1 (tree name, tree def_rhs,\n \t{\n \t  double_int off = mem_ref_offset (rhs);\n \t  tree new_ptr;\n-\t  off = double_int_add (off,\n-\t\t\t\tshwi_to_double_int (def_rhs_offset));\n+\t  off += double_int::from_shwi (def_rhs_offset);\n \t  if (TREE_CODE (def_rhs_base) == MEM_REF)\n \t    {\n-\t      off = double_int_add (off, mem_ref_offset (def_rhs_base));\n+\t      off += mem_ref_offset (def_rhs_base);\n \t      new_ptr = TREE_OPERAND (def_rhs_base, 0);\n \t    }\n \t  else\n@@ -2373,8 +2371,7 @@ associate_pointerplus (gimple_stmt_iterator *gsi)\n   if (gimple_assign_rhs1 (def_stmt) != ptr)\n     return false;\n \n-  algn = double_int_to_tree (TREE_TYPE (ptr),\n-\t\t\t     double_int_not (tree_to_double_int (algn)));\n+  algn = double_int_to_tree (TREE_TYPE (ptr), ~tree_to_double_int (algn));\n   gimple_assign_set_rhs_with_ops (gsi, BIT_AND_EXPR, ptr, algn);\n   fold_stmt_inplace (gsi);\n   update_stmt (stmt);\n@@ -2537,7 +2534,7 @@ combine_conversions (gimple_stmt_iterator *gsi)\n \t  tem = fold_build2 (BIT_AND_EXPR, inside_type,\n \t\t\t     defop0,\n \t\t\t     double_int_to_tree\n-\t\t\t       (inside_type, double_int_mask (inter_prec)));\n+\t\t\t       (inside_type, double_int::mask (inter_prec)));\n \t  if (!useless_type_conversion_p (type, inside_type))\n \t    {\n \t      tem = force_gimple_operand_gsi (gsi, tem, true, NULL_TREE, true,"}, {"sha": "74097f8cccf16c5196ddb93128ad7f109c3109e6", "filename": "gcc/tree-ssa-loop-ivopts.c", "status": "modified", "additions": 17, "deletions": 19, "changes": 36, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-loop-ivopts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-loop-ivopts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-loop-ivopts.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -1571,8 +1571,7 @@ constant_multiple_of (tree top, tree bot, double_int *mul)\n       if (!constant_multiple_of (TREE_OPERAND (top, 0), bot, &res))\n \treturn false;\n \n-      *mul = double_int_sext (double_int_mul (res, tree_to_double_int (mby)),\n-\t\t\t      precision);\n+      *mul = (res * tree_to_double_int (mby)).sext (precision);\n       return true;\n \n     case PLUS_EXPR:\n@@ -1582,21 +1581,20 @@ constant_multiple_of (tree top, tree bot, double_int *mul)\n \treturn false;\n \n       if (code == MINUS_EXPR)\n-\tp1 = double_int_neg (p1);\n-      *mul = double_int_sext (double_int_add (p0, p1), precision);\n+\tp1 = -p1;\n+      *mul = (p0 + p1).sext (precision);\n       return true;\n \n     case INTEGER_CST:\n       if (TREE_CODE (bot) != INTEGER_CST)\n \treturn false;\n \n-      p0 = double_int_sext (tree_to_double_int (top), precision);\n-      p1 = double_int_sext (tree_to_double_int (bot), precision);\n-      if (double_int_zero_p (p1))\n+      p0 = tree_to_double_int (top).sext (precision);\n+      p1 = tree_to_double_int (bot).sext (precision);\n+      if (p1.is_zero ())\n \treturn false;\n-      *mul = double_int_sext (double_int_sdivmod (p0, p1, FLOOR_DIV_EXPR, &res),\n-\t\t\t      precision);\n-      return double_int_zero_p (res);\n+      *mul = p0.sdivmod (p1, FLOOR_DIV_EXPR, &res).sext (precision);\n+      return res.is_zero ();\n \n     default:\n       return false;\n@@ -3000,7 +2998,7 @@ get_computation_aff (struct loop *loop,\n       aff_combination_add (&cbase_aff, &cstep_aff);\n     }\n \n-  aff_combination_scale (&cbase_aff, double_int_neg (rat));\n+  aff_combination_scale (&cbase_aff, -rat);\n   aff_combination_add (aff, &cbase_aff);\n   if (common_type != uutype)\n     aff_combination_convert (aff, uutype);\n@@ -3777,7 +3775,7 @@ compare_aff_trees (aff_tree *aff1, aff_tree *aff2)\n \n   for (i = 0; i < aff1->n; i++)\n     {\n-      if (double_int_cmp (aff1->elts[i].coef, aff2->elts[i].coef, 0) != 0)\n+      if (aff1->elts[i].coef != aff2->elts[i].coef)\n         return false;\n \n       if (!operand_equal_p (aff1->elts[i].val, aff2->elts[i].val, 0))\n@@ -3904,7 +3902,7 @@ get_loop_invariant_expr_id (struct ivopts_data *data, tree ubase,\n   tree_to_aff_combination (ub, TREE_TYPE (ub), &ubase_aff);\n   tree_to_aff_combination (cb, TREE_TYPE (cb), &cbase_aff);\n \n-  aff_combination_scale (&cbase_aff, shwi_to_double_int (-1 * ratio));\n+  aff_combination_scale (&cbase_aff, double_int::from_shwi (-1 * ratio));\n   aff_combination_add (&ubase_aff, &cbase_aff);\n   expr = aff_combination_to_tree (&ubase_aff);\n   return get_expr_id (data, expr);\n@@ -3990,8 +3988,8 @@ get_computation_cost_at (struct ivopts_data *data,\n   if (!constant_multiple_of (ustep, cstep, &rat))\n     return infinite_cost;\n \n-  if (double_int_fits_in_shwi_p (rat))\n-    ratio = double_int_to_shwi (rat);\n+  if (rat.fits_shwi ())\n+    ratio = rat.to_shwi ();\n   else\n     return infinite_cost;\n \n@@ -4504,7 +4502,7 @@ iv_elimination_compare_lt (struct ivopts_data *data,\n   aff_combination_scale (&tmpa, double_int_minus_one);\n   aff_combination_add (&tmpb, &tmpa);\n   aff_combination_add (&tmpb, &nit);\n-  if (tmpb.n != 0 || !double_int_equal_p (tmpb.offset, double_int_one))\n+  if (tmpb.n != 0 || tmpb.offset != double_int_one)\n     return false;\n \n   /* Finally, check that CAND->IV->BASE - CAND->IV->STEP * A does not\n@@ -4594,17 +4592,17 @@ may_eliminate_iv (struct ivopts_data *data,\n \n       max_niter = desc->max;\n       if (stmt_after_increment (loop, cand, use->stmt))\n-        max_niter = double_int_add (max_niter, double_int_one);\n+        max_niter += double_int_one;\n       period_value = tree_to_double_int (period);\n-      if (double_int_ucmp (max_niter, period_value) > 0)\n+      if (max_niter.ugt (period_value))\n         {\n           /* See if we can take advantage of inferred loop bound information.  */\n           if (data->loop_single_exit_p)\n             {\n               if (!max_loop_iterations (loop, &max_niter))\n                 return false;\n               /* The loop bound is already adjusted by adding 1.  */\n-              if (double_int_ucmp (max_niter, period_value) > 0)\n+              if (max_niter.ugt (period_value))\n                 return false;\n             }\n           else"}, {"sha": "84ae6104490533647942ee4ccc66bc42e3b670a4", "filename": "gcc/tree-ssa-loop-niter.c", "status": "modified", "additions": 35, "deletions": 37, "changes": 72, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-loop-niter.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-loop-niter.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-loop-niter.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -91,7 +91,7 @@ split_to_var_and_offset (tree expr, tree *var, mpz_t offset)\n       *var = op0;\n       /* Always sign extend the offset.  */\n       off = tree_to_double_int (op1);\n-      off = double_int_sext (off, TYPE_PRECISION (type));\n+      off = off.sext (TYPE_PRECISION (type));\n       mpz_set_double_int (offset, off, false);\n       if (negate)\n \tmpz_neg (offset, offset);\n@@ -170,7 +170,7 @@ bound_difference_of_offsetted_base (tree type, mpz_t x, mpz_t y,\n     }\n \n   mpz_init (m);\n-  mpz_set_double_int (m, double_int_mask (TYPE_PRECISION (type)), true);\n+  mpz_set_double_int (m, double_int::mask (TYPE_PRECISION (type)), true);\n   mpz_add_ui (m, m, 1);\n   mpz_sub (bnds->up, x, y);\n   mpz_set (bnds->below, bnds->up);\n@@ -457,7 +457,7 @@ bounds_add (bounds *bnds, double_int delta, tree type)\n   mpz_set_double_int (mdelta, delta, false);\n \n   mpz_init (max);\n-  mpz_set_double_int (max, double_int_mask (TYPE_PRECISION (type)), true);\n+  mpz_set_double_int (max, double_int::mask (TYPE_PRECISION (type)), true);\n \n   mpz_add (bnds->up, bnds->up, mdelta);\n   mpz_add (bnds->below, bnds->below, mdelta);\n@@ -573,15 +573,15 @@ number_of_iterations_ne_max (mpz_t bnd, bool no_overflow, tree c, tree s,\n      the whole # of iterations analysis will fail).  */\n   if (!no_overflow)\n     {\n-      max = double_int_mask (TYPE_PRECISION (TREE_TYPE (c))\n+      max = double_int::mask (TYPE_PRECISION (TREE_TYPE (c))\n \t\t\t     - tree_low_cst (num_ending_zeros (s), 1));\n       mpz_set_double_int (bnd, max, true);\n       return;\n     }\n \n   /* Now we know that the induction variable does not overflow, so the loop\n      iterates at most (range of type / S) times.  */\n-  mpz_set_double_int (bnd, double_int_mask (TYPE_PRECISION (TREE_TYPE (c))),\n+  mpz_set_double_int (bnd, double_int::mask (TYPE_PRECISION (TREE_TYPE (c))),\n \t\t      true);\n \n   /* If the induction variable is guaranteed to reach the value of C before\n@@ -922,9 +922,8 @@ assert_loop_rolls_lt (tree type, affine_iv *iv0, affine_iv *iv1,\n     dstep = tree_to_double_int (iv0->step);\n   else\n     {\n-      dstep = double_int_sext (tree_to_double_int (iv1->step),\n-\t\t\t       TYPE_PRECISION (type));\n-      dstep = double_int_neg (dstep);\n+      dstep = tree_to_double_int (iv1->step).sext (TYPE_PRECISION (type));\n+      dstep = -dstep;\n     }\n \n   mpz_init (mstep);\n@@ -935,7 +934,7 @@ assert_loop_rolls_lt (tree type, affine_iv *iv0, affine_iv *iv1,\n   rolls_p = mpz_cmp (mstep, bnds->below) <= 0;\n \n   mpz_init (max);\n-  mpz_set_double_int (max, double_int_mask (TYPE_PRECISION (type)), true);\n+  mpz_set_double_int (max, double_int::mask (TYPE_PRECISION (type)), true);\n   mpz_add (max, max, mstep);\n   no_overflow_p = (mpz_cmp (bnds->up, max) <= 0\n \t\t   /* For pointers, only values lying inside a single object\n@@ -2394,7 +2393,7 @@ derive_constant_upper_bound_ops (tree type, tree op0,\n \n       /* If the bound does not fit in TYPE, max. value of TYPE could be\n \t attained.  */\n-      if (double_int_ucmp (max, bnd) < 0)\n+      if (max.ult (bnd))\n \treturn max;\n \n       return bnd;\n@@ -2410,27 +2409,27 @@ derive_constant_upper_bound_ops (tree type, tree op0,\n \t choose the most logical way how to treat this constant regardless\n \t of the signedness of the type.  */\n       cst = tree_to_double_int (op1);\n-      cst = double_int_sext (cst, TYPE_PRECISION (type));\n+      cst = cst.sext (TYPE_PRECISION (type));\n       if (code != MINUS_EXPR)\n-\tcst = double_int_neg (cst);\n+\tcst = -cst;\n \n       bnd = derive_constant_upper_bound (op0);\n \n-      if (double_int_negative_p (cst))\n+      if (cst.is_negative ())\n \t{\n-\t  cst = double_int_neg (cst);\n+\t  cst = -cst;\n \t  /* Avoid CST == 0x80000...  */\n-\t  if (double_int_negative_p (cst))\n+\t  if (cst.is_negative ())\n \t    return max;;\n \n \t  /* OP0 + CST.  We need to check that\n \t     BND <= MAX (type) - CST.  */\n \n-\t  mmax = double_int_sub (max, cst);\n-\t  if (double_int_ucmp (bnd, mmax) > 0)\n+\t  mmax -= cst;\n+\t  if (bnd.ugt (mmax))\n \t    return max;\n \n-\t  return double_int_add (bnd, cst);\n+\t  return bnd + cst;\n \t}\n       else\n \t{\n@@ -2447,7 +2446,7 @@ derive_constant_upper_bound_ops (tree type, tree op0,\n \t  /* This should only happen if the type is unsigned; however, for\n \t     buggy programs that use overflowing signed arithmetics even with\n \t     -fno-wrapv, this condition may also be true for signed values.  */\n-\t  if (double_int_ucmp (bnd, cst) < 0)\n+\t  if (bnd.ult (cst))\n \t    return max;\n \n \t  if (TYPE_UNSIGNED (type))\n@@ -2458,7 +2457,7 @@ derive_constant_upper_bound_ops (tree type, tree op0,\n \t\treturn max;\n \t    }\n \n-\t  bnd = double_int_sub (bnd, cst);\n+\t  bnd -= cst;\n \t}\n \n       return bnd;\n@@ -2470,7 +2469,7 @@ derive_constant_upper_bound_ops (tree type, tree op0,\n \treturn max;\n \n       bnd = derive_constant_upper_bound (op0);\n-      return double_int_udiv (bnd, tree_to_double_int (op1), FLOOR_DIV_EXPR);\n+      return bnd.udiv (tree_to_double_int (op1), FLOOR_DIV_EXPR);\n \n     case BIT_AND_EXPR:\n       if (TREE_CODE (op1) != INTEGER_CST\n@@ -2503,14 +2502,14 @@ record_niter_bound (struct loop *loop, double_int i_bound, bool realistic,\n      current estimation is smaller.  */\n   if (upper\n       && (!loop->any_upper_bound\n-\t  || double_int_ucmp (i_bound, loop->nb_iterations_upper_bound) < 0))\n+\t  || i_bound.ult (loop->nb_iterations_upper_bound)))\n     {\n       loop->any_upper_bound = true;\n       loop->nb_iterations_upper_bound = i_bound;\n     }\n   if (realistic\n       && (!loop->any_estimate\n-\t  || double_int_ucmp (i_bound, loop->nb_iterations_estimate) < 0))\n+\t  || i_bound.ult (loop->nb_iterations_estimate)))\n     {\n       loop->any_estimate = true;\n       loop->nb_iterations_estimate = i_bound;\n@@ -2520,8 +2519,7 @@ record_niter_bound (struct loop *loop, double_int i_bound, bool realistic,\n      number of iterations, use the upper bound instead.  */\n   if (loop->any_upper_bound\n       && loop->any_estimate\n-      && double_int_ucmp (loop->nb_iterations_upper_bound,\n-\t\t\t  loop->nb_iterations_estimate) < 0)\n+      && loop->nb_iterations_upper_bound.ult (loop->nb_iterations_estimate))\n     loop->nb_iterations_estimate = loop->nb_iterations_upper_bound;\n }\n \n@@ -2583,10 +2581,10 @@ record_estimate (struct loop *loop, tree bound, double_int i_bound,\n     delta = double_int_zero;\n   else\n     delta = double_int_one;\n-  i_bound = double_int_add (i_bound, delta);\n+  i_bound += delta;\n \n   /* If an overflow occurred, ignore the result.  */\n-  if (double_int_ucmp (i_bound, delta) < 0)\n+  if (i_bound.ult (delta))\n     return;\n \n   record_niter_bound (loop, i_bound, realistic, upper);\n@@ -3050,9 +3048,9 @@ estimated_loop_iterations_int (struct loop *loop)\n   if (!estimated_loop_iterations (loop, &nit))\n     return -1;\n \n-  if (!double_int_fits_in_shwi_p (nit))\n+  if (!nit.fits_shwi ())\n     return -1;\n-  hwi_nit = double_int_to_shwi (nit);\n+  hwi_nit = nit.to_shwi ();\n \n   return hwi_nit < 0 ? -1 : hwi_nit;\n }\n@@ -3070,9 +3068,9 @@ max_loop_iterations_int (struct loop *loop)\n   if (!max_loop_iterations (loop, &nit))\n     return -1;\n \n-  if (!double_int_fits_in_shwi_p (nit))\n+  if (!nit.fits_shwi ())\n     return -1;\n-  hwi_nit = double_int_to_shwi (nit);\n+  hwi_nit = nit.to_shwi ();\n \n   return hwi_nit < 0 ? -1 : hwi_nit;\n }\n@@ -3129,9 +3127,9 @@ max_stmt_executions (struct loop *loop, double_int *nit)\n \n   nit_minus_one = *nit;\n \n-  *nit = double_int_add (*nit, double_int_one);\n+  *nit += double_int_one;\n \n-  return double_int_ucmp (*nit, nit_minus_one) > 0;\n+  return (*nit).ugt (nit_minus_one);\n }\n \n /* Sets NIT to the estimated number of executions of the latch of the\n@@ -3148,9 +3146,9 @@ estimated_stmt_executions (struct loop *loop, double_int *nit)\n \n   nit_minus_one = *nit;\n \n-  *nit = double_int_add (*nit, double_int_one);\n+  *nit += double_int_one;\n \n-  return double_int_ucmp (*nit, nit_minus_one) > 0;\n+  return (*nit).ugt (nit_minus_one);\n }\n \n /* Records estimates on numbers of iterations of loops.  */\n@@ -3255,8 +3253,8 @@ n_of_executions_at_most (gimple stmt,\n \t  || (gimple_bb (stmt) != gimple_bb (niter_bound->stmt)\n \t      && !stmt_dominates_stmt_p (niter_bound->stmt, stmt)))\n \t{\n-\t  bound = double_int_add (bound, double_int_one);\n-\t  if (double_int_zero_p (bound)\n+\t  bound += double_int_one;\n+\t  if (bound.is_zero ()\n \t      || !double_int_fits_to_tree_p (nit_type, bound))\n \t    return false;\n \t}"}, {"sha": "c8ec5026659e4abf17960597bc5167106e572c00", "filename": "gcc/tree-ssa-phiopt.c", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-phiopt.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-phiopt.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-phiopt.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -720,9 +720,7 @@ jump_function_from_stmt (tree *arg, gimple stmt)\n \t\t\t\t\t\t&offset);\n       if (tem\n \t  && TREE_CODE (tem) == MEM_REF\n-\t  && double_int_zero_p\n-\t       (double_int_add (mem_ref_offset (tem),\n-\t\t\t\tshwi_to_double_int (offset))))\n+\t  && (mem_ref_offset (tem) + double_int::from_shwi (offset)).is_zero ())\n \t{\n \t  *arg = TREE_OPERAND (tem, 0);\n \t  return true;"}, {"sha": "9e217b5f0a45c18206df61c172f27d4f83cf34fc", "filename": "gcc/tree-ssa-pre.c", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-pre.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-pre.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-pre.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -1600,11 +1600,9 @@ phi_translate_1 (pre_expr expr, bitmap_set_t set1, bitmap_set_t set2,\n \t\t&& TREE_CODE (op[2]) == INTEGER_CST)\n \t      {\n \t\tdouble_int off = tree_to_double_int (op[0]);\n-\t\toff = double_int_add (off,\n-\t\t\t\t      double_int_neg\n-\t\t\t\t        (tree_to_double_int (op[1])));\n-\t\toff = double_int_mul (off, tree_to_double_int (op[2]));\n-\t\tif (double_int_fits_in_shwi_p (off))\n+\t\toff += -tree_to_double_int (op[1]);\n+\t\toff *= tree_to_double_int (op[2]);\n+\t\tif (off.fits_shwi ())\n \t\t  newop.off = off.low;\n \t      }\n \t    VEC_replace (vn_reference_op_s, newoperands, j, newop);"}, {"sha": "5cc88ae224203c5eca8c2fb53fc6bacb149884a6", "filename": "gcc/tree-ssa-sccvn.c", "status": "modified", "additions": 18, "deletions": 23, "changes": 41, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-sccvn.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-sccvn.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-sccvn.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -656,13 +656,12 @@ copy_reference_ops_from_ref (tree ref, VEC(vn_reference_op_s, heap) **result)\n \t\tif (TREE_INT_CST_LOW (bit_offset) % BITS_PER_UNIT == 0)\n \t\t  {\n \t\t    double_int off\n-\t\t      = double_int_add (tree_to_double_int (this_offset),\n-\t\t\t\t\tdouble_int_rshift\n-\t\t\t\t\t  (tree_to_double_int (bit_offset),\n-\t\t\t\t\t   BITS_PER_UNIT == 8\n-\t\t\t\t\t   ? 3 : exact_log2 (BITS_PER_UNIT),\n-\t\t\t\t\t   HOST_BITS_PER_DOUBLE_INT, true));\n-\t\t    if (double_int_fits_in_shwi_p (off))\n+\t\t      = tree_to_double_int (this_offset)\n+\t\t\t+ tree_to_double_int (bit_offset)\n+\t\t\t  .arshift (BITS_PER_UNIT == 8\n+\t\t\t\t    ? 3 : exact_log2 (BITS_PER_UNIT),\n+\t\t\t\t    HOST_BITS_PER_DOUBLE_INT);\n+\t\t    if (off.fits_shwi ())\n \t\t      temp.off = off.low;\n \t\t  }\n \t      }\n@@ -680,11 +679,9 @@ copy_reference_ops_from_ref (tree ref, VEC(vn_reference_op_s, heap) **result)\n \t      && TREE_CODE (temp.op2) == INTEGER_CST)\n \t    {\n \t      double_int off = tree_to_double_int (temp.op0);\n-\t      off = double_int_add (off,\n-\t\t\t\t    double_int_neg\n-\t\t\t\t      (tree_to_double_int (temp.op1)));\n-\t      off = double_int_mul (off, tree_to_double_int (temp.op2));\n-\t      if (double_int_fits_in_shwi_p (off))\n+\t      off += -tree_to_double_int (temp.op1);\n+\t      off *= tree_to_double_int (temp.op2);\n+\t      if (off.fits_shwi ())\n \t\ttemp.off = off.low;\n \t    }\n \t  break;\n@@ -1018,8 +1015,8 @@ vn_reference_fold_indirect (VEC (vn_reference_op_s, heap) **ops,\n   if (addr_base != op->op0)\n     {\n       double_int off = tree_to_double_int (mem_op->op0);\n-      off = double_int_sext (off, TYPE_PRECISION (TREE_TYPE (mem_op->op0)));\n-      off = double_int_add (off, shwi_to_double_int (addr_offset));\n+      off = off.sext (TYPE_PRECISION (TREE_TYPE (mem_op->op0)));\n+      off += double_int::from_shwi (addr_offset);\n       mem_op->op0 = double_int_to_tree (TREE_TYPE (mem_op->op0), off);\n       op->op0 = build_fold_addr_expr (addr_base);\n       if (host_integerp (mem_op->op0, 0))\n@@ -1052,7 +1049,7 @@ vn_reference_maybe_forwprop_address (VEC (vn_reference_op_s, heap) **ops,\n     return;\n \n   off = tree_to_double_int (mem_op->op0);\n-  off = double_int_sext (off, TYPE_PRECISION (TREE_TYPE (mem_op->op0)));\n+  off = off.sext (TYPE_PRECISION (TREE_TYPE (mem_op->op0)));\n \n   /* The only thing we have to do is from &OBJ.foo.bar add the offset\n      from .foo.bar to the preceding MEM_REF offset and replace the\n@@ -1069,8 +1066,8 @@ vn_reference_maybe_forwprop_address (VEC (vn_reference_op_s, heap) **ops,\n \t  || TREE_CODE (addr_base) != MEM_REF)\n \treturn;\n \n-      off = double_int_add (off, shwi_to_double_int (addr_offset));\n-      off = double_int_add (off, mem_ref_offset (addr_base));\n+      off += double_int::from_shwi (addr_offset);\n+      off += mem_ref_offset (addr_base);\n       op->op0 = TREE_OPERAND (addr_base, 0);\n     }\n   else\n@@ -1082,7 +1079,7 @@ vn_reference_maybe_forwprop_address (VEC (vn_reference_op_s, heap) **ops,\n \t  || TREE_CODE (ptroff) != INTEGER_CST)\n \treturn;\n \n-      off = double_int_add (off, tree_to_double_int (ptroff));\n+      off += tree_to_double_int (ptroff);\n       op->op0 = ptr;\n     }\n \n@@ -1242,11 +1239,9 @@ valueize_refs_1 (VEC (vn_reference_op_s, heap) *orig, bool *valueized_anything)\n \t       && TREE_CODE (vro->op2) == INTEGER_CST)\n \t{\n \t  double_int off = tree_to_double_int (vro->op0);\n-\t  off = double_int_add (off,\n-\t\t\t\tdouble_int_neg\n-\t\t\t\t  (tree_to_double_int (vro->op1)));\n-\t  off = double_int_mul (off, tree_to_double_int (vro->op2));\n-\t  if (double_int_fits_in_shwi_p (off))\n+\t  off += -tree_to_double_int (vro->op1);\n+\t  off *= tree_to_double_int (vro->op2);\n+\t  if (off.fits_shwi ())\n \t    vro->off = off.low;\n \t}\n     }"}, {"sha": "efe0698d6b06f340eae1315fa5a97ee74d1aeb28", "filename": "gcc/tree-ssa-structalias.c", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-structalias.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa-structalias.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-structalias.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -2902,10 +2902,9 @@ get_constraint_for_ptr_offset (tree ptr, tree offset,\n   else\n     {\n       /* Sign-extend the offset.  */\n-      double_int soffset\n-\t= double_int_sext (tree_to_double_int (offset),\n-\t\t\t   TYPE_PRECISION (TREE_TYPE (offset)));\n-      if (!double_int_fits_in_shwi_p (soffset))\n+      double_int soffset = tree_to_double_int (offset)\n+\t\t\t   .sext (TYPE_PRECISION (TREE_TYPE (offset)));\n+      if (!soffset.fits_shwi ())\n \trhsoffset = UNKNOWN_OFFSET;\n       else\n \t{"}, {"sha": "f8eb723303557b9f155c0d49fa0ae0566475c56a", "filename": "gcc/tree-ssa.c", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-ssa.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -1833,10 +1833,9 @@ non_rewritable_mem_ref_base (tree ref)\n \t   || TREE_CODE (TREE_TYPE (decl)) == COMPLEX_TYPE)\n \t  && useless_type_conversion_p (TREE_TYPE (base),\n \t\t\t\t\tTREE_TYPE (TREE_TYPE (decl)))\n-\t  && double_int_fits_in_uhwi_p (mem_ref_offset (base))\n-\t  && double_int_ucmp\n-\t       (tree_to_double_int (TYPE_SIZE_UNIT (TREE_TYPE (decl))),\n-\t\tmem_ref_offset (base)) == 1\n+\t  && mem_ref_offset (base).fits_uhwi ()\n+\t  && tree_to_double_int (TYPE_SIZE_UNIT (TREE_TYPE (decl)))\n+\t     .ugt (mem_ref_offset (base))\n \t  && multiple_of_p (sizetype, TREE_OPERAND (base, 1),\n \t\t\t    TYPE_SIZE_UNIT (TREE_TYPE (base))))\n \treturn NULL_TREE;"}, {"sha": "bb89d30fc320c6279abcd5a747d1887eca042a4a", "filename": "gcc/tree-switch-conversion.c", "status": "modified", "additions": 3, "deletions": 6, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-switch-conversion.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-switch-conversion.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-switch-conversion.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -970,17 +970,14 @@ array_value_type (gimple swtch, tree type, int num,\n \t  if (prec > HOST_BITS_PER_WIDE_INT)\n \t    return type;\n \n-\t  if (sign >= 0\n-\t      && double_int_equal_p (cst, double_int_zext (cst, prec)))\n+\t  if (sign >= 0 && cst == cst.zext (prec))\n \t    {\n-\t      if (sign == 0\n-\t\t  && double_int_equal_p (cst, double_int_sext (cst, prec)))\n+\t      if (sign == 0 && cst == cst.sext (prec))\n \t\tbreak;\n \t      sign = 1;\n \t      break;\n \t    }\n-\t  if (sign <= 0\n-\t      && double_int_equal_p (cst, double_int_sext (cst, prec)))\n+\t  if (sign <= 0 && cst == cst.sext (prec))\n \t    {\n \t      sign = -1;\n \t      break;"}, {"sha": "fb354ae28d1e100e87b315a03c7978e47d63cbc0", "filename": "gcc/tree-vect-loop-manip.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-vect-loop-manip.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-vect-loop-manip.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-loop-manip.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -1908,7 +1908,7 @@ vect_do_peeling_for_loop_bound (loop_vec_info loop_vinfo, tree *ratio,\n   max_iter = LOOP_VINFO_VECT_FACTOR (loop_vinfo) - 1;\n   if (check_profitability)\n     max_iter = MAX (max_iter, (int) th);\n-  record_niter_bound (new_loop, shwi_to_double_int (max_iter), false, true);\n+  record_niter_bound (new_loop, double_int::from_shwi (max_iter), false, true);\n   if (dump_file && (dump_flags & TDF_DETAILS))\n     fprintf (dump_file, \"Setting upper bound of nb iterations for epilogue \"\n \t     \"loop to %d\\n\", max_iter);\n@@ -2130,7 +2130,7 @@ vect_do_peeling_for_alignment (loop_vec_info loop_vinfo,\n   max_iter = LOOP_VINFO_VECT_FACTOR (loop_vinfo) - 1;\n   if (check_profitability)\n     max_iter = MAX (max_iter, (int) th);\n-  record_niter_bound (new_loop, shwi_to_double_int (max_iter), false, true);\n+  record_niter_bound (new_loop, double_int::from_shwi (max_iter), false, true);\n   if (dump_file && (dump_flags & TDF_DETAILS))\n     fprintf (dump_file, \"Setting upper bound of nb iterations for prologue \"\n \t     \"loop to %d\\n\", max_iter);"}, {"sha": "9309264092aceb1f44d9b7b23f8e5f35ab6ebcc8", "filename": "gcc/tree-vrp.c", "status": "modified", "additions": 175, "deletions": 215, "changes": 390, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-vrp.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree-vrp.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vrp.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -1961,9 +1961,9 @@ zero_nonzero_bits_from_vr (value_range_t *vr,\n     {\n       double_int dmin = tree_to_double_int (vr->min);\n       double_int dmax = tree_to_double_int (vr->max);\n-      double_int xor_mask = double_int_xor (dmin, dmax);\n-      *may_be_nonzero = double_int_ior (dmin, dmax);\n-      *must_be_nonzero = double_int_and (dmin, dmax);\n+      double_int xor_mask = dmin ^ dmax;\n+      *may_be_nonzero = dmin | dmax;\n+      *must_be_nonzero = dmin & dmax;\n       if (xor_mask.high != 0)\n \t{\n \t  unsigned HOST_WIDE_INT mask\n@@ -2014,16 +2014,14 @@ ranges_from_anti_range (value_range_t *ar,\n       vr0->min = vrp_val_min (type);\n       vr0->max\n \t= double_int_to_tree (type,\n-\t\t\t      double_int_sub (tree_to_double_int (ar->min),\n-\t\t\t\t\t      double_int_one));\n+\t\t\t      tree_to_double_int (ar->min) - double_int_one);\n     }\n   if (!vrp_val_is_max (ar->max))\n     {\n       vr1->type = VR_RANGE;\n       vr1->min\n \t= double_int_to_tree (type,\n-\t\t\t      double_int_add (tree_to_double_int (ar->max),\n-\t\t\t\t\t      double_int_one));\n+\t\t\t      tree_to_double_int (ar->max) + double_int_one);\n       vr1->max = vrp_val_max (type);\n     }\n   if (vr0->type == VR_UNDEFINED)\n@@ -2193,9 +2191,9 @@ static int\n quad_int_cmp (double_int l0, double_int h0,\n \t      double_int l1, double_int h1, bool uns)\n {\n-  int c = double_int_cmp (h0, h1, uns);\n+  int c = h0.cmp (h1, uns);\n   if (c != 0) return c;\n-  return double_int_ucmp (l0, l1);\n+  return l0.ucmp (l1);\n }\n \n static void\n@@ -2389,37 +2387,33 @@ extract_range_from_binary_expr_1 (value_range_t *vr,\n \t  double_int max1 = tree_to_double_int (vr1.max);\n \t  bool uns = TYPE_UNSIGNED (expr_type);\n \t  double_int type_min\n-\t    = double_int_min_value (TYPE_PRECISION (expr_type), uns);\n+\t    = double_int::min_value (TYPE_PRECISION (expr_type), uns);\n \t  double_int type_max\n-\t    = double_int_max_value (TYPE_PRECISION (expr_type), uns);\n+\t    = double_int::max_value (TYPE_PRECISION (expr_type), uns);\n \t  double_int dmin, dmax;\n \t  int min_ovf = 0;\n \t  int max_ovf = 0;\n \n \t  if (code == PLUS_EXPR)\n \t    {\n-\t      dmin = double_int_add (min0, min1);\n-\t      dmax = double_int_add (max0, max1);\n+\t      dmin = min0 + min1;\n+\t      dmax = max0 + max1;\n \n \t      /* Check for overflow in double_int.  */\n-\t      if (double_int_cmp (min1, double_int_zero, uns)\n-\t\t  != double_int_cmp (dmin, min0, uns))\n-\t\tmin_ovf = double_int_cmp (min0, dmin, uns);\n-\t      if (double_int_cmp (max1, double_int_zero, uns)\n-\t\t  != double_int_cmp (dmax, max0, uns))\n-\t\tmax_ovf = double_int_cmp (max0, dmax, uns);\n+\t      if (min1.cmp (double_int_zero, uns) != dmin.cmp (min0, uns))\n+\t\tmin_ovf = min0.cmp (dmin, uns);\n+\t      if (max1.cmp (double_int_zero, uns) != dmax.cmp (max0, uns))\n+\t\tmax_ovf = max0.cmp (dmax, uns);\n \t    }\n \t  else /* if (code == MINUS_EXPR) */\n \t    {\n-\t      dmin = double_int_sub (min0, max1);\n-\t      dmax = double_int_sub (max0, min1);\n-\n-\t      if (double_int_cmp (double_int_zero, max1, uns)\n-\t\t  != double_int_cmp (dmin, min0, uns))\n-\t\tmin_ovf = double_int_cmp (min0, max1, uns);\n-\t      if (double_int_cmp (double_int_zero, min1, uns)\n-\t\t  != double_int_cmp (dmax, max0, uns))\n-\t\tmax_ovf = double_int_cmp (max0, min1, uns);\n+\t      dmin = min0 - max1;\n+\t      dmax = max0 - min1;\n+\n+\t      if (double_int_zero.cmp (max1, uns) != dmin.cmp (min0, uns))\n+\t\tmin_ovf = min0.cmp (max1, uns);\n+\t      if (double_int_zero.cmp (min1, uns) != dmax.cmp (max0, uns))\n+\t\tmax_ovf = max0.cmp (min1, uns);\n \t    }\n \n \t  /* For non-wrapping arithmetic look at possibly smaller\n@@ -2435,16 +2429,16 @@ extract_range_from_binary_expr_1 (value_range_t *vr,\n \t  /* Check for type overflow.  */\n \t  if (min_ovf == 0)\n \t    {\n-\t      if (double_int_cmp (dmin, type_min, uns) == -1)\n+\t      if (dmin.cmp (type_min, uns) == -1)\n \t\tmin_ovf = -1;\n-\t      else if (double_int_cmp (dmin, type_max, uns) == 1)\n+\t      else if (dmin.cmp (type_max, uns) == 1)\n \t\tmin_ovf = 1;\n \t    }\n \t  if (max_ovf == 0)\n \t    {\n-\t      if (double_int_cmp (dmax, type_min, uns) == -1)\n+\t      if (dmax.cmp (type_min, uns) == -1)\n \t\tmax_ovf = -1;\n-\t      else if (double_int_cmp (dmax, type_max, uns) == 1)\n+\t      else if (dmax.cmp (type_max, uns) == 1)\n \t\tmax_ovf = 1;\n \t    }\n \n@@ -2453,9 +2447,9 @@ extract_range_from_binary_expr_1 (value_range_t *vr,\n \t      /* If overflow wraps, truncate the values and adjust the\n \t\t range kind and bounds appropriately.  */\n \t      double_int tmin\n-\t\t= double_int_ext (dmin, TYPE_PRECISION (expr_type), uns);\n+\t\t= dmin.ext (TYPE_PRECISION (expr_type), uns);\n \t      double_int tmax\n-\t\t= double_int_ext (dmax, TYPE_PRECISION (expr_type), uns);\n+\t\t= dmax.ext (TYPE_PRECISION (expr_type), uns);\n \t      if (min_ovf == max_ovf)\n \t\t{\n \t\t  /* No overflow or both overflow or underflow.  The\n@@ -2479,16 +2473,16 @@ extract_range_from_binary_expr_1 (value_range_t *vr,\n \t\t  gcc_assert ((min_ovf == -1 && max_ovf == 0)\n \t\t\t      || (max_ovf == 1 && min_ovf == 0));\n \t\t  type = VR_ANTI_RANGE;\n-\t\t  tmin = double_int_add (tmax, double_int_one);\n-\t\t  if (double_int_cmp (tmin, tmax, uns) < 0)\n+\t\t  tmin = tmax + double_int_one;\n+\t\t  if (tmin.cmp (tmax, uns) < 0)\n \t\t    covers = true;\n-\t\t  tmax = double_int_add (tem, double_int_minus_one);\n+\t\t  tmax = tem + double_int_minus_one;\n \t\t  if (double_int_cmp (tmax, tem, uns) > 0)\n \t\t    covers = true;\n \t\t  /* If the anti-range would cover nothing, drop to varying.\n \t\t     Likewise if the anti-range bounds are outside of the\n \t\t     types values.  */\n-\t\t  if (covers || double_int_cmp (tmin, tmax, uns) > 0)\n+\t\t  if (covers || tmin.cmp (tmax, uns) > 0)\n \t\t    {\n \t\t      set_value_range_to_varying (vr);\n \t\t      return;\n@@ -2605,8 +2599,8 @@ extract_range_from_binary_expr_1 (value_range_t *vr,\n \t\t     prod2l, prod2h, prod3l, prod3h;\n \t  bool uns0, uns1, uns;\n \n-\t  sizem1 = double_int_max_value (TYPE_PRECISION (expr_type), true);\n-\t  size = double_int_add (sizem1, double_int_one);\n+\t  sizem1 = double_int::max_value (TYPE_PRECISION (expr_type), true);\n+\t  size = sizem1 + double_int_one;\n \n \t  min0 = tree_to_double_int (vr0.min);\n \t  max0 = tree_to_double_int (vr0.max);\n@@ -2619,19 +2613,19 @@ extract_range_from_binary_expr_1 (value_range_t *vr,\n \t  /* Canonicalize the intervals.  */\n \t  if (TYPE_UNSIGNED (expr_type))\n \t    {\n-\t      double_int min2 = double_int_sub (size, min0);\n-\t      if (double_int_cmp (min2, max0, true) < 0)\n+\t      double_int min2 = size - min0;\n+\t      if (min2.cmp (max0, true) < 0)\n \t\t{\n-\t\t  min0 = double_int_neg (min2);\n-\t\t  max0 = double_int_sub (max0, size);\n+\t\t  min0 = -min2;\n+\t\t  max0 -= size;\n \t\t  uns0 = false;\n \t\t}\n \n-\t      min2 = double_int_sub (size, min1);\n-\t      if (double_int_cmp (min2, max1, true) < 0)\n+\t      min2 = size - min1;\n+\t      if (min2.cmp (max1, true) < 0)\n \t\t{\n-\t\t  min1 = double_int_neg (min2);\n-\t\t  max1 = double_int_sub (max1, size);\n+\t\t  min1 = -min2;\n+\t\t  max1 -= size;\n \t\t  uns1 = false;\n \t\t}\n \t    }\n@@ -2641,37 +2635,37 @@ extract_range_from_binary_expr_1 (value_range_t *vr,\n \t\t\t\t     min1.low, min1.high,\n \t\t\t\t     &prod0l.low, &prod0l.high,\n \t\t\t\t     &prod0h.low, &prod0h.high, true);\n-\t  if (!uns0 && double_int_negative_p (min0))\n-\t    prod0h = double_int_sub (prod0h, min1);\n-\t  if (!uns1 && double_int_negative_p (min1))\n-\t    prod0h = double_int_sub (prod0h, min0);\n+\t  if (!uns0 && min0.is_negative ())\n+\t    prod0h -= min1;\n+\t  if (!uns1 && min1.is_negative ())\n+\t    prod0h -= min0;\n \n \t  mul_double_wide_with_sign (min0.low, min0.high,\n \t\t\t\t     max1.low, max1.high,\n \t\t\t\t     &prod1l.low, &prod1l.high,\n \t\t\t\t     &prod1h.low, &prod1h.high, true);\n-\t  if (!uns0 && double_int_negative_p (min0))\n-\t    prod1h = double_int_sub (prod1h, max1);\n-\t  if (!uns1 && double_int_negative_p (max1))\n-\t    prod1h = double_int_sub (prod1h, min0);\n+\t  if (!uns0 && min0.is_negative ())\n+\t    prod1h -= max1;\n+\t  if (!uns1 && max1.is_negative ())\n+\t    prod1h -= min0;\n \n \t  mul_double_wide_with_sign (max0.low, max0.high,\n \t\t\t\t     min1.low, min1.high,\n \t\t\t\t     &prod2l.low, &prod2l.high,\n \t\t\t\t     &prod2h.low, &prod2h.high, true);\n-\t  if (!uns0 && double_int_negative_p (max0))\n-\t    prod2h = double_int_sub (prod2h, min1);\n-\t  if (!uns1 && double_int_negative_p (min1))\n-\t    prod2h = double_int_sub (prod2h, max0);\n+\t  if (!uns0 && max0.is_negative ())\n+\t    prod2h -= min1;\n+\t  if (!uns1 && min1.is_negative ())\n+\t    prod2h -= max0;\n \n \t  mul_double_wide_with_sign (max0.low, max0.high,\n \t\t\t\t     max1.low, max1.high,\n \t\t\t\t     &prod3l.low, &prod3l.high,\n \t\t\t\t     &prod3h.low, &prod3h.high, true);\n-\t  if (!uns0 && double_int_negative_p (max0))\n-\t    prod3h = double_int_sub (prod3h, max1);\n-\t  if (!uns1 && double_int_negative_p (max1))\n-\t    prod3h = double_int_sub (prod3h, max0);\n+\t  if (!uns0 && max0.is_negative ())\n+\t    prod3h -= max1;\n+\t  if (!uns1 && max1.is_negative ())\n+\t    prod3h -= max0;\n \n \t  /* Sort the 4 products.  */\n \t  quad_int_pair_sort (&prod0l, &prod0h, &prod3l, &prod3h, uns);\n@@ -2680,23 +2674,23 @@ extract_range_from_binary_expr_1 (value_range_t *vr,\n \t  quad_int_pair_sort (&prod2l, &prod2h, &prod3l, &prod3h, uns);\n \n \t  /* Max - min.  */\n-\t  if (double_int_zero_p (prod0l))\n+\t  if (prod0l.is_zero ())\n \t    {\n \t      prod1l = double_int_zero;\n-\t      prod1h = double_int_neg (prod0h);\n+\t      prod1h = -prod0h;\n \t    }\n \t  else\n \t    {\n-\t      prod1l = double_int_neg (prod0l);\n-\t      prod1h = double_int_not (prod0h);\n+\t      prod1l = -prod0l;\n+\t      prod1h = ~prod0h;\n \t    }\n-\t  prod2l = double_int_add (prod3l, prod1l);\n-\t  prod2h = double_int_add (prod3h, prod1h);\n-\t  if (double_int_ucmp (prod2l, prod3l) < 0)\n-\t    prod2h = double_int_add (prod2h, double_int_one); /* carry */\n+\t  prod2l = prod3l + prod1l;\n+\t  prod2h = prod3h + prod1h;\n+\t  if (prod2l.ult (prod3l))\n+\t    prod2h += double_int_one; /* carry */\n \n-\t  if (!double_int_zero_p (prod2h)\n-\t      || double_int_cmp (prod2l, sizem1, true) >= 0)\n+\t  if (!prod2h.is_zero ()\n+\t      || prod2l.cmp (sizem1, true) >= 0)\n \t    {\n \t      /* the range covers all values.  */\n \t      set_value_range_to_varying (vr);\n@@ -2755,11 +2749,9 @@ extract_range_from_binary_expr_1 (value_range_t *vr,\n \t      vr1p.type = VR_RANGE;\n \t      vr1p.min\n \t\t= double_int_to_tree (expr_type,\n-\t\t\t\t      double_int_lshift\n-\t\t\t\t        (double_int_one,\n-\t\t\t\t\t TREE_INT_CST_LOW (vr1.min),\n-\t\t\t\t\t TYPE_PRECISION (expr_type),\n-\t\t\t\t\t false));\n+\t\t\t\t      double_int_one\n+\t\t\t\t      .llshift (TREE_INT_CST_LOW (vr1.min),\n+\t\t\t\t\t        TYPE_PRECISION (expr_type)));\n \t      vr1p.max = vr1p.min;\n \t      /* We have to use a wrapping multiply though as signed overflow\n \t\t on lshifts is implementation defined in C89.  */\n@@ -2903,80 +2895,68 @@ extract_range_from_binary_expr_1 (value_range_t *vr,\n \t{\n \t  double_int dmax;\n \t  min = double_int_to_tree (expr_type,\n-\t\t\t\t    double_int_and (must_be_nonzero0,\n-\t\t\t\t\t\t    must_be_nonzero1));\n-\t  dmax = double_int_and (may_be_nonzero0, may_be_nonzero1);\n+\t\t\t\t    must_be_nonzero0 & must_be_nonzero1);\n+\t  dmax = may_be_nonzero0 & may_be_nonzero1;\n \t  /* If both input ranges contain only negative values we can\n \t     truncate the result range maximum to the minimum of the\n \t     input range maxima.  */\n \t  if (int_cst_range0 && int_cst_range1\n \t      && tree_int_cst_sgn (vr0.max) < 0\n \t      && tree_int_cst_sgn (vr1.max) < 0)\n \t    {\n-\t      dmax = double_int_min (dmax, tree_to_double_int (vr0.max),\n+\t      dmax = dmax.min (tree_to_double_int (vr0.max),\n \t\t\t\t     TYPE_UNSIGNED (expr_type));\n-\t      dmax = double_int_min (dmax, tree_to_double_int (vr1.max),\n+\t      dmax = dmax.min (tree_to_double_int (vr1.max),\n \t\t\t\t     TYPE_UNSIGNED (expr_type));\n \t    }\n \t  /* If either input range contains only non-negative values\n \t     we can truncate the result range maximum to the respective\n \t     maximum of the input range.  */\n \t  if (int_cst_range0 && tree_int_cst_sgn (vr0.min) >= 0)\n-\t    dmax = double_int_min (dmax, tree_to_double_int (vr0.max),\n+\t    dmax = dmax.min (tree_to_double_int (vr0.max),\n \t\t\t\t   TYPE_UNSIGNED (expr_type));\n \t  if (int_cst_range1 && tree_int_cst_sgn (vr1.min) >= 0)\n-\t    dmax = double_int_min (dmax, tree_to_double_int (vr1.max),\n+\t    dmax = dmax.min (tree_to_double_int (vr1.max),\n \t\t\t\t   TYPE_UNSIGNED (expr_type));\n \t  max = double_int_to_tree (expr_type, dmax);\n \t}\n       else if (code == BIT_IOR_EXPR)\n \t{\n \t  double_int dmin;\n \t  max = double_int_to_tree (expr_type,\n-\t\t\t\t    double_int_ior (may_be_nonzero0,\n-\t\t\t\t\t\t    may_be_nonzero1));\n-\t  dmin = double_int_ior (must_be_nonzero0, must_be_nonzero1);\n+\t\t\t\t    may_be_nonzero0 | may_be_nonzero1);\n+\t  dmin = must_be_nonzero0 | must_be_nonzero1;\n \t  /* If the input ranges contain only positive values we can\n \t     truncate the minimum of the result range to the maximum\n \t     of the input range minima.  */\n \t  if (int_cst_range0 && int_cst_range1\n \t      && tree_int_cst_sgn (vr0.min) >= 0\n \t      && tree_int_cst_sgn (vr1.min) >= 0)\n \t    {\n-\t      dmin = double_int_max (dmin, tree_to_double_int (vr0.min),\n-\t\t\t\t     TYPE_UNSIGNED (expr_type));\n-\t      dmin = double_int_max (dmin, tree_to_double_int (vr1.min),\n-\t\t\t\t     TYPE_UNSIGNED (expr_type));\n+\t      dmin = dmin.max (tree_to_double_int (vr0.min),\n+\t\t\t       TYPE_UNSIGNED (expr_type));\n+\t      dmin = dmin.max (tree_to_double_int (vr1.min),\n+\t\t\t       TYPE_UNSIGNED (expr_type));\n \t    }\n \t  /* If either input range contains only negative values\n \t     we can truncate the minimum of the result range to the\n \t     respective minimum range.  */\n \t  if (int_cst_range0 && tree_int_cst_sgn (vr0.max) < 0)\n-\t    dmin = double_int_max (dmin, tree_to_double_int (vr0.min),\n-\t\t\t\t   TYPE_UNSIGNED (expr_type));\n+\t    dmin = dmin.max (tree_to_double_int (vr0.min),\n+\t\t\t     TYPE_UNSIGNED (expr_type));\n \t  if (int_cst_range1 && tree_int_cst_sgn (vr1.max) < 0)\n-\t    dmin = double_int_max (dmin, tree_to_double_int (vr1.min),\n-\t\t\t\t   TYPE_UNSIGNED (expr_type));\n+\t    dmin = dmin.max (tree_to_double_int (vr1.min),\n+\t\t\t     TYPE_UNSIGNED (expr_type));\n \t  min = double_int_to_tree (expr_type, dmin);\n \t}\n       else if (code == BIT_XOR_EXPR)\n \t{\n \t  double_int result_zero_bits, result_one_bits;\n-\t  result_zero_bits\n-\t    = double_int_ior (double_int_and (must_be_nonzero0,\n-\t\t\t\t\t      must_be_nonzero1),\n-\t\t\t      double_int_not\n-\t\t\t        (double_int_ior (may_be_nonzero0,\n-\t\t\t\t\t\t may_be_nonzero1)));\n-\t  result_one_bits\n-\t    = double_int_ior (double_int_and\n-\t\t\t        (must_be_nonzero0,\n-\t\t\t\t double_int_not (may_be_nonzero1)),\n-\t\t\t      double_int_and\n-\t\t\t        (must_be_nonzero1,\n-\t\t\t\t double_int_not (may_be_nonzero0)));\n-\t  max = double_int_to_tree (expr_type,\n-\t\t\t\t    double_int_not (result_zero_bits));\n+\t  result_zero_bits = (must_be_nonzero0 & must_be_nonzero1)\n+\t\t\t     | ~(may_be_nonzero0 | may_be_nonzero1);\n+\t  result_one_bits = must_be_nonzero0.and_not (may_be_nonzero1)\n+\t\t\t    | must_be_nonzero1.and_not (may_be_nonzero0);\n+\t  max = double_int_to_tree (expr_type, ~result_zero_bits);\n \t  min = double_int_to_tree (expr_type, result_one_bits);\n \t  /* If the range has all positive or all negative values the\n \t     result is better than VARYING.  */\n@@ -3606,10 +3586,10 @@ adjust_range_with_scev (value_range_t *vr, struct loop *loop,\n \t  value_range_t maxvr = VR_INITIALIZER;\n \t  double_int dtmp;\n \t  bool unsigned_p = TYPE_UNSIGNED (TREE_TYPE (step));\n-\t  int overflow = 0;\n+\t  bool overflow = false;\n \n-\t  dtmp = double_int_mul_with_sign (tree_to_double_int (step), nit,\n-\t\t\t\t\t   unsigned_p, &overflow);\n+\t  dtmp = tree_to_double_int (step)\n+\t\t .mul_with_sign (nit, unsigned_p, &overflow);\n \t  /* If the multiplication overflowed we can't do a meaningful\n \t     adjustment.  Likewise if the result doesn't fit in the type\n \t     of the induction variable.  For a signed type we have to\n@@ -4519,19 +4499,19 @@ masked_increment (double_int val, double_int mask, double_int sgnbit,\n   double_int bit = double_int_one, res;\n   unsigned int i;\n \n-  val = double_int_xor (val, sgnbit);\n-  for (i = 0; i < prec; i++, bit = double_int_add (bit, bit))\n+  val ^= sgnbit;\n+  for (i = 0; i < prec; i++, bit += bit)\n     {\n       res = mask;\n-      if (double_int_zero_p (double_int_and (res, bit)))\n+      if ((res & bit).is_zero ())\n \tcontinue;\n-      res = double_int_sub (bit, double_int_one);\n-      res = double_int_and_not (double_int_add (val, bit), res);\n-      res = double_int_and (res, mask);\n-      if (double_int_ucmp (res, val) > 0)\n-\treturn double_int_xor (res, sgnbit);\n+      res = bit - double_int_one;\n+      res = (val + bit).and_not (res);\n+      res &= mask;\n+      if (res.ugt (val))\n+\treturn res ^ sgnbit;\n     }\n-  return double_int_xor (val, sgnbit);\n+  return val ^ sgnbit;\n }\n \n /* Try to register an edge assertion for SSA name NAME on edge E for\n@@ -4735,7 +4715,7 @@ register_edge_assert_for_2 (tree name, edge e, gimple_stmt_iterator bsi,\n \t      && live_on_edge (e, name2)\n \t      && !has_single_use (name2))\n \t    {\n-\t      mask = double_int_mask (tree_low_cst (cst2, 1));\n+\t      mask = double_int::mask (tree_low_cst (cst2, 1));\n \t      val2 = fold_binary (LSHIFT_EXPR, TREE_TYPE (val), val, cst2);\n \t    }\n \t}\n@@ -4766,9 +4746,9 @@ register_edge_assert_for_2 (tree name, edge e, gimple_stmt_iterator bsi,\n \t  else\n \t    {\n \t      double_int maxval\n-\t\t= double_int_max_value (prec, TYPE_UNSIGNED (TREE_TYPE (val)));\n-\t      mask = double_int_ior (tree_to_double_int (val2), mask);\n-\t      if (double_int_equal_p (mask, maxval))\n+\t\t= double_int::max_value (prec, TYPE_UNSIGNED (TREE_TYPE (val)));\n+\t      mask |= tree_to_double_int (val2);\n+\t      if (mask == maxval)\n \t\tnew_val = NULL_TREE;\n \t      else\n \t\tnew_val = double_int_to_tree (TREE_TYPE (val2), mask);\n@@ -4835,69 +4815,65 @@ register_edge_assert_for_2 (tree name, edge e, gimple_stmt_iterator bsi,\n \t  bool valid_p = false, valn = false, cst2n = false;\n \t  enum tree_code ccode = comp_code;\n \n-\t  valv = double_int_zext (tree_to_double_int (val), prec);\n-\t  cst2v = double_int_zext (tree_to_double_int (cst2), prec);\n+\t  valv = tree_to_double_int (val).zext (prec);\n+\t  cst2v = tree_to_double_int (cst2).zext (prec);\n \t  if (!TYPE_UNSIGNED (TREE_TYPE (val)))\n \t    {\n-\t      valn = double_int_negative_p (double_int_sext (valv, prec));\n-\t      cst2n = double_int_negative_p (double_int_sext (cst2v, prec));\n+\t      valn = valv.sext (prec).is_negative ();\n+\t      cst2n = cst2v.sext (prec).is_negative ();\n \t    }\n \t  /* If CST2 doesn't have most significant bit set,\n \t     but VAL is negative, we have comparison like\n \t     if ((x & 0x123) > -4) (always true).  Just give up.  */\n \t  if (!cst2n && valn)\n \t    ccode = ERROR_MARK;\n \t  if (cst2n)\n-\t    sgnbit = double_int_zext (double_int_lshift (double_int_one,\n-\t\t\t\t\t\t\t prec - 1, prec,\n-\t\t\t\t\t\t\t false), prec);\n+\t    sgnbit = double_int_one.llshift (prec - 1, prec).zext (prec);\n \t  else\n \t    sgnbit = double_int_zero;\n-\t  minv = double_int_and (valv, cst2v);\n+\t  minv = valv & cst2v;\n \t  switch (ccode)\n \t    {\n \t    case EQ_EXPR:\n \t      /* Minimum unsigned value for equality is VAL & CST2\n \t\t (should be equal to VAL, otherwise we probably should\n \t\t have folded the comparison into false) and\n \t\t maximum unsigned value is VAL | ~CST2.  */\n-\t      maxv = double_int_ior (valv, double_int_not (cst2v));\n-\t      maxv = double_int_zext (maxv, prec);\n+\t      maxv = valv | ~cst2v;\n+\t      maxv = maxv.zext (prec);\n \t      valid_p = true;\n \t      break;\n \t    case NE_EXPR:\n-\t      tem = double_int_ior (valv, double_int_not (cst2v));\n-\t      tem = double_int_zext (tem, prec);\n+\t      tem = valv | ~cst2v;\n+\t      tem = tem.zext (prec);\n \t      /* If VAL is 0, handle (X & CST2) != 0 as (X & CST2) > 0U.  */\n-\t      if (double_int_zero_p (valv))\n+\t      if (valv.is_zero ())\n \t\t{\n \t\t  cst2n = false;\n \t\t  sgnbit = double_int_zero;\n \t\t  goto gt_expr;\n \t\t}\n \t      /* If (VAL | ~CST2) is all ones, handle it as\n \t\t (X & CST2) < VAL.  */\n-\t      if (double_int_equal_p (tem, double_int_mask (prec)))\n+\t      if (tem == double_int::mask (prec))\n \t\t{\n \t\t  cst2n = false;\n \t\t  valn = false;\n \t\t  sgnbit = double_int_zero;\n \t\t  goto lt_expr;\n \t\t}\n \t      if (!cst2n\n-\t\t  && double_int_negative_p (double_int_sext (cst2v, prec)))\n-\t\tsgnbit = double_int_zext (double_int_lshift (double_int_one,\n-\t\t\t\t\t\t\t     prec - 1, prec,\n-\t\t\t\t\t\t\t     false), prec);\n-\t      if (!double_int_zero_p (sgnbit))\n+\t\t  && cst2v.sext (prec).is_negative ())\n+\t\tsgnbit = double_int_one.llshift (prec - 1, prec).zext (prec);\n+\t      if (!sgnbit.is_zero ())\n \t\t{\n-\t\t  if (double_int_equal_p (valv, sgnbit))\n+\t\t  if (valv == sgnbit)\n \t\t    {\n \t\t      cst2n = true;\n \t\t      valn = true;\n \t\t      goto gt_expr;\n \t\t    }\n-\t\t  if (double_int_equal_p (tem, double_int_mask (prec - 1)))\n+\t\t  if (tem == double_int::mask (prec - 1))\n \t\t    {\n \t\t      cst2n = true;\n \t\t      goto lt_expr;\n@@ -4912,15 +4888,15 @@ register_edge_assert_for_2 (tree name, edge e, gimple_stmt_iterator bsi,\n \t\t comparison, if CST2 doesn't have most significant bit\n \t\t set, handle it similarly.  If CST2 has MSB set,\n \t\t the minimum is the same, and maximum is ~0U/2.  */\n-\t      if (!double_int_equal_p (minv, valv))\n+\t      if (minv != valv)\n \t\t{\n \t\t  /* If (VAL & CST2) != VAL, X & CST2 can't be equal to\n \t\t     VAL.  */\n \t\t  minv = masked_increment (valv, cst2v, sgnbit, prec);\n-\t\t  if (double_int_equal_p (minv, valv))\n+\t\t  if (minv == valv)\n \t\t    break;\n \t\t}\n-\t      maxv = double_int_mask (prec - (cst2n ? 1 : 0));\n+\t      maxv = double_int::mask (prec - (cst2n ? 1 : 0));\n \t      valid_p = true;\n \t      break;\n \t    case GT_EXPR:\n@@ -4929,9 +4905,9 @@ register_edge_assert_for_2 (tree name, edge e, gimple_stmt_iterator bsi,\n \t\t && (MINV & CST2) == MINV, if any.  If VAL is signed and\n \t\t CST2 has MSB set, compute it biased by 1 << (prec - 1).  */\n \t      minv = masked_increment (valv, cst2v, sgnbit, prec);\n-\t      if (double_int_equal_p (minv, valv))\n+\t      if (minv == valv)\n \t\tbreak;\n-\t      maxv = double_int_mask (prec - (cst2n ? 1 : 0));\n+\t      maxv = double_int::mask (prec - (cst2n ? 1 : 0));\n \t      valid_p = true;\n \t      break;\n \t    case LE_EXPR:\n@@ -4943,17 +4919,17 @@ register_edge_assert_for_2 (tree name, edge e, gimple_stmt_iterator bsi,\n \t\t For signed comparison, if CST2 doesn't have most\n \t\t significant bit set, handle it similarly.  If CST2 has\n \t\t MSB set, the maximum is the same and minimum is INT_MIN.  */\n-\t      if (double_int_equal_p (minv, valv))\n+\t      if (minv == valv)\n \t\tmaxv = valv;\n \t      else\n \t\t{\n \t\t  maxv = masked_increment (valv, cst2v, sgnbit, prec);\n-\t\t  if (double_int_equal_p (maxv, valv))\n+\t\t  if (maxv == valv)\n \t\t    break;\n-\t\t  maxv = double_int_sub (maxv, double_int_one);\n+\t\t  maxv -= double_int_one;\n \t\t}\n-\t      maxv = double_int_ior (maxv, double_int_not (cst2v));\n-\t      maxv = double_int_zext (maxv, prec);\n+\t      maxv |= ~cst2v;\n+\t      maxv = maxv.zext (prec);\n \t      minv = sgnbit;\n \t      valid_p = true;\n \t      break;\n@@ -4967,32 +4943,29 @@ register_edge_assert_for_2 (tree name, edge e, gimple_stmt_iterator bsi,\n \t\t For signed comparison, if CST2 doesn't have most\n \t\t significant bit set, handle it similarly.  If CST2 has\n \t\t MSB set, the maximum is the same and minimum is INT_MIN.  */\n-\t      if (double_int_equal_p (minv, valv))\n+\t      if (minv == valv)\n \t\t{\n-\t\t  if (double_int_equal_p (valv, sgnbit))\n+\t\t  if (valv == sgnbit)\n \t\t    break;\n \t\t  maxv = valv;\n \t\t}\n \t      else\n \t\t{\n \t\t  maxv = masked_increment (valv, cst2v, sgnbit, prec);\n-\t\t  if (double_int_equal_p (maxv, valv))\n+\t\t  if (maxv == valv)\n \t\t    break;\n \t\t}\n-\t      maxv = double_int_sub (maxv, double_int_one);\n-\t      maxv = double_int_ior (maxv, double_int_not (cst2v));\n-\t      maxv = double_int_zext (maxv, prec);\n+\t      maxv -= double_int_one;\n+\t      maxv |= ~cst2v;\n+\t      maxv = maxv.zext (prec);\n \t      minv = sgnbit;\n \t      valid_p = true;\n \t      break;\n \t    default:\n \t      break;\n \t    }\n \t  if (valid_p\n-\t      && !double_int_equal_p (double_int_zext (double_int_sub (maxv,\n-\t\t\t\t\t\t\t\t       minv),\n-\t\t\t\t\t\t       prec),\n-\t\t\t\t      double_int_mask (prec)))\n+\t      && (maxv - minv).zext (prec) != double_int::mask (prec))\n \t    {\n \t      tree tmp, new_val, type;\n \t      int i;\n@@ -5008,12 +4981,11 @@ register_edge_assert_for_2 (tree name, edge e, gimple_stmt_iterator bsi,\n \t\t\ttype = build_nonstandard_integer_type (prec, 1);\n \t\t\ttmp = build1 (NOP_EXPR, type, names[i]);\n \t\t      }\n-\t\t    if (!double_int_zero_p (minv))\n+\t\t    if (!minv.is_zero ())\n \t\t      {\n \t\t\ttmp = build2 (PLUS_EXPR, type, tmp,\n-\t\t\t\t      double_int_to_tree (type,\n-\t\t\t\t\t\t\t  double_int_neg (minv)));\n-\t\t\tmaxv2 = double_int_sub (maxv, minv);\n+\t\t\t\t      double_int_to_tree (type, -minv));\n+\t\t\tmaxv2 = maxv - minv;\n \t\t      }\n \t\t    new_val = double_int_to_tree (type, maxv2);\n \n@@ -5947,20 +5919,16 @@ search_for_addr_array (tree t, location_t location)\n \treturn;\n \n       idx = mem_ref_offset (t);\n-      idx = double_int_sdiv (idx, tree_to_double_int (el_sz), TRUNC_DIV_EXPR);\n-      if (double_int_scmp (idx, double_int_zero) < 0)\n+      idx = idx.sdiv (tree_to_double_int (el_sz), TRUNC_DIV_EXPR);\n+      if (idx.slt (double_int_zero))\n \t{\n \t  warning_at (location, OPT_Warray_bounds,\n \t\t      \"array subscript is below array bounds\");\n \t  TREE_NO_WARNING (t) = 1;\n \t}\n-      else if (double_int_scmp (idx,\n-\t\t\t\tdouble_int_add\n-\t\t\t\t  (double_int_add\n-\t\t\t\t    (tree_to_double_int (up_bound),\n-\t\t\t\t     double_int_neg\n-\t\t\t\t       (tree_to_double_int (low_bound))),\n-\t\t\t\t    double_int_one)) > 0)\n+      else if (idx.sgt (tree_to_double_int (up_bound)\n+\t\t\t- tree_to_double_int (low_bound)\n+\t\t\t+ double_int_one))\n \t{\n \t  warning_at (location, OPT_Warray_bounds,\n \t\t      \"array subscript is above array bounds\");\n@@ -8221,28 +8189,28 @@ simplify_bit_ops_using_ranges (gimple_stmt_iterator *gsi, gimple stmt)\n   switch (gimple_assign_rhs_code (stmt))\n     {\n     case BIT_AND_EXPR:\n-      mask = double_int_and_not (may_be_nonzero0, must_be_nonzero1);\n-      if (double_int_zero_p (mask))\n+      mask = may_be_nonzero0.and_not (must_be_nonzero1);\n+      if (mask.is_zero ())\n \t{\n \t  op = op0;\n \t  break;\n \t}\n-      mask = double_int_and_not (may_be_nonzero1, must_be_nonzero0);\n-      if (double_int_zero_p (mask))\n+      mask = may_be_nonzero1.and_not (must_be_nonzero0);\n+      if (mask.is_zero ())\n \t{\n \t  op = op1;\n \t  break;\n \t}\n       break;\n     case BIT_IOR_EXPR:\n-      mask = double_int_and_not (may_be_nonzero0, must_be_nonzero1);\n-      if (double_int_zero_p (mask))\n+      mask = may_be_nonzero0.and_not (must_be_nonzero1);\n+      if (mask.is_zero ())\n \t{\n \t  op = op1;\n \t  break;\n \t}\n-      mask = double_int_and_not (may_be_nonzero1, must_be_nonzero0);\n-      if (double_int_zero_p (mask))\n+      mask = may_be_nonzero1.and_not (must_be_nonzero0);\n+      if (mask.is_zero ())\n \t{\n \t  op = op0;\n \t  break;\n@@ -8549,42 +8517,34 @@ simplify_conversion_using_ranges (gimple stmt)\n \n   /* If the first conversion is not injective, the second must not\n      be widening.  */\n-  if (double_int_cmp (double_int_sub (innermax, innermin),\n-\t\t      double_int_mask (middle_prec), true) > 0\n+  if ((innermax - innermin).ugt (double_int::mask (middle_prec))\n       && middle_prec < final_prec)\n     return false;\n   /* We also want a medium value so that we can track the effect that\n      narrowing conversions with sign change have.  */\n   inner_unsigned_p = TYPE_UNSIGNED (TREE_TYPE (innerop));\n   if (inner_unsigned_p)\n-    innermed = double_int_rshift (double_int_mask (inner_prec),\n-\t\t\t\t  1, inner_prec, false);\n+    innermed = double_int::mask (inner_prec).lrshift (1, inner_prec);\n   else\n     innermed = double_int_zero;\n-  if (double_int_cmp (innermin, innermed, inner_unsigned_p) >= 0\n-      || double_int_cmp (innermed, innermax, inner_unsigned_p) >= 0)\n+  if (innermin.cmp (innermed, inner_unsigned_p) >= 0\n+      || innermed.cmp (innermax, inner_unsigned_p) >= 0)\n     innermed = innermin;\n \n   middle_unsigned_p = TYPE_UNSIGNED (TREE_TYPE (middleop));\n-  middlemin = double_int_ext (innermin, middle_prec, middle_unsigned_p);\n-  middlemed = double_int_ext (innermed, middle_prec, middle_unsigned_p);\n-  middlemax = double_int_ext (innermax, middle_prec, middle_unsigned_p);\n+  middlemin = innermin.ext (middle_prec, middle_unsigned_p);\n+  middlemed = innermed.ext (middle_prec, middle_unsigned_p);\n+  middlemax = innermax.ext (middle_prec, middle_unsigned_p);\n \n   /* Require that the final conversion applied to both the original\n      and the intermediate range produces the same result.  */\n   final_unsigned_p = TYPE_UNSIGNED (finaltype);\n-  if (!double_int_equal_p (double_int_ext (middlemin,\n-\t\t\t\t\t   final_prec, final_unsigned_p),\n-\t\t\t   double_int_ext (innermin,\n-\t\t\t\t\t   final_prec, final_unsigned_p))\n-      || !double_int_equal_p (double_int_ext (middlemed,\n-\t\t\t\t\t      final_prec, final_unsigned_p),\n-\t\t\t      double_int_ext (innermed,\n-\t\t\t\t\t      final_prec, final_unsigned_p))\n-      || !double_int_equal_p (double_int_ext (middlemax,\n-\t\t\t\t\t      final_prec, final_unsigned_p),\n-\t\t\t      double_int_ext (innermax,\n-\t\t\t\t\t      final_prec, final_unsigned_p)))\n+  if (middlemin.ext (final_prec, final_unsigned_p)\n+\t != innermin.ext (final_prec, final_unsigned_p)\n+      || middlemed.ext (final_prec, final_unsigned_p)\n+\t != innermed.ext (final_prec, final_unsigned_p)\n+      || middlemax.ext (final_prec, final_unsigned_p)\n+\t != innermax.ext (final_prec, final_unsigned_p))\n     return false;\n \n   gimple_assign_set_rhs1 (stmt, innerop);\n@@ -8629,11 +8589,11 @@ range_fits_type_p (value_range_t *vr, unsigned precision, bool unsigned_p)\n \n   /* Then we can perform the conversion on both ends and compare\n      the result for equality.  */\n-  tem = double_int_ext (tree_to_double_int (vr->min), precision, unsigned_p);\n-  if (!double_int_equal_p (tree_to_double_int (vr->min), tem))\n+  tem = tree_to_double_int (vr->min).ext (precision, unsigned_p);\n+  if (tree_to_double_int (vr->min) != tem)\n     return false;\n-  tem = double_int_ext (tree_to_double_int (vr->max), precision, unsigned_p);\n-  if (!double_int_equal_p (tree_to_double_int (vr->max), tem))\n+  tem = tree_to_double_int (vr->max).ext (precision, unsigned_p);\n+  if (tree_to_double_int (vr->max) != tem)\n     return false;\n \n   return true;"}, {"sha": "469f47356c89f8a5c9f049e73a737c361391359e", "filename": "gcc/tree.c", "status": "modified", "additions": 23, "deletions": 25, "changes": 48, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -1041,7 +1041,7 @@ build_int_cst (tree type, HOST_WIDE_INT low)\n   if (!type)\n     type = integer_type_node;\n \n-  return double_int_to_tree (type, shwi_to_double_int (low));\n+  return double_int_to_tree (type, double_int::from_shwi (low));\n }\n \n /* Create an INT_CST node with a LOW value sign extended to TYPE.  */\n@@ -1051,7 +1051,7 @@ build_int_cst_type (tree type, HOST_WIDE_INT low)\n {\n   gcc_assert (type);\n \n-  return double_int_to_tree (type, shwi_to_double_int (low));\n+  return double_int_to_tree (type, double_int::from_shwi (low));\n }\n \n /* Constructs tree in type TYPE from with value given by CST.  Signedness\n@@ -1062,7 +1062,7 @@ double_int_to_tree (tree type, double_int cst)\n {\n   bool sign_extended_type = !TYPE_UNSIGNED (type);\n \n-  cst = double_int_ext (cst, TYPE_PRECISION (type), !sign_extended_type);\n+  cst = cst.ext (TYPE_PRECISION (type), !sign_extended_type);\n \n   return build_int_cst_wide (type, cst.low, cst.high);\n }\n@@ -1077,9 +1077,9 @@ double_int_fits_to_tree_p (const_tree type, double_int cst)\n   bool sign_extended_type = !TYPE_UNSIGNED (type);\n \n   double_int ext\n-    = double_int_ext (cst, TYPE_PRECISION (type), !sign_extended_type);\n+    = cst.ext (TYPE_PRECISION (type), !sign_extended_type);\n \n-  return double_int_equal_p (cst, ext);\n+  return cst == ext;\n }\n \n /* We force the double_int CST to the range of the type TYPE by sign or\n@@ -1114,7 +1114,7 @@ force_fit_type_double (tree type, double_int cst, int overflowable,\n \t  || (overflowable > 0 && sign_extended_type))\n \t{\n \t  tree t = make_node (INTEGER_CST);\n-\t  TREE_INT_CST (t) = double_int_ext (cst, TYPE_PRECISION (type),\n+\t  TREE_INT_CST (t) = cst.ext (TYPE_PRECISION (type),\n \t\t\t\t\t     !sign_extended_type);\n \t  TREE_TYPE (t) = type;\n \t  TREE_OVERFLOW (t) = 1;\n@@ -1285,7 +1285,7 @@ build_low_bits_mask (tree type, unsigned bits)\n     /* Sign extended all-ones mask.  */\n     mask = double_int_minus_one;\n   else\n-    mask = double_int_mask (bits);\n+    mask = double_int::mask (bits);\n \n   return build_int_cst_wide (type, mask.low, mask.high);\n }\n@@ -1910,7 +1910,7 @@ int\n fixed_zerop (const_tree expr)\n {\n   return (TREE_CODE (expr) == FIXED_CST\n-\t  && double_int_zero_p (TREE_FIXED_CST (expr).data));\n+\t  && TREE_FIXED_CST (expr).data.is_zero ());\n }\n \n /* Return the power of two represented by a tree node known to be a\n@@ -3998,8 +3998,7 @@ double_int\n mem_ref_offset (const_tree t)\n {\n   tree toff = TREE_OPERAND (t, 1);\n-  return double_int_sext (tree_to_double_int (toff),\n-\t\t\t  TYPE_PRECISION (TREE_TYPE (toff)));\n+  return tree_to_double_int (toff).sext (TYPE_PRECISION (TREE_TYPE (toff)));\n }\n \n /* Return the pointer-type relevant for TBAA purposes from the\n@@ -6557,7 +6556,7 @@ HOST_WIDE_INT\n size_low_cst (const_tree t)\n {\n   double_int d = tree_to_double_int (t);\n-  return double_int_sext (d, TYPE_PRECISION (TREE_TYPE (t))).low;\n+  return d.sext (TYPE_PRECISION (TREE_TYPE (t))).low;\n }\n \n /* Return the most significant (sign) bit of T.  */\n@@ -8295,15 +8294,15 @@ int_fits_type_p (const_tree c, const_tree type)\n       dd = tree_to_double_int (type_low_bound);\n       if (unsc != TYPE_UNSIGNED (TREE_TYPE (type_low_bound)))\n \t{\n-\t  int c_neg = (!unsc && double_int_negative_p (dc));\n-\t  int t_neg = (unsc && double_int_negative_p (dd));\n+\t  int c_neg = (!unsc && dc.is_negative ());\n+\t  int t_neg = (unsc && dd.is_negative ());\n \n \t  if (c_neg && !t_neg)\n \t    return false;\n-\t  if ((c_neg || !t_neg) && double_int_ucmp (dc, dd) < 0)\n+\t  if ((c_neg || !t_neg) && dc.ult (dd))\n \t    return false;\n \t}\n-      else if (double_int_cmp (dc, dd, unsc) < 0)\n+      else if (dc.cmp (dd, unsc) < 0)\n \treturn false;\n       ok_for_low_bound = true;\n     }\n@@ -8316,15 +8315,15 @@ int_fits_type_p (const_tree c, const_tree type)\n       dd = tree_to_double_int (type_high_bound);\n       if (unsc != TYPE_UNSIGNED (TREE_TYPE (type_high_bound)))\n \t{\n-\t  int c_neg = (!unsc && double_int_negative_p (dc));\n-\t  int t_neg = (unsc && double_int_negative_p (dd));\n+\t  int c_neg = (!unsc && dc.is_negative ());\n+\t  int t_neg = (unsc && dd.is_negative ());\n \n \t  if (t_neg && !c_neg)\n \t    return false;\n-\t  if ((t_neg || !c_neg) && double_int_ucmp (dc, dd) > 0)\n+\t  if ((t_neg || !c_neg) && dc.ugt (dd))\n \t    return false;\n \t}\n-      else if (double_int_cmp (dc, dd, unsc) > 0)\n+      else if (dc.cmp (dd, unsc) > 0)\n \treturn false;\n       ok_for_high_bound = true;\n     }\n@@ -8338,7 +8337,7 @@ int_fits_type_p (const_tree c, const_tree type)\n   /* Perform some generic filtering which may allow making a decision\n      even if the bounds are not constant.  First, negative integers\n      never fit in unsigned types, */\n-  if (TYPE_UNSIGNED (type) && !unsc && double_int_negative_p (dc))\n+  if (TYPE_UNSIGNED (type) && !unsc && dc.is_negative ())\n     return false;\n \n   /* Second, narrower types always fit in wider ones.  */\n@@ -8393,9 +8392,8 @@ get_type_static_bounds (const_tree type, mpz_t min, mpz_t max)\n       else\n \t{\n \t  double_int mn;\n-\t  mn = double_int_mask (TYPE_PRECISION (type) - 1);\n-\t  mn = double_int_sext (double_int_add (mn, double_int_one),\n-\t\t\t\tTYPE_PRECISION (type));\n+\t  mn = double_int::mask (TYPE_PRECISION (type) - 1);\n+\t  mn = (mn + double_int_one).sext (TYPE_PRECISION (type));\n \t  mpz_set_double_int (min, mn, false);\n \t}\n     }\n@@ -8407,10 +8405,10 @@ get_type_static_bounds (const_tree type, mpz_t min, mpz_t max)\n   else\n     {\n       if (TYPE_UNSIGNED (type))\n-\tmpz_set_double_int (max, double_int_mask (TYPE_PRECISION (type)),\n+\tmpz_set_double_int (max, double_int::mask (TYPE_PRECISION (type)),\n \t\t\t    true);\n       else\n-\tmpz_set_double_int (max, double_int_mask (TYPE_PRECISION (type) - 1),\n+\tmpz_set_double_int (max, double_int::mask (TYPE_PRECISION (type) - 1),\n \t\t\t    true);\n     }\n }"}, {"sha": "d81aa3c423cb7e62909ef8326a622cab4aa53742", "filename": "gcc/tree.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Ftree.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree.h?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -4718,7 +4718,7 @@ extern tree force_fit_type_double (tree, double_int, int, bool);\n static inline tree\n build_int_cstu (tree type, unsigned HOST_WIDE_INT cst)\n {\n-  return double_int_to_tree (type, uhwi_to_double_int (cst));\n+  return double_int_to_tree (type, double_int::from_uhwi (cst));\n }\n \n extern tree build_int_cst (tree, HOST_WIDE_INT);"}, {"sha": "d476b8ad7cd89dd760628212b566214a5efc9522", "filename": "gcc/varasm.c", "status": "modified", "additions": 7, "deletions": 8, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fvarasm.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7/gcc%2Fvarasm.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fvarasm.c?ref=27bcd47cfab04b1b1e6d2712e34b9b289c7a2dd7", "patch": "@@ -4649,14 +4649,13 @@ array_size_for_constructor (tree val)\n \n   /* Compute the total number of array elements.  */\n   tmp = TYPE_MIN_VALUE (TYPE_DOMAIN (TREE_TYPE (val)));\n-  i = double_int_sub (tree_to_double_int (max_index), tree_to_double_int (tmp));\n-  i = double_int_add (i, double_int_one);\n+  i = tree_to_double_int (max_index) - tree_to_double_int (tmp);\n+  i += double_int_one;\n \n   /* Multiply by the array element unit size to find number of bytes.  */\n-  i = double_int_mul (i, tree_to_double_int\n-\t\t           (TYPE_SIZE_UNIT (TREE_TYPE (TREE_TYPE (val)))));\n+  i *= tree_to_double_int (TYPE_SIZE_UNIT (TREE_TYPE (TREE_TYPE (val))));\n \n-  gcc_assert (double_int_fits_in_uhwi_p (i));\n+  gcc_assert (i.fits_uhwi ());\n   return i.low;\n }\n \n@@ -4740,9 +4739,9 @@ output_constructor_regular_field (oc_local_state *local)\n \t sign-extend the result because Ada has negative DECL_FIELD_OFFSETs\n \t but we are using an unsigned sizetype.  */\n       unsigned prec = TYPE_PRECISION (sizetype);\n-      double_int idx = double_int_sub (tree_to_double_int (local->index),\n-\t\t\t\t       tree_to_double_int (local->min_index));\n-      idx = double_int_sext (idx, prec);\n+      double_int idx = tree_to_double_int (local->index)\n+\t\t       - tree_to_double_int (local->min_index);\n+      idx = idx.sext (prec);\n       fieldpos = (tree_low_cst (TYPE_SIZE_UNIT (TREE_TYPE (local->val)), 1)\n \t\t  * idx.low);\n     }"}]}