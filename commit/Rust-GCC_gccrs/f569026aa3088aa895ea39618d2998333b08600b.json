{"sha": "f569026aa3088aa895ea39618d2998333b08600b", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZjU2OTAyNmFhMzA4OGFhODk1ZWEzOTYxOGQyOTk4MzMzYjA4NjAwYg==", "commit": {"author": {"name": "Eric Botcazou", "email": "ebotcazou@gcc.gnu.org", "date": "2020-07-16T15:50:26Z"}, "committer": {"name": "Eric Botcazou", "email": "ebotcazou@gcc.gnu.org", "date": "2020-07-16T16:13:56Z"}, "message": "Reunify x86 stack checking implementation\n\nThe stack clash protection mechanism in the x86 back-end was implemented\nby largely duplicating the existing stack checking implementation.  Now\nthe only significant difference between them is the probing window, which\nis shifted by 1 probing interval (not 2 as documented in explow.c), but we\ncan certainly do 1 more probe for stack checking even if it is redundant\nin almost all cases.\n\ngcc/ChangeLog:\n\t* config/i386/i386.c (ix86_compute_frame_layout): Minor tweak.\n\t(ix86_adjust_stack_and_probe): Delete.\n\t(ix86_adjust_stack_and_probe_stack_clash): Rename to above and add\n\tPROTECTION_AREA parameter.  If it is true, probe PROBE_INTERVAL plus\n\ta small dope beyond SIZE bytes.\n\t(ix86_emit_probe_stack_range): Use local variable.\n\t(ix86_expand_prologue): Adjust calls to ix86_adjust_stack_and_probe\n\tand tidy up the stack checking code.\n\t* explow.c (get_stack_check_protect): Fix head comment.\n\t(anti_adjust_stack_and_probe_stack_clash): Likewise.\n\t(allocate_dynamic_stack_space): Add comment.\n\n\t* tree-nested.c (lookup_field_for_decl): Set the DECL_IGNORED_P and\n\tTREE_NO_WARNING but not TREE_ADDRESSABLE flags on the field.", "tree": {"sha": "d8d1893f1871eae2ba5da2ae21162b79a053c97a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/d8d1893f1871eae2ba5da2ae21162b79a053c97a"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/f569026aa3088aa895ea39618d2998333b08600b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f569026aa3088aa895ea39618d2998333b08600b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f569026aa3088aa895ea39618d2998333b08600b", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f569026aa3088aa895ea39618d2998333b08600b/comments", "author": null, "committer": null, "parents": [{"sha": "a54d71cb664199c98fbb2694e9314566d6ca0ed2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a54d71cb664199c98fbb2694e9314566d6ca0ed2", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a54d71cb664199c98fbb2694e9314566d6ca0ed2"}], "stats": {"total": 312, "additions": 91, "deletions": 221}, "files": [{"sha": "31757b044c8850cfa63aec10f780592b2ce31496", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 71, "deletions": 206, "changes": 277, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f569026aa3088aa895ea39618d2998333b08600b/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f569026aa3088aa895ea39618d2998333b08600b/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=f569026aa3088aa895ea39618d2998333b08600b", "patch": "@@ -6169,10 +6169,7 @@ ix86_compute_frame_layout (void)\n     }\n \n   frame->save_regs_using_mov\n-    = (TARGET_PROLOGUE_USING_MOVE && m->use_fast_prologue_epilogue\n-       /* If static stack checking is enabled and done with probes,\n-\t  the registers need to be saved before allocating the frame.  */\n-       && flag_stack_check != STATIC_BUILTIN_STACK_CHECK);\n+    = TARGET_PROLOGUE_USING_MOVE && m->use_fast_prologue_epilogue;\n \n   /* Skip return address and error code in exception handler.  */\n   offset = INCOMING_FRAME_SP_OFFSET;\n@@ -6329,6 +6326,9 @@ ix86_compute_frame_layout (void)\n \n   if ((!to_allocate && frame->nregs <= 1)\n       || (TARGET_64BIT && to_allocate >= HOST_WIDE_INT_C (0x80000000))\n+       /* If static stack checking is enabled and done with probes,\n+\t  the registers need to be saved before allocating the frame.  */\n+      || flag_stack_check == STATIC_BUILTIN_STACK_CHECK\n       /* If stack clash probing needs a loop, then it needs a\n \t scratch register.  But the returned register is only guaranteed\n \t to be safe to use after register saves are complete.  So if\n@@ -7122,17 +7122,20 @@ release_scratch_register_on_entry (struct scratch_reg *sr, HOST_WIDE_INT offset,\n \n /* Emit code to adjust the stack pointer by SIZE bytes while probing it.\n \n-   This differs from the next routine in that it tries hard to prevent\n-   attacks that jump the stack guard.  Thus it is never allowed to allocate\n-   more than PROBE_INTERVAL bytes of stack space without a suitable\n-   probe.\n+   If INT_REGISTERS_SAVED is true, then integer registers have already been\n+   pushed on the stack.\n \n-   INT_REGISTERS_SAVED is true if integer registers have already been\n-   pushed on the stack.  */\n+   If PROTECTION AREA is true, then probe PROBE_INTERVAL plus a small dope\n+   beyond SIZE bytes.\n+\n+   This assumes no knowledge of the current probing state, i.e. it is never\n+   allowed to allocate more than PROBE_INTERVAL bytes of stack space without\n+   a suitable probe.  */\n \n static void\n-ix86_adjust_stack_and_probe_stack_clash (HOST_WIDE_INT size,\n-\t\t\t\t\t const bool int_registers_saved)\n+ix86_adjust_stack_and_probe (HOST_WIDE_INT size,\n+\t\t\t     const bool int_registers_saved,\n+\t\t\t     const bool protection_area)\n {\n   struct machine_function *m = cfun->machine;\n \n@@ -7194,10 +7197,17 @@ ix86_adjust_stack_and_probe_stack_clash (HOST_WIDE_INT size,\n       emit_insn (gen_blockage ());\n     }\n \n+  const HOST_WIDE_INT probe_interval = get_probe_interval ();\n+  const int dope = 4 * UNITS_PER_WORD;\n+\n+  /* If there is protection area, take it into account in the size.  */\n+  if (protection_area)\n+    size += probe_interval + dope;\n+\n   /* If we allocate less than the size of the guard statically,\n      then no probing is necessary, but we do need to allocate\n      the stack.  */\n-  if (size < (1 << param_stack_clash_protection_guard_size))\n+  else if (size < (1 << param_stack_clash_protection_guard_size))\n     {\n       pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,\n \t\t\t         GEN_INT (-size), -1,\n@@ -7209,7 +7219,6 @@ ix86_adjust_stack_and_probe_stack_clash (HOST_WIDE_INT size,\n   /* We're allocating a large enough stack frame that we need to\n      emit probes.  Either emit them inline or in a loop depending\n      on the size.  */\n-  HOST_WIDE_INT probe_interval = get_probe_interval ();\n   if (size <= 4 * probe_interval)\n     {\n       HOST_WIDE_INT i;\n@@ -7228,12 +7237,19 @@ ix86_adjust_stack_and_probe_stack_clash (HOST_WIDE_INT size,\n \t}\n \n       /* We need to allocate space for the residual, but we do not need\n-\t to probe the residual.  */\n+\t to probe the residual...  */\n       HOST_WIDE_INT residual = (i - probe_interval - size);\n       if (residual)\n-\tpro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,\n-\t\t\t\t   GEN_INT (residual), -1,\n-\t\t\t\t   m->fs.cfa_reg == stack_pointer_rtx);\n+\t{\n+\t  pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,\n+\t\t\t\t     GEN_INT (residual), -1,\n+\t\t\t\t     m->fs.cfa_reg == stack_pointer_rtx);\n+\n+\t  /* ...except if there is a protection area to maintain.  */\n+\t  if (protection_area)\n+\t    emit_stack_probe (stack_pointer_rtx);\n+\t}\n+\n       dump_stack_clash_frame_info (PROBE_INLINE, residual != 0);\n     }\n   else\n@@ -7296,186 +7312,27 @@ ix86_adjust_stack_and_probe_stack_clash (HOST_WIDE_INT size,\n \t is equal to ROUNDED_SIZE.  */\n \n       if (size != rounded_size)\n-\tpro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,\n-\t\t\t\t   GEN_INT (rounded_size - size), -1,\n-\t\t\t\t   m->fs.cfa_reg == stack_pointer_rtx);\n-      dump_stack_clash_frame_info (PROBE_LOOP, size != rounded_size);\n-\n-      /* This does not deallocate the space reserved for the scratch\n-\t register.  That will be deallocated in the epilogue.  */\n-      release_scratch_register_on_entry (&sr, size, false);\n-    }\n-\n-  /* Make sure nothing is scheduled before we are done.  */\n-  emit_insn (gen_blockage ());\n-}\n-\n-/* Emit code to adjust the stack pointer by SIZE bytes while probing it.\n-\n-   INT_REGISTERS_SAVED is true if integer registers have already been\n-   pushed on the stack.  */\n-\n-static void\n-ix86_adjust_stack_and_probe (HOST_WIDE_INT size,\n-\t\t\t     const bool int_registers_saved)\n-{\n-  /* We skip the probe for the first interval + a small dope of 4 words and\n-     probe that many bytes past the specified size to maintain a protection\n-     area at the botton of the stack.  */\n-  const int dope = 4 * UNITS_PER_WORD;\n-  rtx size_rtx = GEN_INT (size), last;\n-\n-  /* See if we have a constant small number of probes to generate.  If so,\n-     that's the easy case.  The run-time loop is made up of 9 insns in the\n-     generic case while the compile-time loop is made up of 3+2*(n-1) insns\n-     for n # of intervals.  */\n-  if (size <= 4 * get_probe_interval ())\n-    {\n-      HOST_WIDE_INT i, adjust;\n-      bool first_probe = true;\n-\n-      /* Adjust SP and probe at PROBE_INTERVAL + N * PROBE_INTERVAL for\n-\t values of N from 1 until it exceeds SIZE.  If only one probe is\n-\t needed, this will not generate any code.  Then adjust and probe\n-\t to PROBE_INTERVAL + SIZE.  */\n-      for (i = get_probe_interval (); i < size; i += get_probe_interval ())\n \t{\n-\t  if (first_probe)\n-\t    {\n-\t      adjust = 2 * get_probe_interval () + dope;\n-\t      first_probe = false;\n-\t    }\n-\t  else\n-\t    adjust = get_probe_interval ();\n-\n-\t  emit_insn (gen_rtx_SET (stack_pointer_rtx,\n-\t\t\t\t  plus_constant (Pmode, stack_pointer_rtx,\n-\t\t\t\t\t\t -adjust)));\n-\t  emit_stack_probe (stack_pointer_rtx);\n-\t}\n-\n-      if (first_probe)\n-\tadjust = size + get_probe_interval () + dope;\n-      else\n-        adjust = size + get_probe_interval () - i;\n-\n-      emit_insn (gen_rtx_SET (stack_pointer_rtx,\n-\t\t\t      plus_constant (Pmode, stack_pointer_rtx,\n-\t\t\t\t\t     -adjust)));\n-      emit_stack_probe (stack_pointer_rtx);\n-\n-      /* Adjust back to account for the additional first interval.  */\n-      last = emit_insn (gen_rtx_SET (stack_pointer_rtx,\n-\t\t\t\t     plus_constant (Pmode, stack_pointer_rtx,\n-\t\t\t\t\t\t    (get_probe_interval ()\n-\t\t\t\t\t\t     + dope))));\n-    }\n-\n-  /* Otherwise, do the same as above, but in a loop.  Note that we must be\n-     extra careful with variables wrapping around because we might be at\n-     the very top (or the very bottom) of the address space and we have\n-     to be able to handle this case properly; in particular, we use an\n-     equality test for the loop condition.  */\n-  else\n-    {\n-      /* We expect the GP registers to be saved when probes are used\n-\t as the probing sequences might need a scratch register and\n-\t the routine to allocate one assumes the integer registers\n-\t have already been saved.  */\n-      gcc_assert (int_registers_saved);\n-\n-      HOST_WIDE_INT rounded_size;\n-      struct scratch_reg sr;\n-\n-      get_scratch_register_on_entry (&sr);\n-\n-      /* If we needed to save a register, then account for any space\n-\t that was pushed (we are not going to pop the register when\n-\t we do the restore).  */\n-      if (sr.saved)\n-\tsize -= UNITS_PER_WORD;\n-\n-      /* Step 1: round SIZE to the previous multiple of the interval.  */\n-\n-      rounded_size = ROUND_DOWN (size, get_probe_interval ());\n-\n-\n-      /* Step 2: compute initial and final value of the loop counter.  */\n-\n-      /* SP = SP_0 + PROBE_INTERVAL.  */\n-      emit_insn (gen_rtx_SET (stack_pointer_rtx,\n-\t\t\t      plus_constant (Pmode, stack_pointer_rtx,\n-\t\t\t\t\t     - (get_probe_interval () + dope))));\n-\n-      /* LAST_ADDR = SP_0 + PROBE_INTERVAL + ROUNDED_SIZE.  */\n-      if (rounded_size <= (HOST_WIDE_INT_1 << 31))\n-\temit_insn (gen_rtx_SET (sr.reg,\n-\t\t\t\tplus_constant (Pmode, stack_pointer_rtx,\n-\t\t\t\t\t       -rounded_size)));\n-      else\n-\t{\n-\t  emit_move_insn (sr.reg, GEN_INT (-rounded_size));\n-\t  emit_insn (gen_rtx_SET (sr.reg,\n-\t\t\t\t  gen_rtx_PLUS (Pmode, sr.reg,\n-\t\t\t\t\t\tstack_pointer_rtx)));\n-\t}\n-\n-\n-      /* Step 3: the loop\n-\n-\t do\n-\t   {\n-\t     SP = SP + PROBE_INTERVAL\n-\t     probe at SP\n-\t   }\n-\t while (SP != LAST_ADDR)\n-\n-\t adjusts SP and probes to PROBE_INTERVAL + N * PROBE_INTERVAL for\n-\t values of N from 1 until it is equal to ROUNDED_SIZE.  */\n-\n-      emit_insn (gen_adjust_stack_and_probe (Pmode, sr.reg, sr.reg, size_rtx));\n-\n-\n-      /* Step 4: adjust SP and probe at PROBE_INTERVAL + SIZE if we cannot\n-\t assert at compile-time that SIZE is equal to ROUNDED_SIZE.  */\n+\t  pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,\n+\t\t\t\t     GEN_INT (rounded_size - size), -1,\n+\t\t\t\t     m->fs.cfa_reg == stack_pointer_rtx);\n \n-      if (size != rounded_size)\n-\t{\n-\t  emit_insn (gen_rtx_SET (stack_pointer_rtx,\n-\t\t\t          plus_constant (Pmode, stack_pointer_rtx,\n-\t\t\t\t\t\t rounded_size - size)));\n-\t  emit_stack_probe (stack_pointer_rtx);\n+\t  if (protection_area)\n+\t    emit_stack_probe (stack_pointer_rtx);\n \t}\n \n-      /* Adjust back to account for the additional first interval.  */\n-      last = emit_insn (gen_rtx_SET (stack_pointer_rtx,\n-\t\t\t\t     plus_constant (Pmode, stack_pointer_rtx,\n-\t\t\t\t\t\t    (get_probe_interval ()\n-\t\t\t\t\t\t     + dope))));\n+      dump_stack_clash_frame_info (PROBE_LOOP, size != rounded_size);\n \n       /* This does not deallocate the space reserved for the scratch\n \t register.  That will be deallocated in the epilogue.  */\n       release_scratch_register_on_entry (&sr, size, false);\n     }\n \n-  /* Even if the stack pointer isn't the CFA register, we need to correctly\n-     describe the adjustments made to it, in particular differentiate the\n-     frame-related ones from the frame-unrelated ones.  */\n-  if (size > 0)\n-    {\n-      rtx expr = gen_rtx_SEQUENCE (VOIDmode, rtvec_alloc (2));\n-      XVECEXP (expr, 0, 0)\n-\t= gen_rtx_SET (stack_pointer_rtx,\n-\t\t       plus_constant (Pmode, stack_pointer_rtx, -size));\n-      XVECEXP (expr, 0, 1)\n-\t= gen_rtx_SET (stack_pointer_rtx,\n-\t\t       plus_constant (Pmode, stack_pointer_rtx,\n-\t\t\t\t      get_probe_interval () + dope + size));\n-      add_reg_note (last, REG_FRAME_RELATED_EXPR, expr);\n-      RTX_FRAME_RELATED_P (last) = 1;\n-\n-      cfun->machine->fs.sp_offset += size;\n-    }\n+  /* Adjust back to account for the protection area.  */\n+  if (protection_area)\n+    pro_epilogue_adjust_stack (stack_pointer_rtx, stack_pointer_rtx,\n+\t\t\t       GEN_INT (probe_interval + dope), -1,\n+\t\t\t       m->fs.cfa_reg == stack_pointer_rtx);\n \n   /* Make sure nothing is scheduled before we are done.  */\n   emit_insn (gen_blockage ());\n@@ -7527,18 +7384,20 @@ static void\n ix86_emit_probe_stack_range (HOST_WIDE_INT first, HOST_WIDE_INT size,\n \t\t\t     const bool int_registers_saved)\n {\n+  const HOST_WIDE_INT probe_interval = get_probe_interval ();\n+\n   /* See if we have a constant small number of probes to generate.  If so,\n      that's the easy case.  The run-time loop is made up of 6 insns in the\n      generic case while the compile-time loop is made up of n insns for n #\n      of intervals.  */\n-  if (size <= 6 * get_probe_interval ())\n+  if (size <= 6 * probe_interval)\n     {\n       HOST_WIDE_INT i;\n \n       /* Probe at FIRST + N * PROBE_INTERVAL for values of N from 1 until\n \t it exceeds SIZE.  If only one probe is needed, this will not\n \t generate any code.  Then probe at FIRST + SIZE.  */\n-      for (i = get_probe_interval (); i < size; i += get_probe_interval ())\n+      for (i = probe_interval; i < size; i += probe_interval)\n \temit_stack_probe (plus_constant (Pmode, stack_pointer_rtx,\n \t\t\t\t\t -(first + i)));\n \n@@ -7567,7 +7426,7 @@ ix86_emit_probe_stack_range (HOST_WIDE_INT first, HOST_WIDE_INT size,\n \n       /* Step 1: round SIZE to the previous multiple of the interval.  */\n \n-      rounded_size = ROUND_DOWN (size, get_probe_interval ());\n+      rounded_size = ROUND_DOWN (size, probe_interval);\n \n \n       /* Step 2: compute initial and final value of the loop counter.  */\n@@ -8324,27 +8183,33 @@ ix86_expand_prologue (void)\n       sse_registers_saved = true;\n     }\n \n+  /* If stack clash protection is requested, then probe the stack.  */\n+  if (allocate >= 0 && flag_stack_clash_protection)\n+    {\n+      ix86_adjust_stack_and_probe (allocate, int_registers_saved, false);\n+      allocate = 0;\n+    }\n+\n   /* The stack has already been decremented by the instruction calling us\n      so probe if the size is non-negative to preserve the protection area.  */\n-  if (allocate >= 0\n-      && (flag_stack_check == STATIC_BUILTIN_STACK_CHECK\n-\t  || flag_stack_clash_protection))\n+  else if (allocate >= 0 && flag_stack_check == STATIC_BUILTIN_STACK_CHECK)\n     {\n-      if (flag_stack_clash_protection)\n-\t{\n-\t  ix86_adjust_stack_and_probe_stack_clash (allocate,\n-\t\t\t\t\t\t   int_registers_saved);\n-\t  allocate = 0;\n-\t}\n-      else if (STACK_CHECK_MOVING_SP)\n+      const HOST_WIDE_INT probe_interval = get_probe_interval ();\n+\n+      if (STACK_CHECK_MOVING_SP)\n \t{\n-\t  if (!(crtl->is_leaf && !cfun->calls_alloca\n-\t\t&& allocate <= get_probe_interval ()))\n+\t  if (crtl->is_leaf\n+\t      && !cfun->calls_alloca\n+\t      && allocate <= probe_interval)\n+\t    ;\n+\n+\t  else\n \t    {\n-\t      ix86_adjust_stack_and_probe (allocate, int_registers_saved);\n+\t      ix86_adjust_stack_and_probe (allocate, int_registers_saved, true);\n \t      allocate = 0;\n \t    }\n \t}\n+\n       else\n \t{\n \t  HOST_WIDE_INT size = allocate;\n@@ -8356,7 +8221,7 @@ ix86_expand_prologue (void)\n \t    {\n \t      if (crtl->is_leaf && !cfun->calls_alloca)\n \t\t{\n-\t\t  if (size > get_probe_interval ())\n+\t\t  if (size > probe_interval)\n \t\t    ix86_emit_probe_stack_range (0, size, int_registers_saved);\n \t\t}\n \t      else\n@@ -8368,7 +8233,7 @@ ix86_expand_prologue (void)\n \t    {\n \t      if (crtl->is_leaf && !cfun->calls_alloca)\n \t\t{\n-\t\t  if (size > get_probe_interval ()\n+\t\t  if (size > probe_interval\n \t\t      && size > get_stack_check_protect ())\n \t\t    ix86_emit_probe_stack_range (get_stack_check_protect (),\n \t\t\t\t\t\t (size"}, {"sha": "0fbc6d25b816457a3d13ed45d16b5dd0513cfacd", "filename": "gcc/explow.c", "status": "modified", "additions": 12, "deletions": 8, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f569026aa3088aa895ea39618d2998333b08600b/gcc%2Fexplow.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f569026aa3088aa895ea39618d2998333b08600b/gcc%2Fexplow.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexplow.c?ref=f569026aa3088aa895ea39618d2998333b08600b", "patch": "@@ -1293,24 +1293,26 @@ get_dynamic_stack_size (rtx *psize, unsigned size_align,\n \n /* Return the number of bytes to \"protect\" on the stack for -fstack-check.\n \n-   \"protect\" in the context of -fstack-check means how many bytes we\n-   should always ensure are available on the stack.  More importantly\n-   this is how many bytes are skipped when probing the stack.\n+   \"protect\" in the context of -fstack-check means how many bytes we need\n+   to always ensure are available on the stack; as a consequence, this is\n+   also how many bytes are first skipped when probing the stack.\n \n    On some targets we want to reuse the -fstack-check prologue support\n    to give a degree of protection against stack clashing style attacks.\n \n    In that scenario we do not want to skip bytes before probing as that\n    would render the stack clash protections useless.\n \n-   So we never use STACK_CHECK_PROTECT directly.  Instead we indirect though\n-   this helper which allows us to provide different values for\n-   -fstack-check and -fstack-clash-protection.  */\n+   So we never use STACK_CHECK_PROTECT directly.  Instead we indirectly\n+   use it through this helper, which allows to provide different values\n+   for -fstack-check and -fstack-clash-protection.  */\n+\n HOST_WIDE_INT\n get_stack_check_protect (void)\n {\n   if (flag_stack_clash_protection)\n     return 0;\n+\n  return STACK_CHECK_PROTECT;\n }\n \n@@ -1532,6 +1534,8 @@ allocate_dynamic_stack_space (rtx size, unsigned size_align,\n \n       saved_stack_pointer_delta = stack_pointer_delta;\n \n+      /* If stack checking or stack clash protection is requested,\n+\t then probe the stack while allocating space from it.  */\n       if (flag_stack_check && STACK_CHECK_MOVING_SP)\n \tanti_adjust_stack_and_probe (size, false);\n       else if (flag_stack_clash_protection)\n@@ -1940,8 +1944,8 @@ emit_stack_clash_protection_probe_loop_end (rtx loop_lab, rtx end_loop,\n \tprobes were not emitted.\n \n      2. It never skips probes, whereas anti_adjust_stack_and_probe will\n-\tskip probes on the first couple PROBE_INTERVALs on the assumption\n-\tthey're done elsewhere.\n+\tskip the probe on the first PROBE_INTERVAL on the assumption it\n+\twas already done in the prologue and in previous allocations.\n \n      3. It only allocates and probes SIZE bytes, it does not need to\n \tallocate/probe beyond that because this probing style does not"}, {"sha": "4dc5533be843332941aeebeefe715910aab2f5df", "filename": "gcc/tree-nested.c", "status": "modified", "additions": 8, "deletions": 7, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f569026aa3088aa895ea39618d2998333b08600b/gcc%2Ftree-nested.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f569026aa3088aa895ea39618d2998333b08600b/gcc%2Ftree-nested.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-nested.c?ref=f569026aa3088aa895ea39618d2998333b08600b", "patch": "@@ -305,13 +305,14 @@ lookup_field_for_decl (struct nesting_info *info, tree decl,\n \t}\n       else\n \t{\n-          TREE_TYPE (field) = TREE_TYPE (decl);\n-          DECL_SOURCE_LOCATION (field) = DECL_SOURCE_LOCATION (decl);\n-          SET_DECL_ALIGN (field, DECL_ALIGN (decl));\n-          DECL_USER_ALIGN (field) = DECL_USER_ALIGN (decl);\n-          TREE_ADDRESSABLE (field) = TREE_ADDRESSABLE (decl);\n-          DECL_NONADDRESSABLE_P (field) = !TREE_ADDRESSABLE (decl);\n-          TREE_THIS_VOLATILE (field) = TREE_THIS_VOLATILE (decl);\n+\t  TREE_TYPE (field) = TREE_TYPE (decl);\n+\t  DECL_SOURCE_LOCATION (field) = DECL_SOURCE_LOCATION (decl);\n+\t  SET_DECL_ALIGN (field, DECL_ALIGN (decl));\n+\t  DECL_USER_ALIGN (field) = DECL_USER_ALIGN (decl);\n+\t  DECL_IGNORED_P (field) = DECL_IGNORED_P (decl);\n+\t  DECL_NONADDRESSABLE_P (field) = !TREE_ADDRESSABLE (decl);\n+\t  TREE_NO_WARNING (field) = TREE_NO_WARNING (decl);\n+\t  TREE_THIS_VOLATILE (field) = TREE_THIS_VOLATILE (decl);\n \n \t  /* Declare the transformation and adjust the original DECL.  For a\n \t     variable or for a parameter when not optimizing, we make it point"}]}