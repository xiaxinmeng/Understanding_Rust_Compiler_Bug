{"sha": "928353177b4843b05a7675d1b504086b3eb5686a", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OTI4MzUzMTc3YjQ4NDNiMDVhNzY3NWQxYjUwNDA4NmIzZWI1Njg2YQ==", "commit": {"author": {"name": "Tejas Belagod", "email": "tejas.belagod@arm.com", "date": "2013-11-22T15:34:36Z"}, "committer": {"name": "Tejas Belagod", "email": "belagod@gcc.gnu.org", "date": "2013-11-22T15:34:36Z"}, "message": "aarch64-simd.md (vec_pack_trunc_<mode>, [...]): Swap for big-endian.\n\n2013-11-22  Tejas Belagod  <tejas.belagod@arm.com>\n\ngcc/\n\t* config/aarch64/aarch64-simd.md (vec_pack_trunc_<mode>,\n\tvec_pack_trunc_v2df, vec_pack_trunc_df): Swap for big-endian.\n\t(reduc_<sur>plus_<mode>): Factorize V2DI into this.\n\t(reduc_<sur>plus_<mode>): Change this to reduc_splus_<mode> for floats\n\tand also change to float UNSPEC.\n\t(reduc_maxmin_uns>_<mode>): Remove V2DI.\n\t* config/aarch64/arm_neon.h (vaddv<q>_<suf><8,16,32,64>,\n        vmaxv<q>_<suf><8,16,32,64>, vminv<q>_<suf><8,16,32,64>): Fix up scalar\n\tresult access for big-endian.\n        (__LANE0): New macro used to fix up lane access of 'across-lanes'\n         intrinsics for big-endian.\n\t* config/aarch64/iterators.md (VDQV): Add V2DI.\n\t(VDQV_S): New.\n\t(vp): New mode attribute.\n\nFrom-SVN: r205269", "tree": {"sha": "d7f6253ef73018416826d1411fb97175bba312ce", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/d7f6253ef73018416826d1411fb97175bba312ce"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/928353177b4843b05a7675d1b504086b3eb5686a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/928353177b4843b05a7675d1b504086b3eb5686a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/928353177b4843b05a7675d1b504086b3eb5686a", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/928353177b4843b05a7675d1b504086b3eb5686a/comments", "author": {"login": "tejas-belagod-arm", "id": 92718852, "node_id": "U_kgDOBYbHBA", "avatar_url": "https://avatars.githubusercontent.com/u/92718852?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tejas-belagod-arm", "html_url": "https://github.com/tejas-belagod-arm", "followers_url": "https://api.github.com/users/tejas-belagod-arm/followers", "following_url": "https://api.github.com/users/tejas-belagod-arm/following{/other_user}", "gists_url": "https://api.github.com/users/tejas-belagod-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/tejas-belagod-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tejas-belagod-arm/subscriptions", "organizations_url": "https://api.github.com/users/tejas-belagod-arm/orgs", "repos_url": "https://api.github.com/users/tejas-belagod-arm/repos", "events_url": "https://api.github.com/users/tejas-belagod-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/tejas-belagod-arm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "8fcc1c1fc2aaef9e008ed240739d9796185dac39", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8fcc1c1fc2aaef9e008ed240739d9796185dac39", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8fcc1c1fc2aaef9e008ed240739d9796185dac39"}], "stats": {"total": 213, "additions": 128, "deletions": 85}, "files": [{"sha": "600da98e87870e227e95dbfed1a9bb2de4028e4e", "filename": "gcc/ChangeLog", "status": "modified", "additions": 17, "deletions": 0, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/928353177b4843b05a7675d1b504086b3eb5686a/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/928353177b4843b05a7675d1b504086b3eb5686a/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=928353177b4843b05a7675d1b504086b3eb5686a", "patch": "@@ -1,3 +1,20 @@\n+2013-11-22  Tejas Belagod  <tejas.belagod@arm.com>\n+\n+\t* config/aarch64/aarch64-simd.md (vec_pack_trunc_<mode>,\n+\tvec_pack_trunc_v2df, vec_pack_trunc_df): Swap for big-endian.\n+\t(reduc_<sur>plus_<mode>): Factorize V2DI into this.\n+\t(reduc_<sur>plus_<mode>): Change this to reduc_splus_<mode> for floats\n+\tand also change to float UNSPEC.\n+\t(reduc_maxmin_uns>_<mode>): Remove V2DI.\n+\t* config/aarch64/arm_neon.h (vaddv<q>_<suf><8,16,32,64>,\n+\tvmaxv<q>_<suf><8,16,32,64>, vminv<q>_<suf><8,16,32,64>): Fix up scalar\n+\tresult access for big-endian.\n+\t(__LANE0): New macro used to fix up lane access of 'across-lanes'\n+\tintrinsics for big-endian.\n+\t* config/aarch64/iterators.md (VDQV): Add V2DI.\n+\t(VDQV_S): New.\n+\t(vp): New mode attribute.\n+\n 2013-11-22  Tejas Belagod  <tejas.belagod@arm.com>\n \n \t* config/aarch64/aarch64-simd.md (vec_pack_trunc_<mode>,"}, {"sha": "5dcbc62a2905fc99a832691f0e53d985e478d3b1", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 9, "deletions": 28, "changes": 37, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/928353177b4843b05a7675d1b504086b3eb5686a/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/928353177b4843b05a7675d1b504086b3eb5686a/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=928353177b4843b05a7675d1b504086b3eb5686a", "patch": "@@ -1557,19 +1557,10 @@\n        (unspec:VDQV [(match_operand:VDQV 1 \"register_operand\" \"w\")]\n \t\t    SUADDV))]\n  \"TARGET_SIMD\"\n- \"addv\\\\t%<Vetype>0, %1.<Vtype>\"\n+ \"add<VDQV:vp>\\\\t%<Vetype>0, %1.<Vtype>\"\n   [(set_attr \"type\" \"neon_reduc_add<q>\")]\n )\n \n-(define_insn \"reduc_<sur>plus_v2di\"\n- [(set (match_operand:V2DI 0 \"register_operand\" \"=w\")\n-       (unspec:V2DI [(match_operand:V2DI 1 \"register_operand\" \"w\")]\n-\t\t    SUADDV))]\n- \"TARGET_SIMD\"\n- \"addp\\\\t%d0, %1.2d\"\n-  [(set_attr \"type\" \"neon_reduc_add_q\")]\n-)\n-\n (define_insn \"reduc_<sur>plus_v2si\"\n  [(set (match_operand:V2SI 0 \"register_operand\" \"=w\")\n        (unspec:V2SI [(match_operand:V2SI 1 \"register_operand\" \"w\")]\n@@ -1579,10 +1570,10 @@\n   [(set_attr \"type\" \"neon_reduc_add\")]\n )\n \n-(define_insn \"reduc_<sur>plus_<mode>\"\n+(define_insn \"reduc_splus_<mode>\"\n  [(set (match_operand:V2F 0 \"register_operand\" \"=w\")\n        (unspec:V2F [(match_operand:V2F 1 \"register_operand\" \"w\")]\n-\t\t    SUADDV))]\n+\t\t   UNSPEC_FADDV))]\n  \"TARGET_SIMD\"\n  \"faddp\\\\t%<Vetype>0, %1.<Vtype>\"\n   [(set_attr \"type\" \"neon_fp_reduc_add_<Vetype><q>\")]\n@@ -1597,15 +1588,14 @@\n   [(set_attr \"type\" \"neon_fp_reduc_add_s_q\")]\n )\n \n-(define_expand \"reduc_<sur>plus_v4sf\"\n+(define_expand \"reduc_splus_v4sf\"\n  [(set (match_operand:V4SF 0 \"register_operand\")\n        (unspec:V4SF [(match_operand:V4SF 1 \"register_operand\")]\n-\t\t    SUADDV))]\n+\t\t    UNSPEC_FADDV))]\n  \"TARGET_SIMD\"\n {\n-  rtx tmp = gen_reg_rtx (V4SFmode);\n-  emit_insn (gen_aarch64_addpv4sf (tmp, operands[1]));\n-  emit_insn (gen_aarch64_addpv4sf (operands[0], tmp));\n+  emit_insn (gen_aarch64_addpv4sf (operands[0], operands[1]));\n+  emit_insn (gen_aarch64_addpv4sf (operands[0], operands[0]));\n   DONE;\n })\n \n@@ -1620,23 +1610,14 @@\n ;; 'across lanes' max and min ops.\n \n (define_insn \"reduc_<maxmin_uns>_<mode>\"\n- [(set (match_operand:VDQV 0 \"register_operand\" \"=w\")\n-       (unspec:VDQV [(match_operand:VDQV 1 \"register_operand\" \"w\")]\n+ [(set (match_operand:VDQV_S 0 \"register_operand\" \"=w\")\n+       (unspec:VDQV_S [(match_operand:VDQV_S 1 \"register_operand\" \"w\")]\n \t\t    MAXMINV))]\n  \"TARGET_SIMD\"\n  \"<maxmin_uns_op>v\\\\t%<Vetype>0, %1.<Vtype>\"\n   [(set_attr \"type\" \"neon_reduc_minmax<q>\")]\n )\n \n-(define_insn \"reduc_<maxmin_uns>_v2di\"\n- [(set (match_operand:V2DI 0 \"register_operand\" \"=w\")\n-       (unspec:V2DI [(match_operand:V2DI 1 \"register_operand\" \"w\")]\n-\t\t    MAXMINV))]\n- \"TARGET_SIMD\"\n- \"<maxmin_uns_op>p\\\\t%d0, %1.2d\"\n-  [(set_attr \"type\" \"neon_reduc_minmax_q\")]\n-)\n-\n (define_insn \"reduc_<maxmin_uns>_v2si\"\n  [(set (match_operand:V2SI 0 \"register_operand\" \"=w\")\n        (unspec:V2SI [(match_operand:V2SI 1 \"register_operand\" \"w\")]"}, {"sha": "f03d001013a0e4af18121b9b3e90e00001e17b93", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 92, "deletions": 56, "changes": 148, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/928353177b4843b05a7675d1b504086b3eb5686a/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/928353177b4843b05a7675d1b504086b3eb5686a/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=928353177b4843b05a7675d1b504086b3eb5686a", "patch": "@@ -15913,118 +15913,132 @@ vaddd_u64 (uint64x1_t __a, uint64x1_t __b)\n   return __a + __b;\n }\n \n+#if __AARCH64EB__\n+#define __LANE0(__t) ((__t) - 1)\n+#else\n+#define __LANE0(__t) 0\n+#endif\n+\n /* vaddv */\n \n __extension__ static __inline int8_t __attribute__ ((__always_inline__))\n vaddv_s8 (int8x8_t __a)\n {\n-  return vget_lane_s8 (__builtin_aarch64_reduc_splus_v8qi (__a), 0);\n+  return vget_lane_s8 (__builtin_aarch64_reduc_splus_v8qi (__a), __LANE0 (8));\n }\n \n __extension__ static __inline int16_t __attribute__ ((__always_inline__))\n vaddv_s16 (int16x4_t __a)\n {\n-  return vget_lane_s16 (__builtin_aarch64_reduc_splus_v4hi (__a), 0);\n+  return vget_lane_s16 (__builtin_aarch64_reduc_splus_v4hi (__a), __LANE0 (4));\n }\n \n __extension__ static __inline int32_t __attribute__ ((__always_inline__))\n vaddv_s32 (int32x2_t __a)\n {\n-  return vget_lane_s32 (__builtin_aarch64_reduc_splus_v2si (__a), 0);\n+  return vget_lane_s32 (__builtin_aarch64_reduc_splus_v2si (__a), __LANE0 (2));\n }\n \n __extension__ static __inline uint8_t __attribute__ ((__always_inline__))\n vaddv_u8 (uint8x8_t __a)\n {\n   return vget_lane_u8 ((uint8x8_t)\n-\t\t__builtin_aarch64_reduc_uplus_v8qi ((int8x8_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_uplus_v8qi ((int8x8_t) __a),\n+\t\t__LANE0 (8));\n }\n \n __extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n vaddv_u16 (uint16x4_t __a)\n {\n   return vget_lane_u16 ((uint16x4_t)\n-\t\t__builtin_aarch64_reduc_uplus_v4hi ((int16x4_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_uplus_v4hi ((int16x4_t) __a),\n+\t\t__LANE0 (4));\n }\n \n __extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n vaddv_u32 (uint32x2_t __a)\n {\n   return vget_lane_u32 ((uint32x2_t)\n-\t\t__builtin_aarch64_reduc_uplus_v2si ((int32x2_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_uplus_v2si ((int32x2_t) __a),\n+\t\t__LANE0 (2));\n }\n \n __extension__ static __inline int8_t __attribute__ ((__always_inline__))\n vaddvq_s8 (int8x16_t __a)\n {\n-  return vgetq_lane_s8 (__builtin_aarch64_reduc_splus_v16qi (__a), 0);\n+  return vgetq_lane_s8 (__builtin_aarch64_reduc_splus_v16qi (__a),\n+\t\t\t__LANE0 (16));\n }\n \n __extension__ static __inline int16_t __attribute__ ((__always_inline__))\n vaddvq_s16 (int16x8_t __a)\n {\n-  return vgetq_lane_s16 (__builtin_aarch64_reduc_splus_v8hi (__a), 0);\n+  return vgetq_lane_s16 (__builtin_aarch64_reduc_splus_v8hi (__a), __LANE0 (8));\n }\n \n __extension__ static __inline int32_t __attribute__ ((__always_inline__))\n vaddvq_s32 (int32x4_t __a)\n {\n-  return vgetq_lane_s32 (__builtin_aarch64_reduc_splus_v4si (__a), 0);\n+  return vgetq_lane_s32 (__builtin_aarch64_reduc_splus_v4si (__a), __LANE0 (4));\n }\n \n __extension__ static __inline int64_t __attribute__ ((__always_inline__))\n vaddvq_s64 (int64x2_t __a)\n {\n-  return vgetq_lane_s64 (__builtin_aarch64_reduc_splus_v2di (__a), 0);\n+  return vgetq_lane_s64 (__builtin_aarch64_reduc_splus_v2di (__a), __LANE0 (2));\n }\n \n __extension__ static __inline uint8_t __attribute__ ((__always_inline__))\n vaddvq_u8 (uint8x16_t __a)\n {\n   return vgetq_lane_u8 ((uint8x16_t)\n-\t\t__builtin_aarch64_reduc_uplus_v16qi ((int8x16_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_uplus_v16qi ((int8x16_t) __a),\n+\t\t__LANE0 (16));\n }\n \n __extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n vaddvq_u16 (uint16x8_t __a)\n {\n   return vgetq_lane_u16 ((uint16x8_t)\n-\t\t__builtin_aarch64_reduc_uplus_v8hi ((int16x8_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_uplus_v8hi ((int16x8_t) __a),\n+\t\t__LANE0 (8));\n }\n \n __extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n vaddvq_u32 (uint32x4_t __a)\n {\n   return vgetq_lane_u32 ((uint32x4_t)\n-\t\t__builtin_aarch64_reduc_uplus_v4si ((int32x4_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_uplus_v4si ((int32x4_t) __a),\n+\t\t__LANE0 (4));\n }\n \n __extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n vaddvq_u64 (uint64x2_t __a)\n {\n   return vgetq_lane_u64 ((uint64x2_t)\n-\t\t__builtin_aarch64_reduc_uplus_v2di ((int64x2_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_uplus_v2di ((int64x2_t) __a),\n+\t\t__LANE0 (2));\n }\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vaddv_f32 (float32x2_t __a)\n {\n-  float32x2_t t = __builtin_aarch64_reduc_splus_v2sf (__a);\n-  return vget_lane_f32 (t, 0);\n+  float32x2_t __t = __builtin_aarch64_reduc_splus_v2sf (__a);\n+  return vget_lane_f32 (__t, __LANE0 (2));\n }\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vaddvq_f32 (float32x4_t __a)\n {\n-  float32x4_t t = __builtin_aarch64_reduc_splus_v4sf (__a);\n-  return vgetq_lane_f32 (t, 0);\n+  float32x4_t __t = __builtin_aarch64_reduc_splus_v4sf (__a);\n+  return vgetq_lane_f32 (__t, __LANE0 (4));\n }\n \n __extension__ static __inline float64_t __attribute__ ((__always_inline__))\n vaddvq_f64 (float64x2_t __a)\n {\n-  float64x2_t t = __builtin_aarch64_reduc_splus_v2df (__a);\n-  return vgetq_lane_f64 (t, 0);\n+  float64x2_t __t = __builtin_aarch64_reduc_splus_v2df (__a);\n+  return vgetq_lane_f64 (__t, __LANE0 (2));\n }\n \n /* vcage  */\n@@ -20265,117 +20279,127 @@ vmaxnmq_f64 (float64x2_t __a, float64x2_t __b)\n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vmaxv_f32 (float32x2_t __a)\n {\n-  return vget_lane_f32 (__builtin_aarch64_reduc_smax_nan_v2sf (__a), 0);\n+  return vget_lane_f32 (__builtin_aarch64_reduc_smax_nan_v2sf (__a),\n+\t\t\t__LANE0 (2));\n }\n \n __extension__ static __inline int8_t __attribute__ ((__always_inline__))\n vmaxv_s8 (int8x8_t __a)\n {\n-  return vget_lane_s8 (__builtin_aarch64_reduc_smax_v8qi (__a), 0);\n+  return vget_lane_s8 (__builtin_aarch64_reduc_smax_v8qi (__a), __LANE0 (8));\n }\n \n __extension__ static __inline int16_t __attribute__ ((__always_inline__))\n vmaxv_s16 (int16x4_t __a)\n {\n-  return vget_lane_s16 (__builtin_aarch64_reduc_smax_v4hi (__a), 0);\n+  return vget_lane_s16 (__builtin_aarch64_reduc_smax_v4hi (__a), __LANE0 (4));\n }\n \n __extension__ static __inline int32_t __attribute__ ((__always_inline__))\n vmaxv_s32 (int32x2_t __a)\n {\n-  return vget_lane_s32 (__builtin_aarch64_reduc_smax_v2si (__a), 0);\n+  return vget_lane_s32 (__builtin_aarch64_reduc_smax_v2si (__a), __LANE0 (2));\n }\n \n __extension__ static __inline uint8_t __attribute__ ((__always_inline__))\n vmaxv_u8 (uint8x8_t __a)\n {\n   return vget_lane_u8 ((uint8x8_t)\n-\t\t__builtin_aarch64_reduc_umax_v8qi ((int8x8_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_umax_v8qi ((int8x8_t) __a),\n+\t\t__LANE0 (8));\n }\n \n __extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n vmaxv_u16 (uint16x4_t __a)\n {\n   return vget_lane_u16 ((uint16x4_t)\n-\t\t__builtin_aarch64_reduc_umax_v4hi ((int16x4_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_umax_v4hi ((int16x4_t) __a),\n+\t\t__LANE0 (4));\n }\n \n __extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n vmaxv_u32 (uint32x2_t __a)\n {\n   return vget_lane_u32 ((uint32x2_t)\n-\t\t__builtin_aarch64_reduc_umax_v2si ((int32x2_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_umax_v2si ((int32x2_t) __a),\n+\t\t__LANE0 (2));\n }\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vmaxvq_f32 (float32x4_t __a)\n {\n-  return vgetq_lane_f32 (__builtin_aarch64_reduc_smax_nan_v4sf (__a), 0);\n+  return vgetq_lane_f32 (__builtin_aarch64_reduc_smax_nan_v4sf (__a),\n+\t\t\t __LANE0 (4));\n }\n \n __extension__ static __inline float64_t __attribute__ ((__always_inline__))\n vmaxvq_f64 (float64x2_t __a)\n {\n-  return vgetq_lane_f64 (__builtin_aarch64_reduc_smax_nan_v2df (__a), 0);\n+  return vgetq_lane_f64 (__builtin_aarch64_reduc_smax_nan_v2df (__a),\n+\t\t\t __LANE0 (2));\n }\n \n __extension__ static __inline int8_t __attribute__ ((__always_inline__))\n vmaxvq_s8 (int8x16_t __a)\n {\n-  return vgetq_lane_s8 (__builtin_aarch64_reduc_smax_v16qi (__a), 0);\n+  return vgetq_lane_s8 (__builtin_aarch64_reduc_smax_v16qi (__a), __LANE0 (16));\n }\n \n __extension__ static __inline int16_t __attribute__ ((__always_inline__))\n vmaxvq_s16 (int16x8_t __a)\n {\n-  return vgetq_lane_s16 (__builtin_aarch64_reduc_smax_v8hi (__a), 0);\n+  return vgetq_lane_s16 (__builtin_aarch64_reduc_smax_v8hi (__a), __LANE0 (8));\n }\n \n __extension__ static __inline int32_t __attribute__ ((__always_inline__))\n vmaxvq_s32 (int32x4_t __a)\n {\n-  return vgetq_lane_s32 (__builtin_aarch64_reduc_smax_v4si (__a), 0);\n+  return vgetq_lane_s32 (__builtin_aarch64_reduc_smax_v4si (__a), __LANE0 (4));\n }\n \n __extension__ static __inline uint8_t __attribute__ ((__always_inline__))\n vmaxvq_u8 (uint8x16_t __a)\n {\n   return vgetq_lane_u8 ((uint8x16_t)\n-\t\t__builtin_aarch64_reduc_umax_v16qi ((int8x16_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_umax_v16qi ((int8x16_t) __a),\n+\t\t__LANE0 (16));\n }\n \n __extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n vmaxvq_u16 (uint16x8_t __a)\n {\n   return vgetq_lane_u16 ((uint16x8_t)\n-\t\t__builtin_aarch64_reduc_umax_v8hi ((int16x8_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_umax_v8hi ((int16x8_t) __a),\n+\t\t__LANE0 (8));\n }\n \n __extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n vmaxvq_u32 (uint32x4_t __a)\n {\n   return vgetq_lane_u32 ((uint32x4_t)\n-\t\t__builtin_aarch64_reduc_umax_v4si ((int32x4_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_umax_v4si ((int32x4_t) __a),\n+\t\t__LANE0 (4));\n }\n \n /* vmaxnmv  */\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vmaxnmv_f32 (float32x2_t __a)\n {\n-  return vget_lane_f32 (__builtin_aarch64_reduc_smax_v2sf (__a), 0);\n+  return vget_lane_f32 (__builtin_aarch64_reduc_smax_v2sf (__a),\n+\t\t\t__LANE0 (2));\n }\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vmaxnmvq_f32 (float32x4_t __a)\n {\n-  return vgetq_lane_f32 (__builtin_aarch64_reduc_smax_v4sf (__a), 0);\n+  return vgetq_lane_f32 (__builtin_aarch64_reduc_smax_v4sf (__a), __LANE0 (4));\n }\n \n __extension__ static __inline float64_t __attribute__ ((__always_inline__))\n vmaxnmvq_f64 (float64x2_t __a)\n {\n-  return vgetq_lane_f64 (__builtin_aarch64_reduc_smax_v2df (__a), 0);\n+  return vgetq_lane_f64 (__builtin_aarch64_reduc_smax_v2df (__a), __LANE0 (2));\n }\n \n /* vmin  */\n@@ -20501,117 +20525,127 @@ vminnmq_f64 (float64x2_t __a, float64x2_t __b)\n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vminv_f32 (float32x2_t __a)\n {\n-  return vget_lane_f32 (__builtin_aarch64_reduc_smin_nan_v2sf (__a), 0);\n+  return vget_lane_f32 (__builtin_aarch64_reduc_smin_nan_v2sf (__a),\n+\t\t\t__LANE0 (2));\n }\n \n __extension__ static __inline int8_t __attribute__ ((__always_inline__))\n vminv_s8 (int8x8_t __a)\n {\n-  return vget_lane_s8 (__builtin_aarch64_reduc_smin_v8qi (__a), 0);\n+  return vget_lane_s8 (__builtin_aarch64_reduc_smin_v8qi (__a),\n+\t\t       __LANE0 (8));\n }\n \n __extension__ static __inline int16_t __attribute__ ((__always_inline__))\n vminv_s16 (int16x4_t __a)\n {\n-  return vget_lane_s16 (__builtin_aarch64_reduc_smin_v4hi (__a), 0);\n+  return vget_lane_s16 (__builtin_aarch64_reduc_smin_v4hi (__a), __LANE0 (4));\n }\n \n __extension__ static __inline int32_t __attribute__ ((__always_inline__))\n vminv_s32 (int32x2_t __a)\n {\n-  return vget_lane_s32 (__builtin_aarch64_reduc_smin_v2si (__a), 0);\n+  return vget_lane_s32 (__builtin_aarch64_reduc_smin_v2si (__a), __LANE0 (2));\n }\n \n __extension__ static __inline uint8_t __attribute__ ((__always_inline__))\n vminv_u8 (uint8x8_t __a)\n {\n   return vget_lane_u8 ((uint8x8_t)\n-\t\t__builtin_aarch64_reduc_umin_v8qi ((int8x8_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_umin_v8qi ((int8x8_t) __a),\n+\t\t__LANE0 (8));\n }\n \n __extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n vminv_u16 (uint16x4_t __a)\n {\n   return vget_lane_u16 ((uint16x4_t)\n-\t\t__builtin_aarch64_reduc_umin_v4hi ((int16x4_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_umin_v4hi ((int16x4_t) __a),\n+\t\t__LANE0 (4));\n }\n \n __extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n vminv_u32 (uint32x2_t __a)\n {\n   return vget_lane_u32 ((uint32x2_t)\n-\t\t__builtin_aarch64_reduc_umin_v2si ((int32x2_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_umin_v2si ((int32x2_t) __a),\n+\t\t__LANE0 (2));\n }\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vminvq_f32 (float32x4_t __a)\n {\n-  return vgetq_lane_f32 (__builtin_aarch64_reduc_smin_nan_v4sf (__a), 0);\n+  return vgetq_lane_f32 (__builtin_aarch64_reduc_smin_nan_v4sf (__a),\n+\t\t\t __LANE0 (4));\n }\n \n __extension__ static __inline float64_t __attribute__ ((__always_inline__))\n vminvq_f64 (float64x2_t __a)\n {\n-  return vgetq_lane_f64 (__builtin_aarch64_reduc_smin_nan_v2df (__a), 0);\n+  return vgetq_lane_f64 (__builtin_aarch64_reduc_smin_nan_v2df (__a),\n+\t\t\t __LANE0 (2));\n }\n \n __extension__ static __inline int8_t __attribute__ ((__always_inline__))\n vminvq_s8 (int8x16_t __a)\n {\n-  return vgetq_lane_s8 (__builtin_aarch64_reduc_smin_v16qi (__a), 0);\n+  return vgetq_lane_s8 (__builtin_aarch64_reduc_smin_v16qi (__a), __LANE0 (16));\n }\n \n __extension__ static __inline int16_t __attribute__ ((__always_inline__))\n vminvq_s16 (int16x8_t __a)\n {\n-  return vgetq_lane_s16 (__builtin_aarch64_reduc_smin_v8hi (__a), 0);\n+  return vgetq_lane_s16 (__builtin_aarch64_reduc_smin_v8hi (__a), __LANE0 (8));\n }\n \n __extension__ static __inline int32_t __attribute__ ((__always_inline__))\n vminvq_s32 (int32x4_t __a)\n {\n-  return vgetq_lane_s32 (__builtin_aarch64_reduc_smin_v4si (__a), 0);\n+  return vgetq_lane_s32 (__builtin_aarch64_reduc_smin_v4si (__a), __LANE0 (4));\n }\n \n __extension__ static __inline uint8_t __attribute__ ((__always_inline__))\n vminvq_u8 (uint8x16_t __a)\n {\n   return vgetq_lane_u8 ((uint8x16_t)\n-\t\t__builtin_aarch64_reduc_umin_v16qi ((int8x16_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_umin_v16qi ((int8x16_t) __a),\n+\t\t__LANE0 (16));\n }\n \n __extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n vminvq_u16 (uint16x8_t __a)\n {\n   return vgetq_lane_u16 ((uint16x8_t)\n-\t\t__builtin_aarch64_reduc_umin_v8hi ((int16x8_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_umin_v8hi ((int16x8_t) __a),\n+\t\t__LANE0 (8));\n }\n \n __extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n vminvq_u32 (uint32x4_t __a)\n {\n   return vgetq_lane_u32 ((uint32x4_t)\n-\t\t__builtin_aarch64_reduc_umin_v4si ((int32x4_t) __a), 0);\n+\t\t__builtin_aarch64_reduc_umin_v4si ((int32x4_t) __a),\n+\t\t__LANE0 (4));\n }\n \n /* vminnmv  */\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vminnmv_f32 (float32x2_t __a)\n {\n-  return vget_lane_f32 (__builtin_aarch64_reduc_smin_v2sf (__a), 0);\n+  return vget_lane_f32 (__builtin_aarch64_reduc_smin_v2sf (__a), __LANE0 (2));\n }\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vminnmvq_f32 (float32x4_t __a)\n {\n-  return vgetq_lane_f32 (__builtin_aarch64_reduc_smin_v4sf (__a), 0);\n+  return vgetq_lane_f32 (__builtin_aarch64_reduc_smin_v4sf (__a), __LANE0 (4));\n }\n \n __extension__ static __inline float64_t __attribute__ ((__always_inline__))\n vminnmvq_f64 (float64x2_t __a)\n {\n-  return vgetq_lane_f64 (__builtin_aarch64_reduc_smin_v2df (__a), 0);\n+  return vgetq_lane_f64 (__builtin_aarch64_reduc_smin_v2df (__a), __LANE0 (2));\n }\n \n /* vmla */\n@@ -25444,6 +25478,8 @@ __INTERLEAVE_LIST (zip)\n \n /* End of optimal implementations in approved order.  */\n \n+#undef __LANE0\n+\n #undef __aarch64_vget_lane_any\n #undef __aarch64_vget_lane_f32\n #undef __aarch64_vget_lane_f64"}, {"sha": "fd7152c8ff41a61bddf9847bae05f77c00f8e37e", "filename": "gcc/config/aarch64/iterators.md", "status": "modified", "additions": 10, "deletions": 1, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/928353177b4843b05a7675d1b504086b3eb5686a/gcc%2Fconfig%2Faarch64%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/928353177b4843b05a7675d1b504086b3eb5686a/gcc%2Fconfig%2Faarch64%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fiterators.md?ref=928353177b4843b05a7675d1b504086b3eb5686a", "patch": "@@ -108,7 +108,10 @@\n (define_mode_iterator VALLDI [V8QI V16QI V4HI V8HI V2SI V4SI V2DI V2SF V4SF V2DF DI])\n \n ;; Vector modes for Integer reduction across lanes.\n-(define_mode_iterator VDQV [V8QI V16QI V4HI V8HI V4SI])\n+(define_mode_iterator VDQV [V8QI V16QI V4HI V8HI V4SI V2DI])\n+\n+;; Vector modes(except V2DI) for Integer reduction across lanes.\n+(define_mode_iterator VDQV_S [V8QI V16QI V4HI V8HI V4SI])\n \n ;; All double integer narrow-able modes.\n (define_mode_iterator VDN [V4HI V2SI DI])\n@@ -585,6 +588,12 @@\n \t\t\t       (V2DF  \"_q\")\n \t\t     (QI \"\") (HI \"\") (SI \"\") (DI \"\") (SF \"\") (DF \"\")])\n \n+(define_mode_attr vp [(V8QI \"v\") (V16QI \"v\")\n+\t\t      (V4HI \"v\") (V8HI  \"v\")\n+\t\t      (V2SI \"p\") (V4SI  \"v\")\n+\t\t      (V2DI  \"p\") (V2DF  \"p\")\n+\t\t      (V2SF \"p\") (V4SF  \"v\")])\n+\n ;; -------------------------------------------------------------------\n ;; Code Iterators\n ;; -------------------------------------------------------------------"}]}