{"sha": "f7064d11bb781d5cad08cdcb4942712390c477ca", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZjcwNjRkMTFiYjc4MWQ1Y2FkMDhjZGNiNDk0MjcxMjM5MGM0NzdjYQ==", "commit": {"author": {"name": "Dorit Naishlos", "email": "dorit@il.ibm.com", "date": "2005-02-17T08:47:28Z"}, "committer": {"name": "Dorit Nuzman", "email": "dorit@gcc.gnu.org", "date": "2005-02-17T08:47:28Z"}, "message": "Makefile.in (tree-vect-analyze.o, [...]): New.\n\n        * Makefile.in (tree-vect-analyze.o, tree-vect-transform.o): New.\n        (tree-vectorizer.o): Added missing dependencies.\n\n        * tree-vectorizer.h (vect_dump, vect_verbosity_level): Added extern\n        decleration.\n        (slpeel_tree_peel_loop_to_edge): Function externalized (had a static\n        declaration in tree-vectorizer.c, now has an extern declaration in\n        tree-vectorizer.h).\n        (slpeel_make_loop_iterate_ntimes, slpeel_can_duplicate_loop_p,\n        slpeel_verify_cfg_after_peeling, vect_strip_conversion,\n        get_vectype_for_scalar_type, vect_is_simple_use,\n        vect_is_simple_iv_evolution, vect_can_force_dr_alignment_p,\n        vect_supportable_dr_alignment, new_loop_vec_info, destroy_loop_vec_info,\n        new_stmt_vec_info, vect_analyze_loop, vectorizable_load,\n        vectorizable_store, vectorizable_operation, vectorizable_assignment,\n        vect_transform_loop, vect_print_dump_info, vect_set_verbosity_level,\n        find_loop_location): Likewise.\n\n        * tree-vectorizer.c (langhooks.h): #include removed.\n        (slpeel_tree_peel_loop_to_edge): Function externalized. Declaration\n        moved to tree-vectorized.h.\n        (slpeel_make_loop_iterate_ntimes, slpeel_can_duplicate_loop_p,\n        slpeel_verify_cfg_after_peeling, vect_strip_conversion,\n        get_vectype_for_scalar_type, vect_is_simple_use,\n        vect_is_simple_iv_evolution, vect_can_force_dr_alignment_p,\n        vect_supportable_dr_alignment, new_loop_vec_info,\n        destroy_loop_vec_info, new_stmt_vec_info, vect_print_dump_info,\n        vect_set_verbosity_level, find_loop_location): Likewise.\n\n        (vect_analyze_loop): Function externalized. Declaration moved to\n        tree-vectorized.h. Function definition moved to tree-vect-analyze.c.\n        (vect_analyze_loop_form): Moved to tree-vect-analyze.c.\n        (vect_mark_stmts_to_be_vectorized, vect_analyze_scalar_cycles,\n        vect_analyze_data_ref_accesses, vect_analyze_data_ref_dependences,\n        vect_analyze_data_refs_alignment, vect_compute_data_refs_alignment,\n        vect_enhance_data_refs_alignment, vect_analyze_operations,\n        exist_non_indexing_operands_for_use_p, vect_mark_relevant,\n        vect_stmt_relevant_p, vect_get_loop_niters,\n        vect_analyze_data_ref_dependence, vect_compute_data_ref_alignment,\n        vect_analyze_data_ref_access, vect_analyze_pointer_ref_access,\n        vect_can_advance_ivs_p, vect_get_ptr_offset, vect_analyze_offset_expr,\n        vect_base_addr_differ_p, vect_object_analysis, vect_address_analysis,\n        vect_get_memtag): Likewise.\n\n        (vectorizable_load): Function externalized. Declaration moved to\n        tree-vectorized.h. Function definition moved to tree-vect-transform.c.\n        (vectorizable_store, vectorizable_operation, vectorizable_assignment,\n        vect_transform_loop): Likewise.\n        (vect_transform_stmt): Moved to tree-vect-transform.c.\n        (vect_align_data_ref, vect_create_destination_var,\n        vect_create_data_ref_ptr, vect_create_index_for_vector_ref,\n        vect_create_addr_base_for_vector_ref, vect_get_new_vect_var,\n        vect_get_vec_def_for_operand, vect_init_vector,\n        vect_finish_stmt_generation, vect_generate_tmps_on_preheader,\n        vect_build_loop_niters, vect_update_ivs_after_vectorizer,\n        vect_gen_niters_for_prolog_loop, vect_update_inits_of_dr,\n        vect_update_inits_of_drs, vect_do_peeling_for_alignment,\n        vect_do_peeling_for_loop_bound): Likewise.\n\n        * tree-vect-analyze.c: New file.\n        * tree-vect-transform.c: New file.\n\nFrom-SVN: r95153", "tree": {"sha": "156dba144110c82b3dd2deaad5f86c03a6dda30f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/156dba144110c82b3dd2deaad5f86c03a6dda30f"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/f7064d11bb781d5cad08cdcb4942712390c477ca", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f7064d11bb781d5cad08cdcb4942712390c477ca", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f7064d11bb781d5cad08cdcb4942712390c477ca", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f7064d11bb781d5cad08cdcb4942712390c477ca/comments", "author": null, "committer": null, "parents": [{"sha": "96dd155e2c78f10846b8cda792450846978b8644", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/96dd155e2c78f10846b8cda792450846978b8644", "html_url": "https://github.com/Rust-GCC/gccrs/commit/96dd155e2c78f10846b8cda792450846978b8644"}], "stats": {"total": 8890, "additions": 4524, "deletions": 4366}, "files": [{"sha": "eb7522b1cdbe8deb7858f3bb29e2bfa4cdbbb73b", "filename": "gcc/ChangeLog", "status": "modified", "additions": 64, "deletions": 0, "changes": 64, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7064d11bb781d5cad08cdcb4942712390c477ca/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7064d11bb781d5cad08cdcb4942712390c477ca/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=f7064d11bb781d5cad08cdcb4942712390c477ca", "patch": "@@ -1,3 +1,67 @@\n+2005-02-17  Dorit Naishlos  <dorit@il.ibm.com>\n+\n+\t* Makefile.in (tree-vect-analyze.o, tree-vect-transform.o): New.\n+\t(tree-vectorizer.o): Added missing dependencies.\n+\n+\t* tree-vectorizer.h (vect_dump, vect_verbosity_level): Added extern\n+\tdecleration.\n+\t(slpeel_tree_peel_loop_to_edge): Function externalized (had a static\n+\tdeclaration in tree-vectorizer.c, now has an extern declaration in\n+\ttree-vectorizer.h).\n+\t(slpeel_make_loop_iterate_ntimes, slpeel_can_duplicate_loop_p,\n+\tslpeel_verify_cfg_after_peeling, vect_strip_conversion, \n+\tget_vectype_for_scalar_type, vect_is_simple_use, \n+\tvect_is_simple_iv_evolution, vect_can_force_dr_alignment_p,\n+\tvect_supportable_dr_alignment, new_loop_vec_info, destroy_loop_vec_info,\n+\tnew_stmt_vec_info, vect_analyze_loop, vectorizable_load, \n+\tvectorizable_store, vectorizable_operation, vectorizable_assignment,\n+\tvect_transform_loop, vect_print_dump_info, vect_set_verbosity_level,\n+\tfind_loop_location): Likewise.\n+\n+\t* tree-vectorizer.c (langhooks.h): #include removed.\n+\t(slpeel_tree_peel_loop_to_edge): Function externalized. Declaration\n+\tmoved to tree-vectorized.h.\n+\t(slpeel_make_loop_iterate_ntimes, slpeel_can_duplicate_loop_p,\n+\tslpeel_verify_cfg_after_peeling, vect_strip_conversion,\n+\tget_vectype_for_scalar_type, vect_is_simple_use,\n+\tvect_is_simple_iv_evolution, vect_can_force_dr_alignment_p,\n+\tvect_supportable_dr_alignment, new_loop_vec_info,\n+\tdestroy_loop_vec_info, new_stmt_vec_info, vect_print_dump_info,\n+\tvect_set_verbosity_level, find_loop_location): Likewise.\n+\n+\t(vect_analyze_loop): Function externalized. Declaration moved to \n+\ttree-vectorized.h. Function definition moved to tree-vect-analyze.c.\n+\t(vect_analyze_loop_form): Moved to tree-vect-analyze.c.\n+\t(vect_mark_stmts_to_be_vectorized, vect_analyze_scalar_cycles,\n+\tvect_analyze_data_ref_accesses, vect_analyze_data_ref_dependences,\n+\tvect_analyze_data_refs_alignment, vect_compute_data_refs_alignment,\n+\tvect_enhance_data_refs_alignment, vect_analyze_operations,\n+\texist_non_indexing_operands_for_use_p, vect_mark_relevant,\n+\tvect_stmt_relevant_p, vect_get_loop_niters,\n+\tvect_analyze_data_ref_dependence, vect_compute_data_ref_alignment,\n+\tvect_analyze_data_ref_access, vect_analyze_pointer_ref_access,\n+\tvect_can_advance_ivs_p, vect_get_ptr_offset, vect_analyze_offset_expr,\n+\tvect_base_addr_differ_p, vect_object_analysis, vect_address_analysis,\n+\tvect_get_memtag): Likewise.\n+\n+\t(vectorizable_load): Function externalized. Declaration moved to \n+\ttree-vectorized.h. Function definition moved to tree-vect-transform.c.\n+\t(vectorizable_store, vectorizable_operation, vectorizable_assignment,\n+\tvect_transform_loop): Likewise.\n+\t(vect_transform_stmt): Moved to tree-vect-transform.c.\n+\t(vect_align_data_ref, vect_create_destination_var, \n+\tvect_create_data_ref_ptr, vect_create_index_for_vector_ref,\n+\tvect_create_addr_base_for_vector_ref, vect_get_new_vect_var,\n+\tvect_get_vec_def_for_operand, vect_init_vector,\n+\tvect_finish_stmt_generation, vect_generate_tmps_on_preheader,\n+\tvect_build_loop_niters, vect_update_ivs_after_vectorizer,\n+\tvect_gen_niters_for_prolog_loop, vect_update_inits_of_dr,\n+\tvect_update_inits_of_drs, vect_do_peeling_for_alignment,\n+\tvect_do_peeling_for_loop_bound): Likewise.\n+\t\n+\t* tree-vect-analyze.c: New file.\n+\t* tree-vect-transform.c: New file.\n+\n 2005-02-17  Jason Merrill  <jason@redhat.com>\n \n \tPR mudflap/19319, c++/19317"}, {"sha": "93c33992c23d200e2cde6e9c16a099d9f1aec915", "filename": "gcc/Makefile.in", "status": "modified", "additions": 11, "deletions": 2, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7064d11bb781d5cad08cdcb4942712390c477ca/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7064d11bb781d5cad08cdcb4942712390c477ca/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=f7064d11bb781d5cad08cdcb4942712390c477ca", "patch": "@@ -902,7 +902,8 @@ OBJS-common = \\\n  tree-ssa-dom.o domwalk.o tree-tailcall.o gimple-low.o tree-iterator.o\t   \\\n  tree-phinodes.o tree-ssanames.o tree-sra.o tree-complex.o tree-ssa-loop.o \\\n  tree-ssa-loop-niter.o tree-ssa-loop-manip.o tree-ssa-threadupdate.o\t   \\\n- tree-vectorizer.o tree-ssa-loop-ivcanon.o tree-ssa-propagate.o\t \t   \\\n+ tree-vectorizer.o tree-vect-analyze.o tree-vect-transform.o\t\t   \\\n+ tree-ssa-loop-ivcanon.o tree-ssa-propagate.o\t \t   \t\t   \\\n  tree-ssa-loop-ivopts.o tree-if-conv.o tree-ssa-loop-unswitch.o\t\t   \\\n  alias.o bb-reorder.o bitmap.o builtins.o caller-save.o calls.o\t  \t   \\\n  cfg.o cfganal.o cfgbuild.o cfgcleanup.o cfglayout.o cfgloop.o\t\t   \\\n@@ -1765,10 +1766,18 @@ tree-data-ref.o: tree-data-ref.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n    errors.h $(GGC_H) $(TREE_H) $(RTL_H) $(BASIC_BLOCK_H) diagnostic.h \\\n    $(TREE_FLOW_H) $(TREE_DUMP_H) $(TIMEVAR_H) cfgloop.h \\\n    tree-data-ref.h $(SCEV_H) tree-pass.h $(LAMBDA_H)\n+tree-vect-analyze.o: tree-vect-analyze.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   errors.h $(GGC_H) $(OPTABS_H) $(TREE_H) $(RTL_H) $(BASIC_BLOCK_H) diagnostic.h \\\n+   $(TREE_FLOW_H) $(TREE_DUMP_H) $(TIMEVAR_H) cfgloop.h \\\n+   tree-vectorizer.h tree-data-ref.h $(SCEV_H) $(EXPR_H)\n+tree-vect-transform.o: tree-vect-transform.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   errors.h $(GGC_H) $(OPTABS_H) $(TREE_H) $(RTL_H) $(BASIC_BLOCK_H) diagnostic.h \\\n+   $(TREE_FLOW_H) $(TREE_DUMP_H) $(TIMEVAR_H) cfgloop.h target.h tree-pass.h $(EXPR_H) \\\n+   tree-vectorizer.h tree-data-ref.h $(SCEV_H) langhooks.h toplev.h\n tree-vectorizer.o: tree-vectorizer.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n    errors.h $(GGC_H) $(OPTABS_H) $(TREE_H) $(RTL_H) $(BASIC_BLOCK_H) diagnostic.h \\\n    $(TREE_FLOW_H) $(TREE_DUMP_H) $(TIMEVAR_H) cfgloop.h tree-pass.h $(EXPR_H) \\\n-   tree-vectorizer.h tree-data-ref.h $(SCEV_H)\n+   tree-vectorizer.h tree-data-ref.h $(SCEV_H) input.h target.h cfglayout.h\n tree-loop-linear.o: tree-loop-linear.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n    errors.h $(GGC_H) $(OPTABS_H) $(TREE_H) $(RTL_H) $(BASIC_BLOCK_H) diagnostic.h \\\n    $(TREE_FLOW_H) $(TREE_DUMP_H) $(TIMEVAR_H) cfgloop.h tree-pass.h \\"}, {"sha": "9df5375a676c620756cb8f0a5a08d7bb773a0dd0", "filename": "gcc/tree-vect-analyze.c", "status": "added", "additions": 2524, "deletions": 0, "changes": 2524, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7064d11bb781d5cad08cdcb4942712390c477ca/gcc%2Ftree-vect-analyze.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7064d11bb781d5cad08cdcb4942712390c477ca/gcc%2Ftree-vect-analyze.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-analyze.c?ref=f7064d11bb781d5cad08cdcb4942712390c477ca", "patch": "@@ -0,0 +1,2524 @@\n+/* Anlaysis Utilities for Loop Vectorization.\n+   Copyright (C) 2003,2004,2005 Free Software Foundation, Inc.\n+   Contributed by Dorit Naishlos <dorit@il.ibm.com>\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 2, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING.  If not, write to the Free\n+Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n+02111-1307, USA.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"errors.h\"\n+#include \"ggc.h\"\n+#include \"tree.h\"\n+#include \"basic-block.h\"\n+#include \"diagnostic.h\"\n+#include \"tree-flow.h\"\n+#include \"tree-dump.h\"\n+#include \"timevar.h\"\n+#include \"cfgloop.h\"\n+#include \"expr.h\"\n+#include \"optabs.h\"\n+#include \"tree-chrec.h\"\n+#include \"tree-data-ref.h\"\n+#include \"tree-scalar-evolution.h\"\n+#include \"tree-vectorizer.h\"\n+\n+/* Main analysis functions.  */\n+static loop_vec_info vect_analyze_loop_form (struct loop *);\n+static bool vect_analyze_data_refs (loop_vec_info);\n+static bool vect_mark_stmts_to_be_vectorized (loop_vec_info);\n+static bool vect_analyze_scalar_cycles (loop_vec_info);\n+static bool vect_analyze_data_ref_accesses (loop_vec_info);\n+static bool vect_analyze_data_ref_dependences (loop_vec_info);\n+static bool vect_analyze_data_refs_alignment (loop_vec_info);\n+static bool vect_compute_data_refs_alignment (loop_vec_info);\n+static void vect_enhance_data_refs_alignment (loop_vec_info);\n+static bool vect_analyze_operations (loop_vec_info);\n+\n+/* Utility functions for the analyses.  */\n+static bool exist_non_indexing_operands_for_use_p (tree, tree);\n+static void vect_mark_relevant (varray_type *, tree);\n+static bool vect_stmt_relevant_p (tree, loop_vec_info);\n+static tree vect_get_loop_niters (struct loop *, tree *);\n+static bool vect_analyze_data_ref_dependence\n+  (struct data_reference *, struct data_reference *, loop_vec_info);\n+static bool vect_compute_data_ref_alignment (struct data_reference *);\n+static bool vect_analyze_data_ref_access (struct data_reference *);\n+static struct data_reference * vect_analyze_pointer_ref_access \n+  (tree, tree, bool, tree, tree *, tree *);\n+static bool vect_can_advance_ivs_p (loop_vec_info);\n+static tree vect_get_ptr_offset (tree, tree, tree *);\n+static bool vect_analyze_offset_expr (tree, struct loop *, tree, tree *, \n+\t\t\t\t      tree *, tree *);\n+static bool vect_base_addr_differ_p (struct data_reference *,\n+\t\t\t\t     struct data_reference *drb, bool *);\n+static tree vect_object_analysis (tree, tree, bool, tree, \n+\t\t\t\t  struct data_reference **, tree *, tree *, \n+\t\t\t\t  tree *, bool *);\n+static tree vect_address_analysis (tree, tree, bool, tree, \n+\t\t\t\t   struct data_reference *, tree *, tree *, \n+\t\t\t\t   tree *, bool *);\n+static tree vect_get_memtag (tree, struct data_reference *);\n+\n+\n+/* Function vect_get_ptr_offset\n+\n+   Compute the OFFSET modulo vector-type alignment of pointer REF in bits.  */\n+\n+static tree \n+vect_get_ptr_offset (tree ref ATTRIBUTE_UNUSED, \n+\t\t     tree vectype ATTRIBUTE_UNUSED, \n+\t\t     tree *offset ATTRIBUTE_UNUSED)\n+{\n+  /* TODO: Use alignment information.  */\n+  return NULL_TREE; \n+}\n+\n+\n+/* Function vect_analyze_offset_expr\n+\n+   Given an offset expression EXPR received from get_inner_reference, analyze\n+   it and create an expression for INITIAL_OFFSET by substituting the variables \n+   of EXPR with initial_condition of the corresponding access_fn in the loop. \n+   E.g., \n+      for i\n+         for (j = 3; j < N; j++)\n+            a[j].b[i][j] = 0;\n+\t \n+   For a[j].b[i][j], EXPR will be 'i * C_i + j * C_j + C'. 'i' cannot be \n+   substituted, since its access_fn in the inner loop is i. 'j' will be \n+   substituted with 3. An INITIAL_OFFSET will be 'i * C_i + C`', where\n+   C` =  3 * C_j + C.\n+\n+   Compute MISALIGN (the misalignment of the data reference initial access from\n+   its base) if possible. Misalignment can be calculated only if all the\n+   variables can be substituted with constants, or if a variable is multiplied\n+   by a multiple of VECTYPE_ALIGNMENT. In the above example, since 'i' cannot\n+   be substituted, MISALIGN will be NULL_TREE in case that C_i is not a multiple\n+   of VECTYPE_ALIGNMENT, and C` otherwise. (We perform MISALIGN modulo \n+   VECTYPE_ALIGNMENT computation in the caller of this function).\n+\n+   STEP is an evolution of the data reference in this loop in bytes.\n+   In the above example, STEP is C_j.\n+\n+   Return FALSE, if the analysis fails, e.g., there is no access_fn for a \n+   variable. In this case, all the outputs (INITIAL_OFFSET, MISALIGN and STEP) \n+   are NULL_TREEs. Otherwise, return TRUE.\n+\n+*/\n+\n+static bool\n+vect_analyze_offset_expr (tree expr, \n+\t\t\t  struct loop *loop, \n+\t\t\t  tree vectype_alignment,\n+\t\t\t  tree *initial_offset,\n+\t\t\t  tree *misalign,\n+\t\t\t  tree *step)\n+{\n+  tree oprnd0;\n+  tree oprnd1;\n+  tree left_offset = ssize_int (0);\n+  tree right_offset = ssize_int (0);\n+  tree left_misalign = ssize_int (0);\n+  tree right_misalign = ssize_int (0);\n+  tree left_step = ssize_int (0);\n+  tree right_step = ssize_int (0);\n+  enum tree_code code;\n+  tree init, evolution;\n+\n+  *step = NULL_TREE;\n+  *misalign = NULL_TREE;\n+  *initial_offset = NULL_TREE;\n+\n+  /* Strip conversions that don't narrow the mode.  */\n+  expr = vect_strip_conversion (expr);\n+  if (!expr)\n+    return false;\n+\n+  /* Stop conditions:\n+     1. Constant.  */\n+  if (TREE_CODE (expr) == INTEGER_CST)\n+    {\n+      *initial_offset = fold_convert (ssizetype, expr);\n+      *misalign = fold_convert (ssizetype, expr);      \n+      *step = ssize_int (0);\n+      return true;\n+    }\n+\n+  /* 2. Variable. Try to substitute with initial_condition of the corresponding\n+     access_fn in the current loop.  */\n+  if (SSA_VAR_P (expr))\n+    {\n+      tree access_fn = analyze_scalar_evolution (loop, expr);\n+\n+      if (access_fn == chrec_dont_know)\n+\t/* No access_fn.  */\n+\treturn false;\n+\n+      init = initial_condition_in_loop_num (access_fn, loop->num);\n+      if (init == expr && !expr_invariant_in_loop_p (loop, init))\n+\t/* Not enough information: may be not loop invariant.  \n+\t   E.g., for a[b[i]], we get a[D], where D=b[i]. EXPR is D, its \n+\t   initial_condition is D, but it depends on i - loop's induction\n+\t   variable.  */\t  \n+\treturn false;\n+\n+      evolution = evolution_part_in_loop_num (access_fn, loop->num);\n+      if (evolution && TREE_CODE (evolution) != INTEGER_CST)\n+\t/* Evolution is not constant.  */\n+\treturn false;\n+\n+      if (TREE_CODE (init) == INTEGER_CST)\n+\t*misalign = fold_convert (ssizetype, init);\n+      else\n+\t/* Not constant, misalignment cannot be calculated.  */\n+\t*misalign = NULL_TREE;\n+\n+      *initial_offset = fold_convert (ssizetype, init); \n+\n+      *step = evolution ? fold_convert (ssizetype, evolution) : ssize_int (0);\n+      return true;      \n+    }\n+\n+  /* Recursive computation.  */\n+  if (!BINARY_CLASS_P (expr))\n+    {\n+      /* We expect to get binary expressions (PLUS/MINUS and MULT).  */\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+        {\n+\t  fprintf (vect_dump, \"Not binary expression \");\n+          print_generic_expr (vect_dump, expr, TDF_SLIM);\n+\t}\n+      return false;\n+    }\n+  oprnd0 = TREE_OPERAND (expr, 0);\n+  oprnd1 = TREE_OPERAND (expr, 1);\n+\n+  if (!vect_analyze_offset_expr (oprnd0, loop, vectype_alignment, &left_offset, \n+\t\t\t\t&left_misalign, &left_step)\n+      || !vect_analyze_offset_expr (oprnd1, loop, vectype_alignment, \n+\t\t\t\t   &right_offset, &right_misalign, &right_step))\n+    return false;\n+\n+  /* The type of the operation: plus, minus or mult.  */\n+  code = TREE_CODE (expr);\n+  switch (code)\n+    {\n+    case MULT_EXPR:\n+      if (TREE_CODE (right_offset) != INTEGER_CST)\n+\t/* RIGHT_OFFSET can be not constant. For example, for arrays of variable \n+\t   sized types. \n+\t   FORNOW: We don't support such cases.  */\n+\treturn false;\n+\n+      /* Strip conversions that don't narrow the mode.  */\n+      left_offset = vect_strip_conversion (left_offset);      \n+      if (!left_offset)\n+\treturn false;      \n+      /* Misalignment computation.  */\n+      if (SSA_VAR_P (left_offset))\n+\t{\n+\t  /* If the left side contains variables that can't be substituted with \n+\t     constants, we check if the right side is a multiple of ALIGNMENT.\n+\t   */\n+\t  if (integer_zerop (size_binop (TRUNC_MOD_EXPR, right_offset, \n+\t\t\t          fold_convert (ssizetype, vectype_alignment))))\n+\t    *misalign = ssize_int (0);\n+\t  else\n+\t    /* If the remainder is not zero or the right side isn't constant,\n+\t       we can't compute  misalignment.  */\n+\t    *misalign = NULL_TREE;\n+\t}\n+      else \n+\t{\n+\t  /* The left operand was successfully substituted with constant.  */\t  \n+\t  if (left_misalign)\n+\t    /* In case of EXPR '(i * C1 + j) * C2', LEFT_MISALIGN is \n+\t       NULL_TREE.  */\n+\t    *misalign  = size_binop (code, left_misalign, right_misalign);\n+\t  else\n+\t    *misalign = NULL_TREE; \n+\t}\n+\n+      /* Step calculation.  */\n+      /* Multiply the step by the right operand.  */\n+      *step  = size_binop (MULT_EXPR, left_step, right_offset);\n+      break;\n+   \n+    case PLUS_EXPR:\n+    case MINUS_EXPR:\n+      /* Combine the recursive calculations for step and misalignment.  */\n+      *step = size_binop (code, left_step, right_step);\n+   \n+      if (left_misalign && right_misalign)\n+\t*misalign  = size_binop (code, left_misalign, right_misalign);\n+      else\n+\t*misalign = NULL_TREE;\n+    \n+      break;\n+\n+    default:\n+      gcc_unreachable ();\n+    }\n+\n+  /* Compute offset.  */\n+  *initial_offset = fold_convert (ssizetype, \n+\t\t\t\t  fold (build2 (code, TREE_TYPE (left_offset), \n+\t\t\t\t\t\tleft_offset, \n+\t\t\t\t\t\tright_offset)));\n+  return true;\n+}\n+\n+\n+/* Function vect_analyze_operations.\n+\n+   Scan the loop stmts and make sure they are all vectorizable.  */\n+\n+static bool\n+vect_analyze_operations (loop_vec_info loop_vinfo)\n+{\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  basic_block *bbs = LOOP_VINFO_BBS (loop_vinfo);\n+  int nbbs = loop->num_nodes;\n+  block_stmt_iterator si;\n+  unsigned int vectorization_factor = 0;\n+  int i;\n+  bool ok;\n+  tree scalar_type;\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"=== vect_analyze_operations ===\");\n+\n+  for (i = 0; i < nbbs; i++)\n+    {\n+      basic_block bb = bbs[i];\n+\n+      for (si = bsi_start (bb); !bsi_end_p (si); bsi_next (&si))\n+\t{\n+\t  tree stmt = bsi_stmt (si);\n+\t  unsigned int nunits;\n+\t  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+\t  tree vectype;\n+\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    {\n+\t      fprintf (vect_dump, \"==> examining statement: \");\n+\t      print_generic_expr (vect_dump, stmt, TDF_SLIM);\n+\t    }\n+\n+\t  gcc_assert (stmt_info);\n+\n+\t  /* skip stmts which do not need to be vectorized.\n+\t     this is expected to include:\n+\t     - the COND_EXPR which is the loop exit condition\n+\t     - any LABEL_EXPRs in the loop\n+\t     - computations that are used only for array indexing or loop\n+\t     control  */\n+\n+\t  if (!STMT_VINFO_RELEVANT_P (stmt_info))\n+\t    {\n+\t      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t        fprintf (vect_dump, \"irrelevant.\");\n+\t      continue;\n+\t    }\n+\n+\t  if (VECTOR_MODE_P (TYPE_MODE (TREE_TYPE (stmt))))\n+\t    {\n+\t      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+                                         LOOP_LOC (loop_vinfo)))\n+\t\t{\n+                  fprintf (vect_dump, \"not vectorized: vector stmt in loop:\");\n+\t\t  print_generic_expr (vect_dump, stmt, TDF_SLIM);\n+\t\t}\n+\t      return false;\n+\t    }\n+\n+          if (STMT_VINFO_DATA_REF (stmt_info))\n+            scalar_type = TREE_TYPE (DR_REF (STMT_VINFO_DATA_REF (stmt_info)));    \n+          else if (TREE_CODE (stmt) == MODIFY_EXPR)\n+\t    scalar_type = TREE_TYPE (TREE_OPERAND (stmt, 0));\n+\t  else\n+\t    scalar_type = TREE_TYPE (stmt);\n+\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    {\n+\t      fprintf (vect_dump, \"get vectype for scalar type:  \");\n+\t      print_generic_expr (vect_dump, scalar_type, TDF_SLIM);\n+\t    }\n+\n+\t  vectype = get_vectype_for_scalar_type (scalar_type);\n+\t  if (!vectype)\n+\t    {\n+\t      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+                                         LOOP_LOC (loop_vinfo)))\n+\t\t{\n+                  fprintf (vect_dump,\n+                           \"not vectorized: unsupported data-type \");\n+\t\t  print_generic_expr (vect_dump, scalar_type, TDF_SLIM);\n+\t\t}\n+\t      return false;\n+\t    }\n+\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    {\n+\t      fprintf (vect_dump, \"vectype: \");\n+\t      print_generic_expr (vect_dump, vectype, TDF_SLIM);\n+\t    }\n+\t  STMT_VINFO_VECTYPE (stmt_info) = vectype;\n+\n+\t  ok = (vectorizable_operation (stmt, NULL, NULL)\n+\t\t|| vectorizable_assignment (stmt, NULL, NULL)\n+\t\t|| vectorizable_load (stmt, NULL, NULL)\n+\t\t|| vectorizable_store (stmt, NULL, NULL));\n+\n+\t  if (!ok)\n+\t    {\n+\t      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+                                         LOOP_LOC (loop_vinfo)))\n+\t\t{\n+                  fprintf (vect_dump, \"not vectorized: stmt not supported: \");\n+\t\t  print_generic_expr (vect_dump, stmt, TDF_SLIM);\n+\t\t}\n+\t      return false;\n+\t    }\n+\n+\t  nunits = GET_MODE_NUNITS (TYPE_MODE (vectype));\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    fprintf (vect_dump, \"nunits = %d\", nunits);\n+\n+\t  if (vectorization_factor)\n+\t    {\n+\t      /* FORNOW: don't allow mixed units.\n+\t         This restriction will be relaxed in the future.  */\n+\t      if (nunits != vectorization_factor)\n+\t\t{\n+\t          if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+                                             LOOP_LOC (loop_vinfo)))\n+\t\t    fprintf (vect_dump, \"not vectorized: mixed data-types\");\n+\t\t  return false;\n+\t\t}\n+\t    }\n+\t  else\n+\t    vectorization_factor = nunits;\n+\n+#ifdef ENABLE_CHECKING\n+\t  gcc_assert (GET_MODE_SIZE (TYPE_MODE (scalar_type))\n+\t\t\t* vectorization_factor == UNITS_PER_SIMD_WORD);\n+#endif\n+\t}\n+    }\n+\n+  /* TODO: Analyze cost. Decide if worth while to vectorize.  */\n+\n+  if (vectorization_factor <= 1)\n+    {\n+      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+                                 LOOP_LOC (loop_vinfo)))\n+        fprintf (vect_dump, \"not vectorized: unsupported data-type\");\n+      return false;\n+    }\n+  LOOP_VINFO_VECT_FACTOR (loop_vinfo) = vectorization_factor;\n+\n+  if (LOOP_VINFO_NITERS_KNOWN_P (loop_vinfo)\n+      && vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump,\n+        \"vectorization_factor = %d, niters = \" HOST_WIDE_INT_PRINT_DEC,\n+        vectorization_factor, LOOP_VINFO_INT_NITERS (loop_vinfo));\n+\n+  if (LOOP_VINFO_NITERS_KNOWN_P (loop_vinfo)\n+      && LOOP_VINFO_INT_NITERS (loop_vinfo) < vectorization_factor)\n+    {\n+      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+                                 LOOP_LOC (loop_vinfo)))\n+\tfprintf (vect_dump, \"not vectorized: iteration count too small.\");\n+      return false;\n+    }\n+\n+  if (!LOOP_VINFO_NITERS_KNOWN_P (loop_vinfo)\n+      || LOOP_VINFO_INT_NITERS (loop_vinfo) % vectorization_factor != 0)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, LOOP_LOC (loop_vinfo)))\n+        fprintf (vect_dump, \"epilog loop required.\");\n+      if (!vect_can_advance_ivs_p (loop_vinfo))\n+        {\n+          if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+                                     LOOP_LOC (loop_vinfo)))\n+            fprintf (vect_dump,\n+                     \"not vectorized: can't create epilog loop 1.\");\n+          return false;\n+        }\n+      if (!slpeel_can_duplicate_loop_p (loop, loop->exit_edges[0]))\n+        {\n+          if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+                                     LOOP_LOC (loop_vinfo)))\n+            fprintf (vect_dump,\n+                     \"not vectorized: can't create epilog loop 2.\");\n+          return false;\n+        }\n+    }\n+\n+  return true;\n+}\n+\n+\n+/* Function exist_non_indexing_operands_for_use_p \n+\n+   USE is one of the uses attached to STMT. Check if USE is \n+   used in STMT for anything other than indexing an array.  */\n+\n+static bool\n+exist_non_indexing_operands_for_use_p (tree use, tree stmt)\n+{\n+  tree operand;\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+ \n+  /* USE corresponds to some operand in STMT. If there is no data\n+     reference in STMT, then any operand that corresponds to USE\n+     is not indexing an array.  */\n+  if (!STMT_VINFO_DATA_REF (stmt_info))\n+    return true;\n+ \n+  /* STMT has a data_ref. FORNOW this means that its of one of\n+     the following forms:\n+     -1- ARRAY_REF = var\n+     -2- var = ARRAY_REF\n+     (This should have been verified in analyze_data_refs).\n+\n+     'var' in the second case corresponds to a def, not a use,\n+     so USE cannot correspond to any operands that are not used \n+     for array indexing.\n+\n+     Therefore, all we need to check is if STMT falls into the\n+     first case, and whether var corresponds to USE.  */\n+ \n+  if (TREE_CODE (TREE_OPERAND (stmt, 0)) == SSA_NAME)\n+    return false;\n+\n+  operand = TREE_OPERAND (stmt, 1);\n+\n+  if (TREE_CODE (operand) != SSA_NAME)\n+    return false;\n+\n+  if (operand == use)\n+    return true;\n+\n+  return false;\n+}\n+\n+\n+/* Function vect_analyze_scalar_cycles.\n+\n+   Examine the cross iteration def-use cycles of scalar variables, by\n+   analyzing the loop (scalar) PHIs; verify that the cross iteration def-use\n+   cycles that they represent do not impede vectorization.\n+\n+   FORNOW: Reduction as in the following loop, is not supported yet:\n+              loop1:\n+              for (i=0; i<N; i++)\n+                 sum += a[i];\n+\t   The cross-iteration cycle corresponding to variable 'sum' will be\n+\t   considered too complicated and will impede vectorization.\n+\n+   FORNOW: Induction as in the following loop, is not supported yet:\n+              loop2:\n+              for (i=0; i<N; i++)\n+                 a[i] = i;\n+\n+           However, the following loop *is* vectorizable:\n+              loop3:\n+              for (i=0; i<N; i++)\n+                 a[i] = b[i];\n+\n+           In both loops there exists a def-use cycle for the variable i:\n+              loop: i_2 = PHI (i_0, i_1)\n+                    a[i_2] = ...;\n+                    i_1 = i_2 + 1;\n+                    GOTO loop;\n+\n+           The evolution of the above cycle is considered simple enough,\n+\t   however, we also check that the cycle does not need to be\n+\t   vectorized, i.e - we check that the variable that this cycle\n+\t   defines is only used for array indexing or in stmts that do not\n+\t   need to be vectorized. This is not the case in loop2, but it\n+\t   *is* the case in loop3.  */\n+\n+static bool\n+vect_analyze_scalar_cycles (loop_vec_info loop_vinfo)\n+{\n+  tree phi;\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  basic_block bb = loop->header;\n+  tree dummy;\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"=== vect_analyze_scalar_cycles ===\");\n+\n+  for (phi = phi_nodes (bb); phi; phi = PHI_CHAIN (phi))\n+    {\n+      tree access_fn = NULL;\n+\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t{\n+          fprintf (vect_dump, \"Analyze phi: \");\n+          print_generic_expr (vect_dump, phi, TDF_SLIM);\n+\t}\n+\n+      /* Skip virtual phi's. The data dependences that are associated with\n+         virtual defs/uses (i.e., memory accesses) are analyzed elsewhere.  */\n+\n+      if (!is_gimple_reg (SSA_NAME_VAR (PHI_RESULT (phi))))\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    fprintf (vect_dump, \"virtual phi. skip.\");\n+\t  continue;\n+\t}\n+\n+      /* Analyze the evolution function.  */\n+\n+      /* FORNOW: The only scalar cross-iteration cycles that we allow are\n+         those of loop induction variables; This property is verified here.\n+\n+         Furthermore, if that induction variable is used in an operation\n+         that needs to be vectorized (i.e, is not solely used to index\n+         arrays and check the exit condition) - we do not support its\n+         vectorization yet. This property is verified in vect_is_simple_use,\n+         during vect_analyze_operations.  */\n+\n+      access_fn = /* instantiate_parameters\n+\t\t     (loop,*/\n+\t analyze_scalar_evolution (loop, PHI_RESULT (phi));\n+\n+      if (!access_fn)\n+\t{\n+\t  if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\t    LOOP_LOC (loop_vinfo)))\n+\t    fprintf (vect_dump, \"not vectorized: unsupported scalar cycle.\");\n+\t  return false;\n+\t}\n+\n+      if (vect_print_dump_info (REPORT_DETAILS,\n+\t\t\t\tLOOP_LOC (loop_vinfo)))\n+        {\n+           fprintf (vect_dump, \"Access function of PHI: \");\n+           print_generic_expr (vect_dump, access_fn, TDF_SLIM);\n+        }\n+\n+      if (!vect_is_simple_iv_evolution (loop->num, access_fn, &dummy, &dummy))\n+\t{\n+\t  if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\t    LOOP_LOC (loop_vinfo)))\n+\t    fprintf (vect_dump, \"not vectorized: unsupported scalar cycle.\");\n+\t  return false;\n+\t}\n+    }\n+\n+  return true;\n+}\n+\n+\n+/* Function vect_base_addr_differ_p.\n+\n+   This is the simplest data dependence test: determines whether the\n+   data references A and B access the same array/region.  Returns\n+   false when the property is not computable at compile time.\n+   Otherwise return true, and DIFFER_P will record the result. This\n+   utility will not be necessary when alias_sets_conflict_p will be\n+   less conservative.  */\n+\n+static bool\n+vect_base_addr_differ_p (struct data_reference *dra,\n+\t\t\t struct data_reference *drb,\n+\t\t\t bool *differ_p)\n+{\n+  tree stmt_a = DR_STMT (dra);\n+  stmt_vec_info stmt_info_a = vinfo_for_stmt (stmt_a);   \n+  tree stmt_b = DR_STMT (drb);\n+  stmt_vec_info stmt_info_b = vinfo_for_stmt (stmt_b);   \n+  tree addr_a = STMT_VINFO_VECT_DR_BASE_ADDRESS (stmt_info_a);\n+  tree addr_b = STMT_VINFO_VECT_DR_BASE_ADDRESS (stmt_info_b);\n+  tree type_a = TREE_TYPE (addr_a);\n+  tree type_b = TREE_TYPE (addr_b);\n+  HOST_WIDE_INT alias_set_a, alias_set_b;\n+\n+  gcc_assert (POINTER_TYPE_P (type_a) &&  POINTER_TYPE_P (type_b));\n+  \n+  /* Both references are ADDR_EXPR, i.e., we have the objects.  */\n+  if (TREE_CODE (addr_a) == ADDR_EXPR && TREE_CODE (addr_b) == ADDR_EXPR)\n+    return array_base_name_differ_p (dra, drb, differ_p);  \n+\n+  alias_set_a = (TREE_CODE (addr_a) == ADDR_EXPR) ? \n+    get_alias_set (TREE_OPERAND (addr_a, 0)) : get_alias_set (addr_a);\n+  alias_set_b = (TREE_CODE (addr_b) == ADDR_EXPR) ? \n+    get_alias_set (TREE_OPERAND (addr_b, 0)) : get_alias_set (addr_b);\n+\n+  if (!alias_sets_conflict_p (alias_set_a, alias_set_b))\n+    {\n+      *differ_p = true;\n+      return true;\n+    }\n+  \n+  /* An instruction writing through a restricted pointer is \"independent\" of any \n+     instruction reading or writing through a different pointer, in the same \n+     block/scope.  */\n+  else if ((TYPE_RESTRICT (type_a) && !DR_IS_READ (dra))\n+      || (TYPE_RESTRICT (type_b) && !DR_IS_READ (drb)))\n+    {\n+      *differ_p = true;\n+      return true;\n+    }\n+  return false;\n+}\n+\n+\n+/* Function vect_analyze_data_ref_dependence.\n+\n+   Return TRUE if there (might) exist a dependence between a memory-reference\n+   DRA and a memory-reference DRB.  */\n+\n+static bool\n+vect_analyze_data_ref_dependence (struct data_reference *dra,\n+\t\t\t\t  struct data_reference *drb, \n+\t\t\t\t  loop_vec_info loop_vinfo)\n+{\n+  bool differ_p; \n+  struct data_dependence_relation *ddr;\n+  \n+  if (!vect_base_addr_differ_p (dra, drb, &differ_p))\n+    {\n+      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\tLOOP_LOC (loop_vinfo)))\n+        {\n+          fprintf (vect_dump,\n+                \"not vectorized: can't determine dependence between: \");\n+          print_generic_expr (vect_dump, DR_REF (dra), TDF_SLIM);\n+          fprintf (vect_dump, \" and \");\n+          print_generic_expr (vect_dump, DR_REF (drb), TDF_SLIM);\n+        }\n+      return true;\n+    }\n+\n+  if (differ_p)\n+    return false;\n+\n+  ddr = initialize_data_dependence_relation (dra, drb);\n+  compute_affine_dependence (ddr);\n+\n+  if (DDR_ARE_DEPENDENT (ddr) == chrec_known)\n+    return false;\n+  \n+  if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t    LOOP_LOC (loop_vinfo)))\n+    {\n+      fprintf (vect_dump,\n+\t\"not vectorized: possible dependence between data-refs \");\n+      print_generic_expr (vect_dump, DR_REF (dra), TDF_SLIM);\n+      fprintf (vect_dump, \" and \");\n+      print_generic_expr (vect_dump, DR_REF (drb), TDF_SLIM);\n+    }\n+\n+  return true;\n+}\n+\n+\n+/* Function vect_analyze_data_ref_dependences.\n+\n+   Examine all the data references in the loop, and make sure there do not\n+   exist any data dependences between them.\n+\n+   TODO: dependences which distance is greater than the vectorization factor\n+         can be ignored.  */\n+\n+static bool\n+vect_analyze_data_ref_dependences (loop_vec_info loop_vinfo)\n+{\n+  unsigned int i, j;\n+  varray_type loop_write_refs = LOOP_VINFO_DATAREF_WRITES (loop_vinfo);\n+  varray_type loop_read_refs = LOOP_VINFO_DATAREF_READS (loop_vinfo);\n+\n+  /* Examine store-store (output) dependences.  */\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"=== vect_analyze_dependences ===\");\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"compare all store-store pairs.\");\n+\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (loop_write_refs); i++)\n+    {\n+      for (j = i + 1; j < VARRAY_ACTIVE_SIZE (loop_write_refs); j++)\n+\t{\n+\t  struct data_reference *dra =\n+\t    VARRAY_GENERIC_PTR (loop_write_refs, i);\n+\t  struct data_reference *drb =\n+\t    VARRAY_GENERIC_PTR (loop_write_refs, j);\n+\t  if (vect_analyze_data_ref_dependence (dra, drb, loop_vinfo))\n+\t    return false;\n+\t}\n+    }\n+\n+  /* Examine load-store (true/anti) dependences.  */\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"compare all load-store pairs.\");\n+\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (loop_read_refs); i++)\n+    {\n+      for (j = 0; j < VARRAY_ACTIVE_SIZE (loop_write_refs); j++)\n+\t{\n+\t  struct data_reference *dra = VARRAY_GENERIC_PTR (loop_read_refs, i);\n+\t  struct data_reference *drb =\n+\t    VARRAY_GENERIC_PTR (loop_write_refs, j);\n+\t  if (vect_analyze_data_ref_dependence (dra, drb, loop_vinfo))\n+\t    return false;\n+\t}\n+    }\n+\n+  return true;\n+}\n+\n+\n+/* Function vect_compute_data_ref_alignment\n+\n+   Compute the misalignment of the data reference DR.\n+\n+   Output:\n+   1. If during the misalignment computation it is found that the data reference\n+      cannot be vectorized then false is returned.\n+   2. DR_MISALIGNMENT (DR) is defined.\n+\n+   FOR NOW: No analysis is actually performed. Misalignment is calculated\n+   only for trivial cases. TODO.  */\n+\n+static bool\n+vect_compute_data_ref_alignment (struct data_reference *dr)\n+{\n+  tree stmt = DR_STMT (dr);\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);  \n+  tree ref = DR_REF (dr);\n+  tree vectype;\n+  tree base, alignment;\n+  bool base_aligned_p;\n+  tree misalign;\n+   \n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"vect_compute_data_ref_alignment:\");\n+\n+  /* Initialize misalignment to unknown.  */\n+  DR_MISALIGNMENT (dr) = -1;\n+\n+  misalign = STMT_VINFO_VECT_MISALIGNMENT (stmt_info);\n+  base_aligned_p = STMT_VINFO_VECT_BASE_ALIGNED_P (stmt_info);\n+  base = build_fold_indirect_ref (STMT_VINFO_VECT_DR_BASE_ADDRESS (stmt_info));\n+  vectype = STMT_VINFO_VECTYPE (stmt_info);\n+\n+  if (!misalign)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC)) \n+\t{\n+\t  fprintf (vect_dump, \"Unknown alignment for access: \");\n+\t  print_generic_expr (vect_dump, base, TDF_SLIM);\n+\t}\n+      return true;\n+    }\n+\n+  if (!base_aligned_p) \n+    {\n+      if (!vect_can_force_dr_alignment_p (base, TYPE_ALIGN (vectype)))\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    {\n+\t      fprintf (vect_dump, \"can't force alignment of ref: \");\n+\t      print_generic_expr (vect_dump, ref, TDF_SLIM);\n+\t    }\n+\t  return true;\n+\t}\n+      \n+      /* Force the alignment of the decl.\n+\t NOTE: This is the only change to the code we make during\n+\t the analysis phase, before deciding to vectorize the loop.  */\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\tfprintf (vect_dump, \"force alignment\");\n+      DECL_ALIGN (base) = TYPE_ALIGN (vectype);\n+      DECL_USER_ALIGN (base) = 1;\n+    }\n+\n+  /* At this point we assume that the base is aligned.  */\n+  gcc_assert (base_aligned_p \n+\t      || (TREE_CODE (base) == VAR_DECL \n+\t\t  && DECL_ALIGN (base) >= TYPE_ALIGN (vectype)));\n+\n+  /* Alignment required, in bytes:  */\n+  alignment = ssize_int (TYPE_ALIGN (vectype)/BITS_PER_UNIT);\n+\n+  /* Modulo alignment.  */\n+  misalign = size_binop (TRUNC_MOD_EXPR, misalign, alignment);\n+  if (tree_int_cst_sgn (misalign) < 0)\n+    {\n+      /* Negative misalignment value.  */\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\tfprintf (vect_dump, \"unexpected misalign value\");\n+      return false;\n+    }\n+\n+  DR_MISALIGNMENT (dr) = tree_low_cst (misalign, 1);\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"misalign = %d bytes\", DR_MISALIGNMENT (dr));\n+\n+  return true;\n+}\n+\n+\n+/* Function vect_compute_data_refs_alignment\n+\n+   Compute the misalignment of data references in the loop.\n+   This pass may take place at function granularity instead of at loop\n+   granularity.\n+\n+   FOR NOW: No analysis is actually performed. Misalignment is calculated\n+   only for trivial cases. TODO.  */\n+\n+static bool\n+vect_compute_data_refs_alignment (loop_vec_info loop_vinfo)\n+{\n+  varray_type loop_write_datarefs = LOOP_VINFO_DATAREF_WRITES (loop_vinfo);\n+  varray_type loop_read_datarefs = LOOP_VINFO_DATAREF_READS (loop_vinfo);\n+  unsigned int i;\n+\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (loop_write_datarefs); i++)\n+    {\n+      struct data_reference *dr = VARRAY_GENERIC_PTR (loop_write_datarefs, i);\n+      if (!vect_compute_data_ref_alignment (dr))\n+\treturn false;\n+    }\n+\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (loop_read_datarefs); i++)\n+    {\n+      struct data_reference *dr = VARRAY_GENERIC_PTR (loop_read_datarefs, i);\n+      if (!vect_compute_data_ref_alignment (dr))\n+\treturn false;\n+    }\n+\n+  return true;\n+}\n+\n+\n+/* Function vect_enhance_data_refs_alignment\n+\n+   This pass will use loop versioning and loop peeling in order to enhance\n+   the alignment of data references in the loop.\n+\n+   FOR NOW: we assume that whatever versioning/peeling takes place, only the\n+   original loop is to be vectorized; Any other loops that are created by\n+   the transformations performed in this pass - are not supposed to be\n+   vectorized. This restriction will be relaxed.  */\n+\n+static void\n+vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n+{\n+  varray_type loop_read_datarefs = LOOP_VINFO_DATAREF_READS (loop_vinfo);\n+  varray_type loop_write_datarefs = LOOP_VINFO_DATAREF_WRITES (loop_vinfo);\n+  unsigned int i;\n+\n+  /*\n+     This pass will require a cost model to guide it whether to apply peeling \n+     or versioning or a combination of the two. For example, the scheme that\n+     intel uses when given a loop with several memory accesses, is as follows:\n+     choose one memory access ('p') which alignment you want to force by doing \n+     peeling. Then, either (1) generate a loop in which 'p' is aligned and all \n+     other accesses are not necessarily aligned, or (2) use loop versioning to \n+     generate one loop in which all accesses are aligned, and another loop in \n+     which only 'p' is necessarily aligned. \n+\n+     (\"Automatic Intra-Register Vectorization for the Intel Architecture\",\n+      Aart J.C. Bik, Milind Girkar, Paul M. Grey and Ximmin Tian, International\n+      Journal of Parallel Programming, Vol. 30, No. 2, April 2002.)\t\n+\n+     Devising a cost model is the most critical aspect of this work. It will \n+     guide us on which access to peel for, whether to use loop versioning, how \n+     many versions to create, etc. The cost model will probably consist of \n+     generic considerations as well as target specific considerations (on \n+     powerpc for example, misaligned stores are more painful than misaligned \n+     loads). \n+\n+     Here is the general steps involved in alignment enhancements:\n+    \n+     -- original loop, before alignment analysis:\n+\tfor (i=0; i<N; i++){\n+\t  x = q[i];\t\t\t# DR_MISALIGNMENT(q) = unknown\n+\t  p[i] = y;\t\t\t# DR_MISALIGNMENT(p) = unknown\n+\t}\n+\n+     -- After vect_compute_data_refs_alignment:\n+\tfor (i=0; i<N; i++){\n+\t  x = q[i];\t\t\t# DR_MISALIGNMENT(q) = 3\n+\t  p[i] = y;\t\t\t# DR_MISALIGNMENT(p) = unknown\n+\t}\n+\n+     -- Possibility 1: we do loop versioning:\n+     if (p is aligned) {\n+\tfor (i=0; i<N; i++){\t# loop 1A\n+\t  x = q[i];\t\t\t# DR_MISALIGNMENT(q) = 3\n+\t  p[i] = y;\t\t\t# DR_MISALIGNMENT(p) = 0\n+\t}\n+     } \n+     else {\n+\tfor (i=0; i<N; i++){\t# loop 1B\n+\t  x = q[i];\t\t\t# DR_MISALIGNMENT(q) = 3\n+\t  p[i] = y;\t\t\t# DR_MISALIGNMENT(p) = unaligned\n+\t}\n+     }\n+   \n+     -- Possibility 2: we do loop peeling:\n+     for (i = 0; i < 3; i++){\t# (scalar loop, not to be vectorized).\n+\tx = q[i];\n+\tp[i] = y;\n+     }\n+     for (i = 3; i < N; i++){\t# loop 2A\n+\tx = q[i];\t\t\t# DR_MISALIGNMENT(q) = 0\n+\tp[i] = y;\t\t\t# DR_MISALIGNMENT(p) = unknown\n+     }\n+\n+     -- Possibility 3: combination of loop peeling and versioning:\n+     for (i = 0; i < 3; i++){\t# (scalar loop, not to be vectorized).\n+\tx = q[i];\n+\tp[i] = y;\n+     }\n+     if (p is aligned) {\n+\tfor (i = 3; i<N; i++){  # loop 3A\n+\t  x = q[i];\t\t\t# DR_MISALIGNMENT(q) = 0\n+\t  p[i] = y;\t\t\t# DR_MISALIGNMENT(p) = 0\n+\t}\n+     } \n+     else {\n+\tfor (i = 3; i<N; i++){\t# loop 3B\n+\t  x = q[i];\t\t\t# DR_MISALIGNMENT(q) = 0\n+\t  p[i] = y;\t\t\t# DR_MISALIGNMENT(p) = unaligned\n+\t}\n+     }\n+\n+     These loops are later passed to loop_transform to be vectorized. The \n+     vectorizer will use the alignment information to guide the transformation \n+     (whether to generate regular loads/stores, or with special handling for \n+     misalignment). \n+   */\n+\n+  /* (1) Peeling to force alignment.  */\n+\n+  /* (1.1) Decide whether to perform peeling, and how many iterations to peel:\n+     Considerations:\n+     + How many accesses will become aligned due to the peeling\n+     - How many accesses will become unaligned due to the peeling,\n+       and the cost of misaligned accesses.\n+     - The cost of peeling (the extra runtime checks, the increase \n+       in code size).\n+\n+     The scheme we use FORNOW: peel to force the alignment of the first\n+     misaligned store in the loop.\n+     Rationale: misaligned stores are not yet supported.\n+\n+     TODO: Use a better cost model.  */\n+\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (loop_write_datarefs); i++)\n+    {\n+      struct data_reference *dr = VARRAY_GENERIC_PTR (loop_write_datarefs, i);\n+      if (!aligned_access_p (dr))\n+        {\n+          LOOP_VINFO_UNALIGNED_DR (loop_vinfo) = dr;\n+          LOOP_DO_PEELING_FOR_ALIGNMENT (loop_vinfo) = true;\n+\t  break;\n+        }\n+    }\n+\n+  if (!LOOP_VINFO_UNALIGNED_DR (loop_vinfo))\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, LOOP_LOC (loop_vinfo)))\n+\tfprintf (vect_dump, \"Peeling for alignment will not be applied.\");\n+      return;\n+    }\n+  else\n+    if (vect_print_dump_info (REPORT_DETAILS, LOOP_LOC (loop_vinfo)))\n+      fprintf (vect_dump, \"Peeling for alignment will be applied.\");\n+\n+\n+  /* (1.2) Update the alignment info according to the peeling factor.\n+\t   If the misalignment of the DR we peel for is M, then the\n+\t   peeling factor is VF - M, and the misalignment of each access DR_i\n+\t   in the loop is DR_MISALIGNMENT (DR_i) + VF - M.\n+\t   If the misalignment of the DR we peel for is unknown, then the \n+\t   misalignment of each access DR_i in the loop is also unknown.\n+\n+\t   FORNOW: set the misalignment of the accesses to unknown even\n+\t           if the peeling factor is known at compile time.\n+\n+\t   TODO: - if the peeling factor is known at compile time, use that\n+\t\t   when updating the misalignment info of the loop DRs.\n+\t\t - consider accesses that are known to have the same \n+\t\t   alignment, even if that alignment is unknown.  */\n+   \n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (loop_write_datarefs); i++)\n+    {\n+      struct data_reference *dr = VARRAY_GENERIC_PTR (loop_write_datarefs, i);\n+      if (dr == LOOP_VINFO_UNALIGNED_DR (loop_vinfo))\n+\t{\n+\t  DR_MISALIGNMENT (dr) = 0;\n+\t  if (vect_print_dump_info (REPORT_ALIGNMENT, LOOP_LOC (loop_vinfo)))\n+\t    fprintf (vect_dump, \"Alignment of access forced using peeling.\");\n+\t}\n+      else\n+\tDR_MISALIGNMENT (dr) = -1;\n+    }\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (loop_read_datarefs); i++)\n+    {\n+      struct data_reference *dr = VARRAY_GENERIC_PTR (loop_read_datarefs, i);\n+      if (dr == LOOP_VINFO_UNALIGNED_DR (loop_vinfo))\n+\t{\n+\t  DR_MISALIGNMENT (dr) = 0;\n+\t  if (vect_print_dump_info (REPORT_ALIGNMENT, LOOP_LOC (loop_vinfo)))\n+\t    fprintf (vect_dump, \"Alignment of access forced using peeling.\");\n+\t}\n+      else\n+\tDR_MISALIGNMENT (dr) = -1;\n+    }\n+}\n+\n+\n+/* Function vect_analyze_data_refs_alignment\n+\n+   Analyze the alignment of the data-references in the loop.\n+   FOR NOW: Until support for misliagned accesses is in place, only if all\n+   accesses are aligned can the loop be vectorized. This restriction will be \n+   relaxed.  */ \n+\n+static bool\n+vect_analyze_data_refs_alignment (loop_vec_info loop_vinfo)\n+{\n+  varray_type loop_read_datarefs = LOOP_VINFO_DATAREF_READS (loop_vinfo);\n+  varray_type loop_write_datarefs = LOOP_VINFO_DATAREF_WRITES (loop_vinfo);\n+  enum dr_alignment_support supportable_dr_alignment;\n+  unsigned int i;\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"=== vect_analyze_data_refs_alignment ===\");\n+\n+\n+  /* This pass may take place at function granularity instead of at loop\n+     granularity.  */\n+\n+  if (!vect_compute_data_refs_alignment (loop_vinfo))\n+    {\n+      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\tLOOP_LOC (loop_vinfo)))\n+\tfprintf (vect_dump, \n+\t\t \"not vectorized: can't calculate alignment for data ref.\");\n+      return false;\n+    }\n+\n+\n+  /* This pass will decide on using loop versioning and/or loop peeling in \n+     order to enhance the alignment of data references in the loop.  */\n+\n+  vect_enhance_data_refs_alignment (loop_vinfo);\n+\n+\n+  /* Finally, check that all the data references in the loop can be\n+     handled with respect to their alignment.  */\n+\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (loop_read_datarefs); i++)\n+    {\n+      struct data_reference *dr = VARRAY_GENERIC_PTR (loop_read_datarefs, i);\n+      supportable_dr_alignment = vect_supportable_dr_alignment (dr);\n+      if (!supportable_dr_alignment)\n+\t{\n+\t  if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\t    LOOP_LOC (loop_vinfo)))\n+\t    fprintf (vect_dump, \"not vectorized: unsupported unaligned load.\");\n+\t  return false;\n+\t}\n+      if (supportable_dr_alignment != dr_aligned \n+\t  && (vect_print_dump_info (REPORT_ALIGNMENT, LOOP_LOC (loop_vinfo))))\n+\tfprintf (vect_dump, \"Vectorizing an unaligned access.\");\n+    }\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (loop_write_datarefs); i++)\n+    {\n+      struct data_reference *dr = VARRAY_GENERIC_PTR (loop_write_datarefs, i);\n+      supportable_dr_alignment = vect_supportable_dr_alignment (dr);\n+      if (!supportable_dr_alignment)\n+\t{\n+\t  if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\t    LOOP_LOC (loop_vinfo)))\n+\t    fprintf (vect_dump, \"not vectorized: unsupported unaligned store.\");\n+\t  return false;\n+\t}\n+      if (supportable_dr_alignment != dr_aligned \n+\t  && (vect_print_dump_info (REPORT_ALIGNMENT, LOOP_LOC (loop_vinfo))))\n+\tfprintf (vect_dump, \"Vectorizing an unaligned access.\");\n+    }\n+\n+  return true;\n+}\n+\n+\n+/* Function vect_analyze_data_ref_access.\n+\n+   Analyze the access pattern of the data-reference DR. For now, a data access\n+   has to consecutive to be considered vectorizable.  */\n+\n+static bool\n+vect_analyze_data_ref_access (struct data_reference *dr)\n+{\n+  tree stmt = DR_STMT (dr);\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt); \n+  tree step = STMT_VINFO_VECT_STEP (stmt_info);\n+  tree scalar_type = TREE_TYPE (DR_REF (dr));\n+\n+  if (!step || tree_int_cst_compare (step, TYPE_SIZE_UNIT (scalar_type)))\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\tfprintf (vect_dump, \"not consecutive access\");\n+      return false;\n+    }\n+  return true;\n+}\n+\n+\n+/* Function vect_analyze_data_ref_accesses.\n+\n+   Analyze the access pattern of all the data references in the loop.\n+\n+   FORNOW: the only access pattern that is considered vectorizable is a\n+\t   simple step 1 (consecutive) access.\n+\n+   FORNOW: handle only arrays and pointer accesses.  */\n+\n+static bool\n+vect_analyze_data_ref_accesses (loop_vec_info loop_vinfo)\n+{\n+  unsigned int i;\n+  varray_type loop_write_datarefs = LOOP_VINFO_DATAREF_WRITES (loop_vinfo);\n+  varray_type loop_read_datarefs = LOOP_VINFO_DATAREF_READS (loop_vinfo);\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"=== vect_analyze_data_ref_accesses ===\");\n+\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (loop_write_datarefs); i++)\n+    {\n+      struct data_reference *dr = VARRAY_GENERIC_PTR (loop_write_datarefs, i);\n+      bool ok = vect_analyze_data_ref_access (dr);\n+      if (!ok)\n+\t{\n+\t  if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+                                      LOOP_LOC (loop_vinfo)))\n+\t    fprintf (vect_dump, \"not vectorized: complicated access pattern.\");\n+\t  return false;\n+\t}\n+    }\n+\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (loop_read_datarefs); i++)\n+    {\n+      struct data_reference *dr = VARRAY_GENERIC_PTR (loop_read_datarefs, i);\n+      bool ok = vect_analyze_data_ref_access (dr);\n+      if (!ok)\n+\t{\n+\t  if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\t    LOOP_LOC (loop_vinfo)))\n+\t    fprintf (vect_dump, \"not vectorized: complicated access pattern.\");\n+\t  return false;\n+\t}\n+    }\n+\n+  return true;\n+}\n+\n+\n+/* Function vect_analyze_pointer_ref_access.\n+\n+   Input:\n+   STMT - a stmt that contains a data-ref.\n+   MEMREF - a data-ref in STMT, which is an INDIRECT_REF.\n+   ACCESS_FN - the access function of MEMREF.\n+\n+   Output:\n+   If the data-ref access is vectorizable, return a data_reference structure\n+   that represents it (DR). Otherwise - return NULL.  \n+   STEP - the stride of MEMREF in the loop.\n+   INIT - the initial condition of MEMREF in the loop.\n+*/\n+\n+static struct data_reference *\n+vect_analyze_pointer_ref_access (tree memref, tree stmt, bool is_read, \n+\t\t\t\t tree access_fn, tree *ptr_init, tree *ptr_step)\n+{\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  tree step, init;\t\n+  tree reftype, innertype;\n+  tree indx_access_fn; \n+  int loopnum = loop->num;\n+  struct data_reference *dr;\n+\n+  if (!vect_is_simple_iv_evolution (loopnum, access_fn, &init, &step))\n+    {\n+      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS, \n+\t\t\t\tLOOP_LOC (loop_vinfo))) \n+\tfprintf (vect_dump, \"not vectorized: pointer access is not simple.\");\t\n+      return NULL;\n+    }\n+\n+  STRIP_NOPS (init);\n+\n+  if (!expr_invariant_in_loop_p (loop, init))\n+    {\n+      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\tLOOP_LOC (loop_vinfo))) \n+\tfprintf (vect_dump, \n+\t\t \"not vectorized: initial condition is not loop invariant.\");\t\n+      return NULL;\n+    }\n+\n+  if (TREE_CODE (step) != INTEGER_CST)\n+    {\n+      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\tLOOP_LOC (loop_vinfo))) \n+\tfprintf (vect_dump, \n+\t\t\"not vectorized: non constant step for pointer access.\");\t\n+      return NULL;\n+    }\n+\n+  reftype = TREE_TYPE (TREE_OPERAND (memref, 0));\n+  if (TREE_CODE (reftype) != POINTER_TYPE) \n+    {\n+      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\tLOOP_LOC (loop_vinfo)))\n+\tfprintf (vect_dump, \"not vectorized: unexpected pointer access form.\");\t\n+      return NULL;\n+    }\n+\n+  reftype = TREE_TYPE (init);\n+  if (TREE_CODE (reftype) != POINTER_TYPE) \n+    {\n+      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\tLOOP_LOC (loop_vinfo))) \n+\tfprintf (vect_dump, \"not vectorized: unexpected pointer access form.\");\n+      return NULL;\n+    }\n+\n+  *ptr_step = fold_convert (ssizetype, step);\n+  innertype = TREE_TYPE (reftype);\n+  /* Check that STEP is a multiple of type size.  */\n+  if (!integer_zerop (size_binop (TRUNC_MOD_EXPR, *ptr_step, \n+ \t\t        fold_convert (ssizetype, TYPE_SIZE_UNIT (innertype)))))\n+    {\n+      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\tLOOP_LOC (loop_vinfo))) \n+\tfprintf (vect_dump, \"not vectorized: non consecutive access.\");\t\n+      return NULL;\n+    }\n+   \n+  indx_access_fn = \n+\tbuild_polynomial_chrec (loopnum, integer_zero_node, integer_one_node);\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    {\n+      fprintf (vect_dump, \"Access function of ptr indx: \");\n+      print_generic_expr (vect_dump, indx_access_fn, TDF_SLIM);\n+    }\n+  dr = init_data_ref (stmt, memref, NULL_TREE, indx_access_fn, is_read);\n+  *ptr_init = init;\n+  return dr;\n+}\n+\n+\n+/* Function vect_get_memtag.  \n+\n+   The function returns the relevant variable for memory tag (for aliasing \n+   purposes).  */\n+\n+static tree\n+vect_get_memtag (tree memref, struct data_reference *dr)\n+{\n+  tree symbl, tag;\n+\n+  switch (TREE_CODE (memref))\n+    {\n+    case SSA_NAME:\n+      symbl = SSA_NAME_VAR (memref);\n+      tag = get_var_ann (symbl)->type_mem_tag;\n+      if (!tag)\n+\t{\n+\t  tree ptr = TREE_OPERAND (DR_REF (dr), 0);\n+\t  if (TREE_CODE (ptr) == SSA_NAME)\n+\t    tag = get_var_ann (SSA_NAME_VAR (ptr))->type_mem_tag;\n+\t}\n+      return tag;\n+\n+    case ADDR_EXPR:\n+      return TREE_OPERAND (memref, 0);\n+\n+    default:\n+      return NULL_TREE;\n+    }  \n+}\n+\n+\n+/* Function vect_address_analysis\n+\n+   Return the BASE of the address expression EXPR.\n+   Also compute the INITIAL_OFFSET from BASE, MISALIGN and STEP.\n+\n+   Input:\n+   EXPR - the address expression that is being analyzed\n+   STMT - the statement that contains EXPR or its original memory reference\n+   IS_READ - TRUE if STMT reads from EXPR, FALSE if writes to EXPR\n+   VECTYPE - the type that defines the alignment (i.e, we compute\n+             alignment relative to TYPE_ALIGN(VECTYPE))\n+   DR - data_reference struct for the original memory reference\n+\n+   Output:\n+   BASE (returned value) - the base of the data reference EXPR.\n+   INITIAL_OFFSET - initial offset of EXPR from BASE (an expression)\n+   MISALIGN - offset of EXPR from BASE in bytes (a constant) or NULL_TREE if the\n+              computation is impossible\n+   STEP - evolution of EXPR in the loop\n+   BASE_ALIGNED - indicates if BASE is aligned\n+ \n+   If something unexpected is encountered (an unsupported form of data-ref),\n+   then NULL_TREE is returned.  \n+ */\n+\n+static tree\n+vect_address_analysis (tree expr, tree stmt, bool is_read, tree vectype, \n+\t\t       struct data_reference *dr, tree *offset, tree *misalign,\n+\t\t       tree *step, bool *base_aligned)\n+{\n+  tree oprnd0, oprnd1, base_address, offset_expr, base_addr0, base_addr1;\n+  tree address_offset = ssize_int (0), address_misalign = ssize_int (0);\n+\n+  switch (TREE_CODE (expr))\n+    {\n+    case PLUS_EXPR:\n+    case MINUS_EXPR:\n+      /* EXPR is of form {base +/- offset} (or {offset +/- base}).  */\n+      oprnd0 = TREE_OPERAND (expr, 0);\n+      oprnd1 = TREE_OPERAND (expr, 1);\n+\n+      STRIP_NOPS (oprnd0);\n+      STRIP_NOPS (oprnd1);\n+      \n+      /* Recursively try to find the base of the address contained in EXPR.\n+\t For offset, the returned base will be NULL.  */\n+      base_addr0 = vect_address_analysis (oprnd0, stmt, is_read, vectype, dr, \n+\t\t\t\t     &address_offset, &address_misalign, step, \n+\t\t\t\t     base_aligned);\n+\n+      base_addr1 = vect_address_analysis (oprnd1, stmt, is_read, vectype, dr, \n+\t\t\t\t     &address_offset, &address_misalign, step, \n+\t\t\t\t     base_aligned);\n+\n+      /* We support cases where only one of the operands contains an \n+\t address.  */\n+      if ((base_addr0 && base_addr1) || (!base_addr0 && !base_addr1))\n+\treturn NULL_TREE;\n+\n+      /* To revert STRIP_NOPS.  */\n+      oprnd0 = TREE_OPERAND (expr, 0);\n+      oprnd1 = TREE_OPERAND (expr, 1);\n+      \n+      offset_expr = base_addr0 ? \n+\tfold_convert (ssizetype, oprnd1) : fold_convert (ssizetype, oprnd0);\n+\n+      /* EXPR is of form {base +/- offset} (or {offset +/- base}). If offset is \n+\t a number, we can add it to the misalignment value calculated for base,\n+\t otherwise, misalignment is NULL.  */\n+      if (TREE_CODE (offset_expr) == INTEGER_CST && address_misalign)\n+\t*misalign = size_binop (TREE_CODE (expr), address_misalign, \n+\t\t\t\toffset_expr);\n+      else\n+\t*misalign = NULL_TREE;\n+\n+      /* Combine offset (from EXPR {base + offset}) with the offset calculated\n+\t for base.  */\n+      *offset = size_binop (TREE_CODE (expr), address_offset, offset_expr);\n+      return base_addr0 ? base_addr0 : base_addr1;\n+\n+    case ADDR_EXPR:\n+      base_address = vect_object_analysis (TREE_OPERAND (expr, 0), stmt, is_read, \n+\t\t\t\t   vectype, &dr, offset, misalign, step, \n+\t\t\t\t   base_aligned);\n+      return base_address;\n+\n+    case SSA_NAME:\n+      if (TREE_CODE (TREE_TYPE (expr)) != POINTER_TYPE)\n+\treturn NULL_TREE;\n+      \n+      if (TYPE_ALIGN (TREE_TYPE (TREE_TYPE (expr))) < TYPE_ALIGN (vectype)) \n+\t{\n+\t  if (vect_get_ptr_offset (expr, vectype, misalign))\n+\t    *base_aligned = true;\t  \n+\t  else\n+\t    *base_aligned = false;\n+\t}\n+      else\n+\t{\t  \n+\t  *base_aligned = true;\n+\t  *misalign = ssize_int (0);\n+\t}\n+      *offset = ssize_int (0);\n+      *step = ssize_int (0);\n+      return expr;\n+      \n+    default:\n+      return NULL_TREE;\n+    }\n+}\n+\n+\n+/* Function vect_object_analysis\n+\n+   Return the BASE of the data reference MEMREF.\n+   Also compute the INITIAL_OFFSET from BASE, MISALIGN and STEP.\n+   E.g., for EXPR a.b[i] + 4B, BASE is a, and OFFSET is the overall offset  \n+   'a.b[i] + 4B' from a (can be an expression), MISALIGN is an OFFSET \n+   instantiated with initial_conditions of access_functions of variables, \n+   modulo alignment, and STEP is the evolution of the DR_REF in this loop.\n+\n+   Function get_inner_reference is used for the above in case of ARRAY_REF and\n+   COMPONENT_REF.\n+\n+   The structure of the function is as follows:\n+   Part 1:\n+   Case 1. For handled_component_p refs \n+          1.1 call get_inner_reference\n+            1.1.1 analyze offset expr received from get_inner_reference\n+\t  1.2. build data-reference structure for MEMREF\n+        (fall through with BASE)\n+   Case 2. For declarations \n+          2.1 check alignment\n+          2.2 update DR_BASE_NAME if necessary for alias\n+   Case 3. For INDIRECT_REFs \n+          3.1 get the access function\n+\t  3.2 analyze evolution of MEMREF\n+\t  3.3 set data-reference structure for MEMREF\n+          3.4 call vect_address_analysis to analyze INIT of the access function\n+\n+   Part 2:\n+   Combine the results of object and address analysis to calculate \n+   INITIAL_OFFSET, STEP and misalignment info.   \n+\n+   Input:\n+   MEMREF - the memory reference that is being analyzed\n+   STMT - the statement that contains MEMREF\n+   IS_READ - TRUE if STMT reads from MEMREF, FALSE if writes to MEMREF\n+   VECTYPE - the type that defines the alignment (i.e, we compute\n+             alignment relative to TYPE_ALIGN(VECTYPE))\n+   \n+   Output:\n+   BASE_ADDRESS (returned value) - the base address of the data reference MEMREF\n+                                   E.g, if MEMREF is a.b[k].c[i][j] the returned\n+\t\t\t           base is &a.\n+   DR - data_reference struct for MEMREF\n+   INITIAL_OFFSET - initial offset of MEMREF from BASE (an expression)\n+   MISALIGN - offset of MEMREF from BASE in bytes (a constant) or NULL_TREE if \n+              the computation is impossible\n+   STEP - evolution of the DR_REF in the loop\n+   BASE_ALIGNED - indicates if BASE is aligned\n+ \n+   If something unexpected is encountered (an unsupported form of data-ref),\n+   then NULL_TREE is returned.  */\n+\n+static tree\n+vect_object_analysis (tree memref, tree stmt, bool is_read,\n+\t\t      tree vectype, struct data_reference **dr,\n+\t\t      tree *offset, tree *misalign, tree *step,\n+\t\t      bool *base_aligned)\n+{\n+  tree base = NULL_TREE, base_address = NULL_TREE;\n+  tree object_offset = ssize_int (0), object_misalign = ssize_int (0);\n+  tree object_step = ssize_int (0), address_step = ssize_int (0);\n+  bool object_base_aligned = true, address_base_aligned = true;\n+  tree address_offset = ssize_int (0), address_misalign = ssize_int (0);\n+  HOST_WIDE_INT pbitsize, pbitpos;\n+  tree poffset, bit_pos_in_bytes;\n+  enum machine_mode pmode;\n+  int punsignedp, pvolatilep;\n+  tree ptr_step = ssize_int (0), ptr_init = NULL_TREE;\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  struct data_reference *ptr_dr = NULL;\n+  tree access_fn, evolution_part, address_to_analyze;\n+   \n+  /* Part 1: */\n+  /* Case 1. handled_component_p refs.  */\n+  if (handled_component_p (memref))\n+    {\n+      /* 1.1 call get_inner_reference.  */\n+      /* Find the base and the offset from it.  */\n+      base = get_inner_reference (memref, &pbitsize, &pbitpos, &poffset,\n+\t\t\t\t  &pmode, &punsignedp, &pvolatilep, false);\n+      if (!base)\n+\treturn NULL_TREE;\n+\n+      /* 1.1.1 analyze offset expr received from get_inner_reference.  */\n+      if (poffset \n+\t  && !vect_analyze_offset_expr (poffset, loop, TYPE_SIZE_UNIT (vectype), \n+\t\t\t\t&object_offset, &object_misalign, &object_step))\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    {\n+\t      fprintf (vect_dump, \"failed to compute offset or step for \");\n+\t      print_generic_expr (vect_dump, memref, TDF_SLIM);\n+\t    }\n+\t  return NULL_TREE;\n+\t}\n+\n+      /* Add bit position to OFFSET and MISALIGN.  */\n+\n+      bit_pos_in_bytes = ssize_int (pbitpos/BITS_PER_UNIT);\n+      /* Check that there is no remainder in bits.  */\n+      if (pbitpos%BITS_PER_UNIT)\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    fprintf (vect_dump, \"bit offset alignment.\");\n+\t  return NULL_TREE;\n+\t}\n+      object_offset = size_binop (PLUS_EXPR, bit_pos_in_bytes, object_offset);     \n+      if (object_misalign) \n+\tobject_misalign = size_binop (PLUS_EXPR, object_misalign, \n+\t\t\t\t      bit_pos_in_bytes); \n+\n+      /* Create data-reference for MEMREF. TODO: handle COMPONENT_REFs.  */\n+      if (!(*dr))\n+\t{ \n+\t  if (TREE_CODE (memref) == ARRAY_REF)\n+\t    *dr = analyze_array (stmt, memref, is_read);\n+\t  else\n+\t    /* FORNOW.  */\n+\t    return NULL_TREE;\n+\t}\n+      memref = base; /* To continue analysis of BASE.  */\n+      /* fall through  */\n+    }\n+  \n+  /*  Part 1: Case 2. Declarations.  */ \n+  if (DECL_P (memref))\n+    {\n+      /* We expect to get a decl only if we already have a DR.  */\n+      if (!(*dr))\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    {\n+\t      fprintf (vect_dump, \"unhandled decl \");\n+\t      print_generic_expr (vect_dump, memref, TDF_SLIM);\n+\t    }\n+\t  return NULL_TREE;\n+\t}\n+\n+      /* 2.1 check the alignment.  */\n+      if (DECL_ALIGN (memref) >= TYPE_ALIGN (vectype))\n+\tobject_base_aligned = true;\n+      else\n+\tobject_base_aligned = false;\n+\n+      /* 2.2 update DR_BASE_NAME if necessary.  */\n+      if (!DR_BASE_NAME ((*dr)))\n+\t/* For alias analysis.  In case the analysis of INDIRECT_REF brought \n+\t   us to object.  */\n+\tDR_BASE_NAME ((*dr)) = memref;\n+\n+      base_address = build_fold_addr_expr (memref);\n+    }\n+\n+  /* Part 1:  Case 3. INDIRECT_REFs.  */\n+  else if (TREE_CODE (memref) == INDIRECT_REF)\n+    {      \n+      /* 3.1 get the access function.  */\n+      access_fn = analyze_scalar_evolution (loop, TREE_OPERAND (memref, 0));\n+      if (!access_fn)\n+\t{\n+\t  if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\t    LOOP_LOC (loop_vinfo)))\n+\t    fprintf (vect_dump, \"not vectorized: complicated pointer access.\");\t\n+\t  return NULL_TREE;\n+\t}\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t{\n+\t  fprintf (vect_dump, \"Access function of ptr: \");\n+\t  print_generic_expr (vect_dump, access_fn, TDF_SLIM);\n+\t}\n+\n+      /* 3.2 analyze evolution of MEMREF.  */\n+      evolution_part = evolution_part_in_loop_num (access_fn, loop->num);\n+      if (evolution_part)\n+\t{\n+\t  ptr_dr = vect_analyze_pointer_ref_access (memref, stmt, is_read, \n+\t\t\t\t         access_fn, &ptr_init, &ptr_step);\n+\t  if (!(ptr_dr))\n+\t    return NULL_TREE; \n+\t  \n+\t  object_step = size_binop (PLUS_EXPR, object_step, ptr_step);\n+\t  address_to_analyze = ptr_init;\n+\t}\n+      else\n+\t{\n+\t  if (!(*dr))\n+\t    {\n+\t      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\t\tLOOP_LOC (loop_vinfo))) \n+\t\tfprintf (vect_dump, \"not vectorized: ptr is loop invariant.\");\t\n+\t      return NULL_TREE;\n+\t    }\n+\t  /* Since there exists DR for MEMREF, we are analyzing the base of\n+\t     handled component, which not necessary has evolution in the \n+\t     loop.  */\n+\t  address_to_analyze = TREE_OPERAND (base, 0);\n+\t}\n+      \n+      /* 3.3 set data-reference structure for MEMREF.  */\n+      *dr = (*dr) ? *dr : ptr_dr;\n+\n+      /* 3.4 call vect_address_analysis to analyze INIT of the access \n+\t function.  */\n+      base_address = vect_address_analysis (address_to_analyze, stmt, is_read, \n+\t\t\t       vectype, *dr, &address_offset, &address_misalign, \n+\t\t\t       &address_step, &address_base_aligned);\n+    }\n+    \t    \n+  if (!base_address)\n+    /* MEMREF cannot be analyzed.  */\n+    return NULL_TREE;\n+\n+  /* Part 2: Combine the results of object and address analysis to calculate \n+     INITIAL_OFFSET, STEP and misalignment info. */\n+  *offset = size_binop (PLUS_EXPR, object_offset, address_offset);\n+  if (object_misalign && address_misalign)\n+    *misalign = size_binop (PLUS_EXPR, object_misalign, address_misalign);\n+  else\n+    *misalign = NULL_TREE;\n+  *step = size_binop (PLUS_EXPR, object_step, address_step); \n+  *base_aligned = object_base_aligned && address_base_aligned;\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    {\n+      fprintf (vect_dump, \"Results of object analysis for: \");\n+      print_generic_expr (vect_dump, memref, TDF_SLIM);\n+      fprintf (vect_dump, \"\\n\\tbase: \");\n+      print_generic_expr (vect_dump, base, TDF_SLIM);\n+      fprintf (vect_dump, \"\\n\\toffset: \");\n+      print_generic_expr (vect_dump, *offset, TDF_SLIM);\n+      fprintf (vect_dump, \"\\n\\tstep: \");\n+      print_generic_expr (vect_dump, *step, TDF_SLIM);\n+      fprintf (vect_dump, \"\\n\\tbase aligned %d\\n\\tmisalign: \", *base_aligned);\n+      print_generic_expr (vect_dump, *misalign, TDF_SLIM);\n+    }\n+  return base_address;\n+}\n+\n+\n+/* Function vect_analyze_data_refs.\n+\n+   Find all the data references in the loop.\n+\n+   The general structure of the analysis of data refs in the vectorizer is as \n+   follows:\n+   1- vect_analyze_data_refs(loop): \n+      Find and analyze all data-refs in the loop:\n+          foreach ref\n+\t     base_address = vect_object_analysis(ref)\n+             ref_stmt.memtag =  vect_get_memtag(base)\n+      1.1- vect_object_analysis(ref): \n+           Analyze ref, and build a DR (data_referece struct) for it;\n+           compute base, initial_offset, step and alignment. \n+           Call get_inner_reference for refs handled in this function.\n+           Call vect_addr_analysis(addr) to analyze pointer type expressions.\n+      Set ref_stmt.base, ref_stmt.initial_offset, ref_stmt.alignment, and \n+      ref_stmt.step accordingly. \n+   2- vect_analyze_dependences(): apply dependence testing using ref_stmt.DR\n+   3- vect_analyze_drs_alignment(): check that ref_stmt.alignment is ok.\n+   4- vect_analyze_drs_access(): check that ref_stmt.step is ok.\n+\n+   FORNOW: Handle aligned INDIRECT_REFs and ARRAY_REFs \n+\t   which base is really an array (not a pointer) and which alignment \n+\t   can be forced. This restriction will be relaxed.  */\n+\n+static bool\n+vect_analyze_data_refs (loop_vec_info loop_vinfo)\n+{\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  basic_block *bbs = LOOP_VINFO_BBS (loop_vinfo);\n+  int nbbs = loop->num_nodes;\n+  block_stmt_iterator si;\n+  int j;\n+  struct data_reference *dr;\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"=== vect_analyze_data_refs ===\");\n+\n+  for (j = 0; j < nbbs; j++)\n+    {\n+      basic_block bb = bbs[j];\n+      for (si = bsi_start (bb); !bsi_end_p (si); bsi_next (&si))\n+\t{\n+\t  bool is_read = false;\n+\t  tree stmt = bsi_stmt (si);\n+\t  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+\t  v_may_def_optype v_may_defs = STMT_V_MAY_DEF_OPS (stmt);\n+\t  v_must_def_optype v_must_defs = STMT_V_MUST_DEF_OPS (stmt);\n+\t  vuse_optype vuses = STMT_VUSE_OPS (stmt);\n+\t  varray_type *datarefs = NULL;\n+\t  int nvuses, nv_may_defs, nv_must_defs;\n+\t  tree memref = NULL;\n+\t  tree scalar_type, vectype;\t  \n+\t  tree base, offset, misalign, step, tag;\n+\t  bool base_aligned;\n+\n+\t  /* Assumption: there exists a data-ref in stmt, if and only if \n+             it has vuses/vdefs.  */\n+\n+\t  if (!vuses && !v_may_defs && !v_must_defs)\n+\t    continue;\n+\n+\t  nvuses = NUM_VUSES (vuses);\n+\t  nv_may_defs = NUM_V_MAY_DEFS (v_may_defs);\n+\t  nv_must_defs = NUM_V_MUST_DEFS (v_must_defs);\n+\n+\t  if (nvuses && (nv_may_defs || nv_must_defs))\n+\t    {\n+\t      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t\t{\n+\t\t  fprintf (vect_dump, \"unexpected vdefs and vuses in stmt: \");\n+\t\t  print_generic_expr (vect_dump, stmt, TDF_SLIM);\n+\t\t}\n+\t      return false;\n+\t    }\n+\n+\t  if (TREE_CODE (stmt) != MODIFY_EXPR)\n+\t    {\n+\t      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t\t{\n+\t\t  fprintf (vect_dump, \"unexpected vops in stmt: \");\n+\t\t  print_generic_expr (vect_dump, stmt, TDF_SLIM);\n+\t\t}\n+\t      return false;\n+\t    }\n+\n+\t  if (vuses)\n+\t    {\n+\t      memref = TREE_OPERAND (stmt, 1);\n+\t      datarefs = &(LOOP_VINFO_DATAREF_READS (loop_vinfo));\n+\t      is_read = true;\n+\t    } \n+\t  else /* vdefs */\n+\t    {\n+\t      memref = TREE_OPERAND (stmt, 0);\n+\t      datarefs = &(LOOP_VINFO_DATAREF_WRITES (loop_vinfo));\n+\t      is_read = false;\n+\t    }\n+\t  \n+\t  scalar_type = TREE_TYPE (memref);\n+\t  vectype = get_vectype_for_scalar_type (scalar_type);\n+\t  if (!vectype)\n+\t    {\n+\t      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t\t{\n+\t\t  fprintf (vect_dump, \"no vectype for stmt: \");\n+\t\t  print_generic_expr (vect_dump, stmt, TDF_SLIM);\n+\t\t  fprintf (vect_dump, \" scalar_type: \");\n+\t\t  print_generic_expr (vect_dump, scalar_type, TDF_DETAILS);\n+\t\t}\n+\t      /* It is not possible to vectorize this data reference.  */\n+\t      return false;\n+\t    }\n+\t /* Analyze MEMREF. If it is of a supported form, build data_reference\n+\t     struct for it (DR).  */\n+\t  dr = NULL; \n+\t  base = vect_object_analysis (memref, stmt, is_read, vectype, &dr, \n+\t\t\t\t       &offset, &misalign, &step, \n+\t\t\t\t       &base_aligned);\n+\t  if (!base)\n+\t    {\n+\t      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\t\tLOOP_LOC (loop_vinfo)))\n+\t\t{\n+\t\t  fprintf (vect_dump, \"not vectorized: unhandled data ref: \"); \n+\t\t  print_generic_expr (vect_dump, stmt, TDF_SLIM);\n+\t\t}\n+\t      return false;\n+\t    }\n+\t  /*  Find memtag for aliasing purposes.  */\n+\t  tag = vect_get_memtag (base, dr);\n+\t  if (!tag)\n+\t    {\n+\t      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\t\tLOOP_LOC (loop_vinfo)))\n+\t\t{\n+\t\t  fprintf (vect_dump, \"not vectorized: no memtag ref: \"); \n+\t\t  print_generic_expr (vect_dump, memref, TDF_SLIM);\n+\t\t}\n+\t      return false;\n+\t    }\n+\t  STMT_VINFO_VECT_DR_BASE_ADDRESS (stmt_info) = base;\n+\t  STMT_VINFO_VECT_INIT_OFFSET (stmt_info) = offset;\n+\t  STMT_VINFO_VECT_STEP (stmt_info) = step;\n+\t  STMT_VINFO_VECT_MISALIGNMENT (stmt_info) = misalign;\n+\t  STMT_VINFO_VECT_BASE_ALIGNED_P (stmt_info) = base_aligned;\n+\t  STMT_VINFO_MEMTAG (stmt_info) = tag;\n+\t  STMT_VINFO_VECTYPE (stmt_info) = vectype;\n+\t  VARRAY_PUSH_GENERIC_PTR (*datarefs, dr);\n+\t  STMT_VINFO_DATA_REF (stmt_info) = dr;\n+\t}\n+    }\n+\n+  return true;\n+}\n+\n+\n+/* Utility functions used by vect_mark_stmts_to_be_vectorized.  */\n+\n+/* Function vect_mark_relevant.\n+\n+   Mark STMT as \"relevant for vectorization\" and add it to WORKLIST.  */\n+\n+static void\n+vect_mark_relevant (varray_type *worklist, tree stmt)\n+{\n+  stmt_vec_info stmt_info;\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"mark relevant.\");\n+\n+  if (TREE_CODE (stmt) == PHI_NODE)\n+    {\n+      VARRAY_PUSH_TREE (*worklist, stmt);\n+      return;\n+    }\n+\n+  stmt_info = vinfo_for_stmt (stmt);\n+\n+  if (!stmt_info)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t{\n+\t  fprintf (vect_dump, \"mark relevant: no stmt info!!.\");\n+\t  print_generic_expr (vect_dump, stmt, TDF_SLIM);\n+\t}\n+      return;\n+    }\n+\n+  if (STMT_VINFO_RELEVANT_P (stmt_info))\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+        fprintf (vect_dump, \"already marked relevant.\");\n+      return;\n+    }\n+\n+  STMT_VINFO_RELEVANT_P (stmt_info) = 1;\n+  VARRAY_PUSH_TREE (*worklist, stmt);\n+}\n+\n+\n+/* Function vect_stmt_relevant_p.\n+\n+   Return true if STMT in loop that is represented by LOOP_VINFO is\n+   \"relevant for vectorization\".\n+\n+   A stmt is considered \"relevant for vectorization\" if:\n+   - it has uses outside the loop.\n+   - it has vdefs (it alters memory).\n+   - control stmts in the loop (except for the exit condition).\n+\n+   CHECKME: what other side effects would the vectorizer allow?  */\n+\n+static bool\n+vect_stmt_relevant_p (tree stmt, loop_vec_info loop_vinfo)\n+{\n+  v_may_def_optype v_may_defs;\n+  v_must_def_optype v_must_defs;\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  int i;\n+  dataflow_t df;\n+  int num_uses;\n+\n+  /* cond stmt other than loop exit cond.  */\n+  if (is_ctrl_stmt (stmt) && (stmt != LOOP_VINFO_EXIT_COND (loop_vinfo)))\n+    return true;\n+\n+  /* changing memory.  */\n+  if (TREE_CODE (stmt) != PHI_NODE)\n+    {\n+      v_may_defs = STMT_V_MAY_DEF_OPS (stmt);\n+      v_must_defs = STMT_V_MUST_DEF_OPS (stmt);\n+      if (v_may_defs || v_must_defs)\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    fprintf (vect_dump, \"vec_stmt_relevant_p: stmt has vdefs.\");\n+\t  return true;\n+\t}\n+    }\n+\n+  /* uses outside the loop.  */\n+  df = get_immediate_uses (stmt);\n+  num_uses = num_immediate_uses (df);\n+  for (i = 0; i < num_uses; i++)\n+    {\n+      tree use = immediate_use (df, i);\n+      basic_block bb = bb_for_stmt (use);\n+      if (!flow_bb_inside_loop_p (loop, bb))\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    fprintf (vect_dump, \"vec_stmt_relevant_p: used out of loop.\");\n+\t  return true;\n+\t}\n+    }\n+\n+  return false;\n+}\n+\n+\n+/* Function vect_mark_stmts_to_be_vectorized.\n+\n+   Not all stmts in the loop need to be vectorized. For example:\n+\n+     for i...\n+       for j...\n+   1.    T0 = i + j\n+   2.\t T1 = a[T0]\n+\n+   3.    j = j + 1\n+\n+   Stmt 1 and 3 do not need to be vectorized, because loop control and\n+   addressing of vectorized data-refs are handled differently.\n+\n+   This pass detects such stmts.  */\n+\n+static bool\n+vect_mark_stmts_to_be_vectorized (loop_vec_info loop_vinfo)\n+{\n+  varray_type worklist;\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  basic_block *bbs = LOOP_VINFO_BBS (loop_vinfo);\n+  unsigned int nbbs = loop->num_nodes;\n+  block_stmt_iterator si;\n+  tree stmt;\n+  stmt_ann_t ann;\n+  unsigned int i;\n+  int j;\n+  use_optype use_ops;\n+  stmt_vec_info stmt_info;\n+  basic_block bb;\n+  tree phi;\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"=== vect_mark_stmts_to_be_vectorized ===\");\n+\n+  bb = loop->header;\n+  for (phi = phi_nodes (bb); phi; phi = PHI_CHAIN (phi))\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+        {\n+          fprintf (vect_dump, \"init: phi relevant? \");\n+          print_generic_expr (vect_dump, phi, TDF_SLIM);\n+        }\n+\n+      if (vect_stmt_relevant_p (phi, loop_vinfo))\n+\t{\n+\t  if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\t    LOOP_LOC (loop_vinfo)))\n+\t    fprintf (vect_dump, \"unsupported reduction/induction.\");\n+          return false;\n+\t}\n+    }\n+\n+  VARRAY_TREE_INIT (worklist, 64, \"work list\");\n+\n+  /* 1. Init worklist.  */\n+\n+  for (i = 0; i < nbbs; i++)\n+    {\n+      bb = bbs[i];\n+      for (si = bsi_start (bb); !bsi_end_p (si); bsi_next (&si))\n+\t{\n+\t  stmt = bsi_stmt (si);\n+\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    {\n+\t      fprintf (vect_dump, \"init: stmt relevant? \");\n+\t      print_generic_expr (vect_dump, stmt, TDF_SLIM);\n+\t    } \n+\n+\t  stmt_info = vinfo_for_stmt (stmt);\n+\t  STMT_VINFO_RELEVANT_P (stmt_info) = 0;\n+\n+\t  if (vect_stmt_relevant_p (stmt, loop_vinfo))\n+\t    vect_mark_relevant (&worklist, stmt);\n+\t}\n+    }\n+\n+\n+  /* 2. Process_worklist */\n+\n+  while (VARRAY_ACTIVE_SIZE (worklist) > 0)\n+    {\n+      stmt = VARRAY_TOP_TREE (worklist);\n+      VARRAY_POP (worklist);\n+\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t{\n+          fprintf (vect_dump, \"worklist: examine stmt: \");\n+          print_generic_expr (vect_dump, stmt, TDF_SLIM);\n+\t}\n+\n+      /* Examine the USES in this statement. Mark all the statements which\n+         feed this statement's uses as \"relevant\", unless the USE is used as\n+         an array index.  */\n+\n+      if (TREE_CODE (stmt) == PHI_NODE)\n+\t{\n+\t  /* follow the def-use chain inside the loop.  */\n+\t  for (j = 0; j < PHI_NUM_ARGS (stmt); j++)\n+\t    {\n+\t      tree arg = PHI_ARG_DEF (stmt, j);\n+\t      tree def_stmt = NULL_TREE;\n+\t      basic_block bb;\n+\t      if (!vect_is_simple_use (arg, loop_vinfo, &def_stmt))\n+\t\t{\n+\t\t  if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\t\t    LOOP_LOC (loop_vinfo)))\n+\t\t    fprintf (vect_dump, \"not vectorized: unsupported use in stmt.\");\n+\t\t  varray_clear (worklist);\n+\t\t  return false;\n+\t\t}\n+\t      if (!def_stmt)\n+\t\tcontinue;\n+\n+\t      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t        {\n+\t          fprintf (vect_dump, \"worklist: def_stmt: \");\n+\t\t  print_generic_expr (vect_dump, def_stmt, TDF_SLIM);\n+\t\t}\n+\n+\t      bb = bb_for_stmt (def_stmt);\n+\t      if (flow_bb_inside_loop_p (loop, bb))\n+\t        vect_mark_relevant (&worklist, def_stmt);\n+\t    }\n+\t} \n+\n+      ann = stmt_ann (stmt);\n+      use_ops = USE_OPS (ann);\n+\n+      for (i = 0; i < NUM_USES (use_ops); i++)\n+\t{\n+\t  tree use = USE_OP (use_ops, i);\n+\n+\t  /* We are only interested in uses that need to be vectorized. Uses \n+\t     that are used for address computation are not considered relevant.\n+\t   */\n+\t  if (exist_non_indexing_operands_for_use_p (use, stmt))\n+\t    {\n+              tree def_stmt = NULL_TREE;\n+              basic_block bb;\n+              if (!vect_is_simple_use (use, loop_vinfo, &def_stmt))\n+                {\n+                  if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS,\n+\t\t\t\t\t    LOOP_LOC (loop_vinfo)))\n+                    fprintf (vect_dump, \"not vectorized: unsupported use in stmt.\");\n+                  varray_clear (worklist);\n+                  return false;\n+                }\n+\n+\t      if (!def_stmt)\n+\t\tcontinue;\n+\n+              if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+                {\n+                  fprintf (vect_dump, \"worklist: examine use %d: \", i);\n+                  print_generic_expr (vect_dump, use, TDF_SLIM);\n+                }\n+\n+\t      bb = bb_for_stmt (def_stmt);\n+\t      if (flow_bb_inside_loop_p (loop, bb))\n+\t\tvect_mark_relevant (&worklist, def_stmt);\n+\t    }\n+\t}\n+    }\t\t\t\t/* while worklist */\n+\n+  varray_clear (worklist);\n+  return true;\n+}\n+\n+\n+/* Function vect_can_advance_ivs_p\n+\n+   In case the number of iterations that LOOP iterates in unknown at compile \n+   time, an epilog loop will be generated, and the loop induction variables \n+   (IVs) will be \"advanced\" to the value they are supposed to take just before \n+   the epilog loop.  Here we check that the access function of the loop IVs\n+   and the expression that represents the loop bound are simple enough.\n+   These restrictions will be relaxed in the future.  */\n+\n+static bool \n+vect_can_advance_ivs_p (loop_vec_info loop_vinfo)\n+{\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  basic_block bb = loop->header;\n+  tree phi;\n+\n+  /* Analyze phi functions of the loop header.  */\n+\n+  for (phi = phi_nodes (bb); phi; phi = PHI_CHAIN (phi))\n+    {\n+      tree access_fn = NULL;\n+      tree evolution_part;\n+\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t{\n+          fprintf (vect_dump, \"Analyze phi: \");\n+          print_generic_expr (vect_dump, phi, TDF_SLIM);\n+\t}\n+\n+      /* Skip virtual phi's. The data dependences that are associated with\n+         virtual defs/uses (i.e., memory accesses) are analyzed elsewhere.  */\n+\n+      if (!is_gimple_reg (SSA_NAME_VAR (PHI_RESULT (phi))))\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    fprintf (vect_dump, \"virtual phi. skip.\");\n+\t  continue;\n+\t}\n+\n+      /* Analyze the evolution function.  */\n+\n+      access_fn = instantiate_parameters\n+\t(loop, analyze_scalar_evolution (loop, PHI_RESULT (phi)));\n+\n+      if (!access_fn)\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    fprintf (vect_dump, \"No Access function.\");\n+\t  return false;\n+\t}\n+\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+        {\n+\t  fprintf (vect_dump, \"Access function of PHI: \");\n+\t  print_generic_expr (vect_dump, access_fn, TDF_SLIM);\n+        }\n+\n+      evolution_part = evolution_part_in_loop_num (access_fn, loop->num);\n+      \n+      if (evolution_part == NULL_TREE)\n+\treturn false;\n+  \n+      /* FORNOW: We do not transform initial conditions of IVs \n+\t which evolution functions are a polynomial of degree >= 2.  */\n+\n+      if (tree_is_chrec (evolution_part))\n+\treturn false;  \n+    }\n+\n+  return true;\n+}\n+\n+\n+/* Function vect_get_loop_niters.\n+\n+   Determine how many iterations the loop is executed.\n+   If an expression that represents the number of iterations\n+   can be constructed, place it in NUMBER_OF_ITERATIONS.\n+   Return the loop exit condition.  */\n+\n+static tree\n+vect_get_loop_niters (struct loop *loop, tree *number_of_iterations)\n+{\n+  tree niters;\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"=== get_loop_niters ===\");\n+\n+  niters = number_of_iterations_in_loop (loop);\n+\n+  if (niters != NULL_TREE\n+      && niters != chrec_dont_know)\n+    {\n+      *number_of_iterations = niters;\n+\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t{\n+\t  fprintf (vect_dump, \"==> get_loop_niters:\" );\n+\t  print_generic_expr (vect_dump, *number_of_iterations, TDF_SLIM);\n+\t}\n+    }\n+\n+  return get_loop_exit_condition (loop);\n+}\n+\n+\n+/* Function vect_analyze_loop_form.\n+\n+   Verify the following restrictions (some may be relaxed in the future):\n+   - it's an inner-most loop\n+   - number of BBs = 2 (which are the loop header and the latch)\n+   - the loop has a pre-header\n+   - the loop has a single entry and exit\n+   - the loop exit condition is simple enough, and the number of iterations\n+     can be analyzed (a countable loop).  */\n+\n+static loop_vec_info\n+vect_analyze_loop_form (struct loop *loop)\n+{\n+  loop_vec_info loop_vinfo;\n+  tree loop_cond;\n+  tree number_of_iterations = NULL;\n+  bool rescan = false;\n+  LOC loop_loc;\n+\n+  loop_loc = find_loop_location (loop);\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, loop_loc))\n+    fprintf (vect_dump, \"=== vect_analyze_loop_form ===\");\n+\n+  if (loop->inner)\n+    {\n+      if (vect_print_dump_info (REPORT_OUTER_LOOPS, loop_loc))\n+        fprintf (vect_dump, \"not vectorized: nested loop.\");\n+      return NULL;\n+    }\n+  \n+  if (!loop->single_exit \n+      || loop->num_nodes != 2\n+      || EDGE_COUNT (loop->header->preds) != 2\n+      || loop->num_entries != 1)\n+    {\n+      if (vect_print_dump_info (REPORT_BAD_FORM_LOOPS, loop_loc))\n+        {\n+          if (!loop->single_exit)\n+            fprintf (vect_dump, \"not vectorized: multiple exits.\");\n+          else if (loop->num_nodes != 2)\n+            fprintf (vect_dump, \"not vectorized: too many BBs in loop.\");\n+          else if (EDGE_COUNT (loop->header->preds) != 2)\n+            fprintf (vect_dump, \"not vectorized: too many incoming edges.\");\n+          else if (loop->num_entries != 1)\n+            fprintf (vect_dump, \"not vectorized: too many entries.\");\n+        }\n+\n+      return NULL;\n+    }\n+\n+  /* We assume that the loop exit condition is at the end of the loop. i.e,\n+     that the loop is represented as a do-while (with a proper if-guard\n+     before the loop if needed), where the loop header contains all the\n+     executable statements, and the latch is empty.  */\n+  if (!empty_block_p (loop->latch))\n+    {\n+      if (vect_print_dump_info (REPORT_BAD_FORM_LOOPS, loop_loc))\n+        fprintf (vect_dump, \"not vectorized: unexpectd loop form.\");\n+      return NULL;\n+    }\n+\n+  /* Make sure we have a preheader basic block.  */\n+  if (!loop->pre_header)\n+    {\n+      rescan = true;\n+      loop_split_edge_with (loop_preheader_edge (loop), NULL);\n+    }\n+    \n+  /* Make sure there exists a single-predecessor exit bb:  */\n+  if (EDGE_COUNT (loop->exit_edges[0]->dest->preds) != 1)\n+    {\n+      rescan = true;\n+      loop_split_edge_with (loop->exit_edges[0], NULL);\n+    }\n+    \n+  if (rescan)\n+    {\n+      flow_loop_scan (loop, LOOP_ALL);\n+      /* Flow loop scan does not update loop->single_exit field.  */\n+      loop->single_exit = loop->exit_edges[0];\n+    }\n+\n+  if (empty_block_p (loop->header))\n+    {\n+      if (vect_print_dump_info (REPORT_BAD_FORM_LOOPS, loop_loc))\n+        fprintf (vect_dump, \"not vectorized: empty loop.\");\n+      return NULL;\n+    }\n+\n+  loop_cond = vect_get_loop_niters (loop, &number_of_iterations);\n+  if (!loop_cond)\n+    {\n+      if (vect_print_dump_info (REPORT_BAD_FORM_LOOPS, loop_loc))\n+\tfprintf (vect_dump, \"not vectorized: complicated exit condition.\");\n+      return NULL;\n+    }\n+  \n+  if (!number_of_iterations) \n+    {\n+      if (vect_print_dump_info (REPORT_BAD_FORM_LOOPS, loop_loc))\n+\tfprintf (vect_dump, \n+\t\t \"not vectorized: number of iterations cannot be computed.\");\n+      return NULL;\n+    }\n+\n+  if (chrec_contains_undetermined (number_of_iterations))\n+    {\n+      if (vect_print_dump_info (REPORT_BAD_FORM_LOOPS, loop_loc))\n+        fprintf (vect_dump, \"Infinite number of iterations.\");\n+      return false;\n+    }\n+\n+  loop_vinfo = new_loop_vec_info (loop);\n+  LOOP_VINFO_NITERS (loop_vinfo) = number_of_iterations;\n+\n+  if (!LOOP_VINFO_NITERS_KNOWN_P (loop_vinfo))\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, loop_loc))\n+        {\n+          fprintf (vect_dump, \"Symbolic number of iterations is \");\n+          print_generic_expr (vect_dump, number_of_iterations, TDF_DETAILS);\n+        }\n+    }\n+  else\n+  if (LOOP_VINFO_INT_NITERS (loop_vinfo) == 0)\n+    {\n+      if (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS, loop_loc))\n+        fprintf (vect_dump, \"not vectorized: number of iterations = 0.\");\n+      return NULL;\n+    }\n+\n+  LOOP_VINFO_EXIT_COND (loop_vinfo) = loop_cond;\n+  LOOP_VINFO_LOC (loop_vinfo) = loop_loc;\n+\n+  return loop_vinfo;\n+}\n+\n+\n+/* Function vect_analyze_loop.\n+\n+   Apply a set of analyses on LOOP, and create a loop_vec_info struct\n+   for it. The different analyses will record information in the\n+   loop_vec_info struct.  */\n+loop_vec_info\n+vect_analyze_loop (struct loop *loop)\n+{\n+  bool ok;\n+  loop_vec_info loop_vinfo;\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"===== analyze_loop_nest =====\");\n+\n+  /* Check the CFG characteristics of the loop (nesting, entry/exit, etc.  */\n+\n+  loop_vinfo = vect_analyze_loop_form (loop);\n+  if (!loop_vinfo)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\tfprintf (vect_dump, \"bad loop form.\");\n+      return NULL;\n+    }\n+\n+  /* Find all data references in the loop (which correspond to vdefs/vuses)\n+     and analyze their evolution in the loop.\n+\n+     FORNOW: Handle only simple, array references, which\n+     alignment can be forced, and aligned pointer-references.  */\n+\n+  ok = vect_analyze_data_refs (loop_vinfo);\n+  if (!ok)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, LOOP_LOC (loop_vinfo)))\n+\tfprintf (vect_dump, \"bad data references.\");\n+      destroy_loop_vec_info (loop_vinfo);\n+      return NULL;\n+    }\n+\n+  /* Data-flow analysis to detect stmts that do not need to be vectorized.  */\n+\n+  ok = vect_mark_stmts_to_be_vectorized (loop_vinfo);\n+  if (!ok)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, LOOP_LOC (loop_vinfo)))\n+\tfprintf (vect_dump, \"unexpected pattern.\");\n+      destroy_loop_vec_info (loop_vinfo);\n+      return NULL;\n+    }\n+\n+  /* Check that all cross-iteration scalar data-flow cycles are OK.\n+     Cross-iteration cycles caused by virtual phis are analyzed separately.  */\n+\n+  ok = vect_analyze_scalar_cycles (loop_vinfo);\n+  if (!ok)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, LOOP_LOC (loop_vinfo)))\n+\tfprintf (vect_dump, \"bad scalar cycle.\");\n+      destroy_loop_vec_info (loop_vinfo);\n+      return NULL;\n+    }\n+\n+  /* Analyze data dependences between the data-refs in the loop. \n+     FORNOW: fail at the first data dependence that we encounter.  */\n+\n+  ok = vect_analyze_data_ref_dependences (loop_vinfo);\n+  if (!ok)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, LOOP_LOC (loop_vinfo)))\n+\tfprintf (vect_dump, \"bad data dependence.\");\n+      destroy_loop_vec_info (loop_vinfo);\n+      return NULL;\n+    }\n+\n+  /* Analyze the access patterns of the data-refs in the loop (consecutive,\n+     complex, etc.). FORNOW: Only handle consecutive access pattern.  */\n+\n+  ok = vect_analyze_data_ref_accesses (loop_vinfo);\n+  if (!ok)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, LOOP_LOC (loop_vinfo)))\n+\tfprintf (vect_dump, \"bad data access.\");\n+      destroy_loop_vec_info (loop_vinfo);\n+      return NULL;\n+    }\n+\n+  /* Analyze the alignment of the data-refs in the loop.\n+     FORNOW: Only aligned accesses are handled.  */\n+\n+  ok = vect_analyze_data_refs_alignment (loop_vinfo);\n+  if (!ok)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, LOOP_LOC (loop_vinfo)))\n+\tfprintf (vect_dump, \"bad data alignment.\");\n+      destroy_loop_vec_info (loop_vinfo);\n+      return NULL;\n+    }\n+\n+  /* Scan all the operations in the loop and make sure they are\n+     vectorizable.  */\n+\n+  ok = vect_analyze_operations (loop_vinfo);\n+  if (!ok)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, LOOP_LOC (loop_vinfo)))\n+\tfprintf (vect_dump, \"bad operation or unsupported loop bound.\");\n+      destroy_loop_vec_info (loop_vinfo);\n+      return NULL;\n+    }\n+\n+  LOOP_VINFO_VECTORIZABLE_P (loop_vinfo) = 1;\n+\n+  return loop_vinfo;\n+}"}, {"sha": "5f71256e14ea5bf6b36611d8096311bf712416f3", "filename": "gcc/tree-vect-transform.c", "status": "added", "additions": 1746, "deletions": 0, "changes": 1746, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7064d11bb781d5cad08cdcb4942712390c477ca/gcc%2Ftree-vect-transform.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7064d11bb781d5cad08cdcb4942712390c477ca/gcc%2Ftree-vect-transform.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-transform.c?ref=f7064d11bb781d5cad08cdcb4942712390c477ca", "patch": "@@ -0,0 +1,1746 @@\n+/* Transformation Utilities for Loop Vectorization.\n+   Copyright (C) 2003,2004,2005 Free Software Foundation, Inc.\n+   Contributed by Dorit Naishlos <dorit@il.ibm.com>\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 2, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING.  If not, write to the Free\n+Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n+02111-1307, USA.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"errors.h\"\n+#include \"ggc.h\"\n+#include \"tree.h\"\n+#include \"target.h\"\n+#include \"rtl.h\"\n+#include \"basic-block.h\"\n+#include \"diagnostic.h\"\n+#include \"tree-flow.h\"\n+#include \"tree-dump.h\"\n+#include \"timevar.h\"\n+#include \"cfgloop.h\"\n+#include \"expr.h\"\n+#include \"optabs.h\"\n+#include \"tree-data-ref.h\"\n+#include \"tree-chrec.h\"\n+#include \"tree-scalar-evolution.h\"\n+#include \"tree-vectorizer.h\"\n+#include \"langhooks.h\"\n+#include \"tree-pass.h\"\n+#include \"toplev.h\"\n+\n+/* Utility functions for the code transformation.  */\n+static bool vect_transform_stmt (tree, block_stmt_iterator *);\n+static void vect_align_data_ref (tree);\n+static tree vect_create_destination_var (tree, tree);\n+static tree vect_create_data_ref_ptr \n+  (tree, block_stmt_iterator *, tree, tree *, bool); \n+static tree vect_create_index_for_vector_ref (loop_vec_info);\n+static tree vect_create_addr_base_for_vector_ref (tree, tree *, tree);\n+static tree vect_get_new_vect_var (tree, enum vect_var_kind, const char *);\n+static tree vect_get_vec_def_for_operand (tree, tree);\n+static tree vect_init_vector (tree, tree);\n+static void vect_finish_stmt_generation \n+  (tree stmt, tree vec_stmt, block_stmt_iterator *bsi);\n+\n+/* Utility function dealing with loop peeling (not peeling itself).  */\n+static void vect_generate_tmps_on_preheader \n+  (loop_vec_info, tree *, tree *, tree *);\n+static tree vect_build_loop_niters (loop_vec_info);\n+static void vect_update_ivs_after_vectorizer (loop_vec_info, tree, edge); \n+static tree vect_gen_niters_for_prolog_loop (loop_vec_info, tree);\n+static void vect_update_inits_of_dr (struct data_reference *, tree niters);\n+static void vect_update_inits_of_drs (loop_vec_info, tree);\n+static void vect_do_peeling_for_alignment (loop_vec_info, struct loops *);\n+static void vect_do_peeling_for_loop_bound \n+  (loop_vec_info, tree *, struct loops *);\n+\n+\n+/* Function vect_get_new_vect_var.\n+\n+   Returns a name for a new variable. The current naming scheme appends the \n+   prefix \"vect_\" or \"vect_p\" (depending on the value of VAR_KIND) to \n+   the name of vectorizer generated variables, and appends that to NAME if \n+   provided.  */\n+\n+static tree\n+vect_get_new_vect_var (tree type, enum vect_var_kind var_kind, const char *name)\n+{\n+  const char *prefix;\n+  int prefix_len;\n+  tree new_vect_var;\n+\n+  if (var_kind == vect_simple_var)\n+    prefix = \"vect_\"; \n+  else\n+    prefix = \"vect_p\";\n+\n+  prefix_len = strlen (prefix);\n+\n+  if (name)\n+    new_vect_var = create_tmp_var (type, concat (prefix, name, NULL));\n+  else\n+    new_vect_var = create_tmp_var (type, prefix);\n+\n+  return new_vect_var;\n+}\n+\n+\n+/* Function vect_create_index_for_vector_ref.\n+\n+   Create (and return) an index variable, along with it's update chain in the\n+   loop. This variable will be used to access a memory location in a vector\n+   operation.\n+\n+   Input:\n+   LOOP: The loop being vectorized.\n+   BSI: The block_stmt_iterator where STMT is. Any new stmts created by this\n+        function can be added here, or in the loop pre-header.\n+\n+   Output:\n+   Return an index that will be used to index a vector array.  It is expected\n+   that a pointer to the first vector will be used as the base address for the\n+   indexed reference.\n+\n+   FORNOW: we are not trying to be efficient, just creating a new index each\n+   time from scratch.  At this time all vector references could use the same\n+   index.\n+\n+   TODO: create only one index to be used by all vector references.  Record\n+   the index in the LOOP_VINFO the first time this procedure is called and\n+   return it on subsequent calls.  The increment of this index must be placed\n+   just before the conditional expression that ends the single block loop.  */\n+\n+static tree\n+vect_create_index_for_vector_ref (loop_vec_info loop_vinfo)\n+{\n+  tree init, step;\n+  block_stmt_iterator incr_bsi;\n+  bool insert_after;\n+  tree indx_before_incr, indx_after_incr;\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  tree incr;\n+\n+  /* It is assumed that the base pointer used for vectorized access contains\n+     the address of the first vector.  Therefore the index used for vectorized\n+     access must be initialized to zero and incremented by 1.  */\n+\n+  init = integer_zero_node;\n+  step = integer_one_node;\n+\n+  standard_iv_increment_position (loop, &incr_bsi, &insert_after);\n+  create_iv (init, step, NULL_TREE, loop, &incr_bsi, insert_after,\n+\t&indx_before_incr, &indx_after_incr);\n+  incr = bsi_stmt (incr_bsi);\n+  get_stmt_operands (incr);\n+  set_stmt_info (stmt_ann (incr), new_stmt_vec_info (incr, loop_vinfo));\n+\n+  return indx_before_incr;\n+}\n+\n+\n+/* Function vect_create_addr_base_for_vector_ref.\n+\n+   Create an expression that computes the address of the first memory location\n+   that will be accessed for a data reference.\n+\n+   Input:\n+   STMT: The statement containing the data reference.\n+   NEW_STMT_LIST: Must be initialized to NULL_TREE or a statement list.\n+   OFFSET: Optional. If supplied, it is be added to the initial address.\n+\n+   Output:\n+   1. Return an SSA_NAME whose value is the address of the memory location of \n+      the first vector of the data reference.\n+   2. If new_stmt_list is not NULL_TREE after return then the caller must insert\n+      these statement(s) which define the returned SSA_NAME.\n+\n+   FORNOW: We are only handling array accesses with step 1.  */\n+\n+static tree\n+vect_create_addr_base_for_vector_ref (tree stmt,\n+                                      tree *new_stmt_list,\n+\t\t\t\t      tree offset)\n+{\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n+  tree data_ref_base = \n+    unshare_expr (STMT_VINFO_VECT_DR_BASE_ADDRESS (stmt_info));\n+  tree base_name = build_fold_indirect_ref (data_ref_base);\n+  tree ref = DR_REF (dr);\n+  tree scalar_type = TREE_TYPE (ref);\n+  tree scalar_ptr_type = build_pointer_type (scalar_type);\n+  tree vec_stmt;\n+  tree new_temp;\n+  tree addr_base, addr_expr;\n+  tree dest, new_stmt;\n+  tree base_offset = unshare_expr (STMT_VINFO_VECT_INIT_OFFSET (stmt_info));\n+\n+  /* Create base_offset */\n+  dest = create_tmp_var (TREE_TYPE (base_offset), \"base_off\");\n+  add_referenced_tmp_var (dest);\n+  base_offset = force_gimple_operand (base_offset, &new_stmt, false, dest);  \n+  append_to_statement_list_force (new_stmt, new_stmt_list);\n+\n+  if (offset)\n+    {\n+      tree tmp = create_tmp_var (TREE_TYPE (base_offset), \"offset\");\n+      add_referenced_tmp_var (tmp);\n+      offset = fold (build2 (MULT_EXPR, TREE_TYPE (offset), offset, \n+\t\t\t     STMT_VINFO_VECT_STEP (stmt_info)));\n+      base_offset = fold (build2 (PLUS_EXPR, TREE_TYPE (base_offset), \n+\t\t\t\t  base_offset, offset));\n+      base_offset = force_gimple_operand (base_offset, &new_stmt, false, tmp);  \n+      append_to_statement_list_force (new_stmt, new_stmt_list);\n+    }\n+  \n+  /* base + base_offset */\n+  addr_base = fold (build2 (PLUS_EXPR, TREE_TYPE (data_ref_base), data_ref_base, \n+\t\t\t    base_offset));\n+\n+  /* addr_expr = addr_base */\n+  addr_expr = vect_get_new_vect_var (scalar_ptr_type, vect_pointer_var,\n+                                     get_name (base_name));\n+  add_referenced_tmp_var (addr_expr);\n+  vec_stmt = build2 (MODIFY_EXPR, void_type_node, addr_expr, addr_base);\n+  new_temp = make_ssa_name (addr_expr, vec_stmt);\n+  TREE_OPERAND (vec_stmt, 0) = new_temp;\n+  append_to_statement_list_force (vec_stmt, new_stmt_list);\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    {\n+      fprintf (vect_dump, \"created \");\n+      print_generic_expr (vect_dump, vec_stmt, TDF_SLIM);\n+    }\n+  return new_temp;\n+}\n+\n+\n+/* Function vect_align_data_ref.\n+\n+   Handle mislignment of a memory accesses.\n+\n+   FORNOW: Can't handle misaligned accesses. \n+   Make sure that the dataref is aligned.  */\n+\n+static void\n+vect_align_data_ref (tree stmt)\n+{\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n+\n+  /* FORNOW: can't handle misaligned accesses; \n+             all accesses expected to be aligned.  */\n+  gcc_assert (aligned_access_p (dr));\n+}\n+\n+\n+/* Function vect_create_data_ref_ptr.\n+\n+   Create a memory reference expression for vector access, to be used in a\n+   vector load/store stmt. The reference is based on a new pointer to vector\n+   type (vp).\n+\n+   Input:\n+   1. STMT: a stmt that references memory. Expected to be of the form\n+         MODIFY_EXPR <name, data-ref> or MODIFY_EXPR <data-ref, name>.\n+   2. BSI: block_stmt_iterator where new stmts can be added.\n+   3. OFFSET (optional): an offset to be added to the initial address accessed\n+        by the data-ref in STMT.\n+   4. ONLY_INIT: indicate if vp is to be updated in the loop, or remain\n+        pointing to the initial address.\n+\n+   Output:\n+   1. Declare a new ptr to vector_type, and have it point to the base of the\n+      data reference (initial addressed accessed by the data reference).\n+      For example, for vector of type V8HI, the following code is generated:\n+\n+      v8hi *vp;\n+      vp = (v8hi *)initial_address;\n+\n+      if OFFSET is not supplied:\n+         initial_address = &a[init];\n+      if OFFSET is supplied:\n+         initial_address = &a[init + OFFSET];\n+\n+      Return the initial_address in INITIAL_ADDRESS.\n+\n+   2. Create a data-reference in the loop based on the new vector pointer vp,\n+      and using a new index variable 'idx' as follows:\n+\n+      vp' = vp + update\n+\n+      where if ONLY_INIT is true:\n+         update = zero\n+      and otherwise\n+         update = idx + vector_type_size\n+\n+      Return the pointer vp'.\n+\n+\n+   FORNOW: handle only aligned and consecutive accesses.  */\n+\n+static tree\n+vect_create_data_ref_ptr (tree stmt, block_stmt_iterator *bsi, tree offset,\n+                          tree *initial_address, bool only_init)\n+{\n+  tree base_name;\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n+  tree vect_ptr_type;\n+  tree vect_ptr;\n+  tree tag;\n+  v_may_def_optype v_may_defs = STMT_V_MAY_DEF_OPS (stmt);\n+  v_must_def_optype v_must_defs = STMT_V_MUST_DEF_OPS (stmt);\n+  vuse_optype vuses = STMT_VUSE_OPS (stmt);\n+  int nvuses, nv_may_defs, nv_must_defs;\n+  int i;\n+  tree new_temp;\n+  tree vec_stmt;\n+  tree new_stmt_list = NULL_TREE;\n+  tree idx;\n+  edge pe = loop_preheader_edge (loop);\n+  basic_block new_bb;\n+  tree vect_ptr_init;\n+  tree vectype_size;\n+  tree ptr_update;\n+  tree data_ref_ptr;\n+  tree type, tmp, size;\n+\n+  base_name =  build_fold_indirect_ref (unshare_expr (\n+\t\t      STMT_VINFO_VECT_DR_BASE_ADDRESS (stmt_info)));\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    {\n+      tree data_ref_base = base_name;\n+      fprintf (vect_dump, \"create array_ref of type: \");\n+      print_generic_expr (vect_dump, vectype, TDF_SLIM);\n+      if (TREE_CODE (data_ref_base) == VAR_DECL)\n+        fprintf (vect_dump, \"  vectorizing a one dimensional array ref: \");\n+      else if (TREE_CODE (data_ref_base) == ARRAY_REF)\n+        fprintf (vect_dump, \"  vectorizing a multidimensional array ref: \");\n+      else if (TREE_CODE (data_ref_base) == COMPONENT_REF)\n+        fprintf (vect_dump, \"  vectorizing a record based array ref: \");\n+      else if (TREE_CODE (data_ref_base) == SSA_NAME)\n+        fprintf (vect_dump, \"  vectorizing a pointer ref: \");\n+      print_generic_expr (vect_dump, base_name, TDF_SLIM);\n+    }\n+\n+  /** (1) Create the new vector-pointer variable:  **/\n+\n+  vect_ptr_type = build_pointer_type (vectype);\n+  vect_ptr = vect_get_new_vect_var (vect_ptr_type, vect_pointer_var,\n+                                    get_name (base_name));\n+  add_referenced_tmp_var (vect_ptr);\n+  \n+  \n+  /** (2) Handle aliasing information of the new vector-pointer:  **/\n+  \n+  tag = STMT_VINFO_MEMTAG (stmt_info);\n+  gcc_assert (tag);\n+  get_var_ann (vect_ptr)->type_mem_tag = tag;\n+  \n+  /* Mark for renaming all aliased variables\n+     (i.e, the may-aliases of the type-mem-tag).  */\n+  nvuses = NUM_VUSES (vuses);\n+  nv_may_defs = NUM_V_MAY_DEFS (v_may_defs);\n+  nv_must_defs = NUM_V_MUST_DEFS (v_must_defs);\n+  for (i = 0; i < nvuses; i++)\n+    {\n+      tree use = VUSE_OP (vuses, i);\n+      if (TREE_CODE (use) == SSA_NAME)\n+        bitmap_set_bit (vars_to_rename, var_ann (SSA_NAME_VAR (use))->uid);\n+    }\n+  for (i = 0; i < nv_may_defs; i++)\n+    {\n+      tree def = V_MAY_DEF_RESULT (v_may_defs, i);\n+      if (TREE_CODE (def) == SSA_NAME)\n+        bitmap_set_bit (vars_to_rename, var_ann (SSA_NAME_VAR (def))->uid);\n+    }\n+  for (i = 0; i < nv_must_defs; i++)\n+    {\n+      tree def = V_MUST_DEF_RESULT (v_must_defs, i);\n+      if (TREE_CODE (def) == SSA_NAME)\n+        bitmap_set_bit (vars_to_rename, var_ann (SSA_NAME_VAR (def))->uid);\n+    }\n+\n+\n+  /** (3) Calculate the initial address the vector-pointer, and set\n+          the vector-pointer to point to it before the loop:  **/\n+\n+  /* Create: (&(base[init_val+offset]) in the loop preheader.  */\n+  new_temp = vect_create_addr_base_for_vector_ref (stmt, &new_stmt_list,\n+                                                   offset);\n+  pe = loop_preheader_edge (loop);\n+  new_bb = bsi_insert_on_edge_immediate (pe, new_stmt_list);\n+  gcc_assert (!new_bb);\n+  *initial_address = new_temp;\n+\n+  /* Create: p = (vectype *) initial_base  */\n+  vec_stmt = fold_convert (vect_ptr_type, new_temp);\n+  vec_stmt = build2 (MODIFY_EXPR, void_type_node, vect_ptr, vec_stmt);\n+  new_temp = make_ssa_name (vect_ptr, vec_stmt);\n+  TREE_OPERAND (vec_stmt, 0) = new_temp;\n+  new_bb = bsi_insert_on_edge_immediate (pe, vec_stmt);\n+  gcc_assert (!new_bb);\n+  vect_ptr_init = TREE_OPERAND (vec_stmt, 0);\n+\n+\n+  /** (4) Handle the updating of the vector-pointer inside the loop: **/\n+\n+  if (only_init) /* No update in loop is required.  */\n+    return vect_ptr_init;\n+\n+  idx = vect_create_index_for_vector_ref (loop_vinfo);\n+\n+  /* Create: update = idx * vectype_size  */\n+  tmp = create_tmp_var (integer_type_node, \"update\");\n+  add_referenced_tmp_var (tmp);\n+  size = TYPE_SIZE (vect_ptr_type); \n+  type = lang_hooks.types.type_for_size (tree_low_cst (size, 1), 1);\n+  ptr_update = create_tmp_var (type, \"update\");\n+  add_referenced_tmp_var (ptr_update);\n+  vectype_size = TYPE_SIZE_UNIT (vectype);\n+  vec_stmt = build2 (MULT_EXPR, integer_type_node, idx, vectype_size);\n+  vec_stmt = build2 (MODIFY_EXPR, void_type_node, tmp, vec_stmt);\n+  new_temp = make_ssa_name (tmp, vec_stmt);\n+  TREE_OPERAND (vec_stmt, 0) = new_temp;\n+  bsi_insert_before (bsi, vec_stmt, BSI_SAME_STMT);\n+  vec_stmt = fold_convert (type, new_temp);\n+  vec_stmt = build2 (MODIFY_EXPR, void_type_node, ptr_update, vec_stmt);\n+  new_temp = make_ssa_name (ptr_update, vec_stmt);\n+  TREE_OPERAND (vec_stmt, 0) = new_temp;\n+  bsi_insert_before (bsi, vec_stmt, BSI_SAME_STMT);\n+\n+  /* Create: data_ref_ptr = vect_ptr_init + update  */\n+  vec_stmt = build2 (PLUS_EXPR, vect_ptr_type, vect_ptr_init, new_temp);\n+  vec_stmt = build2 (MODIFY_EXPR, void_type_node, vect_ptr, vec_stmt);\n+  new_temp = make_ssa_name (vect_ptr, vec_stmt);\n+  TREE_OPERAND (vec_stmt, 0) = new_temp;\n+  bsi_insert_before (bsi, vec_stmt, BSI_SAME_STMT);\n+  data_ref_ptr = TREE_OPERAND (vec_stmt, 0);\n+\n+  return data_ref_ptr;\n+}\n+\n+\n+/* Function vect_create_destination_var.\n+\n+   Create a new temporary of type VECTYPE.  */\n+\n+static tree\n+vect_create_destination_var (tree scalar_dest, tree vectype)\n+{\n+  tree vec_dest;\n+  const char *new_name;\n+\n+  gcc_assert (TREE_CODE (scalar_dest) == SSA_NAME);\n+\n+  new_name = get_name (scalar_dest);\n+  if (!new_name)\n+    new_name = \"var_\";\n+  vec_dest = vect_get_new_vect_var (vectype, vect_simple_var, new_name);\n+  add_referenced_tmp_var (vec_dest);\n+\n+  return vec_dest;\n+}\n+\n+\n+/* Function vect_init_vector.\n+\n+   Insert a new stmt (INIT_STMT) that initializes a new vector variable with\n+   the vector elements of VECTOR_VAR. Return the DEF of INIT_STMT. It will be\n+   used in the vectorization of STMT.  */\n+\n+static tree\n+vect_init_vector (tree stmt, tree vector_var)\n+{\n+  stmt_vec_info stmt_vinfo = vinfo_for_stmt (stmt);\n+  loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_vinfo);\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  tree new_var;\n+  tree init_stmt;\n+  tree vectype = STMT_VINFO_VECTYPE (stmt_vinfo); \n+  tree vec_oprnd;\n+  edge pe;\n+  tree new_temp;\n+  basic_block new_bb;\n+ \n+  new_var = vect_get_new_vect_var (vectype, vect_simple_var, \"cst_\");\n+  add_referenced_tmp_var (new_var); \n+ \n+  init_stmt = build2 (MODIFY_EXPR, vectype, new_var, vector_var);\n+  new_temp = make_ssa_name (new_var, init_stmt);\n+  TREE_OPERAND (init_stmt, 0) = new_temp;\n+\n+  pe = loop_preheader_edge (loop);\n+  new_bb = bsi_insert_on_edge_immediate (pe, init_stmt);\n+  gcc_assert (!new_bb);\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    {\n+      fprintf (vect_dump, \"created new init_stmt: \");\n+      print_generic_expr (vect_dump, init_stmt, TDF_SLIM);\n+    }\n+\n+  vec_oprnd = TREE_OPERAND (init_stmt, 0);\n+  return vec_oprnd;\n+}\n+\n+\n+/* Function vect_get_vec_def_for_operand.\n+\n+   OP is an operand in STMT. This function returns a (vector) def that will be\n+   used in the vectorized stmt for STMT.\n+\n+   In the case that OP is an SSA_NAME which is defined in the loop, then\n+   STMT_VINFO_VEC_STMT of the defining stmt holds the relevant def.\n+\n+   In case OP is an invariant or constant, a new stmt that creates a vector def\n+   needs to be introduced.  */\n+\n+static tree\n+vect_get_vec_def_for_operand (tree op, tree stmt)\n+{\n+  tree vec_oprnd;\n+  tree vec_stmt;\n+  tree def_stmt;\n+  stmt_vec_info def_stmt_info = NULL;\n+  stmt_vec_info stmt_vinfo = vinfo_for_stmt (stmt);\n+  tree vectype = STMT_VINFO_VECTYPE (stmt_vinfo);\n+  int nunits = GET_MODE_NUNITS (TYPE_MODE (vectype));\n+  loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_vinfo);\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  basic_block bb;\n+  tree vec_inv;\n+  tree t = NULL_TREE;\n+  tree def;\n+  int i;\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    {\n+      fprintf (vect_dump, \"vect_get_vec_def_for_operand: \");\n+      print_generic_expr (vect_dump, op, TDF_SLIM);\n+    }\n+\n+  /** ===> Case 1: operand is a constant.  **/\n+\n+  if (TREE_CODE (op) == INTEGER_CST || TREE_CODE (op) == REAL_CST)\n+    {\n+      /* Create 'vect_cst_ = {cst,cst,...,cst}'  */\n+\n+      tree vec_cst;\n+\n+      /* Build a tree with vector elements.  */\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+        fprintf (vect_dump, \"Create vector_cst. nunits = %d\", nunits);\n+\n+      for (i = nunits - 1; i >= 0; --i)\n+        {\n+          t = tree_cons (NULL_TREE, op, t);\n+        }\n+      vec_cst = build_vector (vectype, t);\n+      return vect_init_vector (stmt, vec_cst);\n+    }\n+\n+  gcc_assert (TREE_CODE (op) == SSA_NAME);\n+ \n+  /** ===> Case 2: operand is an SSA_NAME - find the stmt that defines it.  **/\n+\n+  def_stmt = SSA_NAME_DEF_STMT (op);\n+  def_stmt_info = vinfo_for_stmt (def_stmt);\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    {\n+      fprintf (vect_dump, \"vect_get_vec_def_for_operand: def_stmt: \");\n+      print_generic_expr (vect_dump, def_stmt, TDF_SLIM);\n+    }\n+\n+\n+  /** ==> Case 2.1: operand is defined inside the loop.  **/\n+\n+  if (def_stmt_info)\n+    {\n+      /* Get the def from the vectorized stmt.  */\n+\n+      vec_stmt = STMT_VINFO_VEC_STMT (def_stmt_info);\n+      gcc_assert (vec_stmt);\n+      vec_oprnd = TREE_OPERAND (vec_stmt, 0);\n+      return vec_oprnd;\n+    }\n+\n+\n+  /** ==> Case 2.2: operand is defined by the loop-header phi-node - \n+                    it is a reduction/induction.  **/\n+\n+  bb = bb_for_stmt (def_stmt);\n+  if (TREE_CODE (def_stmt) == PHI_NODE && flow_bb_inside_loop_p (loop, bb))\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\tfprintf (vect_dump, \"reduction/induction - unsupported.\");\n+      internal_error (\"no support for reduction/induction\"); /* FORNOW */\n+    }\n+\n+\n+  /** ==> Case 2.3: operand is defined outside the loop - \n+                    it is a loop invariant.  */\n+\n+  switch (TREE_CODE (def_stmt))\n+    {\n+    case PHI_NODE:\n+      def = PHI_RESULT (def_stmt);\n+      break;\n+    case MODIFY_EXPR:\n+      def = TREE_OPERAND (def_stmt, 0);\n+      break;\n+    case NOP_EXPR:\n+      def = TREE_OPERAND (def_stmt, 0);\n+      gcc_assert (IS_EMPTY_STMT (def_stmt));\n+      def = op;\n+      break;\n+    default:\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t{\n+          fprintf (vect_dump, \"unsupported defining stmt: \");\n+\t  print_generic_expr (vect_dump, def_stmt, TDF_SLIM);\n+\t}\n+      internal_error (\"unsupported defining stmt\");\n+    }\n+\n+  /* Build a tree with vector elements.\n+     Create 'vec_inv = {inv,inv,..,inv}'  */\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"Create vector_inv.\");\n+\n+  for (i = nunits - 1; i >= 0; --i)\n+    {\n+      t = tree_cons (NULL_TREE, def, t);\n+    }\n+\n+  vec_inv = build_constructor (vectype, t);\n+  return vect_init_vector (stmt, vec_inv);\n+}\n+\n+\n+/* Function vect_finish_stmt_generation.\n+\n+   Insert a new stmt.  */\n+\n+static void\n+vect_finish_stmt_generation (tree stmt, tree vec_stmt, block_stmt_iterator *bsi)\n+{\n+  bsi_insert_before (bsi, vec_stmt, BSI_SAME_STMT);\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    {\n+      fprintf (vect_dump, \"add new stmt: \");\n+      print_generic_expr (vect_dump, vec_stmt, TDF_SLIM);\n+    }\n+\n+#ifdef ENABLE_CHECKING\n+  /* Make sure bsi points to the stmt that is being vectorized.  */\n+  gcc_assert (stmt == bsi_stmt (*bsi));\n+#endif\n+\n+#ifdef USE_MAPPED_LOCATION\n+  SET_EXPR_LOCATION (vec_stmt, EXPR_LOCUS (stmt));\n+#else\n+  SET_EXPR_LOCUS (vec_stmt, EXPR_LOCUS (stmt));\n+#endif\n+}\n+\n+\n+/* Function vectorizable_assignment.\n+\n+   Check if STMT performs an assignment (copy) that can be vectorized. \n+   If VEC_STMT is also passed, vectorize the STMT: create a vectorized \n+   stmt to replace it, put it in VEC_STMT, and insert it at BSI.\n+   Return FALSE if not a vectorizable STMT, TRUE otherwise.  */\n+\n+bool\n+vectorizable_assignment (tree stmt, block_stmt_iterator *bsi, tree *vec_stmt)\n+{\n+  tree vec_dest;\n+  tree scalar_dest;\n+  tree op;\n+  tree vec_oprnd;\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n+  loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n+  tree new_temp;\n+\n+  /* Is vectorizable assignment?  */\n+\n+  if (TREE_CODE (stmt) != MODIFY_EXPR)\n+    return false;\n+\n+  scalar_dest = TREE_OPERAND (stmt, 0);\n+  if (TREE_CODE (scalar_dest) != SSA_NAME)\n+    return false;\n+\n+  op = TREE_OPERAND (stmt, 1);\n+  if (!vect_is_simple_use (op, loop_vinfo, NULL))\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+        fprintf (vect_dump, \"use not simple.\");\n+      return false;\n+    }\n+\n+  if (!vec_stmt) /* transformation not required.  */\n+    {\n+      STMT_VINFO_TYPE (stmt_info) = assignment_vec_info_type;\n+      return true;\n+    }\n+\n+  /** Transform.  **/\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"transform assignment.\");\n+\n+  /* Handle def.  */\n+  vec_dest = vect_create_destination_var (scalar_dest, vectype);\n+\n+  /* Handle use.  */\n+  op = TREE_OPERAND (stmt, 1);\n+  vec_oprnd = vect_get_vec_def_for_operand (op, stmt);\n+\n+  /* Arguments are ready. create the new vector stmt.  */\n+  *vec_stmt = build2 (MODIFY_EXPR, vectype, vec_dest, vec_oprnd);\n+  new_temp = make_ssa_name (vec_dest, *vec_stmt);\n+  TREE_OPERAND (*vec_stmt, 0) = new_temp;\n+  vect_finish_stmt_generation (stmt, *vec_stmt, bsi);\n+  \n+  return true;\n+}\n+\n+\n+/* Function vectorizable_operation.\n+\n+   Check if STMT performs a binary or unary operation that can be vectorized. \n+   If VEC_STMT is also passed, vectorize the STMT: create a vectorized \n+   stmt to replace it, put it in VEC_STMT, and insert it at BSI.\n+   Return FALSE if not a vectorizable STMT, TRUE otherwise.  */\n+\n+bool\n+vectorizable_operation (tree stmt, block_stmt_iterator *bsi, tree *vec_stmt)\n+{\n+  tree vec_dest;\n+  tree scalar_dest;\n+  tree operation;\n+  tree op0, op1 = NULL;\n+  tree vec_oprnd0, vec_oprnd1=NULL;\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n+  loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n+  int i;\n+  enum tree_code code;\n+  enum machine_mode vec_mode;\n+  tree new_temp;\n+  int op_type;\n+  tree op;\n+  optab optab;\n+\n+  /* Is STMT a vectorizable binary/unary operation?   */\n+  if (TREE_CODE (stmt) != MODIFY_EXPR)\n+    return false;\n+\n+  if (TREE_CODE (TREE_OPERAND (stmt, 0)) != SSA_NAME)\n+    return false;\n+\n+  operation = TREE_OPERAND (stmt, 1);\n+  code = TREE_CODE (operation);\n+  optab = optab_for_tree_code (code, vectype);\n+\n+  /* Support only unary or binary operations.  */\n+  op_type = TREE_CODE_LENGTH (code);\n+  if (op_type != unary_op && op_type != binary_op)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\tfprintf (vect_dump, \"num. args = %d (not unary/binary op).\", op_type);\n+      return false;\n+    }\n+\n+  for (i = 0; i < op_type; i++)\n+    {\n+      op = TREE_OPERAND (operation, i);\n+      if (!vect_is_simple_use (op, loop_vinfo, NULL))\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    fprintf (vect_dump, \"use not simple.\");\n+\t  return false;\n+\t}\t\n+    } \n+\n+  /* Supportable by target?  */\n+  if (!optab)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\tfprintf (vect_dump, \"no optab.\");\n+      return false;\n+    }\n+  vec_mode = TYPE_MODE (vectype);\n+  if (optab->handlers[(int) vec_mode].insn_code == CODE_FOR_nothing)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\tfprintf (vect_dump, \"op not supported by target.\");\n+      return false;\n+    }\n+\n+  if (!vec_stmt) /* transformation not required.  */\n+    {\n+      STMT_VINFO_TYPE (stmt_info) = op_vec_info_type;\n+      return true;\n+    }\n+\n+  /** Transform.  **/\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"transform binary/unary operation.\");\n+\n+  /* Handle def.  */\n+  scalar_dest = TREE_OPERAND (stmt, 0);\n+  vec_dest = vect_create_destination_var (scalar_dest, vectype);\n+\n+  /* Handle uses.  */\n+  op0 = TREE_OPERAND (operation, 0);\n+  vec_oprnd0 = vect_get_vec_def_for_operand (op0, stmt);\n+\n+  if (op_type == binary_op)\n+    {\n+      op1 = TREE_OPERAND (operation, 1);\n+      vec_oprnd1 = vect_get_vec_def_for_operand (op1, stmt); \n+    }\n+\n+  /* Arguments are ready. create the new vector stmt.  */\n+\n+  if (op_type == binary_op)\n+    *vec_stmt = build2 (MODIFY_EXPR, vectype, vec_dest,\n+\t\tbuild2 (code, vectype, vec_oprnd0, vec_oprnd1));\n+  else\n+    *vec_stmt = build2 (MODIFY_EXPR, vectype, vec_dest,\n+\t\tbuild1 (code, vectype, vec_oprnd0));\n+  new_temp = make_ssa_name (vec_dest, *vec_stmt);\n+  TREE_OPERAND (*vec_stmt, 0) = new_temp;\n+  vect_finish_stmt_generation (stmt, *vec_stmt, bsi);\n+\n+  return true;\n+}\n+\n+\n+/* Function vectorizable_store.\n+\n+   Check if STMT defines a non scalar data-ref (array/pointer/structure) that \n+   can be vectorized. \n+   If VEC_STMT is also passed, vectorize the STMT: create a vectorized \n+   stmt to replace it, put it in VEC_STMT, and insert it at BSI.\n+   Return FALSE if not a vectorizable STMT, TRUE otherwise.  */\n+\n+bool\n+vectorizable_store (tree stmt, block_stmt_iterator *bsi, tree *vec_stmt)\n+{\n+  tree scalar_dest;\n+  tree data_ref;\n+  tree op;\n+  tree vec_oprnd1;\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n+  tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n+  loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n+  enum machine_mode vec_mode;\n+  tree dummy;\n+  enum dr_alignment_support alignment_support_cheme;\n+\n+  /* Is vectorizable store? */\n+\n+  if (TREE_CODE (stmt) != MODIFY_EXPR)\n+    return false;\n+\n+  scalar_dest = TREE_OPERAND (stmt, 0);\n+  if (TREE_CODE (scalar_dest) != ARRAY_REF\n+      && TREE_CODE (scalar_dest) != INDIRECT_REF)\n+    return false;\n+\n+  op = TREE_OPERAND (stmt, 1);\n+  if (!vect_is_simple_use (op, loop_vinfo, NULL))\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+        fprintf (vect_dump, \"use not simple.\");\n+      return false;\n+    }\n+\n+  vec_mode = TYPE_MODE (vectype);\n+  /* FORNOW. In some cases can vectorize even if data-type not supported\n+     (e.g. - array initialization with 0).  */\n+  if (mov_optab->handlers[(int)vec_mode].insn_code == CODE_FOR_nothing)\n+    return false;\n+\n+  if (!STMT_VINFO_DATA_REF (stmt_info))\n+    return false;\n+\n+\n+  if (!vec_stmt) /* transformation not required.  */\n+    {\n+      STMT_VINFO_TYPE (stmt_info) = store_vec_info_type;\n+      return true;\n+    }\n+\n+  /** Transform.  **/\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"transform store\");\n+\n+  alignment_support_cheme = vect_supportable_dr_alignment (dr);\n+  gcc_assert (alignment_support_cheme);\n+  gcc_assert (alignment_support_cheme = dr_aligned);  /* FORNOW */\n+\n+  /* Handle use - get the vectorized def from the defining stmt.  */\n+  vec_oprnd1 = vect_get_vec_def_for_operand (op, stmt);\n+\n+  /* Handle def.  */\n+  /* FORNOW: make sure the data reference is aligned.  */\n+  vect_align_data_ref (stmt);\n+  data_ref = vect_create_data_ref_ptr (stmt, bsi, NULL_TREE, &dummy, false);\n+  data_ref = build_fold_indirect_ref (data_ref);\n+\n+  /* Arguments are ready. create the new vector stmt.  */\n+  *vec_stmt = build2 (MODIFY_EXPR, vectype, data_ref, vec_oprnd1);\n+  vect_finish_stmt_generation (stmt, *vec_stmt, bsi);\n+\n+  return true;\n+}\n+\n+\n+/* vectorizable_load.\n+\n+   Check if STMT reads a non scalar data-ref (array/pointer/structure) that \n+   can be vectorized. \n+   If VEC_STMT is also passed, vectorize the STMT: create a vectorized \n+   stmt to replace it, put it in VEC_STMT, and insert it at BSI.\n+   Return FALSE if not a vectorizable STMT, TRUE otherwise.  */\n+\n+bool\n+vectorizable_load (tree stmt, block_stmt_iterator *bsi, tree *vec_stmt)\n+{\n+  tree scalar_dest;\n+  tree vec_dest = NULL;\n+  tree data_ref = NULL;\n+  tree op;\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n+  tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n+  tree new_temp;\n+  int mode;\n+  tree init_addr;\n+  tree new_stmt;\n+  tree dummy;\n+  basic_block new_bb;\n+  loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  edge pe = loop_preheader_edge (loop);\n+  enum dr_alignment_support alignment_support_cheme;\n+\n+  /* Is vectorizable load? */\n+\n+  if (TREE_CODE (stmt) != MODIFY_EXPR)\n+    return false;\n+\n+  scalar_dest = TREE_OPERAND (stmt, 0);\n+  if (TREE_CODE (scalar_dest) != SSA_NAME)\n+    return false;\n+\n+  op = TREE_OPERAND (stmt, 1);\n+  if (TREE_CODE (op) != ARRAY_REF && TREE_CODE (op) != INDIRECT_REF)\n+    return false;\n+\n+  if (!STMT_VINFO_DATA_REF (stmt_info))\n+    return false;\n+\n+  mode = (int) TYPE_MODE (vectype);\n+\n+  /* FORNOW. In some cases can vectorize even if data-type not supported\n+    (e.g. - data copies).  */\n+  if (mov_optab->handlers[mode].insn_code == CODE_FOR_nothing)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS, LOOP_LOC (loop_vinfo)))\n+\tfprintf (vect_dump, \"Aligned load, but unsupported type.\");\n+      return false;\n+    }\n+\n+  if (!vec_stmt) /* transformation not required.  */\n+    {\n+      STMT_VINFO_TYPE (stmt_info) = load_vec_info_type;\n+      return true;\n+    }\n+\n+  /** Transform.  **/\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"transform load.\");\n+\n+  alignment_support_cheme = vect_supportable_dr_alignment (dr);\n+  gcc_assert (alignment_support_cheme);\n+\n+  if (alignment_support_cheme == dr_aligned\n+      || alignment_support_cheme == dr_unaligned_supported)\n+    {\n+      /* Create:\n+         p = initial_addr;\n+         indx = 0;\n+         loop {\n+           vec_dest = *(p);\n+           indx = indx + 1;\n+         }\n+      */\n+\n+      vec_dest = vect_create_destination_var (scalar_dest, vectype);\n+      data_ref = vect_create_data_ref_ptr (stmt, bsi, NULL_TREE, &dummy, false);\n+      if (aligned_access_p (dr))\n+        data_ref = build_fold_indirect_ref (data_ref);\n+      else\n+\t{\n+\t  int mis = DR_MISALIGNMENT (dr);\n+\t  tree tmis = (mis == -1 ? size_zero_node : size_int (mis));\n+\t  tmis = size_binop (MULT_EXPR, tmis, size_int(BITS_PER_UNIT));\n+\t  data_ref = build2 (MISALIGNED_INDIRECT_REF, vectype, data_ref, tmis);\n+\t}\n+      new_stmt = build2 (MODIFY_EXPR, vectype, vec_dest, data_ref);\n+      new_temp = make_ssa_name (vec_dest, new_stmt);\n+      TREE_OPERAND (new_stmt, 0) = new_temp;\n+      vect_finish_stmt_generation (stmt, new_stmt, bsi);\n+    }\n+  else if (alignment_support_cheme == dr_unaligned_software_pipeline)\n+    {\n+      /* Create:\n+\t p1 = initial_addr;\n+\t msq_init = *(floor(p1))\n+\t p2 = initial_addr + VS - 1;\n+\t magic = have_builtin ? builtin_result : initial_address;\n+\t indx = 0;\n+\t loop {\n+\t   p2' = p2 + indx * vectype_size\n+\t   lsq = *(floor(p2'))\n+\t   vec_dest = realign_load (msq, lsq, magic)\n+\t   indx = indx + 1;\n+\t   msq = lsq;\n+\t }\n+      */\n+\n+      tree offset;\n+      tree magic;\n+      tree phi_stmt;\n+      tree msq_init;\n+      tree msq, lsq;\n+      tree dataref_ptr;\n+      tree params;\n+\n+      /* <1> Create msq_init = *(floor(p1)) in the loop preheader  */\n+      vec_dest = vect_create_destination_var (scalar_dest, vectype);\n+      data_ref = vect_create_data_ref_ptr (stmt, bsi, NULL_TREE, \n+\t\t\t\t\t   &init_addr, true);\n+      data_ref = build1 (ALIGN_INDIRECT_REF, vectype, data_ref);\n+      new_stmt = build2 (MODIFY_EXPR, vectype, vec_dest, data_ref);\n+      new_temp = make_ssa_name (vec_dest, new_stmt);\n+      TREE_OPERAND (new_stmt, 0) = new_temp;\n+      new_bb = bsi_insert_on_edge_immediate (pe, new_stmt);\n+      gcc_assert (!new_bb);\n+      msq_init = TREE_OPERAND (new_stmt, 0);\n+\n+\n+      /* <2> Create lsq = *(floor(p2')) in the loop  */ \n+      offset = build_int_cst (integer_type_node, \n+\t\t\t      GET_MODE_NUNITS (TYPE_MODE (vectype)));\n+      offset = int_const_binop (MINUS_EXPR, offset, integer_one_node, 1);\n+      vec_dest = vect_create_destination_var (scalar_dest, vectype);\n+      dataref_ptr = vect_create_data_ref_ptr (stmt, bsi, offset, &dummy, false);\n+      data_ref = build1 (ALIGN_INDIRECT_REF, vectype, dataref_ptr);\n+      new_stmt = build2 (MODIFY_EXPR, vectype, vec_dest, data_ref);\n+      new_temp = make_ssa_name (vec_dest, new_stmt);\n+      TREE_OPERAND (new_stmt, 0) = new_temp;\n+      vect_finish_stmt_generation (stmt, new_stmt, bsi);\n+      lsq = TREE_OPERAND (new_stmt, 0);\n+\n+\n+      /* <3> */\n+      if (targetm.vectorize.builtin_mask_for_load)\n+\t{\n+\t  /* Create permutation mask, if required, in loop preheader.  */\n+\t  tree builtin_decl;\n+\t  params = build_tree_list (NULL_TREE, init_addr);\n+\t  vec_dest = vect_create_destination_var (scalar_dest, vectype);\n+\t  builtin_decl = targetm.vectorize.builtin_mask_for_load ();\n+\t  new_stmt = build_function_call_expr (builtin_decl, params);\n+\t  new_stmt = build2 (MODIFY_EXPR, vectype, vec_dest, new_stmt);\n+\t  new_temp = make_ssa_name (vec_dest, new_stmt);\n+\t  TREE_OPERAND (new_stmt, 0) = new_temp;\n+\t  new_bb = bsi_insert_on_edge_immediate (pe, new_stmt);\n+\t  gcc_assert (!new_bb);\n+\t  magic = TREE_OPERAND (new_stmt, 0);\n+\n+\t  /* Since we have just created a CALL_EXPR, we may need to\n+\t     rename call-clobbered variables.  */\n+\t  mark_call_clobbered_vars_to_rename ();\n+\t}\n+      else\n+\t{\n+\t  /* Use current address instead of init_addr for reduced reg pressure.\n+\t   */\n+\t  magic = dataref_ptr;\n+\t}\n+\n+\n+      /* <4> Create msq = phi <msq_init, lsq> in loop  */ \n+      vec_dest = vect_create_destination_var (scalar_dest, vectype);\n+      msq = make_ssa_name (vec_dest, NULL_TREE);\n+      phi_stmt = create_phi_node (msq, loop->header); /* CHECKME */\n+      SSA_NAME_DEF_STMT (msq) = phi_stmt;\n+      add_phi_arg (phi_stmt, msq_init, loop_preheader_edge (loop));\n+      add_phi_arg (phi_stmt, lsq, loop_latch_edge (loop));\n+\n+\n+      /* <5> Create <vec_dest = realign_load (msq, lsq, magic)> in loop  */\n+      vec_dest = vect_create_destination_var (scalar_dest, vectype);\n+      new_stmt = build3 (REALIGN_LOAD_EXPR, vectype, msq, lsq, magic);\n+      new_stmt = build2 (MODIFY_EXPR, vectype, vec_dest, new_stmt);\n+      new_temp = make_ssa_name (vec_dest, new_stmt); \n+      TREE_OPERAND (new_stmt, 0) = new_temp;\n+      vect_finish_stmt_generation (stmt, new_stmt, bsi);\n+    }\n+  else\n+    gcc_unreachable ();\n+\n+  *vec_stmt = new_stmt;\n+  return true;\n+}\n+\n+\n+/* Function vect_transform_stmt.\n+\n+   Create a vectorized stmt to replace STMT, and insert it at BSI.  */\n+\n+bool\n+vect_transform_stmt (tree stmt, block_stmt_iterator *bsi)\n+{\n+  bool is_store = false;\n+  tree vec_stmt = NULL_TREE;\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  bool done;\n+\n+  switch (STMT_VINFO_TYPE (stmt_info))\n+    {\n+    case op_vec_info_type:\n+      done = vectorizable_operation (stmt, bsi, &vec_stmt);\n+      gcc_assert (done);\n+      break;\n+\n+    case assignment_vec_info_type:\n+      done = vectorizable_assignment (stmt, bsi, &vec_stmt);\n+      gcc_assert (done);\n+      break;\n+\n+    case load_vec_info_type:\n+      done = vectorizable_load (stmt, bsi, &vec_stmt);\n+      gcc_assert (done);\n+      break;\n+\n+    case store_vec_info_type:\n+      done = vectorizable_store (stmt, bsi, &vec_stmt);\n+      gcc_assert (done);\n+      is_store = true;\n+      break;\n+    default:\n+      if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+        fprintf (vect_dump, \"stmt not supported.\");\n+      gcc_unreachable ();\n+    }\n+\n+  STMT_VINFO_VEC_STMT (stmt_info) = vec_stmt;\n+\n+  return is_store;\n+}\n+\n+\n+/* This function builds ni_name = number of iterations loop executes\n+   on the loop preheader.  */\n+\n+static tree\n+vect_build_loop_niters (loop_vec_info loop_vinfo)\n+{\n+  tree ni_name, stmt, var;\n+  edge pe;\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  tree ni = unshare_expr (LOOP_VINFO_NITERS (loop_vinfo));\n+\n+  var = create_tmp_var (TREE_TYPE (ni), \"niters\");\n+  add_referenced_tmp_var (var);\n+  ni_name = force_gimple_operand (ni, &stmt, false, var);\n+\n+  pe = loop_preheader_edge (loop);\n+  if (stmt)\n+    {\n+      basic_block new_bb = bsi_insert_on_edge_immediate (pe, stmt);\n+      gcc_assert (!new_bb);\n+    }\n+      \n+  return ni_name;\n+}\n+\n+\n+/* This function generates the following statements:\n+\n+ ni_name = number of iterations loop executes\n+ ratio = ni_name / vf\n+ ratio_mult_vf_name = ratio * vf\n+\n+ and places them at the loop preheader edge.  */\n+\n+static void \n+vect_generate_tmps_on_preheader (loop_vec_info loop_vinfo, \n+\t\t\t\t tree *ni_name_ptr,\n+\t\t\t\t tree *ratio_mult_vf_name_ptr, \n+\t\t\t\t tree *ratio_name_ptr)\n+{\n+\n+  edge pe;\n+  basic_block new_bb;\n+  tree stmt, ni_name;\n+  tree var;\n+  tree ratio_name;\n+  tree ratio_mult_vf_name;\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  tree ni = LOOP_VINFO_NITERS (loop_vinfo);\n+  int vf = LOOP_VINFO_VECT_FACTOR (loop_vinfo);\n+  tree log_vf = build_int_cst (unsigned_type_node, exact_log2 (vf));\n+\n+  pe = loop_preheader_edge (loop);\n+\n+  /* Generate temporary variable that contains \n+     number of iterations loop executes.  */\n+\n+  ni_name = vect_build_loop_niters (loop_vinfo);\n+\n+  /* Create: ratio = ni >> log2(vf) */\n+\n+  var = create_tmp_var (TREE_TYPE (ni), \"bnd\");\n+  add_referenced_tmp_var (var);\n+  ratio_name = make_ssa_name (var, NULL_TREE);\n+  stmt = build2 (MODIFY_EXPR, void_type_node, ratio_name,\n+\t   build2 (RSHIFT_EXPR, TREE_TYPE (ni_name), ni_name, log_vf));\n+  SSA_NAME_DEF_STMT (ratio_name) = stmt;\n+\n+  pe = loop_preheader_edge (loop);\n+  new_bb = bsi_insert_on_edge_immediate (pe, stmt);\n+  gcc_assert (!new_bb);\n+       \n+  /* Create: ratio_mult_vf = ratio << log2 (vf).  */\n+\n+  var = create_tmp_var (TREE_TYPE (ni), \"ratio_mult_vf\");\n+  add_referenced_tmp_var (var);\n+  ratio_mult_vf_name = make_ssa_name (var, NULL_TREE);\n+  stmt = build2 (MODIFY_EXPR, void_type_node, ratio_mult_vf_name,\n+\t   build2 (LSHIFT_EXPR, TREE_TYPE (ratio_name), ratio_name, log_vf));\n+  SSA_NAME_DEF_STMT (ratio_mult_vf_name) = stmt;\n+\n+  pe = loop_preheader_edge (loop);\n+  new_bb = bsi_insert_on_edge_immediate (pe, stmt);\n+  gcc_assert (!new_bb);\n+\n+  *ni_name_ptr = ni_name;\n+  *ratio_mult_vf_name_ptr = ratio_mult_vf_name;\n+  *ratio_name_ptr = ratio_name;\n+    \n+  return;  \n+}\n+\n+\n+/*   Function vect_update_ivs_after_vectorizer.\n+\n+     \"Advance\" the induction variables of LOOP to the value they should take\n+     after the execution of LOOP.  This is currently necessary because the\n+     vectorizer does not handle induction variables that are used after the\n+     loop.  Such a situation occurs when the last iterations of LOOP are\n+     peeled, because:\n+     1. We introduced new uses after LOOP for IVs that were not originally used\n+        after LOOP: the IVs of LOOP are now used by an epilog loop.\n+     2. LOOP is going to be vectorized; this means that it will iterate N/VF\n+        times, whereas the loop IVs should be bumped N times.\n+\n+     Input:\n+     - LOOP - a loop that is going to be vectorized. The last few iterations\n+              of LOOP were peeled.\n+     - NITERS - the number of iterations that LOOP executes (before it is\n+                vectorized). i.e, the number of times the ivs should be bumped.\n+     - UPDATE_E - a successor edge of LOOP->exit that is on the (only) path\n+                  coming out from LOOP on which there are uses of the LOOP ivs\n+\t\t  (this is the path from LOOP->exit to epilog_loop->preheader).\n+\n+                  The new definitions of the ivs are placed in LOOP->exit.\n+                  The phi args associated with the edge UPDATE_E in the bb\n+                  UPDATE_E->dest are updated accordingly.\n+\n+     Assumption 1: Like the rest of the vectorizer, this function assumes\n+     a single loop exit that has a single predecessor.\n+\n+     Assumption 2: The phi nodes in the LOOP header and in update_bb are\n+     organized in the same order.\n+\n+     Assumption 3: The access function of the ivs is simple enough (see\n+     vect_can_advance_ivs_p).  This assumption will be relaxed in the future.\n+\n+     Assumption 4: Exactly one of the successors of LOOP exit-bb is on a path\n+     coming out of LOOP on which the ivs of LOOP are used (this is the path \n+     that leads to the epilog loop; other paths skip the epilog loop).  This\n+     path starts with the edge UPDATE_E, and its destination (denoted update_bb)\n+     needs to have its phis updated.\n+ */\n+\n+static void\n+vect_update_ivs_after_vectorizer (loop_vec_info loop_vinfo, tree niters, \n+\t\t\t\t  edge update_e)\n+{\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  basic_block exit_bb = loop->exit_edges[0]->dest;\n+  tree phi, phi1;\n+  basic_block update_bb = update_e->dest;\n+\n+  /* gcc_assert (vect_can_advance_ivs_p (loop_vinfo)); */\n+\n+  /* Make sure there exists a single-predecessor exit bb:  */\n+  gcc_assert (EDGE_COUNT (exit_bb->preds) == 1);\n+\n+  for (phi = phi_nodes (loop->header), phi1 = phi_nodes (update_bb); \n+       phi && phi1; \n+       phi = PHI_CHAIN (phi), phi1 = PHI_CHAIN (phi1))\n+    {\n+      tree access_fn = NULL;\n+      tree evolution_part;\n+      tree init_expr;\n+      tree step_expr;\n+      tree var, stmt, ni, ni_name;\n+      block_stmt_iterator last_bsi;\n+\n+      /* Skip virtual phi's.  */\n+      if (!is_gimple_reg (SSA_NAME_VAR (PHI_RESULT (phi))))\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    fprintf (vect_dump, \"virtual phi. skip.\");\n+\t  continue;\n+\t}\n+\n+      access_fn = analyze_scalar_evolution (loop, PHI_RESULT (phi)); \n+      gcc_assert (access_fn);\n+      evolution_part =\n+\t unshare_expr (evolution_part_in_loop_num (access_fn, loop->num));\n+      gcc_assert (evolution_part != NULL_TREE);\n+      \n+      /* FORNOW: We do not support IVs whose evolution function is a polynomial\n+         of degree >= 2 or exponential.  */\n+      gcc_assert (!tree_is_chrec (evolution_part));\n+\n+      step_expr = evolution_part;\n+      init_expr = unshare_expr (initial_condition_in_loop_num (access_fn, \n+\t\t\t\t\t\t\t       loop->num));\n+\n+      ni = build2 (PLUS_EXPR, TREE_TYPE (init_expr),\n+\t\t  build2 (MULT_EXPR, TREE_TYPE (niters),\n+\t\t       niters, step_expr), init_expr);\n+\n+      var = create_tmp_var (TREE_TYPE (init_expr), \"tmp\");\n+      add_referenced_tmp_var (var);\n+\n+      ni_name = force_gimple_operand (ni, &stmt, false, var);\n+      \n+      /* Insert stmt into exit_bb.  */\n+      last_bsi = bsi_last (exit_bb);\n+      if (stmt)\n+        bsi_insert_before (&last_bsi, stmt, BSI_SAME_STMT);   \n+\n+      /* Fix phi expressions in the successor bb.  */\n+      gcc_assert (PHI_ARG_DEF_FROM_EDGE (phi1, update_e) ==\n+                  PHI_ARG_DEF_FROM_EDGE (phi, EDGE_SUCC (loop->latch, 0)));\n+      SET_PHI_ARG_DEF (phi1, update_e->dest_idx, ni_name);\n+    }\n+}\n+\n+\n+/* Function vect_do_peeling_for_loop_bound\n+\n+   Peel the last iterations of the loop represented by LOOP_VINFO.\n+   The peeled iterations form a new epilog loop.  Given that the loop now \n+   iterates NITERS times, the new epilog loop iterates\n+   NITERS % VECTORIZATION_FACTOR times.\n+   \n+   The original loop will later be made to iterate \n+   NITERS / VECTORIZATION_FACTOR times (this value is placed into RATIO).  */\n+\n+static void \n+vect_do_peeling_for_loop_bound (loop_vec_info loop_vinfo, tree *ratio,\n+\t\t\t\tstruct loops *loops)\n+{\n+\n+  tree ni_name, ratio_mult_vf_name;\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  struct loop *new_loop;\n+  edge update_e;\n+#ifdef ENABLE_CHECKING\n+  int loop_num;\n+#endif\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"=== vect_transtorm_for_unknown_loop_bound ===\");\n+\n+  /* Generate the following variables on the preheader of original loop:\n+\t \n+     ni_name = number of iteration the original loop executes\n+     ratio = ni_name / vf\n+     ratio_mult_vf_name = ratio * vf  */\n+  vect_generate_tmps_on_preheader (loop_vinfo, &ni_name,\n+\t\t\t\t   &ratio_mult_vf_name, ratio);\n+\n+  /* Update loop info.  */\n+  loop->pre_header = loop_preheader_edge (loop)->src;\n+  loop->pre_header_edges[0] = loop_preheader_edge (loop);\n+\n+#ifdef ENABLE_CHECKING\n+  loop_num  = loop->num; \n+#endif\n+  new_loop = slpeel_tree_peel_loop_to_edge (loop, loops, loop->exit_edges[0],\n+\t\t\t\t\t    ratio_mult_vf_name, ni_name, false);\n+#ifdef ENABLE_CHECKING\n+  gcc_assert (new_loop);\n+  gcc_assert (loop_num == loop->num);\n+  slpeel_verify_cfg_after_peeling (loop, new_loop);\n+#endif\n+\n+  /* A guard that controls whether the new_loop is to be executed or skipped\n+     is placed in LOOP->exit.  LOOP->exit therefore has two successors - one\n+     is the preheader of NEW_LOOP, where the IVs from LOOP are used.  The other\n+     is a bb after NEW_LOOP, where these IVs are not used.  Find the edge that\n+     is on the path where the LOOP IVs are used and need to be updated.  */\n+\n+  if (EDGE_PRED (new_loop->pre_header, 0)->src == loop->exit_edges[0]->dest)\n+    update_e = EDGE_PRED (new_loop->pre_header, 0);\n+  else\n+    update_e = EDGE_PRED (new_loop->pre_header, 1);\n+\n+  /* Update IVs of original loop as if they were advanced \n+     by ratio_mult_vf_name steps.  */\n+  vect_update_ivs_after_vectorizer (loop_vinfo, ratio_mult_vf_name, update_e); \n+\n+  /* After peeling we have to reset scalar evolution analyzer.  */\n+  scev_reset ();\n+\n+  return;\n+}\n+\n+\n+/* Function vect_gen_niters_for_prolog_loop\n+\n+   Set the number of iterations for the loop represented by LOOP_VINFO\n+   to the minimum between LOOP_NITERS (the original iteration count of the loop)\n+   and the misalignment of DR - the first data reference recorded in\n+   LOOP_VINFO_UNALIGNED_DR (LOOP_VINFO).  As a result, after the execution of \n+   this loop, the data reference DR will refer to an aligned location.\n+\n+   The following computation is generated:\n+\n+   compute address misalignment in bytes:\n+   addr_mis = addr & (vectype_size - 1)\n+\n+   prolog_niters = min ( LOOP_NITERS , (VF - addr_mis/elem_size)&(VF-1) )\n+   \n+   (elem_size = element type size; an element is the scalar element \n+\twhose type is the inner type of the vectype)  */\n+\n+static tree \n+vect_gen_niters_for_prolog_loop (loop_vec_info loop_vinfo, tree loop_niters)\n+{\n+  struct data_reference *dr = LOOP_VINFO_UNALIGNED_DR (loop_vinfo);\n+  int vf = LOOP_VINFO_VECT_FACTOR (loop_vinfo);\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  tree var, stmt;\n+  tree iters, iters_name;\n+  edge pe;\n+  basic_block new_bb;\n+  tree dr_stmt = DR_STMT (dr);\n+  stmt_vec_info stmt_info = vinfo_for_stmt (dr_stmt);\n+  tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n+  int vectype_align = TYPE_ALIGN (vectype) / BITS_PER_UNIT;\n+  tree elem_misalign;\n+  tree byte_misalign;\n+  tree new_stmts = NULL_TREE;\n+  tree start_addr = \n+\tvect_create_addr_base_for_vector_ref (dr_stmt, &new_stmts, NULL_TREE);\n+  tree ptr_type = TREE_TYPE (start_addr);\n+  tree size = TYPE_SIZE (ptr_type);\n+  tree type = lang_hooks.types.type_for_size (tree_low_cst (size, 1), 1);\n+  tree vectype_size_minus_1 = build_int_cst (type, vectype_align - 1);\n+  tree vf_minus_1 = build_int_cst (unsigned_type_node, vf - 1);\n+  tree niters_type = TREE_TYPE (loop_niters);\n+  tree elem_size_log = \n+\tbuild_int_cst (unsigned_type_node, exact_log2 (vectype_align/vf));\n+  tree vf_tree = build_int_cst (unsigned_type_node, vf);\n+\n+  pe = loop_preheader_edge (loop); \n+  new_bb = bsi_insert_on_edge_immediate (pe, new_stmts); \n+  gcc_assert (!new_bb);\n+\n+  /* Create:  byte_misalign = addr & (vectype_size - 1)  */\n+  byte_misalign = build2 (BIT_AND_EXPR, type, start_addr, vectype_size_minus_1);\n+\n+  /* Create:  elem_misalign = byte_misalign / element_size  */\n+  elem_misalign = \n+\tbuild2 (RSHIFT_EXPR, unsigned_type_node, byte_misalign, elem_size_log);\n+  \n+  /* Create:  (niters_type) (VF - elem_misalign)&(VF - 1)  */\n+  iters = build2 (MINUS_EXPR, unsigned_type_node, vf_tree, elem_misalign);\n+  iters = build2 (BIT_AND_EXPR, unsigned_type_node, iters, vf_minus_1);\n+  iters = fold_convert (niters_type, iters);\n+  \n+  /* Create:  prolog_loop_niters = min (iters, loop_niters) */\n+  /* If the loop bound is known at compile time we already verified that it is\n+     greater than vf; since the misalignment ('iters') is at most vf, there's\n+     no need to generate the MIN_EXPR in this case.  */\n+  if (TREE_CODE (loop_niters) != INTEGER_CST)\n+    iters = build2 (MIN_EXPR, niters_type, iters, loop_niters);\n+\n+  var = create_tmp_var (niters_type, \"prolog_loop_niters\");\n+  add_referenced_tmp_var (var);\n+  iters_name = force_gimple_operand (iters, &stmt, false, var);\n+\n+  /* Insert stmt on loop preheader edge.  */\n+  pe = loop_preheader_edge (loop);\n+  if (stmt)\n+    {\n+      basic_block new_bb = bsi_insert_on_edge_immediate (pe, stmt);\n+      gcc_assert (!new_bb);\n+    }\n+\n+  return iters_name; \n+}\n+\n+\n+/* Function vect_update_inits_of_dr\n+\n+   NITERS iterations were peeled from LOOP.  DR represents a data reference\n+   in LOOP.  This function updates the information recorded in DR to\n+   account for the fact that the first NITERS iterations had already been \n+   executed.  Specifically, it updates the OFFSET field of stmt_info.  */\n+\n+static void\n+vect_update_inits_of_dr (struct data_reference *dr, tree niters)\n+{\n+  stmt_vec_info stmt_info = vinfo_for_stmt (DR_STMT (dr));\n+  tree offset = STMT_VINFO_VECT_INIT_OFFSET (stmt_info);\n+      \n+  niters = fold (build2 (MULT_EXPR, TREE_TYPE (niters), niters, \n+\t\t\t STMT_VINFO_VECT_STEP (stmt_info)));\n+  offset = fold (build2 (PLUS_EXPR, TREE_TYPE (offset), offset, niters));\n+  STMT_VINFO_VECT_INIT_OFFSET (stmt_info) = offset;\n+}\n+\n+\n+/* Function vect_update_inits_of_drs\n+\n+   NITERS iterations were peeled from the loop represented by LOOP_VINFO.  \n+   This function updates the information recorded for the data references in \n+   the loop to account for the fact that the first NITERS iterations had \n+   already been executed.  Specifically, it updates the initial_condition of the\n+   access_function of all the data_references in the loop.  */\n+\n+static void\n+vect_update_inits_of_drs (loop_vec_info loop_vinfo, tree niters)\n+{\n+  unsigned int i;\n+  varray_type loop_write_datarefs = LOOP_VINFO_DATAREF_WRITES (loop_vinfo);\n+  varray_type loop_read_datarefs = LOOP_VINFO_DATAREF_READS (loop_vinfo);\n+\n+  if (vect_dump && (dump_flags & TDF_DETAILS))\n+    fprintf (vect_dump, \"=== vect_update_inits_of_dr ===\");\n+\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (loop_write_datarefs); i++)\n+    {\n+      struct data_reference *dr = VARRAY_GENERIC_PTR (loop_write_datarefs, i);\n+      vect_update_inits_of_dr (dr, niters);\n+    }\n+\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (loop_read_datarefs); i++)\n+    {\n+      struct data_reference *dr = VARRAY_GENERIC_PTR (loop_read_datarefs, i);\n+      vect_update_inits_of_dr (dr, niters);\n+    }\n+}\n+\n+\n+/* Function vect_do_peeling_for_alignment\n+\n+   Peel the first 'niters' iterations of the loop represented by LOOP_VINFO.\n+   'niters' is set to the misalignment of one of the data references in the\n+   loop, thereby forcing it to refer to an aligned location at the beginning\n+   of the execution of this loop.  The data reference for which we are\n+   peeling is recorded in LOOP_VINFO_UNALIGNED_DR.  */\n+\n+static void\n+vect_do_peeling_for_alignment (loop_vec_info loop_vinfo, struct loops *loops)\n+{\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  tree niters_of_prolog_loop, ni_name;\n+  tree n_iters;\n+  struct loop *new_loop;\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"=== vect_do_peeling_for_alignment ===\");\n+\n+  ni_name = vect_build_loop_niters (loop_vinfo);\n+  niters_of_prolog_loop = vect_gen_niters_for_prolog_loop (loop_vinfo, ni_name);\n+  \n+  /* Peel the prolog loop and iterate it niters_of_prolog_loop.  */\n+  new_loop = \n+\tslpeel_tree_peel_loop_to_edge (loop, loops, loop_preheader_edge (loop), \n+\t\t\t\t       niters_of_prolog_loop, ni_name, true); \n+#ifdef ENABLE_CHECKING\n+  gcc_assert (new_loop);\n+  slpeel_verify_cfg_after_peeling (new_loop, loop);\n+#endif\n+\n+  /* Update number of times loop executes.  */\n+  n_iters = LOOP_VINFO_NITERS (loop_vinfo);\n+  LOOP_VINFO_NITERS (loop_vinfo) =\n+    build2 (MINUS_EXPR, TREE_TYPE (n_iters), n_iters, niters_of_prolog_loop);\n+\n+  /* Update the init conditions of the access functions of all data refs.  */\n+  vect_update_inits_of_drs (loop_vinfo, niters_of_prolog_loop);\n+\n+  /* After peeling we have to reset scalar evolution analyzer.  */\n+  scev_reset ();\n+\n+  return;\n+}\n+\n+\n+/* Function vect_transform_loop.\n+\n+   The analysis phase has determined that the loop is vectorizable.\n+   Vectorize the loop - created vectorized stmts to replace the scalar\n+   stmts in the loop, and update the loop exit condition.  */\n+\n+void\n+vect_transform_loop (loop_vec_info loop_vinfo, \n+\t\t     struct loops *loops ATTRIBUTE_UNUSED)\n+{\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  basic_block *bbs = LOOP_VINFO_BBS (loop_vinfo);\n+  int nbbs = loop->num_nodes;\n+  block_stmt_iterator si;\n+  int i;\n+  tree ratio = NULL;\n+  int vectorization_factor = LOOP_VINFO_VECT_FACTOR (loop_vinfo);\n+\n+  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+    fprintf (vect_dump, \"=== vec_transform_loop ===\");\n+\n+  \n+  /* Peel the loop if there are data refs with unknown alignment.\n+     Only one data ref with unknown store is allowed.  */\n+\n+  if (LOOP_DO_PEELING_FOR_ALIGNMENT (loop_vinfo))\n+    vect_do_peeling_for_alignment (loop_vinfo, loops);\n+  \n+  /* If the loop has a symbolic number of iterations 'n' (i.e. it's not a\n+     compile time constant), or it is a constant that doesn't divide by the\n+     vectorization factor, then an epilog loop needs to be created.\n+     We therefore duplicate the loop: the original loop will be vectorized,\n+     and will compute the first (n/VF) iterations. The second copy of the loop\n+     will remain scalar and will compute the remaining (n%VF) iterations.\n+     (VF is the vectorization factor).  */\n+\n+  if (!LOOP_VINFO_NITERS_KNOWN_P (loop_vinfo)\n+      || (LOOP_VINFO_NITERS_KNOWN_P (loop_vinfo)\n+          && LOOP_VINFO_INT_NITERS (loop_vinfo) % vectorization_factor != 0))\n+    vect_do_peeling_for_loop_bound (loop_vinfo, &ratio, loops);\n+  else\n+    ratio = build_int_cst (TREE_TYPE (LOOP_VINFO_NITERS (loop_vinfo)),\n+\t\tLOOP_VINFO_INT_NITERS (loop_vinfo) / vectorization_factor);\n+\n+  /* 1) Make sure the loop header has exactly two entries\n+     2) Make sure we have a preheader basic block.  */\n+\n+  gcc_assert (EDGE_COUNT (loop->header->preds) == 2);\n+\n+  loop_split_edge_with (loop_preheader_edge (loop), NULL);\n+\n+\n+  /* FORNOW: the vectorizer supports only loops which body consist\n+     of one basic block (header + empty latch). When the vectorizer will \n+     support more involved loop forms, the order by which the BBs are \n+     traversed need to be reconsidered.  */\n+\n+  for (i = 0; i < nbbs; i++)\n+    {\n+      basic_block bb = bbs[i];\n+\n+      for (si = bsi_start (bb); !bsi_end_p (si);)\n+\t{\n+\t  tree stmt = bsi_stmt (si);\n+\t  stmt_vec_info stmt_info;\n+\t  bool is_store;\n+\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    {\n+\t      fprintf (vect_dump, \"------>vectorizing statement: \");\n+\t      print_generic_expr (vect_dump, stmt, TDF_SLIM);\n+\t    }\t\n+\t  stmt_info = vinfo_for_stmt (stmt);\n+\t  gcc_assert (stmt_info);\n+\t  if (!STMT_VINFO_RELEVANT_P (stmt_info))\n+\t    {\n+\t      bsi_next (&si);\n+\t      continue;\n+\t    }\n+#ifdef ENABLE_CHECKING\n+\t  /* FORNOW: Verify that all stmts operate on the same number of\n+\t             units and no inner unrolling is necessary.  */\n+\t  gcc_assert \n+\t\t(GET_MODE_NUNITS (TYPE_MODE (STMT_VINFO_VECTYPE (stmt_info)))\n+\t\t == vectorization_factor);\n+#endif\n+\t  /* -------- vectorize statement ------------ */\n+\t  if (vect_print_dump_info (REPORT_DETAILS, UNKNOWN_LOC))\n+\t    fprintf (vect_dump, \"transform statement.\");\n+\n+\t  is_store = vect_transform_stmt (stmt, &si);\n+\t  if (is_store)\n+\t    {\n+\t      /* free the attached stmt_vec_info and remove the stmt.  */\n+\t      stmt_ann_t ann = stmt_ann (stmt);\n+\t      free (stmt_info);\n+\t      set_stmt_info (ann, NULL);\n+\t      bsi_remove (&si);\n+\t      continue;\n+\t    }\n+\n+\t  bsi_next (&si);\n+\t}\t\t        /* stmts in BB */\n+    }\t\t\t\t/* BBs in loop */\n+\n+  slpeel_make_loop_iterate_ntimes (loop, ratio);\n+\n+  if (vect_print_dump_info (REPORT_VECTORIZED_LOOPS, LOOP_LOC (loop_vinfo)))\n+    fprintf (vect_dump, \"LOOP VECTORIZED.\");\n+}"}, {"sha": "a7e5bece823ccf06d88ea23c56eda0a59db86122", "filename": "gcc/tree-vectorizer.c", "status": "modified", "additions": 125, "deletions": 4361, "changes": 4486, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7064d11bb781d5cad08cdcb4942712390c477ca/gcc%2Ftree-vectorizer.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7064d11bb781d5cad08cdcb4942712390c477ca/gcc%2Ftree-vectorizer.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.c?ref=f7064d11bb781d5cad08cdcb4942712390c477ca"}, {"sha": "f974c7087865e149e17bca2fc775bad8d6814313", "filename": "gcc/tree-vectorizer.h", "status": "modified", "additions": 54, "deletions": 3, "changes": 57, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f7064d11bb781d5cad08cdcb4942712390c477ca/gcc%2Ftree-vectorizer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f7064d11bb781d5cad08cdcb4942712390c477ca/gcc%2Ftree-vectorizer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.h?ref=f7064d11bb781d5cad08cdcb4942712390c477ca", "patch": "@@ -258,17 +258,68 @@ unknown_alignment_for_access_p (struct data_reference *data_ref_info)\n /* Perform signed modulo, always returning a non-negative value.  */\n #define VECT_SMODULO(x,y) ((x) % (y) < 0 ? ((x) % (y) + (y)) : (x) % (y))\n \n+/* vect_dump will be set to stderr or dump_file if exist.  */\n+extern FILE *vect_dump;\n+extern enum verbosity_levels vect_verbosity_level;\n \n /*-----------------------------------------------------------------*/\n /* Function prototypes.                                            */\n /*-----------------------------------------------------------------*/\n \n-/* Main driver.  */\n-extern void vectorize_loops (struct loops *);\n+/*************************************************************************\n+  Simple Loop Peeling Utilities - in tree-vectorizer.c\n+ *************************************************************************/\n+/* Entry point for peeling of simple loops.\n+   Peel the first/last iterations of a loop.\n+   It can be used outside of the vectorizer for loops that are simple enough\n+   (see function documentation).  In the vectorizer it is used to peel the\n+   last few iterations when the loop bound is unknown or does not evenly\n+   divide by the vectorization factor, and to peel the first few iterations\n+   to force the alignment of data references in the loop.  */\n+extern struct loop *slpeel_tree_peel_loop_to_edge \n+  (struct loop *, struct loops *, edge, tree, tree, bool);\n+extern void slpeel_make_loop_iterate_ntimes (struct loop *, tree);\n+extern bool slpeel_can_duplicate_loop_p (struct loop *, edge);\n+#ifdef ENABLE_CHECKING\n+extern void slpeel_verify_cfg_after_peeling (struct loop *, struct loop *);\n+#endif\n+\n \n-/* creation and deletion of loop and stmt info structs.  */\n+/*************************************************************************\n+  General Vectorization Utilities\n+ *************************************************************************/\n+/** In tree-vectorizer.c **/\n+extern tree vect_strip_conversion (tree);\n+extern tree get_vectype_for_scalar_type (tree);\n+extern bool vect_is_simple_use (tree , loop_vec_info, tree *);\n+extern bool vect_is_simple_iv_evolution (unsigned, tree, tree *, tree *);\n+extern bool vect_can_force_dr_alignment_p (tree, unsigned int);\n+extern enum dr_alignment_support vect_supportable_dr_alignment\n+  (struct data_reference *);\n+/* Creation and deletion of loop and stmt info structs.  */\n extern loop_vec_info new_loop_vec_info (struct loop *loop);\n extern void destroy_loop_vec_info (loop_vec_info);\n extern stmt_vec_info new_stmt_vec_info (tree stmt, loop_vec_info);\n+/* Main driver.  */\n+extern void vectorize_loops (struct loops *);\n+\n+/** In tree-vect-analyze.c  **/\n+/* Driver for analysis stage.  */\n+extern loop_vec_info vect_analyze_loop (struct loop *);\n+\n+/** In tree-vect-transform.c  **/\n+extern bool vectorizable_load (tree, block_stmt_iterator *, tree *);\n+extern bool vectorizable_store (tree, block_stmt_iterator *, tree *);\n+extern bool vectorizable_operation (tree, block_stmt_iterator *, tree *);\n+extern bool vectorizable_assignment (tree, block_stmt_iterator *, tree *);\n+/* Driver for transformation stage.  */\n+extern void vect_transform_loop (loop_vec_info, struct loops *);\n+\n+/*************************************************************************\n+  Vectorization Debug Information - in tree-vectorizer.c\n+ *************************************************************************/\n+extern bool vect_print_dump_info (enum verbosity_levels, LOC);\n+extern void vect_set_verbosity_level (const char *);\n+extern LOC find_loop_location (struct loop *);\n \n #endif  /* GCC_TREE_VECTORIZER_H  */"}]}