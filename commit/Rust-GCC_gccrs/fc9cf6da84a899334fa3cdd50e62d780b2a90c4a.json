{"sha": "fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZmM5Y2Y2ZGE4NGE4OTkzMzRmYTNjZGQ1MGU2MmQ3ODBiMmE5MGM0YQ==", "commit": {"author": {"name": "H.J. Lu", "email": "hongjiu.lu@intel.com", "date": "2016-04-19T14:33:36Z"}, "committer": {"name": "H.J. Lu", "email": "hjl@gcc.gnu.org", "date": "2016-04-19T14:33:36Z"}, "message": "Remove UNSPEC_LOADU and UNSPEC_STOREU\n\nSince *mov<mode>_internal and <avx512>_(load|store)<mode>_mask patterns\ncan handle unaligned load and store, we can remove UNSPEC_LOADU and\nUNSPEC_STOREU.  We use function prototypes with pointer to scalar for\nunaligned load/store builtin functions so that memory passed to\n*mov<mode>_internal is unaligned.\n\ngcc/\n\n\tPR target/69201\n\t* config/i386/avx512bwintrin.h (_mm512_mask_loadu_epi16): Pass\n\tconst short * to __builtin_ia32_loaddquhi512_mask.\n\t(_mm512_maskz_loadu_epi16): Likewise.\n\t(_mm512_mask_storeu_epi16): Pass short * to\n\t__builtin_ia32_storedquhi512_mask.\n\t(_mm512_mask_loadu_epi8): Pass const char * to\n\t__builtin_ia32_loaddquqi512_mask.\n\t(_mm512_maskz_loadu_epi8): Likewise.\n\t(_mm512_mask_storeu_epi8): Pass char * to\n\t__builtin_ia32_storedquqi512_mask.\n\t* config/i386/avx512fintrin.h (_mm512_loadu_pd): Pass\n\tconst double * to __builtin_ia32_loadupd512_mask.\n\t(_mm512_mask_loadu_pd): Likewise.\n\t(_mm512_maskz_loadu_pd): Likewise.\n\t(_mm512_storeu_pd): Pass double * to\n\t__builtin_ia32_storeupd512_mask.\n\t(_mm512_mask_storeu_pd): Likewise.\n\t(_mm512_loadu_ps): Pass const float * to\n\t__builtin_ia32_loadups512_mask.\n\t(_mm512_mask_loadu_ps): Likewise.\n\t(_mm512_maskz_loadu_ps): Likewise.\n\t(_mm512_storeu_ps): Pass float * to\n\t__builtin_ia32_storeups512_mask.\n\t(_mm512_mask_storeu_ps): Likewise.\n\t(_mm512_mask_loadu_epi64): Pass const long long * to\n\t__builtin_ia32_loaddqudi512_mask.\n\t(_mm512_maskz_loadu_epi64): Likewise.\n\t(_mm512_mask_storeu_epi64): Pass long long *\n\tto __builtin_ia32_storedqudi512_mask.\n\t(_mm512_loadu_si512): Pass const int * to\n\t__builtin_ia32_loaddqusi512_mask.\n\t(_mm512_mask_loadu_epi32): Likewise.\n\t(_mm512_maskz_loadu_epi32): Likewise.\n\t(_mm512_storeu_si512): Pass int * to\n\t__builtin_ia32_storedqusi512_mask.\n\t(_mm512_mask_storeu_epi32): Likewise.\n\t* config/i386/avx512vlbwintrin.h (_mm256_mask_storeu_epi8): Pass\n\tchar * to __builtin_ia32_storedquqi256_mask.\n\t(_mm_mask_storeu_epi8): Likewise.\n\t(_mm256_mask_loadu_epi16): Pass const short * to\n\t__builtin_ia32_loaddquhi256_mask.\n\t(_mm256_maskz_loadu_epi16): Likewise.\n\t(_mm_mask_loadu_epi16): Pass const short * to\n\t__builtin_ia32_loaddquhi128_mask.\n\t(_mm_maskz_loadu_epi16): Likewise.\n\t(_mm256_mask_loadu_epi8): Pass const char * to\n\t__builtin_ia32_loaddquqi256_mask.\n\t(_mm256_maskz_loadu_epi8): Likewise.\n\t(_mm_mask_loadu_epi8): Pass const char * to\n\t__builtin_ia32_loaddquqi128_mask.\n\t(_mm_maskz_loadu_epi8): Likewise.\n\t(_mm256_mask_storeu_epi16): Pass short * to.\n\t__builtin_ia32_storedquhi256_mask.\n\t(_mm_mask_storeu_epi16): Pass short * to.\n\t__builtin_ia32_storedquhi128_mask.\n\t* config/i386/avx512vlintrin.h (_mm256_mask_loadu_pd): Pass\n\tconst double * to __builtin_ia32_loadupd256_mask.\n\t(_mm256_maskz_loadu_pd): Likewise.\n\t(_mm_mask_loadu_pd): Pass onst double * to\n\t__builtin_ia32_loadupd128_mask.\n\t(_mm_maskz_loadu_pd): Likewise.\n\t(_mm256_mask_storeu_pd): Pass double * to\n\t__builtin_ia32_storeupd256_mask.\n\t(_mm_mask_storeu_pd): Pass double * to\n\t__builtin_ia32_storeupd128_mask.\n\t(_mm256_mask_loadu_ps): Pass const float * to\n\t__builtin_ia32_loadups256_mask.\n\t(_mm256_maskz_loadu_ps): Likewise.\n\t(_mm_mask_loadu_ps): Pass const float * to\n\t__builtin_ia32_loadups128_mask.\n\t(_mm_maskz_loadu_ps): Likewise.\n\t(_mm256_mask_storeu_ps): Pass float * to\n\t__builtin_ia32_storeups256_mask.\n\t(_mm_mask_storeu_ps): ass float * to\n\t__builtin_ia32_storeups128_mask.\n\t(_mm256_mask_loadu_epi64): Pass const long long * to\n\t__builtin_ia32_loaddqudi256_mask.\n\t(_mm256_maskz_loadu_epi64): Likewise.\n\t(_mm_mask_loadu_epi64): Pass const long long * to\n\t__builtin_ia32_loaddqudi128_mask.\n\t(_mm_maskz_loadu_epi64): Likewise.\n\t(_mm256_mask_storeu_epi64): Pass long long * to\n\t__builtin_ia32_storedqudi256_mask.\n\t(_mm_mask_storeu_epi64): Pass long long * to\n\t__builtin_ia32_storedqudi128_mask.\n\t(_mm256_mask_loadu_epi32): Pass const int * to\n\t__builtin_ia32_loaddqusi256_mask.\n\t(_mm256_maskz_loadu_epi32): Likewise.\n\t(_mm_mask_loadu_epi32): Pass const int * to\n\t__builtin_ia32_loaddqusi128_mask.\n\t(_mm_maskz_loadu_epi32): Likewise.\n\t(_mm256_mask_storeu_epi32): Pass int * to\n\t__builtin_ia32_storedqusi256_mask.\n\t(_mm_mask_storeu_epi32): Pass int * to\n\t__builtin_ia32_storedqusi128_mask.\n\t* config/i386/i386-builtin-types.def (PCSHORT): New.\n\t(PINT64): Likewise.\n\t(V64QI_FTYPE_PCCHAR_V64QI_UDI): Likewise.\n\t(V32HI_FTYPE_PCSHORT_V32HI_USI): Likewise.\n\t(V32QI_FTYPE_PCCHAR_V32QI_USI): Likewise.\n\t(V16SF_FTYPE_PCFLOAT_V16SF_UHI): Likewise.\n\t(V8DF_FTYPE_PCDOUBLE_V8DF_UQI): Likewise.\n\t(V16SI_FTYPE_PCINT_V16SI_UHI): Likewise.\n\t(V16HI_FTYPE_PCSHORT_V16HI_UHI): Likewise.\n\t(V16QI_FTYPE_PCCHAR_V16QI_UHI): Likewise.\n\t(V8SF_FTYPE_PCFLOAT_V8SF_UQI): Likewise.\n\t(V8DI_FTYPE_PCINT64_V8DI_UQI): Likewise.\n\t(V8SI_FTYPE_PCINT_V8SI_UQI): Likewise.\n\t(V8HI_FTYPE_PCSHORT_V8HI_UQI): Likewise.\n\t(V4DF_FTYPE_PCDOUBLE_V4DF_UQI): Likewise.\n\t(V4SF_FTYPE_PCFLOAT_V4SF_UQI): Likewise.\n\t(V4DI_FTYPE_PCINT64_V4DI_UQI): Likewise.\n\t(V4SI_FTYPE_PCINT_V4SI_UQI): Likewise.\n\t(V2DF_FTYPE_PCDOUBLE_V2DF_UQI): Likewise.\n\t(V2DI_FTYPE_PCINT64_V2DI_UQI): Likewise.\n\t(VOID_FTYPE_PDOUBLE_V8DF_UQI): Likewise.\n\t(VOID_FTYPE_PDOUBLE_V4DF_UQI): Likewise.\n\t(VOID_FTYPE_PDOUBLE_V2DF_UQI): Likewise.\n\t(VOID_FTYPE_PFLOAT_V16SF_UHI): Likewise.\n\t(VOID_FTYPE_PFLOAT_V8SF_UQI): Likewise.\n\t(VOID_FTYPE_PFLOAT_V4SF_UQI): Likewise.\n\t(VOID_FTYPE_PINT64_V8DI_UQI): Likewise.\n\t(VOID_FTYPE_PINT64_V4DI_UQI): Likewise.\n\t(VOID_FTYPE_PINT64_V2DI_UQI): Likewise.\n\t(VOID_FTYPE_PINT_V16SI_UHI): Likewise.\n\t(VOID_FTYPE_PINT_V8SI_UHI): Likewise.\n\t(VOID_FTYPE_PINT_V4SI_UHI): Likewise.\n\t(VOID_FTYPE_PSHORT_V32HI_USI): Likewise.\n\t(VOID_FTYPE_PSHORT_V16HI_UHI): Likewise.\n\t(VOID_FTYPE_PSHORT_V8HI_UQI): Likewise.\n\t(VOID_FTYPE_PCHAR_V64QI_UDI): Likewise.\n\t(VOID_FTYPE_PCHAR_V32QI_USI): Likewise.\n\t(VOID_FTYPE_PCHAR_V16QI_UHI): Likewise.\n\t(V64QI_FTYPE_PCV64QI_V64QI_UDI): Removed.\n\t(V32HI_FTYPE_PCV32HI_V32HI_USI): Likewise.\n\t(V32QI_FTYPE_PCV32QI_V32QI_USI): Likewise.\n\t(V16HI_FTYPE_PCV16HI_V16HI_UHI): Likewise.\n\t(V16QI_FTYPE_PCV16QI_V16QI_UHI): Likewise.\n\t(V8HI_FTYPE_PCV8HI_V8HI_UQI): Likewise.\n\t(VOID_FTYPE_PV32HI_V32HI_USI): Likewise.\n\t(VOID_FTYPE_PV16HI_V16HI_UHI): Likewise.\n\t(VOID_FTYPE_PV8HI_V8HI_UQI): Likewise.\n\t(VOID_FTYPE_PV64QI_V64QI_UDI): Likewise.\n\t(VOID_FTYPE_PV32QI_V32QI_USI): Likewise.\n\t(VOID_FTYPE_PV16QI_V16QI_UHI): Likewise.\n\t* config/i386/i386.c (ix86_emit_save_reg_using_mov): Don't\n\tuse UNSPEC_STOREU.\n\t(ix86_emit_restore_sse_regs_using_mov): Don't use UNSPEC_LOADU.\n\t(ix86_avx256_split_vector_move_misalign): Don't use unaligned\n\tload nor store.\n\t(ix86_expand_vector_move_misalign): Likewise.\n\t(bdesc_special_args): Use CODE_FOR_movvNXY_internal and pointer\n\tto scalar function prototype for unaligned load/store builtins.\n\t(ix86_expand_special_args_builtin): Updated.\n\t* config/i386/sse.md (UNSPEC_LOADU): Removed.\n\t(UNSPEC_STOREU): Likewise.\n\t(VI_ULOADSTORE_BW_AVX512VL): Likewise.\n\t(VI_ULOADSTORE_F_AVX512VL): Likewise.\n\t(ssescalarsize): Handle V4TI, V2TI and V1TI.\n\t(<sse>_loadu<ssemodesuffix><avxsizesuffix><mask_name>): Likewise.\n\t(*<sse>_loadu<ssemodesuffix><avxsizesuffix><mask_name>): Likewise.\n\t(<sse>_storeu<ssemodesuffix><avxsizesuffix>): Likewise.\n\t(<avx512>_storeu<ssemodesuffix><avxsizesuffix>_mask): Likewise.\n\t(<sse2_avx_avx512f>_loaddqu<mode><mask_name>): Likewise.\n\t(*<sse2_avx_avx512f>_loaddqu<mode><mask_name>\"): Likewise.\n\t(sse2_avx_avx512f>_storedqu<mode>): Likewise.\n\t(<avx512>_storedqu<mode>_mask): Likewise.\n\t(*sse4_2_pcmpestr_unaligned): Likewise.\n\t(*sse4_2_pcmpistr_unaligned): Likewise.\n\t(*mov<mode>_internal): Renamed to ...\n\t(mov<mode>_internal): This.  Remove check of AVX and IAMCU on\n\tmisaligned operand.  Replace vmovdqu64 with vmovdqu<ssescalarsize>.\n\t(movsd/movhpd to movupd peephole): Don't use UNSPEC_LOADU.\n\t(movlpd/movhpd to movupd peephole): Don't use UNSPEC_STOREU.\n\ngcc/testsuite/\n\n\tPR target/69201\n\t* gcc.target/i386/avx256-unaligned-store-1.c (a): Make it\n\textern to force it misaligned.\n\t(b): Likewise.\n\t(c): Likewise.\n\t(d): Likewise.\n\tCheck vmovups.*movv8sf_internal/3 instead of avx_storeups256.\n\tDon't check `*' before movv4sf_internal.\n\t* gcc.target/i386/avx256-unaligned-store-2.c: Check\n\tvmovups.*movv32qi_internal/3 instead of avx_storeups256.\n\tDon't check `*' before movv16qi_internal.\n\t* gcc.target/i386/avx256-unaligned-store-3.c (a): Make it\n\textern to force it misaligned.\n\t(b): Likewise.\n\t(c): Likewise.\n\t(d): Likewise.\n\tCheck vmovups.*movv4df_internal/3 instead of avx_storeupd256.\n\tDon't check `*' before movv2df_internal.\n\t* gcc.target/i386/avx256-unaligned-store-4.c (a): Make it\n\textern to force it misaligned.\n\t(b): Likewise.\n\t(c): Likewise.\n\t(d): Likewise.\n\tCheck movv8sf_internal instead of avx_storeups256.\n\tCheck movups.*movv4sf_internal/3 instead of avx_storeups256.\n\nFrom-SVN: r235209", "tree": {"sha": "76df8bb2cb01fcb294df2f981eed031805d9ad92", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/76df8bb2cb01fcb294df2f981eed031805d9ad92"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/comments", "author": {"login": "hjl-tools", "id": 1072356, "node_id": "MDQ6VXNlcjEwNzIzNTY=", "avatar_url": "https://avatars.githubusercontent.com/u/1072356?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hjl-tools", "html_url": "https://github.com/hjl-tools", "followers_url": "https://api.github.com/users/hjl-tools/followers", "following_url": "https://api.github.com/users/hjl-tools/following{/other_user}", "gists_url": "https://api.github.com/users/hjl-tools/gists{/gist_id}", "starred_url": "https://api.github.com/users/hjl-tools/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hjl-tools/subscriptions", "organizations_url": "https://api.github.com/users/hjl-tools/orgs", "repos_url": "https://api.github.com/users/hjl-tools/repos", "events_url": "https://api.github.com/users/hjl-tools/events{/privacy}", "received_events_url": "https://api.github.com/users/hjl-tools/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "ea8927ea15cbe3b1c6470495df939abfdc148689", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ea8927ea15cbe3b1c6470495df939abfdc148689", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ea8927ea15cbe3b1c6470495df939abfdc148689"}], "stats": {"total": 1145, "additions": 436, "deletions": 709}, "files": [{"sha": "9f45806fc148359da639e99bb357a57c24ad4daa", "filename": "gcc/ChangeLog", "status": "modified", "additions": 178, "deletions": 0, "changes": 178, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "patch": "@@ -1,3 +1,181 @@\n+2016-04-19  H.J. Lu  <hongjiu.lu@intel.com>\n+\n+\tPR target/69201\n+\t* config/i386/avx512bwintrin.h (_mm512_mask_loadu_epi16): Pass\n+\tconst short * to __builtin_ia32_loaddquhi512_mask.\n+\t(_mm512_maskz_loadu_epi16): Likewise.\n+\t(_mm512_mask_storeu_epi16): Pass short * to\n+\t__builtin_ia32_storedquhi512_mask.\n+\t(_mm512_mask_loadu_epi8): Pass const char * to\n+\t__builtin_ia32_loaddquqi512_mask.\n+\t(_mm512_maskz_loadu_epi8): Likewise.\n+\t(_mm512_mask_storeu_epi8): Pass char * to\n+\t__builtin_ia32_storedquqi512_mask.\n+\t* config/i386/avx512fintrin.h (_mm512_loadu_pd): Pass\n+\tconst double * to __builtin_ia32_loadupd512_mask.\n+\t(_mm512_mask_loadu_pd): Likewise.\n+\t(_mm512_maskz_loadu_pd): Likewise.\n+\t(_mm512_storeu_pd): Pass double * to\n+\t__builtin_ia32_storeupd512_mask.\n+\t(_mm512_mask_storeu_pd): Likewise.\n+\t(_mm512_loadu_ps): Pass const float * to\n+\t__builtin_ia32_loadups512_mask.\n+\t(_mm512_mask_loadu_ps): Likewise.\n+\t(_mm512_maskz_loadu_ps): Likewise.\n+\t(_mm512_storeu_ps): Pass float * to\n+\t__builtin_ia32_storeups512_mask.\n+\t(_mm512_mask_storeu_ps): Likewise.\n+\t(_mm512_mask_loadu_epi64): Pass const long long * to\n+\t__builtin_ia32_loaddqudi512_mask.\n+\t(_mm512_maskz_loadu_epi64): Likewise.\n+\t(_mm512_mask_storeu_epi64): Pass long long *\n+\tto __builtin_ia32_storedqudi512_mask.\n+\t(_mm512_loadu_si512): Pass const int * to\n+\t__builtin_ia32_loaddqusi512_mask.\n+\t(_mm512_mask_loadu_epi32): Likewise.\n+\t(_mm512_maskz_loadu_epi32): Likewise.\n+\t(_mm512_storeu_si512): Pass int * to\n+\t__builtin_ia32_storedqusi512_mask.\n+\t(_mm512_mask_storeu_epi32): Likewise.\n+\t* config/i386/avx512vlbwintrin.h (_mm256_mask_storeu_epi8): Pass\n+\tchar * to __builtin_ia32_storedquqi256_mask.\n+\t(_mm_mask_storeu_epi8): Likewise.\n+\t(_mm256_mask_loadu_epi16): Pass const short * to\n+\t__builtin_ia32_loaddquhi256_mask.\n+\t(_mm256_maskz_loadu_epi16): Likewise.\n+\t(_mm_mask_loadu_epi16): Pass const short * to\n+\t__builtin_ia32_loaddquhi128_mask.\n+\t(_mm_maskz_loadu_epi16): Likewise.\n+\t(_mm256_mask_loadu_epi8): Pass const char * to\n+\t__builtin_ia32_loaddquqi256_mask.\n+\t(_mm256_maskz_loadu_epi8): Likewise.\n+\t(_mm_mask_loadu_epi8): Pass const char * to\n+\t__builtin_ia32_loaddquqi128_mask.\n+\t(_mm_maskz_loadu_epi8): Likewise.\n+\t(_mm256_mask_storeu_epi16): Pass short * to.\n+\t__builtin_ia32_storedquhi256_mask.\n+\t(_mm_mask_storeu_epi16): Pass short * to.\n+\t__builtin_ia32_storedquhi128_mask.\n+\t* config/i386/avx512vlintrin.h (_mm256_mask_loadu_pd): Pass\n+\tconst double * to __builtin_ia32_loadupd256_mask.\n+\t(_mm256_maskz_loadu_pd): Likewise.\n+\t(_mm_mask_loadu_pd): Pass onst double * to\n+\t__builtin_ia32_loadupd128_mask.\n+\t(_mm_maskz_loadu_pd): Likewise.\n+\t(_mm256_mask_storeu_pd): Pass double * to\n+\t__builtin_ia32_storeupd256_mask.\n+\t(_mm_mask_storeu_pd): Pass double * to\n+\t__builtin_ia32_storeupd128_mask.\n+\t(_mm256_mask_loadu_ps): Pass const float * to\n+\t__builtin_ia32_loadups256_mask.\n+\t(_mm256_maskz_loadu_ps): Likewise.\n+\t(_mm_mask_loadu_ps): Pass const float * to\n+\t__builtin_ia32_loadups128_mask.\n+\t(_mm_maskz_loadu_ps): Likewise.\n+\t(_mm256_mask_storeu_ps): Pass float * to\n+\t__builtin_ia32_storeups256_mask.\n+\t(_mm_mask_storeu_ps): ass float * to\n+\t__builtin_ia32_storeups128_mask.\n+\t(_mm256_mask_loadu_epi64): Pass const long long * to\n+\t__builtin_ia32_loaddqudi256_mask.\n+\t(_mm256_maskz_loadu_epi64): Likewise.\n+\t(_mm_mask_loadu_epi64): Pass const long long * to\n+\t__builtin_ia32_loaddqudi128_mask.\n+\t(_mm_maskz_loadu_epi64): Likewise.\n+\t(_mm256_mask_storeu_epi64): Pass long long * to\n+\t__builtin_ia32_storedqudi256_mask.\n+\t(_mm_mask_storeu_epi64): Pass long long * to\n+\t__builtin_ia32_storedqudi128_mask.\n+\t(_mm256_mask_loadu_epi32): Pass const int * to\n+\t__builtin_ia32_loaddqusi256_mask.\n+\t(_mm256_maskz_loadu_epi32): Likewise.\n+\t(_mm_mask_loadu_epi32): Pass const int * to\n+\t__builtin_ia32_loaddqusi128_mask.\n+\t(_mm_maskz_loadu_epi32): Likewise.\n+\t(_mm256_mask_storeu_epi32): Pass int * to\n+\t__builtin_ia32_storedqusi256_mask.\n+\t(_mm_mask_storeu_epi32): Pass int * to\n+\t__builtin_ia32_storedqusi128_mask.\n+\t* config/i386/i386-builtin-types.def (PCSHORT): New.\n+\t(PINT64): Likewise.\n+\t(V64QI_FTYPE_PCCHAR_V64QI_UDI): Likewise.\n+\t(V32HI_FTYPE_PCSHORT_V32HI_USI): Likewise.\n+\t(V32QI_FTYPE_PCCHAR_V32QI_USI): Likewise.\n+\t(V16SF_FTYPE_PCFLOAT_V16SF_UHI): Likewise.\n+\t(V8DF_FTYPE_PCDOUBLE_V8DF_UQI): Likewise.\n+\t(V16SI_FTYPE_PCINT_V16SI_UHI): Likewise.\n+\t(V16HI_FTYPE_PCSHORT_V16HI_UHI): Likewise.\n+\t(V16QI_FTYPE_PCCHAR_V16QI_UHI): Likewise.\n+\t(V8SF_FTYPE_PCFLOAT_V8SF_UQI): Likewise.\n+\t(V8DI_FTYPE_PCINT64_V8DI_UQI): Likewise.\n+\t(V8SI_FTYPE_PCINT_V8SI_UQI): Likewise.\n+\t(V8HI_FTYPE_PCSHORT_V8HI_UQI): Likewise.\n+\t(V4DF_FTYPE_PCDOUBLE_V4DF_UQI): Likewise.\n+\t(V4SF_FTYPE_PCFLOAT_V4SF_UQI): Likewise.\n+\t(V4DI_FTYPE_PCINT64_V4DI_UQI): Likewise.\n+\t(V4SI_FTYPE_PCINT_V4SI_UQI): Likewise.\n+\t(V2DF_FTYPE_PCDOUBLE_V2DF_UQI): Likewise.\n+\t(V2DI_FTYPE_PCINT64_V2DI_UQI): Likewise.\n+\t(VOID_FTYPE_PDOUBLE_V8DF_UQI): Likewise.\n+\t(VOID_FTYPE_PDOUBLE_V4DF_UQI): Likewise.\n+\t(VOID_FTYPE_PDOUBLE_V2DF_UQI): Likewise.\n+\t(VOID_FTYPE_PFLOAT_V16SF_UHI): Likewise.\n+\t(VOID_FTYPE_PFLOAT_V8SF_UQI): Likewise.\n+\t(VOID_FTYPE_PFLOAT_V4SF_UQI): Likewise.\n+\t(VOID_FTYPE_PINT64_V8DI_UQI): Likewise.\n+\t(VOID_FTYPE_PINT64_V4DI_UQI): Likewise.\n+\t(VOID_FTYPE_PINT64_V2DI_UQI): Likewise.\n+\t(VOID_FTYPE_PINT_V16SI_UHI): Likewise.\n+\t(VOID_FTYPE_PINT_V8SI_UHI): Likewise.\n+\t(VOID_FTYPE_PINT_V4SI_UHI): Likewise.\n+\t(VOID_FTYPE_PSHORT_V32HI_USI): Likewise.\n+\t(VOID_FTYPE_PSHORT_V16HI_UHI): Likewise.\n+\t(VOID_FTYPE_PSHORT_V8HI_UQI): Likewise.\n+\t(VOID_FTYPE_PCHAR_V64QI_UDI): Likewise.\n+\t(VOID_FTYPE_PCHAR_V32QI_USI): Likewise.\n+\t(VOID_FTYPE_PCHAR_V16QI_UHI): Likewise.\n+\t(V64QI_FTYPE_PCV64QI_V64QI_UDI): Removed.\n+\t(V32HI_FTYPE_PCV32HI_V32HI_USI): Likewise.\n+\t(V32QI_FTYPE_PCV32QI_V32QI_USI): Likewise.\n+\t(V16HI_FTYPE_PCV16HI_V16HI_UHI): Likewise.\n+\t(V16QI_FTYPE_PCV16QI_V16QI_UHI): Likewise.\n+\t(V8HI_FTYPE_PCV8HI_V8HI_UQI): Likewise.\n+\t(VOID_FTYPE_PV32HI_V32HI_USI): Likewise.\n+\t(VOID_FTYPE_PV16HI_V16HI_UHI): Likewise.\n+\t(VOID_FTYPE_PV8HI_V8HI_UQI): Likewise.\n+\t(VOID_FTYPE_PV64QI_V64QI_UDI): Likewise.\n+\t(VOID_FTYPE_PV32QI_V32QI_USI): Likewise.\n+\t(VOID_FTYPE_PV16QI_V16QI_UHI): Likewise.\n+\t* config/i386/i386.c (ix86_emit_save_reg_using_mov): Don't\n+\tuse UNSPEC_STOREU.\n+\t(ix86_emit_restore_sse_regs_using_mov): Don't use UNSPEC_LOADU.\n+\t(ix86_avx256_split_vector_move_misalign): Don't use unaligned\n+\tload nor store.\n+\t(ix86_expand_vector_move_misalign): Likewise.\n+\t(bdesc_special_args): Use CODE_FOR_movvNXY_internal and pointer\n+\tto scalar function prototype for unaligned load/store builtins.\n+\t(ix86_expand_special_args_builtin): Updated.\n+\t* config/i386/sse.md (UNSPEC_LOADU): Removed.\n+\t(UNSPEC_STOREU): Likewise.\n+\t(VI_ULOADSTORE_BW_AVX512VL): Likewise.\n+\t(VI_ULOADSTORE_F_AVX512VL): Likewise.\n+\t(ssescalarsize): Handle V4TI, V2TI and V1TI.\n+\t(<sse>_loadu<ssemodesuffix><avxsizesuffix><mask_name>): Likewise.\n+\t(*<sse>_loadu<ssemodesuffix><avxsizesuffix><mask_name>): Likewise.\n+\t(<sse>_storeu<ssemodesuffix><avxsizesuffix>): Likewise.\n+\t(<avx512>_storeu<ssemodesuffix><avxsizesuffix>_mask): Likewise.\n+\t(<sse2_avx_avx512f>_loaddqu<mode><mask_name>): Likewise.\n+\t(*<sse2_avx_avx512f>_loaddqu<mode><mask_name>\"): Likewise.\n+\t(sse2_avx_avx512f>_storedqu<mode>): Likewise.\n+\t(<avx512>_storedqu<mode>_mask): Likewise.\n+\t(*sse4_2_pcmpestr_unaligned): Likewise.\n+\t(*sse4_2_pcmpistr_unaligned): Likewise.\n+\t(*mov<mode>_internal): Renamed to ...\n+\t(mov<mode>_internal): This.  Remove check of AVX and IAMCU on\n+\tmisaligned operand.  Replace vmovdqu64 with vmovdqu<ssescalarsize>.\n+\t(movsd/movhpd to movupd peephole): Don't use UNSPEC_LOADU.\n+\t(movlpd/movhpd to movupd peephole): Don't use UNSPEC_STOREU.\n+\n 2016-04-19  Richard Biener  <rguenther@suse.de>\n \n \tPR tree-optimization/70171"}, {"sha": "e1dafba16fe5d9d67b4c6ac96b9e7c495a2b35cb", "filename": "gcc/config/i386/avx512bwintrin.h", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Fconfig%2Fi386%2Favx512bwintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Fconfig%2Fi386%2Favx512bwintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Favx512bwintrin.h?ref=fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "patch": "@@ -87,7 +87,7 @@ extern __inline __m512i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_loadu_epi16 (__m512i __W, __mmask32 __U, void const *__P)\n {\n-  return (__m512i) __builtin_ia32_loaddquhi512_mask ((__v32hi *) __P,\n+  return (__m512i) __builtin_ia32_loaddquhi512_mask ((const short *) __P,\n \t\t\t\t\t\t     (__v32hi) __W,\n \t\t\t\t\t\t     (__mmask32) __U);\n }\n@@ -96,7 +96,7 @@ extern __inline __m512i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_loadu_epi16 (__mmask32 __U, void const *__P)\n {\n-  return (__m512i) __builtin_ia32_loaddquhi512_mask ((__v32hi *) __P,\n+  return (__m512i) __builtin_ia32_loaddquhi512_mask ((const short *) __P,\n \t\t\t\t\t\t     (__v32hi)\n \t\t\t\t\t\t     _mm512_setzero_hi (),\n \t\t\t\t\t\t     (__mmask32) __U);\n@@ -106,7 +106,7 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_storeu_epi16 (void *__P, __mmask32 __U, __m512i __A)\n {\n-  __builtin_ia32_storedquhi512_mask ((__v32hi *) __P,\n+  __builtin_ia32_storedquhi512_mask ((short *) __P,\n \t\t\t\t     (__v32hi) __A,\n \t\t\t\t     (__mmask32) __U);\n }\n@@ -150,7 +150,7 @@ extern __inline __m512i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_loadu_epi8 (__m512i __W, __mmask64 __U, void const *__P)\n {\n-  return (__m512i) __builtin_ia32_loaddquqi512_mask ((__v64qi *) __P,\n+  return (__m512i) __builtin_ia32_loaddquqi512_mask ((const char *) __P,\n \t\t\t\t\t\t     (__v64qi) __W,\n \t\t\t\t\t\t     (__mmask64) __U);\n }\n@@ -159,7 +159,7 @@ extern __inline __m512i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_loadu_epi8 (__mmask64 __U, void const *__P)\n {\n-  return (__m512i) __builtin_ia32_loaddquqi512_mask ((__v64qi *) __P,\n+  return (__m512i) __builtin_ia32_loaddquqi512_mask ((const char *) __P,\n \t\t\t\t\t\t     (__v64qi)\n \t\t\t\t\t\t     _mm512_setzero_hi (),\n \t\t\t\t\t\t     (__mmask64) __U);\n@@ -169,7 +169,7 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_storeu_epi8 (void *__P, __mmask64 __U, __m512i __A)\n {\n-  __builtin_ia32_storedquqi512_mask ((__v64qi *) __P,\n+  __builtin_ia32_storedquqi512_mask ((char *) __P,\n \t\t\t\t     (__v64qi) __A,\n \t\t\t\t     (__mmask64) __U);\n }"}, {"sha": "2f51be995fa6820bb65281a0f8aa4aaaa38085b3", "filename": "gcc/config/i386/avx512fintrin.h", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Fconfig%2Fi386%2Favx512fintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Fconfig%2Fi386%2Favx512fintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Favx512fintrin.h?ref=fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "patch": "@@ -5671,7 +5671,7 @@ extern __inline __m512d\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_loadu_pd (void const *__P)\n {\n-  return (__m512d) __builtin_ia32_loadupd512_mask ((const __v8df *) __P,\n+  return (__m512d) __builtin_ia32_loadupd512_mask ((const double *) __P,\n \t\t\t\t\t\t   (__v8df)\n \t\t\t\t\t\t   _mm512_undefined_pd (),\n \t\t\t\t\t\t   (__mmask8) -1);\n@@ -5681,7 +5681,7 @@ extern __inline __m512d\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_loadu_pd (__m512d __W, __mmask8 __U, void const *__P)\n {\n-  return (__m512d) __builtin_ia32_loadupd512_mask ((const __v8df *) __P,\n+  return (__m512d) __builtin_ia32_loadupd512_mask ((const double *) __P,\n \t\t\t\t\t\t   (__v8df) __W,\n \t\t\t\t\t\t   (__mmask8) __U);\n }\n@@ -5690,7 +5690,7 @@ extern __inline __m512d\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_loadu_pd (__mmask8 __U, void const *__P)\n {\n-  return (__m512d) __builtin_ia32_loadupd512_mask ((const __v8df *) __P,\n+  return (__m512d) __builtin_ia32_loadupd512_mask ((const double *) __P,\n \t\t\t\t\t\t   (__v8df)\n \t\t\t\t\t\t   _mm512_setzero_pd (),\n \t\t\t\t\t\t   (__mmask8) __U);\n@@ -5700,23 +5700,23 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_storeu_pd (void *__P, __m512d __A)\n {\n-  __builtin_ia32_storeupd512_mask ((__v8df *) __P, (__v8df) __A,\n+  __builtin_ia32_storeupd512_mask ((double *) __P, (__v8df) __A,\n \t\t\t\t   (__mmask8) -1);\n }\n \n extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_storeu_pd (void *__P, __mmask8 __U, __m512d __A)\n {\n-  __builtin_ia32_storeupd512_mask ((__v8df *) __P, (__v8df) __A,\n+  __builtin_ia32_storeupd512_mask ((double *) __P, (__v8df) __A,\n \t\t\t\t   (__mmask8) __U);\n }\n \n extern __inline __m512\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_loadu_ps (void const *__P)\n {\n-  return (__m512) __builtin_ia32_loadups512_mask ((const __v16sf *) __P,\n+  return (__m512) __builtin_ia32_loadups512_mask ((const float *) __P,\n \t\t\t\t\t\t  (__v16sf)\n \t\t\t\t\t\t  _mm512_undefined_ps (),\n \t\t\t\t\t\t  (__mmask16) -1);\n@@ -5726,7 +5726,7 @@ extern __inline __m512\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_loadu_ps (__m512 __W, __mmask16 __U, void const *__P)\n {\n-  return (__m512) __builtin_ia32_loadups512_mask ((const __v16sf *) __P,\n+  return (__m512) __builtin_ia32_loadups512_mask ((const float *) __P,\n \t\t\t\t\t\t  (__v16sf) __W,\n \t\t\t\t\t\t  (__mmask16) __U);\n }\n@@ -5735,7 +5735,7 @@ extern __inline __m512\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_loadu_ps (__mmask16 __U, void const *__P)\n {\n-  return (__m512) __builtin_ia32_loadups512_mask ((const __v16sf *) __P,\n+  return (__m512) __builtin_ia32_loadups512_mask ((const float *) __P,\n \t\t\t\t\t\t  (__v16sf)\n \t\t\t\t\t\t  _mm512_setzero_ps (),\n \t\t\t\t\t\t  (__mmask16) __U);\n@@ -5745,23 +5745,23 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_storeu_ps (void *__P, __m512 __A)\n {\n-  __builtin_ia32_storeups512_mask ((__v16sf *) __P, (__v16sf) __A,\n+  __builtin_ia32_storeups512_mask ((float *) __P, (__v16sf) __A,\n \t\t\t\t   (__mmask16) -1);\n }\n \n extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_storeu_ps (void *__P, __mmask16 __U, __m512 __A)\n {\n-  __builtin_ia32_storeups512_mask ((__v16sf *) __P, (__v16sf) __A,\n+  __builtin_ia32_storeups512_mask ((float *) __P, (__v16sf) __A,\n \t\t\t\t   (__mmask16) __U);\n }\n \n extern __inline __m512i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_loadu_epi64 (__m512i __W, __mmask8 __U, void const *__P)\n {\n-  return (__m512i) __builtin_ia32_loaddqudi512_mask ((const __v8di *) __P,\n+  return (__m512i) __builtin_ia32_loaddqudi512_mask ((const long long *) __P,\n \t\t\t\t\t\t     (__v8di) __W,\n \t\t\t\t\t\t     (__mmask8) __U);\n }\n@@ -5770,7 +5770,7 @@ extern __inline __m512i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_loadu_epi64 (__mmask8 __U, void const *__P)\n {\n-  return (__m512i) __builtin_ia32_loaddqudi512_mask ((const __v8di *) __P,\n+  return (__m512i) __builtin_ia32_loaddqudi512_mask ((const long long *) __P,\n \t\t\t\t\t\t     (__v8di)\n \t\t\t\t\t\t     _mm512_setzero_si512 (),\n \t\t\t\t\t\t     (__mmask8) __U);\n@@ -5780,15 +5780,15 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_storeu_epi64 (void *__P, __mmask8 __U, __m512i __A)\n {\n-  __builtin_ia32_storedqudi512_mask ((__v8di *) __P, (__v8di) __A,\n+  __builtin_ia32_storedqudi512_mask ((long long *) __P, (__v8di) __A,\n \t\t\t\t     (__mmask8) __U);\n }\n \n extern __inline __m512i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_loadu_si512 (void const *__P)\n {\n-  return (__m512i) __builtin_ia32_loaddqusi512_mask ((const __v16si *) __P,\n+  return (__m512i) __builtin_ia32_loaddqusi512_mask ((const int *) __P,\n \t\t\t\t\t\t     (__v16si)\n \t\t\t\t\t\t     _mm512_setzero_si512 (),\n \t\t\t\t\t\t     (__mmask16) -1);\n@@ -5798,7 +5798,7 @@ extern __inline __m512i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_loadu_epi32 (__m512i __W, __mmask16 __U, void const *__P)\n {\n-  return (__m512i) __builtin_ia32_loaddqusi512_mask ((const __v16si *) __P,\n+  return (__m512i) __builtin_ia32_loaddqusi512_mask ((const int *) __P,\n \t\t\t\t\t\t     (__v16si) __W,\n \t\t\t\t\t\t     (__mmask16) __U);\n }\n@@ -5807,7 +5807,7 @@ extern __inline __m512i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_maskz_loadu_epi32 (__mmask16 __U, void const *__P)\n {\n-  return (__m512i) __builtin_ia32_loaddqusi512_mask ((const __v16si *) __P,\n+  return (__m512i) __builtin_ia32_loaddqusi512_mask ((const int *) __P,\n \t\t\t\t\t\t     (__v16si)\n \t\t\t\t\t\t     _mm512_setzero_si512 (),\n \t\t\t\t\t\t     (__mmask16) __U);\n@@ -5817,15 +5817,15 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_storeu_si512 (void *__P, __m512i __A)\n {\n-  __builtin_ia32_storedqusi512_mask ((__v16si *) __P, (__v16si) __A,\n+  __builtin_ia32_storedqusi512_mask ((int *) __P, (__v16si) __A,\n \t\t\t\t     (__mmask16) -1);\n }\n \n extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm512_mask_storeu_epi32 (void *__P, __mmask16 __U, __m512i __A)\n {\n-  __builtin_ia32_storedqusi512_mask ((__v16si *) __P, (__v16si) __A,\n+  __builtin_ia32_storedqusi512_mask ((int *) __P, (__v16si) __A,\n \t\t\t\t     (__mmask16) __U);\n }\n "}, {"sha": "5f3d51c7a34adc1ab6d33e613c8db668ff9d0f5b", "filename": "gcc/config/i386/avx512vlbwintrin.h", "status": "modified", "additions": 12, "deletions": 12, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Fconfig%2Fi386%2Favx512vlbwintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Fconfig%2Fi386%2Favx512vlbwintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Favx512vlbwintrin.h?ref=fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "patch": "@@ -77,7 +77,7 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_storeu_epi8 (void *__P, __mmask32 __U, __m256i __A)\n {\n-  __builtin_ia32_storedquqi256_mask ((__v32qi *) __P,\n+  __builtin_ia32_storedquqi256_mask ((char *) __P,\n \t\t\t\t     (__v32qi) __A,\n \t\t\t\t     (__mmask32) __U);\n }\n@@ -86,7 +86,7 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_storeu_epi8 (void *__P, __mmask16 __U, __m128i __A)\n {\n-  __builtin_ia32_storedquqi128_mask ((__v16qi *) __P,\n+  __builtin_ia32_storedquqi128_mask ((char *) __P,\n \t\t\t\t     (__v16qi) __A,\n \t\t\t\t     (__mmask16) __U);\n }\n@@ -95,7 +95,7 @@ extern __inline __m256i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_loadu_epi16 (__m256i __W, __mmask16 __U, void const *__P)\n {\n-  return (__m256i) __builtin_ia32_loaddquhi256_mask ((__v16hi *) __P,\n+  return (__m256i) __builtin_ia32_loaddquhi256_mask ((const short *) __P,\n \t\t\t\t\t\t     (__v16hi) __W,\n \t\t\t\t\t\t     (__mmask16) __U);\n }\n@@ -104,7 +104,7 @@ extern __inline __m256i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_loadu_epi16 (__mmask16 __U, void const *__P)\n {\n-  return (__m256i) __builtin_ia32_loaddquhi256_mask ((__v16hi *) __P,\n+  return (__m256i) __builtin_ia32_loaddquhi256_mask ((const short *) __P,\n \t\t\t\t\t\t     (__v16hi)\n \t\t\t\t\t\t     _mm256_setzero_si256 (),\n \t\t\t\t\t\t     (__mmask16) __U);\n@@ -114,7 +114,7 @@ extern __inline __m128i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_loadu_epi16 (__m128i __W, __mmask8 __U, void const *__P)\n {\n-  return (__m128i) __builtin_ia32_loaddquhi128_mask ((__v8hi *) __P,\n+  return (__m128i) __builtin_ia32_loaddquhi128_mask ((const short *) __P,\n \t\t\t\t\t\t     (__v8hi) __W,\n \t\t\t\t\t\t     (__mmask8) __U);\n }\n@@ -123,7 +123,7 @@ extern __inline __m128i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_loadu_epi16 (__mmask8 __U, void const *__P)\n {\n-  return (__m128i) __builtin_ia32_loaddquhi128_mask ((__v8hi *) __P,\n+  return (__m128i) __builtin_ia32_loaddquhi128_mask ((const short *) __P,\n \t\t\t\t\t\t     (__v8hi)\n \t\t\t\t\t\t     _mm_setzero_hi (),\n \t\t\t\t\t\t     (__mmask8) __U);\n@@ -172,7 +172,7 @@ extern __inline __m256i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_loadu_epi8 (__m256i __W, __mmask32 __U, void const *__P)\n {\n-  return (__m256i) __builtin_ia32_loaddquqi256_mask ((__v32qi *) __P,\n+  return (__m256i) __builtin_ia32_loaddquqi256_mask ((const char *) __P,\n \t\t\t\t\t\t     (__v32qi) __W,\n \t\t\t\t\t\t     (__mmask32) __U);\n }\n@@ -181,7 +181,7 @@ extern __inline __m256i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_loadu_epi8 (__mmask32 __U, void const *__P)\n {\n-  return (__m256i) __builtin_ia32_loaddquqi256_mask ((__v32qi *) __P,\n+  return (__m256i) __builtin_ia32_loaddquqi256_mask ((const char *) __P,\n \t\t\t\t\t\t     (__v32qi)\n \t\t\t\t\t\t     _mm256_setzero_si256 (),\n \t\t\t\t\t\t     (__mmask32) __U);\n@@ -191,7 +191,7 @@ extern __inline __m128i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_loadu_epi8 (__m128i __W, __mmask16 __U, void const *__P)\n {\n-  return (__m128i) __builtin_ia32_loaddquqi128_mask ((__v16qi *) __P,\n+  return (__m128i) __builtin_ia32_loaddquqi128_mask ((const char *) __P,\n \t\t\t\t\t\t     (__v16qi) __W,\n \t\t\t\t\t\t     (__mmask16) __U);\n }\n@@ -200,7 +200,7 @@ extern __inline __m128i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_loadu_epi8 (__mmask16 __U, void const *__P)\n {\n-  return (__m128i) __builtin_ia32_loaddquqi128_mask ((__v16qi *) __P,\n+  return (__m128i) __builtin_ia32_loaddquqi128_mask ((const char *) __P,\n \t\t\t\t\t\t     (__v16qi)\n \t\t\t\t\t\t     _mm_setzero_hi (),\n \t\t\t\t\t\t     (__mmask16) __U);\n@@ -3679,7 +3679,7 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_storeu_epi16 (void *__P, __mmask16 __U, __m256i __A)\n {\n-  __builtin_ia32_storedquhi256_mask ((__v16hi *) __P,\n+  __builtin_ia32_storedquhi256_mask ((short *) __P,\n \t\t\t\t     (__v16hi) __A,\n \t\t\t\t     (__mmask16) __U);\n }\n@@ -3688,7 +3688,7 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_storeu_epi16 (void *__P, __mmask8 __U, __m128i __A)\n {\n-  __builtin_ia32_storedquhi128_mask ((__v8hi *) __P,\n+  __builtin_ia32_storedquhi128_mask ((short *) __P,\n \t\t\t\t     (__v8hi) __A,\n \t\t\t\t     (__mmask8) __U);\n }"}, {"sha": "d59bc6c7a82b4862273dce45d2e462124bc2824d", "filename": "gcc/config/i386/avx512vlintrin.h", "status": "modified", "additions": 24, "deletions": 24, "changes": 48, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Fconfig%2Fi386%2Favx512vlintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Fconfig%2Fi386%2Favx512vlintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Favx512vlintrin.h?ref=fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "patch": "@@ -626,7 +626,7 @@ extern __inline __m256d\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_loadu_pd (__m256d __W, __mmask8 __U, void const *__P)\n {\n-  return (__m256d) __builtin_ia32_loadupd256_mask ((__v4df *) __P,\n+  return (__m256d) __builtin_ia32_loadupd256_mask ((const double *) __P,\n \t\t\t\t\t\t   (__v4df) __W,\n \t\t\t\t\t\t   (__mmask8) __U);\n }\n@@ -635,7 +635,7 @@ extern __inline __m256d\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_loadu_pd (__mmask8 __U, void const *__P)\n {\n-  return (__m256d) __builtin_ia32_loadupd256_mask ((__v4df *) __P,\n+  return (__m256d) __builtin_ia32_loadupd256_mask ((const double *) __P,\n \t\t\t\t\t\t   (__v4df)\n \t\t\t\t\t\t   _mm256_setzero_pd (),\n \t\t\t\t\t\t   (__mmask8) __U);\n@@ -645,7 +645,7 @@ extern __inline __m128d\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_loadu_pd (__m128d __W, __mmask8 __U, void const *__P)\n {\n-  return (__m128d) __builtin_ia32_loadupd128_mask ((__v2df *) __P,\n+  return (__m128d) __builtin_ia32_loadupd128_mask ((const double *) __P,\n \t\t\t\t\t\t   (__v2df) __W,\n \t\t\t\t\t\t   (__mmask8) __U);\n }\n@@ -654,7 +654,7 @@ extern __inline __m128d\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_loadu_pd (__mmask8 __U, void const *__P)\n {\n-  return (__m128d) __builtin_ia32_loadupd128_mask ((__v2df *) __P,\n+  return (__m128d) __builtin_ia32_loadupd128_mask ((const double *) __P,\n \t\t\t\t\t\t   (__v2df)\n \t\t\t\t\t\t   _mm_setzero_pd (),\n \t\t\t\t\t\t   (__mmask8) __U);\n@@ -664,7 +664,7 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_storeu_pd (void *__P, __mmask8 __U, __m256d __A)\n {\n-  __builtin_ia32_storeupd256_mask ((__v4df *) __P,\n+  __builtin_ia32_storeupd256_mask ((double *) __P,\n \t\t\t\t   (__v4df) __A,\n \t\t\t\t   (__mmask8) __U);\n }\n@@ -673,7 +673,7 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_storeu_pd (void *__P, __mmask8 __U, __m128d __A)\n {\n-  __builtin_ia32_storeupd128_mask ((__v2df *) __P,\n+  __builtin_ia32_storeupd128_mask ((double *) __P,\n \t\t\t\t   (__v2df) __A,\n \t\t\t\t   (__mmask8) __U);\n }\n@@ -682,7 +682,7 @@ extern __inline __m256\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_loadu_ps (__m256 __W, __mmask8 __U, void const *__P)\n {\n-  return (__m256) __builtin_ia32_loadups256_mask ((__v8sf *) __P,\n+  return (__m256) __builtin_ia32_loadups256_mask ((const float *) __P,\n \t\t\t\t\t\t  (__v8sf) __W,\n \t\t\t\t\t\t  (__mmask8) __U);\n }\n@@ -691,7 +691,7 @@ extern __inline __m256\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_loadu_ps (__mmask8 __U, void const *__P)\n {\n-  return (__m256) __builtin_ia32_loadups256_mask ((__v8sf *) __P,\n+  return (__m256) __builtin_ia32_loadups256_mask ((const float *) __P,\n \t\t\t\t\t\t  (__v8sf)\n \t\t\t\t\t\t  _mm256_setzero_ps (),\n \t\t\t\t\t\t  (__mmask8) __U);\n@@ -701,7 +701,7 @@ extern __inline __m128\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_loadu_ps (__m128 __W, __mmask8 __U, void const *__P)\n {\n-  return (__m128) __builtin_ia32_loadups128_mask ((__v4sf *) __P,\n+  return (__m128) __builtin_ia32_loadups128_mask ((const float *) __P,\n \t\t\t\t\t\t  (__v4sf) __W,\n \t\t\t\t\t\t  (__mmask8) __U);\n }\n@@ -710,7 +710,7 @@ extern __inline __m128\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_loadu_ps (__mmask8 __U, void const *__P)\n {\n-  return (__m128) __builtin_ia32_loadups128_mask ((__v4sf *) __P,\n+  return (__m128) __builtin_ia32_loadups128_mask ((const float *) __P,\n \t\t\t\t\t\t  (__v4sf)\n \t\t\t\t\t\t  _mm_setzero_ps (),\n \t\t\t\t\t\t  (__mmask8) __U);\n@@ -720,7 +720,7 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_storeu_ps (void *__P, __mmask8 __U, __m256 __A)\n {\n-  __builtin_ia32_storeups256_mask ((__v8sf *) __P,\n+  __builtin_ia32_storeups256_mask ((float *) __P,\n \t\t\t\t   (__v8sf) __A,\n \t\t\t\t   (__mmask8) __U);\n }\n@@ -729,7 +729,7 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_storeu_ps (void *__P, __mmask8 __U, __m128 __A)\n {\n-  __builtin_ia32_storeups128_mask ((__v4sf *) __P,\n+  __builtin_ia32_storeups128_mask ((float *) __P,\n \t\t\t\t   (__v4sf) __A,\n \t\t\t\t   (__mmask8) __U);\n }\n@@ -738,7 +738,7 @@ extern __inline __m256i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_loadu_epi64 (__m256i __W, __mmask8 __U, void const *__P)\n {\n-  return (__m256i) __builtin_ia32_loaddqudi256_mask ((__v4di *) __P,\n+  return (__m256i) __builtin_ia32_loaddqudi256_mask ((const long long *) __P,\n \t\t\t\t\t\t     (__v4di) __W,\n \t\t\t\t\t\t     (__mmask8) __U);\n }\n@@ -747,7 +747,7 @@ extern __inline __m256i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_loadu_epi64 (__mmask8 __U, void const *__P)\n {\n-  return (__m256i) __builtin_ia32_loaddqudi256_mask ((__v4di *) __P,\n+  return (__m256i) __builtin_ia32_loaddqudi256_mask ((const long long *) __P,\n \t\t\t\t\t\t     (__v4di)\n \t\t\t\t\t\t     _mm256_setzero_si256 (),\n \t\t\t\t\t\t     (__mmask8) __U);\n@@ -757,7 +757,7 @@ extern __inline __m128i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_loadu_epi64 (__m128i __W, __mmask8 __U, void const *__P)\n {\n-  return (__m128i) __builtin_ia32_loaddqudi128_mask ((__v2di *) __P,\n+  return (__m128i) __builtin_ia32_loaddqudi128_mask ((const long long *) __P,\n \t\t\t\t\t\t     (__v2di) __W,\n \t\t\t\t\t\t     (__mmask8) __U);\n }\n@@ -766,7 +766,7 @@ extern __inline __m128i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_loadu_epi64 (__mmask8 __U, void const *__P)\n {\n-  return (__m128i) __builtin_ia32_loaddqudi128_mask ((__v2di *) __P,\n+  return (__m128i) __builtin_ia32_loaddqudi128_mask ((const long long *) __P,\n \t\t\t\t\t\t     (__v2di)\n \t\t\t\t\t\t     _mm_setzero_di (),\n \t\t\t\t\t\t     (__mmask8) __U);\n@@ -776,7 +776,7 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_storeu_epi64 (void *__P, __mmask8 __U, __m256i __A)\n {\n-  __builtin_ia32_storedqudi256_mask ((__v4di *) __P,\n+  __builtin_ia32_storedqudi256_mask ((long long *) __P,\n \t\t\t\t     (__v4di) __A,\n \t\t\t\t     (__mmask8) __U);\n }\n@@ -785,7 +785,7 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_storeu_epi64 (void *__P, __mmask8 __U, __m128i __A)\n {\n-  __builtin_ia32_storedqudi128_mask ((__v2di *) __P,\n+  __builtin_ia32_storedqudi128_mask ((long long *) __P,\n \t\t\t\t     (__v2di) __A,\n \t\t\t\t     (__mmask8) __U);\n }\n@@ -794,7 +794,7 @@ extern __inline __m256i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_loadu_epi32 (__m256i __W, __mmask8 __U, void const *__P)\n {\n-  return (__m256i) __builtin_ia32_loaddqusi256_mask ((__v8si *) __P,\n+  return (__m256i) __builtin_ia32_loaddqusi256_mask ((const int *) __P,\n \t\t\t\t\t\t     (__v8si) __W,\n \t\t\t\t\t\t     (__mmask8) __U);\n }\n@@ -803,7 +803,7 @@ extern __inline __m256i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_maskz_loadu_epi32 (__mmask8 __U, void const *__P)\n {\n-  return (__m256i) __builtin_ia32_loaddqusi256_mask ((__v8si *) __P,\n+  return (__m256i) __builtin_ia32_loaddqusi256_mask ((const int *) __P,\n \t\t\t\t\t\t     (__v8si)\n \t\t\t\t\t\t     _mm256_setzero_si256 (),\n \t\t\t\t\t\t     (__mmask8) __U);\n@@ -813,7 +813,7 @@ extern __inline __m128i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_loadu_epi32 (__m128i __W, __mmask8 __U, void const *__P)\n {\n-  return (__m128i) __builtin_ia32_loaddqusi128_mask ((__v4si *) __P,\n+  return (__m128i) __builtin_ia32_loaddqusi128_mask ((const int *) __P,\n \t\t\t\t\t\t     (__v4si) __W,\n \t\t\t\t\t\t     (__mmask8) __U);\n }\n@@ -822,7 +822,7 @@ extern __inline __m128i\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskz_loadu_epi32 (__mmask8 __U, void const *__P)\n {\n-  return (__m128i) __builtin_ia32_loaddqusi128_mask ((__v4si *) __P,\n+  return (__m128i) __builtin_ia32_loaddqusi128_mask ((const int *) __P,\n \t\t\t\t\t\t     (__v4si)\n \t\t\t\t\t\t     _mm_setzero_si128 (),\n \t\t\t\t\t\t     (__mmask8) __U);\n@@ -832,7 +832,7 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm256_mask_storeu_epi32 (void *__P, __mmask8 __U, __m256i __A)\n {\n-  __builtin_ia32_storedqusi256_mask ((__v8si *) __P,\n+  __builtin_ia32_storedqusi256_mask ((int *) __P,\n \t\t\t\t     (__v8si) __A,\n \t\t\t\t     (__mmask8) __U);\n }\n@@ -841,7 +841,7 @@ extern __inline void\n __attribute__ ((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mask_storeu_epi32 (void *__P, __mmask8 __U, __m128i __A)\n {\n-  __builtin_ia32_storedqusi128_mask ((__v4si *) __P,\n+  __builtin_ia32_storedqusi128_mask ((int *) __P,\n \t\t\t\t     (__v4si) __A,\n \t\t\t\t     (__mmask8) __U);\n }"}, {"sha": "75d57d968c7bfe772f18c0037b74b7bde2e31d45", "filename": "gcc/config/i386/i386-builtin-types.def", "status": "modified", "additions": 38, "deletions": 12, "changes": 50, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Fconfig%2Fi386%2Fi386-builtin-types.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Fconfig%2Fi386%2Fi386-builtin-types.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-builtin-types.def?ref=fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "patch": "@@ -124,6 +124,7 @@ DEF_POINTER_TYPE (PCDOUBLE, DOUBLE, CONST)\n DEF_POINTER_TYPE (PCFLOAT, FLOAT, CONST)\n DEF_POINTER_TYPE (PCINT, INT, CONST)\n DEF_POINTER_TYPE (PCINT64, INT64, CONST)\n+DEF_POINTER_TYPE (PCSHORT, SHORT, CONST)\n DEF_POINTER_TYPE (PCHAR, CHAR)\n DEF_POINTER_TYPE (PCVOID, VOID, CONST)\n DEF_POINTER_TYPE (PVOID, VOID)\n@@ -132,6 +133,7 @@ DEF_POINTER_TYPE (PFLOAT, FLOAT)\n DEF_POINTER_TYPE (PSHORT, SHORT)\n DEF_POINTER_TYPE (PUSHORT, USHORT)\n DEF_POINTER_TYPE (PINT, INT)\n+DEF_POINTER_TYPE (PINT64, INT64)\n DEF_POINTER_TYPE (PLONGLONG, LONGLONG)\n DEF_POINTER_TYPE (PULONGLONG, ULONGLONG)\n DEF_POINTER_TYPE (PUNSIGNED, UNSIGNED)\n@@ -754,24 +756,36 @@ DEF_FUNCTION_TYPE (V16HI, V8HI, V16HI, UHI)\n DEF_FUNCTION_TYPE (V16HI, HI, V16HI, UHI)\n DEF_FUNCTION_TYPE (V8HI, V8HI, V8HI, UQI)\n DEF_FUNCTION_TYPE (V8HI, HI, V8HI, UQI)\n-DEF_FUNCTION_TYPE (V64QI, PCV64QI, V64QI, UDI)\n-DEF_FUNCTION_TYPE (V32HI, PCV32HI, V32HI, USI)\n-DEF_FUNCTION_TYPE (V32QI, PCV32QI, V32QI, USI)\n DEF_FUNCTION_TYPE (V16SF, PCV16SF, V16SF, UHI)\n DEF_FUNCTION_TYPE (V8DF, PCV8DF, V8DF, UQI)\n DEF_FUNCTION_TYPE (V16SI, PCV16SI, V16SI, UHI)\n-DEF_FUNCTION_TYPE (V16HI, PCV16HI, V16HI, UHI)\n-DEF_FUNCTION_TYPE (V16QI, PCV16QI, V16QI, UHI)\n DEF_FUNCTION_TYPE (V8SF, PCV8SF, V8SF, UQI)\n DEF_FUNCTION_TYPE (V8DI, PCV8DI, V8DI, UQI)\n DEF_FUNCTION_TYPE (V8SI, PCV8SI, V8SI, UQI)\n-DEF_FUNCTION_TYPE (V8HI, PCV8HI, V8HI, UQI)\n DEF_FUNCTION_TYPE (V4DF, PCV4DF, V4DF, UQI)\n DEF_FUNCTION_TYPE (V4SF, PCV4SF, V4SF, UQI)\n DEF_FUNCTION_TYPE (V4DI, PCV4DI, V4DI, UQI)\n DEF_FUNCTION_TYPE (V4SI, PCV4SI, V4SI, UQI)\n DEF_FUNCTION_TYPE (V2DF, PCV2DF, V2DF, UQI)\n DEF_FUNCTION_TYPE (V2DI, PCV2DI, V2DI, UQI)\n+DEF_FUNCTION_TYPE (V64QI, PCCHAR, V64QI, UDI)\n+DEF_FUNCTION_TYPE (V32HI, PCSHORT, V32HI, USI)\n+DEF_FUNCTION_TYPE (V32QI, PCCHAR, V32QI, USI)\n+DEF_FUNCTION_TYPE (V16SF, PCFLOAT, V16SF, UHI)\n+DEF_FUNCTION_TYPE (V8DF, PCDOUBLE, V8DF, UQI)\n+DEF_FUNCTION_TYPE (V16SI, PCINT, V16SI, UHI)\n+DEF_FUNCTION_TYPE (V16HI, PCSHORT, V16HI, UHI)\n+DEF_FUNCTION_TYPE (V16QI, PCCHAR, V16QI, UHI)\n+DEF_FUNCTION_TYPE (V8SF, PCFLOAT, V8SF, UQI)\n+DEF_FUNCTION_TYPE (V8DI, PCINT64, V8DI, UQI)\n+DEF_FUNCTION_TYPE (V8SI, PCINT, V8SI, UQI)\n+DEF_FUNCTION_TYPE (V8HI, PCSHORT, V8HI, UQI)\n+DEF_FUNCTION_TYPE (V4DF, PCDOUBLE, V4DF, UQI)\n+DEF_FUNCTION_TYPE (V4SF, PCFLOAT, V4SF, UQI)\n+DEF_FUNCTION_TYPE (V4DI, PCINT64, V4DI, UQI)\n+DEF_FUNCTION_TYPE (V4SI, PCINT, V4SI, UQI)\n+DEF_FUNCTION_TYPE (V2DF, PCDOUBLE, V2DF, UQI)\n+DEF_FUNCTION_TYPE (V2DI, PCINT64, V2DI, UQI)\n DEF_FUNCTION_TYPE (V16HI, V16SI, V16HI, UHI)\n DEF_FUNCTION_TYPE (V8SI, V8DI, V8SI, UQI)\n DEF_FUNCTION_TYPE (V8HI, V8DI, V8HI, UQI)\n@@ -823,12 +837,24 @@ DEF_FUNCTION_TYPE (VOID, PV16QI, V4DI, UQI)\n DEF_FUNCTION_TYPE (VOID, PV16QI, V2DI, UQI)\n DEF_FUNCTION_TYPE (VOID, PV8SI, V8SI, UQI)\n DEF_FUNCTION_TYPE (VOID, PV4SI, V4SI, UQI)\n-DEF_FUNCTION_TYPE (VOID, PV32HI, V32HI, USI)\n-DEF_FUNCTION_TYPE (VOID, PV16HI, V16HI, UHI)\n-DEF_FUNCTION_TYPE (VOID, PV8HI, V8HI, UQI)\n-DEF_FUNCTION_TYPE (VOID, PV64QI, V64QI, UDI)\n-DEF_FUNCTION_TYPE (VOID, PV32QI, V32QI, USI)\n-DEF_FUNCTION_TYPE (VOID, PV16QI, V16QI, UHI)\n+DEF_FUNCTION_TYPE (VOID, PDOUBLE, V8DF, UQI)\n+DEF_FUNCTION_TYPE (VOID, PDOUBLE, V4DF, UQI)\n+DEF_FUNCTION_TYPE (VOID, PDOUBLE, V2DF, UQI)\n+DEF_FUNCTION_TYPE (VOID, PFLOAT, V16SF, UHI)\n+DEF_FUNCTION_TYPE (VOID, PFLOAT, V8SF, UQI)\n+DEF_FUNCTION_TYPE (VOID, PFLOAT, V4SF, UQI)\n+DEF_FUNCTION_TYPE (VOID, PINT64, V8DI, UQI)\n+DEF_FUNCTION_TYPE (VOID, PINT64, V4DI, UQI)\n+DEF_FUNCTION_TYPE (VOID, PINT64, V2DI, UQI)\n+DEF_FUNCTION_TYPE (VOID, PINT, V16SI, UHI)\n+DEF_FUNCTION_TYPE (VOID, PINT, V8SI, UQI)\n+DEF_FUNCTION_TYPE (VOID, PINT, V4SI, UQI)\n+DEF_FUNCTION_TYPE (VOID, PSHORT, V32HI, USI)\n+DEF_FUNCTION_TYPE (VOID, PSHORT, V16HI, UHI)\n+DEF_FUNCTION_TYPE (VOID, PSHORT, V8HI, UQI)\n+DEF_FUNCTION_TYPE (VOID, PCHAR, V64QI, UDI)\n+DEF_FUNCTION_TYPE (VOID, PCHAR, V32QI, USI)\n+DEF_FUNCTION_TYPE (VOID, PCHAR, V16QI, UHI)\n DEF_FUNCTION_TYPE (V8DI, V8DI, V8DI, V8DI, INT, UQI)\n DEF_FUNCTION_TYPE (V8SI, V8SF, V8SI, UQI)\n DEF_FUNCTION_TYPE (V4SI, V4SF, V4SI, UQI)"}, {"sha": "e491dded62b083bbade2442f407f7e5aade57150", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 104, "deletions": 138, "changes": 242, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "patch": "@@ -11706,7 +11706,6 @@ ix86_emit_save_reg_using_mov (machine_mode mode, unsigned int regno,\n {\n   struct machine_function *m = cfun->machine;\n   rtx reg = gen_rtx_REG (mode, regno);\n-  rtx unspec = NULL_RTX;\n   rtx mem, addr, base, insn;\n   unsigned int align;\n \n@@ -11717,13 +11716,7 @@ ix86_emit_save_reg_using_mov (machine_mode mode, unsigned int regno,\n   align = MIN (GET_MODE_ALIGNMENT (mode), INCOMING_STACK_BOUNDARY);\n   set_mem_align (mem, align);\n \n-  /* SSE saves are not within re-aligned local stack frame.\n-     In case INCOMING_STACK_BOUNDARY is misaligned, we have\n-     to emit unaligned store.  */\n-  if (mode == V4SFmode && align < 128)\n-    unspec = gen_rtx_UNSPEC (mode, gen_rtvec (1, reg), UNSPEC_STOREU);\n-\n-  insn = emit_insn (gen_rtx_SET (mem, unspec ? unspec : reg));\n+  insn = emit_insn (gen_rtx_SET (mem, reg));\n   RTX_FRAME_RELATED_P (insn) = 1;\n \n   base = addr;\n@@ -11770,8 +11763,6 @@ ix86_emit_save_reg_using_mov (machine_mode mode, unsigned int regno,\n       mem = gen_rtx_MEM (mode, addr);\n       add_reg_note (insn, REG_CFA_OFFSET, gen_rtx_SET (mem, reg));\n     }\n-  else if (unspec)\n-    add_reg_note (insn, REG_CFA_EXPRESSION, gen_rtx_SET (mem, reg));\n }\n \n /* Emit code to save registers using MOV insns.\n@@ -13323,18 +13314,7 @@ ix86_emit_restore_sse_regs_using_mov (HOST_WIDE_INT cfa_offset,\n \t/* The location is aligned up to INCOMING_STACK_BOUNDARY.  */\n \talign = MIN (GET_MODE_ALIGNMENT (V4SFmode), INCOMING_STACK_BOUNDARY);\n \tset_mem_align (mem, align);\n-\n-\t/* SSE saves are not within re-aligned local stack frame.\n-\t   In case INCOMING_STACK_BOUNDARY is misaligned, we have\n-\t   to emit unaligned load.  */\n-\tif (align < 128)\n-\t  {\n-\t    rtx unspec = gen_rtx_UNSPEC (V4SFmode, gen_rtvec (1, mem),\n-\t\t\t\t\t UNSPEC_LOADU);\n-\t    emit_insn (gen_rtx_SET (reg, unspec));\n-\t  }\n-\telse\n-\t  emit_insn (gen_rtx_SET (reg, mem));\n+\temit_insn (gen_rtx_SET (reg, mem));\n \n \tix86_add_cfa_restore_note (NULL, reg, cfa_offset);\n \n@@ -18837,8 +18817,6 @@ ix86_avx256_split_vector_move_misalign (rtx op0, rtx op1)\n {\n   rtx m;\n   rtx (*extract) (rtx, rtx, rtx);\n-  rtx (*load_unaligned) (rtx, rtx);\n-  rtx (*store_unaligned) (rtx, rtx);\n   machine_mode mode;\n \n   switch (GET_MODE (op0))\n@@ -18847,20 +18825,14 @@ ix86_avx256_split_vector_move_misalign (rtx op0, rtx op1)\n       gcc_unreachable ();\n     case V32QImode:\n       extract = gen_avx_vextractf128v32qi;\n-      load_unaligned = gen_avx_loaddquv32qi;\n-      store_unaligned = gen_avx_storedquv32qi;\n       mode = V16QImode;\n       break;\n     case V8SFmode:\n       extract = gen_avx_vextractf128v8sf;\n-      load_unaligned = gen_avx_loadups256;\n-      store_unaligned = gen_avx_storeups256;\n       mode = V4SFmode;\n       break;\n     case V4DFmode:\n       extract = gen_avx_vextractf128v4df;\n-      load_unaligned = gen_avx_loadupd256;\n-      store_unaligned = gen_avx_storeupd256;\n       mode = V2DFmode;\n       break;\n     }\n@@ -18877,14 +18849,8 @@ ix86_avx256_split_vector_move_misalign (rtx op0, rtx op1)\n \t  r = gen_rtx_VEC_CONCAT (GET_MODE (op0), r, m);\n \t  emit_move_insn (op0, r);\n \t}\n-      /* Normal *mov<mode>_internal pattern will handle\n-\t unaligned loads just fine if misaligned_operand\n-\t is true, and without the UNSPEC it can be combined\n-\t with arithmetic instructions.  */\n-      else if (misaligned_operand (op1, GET_MODE (op1)))\n-\temit_insn (gen_rtx_SET (op0, op1));\n       else\n-\temit_insn (load_unaligned (op0, op1));\n+\temit_insn (gen_rtx_SET (op0, op1));\n     }\n   else if (MEM_P (op0))\n     {\n@@ -18897,7 +18863,7 @@ ix86_avx256_split_vector_move_misalign (rtx op0, rtx op1)\n \t  emit_insn (extract (m, op1, const1_rtx));\n \t}\n       else\n-\temit_insn (store_unaligned (op0, op1));\n+\temit_insn (gen_rtx_SET (op0, op1));\n     }\n   else\n     gcc_unreachable ();\n@@ -18959,8 +18925,6 @@ void\n ix86_expand_vector_move_misalign (machine_mode mode, rtx operands[])\n {\n   rtx op0, op1, orig_op0 = NULL_RTX, m;\n-  rtx (*load_unaligned) (rtx, rtx);\n-  rtx (*store_unaligned) (rtx, rtx);\n \n   op0 = operands[0];\n   op1 = operands[1];\n@@ -18985,30 +18949,8 @@ ix86_expand_vector_move_misalign (machine_mode mode, rtx operands[])\n \t  /* FALLTHRU */\n \n \tcase MODE_VECTOR_FLOAT:\n-\t  switch (GET_MODE (op0))\n-\t    {\n-\t    default:\n-\t      gcc_unreachable ();\n-\t    case V16SImode:\n-\t      load_unaligned = gen_avx512f_loaddquv16si;\n-\t      store_unaligned = gen_avx512f_storedquv16si;\n-\t      break;\n-\t    case V16SFmode:\n-\t      load_unaligned = gen_avx512f_loadups512;\n-\t      store_unaligned = gen_avx512f_storeups512;\n-\t      break;\n-\t    case V8DFmode:\n-\t      load_unaligned = gen_avx512f_loadupd512;\n-\t      store_unaligned = gen_avx512f_storeupd512;\n-\t      break;\n-\t    }\n \n-\t  if (MEM_P (op1))\n-\t    emit_insn (load_unaligned (op0, op1));\n-\t  else if (MEM_P (op0))\n-\t    emit_insn (store_unaligned (op0, op1));\n-\t  else\n-\t    gcc_unreachable ();\n+\t  emit_insn (gen_rtx_SET (op0, op1));\n \t  if (orig_op0)\n \t    emit_move_insn (orig_op0, gen_lowpart (GET_MODE (orig_op0), op0));\n \t  break;\n@@ -19076,7 +19018,7 @@ ix86_expand_vector_move_misalign (machine_mode mode, rtx operands[])\n \t    }\n \t  op1 = gen_lowpart (V16QImode, op1);\n \t  /* We will eventually emit movups based on insn attributes.  */\n-\t  emit_insn (gen_sse2_loaddquv16qi (op0, op1));\n+\t  emit_insn (gen_rtx_SET (op0, op1));\n \t  if (orig_op0)\n \t    emit_move_insn (orig_op0, gen_lowpart (GET_MODE (orig_op0), op0));\n \t}\n@@ -19090,7 +19032,7 @@ ix86_expand_vector_move_misalign (machine_mode mode, rtx operands[])\n \t      || optimize_insn_for_size_p ())\n \t    {\n \t      /* We will eventually emit movups based on insn attributes.  */\n-\t      emit_insn (gen_sse2_loadupd (op0, op1));\n+\t      emit_insn (gen_rtx_SET (op0, op1));\n \t      return;\n \t    }\n \n@@ -19134,7 +19076,7 @@ ix86_expand_vector_move_misalign (machine_mode mode, rtx operands[])\n \t\t  op0 = gen_reg_rtx (V4SFmode);\n \t\t}\n \t      op1 = gen_lowpart (V4SFmode, op1);\n-\t      emit_insn (gen_sse_loadups (op0, op1));\n+\t      emit_insn (gen_rtx_SET (op0, op1));\n \t      if (orig_op0)\n \t\temit_move_insn (orig_op0,\n \t\t\t\tgen_lowpart (GET_MODE (orig_op0), op0));\n@@ -19166,7 +19108,7 @@ ix86_expand_vector_move_misalign (machine_mode mode, rtx operands[])\n \t  op0 = gen_lowpart (V16QImode, op0);\n \t  op1 = gen_lowpart (V16QImode, op1);\n \t  /* We will eventually emit movups based on insn attributes.  */\n-\t  emit_insn (gen_sse2_storedquv16qi (op0, op1));\n+\t  emit_insn (gen_rtx_SET (op0, op1));\n \t}\n       else if (TARGET_SSE2 && mode == V2DFmode)\n \t{\n@@ -19175,7 +19117,7 @@ ix86_expand_vector_move_misalign (machine_mode mode, rtx operands[])\n \t      || TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\n \t      || optimize_insn_for_size_p ())\n \t    /* We will eventually emit movups based on insn attributes.  */\n-\t    emit_insn (gen_sse2_storeupd (op0, op1));\n+\t    emit_insn (gen_rtx_SET (op0, op1));\n \t  else\n \t    {\n \t      m = adjust_address (op0, DFmode, 0);\n@@ -19195,7 +19137,7 @@ ix86_expand_vector_move_misalign (machine_mode mode, rtx operands[])\n \t      || optimize_insn_for_size_p ())\n \t    {\n \t      op0 = gen_lowpart (V4SFmode, op0);\n-\t      emit_insn (gen_sse_storeups (op0, op1));\n+\t      emit_insn (gen_rtx_SET (op0, op1));\n \t    }\n \t  else\n \t    {\n@@ -32654,9 +32596,9 @@ static const struct builtin_description bdesc_special_args[] =\n   { OPTION_MASK_ISA_XSAVEC | OPTION_MASK_ISA_64BIT, CODE_FOR_nothing, \"__builtin_ia32_xsavec64\", IX86_BUILTIN_XSAVEC64, UNKNOWN, (int) VOID_FTYPE_PVOID_INT64 },\n \n   /* SSE */\n-  { OPTION_MASK_ISA_SSE, CODE_FOR_sse_storeups, \"__builtin_ia32_storeups\", IX86_BUILTIN_STOREUPS, UNKNOWN, (int) VOID_FTYPE_PFLOAT_V4SF },\n+  { OPTION_MASK_ISA_SSE, CODE_FOR_movv4sf_internal, \"__builtin_ia32_storeups\", IX86_BUILTIN_STOREUPS, UNKNOWN, (int) VOID_FTYPE_PFLOAT_V4SF },\n   { OPTION_MASK_ISA_SSE, CODE_FOR_sse_movntv4sf, \"__builtin_ia32_movntps\", IX86_BUILTIN_MOVNTPS, UNKNOWN, (int) VOID_FTYPE_PFLOAT_V4SF },\n-  { OPTION_MASK_ISA_SSE, CODE_FOR_sse_loadups, \"__builtin_ia32_loadups\", IX86_BUILTIN_LOADUPS, UNKNOWN, (int) V4SF_FTYPE_PCFLOAT },\n+  { OPTION_MASK_ISA_SSE, CODE_FOR_movv4sf_internal, \"__builtin_ia32_loadups\", IX86_BUILTIN_LOADUPS, UNKNOWN, (int) V4SF_FTYPE_PCFLOAT },\n \n   { OPTION_MASK_ISA_SSE, CODE_FOR_sse_loadhps_exp, \"__builtin_ia32_loadhps\", IX86_BUILTIN_LOADHPS, UNKNOWN, (int) V4SF_FTYPE_V4SF_PCV2SF },\n   { OPTION_MASK_ISA_SSE, CODE_FOR_sse_loadlps_exp, \"__builtin_ia32_loadlps\", IX86_BUILTIN_LOADLPS, UNKNOWN, (int) V4SF_FTYPE_V4SF_PCV2SF },\n@@ -32670,14 +32612,14 @@ static const struct builtin_description bdesc_special_args[] =\n   /* SSE2 */\n   { OPTION_MASK_ISA_SSE2, CODE_FOR_sse2_lfence, \"__builtin_ia32_lfence\", IX86_BUILTIN_LFENCE, UNKNOWN, (int) VOID_FTYPE_VOID },\n   { OPTION_MASK_ISA_SSE2, CODE_FOR_sse2_mfence, 0, IX86_BUILTIN_MFENCE, UNKNOWN, (int) VOID_FTYPE_VOID },\n-  { OPTION_MASK_ISA_SSE2, CODE_FOR_sse2_storeupd, \"__builtin_ia32_storeupd\", IX86_BUILTIN_STOREUPD, UNKNOWN, (int) VOID_FTYPE_PDOUBLE_V2DF },\n-  { OPTION_MASK_ISA_SSE2, CODE_FOR_sse2_storedquv16qi, \"__builtin_ia32_storedqu\", IX86_BUILTIN_STOREDQU, UNKNOWN, (int) VOID_FTYPE_PCHAR_V16QI },\n+  { OPTION_MASK_ISA_SSE2, CODE_FOR_movv2df_internal, \"__builtin_ia32_storeupd\", IX86_BUILTIN_STOREUPD, UNKNOWN, (int) VOID_FTYPE_PDOUBLE_V2DF },\n+  { OPTION_MASK_ISA_SSE2, CODE_FOR_movv16qi_internal, \"__builtin_ia32_storedqu\", IX86_BUILTIN_STOREDQU, UNKNOWN, (int) VOID_FTYPE_PCHAR_V16QI },\n   { OPTION_MASK_ISA_SSE2, CODE_FOR_sse2_movntv2df, \"__builtin_ia32_movntpd\", IX86_BUILTIN_MOVNTPD, UNKNOWN, (int) VOID_FTYPE_PDOUBLE_V2DF },\n   { OPTION_MASK_ISA_SSE2, CODE_FOR_sse2_movntv2di, \"__builtin_ia32_movntdq\", IX86_BUILTIN_MOVNTDQ, UNKNOWN, (int) VOID_FTYPE_PV2DI_V2DI },\n   { OPTION_MASK_ISA_SSE2, CODE_FOR_sse2_movntisi, \"__builtin_ia32_movnti\", IX86_BUILTIN_MOVNTI, UNKNOWN, (int) VOID_FTYPE_PINT_INT },\n   { OPTION_MASK_ISA_SSE2 | OPTION_MASK_ISA_64BIT, CODE_FOR_sse2_movntidi, \"__builtin_ia32_movnti64\", IX86_BUILTIN_MOVNTI64, UNKNOWN, (int) VOID_FTYPE_PLONGLONG_LONGLONG },\n-  { OPTION_MASK_ISA_SSE2, CODE_FOR_sse2_loadupd, \"__builtin_ia32_loadupd\", IX86_BUILTIN_LOADUPD, UNKNOWN, (int) V2DF_FTYPE_PCDOUBLE },\n-  { OPTION_MASK_ISA_SSE2, CODE_FOR_sse2_loaddquv16qi, \"__builtin_ia32_loaddqu\", IX86_BUILTIN_LOADDQU, UNKNOWN, (int) V16QI_FTYPE_PCCHAR },\n+  { OPTION_MASK_ISA_SSE2, CODE_FOR_movv2df_internal, \"__builtin_ia32_loadupd\", IX86_BUILTIN_LOADUPD, UNKNOWN, (int) V2DF_FTYPE_PCDOUBLE },\n+  { OPTION_MASK_ISA_SSE2, CODE_FOR_movv16qi_internal, \"__builtin_ia32_loaddqu\", IX86_BUILTIN_LOADDQU, UNKNOWN, (int) V16QI_FTYPE_PCCHAR },\n \n   { OPTION_MASK_ISA_SSE2, CODE_FOR_sse2_loadhpd_exp, \"__builtin_ia32_loadhpd\", IX86_BUILTIN_LOADHPD, UNKNOWN, (int) V2DF_FTYPE_V2DF_PCDOUBLE },\n   { OPTION_MASK_ISA_SSE2, CODE_FOR_sse2_loadlpd_exp, \"__builtin_ia32_loadlpd\", IX86_BUILTIN_LOADLPD, UNKNOWN, (int) V2DF_FTYPE_V2DF_PCDOUBLE },\n@@ -32702,12 +32644,12 @@ static const struct builtin_description bdesc_special_args[] =\n   { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vbroadcastf128_v4df, \"__builtin_ia32_vbroadcastf128_pd256\", IX86_BUILTIN_VBROADCASTPD256, UNKNOWN, (int) V4DF_FTYPE_PCV2DF },\n   { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vbroadcastf128_v8sf, \"__builtin_ia32_vbroadcastf128_ps256\", IX86_BUILTIN_VBROADCASTPS256, UNKNOWN, (int) V8SF_FTYPE_PCV4SF },\n \n-  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_loadupd256, \"__builtin_ia32_loadupd256\", IX86_BUILTIN_LOADUPD256, UNKNOWN, (int) V4DF_FTYPE_PCDOUBLE },\n-  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_loadups256, \"__builtin_ia32_loadups256\", IX86_BUILTIN_LOADUPS256, UNKNOWN, (int) V8SF_FTYPE_PCFLOAT },\n-  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_storeupd256, \"__builtin_ia32_storeupd256\", IX86_BUILTIN_STOREUPD256, UNKNOWN, (int) VOID_FTYPE_PDOUBLE_V4DF },\n-  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_storeups256, \"__builtin_ia32_storeups256\", IX86_BUILTIN_STOREUPS256, UNKNOWN, (int) VOID_FTYPE_PFLOAT_V8SF },\n-  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_loaddquv32qi, \"__builtin_ia32_loaddqu256\", IX86_BUILTIN_LOADDQU256, UNKNOWN, (int) V32QI_FTYPE_PCCHAR },\n-  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_storedquv32qi, \"__builtin_ia32_storedqu256\", IX86_BUILTIN_STOREDQU256, UNKNOWN, (int) VOID_FTYPE_PCHAR_V32QI },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_movv4df_internal, \"__builtin_ia32_loadupd256\", IX86_BUILTIN_LOADUPD256, UNKNOWN, (int) V4DF_FTYPE_PCDOUBLE },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_movv8sf_internal, \"__builtin_ia32_loadups256\", IX86_BUILTIN_LOADUPS256, UNKNOWN, (int) V8SF_FTYPE_PCFLOAT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_movv4df_internal, \"__builtin_ia32_storeupd256\", IX86_BUILTIN_STOREUPD256, UNKNOWN, (int) VOID_FTYPE_PDOUBLE_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_movv8sf_internal, \"__builtin_ia32_storeups256\", IX86_BUILTIN_STOREUPS256, UNKNOWN, (int) VOID_FTYPE_PFLOAT_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_movv32qi_internal, \"__builtin_ia32_loaddqu256\", IX86_BUILTIN_LOADDQU256, UNKNOWN, (int) V32QI_FTYPE_PCCHAR },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_movv32qi_internal, \"__builtin_ia32_storedqu256\", IX86_BUILTIN_STOREDQU256, UNKNOWN, (int) VOID_FTYPE_PCHAR_V32QI },\n   { OPTION_MASK_ISA_AVX, CODE_FOR_avx_lddqu256, \"__builtin_ia32_lddqu256\", IX86_BUILTIN_LDDQU256, UNKNOWN, (int) V32QI_FTYPE_PCCHAR },\n \n   { OPTION_MASK_ISA_AVX, CODE_FOR_avx_movntv4di, \"__builtin_ia32_movntdq256\", IX86_BUILTIN_MOVNTDQ256, UNKNOWN, (int) VOID_FTYPE_PV4DI_V4DI },\n@@ -32747,10 +32689,10 @@ static const struct builtin_description bdesc_special_args[] =\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_expandv8df_maskz, \"__builtin_ia32_expandloaddf512_maskz\", IX86_BUILTIN_EXPANDPDLOAD512Z, UNKNOWN, (int) V8DF_FTYPE_PCV8DF_V8DF_UQI },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_expandv8di_mask, \"__builtin_ia32_expandloaddi512_mask\", IX86_BUILTIN_PEXPANDQLOAD512, UNKNOWN, (int) V8DI_FTYPE_PCV8DI_V8DI_UQI },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_expandv8di_maskz, \"__builtin_ia32_expandloaddi512_maskz\", IX86_BUILTIN_PEXPANDQLOAD512Z, UNKNOWN, (int) V8DI_FTYPE_PCV8DI_V8DI_UQI },\n-  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_loaddquv16si_mask, \"__builtin_ia32_loaddqusi512_mask\", IX86_BUILTIN_LOADDQUSI512, UNKNOWN, (int) V16SI_FTYPE_PCV16SI_V16SI_UHI },\n-  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_loaddquv8di_mask, \"__builtin_ia32_loaddqudi512_mask\", IX86_BUILTIN_LOADDQUDI512, UNKNOWN, (int) V8DI_FTYPE_PCV8DI_V8DI_UQI },\n-  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_loadupd512_mask, \"__builtin_ia32_loadupd512_mask\", IX86_BUILTIN_LOADUPD512, UNKNOWN, (int) V8DF_FTYPE_PCV8DF_V8DF_UQI },\n-  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_loadups512_mask, \"__builtin_ia32_loadups512_mask\", IX86_BUILTIN_LOADUPS512, UNKNOWN, (int) V16SF_FTYPE_PCV16SF_V16SF_UHI },\n+  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_loadv16si_mask, \"__builtin_ia32_loaddqusi512_mask\", IX86_BUILTIN_LOADDQUSI512, UNKNOWN, (int) V16SI_FTYPE_PCINT_V16SI_UHI },\n+  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_loadv8di_mask, \"__builtin_ia32_loaddqudi512_mask\", IX86_BUILTIN_LOADDQUDI512, UNKNOWN, (int) V8DI_FTYPE_PCINT64_V8DI_UQI },\n+  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_loadv8df_mask, \"__builtin_ia32_loadupd512_mask\", IX86_BUILTIN_LOADUPD512, UNKNOWN, (int) V8DF_FTYPE_PCDOUBLE_V8DF_UQI },\n+  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_loadv16sf_mask, \"__builtin_ia32_loadups512_mask\", IX86_BUILTIN_LOADUPS512, UNKNOWN, (int) V16SF_FTYPE_PCFLOAT_V16SF_UHI },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_loadv16sf_mask, \"__builtin_ia32_loadaps512_mask\", IX86_BUILTIN_LOADAPS512, UNKNOWN, (int) V16SF_FTYPE_PCV16SF_V16SF_UHI },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_loadv16si_mask, \"__builtin_ia32_movdqa32load512_mask\", IX86_BUILTIN_MOVDQA32LOAD512, UNKNOWN, (int) V16SI_FTYPE_PCV16SI_V16SI_UHI },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_loadv8df_mask, \"__builtin_ia32_loadapd512_mask\", IX86_BUILTIN_LOADAPD512, UNKNOWN, (int) V8DF_FTYPE_PCV8DF_V8DF_UQI },\n@@ -32759,9 +32701,9 @@ static const struct builtin_description bdesc_special_args[] =\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_movntv8df, \"__builtin_ia32_movntpd512\", IX86_BUILTIN_MOVNTPD512, UNKNOWN, (int) VOID_FTYPE_PDOUBLE_V8DF },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_movntv8di, \"__builtin_ia32_movntdq512\", IX86_BUILTIN_MOVNTDQ512, UNKNOWN, (int) VOID_FTYPE_PV8DI_V8DI },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_movntdqa, \"__builtin_ia32_movntdqa512\", IX86_BUILTIN_MOVNTDQA512, UNKNOWN, (int) V8DI_FTYPE_PV8DI },\n-  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_storedquv16si_mask, \"__builtin_ia32_storedqusi512_mask\", IX86_BUILTIN_STOREDQUSI512, UNKNOWN, (int) VOID_FTYPE_PV16SI_V16SI_UHI },\n-  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_storedquv8di_mask, \"__builtin_ia32_storedqudi512_mask\", IX86_BUILTIN_STOREDQUDI512, UNKNOWN, (int) VOID_FTYPE_PV8DI_V8DI_UQI },\n-  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_storeupd512_mask, \"__builtin_ia32_storeupd512_mask\", IX86_BUILTIN_STOREUPD512, UNKNOWN, (int) VOID_FTYPE_PV8DF_V8DF_UQI },\n+  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_storev16si_mask, \"__builtin_ia32_storedqusi512_mask\", IX86_BUILTIN_STOREDQUSI512, UNKNOWN, (int) VOID_FTYPE_PINT_V16SI_UHI },\n+  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_storev8di_mask, \"__builtin_ia32_storedqudi512_mask\", IX86_BUILTIN_STOREDQUDI512, UNKNOWN, (int) VOID_FTYPE_PINT64_V8DI_UQI },\n+  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_storev8df_mask, \"__builtin_ia32_storeupd512_mask\", IX86_BUILTIN_STOREUPD512, UNKNOWN, (int) VOID_FTYPE_PDOUBLE_V8DF_UQI },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_us_truncatev8div8si2_mask_store, \"__builtin_ia32_pmovusqd512mem_mask\", IX86_BUILTIN_PMOVUSQD512_MEM, UNKNOWN, (int) VOID_FTYPE_PV8SI_V8DI_UQI },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_ss_truncatev8div8si2_mask_store, \"__builtin_ia32_pmovsqd512mem_mask\", IX86_BUILTIN_PMOVSQD512_MEM, UNKNOWN, (int) VOID_FTYPE_PV8SI_V8DI_UQI },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_truncatev8div8si2_mask_store, \"__builtin_ia32_pmovqd512mem_mask\", IX86_BUILTIN_PMOVQD512_MEM, UNKNOWN, (int) VOID_FTYPE_PV8SI_V8DI_UQI },\n@@ -32777,7 +32719,7 @@ static const struct builtin_description bdesc_special_args[] =\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_us_truncatev16siv16qi2_mask_store, \"__builtin_ia32_pmovusdb512mem_mask\", IX86_BUILTIN_PMOVUSDB512_MEM, UNKNOWN, (int) VOID_FTYPE_PV16QI_V16SI_UHI },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_ss_truncatev16siv16qi2_mask_store, \"__builtin_ia32_pmovsdb512mem_mask\", IX86_BUILTIN_PMOVSDB512_MEM, UNKNOWN, (int) VOID_FTYPE_PV16QI_V16SI_UHI },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_truncatev16siv16qi2_mask_store, \"__builtin_ia32_pmovdb512mem_mask\", IX86_BUILTIN_PMOVDB512_MEM, UNKNOWN, (int) VOID_FTYPE_PV16QI_V16SI_UHI },\n-  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_storeups512_mask, \"__builtin_ia32_storeups512_mask\", IX86_BUILTIN_STOREUPS512, UNKNOWN, (int) VOID_FTYPE_PV16SF_V16SF_UHI },\n+  { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_storev16sf_mask, \"__builtin_ia32_storeups512_mask\", IX86_BUILTIN_STOREUPS512, UNKNOWN, (int) VOID_FTYPE_PFLOAT_V16SF_UHI },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_storev16sf_mask, \"__builtin_ia32_storeaps512_mask\", IX86_BUILTIN_STOREAPS512, UNKNOWN, (int) VOID_FTYPE_PV16SF_V16SF_UHI },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_storev16si_mask, \"__builtin_ia32_movdqa32store512_mask\", IX86_BUILTIN_MOVDQA32STORE512, UNKNOWN, (int) VOID_FTYPE_PV16SI_V16SI_UHI },\n   { OPTION_MASK_ISA_AVX512F, CODE_FOR_avx512f_storev8df_mask, \"__builtin_ia32_storeapd512_mask\", IX86_BUILTIN_STOREAPD512, UNKNOWN, (int) VOID_FTYPE_PV8DF_V8DF_UQI },\n@@ -32806,16 +32748,16 @@ static const struct builtin_description bdesc_special_args[] =\n   { OPTION_MASK_ISA_RTM, CODE_FOR_xtest, \"__builtin_ia32_xtest\", IX86_BUILTIN_XTEST, UNKNOWN, (int) INT_FTYPE_VOID },\n \n   /* AVX512BW */\n-  { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_loaddquv32hi_mask, \"__builtin_ia32_loaddquhi512_mask\", IX86_BUILTIN_LOADDQUHI512_MASK, UNKNOWN, (int) V32HI_FTYPE_PCV32HI_V32HI_USI },\n-  { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512f_loaddquv64qi_mask, \"__builtin_ia32_loaddquqi512_mask\", IX86_BUILTIN_LOADDQUQI512_MASK, UNKNOWN, (int) V64QI_FTYPE_PCV64QI_V64QI_UDI },\n-  { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_storedquv32hi_mask, \"__builtin_ia32_storedquhi512_mask\", IX86_BUILTIN_STOREDQUHI512_MASK, UNKNOWN, (int) VOID_FTYPE_PV32HI_V32HI_USI },\n-  { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_storedquv64qi_mask, \"__builtin_ia32_storedquqi512_mask\", IX86_BUILTIN_STOREDQUQI512_MASK, UNKNOWN, (int) VOID_FTYPE_PV64QI_V64QI_UDI },\n+  { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_loadv32hi_mask, \"__builtin_ia32_loaddquhi512_mask\", IX86_BUILTIN_LOADDQUHI512_MASK, UNKNOWN, (int) V32HI_FTYPE_PCSHORT_V32HI_USI },\n+  { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_loadv64qi_mask, \"__builtin_ia32_loaddquqi512_mask\", IX86_BUILTIN_LOADDQUQI512_MASK, UNKNOWN, (int) V64QI_FTYPE_PCCHAR_V64QI_UDI },\n+  { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_storev32hi_mask, \"__builtin_ia32_storedquhi512_mask\", IX86_BUILTIN_STOREDQUHI512_MASK, UNKNOWN, (int) VOID_FTYPE_PSHORT_V32HI_USI },\n+  { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_storev64qi_mask, \"__builtin_ia32_storedquqi512_mask\", IX86_BUILTIN_STOREDQUQI512_MASK, UNKNOWN, (int) VOID_FTYPE_PCHAR_V64QI_UDI },\n \n   /* AVX512VL */\n-  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loaddquv16hi_mask, \"__builtin_ia32_loaddquhi256_mask\", IX86_BUILTIN_LOADDQUHI256_MASK, UNKNOWN, (int) V16HI_FTYPE_PCV16HI_V16HI_UHI },\n-  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loaddquv8hi_mask, \"__builtin_ia32_loaddquhi128_mask\", IX86_BUILTIN_LOADDQUHI128_MASK, UNKNOWN, (int) V8HI_FTYPE_PCV8HI_V8HI_UQI },\n-  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx_loaddquv32qi_mask, \"__builtin_ia32_loaddquqi256_mask\", IX86_BUILTIN_LOADDQUQI256_MASK, UNKNOWN, (int) V32QI_FTYPE_PCV32QI_V32QI_USI },\n-  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_sse2_loaddquv16qi_mask, \"__builtin_ia32_loaddquqi128_mask\", IX86_BUILTIN_LOADDQUQI128_MASK, UNKNOWN, (int) V16QI_FTYPE_PCV16QI_V16QI_UHI },\n+  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv16hi_mask, \"__builtin_ia32_loaddquhi256_mask\", IX86_BUILTIN_LOADDQUHI256_MASK, UNKNOWN, (int) V16HI_FTYPE_PCSHORT_V16HI_UHI },\n+  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv8hi_mask, \"__builtin_ia32_loaddquhi128_mask\", IX86_BUILTIN_LOADDQUHI128_MASK, UNKNOWN, (int) V8HI_FTYPE_PCSHORT_V8HI_UQI },\n+  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv32qi_mask, \"__builtin_ia32_loaddquqi256_mask\", IX86_BUILTIN_LOADDQUQI256_MASK, UNKNOWN, (int) V32QI_FTYPE_PCCHAR_V32QI_USI },\n+  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv16qi_mask, \"__builtin_ia32_loaddquqi128_mask\", IX86_BUILTIN_LOADDQUQI128_MASK, UNKNOWN, (int) V16QI_FTYPE_PCCHAR_V16QI_UHI },\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv4di_mask, \"__builtin_ia32_movdqa64load256_mask\", IX86_BUILTIN_MOVDQA64LOAD256_MASK, UNKNOWN, (int) V4DI_FTYPE_PCV4DI_V4DI_UQI },\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv2di_mask, \"__builtin_ia32_movdqa64load128_mask\", IX86_BUILTIN_MOVDQA64LOAD128_MASK, UNKNOWN, (int) V2DI_FTYPE_PCV2DI_V2DI_UQI },\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv8si_mask, \"__builtin_ia32_movdqa32load256_mask\", IX86_BUILTIN_MOVDQA32LOAD256_MASK, UNKNOWN, (int) V8SI_FTYPE_PCV8SI_V8SI_UQI },\n@@ -32832,26 +32774,26 @@ static const struct builtin_description bdesc_special_args[] =\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev2df_mask, \"__builtin_ia32_storeapd128_mask\", IX86_BUILTIN_STOREAPD128_MASK, UNKNOWN, (int) VOID_FTYPE_PV2DF_V2DF_UQI },\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev8sf_mask, \"__builtin_ia32_storeaps256_mask\", IX86_BUILTIN_STOREAPS256_MASK, UNKNOWN, (int) VOID_FTYPE_PV8SF_V8SF_UQI },\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev4sf_mask, \"__builtin_ia32_storeaps128_mask\", IX86_BUILTIN_STOREAPS128_MASK, UNKNOWN, (int) VOID_FTYPE_PV4SF_V4SF_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx_loadupd256_mask, \"__builtin_ia32_loadupd256_mask\", IX86_BUILTIN_LOADUPD256_MASK, UNKNOWN, (int) V4DF_FTYPE_PCV4DF_V4DF_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_sse2_loadupd_mask, \"__builtin_ia32_loadupd128_mask\", IX86_BUILTIN_LOADUPD128_MASK, UNKNOWN, (int) V2DF_FTYPE_PCV2DF_V2DF_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx_loadups256_mask, \"__builtin_ia32_loadups256_mask\", IX86_BUILTIN_LOADUPS256_MASK, UNKNOWN, (int) V8SF_FTYPE_PCV8SF_V8SF_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_sse_loadups_mask, \"__builtin_ia32_loadups128_mask\", IX86_BUILTIN_LOADUPS128_MASK, UNKNOWN, (int) V4SF_FTYPE_PCV4SF_V4SF_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storeupd256_mask, \"__builtin_ia32_storeupd256_mask\", IX86_BUILTIN_STOREUPD256_MASK, UNKNOWN, (int) VOID_FTYPE_PV4DF_V4DF_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storeupd_mask, \"__builtin_ia32_storeupd128_mask\", IX86_BUILTIN_STOREUPD128_MASK, UNKNOWN, (int) VOID_FTYPE_PV2DF_V2DF_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storeups256_mask, \"__builtin_ia32_storeups256_mask\", IX86_BUILTIN_STOREUPS256_MASK, UNKNOWN, (int) VOID_FTYPE_PV8SF_V8SF_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storeups_mask, \"__builtin_ia32_storeups128_mask\", IX86_BUILTIN_STOREUPS128_MASK, UNKNOWN, (int) VOID_FTYPE_PV4SF_V4SF_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loaddquv4di_mask, \"__builtin_ia32_loaddqudi256_mask\", IX86_BUILTIN_LOADDQUDI256_MASK, UNKNOWN, (int) V4DI_FTYPE_PCV4DI_V4DI_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loaddquv2di_mask, \"__builtin_ia32_loaddqudi128_mask\", IX86_BUILTIN_LOADDQUDI128_MASK, UNKNOWN, (int) V2DI_FTYPE_PCV2DI_V2DI_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx_loaddquv8si_mask, \"__builtin_ia32_loaddqusi256_mask\", IX86_BUILTIN_LOADDQUSI256_MASK, UNKNOWN, (int) V8SI_FTYPE_PCV8SI_V8SI_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_sse2_loaddquv4si_mask, \"__builtin_ia32_loaddqusi128_mask\", IX86_BUILTIN_LOADDQUSI128_MASK, UNKNOWN, (int) V4SI_FTYPE_PCV4SI_V4SI_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storedquv4di_mask, \"__builtin_ia32_storedqudi256_mask\", IX86_BUILTIN_STOREDQUDI256_MASK, UNKNOWN, (int) VOID_FTYPE_PV4DI_V4DI_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storedquv2di_mask, \"__builtin_ia32_storedqudi128_mask\", IX86_BUILTIN_STOREDQUDI128_MASK, UNKNOWN, (int) VOID_FTYPE_PV2DI_V2DI_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storedquv8si_mask, \"__builtin_ia32_storedqusi256_mask\", IX86_BUILTIN_STOREDQUSI256_MASK, UNKNOWN, (int) VOID_FTYPE_PV8SI_V8SI_UQI },\n-  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storedquv4si_mask, \"__builtin_ia32_storedqusi128_mask\", IX86_BUILTIN_STOREDQUSI128_MASK, UNKNOWN, (int) VOID_FTYPE_PV4SI_V4SI_UQI },\n-  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storedquv16hi_mask, \"__builtin_ia32_storedquhi256_mask\", IX86_BUILTIN_STOREDQUHI256_MASK, UNKNOWN, (int) VOID_FTYPE_PV16HI_V16HI_UHI },\n-  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storedquv8hi_mask, \"__builtin_ia32_storedquhi128_mask\", IX86_BUILTIN_STOREDQUHI128_MASK, UNKNOWN, (int) VOID_FTYPE_PV8HI_V8HI_UQI },\n-  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storedquv32qi_mask, \"__builtin_ia32_storedquqi256_mask\", IX86_BUILTIN_STOREDQUQI256_MASK, UNKNOWN, (int) VOID_FTYPE_PV32QI_V32QI_USI },\n-  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storedquv16qi_mask, \"__builtin_ia32_storedquqi128_mask\", IX86_BUILTIN_STOREDQUQI128_MASK, UNKNOWN, (int) VOID_FTYPE_PV16QI_V16QI_UHI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv4df_mask, \"__builtin_ia32_loadupd256_mask\", IX86_BUILTIN_LOADUPD256_MASK, UNKNOWN, (int) V4DF_FTYPE_PCDOUBLE_V4DF_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv2df_mask, \"__builtin_ia32_loadupd128_mask\", IX86_BUILTIN_LOADUPD128_MASK, UNKNOWN, (int) V2DF_FTYPE_PCDOUBLE_V2DF_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv8sf_mask, \"__builtin_ia32_loadups256_mask\", IX86_BUILTIN_LOADUPS256_MASK, UNKNOWN, (int) V8SF_FTYPE_PCFLOAT_V8SF_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv4sf_mask, \"__builtin_ia32_loadups128_mask\", IX86_BUILTIN_LOADUPS128_MASK, UNKNOWN, (int) V4SF_FTYPE_PCFLOAT_V4SF_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev4df_mask, \"__builtin_ia32_storeupd256_mask\", IX86_BUILTIN_STOREUPD256_MASK, UNKNOWN, (int) VOID_FTYPE_PDOUBLE_V4DF_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev2df_mask, \"__builtin_ia32_storeupd128_mask\", IX86_BUILTIN_STOREUPD128_MASK, UNKNOWN, (int) VOID_FTYPE_PDOUBLE_V2DF_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev8sf_mask, \"__builtin_ia32_storeups256_mask\", IX86_BUILTIN_STOREUPS256_MASK, UNKNOWN, (int) VOID_FTYPE_PFLOAT_V8SF_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev4sf_mask, \"__builtin_ia32_storeups128_mask\", IX86_BUILTIN_STOREUPS128_MASK, UNKNOWN, (int) VOID_FTYPE_PFLOAT_V4SF_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv4di_mask, \"__builtin_ia32_loaddqudi256_mask\", IX86_BUILTIN_LOADDQUDI256_MASK, UNKNOWN, (int) V4DI_FTYPE_PCINT64_V4DI_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv2di_mask, \"__builtin_ia32_loaddqudi128_mask\", IX86_BUILTIN_LOADDQUDI128_MASK, UNKNOWN, (int) V2DI_FTYPE_PCINT64_V2DI_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv8si_mask, \"__builtin_ia32_loaddqusi256_mask\", IX86_BUILTIN_LOADDQUSI256_MASK, UNKNOWN, (int) V8SI_FTYPE_PCINT_V8SI_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv4si_mask, \"__builtin_ia32_loaddqusi128_mask\", IX86_BUILTIN_LOADDQUSI128_MASK, UNKNOWN, (int) V4SI_FTYPE_PCINT_V4SI_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev4di_mask, \"__builtin_ia32_storedqudi256_mask\", IX86_BUILTIN_STOREDQUDI256_MASK, UNKNOWN, (int) VOID_FTYPE_PINT64_V4DI_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev2di_mask, \"__builtin_ia32_storedqudi128_mask\", IX86_BUILTIN_STOREDQUDI128_MASK, UNKNOWN, (int) VOID_FTYPE_PINT64_V2DI_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev8si_mask, \"__builtin_ia32_storedqusi256_mask\", IX86_BUILTIN_STOREDQUSI256_MASK, UNKNOWN, (int) VOID_FTYPE_PINT_V8SI_UQI },\n+  { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev4si_mask, \"__builtin_ia32_storedqusi128_mask\", IX86_BUILTIN_STOREDQUSI128_MASK, UNKNOWN, (int) VOID_FTYPE_PINT_V4SI_UQI },\n+  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev16hi_mask, \"__builtin_ia32_storedquhi256_mask\", IX86_BUILTIN_STOREDQUHI256_MASK, UNKNOWN, (int) VOID_FTYPE_PSHORT_V16HI_UHI },\n+  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev8hi_mask, \"__builtin_ia32_storedquhi128_mask\", IX86_BUILTIN_STOREDQUHI128_MASK, UNKNOWN, (int) VOID_FTYPE_PSHORT_V8HI_UQI },\n+  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev32qi_mask, \"__builtin_ia32_storedquqi256_mask\", IX86_BUILTIN_STOREDQUQI256_MASK, UNKNOWN, (int) VOID_FTYPE_PCHAR_V32QI_USI },\n+  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_storev16qi_mask, \"__builtin_ia32_storedquqi128_mask\", IX86_BUILTIN_STOREDQUQI128_MASK, UNKNOWN, (int) VOID_FTYPE_PCHAR_V16QI_UHI },\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_compressstorev4df_mask, \"__builtin_ia32_compressstoredf256_mask\", IX86_BUILTIN_COMPRESSPDSTORE256, UNKNOWN, (int) VOID_FTYPE_PV4DF_V4DF_UQI },\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_compressstorev2df_mask, \"__builtin_ia32_compressstoredf128_mask\", IX86_BUILTIN_COMPRESSPDSTORE128, UNKNOWN, (int) VOID_FTYPE_PV2DF_V2DF_UQI },\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_compressstorev8sf_mask, \"__builtin_ia32_compressstoresf256_mask\", IX86_BUILTIN_COMPRESSPSSTORE256, UNKNOWN, (int) VOID_FTYPE_PV8SF_V8SF_UQI },\n@@ -33983,10 +33925,10 @@ static const struct builtin_description bdesc_args[] =\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv2df_mask, \"__builtin_ia32_movapd128_mask\", IX86_BUILTIN_MOVAPD128_MASK, UNKNOWN, (int) V2DF_FTYPE_V2DF_V2DF_UQI },\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv8sf_mask, \"__builtin_ia32_movaps256_mask\", IX86_BUILTIN_MOVAPS256_MASK, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF_UQI },\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv4sf_mask, \"__builtin_ia32_movaps128_mask\", IX86_BUILTIN_MOVAPS128_MASK, UNKNOWN, (int) V4SF_FTYPE_V4SF_V4SF_UQI },\n-  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loaddquv16hi_mask, \"__builtin_ia32_movdquhi256_mask\", IX86_BUILTIN_MOVDQUHI256_MASK, UNKNOWN, (int) V16HI_FTYPE_V16HI_V16HI_UHI },\n-  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loaddquv8hi_mask, \"__builtin_ia32_movdquhi128_mask\", IX86_BUILTIN_MOVDQUHI128_MASK, UNKNOWN, (int) V8HI_FTYPE_V8HI_V8HI_UQI },\n-  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx_loaddquv32qi_mask, \"__builtin_ia32_movdquqi256_mask\", IX86_BUILTIN_MOVDQUQI256_MASK, UNKNOWN, (int) V32QI_FTYPE_V32QI_V32QI_USI },\n-  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_sse2_loaddquv16qi_mask, \"__builtin_ia32_movdquqi128_mask\", IX86_BUILTIN_MOVDQUQI128_MASK, UNKNOWN, (int) V16QI_FTYPE_V16QI_V16QI_UHI },\n+  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv16hi_mask, \"__builtin_ia32_movdquhi256_mask\", IX86_BUILTIN_MOVDQUHI256_MASK, UNKNOWN, (int) V16HI_FTYPE_V16HI_V16HI_UHI },\n+  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv8hi_mask, \"__builtin_ia32_movdquhi128_mask\", IX86_BUILTIN_MOVDQUHI128_MASK, UNKNOWN, (int) V8HI_FTYPE_V8HI_V8HI_UQI },\n+  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv32qi_mask, \"__builtin_ia32_movdquqi256_mask\", IX86_BUILTIN_MOVDQUQI256_MASK, UNKNOWN, (int) V32QI_FTYPE_V32QI_V32QI_USI },\n+  { OPTION_MASK_ISA_AVX512BW | OPTION_MASK_ISA_AVX512VL, CODE_FOR_avx512vl_loadv16qi_mask, \"__builtin_ia32_movdquqi128_mask\", IX86_BUILTIN_MOVDQUQI128_MASK, UNKNOWN, (int) V16QI_FTYPE_V16QI_V16QI_UHI },\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_sminv4sf3_mask, \"__builtin_ia32_minps_mask\", IX86_BUILTIN_MINPS128_MASK, UNKNOWN, (int) V4SF_FTYPE_V4SF_V4SF_V4SF_UQI },\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_smaxv4sf3_mask, \"__builtin_ia32_maxps_mask\", IX86_BUILTIN_MAXPS128_MASK, UNKNOWN, (int) V4SF_FTYPE_V4SF_V4SF_V4SF_UQI },\n   { OPTION_MASK_ISA_AVX512VL, CODE_FOR_sminv2df3_mask, \"__builtin_ia32_minpd_mask\", IX86_BUILTIN_MINPD128_MASK, UNKNOWN, (int) V2DF_FTYPE_V2DF_V2DF_V2DF_UQI },\n@@ -34728,8 +34670,8 @@ static const struct builtin_description bdesc_args[] =\n   { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_packssdw_mask, \"__builtin_ia32_packssdw512_mask\",  IX86_BUILTIN_PACKSSDW512, UNKNOWN, (int) V32HI_FTYPE_V16SI_V16SI_V32HI_USI },\n   { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_palignrv4ti, \"__builtin_ia32_palignr512\", IX86_BUILTIN_PALIGNR512, UNKNOWN, (int) V8DI_FTYPE_V8DI_V8DI_INT_CONVERT },\n   { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_palignrv64qi_mask, \"__builtin_ia32_palignr512_mask\", IX86_BUILTIN_PALIGNR512_MASK, UNKNOWN, (int) V8DI_FTYPE_V8DI_V8DI_INT_V8DI_UDI_CONVERT },\n-  { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_loaddquv32hi_mask, \"__builtin_ia32_movdquhi512_mask\", IX86_BUILTIN_MOVDQUHI512_MASK, UNKNOWN, (int) V32HI_FTYPE_V32HI_V32HI_USI },\n-  { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512f_loaddquv64qi_mask, \"__builtin_ia32_movdquqi512_mask\", IX86_BUILTIN_MOVDQUQI512_MASK, UNKNOWN, (int) V64QI_FTYPE_V64QI_V64QI_UDI },\n+  { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_loadv32hi_mask, \"__builtin_ia32_movdquhi512_mask\", IX86_BUILTIN_MOVDQUHI512_MASK, UNKNOWN, (int) V32HI_FTYPE_V32HI_V32HI_USI },\n+  { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_loadv64qi_mask, \"__builtin_ia32_movdquqi512_mask\", IX86_BUILTIN_MOVDQUQI512_MASK, UNKNOWN, (int) V64QI_FTYPE_V64QI_V64QI_UDI },\n   { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512f_psadbw, \"__builtin_ia32_psadbw512\", IX86_BUILTIN_PSADBW512, UNKNOWN, (int) V8DI_FTYPE_V64QI_V64QI },\n   { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_dbpsadbwv32hi_mask, \"__builtin_ia32_dbpsadbw512_mask\", IX86_BUILTIN_DBPSADBW512, UNKNOWN, (int) V32HI_FTYPE_V64QI_V64QI_INT_V32HI_USI },\n   { OPTION_MASK_ISA_AVX512BW, CODE_FOR_avx512bw_vec_dupv64qi_mask, \"__builtin_ia32_pbroadcastb512_mask\", IX86_BUILTIN_PBROADCASTB512, UNKNOWN, (int) V64QI_FTYPE_V16QI_V64QI_UDI },\n@@ -39894,12 +39836,24 @@ ix86_expand_special_args_builtin (const struct builtin_description *d,\n     case VOID_FTYPE_PV16QI_V2DI_UQI:\n     case VOID_FTYPE_PV16QI_V8SI_UQI:\n     case VOID_FTYPE_PV16QI_V4SI_UQI:\n-    case VOID_FTYPE_PV8HI_V8HI_UQI:\n-    case VOID_FTYPE_PV16HI_V16HI_UHI:\n-    case VOID_FTYPE_PV32HI_V32HI_USI:\n-    case VOID_FTYPE_PV16QI_V16QI_UHI:\n-    case VOID_FTYPE_PV32QI_V32QI_USI:\n-    case VOID_FTYPE_PV64QI_V64QI_UDI:\n+    case VOID_FTYPE_PCHAR_V64QI_UDI:\n+    case VOID_FTYPE_PCHAR_V32QI_USI:\n+    case VOID_FTYPE_PCHAR_V16QI_UHI:\n+    case VOID_FTYPE_PSHORT_V32HI_USI:\n+    case VOID_FTYPE_PSHORT_V16HI_UHI:\n+    case VOID_FTYPE_PSHORT_V8HI_UQI:\n+    case VOID_FTYPE_PINT_V16SI_UHI:\n+    case VOID_FTYPE_PINT_V8SI_UQI:\n+    case VOID_FTYPE_PINT_V4SI_UQI:\n+    case VOID_FTYPE_PINT64_V8DI_UQI:\n+    case VOID_FTYPE_PINT64_V4DI_UQI:\n+    case VOID_FTYPE_PINT64_V2DI_UQI:\n+    case VOID_FTYPE_PDOUBLE_V8DF_UQI:\n+    case VOID_FTYPE_PDOUBLE_V4DF_UQI:\n+    case VOID_FTYPE_PDOUBLE_V2DF_UQI:\n+    case VOID_FTYPE_PFLOAT_V16SF_UHI:\n+    case VOID_FTYPE_PFLOAT_V8SF_UQI:\n+    case VOID_FTYPE_PFLOAT_V4SF_UQI:\n       nargs = 2;\n       klass = store;\n       /* Reserve memory operand for target.  */\n@@ -39917,15 +39871,6 @@ ix86_expand_special_args_builtin (const struct builtin_description *d,\n     case V2DI_FTYPE_PCV2DI_V2DI_UQI:\n     case V4DI_FTYPE_PCV4DI_V4DI_UQI:\n     case V8DI_FTYPE_PCV8DI_V8DI_UQI:\n-    case V8HI_FTYPE_PCV8HI_V8HI_UQI:\n-    case V16HI_FTYPE_PCV16HI_V16HI_UHI:\n-    case V32HI_FTYPE_PCV32HI_V32HI_USI:\n-    case V16QI_FTYPE_PCV16QI_V16QI_UHI:\n-    case V32QI_FTYPE_PCV32QI_V32QI_USI:\n-    case V64QI_FTYPE_PCV64QI_V64QI_UDI:\n-      nargs = 3;\n-      klass = load;\n-      memory = 0;\n       switch (icode)\n \t{\n \t/* These builtins and instructions require the memory\n@@ -39953,6 +39898,27 @@ ix86_expand_special_args_builtin (const struct builtin_description *d,\n \tdefault:\n \t  break;\n \t}\n+    case V64QI_FTYPE_PCCHAR_V64QI_UDI:\n+    case V32QI_FTYPE_PCCHAR_V32QI_USI:\n+    case V16QI_FTYPE_PCCHAR_V16QI_UHI:\n+    case V32HI_FTYPE_PCSHORT_V32HI_USI:\n+    case V16HI_FTYPE_PCSHORT_V16HI_UHI:\n+    case V8HI_FTYPE_PCSHORT_V8HI_UQI:\n+    case V16SI_FTYPE_PCINT_V16SI_UHI:\n+    case V8SI_FTYPE_PCINT_V8SI_UQI:\n+    case V4SI_FTYPE_PCINT_V4SI_UQI:\n+    case V8DI_FTYPE_PCINT64_V8DI_UQI:\n+    case V4DI_FTYPE_PCINT64_V4DI_UQI:\n+    case V2DI_FTYPE_PCINT64_V2DI_UQI:\n+    case V8DF_FTYPE_PCDOUBLE_V8DF_UQI:\n+    case V4DF_FTYPE_PCDOUBLE_V4DF_UQI:\n+    case V2DF_FTYPE_PCDOUBLE_V2DF_UQI:\n+    case V16SF_FTYPE_PCFLOAT_V16SF_UHI:\n+    case V8SF_FTYPE_PCFLOAT_V8SF_UQI:\n+    case V4SF_FTYPE_PCFLOAT_V4SF_UQI:\n+      nargs = 3;\n+      klass = load;\n+      memory = 0;\n       break;\n     case VOID_FTYPE_UINT_UINT_UINT:\n     case VOID_FTYPE_UINT64_UINT_UINT:"}, {"sha": "ed0a1a61f4fa6f870de55f5c3fef1ae0bec5f9ec", "filename": "gcc/config/i386/sse.md", "status": "modified", "additions": 17, "deletions": 487, "changes": 504, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Fconfig%2Fi386%2Fsse.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Fconfig%2Fi386%2Fsse.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fsse.md?ref=fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "patch": "@@ -20,8 +20,6 @@\n (define_c_enum \"unspec\" [\n   ;; SSE\n   UNSPEC_MOVNT\n-  UNSPEC_LOADU\n-  UNSPEC_STOREU\n \n   ;; SSE3\n   UNSPEC_LDDQU\n@@ -290,14 +288,6 @@\n (define_mode_iterator VI1\n   [(V32QI \"TARGET_AVX\") V16QI])\n \n-(define_mode_iterator VI_ULOADSTORE_BW_AVX512VL\n-  [V64QI\n-   V32HI (V8HI \"TARGET_AVX512VL\") (V16HI \"TARGET_AVX512VL\")])\n-\n-(define_mode_iterator VI_ULOADSTORE_F_AVX512VL\n-  [V16SI (V8SI \"TARGET_AVX512VL\") (V4SI \"TARGET_AVX512VL\")\n-   V8DI (V4DI \"TARGET_AVX512VL\") (V2DI \"TARGET_AVX512VL\")])\n-\n ;; All DImode vector integer modes\n (define_mode_iterator V_AVX\n   [V16QI V8HI V4SI V2DI V4SF V2DF\n@@ -730,7 +720,8 @@\n    (V4SF \"3\") (V2DF \"1\")])\n \n (define_mode_attr ssescalarsize\n-  [(V8DI  \"64\") (V4DI  \"64\") (V2DI  \"64\")\n+  [(V4TI  \"64\") (V2TI  \"64\") (V1TI  \"64\")\n+   (V8DI  \"64\") (V4DI  \"64\") (V2DI  \"64\")\n    (V64QI \"8\") (V32QI \"8\") (V16QI \"8\")\n    (V32HI \"16\") (V16HI \"16\") (V8HI \"16\")\n    (V16SI \"32\") (V8SI \"32\") (V4SI \"32\")\n@@ -841,7 +832,7 @@\n   DONE;\n })\n \n-(define_insn \"*mov<mode>_internal\"\n+(define_insn \"mov<mode>_internal\"\n   [(set (match_operand:VMOVE 0 \"nonimmediate_operand\"               \"=v,v ,m\")\n \t(match_operand:VMOVE 1 \"nonimmediate_or_sse_const_operand\"  \"BC,vm,v\"))]\n   \"TARGET_SSE\n@@ -902,37 +893,38 @@\n \tcase MODE_V16SF:\n \tcase MODE_V8SF:\n \tcase MODE_V4SF:\n-\t  if ((TARGET_AVX || TARGET_IAMCU)\n-\t      && (misaligned_operand (operands[0], <MODE>mode)\n-\t\t  || misaligned_operand (operands[1], <MODE>mode)))\n+\t  if (misaligned_operand (operands[0], <MODE>mode)\n+\t      || misaligned_operand (operands[1], <MODE>mode))\n \t    return \"%vmovups\\t{%1, %0|%0, %1}\";\n \t  else\n \t    return \"%vmovaps\\t{%1, %0|%0, %1}\";\n \n \tcase MODE_V8DF:\n \tcase MODE_V4DF:\n \tcase MODE_V2DF:\n-\t  if ((TARGET_AVX || TARGET_IAMCU)\n-\t      && (misaligned_operand (operands[0], <MODE>mode)\n-\t\t  || misaligned_operand (operands[1], <MODE>mode)))\n+\t  if (misaligned_operand (operands[0], <MODE>mode)\n+\t      || misaligned_operand (operands[1], <MODE>mode))\n \t    return \"%vmovupd\\t{%1, %0|%0, %1}\";\n \t  else\n \t    return \"%vmovapd\\t{%1, %0|%0, %1}\";\n \n \tcase MODE_OI:\n \tcase MODE_TI:\n-\t  if ((TARGET_AVX || TARGET_IAMCU)\n-\t      && (misaligned_operand (operands[0], <MODE>mode)\n-\t\t  || misaligned_operand (operands[1], <MODE>mode)))\n-\t    return TARGET_AVX512VL ? \"vmovdqu64\\t{%1, %0|%0, %1}\"\n+\t  if (misaligned_operand (operands[0], <MODE>mode)\n+\t      || misaligned_operand (operands[1], <MODE>mode))\n+\t    return TARGET_AVX512VL ? \"vmovdqu<ssescalarsize>\\t{%1, %0|%0, %1}\"\n \t\t\t\t   : \"%vmovdqu\\t{%1, %0|%0, %1}\";\n \t  else\n \t    return TARGET_AVX512VL ? \"vmovdqa64\\t{%1, %0|%0, %1}\"\n \t\t\t\t   : \"%vmovdqa\\t{%1, %0|%0, %1}\";\n \tcase MODE_XI:\n \t  if (misaligned_operand (operands[0], <MODE>mode)\n \t      || misaligned_operand (operands[1], <MODE>mode))\n-\t    return \"vmovdqu64\\t{%1, %0|%0, %1}\";\n+\t    return (<MODE>mode == V16SImode\n+\t\t    || <MODE>mode == V8DImode\n+\t\t    || TARGET_AVX512BW)\n+\t\t   ? \"vmovdqu<ssescalarsize>\\t{%1, %0|%0, %1}\"\n+\t\t   : \"vmovdqu64\\t{%1, %0|%0, %1}\";\n \t  else\n \t    return \"vmovdqa64\\t{%1, %0|%0, %1}\";\n \n@@ -1154,62 +1146,6 @@\n   DONE;\n })\n \n-(define_expand \"<sse>_loadu<ssemodesuffix><avxsizesuffix><mask_name>\"\n-  [(set (match_operand:VF 0 \"register_operand\")\n-\t(unspec:VF [(match_operand:VF 1 \"nonimmediate_operand\")]\n-\t  UNSPEC_LOADU))]\n-  \"TARGET_SSE && <mask_mode512bit_condition>\"\n-{\n-  /* For AVX, normal *mov<mode>_internal pattern will handle unaligned loads\n-     just fine if misaligned_operand is true, and without the UNSPEC it can\n-     be combined with arithmetic instructions.  If misaligned_operand is\n-     false, still emit UNSPEC_LOADU insn to honor user's request for\n-     misaligned load.  */\n-  if (TARGET_AVX\n-      && misaligned_operand (operands[1], <MODE>mode))\n-    {\n-      rtx src = operands[1];\n-      if (<mask_applied>)\n-\tsrc = gen_rtx_VEC_MERGE (<MODE>mode, operands[1],\n-\t\t\t\t operands[2 * <mask_applied>],\n-\t\t\t\t operands[3 * <mask_applied>]);\n-      emit_insn (gen_rtx_SET (operands[0], src));\n-      DONE;\n-    }\n-})\n-\n-(define_insn \"*<sse>_loadu<ssemodesuffix><avxsizesuffix><mask_name>\"\n-  [(set (match_operand:VF 0 \"register_operand\" \"=v\")\n-\t(unspec:VF\n-\t  [(match_operand:VF 1 \"nonimmediate_operand\" \"vm\")]\n-\t  UNSPEC_LOADU))]\n-  \"TARGET_SSE && <mask_mode512bit_condition>\"\n-{\n-  switch (get_attr_mode (insn))\n-    {\n-    case MODE_V16SF:\n-    case MODE_V8SF:\n-    case MODE_V4SF:\n-      return \"%vmovups\\t{%1, %0<mask_operand2>|%0<mask_operand2>, %1}\";\n-    default:\n-      return \"%vmovu<ssemodesuffix>\\t{%1, %0<mask_operand2>|%0<mask_operand2>, %1}\";\n-    }\n-}\n-  [(set_attr \"type\" \"ssemov\")\n-   (set_attr \"movu\" \"1\")\n-   (set_attr \"ssememalign\" \"8\")\n-   (set_attr \"prefix\" \"maybe_vex\")\n-   (set (attr \"mode\")\n-\t(cond [(and (match_test \"<MODE_SIZE> == 16\")\n-\t\t    (match_test \"TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\"))\n-\t\t (const_string \"<ssePSmode>\")\n-\t       (match_test \"TARGET_AVX\")\n-\t\t (const_string \"<MODE>\")\n-\t       (match_test \"optimize_function_for_size_p (cfun)\")\n-\t\t (const_string \"V4SF\")\n-\t      ]\n-\t      (const_string \"<MODE>\")))])\n-\n ;; Merge movsd/movhpd to movupd for TARGET_SSE_UNALIGNED_LOAD_OPTIMAL targets.\n (define_peephole2\n   [(set (match_operand:V2DF 0 \"register_operand\")\n@@ -1221,69 +1157,9 @@\n \t\t\t (match_operand:DF 3 \"memory_operand\")))]\n   \"TARGET_SSE2 && TARGET_SSE_UNALIGNED_LOAD_OPTIMAL\n    && ix86_operands_ok_for_move_multiple (operands, true, DFmode)\"\n-  [(set (match_dup 2)\n-\t(unspec:V2DF [(match_dup 4)] UNSPEC_LOADU))]\n+  [(set (match_dup 2) (match_dup 4))]\n   \"operands[4] = adjust_address (operands[1], V2DFmode, 0);\")\n \n-(define_insn \"<sse>_storeu<ssemodesuffix><avxsizesuffix>\"\n-  [(set (match_operand:VF 0 \"memory_operand\" \"=m\")\n-\t(unspec:VF\n-\t  [(match_operand:VF 1 \"register_operand\" \"v\")]\n-\t  UNSPEC_STOREU))]\n-  \"TARGET_SSE\"\n-{\n-  switch (get_attr_mode (insn))\n-    {\n-    case MODE_V16SF:\n-    case MODE_V8SF:\n-    case MODE_V4SF:\n-      return \"%vmovups\\t{%1, %0|%0, %1}\";\n-    default:\n-      return \"%vmovu<ssemodesuffix>\\t{%1, %0|%0, %1}\";\n-    }\n-}\n-  [(set_attr \"type\" \"ssemov\")\n-   (set_attr \"movu\" \"1\")\n-   (set_attr \"ssememalign\" \"8\")\n-   (set_attr \"prefix\" \"maybe_vex\")\n-   (set (attr \"mode\")\n-\t(cond [(and (match_test \"<MODE_SIZE> == 16\")\n-                    (ior (match_test \"TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\")\n-                         (match_test \"TARGET_SSE_TYPELESS_STORES\")))\n-\t\t (const_string \"<ssePSmode>\")\n-\t       (match_test \"TARGET_AVX\")\n-\t\t (const_string \"<MODE>\")\n-\t       (match_test \"optimize_function_for_size_p (cfun)\")\n-\t\t (const_string \"V4SF\")\n-\t      ]\n-\t      (const_string \"<MODE>\")))])\n-\n-(define_insn \"<avx512>_storeu<ssemodesuffix><avxsizesuffix>_mask\"\n-  [(set (match_operand:VF_AVX512VL 0 \"memory_operand\" \"=m\")\n-\t(vec_merge:VF_AVX512VL\n-\t  (unspec:VF_AVX512VL\n-\t    [(match_operand:VF_AVX512VL 1 \"register_operand\" \"v\")]\n-\t    UNSPEC_STOREU)\n-\t  (match_dup 0)\n-\t  (match_operand:<avx512fmaskmode> 2 \"register_operand\" \"Yk\")))]\n-  \"TARGET_AVX512F\"\n-{\n-  switch (get_attr_mode (insn))\n-    {\n-    case MODE_V16SF:\n-    case MODE_V8SF:\n-    case MODE_V4SF:\n-      return \"vmovups\\t{%1, %0%{%2%}|%0%{%2%}, %1}\";\n-    default:\n-      return \"vmovu<ssemodesuffix>\\t{%1, %0%{%2%}|%0%{%2%}, %1}\";\n-    }\n-}\n-  [(set_attr \"type\" \"ssemov\")\n-   (set_attr \"movu\" \"1\")\n-   (set_attr \"memory\" \"store\")\n-   (set_attr \"prefix\" \"evex\")\n-   (set_attr \"mode\" \"<sseinsnmode>\")])\n-\n ;; Merge movlpd/movhpd to movupd for TARGET_SSE_UNALIGNED_STORE_OPTIMAL targets.\n (define_peephole2\n   [(set (match_operand:DF 0 \"memory_operand\")\n@@ -1294,238 +1170,9 @@\n \t\t       (parallel [(const_int 1)])))]\n   \"TARGET_SSE2 && TARGET_SSE_UNALIGNED_STORE_OPTIMAL\n    && ix86_operands_ok_for_move_multiple (operands, false, DFmode)\"\n-  [(set (match_dup 4)\n-\t(unspec:V2DF [(match_dup 1)] UNSPEC_STOREU))]\n+  [(set (match_dup 4) (match_dup 1))]\n   \"operands[4] = adjust_address (operands[0], V2DFmode, 0);\")\n \n-/* For AVX, normal *mov<mode>_internal pattern will handle unaligned loads\n-   just fine if misaligned_operand is true, and without the UNSPEC it can\n-   be combined with arithmetic instructions.  If misaligned_operand is\n-   false, still emit UNSPEC_LOADU insn to honor user's request for\n-   misaligned load.  */\n-(define_expand \"<sse2_avx_avx512f>_loaddqu<mode><mask_name>\"\n-  [(set (match_operand:VI1 0 \"register_operand\")\n-\t(unspec:VI1\n-\t  [(match_operand:VI1 1 \"nonimmediate_operand\")]\n-\t  UNSPEC_LOADU))]\n-  \"TARGET_SSE2 && <mask_avx512vl_condition> && <mask_avx512bw_condition>\"\n-{\n-  if (TARGET_AVX\n-      && misaligned_operand (operands[1], <MODE>mode))\n-    {\n-      rtx src = operands[1];\n-      if (<mask_applied>)\n-\tsrc = gen_rtx_VEC_MERGE (<MODE>mode, operands[1],\n-\t\t\t\t operands[2 * <mask_applied>],\n-\t\t\t\t operands[3 * <mask_applied>]);\n-      emit_insn (gen_rtx_SET (operands[0], src));\n-      DONE;\n-    }\n-})\n-\n-(define_expand \"<sse2_avx_avx512f>_loaddqu<mode><mask_name>\"\n-  [(set (match_operand:VI_ULOADSTORE_BW_AVX512VL 0 \"register_operand\")\n-\t(unspec:VI_ULOADSTORE_BW_AVX512VL\n-\t  [(match_operand:VI_ULOADSTORE_BW_AVX512VL 1 \"nonimmediate_operand\")]\n-\t  UNSPEC_LOADU))]\n-  \"TARGET_AVX512BW\"\n-{\n-  if (misaligned_operand (operands[1], <MODE>mode))\n-    {\n-      rtx src = operands[1];\n-      if (<mask_applied>)\n-\tsrc = gen_rtx_VEC_MERGE (<MODE>mode, operands[1],\n-\t\t\t\t operands[2 * <mask_applied>],\n-\t\t\t\t operands[3 * <mask_applied>]);\n-      emit_insn (gen_rtx_SET (operands[0], src));\n-      DONE;\n-    }\n-})\n-\n-(define_expand \"<sse2_avx_avx512f>_loaddqu<mode><mask_name>\"\n-  [(set (match_operand:VI_ULOADSTORE_F_AVX512VL 0 \"register_operand\")\n-\t(unspec:VI_ULOADSTORE_F_AVX512VL\n-\t  [(match_operand:VI_ULOADSTORE_F_AVX512VL 1 \"nonimmediate_operand\")]\n-\t  UNSPEC_LOADU))]\n-  \"TARGET_AVX512F\"\n-{\n-  if (misaligned_operand (operands[1], <MODE>mode))\n-    {\n-      rtx src = operands[1];\n-      if (<mask_applied>)\n-\tsrc = gen_rtx_VEC_MERGE (<MODE>mode, operands[1],\n-\t\t\t\t operands[2 * <mask_applied>],\n-\t\t\t\t operands[3 * <mask_applied>]);\n-      emit_insn (gen_rtx_SET (operands[0], src));\n-      DONE;\n-    }\n-})\n-\n-(define_insn \"*<sse2_avx_avx512f>_loaddqu<mode><mask_name>\"\n-  [(set (match_operand:VI1 0 \"register_operand\" \"=v\")\n-\t(unspec:VI1\n-\t  [(match_operand:VI1 1 \"nonimmediate_operand\" \"vm\")]\n-\t  UNSPEC_LOADU))]\n-  \"TARGET_SSE2 && <mask_avx512vl_condition> && <mask_avx512bw_condition>\"\n-{\n-  switch (get_attr_mode (insn))\n-    {\n-    case MODE_V8SF:\n-    case MODE_V4SF:\n-      return \"%vmovups\\t{%1, %0|%0, %1}\";\n-    default:\n-      if (!(TARGET_AVX512VL && TARGET_AVX512BW))\n-\treturn \"%vmovdqu\\t{%1, %0|%0, %1}\";\n-      else\n-\treturn \"vmovdqu<ssescalarsize>\\t{%1, %0<mask_operand2>|%0<mask_operand2>, %1}\";\n-    }\n-}\n-  [(set_attr \"type\" \"ssemov\")\n-   (set_attr \"movu\" \"1\")\n-   (set_attr \"ssememalign\" \"8\")\n-   (set (attr \"prefix_data16\")\n-     (if_then_else\n-       (match_test \"TARGET_AVX\")\n-     (const_string \"*\")\n-     (const_string \"1\")))\n-   (set_attr \"prefix\" \"maybe_vex\")\n-   (set (attr \"mode\")\n-\t(cond [(and (match_test \"<MODE_SIZE> == 16\")\n-\t\t    (match_test \"TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\"))\n-\t\t (const_string \"<ssePSmode>\")\n-\t       (match_test \"TARGET_AVX\")\n-\t\t (const_string \"<sseinsnmode>\")\n-\t       (match_test \"optimize_function_for_size_p (cfun)\")\n-\t         (const_string \"V4SF\")\n-\t      ]\n-\t      (const_string \"<sseinsnmode>\")))])\n-\n-(define_insn \"*<sse2_avx_avx512f>_loaddqu<mode><mask_name>\"\n-  [(set (match_operand:VI_ULOADSTORE_BW_AVX512VL 0 \"register_operand\" \"=v\")\n-\t(unspec:VI_ULOADSTORE_BW_AVX512VL\n-\t  [(match_operand:VI_ULOADSTORE_BW_AVX512VL 1 \"nonimmediate_operand\" \"vm\")]\n-\t  UNSPEC_LOADU))]\n-  \"TARGET_AVX512BW\"\n-  \"vmovdqu<ssescalarsize>\\t{%1, %0<mask_operand2>|%0<mask_operand2>, %1}\";\n-  [(set_attr \"type\" \"ssemov\")\n-   (set_attr \"movu\" \"1\")\n-   (set_attr \"ssememalign\" \"8\")\n-   (set_attr \"prefix\" \"maybe_evex\")])\n-\n-(define_insn \"*<sse2_avx_avx512f>_loaddqu<mode><mask_name>\"\n-  [(set (match_operand:VI_ULOADSTORE_F_AVX512VL 0 \"register_operand\" \"=v\")\n-\t(unspec:VI_ULOADSTORE_F_AVX512VL\n-\t  [(match_operand:VI_ULOADSTORE_F_AVX512VL 1 \"nonimmediate_operand\" \"vm\")]\n-\t  UNSPEC_LOADU))]\n-  \"TARGET_AVX512F\"\n-  \"vmovdqu<ssescalarsize>\\t{%1, %0<mask_operand2>|%0<mask_operand2>, %1}\";\n-  [(set_attr \"type\" \"ssemov\")\n-   (set_attr \"movu\" \"1\")\n-   (set_attr \"ssememalign\" \"8\")\n-   (set_attr \"prefix\" \"maybe_evex\")])\n-\n-(define_insn \"<sse2_avx_avx512f>_storedqu<mode>\"\n-  [(set (match_operand:VI1 0 \"memory_operand\" \"=m\")\n-\t(unspec:VI1\n-\t  [(match_operand:VI1 1 \"register_operand\" \"v\")]\n-\t  UNSPEC_STOREU))]\n-  \"TARGET_SSE2\"\n-{\n-  switch (get_attr_mode (insn))\n-    {\n-    case MODE_V16SF:\n-    case MODE_V8SF:\n-    case MODE_V4SF:\n-      return \"%vmovups\\t{%1, %0|%0, %1}\";\n-    default:\n-      switch (<MODE>mode)\n-      {\n-      case V32QImode:\n-      case V16QImode:\n-\tif (!(TARGET_AVX512VL && TARGET_AVX512BW))\n-\t  return \"%vmovdqu\\t{%1, %0|%0, %1}\";\n-      default:\n-\t  return \"vmovdqu<ssescalarsize>\\t{%1, %0|%0, %1}\";\n-      }\n-    }\n-}\n-  [(set_attr \"type\" \"ssemov\")\n-   (set_attr \"movu\" \"1\")\n-   (set_attr \"ssememalign\" \"8\")\n-   (set (attr \"prefix_data16\")\n-     (if_then_else\n-       (match_test \"TARGET_AVX\")\n-     (const_string \"*\")\n-     (const_string \"1\")))\n-   (set_attr \"prefix\" \"maybe_vex\")\n-   (set (attr \"mode\")\n-\t(cond [(and (match_test \"<MODE_SIZE> == 16\")\n-\t\t    (ior (match_test \"TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\")\n-\t\t\t (match_test \"TARGET_SSE_TYPELESS_STORES\")))\n-\t\t (const_string \"<ssePSmode>\")\n-\t       (match_test \"TARGET_AVX\")\n-\t\t (const_string \"<sseinsnmode>\")\n-\t       (match_test \"optimize_function_for_size_p (cfun)\")\n-\t         (const_string \"V4SF\")\n-\t      ]\n-\t      (const_string \"<sseinsnmode>\")))])\n-\n-(define_insn \"<sse2_avx_avx512f>_storedqu<mode>\"\n-  [(set (match_operand:VI_ULOADSTORE_BW_AVX512VL 0 \"memory_operand\" \"=m\")\n-\t(unspec:VI_ULOADSTORE_BW_AVX512VL\n-\t  [(match_operand:VI_ULOADSTORE_BW_AVX512VL 1 \"register_operand\" \"v\")]\n-\t  UNSPEC_STOREU))]\n-  \"TARGET_AVX512BW\"\n-  \"vmovdqu<ssescalarsize>\\t{%1, %0|%0, %1}\"\n-  [(set_attr \"type\" \"ssemov\")\n-   (set_attr \"movu\" \"1\")\n-   (set_attr \"ssememalign\" \"8\")\n-   (set_attr \"prefix\" \"maybe_evex\")])\n-\n-(define_insn \"<sse2_avx_avx512f>_storedqu<mode>\"\n-  [(set (match_operand:VI_ULOADSTORE_F_AVX512VL 0 \"memory_operand\" \"=m\")\n-\t(unspec:VI_ULOADSTORE_F_AVX512VL\n-\t  [(match_operand:VI_ULOADSTORE_F_AVX512VL 1 \"register_operand\" \"v\")]\n-\t  UNSPEC_STOREU))]\n-  \"TARGET_AVX512F\"\n-  \"vmovdqu<ssescalarsize>\\t{%1, %0|%0, %1}\"\n-  [(set_attr \"type\" \"ssemov\")\n-   (set_attr \"movu\" \"1\")\n-   (set_attr \"ssememalign\" \"8\")\n-   (set_attr \"prefix\" \"maybe_vex\")])\n-\n-(define_insn \"<avx512>_storedqu<mode>_mask\"\n-  [(set (match_operand:VI48_AVX512VL 0 \"memory_operand\" \"=m\")\n-\t(vec_merge:VI48_AVX512VL\n-\t  (unspec:VI48_AVX512VL\n-\t    [(match_operand:VI48_AVX512VL 1 \"register_operand\" \"v\")]\n-\t    UNSPEC_STOREU)\n-\t  (match_dup 0)\n-\t  (match_operand:<avx512fmaskmode> 2 \"register_operand\" \"Yk\")))]\n-  \"TARGET_AVX512F\"\n-  \"vmovdqu<ssescalarsize>\\t{%1, %0%{%2%}|%0%{%2%}, %1}\"\n-  [(set_attr \"type\" \"ssemov\")\n-   (set_attr \"movu\" \"1\")\n-   (set_attr \"memory\" \"store\")\n-   (set_attr \"prefix\" \"evex\")\n-   (set_attr \"mode\" \"<sseinsnmode>\")])\n-\n-(define_insn \"<avx512>_storedqu<mode>_mask\"\n-  [(set (match_operand:VI12_AVX512VL 0 \"memory_operand\" \"=m\")\n-\t(vec_merge:VI12_AVX512VL\n-\t  (unspec:VI12_AVX512VL\n-\t    [(match_operand:VI12_AVX512VL 1 \"register_operand\" \"v\")]\n-\t    UNSPEC_STOREU)\n-\t  (match_dup 0)\n-\t  (match_operand:<avx512fmaskmode> 2 \"register_operand\" \"Yk\")))]\n-  \"TARGET_AVX512BW\"\n-  \"vmovdqu<ssescalarsize>\\t{%1, %0%{%2%}|%0%{%2%}, %1}\"\n-  [(set_attr \"type\" \"ssemov\")\n-   (set_attr \"movu\" \"1\")\n-   (set_attr \"memory\" \"store\")\n-   (set_attr \"prefix\" \"evex\")\n-   (set_attr \"mode\" \"<sseinsnmode>\")])\n-\n (define_insn \"<sse3>_lddqu<avxsizesuffix>\"\n   [(set (match_operand:VI1 0 \"register_operand\" \"=x\")\n \t(unspec:VI1 [(match_operand:VI1 1 \"memory_operand\" \"m\")]\n@@ -15406,69 +15053,6 @@\n    (set_attr \"memory\" \"none,load\")\n    (set_attr \"mode\" \"TI\")])\n \n-(define_insn_and_split \"*sse4_2_pcmpestr_unaligned\"\n-  [(set (match_operand:SI 0 \"register_operand\" \"=c\")\n-\t(unspec:SI\n-\t  [(match_operand:V16QI 2 \"register_operand\" \"x\")\n-\t   (match_operand:SI 3 \"register_operand\" \"a\")\n-\t   (unspec:V16QI\n-\t     [(match_operand:V16QI 4 \"memory_operand\" \"m\")]\n-\t     UNSPEC_LOADU)\n-\t   (match_operand:SI 5 \"register_operand\" \"d\")\n-\t   (match_operand:SI 6 \"const_0_to_255_operand\" \"n\")]\n-\t  UNSPEC_PCMPESTR))\n-   (set (match_operand:V16QI 1 \"register_operand\" \"=Yz\")\n-\t(unspec:V16QI\n-\t  [(match_dup 2)\n-\t   (match_dup 3)\n-\t   (unspec:V16QI [(match_dup 4)] UNSPEC_LOADU)\n-\t   (match_dup 5)\n-\t   (match_dup 6)]\n-\t  UNSPEC_PCMPESTR))\n-   (set (reg:CC FLAGS_REG)\n-\t(unspec:CC\n-\t  [(match_dup 2)\n-\t   (match_dup 3)\n-\t   (unspec:V16QI [(match_dup 4)] UNSPEC_LOADU)\n-\t   (match_dup 5)\n-\t   (match_dup 6)]\n-\t  UNSPEC_PCMPESTR))]\n-  \"TARGET_SSE4_2\n-   && can_create_pseudo_p ()\"\n-  \"#\"\n-  \"&& 1\"\n-  [(const_int 0)]\n-{\n-  int ecx = !find_regno_note (curr_insn, REG_UNUSED, REGNO (operands[0]));\n-  int xmm0 = !find_regno_note (curr_insn, REG_UNUSED, REGNO (operands[1]));\n-  int flags = !find_regno_note (curr_insn, REG_UNUSED, FLAGS_REG);\n-\n-  if (ecx)\n-    emit_insn (gen_sse4_2_pcmpestri (operands[0], operands[2],\n-\t\t\t\t     operands[3], operands[4],\n-\t\t\t\t     operands[5], operands[6]));\n-  if (xmm0)\n-    emit_insn (gen_sse4_2_pcmpestrm (operands[1], operands[2],\n-\t\t\t\t     operands[3], operands[4],\n-\t\t\t\t     operands[5], operands[6]));\n-  if (flags && !(ecx || xmm0))\n-    emit_insn (gen_sse4_2_pcmpestr_cconly (NULL, NULL,\n-\t\t\t\t\t   operands[2], operands[3],\n-\t\t\t\t\t   operands[4], operands[5],\n-\t\t\t\t\t   operands[6]));\n-  if (!(flags || ecx || xmm0))\n-    emit_note (NOTE_INSN_DELETED);\n-\n-  DONE;\n-}\n-  [(set_attr \"type\" \"sselog\")\n-   (set_attr \"prefix_data16\" \"1\")\n-   (set_attr \"prefix_extra\" \"1\")\n-   (set_attr \"ssememalign\" \"8\")\n-   (set_attr \"length_immediate\" \"1\")\n-   (set_attr \"memory\" \"load\")\n-   (set_attr \"mode\" \"TI\")])\n-\n (define_insn \"sse4_2_pcmpestri\"\n   [(set (match_operand:SI 0 \"register_operand\" \"=c,c\")\n \t(unspec:SI\n@@ -15606,60 +15190,6 @@\n    (set_attr \"memory\" \"none,load\")\n    (set_attr \"mode\" \"TI\")])\n \n-(define_insn_and_split \"*sse4_2_pcmpistr_unaligned\"\n-  [(set (match_operand:SI 0 \"register_operand\" \"=c\")\n-\t(unspec:SI\n-\t  [(match_operand:V16QI 2 \"register_operand\" \"x\")\n-\t   (unspec:V16QI\n-\t     [(match_operand:V16QI 3 \"memory_operand\" \"m\")]\n-\t     UNSPEC_LOADU)\n-\t   (match_operand:SI 4 \"const_0_to_255_operand\" \"n\")]\n-\t  UNSPEC_PCMPISTR))\n-   (set (match_operand:V16QI 1 \"register_operand\" \"=Yz\")\n-\t(unspec:V16QI\n-\t  [(match_dup 2)\n-\t   (unspec:V16QI [(match_dup 3)] UNSPEC_LOADU)\n-\t   (match_dup 4)]\n-\t  UNSPEC_PCMPISTR))\n-   (set (reg:CC FLAGS_REG)\n-\t(unspec:CC\n-\t  [(match_dup 2)\n-\t   (unspec:V16QI [(match_dup 3)] UNSPEC_LOADU)\n-\t   (match_dup 4)]\n-\t  UNSPEC_PCMPISTR))]\n-  \"TARGET_SSE4_2\n-   && can_create_pseudo_p ()\"\n-  \"#\"\n-  \"&& 1\"\n-  [(const_int 0)]\n-{\n-  int ecx = !find_regno_note (curr_insn, REG_UNUSED, REGNO (operands[0]));\n-  int xmm0 = !find_regno_note (curr_insn, REG_UNUSED, REGNO (operands[1]));\n-  int flags = !find_regno_note (curr_insn, REG_UNUSED, FLAGS_REG);\n-\n-  if (ecx)\n-    emit_insn (gen_sse4_2_pcmpistri (operands[0], operands[2],\n-\t\t\t\t     operands[3], operands[4]));\n-  if (xmm0)\n-    emit_insn (gen_sse4_2_pcmpistrm (operands[1], operands[2],\n-\t\t\t\t     operands[3], operands[4]));\n-  if (flags && !(ecx || xmm0))\n-    emit_insn (gen_sse4_2_pcmpistr_cconly (NULL, NULL,\n-\t\t\t\t\t   operands[2], operands[3],\n-\t\t\t\t\t   operands[4]));\n-  if (!(flags || ecx || xmm0))\n-    emit_note (NOTE_INSN_DELETED);\n-\n-  DONE;\n-}\n-  [(set_attr \"type\" \"sselog\")\n-   (set_attr \"prefix_data16\" \"1\")\n-   (set_attr \"prefix_extra\" \"1\")\n-   (set_attr \"ssememalign\" \"8\")\n-   (set_attr \"length_immediate\" \"1\")\n-   (set_attr \"memory\" \"load\")\n-   (set_attr \"mode\" \"TI\")])\n-\n (define_insn \"sse4_2_pcmpistri\"\n   [(set (match_operand:SI 0 \"register_operand\" \"=c,c\")\n \t(unspec:SI"}, {"sha": "f97e63ec7c133d225f929ecda210e159c06beb2b", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "patch": "@@ -1,3 +1,31 @@\n+2016-04-19  H.J. Lu  <hongjiu.lu@intel.com>\n+\n+\tPR target/69201\n+\t* gcc.target/i386/avx256-unaligned-store-1.c (a): Make it\n+\textern to force it misaligned.\n+\t(b): Likewise.\n+\t(c): Likewise.\n+\t(d): Likewise.\n+\tCheck vmovups.*movv8sf_internal/3 instead of avx_storeups256.\n+\tDon't check `*' before movv4sf_internal.\n+\t* gcc.target/i386/avx256-unaligned-store-2.c: Check\n+\tvmovups.*movv32qi_internal/3 instead of avx_storeups256.\n+\tDon't check `*' before movv16qi_internal.\n+\t* gcc.target/i386/avx256-unaligned-store-3.c (a): Make it\n+\textern to force it misaligned.\n+\t(b): Likewise.\n+\t(c): Likewise.\n+\t(d): Likewise.\n+\tCheck vmovups.*movv4df_internal/3 instead of avx_storeupd256.\n+\tDon't check `*' before movv2df_internal.\n+\t* gcc.target/i386/avx256-unaligned-store-4.c (a): Make it\n+\textern to force it misaligned.\n+\t(b): Likewise.\n+\t(c): Likewise.\n+\t(d): Likewise.\n+\tCheck movv8sf_internal instead of avx_storeups256.\n+\tCheck movups.*movv4sf_internal/3 instead of avx_storeups256.\n+\n 2016-04-19  Richard Biener  <rguenther@suse.de>\n \n \tPR tree-optimization/70171"}, {"sha": "d82aecffda9ae5e4f5d928e5567da20597fdf55f", "filename": "gcc/testsuite/gcc.target/i386/avx256-unaligned-store-1.c", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx256-unaligned-store-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx256-unaligned-store-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx256-unaligned-store-1.c?ref=fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "patch": "@@ -3,7 +3,7 @@\n \n #define N 1024\n \n-float a[N], b[N+3], c[N], d[N];\n+extern float a[N], b[N+3], c[N], d[N];\n \n void\n avx_test (void)\n@@ -17,6 +17,6 @@ avx_test (void)\n     d[i] = c[i] * 20.0;\n }\n \n-/* { dg-final { scan-assembler-not \"avx_storeups256\" } } */\n-/* { dg-final { scan-assembler \"vmovups.*\\\\*movv4sf_internal/3\" } } */\n+/* { dg-final { scan-assembler-not \"vmovups.*movv8sf_internal/3\" } } */\n+/* { dg-final { scan-assembler \"vmovups.*movv4sf_internal/3\" } } */\n /* { dg-final { scan-assembler \"vextractf128\" } } */"}, {"sha": "817be172b98b6fee3dbf40e79a69fd589ace9eb4", "filename": "gcc/testsuite/gcc.target/i386/avx256-unaligned-store-2.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx256-unaligned-store-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx256-unaligned-store-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx256-unaligned-store-2.c?ref=fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "patch": "@@ -23,6 +23,6 @@ avx_test (void)\n     }\n }\n \n-/* { dg-final { scan-assembler-not \"avx_storedqu256\" } } */\n-/* { dg-final { scan-assembler \"vmovups.*\\\\*movv16qi_internal/3\" } } */\n+/* { dg-final { scan-assembler-not \"vmovups.*movv32qi_internal/3\" } } */\n+/* { dg-final { scan-assembler \"vmovups.*movv16qi_internal/3\" } } */\n /* { dg-final { scan-assembler \"vextract.128\" } } */"}, {"sha": "a439a66ff3482a6b62b257389dc341e6df12917e", "filename": "gcc/testsuite/gcc.target/i386/avx256-unaligned-store-3.c", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx256-unaligned-store-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx256-unaligned-store-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx256-unaligned-store-3.c?ref=fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "patch": "@@ -3,7 +3,7 @@\n \n #define N 1024\n \n-double a[N], b[N+3], c[N], d[N];\n+extern double a[N], b[N+3], c[N], d[N];\n \n void\n avx_test (void)\n@@ -17,6 +17,6 @@ avx_test (void)\n     d[i] = c[i] * 20.0;\n }\n \n-/* { dg-final { scan-assembler-not \"avx_storeupd256\" } } */\n-/* { dg-final { scan-assembler \"vmovups.*\\\\*movv2df_internal/3\" } } */\n+/* { dg-final { scan-assembler-not \"vmovups.*movv4df_internal/3\" } } */\n+/* { dg-final { scan-assembler \"vmovups.*movv2df_internal/3\" } } */\n /* { dg-final { scan-assembler \"vextractf128\" } } */"}, {"sha": "463c1d824eb3a10c748332d1091a6493646eca2a", "filename": "gcc/testsuite/gcc.target/i386/avx256-unaligned-store-4.c", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx256-unaligned-store-4.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fc9cf6da84a899334fa3cdd50e62d780b2a90c4a/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx256-unaligned-store-4.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx256-unaligned-store-4.c?ref=fc9cf6da84a899334fa3cdd50e62d780b2a90c4a", "patch": "@@ -3,7 +3,7 @@\n \n #define N 1024\n \n-float a[N], b[N+3], c[N];\n+extern float a[N], b[N+3], c[N];\n \n void\n avx_test (void)\n@@ -14,7 +14,6 @@ avx_test (void)\n     b[i+3] = a[i] * c[i];\n }\n \n-/* { dg-final { scan-assembler \"avx_storeups256\" } } */\n-/* { dg-final { scan-assembler-not \"sse_storeups\" } } */\n-/* { dg-final { scan-assembler-not \"\\\\*avx_movv4sf_internal/3\" } } */\n+/* { dg-final { scan-assembler \"vmovups.*movv8sf_internal/3\" } } */\n+/* { dg-final { scan-assembler-not \"movups.*movv4sf_internal/3\" } } */\n /* { dg-final { scan-assembler-not \"vextractf128\" } } */"}]}