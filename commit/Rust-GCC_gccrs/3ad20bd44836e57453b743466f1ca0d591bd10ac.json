{"sha": "3ad20bd44836e57453b743466f1ca0d591bd10ac", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6M2FkMjBiZDQ0ODM2ZTU3NDUzYjc0MzQ2NmYxY2EwZDU5MWJkMTBhYw==", "commit": {"author": {"name": "Xinliang David Li", "email": "davidxl@google.com", "date": "2013-08-14T17:41:02Z"}, "committer": {"name": "Xinliang David Li", "email": "davidxl@gcc.gnu.org", "date": "2013-08-14T17:41:02Z"}, "message": "Implement -mno-default, option documentation and code refactoring\n\nFrom-SVN: r201732", "tree": {"sha": "ab4e6edfc55f69d9ee8667cf3775470ee7eab593", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/ab4e6edfc55f69d9ee8667cf3775470ee7eab593"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/3ad20bd44836e57453b743466f1ca0d591bd10ac", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3ad20bd44836e57453b743466f1ca0d591bd10ac", "html_url": "https://github.com/Rust-GCC/gccrs/commit/3ad20bd44836e57453b743466f1ca0d591bd10ac", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3ad20bd44836e57453b743466f1ca0d591bd10ac/comments", "author": {"login": "vns-mn", "id": 57157229, "node_id": "MDQ6VXNlcjU3MTU3MjI5", "avatar_url": "https://avatars.githubusercontent.com/u/57157229?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vns-mn", "html_url": "https://github.com/vns-mn", "followers_url": "https://api.github.com/users/vns-mn/followers", "following_url": "https://api.github.com/users/vns-mn/following{/other_user}", "gists_url": "https://api.github.com/users/vns-mn/gists{/gist_id}", "starred_url": "https://api.github.com/users/vns-mn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vns-mn/subscriptions", "organizations_url": "https://api.github.com/users/vns-mn/orgs", "repos_url": "https://api.github.com/users/vns-mn/repos", "events_url": "https://api.github.com/users/vns-mn/events{/privacy}", "received_events_url": "https://api.github.com/users/vns-mn/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "645e5010bb57adb1e37348875124486ba99d1337", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/645e5010bb57adb1e37348875124486ba99d1337", "html_url": "https://github.com/Rust-GCC/gccrs/commit/645e5010bb57adb1e37348875124486ba99d1337"}], "stats": {"total": 736, "additions": 337, "deletions": 399}, "files": [{"sha": "b6b9dd3aa6f69dd49bf1328db2915d28ce4ee66c", "filename": "gcc/ChangeLog", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3ad20bd44836e57453b743466f1ca0d591bd10ac/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3ad20bd44836e57453b743466f1ca0d591bd10ac/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=3ad20bd44836e57453b743466f1ca0d591bd10ac", "patch": "@@ -1,3 +1,14 @@\n+2013-08-14  Xinliang David Li  <davidxl@google.com>\n+\n+\t* config/i386/i386.opt: Define two new options.\n+\t* config/i386/x86-tune.def: Add arch selector field in macros.\n+\t* config/i386/i386.h: Adjust macro definition.\n+\t* config/i386/i386.c (ix86_option_override_internal):\n+\tRefactor the code.\n+\t(parse_mtune_ctrl_str): New function.\n+\t(set_ix86_tune_features): New function.\n+\t(ix86_function_specific_restore): Call the new helper function.\n+\n 2013-08-14  Andrey Belevantsev  <abel@ispras.ru>\n \n \tPR rtl-optimization/57662"}, {"sha": "8a95f39526a281442f1a0fba5c4541701de164b3", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 83, "deletions": 320, "changes": 403, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3ad20bd44836e57453b743466f1ca0d591bd10ac/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3ad20bd44836e57453b743466f1ca0d591bd10ac/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=3ad20bd44836e57453b743466f1ca0d591bd10ac", "patch": "@@ -1938,7 +1938,7 @@ const struct processor_costs *ix86_cost = &pentium_cost;\n \n const char* ix86_tune_feature_names[X86_TUNE_LAST] = {\n #undef DEF_TUNE\n-#define DEF_TUNE(tune, name) name,\n+#define DEF_TUNE(tune, name, selector) name,\n #include \"x86-tune.def\"\n #undef DEF_TUNE\n };\n@@ -1949,281 +1949,10 @@ unsigned char ix86_tune_features[X86_TUNE_LAST];\n /* Feature tests against the various tunings used to create ix86_tune_features\n    based on the processor mask.  */\n static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n-  /* X86_TUNE_USE_LEAVE: Leave does not affect Nocona SPEC2000 results\n-     negatively, so enabling for Generic64 seems like good code size\n-     tradeoff.  We can't enable it for 32bit generic because it does not\n-     work well with PPro base chips.  */\n-  m_386 | m_CORE_ALL | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC64,\n-\n-  /* X86_TUNE_PUSH_MEMORY */\n-  m_386 | m_P4_NOCONA | m_CORE_ALL | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC,\n-\n-  /* X86_TUNE_ZERO_EXTEND_WITH_AND */\n-  m_486 | m_PENT,\n-\n-  /* X86_TUNE_UNROLL_STRLEN */\n-  m_486 | m_PENT | m_PPRO | m_ATOM | m_SLM | m_CORE_ALL | m_K6 | m_AMD_MULTIPLE | m_GENERIC,\n-\n-  /* X86_TUNE_BRANCH_PREDICTION_HINTS: Branch hints were put in P4 based\n-     on simulation result. But after P4 was made, no performance benefit\n-     was observed with branch hints.  It also increases the code size.\n-     As a result, icc never generates branch hints.  */\n-  0,\n-\n-  /* X86_TUNE_DOUBLE_WITH_ADD */\n-  ~m_386,\n-\n-  /* X86_TUNE_USE_SAHF */\n-  m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_K6_GEODE | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC,\n-\n-  /* X86_TUNE_MOVX: Enable to zero extend integer registers to avoid\n-     partial dependencies.  */\n-  m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_GEODE | m_AMD_MULTIPLE  | m_GENERIC,\n-\n-  /* X86_TUNE_PARTIAL_REG_STALL: We probably ought to watch for partial\n-     register stalls on Generic32 compilation setting as well.  However\n-     in current implementation the partial register stalls are not eliminated\n-     very well - they can be introduced via subregs synthesized by combine\n-     and can happen in caller/callee saving sequences.  Because this option\n-     pays back little on PPro based chips and is in conflict with partial reg\n-     dependencies used by Athlon/P4 based chips, it is better to leave it off\n-     for generic32 for now.  */\n-  m_PPRO,\n-\n-  /* X86_TUNE_PARTIAL_FLAG_REG_STALL */\n-  m_CORE_ALL | m_GENERIC,\n-\n-  /* X86_TUNE_LCP_STALL: Avoid an expensive length-changing prefix stall\n-   * on 16-bit immediate moves into memory on Core2 and Corei7.  */\n-  m_CORE_ALL | m_GENERIC,\n-\n-  /* X86_TUNE_USE_HIMODE_FIOP */\n-  m_386 | m_486 | m_K6_GEODE,\n-\n-  /* X86_TUNE_USE_SIMODE_FIOP */\n-  ~(m_PENT | m_PPRO | m_CORE_ALL | m_ATOM | m_SLM | m_AMD_MULTIPLE | m_GENERIC),\n-\n-  /* X86_TUNE_USE_MOV0 */\n-  m_K6,\n-\n-  /* X86_TUNE_USE_CLTD */\n-  ~(m_PENT | m_ATOM | m_SLM | m_K6),\n-\n-  /* X86_TUNE_USE_XCHGB: Use xchgb %rh,%rl instead of rolw/rorw $8,rx.  */\n-  m_PENT4,\n-\n-  /* X86_TUNE_SPLIT_LONG_MOVES */\n-  m_PPRO,\n-\n-  /* X86_TUNE_READ_MODIFY_WRITE */\n-  ~m_PENT,\n-\n-  /* X86_TUNE_READ_MODIFY */\n-  ~(m_PENT | m_PPRO),\n-\n-  /* X86_TUNE_PROMOTE_QIMODE */\n-  m_386 | m_486 | m_PENT | m_CORE_ALL | m_ATOM | m_SLM | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC,\n-\n-  /* X86_TUNE_FAST_PREFIX */\n-  ~(m_386 | m_486 | m_PENT),\n-\n-  /* X86_TUNE_SINGLE_STRINGOP */\n-  m_386 | m_P4_NOCONA,\n-\n-  /* X86_TUNE_QIMODE_MATH */\n-  ~0,\n-\n-  /* X86_TUNE_HIMODE_MATH: On PPro this flag is meant to avoid partial\n-     register stalls.  Just like X86_TUNE_PARTIAL_REG_STALL this option\n-     might be considered for Generic32 if our scheme for avoiding partial\n-     stalls was more effective.  */\n-  ~m_PPRO,\n-\n-  /* X86_TUNE_PROMOTE_QI_REGS */\n-  0,\n-\n-  /* X86_TUNE_PROMOTE_HI_REGS */\n-  m_PPRO,\n-\n-  /* X86_TUNE_SINGLE_POP: Enable if single pop insn is preferred\n-     over esp addition.  */\n-  m_386 | m_486 | m_PENT | m_PPRO,\n-\n-  /* X86_TUNE_DOUBLE_POP: Enable if double pop insn is preferred\n-     over esp addition.  */\n-  m_PENT,\n-\n-  /* X86_TUNE_SINGLE_PUSH: Enable if single push insn is preferred\n-     over esp subtraction.  */\n-  m_386 | m_486 | m_PENT | m_K6_GEODE,\n-\n-  /* X86_TUNE_DOUBLE_PUSH. Enable if double push insn is preferred\n-     over esp subtraction.  */\n-  m_PENT | m_K6_GEODE,\n-\n-  /* X86_TUNE_INTEGER_DFMODE_MOVES: Enable if integer moves are preferred\n-     for DFmode copies */\n-  ~(m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_GEODE | m_AMD_MULTIPLE | m_GENERIC),\n-\n-  /* X86_TUNE_PARTIAL_REG_DEPENDENCY */\n-  m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_AMD_MULTIPLE | m_GENERIC,\n-\n-  /* X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY: In the Generic model we have a\n-     conflict here in between PPro/Pentium4 based chips that thread 128bit\n-     SSE registers as single units versus K8 based chips that divide SSE\n-     registers to two 64bit halves.  This knob promotes all store destinations\n-     to be 128bit to allow register renaming on 128bit SSE units, but usually\n-     results in one extra microop on 64bit SSE units.  Experimental results\n-     shows that disabling this option on P4 brings over 20% SPECfp regression,\n-     while enabling it on K8 brings roughly 2.4% regression that can be partly\n-     masked by careful scheduling of moves.  */\n-  m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_AMDFAM10 | m_BDVER | m_GENERIC,\n-\n-  /* X86_TUNE_SSE_UNALIGNED_LOAD_OPTIMAL */\n-  m_COREI7 | m_AMDFAM10 | m_BDVER | m_BTVER | m_SLM,\n-\n-  /* X86_TUNE_SSE_UNALIGNED_STORE_OPTIMAL */\n-  m_COREI7 | m_BDVER | m_SLM,\n-\n-  /* X86_TUNE_SSE_PACKED_SINGLE_INSN_OPTIMAL */\n-  m_BDVER ,\n-\n-  /* X86_TUNE_SSE_SPLIT_REGS: Set for machines where the type and dependencies\n-     are resolved on SSE register parts instead of whole registers, so we may\n-     maintain just lower part of scalar values in proper format leaving the\n-     upper part undefined.  */\n-  m_ATHLON_K8,\n-\n-  /* X86_TUNE_SSE_TYPELESS_STORES */\n-  m_AMD_MULTIPLE,\n-\n-  /* X86_TUNE_SSE_LOAD0_BY_PXOR */\n-  m_PPRO | m_P4_NOCONA,\n-\n-  /* X86_TUNE_MEMORY_MISMATCH_STALL */\n-  m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_AMD_MULTIPLE | m_GENERIC,\n-\n-  /* X86_TUNE_PROLOGUE_USING_MOVE */\n-  m_PPRO | m_ATHLON_K8,\n-\n-  /* X86_TUNE_EPILOGUE_USING_MOVE */\n-  m_PPRO | m_ATHLON_K8,\n-\n-  /* X86_TUNE_SHIFT1 */\n-  ~m_486,\n-\n-  /* X86_TUNE_USE_FFREEP */\n-  m_AMD_MULTIPLE,\n-\n-  /* X86_TUNE_INTER_UNIT_MOVES_TO_VEC */\n-  ~(m_AMD_MULTIPLE | m_GENERIC),\n-\n-  /* X86_TUNE_INTER_UNIT_MOVES_FROM_VEC */\n-  ~m_ATHLON_K8,\n-\n-  /* X86_TUNE_INTER_UNIT_CONVERSIONS */\n-  ~(m_AMDFAM10 | m_BDVER ),\n-\n-  /* X86_TUNE_FOUR_JUMP_LIMIT: Some CPU cores are not able to predict more\n-     than 4 branch instructions in the 16 byte window.  */\n-  m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_AMD_MULTIPLE | m_GENERIC,\n-\n-  /* X86_TUNE_SCHEDULE */\n-  m_PENT | m_PPRO | m_CORE_ALL | m_ATOM | m_SLM | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC,\n-\n-  /* X86_TUNE_USE_BT */\n-  m_CORE_ALL | m_ATOM | m_SLM | m_AMD_MULTIPLE | m_GENERIC,\n-\n-  /* X86_TUNE_USE_INCDEC */\n-  ~(m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_GENERIC),\n-\n-  /* X86_TUNE_PAD_RETURNS */\n-  m_CORE_ALL | m_AMD_MULTIPLE | m_GENERIC,\n-\n-  /* X86_TUNE_PAD_SHORT_FUNCTION: Pad short function.  */\n-  m_ATOM,\n-\n-  /* X86_TUNE_EXT_80387_CONSTANTS */\n-  m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_K6_GEODE | m_ATHLON_K8 | m_GENERIC,\n-\n-  /* X86_TUNE_AVOID_VECTOR_DECODE */\n-  m_CORE_ALL | m_K8 | m_GENERIC64,\n-\n-  /* X86_TUNE_PROMOTE_HIMODE_IMUL: Modern CPUs have same latency for HImode\n-     and SImode multiply, but 386 and 486 do HImode multiply faster.  */\n-  ~(m_386 | m_486),\n-\n-  /* X86_TUNE_SLOW_IMUL_IMM32_MEM: Imul of 32-bit constant and memory is\n-     vector path on AMD machines.  */\n-  m_CORE_ALL | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC64,\n-\n-  /* X86_TUNE_SLOW_IMUL_IMM8: Imul of 8-bit constant is vector path on AMD\n-     machines.  */\n-  m_CORE_ALL | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC64,\n-\n-  /* X86_TUNE_MOVE_M1_VIA_OR: On pentiums, it is faster to load -1 via OR\n-     than a MOV.  */\n-  m_PENT,\n-\n-  /* X86_TUNE_NOT_UNPAIRABLE: NOT is not pairable on Pentium, while XOR is,\n-     but one byte longer.  */\n-  m_PENT,\n-\n-  /* X86_TUNE_NOT_VECTORMODE: On AMD K6, NOT is vector decoded with memory\n-     operand that cannot be represented using a modRM byte.  The XOR\n-     replacement is long decoded, so this split helps here as well.  */\n-  m_K6,\n-\n-  /* X86_TUNE_USE_VECTOR_FP_CONVERTS: Prefer vector packed SSE conversion\n-     from FP to FP. */\n-  m_CORE_ALL | m_AMDFAM10 | m_GENERIC,\n-\n-  /* X86_TUNE_USE_VECTOR_CONVERTS: Prefer vector packed SSE conversion\n-     from integer to FP. */\n-  m_AMDFAM10,\n-\n-  /* X86_TUNE_FUSE_CMP_AND_BRANCH: Fuse a compare or test instruction\n-     with a subsequent conditional jump instruction into a single\n-     compare-and-branch uop.  */\n-  m_BDVER,\n-\n-  /* X86_TUNE_OPT_AGU: Optimize for Address Generation Unit. This flag\n-     will impact LEA instruction selection. */\n-  m_ATOM | m_SLM,\n-\n-  /* X86_TUNE_VECTORIZE_DOUBLE: Enable double precision vector\n-     instructions.  */\n-  ~m_ATOM,\n-\n-  /* X86_TUNE_SOFTWARE_PREFETCHING_BENEFICIAL: Enable software prefetching\n-     at -O3.  For the moment, the prefetching seems badly tuned for Intel\n-     chips.  */\n-  m_K6_GEODE | m_AMD_MULTIPLE,\n-\n-  /* X86_TUNE_AVX128_OPTIMAL: Enable 128-bit AVX instruction generation for\n-     the auto-vectorizer.  */\n-  m_BDVER | m_BTVER2,\n-\n-  /* X86_TUNE_REASSOC_INT_TO_PARALLEL: Try to produce parallel computations\n-     during reassociation of integer computation.  */\n-  m_ATOM,\n-\n-  /* X86_TUNE_REASSOC_FP_TO_PARALLEL: Try to produce parallel computations\n-     during reassociation of fp computation.  */\n-  m_ATOM | m_SLM | m_HASWELL | m_BDVER1 | m_BDVER2,\n-\n-  /* X86_TUNE_GENERAL_REGS_SSE_SPILL: Try to spill general regs to SSE\n-     regs instead of memory.  */\n-  m_CORE_ALL,\n-\n-  /* X86_TUNE_AVOID_MEM_OPND_FOR_CMOVE: Try to avoid memory operands for\n-     a conditional move.  */\n-  m_ATOM,\n-\n-  /* X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS: Try to split memory operand for\n-     fp converts to destination register.  */\n-  m_SLM\n-\n+#undef DEF_TUNE\n+#define DEF_TUNE(tune, name, selector) selector,\n+#include \"x86-tune.def\"\n+#undef DEF_TUNE\n };\n \n /* Feature tests against the various architecture variations.  */\n@@ -3171,6 +2900,80 @@ ix86_parse_stringop_strategy_string (char *strategy_str, bool is_memset)\n }\n \n \f\n+/* parse -mtune-ctrl= option. When DUMP is true,\n+   print the features that are explicitly set.  */\n+\n+static void\n+parse_mtune_ctrl_str (bool dump)\n+{\n+  if (!ix86_tune_ctrl_string)\n+    return;\n+\n+  char *next_feature_string = NULL;\n+  char *curr_feature_string = xstrdup (ix86_tune_ctrl_string);\n+  char *orig = curr_feature_string;\n+  int i;\n+  do\n+    {\n+      bool clear = false;\n+\n+      next_feature_string = strchr (curr_feature_string, ',');\n+      if (next_feature_string)\n+        *next_feature_string++ = '\\0';\n+      if (*curr_feature_string == '^')\n+        {\n+          curr_feature_string++;\n+          clear = true;\n+        }\n+      for (i = 0; i < X86_TUNE_LAST; i++)\n+        {\n+          if (!strcmp (curr_feature_string, ix86_tune_feature_names[i]))\n+            {\n+              ix86_tune_features[i] = !clear;\n+              if (dump)\n+                fprintf (stderr, \"Explicitly %s feature %s\\n\",\n+                         clear ? \"clear\" : \"set\", ix86_tune_feature_names[i]);\n+              break;\n+            }\n+        }\n+      if (i == X86_TUNE_LAST)\n+        error (\"Unknown parameter to option -mtune-ctrl: %s\",\n+               clear ? curr_feature_string - 1 : curr_feature_string);\n+      curr_feature_string = next_feature_string;\n+    }\n+  while (curr_feature_string);\n+  free (orig);\n+}\n+\n+/* Helper function to set ix86_tune_features. IX86_TUNE is the\n+   processor type.  */\n+\n+static void\n+set_ix86_tune_features (enum processor_type ix86_tune, bool dump)\n+{\n+  unsigned int ix86_tune_mask = 1u << ix86_tune;\n+  int i;\n+\n+  for (i = 0; i < X86_TUNE_LAST; ++i)\n+    {\n+      if (ix86_tune_no_default)\n+        ix86_tune_features[i] = 0;\n+      else\n+        ix86_tune_features[i] = !!(initial_ix86_tune_features[i] & ix86_tune_mask);\n+    }\n+\n+  if (dump)\n+    {\n+      fprintf (stderr, \"List of x86 specific tuning parameter names:\\n\");\n+      for (i = 0; i < X86_TUNE_LAST; i++)\n+        fprintf (stderr, \"%s : %s\\n\", ix86_tune_feature_names[i],\n+                 ix86_tune_features[i] ? \"on\" : \"off\");\n+    }\n+\n+  parse_mtune_ctrl_str (dump);\n+}\n+\n+\n /* Override various settings based on options.  If MAIN_ARGS_P, the\n    options are from the command line, otherwise they are from\n    attributes.  */\n@@ -3816,43 +3619,7 @@ ix86_option_override_internal (bool main_args_p)\n     error (\"bad value (%s) for %stune=%s %s\",\n \t   ix86_tune_string, prefix, suffix, sw);\n \n-  ix86_tune_mask = 1u << ix86_tune;\n-  for (i = 0; i < X86_TUNE_LAST; ++i)\n-    ix86_tune_features[i] = !!(initial_ix86_tune_features[i] & ix86_tune_mask);\n-\n-  if (ix86_tune_ctrl_string)\n-    {\n-      /* parse the tune ctrl string in the following form:\n-         [^]tune_name1,[^]tune_name2,..a */\n-      char *next_feature_string = NULL;\n-      char *curr_feature_string = xstrdup (ix86_tune_ctrl_string);\n-      char *orig = curr_feature_string;\n-      do {\n-        bool clear = false;\n-\n-        next_feature_string = strchr (curr_feature_string, ',');\n-\tif (next_feature_string)\n-          *next_feature_string++ = '\\0';\n-        if (*curr_feature_string == '^')\n-\t  {\n-\t    curr_feature_string++;\n-\t    clear = true;\n-\t  }\n-        for (i = 0; i < X86_TUNE_LAST; i++)\n-\t  {\n-            if (!strcmp (curr_feature_string, ix86_tune_feature_names[i]))\n-\t      {\n-                ix86_tune_features[i] = !clear;\n-                break;\n-              }\n-\t  }\n-        if (i == X86_TUNE_LAST)\n-\t  warning (0, \"Unknown parameter to option -mtune-ctrl: %s\",\n-\t           clear ? curr_feature_string - 1 : curr_feature_string);\n-\tcurr_feature_string = next_feature_string;    \n-      } while (curr_feature_string);\n-      free (orig);\n-    }\n+  set_ix86_tune_features (ix86_tune, ix86_dump_tunes);\n \n #ifndef USE_IX86_FRAME_POINTER\n #define USE_IX86_FRAME_POINTER 0\n@@ -4088,6 +3855,7 @@ ix86_option_override_internal (bool main_args_p)\n \tgcc_unreachable ();\n       }\n \n+  ix86_tune_mask = 1u << ix86_tune;\n   if ((!USE_IX86_FRAME_POINTER\n        || (x86_accumulate_outgoing_args & ix86_tune_mask))\n       && !(target_flags_explicit & MASK_ACCUMULATE_OUTGOING_ARGS)\n@@ -4450,7 +4218,7 @@ ix86_function_specific_restore (struct cl_target_option *ptr)\n {\n   enum processor_type old_tune = ix86_tune;\n   enum processor_type old_arch = ix86_arch;\n-  unsigned int ix86_arch_mask, ix86_tune_mask;\n+  unsigned int ix86_arch_mask;\n   int i;\n \n   ix86_arch = (enum processor_type) ptr->arch;\n@@ -4474,12 +4242,7 @@ ix86_function_specific_restore (struct cl_target_option *ptr)\n \n   /* Recreate the tune optimization tests */\n   if (old_tune != ix86_tune)\n-    {\n-      ix86_tune_mask = 1u << ix86_tune;\n-      for (i = 0; i < X86_TUNE_LAST; ++i)\n-\tix86_tune_features[i]\n-\t  = !!(initial_ix86_tune_features[i] & ix86_tune_mask);\n-    }\n+    set_ix86_tune_features (ix86_tune, false);\n }\n \n /* Print the current options */"}, {"sha": "6be93ac7df458e3dc3d08d69a32b84d283e7d482", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3ad20bd44836e57453b743466f1ca0d591bd10ac/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3ad20bd44836e57453b743466f1ca0d591bd10ac/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=3ad20bd44836e57453b743466f1ca0d591bd10ac", "patch": "@@ -262,7 +262,7 @@ extern const struct processor_costs ix86_size_cost;\n /* Feature tests against the various tunings.  */\n enum ix86_tune_indices {\n #undef DEF_TUNE\n-#define DEF_TUNE(tune, name) tune,\n+#define DEF_TUNE(tune, name, selector) tune,\n #include \"x86-tune.def\"\n #undef DEF_TUNE\n X86_TUNE_LAST"}, {"sha": "ff9bd5c4734380101cbf28e04b263cfa0f766451", "filename": "gcc/config/i386/i386.opt", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3ad20bd44836e57453b743466f1ca0d591bd10ac/gcc%2Fconfig%2Fi386%2Fi386.opt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3ad20bd44836e57453b743466f1ca0d591bd10ac/gcc%2Fconfig%2Fi386%2Fi386.opt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.opt?ref=3ad20bd44836e57453b743466f1ca0d591bd10ac", "patch": "@@ -382,6 +382,13 @@ mtune-ctrl=\n Target RejectNegative Joined Var(ix86_tune_ctrl_string)\n Fine grain control of tune features\n \n+mno-default\n+Target RejectNegative Var(ix86_tune_no_default) Init(0)\n+Clear all tune features\n+\n+mdump-tune-features\n+Target RejectNegative Var(ix86_dump_tunes) Init(0)\n+\n mabi=\n Target RejectNegative Joined Var(ix86_abi) Enum(calling_abi) Init(SYSV_ABI)\n Generate code that conforms to the given ABI"}, {"sha": "e3a34ee7b2eddf99d060b0bc34afbf03e9a7b8b2", "filename": "gcc/config/i386/x86-tune.def", "status": "modified", "additions": 213, "deletions": 78, "changes": 291, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3ad20bd44836e57453b743466f1ca0d591bd10ac/gcc%2Fconfig%2Fi386%2Fx86-tune.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3ad20bd44836e57453b743466f1ca0d591bd10ac/gcc%2Fconfig%2Fi386%2Fx86-tune.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fx86-tune.def?ref=3ad20bd44836e57453b743466f1ca0d591bd10ac", "patch": "@@ -1,4 +1,4 @@\n-/* Definitions of target machine for GCC for IA-32.\n+/* Definitions of x86 tunable features.\n    Copyright (C) 2013 Free Software Foundation, Inc.\n \n This file is part of GCC.\n@@ -13,85 +13,220 @@ but WITHOUT ANY WARRANTY; without even the implied warranty of\n MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n GNU General Public License for more details.\n \n-Under Section 7 of GPL version 3, you are granted additional\n-permissions described in the GCC Runtime Library Exception, version\n-3.1, as published by the Free Software Foundation.\n-\n You should have received a copy of the GNU General Public License and\n a copy of the GCC Runtime Library Exception along with this program;\n see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n <http://www.gnu.org/licenses/>.  */\n \n-DEF_TUNE (X86_TUNE_USE_LEAVE, \"use_leave\")\n-DEF_TUNE (X86_TUNE_PUSH_MEMORY, \"push_memory\")\n-DEF_TUNE (X86_TUNE_ZERO_EXTEND_WITH_AND, \"zero_extend_with_and\")\n-DEF_TUNE (X86_TUNE_UNROLL_STRLEN, \"unroll_strlen\")\n-DEF_TUNE (X86_TUNE_BRANCH_PREDICTION_HINTS, \"branch_prediction_hints\")\n-DEF_TUNE (X86_TUNE_DOUBLE_WITH_ADD, \"double_with_add\")\n-DEF_TUNE (X86_TUNE_USE_SAHF, \"use_sahf\")\n-DEF_TUNE (X86_TUNE_MOVX, \"movx\")\n-DEF_TUNE (X86_TUNE_PARTIAL_REG_STALL, \"partial_reg_stall\")\n-DEF_TUNE (X86_TUNE_PARTIAL_FLAG_REG_STALL, \"partial_flag_reg_stall\")\n-DEF_TUNE (X86_TUNE_LCP_STALL, \"lcp_stall\")\n-DEF_TUNE (X86_TUNE_USE_HIMODE_FIOP, \"use_himode_fiop\")\n-DEF_TUNE (X86_TUNE_USE_SIMODE_FIOP, \"use_simode_fiop\")\n-DEF_TUNE (X86_TUNE_USE_MOV0, \"use_mov0\")\n-DEF_TUNE (X86_TUNE_USE_CLTD, \"use_cltd\")\n-DEF_TUNE (X86_TUNE_USE_XCHGB, \"use_xchgb\")\n-DEF_TUNE (X86_TUNE_SPLIT_LONG_MOVES, \"split_long_moves\")\n-DEF_TUNE (X86_TUNE_READ_MODIFY_WRITE, \"read_modify_write\")\n-DEF_TUNE (X86_TUNE_READ_MODIFY, \"read_modify\")\n-DEF_TUNE (X86_TUNE_PROMOTE_QIMODE, \"promote_qimode\")\n-DEF_TUNE (X86_TUNE_FAST_PREFIX, \"fast_prefix\")\n-DEF_TUNE (X86_TUNE_SINGLE_STRINGOP, \"single_stringop\")\n-DEF_TUNE (X86_TUNE_QIMODE_MATH, \"qimode_math\")\n-DEF_TUNE (X86_TUNE_HIMODE_MATH, \"himode_math\")\n-DEF_TUNE (X86_TUNE_PROMOTE_QI_REGS, \"promote_qi_regs\")\n-DEF_TUNE (X86_TUNE_PROMOTE_HI_REGS, \"promote_hi_regs\")\n-DEF_TUNE (X86_TUNE_SINGLE_POP, \"single_pop\")\n-DEF_TUNE (X86_TUNE_DOUBLE_POP, \"double_pop\")\n-DEF_TUNE (X86_TUNE_SINGLE_PUSH, \"single_push\")\n-DEF_TUNE (X86_TUNE_DOUBLE_PUSH, \"double_push\")\n-DEF_TUNE (X86_TUNE_INTEGER_DFMODE_MOVES, \"integer_dfmode_moves\")\n-DEF_TUNE (X86_TUNE_PARTIAL_REG_DEPENDENCY, \"partial_reg_dependency\")\n-DEF_TUNE (X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY, \"sse_partial_reg_dependency\")\n-DEF_TUNE (X86_TUNE_SSE_UNALIGNED_LOAD_OPTIMAL, \"sse_unaligned_load_optimal\")\n-DEF_TUNE (X86_TUNE_SSE_UNALIGNED_STORE_OPTIMAL, \"sse_unaligned_store_optimal\")\n-DEF_TUNE (X86_TUNE_SSE_PACKED_SINGLE_INSN_OPTIMAL, \"sse_packed_single_insn_optimal\")\n-DEF_TUNE (X86_TUNE_SSE_SPLIT_REGS, \"sse_split_regs\")\n-DEF_TUNE (X86_TUNE_SSE_TYPELESS_STORES, \"sse_typeless_stores\")\n-DEF_TUNE (X86_TUNE_SSE_LOAD0_BY_PXOR, \"sse_load0_by_pxor\")\n-DEF_TUNE (X86_TUNE_MEMORY_MISMATCH_STALL, \"memory_mismatch_stall\")\n-DEF_TUNE (X86_TUNE_PROLOGUE_USING_MOVE, \"prologue_using_move\")\n-DEF_TUNE (X86_TUNE_EPILOGUE_USING_MOVE, \"epilogue_using_move\")\n-DEF_TUNE (X86_TUNE_SHIFT1, \"shift1\")\n-DEF_TUNE (X86_TUNE_USE_FFREEP, \"use_ffreep\")\n-DEF_TUNE (X86_TUNE_INTER_UNIT_MOVES_TO_VEC, \"inter_unit_moves_to_vec\")\n-DEF_TUNE (X86_TUNE_INTER_UNIT_MOVES_FROM_VEC, \"inter_unit_moves_from_vec\")\n-DEF_TUNE (X86_TUNE_INTER_UNIT_CONVERSIONS, \"inter_unit_conversions\")\n-DEF_TUNE (X86_TUNE_FOUR_JUMP_LIMIT, \"four_jump_limit\")\n-DEF_TUNE (X86_TUNE_SCHEDULE, \"schedule\")\n-DEF_TUNE (X86_TUNE_USE_BT, \"use_bt\")\n-DEF_TUNE (X86_TUNE_USE_INCDEC, \"use_incdec\")\n-DEF_TUNE (X86_TUNE_PAD_RETURNS, \"pad_returns\")\n-DEF_TUNE (X86_TUNE_PAD_SHORT_FUNCTION, \"pad_short_function\")\n-DEF_TUNE (X86_TUNE_EXT_80387_CONSTANTS, \"ext_80387_constants\")\n-DEF_TUNE (X86_TUNE_AVOID_VECTOR_DECODE, \"avoid_vector_decode\")\n-DEF_TUNE (X86_TUNE_PROMOTE_HIMODE_IMUL, \"promote_himode_imul\")\n-DEF_TUNE (X86_TUNE_SLOW_IMUL_IMM32_MEM, \"slow_imul_imm32_mem\")\n-DEF_TUNE (X86_TUNE_SLOW_IMUL_IMM8, \"slow_imul_imm8\")\n-DEF_TUNE (X86_TUNE_MOVE_M1_VIA_OR, \"move_m1_via_or\")\n-DEF_TUNE (X86_TUNE_NOT_UNPAIRABLE, \"not_unpairable\")\n-DEF_TUNE (X86_TUNE_NOT_VECTORMODE, \"not_vectormode\")\n-DEF_TUNE (X86_TUNE_USE_VECTOR_FP_CONVERTS, \"use_vector_fp_converts\")\n-DEF_TUNE (X86_TUNE_USE_VECTOR_CONVERTS, \"use_vector_converts\")\n-DEF_TUNE (X86_TUNE_FUSE_CMP_AND_BRANCH, \"fuse_cmp_and_branch\")\n-DEF_TUNE (X86_TUNE_OPT_AGU, \"opt_agu\")\n-DEF_TUNE (X86_TUNE_VECTORIZE_DOUBLE, \"vectorize_double\")\n-DEF_TUNE (X86_TUNE_SOFTWARE_PREFETCHING_BENEFICIAL, \"software_prefetching_beneficial\")\n-DEF_TUNE (X86_TUNE_AVX128_OPTIMAL, \"avx128_optimal\")\n-DEF_TUNE (X86_TUNE_REASSOC_INT_TO_PARALLEL, \"reassoc_int_to_parallel\")\n-DEF_TUNE (X86_TUNE_REASSOC_FP_TO_PARALLEL, \"reassoc_fp_to_parallel\")\n-DEF_TUNE (X86_TUNE_GENERAL_REGS_SSE_SPILL, \"general_regs_sse_spill\")\n-DEF_TUNE (X86_TUNE_AVOID_MEM_OPND_FOR_CMOVE, \"avoid_mem_opnd_for_cmove\")\n-DEF_TUNE (X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS, \"split_mem_opnd_for_fp_converts\")\n+/* X86_TUNE_USE_LEAVE: Leave does not affect Nocona SPEC2000 results\n+   negatively, so enabling for Generic64 seems like good code size\n+   tradeoff.  We can't enable it for 32bit generic because it does not\n+   work well with PPro base chips.  */\n+DEF_TUNE (X86_TUNE_USE_LEAVE, \"use_leave\", \n+\t  m_386 | m_CORE_ALL | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC64)\n+DEF_TUNE (X86_TUNE_PUSH_MEMORY, \"push_memory\", \n+          m_386 | m_P4_NOCONA | m_CORE_ALL | m_K6_GEODE | m_AMD_MULTIPLE \n+          | m_GENERIC)\n+DEF_TUNE (X86_TUNE_ZERO_EXTEND_WITH_AND, \"zero_extend_with_and\",  m_486 | m_PENT)\n+DEF_TUNE (X86_TUNE_UNROLL_STRLEN, \"unroll_strlen\",\n+\t m_486 | m_PENT | m_PPRO | m_ATOM | m_SLM | m_CORE_ALL | m_K6\n+\t | m_AMD_MULTIPLE | m_GENERIC)\n+/* X86_TUNE_BRANCH_PREDICTION_HINTS: Branch hints were put in P4 based\n+   on simulation result. But after P4 was made, no performance benefit\n+   was observed with branch hints.  It also increases the code size.\n+   As a result, icc never generates branch hints.  */\n+DEF_TUNE (X86_TUNE_BRANCH_PREDICTION_HINTS, \"branch_prediction_hints\", 0)\n+DEF_TUNE (X86_TUNE_DOUBLE_WITH_ADD, \"double_with_add\", ~m_386)\n+DEF_TUNE (X86_TUNE_USE_SAHF, \"use_sahf\",\n+          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_K6_GEODE\n+          | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC)\n+/* X86_TUNE_MOVX: Enable to zero extend integer registers to avoid\n+   partial dependencies.  */\n+DEF_TUNE (X86_TUNE_MOVX, \"movx\",\n+          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_GEODE \n+          | m_AMD_MULTIPLE  | m_GENERIC)\n+/* X86_TUNE_PARTIAL_REG_STALL: We probably ought to watch for partial\n+   register stalls on Generic32 compilation setting as well.  However\n+   in current implementation the partial register stalls are not eliminated\n+   very well - they can be introduced via subregs synthesized by combine\n+   and can happen in caller/callee saving sequences.  Because this option\n+   pays back little on PPro based chips and is in conflict with partial reg\n+   dependencies used by Athlon/P4 based chips, it is better to leave it off\n+   for generic32 for now.  */\n+DEF_TUNE (X86_TUNE_PARTIAL_REG_STALL, \"partial_reg_stall\", m_PPRO)\n+DEF_TUNE (X86_TUNE_PARTIAL_FLAG_REG_STALL, \"partial_flag_reg_stall\",\n+          m_CORE_ALL | m_GENERIC)\n+/* X86_TUNE_LCP_STALL: Avoid an expensive length-changing prefix stall\n+ * on 16-bit immediate moves into memory on Core2 and Corei7.  */\n+DEF_TUNE (X86_TUNE_LCP_STALL, \"lcp_stall\", m_CORE_ALL | m_GENERIC)\n+DEF_TUNE (X86_TUNE_USE_HIMODE_FIOP, \"use_himode_fiop\", \n+          m_386 | m_486 | m_K6_GEODE)\n+DEF_TUNE (X86_TUNE_USE_SIMODE_FIOP, \"use_simode_fiop\",\n+          ~(m_PENT | m_PPRO | m_CORE_ALL | m_ATOM \n+            | m_SLM | m_AMD_MULTIPLE | m_GENERIC))\n+DEF_TUNE (X86_TUNE_USE_MOV0, \"use_mov0\", m_K6)\n+DEF_TUNE (X86_TUNE_USE_CLTD, \"use_cltd\", ~(m_PENT | m_ATOM | m_SLM | m_K6))\n+/* X86_TUNE_USE_XCHGB: Use xchgb %rh,%rl instead of rolw/rorw $8,rx.  */\n+DEF_TUNE (X86_TUNE_USE_XCHGB, \"use_xchgb\", m_PENT4)\n+DEF_TUNE (X86_TUNE_SPLIT_LONG_MOVES, \"split_long_moves\", m_PPRO)\n+DEF_TUNE (X86_TUNE_READ_MODIFY_WRITE, \"read_modify_write\", ~m_PENT)\n+DEF_TUNE (X86_TUNE_READ_MODIFY, \"read_modify\", ~(m_PENT | m_PPRO))\n+DEF_TUNE (X86_TUNE_PROMOTE_QIMODE, \"promote_qimode\",\n+          m_386 | m_486 | m_PENT | m_CORE_ALL | m_ATOM | m_SLM \n+          | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC)\n+DEF_TUNE (X86_TUNE_FAST_PREFIX, \"fast_prefix\", ~(m_386 | m_486 | m_PENT))\n+DEF_TUNE (X86_TUNE_SINGLE_STRINGOP, \"single_stringop\", m_386 | m_P4_NOCONA)\n+DEF_TUNE (X86_TUNE_QIMODE_MATH, \"qimode_math\", ~0)\n+/* X86_TUNE_HIMODE_MATH: On PPro this flag is meant to avoid partial\n+   register stalls.  Just like X86_TUNE_PARTIAL_REG_STALL this option\n+   might be considered for Generic32 if our scheme for avoiding partial\n+   stalls was more effective.  */\n+DEF_TUNE (X86_TUNE_HIMODE_MATH, \"himode_math\", ~m_PPRO)\n+DEF_TUNE (X86_TUNE_PROMOTE_QI_REGS, \"promote_qi_regs\", 0)\n+DEF_TUNE (X86_TUNE_PROMOTE_HI_REGS, \"promote_hi_regs\", m_PPRO)\n+/* X86_TUNE_SINGLE_POP: Enable if single pop insn is preferred\n+   over esp addition.  */\n+DEF_TUNE (X86_TUNE_SINGLE_POP, \"single_pop\", m_386 | m_486 | m_PENT | m_PPRO)\n+/* X86_TUNE_DOUBLE_POP: Enable if double pop insn is preferred\n+   over esp addition.  */\n+DEF_TUNE (X86_TUNE_DOUBLE_POP, \"double_pop\", m_PENT)\n+/* X86_TUNE_SINGLE_PUSH: Enable if single push insn is preferred\n+   over esp subtraction.  */\n+DEF_TUNE (X86_TUNE_SINGLE_PUSH, \"single_push\", m_386 | m_486 | m_PENT \n+          | m_K6_GEODE)\n+/* X86_TUNE_DOUBLE_PUSH. Enable if double push insn is preferred\n+   over esp subtraction.  */\n+DEF_TUNE (X86_TUNE_DOUBLE_PUSH, \"double_push\", m_PENT | m_K6_GEODE)\n+/* X86_TUNE_INTEGER_DFMODE_MOVES: Enable if integer moves are preferred\n+   for DFmode copies */\n+DEF_TUNE (X86_TUNE_INTEGER_DFMODE_MOVES, \"integer_dfmode_moves\",\n+          ~(m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM \n+          | m_GEODE | m_AMD_MULTIPLE | m_GENERIC))\n+DEF_TUNE (X86_TUNE_PARTIAL_REG_DEPENDENCY, \"partial_reg_dependency\",\n+          m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_AMD_MULTIPLE \n+          | m_GENERIC)\n+/* X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY: In the Generic model we have a\n+   conflict here in between PPro/Pentium4 based chips that thread 128bit\n+   SSE registers as single units versus K8 based chips that divide SSE\n+   registers to two 64bit halves.  This knob promotes all store destinations\n+   to be 128bit to allow register renaming on 128bit SSE units, but usually\n+   results in one extra microop on 64bit SSE units.  Experimental results\n+   shows that disabling this option on P4 brings over 20% SPECfp regression,\n+   while enabling it on K8 brings roughly 2.4% regression that can be partly\n+    masked by careful scheduling of moves.  */\n+DEF_TUNE (X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY, \"sse_partial_reg_dependency\",\n+          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_AMDFAM10 \n+          | m_BDVER | m_GENERIC)\n+DEF_TUNE (X86_TUNE_SSE_UNALIGNED_LOAD_OPTIMAL, \"sse_unaligned_load_optimal\",\n+          m_COREI7 | m_AMDFAM10 | m_BDVER | m_BTVER | m_SLM)\n+DEF_TUNE (X86_TUNE_SSE_UNALIGNED_STORE_OPTIMAL, \"sse_unaligned_store_optimal\",\n+          m_COREI7 | m_BDVER | m_SLM)\n+DEF_TUNE (X86_TUNE_SSE_PACKED_SINGLE_INSN_OPTIMAL, \"sse_packed_single_insn_optimal\",\n+          m_BDVER)\n+/* X86_TUNE_SSE_SPLIT_REGS: Set for machines where the type and dependencies\n+   are resolved on SSE register parts instead of whole registers, so we may\n+   maintain just lower part of scalar values in proper format leaving the\n+   upper part undefined.  */\n+DEF_TUNE (X86_TUNE_SSE_SPLIT_REGS, \"sse_split_regs\", m_ATHLON_K8)\n+DEF_TUNE (X86_TUNE_SSE_TYPELESS_STORES, \"sse_typeless_stores\", m_AMD_MULTIPLE)\n+DEF_TUNE (X86_TUNE_SSE_LOAD0_BY_PXOR, \"sse_load0_by_pxor\", m_PPRO | m_P4_NOCONA)\n+DEF_TUNE (X86_TUNE_MEMORY_MISMATCH_STALL, \"memory_mismatch_stall\",\n+          m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_AMD_MULTIPLE | m_GENERIC)\n+DEF_TUNE (X86_TUNE_PROLOGUE_USING_MOVE, \"prologue_using_move\", \n+          m_PPRO | m_ATHLON_K8)\n+DEF_TUNE (X86_TUNE_EPILOGUE_USING_MOVE, \"epilogue_using_move\",\n+          m_PPRO | m_ATHLON_K8)\t\n+DEF_TUNE (X86_TUNE_SHIFT1, \"shift1\", ~m_486)\n+DEF_TUNE (X86_TUNE_USE_FFREEP, \"use_ffreep\", m_AMD_MULTIPLE)\n+DEF_TUNE (X86_TUNE_INTER_UNIT_MOVES_TO_VEC, \"inter_unit_moves_to_vec\",\n+          ~(m_AMD_MULTIPLE | m_GENERIC))\n+DEF_TUNE (X86_TUNE_INTER_UNIT_MOVES_FROM_VEC, \"inter_unit_moves_from_vec\",\n+          ~m_ATHLON_K8)\n+DEF_TUNE (X86_TUNE_INTER_UNIT_CONVERSIONS, \"inter_unit_conversions\",\n+          ~(m_AMDFAM10 | m_BDVER ))\n+/* X86_TUNE_FOUR_JUMP_LIMIT: Some CPU cores are not able to predict more\n+   than 4 branch instructions in the 16 byte window.  */\n+DEF_TUNE (X86_TUNE_FOUR_JUMP_LIMIT, \"four_jump_limit\",\n+          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM| m_AMD_MULTIPLE \n+          | m_GENERIC)\n+DEF_TUNE (X86_TUNE_SCHEDULE, \"schedule\",\n+          m_PENT | m_PPRO | m_CORE_ALL | m_ATOM | m_SLM | m_K6_GEODE \n+          | m_AMD_MULTIPLE | m_GENERIC)\n+DEF_TUNE (X86_TUNE_USE_BT, \"use_bt\",\n+          m_CORE_ALL | m_ATOM | m_SLM | m_AMD_MULTIPLE | m_GENERIC)\n+DEF_TUNE (X86_TUNE_USE_INCDEC, \"use_incdec\",\n+          ~(m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_GENERIC))\n+DEF_TUNE (X86_TUNE_PAD_RETURNS, \"pad_returns\",\n+          m_CORE_ALL | m_AMD_MULTIPLE | m_GENERIC)\n+DEF_TUNE (X86_TUNE_PAD_SHORT_FUNCTION, \"pad_short_function\", m_ATOM)\n+DEF_TUNE (X86_TUNE_EXT_80387_CONSTANTS, \"ext_80387_constants\",\n+          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_K6_GEODE\n+          | m_ATHLON_K8 | m_GENERIC)\n+DEF_TUNE (X86_TUNE_AVOID_VECTOR_DECODE, \"avoid_vector_decode\",\n+          m_CORE_ALL | m_K8 | m_GENERIC64)\n+/* X86_TUNE_PROMOTE_HIMODE_IMUL: Modern CPUs have same latency for HImode\n+   and SImode multiply, but 386 and 486 do HImode multiply faster.  */\n+DEF_TUNE (X86_TUNE_PROMOTE_HIMODE_IMUL, \"promote_himode_imul\",\n+          ~(m_386 | m_486))\n+/* X86_TUNE_SLOW_IMUL_IMM32_MEM: Imul of 32-bit constant and memory is\n+   vector path on AMD machines.  */\n+DEF_TUNE (X86_TUNE_SLOW_IMUL_IMM32_MEM, \"slow_imul_imm32_mem\",\n+          m_CORE_ALL | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC64)\n+/* X86_TUNE_SLOW_IMUL_IMM8: Imul of 8-bit constant is vector path on AMD\n+   machines.  */\n+DEF_TUNE (X86_TUNE_SLOW_IMUL_IMM8, \"slow_imul_imm8\",\n+          m_CORE_ALL | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC64)\n+/* X86_TUNE_MOVE_M1_VIA_OR: On pentiums, it is faster to load -1 via OR\n+   than a MOV.  */\n+DEF_TUNE (X86_TUNE_MOVE_M1_VIA_OR, \"move_m1_via_or\", m_PENT)\n+/* X86_TUNE_NOT_UNPAIRABLE: NOT is not pairable on Pentium, while XOR is,\n+   but one byte longer.  */\n+DEF_TUNE (X86_TUNE_NOT_UNPAIRABLE, \"not_unpairable\", m_PENT)\n+/* X86_TUNE_NOT_VECTORMODE: On AMD K6, NOT is vector decoded with memory\n+   operand that cannot be represented using a modRM byte.  The XOR\n+   replacement is long decoded, so this split helps here as well.  */\n+DEF_TUNE (X86_TUNE_NOT_VECTORMODE, \"not_vectormode\", m_K6)\n+/* X86_TUNE_USE_VECTOR_FP_CONVERTS: Prefer vector packed SSE conversion\n+   from FP to FP. */\n+DEF_TUNE (X86_TUNE_USE_VECTOR_FP_CONVERTS, \"use_vector_fp_converts\",\n+          m_CORE_ALL | m_AMDFAM10 | m_GENERIC)\n+/* X86_TUNE_USE_VECTOR_CONVERTS: Prefer vector packed SSE conversion\n+   from integer to FP. */\n+DEF_TUNE (X86_TUNE_USE_VECTOR_CONVERTS, \"use_vector_converts\", m_AMDFAM10)\n+/* X86_TUNE_FUSE_CMP_AND_BRANCH: Fuse a compare or test instruction\n+   with a subsequent conditional jump instruction into a single\n+   compare-and-branch uop.  */\n+DEF_TUNE (X86_TUNE_FUSE_CMP_AND_BRANCH, \"fuse_cmp_and_branch\", m_BDVER)\n+/* X86_TUNE_OPT_AGU: Optimize for Address Generation Unit. This flag\n+   will impact LEA instruction selection. */\n+DEF_TUNE (X86_TUNE_OPT_AGU, \"opt_agu\", m_ATOM | m_SLM)\n+/* X86_TUNE_VECTORIZE_DOUBLE: Enable double precision vector\n+   instructions.  */\n+DEF_TUNE (X86_TUNE_VECTORIZE_DOUBLE, \"vectorize_double\", ~m_ATOM)\n+/* X86_TUNE_SOFTWARE_PREFETCHING_BENEFICIAL: Enable software prefetching\n+   at -O3.  For the moment, the prefetching seems badly tuned for Intel\n+   chips.  */\n+DEF_TUNE (X86_TUNE_SOFTWARE_PREFETCHING_BENEFICIAL, \"software_prefetching_beneficial\",\n+          m_K6_GEODE | m_AMD_MULTIPLE)\n+/* X86_TUNE_AVX128_OPTIMAL: Enable 128-bit AVX instruction generation for\n+   the auto-vectorizer.  */\n+DEF_TUNE (X86_TUNE_AVX128_OPTIMAL, \"avx128_optimal\", m_BDVER | m_BTVER2)\n+/* X86_TUNE_REASSOC_INT_TO_PARALLEL: Try to produce parallel computations\n+   during reassociation of integer computation.  */\n+DEF_TUNE (X86_TUNE_REASSOC_INT_TO_PARALLEL, \"reassoc_int_to_parallel\",\n+          m_ATOM)\n+/* X86_TUNE_REASSOC_FP_TO_PARALLEL: Try to produce parallel computations\n+   during reassociation of fp computation.  */\n+DEF_TUNE (X86_TUNE_REASSOC_FP_TO_PARALLEL, \"reassoc_fp_to_parallel\",\n+          m_ATOM | m_SLM | m_HASWELL | m_BDVER1 | m_BDVER2)\n+/* X86_TUNE_GENERAL_REGS_SSE_SPILL: Try to spill general regs to SSE\n+   regs instead of memory.  */\n+DEF_TUNE (X86_TUNE_GENERAL_REGS_SSE_SPILL, \"general_regs_sse_spill\",\n+          m_CORE_ALL)\n+/* X86_TUNE_AVOID_MEM_OPND_FOR_CMOVE: Try to avoid memory operands for\n+   a conditional move.  */\n+DEF_TUNE (X86_TUNE_AVOID_MEM_OPND_FOR_CMOVE, \"avoid_mem_opnd_for_cmove\", m_ATOM)\n+/* X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS: Try to split memory operand for\n+   fp converts to destination register.  */\n+DEF_TUNE (X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS, \"split_mem_opnd_for_fp_converts\",\n+          m_SLM)"}, {"sha": "66d7cc5d2eca8d2780c9369bd30df453cb799726", "filename": "gcc/doc/invoke.texi", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3ad20bd44836e57453b743466f1ca0d591bd10ac/gcc%2Fdoc%2Finvoke.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3ad20bd44836e57453b743466f1ca0d591bd10ac/gcc%2Fdoc%2Finvoke.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Finvoke.texi?ref=3ad20bd44836e57453b743466f1ca0d591bd10ac", "patch": "@@ -637,6 +637,7 @@ Objective-C and Objective-C++ Dialects}.\n \n @emph{i386 and x86-64 Options}\n @gccoptlist{-mtune=@var{cpu-type}  -march=@var{cpu-type} @gol\n+-mtune-ctrl=@var{feature-list} -mdump-tune-features -mno-default @gol\n -mfpmath=@var{unit} @gol\n -masm=@var{dialect}  -mno-fancy-math-387 @gol\n -mno-fp-ret-in-387  -msoft-float @gol\n@@ -14429,6 +14430,27 @@ supported architecture, using the appropriate flags.  In particular,\n the file containing the CPU detection code should be compiled without\n these options.\n \n+@item -mdump-tune-features\n+@opindex mdump-tune-features\n+This option instructs GCC to dump the names of the x86 performance \n+tuning features and default settings. The names can be used in \n+@option{-mtune-ctrl=@var{feature-list}}.\n+\n+@item -mtune-ctrl=@var{feature-list}\n+@opindex mtune-ctrl=@var{feature-list}\n+This option is used to do fine grain control of x86 code generation features.\n+@var{feature-list} is a comma separated list of @var{feature} names. See also\n+@option{-mdump-tune-features}. When specified, the @var{feature} will be turned\n+on if it is not preceded with @code{^}, otherwise, it will be turned off. \n+@option{-mtune-ctrl=@var{feature-list}} is intended to be used by GCC\n+developers. Using it may lead to code paths not covered by testing and can\n+potentially result in compiler ICEs or runtime errors.\n+\n+@item -mno-default\n+@opindex mno-default\n+This option instructs GCC to turn off all tunable features. See also \n+@option{-mtune-ctrl=@var{feature-list}} and @option{-mdump-tune-features}.\n+\n @item -mcld\n @opindex mcld\n This option instructs GCC to emit a @code{cld} instruction in the prologue"}]}