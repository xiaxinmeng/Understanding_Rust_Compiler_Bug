{"sha": "568421baa5a4cdb7bb7c5ac323c939492ee3f052", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NTY4NDIxYmFhNWE0Y2RiN2JiN2M1YWMzMjNjOTM5NDkyZWUzZjA1Mg==", "commit": {"author": {"name": "Sameera Deshpande", "email": "sameera.deshpande@linaro.org", "date": "2018-05-31T08:46:20Z"}, "committer": {"name": "Sameera Deshpande", "email": "sameerad@gcc.gnu.org", "date": "2018-05-31T08:46:20Z"}, "message": "Patch implementing vld1_*_x3, vst1_*_x2 and vst1_*_x3 intrinsics for AARCH64 for all types.\n\nFrom-SVN: r260989", "tree": {"sha": "133589691d7224603847a8c115ca2ccfcfe05d0d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/133589691d7224603847a8c115ca2ccfcfe05d0d"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/568421baa5a4cdb7bb7c5ac323c939492ee3f052", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/568421baa5a4cdb7bb7c5ac323c939492ee3f052", "html_url": "https://github.com/Rust-GCC/gccrs/commit/568421baa5a4cdb7bb7c5ac323c939492ee3f052", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/568421baa5a4cdb7bb7c5ac323c939492ee3f052/comments", "author": null, "committer": null, "parents": [{"sha": "5328e74a17d7b8a13f02c193fc4444b1ae579875", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5328e74a17d7b8a13f02c193fc4444b1ae579875", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5328e74a17d7b8a13f02c193fc4444b1ae579875"}], "stats": {"total": 1490, "additions": 1490, "deletions": 0}, "files": [{"sha": "e696c32d3765da39d864eaa683afe050da1a6c53", "filename": "gcc/ChangeLog", "status": "modified", "additions": 97, "deletions": 0, "changes": 97, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=568421baa5a4cdb7bb7c5ac323c939492ee3f052", "patch": "@@ -1,3 +1,100 @@\n+2018-05-31 Sameera Deshpande <sameera.deshpande@linaro.org>\n+\n+\t* config/aarch64/aarch64-simd-builtins.def (ld1x3): New.\n+\t(st1x2): Likewise.\n+\t(st1x3): Likewise.\n+\t* config/aarch64/aarch64-simd.md\n+\t(aarch64_ld1x3<VALLDIF:mode>): New pattern.\n+\t(aarch64_ld1_x3_<mode>): Likewise\n+\t(aarch64_st1x2<VALLDIF:mode>): Likewise\n+\t(aarch64_st1_x2_<mode>): Likewise\n+\t(aarch64_st1x3<VALLDIF:mode>): Likewise\n+\t(aarch64_st1_x3_<mode>): Likewise\n+\t* config/aarch64/arm_neon.h (vld1_u8_x3): New function.\n+\t(vld1_s8_x3): Likewise.\n+\t(vld1_u16_x3): Likewise.\n+\t(vld1_s16_x3): Likewise.\n+\t(vld1_u32_x3): Likewise.\n+\t(vld1_s32_x3): Likewise.\n+\t(vld1_u64_x3): Likewise.\n+\t(vld1_s64_x3): Likewise.\n+\t(vld1_f16_x3): Likewise.\n+\t(vld1_f32_x3): Likewise.\n+\t(vld1_f64_x3): Likewise.\n+\t(vld1_p8_x3): Likewise.\n+\t(vld1_p16_x3): Likewise.\n+\t(vld1_p64_x3): Likewise.\n+\t(vld1q_u8_x3): Likewise.\n+\t(vld1q_s8_x3): Likewise.\n+\t(vld1q_u16_x3): Likewise.\n+\t(vld1q_s16_x3): Likewise.\n+\t(vld1q_u32_x3): Likewise.\n+\t(vld1q_s32_x3): Likewise.\n+\t(vld1q_u64_x3): Likewise.\n+\t(vld1q_s64_x3): Likewise.\n+\t(vld1q_f16_x3): Likewise.\n+\t(vld1q_f32_x3): Likewise.\n+\t(vld1q_f64_x3): Likewise.\n+\t(vld1q_p8_x3): Likewise.\n+\t(vld1q_p16_x3): Likewise.\n+\t(vld1q_p64_x3): Likewise.\n+\t(vst1_s64_x2): Likewise.\n+\t(vst1_u64_x2): Likewise.\n+\t(vst1_f64_x2): Likewise.\n+\t(vst1_s8_x2): Likewise.\n+\t(vst1_p8_x2): Likewise.\n+\t(vst1_s16_x2): Likewise.\n+\t(vst1_p16_x2): Likewise.\n+\t(vst1_s32_x2): Likewise.\n+\t(vst1_u8_x2): Likewise.\n+\t(vst1_u16_x2): Likewise.\n+\t(vst1_u32_x2): Likewise.\n+\t(vst1_f16_x2): Likewise.\n+\t(vst1_f32_x2): Likewise.\n+\t(vst1_p64_x2): Likewise.\n+\t(vst1q_s8_x2): Likewise.\n+\t(vst1q_p8_x2): Likewise.\n+\t(vst1q_s16_x2): Likewise.\n+\t(vst1q_p16_x2): Likewise.\n+\t(vst1q_s32_x2): Likewise.\n+\t(vst1q_s64_x2): Likewise.\n+\t(vst1q_u8_x2): Likewise.\n+\t(vst1q_u16_x2): Likewise.\n+\t(vst1q_u32_x2): Likewise.\n+\t(vst1q_u64_x2): Likewise.\n+\t(vst1q_f16_x2): Likewise.\n+\t(vst1q_f32_x2): Likewise.\n+\t(vst1q_f64_x2): Likewise.\n+\t(vst1q_p64_x2): Likewise.\n+\t(vst1_s64_x3): Likewise.\n+\t(vst1_u64_x3): Likewise.\n+\t(vst1_f64_x3): Likewise.\n+\t(vst1_s8_x3): Likewise.\n+\t(vst1_p8_x3): Likewise.\n+\t(vst1_s16_x3): Likewise.\n+\t(vst1_p16_x3): Likewise.\n+\t(vst1_s32_x3): Likewise.\n+\t(vst1_u8_x3): Likewise.\n+\t(vst1_u16_x3): Likewise.\n+\t(vst1_u32_x3): Likewise.\n+\t(vst1_f16_x3): Likewise.\n+\t(vst1_f32_x3): Likewise.\n+\t(vst1_p64_x3): Likewise.\n+\t(vst1q_s8_x3): Likewise.\n+\t(vst1q_p8_x3): Likewise.\n+\t(vst1q_s16_x3): Likewise.\n+\t(vst1q_p16_x3): Likewise.\n+\t(vst1q_s32_x3): Likewise.\n+\t(vst1q_s64_x3): Likewise.\n+\t(vst1q_u8_x3): Likewise.\n+\t(vst1q_u16_x3): Likewise.\n+\t(vst1q_u32_x3): Likewise.\n+\t(vst1q_u64_x3): Likewise.\n+\t(vst1q_f16_x3): Likewise.\n+\t(vst1q_f32_x3): Likewise.\n+\t(vst1q_f64_x3): Likewise.\n+\t(vst1q_p64_x3): Likewise.\n+\n 2018-05-30  Jozef Lawrynowicz  <jozef.l@mittosystems.com>\n \n \t* config/msp430/msp430.c (msp430_output_labelref): Prepend"}, {"sha": "c4a5d0dbcb202d579ac67b0a85159f452aee814e", "filename": "gcc/config/aarch64/aarch64-simd-builtins.def", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def?ref=568421baa5a4cdb7bb7c5ac323c939492ee3f052", "patch": "@@ -445,6 +445,15 @@\n   BUILTIN_VALL_F16 (STORE1, st1, 0)\n   VAR1(STORE1P, st1, 0, v2di)\n \n+  /* Implemented by aarch64_ld1x3<VALLDIF:mode>.  */\n+  BUILTIN_VALLDIF (LOADSTRUCT, ld1x3, 0)\n+\n+  /* Implemented by aarch64_st1x2<VALLDIF:mode>.  */\n+  BUILTIN_VALLDIF (STORESTRUCT, st1x2, 0)\n+\n+  /* Implemented by aarch64_st1x3<VALLDIF:mode>.  */\n+  BUILTIN_VALLDIF (STORESTRUCT, st1x3, 0)\n+\n   /* Implemented by fma<mode>4.  */\n   BUILTIN_VHSDF (TERNOP, fma, 4)\n   VAR1 (TERNOP, fma, 4, hf)"}, {"sha": "4e5c42b0f15b863f3088dba4aac450f31ca309bb", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 64, "deletions": 0, "changes": 64, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=568421baa5a4cdb7bb7c5ac323c939492ee3f052", "patch": "@@ -5056,6 +5056,70 @@\n     }\n })\n \n+\n+(define_expand \"aarch64_ld1x3<VALLDIF:mode>\"\n+  [(match_operand:CI 0 \"register_operand\" \"=w\")\n+   (match_operand:DI 1 \"register_operand\" \"r\")\n+   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+  \"TARGET_SIMD\"\n+{\n+  rtx mem = gen_rtx_MEM (CImode, operands[1]);\n+  emit_insn (gen_aarch64_ld1_x3_<VALLDIF:mode> (operands[0], mem));\n+  DONE;\n+})\n+\n+(define_insn \"aarch64_ld1_x3_<mode>\"\n+  [(set (match_operand:CI 0 \"register_operand\" \"=w\")\n+        (unspec:CI\n+\t  [(match_operand:CI 1 \"aarch64_simd_struct_operand\" \"Utv\")\n+\t   (unspec:VALLDIF [(const_int 3)] UNSPEC_VSTRUCTDUMMY)] UNSPEC_LD1))]\n+  \"TARGET_SIMD\"\n+  \"ld1\\\\t{%S0.<Vtype> - %U0.<Vtype>}, %1\"\n+  [(set_attr \"type\" \"neon_load1_3reg<q>\")]\n+)\n+\n+(define_expand \"aarch64_st1x2<VALLDIF:mode>\"\n+  [(match_operand:DI 0 \"register_operand\" \"\")\n+   (match_operand:OI 1 \"register_operand\" \"\")\n+   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+  \"TARGET_SIMD\"\n+{\n+  rtx mem = gen_rtx_MEM (OImode, operands[0]);\n+  emit_insn (gen_aarch64_st1_x2_<VALLDIF:mode> (mem, operands[1]));\n+  DONE;\n+})\n+\n+(define_insn \"aarch64_st1_x2_<mode>\"\n+   [(set (match_operand:OI 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n+\t (unspec:OI\n+\t  [(match_operand:OI 1 \"register_operand\" \"w\")\n+          (unspec:VALLDIF [(const_int 2)] UNSPEC_VSTRUCTDUMMY)] UNSPEC_ST1))]\n+  \"TARGET_SIMD\"\n+  \"st1\\\\t{%S1.<Vtype> - %T1.<Vtype>}, %0\"\n+  [(set_attr \"type\" \"neon_store1_2reg<q>\")]\n+)\n+\n+(define_expand \"aarch64_st1x3<VALLDIF:mode>\"\n+  [(match_operand:DI 0 \"register_operand\" \"\")\n+   (match_operand:CI 1 \"register_operand\" \"\")\n+   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+  \"TARGET_SIMD\"\n+{\n+  rtx mem = gen_rtx_MEM (CImode, operands[0]);\n+  emit_insn (gen_aarch64_st1_x3_<VALLDIF:mode> (mem, operands[1]));\n+  DONE;\n+})\n+\n+(define_insn \"aarch64_st1_x3_<mode>\"\n+   [(set (match_operand:CI 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n+\t(unspec:CI\n+         [(match_operand:CI 1 \"register_operand\" \"w\")\n+\t  (unspec:VALLDIF [(const_int 3)] UNSPEC_VSTRUCTDUMMY)] UNSPEC_ST1))]\n+  \"TARGET_SIMD\"\n+  \"st1\\\\t{%S1.<Vtype> - %U1.<Vtype>}, %0\"\n+  [(set_attr \"type\" \"neon_store1_3reg<q>\")]\n+)\n+\n (define_insn \"*aarch64_mov<mode>\"\n   [(set (match_operand:VSTRUCT 0 \"aarch64_simd_nonimmediate_operand\" \"=w,Utv,w\")\n \t(match_operand:VSTRUCT 1 \"aarch64_simd_general_operand\" \" w,w,Utv\"))]"}, {"sha": "2d18400040f031dfcdaf60269ad484647804e1be", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 1068, "deletions": 0, "changes": 1068, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=568421baa5a4cdb7bb7c5ac323c939492ee3f052", "patch": "@@ -17145,6 +17145,374 @@ vld1_u64 (const uint64_t *a)\n   return (uint64x1_t) {*a};\n }\n \n+/* vld1x3  */\n+\n+__extension__ extern __inline uint8x8x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_u8_x3 (const uint8_t *__a)\n+{\n+  uint8x8x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = (__builtin_aarch64_simd_ci)__builtin_aarch64_ld1x3v8qi ((const __builtin_aarch64_simd_qi *) __a);\n+  __i.val[0] = (uint8x8_t) __builtin_aarch64_get_dregciv8qi  (__o, 0);\n+  __i.val[1] = (uint8x8_t) __builtin_aarch64_get_dregciv8qi  (__o, 1);\n+  __i.val[2] = (uint8x8_t) __builtin_aarch64_get_dregciv8qi  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline int8x8x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_s8_x3 (const uint8_t *__a)\n+{\n+  int8x8x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v8qi ((const __builtin_aarch64_simd_qi *) __a);\n+  __i.val[0] = (int8x8_t) __builtin_aarch64_get_dregciv8qi  (__o, 0);\n+  __i.val[1] = (int8x8_t) __builtin_aarch64_get_dregciv8qi  (__o, 1);\n+  __i.val[2] = (int8x8_t) __builtin_aarch64_get_dregciv8qi  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline uint16x4x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_u16_x3 (const uint16_t *__a)\n+{\n+  uint16x4x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v4hi ((const __builtin_aarch64_simd_hi *) __a);\n+  __i.val[0] = (uint16x4_t) __builtin_aarch64_get_dregciv4hi  (__o, 0);\n+  __i.val[1] = (uint16x4_t) __builtin_aarch64_get_dregciv4hi  (__o, 1);\n+  __i.val[2] = (uint16x4_t) __builtin_aarch64_get_dregciv4hi  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline int16x4x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_s16_x3 (const int16_t *__a)\n+{\n+  int16x4x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v4hi ((const __builtin_aarch64_simd_hi *) __a);\n+  __i.val[0] = (int16x4_t) __builtin_aarch64_get_dregciv4hi  (__o, 0);\n+  __i.val[1] = (int16x4_t) __builtin_aarch64_get_dregciv4hi  (__o, 1);\n+  __i.val[2] = (int16x4_t) __builtin_aarch64_get_dregciv4hi  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline uint32x2x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_u32_x3 (const uint32_t *__a)\n+{\n+  uint32x2x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v2si ((const __builtin_aarch64_simd_si *) __a);\n+  __i.val[0] = (uint32x2_t) __builtin_aarch64_get_dregciv2si  (__o, 0);\n+  __i.val[1] = (uint32x2_t) __builtin_aarch64_get_dregciv2si  (__o, 1);\n+  __i.val[2] = (uint32x2_t) __builtin_aarch64_get_dregciv2si  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline int32x2x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_s32_x3 (const uint32_t *__a)\n+{\n+  int32x2x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v2si ((const __builtin_aarch64_simd_si *) __a);\n+  __i.val[0] = (int32x2_t) __builtin_aarch64_get_dregciv2si  (__o, 0);\n+  __i.val[1] = (int32x2_t) __builtin_aarch64_get_dregciv2si  (__o, 1);\n+  __i.val[2] = (int32x2_t) __builtin_aarch64_get_dregciv2si  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline uint64x1x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_u64_x3 (const uint64_t *__a)\n+{\n+  uint64x1x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3di ((const __builtin_aarch64_simd_di *) __a);\n+  __i.val[0] = (uint64x1_t) __builtin_aarch64_get_dregcidi (__o, 0);\n+  __i.val[1] = (uint64x1_t) __builtin_aarch64_get_dregcidi (__o, 1);\n+  __i.val[2] = (uint64x1_t) __builtin_aarch64_get_dregcidi (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline int64x1x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_s64_x3 (const int64_t *__a)\n+{\n+  int64x1x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3di ((const __builtin_aarch64_simd_di *) __a);\n+  __i.val[0] = (int64x1_t) __builtin_aarch64_get_dregcidi (__o, 0);\n+  __i.val[1] = (int64x1_t) __builtin_aarch64_get_dregcidi (__o, 1);\n+  __i.val[2] = (int64x1_t) __builtin_aarch64_get_dregcidi (__o, 2);\n+\n+  return __i;\n+}\n+\n+__extension__ extern __inline float16x4x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_f16_x3 (const float16_t *__a)\n+{\n+  float16x4x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v4hf ((const __builtin_aarch64_simd_hf *) __a);\n+  __i.val[0] = (float16x4_t) __builtin_aarch64_get_dregciv4hf  (__o, 0);\n+  __i.val[1] = (float16x4_t) __builtin_aarch64_get_dregciv4hf  (__o, 1);\n+  __i.val[2] = (float16x4_t) __builtin_aarch64_get_dregciv4hf  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline float32x2x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_f32_x3 (const float32_t *__a)\n+{\n+  float32x2x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v2sf ((const __builtin_aarch64_simd_sf *) __a);\n+  __i.val[0] = (float32x2_t) __builtin_aarch64_get_dregciv2sf  (__o, 0);\n+  __i.val[1] = (float32x2_t) __builtin_aarch64_get_dregciv2sf  (__o, 1);\n+  __i.val[2] = (float32x2_t) __builtin_aarch64_get_dregciv2sf  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline float64x1x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_f64_x3 (const float64_t *__a)\n+{\n+  float64x1x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3df ((const __builtin_aarch64_simd_df *) __a);\n+  __i.val[0] = (float64x1_t) __builtin_aarch64_get_dregcidi (__o, 0);\n+  __i.val[1] = (float64x1_t) __builtin_aarch64_get_dregcidi (__o, 1);\n+  __i.val[2] = (float64x1_t) __builtin_aarch64_get_dregcidi (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline poly8x8x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_p8_x3 (const poly8_t *__a)\n+{\n+  poly8x8x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v8qi ((const __builtin_aarch64_simd_qi *) __a);\n+  __i.val[0] = (poly8x8_t) __builtin_aarch64_get_dregciv8qi  (__o, 0);\n+  __i.val[1] = (poly8x8_t) __builtin_aarch64_get_dregciv8qi  (__o, 1);\n+  __i.val[2] = (poly8x8_t) __builtin_aarch64_get_dregciv8qi  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline poly16x4x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_p16_x3 (const poly16_t *__a)\n+{\n+  poly16x4x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v4hi ((const __builtin_aarch64_simd_hi *) __a);\n+  __i.val[0] = (poly16x4_t) __builtin_aarch64_get_dregciv4hi  (__o, 0);\n+  __i.val[1] = (poly16x4_t) __builtin_aarch64_get_dregciv4hi  (__o, 1);\n+  __i.val[2] = (poly16x4_t) __builtin_aarch64_get_dregciv4hi  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline poly64x1x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1_p64_x3 (const poly64_t *__a)\n+{\n+  poly64x1x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3di ((const __builtin_aarch64_simd_di *) __a);\n+  __i.val[0] = (poly64x1_t) __builtin_aarch64_get_dregcidi (__o, 0);\n+  __i.val[1] = (poly64x1_t) __builtin_aarch64_get_dregcidi (__o, 1);\n+  __i.val[2] = (poly64x1_t) __builtin_aarch64_get_dregcidi (__o, 2);\n+\n+return __i;\n+}\n+\n+__extension__ extern __inline uint8x16x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_u8_x3 (const uint8_t *__a)\n+{\n+  uint8x16x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v16qi ((const __builtin_aarch64_simd_qi *) __a);\n+  __i.val[0] = (uint8x16_t) __builtin_aarch64_get_qregciv16qi  (__o, 0);\n+  __i.val[1] = (uint8x16_t) __builtin_aarch64_get_qregciv16qi  (__o, 1);\n+  __i.val[2] = (uint8x16_t) __builtin_aarch64_get_qregciv16qi  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline int8x16x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_s8_x3 (const int8_t *__a)\n+{\n+  int8x16x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v16qi ((const __builtin_aarch64_simd_qi *) __a);\n+  __i.val[0] = (int8x16_t) __builtin_aarch64_get_qregciv16qi  (__o, 0);\n+  __i.val[1] = (int8x16_t) __builtin_aarch64_get_qregciv16qi  (__o, 1);\n+  __i.val[2] = (int8x16_t) __builtin_aarch64_get_qregciv16qi  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline uint16x8x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_u16_x3 (const uint16_t *__a)\n+{\n+  uint16x8x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v8hi ((const __builtin_aarch64_simd_hi *) __a);\n+  __i.val[0] = (uint16x8_t) __builtin_aarch64_get_qregciv8hi  (__o, 0);\n+  __i.val[1] = (uint16x8_t) __builtin_aarch64_get_qregciv8hi  (__o, 1);\n+  __i.val[2] = (uint16x8_t) __builtin_aarch64_get_qregciv8hi  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline int16x8x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_s16_x3 (const int16_t *__a)\n+{\n+  int16x8x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v8hi ((const __builtin_aarch64_simd_hi *) __a);\n+  __i.val[0] = (int16x8_t) __builtin_aarch64_get_qregciv8hi  (__o, 0);\n+  __i.val[1] = (int16x8_t) __builtin_aarch64_get_qregciv8hi  (__o, 1);\n+  __i.val[2] = (int16x8_t) __builtin_aarch64_get_qregciv8hi  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline uint32x4x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_u32_x3 (const uint32_t *__a)\n+{\n+  uint32x4x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v4si ((const __builtin_aarch64_simd_si *) __a);\n+  __i.val[0] = (uint32x4_t) __builtin_aarch64_get_qregciv4si  (__o, 0);\n+  __i.val[1] = (uint32x4_t) __builtin_aarch64_get_qregciv4si  (__o, 1);\n+  __i.val[2] = (uint32x4_t) __builtin_aarch64_get_qregciv4si  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline int32x4x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_s32_x3 (const int32_t *__a)\n+{\n+  int32x4x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v4si ((const __builtin_aarch64_simd_si *) __a);\n+  __i.val[0] = (int32x4_t) __builtin_aarch64_get_qregciv4si  (__o, 0);\n+  __i.val[1] = (int32x4_t) __builtin_aarch64_get_qregciv4si  (__o, 1);\n+  __i.val[2] = (int32x4_t) __builtin_aarch64_get_qregciv4si  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline uint64x2x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_u64_x3 (const uint64_t *__a)\n+{\n+  uint64x2x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v2di ((const __builtin_aarch64_simd_di *) __a);\n+  __i.val[0] = (uint64x2_t) __builtin_aarch64_get_qregciv2di  (__o, 0);\n+  __i.val[1] = (uint64x2_t) __builtin_aarch64_get_qregciv2di  (__o, 1);\n+  __i.val[2] = (uint64x2_t) __builtin_aarch64_get_qregciv2di  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline int64x2x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_s64_x3 (const int64_t *__a)\n+{\n+  int64x2x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v2di ((const __builtin_aarch64_simd_di *) __a);\n+  __i.val[0] = (int64x2_t) __builtin_aarch64_get_qregciv2di  (__o, 0);\n+  __i.val[1] = (int64x2_t) __builtin_aarch64_get_qregciv2di  (__o, 1);\n+  __i.val[2] = (int64x2_t) __builtin_aarch64_get_qregciv2di  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline float16x8x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_f16_x3 (const float16_t *__a)\n+{\n+  float16x8x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v8hf ((const __builtin_aarch64_simd_hf *) __a);\n+  __i.val[0] = (float16x8_t) __builtin_aarch64_get_qregciv8hf  (__o, 0);\n+  __i.val[1] = (float16x8_t) __builtin_aarch64_get_qregciv8hf  (__o, 1);\n+  __i.val[2] = (float16x8_t) __builtin_aarch64_get_qregciv8hf  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline float32x4x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_f32_x3 (const float32_t *__a)\n+{\n+  float32x4x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v4sf ((const __builtin_aarch64_simd_sf *) __a);\n+  __i.val[0] = (float32x4_t) __builtin_aarch64_get_qregciv4sf  (__o, 0);\n+  __i.val[1] = (float32x4_t) __builtin_aarch64_get_qregciv4sf  (__o, 1);\n+  __i.val[2] = (float32x4_t) __builtin_aarch64_get_qregciv4sf  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline float64x2x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_f64_x3 (const float64_t *__a)\n+{\n+  float64x2x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v2df ((const __builtin_aarch64_simd_df *) __a);\n+  __i.val[0] = (float64x2_t) __builtin_aarch64_get_qregciv2df  (__o, 0);\n+  __i.val[1] = (float64x2_t) __builtin_aarch64_get_qregciv2df  (__o, 1);\n+  __i.val[2] = (float64x2_t) __builtin_aarch64_get_qregciv2df  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline poly8x16x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_p8_x3 (const poly8_t *__a)\n+{\n+  poly8x16x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v16qi ((const __builtin_aarch64_simd_qi *) __a);\n+  __i.val[0] = (poly8x16_t) __builtin_aarch64_get_qregciv16qi  (__o, 0);\n+  __i.val[1] = (poly8x16_t) __builtin_aarch64_get_qregciv16qi  (__o, 1);\n+  __i.val[2] = (poly8x16_t) __builtin_aarch64_get_qregciv16qi  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline poly16x8x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_p16_x3 (const poly16_t *__a)\n+{\n+  poly16x8x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v8hi ((const __builtin_aarch64_simd_hi *) __a);\n+  __i.val[0] = (poly16x8_t) __builtin_aarch64_get_qregciv8hi  (__o, 0);\n+  __i.val[1] = (poly16x8_t) __builtin_aarch64_get_qregciv8hi  (__o, 1);\n+  __i.val[2] = (poly16x8_t) __builtin_aarch64_get_qregciv8hi  (__o, 2);\n+  return __i;\n+}\n+\n+__extension__ extern __inline poly64x2x3_t\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vld1q_p64_x3 (const poly64_t *__a)\n+{\n+  poly64x2x3_t __i;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld1x3v2di ((const __builtin_aarch64_simd_di *) __a);\n+  __i.val[0] = (poly64x2_t) __builtin_aarch64_get_qregciv2di  (__o, 0);\n+  __i.val[1] = (poly64x2_t) __builtin_aarch64_get_qregciv2di  (__o, 1);\n+  __i.val[2] = (poly64x2_t) __builtin_aarch64_get_qregciv2di  (__o, 2);\n+  return __i;\n+}\n+\n /* vld1q */\n \n __extension__ extern __inline float16x8_t\n@@ -27497,6 +27865,706 @@ vst1q_lane_u64 (uint64_t *__a, uint64x2_t __b, const int __lane)\n   *__a = __aarch64_vget_lane_any (__b, __lane);\n }\n \n+/* vst1x2 */\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_s64_x2 (int64_t * __a, int64x1x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  int64x2x2_t temp;\n+  temp.val[0] = vcombine_s64 (val.val[0], vcreate_s64 (__AARCH64_INT64_C (0)));\n+  temp.val[1] = vcombine_s64 (val.val[1], vcreate_s64 (__AARCH64_INT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv2di (__o, (int64x2_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv2di (__o, (int64x2_t) temp.val[1], 1);\n+  __builtin_aarch64_st1x2di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_u64_x2 (uint64_t * __a, uint64x1x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  uint64x2x2_t temp;\n+  temp.val[0] = vcombine_u64 (val.val[0], vcreate_u64 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_u64 (val.val[1], vcreate_u64 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv2di (__o, (int64x2_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv2di (__o, (int64x2_t) temp.val[1], 1);\n+  __builtin_aarch64_st1x2di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_f64_x2 (float64_t * __a, float64x1x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  float64x2x2_t temp;\n+  temp.val[0] = vcombine_f64 (val.val[0], vcreate_f64 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_f64 (val.val[1], vcreate_f64 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv2df (__o, (float64x2_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv2df (__o, (float64x2_t) temp.val[1], 1);\n+  __builtin_aarch64_st1x2df ((__builtin_aarch64_simd_df *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_s8_x2 (int8_t * __a, int8x8x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  int8x16x2_t temp;\n+  temp.val[0] = vcombine_s8 (val.val[0], vcreate_s8 (__AARCH64_INT64_C (0)));\n+  temp.val[1] = vcombine_s8 (val.val[1], vcreate_s8 (__AARCH64_INT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) temp.val[1], 1);\n+  __builtin_aarch64_st1x2v8qi ((__builtin_aarch64_simd_qi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_p8_x2 (poly8_t * __a, poly8x8x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  poly8x16x2_t temp;\n+  temp.val[0] = vcombine_p8 (val.val[0], vcreate_p8 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_p8 (val.val[1], vcreate_p8 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) temp.val[1], 1);\n+  __builtin_aarch64_st1x2v8qi ((__builtin_aarch64_simd_qi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_s16_x2 (int16_t * __a, int16x4x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  int16x8x2_t temp;\n+  temp.val[0] = vcombine_s16 (val.val[0], vcreate_s16 (__AARCH64_INT64_C (0)));\n+  temp.val[1] = vcombine_s16 (val.val[1], vcreate_s16 (__AARCH64_INT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) temp.val[1], 1);\n+  __builtin_aarch64_st1x2v4hi ((__builtin_aarch64_simd_hi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_p16_x2 (poly16_t * __a, poly16x4x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  poly16x8x2_t temp;\n+  temp.val[0] = vcombine_p16 (val.val[0], vcreate_p16 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_p16 (val.val[1], vcreate_p16 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) temp.val[1], 1);\n+  __builtin_aarch64_st1x2v4hi ((__builtin_aarch64_simd_hi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_s32_x2 (int32_t * __a, int32x2x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  int32x4x2_t temp;\n+  temp.val[0] = vcombine_s32 (val.val[0], vcreate_s32 (__AARCH64_INT64_C (0)));\n+  temp.val[1] = vcombine_s32 (val.val[1], vcreate_s32 (__AARCH64_INT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv4si (__o, (int32x4_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv4si (__o, (int32x4_t) temp.val[1], 1);\n+  __builtin_aarch64_st1x2v2si ((__builtin_aarch64_simd_si *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_u8_x2 (uint8_t * __a, uint8x8x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  uint8x16x2_t temp;\n+  temp.val[0] = vcombine_u8 (val.val[0], vcreate_u8 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_u8 (val.val[1], vcreate_u8 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) temp.val[1], 1);\n+  __builtin_aarch64_st1x2v8qi ((__builtin_aarch64_simd_qi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_u16_x2 (uint16_t * __a, uint16x4x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  uint16x8x2_t temp;\n+  temp.val[0] = vcombine_u16 (val.val[0], vcreate_u16 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_u16 (val.val[1], vcreate_u16 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) temp.val[1], 1);\n+  __builtin_aarch64_st1x2v4hi ((__builtin_aarch64_simd_hi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_u32_x2 (uint32_t * __a, uint32x2x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  uint32x4x2_t temp;\n+  temp.val[0] = vcombine_u32 (val.val[0], vcreate_u32 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_u32 (val.val[1], vcreate_u32 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv4si (__o, (int32x4_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv4si (__o, (int32x4_t) temp.val[1], 1);\n+  __builtin_aarch64_st1x2v2si ((__builtin_aarch64_simd_si *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_f16_x2 (float16_t * __a, float16x4x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  float16x8x2_t temp;\n+  temp.val[0] = vcombine_f16 (val.val[0], vcreate_f16 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_f16 (val.val[1], vcreate_f16 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv8hf (__o, temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv8hf (__o, temp.val[1], 1);\n+  __builtin_aarch64_st1x2v4hf (__a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_f32_x2 (float32_t * __a, float32x2x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  float32x4x2_t temp;\n+  temp.val[0] = vcombine_f32 (val.val[0], vcreate_f32 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_f32 (val.val[1], vcreate_f32 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv4sf (__o, (float32x4_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv4sf (__o, (float32x4_t) temp.val[1], 1);\n+  __builtin_aarch64_st1x2v2sf ((__builtin_aarch64_simd_sf *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_p64_x2 (poly64_t * __a, poly64x1x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  poly64x2x2_t temp;\n+  temp.val[0] = vcombine_p64 (val.val[0], vcreate_p64 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_p64 (val.val[1], vcreate_p64 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregoiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) temp.val[1], 1);\n+  __builtin_aarch64_st1x2di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_s8_x2 (int8_t * __a, int8x16x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) val.val[1], 1);\n+  __builtin_aarch64_st1x2v16qi ((__builtin_aarch64_simd_qi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_p8_x2 (poly8_t * __a, poly8x16x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) val.val[1], 1);\n+  __builtin_aarch64_st1x2v16qi ((__builtin_aarch64_simd_qi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_s16_x2 (int16_t * __a, int16x8x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) val.val[1], 1);\n+  __builtin_aarch64_st1x2v8hi ((__builtin_aarch64_simd_hi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_p16_x2 (poly16_t * __a, poly16x8x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) val.val[1], 1);\n+  __builtin_aarch64_st1x2v8hi ((__builtin_aarch64_simd_hi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_s32_x2 (int32_t * __a, int32x4x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv4si (__o, (int32x4_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv4si (__o, (int32x4_t) val.val[1], 1);\n+  __builtin_aarch64_st1x2v4si ((__builtin_aarch64_simd_si *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_s64_x2 (int64_t * __a, int64x2x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv2di (__o, (int64x2_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv2di (__o, (int64x2_t) val.val[1], 1);\n+  __builtin_aarch64_st1x2v2di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_u8_x2 (uint8_t * __a, uint8x16x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv16qi (__o, (int8x16_t) val.val[1], 1);\n+  __builtin_aarch64_st1x2v16qi ((__builtin_aarch64_simd_qi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_u16_x2 (uint16_t * __a, uint16x8x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv8hi (__o, (int16x8_t) val.val[1], 1);\n+  __builtin_aarch64_st1x2v8hi ((__builtin_aarch64_simd_hi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_u32_x2 (uint32_t * __a, uint32x4x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv4si (__o, (int32x4_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv4si (__o, (int32x4_t) val.val[1], 1);\n+  __builtin_aarch64_st1x2v4si ((__builtin_aarch64_simd_si *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_u64_x2 (uint64_t * __a, uint64x2x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv2di (__o, (int64x2_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv2di (__o, (int64x2_t) val.val[1], 1);\n+  __builtin_aarch64_st1x2v2di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_f16_x2 (float16_t * __a, float16x8x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv8hf (__o, val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv8hf (__o, val.val[1], 1);\n+  __builtin_aarch64_st1x2v8hf (__a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_f32_x2 (float32_t * __a, float32x4x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv4sf (__o, (float32x4_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv4sf (__o, (float32x4_t) val.val[1], 1);\n+  __builtin_aarch64_st1x2v4sf ((__builtin_aarch64_simd_sf *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_f64_x2 (float64_t * __a, float64x2x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv2df (__o, (float64x2_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv2df (__o, (float64x2_t) val.val[1], 1);\n+  __builtin_aarch64_st1x2v2df ((__builtin_aarch64_simd_df *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_p64_x2 (poly64_t * __a, poly64x2x2_t val)\n+{\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_set_qregoiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregoiv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) val.val[1], 1);\n+  __builtin_aarch64_st1x2v2di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n+/* vst1x3 */\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_s64_x3 (int64_t * __a, int64x1x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  int64x2x3_t temp;\n+  temp.val[0] = vcombine_s64 (val.val[0], vcreate_s64 (__AARCH64_INT64_C (0)));\n+  temp.val[1] = vcombine_s64 (val.val[1], vcreate_s64 (__AARCH64_INT64_C (0)));\n+  temp.val[2] = vcombine_s64 (val.val[2], vcreate_s64 (__AARCH64_INT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) temp.val[2], 2);\n+  __builtin_aarch64_st1x3di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_u64_x3 (uint64_t * __a, uint64x1x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  uint64x2x3_t temp;\n+  temp.val[0] = vcombine_u64 (val.val[0], vcreate_u64 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_u64 (val.val[1], vcreate_u64 (__AARCH64_UINT64_C (0)));\n+  temp.val[2] = vcombine_u64 (val.val[2], vcreate_u64 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) temp.val[2], 2);\n+  __builtin_aarch64_st1x3di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_f64_x3 (float64_t * __a, float64x1x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  float64x2x3_t temp;\n+  temp.val[0] = vcombine_f64 (val.val[0], vcreate_f64 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_f64 (val.val[1], vcreate_f64 (__AARCH64_UINT64_C (0)));\n+  temp.val[2] = vcombine_f64 (val.val[2], vcreate_f64 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv2df (__o, (float64x2_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv2df (__o, (float64x2_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv2df (__o, (float64x2_t) temp.val[2], 2);\n+  __builtin_aarch64_st1x3df ((__builtin_aarch64_simd_df *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_s8_x3 (int8_t * __a, int8x8x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  int8x16x3_t temp;\n+  temp.val[0] = vcombine_s8 (val.val[0], vcreate_s8 (__AARCH64_INT64_C (0)));\n+  temp.val[1] = vcombine_s8 (val.val[1], vcreate_s8 (__AARCH64_INT64_C (0)));\n+  temp.val[2] = vcombine_s8 (val.val[2], vcreate_s8 (__AARCH64_INT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[2], 2);\n+  __builtin_aarch64_st1x3v8qi ((__builtin_aarch64_simd_qi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_p8_x3 (poly8_t * __a, poly8x8x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  poly8x16x3_t temp;\n+  temp.val[0] = vcombine_p8 (val.val[0], vcreate_p8 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_p8 (val.val[1], vcreate_p8 (__AARCH64_UINT64_C (0)));\n+  temp.val[2] = vcombine_p8 (val.val[2], vcreate_p8 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[2], 2);\n+  __builtin_aarch64_st1x3v8qi ((__builtin_aarch64_simd_qi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_s16_x3 (int16_t * __a, int16x4x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  int16x8x3_t temp;\n+  temp.val[0] = vcombine_s16 (val.val[0], vcreate_s16 (__AARCH64_INT64_C (0)));\n+  temp.val[1] = vcombine_s16 (val.val[1], vcreate_s16 (__AARCH64_INT64_C (0)));\n+  temp.val[2] = vcombine_s16 (val.val[2], vcreate_s16 (__AARCH64_INT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[2], 2);\n+  __builtin_aarch64_st1x3v4hi ((__builtin_aarch64_simd_hi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_p16_x3 (poly16_t * __a, poly16x4x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  poly16x8x3_t temp;\n+  temp.val[0] = vcombine_p16 (val.val[0], vcreate_p16 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_p16 (val.val[1], vcreate_p16 (__AARCH64_UINT64_C (0)));\n+  temp.val[2] = vcombine_p16 (val.val[2], vcreate_p16 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[2], 2);\n+  __builtin_aarch64_st1x3v4hi ((__builtin_aarch64_simd_hi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_s32_x3 (int32_t * __a, int32x2x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  int32x4x3_t temp;\n+  temp.val[0] = vcombine_s32 (val.val[0], vcreate_s32 (__AARCH64_INT64_C (0)));\n+  temp.val[1] = vcombine_s32 (val.val[1], vcreate_s32 (__AARCH64_INT64_C (0)));\n+  temp.val[2] = vcombine_s32 (val.val[2], vcreate_s32 (__AARCH64_INT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) temp.val[2], 2);\n+  __builtin_aarch64_st1x3v2si ((__builtin_aarch64_simd_si *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_u8_x3 (uint8_t * __a, uint8x8x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  uint8x16x3_t temp;\n+  temp.val[0] = vcombine_u8 (val.val[0], vcreate_u8 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_u8 (val.val[1], vcreate_u8 (__AARCH64_UINT64_C (0)));\n+  temp.val[2] = vcombine_u8 (val.val[2], vcreate_u8 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) temp.val[2], 2);\n+  __builtin_aarch64_st1x3v8qi ((__builtin_aarch64_simd_qi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_u16_x3 (uint16_t * __a, uint16x4x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  uint16x8x3_t temp;\n+  temp.val[0] = vcombine_u16 (val.val[0], vcreate_u16 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_u16 (val.val[1], vcreate_u16 (__AARCH64_UINT64_C (0)));\n+  temp.val[2] = vcombine_u16 (val.val[2], vcreate_u16 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) temp.val[2], 2);\n+  __builtin_aarch64_st1x3v4hi ((__builtin_aarch64_simd_hi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_u32_x3 (uint32_t * __a, uint32x2x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  uint32x4x3_t temp;\n+  temp.val[0] = vcombine_u32 (val.val[0], vcreate_u32 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_u32 (val.val[1], vcreate_u32 (__AARCH64_UINT64_C (0)));\n+  temp.val[2] = vcombine_u32 (val.val[2], vcreate_u32 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) temp.val[2], 2);\n+  __builtin_aarch64_st1x3v2si ((__builtin_aarch64_simd_si *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_f16_x3 (float16_t * __a, float16x4x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  float16x8x3_t temp;\n+  temp.val[0] = vcombine_f16 (val.val[0], vcreate_f16 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_f16 (val.val[1], vcreate_f16 (__AARCH64_UINT64_C (0)));\n+  temp.val[2] = vcombine_f16 (val.val[2], vcreate_f16 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv8hf (__o, (float16x8_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv8hf (__o, (float16x8_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv8hf (__o, (float16x8_t) temp.val[2], 2);\n+  __builtin_aarch64_st1x3v4hf ((__builtin_aarch64_simd_hf *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_f32_x3 (float32_t * __a, float32x2x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  float32x4x3_t temp;\n+  temp.val[0] = vcombine_f32 (val.val[0], vcreate_f32 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_f32 (val.val[1], vcreate_f32 (__AARCH64_UINT64_C (0)));\n+  temp.val[2] = vcombine_f32 (val.val[2], vcreate_f32 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv4sf (__o, (float32x4_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv4sf (__o, (float32x4_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv4sf (__o, (float32x4_t) temp.val[2], 2);\n+  __builtin_aarch64_st1x3v2sf ((__builtin_aarch64_simd_sf *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1_p64_x3 (poly64_t * __a, poly64x1x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  poly64x2x3_t temp;\n+  temp.val[0] = vcombine_p64 (val.val[0], vcreate_p64 (__AARCH64_UINT64_C (0)));\n+  temp.val[1] = vcombine_p64 (val.val[1], vcreate_p64 (__AARCH64_UINT64_C (0)));\n+  temp.val[2] = vcombine_p64 (val.val[2], vcreate_p64 (__AARCH64_UINT64_C (0)));\n+  __o = __builtin_aarch64_set_qregciv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) temp.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) temp.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) temp.val[2], 2);\n+  __builtin_aarch64_st1x3di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_s8_x3 (int8_t * __a, int8x16x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) val.val[2], 2);\n+  __builtin_aarch64_st1x3v16qi ((__builtin_aarch64_simd_qi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_p8_x3 (poly8_t * __a, poly8x16x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) val.val[2], 2);\n+  __builtin_aarch64_st1x3v16qi ((__builtin_aarch64_simd_qi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_s16_x3 (int16_t * __a, int16x8x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) val.val[2], 2);\n+  __builtin_aarch64_st1x3v8hi ((__builtin_aarch64_simd_hi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_p16_x3 (poly16_t * __a, poly16x8x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) val.val[2], 2);\n+  __builtin_aarch64_st1x3v8hi ((__builtin_aarch64_simd_hi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_s32_x3 (int32_t * __a, int32x4x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) val.val[2], 2);\n+  __builtin_aarch64_st1x3v4si ((__builtin_aarch64_simd_si *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_s64_x3 (int64_t * __a, int64x2x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) val.val[2], 2);\n+  __builtin_aarch64_st1x3v2di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_u8_x3 (uint8_t * __a, uint8x16x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv16qi (__o, (int8x16_t) val.val[2], 2);\n+  __builtin_aarch64_st1x3v16qi ((__builtin_aarch64_simd_qi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_u16_x3 (uint16_t * __a, uint16x8x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv8hi (__o, (int16x8_t) val.val[2], 2);\n+  __builtin_aarch64_st1x3v8hi ((__builtin_aarch64_simd_hi *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_u32_x3 (uint32_t * __a, uint32x4x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv4si (__o, (int32x4_t) val.val[2], 2);\n+  __builtin_aarch64_st1x3v4si ((__builtin_aarch64_simd_si *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_u64_x3 (uint64_t * __a, uint64x2x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv2di (__o, (int64x2_t) val.val[2], 2);\n+  __builtin_aarch64_st1x3v2di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_f16_x3 (float16_t * __a, float16x8x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv8hf (__o, (float16x8_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv8hf (__o, (float16x8_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv8hf (__o, (float16x8_t) val.val[2], 2);\n+  __builtin_aarch64_st1x3v8hf ((__builtin_aarch64_simd_hf *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_f32_x3 (float32_t * __a, float32x4x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv4sf (__o, (float32x4_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv4sf (__o, (float32x4_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv4sf (__o, (float32x4_t) val.val[2], 2);\n+  __builtin_aarch64_st1x3v4sf ((__builtin_aarch64_simd_sf *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_f64_x3 (float64_t * __a, float64x2x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv2df (__o, (float64x2_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv2df (__o, (float64x2_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv2df (__o, (float64x2_t) val.val[2], 2);\n+  __builtin_aarch64_st1x3v2df ((__builtin_aarch64_simd_df *) __a, __o);\n+}\n+\n+__extension__ extern __inline void\n+__attribute__ ((__always_inline__, __gnu_inline__, __artificial__))\n+vst1q_p64_x3 (poly64_t * __a, poly64x2x3_t val)\n+{\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_set_qregciv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) val.val[0], 0);\n+  __o = __builtin_aarch64_set_qregciv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) val.val[1], 1);\n+  __o = __builtin_aarch64_set_qregciv2di_ssps (__o,\n+\t\t\t\t\t       (poly64x2_t) val.val[2], 2);\n+  __builtin_aarch64_st1x3v2di ((__builtin_aarch64_simd_di *) __a, __o);\n+}\n+\n /* vstn */\n \n __extension__ extern __inline void"}, {"sha": "47fd02efc559088b4211988179dfa7d4788d2d98", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=568421baa5a4cdb7bb7c5ac323c939492ee3f052", "patch": "@@ -1,3 +1,12 @@\n+2018-05-31  Sameera Deshpande  <sameera.deshpande@linaro.org>\n+\n+\t* gcc.target/aarch64/advsimd-intrinsics/vld1x3.c: New test for\n+\tvld1x3 intrinsics for aarch64.\n+\t* gcc.target/aarch64/advsimd-intrinsics/vst1x2.c: New test for\n+\tvst1x2 intrinsics for aarch64.\n+\t* gcc.target/aarch64/advsimd-intrinsics/vst1x3.c: New test for\n+\tvst1x3 intrinsics for aarch64.\n+\n 2018-05-30  Jonathan Wakely  <jwakely@redhat.com>\n \n \tPR c++/77777"}, {"sha": "6ddd507d9cfb97eb5955e3ede8a2b0ecf50bf131", "filename": "gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vld1x3.c", "status": "added", "additions": 82, "deletions": 0, "changes": 82, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvld1x3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvld1x3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvld1x3.c?ref=568421baa5a4cdb7bb7c5ac323c939492ee3f052", "patch": "@@ -0,0 +1,82 @@\n+/* We haven't implemented these intrinsics for arm yet.  */\n+/* { dg-xfail-if \"\" { arm*-*-* } } */\n+/* { dg-do run } */\n+/* { dg-options \"-O3\" } */\n+\n+#include <arm_neon.h>\n+#include \"arm-neon-ref.h\"\n+\n+extern void abort (void);\n+\n+#define TESTMETH(BASE, ELTS, SUFFIX)\t\\\n+int __attribute__ ((noinline))\t\t\t\\\n+test_vld##SUFFIX##_x3 ()\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  BASE##_t data[ELTS * 3];\t\t\t\\\n+  BASE##_t temp[ELTS * 3];\t\t\t\\\n+  BASE##x##ELTS##x##3##_t vectors;\t\t\\\n+  int i,j;\t\t\t\t\t\\\n+  for (i = 0; i < ELTS * 3; i++)\t\t\\\n+    data [i] = (BASE##_t) 3*i;\t\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\\\n+  vectors = vld1##SUFFIX##_x3 (data);\t\t\\\n+  vst1##SUFFIX (temp, vectors.val[0]);\t\t\\\n+  vst1##SUFFIX (&temp[ELTS], vectors.val[1]);\t\\\n+  vst1##SUFFIX (&temp[ELTS * 2], vectors.val[2]);\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\\\n+  for (j = 0; j < ELTS * 3; j++)\t\t\\\n+    if (temp[j] != data[j]) \t\t\t\\\n+      return 1;\t\t\t\t\t\\\n+  return 0;\t\t\t\t\t\\\n+}\n+\n+#define VARIANTS_1(VARIANT)\t\\\n+VARIANT (uint8, 8, _u8)\t\t\\\n+VARIANT (uint16, 4, _u16)\t\\\n+VARIANT (uint32, 2, _u32)\t\\\n+VARIANT (uint64, 1, _u64)\t\\\n+VARIANT (int8, 8, _s8)\t\t\\\n+VARIANT (int16, 4, _s16)\t\\\n+VARIANT (int32, 2, _s32)\t\\\n+VARIANT (int64, 1, _s64)\t\\\n+VARIANT (poly8, 8, _p8)\t\t\\\n+VARIANT (poly16, 4, _p16)\t\\\n+VARIANT (float16, 4, _f16)\t\\\n+VARIANT (float32, 2, _f32)\t\\\n+VARIANT (uint8, 16, q_u8)\t\\\n+VARIANT (uint16, 8, q_u16)\t\\\n+VARIANT (uint32, 4, q_u32)\t\\\n+VARIANT (uint64, 2, q_u64)\t\\\n+VARIANT (int8, 16, q_s8)\t\\\n+VARIANT (int16, 8, q_s16)\t\\\n+VARIANT (int32, 4, q_s32)\t\\\n+VARIANT (int64, 2, q_s64)\t\\\n+VARIANT (poly8, 16, q_p8)\t\\\n+VARIANT (poly16, 8, q_p16)\t\\\n+VARIANT (float16, 8, q_f16)\t\\\n+VARIANT (float32, 4, q_f32)\n+\n+#ifdef __aarch64__\n+#define VARIANTS(VARIANT) VARIANTS_1(VARIANT)\t\\\n+VARIANT (float64, 1, _f64)\t\t\t\\\n+VARIANT (float64, 2, q_f64)\n+#else\n+#define VARIANTS(VARIANT) VARIANTS_1(VARIANT)\n+#endif\n+\n+\n+/* Tests of vld1_x3 and vld1q_x3.  */\n+VARIANTS (TESTMETH)\n+\n+#define CHECKS(BASE, ELTS, SUFFIX)\t\\\n+  if (test_vld##SUFFIX##_x3 () != 0)\t\\\n+    fprintf (stderr, \"test_vld1##SUFFIX##_x3\");\n+\n+int\n+main (int argc, char **argv)\n+{\n+  VARIANTS (CHECKS)\n+\n+  return 0;\n+}\n+"}, {"sha": "cb13da0caed7c0b3e4d925801106d5a7753a297d", "filename": "gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst1x2.c", "status": "added", "additions": 80, "deletions": 0, "changes": 80, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvst1x2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvst1x2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvst1x2.c?ref=568421baa5a4cdb7bb7c5ac323c939492ee3f052", "patch": "@@ -0,0 +1,80 @@\n+/* We haven't implemented these intrinsics for arm yet.  */\n+/* { dg-xfail-if \"\" { arm*-*-* } } */\n+/* { dg-do run } */\n+/* { dg-options \"-O3\" } */\n+\n+#include <arm_neon.h>\n+#include \"arm-neon-ref.h\"\n+\n+extern void abort (void);\n+\n+#define TESTMETH(BASE, ELTS, SUFFIX)\t\\\n+int __attribute__ ((noinline))\t\t\t\\\n+test_vst1##SUFFIX##_x2 ()\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  BASE##_t data[ELTS * 2];\t\t\t\\\n+  BASE##_t temp[ELTS * 2];\t\t\t\\\n+  BASE##x##ELTS##x##2##_t vectors;\t\t\\\n+  int i,j;\t\t\t\t\t\\\n+  for (i = 0; i < ELTS * 2; i++)\t\t\\\n+    data [i] = (BASE##_t) 2*i;\t\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\\\n+  vectors.val[0] = vld1##SUFFIX (data);\t\t\\\n+  vectors.val[1] = vld1##SUFFIX (&data[ELTS]);\t\\\n+  vst1##SUFFIX##_x2 (temp, vectors);\t\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\\\n+  for (j = 0; j < ELTS * 2; j++)\t\t\\\n+    if (temp[j] != data[j])\t\t\t\\\n+      return 1;\t\t\t\t\t\\\n+  return 0;\t\t\t\t\t\\\n+}\n+\n+#define VARIANTS_1(VARIANT)\t\\\n+VARIANT (uint8, 8, _u8)\t\t\\\n+VARIANT (uint16, 4, _u16)\t\\\n+VARIANT (uint32, 2, _u32)\t\\\n+VARIANT (uint64, 1, _u64)\t\\\n+VARIANT (int8, 8, _s8)\t\t\\\n+VARIANT (int16, 4, _s16)\t\\\n+VARIANT (int32, 2, _s32)\t\\\n+VARIANT (int64, 1, _s64)\t\\\n+VARIANT (poly8, 8, _p8)\t\t\\\n+VARIANT (poly16, 4, _p16)\t\\\n+VARIANT (float16, 4, _f16)\t\\\n+VARIANT (float32, 2, _f32)\t\\\n+VARIANT (uint8, 16, q_u8)\t\\\n+VARIANT (uint16, 8, q_u16)\t\\\n+VARIANT (uint32, 4, q_u32)\t\\\n+VARIANT (uint64, 2, q_u64)\t\\\n+VARIANT (int8, 16, q_s8)\t\\\n+VARIANT (int16, 8, q_s16)\t\\\n+VARIANT (int32, 4, q_s32)\t\\\n+VARIANT (int64, 2, q_s64)\t\\\n+VARIANT (poly8, 16, q_p8)\t\\\n+VARIANT (poly16, 8, q_p16)\t\\\n+VARIANT (float16, 8, q_f16)\t\\\n+VARIANT (float32, 4, q_f32)\n+\n+#ifdef __aarch64__\n+#define VARIANTS(VARIANT) VARIANTS_1(VARIANT)\t\\\n+VARIANT (float64, 1, _f64)\t\t\t\\\n+VARIANT (float64, 2, q_f64)\n+#else\n+#define VARIANTS(VARIANT) VARIANTS_1(VARIANT)\n+#endif\n+\n+/* Tests of vst1_x2 and vst1q_x2.  */\n+VARIANTS (TESTMETH)\n+\n+#define CHECKS(BASE, ELTS, SUFFIX)\t\\\n+  if (test_vst1##SUFFIX##_x2 () != 0)\t\\\n+    fprintf (stderr, \"test_vst1##SUFFIX##_x2\");\n+\n+int\n+main (int argc, char **argv)\n+{\n+  VARIANTS (CHECKS)\n+\n+  return 0;\n+}\n+"}, {"sha": "3ce272a500767114347d001e03ef72ba8bc56026", "filename": "gcc/testsuite/gcc.target/aarch64/advsimd-intrinsics/vst1x3.c", "status": "added", "additions": 81, "deletions": 0, "changes": 81, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvst1x3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/568421baa5a4cdb7bb7c5ac323c939492ee3f052/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvst1x3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fadvsimd-intrinsics%2Fvst1x3.c?ref=568421baa5a4cdb7bb7c5ac323c939492ee3f052", "patch": "@@ -0,0 +1,81 @@\n+/* We haven't implemented these intrinsics for arm yet.  */\n+/* { dg-xfail-if \"\" { arm*-*-* } } */\n+/* { dg-do run } */\n+/* { dg-options \"-O3\" } */\n+\n+#include <arm_neon.h>\n+#include \"arm-neon-ref.h\"\n+\n+extern void abort (void);\n+\n+#define TESTMETH(BASE, ELTS, SUFFIX)\t\\\n+int __attribute__ ((noinline))\t\t\t\\\n+test_vst1##SUFFIX##_x3 ()\t\t\t\\\n+{\t\t\t\t\t\t\\\n+  BASE##_t data[ELTS * 3];\t\t\t\\\n+  BASE##_t temp[ELTS * 3];\t\t\t\\\n+  BASE##x##ELTS##x##3##_t vectors;\t\t\\\n+  int i,j;\t\t\t\t\t\\\n+  for (i = 0; i < ELTS * 3; i++)\t\t\\\n+    data [i] = (BASE##_t) 3*i;\t\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\\\n+  vectors.val[0] = vld1##SUFFIX (data);\t\t\\\n+  vectors.val[1] = vld1##SUFFIX (&data[ELTS]);\t\\\n+  vectors.val[2] = vld1##SUFFIX (&data[ELTS * 2]);\t\\\n+  vst1##SUFFIX##_x3 (temp, vectors);\t\t\\\n+  asm volatile (\"\" : : : \"memory\");\t\t\\\n+  for (j = 0; j < ELTS * 3; j++)\t\t\\\n+    if (temp[j] != data[j])\t\t\t\\\n+      return 1;\t\t\t\t\t\\\n+  return 0;\t\t\t\t\t\\\n+}\n+\n+#define VARIANTS_1(VARIANT)\t\\\n+VARIANT (uint8, 8, _u8)\t\t\\\n+VARIANT (uint16, 4, _u16)\t\\\n+VARIANT (uint32, 2, _u32)\t\\\n+VARIANT (uint64, 1, _u64)\t\\\n+VARIANT (int8, 8, _s8)\t\t\\\n+VARIANT (int16, 4, _s16)\t\\\n+VARIANT (int32, 2, _s32)\t\\\n+VARIANT (int64, 1, _s64)\t\\\n+VARIANT (poly8, 8, _p8)\t\t\\\n+VARIANT (poly16, 4, _p16)\t\\\n+VARIANT (float16, 4, _f16)\t\\\n+VARIANT (float32, 2, _f32)\t\\\n+VARIANT (uint8, 16, q_u8)\t\\\n+VARIANT (uint16, 8, q_u16)\t\\\n+VARIANT (uint32, 4, q_u32)\t\\\n+VARIANT (uint64, 2, q_u64)\t\\\n+VARIANT (int8, 16, q_s8)\t\\\n+VARIANT (int16, 8, q_s16)\t\\\n+VARIANT (int32, 4, q_s32)\t\\\n+VARIANT (int64, 2, q_s64)\t\\\n+VARIANT (poly8, 16, q_p8)\t\\\n+VARIANT (poly16, 8, q_p16)\t\\\n+VARIANT (float16, 8, q_f16)\t\\\n+VARIANT (float32, 4, q_f32)\n+\n+#ifdef __aarch64__\n+#define VARIANTS(VARIANT) VARIANTS_1(VARIANT)\t\\\n+VARIANT (float64, 1, _f64)\t\t\t\\\n+VARIANT (float64, 2, q_f64)\n+#else\n+#define VARIANTS(VARIANT) VARIANTS_1(VARIANT)\n+#endif\n+\n+/* Tests of vst1_x3 and vst1q_x3.  */\n+VARIANTS (TESTMETH)\n+\n+#define CHECKS(BASE, ELTS, SUFFIX)\t\\\n+  if (test_vst1##SUFFIX##_x3 () != 0)\t\\\n+    fprintf (stderr, \"test_vst1##SUFFIX##_x3\");\n+\n+int\n+main (int argc, char **argv)\n+{\n+  VARIANTS (CHECKS)\n+\n+  return 0;\n+}\n+"}]}