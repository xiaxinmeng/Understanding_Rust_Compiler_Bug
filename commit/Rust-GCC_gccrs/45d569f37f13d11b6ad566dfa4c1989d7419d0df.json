{"sha": "45d569f37f13d11b6ad566dfa4c1989d7419d0df", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NDVkNTY5ZjM3ZjEzZDExYjZhZDU2NmRmYTRjMTk4OWQ3NDE5ZDBkZg==", "commit": {"author": {"name": "Alan Lawrence", "email": "alan.lawrence@arm.com", "date": "2016-08-10T15:26:14Z"}, "committer": {"name": "Bin Cheng", "email": "amker@gcc.gnu.org", "date": "2016-08-10T15:26:14Z"}, "message": "aarch64-simd.md (vec_cmp<mode><mode>): New pattern.\n\n\t* config/aarch64/aarch64-simd.md (vec_cmp<mode><mode>): New pattern.\n\t(vec_cmp<mode><v_cmp_result>): New pattern.\n\t(vec_cmpu<mode><mode>): New pattern.\n\t(vcond_mask_<mode><v_cmp_result>): New pattern.\n\nCo-Authored-By: Bin Cheng <bin.cheng@arm.com>\nCo-Authored-By: Renlin Li <renlin.li@arm.com>\n\nFrom-SVN: r239327", "tree": {"sha": "db209831289ed442c68abeff2519c1865f11099a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/db209831289ed442c68abeff2519c1865f11099a"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/45d569f37f13d11b6ad566dfa4c1989d7419d0df", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/45d569f37f13d11b6ad566dfa4c1989d7419d0df", "html_url": "https://github.com/Rust-GCC/gccrs/commit/45d569f37f13d11b6ad566dfa4c1989d7419d0df", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/45d569f37f13d11b6ad566dfa4c1989d7419d0df/comments", "author": null, "committer": null, "parents": [{"sha": "ff4fa9934e4ebecfdca45f00efa9f523b11f717a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ff4fa9934e4ebecfdca45f00efa9f523b11f717a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ff4fa9934e4ebecfdca45f00efa9f523b11f717a"}], "stats": {"total": 278, "additions": 278, "deletions": 0}, "files": [{"sha": "6f5f7ebe4efc268d8f5b34ffa3e1b28f8c24a94a", "filename": "gcc/ChangeLog", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/45d569f37f13d11b6ad566dfa4c1989d7419d0df/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/45d569f37f13d11b6ad566dfa4c1989d7419d0df/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=45d569f37f13d11b6ad566dfa4c1989d7419d0df", "patch": "@@ -1,3 +1,12 @@\n+2016-08-10  Alan Lawrence  <alan.lawrence@arm.com>\n+\t    Renlin Li  <renlin.li@arm.com>\n+\t    Bin Cheng  <bin.cheng@arm.com>\n+\n+\t* config/aarch64/aarch64-simd.md (vec_cmp<mode><mode>): New pattern.\n+\t(vec_cmp<mode><v_cmp_result>): New pattern.\n+\t(vec_cmpu<mode><mode>): New pattern.\n+\t(vcond_mask_<mode><v_cmp_result>): New pattern.\n+\n 2016-08-10  Yuri Rumyantsev  <ysrumyan@gmail.com>\n \n \tPR tree-optimization/71734"}, {"sha": "41cc60ad62612c8016f90c401089f9dd44e30a69", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 269, "deletions": 0, "changes": 269, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/45d569f37f13d11b6ad566dfa4c1989d7419d0df/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/45d569f37f13d11b6ad566dfa4c1989d7419d0df/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=45d569f37f13d11b6ad566dfa4c1989d7419d0df", "patch": "@@ -2291,6 +2291,275 @@\n   DONE;\n })\n \n+(define_expand \"vcond_mask_<mode><v_cmp_result>\"\n+  [(match_operand:VALLDI 0 \"register_operand\")\n+   (match_operand:VALLDI 1 \"nonmemory_operand\")\n+   (match_operand:VALLDI 2 \"nonmemory_operand\")\n+   (match_operand:<V_cmp_result> 3 \"register_operand\")]\n+  \"TARGET_SIMD\"\n+{\n+  /* If we have (a = (P) ? -1 : 0);\n+     Then we can simply move the generated mask (result must be int).  */\n+  if (operands[1] == CONSTM1_RTX (<MODE>mode)\n+      && operands[2] == CONST0_RTX (<MODE>mode))\n+    emit_move_insn (operands[0], operands[3]);\n+  /* Similarly, (a = (P) ? 0 : -1) is just inverting the generated mask.  */\n+  else if (operands[1] == CONST0_RTX (<MODE>mode)\n+\t   && operands[2] == CONSTM1_RTX (<MODE>mode))\n+    emit_insn (gen_one_cmpl<v_cmp_result>2 (operands[0], operands[3]));\n+  else\n+    {\n+      if (!REG_P (operands[1]))\n+\toperands[1] = force_reg (<MODE>mode, operands[1]);\n+      if (!REG_P (operands[2]))\n+\toperands[2] = force_reg (<MODE>mode, operands[2]);\n+      emit_insn (gen_aarch64_simd_bsl<mode> (operands[0], operands[3],\n+\t\t\t\t\t     operands[1], operands[2]));\n+    }\n+\n+  DONE;\n+})\n+\n+;; Patterns comparing two vectors to produce a mask.\n+\n+(define_expand \"vec_cmp<mode><mode>\"\n+  [(set (match_operand:VSDQ_I_DI 0 \"register_operand\")\n+\t  (match_operator 1 \"comparison_operator\"\n+\t    [(match_operand:VSDQ_I_DI 2 \"register_operand\")\n+\t     (match_operand:VSDQ_I_DI 3 \"nonmemory_operand\")]))]\n+  \"TARGET_SIMD\"\n+{\n+  rtx mask = operands[0];\n+  enum rtx_code code = GET_CODE (operands[1]);\n+\n+  switch (code)\n+    {\n+    case NE:\n+    case LE:\n+    case LT:\n+    case GE:\n+    case GT:\n+    case EQ:\n+      if (operands[3] == CONST0_RTX (<MODE>mode))\n+\tbreak;\n+\n+      /* Fall through.  */\n+    default:\n+      if (!REG_P (operands[3]))\n+\toperands[3] = force_reg (<MODE>mode, operands[3]);\n+\n+      break;\n+    }\n+\n+  switch (code)\n+    {\n+    case LT:\n+      emit_insn (gen_aarch64_cmlt<mode> (mask, operands[2], operands[3]));\n+      break;\n+\n+    case GE:\n+      emit_insn (gen_aarch64_cmge<mode> (mask, operands[2], operands[3]));\n+      break;\n+\n+    case LE:\n+      emit_insn (gen_aarch64_cmle<mode> (mask, operands[2], operands[3]));\n+      break;\n+\n+    case GT:\n+      emit_insn (gen_aarch64_cmgt<mode> (mask, operands[2], operands[3]));\n+      break;\n+\n+    case LTU:\n+      emit_insn (gen_aarch64_cmgtu<mode> (mask, operands[3], operands[2]));\n+      break;\n+\n+    case GEU:\n+      emit_insn (gen_aarch64_cmgeu<mode> (mask, operands[2], operands[3]));\n+      break;\n+\n+    case LEU:\n+      emit_insn (gen_aarch64_cmgeu<mode> (mask, operands[3], operands[2]));\n+      break;\n+\n+    case GTU:\n+      emit_insn (gen_aarch64_cmgtu<mode> (mask, operands[2], operands[3]));\n+      break;\n+\n+    case NE:\n+      /* Handle NE as !EQ.  */\n+      emit_insn (gen_aarch64_cmeq<mode> (mask, operands[2], operands[3]));\n+      emit_insn (gen_one_cmpl<v_cmp_result>2 (mask, mask));\n+      break;\n+\n+    case EQ:\n+      emit_insn (gen_aarch64_cmeq<mode> (mask, operands[2], operands[3]));\n+      break;\n+\n+    default:\n+      gcc_unreachable ();\n+    }\n+\n+  DONE;\n+})\n+\n+(define_expand \"vec_cmp<mode><v_cmp_result>\"\n+  [(set (match_operand:<V_cmp_result> 0 \"register_operand\")\n+\t(match_operator 1 \"comparison_operator\"\n+\t    [(match_operand:VDQF 2 \"register_operand\")\n+\t     (match_operand:VDQF 3 \"nonmemory_operand\")]))]\n+  \"TARGET_SIMD\"\n+{\n+  int use_zero_form = 0;\n+  enum rtx_code code = GET_CODE (operands[1]);\n+  rtx tmp = gen_reg_rtx (<V_cmp_result>mode);\n+\n+  rtx (*comparison) (rtx, rtx, rtx);\n+\n+  switch (code)\n+    {\n+    case LE:\n+    case LT:\n+    case GE:\n+    case GT:\n+    case EQ:\n+      if (operands[3] == CONST0_RTX (<MODE>mode))\n+\t{\n+\t  use_zero_form = 1;\n+\t  break;\n+\t}\n+      /* Fall through.  */\n+    default:\n+      if (!REG_P (operands[3]))\n+\toperands[3] = force_reg (<MODE>mode, operands[3]);\n+\n+      break;\n+    }\n+\n+  switch (code)\n+    {\n+    case LT:\n+      if (use_zero_form)\n+\t{\n+\t  comparison = gen_aarch64_cmlt<mode>;\n+\t  break;\n+\t}\n+      /* Else, fall through.  */\n+    case UNGE:\n+      std::swap (operands[2], operands[3]);\n+      /* Fall through.  */\n+    case UNLE:\n+    case GT:\n+      comparison = gen_aarch64_cmgt<mode>;\n+      break;\n+    case LE:\n+      if (use_zero_form)\n+\t{\n+\t  comparison = gen_aarch64_cmle<mode>;\n+\t  break;\n+\t}\n+      /* Else, fall through.  */\n+    case UNGT:\n+      std::swap (operands[2], operands[3]);\n+      /* Fall through.  */\n+    case UNLT:\n+    case GE:\n+      comparison = gen_aarch64_cmge<mode>;\n+      break;\n+    case NE:\n+    case EQ:\n+      comparison = gen_aarch64_cmeq<mode>;\n+      break;\n+    case UNEQ:\n+    case ORDERED:\n+    case UNORDERED:\n+      break;\n+    default:\n+      gcc_unreachable ();\n+    }\n+\n+  switch (code)\n+    {\n+    case UNGE:\n+    case UNGT:\n+    case UNLE:\n+    case UNLT:\n+    case NE:\n+      /* FCM returns false for lanes which are unordered, so if we use\n+\t the inverse of the comparison we actually want to emit, then\n+\t invert the result, we will end up with the correct result.\n+\t Note that a NE NaN and NaN NE b are true for all a, b.\n+\n+\t Our transformations are:\n+\t a UNGE b -> !(b GT a)\n+\t a UNGT b -> !(b GE a)\n+\t a UNLE b -> !(a GT b)\n+\t a UNLT b -> !(a GE b)\n+\t a   NE b -> !(a EQ b)  */\n+      emit_insn (comparison (operands[0], operands[2], operands[3]));\n+      emit_insn (gen_one_cmpl<v_cmp_result>2 (operands[0], operands[0]));\n+      break;\n+\n+    case LT:\n+    case LE:\n+    case GT:\n+    case GE:\n+    case EQ:\n+      /* The easy case.  Here we emit one of FCMGE, FCMGT or FCMEQ.\n+\t As a LT b <=> b GE a && a LE b <=> b GT a.  Our transformations are:\n+\t a GE b -> a GE b\n+\t a GT b -> a GT b\n+\t a LE b -> b GE a\n+\t a LT b -> b GT a\n+\t a EQ b -> a EQ b  */\n+      emit_insn (comparison (operands[0], operands[2], operands[3]));\n+      break;\n+\n+    case UNEQ:\n+      /* We first check (a > b ||  b > a) which is !UNEQ, inverting\n+\t this result will then give us (a == b || a UNORDERED b).  */\n+      emit_insn (gen_aarch64_cmgt<mode> (operands[0],\n+\t\t\t\t\t operands[2], operands[3]));\n+      emit_insn (gen_aarch64_cmgt<mode> (tmp, operands[3], operands[2]));\n+      emit_insn (gen_ior<v_cmp_result>3 (operands[0], operands[0], tmp));\n+      emit_insn (gen_one_cmpl<v_cmp_result>2 (operands[0], operands[0]));\n+      break;\n+\n+    case UNORDERED:\n+      /* Operands are ORDERED iff (a > b || b >= a), so we can compute\n+\t UNORDERED as !ORDERED.  */\n+      emit_insn (gen_aarch64_cmgt<mode> (tmp, operands[2], operands[3]));\n+      emit_insn (gen_aarch64_cmge<mode> (operands[0],\n+\t\t\t\t\t operands[3], operands[2]));\n+      emit_insn (gen_ior<v_cmp_result>3 (operands[0], operands[0], tmp));\n+      emit_insn (gen_one_cmpl<v_cmp_result>2 (operands[0], operands[0]));\n+      break;\n+\n+    case ORDERED:\n+      emit_insn (gen_aarch64_cmgt<mode> (tmp, operands[2], operands[3]));\n+      emit_insn (gen_aarch64_cmge<mode> (operands[0],\n+\t\t\t\t\t operands[3], operands[2]));\n+      emit_insn (gen_ior<v_cmp_result>3 (operands[0], operands[0], tmp));\n+      break;\n+\n+    default:\n+      gcc_unreachable ();\n+    }\n+\n+  DONE;\n+})\n+\n+(define_expand \"vec_cmpu<mode><mode>\"\n+  [(set (match_operand:VSDQ_I_DI 0 \"register_operand\")\n+\t  (match_operator 1 \"comparison_operator\"\n+\t    [(match_operand:VSDQ_I_DI 2 \"register_operand\")\n+\t     (match_operand:VSDQ_I_DI 3 \"nonmemory_operand\")]))]\n+  \"TARGET_SIMD\"\n+{\n+  emit_insn (gen_vec_cmp<mode><mode> (operands[0], operands[1],\n+\t\t\t\t      operands[2], operands[3]));\n+  DONE;\n+})\n+\n (define_expand \"aarch64_vcond_internal<mode><mode>\"\n   [(set (match_operand:VSDQ_I_DI 0 \"register_operand\")\n \t(if_then_else:VSDQ_I_DI"}]}