{"sha": "e288e2f51bed5d45a8a89479bc2f98337bc18c80", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZTI4OGUyZjUxYmVkNWQ0NWE4YTg5NDc5YmMyZjk4MzM3YmMxOGM4MA==", "commit": {"author": {"name": "Andrew MacLeod", "email": "amacleod@redhat.com", "date": "2004-11-25T20:24:59Z"}, "committer": {"name": "Andrew Macleod", "email": "amacleod@gcc.gnu.org", "date": "2004-11-25T20:24:59Z"}, "message": "re PR tree-optimization/18587 (build_v_may_defs and build_vuses can be improved when adding)\n\n\n2004-11-25  Andrew Macleod  <amacleod@redhat.com>\n\n\tPR tree-optimization/18587\n\t* tree-flow-inline.h (mark_call_clobbered, mark_non_addressable): Flag\n\tcall clobbered caches as invalid.\n\t* tree-ssa-operands.c (ssa_call_clobbered_cache_valid): New.  Flag\n\tindicating whether the call clobbered operand cache is valid.\n\t(ssa_ro_call_cache_valid): New.  Flag indicating whether the pure/const\n\tcall operand cache is valid.\n\t(clobbered_v_may_defs, clobbered_vuses, ro_call_vuses): New.\n\tcached list of operands for cached call virtual operands.\n\t(clobbered_aliased_loads, clobbered_aliased_stores,\n\tro_call_aliased_load): New.  flags caching whether alias bits are to be\n\tset in call stmt's.  */\n\t(fini_ssa_operands): Remove call operand caches if present.\n\t(get_expr_operands, get_asm_expr_operands, get_indirect_ref_operands):\n\tPass stmt annotation to add_stmt_operand.\n\t(get_call_expr_operands): Add call clobbered variables first.\n\t(add_stmt_operand): Take stmt annotation rather than stmt as a param.\n\t(add_call_clobber_ops, add_call_read_ops): Use the call operand cache\n\tif it is valid, otherise fill the cache.\n\t* tree-ssa-operands.h (ssa_clobbered_cache_valid): Declare extern.\n\n\t* tree-flow.h (struct var_ann_d): Add in_vuse_list and in_v_may_def_list\n\tbits.\n\t* tree-ssa-operands.c (cleanup_v_may_defs): New.  Clear the in_list bits\n\tfor the v_may_def elements and empty the operand build array.\n\t(finalize_ssa_vuses): Use cleanup_v_may_defs and remove redundant VUSES\n\tby checking the in_v_may_def_list bit.\n\t(append_v_may_def, append_vuse): Use the in_list bit rather than\n\tscanning the array for duplicates.\n\nFrom-SVN: r91305", "tree": {"sha": "e6a3919cd01a314a36fdb8c06e651c13ea0543de", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/e6a3919cd01a314a36fdb8c06e651c13ea0543de"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/e288e2f51bed5d45a8a89479bc2f98337bc18c80", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e288e2f51bed5d45a8a89479bc2f98337bc18c80", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e288e2f51bed5d45a8a89479bc2f98337bc18c80", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e288e2f51bed5d45a8a89479bc2f98337bc18c80/comments", "author": null, "committer": null, "parents": [{"sha": "5257260c2bac40d3691c6054520d52aced302b65", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5257260c2bac40d3691c6054520d52aced302b65", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5257260c2bac40d3691c6054520d52aced302b65"}], "stats": {"total": 375, "additions": 286, "deletions": 89}, "files": [{"sha": "25dec91a3ebec3a2ba4e7d937991596cd77a2523", "filename": "gcc/ChangeLog", "status": "modified", "additions": 31, "deletions": 0, "changes": 31, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e288e2f51bed5d45a8a89479bc2f98337bc18c80/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e288e2f51bed5d45a8a89479bc2f98337bc18c80/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=e288e2f51bed5d45a8a89479bc2f98337bc18c80", "patch": "@@ -1,3 +1,34 @@\n+2004-11-25  Andrew MacLeod  <amacleod@redhat.com>\n+\n+\tPR tree-optimization/18587\n+\t* tree-flow-inline.h (mark_call_clobbered, mark_non_addressable): Flag\n+\tcall clobbered caches as invalid.\n+\t* tree-ssa-operands.c (ssa_call_clobbered_cache_valid): New.  Flag \n+\tindicating whether the call clobbered operand cache is valid.\n+\t(ssa_ro_call_cache_valid): New.  Flag indicating whether the pure/const\n+\tcall operand cache is valid.\n+\t(clobbered_v_may_defs, clobbered_vuses, ro_call_vuses): New.  Cached \n+\tlist of operands for cached call virtual operands.\n+\t(clobbered_aliased_loads, clobbered_aliased_stores, \n+\tro_call_aliased_load): New.  flags caching whether alias bits are to be\n+\tset in call stmt's.\n+\t(fini_ssa_operands): Remove call operand caches if present.\n+\t(get_expr_operands, get_asm_expr_operands, get_indirect_ref_operands): \n+\tPass stmt annotation to add_stmt_operand.\n+\t(get_call_expr_operands): Add call clobbered variables first.\n+\t(add_stmt_operand): Take stmt annotation rather than stmt as a param.\n+\t(add_call_clobber_ops, add_call_read_ops): Use the call operand cache\n+\tif it is valid, otherise fill the cache.\n+\t* tree-ssa-operands.h (ssa_clobbered_cache_valid): Declare extern.\n+\t* tree-flow.h (struct var_ann_d): Add in_vuse_list and in_v_may_def_list\n+\tbits.\n+\t* tree-ssa-operands.c (cleanup_v_may_defs): New.  Clear the in_list bits\n+\tfor the v_may_def elements and empty the operand build array.\n+\t(finalize_ssa_vuses): Use cleanup_v_may_defs and remove redundant VUSES\n+\tby checking the in_v_may_def_list bit.\n+\t(append_v_may_def, append_vuse): Use the in_list bit rather than \n+\tscanning the array for duplicates.\n+\n 2004-11-25  Ulrich Weigand  <uweigand@de.ibm.com>\n \n \t* config/s390/s390.c (s390_short_displacement): UNSPEC_GOTNTPOFF"}, {"sha": "fd31a76f84c999ee6bae739f3fc9582252cb8480", "filename": "gcc/tree-flow-inline.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e288e2f51bed5d45a8a89479bc2f98337bc18c80/gcc%2Ftree-flow-inline.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e288e2f51bed5d45a8a89479bc2f98337bc18c80/gcc%2Ftree-flow-inline.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-flow-inline.h?ref=e288e2f51bed5d45a8a89479bc2f98337bc18c80", "patch": "@@ -621,6 +621,8 @@ mark_call_clobbered (tree var)\n   if (ann->mem_tag_kind != NOT_A_TAG)\n     DECL_EXTERNAL (var) = 1;\n   bitmap_set_bit (call_clobbered_vars, ann->uid);\n+  ssa_call_clobbered_cache_valid = false;\n+  ssa_ro_call_cache_valid = false;\n }\n \n /* Mark variable VAR as being non-addressable.  */\n@@ -629,6 +631,8 @@ mark_non_addressable (tree var)\n {\n   bitmap_clear_bit (call_clobbered_vars, var_ann (var)->uid);\n   TREE_ADDRESSABLE (var) = 0;\n+  ssa_call_clobbered_cache_valid = false;\n+  ssa_ro_call_cache_valid = false;\n }\n \n /* Return the common annotation for T.  Return NULL if the annotation"}, {"sha": "9847eaf6ac3290ea7550ed532f3aeb11acdb9100", "filename": "gcc/tree-flow.h", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e288e2f51bed5d45a8a89479bc2f98337bc18c80/gcc%2Ftree-flow.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e288e2f51bed5d45a8a89479bc2f98337bc18c80/gcc%2Ftree-flow.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-flow.h?ref=e288e2f51bed5d45a8a89479bc2f98337bc18c80", "patch": "@@ -166,6 +166,14 @@ struct var_ann_d GTY(())\n      states.  */\n   ENUM_BITFIELD (need_phi_state) need_phi_state : 2;\n \n+  /* Used during operand processing to determine if this variable is already \n+     in the vuse list.  */\n+  unsigned in_vuse_list : 1;\n+\n+  /* Used during operand processing to determine if this variable is already \n+     in the v_may_def list.  */\n+  unsigned in_v_may_def_list : 1;\n+\n   /* An artificial variable representing the memory location pointed-to by\n      all the pointers that TBAA (type-based alias analysis) considers\n      to be aliased.  If the variable is not a pointer or if it is never"}, {"sha": "9138e677450cbf22261cc40b3624d6ca5de9eeab", "filename": "gcc/tree-ssa-operands.c", "status": "modified", "additions": 241, "deletions": 89, "changes": 330, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e288e2f51bed5d45a8a89479bc2f98337bc18c80/gcc%2Ftree-ssa-operands.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e288e2f51bed5d45a8a89479bc2f98337bc18c80/gcc%2Ftree-ssa-operands.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-operands.c?ref=e288e2f51bed5d45a8a89479bc2f98337bc18c80", "patch": "@@ -116,6 +116,17 @@ static GTY (()) varray_type build_vuses;\n /* Array for building all the v_must_def operands.  */\n static GTY (()) varray_type build_v_must_defs;\n \n+/* True if the operands for call clobbered vars are cached and valid.  */\n+bool ssa_call_clobbered_cache_valid;\n+bool ssa_ro_call_cache_valid;\n+\n+/* These arrays are the cached operand vectors for call clobberd calls.  */\n+static GTY (()) varray_type clobbered_v_may_defs;\n+static GTY (()) varray_type clobbered_vuses;\n+static GTY (()) varray_type ro_call_vuses;\n+static bool clobbered_aliased_loads;\n+static bool clobbered_aliased_stores;\n+static bool ro_call_aliased_loads;\n \n #ifdef ENABLE_CHECKING\n /* Used to make sure operand construction is working on the proper stmt.  */\n@@ -136,7 +147,7 @@ static void append_v_may_def (tree);\n static void append_v_must_def (tree);\n static void add_call_clobber_ops (tree);\n static void add_call_read_ops (tree);\n-static void add_stmt_operand (tree *, tree, int);\n+static void add_stmt_operand (tree *, stmt_ann_t, int);\n \n /* Return a vector of contiguous memory for NUM def operands.  */\n \n@@ -302,6 +313,18 @@ fini_ssa_operands (void)\n   build_v_may_defs = NULL;\n   build_vuses = NULL;\n   build_v_must_defs = NULL;\n+  if (clobbered_v_may_defs)\n+    {\n+      ggc_free (clobbered_v_may_defs);\n+      ggc_free (clobbered_vuses);\n+      clobbered_v_may_defs = NULL;\n+      clobbered_vuses = NULL;\n+    }\n+  if (ro_call_vuses)\n+    {\n+      ggc_free (ro_call_vuses);\n+      ro_call_vuses = NULL;\n+    }\n }\n \n \n@@ -490,6 +513,23 @@ finalize_ssa_v_may_defs (v_may_def_optype *old_ops_p)\n }\n \n \n+/* Clear the in_list bits and empty the build array for v_may_defs.  */\n+\n+static inline void\n+cleanup_v_may_defs (void)\n+{\n+  unsigned x, num;\n+  num = VARRAY_ACTIVE_SIZE (build_v_may_defs);\n+\n+  for (x = 0; x < num; x++)\n+    {\n+      tree t = VARRAY_TREE (build_v_may_defs, x);\n+      var_ann_t ann = var_ann (t);\n+      ann->in_v_may_def_list = 0;\n+    }\n+  VARRAY_POP_ALL (build_v_may_defs);\n+}\n+\n /* Return a new vuse operand vector, comparing to OLD_OPS_P.  */\n \n static vuse_optype\n@@ -502,7 +542,7 @@ finalize_ssa_vuses (vuse_optype *old_ops_p)\n   num = VARRAY_ACTIVE_SIZE (build_vuses);\n   if (num == 0)\n     {\n-      VARRAY_POP_ALL (build_v_may_defs);\n+      cleanup_v_may_defs ();\n       return NULL;\n     }\n \n@@ -522,44 +562,55 @@ finalize_ssa_vuses (vuse_optype *old_ops_p)\n \n   if (num_v_may_defs > 0)\n     {\n-      size_t i, j;\n+      size_t i;\n       tree vuse;\n       for (i = 0; i < VARRAY_ACTIVE_SIZE (build_vuses); i++)\n \t{\n \t  vuse = VARRAY_TREE (build_vuses, i);\n-\t  for (j = 0; j < num_v_may_defs; j++)\n+\t  if (TREE_CODE (vuse) != SSA_NAME)\n \t    {\n-\t      if (vuse == VARRAY_TREE (build_v_may_defs, j))\n-\t\tbreak;\n-\t    }\n-\n-\t  /* If we found a useless VUSE operand, remove it from the\n-\t     operand array by replacing it with the last active element\n-\t     in the operand array (unless the useless VUSE was the\n-\t     last operand, in which case we simply remove it.  */\n-\t  if (j != num_v_may_defs)\n-\t    {\n-\t      if (i != VARRAY_ACTIVE_SIZE (build_vuses) - 1)\n-\t\t{\n-\t\t  VARRAY_TREE (build_vuses, i)\n-\t\t    = VARRAY_TREE (build_vuses,\n-\t\t\t\t   VARRAY_ACTIVE_SIZE (build_vuses) - 1);\n+\t      var_ann_t ann = var_ann (vuse);\n+\t      ann->in_vuse_list = 0;\n+\t      if (ann->in_v_may_def_list)\n+\t        {\n+\t\t  /* If we found a useless VUSE operand, remove it from the\n+\t\t     operand array by replacing it with the last active element\n+\t\t     in the operand array (unless the useless VUSE was the\n+\t\t     last operand, in which case we simply remove it.  */\n+\t\t  if (i != VARRAY_ACTIVE_SIZE (build_vuses) - 1)\n+\t\t    {\n+\t\t      VARRAY_TREE (build_vuses, i)\n+\t\t\t= VARRAY_TREE (build_vuses,\n+\t\t\t\t       VARRAY_ACTIVE_SIZE (build_vuses) - 1);\n+\t\t    }\n+\t\t  VARRAY_POP (build_vuses);\n+\n+\t\t  /* We want to rescan the element at this index, unless\n+\t\t     this was the last element, in which case the loop\n+\t\t     terminates.  */\n+\t\t  i--;\n \t\t}\n-\t      VARRAY_POP (build_vuses);\n-\n-\t      /* We want to rescan the element at this index, unless\n-\t\t this was the last element, in which case the loop\n-\t\t terminates.  */\n-\t      i--;\n \t    }\n \t}\n     }\n+  else\n+    /* Clear out the in_list bits.  */\n+    for (x = 0; x < num; x++)\n+      {\n+\ttree t = VARRAY_TREE (build_vuses, x);\n+\tif (TREE_CODE (t) != SSA_NAME)\n+\t  {\n+\t    var_ann_t ann = var_ann (t);\n+\t    ann->in_vuse_list = 0;\n+\t  }\n+      }\n+\n \n   num = VARRAY_ACTIVE_SIZE (build_vuses);\n   /* We could have reduced the size to zero now, however.  */\n   if (num == 0)\n     {\n-      VARRAY_POP_ALL (build_v_may_defs);\n+      cleanup_v_may_defs ();\n       return NULL;\n     }\n \n@@ -618,7 +669,7 @@ finalize_ssa_vuses (vuse_optype *old_ops_p)\n   /* The v_may_def build vector wasn't freed because we needed it here.\n      Free it now with the vuses build vector.  */\n   VARRAY_POP_ALL (build_vuses);\n-  VARRAY_POP_ALL (build_v_may_defs);\n+  cleanup_v_may_defs ();\n \n   return vuse_ops;\n }\n@@ -751,12 +802,12 @@ append_use (tree *use_p)\n static inline void\n append_v_may_def (tree var)\n {\n-  unsigned i;\n+  var_ann_t ann = get_var_ann (var);\n \n   /* Don't allow duplicate entries.  */\n-  for (i = 0; i < VARRAY_ACTIVE_SIZE (build_v_may_defs); i++)\n-    if (var == VARRAY_TREE (build_v_may_defs, i))\n-      return;\n+  if (ann->in_v_may_def_list)\n+    return;\n+  ann->in_v_may_def_list = 1;\n \n   VARRAY_PUSH_TREE (build_v_may_defs, var);\n }\n@@ -767,12 +818,16 @@ append_v_may_def (tree var)\n static inline void\n append_vuse (tree var)\n {\n-  size_t i;\n \n   /* Don't allow duplicate entries.  */\n-  for (i = 0; i < VARRAY_ACTIVE_SIZE (build_vuses); i++)\n-    if (var == VARRAY_TREE (build_vuses, i))\n-      return;\n+  if (TREE_CODE (var) != SSA_NAME)\n+    {\n+      var_ann_t ann = get_var_ann (var);\n+\n+      if (ann->in_vuse_list || ann->in_v_may_def_list)\n+        return;\n+      ann->in_vuse_list = 1;\n+    }\n \n   VARRAY_PUSH_TREE (build_vuses, var);\n }\n@@ -972,6 +1027,7 @@ get_expr_operands (tree stmt, tree *expr_p, int flags)\n   enum tree_code code;\n   enum tree_code_class class;\n   tree expr = *expr_p;\n+  stmt_ann_t s_ann = stmt_ann (stmt);\n \n   if (expr == NULL || expr == error_mark_node)\n     return;\n@@ -987,7 +1043,7 @@ get_expr_operands (tree stmt, tree *expr_p, int flags)\n       /* Taking the address of a variable does not represent a\n \t reference to it, but the fact that the stmt takes its address will be\n \t of interest to some passes (e.g. alias resolution).  */\n-      add_stmt_operand (expr_p, stmt, 0);\n+      add_stmt_operand (expr_p, s_ann, 0);\n \n       /* If the address is invariant, there may be no interesting variable\n \t references inside.  */\n@@ -1010,7 +1066,7 @@ get_expr_operands (tree stmt, tree *expr_p, int flags)\n     case CONST_DECL:\n       /* If we found a variable, add it to DEFS or USES depending\n \t on the operand flags.  */\n-      add_stmt_operand (expr_p, stmt, flags);\n+      add_stmt_operand (expr_p, s_ann, flags);\n       return;\n \n     case MISALIGNED_INDIRECT_REF:\n@@ -1032,7 +1088,7 @@ get_expr_operands (tree stmt, tree *expr_p, int flags)\n \t according to the value of IS_DEF.  Recurse if the LHS of the\n \t ARRAY_REF node is not a regular variable.  */\n       if (SSA_VAR_P (TREE_OPERAND (expr, 0)))\n-\tadd_stmt_operand (expr_p, stmt, flags);\n+\tadd_stmt_operand (expr_p, s_ann, flags);\n       else\n \tget_expr_operands (stmt, &TREE_OPERAND (expr, 0), flags);\n \n@@ -1060,7 +1116,7 @@ get_expr_operands (tree stmt, tree *expr_p, int flags)\n       /* If the LHS of the compound reference is not a regular variable,\n \t recurse to keep looking for more operands in the subexpression.  */\n       if (SSA_VAR_P (TREE_OPERAND (expr, 0)))\n-\tadd_stmt_operand (expr_p, stmt, flags);\n+\tadd_stmt_operand (expr_p, s_ann, flags);\n       else\n \tget_expr_operands (stmt, &TREE_OPERAND (expr, 0), flags);\n \n@@ -1273,19 +1329,19 @@ get_asm_expr_operands (tree stmt)\n \t/* Clobber all call-clobbered variables (or .GLOBAL_VAR if we\n \t   decided to group them).  */\n \tif (global_var)\n-\t  add_stmt_operand (&global_var, stmt, opf_is_def);\n+\t  add_stmt_operand (&global_var, s_ann, opf_is_def);\n \telse\n \t  EXECUTE_IF_SET_IN_BITMAP (call_clobbered_vars, 0, i, bi)\n \t      {\n \t\ttree var = referenced_var (i);\n-\t\tadd_stmt_operand (&var, stmt, opf_is_def);\n+\t\tadd_stmt_operand (&var, s_ann, opf_is_def);\n \t      }\n \n \t/* Now clobber all addressables.  */\n \tEXECUTE_IF_SET_IN_BITMAP (addressable_vars, 0, i, bi)\n \t    {\n \t      tree var = referenced_var (i);\n-\t      add_stmt_operand (&var, stmt, opf_is_def);\n+\t      add_stmt_operand (&var, s_ann, opf_is_def);\n \t    }\n \n \tbreak;\n@@ -1300,7 +1356,7 @@ get_indirect_ref_operands (tree stmt, tree expr, int flags)\n {\n   tree *pptr = &TREE_OPERAND (expr, 0);\n   tree ptr = *pptr;\n-  stmt_ann_t ann = stmt_ann (stmt);\n+  stmt_ann_t s_ann = stmt_ann (stmt);\n \n   /* Stores into INDIRECT_REF operands are never killing definitions.  */\n   flags &= ~opf_kill_def;\n@@ -1327,13 +1383,13 @@ get_indirect_ref_operands (tree stmt, tree expr, int flags)\n \t  && pi->name_mem_tag)\n \t{\n \t  /* PTR has its own memory tag.  Use it.  */\n-\t  add_stmt_operand (&pi->name_mem_tag, stmt, flags);\n+\t  add_stmt_operand (&pi->name_mem_tag, s_ann, flags);\n \t}\n       else\n \t{\n \t  /* If PTR is not an SSA_NAME or it doesn't have a name\n \t     tag, use its type memory tag.  */\n-\t  var_ann_t ann;\n+\t  var_ann_t v_ann;\n \n \t  /* If we are emitting debugging dumps, display a warning if\n \t     PTR is an SSA_NAME with no flow-sensitive alias\n@@ -1352,9 +1408,9 @@ get_indirect_ref_operands (tree stmt, tree expr, int flags)\n \n \t  if (TREE_CODE (ptr) == SSA_NAME)\n \t    ptr = SSA_NAME_VAR (ptr);\n-\t  ann = var_ann (ptr);\n-\t  if (ann->type_mem_tag)\n-\t    add_stmt_operand (&ann->type_mem_tag, stmt, flags);\n+\t  v_ann = var_ann (ptr);\n+\t  if (v_ann->type_mem_tag)\n+\t    add_stmt_operand (&v_ann->type_mem_tag, s_ann, flags);\n \t}\n     }\n \n@@ -1363,8 +1419,8 @@ get_indirect_ref_operands (tree stmt, tree expr, int flags)\n      optimizations from messing things up.  */\n   else if (TREE_CODE (ptr) == INTEGER_CST)\n     {\n-      if (ann)\n-\tann->has_volatile_ops = true;\n+      if (s_ann)\n+\ts_ann->has_volatile_ops = true;\n       return;\n     }\n \n@@ -1379,7 +1435,7 @@ get_indirect_ref_operands (tree stmt, tree expr, int flags)\n     {\n       /* Make sure we know the object is addressable.  */\n       pptr = &TREE_OPERAND (ptr, 0);\n-      add_stmt_operand (pptr, stmt, 0);\n+      add_stmt_operand (pptr, s_ann, 0);\n \n       /* Mark the object itself with a VUSE.  */\n       pptr = &TREE_OPERAND (*pptr, 0);\n@@ -1403,14 +1459,6 @@ get_call_expr_operands (tree stmt, tree expr)\n   tree op;\n   int call_flags = call_expr_flags (expr);\n \n-  /* Find uses in the called function.  */\n-  get_expr_operands (stmt, &TREE_OPERAND (expr, 0), opf_none);\n-\n-  for (op = TREE_OPERAND (expr, 1); op; op = TREE_CHAIN (op))\n-    get_expr_operands (stmt, &TREE_VALUE (op), opf_none);\n-\n-  get_expr_operands (stmt, &TREE_OPERAND (expr, 2), opf_none);\n-\n   if (!bitmap_empty_p (call_clobbered_vars))\n     {\n       /* A 'pure' or a 'const' functions never call clobber anything. \n@@ -1422,6 +1470,15 @@ get_call_expr_operands (tree stmt, tree expr)\n       else if (!(call_flags & ECF_CONST))\n \tadd_call_read_ops (stmt);\n     }\n+\n+  /* Find uses in the called function.  */\n+  get_expr_operands (stmt, &TREE_OPERAND (expr, 0), opf_none);\n+\n+  for (op = TREE_OPERAND (expr, 1); op; op = TREE_CHAIN (op))\n+    get_expr_operands (stmt, &TREE_VALUE (op), opf_none);\n+\n+  get_expr_operands (stmt, &TREE_OPERAND (expr, 2), opf_none);\n+\n }\n \n \n@@ -1431,11 +1488,10 @@ get_call_expr_operands (tree stmt, tree expr)\n    operands.  */\n \n static void\n-add_stmt_operand (tree *var_p, tree stmt, int flags)\n+add_stmt_operand (tree *var_p, stmt_ann_t s_ann, int flags)\n {\n   bool is_real_op;\n   tree var, sym;\n-  stmt_ann_t s_ann = stmt_ann (stmt);\n   var_ann_t v_ann;\n \n   var = *var_p;\n@@ -1586,32 +1642,92 @@ note_addressable (tree var, stmt_ann_t s_ann)\n static void\n add_call_clobber_ops (tree stmt)\n {\n+  unsigned i;\n+  tree t;\n+  bitmap_iterator bi;\n+  stmt_ann_t s_ann = stmt_ann (stmt);\n+  struct stmt_ann_d empty_ann;\n+\n   /* Functions that are not const, pure or never return may clobber\n      call-clobbered variables.  */\n-  if (stmt_ann (stmt))\n-    stmt_ann (stmt)->makes_clobbering_call = true;\n+  if (s_ann)\n+    s_ann->makes_clobbering_call = true;\n \n-  /* If we had created .GLOBAL_VAR earlier, use it.  Otherwise, add \n-     a V_MAY_DEF operand for every call clobbered variable.  See \n-     compute_may_aliases for the heuristic used to decide whether \n-     to create .GLOBAL_VAR or not.  */\n+  /* If we created .GLOBAL_VAR earlier, just use it.  See compute_may_aliases \n+     for the heuristic used to decide whether to create .GLOBAL_VAR or not.  */\n   if (global_var)\n-    add_stmt_operand (&global_var, stmt, opf_is_def);\n-  else\n     {\n-      unsigned i;\n-      bitmap_iterator bi;\n+      add_stmt_operand (&global_var, s_ann, opf_is_def);\n+      return;\n+    }\n \n-      EXECUTE_IF_SET_IN_BITMAP (call_clobbered_vars, 0, i, bi)\n+  /* If cache is valid, copy the elements into the build vectors.  */\n+  if (ssa_call_clobbered_cache_valid)\n+    {\n+      for (i = 0; i < VARRAY_ACTIVE_SIZE (clobbered_vuses); i++)\n \t{\n-\t  tree var = referenced_var (i);\n-\t  if (TREE_READONLY (var)\n-\t      && (TREE_STATIC (var) || DECL_EXTERNAL (var)))\n-\t    add_stmt_operand (&var, stmt, opf_none);\n-\t  else\n-\t    add_stmt_operand (&var, stmt, opf_is_def);\n+\t  t = VARRAY_TREE (clobbered_vuses, i);\n+\t  gcc_assert (TREE_CODE (t) != SSA_NAME);\n+\t  var_ann (t)->in_vuse_list = 1;\n+\t  VARRAY_PUSH_TREE (build_vuses, t);\n+\t}\n+      for (i = 0; i < VARRAY_ACTIVE_SIZE (clobbered_v_may_defs); i++)\n+\t{\n+\t  t = VARRAY_TREE (clobbered_v_may_defs, i);\n+\t  gcc_assert (TREE_CODE (t) != SSA_NAME);\n+\t  var_ann (t)->in_v_may_def_list = 1;\n+\t  VARRAY_PUSH_TREE (build_v_may_defs, t);\n \t}\n+      if (s_ann)\n+\t{\n+\t  s_ann->makes_aliased_loads = clobbered_aliased_loads;\n+\t  s_ann->makes_aliased_stores = clobbered_aliased_stores;\n+\t}\n+      return;\n+    }\n+\n+  memset (&empty_ann, 0, sizeof (struct stmt_ann_d));\n+\n+  /* Add a V_MAY_DEF operand for every call clobbered variable.  */\n+  EXECUTE_IF_SET_IN_BITMAP (call_clobbered_vars, 0, i, bi)\n+    {\n+      tree var = referenced_var (i);\n+      if (TREE_READONLY (var)\n+\t  && (TREE_STATIC (var) || DECL_EXTERNAL (var)))\n+\tadd_stmt_operand (&var, &empty_ann, opf_none);\n+      else\n+\tadd_stmt_operand (&var, &empty_ann, opf_is_def);\n+    }\n+\n+  clobbered_aliased_loads = empty_ann.makes_aliased_loads;\n+  clobbered_aliased_stores = empty_ann.makes_aliased_stores;\n+\n+  /* Set the flags for a stmt's annotation.  */\n+  if (s_ann)\n+    {\n+      s_ann->makes_aliased_loads = empty_ann.makes_aliased_loads;\n+      s_ann->makes_aliased_stores = empty_ann.makes_aliased_stores;\n+    }\n+\n+  /* Perpare empty cache vectors.  */\n+  if (clobbered_v_may_defs)\n+    {\n+      VARRAY_POP_ALL (clobbered_vuses);\n+      VARRAY_POP_ALL (clobbered_v_may_defs);\n     }\n+  else\n+    {\n+      VARRAY_TREE_INIT (clobbered_v_may_defs, 10, \"clobbered_v_may_defs\");\n+      VARRAY_TREE_INIT (clobbered_vuses, 10, \"clobbered_vuses\");\n+    }\n+\n+  /* Now fill the clobbered cache with the values that have been found.  */\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (build_vuses); i++)\n+    VARRAY_PUSH_TREE (clobbered_vuses, VARRAY_TREE (build_vuses, i));\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (build_v_may_defs); i++)\n+    VARRAY_PUSH_TREE (clobbered_v_may_defs, VARRAY_TREE (build_v_may_defs, i));\n+\n+  ssa_call_clobbered_cache_valid = true;\n }\n \n \n@@ -1621,24 +1737,60 @@ add_call_clobber_ops (tree stmt)\n static void\n add_call_read_ops (tree stmt)\n {\n+  unsigned i;\n+  tree t;\n   bitmap_iterator bi;\n+  stmt_ann_t s_ann = stmt_ann (stmt);\n+  struct stmt_ann_d empty_ann;\n \n-  /* Otherwise, if the function is not pure, it may reference memory.  Add\n-     a VUSE for .GLOBAL_VAR if it has been created.  Otherwise, add a VUSE\n-     for each call-clobbered variable.  See add_referenced_var for the\n-     heuristic used to decide whether to create .GLOBAL_VAR.  */\n+  /* if the function is not pure, it may reference memory.  Add\n+     a VUSE for .GLOBAL_VAR if it has been created.  See add_referenced_var\n+     for the heuristic used to decide whether to create .GLOBAL_VAR.  */\n   if (global_var)\n-    add_stmt_operand (&global_var, stmt, opf_none);\n-  else\n     {\n-      unsigned i;\n-      \n-      EXECUTE_IF_SET_IN_BITMAP (call_clobbered_vars, 0, i, bi)\n+      add_stmt_operand (&global_var, s_ann, opf_none);\n+      return;\n+    }\n+  \n+  /* If cache is valid, copy the elements into the build vector.  */\n+  if (ssa_ro_call_cache_valid)\n+    {\n+      for (i = 0; i < VARRAY_ACTIVE_SIZE (ro_call_vuses); i++)\n \t{\n-\t  tree var = referenced_var (i);\n-\t  add_stmt_operand (&var, stmt, opf_none);\n+\t  t = VARRAY_TREE (ro_call_vuses, i);\n+\t  gcc_assert (TREE_CODE (t) != SSA_NAME);\n+\t  var_ann (t)->in_vuse_list = 1;\n+\t  VARRAY_PUSH_TREE (build_vuses, t);\n \t}\n+      if (s_ann)\n+\ts_ann->makes_aliased_loads = ro_call_aliased_loads;\n+      return;\n+    }\n+\n+  memset (&empty_ann, 0, sizeof (struct stmt_ann_d));\n+\n+  /* Add a VUSE for each call-clobbered variable.  */\n+  EXECUTE_IF_SET_IN_BITMAP (call_clobbered_vars, 0, i, bi)\n+    {\n+      tree var = referenced_var (i);\n+      add_stmt_operand (&var, &empty_ann, opf_none);\n     }\n+\n+  ro_call_aliased_loads = empty_ann.makes_aliased_loads;\n+  if (s_ann)\n+    s_ann->makes_aliased_loads = empty_ann.makes_aliased_loads;\n+\n+  /* Perpare empty cache vectors.  */\n+  if (ro_call_vuses)\n+    VARRAY_POP_ALL (ro_call_vuses);\n+  else\n+    VARRAY_TREE_INIT (ro_call_vuses, 10, \"ro_call_vuses\");\n+\n+  /* Now fill the clobbered cache with the values that have been found.  */\n+  for (i = 0; i < VARRAY_ACTIVE_SIZE (build_vuses); i++)\n+    VARRAY_PUSH_TREE (ro_call_vuses, VARRAY_TREE (build_vuses, i));\n+\n+  ssa_ro_call_cache_valid = true;\n }\n \n /* Copies virtual operands from SRC to DST.  */"}, {"sha": "c5e6220a20c574323884b1198af614bb9909dee9", "filename": "gcc/tree-ssa-operands.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e288e2f51bed5d45a8a89479bc2f98337bc18c80/gcc%2Ftree-ssa-operands.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e288e2f51bed5d45a8a89479bc2f98337bc18c80/gcc%2Ftree-ssa-operands.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-operands.h?ref=e288e2f51bed5d45a8a89479bc2f98337bc18c80", "patch": "@@ -188,6 +188,8 @@ extern void get_stmt_operands (tree);\n extern void copy_virtual_operands (tree, tree);\n extern void create_ssa_artficial_load_stmt (stmt_operands_p, tree);\n \n+extern bool ssa_call_clobbered_cache_valid;\n+extern bool ssa_ro_call_cache_valid;\n \n /* This structure is used in the operand iterator loops.  It contains the \n    items required to determine which operand is retrieved next.  During"}]}