{"sha": "2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MmJjY2ViNmZjNTlmY2RhZjUxMDA2ZDRmY2ZjNzFjMmQyNjc2MTM5Ng==", "commit": {"author": {"name": "Matthias Kretz", "email": "kretz@kde.org", "date": "2021-01-21T11:45:15Z"}, "committer": {"name": "Jonathan Wakely", "email": "jwakely@redhat.com", "date": "2021-01-27T16:37:26Z"}, "message": "libstdc++: Add std::experimental::simd from the Parallelism TS 2\n\nAdds <experimental/simd>.\n\nThis implements the simd and simd_mask class templates via\n[[gnu::vector_size(N)]] data members. It implements overloads for all of\n<cmath> for simd. Explicit vectorization of the <cmath> functions is not\nfinished.\n\nThe majority of functions are marked as [[gnu::always_inline]] to enable\nquasi-ODR-conforming linking of TUs with different -m flags.\nPerformance optimization was done for x86_64.  ARM, Aarch64, and POWER\nrely on the compiler to recognize reduction, conversion, and shuffle\npatterns.\n\nBesides verification using many different machine flages, the code was\nalso verified with different fast-math flags.\n\nlibstdc++-v3/ChangeLog:\n\n\t* doc/xml/manual/status_cxx2017.xml: Add implementation status\n\tof the Parallelism TS 2. Document implementation-defined types\n\tand behavior.\n\t* include/Makefile.am: Add new headers.\n\t* include/Makefile.in: Regenerate.\n\t* include/experimental/simd: New file. New header for\n\tParallelism TS 2.\n\t* include/experimental/bits/numeric_traits.h: New file.\n\tImplementation of P1841R1 using internal naming. Addition of\n\tmissing IEC559 functionality query.\n\t* include/experimental/bits/simd.h: New file. Definition of the\n\tpublic simd interfaces and general implementation helpers.\n\t* include/experimental/bits/simd_builtin.h: New file.\n\tImplementation of the _VecBuiltin simd_abi.\n\t* include/experimental/bits/simd_converter.h: New file. Generic\n\tsimd conversions.\n\t* include/experimental/bits/simd_detail.h: New file. Internal\n\tmacros for the simd implementation.\n\t* include/experimental/bits/simd_fixed_size.h: New file. Simd\n\tfixed_size ABI specific implementations.\n\t* include/experimental/bits/simd_math.h: New file. Math\n\toverloads for simd.\n\t* include/experimental/bits/simd_neon.h: New file. Simd NEON\n\tspecific implementations.\n\t* include/experimental/bits/simd_ppc.h: New file. Implement bit\n\tshifts to avoid invalid results for integral types smaller than\n\tint.\n\t* include/experimental/bits/simd_scalar.h: New file. Simd scalar\n\tABI specific implementations.\n\t* include/experimental/bits/simd_x86.h: New file. Simd x86\n\tspecific implementations.\n\t* include/experimental/bits/simd_x86_conversions.h: New file.\n\tx86 specific conversion optimizations. The conversion patterns\n\twork around missing conversion patterns in the compiler and\n\tshould be removed as soon as PR85048 is resolved.\n\t* testsuite/experimental/simd/standard_abi_usable.cc: New file.\n\tTest that all (not all fixed_size<N>, though) standard simd and\n\tsimd_mask types are usable.\n\t* testsuite/experimental/simd/standard_abi_usable_2.cc: New\n\tfile. As above but with -ffast-math.\n\t* testsuite/libstdc++-dg/conformance.exp: Don't build simd tests\n\tfrom the standard test loop. Instead use\n\tcheck_vect_support_and_set_flags to build simd tests with the\n\trelevant machine flags.", "tree": {"sha": "5eb5d4554985a44e2cc30a4284e432990b61cf97", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/5eb5d4554985a44e2cc30a4284e432990b61cf97"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/comments", "author": {"login": "mattkretz", "id": 3306474, "node_id": "MDQ6VXNlcjMzMDY0NzQ=", "avatar_url": "https://avatars.githubusercontent.com/u/3306474?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mattkretz", "html_url": "https://github.com/mattkretz", "followers_url": "https://api.github.com/users/mattkretz/followers", "following_url": "https://api.github.com/users/mattkretz/following{/other_user}", "gists_url": "https://api.github.com/users/mattkretz/gists{/gist_id}", "starred_url": "https://api.github.com/users/mattkretz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mattkretz/subscriptions", "organizations_url": "https://api.github.com/users/mattkretz/orgs", "repos_url": "https://api.github.com/users/mattkretz/repos", "events_url": "https://api.github.com/users/mattkretz/events{/privacy}", "received_events_url": "https://api.github.com/users/mattkretz/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jwakely", "id": 1254480, "node_id": "MDQ6VXNlcjEyNTQ0ODA=", "avatar_url": "https://avatars.githubusercontent.com/u/1254480?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwakely", "html_url": "https://github.com/jwakely", "followers_url": "https://api.github.com/users/jwakely/followers", "following_url": "https://api.github.com/users/jwakely/following{/other_user}", "gists_url": "https://api.github.com/users/jwakely/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwakely/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwakely/subscriptions", "organizations_url": "https://api.github.com/users/jwakely/orgs", "repos_url": "https://api.github.com/users/jwakely/repos", "events_url": "https://api.github.com/users/jwakely/events{/privacy}", "received_events_url": "https://api.github.com/users/jwakely/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "c91db798ec65b3e55f2380ca1530ecb71544f1bb", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c91db798ec65b3e55f2380ca1530ecb71544f1bb", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c91db798ec65b3e55f2380ca1530ecb71544f1bb"}], "stats": {"total": 21803, "additions": 21802, "deletions": 1}, "files": [{"sha": "bc740f8e1baf80da62401a73efa93c8965f9dba9", "filename": "libstdc++-v3/doc/xml/manual/status_cxx2017.xml", "status": "modified", "additions": 216, "deletions": 0, "changes": 216, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Fdoc%2Fxml%2Fmanual%2Fstatus_cxx2017.xml", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Fdoc%2Fxml%2Fmanual%2Fstatus_cxx2017.xml", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Fdoc%2Fxml%2Fmanual%2Fstatus_cxx2017.xml?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -2869,6 +2869,17 @@ since C++14 and the implementation is complete.\n       <entry>Library Fundamentals 2 TS</entry>\n     </row>\n \n+    <row>\n+      <entry>\n+\t<link xmlns:xlink=\"http://www.w3.org/1999/xlink\" xlink:href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2018/p0214r9.pdf\">\n+\t  P0214R9\n+\t</link>\n+      </entry>\n+      <entry>Data-Parallel Types</entry>\n+      <entry>Y</entry>\n+      <entry>Parallelism 2 TS</entry>\n+    </row>\n+\n   </tbody>\n </tgroup>\n </table>\n@@ -3014,6 +3025,211 @@ since C++14 and the implementation is complete.\n       If <code>!is_regular_file(p)</code>, an error is reported.\n    </para>\n \n+   <section xml:id=\"iso.2017.par2ts\" xreflabel=\"Implementation Specific Behavior of the Parallelism 2 TS\"><info><title>Parallelism 2 TS</title></info>\n+\n+     <para>\n+        <emphasis>9.3 [parallel.simd.abi]</emphasis>\n+        <code>max_fixed_size&lt;T&gt;</code> is 32, except when targetting\n+        AVX512BW and <code>sizeof(T)</code> is 1.\n+     </para>\n+\n+     <para>\n+        When targeting 32-bit x86,\n+        <classname>simd_abi::compatible&lt;T&gt;</classname> is an alias for\n+        <classname>simd_abi::scalar</classname>.\n+        When targeting 64-bit x86 (including x32) or Aarch64,\n+        <classname>simd_abi::compatible&lt;T&gt;</classname> is an alias for\n+        <classname>simd_abi::_VecBuiltin&lt;16&gt;</classname>,\n+        unless <code>T</code> is <code>long double</code>, in which case it is\n+        an alias for <classname>simd_abi::scalar</classname>.\n+        When targeting ARM (but not Aarch64) with NEON support,\n+        <classname>simd_abi::compatible&lt;T&gt;</classname> is an alias for\n+        <classname>simd_abi::_VecBuiltin&lt;16&gt;</classname>,\n+        unless <code>sizeof(T) &gt; 4</code>, in which case it is\n+        an alias for <classname>simd_abi::scalar</classname>. Additionally,\n+        <classname>simd_abi::compatible&lt;float&gt;</classname> is an alias for\n+        <classname>simd_abi::scalar</classname> unless compiling with\n+        -ffast-math.\n+     </para>\n+\n+     <para>\n+        When targeting x86 (both 32-bit and 64-bit),\n+        <classname>simd_abi::native&lt;T&gt;</classname> is an alias for one of\n+        <classname>simd_abi::scalar</classname>,\n+        <classname>simd_abi::_VecBuiltin&lt;16&gt;</classname>,\n+        <classname>simd_abi::_VecBuiltin&lt;32&gt;</classname>, or\n+        <classname>simd_abi::_VecBltnBtmsk&lt;64&gt;</classname>, depending on\n+        <code>T</code> and the machine options the compiler was invoked with.\n+     </para>\n+\n+     <para>\n+        When targeting ARM/Aarch64 or POWER,\n+        <classname>simd_abi::native&lt;T&gt;</classname> is an alias for\n+        <classname>simd_abi::scalar</classname> or\n+        <classname>simd_abi::_VecBuiltin&lt;16&gt;</classname>, depending on\n+        <code>T</code> and the machine options the compiler was invoked with.\n+     </para>\n+\n+     <para>\n+        For any other targeted machine\n+        <classname>simd_abi::compatible&lt;T&gt;</classname> and\n+        <classname>simd_abi::native&lt;T&gt;</classname> are aliases for\n+        <classname>simd_abi::scalar</classname>. (subject to change)\n+     </para>\n+\n+     <para>\n+        The extended ABI tag types defined in the\n+        <code>std::experimental::parallelism_v2::simd_abi</code> namespace are:\n+        <classname>simd_abi::_VecBuiltin&lt;Bytes&gt;</classname>, and\n+        <classname>simd_abi::_VecBltnBtmsk&lt;Bytes&gt;</classname>.\n+     </para>\n+\n+     <para>\n+        <classname>simd_abi::deduce&lt;T, N, Abis...&gt;::type</classname>,\n+        with <code>N &gt; 1</code> is an alias for an extended ABI tag, if a\n+        supported extended ABI tag exists. Otherwise it is an alias for\n+        <classname>simd_abi::fixed_size&lt;N&gt;</classname>. The <classname>\n+        simd_abi::_VecBltnBtmsk</classname> ABI tag is preferred over\n+        <classname>simd_abi::_VecBuiltin</classname>.\n+     </para>\n+\n+     <para>\n+        <emphasis>9.4 [parallel.simd.traits]</emphasis>\n+        <classname>memory_alignment&lt;T, U&gt;::value</classname> is\n+        <code>sizeof(U) * T::size()</code> rounded up to the next power-of-two\n+        value.\n+     </para>\n+\n+     <para>\n+        <emphasis>9.6.1 [parallel.simd.overview]</emphasis>\n+        On ARM, <classname>simd&lt;T, _VecBuiltin&lt;Bytes&gt;&gt;</classname>\n+        is supported if <code>__ARM_NEON</code> is defined and\n+        <code>sizeof(T) &lt;= 4</code>. Additionally,\n+        <code>sizeof(T) == 8</code> with integral <code>T</code> is supported if\n+        <code>__ARM_ARCH &gt;= 8</code>, and <code>double</code> is supported if\n+        <code>__aarch64__</code> is defined.\n+\n+        On POWER, <classname>simd&lt;T, _VecBuiltin&lt;Bytes&gt;&gt;</classname>\n+        is supported if <code>__ALTIVEC__</code> is defined and <code>sizeof(T)\n+        &lt; 8</code>. Additionally, <code>double</code> is supported if\n+        <code>__VSX__</code> is defined, and any <code>T</code> with <code>\n+        sizeof(T) &le; 8</code> is supported if <code>__POWER8_VECTOR__</code>\n+        is defined.\n+\n+        On x86, given an extended ABI tag <code>Abi</code>,\n+        <classname>simd&lt;T, Abi&gt;</classname> is supported according to the\n+        following table:\n+        <table frame=\"all\" xml:id=\"table.par2ts_simd_support\">\n+          <title>Support for Extended ABI Tags</title>\n+\n+          <tgroup cols=\"4\" align=\"left\" colsep=\"0\" rowsep=\"1\">\n+          <colspec colname=\"c1\"/>\n+          <colspec colname=\"c2\"/>\n+          <colspec colname=\"c3\"/>\n+          <colspec colname=\"c4\"/>\n+            <thead>\n+              <row>\n+                <entry>ABI tag <code>Abi</code></entry>\n+                <entry>value type <code>T</code></entry>\n+                <entry>values for <code>Bytes</code></entry>\n+                <entry>required machine option</entry>\n+              </row>\n+            </thead>\n+\n+            <tbody>\n+              <row>\n+                <entry morerows=\"5\">\n+                  <classname>_VecBuiltin&lt;Bytes&gt;</classname>\n+                </entry>\n+                <entry morerows=\"1\"><code>float</code></entry>\n+                <entry>8, 12, 16</entry>\n+                <entry>\"-msse\"</entry>\n+              </row>\n+\n+              <row>\n+                <entry>20, 24, 28, 32</entry>\n+                <entry>\"-mavx\"</entry>\n+              </row>\n+\n+              <row>\n+                <entry morerows=\"1\"><code>double</code></entry>\n+                <entry>16</entry>\n+                <entry>\"-msse2\"</entry>\n+              </row>\n+\n+              <row>\n+                <entry>24, 32</entry>\n+                <entry>\"-mavx\"</entry>\n+              </row>\n+\n+              <row>\n+                <entry morerows=\"1\">\n+                  integral types other than <code>bool</code>\n+                </entry>\n+                <entry>\n+                  <code>Bytes</code> \u2264 16 and <code>Bytes</code> divisible by\n+                  <code>sizeof(T)</code>\n+                </entry>\n+                <entry>\"-msse2\"</entry>\n+              </row>\n+\n+              <row>\n+                <entry>\n+                  16 &lt; <code>Bytes</code> \u2264 32 and <code>Bytes</code>\n+                  divisible by <code>sizeof(T)</code>\n+                </entry>\n+                <entry>\"-mavx2\"</entry>\n+              </row>\n+\n+              <row>\n+                <entry morerows=\"1\">\n+                  <classname>_VecBuiltin&lt;Bytes&gt;</classname> and\n+                  <classname>_VecBltnBtmsk&lt;Bytes&gt;</classname>\n+                </entry>\n+                <entry>\n+                  vectorizable types with <code>sizeof(T)</code> \u2265 4\n+                </entry>\n+                <entry morerows=\"1\">\n+                  32 &lt; <code>Bytes</code> \u2264 64 and <code>Bytes</code>\n+                  divisible by <code>sizeof(T)</code>\n+                </entry>\n+                <entry>\"-mavx512f\"</entry>\n+              </row>\n+\n+              <row>\n+                <entry>\n+                  vectorizable types with <code>sizeof(T)</code> &lt; 4\n+                </entry>\n+                <entry>\"-mavx512bw\"</entry>\n+              </row>\n+\n+              <row>\n+                <entry morerows=\"1\">\n+                  <classname>_VecBltnBtmsk&lt;Bytes&gt;</classname>\n+                </entry>\n+                <entry>\n+                  vectorizable types with <code>sizeof(T)</code> \u2265 4\n+                </entry>\n+                <entry morerows=\"1\">\n+                  <code>Bytes</code> \u2264 32 and <code>Bytes</code> divisible by\n+                  <code>sizeof(T)</code>\n+                </entry>\n+                <entry>\"-mavx512vl\"</entry>\n+              </row>\n+\n+              <row>\n+                <entry>\n+                  vectorizable types with <code>sizeof(T)</code> &lt; 4\n+                </entry>\n+                <entry>\"-mavx512bw\" and \"-mavx512vl\"</entry>\n+              </row>\n+\n+            </tbody>\n+          </tgroup>\n+        </table>\n+     </para>\n+\n+   </section>\n \n </section>\n "}, {"sha": "f24a5489e8e426bdd4779b1a10c836d08e51d876", "filename": "libstdc++-v3/include/Makefile.am", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2FMakefile.am", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2FMakefile.am", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2FMakefile.am?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -747,6 +747,7 @@ experimental_headers = \\\n \t${experimental_srcdir}/ratio \\\n \t${experimental_srcdir}/regex \\\n \t${experimental_srcdir}/set \\\n+\t${experimental_srcdir}/simd \\\n \t${experimental_srcdir}/socket \\\n \t${experimental_srcdir}/source_location \\\n \t${experimental_srcdir}/string \\\n@@ -766,7 +767,19 @@ experimental_bits_builddir = ./experimental/bits\n experimental_bits_headers = \\\n \t${experimental_bits_srcdir}/lfts_config.h \\\n \t${experimental_bits_srcdir}/net.h \\\n+\t${experimental_bits_srcdir}/numeric_traits.h \\\n \t${experimental_bits_srcdir}/shared_ptr.h \\\n+\t${experimental_bits_srcdir}/simd.h \\\n+\t${experimental_bits_srcdir}/simd_builtin.h \\\n+\t${experimental_bits_srcdir}/simd_converter.h \\\n+\t${experimental_bits_srcdir}/simd_detail.h \\\n+\t${experimental_bits_srcdir}/simd_fixed_size.h \\\n+\t${experimental_bits_srcdir}/simd_math.h \\\n+\t${experimental_bits_srcdir}/simd_neon.h \\\n+\t${experimental_bits_srcdir}/simd_ppc.h \\\n+\t${experimental_bits_srcdir}/simd_scalar.h \\\n+\t${experimental_bits_srcdir}/simd_x86.h \\\n+\t${experimental_bits_srcdir}/simd_x86_conversions.h \\\n \t${experimental_bits_srcdir}/string_view.tcc \\\n \t${experimental_bits_filesystem_headers}\n "}, {"sha": "12c634007064f3e99fad87ac25e2dd2f5c48a701", "filename": "libstdc++-v3/include/Makefile.in", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2FMakefile.in?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -1097,6 +1097,7 @@ experimental_headers = \\\n \t${experimental_srcdir}/ratio \\\n \t${experimental_srcdir}/regex \\\n \t${experimental_srcdir}/set \\\n+\t${experimental_srcdir}/simd \\\n \t${experimental_srcdir}/socket \\\n \t${experimental_srcdir}/source_location \\\n \t${experimental_srcdir}/string \\\n@@ -1116,7 +1117,19 @@ experimental_bits_builddir = ./experimental/bits\n experimental_bits_headers = \\\n \t${experimental_bits_srcdir}/lfts_config.h \\\n \t${experimental_bits_srcdir}/net.h \\\n+\t${experimental_bits_srcdir}/numeric_traits.h \\\n \t${experimental_bits_srcdir}/shared_ptr.h \\\n+\t${experimental_bits_srcdir}/simd.h \\\n+\t${experimental_bits_srcdir}/simd_builtin.h \\\n+\t${experimental_bits_srcdir}/simd_converter.h \\\n+\t${experimental_bits_srcdir}/simd_detail.h \\\n+\t${experimental_bits_srcdir}/simd_fixed_size.h \\\n+\t${experimental_bits_srcdir}/simd_math.h \\\n+\t${experimental_bits_srcdir}/simd_neon.h \\\n+\t${experimental_bits_srcdir}/simd_ppc.h \\\n+\t${experimental_bits_srcdir}/simd_scalar.h \\\n+\t${experimental_bits_srcdir}/simd_x86.h \\\n+\t${experimental_bits_srcdir}/simd_x86_conversions.h \\\n \t${experimental_bits_srcdir}/string_view.tcc \\\n \t${experimental_bits_filesystem_headers}\n "}, {"sha": "1b60874b788fa4257dfb76f254dcb5d4047351b3", "filename": "libstdc++-v3/include/experimental/bits/numeric_traits.h", "status": "added", "additions": 567, "deletions": 0, "changes": 567, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fnumeric_traits.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fnumeric_traits.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fnumeric_traits.h?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -0,0 +1,567 @@\n+// Definition of numeric_limits replacement traits P1841R1 -*- C++ -*-\n+\n+// Copyright (C) 2020 Free Software Foundation, Inc.\n+//\n+// This file is part of the GNU ISO C++ Library.  This library is free\n+// software; you can redistribute it and/or modify it under the\n+// terms of the GNU General Public License as published by the\n+// Free Software Foundation; either version 3, or (at your option)\n+// any later version.\n+\n+// This library is distributed in the hope that it will be useful,\n+// but WITHOUT ANY WARRANTY; without even the implied warranty of\n+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+// GNU General Public License for more details.\n+\n+// Under Section 7 of GPL version 3, you are granted additional\n+// permissions described in the GCC Runtime Library Exception, version\n+// 3.1, as published by the Free Software Foundation.\n+\n+// You should have received a copy of the GNU General Public License and\n+// a copy of the GCC Runtime Library Exception along with this program;\n+// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+// <http://www.gnu.org/licenses/>.\n+\n+#include <type_traits>\n+\n+namespace std {\n+\n+template <template <typename> class _Trait, typename _Tp, typename = void>\n+  struct __value_exists_impl : false_type {};\n+\n+template <template <typename> class _Trait, typename _Tp>\n+  struct __value_exists_impl<_Trait, _Tp, void_t<decltype(_Trait<_Tp>::value)>>\n+  : true_type {};\n+\n+template <typename _Tp, bool = is_arithmetic_v<_Tp>>\n+  struct __digits_impl {};\n+\n+template <typename _Tp>\n+  struct __digits_impl<_Tp, true>\n+  {\n+    static inline constexpr int value\n+      = sizeof(_Tp) * __CHAR_BIT__ - is_signed_v<_Tp>;\n+  };\n+\n+template <>\n+  struct __digits_impl<float, true>\n+  { static inline constexpr int value = __FLT_MANT_DIG__; };\n+\n+template <>\n+  struct __digits_impl<double, true>\n+  { static inline constexpr int value = __DBL_MANT_DIG__; };\n+\n+template <>\n+  struct __digits_impl<long double, true>\n+  { static inline constexpr int value = __LDBL_MANT_DIG__; };\n+\n+template <typename _Tp, bool = is_arithmetic_v<_Tp>>\n+  struct __digits10_impl {};\n+\n+template <typename _Tp>\n+  struct __digits10_impl<_Tp, true>\n+  {\n+    // The fraction 643/2136 approximates log10(2) to 7 significant digits.\n+    static inline constexpr int value = __digits_impl<_Tp>::value * 643L / 2136;\n+  };\n+\n+template <>\n+  struct __digits10_impl<float, true>\n+  { static inline constexpr int value = __FLT_DIG__; };\n+\n+template <>\n+  struct __digits10_impl<double, true>\n+  { static inline constexpr int value = __DBL_DIG__; };\n+\n+template <>\n+  struct __digits10_impl<long double, true>\n+  { static inline constexpr int value = __LDBL_DIG__; };\n+\n+template <typename _Tp, bool = is_arithmetic_v<_Tp>>\n+  struct __max_digits10_impl {};\n+\n+template <typename _Tp>\n+  struct __max_digits10_impl<_Tp, true>\n+  {\n+    static inline constexpr int value\n+      = is_floating_point_v<_Tp> ? 2 + __digits_impl<_Tp>::value * 643L / 2136\n+\t\t\t\t : __digits10_impl<_Tp>::value + 1;\n+  };\n+\n+template <typename _Tp>\n+  struct __max_exponent_impl {};\n+\n+template <>\n+  struct __max_exponent_impl<float>\n+  { static inline constexpr int value = __FLT_MAX_EXP__; };\n+\n+template <>\n+  struct __max_exponent_impl<double>\n+  { static inline constexpr int value = __DBL_MAX_EXP__; };\n+\n+template <>\n+  struct __max_exponent_impl<long double>\n+  { static inline constexpr int value = __LDBL_MAX_EXP__; };\n+\n+template <typename _Tp>\n+  struct __max_exponent10_impl {};\n+\n+template <>\n+  struct __max_exponent10_impl<float>\n+  { static inline constexpr int value = __FLT_MAX_10_EXP__; };\n+\n+template <>\n+  struct __max_exponent10_impl<double>\n+  { static inline constexpr int value = __DBL_MAX_10_EXP__; };\n+\n+template <>\n+  struct __max_exponent10_impl<long double>\n+  { static inline constexpr int value = __LDBL_MAX_10_EXP__; };\n+\n+template <typename _Tp>\n+  struct __min_exponent_impl {};\n+\n+template <>\n+  struct __min_exponent_impl<float>\n+  { static inline constexpr int value = __FLT_MIN_EXP__; };\n+\n+template <>\n+  struct __min_exponent_impl<double>\n+  { static inline constexpr int value = __DBL_MIN_EXP__; };\n+\n+template <>\n+  struct __min_exponent_impl<long double>\n+  { static inline constexpr int value = __LDBL_MIN_EXP__; };\n+\n+template <typename _Tp>\n+  struct __min_exponent10_impl {};\n+\n+template <>\n+  struct __min_exponent10_impl<float>\n+  { static inline constexpr int value = __FLT_MIN_10_EXP__; };\n+\n+template <>\n+  struct __min_exponent10_impl<double>\n+  { static inline constexpr int value = __DBL_MIN_10_EXP__; };\n+\n+template <>\n+  struct __min_exponent10_impl<long double>\n+  { static inline constexpr int value = __LDBL_MIN_10_EXP__; };\n+\n+template <typename _Tp, bool = is_arithmetic_v<_Tp>>\n+  struct __radix_impl {};\n+\n+template <typename _Tp>\n+  struct __radix_impl<_Tp, true>\n+  {\n+    static inline constexpr int value\n+      = is_floating_point_v<_Tp> ? __FLT_RADIX__ : 2;\n+  };\n+\n+// [num.traits.util], numeric utility traits\n+template <template <typename> class _Trait, typename _Tp>\n+  struct __value_exists : __value_exists_impl<_Trait, _Tp> {};\n+\n+template <template <typename> class _Trait, typename _Tp>\n+  inline constexpr bool __value_exists_v = __value_exists<_Trait, _Tp>::value;\n+\n+template <template <typename> class _Trait, typename _Tp, typename _Up = _Tp>\n+  inline constexpr _Up\n+  __value_or(_Up __def = _Up()) noexcept\n+  {\n+    if constexpr (__value_exists_v<_Trait, _Tp>)\n+      return static_cast<_Up>(_Trait<_Tp>::value);\n+    else\n+      return __def;\n+  }\n+\n+template <typename _Tp, bool = is_arithmetic_v<_Tp>>\n+  struct __norm_min_impl {};\n+\n+template <typename _Tp>\n+  struct __norm_min_impl<_Tp, true>\n+  { static inline constexpr _Tp value = 1; };\n+\n+template <>\n+  struct __norm_min_impl<float, true>\n+  { static inline constexpr float value = __FLT_MIN__; };\n+\n+template <>\n+  struct __norm_min_impl<double, true>\n+  { static inline constexpr double value = __DBL_MIN__; };\n+\n+template <>\n+  struct __norm_min_impl<long double, true>\n+  { static inline constexpr long double value = __LDBL_MIN__; };\n+\n+template <typename _Tp>\n+  struct __denorm_min_impl : __norm_min_impl<_Tp> {};\n+\n+#if __FLT_HAS_DENORM__\n+template <>\n+  struct __denorm_min_impl<float>\n+  { static inline constexpr float value = __FLT_DENORM_MIN__; };\n+#endif\n+\n+#if __DBL_HAS_DENORM__\n+template <>\n+  struct __denorm_min_impl<double>\n+  { static inline constexpr double value = __DBL_DENORM_MIN__; };\n+#endif\n+\n+#if __LDBL_HAS_DENORM__\n+template <>\n+  struct __denorm_min_impl<long double>\n+  { static inline constexpr long double value = __LDBL_DENORM_MIN__; };\n+#endif\n+\n+template <typename _Tp>\n+  struct __epsilon_impl {};\n+\n+template <>\n+  struct __epsilon_impl<float>\n+  { static inline constexpr float value = __FLT_EPSILON__; };\n+\n+template <>\n+  struct __epsilon_impl<double>\n+  { static inline constexpr double value = __DBL_EPSILON__; };\n+\n+template <>\n+  struct __epsilon_impl<long double>\n+  { static inline constexpr long double value = __LDBL_EPSILON__; };\n+\n+template <typename _Tp, bool = is_arithmetic_v<_Tp>>\n+  struct __finite_min_impl {};\n+\n+template <typename _Tp>\n+  struct __finite_min_impl<_Tp, true>\n+  {\n+    static inline constexpr _Tp value\n+      = is_unsigned_v<_Tp> ? _Tp()\n+\t\t\t   : -2 * (_Tp(1) << __digits_impl<_Tp>::value - 1);\n+  };\n+\n+template <>\n+  struct __finite_min_impl<float, true>\n+  { static inline constexpr float value = -__FLT_MAX__; };\n+\n+template <>\n+  struct __finite_min_impl<double, true>\n+  { static inline constexpr double value = -__DBL_MAX__; };\n+\n+template <>\n+  struct __finite_min_impl<long double, true>\n+  { static inline constexpr long double value = -__LDBL_MAX__; };\n+\n+template <typename _Tp, bool = is_arithmetic_v<_Tp>>\n+  struct __finite_max_impl {};\n+\n+template <typename _Tp>\n+  struct __finite_max_impl<_Tp, true>\n+  { static inline constexpr _Tp value = ~__finite_min_impl<_Tp>::value; };\n+\n+template <>\n+  struct __finite_max_impl<float, true>\n+  { static inline constexpr float value = __FLT_MAX__; };\n+\n+template <>\n+  struct __finite_max_impl<double, true>\n+  { static inline constexpr double value = __DBL_MAX__; };\n+\n+template <>\n+  struct __finite_max_impl<long double, true>\n+  { static inline constexpr long double value = __LDBL_MAX__; };\n+\n+template <typename _Tp>\n+  struct __infinity_impl {};\n+\n+#if __FLT_HAS_INFINITY__\n+template <>\n+  struct __infinity_impl<float>\n+  { static inline constexpr float value = __builtin_inff(); };\n+#endif\n+\n+#if __DBL_HAS_INFINITY__\n+template <>\n+  struct __infinity_impl<double>\n+  { static inline constexpr double value = __builtin_inf(); };\n+#endif\n+\n+#if __LDBL_HAS_INFINITY__\n+template <>\n+  struct __infinity_impl<long double>\n+  { static inline constexpr long double value = __builtin_infl(); };\n+#endif\n+\n+template <typename _Tp>\n+  struct __quiet_NaN_impl {};\n+\n+#if __FLT_HAS_QUIET_NAN__\n+template <>\n+  struct __quiet_NaN_impl<float>\n+  { static inline constexpr float value = __builtin_nanf(\"\"); };\n+#endif\n+\n+#if __DBL_HAS_QUIET_NAN__\n+template <>\n+  struct __quiet_NaN_impl<double>\n+  { static inline constexpr double value = __builtin_nan(\"\"); };\n+#endif\n+\n+#if __LDBL_HAS_QUIET_NAN__\n+template <>\n+  struct __quiet_NaN_impl<long double>\n+  { static inline constexpr long double value = __builtin_nanl(\"\"); };\n+#endif\n+\n+template <typename _Tp, bool = is_floating_point_v<_Tp>>\n+  struct __reciprocal_overflow_threshold_impl {};\n+\n+template <typename _Tp>\n+  struct __reciprocal_overflow_threshold_impl<_Tp, true>\n+  {\n+    // This typically yields a subnormal value. Is this incorrect for\n+    // flush-to-zero configurations?\n+    static constexpr _Tp _S_search(_Tp __ok, _Tp __overflows)\n+    {\n+      const _Tp __mid = (__ok + __overflows) / 2;\n+      // 1/__mid without -ffast-math is not a constant expression if it\n+      // overflows. Therefore divide 1 by the radix before division.\n+      // Consequently finite_max (the threshold) must be scaled by the\n+      // same value.\n+      if (__mid == __ok || __mid == __overflows)\n+\treturn __ok;\n+      else if (_Tp(1) / (__radix_impl<_Tp>::value * __mid)\n+\t       <= __finite_max_impl<_Tp>::value / __radix_impl<_Tp>::value)\n+\treturn _S_search(__mid, __overflows);\n+      else\n+\treturn _S_search(__ok, __mid);\n+    }\n+\n+    static inline constexpr _Tp value\n+      = _S_search(_Tp(1.01) / __finite_max_impl<_Tp>::value,\n+\t\t  _Tp(0.99) / __finite_max_impl<_Tp>::value);\n+  };\n+\n+template <typename _Tp, bool = is_floating_point_v<_Tp>>\n+  struct __round_error_impl {};\n+\n+template <typename _Tp>\n+  struct __round_error_impl<_Tp, true>\n+  { static inline constexpr _Tp value = 0.5; };\n+\n+template <typename _Tp>\n+  struct __signaling_NaN_impl {};\n+\n+#if __FLT_HAS_QUIET_NAN__\n+template <>\n+  struct __signaling_NaN_impl<float>\n+  { static inline constexpr float value = __builtin_nansf(\"\"); };\n+#endif\n+\n+#if __DBL_HAS_QUIET_NAN__\n+template <>\n+  struct __signaling_NaN_impl<double>\n+  { static inline constexpr double value = __builtin_nans(\"\"); };\n+#endif\n+\n+#if __LDBL_HAS_QUIET_NAN__\n+template <>\n+  struct __signaling_NaN_impl<long double>\n+  { static inline constexpr long double value = __builtin_nansl(\"\"); };\n+#endif\n+\n+// [num.traits.val], numeric distinguished value traits\n+template <typename _Tp>\n+  struct __denorm_min : __denorm_min_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __epsilon : __epsilon_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __finite_max : __finite_max_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __finite_min : __finite_min_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __infinity : __infinity_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __norm_min : __norm_min_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __quiet_NaN : __quiet_NaN_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __reciprocal_overflow_threshold\n+  : __reciprocal_overflow_threshold_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __round_error : __round_error_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __signaling_NaN : __signaling_NaN_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  inline constexpr auto __denorm_min_v = __denorm_min<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __epsilon_v = __epsilon<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __finite_max_v = __finite_max<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __finite_min_v = __finite_min<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __infinity_v = __infinity<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __norm_min_v = __norm_min<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __quiet_NaN_v = __quiet_NaN<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __reciprocal_overflow_threshold_v\n+    = __reciprocal_overflow_threshold<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __round_error_v = __round_error<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __signaling_NaN_v = __signaling_NaN<_Tp>::value;\n+\n+// [num.traits.char], numeric characteristics traits\n+template <typename _Tp>\n+  struct __digits : __digits_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __digits10 : __digits10_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __max_digits10 : __max_digits10_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __max_exponent : __max_exponent_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __max_exponent10 : __max_exponent10_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __min_exponent : __min_exponent_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __min_exponent10 : __min_exponent10_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  struct __radix : __radix_impl<remove_cv_t<_Tp>> {};\n+\n+template <typename _Tp>\n+  inline constexpr auto __digits_v = __digits<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __digits10_v = __digits10<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __max_digits10_v = __max_digits10<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __max_exponent_v = __max_exponent<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __max_exponent10_v = __max_exponent10<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __min_exponent_v = __min_exponent<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __min_exponent10_v = __min_exponent10<_Tp>::value;\n+\n+template <typename _Tp>\n+  inline constexpr auto __radix_v = __radix<_Tp>::value;\n+\n+// mkretz's extensions\n+// TODO: does GCC tell me? __GCC_IEC_559 >= 2 is not the right answer\n+template <typename _Tp>\n+  struct __has_iec559_storage_format : true_type {};\n+\n+template <typename _Tp>\n+  inline constexpr bool __has_iec559_storage_format_v\n+    = __has_iec559_storage_format<_Tp>::value;\n+\n+/* To propose:\n+   If __has_iec559_behavior<__quiet_NaN, T> is true the following holds:\n+     - nan == nan is false\n+     - isnan(nan) is true\n+     - isnan(nan + x) is true\n+     - isnan(inf/inf) is true\n+     - isnan(0/0) is true\n+     - isunordered(nan, x) is true\n+\n+   If __has_iec559_behavior<__infinity, T> is true the following holds (x is\n+   neither nan nor inf):\n+     - isinf(inf) is true\n+     - isinf(inf + x) is true\n+     - isinf(1/0) is true\n+ */\n+template <template <typename> class _Trait, typename _Tp>\n+  struct __has_iec559_behavior : false_type {};\n+\n+template <template <typename> class _Trait, typename _Tp>\n+  inline constexpr bool __has_iec559_behavior_v\n+    = __has_iec559_behavior<_Trait, _Tp>::value;\n+\n+#if !__FINITE_MATH_ONLY__\n+#if __FLT_HAS_QUIET_NAN__\n+template <>\n+  struct __has_iec559_behavior<__quiet_NaN, float> : true_type {};\n+#endif\n+\n+#if __DBL_HAS_QUIET_NAN__\n+template <>\n+  struct __has_iec559_behavior<__quiet_NaN, double> : true_type {};\n+#endif\n+\n+#if __LDBL_HAS_QUIET_NAN__\n+template <>\n+  struct __has_iec559_behavior<__quiet_NaN, long double> : true_type {};\n+#endif\n+\n+#if __FLT_HAS_INFINITY__\n+template <>\n+  struct __has_iec559_behavior<__infinity, float> : true_type {};\n+#endif\n+\n+#if __DBL_HAS_INFINITY__\n+template <>\n+  struct __has_iec559_behavior<__infinity, double> : true_type {};\n+#endif\n+\n+#if __LDBL_HAS_INFINITY__\n+template <>\n+  struct __has_iec559_behavior<__infinity, long double> : true_type {};\n+#endif\n+\n+#ifdef __SUPPORT_SNAN__\n+#if __FLT_HAS_QUIET_NAN__\n+template <>\n+  struct __has_iec559_behavior<__signaling_NaN, float> : true_type {};\n+#endif\n+\n+#if __DBL_HAS_QUIET_NAN__\n+template <>\n+  struct __has_iec559_behavior<__signaling_NaN, double> : true_type {};\n+#endif\n+\n+#if __LDBL_HAS_QUIET_NAN__\n+template <>\n+  struct __has_iec559_behavior<__signaling_NaN, long double> : true_type {};\n+#endif\n+\n+#endif\n+#endif // __FINITE_MATH_ONLY__\n+\n+} // namespace std"}, {"sha": "00eec50d64f145390e557f90d4200c4e3c2382b1", "filename": "libstdc++-v3/include/experimental/bits/simd.h", "status": "added", "additions": 5051, "deletions": 0, "changes": 5051, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd.h?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396"}, {"sha": "f2c99faa4ee1fbd9f74377f95bc399a86e3ede86", "filename": "libstdc++-v3/include/experimental/bits/simd_builtin.h", "status": "added", "additions": 2949, "deletions": 0, "changes": 2949, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_builtin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_builtin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_builtin.h?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396"}, {"sha": "dc4598743f999b094df529e3b7f73b50f86482c1", "filename": "libstdc++-v3/include/experimental/bits/simd_converter.h", "status": "added", "additions": 354, "deletions": 0, "changes": 354, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_converter.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_converter.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_converter.h?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -0,0 +1,354 @@\n+// Generic simd conversions -*- C++ -*-\n+\n+// Copyright (C) 2020 Free Software Foundation, Inc.\n+//\n+// This file is part of the GNU ISO C++ Library.  This library is free\n+// software; you can redistribute it and/or modify it under the\n+// terms of the GNU General Public License as published by the\n+// Free Software Foundation; either version 3, or (at your option)\n+// any later version.\n+\n+// This library is distributed in the hope that it will be useful,\n+// but WITHOUT ANY WARRANTY; without even the implied warranty of\n+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+// GNU General Public License for more details.\n+\n+// Under Section 7 of GPL version 3, you are granted additional\n+// permissions described in the GCC Runtime Library Exception, version\n+// 3.1, as published by the Free Software Foundation.\n+\n+// You should have received a copy of the GNU General Public License and\n+// a copy of the GCC Runtime Library Exception along with this program;\n+// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+// <http://www.gnu.org/licenses/>.\n+\n+#ifndef _GLIBCXX_EXPERIMENTAL_SIMD_CONVERTER_H_\n+#define _GLIBCXX_EXPERIMENTAL_SIMD_CONVERTER_H_\n+\n+#if __cplusplus >= 201703L\n+\n+_GLIBCXX_SIMD_BEGIN_NAMESPACE\n+// _SimdConverter scalar -> scalar {{{\n+template <typename _From, typename _To>\n+  struct _SimdConverter<_From, simd_abi::scalar, _To, simd_abi::scalar,\n+\t\t\tenable_if_t<!is_same_v<_From, _To>>>\n+  {\n+    _GLIBCXX_SIMD_INTRINSIC constexpr _To operator()(_From __a) const noexcept\n+    { return static_cast<_To>(__a); }\n+  };\n+\n+// }}}\n+// _SimdConverter scalar -> \"native\" {{{\n+template <typename _From, typename _To, typename _Abi>\n+  struct _SimdConverter<_From, simd_abi::scalar, _To, _Abi,\n+\t\t\tenable_if_t<!is_same_v<_Abi, simd_abi::scalar>>>\n+  {\n+    using _Ret = typename _Abi::template __traits<_To>::_SimdMember;\n+\n+    template <typename... _More>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr _Ret\n+      operator()(_From __a, _More... __more) const noexcept\n+      {\n+\tstatic_assert(sizeof...(_More) + 1 == _Abi::template _S_size<_To>);\n+\tstatic_assert(conjunction_v<is_same<_From, _More>...>);\n+\treturn __make_vector<_To>(__a, __more...);\n+      }\n+  };\n+\n+// }}}\n+// _SimdConverter \"native 1\" -> \"native 2\" {{{\n+template <typename _From, typename _To, typename _AFrom, typename _ATo>\n+  struct _SimdConverter<\n+    _From, _AFrom, _To, _ATo,\n+    enable_if_t<!disjunction_v<\n+      __is_fixed_size_abi<_AFrom>, __is_fixed_size_abi<_ATo>,\n+      is_same<_AFrom, simd_abi::scalar>, is_same<_ATo, simd_abi::scalar>,\n+      conjunction<is_same<_From, _To>, is_same<_AFrom, _ATo>>>>>\n+  {\n+    using _Arg = typename _AFrom::template __traits<_From>::_SimdMember;\n+    using _Ret = typename _ATo::template __traits<_To>::_SimdMember;\n+    using _V = __vector_type_t<_To, simd_size_v<_To, _ATo>>;\n+\n+    template <typename... _More>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr _Ret\n+      operator()(_Arg __a, _More... __more) const noexcept\n+      { return __vector_convert<_V>(__a, __more...); }\n+  };\n+\n+// }}}\n+// _SimdConverter scalar -> fixed_size<1> {{{1\n+template <typename _From, typename _To>\n+  struct _SimdConverter<_From, simd_abi::scalar, _To, simd_abi::fixed_size<1>,\n+\t\t\tvoid>\n+  {\n+    _GLIBCXX_SIMD_INTRINSIC constexpr _SimdTuple<_To, simd_abi::scalar>\n+    operator()(_From __x) const noexcept\n+    { return {static_cast<_To>(__x)}; }\n+  };\n+\n+// _SimdConverter fixed_size<1> -> scalar {{{1\n+template <typename _From, typename _To>\n+  struct _SimdConverter<_From, simd_abi::fixed_size<1>, _To, simd_abi::scalar,\n+\t\t\tvoid>\n+  {\n+    _GLIBCXX_SIMD_INTRINSIC constexpr _To\n+    operator()(_SimdTuple<_From, simd_abi::scalar> __x) const noexcept\n+    { return {static_cast<_To>(__x.first)}; }\n+  };\n+\n+// _SimdConverter fixed_size<_Np> -> fixed_size<_Np> {{{1\n+template <typename _From, typename _To, int _Np>\n+  struct _SimdConverter<_From, simd_abi::fixed_size<_Np>, _To,\n+\t\t\tsimd_abi::fixed_size<_Np>,\n+\t\t\tenable_if_t<!is_same_v<_From, _To>>>\n+  {\n+    using _Ret = __fixed_size_storage_t<_To, _Np>;\n+    using _Arg = __fixed_size_storage_t<_From, _Np>;\n+\n+    _GLIBCXX_SIMD_INTRINSIC constexpr _Ret\n+    operator()(const _Arg& __x) const noexcept\n+    {\n+      if constexpr (is_same_v<_From, _To>)\n+\treturn __x;\n+\n+      // special case (optimize) int signedness casts\n+      else if constexpr (sizeof(_From) == sizeof(_To)\n+\t\t\t && is_integral_v<_From> && is_integral_v<_To>)\n+\treturn __bit_cast<_Ret>(__x);\n+\n+      // special case if all ABI tags in _Ret are scalar\n+      else if constexpr (__is_scalar_abi<typename _Ret::_FirstAbi>())\n+\t{\n+\t  return __call_with_subscripts(\n+\t    __x, make_index_sequence<_Np>(),\n+\t    [](auto... __values) constexpr->_Ret {\n+\t      return __make_simd_tuple<_To, decltype((void) __values,\n+\t\t\t\t\t\t     simd_abi::scalar())...>(\n+\t\tstatic_cast<_To>(__values)...);\n+\t    });\n+\t}\n+\n+      // from one vector to one vector\n+      else if constexpr (_Arg::_S_first_size == _Ret::_S_first_size)\n+\t{\n+\t  _SimdConverter<_From, typename _Arg::_FirstAbi, _To,\n+\t\t\t typename _Ret::_FirstAbi>\n+\t    __native_cvt;\n+\t  if constexpr (_Arg::_S_tuple_size == 1)\n+\t    return {__native_cvt(__x.first)};\n+\t  else\n+\t    {\n+\t      constexpr size_t _NRemain = _Np - _Arg::_S_first_size;\n+\t      _SimdConverter<_From, simd_abi::fixed_size<_NRemain>, _To,\n+\t\t\t     simd_abi::fixed_size<_NRemain>>\n+\t\t__remainder_cvt;\n+\t      return {__native_cvt(__x.first), __remainder_cvt(__x.second)};\n+\t    }\n+\t}\n+\n+      // from one vector to multiple vectors\n+      else if constexpr (_Arg::_S_first_size > _Ret::_S_first_size)\n+\t{\n+\t  const auto __multiple_return_chunks\n+\t    = __convert_all<__vector_type_t<_To, _Ret::_S_first_size>>(\n+\t      __x.first);\n+\t  constexpr auto __converted = __multiple_return_chunks.size()\n+\t\t\t\t       * _Ret::_FirstAbi::template _S_size<_To>;\n+\t  constexpr auto __remaining = _Np - __converted;\n+\t  if constexpr (_Arg::_S_tuple_size == 1 && __remaining == 0)\n+\t    return __to_simd_tuple<_To, _Np>(__multiple_return_chunks);\n+\t  else if constexpr (_Arg::_S_tuple_size == 1)\n+\t    { // e.g. <int, 3> -> <double, 2, 1> or <short, 7> -> <double, 4, 2,\n+\t      // 1>\n+\t      using _RetRem\n+\t\t= __remove_cvref_t<decltype(__simd_tuple_pop_front<__converted>(\n+\t\t  _Ret()))>;\n+\t      const auto __return_chunks2\n+\t\t= __convert_all<__vector_type_t<_To, _RetRem::_S_first_size>, 0,\n+\t\t\t\t__converted>(__x.first);\n+\t      constexpr auto __converted2\n+\t\t= __converted\n+\t\t  + __return_chunks2.size() * _RetRem::_S_first_size;\n+\t      if constexpr (__converted2 == _Np)\n+\t\treturn __to_simd_tuple<_To, _Np>(__multiple_return_chunks,\n+\t\t\t\t\t\t __return_chunks2);\n+\t      else\n+\t\t{\n+\t\t  using _RetRem2 = __remove_cvref_t<\n+\t\t    decltype(__simd_tuple_pop_front<__return_chunks2.size()\n+\t\t\t\t\t\t    * _RetRem::_S_first_size>(\n+\t\t      _RetRem()))>;\n+\t\t  const auto __return_chunks3 = __convert_all<\n+\t\t    __vector_type_t<_To, _RetRem2::_S_first_size>, 0,\n+\t\t    __converted2>(__x.first);\n+\t\t  constexpr auto __converted3\n+\t\t    = __converted2\n+\t\t      + __return_chunks3.size() * _RetRem2::_S_first_size;\n+\t\t  if constexpr (__converted3 == _Np)\n+\t\t    return __to_simd_tuple<_To, _Np>(__multiple_return_chunks,\n+\t\t\t\t\t\t     __return_chunks2,\n+\t\t\t\t\t\t     __return_chunks3);\n+\t\t  else\n+\t\t    {\n+\t\t      using _RetRem3\n+\t\t\t= __remove_cvref_t<decltype(__simd_tuple_pop_front<\n+\t\t\t\t\t\t    __return_chunks3.size()\n+\t\t\t\t\t\t    * _RetRem2::_S_first_size>(\n+\t\t\t  _RetRem2()))>;\n+\t\t      const auto __return_chunks4 = __convert_all<\n+\t\t\t__vector_type_t<_To, _RetRem3::_S_first_size>, 0,\n+\t\t\t__converted3>(__x.first);\n+\t\t      constexpr auto __converted4\n+\t\t\t= __converted3\n+\t\t\t  + __return_chunks4.size() * _RetRem3::_S_first_size;\n+\t\t      if constexpr (__converted4 == _Np)\n+\t\t\treturn __to_simd_tuple<_To, _Np>(\n+\t\t\t  __multiple_return_chunks, __return_chunks2,\n+\t\t\t  __return_chunks3, __return_chunks4);\n+\t\t      else\n+\t\t\t__assert_unreachable<_To>();\n+\t\t    }\n+\t\t}\n+\t    }\n+\t  else\n+\t    {\n+\t      constexpr size_t _NRemain = _Np - _Arg::_S_first_size;\n+\t      _SimdConverter<_From, simd_abi::fixed_size<_NRemain>, _To,\n+\t\t\t     simd_abi::fixed_size<_NRemain>>\n+\t\t__remainder_cvt;\n+\t      return __simd_tuple_concat(\n+\t\t__to_simd_tuple<_To, _Arg::_S_first_size>(\n+\t\t  __multiple_return_chunks),\n+\t\t__remainder_cvt(__x.second));\n+\t    }\n+\t}\n+\n+      // from multiple vectors to one vector\n+      // _Arg::_S_first_size < _Ret::_S_first_size\n+      // a) heterogeneous input at the end of the tuple (possible with partial\n+      //    native registers in _Ret)\n+      else if constexpr (_Ret::_S_tuple_size == 1\n+\t\t\t && _Np % _Arg::_S_first_size != 0)\n+\t{\n+\t  static_assert(_Ret::_FirstAbi::template _S_is_partial<_To>);\n+\t  return _Ret{__generate_from_n_evaluations<\n+\t    _Np, typename _VectorTraits<typename _Ret::_FirstType>::type>(\n+\t    [&](auto __i) { return static_cast<_To>(__x[__i]); })};\n+\t}\n+      else\n+\t{\n+\t  static_assert(_Arg::_S_tuple_size > 1);\n+\t  constexpr auto __n\n+\t    = __div_roundup(_Ret::_S_first_size, _Arg::_S_first_size);\n+\t  return __call_with_n_evaluations<__n>(\n+\t    [&__x](auto... __uncvted) {\n+\t      // assuming _Arg Abi tags for all __i are _Arg::_FirstAbi\n+\t      _SimdConverter<_From, typename _Arg::_FirstAbi, _To,\n+\t\t\t     typename _Ret::_FirstAbi>\n+\t\t__native_cvt;\n+\t      if constexpr (_Ret::_S_tuple_size == 1)\n+\t\treturn _Ret{__native_cvt(__uncvted...)};\n+\t      else\n+\t\treturn _Ret{\n+\t\t  __native_cvt(__uncvted...),\n+\t\t  _SimdConverter<\n+\t\t    _From, simd_abi::fixed_size<_Np - _Ret::_S_first_size>, _To,\n+\t\t    simd_abi::fixed_size<_Np - _Ret::_S_first_size>>()(\n+\t\t    __simd_tuple_pop_front<_Ret::_S_first_size>(__x))};\n+\t    },\n+\t    [&__x](auto __i) { return __get_tuple_at<__i>(__x); });\n+\t}\n+    }\n+  };\n+\n+// _SimdConverter \"native\" -> fixed_size<_Np> {{{1\n+// i.e. 1 register to ? registers\n+template <typename _From, typename _Ap, typename _To, int _Np>\n+  struct _SimdConverter<_From, _Ap, _To, simd_abi::fixed_size<_Np>,\n+\t\t\tenable_if_t<!__is_fixed_size_abi_v<_Ap>>>\n+  {\n+    static_assert(\n+      _Np == simd_size_v<_From, _Ap>,\n+      \"_SimdConverter to fixed_size only works for equal element counts\");\n+\n+    using _Ret = __fixed_size_storage_t<_To, _Np>;\n+\n+    _GLIBCXX_SIMD_INTRINSIC constexpr _Ret\n+    operator()(typename _SimdTraits<_From, _Ap>::_SimdMember __x) const noexcept\n+    {\n+      if constexpr (_Ret::_S_tuple_size == 1)\n+\treturn {__vector_convert<typename _Ret::_FirstType::_BuiltinType>(__x)};\n+      else\n+\t{\n+\t  using _FixedNp = simd_abi::fixed_size<_Np>;\n+\t  _SimdConverter<_From, _FixedNp, _To, _FixedNp> __fixed_cvt;\n+\t  using _FromFixedStorage = __fixed_size_storage_t<_From, _Np>;\n+\t  if constexpr (_FromFixedStorage::_S_tuple_size == 1)\n+\t    return __fixed_cvt(_FromFixedStorage{__x});\n+\t  else if constexpr (_FromFixedStorage::_S_tuple_size == 2)\n+\t    {\n+\t      _FromFixedStorage __tmp;\n+\t      static_assert(sizeof(__tmp) <= sizeof(__x));\n+\t      __builtin_memcpy(&__tmp.first, &__x, sizeof(__tmp.first));\n+\t      __builtin_memcpy(&__tmp.second.first,\n+\t\t\t       reinterpret_cast<const char*>(&__x)\n+\t\t\t\t + sizeof(__tmp.first),\n+\t\t\t       sizeof(__tmp.second.first));\n+\t      return __fixed_cvt(__tmp);\n+\t    }\n+\t  else\n+\t    __assert_unreachable<_From>();\n+\t}\n+    }\n+  };\n+\n+// _SimdConverter fixed_size<_Np> -> \"native\" {{{1\n+// i.e. ? register to 1 registers\n+template <typename _From, int _Np, typename _To, typename _Ap>\n+  struct _SimdConverter<_From, simd_abi::fixed_size<_Np>, _To, _Ap,\n+\t\t\tenable_if_t<!__is_fixed_size_abi_v<_Ap>>>\n+  {\n+    static_assert(\n+      _Np == simd_size_v<_To, _Ap>,\n+      \"_SimdConverter to fixed_size only works for equal element counts\");\n+\n+    using _Arg = __fixed_size_storage_t<_From, _Np>;\n+\n+    _GLIBCXX_SIMD_INTRINSIC constexpr\n+      typename _SimdTraits<_To, _Ap>::_SimdMember\n+      operator()(_Arg __x) const noexcept\n+    {\n+      if constexpr (_Arg::_S_tuple_size == 1)\n+\treturn __vector_convert<__vector_type_t<_To, _Np>>(__x.first);\n+      else if constexpr (_Arg::_S_is_homogeneous)\n+\treturn __call_with_n_evaluations<_Arg::_S_tuple_size>(\n+\t  [](auto... __members) {\n+\t    if constexpr ((is_convertible_v<decltype(__members), _To> && ...))\n+\t      return __vector_type_t<_To, _Np>{static_cast<_To>(__members)...};\n+\t    else\n+\t      return __vector_convert<__vector_type_t<_To, _Np>>(__members...);\n+\t  },\n+\t  [&](auto __i) { return __get_tuple_at<__i>(__x); });\n+      else if constexpr (__fixed_size_storage_t<_To, _Np>::_S_tuple_size == 1)\n+\t{\n+\t  _SimdConverter<_From, simd_abi::fixed_size<_Np>, _To,\n+\t\t\t simd_abi::fixed_size<_Np>>\n+\t    __fixed_cvt;\n+\t  return __fixed_cvt(__x).first;\n+\t}\n+      else\n+\t{\n+\t  const _SimdWrapper<_From, _Np> __xv\n+\t    = __generate_from_n_evaluations<_Np, __vector_type_t<_From, _Np>>(\n+\t      [&](auto __i) { return __x[__i]; });\n+\t  return __vector_convert<__vector_type_t<_To, _Np>>(__xv);\n+\t}\n+    }\n+  };\n+\n+// }}}1\n+_GLIBCXX_SIMD_END_NAMESPACE\n+#endif // __cplusplus >= 201703L\n+#endif // _GLIBCXX_EXPERIMENTAL_SIMD_CONVERTER_H_\n+\n+// vim: foldmethod=marker sw=2 noet ts=8 sts=2 tw=80"}, {"sha": "a49a9d88b7f576a10905cc368a41b0fd2e200998", "filename": "libstdc++-v3/include/experimental/bits/simd_detail.h", "status": "added", "additions": 306, "deletions": 0, "changes": 306, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_detail.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_detail.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_detail.h?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -0,0 +1,306 @@\n+// Internal macros for the simd implementation -*- C++ -*-\n+\n+// Copyright (C) 2020 Free Software Foundation, Inc.\n+//\n+// This file is part of the GNU ISO C++ Library.  This library is free\n+// software; you can redistribute it and/or modify it under the\n+// terms of the GNU General Public License as published by the\n+// Free Software Foundation; either version 3, or (at your option)\n+// any later version.\n+\n+// This library is distributed in the hope that it will be useful,\n+// but WITHOUT ANY WARRANTY; without even the implied warranty of\n+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+// GNU General Public License for more details.\n+\n+// Under Section 7 of GPL version 3, you are granted additional\n+// permissions described in the GCC Runtime Library Exception, version\n+// 3.1, as published by the Free Software Foundation.\n+\n+// You should have received a copy of the GNU General Public License and\n+// a copy of the GCC Runtime Library Exception along with this program;\n+// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+// <http://www.gnu.org/licenses/>.\n+\n+#ifndef _GLIBCXX_EXPERIMENTAL_SIMD_DETAIL_H_\n+#define _GLIBCXX_EXPERIMENTAL_SIMD_DETAIL_H_\n+\n+#if __cplusplus >= 201703L\n+\n+#include <cstddef>\n+#include <cstdint>\n+\n+\n+#define _GLIBCXX_SIMD_BEGIN_NAMESPACE                                          \\\n+  namespace std _GLIBCXX_VISIBILITY(default)                                   \\\n+  {                                                                            \\\n+    _GLIBCXX_BEGIN_NAMESPACE_VERSION                                           \\\n+      namespace experimental {                                                 \\\n+      inline namespace parallelism_v2 {\n+#define _GLIBCXX_SIMD_END_NAMESPACE                                            \\\n+  }                                                                            \\\n+  }                                                                            \\\n+  _GLIBCXX_END_NAMESPACE_VERSION                                               \\\n+  }\n+\n+// ISA extension detection. The following defines all the _GLIBCXX_SIMD_HAVE_XXX\n+// macros ARM{{{\n+#if defined __ARM_NEON\n+#define _GLIBCXX_SIMD_HAVE_NEON 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_NEON 0\n+#endif\n+#if defined __ARM_NEON && (__ARM_ARCH >= 8 || defined __aarch64__)\n+#define _GLIBCXX_SIMD_HAVE_NEON_A32 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_NEON_A32 0\n+#endif\n+#if defined __ARM_NEON && defined __aarch64__\n+#define _GLIBCXX_SIMD_HAVE_NEON_A64 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_NEON_A64 0\n+#endif\n+//}}}\n+// x86{{{\n+#ifdef __MMX__\n+#define _GLIBCXX_SIMD_HAVE_MMX 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_MMX 0\n+#endif\n+#if defined __SSE__ || defined __x86_64__\n+#define _GLIBCXX_SIMD_HAVE_SSE 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_SSE 0\n+#endif\n+#if defined __SSE2__ || defined __x86_64__\n+#define _GLIBCXX_SIMD_HAVE_SSE2 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_SSE2 0\n+#endif\n+#ifdef __SSE3__\n+#define _GLIBCXX_SIMD_HAVE_SSE3 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_SSE3 0\n+#endif\n+#ifdef __SSSE3__\n+#define _GLIBCXX_SIMD_HAVE_SSSE3 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_SSSE3 0\n+#endif\n+#ifdef __SSE4_1__\n+#define _GLIBCXX_SIMD_HAVE_SSE4_1 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_SSE4_1 0\n+#endif\n+#ifdef __SSE4_2__\n+#define _GLIBCXX_SIMD_HAVE_SSE4_2 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_SSE4_2 0\n+#endif\n+#ifdef __XOP__\n+#define _GLIBCXX_SIMD_HAVE_XOP 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_XOP 0\n+#endif\n+#ifdef __AVX__\n+#define _GLIBCXX_SIMD_HAVE_AVX 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_AVX 0\n+#endif\n+#ifdef __AVX2__\n+#define _GLIBCXX_SIMD_HAVE_AVX2 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_AVX2 0\n+#endif\n+#ifdef __BMI__\n+#define _GLIBCXX_SIMD_HAVE_BMI1 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_BMI1 0\n+#endif\n+#ifdef __BMI2__\n+#define _GLIBCXX_SIMD_HAVE_BMI2 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_BMI2 0\n+#endif\n+#ifdef __LZCNT__\n+#define _GLIBCXX_SIMD_HAVE_LZCNT 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_LZCNT 0\n+#endif\n+#ifdef __SSE4A__\n+#define _GLIBCXX_SIMD_HAVE_SSE4A 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_SSE4A 0\n+#endif\n+#ifdef __FMA__\n+#define _GLIBCXX_SIMD_HAVE_FMA 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_FMA 0\n+#endif\n+#ifdef __FMA4__\n+#define _GLIBCXX_SIMD_HAVE_FMA4 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_FMA4 0\n+#endif\n+#ifdef __F16C__\n+#define _GLIBCXX_SIMD_HAVE_F16C 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_F16C 0\n+#endif\n+#ifdef __POPCNT__\n+#define _GLIBCXX_SIMD_HAVE_POPCNT 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_POPCNT 0\n+#endif\n+#ifdef __AVX512F__\n+#define _GLIBCXX_SIMD_HAVE_AVX512F 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_AVX512F 0\n+#endif\n+#ifdef __AVX512DQ__\n+#define _GLIBCXX_SIMD_HAVE_AVX512DQ 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_AVX512DQ 0\n+#endif\n+#ifdef __AVX512VL__\n+#define _GLIBCXX_SIMD_HAVE_AVX512VL 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_AVX512VL 0\n+#endif\n+#ifdef __AVX512BW__\n+#define _GLIBCXX_SIMD_HAVE_AVX512BW 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_AVX512BW 0\n+#endif\n+\n+#if _GLIBCXX_SIMD_HAVE_SSE\n+#define _GLIBCXX_SIMD_HAVE_SSE_ABI 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_SSE_ABI 0\n+#endif\n+#if _GLIBCXX_SIMD_HAVE_SSE2\n+#define _GLIBCXX_SIMD_HAVE_FULL_SSE_ABI 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_FULL_SSE_ABI 0\n+#endif\n+\n+#if _GLIBCXX_SIMD_HAVE_AVX\n+#define _GLIBCXX_SIMD_HAVE_AVX_ABI 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_AVX_ABI 0\n+#endif\n+#if _GLIBCXX_SIMD_HAVE_AVX2\n+#define _GLIBCXX_SIMD_HAVE_FULL_AVX_ABI 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_FULL_AVX_ABI 0\n+#endif\n+\n+#if _GLIBCXX_SIMD_HAVE_AVX512F\n+#define _GLIBCXX_SIMD_HAVE_AVX512_ABI 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_AVX512_ABI 0\n+#endif\n+#if _GLIBCXX_SIMD_HAVE_AVX512BW\n+#define _GLIBCXX_SIMD_HAVE_FULL_AVX512_ABI 1\n+#else\n+#define _GLIBCXX_SIMD_HAVE_FULL_AVX512_ABI 0\n+#endif\n+\n+#if defined __x86_64__ && !_GLIBCXX_SIMD_HAVE_SSE2\n+#error \"Use of SSE2 is required on AMD64\"\n+#endif\n+//}}}\n+\n+#ifdef __clang__\n+#define _GLIBCXX_SIMD_NORMAL_MATH\n+#else\n+#define _GLIBCXX_SIMD_NORMAL_MATH                                              \\\n+  [[__gnu__::__optimize__(\"finite-math-only,no-signed-zeros\")]]\n+#endif\n+#define _GLIBCXX_SIMD_NEVER_INLINE [[__gnu__::__noinline__]]\n+#define _GLIBCXX_SIMD_INTRINSIC                                                \\\n+  [[__gnu__::__always_inline__, __gnu__::__artificial__]] inline\n+#define _GLIBCXX_SIMD_ALWAYS_INLINE [[__gnu__::__always_inline__]] inline\n+#define _GLIBCXX_SIMD_IS_UNLIKELY(__x) __builtin_expect(__x, 0)\n+#define _GLIBCXX_SIMD_IS_LIKELY(__x) __builtin_expect(__x, 1)\n+\n+#if defined __STRICT_ANSI__ && __STRICT_ANSI__\n+#define _GLIBCXX_SIMD_CONSTEXPR\n+#define _GLIBCXX_SIMD_USE_CONSTEXPR_API const\n+#else\n+#define _GLIBCXX_SIMD_CONSTEXPR constexpr\n+#define _GLIBCXX_SIMD_USE_CONSTEXPR_API constexpr\n+#endif\n+\n+#if defined __clang__\n+#define _GLIBCXX_SIMD_USE_CONSTEXPR const\n+#else\n+#define _GLIBCXX_SIMD_USE_CONSTEXPR constexpr\n+#endif\n+\n+#define _GLIBCXX_SIMD_LIST_BINARY(__macro) __macro(|) __macro(&) __macro(^)\n+#define _GLIBCXX_SIMD_LIST_SHIFTS(__macro) __macro(<<) __macro(>>)\n+#define _GLIBCXX_SIMD_LIST_ARITHMETICS(__macro)                                \\\n+  __macro(+) __macro(-) __macro(*) __macro(/) __macro(%)\n+\n+#define _GLIBCXX_SIMD_ALL_BINARY(__macro)                                      \\\n+  _GLIBCXX_SIMD_LIST_BINARY(__macro) static_assert(true)\n+#define _GLIBCXX_SIMD_ALL_SHIFTS(__macro)                                      \\\n+  _GLIBCXX_SIMD_LIST_SHIFTS(__macro) static_assert(true)\n+#define _GLIBCXX_SIMD_ALL_ARITHMETICS(__macro)                                 \\\n+  _GLIBCXX_SIMD_LIST_ARITHMETICS(__macro) static_assert(true)\n+\n+#ifdef _GLIBCXX_SIMD_NO_ALWAYS_INLINE\n+#undef _GLIBCXX_SIMD_ALWAYS_INLINE\n+#define _GLIBCXX_SIMD_ALWAYS_INLINE inline\n+#undef _GLIBCXX_SIMD_INTRINSIC\n+#define _GLIBCXX_SIMD_INTRINSIC inline\n+#endif\n+\n+#if _GLIBCXX_SIMD_HAVE_SSE || _GLIBCXX_SIMD_HAVE_MMX\n+#define _GLIBCXX_SIMD_X86INTRIN 1\n+#else\n+#define _GLIBCXX_SIMD_X86INTRIN 0\n+#endif\n+\n+// workaround macros {{{\n+// use aliasing loads to help GCC understand the data accesses better\n+// This also seems to hide a miscompilation on swap(x[i], x[i + 1]) with\n+// fixed_size_simd<float, 16> x.\n+#define _GLIBCXX_SIMD_USE_ALIASING_LOADS 1\n+\n+// vector conversions on x86 not optimized:\n+#if _GLIBCXX_SIMD_X86INTRIN\n+#define _GLIBCXX_SIMD_WORKAROUND_PR85048 1\n+#endif\n+\n+// integer division not optimized\n+#define _GLIBCXX_SIMD_WORKAROUND_PR90993 1\n+\n+// very bad codegen for extraction and concatenation of 128/256 \"subregisters\"\n+// with sizeof(element type) < 8: https://godbolt.org/g/mqUsgM\n+#if _GLIBCXX_SIMD_X86INTRIN\n+#define _GLIBCXX_SIMD_WORKAROUND_XXX_1 1\n+#endif\n+\n+// bad codegen for 8 Byte memcpy to __vector_type_t<char, 16>\n+#define _GLIBCXX_SIMD_WORKAROUND_PR90424 1\n+\n+// bad codegen for zero-extend using simple concat(__x, 0)\n+#if _GLIBCXX_SIMD_X86INTRIN\n+#define _GLIBCXX_SIMD_WORKAROUND_XXX_3 1\n+#endif\n+\n+// https://github.com/cplusplus/parallelism-ts/issues/65 (incorrect return type\n+// of static_simd_cast)\n+#define _GLIBCXX_SIMD_FIX_P2TS_ISSUE65 1\n+\n+// https://github.com/cplusplus/parallelism-ts/issues/66 (incorrect SFINAE\n+// constraint on (static)_simd_cast)\n+#define _GLIBCXX_SIMD_FIX_P2TS_ISSUE66 1\n+// }}}\n+\n+#endif // __cplusplus >= 201703L\n+#endif // _GLIBCXX_EXPERIMENTAL_SIMD_DETAIL_H_\n+\n+// vim: foldmethod=marker"}, {"sha": "fba8c7e466e5ae224b046aba41c80c9fd1a806a7", "filename": "libstdc++-v3/include/experimental/bits/simd_fixed_size.h", "status": "added", "additions": 2066, "deletions": 0, "changes": 2066, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_fixed_size.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_fixed_size.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_fixed_size.h?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -0,0 +1,2066 @@\n+// Simd fixed_size ABI specific implementations -*- C++ -*-\n+\n+// Copyright (C) 2020 Free Software Foundation, Inc.\n+//\n+// This file is part of the GNU ISO C++ Library.  This library is free\n+// software; you can redistribute it and/or modify it under the\n+// terms of the GNU General Public License as published by the\n+// Free Software Foundation; either version 3, or (at your option)\n+// any later version.\n+\n+// This library is distributed in the hope that it will be useful,\n+// but WITHOUT ANY WARRANTY; without even the implied warranty of\n+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+// GNU General Public License for more details.\n+\n+// Under Section 7 of GPL version 3, you are granted additional\n+// permissions described in the GCC Runtime Library Exception, version\n+// 3.1, as published by the Free Software Foundation.\n+\n+// You should have received a copy of the GNU General Public License and\n+// a copy of the GCC Runtime Library Exception along with this program;\n+// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+// <http://www.gnu.org/licenses/>.\n+\n+/*\n+ * The fixed_size ABI gives the following guarantees:\n+ *  - simd objects are passed via the stack\n+ *  - memory layout of `simd<_Tp, _Np>` is equivalent to `array<_Tp, _Np>`\n+ *  - alignment of `simd<_Tp, _Np>` is `_Np * sizeof(_Tp)` if _Np is __a\n+ *    power-of-2 value, otherwise `std::__bit_ceil(_Np * sizeof(_Tp))` (Note:\n+ *    if the alignment were to exceed the system/compiler maximum, it is bounded\n+ *    to that maximum)\n+ *  - simd_mask objects are passed like bitset<_Np>\n+ *  - memory layout of `simd_mask<_Tp, _Np>` is equivalent to `bitset<_Np>`\n+ *  - alignment of `simd_mask<_Tp, _Np>` is equal to the alignment of\n+ *    `bitset<_Np>`\n+ */\n+\n+#ifndef _GLIBCXX_EXPERIMENTAL_SIMD_FIXED_SIZE_H_\n+#define _GLIBCXX_EXPERIMENTAL_SIMD_FIXED_SIZE_H_\n+\n+#if __cplusplus >= 201703L\n+\n+#include <array>\n+\n+_GLIBCXX_SIMD_BEGIN_NAMESPACE\n+\n+// __simd_tuple_element {{{\n+template <size_t _I, typename _Tp>\n+  struct __simd_tuple_element;\n+\n+template <typename _Tp, typename _A0, typename... _As>\n+  struct __simd_tuple_element<0, _SimdTuple<_Tp, _A0, _As...>>\n+  { using type = simd<_Tp, _A0>; };\n+\n+template <size_t _I, typename _Tp, typename _A0, typename... _As>\n+  struct __simd_tuple_element<_I, _SimdTuple<_Tp, _A0, _As...>>\n+  {\n+    using type =\n+      typename __simd_tuple_element<_I - 1, _SimdTuple<_Tp, _As...>>::type;\n+  };\n+\n+template <size_t _I, typename _Tp>\n+  using __simd_tuple_element_t = typename __simd_tuple_element<_I, _Tp>::type;\n+\n+// }}}\n+// __simd_tuple_concat {{{\n+\n+template <typename _Tp, typename... _A0s, typename... _A1s>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr _SimdTuple<_Tp, _A0s..., _A1s...>\n+  __simd_tuple_concat(const _SimdTuple<_Tp, _A0s...>& __left,\n+\t\t      const _SimdTuple<_Tp, _A1s...>& __right)\n+  {\n+    if constexpr (sizeof...(_A0s) == 0)\n+      return __right;\n+    else if constexpr (sizeof...(_A1s) == 0)\n+      return __left;\n+    else\n+      return {__left.first, __simd_tuple_concat(__left.second, __right)};\n+  }\n+\n+template <typename _Tp, typename _A10, typename... _A1s>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr _SimdTuple<_Tp, simd_abi::scalar, _A10,\n+\t\t\t\t\t       _A1s...>\n+  __simd_tuple_concat(const _Tp& __left,\n+\t\t      const _SimdTuple<_Tp, _A10, _A1s...>& __right)\n+  { return {__left, __right}; }\n+\n+// }}}\n+// __simd_tuple_pop_front {{{\n+// Returns the next _SimdTuple in __x that has _Np elements less.\n+// Precondition: _Np must match the number of elements in __first (recursively)\n+template <size_t _Np, typename _Tp>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr decltype(auto)\n+  __simd_tuple_pop_front(_Tp&& __x)\n+  {\n+    if constexpr (_Np == 0)\n+      return static_cast<_Tp&&>(__x);\n+    else\n+      {\n+\tusing _Up = __remove_cvref_t<_Tp>;\n+\tstatic_assert(_Np >= _Up::_S_first_size);\n+\treturn __simd_tuple_pop_front<_Np - _Up::_S_first_size>(__x.second);\n+      }\n+  }\n+\n+// }}}\n+// __get_simd_at<_Np> {{{1\n+struct __as_simd {};\n+\n+struct __as_simd_tuple {};\n+\n+template <typename _Tp, typename _A0, typename... _Abis>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr simd<_Tp, _A0>\n+  __simd_tuple_get_impl(__as_simd, const _SimdTuple<_Tp, _A0, _Abis...>& __t,\n+\t\t\t_SizeConstant<0>)\n+  { return {__private_init, __t.first}; }\n+\n+template <typename _Tp, typename _A0, typename... _Abis>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr const auto&\n+  __simd_tuple_get_impl(__as_simd_tuple,\n+\t\t\tconst _SimdTuple<_Tp, _A0, _Abis...>& __t,\n+\t\t\t_SizeConstant<0>)\n+  { return __t.first; }\n+\n+template <typename _Tp, typename _A0, typename... _Abis>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr auto&\n+  __simd_tuple_get_impl(__as_simd_tuple, _SimdTuple<_Tp, _A0, _Abis...>& __t,\n+\t\t\t_SizeConstant<0>)\n+  { return __t.first; }\n+\n+template <typename _R, size_t _Np, typename _Tp, typename... _Abis>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr auto\n+  __simd_tuple_get_impl(_R, const _SimdTuple<_Tp, _Abis...>& __t,\n+\t\t\t_SizeConstant<_Np>)\n+  { return __simd_tuple_get_impl(_R(), __t.second, _SizeConstant<_Np - 1>()); }\n+\n+template <size_t _Np, typename _Tp, typename... _Abis>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr auto&\n+  __simd_tuple_get_impl(__as_simd_tuple, _SimdTuple<_Tp, _Abis...>& __t,\n+\t\t\t_SizeConstant<_Np>)\n+  {\n+    return __simd_tuple_get_impl(__as_simd_tuple(), __t.second,\n+\t\t\t\t _SizeConstant<_Np - 1>());\n+  }\n+\n+template <size_t _Np, typename _Tp, typename... _Abis>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr auto\n+  __get_simd_at(const _SimdTuple<_Tp, _Abis...>& __t)\n+  { return __simd_tuple_get_impl(__as_simd(), __t, _SizeConstant<_Np>()); }\n+\n+// }}}\n+// __get_tuple_at<_Np> {{{\n+template <size_t _Np, typename _Tp, typename... _Abis>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr auto\n+  __get_tuple_at(const _SimdTuple<_Tp, _Abis...>& __t)\n+  {\n+    return __simd_tuple_get_impl(__as_simd_tuple(), __t, _SizeConstant<_Np>());\n+  }\n+\n+template <size_t _Np, typename _Tp, typename... _Abis>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr auto&\n+  __get_tuple_at(_SimdTuple<_Tp, _Abis...>& __t)\n+  {\n+    return __simd_tuple_get_impl(__as_simd_tuple(), __t, _SizeConstant<_Np>());\n+  }\n+\n+// __tuple_element_meta {{{1\n+template <typename _Tp, typename _Abi, size_t _Offset>\n+  struct __tuple_element_meta : public _Abi::_SimdImpl\n+  {\n+    static_assert(is_same_v<typename _Abi::_SimdImpl::abi_type,\n+\t\t\t    _Abi>); // this fails e.g. when _SimdImpl is an\n+\t\t\t\t    // alias for _SimdImplBuiltin<_DifferentAbi>\n+    using value_type = _Tp;\n+    using abi_type = _Abi;\n+    using _Traits = _SimdTraits<_Tp, _Abi>;\n+    using _MaskImpl = typename _Abi::_MaskImpl;\n+    using _MaskMember = typename _Traits::_MaskMember;\n+    using simd_type = simd<_Tp, _Abi>;\n+    static constexpr size_t _S_offset = _Offset;\n+    static constexpr size_t _S_size() { return simd_size<_Tp, _Abi>::value; }\n+    static constexpr _MaskImpl _S_mask_impl = {};\n+\n+    template <size_t _Np, bool _Sanitized>\n+      _GLIBCXX_SIMD_INTRINSIC static auto\n+      _S_submask(_BitMask<_Np, _Sanitized> __bits)\n+      { return __bits.template _M_extract<_Offset, _S_size()>(); }\n+\n+    template <size_t _Np, bool _Sanitized>\n+      _GLIBCXX_SIMD_INTRINSIC static _MaskMember\n+      _S_make_mask(_BitMask<_Np, _Sanitized> __bits)\n+      {\n+\treturn _MaskImpl::template _S_convert<_Tp>(\n+\t  __bits.template _M_extract<_Offset, _S_size()>()._M_sanitized());\n+      }\n+\n+    _GLIBCXX_SIMD_INTRINSIC static _ULLong\n+    _S_mask_to_shifted_ullong(_MaskMember __k)\n+    { return _MaskImpl::_S_to_bits(__k).to_ullong() << _Offset; }\n+  };\n+\n+template <size_t _Offset, typename _Tp, typename _Abi, typename... _As>\n+  __tuple_element_meta<_Tp, _Abi, _Offset>\n+  __make_meta(const _SimdTuple<_Tp, _Abi, _As...>&)\n+  { return {}; }\n+\n+// }}}1\n+// _WithOffset wrapper class {{{\n+template <size_t _Offset, typename _Base>\n+  struct _WithOffset : public _Base\n+  {\n+    static inline constexpr size_t _S_offset = _Offset;\n+\n+    _GLIBCXX_SIMD_INTRINSIC char* _M_as_charptr()\n+    {\n+      return reinterpret_cast<char*>(this)\n+\t     + _S_offset * sizeof(typename _Base::value_type);\n+    }\n+\n+    _GLIBCXX_SIMD_INTRINSIC const char* _M_as_charptr() const\n+    {\n+      return reinterpret_cast<const char*>(this)\n+\t     + _S_offset * sizeof(typename _Base::value_type);\n+    }\n+  };\n+\n+// make _WithOffset<_WithOffset> ill-formed to use:\n+template <size_t _O0, size_t _O1, typename _Base>\n+  struct _WithOffset<_O0, _WithOffset<_O1, _Base>> {};\n+\n+template <size_t _Offset, typename _Tp>\n+  decltype(auto)\n+  __add_offset(_Tp& __base)\n+  { return static_cast<_WithOffset<_Offset, __remove_cvref_t<_Tp>>&>(__base); }\n+\n+template <size_t _Offset, typename _Tp>\n+  decltype(auto)\n+  __add_offset(const _Tp& __base)\n+  {\n+    return static_cast<const _WithOffset<_Offset, __remove_cvref_t<_Tp>>&>(\n+      __base);\n+  }\n+\n+template <size_t _Offset, size_t _ExistingOffset, typename _Tp>\n+  decltype(auto)\n+  __add_offset(_WithOffset<_ExistingOffset, _Tp>& __base)\n+  {\n+    return static_cast<_WithOffset<_Offset + _ExistingOffset, _Tp>&>(\n+      static_cast<_Tp&>(__base));\n+  }\n+\n+template <size_t _Offset, size_t _ExistingOffset, typename _Tp>\n+  decltype(auto)\n+  __add_offset(const _WithOffset<_ExistingOffset, _Tp>& __base)\n+  {\n+    return static_cast<const _WithOffset<_Offset + _ExistingOffset, _Tp>&>(\n+      static_cast<const _Tp&>(__base));\n+  }\n+\n+template <typename _Tp>\n+  constexpr inline size_t __offset = 0;\n+\n+template <size_t _Offset, typename _Tp>\n+  constexpr inline size_t __offset<_WithOffset<_Offset, _Tp>>\n+    = _WithOffset<_Offset, _Tp>::_S_offset;\n+\n+template <typename _Tp>\n+  constexpr inline size_t __offset<const _Tp> = __offset<_Tp>;\n+\n+template <typename _Tp>\n+  constexpr inline size_t __offset<_Tp&> = __offset<_Tp>;\n+\n+template <typename _Tp>\n+  constexpr inline size_t __offset<_Tp&&> = __offset<_Tp>;\n+\n+// }}}\n+// _SimdTuple specializations {{{1\n+// empty {{{2\n+template <typename _Tp>\n+  struct _SimdTuple<_Tp>\n+  {\n+    using value_type = _Tp;\n+    static constexpr size_t _S_tuple_size = 0;\n+    static constexpr size_t _S_size() { return 0; }\n+  };\n+\n+// _SimdTupleData {{{2\n+template <typename _FirstType, typename _SecondType>\n+  struct _SimdTupleData\n+  {\n+    _FirstType first;\n+    _SecondType second;\n+\n+    _GLIBCXX_SIMD_INTRINSIC\n+    constexpr bool _M_is_constprop() const\n+    {\n+      if constexpr (is_class_v<_FirstType>)\n+\treturn first._M_is_constprop() && second._M_is_constprop();\n+      else\n+\treturn __builtin_constant_p(first) && second._M_is_constprop();\n+    }\n+  };\n+\n+template <typename _FirstType, typename _Tp>\n+  struct _SimdTupleData<_FirstType, _SimdTuple<_Tp>>\n+  {\n+    _FirstType first;\n+    static constexpr _SimdTuple<_Tp> second = {};\n+\n+    _GLIBCXX_SIMD_INTRINSIC\n+    constexpr bool _M_is_constprop() const\n+    {\n+      if constexpr (is_class_v<_FirstType>)\n+\treturn first._M_is_constprop();\n+      else\n+\treturn __builtin_constant_p(first);\n+    }\n+  };\n+\n+// 1 or more {{{2\n+template <typename _Tp, typename _Abi0, typename... _Abis>\n+  struct _SimdTuple<_Tp, _Abi0, _Abis...>\n+    : _SimdTupleData<typename _SimdTraits<_Tp, _Abi0>::_SimdMember,\n+\t\t     _SimdTuple<_Tp, _Abis...>>\n+  {\n+    static_assert(!__is_fixed_size_abi_v<_Abi0>);\n+    using value_type = _Tp;\n+    using _FirstType = typename _SimdTraits<_Tp, _Abi0>::_SimdMember;\n+    using _FirstAbi = _Abi0;\n+    using _SecondType = _SimdTuple<_Tp, _Abis...>;\n+    static constexpr size_t _S_tuple_size = sizeof...(_Abis) + 1;\n+\n+    static constexpr size_t _S_size()\n+    { return simd_size_v<_Tp, _Abi0> + _SecondType::_S_size(); }\n+\n+    static constexpr size_t _S_first_size = simd_size_v<_Tp, _Abi0>;\n+    static constexpr bool _S_is_homogeneous = (is_same_v<_Abi0, _Abis> && ...);\n+\n+    using _Base = _SimdTupleData<typename _SimdTraits<_Tp, _Abi0>::_SimdMember,\n+\t\t\t\t _SimdTuple<_Tp, _Abis...>>;\n+    using _Base::first;\n+    using _Base::second;\n+\n+    _GLIBCXX_SIMD_INTRINSIC constexpr _SimdTuple() = default;\n+    _GLIBCXX_SIMD_INTRINSIC constexpr _SimdTuple(const _SimdTuple&) = default;\n+    _GLIBCXX_SIMD_INTRINSIC constexpr _SimdTuple& operator=(const _SimdTuple&)\n+      = default;\n+\n+    template <typename _Up>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr _SimdTuple(_Up&& __x)\n+      : _Base{static_cast<_Up&&>(__x)} {}\n+\n+    template <typename _Up, typename _Up2>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr _SimdTuple(_Up&& __x, _Up2&& __y)\n+      : _Base{static_cast<_Up&&>(__x), static_cast<_Up2&&>(__y)} {}\n+\n+    template <typename _Up>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr _SimdTuple(_Up&& __x, _SimdTuple<_Tp>)\n+      : _Base{static_cast<_Up&&>(__x)} {}\n+\n+    _GLIBCXX_SIMD_INTRINSIC char* _M_as_charptr()\n+    { return reinterpret_cast<char*>(this); }\n+\n+    _GLIBCXX_SIMD_INTRINSIC const char* _M_as_charptr() const\n+    { return reinterpret_cast<const char*>(this); }\n+\n+    template <size_t _Np>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr auto& _M_at()\n+      {\n+\tif constexpr (_Np == 0)\n+\t  return first;\n+\telse\n+\t  return second.template _M_at<_Np - 1>();\n+      }\n+\n+    template <size_t _Np>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr const auto& _M_at() const\n+      {\n+\tif constexpr (_Np == 0)\n+\t  return first;\n+\telse\n+\t  return second.template _M_at<_Np - 1>();\n+      }\n+\n+    template <size_t _Np>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr auto _M_simd_at() const\n+      {\n+\tif constexpr (_Np == 0)\n+\t  return simd<_Tp, _Abi0>(__private_init, first);\n+\telse\n+\t  return second.template _M_simd_at<_Np - 1>();\n+      }\n+\n+    template <size_t _Offset = 0, typename _Fp>\n+      _GLIBCXX_SIMD_INTRINSIC static constexpr _SimdTuple\n+      _S_generate(_Fp&& __gen, _SizeConstant<_Offset> = {})\n+      {\n+\tauto&& __first = __gen(__tuple_element_meta<_Tp, _Abi0, _Offset>());\n+\tif constexpr (_S_tuple_size == 1)\n+\t  return {__first};\n+\telse\n+\t  return {__first,\n+\t\t  _SecondType::_S_generate(\n+\t\t    static_cast<_Fp&&>(__gen),\n+\t\t    _SizeConstant<_Offset + simd_size_v<_Tp, _Abi0>>())};\n+      }\n+\n+    template <size_t _Offset = 0, typename _Fp, typename... _More>\n+      _GLIBCXX_SIMD_INTRINSIC _SimdTuple\n+      _M_apply_wrapped(_Fp&& __fun, const _More&... __more) const\n+      {\n+\tauto&& __first\n+\t  = __fun(__make_meta<_Offset>(*this), first, __more.first...);\n+\tif constexpr (_S_tuple_size == 1)\n+\t  return {__first};\n+\telse\n+\t  return {\n+\t    __first,\n+\t    second.template _M_apply_wrapped<_Offset + simd_size_v<_Tp, _Abi0>>(\n+\t      static_cast<_Fp&&>(__fun), __more.second...)};\n+      }\n+\n+    template <typename _Tup>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr decltype(auto)\n+      _M_extract_argument(_Tup&& __tup) const\n+      {\n+\tusing _TupT = typename __remove_cvref_t<_Tup>::value_type;\n+\tif constexpr (is_same_v<_SimdTuple, __remove_cvref_t<_Tup>>)\n+\t  return __tup.first;\n+\telse if (__builtin_is_constant_evaluated())\n+\t  return __fixed_size_storage_t<_TupT, _S_first_size>::_S_generate([&](\n+\t    auto __meta) constexpr {\n+\t    return __meta._S_generator(\n+\t      [&](auto __i) constexpr { return __tup[__i]; },\n+\t      static_cast<_TupT*>(nullptr));\n+\t  });\n+\telse\n+\t  return [&]() {\n+\t    __fixed_size_storage_t<_TupT, _S_first_size> __r;\n+\t    __builtin_memcpy(__r._M_as_charptr(), __tup._M_as_charptr(),\n+\t\t\t     sizeof(__r));\n+\t    return __r;\n+\t  }();\n+      }\n+\n+    template <typename _Tup>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr auto&\n+      _M_skip_argument(_Tup&& __tup) const\n+      {\n+\tstatic_assert(_S_tuple_size > 1);\n+\tusing _Up = __remove_cvref_t<_Tup>;\n+\tconstexpr size_t __off = __offset<_Up>;\n+\tif constexpr (_S_first_size == _Up::_S_first_size && __off == 0)\n+\t  return __tup.second;\n+\telse if constexpr (_S_first_size > _Up::_S_first_size\n+\t\t\t   && _S_first_size % _Up::_S_first_size == 0\n+\t\t\t   && __off == 0)\n+\t  return __simd_tuple_pop_front<_S_first_size>(__tup);\n+\telse if constexpr (_S_first_size + __off < _Up::_S_first_size)\n+\t  return __add_offset<_S_first_size>(__tup);\n+\telse if constexpr (_S_first_size + __off == _Up::_S_first_size)\n+\t  return __tup.second;\n+\telse\n+\t  __assert_unreachable<_Tup>();\n+      }\n+\n+    template <size_t _Offset, typename... _More>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr void\n+      _M_assign_front(const _SimdTuple<_Tp, _Abi0, _More...>& __x) &\n+      {\n+\tstatic_assert(_Offset == 0);\n+\tfirst = __x.first;\n+\tif constexpr (sizeof...(_More) > 0)\n+\t  {\n+\t    static_assert(sizeof...(_Abis) >= sizeof...(_More));\n+\t    second.template _M_assign_front<0>(__x.second);\n+\t  }\n+      }\n+\n+    template <size_t _Offset>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr void\n+      _M_assign_front(const _FirstType& __x) &\n+      {\n+\tstatic_assert(_Offset == 0);\n+\tfirst = __x;\n+      }\n+\n+    template <size_t _Offset, typename... _As>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr void\n+      _M_assign_front(const _SimdTuple<_Tp, _As...>& __x) &\n+      {\n+\t__builtin_memcpy(_M_as_charptr() + _Offset * sizeof(value_type),\n+\t\t\t __x._M_as_charptr(),\n+\t\t\t sizeof(_Tp) * _SimdTuple<_Tp, _As...>::_S_size());\n+      }\n+\n+    /*\n+     * Iterate over the first objects in this _SimdTuple and call __fun for each\n+     * of them. If additional arguments are passed via __more, chunk them into\n+     * _SimdTuple or __vector_type_t objects of the same number of values.\n+     */\n+    template <typename _Fp, typename... _More>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr _SimdTuple\n+      _M_apply_per_chunk(_Fp&& __fun, _More&&... __more) const\n+      {\n+\tif constexpr ((...\n+\t\t       || conjunction_v<\n+\t\t\t is_lvalue_reference<_More>,\n+\t\t\t negation<is_const<remove_reference_t<_More>>>>) )\n+\t  {\n+\t    // need to write back at least one of __more after calling __fun\n+\t    auto&& __first = [&](auto... __args) constexpr\n+\t    {\n+\t      auto __r = __fun(__tuple_element_meta<_Tp, _Abi0, 0>(), first,\n+\t\t\t       __args...);\n+\t      [[maybe_unused]] auto&& __ignore_me = {(\n+\t\t[](auto&& __dst, const auto& __src) {\n+\t\t  if constexpr (is_assignable_v<decltype(__dst),\n+\t\t\t\t\t\tdecltype(__dst)>)\n+\t\t    {\n+\t\t      __dst.template _M_assign_front<__offset<decltype(__dst)>>(\n+\t\t\t__src);\n+\t\t    }\n+\t\t}(static_cast<_More&&>(__more), __args),\n+\t\t0)...};\n+\t      return __r;\n+\t    }\n+\t    (_M_extract_argument(__more)...);\n+\t    if constexpr (_S_tuple_size == 1)\n+\t      return {__first};\n+\t    else\n+\t      return {__first,\n+\t\t      second._M_apply_per_chunk(static_cast<_Fp&&>(__fun),\n+\t\t\t\t\t\t_M_skip_argument(__more)...)};\n+\t  }\n+\telse\n+\t  {\n+\t    auto&& __first = __fun(__tuple_element_meta<_Tp, _Abi0, 0>(), first,\n+\t\t\t\t   _M_extract_argument(__more)...);\n+\t    if constexpr (_S_tuple_size == 1)\n+\t      return {__first};\n+\t    else\n+\t      return {__first,\n+\t\t      second._M_apply_per_chunk(static_cast<_Fp&&>(__fun),\n+\t\t\t\t\t\t_M_skip_argument(__more)...)};\n+\t  }\n+      }\n+\n+    template <typename _R = _Tp, typename _Fp, typename... _More>\n+      _GLIBCXX_SIMD_INTRINSIC auto _M_apply_r(_Fp&& __fun,\n+\t\t\t\t\t      const _More&... __more) const\n+      {\n+\tauto&& __first = __fun(__tuple_element_meta<_Tp, _Abi0, 0>(), first,\n+\t\t\t       __more.first...);\n+\tif constexpr (_S_tuple_size == 1)\n+\t  return __first;\n+\telse\n+\t  return __simd_tuple_concat<_R>(\n+\t    __first, second.template _M_apply_r<_R>(static_cast<_Fp&&>(__fun),\n+\t\t\t\t\t\t    __more.second...));\n+      }\n+\n+    template <typename _Fp, typename... _More>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr friend _SanitizedBitMask<_S_size()>\n+      _M_test(const _Fp& __fun, const _SimdTuple& __x, const _More&... __more)\n+      {\n+\tconst _SanitizedBitMask<_S_first_size> __first\n+\t  = _Abi0::_MaskImpl::_S_to_bits(\n+\t    __fun(__tuple_element_meta<_Tp, _Abi0, 0>(), __x.first,\n+\t\t  __more.first...));\n+\tif constexpr (_S_tuple_size == 1)\n+\t  return __first;\n+\telse\n+\t  return _M_test(__fun, __x.second, __more.second...)\n+\t    ._M_prepend(__first);\n+      }\n+\n+    template <typename _Up, _Up _I>\n+      _GLIBCXX_SIMD_INTRINSIC constexpr _Tp\n+      operator[](integral_constant<_Up, _I>) const noexcept\n+      {\n+\tif constexpr (_I < simd_size_v<_Tp, _Abi0>)\n+\t  return _M_subscript_read(_I);\n+\telse\n+\t  return second[integral_constant<_Up, _I - simd_size_v<_Tp, _Abi0>>()];\n+      }\n+\n+    _Tp operator[](size_t __i) const noexcept\n+    {\n+      if constexpr (_S_tuple_size == 1)\n+\treturn _M_subscript_read(__i);\n+      else\n+\t{\n+#ifdef _GLIBCXX_SIMD_USE_ALIASING_LOADS\n+\t  return reinterpret_cast<const __may_alias<_Tp>*>(this)[__i];\n+#else\n+\t  if constexpr (__is_scalar_abi<_Abi0>())\n+\t    {\n+\t      const _Tp* ptr = &first;\n+\t      return ptr[__i];\n+\t    }\n+\t  else\n+\t    return __i < simd_size_v<_Tp, _Abi0>\n+\t\t     ? _M_subscript_read(__i)\n+\t\t     : second[__i - simd_size_v<_Tp, _Abi0>];\n+#endif\n+\t}\n+    }\n+\n+    void _M_set(size_t __i, _Tp __val) noexcept\n+    {\n+      if constexpr (_S_tuple_size == 1)\n+\treturn _M_subscript_write(__i, __val);\n+      else\n+\t{\n+#ifdef _GLIBCXX_SIMD_USE_ALIASING_LOADS\n+\t  reinterpret_cast<__may_alias<_Tp>*>(this)[__i] = __val;\n+#else\n+\t  if (__i < simd_size_v<_Tp, _Abi0>)\n+\t    _M_subscript_write(__i, __val);\n+\t  else\n+\t    second._M_set(__i - simd_size_v<_Tp, _Abi0>, __val);\n+#endif\n+\t}\n+    }\n+\n+  private:\n+    // _M_subscript_read/_write {{{\n+    _Tp _M_subscript_read([[maybe_unused]] size_t __i) const noexcept\n+    {\n+      if constexpr (__is_vectorizable_v<_FirstType>)\n+\treturn first;\n+      else\n+\treturn first[__i];\n+    }\n+\n+    void _M_subscript_write([[maybe_unused]] size_t __i, _Tp __y) noexcept\n+    {\n+      if constexpr (__is_vectorizable_v<_FirstType>)\n+\tfirst = __y;\n+      else\n+\tfirst._M_set(__i, __y);\n+    }\n+\n+    // }}}\n+  };\n+\n+// __make_simd_tuple {{{1\n+template <typename _Tp, typename _A0>\n+  _GLIBCXX_SIMD_INTRINSIC _SimdTuple<_Tp, _A0>\n+  __make_simd_tuple(simd<_Tp, _A0> __x0)\n+  { return {__data(__x0)}; }\n+\n+template <typename _Tp, typename _A0, typename... _As>\n+  _GLIBCXX_SIMD_INTRINSIC _SimdTuple<_Tp, _A0, _As...>\n+  __make_simd_tuple(const simd<_Tp, _A0>& __x0, const simd<_Tp, _As>&... __xs)\n+  { return {__data(__x0), __make_simd_tuple(__xs...)}; }\n+\n+template <typename _Tp, typename _A0>\n+  _GLIBCXX_SIMD_INTRINSIC _SimdTuple<_Tp, _A0>\n+  __make_simd_tuple(const typename _SimdTraits<_Tp, _A0>::_SimdMember& __arg0)\n+  { return {__arg0}; }\n+\n+template <typename _Tp, typename _A0, typename _A1, typename... _Abis>\n+  _GLIBCXX_SIMD_INTRINSIC _SimdTuple<_Tp, _A0, _A1, _Abis...>\n+  __make_simd_tuple(\n+    const typename _SimdTraits<_Tp, _A0>::_SimdMember& __arg0,\n+    const typename _SimdTraits<_Tp, _A1>::_SimdMember& __arg1,\n+    const typename _SimdTraits<_Tp, _Abis>::_SimdMember&... __args)\n+  { return {__arg0, __make_simd_tuple<_Tp, _A1, _Abis...>(__arg1, __args...)}; }\n+\n+// __to_simd_tuple {{{1\n+template <typename _Tp, size_t _Np, typename _V, size_t _NV, typename... _VX>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr __fixed_size_storage_t<_Tp, _Np>\n+  __to_simd_tuple(const array<_V, _NV>& __from, const _VX... __fromX);\n+\n+template <typename _Tp, size_t _Np,\n+\t  size_t _Offset = 0, // skip this many elements in __from0\n+\t  typename _R = __fixed_size_storage_t<_Tp, _Np>, typename _V0,\n+\t  typename _V0VT = _VectorTraits<_V0>, typename... _VX>\n+  _GLIBCXX_SIMD_INTRINSIC _R constexpr __to_simd_tuple(const _V0 __from0,\n+\t\t\t\t\t\t       const _VX... __fromX)\n+  {\n+    static_assert(is_same_v<typename _V0VT::value_type, _Tp>);\n+    static_assert(_Offset < _V0VT::_S_full_size);\n+    using _R0 = __vector_type_t<_Tp, _R::_S_first_size>;\n+    if constexpr (_R::_S_tuple_size == 1)\n+      {\n+\tif constexpr (_Np == 1)\n+\t  return _R{__from0[_Offset]};\n+\telse if constexpr (_Offset == 0 && _V0VT::_S_full_size >= _Np)\n+\t  return _R{__intrin_bitcast<_R0>(__from0)};\n+\telse if constexpr (_Offset * 2 == _V0VT::_S_full_size\n+\t\t\t   && _V0VT::_S_full_size / 2 >= _Np)\n+\t  return _R{__intrin_bitcast<_R0>(__extract_part<1, 2>(__from0))};\n+\telse if constexpr (_Offset * 4 == _V0VT::_S_full_size\n+\t\t\t   && _V0VT::_S_full_size / 4 >= _Np)\n+\t  return _R{__intrin_bitcast<_R0>(__extract_part<1, 4>(__from0))};\n+\telse\n+\t  __assert_unreachable<_Tp>();\n+      }\n+    else\n+      {\n+\tif constexpr (1 == _R::_S_first_size)\n+\t  { // extract one scalar and recurse\n+\t    if constexpr (_Offset + 1 < _V0VT::_S_full_size)\n+\t      return _R{__from0[_Offset],\n+\t\t\t__to_simd_tuple<_Tp, _Np - 1, _Offset + 1>(__from0,\n+\t\t\t\t\t\t\t\t   __fromX...)};\n+\t    else\n+\t      return _R{__from0[_Offset],\n+\t\t\t__to_simd_tuple<_Tp, _Np - 1, 0>(__fromX...)};\n+\t  }\n+\n+\t// place __from0 into _R::first and recurse for __fromX -> _R::second\n+\telse if constexpr (_V0VT::_S_full_size == _R::_S_first_size\n+\t\t\t   && _Offset == 0)\n+\t  return _R{__from0,\n+\t\t    __to_simd_tuple<_Tp, _Np - _R::_S_first_size>(__fromX...)};\n+\n+\t// place lower part of __from0 into _R::first and recurse with _Offset\n+\telse if constexpr (_V0VT::_S_full_size > _R::_S_first_size\n+\t\t\t   && _Offset == 0)\n+\t  return _R{__intrin_bitcast<_R0>(__from0),\n+\t\t    __to_simd_tuple<_Tp, _Np - _R::_S_first_size,\n+\t\t\t\t    _R::_S_first_size>(__from0, __fromX...)};\n+\n+\t// place lower part of second quarter of __from0 into _R::first and\n+\t// recurse with _Offset\n+\telse if constexpr (_Offset * 4 == _V0VT::_S_full_size\n+\t\t\t   && _V0VT::_S_full_size >= 4 * _R::_S_first_size)\n+\t  return _R{__intrin_bitcast<_R0>(__extract_part<2, 4>(__from0)),\n+\t\t    __to_simd_tuple<_Tp, _Np - _R::_S_first_size,\n+\t\t\t\t    _Offset + _R::_S_first_size>(__from0,\n+\t\t\t\t\t\t\t\t __fromX...)};\n+\n+\t// place lower half of high half of __from0 into _R::first and recurse\n+\t// with _Offset\n+\telse if constexpr (_Offset * 2 == _V0VT::_S_full_size\n+\t\t\t   && _V0VT::_S_full_size >= 4 * _R::_S_first_size)\n+\t  return _R{__intrin_bitcast<_R0>(__extract_part<2, 4>(__from0)),\n+\t\t    __to_simd_tuple<_Tp, _Np - _R::_S_first_size,\n+\t\t\t\t    _Offset + _R::_S_first_size>(__from0,\n+\t\t\t\t\t\t\t\t __fromX...)};\n+\n+\t// place high half of __from0 into _R::first and recurse with __fromX\n+\telse if constexpr (_Offset * 2 == _V0VT::_S_full_size\n+\t\t\t   && _V0VT::_S_full_size / 2 >= _R::_S_first_size)\n+\t  return _R{__intrin_bitcast<_R0>(__extract_part<1, 2>(__from0)),\n+\t\t    __to_simd_tuple<_Tp, _Np - _R::_S_first_size, 0>(\n+\t\t      __fromX...)};\n+\n+\t// ill-formed if some unforseen pattern is needed\n+\telse\n+\t  __assert_unreachable<_Tp>();\n+      }\n+  }\n+\n+template <typename _Tp, size_t _Np, typename _V, size_t _NV, typename... _VX>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr __fixed_size_storage_t<_Tp, _Np>\n+  __to_simd_tuple(const array<_V, _NV>& __from, const _VX... __fromX)\n+  {\n+    if constexpr (is_same_v<_Tp, _V>)\n+      {\n+\tstatic_assert(\n+\t  sizeof...(_VX) == 0,\n+\t  \"An array of scalars must be the last argument to __to_simd_tuple\");\n+\treturn __call_with_subscripts(\n+\t  __from,\n+\t  make_index_sequence<_NV>(), [&](const auto... __args) constexpr {\n+\t    return __simd_tuple_concat(\n+\t      _SimdTuple<_Tp, simd_abi::scalar>{__args}..., _SimdTuple<_Tp>());\n+\t  });\n+      }\n+    else\n+      return __call_with_subscripts(\n+\t__from,\n+\tmake_index_sequence<_NV>(), [&](const auto... __args) constexpr {\n+\t  return __to_simd_tuple<_Tp, _Np>(__args..., __fromX...);\n+\t});\n+  }\n+\n+template <size_t, typename _Tp>\n+  using __to_tuple_helper = _Tp;\n+\n+template <typename _Tp, typename _A0, size_t _NOut, size_t _Np,\n+\t  size_t... _Indexes>\n+  _GLIBCXX_SIMD_INTRINSIC __fixed_size_storage_t<_Tp, _NOut>\n+  __to_simd_tuple_impl(index_sequence<_Indexes...>,\n+      const array<__vector_type_t<_Tp, simd_size_v<_Tp, _A0>>, _Np>& __args)\n+  {\n+    return __make_simd_tuple<_Tp, __to_tuple_helper<_Indexes, _A0>...>(\n+      __args[_Indexes]...);\n+  }\n+\n+template <typename _Tp, typename _A0, size_t _NOut, size_t _Np,\n+\t  typename _R = __fixed_size_storage_t<_Tp, _NOut>>\n+  _GLIBCXX_SIMD_INTRINSIC _R\n+  __to_simd_tuple_sized(\n+    const array<__vector_type_t<_Tp, simd_size_v<_Tp, _A0>>, _Np>& __args)\n+  {\n+    static_assert(_Np * simd_size_v<_Tp, _A0> >= _NOut);\n+    return __to_simd_tuple_impl<_Tp, _A0, _NOut>(\n+      make_index_sequence<_R::_S_tuple_size>(), __args);\n+  }\n+\n+// __optimize_simd_tuple {{{1\n+template <typename _Tp>\n+  _GLIBCXX_SIMD_INTRINSIC _SimdTuple<_Tp>\n+  __optimize_simd_tuple(const _SimdTuple<_Tp>)\n+  { return {}; }\n+\n+template <typename _Tp, typename _Ap>\n+  _GLIBCXX_SIMD_INTRINSIC const _SimdTuple<_Tp, _Ap>&\n+  __optimize_simd_tuple(const _SimdTuple<_Tp, _Ap>& __x)\n+  { return __x; }\n+\n+template <typename _Tp, typename _A0, typename _A1, typename... _Abis,\n+\t  typename _R = __fixed_size_storage_t<\n+\t    _Tp, _SimdTuple<_Tp, _A0, _A1, _Abis...>::_S_size()>>\n+  _GLIBCXX_SIMD_INTRINSIC _R\n+  __optimize_simd_tuple(const _SimdTuple<_Tp, _A0, _A1, _Abis...>& __x)\n+  {\n+    using _Tup = _SimdTuple<_Tp, _A0, _A1, _Abis...>;\n+    if constexpr (is_same_v<_R, _Tup>)\n+      return __x;\n+    else if constexpr (is_same_v<typename _R::_FirstType,\n+\t\t\t\t typename _Tup::_FirstType>)\n+      return {__x.first, __optimize_simd_tuple(__x.second)};\n+    else if constexpr (__is_scalar_abi<_A0>()\n+\t\t       || _A0::template _S_is_partial<_Tp>)\n+      return {__generate_from_n_evaluations<_R::_S_first_size,\n+\t\t\t\t\t    typename _R::_FirstType>(\n+\t\t[&](auto __i) { return __x[__i]; }),\n+\t      __optimize_simd_tuple(\n+\t\t__simd_tuple_pop_front<_R::_S_first_size>(__x))};\n+    else if constexpr (is_same_v<_A0, _A1>\n+\t&& _R::_S_first_size == simd_size_v<_Tp, _A0> + simd_size_v<_Tp, _A1>)\n+      return {__concat(__x.template _M_at<0>(), __x.template _M_at<1>()),\n+\t      __optimize_simd_tuple(__x.second.second)};\n+    else if constexpr (sizeof...(_Abis) >= 2\n+\t&& _R::_S_first_size == (4 * simd_size_v<_Tp, _A0>)\n+\t&& simd_size_v<_Tp, _A0> == __simd_tuple_element_t<\n+\t    (sizeof...(_Abis) >= 2 ? 3 : 0), _Tup>::size())\n+      return {\n+\t__concat(__concat(__x.template _M_at<0>(), __x.template _M_at<1>()),\n+\t\t __concat(__x.template _M_at<2>(), __x.template _M_at<3>())),\n+\t__optimize_simd_tuple(__x.second.second.second.second)};\n+    else\n+      {\n+\tstatic_assert(sizeof(_R) == sizeof(__x));\n+\t_R __r;\n+\t__builtin_memcpy(__r._M_as_charptr(), __x._M_as_charptr(),\n+\t\t\t sizeof(_Tp) * _R::_S_size());\n+\treturn __r;\n+      }\n+  }\n+\n+// __for_each(const _SimdTuple &, Fun) {{{1\n+template <size_t _Offset = 0, typename _Tp, typename _A0, typename _Fp>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr void\n+  __for_each(const _SimdTuple<_Tp, _A0>& __t, _Fp&& __fun)\n+  { static_cast<_Fp&&>(__fun)(__make_meta<_Offset>(__t), __t.first); }\n+\n+template <size_t _Offset = 0, typename _Tp, typename _A0, typename _A1,\n+\t  typename... _As, typename _Fp>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr void\n+  __for_each(const _SimdTuple<_Tp, _A0, _A1, _As...>& __t, _Fp&& __fun)\n+  {\n+    __fun(__make_meta<_Offset>(__t), __t.first);\n+    __for_each<_Offset + simd_size<_Tp, _A0>::value>(__t.second,\n+\t\t\t\t\t\t     static_cast<_Fp&&>(__fun));\n+  }\n+\n+// __for_each(_SimdTuple &, Fun) {{{1\n+template <size_t _Offset = 0, typename _Tp, typename _A0, typename _Fp>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr void\n+  __for_each(_SimdTuple<_Tp, _A0>& __t, _Fp&& __fun)\n+  { static_cast<_Fp&&>(__fun)(__make_meta<_Offset>(__t), __t.first); }\n+\n+template <size_t _Offset = 0, typename _Tp, typename _A0, typename _A1,\n+\t  typename... _As, typename _Fp>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr void\n+  __for_each(_SimdTuple<_Tp, _A0, _A1, _As...>& __t, _Fp&& __fun)\n+  {\n+    __fun(__make_meta<_Offset>(__t), __t.first);\n+    __for_each<_Offset + simd_size<_Tp, _A0>::value>(__t.second,\n+\t\t\t\t\t\t     static_cast<_Fp&&>(__fun));\n+  }\n+\n+// __for_each(_SimdTuple &, const _SimdTuple &, Fun) {{{1\n+template <size_t _Offset = 0, typename _Tp, typename _A0, typename _Fp>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr void\n+  __for_each(_SimdTuple<_Tp, _A0>& __a, const _SimdTuple<_Tp, _A0>& __b,\n+\t     _Fp&& __fun)\n+  {\n+    static_cast<_Fp&&>(__fun)(__make_meta<_Offset>(__a), __a.first, __b.first);\n+  }\n+\n+template <size_t _Offset = 0, typename _Tp, typename _A0, typename _A1,\n+\t  typename... _As, typename _Fp>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr void\n+  __for_each(_SimdTuple<_Tp, _A0, _A1, _As...>& __a,\n+\t     const _SimdTuple<_Tp, _A0, _A1, _As...>& __b, _Fp&& __fun)\n+  {\n+    __fun(__make_meta<_Offset>(__a), __a.first, __b.first);\n+    __for_each<_Offset + simd_size<_Tp, _A0>::value>(__a.second, __b.second,\n+\t\t\t\t\t\t     static_cast<_Fp&&>(__fun));\n+  }\n+\n+// __for_each(const _SimdTuple &, const _SimdTuple &, Fun) {{{1\n+template <size_t _Offset = 0, typename _Tp, typename _A0, typename _Fp>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr void\n+  __for_each(const _SimdTuple<_Tp, _A0>& __a, const _SimdTuple<_Tp, _A0>& __b,\n+\t     _Fp&& __fun)\n+  {\n+    static_cast<_Fp&&>(__fun)(__make_meta<_Offset>(__a), __a.first, __b.first);\n+  }\n+\n+template <size_t _Offset = 0, typename _Tp, typename _A0, typename _A1,\n+\t  typename... _As, typename _Fp>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr void\n+  __for_each(const _SimdTuple<_Tp, _A0, _A1, _As...>& __a,\n+\t     const _SimdTuple<_Tp, _A0, _A1, _As...>& __b, _Fp&& __fun)\n+  {\n+    __fun(__make_meta<_Offset>(__a), __a.first, __b.first);\n+    __for_each<_Offset + simd_size<_Tp, _A0>::value>(__a.second, __b.second,\n+\t\t\t\t\t\t     static_cast<_Fp&&>(__fun));\n+  }\n+\n+// }}}1\n+// __extract_part(_SimdTuple) {{{\n+template <int _Index, int _Total, int _Combine, typename _Tp, typename _A0,\n+\t  typename... _As>\n+  _GLIBCXX_SIMD_INTRINSIC auto // __vector_type_t or _SimdTuple\n+  __extract_part(const _SimdTuple<_Tp, _A0, _As...>& __x)\n+  {\n+    // worst cases:\n+    // (a) 4, 4, 4 => 3, 3, 3, 3 (_Total = 4)\n+    // (b) 2, 2, 2 => 3, 3       (_Total = 2)\n+    // (c) 4, 2 => 2, 2, 2       (_Total = 3)\n+    using _Tuple = _SimdTuple<_Tp, _A0, _As...>;\n+    static_assert(_Index + _Combine <= _Total && _Index >= 0 && _Total >= 1);\n+    constexpr size_t _Np = _Tuple::_S_size();\n+    static_assert(_Np >= _Total && _Np % _Total == 0);\n+    constexpr size_t __values_per_part = _Np / _Total;\n+    [[maybe_unused]] constexpr size_t __values_to_skip\n+      = _Index * __values_per_part;\n+    constexpr size_t __return_size = __values_per_part * _Combine;\n+    using _RetAbi = simd_abi::deduce_t<_Tp, __return_size>;\n+\n+    // handle (optimize) the simple cases\n+    if constexpr (_Index == 0 && _Tuple::_S_first_size == __return_size)\n+      return __x.first._M_data;\n+    else if constexpr (_Index == 0 && _Total == _Combine)\n+      return __x;\n+    else if constexpr (_Index == 0 && _Tuple::_S_first_size >= __return_size)\n+      return __intrin_bitcast<__vector_type_t<_Tp, __return_size>>(\n+\t__as_vector(__x.first));\n+\n+    // recurse to skip unused data members at the beginning of _SimdTuple\n+    else if constexpr (__values_to_skip >= _Tuple::_S_first_size)\n+      { // recurse\n+\tif constexpr (_Tuple::_S_first_size % __values_per_part == 0)\n+\t  {\n+\t    constexpr int __parts_in_first\n+\t      = _Tuple::_S_first_size / __values_per_part;\n+\t    return __extract_part<_Index - __parts_in_first,\n+\t\t\t\t  _Total - __parts_in_first, _Combine>(\n+\t      __x.second);\n+\t  }\n+\telse\n+\t  return __extract_part<__values_to_skip - _Tuple::_S_first_size,\n+\t\t\t\t_Np - _Tuple::_S_first_size, __return_size>(\n+\t    __x.second);\n+      }\n+\n+    // extract from multiple _SimdTuple data members\n+    else if constexpr (__return_size > _Tuple::_S_first_size - __values_to_skip)\n+      {\n+#ifdef _GLIBCXX_SIMD_USE_ALIASING_LOADS\n+\tconst __may_alias<_Tp>* const element_ptr\n+\t  = reinterpret_cast<const __may_alias<_Tp>*>(&__x) + __values_to_skip;\n+\treturn __as_vector(simd<_Tp, _RetAbi>(element_ptr, element_aligned));\n+#else\n+\t[[maybe_unused]] constexpr size_t __offset = __values_to_skip;\n+\treturn __as_vector(simd<_Tp, _RetAbi>([&](auto __i) constexpr {\n+\t  constexpr _SizeConstant<__i + __offset> __k;\n+\t  return __x[__k];\n+\t}));\n+#endif\n+      }\n+\n+    // all of the return values are in __x.first\n+    else if constexpr (_Tuple::_S_first_size % __values_per_part == 0)\n+      return __extract_part<_Index, _Tuple::_S_first_size / __values_per_part,\n+\t\t\t    _Combine>(__x.first);\n+    else\n+      return __extract_part<__values_to_skip, _Tuple::_S_first_size,\n+\t\t\t    _Combine * __values_per_part>(__x.first);\n+  }\n+\n+// }}}\n+// __fixed_size_storage_t<_Tp, _Np>{{{\n+template <typename _Tp, int _Np, typename _Tuple,\n+\t  typename _Next = simd<_Tp, _AllNativeAbis::_BestAbi<_Tp, _Np>>,\n+\t  int _Remain = _Np - int(_Next::size())>\n+  struct __fixed_size_storage_builder;\n+\n+template <typename _Tp, int _Np>\n+  struct __fixed_size_storage\n+  : public __fixed_size_storage_builder<_Tp, _Np, _SimdTuple<_Tp>> {};\n+\n+template <typename _Tp, int _Np, typename... _As, typename _Next>\n+  struct __fixed_size_storage_builder<_Tp, _Np, _SimdTuple<_Tp, _As...>, _Next,\n+\t\t\t\t      0>\n+  { using type = _SimdTuple<_Tp, _As..., typename _Next::abi_type>; };\n+\n+template <typename _Tp, int _Np, typename... _As, typename _Next, int _Remain>\n+  struct __fixed_size_storage_builder<_Tp, _Np, _SimdTuple<_Tp, _As...>, _Next,\n+\t\t\t\t      _Remain>\n+  {\n+    using type = typename __fixed_size_storage_builder<\n+      _Tp, _Remain, _SimdTuple<_Tp, _As..., typename _Next::abi_type>>::type;\n+  };\n+\n+// }}}\n+// _AbisInSimdTuple {{{\n+template <typename _Tp>\n+  struct _SeqOp;\n+\n+template <size_t _I0, size_t... _Is>\n+  struct _SeqOp<index_sequence<_I0, _Is...>>\n+  {\n+    using _FirstPlusOne = index_sequence<_I0 + 1, _Is...>;\n+    using _NotFirstPlusOne = index_sequence<_I0, (_Is + 1)...>;\n+    template <size_t _First, size_t _Add>\n+    using _Prepend = index_sequence<_First, _I0 + _Add, (_Is + _Add)...>;\n+  };\n+\n+template <typename _Tp>\n+  struct _AbisInSimdTuple;\n+\n+template <typename _Tp>\n+  struct _AbisInSimdTuple<_SimdTuple<_Tp>>\n+  {\n+    using _Counts = index_sequence<0>;\n+    using _Begins = index_sequence<0>;\n+  };\n+\n+template <typename _Tp, typename _Ap>\n+  struct _AbisInSimdTuple<_SimdTuple<_Tp, _Ap>>\n+  {\n+    using _Counts = index_sequence<1>;\n+    using _Begins = index_sequence<0>;\n+  };\n+\n+template <typename _Tp, typename _A0, typename... _As>\n+  struct _AbisInSimdTuple<_SimdTuple<_Tp, _A0, _A0, _As...>>\n+  {\n+    using _Counts = typename _SeqOp<typename _AbisInSimdTuple<\n+      _SimdTuple<_Tp, _A0, _As...>>::_Counts>::_FirstPlusOne;\n+    using _Begins = typename _SeqOp<typename _AbisInSimdTuple<\n+      _SimdTuple<_Tp, _A0, _As...>>::_Begins>::_NotFirstPlusOne;\n+  };\n+\n+template <typename _Tp, typename _A0, typename _A1, typename... _As>\n+  struct _AbisInSimdTuple<_SimdTuple<_Tp, _A0, _A1, _As...>>\n+  {\n+    using _Counts = typename _SeqOp<typename _AbisInSimdTuple<\n+      _SimdTuple<_Tp, _A1, _As...>>::_Counts>::template _Prepend<1, 0>;\n+    using _Begins = typename _SeqOp<typename _AbisInSimdTuple<\n+      _SimdTuple<_Tp, _A1, _As...>>::_Begins>::template _Prepend<0, 1>;\n+  };\n+\n+// }}}\n+// __autocvt_to_simd {{{\n+template <typename _Tp, bool = is_arithmetic_v<__remove_cvref_t<_Tp>>>\n+  struct __autocvt_to_simd\n+  {\n+    _Tp _M_data;\n+    using _TT = __remove_cvref_t<_Tp>;\n+\n+    operator _TT()\n+    { return _M_data; }\n+\n+    operator _TT&()\n+    {\n+      static_assert(is_lvalue_reference<_Tp>::value, \"\");\n+      static_assert(!is_const<_Tp>::value, \"\");\n+      return _M_data;\n+    }\n+\n+    operator _TT*()\n+    {\n+      static_assert(is_lvalue_reference<_Tp>::value, \"\");\n+      static_assert(!is_const<_Tp>::value, \"\");\n+      return &_M_data;\n+    }\n+\n+    constexpr inline __autocvt_to_simd(_Tp dd) : _M_data(dd) {}\n+\n+    template <typename _Abi>\n+      operator simd<typename _TT::value_type, _Abi>()\n+      { return {__private_init, _M_data}; }\n+\n+    template <typename _Abi>\n+      operator simd<typename _TT::value_type, _Abi>&()\n+      {\n+\treturn *reinterpret_cast<simd<typename _TT::value_type, _Abi>*>(\n+\t  &_M_data);\n+      }\n+\n+    template <typename _Abi>\n+      operator simd<typename _TT::value_type, _Abi>*()\n+      {\n+\treturn reinterpret_cast<simd<typename _TT::value_type, _Abi>*>(\n+\t  &_M_data);\n+      }\n+  };\n+\n+template <typename _Tp>\n+  __autocvt_to_simd(_Tp &&) -> __autocvt_to_simd<_Tp>;\n+\n+template <typename _Tp>\n+  struct __autocvt_to_simd<_Tp, true>\n+  {\n+    using _TT = __remove_cvref_t<_Tp>;\n+    _Tp _M_data;\n+    fixed_size_simd<_TT, 1> _M_fd;\n+\n+    constexpr inline __autocvt_to_simd(_Tp dd) : _M_data(dd), _M_fd(_M_data) {}\n+\n+    ~__autocvt_to_simd()\n+    { _M_data = __data(_M_fd).first; }\n+\n+    operator fixed_size_simd<_TT, 1>()\n+    { return _M_fd; }\n+\n+    operator fixed_size_simd<_TT, 1> &()\n+    {\n+      static_assert(is_lvalue_reference<_Tp>::value, \"\");\n+      static_assert(!is_const<_Tp>::value, \"\");\n+      return _M_fd;\n+    }\n+\n+    operator fixed_size_simd<_TT, 1> *()\n+    {\n+      static_assert(is_lvalue_reference<_Tp>::value, \"\");\n+      static_assert(!is_const<_Tp>::value, \"\");\n+      return &_M_fd;\n+    }\n+  };\n+\n+// }}}\n+\n+struct _CommonImplFixedSize;\n+template <int _Np> struct _SimdImplFixedSize;\n+template <int _Np> struct _MaskImplFixedSize;\n+// simd_abi::_Fixed {{{\n+template <int _Np>\n+  struct simd_abi::_Fixed\n+  {\n+    template <typename _Tp> static constexpr size_t _S_size = _Np;\n+    template <typename _Tp> static constexpr size_t _S_full_size = _Np;\n+    // validity traits {{{\n+    struct _IsValidAbiTag : public __bool_constant<(_Np > 0)> {};\n+\n+    template <typename _Tp>\n+      struct _IsValidSizeFor\n+      : __bool_constant<(_Np <= simd_abi::max_fixed_size<_Tp>)> {};\n+\n+    template <typename _Tp>\n+      struct _IsValid : conjunction<_IsValidAbiTag, __is_vectorizable<_Tp>,\n+\t\t\t\t    _IsValidSizeFor<_Tp>> {};\n+\n+    template <typename _Tp>\n+      static constexpr bool _S_is_valid_v = _IsValid<_Tp>::value;\n+\n+    // }}}\n+    // _S_masked {{{\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr _SanitizedBitMask<_Np>\n+    _S_masked(_BitMask<_Np> __x)\n+    { return __x._M_sanitized(); }\n+\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr _SanitizedBitMask<_Np>\n+    _S_masked(_SanitizedBitMask<_Np> __x)\n+    { return __x; }\n+\n+    // }}}\n+    // _*Impl {{{\n+    using _CommonImpl = _CommonImplFixedSize;\n+    using _SimdImpl = _SimdImplFixedSize<_Np>;\n+    using _MaskImpl = _MaskImplFixedSize<_Np>;\n+\n+    // }}}\n+    // __traits {{{\n+    template <typename _Tp, bool = _S_is_valid_v<_Tp>>\n+      struct __traits : _InvalidTraits {};\n+\n+    template <typename _Tp>\n+      struct __traits<_Tp, true>\n+      {\n+\tusing _IsValid = true_type;\n+\tusing _SimdImpl = _SimdImplFixedSize<_Np>;\n+\tusing _MaskImpl = _MaskImplFixedSize<_Np>;\n+\n+\t// simd and simd_mask member types {{{\n+\tusing _SimdMember = __fixed_size_storage_t<_Tp, _Np>;\n+\tusing _MaskMember = _SanitizedBitMask<_Np>;\n+\n+\tstatic constexpr size_t _S_simd_align\n+\t  = std::__bit_ceil(_Np * sizeof(_Tp));\n+\n+\tstatic constexpr size_t _S_mask_align = alignof(_MaskMember);\n+\n+\t// }}}\n+\t// _SimdBase / base class for simd, providing extra conversions {{{\n+\tstruct _SimdBase\n+\t{\n+\t  // The following ensures, function arguments are passed via the stack.\n+\t  // This is important for ABI compatibility across TU boundaries\n+\t  _SimdBase(const _SimdBase&) {}\n+\t  _SimdBase() = default;\n+\n+\t  explicit operator const _SimdMember &() const\n+\t  { return static_cast<const simd<_Tp, _Fixed>*>(this)->_M_data; }\n+\n+\t  explicit operator array<_Tp, _Np>() const\n+\t  {\n+\t    array<_Tp, _Np> __r;\n+\t    // _SimdMember can be larger because of higher alignment\n+\t    static_assert(sizeof(__r) <= sizeof(_SimdMember), \"\");\n+\t    __builtin_memcpy(__r.data(), &static_cast<const _SimdMember&>(*this),\n+\t\t\t     sizeof(__r));\n+\t    return __r;\n+\t  }\n+\t};\n+\n+\t// }}}\n+\t// _MaskBase {{{\n+\t// empty. The bitset interface suffices\n+\tstruct _MaskBase {};\n+\n+\t// }}}\n+\t// _SimdCastType {{{\n+\tstruct _SimdCastType\n+\t{\n+\t  _SimdCastType(const array<_Tp, _Np>&);\n+\t  _SimdCastType(const _SimdMember& dd) : _M_data(dd) {}\n+\t  explicit operator const _SimdMember &() const { return _M_data; }\n+\n+\tprivate:\n+\t  const _SimdMember& _M_data;\n+\t};\n+\n+\t// }}}\n+\t// _MaskCastType {{{\n+\tclass _MaskCastType\n+\t{\n+\t  _MaskCastType() = delete;\n+\t};\n+\t// }}}\n+      };\n+    // }}}\n+  };\n+\n+// }}}\n+// _CommonImplFixedSize {{{\n+struct _CommonImplFixedSize\n+{\n+  // _S_store {{{\n+  template <typename _Tp, typename... _As>\n+    _GLIBCXX_SIMD_INTRINSIC static void\n+    _S_store(const _SimdTuple<_Tp, _As...>& __x, void* __addr)\n+    {\n+      constexpr size_t _Np = _SimdTuple<_Tp, _As...>::_S_size();\n+      __builtin_memcpy(__addr, &__x, _Np * sizeof(_Tp));\n+    }\n+\n+  // }}}\n+};\n+\n+// }}}\n+// _SimdImplFixedSize {{{1\n+// fixed_size should not inherit from _SimdMathFallback in order for\n+// specializations in the used _SimdTuple Abis to get used\n+template <int _Np>\n+  struct _SimdImplFixedSize\n+  {\n+    // member types {{{2\n+    using _MaskMember = _SanitizedBitMask<_Np>;\n+\n+    template <typename _Tp>\n+      using _SimdMember = __fixed_size_storage_t<_Tp, _Np>;\n+\n+    template <typename _Tp>\n+      static constexpr size_t _S_tuple_size = _SimdMember<_Tp>::_S_tuple_size;\n+\n+    template <typename _Tp>\n+      using _Simd = simd<_Tp, simd_abi::fixed_size<_Np>>;\n+\n+    template <typename _Tp>\n+      using _TypeTag = _Tp*;\n+\n+    // broadcast {{{2\n+    template <typename _Tp>\n+      static constexpr inline _SimdMember<_Tp> _S_broadcast(_Tp __x) noexcept\n+      {\n+\treturn _SimdMember<_Tp>::_S_generate([&](auto __meta) constexpr {\n+\t  return __meta._S_broadcast(__x);\n+\t});\n+      }\n+\n+    // _S_generator {{{2\n+    template <typename _Fp, typename _Tp>\n+      static constexpr inline _SimdMember<_Tp> _S_generator(_Fp&& __gen,\n+\t\t\t\t\t\t\t    _TypeTag<_Tp>)\n+      {\n+\treturn _SimdMember<_Tp>::_S_generate([&__gen](auto __meta) constexpr {\n+\t  return __meta._S_generator(\n+\t    [&](auto __i) constexpr {\n+\t      return __i < _Np ? __gen(_SizeConstant<__meta._S_offset + __i>())\n+\t\t\t       : 0;\n+\t    },\n+\t    _TypeTag<_Tp>());\n+\t});\n+      }\n+\n+    // _S_load {{{2\n+    template <typename _Tp, typename _Up>\n+      static inline _SimdMember<_Tp> _S_load(const _Up* __mem,\n+\t\t\t\t\t     _TypeTag<_Tp>) noexcept\n+      {\n+\treturn _SimdMember<_Tp>::_S_generate([&](auto __meta) {\n+\t  return __meta._S_load(&__mem[__meta._S_offset], _TypeTag<_Tp>());\n+\t});\n+      }\n+\n+    // _S_masked_load {{{2\n+    template <typename _Tp, typename... _As, typename _Up>\n+      static inline _SimdTuple<_Tp, _As...>\n+      _S_masked_load(const _SimdTuple<_Tp, _As...>& __old,\n+\t\t     const _MaskMember __bits, const _Up* __mem) noexcept\n+      {\n+\tauto __merge = __old;\n+\t__for_each(__merge, [&](auto __meta, auto& __native) {\n+\t  if (__meta._S_submask(__bits).any())\n+#pragma GCC diagnostic push\n+\t  // __mem + __mem._S_offset could be UB ([expr.add]/4.3, but it punts\n+\t  // the responsibility for avoiding UB to the caller of the masked load\n+\t  // via the mask. Consequently, the compiler may assume this branch is\n+\t  // unreachable, if the pointer arithmetic is UB.\n+#pragma GCC diagnostic ignored \"-Warray-bounds\"\n+\t    __native\n+\t      = __meta._S_masked_load(__native, __meta._S_make_mask(__bits),\n+\t\t\t\t      __mem + __meta._S_offset);\n+#pragma GCC diagnostic pop\n+\t});\n+\treturn __merge;\n+      }\n+\n+    // _S_store {{{2\n+    template <typename _Tp, typename _Up>\n+      static inline void _S_store(const _SimdMember<_Tp>& __v, _Up* __mem,\n+\t\t\t\t  _TypeTag<_Tp>) noexcept\n+      {\n+\t__for_each(__v, [&](auto __meta, auto __native) {\n+\t  __meta._S_store(__native, &__mem[__meta._S_offset], _TypeTag<_Tp>());\n+\t});\n+      }\n+\n+    // _S_masked_store {{{2\n+    template <typename _Tp, typename... _As, typename _Up>\n+      static inline void _S_masked_store(const _SimdTuple<_Tp, _As...>& __v,\n+\t\t\t\t\t _Up* __mem,\n+\t\t\t\t\t const _MaskMember __bits) noexcept\n+      {\n+\t__for_each(__v, [&](auto __meta, auto __native) {\n+\t  if (__meta._S_submask(__bits).any())\n+#pragma GCC diagnostic push\n+\t  // __mem + __mem._S_offset could be UB ([expr.add]/4.3, but it punts\n+\t  // the responsibility for avoiding UB to the caller of the masked\n+\t  // store via the mask. Consequently, the compiler may assume this\n+\t  // branch is unreachable, if the pointer arithmetic is UB.\n+#pragma GCC diagnostic ignored \"-Warray-bounds\"\n+\t    __meta._S_masked_store(__native, __mem + __meta._S_offset,\n+\t\t\t\t   __meta._S_make_mask(__bits));\n+#pragma GCC diagnostic pop\n+\t});\n+      }\n+\n+    // negation {{{2\n+    template <typename _Tp, typename... _As>\n+      static inline _MaskMember\n+      _S_negate(const _SimdTuple<_Tp, _As...>& __x) noexcept\n+      {\n+\t_MaskMember __bits = 0;\n+\t__for_each(\n+\t  __x, [&__bits](auto __meta, auto __native) constexpr {\n+\t    __bits\n+\t      |= __meta._S_mask_to_shifted_ullong(__meta._S_negate(__native));\n+\t  });\n+\treturn __bits;\n+      }\n+\n+    // reductions {{{2\n+    template <typename _Tp, typename _BinaryOperation>\n+      static constexpr inline _Tp _S_reduce(const _Simd<_Tp>& __x,\n+\t\t\t\t\t    const _BinaryOperation& __binary_op)\n+      {\n+\tusing _Tup = _SimdMember<_Tp>;\n+\tconst _Tup& __tup = __data(__x);\n+\tif constexpr (_Tup::_S_tuple_size == 1)\n+\t  return _Tup::_FirstAbi::_SimdImpl::_S_reduce(\n+\t    __tup.template _M_simd_at<0>(), __binary_op);\n+\telse if constexpr (_Tup::_S_tuple_size == 2 && _Tup::_S_size() > 2\n+\t\t\t   && _Tup::_SecondType::_S_size() == 1)\n+\t  {\n+\t    return __binary_op(simd<_Tp, simd_abi::scalar>(\n+\t\t\t\t reduce(__tup.template _M_simd_at<0>(),\n+\t\t\t\t\t__binary_op)),\n+\t\t\t       __tup.template _M_simd_at<1>())[0];\n+\t  }\n+\telse if constexpr (_Tup::_S_tuple_size == 2 && _Tup::_S_size() > 4\n+\t\t\t   && _Tup::_SecondType::_S_size() == 2)\n+\t  {\n+\t    return __binary_op(\n+\t      simd<_Tp, simd_abi::scalar>(\n+\t\treduce(__tup.template _M_simd_at<0>(), __binary_op)),\n+\t      simd<_Tp, simd_abi::scalar>(\n+\t\treduce(__tup.template _M_simd_at<1>(), __binary_op)))[0];\n+\t  }\n+\telse\n+\t  {\n+\t    const auto& __x2 = __call_with_n_evaluations<\n+\t      __div_roundup(_Tup::_S_tuple_size, 2)>(\n+\t      [](auto __first_simd, auto... __remaining) {\n+\t\tif constexpr (sizeof...(__remaining) == 0)\n+\t\t  return __first_simd;\n+\t\telse\n+\t\t  {\n+\t\t    using _Tup2\n+\t\t      = _SimdTuple<_Tp,\n+\t\t\t\t   typename decltype(__first_simd)::abi_type,\n+\t\t\t\t   typename decltype(__remaining)::abi_type...>;\n+\t\t    return fixed_size_simd<_Tp, _Tup2::_S_size()>(\n+\t\t      __private_init,\n+\t\t      __make_simd_tuple(__first_simd, __remaining...));\n+\t\t  }\n+\t      },\n+\t      [&](auto __i) {\n+\t\tauto __left = __tup.template _M_simd_at<2 * __i>();\n+\t\tif constexpr (2 * __i + 1 == _Tup::_S_tuple_size)\n+\t\t  return __left;\n+\t\telse\n+\t\t  {\n+\t\t    auto __right = __tup.template _M_simd_at<2 * __i + 1>();\n+\t\t    using _LT = decltype(__left);\n+\t\t    using _RT = decltype(__right);\n+\t\t    if constexpr (_LT::size() == _RT::size())\n+\t\t      return __binary_op(__left, __right);\n+\t\t    else\n+\t\t      {\n+\t\t\t_GLIBCXX_SIMD_USE_CONSTEXPR_API\n+\t\t\ttypename _LT::mask_type __k(\n+\t\t\t  __private_init,\n+\t\t\t  [](auto __j) constexpr { return __j < _RT::size(); });\n+\t\t\t_LT __ext_right = __left;\n+\t\t\twhere(__k, __ext_right)\n+\t\t\t  = __proposed::resizing_simd_cast<_LT>(__right);\n+\t\t\twhere(__k, __left) = __binary_op(__left, __ext_right);\n+\t\t\treturn __left;\n+\t\t      }\n+\t\t  }\n+\t      });\n+\t    return reduce(__x2, __binary_op);\n+\t  }\n+      }\n+\n+    // _S_min, _S_max {{{2\n+    template <typename _Tp, typename... _As>\n+      static inline constexpr _SimdTuple<_Tp, _As...>\n+      _S_min(const _SimdTuple<_Tp, _As...>& __a,\n+\t     const _SimdTuple<_Tp, _As...>& __b)\n+      {\n+\treturn __a._M_apply_per_chunk(\n+\t  [](auto __impl, auto __aa, auto __bb) constexpr {\n+\t    return __impl._S_min(__aa, __bb);\n+\t  },\n+\t  __b);\n+      }\n+\n+    template <typename _Tp, typename... _As>\n+      static inline constexpr _SimdTuple<_Tp, _As...>\n+      _S_max(const _SimdTuple<_Tp, _As...>& __a,\n+\t     const _SimdTuple<_Tp, _As...>& __b)\n+      {\n+\treturn __a._M_apply_per_chunk(\n+\t  [](auto __impl, auto __aa, auto __bb) constexpr {\n+\t    return __impl._S_max(__aa, __bb);\n+\t  },\n+\t  __b);\n+      }\n+\n+    // _S_complement {{{2\n+    template <typename _Tp, typename... _As>\n+      static inline constexpr _SimdTuple<_Tp, _As...>\n+      _S_complement(const _SimdTuple<_Tp, _As...>& __x) noexcept\n+      {\n+\treturn __x._M_apply_per_chunk([](auto __impl, auto __xx) constexpr {\n+\t  return __impl._S_complement(__xx);\n+\t});\n+      }\n+\n+    // _S_unary_minus {{{2\n+    template <typename _Tp, typename... _As>\n+      static inline constexpr _SimdTuple<_Tp, _As...>\n+      _S_unary_minus(const _SimdTuple<_Tp, _As...>& __x) noexcept\n+      {\n+\treturn __x._M_apply_per_chunk([](auto __impl, auto __xx) constexpr {\n+\t  return __impl._S_unary_minus(__xx);\n+\t});\n+      }\n+\n+    // arithmetic operators {{{2\n+\n+#define _GLIBCXX_SIMD_FIXED_OP(name_, op_)                                     \\\n+    template <typename _Tp, typename... _As>                                   \\\n+      static inline constexpr _SimdTuple<_Tp, _As...> name_(                   \\\n+\tconst _SimdTuple<_Tp, _As...> __x, const _SimdTuple<_Tp, _As...> __y)  \\\n+      {                                                                        \\\n+\treturn __x._M_apply_per_chunk(                                         \\\n+\t  [](auto __impl, auto __xx, auto __yy) constexpr {                    \\\n+\t    return __impl.name_(__xx, __yy);                                   \\\n+\t  },                                                                   \\\n+\t  __y);                                                                \\\n+      }\n+\n+    _GLIBCXX_SIMD_FIXED_OP(_S_plus, +)\n+    _GLIBCXX_SIMD_FIXED_OP(_S_minus, -)\n+    _GLIBCXX_SIMD_FIXED_OP(_S_multiplies, *)\n+    _GLIBCXX_SIMD_FIXED_OP(_S_divides, /)\n+    _GLIBCXX_SIMD_FIXED_OP(_S_modulus, %)\n+    _GLIBCXX_SIMD_FIXED_OP(_S_bit_and, &)\n+    _GLIBCXX_SIMD_FIXED_OP(_S_bit_or, |)\n+    _GLIBCXX_SIMD_FIXED_OP(_S_bit_xor, ^)\n+    _GLIBCXX_SIMD_FIXED_OP(_S_bit_shift_left, <<)\n+    _GLIBCXX_SIMD_FIXED_OP(_S_bit_shift_right, >>)\n+#undef _GLIBCXX_SIMD_FIXED_OP\n+\n+    template <typename _Tp, typename... _As>\n+      static inline constexpr _SimdTuple<_Tp, _As...>\n+      _S_bit_shift_left(const _SimdTuple<_Tp, _As...>& __x, int __y)\n+      {\n+\treturn __x._M_apply_per_chunk([__y](auto __impl, auto __xx) constexpr {\n+\t  return __impl._S_bit_shift_left(__xx, __y);\n+\t});\n+      }\n+\n+    template <typename _Tp, typename... _As>\n+      static inline constexpr _SimdTuple<_Tp, _As...>\n+      _S_bit_shift_right(const _SimdTuple<_Tp, _As...>& __x, int __y)\n+      {\n+\treturn __x._M_apply_per_chunk([__y](auto __impl, auto __xx) constexpr {\n+\t  return __impl._S_bit_shift_right(__xx, __y);\n+\t});\n+      }\n+\n+  // math {{{2\n+#define _GLIBCXX_SIMD_APPLY_ON_TUPLE(_RetTp, __name)                           \\\n+    template <typename _Tp, typename... _As, typename... _More>                \\\n+      static inline __fixed_size_storage_t<_RetTp, _Np>                        \\\n+\t_S_##__name(const _SimdTuple<_Tp, _As...>& __x,                        \\\n+\t\t    const _More&... __more)                                    \\\n+      {                                                                        \\\n+\tif constexpr (sizeof...(_More) == 0)                                   \\\n+\t  {                                                                    \\\n+\t    if constexpr (is_same_v<_Tp, _RetTp>)                              \\\n+\t      return __x._M_apply_per_chunk(                                   \\\n+\t\t[](auto __impl, auto __xx) constexpr {                         \\\n+\t\t  using _V = typename decltype(__impl)::simd_type;             \\\n+\t\t  return __data(__name(_V(__private_init, __xx)));             \\\n+\t\t});                                                            \\\n+\t    else                                                               \\\n+\t      return __optimize_simd_tuple(                                    \\\n+\t\t__x.template _M_apply_r<_RetTp>([](auto __impl, auto __xx) {   \\\n+\t\t  return __impl._S_##__name(__xx);                             \\\n+\t\t}));                                                           \\\n+\t  }                                                                    \\\n+\telse if constexpr (                                                    \\\n+\t  is_same_v<                                                           \\\n+\t    _Tp,                                                               \\\n+\t    _RetTp> && (... && is_same_v<_SimdTuple<_Tp, _As...>, _More>) )    \\\n+\t  return __x._M_apply_per_chunk(                                       \\\n+\t    [](auto __impl, auto __xx, auto... __pack) constexpr {             \\\n+\t      using _V = typename decltype(__impl)::simd_type;                 \\\n+\t      return __data(__name(_V(__private_init, __xx),                   \\\n+\t\t\t\t   _V(__private_init, __pack)...));            \\\n+\t    },                                                                 \\\n+\t    __more...);                                                        \\\n+\telse if constexpr (is_same_v<_Tp, _RetTp>)                             \\\n+\t  return __x._M_apply_per_chunk(                                       \\\n+\t    [](auto __impl, auto __xx, auto... __pack) constexpr {             \\\n+\t      using _V = typename decltype(__impl)::simd_type;                 \\\n+\t      return __data(__name(_V(__private_init, __xx),                   \\\n+\t\t\t\t   __autocvt_to_simd(__pack)...));             \\\n+\t    },                                                                 \\\n+\t    __more...);                                                        \\\n+\telse                                                                   \\\n+\t  __assert_unreachable<_Tp>();                                         \\\n+      }\n+\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, acos)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, asin)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, atan)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, atan2)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, cos)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, sin)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, tan)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, acosh)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, asinh)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, atanh)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, cosh)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, sinh)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, tanh)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, exp)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, exp2)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, expm1)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(int, ilogb)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, log)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, log10)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, log1p)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, log2)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, logb)\n+    // modf implemented in simd_math.h\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp,\n+\t\t\t\t scalbn) // double scalbn(double x, int exp);\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, scalbln)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, cbrt)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, abs)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, fabs)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, pow)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, sqrt)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, erf)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, erfc)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, lgamma)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, tgamma)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, trunc)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, ceil)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, floor)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, nearbyint)\n+\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, rint)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(long, lrint)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(long long, llrint)\n+\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, round)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(long, lround)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(long long, llround)\n+\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, ldexp)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, fmod)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, remainder)\n+    // copysign in simd_math.h\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, nextafter)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, fdim)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, fmax)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, fmin)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(_Tp, fma)\n+    _GLIBCXX_SIMD_APPLY_ON_TUPLE(int, fpclassify)\n+#undef _GLIBCXX_SIMD_APPLY_ON_TUPLE\n+\n+    template <typename _Tp, typename... _Abis>\n+      static _SimdTuple<_Tp, _Abis...> _S_remquo(\n+\tconst _SimdTuple<_Tp, _Abis...>& __x,\n+\tconst _SimdTuple<_Tp, _Abis...>& __y,\n+\t__fixed_size_storage_t<int, _SimdTuple<_Tp, _Abis...>::_S_size()>* __z)\n+      {\n+\treturn __x._M_apply_per_chunk(\n+\t  [](auto __impl, const auto __xx, const auto __yy, auto& __zz) {\n+\t    return __impl._S_remquo(__xx, __yy, &__zz);\n+\t  },\n+\t  __y, *__z);\n+      }\n+\n+    template <typename _Tp, typename... _As>\n+      static inline _SimdTuple<_Tp, _As...>\n+      _S_frexp(const _SimdTuple<_Tp, _As...>& __x,\n+\t       __fixed_size_storage_t<int, _Np>& __exp) noexcept\n+      {\n+\treturn __x._M_apply_per_chunk(\n+\t  [](auto __impl, const auto& __a, auto& __b) {\n+\t    return __data(\n+\t      frexp(typename decltype(__impl)::simd_type(__private_init, __a),\n+\t\t    __autocvt_to_simd(__b)));\n+\t  },\n+\t  __exp);\n+      }\n+\n+#define _GLIBCXX_SIMD_TEST_ON_TUPLE_(name_)                                    \\\n+    template <typename _Tp, typename... _As>                                   \\\n+      static inline _MaskMember                                                \\\n+\t_S_##name_(const _SimdTuple<_Tp, _As...>& __x) noexcept                \\\n+      {                                                                        \\\n+\treturn _M_test([](auto __impl,                                         \\\n+\t\t\t  auto __xx) { return __impl._S_##name_(__xx); },      \\\n+\t\t       __x);                                                   \\\n+      }\n+\n+    _GLIBCXX_SIMD_TEST_ON_TUPLE_(isinf)\n+    _GLIBCXX_SIMD_TEST_ON_TUPLE_(isfinite)\n+    _GLIBCXX_SIMD_TEST_ON_TUPLE_(isnan)\n+    _GLIBCXX_SIMD_TEST_ON_TUPLE_(isnormal)\n+    _GLIBCXX_SIMD_TEST_ON_TUPLE_(signbit)\n+#undef _GLIBCXX_SIMD_TEST_ON_TUPLE_\n+\n+    // _S_increment & _S_decrement{{{2\n+    template <typename... _Ts>\n+      _GLIBCXX_SIMD_INTRINSIC static constexpr void\n+      _S_increment(_SimdTuple<_Ts...>& __x)\n+      {\n+\t__for_each(\n+\t  __x, [](auto __meta, auto& native) constexpr {\n+\t    __meta._S_increment(native);\n+\t  });\n+      }\n+\n+    template <typename... _Ts>\n+      _GLIBCXX_SIMD_INTRINSIC static constexpr void\n+      _S_decrement(_SimdTuple<_Ts...>& __x)\n+      {\n+\t__for_each(\n+\t  __x, [](auto __meta, auto& native) constexpr {\n+\t    __meta._S_decrement(native);\n+\t  });\n+      }\n+\n+    // compares {{{2\n+#define _GLIBCXX_SIMD_CMP_OPERATIONS(__cmp)                                    \\\n+    template <typename _Tp, typename... _As>                                   \\\n+      _GLIBCXX_SIMD_INTRINSIC constexpr static _MaskMember                     \\\n+      __cmp(const _SimdTuple<_Tp, _As...>& __x,                                \\\n+\t    const _SimdTuple<_Tp, _As...>& __y)                                \\\n+      {                                                                        \\\n+\treturn _M_test(                                                        \\\n+\t  [](auto __impl, auto __xx, auto __yy) constexpr {                    \\\n+\t    return __impl.__cmp(__xx, __yy);                                   \\\n+\t  },                                                                   \\\n+\t  __x, __y);                                                           \\\n+      }\n+\n+    _GLIBCXX_SIMD_CMP_OPERATIONS(_S_equal_to)\n+    _GLIBCXX_SIMD_CMP_OPERATIONS(_S_not_equal_to)\n+    _GLIBCXX_SIMD_CMP_OPERATIONS(_S_less)\n+    _GLIBCXX_SIMD_CMP_OPERATIONS(_S_less_equal)\n+    _GLIBCXX_SIMD_CMP_OPERATIONS(_S_isless)\n+    _GLIBCXX_SIMD_CMP_OPERATIONS(_S_islessequal)\n+    _GLIBCXX_SIMD_CMP_OPERATIONS(_S_isgreater)\n+    _GLIBCXX_SIMD_CMP_OPERATIONS(_S_isgreaterequal)\n+    _GLIBCXX_SIMD_CMP_OPERATIONS(_S_islessgreater)\n+    _GLIBCXX_SIMD_CMP_OPERATIONS(_S_isunordered)\n+#undef _GLIBCXX_SIMD_CMP_OPERATIONS\n+\n+    // smart_reference access {{{2\n+    template <typename _Tp, typename... _As, typename _Up>\n+      _GLIBCXX_SIMD_INTRINSIC static void _S_set(_SimdTuple<_Tp, _As...>& __v,\n+\t\t\t\t\t\t int __i, _Up&& __x) noexcept\n+      { __v._M_set(__i, static_cast<_Up&&>(__x)); }\n+\n+    // _S_masked_assign {{{2\n+    template <typename _Tp, typename... _As>\n+      _GLIBCXX_SIMD_INTRINSIC static void\n+      _S_masked_assign(const _MaskMember __bits, _SimdTuple<_Tp, _As...>& __lhs,\n+\t\t       const __type_identity_t<_SimdTuple<_Tp, _As...>>& __rhs)\n+      {\n+\t__for_each(\n+\t  __lhs, __rhs,\n+\t  [&](auto __meta, auto& __native_lhs, auto __native_rhs) constexpr {\n+\t    __meta._S_masked_assign(__meta._S_make_mask(__bits), __native_lhs,\n+\t\t\t\t    __native_rhs);\n+\t  });\n+      }\n+\n+    // Optimization for the case where the RHS is a scalar. No need to broadcast\n+    // the scalar to a simd first.\n+    template <typename _Tp, typename... _As>\n+      _GLIBCXX_SIMD_INTRINSIC static void\n+      _S_masked_assign(const _MaskMember __bits, _SimdTuple<_Tp, _As...>& __lhs,\n+\t\t       const __type_identity_t<_Tp> __rhs)\n+      {\n+\t__for_each(\n+\t  __lhs, [&](auto __meta, auto& __native_lhs) constexpr {\n+\t    __meta._S_masked_assign(__meta._S_make_mask(__bits), __native_lhs,\n+\t\t\t\t    __rhs);\n+\t  });\n+      }\n+\n+    // _S_masked_cassign {{{2\n+    template <typename _Op, typename _Tp, typename... _As>\n+      static inline void _S_masked_cassign(const _MaskMember __bits,\n+\t\t\t\t\t   _SimdTuple<_Tp, _As...>& __lhs,\n+\t\t\t\t\t   const _SimdTuple<_Tp, _As...>& __rhs,\n+\t\t\t\t\t   _Op __op)\n+      {\n+\t__for_each(\n+\t  __lhs, __rhs,\n+\t  [&](auto __meta, auto& __native_lhs, auto __native_rhs) constexpr {\n+\t    __meta.template _S_masked_cassign(__meta._S_make_mask(__bits),\n+\t\t\t\t\t      __native_lhs, __native_rhs, __op);\n+\t  });\n+      }\n+\n+    // Optimization for the case where the RHS is a scalar. No need to broadcast\n+    // the scalar to a simd first.\n+    template <typename _Op, typename _Tp, typename... _As>\n+      static inline void _S_masked_cassign(const _MaskMember __bits,\n+\t\t\t\t\t   _SimdTuple<_Tp, _As...>& __lhs,\n+\t\t\t\t\t   const _Tp& __rhs, _Op __op)\n+      {\n+\t__for_each(\n+\t  __lhs, [&](auto __meta, auto& __native_lhs) constexpr {\n+\t    __meta.template _S_masked_cassign(__meta._S_make_mask(__bits),\n+\t\t\t\t\t      __native_lhs, __rhs, __op);\n+\t  });\n+      }\n+\n+    // _S_masked_unary {{{2\n+    template <template <typename> class _Op, typename _Tp, typename... _As>\n+      static inline _SimdTuple<_Tp, _As...>\n+      _S_masked_unary(const _MaskMember __bits,\n+\t\t      const _SimdTuple<_Tp, _As...> __v) // TODO: const-ref __v?\n+      {\n+\treturn __v._M_apply_wrapped([&__bits](auto __meta,\n+\t\t\t\t\t      auto __native) constexpr {\n+\t  return __meta.template _S_masked_unary<_Op>(__meta._S_make_mask(\n+\t\t\t\t\t\t\t__bits),\n+\t\t\t\t\t\t      __native);\n+\t});\n+      }\n+\n+    // }}}2\n+  };\n+\n+// _MaskImplFixedSize {{{1\n+template <int _Np>\n+  struct _MaskImplFixedSize\n+  {\n+    static_assert(\n+      sizeof(_ULLong) * __CHAR_BIT__ >= _Np,\n+      \"The fixed_size implementation relies on one _ULLong being able to store \"\n+      \"all boolean elements.\"); // required in load & store\n+\n+    // member types {{{\n+    using _Abi = simd_abi::fixed_size<_Np>;\n+\n+    using _MaskMember = _SanitizedBitMask<_Np>;\n+\n+    template <typename _Tp>\n+      using _FirstAbi = typename __fixed_size_storage_t<_Tp, _Np>::_FirstAbi;\n+\n+    template <typename _Tp>\n+      using _TypeTag = _Tp*;\n+\n+    // }}}\n+    // _S_broadcast {{{\n+    template <typename>\n+      _GLIBCXX_SIMD_INTRINSIC static constexpr _MaskMember\n+      _S_broadcast(bool __x)\n+      { return __x ? ~_MaskMember() : _MaskMember(); }\n+\n+    // }}}\n+    // _S_load {{{\n+    template <typename>\n+      _GLIBCXX_SIMD_INTRINSIC static constexpr _MaskMember\n+      _S_load(const bool* __mem)\n+      {\n+\tusing _Ip = __int_for_sizeof_t<bool>;\n+\t// the following load uses element_aligned and relies on __mem already\n+\t// carrying alignment information from when this load function was\n+\t// called.\n+\tconst simd<_Ip, _Abi> __bools(reinterpret_cast<const __may_alias<_Ip>*>(\n+\t\t\t\t\t__mem),\n+\t\t\t\t      element_aligned);\n+\treturn __data(__bools != 0);\n+      }\n+\n+    // }}}\n+    // _S_to_bits {{{\n+    template <bool _Sanitized>\n+      _GLIBCXX_SIMD_INTRINSIC static constexpr _SanitizedBitMask<_Np>\n+      _S_to_bits(_BitMask<_Np, _Sanitized> __x)\n+      {\n+\tif constexpr (_Sanitized)\n+\t  return __x;\n+\telse\n+\t  return __x._M_sanitized();\n+      }\n+\n+    // }}}\n+    // _S_convert {{{\n+    template <typename _Tp, typename _Up, typename _UAbi>\n+      _GLIBCXX_SIMD_INTRINSIC static constexpr _MaskMember\n+      _S_convert(simd_mask<_Up, _UAbi> __x)\n+      {\n+\treturn _UAbi::_MaskImpl::_S_to_bits(__data(__x))\n+\t  .template _M_extract<0, _Np>();\n+      }\n+\n+    // }}}\n+    // _S_from_bitmask {{{2\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static _MaskMember\n+      _S_from_bitmask(_MaskMember __bits, _TypeTag<_Tp>) noexcept\n+      { return __bits; }\n+\n+    // _S_load {{{2\n+    static inline _MaskMember _S_load(const bool* __mem) noexcept\n+    {\n+      // TODO: _UChar is not necessarily the best type to use here. For smaller\n+      // _Np _UShort, _UInt, _ULLong, float, and double can be more efficient.\n+      _ULLong __r = 0;\n+      using _Vs = __fixed_size_storage_t<_UChar, _Np>;\n+      __for_each(_Vs{}, [&](auto __meta, auto) {\n+\t__r |= __meta._S_mask_to_shifted_ullong(\n+\t  __meta._S_mask_impl._S_load(&__mem[__meta._S_offset],\n+\t\t\t\t      _SizeConstant<__meta._S_size()>()));\n+      });\n+      return __r;\n+    }\n+\n+    // _S_masked_load {{{2\n+    static inline _MaskMember _S_masked_load(_MaskMember __merge,\n+\t\t\t\t\t     _MaskMember __mask,\n+\t\t\t\t\t     const bool* __mem) noexcept\n+    {\n+      _BitOps::_S_bit_iteration(__mask.to_ullong(), [&](auto __i) {\n+\t__merge.set(__i, __mem[__i]);\n+      });\n+      return __merge;\n+    }\n+\n+    // _S_store {{{2\n+    static inline void _S_store(const _MaskMember __bitmask,\n+\t\t\t\tbool* __mem) noexcept\n+    {\n+      if constexpr (_Np == 1)\n+\t__mem[0] = __bitmask[0];\n+      else\n+\t_FirstAbi<_UChar>::_CommonImpl::_S_store_bool_array(__bitmask, __mem);\n+    }\n+\n+    // _S_masked_store {{{2\n+    static inline void _S_masked_store(const _MaskMember __v, bool* __mem,\n+\t\t\t\t       const _MaskMember __k) noexcept\n+    {\n+      _BitOps::_S_bit_iteration(__k, [&](auto __i) { __mem[__i] = __v[__i]; });\n+    }\n+\n+    // logical and bitwise operators {{{2\n+    _GLIBCXX_SIMD_INTRINSIC static _MaskMember\n+    _S_logical_and(const _MaskMember& __x, const _MaskMember& __y) noexcept\n+    { return __x & __y; }\n+\n+    _GLIBCXX_SIMD_INTRINSIC static _MaskMember\n+    _S_logical_or(const _MaskMember& __x, const _MaskMember& __y) noexcept\n+    { return __x | __y; }\n+\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr _MaskMember\n+    _S_bit_not(const _MaskMember& __x) noexcept\n+    { return ~__x; }\n+\n+    _GLIBCXX_SIMD_INTRINSIC static _MaskMember\n+    _S_bit_and(const _MaskMember& __x, const _MaskMember& __y) noexcept\n+    { return __x & __y; }\n+\n+    _GLIBCXX_SIMD_INTRINSIC static _MaskMember\n+    _S_bit_or(const _MaskMember& __x, const _MaskMember& __y) noexcept\n+    { return __x | __y; }\n+\n+    _GLIBCXX_SIMD_INTRINSIC static _MaskMember\n+    _S_bit_xor(const _MaskMember& __x, const _MaskMember& __y) noexcept\n+    { return __x ^ __y; }\n+\n+    // smart_reference access {{{2\n+    _GLIBCXX_SIMD_INTRINSIC static void _S_set(_MaskMember& __k, int __i,\n+\t\t\t\t\t       bool __x) noexcept\n+    { __k.set(__i, __x); }\n+\n+    // _S_masked_assign {{{2\n+    _GLIBCXX_SIMD_INTRINSIC static void\n+    _S_masked_assign(const _MaskMember __k, _MaskMember& __lhs,\n+\t\t     const _MaskMember __rhs)\n+    { __lhs = (__lhs & ~__k) | (__rhs & __k); }\n+\n+    // Optimization for the case where the RHS is a scalar.\n+    _GLIBCXX_SIMD_INTRINSIC static void _S_masked_assign(const _MaskMember __k,\n+\t\t\t\t\t\t\t _MaskMember& __lhs,\n+\t\t\t\t\t\t\t const bool __rhs)\n+    {\n+      if (__rhs)\n+\t__lhs |= __k;\n+      else\n+\t__lhs &= ~__k;\n+    }\n+\n+    // }}}2\n+    // _S_all_of {{{\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static bool _S_all_of(simd_mask<_Tp, _Abi> __k)\n+      { return __data(__k).all(); }\n+\n+    // }}}\n+    // _S_any_of {{{\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static bool _S_any_of(simd_mask<_Tp, _Abi> __k)\n+      { return __data(__k).any(); }\n+\n+    // }}}\n+    // _S_none_of {{{\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static bool _S_none_of(simd_mask<_Tp, _Abi> __k)\n+      { return __data(__k).none(); }\n+\n+    // }}}\n+    // _S_some_of {{{\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static bool\n+      _S_some_of([[maybe_unused]] simd_mask<_Tp, _Abi> __k)\n+      {\n+\tif constexpr (_Np == 1)\n+\t  return false;\n+\telse\n+\t  return __data(__k).any() && !__data(__k).all();\n+      }\n+\n+    // }}}\n+    // _S_popcount {{{\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static int _S_popcount(simd_mask<_Tp, _Abi> __k)\n+      { return __data(__k).count(); }\n+\n+    // }}}\n+    // _S_find_first_set {{{\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static int\n+      _S_find_first_set(simd_mask<_Tp, _Abi> __k)\n+      { return std::__countr_zero(__data(__k).to_ullong()); }\n+\n+    // }}}\n+    // _S_find_last_set {{{\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static int\n+      _S_find_last_set(simd_mask<_Tp, _Abi> __k)\n+      { return std::__bit_width(__data(__k).to_ullong()) - 1; }\n+\n+    // }}}\n+  };\n+// }}}1\n+\n+_GLIBCXX_SIMD_END_NAMESPACE\n+#endif // __cplusplus >= 201703L\n+#endif // _GLIBCXX_EXPERIMENTAL_SIMD_FIXED_SIZE_H_\n+\n+// vim: foldmethod=marker sw=2 noet ts=8 sts=2 tw=80"}, {"sha": "bbaa899faa22563b792d06f5b30608abaeec1e67", "filename": "libstdc++-v3/include/experimental/bits/simd_math.h", "status": "added", "additions": 1500, "deletions": 0, "changes": 1500, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_math.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_math.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_math.h?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -0,0 +1,1500 @@\n+// Math overloads for simd -*- C++ -*-\n+\n+// Copyright (C) 2020 Free Software Foundation, Inc.\n+//\n+// This file is part of the GNU ISO C++ Library.  This library is free\n+// software; you can redistribute it and/or modify it under the\n+// terms of the GNU General Public License as published by the\n+// Free Software Foundation; either version 3, or (at your option)\n+// any later version.\n+\n+// This library is distributed in the hope that it will be useful,\n+// but WITHOUT ANY WARRANTY; without even the implied warranty of\n+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+// GNU General Public License for more details.\n+\n+// Under Section 7 of GPL version 3, you are granted additional\n+// permissions described in the GCC Runtime Library Exception, version\n+// 3.1, as published by the Free Software Foundation.\n+\n+// You should have received a copy of the GNU General Public License and\n+// a copy of the GCC Runtime Library Exception along with this program;\n+// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+// <http://www.gnu.org/licenses/>.\n+\n+#ifndef _GLIBCXX_EXPERIMENTAL_SIMD_MATH_H_\n+#define _GLIBCXX_EXPERIMENTAL_SIMD_MATH_H_\n+\n+#if __cplusplus >= 201703L\n+\n+#include <utility>\n+#include <iomanip>\n+\n+_GLIBCXX_SIMD_BEGIN_NAMESPACE\n+template <typename _Tp, typename _V>\n+  using _Samesize = fixed_size_simd<_Tp, _V::size()>;\n+\n+// _Math_return_type {{{\n+template <typename _DoubleR, typename _Tp, typename _Abi>\n+  struct _Math_return_type;\n+\n+template <typename _DoubleR, typename _Tp, typename _Abi>\n+  using _Math_return_type_t =\n+    typename _Math_return_type<_DoubleR, _Tp, _Abi>::type;\n+\n+template <typename _Tp, typename _Abi>\n+  struct _Math_return_type<double, _Tp, _Abi>\n+  { using type = simd<_Tp, _Abi>; };\n+\n+template <typename _Tp, typename _Abi>\n+  struct _Math_return_type<bool, _Tp, _Abi>\n+  { using type = simd_mask<_Tp, _Abi>; };\n+\n+template <typename _DoubleR, typename _Tp, typename _Abi>\n+  struct _Math_return_type\n+  { using type = fixed_size_simd<_DoubleR, simd_size_v<_Tp, _Abi>>; };\n+\n+//}}}\n+// _GLIBCXX_SIMD_MATH_CALL_ {{{\n+#define _GLIBCXX_SIMD_MATH_CALL_(__name)                                       \\\n+template <typename _Tp, typename _Abi, typename...,                            \\\n+\t  typename _R = _Math_return_type_t<                                   \\\n+\t    decltype(std::__name(declval<double>())), _Tp, _Abi>>              \\\n+  enable_if_t<is_floating_point_v<_Tp>, _R>                                    \\\n+  __name(simd<_Tp, _Abi> __x)                                                  \\\n+  { return {__private_init, _Abi::_SimdImpl::_S_##__name(__data(__x))}; }\n+\n+// }}}\n+//_Extra_argument_type{{{\n+template <typename _Up, typename _Tp, typename _Abi>\n+  struct _Extra_argument_type;\n+\n+template <typename _Tp, typename _Abi>\n+  struct _Extra_argument_type<_Tp*, _Tp, _Abi>\n+  {\n+    using type = simd<_Tp, _Abi>*;\n+    static constexpr double* declval();\n+    static constexpr bool __needs_temporary_scalar = true;\n+\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr auto _S_data(type __x)\n+    { return &__data(*__x); }\n+  };\n+\n+template <typename _Up, typename _Tp, typename _Abi>\n+  struct _Extra_argument_type<_Up*, _Tp, _Abi>\n+  {\n+    static_assert(is_integral_v<_Up>);\n+    using type = fixed_size_simd<_Up, simd_size_v<_Tp, _Abi>>*;\n+    static constexpr _Up* declval();\n+    static constexpr bool __needs_temporary_scalar = true;\n+\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr auto _S_data(type __x)\n+    { return &__data(*__x); }\n+  };\n+\n+template <typename _Tp, typename _Abi>\n+  struct _Extra_argument_type<_Tp, _Tp, _Abi>\n+  {\n+    using type = simd<_Tp, _Abi>;\n+    static constexpr double declval();\n+    static constexpr bool __needs_temporary_scalar = false;\n+\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr decltype(auto)\n+    _S_data(const type& __x)\n+    { return __data(__x); }\n+  };\n+\n+template <typename _Up, typename _Tp, typename _Abi>\n+  struct _Extra_argument_type\n+  {\n+    static_assert(is_integral_v<_Up>);\n+    using type = fixed_size_simd<_Up, simd_size_v<_Tp, _Abi>>;\n+    static constexpr _Up declval();\n+    static constexpr bool __needs_temporary_scalar = false;\n+\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr decltype(auto)\n+    _S_data(const type& __x)\n+    { return __data(__x); }\n+  };\n+\n+//}}}\n+// _GLIBCXX_SIMD_MATH_CALL2_ {{{\n+#define _GLIBCXX_SIMD_MATH_CALL2_(__name, arg2_)                               \\\n+template <                                                                     \\\n+  typename _Tp, typename _Abi, typename...,                                    \\\n+  typename _Arg2 = _Extra_argument_type<arg2_, _Tp, _Abi>,                     \\\n+  typename _R = _Math_return_type_t<                                           \\\n+    decltype(std::__name(declval<double>(), _Arg2::declval())), _Tp, _Abi>>    \\\n+  enable_if_t<is_floating_point_v<_Tp>, _R>                                    \\\n+  __name(const simd<_Tp, _Abi>& __x, const typename _Arg2::type& __y)          \\\n+  {                                                                            \\\n+    return {__private_init,                                                    \\\n+\t    _Abi::_SimdImpl::_S_##__name(__data(__x), _Arg2::_S_data(__y))};   \\\n+  }                                                                            \\\n+template <typename _Up, typename _Tp, typename _Abi>                           \\\n+  _GLIBCXX_SIMD_INTRINSIC _Math_return_type_t<                                 \\\n+    decltype(std::__name(                                                      \\\n+      declval<double>(),                                                       \\\n+      declval<enable_if_t<                                                     \\\n+\tconjunction_v<                                                         \\\n+\t  is_same<arg2_, _Tp>,                                                 \\\n+\t  negation<is_same<__remove_cvref_t<_Up>, simd<_Tp, _Abi>>>,           \\\n+\t  is_convertible<_Up, simd<_Tp, _Abi>>, is_floating_point<_Tp>>,       \\\n+\tdouble>>())),                                                          \\\n+    _Tp, _Abi>                                                                 \\\n+  __name(_Up&& __xx, const simd<_Tp, _Abi>& __yy)                              \\\n+  { return __name(simd<_Tp, _Abi>(static_cast<_Up&&>(__xx)), __yy); }\n+\n+// }}}\n+// _GLIBCXX_SIMD_MATH_CALL3_ {{{\n+#define _GLIBCXX_SIMD_MATH_CALL3_(__name, arg2_, arg3_)                        \\\n+template <typename _Tp, typename _Abi, typename...,                            \\\n+\t  typename _Arg2 = _Extra_argument_type<arg2_, _Tp, _Abi>,             \\\n+\t  typename _Arg3 = _Extra_argument_type<arg3_, _Tp, _Abi>,             \\\n+\t  typename _R = _Math_return_type_t<                                   \\\n+\t    decltype(std::__name(declval<double>(), _Arg2::declval(),          \\\n+\t\t\t\t _Arg3::declval())),                           \\\n+\t    _Tp, _Abi>>                                                        \\\n+  enable_if_t<is_floating_point_v<_Tp>, _R>                                    \\\n+  __name(const simd<_Tp, _Abi>& __x, const typename _Arg2::type& __y,          \\\n+\t const typename _Arg3::type& __z)                                      \\\n+  {                                                                            \\\n+    return {__private_init,                                                    \\\n+\t    _Abi::_SimdImpl::_S_##__name(__data(__x), _Arg2::_S_data(__y),     \\\n+\t\t\t\t\t _Arg3::_S_data(__z))};                \\\n+  }                                                                            \\\n+template <                                                                     \\\n+  typename _T0, typename _T1, typename _T2, typename...,                       \\\n+  typename _U0 = __remove_cvref_t<_T0>,                                        \\\n+  typename _U1 = __remove_cvref_t<_T1>,                                        \\\n+  typename _U2 = __remove_cvref_t<_T2>,                                        \\\n+  typename _Simd = conditional_t<is_simd_v<_U1>, _U1, _U2>,                    \\\n+  typename = enable_if_t<conjunction_v<                                        \\\n+    is_simd<_Simd>, is_convertible<_T0&&, _Simd>,                              \\\n+    is_convertible<_T1&&, _Simd>, is_convertible<_T2&&, _Simd>,                \\\n+    negation<conjunction<                                                      \\\n+      is_simd<_U0>, is_floating_point<__value_type_or_identity_t<_U0>>>>>>>    \\\n+  _GLIBCXX_SIMD_INTRINSIC decltype(__name(declval<const _Simd&>(),             \\\n+\t\t\t\t\t  declval<const _Simd&>(),             \\\n+\t\t\t\t\t  declval<const _Simd&>()))            \\\n+  __name(_T0&& __xx, _T1&& __yy, _T2&& __zz)                                   \\\n+  {                                                                            \\\n+    return __name(_Simd(static_cast<_T0&&>(__xx)),                             \\\n+\t\t  _Simd(static_cast<_T1&&>(__yy)),                             \\\n+\t\t  _Simd(static_cast<_T2&&>(__zz)));                            \\\n+  }\n+\n+// }}}\n+// __cosSeries {{{\n+template <typename _Abi>\n+  _GLIBCXX_SIMD_ALWAYS_INLINE static simd<float, _Abi>\n+  __cosSeries(const simd<float, _Abi>& __x)\n+  {\n+    const simd<float, _Abi> __x2 = __x * __x;\n+    simd<float, _Abi> __y;\n+    __y = 0x1.ap-16f;                  //  1/8!\n+    __y = __y * __x2 - 0x1.6c1p-10f;   // -1/6!\n+    __y = __y * __x2 + 0x1.555556p-5f; //  1/4!\n+    return __y * (__x2 * __x2) - .5f * __x2 + 1.f;\n+  }\n+\n+template <typename _Abi>\n+  _GLIBCXX_SIMD_ALWAYS_INLINE static simd<double, _Abi>\n+  __cosSeries(const simd<double, _Abi>& __x)\n+  {\n+    const simd<double, _Abi> __x2 = __x * __x;\n+    simd<double, _Abi> __y;\n+    __y = 0x1.AC00000000000p-45;              //  1/16!\n+    __y = __y * __x2 - 0x1.9394000000000p-37; // -1/14!\n+    __y = __y * __x2 + 0x1.1EED8C0000000p-29; //  1/12!\n+    __y = __y * __x2 - 0x1.27E4FB7400000p-22; // -1/10!\n+    __y = __y * __x2 + 0x1.A01A01A018000p-16; //  1/8!\n+    __y = __y * __x2 - 0x1.6C16C16C16C00p-10; // -1/6!\n+    __y = __y * __x2 + 0x1.5555555555554p-5;  //  1/4!\n+    return (__y * __x2 - .5f) * __x2 + 1.f;\n+  }\n+\n+// }}}\n+// __sinSeries {{{\n+template <typename _Abi>\n+  _GLIBCXX_SIMD_ALWAYS_INLINE static simd<float, _Abi>\n+  __sinSeries(const simd<float, _Abi>& __x)\n+  {\n+    const simd<float, _Abi> __x2 = __x * __x;\n+    simd<float, _Abi> __y;\n+    __y = -0x1.9CC000p-13f;            // -1/7!\n+    __y = __y * __x2 + 0x1.111100p-7f; //  1/5!\n+    __y = __y * __x2 - 0x1.555556p-3f; // -1/3!\n+    return __y * (__x2 * __x) + __x;\n+  }\n+\n+template <typename _Abi>\n+  _GLIBCXX_SIMD_ALWAYS_INLINE static simd<double, _Abi>\n+  __sinSeries(const simd<double, _Abi>& __x)\n+  {\n+    // __x  = [0, 0.7854 = pi/4]\n+    // __x\u00b2 = [0, 0.6169 = pi\u00b2/8]\n+    const simd<double, _Abi> __x2 = __x * __x;\n+    simd<double, _Abi> __y;\n+    __y = -0x1.ACF0000000000p-41;             // -1/15!\n+    __y = __y * __x2 + 0x1.6124400000000p-33; //  1/13!\n+    __y = __y * __x2 - 0x1.AE64567000000p-26; // -1/11!\n+    __y = __y * __x2 + 0x1.71DE3A5540000p-19; //  1/9!\n+    __y = __y * __x2 - 0x1.A01A01A01A000p-13; // -1/7!\n+    __y = __y * __x2 + 0x1.1111111111110p-7;  //  1/5!\n+    __y = __y * __x2 - 0x1.5555555555555p-3;  // -1/3!\n+    return __y * (__x2 * __x) + __x;\n+  }\n+\n+// }}}\n+// __zero_low_bits {{{\n+template <int _Bits, typename _Tp, typename _Abi>\n+  _GLIBCXX_SIMD_INTRINSIC simd<_Tp, _Abi>\n+  __zero_low_bits(simd<_Tp, _Abi> __x)\n+  {\n+    const simd<_Tp, _Abi> __bitmask\n+      = __bit_cast<_Tp>(~make_unsigned_t<__int_for_sizeof_t<_Tp>>() << _Bits);\n+    return {__private_init,\n+\t    _Abi::_SimdImpl::_S_bit_and(__data(__x), __data(__bitmask))};\n+  }\n+\n+// }}}\n+// __fold_input {{{\n+\n+/**@internal\n+ * Fold @p x into [-\u00bc\u03c0, \u00bc\u03c0] and remember the quadrant it came from:\n+ * quadrant 0: [-\u00bc\u03c0,  \u00bc\u03c0]\n+ * quadrant 1: [ \u00bc\u03c0,  \u00be\u03c0]\n+ * quadrant 2: [ \u00be\u03c0, 1\u00bc\u03c0]\n+ * quadrant 3: [1\u00bc\u03c0, 1\u00be\u03c0]\n+ *\n+ * The algorithm determines `y` as the multiple `x - y * \u00bc\u03c0 = [-\u00bc\u03c0, \u00bc\u03c0]`. Using\n+ * a bitmask, `y` is reduced to `quadrant`. `y` can be calculated as\n+ * ```\n+ * y = trunc(x / \u00bc\u03c0);\n+ * y += fmod(y, 2);\n+ * ```\n+ * This can be simplified by moving the (implicit) division by 2 into the\n+ * truncation expression. The `+= fmod` effect can the be achieved by using\n+ * rounding instead of truncation: `y = round(x / \u00bd\u03c0) * 2`. If precision allows,\n+ * `2/\u03c0 * x` is better (faster).\n+ */\n+template <typename _Tp, typename _Abi>\n+  struct _Folded\n+  {\n+    simd<_Tp, _Abi> _M_x;\n+    rebind_simd_t<int, simd<_Tp, _Abi>> _M_quadrant;\n+  };\n+\n+namespace __math_float {\n+inline constexpr float __pi_over_4 = 0x1.921FB6p-1f; // \u03c0/4\n+inline constexpr float __2_over_pi = 0x1.45F306p-1f; // 2/\u03c0\n+inline constexpr float __pi_2_5bits0\n+  = 0x1.921fc0p0f; // \u03c0/2, 5 0-bits (least significant)\n+inline constexpr float __pi_2_5bits0_rem\n+  = -0x1.5777a6p-21f; // \u03c0/2 - __pi_2_5bits0\n+} // namespace __math_float\n+namespace __math_double {\n+inline constexpr double __pi_over_4 = 0x1.921fb54442d18p-1; // \u03c0/4\n+inline constexpr double __2_over_pi = 0x1.45F306DC9C883p-1; // 2/\u03c0\n+inline constexpr double __pi_2 = 0x1.921fb54442d18p0;       // \u03c0/2\n+} // namespace __math_double\n+\n+template <typename _Abi>\n+  _GLIBCXX_SIMD_ALWAYS_INLINE _Folded<float, _Abi>\n+  __fold_input(const simd<float, _Abi>& __x)\n+  {\n+    using _V = simd<float, _Abi>;\n+    using _IV = rebind_simd_t<int, _V>;\n+    using namespace __math_float;\n+    _Folded<float, _Abi> __r;\n+    __r._M_x = abs(__x);\n+#if 0\n+    // zero most mantissa bits:\n+    constexpr float __1_over_pi = 0x1.45F306p-2f; // 1/\u03c0\n+    const auto __y = (__r._M_x * __1_over_pi + 0x1.8p23f) - 0x1.8p23f;\n+    // split \u03c0 into 4 parts, the first three with 13 trailing zeros (to make the\n+    // following multiplications precise):\n+    constexpr float __pi0 = 0x1.920000p1f;\n+    constexpr float __pi1 = 0x1.fb4000p-11f;\n+    constexpr float __pi2 = 0x1.444000p-23f;\n+    constexpr float __pi3 = 0x1.68c234p-38f;\n+    __r._M_x - __y*__pi0 - __y*__pi1 - __y*__pi2 - __y*__pi3\n+#else\n+    if (_GLIBCXX_SIMD_IS_UNLIKELY(all_of(__r._M_x < __pi_over_4)))\n+      __r._M_quadrant = 0;\n+    else if (_GLIBCXX_SIMD_IS_LIKELY(all_of(__r._M_x < 6 * __pi_over_4)))\n+      {\n+\tconst _V __y = nearbyint(__r._M_x * __2_over_pi);\n+\t__r._M_quadrant = static_simd_cast<_IV>(__y) & 3; // __y mod 4\n+\t__r._M_x -= __y * __pi_2_5bits0;\n+\t__r._M_x -= __y * __pi_2_5bits0_rem;\n+      }\n+    else\n+      {\n+\tusing __math_double::__2_over_pi;\n+\tusing __math_double::__pi_2;\n+\tusing _VD = rebind_simd_t<double, _V>;\n+\t_VD __xd = static_simd_cast<_VD>(__r._M_x);\n+\t_VD __y = nearbyint(__xd * __2_over_pi);\n+\t__r._M_quadrant = static_simd_cast<_IV>(__y) & 3; // = __y mod 4\n+\t__r._M_x = static_simd_cast<_V>(__xd - __y * __pi_2);\n+      }\n+#endif\n+    return __r;\n+  }\n+\n+template <typename _Abi>\n+  _GLIBCXX_SIMD_ALWAYS_INLINE _Folded<double, _Abi>\n+  __fold_input(const simd<double, _Abi>& __x)\n+  {\n+    using _V = simd<double, _Abi>;\n+    using _IV = rebind_simd_t<int, _V>;\n+    using namespace __math_double;\n+\n+    _Folded<double, _Abi> __r;\n+    __r._M_x = abs(__x);\n+    if (_GLIBCXX_SIMD_IS_UNLIKELY(all_of(__r._M_x < __pi_over_4)))\n+      {\n+\t__r._M_quadrant = 0;\n+\treturn __r;\n+      }\n+    const _V __y = nearbyint(__r._M_x / (2 * __pi_over_4));\n+    __r._M_quadrant = static_simd_cast<_IV>(__y) & 3;\n+\n+    if (_GLIBCXX_SIMD_IS_LIKELY(all_of(__r._M_x < 1025 * __pi_over_4)))\n+      {\n+\t// x - y * pi/2, y uses no more than 11 mantissa bits\n+\t__r._M_x -= __y * 0x1.921FB54443000p0;\n+\t__r._M_x -= __y * -0x1.73DCB3B39A000p-43;\n+\t__r._M_x -= __y * 0x1.45C06E0E68948p-86;\n+      }\n+    else if (_GLIBCXX_SIMD_IS_LIKELY(all_of(__y <= 0x1.0p30)))\n+      {\n+\t// x - y * pi/2, y uses no more than 29 mantissa bits\n+\t__r._M_x -= __y * 0x1.921FB40000000p0;\n+\t__r._M_x -= __y * 0x1.4442D00000000p-24;\n+\t__r._M_x -= __y * 0x1.8469898CC5170p-48;\n+      }\n+    else\n+      {\n+\t// x - y * pi/2, y may require all mantissa bits\n+\tconst _V __y_hi = __zero_low_bits<26>(__y);\n+\tconst _V __y_lo = __y - __y_hi;\n+\tconst auto __pi_2_1 = 0x1.921FB50000000p0;\n+\tconst auto __pi_2_2 = 0x1.110B460000000p-26;\n+\tconst auto __pi_2_3 = 0x1.1A62630000000p-54;\n+\tconst auto __pi_2_4 = 0x1.8A2E03707344Ap-81;\n+\t__r._M_x = __r._M_x - __y_hi * __pi_2_1\n+\t\t   - max(__y_hi * __pi_2_2, __y_lo * __pi_2_1)\n+\t\t   - min(__y_hi * __pi_2_2, __y_lo * __pi_2_1)\n+\t\t   - max(__y_hi * __pi_2_3, __y_lo * __pi_2_2)\n+\t\t   - min(__y_hi * __pi_2_3, __y_lo * __pi_2_2)\n+\t\t   - max(__y * __pi_2_4, __y_lo * __pi_2_3)\n+\t\t   - min(__y * __pi_2_4, __y_lo * __pi_2_3);\n+      }\n+    return __r;\n+  }\n+\n+// }}}\n+// __extract_exponent_as_int {{{\n+template <typename _Tp, typename _Abi>\n+  rebind_simd_t<int, simd<_Tp, _Abi>>\n+  __extract_exponent_as_int(const simd<_Tp, _Abi>& __v)\n+  {\n+    using _Vp = simd<_Tp, _Abi>;\n+    using _Up = make_unsigned_t<__int_for_sizeof_t<_Tp>>;\n+    using namespace std::experimental::__float_bitwise_operators;\n+    const _Vp __exponent_mask\n+      = __infinity_v<_Tp>; // 0x7f800000 or 0x7ff0000000000000\n+    return static_simd_cast<rebind_simd_t<int, _Vp>>(\n+      __bit_cast<rebind_simd_t<_Up, _Vp>>(__v & __exponent_mask)\n+      >> (__digits_v<_Tp> - 1));\n+  }\n+\n+// }}}\n+// __impl_or_fallback {{{\n+template <typename ImplFun, typename FallbackFun, typename... _Args>\n+  _GLIBCXX_SIMD_INTRINSIC auto\n+  __impl_or_fallback_dispatch(int, ImplFun&& __impl_fun, FallbackFun&&,\n+\t\t\t      _Args&&... __args)\n+    -> decltype(__impl_fun(static_cast<_Args&&>(__args)...))\n+  { return __impl_fun(static_cast<_Args&&>(__args)...); }\n+\n+template <typename ImplFun, typename FallbackFun, typename... _Args>\n+  inline auto\n+  __impl_or_fallback_dispatch(float, ImplFun&&, FallbackFun&& __fallback_fun,\n+\t\t\t      _Args&&... __args)\n+    -> decltype(__fallback_fun(static_cast<_Args&&>(__args)...))\n+  { return __fallback_fun(static_cast<_Args&&>(__args)...); }\n+\n+template <typename... _Args>\n+  _GLIBCXX_SIMD_INTRINSIC auto\n+  __impl_or_fallback(_Args&&... __args)\n+  {\n+    return __impl_or_fallback_dispatch(int(), static_cast<_Args&&>(__args)...);\n+  }\n+//}}}\n+\n+// trigonometric functions {{{\n+_GLIBCXX_SIMD_MATH_CALL_(acos)\n+_GLIBCXX_SIMD_MATH_CALL_(asin)\n+_GLIBCXX_SIMD_MATH_CALL_(atan)\n+_GLIBCXX_SIMD_MATH_CALL2_(atan2, _Tp)\n+\n+/*\n+ * algorithm for sine and cosine:\n+ *\n+ * The result can be calculated with sine or cosine depending on the \u03c0/4 section\n+ * the input is in. sine   \u2248 __x + __x\u00b3 cosine \u2248 1 - __x\u00b2\n+ *\n+ * sine:\n+ * Map -__x to __x and invert the output\n+ * Extend precision of __x - n * \u03c0/4 by calculating\n+ * ((__x - n * p1) - n * p2) - n * p3 (p1 + p2 + p3 = \u03c0/4)\n+ *\n+ * Calculate Taylor series with tuned coefficients.\n+ * Fix sign.\n+ */\n+// cos{{{\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n+  cos(const simd<_Tp, _Abi>& __x)\n+  {\n+    using _V = simd<_Tp, _Abi>;\n+    if constexpr (__is_scalar_abi<_Abi>() || __is_fixed_size_abi_v<_Abi>)\n+      return {__private_init, _Abi::_SimdImpl::_S_cos(__data(__x))};\n+    else\n+      {\n+\tif constexpr (is_same_v<_Tp, float>)\n+\t  if (_GLIBCXX_SIMD_IS_UNLIKELY(any_of(abs(__x) >= 393382)))\n+\t    return static_simd_cast<_V>(\n+\t      cos(static_simd_cast<rebind_simd_t<double, _V>>(__x)));\n+\n+\tconst auto __f = __fold_input(__x);\n+\t// quadrant | effect\n+\t//        0 | cosSeries, +\n+\t//        1 | sinSeries, -\n+\t//        2 | cosSeries, -\n+\t//        3 | sinSeries, +\n+\tusing namespace std::experimental::__float_bitwise_operators;\n+\tconst _V __sign_flip\n+\t  = _V(-0.f) & static_simd_cast<_V>((1 + __f._M_quadrant) << 30);\n+\n+\tconst auto __need_cos = (__f._M_quadrant & 1) == 0;\n+\tif (_GLIBCXX_SIMD_IS_UNLIKELY(all_of(__need_cos)))\n+\t  return __sign_flip ^ __cosSeries(__f._M_x);\n+\telse if (_GLIBCXX_SIMD_IS_UNLIKELY(none_of(__need_cos)))\n+\t  return __sign_flip ^ __sinSeries(__f._M_x);\n+\telse // some_of(__need_cos)\n+\t  {\n+\t    _V __r = __sinSeries(__f._M_x);\n+\t    where(__need_cos.__cvt(), __r) = __cosSeries(__f._M_x);\n+\t    return __r ^ __sign_flip;\n+\t  }\n+      }\n+  }\n+\n+template <typename _Tp>\n+  _GLIBCXX_SIMD_ALWAYS_INLINE\n+  enable_if_t<is_floating_point<_Tp>::value, simd<_Tp, simd_abi::scalar>>\n+  cos(simd<_Tp, simd_abi::scalar> __x)\n+  { return std::cos(__data(__x)); }\n+\n+//}}}\n+// sin{{{\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n+  sin(const simd<_Tp, _Abi>& __x)\n+  {\n+    using _V = simd<_Tp, _Abi>;\n+    if constexpr (__is_scalar_abi<_Abi>() || __is_fixed_size_abi_v<_Abi>)\n+      return {__private_init, _Abi::_SimdImpl::_S_sin(__data(__x))};\n+    else\n+      {\n+\tif constexpr (is_same_v<_Tp, float>)\n+\t  if (_GLIBCXX_SIMD_IS_UNLIKELY(any_of(abs(__x) >= 527449)))\n+\t    return static_simd_cast<_V>(\n+\t      sin(static_simd_cast<rebind_simd_t<double, _V>>(__x)));\n+\n+\tconst auto __f = __fold_input(__x);\n+\t// quadrant | effect\n+\t//        0 | sinSeries\n+\t//        1 | cosSeries\n+\t//        2 | sinSeries, sign flip\n+\t//        3 | cosSeries, sign flip\n+\tusing namespace std::experimental::__float_bitwise_operators;\n+\tconst auto __sign_flip\n+\t  = (__x ^ static_simd_cast<_V>(1 - __f._M_quadrant)) & _V(_Tp(-0.));\n+\n+\tconst auto __need_sin = (__f._M_quadrant & 1) == 0;\n+\tif (_GLIBCXX_SIMD_IS_UNLIKELY(all_of(__need_sin)))\n+\t  return __sign_flip ^ __sinSeries(__f._M_x);\n+\telse if (_GLIBCXX_SIMD_IS_UNLIKELY(none_of(__need_sin)))\n+\t  return __sign_flip ^ __cosSeries(__f._M_x);\n+\telse // some_of(__need_sin)\n+\t  {\n+\t    _V __r = __cosSeries(__f._M_x);\n+\t    where(__need_sin.__cvt(), __r) = __sinSeries(__f._M_x);\n+\t    return __sign_flip ^ __r;\n+\t  }\n+      }\n+  }\n+\n+template <typename _Tp>\n+  _GLIBCXX_SIMD_ALWAYS_INLINE\n+  enable_if_t<is_floating_point<_Tp>::value, simd<_Tp, simd_abi::scalar>>\n+  sin(simd<_Tp, simd_abi::scalar> __x)\n+  { return std::sin(__data(__x)); }\n+\n+//}}}\n+_GLIBCXX_SIMD_MATH_CALL_(tan)\n+_GLIBCXX_SIMD_MATH_CALL_(acosh)\n+_GLIBCXX_SIMD_MATH_CALL_(asinh)\n+_GLIBCXX_SIMD_MATH_CALL_(atanh)\n+_GLIBCXX_SIMD_MATH_CALL_(cosh)\n+_GLIBCXX_SIMD_MATH_CALL_(sinh)\n+_GLIBCXX_SIMD_MATH_CALL_(tanh)\n+// }}}\n+// exponential functions {{{\n+_GLIBCXX_SIMD_MATH_CALL_(exp)\n+_GLIBCXX_SIMD_MATH_CALL_(exp2)\n+_GLIBCXX_SIMD_MATH_CALL_(expm1)\n+\n+// }}}\n+// frexp {{{\n+#if _GLIBCXX_SIMD_X86INTRIN\n+template <typename _Tp, size_t _Np>\n+  _SimdWrapper<_Tp, _Np>\n+  __getexp(_SimdWrapper<_Tp, _Np> __x)\n+  {\n+    if constexpr (__have_avx512vl && __is_sse_ps<_Tp, _Np>())\n+      return __auto_bitcast(_mm_getexp_ps(__to_intrin(__x)));\n+    else if constexpr (__have_avx512f && __is_sse_ps<_Tp, _Np>())\n+      return __auto_bitcast(_mm512_getexp_ps(__auto_bitcast(__to_intrin(__x))));\n+    else if constexpr (__have_avx512vl && __is_sse_pd<_Tp, _Np>())\n+      return _mm_getexp_pd(__x);\n+    else if constexpr (__have_avx512f && __is_sse_pd<_Tp, _Np>())\n+      return __lo128(_mm512_getexp_pd(__auto_bitcast(__x)));\n+    else if constexpr (__have_avx512vl && __is_avx_ps<_Tp, _Np>())\n+      return _mm256_getexp_ps(__x);\n+    else if constexpr (__have_avx512f && __is_avx_ps<_Tp, _Np>())\n+      return __lo256(_mm512_getexp_ps(__auto_bitcast(__x)));\n+    else if constexpr (__have_avx512vl && __is_avx_pd<_Tp, _Np>())\n+      return _mm256_getexp_pd(__x);\n+    else if constexpr (__have_avx512f && __is_avx_pd<_Tp, _Np>())\n+      return __lo256(_mm512_getexp_pd(__auto_bitcast(__x)));\n+    else if constexpr (__is_avx512_ps<_Tp, _Np>())\n+      return _mm512_getexp_ps(__x);\n+    else if constexpr (__is_avx512_pd<_Tp, _Np>())\n+      return _mm512_getexp_pd(__x);\n+    else\n+      __assert_unreachable<_Tp>();\n+  }\n+\n+template <typename _Tp, size_t _Np>\n+  _SimdWrapper<_Tp, _Np>\n+  __getmant_avx512(_SimdWrapper<_Tp, _Np> __x)\n+  {\n+    if constexpr (__have_avx512vl && __is_sse_ps<_Tp, _Np>())\n+      return __auto_bitcast(_mm_getmant_ps(__to_intrin(__x), _MM_MANT_NORM_p5_1,\n+\t\t\t\t\t   _MM_MANT_SIGN_src));\n+    else if constexpr (__have_avx512f && __is_sse_ps<_Tp, _Np>())\n+      return __auto_bitcast(_mm512_getmant_ps(__auto_bitcast(__to_intrin(__x)),\n+\t\t\t\t\t      _MM_MANT_NORM_p5_1,\n+\t\t\t\t\t      _MM_MANT_SIGN_src));\n+    else if constexpr (__have_avx512vl && __is_sse_pd<_Tp, _Np>())\n+      return _mm_getmant_pd(__x, _MM_MANT_NORM_p5_1, _MM_MANT_SIGN_src);\n+    else if constexpr (__have_avx512f && __is_sse_pd<_Tp, _Np>())\n+      return __lo128(_mm512_getmant_pd(__auto_bitcast(__x), _MM_MANT_NORM_p5_1,\n+\t\t\t\t       _MM_MANT_SIGN_src));\n+    else if constexpr (__have_avx512vl && __is_avx_ps<_Tp, _Np>())\n+      return _mm256_getmant_ps(__x, _MM_MANT_NORM_p5_1, _MM_MANT_SIGN_src);\n+    else if constexpr (__have_avx512f && __is_avx_ps<_Tp, _Np>())\n+      return __lo256(_mm512_getmant_ps(__auto_bitcast(__x), _MM_MANT_NORM_p5_1,\n+\t\t\t\t       _MM_MANT_SIGN_src));\n+    else if constexpr (__have_avx512vl && __is_avx_pd<_Tp, _Np>())\n+      return _mm256_getmant_pd(__x, _MM_MANT_NORM_p5_1, _MM_MANT_SIGN_src);\n+    else if constexpr (__have_avx512f && __is_avx_pd<_Tp, _Np>())\n+      return __lo256(_mm512_getmant_pd(__auto_bitcast(__x), _MM_MANT_NORM_p5_1,\n+\t\t\t\t       _MM_MANT_SIGN_src));\n+    else if constexpr (__is_avx512_ps<_Tp, _Np>())\n+      return _mm512_getmant_ps(__x, _MM_MANT_NORM_p5_1, _MM_MANT_SIGN_src);\n+    else if constexpr (__is_avx512_pd<_Tp, _Np>())\n+      return _mm512_getmant_pd(__x, _MM_MANT_NORM_p5_1, _MM_MANT_SIGN_src);\n+    else\n+      __assert_unreachable<_Tp>();\n+  }\n+#endif // _GLIBCXX_SIMD_X86INTRIN\n+\n+/**\n+ * splits @p __v into exponent and mantissa, the sign is kept with the mantissa\n+ *\n+ * The return value will be in the range [0.5, 1.0[\n+ * The @p __e value will be an integer defining the power-of-two exponent\n+ */\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n+  frexp(const simd<_Tp, _Abi>& __x, _Samesize<int, simd<_Tp, _Abi>>* __exp)\n+  {\n+    if constexpr (simd_size_v<_Tp, _Abi> == 1)\n+      {\n+\tint __tmp;\n+\tconst auto __r = std::frexp(__x[0], &__tmp);\n+\t(*__exp)[0] = __tmp;\n+\treturn __r;\n+      }\n+    else if constexpr (__is_fixed_size_abi_v<_Abi>)\n+      {\n+\treturn {__private_init,\n+\t\t_Abi::_SimdImpl::_S_frexp(__data(__x), __data(*__exp))};\n+#if _GLIBCXX_SIMD_X86INTRIN\n+      }\n+    else if constexpr (__have_avx512f)\n+      {\n+\tconstexpr size_t _Np = simd_size_v<_Tp, _Abi>;\n+\tconstexpr size_t _NI = _Np < 4 ? 4 : _Np;\n+\tconst auto __v = __data(__x);\n+\tconst auto __isnonzero\n+\t  = _Abi::_SimdImpl::_S_isnonzerovalue_mask(__v._M_data);\n+\tconst _SimdWrapper<int, _NI> __exp_plus1\n+\t  = 1 + __convert<_SimdWrapper<int, _NI>>(__getexp(__v))._M_data;\n+\tconst _SimdWrapper<int, _Np> __e = __wrapper_bitcast<int, _Np>(\n+\t  _Abi::_CommonImpl::_S_blend(_SimdWrapper<bool, _NI>(__isnonzero),\n+\t\t\t\t      _SimdWrapper<int, _NI>(), __exp_plus1));\n+\tsimd_abi::deduce_t<int, _Np>::_CommonImpl::_S_store(__e, __exp);\n+\treturn {__private_init,\n+\t\t_Abi::_CommonImpl::_S_blend(_SimdWrapper<bool, _Np>(\n+\t\t\t\t\t      __isnonzero),\n+\t\t\t\t\t    __v, __getmant_avx512(__v))};\n+#endif // _GLIBCXX_SIMD_X86INTRIN\n+      }\n+    else\n+      {\n+\t// fallback implementation\n+\tstatic_assert(sizeof(_Tp) == 4 || sizeof(_Tp) == 8);\n+\tusing _V = simd<_Tp, _Abi>;\n+\tusing _IV = rebind_simd_t<int, _V>;\n+\tusing namespace std::experimental::__proposed;\n+\tusing namespace std::experimental::__float_bitwise_operators;\n+\n+\tconstexpr int __exp_adjust = sizeof(_Tp) == 4 ? 0x7e : 0x3fe;\n+\tconstexpr int __exp_offset = sizeof(_Tp) == 4 ? 0x70 : 0x200;\n+\tconstexpr _Tp __subnorm_scale = sizeof(_Tp) == 4 ? 0x1p112 : 0x1p512;\n+\t_GLIBCXX_SIMD_USE_CONSTEXPR_API _V __exponent_mask\n+\t  = __infinity_v<_Tp>; // 0x7f800000 or 0x7ff0000000000000\n+\t_GLIBCXX_SIMD_USE_CONSTEXPR_API _V __p5_1_exponent\n+\t  = -(2 - __epsilon_v<_Tp>) / 2; // 0xbf7fffff or 0xbfefffffffffffff\n+\n+\t_V __mant = __p5_1_exponent & (__exponent_mask | __x); // +/-[.5, 1)\n+\tconst _IV __exponent_bits = __extract_exponent_as_int(__x);\n+\tif (_GLIBCXX_SIMD_IS_LIKELY(all_of(isnormal(__x))))\n+\t  {\n+\t    *__exp\n+\t      = simd_cast<_Samesize<int, _V>>(__exponent_bits - __exp_adjust);\n+\t    return __mant;\n+\t  }\n+\n+#if __FINITE_MATH_ONLY__\n+\t// at least one element of __x is 0 or subnormal, the rest is normal\n+\t// (inf and NaN are excluded by -ffinite-math-only)\n+\tconst auto __iszero_inf_nan = __x == 0;\n+#else\n+\tconst auto __as_int\n+\t  = __bit_cast<rebind_simd_t<__int_for_sizeof_t<_Tp>, _V>>(abs(__x));\n+\tconst auto __inf\n+\t  = __bit_cast<rebind_simd_t<__int_for_sizeof_t<_Tp>, _V>>(\n+\t    _V(__infinity_v<_Tp>));\n+\tconst auto __iszero_inf_nan = static_simd_cast<typename _V::mask_type>(\n+\t  __as_int == 0 || __as_int >= __inf);\n+#endif\n+\n+\tconst _V __scaled_subnormal = __x * __subnorm_scale;\n+\tconst _V __mant_subnormal\n+\t  = __p5_1_exponent & (__exponent_mask | __scaled_subnormal);\n+\twhere(!isnormal(__x), __mant) = __mant_subnormal;\n+\twhere(__iszero_inf_nan, __mant) = __x;\n+\t_IV __e = __extract_exponent_as_int(__scaled_subnormal);\n+\tusing _MaskType =\n+\t  typename conditional_t<sizeof(typename _V::value_type) == sizeof(int),\n+\t\t\t\t _V, _IV>::mask_type;\n+\tconst _MaskType __value_isnormal = isnormal(__x).__cvt();\n+\twhere(__value_isnormal.__cvt(), __e) = __exponent_bits;\n+\tstatic_assert(sizeof(_IV) == sizeof(__value_isnormal));\n+\tconst _IV __offset\n+\t  = (__bit_cast<_IV>(__value_isnormal) & _IV(__exp_adjust))\n+\t    | (__bit_cast<_IV>(static_simd_cast<_MaskType>(__exponent_bits == 0)\n+\t\t\t       & static_simd_cast<_MaskType>(__x != 0))\n+\t       & _IV(__exp_adjust + __exp_offset));\n+\t*__exp = simd_cast<_Samesize<int, _V>>(__e - __offset);\n+\treturn __mant;\n+      }\n+  }\n+\n+// }}}\n+_GLIBCXX_SIMD_MATH_CALL2_(ldexp, int)\n+_GLIBCXX_SIMD_MATH_CALL_(ilogb)\n+\n+// logarithms {{{\n+_GLIBCXX_SIMD_MATH_CALL_(log)\n+_GLIBCXX_SIMD_MATH_CALL_(log10)\n+_GLIBCXX_SIMD_MATH_CALL_(log1p)\n+_GLIBCXX_SIMD_MATH_CALL_(log2)\n+\n+//}}}\n+// logb{{{\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<is_floating_point<_Tp>::value, simd<_Tp, _Abi>>\n+  logb(const simd<_Tp, _Abi>& __x)\n+  {\n+    constexpr size_t _Np = simd_size_v<_Tp, _Abi>;\n+    if constexpr (_Np == 1)\n+      return std::logb(__x[0]);\n+    else if constexpr (__is_fixed_size_abi_v<_Abi>)\n+      {\n+\treturn {__private_init,\n+\t\t__data(__x)._M_apply_per_chunk([](auto __impl, auto __xx) {\n+\t\t  using _V = typename decltype(__impl)::simd_type;\n+\t\t  return __data(\n+\t\t    std::experimental::logb(_V(__private_init, __xx)));\n+\t\t})};\n+      }\n+#if _GLIBCXX_SIMD_X86INTRIN // {{{\n+    else if constexpr (__have_avx512vl && __is_sse_ps<_Tp, _Np>())\n+      return {__private_init,\n+\t      __auto_bitcast(_mm_getexp_ps(__to_intrin(__as_vector(__x))))};\n+    else if constexpr (__have_avx512vl && __is_sse_pd<_Tp, _Np>())\n+      return {__private_init, _mm_getexp_pd(__data(__x))};\n+    else if constexpr (__have_avx512vl && __is_avx_ps<_Tp, _Np>())\n+      return {__private_init, _mm256_getexp_ps(__data(__x))};\n+    else if constexpr (__have_avx512vl && __is_avx_pd<_Tp, _Np>())\n+      return {__private_init, _mm256_getexp_pd(__data(__x))};\n+    else if constexpr (__have_avx512f && __is_avx_ps<_Tp, _Np>())\n+      return {__private_init,\n+\t      __lo256(_mm512_getexp_ps(__auto_bitcast(__data(__x))))};\n+    else if constexpr (__have_avx512f && __is_avx_pd<_Tp, _Np>())\n+      return {__private_init,\n+\t      __lo256(_mm512_getexp_pd(__auto_bitcast(__data(__x))))};\n+    else if constexpr (__is_avx512_ps<_Tp, _Np>())\n+      return {__private_init, _mm512_getexp_ps(__data(__x))};\n+    else if constexpr (__is_avx512_pd<_Tp, _Np>())\n+      return {__private_init, _mm512_getexp_pd(__data(__x))};\n+#endif // _GLIBCXX_SIMD_X86INTRIN }}}\n+    else\n+      {\n+\tusing _V = simd<_Tp, _Abi>;\n+\tusing namespace std::experimental::__proposed;\n+\tauto __is_normal = isnormal(__x);\n+\n+\t// work on abs(__x) to reflect the return value on Linux for negative\n+\t// inputs (domain-error => implementation-defined value is returned)\n+\tconst _V abs_x = abs(__x);\n+\n+\t// __exponent(__x) returns the exponent value (bias removed) as\n+\t// simd<_Up> with integral _Up\n+\tauto&& __exponent = [](const _V& __v) {\n+\t  using namespace std::experimental::__proposed;\n+\t  using _IV = rebind_simd_t<\n+\t    conditional_t<sizeof(_Tp) == sizeof(_LLong), _LLong, int>, _V>;\n+\t  return (__bit_cast<_IV>(__v) >> (__digits_v<_Tp> - 1))\n+\t\t - (__max_exponent_v<_Tp> - 1);\n+\t};\n+\t_V __r = static_simd_cast<_V>(__exponent(abs_x));\n+\tif (_GLIBCXX_SIMD_IS_LIKELY(all_of(__is_normal)))\n+\t  // without corner cases (nan, inf, subnormal, zero) we have our\n+\t  // answer:\n+\t  return __r;\n+\tconst auto __is_zero = __x == 0;\n+\tconst auto __is_nan = isnan(__x);\n+\tconst auto __is_inf = isinf(__x);\n+\twhere(__is_zero, __r) = -__infinity_v<_Tp>;\n+\twhere(__is_nan, __r) = __x;\n+\twhere(__is_inf, __r) = __infinity_v<_Tp>;\n+\t__is_normal |= __is_zero || __is_nan || __is_inf;\n+\tif (all_of(__is_normal))\n+\t  // at this point everything but subnormals is handled\n+\t  return __r;\n+\t// subnormals repeat the exponent extraction after multiplication of the\n+\t// input with __a floating point value that has 112 (0x70) in its exponent\n+\t// (not too big for sp and large enough for dp)\n+\tconst _V __scaled = abs_x * _Tp(0x1p112);\n+\t_V __scaled_exp = static_simd_cast<_V>(__exponent(__scaled) - 112);\n+\twhere(__is_normal, __scaled_exp) = __r;\n+\treturn __scaled_exp;\n+      }\n+  }\n+\n+//}}}\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n+  modf(const simd<_Tp, _Abi>& __x, simd<_Tp, _Abi>* __iptr)\n+  {\n+    if constexpr (__is_scalar_abi<_Abi>()\n+\t\t  || (__is_fixed_size_abi_v<\n+\t\t\t_Abi> && simd_size_v<_Tp, _Abi> == 1))\n+      {\n+\t_Tp __tmp;\n+\t_Tp __r = std::modf(__x[0], &__tmp);\n+\t__iptr[0] = __tmp;\n+\treturn __r;\n+      }\n+    else\n+      {\n+\tconst auto __integral = trunc(__x);\n+\t*__iptr = __integral;\n+\tauto __r = __x - __integral;\n+#if !__FINITE_MATH_ONLY__\n+\twhere(isinf(__x), __r) = _Tp();\n+#endif\n+\treturn copysign(__r, __x);\n+      }\n+  }\n+\n+_GLIBCXX_SIMD_MATH_CALL2_(scalbn, int)\n+_GLIBCXX_SIMD_MATH_CALL2_(scalbln, long)\n+\n+_GLIBCXX_SIMD_MATH_CALL_(cbrt)\n+\n+_GLIBCXX_SIMD_MATH_CALL_(abs)\n+_GLIBCXX_SIMD_MATH_CALL_(fabs)\n+\n+// [parallel.simd.math] only asks for is_floating_point_v<_Tp> and forgot to\n+// allow signed integral _Tp\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<!is_floating_point_v<_Tp> && is_signed_v<_Tp>, simd<_Tp, _Abi>>\n+  abs(const simd<_Tp, _Abi>& __x)\n+  { return {__private_init, _Abi::_SimdImpl::_S_abs(__data(__x))}; }\n+\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<!is_floating_point_v<_Tp> && is_signed_v<_Tp>, simd<_Tp, _Abi>>\n+  fabs(const simd<_Tp, _Abi>& __x)\n+  { return {__private_init, _Abi::_SimdImpl::_S_abs(__data(__x))}; }\n+\n+// the following are overloads for functions in <cstdlib> and not covered by\n+// [parallel.simd.math]. I don't see much value in making them work, though\n+/*\n+template <typename _Abi> simd<long, _Abi> labs(const simd<long, _Abi> &__x)\n+{ return {__private_init, _Abi::_SimdImpl::abs(__data(__x))}; }\n+\n+template <typename _Abi> simd<long long, _Abi> llabs(const simd<long long, _Abi>\n+&__x)\n+{ return {__private_init, _Abi::_SimdImpl::abs(__data(__x))}; }\n+*/\n+\n+#define _GLIBCXX_SIMD_CVTING2(_NAME)                                           \\\n+template <typename _Tp, typename _Abi>                                         \\\n+  _GLIBCXX_SIMD_INTRINSIC simd<_Tp, _Abi> _NAME(                               \\\n+    const simd<_Tp, _Abi>& __x, const __type_identity_t<simd<_Tp, _Abi>>& __y) \\\n+  {                                                                            \\\n+    return _NAME(__x, __y);                                                    \\\n+  }                                                                            \\\n+                                                                               \\\n+template <typename _Tp, typename _Abi>                                         \\\n+  _GLIBCXX_SIMD_INTRINSIC simd<_Tp, _Abi> _NAME(                               \\\n+    const __type_identity_t<simd<_Tp, _Abi>>& __x, const simd<_Tp, _Abi>& __y) \\\n+  {                                                                            \\\n+    return _NAME(__x, __y);                                                    \\\n+  }\n+\n+#define _GLIBCXX_SIMD_CVTING3(_NAME)                                           \\\n+template <typename _Tp, typename _Abi>                                         \\\n+  _GLIBCXX_SIMD_INTRINSIC simd<_Tp, _Abi> _NAME(                               \\\n+    const __type_identity_t<simd<_Tp, _Abi>>& __x, const simd<_Tp, _Abi>& __y, \\\n+    const simd<_Tp, _Abi>& __z)                                                \\\n+  {                                                                            \\\n+    return _NAME(__x, __y, __z);                                               \\\n+  }                                                                            \\\n+                                                                               \\\n+template <typename _Tp, typename _Abi>                                         \\\n+  _GLIBCXX_SIMD_INTRINSIC simd<_Tp, _Abi> _NAME(                               \\\n+    const simd<_Tp, _Abi>& __x, const __type_identity_t<simd<_Tp, _Abi>>& __y, \\\n+    const simd<_Tp, _Abi>& __z)                                                \\\n+  {                                                                            \\\n+    return _NAME(__x, __y, __z);                                               \\\n+  }                                                                            \\\n+                                                                               \\\n+template <typename _Tp, typename _Abi>                                         \\\n+  _GLIBCXX_SIMD_INTRINSIC simd<_Tp, _Abi> _NAME(                               \\\n+    const simd<_Tp, _Abi>& __x, const simd<_Tp, _Abi>& __y,                    \\\n+    const __type_identity_t<simd<_Tp, _Abi>>& __z)                             \\\n+  {                                                                            \\\n+    return _NAME(__x, __y, __z);                                               \\\n+  }                                                                            \\\n+                                                                               \\\n+template <typename _Tp, typename _Abi>                                         \\\n+  _GLIBCXX_SIMD_INTRINSIC simd<_Tp, _Abi> _NAME(                               \\\n+    const simd<_Tp, _Abi>& __x, const __type_identity_t<simd<_Tp, _Abi>>& __y, \\\n+    const __type_identity_t<simd<_Tp, _Abi>>& __z)                             \\\n+  {                                                                            \\\n+    return _NAME(__x, __y, __z);                                               \\\n+  }                                                                            \\\n+                                                                               \\\n+template <typename _Tp, typename _Abi>                                         \\\n+  _GLIBCXX_SIMD_INTRINSIC simd<_Tp, _Abi> _NAME(                               \\\n+    const __type_identity_t<simd<_Tp, _Abi>>& __x, const simd<_Tp, _Abi>& __y, \\\n+    const __type_identity_t<simd<_Tp, _Abi>>& __z)                             \\\n+  {                                                                            \\\n+    return _NAME(__x, __y, __z);                                               \\\n+  }                                                                            \\\n+                                                                               \\\n+template <typename _Tp, typename _Abi>                                         \\\n+  _GLIBCXX_SIMD_INTRINSIC simd<_Tp, _Abi> _NAME(                               \\\n+    const __type_identity_t<simd<_Tp, _Abi>>& __x,                             \\\n+    const __type_identity_t<simd<_Tp, _Abi>>& __y, const simd<_Tp, _Abi>& __z) \\\n+  {                                                                            \\\n+    return _NAME(__x, __y, __z);                                               \\\n+  }\n+\n+template <typename _R, typename _ToApply, typename _Tp, typename... _Tps>\n+  _GLIBCXX_SIMD_INTRINSIC _R\n+  __fixed_size_apply(_ToApply&& __apply, const _Tp& __arg0,\n+\t\t     const _Tps&... __args)\n+  {\n+    return {__private_init,\n+\t    __data(__arg0)._M_apply_per_chunk(\n+\t      [&](auto __impl, const auto&... __inner) {\n+\t\tusing _V = typename decltype(__impl)::simd_type;\n+\t\treturn __data(__apply(_V(__private_init, __inner)...));\n+\t      },\n+\t      __data(__args)...)};\n+  }\n+\n+template <typename _VV>\n+  __remove_cvref_t<_VV>\n+  __hypot(_VV __x, _VV __y)\n+  {\n+    using _V = __remove_cvref_t<_VV>;\n+    using _Tp = typename _V::value_type;\n+    if constexpr (_V::size() == 1)\n+      return std::hypot(_Tp(__x[0]), _Tp(__y[0]));\n+    else if constexpr (__is_fixed_size_abi_v<typename _V::abi_type>)\n+      {\n+\treturn __fixed_size_apply<_V>([](auto __a,\n+\t\t\t\t\t auto __b) { return hypot(__a, __b); },\n+\t\t\t\t      __x, __y);\n+      }\n+    else\n+      {\n+\t// A simple solution for _Tp == float would be to cast to double and\n+\t// simply calculate sqrt(x\u00b2+y\u00b2) as it can't over-/underflow anymore with\n+\t// dp. It still needs the Annex F fixups though and isn't faster on\n+\t// Skylake-AVX512 (not even for SSE and AVX vectors, and really bad for\n+\t// AVX-512).\n+\tusing namespace __float_bitwise_operators;\n+\t_V __absx = abs(__x);          // no error\n+\t_V __absy = abs(__y);          // no error\n+\t_V __hi = max(__absx, __absy); // no error\n+\t_V __lo = min(__absy, __absx); // no error\n+\n+\t// round __hi down to the next power-of-2:\n+\t_GLIBCXX_SIMD_USE_CONSTEXPR_API _V __inf(__infinity_v<_Tp>);\n+\n+#ifndef __FAST_MATH__\n+\tif constexpr (__have_neon && !__have_neon_a32)\n+\t  { // With ARMv7 NEON, we have no subnormals and must use slightly\n+\t    // different strategy\n+\t    const _V __hi_exp = __hi & __inf;\n+\t    _V __scale_back = __hi_exp;\n+\t    // For large exponents (max & max/2) the inversion comes too close\n+\t    // to subnormals. Subtract 3 from the exponent:\n+\t    where(__hi_exp > 1, __scale_back) = __hi_exp * _Tp(0.125);\n+\t    // Invert and adjust for the off-by-one error of inversion via xor:\n+\t    const _V __scale = (__scale_back ^ __inf) * _Tp(.5);\n+\t    const _V __h1 = __hi * __scale;\n+\t    const _V __l1 = __lo * __scale;\n+\t    _V __r = __scale_back * sqrt(__h1 * __h1 + __l1 * __l1);\n+\t    // Fix up hypot(0, 0) to not be NaN:\n+\t    where(__hi == 0, __r) = 0;\n+\t    return __r;\n+\t  }\n+#endif\n+\n+#ifdef __FAST_MATH__\n+\t// With fast-math, ignore precision of subnormals and inputs from\n+\t// __finite_max_v/2 to __finite_max_v. This removes all\n+\t// branching/masking.\n+\tif constexpr (true)\n+#else\n+\tif (_GLIBCXX_SIMD_IS_LIKELY(all_of(isnormal(__x))\n+\t\t\t\t    && all_of(isnormal(__y))))\n+#endif\n+\t  {\n+\t    const _V __hi_exp = __hi & __inf;\n+\t    //((__hi + __hi) & __inf) ^ __inf almost works for computing\n+\t    //__scale,\n+\t    // except when (__hi + __hi) & __inf == __inf, in which case __scale\n+\t    // becomes 0 (should be min/2 instead) and thus loses the\n+\t    // information from __lo.\n+#ifdef __FAST_MATH__\n+\t    using _Ip = __int_for_sizeof_t<_Tp>;\n+\t    using _IV = rebind_simd_t<_Ip, _V>;\n+\t    const auto __as_int = __bit_cast<_IV>(__hi_exp);\n+\t    const _V __scale\n+\t      = __bit_cast<_V>(2 * __bit_cast<_Ip>(_Tp(1)) - __as_int);\n+#else\n+\t    const _V __scale = (__hi_exp ^ __inf) * _Tp(.5);\n+#endif\n+\t    _GLIBCXX_SIMD_USE_CONSTEXPR_API _V __mant_mask\n+\t      = __norm_min_v<_Tp> - __denorm_min_v<_Tp>;\n+\t    const _V __h1 = (__hi & __mant_mask) | _V(1);\n+\t    const _V __l1 = __lo * __scale;\n+\t    return __hi_exp * sqrt(__h1 * __h1 + __l1 * __l1);\n+\t  }\n+\telse\n+\t  {\n+\t    // slower path to support subnormals\n+\t    // if __hi is subnormal, avoid scaling by inf & final mul by 0\n+\t    // (which yields NaN) by using min()\n+\t    _V __scale = _V(1 / __norm_min_v<_Tp>);\n+\t    // invert exponent w/o error and w/o using the slow divider unit:\n+\t    // xor inverts the exponent but off by 1. Multiplication with .5\n+\t    // adjusts for the discrepancy.\n+\t    where(__hi >= __norm_min_v<_Tp>, __scale)\n+\t      = ((__hi & __inf) ^ __inf) * _Tp(.5);\n+\t    // adjust final exponent for subnormal inputs\n+\t    _V __hi_exp = __norm_min_v<_Tp>;\n+\t    where(__hi >= __norm_min_v<_Tp>, __hi_exp)\n+\t      = __hi & __inf;         // no error\n+\t    _V __h1 = __hi * __scale; // no error\n+\t    _V __l1 = __lo * __scale; // no error\n+\n+\t    // sqrt(x\u00b2+y\u00b2) = e*sqrt((x/e)\u00b2+(y/e)\u00b2):\n+\t    // this ensures no overflow in the argument to sqrt\n+\t    _V __r = __hi_exp * sqrt(__h1 * __h1 + __l1 * __l1);\n+#ifdef __STDC_IEC_559__\n+\t    // fixup for Annex F requirements\n+\t    // the naive fixup goes like this:\n+\t    //\n+\t    // where(__l1 == 0, __r)                      = __hi;\n+\t    // where(isunordered(__x, __y), __r)          = __quiet_NaN_v<_Tp>;\n+\t    // where(isinf(__absx) || isinf(__absy), __r) = __inf;\n+\t    //\n+\t    // The fixup can be prepared in parallel with the sqrt, requiring a\n+\t    // single blend step after hi_exp * sqrt, reducing latency and\n+\t    // throughput:\n+\t    _V __fixup = __hi; // __lo == 0\n+\t    where(isunordered(__x, __y), __fixup) = __quiet_NaN_v<_Tp>;\n+\t    where(isinf(__absx) || isinf(__absy), __fixup) = __inf;\n+\t    where(!(__lo == 0 || isunordered(__x, __y)\n+\t\t    || (isinf(__absx) || isinf(__absy))),\n+\t\t  __fixup)\n+\t      = __r;\n+\t    __r = __fixup;\n+#endif\n+\t    return __r;\n+\t  }\n+      }\n+  }\n+\n+template <typename _Tp, typename _Abi>\n+  _GLIBCXX_SIMD_INTRINSIC simd<_Tp, _Abi>\n+  hypot(const simd<_Tp, _Abi>& __x, const simd<_Tp, _Abi>& __y)\n+  {\n+    return __hypot<conditional_t<__is_fixed_size_abi_v<_Abi>,\n+\t\t\t\t const simd<_Tp, _Abi>&, simd<_Tp, _Abi>>>(__x,\n+\t\t\t\t\t\t\t\t\t   __y);\n+  }\n+\n+_GLIBCXX_SIMD_CVTING2(hypot)\n+\n+  template <typename _VV>\n+  __remove_cvref_t<_VV>\n+  __hypot(_VV __x, _VV __y, _VV __z)\n+  {\n+    using _V = __remove_cvref_t<_VV>;\n+    using _Abi = typename _V::abi_type;\n+    using _Tp = typename _V::value_type;\n+    /* FIXME: enable after PR77776 is resolved\n+    if constexpr (_V::size() == 1)\n+      return std::hypot(_Tp(__x[0]), _Tp(__y[0]), _Tp(__z[0]));\n+    else\n+    */\n+    if constexpr (__is_fixed_size_abi_v<_Abi> && _V::size() > 1)\n+      {\n+\treturn __fixed_size_apply<simd<_Tp, _Abi>>(\n+\t  [](auto __a, auto __b, auto __c) { return hypot(__a, __b, __c); },\n+\t  __x, __y, __z);\n+      }\n+    else\n+      {\n+\tusing namespace __float_bitwise_operators;\n+\tconst _V __absx = abs(__x);                 // no error\n+\tconst _V __absy = abs(__y);                 // no error\n+\tconst _V __absz = abs(__z);                 // no error\n+\t_V __hi = max(max(__absx, __absy), __absz); // no error\n+\t_V __l0 = min(__absz, max(__absx, __absy)); // no error\n+\t_V __l1 = min(__absy, __absx);              // no error\n+\tif constexpr (__digits_v<_Tp> == 64 && __max_exponent_v<_Tp> == 0x4000\n+\t\t      && __min_exponent_v<_Tp> == -0x3FFD && _V::size() == 1)\n+\t  { // Seems like x87 fp80, where bit 63 is always 1 unless subnormal or\n+\t    // NaN. In this case the bit-tricks don't work, they require IEC559\n+\t    // binary32 or binary64 format.\n+#ifdef __STDC_IEC_559__\n+\t    // fixup for Annex F requirements\n+\t    if (isinf(__absx[0]) || isinf(__absy[0]) || isinf(__absz[0]))\n+\t      return __infinity_v<_Tp>;\n+\t    else if (isunordered(__absx[0], __absy[0] + __absz[0]))\n+\t      return __quiet_NaN_v<_Tp>;\n+\t    else if (__l0[0] == 0 && __l1[0] == 0)\n+\t      return __hi;\n+#endif\n+\t    _V __hi_exp = __hi;\n+\t    const _ULLong __tmp = 0x8000'0000'0000'0000ull;\n+\t    __builtin_memcpy(&__data(__hi_exp), &__tmp, 8);\n+\t    const _V __scale = 1 / __hi_exp;\n+\t    __hi *= __scale;\n+\t    __l0 *= __scale;\n+\t    __l1 *= __scale;\n+\t    return __hi_exp * sqrt((__l0 * __l0 + __l1 * __l1) + __hi * __hi);\n+\t  }\n+\telse\n+\t  {\n+\t    // round __hi down to the next power-of-2:\n+\t    _GLIBCXX_SIMD_USE_CONSTEXPR_API _V __inf(__infinity_v<_Tp>);\n+\n+#ifndef __FAST_MATH__\n+\t    if constexpr (_V::size() > 1 && __have_neon && !__have_neon_a32)\n+\t      { // With ARMv7 NEON, we have no subnormals and must use slightly\n+\t\t// different strategy\n+\t\tconst _V __hi_exp = __hi & __inf;\n+\t\t_V __scale_back = __hi_exp;\n+\t\t// For large exponents (max & max/2) the inversion comes too\n+\t\t// close to subnormals. Subtract 3 from the exponent:\n+\t\twhere(__hi_exp > 1, __scale_back) = __hi_exp * _Tp(0.125);\n+\t\t// Invert and adjust for the off-by-one error of inversion via\n+\t\t// xor:\n+\t\tconst _V __scale = (__scale_back ^ __inf) * _Tp(.5);\n+\t\tconst _V __h1 = __hi * __scale;\n+\t\t__l0 *= __scale;\n+\t\t__l1 *= __scale;\n+\t\t_V __lo = __l0 * __l0\n+\t\t\t  + __l1 * __l1; // add the two smaller values first\n+\t\tasm(\"\" : \"+m\"(__lo));\n+\t\t_V __r = __scale_back * sqrt(__h1 * __h1 + __lo);\n+\t\t// Fix up hypot(0, 0, 0) to not be NaN:\n+\t\twhere(__hi == 0, __r) = 0;\n+\t\treturn __r;\n+\t      }\n+#endif\n+\n+#ifdef __FAST_MATH__\n+\t    // With fast-math, ignore precision of subnormals and inputs from\n+\t    // __finite_max_v/2 to __finite_max_v. This removes all\n+\t    // branching/masking.\n+\t    if constexpr (true)\n+#else\n+\t    if (_GLIBCXX_SIMD_IS_LIKELY(all_of(isnormal(__x))\n+\t\t\t\t\t&& all_of(isnormal(__y))\n+\t\t\t\t\t&& all_of(isnormal(__z))))\n+#endif\n+\t      {\n+\t\tconst _V __hi_exp = __hi & __inf;\n+\t\t//((__hi + __hi) & __inf) ^ __inf almost works for computing\n+\t\t//__scale, except when (__hi + __hi) & __inf == __inf, in which\n+\t\t// case __scale\n+\t\t// becomes 0 (should be min/2 instead) and thus loses the\n+\t\t// information from __lo.\n+#ifdef __FAST_MATH__\n+\t\tusing _Ip = __int_for_sizeof_t<_Tp>;\n+\t\tusing _IV = rebind_simd_t<_Ip, _V>;\n+\t\tconst auto __as_int = __bit_cast<_IV>(__hi_exp);\n+\t\tconst _V __scale\n+\t\t  = __bit_cast<_V>(2 * __bit_cast<_Ip>(_Tp(1)) - __as_int);\n+#else\n+\t\tconst _V __scale = (__hi_exp ^ __inf) * _Tp(.5);\n+#endif\n+\t\tconstexpr _Tp __mant_mask\n+\t\t  = __norm_min_v<_Tp> - __denorm_min_v<_Tp>;\n+\t\tconst _V __h1 = (__hi & _V(__mant_mask)) | _V(1);\n+\t\t__l0 *= __scale;\n+\t\t__l1 *= __scale;\n+\t\tconst _V __lo\n+\t\t  = __l0 * __l0\n+\t\t    + __l1 * __l1; // add the two smaller values first\n+\t\treturn __hi_exp * sqrt(__lo + __h1 * __h1);\n+\t      }\n+\t    else\n+\t      {\n+\t\t// slower path to support subnormals\n+\t\t// if __hi is subnormal, avoid scaling by inf & final mul by 0\n+\t\t// (which yields NaN) by using min()\n+\t\t_V __scale = _V(1 / __norm_min_v<_Tp>);\n+\t\t// invert exponent w/o error and w/o using the slow divider\n+\t\t// unit: xor inverts the exponent but off by 1. Multiplication\n+\t\t// with .5 adjusts for the discrepancy.\n+\t\twhere(__hi >= __norm_min_v<_Tp>, __scale)\n+\t\t  = ((__hi & __inf) ^ __inf) * _Tp(.5);\n+\t\t// adjust final exponent for subnormal inputs\n+\t\t_V __hi_exp = __norm_min_v<_Tp>;\n+\t\twhere(__hi >= __norm_min_v<_Tp>, __hi_exp)\n+\t\t  = __hi & __inf;         // no error\n+\t\t_V __h1 = __hi * __scale; // no error\n+\t\t__l0 *= __scale;          // no error\n+\t\t__l1 *= __scale;          // no error\n+\t\t_V __lo = __l0 * __l0\n+\t\t\t  + __l1 * __l1; // add the two smaller values first\n+\t\t_V __r = __hi_exp * sqrt(__lo + __h1 * __h1);\n+#ifdef __STDC_IEC_559__\n+\t\t// fixup for Annex F requirements\n+\t\t_V __fixup = __hi; // __lo == 0\n+\t\t// where(__lo == 0, __fixup)                   = __hi;\n+\t\twhere(isunordered(__x, __y + __z), __fixup)\n+\t\t  = __quiet_NaN_v<_Tp>;\n+\t\twhere(isinf(__absx) || isinf(__absy) || isinf(__absz), __fixup)\n+\t\t  = __inf;\n+\t\t// Instead of __lo == 0, the following could depend on __h1\u00b2 ==\n+\t\t// __h1\u00b2 + __lo (i.e. __hi is so much larger than the other two\n+\t\t// inputs that the result is exactly __hi). While this may\n+\t\t// improve precision, it is likely to reduce efficiency if the\n+\t\t// ISA has FMAs (because __h1\u00b2 + __lo is an FMA, but the\n+\t\t// intermediate\n+\t\t// __h1\u00b2 must be kept)\n+\t\twhere(!(__lo == 0 || isunordered(__x, __y + __z)\n+\t\t\t|| isinf(__absx) || isinf(__absy) || isinf(__absz)),\n+\t\t      __fixup)\n+\t\t  = __r;\n+\t\t__r = __fixup;\n+#endif\n+\t\treturn __r;\n+\t      }\n+\t  }\n+      }\n+  }\n+\n+  template <typename _Tp, typename _Abi>\n+  _GLIBCXX_SIMD_INTRINSIC simd<_Tp, _Abi>\n+  hypot(const simd<_Tp, _Abi>& __x, const simd<_Tp, _Abi>& __y,\n+\tconst simd<_Tp, _Abi>& __z)\n+  {\n+    return __hypot<conditional_t<__is_fixed_size_abi_v<_Abi>,\n+\t\t\t\t const simd<_Tp, _Abi>&, simd<_Tp, _Abi>>>(__x,\n+\t\t\t\t\t\t\t\t\t   __y,\n+\t\t\t\t\t\t\t\t\t   __z);\n+  }\n+\n+_GLIBCXX_SIMD_CVTING3(hypot)\n+\n+_GLIBCXX_SIMD_MATH_CALL2_(pow, _Tp)\n+\n+_GLIBCXX_SIMD_MATH_CALL_(sqrt)\n+_GLIBCXX_SIMD_MATH_CALL_(erf)\n+_GLIBCXX_SIMD_MATH_CALL_(erfc)\n+_GLIBCXX_SIMD_MATH_CALL_(lgamma)\n+_GLIBCXX_SIMD_MATH_CALL_(tgamma)\n+_GLIBCXX_SIMD_MATH_CALL_(ceil)\n+_GLIBCXX_SIMD_MATH_CALL_(floor)\n+_GLIBCXX_SIMD_MATH_CALL_(nearbyint)\n+_GLIBCXX_SIMD_MATH_CALL_(rint)\n+_GLIBCXX_SIMD_MATH_CALL_(lrint)\n+_GLIBCXX_SIMD_MATH_CALL_(llrint)\n+\n+_GLIBCXX_SIMD_MATH_CALL_(round)\n+_GLIBCXX_SIMD_MATH_CALL_(lround)\n+_GLIBCXX_SIMD_MATH_CALL_(llround)\n+\n+_GLIBCXX_SIMD_MATH_CALL_(trunc)\n+\n+_GLIBCXX_SIMD_MATH_CALL2_(fmod, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL2_(remainder, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL3_(remquo, _Tp, int*)\n+\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n+  copysign(const simd<_Tp, _Abi>& __x, const simd<_Tp, _Abi>& __y)\n+  {\n+    if constexpr (simd_size_v<_Tp, _Abi> == 1)\n+      return std::copysign(__x[0], __y[0]);\n+    else if constexpr (is_same_v<_Tp, long double> && sizeof(_Tp) == 12)\n+      // Remove this case once __bit_cast is implemented via __builtin_bit_cast.\n+      // It is necessary, because __signmask below cannot be computed at compile\n+      // time.\n+      return simd<_Tp, _Abi>(\n+\t[&](auto __i) { return std::copysign(__x[__i], __y[__i]); });\n+    else\n+      {\n+\tusing _V = simd<_Tp, _Abi>;\n+\tusing namespace std::experimental::__float_bitwise_operators;\n+\t_GLIBCXX_SIMD_USE_CONSTEXPR_API auto __signmask = _V(1) ^ _V(-1);\n+\treturn (__x & (__x ^ __signmask)) | (__y & __signmask);\n+      }\n+  }\n+\n+_GLIBCXX_SIMD_MATH_CALL2_(nextafter, _Tp)\n+// not covered in [parallel.simd.math]:\n+// _GLIBCXX_SIMD_MATH_CALL2_(nexttoward, long double)\n+_GLIBCXX_SIMD_MATH_CALL2_(fdim, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL2_(fmax, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL2_(fmin, _Tp)\n+\n+_GLIBCXX_SIMD_MATH_CALL3_(fma, _Tp, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL_(fpclassify)\n+_GLIBCXX_SIMD_MATH_CALL_(isfinite)\n+\n+// isnan and isinf require special treatment because old glibc may declare\n+// `int isinf(double)`.\n+template <typename _Tp, typename _Abi, typename...,\n+\t  typename _R = _Math_return_type_t<bool, _Tp, _Abi>>\n+  enable_if_t<is_floating_point_v<_Tp>, _R>\n+  isinf(simd<_Tp, _Abi> __x)\n+  { return {__private_init, _Abi::_SimdImpl::_S_isinf(__data(__x))}; }\n+\n+template <typename _Tp, typename _Abi, typename...,\n+\t  typename _R = _Math_return_type_t<bool, _Tp, _Abi>>\n+  enable_if_t<is_floating_point_v<_Tp>, _R>\n+  isnan(simd<_Tp, _Abi> __x)\n+  { return {__private_init, _Abi::_SimdImpl::_S_isnan(__data(__x))}; }\n+\n+_GLIBCXX_SIMD_MATH_CALL_(isnormal)\n+\n+template <typename..., typename _Tp, typename _Abi>\n+  simd_mask<_Tp, _Abi>\n+  signbit(simd<_Tp, _Abi> __x)\n+  {\n+    if constexpr (is_integral_v<_Tp>)\n+      {\n+\tif constexpr (is_unsigned_v<_Tp>)\n+\t  return simd_mask<_Tp, _Abi>{}; // false\n+\telse\n+\t  return __x < 0;\n+      }\n+    else\n+      return {__private_init, _Abi::_SimdImpl::_S_signbit(__data(__x))};\n+  }\n+\n+_GLIBCXX_SIMD_MATH_CALL2_(isgreater, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL2_(isgreaterequal, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL2_(isless, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL2_(islessequal, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL2_(islessgreater, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL2_(isunordered, _Tp)\n+\n+/* not covered in [parallel.simd.math]\n+template <typename _Abi> __doublev<_Abi> nan(const char* tagp);\n+template <typename _Abi> __floatv<_Abi> nanf(const char* tagp);\n+template <typename _Abi> __ldoublev<_Abi> nanl(const char* tagp);\n+\n+template <typename _V> struct simd_div_t {\n+    _V quot, rem;\n+};\n+\n+template <typename _Abi>\n+simd_div_t<_SCharv<_Abi>> div(_SCharv<_Abi> numer,\n+\t\t\t\t\t _SCharv<_Abi> denom);\n+template <typename _Abi>\n+simd_div_t<__shortv<_Abi>> div(__shortv<_Abi> numer,\n+\t\t\t\t\t __shortv<_Abi> denom);\n+template <typename _Abi>\n+simd_div_t<__intv<_Abi>> div(__intv<_Abi> numer, __intv<_Abi> denom);\n+template <typename _Abi>\n+simd_div_t<__longv<_Abi>> div(__longv<_Abi> numer,\n+\t\t\t\t\t__longv<_Abi> denom);\n+template <typename _Abi>\n+simd_div_t<__llongv<_Abi>> div(__llongv<_Abi> numer,\n+\t\t\t\t\t __llongv<_Abi> denom);\n+*/\n+\n+// special math {{{\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n+  assoc_laguerre(const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __n,\n+\t\t const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __m,\n+\t\t const simd<_Tp, _Abi>& __x)\n+  {\n+    return simd<_Tp, _Abi>([&](auto __i) {\n+      return std::assoc_laguerre(__n[__i], __m[__i], __x[__i]);\n+    });\n+  }\n+\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n+  assoc_legendre(const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __n,\n+\t\t const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __m,\n+\t\t const simd<_Tp, _Abi>& __x)\n+  {\n+    return simd<_Tp, _Abi>([&](auto __i) {\n+      return std::assoc_legendre(__n[__i], __m[__i], __x[__i]);\n+    });\n+  }\n+\n+_GLIBCXX_SIMD_MATH_CALL2_(beta, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL_(comp_ellint_1)\n+_GLIBCXX_SIMD_MATH_CALL_(comp_ellint_2)\n+_GLIBCXX_SIMD_MATH_CALL2_(comp_ellint_3, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL2_(cyl_bessel_i, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL2_(cyl_bessel_j, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL2_(cyl_bessel_k, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL2_(cyl_neumann, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL2_(ellint_1, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL2_(ellint_2, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL3_(ellint_3, _Tp, _Tp)\n+_GLIBCXX_SIMD_MATH_CALL_(expint)\n+\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n+  hermite(const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __n,\n+\t  const simd<_Tp, _Abi>& __x)\n+  {\n+    return simd<_Tp, _Abi>(\n+      [&](auto __i) { return std::hermite(__n[__i], __x[__i]); });\n+  }\n+\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n+  laguerre(const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __n,\n+\t   const simd<_Tp, _Abi>& __x)\n+  {\n+    return simd<_Tp, _Abi>(\n+      [&](auto __i) { return std::laguerre(__n[__i], __x[__i]); });\n+  }\n+\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n+  legendre(const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __n,\n+\t   const simd<_Tp, _Abi>& __x)\n+  {\n+    return simd<_Tp, _Abi>(\n+      [&](auto __i) { return std::legendre(__n[__i], __x[__i]); });\n+  }\n+\n+_GLIBCXX_SIMD_MATH_CALL_(riemann_zeta)\n+\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n+  sph_bessel(const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __n,\n+\t     const simd<_Tp, _Abi>& __x)\n+  {\n+    return simd<_Tp, _Abi>(\n+      [&](auto __i) { return std::sph_bessel(__n[__i], __x[__i]); });\n+  }\n+\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n+  sph_legendre(const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __l,\n+\t       const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __m,\n+\t       const simd<_Tp, _Abi>& theta)\n+  {\n+    return simd<_Tp, _Abi>([&](auto __i) {\n+      return std::assoc_legendre(__l[__i], __m[__i], theta[__i]);\n+    });\n+  }\n+\n+template <typename _Tp, typename _Abi>\n+  enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n+  sph_neumann(const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __n,\n+\t      const simd<_Tp, _Abi>& __x)\n+  {\n+    return simd<_Tp, _Abi>(\n+      [&](auto __i) { return std::sph_neumann(__n[__i], __x[__i]); });\n+  }\n+// }}}\n+\n+#undef _GLIBCXX_SIMD_MATH_CALL_\n+#undef _GLIBCXX_SIMD_MATH_CALL2_\n+#undef _GLIBCXX_SIMD_MATH_CALL3_\n+\n+_GLIBCXX_SIMD_END_NAMESPACE\n+\n+#endif // __cplusplus >= 201703L\n+#endif // _GLIBCXX_EXPERIMENTAL_SIMD_MATH_H_\n+\n+// vim: foldmethod=marker sw=2 ts=8 noet sts=2"}, {"sha": "a3a8ffe165fc7db63efcbaa07a866dc48833c4f5", "filename": "libstdc++-v3/include/experimental/bits/simd_neon.h", "status": "added", "additions": 519, "deletions": 0, "changes": 519, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_neon.h?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -0,0 +1,519 @@\n+// Simd NEON specific implementations -*- C++ -*-\n+\n+// Copyright (C) 2020 Free Software Foundation, Inc.\n+//\n+// This file is part of the GNU ISO C++ Library.  This library is free\n+// software; you can redistribute it and/or modify it under the\n+// terms of the GNU General Public License as published by the\n+// Free Software Foundation; either version 3, or (at your option)\n+// any later version.\n+\n+// This library is distributed in the hope that it will be useful,\n+// but WITHOUT ANY WARRANTY; without even the implied warranty of\n+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+// GNU General Public License for more details.\n+\n+// Under Section 7 of GPL version 3, you are granted additional\n+// permissions described in the GCC Runtime Library Exception, version\n+// 3.1, as published by the Free Software Foundation.\n+\n+// You should have received a copy of the GNU General Public License and\n+// a copy of the GCC Runtime Library Exception along with this program;\n+// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+// <http://www.gnu.org/licenses/>.\n+\n+#ifndef _GLIBCXX_EXPERIMENTAL_SIMD_NEON_H_\n+#define _GLIBCXX_EXPERIMENTAL_SIMD_NEON_H_\n+\n+#if __cplusplus >= 201703L\n+\n+#if !_GLIBCXX_SIMD_HAVE_NEON\n+#error \"simd_neon.h may only be included when NEON on ARM is available\"\n+#endif\n+\n+_GLIBCXX_SIMD_BEGIN_NAMESPACE\n+\n+// _CommonImplNeon {{{\n+struct _CommonImplNeon : _CommonImplBuiltin\n+{\n+  // _S_store {{{\n+  using _CommonImplBuiltin::_S_store;\n+\n+  // }}}\n+};\n+\n+// }}}\n+// _SimdImplNeon {{{\n+template <typename _Abi>\n+  struct _SimdImplNeon : _SimdImplBuiltin<_Abi>\n+  {\n+    using _Base = _SimdImplBuiltin<_Abi>;\n+\n+    template <typename _Tp>\n+      using _MaskMember = typename _Base::template _MaskMember<_Tp>;\n+\n+    template <typename _Tp>\n+      static constexpr size_t _S_max_store_size = 16;\n+\n+    // _S_masked_load {{{\n+    template <typename _Tp, size_t _Np, typename _Up>\n+      static inline _SimdWrapper<_Tp, _Np>\n+      _S_masked_load(_SimdWrapper<_Tp, _Np> __merge, _MaskMember<_Tp> __k,\n+\t\t     const _Up* __mem) noexcept\n+      {\n+\t__execute_n_times<_Np>([&](auto __i) {\n+\t  if (__k[__i] != 0)\n+\t    __merge._M_set(__i, static_cast<_Tp>(__mem[__i]));\n+\t});\n+\treturn __merge;\n+      }\n+\n+    // }}}\n+    // _S_masked_store_nocvt {{{\n+    template <typename _Tp, size_t _Np>\n+      _GLIBCXX_SIMD_INTRINSIC static void\n+      _S_masked_store_nocvt(_SimdWrapper<_Tp, _Np> __v, _Tp* __mem,\n+\t\t\t    _MaskMember<_Tp> __k)\n+      {\n+\t__execute_n_times<_Np>([&](auto __i) {\n+\t  if (__k[__i] != 0)\n+\t    __mem[__i] = __v[__i];\n+\t});\n+      }\n+\n+    // }}}\n+    // _S_reduce {{{\n+    template <typename _Tp, typename _BinaryOperation>\n+      _GLIBCXX_SIMD_INTRINSIC static _Tp\n+      _S_reduce(simd<_Tp, _Abi> __x, _BinaryOperation&& __binary_op)\n+      {\n+\tconstexpr size_t _Np = __x.size();\n+\tif constexpr (sizeof(__x) == 16 && _Np >= 4\n+\t\t      && !_Abi::template _S_is_partial<_Tp>)\n+\t  {\n+\t    const auto __halves = split<simd<_Tp, simd_abi::_Neon<8>>>(__x);\n+\t    const auto __y = __binary_op(__halves[0], __halves[1]);\n+\t    return _SimdImplNeon<simd_abi::_Neon<8>>::_S_reduce(\n+\t      __y, static_cast<_BinaryOperation&&>(__binary_op));\n+\t  }\n+\telse if constexpr (_Np == 8)\n+\t  {\n+\t    __x = __binary_op(__x, _Base::template _M_make_simd<_Tp, _Np>(\n+\t\t\t\t     __vector_permute<1, 0, 3, 2, 5, 4, 7, 6>(\n+\t\t\t\t       __x._M_data)));\n+\t    __x = __binary_op(__x, _Base::template _M_make_simd<_Tp, _Np>(\n+\t\t\t\t     __vector_permute<3, 2, 1, 0, 7, 6, 5, 4>(\n+\t\t\t\t       __x._M_data)));\n+\t    __x = __binary_op(__x, _Base::template _M_make_simd<_Tp, _Np>(\n+\t\t\t\t     __vector_permute<7, 6, 5, 4, 3, 2, 1, 0>(\n+\t\t\t\t       __x._M_data)));\n+\t    return __x[0];\n+\t  }\n+\telse if constexpr (_Np == 4)\n+\t  {\n+\t    __x\n+\t      = __binary_op(__x, _Base::template _M_make_simd<_Tp, _Np>(\n+\t\t\t\t   __vector_permute<1, 0, 3, 2>(__x._M_data)));\n+\t    __x\n+\t      = __binary_op(__x, _Base::template _M_make_simd<_Tp, _Np>(\n+\t\t\t\t   __vector_permute<3, 2, 1, 0>(__x._M_data)));\n+\t    return __x[0];\n+\t  }\n+\telse if constexpr (_Np == 2)\n+\t  {\n+\t    __x = __binary_op(__x, _Base::template _M_make_simd<_Tp, _Np>(\n+\t\t\t\t     __vector_permute<1, 0>(__x._M_data)));\n+\t    return __x[0];\n+\t  }\n+\telse\n+\t  return _Base::_S_reduce(__x,\n+\t\t\t\t  static_cast<_BinaryOperation&&>(__binary_op));\n+      }\n+\n+    // }}}\n+    // math {{{\n+    // _S_sqrt {{{\n+    template <typename _Tp, typename _TVT = _VectorTraits<_Tp>>\n+      _GLIBCXX_SIMD_INTRINSIC static _Tp _S_sqrt(_Tp __x)\n+      {\n+\tif constexpr (__have_neon_a64)\n+\t  {\n+\t    const auto __intrin = __to_intrin(__x);\n+\t    if constexpr (_TVT::template _S_is<float, 2>)\n+\t      return vsqrt_f32(__intrin);\n+\t    else if constexpr (_TVT::template _S_is<float, 4>)\n+\t      return vsqrtq_f32(__intrin);\n+\t    else if constexpr (_TVT::template _S_is<double, 1>)\n+\t      return vsqrt_f64(__intrin);\n+\t    else if constexpr (_TVT::template _S_is<double, 2>)\n+\t      return vsqrtq_f64(__intrin);\n+\t    else\n+\t      __assert_unreachable<_Tp>();\n+\t  }\n+\telse\n+\t  return _Base::_S_sqrt(__x);\n+      }\n+\n+    // }}}\n+    // _S_trunc {{{\n+    template <typename _TW, typename _TVT = _VectorTraits<_TW>>\n+      _GLIBCXX_SIMD_INTRINSIC static _TW _S_trunc(_TW __x)\n+      {\n+\tusing _Tp = typename _TVT::value_type;\n+\tif constexpr (__have_neon_a32)\n+\t  {\n+\t    const auto __intrin = __to_intrin(__x);\n+\t    if constexpr (_TVT::template _S_is<float, 2>)\n+\t      return vrnd_f32(__intrin);\n+\t    else if constexpr (_TVT::template _S_is<float, 4>)\n+\t      return vrndq_f32(__intrin);\n+\t    else if constexpr (_TVT::template _S_is<double, 1>)\n+\t      return vrnd_f64(__intrin);\n+\t    else if constexpr (_TVT::template _S_is<double, 2>)\n+\t      return vrndq_f64(__intrin);\n+\t    else\n+\t      __assert_unreachable<_Tp>();\n+\t  }\n+\telse if constexpr (is_same_v<_Tp, float>)\n+\t  {\n+\t    auto __intrin = __to_intrin(__x);\n+\t    if constexpr (sizeof(__x) == 16)\n+\t      __intrin = vcvtq_f32_s32(vcvtq_s32_f32(__intrin));\n+\t    else\n+\t      __intrin = vcvt_f32_s32(vcvt_s32_f32(__intrin));\n+\t    return _Base::_S_abs(__x)._M_data < 0x1p23f\n+\t\t     ? __vector_bitcast<float>(__intrin)\n+\t\t     : __x._M_data;\n+\t  }\n+\telse\n+\t  return _Base::_S_trunc(__x);\n+      }\n+\n+    // }}}\n+    // _S_round {{{\n+    template <typename _Tp, size_t _Np>\n+      _GLIBCXX_SIMD_INTRINSIC static _SimdWrapper<_Tp, _Np>\n+      _S_round(_SimdWrapper<_Tp, _Np> __x)\n+      {\n+\tif constexpr (__have_neon_a32)\n+\t  {\n+\t    const auto __intrin = __to_intrin(__x);\n+\t    if constexpr (sizeof(_Tp) == 4 && sizeof(__x) == 8)\n+\t      return vrnda_f32(__intrin);\n+\t    else if constexpr (sizeof(_Tp) == 4 && sizeof(__x) == 16)\n+\t      return vrndaq_f32(__intrin);\n+\t    else if constexpr (sizeof(_Tp) == 8 && sizeof(__x) == 8)\n+\t      return vrnda_f64(__intrin);\n+\t    else if constexpr (sizeof(_Tp) == 8 && sizeof(__x) == 16)\n+\t      return vrndaq_f64(__intrin);\n+\t    else\n+\t      __assert_unreachable<_Tp>();\n+\t  }\n+\telse\n+\t  return _Base::_S_round(__x);\n+      }\n+\n+    // }}}\n+    // _S_floor {{{\n+    template <typename _Tp, typename _TVT = _VectorTraits<_Tp>>\n+      _GLIBCXX_SIMD_INTRINSIC static _Tp _S_floor(_Tp __x)\n+      {\n+\tif constexpr (__have_neon_a32)\n+\t  {\n+\t    const auto __intrin = __to_intrin(__x);\n+\t    if constexpr (_TVT::template _S_is<float, 2>)\n+\t      return vrndm_f32(__intrin);\n+\t    else if constexpr (_TVT::template _S_is<float, 4>)\n+\t      return vrndmq_f32(__intrin);\n+\t    else if constexpr (_TVT::template _S_is<double, 1>)\n+\t      return vrndm_f64(__intrin);\n+\t    else if constexpr (_TVT::template _S_is<double, 2>)\n+\t      return vrndmq_f64(__intrin);\n+\t    else\n+\t      __assert_unreachable<_Tp>();\n+\t  }\n+\telse\n+\t  return _Base::_S_floor(__x);\n+      }\n+\n+    // }}}\n+    // _S_ceil {{{\n+    template <typename _Tp, typename _TVT = _VectorTraits<_Tp>>\n+      _GLIBCXX_SIMD_INTRINSIC static _Tp _S_ceil(_Tp __x)\n+      {\n+\tif constexpr (__have_neon_a32)\n+\t  {\n+\t    const auto __intrin = __to_intrin(__x);\n+\t    if constexpr (_TVT::template _S_is<float, 2>)\n+\t      return vrndp_f32(__intrin);\n+\t    else if constexpr (_TVT::template _S_is<float, 4>)\n+\t      return vrndpq_f32(__intrin);\n+\t    else if constexpr (_TVT::template _S_is<double, 1>)\n+\t      return vrndp_f64(__intrin);\n+\t    else if constexpr (_TVT::template _S_is<double, 2>)\n+\t      return vrndpq_f64(__intrin);\n+\t    else\n+\t      __assert_unreachable<_Tp>();\n+\t  }\n+\telse\n+\t  return _Base::_S_ceil(__x);\n+      }\n+\n+    //}}} }}}\n+  }; // }}}\n+// _MaskImplNeonMixin {{{\n+struct _MaskImplNeonMixin\n+{\n+  using _Base = _MaskImplBuiltinMixin;\n+\n+  template <typename _Tp, size_t _Np>\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr _SanitizedBitMask<_Np>\n+    _S_to_bits(_SimdWrapper<_Tp, _Np> __x)\n+    {\n+      if (__builtin_is_constant_evaluated())\n+\treturn _Base::_S_to_bits(__x);\n+\n+      using _I = __int_for_sizeof_t<_Tp>;\n+      if constexpr (sizeof(__x) == 16)\n+\t{\n+\t  auto __asint = __vector_bitcast<_I>(__x);\n+#ifdef __aarch64__\n+\t  [[maybe_unused]] constexpr auto __zero = decltype(__asint)();\n+#else\n+\t  [[maybe_unused]] constexpr auto __zero = decltype(__lo64(__asint))();\n+#endif\n+\t  if constexpr (sizeof(_Tp) == 1)\n+\t    {\n+\t      constexpr auto __bitsel\n+\t\t= __generate_from_n_evaluations<16, __vector_type_t<_I, 16>>(\n+\t\t  [&](auto __i) {\n+\t\t    return static_cast<_I>(\n+\t\t      __i < _Np ? (__i < 8 ? 1 << __i : 1 << (__i - 8)) : 0);\n+\t\t  });\n+\t      __asint &= __bitsel;\n+#ifdef __aarch64__\n+\t      return __vector_bitcast<_UShort>(\n+\t\tvpaddq_s8(vpaddq_s8(vpaddq_s8(__asint, __zero), __zero),\n+\t\t\t  __zero))[0];\n+#else\n+\t      return __vector_bitcast<_UShort>(\n+\t\tvpadd_s8(vpadd_s8(vpadd_s8(__lo64(__asint), __hi64(__asint)),\n+\t\t\t\t  __zero),\n+\t\t\t __zero))[0];\n+#endif\n+\t    }\n+\t  else if constexpr (sizeof(_Tp) == 2)\n+\t    {\n+\t      constexpr auto __bitsel\n+\t\t= __generate_from_n_evaluations<8, __vector_type_t<_I, 8>>(\n+\t\t  [&](auto __i) {\n+\t\t    return static_cast<_I>(__i < _Np ? 1 << __i : 0);\n+\t\t  });\n+\t      __asint &= __bitsel;\n+#ifdef __aarch64__\n+\t      return vpaddq_s16(vpaddq_s16(vpaddq_s16(__asint, __zero), __zero),\n+\t\t\t\t__zero)[0];\n+#else\n+\t      return vpadd_s16(\n+\t\tvpadd_s16(vpadd_s16(__lo64(__asint), __hi64(__asint)), __zero),\n+\t\t__zero)[0];\n+#endif\n+\t    }\n+\t  else if constexpr (sizeof(_Tp) == 4)\n+\t    {\n+\t      constexpr auto __bitsel\n+\t\t= __generate_from_n_evaluations<4, __vector_type_t<_I, 4>>(\n+\t\t  [&](auto __i) {\n+\t\t    return static_cast<_I>(__i < _Np ? 1 << __i : 0);\n+\t\t  });\n+\t      __asint &= __bitsel;\n+#ifdef __aarch64__\n+\t      return vpaddq_s32(vpaddq_s32(__asint, __zero), __zero)[0];\n+#else\n+\t      return vpadd_s32(vpadd_s32(__lo64(__asint), __hi64(__asint)),\n+\t\t\t       __zero)[0];\n+#endif\n+\t    }\n+\t  else if constexpr (sizeof(_Tp) == 8)\n+\t    return (__asint[0] & 1) | (__asint[1] & 2);\n+\t  else\n+\t    __assert_unreachable<_Tp>();\n+\t}\n+      else if constexpr (sizeof(__x) == 8)\n+\t{\n+\t  auto __asint = __vector_bitcast<_I>(__x);\n+\t  [[maybe_unused]] constexpr auto __zero = decltype(__asint)();\n+\t  if constexpr (sizeof(_Tp) == 1)\n+\t    {\n+\t      constexpr auto __bitsel\n+\t\t= __generate_from_n_evaluations<8, __vector_type_t<_I, 8>>(\n+\t\t  [&](auto __i) {\n+\t\t    return static_cast<_I>(__i < _Np ? 1 << __i : 0);\n+\t\t  });\n+\t      __asint &= __bitsel;\n+\t      return vpadd_s8(vpadd_s8(vpadd_s8(__asint, __zero), __zero),\n+\t\t\t      __zero)[0];\n+\t    }\n+\t  else if constexpr (sizeof(_Tp) == 2)\n+\t    {\n+\t      constexpr auto __bitsel\n+\t\t= __generate_from_n_evaluations<4, __vector_type_t<_I, 4>>(\n+\t\t  [&](auto __i) {\n+\t\t    return static_cast<_I>(__i < _Np ? 1 << __i : 0);\n+\t\t  });\n+\t      __asint &= __bitsel;\n+\t      return vpadd_s16(vpadd_s16(__asint, __zero), __zero)[0];\n+\t    }\n+\t  else if constexpr (sizeof(_Tp) == 4)\n+\t    {\n+\t      __asint &= __make_vector<_I>(0x1, 0x2);\n+\t      return vpadd_s32(__asint, __zero)[0];\n+\t    }\n+\t  else\n+\t    __assert_unreachable<_Tp>();\n+\t}\n+      else\n+\treturn _Base::_S_to_bits(__x);\n+    }\n+};\n+\n+// }}}\n+// _MaskImplNeon {{{\n+template <typename _Abi>\n+  struct _MaskImplNeon : _MaskImplNeonMixin, _MaskImplBuiltin<_Abi>\n+  {\n+    using _MaskImplBuiltinMixin::_S_to_maskvector;\n+    using _MaskImplNeonMixin::_S_to_bits;\n+    using _Base = _MaskImplBuiltin<_Abi>;\n+    using _Base::_S_convert;\n+\n+    // _S_all_of {{{\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static bool _S_all_of(simd_mask<_Tp, _Abi> __k)\n+      {\n+\tconst auto __kk\n+\t  = __vector_bitcast<char>(__k._M_data)\n+\t    | ~__vector_bitcast<char>(_Abi::template _S_implicit_mask<_Tp>());\n+\tif constexpr (sizeof(__k) == 16)\n+\t  {\n+\t    const auto __x = __vector_bitcast<long long>(__kk);\n+\t    return __x[0] + __x[1] == -2;\n+\t  }\n+\telse if constexpr (sizeof(__k) <= 8)\n+\t  return __bit_cast<__int_for_sizeof_t<decltype(__kk)>>(__kk) == -1;\n+\telse\n+\t  __assert_unreachable<_Tp>();\n+      }\n+\n+    // }}}\n+    // _S_any_of {{{\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static bool _S_any_of(simd_mask<_Tp, _Abi> __k)\n+      {\n+\tconst auto __kk\n+\t  = __vector_bitcast<char>(__k._M_data)\n+\t    | ~__vector_bitcast<char>(_Abi::template _S_implicit_mask<_Tp>());\n+\tif constexpr (sizeof(__k) == 16)\n+\t  {\n+\t    const auto __x = __vector_bitcast<long long>(__kk);\n+\t    return (__x[0] | __x[1]) != 0;\n+\t  }\n+\telse if constexpr (sizeof(__k) <= 8)\n+\t  return __bit_cast<__int_for_sizeof_t<decltype(__kk)>>(__kk) != 0;\n+\telse\n+\t  __assert_unreachable<_Tp>();\n+      }\n+\n+    // }}}\n+    // _S_none_of {{{\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static bool _S_none_of(simd_mask<_Tp, _Abi> __k)\n+      {\n+\tconst auto __kk = _Abi::_S_masked(__k._M_data);\n+\tif constexpr (sizeof(__k) == 16)\n+\t  {\n+\t    const auto __x = __vector_bitcast<long long>(__kk);\n+\t    return (__x[0] | __x[1]) == 0;\n+\t  }\n+\telse if constexpr (sizeof(__k) <= 8)\n+\t  return __bit_cast<__int_for_sizeof_t<decltype(__kk)>>(__kk) == 0;\n+\telse\n+\t  __assert_unreachable<_Tp>();\n+      }\n+\n+    // }}}\n+    // _S_some_of {{{\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static bool _S_some_of(simd_mask<_Tp, _Abi> __k)\n+      {\n+\tif constexpr (sizeof(__k) <= 8)\n+\t  {\n+\t    const auto __kk = __vector_bitcast<char>(__k._M_data)\n+\t\t\t      | ~__vector_bitcast<char>(\n+\t\t\t\t_Abi::template _S_implicit_mask<_Tp>());\n+\t    using _Up = make_unsigned_t<__int_for_sizeof_t<decltype(__kk)>>;\n+\t    return __bit_cast<_Up>(__kk) + 1 > 1;\n+\t  }\n+\telse\n+\t  return _Base::_S_some_of(__k);\n+      }\n+\n+    // }}}\n+    // _S_popcount {{{\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static int _S_popcount(simd_mask<_Tp, _Abi> __k)\n+      {\n+\tif constexpr (sizeof(_Tp) == 1)\n+\t  {\n+\t    const auto __s8 = __vector_bitcast<_SChar>(__k._M_data);\n+\t    int8x8_t __tmp = __lo64(__s8) + __hi64z(__s8);\n+\t    return -vpadd_s8(vpadd_s8(vpadd_s8(__tmp, int8x8_t()), int8x8_t()),\n+\t\t\t     int8x8_t())[0];\n+\t  }\n+\telse if constexpr (sizeof(_Tp) == 2)\n+\t  {\n+\t    const auto __s16 = __vector_bitcast<short>(__k._M_data);\n+\t    int16x4_t __tmp = __lo64(__s16) + __hi64z(__s16);\n+\t    return -vpadd_s16(vpadd_s16(__tmp, int16x4_t()), int16x4_t())[0];\n+\t  }\n+\telse if constexpr (sizeof(_Tp) == 4)\n+\t  {\n+\t    const auto __s32 = __vector_bitcast<int>(__k._M_data);\n+\t    int32x2_t __tmp = __lo64(__s32) + __hi64z(__s32);\n+\t    return -vpadd_s32(__tmp, int32x2_t())[0];\n+\t  }\n+\telse if constexpr (sizeof(_Tp) == 8)\n+\t  {\n+\t    static_assert(sizeof(__k) == 16);\n+\t    const auto __s64 = __vector_bitcast<long>(__k._M_data);\n+\t    return -(__s64[0] + __s64[1]);\n+\t  }\n+      }\n+\n+    // }}}\n+    // _S_find_first_set {{{\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static int\n+      _S_find_first_set(simd_mask<_Tp, _Abi> __k)\n+      {\n+\t// TODO: the _Base implementation is not optimal for NEON\n+\treturn _Base::_S_find_first_set(__k);\n+      }\n+\n+    // }}}\n+    // _S_find_last_set {{{\n+    template <typename _Tp>\n+      _GLIBCXX_SIMD_INTRINSIC static int\n+      _S_find_last_set(simd_mask<_Tp, _Abi> __k)\n+      {\n+\t// TODO: the _Base implementation is not optimal for NEON\n+\treturn _Base::_S_find_last_set(__k);\n+      }\n+\n+    // }}}\n+  }; // }}}\n+\n+_GLIBCXX_SIMD_END_NAMESPACE\n+#endif // __cplusplus >= 201703L\n+#endif // _GLIBCXX_EXPERIMENTAL_SIMD_NEON_H_\n+// vim: foldmethod=marker sw=2 noet ts=8 sts=2 tw=80"}, {"sha": "c00d2323ac66000cb20cde2f97be9727fe6781f2", "filename": "libstdc++-v3/include/experimental/bits/simd_ppc.h", "status": "added", "additions": 123, "deletions": 0, "changes": 123, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_ppc.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_ppc.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_ppc.h?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -0,0 +1,123 @@\n+// Simd PowerPC specific implementations -*- C++ -*-\n+\n+// Copyright (C) 2020 Free Software Foundation, Inc.\n+//\n+// This file is part of the GNU ISO C++ Library.  This library is free\n+// software; you can redistribute it and/or modify it under the\n+// terms of the GNU General Public License as published by the\n+// Free Software Foundation; either version 3, or (at your option)\n+// any later version.\n+\n+// This library is distributed in the hope that it will be useful,\n+// but WITHOUT ANY WARRANTY; without even the implied warranty of\n+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+// GNU General Public License for more details.\n+\n+// Under Section 7 of GPL version 3, you are granted additional\n+// permissions described in the GCC Runtime Library Exception, version\n+// 3.1, as published by the Free Software Foundation.\n+\n+// You should have received a copy of the GNU General Public License and\n+// a copy of the GCC Runtime Library Exception along with this program;\n+// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+// <http://www.gnu.org/licenses/>.\n+\n+#ifndef _GLIBCXX_EXPERIMENTAL_SIMD_PPC_H_\n+#define _GLIBCXX_EXPERIMENTAL_SIMD_PPC_H_\n+\n+#if __cplusplus >= 201703L\n+\n+#ifndef __ALTIVEC__\n+#error \"simd_ppc.h may only be included when AltiVec/VMX is available\"\n+#endif\n+\n+_GLIBCXX_SIMD_BEGIN_NAMESPACE\n+\n+// _SimdImplPpc {{{\n+template <typename _Abi>\n+  struct _SimdImplPpc : _SimdImplBuiltin<_Abi>\n+  {\n+    using _Base = _SimdImplBuiltin<_Abi>;\n+\n+    // Byte and halfword shift instructions on PPC only consider the low 3 or 4\n+    // bits of the RHS. Consequently, shifting by sizeof(_Tp)*CHAR_BIT (or more)\n+    // is UB without extra measures. To match scalar behavior, byte and halfword\n+    // shifts need an extra fixup step.\n+\n+    // _S_bit_shift_left {{{\n+    template <typename _Tp, size_t _Np>\n+      _GLIBCXX_SIMD_INTRINSIC static constexpr _SimdWrapper<_Tp, _Np>\n+      _S_bit_shift_left(_SimdWrapper<_Tp, _Np> __x, _SimdWrapper<_Tp, _Np> __y)\n+      {\n+\t__x = _Base::_S_bit_shift_left(__x, __y);\n+\tif constexpr (sizeof(_Tp) < sizeof(int))\n+\t  __x._M_data\n+\t    = (__y._M_data < sizeof(_Tp) * __CHAR_BIT__) & __x._M_data;\n+\treturn __x;\n+      }\n+\n+    template <typename _Tp, size_t _Np>\n+      _GLIBCXX_SIMD_INTRINSIC static constexpr _SimdWrapper<_Tp, _Np>\n+      _S_bit_shift_left(_SimdWrapper<_Tp, _Np> __x, int __y)\n+      {\n+\t__x = _Base::_S_bit_shift_left(__x, __y);\n+\tif constexpr (sizeof(_Tp) < sizeof(int))\n+\t  {\n+\t    if (__y >= sizeof(_Tp) * __CHAR_BIT__)\n+\t      return {};\n+\t  }\n+\treturn __x;\n+      }\n+\n+    // }}}\n+    // _S_bit_shift_right {{{\n+    template <typename _Tp, size_t _Np>\n+      _GLIBCXX_SIMD_INTRINSIC static constexpr _SimdWrapper<_Tp, _Np>\n+      _S_bit_shift_right(_SimdWrapper<_Tp, _Np> __x, _SimdWrapper<_Tp, _Np> __y)\n+      {\n+\tif constexpr (sizeof(_Tp) < sizeof(int))\n+\t  {\n+\t    constexpr int __nbits = sizeof(_Tp) * __CHAR_BIT__;\n+\t    if constexpr (is_unsigned_v<_Tp>)\n+\t      return (__y._M_data < __nbits)\n+\t\t     & _Base::_S_bit_shift_right(__x, __y)._M_data;\n+\t    else\n+\t      {\n+\t\t_Base::_S_masked_assign(_SimdWrapper<_Tp, _Np>(__y._M_data\n+\t\t\t\t\t\t\t       >= __nbits),\n+\t\t\t\t\t__y, __nbits - 1);\n+\t\treturn _Base::_S_bit_shift_right(__x, __y);\n+\t      }\n+\t  }\n+\telse\n+\t  return _Base::_S_bit_shift_right(__x, __y);\n+      }\n+\n+    template <typename _Tp, size_t _Np>\n+      _GLIBCXX_SIMD_INTRINSIC static constexpr _SimdWrapper<_Tp, _Np>\n+      _S_bit_shift_right(_SimdWrapper<_Tp, _Np> __x, int __y)\n+      {\n+\tif constexpr (sizeof(_Tp) < sizeof(int))\n+\t  {\n+\t    constexpr int __nbits = sizeof(_Tp) * __CHAR_BIT__;\n+\t    if (__y >= __nbits)\n+\t      {\n+\t\tif constexpr (is_unsigned_v<_Tp>)\n+\t\t  return {};\n+\t\telse\n+\t\t  return _Base::_S_bit_shift_right(__x, __nbits - 1);\n+\t      }\n+\t  }\n+\treturn _Base::_S_bit_shift_right(__x, __y);\n+      }\n+\n+    // }}}\n+  };\n+\n+// }}}\n+\n+_GLIBCXX_SIMD_END_NAMESPACE\n+#endif // __cplusplus >= 201703L\n+#endif // _GLIBCXX_EXPERIMENTAL_SIMD_PPC_H_\n+\n+// vim: foldmethod=marker sw=2 noet ts=8 sts=2 tw=80"}, {"sha": "7680bc39c3099a254884f9d716b10aa57beced32", "filename": "libstdc++-v3/include/experimental/bits/simd_scalar.h", "status": "added", "additions": 772, "deletions": 0, "changes": 772, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_scalar.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_scalar.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_scalar.h?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -0,0 +1,772 @@\n+// Simd scalar ABI specific implementations -*- C++ -*-\n+\n+// Copyright (C) 2020 Free Software Foundation, Inc.\n+//\n+// This file is part of the GNU ISO C++ Library.  This library is free\n+// software; you can redistribute it and/or modify it under the\n+// terms of the GNU General Public License as published by the\n+// Free Software Foundation; either version 3, or (at your option)\n+// any later version.\n+\n+// This library is distributed in the hope that it will be useful,\n+// but WITHOUT ANY WARRANTY; without even the implied warranty of\n+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+// GNU General Public License for more details.\n+\n+// Under Section 7 of GPL version 3, you are granted additional\n+// permissions described in the GCC Runtime Library Exception, version\n+// 3.1, as published by the Free Software Foundation.\n+\n+// You should have received a copy of the GNU General Public License and\n+// a copy of the GCC Runtime Library Exception along with this program;\n+// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+// <http://www.gnu.org/licenses/>.\n+\n+#ifndef _GLIBCXX_EXPERIMENTAL_SIMD_SCALAR_H_\n+#define _GLIBCXX_EXPERIMENTAL_SIMD_SCALAR_H_\n+#if __cplusplus >= 201703L\n+\n+#include <cmath>\n+\n+_GLIBCXX_SIMD_BEGIN_NAMESPACE\n+\n+// __promote_preserving_unsigned{{{\n+// work around crazy semantics of unsigned integers of lower rank than int:\n+// Before applying an operator the operands are promoted to int. In which case\n+// over- or underflow is UB, even though the operand types were unsigned.\n+template <typename _Tp>\n+  _GLIBCXX_SIMD_INTRINSIC constexpr decltype(auto)\n+  __promote_preserving_unsigned(const _Tp& __x)\n+  {\n+    if constexpr (is_signed_v<decltype(+__x)> && is_unsigned_v<_Tp>)\n+      return static_cast<unsigned int>(__x);\n+    else\n+      return __x;\n+  }\n+\n+// }}}\n+\n+struct _CommonImplScalar;\n+struct _CommonImplBuiltin;\n+struct _SimdImplScalar;\n+struct _MaskImplScalar;\n+\n+// simd_abi::_Scalar {{{\n+struct simd_abi::_Scalar\n+{\n+  template <typename _Tp>\n+    static constexpr size_t _S_size = 1;\n+\n+  template <typename _Tp>\n+    static constexpr size_t _S_full_size = 1;\n+\n+  template <typename _Tp>\n+    static constexpr bool _S_is_partial = false;\n+\n+  struct _IsValidAbiTag : true_type {};\n+\n+  template <typename _Tp>\n+    struct _IsValidSizeFor : true_type {};\n+\n+  template <typename _Tp>\n+    struct _IsValid : __is_vectorizable<_Tp> {};\n+\n+  template <typename _Tp>\n+    static constexpr bool _S_is_valid_v = _IsValid<_Tp>::value;\n+\n+  _GLIBCXX_SIMD_INTRINSIC static constexpr bool _S_masked(bool __x)\n+  { return __x; }\n+\n+  using _CommonImpl = _CommonImplScalar;\n+  using _SimdImpl = _SimdImplScalar;\n+  using _MaskImpl = _MaskImplScalar;\n+\n+  template <typename _Tp, bool = _S_is_valid_v<_Tp>>\n+    struct __traits : _InvalidTraits {};\n+\n+  template <typename _Tp>\n+    struct __traits<_Tp, true>\n+    {\n+      using _IsValid = true_type;\n+      using _SimdImpl = _SimdImplScalar;\n+      using _MaskImpl = _MaskImplScalar;\n+      using _SimdMember = _Tp;\n+      using _MaskMember = bool;\n+\n+      static constexpr size_t _S_simd_align = alignof(_SimdMember);\n+      static constexpr size_t _S_mask_align = alignof(_MaskMember);\n+\n+      // nothing the user can spell converts to/from simd/simd_mask\n+      struct _SimdCastType { _SimdCastType() = delete; };\n+      struct _MaskCastType { _MaskCastType() = delete; };\n+      struct _SimdBase {};\n+      struct _MaskBase {};\n+    };\n+};\n+\n+// }}}\n+// _CommonImplScalar {{{\n+struct _CommonImplScalar\n+{\n+  // _S_store {{{\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static void _S_store(_Tp __x, void* __addr)\n+    { __builtin_memcpy(__addr, &__x, sizeof(_Tp)); }\n+\n+  // }}}\n+  // _S_store_bool_array(_BitMask) {{{\n+  template <size_t _Np, bool _Sanitized>\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr void\n+    _S_store_bool_array(_BitMask<_Np, _Sanitized> __x, bool* __mem)\n+    {\n+      __make_dependent_t<decltype(__x), _CommonImplBuiltin>::_S_store_bool_array(\n+\t__x, __mem);\n+    }\n+\n+  // }}}\n+};\n+\n+// }}}\n+// _SimdImplScalar {{{\n+struct _SimdImplScalar\n+{\n+  // member types {{{2\n+  using abi_type = simd_abi::scalar;\n+\n+  template <typename _Tp>\n+    using _TypeTag = _Tp*;\n+\n+  // _S_broadcast {{{2\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr _Tp _S_broadcast(_Tp __x) noexcept\n+    { return __x; }\n+\n+  // _S_generator {{{2\n+  template <typename _Fp, typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr _Tp _S_generator(_Fp&& __gen,\n+\t\t\t\t\t\t\t      _TypeTag<_Tp>)\n+    { return __gen(_SizeConstant<0>()); }\n+\n+  // _S_load {{{2\n+  template <typename _Tp, typename _Up>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_load(const _Up* __mem,\n+\t\t\t\t\t       _TypeTag<_Tp>) noexcept\n+    { return static_cast<_Tp>(__mem[0]); }\n+\n+  // _S_masked_load {{{2\n+  template <typename _Tp, typename _Up>\n+    static inline _Tp _S_masked_load(_Tp __merge, bool __k,\n+\t\t\t\t     const _Up* __mem) noexcept\n+    {\n+      if (__k)\n+\t__merge = static_cast<_Tp>(__mem[0]);\n+      return __merge;\n+    }\n+\n+  // _S_store {{{2\n+  template <typename _Tp, typename _Up>\n+    static inline void _S_store(_Tp __v, _Up* __mem, _TypeTag<_Tp>) noexcept\n+    { __mem[0] = static_cast<_Up>(__v); }\n+\n+  // _S_masked_store {{{2\n+  template <typename _Tp, typename _Up>\n+    static inline void _S_masked_store(const _Tp __v, _Up* __mem,\n+\t\t\t\t       const bool __k) noexcept\n+    { if (__k) __mem[0] = __v; }\n+\n+  // _S_negate {{{2\n+  template <typename _Tp>\n+    static constexpr inline bool _S_negate(_Tp __x) noexcept\n+    { return !__x; }\n+\n+  // _S_reduce {{{2\n+  template <typename _Tp, typename _BinaryOperation>\n+    static constexpr inline _Tp\n+    _S_reduce(const simd<_Tp, simd_abi::scalar>& __x, _BinaryOperation&)\n+    { return __x._M_data; }\n+\n+  // _S_min, _S_max {{{2\n+  template <typename _Tp>\n+    static constexpr inline _Tp _S_min(const _Tp __a, const _Tp __b)\n+    { return std::min(__a, __b); }\n+\n+  template <typename _Tp>\n+    static constexpr inline _Tp _S_max(const _Tp __a, const _Tp __b)\n+    { return std::max(__a, __b); }\n+\n+  // _S_complement {{{2\n+  template <typename _Tp>\n+    static constexpr inline _Tp _S_complement(_Tp __x) noexcept\n+    { return static_cast<_Tp>(~__x); }\n+\n+  // _S_unary_minus {{{2\n+  template <typename _Tp>\n+    static constexpr inline _Tp _S_unary_minus(_Tp __x) noexcept\n+    { return static_cast<_Tp>(-__x); }\n+\n+  // arithmetic operators {{{2\n+  template <typename _Tp>\n+    static constexpr inline _Tp _S_plus(_Tp __x, _Tp __y)\n+    {\n+      return static_cast<_Tp>(__promote_preserving_unsigned(__x)\n+\t\t\t      + __promote_preserving_unsigned(__y));\n+    }\n+\n+  template <typename _Tp>\n+    static constexpr inline _Tp _S_minus(_Tp __x, _Tp __y)\n+    {\n+      return static_cast<_Tp>(__promote_preserving_unsigned(__x)\n+\t\t\t      - __promote_preserving_unsigned(__y));\n+    }\n+\n+  template <typename _Tp>\n+    static constexpr inline _Tp _S_multiplies(_Tp __x, _Tp __y)\n+    {\n+      return static_cast<_Tp>(__promote_preserving_unsigned(__x)\n+\t\t\t      * __promote_preserving_unsigned(__y));\n+    }\n+\n+  template <typename _Tp>\n+    static constexpr inline _Tp _S_divides(_Tp __x, _Tp __y)\n+    {\n+      return static_cast<_Tp>(__promote_preserving_unsigned(__x)\n+\t\t\t      / __promote_preserving_unsigned(__y));\n+    }\n+\n+  template <typename _Tp>\n+    static constexpr inline _Tp _S_modulus(_Tp __x, _Tp __y)\n+    {\n+      return static_cast<_Tp>(__promote_preserving_unsigned(__x)\n+\t\t\t      % __promote_preserving_unsigned(__y));\n+    }\n+\n+  template <typename _Tp>\n+    static constexpr inline _Tp _S_bit_and(_Tp __x, _Tp __y)\n+    {\n+      if constexpr (is_floating_point_v<_Tp>)\n+\t{\n+\t  using _Ip = __int_for_sizeof_t<_Tp>;\n+\t  return __bit_cast<_Tp>(__bit_cast<_Ip>(__x) & __bit_cast<_Ip>(__y));\n+\t}\n+      else\n+\treturn static_cast<_Tp>(__promote_preserving_unsigned(__x)\n+\t\t\t\t& __promote_preserving_unsigned(__y));\n+    }\n+\n+  template <typename _Tp>\n+    static constexpr inline _Tp _S_bit_or(_Tp __x, _Tp __y)\n+    {\n+      if constexpr (is_floating_point_v<_Tp>)\n+\t{\n+\t  using _Ip = __int_for_sizeof_t<_Tp>;\n+\t  return __bit_cast<_Tp>(__bit_cast<_Ip>(__x) | __bit_cast<_Ip>(__y));\n+\t}\n+      else\n+\treturn static_cast<_Tp>(__promote_preserving_unsigned(__x)\n+\t\t\t\t| __promote_preserving_unsigned(__y));\n+    }\n+\n+  template <typename _Tp>\n+    static constexpr inline _Tp _S_bit_xor(_Tp __x, _Tp __y)\n+    {\n+      if constexpr (is_floating_point_v<_Tp>)\n+\t{\n+\t  using _Ip = __int_for_sizeof_t<_Tp>;\n+\t  return __bit_cast<_Tp>(__bit_cast<_Ip>(__x) ^ __bit_cast<_Ip>(__y));\n+\t}\n+      else\n+\treturn static_cast<_Tp>(__promote_preserving_unsigned(__x)\n+\t\t\t\t^ __promote_preserving_unsigned(__y));\n+    }\n+\n+  template <typename _Tp>\n+    static constexpr inline _Tp _S_bit_shift_left(_Tp __x, int __y)\n+    { return static_cast<_Tp>(__promote_preserving_unsigned(__x) << __y); }\n+\n+  template <typename _Tp>\n+    static constexpr inline _Tp _S_bit_shift_right(_Tp __x, int __y)\n+    { return static_cast<_Tp>(__promote_preserving_unsigned(__x) >> __y); }\n+\n+  // math {{{2\n+  // frexp, modf and copysign implemented in simd_math.h\n+  template <typename _Tp>\n+    using _ST = _SimdTuple<_Tp, simd_abi::scalar>;\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_acos(_Tp __x)\n+    { return std::acos(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_asin(_Tp __x)\n+    { return std::asin(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_atan(_Tp __x)\n+    { return std::atan(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_cos(_Tp __x)\n+    { return std::cos(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_sin(_Tp __x)\n+    { return std::sin(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_tan(_Tp __x)\n+    { return std::tan(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_acosh(_Tp __x)\n+    { return std::acosh(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_asinh(_Tp __x)\n+    { return std::asinh(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_atanh(_Tp __x)\n+    { return std::atanh(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_cosh(_Tp __x)\n+    { return std::cosh(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_sinh(_Tp __x)\n+    { return std::sinh(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_tanh(_Tp __x)\n+    { return std::tanh(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_atan2(_Tp __x, _Tp __y)\n+    { return std::atan2(__x, __y); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_exp(_Tp __x)\n+    { return std::exp(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_exp2(_Tp __x)\n+    { return std::exp2(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_expm1(_Tp __x)\n+    { return std::expm1(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_log(_Tp __x)\n+    { return std::log(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_log10(_Tp __x)\n+    { return std::log10(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_log1p(_Tp __x)\n+    { return std::log1p(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_log2(_Tp __x)\n+    { return std::log2(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_logb(_Tp __x)\n+    { return std::logb(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _ST<int> _S_ilogb(_Tp __x)\n+    { return {std::ilogb(__x)}; }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_pow(_Tp __x, _Tp __y)\n+    { return std::pow(__x, __y); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_abs(_Tp __x)\n+    { return std::abs(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_fabs(_Tp __x)\n+    { return std::fabs(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_sqrt(_Tp __x)\n+    { return std::sqrt(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_cbrt(_Tp __x)\n+    { return std::cbrt(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_erf(_Tp __x)\n+    { return std::erf(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_erfc(_Tp __x)\n+    { return std::erfc(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_lgamma(_Tp __x)\n+    { return std::lgamma(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_tgamma(_Tp __x)\n+    { return std::tgamma(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_trunc(_Tp __x)\n+    { return std::trunc(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_floor(_Tp __x)\n+    { return std::floor(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_ceil(_Tp __x)\n+    { return std::ceil(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_nearbyint(_Tp __x)\n+    { return std::nearbyint(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_rint(_Tp __x)\n+    { return std::rint(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _ST<long> _S_lrint(_Tp __x)\n+    { return {std::lrint(__x)}; }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _ST<long long> _S_llrint(_Tp __x)\n+    { return {std::llrint(__x)}; }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_round(_Tp __x)\n+    { return std::round(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _ST<long> _S_lround(_Tp __x)\n+    { return {std::lround(__x)}; }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _ST<long long> _S_llround(_Tp __x)\n+    { return {std::llround(__x)}; }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_ldexp(_Tp __x, _ST<int> __y)\n+    { return std::ldexp(__x, __y.first); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_scalbn(_Tp __x, _ST<int> __y)\n+    { return std::scalbn(__x, __y.first); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_scalbln(_Tp __x, _ST<long> __y)\n+    { return std::scalbln(__x, __y.first); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_fmod(_Tp __x, _Tp __y)\n+    { return std::fmod(__x, __y); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_remainder(_Tp __x, _Tp __y)\n+    { return std::remainder(__x, __y); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_nextafter(_Tp __x, _Tp __y)\n+    { return std::nextafter(__x, __y); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_fdim(_Tp __x, _Tp __y)\n+    { return std::fdim(__x, __y); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_fmax(_Tp __x, _Tp __y)\n+    { return std::fmax(__x, __y); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_fmin(_Tp __x, _Tp __y)\n+    { return std::fmin(__x, __y); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_fma(_Tp __x, _Tp __y, _Tp __z)\n+    { return std::fma(__x, __y, __z); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC static _Tp _S_remquo(_Tp __x, _Tp __y, _ST<int>* __z)\n+    { return std::remquo(__x, __y, &__z->first); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static _ST<int> _S_fpclassify(_Tp __x)\n+    { return {std::fpclassify(__x)}; }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_isfinite(_Tp __x)\n+    { return std::isfinite(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_isinf(_Tp __x)\n+    { return std::isinf(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_isnan(_Tp __x)\n+    { return std::isnan(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_isnormal(_Tp __x)\n+    { return std::isnormal(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_signbit(_Tp __x)\n+    { return std::signbit(__x); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_isgreater(_Tp __x, _Tp __y)\n+    { return std::isgreater(__x, __y); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_isgreaterequal(_Tp __x,\n+\t\t\t\t\t\t\t\t    _Tp __y)\n+    { return std::isgreaterequal(__x, __y); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_isless(_Tp __x, _Tp __y)\n+    { return std::isless(__x, __y); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_islessequal(_Tp __x, _Tp __y)\n+    { return std::islessequal(__x, __y); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_islessgreater(_Tp __x,\n+\t\t\t\t\t\t\t\t   _Tp __y)\n+    { return std::islessgreater(__x, __y); }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_isunordered(_Tp __x,\n+\t\t\t\t\t\t\t\t _Tp __y)\n+    { return std::isunordered(__x, __y); }\n+\n+  // _S_increment & _S_decrement{{{2\n+  template <typename _Tp>\n+    constexpr static inline void _S_increment(_Tp& __x)\n+    { ++__x; }\n+\n+  template <typename _Tp>\n+    constexpr static inline void _S_decrement(_Tp& __x)\n+    { --__x; }\n+\n+\n+  // compares {{{2\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_equal_to(_Tp __x, _Tp __y)\n+    { return __x == __y; }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_not_equal_to(_Tp __x,\n+\t\t\t\t\t\t\t\t  _Tp __y)\n+    { return __x != __y; }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_less(_Tp __x, _Tp __y)\n+    { return __x < __y; }\n+\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool _S_less_equal(_Tp __x,\n+\t\t\t\t\t\t\t\t_Tp __y)\n+    { return __x <= __y; }\n+\n+  // smart_reference access {{{2\n+  template <typename _Tp, typename _Up>\n+    constexpr static void _S_set(_Tp& __v, [[maybe_unused]] int __i,\n+\t\t\t\t _Up&& __x) noexcept\n+    {\n+      _GLIBCXX_DEBUG_ASSERT(__i == 0);\n+      __v = static_cast<_Up&&>(__x);\n+    }\n+\n+  // _S_masked_assign {{{2\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static void\n+    _S_masked_assign(bool __k, _Tp& __lhs, _Tp __rhs)\n+    { if (__k) __lhs = __rhs; }\n+\n+  // _S_masked_cassign {{{2\n+  template <typename _Op, typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static void\n+    _S_masked_cassign(const bool __k, _Tp& __lhs, const _Tp __rhs, _Op __op)\n+    { if (__k) __lhs = __op(_SimdImplScalar{}, __lhs, __rhs); }\n+\n+  // _S_masked_unary {{{2\n+  template <template <typename> class _Op, typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static _Tp _S_masked_unary(const bool __k,\n+\t\t\t\t\t\t\t\t const _Tp __v)\n+    { return static_cast<_Tp>(__k ? _Op<_Tp>{}(__v) : __v); }\n+\n+  // }}}2\n+};\n+\n+// }}}\n+// _MaskImplScalar {{{\n+struct _MaskImplScalar\n+{\n+  // member types {{{\n+  template <typename _Tp>\n+    using _TypeTag = _Tp*;\n+\n+  // }}}\n+  // _S_broadcast {{{\n+  template <typename>\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr bool _S_broadcast(bool __x)\n+    { return __x; }\n+\n+  // }}}\n+  // _S_load {{{\n+  template <typename>\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr bool _S_load(const bool* __mem)\n+    { return __mem[0]; }\n+\n+  // }}}\n+  // _S_to_bits {{{\n+  _GLIBCXX_SIMD_INTRINSIC static constexpr _SanitizedBitMask<1>\n+  _S_to_bits(bool __x)\n+  { return __x; }\n+\n+  // }}}\n+  // _S_convert {{{\n+  template <typename, bool _Sanitized>\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr bool\n+    _S_convert(_BitMask<1, _Sanitized> __x)\n+    { return __x[0]; }\n+\n+  template <typename, typename _Up, typename _UAbi>\n+    _GLIBCXX_SIMD_INTRINSIC static constexpr bool\n+    _S_convert(simd_mask<_Up, _UAbi> __x)\n+    { return __x[0]; }\n+\n+  // }}}\n+  // _S_from_bitmask {{{2\n+  template <typename _Tp>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool\n+    _S_from_bitmask(_SanitizedBitMask<1> __bits, _TypeTag<_Tp>) noexcept\n+    { return __bits[0]; }\n+\n+  // _S_masked_load {{{2\n+  _GLIBCXX_SIMD_INTRINSIC constexpr static bool\n+  _S_masked_load(bool __merge, bool __mask, const bool* __mem) noexcept\n+  {\n+    if (__mask)\n+      __merge = __mem[0];\n+    return __merge;\n+  }\n+\n+  // _S_store {{{2\n+  _GLIBCXX_SIMD_INTRINSIC static void _S_store(bool __v, bool* __mem) noexcept\n+  { __mem[0] = __v; }\n+\n+  // _S_masked_store {{{2\n+  _GLIBCXX_SIMD_INTRINSIC static void\n+  _S_masked_store(const bool __v, bool* __mem, const bool __k) noexcept\n+  {\n+    if (__k)\n+      __mem[0] = __v;\n+  }\n+\n+  // logical and bitwise operators {{{2\n+  static constexpr bool _S_logical_and(bool __x, bool __y)\n+  { return __x && __y; }\n+\n+  static constexpr bool _S_logical_or(bool __x, bool __y)\n+  { return __x || __y; }\n+\n+  static constexpr bool _S_bit_not(bool __x)\n+  { return !__x; }\n+\n+  static constexpr bool _S_bit_and(bool __x, bool __y)\n+  { return __x && __y; }\n+\n+  static constexpr bool _S_bit_or(bool __x, bool __y)\n+  { return __x || __y; }\n+\n+  static constexpr bool _S_bit_xor(bool __x, bool __y)\n+  { return __x != __y; }\n+\n+  // smart_reference access {{{2\n+  constexpr static void _S_set(bool& __k, [[maybe_unused]] int __i,\n+\t\t\t       bool __x) noexcept\n+  {\n+    _GLIBCXX_DEBUG_ASSERT(__i == 0);\n+    __k = __x;\n+  }\n+\n+  // _S_masked_assign {{{2\n+  _GLIBCXX_SIMD_INTRINSIC static void _S_masked_assign(bool __k, bool& __lhs,\n+\t\t\t\t\t\t       bool __rhs)\n+  {\n+    if (__k)\n+      __lhs = __rhs;\n+  }\n+\n+  // }}}2\n+  // _S_all_of {{{\n+  template <typename _Tp, typename _Abi>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool\n+    _S_all_of(simd_mask<_Tp, _Abi> __k)\n+    { return __k._M_data; }\n+\n+  // }}}\n+  // _S_any_of {{{\n+  template <typename _Tp, typename _Abi>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool\n+    _S_any_of(simd_mask<_Tp, _Abi> __k)\n+    { return __k._M_data; }\n+\n+  // }}}\n+  // _S_none_of {{{\n+  template <typename _Tp, typename _Abi>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool\n+    _S_none_of(simd_mask<_Tp, _Abi> __k)\n+    { return !__k._M_data; }\n+\n+  // }}}\n+  // _S_some_of {{{\n+  template <typename _Tp, typename _Abi>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static bool\n+    _S_some_of(simd_mask<_Tp, _Abi>)\n+    { return false; }\n+\n+  // }}}\n+  // _S_popcount {{{\n+  template <typename _Tp, typename _Abi>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static int\n+    _S_popcount(simd_mask<_Tp, _Abi> __k)\n+    { return __k._M_data; }\n+\n+  // }}}\n+  // _S_find_first_set {{{\n+  template <typename _Tp, typename _Abi>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static int\n+    _S_find_first_set(simd_mask<_Tp, _Abi>)\n+    { return 0; }\n+\n+  // }}}\n+  // _S_find_last_set {{{\n+  template <typename _Tp, typename _Abi>\n+    _GLIBCXX_SIMD_INTRINSIC constexpr static int\n+    _S_find_last_set(simd_mask<_Tp, _Abi>)\n+    { return 0; }\n+\n+  // }}}\n+};\n+\n+// }}}\n+\n+_GLIBCXX_SIMD_END_NAMESPACE\n+#endif // __cplusplus >= 201703L\n+#endif // _GLIBCXX_EXPERIMENTAL_SIMD_SCALAR_H_\n+\n+// vim: foldmethod=marker sw=2 noet ts=8 sts=2 tw=80"}, {"sha": "d1d7b9d4bf39a2b68e9f95f7fc351c5fd41fe9d2", "filename": "libstdc++-v3/include/experimental/bits/simd_x86.h", "status": "added", "additions": 5169, "deletions": 0, "changes": 5169, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_x86.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_x86.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_x86.h?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396"}, {"sha": "09f898b7ec9bc22bf7f2eb47e9fc73d118d58d49", "filename": "libstdc++-v3/include/experimental/bits/simd_x86_conversions.h", "status": "added", "additions": 2029, "deletions": 0, "changes": 2029, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_x86_conversions.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_x86_conversions.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_x86_conversions.h?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -0,0 +1,2029 @@\n+// x86 specific conversion optimizations -*- C++ -*-\n+\n+// Copyright (C) 2020 Free Software Foundation, Inc.\n+//\n+// This file is part of the GNU ISO C++ Library.  This library is free\n+// software; you can redistribute it and/or modify it under the\n+// terms of the GNU General Public License as published by the\n+// Free Software Foundation; either version 3, or (at your option)\n+// any later version.\n+\n+// This library is distributed in the hope that it will be useful,\n+// but WITHOUT ANY WARRANTY; without even the implied warranty of\n+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+// GNU General Public License for more details.\n+\n+// Under Section 7 of GPL version 3, you are granted additional\n+// permissions described in the GCC Runtime Library Exception, version\n+// 3.1, as published by the Free Software Foundation.\n+\n+// You should have received a copy of the GNU General Public License and\n+// a copy of the GCC Runtime Library Exception along with this program;\n+// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+// <http://www.gnu.org/licenses/>.\n+\n+#ifndef _GLIBCXX_EXPERIMENTAL_SIMD_X86_CONVERSIONS_H\n+#define _GLIBCXX_EXPERIMENTAL_SIMD_X86_CONVERSIONS_H\n+\n+#if __cplusplus >= 201703L\n+\n+// work around PR85827\n+// 1-arg __convert_x86 {{{1\n+template <typename _To, typename _V, typename _Traits>\n+  _GLIBCXX_SIMD_INTRINSIC _To\n+  __convert_x86(_V __v)\n+  {\n+    static_assert(__is_vector_type_v<_V>);\n+    using _Tp = typename _Traits::value_type;\n+    constexpr size_t _Np = _Traits::_S_full_size;\n+    [[maybe_unused]] const auto __intrin = __to_intrin(__v);\n+    using _Up = typename _VectorTraits<_To>::value_type;\n+    constexpr size_t _M = _VectorTraits<_To>::_S_full_size;\n+\n+    // [xyz]_to_[xyz] {{{2\n+    [[maybe_unused]] constexpr bool __x_to_x\n+      = sizeof(__v) <= 16 && sizeof(_To) <= 16;\n+    [[maybe_unused]] constexpr bool __x_to_y\n+      = sizeof(__v) <= 16 && sizeof(_To) == 32;\n+    [[maybe_unused]] constexpr bool __x_to_z\n+      = sizeof(__v) <= 16 && sizeof(_To) == 64;\n+    [[maybe_unused]] constexpr bool __y_to_x\n+      = sizeof(__v) == 32 && sizeof(_To) <= 16;\n+    [[maybe_unused]] constexpr bool __y_to_y\n+      = sizeof(__v) == 32 && sizeof(_To) == 32;\n+    [[maybe_unused]] constexpr bool __y_to_z\n+      = sizeof(__v) == 32 && sizeof(_To) == 64;\n+    [[maybe_unused]] constexpr bool __z_to_x\n+      = sizeof(__v) == 64 && sizeof(_To) <= 16;\n+    [[maybe_unused]] constexpr bool __z_to_y\n+      = sizeof(__v) == 64 && sizeof(_To) == 32;\n+    [[maybe_unused]] constexpr bool __z_to_z\n+      = sizeof(__v) == 64 && sizeof(_To) == 64;\n+\n+    // iX_to_iX {{{2\n+    [[maybe_unused]] constexpr bool __i_to_i\n+      = is_integral_v<_Up> && is_integral_v<_Tp>;\n+    [[maybe_unused]] constexpr bool __i8_to_i16\n+      = __i_to_i && sizeof(_Tp) == 1 && sizeof(_Up) == 2;\n+    [[maybe_unused]] constexpr bool __i8_to_i32\n+      = __i_to_i && sizeof(_Tp) == 1 && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __i8_to_i64\n+      = __i_to_i && sizeof(_Tp) == 1 && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __i16_to_i8\n+      = __i_to_i && sizeof(_Tp) == 2 && sizeof(_Up) == 1;\n+    [[maybe_unused]] constexpr bool __i16_to_i32\n+      = __i_to_i && sizeof(_Tp) == 2 && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __i16_to_i64\n+      = __i_to_i && sizeof(_Tp) == 2 && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __i32_to_i8\n+      = __i_to_i && sizeof(_Tp) == 4 && sizeof(_Up) == 1;\n+    [[maybe_unused]] constexpr bool __i32_to_i16\n+      = __i_to_i && sizeof(_Tp) == 4 && sizeof(_Up) == 2;\n+    [[maybe_unused]] constexpr bool __i32_to_i64\n+      = __i_to_i && sizeof(_Tp) == 4 && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __i64_to_i8\n+      = __i_to_i && sizeof(_Tp) == 8 && sizeof(_Up) == 1;\n+    [[maybe_unused]] constexpr bool __i64_to_i16\n+      = __i_to_i && sizeof(_Tp) == 8 && sizeof(_Up) == 2;\n+    [[maybe_unused]] constexpr bool __i64_to_i32\n+      = __i_to_i && sizeof(_Tp) == 8 && sizeof(_Up) == 4;\n+\n+    // [fsu]X_to_[fsu]X {{{2\n+    // ibw = integral && byte or word, i.e. char and short with any signedness\n+    [[maybe_unused]] constexpr bool __s64_to_f32\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 8\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __s32_to_f32\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __s16_to_f32\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 2\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __s8_to_f32\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 1\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __u64_to_f32\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 8\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __u32_to_f32\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __u16_to_f32\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 2\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __u8_to_f32\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 1\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __s64_to_f64\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 8\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __s32_to_f64\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __u64_to_f64\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 8\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __u32_to_f64\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __f32_to_s64\n+      = is_integral_v<_Up> && is_signed_v<_Up> && sizeof(_Up) == 8\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f32_to_s32\n+      = is_integral_v<_Up> && is_signed_v<_Up> && sizeof(_Up) == 4\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f32_to_u64\n+      = is_integral_v<_Up> && is_unsigned_v<_Up> && sizeof(_Up) == 8\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f32_to_u32\n+      = is_integral_v<_Up> && is_unsigned_v<_Up> && sizeof(_Up) == 4\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f64_to_s64\n+      = is_integral_v<_Up> && is_signed_v<_Up> && sizeof(_Up) == 8\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __f64_to_s32\n+      = is_integral_v<_Up> && is_signed_v<_Up> && sizeof(_Up) == 4\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __f64_to_u64\n+      = is_integral_v<_Up> && is_unsigned_v<_Up> && sizeof(_Up) == 8\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __f64_to_u32\n+      = is_integral_v<_Up> && is_unsigned_v<_Up> && sizeof(_Up) == 4\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __ibw_to_f32\n+      = is_integral_v<_Tp> && sizeof(_Tp) <= 2\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __ibw_to_f64\n+      = is_integral_v<_Tp> && sizeof(_Tp) <= 2\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __f32_to_ibw\n+      = is_integral_v<_Up> && sizeof(_Up) <= 2\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f64_to_ibw\n+      = is_integral_v<_Up> && sizeof(_Up) <= 2\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __f32_to_f64\n+      = is_floating_point_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __f64_to_f32\n+      = is_floating_point_v<_Tp> && sizeof(_Tp) == 8\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+\n+    if constexpr (__i_to_i && __y_to_x && !__have_avx2) //{{{2\n+      return __convert_x86<_To>(__lo128(__v), __hi128(__v));\n+    else if constexpr (__i_to_i && __x_to_y && !__have_avx2) //{{{2\n+      return __concat(__convert_x86<__vector_type_t<_Up, _M / 2>>(__v),\n+\t\t      __convert_x86<__vector_type_t<_Up, _M / 2>>(\n+\t\t\t__extract_part<1, _Np / _M * 2>(__v)));\n+    else if constexpr (__i_to_i) //{{{2\n+      {\n+\tstatic_assert(__x_to_x || __have_avx2,\n+\t\t      \"integral conversions with ymm registers require AVX2\");\n+\tstatic_assert(__have_avx512bw\n+\t\t\t|| ((sizeof(_Tp) >= 4 || sizeof(__v) < 64)\n+\t\t\t    && (sizeof(_Up) >= 4 || sizeof(_To) < 64)),\n+\t\t      \"8/16-bit integers in zmm registers require AVX512BW\");\n+\tstatic_assert((sizeof(__v) < 64 && sizeof(_To) < 64) || __have_avx512f,\n+\t\t      \"integral conversions with ymm registers require AVX2\");\n+      }\n+    if constexpr (is_floating_point_v<_Tp> == is_floating_point_v<_Up> && //{{{2\n+\t\t  sizeof(_Tp) == sizeof(_Up))\n+      {\n+\t// conversion uses simple bit reinterpretation (or no conversion at all)\n+\tif constexpr (_Np >= _M)\n+\t  return __intrin_bitcast<_To>(__v);\n+\telse\n+\t  return __zero_extend(__vector_bitcast<_Up>(__v));\n+      }\n+    else if constexpr (_Np < _M && sizeof(_To) > 16) //{{{2\n+      // zero extend (eg. xmm -> ymm)\n+      return __zero_extend(\n+\t__convert_x86<__vector_type_t<\n+\t  _Up, (16 / sizeof(_Up) > _Np) ? 16 / sizeof(_Up) : _Np>>(__v));\n+    else if constexpr (_Np > _M && sizeof(__v) > 16) //{{{2\n+      // partial input (eg. ymm -> xmm)\n+      return __convert_x86<_To>(__extract_part<0, _Np / _M>(__v));\n+    else if constexpr (__i64_to_i32) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_avx512vl)\n+\t  return __intrin_bitcast<_To>(_mm_cvtepi64_epi32(__intrin));\n+\telse if constexpr (__x_to_x)\n+\t  return __auto_bitcast(\n+\t    _mm_shuffle_ps(__vector_bitcast<float>(__v), __m128(), 8));\n+\telse if constexpr (__y_to_x && __have_avx512vl)\n+\t  return __intrin_bitcast<_To>(_mm256_cvtepi64_epi32(__intrin));\n+\telse if constexpr (__y_to_x && __have_avx512f)\n+\t  return __intrin_bitcast<_To>(\n+\t    __lo128(_mm512_cvtepi64_epi32(__auto_bitcast(__v))));\n+\telse if constexpr (__y_to_x)\n+\t  return __intrin_bitcast<_To>(\n+\t    __lo128(_mm256_permute4x64_epi64(_mm256_shuffle_epi32(__intrin, 8),\n+\t\t\t\t\t     0 + 4 * 2)));\n+\telse if constexpr (__z_to_y)\n+\t  return __intrin_bitcast<_To>(_mm512_cvtepi64_epi32(__intrin));\n+      }\n+    else if constexpr (__i64_to_i16) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_avx512vl)\n+\t  return __intrin_bitcast<_To>(_mm_cvtepi64_epi16(__intrin));\n+\telse if constexpr (__x_to_x && __have_avx512f)\n+\t  return __intrin_bitcast<_To>(\n+\t    __lo128(_mm512_cvtepi64_epi16(__auto_bitcast(__v))));\n+\telse if constexpr (__x_to_x && __have_ssse3)\n+\t  {\n+\t    return __intrin_bitcast<_To>(\n+\t      _mm_shuffle_epi8(__intrin,\n+\t\t\t       _mm_setr_epi8(0, 1, 8, 9, -0x80, -0x80, -0x80,\n+\t\t\t\t\t     -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t\t\t     -0x80, -0x80, -0x80, -0x80)));\n+\t    // fallback without SSSE3\n+\t  }\n+\telse if constexpr (__y_to_x && __have_avx512vl)\n+\t  return __intrin_bitcast<_To>(_mm256_cvtepi64_epi16(__intrin));\n+\telse if constexpr (__y_to_x && __have_avx512f)\n+\t  return __intrin_bitcast<_To>(\n+\t    __lo128(_mm512_cvtepi64_epi16(__auto_bitcast(__v))));\n+\telse if constexpr (__y_to_x)\n+\t  {\n+\t    const auto __a = _mm256_shuffle_epi8(\n+\t      __intrin,\n+\t      _mm256_setr_epi8(0, 1, 8, 9, -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t       -0x80, -0x80, -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t       -0x80, -0x80, -0x80, -0x80, 0, 1, 8, 9, -0x80,\n+\t\t\t       -0x80, -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t       -0x80));\n+\t    return __intrin_bitcast<_To>(__lo128(__a) | __hi128(__a));\n+\t  }\n+\telse if constexpr (__z_to_x)\n+\t  return __intrin_bitcast<_To>(_mm512_cvtepi64_epi16(__intrin));\n+      }\n+    else if constexpr (__i64_to_i8) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_avx512vl)\n+\t  return __intrin_bitcast<_To>(_mm_cvtepi64_epi8(__intrin));\n+\telse if constexpr (__x_to_x && __have_avx512f)\n+\t  return __intrin_bitcast<_To>(\n+\t    __lo128(_mm512_cvtepi64_epi8(__zero_extend(__intrin))));\n+\telse if constexpr (__y_to_x && __have_avx512vl)\n+\t  return __intrin_bitcast<_To>(_mm256_cvtepi64_epi8(__intrin));\n+\telse if constexpr (__y_to_x && __have_avx512f)\n+\t  return __intrin_bitcast<_To>(\n+\t    _mm512_cvtepi64_epi8(__zero_extend(__intrin)));\n+\telse if constexpr (__z_to_x)\n+\t  return __intrin_bitcast<_To>(_mm512_cvtepi64_epi8(__intrin));\n+      }\n+    else if constexpr (__i32_to_i64) //{{{2\n+      {\n+\tif constexpr (__have_sse4_1 && __x_to_x)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm_cvtepi32_epi64(__intrin)\n+\t\t\t\t\t : _mm_cvtepu32_epi64(__intrin));\n+\telse if constexpr (__x_to_x)\n+\t  {\n+\t    return __intrin_bitcast<_To>(\n+\t      _mm_unpacklo_epi32(__intrin, is_signed_v<_Tp>\n+\t\t\t\t\t     ? _mm_srai_epi32(__intrin, 31)\n+\t\t\t\t\t     : __m128i()));\n+\t  }\n+\telse if constexpr (__x_to_y)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm256_cvtepi32_epi64(__intrin)\n+\t\t\t\t\t : _mm256_cvtepu32_epi64(__intrin));\n+\telse if constexpr (__y_to_z)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm512_cvtepi32_epi64(__intrin)\n+\t\t\t\t\t : _mm512_cvtepu32_epi64(__intrin));\n+      }\n+    else if constexpr (__i32_to_i16) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_avx512vl)\n+\t  return __intrin_bitcast<_To>(_mm_cvtepi32_epi16(__intrin));\n+\telse if constexpr (__x_to_x && __have_avx512f)\n+\t  return __intrin_bitcast<_To>(\n+\t    __lo128(_mm512_cvtepi32_epi16(__auto_bitcast(__v))));\n+\telse if constexpr (__x_to_x && __have_ssse3)\n+\t  return __intrin_bitcast<_To>(_mm_shuffle_epi8(\n+\t    __intrin, _mm_setr_epi8(0, 1, 4, 5, 8, 9, 12, 13, -0x80, -0x80,\n+\t\t\t\t    -0x80, -0x80, -0x80, -0x80, -0x80, -0x80)));\n+\telse if constexpr (__x_to_x)\n+\t  {\n+\t    auto __a = _mm_unpacklo_epi16(__intrin, __m128i()); // 0o.o 1o.o\n+\t    auto __b = _mm_unpackhi_epi16(__intrin, __m128i()); // 2o.o 3o.o\n+\t    auto __c = _mm_unpacklo_epi16(__a, __b);            // 02oo ..oo\n+\t    auto __d = _mm_unpackhi_epi16(__a, __b);            // 13oo ..oo\n+\t    return __intrin_bitcast<_To>(\n+\t      _mm_unpacklo_epi16(__c, __d)); // 0123 oooo\n+\t  }\n+\telse if constexpr (__y_to_x && __have_avx512vl)\n+\t  return __intrin_bitcast<_To>(_mm256_cvtepi32_epi16(__intrin));\n+\telse if constexpr (__y_to_x && __have_avx512f)\n+\t  return __intrin_bitcast<_To>(\n+\t    __lo128(_mm512_cvtepi32_epi16(__auto_bitcast(__v))));\n+\telse if constexpr (__y_to_x)\n+\t  {\n+\t    auto __a = _mm256_shuffle_epi8(\n+\t      __intrin,\n+\t      _mm256_setr_epi8(0, 1, 4, 5, 8, 9, 12, 13, -0x80, -0x80, -0x80,\n+\t\t\t       -0x80, -0x80, -0x80, -0x80, -0x80, 0, 1, 4, 5, 8,\n+\t\t\t       9, 12, 13, -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t       -0x80, -0x80, -0x80));\n+\t    return __intrin_bitcast<_To>(__lo128(\n+\t      _mm256_permute4x64_epi64(__a,\n+\t\t\t\t       0xf8))); // __a[0] __a[2] | __a[3] __a[3]\n+\t  }\n+\telse if constexpr (__z_to_y)\n+\t  return __intrin_bitcast<_To>(_mm512_cvtepi32_epi16(__intrin));\n+      }\n+    else if constexpr (__i32_to_i8) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_avx512vl)\n+\t  return __intrin_bitcast<_To>(_mm_cvtepi32_epi8(__intrin));\n+\telse if constexpr (__x_to_x && __have_avx512f)\n+\t  return __intrin_bitcast<_To>(\n+\t    __lo128(_mm512_cvtepi32_epi8(__zero_extend(__intrin))));\n+\telse if constexpr (__x_to_x && __have_ssse3)\n+\t  {\n+\t    return __intrin_bitcast<_To>(\n+\t      _mm_shuffle_epi8(__intrin,\n+\t\t\t       _mm_setr_epi8(0, 4, 8, 12, -0x80, -0x80, -0x80,\n+\t\t\t\t\t     -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t\t\t     -0x80, -0x80, -0x80, -0x80)));\n+\t  }\n+\telse if constexpr (__x_to_x)\n+\t  {\n+\t    const auto __a\n+\t      = _mm_unpacklo_epi8(__intrin, __intrin); // 0... .... 1... ....\n+\t    const auto __b\n+\t      = _mm_unpackhi_epi8(__intrin, __intrin);    // 2... .... 3... ....\n+\t    const auto __c = _mm_unpacklo_epi8(__a, __b); // 02.. .... .... ....\n+\t    const auto __d = _mm_unpackhi_epi8(__a, __b); // 13.. .... .... ....\n+\t    const auto __e = _mm_unpacklo_epi8(__c, __d); // 0123 .... .... ....\n+\t    return __intrin_bitcast<_To>(__e & _mm_cvtsi32_si128(-1));\n+\t  }\n+\telse if constexpr (__y_to_x && __have_avx512vl)\n+\t  return __intrin_bitcast<_To>(_mm256_cvtepi32_epi8(__intrin));\n+\telse if constexpr (__y_to_x && __have_avx512f)\n+\t  return __intrin_bitcast<_To>(\n+\t    _mm512_cvtepi32_epi8(__zero_extend(__intrin)));\n+\telse if constexpr (__z_to_x)\n+\t  return __intrin_bitcast<_To>(_mm512_cvtepi32_epi8(__intrin));\n+      }\n+    else if constexpr (__i16_to_i64) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_sse4_1)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm_cvtepi16_epi64(__intrin)\n+\t\t\t\t\t : _mm_cvtepu16_epi64(__intrin));\n+\telse if constexpr (__x_to_x && is_signed_v<_Tp>)\n+\t  {\n+\t    auto __x = _mm_srai_epi16(__intrin, 15);\n+\t    auto __y = _mm_unpacklo_epi16(__intrin, __x);\n+\t    __x = _mm_unpacklo_epi16(__x, __x);\n+\t    return __intrin_bitcast<_To>(_mm_unpacklo_epi32(__y, __x));\n+\t  }\n+\telse if constexpr (__x_to_x)\n+\t  return __intrin_bitcast<_To>(\n+\t    _mm_unpacklo_epi32(_mm_unpacklo_epi16(__intrin, __m128i()),\n+\t\t\t       __m128i()));\n+\telse if constexpr (__x_to_y)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm256_cvtepi16_epi64(__intrin)\n+\t\t\t\t\t : _mm256_cvtepu16_epi64(__intrin));\n+\telse if constexpr (__x_to_z)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm512_cvtepi16_epi64(__intrin)\n+\t\t\t\t\t : _mm512_cvtepu16_epi64(__intrin));\n+      }\n+    else if constexpr (__i16_to_i32) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_sse4_1)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm_cvtepi16_epi32(__intrin)\n+\t\t\t\t\t : _mm_cvtepu16_epi32(__intrin));\n+\telse if constexpr (__x_to_x && is_signed_v<_Tp>)\n+\t  return __intrin_bitcast<_To>(\n+\t    _mm_srai_epi32(_mm_unpacklo_epi16(__intrin, __intrin), 16));\n+\telse if constexpr (__x_to_x && is_unsigned_v<_Tp>)\n+\t  return __intrin_bitcast<_To>(_mm_unpacklo_epi16(__intrin, __m128i()));\n+\telse if constexpr (__x_to_y)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm256_cvtepi16_epi32(__intrin)\n+\t\t\t\t\t : _mm256_cvtepu16_epi32(__intrin));\n+\telse if constexpr (__y_to_z)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm512_cvtepi16_epi32(__intrin)\n+\t\t\t\t\t : _mm512_cvtepu16_epi32(__intrin));\n+      }\n+    else if constexpr (__i16_to_i8) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_avx512bw_vl)\n+\t  return __intrin_bitcast<_To>(_mm_cvtepi16_epi8(__intrin));\n+\telse if constexpr (__x_to_x && __have_avx512bw)\n+\t  return __intrin_bitcast<_To>(\n+\t    __lo128(_mm512_cvtepi16_epi8(__zero_extend(__intrin))));\n+\telse if constexpr (__x_to_x && __have_ssse3)\n+\t  return __intrin_bitcast<_To>(_mm_shuffle_epi8(\n+\t    __intrin, _mm_setr_epi8(0, 2, 4, 6, 8, 10, 12, 14, -0x80, -0x80,\n+\t\t\t\t    -0x80, -0x80, -0x80, -0x80, -0x80, -0x80)));\n+\telse if constexpr (__x_to_x)\n+\t  {\n+\t    auto __a\n+\t      = _mm_unpacklo_epi8(__intrin, __intrin); // 00.. 11.. 22.. 33..\n+\t    auto __b\n+\t      = _mm_unpackhi_epi8(__intrin, __intrin); // 44.. 55.. 66.. 77..\n+\t    auto __c = _mm_unpacklo_epi8(__a, __b);    // 0404 .... 1515 ....\n+\t    auto __d = _mm_unpackhi_epi8(__a, __b);    // 2626 .... 3737 ....\n+\t    auto __e = _mm_unpacklo_epi8(__c, __d);    // 0246 0246 .... ....\n+\t    auto __f = _mm_unpackhi_epi8(__c, __d);    // 1357 1357 .... ....\n+\t    return __intrin_bitcast<_To>(_mm_unpacklo_epi8(__e, __f));\n+\t  }\n+\telse if constexpr (__y_to_x && __have_avx512bw_vl)\n+\t  return __intrin_bitcast<_To>(_mm256_cvtepi16_epi8(__intrin));\n+\telse if constexpr (__y_to_x && __have_avx512bw)\n+\t  return __intrin_bitcast<_To>(\n+\t    __lo256(_mm512_cvtepi16_epi8(__zero_extend(__intrin))));\n+\telse if constexpr (__y_to_x)\n+\t  {\n+\t    auto __a = _mm256_shuffle_epi8(\n+\t      __intrin,\n+\t      _mm256_setr_epi8(0, 2, 4, 6, 8, 10, 12, 14, -0x80, -0x80, -0x80,\n+\t\t\t       -0x80, -0x80, -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t       -0x80, -0x80, -0x80, -0x80, -0x80, -0x80, 0, 2,\n+\t\t\t       4, 6, 8, 10, 12, 14));\n+\t    return __intrin_bitcast<_To>(__lo128(__a) | __hi128(__a));\n+\t  }\n+\telse if constexpr (__z_to_y && __have_avx512bw)\n+\t  return __intrin_bitcast<_To>(_mm512_cvtepi16_epi8(__intrin));\n+\telse if constexpr (__z_to_y)\n+\t  __assert_unreachable<_Tp>();\n+      }\n+    else if constexpr (__i8_to_i64) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_sse4_1)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm_cvtepi8_epi64(__intrin)\n+\t\t\t\t\t : _mm_cvtepu8_epi64(__intrin));\n+\telse if constexpr (__x_to_x && is_signed_v<_Tp>)\n+\t  {\n+\t    if constexpr (__have_ssse3)\n+\t      {\n+\t\tauto __dup = _mm_unpacklo_epi8(__intrin, __intrin);\n+\t\tauto __epi16 = _mm_srai_epi16(__dup, 8);\n+\t\t_mm_shuffle_epi8(__epi16,\n+\t\t\t\t _mm_setr_epi8(0, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3,\n+\t\t\t\t\t       3, 3, 3, 3, 3));\n+\t      }\n+\t    else\n+\t      {\n+\t\tauto __x = _mm_unpacklo_epi8(__intrin, __intrin);\n+\t\t__x = _mm_unpacklo_epi16(__x, __x);\n+\t\treturn __intrin_bitcast<_To>(\n+\t\t  _mm_unpacklo_epi32(_mm_srai_epi32(__x, 24),\n+\t\t\t\t     _mm_srai_epi32(__x, 31)));\n+\t      }\n+\t  }\n+\telse if constexpr (__x_to_x)\n+\t  {\n+\t    return __intrin_bitcast<_To>(_mm_unpacklo_epi32(\n+\t      _mm_unpacklo_epi16(_mm_unpacklo_epi8(__intrin, __m128i()),\n+\t\t\t\t __m128i()),\n+\t      __m128i()));\n+\t  }\n+\telse if constexpr (__x_to_y)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm256_cvtepi8_epi64(__intrin)\n+\t\t\t\t\t : _mm256_cvtepu8_epi64(__intrin));\n+\telse if constexpr (__x_to_z)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm512_cvtepi8_epi64(__intrin)\n+\t\t\t\t\t : _mm512_cvtepu8_epi64(__intrin));\n+      }\n+    else if constexpr (__i8_to_i32) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_sse4_1)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm_cvtepi8_epi32(__intrin)\n+\t\t\t\t\t : _mm_cvtepu8_epi32(__intrin));\n+\telse if constexpr (__x_to_x && is_signed_v<_Tp>)\n+\t  {\n+\t    const auto __x = _mm_unpacklo_epi8(__intrin, __intrin);\n+\t    return __intrin_bitcast<_To>(\n+\t      _mm_srai_epi32(_mm_unpacklo_epi16(__x, __x), 24));\n+\t  }\n+\telse if constexpr (__x_to_x && is_unsigned_v<_Tp>)\n+\t  return __intrin_bitcast<_To>(\n+\t    _mm_unpacklo_epi16(_mm_unpacklo_epi8(__intrin, __m128i()),\n+\t\t\t       __m128i()));\n+\telse if constexpr (__x_to_y)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm256_cvtepi8_epi32(__intrin)\n+\t\t\t\t\t : _mm256_cvtepu8_epi32(__intrin));\n+\telse if constexpr (__x_to_z)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm512_cvtepi8_epi32(__intrin)\n+\t\t\t\t\t : _mm512_cvtepu8_epi32(__intrin));\n+      }\n+    else if constexpr (__i8_to_i16) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_sse4_1)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm_cvtepi8_epi16(__intrin)\n+\t\t\t\t\t : _mm_cvtepu8_epi16(__intrin));\n+\telse if constexpr (__x_to_x && is_signed_v<_Tp>)\n+\t  return __intrin_bitcast<_To>(\n+\t    _mm_srai_epi16(_mm_unpacklo_epi8(__intrin, __intrin), 8));\n+\telse if constexpr (__x_to_x && is_unsigned_v<_Tp>)\n+\t  return __intrin_bitcast<_To>(_mm_unpacklo_epi8(__intrin, __m128i()));\n+\telse if constexpr (__x_to_y)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm256_cvtepi8_epi16(__intrin)\n+\t\t\t\t\t : _mm256_cvtepu8_epi16(__intrin));\n+\telse if constexpr (__y_to_z && __have_avx512bw)\n+\t  return __intrin_bitcast<_To>(is_signed_v<_Tp>\n+\t\t\t\t\t ? _mm512_cvtepi8_epi16(__intrin)\n+\t\t\t\t\t : _mm512_cvtepu8_epi16(__intrin));\n+\telse if constexpr (__y_to_z)\n+\t  __assert_unreachable<_Tp>();\n+      }\n+    else if constexpr (__f32_to_s64) //{{{2\n+      {\n+\tif constexpr (__have_avx512dq_vl && __x_to_x)\n+\t  return __intrin_bitcast<_To>(_mm_cvttps_epi64(__intrin));\n+\telse if constexpr (__have_avx512dq_vl && __x_to_y)\n+\t  return __intrin_bitcast<_To>(_mm256_cvttps_epi64(__intrin));\n+\telse if constexpr (__have_avx512dq && __y_to_z)\n+\t  return __intrin_bitcast<_To>(_mm512_cvttps_epi64(__intrin));\n+\t// else use scalar fallback\n+      }\n+    else if constexpr (__f32_to_u64) //{{{2\n+      {\n+\tif constexpr (__have_avx512dq_vl && __x_to_x)\n+\t  return __intrin_bitcast<_To>(_mm_cvttps_epu64(__intrin));\n+\telse if constexpr (__have_avx512dq_vl && __x_to_y)\n+\t  return __intrin_bitcast<_To>(_mm256_cvttps_epu64(__intrin));\n+\telse if constexpr (__have_avx512dq && __y_to_z)\n+\t  return __intrin_bitcast<_To>(_mm512_cvttps_epu64(__intrin));\n+\t// else use scalar fallback\n+      }\n+    else if constexpr (__f32_to_s32) //{{{2\n+      {\n+\tif constexpr (__x_to_x || __y_to_y || __z_to_z)\n+\t  {\n+\t    // go to fallback, it does the right thing\n+\t  }\n+\telse\n+\t  __assert_unreachable<_Tp>();\n+      }\n+    else if constexpr (__f32_to_u32) //{{{2\n+      {\n+\tif constexpr (__have_avx512vl && __x_to_x)\n+\t  return __auto_bitcast(_mm_cvttps_epu32(__intrin));\n+\telse if constexpr (__have_avx512f && __x_to_x)\n+\t  return __auto_bitcast(\n+\t    __lo128(_mm512_cvttps_epu32(__auto_bitcast(__v))));\n+\telse if constexpr (__have_avx512vl && __y_to_y)\n+\t  return __vector_bitcast<_Up>(_mm256_cvttps_epu32(__intrin));\n+\telse if constexpr (__have_avx512f && __y_to_y)\n+\t  return __vector_bitcast<_Up>(\n+\t    __lo256(_mm512_cvttps_epu32(__auto_bitcast(__v))));\n+\telse if constexpr (__x_to_x || __y_to_y || __z_to_z)\n+\t  {\n+\t    // go to fallback, it does the right thing. We can't use the\n+\t    // _mm_floor_ps - 0x8000'0000 trick for f32->u32 because it would\n+\t    // discard small input values (only 24 mantissa bits)\n+\t  }\n+\telse\n+\t  __assert_unreachable<_Tp>();\n+      }\n+    else if constexpr (__f32_to_ibw) //{{{2\n+      return __convert_x86<_To>(__convert_x86<__vector_type_t<int, _Np>>(__v));\n+    else if constexpr (__f64_to_s64) //{{{2\n+      {\n+\tif constexpr (__have_avx512dq_vl && __x_to_x)\n+\t  return __intrin_bitcast<_To>(_mm_cvttpd_epi64(__intrin));\n+\telse if constexpr (__have_avx512dq_vl && __y_to_y)\n+\t  return __intrin_bitcast<_To>(_mm256_cvttpd_epi64(__intrin));\n+\telse if constexpr (__have_avx512dq && __z_to_z)\n+\t  return __intrin_bitcast<_To>(_mm512_cvttpd_epi64(__intrin));\n+\t// else use scalar fallback\n+      }\n+    else if constexpr (__f64_to_u64) //{{{2\n+      {\n+\tif constexpr (__have_avx512dq_vl && __x_to_x)\n+\t  return __intrin_bitcast<_To>(_mm_cvttpd_epu64(__intrin));\n+\telse if constexpr (__have_avx512dq_vl && __y_to_y)\n+\t  return __intrin_bitcast<_To>(_mm256_cvttpd_epu64(__intrin));\n+\telse if constexpr (__have_avx512dq && __z_to_z)\n+\t  return __intrin_bitcast<_To>(_mm512_cvttpd_epu64(__intrin));\n+\t// else use scalar fallback\n+      }\n+    else if constexpr (__f64_to_s32) //{{{2\n+      {\n+\tif constexpr (__x_to_x)\n+\t  return __intrin_bitcast<_To>(_mm_cvttpd_epi32(__intrin));\n+\telse if constexpr (__y_to_x)\n+\t  return __intrin_bitcast<_To>(_mm256_cvttpd_epi32(__intrin));\n+\telse if constexpr (__z_to_y)\n+\t  return __intrin_bitcast<_To>(_mm512_cvttpd_epi32(__intrin));\n+      }\n+    else if constexpr (__f64_to_u32) //{{{2\n+      {\n+\tif constexpr (__have_avx512vl && __x_to_x)\n+\t  return __intrin_bitcast<_To>(_mm_cvttpd_epu32(__intrin));\n+\telse if constexpr (__have_sse4_1 && __x_to_x)\n+\t  return __vector_bitcast<_Up, _M>(\n+\t\t   _mm_cvttpd_epi32(_mm_floor_pd(__intrin) - 0x8000'0000u))\n+\t\t ^ 0x8000'0000u;\n+\telse if constexpr (__x_to_x)\n+\t  {\n+\t    // use scalar fallback: it's only 2 values to convert, can't get\n+\t    // much better than scalar decomposition\n+\t  }\n+\telse if constexpr (__have_avx512vl && __y_to_x)\n+\t  return __intrin_bitcast<_To>(_mm256_cvttpd_epu32(__intrin));\n+\telse if constexpr (__y_to_x)\n+\t  {\n+\t    return __intrin_bitcast<_To>(\n+\t      __vector_bitcast<_Up>(\n+\t\t_mm256_cvttpd_epi32(_mm256_floor_pd(__intrin) - 0x8000'0000u))\n+\t      ^ 0x8000'0000u);\n+\t  }\n+\telse if constexpr (__z_to_y)\n+\t  return __intrin_bitcast<_To>(_mm512_cvttpd_epu32(__intrin));\n+      }\n+    else if constexpr (__f64_to_ibw) //{{{2\n+      {\n+\treturn __convert_x86<_To>(\n+\t  __convert_x86<__vector_type_t<int, (_Np < 4 ? 4 : _Np)>>(__v));\n+      }\n+    else if constexpr (__s64_to_f32) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_avx512dq_vl)\n+\t  return __intrin_bitcast<_To>(_mm_cvtepi64_ps(__intrin));\n+\telse if constexpr (__y_to_x && __have_avx512dq_vl)\n+\t  return __intrin_bitcast<_To>(_mm256_cvtepi64_ps(__intrin));\n+\telse if constexpr (__z_to_y && __have_avx512dq)\n+\t  return __intrin_bitcast<_To>(_mm512_cvtepi64_ps(__intrin));\n+\telse if constexpr (__z_to_y)\n+\t  return __intrin_bitcast<_To>(\n+\t    _mm512_cvtpd_ps(__convert_x86<__vector_type_t<double, 8>>(__v)));\n+      }\n+    else if constexpr (__u64_to_f32) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_avx512dq_vl)\n+\t  return __intrin_bitcast<_To>(_mm_cvtepu64_ps(__intrin));\n+\telse if constexpr (__y_to_x && __have_avx512dq_vl)\n+\t  return __intrin_bitcast<_To>(_mm256_cvtepu64_ps(__intrin));\n+\telse if constexpr (__z_to_y && __have_avx512dq)\n+\t  return __intrin_bitcast<_To>(_mm512_cvtepu64_ps(__intrin));\n+\telse if constexpr (__z_to_y)\n+\t  {\n+\t    return __intrin_bitcast<_To>(\n+\t      __lo256(_mm512_cvtepu32_ps(__auto_bitcast(\n+\t\t_mm512_cvtepi64_epi32(_mm512_srai_epi64(__intrin, 32)))))\n+\t\t* 0x100000000LL\n+\t      + __lo256(_mm512_cvtepu32_ps(\n+\t\t__auto_bitcast(_mm512_cvtepi64_epi32(__intrin)))));\n+\t  }\n+      }\n+    else if constexpr (__s32_to_f32) //{{{2\n+      {\n+\t// use fallback (builtin conversion)\n+      }\n+    else if constexpr (__u32_to_f32) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_avx512vl)\n+\t  {\n+\t    // use fallback\n+\t  }\n+\telse if constexpr (__x_to_x && __have_avx512f)\n+\t  return __intrin_bitcast<_To>(\n+\t    __lo128(_mm512_cvtepu32_ps(__auto_bitcast(__v))));\n+\telse if constexpr (__x_to_x && (__have_fma || __have_fma4))\n+\t  // work around PR85819\n+\t  return __auto_bitcast(0x10000\n+\t\t\t\t  * _mm_cvtepi32_ps(__to_intrin(__v >> 16))\n+\t\t\t\t+ _mm_cvtepi32_ps(__to_intrin(__v & 0xffff)));\n+\telse if constexpr (__y_to_y && __have_avx512vl)\n+\t  {\n+\t    // use fallback\n+\t  }\n+\telse if constexpr (__y_to_y && __have_avx512f)\n+\t  return __intrin_bitcast<_To>(\n+\t    __lo256(_mm512_cvtepu32_ps(__auto_bitcast(__v))));\n+\telse if constexpr (__y_to_y)\n+\t  // work around PR85819\n+\t  return 0x10000 * _mm256_cvtepi32_ps(__to_intrin(__v >> 16))\n+\t\t + _mm256_cvtepi32_ps(__to_intrin(__v & 0xffff));\n+\t// else use fallback (builtin conversion)\n+      }\n+    else if constexpr (__ibw_to_f32) //{{{2\n+      {\n+\tif constexpr (_M <= 4 || __have_avx2)\n+\t  return __convert_x86<_To>(\n+\t    __convert_x86<__vector_type_t<int, _M>>(__v));\n+\telse\n+\t  {\n+\t    static_assert(__x_to_y);\n+\t    __m128i __a, __b;\n+\t    if constexpr (__have_sse4_1)\n+\t      {\n+\t\t__a = sizeof(_Tp) == 2\n+\t\t\t? (is_signed_v<_Tp> ? _mm_cvtepi16_epi32(__intrin)\n+\t\t\t\t\t    : _mm_cvtepu16_epi32(__intrin))\n+\t\t\t: (is_signed_v<_Tp> ? _mm_cvtepi8_epi32(__intrin)\n+\t\t\t\t\t    : _mm_cvtepu8_epi32(__intrin));\n+\t\tconst auto __w\n+\t\t  = _mm_shuffle_epi32(__intrin, sizeof(_Tp) == 2 ? 0xee : 0xe9);\n+\t\t__b = sizeof(_Tp) == 2\n+\t\t\t? (is_signed_v<_Tp> ? _mm_cvtepi16_epi32(__w)\n+\t\t\t\t\t    : _mm_cvtepu16_epi32(__w))\n+\t\t\t: (is_signed_v<_Tp> ? _mm_cvtepi8_epi32(__w)\n+\t\t\t\t\t    : _mm_cvtepu8_epi32(__w));\n+\t      }\n+\t    else\n+\t      {\n+\t\t__m128i __tmp;\n+\t\tif constexpr (sizeof(_Tp) == 1)\n+\t\t  {\n+\t\t    __tmp = is_signed_v<_Tp>\n+\t\t\t      ? _mm_srai_epi16(_mm_unpacklo_epi8(__intrin,\n+\t\t\t\t\t\t\t\t __intrin),\n+\t\t\t\t\t       8)\n+\t\t\t      : _mm_unpacklo_epi8(__intrin, __m128i());\n+\t\t  }\n+\t\telse\n+\t\t  {\n+\t\t    static_assert(sizeof(_Tp) == 2);\n+\t\t    __tmp = __intrin;\n+\t\t  }\n+\t\t__a = is_signed_v<_Tp>\n+\t\t\t? _mm_srai_epi32(_mm_unpacklo_epi16(__tmp, __tmp), 16)\n+\t\t\t: _mm_unpacklo_epi16(__tmp, __m128i());\n+\t\t__b = is_signed_v<_Tp>\n+\t\t\t? _mm_srai_epi32(_mm_unpackhi_epi16(__tmp, __tmp), 16)\n+\t\t\t: _mm_unpackhi_epi16(__tmp, __m128i());\n+\t      }\n+\t    return __convert_x86<_To>(__vector_bitcast<int>(__a),\n+\t\t\t\t      __vector_bitcast<int>(__b));\n+\t  }\n+      }\n+    else if constexpr (__s64_to_f64) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_avx512dq_vl)\n+\t  return __intrin_bitcast<_To>(_mm_cvtepi64_pd(__intrin));\n+\telse if constexpr (__y_to_y && __have_avx512dq_vl)\n+\t  return __intrin_bitcast<_To>(_mm256_cvtepi64_pd(__intrin));\n+\telse if constexpr (__z_to_z && __have_avx512dq)\n+\t  return __intrin_bitcast<_To>(_mm512_cvtepi64_pd(__intrin));\n+\telse if constexpr (__z_to_z)\n+\t  {\n+\t    return __intrin_bitcast<_To>(\n+\t      _mm512_cvtepi32_pd(_mm512_cvtepi64_epi32(__to_intrin(__v >> 32)))\n+\t\t* 0x100000000LL\n+\t      + _mm512_cvtepu32_pd(_mm512_cvtepi64_epi32(__intrin)));\n+\t  }\n+      }\n+    else if constexpr (__u64_to_f64) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_avx512dq_vl)\n+\t  return __intrin_bitcast<_To>(_mm_cvtepu64_pd(__intrin));\n+\telse if constexpr (__y_to_y && __have_avx512dq_vl)\n+\t  return __intrin_bitcast<_To>(_mm256_cvtepu64_pd(__intrin));\n+\telse if constexpr (__z_to_z && __have_avx512dq)\n+\t  return __intrin_bitcast<_To>(_mm512_cvtepu64_pd(__intrin));\n+\telse if constexpr (__z_to_z)\n+\t  {\n+\t    return __intrin_bitcast<_To>(\n+\t      _mm512_cvtepu32_pd(_mm512_cvtepi64_epi32(__to_intrin(__v >> 32)))\n+\t\t* 0x100000000LL\n+\t      + _mm512_cvtepu32_pd(_mm512_cvtepi64_epi32(__intrin)));\n+\t  }\n+      }\n+    else if constexpr (__s32_to_f64) //{{{2\n+      {\n+\tif constexpr (__x_to_x)\n+\t  return __intrin_bitcast<_To>(_mm_cvtepi32_pd(__intrin));\n+\telse if constexpr (__x_to_y)\n+\t  return __intrin_bitcast<_To>(_mm256_cvtepi32_pd(__intrin));\n+\telse if constexpr (__y_to_z)\n+\t  return __intrin_bitcast<_To>(_mm512_cvtepi32_pd(__intrin));\n+      }\n+    else if constexpr (__u32_to_f64) //{{{2\n+      {\n+\tif constexpr (__x_to_x && __have_avx512vl)\n+\t  return __intrin_bitcast<_To>(_mm_cvtepu32_pd(__intrin));\n+\telse if constexpr (__x_to_x && __have_avx512f)\n+\t  return __intrin_bitcast<_To>(\n+\t    __lo128(_mm512_cvtepu32_pd(__auto_bitcast(__v))));\n+\telse if constexpr (__x_to_x)\n+\t  return __intrin_bitcast<_To>(\n+\t    _mm_cvtepi32_pd(__to_intrin(__v ^ 0x8000'0000u)) + 0x8000'0000u);\n+\telse if constexpr (__x_to_y && __have_avx512vl)\n+\t  return __intrin_bitcast<_To>(_mm256_cvtepu32_pd(__intrin));\n+\telse if constexpr (__x_to_y && __have_avx512f)\n+\t  return __intrin_bitcast<_To>(\n+\t    __lo256(_mm512_cvtepu32_pd(__auto_bitcast(__v))));\n+\telse if constexpr (__x_to_y)\n+\t  return __intrin_bitcast<_To>(\n+\t    _mm256_cvtepi32_pd(__to_intrin(__v ^ 0x8000'0000u)) + 0x8000'0000u);\n+\telse if constexpr (__y_to_z)\n+\t  return __intrin_bitcast<_To>(_mm512_cvtepu32_pd(__intrin));\n+      }\n+    else if constexpr (__ibw_to_f64) //{{{2\n+      {\n+\treturn __convert_x86<_To>(\n+\t  __convert_x86<__vector_type_t<int, std::max(size_t(4), _M)>>(__v));\n+      }\n+    else if constexpr (__f32_to_f64) //{{{2\n+      {\n+\tif constexpr (__x_to_x)\n+\t  return __intrin_bitcast<_To>(_mm_cvtps_pd(__intrin));\n+\telse if constexpr (__x_to_y)\n+\t  return __intrin_bitcast<_To>(_mm256_cvtps_pd(__intrin));\n+\telse if constexpr (__y_to_z)\n+\t  return __intrin_bitcast<_To>(_mm512_cvtps_pd(__intrin));\n+      }\n+    else if constexpr (__f64_to_f32) //{{{2\n+      {\n+\tif constexpr (__x_to_x)\n+\t  return __intrin_bitcast<_To>(_mm_cvtpd_ps(__intrin));\n+\telse if constexpr (__y_to_x)\n+\t  return __intrin_bitcast<_To>(_mm256_cvtpd_ps(__intrin));\n+\telse if constexpr (__z_to_y)\n+\t  return __intrin_bitcast<_To>(_mm512_cvtpd_ps(__intrin));\n+      }\n+    else //{{{2\n+      __assert_unreachable<_Tp>();\n+\n+    // fallback:{{{2\n+    return __vector_convert<_To>(__v, make_index_sequence<std::min(_M, _Np)>());\n+    //}}}\n+  }\n+\n+// }}}\n+// 2-arg __convert_x86 {{{1\n+template <typename _To, typename _V, typename _Traits>\n+  _GLIBCXX_SIMD_INTRINSIC _To\n+  __convert_x86(_V __v0, _V __v1)\n+  {\n+    static_assert(__is_vector_type_v<_V>);\n+    using _Tp = typename _Traits::value_type;\n+    constexpr size_t _Np = _Traits::_S_full_size;\n+    [[maybe_unused]] const auto __i0 = __to_intrin(__v0);\n+    [[maybe_unused]] const auto __i1 = __to_intrin(__v1);\n+    using _Up = typename _VectorTraits<_To>::value_type;\n+    constexpr size_t _M = _VectorTraits<_To>::_S_full_size;\n+\n+    static_assert(2 * _Np <= _M,\n+\t\t  \"__v1 would be discarded; use the one-argument \"\n+\t\t  \"__convert_x86 overload instead\");\n+\n+    // [xyz]_to_[xyz] {{{2\n+    [[maybe_unused]] constexpr bool __x_to_x\n+      = sizeof(__v0) <= 16 && sizeof(_To) <= 16;\n+    [[maybe_unused]] constexpr bool __x_to_y\n+      = sizeof(__v0) <= 16 && sizeof(_To) == 32;\n+    [[maybe_unused]] constexpr bool __x_to_z\n+      = sizeof(__v0) <= 16 && sizeof(_To) == 64;\n+    [[maybe_unused]] constexpr bool __y_to_x\n+      = sizeof(__v0) == 32 && sizeof(_To) <= 16;\n+    [[maybe_unused]] constexpr bool __y_to_y\n+      = sizeof(__v0) == 32 && sizeof(_To) == 32;\n+    [[maybe_unused]] constexpr bool __y_to_z\n+      = sizeof(__v0) == 32 && sizeof(_To) == 64;\n+    [[maybe_unused]] constexpr bool __z_to_x\n+      = sizeof(__v0) == 64 && sizeof(_To) <= 16;\n+    [[maybe_unused]] constexpr bool __z_to_y\n+      = sizeof(__v0) == 64 && sizeof(_To) == 32;\n+    [[maybe_unused]] constexpr bool __z_to_z\n+      = sizeof(__v0) == 64 && sizeof(_To) == 64;\n+\n+    // iX_to_iX {{{2\n+    [[maybe_unused]] constexpr bool __i_to_i\n+      = is_integral_v<_Up> && is_integral_v<_Tp>;\n+    [[maybe_unused]] constexpr bool __i8_to_i16\n+      = __i_to_i && sizeof(_Tp) == 1 && sizeof(_Up) == 2;\n+    [[maybe_unused]] constexpr bool __i8_to_i32\n+      = __i_to_i && sizeof(_Tp) == 1 && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __i8_to_i64\n+      = __i_to_i && sizeof(_Tp) == 1 && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __i16_to_i8\n+      = __i_to_i && sizeof(_Tp) == 2 && sizeof(_Up) == 1;\n+    [[maybe_unused]] constexpr bool __i16_to_i32\n+      = __i_to_i && sizeof(_Tp) == 2 && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __i16_to_i64\n+      = __i_to_i && sizeof(_Tp) == 2 && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __i32_to_i8\n+      = __i_to_i && sizeof(_Tp) == 4 && sizeof(_Up) == 1;\n+    [[maybe_unused]] constexpr bool __i32_to_i16\n+      = __i_to_i && sizeof(_Tp) == 4 && sizeof(_Up) == 2;\n+    [[maybe_unused]] constexpr bool __i32_to_i64\n+      = __i_to_i && sizeof(_Tp) == 4 && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __i64_to_i8\n+      = __i_to_i && sizeof(_Tp) == 8 && sizeof(_Up) == 1;\n+    [[maybe_unused]] constexpr bool __i64_to_i16\n+      = __i_to_i && sizeof(_Tp) == 8 && sizeof(_Up) == 2;\n+    [[maybe_unused]] constexpr bool __i64_to_i32\n+      = __i_to_i && sizeof(_Tp) == 8 && sizeof(_Up) == 4;\n+\n+    // [fsu]X_to_[fsu]X {{{2\n+    // ibw = integral && byte or word, i.e. char and short with any signedness\n+    [[maybe_unused]] constexpr bool __i64_to_f32\n+      = is_integral_v<_Tp> && sizeof(_Tp) == 8\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __s32_to_f32\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __s16_to_f32\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 2\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __s8_to_f32\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 1\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __u32_to_f32\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __u16_to_f32\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 2\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __u8_to_f32\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 1\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __s64_to_f64\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 8\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __s32_to_f64\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __s16_to_f64\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 2\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __s8_to_f64\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 1\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __u64_to_f64\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 8\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __u32_to_f64\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __u16_to_f64\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 2\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __u8_to_f64\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 1\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __f32_to_s64\n+      = is_integral_v<_Up> && is_signed_v<_Up> && sizeof(_Up) == 8\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f32_to_s32\n+      = is_integral_v<_Up> && is_signed_v<_Up> && sizeof(_Up) == 4\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f32_to_u64\n+      = is_integral_v<_Up> && is_unsigned_v<_Up> && sizeof(_Up) == 8\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f32_to_u32\n+      = is_integral_v<_Up> && is_unsigned_v<_Up> && sizeof(_Up) == 4\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f64_to_s64\n+      = is_integral_v<_Up> && is_signed_v<_Up> && sizeof(_Up) == 8\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __f64_to_s32\n+      = is_integral_v<_Up> && is_signed_v<_Up> && sizeof(_Up) == 4\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __f64_to_u64\n+      = is_integral_v<_Up> && is_unsigned_v<_Up> && sizeof(_Up) == 8\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __f64_to_u32\n+      = is_integral_v<_Up> && is_unsigned_v<_Up> && sizeof(_Up) == 4\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __f32_to_ibw\n+      = is_integral_v<_Up> && sizeof(_Up) <= 2\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f64_to_ibw\n+      = is_integral_v<_Up> && sizeof(_Up) <= 2\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __f32_to_f64\n+      = is_floating_point_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __f64_to_f32\n+      = is_floating_point_v<_Tp> && sizeof(_Tp) == 8\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+\n+    if constexpr (__i_to_i && __y_to_x && !__have_avx2) //{{{2\n+      // <double, 4>, <double, 4> => <short, 8>\n+      return __convert_x86<_To>(__lo128(__v0), __hi128(__v0), __lo128(__v1),\n+\t\t\t\t__hi128(__v1));\n+    else if constexpr (__i_to_i) // assert ISA {{{2\n+      {\n+\tstatic_assert(__x_to_x || __have_avx2,\n+\t\t      \"integral conversions with ymm registers require AVX2\");\n+\tstatic_assert(__have_avx512bw\n+\t\t\t|| ((sizeof(_Tp) >= 4 || sizeof(__v0) < 64)\n+\t\t\t    && (sizeof(_Up) >= 4 || sizeof(_To) < 64)),\n+\t\t      \"8/16-bit integers in zmm registers require AVX512BW\");\n+\tstatic_assert((sizeof(__v0) < 64 && sizeof(_To) < 64) || __have_avx512f,\n+\t\t      \"integral conversions with ymm registers require AVX2\");\n+      }\n+    // concat => use 1-arg __convert_x86 {{{2\n+    if constexpr (sizeof(__v0) < 16 || (sizeof(__v0) == 16 && __have_avx2)\n+\t\t  || (sizeof(__v0) == 16 && __have_avx\n+\t\t      && is_floating_point_v<_Tp>)\n+\t\t  || (sizeof(__v0) == 32 && __have_avx512f\n+\t\t      && (sizeof(_Tp) >= 4 || __have_avx512bw)))\n+      {\n+\t// The ISA can handle wider input registers, so concat and use one-arg\n+\t// implementation. This reduces code duplication considerably.\n+\treturn __convert_x86<_To>(__concat(__v0, __v1));\n+      }\n+    else //{{{2\n+      {\n+\t// conversion using bit reinterpretation (or no conversion at all)\n+\t// should all go through the concat branch above:\n+\tstatic_assert(\n+\t  !(is_floating_point_v<\n+\t      _Tp> == is_floating_point_v<_Up> && sizeof(_Tp) == sizeof(_Up)));\n+\t// handle all zero extension{{{2\n+\tif constexpr (2 * _Np < _M && sizeof(_To) > 16)\n+\t  {\n+\t    constexpr size_t Min = 16 / sizeof(_Up);\n+\t    return __zero_extend(\n+\t      __convert_x86<\n+\t\t__vector_type_t<_Up, (Min > 2 * _Np) ? Min : 2 * _Np>>(__v0,\n+\t\t\t\t\t\t\t\t       __v1));\n+\t  }\n+\telse if constexpr (__i64_to_i32) //{{{2\n+\t  {\n+\t    if constexpr (__x_to_x)\n+\t      return __auto_bitcast(_mm_shuffle_ps(__auto_bitcast(__v0),\n+\t\t\t\t\t\t   __auto_bitcast(__v1), 0x88));\n+\t    else if constexpr (__y_to_y)\n+\t      {\n+\t\t// AVX512F is not available (would concat otherwise)\n+\t\treturn __auto_bitcast(\n+\t\t  __xzyw(_mm256_shuffle_ps(__auto_bitcast(__v0),\n+\t\t\t\t\t   __auto_bitcast(__v1), 0x88)));\n+\t\t// alternative:\n+\t\t// const auto v0_abxxcdxx = _mm256_shuffle_epi32(__v0, 8);\n+\t\t// const auto v1_efxxghxx = _mm256_shuffle_epi32(__v1, 8);\n+\t\t// const auto v_abefcdgh = _mm256_unpacklo_epi64(v0_abxxcdxx,\n+\t\t// v1_efxxghxx); return _mm256_permute4x64_epi64(v_abefcdgh,\n+\t\t// 0x01 * 0 + 0x04 * 2 + 0x10 * 1 + 0x40 * 3);  // abcdefgh\n+\t      }\n+\t    else if constexpr (__z_to_z)\n+\t      return __intrin_bitcast<_To>(\n+\t\t__concat(_mm512_cvtepi64_epi32(__i0),\n+\t\t\t _mm512_cvtepi64_epi32(__i1)));\n+\t  }\n+\telse if constexpr (__i64_to_i16) //{{{2\n+\t  {\n+\t    if constexpr (__x_to_x)\n+\t      {\n+\t\t// AVX2 is not available (would concat otherwise)\n+\t\tif constexpr (__have_sse4_1)\n+\t\t  {\n+\t\t    return __intrin_bitcast<_To>(_mm_shuffle_epi8(\n+\t\t      _mm_blend_epi16(__i0, _mm_slli_si128(__i1, 4), 0x44),\n+\t\t      _mm_setr_epi8(0, 1, 8, 9, 4, 5, 12, 13, -0x80, -0x80,\n+\t\t\t\t    -0x80, -0x80, -0x80, -0x80, -0x80, -0x80)));\n+\t\t  }\n+\t\telse\n+\t\t  {\n+\t\t    return __vector_type_t<_Up, _M>{_Up(__v0[0]), _Up(__v0[1]),\n+\t\t\t\t\t\t    _Up(__v1[0]), _Up(__v1[1])};\n+\t\t  }\n+\t      }\n+\t    else if constexpr (__y_to_x)\n+\t      {\n+\t\tauto __a\n+\t\t  = _mm256_unpacklo_epi16(__i0, __i1); // 04.. .... 26.. ....\n+\t\tauto __b\n+\t\t  = _mm256_unpackhi_epi16(__i0, __i1); // 15.. .... 37.. ....\n+\t\tauto __c\n+\t\t  = _mm256_unpacklo_epi16(__a, __b); // 0145 .... 2367 ....\n+\t\treturn __intrin_bitcast<_To>(\n+\t\t  _mm_unpacklo_epi32(__lo128(__c), __hi128(__c))); // 0123 4567\n+\t      }\n+\t    else if constexpr (__z_to_y)\n+\t      return __intrin_bitcast<_To>(\n+\t\t__concat(_mm512_cvtepi64_epi16(__i0),\n+\t\t\t _mm512_cvtepi64_epi16(__i1)));\n+\t  }\n+\telse if constexpr (__i64_to_i8) //{{{2\n+\t  {\n+\t    if constexpr (__x_to_x && __have_sse4_1)\n+\t      {\n+\t\treturn __intrin_bitcast<_To>(_mm_shuffle_epi8(\n+\t\t  _mm_blend_epi16(__i0, _mm_slli_si128(__i1, 4), 0x44),\n+\t\t  _mm_setr_epi8(0, 8, 4, 12, -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t\t-0x80, -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t\t-0x80)));\n+\t      }\n+\t    else if constexpr (__x_to_x && __have_ssse3)\n+\t      {\n+\t\treturn __intrin_bitcast<_To>(_mm_unpacklo_epi16(\n+\t\t  _mm_shuffle_epi8(\n+\t\t    __i0, _mm_setr_epi8(0, 8, -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t\t\t-0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t\t\t-0x80, -0x80, -0x80, -0x80)),\n+\t\t  _mm_shuffle_epi8(\n+\t\t    __i1, _mm_setr_epi8(0, 8, -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t\t\t-0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t\t\t-0x80, -0x80, -0x80, -0x80))));\n+\t      }\n+\t    else if constexpr (__x_to_x)\n+\t      {\n+\t\treturn __vector_type_t<_Up, _M>{_Up(__v0[0]), _Up(__v0[1]),\n+\t\t\t\t\t\t_Up(__v1[0]), _Up(__v1[1])};\n+\t      }\n+\t    else if constexpr (__y_to_x)\n+\t      {\n+\t\tconst auto __a = _mm256_shuffle_epi8(\n+\t\t  _mm256_blend_epi32(__i0, _mm256_slli_epi64(__i1, 32), 0xAA),\n+\t\t  _mm256_setr_epi8(0, 8, -0x80, -0x80, 4, 12, -0x80, -0x80,\n+\t\t\t\t   -0x80, -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t\t   -0x80, -0x80, -0x80, -0x80, 0, 8, -0x80,\n+\t\t\t\t   -0x80, 4, 12, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t\t   -0x80, -0x80, -0x80, -0x80));\n+\t\treturn __intrin_bitcast<_To>(__lo128(__a) | __hi128(__a));\n+\t      } // __z_to_x uses concat fallback\n+\t  }\n+\telse if constexpr (__i32_to_i16) //{{{2\n+\t  {\n+\t    if constexpr (__x_to_x)\n+\t      {\n+\t\t// AVX2 is not available (would concat otherwise)\n+\t\tif constexpr (__have_sse4_1)\n+\t\t  {\n+\t\t    return __intrin_bitcast<_To>(_mm_shuffle_epi8(\n+\t\t      _mm_blend_epi16(__i0, _mm_slli_si128(__i1, 2), 0xaa),\n+\t\t      _mm_setr_epi8(0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10,\n+\t\t\t\t    11, 14, 15)));\n+\t\t  }\n+\t\telse if constexpr (__have_ssse3)\n+\t\t  {\n+\t\t    return __intrin_bitcast<_To>(\n+\t\t      _mm_hadd_epi16(__to_intrin(__v0 << 16),\n+\t\t\t\t     __to_intrin(__v1 << 16)));\n+\t\t    /*\n+\t\t    return _mm_unpacklo_epi64(\n+\t\t\t_mm_shuffle_epi8(__i0, _mm_setr_epi8(0, 1, 4, 5, 8, 9,\n+\t\t    12, 13, 8, 9, 12, 13, 12, 13, 14, 15)),\n+\t\t    _mm_shuffle_epi8(__i1, _mm_setr_epi8(0, 1, 4, 5, 8, 9, 12,\n+\t\t    13, 8, 9, 12, 13, 12, 13, 14, 15)));\n+\t\t\t\t\t\t\t   */\n+\t\t  }\n+\t\telse\n+\t\t  {\n+\t\t    auto __a = _mm_unpacklo_epi16(__i0, __i1); // 04.. 15..\n+\t\t    auto __b = _mm_unpackhi_epi16(__i0, __i1); // 26.. 37..\n+\t\t    auto __c = _mm_unpacklo_epi16(__a, __b);   // 0246 ....\n+\t\t    auto __d = _mm_unpackhi_epi16(__a, __b);   // 1357 ....\n+\t\t    return __intrin_bitcast<_To>(\n+\t\t      _mm_unpacklo_epi16(__c, __d)); // 0123 4567\n+\t\t  }\n+\t      }\n+\t    else if constexpr (__y_to_y)\n+\t      {\n+\t\tconst auto __shuf\n+\t\t  = _mm256_setr_epi8(0, 1, 4, 5, 8, 9, 12, 13, -0x80, -0x80,\n+\t\t\t\t     -0x80, -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t\t     0, 1, 4, 5, 8, 9, 12, 13, -0x80, -0x80,\n+\t\t\t\t     -0x80, -0x80, -0x80, -0x80, -0x80, -0x80);\n+\t\tauto __a = _mm256_shuffle_epi8(__i0, __shuf);\n+\t\tauto __b = _mm256_shuffle_epi8(__i1, __shuf);\n+\t\treturn __intrin_bitcast<_To>(\n+\t\t  __xzyw(_mm256_unpacklo_epi64(__a, __b)));\n+\t      } // __z_to_z uses concat fallback\n+\t  }\n+\telse if constexpr (__i32_to_i8) //{{{2\n+\t  {\n+\t    if constexpr (__x_to_x && __have_ssse3)\n+\t      {\n+\t\tconst auto shufmask\n+\t\t  = _mm_setr_epi8(0, 4, 8, 12, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t\t  -0x80, -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t\t  -0x80, -0x80);\n+\t\treturn __intrin_bitcast<_To>(\n+\t\t  _mm_unpacklo_epi32(_mm_shuffle_epi8(__i0, shufmask),\n+\t\t\t\t     _mm_shuffle_epi8(__i1, shufmask)));\n+\t      }\n+\t    else if constexpr (__x_to_x)\n+\t      {\n+\t\tauto __a = _mm_unpacklo_epi8(__i0, __i1); // 04.. .... 15.. ....\n+\t\tauto __b = _mm_unpackhi_epi8(__i0, __i1); // 26.. .... 37.. ....\n+\t\tauto __c = _mm_unpacklo_epi8(__a, __b);   // 0246 .... .... ....\n+\t\tauto __d = _mm_unpackhi_epi8(__a, __b);   // 1357 .... .... ....\n+\t\tauto __e = _mm_unpacklo_epi8(__c, __d);   // 0123 4567 .... ....\n+\t\treturn __intrin_bitcast<_To>(__e & __m128i{-1, 0});\n+\t      }\n+\t    else if constexpr (__y_to_x)\n+\t      {\n+\t\tconst auto __a = _mm256_shuffle_epi8(\n+\t\t  _mm256_blend_epi16(__i0, _mm256_slli_epi32(__i1, 16), 0xAA),\n+\t\t  _mm256_setr_epi8(0, 4, 8, 12, -0x80, -0x80, -0x80, -0x80, 2,\n+\t\t\t\t   6, 10, 14, -0x80, -0x80, -0x80, -0x80, -0x80,\n+\t\t\t\t   -0x80, -0x80, -0x80, 0, 4, 8, 12, -0x80,\n+\t\t\t\t   -0x80, -0x80, -0x80, 2, 6, 10, 14));\n+\t\treturn __intrin_bitcast<_To>(__lo128(__a) | __hi128(__a));\n+\t      } // __z_to_y uses concat fallback\n+\t  }\n+\telse if constexpr (__i16_to_i8) //{{{2\n+\t  {\n+\t    if constexpr (__x_to_x && __have_ssse3)\n+\t      {\n+\t\tconst auto __shuf = reinterpret_cast<__m128i>(\n+\t\t  __vector_type_t<_UChar, 16>{0, 2, 4, 6, 8, 10, 12, 14, 0x80,\n+\t\t\t\t\t      0x80, 0x80, 0x80, 0x80, 0x80,\n+\t\t\t\t\t      0x80, 0x80});\n+\t\treturn __intrin_bitcast<_To>(\n+\t\t  _mm_unpacklo_epi64(_mm_shuffle_epi8(__i0, __shuf),\n+\t\t\t\t     _mm_shuffle_epi8(__i1, __shuf)));\n+\t      }\n+\t    else if constexpr (__x_to_x)\n+\t      {\n+\t\tauto __a = _mm_unpacklo_epi8(__i0, __i1); // 08.. 19.. 2A.. 3B..\n+\t\tauto __b = _mm_unpackhi_epi8(__i0, __i1); // 4C.. 5D.. 6E.. 7F..\n+\t\tauto __c = _mm_unpacklo_epi8(__a, __b);   // 048C .... 159D ....\n+\t\tauto __d = _mm_unpackhi_epi8(__a, __b);   // 26AE .... 37BF ....\n+\t\tauto __e = _mm_unpacklo_epi8(__c, __d);   // 0246 8ACE .... ....\n+\t\tauto __f = _mm_unpackhi_epi8(__c, __d);   // 1357 9BDF .... ....\n+\t\treturn __intrin_bitcast<_To>(_mm_unpacklo_epi8(__e, __f));\n+\t      }\n+\t    else if constexpr (__y_to_y)\n+\t      {\n+\t\treturn __intrin_bitcast<_To>(__xzyw(_mm256_shuffle_epi8(\n+\t\t  (__to_intrin(__v0) & _mm256_set1_epi32(0x00ff00ff))\n+\t\t    | _mm256_slli_epi16(__i1, 8),\n+\t\t  _mm256_setr_epi8(0, 2, 4, 6, 8, 10, 12, 14, 1, 3, 5, 7, 9, 11,\n+\t\t\t\t   13, 15, 0, 2, 4, 6, 8, 10, 12, 14, 1, 3, 5,\n+\t\t\t\t   7, 9, 11, 13, 15))));\n+\t      } // __z_to_z uses concat fallback\n+\t  }\n+\telse if constexpr (__i64_to_f32) //{{{2\n+\t  {\n+\t    if constexpr (__x_to_x)\n+\t      return __make_wrapper<float>(__v0[0], __v0[1], __v1[0], __v1[1]);\n+\t    else if constexpr (__y_to_y)\n+\t      {\n+\t\tstatic_assert(__y_to_y && __have_avx2);\n+\t\tconst auto __a = _mm256_unpacklo_epi32(__i0, __i1); // aeAE cgCG\n+\t\tconst auto __b = _mm256_unpackhi_epi32(__i0, __i1); // bfBF dhDH\n+\t\tconst auto __lo32\n+\t\t  = _mm256_unpacklo_epi32(__a, __b); // abef cdgh\n+\t\tconst auto __hi32 = __vector_bitcast<\n+\t\t  conditional_t<is_signed_v<_Tp>, int, _UInt>>(\n+\t\t  _mm256_unpackhi_epi32(__a, __b)); // ABEF CDGH\n+\t\tconst auto __hi\n+\t\t  = 0x100000000LL\n+\t\t    * __convert_x86<__vector_type_t<float, 8>>(__hi32);\n+\t\tconst auto __mid\n+\t\t  = 0x10000 * _mm256_cvtepi32_ps(_mm256_srli_epi32(__lo32, 16));\n+\t\tconst auto __lo\n+\t\t  = _mm256_cvtepi32_ps(_mm256_set1_epi32(0x0000ffffu) & __lo32);\n+\t\treturn __xzyw((__hi + __mid) + __lo);\n+\t      }\n+\t    else if constexpr (__z_to_z && __have_avx512dq)\n+\t      {\n+\t\treturn is_signed_v<_Tp> ? __concat(_mm512_cvtepi64_ps(__i0),\n+\t\t\t\t\t\t   _mm512_cvtepi64_ps(__i1))\n+\t\t\t\t\t: __concat(_mm512_cvtepu64_ps(__i0),\n+\t\t\t\t\t\t   _mm512_cvtepu64_ps(__i1));\n+\t      }\n+\t    else if constexpr (__z_to_z && is_signed_v<_Tp>)\n+\t      {\n+\t\tconst __m512 __hi32 = _mm512_cvtepi32_ps(\n+\t\t  __concat(_mm512_cvtepi64_epi32(__to_intrin(__v0 >> 32)),\n+\t\t\t   _mm512_cvtepi64_epi32(__to_intrin(__v1 >> 32))));\n+\t\tconst __m512i __lo32 = __concat(_mm512_cvtepi64_epi32(__i0),\n+\t\t\t\t\t\t_mm512_cvtepi64_epi32(__i1));\n+\t\t// split low 32-bits, because if __hi32 is a small negative\n+\t\t// number, the 24-bit mantissa may lose important information if\n+\t\t// any of the high 8 bits of __lo32 is set, leading to\n+\t\t// catastrophic cancelation in the FMA\n+\t\tconst __m512 __hi16\n+\t\t  = _mm512_cvtepu32_ps(_mm512_set1_epi32(0xffff0000u) & __lo32);\n+\t\tconst __m512 __lo16\n+\t\t  = _mm512_cvtepi32_ps(_mm512_set1_epi32(0x0000ffffu) & __lo32);\n+\t\treturn (__hi32 * 0x100000000LL + __hi16) + __lo16;\n+\t      }\n+\t    else if constexpr (__z_to_z && is_unsigned_v<_Tp>)\n+\t      {\n+\t\treturn __intrin_bitcast<_To>(\n+\t\t  _mm512_cvtepu32_ps(__concat(\n+\t\t    _mm512_cvtepi64_epi32(_mm512_srai_epi64(__i0, 32)),\n+\t\t    _mm512_cvtepi64_epi32(_mm512_srai_epi64(__i1, 32))))\n+\t\t    * 0x100000000LL\n+\t\t  + _mm512_cvtepu32_ps(__concat(_mm512_cvtepi64_epi32(__i0),\n+\t\t\t\t\t\t_mm512_cvtepi64_epi32(__i1))));\n+\t      }\n+\t  }\n+\telse if constexpr (__f64_to_s32) //{{{2\n+\t  {\n+\t    // use concat fallback\n+\t  }\n+\telse if constexpr (__f64_to_u32) //{{{2\n+\t  {\n+\t    if constexpr (__x_to_x && __have_sse4_1)\n+\t      {\n+\t\treturn __vector_bitcast<_Up, _M>(_mm_unpacklo_epi64(\n+\t\t\t _mm_cvttpd_epi32(_mm_floor_pd(__i0) - 0x8000'0000u),\n+\t\t\t _mm_cvttpd_epi32(_mm_floor_pd(__i1) - 0x8000'0000u)))\n+\t\t       ^ 0x8000'0000u;\n+\t\t// without SSE4.1 just use the scalar fallback, it's only four\n+\t\t// values\n+\t      }\n+\t    else if constexpr (__y_to_y)\n+\t      {\n+\t\treturn __vector_bitcast<_Up>(\n+\t\t\t __concat(_mm256_cvttpd_epi32(_mm256_floor_pd(__i0)\n+\t\t\t\t\t\t      - 0x8000'0000u),\n+\t\t\t\t  _mm256_cvttpd_epi32(_mm256_floor_pd(__i1)\n+\t\t\t\t\t\t      - 0x8000'0000u)))\n+\t\t       ^ 0x8000'0000u;\n+\t      } // __z_to_z uses fallback\n+\t  }\n+\telse if constexpr (__f64_to_ibw) //{{{2\n+\t  {\n+\t    // one-arg __f64_to_ibw goes via _SimdWrapper<int, ?>. The fallback\n+\t    // would go via two independet conversions to _SimdWrapper<_To> and\n+\t    // subsequent interleaving. This is better, because f64->__i32\n+\t    // allows to combine __v0 and __v1 into one register: if constexpr\n+\t    // (__z_to_x || __y_to_x) {\n+\t    return __convert_x86<_To>(\n+\t      __convert_x86<__vector_type_t<int, _Np * 2>>(__v0, __v1));\n+\t    //}\n+\t  }\n+\telse if constexpr (__f32_to_ibw) //{{{2\n+\t  {\n+\t    return __convert_x86<_To>(\n+\t      __convert_x86<__vector_type_t<int, _Np>>(__v0),\n+\t      __convert_x86<__vector_type_t<int, _Np>>(__v1));\n+\t  } //}}}\n+\n+\t// fallback: {{{2\n+\tif constexpr (sizeof(_To) >= 32)\n+\t  // if _To is ymm or zmm, then _SimdWrapper<_Up, _M / 2> is xmm or ymm\n+\t  return __concat(__convert_x86<__vector_type_t<_Up, _M / 2>>(__v0),\n+\t\t\t  __convert_x86<__vector_type_t<_Up, _M / 2>>(__v1));\n+\telse if constexpr (sizeof(_To) == 16)\n+\t  {\n+\t    const auto __lo = __to_intrin(__convert_x86<_To>(__v0));\n+\t    const auto __hi = __to_intrin(__convert_x86<_To>(__v1));\n+\t    if constexpr (sizeof(_Up) * _Np == 8)\n+\t      {\n+\t\tif constexpr (is_floating_point_v<_Up>)\n+\t\t  return __auto_bitcast(\n+\t\t    _mm_unpacklo_pd(__vector_bitcast<double>(__lo),\n+\t\t\t\t    __vector_bitcast<double>(__hi)));\n+\t\telse\n+\t\t  return __intrin_bitcast<_To>(_mm_unpacklo_epi64(__lo, __hi));\n+\t      }\n+\t    else if constexpr (sizeof(_Up) * _Np == 4)\n+\t      {\n+\t\tif constexpr (is_floating_point_v<_Up>)\n+\t\t  return __auto_bitcast(\n+\t\t    _mm_unpacklo_ps(__vector_bitcast<float>(__lo),\n+\t\t\t\t    __vector_bitcast<float>(__hi)));\n+\t\telse\n+\t\t  return __intrin_bitcast<_To>(_mm_unpacklo_epi32(__lo, __hi));\n+\t      }\n+\t    else if constexpr (sizeof(_Up) * _Np == 2)\n+\t      return __intrin_bitcast<_To>(_mm_unpacklo_epi16(__lo, __hi));\n+\t    else\n+\t      __assert_unreachable<_Tp>();\n+\t  }\n+\telse\n+\t  return __vector_convert<_To>(__v0, __v1, make_index_sequence<_Np>());\n+\t//}}}\n+      }\n+  }\n+\n+//}}}1\n+// 4-arg __convert_x86 {{{1\n+template <typename _To, typename _V, typename _Traits>\n+  _GLIBCXX_SIMD_INTRINSIC _To\n+  __convert_x86(_V __v0, _V __v1, _V __v2, _V __v3)\n+  {\n+    static_assert(__is_vector_type_v<_V>);\n+    using _Tp = typename _Traits::value_type;\n+    constexpr size_t _Np = _Traits::_S_full_size;\n+    [[maybe_unused]] const auto __i0 = __to_intrin(__v0);\n+    [[maybe_unused]] const auto __i1 = __to_intrin(__v1);\n+    [[maybe_unused]] const auto __i2 = __to_intrin(__v2);\n+    [[maybe_unused]] const auto __i3 = __to_intrin(__v3);\n+    using _Up = typename _VectorTraits<_To>::value_type;\n+    constexpr size_t _M = _VectorTraits<_To>::_S_full_size;\n+\n+    static_assert(4 * _Np <= _M,\n+\t\t  \"__v2/__v3 would be discarded; use the two/one-argument \"\n+\t\t  \"__convert_x86 overload instead\");\n+\n+    // [xyz]_to_[xyz] {{{2\n+    [[maybe_unused]] constexpr bool __x_to_x\n+      = sizeof(__v0) <= 16 && sizeof(_To) <= 16;\n+    [[maybe_unused]] constexpr bool __x_to_y\n+      = sizeof(__v0) <= 16 && sizeof(_To) == 32;\n+    [[maybe_unused]] constexpr bool __x_to_z\n+      = sizeof(__v0) <= 16 && sizeof(_To) == 64;\n+    [[maybe_unused]] constexpr bool __y_to_x\n+      = sizeof(__v0) == 32 && sizeof(_To) <= 16;\n+    [[maybe_unused]] constexpr bool __y_to_y\n+      = sizeof(__v0) == 32 && sizeof(_To) == 32;\n+    [[maybe_unused]] constexpr bool __y_to_z\n+      = sizeof(__v0) == 32 && sizeof(_To) == 64;\n+    [[maybe_unused]] constexpr bool __z_to_x\n+      = sizeof(__v0) == 64 && sizeof(_To) <= 16;\n+    [[maybe_unused]] constexpr bool __z_to_y\n+      = sizeof(__v0) == 64 && sizeof(_To) == 32;\n+    [[maybe_unused]] constexpr bool __z_to_z\n+      = sizeof(__v0) == 64 && sizeof(_To) == 64;\n+\n+    // iX_to_iX {{{2\n+    [[maybe_unused]] constexpr bool __i_to_i\n+      = is_integral_v<_Up> && is_integral_v<_Tp>;\n+    [[maybe_unused]] constexpr bool __i8_to_i16\n+      = __i_to_i && sizeof(_Tp) == 1 && sizeof(_Up) == 2;\n+    [[maybe_unused]] constexpr bool __i8_to_i32\n+      = __i_to_i && sizeof(_Tp) == 1 && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __i8_to_i64\n+      = __i_to_i && sizeof(_Tp) == 1 && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __i16_to_i8\n+      = __i_to_i && sizeof(_Tp) == 2 && sizeof(_Up) == 1;\n+    [[maybe_unused]] constexpr bool __i16_to_i32\n+      = __i_to_i && sizeof(_Tp) == 2 && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __i16_to_i64\n+      = __i_to_i && sizeof(_Tp) == 2 && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __i32_to_i8\n+      = __i_to_i && sizeof(_Tp) == 4 && sizeof(_Up) == 1;\n+    [[maybe_unused]] constexpr bool __i32_to_i16\n+      = __i_to_i && sizeof(_Tp) == 4 && sizeof(_Up) == 2;\n+    [[maybe_unused]] constexpr bool __i32_to_i64\n+      = __i_to_i && sizeof(_Tp) == 4 && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __i64_to_i8\n+      = __i_to_i && sizeof(_Tp) == 8 && sizeof(_Up) == 1;\n+    [[maybe_unused]] constexpr bool __i64_to_i16\n+      = __i_to_i && sizeof(_Tp) == 8 && sizeof(_Up) == 2;\n+    [[maybe_unused]] constexpr bool __i64_to_i32\n+      = __i_to_i && sizeof(_Tp) == 8 && sizeof(_Up) == 4;\n+\n+    // [fsu]X_to_[fsu]X {{{2\n+    // ibw = integral && byte or word, i.e. char and short with any signedness\n+    [[maybe_unused]] constexpr bool __i64_to_f32\n+      = is_integral_v<_Tp> && sizeof(_Tp) == 8\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __s32_to_f32\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __s16_to_f32\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 2\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __s8_to_f32\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 1\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __u32_to_f32\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __u16_to_f32\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 2\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __u8_to_f32\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 1\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+    [[maybe_unused]] constexpr bool __s64_to_f64\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 8\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __s32_to_f64\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __s16_to_f64\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 2\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __s8_to_f64\n+      = is_integral_v<_Tp> && is_signed_v<_Tp> && sizeof(_Tp) == 1\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __u64_to_f64\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 8\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __u32_to_f64\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __u16_to_f64\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 2\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __u8_to_f64\n+      = is_integral_v<_Tp> && is_unsigned_v<_Tp> && sizeof(_Tp) == 1\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __f32_to_s64\n+      = is_integral_v<_Up> && is_signed_v<_Up> && sizeof(_Up) == 8\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f32_to_s32\n+      = is_integral_v<_Up> && is_signed_v<_Up> && sizeof(_Up) == 4\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f32_to_u64\n+      = is_integral_v<_Up> && is_unsigned_v<_Up> && sizeof(_Up) == 8\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f32_to_u32\n+      = is_integral_v<_Up> && is_unsigned_v<_Up> && sizeof(_Up) == 4\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f64_to_s64\n+      = is_integral_v<_Up> && is_signed_v<_Up> && sizeof(_Up) == 8\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __f64_to_s32\n+      = is_integral_v<_Up> && is_signed_v<_Up> && sizeof(_Up) == 4\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __f64_to_u64\n+      = is_integral_v<_Up> && is_unsigned_v<_Up> && sizeof(_Up) == 8\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __f64_to_u32\n+      = is_integral_v<_Up> && is_unsigned_v<_Up> && sizeof(_Up) == 4\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __f32_to_ibw\n+      = is_integral_v<_Up> && sizeof(_Up) <= 2\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 4;\n+    [[maybe_unused]] constexpr bool __f64_to_ibw\n+      = is_integral_v<_Up> && sizeof(_Up) <= 2\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+    [[maybe_unused]] constexpr bool __f32_to_f64\n+      = is_floating_point_v<_Tp> && sizeof(_Tp) == 4\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 8;\n+    [[maybe_unused]] constexpr bool __f64_to_f32\n+      = is_floating_point_v<_Tp> && sizeof(_Tp) == 8\n+\t&& is_floating_point_v<_Up> && sizeof(_Up) == 4;\n+\n+    if constexpr (__i_to_i && __y_to_x && !__have_avx2) //{{{2\n+      {\n+\t// <double, 4>, <double, 4>, <double, 4>, <double, 4> => <char, 16>\n+\treturn __convert_x86<_To>(__lo128(__v0), __hi128(__v0), __lo128(__v1),\n+\t\t\t\t  __hi128(__v1), __lo128(__v2), __hi128(__v2),\n+\t\t\t\t  __lo128(__v3), __hi128(__v3));\n+      }\n+    else if constexpr (__i_to_i) // assert ISA {{{2\n+      {\n+\tstatic_assert(__x_to_x || __have_avx2,\n+\t\t      \"integral conversions with ymm registers require AVX2\");\n+\tstatic_assert(__have_avx512bw\n+\t\t\t|| ((sizeof(_Tp) >= 4 || sizeof(__v0) < 64)\n+\t\t\t    && (sizeof(_Up) >= 4 || sizeof(_To) < 64)),\n+\t\t      \"8/16-bit integers in zmm registers require AVX512BW\");\n+\tstatic_assert((sizeof(__v0) < 64 && sizeof(_To) < 64) || __have_avx512f,\n+\t\t      \"integral conversions with ymm registers require AVX2\");\n+      }\n+    // concat => use 2-arg __convert_x86 {{{2\n+    if constexpr (sizeof(__v0) < 16 || (sizeof(__v0) == 16 && __have_avx2)\n+\t\t  || (sizeof(__v0) == 16 && __have_avx\n+\t\t      && is_floating_point_v<_Tp>)\n+\t\t  || (sizeof(__v0) == 32 && __have_avx512f))\n+      {\n+\t// The ISA can handle wider input registers, so concat and use two-arg\n+\t// implementation. This reduces code duplication considerably.\n+\treturn __convert_x86<_To>(__concat(__v0, __v1), __concat(__v2, __v3));\n+      }\n+    else //{{{2\n+      {\n+\t// conversion using bit reinterpretation (or no conversion at all)\n+\t// should all go through the concat branch above:\n+\tstatic_assert(\n+\t  !(is_floating_point_v<\n+\t      _Tp> == is_floating_point_v<_Up> && sizeof(_Tp) == sizeof(_Up)));\n+\t// handle all zero extension{{{2\n+\tif constexpr (4 * _Np < _M && sizeof(_To) > 16)\n+\t  {\n+\t    constexpr size_t Min = 16 / sizeof(_Up);\n+\t    return __zero_extend(\n+\t      __convert_x86<\n+\t\t__vector_type_t<_Up, (Min > 4 * _Np) ? Min : 4 * _Np>>(\n+\t\t__v0, __v1, __v2, __v3));\n+\t  }\n+\telse if constexpr (__i64_to_i16) //{{{2\n+\t  {\n+\t    if constexpr (__x_to_x && __have_sse4_1)\n+\t      {\n+\t\treturn __intrin_bitcast<_To>(_mm_shuffle_epi8(\n+\t\t  _mm_blend_epi16(\n+\t\t    _mm_blend_epi16(__i0, _mm_slli_si128(__i1, 2), 0x22),\n+\t\t    _mm_blend_epi16(_mm_slli_si128(__i2, 4),\n+\t\t\t\t    _mm_slli_si128(__i3, 6), 0x88),\n+\t\t    0xcc),\n+\t\t  _mm_setr_epi8(0, 1, 8, 9, 2, 3, 10, 11, 4, 5, 12, 13, 6, 7,\n+\t\t\t\t14, 15)));\n+\t      }\n+\t    else if constexpr (__y_to_y && __have_avx2)\n+\t      {\n+\t\treturn __intrin_bitcast<_To>(_mm256_shuffle_epi8(\n+\t\t  __xzyw(_mm256_blend_epi16(\n+\t\t    __auto_bitcast(\n+\t\t      _mm256_shuffle_ps(__vector_bitcast<float>(__v0),\n+\t\t\t\t\t__vector_bitcast<float>(__v2),\n+\t\t\t\t\t0x88)), // 0.1. 8.9. 2.3. A.B.\n+\t\t    __to_intrin(__vector_bitcast<int>(_mm256_shuffle_ps(\n+\t\t\t\t  __vector_bitcast<float>(__v1),\n+\t\t\t\t  __vector_bitcast<float>(__v3), 0x88))\n+\t\t\t\t<< 16), // .4.5 .C.D .6.7 .E.F\n+\t\t    0xaa)               // 0415 8C9D 2637 AEBF\n+\t\t\t ),             // 0415 2637 8C9D AEBF\n+\t\t  _mm256_setr_epi8(0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7, 10, 11,\n+\t\t\t\t   14, 15, 0, 1, 4, 5, 8, 9, 12, 13, 2, 3, 6, 7,\n+\t\t\t\t   10, 11, 14, 15)));\n+\t\t/*\n+\t\tauto __a = _mm256_unpacklo_epi16(__v0, __v1);  // 04.. .... 26..\n+\t\t.... auto __b = _mm256_unpackhi_epi16(__v0, __v1);  // 15..\n+\t\t.... 37.. .... auto __c = _mm256_unpacklo_epi16(__v2, __v3);  //\n+\t\t8C.. .... AE.. .... auto __d = _mm256_unpackhi_epi16(__v2,\n+\t\t__v3);\n+\t\t// 9D.. .... BF.. .... auto __e = _mm256_unpacklo_epi16(__a,\n+\t\t__b);\n+\t\t// 0145 .... 2367 .... auto __f = _mm256_unpacklo_epi16(__c,\n+\t\t__d);\n+\t\t// 89CD .... ABEF .... auto __g = _mm256_unpacklo_epi64(__e,\n+\t\t__f);\n+\t\t// 0145 89CD 2367 ABEF return __concat(\n+\t\t    _mm_unpacklo_epi32(__lo128(__g), __hi128(__g)),\n+\t\t    _mm_unpackhi_epi32(__lo128(__g), __hi128(__g)));  // 0123\n+\t\t4567 89AB CDEF\n+\t\t    */\n+\t      } // else use fallback\n+\t  }\n+\telse if constexpr (__i64_to_i8) //{{{2\n+\t  {\n+\t    if constexpr (__x_to_x)\n+\t      {\n+\t\t// TODO: use fallback for now\n+\t      }\n+\t    else if constexpr (__y_to_x)\n+\t      {\n+\t\tauto __a\n+\t\t  = _mm256_srli_epi32(_mm256_slli_epi32(__i0, 24), 24)\n+\t\t    | _mm256_srli_epi32(_mm256_slli_epi32(__i1, 24), 16)\n+\t\t    | _mm256_srli_epi32(_mm256_slli_epi32(__i2, 24), 8)\n+\t\t    | _mm256_slli_epi32(\n+\t\t      __i3, 24); // 048C .... 159D .... 26AE .... 37BF ....\n+\t\t/*return _mm_shuffle_epi8(\n+\t\t    _mm_blend_epi32(__lo128(__a) << 32, __hi128(__a), 0x5),\n+\t\t    _mm_setr_epi8(4, 12, 0, 8, 5, 13, 1, 9, 6, 14, 2, 10, 7, 15,\n+\t\t   3, 11));*/\n+\t\tauto __b = _mm256_unpackhi_epi64(\n+\t\t  __a, __a); // 159D .... 159D .... 37BF .... 37BF ....\n+\t\tauto __c = _mm256_unpacklo_epi8(\n+\t\t  __a, __b); // 0145 89CD .... .... 2367 ABEF .... ....\n+\t\treturn __intrin_bitcast<_To>(\n+\t\t  _mm_unpacklo_epi16(__lo128(__c),\n+\t\t\t\t     __hi128(__c))); // 0123 4567 89AB CDEF\n+\t      }\n+\t  }\n+\telse if constexpr (__i32_to_i8) //{{{2\n+\t  {\n+\t    if constexpr (__x_to_x)\n+\t      {\n+\t\tif constexpr (__have_ssse3)\n+\t\t  {\n+\t\t    const auto __x0 = __vector_bitcast<_UInt>(__v0) & 0xff;\n+\t\t    const auto __x1 = (__vector_bitcast<_UInt>(__v1) & 0xff)\n+\t\t\t\t      << 8;\n+\t\t    const auto __x2 = (__vector_bitcast<_UInt>(__v2) & 0xff)\n+\t\t\t\t      << 16;\n+\t\t    const auto __x3 = __vector_bitcast<_UInt>(__v3) << 24;\n+\t\t    return __intrin_bitcast<_To>(\n+\t\t      _mm_shuffle_epi8(__to_intrin(__x0 | __x1 | __x2 | __x3),\n+\t\t\t\t       _mm_setr_epi8(0, 4, 8, 12, 1, 5, 9, 13,\n+\t\t\t\t\t\t     2, 6, 10, 14, 3, 7, 11,\n+\t\t\t\t\t\t     15)));\n+\t\t  }\n+\t\telse\n+\t\t  {\n+\t\t    auto __a\n+\t\t      = _mm_unpacklo_epi8(__i0, __i2); // 08.. .... 19.. ....\n+\t\t    auto __b\n+\t\t      = _mm_unpackhi_epi8(__i0, __i2); // 2A.. .... 3B.. ....\n+\t\t    auto __c\n+\t\t      = _mm_unpacklo_epi8(__i1, __i3); // 4C.. .... 5D.. ....\n+\t\t    auto __d\n+\t\t      = _mm_unpackhi_epi8(__i1, __i3); // 6E.. .... 7F.. ....\n+\t\t    auto __e\n+\t\t      = _mm_unpacklo_epi8(__a, __c); // 048C .... .... ....\n+\t\t    auto __f\n+\t\t      = _mm_unpackhi_epi8(__a, __c); // 159D .... .... ....\n+\t\t    auto __g\n+\t\t      = _mm_unpacklo_epi8(__b, __d); // 26AE .... .... ....\n+\t\t    auto __h\n+\t\t      = _mm_unpackhi_epi8(__b, __d); // 37BF .... .... ....\n+\t\t    return __intrin_bitcast<_To>(_mm_unpacklo_epi8(\n+\t\t      _mm_unpacklo_epi8(__e, __g), // 0246 8ACE .... ....\n+\t\t      _mm_unpacklo_epi8(__f, __h)  // 1357 9BDF .... ....\n+\t\t      ));                          // 0123 4567 89AB CDEF\n+\t\t  }\n+\t      }\n+\t    else if constexpr (__y_to_y)\n+\t      {\n+\t\tconst auto __a = _mm256_shuffle_epi8(\n+\t\t  __to_intrin((__vector_bitcast<_UShort>(_mm256_blend_epi16(\n+\t\t\t\t __i0, _mm256_slli_epi32(__i1, 16), 0xAA))\n+\t\t\t       & 0xff)\n+\t\t\t      | (__vector_bitcast<_UShort>(_mm256_blend_epi16(\n+\t\t\t\t   __i2, _mm256_slli_epi32(__i3, 16), 0xAA))\n+\t\t\t\t << 8)),\n+\t\t  _mm256_setr_epi8(0, 4, 8, 12, 2, 6, 10, 14, 1, 5, 9, 13, 3, 7,\n+\t\t\t\t   11, 15, 0, 4, 8, 12, 2, 6, 10, 14, 1, 5, 9,\n+\t\t\t\t   13, 3, 7, 11, 15));\n+\t\treturn __intrin_bitcast<_To>(_mm256_permutevar8x32_epi32(\n+\t\t  __a, _mm256_setr_epi32(0, 4, 1, 5, 2, 6, 3, 7)));\n+\t      }\n+\t  }\n+\telse if constexpr (__i64_to_f32) //{{{2\n+\t  {\n+\t    // this branch is only relevant with AVX and w/o AVX2 (i.e. no ymm\n+\t    // integers)\n+\t    if constexpr (__x_to_y)\n+\t      {\n+\t\treturn __make_wrapper<float>(__v0[0], __v0[1], __v1[0], __v1[1],\n+\t\t\t\t\t     __v2[0], __v2[1], __v3[0],\n+\t\t\t\t\t     __v3[1]);\n+\n+\t\tconst auto __a = _mm_unpacklo_epi32(__i0, __i1);   // acAC\n+\t\tconst auto __b = _mm_unpackhi_epi32(__i0, __i1);   // bdBD\n+\t\tconst auto __c = _mm_unpacklo_epi32(__i2, __i3);   // egEG\n+\t\tconst auto __d = _mm_unpackhi_epi32(__i2, __i3);   // fhFH\n+\t\tconst auto __lo32a = _mm_unpacklo_epi32(__a, __b); // abcd\n+\t\tconst auto __lo32b = _mm_unpacklo_epi32(__c, __d); // efgh\n+\t\tconst auto __hi32 = __vector_bitcast<\n+\t\t  conditional_t<is_signed_v<_Tp>, int, _UInt>>(\n+\t\t  __concat(_mm_unpackhi_epi32(__a, __b),\n+\t\t\t   _mm_unpackhi_epi32(__c, __d))); // ABCD EFGH\n+\t\tconst auto __hi\n+\t\t  = 0x100000000LL\n+\t\t    * __convert_x86<__vector_type_t<float, 8>>(__hi32);\n+\t\tconst auto __mid\n+\t\t  = 0x10000\n+\t\t    * _mm256_cvtepi32_ps(__concat(_mm_srli_epi32(__lo32a, 16),\n+\t\t\t\t\t\t  _mm_srli_epi32(__lo32b, 16)));\n+\t\tconst auto __lo = _mm256_cvtepi32_ps(\n+\t\t  __concat(_mm_set1_epi32(0x0000ffffu) & __lo32a,\n+\t\t\t   _mm_set1_epi32(0x0000ffffu) & __lo32b));\n+\t\treturn (__hi + __mid) + __lo;\n+\t      }\n+\t  }\n+\telse if constexpr (__f64_to_ibw) //{{{2\n+\t  {\n+\t    return __convert_x86<_To>(\n+\t      __convert_x86<__vector_type_t<int, _Np * 2>>(__v0, __v1),\n+\t      __convert_x86<__vector_type_t<int, _Np * 2>>(__v2, __v3));\n+\t  }\n+\telse if constexpr (__f32_to_ibw) //{{{2\n+\t  {\n+\t    return __convert_x86<_To>(\n+\t      __convert_x86<__vector_type_t<int, _Np>>(__v0),\n+\t      __convert_x86<__vector_type_t<int, _Np>>(__v1),\n+\t      __convert_x86<__vector_type_t<int, _Np>>(__v2),\n+\t      __convert_x86<__vector_type_t<int, _Np>>(__v3));\n+\t  } //}}}\n+\n+\t// fallback: {{{2\n+\tif constexpr (sizeof(_To) >= 32)\n+\t  // if _To is ymm or zmm, then _SimdWrapper<_Up, _M / 2> is xmm or ymm\n+\t  return __concat(__convert_x86<__vector_type_t<_Up, _M / 2>>(__v0,\n+\t\t\t\t\t\t\t\t      __v1),\n+\t\t\t  __convert_x86<__vector_type_t<_Up, _M / 2>>(__v2,\n+\t\t\t\t\t\t\t\t      __v3));\n+\telse if constexpr (sizeof(_To) == 16)\n+\t  {\n+\t    const auto __lo = __to_intrin(__convert_x86<_To>(__v0, __v1));\n+\t    const auto __hi = __to_intrin(__convert_x86<_To>(__v2, __v3));\n+\t    if constexpr (sizeof(_Up) * _Np * 2 == 8)\n+\t      {\n+\t\tif constexpr (is_floating_point_v<_Up>)\n+\t\t  return __auto_bitcast(_mm_unpacklo_pd(__lo, __hi));\n+\t\telse\n+\t\t  return __intrin_bitcast<_To>(_mm_unpacklo_epi64(__lo, __hi));\n+\t      }\n+\t    else if constexpr (sizeof(_Up) * _Np * 2 == 4)\n+\t      {\n+\t\tif constexpr (is_floating_point_v<_Up>)\n+\t\t  return __auto_bitcast(_mm_unpacklo_ps(__lo, __hi));\n+\t\telse\n+\t\t  return __intrin_bitcast<_To>(_mm_unpacklo_epi32(__lo, __hi));\n+\t      }\n+\t    else\n+\t      __assert_unreachable<_Tp>();\n+\t  }\n+\telse\n+\t  return __vector_convert<_To>(__v0, __v1, __v2, __v3,\n+\t\t\t\t       make_index_sequence<_Np>());\n+\t//}}}2\n+      }\n+  }\n+\n+//}}}\n+// 8-arg __convert_x86 {{{1\n+template <typename _To, typename _V, typename _Traits>\n+  _GLIBCXX_SIMD_INTRINSIC _To\n+  __convert_x86(_V __v0, _V __v1, _V __v2, _V __v3, _V __v4, _V __v5, _V __v6,\n+\t\t_V __v7)\n+  {\n+    static_assert(__is_vector_type_v<_V>);\n+    using _Tp = typename _Traits::value_type;\n+    constexpr size_t _Np = _Traits::_S_full_size;\n+    [[maybe_unused]] const auto __i0 = __to_intrin(__v0);\n+    [[maybe_unused]] const auto __i1 = __to_intrin(__v1);\n+    [[maybe_unused]] const auto __i2 = __to_intrin(__v2);\n+    [[maybe_unused]] const auto __i3 = __to_intrin(__v3);\n+    [[maybe_unused]] const auto __i4 = __to_intrin(__v4);\n+    [[maybe_unused]] const auto __i5 = __to_intrin(__v5);\n+    [[maybe_unused]] const auto __i6 = __to_intrin(__v6);\n+    [[maybe_unused]] const auto __i7 = __to_intrin(__v7);\n+    using _Up = typename _VectorTraits<_To>::value_type;\n+    constexpr size_t _M = _VectorTraits<_To>::_S_full_size;\n+\n+    static_assert(8 * _Np <= _M,\n+\t\t  \"__v4-__v7 would be discarded; use the four/two/one-argument \"\n+\t\t  \"__convert_x86 overload instead\");\n+\n+    // [xyz]_to_[xyz] {{{2\n+    [[maybe_unused]] constexpr bool __x_to_x\n+      = sizeof(__v0) <= 16 && sizeof(_To) <= 16;\n+    [[maybe_unused]] constexpr bool __x_to_y\n+      = sizeof(__v0) <= 16 && sizeof(_To) == 32;\n+    [[maybe_unused]] constexpr bool __x_to_z\n+      = sizeof(__v0) <= 16 && sizeof(_To) == 64;\n+    [[maybe_unused]] constexpr bool __y_to_x\n+      = sizeof(__v0) == 32 && sizeof(_To) <= 16;\n+    [[maybe_unused]] constexpr bool __y_to_y\n+      = sizeof(__v0) == 32 && sizeof(_To) == 32;\n+    [[maybe_unused]] constexpr bool __y_to_z\n+      = sizeof(__v0) == 32 && sizeof(_To) == 64;\n+    [[maybe_unused]] constexpr bool __z_to_x\n+      = sizeof(__v0) == 64 && sizeof(_To) <= 16;\n+    [[maybe_unused]] constexpr bool __z_to_y\n+      = sizeof(__v0) == 64 && sizeof(_To) == 32;\n+    [[maybe_unused]] constexpr bool __z_to_z\n+      = sizeof(__v0) == 64 && sizeof(_To) == 64;\n+\n+    // [if]X_to_i8 {{{2\n+    [[maybe_unused]] constexpr bool __i_to_i\n+      = is_integral_v<_Up> && is_integral_v<_Tp>;\n+    [[maybe_unused]] constexpr bool __i64_to_i8\n+      = __i_to_i && sizeof(_Tp) == 8 && sizeof(_Up) == 1;\n+    [[maybe_unused]] constexpr bool __f64_to_i8\n+      = is_integral_v<_Up> && sizeof(_Up) == 1\n+\t&& is_floating_point_v<_Tp> && sizeof(_Tp) == 8;\n+\n+    if constexpr (__i_to_i) // assert ISA {{{2\n+      {\n+\tstatic_assert(__x_to_x || __have_avx2,\n+\t\t      \"integral conversions with ymm registers require AVX2\");\n+\tstatic_assert(__have_avx512bw\n+\t\t\t|| ((sizeof(_Tp) >= 4 || sizeof(__v0) < 64)\n+\t\t\t    && (sizeof(_Up) >= 4 || sizeof(_To) < 64)),\n+\t\t      \"8/16-bit integers in zmm registers require AVX512BW\");\n+\tstatic_assert((sizeof(__v0) < 64 && sizeof(_To) < 64) || __have_avx512f,\n+\t\t      \"integral conversions with ymm registers require AVX2\");\n+      }\n+    // concat => use 4-arg __convert_x86 {{{2\n+    if constexpr (sizeof(__v0) < 16 || (sizeof(__v0) == 16 && __have_avx2)\n+\t\t  || (sizeof(__v0) == 16 && __have_avx\n+\t\t      && is_floating_point_v<_Tp>)\n+\t\t  || (sizeof(__v0) == 32 && __have_avx512f))\n+      {\n+\t// The ISA can handle wider input registers, so concat and use two-arg\n+\t// implementation. This reduces code duplication considerably.\n+\treturn __convert_x86<_To>(__concat(__v0, __v1), __concat(__v2, __v3),\n+\t\t\t\t  __concat(__v4, __v5), __concat(__v6, __v7));\n+      }\n+    else //{{{2\n+      {\n+\t// conversion using bit reinterpretation (or no conversion at all)\n+\t// should all go through the concat branch above:\n+\tstatic_assert(\n+\t  !(is_floating_point_v<\n+\t      _Tp> == is_floating_point_v<_Up> && sizeof(_Tp) == sizeof(_Up)));\n+\tstatic_assert(!(8 * _Np < _M && sizeof(_To) > 16),\n+\t\t      \"zero extension should be impossible\");\n+\tif constexpr (__i64_to_i8) //{{{2\n+\t  {\n+\t    if constexpr (__x_to_x && __have_ssse3)\n+\t      {\n+\t\t// unsure whether this is better than the variant below\n+\t\treturn __intrin_bitcast<_To>(_mm_shuffle_epi8(\n+\t\t  __to_intrin(\n+\t\t    (((__v0 & 0xff) | ((__v1 & 0xff) << 8))\n+\t\t     | (((__v2 & 0xff) << 16) | ((__v3 & 0xff) << 24)))\n+\t\t    | ((((__v4 & 0xff) << 32) | ((__v5 & 0xff) << 40))\n+\t\t       | (((__v6 & 0xff) << 48) | (__v7 << 56)))),\n+\t\t  _mm_setr_epi8(0, 8, 1, 9, 2, 10, 3, 11, 4, 12, 5, 13, 6, 14,\n+\t\t\t\t7, 15)));\n+\t      }\n+\t    else if constexpr (__x_to_x)\n+\t      {\n+\t\tconst auto __a = _mm_unpacklo_epi8(__i0, __i1); // ac\n+\t\tconst auto __b = _mm_unpackhi_epi8(__i0, __i1); // bd\n+\t\tconst auto __c = _mm_unpacklo_epi8(__i2, __i3); // eg\n+\t\tconst auto __d = _mm_unpackhi_epi8(__i2, __i3); // fh\n+\t\tconst auto __e = _mm_unpacklo_epi8(__i4, __i5); // ik\n+\t\tconst auto __f = _mm_unpackhi_epi8(__i4, __i5); // jl\n+\t\tconst auto __g = _mm_unpacklo_epi8(__i6, __i7); // mo\n+\t\tconst auto __h = _mm_unpackhi_epi8(__i6, __i7); // np\n+\t\treturn __intrin_bitcast<_To>(_mm_unpacklo_epi64(\n+\t\t  _mm_unpacklo_epi32(_mm_unpacklo_epi8(__a, __b),  // abcd\n+\t\t\t\t     _mm_unpacklo_epi8(__c, __d)), // efgh\n+\t\t  _mm_unpacklo_epi32(_mm_unpacklo_epi8(__e, __f),  // ijkl\n+\t\t\t\t     _mm_unpacklo_epi8(__g, __h))  // mnop\n+\t\t  ));\n+\t      }\n+\t    else if constexpr (__y_to_y)\n+\t      {\n+\t\tauto __a = // 048C GKOS 159D HLPT 26AE IMQU 37BF JNRV\n+\t\t  __to_intrin(\n+\t\t    (((__v0 & 0xff) | ((__v1 & 0xff) << 8))\n+\t\t     | (((__v2 & 0xff) << 16) | ((__v3 & 0xff) << 24)))\n+\t\t    | ((((__v4 & 0xff) << 32) | ((__v5 & 0xff) << 40))\n+\t\t       | (((__v6 & 0xff) << 48) | ((__v7 << 56)))));\n+\t\t/*\n+\t\tauto __b = _mm256_unpackhi_epi64(__a, __a);  // 159D HLPT 159D\n+\t\tHLPT 37BF JNRV 37BF JNRV auto __c = _mm256_unpacklo_epi8(__a,\n+\t\t__b);  // 0145 89CD GHKL OPST 2367 ABEF IJMN QRUV auto __d =\n+\t\t__xzyw(__c); // 0145 89CD 2367 ABEF GHKL OPST IJMN QRUV return\n+\t\t_mm256_shuffle_epi8(\n+\t\t    __d, _mm256_setr_epi8(0, 1, 8, 9, 2, 3, 10, 11, 4, 5, 12,\n+\t\t13, 6, 7, 14, 15, 0, 1, 8, 9, 2, 3, 10, 11, 4, 5, 12, 13, 6, 7,\n+\t\t14, 15));\n+\t\t\t\t\t*/\n+\t\tauto __b = _mm256_shuffle_epi8( // 0145 89CD GHKL OPST 2367 ABEF\n+\t\t\t\t\t\t// IJMN QRUV\n+\t\t  __a, _mm256_setr_epi8(0, 8, 1, 9, 2, 10, 3, 11, 4, 12, 5, 13,\n+\t\t\t\t\t6, 14, 7, 15, 0, 8, 1, 9, 2, 10, 3, 11,\n+\t\t\t\t\t4, 12, 5, 13, 6, 14, 7, 15));\n+\t\tauto __c\n+\t\t  = __xzyw(__b); // 0145 89CD 2367 ABEF GHKL OPST IJMN QRUV\n+\t\treturn __intrin_bitcast<_To>(_mm256_shuffle_epi8(\n+\t\t  __c, _mm256_setr_epi8(0, 1, 8, 9, 2, 3, 10, 11, 4, 5, 12, 13,\n+\t\t\t\t\t6, 7, 14, 15, 0, 1, 8, 9, 2, 3, 10, 11,\n+\t\t\t\t\t4, 5, 12, 13, 6, 7, 14, 15)));\n+\t      }\n+\t    else if constexpr (__z_to_z)\n+\t      {\n+\t\treturn __concat(\n+\t\t  __convert_x86<__vector_type_t<_Up, _M / 2>>(__v0, __v1, __v2,\n+\t\t\t\t\t\t\t      __v3),\n+\t\t  __convert_x86<__vector_type_t<_Up, _M / 2>>(__v4, __v5, __v6,\n+\t\t\t\t\t\t\t      __v7));\n+\t      }\n+\t  }\n+\telse if constexpr (__f64_to_i8) //{{{2\n+\t  {\n+\t    return __convert_x86<_To>(\n+\t      __convert_x86<__vector_type_t<int, _Np * 2>>(__v0, __v1),\n+\t      __convert_x86<__vector_type_t<int, _Np * 2>>(__v2, __v3),\n+\t      __convert_x86<__vector_type_t<int, _Np * 2>>(__v4, __v5),\n+\t      __convert_x86<__vector_type_t<int, _Np * 2>>(__v6, __v7));\n+\t  }\n+\telse // unreachable {{{2\n+\t  __assert_unreachable<_Tp>();\n+\t//}}}\n+\n+\t// fallback: {{{2\n+\tif constexpr (sizeof(_To) >= 32)\n+\t  // if _To is ymm or zmm, then _SimdWrapper<_Up, _M / 2> is xmm or ymm\n+\t  return __concat(\n+\t    __convert_x86<__vector_type_t<_Up, _M / 2>>(__v0, __v1, __v2, __v3),\n+\t    __convert_x86<__vector_type_t<_Up, _M / 2>>(__v4, __v5, __v6,\n+\t\t\t\t\t\t\t__v7));\n+\telse if constexpr (sizeof(_To) == 16)\n+\t  {\n+\t    const auto __lo\n+\t      = __to_intrin(__convert_x86<_To>(__v0, __v1, __v2, __v3));\n+\t    const auto __hi\n+\t      = __to_intrin(__convert_x86<_To>(__v4, __v5, __v6, __v7));\n+\t    static_assert(sizeof(_Up) == 1 && _Np == 2);\n+\t    return __intrin_bitcast<_To>(_mm_unpacklo_epi64(__lo, __hi));\n+\t  }\n+\telse\n+\t  {\n+\t    __assert_unreachable<_Tp>();\n+\t    // return __vector_convert<_To>(__v0, __v1, __v2, __v3, __v4, __v5,\n+\t    // __v6, __v7,\n+\t    //                             make_index_sequence<_Np>());\n+\t  } //}}}2\n+      }\n+  }\n+\n+//}}}\n+// 16-arg __convert_x86 {{{1\n+template <typename _To, typename _V, typename _Traits>\n+  _GLIBCXX_SIMD_INTRINSIC _To\n+  __convert_x86(_V __v0, _V __v1, _V __v2, _V __v3, _V __v4, _V __v5, _V __v6,\n+\t\t_V __v7, _V __v8, _V __v9, _V __v10, _V __v11, _V __v12,\n+\t\t_V __v13, _V __v14, _V __v15)\n+  {\n+    // concat => use 8-arg __convert_x86\n+    return __convert_x86<_To>(__concat(__v0, __v1), __concat(__v2, __v3),\n+\t\t\t      __concat(__v4, __v5), __concat(__v6, __v7),\n+\t\t\t      __concat(__v8, __v9), __concat(__v10, __v11),\n+\t\t\t      __concat(__v12, __v13), __concat(__v14, __v15));\n+  }\n+\n+//}}}\n+\n+#endif // __cplusplus >= 201703L\n+#endif // _GLIBCXX_EXPERIMENTAL_SIMD_X86_CONVERSIONS_H\n+\n+// vim: foldmethod=marker"}, {"sha": "43d2ba199033727a062aa8b03cc892734616fafe", "filename": "libstdc++-v3/include/experimental/simd", "status": "added", "additions": 70, "deletions": 0, "changes": 70, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fsimd", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fsimd", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fsimd?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -0,0 +1,70 @@\n+// Components for element-wise operations on data-parallel objects -*- C++ -*-\n+\n+// Copyright (C) 2020 Free Software Foundation, Inc.\n+//\n+// This file is part of the GNU ISO C++ Library.  This library is free\n+// software; you can redistribute it and/or modify it under the\n+// terms of the GNU General Public License as published by the\n+// Free Software Foundation; either version 3, or (at your option)\n+// any later version.\n+\n+// This library is distributed in the hope that it will be useful,\n+// but WITHOUT ANY WARRANTY; without even the implied warranty of\n+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+// GNU General Public License for more details.\n+\n+// Under Section 7 of GPL version 3, you are granted additional\n+// permissions described in the GCC Runtime Library Exception, version\n+// 3.1, as published by the Free Software Foundation.\n+\n+// You should have received a copy of the GNU General Public License and\n+// a copy of the GCC Runtime Library Exception along with this program;\n+// see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+// <http://www.gnu.org/licenses/>.\n+\n+/** @file experimental/simd\n+ *  This is a TS C++ Library header.\n+ */\n+\n+//\n+// N4773 \u00a79 data-parallel types library\n+//\n+\n+#ifndef _GLIBCXX_EXPERIMENTAL_SIMD\n+#define _GLIBCXX_EXPERIMENTAL_SIMD\n+\n+#define __cpp_lib_experimental_parallel_simd 201803\n+\n+#pragma GCC diagnostic push\n+// Many [[gnu::vector_size(N)]] types might lead to a -Wpsabi warning which is\n+// irrelevant as those functions never appear on ABI borders\n+#ifndef __clang__\n+#pragma GCC diagnostic ignored \"-Wpsabi\"\n+#endif\n+\n+// If __OPTIMIZE__ is not defined some intrinsics are defined as macros, making\n+// use of C casts internally. This requires us to disable the warning as it\n+// would otherwise yield many false positives.\n+#ifndef __OPTIMIZE__\n+#pragma GCC diagnostic ignored \"-Wold-style-cast\"\n+#endif\n+\n+#include \"bits/simd_detail.h\"\n+#include \"bits/simd.h\"\n+#include \"bits/simd_fixed_size.h\"\n+#include \"bits/simd_scalar.h\"\n+#include \"bits/simd_builtin.h\"\n+#include \"bits/simd_converter.h\"\n+#if _GLIBCXX_SIMD_X86INTRIN\n+#include \"bits/simd_x86.h\"\n+#elif _GLIBCXX_SIMD_HAVE_NEON\n+#include \"bits/simd_neon.h\"\n+#elif __ALTIVEC__\n+#include \"bits/simd_ppc.h\"\n+#endif\n+#include \"bits/simd_math.h\"\n+\n+#pragma GCC diagnostic pop\n+\n+#endif // _GLIBCXX_EXPERIMENTAL_SIMD\n+// vim: ft=cpp"}, {"sha": "b055eee9c04dcc504867dc8e4af0d71ec50d9beb", "filename": "libstdc++-v3/testsuite/experimental/simd/standard_abi_usable.cc", "status": "added", "additions": 64, "deletions": 0, "changes": 64, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Ftestsuite%2Fexperimental%2Fsimd%2Fstandard_abi_usable.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Ftestsuite%2Fexperimental%2Fsimd%2Fstandard_abi_usable.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Ftestsuite%2Fexperimental%2Fsimd%2Fstandard_abi_usable.cc?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -0,0 +1,64 @@\n+// { dg-options \"-std=c++17 -fno-fast-math\" }\n+// { dg-do compile { target c++17 } }\n+\n+// Copyright (C) 2020 Free Software Foundation, Inc.\n+//\n+// This file is part of the GNU ISO C++ Library.  This library is free\n+// software; you can redistribute it and/or modify it under the\n+// terms of the GNU General Public License as published by the\n+// Free Software Foundation; either version 3, or (at your option)\n+// any later version.\n+\n+// This library is distributed in the hope that it will be useful,\n+// but WITHOUT ANY WARRANTY; without even the implied warranty of\n+// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+// GNU General Public License for more details.\n+\n+// You should have received a copy of the GNU General Public License along\n+// with this library; see the file COPYING3.  If not see\n+// <http://www.gnu.org/licenses/>.\n+\n+#include <experimental/simd>\n+\n+template <typename V>\n+  void\n+  is_usable()\n+  {\n+    static_assert(std::is_default_constructible_v<V>);\n+    static_assert(std::is_destructible_v         <V>);\n+    static_assert(std::is_default_constructible_v<typename V::mask_type>);\n+    static_assert(std::is_destructible_v         <typename V::mask_type>);\n+  }\n+\n+template <typename T>\n+  void\n+  test01()\n+  {\n+    namespace stdx = std::experimental;\n+    is_usable<stdx::simd<T>>();\n+    is_usable<stdx::native_simd<T>>();\n+    is_usable<stdx::fixed_size_simd<T, 3>>();\n+    is_usable<stdx::fixed_size_simd<T, stdx::simd_abi::max_fixed_size<T>>>();\n+  }\n+\n+int main()\n+{\n+  test01<char>();\n+  test01<wchar_t>();\n+  test01<char16_t>();\n+  test01<char32_t>();\n+\n+  test01<signed char>();\n+  test01<unsigned char>();\n+  test01<short>();\n+  test01<unsigned short>();\n+  test01<int>();\n+  test01<unsigned int>();\n+  test01<long>();\n+  test01<unsigned long>();\n+  test01<long long>();\n+  test01<unsigned long long>();\n+  test01<float>();\n+  test01<double>();\n+  test01<long double>();\n+}"}, {"sha": "a609adaf000b3d089e3319621f0b1d4fe08d8b47", "filename": "libstdc++-v3/testsuite/experimental/simd/standard_abi_usable_2.cc", "status": "added", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Ftestsuite%2Fexperimental%2Fsimd%2Fstandard_abi_usable_2.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Ftestsuite%2Fexperimental%2Fsimd%2Fstandard_abi_usable_2.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Ftestsuite%2Fexperimental%2Fsimd%2Fstandard_abi_usable_2.cc?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -0,0 +1,4 @@\n+// { dg-options \"-std=c++17 -ffast-math\" }\n+// { dg-do compile }\n+\n+#include \"standard_abi_usable.cc\""}, {"sha": "4bc64abf693ca90526a3198a69e017021f335b06", "filename": "libstdc++-v3/testsuite/libstdc++-dg/conformance.exp", "status": "modified", "additions": 17, "deletions": 1, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Ftestsuite%2Flibstdc%2B%2B-dg%2Fconformance.exp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2bcceb6fc59fcdaf51006d4fcfc71c2d26761396/libstdc%2B%2B-v3%2Ftestsuite%2Flibstdc%2B%2B-dg%2Fconformance.exp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Ftestsuite%2Flibstdc%2B%2B-dg%2Fconformance.exp?ref=2bcceb6fc59fcdaf51006d4fcfc71c2d26761396", "patch": "@@ -89,12 +89,14 @@ if {[info exists tests_file] && [file exists $tests_file]} {\n \t    # 3. wchar_t tests, if not supported.\n \t    # 4. thread tests, if not supported. \n \t    # 5. *_filebuf, if file I/O is not supported.\n+\t    # 6. simd tests.\n \t    if { [string first _xin $t] == -1\n \t\t && [string first performance $t] == -1\n \t\t && (${v3-wchar_t} || [string first wchar_t $t] == -1) \n \t\t && (${v3-threads} || [string first thread $t] == -1)  \n \t\t && ([string first \"_filebuf\" $t] == -1\n-\t\t     || [check_v3_target_fileio]) } {\n+\t\t     || [check_v3_target_fileio])\n+\t\t && [string first \"/experimental/simd/\" $t] == -1 } {\n \t\tlappend tests $t\n \t    }\n \t}\n@@ -107,5 +109,19 @@ global DEFAULT_CXXFLAGS\n global PCH_CXXFLAGS\n dg-runtest $tests \"\" \"$DEFAULT_CXXFLAGS $PCH_CXXFLAGS\"\n \n+# Finally run simd tests with extra SIMD-relevant flags\n+global DEFAULT_VECTCFLAGS\n+global EFFECTIVE_TARGETS\n+set DEFAULT_VECTCFLAGS \"\"\n+set EFFECTIVE_TARGETS \"\"\n+\n+if [check_vect_support_and_set_flags] {\n+  lappend DEFAULT_VECTCFLAGS \"-O2\"\n+  lappend DEFAULT_VECTCFLAGS \"-Wno-psabi\"\n+  et-dg-runtest dg-runtest [lsort \\\n+    [glob -nocomplain $srcdir/experimental/simd/*.cc]] \\\n+    \"$DEFAULT_VECTCFLAGS\" \"$DEFAULT_CXXFLAGS $PCH_CXXFLAGS\"\n+}\n+\n # All done.\n dg-finish"}]}