{"sha": "ed8d29205b130073bd3147cedf068446689a5386", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZWQ4ZDI5MjA1YjEzMDA3M2JkMzE0N2NlZGYwNjg0NDY2ODlhNTM4Ng==", "commit": {"author": {"name": "Michael Matz", "email": "matz@suse.de", "date": "2002-07-15T14:07:06Z"}, "committer": {"name": "Michael Matz", "email": "matz@gcc.gnu.org", "date": "2002-07-15T14:07:06Z"}, "message": "[multiple changes]\n\n2002-07-15  Michael Matz  <matz@suse.de>,\n            Daniel Berlin  <dberlin@dberlin.org>,\n\t    Denis Chertykov  <denisc@overta.ru>\n\n\tAdd a new register allocator.\n\n\t* ra.c: New file.\n\t* ra.h: New file.\n\t* ra-build.c: New file.\n\t* ra-colorize.c: New file.\n\t* ra-debug.c: New file.\n\t* ra-rewrite.c: New file.\n\n\t* Makefile.in (ra.o, ra-build.o, ra-colorize.o, ra-debug.o,\n\t(ra-rewrite.o): New .o files for libbackend.a.\n\t(GTFILES): Add basic-block.h.\n\n\t* toplev.c (flag_new_regalloc): New.\n\t(f_options): New option \"new-ra\".\n\t(rest_of_compilation): Call initialize_uninitialized_subregs()\n\tonly for the old allocator.  If flag_new_regalloc is set, call\n\tnew allocator, instead of local_alloc(), global_alloc() and\n\tfriends.\n\n\t* doc/invoke.texi: Document -fnew-ra.\n\t* basic-block.h (FOR_ALL_BB): New.\n\t* config/rs6000/rs6000.c (print_operand): Write small constants\n\tas @l+80.\n\n\t* df.c (read_modify_subreg_p): Narrow down cases for a rmw subreg.\n\t(df_reg_table_realloc): Make size at least as large as max_reg_num().\n\t(df_insn_table_realloc): Size argument now is absolute, not relative.\n\tChanged all callers.\n\n\t* gengtype.c (main): Add the pseudo-type \"HARD_REG_SET\".\n\t* regclass.c (reg_scan_mark_refs): Ignore NULL rtx's.\n\n\t2002-06-20  Michael Matz  <matz@suse.de>\n\n\t* df.h (struct ref.id): Make unsigned.\n\t* df.c (df_bb_reg_def_chain_create): Remove unsigned cast.\n\n\t2002-06-13  Michael Matz  <matz@suse.de>\n\n\t* df.h (DF_REF_MODE_CHANGE): New flag.\n\t* df.c (df_def_record_1, df_uses_record): Set this flag for refs\n\tinvolving subregs with invalid mode changes, when\n\tCLASS_CANNOT_CHANGE_MODE is defined.\n\n\t2002-05-07  Michael Matz  <matz@suse.de>\n\n\t* reload1.c (fixup_abnormal_edges): Don't insert on NULL edge.\n\n\t2002-05-03  Michael Matz  <matz@suse.de>\n\n\t* sbitmap.c (sbitmap_difference): Accept sbitmaps of different size.\n\n\tSat Feb  2 18:58:07 2002  Denis Chertykov  <denisc@overta.ru>\n\n\t* regclass.c (regclass): Work with all regs which have sets or\n\trefs.\n\t(reg_scan_mark_refs): Count regs inside (clobber ...).\n\n\t2002-01-04  Michael Matz  <matzmich@cs.tu-berlin.de>\n\n\t* df.c (df_ref_record): Correctly calculate SUBREGs of hardregs.\n\t(df_bb_reg_def_chain_create, df_bb_reg_use_chain_create): Only\n\tadd new refs.\n\t(df_bb_refs_update): Don't clear insns_modified here, ...\n\t(df_analyse): ... but here.\n\n\t* sbitmap.c (dump_sbitmap_file): New.\n\t(debug_sbitmap): Use it.\n\n\t* sbitmap.h (dump_sbitmap_file): Add prototype.\n\n\t2001-08-07  Daniel Berlin  <dan@cgsoftware.com>\n\n\t* df.c (df_insn_modify): Grow the UID table if necessary, rather\n\tthan assume all emits go through df_insns_modify.\n\n\t2001-07-26  Daniel Berlin  <dan@cgsoftware.com>\n\n\t* regclass.c (reg_scan_mark_refs): When we increase REG_N_SETS,\n\tincrease REG_N_REFS (like flow does), so that regclass doesn't\n\tthink a reg is useless, and thus, not calculate a class, when it\n\treally should have.\n\n\t2001-01-28  Daniel Berlin  <dberlin@redhat.com>\n\n\t* sbitmap.h (EXECUTE_IF_SET_IN_SBITMAP_REV): New macro, needed for\n\tdataflow analysis.\n\nFrom-SVN: r55458", "tree": {"sha": "f546521b195da8e2a127d8b63d1d3c1dc7552a6b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/f546521b195da8e2a127d8b63d1d3c1dc7552a6b"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/ed8d29205b130073bd3147cedf068446689a5386", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ed8d29205b130073bd3147cedf068446689a5386", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ed8d29205b130073bd3147cedf068446689a5386", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ed8d29205b130073bd3147cedf068446689a5386/comments", "author": {"login": "susematz", "id": 4117296, "node_id": "MDQ6VXNlcjQxMTcyOTY=", "avatar_url": "https://avatars.githubusercontent.com/u/4117296?v=4", "gravatar_id": "", "url": "https://api.github.com/users/susematz", "html_url": "https://github.com/susematz", "followers_url": "https://api.github.com/users/susematz/followers", "following_url": "https://api.github.com/users/susematz/following{/other_user}", "gists_url": "https://api.github.com/users/susematz/gists{/gist_id}", "starred_url": "https://api.github.com/users/susematz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/susematz/subscriptions", "organizations_url": "https://api.github.com/users/susematz/orgs", "repos_url": "https://api.github.com/users/susematz/repos", "events_url": "https://api.github.com/users/susematz/events{/privacy}", "received_events_url": "https://api.github.com/users/susematz/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "7bc7d27b87e51407d20adf4f071b253262c254fb", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7bc7d27b87e51407d20adf4f071b253262c254fb", "html_url": "https://github.com/Rust-GCC/gccrs/commit/7bc7d27b87e51407d20adf4f071b253262c254fb"}], "stats": {"total": 11010, "additions": 10950, "deletions": 60}, "files": [{"sha": "784a076a6538bd029c7a3ae686ff1df130df0f02", "filename": "gcc/ChangeLog", "status": "modified", "additions": 93, "deletions": 0, "changes": 93, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -1,3 +1,96 @@\n+2002-07-15  Michael Matz  <matz@suse.de>,\n+            Daniel Berlin  <dberlin@dberlin.org>,\n+\t    Denis Chertykov  <denisc@overta.ru>\n+\n+\tAdd a new register allocator.\n+\n+\t* ra.c: New file.\n+\t* ra.h: New file.\n+\t* ra-build.c: New file.\n+\t* ra-colorize.c: New file.\n+\t* ra-debug.c: New file.\n+\t* ra-rewrite.c: New file.\n+\n+\t* Makefile.in (ra.o, ra-build.o, ra-colorize.o, ra-debug.o,\n+\t(ra-rewrite.o): New .o files for libbackend.a.\n+\t(GTFILES): Add basic-block.h.\n+\n+\t* toplev.c (flag_new_regalloc): New.\n+\t(f_options): New option \"new-ra\".\n+\t(rest_of_compilation): Call initialize_uninitialized_subregs()\n+\tonly for the old allocator.  If flag_new_regalloc is set, call\n+\tnew allocator, instead of local_alloc(), global_alloc() and\n+\tfriends.\n+\n+\t* doc/invoke.texi: Document -fnew-ra.\n+\t* basic-block.h (FOR_ALL_BB): New.\n+\t* config/rs6000/rs6000.c (print_operand): Write small constants\n+\tas @l+80.\n+\n+\t* df.c (read_modify_subreg_p): Narrow down cases for a rmw subreg.\n+\t(df_reg_table_realloc): Make size at least as large as max_reg_num().\n+\t(df_insn_table_realloc): Size argument now is absolute, not relative.\n+\tChanged all callers.\n+\n+\t* gengtype.c (main): Add the pseudo-type \"HARD_REG_SET\".\n+\t* regclass.c (reg_scan_mark_refs): Ignore NULL rtx's.\n+\n+\t2002-06-20  Michael Matz  <matz@suse.de>\n+\n+\t* df.h (struct ref.id): Make unsigned.\n+\t* df.c (df_bb_reg_def_chain_create): Remove unsigned cast.\n+\n+\t2002-06-13  Michael Matz  <matz@suse.de>\n+\n+\t* df.h (DF_REF_MODE_CHANGE): New flag.\n+\t* df.c (df_def_record_1, df_uses_record): Set this flag for refs\n+\tinvolving subregs with invalid mode changes, when\n+\tCLASS_CANNOT_CHANGE_MODE is defined.\n+\n+\t2002-05-07  Michael Matz  <matz@suse.de>\n+\n+\t* reload1.c (fixup_abnormal_edges): Don't insert on NULL edge.\n+\n+\t2002-05-03  Michael Matz  <matz@suse.de>\n+\n+\t* sbitmap.c (sbitmap_difference): Accept sbitmaps of different size.\n+\n+\tSat Feb  2 18:58:07 2002  Denis Chertykov  <denisc@overta.ru>\n+\n+\t* regclass.c (regclass): Work with all regs which have sets or\n+\trefs.\n+\t(reg_scan_mark_refs): Count regs inside (clobber ...).\n+\n+\t2002-01-04  Michael Matz  <matzmich@cs.tu-berlin.de>\n+\n+\t* df.c (df_ref_record): Correctly calculate SUBREGs of hardregs.\n+\t(df_bb_reg_def_chain_create, df_bb_reg_use_chain_create): Only\n+\tadd new refs.\n+\t(df_bb_refs_update): Don't clear insns_modified here, ...\n+\t(df_analyse): ... but here.\n+\n+\t* sbitmap.c (dump_sbitmap_file): New.\n+\t(debug_sbitmap): Use it.\n+\n+\t* sbitmap.h (dump_sbitmap_file): Add prototype.\n+\n+\t2001-08-07  Daniel Berlin  <dan@cgsoftware.com>\n+\n+\t* df.c (df_insn_modify): Grow the UID table if necessary, rather\n+\tthan assume all emits go through df_insns_modify.\n+\n+\t2001-07-26  Daniel Berlin  <dan@cgsoftware.com>\n+\n+\t* regclass.c (reg_scan_mark_refs): When we increase REG_N_SETS,\n+\tincrease REG_N_REFS (like flow does), so that regclass doesn't\n+\tthink a reg is useless, and thus, not calculate a class, when it\n+\treally should have.\n+\n+\t2001-01-28  Daniel Berlin  <dberlin@redhat.com>\n+\n+\t* sbitmap.h (EXECUTE_IF_SET_IN_SBITMAP_REV): New macro, needed for\n+\tdataflow analysis.\n+\n 2002-07-15  Jakub Jelinek  <jakub@redhat.com>\n \n \tPR middle-end/7245"}, {"sha": "50f178c335788b9344b588d964fb3892cb64632a", "filename": "gcc/Makefile.in", "status": "modified", "additions": 18, "deletions": 3, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -731,7 +731,8 @@ OBJS = alias.o bb-reorder.o bitmap.o builtins.o caller-save.o calls.o\t   \\\n  insn-extract.o insn-opinit.o insn-output.o insn-peep.o insn-recog.o\t   \\\n  integrate.o intl.o jump.o  langhooks.o lcm.o lists.o local-alloc.o\t   \\\n  loop.o mbchar.o optabs.o params.o predict.o print-rtl.o print-tree.o\t   \\\n- profile.o real.o recog.o reg-stack.o regclass.o regmove.o regrename.o\t   \\\n+ profile.o ra.o ra-build.o ra-colorize.o ra-debug.o ra-rewrite.o\t   \\\n+ real.o recog.o reg-stack.o regclass.o regmove.o regrename.o\t\t   \\\n  reload.o reload1.o reorg.o resource.o rtl.o rtlanal.o rtl-error.o\t   \\\n  sbitmap.o sched-deps.o sched-ebb.o sched-rgn.o sched-vis.o sdbout.o\t   \\\n  sibcall.o simplify-rtx.o ssa.o ssa-ccp.o ssa-dce.o stmt.o\t\t   \\\n@@ -1562,6 +1563,19 @@ global.o : global.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) flags.h reload.h function.h\n    $(BASIC_BLOCK_H) $(REGS_H) hard-reg-set.h insn-config.h output.h toplev.h \\\n    $(TM_P_H)\n varray.o : varray.c $(CONFIG_H) $(SYSTEM_H) varray.h $(GGC_H) errors.h\n+ra.o : ra.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H)  $(TM_P_H) insn-config.h \\\n+   $(RECOG_H) integrate.h function.h $(REGS_H) $(OBSTACK_H) hard-reg-set.h \\\n+   $(BASIC_BLOCK_H) df.h expr.h output.h toplev.h flags.h reload.h ra.h\n+ra-build.o : ra-build.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(TM_P_H) \\\n+   insn-config.h $(RECOG_H) function.h $(REGS_H) hard-reg-set.h \\\n+   $(BASIC_BLOCK_H) df.h output.h ggc.h ra.h gt-ra-build.h\n+ra-colorize.o : ra-colorize.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(TM_P_H) \\\n+    function.h $(REGS_H) hard-reg-set.h $(BASIC_BLOCK_H) df.h output.h ra.h\n+ra-debug.o : ra-debug.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H)  insn-config.h \\\n+   $(RECOG_H) function.h hard-reg-set.h $(BASIC_BLOCK_H) df.h output.h ra.h\n+ra-rewrite.o : ra-rewrite.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) $(TM_P_H) \\\n+   function.h $(REGS_H) hard-reg-set.h $(BASIC_BLOCK_H) df.h expr.h \\\n+   output.h except.h ra.h\n reload.o : reload.c $(CONFIG_H) $(SYSTEM_H) $(RTL_H) flags.h output.h \\\n    $(EXPR_H) $(OPTABS_H) reload.h $(RECOG_H) hard-reg-set.h insn-config.h \\\n    $(REGS_H) function.h real.h toplev.h $(TM_P_H)\n@@ -1820,7 +1834,8 @@ GTFILES = $(GCONFIG_H) $(srcdir)/location.h \\\n   $(srcdir)/except.c $(srcdir)/explow.c $(srcdir)/expr.c \\\n   $(srcdir)/fold-const.c $(srcdir)/function.c \\\n   $(srcdir)/gcse.c $(srcdir)/integrate.c $(srcdir)/lists.c $(srcdir)/optabs.c \\\n-  $(srcdir)/profile.c $(srcdir)/regclass.c $(srcdir)/reg-stack.c \\\n+  $(srcdir)/profile.c $(srcdir)/ra-build.c $(srcdir)/regclass.c \\\n+  $(srcdir)/reg-stack.c \\\n   $(srcdir)/sdbout.c $(srcdir)/stmt.c $(srcdir)/stor-layout.c \\\n   $(srcdir)/tree.c $(srcdir)/varasm.c \\\n   $(out_file) \\\n@@ -1836,7 +1851,7 @@ gt-integrate.h gt-stmt.h gt-tree.h gt-varasm.h gt-emit-rtl.h : s-gtype; @true\n gt-explow.h gt-stor-layout.h gt-regclass.h gt-lists.h : s-gtype; @true\n gt-alias.h gt-cselib.h gt-fold-const.h gt-gcse.h gt-profile.h : s-gtype; @true\n gt-expr.h gt-sdbout.h gt-optabs.h gt-bitmap.h gt-dwarf2out.h : s-gtype ; @true\n-gt-reg-stack.h gt-dependence.h : s-gtype ; @true\n+gt-ra-build.h gt-reg-stack.h gt-dependence.h : s-gtype ; @true\n gt-c-common.h gt-c-decl.h gt-c-parse.h gt-c-pragma.h : s-gtype; @true\n gt-c-objc-common.h gtype-c.h gt-location.h : s-gtype ; @true\n "}, {"sha": "a3c97f95e3429848b7a956ab02e0a680db90f2fd", "filename": "gcc/basic-block.h", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fbasic-block.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fbasic-block.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fbasic-block.h?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -261,6 +261,12 @@ extern varray_type basic_block_info;\n #define FOR_EACH_BB_REVERSE(BB) \\\n   FOR_BB_BETWEEN (BB, EXIT_BLOCK_PTR->prev_bb, ENTRY_BLOCK_PTR, prev_bb)\n \n+/* Cycles through _all_ basic blocks, even the fake ones (entry and\n+   exit block).  */\n+\n+#define FOR_ALL_BB(BB) \\\n+  for (BB = ENTRY_BLOCK_PTR; BB; BB = BB->next_bb)\n+\n /* What registers are live at the setjmp call.  */\n \n extern regset regs_live_at_setjmp;"}, {"sha": "d230ba88b8f43b9ee35d869d7bb985f6d7938a71", "filename": "gcc/config/rs6000/rs6000.c", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fconfig%2Frs6000%2Frs6000.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fconfig%2Frs6000%2Frs6000.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000.c?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -6258,6 +6258,11 @@ print_operand (file, x, code)\n \t    output_operand_lossage (\"invalid %%K value\");\n \t  print_operand_address (file, XEXP (XEXP (x, 0), 0));\n \t  fputs (\"@l\", file);\n+\t  /* For GNU as, there must be a non-alphanumeric character\n+\t     between 'l' and the number.  The '-' is added by\n+\t     print_operand() already.  */\n+\t  if (INTVAL (XEXP (XEXP (x, 0), 1)) >= 0)\n+\t    fputs (\"+\", file);\n \t  print_operand (file, XEXP (XEXP (x, 0), 1), 0);\n \t}\n       return;"}, {"sha": "59f314384a225e6aa56260c254a3b6a8d7d73b44", "filename": "gcc/df.h", "status": "modified", "additions": 11, "deletions": 2, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fdf.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fdf.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdf.h?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -50,7 +50,16 @@ struct df_link\n \n enum df_ref_flags\n   {\n-    DF_REF_READ_WRITE = 1\n+    DF_REF_READ_WRITE = 1,\n+\n+    /* This flag is set on register references itself representing a or\n+       being inside a subreg on machines which have CLASS_CANNOT_CHANGE_MODE\n+       and where the mode change of that subreg expression is invalid for\n+       this class.  Note, that this flag can also be set on df_refs\n+       representing the REG itself (i.e. one might not see the subreg\n+       anyore).  Also note, that this flag is set also for hardreg refs.\n+       I.e. you must check yourself if it's a pseudo.  */\n+    DF_REF_MODE_CHANGE = 2\n   };\n \n /* Define a register reference structure.  */\n@@ -61,7 +70,7 @@ struct ref\n   rtx *loc;\t\t\t/* Loc is the location of the reg.  */\n   struct df_link *chain;\t/* Head of def-use or use-def chain.  */\n   enum df_ref_type type;\t/* Type of ref.  */\n-  int id;\t\t\t/* Ref index.  */\n+  unsigned int id;\t\t/* Ref index.  */\n   enum df_ref_flags flags;\t/* Various flags.  */\n };\n "}, {"sha": "ecf87f470ede31ac563b91997b05b8d375bc0776", "filename": "gcc/doc/invoke.texi", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fdoc%2Finvoke.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fdoc%2Finvoke.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Finvoke.texi?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -270,7 +270,7 @@ in the following sections.\n -fif-conversion -fif-conversion2 @gol\n -finline-functions  -finline-limit=@var{n}  -fkeep-inline-functions @gol\n -fkeep-static-consts  -fmerge-constants  -fmerge-all-constants @gol\n--fmove-all-movables  -fno-default-inline  -fno-defer-pop @gol\n+-fmove-all-movables  -fnew-ra -fno-default-inline  -fno-defer-pop @gol\n -fno-function-cse  -fno-guess-branch-probability @gol\n -fno-inline  -fno-math-errno  -fno-peephole  -fno-peephole2 @gol\n -funsafe-math-optimizations -fno-trapping-math @gol\n@@ -3395,6 +3395,12 @@ types.  Languages like C or C++ require each non-automatic variable to\n have distinct location, so using this option will result in non-conforming\n behavior.\n \n+@item -fnew-ra\n+@opindex fnew-ra\n+Use a graph coloring register allocator.  Currently this option is meant\n+for testing, so we are interested to hear about miscompilations with\n+@option{-fnew-ra}.\n+\n @item -fno-function-cse\n @opindex fno-function-cse\n Do not put function addresses in registers; make each instruction that"}, {"sha": "adc3932293d4128f80c07d2a76019a3821a695c6", "filename": "gcc/gengtype.c", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fgengtype.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fgengtype.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgengtype.c?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -1939,6 +1939,10 @@ main(argc, argv)\n \t\t\t\t\t\t\t strlen (\"void\"))),\n \t      &pos);\n \n+  do_typedef (\"HARD_REG_SET\", create_array (\n+\t      create_scalar_type (\"unsigned long\", strlen (\"unsigned long\")),\n+\t      \"2\"), &pos);\n+\n   for (i = 0; i < NUM_GT_FILES; i++)\n     {\n       int dupflag = 0;"}, {"sha": "d3e24bc8cb95714177578136a2867d33a6e3b276", "filename": "gcc/ra-build.c", "status": "added", "additions": 3264, "deletions": 0, "changes": 3264, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fra-build.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fra-build.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fra-build.c?ref=ed8d29205b130073bd3147cedf068446689a5386"}, {"sha": "297b419e9006e69e4cfed4660ad65c275e1e3b02", "filename": "gcc/ra-colorize.c", "status": "added", "additions": 2734, "deletions": 0, "changes": 2734, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fra-colorize.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fra-colorize.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fra-colorize.c?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -0,0 +1,2734 @@\n+/* Graph coloring register allocator\n+   Copyright (C) 2001, 2002 Free Software Foundation, Inc.\n+   Contributed by Michael Matz <matz@suse.de>\n+   and Daniel Berlin <dan@cgsoftware.com>.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it under the\n+   terms of the GNU General Public License as published by the Free Software\n+   Foundation; either version 2, or (at your option) any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n+   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n+   details.\n+\n+   You should have received a copy of the GNU General Public License along\n+   with GCC; see the file COPYING.  If not, write to the Free Software\n+   Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"function.h\"\n+#include \"regs.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"df.h\"\n+#include \"output.h\"\n+#include \"ra.h\"\n+\n+/* This file is part of the graph coloring register allocator.\n+   It contains the graph colorizer.  Given an interference graph\n+   as set up in ra-build.c the toplevel function in this file\n+   (ra_colorize_graph) colorizes the graph, leaving a list\n+   of colored, coalesced and spilled nodes.\n+\n+   The algorithm used is a merge of George & Appels iterative coalescing\n+   and optimistic coalescing, switchable at runtime.  The current default\n+   is \"optimistic coalescing +\", which is based on the normal Briggs/Cooper\n+   framework.  We can also use biased coloring.  Most of the structure\n+   here follows the different papers.\n+\n+   Additionally there is a custom step to locally improve the overall\n+   spill cost of the colored graph (recolor_spills).  */\n+\n+static void push_list PARAMS ((struct dlist *, struct dlist **));\n+static void push_list_end PARAMS ((struct dlist *, struct dlist **));\n+static void free_dlist PARAMS ((struct dlist **));\n+static void put_web_at_end PARAMS ((struct web *, enum node_type));\n+static void put_move PARAMS ((struct move *, enum move_type));\n+static void build_worklists PARAMS ((struct df *));\n+static void enable_move PARAMS ((struct web *));\n+static void decrement_degree PARAMS ((struct web *, int));\n+static void simplify PARAMS ((void));\n+static void remove_move_1 PARAMS ((struct web *, struct move *));\n+static void remove_move PARAMS ((struct web *, struct move *));\n+static void add_worklist PARAMS ((struct web *));\n+static int ok PARAMS ((struct web *, struct web *));\n+static int conservative PARAMS ((struct web *, struct web *));\n+static inline unsigned int simplify_p PARAMS ((enum node_type));\n+static void combine PARAMS ((struct web *, struct web *));\n+static void coalesce PARAMS ((void));\n+static void freeze_moves PARAMS ((struct web *));\n+static void freeze PARAMS ((void));\n+static void select_spill PARAMS ((void));\n+static int color_usable_p PARAMS ((int, HARD_REG_SET, HARD_REG_SET,\n+\t\t\t\t   enum machine_mode));\n+int get_free_reg PARAMS ((HARD_REG_SET, HARD_REG_SET, enum machine_mode));\n+static int get_biased_reg PARAMS ((HARD_REG_SET, HARD_REG_SET, HARD_REG_SET,\n+\t\t\t\t   HARD_REG_SET, enum machine_mode));\n+static int count_long_blocks PARAMS ((HARD_REG_SET, int));\n+static char * hardregset_to_string PARAMS ((HARD_REG_SET));\n+static void calculate_dont_begin PARAMS ((struct web *, HARD_REG_SET *));\n+static void colorize_one_web PARAMS ((struct web *, int));\n+static void assign_colors PARAMS ((void));\n+static void try_recolor_web PARAMS ((struct web *));\n+static void insert_coalesced_conflicts PARAMS ((void));\n+static int comp_webs_maxcost PARAMS ((const void *, const void *));\n+static void recolor_spills PARAMS ((void));\n+static void check_colors PARAMS ((void));\n+static void restore_conflicts_from_coalesce PARAMS ((struct web *));\n+static void break_coalesced_spills PARAMS ((void));\n+static void unalias_web PARAMS ((struct web *));\n+static void break_aliases_to_web PARAMS ((struct web *));\n+static void break_precolored_alias PARAMS ((struct web *));\n+static void init_web_pairs PARAMS ((void));\n+static void add_web_pair_cost PARAMS ((struct web *, struct web *,\n+\t\t\t               unsigned HOST_WIDE_INT, unsigned int));\n+static int comp_web_pairs PARAMS ((const void *, const void *));\n+static void sort_and_combine_web_pairs PARAMS ((int));\n+static void aggressive_coalesce PARAMS ((void));\n+static void extended_coalesce_2 PARAMS ((void));\n+static void check_uncoalesced_moves PARAMS ((void));\n+\n+static struct dlist *mv_worklist, *mv_coalesced, *mv_constrained;\n+static struct dlist *mv_frozen, *mv_active;\n+\n+/* Push a node onto the front of the list.  */\n+\n+static void\n+push_list (x, list)\n+     struct dlist *x;\n+     struct dlist **list;\n+{\n+  if (x->next || x->prev)\n+    abort ();\n+  x->next = *list;\n+  if (*list)\n+    (*list)->prev = x;\n+  *list = x;\n+}\n+\n+static void\n+push_list_end (x, list)\n+     struct dlist *x;\n+     struct dlist **list;\n+{\n+  if (x->prev || x->next)\n+    abort ();\n+  if (!*list)\n+    {\n+      *list = x;\n+      return;\n+    }\n+  while ((*list)->next)\n+    list = &((*list)->next);\n+  x->prev = *list;\n+  (*list)->next = x;\n+}\n+\n+/* Remove a node from the list.  */\n+\n+void\n+remove_list (x, list)\n+     struct dlist *x;\n+     struct dlist **list;\n+{\n+  struct dlist *y = x->prev;\n+  if (y)\n+    y->next = x->next;\n+  else\n+    *list = x->next;\n+  y = x->next;\n+  if (y)\n+    y->prev = x->prev;\n+  x->next = x->prev = NULL;\n+}\n+\n+/* Pop the front of the list.  */\n+\n+struct dlist *\n+pop_list (list)\n+     struct dlist **list;\n+{\n+  struct dlist *r = *list;\n+  if (r)\n+    remove_list (r, list);\n+  return r;\n+}\n+\n+/* Free the given double linked list.  */\n+\n+static void\n+free_dlist (list)\n+     struct dlist **list;\n+{\n+  *list = NULL;\n+}\n+\n+/* The web WEB should get the given new TYPE.  Put it onto the\n+   appropriate list.\n+   Inline, because it's called with constant TYPE every time.  */\n+\n+inline void\n+put_web (web, type)\n+     struct web *web;\n+     enum node_type type;\n+{\n+  switch (type)\n+    {\n+      case INITIAL:\n+      case FREE:\n+      case FREEZE:\n+      case SPILL:\n+      case SPILLED:\n+      case COALESCED:\n+      case COLORED:\n+      case SELECT:\n+\tpush_list (web->dlink, &WEBS(type));\n+\tbreak;\n+      case PRECOLORED:\n+\tpush_list (web->dlink, &WEBS(INITIAL));\n+\tbreak;\n+      case SIMPLIFY:\n+\tif (web->spill_temp)\n+\t  push_list (web->dlink, &WEBS(type = SIMPLIFY_SPILL));\n+\telse if (web->add_hardregs)\n+\t  push_list (web->dlink, &WEBS(type = SIMPLIFY_FAT));\n+\telse\n+\t  push_list (web->dlink, &WEBS(SIMPLIFY));\n+\tbreak;\n+      default:\n+\tabort ();\n+    }\n+  web->type = type;\n+}\n+\n+/* After we are done with the whole pass of coloring/spilling,\n+   we reset the lists of webs, in preparation of the next pass.\n+   The spilled webs become free, colored webs go to the initial list,\n+   coalesced webs become free or initial, according to what type of web\n+   they are coalesced to.  */\n+\n+void\n+reset_lists ()\n+{\n+  struct dlist *d;\n+  unsigned int i;\n+  if (WEBS(SIMPLIFY) || WEBS(SIMPLIFY_SPILL) || WEBS(SIMPLIFY_FAT)\n+      || WEBS(FREEZE) || WEBS(SPILL) || WEBS(SELECT))\n+    abort ();\n+\n+  while ((d = pop_list (&WEBS(COALESCED))) != NULL)\n+    {\n+      struct web *web = DLIST_WEB (d);\n+      struct web *aweb = alias (web);\n+      /* Note, how alias() becomes invalid through the two put_web()'s\n+\t below.  It might set the type of a web to FREE (from COALESCED),\n+\t which itself is a target of aliasing (i.e. in the middle of\n+\t an alias chain).  We can handle this by checking also for\n+\t type == FREE.  Note nevertheless, that alias() is invalid\n+\t henceforth.  */\n+      if (aweb->type == SPILLED || aweb->type == FREE)\n+\tput_web (web, FREE);\n+      else\n+\tput_web (web, INITIAL);\n+    }\n+  while ((d = pop_list (&WEBS(SPILLED))) != NULL)\n+    put_web (DLIST_WEB (d), FREE);\n+  while ((d = pop_list (&WEBS(COLORED))) != NULL)\n+    put_web (DLIST_WEB (d), INITIAL);\n+\n+  /* All free webs have no conflicts anymore.  */\n+  for (d = WEBS(FREE); d; d = d->next)\n+    {\n+      struct web *web = DLIST_WEB (d);\n+      BITMAP_XFREE (web->useless_conflicts);\n+      web->useless_conflicts = NULL;\n+    }\n+\n+  /* Sanity check, that we only have free, initial or precolored webs.  */\n+  for (i = 0; i < num_webs; i++)\n+    {\n+      struct web *web = ID2WEB (i);\n+      if (web->type != INITIAL && web->type != FREE && web->type != PRECOLORED)\n+\tabort ();\n+    }\n+  free_dlist (&mv_worklist);\n+  free_dlist (&mv_coalesced);\n+  free_dlist (&mv_constrained);\n+  free_dlist (&mv_frozen);\n+  free_dlist (&mv_active);\n+}\n+\n+/* Similar to put_web(), but add the web to the end of the appropriate\n+   list.  Additionally TYPE may not be SIMPLIFY.  */\n+\n+static void\n+put_web_at_end (web, type)\n+     struct web *web;\n+     enum node_type type;\n+{\n+  if (type == PRECOLORED)\n+    type = INITIAL;\n+  else if (type == SIMPLIFY)\n+    abort ();\n+  push_list_end (web->dlink, &WEBS(type));\n+  web->type = type;\n+}\n+\n+/* Unlink WEB from the list it's currently on (which corresponds to\n+   its current type).  */\n+\n+void\n+remove_web_from_list (web)\n+     struct web *web;\n+{\n+  if (web->type == PRECOLORED)\n+    remove_list (web->dlink, &WEBS(INITIAL));\n+  else\n+    remove_list (web->dlink, &WEBS(web->type));\n+}\n+\n+/* Give MOVE the TYPE, and link it into the correct list.  */\n+\n+static inline void\n+put_move (move, type)\n+     struct move *move;\n+     enum move_type type;\n+{\n+  switch (type)\n+    {\n+      case WORKLIST:\n+\tpush_list (move->dlink, &mv_worklist);\n+\tbreak;\n+      case MV_COALESCED:\n+\tpush_list (move->dlink, &mv_coalesced);\n+\tbreak;\n+      case CONSTRAINED:\n+\tpush_list (move->dlink, &mv_constrained);\n+\tbreak;\n+      case FROZEN:\n+\tpush_list (move->dlink, &mv_frozen);\n+\tbreak;\n+      case ACTIVE:\n+\tpush_list (move->dlink, &mv_active);\n+\tbreak;\n+      default:\n+\tabort ();\n+    }\n+  move->type = type;\n+}\n+\n+/* Build the worklists we are going to process.  */\n+\n+static void\n+build_worklists (df)\n+     struct df *df ATTRIBUTE_UNUSED;\n+{\n+  struct dlist *d, *d_next;\n+  struct move_list *ml;\n+\n+  /* If we are not the first pass, put all stackwebs (which are still\n+     backed by a new pseudo, but conceptually can stand for a stackslot,\n+     i.e. it doesn't really matter if they get a color or not), on\n+     the SELECT stack first, those with lowest cost first.  This way\n+     they will be colored last, so do not contrain the coloring of the\n+     normal webs.  But still those with the highest count are colored\n+     before, i.e. get a color more probable.  The use of stackregs is\n+     a pure optimization, and all would work, if we used real stackslots\n+     from the begin.  */\n+  if (ra_pass > 1)\n+    {\n+      unsigned int i, num, max_num;\n+      struct web **order2web;\n+      max_num = num_webs - num_subwebs;\n+      order2web = (struct web **) xmalloc (max_num * sizeof (order2web[0]));\n+      for (i = 0, num = 0; i < max_num; i++)\n+\tif (id2web[i]->regno >= max_normal_pseudo)\n+\t  order2web[num++] = id2web[i];\n+      if (num)\n+\t{\n+\t  qsort (order2web, num, sizeof (order2web[0]), comp_webs_maxcost);\n+\t  for (i = num - 1;; i--)\n+\t    {\n+\t      struct web *web = order2web[i];\n+\t      struct conflict_link *wl;\n+\t      remove_list (web->dlink, &WEBS(INITIAL));\n+\t      put_web (web, SELECT);\n+\t      for (wl = web->conflict_list; wl; wl = wl->next)\n+\t\t{\n+\t\t  struct web *pweb = wl->t;\n+\t\t  pweb->num_conflicts -= 1 + web->add_hardregs;\n+\t\t}\n+\t      if (i == 0)\n+\t\tbreak;\n+\t    }\n+\t}\n+      free (order2web);\n+    }\n+\n+  /* For all remaining initial webs, classify them.  */\n+  for (d = WEBS(INITIAL); d; d = d_next)\n+    {\n+      struct web *web = DLIST_WEB (d);\n+      d_next = d->next;\n+      if (web->type == PRECOLORED)\n+        continue;\n+\n+      remove_list (d, &WEBS(INITIAL));\n+      if (web->num_conflicts >= NUM_REGS (web))\n+\tput_web (web, SPILL);\n+      else if (web->moves)\n+\tput_web (web, FREEZE);\n+      else\n+\tput_web (web, SIMPLIFY);\n+    }\n+\n+  /* And put all moves on the worklist for iterated coalescing.\n+     Note, that if iterated coalescing is off, then wl_moves doesn't\n+     contain any moves.  */\n+  for (ml = wl_moves; ml; ml = ml->next)\n+    if (ml->move)\n+      {\n+\tstruct move *m = ml->move;\n+        d = (struct dlist *) ra_calloc (sizeof (struct dlist));\n+        DLIST_MOVE (d) = m;\n+        m->dlink = d;\n+\tput_move (m, WORKLIST);\n+      }\n+}\n+\n+/* Enable the active moves, in which WEB takes part, to be processed.  */\n+\n+static void\n+enable_move (web)\n+     struct web *web;\n+{\n+  struct move_list *ml;\n+  for (ml = web->moves; ml; ml = ml->next)\n+    if (ml->move->type == ACTIVE)\n+      {\n+\tremove_list (ml->move->dlink, &mv_active);\n+\tput_move (ml->move, WORKLIST);\n+      }\n+}\n+\n+/* Decrement the degree of node WEB by the amount DEC.\n+   Possibly change the type of WEB, if the number of conflicts is\n+   now smaller than its freedom.  */\n+\n+static void\n+decrement_degree (web, dec)\n+     struct web *web;\n+     int dec;\n+{\n+  int before = web->num_conflicts;\n+  web->num_conflicts -= dec;\n+  if (web->num_conflicts < NUM_REGS (web) && before >= NUM_REGS (web))\n+    {\n+      struct conflict_link *a;\n+      enable_move (web);\n+      for (a = web->conflict_list; a; a = a->next)\n+\t{\n+\t  struct web *aweb = a->t;\n+\t  if (aweb->type != SELECT && aweb->type != COALESCED)\n+\t    enable_move (aweb);\n+\t}\n+      if (web->type != FREEZE)\n+\t{\n+\t  remove_web_from_list (web);\n+\t  if (web->moves)\n+\t    put_web (web, FREEZE);\n+\t  else\n+\t    put_web (web, SIMPLIFY);\n+\t}\n+    }\n+}\n+\n+/* Repeatedly simplify the nodes on the simplify worklists.  */\n+\n+static void\n+simplify ()\n+{\n+  struct dlist *d;\n+  struct web *web;\n+  struct conflict_link *wl;\n+  while (1)\n+    {\n+      /* We try hard to color all the webs resulting from spills first.\n+\t Without that on register starved machines (x86 e.g) with some live\n+\t DImode pseudos, -fPIC, and an asm requiring %edx, it might be, that\n+\t we do rounds over rounds, because the conflict graph says, we can\n+\t simplify those short webs, but later due to irregularities we can't\n+\t color those pseudos.  So we have to spill them, which in later rounds\n+\t leads to other spills.  */\n+      d = pop_list (&WEBS(SIMPLIFY));\n+      if (!d)\n+\td = pop_list (&WEBS(SIMPLIFY_FAT));\n+      if (!d)\n+\td = pop_list (&WEBS(SIMPLIFY_SPILL));\n+      if (!d)\n+\tbreak;\n+      web = DLIST_WEB (d);\n+      ra_debug_msg (DUMP_PROCESS, \" simplifying web %3d, conflicts = %d\\n\",\n+\t\t web->id, web->num_conflicts);\n+      put_web (web, SELECT);\n+      for (wl = web->conflict_list; wl; wl = wl->next)\n+\t{\n+\t  struct web *pweb = wl->t;\n+\t  if (pweb->type != SELECT && pweb->type != COALESCED)\n+\t    {\n+\t      decrement_degree (pweb, 1 + web->add_hardregs);\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* Helper function to remove a move from the movelist of the web.  */\n+\n+static void\n+remove_move_1 (web, move)\n+     struct web *web;\n+     struct move *move;\n+{\n+  struct move_list *ml = web->moves;\n+  if (!ml)\n+    return;\n+  if (ml->move == move)\n+    {\n+      web->moves = ml->next;\n+      return;\n+    }\n+  for (; ml->next && ml->next->move != move; ml = ml->next) ;\n+  if (!ml->next)\n+    return;\n+  ml->next = ml->next->next;\n+}\n+\n+/* Remove a move from the movelist of the web.  Actually this is just a\n+   wrapper around remove_move_1(), making sure, the removed move really is\n+   not in the list anymore.  */\n+\n+static void\n+remove_move (web, move)\n+     struct web *web;\n+     struct move *move;\n+{\n+  struct move_list *ml;\n+  remove_move_1 (web, move);\n+  for (ml = web->moves; ml; ml = ml->next)\n+    if (ml->move == move)\n+      abort ();\n+}\n+\n+/* Merge the moves for the two webs into the first web's movelist.  */\n+\n+void\n+merge_moves (u, v)\n+     struct web *u, *v;\n+{\n+  regset seen;\n+  struct move_list *ml;\n+\n+  seen = BITMAP_XMALLOC ();\n+  for (ml = u->moves; ml; ml = ml->next)\n+    bitmap_set_bit (seen, INSN_UID (ml->move->insn));\n+  for (ml = v->moves; ml; ml = ml->next)\n+    {\n+      if (! bitmap_bit_p (seen, INSN_UID (ml->move->insn)))\n+        {\n+\t  ml->next = u->moves;\n+\t  u->moves = ml;\n+\t}\n+    }\n+  BITMAP_XFREE (seen);\n+  v->moves = NULL;\n+}\n+\n+/* Add a web to the simplify worklist, from the freeze worklist.  */\n+\n+static void\n+add_worklist (web)\n+     struct web *web;\n+{\n+  if (web->type != PRECOLORED && !web->moves\n+      && web->num_conflicts < NUM_REGS (web))\n+    {\n+      remove_list (web->dlink, &WEBS(FREEZE));\n+      put_web (web, SIMPLIFY);\n+    }\n+}\n+\n+/* Precolored node coalescing heuristic.  */\n+\n+static int\n+ok (target, source)\n+     struct web *target, *source;\n+{\n+  struct conflict_link *wl;\n+  int i;\n+  int color = source->color;\n+  int size;\n+\n+  /* Normally one would think, the next test wouldn't be needed.\n+     We try to coalesce S and T, and S has already a color, and we checked\n+     when processing the insns, that both have the same mode.  So naively\n+     we could conclude, that of course that mode was valid for this color.\n+     Hah.  But there is sparc.  Before reload there are copy insns\n+     (e.g. the ones copying arguments to locals) which happily refer to\n+     colors in invalid modes.  We can't coalesce those things.  */\n+  if (! HARD_REGNO_MODE_OK (source->color, GET_MODE (target->orig_x)))\n+    return 0;\n+\n+  /* Sanity for funny modes.  */\n+  size = HARD_REGNO_NREGS (color, GET_MODE (target->orig_x));\n+  if (!size)\n+    return 0;\n+\n+  /* We can't coalesce target with a precolored register which isn't in\n+     usable_regs.  */\n+  for (i = size; i--;)\n+    if (TEST_HARD_REG_BIT (never_use_colors, color + i)\n+\t|| !TEST_HARD_REG_BIT (target->usable_regs, color + i)\n+\t/* Before usually calling ok() at all, we already test, if the\n+\t   candidates conflict in sup_igraph.  But when wide webs are\n+\t   coalesced to hardregs, we only test the hardweb coalesced into.\n+\t   This is only the begin color.  When actually coalescing both,\n+\t   it will also take the following size colors, i.e. their webs.\n+\t   We nowhere checked if the candidate possibly conflicts with\n+\t   one of _those_, which is possible with partial conflicts,\n+\t   so we simply do it here (this does one bit-test more than\n+\t   necessary, the first color).  Note, that if X is precolored\n+\t   bit [X*num_webs + Y] can't be set (see add_conflict_edge()).  */\n+\t|| TEST_BIT (sup_igraph,\n+\t\t     target->id * num_webs + hardreg2web[color + i]->id))\n+      return 0;\n+\n+  for (wl = target->conflict_list; wl; wl = wl->next)\n+    {\n+      struct web *pweb = wl->t;\n+      if (pweb->type == SELECT || pweb->type == COALESCED)\n+\tcontinue;\n+\n+      /* Coalescing target (T) and source (S) is o.k, if for\n+\t all conflicts C of T it is true, that:\n+\t  1) C will be colored, or\n+\t  2) C is a hardreg (precolored), or\n+\t  3) C already conflicts with S too, or\n+\t  4) a web which contains C conflicts already with S.\n+\t XXX: we handle here only the special case of 4), that C is\n+\t a subreg, and the containing thing is the reg itself, i.e.\n+\t we dont handle the situation, were T conflicts with\n+\t (subreg:SI x 1), and S conflicts with (subreg:DI x 0), which\n+\t would be allowed also, as the S-conflict overlaps\n+\t the T-conflict.\n+         So, we first test the whole web for any of these conditions, and\n+         continue with the next C, if 1, 2 or 3 is true.  */\n+      if (pweb->num_conflicts < NUM_REGS (pweb)\n+\t  || pweb->type == PRECOLORED\n+\t  || TEST_BIT (igraph, igraph_index (source->id, pweb->id)) )\n+\tcontinue;\n+\n+      /* This is reached, if not one of 1, 2 or 3 was true.  In the case C has\n+         no subwebs, 4 can't be true either, so we can't coalesce S and T.  */\n+      if (wl->sub == NULL)\n+        return 0;\n+      else\n+\t{\n+\t  /* The main webs do _not_ conflict, only some parts of both.  This\n+\t     means, that 4 is possibly true, so we need to check this too.\n+\t     For this we go thru all sub conflicts between T and C, and see if\n+\t     the target part of C already conflicts with S.  When this is not\n+\t     the case we disallow coalescing.  */\n+\t  struct sub_conflict *sl;\n+\t  for (sl = wl->sub; sl; sl = sl->next)\n+\t    {\n+              if (!TEST_BIT (igraph, igraph_index (source->id, sl->t->id)))\n+\t        return 0;\n+\t    }\n+        }\n+    }\n+  return 1;\n+}\n+\n+/* Non-precolored node coalescing heuristic.  */\n+\n+static int\n+conservative (target, source)\n+     struct web *target, *source;\n+{\n+  unsigned int k;\n+  unsigned int loop;\n+  regset seen;\n+  struct conflict_link *wl;\n+  unsigned int num_regs = NUM_REGS (target); /* XXX */\n+\n+  /* k counts the resulting conflict weight, if target and source\n+     would be merged, and all low-degree neighbors would be\n+     removed.  */\n+  k = 0 * MAX (target->add_hardregs, source->add_hardregs);\n+  seen = BITMAP_XMALLOC ();\n+  for (loop = 0; loop < 2; loop++)\n+    for (wl = ((loop == 0) ? target : source)->conflict_list;\n+\t wl; wl = wl->next)\n+      {\n+\tstruct web *pweb = wl->t;\n+\tif (pweb->type != SELECT && pweb->type != COALESCED\n+\t    && pweb->num_conflicts >= NUM_REGS (pweb)\n+\t    && ! REGNO_REG_SET_P (seen, pweb->id))\n+\t  {\n+\t    SET_REGNO_REG_SET (seen, pweb->id);\n+\t    k += 1 + pweb->add_hardregs;\n+\t  }\n+      }\n+  BITMAP_XFREE (seen);\n+\n+  if (k >= num_regs)\n+    return 0;\n+  return 1;\n+}\n+\n+/* If the web is coalesced, return it's alias.  Otherwise, return what\n+   was passed in.  */\n+\n+struct web *\n+alias (web)\n+     struct web *web;\n+{\n+  while (web->type == COALESCED)\n+    web = web->alias;\n+  return web;\n+}\n+\n+/* Returns nonzero, if the TYPE belongs to one of those representing\n+   SIMPLIFY types.  */\n+\n+static inline unsigned int\n+simplify_p (type)\n+     enum node_type type;\n+{\n+  return type == SIMPLIFY || type == SIMPLIFY_SPILL || type == SIMPLIFY_FAT;\n+}\n+\n+/* Actually combine two webs, that can be coalesced.  */\n+\n+static void\n+combine (u, v)\n+     struct web *u, *v;\n+{\n+  int i;\n+  struct conflict_link *wl;\n+  if (u == v || v->type == COALESCED)\n+    abort ();\n+  if ((u->regno >= max_normal_pseudo) != (v->regno >= max_normal_pseudo))\n+    abort ();\n+  remove_web_from_list (v);\n+  put_web (v, COALESCED);\n+  v->alias = u;\n+  u->is_coalesced = 1;\n+  v->is_coalesced = 1;\n+  u->num_aliased += 1 + v->num_aliased;\n+  if (flag_ra_merge_spill_costs && u->type != PRECOLORED)\n+    u->spill_cost += v->spill_cost;\n+    /*u->spill_cost = MAX (u->spill_cost, v->spill_cost);*/\n+  merge_moves (u, v);\n+  /* combine add_hardregs's of U and V.  */\n+\n+  for (wl = v->conflict_list; wl; wl = wl->next)\n+    {\n+      struct web *pweb = wl->t;\n+      /* We don't strictly need to move conflicts between webs which are\n+\t already coalesced or selected, if we do iterated coalescing, or\n+\t better if we need not to be able to break aliases again.\n+\t I.e. normally we would use the condition\n+\t (pweb->type != SELECT && pweb->type != COALESCED).\n+\t But for now we simply merge all conflicts.  It doesn't take that\n+         much time.  */\n+      if (1)\n+\t{\n+\t  struct web *web = u;\n+\t  int nregs = 1 + v->add_hardregs;\n+\t  if (u->type == PRECOLORED)\n+\t    nregs = HARD_REGNO_NREGS (u->color, GET_MODE (v->orig_x));\n+\n+\t  /* For precolored U's we need to make conflicts between V's\n+\t     neighbors and as many hardregs from U as V needed if it gets\n+\t     color U.  For now we approximate this by V->add_hardregs, which\n+\t     could be too much in multi-length classes.  We should really\n+\t     count how many hardregs are needed for V with color U.  When U\n+\t     isn't precolored this loop breaks out after one iteration.  */\n+\t  for (i = 0; i < nregs; i++)\n+\t    {\n+\t      if (u->type == PRECOLORED)\n+\t\tweb = hardreg2web[i + u->color];\n+\t      if (wl->sub == NULL)\n+\t\trecord_conflict (web, pweb);\n+\t      else\n+\t\t{\n+\t\t  struct sub_conflict *sl;\n+\t\t  /* So, between V and PWEB there are sub_conflicts.  We\n+\t\t     need to relocate those conflicts to be between WEB (==\n+\t\t     U when it wasn't precolored) and PWEB.  In the case\n+\t\t     only a part of V conflicted with (part of) PWEB we\n+\t\t     nevertheless make the new conflict between the whole U\n+\t\t     and the (part of) PWEB.  Later we might try to find in\n+\t\t     U the correct subpart corresponding (by size and\n+\t\t     offset) to the part of V (sl->s) which was the source\n+\t\t     of the conflict.  */\n+\t\t  for (sl = wl->sub; sl; sl = sl->next)\n+\t\t    {\n+\t\t      /* Beware: sl->s is no subweb of web (== U) but of V.\n+\t\t\t We try to search a corresponding subpart of U.\n+\t\t\t If we found none we let it conflict with the whole U.\n+\t\t\t Note that find_subweb() only looks for mode and\n+\t\t\t subreg_byte of the REG rtx but not for the pseudo\n+\t\t\t reg number (otherwise it would be guaranteed to\n+\t\t\t _not_ find any subpart).  */\n+\t\t      struct web *sweb = NULL;\n+\t\t      if (SUBWEB_P (sl->s))\n+\t\t\tsweb = find_subweb (web, sl->s->orig_x);\n+\t\t      if (!sweb)\n+\t\t\tsweb = web;\n+\t\t      record_conflict (sweb, sl->t);\n+\t\t    }\n+\t\t}\n+\t      if (u->type != PRECOLORED)\n+\t\tbreak;\n+\t    }\n+\t  if (pweb->type != SELECT && pweb->type != COALESCED)\n+\t    decrement_degree (pweb, 1 + v->add_hardregs);\n+\t}\n+    }\n+\n+  /* Now merge the usable_regs together.  */\n+  /* XXX That merging might normally make it necessary to\n+     adjust add_hardregs, which also means to adjust neighbors.  This can\n+     result in making some more webs trivially colorable, (or the opposite,\n+     if this increases our add_hardregs).  Because we intersect the\n+     usable_regs it should only be possible to decrease add_hardregs.  So a\n+     conservative solution for now is to simply don't change it.  */\n+  u->use_my_regs = 1;\n+  AND_HARD_REG_SET (u->usable_regs, v->usable_regs);\n+  u->regclass = reg_class_subunion[u->regclass][v->regclass];\n+  /* Count number of possible hardregs.  This might make U a spillweb,\n+     but that could also happen, if U and V together had too many\n+     conflicts.  */\n+  u->num_freedom = hard_regs_count (u->usable_regs);\n+  u->num_freedom -= u->add_hardregs;\n+  /* The next would mean an invalid coalesced move (both webs have no\n+     possible hardreg in common), so abort.  */\n+  if (!u->num_freedom)\n+    abort();\n+\n+  if (u->num_conflicts >= NUM_REGS (u)\n+      && (u->type == FREEZE || simplify_p (u->type)))\n+    {\n+      remove_web_from_list (u);\n+      put_web (u, SPILL);\n+    }\n+\n+  /* We want the most relaxed combination of spill_temp state.\n+     I.e. if any was no spilltemp or a spilltemp2, the result is so too,\n+     otherwise if any is short, the result is too.  It remains, when both\n+     are normal spilltemps.  */\n+  if (v->spill_temp == 0)\n+    u->spill_temp = 0;\n+  else if (v->spill_temp == 2 && u->spill_temp != 0)\n+    u->spill_temp = 2;\n+  else if (v->spill_temp == 3 && u->spill_temp == 1)\n+    u->spill_temp = 3;\n+}\n+\n+/* Attempt to coalesce the first thing on the move worklist.\n+   This is used only for iterated coalescing.  */\n+\n+static void\n+coalesce ()\n+{\n+  struct dlist *d = pop_list (&mv_worklist);\n+  struct move *m = DLIST_MOVE (d);\n+  struct web *source = alias (m->source_web);\n+  struct web *target = alias (m->target_web);\n+\n+  if (target->type == PRECOLORED)\n+    {\n+      struct web *h = source;\n+      source = target;\n+      target = h;\n+    }\n+  if (source == target)\n+    {\n+      remove_move (source, m);\n+      put_move (m, MV_COALESCED);\n+      add_worklist (source);\n+    }\n+  else if (target->type == PRECOLORED\n+\t   || TEST_BIT (sup_igraph, source->id * num_webs + target->id)\n+\t   || TEST_BIT (sup_igraph, target->id * num_webs + source->id))\n+    {\n+      remove_move (source, m);\n+      remove_move (target, m);\n+      put_move (m, CONSTRAINED);\n+      add_worklist (source);\n+      add_worklist (target);\n+    }\n+  else if ((source->type == PRECOLORED && ok (target, source))\n+\t   || (source->type != PRECOLORED\n+\t       && conservative (target, source)))\n+    {\n+      remove_move (source, m);\n+      remove_move (target, m);\n+      put_move (m, MV_COALESCED);\n+      combine (source, target);\n+      add_worklist (source);\n+    }\n+  else\n+    put_move (m, ACTIVE);\n+}\n+\n+/* Freeze the moves associated with the web.  Used for iterated coalescing.  */\n+\n+static void\n+freeze_moves (web)\n+     struct web *web;\n+{\n+  struct move_list *ml, *ml_next;\n+  for (ml = web->moves; ml; ml = ml_next)\n+    {\n+      struct move *m = ml->move;\n+      struct web *src, *dest;\n+      ml_next = ml->next;\n+      if (m->type == ACTIVE)\n+\tremove_list (m->dlink, &mv_active);\n+      else\n+\tremove_list (m->dlink, &mv_worklist);\n+      put_move (m, FROZEN);\n+      remove_move (web, m);\n+      src = alias (m->source_web);\n+      dest = alias (m->target_web);\n+      src = (src == web) ? dest : src;\n+      remove_move (src, m);\n+      /* XXX GA use the original v, instead of alias(v) */\n+      if (!src->moves && src->num_conflicts < NUM_REGS (src))\n+\t{\n+\t  remove_list (src->dlink, &WEBS(FREEZE));\n+\t  put_web (src, SIMPLIFY);\n+\t}\n+    }\n+}\n+\n+/* Freeze the first thing on the freeze worklist (only for iterated\n+   coalescing).  */\n+\n+static void\n+freeze ()\n+{\n+  struct dlist *d = pop_list (&WEBS(FREEZE));\n+  put_web (DLIST_WEB (d), SIMPLIFY);\n+  freeze_moves (DLIST_WEB (d));\n+}\n+\n+/* The current spill heuristic.  Returns a number for a WEB.\n+   Webs with higher numbers are selected later.  */\n+\n+static unsigned HOST_WIDE_INT (*spill_heuristic) PARAMS ((struct web *));\n+\n+static unsigned HOST_WIDE_INT default_spill_heuristic PARAMS ((struct web *));\n+\n+/* Our default heuristic is similar to spill_cost / num_conflicts.\n+   Just scaled for integer arithmetic, and it favors coalesced webs,\n+   and webs which span more insns with deaths.  */\n+\n+static unsigned HOST_WIDE_INT\n+default_spill_heuristic (web)\n+     struct web *web;\n+{\n+  unsigned HOST_WIDE_INT ret;\n+  unsigned int divisor = 1;\n+  /* Make coalesce targets cheaper to spill, because they will be broken\n+     up again into smaller parts.  */\n+  if (flag_ra_break_aliases)\n+    divisor += web->num_aliased;\n+  divisor += web->num_conflicts;\n+  ret = ((web->spill_cost << 8) + divisor - 1) / divisor;\n+  /* It is better to spill webs that span more insns (deaths in our\n+     case) than other webs with the otherwise same spill_cost.  So make\n+     them a little bit cheaper.  Remember that spill_cost is unsigned.  */\n+  if (web->span_deaths < ret)\n+    ret -= web->span_deaths;\n+  return ret;\n+}\n+\n+/* Select the cheapest spill to be potentially spilled (we don't\n+   *actually* spill until we need to).  */\n+\n+static void\n+select_spill ()\n+{\n+  unsigned HOST_WIDE_INT best = (unsigned HOST_WIDE_INT) -1;\n+  struct dlist *bestd = NULL;\n+  unsigned HOST_WIDE_INT best2 = (unsigned HOST_WIDE_INT) -1;\n+  struct dlist *bestd2 = NULL;\n+  struct dlist *d;\n+  for (d = WEBS(SPILL); d; d = d->next)\n+    {\n+      struct web *w = DLIST_WEB (d);\n+      unsigned HOST_WIDE_INT cost = spill_heuristic (w);\n+      if ((!w->spill_temp) && cost < best)\n+\t{\n+\t  best = cost;\n+\t  bestd = d;\n+\t}\n+      /* Specially marked spill temps can be spilled.  Also coalesce\n+\t targets can.  Eventually they will be broken up later in the\n+\t colorizing process, so if we have nothing better take that.  */\n+      else if ((w->spill_temp == 2 || w->is_coalesced) && cost < best2)\n+\t{\n+\t  best2 = cost;\n+\t  bestd2 = d;\n+\t}\n+    }\n+  if (!bestd)\n+    {\n+      bestd = bestd2;\n+      best = best2;\n+    }\n+  if (!bestd)\n+    abort ();\n+\n+  /* Note the potential spill.  */\n+  DLIST_WEB (bestd)->was_spilled = 1;\n+  remove_list (bestd, &WEBS(SPILL));\n+  put_web (DLIST_WEB (bestd), SIMPLIFY);\n+  freeze_moves (DLIST_WEB (bestd));\n+  ra_debug_msg (DUMP_PROCESS, \" potential spill web %3d, conflicts = %d\\n\",\n+\t     DLIST_WEB (bestd)->id, DLIST_WEB (bestd)->num_conflicts);\n+}\n+\n+/* Given a set of forbidden colors to begin at, and a set of still\n+   free colors, and MODE, returns nonzero of color C is still usable.  */\n+\n+static int\n+color_usable_p (c, dont_begin_colors, free_colors, mode)\n+     int c;\n+     HARD_REG_SET dont_begin_colors, free_colors;\n+     enum machine_mode mode;\n+{\n+  if (!TEST_HARD_REG_BIT (dont_begin_colors, c)\n+      && TEST_HARD_REG_BIT (free_colors, c)\n+      && HARD_REGNO_MODE_OK (c, mode))\n+    {\n+      int i, size;\n+      size = HARD_REGNO_NREGS (c, mode);\n+      for (i = 1; i < size && TEST_HARD_REG_BIT (free_colors, c + i); i++);\n+      if (i == size)\n+\treturn 1;\n+    }\n+  return 0;\n+}\n+\n+/* Searches in FREE_COLORS for a block of hardregs of the right length\n+   for MODE, which doesn't begin at a hardreg mentioned in DONT_BEGIN_COLORS.\n+   If it needs more than one hardreg it prefers blocks beginning\n+   at an even hardreg, and only gives an odd begin reg if no other\n+   block could be found.  */\n+\n+int\n+get_free_reg (dont_begin_colors, free_colors, mode)\n+     HARD_REG_SET dont_begin_colors, free_colors;\n+     enum machine_mode mode;\n+{\n+  int c;\n+  int last_resort_reg = -1;\n+  int pref_reg = -1;\n+  int pref_reg_order = INT_MAX;\n+  int last_resort_reg_order = INT_MAX;\n+\n+  for (c = 0; c < FIRST_PSEUDO_REGISTER; c++)\n+    if (!TEST_HARD_REG_BIT (dont_begin_colors, c)\n+\t&& TEST_HARD_REG_BIT (free_colors, c)\n+\t&& HARD_REGNO_MODE_OK (c, mode))\n+      {\n+\tint i, size;\n+\tsize = HARD_REGNO_NREGS (c, mode);\n+\tfor (i = 1; i < size && TEST_HARD_REG_BIT (free_colors, c + i); i++);\n+\tif (i != size)\n+\t  {\n+\t    c += i;\n+\t    continue;\n+\t  }\n+\tif (i == size)\n+\t  {\n+\t    if (size < 2 || (c & 1) == 0)\n+\t      {\n+\t\tif (inv_reg_alloc_order[c] < pref_reg_order)\n+\t\t  {\n+\t\t    pref_reg = c;\n+\t\t    pref_reg_order = inv_reg_alloc_order[c];\n+\t\t  }\n+\t      }\n+\t    else if (inv_reg_alloc_order[c] < last_resort_reg_order)\n+\t      {\n+\t\tlast_resort_reg = c;\n+\t\tlast_resort_reg_order = inv_reg_alloc_order[c];\n+\t      }\n+\t  }\n+\telse\n+\t  c += i;\n+      }\n+  return pref_reg >= 0 ? pref_reg : last_resort_reg;\n+}\n+\n+/* Similar to get_free_reg(), but first search in colors provided\n+   by BIAS _and_ PREFER_COLORS, then in BIAS alone, then in PREFER_COLORS\n+   alone, and only then for any free color.  If flag_ra_biased is zero\n+   only do the last two steps.  */\n+\n+static int\n+get_biased_reg (dont_begin_colors, bias, prefer_colors, free_colors, mode)\n+     HARD_REG_SET dont_begin_colors, bias, prefer_colors, free_colors;\n+     enum machine_mode mode;\n+{\n+  int c = -1;\n+  HARD_REG_SET s;\n+  if (flag_ra_biased)\n+    {\n+      COPY_HARD_REG_SET (s, dont_begin_colors);\n+      IOR_COMPL_HARD_REG_SET (s, bias);\n+      IOR_COMPL_HARD_REG_SET (s, prefer_colors);\n+      c = get_free_reg (s, free_colors, mode);\n+      if (c >= 0)\n+\treturn c;\n+      COPY_HARD_REG_SET (s, dont_begin_colors);\n+      IOR_COMPL_HARD_REG_SET (s, bias);\n+      c = get_free_reg (s, free_colors, mode);\n+      if (c >= 0)\n+\treturn c;\n+    }\n+  COPY_HARD_REG_SET (s, dont_begin_colors);\n+  IOR_COMPL_HARD_REG_SET (s, prefer_colors);\n+  c = get_free_reg (s, free_colors, mode);\n+  if (c >= 0)\n+      return c;\n+  c = get_free_reg (dont_begin_colors, free_colors, mode);\n+  return c;\n+}\n+\n+/* Counts the number of non-overlapping bitblocks of length LEN\n+   in FREE_COLORS.  */\n+\n+static int\n+count_long_blocks (free_colors, len)\n+     HARD_REG_SET free_colors;\n+     int len;\n+{\n+  int i, j;\n+  int count = 0;\n+  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+    {\n+      if (!TEST_HARD_REG_BIT (free_colors, i))\n+\tcontinue;\n+      for (j = 1; j < len; j++)\n+\tif (!TEST_HARD_REG_BIT (free_colors, i + j))\n+\t  break;\n+      /* Bits [i .. i+j-1] are free.  */\n+      if (j == len)\n+\tcount++;\n+      i += j - 1;\n+    }\n+  return count;\n+}\n+\n+/* Given a hardreg set S, return a string representing it.\n+   Either as 0/1 string, or as hex value depending on the implementation\n+   of hardreg sets.  Note that this string is statically allocated.  */\n+\n+static char *\n+hardregset_to_string (s)\n+     HARD_REG_SET s;\n+{\n+  static char string[/*FIRST_PSEUDO_REGISTER + 30*/1024];\n+#if FIRST_PSEUDO_REGISTER <= HOST_BITS_PER_WIDE_INT\n+  sprintf (string, \"%x\", s);\n+#else\n+  char *c = string;\n+  int i,j;\n+  c += sprintf (c, \"{ \");\n+  for (i = 0;i < HARD_REG_SET_LONGS; i++)\n+    {\n+      for (j = 0; j < HOST_BITS_PER_WIDE_INT; j++)\n+\t  c += sprintf (c, \"%s\", ( 1 << j) & s[i] ? \"1\" : \"0\");\n+      c += sprintf (c, \"%s\", i ? \", \" : \"\");\n+    }\n+  c += sprintf (c, \" }\");\n+#endif\n+  return string;\n+}\n+\n+/* For WEB, look at its already colored neighbors, and calculate\n+   the set of hardregs which is not allowed as color for WEB.  Place\n+   that set int *RESULT.  Note that the set of forbidden begin colors\n+   is not the same as all colors taken up by neighbors.  E.g. suppose\n+   two DImode webs, but only the lo-part from one conflicts with the\n+   hipart from the other, and suppose the other gets colors 2 and 3\n+   (it needs two SImode hardregs).  Now the first can take also color\n+   1 or 2, although in those cases there's a partial overlap.  Only\n+   3 can't be used as begin color.  */\n+\n+static void\n+calculate_dont_begin (web, result)\n+     struct web *web;\n+     HARD_REG_SET *result;\n+{\n+  struct conflict_link *wl;\n+  HARD_REG_SET dont_begin;\n+  /* The bits set in dont_begin correspond to the hardregs, at which\n+     WEB may not begin.  This differs from the set of _all_ hardregs which\n+     are taken by WEB's conflicts in the presence of wide webs, where only\n+     some parts conflict with others.  */\n+  CLEAR_HARD_REG_SET (dont_begin);\n+  for (wl = web->conflict_list; wl; wl = wl->next)\n+    {\n+      struct web *w;\n+      struct web *ptarget = alias (wl->t);\n+      struct sub_conflict *sl = wl->sub;\n+      w = sl ? sl->t : wl->t;\n+      while (w)\n+\t{\n+\t  if (ptarget->type == COLORED || ptarget->type == PRECOLORED)\n+\t    {\n+\t      struct web *source = (sl) ? sl->s : web;\n+\t      unsigned int tsize = HARD_REGNO_NREGS (ptarget->color,\n+\t\t\t\t\t\t     GET_MODE (w->orig_x));\n+\t      /* ssize is only a first guess for the size.  */\n+\t      unsigned int ssize = HARD_REGNO_NREGS (ptarget->color, GET_MODE\n+\t\t\t\t\t             (source->orig_x));\n+\t      unsigned int tofs = 0;\n+\t      unsigned int sofs = 0;\n+\t      /* C1 and C2 can become negative, so unsigned\n+\t\t would be wrong.  */\n+\t      int c1, c2;\n+\n+\t      if (SUBWEB_P (w)\n+\t\t  && GET_MODE_SIZE (GET_MODE (w->orig_x)) >= UNITS_PER_WORD)\n+\t\ttofs = (SUBREG_BYTE (w->orig_x) / UNITS_PER_WORD);\n+\t      if (SUBWEB_P (source)\n+\t\t  && GET_MODE_SIZE (GET_MODE (source->orig_x))\n+\t\t     >= UNITS_PER_WORD)\n+\t\tsofs = (SUBREG_BYTE (source->orig_x) / UNITS_PER_WORD);\n+\t      c1 = ptarget->color + tofs - sofs - ssize + 1;\n+\t      c2 = ptarget->color + tofs + tsize - 1 - sofs;\n+\t      if (c2 >= 0)\n+\t\t{\n+\t\t  if (c1 < 0)\n+\t\t    c1 = 0;\n+\t\t  /* Because ssize was only guessed above, which influenced our\n+\t\t     begin color (c1), we need adjustment, if for that color\n+\t\t     another size would be needed.  This is done by moving\n+\t\t     c1 to a place, where the last of sources hardregs does not\n+\t\t     overlap the first of targets colors.  */\n+\t\t  while (c1 + sofs\n+\t\t\t + HARD_REGNO_NREGS (c1, GET_MODE (source->orig_x)) - 1\n+\t\t\t < ptarget->color + tofs)\n+\t\t    c1++;\n+\t\t  while (c1 > 0 && c1 + sofs\n+\t\t\t + HARD_REGNO_NREGS (c1, GET_MODE (source->orig_x)) - 1\n+\t\t\t > ptarget->color + tofs)\n+\t\t    c1--;\n+\t\t  for (; c1 <= c2; c1++)\n+\t\t    SET_HARD_REG_BIT (dont_begin, c1);\n+\t\t}\n+\t    }\n+\t  /* The next if() only gets true, if there was no wl->sub at all, in\n+\t     which case we are only making one go thru this loop with W being\n+\t     a whole web.  */\n+\t  if (!sl)\n+\t    break;\n+\t  sl = sl->next;\n+\t  w = sl ? sl->t : NULL;\n+\t}\n+    }\n+  COPY_HARD_REG_SET (*result, dont_begin);\n+}\n+\n+/* Try to assign a color to WEB.  If HARD if nonzero, we try many\n+   tricks to get it one color, including respilling already colored\n+   neighbors.\n+\n+   We also trie very hard, to not constrain the uncolored non-spill\n+   neighbors, which need more hardregs than we.  Consider a situation, 2\n+   hardregs free for us (0 and 1), and one of our neighbors needs 2\n+   hardregs, and only conflicts with us.  There are 3 hardregs at all.  Now\n+   a simple minded method might choose 1 as color for us.  Then our neighbor\n+   has two free colors (0 and 2) as it should, but they are not consecutive,\n+   so coloring it later would fail.  This leads to nasty problems on\n+   register starved machines, so we try to avoid this.  */\n+\n+static void\n+colorize_one_web (web, hard)\n+     struct web *web;\n+     int hard;\n+{\n+  struct conflict_link *wl;\n+  HARD_REG_SET colors, dont_begin;\n+  int c = -1;\n+  int bestc = -1;\n+  int neighbor_needs= 0;\n+  struct web *fat_neighbor = NULL;\n+  struct web *fats_parent = NULL;\n+  int num_fat = 0;\n+  int long_blocks = 0;\n+  int best_long_blocks = -1;\n+  HARD_REG_SET fat_colors;\n+  HARD_REG_SET bias;\n+\n+  if (web->regno >= max_normal_pseudo)\n+    hard = 0;\n+\n+  /* First we want to know the colors at which we can't begin.  */\n+  calculate_dont_begin (web, &dont_begin);\n+  CLEAR_HARD_REG_SET (bias);\n+\n+  /* Now setup the set of colors used by our neighbors neighbors,\n+     and search the biggest noncolored neighbor.  */\n+  neighbor_needs = web->add_hardregs + 1;\n+  for (wl = web->conflict_list; wl; wl = wl->next)\n+    {\n+      struct web *w;\n+      struct web *ptarget = alias (wl->t);\n+      struct sub_conflict *sl = wl->sub;\n+      IOR_HARD_REG_SET (bias, ptarget->bias_colors);\n+      w = sl ? sl->t : wl->t;\n+      if (ptarget->type != COLORED && ptarget->type != PRECOLORED\n+\t  && !ptarget->was_spilled)\n+        while (w)\n+\t  {\n+\t    if (find_web_for_subweb (w)->type != COALESCED\n+\t\t&& w->add_hardregs >= neighbor_needs)\n+\t      {\n+\t\tneighbor_needs = w->add_hardregs;\n+\t\tfat_neighbor = w;\n+\t\tfats_parent = ptarget;\n+\t\tnum_fat++;\n+\t      }\n+\t    if (!sl)\n+\t      break;\n+\t    sl = sl->next;\n+\t    w = sl ? sl->t : NULL;\n+\t  }\n+    }\n+\n+  ra_debug_msg (DUMP_COLORIZE, \"colorize web %d [don't begin at %s]\", web->id,\n+             hardregset_to_string (dont_begin));\n+\n+  /* If there are some fat neighbors, remember their usable regs,\n+     and how many blocks are free in it for that neighbor.  */\n+  if (num_fat)\n+    {\n+      COPY_HARD_REG_SET (fat_colors, fats_parent->usable_regs);\n+      long_blocks = count_long_blocks (fat_colors, neighbor_needs + 1);\n+    }\n+\n+  /* We break out, if we found a color which doesn't constrain\n+     neighbors, or if we can't find any colors.  */\n+  while (1)\n+    {\n+      HARD_REG_SET call_clobbered;\n+\n+      /* Here we choose a hard-reg for the current web.  For non spill\n+         temporaries we first search in the hardregs for it's prefered\n+\t class, then, if we found nothing appropriate, in those of the\n+\t alternate class.  For spill temporaries we only search in\n+\t usable_regs of this web (which is probably larger than that of\n+\t the preferred or alternate class).  All searches first try to\n+\t find a non-call-clobbered hard-reg.\n+         XXX this should be more finegraned... First look into preferred\n+         non-callclobbered hardregs, then _if_ the web crosses calls, in\n+         alternate non-cc hardregs, and only _then_ also in preferred cc\n+         hardregs (and alternate ones).  Currently we don't track the number\n+         of calls crossed for webs.  We should.  */\n+      if (web->use_my_regs)\n+\t{\n+\t  COPY_HARD_REG_SET (colors, web->usable_regs);\n+\t  AND_HARD_REG_SET (colors,\n+\t\t\t    usable_regs[reg_preferred_class (web->regno)]);\n+\t}\n+      else\n+\tCOPY_HARD_REG_SET (colors,\n+\t\t\t   usable_regs[reg_preferred_class (web->regno)]);\n+#ifdef CLASS_CANNOT_CHANGE_MODE\n+      if (web->mode_changed)\n+        AND_COMPL_HARD_REG_SET (colors, reg_class_contents[\n+\t\t\t          (int) CLASS_CANNOT_CHANGE_MODE]);\n+#endif\n+      COPY_HARD_REG_SET (call_clobbered, colors);\n+      AND_HARD_REG_SET (call_clobbered, call_used_reg_set);\n+\n+      /* If this web got a color in the last pass, try to give it the\n+\t same color again.  This will to much better colorization\n+\t down the line, as we spilled for a certain coloring last time.  */\n+      if (web->old_color)\n+\t{\n+\t  c = web->old_color - 1;\n+\t  if (!color_usable_p (c, dont_begin, colors,\n+\t\t\t       PSEUDO_REGNO_MODE (web->regno)))\n+\t    c = -1;\n+\t}\n+      else\n+\tc = -1;\n+      if (c < 0)\n+\tc = get_biased_reg (dont_begin, bias, web->prefer_colors,\n+\t\t\t    call_clobbered, PSEUDO_REGNO_MODE (web->regno));\n+      if (c < 0)\n+\tc = get_biased_reg (dont_begin, bias, web->prefer_colors,\n+\t\t\t  colors, PSEUDO_REGNO_MODE (web->regno));\n+\n+      if (c < 0)\n+\t{\n+\t  if (web->use_my_regs)\n+\t    IOR_HARD_REG_SET (colors, web->usable_regs);\n+\t  else\n+\t    IOR_HARD_REG_SET (colors, usable_regs\n+\t\t\t      [reg_alternate_class (web->regno)]);\n+#ifdef CLASS_CANNOT_CHANGE_MODE\n+\t  if (web->mode_changed)\n+\t    AND_COMPL_HARD_REG_SET (colors, reg_class_contents[\n+\t\t\t\t      (int) CLASS_CANNOT_CHANGE_MODE]);\n+#endif\n+\t  COPY_HARD_REG_SET (call_clobbered, colors);\n+\t  AND_HARD_REG_SET (call_clobbered, call_used_reg_set);\n+\n+\t  c = get_biased_reg (dont_begin, bias, web->prefer_colors,\n+\t\t\t    call_clobbered, PSEUDO_REGNO_MODE (web->regno));\n+\t  if (c < 0)\n+\t    c = get_biased_reg (dont_begin, bias, web->prefer_colors,\n+\t\t\t      colors, PSEUDO_REGNO_MODE (web->regno));\n+\t}\n+      if (c < 0)\n+\tbreak;\n+      if (bestc < 0)\n+        bestc = c;\n+      /* If one of the yet uncolored neighbors, which is not a potential\n+\t spill needs a block of hardregs be sure, not to destroy such a block\n+\t by coloring one reg in the middle.  */\n+      if (num_fat)\n+\t{\n+\t  int i;\n+\t  int new_long;\n+\t  HARD_REG_SET colors1;\n+\t  COPY_HARD_REG_SET (colors1, fat_colors);\n+\t  for (i = 0; i < 1 + web->add_hardregs; i++)\n+\t    CLEAR_HARD_REG_BIT (colors1, c + i);\n+\t  new_long = count_long_blocks (colors1, neighbor_needs + 1);\n+\t  /* If we changed the number of long blocks, and it's now smaller\n+\t     than needed, we try to avoid this color.  */\n+\t  if (long_blocks != new_long && new_long < num_fat)\n+\t    {\n+\t      if (new_long > best_long_blocks)\n+\t\t{\n+\t\t  best_long_blocks = new_long;\n+\t\t  bestc = c;\n+\t\t}\n+\t      SET_HARD_REG_BIT (dont_begin, c);\n+\t      ra_debug_msg (DUMP_COLORIZE, \" avoid %d\", c);\n+\t    }\n+\t  else\n+\t    /* We found a color which doesn't destroy a block.  */\n+\t    break;\n+\t}\n+      /* If we havee no fat neighbors, the current color won't become\n+\t \"better\", so we've found it.  */\n+      else\n+\tbreak;\n+    }\n+  ra_debug_msg (DUMP_COLORIZE, \" --> got %d\", c < 0 ? bestc : c);\n+  if (bestc >= 0 && c < 0 && !web->was_spilled)\n+    {\n+      /* This is a non-potential-spill web, which got a color, which did\n+\t destroy a hardreg block for one of it's neighbors.  We color\n+\t this web anyway and hope for the best for the neighbor, if we are\n+\t a spill temp.  */\n+      if (1 || web->spill_temp)\n+        c = bestc;\n+      ra_debug_msg (DUMP_COLORIZE, \" [constrains neighbors]\");\n+    }\n+  ra_debug_msg (DUMP_COLORIZE, \"\\n\");\n+\n+  if (c < 0)\n+    {\n+      /* Guard against a simplified node being spilled.  */\n+      /* Don't abort.  This can happen, when e.g. enough registers\n+\t are available in colors, but they are not consecutive.  This is a\n+\t very serious issue if this web is a short live one, because\n+\t even if we spill this one here, the situation won't become better\n+\t in the next iteration.  It probably will have the same conflicts,\n+\t those will have the same colors, and we would come here again, for\n+\t all parts, in which this one gets splitted by the spill.  This\n+\t can result in endless iteration spilling the same register again and\n+\t again.  That's why we try to find a neighbor, which spans more\n+\t instructions that ourself, and got a color, and try to spill _that_.\n+\n+\t if (DLIST_WEB (d)->was_spilled < 0)\n+\t abort (); */\n+      if (hard && (!web->was_spilled || web->spill_temp))\n+\t{\n+\t  unsigned int loop;\n+\t  struct web *try = NULL;\n+\t  struct web *candidates[8];\n+\n+\t  ra_debug_msg (DUMP_COLORIZE, \"  *** %d spilled, although %s ***\\n\",\n+\t\t     web->id, web->spill_temp ? \"spilltemp\" : \"non-spill\");\n+\t  /* We make multiple passes over our conflicts, first trying to\n+\t     spill those webs, which only got a color by chance, but\n+\t     were potential spill ones, and if that isn't enough, in a second\n+\t     pass also to spill normal colored webs.  If we still didn't find\n+\t     a candidate, but we are a spill-temp, we make a third pass\n+\t     and include also webs, which were targets for coalescing, and\n+\t     spill those.  */\n+\t  memset (candidates, 0, sizeof candidates);\n+#define set_cand(i, w) \\\n+\t  do { \\\n+\t      if (!candidates[(i)] \\\n+\t\t  || (candidates[(i)]->spill_cost < (w)->spill_cost)) \\\n+\t\tcandidates[(i)] = (w); \\\n+\t  } while (0)\n+\t  for (wl = web->conflict_list; wl; wl = wl->next)\n+\t    {\n+\t      struct web *w = wl->t;\n+\t      struct web *aw = alias (w);\n+\t      /* If we are a spill-temp, we also look at webs coalesced\n+\t\t to precolored ones.  Otherwise we only look at webs which\n+\t\t themself were colored, or coalesced to one.  */\n+\t      if (aw->type == PRECOLORED && w != aw && web->spill_temp\n+\t\t  && flag_ra_optimistic_coalescing)\n+\t\t{\n+\t\t  if (!w->spill_temp)\n+\t\t    set_cand (4, w);\n+\t\t  else if (web->spill_temp == 2\n+\t\t\t   && w->spill_temp == 2\n+\t\t\t   && w->spill_cost < web->spill_cost)\n+\t\t    set_cand (5, w);\n+\t\t  else if (web->spill_temp != 2\n+\t\t\t   && (w->spill_temp == 2\n+\t\t\t       || w->spill_cost < web->spill_cost))\n+\t\t    set_cand (6, w);\n+\t\t  continue;\n+\t\t}\n+\t      if (aw->type != COLORED)\n+\t\tcontinue;\n+\t      if (w->type == COLORED && !w->spill_temp && !w->is_coalesced\n+\t\t  && w->was_spilled)\n+\t\t{\n+\t\t  if (w->spill_cost < web->spill_cost)\n+\t\t    set_cand (0, w);\n+\t\t  else if (web->spill_temp)\n+\t\t    set_cand (1, w);\n+\t\t}\n+\t      if (w->type == COLORED && !w->spill_temp && !w->is_coalesced\n+\t\t  && !w->was_spilled)\n+\t\t{\n+\t\t  if (w->spill_cost < web->spill_cost)\n+\t\t    set_cand (2, w);\n+\t\t  else if (web->spill_temp && web->spill_temp != 2)\n+\t\t    set_cand (3, w);\n+\t\t}\n+\t      if (web->spill_temp)\n+\t\t{\n+\t\t  if (w->type == COLORED && w->spill_temp == 2\n+\t\t      && !w->is_coalesced\n+\t\t      && (w->spill_cost < web->spill_cost\n+\t\t\t  || web->spill_temp != 2))\n+\t\t    set_cand (4, w);\n+\t\t  if (!aw->spill_temp)\n+\t\t    set_cand (5, aw);\n+\t\t  if (aw->spill_temp == 2\n+\t\t      && (aw->spill_cost < web->spill_cost\n+\t\t\t  || web->spill_temp != 2))\n+\t\t    set_cand (6, aw);\n+\t\t  /* For boehm-gc/misc.c.  If we are a difficult spilltemp,\n+\t\t     also coalesced neighbors are a chance, _even_ if they\n+\t\t     too are spilltemps.  At least their coalscing can be\n+\t\t     broken up, which may be reset usable_regs, and makes\n+\t\t     it easier colorable.  */\n+\t\t  if (web->spill_temp != 2 && aw->is_coalesced\n+\t\t      && flag_ra_optimistic_coalescing)\n+\t\t    set_cand (7, aw);\n+\t\t}\n+\t    }\n+\t  for (loop = 0; try == NULL && loop < 8; loop++)\n+\t    if (candidates[loop])\n+\t      try = candidates[loop];\n+#undef set_cand\n+\t  if (try)\n+\t    {\n+\t      int old_c = try->color;\n+\t      if (try->type == COALESCED)\n+\t\t{\n+\t\t  if (alias (try)->type != PRECOLORED)\n+\t\t    abort ();\n+\t\t  ra_debug_msg (DUMP_COLORIZE, \"  breaking alias %d -> %d\\n\",\n+\t\t\t     try->id, alias (try)->id);\n+\t\t  break_precolored_alias (try);\n+\t\t  colorize_one_web (web, hard);\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  remove_list (try->dlink, &WEBS(COLORED));\n+\t\t  put_web (try, SPILLED);\n+\t\t  /* Now try to colorize us again.  Can recursively make other\n+\t\t     webs also spill, until there are no more unspilled\n+\t\t     neighbors.  */\n+\t\t  ra_debug_msg (DUMP_COLORIZE, \"  trying to spill %d\\n\", try->id);\n+\t\t  colorize_one_web (web, hard);\n+\t\t  if (web->type != COLORED)\n+\t\t    {\n+\t\t      /* We tried recursively to spill all already colored\n+\t\t\t neighbors, but we are still uncolorable.  So it made\n+\t\t\t no sense to spill those neighbors.  Recolor them.  */\n+\t\t      remove_list (try->dlink, &WEBS(SPILLED));\n+\t\t      put_web (try, COLORED);\n+\t\t      try->color = old_c;\n+\t\t      ra_debug_msg (DUMP_COLORIZE,\n+\t\t\t\t    \"  spilling %d was useless\\n\", try->id);\n+\t\t    }\n+\t\t  else\n+\t\t    {\n+\t\t      ra_debug_msg (DUMP_COLORIZE,\n+\t\t\t\t    \"  to spill %d was a good idea\\n\",\n+\t\t\t\t    try->id);\n+\t\t      remove_list (try->dlink, &WEBS(SPILLED));\n+\t\t      if (try->was_spilled)\n+\t\t\tcolorize_one_web (try, 0);\n+\t\t      else\n+\t\t\tcolorize_one_web (try, hard - 1);\n+\t\t    }\n+\t\t}\n+\t    }\n+\t  else\n+\t    /* No more chances to get a color, so give up hope and\n+\t       spill us.  */\n+\t    put_web (web, SPILLED);\n+\t}\n+      else\n+        put_web (web, SPILLED);\n+    }\n+  else\n+    {\n+      put_web (web, COLORED);\n+      web->color = c;\n+      if (flag_ra_biased)\n+\t{\n+\t  int nregs = HARD_REGNO_NREGS (c, GET_MODE (web->orig_x));\n+\t  for (wl = web->conflict_list; wl; wl = wl->next)\n+\t    {\n+\t      struct web *ptarget = alias (wl->t);\n+\t      int i;\n+\t      for (i = 0; i < nregs; i++)\n+\t\tSET_HARD_REG_BIT (ptarget->bias_colors, c + i);\n+\t    }\n+\t}\n+    }\n+  if (web->regno >= max_normal_pseudo && web->type == SPILLED)\n+    {\n+      web->color = an_unusable_color;\n+      remove_list (web->dlink, &WEBS(SPILLED));\n+      put_web (web, COLORED);\n+    }\n+  if (web->type == SPILLED && flag_ra_optimistic_coalescing\n+      && web->is_coalesced)\n+    {\n+      ra_debug_msg (DUMP_COLORIZE, \"breaking aliases to web %d:\", web->id);\n+      restore_conflicts_from_coalesce (web);\n+      break_aliases_to_web (web);\n+      insert_coalesced_conflicts ();\n+      ra_debug_msg (DUMP_COLORIZE, \"\\n\");\n+      remove_list (web->dlink, &WEBS(SPILLED));\n+      put_web (web, SELECT);\n+      web->color = -1;\n+    }\n+}\n+\n+/* Assign the colors to all nodes on the select stack.  And update the\n+   colors of coalesced webs.  */\n+\n+static void\n+assign_colors ()\n+{\n+  struct dlist *d;\n+\n+  while (WEBS(SELECT))\n+    {\n+      struct web *web;\n+      d = pop_list (&WEBS(SELECT));\n+      web = DLIST_WEB (d);\n+      colorize_one_web (DLIST_WEB (d), 1);\n+    }\n+\n+  for (d = WEBS(COALESCED); d; d = d->next)\n+    {\n+      struct web *a = alias (DLIST_WEB (d));\n+      DLIST_WEB (d)->color = a->color;\n+    }\n+}\n+\n+/* WEB is a spilled web.  Look if we can improve the cost of the graph,\n+   by coloring WEB, even if we then need to spill some of it's neighbors.\n+   For this we calculate the cost for each color C, that results when we\n+   _would_ give WEB color C (i.e. the cost of the then spilled neighbors).\n+   If the lowest cost among them is smaller than the spillcost of WEB, we\n+   do that recoloring, and instead spill the neighbors.\n+\n+   This can sometime help, when due to irregularities in register file,\n+   and due to multi word pseudos, the colorization is suboptimal.  But\n+   be aware, that currently this pass is quite slow.  */\n+\n+static void\n+try_recolor_web (web)\n+     struct web *web;\n+{\n+  struct conflict_link *wl;\n+  unsigned HOST_WIDE_INT *cost_neighbors;\n+  unsigned int *min_color;\n+  int newcol, c;\n+  HARD_REG_SET precolored_neighbors, spill_temps;\n+  HARD_REG_SET possible_begin, wide_seen;\n+  cost_neighbors = (unsigned HOST_WIDE_INT *)\n+    xcalloc (FIRST_PSEUDO_REGISTER, sizeof (cost_neighbors[0]));\n+  /* For each hard-regs count the number of preceding hardregs, which\n+     would overlap this color, if used in WEB's mode.  */\n+  min_color = (unsigned int *) xcalloc (FIRST_PSEUDO_REGISTER, sizeof (int));\n+  CLEAR_HARD_REG_SET (possible_begin);\n+  for (c = 0; c < FIRST_PSEUDO_REGISTER; c++)\n+    {\n+      int i, nregs;\n+      if (!HARD_REGNO_MODE_OK (c, GET_MODE (web->orig_x)))\n+\tcontinue;\n+      nregs = HARD_REGNO_NREGS (c, GET_MODE (web->orig_x));\n+      for (i = 0; i < nregs; i++)\n+\tif (!TEST_HARD_REG_BIT (web->usable_regs, c + i))\n+\t  break;\n+      if (i < nregs || nregs == 0)\n+\tcontinue;\n+      SET_HARD_REG_BIT (possible_begin, c);\n+      for (; nregs--;)\n+\tif (!min_color[c + nregs])\n+\t  min_color[c + nregs] = 1 + c;\n+    }\n+  CLEAR_HARD_REG_SET (precolored_neighbors);\n+  CLEAR_HARD_REG_SET (spill_temps);\n+  CLEAR_HARD_REG_SET (wide_seen);\n+  for (wl = web->conflict_list; wl; wl = wl->next)\n+    {\n+      HARD_REG_SET dont_begin;\n+      struct web *web2 = alias (wl->t);\n+      struct conflict_link *nn;\n+      int c1, c2;\n+      int wide_p = 0;\n+      if (wl->t->type == COALESCED || web2->type != COLORED)\n+\t{\n+\t  if (web2->type == PRECOLORED)\n+\t    {\n+\t      c1 = min_color[web2->color];\n+\t      c1 = (c1 == 0) ? web2->color : (c1 - 1);\n+\t      c2 = web2->color;\n+\t      for (; c1 <= c2; c1++)\n+\t        SET_HARD_REG_BIT (precolored_neighbors, c1);\n+\t    }\n+\t  continue;\n+\t}\n+      /* Mark colors for which some wide webs are involved.  For\n+\t those the independent sets are not simply one-node graphs, so\n+\t they can't be recolored independ from their neighborhood.  This\n+\t means, that our cost calculation can be incorrect (assuming it\n+\t can avoid spilling a web because it thinks some colors are available,\n+\t although it's neighbors which itself need recoloring might take\n+\t away exactly those colors).  */\n+      if (web2->add_hardregs)\n+\twide_p = 1;\n+      for (nn = web2->conflict_list; nn && !wide_p; nn = nn->next)\n+\tif (alias (nn->t)->add_hardregs)\n+\t  wide_p = 1;\n+      calculate_dont_begin (web2, &dont_begin);\n+      c1 = min_color[web2->color];\n+      /* Note that min_color[] contains 1-based values (zero means\n+\t undef).  */\n+      c1 = c1 == 0 ? web2->color : (c1 - 1);\n+      c2 = web2->color + HARD_REGNO_NREGS (web2->color, GET_MODE\n+\t\t\t\t\t   (web2->orig_x)) - 1;\n+      for (; c1 <= c2; c1++)\n+\tif (TEST_HARD_REG_BIT (possible_begin, c1))\n+\t  {\n+\t    int nregs;\n+\t    HARD_REG_SET colors;\n+\t    nregs = HARD_REGNO_NREGS (c1, GET_MODE (web->orig_x));\n+\t    COPY_HARD_REG_SET (colors, web2->usable_regs);\n+\t    for (; nregs--;)\n+\t      CLEAR_HARD_REG_BIT (colors, c1 + nregs);\n+\t    if (wide_p)\n+\t      SET_HARD_REG_BIT (wide_seen, c1);\n+\t    if (get_free_reg (dont_begin, colors,\n+\t\t\t      GET_MODE (web2->orig_x)) < 0)\n+\t      {\n+\t\tif (web2->spill_temp)\n+\t\t  SET_HARD_REG_BIT (spill_temps, c1);\n+\t\telse\n+\t\t  cost_neighbors[c1] += web2->spill_cost;\n+\t      }\n+\t  }\n+    }\n+  newcol = -1;\n+  for (c = 0; c < FIRST_PSEUDO_REGISTER; c++)\n+    if (TEST_HARD_REG_BIT (possible_begin, c)\n+\t&& !TEST_HARD_REG_BIT (precolored_neighbors, c)\n+\t&& !TEST_HARD_REG_BIT (spill_temps, c)\n+\t&& (newcol == -1\n+\t    || cost_neighbors[c] < cost_neighbors[newcol]))\n+      newcol = c;\n+  if (newcol >= 0 && cost_neighbors[newcol] < web->spill_cost)\n+    {\n+      int nregs = HARD_REGNO_NREGS (newcol, GET_MODE (web->orig_x));\n+      unsigned HOST_WIDE_INT cost = 0;\n+      int *old_colors;\n+      struct conflict_link *wl_next;\n+      ra_debug_msg (DUMP_COLORIZE, \"try to set web %d to color %d\\n\", web->id,\n+\t\t newcol);\n+      remove_list (web->dlink, &WEBS(SPILLED));\n+      put_web (web, COLORED);\n+      web->color = newcol;\n+      old_colors = (int *) xcalloc (num_webs, sizeof (int));\n+      for (wl = web->conflict_list; wl; wl = wl_next)\n+\t{\n+\t  struct web *web2 = alias (wl->t);\n+\t  /* If web2 is a coalesce-target, and will become spilled\n+\t     below in colorize_one_web(), and the current conflict wl\n+\t     between web and web2 was only the result of that coalescing\n+\t     this conflict will be deleted, making wl invalid.  So save\n+\t     the next conflict right now.  Note that if web2 has indeed\n+\t     such state, then wl->next can not be deleted in this\n+\t     iteration.  */\n+\t  wl_next = wl->next;\n+\t  if (web2->type == COLORED)\n+\t    {\n+\t      int nregs2 = HARD_REGNO_NREGS (web2->color, GET_MODE\n+\t\t\t\t\t     (web2->orig_x));\n+\t      if (web->color >= web2->color + nregs2\n+\t\t  || web2->color >= web->color + nregs)\n+\t\tcontinue;\n+\t      old_colors[web2->id] = web2->color + 1;\n+\t      web2->color = -1;\n+\t      remove_list (web2->dlink, &WEBS(COLORED));\n+\t      web2->type = SELECT;\n+\t      /* Allow webs to be spilled.  */\n+\t      if (web2->spill_temp == 0 || web2->spill_temp == 2)\n+\t\tweb2->was_spilled = 1;\n+\t      colorize_one_web (web2, 1);\n+\t      if (web2->type == SPILLED)\n+\t\tcost += web2->spill_cost;\n+\t    }\n+\t}\n+      /* The actual cost may be smaller than the guessed one, because\n+\t partial conflicts could result in some conflicting webs getting\n+\t a color, where we assumed it must be spilled.  See the comment\n+         above what happens, when wide webs are involved, and why in that\n+         case there might actually be some webs spilled although thought to\n+         be colorable.  */\n+      if (cost > cost_neighbors[newcol]\n+\t  && nregs == 1 && !TEST_HARD_REG_BIT (wide_seen, newcol))\n+\tabort ();\n+      /* But if the new spill-cost is higher than our own, then really loose.\n+\t Respill us and recolor neighbors as before.  */\n+      if (cost > web->spill_cost)\n+\t{\n+\t  ra_debug_msg (DUMP_COLORIZE,\n+\t\t     \"reset coloring of web %d, too expensive\\n\", web->id);\n+\t  remove_list (web->dlink, &WEBS(COLORED));\n+\t  web->color = -1;\n+\t  put_web (web, SPILLED);\n+\t  for (wl = web->conflict_list; wl; wl = wl->next)\n+\t    {\n+\t      struct web *web2 = alias (wl->t);\n+\t      if (old_colors[web2->id])\n+\t\t{\n+\t\t  if (web2->type == SPILLED)\n+\t\t    {\n+\t\t      remove_list (web2->dlink, &WEBS(SPILLED));\n+\t\t      web2->color = old_colors[web2->id] - 1;\n+\t\t      put_web (web2, COLORED);\n+\t\t    }\n+\t\t  else if (web2->type == COLORED)\n+\t\t    web2->color = old_colors[web2->id] - 1;\n+\t\t  else if (web2->type == SELECT)\n+\t\t    /* This means, that WEB2 once was a part of a coalesced\n+\t\t       web, which got spilled in the above colorize_one_web()\n+\t\t       call, and whose parts then got splitted and put back\n+\t\t       onto the SELECT stack.  As the cause for that splitting\n+\t\t       (the coloring of WEB) was worthless, we should again\n+\t\t       coalesce the parts, as they were before.  For now we\n+\t\t       simply leave them SELECTed, for our caller to take\n+\t\t       care.  */\n+\t\t    ;\n+\t\t  else\n+\t\t    abort ();\n+\t\t}\n+\t    }\n+\t}\n+      free (old_colors);\n+    }\n+  free (min_color);\n+  free (cost_neighbors);\n+}\n+\n+/* This ensures that all conflicts of coalesced webs are seen from\n+   the webs coalesced into.  combine() only adds the conflicts which\n+   at the time of combining were not already SELECTed or COALESCED\n+   to not destroy num_conflicts.  Here we add all remaining conflicts\n+   and thereby destroy num_conflicts.  This should be used when num_conflicts\n+   isn't used anymore, e.g. on a completely colored graph.  */\n+\n+static void\n+insert_coalesced_conflicts ()\n+{\n+  struct dlist *d;\n+  for (d = WEBS(COALESCED); 0 && d; d = d->next)\n+    {\n+      struct web *web = DLIST_WEB (d);\n+      struct web *aweb = alias (web);\n+      struct conflict_link *wl;\n+      for (wl = web->conflict_list; wl; wl = wl->next)\n+\t{\n+\t  struct web *tweb = aweb;\n+\t  int i;\n+\t  int nregs = 1 + web->add_hardregs;\n+\t  if (aweb->type == PRECOLORED)\n+\t    nregs = HARD_REGNO_NREGS (aweb->color, GET_MODE (web->orig_x));\n+\t  for (i = 0; i < nregs; i++)\n+\t    {\n+\t      if (aweb->type == PRECOLORED)\n+\t\ttweb = hardreg2web[i + aweb->color];\n+\t      /* There might be some conflict edges laying around\n+\t\t where the usable_regs don't intersect.  This can happen\n+\t\t when first some webs were coalesced and conflicts\n+\t\t propagated, then some combining narrowed usable_regs and\n+\t\t further coalescing ignored those conflicts.  Now there are\n+\t\t some edges to COALESCED webs but not to it's alias.\n+\t\t So abort only when they really should conflict.  */\n+\t      if ((!(tweb->type == PRECOLORED\n+\t\t     || TEST_BIT (sup_igraph, tweb->id * num_webs + wl->t->id))\n+\t\t   || !(wl->t->type == PRECOLORED\n+\t\t        || TEST_BIT (sup_igraph,\n+\t\t\t\t     wl->t->id * num_webs + tweb->id)))\n+\t\t  && hard_regs_intersect_p (&tweb->usable_regs,\n+\t\t\t\t\t    &wl->t->usable_regs))\n+\t\tabort ();\n+\t      /*if (wl->sub == NULL)\n+\t\trecord_conflict (tweb, wl->t);\n+\t      else\n+\t\t{\n+\t\t  struct sub_conflict *sl;\n+\t\t  for (sl = wl->sub; sl; sl = sl->next)\n+\t\t    record_conflict (tweb, sl->t);\n+\t\t}*/\n+\t      if (aweb->type != PRECOLORED)\n+\t\tbreak;\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* A function suitable to pass to qsort().  Compare the spill costs\n+   of webs W1 and W2.  When used by qsort, this would order webs with\n+   largest cost first.  */\n+\n+static int\n+comp_webs_maxcost (w1, w2)\n+     const void *w1, *w2;\n+{\n+  struct web *web1 = *(struct web **)w1;\n+  struct web *web2 = *(struct web **)w2;\n+  if (web1->spill_cost > web2->spill_cost)\n+    return -1;\n+  else if (web1->spill_cost < web2->spill_cost)\n+    return 1;\n+  else\n+    return 0;\n+}\n+\n+/* This tries to recolor all spilled webs.  See try_recolor_web()\n+   how this is done.  This just calls it for each spilled web.  */\n+\n+static void\n+recolor_spills ()\n+{\n+  unsigned int i, num;\n+  struct web **order2web;\n+  num = num_webs - num_subwebs;\n+  order2web = (struct web **) xmalloc (num * sizeof (order2web[0]));\n+  for (i = 0; i < num; i++)\n+    {\n+      order2web[i] = id2web[i];\n+      /* If we aren't breaking aliases, combine() wasn't merging the\n+         spill_costs.  So do that here to have sane measures.  */\n+      if (!flag_ra_merge_spill_costs && id2web[i]->type == COALESCED)\n+\talias (id2web[i])->spill_cost += id2web[i]->spill_cost;\n+    }\n+  qsort (order2web, num, sizeof (order2web[0]), comp_webs_maxcost);\n+  insert_coalesced_conflicts ();\n+  dump_graph_cost (DUMP_COSTS, \"before spill-recolor\");\n+  for (i = 0; i < num; i++)\n+    {\n+      struct web *web = order2web[i];\n+      if (web->type == SPILLED)\n+\ttry_recolor_web (web);\n+    }\n+  /* It might have been decided in try_recolor_web() (in colorize_one_web())\n+     that a coalesced web should be spilled, so it was put on the\n+     select stack.  Those webs need recoloring again, and all remaining\n+     coalesced webs might need their color updated, so simply call\n+     assign_colors() again.  */\n+  assign_colors ();\n+  free (order2web);\n+}\n+\n+/* This checks the current color assignment for obvious errors,\n+   like two conflicting webs overlapping in colors, or the used colors\n+   not being in usable regs.  */\n+\n+static void\n+check_colors ()\n+{\n+  unsigned int i;\n+  for (i = 0; i < num_webs - num_subwebs; i++)\n+    {\n+      struct web *web = id2web[i];\n+      struct web *aweb = alias (web);\n+      struct conflict_link *wl;\n+      int nregs, c;\n+      if (aweb->type == SPILLED || web->regno >= max_normal_pseudo)\n+\tcontinue;\n+      else if (aweb->type == COLORED)\n+\tnregs = HARD_REGNO_NREGS (aweb->color, GET_MODE (web->orig_x));\n+      else if (aweb->type == PRECOLORED)\n+\tnregs = 1;\n+      else\n+\tabort ();\n+      /* The color must be valid for the original usable_regs.  */\n+      for (c = 0; c < nregs; c++)\n+\tif (!TEST_HARD_REG_BIT (web->usable_regs, aweb->color + c))\n+\t  abort ();\n+      /* Search the original (pre-coalesce) conflict list.  In the current\n+\t one some inprecise conflicts may be noted (due to combine() or\n+\t insert_coalesced_conflicts() relocating partial conflicts) making\n+\t it look like some wide webs are in conflict and having the same\n+\t color.  */\n+      wl = (web->have_orig_conflicts ? web->orig_conflict_list\n+\t    : web->conflict_list);\n+      for (; wl; wl = wl->next)\n+\tif (wl->t->regno >= max_normal_pseudo)\n+\t  continue;\n+\telse if (!wl->sub)\n+\t  {\n+\t    struct web *web2 = alias (wl->t);\n+\t    int nregs2;\n+\t    if (web2->type == COLORED)\n+\t      nregs2 = HARD_REGNO_NREGS (web2->color, GET_MODE (web2->orig_x));\n+\t    else if (web2->type == PRECOLORED)\n+\t      nregs2 = 1;\n+\t    else\n+\t      continue;\n+\t    if (aweb->color >= web2->color + nregs2\n+\t        || web2->color >= aweb->color + nregs)\n+\t      continue;\n+\t    abort ();\n+\t  }\n+\telse\n+\t  {\n+\t    struct sub_conflict *sl;\n+\t    int scol = aweb->color;\n+\t    int tcol = alias (wl->t)->color;\n+\t    if (alias (wl->t)->type == SPILLED)\n+\t      continue;\n+\t    for (sl = wl->sub; sl; sl = sl->next)\n+\t      {\n+\t\tint ssize = HARD_REGNO_NREGS (scol, GET_MODE (sl->s->orig_x));\n+\t\tint tsize = HARD_REGNO_NREGS (tcol, GET_MODE (sl->t->orig_x));\n+\t\tint sofs = 0, tofs = 0;\n+\t        if (SUBWEB_P (sl->t)\n+\t\t    && GET_MODE_SIZE (GET_MODE (sl->t->orig_x)) >= UNITS_PER_WORD)\n+\t\t  tofs = (SUBREG_BYTE (sl->t->orig_x) / UNITS_PER_WORD);\n+\t        if (SUBWEB_P (sl->s)\n+\t\t    && GET_MODE_SIZE (GET_MODE (sl->s->orig_x))\n+\t\t       >= UNITS_PER_WORD)\n+\t\t  sofs = (SUBREG_BYTE (sl->s->orig_x) / UNITS_PER_WORD);\n+\t\tif ((tcol + tofs >= scol + sofs + ssize)\n+\t\t    || (scol + sofs >= tcol + tofs + tsize))\n+\t\t  continue;\n+\t\tabort ();\n+\t      }\n+\t  }\n+    }\n+}\n+\n+/* WEB was a coalesced web.  Make it unaliased again, and put it\n+   back onto SELECT stack.  */\n+\n+static void\n+unalias_web (web)\n+     struct web *web;\n+{\n+  web->alias = NULL;\n+  web->is_coalesced = 0;\n+  web->color = -1;\n+  /* Well, initially everything was spilled, so it isn't incorrect,\n+     that also the individual parts can be spilled.\n+     XXX this isn't entirely correct, as we also relaxed the\n+     spill_temp flag in combine(), which might have made components\n+     spill, although they were a short or spilltemp web.  */\n+  web->was_spilled = 1;\n+  remove_list (web->dlink, &WEBS(COALESCED));\n+  /* Spilltemps must be colored right now (i.e. as early as possible),\n+     other webs can be deferred to the end (the code building the\n+     stack assumed that in this stage only one web was colored).  */\n+  if (web->spill_temp && web->spill_temp != 2)\n+    put_web (web, SELECT);\n+  else\n+    put_web_at_end (web, SELECT);\n+}\n+\n+/* WEB is a _target_ for coalescing which got spilled.\n+   Break all aliases to WEB, and restore some of its member to the state\n+   they were before coalescing.  Due to the suboptimal structure of\n+   the interference graph we need to go through all coalesced webs.\n+   Somewhen we'll change this to be more sane.  */\n+\n+static void\n+break_aliases_to_web (web)\n+     struct web *web;\n+{\n+  struct dlist *d, *d_next;\n+  if (web->type != SPILLED)\n+    abort ();\n+  for (d = WEBS(COALESCED); d; d = d_next)\n+    {\n+      struct web *other = DLIST_WEB (d);\n+      d_next = d->next;\n+      /* Beware: Don't use alias() here.  We really want to check only\n+\t one level of aliasing, i.e. only break up webs directly\n+\t aliased to WEB, not also those aliased through other webs.  */\n+      if (other->alias == web)\n+\t{\n+\t  unalias_web (other);\n+\t  ra_debug_msg (DUMP_COLORIZE, \" %d\", other->id);\n+\t}\n+    }\n+  web->spill_temp = web->orig_spill_temp;\n+  web->spill_cost = web->orig_spill_cost;\n+  /* Beware: The following possibly widens usable_regs again.  While\n+     it was narrower there might have been some conflicts added which got\n+     ignored because of non-intersecting hardregsets.  All those conflicts\n+     would now matter again.  Fortunately we only add conflicts when\n+     coalescing, which is also the time of narrowing.  And we remove all\n+     those added conflicts again now that we unalias this web.\n+     Therefore this is safe to do.  */\n+  COPY_HARD_REG_SET (web->usable_regs, web->orig_usable_regs);\n+  web->is_coalesced = 0;\n+  web->num_aliased = 0;\n+  web->was_spilled = 1;\n+  /* Reset is_coalesced flag for webs which itself are target of coalescing.\n+     It was cleared above if it was coalesced to WEB.  */\n+  for (d = WEBS(COALESCED); d; d = d->next)\n+    DLIST_WEB (d)->alias->is_coalesced = 1;\n+}\n+\n+/* WEB is a web coalesced into a precolored one.  Break that alias,\n+   making WEB SELECTed again.  Also restores the conflicts which resulted\n+   from initially coalescing both.  */\n+\n+static void\n+break_precolored_alias (web)\n+     struct web *web;\n+{\n+  struct web *pre = web->alias;\n+  struct conflict_link *wl;\n+  unsigned int c = pre->color;\n+  unsigned int nregs = HARD_REGNO_NREGS (c, GET_MODE (web->orig_x));\n+  if (pre->type != PRECOLORED)\n+    abort ();\n+  unalias_web (web);\n+  /* Now we need to look at each conflict X of WEB, if it conflicts\n+     with [PRE, PRE+nregs), and remove such conflicts, of X has not other\n+     conflicts, which are coalesced into those precolored webs.  */\n+  for (wl = web->conflict_list; wl; wl = wl->next)\n+    {\n+      struct web *x = wl->t;\n+      struct web *y;\n+      unsigned int i;\n+      struct conflict_link *wl2;\n+      struct conflict_link **pcl;\n+      HARD_REG_SET regs;\n+      if (!x->have_orig_conflicts)\n+\tcontinue;\n+      /* First look at which colors can not go away, due to other coalesces\n+\t still existing.  */\n+      CLEAR_HARD_REG_SET (regs);\n+      for (i = 0; i < nregs; i++)\n+\tSET_HARD_REG_BIT (regs, c + i);\n+      for (wl2 = x->conflict_list; wl2; wl2 = wl2->next)\n+\tif (wl2->t->type == COALESCED && alias (wl2->t)->type == PRECOLORED)\n+\t  CLEAR_HARD_REG_BIT (regs, alias (wl2->t)->color);\n+      /* Now also remove the colors of those conflicts which already\n+\t were there before coalescing at all.  */\n+      for (wl2 = x->orig_conflict_list; wl2; wl2 = wl2->next)\n+\tif (wl2->t->type == PRECOLORED)\n+\t  CLEAR_HARD_REG_BIT (regs, wl2->t->color);\n+      /* The colors now still set are those for which WEB was the last\n+\t cause, i.e. those which can be removed.  */\n+      y = NULL;\n+      for (i = 0; i < nregs; i++)\n+\tif (TEST_HARD_REG_BIT (regs, c + i))\n+\t  {\n+\t    struct web *sub;\n+\t    y = hardreg2web[c + i];\n+\t    RESET_BIT (sup_igraph, x->id * num_webs + y->id);\n+\t    RESET_BIT (sup_igraph, y->id * num_webs + x->id);\n+\t    RESET_BIT (igraph, igraph_index (x->id, y->id));\n+\t    for (sub = x->subreg_next; sub; sub = sub->subreg_next)\n+\t      RESET_BIT (igraph, igraph_index (sub->id, y->id));\n+\t  }\n+      if (!y)\n+\tcontinue;\n+      pcl = &(x->conflict_list);\n+      while (*pcl)\n+\t{\n+\t  struct web *y = (*pcl)->t;\n+\t  if (y->type != PRECOLORED || !TEST_HARD_REG_BIT (regs, y->color))\n+\t    pcl = &((*pcl)->next);\n+\t  else\n+\t    *pcl = (*pcl)->next;\n+\t}\n+    }\n+}\n+\n+/* WEB is a spilled web which was target for coalescing.\n+   Delete all interference edges which were added due to that coalescing,\n+   and break up the coalescing.  */\n+\n+static void\n+restore_conflicts_from_coalesce (web)\n+     struct web *web;\n+{\n+  struct conflict_link **pcl;\n+  struct conflict_link *wl;\n+  pcl = &(web->conflict_list);\n+  /* No original conflict list means no conflict was added at all\n+     after building the graph.  So neither we nor any neighbors have\n+     conflicts due to this coalescing.  */\n+  if (!web->have_orig_conflicts)\n+    return;\n+  while (*pcl)\n+    {\n+      struct web *other = (*pcl)->t;\n+      for (wl = web->orig_conflict_list; wl; wl = wl->next)\n+\tif (wl->t == other)\n+\t  break;\n+      if (wl)\n+\t{\n+\t  /* We found this conflict also in the original list, so this\n+\t     was no new conflict.  */\n+\t  pcl = &((*pcl)->next);\n+\t}\n+      else\n+\t{\n+\t  /* This is a new conflict, so delete it from us and\n+\t     the neighbor.  */\n+\t  struct conflict_link **opcl;\n+\t  struct conflict_link *owl;\n+\t  struct sub_conflict *sl;\n+\t  wl = *pcl;\n+\t  *pcl = wl->next;\n+\t  if (!other->have_orig_conflicts && other->type != PRECOLORED)\n+\t    abort ();\n+\t  for (owl = other->orig_conflict_list; owl; owl = owl->next)\n+\t    if (owl->t == web)\n+\t      break;\n+\t  if (owl)\n+\t    abort ();\n+\t  opcl = &(other->conflict_list);\n+\t  while (*opcl)\n+\t    {\n+\t      if ((*opcl)->t == web)\n+\t\t{\n+\t\t  owl = *opcl;\n+\t\t  *opcl = owl->next;\n+\t\t  break;\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  opcl = &((*opcl)->next);\n+\t\t}\n+\t    }\n+\t  if (!owl && other->type != PRECOLORED)\n+\t    abort ();\n+\t  /* wl and owl contain the edge data to be deleted.  */\n+\t  RESET_BIT (sup_igraph, web->id * num_webs + other->id);\n+\t  RESET_BIT (sup_igraph, other->id * num_webs + web->id);\n+\t  RESET_BIT (igraph, igraph_index (web->id, other->id));\n+\t  for (sl = wl->sub; sl; sl = sl->next)\n+\t    RESET_BIT (igraph, igraph_index (sl->s->id, sl->t->id));\n+\t  if (other->type != PRECOLORED)\n+\t    {\n+\t      for (sl = owl->sub; sl; sl = sl->next)\n+\t\tRESET_BIT (igraph, igraph_index (sl->s->id, sl->t->id));\n+\t    }\n+\t}\n+    }\n+\n+  /* We must restore usable_regs because record_conflict will use it.  */\n+  COPY_HARD_REG_SET (web->usable_regs, web->orig_usable_regs);\n+  /* We might have deleted some conflicts above, which really are still\n+     there (diamond pattern coalescing).  This is because we don't reference\n+     count interference edges but some of them were the result of different\n+     coalesces.  */\n+  for (wl = web->conflict_list; wl; wl = wl->next)\n+    if (wl->t->type == COALESCED)\n+      {\n+\tstruct web *tweb;\n+\tfor (tweb = wl->t->alias; tweb; tweb = tweb->alias)\n+\t  {\n+\t    if (wl->sub == NULL)\n+\t      record_conflict (web, tweb);\n+\t    else\n+\t      {\n+\t\tstruct sub_conflict *sl;\n+\t\tfor (sl = wl->sub; sl; sl = sl->next)\n+\t\t  {\n+\t\t    struct web *sweb = NULL;\n+\t\t    if (SUBWEB_P (sl->t))\n+\t\t      sweb = find_subweb (tweb, sl->t->orig_x);\n+\t\t    if (!sweb)\n+\t\t      sweb = tweb;\n+\t\t    record_conflict (sl->s, sweb);\n+\t\t  }\n+\t      }\n+\t    if (tweb->type != COALESCED)\n+\t      break;\n+\t  }\n+      }\n+}\n+\n+/* Repeatedly break aliases for spilled webs, which were target for\n+   coalescing, and recolorize the resulting parts.  Do this as long as\n+   there are any spilled coalesce targets.  */\n+\n+static void\n+break_coalesced_spills ()\n+{\n+  int changed = 0;\n+  while (1)\n+    {\n+      struct dlist *d;\n+      struct web *web;\n+      for (d = WEBS(SPILLED); d; d = d->next)\n+\tif (DLIST_WEB (d)->is_coalesced)\n+\t  break;\n+      if (!d)\n+\tbreak;\n+      changed = 1;\n+      web = DLIST_WEB (d);\n+      ra_debug_msg (DUMP_COLORIZE, \"breaking aliases to web %d:\", web->id);\n+      restore_conflicts_from_coalesce (web);\n+      break_aliases_to_web (web);\n+      /* WEB was a spilled web and isn't anymore.  Everything coalesced\n+\t to WEB is now SELECTed and might potentially get a color.\n+\t If those other webs were itself targets of coalescing it might be\n+\t that there are still some conflicts from aliased webs missing,\n+\t because they were added in combine() right into the now\n+\t SELECTed web.  So we need to add those missing conflicts here.  */\n+      insert_coalesced_conflicts ();\n+      ra_debug_msg (DUMP_COLORIZE, \"\\n\");\n+      remove_list (d, &WEBS(SPILLED));\n+      put_web (web, SELECT);\n+      web->color = -1;\n+      while (WEBS(SELECT))\n+\t{\n+\t  d = pop_list (&WEBS(SELECT));\n+\t  colorize_one_web (DLIST_WEB (d), 1);\n+\t}\n+    }\n+  if (changed)\n+    {\n+      struct dlist *d;\n+      for (d = WEBS(COALESCED); d; d = d->next)\n+\t{\n+\t  struct web *a = alias (DLIST_WEB (d));\n+\t  DLIST_WEB (d)->color = a->color;\n+\t}\n+    }\n+  dump_graph_cost (DUMP_COSTS, \"after alias-breaking\");\n+}\n+\n+/* A structure for fast hashing of a pair of webs.\n+   Used to cumulate savings (from removing copy insns) for coalesced webs.\n+   All the pairs are also put into a single linked list.  */\n+struct web_pair\n+{\n+  struct web_pair *next_hash;\n+  struct web_pair *next_list;\n+  struct web *smaller;\n+  struct web *larger;\n+  unsigned int conflicts;\n+  unsigned HOST_WIDE_INT cost;\n+};\n+\n+/* The actual hash table.  */\n+#define WEB_PAIR_HASH_SIZE 8192\n+static struct web_pair *web_pair_hash[WEB_PAIR_HASH_SIZE];\n+static struct web_pair *web_pair_list;\n+static unsigned int num_web_pairs;\n+\n+/* Clear the hash table of web pairs.  */\n+\n+static void\n+init_web_pairs ()\n+{\n+  memset (web_pair_hash, 0, sizeof web_pair_hash);\n+  num_web_pairs = 0;\n+  web_pair_list = NULL;\n+}\n+\n+/* Given two webs connected by a move with cost COST which together\n+   have CONFLICTS conflicts, add that pair to the hash table, or if\n+   already in, cumulate the costs and conflict number.  */\n+\n+static void\n+add_web_pair_cost (web1, web2, cost, conflicts)\n+     struct web *web1, *web2;\n+     unsigned HOST_WIDE_INT cost;\n+     unsigned int conflicts;\n+{\n+  unsigned int hash;\n+  struct web_pair *p;\n+  if (web1->id > web2->id)\n+    {\n+      struct web *h = web1;\n+      web1 = web2;\n+      web2 = h;\n+    }\n+  hash = (web1->id * num_webs + web2->id) % WEB_PAIR_HASH_SIZE;\n+  for (p = web_pair_hash[hash]; p; p = p->next_hash)\n+    if (p->smaller == web1 && p->larger == web2)\n+      {\n+\tp->cost += cost;\n+\tp->conflicts += conflicts;\n+\treturn;\n+      }\n+  p = (struct web_pair *) ra_alloc (sizeof *p);\n+  p->next_hash = web_pair_hash[hash];\n+  p->next_list = web_pair_list;\n+  p->smaller = web1;\n+  p->larger = web2;\n+  p->conflicts = conflicts;\n+  p->cost = cost;\n+  web_pair_hash[hash] = p;\n+  web_pair_list = p;\n+  num_web_pairs++;\n+}\n+\n+/* Suitable to be passed to qsort().  Sort web pairs so, that those\n+   with more conflicts and higher cost (which actually is a saving\n+   when the moves are removed) come first.  */\n+\n+static int\n+comp_web_pairs (w1, w2)\n+     const void *w1, *w2;\n+{\n+  struct web_pair *p1 = *(struct web_pair **)w1;\n+  struct web_pair *p2 = *(struct web_pair **)w2;\n+  if (p1->conflicts > p2->conflicts)\n+    return -1;\n+  else if (p1->conflicts < p2->conflicts)\n+    return 1;\n+  else if (p1->cost > p2->cost)\n+    return -1;\n+  else if (p1->cost < p2->cost)\n+    return 1;\n+  else\n+    return 0;\n+}\n+\n+/* Given the list of web pairs, begin to combine them from the one\n+   with the most savings.  */\n+\n+static void\n+sort_and_combine_web_pairs (for_move)\n+     int for_move;\n+{\n+  unsigned int i;\n+  struct web_pair **sorted;\n+  struct web_pair *p;\n+  if (!num_web_pairs)\n+    return;\n+  sorted = (struct web_pair **) xmalloc (num_web_pairs * sizeof (sorted[0]));\n+  for (p = web_pair_list, i = 0; p; p = p->next_list)\n+    sorted[i++] = p;\n+  if (i != num_web_pairs)\n+    abort ();\n+  qsort (sorted, num_web_pairs, sizeof (sorted[0]), comp_web_pairs);\n+\n+  /* After combining one pair, we actually should adjust the savings\n+     of the other pairs, if they are connected to one of the just coalesced\n+     pair.  Later.  */\n+  for (i = 0; i < num_web_pairs; i++)\n+    {\n+      struct web *w1, *w2;\n+      p = sorted[i];\n+      w1 = alias (p->smaller);\n+      w2 = alias (p->larger);\n+      if (!for_move && (w1->type == PRECOLORED || w2->type == PRECOLORED))\n+\tcontinue;\n+      else if (w2->type == PRECOLORED)\n+\t{\n+\t  struct web *h = w1;\n+\t  w1 = w2;\n+\t  w2 = h;\n+\t}\n+      if (w1 != w2\n+\t  && !TEST_BIT (sup_igraph, w1->id * num_webs + w2->id)\n+\t  && !TEST_BIT (sup_igraph, w2->id * num_webs + w1->id)\n+\t  && w2->type != PRECOLORED\n+\t  && hard_regs_intersect_p (&w1->usable_regs, &w2->usable_regs))\n+\t  {\n+\t    if (w1->type != PRECOLORED\n+\t\t|| (w1->type == PRECOLORED && ok (w2, w1)))\n+\t      combine (w1, w2);\n+\t    else if (w1->type == PRECOLORED)\n+\t      SET_HARD_REG_BIT (w2->prefer_colors, w1->color);\n+\t  }\n+    }\n+  free (sorted);\n+}\n+\n+/* Greedily coalesce all moves possible.  Begin with the web pair\n+   giving the most saving if coalesced.  */\n+\n+static void\n+aggressive_coalesce ()\n+{\n+  struct dlist *d;\n+  struct move *m;\n+  init_web_pairs ();\n+  while ((d = pop_list (&mv_worklist)) != NULL)\n+    if ((m = DLIST_MOVE (d)))\n+      {\n+\tstruct web *s = alias (m->source_web);\n+\tstruct web *t = alias (m->target_web);\n+\tif (t->type == PRECOLORED)\n+\t  {\n+\t    struct web *h = s;\n+\t    s = t;\n+\t    t = h;\n+\t  }\n+\tif (s != t\n+\t    && t->type != PRECOLORED\n+\t    && !TEST_BIT (sup_igraph, s->id * num_webs + t->id)\n+\t    && !TEST_BIT (sup_igraph, t->id * num_webs + s->id))\n+\t  {\n+\t    if ((s->type == PRECOLORED && ok (t, s))\n+\t\t|| s->type != PRECOLORED)\n+\t      {\n+\t        put_move (m, MV_COALESCED);\n+\t\tadd_web_pair_cost (s, t, BLOCK_FOR_INSN (m->insn)->frequency,\n+\t\t\t\t   0);\n+\t      }\n+\t    else if (s->type == PRECOLORED)\n+\t      /* It is !ok(t, s).  But later when coloring the graph it might\n+\t\t be possible to take that color.  So we remember the preferred\n+\t\t color to try that first.  */\n+\t      {\n+\t\tput_move (m, CONSTRAINED);\n+\t\tSET_HARD_REG_BIT (t->prefer_colors, s->color);\n+\t      }\n+\t  }\n+\telse\n+\t  {\n+\t    put_move (m, CONSTRAINED);\n+\t  }\n+      }\n+  sort_and_combine_web_pairs (1);\n+}\n+\n+/* This is the difference between optimistic coalescing and\n+   optimistic coalescing+.  Extended coalesce tries to coalesce also\n+   non-conflicting nodes, not related by a move.  The criteria here is,\n+   the one web must be a source, the other a destination of the same insn.\n+   This actually makes sense, as (because they are in the same insn) they\n+   share many of their neighbors, and if they are coalesced, reduce the\n+   number of conflicts of those neighbors by one.  For this we sort the\n+   candidate pairs again according to savings (and this time also conflict\n+   number).\n+\n+   This is also a comparatively slow operation, as we need to go through\n+   all insns, and for each insn, through all defs and uses.  */\n+\n+static void\n+extended_coalesce_2 ()\n+{\n+  rtx insn;\n+  struct ra_insn_info info;\n+  unsigned int n;\n+  init_web_pairs ();\n+  for (insn = get_insns (); insn; insn = NEXT_INSN (insn))\n+    if (INSN_P (insn) && (info = insn_df[INSN_UID (insn)]).num_defs)\n+      for (n = 0; n < info.num_defs; n++)\n+\t{\n+\t  struct web *dest = def2web[DF_REF_ID (info.defs[n])];\n+\t  dest = alias (find_web_for_subweb (dest));\n+\t  if (dest->type != PRECOLORED && dest->regno < max_normal_pseudo)\n+\t    {\n+\t      unsigned int n2;\n+\t      for (n2 = 0; n2 < info.num_uses; n2++)\n+\t\t{\n+\t\t  struct web *source = use2web[DF_REF_ID (info.uses[n2])];\n+\t\t  source = alias (find_web_for_subweb (source));\n+\t\t  if (source->type != PRECOLORED\n+\t\t      && source != dest\n+\t\t      && source->regno < max_normal_pseudo\n+\t\t      /* Coalesced webs end up using the same REG rtx in\n+\t\t\t emit_colors().  So we can only coalesce something\n+\t\t\t of equal modes.  */\n+\t\t      && GET_MODE (source->orig_x) == GET_MODE (dest->orig_x)\n+\t\t      && !TEST_BIT (sup_igraph,\n+\t\t\t\t    dest->id * num_webs + source->id)\n+\t\t      && !TEST_BIT (sup_igraph,\n+\t\t\t\t    source->id * num_webs + dest->id)\n+\t\t      && hard_regs_intersect_p (&source->usable_regs,\n+\t\t\t\t\t\t&dest->usable_regs))\n+\t\t    add_web_pair_cost (dest, source,\n+\t\t\t\t       BLOCK_FOR_INSN (insn)->frequency,\n+\t\t\t\t       dest->num_conflicts\n+\t\t\t\t       + source->num_conflicts);\n+\t\t}\n+\t    }\n+\t}\n+  sort_and_combine_web_pairs (0);\n+}\n+\n+/* Check if we forgot to coalesce some moves.  */\n+\n+static void\n+check_uncoalesced_moves ()\n+{\n+  struct move_list *ml;\n+  struct move *m;\n+  for (ml = wl_moves; ml; ml = ml->next)\n+    if ((m = ml->move))\n+      {\n+\tstruct web *s = alias (m->source_web);\n+\tstruct web *t = alias (m->target_web);\n+\tif (t->type == PRECOLORED)\n+\t  {\n+\t    struct web *h = s;\n+\t    s = t;\n+\t    t = h;\n+\t  }\n+\tif (s != t\n+\t    && m->type != CONSTRAINED\n+\t    /* Following can happen when a move was coalesced, but later\n+\t       broken up again.  Then s!=t, but m is still MV_COALESCED.  */\n+\t    && m->type != MV_COALESCED\n+\t    && t->type != PRECOLORED\n+\t    && ((s->type == PRECOLORED && ok (t, s))\n+\t\t|| s->type != PRECOLORED)\n+\t    && !TEST_BIT (sup_igraph, s->id * num_webs + t->id)\n+\t    && !TEST_BIT (sup_igraph, t->id * num_webs + s->id))\n+\t  abort ();\n+      }\n+}\n+\n+/* The toplevel function in this file.  Precondition is, that\n+   the interference graph is built completely by ra-build.c.  This\n+   produces a list of spilled, colored and coalesced nodes.  */\n+\n+void\n+ra_colorize_graph (df)\n+     struct df *df;\n+{\n+  if (rtl_dump_file)\n+    dump_igraph (df);\n+  build_worklists (df);\n+\n+  /* With optimistic coalescing we coalesce everything we can.  */\n+  if (flag_ra_optimistic_coalescing)\n+    {\n+      aggressive_coalesce ();\n+      extended_coalesce_2 ();\n+    }\n+\n+  /* Now build the select stack.  */\n+  do\n+    {\n+      simplify ();\n+      if (mv_worklist)\n+\tcoalesce ();\n+      else if (WEBS(FREEZE))\n+\tfreeze ();\n+      else if (WEBS(SPILL))\n+\tselect_spill ();\n+    }\n+  while (WEBS(SIMPLIFY) || WEBS(SIMPLIFY_FAT) || WEBS(SIMPLIFY_SPILL)\n+\t || mv_worklist || WEBS(FREEZE) || WEBS(SPILL));\n+  if (flag_ra_optimistic_coalescing)\n+    check_uncoalesced_moves ();\n+\n+  /* Actually colorize the webs from the select stack.  */\n+  assign_colors ();\n+  check_colors ();\n+  dump_graph_cost (DUMP_COSTS, \"initially\");\n+  if (flag_ra_break_aliases)\n+    break_coalesced_spills ();\n+  check_colors ();\n+\n+  /* And try to improve the cost by recoloring spilled webs.  */\n+  recolor_spills ();\n+  dump_graph_cost (DUMP_COSTS, \"after spill-recolor\");\n+  check_colors ();\n+}\n+\n+/* Initialize this module.  */\n+\n+void ra_colorize_init ()\n+{\n+  /* FIXME: Choose spill heuristic for platform if we have one */\n+  spill_heuristic = default_spill_heuristic;\n+}\n+\n+/* Free all memory.  (Note that we don't need to free any per pass\n+   memory).  */\n+\n+void\n+ra_colorize_free_all ()\n+{\n+  struct dlist *d;\n+  while ((d = pop_list (&WEBS(FREE))) != NULL)\n+    put_web (DLIST_WEB (d), INITIAL);\n+  while ((d = pop_list (&WEBS(INITIAL))) != NULL)\n+    {\n+      struct web *web =DLIST_WEB (d);\n+      struct web *wnext;\n+      web->orig_conflict_list = NULL;\n+      web->conflict_list = NULL;\n+      for (web = web->subreg_next; web; web = wnext)\n+\t{\n+\t  wnext = web->subreg_next;\n+\t  free (web);\n+\t}\n+      free (DLIST_WEB (d));\n+    }\n+}\n+\n+/*\n+vim:cinoptions={.5s,g0,p5,t0,(0,^-0.5s,n-0.5s:tw=78:cindent:sw=4:\n+*/"}, {"sha": "239778a240b4c1e571cb693983bb886b85264d9f", "filename": "gcc/ra-debug.c", "status": "added", "additions": 1118, "deletions": 0, "changes": 1118, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fra-debug.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fra-debug.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fra-debug.c?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -0,0 +1,1118 @@\n+/* Graph coloring register allocator\n+   Copyright (C) 2001, 2002 Free Software Foundation, Inc.\n+   Contributed by Michael Matz <matz@suse.de>\n+   and Daniel Berlin <dan@cgsoftware.com>.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it under the\n+   terms of the GNU General Public License as published by the Free Software\n+   Foundation; either version 2, or (at your option) any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n+   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n+   details.\n+\n+   You should have received a copy of the GNU General Public License along\n+   with GCC; see the file COPYING.  If not, write to the Free Software\n+   Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"rtl.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"function.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"df.h\"\n+#include \"output.h\"\n+#include \"ra.h\"\n+\n+/* This file contains various dumping and debug functions for\n+   the graph coloring register allocator.  */\n+\n+static void ra_print_rtx_1op PARAMS ((FILE *, rtx));\n+static void ra_print_rtx_2op PARAMS ((FILE *, rtx));\n+static void ra_print_rtx_3op PARAMS ((FILE *, rtx));\n+static void ra_print_rtx_object PARAMS ((FILE *, rtx));\n+\n+/* The hardregs as names, for debugging.  */\n+static const char *const reg_class_names[] = REG_CLASS_NAMES;\n+\n+/* Print a message to the dump file, if debug_new_regalloc and LEVEL\n+   have any bits in common.  */\n+\n+void\n+ra_debug_msg VPARAMS ((unsigned int level, const char *format, ...))\n+{\n+#ifndef ANSI_PROTOTYPES\n+  int level;\n+  const char *format;\n+#endif\n+  va_list ap;\n+  if ((debug_new_regalloc & level) != 0 && rtl_dump_file != NULL)\n+    {\n+      VA_START (ap, format);\n+\n+#ifndef ANSI_PROTOTYPES\n+      format = va_arg (ap, const char *);\n+#endif\n+\n+      vfprintf (rtl_dump_file, format, ap);\n+      va_end (ap);\n+    }\n+}\n+\n+\n+/* The following ra_print_xxx() functions print RTL expressions\n+   in concise infix form.  If the mode can be seen from context it's\n+   left out.  Most operators are represented by their graphical\n+   characters, e.g. LE as \"<=\".  Unknown constructs are currently\n+   printed with print_inline_rtx(), which disrupts the nice layout.\n+   Currently only the inline asm things are written this way.  */\n+\n+/* Print rtx X, which is a one operand rtx (op:mode (Y)), as\n+   \"op(Y)\" to FILE.  */\n+\n+static void\n+ra_print_rtx_1op (file, x)\n+     FILE *file;\n+     rtx x;\n+{\n+  enum rtx_code code = GET_CODE (x);\n+  rtx op0 = XEXP (x, 0);\n+  switch (code)\n+    {\n+      case NEG:\n+      case NOT:\n+\t  fputs ((code == NEG) ? \"-(\" : \"~(\", file);\n+\t  ra_print_rtx (file, op0, 0);\n+\t  fputs (\")\", file);\n+\t  break;\n+      case HIGH:\n+\t  fputs (\"hi(\", file);\n+\t  ra_print_rtx (file, op0, 0);\n+\t  fputs (\")\", file);\n+\t  break;\n+      default:\n+\t  fprintf (file, \"%s\", GET_RTX_NAME (code));\n+\t  if (GET_MODE (x) != VOIDmode)\n+\t    fprintf (file, \":%s(\", GET_MODE_NAME (GET_MODE (x)));\n+\t  else\n+\t    fputs (\"(\", file);\n+\t  ra_print_rtx (file, op0, 0);\n+\t  fputs (\")\", file);\n+\t  break;\n+    }\n+}\n+\n+/* Print rtx X, which is a two operand rtx (op:mode (Y) (Z))\n+   as \"(Y op Z)\", if the operand is know, or as \"op(Y, Z)\", if not,\n+   to FILE.  */\n+\n+static void\n+ra_print_rtx_2op (file, x)\n+     FILE *file;\n+     rtx x;\n+{\n+  int infix = 1;\n+  const char *opname = \"shitop\";\n+  enum rtx_code code = GET_CODE (x);\n+  rtx op0 = XEXP (x, 0);\n+  rtx op1 = XEXP (x, 1);\n+  switch (code)\n+    {\n+      /* class '2' */\n+      case COMPARE: opname = \"?\"; break;\n+      case MINUS: opname = \"-\"; break;\n+      case DIV: opname = \"/\"; break;\n+      case UDIV: opname = \"u/\"; break;\n+      case MOD: opname = \"%\"; break;\n+      case UMOD: opname = \"u%\"; break;\n+      case ASHIFT: opname = \"<<\"; break;\n+      case ASHIFTRT: opname = \"a>>\"; break;\n+      case LSHIFTRT: opname = \"l>>\"; break;\n+      /* class 'c' */\n+      case PLUS: opname = \"+\"; break;\n+      case MULT: opname = \"*\"; break;\n+      case AND: opname = \"&\"; break;\n+      case IOR: opname = \"|\"; break;\n+      case XOR: opname = \"^\"; break;\n+      /* class '<' */\n+      case NE: opname = \"!=\"; break;\n+      case EQ: opname = \"==\"; break;\n+      case GE: opname = \"s>=\"; break;\n+      case GT: opname = \"s>\"; break;\n+      case LE: opname = \"s<=\"; break;\n+      case LT: opname = \"s<\"; break;\n+      case GEU: opname = \"u>=\"; break;\n+      case GTU: opname = \"u>\"; break;\n+      case LEU: opname = \"u<=\"; break;\n+      case LTU: opname = \"u<\"; break;\n+      default:\n+\t\tinfix = 0;\n+\t\topname = GET_RTX_NAME (code);\n+\t\tbreak;\n+    }\n+  if (infix)\n+    {\n+      fputs (\"(\", file);\n+      ra_print_rtx (file, op0, 0);\n+      fprintf (file, \" %s \", opname);\n+      ra_print_rtx (file, op1, 0);\n+      fputs (\")\", file);\n+    }\n+  else\n+    {\n+      fprintf (file, \"%s(\", opname);\n+      ra_print_rtx (file, op0, 0);\n+      fputs (\", \", file);\n+      ra_print_rtx (file, op1, 0);\n+      fputs (\")\", file);\n+    }\n+}\n+\n+/* Print rtx X, which a three operand rtx to FILE.\n+   I.e. X is either an IF_THEN_ELSE, or a bitmap operation.  */\n+\n+static void\n+ra_print_rtx_3op (file, x)\n+     FILE *file;\n+     rtx x;\n+{\n+  enum rtx_code code = GET_CODE (x);\n+  rtx op0 = XEXP (x, 0);\n+  rtx op1 = XEXP (x, 1);\n+  rtx op2 = XEXP (x, 2);\n+  if (code == IF_THEN_ELSE)\n+    {\n+      ra_print_rtx (file, op0, 0);\n+      fputs (\" ? \", file);\n+      ra_print_rtx (file, op1, 0);\n+      fputs (\" : \", file);\n+      ra_print_rtx (file, op2, 0);\n+    }\n+  else\n+    {\n+      /* Bitmap-operation */\n+      fprintf (file, \"%s:%s(\", GET_RTX_NAME (code),\n+\t       GET_MODE_NAME (GET_MODE (x)));\n+      ra_print_rtx (file, op0, 0);\n+      fputs (\", \", file);\n+      ra_print_rtx (file, op1, 0);\n+      fputs (\", \", file);\n+      ra_print_rtx (file, op2, 0);\n+      fputs (\")\", file);\n+    }\n+}\n+\n+/* Print rtx X, which represents an object (class 'o' or some constructs\n+   of class 'x' (e.g. subreg)), to FILE.\n+   (reg XX) rtl is represented as \"pXX\", of XX was a pseudo,\n+   as \"name\" it name is the nonnull hardreg name, or as \"hXX\", if XX\n+   is a hardreg, whose name is NULL, or empty.  */\n+\n+static void\n+ra_print_rtx_object (file, x)\n+     FILE *file;\n+     rtx x;\n+{\n+  enum rtx_code code = GET_CODE (x);\n+  enum machine_mode mode = GET_MODE (x);\n+  switch (code)\n+    {\n+      case CONST_INT:\n+\t  fprintf (file, HOST_WIDE_INT_PRINT_DEC, XWINT (x, 0));\n+\t  break;\n+      case CONST_DOUBLE:\n+\t    {\n+\t      int i, num = 0;\n+\t      const char *fmt = GET_RTX_FORMAT (code);\n+\t      fputs (\"dbl(\", file);\n+\t      for (i = 0; i < GET_RTX_LENGTH (code); i++)\n+\t\t{\n+\t\t  if (num)\n+\t\t    fputs (\", \", file);\n+\t\t  if (fmt[i] == 'e' && XEXP (x, i))\n+\t\t    /* The MEM or other stuff */\n+\t\t    {\n+\t\t      ra_print_rtx (file, XEXP (x, i), 0);\n+\t\t      num++;\n+\t\t    }\n+\t\t  else if (fmt[i] == 'w')\n+\t\t    {\n+\t\t      fprintf (file, HOST_WIDE_INT_PRINT_HEX, XWINT (x, i));\n+\t\t      num++;\n+\t\t    }\n+\t\t}\n+\t      break;\n+\t    }\n+      case CONST_STRING: fprintf (file, \"\\\"%s\\\"\", XSTR (x, 0)); break;\n+      case CONST: fputs (\"const(\", file);\n+\t\t  ra_print_rtx (file, XEXP (x, 0), 0);\n+\t\t  fputs (\")\", file);\n+\t\t  break;\n+      case PC: fputs (\"pc\", file); break;\n+      case REG:\n+\t       {\n+\t\t int regno = REGNO (x);\n+\t\t if (regno < FIRST_PSEUDO_REGISTER)\n+\t\t   {\n+\t\t     int i, nregs = HARD_REGNO_NREGS (regno, mode);\n+\t\t     if (nregs > 1)\n+\t\t       fputs (\"[\", file);\n+\t\t     for (i = 0; i < nregs; i++)\n+\t\t       {\n+\t\t\t if (i)\n+\t\t\t   fputs (\", \", file);\n+\t\t\t if (reg_names[regno+i] && *reg_names[regno + i])\n+\t\t\t   fprintf (file, \"%s\", reg_names[regno + i]);\n+\t\t\t else\n+\t\t\t   fprintf (file, \"h%d\", regno + i);\n+\t\t       }\n+\t\t     if (nregs > 1)\n+\t\t       fputs (\"]\", file);\n+\t\t   }\n+\t\t else\n+\t\t   fprintf (file, \"p%d\", regno);\n+\t\t break;\n+\t       }\n+      case SUBREG:\n+\t       {\n+\t\t rtx sub = SUBREG_REG (x);\n+\t\t int ofs = SUBREG_BYTE (x);\n+\t\t if (GET_CODE (sub) == REG\n+\t\t     && REGNO (sub) < FIRST_PSEUDO_REGISTER)\n+\t\t   {\n+\t\t     int regno = REGNO (sub);\n+\t\t     int i, nregs = HARD_REGNO_NREGS (regno, mode);\n+\t\t     regno += subreg_regno_offset (regno, GET_MODE (sub),\n+\t\t\t\t\t\t   ofs, mode);\n+\t\t     if (nregs > 1)\n+\t\t       fputs (\"[\", file);\n+\t\t     for (i = 0; i < nregs; i++)\n+\t\t       {\n+\t\t\t if (i)\n+\t\t\t   fputs (\", \", file);\n+\t\t\t if (reg_names[regno+i])\n+\t\t\t   fprintf (file, \"%s\", reg_names[regno + i]);\n+\t\t\t else\n+\t\t\t   fprintf (file, \"h%d\", regno + i);\n+\t\t       }\n+\t\t     if (nregs > 1)\n+\t\t       fputs (\"]\", file);\n+\t\t   }\n+\t\t else\n+\t\t   {\n+\t\t     ra_print_rtx (file, sub, 0);\n+\t\t     fprintf (file, \":[%s+%d]\", GET_MODE_NAME (mode), ofs);\n+\t\t   }\n+\t\t break;\n+\t       }\n+      case SCRATCH: fputs (\"scratch\", file); break;\n+      case CONCAT: ra_print_rtx_2op (file, x); break;\n+      case HIGH: ra_print_rtx_1op (file, x); break;\n+      case LO_SUM:\n+\t\t fputs (\"(\", file);\n+\t\t ra_print_rtx (file, XEXP (x, 0), 0);\n+\t\t fputs (\" + lo(\", file);\n+\t\t ra_print_rtx (file, XEXP (x, 1), 0);\n+\t\t fputs (\"))\", file);\n+\t\t break;\n+      case MEM: fputs (\"[\", file);\n+\t\tra_print_rtx (file, XEXP (x, 0), 0);\n+\t\tfprintf (file, \"]:%s\", GET_MODE_NAME (GET_MODE (x)));\n+\t\t/* XXX print alias set too ?? */\n+\t\tbreak;\n+      case LABEL_REF:\n+\t\t  {\n+\t\t    rtx sub = XEXP (x, 0);\n+\t\t    if (GET_CODE (sub) == NOTE\n+\t\t\t&& NOTE_LINE_NUMBER (sub) == NOTE_INSN_DELETED_LABEL)\n+\t\t      fprintf (file, \"(deleted uid=%d)\", INSN_UID (sub));\n+\t\t    else if (GET_CODE (sub) == CODE_LABEL)\n+\t\t      fprintf (file, \"L%d\", CODE_LABEL_NUMBER (sub));\n+\t\t    else\n+\t\t      fprintf (file, \"(nonlabel uid=%d)\", INSN_UID (sub));\n+\t\t  }\n+\t\tbreak;\n+      case SYMBOL_REF:\n+\t\tfprintf (file, \"sym(\\\"%s\\\")\", XSTR (x, 0)); break;\n+      case CC0: fputs (\"cc0\", file); break;\n+      default: print_inline_rtx (file, x, 0); break;\n+    }\n+}\n+\n+/* Print a general rtx X to FILE in nice infix form.\n+   If WITH_PN is set, and X is one of the toplevel constructs\n+   (insns, notes, labels or barriers), then print also the UIDs of\n+   the preceding and following insn.  */\n+\n+void\n+ra_print_rtx (file, x, with_pn)\n+     FILE *file;\n+     rtx x;\n+     int with_pn;\n+{\n+  enum rtx_code code;\n+  char class;\n+  int unhandled = 0;\n+  if (!x)\n+    return;\n+  code = GET_CODE (x);\n+  class = GET_RTX_CLASS (code);\n+\n+  /* First handle the insn like constructs.  */\n+  if (INSN_P (x) || code == NOTE || code == CODE_LABEL || code == BARRIER)\n+    {\n+      if (INSN_P (x))\n+\tfputs (\"  \", file);\n+      /* Non-insns are prefixed by a ';'.  */\n+      if (code == BARRIER)\n+\tfputs (\"; \", file);\n+      else if (code == NOTE)\n+\t/* But notes are indented very far right.  */\n+\tfprintf (file, \"\\t\\t\\t\\t\\t; \");\n+      else if (code == CODE_LABEL)\n+\t/* And labels have their Lxx name first, before the actual UID.  */\n+\t{\n+\t  fprintf (file, \"L%d:\\t; \", CODE_LABEL_NUMBER (x));\n+\t  if (LABEL_NAME (x))\n+\t    fprintf (file, \"(%s) \", LABEL_NAME (x));\n+\t  if (LABEL_ALTERNATE_NAME (x))\n+\t    fprintf (file, \"(alternate: %s) \", LABEL_ALTERNATE_NAME (x));\n+\t  fprintf (file, \" [%d uses] uid=(\", LABEL_NUSES (x));\n+\t}\n+      fprintf (file, \"%d\", INSN_UID (x));\n+      if (with_pn)\n+\tfprintf (file, \" %d %d\", PREV_INSN (x) ? INSN_UID (PREV_INSN (x)) : 0,\n+\t\t NEXT_INSN (x) ? INSN_UID (NEXT_INSN (x)) : 0);\n+      if (code == BARRIER)\n+\tfputs (\" -------- barrier ---------\", file);\n+      else if (code == CODE_LABEL)\n+\tfputs (\")\", file);\n+      else if (code == NOTE)\n+\t{\n+\t  int ln = NOTE_LINE_NUMBER (x);\n+\t  if (ln >= (int) NOTE_INSN_BIAS && ln < (int) NOTE_INSN_MAX)\n+\t    fprintf (file, \" %s\", GET_NOTE_INSN_NAME (ln));\n+\t  else\n+\t    {\n+\t      fprintf (file, \" line %d\", ln);\n+\t      if (NOTE_SOURCE_FILE (x))\n+\t\tfprintf (file, \":%s\", NOTE_SOURCE_FILE (x));\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  fprintf (file, \"\\t\");\n+\t  ra_print_rtx (file, PATTERN (x), 0);\n+\t}\n+      return;\n+    }\n+  switch (code)\n+    {\n+      /* Top-level stuff.  */\n+      case PARALLEL:\n+\t    {\n+\t      int j;\n+\t      for (j = 0; j < XVECLEN (x, 0); j++)\n+\t\t{\n+\t\t  if (j)\n+\t\t    fputs (\"\\t;; \", file);\n+\t\t  ra_print_rtx (file, XVECEXP (x, 0, j), 0);\n+\t\t}\n+\t      break;\n+\t    }\n+      case UNSPEC: case UNSPEC_VOLATILE:\n+\t    {\n+\t      int j;\n+\t      fprintf (file, \"unspec%s(%d\",\n+\t\t       (code == UNSPEC) ? \"\" : \"_vol\", XINT (x, 1));\n+\t      for (j = 0; j < XVECLEN (x, 0); j++)\n+\t\t{\n+\t\t  fputs (\", \", file);\n+\t\t  ra_print_rtx (file, XVECEXP (x, 0, j), 0);\n+\t\t}\n+\t      fputs (\")\", file);\n+\t      break;\n+\t    }\n+      case SET:\n+\t  if (GET_CODE (SET_DEST (x)) == PC)\n+\t    {\n+\t      if (GET_CODE (SET_SRC (x)) == IF_THEN_ELSE\n+\t\t  && GET_CODE (XEXP (SET_SRC(x), 2)) == PC)\n+\t\t{\n+\t\t  fputs (\"if \", file);\n+\t\t  ra_print_rtx (file, XEXP (SET_SRC (x), 0), 0);\n+\t\t  fputs (\" jump \", file);\n+\t\t  ra_print_rtx (file, XEXP (SET_SRC (x), 1), 0);\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  fputs (\"jump \", file);\n+\t\t  ra_print_rtx (file, SET_SRC (x), 0);\n+\t\t}\n+\t    }\n+\t  else\n+\t    {\n+\t      ra_print_rtx (file, SET_DEST (x), 0);\n+\t      fputs (\" <= \", file);\n+\t      ra_print_rtx (file, SET_SRC (x), 0);\n+\t    }\n+\t  break;\n+      case USE:\n+\t      fputs (\"use <= \", file);\n+\t      ra_print_rtx (file, XEXP (x, 0), 0);\n+\t      break;\n+      case CLOBBER:\n+\t      ra_print_rtx (file, XEXP (x, 0), 0);\n+\t      fputs (\" <= clobber\", file);\n+\t      break;\n+      case CALL:\n+\t      fputs (\"call \", file);\n+\t      ra_print_rtx (file, XEXP (x, 0), 0); /* Address */\n+\t      fputs (\" numargs=\", file);\n+\t      ra_print_rtx (file, XEXP (x, 1), 0); /* Num arguments */\n+\t      break;\n+      case RETURN:\n+\t      fputs (\"return\", file);\n+\t      break;\n+      case TRAP_IF:\n+\t      fputs (\"if (\", file);\n+\t      ra_print_rtx (file, XEXP (x, 0), 0);\n+\t      fputs (\") trap \", file);\n+\t      ra_print_rtx (file, XEXP (x, 1), 0);\n+\t      break;\n+      case RESX:\n+\t      fprintf (file, \"resx from region %d\", XINT (x, 0));\n+\t      break;\n+\n+      /* Different things of class 'x' */\n+      case SUBREG: ra_print_rtx_object (file, x); break;\n+      case STRICT_LOW_PART:\n+\t\t   fputs (\"low(\", file);\n+\t\t   ra_print_rtx (file, XEXP (x, 0), 0);\n+\t\t   fputs (\")\", file);\n+\t\t   break;\n+      default:\n+\tunhandled = 1;\n+\tbreak;\n+    }\n+  if (!unhandled)\n+    return;\n+  if (class == '1')\n+    ra_print_rtx_1op (file, x);\n+  else if (class == '2' || class == 'c' || class == '<')\n+    ra_print_rtx_2op (file, x);\n+  else if (class == '3' || class == 'b')\n+    ra_print_rtx_3op (file, x);\n+  else if (class == 'o')\n+    ra_print_rtx_object (file, x);\n+  else\n+    print_inline_rtx (file, x, 0);\n+}\n+\n+/* This only calls ra_print_rtx(), but emits a final newline.  */\n+\n+void\n+ra_print_rtx_top (file, x, with_pn)\n+     FILE *file;\n+     rtx x;\n+     int with_pn;\n+{\n+  ra_print_rtx (file, x, with_pn);\n+  fprintf (file, \"\\n\");\n+}\n+\n+/* Callable from gdb.  This prints rtx X onto stderr.  */\n+\n+void\n+ra_debug_rtx (x)\n+     rtx x;\n+{\n+  ra_print_rtx_top (stderr, x, 1);\n+}\n+\n+/* This prints the content of basic block with index BBI.\n+   The first and last insn are emitted with UIDs of prev and next insns.  */\n+\n+void\n+ra_debug_bbi (bbi)\n+     int bbi;\n+{\n+  basic_block bb = BASIC_BLOCK (bbi);\n+  rtx insn;\n+  for (insn = bb->head; insn; insn = NEXT_INSN (insn))\n+    {\n+      ra_print_rtx_top (stderr, insn, (insn == bb->head || insn == bb->end));\n+      fprintf (stderr, \"\\n\");\n+      if (insn == bb->end)\n+\tbreak;\n+    }\n+}\n+\n+/* Beginning from INSN, emit NUM insns (if NUM is non-negative)\n+   or emit a window of NUM insns around INSN, to stderr.  */\n+\n+void\n+ra_debug_insns (insn, num)\n+     rtx insn;\n+     int num;\n+{\n+  int i, count = (num == 0 ? 1 : num < 0 ? -num : num);\n+  if (num < 0)\n+    for (i = count / 2; i > 0 && PREV_INSN (insn); i--)\n+      insn = PREV_INSN (insn);\n+  for (i = count; i > 0 && insn; insn = NEXT_INSN (insn), i--)\n+    {\n+      if (GET_CODE (insn) == CODE_LABEL)\n+\tfprintf (stderr, \"\\n\");\n+      ra_print_rtx_top (stderr, insn, (i == count || i == 1));\n+    }\n+}\n+\n+/* Beginning with INSN, emit the whole insn chain into FILE.\n+   This also outputs comments when basic blocks start or end and omits\n+   some notes, if flag_ra_dump_notes is zero.  */\n+\n+void\n+ra_print_rtl_with_bb (file, insn)\n+     FILE *file;\n+     rtx insn;\n+{\n+  basic_block last_bb, bb;\n+  unsigned int num = 0;\n+  if (!insn)\n+    fputs (\"nil\", file);\n+  last_bb = NULL;\n+  for (; insn; insn = NEXT_INSN (insn))\n+    {\n+      if (GET_CODE (insn) == BARRIER)\n+\tbb = NULL;\n+      else\n+\tbb = BLOCK_FOR_INSN (insn);\n+      if (bb != last_bb)\n+\t{\n+\t  if (last_bb)\n+\t    fprintf (file, \";; End of basic block %d\\n\", last_bb->index);\n+\t  if (bb)\n+\t    fprintf (file, \";; Begin of basic block %d\\n\", bb->index);\n+\t  last_bb = bb;\n+\t}\n+      if (GET_CODE (insn) == CODE_LABEL)\n+\tfputc ('\\n', file);\n+      if (GET_CODE (insn) == NOTE)\n+\t{\n+\t  /* Ignore basic block and maybe other notes not referencing\n+\t     deleted things.  */\n+\t  if (NOTE_LINE_NUMBER (insn) != NOTE_INSN_BASIC_BLOCK\n+\t      && (flag_ra_dump_notes\n+\t\t  || NOTE_LINE_NUMBER (insn) == NOTE_INSN_DELETED\n+\t\t  || NOTE_LINE_NUMBER (insn) == NOTE_INSN_DELETED_LABEL))\n+\t    {\n+\t      ra_print_rtx_top (file, insn, (num == 0 || !NEXT_INSN (insn)));\n+\t      num++;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  ra_print_rtx_top (file, insn, (num == 0 || !NEXT_INSN (insn)));\n+\t  num++;\n+\t}\n+    }\n+}\n+\n+/* Count how many insns were seen how often, while building the interference\n+   graph, and prints the findings.  */\n+\n+void\n+dump_number_seen ()\n+{\n+#define N 17\n+  int num[N];\n+  int i;\n+\n+  for (i = 0; i < N; i++)\n+    num[i] = 0;\n+  for (i = 0; i < get_max_uid (); i++)\n+    if (number_seen[i] < N - 1)\n+      num[number_seen[i]]++;\n+    else\n+      num[N - 1]++;\n+  for (i = 0; i < N - 1; i++)\n+    if (num[i])\n+      ra_debug_msg (DUMP_PROCESS, \"%d insns seen %d times\\n\", num[i], i);\n+  if (num[N - 1])\n+    ra_debug_msg (DUMP_PROCESS, \"%d insns seen %d and more times\\n\", num[i],\n+\t       N - 1);\n+  ra_debug_msg (DUMP_PROCESS, \"from overall %d insns\\n\", get_max_uid ());\n+#undef N\n+}\n+\n+/* Dump the interference graph, the move list and the webs.  */\n+\n+void\n+dump_igraph (df)\n+     struct df *df ATTRIBUTE_UNUSED;\n+{\n+  struct move_list *ml;\n+  unsigned int def1, def2;\n+  int num = 0;\n+  int num2;\n+  unsigned int i;\n+  if (!rtl_dump_file || (debug_new_regalloc & (DUMP_IGRAPH | DUMP_WEBS)) == 0)\n+    return;\n+  ra_debug_msg (DUMP_IGRAPH, \"conflicts:\\n  \");\n+  for (def1 = 0; def1 < num_webs; def1++)\n+    {\n+      int num1 = num;\n+      for (num2=0, def2 = 0; def2 < num_webs; def2++)\n+        if (def1 != def2 && TEST_BIT (igraph, igraph_index (def1, def2)))\n+\t  {\n+\t    if (num1 == num)\n+\t      {\n+\t        if (SUBWEB_P (ID2WEB (def1)))\n+\t\t  ra_debug_msg (DUMP_IGRAPH, \"%d (SUBREG %d, %d) with \", def1,\n+\t\t\t     ID2WEB (def1)->regno,\n+\t\t\t     SUBREG_BYTE (ID2WEB (def1)->orig_x));\n+\t        else\n+\t          ra_debug_msg (DUMP_IGRAPH, \"%d (REG %d) with \", def1,\n+\t\t\t     ID2WEB (def1)->regno);\n+\t      }\n+\t    if ((num2 % 9) == 8)\n+\t      ra_debug_msg (DUMP_IGRAPH, \"\\n              \");\n+\t    num++;\n+\t    num2++;\n+\t    if (SUBWEB_P (ID2WEB (def2)))\n+\t      ra_debug_msg (DUMP_IGRAPH, \"%d(%d,%d) \", def2, ID2WEB (def2)->regno,\n+\t\t\t SUBREG_BYTE (ID2WEB (def2)->orig_x));\n+\t    else\n+\t      ra_debug_msg (DUMP_IGRAPH, \"%d(%d) \", def2, ID2WEB (def2)->regno);\n+\t  }\n+      if (num1 != num)\n+\tra_debug_msg (DUMP_IGRAPH, \"\\n  \");\n+    }\n+  ra_debug_msg (DUMP_IGRAPH, \"\\n\");\n+  for (ml = wl_moves; ml; ml = ml->next)\n+    if (ml->move)\n+      {\n+        ra_debug_msg (DUMP_IGRAPH, \"move: insn %d: Web %d <-- Web %d\\n\",\n+\t         INSN_UID (ml->move->insn), ml->move->target_web->id,\n+\t         ml->move->source_web->id);\n+      }\n+  ra_debug_msg (DUMP_WEBS, \"\\nWebs:\\n\");\n+  for (i = 0; i < num_webs; i++)\n+    {\n+      struct web *web = ID2WEB (i);\n+\n+      ra_debug_msg (DUMP_WEBS, \"  %4d : regno %3d\", i, web->regno);\n+      if (SUBWEB_P (web))\n+\t{\n+\t  ra_debug_msg (DUMP_WEBS, \" sub %d\", SUBREG_BYTE (web->orig_x));\n+\t  ra_debug_msg (DUMP_WEBS, \" par %d\", find_web_for_subweb (web)->id);\n+\t}\n+      ra_debug_msg (DUMP_WEBS, \" +%d (span %d, cost \"\n+\t\t HOST_WIDE_INT_PRINT_DEC \") (%s)\",\n+\t         web->add_hardregs, web->span_deaths, web->spill_cost,\n+\t         reg_class_names[web->regclass]);\n+      if (web->spill_temp == 1)\n+\tra_debug_msg (DUMP_WEBS, \" (spilltemp)\");\n+      else if (web->spill_temp == 2)\n+\tra_debug_msg (DUMP_WEBS, \" (spilltem2)\");\n+      else if (web->spill_temp == 3)\n+\tra_debug_msg (DUMP_WEBS, \" (short)\");\n+      if (web->type == PRECOLORED)\n+        ra_debug_msg (DUMP_WEBS, \" (precolored, color=%d)\", web->color);\n+      else if (find_web_for_subweb (web)->num_uses == 0)\n+\tra_debug_msg (DUMP_WEBS, \" dead\");\n+      if (web->crosses_call)\n+\tra_debug_msg (DUMP_WEBS, \" xcall\");\n+      if (web->regno >= max_normal_pseudo)\n+\tra_debug_msg (DUMP_WEBS, \" stack\");\n+      ra_debug_msg (DUMP_WEBS, \"\\n\");\n+    }\n+}\n+\n+/* Dump the interference graph and webs in a format easily\n+   parsable by programs.  Used to emit real world interference graph\n+   to my custom graph colorizer.  */\n+\n+void\n+dump_igraph_machine ()\n+{\n+  unsigned int i;\n+\n+  if (!rtl_dump_file || (debug_new_regalloc & DUMP_IGRAPH_M) == 0)\n+    return;\n+  ra_debug_msg (DUMP_IGRAPH_M, \"g %d %d\\n\", num_webs - num_subwebs,\n+\t     FIRST_PSEUDO_REGISTER);\n+  for (i = 0; i < num_webs - num_subwebs; i++)\n+    {\n+      struct web *web = ID2WEB (i);\n+      struct conflict_link *cl;\n+      int flags = 0;\n+      int numc = 0;\n+      int col = 0;\n+      flags = web->spill_temp & 0xF;\n+      flags |= ((web->type == PRECOLORED) ? 1 : 0) << 4;\n+      flags |= (web->add_hardregs & 0xF) << 5;\n+      for (cl = web->conflict_list; cl; cl = cl->next)\n+\tif (cl->t->id < web->id)\n+\t  numc++;\n+      ra_debug_msg (DUMP_IGRAPH_M, \"n %d %d %d %d %d %d %d\\n\",\n+\t\t web->id, web->color, flags,\n+\t\t (unsigned int)web->spill_cost, web->num_defs, web->num_uses,\n+\t\t numc);\n+      if (web->type != PRECOLORED)\n+\t{\n+\t  ra_debug_msg (DUMP_IGRAPH_M, \"s %d\", web->id);\n+\t  while (1)\n+\t    {\n+\t      unsigned int u = 0;\n+\t      int n;\n+\t      for (n = 0; n < 32 && col < FIRST_PSEUDO_REGISTER; n++, col++)\n+\t\tif (TEST_HARD_REG_BIT (web->usable_regs, col))\n+\t\t  u |= 1 << n;\n+\t      ra_debug_msg (DUMP_IGRAPH_M, \" %u\", u);\n+\t      if (col >= FIRST_PSEUDO_REGISTER)\n+\t\tbreak;\n+\t    }\n+\t  ra_debug_msg (DUMP_IGRAPH_M, \"\\n\");\n+\t}\n+      if (numc)\n+\t{\n+\t  ra_debug_msg (DUMP_IGRAPH_M, \"c %d\", web->id);\n+\t  for (cl = web->conflict_list; cl; cl = cl->next)\n+\t    {\n+\t      if (cl->t->id < web->id)\n+\t\tra_debug_msg (DUMP_IGRAPH_M, \" %d\", cl->t->id);\n+\t    }\n+\t  ra_debug_msg (DUMP_IGRAPH_M, \"\\n\");\n+\t}\n+    }\n+  ra_debug_msg (DUMP_IGRAPH_M, \"e\\n\");\n+}\n+\n+/* This runs after colorization and changing the insn stream.\n+   It temporarily replaces all pseudo registers with their colors,\n+   and emits information, if the resulting insns are strictly valid.  */\n+\n+void\n+dump_constraints ()\n+{\n+  rtx insn;\n+  int i;\n+  if (!rtl_dump_file || (debug_new_regalloc & DUMP_CONSTRAINTS) == 0)\n+    return;\n+  for (i = FIRST_PSEUDO_REGISTER; i < ra_max_regno; i++)\n+    if (regno_reg_rtx[i] && GET_CODE (regno_reg_rtx[i]) == REG)\n+      REGNO (regno_reg_rtx[i])\n+\t  = ra_reg_renumber[i] >= 0 ? ra_reg_renumber[i] : i;\n+  for (insn = get_insns (); insn; insn = NEXT_INSN (insn))\n+    if (INSN_P (insn))\n+      {\n+\tint code;\n+\tint uid = INSN_UID (insn);\n+\tint o;\n+\t/* Don't simply force rerecognition, as combine might left us\n+\t   with some unrecongnizable ones, which later leads to aborts\n+\t   in regclass, if we now destroy the remembered INSN_CODE().  */\n+\t/*INSN_CODE (insn) = -1;*/\n+\tcode = recog_memoized (insn);\n+\tif (code < 0)\n+\t  {\n+\t    ra_debug_msg (DUMP_CONSTRAINTS,\n+\t\t       \"%d: asm insn or not recognizable.\\n\", uid);\n+\t    continue;\n+\t  }\n+\tra_debug_msg (DUMP_CONSTRAINTS,\n+\t\t   \"%d: code %d {%s}, %d operands, constraints: \",\n+\t\t   uid, code, insn_data[code].name, recog_data.n_operands);\n+        extract_insn (insn);\n+\t/*preprocess_constraints ();*/\n+\tfor (o = 0; o < recog_data.n_operands; o++)\n+\t  {\n+\t    ra_debug_msg (DUMP_CONSTRAINTS,\n+\t\t       \"%d:%s \", o, recog_data.constraints[o]);\n+\t  }\n+\tif (constrain_operands (1))\n+\t  ra_debug_msg (DUMP_CONSTRAINTS, \"matches strictly alternative %d\",\n+\t\t     which_alternative);\n+\telse\n+\t  ra_debug_msg (DUMP_CONSTRAINTS, \"doesn't match strictly\");\n+\tra_debug_msg (DUMP_CONSTRAINTS, \"\\n\");\n+      }\n+  for (i = FIRST_PSEUDO_REGISTER; i < ra_max_regno; i++)\n+    if (regno_reg_rtx[i] && GET_CODE (regno_reg_rtx[i]) == REG)\n+      REGNO (regno_reg_rtx[i]) = i;\n+}\n+\n+/* This counts and emits the cumulated cost of all spilled webs,\n+   preceded by a custom message MSG, with debug level LEVEL.  */\n+\n+void\n+dump_graph_cost (level, msg)\n+     unsigned int level;\n+     const char *msg;\n+{\n+  unsigned int i;\n+  unsigned HOST_WIDE_INT cost;\n+#define LU HOST_WIDE_INT_PRINT_UNSIGNED\n+  if (!rtl_dump_file || (debug_new_regalloc & level) == 0)\n+    return;\n+\n+  cost = 0;\n+  for (i = 0; i < num_webs; i++)\n+    {\n+      struct web *web = id2web[i];\n+      if (alias (web)->type == SPILLED)\n+\tcost += web->orig_spill_cost;\n+    }\n+  ra_debug_msg (level, \" spill cost of graph (%s) = \" LU \"\\n\",\n+\t     msg ? msg : \"\", cost);\n+#undef LU\n+}\n+\n+/* Dump the color assignment per web, the coalesced and spilled webs.  */\n+\n+void\n+dump_ra (df)\n+     struct df *df ATTRIBUTE_UNUSED;\n+{\n+  struct web *web;\n+  struct dlist *d;\n+  if (!rtl_dump_file || (debug_new_regalloc & DUMP_RESULTS) == 0)\n+    return;\n+\n+  ra_debug_msg (DUMP_RESULTS, \"\\nColored:\\n\");\n+  for (d = WEBS(COLORED); d; d = d->next)\n+    {\n+      web = DLIST_WEB (d);\n+      ra_debug_msg (DUMP_RESULTS, \"  %4d : color %d\\n\", web->id, web->color);\n+    }\n+  ra_debug_msg (DUMP_RESULTS, \"\\nCoalesced:\\n\");\n+  for (d = WEBS(COALESCED); d; d = d->next)\n+    {\n+      web = DLIST_WEB (d);\n+      ra_debug_msg (DUMP_RESULTS, \"  %4d : to web %d, color %d\\n\", web->id,\n+\t         alias (web)->id, web->color);\n+    }\n+  ra_debug_msg (DUMP_RESULTS, \"\\nSpilled:\\n\");\n+  for (d = WEBS(SPILLED); d; d = d->next)\n+    {\n+      web = DLIST_WEB (d);\n+      ra_debug_msg (DUMP_RESULTS, \"  %4d\\n\", web->id);\n+    }\n+  ra_debug_msg (DUMP_RESULTS, \"\\n\");\n+  dump_cost (DUMP_RESULTS);\n+}\n+\n+/* Calculate and dump the cumulated costs of certain types of insns\n+   (loads, stores and copies).  */\n+\n+void\n+dump_static_insn_cost (file, message, prefix)\n+     FILE *file;\n+     const char *message;\n+     const char *prefix;\n+{\n+  struct cost\n+    {\n+      unsigned HOST_WIDE_INT cost;\n+      unsigned int count;\n+    };\n+  struct cost load = {0, 0};\n+  struct cost store = {0, 0};\n+  struct cost regcopy = {0, 0};\n+  struct cost selfcopy = {0, 0};\n+  struct cost overall = {0, 0};\n+  basic_block bb;\n+\n+  if (!file)\n+    return;\n+\n+  FOR_EACH_BB (bb)\n+    {\n+      unsigned HOST_WIDE_INT block_cost = bb->frequency;\n+      rtx insn, set;\n+      for (insn = bb->head; insn; insn = NEXT_INSN (insn))\n+\t{\n+\t  /* Yes, yes.  We don't calculate the costs precisely.\n+\t     Only for \"simple enough\" insns.  Those containing single\n+\t     sets only.  */\n+\t  if (INSN_P (insn) && ((set = single_set (insn)) != NULL))\n+\t    {\n+\t      rtx src = SET_SRC (set);\n+\t      rtx dest = SET_DEST (set);\n+\t      struct cost *pcost = NULL;\n+\t      overall.cost += block_cost;\n+\t      overall.count++;\n+\t      if (rtx_equal_p (src, dest))\n+\t\tpcost = &selfcopy;\n+\t      else if (GET_CODE (src) == GET_CODE (dest)\n+\t\t       && ((GET_CODE (src) == REG)\n+\t\t\t   || (GET_CODE (src) == SUBREG\n+\t\t\t       && GET_CODE (SUBREG_REG (src)) == REG\n+\t\t\t       && GET_CODE (SUBREG_REG (dest)) == REG)))\n+\t\tpcost = &regcopy;\n+\t      else\n+\t\t{\n+\t\t  if (GET_CODE (src) == SUBREG)\n+\t\t    src = SUBREG_REG (src);\n+\t\t  if (GET_CODE (dest) == SUBREG)\n+\t\t    dest = SUBREG_REG (dest);\n+\t\t  if (GET_CODE (src) == MEM && GET_CODE (dest) != MEM\n+\t\t      && memref_is_stack_slot (src))\n+\t\t    pcost = &load;\n+\t\t  else if (GET_CODE (src) != MEM && GET_CODE (dest) == MEM\n+\t\t\t   && memref_is_stack_slot (dest))\n+\t\t    pcost = &store;\n+\t\t}\n+\t      if (pcost)\n+\t\t{\n+\t\t  pcost->cost += block_cost;\n+\t\t  pcost->count++;\n+\t\t}\n+\t    }\n+\t  if (insn == bb->end)\n+\t    break;\n+\t}\n+    }\n+\n+  if (!prefix)\n+    prefix = \"\";\n+  fprintf (file, \"static insn cost %s\\n\", message ? message : \"\");\n+  fprintf (file, \"  %soverall:\\tnum=%6d\\tcost=%8d\\n\", prefix, overall.count,\n+\t   overall.cost);\n+  fprintf (file, \"  %sloads:\\tnum=%6d\\tcost=%8d\\n\", prefix, load.count,\n+\t   load.cost);\n+  fprintf (file, \"  %sstores:\\tnum=%6d\\tcost=%8d\\n\", prefix,\n+\t   store.count, store.cost);\n+  fprintf (file, \"  %sregcopy:\\tnum=%6d\\tcost=%8d\\n\", prefix, regcopy.count,\n+\t   regcopy.cost);\n+  fprintf (file, \"  %sselfcpy:\\tnum=%6d\\tcost=%8d\\n\", prefix, selfcopy.count,\n+\t   selfcopy.cost);\n+}\n+\n+/* Returns nonzero, if WEB1 and WEB2 have some possible\n+   hardregs in common.  */\n+\n+int\n+web_conflicts_p (web1, web2)\n+     struct web *web1;\n+     struct web *web2;\n+{\n+  if (web1->type == PRECOLORED && web2->type == PRECOLORED)\n+    return 0;\n+\n+  if (web1->type == PRECOLORED)\n+    return TEST_HARD_REG_BIT (web2->usable_regs, web1->regno);\n+\n+  if (web2->type == PRECOLORED)\n+    return TEST_HARD_REG_BIT (web1->usable_regs, web2->regno);\n+\n+  return hard_regs_intersect_p (&web1->usable_regs, &web2->usable_regs);\n+}\n+\n+/* Dump all uids of insns in which WEB is mentioned.  */\n+\n+void\n+dump_web_insns (web)\n+     struct web *web;\n+{\n+  unsigned int i;\n+\n+  ra_debug_msg (DUMP_EVER, \"Web: %i(%i)+%i class: %s freedom: %i degree %i\\n\",\n+\t     web->id, web->regno, web->add_hardregs,\n+\t     reg_class_names[web->regclass],\n+\t     web->num_freedom, web->num_conflicts);\n+  ra_debug_msg (DUMP_EVER, \"   def insns:\");\n+\n+  for (i = 0; i < web->num_defs; ++i)\n+    {\n+      ra_debug_msg (DUMP_EVER, \" %d \", INSN_UID (web->defs[i]->insn));\n+    }\n+\n+  ra_debug_msg (DUMP_EVER, \"\\n   use insns:\");\n+  for (i = 0; i < web->num_uses; ++i)\n+    {\n+      ra_debug_msg (DUMP_EVER, \" %d \", INSN_UID (web->uses[i]->insn));\n+    }\n+  ra_debug_msg (DUMP_EVER, \"\\n\");\n+}\n+\n+/* Dump conflicts for web WEB.  */\n+\n+void\n+dump_web_conflicts (web)\n+     struct web *web;\n+{\n+  int num = 0;\n+  unsigned int def2;\n+\n+  ra_debug_msg (DUMP_EVER, \"Web: %i(%i)+%i class: %s freedom: %i degree %i\\n\",\n+\t     web->id, web->regno, web->add_hardregs,\n+\t     reg_class_names[web->regclass],\n+\t     web->num_freedom, web->num_conflicts);\n+\n+  for (def2 = 0; def2 < num_webs; def2++)\n+    if (TEST_BIT (igraph, igraph_index (web->id, def2)) && web->id != def2)\n+      {\n+\tif ((num % 9) == 5)\n+\t  ra_debug_msg (DUMP_EVER, \"\\n             \");\n+\tnum++;\n+\n+\tra_debug_msg (DUMP_EVER, \" %d(%d)\", def2, id2web[def2]->regno);\n+\tif (id2web[def2]->add_hardregs)\n+\t  ra_debug_msg (DUMP_EVER, \"+%d\", id2web[def2]->add_hardregs);\n+\n+\tif (web_conflicts_p (web, id2web[def2]))\n+\t  ra_debug_msg (DUMP_EVER, \"/x\");\n+\n+\tif (id2web[def2]->type == SELECT)\n+\t  ra_debug_msg (DUMP_EVER, \"/s\");\n+\n+\tif (id2web[def2]->type == COALESCED)\n+\t  ra_debug_msg (DUMP_EVER,\"/c/%d\", alias (id2web[def2])->id);\n+      }\n+  ra_debug_msg (DUMP_EVER, \"\\n\");\n+  {\n+    struct conflict_link *wl;\n+    num = 0;\n+    ra_debug_msg (DUMP_EVER, \"By conflicts:     \");\n+    for (wl = web->conflict_list; wl; wl = wl->next)\n+      {\n+\tstruct web* w = wl->t;\n+\tif ((num % 9) == 8)\n+\t  ra_debug_msg (DUMP_EVER, \"\\n              \");\n+\tnum++;\n+\tra_debug_msg (DUMP_EVER, \"%d(%d)%s \", w->id, w->regno,\n+\t\t   web_conflicts_p (web, w) ? \"+\" : \"\");\n+      }\n+    ra_debug_msg (DUMP_EVER, \"\\n\");\n+  }\n+}\n+\n+/* Output HARD_REG_SET to stderr.  */\n+\n+void\n+debug_hard_reg_set (set)\n+     HARD_REG_SET set;\n+{\n+  int i;\n+  for (i=0; i < FIRST_PSEUDO_REGISTER; ++i)\n+    {\n+      if (TEST_HARD_REG_BIT (set, i))\n+\t{\n+\t  fprintf (stderr, \"%s \", reg_names[i]);\n+\t}\n+    }\n+  fprintf (stderr, \"\\n\");\n+}\n+\n+/*\n+vim:cinoptions={.5s,g0,p5,t0,(0,^-0.5s,n-0.5s:tw=78:cindent:sw=4:\n+*/"}, {"sha": "dabd226f65b5c73b3c51977b8ddb0b8816f9c28e", "filename": "gcc/ra-rewrite.c", "status": "added", "additions": 1985, "deletions": 0, "changes": 1985, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fra-rewrite.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fra-rewrite.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fra-rewrite.c?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -0,0 +1,1985 @@\n+/* Graph coloring register allocator\n+   Copyright (C) 2001, 2002 Free Software Foundation, Inc.\n+   Contributed by Michael Matz <matz@suse.de>\n+   and Daniel Berlin <dan@cgsoftware.com>.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it under the\n+   terms of the GNU General Public License as published by the Free Software\n+   Foundation; either version 2, or (at your option) any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n+   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n+   details.\n+\n+   You should have received a copy of the GNU General Public License along\n+   with GCC; see the file COPYING.  If not, write to the Free Software\n+   Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"function.h\"\n+#include \"regs.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"df.h\"\n+#include \"expr.h\"\n+#include \"output.h\"\n+#include \"except.h\"\n+#include \"ra.h\"\n+\n+/* This file is part of the graph coloring register allocator, and\n+   contains the functions to change the insn stream.  I.e. it adds\n+   spill code, rewrites insns to use the new registers after\n+   coloring and deletes coalesced moves.  */\n+\n+struct rewrite_info;\n+struct rtx_list;\n+\n+static void spill_coalescing PARAMS ((sbitmap, sbitmap));\n+static unsigned HOST_WIDE_INT spill_prop_savings PARAMS ((struct web *,\n+\t\t\t\t\t\t\t  sbitmap));\n+static void spill_prop_insert PARAMS ((struct web *, sbitmap, sbitmap));\n+static int spill_propagation PARAMS ((sbitmap, sbitmap, sbitmap));\n+static void spill_coalprop PARAMS ((void));\n+static void allocate_spill_web PARAMS ((struct web *));\n+static void choose_spill_colors PARAMS ((void));\n+static void rewrite_program PARAMS ((bitmap));\n+static void remember_slot PARAMS ((struct rtx_list **, rtx));\n+static int slots_overlap_p PARAMS ((rtx, rtx));\n+static void delete_overlapping_slots PARAMS ((struct rtx_list **, rtx));\n+static int slot_member_p PARAMS ((struct rtx_list *, rtx));\n+static void insert_stores PARAMS ((bitmap));\n+static int spill_same_color_p PARAMS ((struct web *, struct web *));\n+static int is_partly_live_1 PARAMS ((sbitmap, struct web *));\n+static void update_spill_colors PARAMS ((HARD_REG_SET *, struct web *, int));\n+static int spill_is_free PARAMS ((HARD_REG_SET *, struct web *));\n+static void emit_loads PARAMS ((struct rewrite_info *, int, rtx));\n+static void reloads_to_loads PARAMS ((struct rewrite_info *, struct ref **,\n+\t\t\t\t      unsigned int, struct web **));\n+static void rewrite_program2 PARAMS ((bitmap));\n+static void mark_refs_for_checking PARAMS ((struct web *, bitmap));\n+static void detect_web_parts_to_rebuild PARAMS ((void));\n+static void delete_useless_defs PARAMS ((void));\n+static void detect_non_changed_webs PARAMS ((void));\n+static void reset_changed_flag PARAMS ((void));\n+\n+/* For tracking some statistics, we count the number (and cost)\n+   of deleted move insns.  */\n+static unsigned int deleted_move_insns;\n+static unsigned HOST_WIDE_INT deleted_move_cost;\n+\n+/* This is the spill coalescing phase.  In SPILLED the IDs of all\n+   already spilled webs are noted.  In COALESCED the IDs of webs still\n+   to check for coalescing.  This tries to coalesce two webs, which were\n+   spilled, are connected by a move, and don't conflict.  Greatly\n+   reduces memory shuffling.  */\n+\n+static void\n+spill_coalescing (coalesce, spilled)\n+     sbitmap coalesce, spilled;\n+{\n+  struct move_list *ml;\n+  struct move *m;\n+  for (ml = wl_moves; ml; ml = ml->next)\n+    if ((m = ml->move) != NULL)\n+      {\n+\tstruct web *s = alias (m->source_web);\n+\tstruct web *t = alias (m->target_web);\n+\tif ((TEST_BIT (spilled, s->id) && TEST_BIT (coalesce, t->id))\n+\t    || (TEST_BIT (spilled, t->id) && TEST_BIT (coalesce, s->id)))\n+\t  {\n+\t    struct conflict_link *wl;\n+\t    if (TEST_BIT (sup_igraph, s->id * num_webs + t->id)\n+\t\t|| TEST_BIT (sup_igraph, t->id * num_webs + s->id)\n+\t\t|| s->pattern || t->pattern)\n+\t      continue;\n+\n+\t    deleted_move_insns++;\n+\t    deleted_move_cost += BLOCK_FOR_INSN (m->insn)->frequency + 1;\n+\t    PUT_CODE (m->insn, NOTE);\n+\t    NOTE_LINE_NUMBER (m->insn) = NOTE_INSN_DELETED;\n+\t    df_insn_modify (df, BLOCK_FOR_INSN (m->insn), m->insn);\n+\n+\t    m->target_web->target_of_spilled_move = 1;\n+\t    if (s == t)\n+\t      /* May be, already coalesced due to a former move.  */\n+\t      continue;\n+\t    /* Merge the nodes S and T in the I-graph.  Beware: the merging\n+\t       of conflicts relies on the fact, that in the conflict list\n+\t       of T all of it's conflicts are noted.  This is currently not\n+\t       the case if T would be the target of a coalesced web, because\n+\t       then (in combine () above) only those conflicts were noted in\n+\t       T from the web which was coalesced into T, which at the time\n+\t       of combine() were not already on the SELECT stack or were\n+\t       itself coalesced to something other.  */\n+\t    if (t->type != SPILLED || s->type != SPILLED)\n+\t      abort ();\n+\t    remove_list (t->dlink, &WEBS(SPILLED));\n+\t    put_web (t, COALESCED);\n+\t    t->alias = s;\n+\t    s->is_coalesced = 1;\n+\t    t->is_coalesced = 1;\n+\t    merge_moves (s, t);\n+\t    for (wl = t->conflict_list; wl; wl = wl->next)\n+\t      {\n+\t\tstruct web *pweb = wl->t;\n+\t\tif (wl->sub == NULL)\n+\t\t  record_conflict (s, pweb);\n+\t\telse\n+\t\t  {\n+\t\t    struct sub_conflict *sl;\n+\t\t    for (sl = wl->sub; sl; sl = sl->next)\n+\t\t      {\n+\t\t\tstruct web *sweb = NULL;\n+\t\t\tif (SUBWEB_P (sl->s))\n+\t\t\t  sweb = find_subweb (s, sl->s->orig_x);\n+\t\t\tif (!sweb)\n+\t\t\t  sweb = s;\n+\t\t\trecord_conflict (sweb, sl->t);\n+\t\t      }\n+\t\t  }\n+\t\t/* No decrement_degree here, because we already have colored\n+\t\t   the graph, and don't want to insert pweb into any other\n+\t\t   list.  */\n+\t\tpweb->num_conflicts -= 1 + t->add_hardregs;\n+\t      }\n+\t  }\n+      }\n+}\n+\n+/* Returns the probable saving of coalescing WEB with webs from\n+   SPILLED, in terms of removed move insn cost.  */\n+\n+static unsigned HOST_WIDE_INT\n+spill_prop_savings (web, spilled)\n+     struct web *web;\n+     sbitmap spilled;\n+{\n+  unsigned HOST_WIDE_INT savings = 0;\n+  struct move_list *ml;\n+  struct move *m;\n+  unsigned int cost;\n+  if (web->pattern)\n+    return 0;\n+  cost = 1 + MEMORY_MOVE_COST (GET_MODE (web->orig_x), web->regclass, 1);\n+  cost += 1 + MEMORY_MOVE_COST (GET_MODE (web->orig_x), web->regclass, 0);\n+  for (ml = wl_moves; ml; ml = ml->next)\n+    if ((m = ml->move) != NULL)\n+      {\n+\tstruct web *s = alias (m->source_web);\n+\tstruct web *t = alias (m->target_web);\n+\tif (s != web)\n+\t  {\n+\t    struct web *h = s;\n+\t    s = t;\n+\t    t = h;\n+\t  }\n+\tif (s != web || !TEST_BIT (spilled, t->id) || t->pattern\n+\t    || TEST_BIT (sup_igraph, s->id * num_webs + t->id)\n+\t    || TEST_BIT (sup_igraph, t->id * num_webs + s->id))\n+\t  continue;\n+\tsavings += BLOCK_FOR_INSN (m->insn)->frequency * cost;\n+      }\n+  return savings;\n+}\n+\n+/* This add all IDs of colored webs, which are connected to WEB by a move\n+   to LIST and PROCESSED.  */\n+\n+static void\n+spill_prop_insert (web, list, processed)\n+     struct web *web;\n+     sbitmap list, processed;\n+{\n+  struct move_list *ml;\n+  struct move *m;\n+  for (ml = wl_moves; ml; ml = ml->next)\n+    if ((m = ml->move) != NULL)\n+      {\n+\tstruct web *s = alias (m->source_web);\n+\tstruct web *t = alias (m->target_web);\n+\tif (s != web)\n+\t  {\n+\t    struct web *h = s;\n+\t    s = t;\n+\t    t = h;\n+\t  }\n+\tif (s != web || t->type != COLORED || TEST_BIT (processed, t->id))\n+\t  continue;\n+\tSET_BIT (list, t->id);\n+\tSET_BIT (processed, t->id);\n+      }\n+}\n+\n+/* The spill propagation pass.  If we have to spilled webs, the first\n+   connected through a move to a colored one, and the second also connected\n+   to that colored one, and this colored web is only used to connect both\n+   spilled webs, it might be worthwhile to spill that colored one.\n+   This is the case, if the cost of the removed copy insns (all three webs\n+   could be placed into the same stack slot) is higher than the spill cost\n+   of the web.\n+   TO_PROP are the webs we try to propagate from (i.e. spilled ones),\n+   SPILLED the set of all spilled webs so far and PROCESSED the set\n+   of all webs processed so far, so we don't do work twice.  */\n+\n+static int\n+spill_propagation (to_prop, spilled, processed)\n+     sbitmap to_prop, spilled, processed;\n+{\n+  int id;\n+  int again = 0;\n+  sbitmap list = sbitmap_alloc (num_webs);\n+  sbitmap_zero (list);\n+\n+  /* First insert colored move neighbors into the candidate list.  */\n+  EXECUTE_IF_SET_IN_SBITMAP (to_prop, 0, id,\n+    {\n+      spill_prop_insert (ID2WEB (id), list, processed);\n+    });\n+  sbitmap_zero (to_prop);\n+\n+  /* For all candidates, see, if the savings are higher than it's\n+     spill cost.  */\n+  while ((id = sbitmap_first_set_bit (list)) >= 0)\n+    {\n+      struct web *web = ID2WEB (id);\n+      RESET_BIT (list, id);\n+      if (spill_prop_savings (web, spilled) >= web->spill_cost)\n+\t{\n+\t  /* If so, we found a new spilled web.  Insert it's colored\n+\t     move neighbors again, and mark, that we need to repeat the\n+\t     whole mainloop of spillprog/coalescing again.  */\n+\t  remove_web_from_list (web);\n+\t  web->color = -1;\n+\t  put_web (web, SPILLED);\n+\t  SET_BIT (spilled, id);\n+\t  SET_BIT (to_prop, id);\n+\t  spill_prop_insert (web, list, processed);\n+\t  again = 1;\n+\t}\n+    }\n+  sbitmap_free (list);\n+  return again;\n+}\n+\n+/* The main phase to improve spill costs.  This repeatedly runs\n+   spill coalescing and spill propagation, until nothing changes.  */\n+\n+static void\n+spill_coalprop ()\n+{\n+  sbitmap spilled, processed, to_prop;\n+  struct dlist *d;\n+  int again;\n+  spilled = sbitmap_alloc (num_webs);\n+  processed = sbitmap_alloc (num_webs);\n+  to_prop = sbitmap_alloc (num_webs);\n+  sbitmap_zero (spilled);\n+  for (d = WEBS(SPILLED); d; d = d->next)\n+    SET_BIT (spilled, DLIST_WEB (d)->id);\n+  sbitmap_copy (to_prop, spilled);\n+  sbitmap_zero (processed);\n+  do\n+    {\n+      spill_coalescing (to_prop, spilled);\n+      /* XXX Currently (with optimistic coalescing) spill_propagation()\n+\t doesn't give better code, sometimes it gives worse (but not by much)\n+\t code.  I believe this is because of slightly wrong cost\n+\t measurements.  Anyway right now it isn't worth the time it takes,\n+\t so deactivate it for now.  */\n+      again = 0 && spill_propagation (to_prop, spilled, processed);\n+    }\n+  while (again);\n+  sbitmap_free (to_prop);\n+  sbitmap_free (processed);\n+  sbitmap_free (spilled);\n+}\n+\n+/* Allocate a spill slot for a WEB.  Currently we spill to pseudo\n+   registers, to be able to track also webs for \"stack slots\", and also\n+   to possibly colorize them.  These pseudos are sometimes handled\n+   in a special way, where we know, that they also can represent\n+   MEM references.  */\n+\n+static void\n+allocate_spill_web (web)\n+     struct web *web;\n+{\n+  int regno = web->regno;\n+  rtx slot;\n+  if (web->stack_slot)\n+    return;\n+  slot = gen_reg_rtx (PSEUDO_REGNO_MODE (regno));\n+  web->stack_slot = slot;\n+}\n+\n+/* This chooses a color for all SPILLED webs for interference region\n+   spilling.  The heuristic isn't good in any way.  */\n+\n+static void\n+choose_spill_colors ()\n+{\n+  struct dlist *d;\n+  unsigned HOST_WIDE_INT *costs = (unsigned HOST_WIDE_INT *)\n+    xmalloc (FIRST_PSEUDO_REGISTER * sizeof (costs[0]));\n+  for (d = WEBS(SPILLED); d; d = d->next)\n+    {\n+      struct web *web = DLIST_WEB (d);\n+      struct conflict_link *wl;\n+      int bestc, c;\n+      HARD_REG_SET avail;\n+      memset (costs, 0, FIRST_PSEUDO_REGISTER * sizeof (costs[0]));\n+      for (wl = web->conflict_list; wl; wl = wl->next)\n+\t{\n+\t  struct web *pweb = wl->t;\n+\t  if (pweb->type == COLORED || pweb->type == PRECOLORED)\n+\t    costs[pweb->color] += pweb->spill_cost;\n+\t}\n+\n+      COPY_HARD_REG_SET (avail, web->usable_regs);\n+      if (web->crosses_call)\n+\t{\n+\t  /* Add an arbitrary constant cost to colors not usable by\n+\t     call-crossing webs without saves/loads.  */\n+\t  for (c = 0; c < FIRST_PSEUDO_REGISTER; c++)\n+\t    if (TEST_HARD_REG_BIT (call_used_reg_set, c))\n+\t      costs[c] += 1000;\n+\t}\n+      bestc = -1;\n+      for (c = 0; c < FIRST_PSEUDO_REGISTER; c++)\n+\tif ((bestc < 0 || costs[bestc] > costs[c])\n+            && TEST_HARD_REG_BIT (avail, c)\n+\t    && HARD_REGNO_MODE_OK (c, PSEUDO_REGNO_MODE (web->regno)))\n+\t  {\n+\t    int i, size;\n+\t    size = HARD_REGNO_NREGS (c, PSEUDO_REGNO_MODE (web->regno));\n+\t    for (i = 1; i < size\n+\t\t && TEST_HARD_REG_BIT (avail, c + i); i++);\n+\t    if (i == size)\n+\t      bestc = c;\n+\t  }\n+      web->color = bestc;\n+      ra_debug_msg (DUMP_PROCESS, \"choosing color %d for spilled web %d\\n\",\n+\t\t bestc, web->id);\n+    }\n+\n+  free (costs);\n+}\n+\n+/* For statistics sake we count the number and cost of all new loads,\n+   stores and emitted rematerializations.  */\n+static unsigned int emitted_spill_loads;\n+static unsigned int emitted_spill_stores;\n+static unsigned int emitted_remat;\n+static unsigned HOST_WIDE_INT spill_load_cost;\n+static unsigned HOST_WIDE_INT spill_store_cost;\n+static unsigned HOST_WIDE_INT spill_remat_cost;\n+\n+/* In rewrite_program2() we detect if some def us useless, in the sense,\n+   that the pseudo set is not live anymore at that point.  The REF_IDs\n+   of such defs are noted here.  */\n+static bitmap useless_defs;\n+\n+/* This is the simple and fast version of rewriting the program to\n+   include spill code.  It spills at every insn containing spilled\n+   defs or uses.  Loads are added only if flag_ra_spill_every_use is\n+   nonzero, otherwise only stores will be added.  This doesn't\n+   support rematerialization. \n+   NEW_DEATHS is filled with uids for insns, which probably contain\n+   deaths.  */\n+\n+static void\n+rewrite_program (new_deaths)\n+     bitmap new_deaths;\n+{\n+  unsigned int i;\n+  struct dlist *d;\n+  bitmap b = BITMAP_XMALLOC ();\n+\n+  /* We walk over all webs, over all uses/defs.  For all webs, we need\n+     to look at spilled webs, and webs coalesced to spilled ones, in case\n+     their alias isn't broken up, or they got spill coalesced.  */\n+  for (i = 0; i < 2; i++)\n+    for (d = (i == 0) ? WEBS(SPILLED) : WEBS(COALESCED); d; d = d->next)\n+      {\n+\tstruct web *web = DLIST_WEB (d);\n+\tstruct web *aweb = alias (web);\n+\tunsigned int j;\n+\trtx slot;\n+\n+\t/* Is trivially true for spilled webs, but not for coalesced ones.  */\n+\tif (aweb->type != SPILLED)\n+\t  continue;\n+\n+\t/* First add loads before every use, if we have to.  */\n+\tif (flag_ra_spill_every_use)\n+\t  {\n+\t    bitmap_clear (b);\n+\t    allocate_spill_web (aweb);\n+\t    slot = aweb->stack_slot;\n+\t    for (j = 0; j < web->num_uses; j++)\n+\t      {\n+\t\trtx insns, target, source;\n+\t\trtx insn = DF_REF_INSN (web->uses[j]);\n+\t\trtx prev = PREV_INSN (insn);\n+\t\tbasic_block bb = BLOCK_FOR_INSN (insn);\n+\t\t/* Happens when spill_coalescing() deletes move insns.  */\n+\t\tif (!INSN_P (insn))\n+\t\t  continue;\n+\n+\t\t/* Check that we didn't already added a load for this web\n+\t\t   and insn.  Happens, when the an insn uses the same web\n+\t\t   multiple times.  */\n+\t        if (bitmap_bit_p (b, INSN_UID (insn)))\n+\t\t  continue;\n+\t        bitmap_set_bit (b, INSN_UID (insn));\n+\t        target = DF_REF_REG (web->uses[j]);\n+\t        source = slot;\n+\t\tstart_sequence ();\n+\t        if (GET_CODE (target) == SUBREG)\n+\t\t  source = simplify_gen_subreg (GET_MODE (target), source,\n+\t\t\t\t\t\tGET_MODE (source),\n+\t\t\t\t\t\tSUBREG_BYTE (target));\n+\t\tra_emit_move_insn (target, source);\n+\t\tinsns = get_insns ();\n+\t\tend_sequence ();\n+\t\temit_insn_before (insns, insn);\n+\n+\t        if (bb->head == insn)\n+\t\t  bb->head = NEXT_INSN (prev);\n+\t\tfor (insn = PREV_INSN (insn); insn != prev;\n+\t\t     insn = PREV_INSN (insn))\n+\t\t  {\n+\t\t    set_block_for_insn (insn, bb);\n+\t\t    df_insn_modify (df, bb, insn);\n+\t\t  }\n+\n+\t\temitted_spill_loads++;\n+\t\tspill_load_cost += bb->frequency + 1;\n+\t      }\n+\t  }\n+\n+\t/* Now emit the stores after each def.\n+\t   If any uses were loaded from stackslots (compared to\n+\t   rematerialized or not reloaded due to IR spilling),\n+\t   aweb->stack_slot will be set.  If not, we don't need to emit\n+\t   any stack stores.  */\n+\tslot = aweb->stack_slot;\n+\tbitmap_clear (b);\n+\tif (slot)\n+\t  for (j = 0; j < web->num_defs; j++)\n+\t    {\n+\t      rtx insns, source, dest;\n+\t      rtx insn = DF_REF_INSN (web->defs[j]);\n+\t      rtx following = NEXT_INSN (insn);\n+\t      basic_block bb = BLOCK_FOR_INSN (insn);\n+\t      /* Happens when spill_coalescing() deletes move insns.  */\n+\t      if (!INSN_P (insn))\n+\t\tcontinue;\n+\t      if (bitmap_bit_p (b, INSN_UID (insn)))\n+\t\tcontinue;\n+\t      bitmap_set_bit (b, INSN_UID (insn));\n+\t      start_sequence ();\n+\t      source = DF_REF_REG (web->defs[j]);\n+\t      dest = slot;\n+\t      if (GET_CODE (source) == SUBREG)\n+\t\tdest = simplify_gen_subreg (GET_MODE (source), dest,\n+\t\t\t\t\t    GET_MODE (dest),\n+\t\t\t\t\t    SUBREG_BYTE (source));\n+\t      ra_emit_move_insn (dest, source);\n+\n+\t      insns = get_insns ();\n+\t      end_sequence ();\n+\t      if (insns)\n+\t\t{\n+\t\t  emit_insn_after (insns, insn);\n+\t\t  if (bb->end == insn)\n+\t\t    bb->end = PREV_INSN (following);\n+\t\t  for (insn = insns; insn != following; insn = NEXT_INSN (insn))\n+\t\t    {\n+\t\t      set_block_for_insn (insn, bb);\n+\t\t      df_insn_modify (df, bb, insn);\n+\t\t    }\n+\t\t}\n+\t      else\n+\t\tdf_insn_modify (df, bb, insn);\n+\t      emitted_spill_stores++;\n+\t      spill_store_cost += bb->frequency + 1;\n+\t      /* XXX we should set new_deaths for all inserted stores\n+\t\t whose pseudo dies here.\n+\t\t Note, that this isn't the case for _all_ stores.  */\n+\t      /* I.e. the next is wrong, and might cause some spilltemps\n+\t\t to be categorized as spilltemp2's (i.e. live over a death),\n+\t\t although they aren't.  This might make them spill again,\n+\t\t which causes endlessness in the case, this insn is in fact\n+\t\t _no_ death.  */\n+\t      bitmap_set_bit (new_deaths, INSN_UID (PREV_INSN (following)));\n+\t    }\n+      }\n+\n+  BITMAP_XFREE (b);\n+}\n+\n+/* A simple list of rtx's.  */\n+struct rtx_list\n+{\n+  struct rtx_list *next;\n+  rtx x;\n+};\n+\n+/* Adds X to *LIST.  */\n+\n+static void\n+remember_slot (list, x)\n+     struct rtx_list **list;\n+     rtx x;\n+{\n+  struct rtx_list *l;\n+  /* PRE: X is not already in LIST.  */\n+  l = (struct rtx_list *) ra_alloc (sizeof (*l));\n+  l->next = *list;\n+  l->x = x;\n+  *list = l;\n+}\n+\n+/* Given two rtx' S1 and S2, either being REGs or MEMs (or SUBREGs\n+   thereof), return non-zero, if they overlap.  REGs and MEMs don't\n+   overlap, and if they are MEMs they must have an easy address\n+   (plus (basereg) (const_inst x)), otherwise they overlap.  */\n+\n+static int\n+slots_overlap_p (s1, s2)\n+     rtx s1, s2;\n+{\n+  rtx base1, base2;\n+  HOST_WIDE_INT ofs1 = 0, ofs2 = 0;\n+  int size1 = GET_MODE_SIZE (GET_MODE (s1));\n+  int size2 = GET_MODE_SIZE (GET_MODE (s2));\n+  if (GET_CODE (s1) == SUBREG)\n+    ofs1 = SUBREG_BYTE (s1), s1 = SUBREG_REG (s1);\n+  if (GET_CODE (s2) == SUBREG)\n+    ofs2 = SUBREG_BYTE (s2), s2 = SUBREG_REG (s2);\n+\n+  if (s1 == s2)\n+    return 1;\n+\n+  if (GET_CODE (s1) != GET_CODE (s2))\n+    return 0;\n+\n+  if (GET_CODE (s1) == REG && GET_CODE (s2) == REG)\n+    {\n+      if (REGNO (s1) != REGNO (s2))\n+\treturn 0;\n+      if (ofs1 >= ofs2 + size2 || ofs2 >= ofs1 + size1)\n+\treturn 0;\n+      return 1;\n+    }\n+  if (GET_CODE (s1) != MEM || GET_CODE (s2) != MEM)\n+    abort ();\n+  s1 = XEXP (s1, 0);\n+  s2 = XEXP (s2, 0);\n+  if (GET_CODE (s1) != PLUS || GET_CODE (XEXP (s1, 0)) != REG\n+      || GET_CODE (XEXP (s1, 1)) != CONST_INT)\n+    return 1;\n+  if (GET_CODE (s2) != PLUS || GET_CODE (XEXP (s2, 0)) != REG\n+      || GET_CODE (XEXP (s2, 1)) != CONST_INT)\n+    return 1;\n+  base1 = XEXP (s1, 0);\n+  base2 = XEXP (s2, 0);\n+  if (!rtx_equal_p (base1, base2))\n+    return 1;\n+  ofs1 += INTVAL (XEXP (s1, 1));\n+  ofs2 += INTVAL (XEXP (s2, 1));\n+  if (ofs1 >= ofs2 + size2 || ofs2 >= ofs1 + size1)\n+    return 0;\n+  return 1;\n+}\n+\n+/* This deletes from *LIST all rtx's which overlap with X in the sense\n+   of slots_overlap_p().  */\n+\n+static void\n+delete_overlapping_slots (list, x)\n+     struct rtx_list **list;\n+     rtx x;\n+{\n+  while (*list)\n+    {\n+      if (slots_overlap_p ((*list)->x, x))\n+\t*list = (*list)->next;\n+      else\n+\tlist = &((*list)->next);\n+    }\n+}\n+\n+/* Returns nonzero, of X is member of LIST.  */\n+\n+static int\n+slot_member_p (list, x)\n+     struct rtx_list *list;\n+     rtx x;\n+{\n+  for (;list; list = list->next)\n+    if (rtx_equal_p (list->x, x))\n+      return 1;\n+  return 0;\n+}\n+\n+/* A more sophisticated (and slower) method of adding the stores, than\n+   rewrite_program().  This goes backward the insn stream, adding\n+   stores as it goes, but only if it hasn't just added a store to the\n+   same location.  NEW_DEATHS is a bitmap filled with uids of insns\n+   containing deaths.  */\n+\n+static void\n+insert_stores (new_deaths)\n+     bitmap new_deaths;\n+{\n+  rtx insn;\n+  rtx last_slot = NULL_RTX;\n+  struct rtx_list *slots = NULL;\n+\n+  /* We go simply backwards over basic block borders.  */\n+  for (insn = get_last_insn (); insn; insn = PREV_INSN (insn))\n+    {\n+      int uid = INSN_UID (insn);\n+\n+      /* If we reach a basic block border, which has more than one\n+\t outgoing edge, we simply forget all already emitted stores.  */\n+      if (GET_CODE (insn) == BARRIER\n+\t  || JUMP_P (insn) || can_throw_internal (insn))\n+\t{\n+\t  last_slot = NULL_RTX;\n+\t  slots = NULL;\n+\t}\n+      if (!INSN_P (insn))\n+\tcontinue;\n+\n+      /* If this insn was not just added in this pass.  */\n+      if (uid < insn_df_max_uid)\n+\t{\n+\t  unsigned int n;\n+\t  struct ra_insn_info info = insn_df[uid];\n+\t  rtx following = NEXT_INSN (insn);\n+\t  basic_block bb = BLOCK_FOR_INSN (insn);\n+\t  for (n = 0; n < info.num_defs; n++)\n+\t    {\n+\t      struct web *web = def2web[DF_REF_ID (info.defs[n])];\n+\t      struct web *aweb = alias (find_web_for_subweb (web));\n+\t      rtx slot, source;\n+\t      if (aweb->type != SPILLED || !aweb->stack_slot)\n+\t\tcontinue;\n+\t      slot = aweb->stack_slot;\n+\t      source = DF_REF_REG (info.defs[n]);\n+\t      /* adjust_address() might generate code.  */\n+\t      start_sequence ();\n+\t      if (GET_CODE (source) == SUBREG)\n+\t\tslot = simplify_gen_subreg (GET_MODE (source), slot,\n+\t\t\t\t\t    GET_MODE (slot),\n+\t\t\t\t\t    SUBREG_BYTE (source));\n+\t      /* If we have no info about emitted stores, or it didn't\n+\t\t contain the location we intend to use soon, then\n+\t\t add the store.  */\n+\t      if ((!last_slot || !rtx_equal_p (slot, last_slot))\n+\t\t  && ! slot_member_p (slots, slot))\n+\t\t{\n+\t\t  rtx insns, ni;\n+\t\t  last_slot = slot;\n+\t\t  remember_slot (&slots, slot);\n+\t\t  ra_emit_move_insn (slot, source);\n+\t\t  insns = get_insns ();\n+\t\t  end_sequence ();\n+\t\t  if (insns)\n+\t\t    {\n+\t\t      emit_insn_after (insns, insn);\n+\t\t      if (bb->end == insn)\n+\t\t\tbb->end = PREV_INSN (following);\n+\t\t      for (ni = insns; ni != following; ni = NEXT_INSN (ni))\n+\t\t\t{\n+\t\t\t  set_block_for_insn (ni, bb);\n+\t\t\t  df_insn_modify (df, bb, ni);\n+\t\t\t}\n+\t\t    }\n+\t\t  else\n+\t\t    df_insn_modify (df, bb, insn);\n+\t\t  emitted_spill_stores++;\n+\t\t  spill_store_cost += bb->frequency + 1;\n+\t\t  bitmap_set_bit (new_deaths, INSN_UID (PREV_INSN (following)));\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  /* Otherwise ignore insns from adjust_address() above.  */\n+\t\t  end_sequence ();\n+\t\t}\n+\t    }\n+\t}\n+      /* If we look at a load generated by the allocator, forget\n+\t the last emitted slot, and additionally clear all slots\n+\t overlapping it's source (after all, we need it again).  */\n+      /* XXX If we emit the stack-ref directly into the using insn the\n+         following needs a change, because that is no new insn.  Preferably\n+\t we would add some notes to the insn, what stackslots are needed\n+\t for it.  */\n+      if (uid >= last_max_uid)\n+\t{\n+\t  rtx set = single_set (insn);\n+\t  last_slot = NULL_RTX;\n+\t  /* If this was no simple set, give up, and forget everything.  */\n+\t  if (!set)\n+\t    slots = NULL;\n+\t  else\n+\t    {\n+\t      if (1 || GET_CODE (SET_SRC (set)) == MEM)\n+\t        delete_overlapping_slots (&slots, SET_SRC (set));\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* Returns 1 if both colored webs have some hardregs in common, even if\n+   they are not the same width.  */\n+\n+static int\n+spill_same_color_p (web1, web2)\n+     struct web *web1, *web2;\n+{\n+  int c1, size1, c2, size2;\n+  if ((c1 = alias (web1)->color) < 0 || c1 == an_unusable_color)\n+    return 0;\n+  if ((c2 = alias (web2)->color) < 0 || c2 == an_unusable_color)\n+    return 0;\n+\n+  size1 = web1->type == PRECOLORED\n+          ? 1 : HARD_REGNO_NREGS (c1, PSEUDO_REGNO_MODE (web1->regno));\n+  size2 = web2->type == PRECOLORED\n+          ? 1 : HARD_REGNO_NREGS (c2, PSEUDO_REGNO_MODE (web2->regno));\n+  if (c1 >= c2 + size2 || c2 >= c1 + size1)\n+    return 0;\n+  return 1;\n+}\n+\n+/* Given the set of live web IDs LIVE, returns nonzero, if any of WEBs\n+   subwebs (or WEB itself) is live.  */\n+\n+static int\n+is_partly_live_1 (live, web)\n+     sbitmap live;\n+     struct web *web;\n+{\n+  do\n+    if (TEST_BIT (live, web->id))\n+      return 1;\n+  while ((web = web->subreg_next));\n+  return 0;\n+}\n+\n+/* Fast version in case WEB has no subwebs.  */\n+#define is_partly_live(live, web) ((!web->subreg_next)\t\\\n+\t\t\t\t   ? TEST_BIT (live, web->id)\t\\\n+\t\t\t\t   : is_partly_live_1 (live, web))\n+\n+/* Change the set of currently IN_USE colors according to\n+   WEB's color.  Either add those colors to the hardreg set (if ADD\n+   is nonzero), or remove them.  */\n+\n+static void\n+update_spill_colors (in_use, web, add)\n+     HARD_REG_SET *in_use;\n+     struct web *web;\n+     int add;\n+{\n+  int c, size;\n+  if ((c = alias (find_web_for_subweb (web))->color) < 0\n+      || c == an_unusable_color)\n+    return;\n+  size = HARD_REGNO_NREGS (c, GET_MODE (web->orig_x));\n+  if (SUBWEB_P (web))\n+    {\n+      c += subreg_regno_offset (c, GET_MODE (SUBREG_REG (web->orig_x)),\n+\t\t\t\tSUBREG_BYTE (web->orig_x),\n+\t\t\t\tGET_MODE (web->orig_x));\n+    }\n+  else if (web->type == PRECOLORED)\n+    size = 1;\n+  if (add)\n+    for (; size--;)\n+      SET_HARD_REG_BIT (*in_use, c + size);\n+  else\n+    for (; size--;)\n+      CLEAR_HARD_REG_BIT (*in_use, c + size);\n+}\n+\n+/* Given a set of hardregs currently IN_USE and the color C of WEB,\n+   return -1 if WEB has no color, 1 of it has the unusable color,\n+   0 if one of it's used hardregs are in use, and 1 otherwise.\n+   Generally, if WEB can't be left colorized return 1.  */\n+\n+static int\n+spill_is_free (in_use, web)\n+     HARD_REG_SET *in_use;\n+     struct web *web;\n+{\n+  int c, size;\n+  if ((c = alias (web)->color) < 0)\n+    return -1;\n+  if (c == an_unusable_color)\n+    return 1;\n+  size = web->type == PRECOLORED\n+         ? 1 : HARD_REGNO_NREGS (c, PSEUDO_REGNO_MODE (web->regno));\n+  for (; size--;)\n+    if (TEST_HARD_REG_BIT (*in_use, c + size))\n+      return 0;\n+  return 1;\n+}\n+\n+\n+/* Structure for passing between rewrite_program2() and emit_loads().  */\n+struct rewrite_info\n+{\n+  /* The web IDs which currently would need a reload.  These are\n+     currently live spilled webs, whose color was still free.  */\n+  bitmap need_reload;\n+  /* We need a scratch bitmap, but don't want to allocate one a zillion\n+     times.  */\n+  bitmap scratch;\n+  /* Web IDs of currently live webs.  This are the precise IDs,\n+     not just those of the superwebs.  If only on part is live, only\n+     that ID is placed here.  */\n+  sbitmap live;\n+  /* An array of webs, which currently need a load added.\n+     They will be emitted when seeing the first death.  */ \n+  struct web **needed_loads;\n+  /* The current number of entries in needed_loads.  */\n+  int nl_size;\n+  /* The number of bits set in need_reload.  */\n+  int num_reloads;\n+  /* The current set of hardregs not available.  */\n+  HARD_REG_SET colors_in_use;\n+  /* Nonzero, if we just added some spill temps to need_reload or\n+     needed_loads.  In this case we don't wait for the next death\n+     to emit their loads.  */\n+  int any_spilltemps_spilled;\n+  /* Nonzero, if we currently need to emit the loads.  E.g. when we\n+     saw an insn containing deaths.  */\n+  int need_load;\n+};\n+\n+/* The needed_loads list of RI contains some webs for which\n+   we add the actual load insns here.  They are added just before\n+   their use last seen.  NL_FIRST_RELOAD is the index of the first\n+   load which is a converted reload, all other entries are normal\n+   loads.  LAST_BLOCK_INSN is the last insn of the current basic block.  */\n+\n+static void\n+emit_loads (ri, nl_first_reload, last_block_insn)\n+     struct rewrite_info *ri;\n+     int nl_first_reload;\n+     rtx last_block_insn;\n+{\n+  int j;\n+  for (j = ri->nl_size; j;)\n+    {\n+      struct web *web = ri->needed_loads[--j];\n+      struct web *supweb;\n+      struct web *aweb;\n+      rtx ni, slot, reg;\n+      rtx before = NULL_RTX, after = NULL_RTX;\n+      basic_block bb;\n+      /* When spilltemps were spilled for the last insns, their\n+\t loads already are emitted, which is noted by setting\n+\t needed_loads[] for it to 0.  */\n+      if (!web)\n+\tcontinue;\n+      supweb = find_web_for_subweb (web);\n+      if (supweb->regno >= max_normal_pseudo)\n+\tabort ();\n+      /* Check for web being a spilltemp, if we only want to\n+\t load spilltemps.  Also remember, that we emitted that\n+\t load, which we don't need to do when we have a death,\n+\t because then all of needed_loads[] is emptied.  */\n+      if (!ri->need_load)\n+\t{\n+\t  if (!supweb->spill_temp)\n+\t    continue;\n+\t  else\n+\t    ri->needed_loads[j] = 0;\n+\t}\n+      web->in_load = 0;\n+      /* The adding of reloads doesn't depend on liveness.  */\n+      if (j < nl_first_reload && !TEST_BIT (ri->live, web->id))\n+\tcontinue;\n+      aweb = alias (supweb);\n+      aweb->changed = 1;\n+      start_sequence ();\n+      if (supweb->pattern)\n+\t{\n+\t  /* XXX If we later allow non-constant sources for rematerialization\n+\t     we must also disallow coalescing _to_ rematerialized webs\n+\t     (at least then disallow spilling them, which we already ensure\n+\t     when flag_ra_break_aliases), or not take the pattern but a\n+\t     stackslot.  */\n+\t  if (aweb != supweb)\n+\t    abort ();\n+\t  slot = copy_rtx (supweb->pattern);\n+\t  reg = copy_rtx (supweb->orig_x);\n+\t  /* Sanity check.  orig_x should be a REG rtx, which should be\n+\t     shared over all RTL, so copy_rtx should have no effect.  */\n+\t  if (reg != supweb->orig_x)\n+\t    abort ();\n+\t}\n+      else\n+\t{\n+\t  allocate_spill_web (aweb);\n+\t  slot = aweb->stack_slot;\n+\n+\t  /* If we don't copy the RTL there might be some SUBREG\n+\t     rtx shared in the next iteration although being in\n+\t     different webs, which leads to wrong code.  */\n+\t  reg = copy_rtx (web->orig_x);\n+\t  if (GET_CODE (reg) == SUBREG)\n+\t    /*slot = adjust_address (slot, GET_MODE (reg), SUBREG_BYTE\n+\t       (reg));*/\n+\t    slot = simplify_gen_subreg (GET_MODE (reg), slot, GET_MODE (slot),\n+\t\t\t\t\tSUBREG_BYTE (reg));\n+\t}\n+      ra_emit_move_insn (reg, slot);\n+      ni = get_insns ();\n+      end_sequence ();\n+      before = web->last_use_insn;\n+      web->last_use_insn = NULL_RTX;\n+      if (!before)\n+\t{\n+\t  if (JUMP_P (last_block_insn))\n+\t    before = last_block_insn;\n+\t  else\n+\t    after = last_block_insn;\n+\t}\n+      if (after)\n+\t{\n+\t  rtx foll = NEXT_INSN (after);\n+\t  bb = BLOCK_FOR_INSN (after);\n+\t  emit_insn_after (ni, after);\n+\t  if (bb->end == after)\n+\t    bb->end = PREV_INSN (foll);\n+\t  for (ni = NEXT_INSN (after); ni != foll; ni = NEXT_INSN (ni))\n+\t    {\n+\t      set_block_for_insn (ni, bb);\n+\t      df_insn_modify (df, bb, ni);\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  rtx prev = PREV_INSN (before);\n+\t  bb = BLOCK_FOR_INSN (before);\n+\t  emit_insn_before (ni, before);\n+\t  if (bb->head == before)\n+\t    bb->head = NEXT_INSN (prev);\n+\t  for (; ni != before; ni = NEXT_INSN (ni))\n+\t    {\n+\t      set_block_for_insn (ni, bb);\n+\t      df_insn_modify (df, bb, ni);\n+\t    }\n+\t}\n+      if (supweb->pattern)\n+\t{\n+\t  emitted_remat++;\n+\t  spill_remat_cost += bb->frequency + 1;\n+\t}\n+      else\n+\t{\n+\t  emitted_spill_loads++;\n+\t  spill_load_cost += bb->frequency + 1;\n+\t}\n+      RESET_BIT (ri->live, web->id);\n+      /* In the special case documented above only emit the reloads and\n+\t one load.  */\n+      if (ri->need_load == 2 && j < nl_first_reload)\n+\tbreak;\n+    }\n+  if (ri->need_load)\n+    ri->nl_size = j;\n+}\n+\n+/* Given a set of reloads in RI, an array of NUM_REFS references (either\n+   uses or defs) in REFS, and REF2WEB to translate ref IDs to webs\n+   (either use2web or def2web) convert some reloads to loads.\n+   This looks at the webs referenced, and how they change the set of\n+   available colors.  Now put all still live webs, which needed reloads,\n+   and whose colors isn't free anymore, on the needed_loads list.  */\n+\n+static void\n+reloads_to_loads (ri, refs, num_refs, ref2web)\n+     struct rewrite_info *ri;\n+     struct ref **refs;\n+     unsigned int num_refs;\n+     struct web **ref2web;\n+{\n+  unsigned int n;\n+  int num_reloads = ri->num_reloads;\n+  for (n = 0; n < num_refs && num_reloads; n++)\n+    {\n+      struct web *web = ref2web[DF_REF_ID (refs[n])];\n+      struct web *supweb = find_web_for_subweb (web);\n+      int is_death;\n+      int j;\n+      /* Only emit reloads when entering their interference\n+\t region.  A use of a spilled web never opens an\n+\t interference region, independent of it's color.  */\n+      if (alias (supweb)->type == SPILLED)\n+\tcontinue;\n+      if (supweb->type == PRECOLORED\n+\t  && TEST_HARD_REG_BIT (never_use_colors, supweb->color))\n+\tcontinue;\n+      /* Note, that if web (and supweb) are DEFs, we already cleared\n+\t the corresponding bits in live.  I.e. is_death becomes true, which\n+\t is what we want.  */\n+      is_death = !TEST_BIT (ri->live, supweb->id);\n+      is_death &= !TEST_BIT (ri->live, web->id);\n+      if (is_death)\n+\t{\n+\t  int old_num_r = num_reloads;\n+\t  bitmap_clear (ri->scratch);\n+\t  EXECUTE_IF_SET_IN_BITMAP (ri->need_reload, 0, j,\n+\t    {\n+\t      struct web *web2 = ID2WEB (j);\n+\t      struct web *aweb2 = alias (find_web_for_subweb (web2));\n+\t      if (spill_is_free (&(ri->colors_in_use), aweb2) == 0)\n+\t\tabort ();\n+\t      if (spill_same_color_p (supweb, aweb2)\n+\t\t  /* && interfere (web, web2) */)\n+\t\t{\n+\t\t  if (!web2->in_load)\n+\t\t    {\n+\t\t      ri->needed_loads[ri->nl_size++] = web2;\n+\t\t      web2->in_load = 1;\n+\t\t    }\n+\t\t  bitmap_set_bit (ri->scratch, j);\n+\t\t  num_reloads--;\n+\t\t}\n+\t    });\n+\t  if (num_reloads != old_num_r)\n+\t    bitmap_operation (ri->need_reload, ri->need_reload, ri->scratch,\n+\t\t\t      BITMAP_AND_COMPL);\n+\t}\n+    }\n+  ri->num_reloads = num_reloads;\n+}\n+\n+/* This adds loads for spilled webs to the program.  It uses a kind of\n+   interference region spilling.  If flag_ra_ir_spilling is zero it\n+   only uses improved chaitin spilling (adding loads only at insns\n+   containing deaths).  */\n+\n+static void\n+rewrite_program2 (new_deaths)\n+     bitmap new_deaths;\n+{\n+  basic_block bb;\n+  int nl_first_reload;\n+  struct rewrite_info ri;\n+  rtx insn;\n+  ri.needed_loads = (struct web **) xmalloc (num_webs * sizeof (struct web *));\n+  ri.need_reload = BITMAP_XMALLOC ();\n+  ri.scratch = BITMAP_XMALLOC ();\n+  ri.live = sbitmap_alloc (num_webs);\n+  ri.nl_size = 0;\n+  ri.num_reloads = 0;\n+  for (insn = get_last_insn (); insn; insn = PREV_INSN (insn))\n+    {\n+      basic_block last_bb = NULL;\n+      rtx last_block_insn;\n+      int i, j;\n+      if (!INSN_P (insn))\n+\tinsn = prev_real_insn (insn);\n+      while (insn && !(bb = BLOCK_FOR_INSN (insn)))\n+\tinsn = prev_real_insn (insn);\n+      if (!insn)\n+\tbreak;\n+      i = bb->index + 2;\n+      last_block_insn = insn;\n+\n+      sbitmap_zero (ri.live);\n+      CLEAR_HARD_REG_SET (ri.colors_in_use);\n+      EXECUTE_IF_SET_IN_BITMAP (live_at_end[i - 2], 0, j,\n+\t{\n+\t  struct web *web = use2web[j];\n+\t  struct web *aweb = alias (find_web_for_subweb (web));\n+\t  /* A web is only live at end, if it isn't spilled.  If we wouldn't\n+\t     check this, the last uses of spilled web per basic block\n+\t     wouldn't be detected as deaths, although they are in the final\n+\t     code.  This would lead to cumulating many loads without need,\n+\t     only increasing register pressure.  */\n+\t  /* XXX do add also spilled webs which got a color for IR spilling.\n+\t     Remember to not add to colors_in_use in that case.  */\n+\t  if (aweb->type != SPILLED /*|| aweb->color >= 0*/)\n+\t    {\n+\t      SET_BIT (ri.live, web->id);\n+\t      if (aweb->type != SPILLED)\n+\t        update_spill_colors (&(ri.colors_in_use), web, 1);\n+\t    }\n+\t});\n+\n+      bitmap_clear (ri.need_reload);\n+      ri.num_reloads = 0;\n+      ri.any_spilltemps_spilled = 0;\n+      if (flag_ra_ir_spilling)\n+\t{\n+\t  struct dlist *d;\n+\t  int pass;\n+\t  /* XXX If we don't add spilled nodes into live above, the following\n+\t     becomes an empty loop.  */\n+\t  for (pass = 0; pass < 2; pass++)\n+\t    for (d = (pass) ? WEBS(SPILLED) : WEBS(COALESCED); d; d = d->next)\n+\t      {\n+\t        struct web *web = DLIST_WEB (d);\n+\t\tstruct web *aweb = alias (web);\n+\t\tif (aweb->type != SPILLED)\n+\t\t  continue;\n+\t        if (is_partly_live (ri.live, web)\n+\t\t    && spill_is_free (&(ri.colors_in_use), web) > 0)\n+\t\t  {\n+\t\t    ri.num_reloads++;\n+\t            bitmap_set_bit (ri.need_reload, web->id);\n+\t\t    /* Last using insn is somewhere in another block.  */\n+\t\t    web->last_use_insn = NULL_RTX;\n+\t\t  }\n+\t      }\n+\t}\n+\n+      last_bb = bb;\n+      for (; insn; insn = PREV_INSN (insn))\n+\t{\n+\t  struct ra_insn_info info;\n+\t  unsigned int n;\n+\n+\t  if (INSN_P (insn) && BLOCK_FOR_INSN (insn) != last_bb)\n+\t    {\n+\t      int index = BLOCK_FOR_INSN (insn)->index + 2;\n+\t      EXECUTE_IF_SET_IN_BITMAP (live_at_end[index - 2], 0, j,\n+\t\t{\n+\t\t  struct web *web = use2web[j];\n+\t\t  struct web *aweb = alias (find_web_for_subweb (web));\n+\t\t  if (aweb->type != SPILLED)\n+\t\t    {\n+\t\t      SET_BIT (ri.live, web->id);\n+\t\t      update_spill_colors (&(ri.colors_in_use), web, 1);\n+\t\t    }\n+\t\t});\n+\t      bitmap_clear (ri.scratch);\n+\t      EXECUTE_IF_SET_IN_BITMAP (ri.need_reload, 0, j,\n+\t\t{\n+\t\t  struct web *web2 = ID2WEB (j);\n+\t\t  struct web *supweb2 = find_web_for_subweb (web2);\n+\t\t  struct web *aweb2 = alias (supweb2);\n+\t\t  if (spill_is_free (&(ri.colors_in_use), aweb2) <= 0)\n+\t\t    {\n+\t\t      if (!web2->in_load)\n+\t\t\t{\n+\t\t\t  ri.needed_loads[ri.nl_size++] = web2;\n+\t\t\t  web2->in_load = 1;\n+\t\t\t}\n+\t\t      bitmap_set_bit (ri.scratch, j);\n+\t\t      ri.num_reloads--;\n+\t\t    }\n+\t\t});\n+\t      bitmap_operation (ri.need_reload, ri.need_reload, ri.scratch,\n+\t\t\t\tBITMAP_AND_COMPL);\n+\t      last_bb = BLOCK_FOR_INSN (insn);\n+\t      last_block_insn = insn;\n+\t      if (!INSN_P (last_block_insn))\n+\t        last_block_insn = prev_real_insn (last_block_insn);\n+\t    }\n+\n+\t  ri.need_load = 0;\n+\t  if (INSN_P (insn))\n+\t    info = insn_df[INSN_UID (insn)];\n+\n+\t  if (INSN_P (insn))\n+\t    for (n = 0; n < info.num_defs; n++)\n+\t      {\n+\t\tstruct ref *ref = info.defs[n];\n+\t\tstruct web *web = def2web[DF_REF_ID (ref)];\n+\t\tstruct web *supweb = find_web_for_subweb (web);\n+\t\tint is_non_def = 0;\n+\t\tunsigned int n2;\n+\n+\t\tsupweb = find_web_for_subweb (web);\n+\t\t/* Webs which are defined here, but also used in the same insn\n+\t\t   are rmw webs, or this use isn't a death because of looping\n+\t\t   constructs.  In neither case makes this def available it's\n+\t\t   resources.  Reloads for it are still needed, it's still\n+\t\t   live and it's colors don't become free.  */\n+\t\tfor (n2 = 0; n2 < info.num_uses; n2++)\n+\t\t  {\n+\t\t    struct web *web2 = use2web[DF_REF_ID (info.uses[n2])];\n+\t\t    if (supweb == find_web_for_subweb (web2))\n+\t\t      {\n+\t\t\tis_non_def = 1;\n+\t\t\tbreak;\n+\t\t      }\n+\t\t  }\n+\t\tif (is_non_def)\n+\t\t  continue;\n+\n+\t\tif (!is_partly_live (ri.live, supweb))\n+\t\t  bitmap_set_bit (useless_defs, DF_REF_ID (ref));\n+\n+\t\tRESET_BIT (ri.live, web->id);\n+\t\tif (bitmap_bit_p (ri.need_reload, web->id))\n+\t\t  {\n+\t\t    ri.num_reloads--;\n+\t\t    bitmap_clear_bit (ri.need_reload, web->id);\n+\t\t  }\n+\t\tif (web != supweb)\n+\t\t  {\n+\t\t    /* XXX subwebs aren't precisely tracked here.  We have\n+\t\t       everything we need (inverse webs), but the code isn't\n+\t\t       yet written.  We need to make all completely\n+\t\t       overlapping web parts non-live here.  */\n+\t\t    /* If by luck now the whole web isn't live anymore, no\n+\t\t       reloads for it are needed.  */\n+\t\t    if (!is_partly_live (ri.live, supweb)\n+\t\t\t&& bitmap_bit_p (ri.need_reload, supweb->id))\n+\t\t      {\n+\t\t\tri.num_reloads--;\n+\t\t\tbitmap_clear_bit (ri.need_reload, supweb->id);\n+\t\t      }\n+\t\t  }\n+\t\telse\n+\t\t  {\n+\t\t    struct web *sweb;\n+\t\t    /* If the whole web is defined here, no parts of it are\n+\t\t       live anymore and no reloads are needed for them.  */\n+\t\t    for (sweb = supweb->subreg_next; sweb;\n+\t\t\t sweb = sweb->subreg_next)\n+\t\t      {\n+\t\t        RESET_BIT (ri.live, sweb->id);\n+\t\t\tif (bitmap_bit_p (ri.need_reload, sweb->id))\n+\t\t\t  {\n+\t\t            ri.num_reloads--;\n+\t\t            bitmap_clear_bit (ri.need_reload, sweb->id);\n+\t\t\t  }\n+\t\t      }\n+\t\t  }\n+\t\tif (alias (supweb)->type != SPILLED)\n+\t\t  update_spill_colors (&(ri.colors_in_use), web, 0);\n+\t      }\n+\n+\t  nl_first_reload = ri.nl_size;\n+\n+\t  /* CALL_INSNs are not really deaths, but still more registers\n+\t     are free after a call, than before.\n+\t     XXX Note, that sometimes reload barfs when we emit insns between\n+\t     a call and the insn which copies the return register into a\n+\t     pseudo.  */\n+\t  if (GET_CODE (insn) == CALL_INSN)\n+\t    ri.need_load = 1;\n+\t  else if (INSN_P (insn))\n+\t    for (n = 0; n < info.num_uses; n++)\n+\t      {\n+\t\tstruct web *web = use2web[DF_REF_ID (info.uses[n])];\n+\t\tstruct web *supweb = find_web_for_subweb (web);\n+\t\tint is_death;\n+\t\tif (supweb->type == PRECOLORED\n+\t\t    && TEST_HARD_REG_BIT (never_use_colors, supweb->color))\n+\t\t  continue;\n+\t\tis_death = !TEST_BIT (ri.live, supweb->id);\n+\t\tis_death &= !TEST_BIT (ri.live, web->id);\n+\t\tif (is_death)\n+\t\t  {\n+\t\t    ri.need_load = 1;\n+\t\t    bitmap_set_bit (new_deaths, INSN_UID (insn));\n+\t\t    break;\n+\t\t  }\n+\t      }\n+\n+\t  if (INSN_P (insn) && ri.num_reloads)\n+\t    {\n+              int old_num_reloads = ri.num_reloads;\n+\t      reloads_to_loads (&ri, info.uses, info.num_uses, use2web);\n+\n+\t      /* If this insn sets a pseudo, which isn't used later\n+\t\t (i.e. wasn't live before) it is a dead store.  We need\n+\t\t to emit all reloads which have the same color as this def.\n+\t\t We don't need to check for non-liveness here to detect\n+\t\t the deadness (it anyway is too late, as we already cleared\n+\t\t the liveness in the first loop over the defs), because if it\n+\t\t _would_ be live here, no reload could have that color, as\n+\t\t they would already have been converted to a load.  */\n+\t      if (ri.num_reloads)\n+\t\treloads_to_loads (&ri, info.defs, info.num_defs, def2web);\n+\t      if (ri.num_reloads != old_num_reloads && !ri.need_load)\n+\t\tri.need_load = 1;\n+\t    }\n+\n+\t  if (ri.nl_size && (ri.need_load || ri.any_spilltemps_spilled))\n+\t    emit_loads (&ri, nl_first_reload, last_block_insn);\n+\n+\t  if (INSN_P (insn) && flag_ra_ir_spilling)\n+\t    for (n = 0; n < info.num_uses; n++)\n+\t      {\n+\t\tstruct web *web = use2web[DF_REF_ID (info.uses[n])];\n+\t\tstruct web *aweb = alias (find_web_for_subweb (web));\n+\t\tif (aweb->type != SPILLED)\n+\t\t  update_spill_colors (&(ri.colors_in_use), web, 1);\n+\t      }\n+\n+\t  ri.any_spilltemps_spilled = 0;\n+\t  if (INSN_P (insn))\n+\t    for (n = 0; n < info.num_uses; n++)\n+\t      {\n+\t\tstruct web *web = use2web[DF_REF_ID (info.uses[n])];\n+\t\tstruct web *supweb = find_web_for_subweb (web);\n+\t\tstruct web *aweb = alias (supweb);\n+\t\tSET_BIT (ri.live, web->id);\n+\t\tif (aweb->type != SPILLED)\n+\t\t  continue;\n+\t\tif (supweb->spill_temp)\n+\t\t  ri.any_spilltemps_spilled = 1;\n+\t\tweb->last_use_insn = insn;\n+\t\tif (!web->in_load)\n+\t\t  {\n+\t\t    if (spill_is_free (&(ri.colors_in_use), aweb) <= 0\n+\t\t\t|| !flag_ra_ir_spilling)\n+\t\t      {\n+\t\t\tri.needed_loads[ri.nl_size++] = web;\n+\t\t\tweb->in_load = 1;\n+\t\t\tweb->one_load = 1;\n+\t\t      }\n+\t\t    else if (!bitmap_bit_p (ri.need_reload, web->id))\n+\t\t      {\n+\t\t        bitmap_set_bit (ri.need_reload, web->id);\n+\t\t\tri.num_reloads++;\n+\t\t\tweb->one_load = 1;\n+\t\t      }\n+\t\t    else\n+\t\t      web->one_load = 0;\n+\t\t  }\n+\t\telse\n+\t\t  web->one_load = 0;\n+\t      }\n+\n+\t  if (GET_CODE (insn) == CODE_LABEL)\n+\t    break;\n+\t}\n+\n+      nl_first_reload = ri.nl_size;\n+      if (ri.num_reloads)\n+\t{\n+\t  int in_ir = 0;\n+\t  edge e;\n+\t  int num = 0;\n+\t  HARD_REG_SET cum_colors, colors;\n+\t  CLEAR_HARD_REG_SET (cum_colors);\n+\t  for (e = bb->pred; e && num < 5; e = e->pred_next, num++)\n+\t    {\n+\t      int j;\n+\t      CLEAR_HARD_REG_SET (colors);\n+\t      EXECUTE_IF_SET_IN_BITMAP (live_at_end[e->src->index], 0, j,\n+\t\t{\n+\t\t  struct web *web = use2web[j];\n+\t\t  struct web *aweb = alias (find_web_for_subweb (web));\n+\t\t  if (aweb->type != SPILLED)\n+\t\t    update_spill_colors (&colors, web, 1);\n+\t\t});\n+\t      IOR_HARD_REG_SET (cum_colors, colors);\n+\t    }\n+\t  if (num == 5)\n+\t    in_ir = 1;\n+\n+\t  bitmap_clear (ri.scratch);\n+\t  EXECUTE_IF_SET_IN_BITMAP (ri.need_reload, 0, j,\n+\t    {\n+\t      struct web *web2 = ID2WEB (j);\n+\t      struct web *supweb2 = find_web_for_subweb (web2);\n+\t      struct web *aweb2 = alias (supweb2);\n+\t      /* block entry is IR boundary for aweb2?\n+\t\t Currently more some tries for good conditions.  */\n+\t      if (((ra_pass > 0 || supweb2->target_of_spilled_move)\n+\t\t  && (1 || in_ir || spill_is_free (&cum_colors, aweb2) <= 0))\n+\t\t  || (ra_pass == 1\n+\t\t      && (in_ir\n+\t\t\t  || spill_is_free (&cum_colors, aweb2) <= 0)))\n+\t\t{\n+\t\t  if (!web2->in_load)\n+\t\t    {\n+\t\t      ri.needed_loads[ri.nl_size++] = web2;\n+\t\t      web2->in_load = 1;\n+\t\t    }\n+\t\t  bitmap_set_bit (ri.scratch, j);\n+\t\t  ri.num_reloads--;\n+\t\t}\n+\t    });\n+\t  bitmap_operation (ri.need_reload, ri.need_reload, ri.scratch,\n+\t\t\t    BITMAP_AND_COMPL);\n+\t}\n+\n+      ri.need_load = 1;\n+      emit_loads (&ri, nl_first_reload, last_block_insn);\n+      if (ri.nl_size != 0 /*|| ri.num_reloads != 0*/)\n+\tabort ();\n+      if (!insn)\n+\tbreak;\n+    }\n+  free (ri.needed_loads);\n+  sbitmap_free (ri.live);\n+  BITMAP_XFREE (ri.scratch);\n+  BITMAP_XFREE (ri.need_reload);\n+}\n+\n+/* WEBS is a web conflicting with a spilled one.  Prepare it\n+   to be able to rescan it in the next pass.  Mark all it's uses\n+   for checking, and clear the some members of their web parts\n+   (of defs and uses).  Notably don't clear the uplink.  We don't\n+   change the layout of this web, just it's conflicts.\n+   Also remember all IDs of its uses in USES_AS_BITMAP.  */\n+\n+static void\n+mark_refs_for_checking (web, uses_as_bitmap)\n+     struct web *web;\n+     bitmap uses_as_bitmap;\n+{\n+  unsigned int i;\n+  for (i = 0; i < web->num_uses; i++)\n+    {\n+      unsigned int id = DF_REF_ID (web->uses[i]);\n+      SET_BIT (last_check_uses, id);\n+      bitmap_set_bit (uses_as_bitmap, id);\n+      web_parts[df->def_id + id].spanned_deaths = 0;\n+      web_parts[df->def_id + id].crosses_call = 0;\n+    }\n+  for (i = 0; i < web->num_defs; i++)\n+    {\n+      unsigned int id = DF_REF_ID (web->defs[i]);\n+      web_parts[id].spanned_deaths = 0;\n+      web_parts[id].crosses_call = 0;\n+    }\n+}\n+\n+/* The last step of the spill phase is to set up the structures for\n+   incrementally rebuilding the interference graph.  We break up\n+   the web part structure of all spilled webs, mark their uses for\n+   rechecking, look at their neighbors, and clean up some global\n+   information, we will rebuild.  */\n+\n+static void\n+detect_web_parts_to_rebuild ()\n+{\n+  bitmap uses_as_bitmap;\n+  unsigned int i, pass;\n+  struct dlist *d;\n+  sbitmap already_webs = sbitmap_alloc (num_webs);\n+\n+  uses_as_bitmap = BITMAP_XMALLOC ();\n+  if (last_check_uses)\n+    sbitmap_free (last_check_uses);\n+  last_check_uses = sbitmap_alloc (df->use_id);\n+  sbitmap_zero (last_check_uses);\n+  sbitmap_zero (already_webs);\n+  /* We need to recheck all uses of all webs involved in spilling (and the\n+     uses added by spill insns, but those are not analyzed yet).\n+     Those are the spilled webs themself, webs coalesced to spilled ones,\n+     and webs conflicting with any of them.  */\n+  for (pass = 0; pass < 2; pass++)\n+    for (d = (pass == 0) ? WEBS(SPILLED) : WEBS(COALESCED); d; d = d->next)\n+      {\n+        struct web *web = DLIST_WEB (d);\n+\tstruct conflict_link *wl;\n+\tunsigned int j;\n+\t/* This check is only needed for coalesced nodes, but hey.  */\n+\tif (alias (web)->type != SPILLED)\n+\t  continue;\n+\n+\t/* For the spilled web itself we also need to clear it's\n+\t   uplink, to be able to rebuild smaller webs.  After all\n+\t   spilling has split the web.  */\n+        for (i = 0; i < web->num_uses; i++)\n+\t  {\n+\t    unsigned int id = DF_REF_ID (web->uses[i]);\n+\t    SET_BIT (last_check_uses, id);\n+\t    bitmap_set_bit (uses_as_bitmap, id);\n+\t    web_parts[df->def_id + id].uplink = NULL;\n+\t    web_parts[df->def_id + id].spanned_deaths = 0;\n+\t    web_parts[df->def_id + id].crosses_call = 0;\n+\t  }\n+\tfor (i = 0; i < web->num_defs; i++)\n+\t  {\n+\t    unsigned int id = DF_REF_ID (web->defs[i]);\n+\t    web_parts[id].uplink = NULL;\n+\t    web_parts[id].spanned_deaths = 0;\n+\t    web_parts[id].crosses_call = 0;\n+\t  }\n+\n+\t/* Now look at all neighbors of this spilled web.  */\n+\tif (web->have_orig_conflicts)\n+\t  wl = web->orig_conflict_list;\n+\telse\n+\t  wl = web->conflict_list;\n+\tfor (; wl; wl = wl->next)\n+\t  {\n+\t    if (TEST_BIT (already_webs, wl->t->id))\n+\t      continue;\n+\t    SET_BIT (already_webs, wl->t->id);\n+\t    mark_refs_for_checking (wl->t, uses_as_bitmap);\n+\t  }\n+\tEXECUTE_IF_SET_IN_BITMAP (web->useless_conflicts, 0, j,\n+\t  {\n+\t    struct web *web2 = ID2WEB (j);\n+\t    if (TEST_BIT (already_webs, web2->id))\n+\t      continue;\n+\t    SET_BIT (already_webs, web2->id);\n+\t    mark_refs_for_checking (web2, uses_as_bitmap);\n+\t  });\n+      }\n+\n+  /* We also recheck unconditionally all uses of any hardregs.  This means\n+     we _can_ delete all these uses from the live_at_end[] bitmaps.\n+     And because we sometimes delete insn refering to hardregs (when\n+     they became useless because they setup a rematerializable pseudo, which\n+     then was rematerialized), some of those uses will go away with the next\n+     df_analyse().  This means we even _must_ delete those uses from\n+     the live_at_end[] bitmaps.  For simplicity we simply delete\n+     all of them.  */\n+  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+    if (!fixed_regs[i])\n+      {\n+\tstruct df_link *link;\n+\tfor (link = df->regs[i].uses; link; link = link->next)\n+\t  if (link->ref)\n+\t    bitmap_set_bit (uses_as_bitmap, DF_REF_ID (link->ref));\n+      }\n+\n+  /* The information in live_at_end[] will be rebuild for all uses\n+     we recheck, so clear it here (the uses of spilled webs, might\n+     indeed not become member of it again).  */\n+  live_at_end -= 2;\n+  for (i = 0; i < (unsigned int) last_basic_block + 2; i++)\n+    bitmap_operation (live_at_end[i], live_at_end[i], uses_as_bitmap,\n+\t\t      BITMAP_AND_COMPL);\n+  live_at_end += 2;\n+\n+  if (rtl_dump_file && (debug_new_regalloc & DUMP_REBUILD) != 0)\n+    {\n+      ra_debug_msg (DUMP_REBUILD, \"need to check these uses:\\n\");\n+      dump_sbitmap_file (rtl_dump_file, last_check_uses);\n+    }\n+  sbitmap_free (already_webs);\n+  BITMAP_XFREE (uses_as_bitmap);\n+}\n+\n+/* Statistics about deleted insns, which are useless now.  */\n+static unsigned int deleted_def_insns;\n+static unsigned HOST_WIDE_INT deleted_def_cost;\n+\n+/* In rewrite_program2() we noticed, when a certain insn set a pseudo\n+   which wasn't live.  Try to delete all those insns.  */\n+\n+static void\n+delete_useless_defs ()\n+{\n+  unsigned int i;\n+  /* If the insn only sets the def without any sideeffect (besides\n+     clobbers or uses), we can delete it.  single_set() also tests\n+     for INSN_P(insn).  */\n+  EXECUTE_IF_SET_IN_BITMAP (useless_defs, 0, i,\n+    {\n+      rtx insn = DF_REF_INSN (df->defs[i]);\n+      rtx set = single_set (insn);\n+      struct web *web = find_web_for_subweb (def2web[i]);\n+      if (set && web->type == SPILLED && web->stack_slot == NULL)\n+        {\n+\t  deleted_def_insns++;\n+\t  deleted_def_cost += BLOCK_FOR_INSN (insn)->frequency + 1;\n+\t  PUT_CODE (insn, NOTE);\n+\t  NOTE_LINE_NUMBER (insn) = NOTE_INSN_DELETED;\n+\t  df_insn_modify (df, BLOCK_FOR_INSN (insn), insn);\n+\t}\n+    });\n+}\n+\n+/* Look for spilled webs, on whose behalf no insns were emitted.\n+   We inversify (sp?) the changed flag of the webs, so after this function\n+   a nonzero changed flag means, that this web was not spillable (at least\n+   in this pass).  */\n+\n+static void\n+detect_non_changed_webs ()\n+{\n+  struct dlist *d, *d_next;\n+  for (d = WEBS(SPILLED); d; d = d_next)\n+    {\n+      struct web *web = DLIST_WEB (d);\n+      d_next = d->next;\n+      if (!web->changed)\n+\t{\n+\t  ra_debug_msg (DUMP_PROCESS, \"no insns emitted for spilled web %d\\n\",\n+\t\t     web->id);\n+\t  remove_web_from_list (web);\n+\t  put_web (web, COLORED);\n+\t  web->changed = 1;\n+\t}\n+      else\n+\tweb->changed = 0;\n+      /* From now on web->changed is used as the opposite flag.\n+\t I.e. colored webs, which have changed set were formerly\n+\t spilled webs for which no insns were emitted.  */\n+    }\n+}\n+\n+/* Before spilling we clear the changed flags for all spilled webs.  */\n+\n+static void\n+reset_changed_flag ()\n+{\n+  struct dlist *d;\n+  for (d = WEBS(SPILLED); d; d = d->next)\n+    DLIST_WEB(d)->changed = 0;\n+}\n+\n+/* The toplevel function for this file.  Given a colorized graph,\n+   and lists of spilled, coalesced and colored webs, we add some\n+   spill code.  This also sets up the structures for incrementally\n+   building the interference graph in the next pass.  */\n+\n+void\n+actual_spill ()\n+{\n+  int i;\n+  bitmap new_deaths = BITMAP_XMALLOC ();\n+  reset_changed_flag ();\n+  spill_coalprop ();\n+  choose_spill_colors ();\n+  useless_defs = BITMAP_XMALLOC ();\n+  if (flag_ra_improved_spilling)\n+    rewrite_program2 (new_deaths);\n+  else\n+    rewrite_program (new_deaths);\n+  insert_stores (new_deaths);\n+  delete_useless_defs ();\n+  BITMAP_XFREE (useless_defs);\n+  sbitmap_free (insns_with_deaths);\n+  insns_with_deaths = sbitmap_alloc (get_max_uid ());\n+  death_insns_max_uid = get_max_uid ();\n+  sbitmap_zero (insns_with_deaths);\n+  EXECUTE_IF_SET_IN_BITMAP (new_deaths, 0, i,\n+    { SET_BIT (insns_with_deaths, i);});\n+  detect_non_changed_webs ();\n+  detect_web_parts_to_rebuild ();\n+  BITMAP_XFREE (new_deaths);\n+}\n+\n+/* A bitmap of pseudo reg numbers which are coalesced directly\n+   to a hardreg.  Set in emit_colors(), used and freed in\n+   remove_suspicious_death_notes().  */\n+static bitmap regnos_coalesced_to_hardregs;\n+\n+/* Create new pseudos for each web we colored, change insns to\n+   use those pseudos and set up ra_reg_renumber.  */\n+\n+void\n+emit_colors (df)\n+     struct df *df;\n+{\n+  unsigned int i;\n+  int si;\n+  struct web *web;\n+  int old_max_regno = max_reg_num ();\n+  regset old_regs;\n+  basic_block bb;\n+\n+  /* This bitmap is freed in remove_suspicious_death_notes(),\n+     which is also the user of it.  */\n+  regnos_coalesced_to_hardregs = BITMAP_XMALLOC ();\n+  /* First create the (REG xx) rtx's for all webs, as we need to know\n+     the number, to make sure, flow has enough memory for them in the\n+     various tables.  */\n+  for (i = 0; i < num_webs - num_subwebs; i++)\n+    {\n+      web = ID2WEB (i);\n+      if (web->type != COLORED && web->type != COALESCED)\n+\tcontinue;\n+      if (web->type == COALESCED && alias (web)->type == COLORED)\n+\tcontinue;\n+      if (web->reg_rtx || web->regno < FIRST_PSEUDO_REGISTER)\n+\tabort ();\n+\n+      if (web->regno >= max_normal_pseudo)\n+\t{\n+\t  rtx place;\n+\t  if (web->color == an_unusable_color)\n+\t    {\n+\t      unsigned int inherent_size = PSEUDO_REGNO_BYTES (web->regno);\n+\t      unsigned int total_size = MAX (inherent_size, 0);\n+\t      place = assign_stack_local (PSEUDO_REGNO_MODE (web->regno),\n+\t\t\t\t\t  total_size,\n+\t\t\t\t\t  inherent_size == total_size ? 0 : -1);\n+\t      RTX_UNCHANGING_P (place) =\n+\t\t  RTX_UNCHANGING_P (regno_reg_rtx[web->regno]);\n+\t      set_mem_alias_set (place, new_alias_set ());\n+\t    }\n+\t  else\n+\t    {\n+\t      place = gen_reg_rtx (PSEUDO_REGNO_MODE (web->regno));\n+\t    }\n+\t  web->reg_rtx = place;\n+\t}\n+      else\n+\t{\n+\t  /* Special case for i386 'fix_truncdi_nomemory' insn.\n+\t     We must choose mode from insns not from PSEUDO_REGNO_MODE.\n+\t     Actual only for clobbered register.  */\n+\t  if (web->num_uses == 0 && web->num_defs == 1)\n+\t    web->reg_rtx = gen_reg_rtx (GET_MODE (DF_REF_REAL_REG (web->defs[0])));\n+\t  else\n+\t    web->reg_rtx = gen_reg_rtx (PSEUDO_REGNO_MODE (web->regno));\n+\t  /* Remember the different parts directly coalesced to a hardreg.  */\n+\t  if (web->type == COALESCED)\n+\t    bitmap_set_bit (regnos_coalesced_to_hardregs, REGNO (web->reg_rtx));\n+\t}\n+    }\n+  ra_max_regno = max_regno = max_reg_num ();\n+  allocate_reg_info (max_regno, FALSE, FALSE);\n+  ra_reg_renumber = (short *) xmalloc (max_regno * sizeof (short));\n+  for (si = 0; si < max_regno; si++)\n+    ra_reg_renumber[si] = -1;\n+\n+  /* Then go through all references, and replace them by a new\n+     pseudoreg for each web.  All uses.  */\n+  /* XXX\n+     Beware: The order of replacements (first uses, then defs) matters only\n+     for read-mod-write insns, where the RTL expression for the REG is\n+     shared between def and use.  For normal rmw insns we connected all such\n+     webs, i.e. both the use and the def (which are the same memory)\n+     there get the same new pseudo-reg, so order would not matter.\n+     _However_ we did not connect webs, were the read cycle was an\n+     uninitialized read.  If we now would first replace the def reference\n+     and then the use ref, we would initialize it with a REG rtx, which\n+     gets never initialized, and yet more wrong, which would overwrite\n+     the definition of the other REG rtx.  So we must replace the defs last.\n+   */\n+  for (i = 0; i < df->use_id; i++)\n+    if (df->uses[i])\n+      {\n+\tregset rs = DF_REF_BB (df->uses[i])->global_live_at_start;\n+\trtx regrtx;\n+\tweb = use2web[i];\n+\tweb = find_web_for_subweb (web);\n+\tif (web->type != COLORED && web->type != COALESCED)\n+\t  continue;\n+\tregrtx = alias (web)->reg_rtx;\n+\tif (!regrtx)\n+\t  regrtx = web->reg_rtx;\n+\t*DF_REF_REAL_LOC (df->uses[i]) = regrtx;\n+\tif (REGNO_REG_SET_P (rs, web->regno) && REG_P (regrtx))\n+\t  {\n+\t    /*CLEAR_REGNO_REG_SET (rs, web->regno);*/\n+\t    SET_REGNO_REG_SET (rs, REGNO (regrtx));\n+\t  }\n+      }\n+\n+  /* And all defs.  */\n+  for (i = 0; i < df->def_id; i++)\n+    {\n+      regset rs;\n+      rtx regrtx;\n+      if (!df->defs[i])\n+\tcontinue;\n+      rs = DF_REF_BB (df->defs[i])->global_live_at_start;\n+      web = def2web[i];\n+      web = find_web_for_subweb (web);\n+      if (web->type != COLORED && web->type != COALESCED)\n+\tcontinue;\n+      regrtx = alias (web)->reg_rtx;\n+      if (!regrtx)\n+\tregrtx = web->reg_rtx;\n+      *DF_REF_REAL_LOC (df->defs[i]) = regrtx;\n+      if (REGNO_REG_SET_P (rs, web->regno) && REG_P (regrtx))\n+\t{\n+\t  /* Don't simply clear the current regno, as it might be\n+\t     replaced by two webs.  */\n+          /*CLEAR_REGNO_REG_SET (rs, web->regno);*/\n+          SET_REGNO_REG_SET (rs, REGNO (regrtx));\n+\t}\n+    }\n+\n+  /* And now set up the ra_reg_renumber array for reload with all the new\n+     pseudo-regs.  */\n+  for (i = 0; i < num_webs - num_subwebs; i++)\n+    {\n+      web = ID2WEB (i);\n+      if (web->reg_rtx && REG_P (web->reg_rtx))\n+\t{\n+\t  int r = REGNO (web->reg_rtx);\n+          ra_reg_renumber[r] = web->color;\n+          ra_debug_msg (DUMP_COLORIZE, \"Renumber pseudo %d (== web %d) to %d\\n\",\n+\t\t     r, web->id, ra_reg_renumber[r]);\n+\t}\n+    }\n+\n+  old_regs = BITMAP_XMALLOC ();\n+  for (si = FIRST_PSEUDO_REGISTER; si < old_max_regno; si++)\n+    SET_REGNO_REG_SET (old_regs, si);\n+  FOR_EACH_BB (bb)\n+    {\n+      AND_COMPL_REG_SET (bb->global_live_at_start, old_regs);\n+      AND_COMPL_REG_SET (bb->global_live_at_end, old_regs);\n+    }\n+  BITMAP_XFREE (old_regs);\n+}\n+\n+/* Delete some coalesced moves from the insn stream.  */\n+\n+void\n+delete_moves ()\n+{\n+  struct move_list *ml;\n+  struct web *s, *t;\n+  /* XXX Beware: We normally would test here each copy insn, if\n+     source and target got the same color (either by coalescing or by pure\n+     luck), and then delete it.\n+     This will currently not work.  One problem is, that we don't color\n+     the regs ourself, but instead defer to reload.  So the colorization\n+     is only a kind of suggestion, which reload doesn't have to follow.\n+     For webs which are coalesced to a normal colored web, we only have one\n+     new pseudo, so in this case we indeed can delete copy insns involving\n+     those (because even if reload colors them different from our suggestion,\n+     it still has to color them the same, as only one pseudo exists).  But for\n+     webs coalesced to precolored ones, we have not a single pseudo, but\n+     instead one for each coalesced web.  This means, that we can't delete\n+     copy insns, where source and target are webs coalesced to precolored\n+     ones, because then the connection between both webs is destroyed.  Note\n+     that this not only means copy insns, where one side is the precolored one\n+     itself, but also those between webs which are coalesced to one color.\n+     Also because reload we can't delete copy insns which involve any\n+     precolored web at all.  These often have also special meaning (e.g.\n+     copying a return value of a call to a pseudo, or copying pseudo to the\n+     return register), and the deletion would confuse reload in thinking the\n+     pseudo isn't needed.  One of those days reload will get away and we can\n+     do everything we want.\n+     In effect because of the later reload, we can't base our deletion on the\n+     colors itself, but instead need to base them on the newly created\n+     pseudos.  */\n+  for (ml = wl_moves; ml; ml = ml->next)\n+    /* The real condition we would ideally use is: s->color == t->color.\n+       Additionally: s->type != PRECOLORED && t->type != PRECOLORED, in case\n+       we want to prevent deletion of \"special\" copies.  */\n+    if (ml->move\n+\t&& (s = alias (ml->move->source_web))->reg_rtx\n+\t    == (t = alias (ml->move->target_web))->reg_rtx\n+\t&& s->type != PRECOLORED && t->type != PRECOLORED)\n+      {\n+\tbasic_block bb = BLOCK_FOR_INSN (ml->move->insn);\n+\tdf_insn_delete (df, bb, ml->move->insn);\n+\tdeleted_move_insns++;\n+\tdeleted_move_cost += bb->frequency + 1;\n+      }\n+}\n+\n+/* Due to resons documented elsewhere we create different pseudos\n+   for all webs coalesced to hardregs.  For these parts life_analysis()\n+   might have added REG_DEAD notes without considering, that only this part\n+   but not the whole coalesced web dies.  The RTL is correct, there is no\n+   coalescing yet.  But if later reload's alter_reg() substitutes the\n+   hardreg into the REG rtx it looks like that particular hardreg dies here,\n+   although (due to coalescing) it still is live.  This might make different\n+   places of reload think, it can use that hardreg for reload regs,\n+   accidentally overwriting it.  So we need to remove those REG_DEAD notes.\n+   (Or better teach life_analysis() and reload about our coalescing, but\n+   that comes later) Bah.  */\n+\n+void\n+remove_suspicious_death_notes ()\n+{\n+  rtx insn;\n+  for (insn = get_insns(); insn; insn = NEXT_INSN (insn))\n+    if (INSN_P (insn))\n+      {\n+\trtx *pnote = &REG_NOTES (insn);\n+\twhile (*pnote)\n+\t  {\n+\t    rtx note = *pnote;\n+\t    if ((REG_NOTE_KIND (note) == REG_DEAD\n+\t\t || REG_NOTE_KIND (note) == REG_UNUSED)\n+\t\t&& (GET_CODE (XEXP (note, 0)) == REG\n+\t\t    && bitmap_bit_p (regnos_coalesced_to_hardregs,\n+\t\t\t\t     REGNO (XEXP (note, 0)))))\n+\t      *pnote = XEXP (note, 1);\n+\t    else\n+\t      pnote = &XEXP (*pnote, 1);\n+\t  }\n+      }\n+  BITMAP_XFREE (regnos_coalesced_to_hardregs);\n+  regnos_coalesced_to_hardregs = NULL;\n+}\n+\n+/* Allocate space for max_reg_num() pseudo registers, and\n+   fill reg_renumber[] from ra_reg_renumber[].  If FREE_IT\n+   is nonzero, also free ra_reg_renumber and reset ra_max_regno.  */\n+\n+void\n+setup_renumber (free_it)\n+     int free_it;\n+{\n+  int i;\n+  max_regno = max_reg_num ();\n+  allocate_reg_info (max_regno, FALSE, TRUE);\n+  for (i = 0; i < max_regno; i++)\n+    {\n+      reg_renumber[i] = (i < ra_max_regno) ? ra_reg_renumber[i] : -1;\n+    }\n+  if (free_it)\n+    {\n+      free (ra_reg_renumber);\n+      ra_reg_renumber = NULL;\n+      ra_max_regno = 0;\n+    }\n+}\n+\n+/* Dump the costs and savings due to spilling, i.e. of added spill insns\n+   and removed moves or useless defs.  */\n+\n+void\n+dump_cost (level)\n+     unsigned int level;\n+{\n+#define LU HOST_WIDE_INT_PRINT_UNSIGNED\n+  ra_debug_msg (level, \"Instructions for spilling\\n added:\\n\");\n+  ra_debug_msg (level, \"  loads =%d cost=\" LU \"\\n\", emitted_spill_loads,\n+\t     spill_load_cost);\n+  ra_debug_msg (level, \"  stores=%d cost=\" LU \"\\n\", emitted_spill_stores,\n+\t     spill_store_cost);\n+  ra_debug_msg (level, \"  remat =%d cost=\" LU \"\\n\", emitted_remat,\n+\t     spill_remat_cost);\n+  ra_debug_msg (level, \" removed:\\n\");\n+  ra_debug_msg (level, \"  moves =%d cost=\" LU \"\\n\", deleted_move_insns,\n+\t     deleted_move_cost);\n+  ra_debug_msg (level, \"  others=%d cost=\" LU \"\\n\", deleted_def_insns,\n+\t     deleted_def_cost);\n+#undef LU\n+}\n+\n+/* Initialization of the rewrite phase.  */\n+\n+void\n+ra_rewrite_init ()\n+{\n+  emitted_spill_loads = 0;\n+  emitted_spill_stores = 0;\n+  emitted_remat = 0;\n+  spill_load_cost = 0;\n+  spill_store_cost = 0;\n+  spill_remat_cost = 0;\n+  deleted_move_insns = 0;\n+  deleted_move_cost = 0;\n+  deleted_def_insns = 0;\n+  deleted_def_cost = 0;\n+}\n+\n+/*\n+vim:cinoptions={.5s,g0,p5,t0,(0,^-0.5s,n-0.5s:tw=78:cindent:sw=4:\n+*/"}, {"sha": "34ac436209cf60e41d97d4c2d5284afdfeb6457f", "filename": "gcc/ra.c", "status": "added", "additions": 902, "deletions": 0, "changes": 902, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fra.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fra.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fra.c?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -0,0 +1,902 @@\n+/* Graph coloring register allocator\n+   Copyright (C) 2001, 2002 Free Software Foundation, Inc.\n+   Contributed by Michael Matz <matz@suse.de>\n+   and Daniel Berlin <dan@cgsoftware.com>.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it under the\n+   terms of the GNU General Public License as published by the Free Software\n+   Foundation; either version 2, or (at your option) any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n+   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n+   details.\n+\n+   You should have received a copy of the GNU General Public License along\n+   with GCC; see the file COPYING.  If not, write to the Free Software\n+   Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"reload.h\"\n+#include \"integrate.h\"\n+#include \"function.h\"\n+#include \"regs.h\"\n+#include \"obstack.h\"\n+#include \"hard-reg-set.h\"\n+#include \"basic-block.h\"\n+#include \"df.h\"\n+#include \"expr.h\"\n+#include \"output.h\"\n+#include \"toplev.h\"\n+#include \"flags.h\"\n+#include \"ra.h\"\n+\n+#define obstack_chunk_alloc xmalloc\n+#define obstack_chunk_free free\n+\n+/* This is the toplevel file of a graph coloring register allocator.\n+   It is able to act like a George & Appel allocator, i.e. with iterative\n+   coalescing plus spill coalescing/propagation.\n+   And it can act as a traditional Briggs allocator, although with\n+   optimistic coalescing.  Additionally it has a custom pass, which\n+   tries to reduce the overall cost of the colored graph.\n+\n+   We support two modes of spilling: spill-everywhere, which is extremely\n+   fast, and interference region spilling, which reduces spill code to a\n+   large extent, but is slower.\n+\n+   Helpful documents:\n+\n+   Briggs, P., Cooper, K. D., and Torczon, L. 1994. Improvements to graph\n+   coloring register allocation. ACM Trans. Program. Lang. Syst. 16, 3 (May),\n+   428-455.\n+\n+   Bergner, P., Dahl, P., Engebretsen, D., and O'Keefe, M. 1997. Spill code\n+   minimization via interference region spilling. In Proc. ACM SIGPLAN '97\n+   Conf. on Prog. Language Design and Implementation. ACM, 287-295.\n+\n+   George, L., Appel, A.W. 1996.  Iterated register coalescing.\n+   ACM Trans. Program. Lang. Syst. 18, 3 (May), 300-324.\n+\n+*/\n+\n+/* This file contains the main entry point (reg_alloc), some helper routines\n+   used by more than one file of the register allocator, and the toplevel\n+   driver procedure (one_pass).  */\n+\n+/* Things, one might do somewhen:\n+\n+   * Lattice based rematerialization\n+   * create definitions of ever-life regs at the beginning of\n+     the insn chain\n+   * insert loads as soon, stores as late as possile\n+   * insert spill insns as outward as possible (either looptree, or LCM)\n+   * reuse stack-slots\n+   * delete coalesced insns.  Partly done.  The rest can only go, when we get\n+     rid of reload.\n+   * don't destroy coalescing information completely when spilling\n+   * use the constraints from asms\n+  */\n+\n+static struct obstack ra_obstack;\n+static void create_insn_info PARAMS ((struct df *));\n+static void free_insn_info PARAMS ((void));\n+static void alloc_mem PARAMS ((struct df *));\n+static void free_mem PARAMS ((struct df *));\n+static void free_all_mem PARAMS ((struct df *df));\n+static int one_pass PARAMS ((struct df *, int));\n+static void check_df PARAMS ((struct df *));\n+static void init_ra PARAMS ((void));\n+\n+void reg_alloc PARAMS ((void));\n+\n+/* These global variables are \"internal\" to the register allocator.\n+   They are all documented at their declarations in ra.h.  */\n+\n+/* Somewhen we want to get rid of one of those sbitmaps.\n+   (for now I need the sup_igraph to note if there is any conflict between\n+   parts of webs at all.  I can't use igraph for this, as there only the real\n+   conflicts are noted.)  This is only used to prevent coalescing two\n+   conflicting webs, were only parts of them are in conflict.  */\n+sbitmap igraph;\n+sbitmap sup_igraph;\n+\n+/* Note the insns not inserted by the allocator, where we detected any\n+   deaths of pseudos.  It is used to detect closeness of defs and uses.\n+   In the first pass this is empty (we could initialize it from REG_DEAD\n+   notes), in the other passes it is left from the pass before.  */\n+sbitmap insns_with_deaths;\n+int death_insns_max_uid;\n+\n+struct web_part *web_parts;\n+\n+unsigned int num_webs;\n+unsigned int num_subwebs;\n+unsigned int num_allwebs;\n+struct web **id2web;\n+struct web *hardreg2web[FIRST_PSEUDO_REGISTER];\n+struct web **def2web;\n+struct web **use2web;\n+struct move_list *wl_moves;\n+int ra_max_regno;\n+short *ra_reg_renumber;\n+struct df *df;\n+bitmap *live_at_end;\n+int ra_pass;\n+unsigned int max_normal_pseudo;\n+int an_unusable_color;\n+\n+/* The different lists on which a web can be (based on the type).  */\n+struct dlist *web_lists[(int) LAST_NODE_TYPE];\n+\n+unsigned int last_def_id;\n+unsigned int last_use_id;\n+unsigned int last_num_webs;\n+int last_max_uid;\n+sbitmap last_check_uses;\n+unsigned int remember_conflicts;\n+\n+int orig_max_uid;\n+\n+HARD_REG_SET never_use_colors;\n+HARD_REG_SET usable_regs[N_REG_CLASSES];\n+unsigned int num_free_regs[N_REG_CLASSES];\n+HARD_REG_SET hardregs_for_mode[NUM_MACHINE_MODES];\n+unsigned char byte2bitcount[256];\n+\n+unsigned int debug_new_regalloc = -1;\n+int flag_ra_biased = 0;\n+int flag_ra_improved_spilling = 0;\n+int flag_ra_ir_spilling = 0;\n+int flag_ra_optimistic_coalescing = 0;\n+int flag_ra_break_aliases = 0;\n+int flag_ra_merge_spill_costs = 0;\n+int flag_ra_spill_every_use = 0;\n+int flag_ra_dump_notes = 0;\n+\n+/* Fast allocation of small objects, which live until the allocator\n+   is done.  Allocate an object of SIZE bytes.  */\n+\n+void *\n+ra_alloc (size)\n+     size_t size;\n+{\n+  return obstack_alloc (&ra_obstack, size);\n+}\n+\n+/* Like ra_alloc(), but clear the returned memory.  */\n+\n+void *\n+ra_calloc (size)\n+     size_t size;\n+{\n+  void *p = obstack_alloc (&ra_obstack, size);\n+  memset (p, 0, size);\n+  return p;\n+}\n+\n+/* Returns the number of hardregs in HARD_REG_SET RS.  */\n+\n+int\n+hard_regs_count (rs)\n+     HARD_REG_SET rs;\n+{\n+  int count = 0;\n+#ifdef HARD_REG_SET\n+  while (rs)\n+    {\n+      unsigned char byte = rs & 0xFF;\n+      rs >>= 8;\n+      /* Avoid memory access, if nothing is set.  */\n+      if (byte)\n+        count += byte2bitcount[byte];\n+    }\n+#else\n+  unsigned int ofs;\n+  for (ofs = 0; ofs < HARD_REG_SET_LONGS; ofs++)\n+    {\n+      HARD_REG_ELT_TYPE elt = rs[ofs];\n+      while (elt)\n+\t{\n+\t  unsigned char byte = elt & 0xFF;\n+\t  elt >>= 8;\n+\t  if (byte)\n+\t    count += byte2bitcount[byte];\n+\t}\n+    }\n+#endif\n+  return count;\n+}\n+\n+/* Basically like emit_move_insn (i.e. validifies constants and such),\n+   but also handle MODE_CC moves (but then the operands must already\n+   be basically valid.  */\n+\n+rtx\n+ra_emit_move_insn (x, y)\n+     rtx x, y;\n+{\n+  enum machine_mode mode = GET_MODE (x);\n+  if (GET_MODE_CLASS (mode) == MODE_CC)\n+    return emit_insn (gen_move_insn (x, y));\n+  else\n+    return emit_move_insn (x, y);\n+}\n+\n+int insn_df_max_uid;\n+struct ra_insn_info *insn_df;\n+static struct ref **refs_for_insn_df;\n+\n+/* Create the insn_df structure for each insn to have fast access to\n+   all valid defs and uses in an insn.  */\n+\n+static void\n+create_insn_info (df)\n+     struct df *df;\n+{\n+  rtx insn;\n+  struct ref **act_refs;\n+  insn_df_max_uid = get_max_uid ();\n+  insn_df = xcalloc (insn_df_max_uid, sizeof (insn_df[0]));\n+  refs_for_insn_df = xcalloc (df->def_id + df->use_id, sizeof (struct ref *));\n+  act_refs = refs_for_insn_df;\n+  /* We create those things backwards to mimic the order in which\n+     the insns are visited in rewrite_program2() and live_in().  */\n+  for (insn = get_last_insn (); insn; insn = PREV_INSN (insn))\n+    {\n+      int uid = INSN_UID (insn);\n+      unsigned int n;\n+      struct df_link *link;\n+      if (!INSN_P (insn))\n+\tcontinue;\n+      for (n = 0, link = DF_INSN_DEFS (df, insn); link; link = link->next)\n+        if (link->ref\n+\t    && (DF_REF_REGNO (link->ref) >= FIRST_PSEUDO_REGISTER\n+\t\t|| !TEST_HARD_REG_BIT (never_use_colors,\n+\t\t\t\t       DF_REF_REGNO (link->ref))))\n+\t  {\n+\t    if (n == 0)\n+\t      insn_df[uid].defs = act_refs;\n+\t    insn_df[uid].defs[n++] = link->ref;\n+\t  }\n+      act_refs += n;\n+      insn_df[uid].num_defs = n;\n+      for (n = 0, link = DF_INSN_USES (df, insn); link; link = link->next)\n+        if (link->ref\n+\t    && (DF_REF_REGNO (link->ref) >= FIRST_PSEUDO_REGISTER\n+\t\t|| !TEST_HARD_REG_BIT (never_use_colors,\n+\t\t\t\t       DF_REF_REGNO (link->ref))))\n+\t  {\n+\t    if (n == 0)\n+\t      insn_df[uid].uses = act_refs;\n+\t    insn_df[uid].uses[n++] = link->ref;\n+\t  }\n+      act_refs += n;\n+      insn_df[uid].num_uses = n;\n+    }\n+  if (refs_for_insn_df + (df->def_id + df->use_id) < act_refs)\n+    abort ();\n+}\n+\n+/* Free the insn_df structures.  */\n+\n+static void\n+free_insn_info ()\n+{\n+  free (refs_for_insn_df);\n+  refs_for_insn_df = NULL;\n+  free (insn_df);\n+  insn_df = NULL;\n+  insn_df_max_uid = 0;\n+}\n+\n+/* Search WEB for a subweb, which represents REG.  REG needs to\n+   be a SUBREG, and the inner reg of it needs to be the one which is\n+   represented by WEB.  Returns the matching subweb or NULL.  */\n+\n+struct web *\n+find_subweb (web, reg)\n+     struct web *web;\n+     rtx reg;\n+{\n+  struct web *w;\n+  if (GET_CODE (reg) != SUBREG)\n+    abort ();\n+  for (w = web->subreg_next; w; w = w->subreg_next)\n+    if (GET_MODE (w->orig_x) == GET_MODE (reg)\n+\t&& SUBREG_BYTE (w->orig_x) == SUBREG_BYTE (reg))\n+      return w;\n+  return NULL;\n+}\n+\n+/* Similar to find_subweb(), but matches according to SIZE_WORD,\n+   a collection of the needed size and offset (in bytes).  */\n+\n+struct web *\n+find_subweb_2 (web, size_word)\n+     struct web *web;\n+     unsigned int size_word;\n+{\n+  struct web *w = web;\n+  if (size_word == GET_MODE_SIZE (GET_MODE (web->orig_x)))\n+    /* size_word == size means BYTE_BEGIN(size_word) == 0.  */\n+    return web;\n+  for (w = web->subreg_next; w; w = w->subreg_next)\n+    {\n+      unsigned int bl = rtx_to_bits (w->orig_x);\n+      if (size_word == bl)\n+        return w;\n+    }\n+  return NULL;\n+}\n+\n+/* Returns the superweb for SUBWEB.  */\n+\n+struct web *\n+find_web_for_subweb_1 (subweb)\n+     struct web *subweb;\n+{\n+  while (subweb->parent_web)\n+    subweb = subweb->parent_web;\n+  return subweb;\n+}\n+\n+/* Determine if two hard register sets intersect.\n+   Return 1 if they do.  */\n+\n+int\n+hard_regs_intersect_p (a, b)\n+     HARD_REG_SET *a, *b;\n+{\n+  HARD_REG_SET c;\n+  COPY_HARD_REG_SET (c, *a);\n+  AND_HARD_REG_SET (c, *b);\n+  GO_IF_HARD_REG_SUBSET (c, reg_class_contents[(int) NO_REGS], lose);\n+  return 1;\n+lose:\n+  return 0;\n+}\n+\n+/* Allocate and initialize the memory necessary for one pass of the\n+   register allocator.  */\n+\n+static void\n+alloc_mem (df)\n+     struct df *df;\n+{\n+  int i;\n+  ra_build_realloc (df);\n+  if (!live_at_end)\n+    {\n+      live_at_end = (bitmap *) xmalloc ((last_basic_block + 2)\n+\t\t\t\t\t* sizeof (bitmap));\n+      for (i = 0; i < last_basic_block + 2; i++)\n+\tlive_at_end[i] = BITMAP_XMALLOC ();\n+      live_at_end += 2;\n+    }\n+  create_insn_info (df);\n+}\n+\n+/* Free the memory which isn't necessary for the next pass.  */\n+\n+static void\n+free_mem (df)\n+     struct df *df ATTRIBUTE_UNUSED;\n+{\n+  free_insn_info ();\n+  ra_build_free ();\n+}\n+\n+/* Free all memory allocated for the register allocator.  Used, when\n+   it's done.  */\n+\n+static void\n+free_all_mem (df)\n+     struct df *df;\n+{\n+  unsigned int i;\n+  live_at_end -= 2;\n+  for (i = 0; i < (unsigned)last_basic_block + 2; i++)\n+    BITMAP_XFREE (live_at_end[i]);\n+  free (live_at_end);\n+\n+  ra_colorize_free_all ();\n+  ra_build_free_all (df);\n+  obstack_free (&ra_obstack, NULL);\n+}\n+\n+static long ticks_build;\n+static long ticks_rebuild;\n+\n+/* Perform one pass of allocation.  Returns nonzero, if some spill code\n+   was added, i.e. if the allocator needs to rerun.  */\n+\n+static int\n+one_pass (df, rebuild)\n+     struct df *df;\n+     int rebuild;\n+{\n+  long ticks = clock ();\n+  int something_spilled;\n+  remember_conflicts = 0;\n+\n+  /* Build the complete interference graph, or if this is not the first\n+     pass, rebuild it incrementally.  */\n+  build_i_graph (df);\n+\n+  /* From now on, if we create new conflicts, we need to remember the\n+     initial list of conflicts per web.  */\n+  remember_conflicts = 1;\n+  if (!rebuild)\n+    dump_igraph_machine ();\n+\n+  /* Colorize the I-graph.  This results in either a list of\n+     spilled_webs, in which case we need to run the spill phase, and\n+     rerun the allocator, or that list is empty, meaning we are done.  */\n+  ra_colorize_graph (df);\n+\n+  last_max_uid = get_max_uid ();\n+  /* actual_spill() might change WEBS(SPILLED) and even empty it,\n+     so we need to remember it's state.  */\n+  something_spilled = !!WEBS(SPILLED);\n+\n+  /* Add spill code if necessary.  */\n+  if (something_spilled)\n+    actual_spill ();\n+\n+  ticks = clock () - ticks;\n+  if (rebuild)\n+    ticks_rebuild += ticks;\n+  else\n+    ticks_build += ticks;\n+  return something_spilled;\n+}\n+\n+/* Initialize various arrays for the register allocator.  */\n+\n+static void\n+init_ra ()\n+{\n+  int i;\n+  HARD_REG_SET rs;\n+#ifdef ELIMINABLE_REGS\n+  static struct {int from, to; } eliminables[] = ELIMINABLE_REGS;\n+  unsigned int j;\n+#endif\n+  int need_fp\n+    = (! flag_omit_frame_pointer\n+#ifdef EXIT_IGNORE_STACK\n+       || (current_function_calls_alloca && EXIT_IGNORE_STACK)\n+#endif\n+       || FRAME_POINTER_REQUIRED);\n+\n+  ra_colorize_init ();\n+\n+  /* We can't ever use any of the fixed regs.  */\n+  COPY_HARD_REG_SET (never_use_colors, fixed_reg_set);\n+\n+  /* Additionally don't even try to use hardregs, which we already\n+     know are not eliminable.  This includes also either the\n+     hard framepointer or all regs which are eliminable into the\n+     stack pointer, if need_fp is set.  */\n+#ifdef ELIMINABLE_REGS\n+  for (j = 0; j < ARRAY_SIZE (eliminables); j++)\n+    {\n+      if (! CAN_ELIMINATE (eliminables[j].from, eliminables[j].to)\n+\t  || (eliminables[j].to == STACK_POINTER_REGNUM && need_fp))\n+\tfor (i = HARD_REGNO_NREGS (eliminables[j].from, Pmode); i--;)\n+\t  SET_HARD_REG_BIT (never_use_colors, eliminables[j].from + i);\n+    }\n+#if FRAME_POINTER_REGNUM != HARD_FRAME_POINTER_REGNUM\n+  if (need_fp)\n+    for (i = HARD_REGNO_NREGS (HARD_FRAME_POINTER_REGNUM, Pmode); i--;)\n+      SET_HARD_REG_BIT (never_use_colors, HARD_FRAME_POINTER_REGNUM + i);\n+#endif\n+\n+#else\n+  if (need_fp)\n+    for (i = HARD_REGNO_NREGS (FRAME_POINTER_REGNUM, Pmode); i--;)\n+      SET_HARD_REG_BIT (never_use_colors, FRAME_POINTER_REGNUM + i);\n+#endif\n+\n+  /* Stack and argument pointer are also rather useless to us.  */\n+  for (i = HARD_REGNO_NREGS (STACK_POINTER_REGNUM, Pmode); i--;)\n+    SET_HARD_REG_BIT (never_use_colors, STACK_POINTER_REGNUM + i);\n+\n+  for (i = HARD_REGNO_NREGS (ARG_POINTER_REGNUM, Pmode); i--;)\n+    SET_HARD_REG_BIT (never_use_colors, ARG_POINTER_REGNUM + i);\n+\n+  for (i = 0; i < 256; i++)\n+    {\n+      unsigned char byte = ((unsigned) i) & 0xFF;\n+      unsigned char count = 0;\n+      while (byte)\n+\t{\n+\t  if (byte & 1)\n+\t    count++;\n+\t  byte >>= 1;\n+\t}\n+      byte2bitcount[i] = count;\n+    }\n+\n+  for (i = 0; i < N_REG_CLASSES; i++)\n+    {\n+      int size;\n+      COPY_HARD_REG_SET (rs, reg_class_contents[i]);\n+      AND_COMPL_HARD_REG_SET (rs, never_use_colors);\n+      size = hard_regs_count (rs);\n+      num_free_regs[i] = size;\n+      COPY_HARD_REG_SET (usable_regs[i], rs);\n+    }\n+\n+  /* Setup hardregs_for_mode[].\n+     We are not interested only in the beginning of a multi-reg, but in\n+     all the hardregs involved.  Maybe HARD_REGNO_MODE_OK() only ok's\n+     for beginnings.  */\n+  for (i = 0; i < NUM_MACHINE_MODES; i++)\n+    {\n+      int reg, size;\n+      CLEAR_HARD_REG_SET (rs);\n+      for (reg = 0; reg < FIRST_PSEUDO_REGISTER; reg++)\n+\tif (HARD_REGNO_MODE_OK (reg, i)\n+\t    /* Ignore VOIDmode and similar things.  */\n+\t    && (size = HARD_REGNO_NREGS (reg, i)) != 0\n+\t    && (reg + size) <= FIRST_PSEUDO_REGISTER)\n+\t  {\n+\t    while (size--)\n+\t      SET_HARD_REG_BIT (rs, reg + size);\n+\t  }\n+      COPY_HARD_REG_SET (hardregs_for_mode[i], rs);\n+    }\n+\n+  for (an_unusable_color = 0; an_unusable_color < FIRST_PSEUDO_REGISTER;\n+       an_unusable_color++)\n+    if (TEST_HARD_REG_BIT (never_use_colors, an_unusable_color))\n+      break;\n+  if (an_unusable_color == FIRST_PSEUDO_REGISTER)\n+    abort ();\n+\n+  orig_max_uid = get_max_uid ();\n+  compute_bb_for_insn ();\n+  ra_reg_renumber = NULL;\n+  insns_with_deaths = sbitmap_alloc (orig_max_uid);\n+  death_insns_max_uid = orig_max_uid;\n+  sbitmap_ones (insns_with_deaths);\n+  gcc_obstack_init (&ra_obstack);\n+}\n+\n+/* Check the consistency of DF.  This aborts if it violates some\n+   invariances we expect.  */\n+\n+static void\n+check_df (df)\n+     struct df *df;\n+{\n+  struct df_link *link;\n+  rtx insn;\n+  int regno;\n+  unsigned int ui;\n+  bitmap b = BITMAP_XMALLOC ();\n+  bitmap empty_defs = BITMAP_XMALLOC ();\n+  bitmap empty_uses = BITMAP_XMALLOC ();\n+\n+  /* Collect all the IDs of NULL references in the ID->REF arrays,\n+     as df.c leaves them when updating the df structure.  */\n+  for (ui = 0; ui < df->def_id; ui++)\n+    if (!df->defs[ui])\n+      bitmap_set_bit (empty_defs, ui);\n+  for (ui = 0; ui < df->use_id; ui++)\n+    if (!df->uses[ui])\n+      bitmap_set_bit (empty_uses, ui);\n+\n+  /* For each insn we check if the chain of references contain each\n+     ref only once, doesn't contain NULL refs, or refs whose ID is invalid\n+     (it df->refs[id] element is NULL).  */\n+  for (insn = get_insns (); insn; insn = NEXT_INSN (insn))\n+    if (INSN_P (insn))\n+      {\n+\tbitmap_clear (b);\n+\tfor (link = DF_INSN_DEFS (df, insn); link; link = link->next)\n+\t  if (!link->ref || bitmap_bit_p (empty_defs, DF_REF_ID (link->ref))\n+\t      || bitmap_bit_p (b, DF_REF_ID (link->ref)))\n+\t    abort ();\n+\t  else\n+\t    bitmap_set_bit (b, DF_REF_ID (link->ref));\n+\n+\tbitmap_clear (b);\n+\tfor (link = DF_INSN_USES (df, insn); link; link = link->next)\n+\t  if (!link->ref || bitmap_bit_p (empty_uses, DF_REF_ID (link->ref))\n+\t      || bitmap_bit_p (b, DF_REF_ID (link->ref)))\n+\t    abort ();\n+\t  else\n+\t    bitmap_set_bit (b, DF_REF_ID (link->ref));\n+      }\n+\n+  /* Now the same for the chains per register number.  */\n+  for (regno = 0; regno < max_reg_num (); regno++)\n+    {\n+      bitmap_clear (b);\n+      for (link = df->regs[regno].defs; link; link = link->next)\n+\tif (!link->ref || bitmap_bit_p (empty_defs, DF_REF_ID (link->ref))\n+\t    || bitmap_bit_p (b, DF_REF_ID (link->ref)))\n+\t  abort ();\n+\telse\n+\t  bitmap_set_bit (b, DF_REF_ID (link->ref));\n+\n+      bitmap_clear (b);\n+      for (link = df->regs[regno].uses; link; link = link->next)\n+\tif (!link->ref || bitmap_bit_p (empty_uses, DF_REF_ID (link->ref))\n+\t    || bitmap_bit_p (b, DF_REF_ID (link->ref)))\n+\t  abort ();\n+\telse\n+\t  bitmap_set_bit (b, DF_REF_ID (link->ref));\n+    }\n+\n+  BITMAP_XFREE (empty_uses);\n+  BITMAP_XFREE (empty_defs);\n+  BITMAP_XFREE (b);\n+}\n+\n+/* Main register allocator entry point.  */\n+\n+void\n+reg_alloc ()\n+{\n+  int changed;\n+  FILE *ra_dump_file = rtl_dump_file;\n+  rtx last = get_last_insn ();\n+\n+  if (! INSN_P (last))\n+    last = prev_real_insn (last);\n+  /* If this is an empty function we shouldn't do all the following,\n+     but instead just setup what's necessary, and return.  */\n+\n+  /* We currently rely on the existance of the return value USE as\n+     one of the last insns.  Add it if it's not there anymore.  */\n+  if (last)\n+    {\n+      edge e;\n+      for (e = EXIT_BLOCK_PTR->pred; e; e = e->pred_next)\n+\t{\n+\t  basic_block bb = e->src;\n+\t  last = bb->end;\n+\t  if (!INSN_P (last) || GET_CODE (PATTERN (last)) != USE)\n+\t    {\n+\t      rtx insns;\n+\t      start_sequence ();\n+\t      use_return_register ();\n+\t      insns = get_insns ();\n+\t      end_sequence ();\n+\t      emit_insn_after (insns, last);\n+\t    }\n+\t}\n+    }\n+\n+  /* Setup debugging levels.  */\n+  switch (0)\n+    {\n+      /* Some usefull presets of the debug level, I often use.  */\n+      case 0: debug_new_regalloc = DUMP_EVER; break;\n+      case 1: debug_new_regalloc = DUMP_COSTS; break;\n+      case 2: debug_new_regalloc = DUMP_IGRAPH_M; break;\n+      case 3: debug_new_regalloc = DUMP_COLORIZE + DUMP_COSTS; break;\n+      case 4: debug_new_regalloc = DUMP_COLORIZE + DUMP_COSTS + DUMP_WEBS;\n+\t      break;\n+      case 5: debug_new_regalloc = DUMP_FINAL_RTL + DUMP_COSTS +\n+\t      DUMP_CONSTRAINTS;\n+\t      break;\n+      case 6: debug_new_regalloc = DUMP_VALIDIFY; break;\n+    }\n+  if (!rtl_dump_file)\n+    debug_new_regalloc = 0;\n+\n+  /* Run regclass first, so we know the preferred and alternate classes\n+     for each pseudo.  Deactivate emitting of debug info, if it's not\n+     explicitely requested.  */\n+  if ((debug_new_regalloc & DUMP_REGCLASS) == 0)\n+    rtl_dump_file = NULL;\n+  regclass (get_insns (), max_reg_num (), rtl_dump_file);\n+  rtl_dump_file = ra_dump_file;\n+\n+  /* We don't use those NOTEs, and as we anyway change all registers,\n+     they only make problems later.  */\n+  count_or_remove_death_notes (NULL, 1);\n+\n+  /* Initialize the different global arrays and regsets.  */\n+  init_ra ();\n+\n+  /* And some global variables.  */\n+  ra_pass = 0;\n+  no_new_pseudos = 0;\n+  max_normal_pseudo = (unsigned) max_reg_num ();\n+  ra_rewrite_init ();\n+  last_def_id = 0;\n+  last_use_id = 0;\n+  last_num_webs = 0;\n+  last_max_uid = 0;\n+  last_check_uses = NULL;\n+  live_at_end = NULL;\n+  WEBS(INITIAL) = NULL;\n+  WEBS(FREE) = NULL;\n+  memset (hardreg2web, 0, sizeof (hardreg2web));\n+  ticks_build = ticks_rebuild = 0;\n+\n+  /* The default is to use optimistic coalescing with interference\n+     region spilling, without biased coloring.  */\n+  flag_ra_biased = 0;\n+  flag_ra_spill_every_use = 0;\n+  flag_ra_improved_spilling = 1;\n+  flag_ra_ir_spilling = 1;\n+  flag_ra_break_aliases = 0;\n+  flag_ra_optimistic_coalescing = 1;\n+  flag_ra_merge_spill_costs = 1;\n+  if (flag_ra_optimistic_coalescing)\n+    flag_ra_break_aliases = 1;\n+  flag_ra_dump_notes = 0;\n+\n+  /* Allocate the global df structure.  */\n+  df = df_init ();\n+\n+  /* This is the main loop, calling one_pass as long as there are still\n+     some spilled webs.  */\n+  do\n+    {\n+      ra_debug_msg (DUMP_NEARLY_EVER, \"RegAlloc Pass %d\\n\\n\", ra_pass);\n+      if (ra_pass++ > 40)\n+\tinternal_error (\"Didn't find a coloring.\\n\");\n+\n+      /* First collect all the register refs and put them into\n+\t chains per insn, and per regno.  In later passes only update\n+         that info from the new and modified insns.  */\n+      df_analyse (df, (ra_pass == 1) ? 0 : (bitmap) -1,\n+\t\t  DF_HARD_REGS | DF_RD_CHAIN | DF_RU_CHAIN);\n+\n+      if ((debug_new_regalloc & DUMP_DF) != 0)\n+\t{\n+\t  rtx insn;\n+\t  df_dump (df, DF_HARD_REGS, rtl_dump_file);\n+\t  for (insn = get_insns (); insn; insn = NEXT_INSN (insn))\n+            if (INSN_P (insn))\n+\t      df_insn_debug_regno (df, insn, rtl_dump_file);\n+\t}\n+      check_df (df);\n+\n+      /* Now allocate the memory needed for this pass, or (if it's not the\n+\t first pass), reallocate only additional memory.  */\n+      alloc_mem (df);\n+\n+      /* Build and colorize the interference graph, and possibly emit\n+\t spill insns.  This also might delete certain move insns.  */\n+      changed = one_pass (df, ra_pass > 1);\n+\n+      /* If that produced no changes, the graph was colorizable.  */\n+      if (!changed)\n+\t{\n+\t  /* Change the insns to refer to the new pseudos (one per web).  */\n+          emit_colors (df);\n+\t  /* Already setup a preliminary reg_renumber[] array, but don't\n+\t     free our own version.  reg_renumber[] will again be destroyed\n+\t     later.  We right now need it in dump_constraints() for\n+\t     constrain_operands(1) whose subproc sometimes reference\n+\t     it (because we are checking strictly, i.e. as if\n+\t     after reload).  */\n+\t  setup_renumber (0);\n+\t  /* Delete some more of the coalesced moves.  */\n+\t  delete_moves ();\n+\t  dump_constraints ();\n+\t}\n+      else\n+\t{\n+\t  /* If there were changes, this means spill code was added,\n+\t     therefore repeat some things, including some initialization\n+\t     of global data structures.  */\n+\t  if ((debug_new_regalloc & DUMP_REGCLASS) == 0)\n+\t    rtl_dump_file = NULL;\n+\t  /* We have new pseudos (the stackwebs).  */\n+\t  allocate_reg_info (max_reg_num (), FALSE, FALSE);\n+\t  /* And new insns.  */\n+\t  compute_bb_for_insn ();\n+\t  /* Some of them might be dead.  */\n+\t  delete_trivially_dead_insns (get_insns (), max_reg_num ());\n+\t  /* Those new pseudos need to have their REFS count set.  */\n+\t  reg_scan_update (get_insns (), NULL, max_regno);\n+\t  max_regno = max_reg_num ();\n+\t  /* And they need usefull classes too.  */\n+\t  regclass (get_insns (), max_reg_num (), rtl_dump_file);\n+\t  rtl_dump_file = ra_dump_file;\n+\n+\t  /* Remember the number of defs and uses, so we can distinguish\n+\t     new from old refs in the next pass.  */\n+\t  last_def_id = df->def_id;\n+\t  last_use_id = df->use_id;\n+\t}\n+\n+      /* Output the graph, and possibly the current insn sequence.  */\n+      dump_ra (df);\n+      if (changed && (debug_new_regalloc & DUMP_RTL) != 0)\n+\t{\n+\t  ra_print_rtl_with_bb (rtl_dump_file, get_insns ());\n+\t  fflush (rtl_dump_file);\n+\t}\n+\n+      /* Reset the web lists.  */\n+      reset_lists ();\n+      free_mem (df);\n+    }\n+  while (changed);\n+\n+  /* We are done with allocation, free all memory and output some\n+     debug info.  */\n+  free_all_mem (df);\n+  df_finish (df);\n+  if ((debug_new_regalloc & DUMP_RESULTS) == 0)\n+    dump_cost (DUMP_COSTS);\n+  ra_debug_msg (DUMP_COSTS, \"ticks for build-phase: %ld\\n\", ticks_build);\n+  ra_debug_msg (DUMP_COSTS, \"ticks for rebuild-phase: %ld\\n\", ticks_rebuild);\n+  if ((debug_new_regalloc & (DUMP_FINAL_RTL | DUMP_RTL)) != 0)\n+    ra_print_rtl_with_bb (rtl_dump_file, get_insns ());\n+\n+  /* We might have new pseudos, so allocate the info arrays for them.  */\n+  if ((debug_new_regalloc & DUMP_SM) == 0)\n+    rtl_dump_file = NULL;\n+  no_new_pseudos = 0;\n+  allocate_reg_info (max_reg_num (), FALSE, FALSE);\n+  no_new_pseudos = 1;\n+  rtl_dump_file = ra_dump_file;\n+\n+  /* Some spill insns could've been inserted after trapping calls, i.e.\n+     at the end of a basic block, which really ends at that call.\n+     Fixup that breakages by adjusting basic block boundaries.  */\n+  fixup_abnormal_edges ();\n+\n+  /* Cleanup the flow graph.  */\n+  if ((debug_new_regalloc & DUMP_LAST_FLOW) == 0)\n+    rtl_dump_file = NULL;\n+  life_analysis (get_insns (), rtl_dump_file,\n+\t\t PROP_DEATH_NOTES | PROP_LOG_LINKS  | PROP_REG_INFO);\n+  cleanup_cfg (CLEANUP_EXPENSIVE);\n+  recompute_reg_usage (get_insns (), TRUE);\n+  if (rtl_dump_file)\n+    dump_flow_info (rtl_dump_file);\n+  rtl_dump_file = ra_dump_file;\n+\n+  /* update_equiv_regs() can't be called after register allocation.\n+     It might delete some pseudos, and insert other insns setting\n+     up those pseudos in different places.  This of course screws up\n+     the allocation because that may destroy a hardreg for another\n+     pseudo.\n+     XXX we probably should do something like that on our own.  I.e.\n+     creating REG_EQUIV notes.  */\n+  /*update_equiv_regs ();*/\n+\n+  /* Setup the reg_renumber[] array for reload.  */\n+  setup_renumber (1);\n+  sbitmap_free (insns_with_deaths);\n+\n+  /* Remove REG_DEAD notes which are incorrectly set.  See the docu\n+     of that function.  */\n+  remove_suspicious_death_notes ();\n+\n+  if ((debug_new_regalloc & DUMP_LAST_RTL) != 0)\n+    ra_print_rtl_with_bb (rtl_dump_file, get_insns ());\n+  dump_static_insn_cost (rtl_dump_file,\n+\t\t\t \"after allocation/spilling, before reload\", NULL);\n+\n+  /* Allocate the reg_equiv_memory_loc array for reload.  */\n+  reg_equiv_memory_loc = (rtx *) xcalloc (max_regno, sizeof (rtx));\n+  /* And possibly initialize it.  */\n+  allocate_initial_values (reg_equiv_memory_loc);\n+  /* And one last regclass pass just before reload.  */\n+  regclass (get_insns (), max_reg_num (), rtl_dump_file);\n+}\n+\n+/*\n+vim:cinoptions={.5s,g0,p5,t0,(0,^-0.5s,n-0.5s:tw=78:cindent:sw=4:\n+*/"}, {"sha": "0da1bc3b9f5f2f10430b0928d767f826d1e3b7c1", "filename": "gcc/ra.h", "status": "added", "additions": 624, "deletions": 0, "changes": 624, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fra.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fra.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fra.h?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -0,0 +1,624 @@\n+/* Graph coloring register allocator\n+   Copyright (C) 2001, 2002 Free Software Foundation, Inc.\n+   Contributed by Michael Matz <matz@suse.de>\n+   and Daniel Berlin <dan@cgsoftware.com>.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it under the\n+   terms of the GNU General Public License as published by the Free Software\n+   Foundation; either version 2, or (at your option) any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n+   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for more\n+   details.\n+\n+   You should have received a copy of the GNU General Public License along\n+   with GCC; see the file COPYING.  If not, write to the Free Software\n+   Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */\n+\n+/* Double linked list to implement the per-type lists of webs\n+   and moves.  */\n+struct dlist\n+{\n+  struct dlist *prev;\n+  struct dlist *next;\n+  union\n+    {\n+      struct web *web;\n+      struct move *move;\n+    } value;\n+};\n+/* Simple helper macros for ease of misuse.  */\n+#define DLIST_WEB(l) ((l)->value.web)\n+#define DLIST_MOVE(l) ((l)->value.move)\n+\n+/* Classification of a given node (i.e. what state it's in).  */\n+enum node_type\n+{\n+  INITIAL = 0, FREE,\n+  PRECOLORED,\n+  SIMPLIFY, SIMPLIFY_SPILL, SIMPLIFY_FAT, FREEZE, SPILL,\n+  SELECT,\n+  SPILLED, COALESCED, COLORED,\n+  LAST_NODE_TYPE\n+};\n+\n+/* A list of conflict bitmaps, factorized on the exact part of\n+   the source, which conflicts with the DEFs, whose ID are noted in\n+   the bitmap.  This is used while building web-parts with conflicts.  */\n+struct tagged_conflict\n+{\n+  struct tagged_conflict *next;\n+  bitmap conflicts;\n+\n+  /* If the part of source identified by size S, byteoffset O conflicts,\n+     then size_word == S | (O << 16).  */\n+  unsigned int size_word;\n+};\n+\n+/* Such a structure is allocated initially for each def and use.\n+   In the process of building the interference graph web parts are\n+   connected together, if they have common instructions and reference the\n+   same register.  That way live ranges are build (by connecting defs and\n+   uses) and implicitely complete webs (by connecting web parts in common\n+   uses).  */\n+struct web_part\n+{\n+  /* The def or use for this web part.  */\n+  struct ref *ref;\n+  /* The uplink implementing the disjoint set.  */\n+  struct web_part *uplink;\n+\n+  /* Here dynamic information associated with each def/use is saved.\n+     This all is only valid for root web parts (uplink==NULL).\n+     That's the information we need to merge, if web parts are unioned.  */\n+\n+  /* Number of spanned insns containing any deaths.  */\n+  unsigned int spanned_deaths;\n+  /* The list of bitmaps of DEF ID's with which this part conflicts.  */\n+  struct tagged_conflict *sub_conflicts;\n+  /* If there's any call_insn, while this part is live.  */\n+  unsigned int crosses_call : 1;\n+};\n+\n+/* Web structure used to store info about connected live ranges.\n+   This represents the nodes of the interference graph, which gets\n+   colored.  It can also hold subwebs, which are contained in webs\n+   and represent subregs.  */\n+struct web\n+{\n+  /* Unique web ID.  */\n+  unsigned int id;\n+\n+  /* Register number of the live range's variable.  */\n+  unsigned int regno;\n+\n+  /* How many insns containing deaths do we span?  */\n+  unsigned int span_deaths;\n+\n+  /* Spill_temp indicates if this web was part of a web spilled in the\n+     last iteration, or or reasons why this shouldn't be spilled again.\n+     1 spill web, can't be spilled.\n+     2 big spill web (live over some deaths).  Discouraged, but not\n+       impossible to spill again.\n+     3 short web (spans no deaths), can't be spilled.  */\n+  unsigned int spill_temp;\n+\n+  /* When coalescing we might change spill_temp.  If breaking aliases we\n+     need to restore it.  */\n+  unsigned int orig_spill_temp;\n+\n+  /* Cost of spilling.  */\n+  unsigned HOST_WIDE_INT spill_cost;\n+  unsigned HOST_WIDE_INT orig_spill_cost;\n+\n+  /* How many webs are aliased to us?  */\n+  unsigned int num_aliased;\n+\n+  /* The color we got.  This is a hardreg number.  */\n+  int color;\n+  /* 1 + the color this web got in the last pass.  If it hadn't got a color,\n+     or we are in the first pass, or this web is a new one, this is zero.  */\n+  int old_color;\n+\n+  /* Now follow some flags characterizing the web.  */\n+\n+  /* Nonzero, if we should use usable_regs for this web, instead of\n+     preferred_class() or alternate_class().  */\n+  unsigned int use_my_regs:1;\n+\n+  /* Nonzero if we selected this web as possible spill candidate in\n+     select_spill().  */\n+  unsigned int was_spilled:1;\n+\n+  /* We need to distinguish also webs which are targets of coalescing\n+     (all x with some y, so that x==alias(y)), but the alias field is\n+     only set for sources of coalescing.  This flag is set for all webs\n+     involved in coalescing in some way.  */\n+  unsigned int is_coalesced:1;\n+\n+  /* Nonzero, if this web (or subweb) doesn't correspond with any of\n+     the current functions actual use of reg rtx.  Happens e.g. with\n+     conflicts to a web, of which only a part was still undefined at the\n+     point of that conflict.  In this case we construct a subweb\n+     representing these yet undefined bits to have a target for the\n+     conflict.  Suppose e.g. this sequence:\n+     (set (reg:DI x) ...)\n+     (set (reg:SI y) ...)\n+     (set (subreg:SI (reg:DI x) 0) ...)\n+     (use (reg:DI x))\n+     Here x only partly conflicts with y.  Namely only (subreg:SI (reg:DI x)\n+     1) conflicts with it, but this rtx doesn't show up in the program.  For\n+     such things an \"artificial\" subweb is built, and this flag is true for\n+     them.  */\n+  unsigned int artificial:1;\n+\n+  /* Nonzero if we span a call_insn.  */\n+  unsigned int crosses_call:1;\n+\n+  /* Wether the web is involved in a move insn.  */\n+  unsigned int move_related:1;\n+\n+  /* 1 when this web (or parts thereof) are live over an abnormal edge.  */\n+  unsigned int live_over_abnormal:1;\n+\n+  /* Nonzero if this web is used in subregs where the mode change\n+     was illegal for hardregs in CLASS_CANNOT_CHANGE_MODE.  */\n+  unsigned int mode_changed:1;\n+\n+  /* Nonzero, when this web stems from the last pass of the allocator,\n+     and all info is still valid (i.e. it wasn't spilled).  */\n+  unsigned int old_web:1;\n+\n+  /* Used in rewrite_program2() to remember webs, which\n+     are already marked for (re)loading.  */\n+  unsigned int in_load:1;\n+\n+  /* If in_load is != 0, then this is nonzero, if only one use was seen\n+     since insertion in loadlist.  Zero if more uses currently need a\n+     reload.  Used to differentiate between inserting register loads or\n+     directly substituting the stackref.  */\n+  unsigned int one_load:1;\n+\n+  /* When using rewrite_program2() this flag gets set if some insns\n+     were inserted on behalf of this web.  IR spilling might ignore some\n+     deaths up to the def, so no code might be emitted and we need not to\n+     spill such a web again.  */\n+  unsigned int changed:1;\n+\n+  /* With interference region spilling it's sometimes the case, that a\n+     bb border is also an IR border for webs, which were targets of moves,\n+     which are already removed due to coalescing.  All webs, which are\n+     a destination of such a removed move, have this flag set.  */\n+  unsigned int target_of_spilled_move:1;\n+\n+  /* For optimistic coalescing we need to be able to break aliases, which\n+     includes restoring conflicts to those before coalescing.  This flag\n+     is set, if we have a list of conflicts before coalescing.  It's needed\n+     because that list is lazily constructed only when actually needed.  */\n+  unsigned int have_orig_conflicts:1;\n+\n+  /* Current state of the node.  */\n+  ENUM_BITFIELD(node_type) type:5;\n+\n+  /* A regclass, combined from preferred and alternate class, or calculated\n+     from usable_regs.  Used only for debugging, and to determine\n+     add_hardregs.  */\n+  ENUM_BITFIELD(reg_class) regclass:10;\n+\n+  /* Additional consecutive hardregs needed for this web.  */\n+  int add_hardregs;\n+\n+  /* Number of conflicts currently.  */\n+  int num_conflicts;\n+\n+  /* Numbers of uses and defs, which belong to this web.  */\n+  unsigned int num_uses;\n+  unsigned int num_defs;\n+\n+  /* The (reg:M a) or (subreg:M1 (reg:M2 a) x) rtx which this\n+     web is based on.  This is used to distinguish subreg webs\n+     from it's reg parents, and to get hold of the mode.  */\n+  rtx orig_x;\n+\n+  /* If this web is a subweb, this point to the super web.  Otherwise\n+     it's NULL.  */\n+  struct web *parent_web;\n+\n+  /* If this web is a subweb, but not the last one, this points to the\n+     next subweb of the same super web.  Otherwise it's NULL.  */\n+  struct web *subreg_next;\n+\n+  /* The set of webs (or subwebs), this web conflicts with.  */\n+  struct conflict_link *conflict_list;\n+\n+  /* If have_orig_conflicts is set this contains a copy of conflict_list,\n+     as it was right after building the interference graph.\n+     It's used for incremental i-graph building and for breaking\n+     coalescings again.  */\n+  struct conflict_link *orig_conflict_list;\n+\n+  /* Bitmap of all conflicts which don't count this pass, because of\n+     non-intersecting hardregs of the conflicting webs.  See also\n+     record_conflict().  */\n+  bitmap useless_conflicts;\n+\n+  /* Different sets of hard registers, for all usable registers, ...  */\n+  HARD_REG_SET usable_regs;\n+  /* ... the same before coalescing, ...  */\n+  HARD_REG_SET orig_usable_regs;\n+  /* ... colors of all already colored neighbors (used when biased coloring\n+     is active), and ...  */\n+  HARD_REG_SET bias_colors;\n+  /* ... colors of PRECOLORED webs this web is connected to by a move.  */\n+  HARD_REG_SET prefer_colors;\n+\n+  /* Number of usable colors in usable_regs.  */\n+  int num_freedom;\n+\n+  /* After successfull coloring the graph each web gets a new reg rtx,\n+     with which the original uses and defs are replaced.  This is it.  */\n+  rtx reg_rtx;\n+\n+  /* While spilling this is the rtx of the home of spilled webs.\n+     It can be a mem ref (a stack slot), or a pseudo register.  */\n+  rtx stack_slot;\n+\n+  /* Used in rewrite_program2() to remember the using\n+     insn last seen for webs needing (re)loads.  */\n+  rtx last_use_insn;\n+\n+  /* If this web is rematerializable, this contains the RTL pattern\n+     usable as source for that.  Otherwise it's NULL.  */\n+  rtx pattern;\n+\n+  /* All the defs and uses.  There are num_defs, resp.\n+     num_uses elements.  */\n+  struct ref **defs; /* [0..num_defs-1] */\n+  struct ref **uses; /* [0..num_uses-1] */\n+\n+  /* The web to which this web is aliased (coalesced).  If NULL, this\n+     web is not coalesced into some other (but might still be a target\n+     for other webs).  */\n+  struct web *alias;\n+\n+  /* With iterated coalescing this is a list of active moves this web\n+     is involved in.  */\n+  struct move_list *moves;\n+\n+  /* The list implementation.  */\n+  struct dlist *dlink;\n+\n+  /* While building webs, out of web-parts, this holds a (partial)\n+     list of all refs for this web seen so far.  */\n+  struct df_link *temp_refs;\n+};\n+\n+/* For implementing a single linked list.  */\n+struct web_link\n+{\n+  struct web_link *next;\n+  struct web *web;\n+};\n+\n+/* A subconflict is part of an conflict edge to track precisely,\n+   which parts of two webs conflict, in case not all of both webs do.  */\n+struct sub_conflict\n+{\n+  /* The next partial conflict.  For one such list the parent-web of\n+     all the S webs, resp. the parent of all the T webs are constant.  */\n+  struct sub_conflict *next;\n+  struct web *s;\n+  struct web *t;\n+};\n+\n+/* This represents an edge in the conflict graph.  */\n+struct conflict_link\n+{\n+  struct conflict_link *next;\n+\n+  /* The web we conflict with (the Target of the edge).  */\n+  struct web *t;\n+\n+  /* If not the complete source web and T conflict, this points to\n+     the list of parts which really conflict.  */\n+  struct sub_conflict *sub;\n+};\n+\n+/* For iterated coalescing the moves can be in these states.  */\n+enum move_type\n+{\n+  WORKLIST, MV_COALESCED, CONSTRAINED, FROZEN, ACTIVE,\n+  LAST_MOVE_TYPE\n+};\n+\n+/* Structure of a move we are considering coalescing.  */\n+struct move\n+{\n+  rtx insn;\n+  struct web *source_web;\n+  struct web *target_web;\n+  enum move_type type;\n+  struct dlist *dlink;\n+};\n+\n+/* List of moves.  */\n+struct move_list\n+{\n+  struct move_list *next;\n+  struct move *move;\n+};\n+\n+/* To have fast access to the defs and uses per insn, we have one such\n+   structure per insn.  The difference to the normal df.c structures is,\n+   that it doesn't contain any NULL refs, which df.c produces in case\n+   an insn was modified and it only contains refs to pseudo regs, or to\n+   hardregs which matter for allocation, i.e. those not in\n+   never_use_colors.  */\n+struct ra_insn_info\n+{\n+  unsigned int num_defs, num_uses;\n+  struct ref **defs, **uses;\n+};\n+\n+/* The above structures are stored in this array, indexed by UID...  */\n+extern struct ra_insn_info *insn_df;\n+/* ... and the size of that array, as we add insn after setting it up.  */\n+extern int insn_df_max_uid;\n+\n+/* The interference graph.  */\n+extern sbitmap igraph;\n+/* And how to access it.  I and J are web indices.  If the bit\n+   igraph_index(I, J) is set, then they conflict.  Note, that\n+   if only parts of webs conflict, then also only those parts\n+   are noted in the I-graph (i.e. the parent webs not).  */\n+#define igraph_index(i, j) ((i) < (j) ? ((j)*((j)-1)/2)+(i) : ((i)*((i)-1)/2)+(j))\n+/* This is the bitmap of all (even partly) conflicting super webs.\n+   If bit I*num_webs+J or J*num_webs+I is set, then I and J (both being\n+   super web indices) conflict, maybe only partially.  Note the\n+   assymetry.  */\n+extern sbitmap sup_igraph;\n+\n+/* After the first pass, and when interference region spilling is\n+   activated, bit I is set, when the insn with UID I contains some\n+   refs to pseudos which die at the insn.  */\n+extern sbitmap insns_with_deaths;\n+/* The size of that sbitmap.  */\n+extern int death_insns_max_uid;\n+\n+/* All the web-parts.  There are exactly as many web-parts as there\n+   are register refs in the insn stream.  */\n+extern struct web_part *web_parts;\n+\n+/* The number of all webs, including subwebs.  */\n+extern unsigned int num_webs;\n+/* The number of just the subwebs.  */\n+extern unsigned int num_subwebs;\n+/* The number of all webs, including subwebs.  */\n+extern unsigned int num_allwebs;\n+\n+/* For easy access when given a web ID: id2web[W->id] == W.  */\n+extern struct web **id2web;\n+/* For each hardreg, the web which represents it.  */\n+extern struct web *hardreg2web[FIRST_PSEUDO_REGISTER];\n+\n+/* Given the ID of a df_ref, which represent a DEF, def2web[ID] is\n+   the web, to which this def belongs.  */\n+extern struct web **def2web;\n+/* The same as def2web, just for uses.  */\n+extern struct web **use2web;\n+\n+/* The list of all recognized and coalescable move insns.  */\n+extern struct move_list *wl_moves;\n+\n+\n+/* Some parts of the compiler which we run after colorizing\n+   clean reg_renumber[], so we need another place for the colors.\n+   This is copied to reg_renumber[] just before returning to toplev.  */\n+extern short *ra_reg_renumber;\n+/* The size of that array.  Some passes after coloring might have created\n+   new pseudos, which will get no color.  */\n+extern int ra_max_regno;\n+\n+/* The dataflow structure of the current function, while regalloc\n+   runs.  */\n+extern struct df *df;\n+\n+/* For each basic block B we have a bitmap of DF_REF_ID's of uses,\n+   which backward reach the end of B.  */\n+extern bitmap *live_at_end;\n+\n+/* One pass is: collecting registers refs, buiding I-graph, spilling.\n+   And this is how often we already ran that for the current function.  */\n+extern int ra_pass;\n+\n+/* The maximum pseudo regno, just before register allocation starts.\n+   While regalloc runs all pseudos with a larger number represent\n+   potentially stack slots or hardregs.  I call them stackwebs or\n+   stackpseudos.  */\n+extern unsigned int max_normal_pseudo;\n+\n+/* One of the fixed colors.  It must be < FIRST_PSEUDO_REGISTER, because\n+   we sometimes want to check the color against a HARD_REG_SET.  It must\n+   be >= 0, because negative values mean \"no color\".\n+   This color is used for the above stackwebs, when they can't be colored.\n+   I.e. normally they would be spilled, but they already represent\n+   stackslots.  So they are colored with an invalid color.  It has\n+   the property that even webs which conflict can have that color at the\n+   same time.  I.e. a stackweb with that color really represents a\n+   stackslot.  */\n+extern int an_unusable_color;\n+\n+/* While building the I-graph, every time insn UID is looked at,\n+   number_seen[UID] is incremented.  For debugging.  */\n+extern int *number_seen;\n+\n+/* The different lists on which a web can be (based on the type).  */\n+extern struct dlist *web_lists[(int) LAST_NODE_TYPE];\n+#define WEBS(type) (web_lists[(int)(type)])\n+\n+/* The largest DF_REF_ID of defs resp. uses, as it was in the\n+   last pass.  In the first pass this is zero.  Used to distinguish new\n+   from old refrences.  */\n+extern unsigned int last_def_id;\n+extern unsigned int last_use_id;\n+\n+/* Similar for UIDs and number of webs.  */\n+extern int last_max_uid;\n+extern unsigned int last_num_webs;\n+\n+/* If I is the ID of an old use, and last_check_uses[I] is set,\n+   then we must reevaluate it's flow while building the new I-graph.  */\n+extern sbitmap last_check_uses;\n+\n+/* If nonzero, record_conflict() saves the current conflict list of\n+   webs in orig_conflict_list, when not already done so, and the conflict\n+   list is going to be changed.  It is set, after initially building the\n+   I-graph.  I.e. new conflicts due to coalescing trigger that copying.  */\n+extern unsigned int remember_conflicts;\n+\n+/* The maximum UID right before calling regalloc().\n+   Used to detect any instructions inserted by the allocator.  */\n+extern int orig_max_uid;\n+\n+/* A HARD_REG_SET of those color, which can't be used for coalescing.\n+   Includes e.g. fixed_regs.  */\n+extern HARD_REG_SET never_use_colors;\n+/* For each class C this is reg_class_contents[C] \\ never_use_colors.  */\n+extern HARD_REG_SET usable_regs[N_REG_CLASSES];\n+/* For each class C the count of hardregs in usable_regs[C].  */\n+extern unsigned int num_free_regs[N_REG_CLASSES];\n+/* For each mode M the hardregs, which are MODE_OK for M, and have\n+   enough space behind them to hold an M value.  Additinally\n+   if reg R is OK for mode M, but it needs two hardregs, then R+1 will\n+   also be set here, even if R+1 itself is not OK for M.  I.e. this\n+   represent the possible resources which could be taken away be a value\n+   in mode M.  */\n+extern HARD_REG_SET hardregs_for_mode[NUM_MACHINE_MODES];\n+/* For 0 <= I <= 255, the number of bits set in I.  Used to calculate\n+   the number of set bits in a HARD_REG_SET.  */\n+extern unsigned char byte2bitcount[256];\n+\n+/* Expressive helper macros.  */\n+#define ID2WEB(I) id2web[I]\n+#define NUM_REGS(W) (((W)->type == PRECOLORED) ? 1 : (W)->num_freedom)\n+#define SUBWEB_P(W) (GET_CODE ((W)->orig_x) == SUBREG)\n+\n+/* Constant usable as debug area to ra_debug_msg.  */\n+#define DUMP_COSTS\t\t0x0001\n+#define DUMP_WEBS\t\t0x0002\n+#define DUMP_IGRAPH\t\t0x0004\n+#define DUMP_PROCESS\t\t0x0008\n+#define DUMP_COLORIZE\t\t0x0010\n+#define DUMP_ASM\t\t0x0020\n+#define DUMP_CONSTRAINTS\t0x0040\n+#define DUMP_RESULTS\t\t0x0080\n+#define DUMP_DF\t\t\t0x0100\n+#define DUMP_RTL\t\t0x0200\n+#define DUMP_FINAL_RTL\t\t0x0400\n+#define DUMP_REGCLASS\t\t0x0800\n+#define DUMP_SM\t\t\t0x1000\n+#define DUMP_LAST_FLOW\t\t0x2000\n+#define DUMP_LAST_RTL\t\t0x4000\n+#define DUMP_REBUILD\t\t0x8000\n+#define DUMP_IGRAPH_M\t\t0x10000\n+#define DUMP_VALIDIFY\t\t0x20000\n+#define DUMP_EVER\t\t((unsigned int)-1)\n+#define DUMP_NEARLY_EVER\t(DUMP_EVER - DUMP_COSTS - DUMP_IGRAPH_M)\n+\n+/* All the wanted debug levels as ORing of the various DUMP_xxx\n+   constants.  */\n+extern unsigned int debug_new_regalloc;\n+\n+/* Nonzero means we want biased coloring.  */\n+extern int flag_ra_biased;\n+\n+/* Nonzero if we want to use improved (and slow) spilling.  This\n+   includes also interference region spilling (see below).  */\n+extern int flag_ra_improved_spilling;\n+\n+/* Nonzero for using interference region spilling.  Zero for improved\n+   Chaintin style spilling (only at deaths).  */\n+extern int flag_ra_ir_spilling;\n+\n+/* Nonzero if we use optimistic coalescing, zero for iterated\n+   coalescing.  */\n+extern int flag_ra_optimistic_coalescing;\n+\n+/* Nonzero if we want to break aliases of spilled webs.  Forced to\n+   nonzero, when flag_ra_optimistic_coalescing is.  */\n+extern int flag_ra_break_aliases;\n+\n+/* Nonzero if we want to merge the spill costs of webs which\n+   are coalesced.  */\n+extern int flag_ra_merge_spill_costs;\n+\n+/* Nonzero if we want to spill at every use, instead of at deaths,\n+   or intereference region borders.  */\n+extern int flag_ra_spill_every_use;\n+\n+/* Nonzero to output all notes in the debug dumps.  */\n+extern int flag_ra_dump_notes;\n+\n+extern inline void * ra_alloc PARAMS ((size_t));\n+extern inline void * ra_calloc PARAMS ((size_t));\n+extern int hard_regs_count PARAMS ((HARD_REG_SET));\n+extern rtx ra_emit_move_insn PARAMS ((rtx, rtx));\n+extern void ra_debug_msg PARAMS ((unsigned int,\n+\t\t\t          const char *, ...)) ATTRIBUTE_PRINTF_2;\n+extern int hard_regs_intersect_p PARAMS ((HARD_REG_SET *, HARD_REG_SET *));\n+extern unsigned int rtx_to_bits PARAMS ((rtx));\n+extern struct web * find_subweb PARAMS ((struct web *, rtx));\n+extern struct web * find_subweb_2 PARAMS ((struct web *, unsigned int));\n+extern struct web * find_web_for_subweb_1 PARAMS ((struct web *));\n+\n+#define find_web_for_subweb(w) (((w)->parent_web) \\\n+\t\t\t\t? find_web_for_subweb_1 ((w)->parent_web) \\\n+\t\t\t\t: (w))\n+\n+extern void ra_build_realloc PARAMS ((struct df *));\n+extern void ra_build_free PARAMS ((void));\n+extern void ra_build_free_all PARAMS ((struct df *));\n+extern void ra_colorize_init PARAMS ((void));\n+extern void ra_colorize_free_all PARAMS ((void));\n+extern void ra_rewrite_init PARAMS ((void));\n+\n+extern void ra_print_rtx PARAMS ((FILE *, rtx, int));\n+extern void ra_print_rtx_top PARAMS ((FILE *, rtx, int));\n+extern void ra_debug_rtx PARAMS ((rtx));\n+extern void ra_debug_insns PARAMS ((rtx, int));\n+extern void ra_debug_bbi PARAMS ((int));\n+extern void ra_print_rtl_with_bb PARAMS ((FILE *, rtx));\n+extern void dump_igraph PARAMS ((struct df *));\n+extern void dump_igraph_machine PARAMS ((void));\n+extern void dump_constraints PARAMS ((void));\n+extern void dump_cost PARAMS ((unsigned int));\n+extern void dump_graph_cost PARAMS ((unsigned int, const char *));\n+extern void dump_ra PARAMS ((struct df *));\n+extern void dump_number_seen PARAMS ((void));\n+extern void dump_static_insn_cost PARAMS ((FILE *, const char *,\n+\t\t\t\t\t   const char *));\n+extern void dump_web_conflicts PARAMS ((struct web *));\n+extern void dump_web_insns PARAMS ((struct web*));\n+extern int web_conflicts_p PARAMS ((struct web *, struct web *));\n+extern void debug_hard_reg_set PARAMS ((HARD_REG_SET));\n+\n+extern void remove_list PARAMS ((struct dlist *, struct dlist **));\n+extern struct dlist * pop_list PARAMS ((struct dlist **));\n+extern void record_conflict PARAMS ((struct web *, struct web *));\n+extern int memref_is_stack_slot PARAMS ((rtx));\n+extern void build_i_graph PARAMS ((struct df *));\n+extern void put_web PARAMS ((struct web *, enum node_type));\n+extern void remove_web_from_list PARAMS ((struct web *));\n+extern void reset_lists PARAMS ((void));\n+extern struct web * alias PARAMS ((struct web *));\n+extern void merge_moves PARAMS ((struct web *, struct web *));\n+extern void ra_colorize_graph PARAMS ((struct df *));\n+\n+extern void actual_spill PARAMS ((void));\n+extern void emit_colors PARAMS ((struct df *));\n+extern void delete_moves PARAMS ((void));\n+extern void setup_renumber PARAMS ((int));\n+extern void remove_suspicious_death_notes PARAMS ((void));"}, {"sha": "08051cdb64273409575a58b61ae18f92d6f8343d", "filename": "gcc/regclass.c", "status": "modified", "additions": 19, "deletions": 1, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fregclass.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fregclass.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fregclass.c?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -1317,7 +1317,7 @@ regclass (f, nregs, dump)\n \n \t  /* In non-optimizing compilation REG_N_REFS is not initialized\n \t     yet.  */\n-\t  if (optimize && !REG_N_REFS (i))\n+\t  if (optimize && !REG_N_REFS (i) && !REG_N_SETS (i))\n \t    continue;\n \n \t  for (class = (int) ALL_REGS - 1; class > 0; class--)\n@@ -2397,6 +2397,8 @@ reg_scan_mark_refs (x, insn, note_flag, min_regno)\n   rtx dest;\n   rtx note;\n \n+  if (!x)\n+    return;\n   code = GET_CODE (x);\n   switch (code)\n     {\n@@ -2423,6 +2425,10 @@ reg_scan_mark_refs (x, insn, note_flag, min_regno)\n \t      REGNO_LAST_UID (regno) = INSN_UID (insn);\n \t    if (REGNO_FIRST_UID (regno) == 0)\n \t      REGNO_FIRST_UID (regno) = INSN_UID (insn);\n+\t    /* If we are called by reg_scan_update() (indicated by min_regno\n+\t       being set), we also need to update the reference count.  */\n+\t    if (min_regno)\n+\t      REG_N_REFS (regno)++;\n \t  }\n       }\n       break;\n@@ -2439,6 +2445,18 @@ reg_scan_mark_refs (x, insn, note_flag, min_regno)\n \treg_scan_mark_refs (XEXP (x, 1), insn, note_flag, min_regno);\n       break;\n \n+    case CLOBBER:\n+      {\n+\trtx reg = XEXP (x, 0);\n+\tif (REG_P (reg)\n+\t    && REGNO (reg) >= min_regno)\n+\t  {\n+\t    REG_N_SETS (REGNO (reg))++;\n+\t    REG_N_REFS (REGNO (reg))++;\n+\t  }\n+      }\n+      break;\n+\n     case SET:\n       /* Count a set of the destination if it is a register.  */\n       for (dest = SET_DEST (x);"}, {"sha": "698c23c4ed2076f72a36b6de4c48ea071d95bd17", "filename": "gcc/reload1.c", "status": "modified", "additions": 14, "deletions": 5, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Freload1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Freload1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Freload1.c?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -9475,12 +9475,21 @@ fixup_abnormal_edges ()\n \t\t{\n \t          delete_insn (insn);\n \n-\t\t  /* We're not deleting it, we're moving it.  */\n-\t\t  INSN_DELETED_P (insn) = 0;\n-\t\t  PREV_INSN (insn) = NULL_RTX;\n-\t\t  NEXT_INSN (insn) = NULL_RTX;\n+\t\t  /* Sometimes there's still the return value USE.\n+\t\t     If it's placed after a trapping call (i.e. that\n+\t\t     call is the last insn anyway), we have no fallthru\n+\t\t     edge.  Simply delete this use and don't try to insert\n+\t\t     on the non-existant edge.  */\n+\t\t  if (GET_CODE (PATTERN (insn)) != USE)\n+\t\t    {\n+\t\t      rtx seq;\n+\t\t      /* We're not deleting it, we're moving it.  */\n+\t\t      INSN_DELETED_P (insn) = 0;\n+\t\t      PREV_INSN (insn) = NULL_RTX;\n+\t\t      NEXT_INSN (insn) = NULL_RTX;\n \n-\t          insert_insn_on_edge (insn, e);\n+\t\t      insert_insn_on_edge (insn, e);\n+\t\t    }\n \t\t}\n \t      insn = next;\n \t    }"}, {"sha": "170d87df839de7c6933cee2b0392130ccfcd1bbd", "filename": "gcc/sbitmap.c", "status": "modified", "additions": 31, "deletions": 10, "changes": 41, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fsbitmap.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fsbitmap.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsbitmap.c?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -218,13 +218,26 @@ void\n sbitmap_difference (dst, a, b)\n      sbitmap dst, a, b;\n {\n-  unsigned int i, n = dst->size;\n+  unsigned int i, dst_size = dst->size;\n+  unsigned int min_size = dst->size;\n   sbitmap_ptr dstp = dst->elms;\n   sbitmap_ptr ap = a->elms;\n   sbitmap_ptr bp = b->elms;\n-\n-  for (i = 0; i < n; i++)\n-    *dstp++ = *ap++ & ~*bp++;\n+  \n+  /* A should be at least as large as DEST, to have a defined source.  */\n+  if (a->size < dst_size)\n+    abort ();\n+  /* If minuend is smaller, we simply pretend it to be zero bits, i.e.\n+     only copy the subtrahend into dest.  */\n+  if (b->size < min_size)\n+    min_size = b->size;\n+  for (i = 0; i < min_size; i++)\n+    *dstp++ = *ap++ & (~*bp++);\n+  /* Now fill the rest of dest from A, if B was too short.\n+     This makes sense only when destination and A differ.  */\n+  if (dst != a && i != dst_size)\n+    for (; i < dst_size; i++)\n+      *dstp++ = *ap++;\n }\n \n /* Set DST to be (A and B).\n@@ -658,27 +671,35 @@ dump_sbitmap (file, bmap)\n }\n \n void\n-debug_sbitmap (bmap)\n+dump_sbitmap_file (file, bmap)\n+     FILE *file;\n      sbitmap bmap;\n {\n   unsigned int i, pos;\n \n-  fprintf (stderr, \"n_bits = %d, set = {\", bmap->n_bits);\n+  fprintf (file, \"n_bits = %d, set = {\", bmap->n_bits);\n \n   for (pos = 30, i = 0; i < bmap->n_bits; i++)\n     if (TEST_BIT (bmap, i))\n       {\n \tif (pos > 70)\n \t  {\n-\t    fprintf (stderr, \"\\n\");\n+\t    fprintf (file, \"\\n  \");\n \t    pos = 0;\n \t  }\n \n-\tfprintf (stderr, \"%d \", i);\n-\tpos += 1 + (i >= 10) + (i >= 100);\n+\tfprintf (file, \"%d \", i);\n+\tpos += 2 + (i >= 10) + (i >= 100) + (i >= 1000);\n       }\n \n-  fprintf (stderr, \"}\\n\");\n+  fprintf (file, \"}\\n\");\n+}\n+\n+void\n+debug_sbitmap (bmap)\n+     sbitmap bmap;\n+{\n+  dump_sbitmap_file (stderr, bmap);\n }\n \n void"}, {"sha": "d39759e00f20333c379f13533d69e0c068e9ac35", "filename": "gcc/sbitmap.h", "status": "modified", "additions": 29, "deletions": 0, "changes": 29, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fsbitmap.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Fsbitmap.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsbitmap.h?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -85,12 +85,41 @@ do {\t\t\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n } while (0)\n \n+#define EXECUTE_IF_SET_IN_SBITMAP_REV(SBITMAP, N, CODE)\t\t\t\\\n+do {\t\t\t\t\t\t\t\t\t\\\n+  unsigned int word_num_;\t\t\t\t\t\t\\\n+  unsigned int bit_num_;\t\t\t\t\t\t\\\n+  unsigned int size_ = (SBITMAP)->size;\t\t\t\t\t\\\n+  SBITMAP_ELT_TYPE *ptr_ = (SBITMAP)->elms;\t\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+  for (word_num_ = size_; word_num_ > 0; word_num_--)\t\t\t\\\n+    {\t\t\t\t\t\t\t\t\t\\\n+      SBITMAP_ELT_TYPE word_ = ptr_[word_num_ - 1];\t\t\t\\\n+\t\t\t\t\t\t\t\t\t\\\n+      if (word_ != 0)\t\t\t\t\t\t\t\\\n+\tfor (bit_num_ = SBITMAP_ELT_BITS; bit_num_ > 0; bit_num_--)\t\\\n+\t  {\t\t\t\t\t\t\t\t\\\n+\t    SBITMAP_ELT_TYPE _mask = (SBITMAP_ELT_TYPE)1 << (bit_num_ - 1);\\\n+\t\t\t\t\t\t\t\t\t\\\n+\t    if ((word_ & _mask) != 0)\t\t\t\t\t\\\n+\t      {\t\t\t\t\t\t\t\t\\\n+\t\tword_ &= ~ _mask;\t\t\t\t\t\\\n+\t\t(N) = (word_num_ - 1) * SBITMAP_ELT_BITS + bit_num_ - 1;\\\n+\t\tCODE;\t\t\t\t\t\t\t\\\n+\t\tif (word_ == 0)\t\t\t\t\t\t\\\n+\t\t  break;\t\t\t\t\t\t\\\n+\t      }\t\t\t\t\t\t\t\t\\\n+\t  }\t\t\t\t\t\t\t\t\\\n+    }\t\t\t\t\t\t\t\t\t\\\n+} while (0)\n+\n #define sbitmap_free(MAP)\t\tfree(MAP)\n #define sbitmap_vector_free(VEC)\tfree(VEC)\n \n struct int_list;\n \n extern void dump_sbitmap\t\tPARAMS ((FILE *, sbitmap));\n+extern void dump_sbitmap_file\t\tPARAMS ((FILE *, sbitmap));\n extern void dump_sbitmap_vector \tPARAMS ((FILE *, const char *,\n \t\t\t\t\t\t const char *, sbitmap *,\n \t\t\t\t\t\t int));"}, {"sha": "e9a15a572231ca16ed3926a8bf2e700ee7dca26b", "filename": "gcc/toplev.c", "status": "modified", "additions": 86, "deletions": 38, "changes": 124, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Ftoplev.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ed8d29205b130073bd3147cedf068446689a5386/gcc%2Ftoplev.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftoplev.c?ref=ed8d29205b130073bd3147cedf068446689a5386", "patch": "@@ -95,6 +95,8 @@ Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n extern int size_directive_output;\n extern tree last_assemble_variable_decl;\n \n+extern void reg_alloc PARAMS ((void));\n+\n static void general_init PARAMS ((char *));\n static bool parse_options_and_default_flags PARAMS ((int, char **));\n static void do_compile PARAMS ((int));\n@@ -865,6 +867,9 @@ int flag_merge_constants = 1;\n    one, unconditionally renumber instruction UIDs.  */\n int flag_renumber_insns = 1;\n \n+/* If nonzero, use the graph coloring register allocator.  */\n+int flag_new_regalloc = 0;\n+\n /* Nonzero if we perform superblock formation.  */\n \n int flag_tracer = 0;\n@@ -1173,6 +1178,8 @@ static const lang_independent_options f_options[] =\n    N_(\"Report on permanent memory allocation at end of run\") },\n   { \"trapv\", &flag_trapv, 1,\n    N_(\"Trap for signed overflow in addition / subtraction / multiplication\") },\n+  { \"new-ra\", &flag_new_regalloc, 1,\n+   N_(\"Use graph coloring register allocation.\") },\n };\n \n /* Table of language-specific options.  */\n@@ -3039,7 +3046,7 @@ rest_of_compilation (decl)\n   if (optimize)\n     {\n       clear_bb_flags ();\n-      if (initialize_uninitialized_subregs ())\n+      if (!flag_new_regalloc && initialize_uninitialized_subregs ())\n \t{\n \t  /* Insns were inserted, so things might look a bit different.  */\n \t  insns = get_insns ();\n@@ -3174,60 +3181,101 @@ rest_of_compilation (decl)\n   if (! register_life_up_to_date)\n     recompute_reg_usage (insns, ! optimize_size);\n \n-  /* Allocate the reg_renumber array.  */\n-  allocate_reg_info (max_regno, FALSE, TRUE);\n+  if (flag_new_regalloc)\n+    {\n+      delete_trivially_dead_insns (insns, max_reg_num ());\n+      reg_alloc ();\n \n-  /* And the reg_equiv_memory_loc array.  */\n-  reg_equiv_memory_loc = (rtx *) xcalloc (max_regno, sizeof (rtx));\n+      timevar_pop (TV_LOCAL_ALLOC);\n+      if (dump_file[DFI_lreg].enabled)\n+        {\n+          timevar_push (TV_DUMP);\n \n-  allocate_initial_values (reg_equiv_memory_loc);\n+          close_dump_file (DFI_lreg, NULL, NULL);\n+          timevar_pop (TV_DUMP);\n+        }\n \n-  regclass (insns, max_reg_num (), rtl_dump_file);\n-  rebuild_label_notes_after_reload = local_alloc ();\n+      /* XXX clean up the whole mess to bring live info in shape again.  */\n+      timevar_push (TV_GLOBAL_ALLOC);\n+      open_dump_file (DFI_greg, decl);\n \n-  timevar_pop (TV_LOCAL_ALLOC);\n+      build_insn_chain (insns);\n+      failure = reload (insns, 0);\n \n-  if (dump_file[DFI_lreg].enabled)\n-    {\n-      timevar_push (TV_DUMP);\n+      timevar_pop (TV_GLOBAL_ALLOC);\n \n-      dump_flow_info (rtl_dump_file);\n-      dump_local_alloc (rtl_dump_file);\n+      if (dump_file[DFI_greg].enabled)\n+        {\n+          timevar_push (TV_DUMP);\n \n-      close_dump_file (DFI_lreg, print_rtl_with_bb, insns);\n-      timevar_pop (TV_DUMP);\n+          dump_global_regs (rtl_dump_file);\n+\n+          close_dump_file (DFI_greg, print_rtl_with_bb, insns);\n+          timevar_pop (TV_DUMP);\n+        }\n+\n+      if (failure)\n+        goto exit_rest_of_compilation;\n+      reload_completed = 1;\n+      rebuild_label_notes_after_reload = 0;\n     }\n+  else\n+    {\n+      /* Allocate the reg_renumber array.  */\n+      allocate_reg_info (max_regno, FALSE, TRUE);\n \n-  ggc_collect ();\n+      /* And the reg_equiv_memory_loc array.  */\n+      reg_equiv_memory_loc = (rtx *) xcalloc (max_regno, sizeof (rtx));\n \n-  timevar_push (TV_GLOBAL_ALLOC);\n-  open_dump_file (DFI_greg, decl);\n+      allocate_initial_values (reg_equiv_memory_loc);\n \n-  /* If optimizing, allocate remaining pseudo-regs.  Do the reload\n-     pass fixing up any insns that are invalid.  */\n+      regclass (insns, max_reg_num (), rtl_dump_file);\n+      rebuild_label_notes_after_reload = local_alloc ();\n \n-  if (optimize)\n-    failure = global_alloc (rtl_dump_file);\n-  else\n-    {\n-      build_insn_chain (insns);\n-      failure = reload (insns, 0);\n-    }\n+      timevar_pop (TV_LOCAL_ALLOC);\n+\n+      if (dump_file[DFI_lreg].enabled)\n+\t{\n+\t  timevar_push (TV_DUMP);\n \n-  timevar_pop (TV_GLOBAL_ALLOC);\n+\t  dump_flow_info (rtl_dump_file);\n+\t  dump_local_alloc (rtl_dump_file);\n \n-  if (dump_file[DFI_greg].enabled)\n-    {\n-      timevar_push (TV_DUMP);\n+\t  close_dump_file (DFI_lreg, print_rtl_with_bb, insns);\n+\t  timevar_pop (TV_DUMP);\n+\t}\n \n-      dump_global_regs (rtl_dump_file);\n+      ggc_collect ();\n \n-      close_dump_file (DFI_greg, print_rtl_with_bb, insns);\n-      timevar_pop (TV_DUMP);\n-    }\n+      timevar_push (TV_GLOBAL_ALLOC);\n+      open_dump_file (DFI_greg, decl);\n \n-  if (failure)\n-    goto exit_rest_of_compilation;\n+      /* If optimizing, allocate remaining pseudo-regs.  Do the reload\n+\t pass fixing up any insns that are invalid.  */\n+\n+      if (optimize)\n+\tfailure = global_alloc (rtl_dump_file);\n+      else\n+\t{\n+\t  build_insn_chain (insns);\n+\t  failure = reload (insns, 0);\n+\t}\n+\n+      timevar_pop (TV_GLOBAL_ALLOC);\n+\n+      if (dump_file[DFI_greg].enabled)\n+\t{\n+\t  timevar_push (TV_DUMP);\n+\n+\t  dump_global_regs (rtl_dump_file);\n+\n+\t  close_dump_file (DFI_greg, print_rtl_with_bb, insns);\n+\t  timevar_pop (TV_DUMP);\n+\t}\n+\n+      if (failure)\n+\tgoto exit_rest_of_compilation;\n+    }\n \n   ggc_collect ();\n "}]}