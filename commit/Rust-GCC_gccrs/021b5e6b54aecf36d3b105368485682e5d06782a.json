{"sha": "021b5e6b54aecf36d3b105368485682e5d06782a", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MDIxYjVlNmI1NGFlY2YzNmQzYjEwNTM2ODQ4NTY4MmU1ZDA2NzgyYQ==", "commit": {"author": {"name": "Kyrylo Tkachov", "email": "kyrylo.tkachov@arm.com", "date": "2013-12-19T18:21:10Z"}, "committer": {"name": "Kyrylo Tkachov", "email": "ktkachov@gcc.gnu.org", "date": "2013-12-19T18:21:10Z"}, "message": "arm.c (enum arm_builtins): Add crypto builtins.\n\n2013-12-19  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>\n\n\t * config/arm/arm.c (enum arm_builtins): Add crypto builtins.\n\t (arm_init_neon_builtins): Handle crypto builtins.\n\t (bdesc_2arg): Likewise.\n\t (bdesc_1arg): Likewise.\n\t (bdesc_3arg): New table.\n\t (arm_expand_ternop_builtin): New function.\n\t (arm_expand_unop_builtin): Handle sha1h explicitly.\n\t (arm_expand_builtin): Handle ternary builtins.\n\t * config/arm/arm.h (TARGET_CPU_CPP_BUILTINS):\n\t Define __ARM_FEATURE_CRYPTO.\n\t * config/arm/arm.md: Include crypto.md.\n\t (is_neon_type): Add crypto types.\n\t * config/arm/arm_neon_builtins.def: Add TImode reinterprets.\n\t * config/arm/crypto.def: New.\n\t * config/arm/crypto.md: Likewise.\n\t * config/arm/iterators.md (CRYPTO_UNARY): New int iterator.\n\t (CRYPTO_BINARY): Likewise.\n\t (CRYPTO_TERNARY): Likewise.\n\t (CRYPTO_SELECTING): Likewise.\n\t (crypto_pattern): New int attribute.\n\t (crypto_size_sfx): Likewise.\n\t (crypto_mode): Likewise.\n\t (crypto_type): Likewise.\n\t * config/arm/neon-gen.ml: Handle poly64_t and poly128_t types.\n\t Handle crypto intrinsics.\n\t * config/arm/neon.ml: Add support for poly64 and polt128 types\n\t and intrinsics. Define crypto intrinsics.\n\t * config/arm/neon.md (neon_vreinterpretti<mode>): New pattern.\n\t (neon_vreinterpretv16qi<mode>): Use VQXMOV mode iterator.\n\t (neon_vreinterpretv8hi<mode>): Likewise.\n\t (neon_vreinterpretv4si<mode>): Likewise.\n\t (neon_vreinterpretv4sf<mode>): Likewise.\n\t (neon_vreinterpretv2di<mode>): Likewise.\n\t * config/arm/unspecs.md (UNSPEC_AESD, UNSPEC_AESE, UNSPEC_AESIMC,\n\t UNSPEC_AESMC, UNSPEC_SHA1C, UNSPEC_SHA1M, UNSPEC_SHA1P, UNSPEC_SHA1H,\n\t UNSPEC_SHA1SU0, UNSPEC_SHA1SU1, UNSPEC_SHA256H, UNSPEC_SHA256H2,\n\t UNSPEC_SHA256SU0, UNSPEC_SHA256SU1, VMULLP64): Define.\n\t * config/arm/arm_neon.h: Regenerate.\n\nFrom-SVN: r206130", "tree": {"sha": "9677a093f409fce0942b03b172447f9574d12b77", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/9677a093f409fce0942b03b172447f9574d12b77"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/021b5e6b54aecf36d3b105368485682e5d06782a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/021b5e6b54aecf36d3b105368485682e5d06782a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/021b5e6b54aecf36d3b105368485682e5d06782a", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/021b5e6b54aecf36d3b105368485682e5d06782a/comments", "author": {"login": "ktkachov-arm", "id": 74917949, "node_id": "MDQ6VXNlcjc0OTE3OTQ5", "avatar_url": "https://avatars.githubusercontent.com/u/74917949?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ktkachov-arm", "html_url": "https://github.com/ktkachov-arm", "followers_url": "https://api.github.com/users/ktkachov-arm/followers", "following_url": "https://api.github.com/users/ktkachov-arm/following{/other_user}", "gists_url": "https://api.github.com/users/ktkachov-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/ktkachov-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ktkachov-arm/subscriptions", "organizations_url": "https://api.github.com/users/ktkachov-arm/orgs", "repos_url": "https://api.github.com/users/ktkachov-arm/repos", "events_url": "https://api.github.com/users/ktkachov-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/ktkachov-arm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "b78e932d513b7d3373bc5e1aab456dda737d07d0", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b78e932d513b7d3373bc5e1aab456dda737d07d0", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b78e932d513b7d3373bc5e1aab456dda737d07d0"}], "stats": {"total": 3255, "additions": 2555, "deletions": 700}, "files": [{"sha": "3a19c2ec45c016a868682b798bac89453b9da5b2", "filename": "gcc/ChangeLog", "status": "modified", "additions": 41, "deletions": 0, "changes": 41, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=021b5e6b54aecf36d3b105368485682e5d06782a", "patch": "@@ -1,3 +1,44 @@\n+2013-12-19  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>\n+\n+\t * config/arm/arm.c (enum arm_builtins): Add crypto builtins.\n+\t (arm_init_neon_builtins): Handle crypto builtins.\n+\t (bdesc_2arg): Likewise.\n+\t (bdesc_1arg): Likewise.\n+\t (bdesc_3arg): New table.\n+\t (arm_expand_ternop_builtin): New function.\n+\t (arm_expand_unop_builtin): Handle sha1h explicitly.\n+\t (arm_expand_builtin): Handle ternary builtins.\n+\t * config/arm/arm.h (TARGET_CPU_CPP_BUILTINS):\n+\t Define __ARM_FEATURE_CRYPTO.\n+\t * config/arm/arm.md: Include crypto.md.\n+\t (is_neon_type): Add crypto types.\n+\t * config/arm/arm_neon_builtins.def: Add TImode reinterprets.\n+\t * config/arm/crypto.def: New.\n+\t * config/arm/crypto.md: Likewise.\n+\t * config/arm/iterators.md (CRYPTO_UNARY): New int iterator.\n+\t (CRYPTO_BINARY): Likewise.\n+\t (CRYPTO_TERNARY): Likewise.\n+\t (CRYPTO_SELECTING): Likewise.\n+\t (crypto_pattern): New int attribute.\n+\t (crypto_size_sfx): Likewise.\n+\t (crypto_mode): Likewise.\n+\t (crypto_type): Likewise.\n+\t * config/arm/neon-gen.ml: Handle poly64_t and poly128_t types.\n+\t Handle crypto intrinsics.\n+\t * config/arm/neon.ml: Add support for poly64 and polt128 types\n+\t and intrinsics. Define crypto intrinsics.\n+\t * config/arm/neon.md (neon_vreinterpretti<mode>): New pattern.\n+\t (neon_vreinterpretv16qi<mode>): Use VQXMOV mode iterator.\n+\t (neon_vreinterpretv8hi<mode>): Likewise.\n+\t (neon_vreinterpretv4si<mode>): Likewise.\n+\t (neon_vreinterpretv4sf<mode>): Likewise.\n+\t (neon_vreinterpretv2di<mode>): Likewise.\n+\t * config/arm/unspecs.md (UNSPEC_AESD, UNSPEC_AESE, UNSPEC_AESIMC,\n+\t UNSPEC_AESMC, UNSPEC_SHA1C, UNSPEC_SHA1M, UNSPEC_SHA1P, UNSPEC_SHA1H,\n+\t UNSPEC_SHA1SU0, UNSPEC_SHA1SU1, UNSPEC_SHA256H, UNSPEC_SHA256H2,\n+\t UNSPEC_SHA256SU0, UNSPEC_SHA256SU1, VMULLP64): Define.\n+\t * config/arm/arm_neon.h: Regenerate.\n+\n 2013-12-19  H.J. Lu  <hongjiu.lu@intel.com>\n \n \tPR driver/59321"}, {"sha": "8c6eaaa66959a4d8564925fd30833d9aa861efb8", "filename": "gcc/config/arm/arm.c", "status": "modified", "additions": 244, "deletions": 11, "changes": 255, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Farm.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Farm.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm.c?ref=021b5e6b54aecf36d3b105368485682e5d06782a", "patch": "@@ -23151,6 +23151,23 @@ enum arm_builtins\n   ARM_BUILTIN_CRC32CH,\n   ARM_BUILTIN_CRC32CW,\n \n+#undef CRYPTO1\n+#undef CRYPTO2\n+#undef CRYPTO3\n+\n+#define CRYPTO1(L, U, M1, M2) \\\n+  ARM_BUILTIN_CRYPTO_##U,\n+#define CRYPTO2(L, U, M1, M2, M3) \\\n+  ARM_BUILTIN_CRYPTO_##U,\n+#define CRYPTO3(L, U, M1, M2, M3, M4) \\\n+  ARM_BUILTIN_CRYPTO_##U,\n+\n+#include \"crypto.def\"\n+\n+#undef CRYPTO1\n+#undef CRYPTO2\n+#undef CRYPTO3\n+\n #include \"arm_neon_builtins.def\"\n \n   ,ARM_BUILTIN_MAX\n@@ -23172,6 +23189,9 @@ enum arm_builtins\n \n static GTY(()) tree arm_builtin_decls[ARM_BUILTIN_MAX];\n \n+#define NUM_DREG_TYPES 5\n+#define NUM_QREG_TYPES 6\n+\n static void\n arm_init_neon_builtins (void)\n {\n@@ -23185,6 +23205,7 @@ arm_init_neon_builtins (void)\n   tree neon_polyHI_type_node;\n   tree neon_intSI_type_node;\n   tree neon_intDI_type_node;\n+  tree neon_intUTI_type_node;\n   tree neon_float_type_node;\n \n   tree intQI_pointer_node;\n@@ -23247,9 +23268,9 @@ arm_init_neon_builtins (void)\n   tree void_ftype_pv4sf_v4sf_v4sf;\n   tree void_ftype_pv2di_v2di_v2di;\n \n-  tree reinterp_ftype_dreg[5][5];\n-  tree reinterp_ftype_qreg[5][5];\n-  tree dreg_types[5], qreg_types[5];\n+  tree reinterp_ftype_dreg[NUM_DREG_TYPES][NUM_DREG_TYPES];\n+  tree reinterp_ftype_qreg[NUM_QREG_TYPES][NUM_QREG_TYPES];\n+  tree dreg_types[NUM_DREG_TYPES], qreg_types[NUM_QREG_TYPES];\n \n   /* Create distinguished type nodes for NEON vector element types,\n      and pointers to values of such types, so we can detect them later.  */\n@@ -23339,6 +23360,8 @@ arm_init_neon_builtins (void)\n   intUHI_type_node = make_unsigned_type (GET_MODE_PRECISION (HImode));\n   intUSI_type_node = make_unsigned_type (GET_MODE_PRECISION (SImode));\n   intUDI_type_node = make_unsigned_type (GET_MODE_PRECISION (DImode));\n+  neon_intUTI_type_node = make_unsigned_type (GET_MODE_PRECISION (TImode));\n+\n \n   (*lang_hooks.types.register_builtin_type) (intUQI_type_node,\n \t\t\t\t\t     \"__builtin_neon_uqi\");\n@@ -23348,6 +23371,10 @@ arm_init_neon_builtins (void)\n \t\t\t\t\t     \"__builtin_neon_usi\");\n   (*lang_hooks.types.register_builtin_type) (intUDI_type_node,\n \t\t\t\t\t     \"__builtin_neon_udi\");\n+  (*lang_hooks.types.register_builtin_type) (intUDI_type_node,\n+\t\t\t\t\t     \"__builtin_neon_poly64\");\n+  (*lang_hooks.types.register_builtin_type) (neon_intUTI_type_node,\n+\t\t\t\t\t     \"__builtin_neon_poly128\");\n \n   /* Opaque integer types for structures of vectors.  */\n   intEI_type_node = make_signed_type (GET_MODE_PRECISION (EImode));\n@@ -23409,6 +23436,80 @@ arm_init_neon_builtins (void)\n     build_function_type_list (void_type_node, V2DI_pointer_node, V2DI_type_node,\n \t\t\t      V2DI_type_node, NULL);\n \n+  if (TARGET_CRYPTO && TARGET_HARD_FLOAT)\n+  {\n+    tree V4USI_type_node =\n+      build_vector_type_for_mode (intUSI_type_node, V4SImode);\n+\n+    tree V16UQI_type_node =\n+      build_vector_type_for_mode (intUQI_type_node, V16QImode);\n+\n+    tree v16uqi_ftype_v16uqi\n+      = build_function_type_list (V16UQI_type_node, V16UQI_type_node, NULL_TREE);\n+\n+    tree v16uqi_ftype_v16uqi_v16uqi\n+      = build_function_type_list (V16UQI_type_node, V16UQI_type_node,\n+                                  V16UQI_type_node, NULL_TREE);\n+\n+    tree v4usi_ftype_v4usi\n+      = build_function_type_list (V4USI_type_node, V4USI_type_node, NULL_TREE);\n+\n+    tree v4usi_ftype_v4usi_v4usi\n+      = build_function_type_list (V4USI_type_node, V4USI_type_node,\n+                                  V4USI_type_node, NULL_TREE);\n+\n+    tree v4usi_ftype_v4usi_v4usi_v4usi\n+      = build_function_type_list (V4USI_type_node, V4USI_type_node,\n+                                  V4USI_type_node, V4USI_type_node, NULL_TREE);\n+\n+    tree uti_ftype_udi_udi\n+      = build_function_type_list (neon_intUTI_type_node, intUDI_type_node,\n+                                  intUDI_type_node, NULL_TREE);\n+\n+    #undef CRYPTO1\n+    #undef CRYPTO2\n+    #undef CRYPTO3\n+    #undef C\n+    #undef N\n+    #undef CF\n+    #undef FT1\n+    #undef FT2\n+    #undef FT3\n+\n+    #define C(U) \\\n+      ARM_BUILTIN_CRYPTO_##U\n+    #define N(L) \\\n+      \"__builtin_arm_crypto_\"#L\n+    #define FT1(R, A) \\\n+      R##_ftype_##A\n+    #define FT2(R, A1, A2) \\\n+      R##_ftype_##A1##_##A2\n+    #define FT3(R, A1, A2, A3) \\\n+      R##_ftype_##A1##_##A2##_##A3\n+    #define CRYPTO1(L, U, R, A) \\\n+      arm_builtin_decls[C (U)] = add_builtin_function (N (L), FT1 (R, A), \\\n+                                                       C (U), BUILT_IN_MD, \\\n+                                                       NULL, NULL_TREE);\n+    #define CRYPTO2(L, U, R, A1, A2) \\\n+      arm_builtin_decls[C (U)] = add_builtin_function (N (L), FT2 (R, A1, A2), \\\n+                                                       C (U), BUILT_IN_MD, \\\n+                                                       NULL, NULL_TREE);\n+\n+    #define CRYPTO3(L, U, R, A1, A2, A3) \\\n+      arm_builtin_decls[C (U)] = add_builtin_function (N (L), FT3 (R, A1, A2, A3), \\\n+                                                       C (U), BUILT_IN_MD, \\\n+                                                       NULL, NULL_TREE);\n+    #include \"crypto.def\"\n+\n+    #undef CRYPTO1\n+    #undef CRYPTO2\n+    #undef CRYPTO3\n+    #undef C\n+    #undef N\n+    #undef FT1\n+    #undef FT2\n+    #undef FT3\n+  }\n   dreg_types[0] = V8QI_type_node;\n   dreg_types[1] = V4HI_type_node;\n   dreg_types[2] = V2SI_type_node;\n@@ -23420,14 +23521,17 @@ arm_init_neon_builtins (void)\n   qreg_types[2] = V4SI_type_node;\n   qreg_types[3] = V4SF_type_node;\n   qreg_types[4] = V2DI_type_node;\n+  qreg_types[5] = neon_intUTI_type_node;\n \n-  for (i = 0; i < 5; i++)\n+  for (i = 0; i < NUM_QREG_TYPES; i++)\n     {\n       int j;\n-      for (j = 0; j < 5; j++)\n+      for (j = 0; j < NUM_QREG_TYPES; j++)\n         {\n-          reinterp_ftype_dreg[i][j]\n-            = build_function_type_list (dreg_types[i], dreg_types[j], NULL);\n+          if (i < NUM_DREG_TYPES && j < NUM_DREG_TYPES)\n+            reinterp_ftype_dreg[i][j]\n+              = build_function_type_list (dreg_types[i], dreg_types[j], NULL);\n+\n           reinterp_ftype_qreg[i][j]\n             = build_function_type_list (qreg_types[i], qreg_types[j], NULL);\n         }\n@@ -23642,10 +23746,14 @@ arm_init_neon_builtins (void)\n \n \tcase NEON_REINTERP:\n \t  {\n-\t    /* We iterate over 5 doubleword types, then 5 quadword\n-\t       types. V4HF is not a type used in reinterpret, so we translate\n+\t    /* We iterate over NUM_DREG_TYPES doubleword types,\n+\t       then NUM_QREG_TYPES quadword  types.\n+\t       V4HF is not a type used in reinterpret, so we translate\n \t       d->mode to the correct index in reinterp_ftype_dreg.  */\n-\t    int rhs = (d->mode - ((d->mode > T_V4HF) ? 1 : 0)) % 5;\n+\t    bool qreg_p\n+\t      = GET_MODE_SIZE (insn_data[d->code].operand[0].mode) > 8;\n+\t    int rhs = (d->mode - ((!qreg_p && (d->mode > T_V4HF)) ? 1 : 0))\n+\t              % NUM_QREG_TYPES;\n \t    switch (insn_data[d->code].operand[0].mode)\n \t      {\n \t      case V8QImode: ftype = reinterp_ftype_dreg[0][rhs]; break;\n@@ -23658,6 +23766,7 @@ arm_init_neon_builtins (void)\n \t      case V4SImode: ftype = reinterp_ftype_qreg[2][rhs]; break;\n \t      case V4SFmode: ftype = reinterp_ftype_qreg[3][rhs]; break;\n \t      case V2DImode: ftype = reinterp_ftype_qreg[4][rhs]; break;\n+\t      case TImode: ftype = reinterp_ftype_qreg[5][rhs]; break;\n \t      default: gcc_unreachable ();\n \t      }\n \t  }\n@@ -23708,6 +23817,9 @@ arm_init_neon_builtins (void)\n     }\n }\n \n+#undef NUM_DREG_TYPES\n+#undef NUM_QREG_TYPES\n+\n #define def_mbuiltin(MASK, NAME, TYPE, CODE)\t\t\t\t\\\n   do\t\t\t\t\t\t\t\t\t\\\n     {\t\t\t\t\t\t\t\t\t\\\n@@ -23847,6 +23959,22 @@ static const struct builtin_description bdesc_2arg[] =\n    CRC32_BUILTIN (crc32ch, CRC32CH)\n    CRC32_BUILTIN (crc32cw, CRC32CW)\n #undef CRC32_BUILTIN\n+\n+\n+#define CRYPTO_BUILTIN(L, U) \\\n+  {0, CODE_FOR_crypto_##L, \"__builtin_arm_crypto_\"#L, ARM_BUILTIN_CRYPTO_##U, \\\n+   UNKNOWN, 0},\n+#undef CRYPTO1\n+#undef CRYPTO2\n+#undef CRYPTO3\n+#define CRYPTO2(L, U, R, A1, A2) CRYPTO_BUILTIN (L, U)\n+#define CRYPTO1(L, U, R, A)\n+#define CRYPTO3(L, U, R, A1, A2, A3)\n+#include \"crypto.def\"\n+#undef CRYPTO1\n+#undef CRYPTO2\n+#undef CRYPTO3\n+\n };\n \n static const struct builtin_description bdesc_1arg[] =\n@@ -23875,8 +24003,28 @@ static const struct builtin_description bdesc_1arg[] =\n   IWMMXT_BUILTIN (tbcstv8qi, \"tbcstb\", TBCSTB)\n   IWMMXT_BUILTIN (tbcstv4hi, \"tbcsth\", TBCSTH)\n   IWMMXT_BUILTIN (tbcstv2si, \"tbcstw\", TBCSTW)\n+\n+#define CRYPTO1(L, U, R, A) CRYPTO_BUILTIN (L, U)\n+#define CRYPTO2(L, U, R, A1, A2)\n+#define CRYPTO3(L, U, R, A1, A2, A3)\n+#include \"crypto.def\"\n+#undef CRYPTO1\n+#undef CRYPTO2\n+#undef CRYPTO3\n };\n \n+static const struct builtin_description bdesc_3arg[] =\n+{\n+#define CRYPTO3(L, U, R, A1, A2, A3) CRYPTO_BUILTIN (L, U)\n+#define CRYPTO1(L, U, R, A)\n+#define CRYPTO2(L, U, R, A1, A2)\n+#include \"crypto.def\"\n+#undef CRYPTO1\n+#undef CRYPTO2\n+#undef CRYPTO3\n+ };\n+#undef CRYPTO_BUILTIN\n+\n /* Set up all the iWMMXt builtins.  This is not called if\n    TARGET_IWMMXT is zero.  */\n \n@@ -24408,6 +24556,73 @@ safe_vector_operand (rtx x, enum machine_mode mode)\n   return x;\n }\n \n+/* Function to expand ternary builtins.  */\n+static rtx\n+arm_expand_ternop_builtin (enum insn_code icode,\n+                           tree exp, rtx target)\n+{\n+  rtx pat;\n+  tree arg0 = CALL_EXPR_ARG (exp, 0);\n+  tree arg1 = CALL_EXPR_ARG (exp, 1);\n+  tree arg2 = CALL_EXPR_ARG (exp, 2);\n+\n+  rtx op0 = expand_normal (arg0);\n+  rtx op1 = expand_normal (arg1);\n+  rtx op2 = expand_normal (arg2);\n+  rtx op3 = NULL_RTX;\n+\n+  /* The sha1c, sha1p, sha1m crypto builtins require a different vec_select\n+     lane operand depending on endianness.  */\n+  bool builtin_sha1cpm_p = false;\n+\n+  if (insn_data[icode].n_operands == 5)\n+    {\n+      gcc_assert (icode == CODE_FOR_crypto_sha1c\n+                  || icode == CODE_FOR_crypto_sha1p\n+                  || icode == CODE_FOR_crypto_sha1m);\n+      builtin_sha1cpm_p = true;\n+    }\n+  enum machine_mode tmode = insn_data[icode].operand[0].mode;\n+  enum machine_mode mode0 = insn_data[icode].operand[1].mode;\n+  enum machine_mode mode1 = insn_data[icode].operand[2].mode;\n+  enum machine_mode mode2 = insn_data[icode].operand[3].mode;\n+\n+\n+  if (VECTOR_MODE_P (mode0))\n+    op0 = safe_vector_operand (op0, mode0);\n+  if (VECTOR_MODE_P (mode1))\n+    op1 = safe_vector_operand (op1, mode1);\n+  if (VECTOR_MODE_P (mode2))\n+    op2 = safe_vector_operand (op2, mode2);\n+\n+  if (! target\n+      || GET_MODE (target) != tmode\n+      || ! (*insn_data[icode].operand[0].predicate) (target, tmode))\n+    target = gen_reg_rtx (tmode);\n+\n+  gcc_assert ((GET_MODE (op0) == mode0 || GET_MODE (op0) == VOIDmode)\n+\t      && (GET_MODE (op1) == mode1 || GET_MODE (op1) == VOIDmode)\n+\t      && (GET_MODE (op2) == mode2 || GET_MODE (op2) == VOIDmode));\n+\n+  if (! (*insn_data[icode].operand[1].predicate) (op0, mode0))\n+    op0 = copy_to_mode_reg (mode0, op0);\n+  if (! (*insn_data[icode].operand[2].predicate) (op1, mode1))\n+    op1 = copy_to_mode_reg (mode1, op1);\n+  if (! (*insn_data[icode].operand[3].predicate) (op2, mode2))\n+    op2 = copy_to_mode_reg (mode2, op2);\n+  if (builtin_sha1cpm_p)\n+    op3 = GEN_INT (TARGET_BIG_END ? 1 : 0);\n+\n+  if (builtin_sha1cpm_p)\n+    pat = GEN_FCN (icode) (target, op0, op1, op2, op3);\n+  else\n+    pat = GEN_FCN (icode) (target, op0, op1, op2);\n+  if (! pat)\n+    return 0;\n+  emit_insn (pat);\n+  return target;\n+}\n+\n /* Subroutine of arm_expand_builtin to take care of binop insns.  */\n \n static rtx\n@@ -24457,8 +24672,16 @@ arm_expand_unop_builtin (enum insn_code icode,\n   rtx pat;\n   tree arg0 = CALL_EXPR_ARG (exp, 0);\n   rtx op0 = expand_normal (arg0);\n+  rtx op1 = NULL_RTX;\n   enum machine_mode tmode = insn_data[icode].operand[0].mode;\n   enum machine_mode mode0 = insn_data[icode].operand[1].mode;\n+  bool builtin_sha1h_p = false;\n+\n+  if (insn_data[icode].n_operands == 3)\n+    {\n+      gcc_assert (icode == CODE_FOR_crypto_sha1h);\n+      builtin_sha1h_p = true;\n+    }\n \n   if (! target\n       || GET_MODE (target) != tmode\n@@ -24474,8 +24697,13 @@ arm_expand_unop_builtin (enum insn_code icode,\n       if (! (*insn_data[icode].operand[1].predicate) (op0, mode0))\n \top0 = copy_to_mode_reg (mode0, op0);\n     }\n+  if (builtin_sha1h_p)\n+    op1 = GEN_INT (TARGET_BIG_END ? 1 : 0);\n \n-  pat = GEN_FCN (icode) (target, op0);\n+  if (builtin_sha1h_p)\n+    pat = GEN_FCN (icode) (target, op0, op1);\n+  else\n+    pat = GEN_FCN (icode) (target, op0);\n   if (! pat)\n     return 0;\n   emit_insn (pat);\n@@ -25433,6 +25661,10 @@ arm_expand_builtin (tree exp,\n     if (d->code == (const enum arm_builtins) fcode)\n       return arm_expand_unop_builtin (d->icode, exp, target, 0);\n \n+  for (i = 0, d = bdesc_3arg; i < ARRAY_SIZE (bdesc_3arg); i++, d++)\n+    if (d->code == (const enum arm_builtins) fcode)\n+      return arm_expand_ternop_builtin (d->icode, exp, target);\n+\n   /* @@@ Should really do something sensible here.  */\n   return NULL_RTX;\n }\n@@ -29096,6 +29328,7 @@ static arm_mangle_map_entry arm_mangle_map[] = {\n   { V2SFmode,  \"__builtin_neon_sf\",     \"18__simd64_float32_t\" },\n   { V8QImode,  \"__builtin_neon_poly8\",  \"16__simd64_poly8_t\" },\n   { V4HImode,  \"__builtin_neon_poly16\", \"17__simd64_poly16_t\" },\n+\n   /* 128-bit containerized types.  */\n   { V16QImode, \"__builtin_neon_qi\",     \"16__simd128_int8_t\" },\n   { V16QImode, \"__builtin_neon_uqi\",    \"17__simd128_uint8_t\" },"}, {"sha": "fb5ce1c19b03eed7b6d791e976a261ceb0bd1a30", "filename": "gcc/config/arm/arm.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Farm.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Farm.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm.h?ref=021b5e6b54aecf36d3b105368485682e5d06782a", "patch": "@@ -49,6 +49,8 @@ extern char arm_arch_name[];\n            builtin_define (\"__ARM_FEATURE_QBIT\");\t\\\n         if (TARGET_ARM_SAT)\t\t\t\t\\\n            builtin_define (\"__ARM_FEATURE_SAT\");\t\\\n+        if (TARGET_CRYPTO)\t\t\t\t\\\n+\t   builtin_define (\"__ARM_FEATURE_CRYPTO\");\t\\\n \tif (unaligned_access)\t\t\t\t\\\n \t  builtin_define (\"__ARM_FEATURE_UNALIGNED\");\t\\\n \tif (TARGET_CRC32)\t\t\t\t\\"}, {"sha": "9e114b49d2f62f60b0665fd52e33f88e7a16e39d", "filename": "gcc/config/arm/arm.md", "status": "modified", "additions": 6, "deletions": 2, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Farm.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Farm.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm.md?ref=021b5e6b54aecf36d3b105368485682e5d06782a", "patch": "@@ -293,7 +293,7 @@\n           neon_ext, neon_ext_q, neon_rbit, neon_rbit_q,\\\n           neon_rev, neon_rev_q, neon_mul_b, neon_mul_b_q, neon_mul_h,\\\n           neon_mul_h_q, neon_mul_s, neon_mul_s_q, neon_mul_b_long,\\\n-          neon_mul_h_long, neon_mul_s_long, neon_mul_h_scalar,\\\n+          neon_mul_h_long, neon_mul_s_long, neon_mul_d_long, neon_mul_h_scalar,\\\n           neon_mul_h_scalar_q, neon_mul_s_scalar, neon_mul_s_scalar_q,\\\n           neon_mul_h_scalar_long, neon_mul_s_scalar_long, neon_sat_mul_b,\\\n           neon_sat_mul_b_q, neon_sat_mul_h, neon_sat_mul_h_q,\\\n@@ -355,7 +355,9 @@\n           neon_fp_mla_s_scalar, neon_fp_mla_s_scalar_q, neon_fp_mla_d,\\\n           neon_fp_mla_d_q, neon_fp_mla_d_scalar_q, neon_fp_sqrt_s,\\\n           neon_fp_sqrt_s_q, neon_fp_sqrt_d, neon_fp_sqrt_d_q,\\\n-          neon_fp_div_s, neon_fp_div_s_q, neon_fp_div_d, neon_fp_div_d_q\")\n+          neon_fp_div_s, neon_fp_div_s_q, neon_fp_div_d, neon_fp_div_d_q, crypto_aes,\\\n+          crypto_sha1_xor, crypto_sha1_fast, crypto_sha1_slow, crypto_sha256_fast,\\\n+          crypto_sha256_slow\")\n         (const_string \"yes\")\n         (const_string \"no\")))\n \n@@ -12918,6 +12920,8 @@\n (include \"thumb2.md\")\n ;; Neon patterns\n (include \"neon.md\")\n+;; Crypto patterns\n+(include \"crypto.md\")\n ;; Synchronization Primitives\n (include \"sync.md\")\n ;; Fixed-point patterns"}, {"sha": "59ef22c530d90a4a98f8667bcc136ae43f0bf44b", "filename": "gcc/config/arm/arm_neon.h", "status": "modified", "additions": 1642, "deletions": 607, "changes": 2249, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm_neon.h?ref=021b5e6b54aecf36d3b105368485682e5d06782a"}, {"sha": "5fa17bd0bf1e1a365ca1d6d5a4fbc7bdedb6c18f", "filename": "gcc/config/arm/arm_neon_builtins.def", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Farm_neon_builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Farm_neon_builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm_neon_builtins.def?ref=021b5e6b54aecf36d3b105368485682e5d06782a", "patch": "@@ -158,11 +158,12 @@ VAR5 (REINTERP, vreinterpretv4hi, v8qi, v4hi, v2si, v2sf, di),\n VAR5 (REINTERP, vreinterpretv2si, v8qi, v4hi, v2si, v2sf, di),\n VAR5 (REINTERP, vreinterpretv2sf, v8qi, v4hi, v2si, v2sf, di),\n VAR5 (REINTERP, vreinterpretdi, v8qi, v4hi, v2si, v2sf, di),\n-VAR5 (REINTERP, vreinterpretv16qi, v16qi, v8hi, v4si, v4sf, v2di),\n-VAR5 (REINTERP, vreinterpretv8hi, v16qi, v8hi, v4si, v4sf, v2di),\n-VAR5 (REINTERP, vreinterpretv4si, v16qi, v8hi, v4si, v4sf, v2di),\n-VAR5 (REINTERP, vreinterpretv4sf, v16qi, v8hi, v4si, v4sf, v2di),\n-VAR5 (REINTERP, vreinterpretv2di, v16qi, v8hi, v4si, v4sf, v2di),\n+VAR6 (REINTERP, vreinterpretv16qi, v16qi, v8hi, v4si, v4sf, v2di, ti),\n+VAR6 (REINTERP, vreinterpretv8hi, v16qi, v8hi, v4si, v4sf, v2di, ti),\n+VAR6 (REINTERP, vreinterpretv4si, v16qi, v8hi, v4si, v4sf, v2di, ti),\n+VAR6 (REINTERP, vreinterpretv4sf, v16qi, v8hi, v4si, v4sf, v2di, ti),\n+VAR6 (REINTERP, vreinterpretv2di, v16qi, v8hi, v4si, v4sf, v2di, ti),\n+VAR6 (REINTERP, vreinterpretti, v16qi, v8hi, v4si, v4sf, v2di, ti),\n VAR10 (LOAD1, vld1,\n          v8qi, v4hi, v2si, v2sf, di, v16qi, v8hi, v4si, v4sf, v2di),\n VAR10 (LOAD1LANE, vld1_lane,"}, {"sha": "0b3a395b452029be035a9187559b897c48e3ee9d", "filename": "gcc/config/arm/crypto.def", "status": "added", "additions": 35, "deletions": 0, "changes": 35, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Fcrypto.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Fcrypto.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Fcrypto.def?ref=021b5e6b54aecf36d3b105368485682e5d06782a", "patch": "@@ -0,0 +1,35 @@\n+/* Cryptographic instruction builtin definitions.\n+   Copyright (C) 2013\n+   Free Software Foundation, Inc.\n+   Contributed by ARM Ltd.\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published\n+   by the Free Software Foundation; either version 3, or (at your\n+   option) any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but WITHOUT\n+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY\n+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public\n+   License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+CRYPTO2 (aesd, AESD, v16uqi, v16uqi, v16uqi)\n+CRYPTO2 (aese, AESE, v16uqi, v16uqi, v16uqi)\n+CRYPTO1 (aesimc, AESIMC, v16uqi, v16uqi)\n+CRYPTO1 (aesmc, AESMC, v16uqi, v16uqi)\n+CRYPTO1 (sha1h, SHA1H, v4usi, v4usi)\n+CRYPTO2 (sha1su1, SHA1SU1, v4usi, v4usi, v4usi)\n+CRYPTO2 (sha256su0, SHA256SU0, v4usi, v4usi, v4usi)\n+CRYPTO3 (sha1c, SHA1C, v4usi, v4usi, v4usi, v4usi)\n+CRYPTO3 (sha1m, SHA1M, v4usi, v4usi, v4usi, v4usi)\n+CRYPTO3 (sha1p, SHA1P, v4usi, v4usi, v4usi, v4usi)\n+CRYPTO3 (sha1su0, SHA1SU0, v4usi, v4usi, v4usi, v4usi)\n+CRYPTO3 (sha256h, SHA256H, v4usi, v4usi, v4usi, v4usi)\n+CRYPTO3 (sha256h2, SHA256H2, v4usi, v4usi, v4usi, v4usi)\n+CRYPTO3 (sha256su1, SHA256SU1, v4usi, v4usi, v4usi, v4usi)\n+CRYPTO2 (vmullp64, VMULLP64, uti, udi, udi)"}, {"sha": "e365cf85281e25f4d9e1cbd2cf277f967fefe076", "filename": "gcc/config/arm/crypto.md", "status": "added", "additions": 86, "deletions": 0, "changes": 86, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Fcrypto.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Fcrypto.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Fcrypto.md?ref=021b5e6b54aecf36d3b105368485682e5d06782a", "patch": "@@ -0,0 +1,86 @@\n+;; ARMv8-A crypto patterns.\n+;; Copyright (C) 2013 Free Software Foundation, Inc.\n+;; Contributed by ARM Ltd.\n+\n+;; This file is part of GCC.\n+\n+;; GCC is free software; you can redistribute it and/or modify it\n+;; under the terms of the GNU General Public License as published\n+;; by the Free Software Foundation; either version 3, or (at your\n+;; option) any later version.\n+\n+;; GCC is distributed in the hope that it will be useful, but WITHOUT\n+;; ANY WARRANTY; without even the implied warranty of MERCHANTABILITY\n+;; or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public\n+;; License for more details.\n+\n+;; You should have received a copy of the GNU General Public License\n+;; along with GCC; see the file COPYING3.  If not see\n+;; <http://www.gnu.org/licenses/>.\n+\n+(define_insn \"crypto_<crypto_pattern>\"\n+  [(set (match_operand:<crypto_mode> 0 \"register_operand\" \"=w\")\n+        (unspec:<crypto_mode> [(match_operand:<crypto_mode> 1\n+                       \"register_operand\" \"w\")]\n+         CRYPTO_UNARY))]\n+  \"TARGET_CRYPTO\"\n+  \"<crypto_pattern>.<crypto_size_sfx>\\\\t%q0, %q1\"\n+  [(set_attr \"type\" \"<crypto_type>\")]\n+)\n+\n+(define_insn \"crypto_<crypto_pattern>\"\n+  [(set (match_operand:<crypto_mode> 0 \"register_operand\" \"=w\")\n+        (unspec:<crypto_mode> [(match_operand:<crypto_mode> 1 \"register_operand\" \"0\")\n+                      (match_operand:<crypto_mode> 2 \"register_operand\" \"w\")]\n+         CRYPTO_BINARY))]\n+  \"TARGET_CRYPTO\"\n+  \"<crypto_pattern>.<crypto_size_sfx>\\\\t%q0, %q2\"\n+  [(set_attr \"type\" \"<crypto_type>\")]\n+)\n+\n+(define_insn \"crypto_<crypto_pattern>\"\n+  [(set (match_operand:<crypto_mode> 0 \"register_operand\" \"=w\")\n+        (unspec:<crypto_mode> [(match_operand:<crypto_mode> 1 \"register_operand\" \"0\")\n+                      (match_operand:<crypto_mode> 2 \"register_operand\" \"w\")\n+                      (match_operand:<crypto_mode> 3 \"register_operand\" \"w\")]\n+         CRYPTO_TERNARY))]\n+  \"TARGET_CRYPTO\"\n+  \"<crypto_pattern>.<crypto_size_sfx>\\\\t%q0, %q2, %q3\"\n+  [(set_attr \"type\" \"<crypto_type>\")]\n+)\n+\n+(define_insn \"crypto_sha1h\"\n+  [(set (match_operand:V4SI 0 \"register_operand\" \"=w\")\n+        (zero_extend:V4SI\n+          (unspec:SI [(vec_select:SI\n+                        (match_operand:V4SI 1 \"register_operand\" \"w\")\n+                        (parallel [(match_operand:SI 2 \"immediate_operand\" \"i\")]))]\n+           UNSPEC_SHA1H)))]\n+  \"TARGET_CRYPTO\"\n+  \"sha1h.32\\\\t%q0, %q1\"\n+  [(set_attr \"type\" \"crypto_sha1_fast\")]\n+)\n+\n+(define_insn \"crypto_vmullp64\"\n+  [(set (match_operand:TI 0 \"register_operand\" \"=w\")\n+        (unspec:TI [(match_operand:DI 1 \"register_operand\" \"w\")\n+                    (match_operand:DI 2 \"register_operand\" \"w\")]\n+         UNSPEC_VMULLP64))]\n+  \"TARGET_CRYPTO\"\n+  \"vmull.p64\\\\t%q0, %P1, %P2\"\n+  [(set_attr \"type\" \"neon_mul_d_long\")]\n+)\n+\n+(define_insn \"crypto_<crypto_pattern>\"\n+  [(set (match_operand:V4SI 0 \"register_operand\" \"=w\")\n+        (unspec:<crypto_mode>\n+                     [(match_operand:<crypto_mode> 1 \"register_operand\" \"0\")\n+                      (vec_select:SI\n+                        (match_operand:<crypto_mode> 2 \"register_operand\" \"w\")\n+                        (parallel [(match_operand:SI 4 \"immediate_operand\" \"i\")]))\n+                      (match_operand:<crypto_mode> 3 \"register_operand\" \"w\")]\n+         CRYPTO_SELECTING))]\n+  \"TARGET_CRYPTO\"\n+  \"<crypto_pattern>.<crypto_size_sfx>\\\\t%q0, %q2, %q3\"\n+  [(set_attr \"type\" \"<crypto_type>\")]\n+)"}, {"sha": "299c9594532358d3a7dcdc816b2230f41b5f351d", "filename": "gcc/config/arm/iterators.md", "status": "modified", "additions": 45, "deletions": 0, "changes": 45, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Fiterators.md?ref=021b5e6b54aecf36d3b105368485682e5d06782a", "patch": "@@ -204,6 +204,17 @@\n (define_int_iterator CRC [UNSPEC_CRC32B UNSPEC_CRC32H UNSPEC_CRC32W\n                           UNSPEC_CRC32CB UNSPEC_CRC32CH UNSPEC_CRC32CW])\n \n+(define_int_iterator CRYPTO_UNARY [UNSPEC_AESMC UNSPEC_AESIMC])\n+\n+(define_int_iterator CRYPTO_BINARY [UNSPEC_AESD UNSPEC_AESE\n+                                    UNSPEC_SHA1SU1 UNSPEC_SHA256SU0])\n+\n+(define_int_iterator CRYPTO_TERNARY [UNSPEC_SHA1SU0 UNSPEC_SHA256H\n+                                     UNSPEC_SHA256H2 UNSPEC_SHA256SU1])\n+\n+(define_int_iterator CRYPTO_SELECTING [UNSPEC_SHA1C UNSPEC_SHA1M\n+                                       UNSPEC_SHA1P])\n+\n ;;----------------------------------------------------------------------------\n ;; Mode attributes\n ;;----------------------------------------------------------------------------\n@@ -530,6 +541,40 @@\n                         (UNSPEC_CRC32W \"SI\") (UNSPEC_CRC32CB \"QI\")\n                         (UNSPEC_CRC32CH \"HI\") (UNSPEC_CRC32CW \"SI\")])\n \n+(define_int_attr crypto_pattern [(UNSPEC_SHA1H \"sha1h\") (UNSPEC_AESMC \"aesmc\")\n+                          (UNSPEC_AESIMC \"aesimc\") (UNSPEC_AESD \"aesd\")\n+                          (UNSPEC_AESE \"aese\") (UNSPEC_SHA1SU1 \"sha1su1\")\n+                          (UNSPEC_SHA256SU0 \"sha256su0\") (UNSPEC_SHA1C \"sha1c\")\n+                          (UNSPEC_SHA1M \"sha1m\") (UNSPEC_SHA1P \"sha1p\")\n+                          (UNSPEC_SHA1SU0 \"sha1su0\") (UNSPEC_SHA256H \"sha256h\")\n+                          (UNSPEC_SHA256H2 \"sha256h2\")\n+                          (UNSPEC_SHA256SU1 \"sha256su1\")])\n+\n+(define_int_attr crypto_type\n+ [(UNSPEC_AESE \"crypto_aes\") (UNSPEC_AESD \"crypto_aes\")\n+ (UNSPEC_AESMC \"crypto_aes\") (UNSPEC_AESIMC \"crypto_aes\")\n+ (UNSPEC_SHA1C \"crypto_sha1_slow\") (UNSPEC_SHA1P \"crypto_sha1_slow\")\n+ (UNSPEC_SHA1M \"crypto_sha1_slow\") (UNSPEC_SHA1SU1 \"crypto_sha1_fast\")\n+ (UNSPEC_SHA1SU0 \"crypto_sha1_xor\") (UNSPEC_SHA256H \"crypto_sha256_slow\")\n+ (UNSPEC_SHA256H2 \"crypto_sha256_slow\") (UNSPEC_SHA256SU0 \"crypto_sha256_fast\")\n+ (UNSPEC_SHA256SU1 \"crypto_sha256_slow\")])\n+\n+(define_int_attr crypto_size_sfx [(UNSPEC_SHA1H \"32\") (UNSPEC_AESMC \"8\")\n+                          (UNSPEC_AESIMC \"8\") (UNSPEC_AESD \"8\")\n+                          (UNSPEC_AESE \"8\") (UNSPEC_SHA1SU1 \"32\")\n+                          (UNSPEC_SHA256SU0 \"32\") (UNSPEC_SHA1C \"32\")\n+                          (UNSPEC_SHA1M \"32\") (UNSPEC_SHA1P \"32\")\n+                          (UNSPEC_SHA1SU0 \"32\") (UNSPEC_SHA256H \"32\")\n+                          (UNSPEC_SHA256H2 \"32\") (UNSPEC_SHA256SU1 \"32\")])\n+\n+(define_int_attr crypto_mode [(UNSPEC_SHA1H \"V4SI\") (UNSPEC_AESMC \"V16QI\")\n+                          (UNSPEC_AESIMC \"V16QI\") (UNSPEC_AESD \"V16QI\")\n+                          (UNSPEC_AESE \"V16QI\") (UNSPEC_SHA1SU1 \"V4SI\")\n+                          (UNSPEC_SHA256SU0 \"V4SI\") (UNSPEC_SHA1C \"V4SI\")\n+                          (UNSPEC_SHA1M \"V4SI\") (UNSPEC_SHA1P \"V4SI\")\n+                          (UNSPEC_SHA1SU0 \"V4SI\") (UNSPEC_SHA256H \"V4SI\")\n+                          (UNSPEC_SHA256H2 \"V4SI\") (UNSPEC_SHA256SU1 \"V4SI\")])\n+\n ;; Both kinds of return insn.\n (define_code_iterator returns [return simple_return])\n (define_code_attr return_str [(return \"\") (simple_return \"simple_\")])"}, {"sha": "e5da658687f76ff50eeb09611f831b65f385be3b", "filename": "gcc/config/arm/neon-gen.ml", "status": "modified", "additions": 60, "deletions": 39, "changes": 99, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Fneon-gen.ml", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Fneon-gen.ml", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Fneon-gen.ml?ref=021b5e6b54aecf36d3b105368485682e5d06782a", "patch": "@@ -114,6 +114,7 @@ let rec signed_ctype = function\n   | T_uint32x4 -> T_int32x4\n   | T_uint64x1 -> T_int64x1\n   | T_uint64x2 -> T_int64x2\n+  | T_poly64x2 -> T_int64x2\n   (* Cast to types defined by mode in arm.c, not random types pulled in from\n      the <stdint.h> header in use. This fixes incompatible pointer errors when\n      compiling with C++.  *)\n@@ -125,6 +126,8 @@ let rec signed_ctype = function\n   | T_float32 -> T_floatSF\n   | T_poly8 -> T_intQI\n   | T_poly16 -> T_intHI\n+  | T_poly64 -> T_intDI\n+  | T_poly128 -> T_intTI\n   | T_arrayof (n, elt) -> T_arrayof (n, signed_ctype elt)\n   | T_ptrto elt -> T_ptrto (signed_ctype elt)\n   | T_const elt -> T_const (signed_ctype elt)\n@@ -362,80 +365,96 @@ let print_ops ops =\n      abase : \"ARM\" base name for the type (i.e. int in int8x8_t).\n      esize : element size.\n      enum : element count.\n+     alevel: architecture level at which available.\n *)\n \n+type fpulevel = CRYPTO | ALL\n+\n let deftypes () =\n   let typeinfo = [\n     (* Doubleword vector types.  *)\n-    \"__builtin_neon_qi\", \"int\", 8, 8;\n-    \"__builtin_neon_hi\", \"int\", 16, 4;\n-    \"__builtin_neon_si\", \"int\", 32, 2;\n-    \"__builtin_neon_di\", \"int\", 64, 1;\n-    \"__builtin_neon_hf\", \"float\", 16, 4;\n-    \"__builtin_neon_sf\", \"float\", 32, 2;\n-    \"__builtin_neon_poly8\", \"poly\", 8, 8;\n-    \"__builtin_neon_poly16\", \"poly\", 16, 4;\n-    \"__builtin_neon_uqi\", \"uint\", 8, 8;\n-    \"__builtin_neon_uhi\", \"uint\", 16, 4;\n-    \"__builtin_neon_usi\", \"uint\", 32, 2;\n-    \"__builtin_neon_udi\", \"uint\", 64, 1;\n+    \"__builtin_neon_qi\", \"int\", 8, 8, ALL;\n+    \"__builtin_neon_hi\", \"int\", 16, 4, ALL;\n+    \"__builtin_neon_si\", \"int\", 32, 2, ALL;\n+    \"__builtin_neon_di\", \"int\", 64, 1, ALL;\n+    \"__builtin_neon_hf\", \"float\", 16, 4, ALL;\n+    \"__builtin_neon_sf\", \"float\", 32, 2, ALL;\n+    \"__builtin_neon_poly8\", \"poly\", 8, 8, ALL;\n+    \"__builtin_neon_poly16\", \"poly\", 16, 4, ALL;\n+    \"__builtin_neon_poly64\", \"poly\", 64, 1, CRYPTO;\n+    \"__builtin_neon_uqi\", \"uint\", 8, 8, ALL;\n+    \"__builtin_neon_uhi\", \"uint\", 16, 4, ALL;\n+    \"__builtin_neon_usi\", \"uint\", 32, 2, ALL;\n+    \"__builtin_neon_udi\", \"uint\", 64, 1, ALL;\n \n     (* Quadword vector types.  *)\n-    \"__builtin_neon_qi\", \"int\", 8, 16;\n-    \"__builtin_neon_hi\", \"int\", 16, 8;\n-    \"__builtin_neon_si\", \"int\", 32, 4;\n-    \"__builtin_neon_di\", \"int\", 64, 2;\n-    \"__builtin_neon_sf\", \"float\", 32, 4;\n-    \"__builtin_neon_poly8\", \"poly\", 8, 16;\n-    \"__builtin_neon_poly16\", \"poly\", 16, 8;\n-    \"__builtin_neon_uqi\", \"uint\", 8, 16;\n-    \"__builtin_neon_uhi\", \"uint\", 16, 8;\n-    \"__builtin_neon_usi\", \"uint\", 32, 4;\n-    \"__builtin_neon_udi\", \"uint\", 64, 2\n+    \"__builtin_neon_qi\", \"int\", 8, 16, ALL;\n+    \"__builtin_neon_hi\", \"int\", 16, 8, ALL;\n+    \"__builtin_neon_si\", \"int\", 32, 4, ALL;\n+    \"__builtin_neon_di\", \"int\", 64, 2, ALL;\n+    \"__builtin_neon_sf\", \"float\", 32, 4, ALL;\n+    \"__builtin_neon_poly8\", \"poly\", 8, 16, ALL;\n+    \"__builtin_neon_poly16\", \"poly\", 16, 8, ALL;\n+    \"__builtin_neon_poly64\", \"poly\", 64, 2, CRYPTO;\n+    \"__builtin_neon_uqi\", \"uint\", 8, 16, ALL;\n+    \"__builtin_neon_uhi\", \"uint\", 16, 8, ALL;\n+    \"__builtin_neon_usi\", \"uint\", 32, 4, ALL;\n+    \"__builtin_neon_udi\", \"uint\", 64, 2, ALL\n   ] in\n   List.iter\n-    (fun (cbase, abase, esize, enum) ->\n+    (fun (cbase, abase, esize, enum, fpulevel) ->\n       let attr =\n         match enum with\n           1 -> \"\"\n         | _ -> Printf.sprintf \"\\t__attribute__ ((__vector_size__ (%d)))\"\n                               (esize * enum / 8) in\n-      Format.printf \"typedef %s %s%dx%d_t%s;@\\n\" cbase abase esize enum attr)\n+      if fpulevel == CRYPTO then\n+        Format.printf \"#ifdef __ARM_FEATURE_CRYPTO\\n\";\n+      Format.printf \"typedef %s %s%dx%d_t%s;@\\n\" cbase abase esize enum attr;\n+      if fpulevel == CRYPTO then\n+        Format.printf \"#endif\\n\";)\n     typeinfo;\n   Format.print_newline ();\n   (* Extra types not in <stdint.h>.  *)\n   Format.printf \"typedef float float32_t;\\n\";\n   Format.printf \"typedef __builtin_neon_poly8 poly8_t;\\n\";\n-  Format.printf \"typedef __builtin_neon_poly16 poly16_t;\\n\"\n+  Format.printf \"typedef __builtin_neon_poly16 poly16_t;\\n\";\n+  Format.printf \"#ifdef __ARM_FEATURE_CRYPTO\\n\";\n+  Format.printf \"typedef __builtin_neon_poly64 poly64_t;\\n\";\n+  Format.printf \"typedef __builtin_neon_poly128 poly128_t;\\n\";\n+  Format.printf \"#endif\\n\"\n \n-(* Output structs containing arrays, for load & store instructions etc.  *)\n+(* Output structs containing arrays, for load & store instructions etc.\n+   poly128_t is deliberately not included here because it has no array types\n+   defined for it.  *)\n \n let arrtypes () =\n   let typeinfo = [\n-    \"int\", 8;    \"int\", 16;\n-    \"int\", 32;   \"int\", 64;\n-    \"uint\", 8;   \"uint\", 16;\n-    \"uint\", 32;  \"uint\", 64;\n-    \"float\", 32; \"poly\", 8;\n-    \"poly\", 16\n+    \"int\", 8, ALL;    \"int\", 16, ALL;\n+    \"int\", 32, ALL;   \"int\", 64, ALL;\n+    \"uint\", 8, ALL;   \"uint\", 16, ALL;\n+    \"uint\", 32, ALL;  \"uint\", 64, ALL;\n+    \"float\", 32, ALL; \"poly\", 8, ALL;\n+    \"poly\", 16, ALL; \"poly\", 64, CRYPTO\n   ] in\n-  let writestruct elname elsize regsize arrsize =\n+  let writestruct elname elsize regsize arrsize fpulevel =\n     let elnum = regsize / elsize in\n     let structname =\n       Printf.sprintf \"%s%dx%dx%d_t\" elname elsize elnum arrsize in\n     let sfmt = start_function () in\n-    Format.printf \"typedef struct %s\" structname;\n+    Format.printf \"%stypedef struct %s\"\n+      (if fpulevel == CRYPTO then \"#ifdef __ARM_FEATURE_CRYPTO\\n\" else \"\") structname;\n     open_braceblock sfmt;\n     Format.printf \"%s%dx%d_t val[%d];\" elname elsize elnum arrsize;\n     close_braceblock sfmt;\n-    Format.printf \" %s;\" structname;\n+    Format.printf \" %s;%s\" structname (if fpulevel == CRYPTO then \"\\n#endif\\n\" else \"\");\n     end_function sfmt;\n   in\n     for n = 2 to 4 do\n       List.iter\n-        (fun (elname, elsize) ->\n-          writestruct elname elsize 64 n;\n-          writestruct elname elsize 128 n)\n+        (fun (elname, elsize, alevel) ->\n+          writestruct elname elsize 64 n alevel;\n+          writestruct elname elsize 128 n alevel)\n         typeinfo\n     done\n \n@@ -491,6 +510,8 @@ let _ =\n   print_ops ops;\n   Format.print_newline ();\n   print_ops reinterp;\n+  print_ops reinterpq;\n+  Format.printf \"%s\" crypto_intrinsics;\n   print_lines [\n \"#ifdef __cplusplus\";\n \"}\";"}, {"sha": "5e9b4410662029dbef449038b3513ef73587faa5", "filename": "gcc/config/arm/neon.md", "status": "modified", "additions": 15, "deletions": 5, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Fneon.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Fneon.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Fneon.md?ref=021b5e6b54aecf36d3b105368485682e5d06782a", "patch": "@@ -4259,9 +4259,19 @@\n   DONE;\n })\n \n+(define_expand \"neon_vreinterpretti<mode>\"\n+  [(match_operand:TI 0 \"s_register_operand\" \"\")\n+   (match_operand:VQXMOV 1 \"s_register_operand\" \"\")]\n+  \"TARGET_NEON\"\n+{\n+  neon_reinterpret (operands[0], operands[1]);\n+  DONE;\n+})\n+\n+\n (define_expand \"neon_vreinterpretv16qi<mode>\"\n   [(match_operand:V16QI 0 \"s_register_operand\" \"\")\n-   (match_operand:VQX 1 \"s_register_operand\" \"\")]\n+   (match_operand:VQXMOV 1 \"s_register_operand\" \"\")]\n   \"TARGET_NEON\"\n {\n   neon_reinterpret (operands[0], operands[1]);\n@@ -4270,7 +4280,7 @@\n \n (define_expand \"neon_vreinterpretv8hi<mode>\"\n   [(match_operand:V8HI 0 \"s_register_operand\" \"\")\n-   (match_operand:VQX 1 \"s_register_operand\" \"\")]\n+   (match_operand:VQXMOV 1 \"s_register_operand\" \"\")]\n   \"TARGET_NEON\"\n {\n   neon_reinterpret (operands[0], operands[1]);\n@@ -4279,7 +4289,7 @@\n \n (define_expand \"neon_vreinterpretv4si<mode>\"\n   [(match_operand:V4SI 0 \"s_register_operand\" \"\")\n-   (match_operand:VQX 1 \"s_register_operand\" \"\")]\n+   (match_operand:VQXMOV 1 \"s_register_operand\" \"\")]\n   \"TARGET_NEON\"\n {\n   neon_reinterpret (operands[0], operands[1]);\n@@ -4288,7 +4298,7 @@\n \n (define_expand \"neon_vreinterpretv4sf<mode>\"\n   [(match_operand:V4SF 0 \"s_register_operand\" \"\")\n-   (match_operand:VQX 1 \"s_register_operand\" \"\")]\n+   (match_operand:VQXMOV 1 \"s_register_operand\" \"\")]\n   \"TARGET_NEON\"\n {\n   neon_reinterpret (operands[0], operands[1]);\n@@ -4297,7 +4307,7 @@\n \n (define_expand \"neon_vreinterpretv2di<mode>\"\n   [(match_operand:V2DI 0 \"s_register_operand\" \"\")\n-   (match_operand:VQX 1 \"s_register_operand\" \"\")]\n+   (match_operand:VQXMOV 1 \"s_register_operand\" \"\")]\n   \"TARGET_NEON\"\n {\n   neon_reinterpret (operands[0], operands[1]);"}, {"sha": "968c17121e731ef7dd3fa540f01ed1fbcde9c31c", "filename": "gcc/config/arm/neon.ml", "status": "modified", "additions": 358, "deletions": 31, "changes": 389, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Fneon.ml", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Fneon.ml", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Fneon.ml?ref=021b5e6b54aecf36d3b105368485682e5d06782a", "patch": "@@ -22,7 +22,7 @@\n \n (* Shorthand types for vector elements.  *)\n type elts = S8 | S16 | S32 | S64 | F16 | F32 | U8 | U16 | U32 | U64 | P8 | P16\n-          | I8 | I16 | I32 | I64 | B8 | B16 | B32 | B64 | Conv of elts * elts\n+          | P64 | P128 | I8 | I16 | I32 | I64 | B8 | B16 | B32 | B64 | Conv of elts * elts\n           | Cast of elts * elts | NoElts\n \n type eltclass = Signed | Unsigned | Float | Poly | Int | Bits\n@@ -47,13 +47,15 @@ type vectype = T_int8x8    | T_int8x16\n              | T_uint8     | T_uint16\n              | T_uint32    | T_uint64\n              | T_poly8     | T_poly16\n+             | T_poly64    | T_poly64x1\n+             | T_poly64x2  | T_poly128\n              | T_float16   | T_float32\n              | T_arrayof of int * vectype\n              | T_ptrto of vectype | T_const of vectype\n              | T_void      | T_intQI\n              | T_intHI     | T_intSI\n-             | T_intDI     | T_floatHF\n-             | T_floatSF\n+             | T_intDI     | T_intTI\n+             | T_floatHF   | T_floatSF\n \n (* The meanings of the following are:\n      TImode : \"Tetra\", two registers (four words).\n@@ -96,7 +98,7 @@ type arity = Arity0 of vectype\n            | Arity4 of vectype * vectype * vectype * vectype * vectype\n \n type vecmode = V8QI | V4HI | V4HF |V2SI | V2SF | DI\n-             | V16QI | V8HI | V4SI | V4SF | V2DI\n+             | V16QI | V8HI | V4SI | V4SF | V2DI | TI\n              | QI | HI | SI | SF\n \n type opcode =\n@@ -299,7 +301,8 @@ let rec elt_width = function\n     S8 | U8 | P8 | I8 | B8 -> 8\n   | S16 | U16 | P16 | I16 | B16 | F16 -> 16\n   | S32 | F32 | U32 | I32 | B32 -> 32\n-  | S64 | U64 | I64 | B64 -> 64\n+  | S64 | U64 | P64 | I64 | B64 -> 64\n+  | P128 -> 128\n   | Conv (a, b) ->\n       let wa = elt_width a and wb = elt_width b in\n       if wa = wb then wa else raise (MixedMode (a, b))\n@@ -309,7 +312,7 @@ let rec elt_width = function\n let rec elt_class = function\n     S8 | S16 | S32 | S64 -> Signed\n   | U8 | U16 | U32 | U64 -> Unsigned\n-  | P8 | P16 -> Poly\n+  | P8 | P16 | P64 | P128 -> Poly\n   | F16 | F32 -> Float\n   | I8 | I16 | I32 | I64 -> Int\n   | B8 | B16 | B32 | B64 -> Bits\n@@ -330,6 +333,8 @@ let elt_of_class_width c w =\n   | Unsigned, 64 -> U64\n   | Poly, 8 -> P8\n   | Poly, 16 -> P16\n+  | Poly, 64 -> P64\n+  | Poly, 128 -> P128\n   | Int, 8 -> I8\n   | Int, 16 -> I16\n   | Int, 32 -> I32\n@@ -402,7 +407,7 @@ let rec mode_of_elt ?argpos elt shape =\n     Float | ConvClass(_, Float) -> true | _ -> false in\n   let idx =\n     match elt_width elt with\n-      8 -> 0 | 16 -> 1 | 32 -> 2 | 64 -> 3\n+      8 -> 0 | 16 -> 1 | 32 -> 2 | 64 -> 3 | 128 -> 4\n     | _ -> failwith \"Bad element width\"\n   in match shape with\n     All (_, Dreg) | By_scalar Dreg | Pair_result Dreg | Unary_scalar Dreg\n@@ -413,7 +418,7 @@ let rec mode_of_elt ?argpos elt shape =\n         [| V8QI; V4HI; V2SI; DI |].(idx)\n   | All (_, Qreg) | By_scalar Qreg | Pair_result Qreg | Unary_scalar Qreg\n   | Binary_imm Qreg | Long_noreg Qreg | Wide_noreg Qreg ->\n-      [| V16QI; V8HI; if flt then V4SF else V4SI; V2DI |].(idx)\n+      [| V16QI; V8HI; if flt then V4SF else V4SI; V2DI; TI|].(idx)\n   | All (_, (Corereg | PtrTo _ | CstPtrTo _)) ->\n       [| QI; HI; if flt then SF else SI; DI |].(idx)\n   | Long | Wide | Wide_lane | Wide_scalar\n@@ -474,6 +479,8 @@ let type_for_elt shape elt no =\n         | U16 -> T_uint16x4\n         | U32 -> T_uint32x2\n         | U64 -> T_uint64x1\n+        | P64 -> T_poly64x1\n+        | P128 -> T_poly128\n         | F16 -> T_float16x4\n         | F32 -> T_float32x2\n         | P8 -> T_poly8x8\n@@ -493,6 +500,8 @@ let type_for_elt shape elt no =\n         | F32 -> T_float32x4\n         | P8 -> T_poly8x16\n         | P16 -> T_poly16x8\n+        | P64 -> T_poly64x2\n+        | P128 -> T_poly128\n         | _ -> failwith \"Bad elt type for Qreg\"\n         end\n     | Corereg ->\n@@ -507,6 +516,8 @@ let type_for_elt shape elt no =\n         | U64 -> T_uint64\n         | P8 -> T_poly8\n         | P16 -> T_poly16\n+        | P64 -> T_poly64\n+        | P128 -> T_poly128\n         | F32 -> T_float32\n         | _ -> failwith \"Bad elt type for Corereg\"\n         end\n@@ -527,10 +538,10 @@ let type_for_elt shape elt no =\n let vectype_size = function\n     T_int8x8 | T_int16x4 | T_int32x2 | T_int64x1\n   | T_uint8x8 | T_uint16x4 | T_uint32x2 | T_uint64x1\n-  | T_float32x2 | T_poly8x8 | T_poly16x4 | T_float16x4 -> 64\n+  | T_float32x2 | T_poly8x8 | T_poly64x1 | T_poly16x4 | T_float16x4 -> 64\n   | T_int8x16 | T_int16x8 | T_int32x4 | T_int64x2\n   | T_uint8x16 | T_uint16x8  | T_uint32x4  | T_uint64x2\n-  | T_float32x4 | T_poly8x16 | T_poly16x8 -> 128\n+  | T_float32x4 | T_poly8x16 | T_poly64x2 | T_poly16x8 -> 128\n   | _ -> raise Not_found\n \n let inttype_for_array num elttype =\n@@ -1041,14 +1052,22 @@ let ops =\n       \"vRsraQ_n\", shift_right_acc, su_8_64;\n \n     (* Vector shift right and insert.  *)\n+    Vsri, [Requires_feature \"CRYPTO\"], Use_operands [| Dreg; Dreg; Immed |], \"vsri_n\", shift_insert,\n+      [P64];\n     Vsri, [], Use_operands [| Dreg; Dreg; Immed |], \"vsri_n\", shift_insert,\n       P8 :: P16 :: su_8_64;\n+    Vsri, [Requires_feature \"CRYPTO\"], Use_operands [| Qreg; Qreg; Immed |], \"vsriQ_n\", shift_insert,\n+      [P64];\n     Vsri, [], Use_operands [| Qreg; Qreg; Immed |], \"vsriQ_n\", shift_insert,\n       P8 :: P16 :: su_8_64;\n \n     (* Vector shift left and insert.  *)\n+    Vsli, [Requires_feature \"CRYPTO\"], Use_operands [| Dreg; Dreg; Immed |], \"vsli_n\", shift_insert,\n+      [P64];\n     Vsli, [], Use_operands [| Dreg; Dreg; Immed |], \"vsli_n\", shift_insert,\n       P8 :: P16 :: su_8_64;\n+    Vsli, [Requires_feature \"CRYPTO\"], Use_operands [| Qreg; Qreg; Immed |], \"vsliQ_n\", shift_insert,\n+      [P64];\n     Vsli, [], Use_operands [| Qreg; Qreg; Immed |], \"vsliQ_n\", shift_insert,\n       P8 :: P16 :: su_8_64;\n \n@@ -1134,6 +1153,11 @@ let ops =\n       set_lane_notype, [S64; U64];\n \n     (* Create vector from literal bit pattern.  *)\n+    Vcreate,\n+      [Requires_feature \"CRYPTO\"; No_op], (* Not really, but it can yield various things that are too\n+                                   hard for the test generator at this time.  *)\n+      Use_operands [| Dreg; Corereg |], \"vcreate\", create_vector,\n+      [P64];\n     Vcreate,\n       [No_op], (* Not really, but it can yield various things that are too\n                   hard for the test generator at this time.  *)\n@@ -1147,12 +1171,25 @@ let ops =\n                                                        Element_of_dreg ] |]]],\n       Use_operands [| Dreg; Corereg |], \"vdup_n\", bits_1,\n       pf_su_8_32;\n+    Vdup_n,\n+      [No_op; Requires_feature \"CRYPTO\";\n+       Instruction_name [\"vmov\"];\n+       Disassembles_as [Use_operands [| Dreg; Corereg; Corereg |]]],\n+      Use_operands [| Dreg; Corereg |], \"vdup_n\", notype_1,\n+      [P64];\n     Vdup_n,\n       [No_op;\n        Instruction_name [\"vmov\"];\n        Disassembles_as [Use_operands [| Dreg; Corereg; Corereg |]]],\n       Use_operands [| Dreg; Corereg |], \"vdup_n\", notype_1,\n       [S64; U64];\n+    Vdup_n,\n+      [No_op; Requires_feature \"CRYPTO\";\n+       Disassembles_as [Use_operands [| Qreg;\n+                                        Alternatives [ Corereg;\n+                                                       Element_of_dreg ] |]]],\n+      Use_operands [| Qreg; Corereg |], \"vdupQ_n\", bits_1,\n+      [P64];\n     Vdup_n,\n       [Disassembles_as [Use_operands [| Qreg;\n                                         Alternatives [ Corereg;\n@@ -1205,22 +1242,34 @@ let ops =\n     Vdup_lane,\n       [Disassembles_as [Use_operands [| Dreg; Element_of_dreg |]]],\n       Unary_scalar Dreg, \"vdup_lane\", bits_2, pf_su_8_32;\n+    Vdup_lane,\n+      [No_op; Requires_feature \"CRYPTO\"; Const_valuator (fun _ -> 0)],\n+      Unary_scalar Dreg, \"vdup_lane\", bits_2, [P64];\n     Vdup_lane,\n       [No_op; Const_valuator (fun _ -> 0)],\n       Unary_scalar Dreg, \"vdup_lane\", bits_2, [S64; U64];\n     Vdup_lane,\n       [Disassembles_as [Use_operands [| Qreg; Element_of_dreg |]]],\n       Unary_scalar Qreg, \"vdupQ_lane\", bits_2, pf_su_8_32;\n+    Vdup_lane,\n+      [No_op; Requires_feature \"CRYPTO\"; Const_valuator (fun _ -> 0)],\n+      Unary_scalar Qreg, \"vdupQ_lane\", bits_2, [P64];\n     Vdup_lane,\n       [No_op; Const_valuator (fun _ -> 0)],\n       Unary_scalar Qreg, \"vdupQ_lane\", bits_2, [S64; U64];\n \n     (* Combining vectors.  *)\n+    Vcombine, [Requires_feature \"CRYPTO\"; No_op],\n+      Use_operands [| Qreg; Dreg; Dreg |], \"vcombine\", notype_2,\n+      [P64];\n     Vcombine, [No_op],\n       Use_operands [| Qreg; Dreg; Dreg |], \"vcombine\", notype_2,\n       pf_su_8_64;\n \n     (* Splitting vectors.  *)\n+    Vget_high, [Requires_feature \"CRYPTO\"; No_op],\n+      Use_operands [| Dreg; Qreg |], \"vget_high\",\n+      notype_1, [P64];\n     Vget_high, [No_op],\n       Use_operands [| Dreg; Qreg |], \"vget_high\",\n       notype_1, pf_su_8_64;\n@@ -1229,7 +1278,10 @@ let ops =\n \t       Fixed_vector_reg],\n       Use_operands [| Dreg; Qreg |], \"vget_low\",\n       notype_1, pf_su_8_32;\n-     Vget_low, [No_op],\n+    Vget_low, [Requires_feature \"CRYPTO\"; No_op],\n+      Use_operands [| Dreg; Qreg |], \"vget_low\",\n+      notype_1, [P64];\n+    Vget_low, [No_op],\n       Use_operands [| Dreg; Qreg |], \"vget_low\",\n       notype_1, [S64; U64];\n \n@@ -1412,9 +1464,15 @@ let ops =\n       [S16; S32];\n \n     (* Vector extract.  *)\n+    Vext, [Requires_feature \"CRYPTO\"; Const_valuator (fun _ -> 0)],\n+      Use_operands [| Dreg; Dreg; Dreg; Immed |], \"vext\", extend,\n+      [P64];\n     Vext, [Const_valuator (fun _ -> 0)],\n       Use_operands [| Dreg; Dreg; Dreg; Immed |], \"vext\", extend,\n       pf_su_8_64;\n+    Vext, [Requires_feature \"CRYPTO\"; Const_valuator (fun _ -> 0)],\n+      Use_operands [| Qreg; Qreg; Qreg; Immed |], \"vextQ\", extend,\n+      [P64];\n     Vext, [Const_valuator (fun _ -> 0)],\n       Use_operands [| Qreg; Qreg; Qreg; Immed |], \"vextQ\", extend,\n       pf_su_8_64;\n@@ -1434,11 +1492,21 @@ let ops =\n       [P8; S8; U8];\n \n     (* Bit selection.  *)\n+    Vbsl,\n+      [Requires_feature \"CRYPTO\"; Instruction_name [\"vbsl\"; \"vbit\"; \"vbif\"];\n+       Disassembles_as [Use_operands [| Dreg; Dreg; Dreg |]]],\n+      Use_operands [| Dreg; Dreg; Dreg; Dreg |], \"vbsl\", bit_select,\n+      [P64];\n     Vbsl,\n       [Instruction_name [\"vbsl\"; \"vbit\"; \"vbif\"];\n        Disassembles_as [Use_operands [| Dreg; Dreg; Dreg |]]],\n       Use_operands [| Dreg; Dreg; Dreg; Dreg |], \"vbsl\", bit_select,\n       pf_su_8_64;\n+    Vbsl,\n+      [Requires_feature \"CRYPTO\"; Instruction_name [\"vbsl\"; \"vbit\"; \"vbif\"];\n+       Disassembles_as [Use_operands [| Qreg; Qreg; Qreg |]]],\n+      Use_operands [| Qreg; Qreg; Qreg; Qreg |], \"vbslQ\", bit_select,\n+      [P64];\n     Vbsl,\n       [Instruction_name [\"vbsl\"; \"vbit\"; \"vbif\"];\n        Disassembles_as [Use_operands [| Qreg; Qreg; Qreg |]]],\n@@ -1460,11 +1528,22 @@ let ops =\n       pf_su_8_32;\n \n     (* Element/structure loads.  VLD1 variants.  *)\n+    Vldx 1,\n+      [Requires_feature \"CRYPTO\";\n+       Disassembles_as [Use_operands [| VecArray (1, Dreg);\n+                                        CstPtrTo Corereg |]]],\n+      Use_operands [| Dreg; CstPtrTo Corereg |], \"vld1\", bits_1,\n+      [P64];\n     Vldx 1,\n       [Disassembles_as [Use_operands [| VecArray (1, Dreg);\n                                         CstPtrTo Corereg |]]],\n       Use_operands [| Dreg; CstPtrTo Corereg |], \"vld1\", bits_1,\n       pf_su_8_64;\n+    Vldx 1, [Requires_feature \"CRYPTO\";\n+             Disassembles_as [Use_operands [| VecArray (2, Dreg);\n+\t\t\t\t\t      CstPtrTo Corereg |]]],\n+      Use_operands [| Qreg; CstPtrTo Corereg |], \"vld1Q\", bits_1,\n+      [P64];\n     Vldx 1, [Disassembles_as [Use_operands [| VecArray (2, Dreg);\n \t\t\t\t\t      CstPtrTo Corereg |]]],\n       Use_operands [| Qreg; CstPtrTo Corereg |], \"vld1Q\", bits_1,\n@@ -1475,6 +1554,13 @@ let ops =\n                                         CstPtrTo Corereg |]]],\n       Use_operands [| Dreg; CstPtrTo Corereg; Dreg; Immed |],\n       \"vld1_lane\", bits_3, pf_su_8_32;\n+    Vldx_lane 1,\n+      [Requires_feature \"CRYPTO\";\n+       Disassembles_as [Use_operands [| VecArray (1, Dreg);\n+                                        CstPtrTo Corereg |]];\n+       Const_valuator (fun _ -> 0)],\n+      Use_operands [| Dreg; CstPtrTo Corereg; Dreg; Immed |],\n+      \"vld1_lane\", bits_3, [P64];\n     Vldx_lane 1,\n       [Disassembles_as [Use_operands [| VecArray (1, Dreg);\n                                         CstPtrTo Corereg |]];\n@@ -1486,6 +1572,12 @@ let ops =\n                                         CstPtrTo Corereg |]]],\n       Use_operands [| Qreg; CstPtrTo Corereg; Qreg; Immed |],\n       \"vld1Q_lane\", bits_3, pf_su_8_32;\n+    Vldx_lane 1,\n+      [Requires_feature \"CRYPTO\";\n+       Disassembles_as [Use_operands [| VecArray (1, Dreg);\n+                                        CstPtrTo Corereg |]]],\n+      Use_operands [| Qreg; CstPtrTo Corereg; Qreg; Immed |],\n+      \"vld1Q_lane\", bits_3, [P64];\n     Vldx_lane 1,\n       [Disassembles_as [Use_operands [| VecArray (1, Dreg);\n                                         CstPtrTo Corereg |]]],\n@@ -1497,6 +1589,12 @@ let ops =\n                                         CstPtrTo Corereg |]]],\n       Use_operands [| Dreg; CstPtrTo Corereg |], \"vld1_dup\",\n       bits_1, pf_su_8_32;\n+    Vldx_dup 1,\n+      [Requires_feature \"CRYPTO\";\n+       Disassembles_as [Use_operands [| VecArray (1, Dreg);\n+                                        CstPtrTo Corereg |]]],\n+      Use_operands [| Dreg; CstPtrTo Corereg |], \"vld1_dup\",\n+      bits_1, [P64];\n     Vldx_dup 1,\n       [Disassembles_as [Use_operands [| VecArray (1, Dreg);\n                                         CstPtrTo Corereg |]]],\n@@ -1509,17 +1607,33 @@ let ops =\n       bits_1, pf_su_8_32;\n     (* Treated identically to vld1_dup above as we now\n        do a single load followed by a duplicate.  *)\n+    Vldx_dup 1,\n+      [Requires_feature \"CRYPTO\";\n+       Disassembles_as [Use_operands [| VecArray (1, Dreg);\n+                                        CstPtrTo Corereg |]]],\n+      Use_operands [| Qreg; CstPtrTo Corereg |], \"vld1Q_dup\",\n+      bits_1, [P64];\n     Vldx_dup 1,\n       [Disassembles_as [Use_operands [| VecArray (1, Dreg);\n                                         CstPtrTo Corereg |]]],\n       Use_operands [| Qreg; CstPtrTo Corereg |], \"vld1Q_dup\",\n       bits_1, [S64; U64];\n \n     (* VST1 variants.  *)\n+    Vstx 1, [Requires_feature \"CRYPTO\";\n+             Disassembles_as [Use_operands [| VecArray (1, Dreg);\n+                                              PtrTo Corereg |]]],\n+      Use_operands [| PtrTo Corereg; Dreg |], \"vst1\",\n+      store_1, [P64];\n     Vstx 1, [Disassembles_as [Use_operands [| VecArray (1, Dreg);\n                                               PtrTo Corereg |]]],\n       Use_operands [| PtrTo Corereg; Dreg |], \"vst1\",\n       store_1, pf_su_8_64;\n+    Vstx 1, [Requires_feature \"CRYPTO\";\n+             Disassembles_as [Use_operands [| VecArray (2, Dreg);\n+\t\t\t\t\t      PtrTo Corereg |]]],\n+      Use_operands [| PtrTo Corereg; Qreg |], \"vst1Q\",\n+      store_1, [P64];\n     Vstx 1, [Disassembles_as [Use_operands [| VecArray (2, Dreg);\n \t\t\t\t\t      PtrTo Corereg |]]],\n       Use_operands [| PtrTo Corereg; Qreg |], \"vst1Q\",\n@@ -1530,6 +1644,13 @@ let ops =\n                                         CstPtrTo Corereg |]]],\n       Use_operands [| PtrTo Corereg; Dreg; Immed |],\n       \"vst1_lane\", store_3, pf_su_8_32;\n+    Vstx_lane 1,\n+      [Requires_feature \"CRYPTO\";\n+       Disassembles_as [Use_operands [| VecArray (1, Dreg);\n+                                        CstPtrTo Corereg |]];\n+       Const_valuator (fun _ -> 0)],\n+      Use_operands [| PtrTo Corereg; Dreg; Immed |],\n+      \"vst1_lane\", store_3, [P64];\n     Vstx_lane 1,\n       [Disassembles_as [Use_operands [| VecArray (1, Dreg);\n                                         CstPtrTo Corereg |]];\n@@ -1541,6 +1662,12 @@ let ops =\n                                         CstPtrTo Corereg |]]],\n       Use_operands [| PtrTo Corereg; Qreg; Immed |],\n       \"vst1Q_lane\", store_3, pf_su_8_32;\n+    Vstx_lane 1,\n+      [Requires_feature \"CRYPTO\";\n+       Disassembles_as [Use_operands [| VecArray (1, Dreg);\n+                                        CstPtrTo Corereg |]]],\n+      Use_operands [| PtrTo Corereg; Qreg; Immed |],\n+      \"vst1Q_lane\", store_3, [P64];\n     Vstx_lane 1,\n       [Disassembles_as [Use_operands [| VecArray (1, Dreg);\n                                         CstPtrTo Corereg |]]],\n@@ -1550,6 +1677,9 @@ let ops =\n     (* VLD2 variants.  *)\n     Vldx 2, [], Use_operands [| VecArray (2, Dreg); CstPtrTo Corereg |],\n       \"vld2\", bits_1, pf_su_8_32;\n+    Vldx 2, [Requires_feature \"CRYPTO\"; Instruction_name [\"vld1\"]],\n+       Use_operands [| VecArray (2, Dreg); CstPtrTo Corereg |],\n+      \"vld2\", bits_1, [P64];\n     Vldx 2, [Instruction_name [\"vld1\"]],\n        Use_operands [| VecArray (2, Dreg); CstPtrTo Corereg |],\n       \"vld2\", bits_1, [S64; U64];\n@@ -1580,6 +1710,12 @@ let ops =\n         [| VecArray (2, All_elements_of_dreg); CstPtrTo Corereg |]]],\n       Use_operands [| VecArray (2, Dreg); CstPtrTo Corereg |],\n       \"vld2_dup\", bits_1, pf_su_8_32;\n+    Vldx_dup 2,\n+      [Requires_feature \"CRYPTO\";\n+       Instruction_name [\"vld1\"]; Disassembles_as [Use_operands\n+        [| VecArray (2, Dreg); CstPtrTo Corereg |]]],\n+      Use_operands [| VecArray (2, Dreg); CstPtrTo Corereg |],\n+      \"vld2_dup\", bits_1, [P64];\n     Vldx_dup 2,\n       [Instruction_name [\"vld1\"]; Disassembles_as [Use_operands\n         [| VecArray (2, Dreg); CstPtrTo Corereg |]]],\n@@ -1591,6 +1727,12 @@ let ops =\n                                               PtrTo Corereg |]]],\n       Use_operands [| PtrTo Corereg; VecArray (2, Dreg) |], \"vst2\",\n       store_1, pf_su_8_32;\n+    Vstx 2, [Requires_feature \"CRYPTO\";\n+             Disassembles_as [Use_operands [| VecArray (2, Dreg);\n+                                              PtrTo Corereg |]];\n+             Instruction_name [\"vst1\"]],\n+      Use_operands [| PtrTo Corereg; VecArray (2, Dreg) |], \"vst2\",\n+      store_1, [P64];\n     Vstx 2, [Disassembles_as [Use_operands [| VecArray (2, Dreg);\n                                               PtrTo Corereg |]];\n              Instruction_name [\"vst1\"]],\n@@ -1619,6 +1761,9 @@ let ops =\n     (* VLD3 variants.  *)\n     Vldx 3, [], Use_operands [| VecArray (3, Dreg); CstPtrTo Corereg |],\n       \"vld3\", bits_1, pf_su_8_32;\n+    Vldx 3, [Requires_feature \"CRYPTO\"; Instruction_name [\"vld1\"]],\n+      Use_operands [| VecArray (3, Dreg); CstPtrTo Corereg |],\n+      \"vld3\", bits_1, [P64];\n     Vldx 3, [Instruction_name [\"vld1\"]],\n       Use_operands [| VecArray (3, Dreg); CstPtrTo Corereg |],\n       \"vld3\", bits_1, [S64; U64];\n@@ -1649,6 +1794,12 @@ let ops =\n         [| VecArray (3, All_elements_of_dreg); CstPtrTo Corereg |]]],\n       Use_operands [| VecArray (3, Dreg); CstPtrTo Corereg |],\n       \"vld3_dup\", bits_1, pf_su_8_32;\n+    Vldx_dup 3,\n+      [Requires_feature \"CRYPTO\";\n+       Instruction_name [\"vld1\"]; Disassembles_as [Use_operands\n+        [| VecArray (3, Dreg); CstPtrTo Corereg |]]],\n+      Use_operands [| VecArray (3, Dreg); CstPtrTo Corereg |],\n+      \"vld3_dup\", bits_1, [P64];\n     Vldx_dup 3,\n       [Instruction_name [\"vld1\"]; Disassembles_as [Use_operands\n         [| VecArray (3, Dreg); CstPtrTo Corereg |]]],\n@@ -1660,6 +1811,12 @@ let ops =\n                                               PtrTo Corereg |]]],\n       Use_operands [| PtrTo Corereg; VecArray (3, Dreg) |], \"vst3\",\n       store_1, pf_su_8_32;\n+    Vstx 3, [Requires_feature \"CRYPTO\";\n+             Disassembles_as [Use_operands [| VecArray (4, Dreg);\n+                                              PtrTo Corereg |]];\n+             Instruction_name [\"vst1\"]],\n+      Use_operands [| PtrTo Corereg; VecArray (3, Dreg) |], \"vst3\",\n+      store_1, [P64];\n     Vstx 3, [Disassembles_as [Use_operands [| VecArray (4, Dreg);\n                                               PtrTo Corereg |]];\n              Instruction_name [\"vst1\"]],\n@@ -1688,6 +1845,9 @@ let ops =\n     (* VLD4/VST4 variants.  *)\n     Vldx 4, [], Use_operands [| VecArray (4, Dreg); CstPtrTo Corereg |],\n       \"vld4\", bits_1, pf_su_8_32;\n+    Vldx 4, [Requires_feature \"CRYPTO\"; Instruction_name [\"vld1\"]],\n+      Use_operands [| VecArray (4, Dreg); CstPtrTo Corereg |],\n+      \"vld4\", bits_1, [P64];\n     Vldx 4, [Instruction_name [\"vld1\"]],\n       Use_operands [| VecArray (4, Dreg); CstPtrTo Corereg |],\n       \"vld4\", bits_1, [S64; U64];\n@@ -1718,6 +1878,12 @@ let ops =\n         [| VecArray (4, All_elements_of_dreg); CstPtrTo Corereg |]]],\n       Use_operands [| VecArray (4, Dreg); CstPtrTo Corereg |],\n       \"vld4_dup\", bits_1, pf_su_8_32;\n+    Vldx_dup 4,\n+      [Requires_feature \"CRYPTO\";\n+       Instruction_name [\"vld1\"]; Disassembles_as [Use_operands\n+        [| VecArray (4, Dreg); CstPtrTo Corereg |]]],\n+      Use_operands [| VecArray (4, Dreg); CstPtrTo Corereg |],\n+      \"vld4_dup\", bits_1, [P64];\n     Vldx_dup 4,\n       [Instruction_name [\"vld1\"]; Disassembles_as [Use_operands\n         [| VecArray (4, Dreg); CstPtrTo Corereg |]]],\n@@ -1728,6 +1894,12 @@ let ops =\n                                               PtrTo Corereg |]]],\n       Use_operands [| PtrTo Corereg; VecArray (4, Dreg) |], \"vst4\",\n       store_1, pf_su_8_32;\n+    Vstx 4, [Requires_feature \"CRYPTO\";\n+             Disassembles_as [Use_operands [| VecArray (4, Dreg);\n+                                              PtrTo Corereg |]];\n+             Instruction_name [\"vst1\"]],\n+      Use_operands [| PtrTo Corereg; VecArray (4, Dreg) |], \"vst4\",\n+      store_1, [P64];\n     Vstx 4, [Disassembles_as [Use_operands [| VecArray (4, Dreg);\n                                               PtrTo Corereg |]];\n              Instruction_name [\"vst1\"]],\n@@ -1779,26 +1951,32 @@ let ops =\n     Vorn, [], All (3, Qreg), \"vornQ\", notype_2, su_8_64;\n   ]\n \n+let type_in_crypto_only t\n+  = (t == P64) or (t == P128)\n+\n+let cross_product s1 s2\n+  = List.filter (fun (e, e') -> e <> e')\n+                (List.concat (List.map (fun e1 -> List.map (fun e2 -> (e1,e2)) s1) s2))\n+\n let reinterp =\n-  let elems = P8 :: P16 :: F32 :: su_8_64 in\n-  List.fold_right\n-    (fun convto acc ->\n-      let types = List.fold_right\n-        (fun convfrom acc ->\n-          if convfrom <> convto then\n-            Cast (convto, convfrom) :: acc\n-          else\n-            acc)\n-        elems\n-        []\n-      in\n-        let dconv = Vreinterp, [No_op], Use_operands [| Dreg; Dreg |],\n-                      \"vreinterpret\", conv_1, types\n-        and qconv = Vreinterp, [No_op], Use_operands [| Qreg; Qreg |],\n-\t\t      \"vreinterpretQ\", conv_1, types in\n-        dconv :: qconv :: acc)\n-    elems\n-    []\n+  let elems = P8 :: P16 :: F32 :: P64 :: su_8_64 in\n+  let casts = cross_product elems elems in\n+  List.map\n+    (fun (convto, convfrom) ->\n+       Vreinterp, (if (type_in_crypto_only convto) or (type_in_crypto_only convfrom)\n+                   then [Requires_feature \"CRYPTO\"] else []) @ [No_op], Use_operands [| Dreg; Dreg |],\n+                   \"vreinterpret\", conv_1, [Cast (convto, convfrom)])\n+    casts\n+\n+let reinterpq =\n+  let elems = P8 :: P16 :: F32 :: P64 :: P128 :: su_8_64 in\n+  let casts = cross_product elems elems in\n+  List.map\n+    (fun (convto, convfrom) ->\n+       Vreinterp, (if (type_in_crypto_only convto) or (type_in_crypto_only convfrom)\n+                   then [Requires_feature \"CRYPTO\"] else []) @ [No_op], Use_operands [| Qreg; Qreg |],\n+                   \"vreinterpretQ\", conv_1, [Cast (convto, convfrom)])\n+    casts\n \n (* Output routines.  *)\n \n@@ -1808,6 +1986,7 @@ let rec string_of_elt = function\n   | I8 -> \"i8\" | I16 -> \"i16\" | I32 -> \"i32\" | I64 -> \"i64\"\n   | B8 -> \"8\" | B16 -> \"16\" | B32 -> \"32\" | B64 -> \"64\"\n   | F16 -> \"f16\" | F32 -> \"f32\" | P8 -> \"p8\" | P16 -> \"p16\"\n+  | P64 -> \"p64\" | P128 -> \"p128\"\n   | Conv (a, b) | Cast (a, b) -> string_of_elt a ^ \"_\" ^ string_of_elt b\n   | NoElts -> failwith \"No elts\"\n \n@@ -1851,6 +2030,10 @@ let string_of_vectype vt =\n   | T_uint64 -> affix \"uint64\"\n   | T_poly8 -> affix \"poly8\"\n   | T_poly16 -> affix \"poly16\"\n+  | T_poly64 -> affix \"poly64\"\n+  | T_poly64x1 -> affix \"poly64x1\"\n+  | T_poly64x2 -> affix \"poly64x2\"\n+  | T_poly128 -> affix \"poly128\"\n   | T_float16 -> affix \"float16\"\n   | T_float32 -> affix \"float32\"\n   | T_immediate _ -> \"const int\"\n@@ -1859,6 +2042,7 @@ let string_of_vectype vt =\n   | T_intHI -> \"__builtin_neon_hi\"\n   | T_intSI -> \"__builtin_neon_si\"\n   | T_intDI -> \"__builtin_neon_di\"\n+  | T_intTI -> \"__builtin_neon_ti\"\n   | T_floatHF -> \"__builtin_neon_hf\"\n   | T_floatSF -> \"__builtin_neon_sf\"\n   | T_arrayof (num, base) ->\n@@ -1884,7 +2068,7 @@ let string_of_mode = function\n     V8QI -> \"v8qi\" | V4HI -> \"v4hi\" | V4HF  -> \"v4hf\"  | V2SI -> \"v2si\"\n   | V2SF -> \"v2sf\" | DI   -> \"di\"   | V16QI -> \"v16qi\" | V8HI -> \"v8hi\"\n   | V4SI -> \"v4si\" | V4SF -> \"v4sf\" | V2DI  -> \"v2di\"  | QI   -> \"qi\"\n-  | HI -> \"hi\" | SI -> \"si\" | SF -> \"sf\"\n+  | HI -> \"hi\" | SI -> \"si\" | SF -> \"sf\" | TI -> \"ti\"\n \n (* Use uppercase chars for letters which form part of the intrinsic name, but\n    should be omitted from the builtin name (the info is passed in an extra\n@@ -1991,3 +2175,146 @@ let analyze_all_shapes features shape f =\n     | _ -> assert false\n   with Not_found -> [f shape]\n \n+(* The crypto intrinsics have unconventional shapes and are not that\n+   numerous to be worth the trouble of encoding here.  We implement them\n+   explicitly here.  *)\n+let crypto_intrinsics =\n+\"\n+#ifdef __ARM_FEATURE_CRYPTO\n+\n+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))\n+vldrq_p128 (poly128_t const * __ptr)\n+{\n+#ifdef __ARM_BIG_ENDIAN\n+  poly64_t* __ptmp = (poly64_t*) __ptr;\n+  poly64_t __d0 = vld1_p64 (__ptmp);\n+  poly64_t __d1 = vld1_p64 (__ptmp + 1);\n+  return vreinterpretq_p128_p64 (vcombine_p64 (__d1, __d0));\n+#else\n+  return vreinterpretq_p128_p64 (vld1q_p64 ((poly64_t*) __ptr));\n+#endif\n+}\n+\n+__extension__ static __inline void __attribute__ ((__always_inline__))\n+vstrq_p128 (poly128_t * __ptr, poly128_t __val)\n+{\n+#ifdef __ARM_BIG_ENDIAN\n+  poly64x2_t __tmp = vreinterpretq_p64_p128 (__val);\n+  poly64_t __d0 = vget_high_p64 (__tmp);\n+  poly64_t __d1 = vget_low_p64 (__tmp);\n+  vst1q_p64 ((poly64_t*) __ptr, vcombine_p64 (__d0, __d1));\n+#else\n+  vst1q_p64 ((poly64_t*) __ptr, vreinterpretq_p64_p128 (__val));\n+#endif\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vaeseq_u8 (uint8x16_t __data, uint8x16_t __key)\n+{\n+  return __builtin_arm_crypto_aese (__data, __key);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vaesdq_u8 (uint8x16_t __data, uint8x16_t __key)\n+{\n+  return __builtin_arm_crypto_aesd (__data, __key);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vaesmcq_u8 (uint8x16_t __data)\n+{\n+  return __builtin_arm_crypto_aesmc (__data);\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vaesimcq_u8 (uint8x16_t __data)\n+{\n+  return __builtin_arm_crypto_aesimc (__data);\n+}\n+\n+__extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n+vsha1h_u32 (uint32_t __hash_e)\n+{\n+  uint32x4_t __t = vdupq_n_u32 (0);\n+  __t = vsetq_lane_u32 (__hash_e, __t, 0);\n+  __t = __builtin_arm_crypto_sha1h (__t);\n+  return vgetq_lane_u32 (__t, 0);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vsha1cq_u32 (uint32x4_t __hash_abcd, uint32_t __hash_e, uint32x4_t __wk)\n+{\n+  uint32x4_t __t = vdupq_n_u32 (0);\n+  __t = vsetq_lane_u32 (__hash_e, __t, 0);\n+  return __builtin_arm_crypto_sha1c (__hash_abcd, __t, __wk);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vsha1pq_u32 (uint32x4_t __hash_abcd, uint32_t __hash_e, uint32x4_t __wk)\n+{\n+  uint32x4_t __t = vdupq_n_u32 (0);\n+  __t = vsetq_lane_u32 (__hash_e, __t, 0);\n+  return __builtin_arm_crypto_sha1p (__hash_abcd, __t, __wk);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vsha1mq_u32 (uint32x4_t __hash_abcd, uint32_t __hash_e, uint32x4_t __wk)\n+{\n+  uint32x4_t __t = vdupq_n_u32 (0);\n+  __t = vsetq_lane_u32 (__hash_e, __t, 0);\n+  return __builtin_arm_crypto_sha1m (__hash_abcd, __t, __wk);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vsha1su0q_u32 (uint32x4_t __w0_3, uint32x4_t __w4_7, uint32x4_t __w8_11)\n+{\n+  return __builtin_arm_crypto_sha1su0 (__w0_3, __w4_7, __w8_11);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vsha1su1q_u32 (uint32x4_t __tw0_3, uint32x4_t __w12_15)\n+{\n+  return __builtin_arm_crypto_sha1su1 (__tw0_3, __w12_15);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vsha256hq_u32 (uint32x4_t __hash_abcd, uint32x4_t __hash_efgh, uint32x4_t __wk)\n+{\n+  return __builtin_arm_crypto_sha256h (__hash_abcd, __hash_efgh, __wk);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vsha256h2q_u32 (uint32x4_t __hash_abcd, uint32x4_t __hash_efgh, uint32x4_t __wk)\n+{\n+  return __builtin_arm_crypto_sha256h2 (__hash_abcd, __hash_efgh, __wk);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vsha256su0q_u32 (uint32x4_t __w0_3, uint32x4_t __w4_7)\n+{\n+  return __builtin_arm_crypto_sha256su0 (__w0_3, __w4_7);\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vsha256su1q_u32 (uint32x4_t __tw0_3, uint32x4_t __w8_11, uint32x4_t __w12_15)\n+{\n+  return __builtin_arm_crypto_sha256su1 (__tw0_3, __w8_11, __w12_15);\n+}\n+\n+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))\n+vmull_p64 (poly64_t __a, poly64_t __b)\n+{\n+  return (poly128_t) __builtin_arm_crypto_vmullp64 ((uint64_t) __a, (uint64_t) __b);\n+}\n+\n+__extension__ static __inline poly128_t __attribute__ ((__always_inline__))\n+vmull_high_p64 (poly64x2_t __a, poly64x2_t __b)\n+{\n+  poly64_t __t1 = vget_high_p64 (__a);\n+  poly64_t __t2 = vget_high_p64 (__b);\n+\n+  return (poly128_t) __builtin_arm_crypto_vmullp64 ((uint64_t) __t1, (uint64_t) __t2);\n+}\n+\n+#endif\n+\""}, {"sha": "af4b832e88b2f977e25cf6cc5e536f3e3dc894e1", "filename": "gcc/config/arm/unspecs.md", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Funspecs.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/021b5e6b54aecf36d3b105368485682e5d06782a/gcc%2Fconfig%2Farm%2Funspecs.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Funspecs.md?ref=021b5e6b54aecf36d3b105368485682e5d06782a", "patch": "@@ -155,6 +155,21 @@\n   UNSPEC_CRC32CB\n   UNSPEC_CRC32CH\n   UNSPEC_CRC32CW\n+  UNSPEC_AESD\n+  UNSPEC_AESE\n+  UNSPEC_AESIMC\n+  UNSPEC_AESMC\n+  UNSPEC_SHA1C\n+  UNSPEC_SHA1M\n+  UNSPEC_SHA1P\n+  UNSPEC_SHA1H\n+  UNSPEC_SHA1SU0\n+  UNSPEC_SHA1SU1\n+  UNSPEC_SHA256H\n+  UNSPEC_SHA256H2\n+  UNSPEC_SHA256SU0\n+  UNSPEC_SHA256SU1\n+  UNSPEC_VMULLP64\n   UNSPEC_LOAD_COUNT\n   UNSPEC_VABD\n   UNSPEC_VABDL"}]}