{"sha": "6033b27eade9c31c0be50657094c89ef9068892d", "node_id": "C_kwDOANBUbNoAKDYwMzNiMjdlYWRlOWMzMWMwYmU1MDY1NzA5NGM4OWVmOTA2ODg5MmQ", "commit": {"author": {"name": "Jan Hubicka", "email": "jh@suse.cz", "date": "2021-11-23T09:55:56Z"}, "committer": {"name": "Jan Hubicka", "email": "jh@suse.cz", "date": "2021-11-23T09:57:14Z"}, "message": "Improve bytewise DSE\n\ntestcase modref-dse-4.c and modref-dse-5.c fails on some targets because they\ndepend on store merging.  What really happens is that without store merging\nwe produce for kill_me combined write that is ao_ref with offset=0, size=32\nand max_size=96.  We have size != max_size becaue we do ont track the info that\nall 3 writes must happen in a group and conider case only some of them are done.\n\nThis disables byte-wise DSE which checks that size == max_size.  This is\ncompletely unnecesary for store being proved to be dead or load being checked\nto not read live bytes.  It is only necessary for kill store that is used to\nprove that given store is dead.\n\nWhile looking into this I also noticed that we check that everything is byte\naligned.  This is also unnecessary and with access merging in modref may more\ncommonly fire on accesses that we could otherwise handle.\n\nThis patch fixes both also also changes interface to normalize_ref that I found\nconfusing since it modifies the ref. Instead of that we have get_byte_range\nthat is computing range in bytes (since that is what we need to maintain the\nbitmap) and has additional parameter specifying if the store in question should\nbe turned into sub-range or super-range depending whether we compute range\nfor kill or load.\n\ngcc/ChangeLog:\n\n2021-11-23  Jan Hubicka  <hubicka@ucw.cz>\n\n\tPR tree-optimization/103335\n\t* tree-ssa-dse.c (valid_ao_ref_for_dse): Rename to ...\n\t(valid_ao_ref_kill_for_dse): ... this; do not check that boundaries\n\tare divisible by BITS_PER_UNIT.\n\t(get_byte_aligned_range_containing_ref): New function.\n\t(get_byte_aligned_range_contained_in_ref): New function.\n\t(normalize_ref): Rename to ...\n\t(get_byte_range): ... this one; handle accesses not aligned to byte\n\tboundary; return range in bytes rater than updating ao_ref.\n\t(clear_live_bytes_for_ref): Take write ref by reference; simplify using\n\tget_byte_access.\n\t(setup_live_bytes_from_ref): Likewise.\n\t(clear_bytes_written_by): Update.\n\t(live_bytes_read): Update.\n\t(dse_classify_store): Simplify tech before live_bytes_read checks.\n\ngcc/testsuite/ChangeLog:\n\n2021-11-23  Jan Hubicka  <hubicka@ucw.cz>\n\n\t* gcc.dg/tree-ssa/modref-dse-4.c: Update template.\n\t* gcc.dg/tree-ssa/modref-dse-5.c: Update template.", "tree": {"sha": "32c362eeac297944c743ed12fff9343d44c9095e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/32c362eeac297944c743ed12fff9343d44c9095e"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/6033b27eade9c31c0be50657094c89ef9068892d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6033b27eade9c31c0be50657094c89ef9068892d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6033b27eade9c31c0be50657094c89ef9068892d", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6033b27eade9c31c0be50657094c89ef9068892d/comments", "author": null, "committer": null, "parents": [{"sha": "911b633803dcbb298c98777e29fd260834c0d04a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/911b633803dcbb298c98777e29fd260834c0d04a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/911b633803dcbb298c98777e29fd260834c0d04a"}], "stats": {"total": 182, "additions": 128, "deletions": 54}, "files": [{"sha": "19e91b00f15049941259e5dbe23c41641ff768b5", "filename": "gcc/testsuite/gcc.dg/tree-ssa/modref-dse-4.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6033b27eade9c31c0be50657094c89ef9068892d/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fmodref-dse-4.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6033b27eade9c31c0be50657094c89ef9068892d/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fmodref-dse-4.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fmodref-dse-4.c?ref=6033b27eade9c31c0be50657094c89ef9068892d", "patch": "@@ -1,5 +1,5 @@\n /* { dg-do compile } */\n-/* { dg-options \"-O2 -fdump-tree-dse2-details\"  } */\n+/* { dg-options \"-O2 -fdump-tree-dse1-details\"  } */\n struct a {int a,b,c;};\n __attribute__ ((noinline))\n void\n@@ -23,4 +23,4 @@ set (struct a *a)\n   my_pleasure (a);\n   a->b=1;\n }\n-/* { dg-final { scan-tree-dump \"Deleted dead store: kill_me\" \"dse2\" } } */\n+/* { dg-final { scan-tree-dump \"Deleted dead store: kill_me\" \"dse1\" } } */"}, {"sha": "dc2c2892615e4df340cff8308405f870484d6638", "filename": "gcc/testsuite/gcc.dg/tree-ssa/modref-dse-5.c", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6033b27eade9c31c0be50657094c89ef9068892d/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fmodref-dse-5.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6033b27eade9c31c0be50657094c89ef9068892d/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fmodref-dse-5.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fmodref-dse-5.c?ref=6033b27eade9c31c0be50657094c89ef9068892d", "patch": "@@ -1,5 +1,5 @@\n /* { dg-do compile } */\n-/* { dg-options \"-O2 -fdump-tree-dse2-details\"  } */\n+/* { dg-options \"-O2 -fdump-tree-dse1-details\"  } */\n struct a {int a,b,c;};\n __attribute__ ((noinline))\n void\n@@ -36,8 +36,7 @@ set (struct a *a)\n {\n   wrap (0, a);\n   int ret = wrap2 (0, a);\n-  //int ret = my_pleasure (a);\n   a->b=1;\n   return ret;\n }\n-/* { dg-final { scan-tree-dump \"Deleted dead store: wrap\" \"dse2\" } } */\n+/* { dg-final { scan-tree-dump \"Deleted dead store: wrap\" \"dse1\" } } */"}, {"sha": "8717d654e5ae3b732bb8a37f35f5863d2678f099", "filename": "gcc/tree-ssa-dse.c", "status": "modified", "additions": 124, "deletions": 49, "changes": 173, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6033b27eade9c31c0be50657094c89ef9068892d/gcc%2Ftree-ssa-dse.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6033b27eade9c31c0be50657094c89ef9068892d/gcc%2Ftree-ssa-dse.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-dse.c?ref=6033b27eade9c31c0be50657094c89ef9068892d", "patch": "@@ -156,78 +156,152 @@ initialize_ao_ref_for_dse (gimple *stmt, ao_ref *write)\n }\n \n /* Given REF from the alias oracle, return TRUE if it is a valid\n-   memory reference for dead store elimination, false otherwise.\n+   kill memory reference for dead store elimination, false otherwise.\n \n    In particular, the reference must have a known base, known maximum\n    size, start at a byte offset and have a size that is one or more\n    bytes.  */\n \n static bool\n-valid_ao_ref_for_dse (ao_ref *ref)\n+valid_ao_ref_kill_for_dse (ao_ref *ref)\n {\n   return (ao_ref_base (ref)\n \t  && known_size_p (ref->max_size)\n \t  && maybe_ne (ref->size, 0)\n \t  && known_eq (ref->max_size, ref->size)\n-\t  && known_ge (ref->offset, 0)\n-\t  && multiple_p (ref->offset, BITS_PER_UNIT)\n-\t  && multiple_p (ref->size, BITS_PER_UNIT));\n+\t  && known_ge (ref->offset, 0));\n+}\n+\n+/* Given REF from the alias oracle, return TRUE if it is a valid\n+   load or store memory reference for dead store elimination, false otherwise.\n+\n+   Unlike for valid_ao_ref_kill_for_dse we can accept writes where max_size\n+   is not same as size since we can handle conservatively the larger range.  */\n+\n+static bool\n+valid_ao_ref_for_dse (ao_ref *ref)\n+{\n+  return (ao_ref_base (ref)\n+\t  && known_size_p (ref->max_size)\n+\t  && known_ge (ref->offset, 0));\n+}\n+\n+/* Initialize OFFSET and SIZE to a range known to contain REF\n+   where the boundaries are divisible by BITS_PER_UNIT (bit still in bits).\n+   Return false if this is impossible.  */\n+\n+static bool\n+get_byte_aligned_range_containing_ref (ao_ref *ref, poly_int64 *offset,\n+\t\t\t\t       HOST_WIDE_INT *size)\n+{\n+  if (!known_size_p (ref->max_size))\n+    return false;\n+  *offset = aligned_lower_bound (ref->offset, BITS_PER_UNIT);\n+  poly_int64 end = aligned_upper_bound (ref->offset + ref->max_size,\n+\t\t\t\t\tBITS_PER_UNIT);\n+  return (end - *offset).is_constant (size);\n+}\n+\n+/* Initialize OFFSET and SIZE to a range known to be contained REF\n+   where the boundaries are divisible by BITS_PER_UNIT (but still in bits).\n+   Return false if this is impossible.  */\n+\n+static bool\n+get_byte_aligned_range_contained_in_ref (ao_ref *ref, poly_int64 *offset,\n+\t\t\t\t\t HOST_WIDE_INT *size)\n+{\n+  if (!known_size_p (ref->size)\n+      || !known_eq (ref->size, ref->max_size))\n+    return false;\n+  *offset = aligned_upper_bound (ref->offset, BITS_PER_UNIT);\n+  poly_int64 end = aligned_lower_bound (ref->offset + ref->max_size,\n+\t\t\t\t\tBITS_PER_UNIT);\n+  /* For bit accesses we can get -1 here, but also 0 sized kill is not\n+     useful.  */\n+  if (!known_gt (end, *offset))\n+    return false;\n+  return (end - *offset).is_constant (size);\n }\n \n-/* Try to normalize COPY (an ao_ref) relative to REF.  Essentially when we are\n-   done COPY will only refer bytes found within REF.  Return true if COPY\n-   is known to intersect at least one byte of REF.  */\n+/* Compute byte range (returned iN REF_OFFSET and RET_SIZE) for access COPY\n+   inside REF.  If KILL is true, then COPY represent a kill and the byte range\n+   needs to be fully contained in bit range given by COPY.  If KILL is false\n+   then the byte range returned must contain the range of COPY.  */\n \n static bool\n-normalize_ref (ao_ref *copy, ao_ref *ref)\n+get_byte_range (ao_ref *copy, ao_ref *ref, bool kill,\n+\t\tHOST_WIDE_INT *ret_offset, HOST_WIDE_INT *ret_size)\n {\n-  if (!ordered_p (copy->offset, ref->offset))\n+  HOST_WIDE_INT copy_size, ref_size;\n+  poly_int64 copy_offset, ref_offset;\n+  HOST_WIDE_INT diff;\n+\n+  /* First translate from bits to bytes, rounding to bigger or smaller ranges\n+     as needed.  Kills needs to be always rounded to smaller ranges while\n+     uses and stores to larger ranges.  */\n+  if (kill)\n+    {\n+      if (!get_byte_aligned_range_contained_in_ref (copy, &copy_offset,\n+\t\t\t\t\t\t    &copy_size))\n+\treturn false;\n+    }\n+  else\n+    {\n+      if (!get_byte_aligned_range_containing_ref (copy, &copy_offset,\n+\t\t\t\t\t\t  &copy_size))\n+\treturn false;\n+    }\n+\n+  if (!get_byte_aligned_range_containing_ref (ref, &ref_offset, &ref_size)\n+      || !ordered_p (copy_offset, ref_offset))\n     return false;\n \n+  /* Switch sizes from bits to bytes so we do not need to care about\n+     overflows.  Offset calculation needs to stay in bits until we compute\n+     the difference and can switch to HOST_WIDE_INT.  */\n+  copy_size /= BITS_PER_UNIT;\n+  ref_size /= BITS_PER_UNIT;\n+\n   /* If COPY starts before REF, then reset the beginning of\n      COPY to match REF and decrease the size of COPY by the\n      number of bytes removed from COPY.  */\n-  if (maybe_lt (copy->offset, ref->offset))\n+  if (maybe_lt (copy_offset, ref_offset))\n     {\n-      poly_int64 diff = ref->offset - copy->offset;\n-      if (maybe_le (copy->size, diff))\n+      if (!(ref_offset - copy_offset).is_constant (&diff)\n+\t  || copy_size < diff / BITS_PER_UNIT)\n \treturn false;\n-      copy->size -= diff;\n-      copy->offset = ref->offset;\n+      copy_size -= diff / BITS_PER_UNIT;\n+      copy_offset = ref_offset;\n     }\n \n-  poly_int64 diff = copy->offset - ref->offset;\n-  if (maybe_le (ref->size, diff))\n+  if (!(copy_offset - ref_offset).is_constant (&diff)\n+      || ref_size <= diff / BITS_PER_UNIT)\n     return false;\n \n   /* If COPY extends beyond REF, chop off its size appropriately.  */\n-  poly_int64 limit = ref->size - diff;\n-  if (!ordered_p (limit, copy->size))\n-    return false;\n+  HOST_WIDE_INT limit = ref_size - diff / BITS_PER_UNIT;\n \n-  if (maybe_gt (copy->size, limit))\n-    copy->size = limit;\n+  if (copy_size > limit)\n+    copy_size = limit;\n+  *ret_size = copy_size;\n+  if (!(copy_offset - ref_offset).is_constant (ret_offset))\n+    return false;\n+  *ret_offset /= BITS_PER_UNIT;\n   return true;\n }\n \n /* Update LIVE_BYTES tracking REF for write to WRITE:\n    Verify we have the same base memory address, the write\n    has a known size and overlaps with REF.  */\n static void\n-clear_live_bytes_for_ref (sbitmap live_bytes, ao_ref *ref, ao_ref write)\n+clear_live_bytes_for_ref (sbitmap live_bytes, ao_ref *ref, ao_ref *write)\n {\n   HOST_WIDE_INT start, size;\n \n-  if (valid_ao_ref_for_dse (&write)\n-      && operand_equal_p (write.base, ref->base, OEP_ADDRESS_OF)\n-      && known_eq (write.size, write.max_size)\n-      /* normalize_ref modifies write and for that reason write is not\n-\t passed by reference.  */\n-      && normalize_ref (&write, ref)\n-      && (write.offset - ref->offset).is_constant (&start)\n-      && write.size.is_constant (&size))\n-    bitmap_clear_range (live_bytes, start / BITS_PER_UNIT,\n-\t\t\tsize / BITS_PER_UNIT);\n+  if (valid_ao_ref_kill_for_dse (write)\n+      && operand_equal_p (write->base, ref->base, OEP_ADDRESS_OF)\n+      && get_byte_range (write, ref, true, &start, &size))\n+    bitmap_clear_range (live_bytes, start, size);\n }\n \n /* Clear any bytes written by STMT from the bitmap LIVE_BYTES.  The base\n@@ -250,12 +324,12 @@ clear_bytes_written_by (sbitmap live_bytes, gimple *stmt, ao_ref *ref)\n       if (summary && !interposed)\n \tfor (auto kill : summary->kills)\n \t  if (kill.get_ao_ref (as_a <gcall *> (stmt), &write))\n-\t    clear_live_bytes_for_ref (live_bytes, ref, write);\n+\t    clear_live_bytes_for_ref (live_bytes, ref, &write);\n     }\n   if (!initialize_ao_ref_for_dse (stmt, &write))\n     return;\n \n-  clear_live_bytes_for_ref (live_bytes, ref, write);\n+  clear_live_bytes_for_ref (live_bytes, ref, &write);\n }\n \n /* REF is a memory write.  Extract relevant information from it and\n@@ -267,9 +341,11 @@ setup_live_bytes_from_ref (ao_ref *ref, sbitmap live_bytes)\n {\n   HOST_WIDE_INT const_size;\n   if (valid_ao_ref_for_dse (ref)\n-      && ref->size.is_constant (&const_size)\n-      && (const_size / BITS_PER_UNIT\n-\t  <= param_dse_max_object_size))\n+      && ((aligned_upper_bound (ref->offset + ref->max_size, BITS_PER_UNIT)\n+\t   - aligned_lower_bound (ref->offset,\n+\t\t\t\t  BITS_PER_UNIT)).is_constant (&const_size))\n+      && (const_size / BITS_PER_UNIT <= param_dse_max_object_size)\n+      && const_size > 1)\n     {\n       bitmap_clear (live_bytes);\n       bitmap_set_range (live_bytes, 0, const_size / BITS_PER_UNIT);\n@@ -645,24 +721,21 @@ maybe_trim_partially_dead_store (ao_ref *ref, sbitmap live, gimple *stmt)\n    location.  So callers do not see those modifications.  */\n \n static bool\n-live_bytes_read (ao_ref use_ref, ao_ref *ref, sbitmap live)\n+live_bytes_read (ao_ref *use_ref, ao_ref *ref, sbitmap live)\n {\n   /* We have already verified that USE_REF and REF hit the same object.\n      Now verify that there's actually an overlap between USE_REF and REF.  */\n   HOST_WIDE_INT start, size;\n-  if (normalize_ref (&use_ref, ref)\n-      && (use_ref.offset - ref->offset).is_constant (&start)\n-      && use_ref.size.is_constant (&size))\n+  if (get_byte_range (use_ref, ref, false, &start, &size))\n     {\n       /* If USE_REF covers all of REF, then it will hit one or more\n \t live bytes.   This avoids useless iteration over the bitmap\n \t below.  */\n-      if (start == 0 && known_eq (size, ref->size))\n+      if (start == 0 && known_eq (size * 8, ref->size))\n \treturn true;\n \n       /* Now check if any of the remaining bits in use_ref are set in LIVE.  */\n-      return bitmap_bit_in_range_p (live, start / BITS_PER_UNIT,\n-\t\t\t\t    (start + size - 1) / BITS_PER_UNIT);\n+      return bitmap_bit_in_range_p (live, start, (start + size - 1));\n     }\n   return true;\n }\n@@ -861,16 +934,18 @@ dse_classify_store (ao_ref *ref, gimple *stmt,\n \t    {\n \t      /* Handle common cases where we can easily build an ao_ref\n \t\t structure for USE_STMT and in doing so we find that the\n-\t\t references hit non-live bytes and thus can be ignored.  */\n+\t\t references hit non-live bytes and thus can be ignored.\n+\n+\t\t TODO: We can also use modref summary to handle calls.  */\n \t      if (byte_tracking_enabled\n \t\t  && is_gimple_assign (use_stmt))\n \t\t{\n \t\t  ao_ref use_ref;\n \t\t  ao_ref_init (&use_ref, gimple_assign_rhs1 (use_stmt));\n \t\t  if (valid_ao_ref_for_dse (&use_ref)\n-\t\t      && use_ref.base == ref->base\n-\t\t      && known_eq (use_ref.size, use_ref.max_size)\n-\t\t      && !live_bytes_read (use_ref, ref, live_bytes))\n+\t\t      && operand_equal_p (use_ref.base, ref->base,\n+\t\t\t\t\t  OEP_ADDRESS_OF)\n+\t\t      && !live_bytes_read (&use_ref, ref, live_bytes))\n \t\t    {\n \t\t      /* If this is a store, remember it as we possibly\n \t\t\t need to walk the defs uses.  */"}]}