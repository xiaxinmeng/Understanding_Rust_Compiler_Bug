{"sha": "f837785cc430f1924ec2842bf559fe602d76e5e3", "node_id": "C_kwDOANBUbNoAKGY4Mzc3ODVjYzQzMGYxOTI0ZWMyODQyYmY1NTlmZTYwMmQ3NmU1ZTM", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2021-10-13T18:32:36Z"}, "committer": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2021-10-14T10:57:51Z"}, "message": "aarch64: Remove redundant flag_vect_cost_model test\n\nThe aarch64 version of add_stmt_cost has a redundant test\nof flag_vect_cost_model.  The current structure was based\non the contemporaneous definition of default_add_stmt_cost,\nbut g:d6d1127249564146429009e0682f25bd58d7a791 later removed\nthe flag_vect_cost_model test from the default version.\n\ngcc/\n\t* config/aarch64/aarch64.c (aarch64_add_stmt_cost): Remove\n\tredundant test for flag_vect_cost_model.", "tree": {"sha": "f58938a591b4504d7ab29b010fdabd314c7ba4a2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/f58938a591b4504d7ab29b010fdabd314c7ba4a2"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/f837785cc430f1924ec2842bf559fe602d76e5e3", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f837785cc430f1924ec2842bf559fe602d76e5e3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f837785cc430f1924ec2842bf559fe602d76e5e3", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f837785cc430f1924ec2842bf559fe602d76e5e3/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "3d0a7271b383c95d5c8dc9647966517a61e71abb", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3d0a7271b383c95d5c8dc9647966517a61e71abb", "html_url": "https://github.com/Rust-GCC/gccrs/commit/3d0a7271b383c95d5c8dc9647966517a61e71abb"}], "stats": {"total": 214, "additions": 105, "deletions": 109}, "files": [{"sha": "76d99d247ae460198366a4380b17f0880fec8179", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 105, "deletions": 109, "changes": 214, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f837785cc430f1924ec2842bf559fe602d76e5e3/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f837785cc430f1924ec2842bf559fe602d76e5e3/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=f837785cc430f1924ec2842bf559fe602d76e5e3", "patch": "@@ -15487,125 +15487,121 @@ aarch64_add_stmt_cost (class vec_info *vinfo, void *data, int count,\n \t\t       int misalign, enum vect_cost_model_location where)\n {\n   auto *costs = static_cast<aarch64_vector_costs *> (data);\n-  unsigned retval = 0;\n \n-  if (flag_vect_cost_model)\n-    {\n-      fractional_cost stmt_cost\n-\t= aarch64_builtin_vectorization_cost (kind, vectype, misalign);\n-\n-      bool in_inner_loop_p = (where == vect_body\n-\t\t\t      && stmt_info\n-\t\t\t      && stmt_in_inner_loop_p (vinfo, stmt_info));\n-\n-      /* Do one-time initialization based on the vinfo.  */\n-      loop_vec_info loop_vinfo = dyn_cast<loop_vec_info> (vinfo);\n-      bb_vec_info bb_vinfo = dyn_cast<bb_vec_info> (vinfo);\n-      if (!costs->analyzed_vinfo && aarch64_use_new_vector_costs_p ())\n-\t{\n-\t  if (loop_vinfo)\n-\t    aarch64_analyze_loop_vinfo (loop_vinfo, costs);\n-\t  else\n-\t    aarch64_analyze_bb_vinfo (bb_vinfo, costs);\n-\t  costs->analyzed_vinfo = true;\n-\t}\n+  fractional_cost stmt_cost\n+    = aarch64_builtin_vectorization_cost (kind, vectype, misalign);\n \n-      /* Try to get a more accurate cost by looking at STMT_INFO instead\n-\t of just looking at KIND.  */\n-      if (stmt_info && aarch64_use_new_vector_costs_p ())\n-\t{\n-\t  if (vectype && aarch64_sve_only_stmt_p (stmt_info, vectype))\n-\t    costs->saw_sve_only_op = true;\n-\n-\t  /* If we scalarize a strided store, the vectorizer costs one\n-\t     vec_to_scalar for each element.  However, we can store the first\n-\t     element using an FP store without a separate extract step.  */\n-\t  if (vect_is_store_elt_extraction (kind, stmt_info))\n-\t    count -= 1;\n-\n-\t  stmt_cost = aarch64_detect_scalar_stmt_subtype\n-\t    (vinfo, kind, stmt_info, stmt_cost);\n+  bool in_inner_loop_p = (where == vect_body\n+\t\t\t  && stmt_info\n+\t\t\t  && stmt_in_inner_loop_p (vinfo, stmt_info));\n \n-\t  if (vectype && costs->vec_flags)\n-\t    stmt_cost = aarch64_detect_vector_stmt_subtype (vinfo, kind,\n-\t\t\t\t\t\t\t    stmt_info, vectype,\n-\t\t\t\t\t\t\t    where, stmt_cost);\n-\t}\n-\n-      /* Do any SVE-specific adjustments to the cost.  */\n-      if (stmt_info && vectype && aarch64_sve_mode_p (TYPE_MODE (vectype)))\n-\tstmt_cost = aarch64_sve_adjust_stmt_cost (vinfo, kind, stmt_info,\n-\t\t\t\t\t\t  vectype, stmt_cost);\n-\n-      if (stmt_info && aarch64_use_new_vector_costs_p ())\n-\t{\n-\t  /* Account for any extra \"embedded\" costs that apply additively\n-\t     to the base cost calculated above.  */\n-\t  stmt_cost = aarch64_adjust_stmt_cost (kind, stmt_info, vectype,\n-\t\t\t\t\t\tstmt_cost);\n-\n-\t  /* If we're recording a nonzero vector loop body cost for the\n-\t     innermost loop, also estimate the operations that would need\n-\t     to be issued by all relevant implementations of the loop.  */\n-\t  auto *issue_info = aarch64_tune_params.vec_costs->issue_info;\n-\t  if (loop_vinfo\n-\t      && issue_info\n-\t      && costs->vec_flags\n-\t      && where == vect_body\n-\t      && (!LOOP_VINFO_LOOP (loop_vinfo)->inner || in_inner_loop_p)\n-\t      && vectype\n-\t      && stmt_cost != 0)\n+  /* Do one-time initialization based on the vinfo.  */\n+  loop_vec_info loop_vinfo = dyn_cast<loop_vec_info> (vinfo);\n+  bb_vec_info bb_vinfo = dyn_cast<bb_vec_info> (vinfo);\n+  if (!costs->analyzed_vinfo && aarch64_use_new_vector_costs_p ())\n+    {\n+      if (loop_vinfo)\n+\taarch64_analyze_loop_vinfo (loop_vinfo, costs);\n+      else\n+\taarch64_analyze_bb_vinfo (bb_vinfo, costs);\n+      costs->analyzed_vinfo = true;\n+    }\n+\n+  /* Try to get a more accurate cost by looking at STMT_INFO instead\n+     of just looking at KIND.  */\n+  if (stmt_info && aarch64_use_new_vector_costs_p ())\n+    {\n+      if (vectype && aarch64_sve_only_stmt_p (stmt_info, vectype))\n+\tcosts->saw_sve_only_op = true;\n+\n+      /* If we scalarize a strided store, the vectorizer costs one\n+\t vec_to_scalar for each element.  However, we can store the first\n+\t element using an FP store without a separate extract step.  */\n+      if (vect_is_store_elt_extraction (kind, stmt_info))\n+\tcount -= 1;\n+\n+      stmt_cost = aarch64_detect_scalar_stmt_subtype\n+\t(vinfo, kind, stmt_info, stmt_cost);\n+\n+      if (vectype && costs->vec_flags)\n+\tstmt_cost = aarch64_detect_vector_stmt_subtype (vinfo, kind,\n+\t\t\t\t\t\t\tstmt_info, vectype,\n+\t\t\t\t\t\t\twhere, stmt_cost);\n+    }\n+\n+  /* Do any SVE-specific adjustments to the cost.  */\n+  if (stmt_info && vectype && aarch64_sve_mode_p (TYPE_MODE (vectype)))\n+    stmt_cost = aarch64_sve_adjust_stmt_cost (vinfo, kind, stmt_info,\n+\t\t\t\t\t      vectype, stmt_cost);\n+\n+  if (stmt_info && aarch64_use_new_vector_costs_p ())\n+    {\n+      /* Account for any extra \"embedded\" costs that apply additively\n+\t to the base cost calculated above.  */\n+      stmt_cost = aarch64_adjust_stmt_cost (kind, stmt_info, vectype,\n+\t\t\t\t\t    stmt_cost);\n+\n+      /* If we're recording a nonzero vector loop body cost for the\n+\t innermost loop, also estimate the operations that would need\n+\t to be issued by all relevant implementations of the loop.  */\n+      auto *issue_info = aarch64_tune_params.vec_costs->issue_info;\n+      if (loop_vinfo\n+\t  && issue_info\n+\t  && costs->vec_flags\n+\t  && where == vect_body\n+\t  && (!LOOP_VINFO_LOOP (loop_vinfo)->inner || in_inner_loop_p)\n+\t  && vectype\n+\t  && stmt_cost != 0)\n+\t{\n+\t  /* Record estimates for the scalar code.  */\n+\t  aarch64_count_ops (vinfo, costs, count, kind, stmt_info, vectype,\n+\t\t\t     0, &costs->scalar_ops, issue_info->scalar,\n+\t\t\t     vect_nunits_for_cost (vectype));\n+\n+\t  if (aarch64_sve_mode_p (vinfo->vector_mode) && issue_info->sve)\n \t    {\n-\t      /* Record estimates for the scalar code.  */\n-\t      aarch64_count_ops (vinfo, costs, count, kind, stmt_info, vectype,\n-\t\t\t\t 0, &costs->scalar_ops, issue_info->scalar,\n-\t\t\t\t vect_nunits_for_cost (vectype));\n-\n-\t      if (aarch64_sve_mode_p (vinfo->vector_mode) && issue_info->sve)\n-\t\t{\n-\t\t  /* Record estimates for a possible Advanced SIMD version\n-\t\t     of the SVE code.  */\n-\t\t  aarch64_count_ops (vinfo, costs, count, kind, stmt_info,\n-\t\t\t\t     vectype, VEC_ADVSIMD, &costs->advsimd_ops,\n-\t\t\t\t     issue_info->advsimd,\n-\t\t\t\t     aarch64_estimated_sve_vq ());\n-\n-\t\t  /* Record estimates for the SVE code itself.  */\n-\t\t  aarch64_count_ops (vinfo, costs, count, kind, stmt_info,\n-\t\t\t\t     vectype, VEC_ANY_SVE, &costs->sve_ops,\n-\t\t\t\t     issue_info->sve, 1);\n-\t\t}\n-\t      else\n-\t\t/* Record estimates for the Advanced SIMD code.  Treat SVE like\n-\t\t   Advanced SIMD if the CPU has no specific SVE costs.  */\n-\t\taarch64_count_ops (vinfo, costs, count, kind, stmt_info,\n-\t\t\t\t   vectype, VEC_ADVSIMD, &costs->advsimd_ops,\n-\t\t\t\t   issue_info->advsimd, 1);\n+\t      /* Record estimates for a possible Advanced SIMD version\n+\t\t of the SVE code.  */\n+\t      aarch64_count_ops (vinfo, costs, count, kind, stmt_info,\n+\t\t\t\t vectype, VEC_ADVSIMD, &costs->advsimd_ops,\n+\t\t\t\t issue_info->advsimd,\n+\t\t\t\t aarch64_estimated_sve_vq ());\n+\n+\t      /* Record estimates for the SVE code itself.  */\n+\t      aarch64_count_ops (vinfo, costs, count, kind, stmt_info,\n+\t\t\t\t vectype, VEC_ANY_SVE, &costs->sve_ops,\n+\t\t\t\t issue_info->sve, 1);\n \t    }\n-\n-\t  /* If we're applying the SVE vs. Advanced SIMD unrolling heuristic,\n-\t     estimate the number of statements in the unrolled Advanced SIMD\n-\t     loop.  For simplicitly, we assume that one iteration of the\n-\t     Advanced SIMD loop would need the same number of statements\n-\t     as one iteration of the SVE loop.  */\n-\t  if (where == vect_body && costs->unrolled_advsimd_niters)\n-\t    costs->unrolled_advsimd_stmts\n-\t      += count * costs->unrolled_advsimd_niters;\n+\t  else\n+\t    /* Record estimates for the Advanced SIMD code.  Treat SVE like\n+\t       Advanced SIMD if the CPU has no specific SVE costs.  */\n+\t    aarch64_count_ops (vinfo, costs, count, kind, stmt_info,\n+\t\t\t       vectype, VEC_ADVSIMD, &costs->advsimd_ops,\n+\t\t\t       issue_info->advsimd, 1);\n \t}\n \n-      /* Statements in an inner loop relative to the loop being\n-\t vectorized are weighted more heavily.  The value here is\n-\t arbitrary and could potentially be improved with analysis.  */\n-      if (in_inner_loop_p)\n-\t{\n-\t  gcc_assert (loop_vinfo);\n-\t  count *= LOOP_VINFO_INNER_LOOP_COST_FACTOR (loop_vinfo); /*  FIXME  */\n-\t}\n+      /* If we're applying the SVE vs. Advanced SIMD unrolling heuristic,\n+\t estimate the number of statements in the unrolled Advanced SIMD\n+\t loop.  For simplicitly, we assume that one iteration of the\n+\t Advanced SIMD loop would need the same number of statements\n+\t as one iteration of the SVE loop.  */\n+      if (where == vect_body && costs->unrolled_advsimd_niters)\n+\tcosts->unrolled_advsimd_stmts\n+\t  += count * costs->unrolled_advsimd_niters;\n+    }\n \n-      retval = (count * stmt_cost).ceil ();\n-      costs->region[where] += retval;\n+  /* Statements in an inner loop relative to the loop being\n+     vectorized are weighted more heavily.  The value here is\n+     arbitrary and could potentially be improved with analysis.  */\n+  if (in_inner_loop_p)\n+    {\n+      gcc_assert (loop_vinfo);\n+      count *= LOOP_VINFO_INNER_LOOP_COST_FACTOR (loop_vinfo); /*  FIXME  */\n     }\n \n+  unsigned retval = (count * stmt_cost).ceil ();\n+  costs->region[where] += retval;\n+\n   return retval;\n }\n "}]}