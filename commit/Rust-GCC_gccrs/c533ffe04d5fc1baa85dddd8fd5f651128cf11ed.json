{"sha": "c533ffe04d5fc1baa85dddd8fd5f651128cf11ed", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YzUzM2ZmZTA0ZDVmYzFiYWE4NWRkZGQ4ZmQ1ZjY1MTEyOGNmMTFlZA==", "commit": {"author": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2019-06-03T23:07:54Z"}, "committer": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2019-06-03T23:07:54Z"}, "message": "libgo: delay applying profile stack-frame skip until fixup\n    \n    When the runtime collects a stack trace to associate it with some\n    profiling event (mem alloc, mutex, etc) there is a skip count passed\n    to runtime.Callers (or equivalent) to skip some known count of frames\n    in order to get to the \"interesting\" frame corresponding to the\n    profile event. Now that the profiling mechanism uses lazy fixup (when\n    removing compiler artifacts like thunks, morestack calls etc), we also\n    need to move the frame skipping logic after the fixup, so as to insure\n    that the skip count isn't thrown off by these artifacts.\n    \n    Fixes golang/go#32290.\n    \n    Reviewed-on: https://go-review.googlesource.com/c/gofrontend/+/179740\n\nFrom-SVN: r271892", "tree": {"sha": "9b0df7f8a2130484fd7458745c8ec0361f3f9850", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/9b0df7f8a2130484fd7458745c8ec0361f3f9850"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/c533ffe04d5fc1baa85dddd8fd5f651128cf11ed", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c533ffe04d5fc1baa85dddd8fd5f651128cf11ed", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c533ffe04d5fc1baa85dddd8fd5f651128cf11ed", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c533ffe04d5fc1baa85dddd8fd5f651128cf11ed/comments", "author": null, "committer": null, "parents": [{"sha": "3e6f8fe1bc4c60859113bca7970b0b8db56eb442", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3e6f8fe1bc4c60859113bca7970b0b8db56eb442", "html_url": "https://github.com/Rust-GCC/gccrs/commit/3e6f8fe1bc4c60859113bca7970b0b8db56eb442"}], "stats": {"total": 56, "additions": 32, "deletions": 24}, "files": [{"sha": "1b2fd70bc56d2e7f81369ce7649b90ea222865ca", "filename": "gcc/go/gofrontend/MERGE", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/c533ffe04d5fc1baa85dddd8fd5f651128cf11ed/gcc%2Fgo%2Fgofrontend%2FMERGE", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/c533ffe04d5fc1baa85dddd8fd5f651128cf11ed/gcc%2Fgo%2Fgofrontend%2FMERGE", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2FMERGE?ref=c533ffe04d5fc1baa85dddd8fd5f651128cf11ed", "patch": "@@ -1,4 +1,4 @@\n-37a47e4691b4602dd167f82c64a6569019584a80\n+951c83af46375019b2fe262635746368a6b9c4ba\n \n The first line of this file holds the git revision number of the last\n merge done from the gofrontend repository."}, {"sha": "132c2ff5c210ea366a0b52ad1d3c0cb18134bee4", "filename": "libgo/go/runtime/mprof.go", "status": "modified", "additions": 27, "deletions": 11, "changes": 38, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/c533ffe04d5fc1baa85dddd8fd5f651128cf11ed/libgo%2Fgo%2Fruntime%2Fmprof.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/c533ffe04d5fc1baa85dddd8fd5f651128cf11ed/libgo%2Fgo%2Fruntime%2Fmprof.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmprof.go?ref=c533ffe04d5fc1baa85dddd8fd5f651128cf11ed", "patch": "@@ -56,6 +56,7 @@ type bucket struct {\n \thash    uintptr\n \tsize    uintptr\n \tnstk    uintptr\n+\tskip    int\n }\n \n // A memRecord is the bucket data for a bucket of type memProfile,\n@@ -185,7 +186,7 @@ func max(x, y uintptr) uintptr {\n }\n \n // newBucket allocates a bucket with the given type and number of stack entries.\n-func newBucket(typ bucketType, nstk int) *bucket {\n+func newBucket(typ bucketType, nstk int, skipCount int) *bucket {\n \tsize := payloadOffset(typ, uintptr(nstk))\n \tswitch typ {\n \tdefault:\n@@ -203,6 +204,7 @@ func newBucket(typ bucketType, nstk int) *bucket {\n \tbucketmem += size\n \tb.typ = typ\n \tb.nstk = uintptr(nstk)\n+\tb.skip = skipCount\n \treturn b\n }\n \n@@ -229,7 +231,7 @@ func (b *bucket) bp() *blockRecord {\n }\n \n // Return the bucket for stk[0:nstk], allocating new bucket if needed.\n-func stkbucket(typ bucketType, size uintptr, stk []uintptr, alloc bool) *bucket {\n+func stkbucket(typ bucketType, size uintptr, skip int, stk []uintptr, alloc bool) *bucket {\n \tif buckhash == nil {\n \t\tbuckhash = (*[buckHashSize]*bucket)(sysAlloc(unsafe.Sizeof(*buckhash), &memstats.buckhash_sys))\n \t\tif buckhash == nil {\n@@ -264,7 +266,7 @@ func stkbucket(typ bucketType, size uintptr, stk []uintptr, alloc bool) *bucket\n \t}\n \n \t// Create new bucket.\n-\tb := newBucket(typ, len(stk))\n+\tb := newBucket(typ, len(stk), skip)\n \tcopy(b.stk(), stk)\n \tb.hash = h\n \tb.size = size\n@@ -369,9 +371,10 @@ func mProf_PostSweep() {\n // Called by malloc to record a profiled block.\n func mProf_Malloc(p unsafe.Pointer, size uintptr) {\n \tvar stk [maxStack]uintptr\n-\tnstk := callersRaw(1, stk[:])\n+\tnstk := callersRaw(stk[:])\n \tlock(&proflock)\n-\tb := stkbucket(memProfile, size, stk[:nstk], true)\n+\tskip := 1\n+\tb := stkbucket(memProfile, size, skip, stk[:nstk], true)\n \tc := mProf.cycle\n \tmp := b.mp()\n \tmpc := &mp.future[(c+2)%uint32(len(mp.future))]\n@@ -446,14 +449,14 @@ func saveblockevent(cycles int64, skip int, which bucketType) {\n \tvar nstk int\n \tvar stk [maxStack]uintptr\n \tif gp.m.curg == nil || gp.m.curg == gp {\n-\t\tnstk = callersRaw(skip, stk[:])\n+\t\tnstk = callersRaw(stk[:])\n \t} else {\n \t\t// FIXME: This should get a traceback of gp.m.curg.\n \t\t// nstk = gcallers(gp.m.curg, skip, stk[:])\n-\t\tnstk = callersRaw(skip, stk[:])\n+\t\tnstk = callersRaw(stk[:])\n \t}\n \tlock(&proflock)\n-\tb := stkbucket(which, 0, stk[:nstk], true)\n+\tb := stkbucket(which, 0, skip, stk[:nstk], true)\n \tb.bp().count++\n \tb.bp().cycles += cycles\n \tunlock(&proflock)\n@@ -605,9 +608,12 @@ func freebucket(tofree *bucket) *bucket {\n // later. Note: there is code in go-callers.c's backtrace_full callback()\n // function that performs very similar fixups; these two code paths\n // should be kept in sync.\n-func fixupStack(stk []uintptr, canonStack *[maxStack]uintptr, size uintptr) int {\n+func fixupStack(stk []uintptr, skip int, canonStack *[maxStack]uintptr, size uintptr) int {\n \tvar cidx int\n \tvar termTrace bool\n+\t// Increase the skip count to take into account the frames corresponding\n+\t// to runtime.callersRaw and to the C routine that it invokes.\n+\tskip += 2\n \tfor _, pc := range stk {\n \t\t// Subtract 1 from PC to undo the 1 we added in callback in\n \t\t// go-callers.c.\n@@ -669,6 +675,16 @@ func fixupStack(stk []uintptr, canonStack *[maxStack]uintptr, size uintptr) int\n \t\t\tbreak\n \t\t}\n \t}\n+\n+\t// Apply skip count. Needs to be done after expanding inline frames.\n+\tif skip != 0 {\n+\t\tif skip >= cidx {\n+\t\t\treturn 0\n+\t\t}\n+\t\tcopy(canonStack[:cidx-skip], canonStack[skip:])\n+\t\treturn cidx - skip\n+\t}\n+\n \treturn cidx\n }\n \n@@ -680,8 +696,8 @@ func fixupStack(stk []uintptr, canonStack *[maxStack]uintptr, size uintptr) int\n // the new bucket.\n func fixupBucket(b *bucket) {\n \tvar canonStack [maxStack]uintptr\n-\tframes := fixupStack(b.stk(), &canonStack, b.size)\n-\tcb := stkbucket(prunedProfile, b.size, canonStack[:frames], true)\n+\tframes := fixupStack(b.stk(), b.skip, &canonStack, b.size)\n+\tcb := stkbucket(prunedProfile, b.size, 0, canonStack[:frames], true)\n \tswitch b.typ {\n \tdefault:\n \t\tthrow(\"invalid profile bucket type\")"}, {"sha": "4134d28a599c49a7c69fbc02c388c87d63ceaa6b", "filename": "libgo/go/runtime/traceback_gccgo.go", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/c533ffe04d5fc1baa85dddd8fd5f651128cf11ed/libgo%2Fgo%2Fruntime%2Ftraceback_gccgo.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/c533ffe04d5fc1baa85dddd8fd5f651128cf11ed/libgo%2Fgo%2Fruntime%2Ftraceback_gccgo.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Ftraceback_gccgo.go?ref=c533ffe04d5fc1baa85dddd8fd5f651128cf11ed", "patch": "@@ -63,11 +63,11 @@ func callers(skip int, locbuf []location) int {\n \n //go:noescape\n //extern runtime_callersRaw\n-func c_callersRaw(skip int32, pcs *uintptr, max int32) int32\n+func c_callersRaw(pcs *uintptr, max int32) int32\n \n // callersRaw returns a raw (PCs only) stack trace of the current goroutine.\n-func callersRaw(skip int, pcbuf []uintptr) int {\n-\tn := c_callersRaw(int32(skip)+1, &pcbuf[0], int32(len(pcbuf)))\n+func callersRaw(pcbuf []uintptr) int {\n+\tn := c_callersRaw(&pcbuf[0], int32(len(pcbuf)))\n \treturn int(n)\n }\n "}, {"sha": "e7d53a32a5fb8a7d9b17b5adf0e3dbcdc3a74cfc", "filename": "libgo/runtime/go-callers.c", "status": "modified", "additions": 1, "deletions": 9, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/c533ffe04d5fc1baa85dddd8fd5f651128cf11ed/libgo%2Fruntime%2Fgo-callers.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/c533ffe04d5fc1baa85dddd8fd5f651128cf11ed/libgo%2Fruntime%2Fgo-callers.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-callers.c?ref=c533ffe04d5fc1baa85dddd8fd5f651128cf11ed", "patch": "@@ -268,7 +268,6 @@ Callers (intgo skip, struct __go_open_array pc)\n struct callersRaw_data\n {\n   uintptr* pcbuf;\n-  int skip;\n   int index;\n   int max;\n };\n@@ -280,12 +279,6 @@ static int callback_raw (void *data, uintptr_t pc)\n {\n   struct callersRaw_data *arg = (struct callersRaw_data *) data;\n \n-  if (arg->skip > 0)\n-    {\n-      --arg->skip;\n-      return 0;\n-    }\n-\n   /* On the call to backtrace_simple the pc value was most likely\n      decremented if there was a normal call, since the pc referred to\n      the instruction where the call returned and not the call itself.\n@@ -306,13 +299,12 @@ static int callback_raw (void *data, uintptr_t pc)\n /* runtime_callersRaw is similar to runtime_callers() above, but\n    it returns raw PC values as opposed to file/func/line locations. */\n int32\n-runtime_callersRaw (int32 skip, uintptr *pcbuf, int32 m)\n+runtime_callersRaw (uintptr *pcbuf, int32 m)\n {\n   struct callersRaw_data data;\n   struct backtrace_state* state;\n \n   data.pcbuf = pcbuf;\n-  data.skip = skip + 1;\n   data.index = 0;\n   data.max = m;\n   runtime_xadd (&__go_runtime_in_callers, 1);"}]}