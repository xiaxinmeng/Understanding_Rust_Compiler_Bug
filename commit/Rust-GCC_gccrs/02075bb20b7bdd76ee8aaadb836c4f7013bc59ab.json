{"sha": "02075bb20b7bdd76ee8aaadb836c4f7013bc59ab", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MDIwNzViYjIwYjdiZGQ3NmVlOGFhYWRiODM2YzRmNzAxM2JjNTlhYg==", "commit": {"author": {"name": "Diego Novillo", "email": "dnovillo@gcc.gnu.org", "date": "2006-03-01T16:07:47Z"}, "committer": {"name": "Diego Novillo", "email": "dnovillo@gcc.gnu.org", "date": "2006-03-01T16:07:47Z"}, "message": "tree-ssa-operands.c: Cleanup whitespace.\n\n\n\t* tree-ssa-operands.c: Cleanup whitespace.\n\t(get_asm_expr_operands): Move before first invocation.\n\t(get_indirect_ref_operands): Likewise.\n\t(get_tmr_operands): Likewise.\n\t(get_call_expr_operands): Likewise.\n\t(append_def): Likewise.\n\t(append_use): Likewise.\n\t(append_v_may_def): Likewise.\n\t(append_v_must_def): Likewise.\n\t(add_call_clobber_ops): Likewise.\n\t(add_call_read_ops): Likewise.\n\t(add_stmt_operand): Likewise.\n\t(add_virtual_operand): Likewise.\n\t(build_ssa_operands): Likewise.\n\nFrom-SVN: r111604", "tree": {"sha": "92170abb907d0ebc4510192ace96dead33fca9b7", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/92170abb907d0ebc4510192ace96dead33fca9b7"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/02075bb20b7bdd76ee8aaadb836c4f7013bc59ab", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/02075bb20b7bdd76ee8aaadb836c4f7013bc59ab", "html_url": "https://github.com/Rust-GCC/gccrs/commit/02075bb20b7bdd76ee8aaadb836c4f7013bc59ab", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/02075bb20b7bdd76ee8aaadb836c4f7013bc59ab/comments", "author": null, "committer": null, "parents": [{"sha": "2bb6e0cefeba590b744da9fe75a299f2deacff87", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2bb6e0cefeba590b744da9fe75a299f2deacff87", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2bb6e0cefeba590b744da9fe75a299f2deacff87"}], "stats": {"total": 2146, "additions": 1081, "deletions": 1065}, "files": [{"sha": "f59ad08d1eb662583476ceb1e5bd124128c96554", "filename": "gcc/ChangeLog", "status": "modified", "additions": 18, "deletions": 1, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/02075bb20b7bdd76ee8aaadb836c4f7013bc59ab/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/02075bb20b7bdd76ee8aaadb836c4f7013bc59ab/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=02075bb20b7bdd76ee8aaadb836c4f7013bc59ab", "patch": "@@ -1,4 +1,21 @@\n-2006-01-23  Andrew Pinski  <pinskia@physics.uc.edu>\n+2006-03-01  Diego Novillo  <dnovillo@redhat.com>\n+\n+\t* tree-ssa-operands.c: Cleanup whitespace.\n+\t(get_asm_expr_operands): Move before first invocation.\n+\t(get_indirect_ref_operands): Likewise.\n+\t(get_tmr_operands): Likewise.\n+\t(get_call_expr_operands): Likewise.\n+\t(append_def): Likewise.\n+\t(append_use): Likewise.\n+\t(append_v_may_def): Likewise.\n+\t(append_v_must_def): Likewise.\n+\t(add_call_clobber_ops): Likewise.\n+\t(add_call_read_ops): Likewise.\n+\t(add_stmt_operand): Likewise.\n+\t(add_virtual_operand): Likewise.\n+\t(build_ssa_operands): Likewise.\n+\n+2006-03-01  Andrew Pinski  <pinskia@physics.uc.edu>\n \n \tPR middle-end/26022\n \tRevert:"}, {"sha": "39c74ec1205de3ff008cd2ab9015d27284d526cb", "filename": "gcc/tree-ssa-operands.c", "status": "modified", "additions": 1063, "deletions": 1064, "changes": 2127, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/02075bb20b7bdd76ee8aaadb836c4f7013bc59ab/gcc%2Ftree-ssa-operands.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/02075bb20b7bdd76ee8aaadb836c4f7013bc59ab/gcc%2Ftree-ssa-operands.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-operands.c?ref=02075bb20b7bdd76ee8aaadb836c4f7013bc59ab", "patch": "@@ -74,10 +74,7 @@ Boston, MA 02110-1301, USA.  */\n \n   i.e., if a stmt had a VUSE of 'a_5', and 'a' occurs in the new operand \n   vector for VUSE, then the new vector will also be modified such that \n-  it contains 'a_5' rather than 'a'.\n-\n-*/\n-\n+  it contains 'a_5' rather than 'a'.  */\n \n /* Flags to describe operand properties in helpers.  */\n \n@@ -103,7 +100,6 @@ Boston, MA 02110-1301, USA.  */\n    to distinguish \"reset the world\" events from explicit MODIFY_EXPRs.  */\n #define opf_non_specific  (1 << 3)\n \n-\n /* Array for building all the def operands.  */\n static VEC(tree,heap) *build_defs;\n \n@@ -119,31 +115,14 @@ static VEC(tree,heap) *build_vuses;\n /* Array for building all the v_must_def operands.  */\n static VEC(tree,heap) *build_v_must_defs;\n \n-\n /* These arrays are the cached operand vectors for call clobbered calls.  */\n static bool ops_active = false;\n \n static GTY (()) struct ssa_operand_memory_d *operand_memory = NULL;\n static unsigned operand_memory_index;\n \n static void get_expr_operands (tree, tree *, int);\n-static void get_asm_expr_operands (tree);\n-static void get_indirect_ref_operands (tree, tree, int, tree, HOST_WIDE_INT,\n-\t\t\t\t       HOST_WIDE_INT, bool);\n-static void get_tmr_operands (tree, tree, int);\n-static void get_call_expr_operands (tree, tree);\n-static inline void append_def (tree *);\n-static inline void append_use (tree *);\n-static void append_v_may_def (tree);\n-static void append_v_must_def (tree);\n-static void add_call_clobber_ops (tree, tree);\n-static void add_call_read_ops (tree, tree);\n-static void add_stmt_operand (tree *, stmt_ann_t, int);\n-static void add_virtual_operand (tree, stmt_ann_t, int, tree,\n-\t\t\t\t HOST_WIDE_INT, HOST_WIDE_INT, \n-\t\t\t\t bool);\n-static void build_ssa_operands (tree stmt);\n-                                                                                \n+\n static def_optype_p free_defs = NULL;\n static use_optype_p free_uses = NULL;\n static vuse_optype_p free_vuses = NULL;\n@@ -162,6 +141,7 @@ get_name_decl (tree t)\n     return DECL_UID (SSA_NAME_VAR (t));\n }\n \n+\n /* Comparison function for qsort used in operand_build_sort_virtual.  */\n \n static int\n@@ -181,6 +161,7 @@ operand_build_cmp (const void *p, const void *q)\n   return (u1 > u2 ? 1 : -1);\n }\n \n+\n /* Sort the virtual operands in LIST from lowest DECL_UID to highest.  */\n \n static inline void\n@@ -209,7 +190,6 @@ operand_build_sort_virtual (VEC(tree,heap) *list)\n }\n \n \n-\n /*  Return true if the ssa operands cache is active.  */\n \n bool\n@@ -218,8 +198,10 @@ ssa_operands_active (void)\n   return ops_active;\n }\n \n+\n /* Structure storing statistics on how many call clobbers we have, and\n    how many where avoided.  */\n+\n static struct \n {\n   /* Number of call-clobbered ops we attempt to add to calls in\n@@ -246,6 +228,7 @@ static struct\n   unsigned int static_readonly_clobbers_avoided;\n } clobber_stats;\n   \n+\n /* Initialize the operand cache routines.  */\n \n void\n@@ -291,12 +274,18 @@ fini_ssa_operands (void)\n   \n   if (dump_file && (dump_flags & TDF_STATS))\n     {\n-      fprintf (dump_file, \"Original clobbered vars:%d\\n\", clobber_stats.clobbered_vars);\n-      fprintf (dump_file, \"Static write clobbers avoided:%d\\n\", clobber_stats.static_write_clobbers_avoided);\n-      fprintf (dump_file, \"Static read clobbers avoided:%d\\n\", clobber_stats.static_read_clobbers_avoided);\n-      fprintf (dump_file, \"Unescapable clobbers avoided:%d\\n\", clobber_stats.unescapable_clobbers_avoided);\n-      fprintf (dump_file, \"Original readonly clobbers:%d\\n\", clobber_stats.readonly_clobbers);\n-      fprintf (dump_file, \"Static readonly clobbers avoided:%d\\n\", clobber_stats.static_readonly_clobbers_avoided);\n+      fprintf (dump_file, \"Original clobbered vars:%d\\n\",\n+\t       clobber_stats.clobbered_vars);\n+      fprintf (dump_file, \"Static write clobbers avoided:%d\\n\",\n+\t       clobber_stats.static_write_clobbers_avoided);\n+      fprintf (dump_file, \"Static read clobbers avoided:%d\\n\",\n+\t       clobber_stats.static_read_clobbers_avoided);\n+      fprintf (dump_file, \"Unescapable clobbers avoided:%d\\n\",\n+\t       clobber_stats.unescapable_clobbers_avoided);\n+      fprintf (dump_file, \"Original readonly clobbers:%d\\n\",\n+\t       clobber_stats.readonly_clobbers);\n+      fprintf (dump_file, \"Static readonly clobbers avoided:%d\\n\",\n+\t       clobber_stats.static_readonly_clobbers_avoided);\n     }\n }\n \n@@ -352,7 +341,8 @@ correct_use_link (use_operand_p ptr, tree stmt)\n       if (root == *(ptr->use))\n \treturn;\n     }\n-  /* Its in the wrong list if we reach here.  */\n+\n+  /* It is in the wrong list if we reach here.  */\n   delink_imm_use (ptr);\n   link_imm_use (ptr, *(ptr->use));\n }\n@@ -362,6 +352,7 @@ correct_use_link (use_operand_p ptr, tree stmt)\n    sure the stmt pointer is set to the current stmt.  Virtual uses do not need\n    the overhead of correct_use_link since they cannot be directly manipulated\n    like a real use can be.  (They don't exist in the TREE_OPERAND nodes.)  */\n+\n static inline void\n set_virtual_use_link (use_operand_p ptr, tree stmt)\n {\n@@ -375,7 +366,6 @@ set_virtual_use_link (use_operand_p ptr, tree stmt)\n }\n \n \n-\n #define FINALIZE_OPBUILD\t\tbuild_defs\n #define FINALIZE_OPBUILD_BASE(I)\t(tree *)VEC_index (tree,\t\\\n \t\t\t\t\t\t\t   build_defs, (I))\n@@ -400,12 +390,12 @@ static void\n finalize_ssa_defs (tree stmt)\n {\n   unsigned int num = VEC_length (tree, build_defs);\n+\n   /* There should only be a single real definition per assignment.  */\n   gcc_assert ((stmt && TREE_CODE (stmt) != MODIFY_EXPR) || num <= 1);\n \n   /* If there is an old list, often the new list is identical, or close, so\n      find the elements at the beginning that are the same as the vector.  */\n-\n   finalize_ssa_def_ops (stmt);\n   VEC_truncate (tree, build_defs, 0);\n }\n@@ -630,7 +620,6 @@ finalize_ssa_v_must_defs (tree stmt)\n      all must-defs in a statement belong to subvars if there is more than one\n      MUST-def, so we don't do it.  Suffice to say, if you reach here without\n      having subvars, and have num >1, you have hit a bug. */\n-\n   finalize_ssa_v_must_def_ops (stmt);\n   VEC_truncate (tree, build_v_must_defs, 0);\n }\n@@ -735,421 +724,842 @@ append_v_must_def (tree var)\n }\n \n \n-/* Parse STMT looking for operands.  OLD_OPS is the original stmt operand\n-   cache for STMT, if it existed before.  When finished, the various build_*\n-   operand vectors will have potential operands. in them.  */\n-                                                                                \n-static void\n-parse_ssa_operands (tree stmt)\n-{\n-  enum tree_code code;\n-\n-  code = TREE_CODE (stmt);\n-  switch (code)\n-    {\n-    case MODIFY_EXPR:\n-      /* First get operands from the RHS.  For the LHS, we use a V_MAY_DEF if\n-\t either only part of LHS is modified or if the RHS might throw,\n-\t otherwise, use V_MUST_DEF.\n-\n-\t ??? If it might throw, we should represent somehow that it is killed\n-\t on the fallthrough path.  */\n-      {\n-\ttree lhs = TREE_OPERAND (stmt, 0);\n-\tint lhs_flags = opf_is_def;\n-\n-\tget_expr_operands (stmt, &TREE_OPERAND (stmt, 1), opf_none);\n-\n-\t/* If the LHS is a VIEW_CONVERT_EXPR, it isn't changing whether\n-\t   or not the entire LHS is modified; that depends on what's\n-\t   inside the VIEW_CONVERT_EXPR.  */\n-\tif (TREE_CODE (lhs) == VIEW_CONVERT_EXPR)\n-\t  lhs = TREE_OPERAND (lhs, 0);\n-\n-\tif (TREE_CODE (lhs) != ARRAY_RANGE_REF\n-\t    && TREE_CODE (lhs) != BIT_FIELD_REF)\n-\t  lhs_flags |= opf_kill_def;\n+/* REF is a tree that contains the entire pointer dereference\n+   expression, if available, or NULL otherwise.  ALIAS is the variable\n+   we are asking if REF can access.  OFFSET and SIZE come from the\n+   memory access expression that generated this virtual operand.\n+   FOR_CLOBBER is true is this is adding a virtual operand for a call\n+   clobber.  */\n \n-        get_expr_operands (stmt, &TREE_OPERAND (stmt, 0), lhs_flags);\n-      }\n-      break;\n+static bool\n+access_can_touch_variable (tree ref, tree alias, HOST_WIDE_INT offset,\n+\t\t\t   HOST_WIDE_INT size)\n+{  \n+  bool offsetgtz = offset > 0;\n+  unsigned HOST_WIDE_INT uoffset = (unsigned HOST_WIDE_INT) offset;\n+  tree base = ref ? get_base_address (ref) : NULL;\n \n-    case COND_EXPR:\n-      get_expr_operands (stmt, &COND_EXPR_COND (stmt), opf_none);\n-      break;\n+  /* If ALIAS is an SFT, it can't be touched if the offset     \n+     and size of the access is not overlapping with the SFT offset and\n+     size.  This is only true if we are accessing through a pointer\n+     to a type that is the same as SFT_PARENT_VAR.  Otherwise, we may\n+     be accessing through a pointer to some substruct of the\n+     structure, and if we try to prune there, we will have the wrong\n+     offset, and get the wrong answer.\n+     i.e., we can't prune without more work if we have something like\n \n-    case SWITCH_EXPR:\n-      get_expr_operands (stmt, &SWITCH_COND (stmt), opf_none);\n-      break;\n+     struct gcc_target\n+     {\n+       struct asm_out\n+       {\n+         const char *byte_op;\n+\t struct asm_int_op\n+\t {    \n+\t   const char *hi;\n+\t } aligned_op;\n+       } asm_out;\n+     } targetm;\n+     \n+     foo = &targetm.asm_out.aligned_op;\n+     return foo->hi;\n \n-    case ASM_EXPR:\n-      get_asm_expr_operands (stmt);\n-      break;\n+     SFT.1, which represents hi, will have SFT_OFFSET=32 because in\n+     terms of SFT_PARENT_VAR, that is where it is.\n+     However, the access through the foo pointer will be at offset 0.  */\n+  if (size != -1\n+      && TREE_CODE (alias) == STRUCT_FIELD_TAG\n+      && base\n+      && TREE_TYPE (base) == TREE_TYPE (SFT_PARENT_VAR (alias))\n+      && !overlap_subvar (offset, size, alias, NULL))\n+    {\n+#ifdef ACCESS_DEBUGGING\n+      fprintf (stderr, \"Access to \");\n+      print_generic_expr (stderr, ref, 0);\n+      fprintf (stderr, \" may not touch \");\n+      print_generic_expr (stderr, alias, 0);\n+      fprintf (stderr, \" in function %s\\n\", get_name (current_function_decl));\n+#endif\n+      return false;\n+    }\n \n-    case RETURN_EXPR:\n-      get_expr_operands (stmt, &TREE_OPERAND (stmt, 0), opf_none);\n-      break;\n+  /* Without strict aliasing, it is impossible for a component access\n+     through a pointer to touch a random variable, unless that\n+     variable *is* a structure or a pointer.\n \n-    case GOTO_EXPR:\n-      get_expr_operands (stmt, &GOTO_DESTINATION (stmt), opf_none);\n-      break;\n+     That is, given p->c, and some random global variable b,\n+     there is no legal way that p->c could be an access to b.\n+     \n+     Without strict aliasing on, we consider it legal to do something\n+     like:\n \n-    case LABEL_EXPR:\n-      get_expr_operands (stmt, &LABEL_EXPR_LABEL (stmt), opf_none);\n-      break;\n+     struct foos { int l; };\n+     int foo;\n+     static struct foos *getfoo(void);\n+     int main (void)\n+     {\n+       struct foos *f = getfoo();\n+       f->l = 1;\n+       foo = 2;\n+       if (f->l == 1)\n+         abort();\n+       exit(0);\n+     }\n+     static struct foos *getfoo(void)     \n+     { return (struct foos *)&foo; }\n+     \n+     (taken from 20000623-1.c)\n+  */\n+  else if (ref \n+\t   && flag_strict_aliasing\n+\t   && TREE_CODE (ref) != INDIRECT_REF\n+\t   && !MTAG_P (alias)\n+\t   && !AGGREGATE_TYPE_P (TREE_TYPE (alias))\n+\t   && TREE_CODE (TREE_TYPE (alias)) != COMPLEX_TYPE\n+\t   && !POINTER_TYPE_P (TREE_TYPE (alias)))\n+    {\n+#ifdef ACCESS_DEBUGGING\n+      fprintf (stderr, \"Access to \");\n+      print_generic_expr (stderr, ref, 0);\n+      fprintf (stderr, \" may not touch \");\n+      print_generic_expr (stderr, alias, 0);\n+      fprintf (stderr, \" in function %s\\n\", get_name (current_function_decl));\n+#endif\n+      return false;\n+    }\n \n-      /* These nodes contain no variable references.  */\n-    case BIND_EXPR:\n-    case CASE_LABEL_EXPR:\n-    case TRY_CATCH_EXPR:\n-    case TRY_FINALLY_EXPR:\n-    case EH_FILTER_EXPR:\n-    case CATCH_EXPR:\n-    case RESX_EXPR:\n-      break;\n+  /* If the offset of the access is greater than the size of one of\n+     the possible aliases, it can't be touching that alias, because it\n+     would be past the end of the structure.  */\n+  else if (ref\n+\t   && flag_strict_aliasing\n+\t   && TREE_CODE (ref) != INDIRECT_REF\n+\t   && !MTAG_P (alias)\n+\t   && !POINTER_TYPE_P (TREE_TYPE (alias))\n+\t   && offsetgtz\n+\t   && DECL_SIZE (alias)\n+\t   && TREE_CODE (DECL_SIZE (alias)) == INTEGER_CST\n+\t   && uoffset > TREE_INT_CST_LOW (DECL_SIZE (alias)))\n+    {\n+#ifdef ACCESS_DEBUGGING\n+      fprintf (stderr, \"Access to \");\n+      print_generic_expr (stderr, ref, 0);\n+      fprintf (stderr, \" may not touch \");\n+      print_generic_expr (stderr, alias, 0);\n+      fprintf (stderr, \" in function %s\\n\", get_name (current_function_decl));\n+#endif\n+      return false;\n+    }\t   \n \n-    default:\n-      /* Notice that if get_expr_operands tries to use &STMT as the operand\n-\t pointer (which may only happen for USE operands), we will fail in\n-\t append_use.  This default will handle statements like empty\n-\t statements, or CALL_EXPRs that may appear on the RHS of a statement\n-\t or as statements themselves.  */\n-      get_expr_operands (stmt, &stmt, opf_none);\n-      break;\n-    }\n+  return true;\n }\n \n-/* Create an operands cache for STMT.  */\n \n-static void\n-build_ssa_operands (tree stmt)\n+/* Add VAR to the virtual operands array.  FLAGS is as in\n+   get_expr_operands.  FULL_REF is a tree that contains the entire\n+   pointer dereference expression, if available, or NULL otherwise.\n+   OFFSET and SIZE come from the memory access expression that\n+   generated this virtual operand.  FOR_CLOBBER is true is this is\n+   adding a virtual operand for a call clobber.  */\n+\n+static void \n+add_virtual_operand (tree var, stmt_ann_t s_ann, int flags,\n+\t\t     tree full_ref, HOST_WIDE_INT offset,\n+\t\t     HOST_WIDE_INT size, bool for_clobber)\n {\n-  stmt_ann_t ann = get_stmt_ann (stmt);\n+  VEC(tree,gc) *aliases;\n+  tree sym;\n+  var_ann_t v_ann;\n   \n-  /* Initially assume that the statement has no volatile operands.  */\n-  if (ann)\n-    ann->has_volatile_ops = false;\n-\n-  start_ssa_stmt_operands ();\n-\n-  parse_ssa_operands (stmt);\n-  operand_build_sort_virtual (build_vuses);\n-  operand_build_sort_virtual (build_v_may_defs);\n-  operand_build_sort_virtual (build_v_must_defs);\n+  sym = (TREE_CODE (var) == SSA_NAME ? SSA_NAME_VAR (var) : var);\n+  v_ann = var_ann (sym);\n+  \n+  /* Mark statements with volatile operands.  Optimizers should back\n+     off from statements having volatile operands.  */\n+  if (TREE_THIS_VOLATILE (sym) && s_ann)\n+    s_ann->has_volatile_ops = true;\n \n-  finalize_ssa_stmt_operands (stmt);\n-}\n+  /* If the variable cannot be modified and this is a V_MAY_DEF change\n+     it into a VUSE.  This happens when read-only variables are marked\n+     call-clobbered and/or aliased to writable variables.  So we only\n+     check that this only happens on non-specific stores.\n \n+     Note that if this is a specific store, i.e. associated with a\n+     modify_expr, then we can't suppress the V_MAY_DEF, lest we run\n+     into validation problems.\n \n-/* Free any operands vectors in OPS.  */\n-void \n-free_ssa_operands (stmt_operands_p ops)\n-{\n-  ops->def_ops = NULL;\n-  ops->use_ops = NULL;\n-  ops->maydef_ops = NULL;\n-  ops->mustdef_ops = NULL;\n-  ops->vuse_ops = NULL;\n-}\n-\n-\n-/* Get the operands of statement STMT.  Note that repeated calls to\n-   get_stmt_operands for the same statement will do nothing until the\n-   statement is marked modified by a call to mark_stmt_modified().  */\n-\n-void\n-update_stmt_operands (tree stmt)\n-{\n-  stmt_ann_t ann = get_stmt_ann (stmt);\n-  /* If get_stmt_operands is called before SSA is initialized, dont\n-  do anything.  */\n-  if (!ssa_operands_active ())\n+     This can happen when programs cast away const, leaving us with a\n+     store to read-only memory.  If the statement is actually executed\n+     at runtime, then the program is ill formed.  If the statement is\n+     not executed then all is well.  At the very least, we cannot ICE.  */\n+  if ((flags & opf_non_specific) && unmodifiable_var_p (var))\n+    flags &= ~(opf_is_def | opf_kill_def);\n+  \n+  /* The variable is not a GIMPLE register.  Add it (or its aliases) to\n+     virtual operands, unless the caller has specifically requested\n+     not to add virtual operands (used when adding operands inside an\n+     ADDR_EXPR expression).  */\n+  if (flags & opf_no_vops)\n     return;\n-  /* The optimizers cannot handle statements that are nothing but a\n-     _DECL.  This indicates a bug in the gimplifier.  */\n-  gcc_assert (!SSA_VAR_P (stmt));\n+  \n+  aliases = v_ann->may_aliases;\n+  if (aliases == NULL)\n+    {\n+      /* The variable is not aliased or it is an alias tag.  */\n+      if (flags & opf_is_def)\n+\t{\n+\t  if (flags & opf_kill_def)\n+\t    {\n+\t      /* V_MUST_DEF for non-aliased, non-GIMPLE register \n+\t\t variable definitions.  */\n+\t      gcc_assert (!MTAG_P (var)\n+\t\t\t  || TREE_CODE (var) == STRUCT_FIELD_TAG);\n+\t      append_v_must_def (var);\n+\t    }\n+\t  else\n+\t    {\n+\t      /* Add a V_MAY_DEF for call-clobbered variables and\n+\t\t memory tags.  */\n+\t      append_v_may_def (var);\n+\t    }\n+\t}\n+      else\n+\tappend_vuse (var);\n+    }\n+  else\n+    {\n+      unsigned i;\n+      tree al;\n+      \n+      /* The variable is aliased.  Add its aliases to the virtual\n+\t operands.  */\n+      gcc_assert (VEC_length (tree, aliases) != 0);\n+      \n+      if (flags & opf_is_def)\n+\t{\n+\t  \n+\t  bool none_added = true;\n \n-  gcc_assert (ann->modified);\n+\t  for (i = 0; VEC_iterate (tree, aliases, i, al); i++)\n+\t    {\n+\t      if (!access_can_touch_variable (full_ref, al, offset, size))\n+\t\tcontinue;\n+\t      \n+\t      none_added = false;\n+\t      append_v_may_def (al);\n+\t    }\n \n-  timevar_push (TV_TREE_OPS);\n+\t  /* If the variable is also an alias tag, add a virtual\n+\t     operand for it, otherwise we will miss representing\n+\t     references to the members of the variable's alias set.\t     \n+\t     This fixes the bug in gcc.c-torture/execute/20020503-1.c.\n+\t     \n+\t     It is also necessary to add bare defs on clobbers for\n+\t     TMT's, so that bare TMT uses caused by pruning all the\n+\t     aliases will link up properly with calls.   In order to\n+\t     keep the number of these bare defs we add down to the\n+\t     minimum necessary, we keep track of which TMT's were used\n+\t     alone in statement defs or vuses.  */\n+\t  if (v_ann->is_aliased\n+\t      || none_added\n+\t      || (TREE_CODE (var) == TYPE_MEMORY_TAG && for_clobber\n+\t\t  && TMT_USED_ALONE (var)))\n+\t    {\n+\t      /* Every bare tmt def we add should have TMT_USED_ALONE\n+\t\t set on it, or else we will get the wrong answer on\n+\t\t clobbers.  */\n \n-  build_ssa_operands (stmt);\n+\t      if (none_added && !updating_used_alone && aliases_computed_p\n+\t\t  && TREE_CODE (var) == TYPE_MEMORY_TAG)\n+\t\tgcc_assert (TMT_USED_ALONE (var));\n \n-  /* Clear the modified bit for STMT.  Subsequent calls to\n-     get_stmt_operands for this statement will do nothing until the\n-     statement is marked modified by a call to mark_stmt_modified().  */\n-  ann->modified = 0;\n+\t      append_v_may_def (var);\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  bool none_added = true;\n+\t  for (i = 0; VEC_iterate (tree, aliases, i, al); i++)\n+\t    {\n+\t      if (!access_can_touch_variable (full_ref, al, offset, size))\n+\t\tcontinue;\n+\t      none_added = false;\n+\t      append_vuse (al);\n+\t    }\n \n-  timevar_pop (TV_TREE_OPS);\n+\t  /* Similarly, append a virtual uses for VAR itself, when\n+\t     it is an alias tag.  */\n+\t  if (v_ann->is_aliased || none_added)\n+\t    append_vuse (var);\n+\t}\n+    }\n }\n \n-  \n-/* Copies virtual operands from SRC to DST.  */\n \n-void\n-copy_virtual_operands (tree dest, tree src)\n+/* Add *VAR_P to the appropriate operand array for S_ANN.  FLAGS is as in\n+   get_expr_operands.  If *VAR_P is a GIMPLE register, it will be added to\n+   the statement's real operands, otherwise it is added to virtual\n+   operands.  */\n+\n+static void\n+add_stmt_operand (tree *var_p, stmt_ann_t s_ann, int flags)\n {\n-  tree t;\n-  ssa_op_iter iter, old_iter;\n-  use_operand_p use_p, u2;\n-  def_operand_p def_p, d2;\n+  bool is_real_op;\n+  tree var, sym;\n+  var_ann_t v_ann;\n \n-  build_ssa_operands (dest);\n+  var = *var_p;\n+  gcc_assert (SSA_VAR_P (var));\n \n-  /* Copy all the virtual fields.  */\n-  FOR_EACH_SSA_TREE_OPERAND (t, src, iter, SSA_OP_VUSE)\n-    append_vuse (t);\n-  FOR_EACH_SSA_TREE_OPERAND (t, src, iter, SSA_OP_VMAYDEF)\n-    append_v_may_def (t);\n-  FOR_EACH_SSA_TREE_OPERAND (t, src, iter, SSA_OP_VMUSTDEF)\n-    append_v_must_def (t);\n+  is_real_op = is_gimple_reg (var);\n \n-  if (VEC_length (tree, build_vuses) == 0\n-      && VEC_length (tree, build_v_may_defs) == 0\n-      && VEC_length (tree, build_v_must_defs) == 0)\n-    return;\n+  /* If this is a real operand, the operand is either an SSA name or a \n+     decl.  Virtual operands may only be decls.  */\n+  gcc_assert (is_real_op || DECL_P (var));\n \n-  /* Now commit the virtual operands to this stmt.  */\n-  finalize_ssa_v_must_defs (dest);\n-  finalize_ssa_v_may_defs (dest);\n-  finalize_ssa_vuses (dest);\n+  sym = (TREE_CODE (var) == SSA_NAME ? SSA_NAME_VAR (var) : var);\n+  v_ann = var_ann (sym);\n \n-  /* Finally, set the field to the same values as then originals.  */\n+  /* Mark statements with volatile operands.  Optimizers should back\n+     off from statements having volatile operands.  */\n+  if (TREE_THIS_VOLATILE (sym) && s_ann)\n+    s_ann->has_volatile_ops = true;\n \n-  \n-  t = op_iter_init_tree (&old_iter, src, SSA_OP_VUSE);\n-  FOR_EACH_SSA_USE_OPERAND (use_p, dest, iter, SSA_OP_VUSE)\n+  if (is_real_op)\n     {\n-      gcc_assert (!op_iter_done (&old_iter));\n-      SET_USE (use_p, t);\n-      t = op_iter_next_tree (&old_iter);\n+      /* The variable is a GIMPLE register.  Add it to real operands.  */\n+      if (flags & opf_is_def)\n+\tappend_def (var_p);\n+      else\n+\tappend_use (var_p);\n     }\n-  gcc_assert (op_iter_done (&old_iter));\n+  else\n+    add_virtual_operand (var, s_ann, flags, NULL_TREE, 0, -1, false);\n+}\n \n-  op_iter_init_maydef (&old_iter, src, &u2, &d2);\n-  FOR_EACH_SSA_MAYDEF_OPERAND (def_p, use_p, dest, iter)\n-    {\n-      gcc_assert (!op_iter_done (&old_iter));\n-      SET_USE (use_p, USE_FROM_PTR (u2));\n-      SET_DEF (def_p, DEF_FROM_PTR (d2));\n-      op_iter_next_maymustdef (&u2, &d2, &old_iter);\n-    }\n-  gcc_assert (op_iter_done (&old_iter));\n \n-  op_iter_init_mustdef (&old_iter, src, &u2, &d2);\n-  FOR_EACH_SSA_MUSTDEF_OPERAND (def_p, use_p, dest, iter)\n-    {\n-      gcc_assert (!op_iter_done (&old_iter));\n-      SET_USE (use_p, USE_FROM_PTR (u2));\n-      SET_DEF (def_p, DEF_FROM_PTR (d2));\n-      op_iter_next_maymustdef (&u2, &d2, &old_iter);\n-    }\n-  gcc_assert (op_iter_done (&old_iter));\n+/* A subroutine of get_expr_operands to handle INDIRECT_REF,\n+   ALIGN_INDIRECT_REF and MISALIGNED_INDIRECT_REF.  \n \n-}\n+   STMT is the statement being processed, EXPR is the INDIRECT_REF\n+      that got us here.\n+   \n+   FLAGS is as in get_expr_operands.\n \n+   FULL_REF contains the full pointer dereference expression, if we\n+      have it, or NULL otherwise.\n \n-/* Specifically for use in DOM's expression analysis.  Given a store, we\n-   create an artificial stmt which looks like a load from the store, this can\n-   be used to eliminate redundant loads.  OLD_OPS are the operands from the \n-   store stmt, and NEW_STMT is the new load which represents a load of the\n-   values stored.  */\n+   OFFSET and SIZE are the location of the access inside the\n+      dereferenced pointer, if known.\n \n-void\n-create_ssa_artficial_load_stmt (tree new_stmt, tree old_stmt)\n-{\n-  stmt_ann_t ann;\n-  tree op;\n-  ssa_op_iter iter;\n-  use_operand_p use_p;\n-  unsigned x;\n+   RECURSE_ON_BASE should be set to true if we want to continue\n+      calling get_expr_operands on the base pointer, and false if\n+      something else will do it for us.  */\n \n-  ann = get_stmt_ann (new_stmt);\n+static void\n+get_indirect_ref_operands (tree stmt, tree expr, int flags,\n+\t\t\t   tree full_ref,\n+\t\t\t   HOST_WIDE_INT offset, HOST_WIDE_INT size,\n+\t\t\t   bool recurse_on_base)\n+{\n+  tree *pptr = &TREE_OPERAND (expr, 0);\n+  tree ptr = *pptr;\n+  stmt_ann_t s_ann = stmt_ann (stmt);\n \n-  /* process the stmt looking for operands.  */\n-  start_ssa_stmt_operands ();\n-  parse_ssa_operands (new_stmt);\n+  /* Stores into INDIRECT_REF operands are never killing definitions.  */\n+  flags &= ~opf_kill_def;\n \n-  for (x = 0; x < VEC_length (tree, build_vuses); x++)\n+  if (SSA_VAR_P (ptr))\n     {\n-      tree t = VEC_index (tree, build_vuses, x);\n-      if (TREE_CODE (t) != SSA_NAME)\n+      struct ptr_info_def *pi = NULL;\n+\n+      /* If PTR has flow-sensitive points-to information, use it.  */\n+      if (TREE_CODE (ptr) == SSA_NAME\n+\t  && (pi = SSA_NAME_PTR_INFO (ptr)) != NULL\n+\t  && pi->name_mem_tag)\n \t{\n-\t  var_ann_t ann = var_ann (t);\n-\t  ann->in_vuse_list = 0;\n+\t  /* PTR has its own memory tag.  Use it.  */\n+\t  add_virtual_operand (pi->name_mem_tag, s_ann, flags,\n+\t\t\t       full_ref, offset, size, false);\n \t}\n-    }\n-   \n-  for (x = 0; x < VEC_length (tree, build_v_may_defs); x++)\n-    {\n-      tree t = VEC_index (tree, build_v_may_defs, x);\n-      if (TREE_CODE (t) != SSA_NAME)\n+      else\n \t{\n-\t  var_ann_t ann = var_ann (t);\n-\t  ann->in_v_may_def_list = 0;\n-\t}\n-    }\n-  /* Remove any virtual operands that were found.  */\n-  VEC_truncate (tree, build_v_may_defs, 0);\n-  VEC_truncate (tree, build_v_must_defs, 0);\n-  VEC_truncate (tree, build_vuses, 0);\n-\n-  /* For each VDEF on the original statement, we want to create a\n-     VUSE of the V_MAY_DEF result or V_MUST_DEF op on the new \n-     statement.  */\n-  FOR_EACH_SSA_TREE_OPERAND (op, old_stmt, iter, \n-\t\t\t     (SSA_OP_VMAYDEF | SSA_OP_VMUSTDEF))\n-    append_vuse (op);\n-    \n-  /* Now build the operands for this new stmt.  */\n-  finalize_ssa_stmt_operands (new_stmt);\n+\t  /* If PTR is not an SSA_NAME or it doesn't have a name\n+\t     tag, use its type memory tag.  */\n+\t  var_ann_t v_ann;\n \n-  /* All uses in this fake stmt must not be in the immediate use lists.  */\n-  FOR_EACH_SSA_USE_OPERAND (use_p, new_stmt, iter, SSA_OP_ALL_USES)\n-    delink_imm_use (use_p);\n-}\n+\t  /* If we are emitting debugging dumps, display a warning if\n+\t     PTR is an SSA_NAME with no flow-sensitive alias\n+\t     information.  That means that we may need to compute\n+\t     aliasing again.  */\n+\t  if (dump_file\n+\t      && TREE_CODE (ptr) == SSA_NAME\n+\t      && pi == NULL)\n+\t    {\n+\t      fprintf (dump_file,\n+\t\t  \"NOTE: no flow-sensitive alias info for \");\n+\t      print_generic_expr (dump_file, ptr, dump_flags);\n+\t      fprintf (dump_file, \" in \");\n+\t      print_generic_stmt (dump_file, stmt, dump_flags);\n+\t    }\n \n-void\n-swap_tree_operands (tree stmt, tree *exp0, tree *exp1)\n-{\n-  tree op0, op1;\n-  op0 = *exp0;\n-  op1 = *exp1;\n+\t  if (TREE_CODE (ptr) == SSA_NAME)\n+\t    ptr = SSA_NAME_VAR (ptr);\n+\t  v_ann = var_ann (ptr);\n \n-  /* If the operand cache is active, attempt to preserve the relative positions\n-     of these two operands in their respective immediate use lists.  */\n-  if (ssa_operands_active () && op0 != op1)\n-    {\n-      use_optype_p use0, use1, ptr;\n-      use0 = use1 = NULL;\n-      /* Find the 2 operands in the cache, if they are there.  */\n-      for (ptr = USE_OPS (stmt); ptr; ptr = ptr->next)\n-\tif (USE_OP_PTR (ptr)->use == exp0)\n-\t  {\n-\t    use0 = ptr;\n-\t    break;\n-\t  }\n-      for (ptr = USE_OPS (stmt); ptr; ptr = ptr->next)\n-\tif (USE_OP_PTR (ptr)->use == exp1)\n-\t  {\n-\t    use1 = ptr;\n-\t    break;\n-\t  }\n-      /* If both uses don't have operand entries, there isn't much we can do\n-         at this point.  Presumably we dont need to worry about it.  */\n-      if (use0 && use1)\n-        {\n-\t  tree *tmp = USE_OP_PTR (use1)->use;\n-\t  USE_OP_PTR (use1)->use = USE_OP_PTR (use0)->use;\n-\t  USE_OP_PTR (use0)->use = tmp;\n+\t  if (v_ann->type_mem_tag)\n+\t    add_virtual_operand (v_ann->type_mem_tag, s_ann, flags,\n+\t\t\t\t full_ref, offset, size, false);\n \t}\n     }\n+  else if (TREE_CODE (ptr) == INTEGER_CST)\n+    {\n+      /* If a constant is used as a pointer, we can't generate a real\n+\t operand for it but we mark the statement volatile to prevent\n+\t optimizations from messing things up.  */\n+      if (s_ann)\n+\ts_ann->has_volatile_ops = true;\n+      return;\n+    }\n+  else\n+    {\n+      /* Ok, this isn't even is_gimple_min_invariant.  Something's broke.  */\n+      gcc_unreachable ();\n+    }\n \n-  /* Now swap the data.  */\n-  *exp0 = op1;\n-  *exp1 = op0;\n+  /* If requested, add a USE operand for the base pointer.  */\n+  if (recurse_on_base)\n+    get_expr_operands (stmt, pptr, opf_none);\n }\n \n \n-/* Recursively scan the expression pointed to by EXPR_P in statement\n-   referred to by INFO.  FLAGS is one of the OPF_* constants modifying\n-   how to interpret the operands found.  */\n+/* A subroutine of get_expr_operands to handle TARGET_MEM_REF.  */\n \n static void\n-get_expr_operands (tree stmt, tree *expr_p, int flags)\n+get_tmr_operands (tree stmt, tree expr, int flags)\n {\n-  enum tree_code code;\n-  enum tree_code_class class;\n-  tree expr = *expr_p;\n+  tree tag = TMR_TAG (expr), ref;\n+  HOST_WIDE_INT offset, size, maxsize;\n+  subvar_t svars, sv;\n   stmt_ann_t s_ann = stmt_ann (stmt);\n \n-  if (expr == NULL)\n-    return;\n+  /* First record the real operands.  */\n+  get_expr_operands (stmt, &TMR_BASE (expr), opf_none);\n+  get_expr_operands (stmt, &TMR_INDEX (expr), opf_none);\n \n-  code = TREE_CODE (expr);\n-  class = TREE_CODE_CLASS (code);\n+  /* MEM_REFs should never be killing.  */\n+  flags &= ~opf_kill_def;\n \n-  switch (code)\n+  if (TMR_SYMBOL (expr))\n     {\n-    case ADDR_EXPR:\n-      /* Taking the address of a variable does not represent a\n-\t reference to it, but the fact that the statement takes its\n-\t address will be of interest to some passes (e.g. alias\n-\t resolution).  */\n-      add_to_addressable_set (TREE_OPERAND (expr, 0), &s_ann->addresses_taken);\n-\n-      /* If the address is invariant, there may be no interesting\n-\t variable references inside.  */\n-      if (is_gimple_min_invariant (expr))\n-\treturn;\n+      stmt_ann_t ann = stmt_ann (stmt);\n+      add_to_addressable_set (TMR_SYMBOL (expr), &ann->addresses_taken);\n+    }\n \n-      /* Otherwise, there may be variables referenced inside but there\n-\t should be no VUSEs created, since the referenced objects are\n-\t not really accessed.  The only operands that we should find\n-\t here are ARRAY_REF indices which will always be real operands\n-\t (GIMPLE does not allow non-registers as array indices).  */\n-      flags |= opf_no_vops;\n-      get_expr_operands (stmt, &TREE_OPERAND (expr, 0), flags);\n+  if (!tag)\n+    {\n+      /* Something weird, so ensure that we will be careful.  */\n+      stmt_ann (stmt)->has_volatile_ops = true;\n       return;\n+    }\n \n-    case SSA_NAME:\n-    case STRUCT_FIELD_TAG:\n-    case TYPE_MEMORY_TAG:\n-    case NAME_MEMORY_TAG:\n-     add_stmt_operand (expr_p, s_ann, flags);\n-     return;\n-\n-    case VAR_DECL:\n-    case PARM_DECL:\n-    case RESULT_DECL:\n-      {\n-\tsubvar_t svars;\n-\t\n-\t/* Add the subvars for a variable if it has subvars, to DEFS\n-\t   or USES.  Otherwise, add the variable itself.  Whether it\n-\t   goes to USES or DEFS depends on the operand flags.  */\n-\tif (var_can_have_subvars (expr)\n-\t    && (svars = get_subvars_for_var (expr)))\n-\t  {\n-\t    subvar_t sv;\n-\t    for (sv = svars; sv; sv = sv->next)\n-\t      add_stmt_operand (&sv->var, s_ann, flags);\n-\t  }\n-\telse\n-\t  add_stmt_operand (expr_p, s_ann, flags);\n+  if (DECL_P (tag))\n+    {\n+      get_expr_operands (stmt, &tag, flags);\n+      return;\n+    }\n \n-\treturn;\n-      }\n+  ref = get_ref_base_and_extent (tag, &offset, &size, &maxsize);\n+  gcc_assert (ref != NULL_TREE);\n+  svars = get_subvars_for_var (ref);\n+  for (sv = svars; sv; sv = sv->next)\n+    {\n+      bool exact;\t\t\n+      if (overlap_subvar (offset, maxsize, sv->var, &exact))\n+\t{\n+\t  int subvar_flags = flags;\n+\t  if (!exact || size != maxsize)\n+\t    subvar_flags &= ~opf_kill_def;\n+\t  add_stmt_operand (&sv->var, s_ann, subvar_flags);\n+\t}\n+    }\n+}\n \n-    case MISALIGNED_INDIRECT_REF:\n-      get_expr_operands (stmt, &TREE_OPERAND (expr, 1), flags);\n-      /* fall through */\n \n-    case ALIGN_INDIRECT_REF:\n-    case INDIRECT_REF:\n-      get_indirect_ref_operands (stmt, expr, flags, NULL_TREE,\n-\t\t\t\t 0, -1, true);\n-      return;\n+/* Add clobbering definitions for .GLOBAL_VAR or for each of the call\n+   clobbered variables in the function.  */\n \n-    case TARGET_MEM_REF:\n-      get_tmr_operands (stmt, expr, flags);\n-      return;\n+static void\n+add_call_clobber_ops (tree stmt, tree callee)\n+{\n+  unsigned u;\n+  bitmap_iterator bi;\n+  stmt_ann_t s_ann = stmt_ann (stmt);\n+  bitmap not_read_b, not_written_b;\n+  \n+  /* Functions that are not const, pure or never return may clobber\n+     call-clobbered variables.  */\n+  if (s_ann)\n+    s_ann->makes_clobbering_call = true;\n \n-    case ARRAY_RANGE_REF:\n-      /* Treat array references as references to the virtual variable\n-\t representing the array.  The virtual variable for an ARRAY_REF\n-\t is the VAR_DECL for the array.  */\n-      get_expr_operands (stmt, &TREE_OPERAND (expr, 0), flags);\n-      get_expr_operands (stmt, &TREE_OPERAND (expr, 1), opf_none);\n-      get_expr_operands (stmt, &TREE_OPERAND (expr, 2), opf_none);\n-      get_expr_operands (stmt, &TREE_OPERAND (expr, 3), opf_none);\n+  /* If we created .GLOBAL_VAR earlier, just use it.  See compute_may_aliases \n+     for the heuristic used to decide whether to create .GLOBAL_VAR or not.  */\n+  if (global_var)\n+    {\n+      add_stmt_operand (&global_var, s_ann, opf_is_def);\n       return;\n+    }\n \n-    case ARRAY_REF:\n-    case COMPONENT_REF:\n-    case REALPART_EXPR:\n-    case IMAGPART_EXPR:\n-      {\n+  /* Get info for local and module level statics.  There is a bit\n+     set for each static if the call being processed does not read\n+     or write that variable.  */\n+  not_read_b = callee ? ipa_reference_get_not_read_global (callee) : NULL; \n+  not_written_b = callee ? ipa_reference_get_not_written_global (callee) : NULL; \n+  /* Add a V_MAY_DEF operand for every call clobbered variable.  */\n+  EXECUTE_IF_SET_IN_BITMAP (call_clobbered_vars, 0, u, bi)\n+    {\n+      tree var = referenced_var_lookup (u);\n+      unsigned int escape_mask = var_ann (var)->escape_mask;\n+      tree real_var = var;\n+      bool not_read;\n+      bool not_written;\n+      \n+      /* Not read and not written are computed on regular vars, not\n+\t subvars, so look at the parent var if this is an SFT. */\n+      if (TREE_CODE (var) == STRUCT_FIELD_TAG)\n+\treal_var = SFT_PARENT_VAR (var);\n+\n+      not_read = not_read_b ? bitmap_bit_p (not_read_b, \n+\t\t\t\t\t    DECL_UID (real_var)) : false;\n+      not_written = not_written_b ? bitmap_bit_p (not_written_b, \n+\t\t\t\t\t\t  DECL_UID (real_var)) : false;\n+      gcc_assert (!unmodifiable_var_p (var));\n+      \n+      clobber_stats.clobbered_vars++;\n+\n+      /* See if this variable is really clobbered by this function.  */\n+\n+      /* Trivial case: Things escaping only to pure/const are not\n+\t clobbered by non-pure-const, and only read by pure/const. */\n+      if ((escape_mask & ~(ESCAPE_TO_PURE_CONST)) == 0)\n+\t{\n+\t  tree call = get_call_expr_in (stmt);\n+\t  if (call_expr_flags (call) & (ECF_CONST | ECF_PURE))\n+\t    {\n+\t      add_stmt_operand (&var, s_ann, opf_none);\n+\t      clobber_stats.unescapable_clobbers_avoided++;\n+\t      continue;\n+\t    }\n+\t  else\n+\t    {\n+\t      clobber_stats.unescapable_clobbers_avoided++;\n+\t      continue;\n+\t    }\n+\t}\n+            \n+      if (not_written)\n+\t{\n+\t  clobber_stats.static_write_clobbers_avoided++;\n+\t  if (!not_read)\n+\t    add_stmt_operand (&var, s_ann, opf_none);\n+\t  else\n+\t    clobber_stats.static_read_clobbers_avoided++;\n+\t}\n+      else\n+\tadd_virtual_operand (var, s_ann, opf_is_def, \n+\t\t\t     NULL, 0, -1, true);\n+    }\n+  \n+}\n+\n+\n+/* Add VUSE operands for .GLOBAL_VAR or all call clobbered variables in the\n+   function.  */\n+\n+static void\n+add_call_read_ops (tree stmt, tree callee)\n+{\n+  unsigned u;\n+  bitmap_iterator bi;\n+  stmt_ann_t s_ann = stmt_ann (stmt);\n+  bitmap not_read_b;\n+\n+  /* if the function is not pure, it may reference memory.  Add\n+     a VUSE for .GLOBAL_VAR if it has been created.  See add_referenced_var\n+     for the heuristic used to decide whether to create .GLOBAL_VAR.  */\n+  if (global_var)\n+    {\n+      add_stmt_operand (&global_var, s_ann, opf_none);\n+      return;\n+    }\n+  \n+  not_read_b = callee ? ipa_reference_get_not_read_global (callee) : NULL; \n+\n+  /* Add a VUSE for each call-clobbered variable.  */\n+  EXECUTE_IF_SET_IN_BITMAP (call_clobbered_vars, 0, u, bi)\n+    {\n+      tree var = referenced_var (u);\n+      tree real_var = var;\n+      bool not_read;\n+      \n+      clobber_stats.readonly_clobbers++;\n+\n+      /* Not read and not written are computed on regular vars, not\n+\t subvars, so look at the parent var if this is an SFT. */\n+\n+      if (TREE_CODE (var) == STRUCT_FIELD_TAG)\n+\treal_var = SFT_PARENT_VAR (var);\n+\n+      not_read = not_read_b ? bitmap_bit_p (not_read_b, \n+\t\t\t\t\t    DECL_UID (real_var)) : false;\n+      \n+      if (not_read)\n+\t{\n+\t  clobber_stats.static_readonly_clobbers_avoided++;\n+\t  continue;\n+\t}\n+            \n+      add_stmt_operand (&var, s_ann, opf_none | opf_non_specific);\n+    }\n+}\n+\n+\n+/* A subroutine of get_expr_operands to handle CALL_EXPR.  */\n+\n+static void\n+get_call_expr_operands (tree stmt, tree expr)\n+{\n+  tree op;\n+  int call_flags = call_expr_flags (expr);\n+\n+  /* If aliases have been computed already, add V_MAY_DEF or V_USE\n+     operands for all the symbols that have been found to be\n+     call-clobbered.\n+     \n+     Note that if aliases have not been computed, the global effects\n+     of calls will not be included in the SSA web. This is fine\n+     because no optimizer should run before aliases have been\n+     computed.  By not bothering with virtual operands for CALL_EXPRs\n+     we avoid adding superfluous virtual operands, which can be a\n+     significant compile time sink (See PR 15855).  */\n+  if (aliases_computed_p\n+      && !bitmap_empty_p (call_clobbered_vars)\n+      && !(call_flags & ECF_NOVOPS))\n+    {\n+      /* A 'pure' or a 'const' function never call-clobbers anything. \n+\t A 'noreturn' function might, but since we don't return anyway \n+\t there is no point in recording that.  */ \n+      if (TREE_SIDE_EFFECTS (expr)\n+\t  && !(call_flags & (ECF_PURE | ECF_CONST | ECF_NORETURN)))\n+\tadd_call_clobber_ops (stmt, get_callee_fndecl (expr));\n+      else if (!(call_flags & ECF_CONST))\n+\tadd_call_read_ops (stmt, get_callee_fndecl (expr));\n+    }\n+\n+  /* Find uses in the called function.  */\n+  get_expr_operands (stmt, &TREE_OPERAND (expr, 0), opf_none);\n+\n+  for (op = TREE_OPERAND (expr, 1); op; op = TREE_CHAIN (op))\n+    get_expr_operands (stmt, &TREE_VALUE (op), opf_none);\n+\n+  get_expr_operands (stmt, &TREE_OPERAND (expr, 2), opf_none);\n+}\n+\n+\n+/* Scan operands in the ASM_EXPR stmt referred to in INFO.  */\n+\n+static void\n+get_asm_expr_operands (tree stmt)\n+{\n+  stmt_ann_t s_ann = stmt_ann (stmt);\n+  int noutputs = list_length (ASM_OUTPUTS (stmt));\n+  const char **oconstraints\n+    = (const char **) alloca ((noutputs) * sizeof (const char *));\n+  int i;\n+  tree link;\n+  const char *constraint;\n+  bool allows_mem, allows_reg, is_inout;\n+\n+  for (i=0, link = ASM_OUTPUTS (stmt); link; ++i, link = TREE_CHAIN (link))\n+    {\n+      oconstraints[i] = constraint\n+\t= TREE_STRING_POINTER (TREE_VALUE (TREE_PURPOSE (link)));\n+      parse_output_constraint (&constraint, i, 0, 0,\n+\t  &allows_mem, &allows_reg, &is_inout);\n+\n+      /* This should have been split in gimplify_asm_expr.  */\n+      gcc_assert (!allows_reg || !is_inout);\n+\n+      /* Memory operands are addressable.  Note that STMT needs the\n+\t address of this operand.  */\n+      if (!allows_reg && allows_mem)\n+\t{\n+\t  tree t = get_base_address (TREE_VALUE (link));\n+\t  if (t && DECL_P (t) && s_ann)\n+\t    add_to_addressable_set (t, &s_ann->addresses_taken);\n+\t}\n+\n+      get_expr_operands (stmt, &TREE_VALUE (link), opf_is_def);\n+    }\n+\n+  for (link = ASM_INPUTS (stmt); link; link = TREE_CHAIN (link))\n+    {\n+      constraint = TREE_STRING_POINTER (TREE_VALUE (TREE_PURPOSE (link)));\n+      parse_input_constraint (&constraint, 0, 0, noutputs, 0,\n+\t\t\t      oconstraints, &allows_mem, &allows_reg);\n+\n+      /* Memory operands are addressable.  Note that STMT needs the\n+\t address of this operand.  */\n+      if (!allows_reg && allows_mem)\n+\t{\n+\t  tree t = get_base_address (TREE_VALUE (link));\n+\t  if (t && DECL_P (t) && s_ann)\n+\t    add_to_addressable_set (t, &s_ann->addresses_taken);\n+\t}\n+\n+      get_expr_operands (stmt, &TREE_VALUE (link), 0);\n+    }\n+\n+\n+  /* Clobber memory for asm (\"\" : : : \"memory\");  */\n+  for (link = ASM_CLOBBERS (stmt); link; link = TREE_CHAIN (link))\n+    if (strcmp (TREE_STRING_POINTER (TREE_VALUE (link)), \"memory\") == 0)\n+      {\n+\tunsigned i;\n+\tbitmap_iterator bi;\n+\n+\t/* Clobber all call-clobbered variables (or .GLOBAL_VAR if we\n+\t   decided to group them).  */\n+\tif (global_var)\n+\t  add_stmt_operand (&global_var, s_ann, opf_is_def);\n+\telse\n+\t  EXECUTE_IF_SET_IN_BITMAP (call_clobbered_vars, 0, i, bi)\n+\t    {\n+\t      tree var = referenced_var (i);\n+\t      add_stmt_operand (&var, s_ann, opf_is_def | opf_non_specific);\n+\t    }\n+\n+\t/* Now clobber all addressables.  */\n+\tEXECUTE_IF_SET_IN_BITMAP (addressable_vars, 0, i, bi)\n+\t    {\n+\t      tree var = referenced_var (i);\n+\n+\t      /* Subvars are explicitly represented in this list, so\n+\t\t we don't need the original to be added to the clobber\n+\t\t ops, but the original *will* be in this list because \n+\t\t we keep the addressability of the original\n+\t\t variable up-to-date so we don't screw up the rest of\n+\t\t the backend.  */\n+\t      if (var_can_have_subvars (var)\n+\t\t  && get_subvars_for_var (var) != NULL)\n+\t\tcontinue;\t\t\n+\n+\t      add_stmt_operand (&var, s_ann, opf_is_def | opf_non_specific);\n+\t    }\n+\n+\tbreak;\n+      }\n+}\n+\n+\n+/* Recursively scan the expression pointed to by EXPR_P in statement\n+   referred to by INFO.  FLAGS is one of the OPF_* constants modifying\n+   how to interpret the operands found.  */\n+\n+static void\n+get_expr_operands (tree stmt, tree *expr_p, int flags)\n+{\n+  enum tree_code code;\n+  enum tree_code_class class;\n+  tree expr = *expr_p;\n+  stmt_ann_t s_ann = stmt_ann (stmt);\n+\n+  if (expr == NULL)\n+    return;\n+\n+  code = TREE_CODE (expr);\n+  class = TREE_CODE_CLASS (code);\n+\n+  switch (code)\n+    {\n+    case ADDR_EXPR:\n+      /* Taking the address of a variable does not represent a\n+\t reference to it, but the fact that the statement takes its\n+\t address will be of interest to some passes (e.g. alias\n+\t resolution).  */\n+      add_to_addressable_set (TREE_OPERAND (expr, 0), &s_ann->addresses_taken);\n+\n+      /* If the address is invariant, there may be no interesting\n+\t variable references inside.  */\n+      if (is_gimple_min_invariant (expr))\n+\treturn;\n+\n+      /* Otherwise, there may be variables referenced inside but there\n+\t should be no VUSEs created, since the referenced objects are\n+\t not really accessed.  The only operands that we should find\n+\t here are ARRAY_REF indices which will always be real operands\n+\t (GIMPLE does not allow non-registers as array indices).  */\n+      flags |= opf_no_vops;\n+      get_expr_operands (stmt, &TREE_OPERAND (expr, 0), flags);\n+      return;\n+\n+    case SSA_NAME:\n+    case STRUCT_FIELD_TAG:\n+    case TYPE_MEMORY_TAG:\n+    case NAME_MEMORY_TAG:\n+     add_stmt_operand (expr_p, s_ann, flags);\n+     return;\n+\n+    case VAR_DECL:\n+    case PARM_DECL:\n+    case RESULT_DECL:\n+      {\n+\tsubvar_t svars;\n+\t\n+\t/* Add the subvars for a variable if it has subvars, to DEFS\n+\t   or USES.  Otherwise, add the variable itself.  Whether it\n+\t   goes to USES or DEFS depends on the operand flags.  */\n+\tif (var_can_have_subvars (expr)\n+\t    && (svars = get_subvars_for_var (expr)))\n+\t  {\n+\t    subvar_t sv;\n+\t    for (sv = svars; sv; sv = sv->next)\n+\t      add_stmt_operand (&sv->var, s_ann, flags);\n+\t  }\n+\telse\n+\t  add_stmt_operand (expr_p, s_ann, flags);\n+\n+\treturn;\n+      }\n+\n+    case MISALIGNED_INDIRECT_REF:\n+      get_expr_operands (stmt, &TREE_OPERAND (expr, 1), flags);\n+      /* fall through */\n+\n+    case ALIGN_INDIRECT_REF:\n+    case INDIRECT_REF:\n+      get_indirect_ref_operands (stmt, expr, flags, NULL_TREE,\n+\t\t\t\t 0, -1, true);\n+      return;\n+\n+    case TARGET_MEM_REF:\n+      get_tmr_operands (stmt, expr, flags);\n+      return;\n+\n+    case ARRAY_RANGE_REF:\n+      /* Treat array references as references to the virtual variable\n+\t representing the array.  The virtual variable for an ARRAY_REF\n+\t is the VAR_DECL for the array.  */\n+      get_expr_operands (stmt, &TREE_OPERAND (expr, 0), flags);\n+      get_expr_operands (stmt, &TREE_OPERAND (expr, 1), opf_none);\n+      get_expr_operands (stmt, &TREE_OPERAND (expr, 2), opf_none);\n+      get_expr_operands (stmt, &TREE_OPERAND (expr, 3), opf_none);\n+      return;\n+\n+    case ARRAY_REF:\n+    case COMPONENT_REF:\n+    case REALPART_EXPR:\n+    case IMAGPART_EXPR:\n+      {\n \ttree ref;\n \tHOST_WIDE_INT offset, size, maxsize;\n \tbool none = true;\n@@ -1309,625 +1719,352 @@ get_expr_operands (tree stmt, tree *expr_p, int flags)\n     case OMP_MASTER:\n     case OMP_ORDERED:\n     case OMP_CRITICAL:\n-      /* Expressions that make no memory references.  */\n-      return;\n-\n-    default:\n-      if (class == tcc_unary)\n-\tgoto do_unary;\n-      if (class == tcc_binary || class == tcc_comparison)\n-\tgoto do_binary;\n-      if (class == tcc_constant || class == tcc_type)\n-\treturn;\n-    }\n-\n-  /* If we get here, something has gone wrong.  */\n-#ifdef ENABLE_CHECKING\n-  fprintf (stderr, \"unhandled expression in get_expr_operands():\\n\");\n-  debug_tree (expr);\n-  fputs (\"\\n\", stderr);\n-  internal_error (\"internal error\");\n-#endif\n-  gcc_unreachable ();\n-}\n-\n-\n-/* Scan operands in the ASM_EXPR stmt referred to in INFO.  */\n-\n-static void\n-get_asm_expr_operands (tree stmt)\n-{\n-  stmt_ann_t s_ann = stmt_ann (stmt);\n-  int noutputs = list_length (ASM_OUTPUTS (stmt));\n-  const char **oconstraints\n-    = (const char **) alloca ((noutputs) * sizeof (const char *));\n-  int i;\n-  tree link;\n-  const char *constraint;\n-  bool allows_mem, allows_reg, is_inout;\n-\n-  for (i=0, link = ASM_OUTPUTS (stmt); link; ++i, link = TREE_CHAIN (link))\n-    {\n-      oconstraints[i] = constraint\n-\t= TREE_STRING_POINTER (TREE_VALUE (TREE_PURPOSE (link)));\n-      parse_output_constraint (&constraint, i, 0, 0,\n-\t  &allows_mem, &allows_reg, &is_inout);\n-\n-      /* This should have been split in gimplify_asm_expr.  */\n-      gcc_assert (!allows_reg || !is_inout);\n-\n-      /* Memory operands are addressable.  Note that STMT needs the\n-\t address of this operand.  */\n-      if (!allows_reg && allows_mem)\n-\t{\n-\t  tree t = get_base_address (TREE_VALUE (link));\n-\t  if (t && DECL_P (t) && s_ann)\n-\t    add_to_addressable_set (t, &s_ann->addresses_taken);\n-\t}\n-\n-      get_expr_operands (stmt, &TREE_VALUE (link), opf_is_def);\n-    }\n-\n-  for (link = ASM_INPUTS (stmt); link; link = TREE_CHAIN (link))\n-    {\n-      constraint = TREE_STRING_POINTER (TREE_VALUE (TREE_PURPOSE (link)));\n-      parse_input_constraint (&constraint, 0, 0, noutputs, 0,\n-\t\t\t      oconstraints, &allows_mem, &allows_reg);\n-\n-      /* Memory operands are addressable.  Note that STMT needs the\n-\t address of this operand.  */\n-      if (!allows_reg && allows_mem)\n-\t{\n-\t  tree t = get_base_address (TREE_VALUE (link));\n-\t  if (t && DECL_P (t) && s_ann)\n-\t    add_to_addressable_set (t, &s_ann->addresses_taken);\n-\t}\n-\n-      get_expr_operands (stmt, &TREE_VALUE (link), 0);\n-    }\n-\n-\n-  /* Clobber memory for asm (\"\" : : : \"memory\");  */\n-  for (link = ASM_CLOBBERS (stmt); link; link = TREE_CHAIN (link))\n-    if (strcmp (TREE_STRING_POINTER (TREE_VALUE (link)), \"memory\") == 0)\n-      {\n-\tunsigned i;\n-\tbitmap_iterator bi;\n-\n-\t/* Clobber all call-clobbered variables (or .GLOBAL_VAR if we\n-\t   decided to group them).  */\n-\tif (global_var)\n-\t  add_stmt_operand (&global_var, s_ann, opf_is_def);\n-\telse\n-\t  EXECUTE_IF_SET_IN_BITMAP (call_clobbered_vars, 0, i, bi)\n-\t    {\n-\t      tree var = referenced_var (i);\n-\t      add_stmt_operand (&var, s_ann, opf_is_def | opf_non_specific);\n-\t    }\n-\n-\t/* Now clobber all addressables.  */\n-\tEXECUTE_IF_SET_IN_BITMAP (addressable_vars, 0, i, bi)\n-\t    {\n-\t      tree var = referenced_var (i);\n-\n-\t      /* Subvars are explicitly represented in this list, so\n-\t\t we don't need the original to be added to the clobber\n-\t\t ops, but the original *will* be in this list because \n-\t\t we keep the addressability of the original\n-\t\t variable up-to-date so we don't screw up the rest of\n-\t\t the backend.  */\n-\t      if (var_can_have_subvars (var)\n-\t\t  && get_subvars_for_var (var) != NULL)\n-\t\tcontinue;\t\t\n-\n-\t      add_stmt_operand (&var, s_ann, opf_is_def | opf_non_specific);\n-\t    }\n-\n-\tbreak;\n-      }\n-}\n-\n-/* A subroutine of get_expr_operands to handle INDIRECT_REF,\n-   ALIGN_INDIRECT_REF and MISALIGNED_INDIRECT_REF.  \n-\n-   STMT is the statement being processed, EXPR is the INDIRECT_REF\n-      that got us here.\n-   \n-   FLAGS is as in get_expr_operands.\n-\n-   FULL_REF contains the full pointer dereference expression, if we\n-      have it, or NULL otherwise.\n-\n-   OFFSET and SIZE are the location of the access inside the\n-      dereferenced pointer, if known.\n-\n-   RECURSE_ON_BASE should be set to true if we want to continue\n-      calling get_expr_operands on the base pointer, and false if\n-      something else will do it for us.  */\n-\n-static void\n-get_indirect_ref_operands (tree stmt, tree expr, int flags,\n-\t\t\t   tree full_ref,\n-\t\t\t   HOST_WIDE_INT offset, HOST_WIDE_INT size,\n-\t\t\t   bool recurse_on_base)\n-{\n-  tree *pptr = &TREE_OPERAND (expr, 0);\n-  tree ptr = *pptr;\n-  stmt_ann_t s_ann = stmt_ann (stmt);\n-\n-  /* Stores into INDIRECT_REF operands are never killing definitions.  */\n-  flags &= ~opf_kill_def;\n-\n-  if (SSA_VAR_P (ptr))\n-    {\n-      struct ptr_info_def *pi = NULL;\n-\n-      /* If PTR has flow-sensitive points-to information, use it.  */\n-      if (TREE_CODE (ptr) == SSA_NAME\n-\t  && (pi = SSA_NAME_PTR_INFO (ptr)) != NULL\n-\t  && pi->name_mem_tag)\n-\t{\n-\t  /* PTR has its own memory tag.  Use it.  */\n-\t  add_virtual_operand (pi->name_mem_tag, s_ann, flags,\n-\t\t\t       full_ref, offset, size, false);\n-\t}\n-      else\n-\t{\n-\t  /* If PTR is not an SSA_NAME or it doesn't have a name\n-\t     tag, use its type memory tag.  */\n-\t  var_ann_t v_ann;\n-\n-\t  /* If we are emitting debugging dumps, display a warning if\n-\t     PTR is an SSA_NAME with no flow-sensitive alias\n-\t     information.  That means that we may need to compute\n-\t     aliasing again.  */\n-\t  if (dump_file\n-\t      && TREE_CODE (ptr) == SSA_NAME\n-\t      && pi == NULL)\n-\t    {\n-\t      fprintf (dump_file,\n-\t\t  \"NOTE: no flow-sensitive alias info for \");\n-\t      print_generic_expr (dump_file, ptr, dump_flags);\n-\t      fprintf (dump_file, \" in \");\n-\t      print_generic_stmt (dump_file, stmt, dump_flags);\n-\t    }\n-\n-\t  if (TREE_CODE (ptr) == SSA_NAME)\n-\t    ptr = SSA_NAME_VAR (ptr);\n-\t  v_ann = var_ann (ptr);\n-\n-\t  if (v_ann->type_mem_tag)\n-\t    add_virtual_operand (v_ann->type_mem_tag, s_ann, flags,\n-\t\t\t\t full_ref, offset, size, false);\n-\t}\n-    }\n-  else if (TREE_CODE (ptr) == INTEGER_CST)\n-    {\n-      /* If a constant is used as a pointer, we can't generate a real\n-\t operand for it but we mark the statement volatile to prevent\n-\t optimizations from messing things up.  */\n-      if (s_ann)\n-\ts_ann->has_volatile_ops = true;\n+      /* Expressions that make no memory references.  */\n       return;\n-    }\n-  else\n-    {\n-      /* Ok, this isn't even is_gimple_min_invariant.  Something's broke.  */\n-      gcc_unreachable ();\n+\n+    default:\n+      if (class == tcc_unary)\n+\tgoto do_unary;\n+      if (class == tcc_binary || class == tcc_comparison)\n+\tgoto do_binary;\n+      if (class == tcc_constant || class == tcc_type)\n+\treturn;\n     }\n \n-  /* If requested, add a USE operand for the base pointer.  */\n-  if (recurse_on_base)\n-    get_expr_operands (stmt, pptr, opf_none);\n+  /* If we get here, something has gone wrong.  */\n+#ifdef ENABLE_CHECKING\n+  fprintf (stderr, \"unhandled expression in get_expr_operands():\\n\");\n+  debug_tree (expr);\n+  fputs (\"\\n\", stderr);\n+#endif\n+  gcc_unreachable ();\n }\n \n \n-/* A subroutine of get_expr_operands to handle TARGET_MEM_REF.  */\n-\n+/* Parse STMT looking for operands.  OLD_OPS is the original stmt operand\n+   cache for STMT, if it existed before.  When finished, the various build_*\n+   operand vectors will have potential operands. in them.  */\n+                                                                                \n static void\n-get_tmr_operands (tree stmt, tree expr, int flags)\n+parse_ssa_operands (tree stmt)\n {\n-  tree tag = TMR_TAG (expr), ref;\n-  HOST_WIDE_INT offset, size, maxsize;\n-  subvar_t svars, sv;\n-  stmt_ann_t s_ann = stmt_ann (stmt);\n+  enum tree_code code;\n \n-  /* First record the real operands.  */\n-  get_expr_operands (stmt, &TMR_BASE (expr), opf_none);\n-  get_expr_operands (stmt, &TMR_INDEX (expr), opf_none);\n+  code = TREE_CODE (stmt);\n+  switch (code)\n+    {\n+    case MODIFY_EXPR:\n+      /* First get operands from the RHS.  For the LHS, we use a V_MAY_DEF if\n+\t either only part of LHS is modified or if the RHS might throw,\n+\t otherwise, use V_MUST_DEF.\n \n-  /* MEM_REFs should never be killing.  */\n-  flags &= ~opf_kill_def;\n+\t ??? If it might throw, we should represent somehow that it is killed\n+\t on the fallthrough path.  */\n+      {\n+\ttree lhs = TREE_OPERAND (stmt, 0);\n+\tint lhs_flags = opf_is_def;\n \n-  if (TMR_SYMBOL (expr))\n-    {\n-      stmt_ann_t ann = stmt_ann (stmt);\n-      add_to_addressable_set (TMR_SYMBOL (expr), &ann->addresses_taken);\n-    }\n+\tget_expr_operands (stmt, &TREE_OPERAND (stmt, 1), opf_none);\n \n-  if (!tag)\n-    {\n-      /* Something weird, so ensure that we will be careful.  */\n-      stmt_ann (stmt)->has_volatile_ops = true;\n-      return;\n-    }\n+\t/* If the LHS is a VIEW_CONVERT_EXPR, it isn't changing whether\n+\t   or not the entire LHS is modified; that depends on what's\n+\t   inside the VIEW_CONVERT_EXPR.  */\n+\tif (TREE_CODE (lhs) == VIEW_CONVERT_EXPR)\n+\t  lhs = TREE_OPERAND (lhs, 0);\n \n-  if (DECL_P (tag))\n-    {\n-      get_expr_operands (stmt, &tag, flags);\n-      return;\n-    }\n+\tif (TREE_CODE (lhs) != ARRAY_RANGE_REF\n+\t    && TREE_CODE (lhs) != BIT_FIELD_REF)\n+\t  lhs_flags |= opf_kill_def;\n \n-  ref = get_ref_base_and_extent (tag, &offset, &size, &maxsize);\n-  gcc_assert (ref != NULL_TREE);\n-  svars = get_subvars_for_var (ref);\n-  for (sv = svars; sv; sv = sv->next)\n-    {\n-      bool exact;\t\t\n-      if (overlap_subvar (offset, maxsize, sv->var, &exact))\n-\t{\n-\t  int subvar_flags = flags;\n-\t  if (!exact || size != maxsize)\n-\t    subvar_flags &= ~opf_kill_def;\n-\t  add_stmt_operand (&sv->var, s_ann, subvar_flags);\n-\t}\n+        get_expr_operands (stmt, &TREE_OPERAND (stmt, 0), lhs_flags);\n+      }\n+      break;\n+\n+    case COND_EXPR:\n+      get_expr_operands (stmt, &COND_EXPR_COND (stmt), opf_none);\n+      break;\n+\n+    case SWITCH_EXPR:\n+      get_expr_operands (stmt, &SWITCH_COND (stmt), opf_none);\n+      break;\n+\n+    case ASM_EXPR:\n+      get_asm_expr_operands (stmt);\n+      break;\n+\n+    case RETURN_EXPR:\n+      get_expr_operands (stmt, &TREE_OPERAND (stmt, 0), opf_none);\n+      break;\n+\n+    case GOTO_EXPR:\n+      get_expr_operands (stmt, &GOTO_DESTINATION (stmt), opf_none);\n+      break;\n+\n+    case LABEL_EXPR:\n+      get_expr_operands (stmt, &LABEL_EXPR_LABEL (stmt), opf_none);\n+      break;\n+\n+      /* These nodes contain no variable references.  */\n+    case BIND_EXPR:\n+    case CASE_LABEL_EXPR:\n+    case TRY_CATCH_EXPR:\n+    case TRY_FINALLY_EXPR:\n+    case EH_FILTER_EXPR:\n+    case CATCH_EXPR:\n+    case RESX_EXPR:\n+      break;\n+\n+    default:\n+      /* Notice that if get_expr_operands tries to use &STMT as the operand\n+\t pointer (which may only happen for USE operands), we will fail in\n+\t append_use.  This default will handle statements like empty\n+\t statements, or CALL_EXPRs that may appear on the RHS of a statement\n+\t or as statements themselves.  */\n+      get_expr_operands (stmt, &stmt, opf_none);\n+      break;\n     }\n }\n \n \n-/* A subroutine of get_expr_operands to handle CALL_EXPR.  */\n+/* Create an operands cache for STMT.  */\n \n static void\n-get_call_expr_operands (tree stmt, tree expr)\n+build_ssa_operands (tree stmt)\n {\n-  tree op;\n-  int call_flags = call_expr_flags (expr);\n+  stmt_ann_t ann = get_stmt_ann (stmt);\n+  \n+  /* Initially assume that the statement has no volatile operands.  */\n+  if (ann)\n+    ann->has_volatile_ops = false;\n \n-  /* If aliases have been computed already, add V_MAY_DEF or V_USE\n-     operands for all the symbols that have been found to be\n-     call-clobbered.\n-     \n-     Note that if aliases have not been computed, the global effects\n-     of calls will not be included in the SSA web. This is fine\n-     because no optimizer should run before aliases have been\n-     computed.  By not bothering with virtual operands for CALL_EXPRs\n-     we avoid adding superfluous virtual operands, which can be a\n-     significant compile time sink (See PR 15855).  */\n-  if (aliases_computed_p\n-      && !bitmap_empty_p (call_clobbered_vars)\n-      && !(call_flags & ECF_NOVOPS))\n-    {\n-      /* A 'pure' or a 'const' function never call-clobbers anything. \n-\t A 'noreturn' function might, but since we don't return anyway \n-\t there is no point in recording that.  */ \n-      if (TREE_SIDE_EFFECTS (expr)\n-\t  && !(call_flags & (ECF_PURE | ECF_CONST | ECF_NORETURN)))\n-\tadd_call_clobber_ops (stmt, get_callee_fndecl (expr));\n-      else if (!(call_flags & ECF_CONST))\n-\tadd_call_read_ops (stmt, get_callee_fndecl (expr));\n-    }\n+  start_ssa_stmt_operands ();\n \n-  /* Find uses in the called function.  */\n-  get_expr_operands (stmt, &TREE_OPERAND (expr, 0), opf_none);\n+  parse_ssa_operands (stmt);\n+  operand_build_sort_virtual (build_vuses);\n+  operand_build_sort_virtual (build_v_may_defs);\n+  operand_build_sort_virtual (build_v_must_defs);\n \n-  for (op = TREE_OPERAND (expr, 1); op; op = TREE_CHAIN (op))\n-    get_expr_operands (stmt, &TREE_VALUE (op), opf_none);\n+  finalize_ssa_stmt_operands (stmt);\n+}\n \n-  get_expr_operands (stmt, &TREE_OPERAND (expr, 2), opf_none);\n \n+/* Free any operands vectors in OPS.  */\n+void \n+free_ssa_operands (stmt_operands_p ops)\n+{\n+  ops->def_ops = NULL;\n+  ops->use_ops = NULL;\n+  ops->maydef_ops = NULL;\n+  ops->mustdef_ops = NULL;\n+  ops->vuse_ops = NULL;\n }\n \n-/* REF is a tree that contains the entire pointer dereference\n-   expression, if available, or NULL otherwise.  ALIAS is the variable\n-   we are asking if REF can access.  OFFSET and SIZE come from the\n-   memory access expression that generated this virtual operand.\n-   FOR_CLOBBER is true is this is adding a virtual operand for a call\n-   clobber.  */\n-\n-static bool\n-access_can_touch_variable (tree ref, tree alias, HOST_WIDE_INT offset,\n-\t\t\t   HOST_WIDE_INT size)\n-{  \n-  bool offsetgtz = offset > 0;\n-  unsigned HOST_WIDE_INT uoffset = (unsigned HOST_WIDE_INT) offset;\n-  tree base = ref ? get_base_address (ref) : NULL;\n \n-  /* If ALIAS is an SFT, it can't be touched if the offset     \n-     and size of the access is not overlapping with the SFT offset and\n-     size.  This is only true if we are accessing through a pointer\n-     to a type that is the same as SFT_PARENT_VAR.  Otherwise, we may\n-     be accessing through a pointer to some substruct of the\n-     structure, and if we try to prune there, we will have the wrong\n-     offset, and get the wrong answer.\n-     i.e., we can't prune without more work if we have something like\n+/* Get the operands of statement STMT.  Note that repeated calls to\n+   get_stmt_operands for the same statement will do nothing until the\n+   statement is marked modified by a call to mark_stmt_modified().  */\n \n-     struct gcc_target\n-     {\n-       struct asm_out\n-       {\n-         const char *byte_op;\n-\t struct asm_int_op\n-\t {    \n-\t   const char *hi;\n-\t } aligned_op;\n-       } asm_out;\n-     } targetm;\n-     \n-     foo = &targetm.asm_out.aligned_op;\n-     return foo->hi;\n+void\n+update_stmt_operands (tree stmt)\n+{\n+  stmt_ann_t ann = get_stmt_ann (stmt);\n \n-     SFT.1, which represents hi, will have SFT_OFFSET=32 because in\n-     terms of SFT_PARENT_VAR, that is where it is.\n-     However, the access through the foo pointer will be at offset 0.  */\n-  if (size != -1\n-      && TREE_CODE (alias) == STRUCT_FIELD_TAG\n-      && base\n-      && TREE_TYPE (base) == TREE_TYPE (SFT_PARENT_VAR (alias))\n-      && !overlap_subvar (offset, size, alias, NULL))\n-    {\n-#ifdef ACCESS_DEBUGGING\n-      fprintf (stderr, \"Access to \");\n-      print_generic_expr (stderr, ref, 0);\n-      fprintf (stderr, \" may not touch \");\n-      print_generic_expr (stderr, alias, 0);\n-      fprintf (stderr, \" in function %s\\n\", get_name (current_function_decl));\n-#endif\n-      return false;\n-    }\n+  /* If get_stmt_operands is called before SSA is initialized, dont\n+  do anything.  */\n+  if (!ssa_operands_active ())\n+    return;\n \n-  /* Without strict aliasing, it is impossible for a component access\n-     through a pointer to touch a random variable, unless that\n-     variable *is* a structure or a pointer.\n+  /* The optimizers cannot handle statements that are nothing but a\n+     _DECL.  This indicates a bug in the gimplifier.  */\n+  gcc_assert (!SSA_VAR_P (stmt));\n \n-     That is, given p->c, and some random global variable b,\n-     there is no legal way that p->c could be an access to b.\n-     \n-     Without strict aliasing on, we consider it legal to do something\n-     like:\n+  gcc_assert (ann->modified);\n \n-     struct foos { int l; };\n-     int foo;\n-     static struct foos *getfoo(void);\n-     int main (void)\n-     {\n-       struct foos *f = getfoo();\n-       f->l = 1;\n-       foo = 2;\n-       if (f->l == 1)\n-         abort();\n-       exit(0);\n-     }\n-     static struct foos *getfoo(void)     \n-     { return (struct foos *)&foo; }\n-     \n-     (taken from 20000623-1.c)\n-  */\n-  else if (ref \n-\t   && flag_strict_aliasing\n-\t   && TREE_CODE (ref) != INDIRECT_REF\n-\t   && !MTAG_P (alias)\n-\t   && !AGGREGATE_TYPE_P (TREE_TYPE (alias))\n-\t   && TREE_CODE (TREE_TYPE (alias)) != COMPLEX_TYPE\n-\t   && !POINTER_TYPE_P (TREE_TYPE (alias)))\n-    {\n-#ifdef ACCESS_DEBUGGING\n-      fprintf (stderr, \"Access to \");\n-      print_generic_expr (stderr, ref, 0);\n-      fprintf (stderr, \" may not touch \");\n-      print_generic_expr (stderr, alias, 0);\n-      fprintf (stderr, \" in function %s\\n\", get_name (current_function_decl));\n-#endif\n-      return false;\n-    }\n+  timevar_push (TV_TREE_OPS);\n \n-  /* If the offset of the access is greater than the size of one of\n-     the possible aliases, it can't be touching that alias, because it\n-     would be past the end of the structure.  */\n-  else if (ref\n-\t   && flag_strict_aliasing\n-\t   && TREE_CODE (ref) != INDIRECT_REF\n-\t   && !MTAG_P (alias)\n-\t   && !POINTER_TYPE_P (TREE_TYPE (alias))\n-\t   && offsetgtz\n-\t   && DECL_SIZE (alias)\n-\t   && TREE_CODE (DECL_SIZE (alias)) == INTEGER_CST\n-\t   && uoffset > TREE_INT_CST_LOW (DECL_SIZE (alias)))\n-    {\n-#ifdef ACCESS_DEBUGGING\n-      fprintf (stderr, \"Access to \");\n-      print_generic_expr (stderr, ref, 0);\n-      fprintf (stderr, \" may not touch \");\n-      print_generic_expr (stderr, alias, 0);\n-      fprintf (stderr, \" in function %s\\n\", get_name (current_function_decl));\n-#endif\n-      return false;\n-    }\t   \n+  build_ssa_operands (stmt);\n \n-  return true;\n-}\n+  /* Clear the modified bit for STMT.  Subsequent calls to\n+     get_stmt_operands for this statement will do nothing until the\n+     statement is marked modified by a call to mark_stmt_modified().  */\n+  ann->modified = 0;\n \n+  timevar_pop (TV_TREE_OPS);\n+}\n \n-/* Add VAR to the virtual operands array.  FLAGS is as in\n-   get_expr_operands.  FULL_REF is a tree that contains the entire\n-   pointer dereference expression, if available, or NULL otherwise.\n-   OFFSET and SIZE come from the memory access expression that\n-   generated this virtual operand.  FOR_CLOBBER is true is this is\n-   adding a virtual operand for a call clobber.  */\n+  \n+/* Copies virtual operands from SRC to DST.  */\n \n-static void \n-add_virtual_operand (tree var, stmt_ann_t s_ann, int flags,\n-\t\t     tree full_ref, HOST_WIDE_INT offset,\n-\t\t     HOST_WIDE_INT size, bool for_clobber)\n+void\n+copy_virtual_operands (tree dest, tree src)\n {\n-  VEC(tree,gc) *aliases;\n-  tree sym;\n-  var_ann_t v_ann;\n-  \n-  sym = (TREE_CODE (var) == SSA_NAME ? SSA_NAME_VAR (var) : var);\n-  v_ann = var_ann (sym);\n-  \n-  /* Mark statements with volatile operands.  Optimizers should back\n-     off from statements having volatile operands.  */\n-  if (TREE_THIS_VOLATILE (sym) && s_ann)\n-    s_ann->has_volatile_ops = true;\n+  tree t;\n+  ssa_op_iter iter, old_iter;\n+  use_operand_p use_p, u2;\n+  def_operand_p def_p, d2;\n \n-  /* If the variable cannot be modified and this is a V_MAY_DEF change\n-     it into a VUSE.  This happens when read-only variables are marked\n-     call-clobbered and/or aliased to writable variables.  So we only\n-     check that this only happens on non-specific stores.\n+  build_ssa_operands (dest);\n \n-     Note that if this is a specific store, i.e. associated with a\n-     modify_expr, then we can't suppress the V_MAY_DEF, lest we run\n-     into validation problems.\n+  /* Copy all the virtual fields.  */\n+  FOR_EACH_SSA_TREE_OPERAND (t, src, iter, SSA_OP_VUSE)\n+    append_vuse (t);\n+  FOR_EACH_SSA_TREE_OPERAND (t, src, iter, SSA_OP_VMAYDEF)\n+    append_v_may_def (t);\n+  FOR_EACH_SSA_TREE_OPERAND (t, src, iter, SSA_OP_VMUSTDEF)\n+    append_v_must_def (t);\n \n-     This can happen when programs cast away const, leaving us with a\n-     store to read-only memory.  If the statement is actually executed\n-     at runtime, then the program is ill formed.  If the statement is\n-     not executed then all is well.  At the very least, we cannot ICE.  */\n-  if ((flags & opf_non_specific) && unmodifiable_var_p (var))\n-    flags &= ~(opf_is_def | opf_kill_def);\n-  \n-  /* The variable is not a GIMPLE register.  Add it (or its aliases) to\n-     virtual operands, unless the caller has specifically requested\n-     not to add virtual operands (used when adding operands inside an\n-     ADDR_EXPR expression).  */\n-  if (flags & opf_no_vops)\n+  if (VEC_length (tree, build_vuses) == 0\n+      && VEC_length (tree, build_v_may_defs) == 0\n+      && VEC_length (tree, build_v_must_defs) == 0)\n     return;\n+\n+  /* Now commit the virtual operands to this stmt.  */\n+  finalize_ssa_v_must_defs (dest);\n+  finalize_ssa_v_may_defs (dest);\n+  finalize_ssa_vuses (dest);\n+\n+  /* Finally, set the field to the same values as then originals.  */\n+\n   \n-  aliases = v_ann->may_aliases;\n-  if (aliases == NULL)\n+  t = op_iter_init_tree (&old_iter, src, SSA_OP_VUSE);\n+  FOR_EACH_SSA_USE_OPERAND (use_p, dest, iter, SSA_OP_VUSE)\n     {\n-      /* The variable is not aliased or it is an alias tag.  */\n-      if (flags & opf_is_def)\n-\t{\n-\t  if (flags & opf_kill_def)\n-\t    {\n-\t      /* V_MUST_DEF for non-aliased, non-GIMPLE register \n-\t\t variable definitions.  */\n-\t      gcc_assert (!MTAG_P (var)\n-\t\t\t  || TREE_CODE (var) == STRUCT_FIELD_TAG);\n-\t      append_v_must_def (var);\n-\t    }\n-\t  else\n-\t    {\n-\t      /* Add a V_MAY_DEF for call-clobbered variables and\n-\t\t memory tags.  */\n-\t      append_v_may_def (var);\n-\t    }\n-\t}\n-      else\n-\tappend_vuse (var);\n+      gcc_assert (!op_iter_done (&old_iter));\n+      SET_USE (use_p, t);\n+      t = op_iter_next_tree (&old_iter);\n     }\n-  else\n+  gcc_assert (op_iter_done (&old_iter));\n+\n+  op_iter_init_maydef (&old_iter, src, &u2, &d2);\n+  FOR_EACH_SSA_MAYDEF_OPERAND (def_p, use_p, dest, iter)\n     {\n-      unsigned i;\n-      tree al;\n-      \n-      /* The variable is aliased.  Add its aliases to the virtual\n-\t operands.  */\n-      gcc_assert (VEC_length (tree, aliases) != 0);\n-      \n-      if (flags & opf_is_def)\n-\t{\n-\t  \n-\t  bool none_added = true;\n+      gcc_assert (!op_iter_done (&old_iter));\n+      SET_USE (use_p, USE_FROM_PTR (u2));\n+      SET_DEF (def_p, DEF_FROM_PTR (d2));\n+      op_iter_next_maymustdef (&u2, &d2, &old_iter);\n+    }\n+  gcc_assert (op_iter_done (&old_iter));\n \n-\t  for (i = 0; VEC_iterate (tree, aliases, i, al); i++)\n-\t    {\n-\t      if (!access_can_touch_variable (full_ref, al, offset, size))\n-\t\tcontinue;\n-\t      \n-\t      none_added = false;\n-\t      append_v_may_def (al);\n-\t    }\n+  op_iter_init_mustdef (&old_iter, src, &u2, &d2);\n+  FOR_EACH_SSA_MUSTDEF_OPERAND (def_p, use_p, dest, iter)\n+    {\n+      gcc_assert (!op_iter_done (&old_iter));\n+      SET_USE (use_p, USE_FROM_PTR (u2));\n+      SET_DEF (def_p, DEF_FROM_PTR (d2));\n+      op_iter_next_maymustdef (&u2, &d2, &old_iter);\n+    }\n+  gcc_assert (op_iter_done (&old_iter));\n \n-\t  /* If the variable is also an alias tag, add a virtual\n-\t     operand for it, otherwise we will miss representing\n-\t     references to the members of the variable's alias set.\t     \n-\t     This fixes the bug in gcc.c-torture/execute/20020503-1.c.\n-\t     \n-\t     It is also necessary to add bare defs on clobbers for\n-\t     TMT's, so that bare TMT uses caused by pruning all the\n-\t     aliases will link up properly with calls.   In order to\n-\t     keep the number of these bare defs we add down to the\n-\t     minimum necessary, we keep track of which TMT's were used\n-\t     alone in statement defs or vuses.  */\n+}\n \n-\t  if (v_ann->is_aliased\n-\t      || none_added\n-\t      || (TREE_CODE (var) == TYPE_MEMORY_TAG && for_clobber\n-\t\t  && TMT_USED_ALONE (var)))\n-\t    {\n-\t      /* Every bare tmt def we add should have TMT_USED_ALONE\n-\t\t set on it, or else we will get the wrong answer on\n-\t\t clobbers.  */\n \n-\t      if (none_added && !updating_used_alone && aliases_computed_p\n-\t\t  && TREE_CODE (var) == TYPE_MEMORY_TAG)\n-\t\tgcc_assert (TMT_USED_ALONE (var));\n+/* Specifically for use in DOM's expression analysis.  Given a store, we\n+   create an artificial stmt which looks like a load from the store, this can\n+   be used to eliminate redundant loads.  OLD_OPS are the operands from the \n+   store stmt, and NEW_STMT is the new load which represents a load of the\n+   values stored.  */\n+\n+void\n+create_ssa_artficial_load_stmt (tree new_stmt, tree old_stmt)\n+{\n+  stmt_ann_t ann;\n+  tree op;\n+  ssa_op_iter iter;\n+  use_operand_p use_p;\n+  unsigned x;\n+\n+  ann = get_stmt_ann (new_stmt);\n+\n+  /* process the stmt looking for operands.  */\n+  start_ssa_stmt_operands ();\n+  parse_ssa_operands (new_stmt);\n \n-\t      append_v_may_def (var);\n-\t    }\n+  for (x = 0; x < VEC_length (tree, build_vuses); x++)\n+    {\n+      tree t = VEC_index (tree, build_vuses, x);\n+      if (TREE_CODE (t) != SSA_NAME)\n+\t{\n+\t  var_ann_t ann = var_ann (t);\n+\t  ann->in_vuse_list = 0;\n \t}\n-      else\n+    }\n+   \n+  for (x = 0; x < VEC_length (tree, build_v_may_defs); x++)\n+    {\n+      tree t = VEC_index (tree, build_v_may_defs, x);\n+      if (TREE_CODE (t) != SSA_NAME)\n \t{\n-\t  bool none_added = true;\n-\t  for (i = 0; VEC_iterate (tree, aliases, i, al); i++)\n-\t    {\n-\t      if (!access_can_touch_variable (full_ref, al, offset, size))\n-\t\tcontinue;\n-\t      none_added = false;\n-\t      append_vuse (al);\n-\t    }\n-\n-\t  /* Similarly, append a virtual uses for VAR itself, when\n-\t     it is an alias tag.  */\n-\t  if (v_ann->is_aliased || none_added)\n-\t    append_vuse (var);\n+\t  var_ann_t ann = var_ann (t);\n+\t  ann->in_v_may_def_list = 0;\n \t}\n     }\n-}\n \n+  /* Remove any virtual operands that were found.  */\n+  VEC_truncate (tree, build_v_may_defs, 0);\n+  VEC_truncate (tree, build_v_must_defs, 0);\n+  VEC_truncate (tree, build_vuses, 0);\n \n-/* Add *VAR_P to the appropriate operand array for S_ANN.  FLAGS is as in\n-   get_expr_operands.  If *VAR_P is a GIMPLE register, it will be added to\n-   the statement's real operands, otherwise it is added to virtual\n-   operands.  */\n+  /* For each VDEF on the original statement, we want to create a\n+     VUSE of the V_MAY_DEF result or V_MUST_DEF op on the new \n+     statement.  */\n+  FOR_EACH_SSA_TREE_OPERAND (op, old_stmt, iter, \n+\t\t\t     (SSA_OP_VMAYDEF | SSA_OP_VMUSTDEF))\n+    append_vuse (op);\n+    \n+  /* Now build the operands for this new stmt.  */\n+  finalize_ssa_stmt_operands (new_stmt);\n \n-static void\n-add_stmt_operand (tree *var_p, stmt_ann_t s_ann, int flags)\n-{\n-  bool is_real_op;\n-  tree var, sym;\n-  var_ann_t v_ann;\n+  /* All uses in this fake stmt must not be in the immediate use lists.  */\n+  FOR_EACH_SSA_USE_OPERAND (use_p, new_stmt, iter, SSA_OP_ALL_USES)\n+    delink_imm_use (use_p);\n+}\n \n-  var = *var_p;\n-  gcc_assert (SSA_VAR_P (var));\n \n-  is_real_op = is_gimple_reg (var);\n+/* Swap operands EXP0 and EXP1 in statement STMT.  No attempt is done\n+   to test the validity of the swap operation.  */\n \n-  /* If this is a real operand, the operand is either an SSA name or a \n-     decl.  Virtual operands may only be decls.  */\n-  gcc_assert (is_real_op || DECL_P (var));\n+void\n+swap_tree_operands (tree stmt, tree *exp0, tree *exp1)\n+{\n+  tree op0, op1;\n+  op0 = *exp0;\n+  op1 = *exp1;\n \n-  sym = (TREE_CODE (var) == SSA_NAME ? SSA_NAME_VAR (var) : var);\n-  v_ann = var_ann (sym);\n+  /* If the operand cache is active, attempt to preserve the relative positions\n+     of these two operands in their respective immediate use lists.  */\n+  if (ssa_operands_active () && op0 != op1)\n+    {\n+      use_optype_p use0, use1, ptr;\n+      use0 = use1 = NULL;\n \n-  /* Mark statements with volatile operands.  Optimizers should back\n-     off from statements having volatile operands.  */\n-  if (TREE_THIS_VOLATILE (sym) && s_ann)\n-    s_ann->has_volatile_ops = true;\n+      /* Find the 2 operands in the cache, if they are there.  */\n+      for (ptr = USE_OPS (stmt); ptr; ptr = ptr->next)\n+\tif (USE_OP_PTR (ptr)->use == exp0)\n+\t  {\n+\t    use0 = ptr;\n+\t    break;\n+\t  }\n \n-  if (is_real_op)\n-    {\n-      /* The variable is a GIMPLE register.  Add it to real operands.  */\n-      if (flags & opf_is_def)\n-\tappend_def (var_p);\n-      else\n-\tappend_use (var_p);\n+      for (ptr = USE_OPS (stmt); ptr; ptr = ptr->next)\n+\tif (USE_OP_PTR (ptr)->use == exp1)\n+\t  {\n+\t    use1 = ptr;\n+\t    break;\n+\t  }\n+\n+      /* If both uses don't have operand entries, there isn't much we can do\n+         at this point.  Presumably we dont need to worry about it.  */\n+      if (use0 && use1)\n+        {\n+\t  tree *tmp = USE_OP_PTR (use1)->use;\n+\t  USE_OP_PTR (use1)->use = USE_OP_PTR (use0)->use;\n+\t  USE_OP_PTR (use0)->use = tmp;\n+\t}\n     }\n-  else\n-    add_virtual_operand (var, s_ann, flags, NULL_TREE, 0, -1, false);\n+\n+  /* Now swap the data.  */\n+  *exp0 = op1;\n+  *exp1 = op0;\n }\n \n \n@@ -1976,144 +2113,6 @@ add_to_addressable_set (tree ref, bitmap *addresses_taken)\n }\n \n \n-/* Add clobbering definitions for .GLOBAL_VAR or for each of the call\n-   clobbered variables in the function.  */\n-\n-static void\n-add_call_clobber_ops (tree stmt, tree callee)\n-{\n-  unsigned u;\n-  bitmap_iterator bi;\n-  stmt_ann_t s_ann = stmt_ann (stmt);\n-  bitmap not_read_b, not_written_b;\n-  \n-  /* Functions that are not const, pure or never return may clobber\n-     call-clobbered variables.  */\n-  if (s_ann)\n-    s_ann->makes_clobbering_call = true;\n-\n-  /* If we created .GLOBAL_VAR earlier, just use it.  See compute_may_aliases \n-     for the heuristic used to decide whether to create .GLOBAL_VAR or not.  */\n-  if (global_var)\n-    {\n-      add_stmt_operand (&global_var, s_ann, opf_is_def);\n-      return;\n-    }\n-\n-  /* Get info for local and module level statics.  There is a bit\n-     set for each static if the call being processed does not read\n-     or write that variable.  */\n-  not_read_b = callee ? ipa_reference_get_not_read_global (callee) : NULL; \n-  not_written_b = callee ? ipa_reference_get_not_written_global (callee) : NULL; \n-  /* Add a V_MAY_DEF operand for every call clobbered variable.  */\n-  EXECUTE_IF_SET_IN_BITMAP (call_clobbered_vars, 0, u, bi)\n-    {\n-      tree var = referenced_var_lookup (u);\n-      unsigned int escape_mask = var_ann (var)->escape_mask;\n-      tree real_var = var;\n-      bool not_read;\n-      bool not_written;\n-      \n-      /* Not read and not written are computed on regular vars, not\n-\t subvars, so look at the parent var if this is an SFT. */\n-      if (TREE_CODE (var) == STRUCT_FIELD_TAG)\n-\treal_var = SFT_PARENT_VAR (var);\n-\n-      not_read = not_read_b ? bitmap_bit_p (not_read_b, \n-\t\t\t\t\t    DECL_UID (real_var)) : false;\n-      not_written = not_written_b ? bitmap_bit_p (not_written_b, \n-\t\t\t\t\t\t  DECL_UID (real_var)) : false;\n-      gcc_assert (!unmodifiable_var_p (var));\n-      \n-      clobber_stats.clobbered_vars++;\n-\n-      /* See if this variable is really clobbered by this function.  */\n-\n-      /* Trivial case: Things escaping only to pure/const are not\n-\t clobbered by non-pure-const, and only read by pure/const. */\n-      if ((escape_mask & ~(ESCAPE_TO_PURE_CONST)) == 0)\n-\t{\n-\t  tree call = get_call_expr_in (stmt);\n-\t  if (call_expr_flags (call) & (ECF_CONST | ECF_PURE))\n-\t    {\n-\t      add_stmt_operand (&var, s_ann, opf_none);\n-\t      clobber_stats.unescapable_clobbers_avoided++;\n-\t      continue;\n-\t    }\n-\t  else\n-\t    {\n-\t      clobber_stats.unescapable_clobbers_avoided++;\n-\t      continue;\n-\t    }\n-\t}\n-            \n-      if (not_written)\n-\t{\n-\t  clobber_stats.static_write_clobbers_avoided++;\n-\t  if (!not_read)\n-\t    add_stmt_operand (&var, s_ann, opf_none);\n-\t  else\n-\t    clobber_stats.static_read_clobbers_avoided++;\n-\t}\n-      else\n-\tadd_virtual_operand (var, s_ann, opf_is_def, \n-\t\t\t     NULL, 0, -1, true);\n-    }\n-  \n-}\n-\n-\n-/* Add VUSE operands for .GLOBAL_VAR or all call clobbered variables in the\n-   function.  */\n-\n-static void\n-add_call_read_ops (tree stmt, tree callee)\n-{\n-  unsigned u;\n-  bitmap_iterator bi;\n-  stmt_ann_t s_ann = stmt_ann (stmt);\n-  bitmap not_read_b;\n-\n-  /* if the function is not pure, it may reference memory.  Add\n-     a VUSE for .GLOBAL_VAR if it has been created.  See add_referenced_var\n-     for the heuristic used to decide whether to create .GLOBAL_VAR.  */\n-  if (global_var)\n-    {\n-      add_stmt_operand (&global_var, s_ann, opf_none);\n-      return;\n-    }\n-  \n-  not_read_b = callee ? ipa_reference_get_not_read_global (callee) : NULL; \n-\n-  /* Add a VUSE for each call-clobbered variable.  */\n-  EXECUTE_IF_SET_IN_BITMAP (call_clobbered_vars, 0, u, bi)\n-    {\n-      tree var = referenced_var (u);\n-      tree real_var = var;\n-      bool not_read;\n-      \n-      clobber_stats.readonly_clobbers++;\n-\n-      /* Not read and not written are computed on regular vars, not\n-\t subvars, so look at the parent var if this is an SFT. */\n-\n-      if (TREE_CODE (var) == STRUCT_FIELD_TAG)\n-\treal_var = SFT_PARENT_VAR (var);\n-\n-      not_read = not_read_b ? bitmap_bit_p (not_read_b, \n-\t\t\t\t\t    DECL_UID (real_var)) : false;\n-      \n-      if (not_read)\n-\t{\n-\t  clobber_stats.static_readonly_clobbers_avoided++;\n-\t  continue;\n-\t}\n-            \n-      add_stmt_operand (&var, s_ann, opf_none | opf_non_specific);\n-    }\n-}\n-\n-\n /* Scan the immediate_use list for VAR making sure its linked properly.\n    return RTUE iof there is a problem.  */\n "}]}