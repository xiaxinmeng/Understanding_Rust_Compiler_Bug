{"sha": "dbafa0f51f711a04d22353d50ad49a14b24151cb", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZGJhZmEwZjUxZjcxMWEwNGQyMjM1M2Q1MGFkNDlhMTRiMjQxNTFjYg==", "commit": {"author": {"name": "Paul A. Clarke", "email": "pc@us.ibm.com", "date": "2018-10-26T17:23:46Z"}, "committer": {"name": "Paul Clarke", "email": "pc@gcc.gnu.org", "date": "2018-10-26T17:23:46Z"}, "message": "[rs6000] x86 vector intrinsics compatibility: clean-ups for 32bit support \n\nImplement various corrections in the compatibility implementations of the\nx86 vector intrinsics found after enabling 32bit mode for the associated\ntest cases.  (Actual enablement coming in a subsequent patch.)\n\n2018-10-26  Paul A. Clarke  <pc@us.ibm.com>\n\ngcc/ChangeLog:\n\n\t* config/rs6000/mmintrin.h: Enable 32bit compilation.\n\t* config/rs6000/xmmintrin.h: Likewise.\n\nFrom-SVN: r265535", "tree": {"sha": "d9a56838675ad6da249e36d47f1aad47d3f86f80", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/d9a56838675ad6da249e36d47f1aad47d3f86f80"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/dbafa0f51f711a04d22353d50ad49a14b24151cb", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/dbafa0f51f711a04d22353d50ad49a14b24151cb", "html_url": "https://github.com/Rust-GCC/gccrs/commit/dbafa0f51f711a04d22353d50ad49a14b24151cb", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/dbafa0f51f711a04d22353d50ad49a14b24151cb/comments", "author": {"login": "ThinkOpenly", "id": 12301795, "node_id": "MDQ6VXNlcjEyMzAxNzk1", "avatar_url": "https://avatars.githubusercontent.com/u/12301795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ThinkOpenly", "html_url": "https://github.com/ThinkOpenly", "followers_url": "https://api.github.com/users/ThinkOpenly/followers", "following_url": "https://api.github.com/users/ThinkOpenly/following{/other_user}", "gists_url": "https://api.github.com/users/ThinkOpenly/gists{/gist_id}", "starred_url": "https://api.github.com/users/ThinkOpenly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ThinkOpenly/subscriptions", "organizations_url": "https://api.github.com/users/ThinkOpenly/orgs", "repos_url": "https://api.github.com/users/ThinkOpenly/repos", "events_url": "https://api.github.com/users/ThinkOpenly/events{/privacy}", "received_events_url": "https://api.github.com/users/ThinkOpenly/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "dbd93b9dbb64ba642ce82b658030fb0db4aad666", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/dbd93b9dbb64ba642ce82b658030fb0db4aad666", "html_url": "https://github.com/Rust-GCC/gccrs/commit/dbd93b9dbb64ba642ce82b658030fb0db4aad666"}], "stats": {"total": 122, "additions": 61, "deletions": 61}, "files": [{"sha": "72a69a83f6685d37c362f8ed66b0ed7948a0e34e", "filename": "gcc/ChangeLog", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/dbafa0f51f711a04d22353d50ad49a14b24151cb/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/dbafa0f51f711a04d22353d50ad49a14b24151cb/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=dbafa0f51f711a04d22353d50ad49a14b24151cb", "patch": "@@ -1,3 +1,8 @@\n+2018-10-25  Paul A. Clarke  <pc@us.ibm.com>\n+\n+\t* config/rs6000/mmintrin.h: Enable 32bit compilation.\n+\t* config/rs6000/xmmintrin.h: Likewise.\n+\n 2018-10-26  Paul A. Clarke  <pc@us.ibm.com>\n \n \t* config/rs6000/xmmintrin.h (_mm_extract_pi16): Fix for big-endian."}, {"sha": "2f6bccaa7a379b45481cd6fc9471340f7f2c87cc", "filename": "gcc/config/rs6000/mmintrin.h", "status": "modified", "additions": 35, "deletions": 37, "changes": 72, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/dbafa0f51f711a04d22353d50ad49a14b24151cb/gcc%2Fconfig%2Frs6000%2Fmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/dbafa0f51f711a04d22353d50ad49a14b24151cb/gcc%2Fconfig%2Frs6000%2Fmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Fmmintrin.h?ref=dbafa0f51f711a04d22353d50ad49a14b24151cb", "patch": "@@ -112,7 +112,6 @@ _m_to_int (__m64 __i)\n   return _mm_cvtsi64_si32 (__i);\n }\n \n-#ifdef __powerpc64__\n /* Convert I to a __m64 object.  */\n \n /* Intel intrinsic.  */\n@@ -173,9 +172,9 @@ _mm_packs_pi16 (__m64 __m1, __m64 __m2)\n   __vector signed short vm1;\n   __vector signed char vresult;\n \n-  vm1 = (__vector signed short)__builtin_pack_vector_int128 (__m2, __m1);\n+  vm1 = (__vector signed short) (__vector unsigned long long) { __m2, __m1 };\n   vresult = vec_vpkshss (vm1, vm1);\n-  return (__m64) __builtin_unpack_vector_int128 ((__vector __int128)vresult, 0);\n+  return (__m64) ((vector long long) vresult)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -193,9 +192,9 @@ _mm_packs_pi32 (__m64 __m1, __m64 __m2)\n   __vector signed int vm1;\n   __vector signed short vresult;\n \n-  vm1 = (__vector signed int)__builtin_pack_vector_int128 (__m2, __m1);\n+  vm1 = (__vector signed int) (__vector unsigned long long) { __m2, __m1 };\n   vresult = vec_vpkswss (vm1, vm1);\n-  return ((__m64) __builtin_unpack_vector_int128 ((__vector __int128)vresult, 0));\n+  return (__m64) ((vector long long) vresult)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -213,9 +212,9 @@ _mm_packs_pu16 (__m64 __m1, __m64 __m2)\n   __vector signed short vm1;\n   __vector unsigned char vresult;\n \n-  vm1 = (__vector signed short)__builtin_pack_vector_int128 (__m2, __m1);\n+  vm1 = (__vector signed short) (__vector unsigned long long) { __m2, __m1 };\n   vresult = vec_vpkshus (vm1, vm1);\n-  return ((__m64) __builtin_unpack_vector_int128 ((__vector __int128)vresult, 0));\n+  return (__m64) ((vector long long) vresult)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -236,7 +235,7 @@ _mm_unpackhi_pi8 (__m64 __m1, __m64 __m2)\n   a = (__vector unsigned char)vec_splats (__m1);\n   b = (__vector unsigned char)vec_splats (__m2);\n   c = vec_mergel (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -317,7 +316,7 @@ _mm_unpacklo_pi8 (__m64 __m1, __m64 __m2)\n   a = (__vector unsigned char)vec_splats (__m1);\n   b = (__vector unsigned char)vec_splats (__m2);\n   c = vec_mergel (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 1));\n+  return (__m64) ((vector long long) c)[1];\n #else\n   __m64_union m1, m2, res;\n \n@@ -398,7 +397,7 @@ _mm_add_pi8 (__m64 __m1, __m64 __m2)\n   a = (__vector signed char)vec_splats (__m1);\n   b = (__vector signed char)vec_splats (__m2);\n   c = vec_add (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -434,7 +433,7 @@ _mm_add_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = vec_add (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -466,7 +465,7 @@ _mm_add_pi32 (__m64 __m1, __m64 __m2)\n   a = (__vector signed int)vec_splats (__m1);\n   b = (__vector signed int)vec_splats (__m2);\n   c = vec_add (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -496,7 +495,7 @@ _mm_sub_pi8 (__m64 __m1, __m64 __m2)\n   a = (__vector signed char)vec_splats (__m1);\n   b = (__vector signed char)vec_splats (__m2);\n   c = vec_sub (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -532,7 +531,7 @@ _mm_sub_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = vec_sub (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -564,7 +563,7 @@ _mm_sub_pi32 (__m64 __m1, __m64 __m2)\n   a = (__vector signed int)vec_splats (__m1);\n   b = (__vector signed int)vec_splats (__m2);\n   c = vec_sub (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -754,7 +753,7 @@ _mm_cmpgt_pi8 (__m64 __m1, __m64 __m2)\n   a = (__vector signed char)vec_splats (__m1);\n   b = (__vector signed char)vec_splats (__m2);\n   c = (__vector signed char)vec_cmpgt (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -791,7 +790,7 @@ _mm_cmpeq_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = (__vector signed short)vec_cmpeq (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -822,7 +821,7 @@ _mm_cmpgt_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = (__vector signed short)vec_cmpgt (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -855,7 +854,7 @@ _mm_cmpeq_pi32 (__m64 __m1, __m64 __m2)\n   a = (__vector signed int)vec_splats (__m1);\n   b = (__vector signed int)vec_splats (__m2);\n   c = (__vector signed int)vec_cmpeq (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -884,7 +883,7 @@ _mm_cmpgt_pi32 (__m64 __m1, __m64 __m2)\n   a = (__vector signed int)vec_splats (__m1);\n   b = (__vector signed int)vec_splats (__m2);\n   c = (__vector signed int)vec_cmpgt (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -915,7 +914,7 @@ _mm_adds_pi8 (__m64 __m1, __m64 __m2)\n   a = (__vector signed char)vec_splats (__m1);\n   b = (__vector signed char)vec_splats (__m2);\n   c = vec_adds (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -933,7 +932,7 @@ _mm_adds_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = vec_adds (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -951,7 +950,7 @@ _mm_adds_pu8 (__m64 __m1, __m64 __m2)\n   a = (__vector unsigned char)vec_splats (__m1);\n   b = (__vector unsigned char)vec_splats (__m2);\n   c = vec_adds (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -970,7 +969,7 @@ _mm_adds_pu16 (__m64 __m1, __m64 __m2)\n   a = (__vector unsigned short)vec_splats (__m1);\n   b = (__vector unsigned short)vec_splats (__m2);\n   c = vec_adds (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -989,7 +988,7 @@ _mm_subs_pi8 (__m64 __m1, __m64 __m2)\n   a = (__vector signed char)vec_splats (__m1);\n   b = (__vector signed char)vec_splats (__m2);\n   c = vec_subs (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1008,7 +1007,7 @@ _mm_subs_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = vec_subs (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1027,7 +1026,7 @@ _mm_subs_pu8 (__m64 __m1, __m64 __m2)\n   a = (__vector unsigned char)vec_splats (__m1);\n   b = (__vector unsigned char)vec_splats (__m2);\n   c = vec_subs (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1046,7 +1045,7 @@ _mm_subs_pu16 (__m64 __m1, __m64 __m2)\n   a = (__vector unsigned short)vec_splats (__m1);\n   b = (__vector unsigned short)vec_splats (__m2);\n   c = vec_subs (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1068,7 +1067,7 @@ _mm_madd_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = vec_vmsumshm (a, b, zero);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1096,7 +1095,7 @@ _mm_mulhi_pi16 (__m64 __m1, __m64 __m2)\n   w1 = vec_vmulosh (a, b);\n   c = (__vector signed short)vec_perm (w0, w1, xform1);\n \n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1115,7 +1114,7 @@ _mm_mullo_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = a * b;\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1136,7 +1135,7 @@ _mm_sll_pi16 (__m64 __m, __m64 __count)\n       m = (__vector signed short)vec_splats (__m);\n       c = (__vector unsigned short)vec_splats ((unsigned short)__count);\n       r = vec_sl (m, (__vector unsigned short)c);\n-      return (__builtin_unpack_vector_int128 ((__vector __int128)r, 0));\n+      return (__m64) ((vector long long) r)[0];\n     }\n   else\n   return (0);\n@@ -1205,7 +1204,7 @@ _mm_sra_pi16 (__m64 __m, __m64 __count)\n \tm = (__vector signed short)vec_splats (__m);\n \tc = (__vector unsigned short)vec_splats ((unsigned short)__count);\n \tr = vec_sra (m, (__vector unsigned short)c);\n-\treturn (__builtin_unpack_vector_int128 ((__vector __int128)r, 0));\n+        return (__m64) ((vector long long) r)[0];\n     }\n   else\n   return (0);\n@@ -1274,7 +1273,7 @@ _mm_srl_pi16 (__m64 __m, __m64 __count)\n \tm = (__vector unsigned short)vec_splats (__m);\n \tc = (__vector unsigned short)vec_splats ((unsigned short)__count);\n \tr = vec_sr (m, (__vector unsigned short)c);\n-\treturn (__builtin_unpack_vector_int128 ((__vector __int128)r, 0));\n+        return (__m64) ((vector long long) r)[0];\n     }\n   else\n     return (0);\n@@ -1417,7 +1416,7 @@ _mm_set1_pi16 (short __w)\n   __vector signed short w;\n \n   w = (__vector signed short)vec_splats (__w);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)w, 0));\n+  return (__m64) ((vector long long) w)[0];\n #else\n   __m64_union res;\n \n@@ -1437,7 +1436,7 @@ _mm_set1_pi8 (signed char __b)\n   __vector signed char b;\n \n   b = (__vector signed char)vec_splats (__b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)b, 0));\n+  return (__m64) ((vector long long) b)[0];\n #else\n   __m64_union res;\n \n@@ -1452,5 +1451,4 @@ _mm_set1_pi8 (signed char __b)\n   return (res.as_m64);\n #endif\n }\n-#endif /* __powerpc64__ */\n #endif /* _MMINTRIN_H_INCLUDED */"}, {"sha": "e2854a45dfd63c1147dd271756bb36001cdf3de2", "filename": "gcc/config/rs6000/xmmintrin.h", "status": "modified", "additions": 21, "deletions": 24, "changes": 45, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/dbafa0f51f711a04d22353d50ad49a14b24151cb/gcc%2Fconfig%2Frs6000%2Fxmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/dbafa0f51f711a04d22353d50ad49a14b24151cb/gcc%2Fconfig%2Frs6000%2Fxmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Fxmmintrin.h?ref=dbafa0f51f711a04d22353d50ad49a14b24151cb", "patch": "@@ -996,7 +996,7 @@ _mm_cvtps_pi32 (__m128 __A)\n   rounded = vec_rint(temp);\n   result = (__vector unsigned long long) vec_cts (rounded, 0);\n \n-  return ((__m64) __builtin_unpack_vector_int128 ((__vector __int128)result, 0));\n+  return (__m64) ((vector long long) result)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1053,7 +1053,7 @@ _mm_cvttps_pi32 (__m128 __A)\n   temp = (__v4sf) vec_splat ((__vector long long)__A, 0);\n   result = (__vector unsigned long long) vec_cts (temp, 0);\n \n-  return ((__m64) __builtin_unpack_vector_int128 ((__vector __int128)result, 0));\n+  return (__m64) ((vector long long) result)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1104,7 +1104,7 @@ _mm_cvtpi32_ps (__m128        __A, __m64        __B)\n   __vector signed int vm1;\n   __vector float vf1;\n \n-  vm1 = (__vector signed int) __builtin_pack_vector_int128 (__B, __B);\n+  vm1 = (__vector signed int) (__vector unsigned long long) {__B, __B};\n   vf1 = (__vector float) vec_ctf (vm1, 0);\n \n   return ((__m128) (__vector unsigned long long)\n@@ -1126,7 +1126,7 @@ _mm_cvtpi16_ps (__m64 __A)\n   __vector signed int vi4;\n   __vector float vf1;\n \n-  vs8 = (__vector signed short) __builtin_pack_vector_int128 (__A, __A);\n+  vs8 = (__vector signed short) (__vector unsigned long long) { __A, __A };\n   vi4 = vec_vupklsh (vs8);\n   vf1 = (__vector float) vec_ctf (vi4, 0);\n \n@@ -1143,7 +1143,7 @@ _mm_cvtpu16_ps (__m64 __A)\n   __vector unsigned int vi4;\n   __vector float vf1;\n \n-  vs8 = (__vector unsigned short) __builtin_pack_vector_int128 (__A, __A);\n+  vs8 = (__vector unsigned short) (__vector unsigned long long) { __A, __A };\n   vi4 = (__vector unsigned int) vec_vmrglh (vs8, zero);\n   vf1 = (__vector float) vec_ctf (vi4, 0);\n \n@@ -1159,7 +1159,7 @@ _mm_cvtpi8_ps (__m64 __A)\n   __vector signed int vi4;\n   __vector float vf1;\n \n-  vc16 = (__vector signed char) __builtin_pack_vector_int128 (__A, __A);\n+  vc16 = (__vector signed char) (__vector unsigned long long) { __A, __A };\n   vs8 = vec_vupkhsb (vc16);\n   vi4 = vec_vupkhsh (vs8);\n   vf1 = (__vector float) vec_ctf (vi4, 0);\n@@ -1179,7 +1179,7 @@ _mm_cvtpu8_ps (__m64  __A)\n   __vector unsigned int vi4;\n   __vector float vf1;\n \n-  vc16 = (__vector unsigned char) __builtin_pack_vector_int128 (__A, __A);\n+  vc16 = (__vector unsigned char) (__vector unsigned long long) { __A, __A };\n   vs8 = (__vector unsigned short) vec_vmrglb (vc16, zero);\n   vi4 = (__vector unsigned int) vec_vmrghh (vs8,\n \t\t\t\t\t    (__vector unsigned short) zero);\n@@ -1195,7 +1195,7 @@ _mm_cvtpi32x2_ps(__m64 __A, __m64 __B)\n   __vector signed int vi4;\n   __vector float vf4;\n \n-  vi4 = (__vector signed int) __builtin_pack_vector_int128 (__B, __A);\n+  vi4 = (__vector signed int) (__vector unsigned long long) { __B, __A };\n   vf4 = (__vector float) vec_ctf (vi4, 0);\n   return (__m128) vf4;\n }\n@@ -1212,7 +1212,7 @@ _mm_cvtps_pi16(__m128 __A)\n   temp = vec_cts (rounded, 0);\n   result = (__vector unsigned long long) vec_pack (temp, temp);\n \n-  return ((__m64) __builtin_unpack_vector_int128 ((__vector __int128)result, 0));\n+  return (__m64) ((vector long long) result)[0];\n }\n \n /* Convert the four SPFP values in A to four signed 8-bit integers.  */\n@@ -1224,15 +1224,12 @@ _mm_cvtps_pi8(__m128 __A)\n   static const __vector signed int zero = {0, 0, 0, 0};\n   __vector signed short tmp_s;\n   __vector signed char res_v;\n-  __m64 result;\n \n   rounded = vec_rint(__A);\n   tmp_i = vec_cts (rounded, 0);\n   tmp_s = vec_pack (tmp_i, zero);\n   res_v = vec_pack (tmp_s, tmp_s);\n-  result = (__m64) __builtin_unpack_vector_int128 ((__vector __int128)res_v, 0);\n-\n-  return (result);\n+  return (__m64) ((vector long long) res_v)[0];\n }\n \n /* Selects four specific SPFP values from A and B based on MASK.  */\n@@ -1432,7 +1429,7 @@ _mm_max_pi16 (__m64 __A, __m64 __B)\n   b = (__vector signed short)vec_splats (__B);\n   c = (__vector __bool short)vec_cmpgt (a, b);\n   r = vec_sel (b, a, c);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)r, 0));\n+  return (__m64) ((vector long long) r)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -1470,7 +1467,7 @@ _mm_max_pu8 (__m64 __A, __m64 __B)\n   b = (__vector unsigned char)vec_splats (__B);\n   c = (__vector __bool char)vec_cmpgt (a, b);\n   r = vec_sel (b, a, c);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)r, 0));\n+  return (__m64) ((vector long long) r)[0];\n #else\n   __m64_union m1, m2, res;\n   long i;\n@@ -1506,7 +1503,7 @@ _mm_min_pi16 (__m64 __A, __m64 __B)\n   b = (__vector signed short)vec_splats (__B);\n   c = (__vector __bool short)vec_cmplt (a, b);\n   r = vec_sel (b, a, c);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)r, 0));\n+  return (__m64) ((vector long long) r)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -1544,7 +1541,7 @@ _mm_min_pu8 (__m64 __A, __m64 __B)\n   b = (__vector unsigned char)vec_splats (__B);\n   c = (__vector __bool char)vec_cmplt (a, b);\n   r = vec_sel (b, a, c);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)r, 0));\n+  return (__m64) ((vector long long) r)[0];\n #else\n   __m64_union m1, m2, res;\n   long i;\n@@ -1572,7 +1569,7 @@ _m_pminub (__m64 __A, __m64 __B)\n extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_movemask_pi8 (__m64 __A)\n {\n-  unsigned long p = 0x0008101820283038UL; // permute control for sign bits\n+  unsigned long long p = 0x0008101820283038UL; // permute control for sign bits\n \n   return __builtin_bpermd (p, __A);\n }\n@@ -1603,7 +1600,7 @@ _mm_mulhi_pu16 (__m64 __A, __m64 __B)\n   w1 = vec_vmulouh (a, b);\n   c = (__vector unsigned short)vec_perm (w0, w1, xform1);\n \n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1646,7 +1643,7 @@ _mm_shuffle_pi16 (__m64 __A, int const __N)\n   p = vec_splats (t.as_m64);\n   a = vec_splats (__A);\n   r = vec_perm (a, a, (__vector unsigned char)p);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)r, 0));\n+  return (__m64) ((vector long long) r)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1686,7 +1683,7 @@ _mm_avg_pu8 (__m64 __A, __m64 __B)\n   a = (__vector unsigned char)vec_splats (__A);\n   b = (__vector unsigned char)vec_splats (__B);\n   c = vec_avg (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1704,7 +1701,7 @@ _mm_avg_pu16 (__m64 __A, __m64 __B)\n   a = (__vector unsigned short)vec_splats (__A);\n   b = (__vector unsigned short)vec_splats (__B);\n   c = vec_avg (a, b);\n-  return (__builtin_unpack_vector_int128 ((__vector __int128)c, 0));\n+  return (__m64) ((vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1726,8 +1723,8 @@ _mm_sad_pu8 (__m64  __A, __m64  __B)\n     { 0, 0, 0, 0 };\n   unsigned short result;\n \n-  a = (__vector unsigned char) __builtin_pack_vector_int128 (0UL, __A);\n-  b = (__vector unsigned char) __builtin_pack_vector_int128 (0UL, __B);\n+  a = (__vector unsigned char) (__vector unsigned long long) { 0UL, __A };\n+  b = (__vector unsigned char) (__vector unsigned long long) { 0UL, __B };\n   vmin = vec_min (a, b);\n   vmax = vec_max (a, b);\n   vabsdiff = vec_sub (vmax, vmin);"}]}