{"sha": "b6f611637e7411536dc883100c543547f4b7c03a", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YjZmNjExNjM3ZTc0MTE1MzZkYzg4MzEwMGM1NDM1NDdmNGI3YzAzYQ==", "commit": {"author": {"name": "Daniel Berlin", "email": "dberlin@dberlin.org", "date": "2003-10-27T00:26:52Z"}, "committer": {"name": "Daniel Berlin", "email": "dberlin@gcc.gnu.org", "date": "2003-10-27T00:26:52Z"}, "message": "ggc-zone.c: New file, zone allocating collector.\n\n2003-10-26  Daniel Berlin  <dberlin@dberlin.org>\n\n\t* ggc-zone.c:  New file, zone allocating collector.\n\t* configure: Accept zone option for --with-gc\n\t* configure.in: Ditto.\n\t* ggc.h (ggc_pch_count_object): Pass bool indicating\n\tstringiness. Update all callers.\n\t(ggc_pch_alloc_object): Ditto.\n\t(ggc_pch_write_object): Ditto.\n\t(ggc_alloc_rtx): Use typed allocation, since all RTX's are of a single\n\ttype.\n\t(ggc_alloc_rtvec): Ditto.\n\t(ggc_alloc_tree): Use zone allocation, since some things using this macro\n\taren't a single typecode.\n\t* ggc-none.c (ggc_alloc_typed): New function.\n\t(ggc_alloc_zone): Ditto.\n\t* ggc-page.c: Ditto on both functions.\n\nFrom-SVN: r72971", "tree": {"sha": "4348e45b7dfeeb6d8cce2f6f38df5cb802f97b60", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/4348e45b7dfeeb6d8cce2f6f38df5cb802f97b60"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/b6f611637e7411536dc883100c543547f4b7c03a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b6f611637e7411536dc883100c543547f4b7c03a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b6f611637e7411536dc883100c543547f4b7c03a", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b6f611637e7411536dc883100c543547f4b7c03a/comments", "author": {"login": "dberlin", "id": 324715, "node_id": "MDQ6VXNlcjMyNDcxNQ==", "avatar_url": "https://avatars.githubusercontent.com/u/324715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dberlin", "html_url": "https://github.com/dberlin", "followers_url": "https://api.github.com/users/dberlin/followers", "following_url": "https://api.github.com/users/dberlin/following{/other_user}", "gists_url": "https://api.github.com/users/dberlin/gists{/gist_id}", "starred_url": "https://api.github.com/users/dberlin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dberlin/subscriptions", "organizations_url": "https://api.github.com/users/dberlin/orgs", "repos_url": "https://api.github.com/users/dberlin/repos", "events_url": "https://api.github.com/users/dberlin/events{/privacy}", "received_events_url": "https://api.github.com/users/dberlin/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "60b6a81550ece89cc0d49dc7741f609c0886d191", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/60b6a81550ece89cc0d49dc7741f609c0886d191", "html_url": "https://github.com/Rust-GCC/gccrs/commit/60b6a81550ece89cc0d49dc7741f609c0886d191"}], "stats": {"total": 1816, "additions": 1795, "deletions": 21}, "files": [{"sha": "e62f2451f4350ff404ecf1803dc87f2e009ce55c", "filename": "gcc/ChangeLog", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=b6f611637e7411536dc883100c543547f4b7c03a", "patch": "@@ -1,3 +1,21 @@\n+2003-10-26  Daniel Berlin  <dberlin@dberlin.org>\n+\n+\t* ggc-zone.c:  New file, zone allocating collector.\n+\t* configure: Accept zone option for --with-gc\n+\t* configure.in: Ditto.\n+\t* ggc.h (ggc_pch_count_object): Pass bool indicating\n+\tstringiness. Update all callers.\n+\t(ggc_pch_alloc_object): Ditto.\n+\t(ggc_pch_write_object): Ditto.\n+\t(ggc_alloc_rtx): Use typed allocation, since all RTX's are of a single\n+\ttype.\n+\t(ggc_alloc_rtvec): Ditto.\n+\t(ggc_alloc_tree): Use zone allocation, since some things using this macro\n+\taren't a single typecode.\n+\t* ggc-none.c (ggc_alloc_typed): New function.\n+\t(ggc_alloc_zone): Ditto.\n+\t* ggc-page.c: Ditto on both functions.\n+\n 2003-10-26  Gunther Nikl  <gni@gecko.de>\n \n \t* config/m68k/m68k.c (m68k_compute_frame_layout): Ensure FPU related"}, {"sha": "81f819c256ff7715aef5dd472ca1352f1366a901", "filename": "gcc/Makefile.in", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=b6f611637e7411536dc883100c543547f4b7c03a", "patch": "@@ -850,7 +850,7 @@ OBJS-common = \\\n  insn-extract.o insn-opinit.o insn-output.o insn-peep.o insn-recog.o\t   \\\n  integrate.o intl.o jump.o  langhooks.o lcm.o lists.o local-alloc.o  \t   \\\n  loop.o optabs.o options.o opts.o params.o postreload.o predict.o\t   \\\n- print-rtl.o print-tree.o value-prof.o\t\t\t\t\t   \\\n+ print-rtl.o print-tree.o value-prof.o \t\t\t\t\t\t\t\t   \\\n  profile.o ra.o ra-build.o ra-colorize.o ra-debug.o ra-rewrite.o\t   \\\n  real.o recog.o reg-stack.o regclass.o regmove.o regrename.o\t\t   \\\n  reload.o reload1.o reorg.o resource.o rtl.o rtlanal.o rtl-error.o\t   \\\n@@ -1462,6 +1462,9 @@ ggc-simple.o: ggc-simple.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H)\n ggc-page.o: ggc-page.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) $(TREE_H) \\\n \tflags.h toplev.h $(GGC_H) $(TIMEVAR_H) $(TM_P_H) $(PARAMS_H)\n \n+ggc-zone.o: ggc-zone.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) $(TREE_H) \\\n+\tflags.h toplev.h $(GGC_H) $(TIMEVAR_H) $(TM_P_H) $(PARAMS_H)\n+\n stringpool.o: stringpool.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n \t$(TREE_H) $(GGC_H) gt-stringpool.h\n "}, {"sha": "e539c125d94e3242854383f1c85b23c6f75e2f1d", "filename": "gcc/configure", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2Fconfigure", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2Fconfigure", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfigure?ref=b6f611637e7411536dc883100c543547f4b7c03a", "patch": "@@ -83,7 +83,7 @@ ac_help=\"$ac_help\n                           use KEY instead of GCC version as the last portion\n                           of the registry key\"\n ac_help=\"$ac_help\n-  --with-gc={simple,page} choose the garbage collection mechanism to use\n+  --with-gc={simple,page,zone} choose the garbage collection mechanism to use\n                           with the compiler\"\n ac_help=\"$ac_help\n   --with-system-zlib      use installed libz\"\n@@ -7609,7 +7609,7 @@ fi\n if test \"${with_gc+set}\" = set; then\n   withval=\"$with_gc\"\n   case \"$withval\" in\n-  simple | page)\n+  simple | page | zone)\n     GGC=ggc-$withval\n     ;;\n   *)"}, {"sha": "f424f03b6fcffdad75c01070ed03d3daf5914ca1", "filename": "gcc/configure.in", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2Fconfigure.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2Fconfigure.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfigure.in?ref=b6f611637e7411536dc883100c543547f4b7c03a", "patch": "@@ -2695,10 +2695,10 @@ AC_SUBST(gthread_flags)\n \n # Find out what GC implementation we want, or may, use.\n AC_ARG_WITH(gc,\n-[  --with-gc={simple,page} choose the garbage collection mechanism to use\n+[  --with-gc={simple,page,zone} choose the garbage collection mechanism to use\n                           with the compiler],\n [case \"$withval\" in\n-  simple | page)\n+  simple | page | zone)\n     GGC=ggc-$withval\n     ;;\n   *)"}, {"sha": "eab766e0378c1cb27c3bb7615342c671c7dd5886", "filename": "gcc/ggc-common.c", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2Fggc-common.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2Fggc-common.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fggc-common.c?ref=b6f611637e7411536dc883100c543547f4b7c03a", "patch": "@@ -328,7 +328,7 @@ call_count (void **slot, void *state_p)\n   struct ptr_data *d = (struct ptr_data *)*slot;\n   struct traversal_state *state = (struct traversal_state *)state_p;\n \n-  ggc_pch_count_object (state->d, d->obj, d->size);\n+  ggc_pch_count_object (state->d, d->obj, d->size, d->note_ptr_fn == gt_pch_p_S);\n   state->count++;\n   return 1;\n }\n@@ -339,7 +339,7 @@ call_alloc (void **slot, void *state_p)\n   struct ptr_data *d = (struct ptr_data *)*slot;\n   struct traversal_state *state = (struct traversal_state *)state_p;\n \n-  d->new_addr = ggc_pch_alloc_object (state->d, d->obj, d->size);\n+  d->new_addr = ggc_pch_alloc_object (state->d, d->obj, d->size, d->note_ptr_fn == gt_pch_p_S);\n   state->ptrs[state->ptrs_i++] = d;\n   return 1;\n }\n@@ -524,7 +524,7 @@ gt_pch_save (FILE *f)\n \t\t\t\t  state.ptrs[i]->note_ptr_cookie,\n \t\t\t\t  relocate_ptrs, &state);\n       ggc_pch_write_object (state.d, state.f, state.ptrs[i]->obj,\n-\t\t\t    state.ptrs[i]->new_addr, state.ptrs[i]->size);\n+\t\t\t    state.ptrs[i]->new_addr, state.ptrs[i]->size, state.ptrs[i]->note_ptr_fn == gt_pch_p_S);\n       if (state.ptrs[i]->note_ptr_fn != gt_pch_p_S)\n \tmemcpy (state.ptrs[i]->obj, this_object, state.ptrs[i]->size);\n     }"}, {"sha": "659bf931d09102b0899824245a94dff6f5dfba28", "filename": "gcc/ggc-none.c", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2Fggc-none.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2Fggc-none.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fggc-none.c?ref=b6f611637e7411536dc883100c543547f4b7c03a", "patch": "@@ -28,13 +28,27 @@\n #include \"coretypes.h\"\n #include \"tm.h\"\n #include \"ggc.h\"\n+struct alloc_zone *rtl_zone = NULL;\n+struct alloc_zone *garbage_zone = NULL;\n+\n+void *\n+ggc_alloc_typed (enum gt_types_enum gte ATTRIBUTE_UNUSED, size_t size)\n+{\n+  return xmalloc (size);\n+}\n \n void *\n ggc_alloc (size_t size)\n {\n   return xmalloc (size);\n }\n \n+void *\n+ggc_alloc_zone (size_t size, struct alloc_zone *zone ATTRIBUTE_UNUSED)\n+{\n+  return xmalloc (size);\n+}\n+\n void *\n ggc_alloc_cleared (size_t size)\n {"}, {"sha": "198b73364bf6db0956a8bce0442b0866a470a57d", "filename": "gcc/ggc-page.c", "status": "modified", "additions": 23, "deletions": 4, "changes": 27, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2Fggc-page.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2Fggc-page.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fggc-page.c?ref=b6f611637e7411536dc883100c543547f4b7c03a", "patch": "@@ -469,7 +469,10 @@ static void poison_pages (void);\n void debug_print_page_list (int);\n static void push_depth (unsigned int);\n static void push_by_depth (page_entry *, unsigned long *);\n-\f\n+struct alloc_zone *rtl_zone = NULL;\n+struct alloc_zone *tree_zone = NULL;\n+struct alloc_zone *garbage_zone = NULL;\n+\n /* Push an entry onto G.depth.  */\n \n inline static void\n@@ -1021,6 +1024,22 @@ static unsigned char size_lookup[257] =\n   8\n };\n \n+/* Typed allocation function.  Does nothing special in this collector.  */\n+\n+void *\n+ggc_alloc_typed (enum gt_types_enum type ATTRIBUTE_UNUSED, size_t size)\n+{\n+  return ggc_alloc (size);\n+}\n+\n+/* Zone allocation function.  Does nothing special in this collector.  */\n+\n+void *\n+ggc_alloc_zone (size_t size, struct alloc_zone *zone ATTRIBUTE_UNUSED)\n+{\n+  return ggc_alloc (size);\n+}\n+\n /* Allocate a chunk of memory of SIZE bytes.  Its contents are undefined.  */\n \n void *\n@@ -1894,7 +1913,7 @@ init_ggc_pch (void)\n \n void\n ggc_pch_count_object (struct ggc_pch_data *d, void *x ATTRIBUTE_UNUSED,\n-\t\t      size_t size)\n+\t\t      size_t size, bool is_string ATTRIBUTE_UNUSED)\n {\n   unsigned order;\n \n@@ -1937,7 +1956,7 @@ ggc_pch_this_base (struct ggc_pch_data *d, void *base)\n \n char *\n ggc_pch_alloc_object (struct ggc_pch_data *d, void *x ATTRIBUTE_UNUSED,\n-\t\t      size_t size)\n+\t\t      size_t size, bool is_string ATTRIBUTE_UNUSED)\n {\n   unsigned order;\n   char *result;\n@@ -1966,7 +1985,7 @@ ggc_pch_prepare_write (struct ggc_pch_data *d ATTRIBUTE_UNUSED,\n void\n ggc_pch_write_object (struct ggc_pch_data *d ATTRIBUTE_UNUSED,\n \t\t      FILE *f, void *x, void *newx ATTRIBUTE_UNUSED,\n-\t\t      size_t size)\n+\t\t      size_t size, bool is_string ATTRIBUTE_UNUSED)\n {\n   unsigned order;\n   static const char emptyBytes[256];"}, {"sha": "897389e0286313b14a98092830af819b94283baa", "filename": "gcc/ggc-zone.c", "status": "added", "additions": 1701, "deletions": 0, "changes": 1701, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2Fggc-zone.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2Fggc-zone.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fggc-zone.c?ref=b6f611637e7411536dc883100c543547f4b7c03a", "patch": "@@ -0,0 +1,1701 @@\n+/* \"Bag-of-pages\" zone garbage collector for the GNU compiler.\n+   Copyright (C) 1999, 2000, 2001, 2002, 2003 Free Software Foundation, Inc.\n+   Contributed by Richard Henderson (rth@redhat.com) and Daniel Berlin (dberlin@dberlin.org)\n+\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 2, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING.  If not, write to the Free\n+Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n+02111-1307, USA.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"tree.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"toplev.h\"\n+#include \"varray.h\"\n+#include \"flags.h\"\n+#include \"ggc.h\"\n+#include \"timevar.h\"\n+#include \"params.h\"\n+#include \"bitmap.h\"\n+\n+#ifdef ENABLE_VALGRIND_CHECKING\n+#include <valgrind/memcheck.h>\n+#else\n+/* Avoid #ifdef:s when we can help it.  */\n+#define VALGRIND_DISCARD(x)\n+#define VALGRIND_MALLOCLIKE_BLOCK(w,x,y,z)\n+#define VALGRIND_FREELIKE_BLOCK(x,y)\n+#endif\n+/* Prefer MAP_ANON(YMOUS) to /dev/zero, since we don't need to keep a\n+   file open.  Prefer either to valloc.  */\n+#ifdef HAVE_MMAP_ANON\n+# undef HAVE_MMAP_DEV_ZERO\n+\n+# include <sys/mman.h>\n+# ifndef MAP_FAILED\n+#  define MAP_FAILED -1\n+# endif\n+# if !defined (MAP_ANONYMOUS) && defined (MAP_ANON)\n+#  define MAP_ANONYMOUS MAP_ANON\n+# endif\n+# define USING_MMAP\n+\n+#endif\n+\n+#ifdef HAVE_MMAP_DEV_ZERO\n+\n+# include <sys/mman.h>\n+# ifndef MAP_FAILED\n+#  define MAP_FAILED -1\n+# endif\n+# define USING_MMAP\n+\n+#endif\n+\n+#ifndef USING_MMAP\n+#define USING_MALLOC_PAGE_GROUPS\n+#endif\n+\n+#if (GCC_VERSION < 3001)\n+#define prefetch(X) ((void) X)\n+#else\n+#define prefetch(X) __builtin_prefetch (X)\n+#endif\n+\n+/* NOTES:\n+   If we track inter-zone pointers, we can mark single zones at a\n+   time.\n+   If we have a zone where we guarantee no inter-zone pointers, we\n+   could mark that zone seperately.\n+   The garbage zone should not be marked, and we should return 1 in\n+   ggc_set_mark for any object in the garbage zone, which cuts off\n+   marking quickly.  */\n+/* Stategy:\n+\n+   This garbage-collecting allocator segregates objects into zones.\n+   It also segregates objects into \"large\" and \"small\" bins.  Large\n+   objects are greater or equal to page size.\n+\n+   Pages for small objects are broken up into chunks, each of which\n+   are described by a struct alloc_chunk.  One can walk over all\n+   chunks on the page by adding the chunk size to the chunk's data\n+   address.  The free space for a page exists in the free chunk bins.\n+\n+   Each page-entry also has a context depth, which is used to track\n+   pushing and popping of allocation contexts.  Only objects allocated\n+   in the current (highest-numbered) context may be collected.\n+\n+   Empty pages (of all sizes) are kept on a single page cache list,\n+   and are considered first when new pages are required; they are\n+   deallocated at the start of the next collection if they haven't\n+   been recycled by then.  */\n+\n+/* Define GGC_DEBUG_LEVEL to print debugging information.\n+     0: No debugging output.\n+     1: GC statistics only.\n+     2: Page-entry allocations/deallocations as well.\n+     3: Object allocations as well.\n+     4: Object marks as well.  */\n+#define GGC_DEBUG_LEVEL (0)\n+\n+#ifndef HOST_BITS_PER_PTR\n+#define HOST_BITS_PER_PTR  HOST_BITS_PER_LONG\n+#endif\n+#ifdef COOKIE_CHECKING\n+#define CHUNK_MAGIC 0x95321123\n+#define DEADCHUNK_MAGIC 0x12817317\n+#endif\n+\n+/* This structure manages small chunks.  When the chunk is free, it's\n+   linked with other chunks via free_next.  When the chunk is allocated,\n+   the data starts at u.  Large chunks are allocated one at a time to\n+   their own page, and so don't come in here.\n+\n+   The \"type\" field is a placeholder for a future change to do\n+   generational collection.  At present it is 0 when free and\n+   and 1 when allocated.  */\n+\n+struct alloc_chunk {\n+#ifdef COOKIE_CHECKING\n+  unsigned int magic;\n+#endif\n+  unsigned int type:1;\n+  unsigned int typecode:15;\n+  unsigned int size:15;\n+  unsigned int mark:1;\n+  union {\n+    struct alloc_chunk *next_free;\n+    char data[1];\n+\n+    /* Make sure the data is sufficiently aligned.  */\n+    HOST_WIDEST_INT align_i;\n+#ifdef HAVE_LONG_DOUBLE\n+    long double align_d;\n+#else\n+    double align_d;\n+#endif\n+  } u;\n+} __attribute__ ((packed));\n+\n+#define CHUNK_OVERHEAD\t(offsetof (struct alloc_chunk, u))\n+\n+/* We maintain several bins of free lists for chunks for very small\n+   objects.  We never exhaustively search other bins -- if we don't\n+   find one of the proper size, we allocate from the \"larger\" bin.  */\n+\n+/* Decreasing the number of free bins increases the time it takes to allocate.\n+   Similar with increasing max_free_bin_size without increasing num_free_bins.\n+\n+   After much histogramming of allocation sizes and time spent on gc,\n+   on a powerpc G4 7450 - 667 mhz, and an pentium 4 - 2.8ghz,\n+   these were determined to be the optimal values.  */\n+#define NUM_FREE_BINS\t\t64\n+#define MAX_FREE_BIN_SIZE\t256\n+#define FREE_BIN_DELTA\t\t(MAX_FREE_BIN_SIZE / NUM_FREE_BINS)\n+#define SIZE_BIN_UP(SIZE)\t(((SIZE) + FREE_BIN_DELTA - 1) / FREE_BIN_DELTA)\n+#define SIZE_BIN_DOWN(SIZE)\t((SIZE) / FREE_BIN_DELTA)\n+\n+/* Marker used as chunk->size for a large object.  Should correspond\n+   to the size of the bitfield above.  */\n+#define LARGE_OBJECT_SIZE\t0x7fff\n+\n+/* We use this structure to determine the alignment required for\n+   allocations.  For power-of-two sized allocations, that's not a\n+   problem, but it does matter for odd-sized allocations.  */\n+\n+struct max_alignment {\n+  char c;\n+  union {\n+    HOST_WIDEST_INT i;\n+#ifdef HAVE_LONG_DOUBLE\n+    long double d;\n+#else\n+    double d;\n+#endif\n+  } u;\n+};\n+\n+/* The biggest alignment required.  */\n+\n+#define MAX_ALIGNMENT (offsetof (struct max_alignment, u))\n+\n+/* Compute the smallest nonnegative number which when added to X gives\n+   a multiple of F.  */\n+\n+#define ROUND_UP_VALUE(x, f) ((f) - 1 - ((f) - 1 + (x)) % (f))\n+\n+/* Compute the smallest multiple of F that is >= X.  */\n+\n+#define ROUND_UP(x, f) (CEIL (x, f) * (f))\n+\n+/* A two-level tree is used to look up the page-entry for a given\n+   pointer.  Two chunks of the pointer's bits are extracted to index\n+   the first and second levels of the tree, as follows:\n+\n+\t\t\t\t   HOST_PAGE_SIZE_BITS\n+\t\t\t   32\t\t|      |\n+       msb +----------------+----+------+------+ lsb\n+\t\t\t    |    |      |\n+\t\t\t PAGE_L1_BITS   |\n+\t\t\t\t |      |\n+\t\t\t       PAGE_L2_BITS\n+\n+   The bottommost HOST_PAGE_SIZE_BITS are ignored, since page-entry\n+   pages are aligned on system page boundaries.  The next most\n+   significant PAGE_L2_BITS and PAGE_L1_BITS are the second and first\n+   index values in the lookup table, respectively.\n+\n+   For 32-bit architectures and the settings below, there are no\n+   leftover bits.  For architectures with wider pointers, the lookup\n+   tree points to a list of pages, which must be scanned to find the\n+   correct one.  */\n+\n+#define PAGE_L1_BITS\t(8)\n+#define PAGE_L2_BITS\t(32 - PAGE_L1_BITS - G.lg_pagesize)\n+#define PAGE_L1_SIZE\t((size_t) 1 << PAGE_L1_BITS)\n+#define PAGE_L2_SIZE\t((size_t) 1 << PAGE_L2_BITS)\n+\n+#define LOOKUP_L1(p) \\\n+  (((size_t) (p) >> (32 - PAGE_L1_BITS)) & ((1 << PAGE_L1_BITS) - 1))\n+\n+#define LOOKUP_L2(p) \\\n+  (((size_t) (p) >> G.lg_pagesize) & ((1 << PAGE_L2_BITS) - 1))\n+\n+struct alloc_zone;\n+/* A page_entry records the status of an allocation page.  */\n+typedef struct page_entry\n+{\n+  /* The next page-entry with objects of the same size, or NULL if\n+     this is the last page-entry.  */\n+  struct page_entry *next;\n+\n+  /* The number of bytes allocated.  (This will always be a multiple\n+     of the host system page size.)  */\n+  size_t bytes;\n+\n+  /* How many collections we've survived.  */\n+  size_t survived;\n+\n+  /* The address at which the memory is allocated.  */\n+  char *page;\n+\n+#ifdef USING_MALLOC_PAGE_GROUPS\n+  /* Back pointer to the page group this page came from.  */\n+  struct page_group *group;\n+#endif\n+\n+  /* Number of bytes on the page unallocated.  Only used during\n+     collection, and even then large pages merely set this non-zero.  */\n+  size_t bytes_free;\n+\n+  /* Context depth of this page.  */\n+  unsigned short context_depth;\n+\n+  /* Does this page contain small objects, or one large object?  */\n+  bool large_p;\n+\n+  struct alloc_zone *zone;\n+} page_entry;\n+\n+#ifdef USING_MALLOC_PAGE_GROUPS\n+/* A page_group describes a large allocation from malloc, from which\n+   we parcel out aligned pages.  */\n+typedef struct page_group\n+{\n+  /* A linked list of all extant page groups.  */\n+  struct page_group *next;\n+\n+  /* The address we received from malloc.  */\n+  char *allocation;\n+\n+  /* The size of the block.  */\n+  size_t alloc_size;\n+\n+  /* A bitmask of pages in use.  */\n+  unsigned int in_use;\n+} page_group;\n+#endif\n+\n+#if HOST_BITS_PER_PTR <= 32\n+\n+/* On 32-bit hosts, we use a two level page table, as pictured above.  */\n+typedef page_entry **page_table[PAGE_L1_SIZE];\n+\n+#else\n+\n+/* On 64-bit hosts, we use the same two level page tables plus a linked\n+   list that disambiguates the top 32-bits.  There will almost always be\n+   exactly one entry in the list.  */\n+typedef struct page_table_chain\n+{\n+  struct page_table_chain *next;\n+  size_t high_bits;\n+  page_entry **table[PAGE_L1_SIZE];\n+} *page_table;\n+\n+#endif\n+\n+/* The global variables.  */\n+static struct globals\n+{\n+  /* The page lookup table.  A single page can only belong to one\n+     zone.  This means free pages are zone-specific ATM.  */\n+  page_table lookup;\n+  /* The linked list of zones.  */\n+  struct alloc_zone *zones;\n+\n+  /* The system's page size.  */\n+  size_t pagesize;\n+  size_t lg_pagesize;\n+\n+  /* A file descriptor open to /dev/zero for reading.  */\n+#if defined (HAVE_MMAP_DEV_ZERO)\n+  int dev_zero_fd;\n+#endif\n+\n+  /* The file descriptor for debugging output.  */\n+  FILE *debug_file;\n+} G;\n+\n+/*  The zone allocation structure.  */\n+struct alloc_zone\n+{\n+  /* Name of the zone.  */\n+  const char *name;\n+\n+  /* Linked list of pages in a zone.  */\n+  page_entry *pages;\n+\n+  /* Linked lists of free storage.  Slots 1 ... NUM_FREE_BINS have chunks of size\n+     FREE_BIN_DELTA.  All other chunks are in slot 0.  */\n+  struct alloc_chunk *free_chunks[NUM_FREE_BINS + 1];\n+\n+  /* Bytes currently allocated.  */\n+  size_t allocated;\n+\n+  /* Bytes currently allocated at the end of the last collection.  */\n+  size_t allocated_last_gc;\n+\n+  /* Total amount of memory mapped.  */\n+  size_t bytes_mapped;\n+\n+  /* Bit N set if any allocations have been done at context depth N.  */\n+  unsigned long context_depth_allocations;\n+\n+  /* Bit N set if any collections have been done at context depth N.  */\n+  unsigned long context_depth_collections;\n+\n+  /* The current depth in the context stack.  */\n+  unsigned short context_depth;\n+\n+  /* A cache of free system pages.  */\n+  page_entry *free_pages;\n+\n+#ifdef USING_MALLOC_PAGE_GROUPS\n+  page_group *page_groups;\n+#endif\n+\n+  /* Next zone in the linked list of zones.  */\n+  struct alloc_zone *next_zone;\n+\n+  /* Return true if this zone was collected during this collection.  */\n+  bool was_collected;\n+} main_zone;\n+\n+struct alloc_zone *rtl_zone;\n+struct alloc_zone *garbage_zone;\n+struct alloc_zone *tree_zone;\n+\n+/* Allocate pages in chunks of this size, to throttle calls to memory\n+   allocation routines.  The first page is used, the rest go onto the\n+   free list.  This cannot be larger than HOST_BITS_PER_INT for the\n+   in_use bitmask for page_group.  */\n+#define GGC_QUIRE_SIZE 16\n+\n+static int ggc_allocated_p (const void *);\n+static page_entry *lookup_page_table_entry (const void *);\n+static void set_page_table_entry (void *, page_entry *);\n+#ifdef USING_MMAP\n+static char *alloc_anon (char *, size_t, struct alloc_zone *);\n+#endif\n+#ifdef USING_MALLOC_PAGE_GROUPS\n+static size_t page_group_index (char *, char *);\n+static void set_page_group_in_use (page_group *, char *);\n+static void clear_page_group_in_use (page_group *, char *);\n+#endif\n+static struct page_entry * alloc_small_page ( struct alloc_zone *);\n+static struct page_entry * alloc_large_page (size_t, struct alloc_zone *);\n+static void free_chunk (struct alloc_chunk *, size_t, struct alloc_zone *);\n+static void free_page (struct page_entry *);\n+static void release_pages (struct alloc_zone *);\n+static void sweep_pages (struct alloc_zone *);\n+static void * ggc_alloc_zone_1 (size_t, struct alloc_zone *, short);\n+static bool ggc_collect_1 (struct alloc_zone *, bool);\n+static void check_cookies (void);\n+\n+\n+/* Returns nonzero if P was allocated in GC'able memory.  */\n+\n+static inline int\n+ggc_allocated_p (const void *p)\n+{\n+  page_entry ***base;\n+  size_t L1, L2;\n+\n+#if HOST_BITS_PER_PTR <= 32\n+  base = &G.lookup[0];\n+#else\n+  page_table table = G.lookup;\n+  size_t high_bits = (size_t) p & ~ (size_t) 0xffffffff;\n+  while (1)\n+    {\n+      if (table == NULL)\n+\treturn 0;\n+      if (table->high_bits == high_bits)\n+\tbreak;\n+      table = table->next;\n+    }\n+  base = &table->table[0];\n+#endif\n+\n+  /* Extract the level 1 and 2 indices.  */\n+  L1 = LOOKUP_L1 (p);\n+  L2 = LOOKUP_L2 (p);\n+\n+  return base[L1] && base[L1][L2];\n+}\n+\n+/* Traverse the page table and find the entry for a page.\n+   Die (probably) if the object wasn't allocated via GC.  */\n+\n+static inline page_entry *\n+lookup_page_table_entry(const void *p)\n+{\n+  page_entry ***base;\n+  size_t L1, L2;\n+\n+#if HOST_BITS_PER_PTR <= 32\n+  base = &G.lookup[0];\n+#else\n+  page_table table = G.lookup;\n+  size_t high_bits = (size_t) p & ~ (size_t) 0xffffffff;\n+  while (table->high_bits != high_bits)\n+    table = table->next;\n+  base = &table->table[0];\n+#endif\n+\n+  /* Extract the level 1 and 2 indices.  */\n+  L1 = LOOKUP_L1 (p);\n+  L2 = LOOKUP_L2 (p);\n+\n+  return base[L1][L2];\n+\n+}\n+\n+/* Set the page table entry for a page.  */\n+\n+static void\n+set_page_table_entry(void *p, page_entry *entry)\n+{\n+  page_entry ***base;\n+  size_t L1, L2;\n+\n+#if HOST_BITS_PER_PTR <= 32\n+  base = &G.lookup[0];\n+#else\n+  page_table table;\n+  size_t high_bits = (size_t) p & ~ (size_t) 0xffffffff;\n+  for (table = G.lookup; table; table = table->next)\n+    if (table->high_bits == high_bits)\n+      goto found;\n+\n+  /* Not found -- allocate a new table.  */\n+  table = (page_table) xcalloc (1, sizeof(*table));\n+  table->next = G.lookup;\n+  table->high_bits = high_bits;\n+  G.lookup = table;\n+found:\n+  base = &table->table[0];\n+#endif\n+\n+  /* Extract the level 1 and 2 indices.  */\n+  L1 = LOOKUP_L1 (p);\n+  L2 = LOOKUP_L2 (p);\n+\n+  if (base[L1] == NULL)\n+    base[L1] = (page_entry **) xcalloc (PAGE_L2_SIZE, sizeof (page_entry *));\n+\n+  base[L1][L2] = entry;\n+}\n+\n+#ifdef USING_MMAP\n+/* Allocate SIZE bytes of anonymous memory, preferably near PREF,\n+   (if non-null).  The ifdef structure here is intended to cause a\n+   compile error unless exactly one of the HAVE_* is defined.  */\n+\n+static inline char *\n+alloc_anon (char *pref ATTRIBUTE_UNUSED, size_t size, struct alloc_zone *zone)\n+{\n+#ifdef HAVE_MMAP_ANON\n+  char *page = (char *) mmap (pref, size, PROT_READ | PROT_WRITE,\n+\t\t\t      MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);\n+#endif\n+#ifdef HAVE_MMAP_DEV_ZERO\n+  char *page = (char *) mmap (pref, size, PROT_READ | PROT_WRITE,\n+\t\t\t      MAP_PRIVATE, G.dev_zero_fd, 0);\n+#endif\n+  VALGRIND_MALLOCLIKE_BLOCK(page, size, 0, 0);\n+\n+  if (page == (char *) MAP_FAILED)\n+    {\n+      perror (\"virtual memory exhausted\");\n+      exit (FATAL_EXIT_CODE);\n+    }\n+\n+  /* Remember that we allocated this memory.  */\n+  zone->bytes_mapped += size;\n+  /* Pretend we don't have access to the allocated pages.  We'll enable\n+     access to smaller pieces of the area in ggc_alloc.  Discard the\n+     handle to avoid handle leak.  */\n+  VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS (page, size));\n+  return page;\n+}\n+#endif\n+#ifdef USING_MALLOC_PAGE_GROUPS\n+/* Compute the index for this page into the page group.  */\n+\n+static inline size_t\n+page_group_index (char *allocation, char *page)\n+{\n+  return (size_t) (page - allocation) >> G.lg_pagesize;\n+}\n+\n+/* Set and clear the in_use bit for this page in the page group.  */\n+\n+static inline void\n+set_page_group_in_use (page_group *group, char *page)\n+{\n+  group->in_use |= 1 << page_group_index (group->allocation, page);\n+}\n+\n+static inline void\n+clear_page_group_in_use (page_group *group, char *page)\n+{\n+  group->in_use &= ~(1 << page_group_index (group->allocation, page));\n+}\n+#endif\n+\n+/* Allocate a new page for allocating objects of size 2^ORDER,\n+   and return an entry for it.  The entry is not added to the\n+   appropriate page_table list.  */\n+\n+static inline struct page_entry *\n+alloc_small_page (struct alloc_zone *zone)\n+{\n+  struct page_entry *entry;\n+  char *page;\n+#ifdef USING_MALLOC_PAGE_GROUPS\n+  page_group *group;\n+#endif\n+\n+  page = NULL;\n+\n+  /* Check the list of free pages for one we can use.  */\n+  entry = zone->free_pages;\n+  if (entry != NULL)\n+    {\n+      /* Recycle the allocated memory from this page ...  */\n+      zone->free_pages = entry->next;\n+      page = entry->page;\n+\n+#ifdef USING_MALLOC_PAGE_GROUPS\n+      group = entry->group;\n+#endif\n+    }\n+#ifdef USING_MMAP\n+  else\n+    {\n+      /* We want just one page.  Allocate a bunch of them and put the\n+\t extras on the freelist.  (Can only do this optimization with\n+\t mmap for backing store.)  */\n+      struct page_entry *e, *f = zone->free_pages;\n+      int i;\n+\n+      page = alloc_anon (NULL, G.pagesize * GGC_QUIRE_SIZE, zone);\n+\n+      /* This loop counts down so that the chain will be in ascending\n+\t memory order.  */\n+      for (i = GGC_QUIRE_SIZE - 1; i >= 1; i--)\n+\t{\n+\t  e = (struct page_entry *) xmalloc (sizeof (struct page_entry));\n+\t  e->bytes = G.pagesize;\n+\t  e->page = page + (i << G.lg_pagesize);\n+\t  e->next = f;\n+\t  f = e;\n+\t}\n+\n+      zone->free_pages = f;\n+    }\n+#endif\n+#ifdef USING_MALLOC_PAGE_GROUPS\n+  else\n+    {\n+      /* Allocate a large block of memory and serve out the aligned\n+\t pages therein.  This results in much less memory wastage\n+\t than the traditional implementation of valloc.  */\n+\n+      char *allocation, *a, *enda;\n+      size_t alloc_size, head_slop, tail_slop;\n+      int multiple_pages = (entry_size == G.pagesize);\n+\n+      if (multiple_pages)\n+\talloc_size = GGC_QUIRE_SIZE * G.pagesize;\n+      else\n+\talloc_size = entry_size + G.pagesize - 1;\n+      allocation = xmalloc (alloc_size);\n+      VALGRIND_MALLOCLIKE_BLOCK(addr, alloc_size, 0, 0);\n+\n+      page = (char *) (((size_t) allocation + G.pagesize - 1) & -G.pagesize);\n+      head_slop = page - allocation;\n+      if (multiple_pages)\n+\ttail_slop = ((size_t) allocation + alloc_size) & (G.pagesize - 1);\n+      else\n+\ttail_slop = alloc_size - entry_size - head_slop;\n+      enda = allocation + alloc_size - tail_slop;\n+\n+      /* We allocated N pages, which are likely not aligned, leaving\n+\t us with N-1 usable pages.  We plan to place the page_group\n+\t structure somewhere in the slop.  */\n+      if (head_slop >= sizeof (page_group))\n+\tgroup = (page_group *)page - 1;\n+      else\n+\t{\n+\t  /* We magically got an aligned allocation.  Too bad, we have\n+\t     to waste a page anyway.  */\n+\t  if (tail_slop == 0)\n+\t    {\n+\t      enda -= G.pagesize;\n+\t      tail_slop += G.pagesize;\n+\t    }\n+\t  if (tail_slop < sizeof (page_group))\n+\t    abort ();\n+\t  group = (page_group *)enda;\n+\t  tail_slop -= sizeof (page_group);\n+\t}\n+\n+      /* Remember that we allocated this memory.  */\n+      group->next = G.page_groups;\n+      group->allocation = allocation;\n+      group->alloc_size = alloc_size;\n+      group->in_use = 0;\n+      zone->page_groups = group;\n+      G.bytes_mapped += alloc_size;\n+\n+      /* If we allocated multiple pages, put the rest on the free list.  */\n+      if (multiple_pages)\n+\t{\n+\t  struct page_entry *e, *f = G.free_pages;\n+\t  for (a = enda - G.pagesize; a != page; a -= G.pagesize)\n+\t    {\n+\t      e = (struct page_entry *) xmalloc (sizeof (struct page_entry));\n+\t      e->bytes = G.pagesize;\n+\t      e->page = a;\n+\t      e->group = group;\n+\t      e->next = f;\n+\t      f = e;\n+\t    }\n+\t  zone->free_pages = f;\n+\t}\n+    }\n+#endif\n+\n+  if (entry == NULL)\n+    entry = (struct page_entry *) xmalloc (sizeof (struct page_entry));\n+\n+  entry->next = 0;\n+  entry->bytes = G.pagesize;\n+  entry->bytes_free = G.pagesize;\n+  entry->page = page;\n+  entry->context_depth = zone->context_depth;\n+  entry->large_p = false;\n+  entry->zone = zone;\n+  zone->context_depth_allocations |= (unsigned long)1 << zone->context_depth;\n+\n+#ifdef USING_MALLOC_PAGE_GROUPS\n+  entry->group = group;\n+  set_page_group_in_use (group, page);\n+#endif\n+\n+  set_page_table_entry (page, entry);\n+\n+  if (GGC_DEBUG_LEVEL >= 2)\n+    fprintf (G.debug_file,\n+\t     \"Allocating %s page at %p, data %p-%p\\n\", entry->zone->name,\n+\t     (PTR) entry, page, page + G.pagesize - 1);\n+\n+  return entry;\n+}\n+\n+/* Allocate a large page of size SIZE in ZONE.  */\n+\n+static inline struct page_entry *\n+alloc_large_page (size_t size, struct alloc_zone *zone)\n+{\n+  struct page_entry *entry;\n+  char *page;\n+\n+  page = (char *) xmalloc (size + CHUNK_OVERHEAD + sizeof (struct page_entry));\n+  entry = (struct page_entry *) (page + size + CHUNK_OVERHEAD);\n+\n+  entry->next = 0;\n+  entry->bytes = size;\n+  entry->bytes_free = LARGE_OBJECT_SIZE + CHUNK_OVERHEAD;\n+  entry->page = page;\n+  entry->context_depth = zone->context_depth;\n+  entry->large_p = true;\n+  entry->zone = zone;\n+  zone->context_depth_allocations |= (unsigned long)1 << zone->context_depth;\n+\n+#ifdef USING_MALLOC_PAGE_GROUPS\n+  entry->group = NULL;\n+#endif\n+  set_page_table_entry (page, entry);\n+\n+  if (GGC_DEBUG_LEVEL >= 2)\n+    fprintf (G.debug_file,\n+\t     \"Allocating %s large page at %p, data %p-%p\\n\", entry->zone->name,\n+\t     (PTR) entry, page, page + size - 1);\n+\n+  return entry;\n+}\n+\n+\n+/* For a page that is no longer needed, put it on the free page list.  */\n+\n+static inline void\n+free_page (page_entry *entry)\n+{\n+  if (GGC_DEBUG_LEVEL >= 2)\n+    fprintf (G.debug_file,\n+\t     \"Deallocating %s page at %p, data %p-%p\\n\", entry->zone->name, (PTR) entry,\n+\t     entry->page, entry->page + entry->bytes - 1);\n+\n+  set_page_table_entry (entry->page, NULL);\n+\n+  if (entry->large_p)\n+    {\n+      free (entry->page);\n+      VALGRIND_FREELIKE_BLOCK (entry->page, entry->bytes);\n+    }\n+  else\n+    {\n+      /* Mark the page as inaccessible.  Discard the handle to\n+\t avoid handle leak.  */\n+      VALGRIND_DISCARD (VALGRIND_MAKE_NOACCESS (entry->page, entry->bytes));\n+\n+#ifdef USING_MALLOC_PAGE_GROUPS\n+      clear_page_group_in_use (entry->group, entry->page);\n+#endif\n+\n+      entry->next = entry->zone->free_pages;\n+      entry->zone->free_pages = entry;\n+    }\n+}\n+\n+/* Release the free page cache to the system.  */\n+\n+static void\n+release_pages (struct alloc_zone *zone)\n+{\n+#ifdef USING_MMAP\n+  page_entry *p, *next;\n+  char *start;\n+  size_t len;\n+\n+  /* Gather up adjacent pages so they are unmapped together.  */\n+  p = zone->free_pages;\n+\n+  while (p)\n+    {\n+      start = p->page;\n+      next = p->next;\n+      len = p->bytes;\n+      free (p);\n+      p = next;\n+\n+      while (p && p->page == start + len)\n+\t{\n+\t  next = p->next;\n+\t  len += p->bytes;\n+\t  free (p);\n+\t  p = next;\n+\t}\n+\n+      munmap (start, len);\n+      zone->bytes_mapped -= len;\n+    }\n+\n+  zone->free_pages = NULL;\n+#endif\n+#ifdef USING_MALLOC_PAGE_GROUPS\n+  page_entry **pp, *p;\n+  page_group **gp, *g;\n+\n+  /* Remove all pages from free page groups from the list.  */\n+  pp = &(zone->free_pages);\n+  while ((p = *pp) != NULL)\n+    if (p->group->in_use == 0)\n+      {\n+\t*pp = p->next;\n+\tfree (p);\n+      }\n+    else\n+      pp = &p->next;\n+\n+  /* Remove all free page groups, and release the storage.  */\n+  gp = &(zone->page_groups);\n+  while ((g = *gp) != NULL)\n+    if (g->in_use == 0)\n+      {\n+\t*gp = g->next;\n+\tzone->bytes_mapped -= g->alloc_size;\n+\tfree (g->allocation);\n+\tVALGRIND_FREELIKE_BLOCK(g->allocation, 0);\n+      }\n+    else\n+      gp = &g->next;\n+#endif\n+}\n+\n+/* Place CHUNK of size SIZE on the free list for ZONE.  */\n+\n+static inline void\n+free_chunk (struct alloc_chunk *chunk, size_t size, struct alloc_zone *zone)\n+{\n+  size_t bin = 0;\n+\n+  bin = SIZE_BIN_DOWN (size);\n+  if (bin == 0)\n+    abort ();\n+  if (bin > NUM_FREE_BINS)\n+    bin = 0;\n+#ifdef COOKIE_CHECKING\n+  if (chunk->magic != CHUNK_MAGIC && chunk->magic != DEADCHUNK_MAGIC)\n+    abort ();\n+  chunk->magic = DEADCHUNK_MAGIC;\n+#endif\n+  chunk->u.next_free = zone->free_chunks[bin];\n+  zone->free_chunks[bin] = chunk;\n+  if (GGC_DEBUG_LEVEL >= 3)\n+    fprintf (G.debug_file, \"Deallocating object, chunk=%p\\n\", (void *)chunk);\n+  VALGRIND_DISCARD (VALGRIND_MAKE_READABLE (chunk, sizeof (struct alloc_chunk)));\n+}\n+\n+/* Allocate a chunk of memory of SIZE bytes.  */\n+\n+static void *\n+ggc_alloc_zone_1 (size_t size, struct alloc_zone *zone, short type)\n+{\n+  size_t bin = 0;\n+  size_t lsize = 0;\n+  struct page_entry *entry;\n+  struct alloc_chunk *chunk, *lchunk, **pp;\n+  void *result;\n+\n+  /* Align size, so that we're assured of aligned allocations.  */\n+  if (size < FREE_BIN_DELTA)\n+    size = FREE_BIN_DELTA;\n+  size = (size + MAX_ALIGNMENT - 1) & -MAX_ALIGNMENT;\n+\n+  /* Large objects are handled specially.  */\n+  if (size >= G.pagesize - 2*CHUNK_OVERHEAD - FREE_BIN_DELTA)\n+    {\n+      entry = alloc_large_page (size, zone);\n+      entry->survived = 0;\n+      entry->next = entry->zone->pages;\n+      entry->zone->pages = entry;\n+\n+\n+      chunk = (struct alloc_chunk *) entry->page;\n+      VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (chunk, sizeof (struct alloc_chunk)));\n+      chunk->size = LARGE_OBJECT_SIZE;\n+\n+      goto found;\n+    }\n+\n+  /* First look for a tiny object already segregated into its own\n+     size bucket.  */\n+  bin = SIZE_BIN_UP (size);\n+  if (bin <= NUM_FREE_BINS)\n+    {\n+      chunk = zone->free_chunks[bin];\n+      if (chunk)\n+\t{\n+\t  zone->free_chunks[bin] = chunk->u.next_free;\n+\t  VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (chunk, sizeof (struct alloc_chunk)));\n+\t  goto found;\n+\t}\n+    }\n+\n+  /* Failing that, look through the \"other\" bucket for a chunk\n+     that is large enough.  */\n+  pp = &(zone->free_chunks[0]);\n+  chunk = *pp;\n+  while (chunk && chunk->size < size)\n+    {\n+      pp = &chunk->u.next_free;\n+      chunk = *pp;\n+    }\n+\n+  /* Failing that, allocate new storage.  */\n+  if (!chunk)\n+    {\n+      entry = alloc_small_page (zone);\n+      entry->next = entry->zone->pages;\n+      entry->zone->pages = entry;\n+\n+      chunk = (struct alloc_chunk *) entry->page;\n+      VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (chunk, sizeof (struct alloc_chunk)));\n+      chunk->size = G.pagesize - CHUNK_OVERHEAD;\n+    }\n+  else\n+    {\n+      *pp = chunk->u.next_free;\n+      VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (chunk, sizeof (struct alloc_chunk)));\n+    }\n+  /* Release extra memory from a chunk that's too big.  */\n+  lsize = chunk->size - size;\n+  if (lsize >= CHUNK_OVERHEAD + FREE_BIN_DELTA)\n+    {\n+      VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (chunk, sizeof (struct alloc_chunk)));\n+      chunk->size = size;\n+\n+      lsize -= CHUNK_OVERHEAD;\n+      lchunk = (struct alloc_chunk *)(chunk->u.data + size);\n+      VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (lchunk, sizeof (struct alloc_chunk)));\n+#ifdef COOKIE_CHECKING\n+      lchunk->magic = CHUNK_MAGIC;\n+#endif\n+      lchunk->type = 0;\n+      lchunk->mark = 0;\n+      lchunk->size = lsize;\n+      free_chunk (lchunk, lsize, zone);\n+    }\n+  /* Calculate the object's address.  */\n+ found:\n+#ifdef COOKIE_CHECKING\n+  chunk->magic = CHUNK_MAGIC;\n+#endif\n+  chunk->type = 1;\n+  chunk->mark = 0;\n+  chunk->typecode = type;\n+  result = chunk->u.data;\n+\n+#ifdef ENABLE_GC_CHECKING\n+  /* Keep poisoning-by-writing-0xaf the object, in an attempt to keep the\n+     exact same semantics in presence of memory bugs, regardless of\n+     ENABLE_VALGRIND_CHECKING.  We override this request below.  Drop the\n+     handle to avoid handle leak.  */\n+  VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (result, size));\n+\n+  /* `Poison' the entire allocated object.  */\n+  memset (result, 0xaf, size);\n+#endif\n+\n+  /* Tell Valgrind that the memory is there, but its content isn't\n+     defined.  The bytes at the end of the object are still marked\n+     unaccessible.  */\n+  VALGRIND_DISCARD (VALGRIND_MAKE_WRITABLE (result, size));\n+\n+  /* Keep track of how many bytes are being allocated.  This\n+     information is used in deciding when to collect.  */\n+  zone->allocated += size + CHUNK_OVERHEAD;\n+\n+  if (GGC_DEBUG_LEVEL >= 3)\n+    fprintf (G.debug_file, \"Allocating object, chunk=%p size=%lu at %p\\n\",\n+\t     (void *)chunk, (unsigned long) size, result);\n+\n+  return result;\n+}\n+\n+/* Allocate a SIZE of chunk memory of GTE type, into an approriate zone\n+   for that type.  */\n+\n+void *\n+ggc_alloc_typed (enum gt_types_enum gte, size_t size)\n+{\n+  if (gte == gt_ggc_e_14lang_tree_node)\n+    return ggc_alloc_zone_1 (size, tree_zone, gte);\n+  else if (gte == gt_ggc_e_7rtx_def)\n+    return ggc_alloc_zone_1 (size, rtl_zone, gte);\n+  else if (gte == gt_ggc_e_9rtvec_def)\n+    return ggc_alloc_zone_1 (size, rtl_zone, gte);\n+  else\n+    return ggc_alloc_zone_1 (size, &main_zone, gte);\n+}\n+\n+/* Normal ggc_alloc simply allocates into the main zone.  */\n+\n+void *\n+ggc_alloc (size_t size)\n+{\n+  return ggc_alloc_zone_1 (size, &main_zone, -1);\n+}\n+\n+/* Zone allocation allocates into the specified zone.  */\n+\n+void *\n+ggc_alloc_zone (size_t size, struct alloc_zone *zone)\n+{\n+  return ggc_alloc_zone_1 (size, zone, -1);\n+}\n+\n+/* If P is not marked, mark it and return false.  Otherwise return true.\n+   P must have been allocated by the GC allocator; it mustn't point to\n+   static objects, stack variables, or memory allocated with malloc.  */\n+\n+int\n+ggc_set_mark (const void *p)\n+{\n+  page_entry *entry;\n+  struct alloc_chunk *chunk;\n+\n+#ifdef ENABLE_CHECKING\n+  /* Look up the page on which the object is alloced.  If the object\n+     wasn't allocated by the collector, we'll probably die.  */\n+  entry = lookup_page_table_entry (p);\n+  if (entry == NULL)\n+    abort ();\n+#endif\n+  chunk = (struct alloc_chunk *) ((char *)p - CHUNK_OVERHEAD);\n+#ifdef COOKIE_CHECKING\n+  if (chunk->magic != CHUNK_MAGIC)\n+    abort ();\n+#endif\n+  if (chunk->mark)\n+    return 1;\n+  chunk->mark = 1;\n+\n+#ifndef ENABLE_CHECKING\n+  entry = lookup_page_table_entry (p);\n+#endif\n+\n+  /* Large pages are either completely full or completely empty. So if\n+     they are marked, they are completely full.  */\n+  if (entry->large_p)\n+    entry->bytes_free = 0;\n+  else\n+    entry->bytes_free -= chunk->size + CHUNK_OVERHEAD;\n+\n+  if (GGC_DEBUG_LEVEL >= 4)\n+    fprintf (G.debug_file, \"Marking %p\\n\", p);\n+\n+  return 0;\n+}\n+\n+/* Return 1 if P has been marked, zero otherwise.\n+   P must have been allocated by the GC allocator; it mustn't point to\n+   static objects, stack variables, or memory allocated with malloc.  */\n+\n+int\n+ggc_marked_p (const void *p)\n+{\n+  struct alloc_chunk *chunk;\n+\n+#ifdef ENABLE_CHECKING\n+  {\n+    page_entry *entry = lookup_page_table_entry (p);\n+    if (entry == NULL)\n+      abort ();\n+  }\n+#endif\n+\n+  chunk = (struct alloc_chunk *) ((char *)p - CHUNK_OVERHEAD);\n+#ifdef COOKIE_CHECKING\n+  if (chunk->magic != CHUNK_MAGIC)\n+    abort ();\n+#endif\n+  return chunk->mark;\n+}\n+\n+/* Return the size of the gc-able object P.  */\n+\n+size_t\n+ggc_get_size (const void *p)\n+{\n+  struct alloc_chunk *chunk;\n+  struct page_entry *entry;\n+\n+#ifdef ENABLE_CHECKING\n+  entry = lookup_page_table_entry (p);\n+  if (entry == NULL)\n+    abort ();\n+#endif\n+\n+  chunk = (struct alloc_chunk *) ((char *)p - CHUNK_OVERHEAD);\n+#ifdef COOKIE_CHECKING\n+  if (chunk->magic != CHUNK_MAGIC)\n+    abort ();\n+#endif\n+  if (chunk->size == LARGE_OBJECT_SIZE)\n+    {\n+#ifndef ENABLE_CHECKING\n+      entry = lookup_page_table_entry (p);\n+#endif\n+      return entry->bytes;\n+    }\n+\n+  return chunk->size;\n+}\n+\n+/* Initialize the ggc-zone-mmap allocator.  */\n+void\n+init_ggc ()\n+{\n+  /* Create the zones.  */\n+  main_zone.name = \"Main zone\";\n+  G.zones = &main_zone;\n+\n+  rtl_zone = xcalloc (1, sizeof (struct alloc_zone));\n+  rtl_zone->name = \"RTL zone\";\n+  /* The main zone's connected to the ... rtl_zone */\n+  G.zones->next_zone = rtl_zone;\n+\n+  garbage_zone = xcalloc (1, sizeof (struct alloc_zone));\n+  garbage_zone->name = \"Garbage zone\";\n+  /* The rtl zone's connected to the ... garbage zone */\n+  rtl_zone->next_zone = garbage_zone;\n+\n+  tree_zone = xcalloc (1, sizeof (struct alloc_zone));\n+  tree_zone->name = \"Tree zone\";\n+  /* The garbage zone's connected to ... the tree zone */\n+  garbage_zone->next_zone = tree_zone;\n+\n+  G.pagesize = getpagesize();\n+  G.lg_pagesize = exact_log2 (G.pagesize);\n+#ifdef HAVE_MMAP_DEV_ZERO\n+  G.dev_zero_fd = open (\"/dev/zero\", O_RDONLY);\n+  if (G.dev_zero_fd == -1)\n+    abort ();\n+#endif\n+\n+#if 0\n+  G.debug_file = fopen (\"ggc-mmap.debug\", \"w\");\n+  setlinebuf (G.debug_file);\n+#else\n+  G.debug_file = stdout;\n+#endif\n+\n+#ifdef USING_MMAP\n+  /* StunOS has an amazing off-by-one error for the first mmap allocation\n+     after fiddling with RLIMIT_STACK.  The result, as hard as it is to\n+     believe, is an unaligned page allocation, which would cause us to\n+     hork badly if we tried to use it.  */\n+  {\n+    char *p = alloc_anon (NULL, G.pagesize, &main_zone);\n+    struct page_entry *e;\n+    if ((size_t)p & (G.pagesize - 1))\n+      {\n+\t/* How losing.  Discard this one and try another.  If we still\n+\t   can't get something useful, give up.  */\n+\n+\tp = alloc_anon (NULL, G.pagesize, &main_zone);\n+\tif ((size_t)p & (G.pagesize - 1))\n+\t  abort ();\n+      }\n+\n+    /* We have a good page, might as well hold onto it...  */\n+    e = (struct page_entry *) xmalloc (sizeof (struct page_entry));\n+    e->bytes = G.pagesize;\n+    e->page = p;\n+    e->next = main_zone.free_pages;\n+    main_zone.free_pages = e;\n+  }\n+#endif\n+}\n+\n+/* Increment the `GC context'.  Objects allocated in an outer context\n+   are never freed, eliminating the need to register their roots.  */\n+\n+void\n+ggc_push_context ()\n+{\n+  struct alloc_zone *zone;\n+  for (zone = G.zones; zone; zone = zone->next_zone)\n+    ++(zone->context_depth);\n+  /* Die on wrap.  */\n+  if (main_zone.context_depth >= HOST_BITS_PER_LONG)\n+    abort ();\n+}\n+\n+/* Decrement the `GC context'.  All objects allocated since the\n+   previous ggc_push_context are migrated to the outer context.  */\n+\n+static void\n+ggc_pop_context_1 (struct alloc_zone *zone)\n+{\n+  unsigned long omask;\n+  unsigned depth;\n+  page_entry *p;\n+\n+  depth = --(zone->context_depth);\n+  omask = (unsigned long)1 << (depth + 1);\n+\n+  if (!((zone->context_depth_allocations | zone->context_depth_collections) & omask))\n+    return;\n+\n+  zone->context_depth_allocations |= (zone->context_depth_allocations & omask) >> 1;\n+  zone->context_depth_allocations &= omask - 1;\n+  zone->context_depth_collections &= omask - 1;\n+\n+  /* Any remaining pages in the popped context are lowered to the new\n+     current context; i.e. objects allocated in the popped context and\n+     left over are imported into the previous context.  */\n+    for (p = zone->pages; p != NULL; p = p->next)\n+      if (p->context_depth > depth)\n+\tp->context_depth = depth;\n+}\n+\n+/* Pop all the zone contexts.  */\n+\n+void\n+ggc_pop_context ()\n+{\n+  struct alloc_zone *zone;\n+  for (zone = G.zones; zone; zone = zone->next_zone)\n+    ggc_pop_context_1 (zone);\n+}\n+\n+\n+/* Poison the chunk.  */\n+#ifdef ENABLE_GC_CHECKING\n+#define poison_chunk(CHUNK, SIZE) \\\n+  memset ((CHUNK)->u.data, 0xa5, (SIZE))\n+#else\n+#define poison_chunk(CHUNK, SIZE)\n+#endif\n+\n+/* Free all empty pages and objects within a page for a given zone  */\n+\n+static void\n+sweep_pages (struct alloc_zone *zone)\n+{\n+  page_entry **pp, *p, *next;\n+  struct alloc_chunk *chunk, *last_free, *end;\n+  size_t last_free_size, allocated = 0;\n+\n+  /* First, reset the free_chunks lists, since we are going to\n+     re-free free chunks in hopes of coalescing them into large chunks.  */\n+  memset (zone->free_chunks, 0, sizeof (zone->free_chunks));\n+  pp = &zone->pages;\n+  for (p = zone->pages; p ; p = next)\n+    {\n+      next = p->next;\n+\n+      /* For empty pages, just free the page.  */\n+      if (p->bytes_free == G.pagesize && p->context_depth == zone->context_depth)\n+\t{\n+\t  *pp = next;\n+#ifdef ENABLE_GC_CHECKING\n+\t  /* Poison the page.  */\n+\t  memset (p->page, 0xb5, p->bytes);\n+#endif\n+\t  free_page (p);\n+\t  continue;\n+\t}\n+\n+      /* Large pages are all or none affairs. Either they are\n+\t completely empty, or they are completeley full.\n+\t Thus, if the above didn't catch it, we need not do anything\n+\t except remove the mark and reset the bytes_free.\n+\n+\t XXX: Should we bother to increment allocated.  */\n+      else if (p->large_p)\n+\t{\n+\t  p->bytes_free = p->bytes;\n+\t  ((struct alloc_chunk *)p->page)->mark = 0;\n+\t  continue;\n+\t}\n+      pp = &p->next;\n+\n+      /* This page has now survived another collection.  */\n+      p->survived++;\n+\n+      /* Which leaves full and partial pages.  Step through all chunks,\n+\t consolidate those that are free and insert them into the free\n+\t lists.  Note that consolidation slows down collection\n+\t slightly.  */\n+\n+      chunk = (struct alloc_chunk *)p->page;\n+      end = (struct alloc_chunk *)(p->page + G.pagesize);\n+      last_free = NULL;\n+      last_free_size = 0;\n+\n+      do\n+\t{\n+\t  prefetch ((struct alloc_chunk *)(chunk->u.data + chunk->size));\n+\t  if (chunk->mark || p->context_depth < zone->context_depth)\n+\t    {\n+\t      if (last_free)\n+\t\t{\n+\t\t  last_free->type = 0;\n+\t\t  last_free->size = last_free_size;\n+\t\t  last_free->mark = 0;\n+\t\t  poison_chunk (last_free, last_free_size);\n+\t\t  free_chunk (last_free, last_free_size, zone);\n+\t\t  last_free = NULL;\n+\t\t}\n+\t      if (chunk->mark)\n+\t        {\n+\t          allocated += chunk->size + CHUNK_OVERHEAD;\n+ \t          p->bytes_free += chunk->size + CHUNK_OVERHEAD;\n+\t\t}\n+\t      chunk->mark = 0;\n+#ifdef ENABLE_CHECKING\n+\t      if (p->bytes_free > p->bytes)\n+\t\tabort ();\n+#endif\n+\t    }\n+\t  else\n+\t    {\n+\t      if (last_free)\n+\t        {\n+\t\t  last_free_size += CHUNK_OVERHEAD + chunk->size;\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  last_free = chunk;\n+\t\t  last_free_size = chunk->size;\n+\t\t}\n+\t    }\n+\n+\t  chunk = (struct alloc_chunk *)(chunk->u.data + chunk->size);\n+\t}\n+      while (chunk < end);\n+\n+      if (last_free)\n+\t{\n+\t  last_free->type = 0;\n+\t  last_free->size = last_free_size;\n+\t  last_free->mark = 0;\n+\t  poison_chunk (last_free, last_free_size);\n+\t  free_chunk (last_free, last_free_size, zone);\n+\t}\n+    }\n+\n+  zone->allocated = allocated;\n+}\n+\n+/* mark-and-sweep routine for collecting a single zone.  NEED_MARKING\n+   is true if we need to mark before sweeping, false if some other\n+   zone collection has already performed marking for us.  Returns true\n+   if we collected, false otherwise.  */\n+\n+static bool\n+ggc_collect_1 (struct alloc_zone *zone, bool need_marking)\n+{\n+  /* Avoid frequent unnecessary work by skipping collection if the\n+     total allocations haven't expanded much since the last\n+     collection.  */\n+  float allocated_last_gc =\n+    MAX (zone->allocated_last_gc, (size_t)PARAM_VALUE (GGC_MIN_HEAPSIZE) * 1024);\n+\n+  float min_expand = allocated_last_gc * PARAM_VALUE (GGC_MIN_EXPAND) / 100;\n+\n+  if (zone->allocated < allocated_last_gc + min_expand)\n+    return false;\n+\n+  if (!quiet_flag)\n+    fprintf (stderr, \" {%s GC %luk -> \", zone->name, (unsigned long) zone->allocated / 1024);\n+\n+  /* Zero the total allocated bytes.  This will be recalculated in the\n+     sweep phase.  */\n+  zone->allocated = 0;\n+\n+  /* Release the pages we freed the last time we collected, but didn't\n+     reuse in the interim.  */\n+  release_pages (zone);\n+\n+  /* Indicate that we've seen collections at this context depth.  */\n+  zone->context_depth_collections\n+    = ((unsigned long)1 << (zone->context_depth + 1)) - 1;\n+  if (need_marking)\n+    ggc_mark_roots ();\n+  sweep_pages (zone);\n+  zone->was_collected = true;\n+  zone->allocated_last_gc = zone->allocated;\n+\n+\n+  if (!quiet_flag)\n+    fprintf (stderr, \"%luk}\", (unsigned long) zone->allocated / 1024);\n+  return true;\n+}\n+\n+/* Calculate the average page survival rate in terms of number of\n+   collections.  */\n+\n+static float\n+calculate_average_page_survival (struct alloc_zone *zone)\n+{\n+  float count = 0.0;\n+  float survival = 0.0;\n+  page_entry *p;\n+  for (p = zone->pages; p; p = p->next)\n+    {\n+      count += 1.0;\n+      survival += p->survived;\n+    }\n+  return survival/count;\n+}\n+\n+/* Check the magic cookies all of the chunks contain, to make sure we\n+   aren't doing anything stupid, like stomping on alloc_chunk\n+   structures.  */\n+\n+static inline void\n+check_cookies ()\n+{\n+#ifdef COOKIE_CHECKING\n+  page_entry *p;\n+  for (zone = G.zones; zone; zone = zone->next_zone)\n+    {\n+      for (p = zone->pages; p; p = p->next)\n+\t{\n+\t  if (!p->large_p)\n+\t    {\n+\t      struct alloc_chunk *chunk = (struct alloc_chunk *)p->page;\n+\t      struct alloc_chunk *end = (struct alloc_chunk *)(p->page + G.pagesize);\n+\t      do\n+\t\t{\n+\t\t  if (chunk->magic != CHUNK_MAGIC && chunk->magic != DEADCHUNK_MAGIC)\n+\t\t    abort ();\n+\t\t  chunk = (struct alloc_chunk *)(chunk->u.data + chunk->size);\n+\t\t}\n+\t      while (chunk < end);\n+\t    }\n+\t}\n+    }\n+#endif\n+}\n+\n+\n+/* Top level collection routine.  */\n+\n+void\n+ggc_collect ()\n+{\n+  struct alloc_zone *zone;\n+  bool marked = false;\n+  float f;\n+\n+  timevar_push (TV_GC);\n+  check_cookies ();\n+  /* Start by possibly collecting the main zone.  */\n+  main_zone.was_collected = false;\n+  marked |= ggc_collect_1 (&main_zone, true);\n+  /* In order to keep the number of collections down, we don't\n+     collect other zones unless we are collecting the main zone.  This\n+     gives us roughly the same number of collections as we used to\n+     have with the old gc.  The number of collection is important\n+     because our main slowdown (according to profiling) is now in\n+     marking.  So if we mark twice as often as we used to, we'll be\n+     twice as slow.  Hopefully we'll avoid this cost when we mark\n+     zone-at-a-time.  */\n+\n+  if (main_zone.was_collected)\n+    {\n+      check_cookies ();\n+      rtl_zone->was_collected = false;\n+      marked |= ggc_collect_1 (rtl_zone, !marked);\n+      check_cookies ();\n+      tree_zone->was_collected = false;\n+      marked |= ggc_collect_1 (tree_zone, !marked);\n+      check_cookies ();\n+      garbage_zone->was_collected = false;\n+      marked |= ggc_collect_1 (garbage_zone, !marked);\n+    }\n+\n+  /* Print page survival stats, if someone wants them.  */\n+  if (GGC_DEBUG_LEVEL >= 2)\n+    {\n+      if (rtl_zone->was_collected)\n+\t{\n+\t  f = calculate_average_page_survival (rtl_zone);\n+\t  printf (\"Average RTL page survival is %f\\n\", f);\n+\t}\n+      if (main_zone.was_collected)\n+\t{\n+\t  f = calculate_average_page_survival (&main_zone);\n+\t  printf (\"Average main page survival is %f\\n\", f);\n+\t}\n+      if (tree_zone->was_collected)\n+\t{\n+\t  f = calculate_average_page_survival (tree_zone);\n+\t  printf (\"Average tree page survival is %f\\n\", f);\n+\t}\n+    }\n+  /* Since we don't mark zone at a time right now, marking in any\n+     zone means marking in every zone. So we have to clear all the\n+     marks in all the zones that weren't collected already.  */\n+  if (marked)\n+    {\n+      page_entry *p;\n+      for (zone = G.zones; zone; zone = zone->next_zone)\n+      {\n+\tif (zone->was_collected)\n+\t  continue;\n+\tfor (p = zone->pages; p; p = p->next)\n+\t  {\n+\t    if (!p->large_p)\n+\t      {\n+\t\tstruct alloc_chunk *chunk = (struct alloc_chunk *)p->page;\n+\t\tstruct alloc_chunk *end = (struct alloc_chunk *)(p->page + G.pagesize);\n+\t\tdo\n+\t\t  {\n+\t\t    prefetch ((struct alloc_chunk *)(chunk->u.data + chunk->size));\n+\t\t    if (chunk->mark || p->context_depth < zone->context_depth)\n+\t\t      {\n+\t\t        if (chunk->mark)\n+\t\t \t  p->bytes_free += chunk->size + CHUNK_OVERHEAD;\n+#ifdef ENABLE_CHECKING\n+\t\t\tif (p->bytes_free > p->bytes)\n+\t\t\t  abort ();\n+#endif\n+\t\t\tchunk->mark = 0;\n+\t\t      }\n+\t\t    chunk = (struct alloc_chunk *)(chunk->u.data + chunk->size);\n+\t\t  }\n+\t\twhile (chunk < end);\n+\t      }\n+\t    else\n+\t      {\n+\t\tp->bytes_free = p->bytes;\n+\t\t((struct alloc_chunk *)p->page)->mark = 0;\n+\t      }\n+\t  }\n+      }\n+    }\n+  timevar_pop (TV_GC);\n+}\n+\n+/* Print allocation statistics.  */\n+\n+void\n+ggc_print_statistics ()\n+{\n+}\n+\n+struct ggc_pch_data\n+{\n+  struct ggc_pch_ondisk\n+  {\n+    unsigned total;\n+  } d;\n+  size_t base;\n+  size_t written;\n+\n+};\n+\n+/* Initialize the PCH datastructure.  */\n+\n+struct ggc_pch_data *\n+init_ggc_pch (void)\n+{\n+  return xcalloc (sizeof (struct ggc_pch_data), 1);\n+}\n+\n+/* Add the size of object X to the size of the PCH data.  */\n+\n+void\n+ggc_pch_count_object (struct ggc_pch_data *d, void *x ATTRIBUTE_UNUSED,\n+\t\t      size_t size, bool is_string)\n+{\n+  if (!is_string)\n+    {\n+      d->d.total += size + CHUNK_OVERHEAD;\n+    }\n+  else\n+    d->d.total += size;\n+}\n+\n+/* Return the total size of the PCH data.  */\n+\n+size_t\n+ggc_pch_total_size (struct ggc_pch_data *d)\n+{\n+  return d->d.total;\n+}\n+\n+/* Set the base address for the objects in the PCH file.  */\n+\n+void\n+ggc_pch_this_base (struct ggc_pch_data *d, void *base)\n+{\n+  d->base = (size_t) base;\n+}\n+\n+/* Allocate a place for object X of size SIZE in the PCH file.  */\n+\n+char *\n+ggc_pch_alloc_object (struct ggc_pch_data *d, void *x,\n+\t\t      size_t size, bool is_string)\n+{\n+  char *result;\n+  result = (char *)d->base;\n+  if (!is_string)\n+    {\n+      struct alloc_chunk *chunk = (struct alloc_chunk *) ((char *)x - CHUNK_OVERHEAD);\n+      if (chunk->size == LARGE_OBJECT_SIZE)\n+\td->base += ggc_get_size (x) + CHUNK_OVERHEAD;\n+      else\n+\td->base += chunk->size + CHUNK_OVERHEAD;\n+      return result + CHUNK_OVERHEAD;\n+    }\n+  else\n+    {\n+      d->base += size;\n+      return result;\n+    }\n+\n+}\n+\n+/* Prepare to write out the PCH data to file F.  */\n+\n+void\n+ggc_pch_prepare_write (struct ggc_pch_data *d ATTRIBUTE_UNUSED,\n+\t\t       FILE *f ATTRIBUTE_UNUSED)\n+{\n+  /* Nothing to do.  */\n+}\n+\n+/* Write out object X of SIZE to file F.  */\n+\n+void\n+ggc_pch_write_object (struct ggc_pch_data *d ATTRIBUTE_UNUSED,\n+\t\t      FILE *f, void *x, void *newx ATTRIBUTE_UNUSED,\n+\t\t      size_t size, bool is_string)\n+{\n+  if (!is_string)\n+    {\n+      struct alloc_chunk *chunk = (struct alloc_chunk *) ((char *)x - CHUNK_OVERHEAD);\n+      size = chunk->size;\n+      if (fwrite (chunk, size + CHUNK_OVERHEAD, 1, f) != 1)\n+\tfatal_error (\"can't write PCH file: %m\");\n+      d->written += size + CHUNK_OVERHEAD;\n+    }\n+   else\n+     {\n+       if (fwrite (x, size, 1, f) != 1)\n+\t fatal_error (\"can't write PCH file: %m\");\n+       d->written += size;\n+     }\n+  if (d->written == d->d.total\n+      && fseek (f, ROUND_UP_VALUE (d->d.total, G.pagesize), SEEK_CUR) != 0)\n+    fatal_error (\"can't write PCH file: %m\");\n+}\n+\n+void\n+ggc_pch_finish (struct ggc_pch_data *d, FILE *f)\n+{\n+  if (fwrite (&d->d, sizeof (d->d), 1, f) != 1)\n+    fatal_error (\"can't write PCH file: %m\");\n+  free (d);\n+}\n+\n+\n+void\n+ggc_pch_read (FILE *f, void *addr)\n+{\n+  struct ggc_pch_ondisk d;\n+  struct page_entry *entry;\n+  char *pte;\n+  if (fread (&d, sizeof (d), 1, f) != 1)\n+    fatal_error (\"can't read PCH file: %m\");\n+  entry = xcalloc (1, sizeof (struct page_entry));\n+  entry->bytes = d.total;\n+  entry->page = addr;\n+  entry->context_depth = 0;\n+  entry->zone = &main_zone;\n+  for (pte = entry->page;\n+       pte < entry->page + entry->bytes;\n+       pte += G.pagesize)\n+    set_page_table_entry (pte, entry);\n+\n+}"}, {"sha": "4f25f7373c2c5d9bf21d34cc5cd327e2578bf2fc", "filename": "gcc/ggc.h", "status": "modified", "additions": 28, "deletions": 9, "changes": 37, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2Fggc.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b6f611637e7411536dc883100c543547f4b7c03a/gcc%2Fggc.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fggc.h?ref=b6f611637e7411536dc883100c543547f4b7c03a", "patch": "@@ -159,8 +159,9 @@ extern struct ggc_pch_data *init_ggc_pch (void);\n \n /* The second parameter and third parameters give the address and size\n    of an object.  Update the ggc_pch_data structure with as much of\n-   that information as is necessary.  */\n-extern void ggc_pch_count_object (struct ggc_pch_data *, void *, size_t);\n+   that information as is necessary. The last argument should be true\n+   if the object is a string.  */\n+extern void ggc_pch_count_object (struct ggc_pch_data *, void *, size_t, bool);\n \n /* Return the total size of the data to be written to hold all\n    the objects previously passed to ggc_pch_count_object.  */\n@@ -171,14 +172,16 @@ extern size_t ggc_pch_total_size (struct ggc_pch_data *);\n extern void ggc_pch_this_base (struct ggc_pch_data *, void *);\n \n /* Assuming that the objects really do end up at the address\n-   passed to ggc_pch_this_base, return the address of this object.  */\n-extern char *ggc_pch_alloc_object (struct ggc_pch_data *, void *, size_t);\n+   passed to ggc_pch_this_base, return the address of this object.\n+   The last argument should be true if the object is a string.  */\n+extern char *ggc_pch_alloc_object (struct ggc_pch_data *, void *, size_t, bool);\n \n /* Write out any initial information required.  */\n extern void ggc_pch_prepare_write (struct ggc_pch_data *, FILE *);\n-/* Write out this object, including any padding.  */\n+/* Write out this object, including any padding.  The last argument should be\n+   true if the object is a string.  */\n extern void ggc_pch_write_object (struct ggc_pch_data *, FILE *, void *,\n-\t\t\t\t  void *, size_t);\n+\t\t\t\t  void *, size_t, bool);\n /* All objects have been written, write out any final information\n    required.  */\n extern void ggc_pch_finish (struct ggc_pch_data *, FILE *);\n@@ -190,22 +193,38 @@ extern void ggc_pch_read (FILE *, void *);\n \f\n /* Allocation.  */\n \n+/* Zone structure.  */\n+struct alloc_zone;\n+/* For single pass garbage.  */\n+extern struct alloc_zone *garbage_zone;\n+/* For regular rtl allocations.  */\n+extern struct alloc_zone *rtl_zone;\n+/* For regular tree allocations.  */\n+extern struct alloc_zone *tree_zone;\n+\n /* The internal primitive.  */\n extern void *ggc_alloc (size_t);\n+/* Allocate an object into the specified allocation zone.  */\n+extern void *ggc_alloc_zone (size_t, struct alloc_zone *);\n+/* Allocate an object of the specified type and size. */\n+extern void *ggc_alloc_typed (enum gt_types_enum, size_t);\n /* Like ggc_alloc, but allocates cleared memory.  */\n extern void *ggc_alloc_cleared (size_t);\n+/* Like ggc_alloc_zone, but allocates cleared memory.  */\n+extern void *ggc_alloc_cleared_zone (size_t, struct alloc_zone *);\n /* Resize a block.  */\n extern void *ggc_realloc (void *, size_t);\n /* Like ggc_alloc_cleared, but performs a multiplication.  */\n extern void *ggc_calloc (size_t, size_t);\n \n-#define ggc_alloc_rtx(CODE) ((rtx) ggc_alloc (RTX_SIZE (CODE)))\n+#define ggc_alloc_rtx(CODE)                    \\\n+  ((rtx) ggc_alloc_typed (gt_ggc_e_7rtx_def, RTX_SIZE (CODE)))\n \n #define ggc_alloc_rtvec(NELT)\t\t\t\t\t\t  \\\n-  ((rtvec) ggc_alloc (sizeof (struct rtvec_def)\t\t\t\t  \\\n+  ((rtvec) ggc_alloc_typed (gt_ggc_e_9rtvec_def, sizeof (struct rtvec_def) \\\n \t\t      + ((NELT) - 1) * sizeof (rtx)))\n \n-#define ggc_alloc_tree(LENGTH) ((tree) ggc_alloc (LENGTH))\n+#define ggc_alloc_tree(LENGTH) ((tree) ggc_alloc_zone (LENGTH, tree_zone))\n \n #define htab_create_ggc(SIZE, HASH, EQ, DEL) \\\n   htab_create_alloc (SIZE, HASH, EQ, DEL, ggc_calloc, NULL)"}]}