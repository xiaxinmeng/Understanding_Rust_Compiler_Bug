{"sha": "6fcb740ac456e4703b337584b26305a77a5c5038", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NmZjYjc0MGFjNDU2ZTQ3MDNiMzM3NTg0YjI2MzA1YTc3YTVjNTAzOA==", "commit": {"author": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2017-01-10T21:09:00Z"}, "committer": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2017-01-10T21:09:00Z"}, "message": "runtime: copy more scheduler code from Go 1.7 runtime\n    \n    I looked at a diff of proc.go between Go 1.7 and gccgo, and copied\n    over all the easy stuff.\n    \n    Reviewed-on: https://go-review.googlesource.com/35090\n\nFrom-SVN: r244291", "tree": {"sha": "74b83d3a2724b1e617c8716e2cfad76fddcc6bd8", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/74b83d3a2724b1e617c8716e2cfad76fddcc6bd8"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/6fcb740ac456e4703b337584b26305a77a5c5038", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6fcb740ac456e4703b337584b26305a77a5c5038", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6fcb740ac456e4703b337584b26305a77a5c5038", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6fcb740ac456e4703b337584b26305a77a5c5038/comments", "author": null, "committer": null, "parents": [{"sha": "c16880eff008084f55a56a07cb7a16453d2b94c4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c16880eff008084f55a56a07cb7a16453d2b94c4", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c16880eff008084f55a56a07cb7a16453d2b94c4"}], "stats": {"total": 981, "additions": 587, "deletions": 394}, "files": [{"sha": "4cf7fc9eaa252d98e97b460899d99d783f72b88f", "filename": "gcc/go/gofrontend/MERGE", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6fcb740ac456e4703b337584b26305a77a5c5038/gcc%2Fgo%2Fgofrontend%2FMERGE", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6fcb740ac456e4703b337584b26305a77a5c5038/gcc%2Fgo%2Fgofrontend%2FMERGE", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2FMERGE?ref=6fcb740ac456e4703b337584b26305a77a5c5038", "patch": "@@ -1,4 +1,4 @@\n-f439989e483b7c2eada6ddcf6e730a791cce603f\n+d3725d876496f2cca3d6ce538e98b58c85d90bfb\n \n The first line of this file holds the git revision number of the last\n merge done from the gofrontend repository."}, {"sha": "a8827f20b2abb175f2ce1f140ebc5b8e9b3a9fb9", "filename": "libgo/go/runtime/debug.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fgo%2Fruntime%2Fdebug.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fgo%2Fruntime%2Fdebug.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fdebug.go?ref=6fcb740ac456e4703b337584b26305a77a5c5038", "patch": "@@ -44,7 +44,7 @@ func NumCPU() int\n // NumCgoCall returns the number of cgo calls made by the current process.\n func NumCgoCall() int64 {\n \tvar n int64\n-\tfor mp := (*m)(atomic.Loadp(unsafe.Pointer(allm()))); mp != nil; mp = mp.alllink {\n+\tfor mp := (*m)(atomic.Loadp(unsafe.Pointer(&allm))); mp != nil; mp = mp.alllink {\n \t\tn += int64(mp.ncgocall)\n \t}\n \treturn n"}, {"sha": "77b59000f32117715c24fd9515ca824d8d19ba32", "filename": "libgo/go/runtime/export_test.go", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fgo%2Fruntime%2Fexport_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fgo%2Fruntime%2Fexport_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fexport_test.go?ref=6fcb740ac456e4703b337584b26305a77a5c5038", "patch": "@@ -24,8 +24,7 @@ import (\n \n var Entersyscall = entersyscall\n var Exitsyscall = exitsyscall\n-\n-// var LockedOSThread = lockedOSThread\n+var LockedOSThread = lockedOSThread\n \n // var Xadduintptr = xadduintptr\n "}, {"sha": "e1900176637fa43cf18b70c90fc85413e52cb2f9", "filename": "libgo/go/runtime/mprof.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fgo%2Fruntime%2Fmprof.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fgo%2Fruntime%2Fmprof.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmprof.go?ref=6fcb740ac456e4703b337584b26305a77a5c5038", "patch": "@@ -521,7 +521,7 @@ func BlockProfile(p []BlockProfileRecord) (n int, ok bool) {\n // Most clients should use the runtime/pprof package instead\n // of calling ThreadCreateProfile directly.\n func ThreadCreateProfile(p []StackRecord) (n int, ok bool) {\n-\tfirst := (*m)(atomic.Loadp(unsafe.Pointer(allm())))\n+\tfirst := (*m)(atomic.Loadp(unsafe.Pointer(&allm)))\n \tfor mp := first; mp != nil; mp = mp.alllink {\n \t\tn++\n \t}"}, {"sha": "1bdef7d43c213371cbf9b0131de00f79be8de275", "filename": "libgo/go/runtime/os_gccgo.go", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fgo%2Fruntime%2Fos_gccgo.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fgo%2Fruntime%2Fos_gccgo.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fos_gccgo.go?ref=6fcb740ac456e4703b337584b26305a77a5c5038", "patch": "@@ -11,6 +11,13 @@ import (\n // Temporary for C code to call:\n //go:linkname minit runtime.minit\n \n+// Called to initialize a new m (including the bootstrap m).\n+// Called on the parent thread (main thread in case of bootstrap), can allocate memory.\n+func mpreinit(mp *m) {\n+\tmp.gsignal = malg(true, true, &mp.gsignalstack, &mp.gsignalstacksize)\n+\tmp.gsignal.m = mp\n+}\n+\n // minit is called to initialize a new m (including the bootstrap m).\n // Called on the new thread, cannot allocate memory.\n func minit() {"}, {"sha": "ef863c8ec555abef80a9571a87decd863c8dc326", "filename": "libgo/go/runtime/proc.go", "status": "modified", "additions": 558, "deletions": 18, "changes": 576, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fgo%2Fruntime%2Fproc.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fgo%2Fruntime%2Fproc.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fproc.go?ref=6fcb740ac456e4703b337584b26305a77a5c5038", "patch": "@@ -18,6 +18,7 @@ import (\n //go:linkname sysmon runtime.sysmon\n //go:linkname schedtrace runtime.schedtrace\n //go:linkname allgadd runtime.allgadd\n+//go:linkname mcommoninit runtime.mcommoninit\n //go:linkname ready runtime.ready\n //go:linkname gcprocs runtime.gcprocs\n //go:linkname needaddgcproc runtime.needaddgcproc\n@@ -27,6 +28,10 @@ import (\n //go:linkname stoplockedm runtime.stoplockedm\n //go:linkname schedule runtime.schedule\n //go:linkname execute runtime.execute\n+//go:linkname gfput runtime.gfput\n+//go:linkname gfget runtime.gfget\n+//go:linkname lockOSThread runtime.lockOSThread\n+//go:linkname unlockOSThread runtime.unlockOSThread\n //go:linkname procresize runtime.procresize\n //go:linkname helpgc runtime.helpgc\n //go:linkname stopTheWorldWithSema runtime.stopTheWorldWithSema\n@@ -66,6 +71,113 @@ func goready(gp *g, traceskip int) {\n \t})\n }\n \n+//go:nosplit\n+func acquireSudog() *sudog {\n+\t// Delicate dance: the semaphore implementation calls\n+\t// acquireSudog, acquireSudog calls new(sudog),\n+\t// new calls malloc, malloc can call the garbage collector,\n+\t// and the garbage collector calls the semaphore implementation\n+\t// in stopTheWorld.\n+\t// Break the cycle by doing acquirem/releasem around new(sudog).\n+\t// The acquirem/releasem increments m.locks during new(sudog),\n+\t// which keeps the garbage collector from being invoked.\n+\tmp := acquirem()\n+\tpp := mp.p.ptr()\n+\tif len(pp.sudogcache) == 0 {\n+\t\tlock(&sched.sudoglock)\n+\t\t// First, try to grab a batch from central cache.\n+\t\tfor len(pp.sudogcache) < cap(pp.sudogcache)/2 && sched.sudogcache != nil {\n+\t\t\ts := sched.sudogcache\n+\t\t\tsched.sudogcache = s.next\n+\t\t\ts.next = nil\n+\t\t\tpp.sudogcache = append(pp.sudogcache, s)\n+\t\t}\n+\t\tunlock(&sched.sudoglock)\n+\t\t// If the central cache is empty, allocate a new one.\n+\t\tif len(pp.sudogcache) == 0 {\n+\t\t\tpp.sudogcache = append(pp.sudogcache, new(sudog))\n+\t\t}\n+\t}\n+\tn := len(pp.sudogcache)\n+\ts := pp.sudogcache[n-1]\n+\tpp.sudogcache[n-1] = nil\n+\tpp.sudogcache = pp.sudogcache[:n-1]\n+\tif s.elem != nil {\n+\t\tthrow(\"acquireSudog: found s.elem != nil in cache\")\n+\t}\n+\treleasem(mp)\n+\treturn s\n+}\n+\n+//go:nosplit\n+func releaseSudog(s *sudog) {\n+\tif s.elem != nil {\n+\t\tthrow(\"runtime: sudog with non-nil elem\")\n+\t}\n+\tif s.selectdone != nil {\n+\t\tthrow(\"runtime: sudog with non-nil selectdone\")\n+\t}\n+\tif s.next != nil {\n+\t\tthrow(\"runtime: sudog with non-nil next\")\n+\t}\n+\tif s.prev != nil {\n+\t\tthrow(\"runtime: sudog with non-nil prev\")\n+\t}\n+\tif s.waitlink != nil {\n+\t\tthrow(\"runtime: sudog with non-nil waitlink\")\n+\t}\n+\tif s.c != nil {\n+\t\tthrow(\"runtime: sudog with non-nil c\")\n+\t}\n+\tgp := getg()\n+\tif gp.param != nil {\n+\t\tthrow(\"runtime: releaseSudog with non-nil gp.param\")\n+\t}\n+\tmp := acquirem() // avoid rescheduling to another P\n+\tpp := mp.p.ptr()\n+\tif len(pp.sudogcache) == cap(pp.sudogcache) {\n+\t\t// Transfer half of local cache to the central cache.\n+\t\tvar first, last *sudog\n+\t\tfor len(pp.sudogcache) > cap(pp.sudogcache)/2 {\n+\t\t\tn := len(pp.sudogcache)\n+\t\t\tp := pp.sudogcache[n-1]\n+\t\t\tpp.sudogcache[n-1] = nil\n+\t\t\tpp.sudogcache = pp.sudogcache[:n-1]\n+\t\t\tif first == nil {\n+\t\t\t\tfirst = p\n+\t\t\t} else {\n+\t\t\t\tlast.next = p\n+\t\t\t}\n+\t\t\tlast = p\n+\t\t}\n+\t\tlock(&sched.sudoglock)\n+\t\tlast.next = sched.sudogcache\n+\t\tsched.sudogcache = first\n+\t\tunlock(&sched.sudoglock)\n+\t}\n+\tpp.sudogcache = append(pp.sudogcache, s)\n+\treleasem(mp)\n+}\n+\n+// funcPC returns the entry PC of the function f.\n+// It assumes that f is a func value. Otherwise the behavior is undefined.\n+// For gccgo here unless and until we port proc.go.\n+// Note that this differs from the gc implementation; the gc implementation\n+// adds sys.PtrSize to the address of the interface value, but GCC's\n+// alias analysis decides that that can not be a reference to the second\n+// field of the interface, and in some cases it drops the initialization\n+// of the second field as a dead store.\n+//go:nosplit\n+func funcPC(f interface{}) uintptr {\n+\ti := (*iface)(unsafe.Pointer(&f))\n+\treturn **(**uintptr)(i.data)\n+}\n+\n+func lockedOSThread() bool {\n+\tgp := getg()\n+\treturn gp.lockedm != nil && gp.m.lockedg != nil\n+}\n+\n var (\n \tallgs    []*g\n \tallglock mutex\n@@ -98,6 +210,43 @@ func dumpgstatus(gp *g) {\n \tprint(\"runtime:  g:  g=\", _g_, \", goid=\", _g_.goid, \",  g->atomicstatus=\", readgstatus(_g_), \"\\n\")\n }\n \n+func checkmcount() {\n+\t// sched lock is held\n+\tif sched.mcount > sched.maxmcount {\n+\t\tprint(\"runtime: program exceeds \", sched.maxmcount, \"-thread limit\\n\")\n+\t\tthrow(\"thread exhaustion\")\n+\t}\n+}\n+\n+func mcommoninit(mp *m) {\n+\t_g_ := getg()\n+\n+\t// g0 stack won't make sense for user (and is not necessary unwindable).\n+\tif _g_ != _g_.m.g0 {\n+\t\tcallers(1, mp.createstack[:])\n+\t}\n+\n+\tmp.fastrand = 0x49f6428a + uint32(mp.id) + uint32(cputicks())\n+\tif mp.fastrand == 0 {\n+\t\tmp.fastrand = 0x49f6428a\n+\t}\n+\n+\tlock(&sched.lock)\n+\tmp.id = sched.mcount\n+\tsched.mcount++\n+\tcheckmcount()\n+\tmpreinit(mp)\n+\n+\t// Add to allm so garbage collector doesn't free g->m\n+\t// when it is just in a register or thread-local storage.\n+\tmp.alllink = allm\n+\n+\t// NumCgoCall() iterates over allm w/o schedlock,\n+\t// so we need to publish it safely.\n+\tatomicstorep(unsafe.Pointer(&allm), unsafe.Pointer(mp))\n+\tunlock(&sched.lock)\n+}\n+\n // Mark gp ready to run.\n func ready(gp *g, traceskip int, next bool) {\n \tif trace.enabled {\n@@ -203,13 +352,77 @@ func freezetheworld() {\n \tusleep(1000)\n }\n \n+func isscanstatus(status uint32) bool {\n+\tif status == _Gscan {\n+\t\tthrow(\"isscanstatus: Bad status Gscan\")\n+\t}\n+\treturn status&_Gscan == _Gscan\n+}\n+\n // All reads and writes of g's status go through readgstatus, casgstatus\n // castogscanstatus, casfrom_Gscanstatus.\n //go:nosplit\n func readgstatus(gp *g) uint32 {\n \treturn atomic.Load(&gp.atomicstatus)\n }\n \n+// Ownership of gcscanvalid:\n+//\n+// If gp is running (meaning status == _Grunning or _Grunning|_Gscan),\n+// then gp owns gp.gcscanvalid, and other goroutines must not modify it.\n+//\n+// Otherwise, a second goroutine can lock the scan state by setting _Gscan\n+// in the status bit and then modify gcscanvalid, and then unlock the scan state.\n+//\n+// Note that the first condition implies an exception to the second:\n+// if a second goroutine changes gp's status to _Grunning|_Gscan,\n+// that second goroutine still does not have the right to modify gcscanvalid.\n+\n+// The Gscanstatuses are acting like locks and this releases them.\n+// If it proves to be a performance hit we should be able to make these\n+// simple atomic stores but for now we are going to throw if\n+// we see an inconsistent state.\n+func casfrom_Gscanstatus(gp *g, oldval, newval uint32) {\n+\tsuccess := false\n+\n+\t// Check that transition is valid.\n+\tswitch oldval {\n+\tdefault:\n+\t\tprint(\"runtime: casfrom_Gscanstatus bad oldval gp=\", gp, \", oldval=\", hex(oldval), \", newval=\", hex(newval), \"\\n\")\n+\t\tdumpgstatus(gp)\n+\t\tthrow(\"casfrom_Gscanstatus:top gp->status is not in scan state\")\n+\tcase _Gscanrunnable,\n+\t\t_Gscanwaiting,\n+\t\t_Gscanrunning,\n+\t\t_Gscansyscall:\n+\t\tif newval == oldval&^_Gscan {\n+\t\t\tsuccess = atomic.Cas(&gp.atomicstatus, oldval, newval)\n+\t\t}\n+\t}\n+\tif !success {\n+\t\tprint(\"runtime: casfrom_Gscanstatus failed gp=\", gp, \", oldval=\", hex(oldval), \", newval=\", hex(newval), \"\\n\")\n+\t\tdumpgstatus(gp)\n+\t\tthrow(\"casfrom_Gscanstatus: gp->status is not in scan state\")\n+\t}\n+}\n+\n+// This will return false if the gp is not in the expected status and the cas fails.\n+// This acts like a lock acquire while the casfromgstatus acts like a lock release.\n+func castogscanstatus(gp *g, oldval, newval uint32) bool {\n+\tswitch oldval {\n+\tcase _Grunnable,\n+\t\t_Grunning,\n+\t\t_Gwaiting,\n+\t\t_Gsyscall:\n+\t\tif newval == oldval|_Gscan {\n+\t\t\treturn atomic.Cas(&gp.atomicstatus, oldval, newval)\n+\t\t}\n+\t}\n+\tprint(\"runtime: castogscanstatus oldval=\", hex(oldval), \" newval=\", hex(newval), \"\\n\")\n+\tthrow(\"castogscanstatus\")\n+\tpanic(\"not reached\")\n+}\n+\n // If asked to move to or from a Gscanstatus this will throw. Use the castogscanstatus\n // and casfrom_Gscanstatus instead.\n // casgstatus will loop if the g->atomicstatus is in a Gscan status until the routine that\n@@ -453,6 +666,100 @@ func startTheWorldWithSema() {\n \t_g_.m.locks--\n }\n \n+// forEachP calls fn(p) for every P p when p reaches a GC safe point.\n+// If a P is currently executing code, this will bring the P to a GC\n+// safe point and execute fn on that P. If the P is not executing code\n+// (it is idle or in a syscall), this will call fn(p) directly while\n+// preventing the P from exiting its state. This does not ensure that\n+// fn will run on every CPU executing Go code, but it acts as a global\n+// memory barrier. GC uses this as a \"ragged barrier.\"\n+//\n+// The caller must hold worldsema.\n+//\n+//go:systemstack\n+func forEachP(fn func(*p)) {\n+\tmp := acquirem()\n+\t_p_ := getg().m.p.ptr()\n+\n+\tlock(&sched.lock)\n+\tif sched.safePointWait != 0 {\n+\t\tthrow(\"forEachP: sched.safePointWait != 0\")\n+\t}\n+\tsched.safePointWait = gomaxprocs - 1\n+\tsched.safePointFn = fn\n+\n+\t// Ask all Ps to run the safe point function.\n+\tfor _, p := range allp[:gomaxprocs] {\n+\t\tif p != _p_ {\n+\t\t\tatomic.Store(&p.runSafePointFn, 1)\n+\t\t}\n+\t}\n+\tpreemptall()\n+\n+\t// Any P entering _Pidle or _Psyscall from now on will observe\n+\t// p.runSafePointFn == 1 and will call runSafePointFn when\n+\t// changing its status to _Pidle/_Psyscall.\n+\n+\t// Run safe point function for all idle Ps. sched.pidle will\n+\t// not change because we hold sched.lock.\n+\tfor p := sched.pidle.ptr(); p != nil; p = p.link.ptr() {\n+\t\tif atomic.Cas(&p.runSafePointFn, 1, 0) {\n+\t\t\tfn(p)\n+\t\t\tsched.safePointWait--\n+\t\t}\n+\t}\n+\n+\twait := sched.safePointWait > 0\n+\tunlock(&sched.lock)\n+\n+\t// Run fn for the current P.\n+\tfn(_p_)\n+\n+\t// Force Ps currently in _Psyscall into _Pidle and hand them\n+\t// off to induce safe point function execution.\n+\tfor i := 0; i < int(gomaxprocs); i++ {\n+\t\tp := allp[i]\n+\t\ts := p.status\n+\t\tif s == _Psyscall && p.runSafePointFn == 1 && atomic.Cas(&p.status, s, _Pidle) {\n+\t\t\tif trace.enabled {\n+\t\t\t\ttraceGoSysBlock(p)\n+\t\t\t\ttraceProcStop(p)\n+\t\t\t}\n+\t\t\tp.syscalltick++\n+\t\t\thandoffp(p)\n+\t\t}\n+\t}\n+\n+\t// Wait for remaining Ps to run fn.\n+\tif wait {\n+\t\tfor {\n+\t\t\t// Wait for 100us, then try to re-preempt in\n+\t\t\t// case of any races.\n+\t\t\t//\n+\t\t\t// Requires system stack.\n+\t\t\tif notetsleep(&sched.safePointNote, 100*1000) {\n+\t\t\t\tnoteclear(&sched.safePointNote)\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tpreemptall()\n+\t\t}\n+\t}\n+\tif sched.safePointWait != 0 {\n+\t\tthrow(\"forEachP: not done\")\n+\t}\n+\tfor i := 0; i < int(gomaxprocs); i++ {\n+\t\tp := allp[i]\n+\t\tif p.runSafePointFn != 0 {\n+\t\t\tthrow(\"forEachP: P did not run fn\")\n+\t\t}\n+\t}\n+\n+\tlock(&sched.lock)\n+\tsched.safePointFn = nil\n+\tunlock(&sched.lock)\n+\treleasem(mp)\n+}\n+\n // runSafePointFn runs the safe point function, if any, for this P.\n // This should be called like\n //\n@@ -1245,6 +1552,108 @@ top:\n \texecute(gp, inheritTime)\n }\n \n+// dropg removes the association between m and the current goroutine m->curg (gp for short).\n+// Typically a caller sets gp's status away from Grunning and then\n+// immediately calls dropg to finish the job. The caller is also responsible\n+// for arranging that gp will be restarted using ready at an\n+// appropriate time. After calling dropg and arranging for gp to be\n+// readied later, the caller can do other work but eventually should\n+// call schedule to restart the scheduling of goroutines on this m.\n+func dropg() {\n+\t_g_ := getg()\n+\n+\t_g_.m.curg.m = nil\n+\t_g_.m.curg = nil\n+}\n+\n+func beforefork() {\n+\tgp := getg().m.curg\n+\n+\t// Fork can hang if preempted with signals frequently enough (see issue 5517).\n+\t// Ensure that we stay on the same M where we disable profiling.\n+\tgp.m.locks++\n+\tif gp.m.profilehz != 0 {\n+\t\tresetcpuprofiler(0)\n+\t}\n+}\n+\n+// Called from syscall package before fork.\n+//go:linkname syscall_runtime_BeforeFork syscall.runtime_BeforeFork\n+//go:nosplit\n+func syscall_runtime_BeforeFork() {\n+\tsystemstack(beforefork)\n+}\n+\n+func afterfork() {\n+\tgp := getg().m.curg\n+\n+\thz := sched.profilehz\n+\tif hz != 0 {\n+\t\tresetcpuprofiler(hz)\n+\t}\n+\tgp.m.locks--\n+}\n+\n+// Called from syscall package after fork in parent.\n+//go:linkname syscall_runtime_AfterFork syscall.runtime_AfterFork\n+//go:nosplit\n+func syscall_runtime_AfterFork() {\n+\tsystemstack(afterfork)\n+}\n+\n+// Put on gfree list.\n+// If local list is too long, transfer a batch to the global list.\n+func gfput(_p_ *p, gp *g) {\n+\tif readgstatus(gp) != _Gdead {\n+\t\tthrow(\"gfput: bad status (not Gdead)\")\n+\t}\n+\n+\tgp.schedlink.set(_p_.gfree)\n+\t_p_.gfree = gp\n+\t_p_.gfreecnt++\n+\tif _p_.gfreecnt >= 64 {\n+\t\tlock(&sched.gflock)\n+\t\tfor _p_.gfreecnt >= 32 {\n+\t\t\t_p_.gfreecnt--\n+\t\t\tgp = _p_.gfree\n+\t\t\t_p_.gfree = gp.schedlink.ptr()\n+\t\t\tgp.schedlink.set(sched.gfree)\n+\t\t\tsched.gfree = gp\n+\t\t\tsched.ngfree++\n+\t\t}\n+\t\tunlock(&sched.gflock)\n+\t}\n+}\n+\n+// Get from gfree list.\n+// If local list is empty, grab a batch from global list.\n+func gfget(_p_ *p) *g {\n+retry:\n+\tgp := _p_.gfree\n+\tif gp == nil && sched.gfree != nil {\n+\t\tlock(&sched.gflock)\n+\t\tfor _p_.gfreecnt < 32 {\n+\t\t\tif sched.gfree != nil {\n+\t\t\t\tgp = sched.gfree\n+\t\t\t\tsched.gfree = gp.schedlink.ptr()\n+\t\t\t} else {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\t_p_.gfreecnt++\n+\t\t\tsched.ngfree--\n+\t\t\tgp.schedlink.set(_p_.gfree)\n+\t\t\t_p_.gfree = gp\n+\t\t}\n+\t\tunlock(&sched.gflock)\n+\t\tgoto retry\n+\t}\n+\tif gp != nil {\n+\t\t_p_.gfree = gp.schedlink.ptr()\n+\t\t_p_.gfreecnt--\n+\t}\n+\treturn gp\n+}\n+\n // Purge all cached G's from gfree list to the global list.\n func gfpurge(_p_ *p) {\n \tlock(&sched.gflock)\n@@ -1259,6 +1668,90 @@ func gfpurge(_p_ *p) {\n \tunlock(&sched.gflock)\n }\n \n+// dolockOSThread is called by LockOSThread and lockOSThread below\n+// after they modify m.locked. Do not allow preemption during this call,\n+// or else the m might be different in this function than in the caller.\n+//go:nosplit\n+func dolockOSThread() {\n+\t_g_ := getg()\n+\t_g_.m.lockedg = _g_\n+\t_g_.lockedm = _g_.m\n+}\n+\n+//go:nosplit\n+\n+// LockOSThread wires the calling goroutine to its current operating system thread.\n+// Until the calling goroutine exits or calls UnlockOSThread, it will always\n+// execute in that thread, and no other goroutine can.\n+func LockOSThread() {\n+\tgetg().m.locked |= _LockExternal\n+\tdolockOSThread()\n+}\n+\n+//go:nosplit\n+func lockOSThread() {\n+\tgetg().m.locked += _LockInternal\n+\tdolockOSThread()\n+}\n+\n+// dounlockOSThread is called by UnlockOSThread and unlockOSThread below\n+// after they update m->locked. Do not allow preemption during this call,\n+// or else the m might be in different in this function than in the caller.\n+//go:nosplit\n+func dounlockOSThread() {\n+\t_g_ := getg()\n+\tif _g_.m.locked != 0 {\n+\t\treturn\n+\t}\n+\t_g_.m.lockedg = nil\n+\t_g_.lockedm = nil\n+}\n+\n+//go:nosplit\n+\n+// UnlockOSThread unwires the calling goroutine from its fixed operating system thread.\n+// If the calling goroutine has not called LockOSThread, UnlockOSThread is a no-op.\n+func UnlockOSThread() {\n+\tgetg().m.locked &^= _LockExternal\n+\tdounlockOSThread()\n+}\n+\n+//go:nosplit\n+func unlockOSThread() {\n+\t_g_ := getg()\n+\tif _g_.m.locked < _LockInternal {\n+\t\tsystemstack(badunlockosthread)\n+\t}\n+\t_g_.m.locked -= _LockInternal\n+\tdounlockOSThread()\n+}\n+\n+func badunlockosthread() {\n+\tthrow(\"runtime: internal error: misuse of lockOSThread/unlockOSThread\")\n+}\n+\n+func gcount() int32 {\n+\tn := int32(allglen) - sched.ngfree - int32(atomic.Load(&sched.ngsys))\n+\tfor i := 0; ; i++ {\n+\t\t_p_ := allp[i]\n+\t\tif _p_ == nil {\n+\t\t\tbreak\n+\t\t}\n+\t\tn -= _p_.gfreecnt\n+\t}\n+\n+\t// All these variables can be changed concurrently, so the result can be inconsistent.\n+\t// But at least the current goroutine is running.\n+\tif n < 1 {\n+\t\tn = 1\n+\t}\n+\treturn n\n+}\n+\n+func mcount() int32 {\n+\treturn sched.mcount\n+}\n+\n // Change number of processors. The world is stopped, sched is locked.\n // gcworkbufs are not being modified by either the GC or\n // the write barrier code.\n@@ -1513,23 +2006,21 @@ func checkdead() {\n \t// Maybe jump time forward for playground.\n \tgp := timejump()\n \tif gp != nil {\n-\t\t// Temporarily commented out for gccgo.\n-\t\t// For gccgo this code will never run anyhow.\n-\t\t// casgstatus(gp, _Gwaiting, _Grunnable)\n-\t\t// globrunqput(gp)\n-\t\t// _p_ := pidleget()\n-\t\t// if _p_ == nil {\n-\t\t// \tthrow(\"checkdead: no p for timer\")\n-\t\t// }\n-\t\t// mp := mget()\n-\t\t// if mp == nil {\n-\t\t// \t// There should always be a free M since\n-\t\t// \t// nothing is running.\n-\t\t// \tthrow(\"checkdead: no m for timer\")\n-\t\t// }\n-\t\t// nmp.nextp.set(_p_)\n-\t\t// notewakeup(&mp.park)\n-\t\t// return\n+\t\tcasgstatus(gp, _Gwaiting, _Grunnable)\n+\t\tglobrunqput(gp)\n+\t\t_p_ := pidleget()\n+\t\tif _p_ == nil {\n+\t\t\tthrow(\"checkdead: no p for timer\")\n+\t\t}\n+\t\tmp := mget()\n+\t\tif mp == nil {\n+\t\t\t// There should always be a free M since\n+\t\t\t// nothing is running.\n+\t\t\tthrow(\"checkdead: no m for timer\")\n+\t\t}\n+\t\tmp.nextp.set(_p_)\n+\t\tnotewakeup(&mp.park)\n+\t\treturn\n \t}\n \n \tgetg().m.throwing = -1 // do not dump full stacks\n@@ -1815,7 +2306,7 @@ func schedtrace(detailed bool) {\n \t\treturn\n \t}\n \n-\tfor mp := allm(); mp != nil; mp = mp.alllink {\n+\tfor mp := allm; mp != nil; mp = mp.alllink {\n \t\t_p_ := mp.p.ptr()\n \t\tgp := mp.curg\n \t\tlockedg := mp.lockedg\n@@ -2186,6 +2677,55 @@ func runqsteal(_p_, p2 *p, stealRunNextG bool) *g {\n \treturn gp\n }\n \n+//go:linkname setMaxThreads runtime_debug.setMaxThreads\n+func setMaxThreads(in int) (out int) {\n+\tlock(&sched.lock)\n+\tout = int(sched.maxmcount)\n+\tsched.maxmcount = int32(in)\n+\tcheckmcount()\n+\tunlock(&sched.lock)\n+\treturn\n+}\n+\n+//go:nosplit\n+func procPin() int {\n+\t_g_ := getg()\n+\tmp := _g_.m\n+\n+\tmp.locks++\n+\treturn int(mp.p.ptr().id)\n+}\n+\n+//go:nosplit\n+func procUnpin() {\n+\t_g_ := getg()\n+\t_g_.m.locks--\n+}\n+\n+//go:linkname sync_runtime_procPin sync.runtime_procPin\n+//go:nosplit\n+func sync_runtime_procPin() int {\n+\treturn procPin()\n+}\n+\n+//go:linkname sync_runtime_procUnpin sync.runtime_procUnpin\n+//go:nosplit\n+func sync_runtime_procUnpin() {\n+\tprocUnpin()\n+}\n+\n+//go:linkname sync_atomic_runtime_procPin sync_atomic.runtime_procPin\n+//go:nosplit\n+func sync_atomic_runtime_procPin() int {\n+\treturn procPin()\n+}\n+\n+//go:linkname sync_atomic_runtime_procUnpin sync_atomic.runtime_procUnpin\n+//go:nosplit\n+func sync_atomic_runtime_procUnpin() {\n+\tprocUnpin()\n+}\n+\n // Active spinning for sync.Mutex.\n //go:linkname sync_runtime_canSpin sync.runtime_canSpin\n //go:nosplit"}, {"sha": "7dc27433bae8206987c75d5e9ba69023c0fbcfd1", "filename": "libgo/go/runtime/runtime2.go", "status": "modified", "additions": 2, "deletions": 4, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fgo%2Fruntime%2Fruntime2.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fgo%2Fruntime%2Fruntime2.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fruntime2.go?ref=6fcb740ac456e4703b337584b26305a77a5c5038", "patch": "@@ -755,10 +755,8 @@ const _TracebackMaxFrames = 100\n var (\n \t//\temptystring string\n \n-\tallglen uintptr\n-\n-\t//\tallm        *m\n-\n+\tallglen    uintptr\n+\tallm       *m\n \tallp       [_MaxGomaxprocs + 1]*p\n \tgomaxprocs int32\n \tpanicking  uint32"}, {"sha": "a5f0470155e0af8ee9e5d92edaac40d6327f3cb2", "filename": "libgo/go/runtime/stubs.go", "status": "modified", "additions": 6, "deletions": 89, "changes": 95, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fgo%2Fruntime%2Fstubs.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fgo%2Fruntime%2Fstubs.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fstubs.go?ref=6fcb740ac456e4703b337584b26305a77a5c5038", "patch": "@@ -234,20 +234,6 @@ func newobject(*_type) unsafe.Pointer\n // For gccgo unless and until we port malloc.go.\n func newarray(*_type, int) unsafe.Pointer\n \n-// funcPC returns the entry PC of the function f.\n-// It assumes that f is a func value. Otherwise the behavior is undefined.\n-// For gccgo here unless and until we port proc.go.\n-// Note that this differs from the gc implementation; the gc implementation\n-// adds sys.PtrSize to the address of the interface value, but GCC's\n-// alias analysis decides that that can not be a reference to the second\n-// field of the interface, and in some cases it drops the initialization\n-// of the second field as a dead store.\n-//go:nosplit\n-func funcPC(f interface{}) uintptr {\n-\ti := (*iface)(unsafe.Pointer(&f))\n-\treturn **(**uintptr)(i.data)\n-}\n-\n // For gccgo, to communicate from the C code to the Go code.\n //go:linkname setIsCgo runtime.setIsCgo\n func setIsCgo() {\n@@ -352,56 +338,6 @@ func exitsyscall(int32)\n func gopark(func(*g, unsafe.Pointer) bool, unsafe.Pointer, string, byte, int)\n func goparkunlock(*mutex, string, byte, int)\n \n-// Temporary hack for gccgo until we port proc.go.\n-//go:nosplit\n-func acquireSudog() *sudog {\n-\tmp := acquirem()\n-\tpp := mp.p.ptr()\n-\tif len(pp.sudogcache) == 0 {\n-\t\tpp.sudogcache = append(pp.sudogcache, new(sudog))\n-\t}\n-\tn := len(pp.sudogcache)\n-\ts := pp.sudogcache[n-1]\n-\tpp.sudogcache[n-1] = nil\n-\tpp.sudogcache = pp.sudogcache[:n-1]\n-\tif s.elem != nil {\n-\t\tthrow(\"acquireSudog: found s.elem != nil in cache\")\n-\t}\n-\treleasem(mp)\n-\treturn s\n-}\n-\n-// Temporary hack for gccgo until we port proc.go.\n-//go:nosplit\n-func releaseSudog(s *sudog) {\n-\tif s.elem != nil {\n-\t\tthrow(\"runtime: sudog with non-nil elem\")\n-\t}\n-\tif s.selectdone != nil {\n-\t\tthrow(\"runtime: sudog with non-nil selectdone\")\n-\t}\n-\tif s.next != nil {\n-\t\tthrow(\"runtime: sudog with non-nil next\")\n-\t}\n-\tif s.prev != nil {\n-\t\tthrow(\"runtime: sudog with non-nil prev\")\n-\t}\n-\tif s.waitlink != nil {\n-\t\tthrow(\"runtime: sudog with non-nil waitlink\")\n-\t}\n-\tif s.c != nil {\n-\t\tthrow(\"runtime: sudog with non-nil c\")\n-\t}\n-\tgp := getg()\n-\tif gp.param != nil {\n-\t\tthrow(\"runtime: releaseSudog with non-nil gp.param\")\n-\t}\n-\tmp := acquirem() // avoid rescheduling to another P\n-\tpp := mp.p.ptr()\n-\tpp.sudogcache = append(pp.sudogcache, s)\n-\treleasem(mp)\n-}\n-\n // Temporary hack for gccgo until we port the garbage collector.\n func typeBitsBulkBarrier(typ *_type, p, size uintptr) {}\n \n@@ -450,7 +386,6 @@ func LockOSThread()\n func UnlockOSThread()\n func lockOSThread()\n func unlockOSThread()\n-func allm() *m\n \n // Temporary for gccgo until we port malloc.go\n func persistentalloc(size, align uintptr, sysStat *uint64) unsafe.Pointer\n@@ -466,14 +401,6 @@ func setGCPercent(in int32) (out int32) {\n \treturn setgcpercent(in)\n }\n \n-// Temporary for gccgo until we port proc.go.\n-func setmaxthreads(int) int\n-\n-//go:linkname setMaxThreads runtime_debug.setMaxThreads\n-func setMaxThreads(in int) (out int) {\n-\treturn setmaxthreads(in)\n-}\n-\n // Temporary for gccgo until we port atomic_pointer.go.\n //go:nosplit\n func atomicstorep(ptr unsafe.Pointer, new unsafe.Pointer) {\n@@ -495,7 +422,6 @@ func getZerobase() *uintptr {\n \n // Temporary for gccgo until we port proc.go.\n func sigprof()\n-func mcount() int32\n func goexit1()\n \n // Get signal trampoline, written in C.\n@@ -549,6 +475,12 @@ func getallg(i int) *g {\n \treturn allgs[i]\n }\n \n+// Temporary for gccgo until we port the garbage collector.\n+//go:linkname getallm runtime.getallm\n+func getallm() *m {\n+\treturn allm\n+}\n+\n // Throw and rethrow an exception.\n func throwException()\n func rethrowException()\n@@ -577,21 +509,6 @@ var work struct {\n \t}\n }\n \n-// gcount is temporary for gccgo until more of proc.go is ported.\n-// This is a copy of the C function we used to use.\n-func gcount() int32 {\n-\tn := int32(0)\n-\tlock(&allglock)\n-\tfor _, gp := range allgs {\n-\t\ts := readgstatus(gp)\n-\t\tif s == _Grunnable || s == _Grunning || s == _Gsyscall || s == _Gwaiting {\n-\t\t\tn++\n-\t\t}\n-\t}\n-\tunlock(&allglock)\n-\treturn n\n-}\n-\n // Temporary for gccgo until we port mgc.go.\n var gcBlackenEnabled uint32\n "}, {"sha": "4c673f418252219602ac9d6f067a272ffad10928", "filename": "libgo/runtime/heapdump.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fruntime%2Fheapdump.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fruntime%2Fheapdump.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fheapdump.c?ref=6fcb740ac456e4703b337584b26305a77a5c5038", "patch": "@@ -474,7 +474,7 @@ dumpms(void)\n {\n \tM *mp;\n \n-\tfor(mp = runtime_allm; mp != nil; mp = mp->alllink) {\n+\tfor(mp = runtime_getallm(); mp != nil; mp = mp->alllink) {\n \t\tdumpint(TagOSThread);\n \t\tdumpint((uintptr)mp);\n \t\tdumpint(mp->id);"}, {"sha": "1fc86dc1be0401cb2e48c0d2518c237c43ccd212", "filename": "libgo/runtime/mgc0.c", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fruntime%2Fmgc0.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fruntime%2Fmgc0.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmgc0.c?ref=6fcb740ac456e4703b337584b26305a77a5c5038", "patch": "@@ -1279,7 +1279,6 @@ markroot(ParFor *desc, uint32 i)\n \t\t// For gccgo we use this for all the other global roots.\n \t\tenqueue1(&wbuf, (Obj){(byte*)&runtime_m0, sizeof runtime_m0, 0});\n \t\tenqueue1(&wbuf, (Obj){(byte*)&runtime_g0, sizeof runtime_g0, 0});\n-\t\tenqueue1(&wbuf, (Obj){(byte*)&runtime_allm, sizeof runtime_allm, 0});\n \t\tenqueue1(&wbuf, (Obj){(byte*)&runtime_allp, sizeof runtime_allp, 0});\n \t\tenqueue1(&wbuf, (Obj){(byte*)&work, sizeof work, 0});\n \t\tbreak;\n@@ -2002,7 +2001,7 @@ runtime_updatememstats(GCStats *stats)\n \tif(stats)\n \t\truntime_memclr((byte*)stats, sizeof(*stats));\n \tstacks_inuse = 0;\n-\tfor(mp=runtime_allm; mp; mp=mp->alllink) {\n+\tfor(mp=runtime_getallm(); mp; mp=mp->alllink) {\n \t\t//stacks_inuse += mp->stackinuse*FixedStack;\n \t\tif(stats) {\n \t\t\tsrc = (uint64*)&mp->gcstats;"}, {"sha": "90ff352e7c6a156aa615d89782dfd2f36f19276c", "filename": "libgo/runtime/proc.c", "status": "modified", "additions": 6, "deletions": 258, "changes": 264, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fruntime%2Fproc.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fruntime%2Fproc.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fproc.c?ref=6fcb740ac456e4703b337584b26305a77a5c5038", "patch": "@@ -376,7 +376,6 @@ Sched*\truntime_sched;\n M\truntime_m0;\n G\truntime_g0;\t// idle goroutine for m0\n G*\truntime_lastg;\n-M*\truntime_allm;\n P**\truntime_allp;\n int8*\truntime_goos;\n int32\truntime_ncpu;\n@@ -385,18 +384,17 @@ bool\truntime_precisestack;\n bool\truntime_isarchive;\n \n void* runtime_mstart(void*);\n-static void mcommoninit(M*);\n static void exitsyscall0(G*);\n static void park0(G*);\n static void goexit0(G*);\n-static void gfput(P*, G*);\n-static G* gfget(P*);\n static bool exitsyscallfast(void);\n \n extern void setncpu(int32)\n   __asm__(GOSYM_PREFIX \"runtime.setncpu\");\n extern void allgadd(G*)\n   __asm__(GOSYM_PREFIX \"runtime.allgadd\");\n+extern void mcommoninit(M*)\n+  __asm__(GOSYM_PREFIX \"runtime.mcommoninit\");\n extern void stopm(void)\n   __asm__(GOSYM_PREFIX \"runtime.stopm\");\n extern void handoffp(P*)\n@@ -409,6 +407,10 @@ extern void schedule(void)\n   __asm__(GOSYM_PREFIX \"runtime.schedule\");\n extern void execute(G*, bool)\n   __asm__(GOSYM_PREFIX \"runtime.execute\");\n+extern void gfput(P*, G*)\n+  __asm__(GOSYM_PREFIX \"runtime.gfput\");\n+extern G* gfget(P*)\n+  __asm__(GOSYM_PREFIX \"runtime.gfget\");\n extern void procresize(int32)\n   __asm__(GOSYM_PREFIX \"runtime.procresize\");\n extern void acquirep(P*)\n@@ -620,16 +622,6 @@ void getTraceback(G* me, G* gp)\n \t}\n }\n \n-static void\n-checkmcount(void)\n-{\n-\t// sched lock is held\n-\tif(runtime_sched->mcount > runtime_sched->maxmcount) {\n-\t\truntime_printf(\"runtime: program exceeds %d-thread limit\\n\", runtime_sched->maxmcount);\n-\t\truntime_throw(\"thread exhaustion\");\n-\t}\n-}\n-\n // Do a stack trace of gp, and then restore the context to\n // gp->dotraceback.\n \n@@ -649,30 +641,6 @@ gtraceback(G* gp)\n \truntime_gogo(traceback->gp);\n }\n \n-static void\n-mcommoninit(M *mp)\n-{\n-\t// If there is no mcache runtime_callers() will crash,\n-\t// and we are most likely in sysmon thread so the stack is senseless anyway.\n-\tif(g->m->mcache)\n-\t\truntime_callers(1, mp->createstack, nelem(mp->createstack), false);\n-\n-\tmp->fastrand = 0x49f6428aUL + mp->id + runtime_cputicks();\n-\n-\truntime_lock(&runtime_sched->lock);\n-\tmp->id = runtime_sched->mcount++;\n-\tcheckmcount();\n-\truntime_mpreinit(mp);\n-\n-\t// Add to runtime_allm so garbage collector doesn't free m\n-\t// when it is just in a register or thread-local storage.\n-\tmp->alllink = runtime_allm;\n-\t// runtime_NumCgoCall() iterates over allm w/o schedlock,\n-\t// so we need to publish it safely.\n-\truntime_atomicstorep(&runtime_allm, mp);\n-\truntime_unlock(&runtime_sched->lock);\n-}\n-\n // Called to start an M.\n void*\n runtime_mstart(void* mp)\n@@ -1332,33 +1300,6 @@ syscall_exitsyscall()\n   runtime_exitsyscall(0);\n }\n \n-// Called from syscall package before fork.\n-void syscall_runtime_BeforeFork(void)\n-  __asm__(GOSYM_PREFIX \"syscall.runtime_BeforeFork\");\n-void\n-syscall_runtime_BeforeFork(void)\n-{\n-\t// Fork can hang if preempted with signals frequently enough (see issue 5517).\n-\t// Ensure that we stay on the same M where we disable profiling.\n-\truntime_m()->locks++;\n-\tif(runtime_m()->profilehz != 0)\n-\t\truntime_resetcpuprofiler(0);\n-}\n-\n-// Called from syscall package after fork in parent.\n-void syscall_runtime_AfterFork(void)\n-  __asm__(GOSYM_PREFIX \"syscall.runtime_AfterFork\");\n-void\n-syscall_runtime_AfterFork(void)\n-{\n-\tint32 hz;\n-\n-\thz = runtime_sched->profilehz;\n-\tif(hz != 0)\n-\t\truntime_resetcpuprofiler(hz);\n-\truntime_m()->locks--;\n-}\n-\n // Allocate a new g, with a stack big enough for stacksize bytes.\n G*\n runtime_malg(bool allocatestack, bool signalstack, byte** ret_stack, uintptr* ret_stacksize)\n@@ -1480,55 +1421,6 @@ __go_go(void (*fn)(void*), void* arg)\n \treturn newg;\n }\n \n-// Put on gfree list.\n-// If local list is too long, transfer a batch to the global list.\n-static void\n-gfput(P *p, G *gp)\n-{\n-\tgp->schedlink = (uintptr)p->gfree;\n-\tp->gfree = gp;\n-\tp->gfreecnt++;\n-\tif(p->gfreecnt >= 64) {\n-\t\truntime_lock(&runtime_sched->gflock);\n-\t\twhile(p->gfreecnt >= 32) {\n-\t\t\tp->gfreecnt--;\n-\t\t\tgp = p->gfree;\n-\t\t\tp->gfree = (G*)gp->schedlink;\n-\t\t\tgp->schedlink = (uintptr)runtime_sched->gfree;\n-\t\t\truntime_sched->gfree = gp;\n-\t\t}\n-\t\truntime_unlock(&runtime_sched->gflock);\n-\t}\n-}\n-\n-// Get from gfree list.\n-// If local list is empty, grab a batch from global list.\n-static G*\n-gfget(P *p)\n-{\n-\tG *gp;\n-\n-retry:\n-\tgp = p->gfree;\n-\tif(gp == nil && runtime_sched->gfree) {\n-\t\truntime_lock(&runtime_sched->gflock);\n-\t\twhile(p->gfreecnt < 32 && runtime_sched->gfree) {\n-\t\t\tp->gfreecnt++;\n-\t\t\tgp = runtime_sched->gfree;\n-\t\t\truntime_sched->gfree = (G*)gp->schedlink;\n-\t\t\tgp->schedlink = (uintptr)p->gfree;\n-\t\t\tp->gfree = gp;\n-\t\t}\n-\t\truntime_unlock(&runtime_sched->gflock);\n-\t\tgoto retry;\n-\t}\n-\tif(gp) {\n-\t\tp->gfree = (G*)gp->schedlink;\n-\t\tp->gfreecnt--;\n-\t}\n-\treturn gp;\n-}\n-\n void\n runtime_Breakpoint(void)\n {\n@@ -1543,74 +1435,6 @@ runtime_Gosched(void)\n \truntime_gosched();\n }\n \n-// lockOSThread is called by runtime.LockOSThread and runtime.lockOSThread below\n-// after they modify m->locked. Do not allow preemption during this call,\n-// or else the m might be different in this function than in the caller.\n-static void\n-lockOSThread(void)\n-{\n-\tg->m->lockedg = g;\n-\tg->lockedm = g->m;\n-}\n-\n-void\truntime_LockOSThread(void) __asm__ (GOSYM_PREFIX \"runtime.LockOSThread\");\n-void\n-runtime_LockOSThread(void)\n-{\n-\tg->m->locked |= _LockExternal;\n-\tlockOSThread();\n-}\n-\n-void\n-runtime_lockOSThread(void)\n-{\n-\tg->m->locked += _LockInternal;\n-\tlockOSThread();\n-}\n-\n-\n-// unlockOSThread is called by runtime.UnlockOSThread and runtime.unlockOSThread below\n-// after they update m->locked. Do not allow preemption during this call,\n-// or else the m might be in different in this function than in the caller.\n-static void\n-unlockOSThread(void)\n-{\n-\tif(g->m->locked != 0)\n-\t\treturn;\n-\tg->m->lockedg = nil;\n-\tg->lockedm = nil;\n-}\n-\n-void\truntime_UnlockOSThread(void) __asm__ (GOSYM_PREFIX \"runtime.UnlockOSThread\");\n-\n-void\n-runtime_UnlockOSThread(void)\n-{\n-\tg->m->locked &= ~_LockExternal;\n-\tunlockOSThread();\n-}\n-\n-void\n-runtime_unlockOSThread(void)\n-{\n-\tif(g->m->locked < _LockInternal)\n-\t\truntime_throw(\"runtime: internal error: misuse of lockOSThread/unlockOSThread\");\n-\tg->m->locked -= _LockInternal;\n-\tunlockOSThread();\n-}\n-\n-bool\n-runtime_lockedOSThread(void)\n-{\n-\treturn g->lockedm != nil && g->m->lockedg != nil;\n-}\n-\n-int32\n-runtime_mcount(void)\n-{\n-\treturn runtime_sched->mcount;\n-}\n-\n static struct {\n \tuint32 lock;\n \tint32 hz;\n@@ -1719,71 +1543,6 @@ runtime_setcpuprofilerate_m(int32 hz)\n \tg->m->locks--;\n }\n \n-intgo\n-runtime_setmaxthreads(intgo in)\n-{\n-\tintgo out;\n-\n-\truntime_lock(&runtime_sched->lock);\n-\tout = (intgo)runtime_sched->maxmcount;\n-\truntime_sched->maxmcount = (int32)in;\n-\tcheckmcount();\n-\truntime_unlock(&runtime_sched->lock);\n-\treturn out;\n-}\n-\n-static intgo\n-procPin()\n-{\n-\tM *mp;\n-\n-\tmp = runtime_m();\n-\tmp->locks++;\n-\treturn (intgo)(((P*)mp->p)->id);\n-}\n-\n-static void\n-procUnpin()\n-{\n-\truntime_m()->locks--;\n-}\n-\n-intgo sync_runtime_procPin(void)\n-  __asm__ (GOSYM_PREFIX \"sync.runtime_procPin\");\n-\n-intgo\n-sync_runtime_procPin()\n-{\n-\treturn procPin();\n-}\n-\n-void sync_runtime_procUnpin(void)\n-  __asm__ (GOSYM_PREFIX  \"sync.runtime_procUnpin\");\n-\n-void\n-sync_runtime_procUnpin()\n-{\n-\tprocUnpin();\n-}\n-\n-intgo sync_atomic_runtime_procPin(void)\n-  __asm__ (GOSYM_PREFIX \"sync_atomic.runtime_procPin\");\n-\n-intgo\n-sync_atomic_runtime_procPin()\n-{\n-\treturn procPin();\n-}\n-\n-void sync_atomic_runtime_procUnpin(void)\n-  __asm__ (GOSYM_PREFIX  \"sync_atomic.runtime_procUnpin\");\n-\n-void\n-sync_atomic_runtime_procUnpin()\n-{\n-\tprocUnpin();\n-}\n-\n // Return whether we are waiting for a GC.  This gc toolchain uses\n // preemption instead.\n bool\n@@ -1802,17 +1561,6 @@ os_beforeExit()\n {\n }\n \n-// For Go code to look at variables, until we port proc.go.\n-\n-extern M* runtime_go_allm(void)\n-  __asm__ (GOSYM_PREFIX \"runtime.allm\");\n-\n-M*\n-runtime_go_allm()\n-{\n-\treturn runtime_allm;\n-}\n-\n intgo NumCPU(void) __asm__ (GOSYM_PREFIX \"runtime.NumCPU\");\n \n intgo"}, {"sha": "57d2b964595cc6f39f889f49829d4f7b57b896db", "filename": "libgo/runtime/runtime.h", "status": "modified", "additions": 2, "deletions": 8, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fruntime%2Fruntime.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fruntime%2Fruntime.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime.h?ref=6fcb740ac456e4703b337584b26305a77a5c5038", "patch": "@@ -237,7 +237,8 @@ extern G* runtime_getallg(intgo)\n extern uintptr runtime_getallglen(void)\n   __asm__(GOSYM_PREFIX \"runtime.getallglen\");\n extern\tG*\truntime_lastg;\n-extern\tM*\truntime_allm;\n+extern\tM*\truntime_getallm(void)\n+  __asm__(GOSYM_PREFIX \"runtime.getallm\");\n extern\tP**\truntime_allp;\n extern\tSched*  runtime_sched;\n extern\tuint32\truntime_panicking(void)\n@@ -301,7 +302,6 @@ int32\truntime_atoi(const byte*, intgo);\n void*\truntime_mstart(void*);\n G*\truntime_malg(bool, bool, byte**, uintptr*)\n \t__asm__(GOSYM_PREFIX \"runtime.malg\");\n-void\truntime_mpreinit(M*);\n void\truntime_minit(void)\n   __asm__ (GOSYM_PREFIX \"runtime.minit\");\n void\truntime_signalstack(byte*, uintptr)\n@@ -313,8 +313,6 @@ void\truntime_freemcache(MCache*)\n void\truntime_mallocinit(void);\n void\truntime_mprofinit(void);\n #define runtime_getcallersp(p) __builtin_frame_address(0)\n-int32\truntime_mcount(void)\n-  __asm__ (GOSYM_PREFIX \"runtime.mcount\");\n void\truntime_mcall(void(*)(G*));\n uint32\truntime_fastrand1(void) __asm__ (GOSYM_PREFIX \"runtime.fastrand1\");\n int32\truntime_timediv(int64, int32, int32*)\n@@ -394,8 +392,6 @@ void\truntime_crash(void)\n void\truntime_parsedebugvars(void)\n   __asm__(GOSYM_PREFIX \"runtime.parsedebugvars\");\n void\t_rt0_go(void);\n-intgo\truntime_setmaxthreads(intgo)\n-  __asm__ (GOSYM_PREFIX \"runtime.setmaxthreads\");\n G*\truntime_timejump(void);\n void\truntime_iterate_finq(void (*callback)(FuncVal*, void*, const FuncType*, const PtrType*));\n \n@@ -522,8 +518,6 @@ void\truntime_lockOSThread(void)\n   __asm__(GOSYM_PREFIX \"runtime.lockOSThread\");\n void\truntime_unlockOSThread(void)\n   __asm__(GOSYM_PREFIX \"runtime.unlockOSThread\");\n-bool\truntime_lockedOSThread(void)\n-  __asm__(GOSYM_PREFIX \"runtime.lockedOSThread\");\n \n void\truntime_printcreatedby(G*)\n   __asm__(GOSYM_PREFIX \"runtime.printcreatedby\");"}, {"sha": "13588047d729af88486c5ccc903c98e630da6b7c", "filename": "libgo/runtime/runtime_c.c", "status": "modified", "additions": 0, "deletions": 9, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fruntime%2Fruntime_c.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6fcb740ac456e4703b337584b26305a77a5c5038/libgo%2Fruntime%2Fruntime_c.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime_c.c?ref=6fcb740ac456e4703b337584b26305a77a5c5038", "patch": "@@ -95,15 +95,6 @@ runtime_cputicks(void)\n #endif\n }\n \n-// Called to initialize a new m (including the bootstrap m).\n-// Called on the parent thread (main thread in case of bootstrap), can allocate memory.\n-void\n-runtime_mpreinit(M *mp)\n-{\n-\tmp->gsignal = runtime_malg(true, true, (byte**)&mp->gsignalstack, &mp->gsignalstacksize);\n-\tmp->gsignal->m = mp;\n-}\n-\n void\n runtime_signalstack(byte *p, uintptr n)\n {"}]}