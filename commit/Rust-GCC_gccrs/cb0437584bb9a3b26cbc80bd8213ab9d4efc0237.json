{"sha": "cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "node_id": "C_kwDOANBUbNoAKGNiMDQzNzU4NGJiOWEzYjI2Y2JjODBiZDgyMTNhYjlkNGVmYzAyMzc", "commit": {"author": {"name": "Martin Liska", "email": "mliska@suse.cz", "date": "2021-11-04T08:20:14Z"}, "committer": {"name": "Martin Liska", "email": "mliska@suse.cz", "date": "2021-11-04T12:24:53Z"}, "message": "libsanitizer: merge from master (c86b4503a94c277534ce4b9a5c015a6ac151b98a).", "tree": {"sha": "3992b2ac865181ba283a45643ee58e5a5d95f241", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/3992b2ac865181ba283a45643ee58e5a5d95f241"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "html_url": "https://github.com/Rust-GCC/gccrs/commit/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/comments", "author": {"login": "marxin", "id": 2658545, "node_id": "MDQ6VXNlcjI2NTg1NDU=", "avatar_url": "https://avatars.githubusercontent.com/u/2658545?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marxin", "html_url": "https://github.com/marxin", "followers_url": "https://api.github.com/users/marxin/followers", "following_url": "https://api.github.com/users/marxin/following{/other_user}", "gists_url": "https://api.github.com/users/marxin/gists{/gist_id}", "starred_url": "https://api.github.com/users/marxin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marxin/subscriptions", "organizations_url": "https://api.github.com/users/marxin/orgs", "repos_url": "https://api.github.com/users/marxin/repos", "events_url": "https://api.github.com/users/marxin/events{/privacy}", "received_events_url": "https://api.github.com/users/marxin/received_events", "type": "User", "site_admin": false}, "committer": {"login": "marxin", "id": 2658545, "node_id": "MDQ6VXNlcjI2NTg1NDU=", "avatar_url": "https://avatars.githubusercontent.com/u/2658545?v=4", "gravatar_id": "", "url": "https://api.github.com/users/marxin", "html_url": "https://github.com/marxin", "followers_url": "https://api.github.com/users/marxin/followers", "following_url": "https://api.github.com/users/marxin/following{/other_user}", "gists_url": "https://api.github.com/users/marxin/gists{/gist_id}", "starred_url": "https://api.github.com/users/marxin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/marxin/subscriptions", "organizations_url": "https://api.github.com/users/marxin/orgs", "repos_url": "https://api.github.com/users/marxin/repos", "events_url": "https://api.github.com/users/marxin/events{/privacy}", "received_events_url": "https://api.github.com/users/marxin/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "bb27f5e9ec3c7ab0f5c859d90c59dd4573b53d97", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/bb27f5e9ec3c7ab0f5c859d90c59dd4573b53d97", "html_url": "https://github.com/Rust-GCC/gccrs/commit/bb27f5e9ec3c7ab0f5c859d90c59dd4573b53d97"}], "stats": {"total": 2757, "additions": 1457, "deletions": 1300}, "files": [{"sha": "ed4b01332fa9bf166c325c4ac2136e7c09f97e0e", "filename": "libsanitizer/MERGE", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2FMERGE", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2FMERGE", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2FMERGE?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -1,4 +1,4 @@\n-fdf4c035225de52f596899931b1f6100e5e3e928\n+c86b4503a94c277534ce4b9a5c015a6ac151b98a\n \n The first line of this file holds the git revision number of the\n last merge done from the master library sources."}, {"sha": "6d7073710bd1936f0a34b85d2b5bc89a5f36c90a", "filename": "libsanitizer/asan/asan_allocator.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_allocator.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_allocator.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_allocator.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -522,7 +522,7 @@ struct Allocator {\n         size > max_user_defined_malloc_size) {\n       if (AllocatorMayReturnNull()) {\n         Report(\"WARNING: AddressSanitizer failed to allocate 0x%zx bytes\\n\",\n-               (void*)size);\n+               size);\n         return nullptr;\n       }\n       uptr malloc_limit ="}, {"sha": "d7d961685793d0a45684f94b7d44ffd6b7706cc3", "filename": "libsanitizer/asan/asan_descriptions.cpp", "status": "modified", "additions": 5, "deletions": 4, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_descriptions.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_descriptions.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_descriptions.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -251,7 +251,7 @@ static void PrintAccessAndVarIntersection(const StackVarDescr &var, uptr addr,\n   }\n   str.append(\"'\");\n   if (var.line > 0) {\n-    str.append(\" (line %d)\", var.line);\n+    str.append(\" (line %zd)\", var.line);\n   }\n   if (pos_descr) {\n     Decorator d;\n@@ -318,7 +318,8 @@ bool DescribeAddressIfGlobal(uptr addr, uptr access_size,\n }\n \n void ShadowAddressDescription::Print() const {\n-  Printf(\"Address %p is located in the %s area.\\n\", addr, ShadowNames[kind]);\n+  Printf(\"Address %p is located in the %s area.\\n\", (void *)addr,\n+         ShadowNames[kind]);\n }\n \n void GlobalAddressDescription::Print(const char *bug_type) const {\n@@ -356,7 +357,7 @@ bool GlobalAddressDescription::PointsInsideTheSameVariable(\n void StackAddressDescription::Print() const {\n   Decorator d;\n   Printf(\"%s\", d.Location());\n-  Printf(\"Address %p is located in stack of thread %s\", addr,\n+  Printf(\"Address %p is located in stack of thread %s\", (void *)addr,\n          AsanThreadIdAndName(tid).c_str());\n \n   if (!frame_descr) {\n@@ -469,7 +470,7 @@ AddressDescription::AddressDescription(uptr addr, uptr access_size,\n \n void WildAddressDescription::Print() const {\n   Printf(\"Address %p is a wild pointer inside of access range of size %p.\\n\",\n-         addr, access_size);\n+         (void *)addr, (void *)access_size);\n }\n \n void PrintAddressDescription(uptr addr, uptr access_size,"}, {"sha": "686ca7cc2b0e6446d95761a68d122f8942877392", "filename": "libsanitizer/asan/asan_errors.cpp", "status": "modified", "additions": 20, "deletions": 20, "changes": 40, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_errors.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_errors.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_errors.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -46,10 +46,9 @@ void ErrorDeadlySignal::Print() {\n void ErrorDoubleFree::Print() {\n   Decorator d;\n   Printf(\"%s\", d.Error());\n-  Report(\n-      \"ERROR: AddressSanitizer: attempting %s on %p in thread %s:\\n\",\n-      scariness.GetDescription(), addr_description.addr,\n-      AsanThreadIdAndName(tid).c_str());\n+  Report(\"ERROR: AddressSanitizer: attempting %s on %p in thread %s:\\n\",\n+         scariness.GetDescription(), (void *)addr_description.addr,\n+         AsanThreadIdAndName(tid).c_str());\n   Printf(\"%s\", d.Default());\n   scariness.Print();\n   GET_STACK_TRACE_FATAL(second_free_stack->trace[0],\n@@ -62,10 +61,9 @@ void ErrorDoubleFree::Print() {\n void ErrorNewDeleteTypeMismatch::Print() {\n   Decorator d;\n   Printf(\"%s\", d.Error());\n-  Report(\n-      \"ERROR: AddressSanitizer: %s on %p in thread %s:\\n\",\n-      scariness.GetDescription(), addr_description.addr,\n-      AsanThreadIdAndName(tid).c_str());\n+  Report(\"ERROR: AddressSanitizer: %s on %p in thread %s:\\n\",\n+         scariness.GetDescription(), (void *)addr_description.addr,\n+         AsanThreadIdAndName(tid).c_str());\n   Printf(\"%s  object passed to delete has wrong type:\\n\", d.Default());\n   if (delete_size != 0) {\n     Printf(\n@@ -106,7 +104,7 @@ void ErrorFreeNotMalloced::Print() {\n   Report(\n       \"ERROR: AddressSanitizer: attempting free on address \"\n       \"which was not malloc()-ed: %p in thread %s\\n\",\n-      addr_description.Address(), AsanThreadIdAndName(tid).c_str());\n+      (void *)addr_description.Address(), AsanThreadIdAndName(tid).c_str());\n   Printf(\"%s\", d.Default());\n   CHECK_GT(free_stack->size, 0);\n   scariness.Print();\n@@ -126,7 +124,7 @@ void ErrorAllocTypeMismatch::Print() {\n   Printf(\"%s\", d.Error());\n   Report(\"ERROR: AddressSanitizer: %s (%s vs %s) on %p\\n\",\n          scariness.GetDescription(), alloc_names[alloc_type],\n-         dealloc_names[dealloc_type], addr_description.Address());\n+         dealloc_names[dealloc_type], (void *)addr_description.Address());\n   Printf(\"%s\", d.Default());\n   CHECK_GT(dealloc_stack->size, 0);\n   scariness.Print();\n@@ -145,7 +143,7 @@ void ErrorMallocUsableSizeNotOwned::Print() {\n   Report(\n       \"ERROR: AddressSanitizer: attempting to call malloc_usable_size() for \"\n       \"pointer which is not owned: %p\\n\",\n-      addr_description.Address());\n+      (void *)addr_description.Address());\n   Printf(\"%s\", d.Default());\n   stack->Print();\n   addr_description.Print();\n@@ -158,7 +156,7 @@ void ErrorSanitizerGetAllocatedSizeNotOwned::Print() {\n   Report(\n       \"ERROR: AddressSanitizer: attempting to call \"\n       \"__sanitizer_get_allocated_size() for pointer which is not owned: %p\\n\",\n-      addr_description.Address());\n+      (void *)addr_description.Address());\n   Printf(\"%s\", d.Default());\n   stack->Print();\n   addr_description.Print();\n@@ -298,9 +296,10 @@ void ErrorStringFunctionMemoryRangesOverlap::Print() {\n   Report(\n       \"ERROR: AddressSanitizer: %s: memory ranges [%p,%p) and [%p, %p) \"\n       \"overlap\\n\",\n-      bug_type, addr1_description.Address(),\n-      addr1_description.Address() + length1, addr2_description.Address(),\n-      addr2_description.Address() + length2);\n+      bug_type, (void *)addr1_description.Address(),\n+      (void *)(addr1_description.Address() + length1),\n+      (void *)addr2_description.Address(),\n+      (void *)(addr2_description.Address() + length2));\n   Printf(\"%s\", d.Default());\n   scariness.Print();\n   stack->Print();\n@@ -329,10 +328,10 @@ void ErrorBadParamsToAnnotateContiguousContainer::Print() {\n       \"      end     : %p\\n\"\n       \"      old_mid : %p\\n\"\n       \"      new_mid : %p\\n\",\n-      beg, end, old_mid, new_mid);\n+      (void *)beg, (void *)end, (void *)old_mid, (void *)new_mid);\n   uptr granularity = SHADOW_GRANULARITY;\n   if (!IsAligned(beg, granularity))\n-    Report(\"ERROR: beg is not aligned by %d\\n\", granularity);\n+    Report(\"ERROR: beg is not aligned by %zu\\n\", granularity);\n   stack->Print();\n   ReportErrorSummary(scariness.GetDescription(), stack);\n }\n@@ -341,7 +340,7 @@ void ErrorODRViolation::Print() {\n   Decorator d;\n   Printf(\"%s\", d.Error());\n   Report(\"ERROR: AddressSanitizer: %s (%p):\\n\", scariness.GetDescription(),\n-         global1.beg);\n+         (void *)global1.beg);\n   Printf(\"%s\", d.Default());\n   InternalScopedString g1_loc;\n   InternalScopedString g2_loc;\n@@ -371,7 +370,8 @@ void ErrorInvalidPointerPair::Print() {\n   Decorator d;\n   Printf(\"%s\", d.Error());\n   Report(\"ERROR: AddressSanitizer: %s: %p %p\\n\", scariness.GetDescription(),\n-         addr1_description.Address(), addr2_description.Address());\n+         (void *)addr1_description.Address(),\n+         (void *)addr2_description.Address());\n   Printf(\"%s\", d.Default());\n   GET_STACK_TRACE_FATAL(pc, bp);\n   stack.Print();\n@@ -575,7 +575,7 @@ void ErrorGeneric::Print() {\n   Printf(\"%s\", d.Error());\n   uptr addr = addr_description.Address();\n   Report(\"ERROR: AddressSanitizer: %s on address %p at pc %p bp %p sp %p\\n\",\n-         bug_descr, (void *)addr, pc, bp, sp);\n+         bug_descr, (void *)addr, (void *)pc, (void *)bp, (void *)sp);\n   Printf(\"%s\", d.Default());\n \n   Printf(\"%s%s of size %zu at %p thread %s%s\\n\", d.Access(),"}, {"sha": "168bf81e9200674fc5e7054a3952297e8cd72e7f", "filename": "libsanitizer/asan/asan_globals.cpp", "status": "modified", "additions": 23, "deletions": 3, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_globals.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_globals.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_globals.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -85,10 +85,10 @@ static void ReportGlobal(const Global &g, const char *prefix) {\n   Report(\n       \"%s Global[%p]: beg=%p size=%zu/%zu name=%s module=%s dyn_init=%zu \"\n       \"odr_indicator=%p\\n\",\n-      prefix, &g, (void *)g.beg, g.size, g.size_with_redzone, g.name,\n+      prefix, (void *)&g, (void *)g.beg, g.size, g.size_with_redzone, g.name,\n       g.module_name, g.has_dynamic_init, (void *)g.odr_indicator);\n   if (g.location) {\n-    Report(\"  location (%p): name=%s[%p], %d %d\\n\", g.location,\n+    Report(\"  location (%p): name=%s[%p], %d %d\\n\", (void *)g.location,\n            g.location->filename, g.location->filename, g.location->line_no,\n            g.location->column_no);\n   }\n@@ -154,6 +154,23 @@ static void CheckODRViolationViaIndicator(const Global *g) {\n   }\n }\n \n+// Check ODR violation for given global G by checking if it's already poisoned.\n+// We use this method in case compiler doesn't use private aliases for global\n+// variables.\n+static void CheckODRViolationViaPoisoning(const Global *g) {\n+  if (__asan_region_is_poisoned(g->beg, g->size_with_redzone)) {\n+    // This check may not be enough: if the first global is much larger\n+    // the entire redzone of the second global may be within the first global.\n+    for (ListOfGlobals *l = list_of_all_globals; l; l = l->next) {\n+      if (g->beg == l->g->beg &&\n+          (flags()->detect_odr_violation >= 2 || g->size != l->g->size) &&\n+          !IsODRViolationSuppressed(g->name))\n+        ReportODRViolation(g, FindRegistrationSite(g),\n+                           l->g, FindRegistrationSite(l->g));\n+    }\n+  }\n+}\n+\n // Clang provides two different ways for global variables protection:\n // it can poison the global itself or its private alias. In former\n // case we may poison same symbol multiple times, that can help us to\n@@ -199,6 +216,8 @@ static void RegisterGlobal(const Global *g) {\n     // where two globals with the same name are defined in different modules.\n     if (UseODRIndicator(g))\n       CheckODRViolationViaIndicator(g);\n+    else\n+      CheckODRViolationViaPoisoning(g);\n   }\n   if (CanPoisonMemory())\n     PoisonRedZones(*g);\n@@ -350,7 +369,8 @@ void __asan_register_globals(__asan_global *globals, uptr n) {\n   global_registration_site_vector->push_back(site);\n   if (flags()->report_globals >= 2) {\n     PRINT_CURRENT_STACK();\n-    Printf(\"=== ID %d; %p %p\\n\", stack_id, &globals[0], &globals[n - 1]);\n+    Printf(\"=== ID %d; %p %p\\n\", stack_id, (void *)&globals[0],\n+           (void *)&globals[n - 1]);\n   }\n   for (uptr i = 0; i < n; i++) {\n     if (SANITIZER_WINDOWS && globals[i].beg == 0) {"}, {"sha": "047b044c8bf47da51e74436dc088b2740fd162dc", "filename": "libsanitizer/asan/asan_interceptors.h", "status": "modified", "additions": 1, "deletions": 6, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_interceptors.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_interceptors.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_interceptors.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -81,12 +81,7 @@ void InitializePlatformInterceptors();\n #if ASAN_HAS_EXCEPTIONS && !SANITIZER_WINDOWS && !SANITIZER_SOLARIS && \\\n     !SANITIZER_NETBSD\n # define ASAN_INTERCEPT___CXA_THROW 1\n-# if ! defined(ASAN_HAS_CXA_RETHROW_PRIMARY_EXCEPTION) \\\n-     || ASAN_HAS_CXA_RETHROW_PRIMARY_EXCEPTION\n-#   define ASAN_INTERCEPT___CXA_RETHROW_PRIMARY_EXCEPTION 1\n-# else\n-#   define ASAN_INTERCEPT___CXA_RETHROW_PRIMARY_EXCEPTION 0\n-# endif\n+# define ASAN_INTERCEPT___CXA_RETHROW_PRIMARY_EXCEPTION 1\n # if defined(_GLIBCXX_SJLJ_EXCEPTIONS) || (SANITIZER_IOS && defined(__arm__))\n #  define ASAN_INTERCEPT__UNWIND_SJLJ_RAISEEXCEPTION 1\n # else"}, {"sha": "ad3693d5e6a2ce8d609d73fafb84a7ba10a099f2", "filename": "libsanitizer/asan/asan_linux.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_linux.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_linux.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_linux.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -128,8 +128,8 @@ void AsanCheckIncompatibleRT() {}\n #else\n static int FindFirstDSOCallback(struct dl_phdr_info *info, size_t size,\n                                 void *data) {\n-  VReport(2, \"info->dlpi_name = %s\\tinfo->dlpi_addr = %p\\n\",\n-          info->dlpi_name, info->dlpi_addr);\n+  VReport(2, \"info->dlpi_name = %s\\tinfo->dlpi_addr = %p\\n\", info->dlpi_name,\n+          (void *)info->dlpi_addr);\n \n   // Continue until the first dynamic library is found\n   if (!info->dlpi_name || info->dlpi_name[0] == 0)"}, {"sha": "e5a7f2007aea8b8208e929e08ee675a55cd252eb", "filename": "libsanitizer/asan/asan_mapping.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_mapping.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_mapping.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_mapping.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -165,7 +165,7 @@ static const u64 kAArch64_ShadowOffset64 = 1ULL << 36;\n static const u64 kRiscv64_ShadowOffset64 = 0xd55550000;\n static const u64 kMIPS32_ShadowOffset32 = 0x0aaa0000;\n static const u64 kMIPS64_ShadowOffset64 = 1ULL << 37;\n-static const u64 kPPC64_ShadowOffset64 = 1ULL << 41;\n+static const u64 kPPC64_ShadowOffset64 = 1ULL << 44;\n static const u64 kSystemZ_ShadowOffset64 = 1ULL << 52;\n static const u64 kSPARC64_ShadowOffset64 = 1ULL << 43;  // 0x80000000000\n static const u64 kFreeBSD_ShadowOffset32 = 1ULL << 30;  // 0x40000000"}, {"sha": "d97af91e692dcdd243a71ee53cbd3a569a99272a", "filename": "libsanitizer/asan/asan_poisoning.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_poisoning.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_poisoning.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_poisoning.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -66,7 +66,7 @@ void AsanPoisonOrUnpoisonIntraObjectRedzone(uptr ptr, uptr size, bool poison) {\n   uptr end = ptr + size;\n   if (Verbosity()) {\n     Printf(\"__asan_%spoison_intra_object_redzone [%p,%p) %zd\\n\",\n-           poison ? \"\" : \"un\", ptr, end, size);\n+           poison ? \"\" : \"un\", (void *)ptr, (void *)end, size);\n     if (Verbosity() >= 2)\n       PRINT_CURRENT_STACK();\n   }"}, {"sha": "1f266334b31150f88a50881257efb3389445476a", "filename": "libsanitizer/asan/asan_report.cpp", "status": "modified", "additions": 8, "deletions": 7, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_report.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_report.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_report.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -67,14 +67,14 @@ static void PrintZoneForPointer(uptr ptr, uptr zone_ptr,\n                                 const char *zone_name) {\n   if (zone_ptr) {\n     if (zone_name) {\n-      Printf(\"malloc_zone_from_ptr(%p) = %p, which is %s\\n\",\n-                 ptr, zone_ptr, zone_name);\n+      Printf(\"malloc_zone_from_ptr(%p) = %p, which is %s\\n\", (void *)ptr,\n+             (void *)zone_ptr, zone_name);\n     } else {\n       Printf(\"malloc_zone_from_ptr(%p) = %p, which doesn't have a name\\n\",\n-                 ptr, zone_ptr);\n+             (void *)ptr, (void *)zone_ptr);\n     }\n   } else {\n-    Printf(\"malloc_zone_from_ptr(%p) = 0\\n\", ptr);\n+    Printf(\"malloc_zone_from_ptr(%p) = 0\\n\", (void *)ptr);\n   }\n }\n \n@@ -435,9 +435,10 @@ static inline void CheckForInvalidPointerPair(void *p1, void *p2) {\n void ReportMacMzReallocUnknown(uptr addr, uptr zone_ptr, const char *zone_name,\n                                BufferedStackTrace *stack) {\n   ScopedInErrorReport in_report;\n-  Printf(\"mz_realloc(%p) -- attempting to realloc unallocated memory.\\n\"\n-             \"This is an unrecoverable problem, exiting now.\\n\",\n-             addr);\n+  Printf(\n+      \"mz_realloc(%p) -- attempting to realloc unallocated memory.\\n\"\n+      \"This is an unrecoverable problem, exiting now.\\n\",\n+      (void *)addr);\n   PrintZoneForPointer(addr, zone_ptr, zone_name);\n   stack->Print();\n   DescribeAddressIfHeap(addr);"}, {"sha": "1b150b393cfe0b0972b530d2239fff82dd235b24", "filename": "libsanitizer/asan/asan_rtl.cpp", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_rtl.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_rtl.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_rtl.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -557,7 +557,8 @@ void UnpoisonStack(uptr bottom, uptr top, const char *type) {\n         \"False positive error reports may follow\\n\"\n         \"For details see \"\n         \"https://github.com/google/sanitizers/issues/189\\n\",\n-        type, top, bottom, top - bottom, top - bottom);\n+        type, (void *)top, (void *)bottom, (void *)(top - bottom),\n+        top - bottom);\n     return;\n   }\n   PoisonShadow(bottom, RoundUpTo(top - bottom, SHADOW_GRANULARITY), 0);"}, {"sha": "fc6de39622b510aaeaa8e6c833f01aa8964f704b", "filename": "libsanitizer/asan/asan_shadow_setup.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_shadow_setup.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_shadow_setup.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_shadow_setup.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -33,7 +33,7 @@ static void ProtectGap(uptr addr, uptr size) {\n           \"protect_shadow_gap=0:\"\n           \" not protecting shadow gap, allocating gap's shadow\\n\"\n           \"|| `[%p, %p]` || ShadowGap's shadow ||\\n\",\n-          GapShadowBeg, GapShadowEnd);\n+          (void*)GapShadowBeg, (void*)GapShadowEnd);\n     ReserveShadowMemoryRange(GapShadowBeg, GapShadowEnd,\n                              \"unprotected gap shadow\");\n     return;\n@@ -113,7 +113,7 @@ void InitializeShadowMemory() {\n         \"Shadow memory range interleaves with an existing memory mapping. \"\n         \"ASan cannot proceed correctly. ABORTING.\\n\");\n     Report(\"ASan shadow was supposed to be located in the [%p-%p] range.\\n\",\n-           shadow_start, kHighShadowEnd);\n+           (void*)shadow_start, (void*)kHighShadowEnd);\n     MaybeReportLinuxPIEBug();\n     DumpProcessMap();\n     Die();"}, {"sha": "8af74254cdc767a696a38b33088c1ab82c46a001", "filename": "libsanitizer/asan/asan_thread.cpp", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_thread.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fasan%2Fasan_thread.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fasan%2Fasan_thread.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -254,7 +254,7 @@ void AsanThread::Init(const InitOptions *options) {\n   int local = 0;\n   VReport(1, \"T%d: stack [%p,%p) size 0x%zx; local=%p\\n\", tid(),\n           (void *)stack_bottom_, (void *)stack_top_, stack_top_ - stack_bottom_,\n-          &local);\n+          (void *)&local);\n }\n \n // Fuchsia doesn't use ThreadStart.\n@@ -443,7 +443,7 @@ AsanThread *GetCurrentThread() {\n \n void SetCurrentThread(AsanThread *t) {\n   CHECK(t->context());\n-  VReport(2, \"SetCurrentThread: %p for thread %p\\n\", t->context(),\n+  VReport(2, \"SetCurrentThread: %p for thread %p\\n\", (void *)t->context(),\n           (void *)GetThreadSelf());\n   // Make sure we do not reset the current AsanThread.\n   CHECK_EQ(0, AsanTSDGet());"}, {"sha": "c2863400d9d905223a4f4942cd867d16470afa1b", "filename": "libsanitizer/hwasan/hwasan.cpp", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fhwasan%2Fhwasan.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fhwasan%2Fhwasan.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fhwasan%2Fhwasan.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -16,6 +16,7 @@\n #include \"hwasan_checks.h\"\n #include \"hwasan_dynamic_shadow.h\"\n #include \"hwasan_globals.h\"\n+#include \"hwasan_mapping.h\"\n #include \"hwasan_poisoning.h\"\n #include \"hwasan_report.h\"\n #include \"hwasan_thread.h\"\n@@ -391,8 +392,15 @@ void __hwasan_print_shadow(const void *p, uptr sz) {\n   uptr shadow_last = MemToShadow(ptr_raw + sz - 1);\n   Printf(\"HWASan shadow map for %zx .. %zx (pointer tag %x)\\n\", ptr_raw,\n          ptr_raw + sz, GetTagFromPointer((uptr)p));\n-  for (uptr s = shadow_first; s <= shadow_last; ++s)\n-    Printf(\"  %zx: %x\\n\", ShadowToMem(s), *(tag_t *)s);\n+  for (uptr s = shadow_first; s <= shadow_last; ++s) {\n+    tag_t mem_tag = *reinterpret_cast<tag_t *>(s);\n+    uptr granule_addr = ShadowToMem(s);\n+    if (mem_tag && mem_tag < kShadowAlignment)\n+      Printf(\"  %zx: %02x(%02x)\\n\", granule_addr, mem_tag,\n+             *reinterpret_cast<tag_t *>(granule_addr + kShadowAlignment - 1));\n+    else\n+      Printf(\"  %zx: %02x\\n\", granule_addr, mem_tag);\n+  }\n }\n \n sptr __hwasan_test_shadow(const void *p, uptr sz) {"}, {"sha": "0107b8b772a9d004827b44b4ac79703454c58736", "filename": "libsanitizer/hwasan/hwasan_report.cpp", "status": "modified", "additions": 26, "deletions": 5, "changes": 31, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fhwasan%2Fhwasan_report.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fhwasan%2Fhwasan_report.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fhwasan%2Fhwasan_report.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -702,12 +702,33 @@ void ReportTagMismatch(StackTrace *stack, uptr tagged_addr, uptr access_size,\n   tag_t mem_tag = *tag_ptr;\n \n   Printf(\"%s\", d.Access());\n-  Printf(\"%s of size %zu at %p tags: %02x/%02x (ptr/mem) in thread T%zd\\n\",\n-         is_store ? \"WRITE\" : \"READ\", access_size, untagged_addr, ptr_tag,\n-         mem_tag, t->unique_id());\n+  if (mem_tag && mem_tag < kShadowAlignment) {\n+    tag_t *granule_ptr = reinterpret_cast<tag_t *>((untagged_addr + offset) &\n+                                                   ~(kShadowAlignment - 1));\n+    // If offset is 0, (untagged_addr + offset) is not aligned to granules.\n+    // This is the offset of the leftmost accessed byte within the bad granule.\n+    u8 in_granule_offset = (untagged_addr + offset) & (kShadowAlignment - 1);\n+    tag_t short_tag = granule_ptr[kShadowAlignment - 1];\n+    // The first mismatch was a short granule that matched the ptr_tag.\n+    if (short_tag == ptr_tag) {\n+      // If the access starts after the end of the short granule, then the first\n+      // bad byte is the first byte of the access; otherwise it is the first\n+      // byte past the end of the short granule\n+      if (mem_tag > in_granule_offset) {\n+        offset += mem_tag - in_granule_offset;\n+      }\n+    }\n+    Printf(\n+        \"%s of size %zu at %p tags: %02x/%02x(%02x) (ptr/mem) in thread T%zd\\n\",\n+        is_store ? \"WRITE\" : \"READ\", access_size, untagged_addr, ptr_tag,\n+        mem_tag, short_tag, t->unique_id());\n+  } else {\n+    Printf(\"%s of size %zu at %p tags: %02x/%02x (ptr/mem) in thread T%zd\\n\",\n+           is_store ? \"WRITE\" : \"READ\", access_size, untagged_addr, ptr_tag,\n+           mem_tag, t->unique_id());\n+  }\n   if (offset != 0)\n-    Printf(\"Invalid access starting at offset [%zu, %zu)\\n\", offset,\n-           Min(access_size, static_cast<uptr>(offset) + (1 << kShadowScale)));\n+    Printf(\"Invalid access starting at offset %zu\\n\", offset);\n   Printf(\"%s\", d.Default());\n \n   stack->Print();"}, {"sha": "139abd07755474cbf3c5c8a875db7ad6651256f3", "filename": "libsanitizer/lsan/lsan_common.cpp", "status": "modified", "additions": 38, "deletions": 36, "changes": 74, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Flsan%2Flsan_common.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Flsan%2Flsan_common.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_common.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -188,7 +188,8 @@ void ScanRangeForPointers(uptr begin, uptr end,\n                           const char *region_type, ChunkTag tag) {\n   CHECK(tag == kReachable || tag == kIndirectlyLeaked);\n   const uptr alignment = flags()->pointer_alignment();\n-  LOG_POINTERS(\"Scanning %s range %p-%p.\\n\", region_type, begin, end);\n+  LOG_POINTERS(\"Scanning %s range %p-%p.\\n\", region_type, (void *)begin,\n+               (void *)end);\n   uptr pp = begin;\n   if (pp % alignment)\n     pp = pp + alignment - pp % alignment;\n@@ -207,13 +208,15 @@ void ScanRangeForPointers(uptr begin, uptr end,\n       LOG_POINTERS(\n           \"%p is poisoned: ignoring %p pointing into chunk %p-%p of size \"\n           \"%zu.\\n\",\n-          pp, p, chunk, chunk + m.requested_size(), m.requested_size());\n+          (void *)pp, p, (void *)chunk, (void *)(chunk + m.requested_size()),\n+          m.requested_size());\n       continue;\n     }\n \n     m.set_tag(tag);\n-    LOG_POINTERS(\"%p: found %p pointing into chunk %p-%p of size %zu.\\n\", pp, p,\n-                 chunk, chunk + m.requested_size(), m.requested_size());\n+    LOG_POINTERS(\"%p: found %p pointing into chunk %p-%p of size %zu.\\n\",\n+                 (void *)pp, p, (void *)chunk,\n+                 (void *)(chunk + m.requested_size()), m.requested_size());\n     if (frontier)\n       frontier->push_back(chunk);\n   }\n@@ -281,7 +284,7 @@ static void ProcessThreads(SuspendedThreadsList const &suspended_threads,\n   InternalMmapVector<uptr> registers;\n   for (uptr i = 0; i < suspended_threads.ThreadCount(); i++) {\n     tid_t os_id = static_cast<tid_t>(suspended_threads.GetThreadID(i));\n-    LOG_THREADS(\"Processing thread %d.\\n\", os_id);\n+    LOG_THREADS(\"Processing thread %llu.\\n\", os_id);\n     uptr stack_begin, stack_end, tls_begin, tls_end, cache_begin, cache_end;\n     DTLS *dtls;\n     bool thread_found = GetThreadRangesLocked(os_id, &stack_begin, &stack_end,\n@@ -290,14 +293,14 @@ static void ProcessThreads(SuspendedThreadsList const &suspended_threads,\n     if (!thread_found) {\n       // If a thread can't be found in the thread registry, it's probably in the\n       // process of destruction. Log this event and move on.\n-      LOG_THREADS(\"Thread %d not found in registry.\\n\", os_id);\n+      LOG_THREADS(\"Thread %llu not found in registry.\\n\", os_id);\n       continue;\n     }\n     uptr sp;\n     PtraceRegistersStatus have_registers =\n         suspended_threads.GetRegistersAndSP(i, &registers, &sp);\n     if (have_registers != REGISTERS_AVAILABLE) {\n-      Report(\"Unable to get registers from thread %d.\\n\", os_id);\n+      Report(\"Unable to get registers from thread %llu.\\n\", os_id);\n       // If unable to get SP, consider the entire stack to be reachable unless\n       // GetRegistersAndSP failed with ESRCH.\n       if (have_registers == REGISTERS_UNAVAILABLE_FATAL) continue;\n@@ -313,7 +316,8 @@ static void ProcessThreads(SuspendedThreadsList const &suspended_threads,\n     }\n \n     if (flags()->use_stacks) {\n-      LOG_THREADS(\"Stack at %p-%p (SP = %p).\\n\", stack_begin, stack_end, sp);\n+      LOG_THREADS(\"Stack at %p-%p (SP = %p).\\n\", (void *)stack_begin,\n+                  (void *)stack_end, (void *)sp);\n       if (sp < stack_begin || sp >= stack_end) {\n         // SP is outside the recorded stack range (e.g. the thread is running a\n         // signal handler on alternate stack, or swapcontext was used).\n@@ -327,7 +331,7 @@ static void ProcessThreads(SuspendedThreadsList const &suspended_threads,\n           stack_begin += page_size;\n         }\n         LOG_THREADS(\"Skipped %d guard page(s) to obtain stack %p-%p.\\n\",\n-                    skipped, stack_begin, stack_end);\n+                    skipped, (void *)stack_begin, (void *)stack_end);\n       } else {\n         // Shrink the stack range to ignore out-of-scope values.\n         stack_begin = sp;\n@@ -339,7 +343,7 @@ static void ProcessThreads(SuspendedThreadsList const &suspended_threads,\n \n     if (flags()->use_tls) {\n       if (tls_begin) {\n-        LOG_THREADS(\"TLS at %p-%p.\\n\", tls_begin, tls_end);\n+        LOG_THREADS(\"TLS at %p-%p.\\n\", (void *)tls_begin, (void *)tls_end);\n         // If the tls and cache ranges don't overlap, scan full tls range,\n         // otherwise, only scan the non-overlapping portions\n         if (cache_begin == cache_end || tls_end < cache_begin ||\n@@ -373,15 +377,16 @@ static void ProcessThreads(SuspendedThreadsList const &suspended_threads,\n           uptr dtls_beg = dtv.beg;\n           uptr dtls_end = dtls_beg + dtv.size;\n           if (dtls_beg < dtls_end) {\n-            LOG_THREADS(\"DTLS %zu at %p-%p.\\n\", id, dtls_beg, dtls_end);\n+            LOG_THREADS(\"DTLS %d at %p-%p.\\n\", id, (void *)dtls_beg,\n+                        (void *)dtls_end);\n             ScanRangeForPointers(dtls_beg, dtls_end, frontier, \"DTLS\",\n                                  kReachable);\n           }\n         });\n       } else {\n         // We are handling a thread with DTLS under destruction. Log about\n         // this and continue.\n-        LOG_THREADS(\"Thread %d has DTLS under destruction.\\n\", os_id);\n+        LOG_THREADS(\"Thread %llu has DTLS under destruction.\\n\", os_id);\n       }\n #endif\n     }\n@@ -399,8 +404,9 @@ void ScanRootRegion(Frontier *frontier, const RootRegion &root_region,\n   uptr intersection_end = Min(region_end, root_region.begin + root_region.size);\n   if (intersection_begin >= intersection_end) return;\n   LOG_POINTERS(\"Root region %p-%p intersects with mapped region %p-%p (%s)\\n\",\n-               root_region.begin, root_region.begin + root_region.size,\n-               region_begin, region_end,\n+               (void *)root_region.begin,\n+               (void *)(root_region.begin + root_region.size),\n+               (void *)region_begin, (void *)region_end,\n                is_readable ? \"readable\" : \"unreadable\");\n   if (is_readable)\n     ScanRangeForPointers(intersection_begin, intersection_end, frontier, \"ROOT\",\n@@ -460,8 +466,8 @@ static void IgnoredSuppressedCb(uptr chunk, void *arg) {\n   if (idx >= suppressed.size() || m.stack_trace_id() != suppressed[idx])\n     return;\n \n-  LOG_POINTERS(\"Suppressed: chunk %p-%p of size %zu.\\n\", chunk,\n-               chunk + m.requested_size(), m.requested_size());\n+  LOG_POINTERS(\"Suppressed: chunk %p-%p of size %zu.\\n\", (void *)chunk,\n+               (void *)(chunk + m.requested_size()), m.requested_size());\n   m.set_tag(kIgnored);\n }\n \n@@ -472,8 +478,8 @@ static void CollectIgnoredCb(uptr chunk, void *arg) {\n   chunk = GetUserBegin(chunk);\n   LsanMetadata m(chunk);\n   if (m.allocated() && m.tag() == kIgnored) {\n-    LOG_POINTERS(\"Ignored: chunk %p-%p of size %zu.\\n\",\n-                 chunk, chunk + m.requested_size(), m.requested_size());\n+    LOG_POINTERS(\"Ignored: chunk %p-%p of size %zu.\\n\", (void *)chunk,\n+                 (void *)(chunk + m.requested_size()), m.requested_size());\n     reinterpret_cast<Frontier *>(arg)->push_back(chunk);\n   }\n }\n@@ -487,7 +493,6 @@ static uptr GetCallerPC(const StackTrace &stack) {\n \n struct InvalidPCParam {\n   Frontier *frontier;\n-  const StackDepotReverseMap *stack_depot;\n   bool skip_linker_allocations;\n };\n \n@@ -502,7 +507,7 @@ static void MarkInvalidPCCb(uptr chunk, void *arg) {\n     u32 stack_id = m.stack_trace_id();\n     uptr caller_pc = 0;\n     if (stack_id > 0)\n-      caller_pc = GetCallerPC(param->stack_depot->Get(stack_id));\n+      caller_pc = GetCallerPC(StackDepotGet(stack_id));\n     // If caller_pc is unknown, this chunk may be allocated in a coroutine. Mark\n     // it as reachable, as we can't properly report its allocation stack anyway.\n     if (caller_pc == 0 || (param->skip_linker_allocations &&\n@@ -533,19 +538,16 @@ static void MarkInvalidPCCb(uptr chunk, void *arg) {\n // which we don't care about).\n // On all other platforms, this simply checks to ensure that the caller pc is\n // valid before reporting chunks as leaked.\n-static void ProcessPC(Frontier *frontier,\n-                      const StackDepotReverseMap &stack_depot) {\n+static void ProcessPC(Frontier *frontier) {\n   InvalidPCParam arg;\n   arg.frontier = frontier;\n-  arg.stack_depot = &stack_depot;\n   arg.skip_linker_allocations =\n       flags()->use_tls && flags()->use_ld_allocations && GetLinker() != nullptr;\n   ForEachChunk(MarkInvalidPCCb, &arg);\n }\n \n // Sets the appropriate tag on each chunk.\n static void ClassifyAllChunks(SuspendedThreadsList const &suspended_threads,\n-                              const StackDepotReverseMap &stack_depot,\n                               Frontier *frontier) {\n   const InternalMmapVector<u32> &suppressed_stacks =\n       GetSuppressionContext()->GetSortedSuppressedStacks();\n@@ -560,7 +562,7 @@ static void ClassifyAllChunks(SuspendedThreadsList const &suspended_threads,\n   FloodFillTag(frontier, kReachable);\n \n   CHECK_EQ(0, frontier->size());\n-  ProcessPC(frontier, stack_depot);\n+  ProcessPC(frontier);\n \n   // The check here is relatively expensive, so we do this in a separate flood\n   // fill. That way we can skip the check for chunks that are reachable\n@@ -621,8 +623,9 @@ static void ReportIfNotSuspended(ThreadContextBase *tctx, void *arg) {\n   if (tctx->status == ThreadStatusRunning) {\n     uptr i = InternalLowerBound(suspended_threads, tctx->os_id);\n     if (i >= suspended_threads.size() || suspended_threads[i] != tctx->os_id)\n-      Report(\"Running thread %d was not suspended. False leaks are possible.\\n\",\n-             tctx->os_id);\n+      Report(\n+          \"Running thread %llu was not suspended. False leaks are possible.\\n\",\n+          tctx->os_id);\n   }\n }\n \n@@ -654,8 +657,7 @@ static void CheckForLeaksCallback(const SuspendedThreadsList &suspended_threads,\n   CHECK(param);\n   CHECK(!param->success);\n   ReportUnsuspendedThreads(suspended_threads);\n-  ClassifyAllChunks(suspended_threads, param->leak_report.stack_depot(),\n-                    &param->frontier);\n+  ClassifyAllChunks(suspended_threads, &param->frontier);\n   ForEachChunk(CollectLeaksCb, &param->leak_report);\n   // Clean up for subsequent leak checks. This assumes we did not overwrite any\n   // kIgnored tags.\n@@ -795,7 +797,7 @@ void LeakReport::AddLeakedChunk(uptr chunk, u32 stack_trace_id,\n   CHECK(tag == kDirectlyLeaked || tag == kIndirectlyLeaked);\n \n   if (u32 resolution = flags()->resolution) {\n-    StackTrace stack = stack_depot_.Get(stack_trace_id);\n+    StackTrace stack = StackDepotGet(stack_trace_id);\n     stack.size = Min(stack.size, resolution);\n     stack_trace_id = StackDepotPut(stack);\n   }\n@@ -863,7 +865,7 @@ void LeakReport::PrintReportForLeak(uptr index) {\n   Printf(\"%s\", d.Default());\n \n   CHECK(leaks_[index].stack_trace_id);\n-  stack_depot_.Get(leaks_[index].stack_trace_id).Print();\n+  StackDepotGet(leaks_[index].stack_trace_id).Print();\n \n   if (flags()->report_objects) {\n     Printf(\"Objects leaked above:\\n\");\n@@ -876,7 +878,7 @@ void LeakReport::PrintLeakedObjectsForLeak(uptr index) {\n   u32 leak_id = leaks_[index].id;\n   for (uptr j = 0; j < leaked_objects_.size(); j++) {\n     if (leaked_objects_[j].leak_id == leak_id)\n-      Printf(\"%p (%zu bytes)\\n\", leaked_objects_[j].addr,\n+      Printf(\"%p (%zu bytes)\\n\", (void *)leaked_objects_[j].addr,\n              leaked_objects_[j].size);\n   }\n }\n@@ -900,7 +902,7 @@ uptr LeakReport::ApplySuppressions() {\n   uptr new_suppressions = false;\n   for (uptr i = 0; i < leaks_.size(); i++) {\n     Suppression *s = suppressions->GetSuppressionForStack(\n-        leaks_[i].stack_trace_id, stack_depot_.Get(leaks_[i].stack_trace_id));\n+        leaks_[i].stack_trace_id, StackDepotGet(leaks_[i].stack_trace_id));\n     if (s) {\n       s->weight += leaks_[i].total_size;\n       atomic_store_relaxed(&s->hit_count, atomic_load_relaxed(&s->hit_count) +\n@@ -967,7 +969,7 @@ void __lsan_register_root_region(const void *begin, uptr size) {\n   CHECK(root_regions);\n   RootRegion region = {reinterpret_cast<uptr>(begin), size};\n   root_regions->push_back(region);\n-  VReport(1, \"Registered root region at %p of size %llu\\n\", begin, size);\n+  VReport(1, \"Registered root region at %p of size %zu\\n\", begin, size);\n #endif // CAN_SANITIZE_LEAKS\n }\n \n@@ -984,13 +986,13 @@ void __lsan_unregister_root_region(const void *begin, uptr size) {\n       uptr last_index = root_regions->size() - 1;\n       (*root_regions)[i] = (*root_regions)[last_index];\n       root_regions->pop_back();\n-      VReport(1, \"Unregistered root region at %p of size %llu\\n\", begin, size);\n+      VReport(1, \"Unregistered root region at %p of size %zu\\n\", begin, size);\n       break;\n     }\n   }\n   if (!removed) {\n     Report(\n-        \"__lsan_unregister_root_region(): region at %p of size %llu has not \"\n+        \"__lsan_unregister_root_region(): region at %p of size %zu has not \"\n         \"been registered.\\n\",\n         begin, size);\n     Die();"}, {"sha": "93b7d4e2d7e1a45e90e395dc154a20c6e274c32f", "filename": "libsanitizer/lsan/lsan_common.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Flsan%2Flsan_common.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Flsan%2Flsan_common.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Flsan%2Flsan_common.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -108,14 +108,12 @@ class LeakReport {\n   uptr ApplySuppressions();\n   uptr UnsuppressedLeakCount();\n   uptr IndirectUnsuppressedLeakCount();\n-  const StackDepotReverseMap &stack_depot() { return stack_depot_; }\n \n  private:\n   void PrintReportForLeak(uptr index);\n   void PrintLeakedObjectsForLeak(uptr index);\n \n   u32 next_id_ = 0;\n-  StackDepotReverseMap stack_depot_;\n   InternalMmapVector<Leak> leaks_;\n   InternalMmapVector<LeakedObject> leaked_objects_;\n };"}, {"sha": "ec23465d958464272bbb743614124a4b11f71216", "filename": "libsanitizer/sanitizer_common/sanitizer_allocator.h", "status": "modified", "additions": 1, "deletions": 7, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -14,6 +14,7 @@\n #define SANITIZER_ALLOCATOR_H\n \n #include \"sanitizer_common.h\"\n+#include \"sanitizer_flat_map.h\"\n #include \"sanitizer_internal_defs.h\"\n #include \"sanitizer_lfstack.h\"\n #include \"sanitizer_libc.h\"\n@@ -43,12 +44,6 @@ void SetAllocatorOutOfMemory();\n \n void PrintHintAllocatorCannotReturnNull();\n \n-// Allocators call these callbacks on mmap/munmap.\n-struct NoOpMapUnmapCallback {\n-  void OnMap(uptr p, uptr size) const { }\n-  void OnUnmap(uptr p, uptr size) const { }\n-};\n-\n // Callback type for iterating over chunks.\n typedef void (*ForEachChunkCallback)(uptr chunk, void *arg);\n \n@@ -70,7 +65,6 @@ inline void RandomShuffle(T *a, u32 n, u32 *rand_state) {\n #include \"sanitizer_allocator_size_class_map.h\"\n #include \"sanitizer_allocator_stats.h\"\n #include \"sanitizer_allocator_primary64.h\"\n-#include \"sanitizer_allocator_bytemap.h\"\n #include \"sanitizer_allocator_primary32.h\"\n #include \"sanitizer_allocator_local_cache.h\"\n #include \"sanitizer_allocator_secondary.h\""}, {"sha": "0084bb62c83c8063504a5914164cee868934a317", "filename": "libsanitizer/sanitizer_common/sanitizer_allocator_bytemap.h", "status": "removed", "additions": 0, "deletions": 107, "changes": 107, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bb27f5e9ec3c7ab0f5c859d90c59dd4573b53d97/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_bytemap.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bb27f5e9ec3c7ab0f5c859d90c59dd4573b53d97/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_bytemap.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_bytemap.h?ref=bb27f5e9ec3c7ab0f5c859d90c59dd4573b53d97", "patch": "@@ -1,107 +0,0 @@\n-//===-- sanitizer_allocator_bytemap.h ---------------------------*- C++ -*-===//\n-//\n-// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n-// See https://llvm.org/LICENSE.txt for license information.\n-// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n-//\n-//===----------------------------------------------------------------------===//\n-//\n-// Part of the Sanitizer Allocator.\n-//\n-//===----------------------------------------------------------------------===//\n-#ifndef SANITIZER_ALLOCATOR_H\n-#error This file must be included inside sanitizer_allocator.h\n-#endif\n-\n-// Maps integers in rage [0, kSize) to u8 values.\n-template <u64 kSize, typename AddressSpaceViewTy = LocalAddressSpaceView>\n-class FlatByteMap {\n- public:\n-  using AddressSpaceView = AddressSpaceViewTy;\n-  void Init() {\n-    internal_memset(map_, 0, sizeof(map_));\n-  }\n-\n-  void set(uptr idx, u8 val) {\n-    CHECK_LT(idx, kSize);\n-    CHECK_EQ(0U, map_[idx]);\n-    map_[idx] = val;\n-  }\n-  u8 operator[] (uptr idx) {\n-    CHECK_LT(idx, kSize);\n-    // FIXME: CHECK may be too expensive here.\n-    return map_[idx];\n-  }\n- private:\n-  u8 map_[kSize];\n-};\n-\n-// TwoLevelByteMap maps integers in range [0, kSize1*kSize2) to u8 values.\n-// It is implemented as a two-dimensional array: array of kSize1 pointers\n-// to kSize2-byte arrays. The secondary arrays are mmaped on demand.\n-// Each value is initially zero and can be set to something else only once.\n-// Setting and getting values from multiple threads is safe w/o extra locking.\n-template <u64 kSize1, u64 kSize2,\n-          typename AddressSpaceViewTy = LocalAddressSpaceView,\n-          class MapUnmapCallback = NoOpMapUnmapCallback>\n-class TwoLevelByteMap {\n- public:\n-  using AddressSpaceView = AddressSpaceViewTy;\n-  void Init() {\n-    internal_memset(map1_, 0, sizeof(map1_));\n-    mu_.Init();\n-  }\n-\n-  void TestOnlyUnmap() {\n-    for (uptr i = 0; i < kSize1; i++) {\n-      u8 *p = Get(i);\n-      if (!p) continue;\n-      MapUnmapCallback().OnUnmap(reinterpret_cast<uptr>(p), kSize2);\n-      UnmapOrDie(p, kSize2);\n-    }\n-  }\n-\n-  uptr size() const { return kSize1 * kSize2; }\n-  uptr size1() const { return kSize1; }\n-  uptr size2() const { return kSize2; }\n-\n-  void set(uptr idx, u8 val) {\n-    CHECK_LT(idx, kSize1 * kSize2);\n-    u8 *map2 = GetOrCreate(idx / kSize2);\n-    CHECK_EQ(0U, map2[idx % kSize2]);\n-    map2[idx % kSize2] = val;\n-  }\n-\n-  u8 operator[] (uptr idx) const {\n-    CHECK_LT(idx, kSize1 * kSize2);\n-    u8 *map2 = Get(idx / kSize2);\n-    if (!map2) return 0;\n-    auto value_ptr = AddressSpaceView::Load(&map2[idx % kSize2]);\n-    return *value_ptr;\n-  }\n-\n- private:\n-  u8 *Get(uptr idx) const {\n-    CHECK_LT(idx, kSize1);\n-    return reinterpret_cast<u8 *>(\n-        atomic_load(&map1_[idx], memory_order_acquire));\n-  }\n-\n-  u8 *GetOrCreate(uptr idx) {\n-    u8 *res = Get(idx);\n-    if (!res) {\n-      SpinMutexLock l(&mu_);\n-      if (!(res = Get(idx))) {\n-        res = (u8*)MmapOrDie(kSize2, \"TwoLevelByteMap\");\n-        MapUnmapCallback().OnMap(reinterpret_cast<uptr>(res), kSize2);\n-        atomic_store(&map1_[idx], reinterpret_cast<uptr>(res),\n-                     memory_order_release);\n-      }\n-    }\n-    return res;\n-  }\n-\n-  atomic_uintptr_t map1_[kSize1];\n-  StaticSpinMutex mu_;\n-};\n-"}, {"sha": "22180f5fbf704018d84e05778eb511226f3950bf", "filename": "libsanitizer/sanitizer_common/sanitizer_allocator_primary32.h", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_primary32.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_primary32.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_primary32.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -198,8 +198,9 @@ class SizeClassAllocator32 {\n     return GetSizeClass(p) != 0;\n   }\n \n-  uptr GetSizeClass(const void *p) {\n-    return possible_regions[ComputeRegionId(reinterpret_cast<uptr>(p))];\n+  uptr GetSizeClass(const void *p) const {\n+    uptr id = ComputeRegionId(reinterpret_cast<uptr>(p));\n+    return possible_regions.contains(id) ? possible_regions[id] : 0;\n   }\n \n   void *GetBlockBegin(const void *p) {\n@@ -251,9 +252,9 @@ class SizeClassAllocator32 {\n \n   // Iterate over all existing chunks.\n   // The allocator must be locked when calling this function.\n-  void ForEachChunk(ForEachChunkCallback callback, void *arg) {\n+  void ForEachChunk(ForEachChunkCallback callback, void *arg) const {\n     for (uptr region = 0; region < kNumPossibleRegions; region++)\n-      if (possible_regions[region]) {\n+      if (possible_regions.contains(region) && possible_regions[region]) {\n         uptr chunk_size = ClassIdToSize(possible_regions[region]);\n         uptr max_chunks_in_region = kRegionSize / (chunk_size + kMetadataSize);\n         uptr region_beg = region * kRegionSize;\n@@ -305,7 +306,7 @@ class SizeClassAllocator32 {\n     MapUnmapCallback().OnMap(res, kRegionSize);\n     stat->Add(AllocatorStatMapped, kRegionSize);\n     CHECK(IsAligned(res, kRegionSize));\n-    possible_regions.set(ComputeRegionId(res), static_cast<u8>(class_id));\n+    possible_regions[ComputeRegionId(res)] = class_id;\n     return res;\n   }\n "}, {"sha": "f917310cfebb41e21b3b3e025bdc52dd57c3d274", "filename": "libsanitizer/sanitizer_common/sanitizer_allocator_primary64.h", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_primary64.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_primary64.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_allocator_primary64.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -302,9 +302,8 @@ class SizeClassAllocator64 {\n     UnmapWithCallbackOrDie((uptr)address_range.base(), address_range.size());\n   }\n \n-  static void FillMemoryProfile(uptr start, uptr rss, bool file, uptr *stats,\n-                           uptr stats_size) {\n-    for (uptr class_id = 0; class_id < stats_size; class_id++)\n+  static void FillMemoryProfile(uptr start, uptr rss, bool file, uptr *stats) {\n+    for (uptr class_id = 0; class_id < kNumClasses; class_id++)\n       if (stats[class_id] == start)\n         stats[class_id] = rss;\n   }\n@@ -330,7 +329,7 @@ class SizeClassAllocator64 {\n     uptr rss_stats[kNumClasses];\n     for (uptr class_id = 0; class_id < kNumClasses; class_id++)\n       rss_stats[class_id] = SpaceBeg() + kRegionSize * class_id;\n-    GetMemoryProfile(FillMemoryProfile, rss_stats, kNumClasses);\n+    GetMemoryProfile(FillMemoryProfile, rss_stats);\n \n     uptr total_mapped = 0;\n     uptr total_rss = 0;"}, {"sha": "9ebba91da73ffe0fae49dc6dc5eb3da5f804d6ff", "filename": "libsanitizer/sanitizer_common/sanitizer_asm.h", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_asm.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_asm.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_asm.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -67,6 +67,9 @@\n #define NO_EXEC_STACK_DIRECTIVE\n #endif\n \n-#if defined(__x86_64__) || defined(__i386__)\n+#if (defined(__x86_64__) || defined(__i386__)) && defined(__has_include) && __has_include(<cet.h>)\n #include <cet.h>\n #endif\n+#ifndef _CET_ENDBR\n+#define _CET_ENDBR\n+#endif"}, {"sha": "626777d694370aaf9ecd8e70ccf6e38b5f72d728", "filename": "libsanitizer/sanitizer_common/sanitizer_chained_origin_depot.cpp", "status": "modified", "additions": 58, "deletions": 22, "changes": 80, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_chained_origin_depot.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_chained_origin_depot.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_chained_origin_depot.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -11,16 +11,58 @@\n \n #include \"sanitizer_chained_origin_depot.h\"\n \n+#include \"sanitizer_persistent_allocator.h\"\n+#include \"sanitizer_stackdepotbase.h\"\n+\n namespace __sanitizer {\n \n-bool ChainedOriginDepot::ChainedOriginDepotNode::eq(\n-    hash_type hash, const args_type &args) const {\n-  return here_id == args.here_id && prev_id == args.prev_id;\n-}\n+namespace {\n+struct ChainedOriginDepotDesc {\n+  u32 here_id;\n+  u32 prev_id;\n+};\n \n-uptr ChainedOriginDepot::ChainedOriginDepotNode::storage_size(\n-    const args_type &args) {\n-  return sizeof(ChainedOriginDepotNode);\n+struct ChainedOriginDepotNode {\n+  using hash_type = u32;\n+  u32 link;\n+  u32 here_id;\n+  u32 prev_id;\n+\n+  typedef ChainedOriginDepotDesc args_type;\n+\n+  bool eq(hash_type hash, const args_type &args) const;\n+\n+  static uptr allocated() { return 0; }\n+\n+  static hash_type hash(const args_type &args);\n+\n+  static bool is_valid(const args_type &args);\n+\n+  void store(u32 id, const args_type &args, hash_type other_hash);\n+\n+  args_type load(u32 id) const;\n+\n+  struct Handle {\n+    const ChainedOriginDepotNode *node_ = nullptr;\n+    u32 id_ = 0;\n+    Handle(const ChainedOriginDepotNode *node, u32 id) : node_(node), id_(id) {}\n+    bool valid() const { return node_; }\n+    u32 id() const { return id_; }\n+    int here_id() const { return node_->here_id; }\n+    int prev_id() const { return node_->prev_id; }\n+  };\n+\n+  static Handle get_handle(u32 id);\n+\n+  typedef Handle handle_type;\n+};\n+\n+}  // namespace\n+\n+static StackDepotBase<ChainedOriginDepotNode, 4, 20> depot;\n+\n+bool ChainedOriginDepotNode::eq(hash_type hash, const args_type &args) const {\n+  return here_id == args.here_id && prev_id == args.prev_id;\n }\n \n /* This is murmur2 hash for the 64->32 bit case.\n@@ -36,8 +78,8 @@ uptr ChainedOriginDepot::ChainedOriginDepotNode::storage_size(\n    split, or one of two reserved values (-1) or (-2). Either case can\n    dominate depending on the workload.\n */\n-ChainedOriginDepot::ChainedOriginDepotNode::hash_type\n-ChainedOriginDepot::ChainedOriginDepotNode::hash(const args_type &args) {\n+ChainedOriginDepotNode::hash_type ChainedOriginDepotNode::hash(\n+    const args_type &args) {\n   const u32 m = 0x5bd1e995;\n   const u32 seed = 0x9747b28c;\n   const u32 r = 24;\n@@ -62,26 +104,21 @@ ChainedOriginDepot::ChainedOriginDepotNode::hash(const args_type &args) {\n   return h;\n }\n \n-bool ChainedOriginDepot::ChainedOriginDepotNode::is_valid(\n-    const args_type &args) {\n-  return true;\n-}\n+bool ChainedOriginDepotNode::is_valid(const args_type &args) { return true; }\n \n-void ChainedOriginDepot::ChainedOriginDepotNode::store(const args_type &args,\n-                                                       hash_type other_hash) {\n+void ChainedOriginDepotNode::store(u32 id, const args_type &args,\n+                                   hash_type other_hash) {\n   here_id = args.here_id;\n   prev_id = args.prev_id;\n }\n \n-ChainedOriginDepot::ChainedOriginDepotNode::args_type\n-ChainedOriginDepot::ChainedOriginDepotNode::load() const {\n+ChainedOriginDepotNode::args_type ChainedOriginDepotNode::load(u32 id) const {\n   args_type ret = {here_id, prev_id};\n   return ret;\n }\n \n-ChainedOriginDepot::ChainedOriginDepotNode::Handle\n-ChainedOriginDepot::ChainedOriginDepotNode::get_handle() {\n-  return Handle(this);\n+ChainedOriginDepotNode::Handle ChainedOriginDepotNode::get_handle(u32 id) {\n+  return Handle(&depot.nodes[id], id);\n }\n \n ChainedOriginDepot::ChainedOriginDepot() {}\n@@ -93,8 +130,7 @@ StackDepotStats ChainedOriginDepot::GetStats() const {\n bool ChainedOriginDepot::Put(u32 here_id, u32 prev_id, u32 *new_id) {\n   ChainedOriginDepotDesc desc = {here_id, prev_id};\n   bool inserted;\n-  ChainedOriginDepotNode::Handle h = depot.Put(desc, &inserted);\n-  *new_id = h.valid() ? h.id() : 0;\n+  *new_id = depot.Put(desc, &inserted);\n   return inserted;\n }\n "}, {"sha": "2e800964a45dd83a94a354e0c8e3d420a2d37e9e", "filename": "libsanitizer/sanitizer_common/sanitizer_chained_origin_depot.h", "status": "modified", "additions": 0, "deletions": 44, "changes": 44, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_chained_origin_depot.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_chained_origin_depot.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_chained_origin_depot.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -13,7 +13,6 @@\n #define SANITIZER_CHAINED_ORIGIN_DEPOT_H\n \n #include \"sanitizer_common.h\"\n-#include \"sanitizer_stackdepotbase.h\"\n \n namespace __sanitizer {\n \n@@ -37,49 +36,6 @@ class ChainedOriginDepot {\n   void UnlockAll();\n \n  private:\n-  struct ChainedOriginDepotDesc {\n-    u32 here_id;\n-    u32 prev_id;\n-  };\n-\n-  struct ChainedOriginDepotNode {\n-    using hash_type = u32;\n-    ChainedOriginDepotNode *link;\n-    u32 id;\n-    u32 here_id;\n-    u32 prev_id;\n-\n-    typedef ChainedOriginDepotDesc args_type;\n-\n-    bool eq(hash_type hash, const args_type &args) const;\n-\n-    static uptr storage_size(const args_type &args);\n-\n-    static hash_type hash(const args_type &args);\n-\n-    static bool is_valid(const args_type &args);\n-\n-    void store(const args_type &args, hash_type other_hash);\n-\n-    args_type load() const;\n-\n-    struct Handle {\n-      ChainedOriginDepotNode *node_;\n-      Handle() : node_(nullptr) {}\n-      explicit Handle(ChainedOriginDepotNode *node) : node_(node) {}\n-      bool valid() { return node_; }\n-      u32 id() { return node_->id; }\n-      int here_id() { return node_->here_id; }\n-      int prev_id() { return node_->prev_id; }\n-    };\n-\n-    Handle get_handle();\n-\n-    typedef Handle handle_type;\n-  };\n-\n-  StackDepotBase<ChainedOriginDepotNode, 4, 20> depot;\n-\n   ChainedOriginDepot(const ChainedOriginDepot &) = delete;\n   void operator=(const ChainedOriginDepot &) = delete;\n };"}, {"sha": "065154496eb5fbed92b8f46e4dc48f349fc3d0ff", "filename": "libsanitizer/sanitizer_common/sanitizer_common.h", "status": "modified", "additions": 15, "deletions": 13, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_common.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -192,12 +192,13 @@ class ReservedAddressRange {\n };\n \n typedef void (*fill_profile_f)(uptr start, uptr rss, bool file,\n-                               /*out*/uptr *stats, uptr stats_size);\n+                               /*out*/ uptr *stats);\n \n // Parse the contents of /proc/self/smaps and generate a memory profile.\n-// |cb| is a tool-specific callback that fills the |stats| array containing\n-// |stats_size| elements.\n-void GetMemoryProfile(fill_profile_f cb, uptr *stats, uptr stats_size);\n+// |cb| is a tool-specific callback that fills the |stats| array.\n+void GetMemoryProfile(fill_profile_f cb, uptr *stats);\n+void ParseUnixMemoryProfile(fill_profile_f cb, uptr *stats, char *smaps,\n+                            uptr smaps_len);\n \n // Simple low-level (mmap-based) allocator for internal use. Doesn't have\n // constructor, so all instances of LowLevelAllocator should be\n@@ -371,7 +372,7 @@ void ReportErrorSummary(const char *error_type, const AddressInfo &info,\n void ReportErrorSummary(const char *error_type, const StackTrace *trace,\n                         const char *alt_tool_name = nullptr);\n \n-void ReportMmapWriteExec(int prot);\n+void ReportMmapWriteExec(int prot, int mflags);\n \n // Math\n #if SANITIZER_WINDOWS && !defined(__clang__) && !defined(__GNUC__)\n@@ -419,9 +420,7 @@ inline uptr LeastSignificantSetBitIndex(uptr x) {\n   return up;\n }\n \n-inline bool IsPowerOfTwo(uptr x) {\n-  return (x & (x - 1)) == 0;\n-}\n+inline constexpr bool IsPowerOfTwo(uptr x) { return (x & (x - 1)) == 0; }\n \n inline uptr RoundUpToPowerOfTwo(uptr size) {\n   CHECK(size);\n@@ -433,16 +432,16 @@ inline uptr RoundUpToPowerOfTwo(uptr size) {\n   return 1ULL << (up + 1);\n }\n \n-inline uptr RoundUpTo(uptr size, uptr boundary) {\n+inline constexpr uptr RoundUpTo(uptr size, uptr boundary) {\n   RAW_CHECK(IsPowerOfTwo(boundary));\n   return (size + boundary - 1) & ~(boundary - 1);\n }\n \n-inline uptr RoundDownTo(uptr x, uptr boundary) {\n+inline constexpr uptr RoundDownTo(uptr x, uptr boundary) {\n   return x & ~(boundary - 1);\n }\n \n-inline bool IsAligned(uptr a, uptr alignment) {\n+inline constexpr bool IsAligned(uptr a, uptr alignment) {\n   return (a & (alignment - 1)) == 0;\n }\n \n@@ -722,12 +721,15 @@ void SortAndDedup(Container &v, Compare comp = {}) {\n   v.resize(last + 1);\n }\n \n+constexpr uptr kDefaultFileMaxSize = FIRST_32_SECOND_64(1 << 26, 1 << 28);\n+\n // Opens the file 'file_name\" and reads up to 'max_len' bytes.\n // The resulting buffer is mmaped and stored in '*buff'.\n // Returns true if file was successfully opened and read.\n bool ReadFileToVector(const char *file_name,\n                       InternalMmapVectorNoCtor<char> *buff,\n-                      uptr max_len = 1 << 26, error_t *errno_p = nullptr);\n+                      uptr max_len = kDefaultFileMaxSize,\n+                      error_t *errno_p = nullptr);\n \n // Opens the file 'file_name\" and reads up to 'max_len' bytes.\n // This function is less I/O efficient than ReadFileToVector as it may reread\n@@ -738,7 +740,7 @@ bool ReadFileToVector(const char *file_name,\n // The total number of read bytes is stored in '*read_len'.\n // Returns true if file was successfully opened and read.\n bool ReadFileToBuffer(const char *file_name, char **buff, uptr *buff_size,\n-                      uptr *read_len, uptr max_len = 1 << 26,\n+                      uptr *read_len, uptr max_len = kDefaultFileMaxSize,\n                       error_t *errno_p = nullptr);\n \n // When adding a new architecture, don't forget to also update"}, {"sha": "abb38ccfa15d21740b466dc17b80031e1211257c", "filename": "libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc", "status": "modified", "additions": 76, "deletions": 14, "changes": 90, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_interceptors.inc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_interceptors.inc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_common_interceptors.inc?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -2422,6 +2422,60 @@ INTERCEPTOR(int, glob64, const char *pattern, int flags,\n #define INIT_GLOB64\n #endif  // SANITIZER_INTERCEPT_GLOB64\n \n+#if SANITIZER_INTERCEPT_POSIX_SPAWN\n+\n+template <class RealSpawnPtr>\n+static int PosixSpawnImpl(void *ctx, RealSpawnPtr *real_posix_spawn, pid_t *pid,\n+                          const char *file_or_path, const void *file_actions,\n+                          const void *attrp, char *const argv[],\n+                          char *const envp[]) {\n+  COMMON_INTERCEPTOR_READ_RANGE(ctx, file_or_path,\n+                                internal_strlen(file_or_path) + 1);\n+  if (argv) {\n+    for (char *const *s = argv; ; ++s) {\n+      COMMON_INTERCEPTOR_READ_RANGE(ctx, s, sizeof(*s));\n+      if (!*s) break;\n+      COMMON_INTERCEPTOR_READ_RANGE(ctx, *s, internal_strlen(*s) + 1);\n+    }\n+  }\n+  if (envp) {\n+    for (char *const *s = envp; ; ++s) {\n+      COMMON_INTERCEPTOR_READ_RANGE(ctx, s, sizeof(*s));\n+      if (!*s) break;\n+      COMMON_INTERCEPTOR_READ_RANGE(ctx, *s, internal_strlen(*s) + 1);\n+    }\n+  }\n+  int res =\n+      real_posix_spawn(pid, file_or_path, file_actions, attrp, argv, envp);\n+  if (res == 0)\n+    COMMON_INTERCEPTOR_WRITE_RANGE(ctx, pid, sizeof(*pid));\n+  return res;\n+}\n+INTERCEPTOR(int, posix_spawn, pid_t *pid, const char *path,\n+            const void *file_actions, const void *attrp, char *const argv[],\n+            char *const envp[]) {\n+  void *ctx;\n+  COMMON_INTERCEPTOR_ENTER(ctx, posix_spawn, pid, path, file_actions, attrp,\n+                           argv, envp);\n+  return PosixSpawnImpl(ctx, REAL(posix_spawn), pid, path, file_actions, attrp,\n+                        argv, envp);\n+}\n+INTERCEPTOR(int, posix_spawnp, pid_t *pid, const char *file,\n+            const void *file_actions, const void *attrp, char *const argv[],\n+            char *const envp[]) {\n+  void *ctx;\n+  COMMON_INTERCEPTOR_ENTER(ctx, posix_spawnp, pid, file, file_actions, attrp,\n+                           argv, envp);\n+  return PosixSpawnImpl(ctx, REAL(posix_spawnp), pid, file, file_actions, attrp,\n+                        argv, envp);\n+}\n+#  define INIT_POSIX_SPAWN                  \\\n+    COMMON_INTERCEPT_FUNCTION(posix_spawn); \\\n+    COMMON_INTERCEPT_FUNCTION(posix_spawnp);\n+#else  // SANITIZER_INTERCEPT_POSIX_SPAWN\n+#  define INIT_POSIX_SPAWN\n+#endif  // SANITIZER_INTERCEPT_POSIX_SPAWN\n+\n #if SANITIZER_INTERCEPT_WAIT\n // According to sys/wait.h, wait(), waitid(), waitpid() may have symbol version\n // suffixes on Darwin. See the declaration of INTERCEPTOR_WITH_SUFFIX for\n@@ -2658,17 +2712,20 @@ INTERCEPTOR(int, getnameinfo, void *sockaddr, unsigned salen, char *host,\n #endif\n \n #if SANITIZER_INTERCEPT_GETSOCKNAME\n-INTERCEPTOR(int, getsockname, int sock_fd, void *addr, int *addrlen) {\n+INTERCEPTOR(int, getsockname, int sock_fd, void *addr, unsigned *addrlen) {\n   void *ctx;\n   COMMON_INTERCEPTOR_ENTER(ctx, getsockname, sock_fd, addr, addrlen);\n-  COMMON_INTERCEPTOR_READ_RANGE(ctx, addrlen, sizeof(*addrlen));\n-  int addrlen_in = *addrlen;\n+  unsigned addr_sz;\n+  if (addrlen) {\n+    COMMON_INTERCEPTOR_READ_RANGE(ctx, addrlen, sizeof(*addrlen));\n+    addr_sz = *addrlen;\n+  }\n   // FIXME: under ASan the call below may write to freed memory and corrupt\n   // its metadata. See\n   // https://github.com/google/sanitizers/issues/321.\n   int res = REAL(getsockname)(sock_fd, addr, addrlen);\n-  if (res == 0) {\n-    COMMON_INTERCEPTOR_WRITE_RANGE(ctx, addr, Min(addrlen_in, *addrlen));\n+  if (!res && addr && addrlen) {\n+    COMMON_INTERCEPTOR_WRITE_RANGE(ctx, addr, Min(addr_sz, *addrlen));\n   }\n   return res;\n }\n@@ -3173,13 +3230,17 @@ INTERCEPTOR(int, getpeername, int sockfd, void *addr, unsigned *addrlen) {\n   void *ctx;\n   COMMON_INTERCEPTOR_ENTER(ctx, getpeername, sockfd, addr, addrlen);\n   unsigned addr_sz;\n-  if (addrlen) addr_sz = *addrlen;\n+  if (addrlen) {\n+    COMMON_INTERCEPTOR_READ_RANGE(ctx, addrlen, sizeof(*addrlen));\n+    addr_sz = *addrlen;\n+  }\n   // FIXME: under ASan the call below may write to freed memory and corrupt\n   // its metadata. See\n   // https://github.com/google/sanitizers/issues/321.\n   int res = REAL(getpeername)(sockfd, addr, addrlen);\n-  if (!res && addr && addrlen)\n+  if (!res && addr && addrlen) {\n     COMMON_INTERCEPTOR_WRITE_RANGE(ctx, addr, Min(addr_sz, *addrlen));\n+  }\n   return res;\n }\n #define INIT_GETPEERNAME COMMON_INTERCEPT_FUNCTION(getpeername);\n@@ -7418,7 +7479,7 @@ INTERCEPTOR(void *, mmap, void *addr, SIZE_T sz, int prot, int flags, int fd,\n             OFF_T off) {\n   void *ctx;\n   if (common_flags()->detect_write_exec)\n-    ReportMmapWriteExec(prot);\n+    ReportMmapWriteExec(prot, flags);\n   if (COMMON_INTERCEPTOR_NOTHING_IS_INITIALIZED)\n     return (void *)internal_mmap(addr, sz, prot, flags, fd, off);\n   COMMON_INTERCEPTOR_ENTER(ctx, mmap, addr, sz, prot, flags, fd, off);\n@@ -7428,7 +7489,7 @@ INTERCEPTOR(void *, mmap, void *addr, SIZE_T sz, int prot, int flags, int fd,\n INTERCEPTOR(int, mprotect, void *addr, SIZE_T sz, int prot) {\n   void *ctx;\n   if (common_flags()->detect_write_exec)\n-    ReportMmapWriteExec(prot);\n+    ReportMmapWriteExec(prot, 0);\n   if (COMMON_INTERCEPTOR_NOTHING_IS_INITIALIZED)\n     return (int)internal_mprotect(addr, sz, prot);\n   COMMON_INTERCEPTOR_ENTER(ctx, mprotect, addr, sz, prot);\n@@ -7447,7 +7508,7 @@ INTERCEPTOR(void *, mmap64, void *addr, SIZE_T sz, int prot, int flags, int fd,\n             OFF64_T off) {\n   void *ctx;\n   if (common_flags()->detect_write_exec)\n-    ReportMmapWriteExec(prot);\n+    ReportMmapWriteExec(prot, flags);\n   if (COMMON_INTERCEPTOR_NOTHING_IS_INITIALIZED)\n     return (void *)internal_mmap(addr, sz, prot, flags, fd, off);\n   COMMON_INTERCEPTOR_ENTER(ctx, mmap64, addr, sz, prot, flags, fd, off);\n@@ -9033,10 +9094,10 @@ INTERCEPTOR(char *, MD2Data, const unsigned char *data, unsigned int len,\n     return ret; \\\n   }\n \n-SHA2_INTERCEPTORS(224, u32);\n-SHA2_INTERCEPTORS(256, u32);\n-SHA2_INTERCEPTORS(384, u64);\n-SHA2_INTERCEPTORS(512, u64);\n+SHA2_INTERCEPTORS(224, u32)\n+SHA2_INTERCEPTORS(256, u32)\n+SHA2_INTERCEPTORS(384, u64)\n+SHA2_INTERCEPTORS(512, u64)\n \n #define INIT_SHA2_INTECEPTORS(LEN) \\\n   COMMON_INTERCEPT_FUNCTION(SHA##LEN##_Init); \\\n@@ -10229,6 +10290,7 @@ static void InitializeCommonInterceptors() {\n   INIT_TIME;\n   INIT_GLOB;\n   INIT_GLOB64;\n+  INIT_POSIX_SPAWN;\n   INIT_WAIT;\n   INIT_WAIT4;\n   INIT_INET;"}, {"sha": "05fb554d20c146b6d417a298570582c10cafef01", "filename": "libsanitizer/sanitizer_common/sanitizer_flat_map.h", "status": "added", "additions": 173, "deletions": 0, "changes": 173, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_flat_map.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_flat_map.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_flat_map.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -0,0 +1,173 @@\n+//===-- sanitizer_flat_map.h ------------------------------------*- C++ -*-===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+//\n+// Part of the Sanitizer Allocator.\n+//\n+//===----------------------------------------------------------------------===//\n+\n+#ifndef SANITIZER_FLAT_MAP_H\n+#define SANITIZER_FLAT_MAP_H\n+\n+#include \"sanitizer_atomic.h\"\n+#include \"sanitizer_common.h\"\n+#include \"sanitizer_internal_defs.h\"\n+#include \"sanitizer_local_address_space_view.h\"\n+#include \"sanitizer_mutex.h\"\n+\n+namespace __sanitizer {\n+\n+// Call these callbacks on mmap/munmap.\n+struct NoOpMapUnmapCallback {\n+  void OnMap(uptr p, uptr size) const {}\n+  void OnUnmap(uptr p, uptr size) const {}\n+};\n+\n+// Maps integers in rage [0, kSize) to values.\n+template <typename T, u64 kSize,\n+          typename AddressSpaceViewTy = LocalAddressSpaceView>\n+class FlatMap {\n+ public:\n+  using AddressSpaceView = AddressSpaceViewTy;\n+  void Init() { internal_memset(map_, 0, sizeof(map_)); }\n+\n+  constexpr uptr size() const { return kSize; }\n+\n+  bool contains(uptr idx) const {\n+    CHECK_LT(idx, kSize);\n+    return true;\n+  }\n+\n+  T &operator[](uptr idx) {\n+    DCHECK_LT(idx, kSize);\n+    return map_[idx];\n+  }\n+\n+  const T &operator[](uptr idx) const {\n+    DCHECK_LT(idx, kSize);\n+    return map_[idx];\n+  }\n+\n+ private:\n+  T map_[kSize];\n+};\n+\n+// TwoLevelMap maps integers in range [0, kSize1*kSize2) to values.\n+// It is implemented as a two-dimensional array: array of kSize1 pointers\n+// to kSize2-byte arrays. The secondary arrays are mmaped on demand.\n+// Each value is initially zero and can be set to something else only once.\n+// Setting and getting values from multiple threads is safe w/o extra locking.\n+template <typename T, u64 kSize1, u64 kSize2,\n+          typename AddressSpaceViewTy = LocalAddressSpaceView,\n+          class MapUnmapCallback = NoOpMapUnmapCallback>\n+class TwoLevelMap {\n+  static_assert(IsPowerOfTwo(kSize2), \"Use a power of two for performance.\");\n+\n+ public:\n+  using AddressSpaceView = AddressSpaceViewTy;\n+  void Init() {\n+    mu_.Init();\n+    internal_memset(map1_, 0, sizeof(map1_));\n+  }\n+\n+  void TestOnlyUnmap() {\n+    for (uptr i = 0; i < kSize1; i++) {\n+      T *p = Get(i);\n+      if (!p)\n+        continue;\n+      MapUnmapCallback().OnUnmap(reinterpret_cast<uptr>(p), MmapSize());\n+      UnmapOrDie(p, kSize2);\n+    }\n+    Init();\n+  }\n+\n+  uptr MemoryUsage() const {\n+    uptr res = 0;\n+    for (uptr i = 0; i < kSize1; i++) {\n+      T *p = Get(i);\n+      if (!p)\n+        continue;\n+      res += MmapSize();\n+    }\n+    return res;\n+  }\n+\n+  constexpr uptr size() const { return kSize1 * kSize2; }\n+  constexpr uptr size1() const { return kSize1; }\n+  constexpr uptr size2() const { return kSize2; }\n+\n+  bool contains(uptr idx) const {\n+    CHECK_LT(idx, kSize1 * kSize2);\n+    return Get(idx / kSize2);\n+  }\n+\n+  const T &operator[](uptr idx) const {\n+    DCHECK_LT(idx, kSize1 * kSize2);\n+    T *map2 = GetOrCreate(idx / kSize2);\n+    return *AddressSpaceView::Load(&map2[idx % kSize2]);\n+  }\n+\n+  T &operator[](uptr idx) {\n+    DCHECK_LT(idx, kSize1 * kSize2);\n+    T *map2 = GetOrCreate(idx / kSize2);\n+    return *AddressSpaceView::LoadWritable(&map2[idx % kSize2]);\n+  }\n+\n+ private:\n+  constexpr uptr MmapSize() const {\n+    return RoundUpTo(kSize2 * sizeof(T), GetPageSizeCached());\n+  }\n+\n+  T *Get(uptr idx) const {\n+    DCHECK_LT(idx, kSize1);\n+    return reinterpret_cast<T *>(\n+        atomic_load(&map1_[idx], memory_order_acquire));\n+  }\n+\n+  T *GetOrCreate(uptr idx) const {\n+    DCHECK_LT(idx, kSize1);\n+    // This code needs to use memory_order_acquire/consume, but we use\n+    // memory_order_relaxed for performance reasons (matters for arm64). We\n+    // expect memory_order_relaxed to be effectively equivalent to\n+    // memory_order_consume in this case for all relevant architectures: all\n+    // dependent data is reachable only by dereferencing the resulting pointer.\n+    // If relaxed load fails to see stored ptr, the code will fall back to\n+    // Create() and reload the value again with locked mutex as a memory\n+    // barrier.\n+    T *res = reinterpret_cast<T *>(atomic_load_relaxed(&map1_[idx]));\n+    if (LIKELY(res))\n+      return res;\n+    return Create(idx);\n+  }\n+\n+  NOINLINE T *Create(uptr idx) const {\n+    SpinMutexLock l(&mu_);\n+    T *res = Get(idx);\n+    if (!res) {\n+      res = reinterpret_cast<T *>(MmapOrDie(MmapSize(), \"TwoLevelMap\"));\n+      MapUnmapCallback().OnMap(reinterpret_cast<uptr>(res), kSize2);\n+      atomic_store(&map1_[idx], reinterpret_cast<uptr>(res),\n+                   memory_order_release);\n+    }\n+    return res;\n+  }\n+\n+  mutable StaticSpinMutex mu_;\n+  mutable atomic_uintptr_t map1_[kSize1];\n+};\n+\n+template <u64 kSize, typename AddressSpaceViewTy = LocalAddressSpaceView>\n+using FlatByteMap = FlatMap<u8, kSize, AddressSpaceViewTy>;\n+\n+template <u64 kSize1, u64 kSize2,\n+          typename AddressSpaceViewTy = LocalAddressSpaceView,\n+          class MapUnmapCallback = NoOpMapUnmapCallback>\n+using TwoLevelByteMap =\n+    TwoLevelMap<u8, kSize1, kSize2, AddressSpaceViewTy, MapUnmapCallback>;\n+}  // namespace __sanitizer\n+\n+#endif"}, {"sha": "c7b30d9883652330f971aeba1421130902f4e059", "filename": "libsanitizer/sanitizer_common/sanitizer_fuchsia.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_fuchsia.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_fuchsia.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_fuchsia.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -372,7 +372,7 @@ bool IsAccessibleMemoryRange(uptr beg, uptr size) {\n }\n \n // FIXME implement on this platform.\n-void GetMemoryProfile(fill_profile_f cb, uptr *stats, uptr stats_size) {}\n+void GetMemoryProfile(fill_profile_f cb, uptr *stats) {}\n \n bool ReadFileToBuffer(const char *file_name, char **buff, uptr *buff_size,\n                       uptr *read_len, uptr max_len, error_t *errno_p) {"}, {"sha": "7ce9e25da342d8d70312d956a68b137f29326eb8", "filename": "libsanitizer/sanitizer_common/sanitizer_linux_libcdep.cpp", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux_libcdep.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux_libcdep.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_linux_libcdep.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -759,13 +759,9 @@ u32 GetNumberOfCPUs() {\n #elif SANITIZER_SOLARIS\n   return sysconf(_SC_NPROCESSORS_ONLN);\n #else\n-#if defined(CPU_COUNT)\n   cpu_set_t CPUs;\n   CHECK_EQ(sched_getaffinity(0, sizeof(cpu_set_t), &CPUs), 0);\n   return CPU_COUNT(&CPUs);\n-#else\n-  return 1;\n-#endif\n #endif\n }\n "}, {"sha": "36a9d509804216f1e1175a9225340636edf55e44", "filename": "libsanitizer/sanitizer_common/sanitizer_mac.cpp", "status": "modified", "additions": 3, "deletions": 11, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_mac.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_mac.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_mac.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -37,7 +37,7 @@\n extern char **environ;\n #endif\n \n-#if defined(__has_include) && __has_include(<os/trace.h>) && defined(__BLOCKS__)\n+#if defined(__has_include) && __has_include(<os/trace.h>)\n #define SANITIZER_OS_TRACE 1\n #include <os/trace.h>\n #else\n@@ -70,15 +70,7 @@ extern \"C\" {\n #include <mach/mach_time.h>\n #include <mach/vm_statistics.h>\n #include <malloc/malloc.h>\n-#if defined(__has_builtin) && __has_builtin(__builtin_os_log_format)\n-# include <os/log.h>\n-#else\n-   /* Without support for __builtin_os_log_format, fall back to the older\n-      method.  */\n-# define OS_LOG_DEFAULT 0\n-# define os_log_error(A,B,C) \\\n-  asl_log(nullptr, nullptr, ASL_LEVEL_ERR, \"%s\", (C));\n-#endif\n+#include <os/log.h>\n #include <pthread.h>\n #include <sched.h>\n #include <signal.h>\n@@ -1319,7 +1311,7 @@ uptr FindAvailableMemoryRange(uptr size, uptr alignment, uptr left_padding,\n }\n \n // FIXME implement on this platform.\n-void GetMemoryProfile(fill_profile_f cb, uptr *stats, uptr stats_size) { }\n+void GetMemoryProfile(fill_profile_f cb, uptr *stats) {}\n \n void SignalContext::DumpAllRegisters(void *context) {\n   Report(\"Register values:\\n\");"}, {"sha": "0b6af5a3c0edc649c4d65dfd1a6ca1bce13aa9b9", "filename": "libsanitizer/sanitizer_common/sanitizer_mac.h", "status": "modified", "additions": 0, "deletions": 20, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_mac.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_mac.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_mac.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -14,26 +14,6 @@\n \n #include \"sanitizer_common.h\"\n #include \"sanitizer_platform.h\"\n-\n-/* TARGET_OS_OSX is not present in SDKs before Darwin16 (macOS 10.12) use\n-   TARGET_OS_MAC (we have no support for iOS in any form for these versions,\n-   so there's no ambiguity).  */\n-#if !defined(TARGET_OS_OSX) && TARGET_OS_MAC\n-# define TARGET_OS_OSX 1\n-#endif\n-\n-/* Other TARGET_OS_xxx are not present on earlier versions, define them to\n-   0 (we have no support for them; they are not valid targets anyway).  */\n-#ifndef TARGET_OS_IOS\n-#define TARGET_OS_IOS 0\n-#endif\n-#ifndef TARGET_OS_TV\n-#define TARGET_OS_TV 0\n-#endif\n-#ifndef TARGET_OS_WATCH\n-#define TARGET_OS_WATCH 0\n-#endif\n-\n #if SANITIZER_MAC\n #include \"sanitizer_posix.h\"\n "}, {"sha": "40fe56661250e5a94cef4425f265d494bb54a338", "filename": "libsanitizer/sanitizer_common/sanitizer_mutex.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_mutex.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_mutex.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_mutex.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -174,7 +174,7 @@ struct InternalDeadlockDetector {\n     if (max_idx != MutexInvalid && !mutex_can_lock[max_idx][type]) {\n       Printf(\"%s: internal deadlock: can't lock %s under %s mutex\\n\", SanitizerToolName,\n              mutex_meta[type].name, mutex_meta[max_idx].name);\n-      PrintMutexPC(pc);\n+      PrintMutexPC(locked[max_idx].pc);\n       CHECK(0);\n     }\n     locked[type].seq = ++sequence;"}, {"sha": "5ec6efaa6490c84d4f48d79b56ef454fa39856d5", "filename": "libsanitizer/sanitizer_common/sanitizer_mutex.h", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_mutex.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_mutex.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_mutex.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -95,8 +95,11 @@ enum {\n \n // Go linker does not support THREADLOCAL variables,\n // so we can't use per-thread state.\n+// Disable checked locks on Darwin. Although Darwin platforms support\n+// THREADLOCAL variables they are not usable early on during process init when\n+// `__sanitizer::Mutex` is used.\n #define SANITIZER_CHECK_DEADLOCKS \\\n-  (SANITIZER_DEBUG && !SANITIZER_GO && SANITIZER_SUPPORTS_THREADLOCAL)\n+  (SANITIZER_DEBUG && !SANITIZER_GO && SANITIZER_SUPPORTS_THREADLOCAL && !SANITIZER_MAC)\n \n #if SANITIZER_CHECK_DEADLOCKS\n struct MutexMeta {"}, {"sha": "1ca0375b8a54c07ecb5521af54bacb61e5279517", "filename": "libsanitizer/sanitizer_common/sanitizer_persistent_allocator.cpp", "status": "removed", "additions": 0, "deletions": 18, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bb27f5e9ec3c7ab0f5c859d90c59dd4573b53d97/libsanitizer%2Fsanitizer_common%2Fsanitizer_persistent_allocator.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bb27f5e9ec3c7ab0f5c859d90c59dd4573b53d97/libsanitizer%2Fsanitizer_common%2Fsanitizer_persistent_allocator.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_persistent_allocator.cpp?ref=bb27f5e9ec3c7ab0f5c859d90c59dd4573b53d97", "patch": "@@ -1,18 +0,0 @@\n-//===-- sanitizer_persistent_allocator.cpp ----------------------*- C++ -*-===//\n-//\n-// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n-// See https://llvm.org/LICENSE.txt for license information.\n-// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n-//\n-//===----------------------------------------------------------------------===//\n-//\n-// This file is shared between AddressSanitizer and ThreadSanitizer\n-// run-time libraries.\n-//===----------------------------------------------------------------------===//\n-#include \"sanitizer_persistent_allocator.h\"\n-\n-namespace __sanitizer {\n-\n-PersistentAllocator thePersistentAllocator;\n-\n-}  // namespace __sanitizer"}, {"sha": "e18b0030567f63334161791995583f1b6300c562", "filename": "libsanitizer/sanitizer_common/sanitizer_persistent_allocator.h", "status": "modified", "additions": 55, "deletions": 16, "changes": 71, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_persistent_allocator.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_persistent_allocator.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_persistent_allocator.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -20,50 +20,89 @@\n \n namespace __sanitizer {\n \n+template <typename T>\n class PersistentAllocator {\n  public:\n-  void *alloc(uptr size);\n+  T *alloc(uptr count = 1);\n+  uptr allocated() const { return atomic_load_relaxed(&mapped_size); }\n+\n+  void TestOnlyUnmap();\n \n  private:\n-  void *tryAlloc(uptr size);\n-  StaticSpinMutex mtx;  // Protects alloc of new blocks for region allocator.\n+  T *tryAlloc(uptr count);\n+  T *refillAndAlloc(uptr count);\n+  mutable StaticSpinMutex mtx;  // Protects alloc of new blocks.\n   atomic_uintptr_t region_pos;  // Region allocator for Node's.\n   atomic_uintptr_t region_end;\n+  atomic_uintptr_t mapped_size;\n+\n+  struct BlockInfo {\n+    const BlockInfo *next;\n+    uptr ptr;\n+    uptr size;\n+  };\n+  const BlockInfo *curr;\n };\n \n-inline void *PersistentAllocator::tryAlloc(uptr size) {\n+template <typename T>\n+inline T *PersistentAllocator<T>::tryAlloc(uptr count) {\n   // Optimisic lock-free allocation, essentially try to bump the region ptr.\n   for (;;) {\n     uptr cmp = atomic_load(&region_pos, memory_order_acquire);\n     uptr end = atomic_load(&region_end, memory_order_acquire);\n-    if (cmp == 0 || cmp + size > end) return nullptr;\n+    uptr size = count * sizeof(T);\n+    if (cmp == 0 || cmp + size > end)\n+      return nullptr;\n     if (atomic_compare_exchange_weak(&region_pos, &cmp, cmp + size,\n                                      memory_order_acquire))\n-      return (void *)cmp;\n+      return reinterpret_cast<T *>(cmp);\n   }\n }\n \n-inline void *PersistentAllocator::alloc(uptr size) {\n+template <typename T>\n+inline T *PersistentAllocator<T>::alloc(uptr count) {\n   // First, try to allocate optimisitically.\n-  void *s = tryAlloc(size);\n-  if (s) return s;\n+  T *s = tryAlloc(count);\n+  if (LIKELY(s))\n+    return s;\n+  return refillAndAlloc(count);\n+}\n+\n+template <typename T>\n+inline T *PersistentAllocator<T>::refillAndAlloc(uptr count) {\n   // If failed, lock, retry and alloc new superblock.\n   SpinMutexLock l(&mtx);\n   for (;;) {\n-    s = tryAlloc(size);\n-    if (s) return s;\n+    T *s = tryAlloc(count);\n+    if (s)\n+      return s;\n     atomic_store(&region_pos, 0, memory_order_relaxed);\n-    uptr allocsz = 64 * 1024;\n-    if (allocsz < size) allocsz = size;\n+    uptr size = count * sizeof(T) + sizeof(BlockInfo);\n+    uptr allocsz = RoundUpTo(Max<uptr>(size, 64u * 1024u), GetPageSizeCached());\n     uptr mem = (uptr)MmapOrDie(allocsz, \"stack depot\");\n+    BlockInfo *new_block = (BlockInfo *)(mem + allocsz) - 1;\n+    new_block->next = curr;\n+    new_block->ptr = mem;\n+    new_block->size = allocsz;\n+    curr = new_block;\n+\n+    atomic_fetch_add(&mapped_size, allocsz, memory_order_relaxed);\n+\n+    allocsz -= sizeof(BlockInfo);\n     atomic_store(&region_end, mem + allocsz, memory_order_release);\n     atomic_store(&region_pos, mem, memory_order_release);\n   }\n }\n \n-extern PersistentAllocator thePersistentAllocator;\n-inline void *PersistentAlloc(uptr sz) {\n-  return thePersistentAllocator.alloc(sz);\n+template <typename T>\n+void PersistentAllocator<T>::TestOnlyUnmap() {\n+  while (curr) {\n+    uptr mem = curr->ptr;\n+    uptr allocsz = curr->size;\n+    curr = curr->next;\n+    UnmapOrDie((void *)mem, allocsz);\n+  }\n+  internal_memset(this, 0, sizeof(*this));\n }\n \n } // namespace __sanitizer"}, {"sha": "e43fe3a3cf97b355dfdf0ccae19e4a1ffd9bf38c", "filename": "libsanitizer/sanitizer_common/sanitizer_platform_interceptors.h", "status": "modified", "additions": 8, "deletions": 4, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_interceptors.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_interceptors.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_interceptors.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -235,6 +235,7 @@\n #define SANITIZER_INTERCEPT_TIME SI_POSIX\n #define SANITIZER_INTERCEPT_GLOB (SI_GLIBC || SI_SOLARIS)\n #define SANITIZER_INTERCEPT_GLOB64 SI_GLIBC\n+#define SANITIZER_INTERCEPT_POSIX_SPAWN SI_POSIX\n #define SANITIZER_INTERCEPT_WAIT SI_POSIX\n #define SANITIZER_INTERCEPT_INET SI_POSIX\n #define SANITIZER_INTERCEPT_PTHREAD_GETSCHEDPARAM SI_POSIX\n@@ -460,10 +461,13 @@\n #define SANITIZER_INTERCEPT_SEND_SENDTO SI_POSIX\n #define SANITIZER_INTERCEPT_EVENTFD_READ_WRITE SI_LINUX\n \n-#define SANITIZER_INTERCEPT_STAT \\\n-  (SI_FREEBSD || SI_MAC || SI_ANDROID || SI_NETBSD || SI_SOLARIS)\n+#define SI_STAT_LINUX (SI_LINUX && __GLIBC_PREREQ(2, 33))\n+#define SANITIZER_INTERCEPT_STAT                                        \\\n+  (SI_FREEBSD || SI_MAC || SI_ANDROID || SI_NETBSD || SI_SOLARIS ||     \\\n+   SI_STAT_LINUX)\n #define SANITIZER_INTERCEPT_LSTAT (SI_NETBSD || SI_FREEBSD)\n-#define SANITIZER_INTERCEPT___XSTAT (!SANITIZER_INTERCEPT_STAT && SI_POSIX)\n+#define SANITIZER_INTERCEPT___XSTAT                             \\\n+  (!SANITIZER_INTERCEPT_STAT && SI_POSIX) || SI_STAT_LINUX\n #define SANITIZER_INTERCEPT___XSTAT64 SI_LINUX_NOT_ANDROID\n #define SANITIZER_INTERCEPT___LXSTAT SANITIZER_INTERCEPT___XSTAT\n #define SANITIZER_INTERCEPT___LXSTAT64 SI_LINUX_NOT_ANDROID\n@@ -477,7 +481,7 @@\n   (SI_LINUX_NOT_ANDROID || SI_MAC || SI_FREEBSD || SI_NETBSD)\n \n #define SANITIZER_INTERCEPT_MMAP SI_POSIX\n-#define SANITIZER_INTERCEPT_MMAP64 SI_LINUX_NOT_ANDROID\n+#define SANITIZER_INTERCEPT_MMAP64 SI_LINUX_NOT_ANDROID || SI_SOLARIS\n #define SANITIZER_INTERCEPT_MALLOPT_AND_MALLINFO (SI_GLIBC || SI_ANDROID)\n #define SANITIZER_INTERCEPT_MEMALIGN (!SI_FREEBSD && !SI_MAC && !SI_NETBSD)\n #define SANITIZER_INTERCEPT___LIBC_MEMALIGN SI_GLIBC"}, {"sha": "9d577570ea1e2e219b4a08c64f450794d6a145d7", "filename": "libsanitizer/sanitizer_common/sanitizer_platform_limits_linux.cpp", "status": "modified", "additions": 1, "deletions": 4, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_linux.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_linux.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_linux.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -26,10 +26,7 @@\n \n // With old kernels (and even new kernels on powerpc) asm/stat.h uses types that\n // are not defined anywhere in userspace headers. Fake them. This seems to work\n-// fine with newer headers, too.  Beware that with <sys/stat.h>, struct stat\n-// takes the form of struct stat64 on 32-bit platforms if _FILE_OFFSET_BITS=64.\n-// Also, for some platforms (e.g. mips) there are additional members in the\n-// <sys/stat.h> struct stat:s.\n+// fine with newer headers, too.\n #include <linux/posix_types.h>\n #  if defined(__x86_64__) || defined(__mips__) || defined(__hexagon__)\n #    include <sys/stat.h>"}, {"sha": "d69b344dd613d6e2f25c39d839cbf5a899ddf81d", "filename": "libsanitizer/sanitizer_common/sanitizer_platform_limits_posix.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_posix.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_posix.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_platform_limits_posix.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -83,7 +83,7 @@ const unsigned struct_kernel_stat64_sz = 104;\n #elif defined(__mips__)\n const unsigned struct_kernel_stat_sz = SANITIZER_ANDROID\n                                            ? FIRST_32_SECOND_64(104, 128)\n-                                           : FIRST_32_SECOND_64(144, 216);\n+                                           : FIRST_32_SECOND_64(160, 216);\n const unsigned struct_kernel_stat64_sz = 104;\n #elif defined(__s390__) && !defined(__s390x__)\n const unsigned struct_kernel_stat_sz = 64;"}, {"sha": "eb351b0f06fd0b576608ca12725160718eefc39b", "filename": "libsanitizer/sanitizer_common/sanitizer_procmaps_common.cpp", "status": "modified", "additions": 21, "deletions": 5, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_procmaps_common.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_procmaps_common.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_procmaps_common.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -145,28 +145,44 @@ void MemoryMappingLayout::DumpListOfModules(\n   }\n }\n \n-void GetMemoryProfile(fill_profile_f cb, uptr *stats, uptr stats_size) {\n+void GetMemoryProfile(fill_profile_f cb, uptr *stats) {\n   char *smaps = nullptr;\n   uptr smaps_cap = 0;\n   uptr smaps_len = 0;\n   if (!ReadFileToBuffer(\"/proc/self/smaps\", &smaps, &smaps_cap, &smaps_len))\n     return;\n+  ParseUnixMemoryProfile(cb, stats, smaps, smaps_len);\n+  UnmapOrDie(smaps, smaps_cap);\n+}\n+\n+void ParseUnixMemoryProfile(fill_profile_f cb, uptr *stats, char *smaps,\n+                            uptr smaps_len) {\n   uptr start = 0;\n   bool file = false;\n   const char *pos = smaps;\n-  while (pos < smaps + smaps_len) {\n+  char *end = smaps + smaps_len;\n+  if (smaps_len < 2)\n+    return;\n+  // The following parsing can crash on almost every line\n+  // in the case of malformed/truncated input.\n+  // Fixing that is hard b/c e.g. ParseDecimal does not\n+  // even accept end of the buffer and assumes well-formed input.\n+  // So instead we patch end of the input a bit,\n+  // it does not affect well-formed complete inputs.\n+  *--end = 0;\n+  *--end = '\\n';\n+  while (pos < end) {\n     if (IsHex(pos[0])) {\n       start = ParseHex(&pos);\n       for (; *pos != '/' && *pos > '\\n'; pos++) {}\n       file = *pos == '/';\n     } else if (internal_strncmp(pos, \"Rss:\", 4) == 0) {\n-      while (!IsDecimal(*pos)) pos++;\n+      while (pos < end && !IsDecimal(*pos)) pos++;\n       uptr rss = ParseDecimal(&pos) * 1024;\n-      cb(start, rss, file, stats, stats_size);\n+      cb(start, rss, file, stats);\n     }\n     while (*pos++ != '\\n') {}\n   }\n-  UnmapOrDie(smaps, smaps_cap);\n }\n \n } // namespace __sanitizer"}, {"sha": "e16c4e938cb236a286bef0faa980c30b80921a71", "filename": "libsanitizer/sanitizer_common/sanitizer_procmaps_solaris.cpp", "status": "modified", "additions": 9, "deletions": 1, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_procmaps_solaris.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_procmaps_solaris.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_procmaps_solaris.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -55,7 +55,15 @@ bool MemoryMappingLayout::Next(MemoryMappedSegment *segment) {\n \n     internal_snprintf(proc_path, sizeof(proc_path), \"/proc/self/path/%s\",\n                       xmapentry->pr_mapname);\n-    internal_readlink(proc_path, segment->filename, segment->filename_size);\n+    ssize_t sz = internal_readlink(proc_path, segment->filename,\n+                                   segment->filename_size - 1);\n+\n+    // If readlink failed, the map is anonymous.\n+    if (sz == -1) {\n+      segment->filename[0] = '\\0';\n+    } else if ((size_t)sz < segment->filename_size)\n+      // readlink doesn't NUL-terminate.\n+      segment->filename[sz] = '\\0';\n   }\n \n   data_.current += sizeof(prxmap_t);"}, {"sha": "02855459922d250b24dc827f6f7368f24805320a", "filename": "libsanitizer/sanitizer_common/sanitizer_stackdepot.cpp", "status": "modified", "additions": 61, "deletions": 84, "changes": 145, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepot.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepot.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepot.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -14,93 +14,93 @@\n \n #include \"sanitizer_common.h\"\n #include \"sanitizer_hash.h\"\n+#include \"sanitizer_persistent_allocator.h\"\n #include \"sanitizer_stackdepotbase.h\"\n \n namespace __sanitizer {\n \n+static PersistentAllocator<uptr> traceAllocator;\n+\n struct StackDepotNode {\n-  using hash_type = u32;\n-  StackDepotNode *link;\n-  u32 id;\n+  using hash_type = u64;\n   hash_type stack_hash;\n-  u32 size;\n-  atomic_uint32_t tag_and_use_count;  // tag : 12 high bits; use_count : 20;\n-  uptr stack[1];  // [size]\n+  u32 link;\n \n   static const u32 kTabSizeLog = SANITIZER_ANDROID ? 16 : 20;\n-  static const u32 kUseCountBits = 20;\n-  static const u32 kMaxUseCount = 1 << kUseCountBits;\n-  static const u32 kUseCountMask = (1 << kUseCountBits) - 1;\n+  static const u32 kStackSizeBits = 16;\n \n   typedef StackTrace args_type;\n   bool eq(hash_type hash, const args_type &args) const {\n-    u32 tag =\n-        atomic_load(&tag_and_use_count, memory_order_relaxed) >> kUseCountBits;\n-    if (stack_hash != hash || args.size != size || args.tag != tag)\n-      return false;\n-    uptr i = 0;\n-    for (; i < size; i++) {\n-      if (stack[i] != args.trace[i]) return false;\n-    }\n-    return true;\n-  }\n-  static uptr storage_size(const args_type &args) {\n-    return sizeof(StackDepotNode) + (args.size - 1) * sizeof(uptr);\n+    return hash == stack_hash;\n   }\n+  static uptr allocated();\n   static hash_type hash(const args_type &args) {\n-    MurMur2HashBuilder H(args.size * sizeof(uptr));\n+    MurMur2Hash64Builder H(args.size * sizeof(uptr));\n     for (uptr i = 0; i < args.size; i++) H.add(args.trace[i]);\n+    H.add(args.tag);\n     return H.get();\n   }\n   static bool is_valid(const args_type &args) {\n     return args.size > 0 && args.trace;\n   }\n-  void store(const args_type &args, hash_type hash) {\n-    CHECK_EQ(args.tag & (~kUseCountMask >> kUseCountBits), args.tag);\n-    atomic_store(&tag_and_use_count, args.tag << kUseCountBits,\n-                 memory_order_relaxed);\n-    stack_hash = hash;\n-    size = args.size;\n-    internal_memcpy(stack, args.trace, size * sizeof(uptr));\n-  }\n-  args_type load() const {\n-    u32 tag =\n-        atomic_load(&tag_and_use_count, memory_order_relaxed) >> kUseCountBits;\n-    return args_type(&stack[0], size, tag);\n-  }\n-  StackDepotHandle get_handle() { return StackDepotHandle(this); }\n+  void store(u32 id, const args_type &args, hash_type hash);\n+  args_type load(u32 id) const;\n+  static StackDepotHandle get_handle(u32 id);\n \n   typedef StackDepotHandle handle_type;\n };\n \n-COMPILER_CHECK(StackDepotNode::kMaxUseCount >= (u32)kStackDepotMaxUseCount);\n-\n-u32 StackDepotHandle::id() { return node_->id; }\n-int StackDepotHandle::use_count() {\n-  return atomic_load(&node_->tag_and_use_count, memory_order_relaxed) &\n-         StackDepotNode::kUseCountMask;\n-}\n-void StackDepotHandle::inc_use_count_unsafe() {\n-  u32 prev =\n-      atomic_fetch_add(&node_->tag_and_use_count, 1, memory_order_relaxed) &\n-      StackDepotNode::kUseCountMask;\n-  CHECK_LT(prev + 1, StackDepotNode::kMaxUseCount);\n-}\n-\n // FIXME(dvyukov): this single reserved bit is used in TSan.\n typedef StackDepotBase<StackDepotNode, 1, StackDepotNode::kTabSizeLog>\n     StackDepot;\n static StackDepot theDepot;\n+// Keep rarely accessed stack traces out of frequently access nodes to improve\n+// caching efficiency.\n+static TwoLevelMap<uptr *, StackDepot::kNodesSize1, StackDepot::kNodesSize2>\n+    tracePtrs;\n+// Keep mutable data out of frequently access nodes to improve caching\n+// efficiency.\n+static TwoLevelMap<atomic_uint32_t, StackDepot::kNodesSize1,\n+                   StackDepot::kNodesSize2>\n+    useCounts;\n+\n+int StackDepotHandle::use_count() const {\n+  return atomic_load_relaxed(&useCounts[id_]);\n+}\n \n-StackDepotStats StackDepotGetStats() { return theDepot.GetStats(); }\n+void StackDepotHandle::inc_use_count_unsafe() {\n+  atomic_fetch_add(&useCounts[id_], 1, memory_order_relaxed);\n+}\n+\n+uptr StackDepotNode::allocated() {\n+  return traceAllocator.allocated() + tracePtrs.MemoryUsage() +\n+         useCounts.MemoryUsage();\n+}\n \n-u32 StackDepotPut(StackTrace stack) {\n-  StackDepotHandle h = theDepot.Put(stack);\n-  return h.valid() ? h.id() : 0;\n+void StackDepotNode::store(u32 id, const args_type &args, hash_type hash) {\n+  stack_hash = hash;\n+  uptr *stack_trace = traceAllocator.alloc(args.size + 1);\n+  CHECK_LT(args.size, 1 << kStackSizeBits);\n+  *stack_trace = args.size + (args.tag << kStackSizeBits);\n+  internal_memcpy(stack_trace + 1, args.trace, args.size * sizeof(uptr));\n+  tracePtrs[id] = stack_trace;\n }\n \n+StackDepotNode::args_type StackDepotNode::load(u32 id) const {\n+  const uptr *stack_trace = tracePtrs[id];\n+  if (!stack_trace)\n+    return {};\n+  uptr size = *stack_trace & ((1 << kStackSizeBits) - 1);\n+  uptr tag = *stack_trace >> kStackSizeBits;\n+  return args_type(stack_trace + 1, size, tag);\n+}\n+\n+StackDepotStats StackDepotGetStats() { return theDepot.GetStats(); }\n+\n+u32 StackDepotPut(StackTrace stack) { return theDepot.Put(stack); }\n+\n StackDepotHandle StackDepotPut_WithHandle(StackTrace stack) {\n-  return theDepot.Put(stack);\n+  return StackDepotNode::get_handle(theDepot.Put(stack));\n }\n \n StackTrace StackDepotGet(u32 id) {\n@@ -121,37 +121,14 @@ void StackDepotPrintAll() {\n #endif\n }\n \n-bool StackDepotReverseMap::IdDescPair::IdComparator(\n-    const StackDepotReverseMap::IdDescPair &a,\n-    const StackDepotReverseMap::IdDescPair &b) {\n-  return a.id < b.id;\n-}\n-\n-void StackDepotReverseMap::Init() const {\n-  if (LIKELY(map_.capacity()))\n-    return;\n-  map_.reserve(StackDepotGetStats().n_uniq_ids + 100);\n-  for (int idx = 0; idx < StackDepot::kTabSize; idx++) {\n-    atomic_uintptr_t *p = &theDepot.tab[idx];\n-    uptr v = atomic_load(p, memory_order_consume);\n-    StackDepotNode *s = (StackDepotNode*)(v & ~1);\n-    for (; s; s = s->link) {\n-      IdDescPair pair = {s->id, s};\n-      map_.push_back(pair);\n-    }\n-  }\n-  Sort(map_.data(), map_.size(), &IdDescPair::IdComparator);\n+StackDepotHandle StackDepotNode::get_handle(u32 id) {\n+  return StackDepotHandle(&theDepot.nodes[id], id);\n }\n \n-StackTrace StackDepotReverseMap::Get(u32 id) const {\n-  Init();\n-  if (!map_.size())\n-    return StackTrace();\n-  IdDescPair pair = {id, nullptr};\n-  uptr idx = InternalLowerBound(map_, pair, IdDescPair::IdComparator);\n-  if (idx > map_.size() || map_[idx].id != id)\n-    return StackTrace();\n-  return map_[idx].desc->load();\n+void StackDepotTestOnlyUnmap() {\n+  theDepot.TestOnlyUnmap();\n+  tracePtrs.TestOnlyUnmap();\n+  traceAllocator.TestOnlyUnmap();\n }\n \n } // namespace __sanitizer"}, {"sha": "56d655d9404ca2e53470d1a2013e6ef8f10e6b23", "filename": "libsanitizer/sanitizer_common/sanitizer_stackdepot.h", "status": "modified", "additions": 7, "deletions": 31, "changes": 38, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepot.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepot.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepot.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -22,12 +22,12 @@ namespace __sanitizer {\n // StackDepot efficiently stores huge amounts of stack traces.\n struct StackDepotNode;\n struct StackDepotHandle {\n-  StackDepotNode *node_;\n-  StackDepotHandle() : node_(nullptr) {}\n-  explicit StackDepotHandle(StackDepotNode *node) : node_(node) {}\n-  bool valid() { return node_; }\n-  u32 id();\n-  int use_count();\n+  StackDepotNode *node_ = nullptr;\n+  u32 id_ = 0;\n+  StackDepotHandle(StackDepotNode *node, u32 id) : node_(node), id_(id) {}\n+  bool valid() const { return node_; }\n+  u32 id() const { return id_; }\n+  int use_count() const;\n   void inc_use_count_unsafe();\n };\n \n@@ -43,31 +43,7 @@ void StackDepotLockAll();\n void StackDepotUnlockAll();\n void StackDepotPrintAll();\n \n-// Instantiating this class creates a snapshot of StackDepot which can be\n-// efficiently queried with StackDepotGet(). You can use it concurrently with\n-// StackDepot, but the snapshot is only guaranteed to contain those stack traces\n-// which were stored before it was instantiated.\n-class StackDepotReverseMap {\n- public:\n-  StackDepotReverseMap() = default;\n-  StackTrace Get(u32 id) const;\n-\n- private:\n-  struct IdDescPair {\n-    u32 id;\n-    StackDepotNode *desc;\n-\n-    static bool IdComparator(const IdDescPair &a, const IdDescPair &b);\n-  };\n-\n-  void Init() const;\n-\n-  mutable InternalMmapVector<IdDescPair> map_;\n-\n-  // Disallow evil constructors.\n-  StackDepotReverseMap(const StackDepotReverseMap&);\n-  void operator=(const StackDepotReverseMap&);\n-};\n+void StackDepotTestOnlyUnmap();\n \n } // namespace __sanitizer\n "}, {"sha": "96d1ddc87fd032d2a6a5746366e652d7231e8213", "filename": "libsanitizer/sanitizer_common/sanitizer_stackdepotbase.h", "status": "modified", "additions": 84, "deletions": 86, "changes": 170, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepotbase.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepotbase.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_stackdepotbase.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -16,72 +16,87 @@\n #include <stdio.h>\n \n #include \"sanitizer_atomic.h\"\n+#include \"sanitizer_flat_map.h\"\n #include \"sanitizer_internal_defs.h\"\n #include \"sanitizer_mutex.h\"\n-#include \"sanitizer_persistent_allocator.h\"\n \n namespace __sanitizer {\n \n template <class Node, int kReservedBits, int kTabSizeLog>\n class StackDepotBase {\n+  static constexpr u32 kIdSizeLog =\n+      sizeof(u32) * 8 - Max(kReservedBits, 1 /* At least 1 bit for locking. */);\n+  static constexpr u32 kNodesSize1Log = kIdSizeLog / 2;\n+  static constexpr u32 kNodesSize2Log = kIdSizeLog - kNodesSize1Log;\n+  static constexpr int kTabSize = 1 << kTabSizeLog;  // Hash table size.\n+  static constexpr u32 kUnlockMask = (1ull << kIdSizeLog) - 1;\n+  static constexpr u32 kLockMask = ~kUnlockMask;\n+\n  public:\n   typedef typename Node::args_type args_type;\n   typedef typename Node::handle_type handle_type;\n   typedef typename Node::hash_type hash_type;\n+\n+  static constexpr u64 kNodesSize1 = 1ull << kNodesSize1Log;\n+  static constexpr u64 kNodesSize2 = 1ull << kNodesSize2Log;\n+\n   // Maps stack trace to an unique id.\n-  handle_type Put(args_type args, bool *inserted = nullptr);\n+  u32 Put(args_type args, bool *inserted = nullptr);\n   // Retrieves a stored stack trace by the id.\n   args_type Get(u32 id);\n \n-  StackDepotStats GetStats() const { return stats; }\n+  StackDepotStats GetStats() const {\n+    return {\n+        atomic_load_relaxed(&n_uniq_ids),\n+        nodes.MemoryUsage() + Node::allocated(),\n+    };\n+  }\n \n   void LockAll();\n   void UnlockAll();\n   void PrintAll();\n \n- private:\n-  static Node *find(Node *s, args_type args, hash_type hash);\n-  static Node *lock(atomic_uintptr_t *p);\n-  static void unlock(atomic_uintptr_t *p, Node *s);\n+  void TestOnlyUnmap() {\n+    nodes.TestOnlyUnmap();\n+    internal_memset(this, 0, sizeof(*this));\n+  }\n \n-  static const int kTabSize = 1 << kTabSizeLog;  // Hash table size.\n-  static const int kPartBits = 8;\n-  static const int kPartShift = sizeof(u32) * 8 - kPartBits - kReservedBits;\n-  static const int kPartCount =\n-      1 << kPartBits;  // Number of subparts in the table.\n-  static const int kPartSize = kTabSize / kPartCount;\n-  static const int kMaxId = 1 << kPartShift;\n+ private:\n+  friend Node;\n+  u32 find(u32 s, args_type args, hash_type hash) const;\n+  static u32 lock(atomic_uint32_t *p);\n+  static void unlock(atomic_uint32_t *p, u32 s);\n+  atomic_uint32_t tab[kTabSize];  // Hash table of Node's.\n \n-  atomic_uintptr_t tab[kTabSize];   // Hash table of Node's.\n-  atomic_uint32_t seq[kPartCount];  // Unique id generators.\n+  atomic_uint32_t n_uniq_ids;\n \n-  StackDepotStats stats;\n+  TwoLevelMap<Node, kNodesSize1, kNodesSize2> nodes;\n \n   friend class StackDepotReverseMap;\n };\n \n template <class Node, int kReservedBits, int kTabSizeLog>\n-Node *StackDepotBase<Node, kReservedBits, kTabSizeLog>::find(Node *s,\n-                                                             args_type args,\n-                                                             hash_type hash) {\n+u32 StackDepotBase<Node, kReservedBits, kTabSizeLog>::find(\n+    u32 s, args_type args, hash_type hash) const {\n   // Searches linked list s for the stack, returns its id.\n-  for (; s; s = s->link) {\n-    if (s->eq(hash, args)) {\n+  for (; s;) {\n+    const Node &node = nodes[s];\n+    if (node.eq(hash, args))\n       return s;\n-    }\n+    s = node.link;\n   }\n-  return nullptr;\n+  return 0;\n }\n \n template <class Node, int kReservedBits, int kTabSizeLog>\n-Node *StackDepotBase<Node, kReservedBits, kTabSizeLog>::lock(\n-    atomic_uintptr_t *p) {\n+u32 StackDepotBase<Node, kReservedBits, kTabSizeLog>::lock(atomic_uint32_t *p) {\n   // Uses the pointer lsb as mutex.\n   for (int i = 0;; i++) {\n-    uptr cmp = atomic_load(p, memory_order_relaxed);\n-    if ((cmp & 1) == 0 &&\n-        atomic_compare_exchange_weak(p, &cmp, cmp | 1, memory_order_acquire))\n-      return (Node *)cmp;\n+    u32 cmp = atomic_load(p, memory_order_relaxed);\n+    if ((cmp & kLockMask) == 0 &&\n+        atomic_compare_exchange_weak(p, &cmp, cmp | kLockMask,\n+                                     memory_order_acquire))\n+      return cmp;\n     if (i < 10)\n       proc_yield(10);\n     else\n@@ -91,73 +106,57 @@ Node *StackDepotBase<Node, kReservedBits, kTabSizeLog>::lock(\n \n template <class Node, int kReservedBits, int kTabSizeLog>\n void StackDepotBase<Node, kReservedBits, kTabSizeLog>::unlock(\n-    atomic_uintptr_t *p, Node *s) {\n-  DCHECK_EQ((uptr)s & 1, 0);\n-  atomic_store(p, (uptr)s, memory_order_release);\n+    atomic_uint32_t *p, u32 s) {\n+  DCHECK_EQ(s & kLockMask, 0);\n+  atomic_store(p, s, memory_order_release);\n }\n \n template <class Node, int kReservedBits, int kTabSizeLog>\n-typename StackDepotBase<Node, kReservedBits, kTabSizeLog>::handle_type\n-StackDepotBase<Node, kReservedBits, kTabSizeLog>::Put(args_type args,\n-                                                      bool *inserted) {\n-  if (inserted) *inserted = false;\n-  if (!Node::is_valid(args)) return handle_type();\n+u32 StackDepotBase<Node, kReservedBits, kTabSizeLog>::Put(args_type args,\n+                                                          bool *inserted) {\n+  if (inserted)\n+    *inserted = false;\n+  if (!LIKELY(Node::is_valid(args)))\n+    return 0;\n   hash_type h = Node::hash(args);\n-  atomic_uintptr_t *p = &tab[h % kTabSize];\n-  uptr v = atomic_load(p, memory_order_consume);\n-  Node *s = (Node *)(v & ~1);\n+  atomic_uint32_t *p = &tab[h % kTabSize];\n+  u32 v = atomic_load(p, memory_order_consume);\n+  u32 s = v & kUnlockMask;\n   // First, try to find the existing stack.\n-  Node *node = find(s, args, h);\n-  if (node) return node->get_handle();\n+  u32 node = find(s, args, h);\n+  if (LIKELY(node))\n+    return node;\n+\n   // If failed, lock, retry and insert new.\n-  Node *s2 = lock(p);\n+  u32 s2 = lock(p);\n   if (s2 != s) {\n     node = find(s2, args, h);\n     if (node) {\n       unlock(p, s2);\n-      return node->get_handle();\n+      return node;\n     }\n   }\n-  uptr part = (h % kTabSize) / kPartSize;\n-  u32 id = atomic_fetch_add(&seq[part], 1, memory_order_relaxed) + 1;\n-  stats.n_uniq_ids++;\n-  CHECK_LT(id, kMaxId);\n-  id |= part << kPartShift;\n-  CHECK_NE(id, 0);\n-  CHECK_EQ(id & (((u32)-1) >> kReservedBits), id);\n-  uptr memsz = Node::storage_size(args);\n-  s = (Node *)PersistentAlloc(memsz);\n-  stats.allocated += memsz;\n-  s->id = id;\n-  s->store(args, h);\n-  s->link = s2;\n+  s = atomic_fetch_add(&n_uniq_ids, 1, memory_order_relaxed) + 1;\n+  CHECK_EQ(s & kUnlockMask, s);\n+  CHECK_EQ(s & (((u32)-1) >> kReservedBits), s);\n+  Node &new_node = nodes[s];\n+  new_node.store(s, args, h);\n+  new_node.link = s2;\n   unlock(p, s);\n   if (inserted) *inserted = true;\n-  return s->get_handle();\n+  return s;\n }\n \n template <class Node, int kReservedBits, int kTabSizeLog>\n typename StackDepotBase<Node, kReservedBits, kTabSizeLog>::args_type\n StackDepotBase<Node, kReservedBits, kTabSizeLog>::Get(u32 id) {\n-  if (id == 0) {\n+  if (id == 0)\n     return args_type();\n-  }\n   CHECK_EQ(id & (((u32)-1) >> kReservedBits), id);\n-  // High kPartBits contain part id, so we need to scan at most kPartSize lists.\n-  uptr part = id >> kPartShift;\n-  for (int i = 0; i != kPartSize; i++) {\n-    uptr idx = part * kPartSize + i;\n-    CHECK_LT(idx, kTabSize);\n-    atomic_uintptr_t *p = &tab[idx];\n-    uptr v = atomic_load(p, memory_order_consume);\n-    Node *s = (Node *)(v & ~1);\n-    for (; s; s = s->link) {\n-      if (s->id == id) {\n-        return s->load();\n-      }\n-    }\n-  }\n-  return args_type();\n+  if (!nodes.contains(id))\n+    return args_type();\n+  const Node &node = nodes[id];\n+  return node.load(id);\n }\n \n template <class Node, int kReservedBits, int kTabSizeLog>\n@@ -170,24 +169,23 @@ void StackDepotBase<Node, kReservedBits, kTabSizeLog>::LockAll() {\n template <class Node, int kReservedBits, int kTabSizeLog>\n void StackDepotBase<Node, kReservedBits, kTabSizeLog>::UnlockAll() {\n   for (int i = 0; i < kTabSize; ++i) {\n-    atomic_uintptr_t *p = &tab[i];\n+    atomic_uint32_t *p = &tab[i];\n     uptr s = atomic_load(p, memory_order_relaxed);\n-    unlock(p, (Node *)(s & ~1UL));\n+    unlock(p, s & kUnlockMask);\n   }\n }\n \n template <class Node, int kReservedBits, int kTabSizeLog>\n void StackDepotBase<Node, kReservedBits, kTabSizeLog>::PrintAll() {\n   for (int i = 0; i < kTabSize; ++i) {\n-    atomic_uintptr_t *p = &tab[i];\n-    lock(p);\n-    uptr v = atomic_load(p, memory_order_relaxed);\n-    Node *s = (Node *)(v & ~1UL);\n-    for (; s; s = s->link) {\n-      Printf(\"Stack for id %u:\\n\", s->id);\n-      s->load().Print();\n+    atomic_uint32_t *p = &tab[i];\n+    u32 s = atomic_load(p, memory_order_consume) & kUnlockMask;\n+    for (; s;) {\n+      const Node &node = nodes[s];\n+      Printf(\"Stack for id %u:\\n\", s);\n+      node.load(s).Print();\n+      s = node.link;\n     }\n-    unlock(p, s);\n   }\n }\n "}, {"sha": "37e9e6dd08d7b2bc3eba2582536ef98cde0228a3", "filename": "libsanitizer/sanitizer_common/sanitizer_stacktrace.cpp", "status": "modified", "additions": 6, "deletions": 13, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_stacktrace.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -24,7 +24,7 @@ uptr StackTrace::GetNextInstructionPc(uptr pc) {\n   return pc + 8;\n #elif defined(__powerpc__) || defined(__arm__) || defined(__aarch64__) || \\\n     defined(__hexagon__)\n-  return pc + 4;\n+  return STRIP_PAC_PC((void *)pc) + 4;\n #elif SANITIZER_RISCV64\n   // Current check order is 4 -> 2 -> 6 -> 8\n   u8 InsnByte = *(u8 *)(pc);\n@@ -86,8 +86,8 @@ static inline uhwptr *GetCanonicFrame(uptr bp,\n   // Nope, this does not look right either. This means the frame after next does\n   // not have a valid frame pointer, but we can still extract the caller PC.\n   // Unfortunately, there is no way to decide between GCC and LLVM frame\n-  // layouts. Assume GCC.\n-  return bp_prev - 1;\n+  // layouts. Assume LLVM.\n+  return bp_prev;\n #else\n   return (uhwptr*)bp;\n #endif\n@@ -110,21 +110,14 @@ void BufferedStackTrace::UnwindFast(uptr pc, uptr bp, uptr stack_top,\n          IsAligned((uptr)frame, sizeof(*frame)) &&\n          size < max_depth) {\n #ifdef __powerpc__\n-    // PowerPC ABIs specify that the return address is saved on the\n-    // *caller's* stack frame.  Thus we must dereference the back chain\n-    // to find the caller frame before extracting it.\n+    // PowerPC ABIs specify that the return address is saved at offset\n+    // 16 of the *caller's* stack frame.  Thus we must dereference the\n+    // back chain to find the caller frame before extracting it.\n     uhwptr *caller_frame = (uhwptr*)frame[0];\n     if (!IsValidFrame((uptr)caller_frame, stack_top, bottom) ||\n         !IsAligned((uptr)caller_frame, sizeof(uhwptr)))\n       break;\n-    // For most ABIs the offset where the return address is saved is two\n-    // register sizes.  The exception is the SVR4 ABI, which uses an\n-    // offset of only one register size.\n-#ifdef _CALL_SYSV\n-    uhwptr pc1 = caller_frame[1];\n-#else\n     uhwptr pc1 = caller_frame[2];\n-#endif\n #elif defined(__s390__)\n     uhwptr pc1 = frame[14];\n #elif defined(__riscv)"}, {"sha": "869c8935330d9b16c5489ae9fa6f51a3200405b9", "filename": "libsanitizer/sanitizer_common/sanitizer_symbolizer_report.cpp", "status": "modified", "additions": 8, "deletions": 2, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_report.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_report.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_symbolizer_report.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -88,11 +88,17 @@ void ReportErrorSummary(const char *error_type, const StackTrace *stack,\n #endif\n }\n \n-void ReportMmapWriteExec(int prot) {\n+void ReportMmapWriteExec(int prot, int flags) {\n #if SANITIZER_POSIX && (!SANITIZER_GO && !SANITIZER_ANDROID)\n-  if ((prot & (PROT_WRITE | PROT_EXEC)) != (PROT_WRITE | PROT_EXEC))\n+  int pflags = (PROT_WRITE | PROT_EXEC);\n+  if ((prot & pflags) != pflags)\n     return;\n \n+#  if SANITIZER_MAC && defined(MAP_JIT)\n+  if ((flags & MAP_JIT) == MAP_JIT)\n+    return;\n+#  endif\n+\n   ScopedErrorReportLock l;\n   SanitizerCommonDecorator d;\n "}, {"sha": "b13e2dc9e332784b35b0d0c4b4ac0d561b293668", "filename": "libsanitizer/sanitizer_common/sanitizer_tls_get_addr.cpp", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_tls_get_addr.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_tls_get_addr.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_tls_get_addr.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -44,7 +44,7 @@ static atomic_uintptr_t number_of_live_dtls;\n static const uptr kDestroyedThread = -1;\n \n static void DTLS_Deallocate(DTLS::DTVBlock *block) {\n-  VReport(2, \"__tls_get_addr: DTLS_Deallocate %p\\n\", block);\n+  VReport(2, \"__tls_get_addr: DTLS_Deallocate %p\\n\", (void *)block);\n   UnmapOrDie(block, sizeof(DTLS::DTVBlock));\n   atomic_fetch_sub(&number_of_live_dtls, 1, memory_order_relaxed);\n }\n@@ -66,12 +66,13 @@ static DTLS::DTVBlock *DTLS_NextBlock(atomic_uintptr_t *cur) {\n   }\n   uptr num_live_dtls =\n       atomic_fetch_add(&number_of_live_dtls, 1, memory_order_relaxed);\n-  VReport(2, \"__tls_get_addr: DTLS_NextBlock %p %zd\\n\", &dtls, num_live_dtls);\n+  VReport(2, \"__tls_get_addr: DTLS_NextBlock %p %zd\\n\", (void *)&dtls,\n+          num_live_dtls);\n   return new_dtv;\n }\n \n static DTLS::DTV *DTLS_Find(uptr id) {\n-  VReport(2, \"__tls_get_addr: DTLS_Find %p %zd\\n\", &dtls, id);\n+  VReport(2, \"__tls_get_addr: DTLS_Find %p %zd\\n\", (void *)&dtls, id);\n   static constexpr uptr kPerBlock = ARRAY_SIZE(DTLS::DTVBlock::dtvs);\n   DTLS::DTVBlock *cur = DTLS_NextBlock(&dtls.dtv_block);\n   if (!cur)\n@@ -82,7 +83,7 @@ static DTLS::DTV *DTLS_Find(uptr id) {\n \n void DTLS_Destroy() {\n   if (!common_flags()->intercept_tls_get_addr) return;\n-  VReport(2, \"__tls_get_addr: DTLS_Destroy %p\\n\", &dtls);\n+  VReport(2, \"__tls_get_addr: DTLS_Destroy %p\\n\", (void *)&dtls);\n   DTLS::DTVBlock *block = (DTLS::DTVBlock *)atomic_exchange(\n       &dtls.dtv_block, kDestroyedThread, memory_order_release);\n   while (block) {\n@@ -120,7 +121,7 @@ DTLS::DTV *DTLS_on_tls_get_addr(void *arg_void, void *res,\n   VReport(2,\n           \"__tls_get_addr: %p {0x%zx,0x%zx} => %p; tls_beg: 0x%zx; sp: %p \"\n           \"num_live_dtls %zd\\n\",\n-          arg, arg->dso_id, arg->offset, res, tls_beg, &tls_beg,\n+          (void *)arg, arg->dso_id, arg->offset, res, tls_beg, (void *)&tls_beg,\n           atomic_load(&number_of_live_dtls, memory_order_relaxed));\n   if (dtls.last_memalign_ptr == tls_beg) {\n     tls_size = dtls.last_memalign_size;"}, {"sha": "c3607dbed23e1118fe0250ba4ac35e13fd529a46", "filename": "libsanitizer/sanitizer_common/sanitizer_win.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_win.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fsanitizer_common%2Fsanitizer_win.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fsanitizer_common%2Fsanitizer_win.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -1113,7 +1113,7 @@ bool IsProcessRunning(pid_t pid) {\n int WaitForProcess(pid_t pid) { return -1; }\n \n // FIXME implement on this platform.\n-void GetMemoryProfile(fill_profile_f cb, uptr *stats, uptr stats_size) { }\n+void GetMemoryProfile(fill_profile_f cb, uptr *stats) {}\n \n void CheckNoDeepBind(const char *filename, int flag) {\n   // Do nothing."}, {"sha": "61dbb81ffec43159ac75c57cb297098d1140d91e", "filename": "libsanitizer/tsan/tsan_interceptors.h", "status": "modified", "additions": 22, "deletions": 10, "changes": 32, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_interceptors.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_interceptors.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_interceptors.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -38,19 +38,31 @@ inline bool in_symbolizer() {\n \n }  // namespace __tsan\n \n-#define SCOPED_INTERCEPTOR_RAW(func, ...)      \\\n-  ThreadState *thr = cur_thread_init();        \\\n-  const uptr caller_pc = GET_CALLER_PC();      \\\n-  ScopedInterceptor si(thr, #func, caller_pc); \\\n-  const uptr pc = GET_CURRENT_PC();            \\\n-  (void)pc;\n+#define SCOPED_INTERCEPTOR_RAW(func, ...)            \\\n+  ThreadState *thr = cur_thread_init();              \\\n+  ScopedInterceptor si(thr, #func, GET_CALLER_PC()); \\\n+  UNUSED const uptr pc = GET_CURRENT_PC();\n+\n+#ifdef __powerpc64__\n+// Debugging of crashes on powerpc after commit:\n+// c80604f7a3 (\"tsan: remove real func check from interceptors\")\n+// Somehow replacing if with DCHECK leads to strange failures in:\n+// SanitizerCommon-tsan-powerpc64le-Linux :: Linux/ptrace.cpp\n+// https://lab.llvm.org/buildbot/#/builders/105\n+// https://lab.llvm.org/buildbot/#/builders/121\n+// https://lab.llvm.org/buildbot/#/builders/57\n+#  define CHECK_REAL_FUNC(func)                                          \\\n+    if (REAL(func) == 0) {                                               \\\n+      Report(\"FATAL: ThreadSanitizer: failed to intercept %s\\n\", #func); \\\n+      Die();                                                             \\\n+    }\n+#else\n+#  define CHECK_REAL_FUNC(func) DCHECK(REAL(func))\n+#endif\n \n #define SCOPED_TSAN_INTERCEPTOR(func, ...)                                \\\n   SCOPED_INTERCEPTOR_RAW(func, __VA_ARGS__);                              \\\n-  if (REAL(func) == 0) {                                                  \\\n-    Report(\"FATAL: ThreadSanitizer: failed to intercept %s\\n\", #func);    \\\n-    Die();                                                                \\\n-  }                                                                       \\\n+  CHECK_REAL_FUNC(func);                                                  \\\n   if (!thr->is_inited || thr->ignore_interceptors || thr->in_ignored_lib) \\\n     return REAL(func)(__VA_ARGS__);\n "}, {"sha": "735179686ba956cdd48ba6a11a2c36a162b1d851", "filename": "libsanitizer/tsan/tsan_mutexset.cpp", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_mutexset.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_mutexset.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_mutexset.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -10,6 +10,8 @@\n //\n //===----------------------------------------------------------------------===//\n #include \"tsan_mutexset.h\"\n+\n+#include \"sanitizer_common/sanitizer_placement_new.h\"\n #include \"tsan_rtl.h\"\n \n namespace __tsan {\n@@ -124,4 +126,7 @@ MutexSet::Desc MutexSet::Get(uptr i) const {\n   return descs_[i];\n }\n \n+DynamicMutexSet::DynamicMutexSet() : ptr_(New<MutexSet>()) {}\n+DynamicMutexSet::~DynamicMutexSet() { DestroyAndFree(ptr_); }\n+\n }  // namespace __tsan"}, {"sha": "93776a664135136c33066eaa6f667da0a1464a5e", "filename": "libsanitizer/tsan/tsan_mutexset.h", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_mutexset.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_mutexset.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_mutexset.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -59,6 +59,24 @@ class MutexSet {\n #endif\n };\n \n+// MutexSet is too large to live on stack.\n+// DynamicMutexSet can be use used to create local MutexSet's.\n+class DynamicMutexSet {\n+ public:\n+  DynamicMutexSet();\n+  ~DynamicMutexSet();\n+  MutexSet* operator->() { return ptr_; }\n+  operator MutexSet*() { return ptr_; }\n+  DynamicMutexSet(const DynamicMutexSet&) = delete;\n+  DynamicMutexSet& operator=(const DynamicMutexSet&) = delete;\n+\n+ private:\n+  MutexSet* ptr_;\n+#if SANITIZER_GO\n+  MutexSet set_;\n+#endif\n+};\n+\n // Go does not have mutexes, so do not spend memory and time.\n // (Go sync.Mutex is actually a semaphore -- can be unlocked\n // in different goroutine).\n@@ -71,6 +89,8 @@ void MutexSet::AddAddr(uptr addr, StackID stack_id, bool write) {}\n void MutexSet::DelAddr(uptr addr, bool destroy) {}\n uptr MutexSet::Size() const { return 0; }\n MutexSet::Desc MutexSet::Get(uptr i) const { return Desc(); }\n+DynamicMutexSet::DynamicMutexSet() : ptr_(&set_) {}\n+DynamicMutexSet::~DynamicMutexSet() {}\n #endif\n \n }  // namespace __tsan"}, {"sha": "7ff0acace8f6d2da236235c4deb8071353ec4c5d", "filename": "libsanitizer/tsan/tsan_platform.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_platform.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_platform.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_platform.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -906,7 +906,7 @@ struct RestoreAddrImpl {\n     // 3 bits of the compressed addr match that of the app range. If yes, we\n     // assume that the compressed address come from that range and restore the\n     // missing top bits to match the app range address.\n-    static constexpr uptr ranges[] = {\n+    const uptr ranges[] = {\n         Mapping::kLoAppMemBeg,  Mapping::kLoAppMemEnd, Mapping::kMidAppMemBeg,\n         Mapping::kMidAppMemEnd, Mapping::kHiAppMemBeg, Mapping::kHiAppMemEnd,\n         Mapping::kHeapMemBeg,   Mapping::kHeapMemEnd,"}, {"sha": "73ec14892d28face2e329c6e63c008c8d4f86e12", "filename": "libsanitizer/tsan/tsan_platform_linux.cpp", "status": "modified", "additions": 2, "deletions": 3, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_platform_linux.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_platform_linux.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_platform_linux.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -100,8 +100,7 @@ enum {\n   MemCount,\n };\n \n-void FillProfileCallback(uptr p, uptr rss, bool file,\n-                         uptr *mem, uptr stats_size) {\n+void FillProfileCallback(uptr p, uptr rss, bool file, uptr *mem) {\n   mem[MemTotal] += rss;\n   if (p >= ShadowBeg() && p < ShadowEnd())\n     mem[MemShadow] += rss;\n@@ -122,7 +121,7 @@ void FillProfileCallback(uptr p, uptr rss, bool file,\n void WriteMemoryProfile(char *buf, uptr buf_size, u64 uptime_ns) {\n   uptr mem[MemCount];\n   internal_memset(mem, 0, sizeof(mem));\n-  GetMemoryProfile(FillProfileCallback, mem, MemCount);\n+  GetMemoryProfile(FillProfileCallback, mem);\n   auto meta = ctx->metamap.GetMemoryStats();\n   StackDepotStats stacks = StackDepotGetStats();\n   uptr nthread, nlive;"}, {"sha": "3faa2d0c6192c98cd81aee61389a85a71ce207db", "filename": "libsanitizer/tsan/tsan_platform_mac.cpp", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_platform_mac.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_platform_mac.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_platform_mac.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -240,7 +240,7 @@ void InitializePlatformEarly() {\n   uptr max_vm = GetMaxUserVirtualAddress() + 1;\n   if (max_vm != HiAppMemEnd()) {\n     Printf(\"ThreadSanitizer: unsupported vm address limit %p, expected %p.\\n\",\n-           max_vm, HiAppMemEnd());\n+           (void *)max_vm, (void *)HiAppMemEnd());\n     Die();\n   }\n #endif"}, {"sha": "6e57d4aeb093754ea6530a9b332a0b1a9b5c0ec4", "filename": "libsanitizer/tsan/tsan_rtl.cpp", "status": "modified", "additions": 0, "deletions": 543, "changes": 543, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_rtl.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_rtl.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -567,123 +567,6 @@ StackID CurrentStackId(ThreadState *thr, uptr pc) {\n \n namespace v3 {\n \n-ALWAYS_INLINE USED bool TryTraceMemoryAccess(ThreadState *thr, uptr pc,\n-                                             uptr addr, uptr size,\n-                                             AccessType typ) {\n-  DCHECK(size == 1 || size == 2 || size == 4 || size == 8);\n-  if (!kCollectHistory)\n-    return true;\n-  EventAccess *ev;\n-  if (UNLIKELY(!TraceAcquire(thr, &ev)))\n-    return false;\n-  u64 size_log = size == 1 ? 0 : size == 2 ? 1 : size == 4 ? 2 : 3;\n-  uptr pc_delta = pc - thr->trace_prev_pc + (1 << (EventAccess::kPCBits - 1));\n-  thr->trace_prev_pc = pc;\n-  if (LIKELY(pc_delta < (1 << EventAccess::kPCBits))) {\n-    ev->is_access = 1;\n-    ev->is_read = !!(typ & kAccessRead);\n-    ev->is_atomic = !!(typ & kAccessAtomic);\n-    ev->size_log = size_log;\n-    ev->pc_delta = pc_delta;\n-    DCHECK_EQ(ev->pc_delta, pc_delta);\n-    ev->addr = CompressAddr(addr);\n-    TraceRelease(thr, ev);\n-    return true;\n-  }\n-  auto *evex = reinterpret_cast<EventAccessExt *>(ev);\n-  evex->is_access = 0;\n-  evex->is_func = 0;\n-  evex->type = EventType::kAccessExt;\n-  evex->is_read = !!(typ & kAccessRead);\n-  evex->is_atomic = !!(typ & kAccessAtomic);\n-  evex->size_log = size_log;\n-  evex->addr = CompressAddr(addr);\n-  evex->pc = pc;\n-  TraceRelease(thr, evex);\n-  return true;\n-}\n-\n-ALWAYS_INLINE USED bool TryTraceMemoryAccessRange(ThreadState *thr, uptr pc,\n-                                                  uptr addr, uptr size,\n-                                                  AccessType typ) {\n-  if (!kCollectHistory)\n-    return true;\n-  EventAccessRange *ev;\n-  if (UNLIKELY(!TraceAcquire(thr, &ev)))\n-    return false;\n-  thr->trace_prev_pc = pc;\n-  ev->is_access = 0;\n-  ev->is_func = 0;\n-  ev->type = EventType::kAccessRange;\n-  ev->is_read = !!(typ & kAccessRead);\n-  ev->is_free = !!(typ & kAccessFree);\n-  ev->size_lo = size;\n-  ev->pc = CompressAddr(pc);\n-  ev->addr = CompressAddr(addr);\n-  ev->size_hi = size >> EventAccessRange::kSizeLoBits;\n-  TraceRelease(thr, ev);\n-  return true;\n-}\n-\n-void TraceMemoryAccessRange(ThreadState *thr, uptr pc, uptr addr, uptr size,\n-                            AccessType typ) {\n-  if (LIKELY(TryTraceMemoryAccessRange(thr, pc, addr, size, typ)))\n-    return;\n-  TraceSwitchPart(thr);\n-  UNUSED bool res = TryTraceMemoryAccessRange(thr, pc, addr, size, typ);\n-  DCHECK(res);\n-}\n-\n-void TraceFunc(ThreadState *thr, uptr pc) {\n-  if (LIKELY(TryTraceFunc(thr, pc)))\n-    return;\n-  TraceSwitchPart(thr);\n-  UNUSED bool res = TryTraceFunc(thr, pc);\n-  DCHECK(res);\n-}\n-\n-void TraceMutexLock(ThreadState *thr, EventType type, uptr pc, uptr addr,\n-                    StackID stk) {\n-  DCHECK(type == EventType::kLock || type == EventType::kRLock);\n-  if (!kCollectHistory)\n-    return;\n-  EventLock ev;\n-  ev.is_access = 0;\n-  ev.is_func = 0;\n-  ev.type = type;\n-  ev.pc = CompressAddr(pc);\n-  ev.stack_lo = stk;\n-  ev.stack_hi = stk >> EventLock::kStackIDLoBits;\n-  ev._ = 0;\n-  ev.addr = CompressAddr(addr);\n-  TraceEvent(thr, ev);\n-}\n-\n-void TraceMutexUnlock(ThreadState *thr, uptr addr) {\n-  if (!kCollectHistory)\n-    return;\n-  EventUnlock ev;\n-  ev.is_access = 0;\n-  ev.is_func = 0;\n-  ev.type = EventType::kUnlock;\n-  ev._ = 0;\n-  ev.addr = CompressAddr(addr);\n-  TraceEvent(thr, ev);\n-}\n-\n-void TraceTime(ThreadState *thr) {\n-  if (!kCollectHistory)\n-    return;\n-  EventTime ev;\n-  ev.is_access = 0;\n-  ev.is_func = 0;\n-  ev.type = EventType::kTime;\n-  ev.sid = static_cast<u64>(thr->sid);\n-  ev.epoch = static_cast<u64>(thr->epoch);\n-  ev._ = 0;\n-  TraceEvent(thr, ev);\n-}\n-\n NOINLINE\n void TraceSwitchPart(ThreadState *thr) {\n   Trace *trace = &thr->tctx->trace;\n@@ -789,427 +672,6 @@ extern \"C\" void __tsan_report_race() {\n }\n #endif\n \n-ALWAYS_INLINE\n-Shadow LoadShadow(u64 *p) {\n-  u64 raw = atomic_load((atomic_uint64_t*)p, memory_order_relaxed);\n-  return Shadow(raw);\n-}\n-\n-ALWAYS_INLINE\n-void StoreShadow(u64 *sp, u64 s) {\n-  atomic_store((atomic_uint64_t*)sp, s, memory_order_relaxed);\n-}\n-\n-ALWAYS_INLINE\n-void StoreIfNotYetStored(u64 *sp, u64 *s) {\n-  StoreShadow(sp, *s);\n-  *s = 0;\n-}\n-\n-ALWAYS_INLINE\n-void HandleRace(ThreadState *thr, u64 *shadow_mem,\n-                              Shadow cur, Shadow old) {\n-  thr->racy_state[0] = cur.raw();\n-  thr->racy_state[1] = old.raw();\n-  thr->racy_shadow_addr = shadow_mem;\n-#if !SANITIZER_GO\n-  HACKY_CALL(__tsan_report_race);\n-#else\n-  ReportRace(thr);\n-#endif\n-}\n-\n-static inline bool HappensBefore(Shadow old, ThreadState *thr) {\n-  return thr->clock.get(old.TidWithIgnore()) >= old.epoch();\n-}\n-\n-ALWAYS_INLINE\n-void MemoryAccessImpl1(ThreadState *thr, uptr addr,\n-    int kAccessSizeLog, bool kAccessIsWrite, bool kIsAtomic,\n-    u64 *shadow_mem, Shadow cur) {\n-\n-  // This potentially can live in an MMX/SSE scratch register.\n-  // The required intrinsics are:\n-  // __m128i _mm_move_epi64(__m128i*);\n-  // _mm_storel_epi64(u64*, __m128i);\n-  u64 store_word = cur.raw();\n-  bool stored = false;\n-\n-  // scan all the shadow values and dispatch to 4 categories:\n-  // same, replace, candidate and race (see comments below).\n-  // we consider only 3 cases regarding access sizes:\n-  // equal, intersect and not intersect. initially I considered\n-  // larger and smaller as well, it allowed to replace some\n-  // 'candidates' with 'same' or 'replace', but I think\n-  // it's just not worth it (performance- and complexity-wise).\n-\n-  Shadow old(0);\n-\n-  // It release mode we manually unroll the loop,\n-  // because empirically gcc generates better code this way.\n-  // However, we can't afford unrolling in debug mode, because the function\n-  // consumes almost 4K of stack. Gtest gives only 4K of stack to death test\n-  // threads, which is not enough for the unrolled loop.\n-#if SANITIZER_DEBUG\n-  for (int idx = 0; idx < 4; idx++) {\n-#  include \"tsan_update_shadow_word.inc\"\n-  }\n-#else\n-  int idx = 0;\n-#  include \"tsan_update_shadow_word.inc\"\n-  idx = 1;\n-  if (stored) {\n-#  include \"tsan_update_shadow_word.inc\"\n-  } else {\n-#  include \"tsan_update_shadow_word.inc\"\n-  }\n-  idx = 2;\n-  if (stored) {\n-#  include \"tsan_update_shadow_word.inc\"\n-  } else {\n-#  include \"tsan_update_shadow_word.inc\"\n-  }\n-  idx = 3;\n-  if (stored) {\n-#  include \"tsan_update_shadow_word.inc\"\n-  } else {\n-#  include \"tsan_update_shadow_word.inc\"\n-  }\n-#endif\n-\n-  // we did not find any races and had already stored\n-  // the current access info, so we are done\n-  if (LIKELY(stored))\n-    return;\n-  // choose a random candidate slot and replace it\n-  StoreShadow(shadow_mem + (cur.epoch() % kShadowCnt), store_word);\n-  return;\n- RACE:\n-  HandleRace(thr, shadow_mem, cur, old);\n-  return;\n-}\n-\n-void UnalignedMemoryAccess(ThreadState *thr, uptr pc, uptr addr, uptr size,\n-                           AccessType typ) {\n-  DCHECK(!(typ & kAccessAtomic));\n-  const bool kAccessIsWrite = !(typ & kAccessRead);\n-  const bool kIsAtomic = false;\n-  while (size) {\n-    int size1 = 1;\n-    int kAccessSizeLog = kSizeLog1;\n-    if (size >= 8 && (addr & ~7) == ((addr + 7) & ~7)) {\n-      size1 = 8;\n-      kAccessSizeLog = kSizeLog8;\n-    } else if (size >= 4 && (addr & ~7) == ((addr + 3) & ~7)) {\n-      size1 = 4;\n-      kAccessSizeLog = kSizeLog4;\n-    } else if (size >= 2 && (addr & ~7) == ((addr + 1) & ~7)) {\n-      size1 = 2;\n-      kAccessSizeLog = kSizeLog2;\n-    }\n-    MemoryAccess(thr, pc, addr, kAccessSizeLog, kAccessIsWrite, kIsAtomic);\n-    addr += size1;\n-    size -= size1;\n-  }\n-}\n-\n-ALWAYS_INLINE\n-bool ContainsSameAccessSlow(u64 *s, u64 a, u64 sync_epoch, bool is_write) {\n-  Shadow cur(a);\n-  for (uptr i = 0; i < kShadowCnt; i++) {\n-    Shadow old(LoadShadow(&s[i]));\n-    if (Shadow::Addr0AndSizeAreEqual(cur, old) &&\n-        old.TidWithIgnore() == cur.TidWithIgnore() &&\n-        old.epoch() > sync_epoch &&\n-        old.IsAtomic() == cur.IsAtomic() &&\n-        old.IsRead() <= cur.IsRead())\n-      return true;\n-  }\n-  return false;\n-}\n-\n-#if TSAN_VECTORIZE\n-#  define SHUF(v0, v1, i0, i1, i2, i3)                    \\\n-    _mm_castps_si128(_mm_shuffle_ps(_mm_castsi128_ps(v0), \\\n-                                    _mm_castsi128_ps(v1), \\\n-                                    (i0)*1 + (i1)*4 + (i2)*16 + (i3)*64))\n-ALWAYS_INLINE\n-bool ContainsSameAccessFast(u64 *s, u64 a, u64 sync_epoch, bool is_write) {\n-  // This is an optimized version of ContainsSameAccessSlow.\n-  // load current access into access[0:63]\n-  const m128 access     = _mm_cvtsi64_si128(a);\n-  // duplicate high part of access in addr0:\n-  // addr0[0:31]        = access[32:63]\n-  // addr0[32:63]       = access[32:63]\n-  // addr0[64:95]       = access[32:63]\n-  // addr0[96:127]      = access[32:63]\n-  const m128 addr0      = SHUF(access, access, 1, 1, 1, 1);\n-  // load 4 shadow slots\n-  const m128 shadow0    = _mm_load_si128((__m128i*)s);\n-  const m128 shadow1    = _mm_load_si128((__m128i*)s + 1);\n-  // load high parts of 4 shadow slots into addr_vect:\n-  // addr_vect[0:31]    = shadow0[32:63]\n-  // addr_vect[32:63]   = shadow0[96:127]\n-  // addr_vect[64:95]   = shadow1[32:63]\n-  // addr_vect[96:127]  = shadow1[96:127]\n-  m128 addr_vect        = SHUF(shadow0, shadow1, 1, 3, 1, 3);\n-  if (!is_write) {\n-    // set IsRead bit in addr_vect\n-    const m128 rw_mask1 = _mm_cvtsi64_si128(1<<15);\n-    const m128 rw_mask  = SHUF(rw_mask1, rw_mask1, 0, 0, 0, 0);\n-    addr_vect           = _mm_or_si128(addr_vect, rw_mask);\n-  }\n-  // addr0 == addr_vect?\n-  const m128 addr_res   = _mm_cmpeq_epi32(addr0, addr_vect);\n-  // epoch1[0:63]       = sync_epoch\n-  const m128 epoch1     = _mm_cvtsi64_si128(sync_epoch);\n-  // epoch[0:31]        = sync_epoch[0:31]\n-  // epoch[32:63]       = sync_epoch[0:31]\n-  // epoch[64:95]       = sync_epoch[0:31]\n-  // epoch[96:127]      = sync_epoch[0:31]\n-  const m128 epoch      = SHUF(epoch1, epoch1, 0, 0, 0, 0);\n-  // load low parts of shadow cell epochs into epoch_vect:\n-  // epoch_vect[0:31]   = shadow0[0:31]\n-  // epoch_vect[32:63]  = shadow0[64:95]\n-  // epoch_vect[64:95]  = shadow1[0:31]\n-  // epoch_vect[96:127] = shadow1[64:95]\n-  const m128 epoch_vect = SHUF(shadow0, shadow1, 0, 2, 0, 2);\n-  // epoch_vect >= sync_epoch?\n-  const m128 epoch_res  = _mm_cmpgt_epi32(epoch_vect, epoch);\n-  // addr_res & epoch_res\n-  const m128 res        = _mm_and_si128(addr_res, epoch_res);\n-  // mask[0] = res[7]\n-  // mask[1] = res[15]\n-  // ...\n-  // mask[15] = res[127]\n-  const int mask        = _mm_movemask_epi8(res);\n-  return mask != 0;\n-}\n-#endif\n-\n-ALWAYS_INLINE\n-bool ContainsSameAccess(u64 *s, u64 a, u64 sync_epoch, bool is_write) {\n-#if TSAN_VECTORIZE\n-  bool res = ContainsSameAccessFast(s, a, sync_epoch, is_write);\n-  // NOTE: this check can fail if the shadow is concurrently mutated\n-  // by other threads. But it still can be useful if you modify\n-  // ContainsSameAccessFast and want to ensure that it's not completely broken.\n-  // DCHECK_EQ(res, ContainsSameAccessSlow(s, a, sync_epoch, is_write));\n-  return res;\n-#else\n-  return ContainsSameAccessSlow(s, a, sync_epoch, is_write);\n-#endif\n-}\n-\n-ALWAYS_INLINE USED\n-void MemoryAccess(ThreadState *thr, uptr pc, uptr addr,\n-    int kAccessSizeLog, bool kAccessIsWrite, bool kIsAtomic) {\n-  RawShadow *shadow_mem = MemToShadow(addr);\n-  DPrintf2(\"#%d: MemoryAccess: @%p %p size=%d\"\n-      \" is_write=%d shadow_mem=%p {%zx, %zx, %zx, %zx}\\n\",\n-      (int)thr->fast_state.tid(), (void*)pc, (void*)addr,\n-      (int)(1 << kAccessSizeLog), kAccessIsWrite, shadow_mem,\n-      (uptr)shadow_mem[0], (uptr)shadow_mem[1],\n-      (uptr)shadow_mem[2], (uptr)shadow_mem[3]);\n-#if SANITIZER_DEBUG\n-  if (!IsAppMem(addr)) {\n-    Printf(\"Access to non app mem %zx\\n\", addr);\n-    DCHECK(IsAppMem(addr));\n-  }\n-  if (!IsShadowMem(shadow_mem)) {\n-    Printf(\"Bad shadow addr %p (%zx)\\n\", shadow_mem, addr);\n-    DCHECK(IsShadowMem(shadow_mem));\n-  }\n-#endif\n-\n-  if (!SANITIZER_GO && !kAccessIsWrite && *shadow_mem == kShadowRodata) {\n-    // Access to .rodata section, no races here.\n-    // Measurements show that it can be 10-20% of all memory accesses.\n-    return;\n-  }\n-\n-  FastState fast_state = thr->fast_state;\n-  if (UNLIKELY(fast_state.GetIgnoreBit())) {\n-    return;\n-  }\n-\n-  Shadow cur(fast_state);\n-  cur.SetAddr0AndSizeLog(addr & 7, kAccessSizeLog);\n-  cur.SetWrite(kAccessIsWrite);\n-  cur.SetAtomic(kIsAtomic);\n-\n-  if (LIKELY(ContainsSameAccess(shadow_mem, cur.raw(),\n-      thr->fast_synch_epoch, kAccessIsWrite))) {\n-    return;\n-  }\n-\n-  if (kCollectHistory) {\n-    fast_state.IncrementEpoch();\n-    thr->fast_state = fast_state;\n-    TraceAddEvent(thr, fast_state, EventTypeMop, pc);\n-    cur.IncrementEpoch();\n-  }\n-\n-  MemoryAccessImpl1(thr, addr, kAccessSizeLog, kAccessIsWrite, kIsAtomic,\n-      shadow_mem, cur);\n-}\n-\n-// Called by MemoryAccessRange in tsan_rtl_thread.cpp\n-ALWAYS_INLINE USED\n-void MemoryAccessImpl(ThreadState *thr, uptr addr,\n-    int kAccessSizeLog, bool kAccessIsWrite, bool kIsAtomic,\n-    u64 *shadow_mem, Shadow cur) {\n-  if (LIKELY(ContainsSameAccess(shadow_mem, cur.raw(),\n-      thr->fast_synch_epoch, kAccessIsWrite))) {\n-    return;\n-  }\n-\n-  MemoryAccessImpl1(thr, addr, kAccessSizeLog, kAccessIsWrite, kIsAtomic,\n-      shadow_mem, cur);\n-}\n-\n-static void MemoryRangeSet(ThreadState *thr, uptr pc, uptr addr, uptr size,\n-                           u64 val) {\n-  (void)thr;\n-  (void)pc;\n-  if (size == 0)\n-    return;\n-  // FIXME: fix me.\n-  uptr offset = addr % kShadowCell;\n-  if (offset) {\n-    offset = kShadowCell - offset;\n-    if (size <= offset)\n-      return;\n-    addr += offset;\n-    size -= offset;\n-  }\n-  DCHECK_EQ(addr % 8, 0);\n-  // If a user passes some insane arguments (memset(0)),\n-  // let it just crash as usual.\n-  if (!IsAppMem(addr) || !IsAppMem(addr + size - 1))\n-    return;\n-  // Don't want to touch lots of shadow memory.\n-  // If a program maps 10MB stack, there is no need reset the whole range.\n-  size = (size + (kShadowCell - 1)) & ~(kShadowCell - 1);\n-  // UnmapOrDie/MmapFixedNoReserve does not work on Windows.\n-  if (SANITIZER_WINDOWS || size < common_flags()->clear_shadow_mmap_threshold) {\n-    RawShadow *p = MemToShadow(addr);\n-    CHECK(IsShadowMem(p));\n-    CHECK(IsShadowMem(p + size * kShadowCnt / kShadowCell - 1));\n-    // FIXME: may overwrite a part outside the region\n-    for (uptr i = 0; i < size / kShadowCell * kShadowCnt;) {\n-      p[i++] = val;\n-      for (uptr j = 1; j < kShadowCnt; j++)\n-        p[i++] = 0;\n-    }\n-  } else {\n-    // The region is big, reset only beginning and end.\n-    const uptr kPageSize = GetPageSizeCached();\n-    RawShadow *begin = MemToShadow(addr);\n-    RawShadow *end = begin + size / kShadowCell * kShadowCnt;\n-    RawShadow *p = begin;\n-    // Set at least first kPageSize/2 to page boundary.\n-    while ((p < begin + kPageSize / kShadowSize / 2) || ((uptr)p % kPageSize)) {\n-      *p++ = val;\n-      for (uptr j = 1; j < kShadowCnt; j++)\n-        *p++ = 0;\n-    }\n-    // Reset middle part.\n-    RawShadow *p1 = p;\n-    p = RoundDown(end, kPageSize);\n-    if (!MmapFixedSuperNoReserve((uptr)p1, (uptr)p - (uptr)p1))\n-      Die();\n-    // Set the ending.\n-    while (p < end) {\n-      *p++ = val;\n-      for (uptr j = 1; j < kShadowCnt; j++)\n-        *p++ = 0;\n-    }\n-  }\n-}\n-\n-void MemoryResetRange(ThreadState *thr, uptr pc, uptr addr, uptr size) {\n-  MemoryRangeSet(thr, pc, addr, size, 0);\n-}\n-\n-void MemoryRangeFreed(ThreadState *thr, uptr pc, uptr addr, uptr size) {\n-  // Processing more than 1k (4k of shadow) is expensive,\n-  // can cause excessive memory consumption (user does not necessary touch\n-  // the whole range) and most likely unnecessary.\n-  if (size > 1024)\n-    size = 1024;\n-  CHECK_EQ(thr->is_freeing, false);\n-  thr->is_freeing = true;\n-  MemoryAccessRange(thr, pc, addr, size, true);\n-  thr->is_freeing = false;\n-  if (kCollectHistory) {\n-    thr->fast_state.IncrementEpoch();\n-    TraceAddEvent(thr, thr->fast_state, EventTypeMop, pc);\n-  }\n-  Shadow s(thr->fast_state);\n-  s.ClearIgnoreBit();\n-  s.MarkAsFreed();\n-  s.SetWrite(true);\n-  s.SetAddr0AndSizeLog(0, 3);\n-  MemoryRangeSet(thr, pc, addr, size, s.raw());\n-}\n-\n-void MemoryRangeImitateWrite(ThreadState *thr, uptr pc, uptr addr, uptr size) {\n-  if (kCollectHistory) {\n-    thr->fast_state.IncrementEpoch();\n-    TraceAddEvent(thr, thr->fast_state, EventTypeMop, pc);\n-  }\n-  Shadow s(thr->fast_state);\n-  s.ClearIgnoreBit();\n-  s.SetWrite(true);\n-  s.SetAddr0AndSizeLog(0, 3);\n-  MemoryRangeSet(thr, pc, addr, size, s.raw());\n-}\n-\n-void MemoryRangeImitateWriteOrResetRange(ThreadState *thr, uptr pc, uptr addr,\n-                                         uptr size) {\n-  if (thr->ignore_reads_and_writes == 0)\n-    MemoryRangeImitateWrite(thr, pc, addr, size);\n-  else\n-    MemoryResetRange(thr, pc, addr, size);\n-}\n-\n-ALWAYS_INLINE USED\n-void FuncEntry(ThreadState *thr, uptr pc) {\n-  DPrintf2(\"#%d: FuncEntry %p\\n\", (int)thr->fast_state.tid(), (void*)pc);\n-  if (kCollectHistory) {\n-    thr->fast_state.IncrementEpoch();\n-    TraceAddEvent(thr, thr->fast_state, EventTypeFuncEnter, pc);\n-  }\n-\n-  // Shadow stack maintenance can be replaced with\n-  // stack unwinding during trace switch (which presumably must be faster).\n-  DCHECK_GE(thr->shadow_stack_pos, thr->shadow_stack);\n-#if !SANITIZER_GO\n-  DCHECK_LT(thr->shadow_stack_pos, thr->shadow_stack_end);\n-#else\n-  if (thr->shadow_stack_pos == thr->shadow_stack_end)\n-    GrowShadowStack(thr);\n-#endif\n-  thr->shadow_stack_pos[0] = pc;\n-  thr->shadow_stack_pos++;\n-}\n-\n-ALWAYS_INLINE USED\n-void FuncExit(ThreadState *thr) {\n-  DPrintf2(\"#%d: FuncExit\\n\", (int)thr->fast_state.tid());\n-  if (kCollectHistory) {\n-    thr->fast_state.IncrementEpoch();\n-    TraceAddEvent(thr, thr->fast_state, EventTypeFuncExit, 0);\n-  }\n-\n-  DCHECK_GT(thr->shadow_stack_pos, thr->shadow_stack);\n-#if !SANITIZER_GO\n-  DCHECK_LT(thr->shadow_stack_pos, thr->shadow_stack_end);\n-#endif\n-  thr->shadow_stack_pos--;\n-}\n-\n void ThreadIgnoreBegin(ThreadState *thr, uptr pc) {\n   DPrintf(\"#%d: ThreadIgnoreBegin\\n\", thr->tid);\n   thr->ignore_reads_and_writes++;\n@@ -1293,8 +755,3 @@ MutexMeta mutex_meta[] = {\n void PrintMutexPC(uptr pc) { StackTrace(&pc, 1).Print(); }\n }  // namespace __sanitizer\n #endif\n-\n-#if !SANITIZER_GO\n-// Must be included in this file to make sure everything is inlined.\n-#  include \"tsan_interface.inc\"\n-#endif"}, {"sha": "089144c17ff0617b053e5c8b27d62fbb9cc72d92", "filename": "libsanitizer/tsan/tsan_rtl.h", "status": "modified", "additions": 38, "deletions": 0, "changes": 38, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_rtl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_rtl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -749,6 +749,44 @@ void TraceTime(ThreadState *thr);\n \n }  // namespace v3\n \n+void GrowShadowStack(ThreadState *thr);\n+\n+ALWAYS_INLINE\n+void FuncEntry(ThreadState *thr, uptr pc) {\n+  DPrintf2(\"#%d: FuncEntry %p\\n\", (int)thr->fast_state.tid(), (void *)pc);\n+  if (kCollectHistory) {\n+    thr->fast_state.IncrementEpoch();\n+    TraceAddEvent(thr, thr->fast_state, EventTypeFuncEnter, pc);\n+  }\n+\n+  // Shadow stack maintenance can be replaced with\n+  // stack unwinding during trace switch (which presumably must be faster).\n+  DCHECK_GE(thr->shadow_stack_pos, thr->shadow_stack);\n+#if !SANITIZER_GO\n+  DCHECK_LT(thr->shadow_stack_pos, thr->shadow_stack_end);\n+#else\n+  if (thr->shadow_stack_pos == thr->shadow_stack_end)\n+    GrowShadowStack(thr);\n+#endif\n+  thr->shadow_stack_pos[0] = pc;\n+  thr->shadow_stack_pos++;\n+}\n+\n+ALWAYS_INLINE\n+void FuncExit(ThreadState *thr) {\n+  DPrintf2(\"#%d: FuncExit\\n\", (int)thr->fast_state.tid());\n+  if (kCollectHistory) {\n+    thr->fast_state.IncrementEpoch();\n+    TraceAddEvent(thr, thr->fast_state, EventTypeFuncExit, 0);\n+  }\n+\n+  DCHECK_GT(thr->shadow_stack_pos, thr->shadow_stack);\n+#if !SANITIZER_GO\n+  DCHECK_LT(thr->shadow_stack_pos, thr->shadow_stack_end);\n+#endif\n+  thr->shadow_stack_pos--;\n+}\n+\n #if !SANITIZER_GO\n extern void (*on_initialize)(void);\n extern int (*on_finalize)(int);"}, {"sha": "7365fdaa303843d1aefe6b5bc15786b51d40ac2f", "filename": "libsanitizer/tsan/tsan_rtl_access.cpp", "status": "added", "additions": 604, "deletions": 0, "changes": 604, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_rtl_access.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_rtl_access.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl_access.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -0,0 +1,604 @@\n+//===-- tsan_rtl_access.cpp -----------------------------------------------===//\n+//\n+// Part of the LLVM Project, under the Apache License v2.0 with LLVM Exceptions.\n+// See https://llvm.org/LICENSE.txt for license information.\n+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception\n+//\n+//===----------------------------------------------------------------------===//\n+//\n+// This file is a part of ThreadSanitizer (TSan), a race detector.\n+//\n+// Definitions of memory access and function entry/exit entry points.\n+//===----------------------------------------------------------------------===//\n+\n+#include \"tsan_rtl.h\"\n+\n+namespace __tsan {\n+\n+namespace v3 {\n+\n+ALWAYS_INLINE USED bool TryTraceMemoryAccess(ThreadState *thr, uptr pc,\n+                                             uptr addr, uptr size,\n+                                             AccessType typ) {\n+  DCHECK(size == 1 || size == 2 || size == 4 || size == 8);\n+  if (!kCollectHistory)\n+    return true;\n+  EventAccess *ev;\n+  if (UNLIKELY(!TraceAcquire(thr, &ev)))\n+    return false;\n+  u64 size_log = size == 1 ? 0 : size == 2 ? 1 : size == 4 ? 2 : 3;\n+  uptr pc_delta = pc - thr->trace_prev_pc + (1 << (EventAccess::kPCBits - 1));\n+  thr->trace_prev_pc = pc;\n+  if (LIKELY(pc_delta < (1 << EventAccess::kPCBits))) {\n+    ev->is_access = 1;\n+    ev->is_read = !!(typ & kAccessRead);\n+    ev->is_atomic = !!(typ & kAccessAtomic);\n+    ev->size_log = size_log;\n+    ev->pc_delta = pc_delta;\n+    DCHECK_EQ(ev->pc_delta, pc_delta);\n+    ev->addr = CompressAddr(addr);\n+    TraceRelease(thr, ev);\n+    return true;\n+  }\n+  auto *evex = reinterpret_cast<EventAccessExt *>(ev);\n+  evex->is_access = 0;\n+  evex->is_func = 0;\n+  evex->type = EventType::kAccessExt;\n+  evex->is_read = !!(typ & kAccessRead);\n+  evex->is_atomic = !!(typ & kAccessAtomic);\n+  evex->size_log = size_log;\n+  evex->addr = CompressAddr(addr);\n+  evex->pc = pc;\n+  TraceRelease(thr, evex);\n+  return true;\n+}\n+\n+ALWAYS_INLINE USED bool TryTraceMemoryAccessRange(ThreadState *thr, uptr pc,\n+                                                  uptr addr, uptr size,\n+                                                  AccessType typ) {\n+  if (!kCollectHistory)\n+    return true;\n+  EventAccessRange *ev;\n+  if (UNLIKELY(!TraceAcquire(thr, &ev)))\n+    return false;\n+  thr->trace_prev_pc = pc;\n+  ev->is_access = 0;\n+  ev->is_func = 0;\n+  ev->type = EventType::kAccessRange;\n+  ev->is_read = !!(typ & kAccessRead);\n+  ev->is_free = !!(typ & kAccessFree);\n+  ev->size_lo = size;\n+  ev->pc = CompressAddr(pc);\n+  ev->addr = CompressAddr(addr);\n+  ev->size_hi = size >> EventAccessRange::kSizeLoBits;\n+  TraceRelease(thr, ev);\n+  return true;\n+}\n+\n+void TraceMemoryAccessRange(ThreadState *thr, uptr pc, uptr addr, uptr size,\n+                            AccessType typ) {\n+  if (LIKELY(TryTraceMemoryAccessRange(thr, pc, addr, size, typ)))\n+    return;\n+  TraceSwitchPart(thr);\n+  UNUSED bool res = TryTraceMemoryAccessRange(thr, pc, addr, size, typ);\n+  DCHECK(res);\n+}\n+\n+void TraceFunc(ThreadState *thr, uptr pc) {\n+  if (LIKELY(TryTraceFunc(thr, pc)))\n+    return;\n+  TraceSwitchPart(thr);\n+  UNUSED bool res = TryTraceFunc(thr, pc);\n+  DCHECK(res);\n+}\n+\n+void TraceMutexLock(ThreadState *thr, EventType type, uptr pc, uptr addr,\n+                    StackID stk) {\n+  DCHECK(type == EventType::kLock || type == EventType::kRLock);\n+  if (!kCollectHistory)\n+    return;\n+  EventLock ev;\n+  ev.is_access = 0;\n+  ev.is_func = 0;\n+  ev.type = type;\n+  ev.pc = CompressAddr(pc);\n+  ev.stack_lo = stk;\n+  ev.stack_hi = stk >> EventLock::kStackIDLoBits;\n+  ev._ = 0;\n+  ev.addr = CompressAddr(addr);\n+  TraceEvent(thr, ev);\n+}\n+\n+void TraceMutexUnlock(ThreadState *thr, uptr addr) {\n+  if (!kCollectHistory)\n+    return;\n+  EventUnlock ev;\n+  ev.is_access = 0;\n+  ev.is_func = 0;\n+  ev.type = EventType::kUnlock;\n+  ev._ = 0;\n+  ev.addr = CompressAddr(addr);\n+  TraceEvent(thr, ev);\n+}\n+\n+void TraceTime(ThreadState *thr) {\n+  if (!kCollectHistory)\n+    return;\n+  EventTime ev;\n+  ev.is_access = 0;\n+  ev.is_func = 0;\n+  ev.type = EventType::kTime;\n+  ev.sid = static_cast<u64>(thr->sid);\n+  ev.epoch = static_cast<u64>(thr->epoch);\n+  ev._ = 0;\n+  TraceEvent(thr, ev);\n+}\n+\n+}  // namespace v3\n+\n+ALWAYS_INLINE\n+Shadow LoadShadow(u64 *p) {\n+  u64 raw = atomic_load((atomic_uint64_t *)p, memory_order_relaxed);\n+  return Shadow(raw);\n+}\n+\n+ALWAYS_INLINE\n+void StoreShadow(u64 *sp, u64 s) {\n+  atomic_store((atomic_uint64_t *)sp, s, memory_order_relaxed);\n+}\n+\n+ALWAYS_INLINE\n+void StoreIfNotYetStored(u64 *sp, u64 *s) {\n+  StoreShadow(sp, *s);\n+  *s = 0;\n+}\n+\n+extern \"C\" void __tsan_report_race();\n+\n+ALWAYS_INLINE\n+void HandleRace(ThreadState *thr, u64 *shadow_mem, Shadow cur, Shadow old) {\n+  thr->racy_state[0] = cur.raw();\n+  thr->racy_state[1] = old.raw();\n+  thr->racy_shadow_addr = shadow_mem;\n+#if !SANITIZER_GO\n+  HACKY_CALL(__tsan_report_race);\n+#else\n+  ReportRace(thr);\n+#endif\n+}\n+\n+static inline bool HappensBefore(Shadow old, ThreadState *thr) {\n+  return thr->clock.get(old.TidWithIgnore()) >= old.epoch();\n+}\n+\n+ALWAYS_INLINE\n+void MemoryAccessImpl1(ThreadState *thr, uptr addr, int kAccessSizeLog,\n+                       bool kAccessIsWrite, bool kIsAtomic, u64 *shadow_mem,\n+                       Shadow cur) {\n+  // This potentially can live in an MMX/SSE scratch register.\n+  // The required intrinsics are:\n+  // __m128i _mm_move_epi64(__m128i*);\n+  // _mm_storel_epi64(u64*, __m128i);\n+  u64 store_word = cur.raw();\n+  bool stored = false;\n+\n+  // scan all the shadow values and dispatch to 4 categories:\n+  // same, replace, candidate and race (see comments below).\n+  // we consider only 3 cases regarding access sizes:\n+  // equal, intersect and not intersect. initially I considered\n+  // larger and smaller as well, it allowed to replace some\n+  // 'candidates' with 'same' or 'replace', but I think\n+  // it's just not worth it (performance- and complexity-wise).\n+\n+  Shadow old(0);\n+\n+  // It release mode we manually unroll the loop,\n+  // because empirically gcc generates better code this way.\n+  // However, we can't afford unrolling in debug mode, because the function\n+  // consumes almost 4K of stack. Gtest gives only 4K of stack to death test\n+  // threads, which is not enough for the unrolled loop.\n+#if SANITIZER_DEBUG\n+  for (int idx = 0; idx < 4; idx++) {\n+#  include \"tsan_update_shadow_word.inc\"\n+  }\n+#else\n+  int idx = 0;\n+#  include \"tsan_update_shadow_word.inc\"\n+  idx = 1;\n+  if (stored) {\n+#  include \"tsan_update_shadow_word.inc\"\n+  } else {\n+#  include \"tsan_update_shadow_word.inc\"\n+  }\n+  idx = 2;\n+  if (stored) {\n+#  include \"tsan_update_shadow_word.inc\"\n+  } else {\n+#  include \"tsan_update_shadow_word.inc\"\n+  }\n+  idx = 3;\n+  if (stored) {\n+#  include \"tsan_update_shadow_word.inc\"\n+  } else {\n+#  include \"tsan_update_shadow_word.inc\"\n+  }\n+#endif\n+\n+  // we did not find any races and had already stored\n+  // the current access info, so we are done\n+  if (LIKELY(stored))\n+    return;\n+  // choose a random candidate slot and replace it\n+  StoreShadow(shadow_mem + (cur.epoch() % kShadowCnt), store_word);\n+  return;\n+RACE:\n+  HandleRace(thr, shadow_mem, cur, old);\n+  return;\n+}\n+\n+void UnalignedMemoryAccess(ThreadState *thr, uptr pc, uptr addr, uptr size,\n+                           AccessType typ) {\n+  DCHECK(!(typ & kAccessAtomic));\n+  const bool kAccessIsWrite = !(typ & kAccessRead);\n+  const bool kIsAtomic = false;\n+  while (size) {\n+    int size1 = 1;\n+    int kAccessSizeLog = kSizeLog1;\n+    if (size >= 8 && (addr & ~7) == ((addr + 7) & ~7)) {\n+      size1 = 8;\n+      kAccessSizeLog = kSizeLog8;\n+    } else if (size >= 4 && (addr & ~7) == ((addr + 3) & ~7)) {\n+      size1 = 4;\n+      kAccessSizeLog = kSizeLog4;\n+    } else if (size >= 2 && (addr & ~7) == ((addr + 1) & ~7)) {\n+      size1 = 2;\n+      kAccessSizeLog = kSizeLog2;\n+    }\n+    MemoryAccess(thr, pc, addr, kAccessSizeLog, kAccessIsWrite, kIsAtomic);\n+    addr += size1;\n+    size -= size1;\n+  }\n+}\n+\n+ALWAYS_INLINE\n+bool ContainsSameAccessSlow(u64 *s, u64 a, u64 sync_epoch, bool is_write) {\n+  Shadow cur(a);\n+  for (uptr i = 0; i < kShadowCnt; i++) {\n+    Shadow old(LoadShadow(&s[i]));\n+    if (Shadow::Addr0AndSizeAreEqual(cur, old) &&\n+        old.TidWithIgnore() == cur.TidWithIgnore() &&\n+        old.epoch() > sync_epoch && old.IsAtomic() == cur.IsAtomic() &&\n+        old.IsRead() <= cur.IsRead())\n+      return true;\n+  }\n+  return false;\n+}\n+\n+#if TSAN_VECTORIZE\n+#  define SHUF(v0, v1, i0, i1, i2, i3)                    \\\n+    _mm_castps_si128(_mm_shuffle_ps(_mm_castsi128_ps(v0), \\\n+                                    _mm_castsi128_ps(v1), \\\n+                                    (i0)*1 + (i1)*4 + (i2)*16 + (i3)*64))\n+ALWAYS_INLINE\n+bool ContainsSameAccessFast(u64 *s, u64 a, u64 sync_epoch, bool is_write) {\n+  // This is an optimized version of ContainsSameAccessSlow.\n+  // load current access into access[0:63]\n+  const m128 access = _mm_cvtsi64_si128(a);\n+  // duplicate high part of access in addr0:\n+  // addr0[0:31]        = access[32:63]\n+  // addr0[32:63]       = access[32:63]\n+  // addr0[64:95]       = access[32:63]\n+  // addr0[96:127]      = access[32:63]\n+  const m128 addr0 = SHUF(access, access, 1, 1, 1, 1);\n+  // load 4 shadow slots\n+  const m128 shadow0 = _mm_load_si128((__m128i *)s);\n+  const m128 shadow1 = _mm_load_si128((__m128i *)s + 1);\n+  // load high parts of 4 shadow slots into addr_vect:\n+  // addr_vect[0:31]    = shadow0[32:63]\n+  // addr_vect[32:63]   = shadow0[96:127]\n+  // addr_vect[64:95]   = shadow1[32:63]\n+  // addr_vect[96:127]  = shadow1[96:127]\n+  m128 addr_vect = SHUF(shadow0, shadow1, 1, 3, 1, 3);\n+  if (!is_write) {\n+    // set IsRead bit in addr_vect\n+    const m128 rw_mask1 = _mm_cvtsi64_si128(1 << 15);\n+    const m128 rw_mask = SHUF(rw_mask1, rw_mask1, 0, 0, 0, 0);\n+    addr_vect = _mm_or_si128(addr_vect, rw_mask);\n+  }\n+  // addr0 == addr_vect?\n+  const m128 addr_res = _mm_cmpeq_epi32(addr0, addr_vect);\n+  // epoch1[0:63]       = sync_epoch\n+  const m128 epoch1 = _mm_cvtsi64_si128(sync_epoch);\n+  // epoch[0:31]        = sync_epoch[0:31]\n+  // epoch[32:63]       = sync_epoch[0:31]\n+  // epoch[64:95]       = sync_epoch[0:31]\n+  // epoch[96:127]      = sync_epoch[0:31]\n+  const m128 epoch = SHUF(epoch1, epoch1, 0, 0, 0, 0);\n+  // load low parts of shadow cell epochs into epoch_vect:\n+  // epoch_vect[0:31]   = shadow0[0:31]\n+  // epoch_vect[32:63]  = shadow0[64:95]\n+  // epoch_vect[64:95]  = shadow1[0:31]\n+  // epoch_vect[96:127] = shadow1[64:95]\n+  const m128 epoch_vect = SHUF(shadow0, shadow1, 0, 2, 0, 2);\n+  // epoch_vect >= sync_epoch?\n+  const m128 epoch_res = _mm_cmpgt_epi32(epoch_vect, epoch);\n+  // addr_res & epoch_res\n+  const m128 res = _mm_and_si128(addr_res, epoch_res);\n+  // mask[0] = res[7]\n+  // mask[1] = res[15]\n+  // ...\n+  // mask[15] = res[127]\n+  const int mask = _mm_movemask_epi8(res);\n+  return mask != 0;\n+}\n+#endif\n+\n+ALWAYS_INLINE\n+bool ContainsSameAccess(u64 *s, u64 a, u64 sync_epoch, bool is_write) {\n+#if TSAN_VECTORIZE\n+  bool res = ContainsSameAccessFast(s, a, sync_epoch, is_write);\n+  // NOTE: this check can fail if the shadow is concurrently mutated\n+  // by other threads. But it still can be useful if you modify\n+  // ContainsSameAccessFast and want to ensure that it's not completely broken.\n+  // DCHECK_EQ(res, ContainsSameAccessSlow(s, a, sync_epoch, is_write));\n+  return res;\n+#else\n+  return ContainsSameAccessSlow(s, a, sync_epoch, is_write);\n+#endif\n+}\n+\n+ALWAYS_INLINE USED void MemoryAccess(ThreadState *thr, uptr pc, uptr addr,\n+                                     int kAccessSizeLog, bool kAccessIsWrite,\n+                                     bool kIsAtomic) {\n+  RawShadow *shadow_mem = MemToShadow(addr);\n+  DPrintf2(\n+      \"#%d: MemoryAccess: @%p %p size=%d\"\n+      \" is_write=%d shadow_mem=%p {%zx, %zx, %zx, %zx}\\n\",\n+      (int)thr->fast_state.tid(), (void *)pc, (void *)addr,\n+      (int)(1 << kAccessSizeLog), kAccessIsWrite, shadow_mem,\n+      (uptr)shadow_mem[0], (uptr)shadow_mem[1], (uptr)shadow_mem[2],\n+      (uptr)shadow_mem[3]);\n+#if SANITIZER_DEBUG\n+  if (!IsAppMem(addr)) {\n+    Printf(\"Access to non app mem %zx\\n\", addr);\n+    DCHECK(IsAppMem(addr));\n+  }\n+  if (!IsShadowMem(shadow_mem)) {\n+    Printf(\"Bad shadow addr %p (%zx)\\n\", shadow_mem, addr);\n+    DCHECK(IsShadowMem(shadow_mem));\n+  }\n+#endif\n+\n+  if (!SANITIZER_GO && !kAccessIsWrite && *shadow_mem == kShadowRodata) {\n+    // Access to .rodata section, no races here.\n+    // Measurements show that it can be 10-20% of all memory accesses.\n+    return;\n+  }\n+\n+  FastState fast_state = thr->fast_state;\n+  if (UNLIKELY(fast_state.GetIgnoreBit())) {\n+    return;\n+  }\n+\n+  Shadow cur(fast_state);\n+  cur.SetAddr0AndSizeLog(addr & 7, kAccessSizeLog);\n+  cur.SetWrite(kAccessIsWrite);\n+  cur.SetAtomic(kIsAtomic);\n+\n+  if (LIKELY(ContainsSameAccess(shadow_mem, cur.raw(), thr->fast_synch_epoch,\n+                                kAccessIsWrite))) {\n+    return;\n+  }\n+\n+  if (kCollectHistory) {\n+    fast_state.IncrementEpoch();\n+    thr->fast_state = fast_state;\n+    TraceAddEvent(thr, fast_state, EventTypeMop, pc);\n+    cur.IncrementEpoch();\n+  }\n+\n+  MemoryAccessImpl1(thr, addr, kAccessSizeLog, kAccessIsWrite, kIsAtomic,\n+                    shadow_mem, cur);\n+}\n+\n+// Called by MemoryAccessRange in tsan_rtl_thread.cpp\n+ALWAYS_INLINE USED void MemoryAccessImpl(ThreadState *thr, uptr addr,\n+                                         int kAccessSizeLog,\n+                                         bool kAccessIsWrite, bool kIsAtomic,\n+                                         u64 *shadow_mem, Shadow cur) {\n+  if (LIKELY(ContainsSameAccess(shadow_mem, cur.raw(), thr->fast_synch_epoch,\n+                                kAccessIsWrite))) {\n+    return;\n+  }\n+\n+  MemoryAccessImpl1(thr, addr, kAccessSizeLog, kAccessIsWrite, kIsAtomic,\n+                    shadow_mem, cur);\n+}\n+\n+static void MemoryRangeSet(ThreadState *thr, uptr pc, uptr addr, uptr size,\n+                           u64 val) {\n+  (void)thr;\n+  (void)pc;\n+  if (size == 0)\n+    return;\n+  // FIXME: fix me.\n+  uptr offset = addr % kShadowCell;\n+  if (offset) {\n+    offset = kShadowCell - offset;\n+    if (size <= offset)\n+      return;\n+    addr += offset;\n+    size -= offset;\n+  }\n+  DCHECK_EQ(addr % 8, 0);\n+  // If a user passes some insane arguments (memset(0)),\n+  // let it just crash as usual.\n+  if (!IsAppMem(addr) || !IsAppMem(addr + size - 1))\n+    return;\n+  // Don't want to touch lots of shadow memory.\n+  // If a program maps 10MB stack, there is no need reset the whole range.\n+  size = (size + (kShadowCell - 1)) & ~(kShadowCell - 1);\n+  // UnmapOrDie/MmapFixedNoReserve does not work on Windows.\n+  if (SANITIZER_WINDOWS || size < common_flags()->clear_shadow_mmap_threshold) {\n+    RawShadow *p = MemToShadow(addr);\n+    CHECK(IsShadowMem(p));\n+    CHECK(IsShadowMem(p + size * kShadowCnt / kShadowCell - 1));\n+    // FIXME: may overwrite a part outside the region\n+    for (uptr i = 0; i < size / kShadowCell * kShadowCnt;) {\n+      p[i++] = val;\n+      for (uptr j = 1; j < kShadowCnt; j++) p[i++] = 0;\n+    }\n+  } else {\n+    // The region is big, reset only beginning and end.\n+    const uptr kPageSize = GetPageSizeCached();\n+    RawShadow *begin = MemToShadow(addr);\n+    RawShadow *end = begin + size / kShadowCell * kShadowCnt;\n+    RawShadow *p = begin;\n+    // Set at least first kPageSize/2 to page boundary.\n+    while ((p < begin + kPageSize / kShadowSize / 2) || ((uptr)p % kPageSize)) {\n+      *p++ = val;\n+      for (uptr j = 1; j < kShadowCnt; j++) *p++ = 0;\n+    }\n+    // Reset middle part.\n+    RawShadow *p1 = p;\n+    p = RoundDown(end, kPageSize);\n+    if (!MmapFixedSuperNoReserve((uptr)p1, (uptr)p - (uptr)p1))\n+      Die();\n+    // Set the ending.\n+    while (p < end) {\n+      *p++ = val;\n+      for (uptr j = 1; j < kShadowCnt; j++) *p++ = 0;\n+    }\n+  }\n+}\n+\n+void MemoryResetRange(ThreadState *thr, uptr pc, uptr addr, uptr size) {\n+  MemoryRangeSet(thr, pc, addr, size, 0);\n+}\n+\n+void MemoryRangeFreed(ThreadState *thr, uptr pc, uptr addr, uptr size) {\n+  // Processing more than 1k (4k of shadow) is expensive,\n+  // can cause excessive memory consumption (user does not necessary touch\n+  // the whole range) and most likely unnecessary.\n+  if (size > 1024)\n+    size = 1024;\n+  CHECK_EQ(thr->is_freeing, false);\n+  thr->is_freeing = true;\n+  MemoryAccessRange(thr, pc, addr, size, true);\n+  thr->is_freeing = false;\n+  if (kCollectHistory) {\n+    thr->fast_state.IncrementEpoch();\n+    TraceAddEvent(thr, thr->fast_state, EventTypeMop, pc);\n+  }\n+  Shadow s(thr->fast_state);\n+  s.ClearIgnoreBit();\n+  s.MarkAsFreed();\n+  s.SetWrite(true);\n+  s.SetAddr0AndSizeLog(0, 3);\n+  MemoryRangeSet(thr, pc, addr, size, s.raw());\n+}\n+\n+void MemoryRangeImitateWrite(ThreadState *thr, uptr pc, uptr addr, uptr size) {\n+  if (kCollectHistory) {\n+    thr->fast_state.IncrementEpoch();\n+    TraceAddEvent(thr, thr->fast_state, EventTypeMop, pc);\n+  }\n+  Shadow s(thr->fast_state);\n+  s.ClearIgnoreBit();\n+  s.SetWrite(true);\n+  s.SetAddr0AndSizeLog(0, 3);\n+  MemoryRangeSet(thr, pc, addr, size, s.raw());\n+}\n+\n+void MemoryRangeImitateWriteOrResetRange(ThreadState *thr, uptr pc, uptr addr,\n+                                         uptr size) {\n+  if (thr->ignore_reads_and_writes == 0)\n+    MemoryRangeImitateWrite(thr, pc, addr, size);\n+  else\n+    MemoryResetRange(thr, pc, addr, size);\n+}\n+\n+void MemoryAccessRange(ThreadState *thr, uptr pc, uptr addr, uptr size,\n+                       bool is_write) {\n+  if (size == 0)\n+    return;\n+\n+  RawShadow *shadow_mem = MemToShadow(addr);\n+  DPrintf2(\"#%d: MemoryAccessRange: @%p %p size=%d is_write=%d\\n\", thr->tid,\n+           (void *)pc, (void *)addr, (int)size, is_write);\n+\n+#if SANITIZER_DEBUG\n+  if (!IsAppMem(addr)) {\n+    Printf(\"Access to non app mem %zx\\n\", addr);\n+    DCHECK(IsAppMem(addr));\n+  }\n+  if (!IsAppMem(addr + size - 1)) {\n+    Printf(\"Access to non app mem %zx\\n\", addr + size - 1);\n+    DCHECK(IsAppMem(addr + size - 1));\n+  }\n+  if (!IsShadowMem(shadow_mem)) {\n+    Printf(\"Bad shadow addr %p (%zx)\\n\", shadow_mem, addr);\n+    DCHECK(IsShadowMem(shadow_mem));\n+  }\n+  if (!IsShadowMem(shadow_mem + size * kShadowCnt / 8 - 1)) {\n+    Printf(\"Bad shadow addr %p (%zx)\\n\", shadow_mem + size * kShadowCnt / 8 - 1,\n+           addr + size - 1);\n+    DCHECK(IsShadowMem(shadow_mem + size * kShadowCnt / 8 - 1));\n+  }\n+#endif\n+\n+  if (*shadow_mem == kShadowRodata) {\n+    DCHECK(!is_write);\n+    // Access to .rodata section, no races here.\n+    // Measurements show that it can be 10-20% of all memory accesses.\n+    return;\n+  }\n+\n+  FastState fast_state = thr->fast_state;\n+  if (fast_state.GetIgnoreBit())\n+    return;\n+\n+  fast_state.IncrementEpoch();\n+  thr->fast_state = fast_state;\n+  TraceAddEvent(thr, fast_state, EventTypeMop, pc);\n+\n+  bool unaligned = (addr % kShadowCell) != 0;\n+\n+  // Handle unaligned beginning, if any.\n+  for (; addr % kShadowCell && size; addr++, size--) {\n+    int const kAccessSizeLog = 0;\n+    Shadow cur(fast_state);\n+    cur.SetWrite(is_write);\n+    cur.SetAddr0AndSizeLog(addr & (kShadowCell - 1), kAccessSizeLog);\n+    MemoryAccessImpl(thr, addr, kAccessSizeLog, is_write, false, shadow_mem,\n+                     cur);\n+  }\n+  if (unaligned)\n+    shadow_mem += kShadowCnt;\n+  // Handle middle part, if any.\n+  for (; size >= kShadowCell; addr += kShadowCell, size -= kShadowCell) {\n+    int const kAccessSizeLog = 3;\n+    Shadow cur(fast_state);\n+    cur.SetWrite(is_write);\n+    cur.SetAddr0AndSizeLog(0, kAccessSizeLog);\n+    MemoryAccessImpl(thr, addr, kAccessSizeLog, is_write, false, shadow_mem,\n+                     cur);\n+    shadow_mem += kShadowCnt;\n+  }\n+  // Handle ending, if any.\n+  for (; size; addr++, size--) {\n+    int const kAccessSizeLog = 0;\n+    Shadow cur(fast_state);\n+    cur.SetWrite(is_write);\n+    cur.SetAddr0AndSizeLog(addr & (kShadowCell - 1), kAccessSizeLog);\n+    MemoryAccessImpl(thr, addr, kAccessSizeLog, is_write, false, shadow_mem,\n+                     cur);\n+  }\n+}\n+\n+}  // namespace __tsan\n+\n+#if !SANITIZER_GO\n+// Must be included in this file to make sure everything is inlined.\n+#  include \"tsan_interface.inc\"\n+#endif"}, {"sha": "8285e21aa1ec7a797dfcf4840ee5a7851106b497", "filename": "libsanitizer/tsan/tsan_rtl_ppc64.S", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_rtl_ppc64.S", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_rtl_ppc64.S", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl_ppc64.S?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -1,6 +1,5 @@\n #include \"tsan_ppc_regs.h\"\n \n-        .machine altivec\n         .section .text\n         .hidden __tsan_setjmp\n         .globl _setjmp"}, {"sha": "811695d144c56857ac0efd485d648aaa5136bd60", "filename": "libsanitizer/tsan/tsan_rtl_report.cpp", "status": "modified", "additions": 8, "deletions": 14, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_rtl_report.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_rtl_report.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl_report.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -560,9 +560,7 @@ bool RestoreStack(Tid tid, EventType type, Sid sid, Epoch epoch, uptr addr,\n     if (tctx->thr)\n       last_pos = (Event *)atomic_load_relaxed(&tctx->thr->trace_pos);\n   }\n-  // Too large for stack.\n-  alignas(MutexSet) static char mset_storage[sizeof(MutexSet)];\n-  MutexSet &mset = *new (mset_storage) MutexSet();\n+  DynamicMutexSet mset;\n   Vector<uptr> stack;\n   uptr prev_pc = 0;\n   bool found = false;\n@@ -588,7 +586,7 @@ bool RestoreStack(Tid tid, EventType type, Sid sid, Epoch epoch, uptr addr,\n           if (match && type == EventType::kAccessExt &&\n               IsWithinAccess(addr, size, ev_addr, ev_size) &&\n               is_read == ev->is_read && is_atomic == ev->is_atomic && !is_free)\n-            RestoreStackMatch(pstk, pmset, &stack, &mset, ev_pc, &found);\n+            RestoreStackMatch(pstk, pmset, &stack, mset, ev_pc, &found);\n           return;\n         }\n         if (evp->is_func) {\n@@ -615,7 +613,7 @@ bool RestoreStack(Tid tid, EventType type, Sid sid, Epoch epoch, uptr addr,\n                 IsWithinAccess(addr, size, ev_addr, ev_size) &&\n                 is_read == ev->is_read && is_atomic == ev->is_atomic &&\n                 !is_free)\n-              RestoreStackMatch(pstk, pmset, &stack, &mset, ev->pc, &found);\n+              RestoreStackMatch(pstk, pmset, &stack, mset, ev->pc, &found);\n             break;\n           }\n           case EventType::kAccessRange: {\n@@ -630,7 +628,7 @@ bool RestoreStack(Tid tid, EventType type, Sid sid, Epoch epoch, uptr addr,\n             if (match && type == EventType::kAccessExt &&\n                 IsWithinAccess(addr, size, ev_addr, ev_size) &&\n                 is_read == ev->is_read && !is_atomic && is_free == ev->is_free)\n-              RestoreStackMatch(pstk, pmset, &stack, &mset, ev_pc, &found);\n+              RestoreStackMatch(pstk, pmset, &stack, mset, ev_pc, &found);\n             break;\n           }\n           case EventType::kLock:\n@@ -644,18 +642,18 @@ bool RestoreStack(Tid tid, EventType type, Sid sid, Epoch epoch, uptr addr,\n                 (ev->stack_hi << EventLock::kStackIDLoBits) + ev->stack_lo;\n             DPrintf2(\"  Lock: pc=0x%zx addr=0x%zx stack=%u write=%d\\n\", ev_pc,\n                      ev_addr, stack_id, is_write);\n-            mset.AddAddr(ev_addr, stack_id, is_write);\n+            mset->AddAddr(ev_addr, stack_id, is_write);\n             // Events with ev_pc == 0 are written to the beginning of trace\n             // part as initial mutex set (are not real).\n             if (match && type == EventType::kLock && addr == ev_addr && ev_pc)\n-              RestoreStackMatch(pstk, pmset, &stack, &mset, ev_pc, &found);\n+              RestoreStackMatch(pstk, pmset, &stack, mset, ev_pc, &found);\n             break;\n           }\n           case EventType::kUnlock: {\n             auto *ev = reinterpret_cast<EventUnlock *>(evp);\n             uptr ev_addr = RestoreAddr(ev->addr);\n             DPrintf2(\"  Unlock: addr=0x%zx\\n\", ev_addr);\n-            mset.DelAddr(ev_addr);\n+            mset->DelAddr(ev_addr);\n             break;\n           }\n           case EventType::kTime:\n@@ -897,11 +895,7 @@ void ReportRace(ThreadState *thr) {\n   if (IsFiredSuppression(ctx, typ, traces[0]))\n     return;\n \n-  // MutexSet is too large to live on stack.\n-  Vector<u64> mset_buffer;\n-  mset_buffer.Resize(sizeof(MutexSet) / sizeof(u64) + 1);\n-  MutexSet *mset2 = new(&mset_buffer[0]) MutexSet();\n-\n+  DynamicMutexSet mset2;\n   Shadow s2(thr->racy_state[1]);\n   RestoreStack(s2.tid(), s2.epoch(), &traces[1], mset2, &tags[1]);\n   if (IsFiredSuppression(ctx, typ, traces[1]))"}, {"sha": "6e652ee8a654847f6a9b48c4b5066988dd9d9948", "filename": "libsanitizer/tsan/tsan_rtl_thread.cpp", "status": "modified", "additions": 0, "deletions": 79, "changes": 79, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_rtl_thread.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Ftsan%2Ftsan_rtl_thread.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Ftsan%2Ftsan_rtl_thread.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -323,85 +323,6 @@ void ThreadSetName(ThreadState *thr, const char *name) {\n   ctx->thread_registry.SetThreadName(thr->tid, name);\n }\n \n-void MemoryAccessRange(ThreadState *thr, uptr pc, uptr addr,\n-                       uptr size, bool is_write) {\n-  if (size == 0)\n-    return;\n-\n-  RawShadow *shadow_mem = MemToShadow(addr);\n-  DPrintf2(\"#%d: MemoryAccessRange: @%p %p size=%d is_write=%d\\n\",\n-      thr->tid, (void*)pc, (void*)addr,\n-      (int)size, is_write);\n-\n-#if SANITIZER_DEBUG\n-  if (!IsAppMem(addr)) {\n-    Printf(\"Access to non app mem %zx\\n\", addr);\n-    DCHECK(IsAppMem(addr));\n-  }\n-  if (!IsAppMem(addr + size - 1)) {\n-    Printf(\"Access to non app mem %zx\\n\", addr + size - 1);\n-    DCHECK(IsAppMem(addr + size - 1));\n-  }\n-  if (!IsShadowMem(shadow_mem)) {\n-    Printf(\"Bad shadow addr %p (%zx)\\n\", shadow_mem, addr);\n-    DCHECK(IsShadowMem(shadow_mem));\n-  }\n-  if (!IsShadowMem(shadow_mem + size * kShadowCnt / 8 - 1)) {\n-    Printf(\"Bad shadow addr %p (%zx)\\n\",\n-               shadow_mem + size * kShadowCnt / 8 - 1, addr + size - 1);\n-    DCHECK(IsShadowMem(shadow_mem + size * kShadowCnt / 8 - 1));\n-  }\n-#endif\n-\n-  if (*shadow_mem == kShadowRodata) {\n-    DCHECK(!is_write);\n-    // Access to .rodata section, no races here.\n-    // Measurements show that it can be 10-20% of all memory accesses.\n-    return;\n-  }\n-\n-  FastState fast_state = thr->fast_state;\n-  if (fast_state.GetIgnoreBit())\n-    return;\n-\n-  fast_state.IncrementEpoch();\n-  thr->fast_state = fast_state;\n-  TraceAddEvent(thr, fast_state, EventTypeMop, pc);\n-\n-  bool unaligned = (addr % kShadowCell) != 0;\n-\n-  // Handle unaligned beginning, if any.\n-  for (; addr % kShadowCell && size; addr++, size--) {\n-    int const kAccessSizeLog = 0;\n-    Shadow cur(fast_state);\n-    cur.SetWrite(is_write);\n-    cur.SetAddr0AndSizeLog(addr & (kShadowCell - 1), kAccessSizeLog);\n-    MemoryAccessImpl(thr, addr, kAccessSizeLog, is_write, false,\n-        shadow_mem, cur);\n-  }\n-  if (unaligned)\n-    shadow_mem += kShadowCnt;\n-  // Handle middle part, if any.\n-  for (; size >= kShadowCell; addr += kShadowCell, size -= kShadowCell) {\n-    int const kAccessSizeLog = 3;\n-    Shadow cur(fast_state);\n-    cur.SetWrite(is_write);\n-    cur.SetAddr0AndSizeLog(0, kAccessSizeLog);\n-    MemoryAccessImpl(thr, addr, kAccessSizeLog, is_write, false,\n-        shadow_mem, cur);\n-    shadow_mem += kShadowCnt;\n-  }\n-  // Handle ending, if any.\n-  for (; size; addr++, size--) {\n-    int const kAccessSizeLog = 0;\n-    Shadow cur(fast_state);\n-    cur.SetWrite(is_write);\n-    cur.SetAddr0AndSizeLog(addr & (kShadowCell - 1), kAccessSizeLog);\n-    MemoryAccessImpl(thr, addr, kAccessSizeLog, is_write, false,\n-        shadow_mem, cur);\n-  }\n-}\n-\n #if !SANITIZER_GO\n void FiberSwitchImpl(ThreadState *from, ThreadState *to) {\n   Processor *proc = from->proc();"}, {"sha": "25cefd46ce27ced7fb6092d8d04b5074c56ebe95", "filename": "libsanitizer/ubsan/ubsan_flags.cpp", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fubsan%2Fubsan_flags.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fubsan%2Fubsan_flags.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fubsan%2Fubsan_flags.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -50,7 +50,6 @@ void InitializeFlags() {\n   {\n     CommonFlags cf;\n     cf.CopyFrom(*common_flags());\n-    cf.print_summary = false;\n     cf.external_symbolizer_path = GetFlag(\"UBSAN_SYMBOLIZER_PATH\");\n     OverrideCommonFlags(cf);\n   }"}, {"sha": "e201e6bba22078e3d873aeb5792b98ecd860cdaa", "filename": "libsanitizer/ubsan/ubsan_handlers.cpp", "status": "modified", "additions": 0, "deletions": 15, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fubsan%2Fubsan_handlers.cpp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fubsan%2Fubsan_handlers.cpp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fubsan%2Fubsan_handlers.cpp?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -894,21 +894,6 @@ void __ubsan_handle_cfi_bad_type(CFICheckFailData *Data, ValueHandle Vtable,\n \n }  // namespace __ubsan\n \n-void __ubsan::__ubsan_handle_cfi_bad_icall(CFIBadIcallData *CallData,\n-                                           ValueHandle Function) {\n-  GET_REPORT_OPTIONS(false);\n-  CFICheckFailData Data = {CFITCK_ICall, CallData->Loc, CallData->Type};\n-  handleCFIBadIcall(&Data, Function, Opts);\n-}\n-\n-void __ubsan::__ubsan_handle_cfi_bad_icall_abort(CFIBadIcallData *CallData,\n-                                                 ValueHandle Function) {\n-  GET_REPORT_OPTIONS(true);\n-  CFICheckFailData Data = {CFITCK_ICall, CallData->Loc, CallData->Type};\n-  handleCFIBadIcall(&Data, Function, Opts);\n-  Die();\n-}\n-\n void __ubsan::__ubsan_handle_cfi_check_fail(CFICheckFailData *Data,\n                                             ValueHandle Value,\n                                             uptr ValidVtable) {"}, {"sha": "219fb15de55fe02a4544422095baa0d8532baaa0", "filename": "libsanitizer/ubsan/ubsan_handlers.h", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fubsan%2Fubsan_handlers.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fubsan%2Fubsan_handlers.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fubsan%2Fubsan_handlers.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -215,20 +215,12 @@ enum CFITypeCheckKind : unsigned char {\n   CFITCK_VMFCall,\n };\n \n-struct CFIBadIcallData {\n-  SourceLocation Loc;\n-  const TypeDescriptor &Type;\n-};\n-\n struct CFICheckFailData {\n   CFITypeCheckKind CheckKind;\n   SourceLocation Loc;\n   const TypeDescriptor &Type;\n };\n \n-/// \\brief Handle control flow integrity failure for indirect function calls.\n-RECOVERABLE(cfi_bad_icall, CFIBadIcallData *Data, ValueHandle Function)\n-\n /// \\brief Handle control flow integrity failures.\n RECOVERABLE(cfi_check_fail, CFICheckFailData *Data, ValueHandle Function,\n             uptr VtableIsValid)"}, {"sha": "d2cc2e10bd2f023b8d9aa1685a79a192a6d1e1e8", "filename": "libsanitizer/ubsan/ubsan_platform.h", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fubsan%2Fubsan_platform.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cb0437584bb9a3b26cbc80bd8213ab9d4efc0237/libsanitizer%2Fubsan%2Fubsan_platform.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libsanitizer%2Fubsan%2Fubsan_platform.h?ref=cb0437584bb9a3b26cbc80bd8213ab9d4efc0237", "patch": "@@ -12,7 +12,6 @@\n #ifndef UBSAN_PLATFORM_H\n #define UBSAN_PLATFORM_H\n \n-#ifndef CAN_SANITIZE_UB\n // Other platforms should be easy to add, and probably work as-is.\n #if defined(__linux__) || defined(__FreeBSD__) || defined(__APPLE__) ||        \\\n     defined(__NetBSD__) || defined(__DragonFly__) ||                           \\\n@@ -22,6 +21,5 @@\n #else\n # define CAN_SANITIZE_UB 0\n #endif\n-#endif //CAN_SANITIZE_UB\n \n #endif"}]}