{"sha": "1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MWZmYmFhM2ZlYzdmM2RjYmMzZDFmOTk5ZThmYTNmMDkzMGIzZTFhZA==", "commit": {"author": {"name": "Richard Biener", "email": "rguenther@suse.de", "date": "2015-06-25T14:02:35Z"}, "committer": {"name": "Richard Biener", "email": "rguenth@gcc.gnu.org", "date": "2015-06-25T14:02:35Z"}, "message": "fold-const.c (fold_binary_loc): Move simplification of (X <<>> C1) & C2 ...\n\n2015-06-25  Richard Biener  <rguenther@suse.de>\n\n\t* fold-const.c (fold_binary_loc): Move simplification of\n\t(X <<>> C1) & C2 ...\n\t* match.pd: ... here.\n\n\t* gcc.dg/tree-ssa/pr52631.c: Disable forwprop.\n\nFrom-SVN: r224943", "tree": {"sha": "79fe2be8da7e7adc8c841bd6be0c8371ed853624", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/79fe2be8da7e7adc8c841bd6be0c8371ed853624"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad", "html_url": "https://github.com/Rust-GCC/gccrs/commit/1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad/comments", "author": {"login": "rguenth", "id": 2046526, "node_id": "MDQ6VXNlcjIwNDY1MjY=", "avatar_url": "https://avatars.githubusercontent.com/u/2046526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rguenth", "html_url": "https://github.com/rguenth", "followers_url": "https://api.github.com/users/rguenth/followers", "following_url": "https://api.github.com/users/rguenth/following{/other_user}", "gists_url": "https://api.github.com/users/rguenth/gists{/gist_id}", "starred_url": "https://api.github.com/users/rguenth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rguenth/subscriptions", "organizations_url": "https://api.github.com/users/rguenth/orgs", "repos_url": "https://api.github.com/users/rguenth/repos", "events_url": "https://api.github.com/users/rguenth/events{/privacy}", "received_events_url": "https://api.github.com/users/rguenth/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "0cf094c0fcd24069fee5d4eebdcfde87c066b3c3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0cf094c0fcd24069fee5d4eebdcfde87c066b3c3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0cf094c0fcd24069fee5d4eebdcfde87c066b3c3"}], "stats": {"total": 203, "additions": 102, "deletions": 101}, "files": [{"sha": "604b7635b061be7fa88e7c188a719794199ee95b", "filename": "gcc/ChangeLog", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad", "patch": "@@ -1,3 +1,9 @@\n+2015-06-25  Richard Biener  <rguenther@suse.de>\n+\n+\t* fold-const.c (fold_binary_loc): Move simplification of\n+\t(X <<>> C1) & C2 ...\n+\t* match.pd: ... here.\n+\n 2015-06-25  Eric Botcazou  <ebotcazou@adacore.com>\n \n \t* lto-streamer-out.c (DFS::hash_scc): Fix typos & formatting glitches."}, {"sha": "6fa784a17cbf704975b62c4f8162113386cbf2a9", "filename": "gcc/fold-const.c", "status": "modified", "additions": 0, "deletions": 100, "changes": 100, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad/gcc%2Ffold-const.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad/gcc%2Ffold-const.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ffold-const.c?ref=1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad", "patch": "@@ -11516,106 +11516,6 @@ fold_binary_loc (location_t loc,\n \t    return build_int_cst (type, residue & low);\n \t}\n \n-      /* Fold (X << C1) & C2 into (X << C1) & (C2 | ((1 << C1) - 1))\n-\t      (X >> C1) & C2 into (X >> C1) & (C2 | ~((type) -1 >> C1))\n-\t if the new mask might be further optimized.  */\n-      if ((TREE_CODE (arg0) == LSHIFT_EXPR\n-\t   || TREE_CODE (arg0) == RSHIFT_EXPR)\n-\t  && TYPE_PRECISION (TREE_TYPE (arg0)) <= HOST_BITS_PER_WIDE_INT\n-\t  && TREE_CODE (arg1) == INTEGER_CST\n-\t  && tree_fits_uhwi_p (TREE_OPERAND (arg0, 1))\n-\t  && tree_to_uhwi (TREE_OPERAND (arg0, 1)) > 0\n-\t  && (tree_to_uhwi (TREE_OPERAND (arg0, 1))\n-\t      < TYPE_PRECISION (TREE_TYPE (arg0))))\n-\t{\n-\t  unsigned int shiftc = tree_to_uhwi (TREE_OPERAND (arg0, 1));\n-\t  unsigned HOST_WIDE_INT mask = TREE_INT_CST_LOW (arg1);\n-\t  unsigned HOST_WIDE_INT newmask, zerobits = 0;\n-\t  tree shift_type = TREE_TYPE (arg0);\n-\n-\t  if (TREE_CODE (arg0) == LSHIFT_EXPR)\n-\t    zerobits = ((((unsigned HOST_WIDE_INT) 1) << shiftc) - 1);\n-\t  else if (TREE_CODE (arg0) == RSHIFT_EXPR\n-\t\t   && TYPE_PRECISION (TREE_TYPE (arg0))\n-\t\t      == GET_MODE_PRECISION (TYPE_MODE (TREE_TYPE (arg0))))\n-\t    {\n-\t      prec = TYPE_PRECISION (TREE_TYPE (arg0));\n-\t      tree arg00 = TREE_OPERAND (arg0, 0);\n-\t      /* See if more bits can be proven as zero because of\n-\t\t zero extension.  */\n-\t      if (TREE_CODE (arg00) == NOP_EXPR\n-\t\t  && TYPE_UNSIGNED (TREE_TYPE (TREE_OPERAND (arg00, 0))))\n-\t\t{\n-\t\t  tree inner_type = TREE_TYPE (TREE_OPERAND (arg00, 0));\n-\t\t  if (TYPE_PRECISION (inner_type)\n-\t\t      == GET_MODE_PRECISION (TYPE_MODE (inner_type))\n-\t\t      && TYPE_PRECISION (inner_type) < prec)\n-\t\t    {\n-\t\t      prec = TYPE_PRECISION (inner_type);\n-\t\t      /* See if we can shorten the right shift.  */\n-\t\t      if (shiftc < prec)\n-\t\t\tshift_type = inner_type;\n-\t\t      /* Otherwise X >> C1 is all zeros, so we'll optimize\n-\t\t\t it into (X, 0) later on by making sure zerobits\n-\t\t\t is all ones.  */\n-\t\t    }\n-\t\t}\n-\t      zerobits = ~(unsigned HOST_WIDE_INT) 0;\n-\t      if (shiftc < prec)\n-\t\t{\n-\t\t  zerobits >>= HOST_BITS_PER_WIDE_INT - shiftc;\n-\t\t  zerobits <<= prec - shiftc;\n-\t\t}\n-\t      /* For arithmetic shift if sign bit could be set, zerobits\n-\t\t can contain actually sign bits, so no transformation is\n-\t\t possible, unless MASK masks them all away.  In that\n-\t\t case the shift needs to be converted into logical shift.  */\n-\t      if (!TYPE_UNSIGNED (TREE_TYPE (arg0))\n-\t\t  && prec == TYPE_PRECISION (TREE_TYPE (arg0)))\n-\t\t{\n-\t\t  if ((mask & zerobits) == 0)\n-\t\t    shift_type = unsigned_type_for (TREE_TYPE (arg0));\n-\t\t  else\n-\t\t    zerobits = 0;\n-\t\t}\n-\t    }\n-\n-\t  /* ((X << 16) & 0xff00) is (X, 0).  */\n-\t  if ((mask & zerobits) == mask)\n-\t    return omit_one_operand_loc (loc, type,\n-\t\t\t\t\t build_int_cst (type, 0), arg0);\n-\n-\t  newmask = mask | zerobits;\n-\t  if (newmask != mask && (newmask & (newmask + 1)) == 0)\n-\t    {\n-\t      /* Only do the transformation if NEWMASK is some integer\n-\t\t mode's mask.  */\n-\t      for (prec = BITS_PER_UNIT;\n-\t\t   prec < HOST_BITS_PER_WIDE_INT; prec <<= 1)\n-\t\tif (newmask == (((unsigned HOST_WIDE_INT) 1) << prec) - 1)\n-\t\t  break;\n-\t      if (prec < HOST_BITS_PER_WIDE_INT\n-\t\t  || newmask == ~(unsigned HOST_WIDE_INT) 0)\n-\t\t{\n-\t\t  tree newmaskt;\n-\n-\t\t  if (shift_type != TREE_TYPE (arg0))\n-\t\t    {\n-\t\t      tem = fold_build2_loc (loc, TREE_CODE (arg0), shift_type,\n-\t\t\t\t\t fold_convert_loc (loc, shift_type,\n-\t\t\t\t\t\t\t   TREE_OPERAND (arg0, 0)),\n-\t\t\t\t\t TREE_OPERAND (arg0, 1));\n-\t\t      tem = fold_convert_loc (loc, type, tem);\n-\t\t    }\n-\t\t  else\n-\t\t    tem = op0;\n-\t\t  newmaskt = build_int_cst_type (TREE_TYPE (op1), newmask);\n-\t\t  if (!tree_int_cst_equal (newmaskt, arg1))\n-\t\t    return fold_build2_loc (loc, BIT_AND_EXPR, type, tem, newmaskt);\n-\t\t}\n-\t    }\n-\t}\n-\n       goto associate;\n \n     case RDIV_EXPR:"}, {"sha": "292ce6d75c690347a08a810aa91370de881ca467", "filename": "gcc/match.pd", "status": "modified", "additions": 91, "deletions": 0, "changes": 91, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad/gcc%2Fmatch.pd", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad/gcc%2Fmatch.pd", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fmatch.pd?ref=1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad", "patch": "@@ -738,6 +738,97 @@ along with GCC; see the file COPYING3.  If not see\n \t&& wi::eq_p (wi::lshift (@0, cand), @2))\n     (cmp @1 { build_int_cst (TREE_TYPE (@1), cand); })))))\n \n+/* Fold (X << C1) & C2 into (X << C1) & (C2 | ((1 << C1) - 1))\n+        (X >> C1) & C2 into (X >> C1) & (C2 | ~((type) -1 >> C1))\n+   if the new mask might be further optimized.  */\n+(for shift (lshift rshift)\n+ (simplify\n+  (bit_and (convert?@4 (shift@5 (convert1?@3 @0) INTEGER_CST@1)) INTEGER_CST@2)\n+   (if (tree_nop_conversion_p (TREE_TYPE (@4), TREE_TYPE (@5))\n+\t&& TYPE_PRECISION (type) <= HOST_BITS_PER_WIDE_INT\n+\t&& tree_fits_uhwi_p (@1)\n+\t&& tree_to_uhwi (@1) > 0\n+\t&& tree_to_uhwi (@1) < TYPE_PRECISION (type))\n+    (with\n+     {\n+       unsigned int shiftc = tree_to_uhwi (@1);\n+       unsigned HOST_WIDE_INT mask = TREE_INT_CST_LOW (@2);\n+       unsigned HOST_WIDE_INT newmask, zerobits = 0;\n+       tree shift_type = TREE_TYPE (@3);\n+       unsigned int prec;\n+\n+       if (shift == LSHIFT_EXPR)\n+\t zerobits = ((((unsigned HOST_WIDE_INT) 1) << shiftc) - 1);\n+       else if (shift == RSHIFT_EXPR\n+\t\t&& (TYPE_PRECISION (shift_type)\n+\t\t    == GET_MODE_PRECISION (TYPE_MODE (shift_type))))\n+\t {\n+\t   prec = TYPE_PRECISION (TREE_TYPE (@3));\n+\t   tree arg00 = @0;\n+\t   /* See if more bits can be proven as zero because of\n+\t      zero extension.  */\n+\t   if (@3 != @0\n+\t       && TYPE_UNSIGNED (TREE_TYPE (@0)))\n+\t     {\n+\t       tree inner_type = TREE_TYPE (@0);\n+\t       if ((TYPE_PRECISION (inner_type)\n+\t\t    == GET_MODE_PRECISION (TYPE_MODE (inner_type)))\n+\t\t   && TYPE_PRECISION (inner_type) < prec)\n+\t\t {\n+\t\t   prec = TYPE_PRECISION (inner_type);\n+\t\t   /* See if we can shorten the right shift.  */\n+\t\t   if (shiftc < prec)\n+\t\t     shift_type = inner_type;\n+\t\t   /* Otherwise X >> C1 is all zeros, so we'll optimize\n+\t\t      it into (X, 0) later on by making sure zerobits\n+\t\t      is all ones.  */\n+\t\t }\n+\t     }\n+\t   zerobits = ~(unsigned HOST_WIDE_INT) 0;\n+\t   if (shiftc < prec)\n+\t     {\n+\t       zerobits >>= HOST_BITS_PER_WIDE_INT - shiftc;\n+\t       zerobits <<= prec - shiftc;\n+\t     }\n+\t   /* For arithmetic shift if sign bit could be set, zerobits\n+\t      can contain actually sign bits, so no transformation is\n+\t      possible, unless MASK masks them all away.  In that\n+\t      case the shift needs to be converted into logical shift.  */\n+\t   if (!TYPE_UNSIGNED (TREE_TYPE (@3))\n+\t       && prec == TYPE_PRECISION (TREE_TYPE (@3)))\n+\t     {\n+\t       if ((mask & zerobits) == 0)\n+\t\t shift_type = unsigned_type_for (TREE_TYPE (@3));\n+\t       else\n+\t\t zerobits = 0;\n+\t     }\n+\t }\n+     }\n+     /* ((X << 16) & 0xff00) is (X, 0).  */\n+     (if ((mask & zerobits) == mask)\n+      { build_int_cst (type, 0); })\n+     (with { newmask = mask | zerobits; }\n+      (if (newmask != mask && (newmask & (newmask + 1)) == 0)\n+       (with\n+\t{\n+\t  /* Only do the transformation if NEWMASK is some integer\n+\t     mode's mask.  */\n+\t  for (prec = BITS_PER_UNIT;\n+\t       prec < HOST_BITS_PER_WIDE_INT; prec <<= 1)\n+\t    if (newmask == (((unsigned HOST_WIDE_INT) 1) << prec) - 1)\n+\t      break;\n+\t}\n+\t(if (prec < HOST_BITS_PER_WIDE_INT\n+\t     || newmask == ~(unsigned HOST_WIDE_INT) 0)\n+\t (with\n+\t  { tree newmaskt = build_int_cst_type (TREE_TYPE (@2), newmask); }\n+\t  (if (!tree_int_cst_equal (newmaskt, @2))\n+\t   (if (shift_type != TREE_TYPE (@3)\n+\t\t&& single_use (@4) && single_use (@5))\n+\t    (bit_and (convert (shift:shift_type (convert @3) @1)) { newmaskt; }))\n+\t   (if (shift_type == TREE_TYPE (@3))\n+\t    (bit_and @4 { newmaskt; }))))))))))))\n+\n /* Simplifications of conversions.  */\n \n /* Basic strip-useless-type-conversions / strip_nops.  */"}, {"sha": "b6486ac18e4cc1c95e15fbd8c83df6a23606d521", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad", "patch": "@@ -1,3 +1,7 @@\n+2015-06-25  Richard Biener  <rguenther@suse.de>\n+\n+\t* gcc.dg/tree-ssa/pr52631.c: Disable forwprop.\n+\n 2015-06-25  Richard Sandiford  <richard.sandiford@arm.com>\n \n \t* gcc.target/aarch64/vect-add-sub-cond.c: New test."}, {"sha": "c18a5d570b446e62a8f77baaa6a12f47721bdf3f", "filename": "gcc/testsuite/gcc.dg/tree-ssa/pr52631.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fpr52631.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fpr52631.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Ftree-ssa%2Fpr52631.c?ref=1ffbaa3fec7f3dcbc3d1f999e8fa3f0930b3e1ad", "patch": "@@ -1,5 +1,5 @@\n /* { dg-do compile } */\n-/* { dg-options \"-O2 -fdump-tree-fre1-details\" } */\n+/* { dg-options \"-O2 -fno-tree-forwprop -fdump-tree-fre1-details\" } */\n \n unsigned f(unsigned a)\n {"}]}