{"sha": "496d7bb03214b7835638fa14d7275e89d3bec954", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NDk2ZDdiYjAzMjE0Yjc4MzU2MzhmYTE0ZDcyNzVlODlkM2JlYzk1NA==", "commit": {"author": {"name": "Maxim Kuvyrkov", "email": "mkuvyrkov@ispras.ru", "date": "2006-03-16T05:27:03Z"}, "committer": {"name": "Maxim Kuvyrkov", "email": "mkuvyrkov@gcc.gnu.org", "date": "2006-03-16T05:27:03Z"}, "message": "target.h (struct spec_info_def): New opaque declaration.\n\n2006-03-16  Maxim Kuvyrkov <mkuvyrkov@ispras.ru>\n\n        * target.h (struct spec_info_def): New opaque declaration.\n        (struct gcc_target.sched): New fields: adjust_cost_2, h_i_d_extended,\n        speculate_insn, needs_block_p, gen_check,\n        first_cycle_multipass_dfa_lookahead_guard_spec, set_sched_flags.\n        * target-def.h (TARGET_SCHED_ADJUST_COST_2,\n        TARGET_SCHED_H_I_D_EXTENDED, TARGET_SCHED_SPECULATE_INSN,\n        TARGET_SCHED_NEEDS_BLOCK_P, TARGET_SCHED_GEN_CHECK,\n        TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD_SPEC,\n        TARGET_SCHED_SET_SCHED_FLAGS): New macros to initialize fields in\n        gcc_target.sched.\n        (TARGET_SCHED): Use new macros.\n        * rtl.h (copy_DEPS_LIST_list): New prototype.\n        * sched-int.h (struct sched_info): Change signature of new_ready field,\n\tadjust all initializations. New fields: add_remove_insn,\n        begin_schedule_ready, add_block, advance_target_bb, fix_recovery_cfg,\n\tregion_head_or_leaf_p.\n        (struct spec_info_def): New structure declaration.\n        (spec_info_t): New typedef.\n        (struct haifa_insn_data): New fields: todo_spec, done_spec, check_spec,\n        recovery_block, orig_pat.\n        (glat_start, glat_end): New variables declaraions.\n        (TODO_SPEC, DONE_SPEC, CHECK_SPEC, RECOVERY_BLOCK, ORIG_PAT):\n\tNew access macros.\n        (enum SCHED_FLAGS): New constants: SCHED_RGN, SCHED_EBB,\n        DETACH_LIFE_INFO, USE_GLAT.\n        (enum SPEC_SCHED_FLAGS): New enumeration.\n        (NOTE_NOTE_BB_P): New macro.\n        (extend_dependency_caches, xrecalloc, unlink_bb_notes, add_block,\n        attach_life_info, debug_spec_status, check_reg_live): New functions.\n        (get_block_head_tail): Change signature to get_ebb_head_tail, adjust\n        all uses in ddg.c, modulo-sched.c, haifa-sched.c, sched-rgn.c,\n        sched-ebb.c\n\t(get_dep_weak, ds_merge): Prototype functions from sched-deps.c .\n        * ddg.c (get_block_head_tail): Adjust all uses.\n        * modulo-sched.c (get_block_head_tail): Adjust all uses.\n\t(sms_sched_info): Initialize new fields.\n\t(contributes_to_priority): Removed.\n        * haifa-sched.c (params.h): New include.\n\t(get_block_head_tail): Adjust all uses.\n        (ISSUE_POINTS): New macro.\n        (glat_start, glat_end): New global variables.\n        (spec_info_var, spec_info, added_recovery_block_p, nr_begin_data,\n\tnr_be_in_data, nr_begin_control, nr_be_in_control, bb_header,\n\told_last_basic_block, before_recovery, current_sched_info_var,\n\trgn_n_insns, luid): New static variables.\n        (insn_cost1): New function.  Move logic from insn_cost to here.\n        (find_insn_reg_weight1): New function.  Move logic from\n        find_insn_reg_weight to here.\n        (reemit_notes, move_insn, max_issue): Change signature.\n        (move_insn1): Removed.\n        (extend_h_i_d, extend_ready, extend_global, extend_all, init_h_i_d,\n        extend_bb): New static functions to support extension of scheduler's\n        data structures.\n        (generate_recovery_code, process_insn_depend_be_in_spec,\n        begin_speculative_block, add_to_speculative_block,\n        init_before_recovery, create_recovery_block, create_check_block_twin,\n        fix_recovery_deps): New static functions to support\n        generation of recovery code.\n        (fix_jump_move, find_fallthru_edge, dump_new_block_header,\n        restore_bb_notes, move_block_after_check, move_succs): New static\n        functions to support ebb scheduling.\n        (init_glat, init_glat1, attach_life_info1, free_glat): New static\n        functions to support handling of register live information.\n        (associate_line_notes_with_blocks, change_pattern, speculate_insn,\n\tsched_remove_insn, clear_priorities, calc_priorities, bb_note,\n\tadd_jump_dependencies):\tNew static functions.\n        (check_cfg, has_edge_p, check_sched_flags): New static functions for\n\tconsistancy checking.\n\t(debug_spec_status): New function to call from debugger.\n\t(priority): Added code to handle speculation checks.\n\t(rank_for_schedule): Added code to distinguish speculative instructions.\n\t(schedule_insn): Added code to handle speculation checks.\n\t(unlink_other_notes, rm_line_notes, restore_line_notes, rm_other_notes):\n\tFixed to handle ebbs.\n        (move_insn): Added code to handle ebb scheduling.\n\t(max_issue): Added code to use ISSUE_POINTS of instructions.\n        (choose_ready): Added code to choose between speculative and\n        non-speculative instructions.\n        (schedule_block): Added code to handle ebb scheduling and scheduling of\n        speculative instructions.\n        (sched_init): Initialize new variables.\n        (sched_finish): Free new variables.  Print statistics.\n        (try_ready): Added code to handle speculative instructions.\n        * lists.c (copy_DEPS_LIST_list): New function.\n        * sched-deps.c (extend_dependency_caches): New function.  Move logic\n        from create_dependency_caches to here.\n\t(get_dep_weak, ds_merge): Make global.\n        * genattr.c (main): Code to output prototype for\n        dfa_clear_single_insn_cache.\n        * genautomata.c (DFA_CLEAR_SINGLE_INSN_CACHE_FUNC_NAME): New macros.\n        (output_dfa_clean_insn_cache_func): Code to output\n        dfa_clear_single_insn_cache function.\n        * sched-ebb.c (target_n_insns): Remove.  Adjust all users to use\n\tn_insns.\n        (can_schedule_ready_p, fix_basic_block_boundaries, add_missing_bbs):\n        Removed.\n        (n_insns, dont_calc_deps, ebb_head, ebb_tail, last_bb):\n        New static variables.\n        (begin_schedule_ready, add_remove_insn, add_block1, advance_target_bb,\n\tfix_recovery_cfg, ebb_head_or_leaf_p): Implement hooks from\n\tstruct sched_info.\n        (ebb_sched_info): Initialize new fields.\n\t(get_block_head_tail): Adjust all uses.\n\t(compute_jump_reg_dependencies): Fixed to use glat_start.\n\t(schedule_ebb): Code to remove unreachable last block.\n        (schedule_ebbs): Added code to update register live information.\n        * sched-rgn.c (region_sched_info): Initialize new fields.\n\t(get_block_head_tail): Adjust all uses.\n\t(last_was_jump): Removed.  Adjust users.\n        (begin_schedule_ready, add_remove_insn, insn_points, extend_regions,\n\tadd_block1, fix_recovery_cfg, advance_target_bb, region_head_or_leaf_p):\n\tImplement new hooks.\n        (check_dead_notes1): New static function.\n        (struct region): New fields: dont_calc_deps, has_real_ebb.\n        (RGN_DONT_CALC_DEPS, RGN_HAS_REAL_EBB): New access macros.\n        (BB_TO_BLOCK): Fixed to handle EBBs.\n        (EBB_FIRST_BB, EBB_LAST_BB): New macros.\n        (ebb_head): New static variable.\n        (debug_regions, contributes_to_priority): Fixed to handle EBBs.\n        (find_single_block_regions, find_rgns, find_more_rgns): Initialize\n\tnew fields.\n\t(compute_dom_prob_ps): New assertion.\n        (check_live_1, update_live_1): Fixed to work with glat_start instead of\n        global_live_at_start.\n\t(init_ready_list): New assertions.\n\t(can_schedule_ready_p): Split update code to begin_schedule_ready.\n\t(new_ready): Add support for BEGIN_CONTROL speculation.\n        (schedule_insns): Fixed code that updates register live information\n        to handle EBBs.\n        (schedule_region): Fixed to handle EBBs.\n\t(init_regions): Use extend_regions and check_dead_notes1.\n        * params.def (PARAM_MAX_SCHED_INSN_CONFLICT_DELAY,\n        PARAM_SCHED_SPEC_PROB_CUTOFF): New parameters.\n\t* doc/tm.texi (TARGET_SCHED_ADJUST_COST_2, TARGET_SCHED_H_I_D_EXTENDED,\n\tTARGET_SCHED_SPECULATE_INSN, TARGET_SCHED_NEEDS_BLOCK_P,\n\tTARGET_SCHED_GEN_CHECK,\n\tTARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD_SPEC,\n\tTARGET_SCHED_SET_SCHED_FLAGS): Document.\n        * doc/invoke.texi (max-sched-insn-conflict-delay,\n\tsched-spec-prob-cutoff): Document.\n\nFrom-SVN: r112128", "tree": {"sha": "89738976186be03d3235bbf23baa42f8eeb612ca", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/89738976186be03d3235bbf23baa42f8eeb612ca"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/496d7bb03214b7835638fa14d7275e89d3bec954", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/496d7bb03214b7835638fa14d7275e89d3bec954", "html_url": "https://github.com/Rust-GCC/gccrs/commit/496d7bb03214b7835638fa14d7275e89d3bec954", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/496d7bb03214b7835638fa14d7275e89d3bec954/comments", "author": null, "committer": null, "parents": [{"sha": "63f54b1abd832e2c6f7938aac2e2c455b23c91b7", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/63f54b1abd832e2c6f7938aac2e2c455b23c91b7", "html_url": "https://github.com/Rust-GCC/gccrs/commit/63f54b1abd832e2c6f7938aac2e2c455b23c91b7"}], "stats": {"total": 4033, "additions": 3450, "deletions": 583}, "files": [{"sha": "b191bcffb60387c8795161199b909e8144e0944d", "filename": "gcc/ChangeLog", "status": "modified", "additions": 143, "deletions": 0, "changes": 143, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -1,3 +1,146 @@\n+2006-03-16  Maxim Kuvyrkov <mkuvyrkov@ispras.ru>\n+\n+        * target.h (struct spec_info_def): New opaque declaration.\n+        (struct gcc_target.sched): New fields: adjust_cost_2, h_i_d_extended,\n+        speculate_insn, needs_block_p, gen_check,\n+        first_cycle_multipass_dfa_lookahead_guard_spec, set_sched_flags.\n+        * target-def.h (TARGET_SCHED_ADJUST_COST_2,\n+        TARGET_SCHED_H_I_D_EXTENDED, TARGET_SCHED_SPECULATE_INSN,\n+        TARGET_SCHED_NEEDS_BLOCK_P, TARGET_SCHED_GEN_CHECK,\n+        TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD_SPEC,\n+        TARGET_SCHED_SET_SCHED_FLAGS): New macros to initialize fields in\n+        gcc_target.sched.\n+        (TARGET_SCHED): Use new macros.\n+        * rtl.h (copy_DEPS_LIST_list): New prototype.\n+        * sched-int.h (struct sched_info): Change signature of new_ready field,\n+\tadjust all initializations. New fields: add_remove_insn,\n+        begin_schedule_ready, add_block, advance_target_bb, fix_recovery_cfg,\n+\tregion_head_or_leaf_p.\n+        (struct spec_info_def): New structure declaration.\n+        (spec_info_t): New typedef.\n+        (struct haifa_insn_data): New fields: todo_spec, done_spec, check_spec,\n+        recovery_block, orig_pat.\n+        (glat_start, glat_end): New variables declaraions.\n+        (TODO_SPEC, DONE_SPEC, CHECK_SPEC, RECOVERY_BLOCK, ORIG_PAT):\n+\tNew access macros.\n+        (enum SCHED_FLAGS): New constants: SCHED_RGN, SCHED_EBB,\n+        DETACH_LIFE_INFO, USE_GLAT.\n+        (enum SPEC_SCHED_FLAGS): New enumeration.\n+        (NOTE_NOTE_BB_P): New macro.\n+        (extend_dependency_caches, xrecalloc, unlink_bb_notes, add_block,\n+        attach_life_info, debug_spec_status, check_reg_live): New functions.\n+        (get_block_head_tail): Change signature to get_ebb_head_tail, adjust\n+        all uses in ddg.c, modulo-sched.c, haifa-sched.c, sched-rgn.c,\n+        sched-ebb.c\n+\t(get_dep_weak, ds_merge): Prototype functions from sched-deps.c .\n+        * ddg.c (get_block_head_tail): Adjust all uses.\n+        * modulo-sched.c (get_block_head_tail): Adjust all uses.\n+\t(sms_sched_info): Initialize new fields.\n+\t(contributes_to_priority): Removed.\n+        * haifa-sched.c (params.h): New include.\n+\t(get_block_head_tail): Adjust all uses.\n+        (ISSUE_POINTS): New macro.\n+        (glat_start, glat_end): New global variables.\n+        (spec_info_var, spec_info, added_recovery_block_p, nr_begin_data,\n+\tnr_be_in_data, nr_begin_control, nr_be_in_control, bb_header,\n+\told_last_basic_block, before_recovery, current_sched_info_var,\n+\trgn_n_insns, luid): New static variables.\n+        (insn_cost1): New function.  Move logic from insn_cost to here.\n+        (find_insn_reg_weight1): New function.  Move logic from\n+        find_insn_reg_weight to here.\n+        (reemit_notes, move_insn, max_issue): Change signature.\n+        (move_insn1): Removed.\n+        (extend_h_i_d, extend_ready, extend_global, extend_all, init_h_i_d,\n+        extend_bb): New static functions to support extension of scheduler's\n+        data structures.\n+        (generate_recovery_code, process_insn_depend_be_in_spec,\n+        begin_speculative_block, add_to_speculative_block,\n+        init_before_recovery, create_recovery_block, create_check_block_twin,\n+        fix_recovery_deps): New static functions to support\n+        generation of recovery code.\n+        (fix_jump_move, find_fallthru_edge, dump_new_block_header,\n+        restore_bb_notes, move_block_after_check, move_succs): New static\n+        functions to support ebb scheduling.\n+        (init_glat, init_glat1, attach_life_info1, free_glat): New static\n+        functions to support handling of register live information.\n+        (associate_line_notes_with_blocks, change_pattern, speculate_insn,\n+\tsched_remove_insn, clear_priorities, calc_priorities, bb_note,\n+\tadd_jump_dependencies):\tNew static functions.\n+        (check_cfg, has_edge_p, check_sched_flags): New static functions for\n+\tconsistancy checking.\n+\t(debug_spec_status): New function to call from debugger.\n+\t(priority): Added code to handle speculation checks.\n+\t(rank_for_schedule): Added code to distinguish speculative instructions.\n+\t(schedule_insn): Added code to handle speculation checks.\n+\t(unlink_other_notes, rm_line_notes, restore_line_notes, rm_other_notes):\n+\tFixed to handle ebbs.\n+        (move_insn): Added code to handle ebb scheduling.\n+\t(max_issue): Added code to use ISSUE_POINTS of instructions.\n+        (choose_ready): Added code to choose between speculative and\n+        non-speculative instructions.\n+        (schedule_block): Added code to handle ebb scheduling and scheduling of\n+        speculative instructions.\n+        (sched_init): Initialize new variables.\n+        (sched_finish): Free new variables.  Print statistics.\n+        (try_ready): Added code to handle speculative instructions.\n+        * lists.c (copy_DEPS_LIST_list): New function.\n+        * sched-deps.c (extend_dependency_caches): New function.  Move logic\n+        from create_dependency_caches to here.\n+\t(get_dep_weak, ds_merge): Make global.\n+        * genattr.c (main): Code to output prototype for\n+        dfa_clear_single_insn_cache.\n+        * genautomata.c (DFA_CLEAR_SINGLE_INSN_CACHE_FUNC_NAME): New macros.\n+        (output_dfa_clean_insn_cache_func): Code to output\n+        dfa_clear_single_insn_cache function.\n+        * sched-ebb.c (target_n_insns): Remove.  Adjust all users to use\n+\tn_insns.\n+        (can_schedule_ready_p, fix_basic_block_boundaries, add_missing_bbs):\n+        Removed.\n+        (n_insns, dont_calc_deps, ebb_head, ebb_tail, last_bb):\n+        New static variables.\n+        (begin_schedule_ready, add_remove_insn, add_block1, advance_target_bb,\n+\tfix_recovery_cfg, ebb_head_or_leaf_p): Implement hooks from\n+\tstruct sched_info.\n+        (ebb_sched_info): Initialize new fields.\n+\t(get_block_head_tail): Adjust all uses.\n+\t(compute_jump_reg_dependencies): Fixed to use glat_start.\n+\t(schedule_ebb): Code to remove unreachable last block.\n+        (schedule_ebbs): Added code to update register live information.\n+        * sched-rgn.c (region_sched_info): Initialize new fields.\n+\t(get_block_head_tail): Adjust all uses.\n+\t(last_was_jump): Removed.  Adjust users.\n+        (begin_schedule_ready, add_remove_insn, insn_points, extend_regions,\n+\tadd_block1, fix_recovery_cfg, advance_target_bb, region_head_or_leaf_p):\n+\tImplement new hooks.\n+        (check_dead_notes1): New static function.\n+        (struct region): New fields: dont_calc_deps, has_real_ebb.\n+        (RGN_DONT_CALC_DEPS, RGN_HAS_REAL_EBB): New access macros.\n+        (BB_TO_BLOCK): Fixed to handle EBBs.\n+        (EBB_FIRST_BB, EBB_LAST_BB): New macros.\n+        (ebb_head): New static variable.\n+        (debug_regions, contributes_to_priority): Fixed to handle EBBs.\n+        (find_single_block_regions, find_rgns, find_more_rgns): Initialize\n+\tnew fields.\n+\t(compute_dom_prob_ps): New assertion.\n+        (check_live_1, update_live_1): Fixed to work with glat_start instead of\n+        global_live_at_start.\n+\t(init_ready_list): New assertions.\n+\t(can_schedule_ready_p): Split update code to begin_schedule_ready.\n+\t(new_ready): Add support for BEGIN_CONTROL speculation.\n+        (schedule_insns): Fixed code that updates register live information\n+        to handle EBBs.\n+        (schedule_region): Fixed to handle EBBs.\n+\t(init_regions): Use extend_regions and check_dead_notes1.\n+        * params.def (PARAM_MAX_SCHED_INSN_CONFLICT_DELAY,\n+        PARAM_SCHED_SPEC_PROB_CUTOFF): New parameters.\n+\t* doc/tm.texi (TARGET_SCHED_ADJUST_COST_2, TARGET_SCHED_H_I_D_EXTENDED,\n+\tTARGET_SCHED_SPECULATE_INSN, TARGET_SCHED_NEEDS_BLOCK_P,\n+\tTARGET_SCHED_GEN_CHECK,\n+\tTARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD_SPEC,\n+\tTARGET_SCHED_SET_SCHED_FLAGS): Document.\n+        * doc/invoke.texi (max-sched-insn-conflict-delay,\n+\tsched-spec-prob-cutoff): Document.\n+\n 2006-03-16  Maxim Kuvyrkov <mkuvyrkov@ispras.ru>\n \n         * sched-int.h (struct haifa_insn_data): New fields: resolved_deps,"}, {"sha": "fe7ffeef7e71b0078c276b451fb0bdad1cc8be6e", "filename": "gcc/Makefile.in", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -2515,7 +2515,8 @@ modulo-sched.o : modulo-sched.c $(DDG_H) $(CONFIG_H) $(CONFIG_H) $(SYSTEM_H) \\\n    cfghooks.h $(DF_H) $(GCOV_IO_H) hard-reg-set.h $(TM_H) timevar.h tree-pass.h\n haifa-sched.o : haifa-sched.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \\\n    $(SCHED_INT_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) insn-config.h $(FUNCTION_H) \\\n-   $(INSN_ATTR_H) toplev.h $(RECOG_H) except.h $(TM_P_H) $(TARGET_H) output.h\n+   $(INSN_ATTR_H) toplev.h $(RECOG_H) except.h $(TM_P_H) $(TARGET_H) output.h \\\n+   $(PARAMS_H)\n sched-deps.o : sched-deps.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n    $(RTL_H) $(SCHED_INT_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) insn-config.h \\\n    $(FUNCTION_H) $(INSN_ATTR_H) toplev.h $(RECOG_H) except.h cselib.h \\"}, {"sha": "c00e4991157e439c173818e8e278a00f5e689a3b", "filename": "gcc/ddg.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fddg.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fddg.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fddg.c?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -382,7 +382,7 @@ build_intra_loop_deps (ddg_ptr g)\n   init_deps (&tmp_deps);\n \n   /* Do the intra-block data dependence analysis for the given block.  */\n-  get_block_head_tail (g->bb->index, &head, &tail);\n+  get_ebb_head_tail (g->bb, g->bb, &head, &tail);\n   sched_analyze (&tmp_deps, head, tail);\n \n   /* Build intra-loop data dependencies using the scheduler dependency"}, {"sha": "f037f4b09918528939c50da705f39a2169f4ac91", "filename": "gcc/doc/invoke.texi", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fdoc%2Finvoke.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fdoc%2Finvoke.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Finvoke.texi?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -6183,6 +6183,15 @@ The maximum number of iterations through CFG to extend regions.\n N - do at most N iterations.\n The default value is 2.\n \n+@item max-sched-insn-conflict-delay\n+The maximum conflict delay for an insn to be considered for speculative motion.\n+The default value is 3.\n+\n+@item sched-spec-prob-cutoff\n+The minimal probability of speculation success (in percents), so that\n+speculative insn will be scheduled.\n+The default value is 40.\n+\n @item max-last-value-rtl\n \n The maximum size measured as number of RTLs that can be recorded in an expression"}, {"sha": "d18b0ba3e396415c792085c6759e9aa55885c071", "filename": "gcc/doc/tm.texi", "status": "modified", "additions": 66, "deletions": 2, "changes": 68, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fdoc%2Ftm.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fdoc%2Ftm.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Ftm.texi?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -5838,8 +5838,8 @@ acceptable, you could use the hook to modify them too.  See also\n \n @deftypefn {Target Hook} int TARGET_SCHED_ADJUST_PRIORITY (rtx @var{insn}, int @var{priority})\n This hook adjusts the integer scheduling priority @var{priority} of\n-@var{insn}.  It should return the new priority.  Reduce the priority to\n-execute @var{insn} earlier, increase the priority to execute @var{insn}\n+@var{insn}.  It should return the new priority.  Increase the priority to\n+execute @var{insn} earlier, reduce the priority to execute @var{insn}\n later.  Do not define this hook if you do not need to adjust the\n scheduling priorities of insns.\n @end deftypefn\n@@ -6014,6 +6014,70 @@ closer to one another---i.e., closer than the dependence distance;  however,\n not in cases of \"costly dependences\", which this hooks allows to define.\n @end deftypefn\n \n+@deftypefn {Target Hook} int TARGET_SCHED_ADJUST_COST_2 (rtx @var{insn}, int @var{dep_type}, rtx @var{dep_insn}, int @var{cost})\n+This hook is a modified version of @samp{TARGET_SCHED_ADJUST_COST}.  Instead\n+of passing dependence as a second parameter, it passes a type of that\n+dependence.  This is useful to calculate cost of dependence between insns\n+not having the corresponding link.  If @samp{TARGET_SCHED_ADJUST_COST_2} is\n+definded it is used instead of @samp{TARGET_SCHED_ADJUST_COST}.\n+@end deftypefn\n+\n+@deftypefn {Target Hook} void TARGET_SCHED_H_I_D_EXTENDED (void)\n+This hook is called by the insn scheduler after emitting a new instruction to\n+the instruction stream.  The hook notifies a target backend to extend its\n+per instruction data structures.\n+@end deftypefn\n+\n+@deftypefn {Target Hook} int TARGET_SCHED_SPECULATE_INSN (rtx @var{insn}, int @var{request}, rtx *@var{new_pat})\n+This hook is called by the insn scheduler when @var{insn} has only \n+speculative dependencies and therefore can be scheduled speculatively.  \n+The hook is used to check if the pattern of @var{insn} has a speculative \n+version and, in case of successful check, to generate that speculative \n+pattern.  The hook should return 1, if the instruction has a speculative form, \n+or -1, if it doesn't.  @var{request} describes the type of requested \n+speculation.  If the return value equals 1 then @var{new_pat} is assigned\n+the generated speculative pattern.\n+@end deftypefn\n+\n+@deftypefn {Target Hook} int TARGET_SCHED_NEEDS_BLOCK_P (rtx @var{insn})\n+This hook is called by the insn scheduler during generation of recovery code\n+for @var{insn}.  It should return non-zero, if the corresponding check\n+instruction should branch to recovery code, or zero otherwise.\n+@end deftypefn\n+\n+@deftypefn {Target Hook} rtx TARGET_SCHED_GEN_CHECK (rtx @var{insn}, rtx @var{label}, int @var{mutate_p})\n+This hook is called by the insn scheduler to generate a pattern for recovery\n+check instruction.  If @var{mutate_p} is zero, then @var{insn} is a \n+speculative instruction for which the check should be generated.  \n+@var{label} is either a label of a basic block, where recovery code should \n+be emitted, or a null pointer, when requested check doesn't branch to \n+recovery code (a simple check).  If @var{mutate_p} is non-zero, then \n+a pattern for a branchy check corresponding to a simple check denoted by \n+@var{insn} should be generated.  In this case @var{label} can't be null.\n+@end deftypefn\n+\n+@deftypefn {Target Hook} int TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD_SPEC (rtx @var{insn})\n+This hook is used as a workaround for\n+@samp{TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD} not being\n+called on the first instruction of the ready list.  The hook is used to\n+discard speculative instruction that stand first in the ready list from\n+being scheduled on the current cycle.  For non-speculative instructions, \n+the hook should always return non-zero.  For example, in the ia64 backend\n+the hook is used to cancel data speculative insns when the ALAT table\n+is nearly full.\n+@end deftypefn\n+\n+@deftypefn {Target Hook} void TARGET_SCHED_SET_SCHED_FLAGS (unsigned int *@var{flags}, spec_info_t @var{spec_info})\n+This hook is used by the insn scheduler to find out what features should be \n+enabled/used.  @var{flags} initially may have either the SCHED_RGN or SCHED_EBB\n+bit set.  This denotes the scheduler pass for which the data should be \n+provided.  The target backend should modify @var{flags} by modifying\n+the bits correponding to the following features: USE_DEPS_LIST, USE_GLAT,\n+DETACH_LIFE_INFO, and DO_SPECULATION.  For the DO_SPECULATION feature \n+an additional structure @var{spec_info} should be filled by the target.  \n+The structure describes speculation types that can be used in the scheduler.\n+@end deftypefn\n+\n @node Sections\n @section Dividing the Output into Sections (Texts, Data, @dots{})\n @c the above section title is WAY too long.  maybe cut the part between"}, {"sha": "83fb5e88f2c6b2f962e01f4283e4017124722205", "filename": "gcc/genattr.c", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fgenattr.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fgenattr.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgenattr.c?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -250,6 +250,7 @@ main (int argc, char **argv)\n       printf (\"   define_insn_reservation will be changed after\\n\");\n       printf (\"   last call of dfa_start.  */\\n\");\n       printf (\"extern void dfa_clean_insn_cache (void);\\n\\n\");\n+      printf (\"extern void dfa_clear_single_insn_cache (rtx);\\n\\n\");      \n       printf (\"/* Initiate and finish work with DFA.  They should be\\n\");\n       printf (\"   called as the first and the last interface\\n\");\n       printf (\"   functions.  */\\n\");"}, {"sha": "eb06e6ccff8b5eed1a86d684b344cb075459478a", "filename": "gcc/genautomata.c", "status": "modified", "additions": 14, "deletions": 1, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fgenautomata.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fgenautomata.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgenautomata.c?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -6852,6 +6852,8 @@ output_reserved_units_table_name (FILE *f, automaton_t automaton)\n \n #define DFA_CLEAN_INSN_CACHE_FUNC_NAME  \"dfa_clean_insn_cache\"\n \n+#define DFA_CLEAR_SINGLE_INSN_CACHE_FUNC_NAME \"dfa_clear_single_insn_cache\"\n+\n #define DFA_START_FUNC_NAME  \"dfa_start\"\n \n #define DFA_FINISH_FUNC_NAME \"dfa_finish\"\n@@ -8335,7 +8337,8 @@ output_cpu_unit_reservation_p (void)\n   fprintf (output_file, \"  return 0;\\n}\\n\\n\");\n }\n \n-/* The function outputs PHR interface function `dfa_clean_insn_cache'.  */\n+/* The function outputs PHR interface functions `dfa_clean_insn_cache'\n+   and 'dfa_clear_single_insn_cache'.  */\n static void\n output_dfa_clean_insn_cache_func (void)\n {\n@@ -8347,6 +8350,16 @@ output_dfa_clean_insn_cache_func (void)\n \t   I_VARIABLE_NAME, I_VARIABLE_NAME,\n \t   DFA_INSN_CODES_LENGTH_VARIABLE_NAME, I_VARIABLE_NAME,\n \t   DFA_INSN_CODES_VARIABLE_NAME, I_VARIABLE_NAME);\n+\n+  fprintf (output_file,\n+           \"void\\n%s (rtx %s)\\n{\\n  int %s;\\n\\n\",\n+           DFA_CLEAR_SINGLE_INSN_CACHE_FUNC_NAME, INSN_PARAMETER_NAME,\n+\t   I_VARIABLE_NAME);\n+  fprintf (output_file,\n+           \"  %s = INSN_UID (%s);\\n  if (%s < %s)\\n    %s [%s] = -1;\\n}\\n\\n\",\n+           I_VARIABLE_NAME, INSN_PARAMETER_NAME, I_VARIABLE_NAME,\n+\t   DFA_INSN_CODES_LENGTH_VARIABLE_NAME, DFA_INSN_CODES_VARIABLE_NAME,\n+\t   I_VARIABLE_NAME);\n }\n \n /* The function outputs PHR interface function `dfa_start'.  */"}, {"sha": "84311b1ba340b09ee3c934edec58045556b3ced1", "filename": "gcc/haifa-sched.c", "status": "modified", "additions": 2224, "deletions": 241, "changes": 2465, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fhaifa-sched.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fhaifa-sched.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fhaifa-sched.c?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -143,6 +143,7 @@ Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA\n #include \"sched-int.h\"\n #include \"target.h\"\n #include \"output.h\"\n+#include \"params.h\"\n \n #ifdef INSN_SCHEDULING\n \n@@ -195,6 +196,10 @@ struct haifa_insn_data *h_i_d;\n /* The minimal value of the INSN_TICK of an instruction.  */\n #define MIN_TICK (-max_insn_queue_index)\n \n+/* Issue points are used to distinguish between instructions in max_issue ().\n+   For now, all instructions are equally good.  */\n+#define ISSUE_POINTS(INSN) 1\n+\n /* Vector indexed by basic block number giving the starting line-number\n    for each basic block.  */\n static rtx *line_note_head;\n@@ -203,6 +208,30 @@ static rtx *line_note_head;\n    last element in the list.  */\n static rtx note_list;\n \n+static struct spec_info_def spec_info_var;\n+/* Description of the speculative part of the scheduling.\n+   If NULL - no speculation.  */\n+static spec_info_t spec_info;\n+\n+/* True, if recovery block was added during scheduling of current block.\n+   Used to determine, if we need to fix INSN_TICKs.  */\n+static bool added_recovery_block_p;\n+\n+/* Counters of different types of speculative isntructions.  */\n+static int nr_begin_data, nr_be_in_data, nr_begin_control, nr_be_in_control;\n+\n+/* Pointers to GLAT data.  See init_glat for more information.  */\n+regset *glat_start, *glat_end;\n+\n+/* Array used in {unlink, restore}_bb_notes.  */\n+static rtx *bb_header = 0;\n+\n+/* Number of basic_blocks.  */\n+static int old_last_basic_block;\n+\n+/* Basic block after which recovery blocks will be created.  */\n+static basic_block before_recovery;\n+\n /* Queues, etc.  */\n \n /* An instruction is ready to be scheduled when all insns preceding it\n@@ -299,6 +328,9 @@ static struct ready_list *readyp;\n /* Scheduling clock.  */\n static int clock_var;\n \n+/* Number of instructions in current scheduling region.  */\n+static int rgn_n_insns;\n+\n static int may_trap_exp (rtx, int);\n \n /* Nonzero iff the address is comprised from at most 1 register.  */\n@@ -462,13 +494,15 @@ haifa_classify_insn (rtx insn)\n \n /* Forward declarations.  */\n \n+HAIFA_INLINE static int insn_cost1 (rtx, enum reg_note, rtx, rtx);\n static int priority (rtx);\n static int rank_for_schedule (const void *, const void *);\n static void swap_sort (rtx *, int);\n static void queue_insn (rtx, int);\n static int schedule_insn (rtx);\n static int find_set_reg_weight (rtx);\n-static void find_insn_reg_weight (int);\n+static void find_insn_reg_weight (basic_block);\n+static void find_insn_reg_weight1 (rtx);\n static void adjust_priority (rtx);\n static void advance_one_cycle (void);\n \n@@ -497,7 +531,7 @@ static void advance_one_cycle (void);\n \n static rtx unlink_other_notes (rtx, rtx);\n static rtx unlink_line_notes (rtx, rtx);\n-static rtx reemit_notes (rtx, rtx);\n+static void reemit_notes (rtx);\n \n static rtx *ready_lastpos (struct ready_list *);\n static void ready_add (struct ready_list *, rtx, bool);\n@@ -509,15 +543,14 @@ static int early_queue_to_ready (state_t, struct ready_list *);\n \n static void debug_ready_list (struct ready_list *);\n \n-static rtx move_insn1 (rtx, rtx);\n-static rtx move_insn (rtx, rtx);\n+static void move_insn (rtx);\n \n /* The following functions are used to implement multi-pass scheduling\n    on the first cycle.  */\n static rtx ready_element (struct ready_list *, int);\n static rtx ready_remove (struct ready_list *, int);\n static void ready_remove_insn (rtx);\n-static int max_issue (struct ready_list *, int *);\n+static int max_issue (struct ready_list *, int *, int);\n \n static rtx choose_ready (struct ready_list *);\n \n@@ -526,6 +559,48 @@ static int fix_tick_ready (rtx);\n static void change_queue_index (rtx, int);\n static void resolve_dep (rtx, rtx);\n \n+/* The following functions are used to implement scheduling of data/control\n+   speculative instructions.  */\n+\n+static void extend_h_i_d (void);\n+static void extend_ready (int);\n+static void extend_global (rtx);\n+static void extend_all (rtx);\n+static void init_h_i_d (rtx);\n+static void generate_recovery_code (rtx);\n+static void process_insn_depend_be_in_spec (rtx, rtx, ds_t);\n+static void begin_speculative_block (rtx);\n+static void add_to_speculative_block (rtx);\n+static dw_t dep_weak (ds_t);\n+static edge find_fallthru_edge (basic_block);\n+static void init_before_recovery (void);\n+static basic_block create_recovery_block (void);\n+static void create_check_block_twin (rtx, bool);\n+static void fix_recovery_deps (basic_block);\n+static void associate_line_notes_with_blocks (basic_block);\n+static void change_pattern (rtx, rtx);\n+static int speculate_insn (rtx, ds_t, rtx *);\n+static void dump_new_block_header (int, basic_block, rtx, rtx);\n+static void restore_bb_notes (basic_block);\n+static void extend_bb (basic_block);\n+static void fix_jump_move (rtx);\n+static void move_block_after_check (rtx);\n+static void move_succs (VEC(edge,gc) **, basic_block);\n+static void init_glat (void);\n+static void init_glat1 (basic_block);\n+static void attach_life_info1 (basic_block);\n+static void free_glat (void);\n+static void sched_remove_insn (rtx);\n+static void clear_priorities (rtx);\n+static void add_jump_dependencies (rtx, rtx);\n+static rtx bb_note (basic_block);\n+static void calc_priorities (rtx);\n+#ifdef ENABLE_CHECKING\n+static int has_edge_p (VEC(edge,gc) *, int);\n+static void check_cfg (rtx, rtx);\n+static void check_sched_flags (void);\n+#endif\n+\n #endif /* INSN_SCHEDULING */\n \f\n /* Point to state used for the current scheduling pass.  */\n@@ -538,6 +613,9 @@ schedule_insns (void)\n }\n #else\n \n+/* Working copy of frontend's sched_info variable.  */\n+static struct sched_info current_sched_info_var;\n+\n /* Pointer to the last instruction scheduled.  Used by rank_for_schedule,\n    so that insns independent of the last scheduled insn will be preferred\n    over dependent instructions.  */\n@@ -550,6 +628,21 @@ static rtx last_scheduled_insn;\n \n HAIFA_INLINE int\n insn_cost (rtx insn, rtx link, rtx used)\n+{\n+  return insn_cost1 (insn, used ? REG_NOTE_KIND (link) : REG_NOTE_MAX,\n+\t\t     link, used);\n+}\n+\n+/* Compute cost of executing INSN given the dependence on the insn USED.\n+   If LINK is not NULL, then its REG_NOTE_KIND is used as a dependence type.\n+   Otherwise, dependence between INSN and USED is assumed to be of type\n+   DEP_TYPE.  This function was introduced as a workaround for\n+   targetm.adjust_cost hook.\n+   This is the number of cycles between instruction issue and\n+   instruction results.  */\n+\n+HAIFA_INLINE static int\n+insn_cost1 (rtx insn, enum reg_note dep_type, rtx link, rtx used)\n {\n   int cost = INSN_COST (insn);\n \n@@ -575,7 +668,7 @@ insn_cost (rtx insn, rtx link, rtx used)\n     }\n \n   /* In this case estimate cost without caring how insn is used.  */\n-  if (link == 0 || used == 0)\n+  if (used == 0)\n     return cost;\n \n   /* A USE insn should never require the value used to be computed.\n@@ -585,11 +678,13 @@ insn_cost (rtx insn, rtx link, rtx used)\n     cost = 0;\n   else\n     {\n+      gcc_assert (!link || dep_type == REG_NOTE_KIND (link));\n+\n       if (INSN_CODE (insn) >= 0)\n \t{\n-\t  if (REG_NOTE_KIND (link) == REG_DEP_ANTI)\n+\t  if (dep_type == REG_DEP_ANTI)\n \t    cost = 0;\n-\t  else if (REG_NOTE_KIND (link) == REG_DEP_OUTPUT)\n+\t  else if (dep_type == REG_DEP_OUTPUT)\n \t    {\n \t      cost = (insn_default_latency (insn)\n \t\t      - insn_default_latency (used));\n@@ -600,8 +695,14 @@ insn_cost (rtx insn, rtx link, rtx used)\n \t    cost = insn_latency (insn, used);\n \t}\n \n-      if (targetm.sched.adjust_cost)\n-\tcost = targetm.sched.adjust_cost (used, link, insn, cost);\n+      if (targetm.sched.adjust_cost_2)\n+\tcost = targetm.sched.adjust_cost_2 (used, (int) dep_type, insn, cost);\n+      else\n+\t{\n+\t  gcc_assert (link);\n+\t  if (targetm.sched.adjust_cost)\n+\t    cost = targetm.sched.adjust_cost (used, link, insn, cost);\n+\t}\n \n       if (cost < 0)\n \tcost = 0;\n@@ -628,21 +729,68 @@ priority (rtx insn)\n \tthis_priority = insn_cost (insn, 0, 0);\n       else\n \t{\n-\t  for (link = INSN_DEPEND (insn); link; link = XEXP (link, 1))\n-\t    {\n-\t      rtx next;\n-\t      int next_priority;\n+\t  rtx prev_first, twin;\n+\t  basic_block rec;\n \n-\t      next = XEXP (link, 0);\n+\t  /* For recovery check instructions we calculate priority slightly\n+\t     different than that of normal instructions.  Instead of walking\n+\t     through INSN_DEPEND (check) list, we walk through INSN_DEPEND list\n+\t     of each instruction in the corresponding recovery block.  */ \n \n-\t      /* Critical path is meaningful in block boundaries only.  */\n-\t      if (! (*current_sched_info->contributes_to_priority) (next, insn))\n-\t\tcontinue;\n+\t  rec = RECOVERY_BLOCK (insn);\n+\t  if (!rec || rec == EXIT_BLOCK_PTR)\n+\t    {\n+\t      prev_first = PREV_INSN (insn);\n+\t      twin = insn;\n+\t    }\n+\t  else\n+\t    {\n+\t      prev_first = NEXT_INSN (BB_HEAD (rec));\n+\t      twin = PREV_INSN (BB_END (rec));\n+\t    }\n \n-\t      next_priority = insn_cost (insn, link, next) + priority (next);\n-\t      if (next_priority > this_priority)\n-\t\tthis_priority = next_priority;\n+\t  do\n+\t    {\n+\t      for (link = INSN_DEPEND (twin); link; link = XEXP (link, 1))\n+\t\t{\n+\t\t  rtx next;\n+\t\t  int next_priority;\n+\t\t  \n+\t\t  next = XEXP (link, 0);\n+\t\t  \n+\t\t  if (BLOCK_FOR_INSN (next) != rec)\n+\t\t    {\n+\t\t      /* Critical path is meaningful in block boundaries\n+\t\t\t only.  */\n+\t\t      if (! (*current_sched_info->contributes_to_priority)\n+\t\t\t  (next, insn)\n+\t\t\t  /* If flag COUNT_SPEC_IN_CRITICAL_PATH is set,\n+\t\t\t     then speculative instructions will less likely be\n+\t\t\t     scheduled.  That is because the priority of\n+\t\t\t     their producers will increase, and, thus, the\n+\t\t\t     producers will more likely be scheduled, thus,\n+\t\t\t     resolving the dependence.  */\n+\t\t\t  || ((current_sched_info->flags & DO_SPECULATION)\n+\t\t\t      && (DEP_STATUS (link) & SPECULATIVE)\n+\t\t\t      && !(spec_info->flags\n+\t\t\t\t   & COUNT_SPEC_IN_CRITICAL_PATH)))\n+\t\t\tcontinue;\n+\t\t      \n+\t\t      next_priority = insn_cost1 (insn,\n+\t\t\t\t\t\t  twin == insn ?\n+\t\t\t\t\t\t  REG_NOTE_KIND (link) :\n+\t\t\t\t\t\t  REG_DEP_ANTI,\n+\t\t\t\t\t\t  twin == insn ? link : 0,\n+\t\t\t\t\t\t  next) + priority (next);\n+\n+\t\t      if (next_priority > this_priority)\n+\t\t\tthis_priority = next_priority;\n+\t\t    }\n+\t\t}\n+\t      \n+\t      twin = PREV_INSN (twin);\n \t    }\n+\t  while (twin != prev_first);\n \t}\n       INSN_PRIORITY (insn) = this_priority;\n       INSN_PRIORITY_KNOWN (insn) = 1;\n@@ -684,6 +832,30 @@ rank_for_schedule (const void *x, const void *y)\n   if (priority_val)\n     return priority_val;\n \n+  /* Prefer speculative insn with greater dependencies weakness.  */\n+  if (spec_info)\n+    {\n+      ds_t ds1, ds2;\n+      dw_t dw1, dw2;\n+      int dw;\n+\n+      ds1 = TODO_SPEC (tmp) & SPECULATIVE;\n+      if (ds1)\n+\tdw1 = dep_weak (ds1);\n+      else\n+\tdw1 = NO_DEP_WEAK;\n+      \n+      ds2 = TODO_SPEC (tmp2) & SPECULATIVE;\n+      if (ds2)\n+\tdw2 = dep_weak (ds2);\n+      else\n+\tdw2 = NO_DEP_WEAK;\n+\n+      dw = dw2 - dw1;\n+      if (dw > (NO_DEP_WEAK / 8) || dw < -(NO_DEP_WEAK / 8))\n+\treturn dw;\n+    }\n+\n   /* Prefer an insn with smaller contribution to registers-pressure.  */\n   if (!reload_completed &&\n       (weight_val = INSN_REG_WEIGHT (tmp) - INSN_REG_WEIGHT (tmp2)))\n@@ -1015,17 +1187,29 @@ schedule_insn (rtx insn)\n   /* Update dependent instructions.  */\n   for (link = INSN_DEPEND (insn); link; link = XEXP (link, 1))\n     {\n-      int effective_cost;      \n       rtx next = XEXP (link, 0);\n \n       resolve_dep (next, insn);\n \n-      effective_cost = try_ready (next);\n-      \n-      if (effective_cost >= 0\n-\t  && SCHED_GROUP_P (next)\n-\t  && advance < effective_cost)\n-\tadvance = effective_cost;\n+      if (!RECOVERY_BLOCK (insn)\n+\t  || RECOVERY_BLOCK (insn) == EXIT_BLOCK_PTR)\n+\t{\n+\t  int effective_cost;      \n+\t  \n+\t  effective_cost = try_ready (next);\n+\t  \n+\t  if (effective_cost >= 0\n+\t      && SCHED_GROUP_P (next)\n+\t      && advance < effective_cost)\n+\t    advance = effective_cost;\n+\t}\n+      else\n+\t/* Check always has only one forward dependence (to the first insn in\n+\t   the recovery block), therefore, this will be executed only once.  */\n+\t{\n+\t  gcc_assert (XEXP (link, 1) == 0);\n+\t  fix_recovery_deps (RECOVERY_BLOCK (insn));\n+\t}\n     }\n \n   /* Annotate the instruction with issue information -- TImode\n@@ -1056,7 +1240,7 @@ unlink_other_notes (rtx insn, rtx tail)\n {\n   rtx prev = PREV_INSN (insn);\n \n-  while (insn != tail && NOTE_P (insn))\n+  while (insn != tail && NOTE_NOT_BB_P (insn))\n     {\n       rtx next = NEXT_INSN (insn);\n       /* Delete the note from its current position.  */\n@@ -1066,8 +1250,7 @@ unlink_other_notes (rtx insn, rtx tail)\n \tPREV_INSN (next) = prev;\n \n       /* See sched_analyze to see how these are handled.  */\n-      if (NOTE_LINE_NUMBER (insn) != NOTE_INSN_BASIC_BLOCK\n-\t  && NOTE_LINE_NUMBER (insn) != NOTE_INSN_EH_REGION_BEG\n+      if (NOTE_LINE_NUMBER (insn) != NOTE_INSN_EH_REGION_BEG\n \t  && NOTE_LINE_NUMBER (insn) != NOTE_INSN_EH_REGION_END)\n \t{\n \t  /* Insert the note at the end of the notes list.  */\n@@ -1113,31 +1296,43 @@ unlink_line_notes (rtx insn, rtx tail)\n   return insn;\n }\n \n-/* Return the head and tail pointers of BB.  */\n+/* Return the head and tail pointers of ebb starting at BEG and ending\n+   at END.  */\n \n void\n-get_block_head_tail (int b, rtx *headp, rtx *tailp)\n-{\n-  /* HEAD and TAIL delimit the basic block being scheduled.  */\n-  rtx head = BB_HEAD (BASIC_BLOCK (b));\n-  rtx tail = BB_END (BASIC_BLOCK (b));\n-\n-  /* Don't include any notes or labels at the beginning of the\n-     basic block, or notes at the ends of basic blocks.  */\n-  while (head != tail)\n-    {\n-      if (NOTE_P (head))\n-\thead = NEXT_INSN (head);\n-      else if (NOTE_P (tail))\n-\ttail = PREV_INSN (tail);\n-      else if (LABEL_P (head))\n-\thead = NEXT_INSN (head);\n-      else\n-\tbreak;\n-    }\n+get_ebb_head_tail (basic_block beg, basic_block end, rtx *headp, rtx *tailp)\n+{\n+  rtx beg_head = BB_HEAD (beg);\n+  rtx beg_tail = BB_END (beg);\n+  rtx end_head = BB_HEAD (end);\n+  rtx end_tail = BB_END (end);\n+\n+  /* Don't include any notes or labels at the beginning of the BEG\n+     basic block, or notes at the end of the END basic blocks.  */\n+\n+  if (LABEL_P (beg_head))\n+    beg_head = NEXT_INSN (beg_head);\n+\n+  while (beg_head != beg_tail)\n+    if (NOTE_P (beg_head))\n+      beg_head = NEXT_INSN (beg_head);\n+    else\n+      break;\n+\n+  *headp = beg_head;\n+\n+  if (beg == end)\n+    end_head = beg_head;\n+  else if (LABEL_P (end_head))\n+    end_head = NEXT_INSN (end_head);\n+\n+  while (end_head != end_tail)\n+    if (NOTE_P (end_tail))\n+      end_tail = PREV_INSN (end_tail);\n+    else\n+      break;\n \n-  *headp = head;\n-  *tailp = tail;\n+  *tailp = end_tail;\n }\n \n /* Return nonzero if there are no real insns in the range [ HEAD, TAIL ].  */\n@@ -1172,7 +1367,7 @@ rm_line_notes (rtx head, rtx tail)\n       /* Farm out notes, and maybe save them in NOTE_LIST.\n          This is needed to keep the debugger from\n          getting completely deranged.  */\n-      if (NOTE_P (insn))\n+      if (NOTE_NOT_BB_P (insn))\n \t{\n \t  prev = insn;\n \t  insn = unlink_line_notes (insn, next_tail);\n@@ -1263,6 +1458,7 @@ restore_line_notes (rtx head, rtx tail)\n \t    NEXT_INSN (prev) = note;\n \t    PREV_INSN (insn) = note;\n \t    NEXT_INSN (note) = insn;\n+\t    set_block_for_insn (note, BLOCK_FOR_INSN (insn));\n \t  }\n \telse\n \t  {\n@@ -1350,7 +1546,7 @@ rm_other_notes (rtx head, rtx tail)\n       /* Farm out notes, and maybe save them in NOTE_LIST.\n          This is needed to keep the debugger from\n          getting completely deranged.  */\n-      if (NOTE_P (insn))\n+      if (NOTE_NOT_BB_P (insn))\n \t{\n \t  prev = insn;\n \n@@ -1391,44 +1587,51 @@ find_set_reg_weight (rtx x)\n /* Calculate INSN_REG_WEIGHT for all insns of a block.  */\n \n static void\n-find_insn_reg_weight (int b)\n+find_insn_reg_weight (basic_block bb)\n {\n   rtx insn, next_tail, head, tail;\n \n-  get_block_head_tail (b, &head, &tail);\n+  get_ebb_head_tail (bb, bb, &head, &tail);\n   next_tail = NEXT_INSN (tail);\n \n   for (insn = head; insn != next_tail; insn = NEXT_INSN (insn))\n-    {\n-      int reg_weight = 0;\n-      rtx x;\n-\n-      /* Handle register life information.  */\n-      if (! INSN_P (insn))\n-\tcontinue;\n+    find_insn_reg_weight1 (insn);    \n+}\n \n-      /* Increment weight for each register born here.  */\n-      x = PATTERN (insn);\n-      reg_weight += find_set_reg_weight (x);\n-      if (GET_CODE (x) == PARALLEL)\n-\t{\n-\t  int j;\n-\t  for (j = XVECLEN (x, 0) - 1; j >= 0; j--)\n-\t    {\n-\t      x = XVECEXP (PATTERN (insn), 0, j);\n-\t      reg_weight += find_set_reg_weight (x);\n-\t    }\n-\t}\n-      /* Decrement weight for each register that dies here.  */\n-      for (x = REG_NOTES (insn); x; x = XEXP (x, 1))\n+/* Calculate INSN_REG_WEIGHT for single insntruction.\n+   Separated from find_insn_reg_weight because of need\n+   to initialize new instruction in generate_recovery_code.  */\n+static void\n+find_insn_reg_weight1 (rtx insn)\n+{\n+  int reg_weight = 0;\n+  rtx x;\n+  \n+  /* Handle register life information.  */\n+  if (! INSN_P (insn))\n+    return;\n+  \n+  /* Increment weight for each register born here.  */\n+  x = PATTERN (insn);\n+  reg_weight += find_set_reg_weight (x);\n+  if (GET_CODE (x) == PARALLEL)\n+    {\n+      int j;\n+      for (j = XVECLEN (x, 0) - 1; j >= 0; j--)\n \t{\n-\t  if (REG_NOTE_KIND (x) == REG_DEAD\n-\t      || REG_NOTE_KIND (x) == REG_UNUSED)\n-\t    reg_weight--;\n+\t  x = XVECEXP (PATTERN (insn), 0, j);\n+\t  reg_weight += find_set_reg_weight (x);\n \t}\n-\n-      INSN_REG_WEIGHT (insn) = reg_weight;\n     }\n+  /* Decrement weight for each register that dies here.  */\n+  for (x = REG_NOTES (insn); x; x = XEXP (x, 1))\n+    {\n+      if (REG_NOTE_KIND (x) == REG_DEAD\n+\t  || REG_NOTE_KIND (x) == REG_UNUSED)\n+\treg_weight--;\n+    }\n+  \n+  INSN_REG_WEIGHT (insn) = reg_weight;\n }\n \n /* Move insns that became ready to fire from queue to ready list.  */\n@@ -1670,36 +1873,17 @@ debug_ready_list (struct ready_list *ready)\n   fprintf (sched_dump, \"\\n\");\n }\n \n-/* move_insn1: Remove INSN from insn chain, and link it after LAST insn.  */\n-\n-static rtx\n-move_insn1 (rtx insn, rtx last)\n-{\n-  NEXT_INSN (PREV_INSN (insn)) = NEXT_INSN (insn);\n-  PREV_INSN (NEXT_INSN (insn)) = PREV_INSN (insn);\n-\n-  NEXT_INSN (insn) = NEXT_INSN (last);\n-  PREV_INSN (NEXT_INSN (last)) = insn;\n-\n-  NEXT_INSN (last) = insn;\n-  PREV_INSN (insn) = last;\n-\n-  return insn;\n-}\n-\n /* Search INSN for REG_SAVE_NOTE note pairs for\n    NOTE_INSN_EHREGION_{BEG,END}; and convert them back into\n    NOTEs.  The REG_SAVE_NOTE note following first one is contains the\n    saved value for NOTE_BLOCK_NUMBER which is useful for\n-   NOTE_INSN_EH_REGION_{BEG,END} NOTEs.  LAST is the last instruction\n-   output by the instruction scheduler.  Return the new value of LAST.  */\n+   NOTE_INSN_EH_REGION_{BEG,END} NOTEs.  */\n \n-static rtx\n-reemit_notes (rtx insn, rtx last)\n+static void\n+reemit_notes (rtx insn)\n {\n-  rtx note, retval;\n+  rtx note, last = insn;\n \n-  retval = last;\n   for (note = REG_NOTES (insn); note; note = XEXP (note, 1))\n     {\n       if (REG_NOTE_KIND (note) == REG_SAVE_NOTE)\n@@ -1710,31 +1894,96 @@ reemit_notes (rtx insn, rtx last)\n \t  remove_note (insn, note);\n \t}\n     }\n-  return retval;\n }\n \n-/* Move INSN.  Reemit notes if needed.\n+/* Move INSN.  Reemit notes if needed.  Update CFG, if needed.  */\n+static void\n+move_insn (rtx insn)\n+{\n+  rtx last = last_scheduled_insn;\n \n-   Return the last insn emitted by the scheduler, which is the\n-   return value from the first call to reemit_notes.  */\n+  if (PREV_INSN (insn) != last)\n+    {\n+      basic_block bb;\n+      rtx note;\n+      int jump_p = 0;\n \n-static rtx\n-move_insn (rtx insn, rtx last)\n-{\n-  rtx retval = NULL;\n+      bb = BLOCK_FOR_INSN (insn);\n+ \n+      /* BB_HEAD is either LABEL or NOTE.  */\n+      gcc_assert (BB_HEAD (bb) != insn);      \n+\n+      if (BB_END (bb) == insn)\n+\t/* If this is last instruction in BB, move end marker one\n+\t   instruction up.  */\n+\t{\n+\t  /* Jumps are always placed at the end of basic block.  */\n+\t  jump_p = control_flow_insn_p (insn);\n+\n+\t  gcc_assert (!jump_p\n+\t\t      || ((current_sched_info->flags & SCHED_RGN)\n+\t\t\t  && RECOVERY_BLOCK (insn)\n+\t\t\t  && RECOVERY_BLOCK (insn) != EXIT_BLOCK_PTR)\n+\t\t      || (current_sched_info->flags & SCHED_EBB));\n+\t  \n+\t  gcc_assert (BLOCK_FOR_INSN (PREV_INSN (insn)) == bb);\n+\n+\t  BB_END (bb) = PREV_INSN (insn);\n+\t}\n \n-  move_insn1 (insn, last);\n+      gcc_assert (BB_END (bb) != last);\n \n-  /* If this is the first call to reemit_notes, then record\n-     its return value.  */\n-  if (retval == NULL_RTX)\n-    retval = reemit_notes (insn, insn);\n-  else\n-    reemit_notes (insn, insn);\n+      if (jump_p)\n+\t/* We move the block note along with jump.  */\n+\t{\n+\t  /* NT is needed for assertion below.  */\n+\t  rtx nt = current_sched_info->next_tail;\n+\n+\t  note = NEXT_INSN (insn);\n+\t  while (NOTE_NOT_BB_P (note) && note != nt)\n+\t    note = NEXT_INSN (note);\n+\n+\t  if (note != nt\n+\t      && (LABEL_P (note)\n+\t\t  || BARRIER_P (note)))\n+\t    note = NEXT_INSN (note);\n+      \n+\t  gcc_assert (NOTE_INSN_BASIC_BLOCK_P (note));\n+\t}\n+      else\n+\tnote = insn;\n+\n+      NEXT_INSN (PREV_INSN (insn)) = NEXT_INSN (note);\n+      PREV_INSN (NEXT_INSN (note)) = PREV_INSN (insn);\n+\n+      NEXT_INSN (note) = NEXT_INSN (last);\n+      PREV_INSN (NEXT_INSN (last)) = note;\n \n-  SCHED_GROUP_P (insn) = 0;\n+      NEXT_INSN (last) = insn;\n+      PREV_INSN (insn) = last;\n+\n+      bb = BLOCK_FOR_INSN (last);\n+\n+      if (jump_p)\n+\t{\n+\t  fix_jump_move (insn);\n+\n+\t  if (BLOCK_FOR_INSN (insn) != bb)\n+\t    move_block_after_check (insn);\n+\n+\t  gcc_assert (BB_END (bb) == last);\n+\t}\n+\n+      set_block_for_insn (insn, bb);    \n+  \n+      /* Update BB_END, if needed.  */\n+      if (BB_END (bb) == last)\n+\tBB_END (bb) = insn;  \n+    }\n+  \n+  reemit_notes (insn);\n \n-  return retval;\n+  SCHED_GROUP_P (insn) = 0;  \n }\n \n /* The following structure describe an entry of the stack of choices.  */\n@@ -1784,13 +2033,15 @@ static int cached_issue_rate = 0;\n    insns is insns with the best rank (the first insn in READY).  To\n    make this function tries different samples of ready insns.  READY\n    is current queue `ready'.  Global array READY_TRY reflects what\n-   insns are already issued in this try.  INDEX will contain index\n-   of the best insn in READY.  The following function is used only for\n-   first cycle multipass scheduling.  */\n+   insns are already issued in this try.  MAX_POINTS is the sum of points\n+   of all instructions in READY.  The function stops immediatelly,\n+   if it reached the such a solution, that all instruction can be issued.\n+   INDEX will contain index of the best insn in READY.  The following\n+   function is used only for first cycle multipass scheduling.  */\n static int\n-max_issue (struct ready_list *ready, int *index)\n+max_issue (struct ready_list *ready, int *index, int max_points)\n {\n-  int n, i, all, n_ready, best, delay, tries_num;\n+  int n, i, all, n_ready, best, delay, tries_num, points = -1;\n   struct choice_entry *top;\n   rtx insn;\n \n@@ -1815,7 +2066,8 @@ max_issue (struct ready_list *ready, int *index)\n \t    {\n \t      best = top - choice_stack;\n \t      *index = choice_stack [1].index;\n-\t      if (top->n == issue_rate - cycle_issued_insns || best == all)\n+\t      points = top->n;\n+\t      if (top->n == max_points || best == all)\n \t\tbreak;\n \t    }\n \t  i = top->index;\n@@ -1838,7 +2090,7 @@ max_issue (struct ready_list *ready, int *index)\n \t\ttop->rest--;\n \t      n = top->n;\n \t      if (memcmp (top->state, curr_state, dfa_state_size) != 0)\n-\t\tn++;\n+\t\tn += ISSUE_POINTS (insn);\n \t      top++;\n \t      top->rest = cached_first_cycle_multipass_dfa_lookahead;\n \t      top->index = i;\n@@ -1855,7 +2107,14 @@ max_issue (struct ready_list *ready, int *index)\n       ready_try [top->index] = 0;\n       top--;\n     }\n-  memcpy (curr_state, choice_stack->state, dfa_state_size);\n+  memcpy (curr_state, choice_stack->state, dfa_state_size);  \n+\n+  if (sched_verbose >= 4)    \n+    fprintf (sched_dump, \";;\\t\\tChoosed insn : %s; points: %d/%d\\n\",\n+\t     (*current_sched_info->print_insn) (ready_element (ready, *index),\n+\t\t\t\t\t\t0), \n+\t     points, max_points);\n+  \n   return best;\n }\n \n@@ -1875,9 +2134,10 @@ choose_ready (struct ready_list *ready)\n   else\n     {\n       /* Try to choose the better insn.  */\n-      int index = 0, i;\n+      int index = 0, i, n;\n       rtx insn;\n-\n+      int more_issue, max_points, try_data = 1, try_control = 1;\n+      \n       if (cached_first_cycle_multipass_dfa_lookahead != lookahead)\n \t{\n \t  cached_first_cycle_multipass_dfa_lookahead = lookahead;\n@@ -1888,26 +2148,79 @@ choose_ready (struct ready_list *ready)\n       insn = ready_element (ready, 0);\n       if (INSN_CODE (insn) < 0)\n \treturn ready_remove_first (ready);\n+\n+      if (spec_info\n+\t  && spec_info->flags & (PREFER_NON_DATA_SPEC\n+\t\t\t\t | PREFER_NON_CONTROL_SPEC))\n+\t{\n+\t  rtx x;\n+\t  int s;\n+\n+\t  for (i = 0, n = ready->n_ready; i < n; i++)\n+\t    {\n+\t      x = ready_element (ready, i);\n+\t      s = TODO_SPEC (x);\n+\t      \n+\t      if (spec_info->flags & PREFER_NON_DATA_SPEC\n+\t\t  && !(s & DATA_SPEC))\n+\t\t{\t\t  \n+\t\t  try_data = 0;\n+\t\t  if (!(spec_info->flags & PREFER_NON_CONTROL_SPEC)\n+\t\t      || !try_control)\n+\t\t    break;\n+\t\t}\n+\t      \n+\t      if (spec_info->flags & PREFER_NON_CONTROL_SPEC\n+\t\t  && !(s & CONTROL_SPEC))\n+\t\t{\n+\t\t  try_control = 0;\n+\t\t  if (!(spec_info->flags & PREFER_NON_DATA_SPEC) || !try_data)\n+\t\t    break;\n+\t\t}\n+\t    }\n+\t}\n+\n+      if ((!try_data && (TODO_SPEC (insn) & DATA_SPEC))\n+\t  || (!try_control && (TODO_SPEC (insn) & CONTROL_SPEC))\n+\t  || (targetm.sched.first_cycle_multipass_dfa_lookahead_guard_spec\n+\t      && !targetm.sched.first_cycle_multipass_dfa_lookahead_guard_spec\n+\t      (insn)))\n+\t{\n+\t  change_queue_index (insn, 1);\n+\t  return 0;\n+\t}\n+\n+      max_points = ISSUE_POINTS (insn);\n+      more_issue = issue_rate - cycle_issued_insns - 1;\n+\n       for (i = 1; i < ready->n_ready; i++)\n \t{\n \t  insn = ready_element (ready, i);\n \t  ready_try [i]\n \t    = (INSN_CODE (insn) < 0\n+               || (!try_data && (TODO_SPEC (insn) & DATA_SPEC))\n+               || (!try_control && (TODO_SPEC (insn) & CONTROL_SPEC))\n \t       || (targetm.sched.first_cycle_multipass_dfa_lookahead_guard\n-\t\t   && !targetm.sched.first_cycle_multipass_dfa_lookahead_guard (insn)));\n+\t\t   && !targetm.sched.first_cycle_multipass_dfa_lookahead_guard\n+\t\t   (insn)));\n+\n+\t  if (!ready_try [i] && more_issue-- > 0)\n+\t    max_points += ISSUE_POINTS (insn);\n \t}\n-      if (max_issue (ready, &index) == 0)\n+\n+      if (max_issue (ready, &index, max_points) == 0)\n \treturn ready_remove_first (ready);\n       else\n \treturn ready_remove (ready, index);\n     }\n }\n \n-/* Use forward list scheduling to rearrange insns of block B in region RGN,\n-   possibly bringing insns from subsequent blocks in the same region.  */\n+/* Use forward list scheduling to rearrange insns of block pointed to by\n+   TARGET_BB, possibly bringing insns from subsequent blocks in the same\n+   region.  */\n \n void\n-schedule_block (int b, int rgn_n_insns)\n+schedule_block (basic_block *target_bb, int rgn_n_insns1)\n {\n   struct ready_list ready;\n   int i, first_cycle_insn_p;\n@@ -1930,42 +2243,38 @@ schedule_block (int b, int rgn_n_insns)\n \n   gcc_assert (head != tail || INSN_P (head));\n \n+  added_recovery_block_p = false;\n+\n   /* Debug info.  */\n   if (sched_verbose)\n-    {\n-      fprintf (sched_dump,\n-\t       \";;   ======================================================\\n\");\n-      fprintf (sched_dump,\n-\t       \";;   -- basic block %d from %d to %d -- %s reload\\n\",\n-\t       b, INSN_UID (head), INSN_UID (tail),\n-\t       (reload_completed ? \"after\" : \"before\"));\n-      fprintf (sched_dump,\n-\t       \";;   ======================================================\\n\");\n-      fprintf (sched_dump, \"\\n\");\n-    }\n+    dump_new_block_header (0, *target_bb, head, tail);\n \n   state_reset (curr_state);\n \n   /* Allocate the ready list.  */\n   readyp = &ready;\n-  ready.veclen = rgn_n_insns + 1 + issue_rate;\n+  ready.vec = NULL;\n+  ready_try = NULL;\n+  choice_stack = NULL;\n+\n+  rgn_n_insns = -1;\n+  extend_ready (rgn_n_insns1 + 1);\n+\n   ready.first = ready.veclen - 1;\n-  ready.vec = XNEWVEC (rtx, ready.veclen);\n   ready.n_ready = 0;\n \n   /* It is used for first cycle multipass scheduling.  */\n   temp_state = alloca (dfa_state_size);\n-  ready_try = XCNEWVEC (char, rgn_n_insns + 1);\n-  choice_stack = XNEWVEC (struct choice_entry, rgn_n_insns + 1);\n-  for (i = 0; i <= rgn_n_insns; i++)\n-    choice_stack[i].state = xmalloc (dfa_state_size);\n \n   if (targetm.sched.md_init)\n     targetm.sched.md_init (sched_dump, sched_verbose, ready.veclen);\n \n   /* We start inserting insns after PREV_HEAD.  */\n   last_scheduled_insn = prev_head;\n \n+  gcc_assert (NOTE_P (last_scheduled_insn)\n+\t      && BLOCK_FOR_INSN (last_scheduled_insn) == *target_bb);\n+\n   /* Initialize INSN_QUEUE.  Q_SIZE is the total number of insns in the\n      queue.  */\n   q_ptr = 0;\n@@ -1981,6 +2290,9 @@ schedule_block (int b, int rgn_n_insns)\n      in try_ready () (which is called through init_ready_list ()).  */\n   (*current_sched_info->init_ready_list) ();\n \n+  /* Now we can restore basic block notes and maintain precise cfg.  */\n+  restore_bb_notes (*target_bb);\n+\n   last_clock_var = -1;\n \n   advance = 0;\n@@ -2048,7 +2360,7 @@ schedule_block (int b, int rgn_n_insns)\n \n \t  if (sched_verbose >= 2)\n \t    {\n-\t      fprintf (sched_dump, \";;\\tReady list (t =%3d):  \",\n+\t      fprintf (sched_dump, \";;\\tReady list (t = %3d):  \",\n \t\t       clock_var);\n \t      debug_ready_list (&ready);\n \t    }\n@@ -2074,7 +2386,11 @@ schedule_block (int b, int rgn_n_insns)\n \n \t  /* Select and remove the insn from the ready list.  */\n \t  if (sort_p)\n-\t    insn = choose_ready (&ready);\n+\t    {\n+\t      insn = choose_ready (&ready);\n+\t      if (!insn)\n+\t\tcontinue;\n+\t    }\n \t  else\n \t    insn = ready_remove_first (&ready);\n \n@@ -2136,11 +2452,50 @@ schedule_block (int b, int rgn_n_insns)\n \t      continue;\n \t    }\n \n-\t  if (! (*current_sched_info->can_schedule_ready_p) (insn))\n-\t    goto next;\n+\t  if (current_sched_info->can_schedule_ready_p\n+\t      && ! (*current_sched_info->can_schedule_ready_p) (insn))\n+\t    /* We normally get here only if we don't want to move\n+\t       insn from the split block.  */\n+\t    {\n+\t      TODO_SPEC (insn) = (TODO_SPEC (insn) & ~SPECULATIVE) | HARD_DEP;\n+\t      continue;\n+\t    }\n+\n+\t  /* DECISSION is made.  */\t\n+  \n+          if (TODO_SPEC (insn) & SPECULATIVE)\n+            generate_recovery_code (insn);\n+\n+\t  if (control_flow_insn_p (last_scheduled_insn)\t     \n+\t      /* This is used to to switch basic blocks by request\n+\t\t from scheduler front-end (actually, sched-ebb.c only).\n+\t\t This is used to process blocks with single fallthru\n+\t\t edge.  If successing block has jump, it [jump] will try\n+\t\t move at the end of current bb, thus corrupting CFG.  */\n+\t      || current_sched_info->advance_target_bb (*target_bb, insn))\n+\t    {\n+\t      *target_bb = current_sched_info->advance_target_bb\n+\t\t(*target_bb, 0);\n+\t      \n+\t      if (sched_verbose)\n+\t\t{\n+\t\t  rtx x;\n \n-\t  last_scheduled_insn = move_insn (insn, last_scheduled_insn);\n+\t\t  x = next_real_insn (last_scheduled_insn);\n+\t\t  gcc_assert (x);\n+\t\t  dump_new_block_header (1, *target_bb, x, tail);\n+\t\t}\n \n+\t      last_scheduled_insn = bb_note (*target_bb);\n+\t    }\n+ \n+\t  /* Update counters, etc in the scheduler's front end.  */\n+\t  (*current_sched_info->begin_schedule_ready) (insn,\n+\t\t\t\t\t\t       last_scheduled_insn);\n+ \n+\t  move_insn (insn);\n+\t  last_scheduled_insn = insn;\n+\t  \n \t  if (memcmp (curr_state, temp_state, dfa_state_size) != 0)\n             {\n               cycle_issued_insns++;\n@@ -2165,7 +2520,6 @@ schedule_block (int b, int rgn_n_insns)\n \t  if (advance != 0)\n \t    break;\n \n-\tnext:\n \t  first_cycle_insn_p = 0;\n \n \t  /* Sort the ready list based on priority.  This must be\n@@ -2202,17 +2556,33 @@ schedule_block (int b, int rgn_n_insns)\n     {\n       /* We must maintain QUEUE_INDEX between blocks in region.  */\n       for (i = ready.n_ready - 1; i >= 0; i--)\n-\tQUEUE_INDEX (ready_element (&ready, i)) = QUEUE_NOWHERE;\n+\t{\n+\t  rtx x;\n+\t  \n+\t  x = ready_element (&ready, i);\n+\t  QUEUE_INDEX (x) = QUEUE_NOWHERE;\n+\t  TODO_SPEC (x) = (TODO_SPEC (x) & ~SPECULATIVE) | HARD_DEP;\n+\t}\n \n       if (q_size)   \n \tfor (i = 0; i <= max_insn_queue_index; i++)\n \t  {\n \t    rtx link;\n \t    for (link = insn_queue[i]; link; link = XEXP (link, 1))\n-\t      QUEUE_INDEX (XEXP (link, 0)) = QUEUE_NOWHERE;\n+\t      {\n+\t\trtx x;\n+\n+\t\tx = XEXP (link, 0);\n+\t\tQUEUE_INDEX (x) = QUEUE_NOWHERE;\n+\t\tTODO_SPEC (x) = (TODO_SPEC (x) & ~SPECULATIVE) | HARD_DEP;\n+\t      }\n \t    free_INSN_LIST_list (&insn_queue[i]);\n \t  }\n+    }\n \n+  if (!current_sched_info->queue_must_finish_empty\n+      || added_recovery_block_p)\n+    {\n       /* INSN_TICK (minimum clock tick at which the insn becomes\n          ready) may be not correct for the insn in the subsequent\n          blocks of the region.  We should use a correct value of\n@@ -2222,6 +2592,14 @@ schedule_block (int b, int rgn_n_insns)\n       fix_inter_tick (NEXT_INSN (prev_head), last_scheduled_insn);\n     }\n \n+#ifdef ENABLE_CHECKING\n+  /* After the reload the ia64 backend doesn't maintain BB_END, so\n+     if we want to check anything, better do it now. \n+     And it already clobbered previously scheduled code.  */\n+  if (reload_completed)\n+    check_cfg (BB_HEAD (BLOCK_FOR_INSN (prev_head)), 0);\n+#endif\n+\n   if (targetm.sched.md_finish)\n     targetm.sched.md_finish (sched_dump, sched_verbose);\n \n@@ -2234,12 +2612,16 @@ schedule_block (int b, int rgn_n_insns)\n      of the insns.  */\n   if (note_list != 0)\n     {\n+      basic_block head_bb = BLOCK_FOR_INSN (head);\n       rtx note_head = note_list;\n \n       while (PREV_INSN (note_head))\n \t{\n+\t  set_block_for_insn (note_head, head_bb);\n \t  note_head = PREV_INSN (note_head);\n \t}\n+      /* In the above cycle we've missed this note:  */\n+      set_block_for_insn (note_head, head_bb);\n \n       PREV_INSN (note_head) = PREV_INSN (head);\n       NEXT_INSN (PREV_INSN (head)) = note_head;\n@@ -2303,16 +2685,23 @@ set_priorities (rtx head, rtx tail)\n   return n_insn;\n }\n \n+/* Next LUID to assign to an instruction.  */\n+static int luid;\n+\n /* Initialize some global state for the scheduler.  */\n \n void\n sched_init (void)\n {\n-  int luid;\n   basic_block b;\n   rtx insn;\n   int i;\n \n+  /* Switch to working copy of sched_info.  */\n+  memcpy (&current_sched_info_var, current_sched_info,\n+\t  sizeof (current_sched_info_var));\n+  current_sched_info = &current_sched_info_var;\n+      \n   /* Disable speculative loads in their presence if cc0 defined.  */\n #ifdef HAVE_cc0\n   flag_schedule_speculative_load = 0;\n@@ -2327,6 +2716,25 @@ sched_init (void)\n   sched_dump = ((sched_verbose_param >= 10 || !dump_file)\n \t\t? stderr : dump_file);\n \n+  /* Initialize SPEC_INFO.  */\n+  if (targetm.sched.set_sched_flags)\n+    {\n+      spec_info = &spec_info_var;\n+      targetm.sched.set_sched_flags (spec_info);\n+      if (current_sched_info->flags & DO_SPECULATION)\n+\tspec_info->weakness_cutoff =\n+\t  (PARAM_VALUE (PARAM_SCHED_SPEC_PROB_CUTOFF) * MAX_DEP_WEAK) / 100;\n+      else\n+\t/* So we won't read anything accidently.  */\n+\tspec_info = 0;\n+#ifdef ENABLE_CHECKING\n+      check_sched_flags ();\n+#endif\n+    }\n+  else\n+    /* So we won't read anything accidently.  */\n+    spec_info = 0;\n+\n   /* Initialize issue_rate.  */\n   if (targetm.sched.issue_rate)\n     issue_rate = targetm.sched.issue_rate ();\n@@ -2340,15 +2748,14 @@ sched_init (void)\n       cached_first_cycle_multipass_dfa_lookahead = 0;\n     }\n \n-  /* We use LUID 0 for the fake insn (UID 0) which holds dependencies for\n-     pseudos which do not cross calls.  */\n-  old_max_uid = get_max_uid () + 1;\n-\n-  h_i_d = XCNEWVEC (struct haifa_insn_data, old_max_uid);\n+  old_max_uid = 0;\n+  h_i_d = 0;\n+  extend_h_i_d ();\n \n   for (i = 0; i < old_max_uid; i++)\n     {\n       h_i_d[i].cost = -1;\n+      h_i_d[i].todo_spec = HARD_DEP;\n       h_i_d[i].queue_index = QUEUE_NOWHERE;\n       h_i_d[i].tick = INVALID_TICK;\n       h_i_d[i].inter_tick = INVALID_TICK;\n@@ -2387,59 +2794,30 @@ sched_init (void)\n \n   init_alias_analysis ();\n \n-  if (write_symbols != NO_DEBUG)\n-    {\n-      rtx line;\n-\n-      line_note_head = XCNEWVEC (rtx, last_basic_block);\n-\n-      /* Save-line-note-head:\n-         Determine the line-number at the start of each basic block.\n-         This must be computed and saved now, because after a basic block's\n-         predecessor has been scheduled, it is impossible to accurately\n-         determine the correct line number for the first insn of the block.  */\n-\n-      FOR_EACH_BB (b)\n-\t{\n-\t  for (line = BB_HEAD (b); line; line = PREV_INSN (line))\n-\t    if (NOTE_P (line) && NOTE_LINE_NUMBER (line) > 0)\n-\t      {\n-\t\tline_note_head[b->index] = line;\n-\t\tbreak;\n-\t      }\n-\t  /* Do a forward search as well, since we won't get to see the first\n-\t     notes in a basic block.  */\n-\t  for (line = BB_HEAD (b); line; line = NEXT_INSN (line))\n-\t    {\n-\t      if (INSN_P (line))\n-\t\tbreak;\n-\t      if (NOTE_P (line) && NOTE_LINE_NUMBER (line) > 0)\n-\t\tline_note_head[b->index] = line;\n-\t    }\n-\t}\n-    }\n-\n-  /* The following is done to keep current_sched_info->next_tail non null.  */\n+  line_note_head = 0;\n+  old_last_basic_block = 0;\n+  glat_start = 0;  \n+  glat_end = 0;\n+  extend_bb (0);\n \n-  insn = BB_END (EXIT_BLOCK_PTR->prev_bb);\n-  if (NEXT_INSN (insn) == 0\n-      || (!NOTE_P (insn)\n-\t  && !LABEL_P (insn)\n-\t  /* Don't emit a NOTE if it would end up before a BARRIER.  */\n-\t  && !BARRIER_P (NEXT_INSN (insn))))\n-    {\n-      emit_note_after (NOTE_INSN_DELETED, insn);\n-      /* Make insn to appear outside BB.  */\n-      BB_END (EXIT_BLOCK_PTR->prev_bb) = insn;\n-    }\n+  if (current_sched_info->flags & USE_GLAT)\n+    init_glat ();\n \n   /* Compute INSN_REG_WEIGHT for all blocks.  We must do this before\n      removing death notes.  */\n   FOR_EACH_BB_REVERSE (b)\n-    find_insn_reg_weight (b->index);\n+    find_insn_reg_weight (b);\n \n   if (targetm.sched.md_init_global)\n       targetm.sched.md_init_global (sched_dump, sched_verbose, old_max_uid);\n+\n+  nr_begin_data = nr_begin_control = nr_be_in_data = nr_be_in_control = 0;\n+  before_recovery = 0;\n+\n+#ifdef ENABLE_CHECKING\n+  /* This is used preferably for finding bugs in check_cfg () itself.  */\n+  check_cfg (0, 0);\n+#endif\n }\n \n /* Free global data used during insn scheduling.  */\n@@ -2452,11 +2830,38 @@ sched_finish (void)\n   dfa_finish ();\n   free_dependency_caches ();\n   end_alias_analysis ();\n-  if (write_symbols != NO_DEBUG)\n-    free (line_note_head);\n+  free (line_note_head);\n+  free_glat ();\n \n   if (targetm.sched.md_finish_global)\n-      targetm.sched.md_finish_global (sched_dump, sched_verbose);\n+    targetm.sched.md_finish_global (sched_dump, sched_verbose);\n+  \n+  if (spec_info && spec_info->dump)\n+    {\n+      char c = reload_completed ? 'a' : 'b';\n+\n+      fprintf (spec_info->dump,\n+\t       \";; %s:\\n\", current_function_name ());\n+\n+      fprintf (spec_info->dump,\n+               \";; Procedure %cr-begin-data-spec motions == %d\\n\",\n+               c, nr_begin_data);\n+      fprintf (spec_info->dump,\n+               \";; Procedure %cr-be-in-data-spec motions == %d\\n\",\n+               c, nr_be_in_data);\n+      fprintf (spec_info->dump,\n+               \";; Procedure %cr-begin-control-spec motions == %d\\n\",\n+               c, nr_begin_control);\n+      fprintf (spec_info->dump,\n+               \";; Procedure %cr-be-in-control-spec motions == %d\\n\",\n+               c, nr_be_in_control);\n+    }\n+\n+#ifdef ENABLE_CHECKING\n+  /* After reload ia64 backend clobbers CFG, so can't check anything.  */\n+  if (!reload_completed)\n+    check_cfg (0, 0);\n+#endif\n \n   current_sched_info = NULL;\n }\n@@ -2538,26 +2943,164 @@ fix_inter_tick (rtx head, rtx tail)\n    0 < N - queued for N cycles.  */\n int\n try_ready (rtx next)\n-{\n-  if (LOG_LINKS (next)\n-      || (current_sched_info->new_ready\n-\t  && !current_sched_info->new_ready (next)))\n-    {\n-      gcc_assert (QUEUE_INDEX (next) == QUEUE_NOWHERE);\n-      return -1;\n-    }\n-\n-  if (sched_verbose >= 2)\n-    fprintf (sched_dump, \";;\\t\\tdependencies resolved: insn %s\\n\",\n-\t     (*current_sched_info->print_insn) (next, 0));          \n-        \n-  adjust_priority (next);\n+{  \n+  ds_t old_ts, *ts;\n+  rtx link;\n \n-  return fix_tick_ready (next);\n-}\n+  ts = &TODO_SPEC (next);\n+  old_ts = *ts;\n \n-/* Calculate INSN_TICK of NEXT and add it to either ready or queue list.  */\n-static int\n+  gcc_assert (!(old_ts & ~(SPECULATIVE | HARD_DEP))\n+\t      && ((old_ts & HARD_DEP)\n+\t\t  || (old_ts & SPECULATIVE)));\n+  \n+  if (!(current_sched_info->flags & DO_SPECULATION))\n+    {\n+      if (!LOG_LINKS (next))\n+        *ts &= ~HARD_DEP;\n+    }\n+  else\n+    {\n+      *ts &= ~SPECULATIVE & ~HARD_DEP;          \n+  \n+      link = LOG_LINKS (next);\n+      if (link)\n+        {\n+          /* LOG_LINKS are maintained sorted. \n+             So if DEP_STATUS of the first dep is SPECULATIVE,\n+             than all other deps are speculative too.  */\n+          if (DEP_STATUS (link) & SPECULATIVE)          \n+            {          \n+              /* Now we've got NEXT with speculative deps only.\n+                 1. Look at the deps to see what we have to do.\n+                 2. Check if we can do 'todo'.  */\n+\t      *ts = DEP_STATUS (link) & SPECULATIVE;\n+              while ((link = XEXP (link, 1)))\n+\t\t*ts = ds_merge (*ts, DEP_STATUS (link) & SPECULATIVE);\n+\n+\t      if (dep_weak (*ts) < spec_info->weakness_cutoff)\n+\t\t/* Too few points.  */\n+\t\t*ts = (*ts & ~SPECULATIVE) | HARD_DEP;\n+\t    }\n+          else\n+            *ts |= HARD_DEP;\n+        }\n+    }\n+  \n+  if (*ts & HARD_DEP)\n+    gcc_assert (*ts == old_ts\n+\t\t&& QUEUE_INDEX (next) == QUEUE_NOWHERE);\n+  else if (current_sched_info->new_ready)\n+    *ts = current_sched_info->new_ready (next, *ts);  \n+\n+  /* * if !(old_ts & SPECULATIVE) (e.g. HARD_DEP or 0), then insn might \n+     have its original pattern or changed (speculative) one.  This is due\n+     to changing ebb in region scheduling.\n+     * But if (old_ts & SPECULATIVE), then we are pretty sure that insn\n+     has speculative pattern.\n+     \n+     We can't assert (!(*ts & HARD_DEP) || *ts == old_ts) here because\n+     control-speculative NEXT could have been discarded by sched-rgn.c\n+     (the same case as when discarded by can_schedule_ready_p ()).  */\n+  \n+  if ((*ts & SPECULATIVE)\n+      /* If (old_ts == *ts), then (old_ts & SPECULATIVE) and we don't \n+\t need to change anything.  */\n+      && *ts != old_ts)\n+    {\n+      int res;\n+      rtx new_pat;\n+      \n+      gcc_assert ((*ts & SPECULATIVE) && !(*ts & ~SPECULATIVE));\n+      \n+      res = speculate_insn (next, *ts, &new_pat);\n+\t\n+      switch (res)\n+\t{\n+\tcase -1:\n+\t  /* It would be nice to change DEP_STATUS of all dependences,\n+\t     which have ((DEP_STATUS & SPECULATIVE) == *ts) to HARD_DEP,\n+\t     so we won't reanalyze anything.  */\n+\t  *ts = (*ts & ~SPECULATIVE) | HARD_DEP;\n+\t  break;\n+\t  \n+\tcase 0:\n+\t  /* We follow the rule, that every speculative insn\n+\t     has non-null ORIG_PAT.  */\n+\t  if (!ORIG_PAT (next))\n+\t    ORIG_PAT (next) = PATTERN (next);\n+\t  break;\n+\t  \n+\tcase 1:                  \n+\t  if (!ORIG_PAT (next))\n+\t    /* If we gonna to overwrite the original pattern of insn,\n+\t       save it.  */\n+\t    ORIG_PAT (next) = PATTERN (next);\n+\t  \n+\t  change_pattern (next, new_pat);\n+\t  break;\n+\t  \n+\tdefault:\n+\t  gcc_unreachable ();\n+\t}\n+    }\n+  \n+  /* We need to restore pattern only if (*ts == 0), because otherwise it is\n+     either correct (*ts & SPECULATIVE),\n+     or we simply don't care (*ts & HARD_DEP).  */\n+  \n+  gcc_assert (!ORIG_PAT (next)\n+\t      || !RECOVERY_BLOCK (next)\n+\t      || RECOVERY_BLOCK (next) == EXIT_BLOCK_PTR);\n+  \n+  if (*ts == 0 && ORIG_PAT (next) && !RECOVERY_BLOCK (next))\n+    /* We should change pattern of every previously speculative \n+       instruction - and we determine if NEXT was speculative by using\n+       ORIG_PAT field.  Except one case - simple checks have ORIG_PAT\n+       pat too, hence we also check for the RECOVERY_BLOCK.  */\n+    {\n+      change_pattern (next, ORIG_PAT (next));\n+      ORIG_PAT (next) = 0;\n+    }\n+\n+  if (*ts & HARD_DEP)\n+    {\n+      /* We can't assert (QUEUE_INDEX (next) == QUEUE_NOWHERE) here because\n+\t control-speculative NEXT could have been discarded by sched-rgn.c\n+\t (the same case as when discarded by can_schedule_ready_p ()).  */\n+      /*gcc_assert (QUEUE_INDEX (next) == QUEUE_NOWHERE);*/\n+      \n+      change_queue_index (next, QUEUE_NOWHERE);\n+      return -1;\n+    }\n+\n+  if (sched_verbose >= 2)\n+    {\t      \n+      int s = TODO_SPEC (next);\n+          \n+      fprintf (sched_dump, \";;\\t\\tdependencies resolved: insn %s\",\n+               (*current_sched_info->print_insn) (next, 0));\n+          \n+      if (spec_info && spec_info->dump)\n+        {\n+          if (s & BEGIN_DATA)\n+            fprintf (spec_info->dump, \"; data-spec;\");\n+          if (s & BEGIN_CONTROL)\n+            fprintf (spec_info->dump, \"; control-spec;\");\n+          if (s & BE_IN_CONTROL)\n+            fprintf (spec_info->dump, \"; in-control-spec;\");\n+        }\n+\n+      fprintf (sched_dump, \"\\n\");\n+    }          \n+  \n+  adjust_priority (next);\n+        \n+  return fix_tick_ready (next);\n+}\n+\n+/* Calculate INSN_TICK of NEXT and add it to either ready or queue list.  */\n+static int\n fix_tick_ready (rtx next)\n {\n   rtx link;\n@@ -2570,7 +3113,7 @@ fix_tick_ready (rtx next)\n       int full_p;\n \n       tick = INSN_TICK (next);\n-      /* if tick is note equals to INVALID_TICK, then update\n+      /* if tick is not equal to INVALID_TICK, then update\n \t INSN_TICK of NEXT with the most recent resolved dependence\n \t cost.  Overwise, recalculate from scratch.  */\n       full_p = tick == INVALID_TICK;\n@@ -2581,10 +3124,7 @@ fix_tick_ready (rtx next)\n               \n           pro = XEXP (link, 0);\n \t  gcc_assert (INSN_TICK (pro) >= MIN_TICK);\n-          /* We should specify FORWARD link to insn_cost,\n-\t     but are giving a BACKWARD one.\n-             This is ok, because only REG_NOTE_KIND of link is used.\n-             May be substitute LINK with REG_NOTE_KIND?  */\n+\n           tick1 = INSN_TICK (pro) + insn_cost (pro, link, next);\n           if (tick1 > tick)\n             tick = tick1;\n@@ -2601,7 +3141,7 @@ fix_tick_ready (rtx next)\n     delay = QUEUE_READY;\n \n   change_queue_index (next, delay);\n-  \n+\n   return delay;\n }\n \n@@ -2664,4 +3204,1447 @@ resolve_dep (rtx next, rtx insn)\n \t      && (LOG_LINKS (next) || INSN_DEP_COUNT (next) == 0));\n }\n \n+/* Extend H_I_D data.  */\n+static void\n+extend_h_i_d (void)\n+{\n+  /* We use LUID 0 for the fake insn (UID 0) which holds dependencies for\n+     pseudos which do not cross calls.  */\n+  int new_max_uid = get_max_uid() + 1;  \n+\n+  h_i_d = xrecalloc (h_i_d, new_max_uid, old_max_uid, sizeof (*h_i_d));\n+  old_max_uid = new_max_uid;\n+\n+  if (targetm.sched.h_i_d_extended)\n+    targetm.sched.h_i_d_extended ();\n+}\n+\n+/* Extend READY, READY_TRY and CHOICE_STACK arrays.\n+   N_NEW_INSNS is the number of additional elements to allocate.  */\n+static void\n+extend_ready (int n_new_insns)\n+{\n+  int i;\n+\n+  readyp->veclen = rgn_n_insns + n_new_insns + 1 + issue_rate;\n+  readyp->vec = XRESIZEVEC (rtx, readyp->vec, readyp->veclen);\n+ \n+  ready_try = xrecalloc (ready_try, rgn_n_insns + n_new_insns + 1,\n+\t\t\t rgn_n_insns + 1, sizeof (char));\n+\n+  rgn_n_insns += n_new_insns;\n+\n+  choice_stack = XRESIZEVEC (struct choice_entry, choice_stack,\n+\t\t\t     rgn_n_insns + 1);\n+\n+  for (i = rgn_n_insns; n_new_insns--; i--)\n+    choice_stack[i].state = xmalloc (dfa_state_size);\n+}\n+\n+/* Extend global scheduler structures (those, that live across calls to\n+   schedule_block) to include information about just emitted INSN.  */\n+static void\n+extend_global (rtx insn)\n+{\n+  gcc_assert (INSN_P (insn));\n+  /* These structures have scheduler scope.  */\n+  extend_h_i_d ();\n+  init_h_i_d (insn);\n+\n+  extend_dependency_caches (1, 0);\n+}\n+\n+/* Extends global and local scheduler structures to include information\n+   about just emitted INSN.  */\n+static void\n+extend_all (rtx insn)\n+{ \n+  extend_global (insn);\n+\n+  /* These structures have block scope.  */\n+  extend_ready (1);\n+  \n+  (*current_sched_info->add_remove_insn) (insn, 0);\n+}\n+\n+/* Initialize h_i_d entry of the new INSN with default values.\n+   Values, that are not explicitly initialized here, hold zero.  */\n+static void\n+init_h_i_d (rtx insn)\n+{\n+  INSN_LUID (insn) = luid++;\n+  INSN_COST (insn) = -1;\n+  TODO_SPEC (insn) = HARD_DEP;\n+  QUEUE_INDEX (insn) = QUEUE_NOWHERE;\n+  INSN_TICK (insn) = INVALID_TICK;\n+  INTER_TICK (insn) = INVALID_TICK;\n+  find_insn_reg_weight1 (insn);  \n+}\n+\n+/* Generates recovery code for INSN.  */\n+static void\n+generate_recovery_code (rtx insn)\n+{\n+  if (TODO_SPEC (insn) & BEGIN_SPEC)\n+    begin_speculative_block (insn);\n+  \n+  /* Here we have insn with no dependencies to\n+     instructions other then CHECK_SPEC ones.  */\n+  \n+  if (TODO_SPEC (insn) & BE_IN_SPEC)\n+    add_to_speculative_block (insn);\n+}\n+\n+/* Helper function.\n+   Tries to add speculative dependencies of type FS between instructions\n+   in LINK list and TWIN.  */\n+static void\n+process_insn_depend_be_in_spec (rtx link, rtx twin, ds_t fs)\n+{\n+  for (; link; link = XEXP (link, 1))\n+    {\n+      ds_t ds;\n+      rtx consumer;\n+\n+      consumer = XEXP (link, 0);\n+\n+      ds = DEP_STATUS (link);\n+\n+      if (fs && (ds & DEP_TYPES) == DEP_TRUE)\n+\tds = (ds & ~BEGIN_SPEC) | fs;\n+\n+      add_back_forw_dep (consumer, twin, REG_NOTE_KIND (link), ds);\n+    }\n+}\n+\n+/* Generates recovery code for BEGIN speculative INSN.  */\n+static void\n+begin_speculative_block (rtx insn)\n+{\n+  if (TODO_SPEC (insn) & BEGIN_DATA)\n+    nr_begin_data++;      \n+  if (TODO_SPEC (insn) & BEGIN_CONTROL)\n+    nr_begin_control++;\n+\n+  create_check_block_twin (insn, false);\n+\n+  TODO_SPEC (insn) &= ~BEGIN_SPEC;\n+}\n+\n+/* Generates recovery code for BE_IN speculative INSN.  */\n+static void\n+add_to_speculative_block (rtx insn)\n+{\n+  ds_t ts;\n+  rtx link, twins = NULL;\n+\n+  ts = TODO_SPEC (insn);\n+  gcc_assert (!(ts & ~BE_IN_SPEC));\n+\n+  if (ts & BE_IN_DATA)\n+    nr_be_in_data++;\n+  if (ts & BE_IN_CONTROL)\n+    nr_be_in_control++;\n+\n+  TODO_SPEC (insn) &= ~BE_IN_SPEC;\n+  gcc_assert (!TODO_SPEC (insn));\n+  \n+  DONE_SPEC (insn) |= ts;\n+\n+  /* First we convert all simple checks to branchy.  */\n+  for (link = LOG_LINKS (insn); link;)\n+    {\n+      rtx check;\n+\n+      check = XEXP (link, 0);\n+\n+      if (RECOVERY_BLOCK (check))\n+\t{\n+\t  create_check_block_twin (check, true);\n+\t  link = LOG_LINKS (insn);\n+\t}\n+      else\n+\tlink = XEXP (link, 1);\n+    }\n+\n+  clear_priorities (insn);\n+ \n+  do\n+    {\n+      rtx link, check, twin;\n+      basic_block rec;\n+\n+      link = LOG_LINKS (insn);\n+      gcc_assert (!(DEP_STATUS (link) & BEGIN_SPEC)\n+\t\t  && (DEP_STATUS (link) & BE_IN_SPEC)\n+\t\t  && (DEP_STATUS (link) & DEP_TYPES) == DEP_TRUE);\n+\n+      check = XEXP (link, 0);\n+      gcc_assert (!RECOVERY_BLOCK (check) && !ORIG_PAT (check)\n+\t\t  && QUEUE_INDEX (check) == QUEUE_NOWHERE);\n+      \n+      rec = BLOCK_FOR_INSN (check);\n+      \n+      twin = emit_insn_before (copy_rtx (PATTERN (insn)), BB_END (rec));\n+      extend_global (twin);\n+\n+      RESOLVED_DEPS (twin) = copy_DEPS_LIST_list (RESOLVED_DEPS (insn));\n+\n+      if (sched_verbose && spec_info->dump)\n+        /* INSN_BB (insn) isn't determined for twin insns yet.\n+           So we can't use current_sched_info->print_insn.  */\n+        fprintf (spec_info->dump, \";;\\t\\tGenerated twin insn : %d/rec%d\\n\",\n+                 INSN_UID (twin), rec->index);\n+\n+      twins = alloc_INSN_LIST (twin, twins);\n+\n+      /* Add dependences between TWIN and all apropriate\n+\t instructions from REC.  */\n+      do\n+\t{\t  \n+\t  add_back_forw_dep (twin, check, REG_DEP_TRUE, DEP_TRUE);\n+\t  \n+\t  do\t    \t  \n+\t    {  \n+\t      link = XEXP (link, 1);\n+\t      if (link)\n+\t\t{\n+\t\t  check = XEXP (link, 0);\n+\t\t  if (BLOCK_FOR_INSN (check) == rec)\n+\t\t    break;\n+\t\t}\n+\t      else\n+\t\tbreak;\n+\t    }\n+\t  while (1);\n+\t}\n+      while (link);\n+\n+      process_insn_depend_be_in_spec (INSN_DEPEND (insn), twin, ts);\n+\n+      for (link = LOG_LINKS (insn); link;)\n+\t{\n+\t  check = XEXP (link, 0);\n+\n+\t  if (BLOCK_FOR_INSN (check) == rec)\n+\t    {\n+\t      delete_back_forw_dep (insn, check);\n+\t      link = LOG_LINKS (insn);\n+\t    }\n+\t  else\n+\t    link = XEXP (link, 1);\n+\t}\n+    }\n+  while (LOG_LINKS (insn));\n+\n+  /* We can't add the dependence between insn and twin earlier because\n+     that would make twin appear in the INSN_DEPEND (insn).  */\n+  while (twins)\n+    {\n+      rtx twin;\n+\n+      twin = XEXP (twins, 0);\n+      calc_priorities (twin);\n+      add_back_forw_dep (twin, insn, REG_DEP_OUTPUT, DEP_OUTPUT);\n+\n+      twin = XEXP (twins, 1);\n+      free_INSN_LIST_node (twins);\n+      twins = twin;      \n+    }\n+}\n+\n+/* Extends and fills with zeros (only the new part) array pointed to by P.  */\n+void *\n+xrecalloc (void *p, size_t new_nmemb, size_t old_nmemb, size_t size)\n+{\n+  gcc_assert (new_nmemb >= old_nmemb);\n+  p = XRESIZEVAR (void, p, new_nmemb * size);\n+  memset (((char *) p) + old_nmemb * size, 0, (new_nmemb - old_nmemb) * size);\n+  return p;\n+}\n+\n+/* Return the probability of speculation success for the speculation\n+   status DS.  */\n+static dw_t\n+dep_weak (ds_t ds)\n+{\n+  ds_t res = 1, dt;\n+  int n = 0;\n+\n+  dt = FIRST_SPEC_TYPE;\n+  do\n+    {\n+      if (ds & dt)\n+\t{\n+\t  res *= (ds_t) get_dep_weak (ds, dt);\n+\t  n++;\n+\t}\n+\n+      if (dt == LAST_SPEC_TYPE)\n+\tbreak;\n+      dt <<= SPEC_TYPE_SHIFT;\n+    }\n+  while (1);\n+\n+  gcc_assert (n);\n+  while (--n)\n+    res /= MAX_DEP_WEAK;\n+\n+  if (res < MIN_DEP_WEAK)\n+    res = MIN_DEP_WEAK;\n+\n+  gcc_assert (res <= MAX_DEP_WEAK);\n+\n+  return (dw_t) res;\n+}\n+\n+/* Helper function.\n+   Find fallthru edge from PRED.  */\n+static edge\n+find_fallthru_edge (basic_block pred)\n+{\n+  edge e;\n+  edge_iterator ei;\n+  basic_block succ;\n+\n+  succ = pred->next_bb;\n+  gcc_assert (succ->prev_bb == pred);\n+\n+  if (EDGE_COUNT (pred->succs) <= EDGE_COUNT (succ->preds))\n+    {\n+      FOR_EACH_EDGE (e, ei, pred->succs)\n+\tif (e->flags & EDGE_FALLTHRU)\n+\t  {\n+\t    gcc_assert (e->dest == succ);\n+\t    return e;\n+\t  }\n+    }\n+  else\n+    {\n+      FOR_EACH_EDGE (e, ei, succ->preds)\n+\tif (e->flags & EDGE_FALLTHRU)\n+\t  {\n+\t    gcc_assert (e->src == pred);\n+\t    return e;\n+\t  }\n+    }\n+\n+  return NULL;\n+}\n+\n+/* Initialize BEFORE_RECOVERY variable.  */\n+static void\n+init_before_recovery (void)\n+{\n+  basic_block last;\n+  edge e;\n+\n+  last = EXIT_BLOCK_PTR->prev_bb;\n+  e = find_fallthru_edge (last);\n+\n+  if (e)\n+    {\n+      /* We create two basic blocks: \n+         1. Single instruction block is inserted right after E->SRC\n+         and has jump to \n+         2. Empty block right before EXIT_BLOCK.\n+         Between these two blocks recovery blocks will be emitted.  */\n+\n+      basic_block single, empty;\n+      rtx x, label;\n+\n+      single = create_empty_bb (last);\n+      empty = create_empty_bb (single);            \n+\n+      single->count = last->count;     \n+      empty->count = last->count;\n+      single->frequency = last->frequency;\n+      empty->frequency = last->frequency;\n+      BB_COPY_PARTITION (single, last);\n+      BB_COPY_PARTITION (empty, last);\n+\n+      redirect_edge_succ (e, single);\n+      make_single_succ_edge (single, empty, 0);\n+      make_single_succ_edge (empty, EXIT_BLOCK_PTR,\n+\t\t\t     EDGE_FALLTHRU | EDGE_CAN_FALLTHRU);\n+\n+      label = block_label (empty);\n+      x = emit_jump_insn_after (gen_jump (label), BB_END (single));\n+      JUMP_LABEL (x) = label;\n+      LABEL_NUSES (label)++;\n+      extend_global (x);\n+          \n+      emit_barrier_after (x);\n+\n+      add_block (empty, 0);\n+      add_block (single, 0);\n+\n+      before_recovery = single;\n+\n+      if (sched_verbose >= 2 && spec_info->dump)\n+        fprintf (spec_info->dump,\n+\t\t \";;\\t\\tFixed fallthru to EXIT : %d->>%d->%d->>EXIT\\n\", \n+                 last->index, single->index, empty->index);      \n+    }\n+  else\n+    before_recovery = last;\n+}\n+\n+/* Returns new recovery block.  */\n+static basic_block\n+create_recovery_block (void)\n+{\n+  rtx label;\n+  basic_block rec;\n+  \n+  added_recovery_block_p = true;\n+\n+  if (!before_recovery)\n+    init_before_recovery ();\n+ \n+  label = gen_label_rtx ();\n+  gcc_assert (BARRIER_P (NEXT_INSN (BB_END (before_recovery))));\n+  label = emit_label_after (label, NEXT_INSN (BB_END (before_recovery)));\n+\n+  rec = create_basic_block (label, label, before_recovery); \n+  emit_barrier_after (BB_END (rec));\n+\n+  if (BB_PARTITION (before_recovery) != BB_UNPARTITIONED)\n+    BB_SET_PARTITION (rec, BB_COLD_PARTITION);\n+  \n+  if (sched_verbose && spec_info->dump)    \n+    fprintf (spec_info->dump, \";;\\t\\tGenerated recovery block rec%d\\n\",\n+             rec->index);\n+\n+  before_recovery = rec;\n+\n+  return rec;\n+}\n+\n+/* This function creates recovery code for INSN.  If MUTATE_P is nonzero,\n+   INSN is a simple check, that should be converted to branchy one.  */\n+static void\n+create_check_block_twin (rtx insn, bool mutate_p)\n+{\n+  basic_block rec;\n+  rtx label, check, twin, link;\n+  ds_t fs;\n+\n+  gcc_assert (ORIG_PAT (insn)\n+\t      && (!mutate_p \n+\t\t  || (RECOVERY_BLOCK (insn) == EXIT_BLOCK_PTR\n+\t\t      && !(TODO_SPEC (insn) & SPECULATIVE))));\n+\n+  /* Create recovery block.  */\n+  if (mutate_p || targetm.sched.needs_block_p (insn))\n+    {\n+      rec = create_recovery_block ();\n+      label = BB_HEAD (rec);\n+    }\n+  else\n+    {\n+      rec = EXIT_BLOCK_PTR;\n+      label = 0;\n+    }\n+\n+  /* Emit CHECK.  */\n+  check = targetm.sched.gen_check (insn, label, mutate_p);\n+\n+  if (rec != EXIT_BLOCK_PTR)\n+    {\n+      /* To have mem_reg alive at the beginning of second_bb,\n+\t we emit check BEFORE insn, so insn after splitting \n+\t insn will be at the beginning of second_bb, which will\n+\t provide us with the correct life information.  */\n+      check = emit_jump_insn_before (check, insn);\n+      JUMP_LABEL (check) = label;\n+      LABEL_NUSES (label)++;\n+    }\n+  else\n+    check = emit_insn_before (check, insn);\n+\n+  /* Extend data structures.  */\n+  extend_all (check);\n+  RECOVERY_BLOCK (check) = rec;\n+\n+  if (sched_verbose && spec_info->dump)\n+    fprintf (spec_info->dump, \";;\\t\\tGenerated check insn : %s\\n\",\n+             (*current_sched_info->print_insn) (check, 0));\n+\n+  gcc_assert (ORIG_PAT (insn));\n+\n+  /* Initialize TWIN (twin is a dublicate of original instruction\n+     in the recovery block).  */\n+  if (rec != EXIT_BLOCK_PTR)\n+    {\n+      rtx link;\n+\n+      for (link = RESOLVED_DEPS (insn); link; link = XEXP (link, 1))    \n+\tif (DEP_STATUS (link) & DEP_OUTPUT)\n+\t  {\n+\t    RESOLVED_DEPS (check) = \n+\t      alloc_DEPS_LIST (XEXP (link, 0), RESOLVED_DEPS (check), DEP_TRUE);\n+\t    PUT_REG_NOTE_KIND (RESOLVED_DEPS (check), REG_DEP_TRUE);\n+\t  }\n+\n+      twin = emit_insn_after (ORIG_PAT (insn), BB_END (rec));\n+      extend_global (twin);\n+\n+      if (sched_verbose && spec_info->dump)\n+\t/* INSN_BB (insn) isn't determined for twin insns yet.\n+\t   So we can't use current_sched_info->print_insn.  */\n+\tfprintf (spec_info->dump, \";;\\t\\tGenerated twin insn : %d/rec%d\\n\",\n+\t\t INSN_UID (twin), rec->index);\n+    }\n+  else\n+    {\n+      ORIG_PAT (check) = ORIG_PAT (insn);\n+      HAS_INTERNAL_DEP (check) = 1;\n+      twin = check;\n+      /* ??? We probably should change all OUTPUT dependencies to\n+\t (TRUE | OUTPUT).  */\n+    }\n+\n+  RESOLVED_DEPS (twin) = copy_DEPS_LIST_list (RESOLVED_DEPS (insn));  \n+\n+  if (rec != EXIT_BLOCK_PTR)\n+    /* In case of branchy check, fix CFG.  */\n+    {\n+      basic_block first_bb, second_bb;\n+      rtx jump;\n+      edge e;\n+      int edge_flags;\n+\n+      first_bb = BLOCK_FOR_INSN (check);\n+      e = split_block (first_bb, check);\n+      /* split_block emits note if *check == BB_END.  Probably it \n+\t is better to rip that note off.  */\n+      gcc_assert (e->src == first_bb);\n+      second_bb = e->dest;\n+\n+      /* This is fixing of incoming edge.  */\n+      /* ??? Which other flags should be specified?  */      \n+      if (BB_PARTITION (first_bb) != BB_PARTITION (rec))\n+\t/* Partition type is the same, if it is \"unpartitioned\".  */\n+\tedge_flags = EDGE_CROSSING;\n+      else\n+\tedge_flags = 0;\n+      \n+      e = make_edge (first_bb, rec, edge_flags);\n+\n+      add_block (second_bb, first_bb);\n+      \n+      gcc_assert (NOTE_INSN_BASIC_BLOCK_P (BB_HEAD (second_bb)));\n+      label = block_label (second_bb);\n+      jump = emit_jump_insn_after (gen_jump (label), BB_END (rec));\n+      JUMP_LABEL (jump) = label;\n+      LABEL_NUSES (label)++;\n+      extend_global (jump);\n+\n+      if (BB_PARTITION (second_bb) != BB_PARTITION (rec))\n+\t/* Partition type is the same, if it is \"unpartitioned\".  */\n+\t{\n+\t  /* Rewritten from cfgrtl.c.  */\n+\t  if (flag_reorder_blocks_and_partition\n+\t      && targetm.have_named_sections\n+\t      /*&& !any_condjump_p (jump)*/)\n+\t    /* any_condjump_p (jump) == false.\n+\t       We don't need the same note for the check because\n+\t       any_condjump_p (check) == true.  */\n+\t    {\n+\t      REG_NOTES (jump) = gen_rtx_EXPR_LIST (REG_CROSSING_JUMP,\n+\t\t\t\t\t\t    NULL_RTX,\n+\t\t\t\t\t\t    REG_NOTES (jump));\n+\t    }\n+\t  edge_flags = EDGE_CROSSING;\n+\t}\n+      else\n+\tedge_flags = 0;  \n+      \n+      make_single_succ_edge (rec, second_bb, edge_flags);  \n+      \n+      add_block (rec, EXIT_BLOCK_PTR);\n+    }\n+\n+  /* Move backward dependences from INSN to CHECK and \n+     move forward dependences from INSN to TWIN.  */\n+  for (link = LOG_LINKS (insn); link; link = XEXP (link, 1))\n+    {\n+      ds_t ds;\n+\n+      /* If BEGIN_DATA: [insn ~~TRUE~~> producer]:\n+\t check --TRUE--> producer  ??? or ANTI ???\n+\t twin  --TRUE--> producer\n+\t twin  --ANTI--> check\n+\t \n+\t If BEGIN_CONTROL: [insn ~~ANTI~~> producer]:\n+\t check --ANTI--> producer\n+\t twin  --ANTI--> producer\n+\t twin  --ANTI--> check\n+\n+\t If BE_IN_SPEC: [insn ~~TRUE~~> producer]:\n+\t check ~~TRUE~~> producer\n+\t twin  ~~TRUE~~> producer\n+\t twin  --ANTI--> check  */\t      \t  \n+\n+      ds = DEP_STATUS (link);\n+\n+      if (ds & BEGIN_SPEC)\n+\t{\n+\t  gcc_assert (!mutate_p);\n+\t  ds &= ~BEGIN_SPEC;\n+\t}\n+\n+      if (rec != EXIT_BLOCK_PTR)\n+\t{\n+\t  add_back_forw_dep (check, XEXP (link, 0), REG_NOTE_KIND (link), ds);\n+\t  add_back_forw_dep (twin, XEXP (link, 0), REG_NOTE_KIND (link), ds);\n+\t}    \n+      else\n+\tadd_back_forw_dep (check, XEXP (link, 0), REG_NOTE_KIND (link), ds);\n+    }\n+\n+  for (link = LOG_LINKS (insn); link;)\n+    if ((DEP_STATUS (link) & BEGIN_SPEC)\n+\t|| mutate_p)\n+      /* We can delete this dep only if we totally overcome it with\n+\t BEGIN_SPECULATION.  */\n+      {\n+        delete_back_forw_dep (insn, XEXP (link, 0));\n+        link = LOG_LINKS (insn);\n+      }\n+    else\n+      link = XEXP (link, 1);    \n+\n+  fs = 0;\n+\n+  /* Fields (DONE_SPEC (x) & BEGIN_SPEC) and CHECK_SPEC (x) are set only\n+     here.  */\n+  \n+  gcc_assert (!DONE_SPEC (insn));\n+  \n+  if (!mutate_p)\n+    { \n+      ds_t ts = TODO_SPEC (insn);\n+\n+      DONE_SPEC (insn) = ts & BEGIN_SPEC;\n+      CHECK_SPEC (check) = ts & BEGIN_SPEC;\n+\n+      if (ts & BEGIN_DATA)\n+\tfs = set_dep_weak (fs, BE_IN_DATA, get_dep_weak (ts, BEGIN_DATA));\n+      if (ts & BEGIN_CONTROL)\n+\tfs = set_dep_weak (fs, BE_IN_CONTROL, get_dep_weak (ts, BEGIN_CONTROL));\n+    }\n+  else\n+    CHECK_SPEC (check) = CHECK_SPEC (insn);\n+\n+  /* Future speculations: call the helper.  */\n+  process_insn_depend_be_in_spec (INSN_DEPEND (insn), twin, fs);\n+\n+  if (rec != EXIT_BLOCK_PTR)\n+    {\n+      /* Which types of dependencies should we use here is,\n+\t generally, machine-dependent question...  But, for now,\n+\t it is not.  */\n+\n+      if (!mutate_p)\n+\t{\n+\t  add_back_forw_dep (check, insn, REG_DEP_TRUE, DEP_TRUE);\n+\t  add_back_forw_dep (twin, insn, REG_DEP_OUTPUT, DEP_OUTPUT);\n+\t}\n+      else\n+\t{\n+\t  if (spec_info->dump)    \n+\t    fprintf (spec_info->dump, \";;\\t\\tRemoved simple check : %s\\n\",\n+\t\t     (*current_sched_info->print_insn) (insn, 0));\n+\n+\t  for (link = INSN_DEPEND (insn); link; link = INSN_DEPEND (insn))\n+\t    delete_back_forw_dep (XEXP (link, 0), insn);\n+\n+\t  if (QUEUE_INDEX (insn) != QUEUE_NOWHERE)\n+\t    try_ready (check);\n+\n+\t  sched_remove_insn (insn);\n+\t}\n+\n+      add_back_forw_dep (twin, check, REG_DEP_ANTI, DEP_ANTI);\n+    }\n+  else\n+    add_back_forw_dep (check, insn, REG_DEP_TRUE, DEP_TRUE | DEP_OUTPUT);\n+\n+  if (!mutate_p)\n+    /* Fix priorities.  If MUTATE_P is nonzero, this is not neccessary,\n+       because it'll be done later in add_to_speculative_block.  */\n+    {\n+      clear_priorities (twin);\n+      calc_priorities (twin);\n+    }\n+}\n+\n+/* Removes dependency between instructions in the recovery block REC\n+   and usual region instructions.  It keeps inner dependences so it\n+   won't be neccessary to recompute them.  */\n+static void\n+fix_recovery_deps (basic_block rec)\n+{\n+  rtx note, insn, link, jump, ready_list = 0;\n+  bitmap_head in_ready;\n+\n+  bitmap_initialize (&in_ready, 0);\n+  \n+  /* NOTE - a basic block note.  */\n+  note = NEXT_INSN (BB_HEAD (rec));\n+  gcc_assert (NOTE_INSN_BASIC_BLOCK_P (note));\n+  insn = BB_END (rec);\n+  gcc_assert (JUMP_P (insn));\n+  insn = PREV_INSN (insn);\n+\n+  do\n+    {    \n+      for (link = INSN_DEPEND (insn); link;)\n+\t{\n+\t  rtx consumer;\n+\n+\t  consumer = XEXP (link, 0);\n+\n+\t  if (BLOCK_FOR_INSN (consumer) != rec)\n+\t    {\n+\t      delete_back_forw_dep (consumer, insn);\n+\n+\t      if (!bitmap_bit_p (&in_ready, INSN_LUID (consumer)))\n+\t\t{\n+\t\t  ready_list = alloc_INSN_LIST (consumer, ready_list);\n+\t\t  bitmap_set_bit (&in_ready, INSN_LUID (consumer));\n+\t\t}\n+\t      \n+\t      link = INSN_DEPEND (insn);\n+\t    }\n+\t  else\n+\t    {\n+\t      gcc_assert ((DEP_STATUS (link) & DEP_TYPES) == DEP_TRUE);\n+\n+\t      link = XEXP (link, 1);\n+\t    }\n+\t}\n+      \n+      insn = PREV_INSN (insn);\n+    }\n+  while (insn != note);\n+\n+  bitmap_clear (&in_ready);\n+\n+  /* Try to add instructions to the ready or queue list.  */\n+  for (link = ready_list; link; link = XEXP (link, 1))\n+    try_ready (XEXP (link, 0));\n+  free_INSN_LIST_list (&ready_list);\n+\n+  /* Fixing jump's dependences.  */\n+  insn = BB_HEAD (rec);\n+  jump = BB_END (rec);\n+      \n+  gcc_assert (LABEL_P (insn));\n+  insn = NEXT_INSN (insn);\n+  \n+  gcc_assert (NOTE_INSN_BASIC_BLOCK_P (insn));\n+  add_jump_dependencies (insn, jump);\n+}\n+\n+/* The function saves line notes at the beginning of block B.  */\n+static void\n+associate_line_notes_with_blocks (basic_block b)\n+{\n+  rtx line;\n+\n+  for (line = BB_HEAD (b); line; line = PREV_INSN (line))\n+    if (NOTE_P (line) && NOTE_LINE_NUMBER (line) > 0)\n+      {\n+        line_note_head[b->index] = line;\n+        break;\n+      }\n+  /* Do a forward search as well, since we won't get to see the first\n+     notes in a basic block.  */\n+  for (line = BB_HEAD (b); line; line = NEXT_INSN (line))\n+    {\n+      if (INSN_P (line))\n+        break;\n+      if (NOTE_P (line) && NOTE_LINE_NUMBER (line) > 0)\n+        line_note_head[b->index] = line;\n+    }\n+}\n+\n+/* Changes pattern of the INSN to NEW_PAT.  */\n+static void\n+change_pattern (rtx insn, rtx new_pat)\n+{\n+  int t;\n+\n+  t = validate_change (insn, &PATTERN (insn), new_pat, 0);\n+  gcc_assert (t);\n+  /* Invalidate INSN_COST, so it'll be recalculated.  */\n+  INSN_COST (insn) = -1;\n+  /* Invalidate INSN_TICK, so it'll be recalculated.  */\n+  INSN_TICK (insn) = INVALID_TICK;\n+  dfa_clear_single_insn_cache (insn);\n+}\n+\n+\n+/* -1 - can't speculate,\n+   0 - for speculation with REQUEST mode it is OK to use\n+   current instruction pattern,\n+   1 - need to change pattern for *NEW_PAT to be speculative.  */\n+static int\n+speculate_insn (rtx insn, ds_t request, rtx *new_pat)\n+{\n+  gcc_assert (current_sched_info->flags & DO_SPECULATION\n+              && (request & SPECULATIVE));\n+\n+  if (!NONJUMP_INSN_P (insn)\n+      || HAS_INTERNAL_DEP (insn)\n+      || SCHED_GROUP_P (insn)\n+      || side_effects_p (PATTERN (insn))\n+      || (request & spec_info->mask) != request)    \n+    return -1;\n+  \n+  gcc_assert (!RECOVERY_BLOCK (insn));\n+\n+  if (request & BE_IN_SPEC)\n+    {            \n+      if (may_trap_p (PATTERN (insn)))\n+        return -1;\n+      \n+      if (!(request & BEGIN_SPEC))\n+        return 0;\n+    }\n+\n+  return targetm.sched.speculate_insn (insn, request & BEGIN_SPEC, new_pat);\n+}\n+\n+/* Print some information about block BB, which starts with HEAD and\n+   ends with TAIL, before scheduling it.\n+   I is zero, if scheduler is about to start with the fresh ebb.  */\n+static void\n+dump_new_block_header (int i, basic_block bb, rtx head, rtx tail)\n+{\n+  if (!i)\n+    fprintf (sched_dump,\n+\t     \";;   ======================================================\\n\");\n+  else\n+    fprintf (sched_dump,\n+\t     \";;   =====================ADVANCING TO=====================\\n\");\n+  fprintf (sched_dump,\n+\t   \";;   -- basic block %d from %d to %d -- %s reload\\n\",\n+\t   bb->index, INSN_UID (head), INSN_UID (tail),\n+\t   (reload_completed ? \"after\" : \"before\"));\n+  fprintf (sched_dump,\n+\t   \";;   ======================================================\\n\");\n+  fprintf (sched_dump, \"\\n\");\n+}\n+\n+/* Unlink basic block notes and labels and saves them, so they\n+   can be easily restored.  We unlink basic block notes in EBB to\n+   provide back-compatability with the previous code, as target backends\n+   assume, that there'll be only instructions between\n+   current_sched_info->{head and tail}.  We restore these notes as soon\n+   as we can.\n+   FIRST (LAST) is the first (last) basic block in the ebb.\n+   NB: In usual case (FIRST == LAST) nothing is really done.  */\n+void\n+unlink_bb_notes (basic_block first, basic_block last)\n+{\n+  /* We DON'T unlink basic block notes of the first block in the ebb.  */\n+  if (first == last)\n+    return;\n+\n+  bb_header = xmalloc (last_basic_block * sizeof (*bb_header));\n+\n+  /* Make a sentinel.  */\n+  if (last->next_bb != EXIT_BLOCK_PTR)\n+    bb_header[last->next_bb->index] = 0;\n+\n+  first = first->next_bb;\n+  do\n+    {\n+      rtx prev, label, note, next;\n+\n+      label = BB_HEAD (last);\n+      if (LABEL_P (label))\n+\tnote = NEXT_INSN (label);\n+      else\n+\tnote = label;      \n+      gcc_assert (NOTE_INSN_BASIC_BLOCK_P (note));\n+\n+      prev = PREV_INSN (label);\n+      next = NEXT_INSN (note);\n+      gcc_assert (prev && next);\n+\n+      NEXT_INSN (prev) = next;\n+      PREV_INSN (next) = prev;\n+\n+      bb_header[last->index] = label;\n+\n+      if (last == first)\n+\tbreak;\n+      \n+      last = last->prev_bb;\n+    }\n+  while (1);\n+}\n+\n+/* Restore basic block notes.\n+   FIRST is the first basic block in the ebb.  */\n+static void\n+restore_bb_notes (basic_block first)\n+{\n+  if (!bb_header)\n+    return;\n+\n+  /* We DON'T unlink basic block notes of the first block in the ebb.  */\n+  first = first->next_bb;  \n+  /* Remember: FIRST is actually a second basic block in the ebb.  */\n+\n+  while (first != EXIT_BLOCK_PTR\n+\t && bb_header[first->index])\n+    {\n+      rtx prev, label, note, next;\n+      \n+      label = bb_header[first->index];\n+      prev = PREV_INSN (label);\n+      next = NEXT_INSN (prev);\n+\n+      if (LABEL_P (label))\n+\tnote = NEXT_INSN (label);\n+      else\n+\tnote = label;      \n+      gcc_assert (NOTE_INSN_BASIC_BLOCK_P (note));\n+\n+      bb_header[first->index] = 0;\n+\n+      NEXT_INSN (prev) = label;\n+      NEXT_INSN (note) = next;\n+      PREV_INSN (next) = note;\n+      \n+      first = first->next_bb;\n+    }\n+\n+  free (bb_header);\n+  bb_header = 0;\n+}\n+\n+/* Extend per basic block data structures of the scheduler.\n+   If BB is NULL, initialize structures for the whole CFG.\n+   Otherwise, initialize them for the just created BB.  */\n+static void\n+extend_bb (basic_block bb)\n+{\n+  rtx insn;\n+\n+  if (write_symbols != NO_DEBUG)\n+    {\n+      /* Save-line-note-head:\n+         Determine the line-number at the start of each basic block.\n+         This must be computed and saved now, because after a basic block's\n+         predecessor has been scheduled, it is impossible to accurately\n+         determine the correct line number for the first insn of the block.  */\n+      line_note_head = xrecalloc (line_note_head, last_basic_block, \n+\t\t\t\t  old_last_basic_block,\n+\t\t\t\t  sizeof (*line_note_head));\n+\n+      if (bb)\n+\tassociate_line_notes_with_blocks (bb);\n+      else\n+\tFOR_EACH_BB (bb)\n+\t  associate_line_notes_with_blocks (bb);\n+    }        \n+  \n+  old_last_basic_block = last_basic_block;\n+\n+  if (current_sched_info->flags & USE_GLAT)\n+    {\n+      glat_start = xrealloc (glat_start,\n+                             last_basic_block * sizeof (*glat_start));\n+      glat_end = xrealloc (glat_end, last_basic_block * sizeof (*glat_end));\n+    }\n+\n+  /* The following is done to keep current_sched_info->next_tail non null.  */\n+\n+  insn = BB_END (EXIT_BLOCK_PTR->prev_bb);\n+  if (NEXT_INSN (insn) == 0\n+      || (!NOTE_P (insn)\n+\t  && !LABEL_P (insn)\n+\t  /* Don't emit a NOTE if it would end up before a BARRIER.  */\n+\t  && !BARRIER_P (NEXT_INSN (insn))))\n+    {\n+      emit_note_after (NOTE_INSN_DELETED, insn);\n+      /* Make insn to appear outside BB.  */\n+      BB_END (EXIT_BLOCK_PTR->prev_bb) = insn;\n+    }\n+}\n+\n+/* Add a basic block BB to extended basic block EBB.\n+   If EBB is EXIT_BLOCK_PTR, then BB is recovery block.\n+   If EBB is NULL, then BB should be a new region.  */\n+void\n+add_block (basic_block bb, basic_block ebb)\n+{\n+  gcc_assert (current_sched_info->flags & DETACH_LIFE_INFO\n+\t      && bb->il.rtl->global_live_at_start == 0\n+\t      && bb->il.rtl->global_live_at_end == 0);\n+\n+  extend_bb (bb);\n+\n+  glat_start[bb->index] = 0;\n+  glat_end[bb->index] = 0;\n+\n+  if (current_sched_info->add_block)\n+    /* This changes only data structures of the front-end.  */\n+    current_sched_info->add_block (bb, ebb);\n+}\n+\n+/* Helper function.\n+   Fix CFG after both in- and inter-block movement of\n+   control_flow_insn_p JUMP.  */\n+static void\n+fix_jump_move (rtx jump)\n+{\n+  basic_block bb, jump_bb, jump_bb_next;\n+\n+  bb = BLOCK_FOR_INSN (PREV_INSN (jump));\n+  jump_bb = BLOCK_FOR_INSN (jump);\n+  jump_bb_next = jump_bb->next_bb;\n+\n+  gcc_assert (current_sched_info->flags & SCHED_EBB\n+\t      || (RECOVERY_BLOCK (jump)\n+\t\t  && RECOVERY_BLOCK (jump) != EXIT_BLOCK_PTR));\n+  \n+  if (!NOTE_INSN_BASIC_BLOCK_P (BB_END (jump_bb_next)))\n+    /* if jump_bb_next is not empty.  */\n+    BB_END (jump_bb) = BB_END (jump_bb_next);\n+\n+  if (BB_END (bb) != PREV_INSN (jump))\n+    /* Then there are instruction after jump that should be placed\n+       to jump_bb_next.  */\n+    BB_END (jump_bb_next) = BB_END (bb);\n+  else\n+    /* Otherwise jump_bb_next is empty.  */\n+    BB_END (jump_bb_next) = NEXT_INSN (BB_HEAD (jump_bb_next));\n+\n+  /* To make assertion in move_insn happy.  */\n+  BB_END (bb) = PREV_INSN (jump);\n+\n+  update_bb_for_insn (jump_bb_next);\n+}\n+\n+/* Fix CFG after interblock movement of control_flow_insn_p JUMP.  */\n+static void\n+move_block_after_check (rtx jump)\n+{\n+  basic_block bb, jump_bb, jump_bb_next;\n+  VEC(edge,gc) *t;\n+\n+  bb = BLOCK_FOR_INSN (PREV_INSN (jump));\n+  jump_bb = BLOCK_FOR_INSN (jump);\n+  jump_bb_next = jump_bb->next_bb;\n+  \n+  update_bb_for_insn (jump_bb);\n+  \n+  gcc_assert (RECOVERY_BLOCK (jump)\n+\t      || RECOVERY_BLOCK (BB_END (jump_bb_next)));\n+\n+  unlink_block (jump_bb_next);\n+  link_block (jump_bb_next, bb);\n+\n+  t = bb->succs;\n+  bb->succs = 0;\n+  move_succs (&(jump_bb->succs), bb);\n+  move_succs (&(jump_bb_next->succs), jump_bb);\n+  move_succs (&t, jump_bb_next);\n+  \n+  if (current_sched_info->fix_recovery_cfg)\n+    current_sched_info->fix_recovery_cfg \n+      (bb->index, jump_bb->index, jump_bb_next->index);\n+}\n+\n+/* Helper function for move_block_after_check.\n+   This functions attaches edge vector pointed to by SUCCSP to\n+   block TO.  */\n+static void\n+move_succs (VEC(edge,gc) **succsp, basic_block to)\n+{\n+  edge e;\n+  edge_iterator ei;\n+\n+  gcc_assert (to->succs == 0);\n+\n+  to->succs = *succsp;\n+\n+  FOR_EACH_EDGE (e, ei, to->succs)\n+    e->src = to;\n+\n+  *succsp = 0;\n+}\n+\n+/* Initialize GLAT (global_live_at_{start, end}) structures.\n+   GLAT structures are used to substitute global_live_{start, end}\n+   regsets during scheduling.  This is neccessary to use such functions as\n+   split_block (), as they assume consistancy of register live information.  */\n+static void\n+init_glat (void)\n+{\n+  basic_block bb;\n+\n+  FOR_ALL_BB (bb)\n+    init_glat1 (bb);\n+}\n+\n+/* Helper function for init_glat.  */\n+static void\n+init_glat1 (basic_block bb)\n+{\n+  gcc_assert (bb->il.rtl->global_live_at_start != 0\n+\t      && bb->il.rtl->global_live_at_end != 0);\n+\n+  glat_start[bb->index] = bb->il.rtl->global_live_at_start;\n+  glat_end[bb->index] = bb->il.rtl->global_live_at_end;\n+  \n+  if (current_sched_info->flags & DETACH_LIFE_INFO)\n+    {\n+      bb->il.rtl->global_live_at_start = 0;\n+      bb->il.rtl->global_live_at_end = 0;\n+    }\n+}\n+\n+/* Attach reg_live_info back to basic blocks.\n+   Also save regsets, that should not have been changed during scheduling,\n+   for checking purposes (see check_reg_live).  */\n+void\n+attach_life_info (void)\n+{\n+  basic_block bb;\n+\n+  FOR_ALL_BB (bb)\n+    attach_life_info1 (bb);\n+}\n+\n+/* Helper function for attach_life_info.  */\n+static void\n+attach_life_info1 (basic_block bb)\n+{\n+  gcc_assert (bb->il.rtl->global_live_at_start == 0\n+\t      && bb->il.rtl->global_live_at_end == 0);\n+\n+  if (glat_start[bb->index])\n+    {\n+      gcc_assert (glat_end[bb->index]);    \n+\n+      bb->il.rtl->global_live_at_start = glat_start[bb->index];\n+      bb->il.rtl->global_live_at_end = glat_end[bb->index];\n+\n+      /* Make them NULL, so they won't be freed in free_glat.  */\n+      glat_start[bb->index] = 0;\n+      glat_end[bb->index] = 0;\n+\n+#ifdef ENABLE_CHECKING\n+      if (bb->index < NUM_FIXED_BLOCKS\n+\t  || current_sched_info->region_head_or_leaf_p (bb, 0))\n+\t{\n+\t  glat_start[bb->index] = ALLOC_REG_SET (&reg_obstack);\n+\t  COPY_REG_SET (glat_start[bb->index],\n+\t\t\tbb->il.rtl->global_live_at_start);\n+\t}\n+\n+      if (bb->index < NUM_FIXED_BLOCKS\n+\t  || current_sched_info->region_head_or_leaf_p (bb, 1))\n+\t{       \n+\t  glat_end[bb->index] = ALLOC_REG_SET (&reg_obstack);\n+\t  COPY_REG_SET (glat_end[bb->index], bb->il.rtl->global_live_at_end);\n+\t}\n+#endif\n+    }\n+  else\n+    {\n+      gcc_assert (!glat_end[bb->index]);\n+\n+      bb->il.rtl->global_live_at_start = ALLOC_REG_SET (&reg_obstack);\n+      bb->il.rtl->global_live_at_end = ALLOC_REG_SET (&reg_obstack);\n+    }\n+}\n+\n+/* Free GLAT information.  */\n+static void\n+free_glat (void)\n+{\n+#ifdef ENABLE_CHECKING\n+  if (current_sched_info->flags & DETACH_LIFE_INFO)\n+    {\n+      basic_block bb;\n+\n+      FOR_ALL_BB (bb)\n+\t{\n+\t  if (glat_start[bb->index])\n+\t    FREE_REG_SET (glat_start[bb->index]);\n+\t  if (glat_end[bb->index])\n+\t    FREE_REG_SET (glat_end[bb->index]);\n+\t}\n+    }\n+#endif\n+\n+  free (glat_start);\n+  free (glat_end);\n+}\n+\n+/* Remove INSN from the instruction stream.\n+   INSN should have any dependencies.  */\n+static void\n+sched_remove_insn (rtx insn)\n+{\n+  change_queue_index (insn, QUEUE_NOWHERE);\n+  current_sched_info->add_remove_insn (insn, 1);\n+  remove_insn (insn);\n+}\n+\n+/* Clear priorities of all instructions, that are\n+   forward dependent on INSN.  */\n+static void\n+clear_priorities (rtx insn)\n+{\n+  rtx link;\n+\n+  for (link = LOG_LINKS (insn); link; link = XEXP (link, 1))\n+    {\n+      rtx pro;\n+\n+      pro = XEXP (link, 0);\n+      if (INSN_PRIORITY_KNOWN (pro))\n+\t{\n+\t  INSN_PRIORITY_KNOWN (pro) = 0;\n+\t  clear_priorities (pro);\n+\t}\n+    }\n+}\n+\n+/* Recompute priorities of instructions, whose priorities might have been\n+   changed due to changes in INSN.  */\n+static void\n+calc_priorities (rtx insn)\n+{\n+  rtx link;\n+\n+  for (link = LOG_LINKS (insn); link; link = XEXP (link, 1))\n+    {\n+      rtx pro;\n+\n+      pro = XEXP (link, 0);\n+      if (!INSN_PRIORITY_KNOWN (pro))\n+\t{\n+\t  priority (pro);\n+\t  calc_priorities (pro);\n+\t}\n+    }\n+}\n+\n+\n+/* Add dependences between JUMP and other instructions in the recovery\n+   block.  INSN is the first insn the recovery block.  */\n+static void\n+add_jump_dependencies (rtx insn, rtx jump)\n+{\n+  do\n+    {\n+      insn = NEXT_INSN (insn);\n+      if (insn == jump)\n+\tbreak;\n+      \n+      if (!INSN_DEPEND (insn))\t    \n+\tadd_back_forw_dep (jump, insn, REG_DEP_ANTI, DEP_ANTI);\n+    }\n+  while (1);\n+  gcc_assert (LOG_LINKS (jump));\n+}\n+\n+/* Return the NOTE_INSN_BASIC_BLOCK of BB.  */\n+static rtx\n+bb_note (basic_block bb)\n+{\n+  rtx note;\n+\n+  note = BB_HEAD (bb);\n+  if (LABEL_P (note))\n+    note = NEXT_INSN (note);\n+\n+  gcc_assert (NOTE_INSN_BASIC_BLOCK_P (note));\n+  return note;\n+}\n+\n+#ifdef ENABLE_CHECKING\n+extern void debug_spec_status (ds_t);\n+\n+/* Dump information about the dependence status S.  */\n+void\n+debug_spec_status (ds_t s)\n+{\n+  FILE *f = stderr;\n+\n+  if (s & BEGIN_DATA)\n+    fprintf (f, \"BEGIN_DATA: %d; \", get_dep_weak (s, BEGIN_DATA));\n+  if (s & BE_IN_DATA)\n+    fprintf (f, \"BE_IN_DATA: %d; \", get_dep_weak (s, BE_IN_DATA));\n+  if (s & BEGIN_CONTROL)\n+    fprintf (f, \"BEGIN_CONTROL: %d; \", get_dep_weak (s, BEGIN_CONTROL));\n+  if (s & BE_IN_CONTROL)\n+    fprintf (f, \"BE_IN_CONTROL: %d; \", get_dep_weak (s, BE_IN_CONTROL));\n+\n+  if (s & HARD_DEP)\n+    fprintf (f, \"HARD_DEP; \");\n+\n+  if (s & DEP_TRUE)\n+    fprintf (f, \"DEP_TRUE; \");\n+  if (s & DEP_ANTI)\n+    fprintf (f, \"DEP_ANTI; \");\n+  if (s & DEP_OUTPUT)\n+    fprintf (f, \"DEP_OUTPUT; \");\n+\n+  fprintf (f, \"\\n\");\n+}\n+\n+/* Helper function for check_cfg.\n+   Return non-zero, if edge vector pointed to by EL has edge with TYPE in\n+   its flags.  */\n+static int\n+has_edge_p (VEC(edge,gc) *el, int type)\n+{\n+  edge e;\n+  edge_iterator ei;\n+\n+  FOR_EACH_EDGE (e, ei, el)\n+    if (e->flags & type)\n+      return 1;\n+  return 0;\n+}\n+\n+/* Check few properties of CFG between HEAD and TAIL.\n+   If HEAD (TAIL) is NULL check from the beginning (till the end) of the\n+   instruction stream.  */\n+static void\n+check_cfg (rtx head, rtx tail)\n+{\n+  rtx next_tail;\n+  basic_block bb = 0;\n+  int not_first = 0, not_last;\n+\n+  if (head == NULL)\n+    head = get_insns ();\n+  if (tail == NULL)\n+    tail = get_last_insn ();\n+  next_tail = NEXT_INSN (tail);\n+\n+  do\n+    {      \n+      not_last = head != tail;        \n+\n+      if (not_first)\n+\tgcc_assert (NEXT_INSN (PREV_INSN (head)) == head);\n+      if (not_last)\n+\tgcc_assert (PREV_INSN (NEXT_INSN (head)) == head);\n+\n+      if (LABEL_P (head) \n+\t  || (NOTE_INSN_BASIC_BLOCK_P (head)\n+\t      && (!not_first\n+\t\t  || (not_first && !LABEL_P (PREV_INSN (head))))))\n+\t{\n+\t  gcc_assert (bb == 0);\t  \n+\t  bb = BLOCK_FOR_INSN (head);\n+\t  if (bb != 0)\n+\t    gcc_assert (BB_HEAD (bb) == head);      \n+\t  else\n+\t    /* This is the case of jump table.  See inside_basic_block_p ().  */\n+\t    gcc_assert (LABEL_P (head) && !inside_basic_block_p (head));\n+\t}\n+\n+      if (bb == 0)\n+\t{\n+\t  gcc_assert (!inside_basic_block_p (head));\n+\t  head = NEXT_INSN (head);\n+\t}\n+      else\n+\t{\n+\t  gcc_assert (inside_basic_block_p (head)\n+\t\t      || NOTE_P (head));\n+\t  gcc_assert (BLOCK_FOR_INSN (head) == bb);\n+\t\n+\t  if (LABEL_P (head))\n+\t    {\n+\t      head = NEXT_INSN (head);\n+\t      gcc_assert (NOTE_INSN_BASIC_BLOCK_P (head));\n+\t    }\n+\t  else\n+\t    {\n+\t      if (control_flow_insn_p (head))\n+\t\t{\n+\t\t  gcc_assert (BB_END (bb) == head);\n+\t\t  \n+\t\t  if (any_uncondjump_p (head))\n+\t\t    gcc_assert (EDGE_COUNT (bb->succs) == 1\n+\t\t\t\t&& BARRIER_P (NEXT_INSN (head)));\n+\t\t  else if (any_condjump_p (head))\n+\t\t    gcc_assert (EDGE_COUNT (bb->succs) > 1\n+\t\t\t\t&& !BARRIER_P (NEXT_INSN (head)));\n+\t\t}\n+\t      if (BB_END (bb) == head)\n+\t\t{\n+\t\t  if (EDGE_COUNT (bb->succs) > 1)\n+\t\t    gcc_assert (control_flow_insn_p (head)\n+\t\t\t\t|| has_edge_p (bb->succs, EDGE_COMPLEX));\n+\t\t  bb = 0;\n+\t\t}\n+\t\t\t      \n+\t      head = NEXT_INSN (head);\n+\t    }\n+\t}\n+\n+      not_first = 1;\n+    }\n+  while (head != next_tail);\n+\n+  gcc_assert (bb == 0);\n+}\n+\n+/* Perform few consistancy checks of flags in different data structures.  */\n+static void\n+check_sched_flags (void)\n+{\n+  unsigned int f = current_sched_info->flags;\n+\n+  if (flag_sched_stalled_insns)\n+    gcc_assert (!(f & DO_SPECULATION));\n+  if (f & DO_SPECULATION)\n+    gcc_assert (!flag_sched_stalled_insns\n+\t\t&& (f & DETACH_LIFE_INFO)\n+\t\t&& spec_info\n+\t\t&& spec_info->mask);\n+  if (f & DETACH_LIFE_INFO)\n+    gcc_assert (f & USE_GLAT);\n+}\n+\n+/* Checks global_live_at_{start, end} regsets.  */\n+void\n+check_reg_live (void)\n+{\n+  basic_block bb;\n+\n+  FOR_ALL_BB (bb)\n+    {\n+      int i;\n+\n+      i = bb->index;\n+\n+      if (glat_start[i])\n+\tgcc_assert (bitmap_equal_p (bb->il.rtl->global_live_at_start,\n+\t\t\t\t     glat_start[i]));\n+      if (glat_end[i])\n+\tgcc_assert (bitmap_equal_p (bb->il.rtl->global_live_at_end,\n+\t\t\t\t     glat_end[i]));\n+    }\n+}\n+#endif /* ENABLE_CHECKING */\n+\n #endif /* INSN_SCHEDULING */"}, {"sha": "907ccf5ef2b44b949350cdd6b42c9cad774b8f9a", "filename": "gcc/lists.c", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Flists.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Flists.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Flists.c?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -249,4 +249,20 @@ remove_free_INSN_LIST_elem (rtx elem, rtx *listp)\n   free_INSN_LIST_node (remove_list_elem (elem, listp));\n }\n \n+/* Create and return a copy of the DEPS_LIST LIST.  */\n+rtx\n+copy_DEPS_LIST_list (rtx list)\n+{\n+  rtx res = NULL_RTX, *resp = &res;\n+\n+  while (list)\n+    {\n+      *resp = alloc_DEPS_LIST (XEXP (list, 0), 0, XWINT (list, 2));\n+      PUT_REG_NOTE_KIND (*resp, REG_NOTE_KIND (list));\n+      resp = &XEXP (*resp, 1);\n+      list = XEXP (list, 1);\n+    }\n+  return res;\n+}\n+\n #include \"gt-lists.h\""}, {"sha": "5a270339492ad70c2145ee4236d3e9a3d773659b", "filename": "gcc/modulo-sched.c", "status": "modified", "additions": 9, "deletions": 11, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fmodulo-sched.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fmodulo-sched.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fmodulo-sched.c?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -237,12 +237,6 @@ sms_print_insn (rtx insn, int aligned ATTRIBUTE_UNUSED)\n   return tmp;\n }\n \n-static int\n-contributes_to_priority (rtx next, rtx insn)\n-{\n-  return BLOCK_NUM (next) == BLOCK_NUM (insn);\n-}\n-\n static void\n compute_jump_reg_dependencies (rtx insn ATTRIBUTE_UNUSED,\n \t\t\t       regset cond_exec ATTRIBUTE_UNUSED,\n@@ -259,12 +253,16 @@ static struct sched_info sms_sched_info =\n   NULL,\n   NULL,\n   sms_print_insn,\n-  contributes_to_priority,\n+  NULL,\n   compute_jump_reg_dependencies,\n   NULL, NULL,\n   NULL, NULL,\n   0, 0, 0,\n \n+  NULL, NULL, NULL, NULL, NULL,\n+#ifdef ENABLE_CHECKING\n+  NULL,\n+#endif\n   0\n };\n \n@@ -314,7 +312,7 @@ const_iteration_count (rtx count_reg, basic_block pre_header,\n   if (! pre_header)\n     return NULL_RTX;\n \n-  get_block_head_tail (pre_header->index, &head, &tail);\n+  get_ebb_head_tail (pre_header, pre_header, &head, &tail);\n \n   for (insn = tail; insn != PREV_INSN (head); insn = PREV_INSN (insn))\n     if (INSN_P (insn) && single_set (insn) &&\n@@ -794,7 +792,7 @@ loop_single_full_bb_p (struct loop *loop)\n \n       /* Make sure that basic blocks other than the header\n          have only notes labels or jumps.  */\n-      get_block_head_tail (bbs[i]->index, &head, &tail);\n+      get_ebb_head_tail (bbs[i], bbs[i], &head, &tail);\n       for (; head != NEXT_INSN (tail); head = NEXT_INSN (head))\n         {\n           if (NOTE_P (head) || LABEL_P (head)\n@@ -972,7 +970,7 @@ sms_schedule (void)\n \n       bb = loop->header;\n \n-      get_block_head_tail (bb->index, &head, &tail);\n+      get_ebb_head_tail (bb, bb, &head, &tail);\n       latch_edge = loop_latch_edge (loop);\n       gcc_assert (loop->single_exit);\n       if (loop->single_exit->count)\n@@ -1074,7 +1072,7 @@ sms_schedule (void)\n       if (dump_file)\n \tprint_ddg (dump_file, g);\n \n-      get_block_head_tail (loop->header->index, &head, &tail);\n+      get_ebb_head_tail (loop->header, loop->header, &head, &tail);\n \n       latch_edge = loop_latch_edge (loop);\n       gcc_assert (loop->single_exit);"}, {"sha": "69241e77814460e6893c367f72556a244f0b498c", "filename": "gcc/params.def", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fparams.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fparams.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fparams.def?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -504,6 +504,16 @@ DEFPARAM(PARAM_MAX_SCHED_EXTEND_REGIONS_ITERS,\n          \"The maximum number of iterations through CFG to extend regions\",\n          2, 0, 0)\n \n+DEFPARAM(PARAM_MAX_SCHED_INSN_CONFLICT_DELAY,\n+         \"max-sched-insn-conflict-delay\",\n+         \"The maximum conflict delay for an insn to be considered for speculative motion\",\n+         3, 1, 10)\n+\n+DEFPARAM(PARAM_SCHED_SPEC_PROB_CUTOFF,\n+         \"sched-spec-prob-cutoff\",\n+         \"The minimal probability of speculation success (in percents), so that speculative insn will be scheduled.\",\n+         40, 0, 100)\n+\n DEFPARAM(PARAM_MAX_LAST_VALUE_RTL,\n \t \"max-last-value-rtl\",\n \t \"The maximum number of RTL nodes that can be recorded as combiner's last value\","}, {"sha": "22ecd16f77cb5beace6630d628952a23148e525f", "filename": "gcc/rtl.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Frtl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Frtl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtl.h?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -1758,6 +1758,7 @@ rtx alloc_DEPS_LIST (rtx, rtx, HOST_WIDE_INT);\n void remove_free_DEPS_LIST_elem (rtx, rtx *);\n void remove_free_INSN_LIST_elem (rtx, rtx *);\n rtx remove_list_elem (rtx, rtx *);\n+rtx copy_DEPS_LIST_list (rtx);\n \n /* regclass.c */\n "}, {"sha": "33ee695a930a6eb80f8fbdc048d7a2d696d67b89", "filename": "gcc/sched-deps.c", "status": "modified", "additions": 24, "deletions": 10, "changes": 34, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fsched-deps.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fsched-deps.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-deps.c?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -112,8 +112,6 @@ static void adjust_add_sorted_back_dep (rtx, rtx, rtx *);\n static void adjust_back_add_forw_dep (rtx, rtx *);\n static void delete_forw_dep (rtx, rtx);\n static dw_t estimate_dep_weak (rtx, rtx);\n-static dw_t get_dep_weak (ds_t, ds_t);\n-static ds_t ds_merge (ds_t, ds_t);\n #ifdef INSN_SCHEDULING\n #ifdef ENABLE_CHECKING\n static void check_dep_status (enum reg_note, ds_t, bool);\n@@ -1777,19 +1775,35 @@ init_dependency_caches (int luid)\n      what we consider \"very high\".  */\n   if (luid / n_basic_blocks > 100 * 5)\n     {\n-      int i;\n+      cache_size = 0;\n+      extend_dependency_caches (luid, true);\n+    }\n+}\n \n-      true_dependency_cache = XNEWVEC (bitmap_head, luid);\n-      anti_dependency_cache = XNEWVEC (bitmap_head, luid);\n-      output_dependency_cache = XNEWVEC (bitmap_head, luid);\n+/* Create or extend (depending on CREATE_P) dependency caches to\n+   size N.  */\n+void\n+extend_dependency_caches (int n, bool create_p)\n+{\n+  if (create_p || true_dependency_cache)\n+    {\n+      int i, luid = cache_size + n;\n+\n+      true_dependency_cache = XRESIZEVEC (bitmap_head, true_dependency_cache,\n+\t\t\t\t\t  luid);\n+      output_dependency_cache = XRESIZEVEC (bitmap_head,\n+\t\t\t\t\t    output_dependency_cache, luid);\n+      anti_dependency_cache = XRESIZEVEC (bitmap_head, anti_dependency_cache,\n+\t\t\t\t\t  luid);\n #ifdef ENABLE_CHECKING\n-      forward_dependency_cache = XNEWVEC (bitmap_head, luid);\n+      forward_dependency_cache = XRESIZEVEC (bitmap_head,\n+\t\t\t\t\t     forward_dependency_cache, luid);\n #endif\n       if (current_sched_info->flags & DO_SPECULATION)\n         spec_dependency_cache = XRESIZEVEC (bitmap_head, spec_dependency_cache,\n \t\t\t\t\t    luid);\n \n-      for (i = 0; i < luid; i++)\n+      for (i = cache_size; i < luid; i++)\n \t{\n \t  bitmap_initialize (&true_dependency_cache[i], 0);\n \t  bitmap_initialize (&output_dependency_cache[i], 0);\n@@ -2037,7 +2051,7 @@ delete_back_forw_dep (rtx insn, rtx elem)\n }\n \n /* Return weakness of speculative type TYPE in the dep_status DS.  */\n-static dw_t\n+dw_t\n get_dep_weak (ds_t ds, ds_t type)\n {\n   ds = ds & type;\n@@ -2074,7 +2088,7 @@ set_dep_weak (ds_t ds, ds_t type, dw_t dw)\n }\n \n /* Return the join of two dep_statuses DS1 and DS2.  */\n-static ds_t\n+ds_t\n ds_merge (ds_t ds1, ds_t ds2)\n {\n   ds_t ds, t;"}, {"sha": "4126a5d75614d2fadd6a9d8fc706bdc4b7f503ad", "filename": "gcc/sched-ebb.c", "status": "modified", "additions": 289, "deletions": 174, "changes": 463, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fsched-ebb.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fsched-ebb.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-ebb.c?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -43,14 +43,23 @@ Software Foundation, 51 Franklin Street, Fifth Floor, Boston, MA\n #include \"target.h\"\n #include \"output.h\"\n \f\n-/* The number of insns to be scheduled in total.  */\n-static int target_n_insns;\n /* The number of insns scheduled so far.  */\n static int sched_n_insns;\n \n+/* The number of insns to be scheduled in total.  */\n+static int n_insns;\n+\n+/* Set of blocks, that already have their dependencies calculated.  */\n+static bitmap_head dont_calc_deps;\n+/* Set of basic blocks, that are ebb heads of tails respectively.  */\n+static bitmap_head ebb_head, ebb_tail;\n+\n+/* Last basic block in current ebb.  */\n+static basic_block last_bb;\n+\n /* Implementations of the sched_info functions for region scheduling.  */\n static void init_ready_list (void);\n-static int can_schedule_ready_p (rtx);\n+static void begin_schedule_ready (rtx, rtx);\n static int schedule_more_p (void);\n static const char *ebb_print_insn (rtx, int);\n static int rank (rtx, rtx);\n@@ -59,16 +68,22 @@ static void compute_jump_reg_dependencies (rtx, regset, regset, regset);\n static basic_block earliest_block_with_similiar_load (basic_block, rtx);\n static void add_deps_for_risky_insns (rtx, rtx);\n static basic_block schedule_ebb (rtx, rtx);\n-static basic_block fix_basic_block_boundaries (basic_block, basic_block, rtx,\n-\t\t\t\t\t       rtx);\n-static void add_missing_bbs (rtx, basic_block, basic_block);\n+\n+static void add_remove_insn (rtx, int);\n+static void add_block1 (basic_block, basic_block);\n+static basic_block advance_target_bb (basic_block, rtx);\n+static void fix_recovery_cfg (int, int, int);\n+\n+#ifdef ENABLE_CHECKING\n+static int ebb_head_or_leaf_p (basic_block, int);\n+#endif\n \n /* Return nonzero if there are more insns that should be scheduled.  */\n \n static int\n schedule_more_p (void)\n {\n-  return sched_n_insns < target_n_insns;\n+  return sched_n_insns < n_insns;\n }\n \n /* Add all insns that are initially ready to the ready list READY.  Called\n@@ -77,11 +92,11 @@ schedule_more_p (void)\n static void\n init_ready_list (void)\n {\n+  int n = 0;\n   rtx prev_head = current_sched_info->prev_head;\n   rtx next_tail = current_sched_info->next_tail;\n   rtx insn;\n \n-  target_n_insns = 0;\n   sched_n_insns = 0;\n \n #if 0\n@@ -95,18 +110,74 @@ init_ready_list (void)\n   for (insn = NEXT_INSN (prev_head); insn != next_tail; insn = NEXT_INSN (insn))\n     {\n       try_ready (insn);\n-      target_n_insns++;\n+      n++;\n     }\n-}\n \n-/* Called after taking INSN from the ready list.  Returns nonzero if this\n-   insn can be scheduled, nonzero if we should silently discard it.  */\n+  gcc_assert (n == n_insns);\n+}\n \n-static int\n-can_schedule_ready_p (rtx insn ATTRIBUTE_UNUSED)\n+/* INSN is being scheduled after LAST.  Update counters.  */\n+static void\n+begin_schedule_ready (rtx insn, rtx last)\n {\n   sched_n_insns++;\n-  return 1;\n+\n+  if (BLOCK_FOR_INSN (insn) == last_bb\n+      /* INSN is a jump in the last block, ...  */\n+      && control_flow_insn_p (insn)\n+      /* that is going to be moved over some instructions.  */\n+      && last != PREV_INSN (insn))\n+    {\n+      edge e;\n+      edge_iterator ei;\n+      basic_block bb;\n+\n+      /* An obscure special case, where we do have partially dead\n+\t instruction scheduled after last control flow instruction.\n+\t In this case we can create new basic block.  It is\n+\t always exactly one basic block last in the sequence.  */\n+      \n+      FOR_EACH_EDGE (e, ei, last_bb->succs)\n+\tif (e->flags & EDGE_FALLTHRU)\n+\t  break;\n+\n+#ifdef ENABLE_CHECKING\n+      gcc_assert (!e || !(e->flags & EDGE_COMPLEX));\t    \n+\n+      gcc_assert (BLOCK_FOR_INSN (insn) == last_bb\n+\t\t  && !RECOVERY_BLOCK (insn)\n+\t\t  && BB_HEAD (last_bb) != insn\n+\t\t  && BB_END (last_bb) == insn);\n+\n+      {\n+\trtx x;\n+\n+\tx = NEXT_INSN (insn);\n+\tif (e)\n+\t  gcc_assert (NOTE_P (x) || LABEL_P (x));\n+\telse\n+\t  gcc_assert (BARRIER_P (x));\n+      }\n+#endif\n+\n+      if (e)\n+\t{\n+\t  bb = split_edge (e);\n+\t  gcc_assert (NOTE_INSN_BASIC_BLOCK_P (BB_END (bb)));\n+\t}\n+      else\n+\tbb = create_basic_block (insn, 0, last_bb);\n+      \n+      /* split_edge () creates BB before E->DEST.  Keep in mind, that\n+\t this operation extends scheduling region till the end of BB.\n+\t Hence, we need to shift NEXT_TAIL, so haifa-sched.c won't go out\n+\t of the scheduling region.  */\n+      current_sched_info->next_tail = NEXT_INSN (BB_END (bb));\n+      gcc_assert (current_sched_info->next_tail);\n+\n+      add_block (bb, last_bb);\n+      gcc_assert (last_bb == bb);\n+    }\n }\n \n /* Return a string that contains the insn uid and optionally anything else\n@@ -173,9 +244,9 @@ compute_jump_reg_dependencies (rtx insn, regset cond_set, regset used,\n \t it may guard the fallthrough block from using a value that has\n \t conditionally overwritten that of the main codepath.  So we\n \t consider that it restores the value of the main codepath.  */\n-      bitmap_and (set, e->dest->il.rtl->global_live_at_start, cond_set);\n+      bitmap_and (set, glat_start [e->dest->index], cond_set);\n     else\n-      bitmap_ior_into (used, e->dest->il.rtl->global_live_at_start);\n+      bitmap_ior_into (used, glat_start [e->dest->index]);\n }\n \n /* Used in schedule_insns to initialize current_sched_info for scheduling\n@@ -184,7 +255,7 @@ compute_jump_reg_dependencies (rtx insn, regset cond_set, regset used,\n static struct sched_info ebb_sched_info =\n {\n   init_ready_list,\n-  can_schedule_ready_p,\n+  NULL,\n   schedule_more_p,\n   NULL,\n   rank,\n@@ -196,143 +267,19 @@ static struct sched_info ebb_sched_info =\n   NULL, NULL,\n   0, 1, 0,\n \n-  0\n+  add_remove_insn,\n+  begin_schedule_ready,\n+  add_block1,\n+  advance_target_bb,\n+  fix_recovery_cfg,\n+#ifdef ENABLE_CHECKING\n+  ebb_head_or_leaf_p,\n+#endif\n+  /* We need to DETACH_LIVE_INFO to be able to create new basic blocks.\n+     See begin_schedule_ready ().  */\n+  SCHED_EBB | USE_GLAT | DETACH_LIFE_INFO\n };\n \f\n-/* It is possible that ebb scheduling eliminated some blocks.\n-   Place blocks from FIRST to LAST before BEFORE.  */\n-\n-static void\n-add_missing_bbs (rtx before, basic_block first, basic_block last)\n-{\n-  for (; last != first->prev_bb; last = last->prev_bb)\n-    {\n-      before = emit_note_before (NOTE_INSN_BASIC_BLOCK, before);\n-      NOTE_BASIC_BLOCK (before) = last;\n-      BB_HEAD (last) = before;\n-      BB_END (last) = before;\n-      update_bb_for_insn (last);\n-    }\n-}\n-\n-/* Fixup the CFG after EBB scheduling.  Re-recognize the basic\n-   block boundaries in between HEAD and TAIL and update basic block\n-   structures between BB and LAST.  */\n-\n-static basic_block\n-fix_basic_block_boundaries (basic_block bb, basic_block last, rtx head,\n-\t\t\t    rtx tail)\n-{\n-  rtx insn = head;\n-  rtx last_inside = BB_HEAD (bb);\n-  rtx aftertail = NEXT_INSN (tail);\n-\n-  head = BB_HEAD (bb);\n-\n-  for (; insn != aftertail; insn = NEXT_INSN (insn))\n-    {\n-      gcc_assert (!LABEL_P (insn));\n-      /* Create new basic blocks just before first insn.  */\n-      if (inside_basic_block_p (insn))\n-\t{\n-\t  if (!last_inside)\n-\t    {\n-\t      rtx note;\n-\n-\t      /* Re-emit the basic block note for newly found BB header.  */\n-\t      if (LABEL_P (insn))\n-\t\t{\n-\t\t  note = emit_note_after (NOTE_INSN_BASIC_BLOCK, insn);\n-\t\t  head = insn;\n-\t\t  last_inside = note;\n-\t\t}\n-\t      else\n-\t\t{\n-\t\t  note = emit_note_before (NOTE_INSN_BASIC_BLOCK, insn);\n-\t\t  head = note;\n-\t\t  last_inside = insn;\n-\t\t}\n-\t    }\n-\t  else\n-\t    last_inside = insn;\n-\t}\n-      /* Control flow instruction terminate basic block.  It is possible\n-\t that we've eliminated some basic blocks (made them empty).\n-\t Find the proper basic block using BLOCK_FOR_INSN and arrange things in\n-\t a sensible way by inserting empty basic blocks as needed.  */\n-      if (control_flow_insn_p (insn) || (insn == tail && last_inside))\n-\t{\n-\t  basic_block curr_bb = BLOCK_FOR_INSN (insn);\n-\t  rtx note;\n-\n-\t  if (!control_flow_insn_p (insn))\n-\t    curr_bb = last;\n-\t  if (bb == last->next_bb)\n-\t    {\n-\t      edge f;\n-\t      rtx h;\n-\t      edge_iterator ei;\n-\n-\t      /* An obscure special case, where we do have partially dead\n-\t         instruction scheduled after last control flow instruction.\n-\t         In this case we can create new basic block.  It is\n-\t         always exactly one basic block last in the sequence.  Handle\n-\t         it by splitting the edge and repositioning the block.\n-\t         This is somewhat hackish, but at least avoid cut&paste\n-\n-\t         A safer solution can be to bring the code into sequence,\n-\t         do the split and re-emit it back in case this will ever\n-\t         trigger problem.  */\n-\n-\t      FOR_EACH_EDGE (f, ei, bb->prev_bb->succs)\n-\t\tif (f->flags & EDGE_FALLTHRU)\n-\t\t  break;\n-\n-\t      if (f)\n-\t\t{\n-\t\t  last = curr_bb = split_edge (f);\n-\t\t  h = BB_HEAD (curr_bb);\n-\t\t  BB_HEAD (curr_bb) = head;\n-\t\t  BB_END (curr_bb) = insn;\n-\t\t  /* Edge splitting created misplaced BASIC_BLOCK note, kill\n-\t\t     it.  */\n-\t\t  delete_insn (h);\n-\t\t}\n-\t      /* It may happen that code got moved past unconditional jump in\n-\t         case the code is completely dead.  Kill it.  */\n-\t      else\n-\t\t{\n-\t\t  rtx next = next_nonnote_insn (insn);\n-\t\t  delete_insn_chain (head, insn);\n-\t\t  /* We keep some notes in the way that may split barrier from the\n-\t\t     jump.  */\n-\t\t  if (BARRIER_P (next))\n-\t\t     {\n-\t\t       emit_barrier_after (prev_nonnote_insn (head));\n-\t\t       delete_insn (next);\n-\t\t     }\n-\t\t  insn = NULL;\n-\t\t}\n-\t    }\n-\t  else\n-\t    {\n-\t      BB_HEAD (curr_bb) = head;\n-\t      BB_END (curr_bb) = insn;\n-\t      add_missing_bbs (BB_HEAD (curr_bb), bb, curr_bb->prev_bb);\n-\t    }\n-\t  note = LABEL_P (head) ? NEXT_INSN (head) : head;\n-\t  NOTE_BASIC_BLOCK (note) = curr_bb;\n-\t  update_bb_for_insn (curr_bb);\n-\t  bb = curr_bb->next_bb;\n-\t  last_inside = NULL;\n-\t  if (!insn)\n-\t     break;\n-\t}\n-    }\n-  add_missing_bbs (BB_HEAD (last->next_bb), bb, last);\n-  return bb->prev_bb;\n-}\n-\n /* Returns the earliest block in EBB currently being processed where a\n    \"similar load\" 'insn2' is found, and hence LOAD_INSN can move\n    speculatively into the found block.  All the following must hold:\n@@ -488,29 +435,40 @@ add_deps_for_risky_insns (rtx head, rtx tail)\n static basic_block\n schedule_ebb (rtx head, rtx tail)\n {\n-  int n_insns;\n-  basic_block b;\n+  basic_block first_bb, target_bb;\n   struct deps tmp_deps;\n-  basic_block first_bb = BLOCK_FOR_INSN (head);\n-  basic_block last_bb = BLOCK_FOR_INSN (tail);\n+  \n+  first_bb = BLOCK_FOR_INSN (head);\n+  last_bb = BLOCK_FOR_INSN (tail);\n \n   if (no_real_insns_p (head, tail))\n     return BLOCK_FOR_INSN (tail);\n \n-  init_deps_global ();\n+  gcc_assert (INSN_P (head) && INSN_P (tail));\n+\n+  if (!bitmap_bit_p (&dont_calc_deps, first_bb->index))\n+    {\n+      init_deps_global ();\n \n-  /* Compute LOG_LINKS.  */\n-  init_deps (&tmp_deps);\n-  sched_analyze (&tmp_deps, head, tail);\n-  free_deps (&tmp_deps);\n+      /* Compute LOG_LINKS.  */\n+      init_deps (&tmp_deps);\n+      sched_analyze (&tmp_deps, head, tail);\n+      free_deps (&tmp_deps);\n \n-  /* Compute INSN_DEPEND.  */\n-  compute_forward_dependences (head, tail);\n+      /* Compute INSN_DEPEND.  */\n+      compute_forward_dependences (head, tail);\n \n-  add_deps_for_risky_insns (head, tail);\n+      add_deps_for_risky_insns (head, tail);\n \n-  if (targetm.sched.dependencies_evaluation_hook)\n-    targetm.sched.dependencies_evaluation_hook (head, tail);\n+      if (targetm.sched.dependencies_evaluation_hook)\n+        targetm.sched.dependencies_evaluation_hook (head, tail);\n+\n+      finish_deps_global ();\n+    }\n+  else\n+    /* Only recovery blocks can have their dependencies already calculated,\n+       and they always are single block ebbs.  */       \n+    gcc_assert (first_bb == last_bb);\n \n   /* Set priorities.  */\n   current_sched_info->sched_max_insns_priority = 0;\n@@ -546,21 +504,34 @@ schedule_ebb (rtx head, rtx tail)\n      schedule_block ().  */\n   rm_other_notes (head, tail);\n \n+  unlink_bb_notes (first_bb, last_bb);\n+\n   current_sched_info->queue_must_finish_empty = 1;\n \n-  schedule_block (-1, n_insns);\n+  target_bb = first_bb;\n+  schedule_block (&target_bb, n_insns);\n \n+  /* We might pack all instructions into fewer blocks,\n+     so we may made some of them empty.  Can't assert (b == last_bb).  */\n+  \n   /* Sanity check: verify that all region insns were scheduled.  */\n   gcc_assert (sched_n_insns == n_insns);\n   head = current_sched_info->head;\n   tail = current_sched_info->tail;\n \n   if (write_symbols != NO_DEBUG)\n     restore_line_notes (head, tail);\n-  b = fix_basic_block_boundaries (first_bb, last_bb, head, tail);\n \n-  finish_deps_global ();\n-  return b;\n+  if (EDGE_COUNT (last_bb->preds) == 0)\n+    /* LAST_BB is unreachable.  */\n+    {\n+      gcc_assert (first_bb != last_bb\n+\t\t  && EDGE_COUNT (last_bb->succs) == 0);\n+      last_bb = last_bb->prev_bb;\n+      delete_basic_block (last_bb->next_bb);\n+    }\n+\n+  return last_bb;\n }\n \n /* The one entry point in this file.  */\n@@ -570,6 +541,9 @@ schedule_ebbs (void)\n {\n   basic_block bb;\n   int probability_cutoff;\n+  rtx tail;\n+  sbitmap large_region_blocks, blocks;\n+  int any_large_regions;\n \n   if (profile_info && flag_branch_probabilities)\n     probability_cutoff = PARAM_VALUE (TRACER_MIN_BRANCH_PROBABILITY_FEEDBACK);\n@@ -590,11 +564,18 @@ schedule_ebbs (void)\n \n   compute_bb_for_insn ();\n \n+  /* Initialize DONT_CALC_DEPS and ebb-{start, end} markers.  */\n+  bitmap_initialize (&dont_calc_deps, 0);\n+  bitmap_clear (&dont_calc_deps);\n+  bitmap_initialize (&ebb_head, 0);\n+  bitmap_clear (&ebb_head);\n+  bitmap_initialize (&ebb_tail, 0);\n+  bitmap_clear (&ebb_tail);\n+\n   /* Schedule every region in the subroutine.  */\n   FOR_EACH_BB (bb)\n     {\n       rtx head = BB_HEAD (bb);\n-      rtx tail;\n \n       for (;;)\n \t{\n@@ -628,11 +609,71 @@ schedule_ebbs (void)\n \t    break;\n \t}\n \n+      bitmap_set_bit (&ebb_head, BLOCK_NUM (head));\n       bb = schedule_ebb (head, tail);\n+      bitmap_set_bit (&ebb_tail, bb->index);\n     }\n+  bitmap_clear (&dont_calc_deps);\n \n-  /* Updating life info can be done by local propagation over the modified\n-     superblocks.  */\n+  gcc_assert (current_sched_info->flags & DETACH_LIFE_INFO);\n+  /* We can create new basic blocks during scheduling, and\n+     attach_life_info () will create regsets for them\n+     (along with attaching existing info back).  */\n+  attach_life_info ();\n+\n+  /* Updating register live information.  */\n+  allocate_reg_life_data ();\n+\n+  any_large_regions = 0;\n+  large_region_blocks = sbitmap_alloc (last_basic_block);\n+  sbitmap_zero (large_region_blocks);\n+  FOR_EACH_BB (bb)\n+    SET_BIT (large_region_blocks, bb->index);\n+\n+  blocks = sbitmap_alloc (last_basic_block);\n+  sbitmap_zero (blocks);\n+\n+  /* Update life information.  For regions consisting of multiple blocks\n+     we've possibly done interblock scheduling that affects global liveness.\n+     For regions consisting of single blocks we need to do only local\n+     liveness.  */\n+  FOR_EACH_BB (bb)\n+    {\n+      int bbi;\n+      \n+      bbi = bb->index;\n+\n+      if (!bitmap_bit_p (&ebb_head, bbi)\n+\t  || !bitmap_bit_p (&ebb_tail, bbi)\n+\t  /* New blocks (e.g. recovery blocks) should be processed\n+\t     as parts of large regions.  */\n+\t  || !glat_start[bbi])\n+\tany_large_regions = 1;\n+      else\n+\t{\n+\t  SET_BIT (blocks, bbi);\n+\t  RESET_BIT (large_region_blocks, bbi);\n+\t}\n+    }\n+\n+  update_life_info (blocks, UPDATE_LIFE_LOCAL, 0);\n+  sbitmap_free (blocks);\n+  \n+  if (any_large_regions)\n+    {\n+      update_life_info (large_region_blocks, UPDATE_LIFE_GLOBAL, 0);\n+\n+#ifdef ENABLE_CHECKING\n+      /* !!! We can't check reg_live_info here because of the fact,\n+\t that destination registers of COND_EXEC's may be dead\n+\t before scheduling (while they should be alive).  Don't know why.  */\n+      /*check_reg_live ();*/\n+#endif\n+    }\n+  sbitmap_free (large_region_blocks);\n+\n+  bitmap_clear (&ebb_head);\n+  bitmap_clear (&ebb_tail);\n \n   /* Reposition the prologue and epilogue notes in case we moved the\n      prologue/epilogue insns.  */\n@@ -644,3 +685,77 @@ schedule_ebbs (void)\n \n   sched_finish ();\n }\n+\n+/* INSN has been added to/removed from current ebb.  */\n+static void\n+add_remove_insn (rtx insn ATTRIBUTE_UNUSED, int remove_p)\n+{\n+  if (!remove_p)\n+    n_insns++;\n+  else\n+    n_insns--;\n+}\n+\n+/* BB was added to ebb after AFTER.  */\n+static void\n+add_block1 (basic_block bb, basic_block after)\n+{\n+  /* Recovery blocks are always bounded by BARRIERS, \n+     therefore, they always form single block EBB,\n+     therefore, we can use rec->index to identify such EBBs.  */\n+  if (after == EXIT_BLOCK_PTR)\n+    bitmap_set_bit (&dont_calc_deps, bb->index);\n+  else if (after == last_bb)\n+    last_bb = bb;\n+}\n+\n+/* Return next block in ebb chain.  For parameter meaning please refer to\n+   sched-int.h: struct sched_info: advance_target_bb.  */\n+static basic_block\n+advance_target_bb (basic_block bb, rtx insn)\n+{\n+  if (insn)\n+    {\n+      if (BLOCK_FOR_INSN (insn) != bb\n+\t  && control_flow_insn_p (insn)\n+\t  && !RECOVERY_BLOCK (insn)\n+\t  && !RECOVERY_BLOCK (BB_END (bb)))\n+\t{\n+\t  gcc_assert (!control_flow_insn_p (BB_END (bb))\n+\t\t      && NOTE_INSN_BASIC_BLOCK_P (BB_HEAD (bb->next_bb)));\n+\t  return bb;\n+\t}\n+      else\n+\treturn 0;\n+    }\n+  else if (bb != last_bb)\n+    return bb->next_bb;\n+  else\n+    gcc_unreachable ();\n+}\n+\n+/* Fix internal data after interblock movement of jump instruction.\n+   For parameter meaning please refer to\n+   sched-int.h: struct sched_info: fix_recovery_cfg.  */\n+static void\n+fix_recovery_cfg (int bbi ATTRIBUTE_UNUSED, int jump_bbi, int jump_bb_nexti)\n+{\n+  gcc_assert (last_bb->index != bbi);\n+\n+  if (jump_bb_nexti == last_bb->index)\n+    last_bb = BASIC_BLOCK (jump_bbi);\n+}\n+\n+#ifdef ENABLE_CHECKING\n+/* Return non zero, if BB is first or last (depending of LEAF_P) block in\n+   current ebb.  For more information please refer to\n+   sched-int.h: struct sched_info: region_head_or_leaf_p.  */\n+static int\n+ebb_head_or_leaf_p (basic_block bb, int leaf_p)\n+{\n+  if (!leaf_p)    \n+    return bitmap_bit_p (&ebb_head, bb->index);\n+  else\n+    return bitmap_bit_p (&ebb_tail, bb->index);\n+}\n+#endif /* ENABLE_CHECKING  */"}, {"sha": "cdaca1b83f0447fae7a7eec6f93aa921141961be", "filename": "gcc/sched-int.h", "status": "modified", "additions": 122, "deletions": 7, "changes": 129, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fsched-int.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fsched-int.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-int.h?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -148,10 +148,12 @@ struct sched_info\n   int (*can_schedule_ready_p) (rtx);\n   /* Return nonzero if there are more insns that should be scheduled.  */\n   int (*schedule_more_p) (void);\n-  /* Called after an insn has all its dependencies resolved.  Return nonzero\n-     if it should be moved to the ready list or the queue, or zero if we\n-     should silently discard it.  */\n-  int (*new_ready) (rtx);\n+  /* Called after an insn has all its hard dependencies resolved. \n+     Adjusts status of instruction (which is passed through second parameter)\n+     to indicate if instruction should be moved to the ready list or the\n+     queue, or if it should silently discard it (until next resolved\n+     dependence).  */\n+  ds_t (*new_ready) (rtx, ds_t);\n   /* Compare priority of two insns.  Return a positive number if the second\n      insn is to be preferred for scheduling, and a negative one if the first\n      is to be preferred.  Zero if they are equally good.  */\n@@ -187,11 +189,73 @@ struct sched_info\n   /* Maximum priority that has been assigned to an insn.  */\n   int sched_max_insns_priority;\n \n+  /* Hooks to support speculative scheduling.  */\n+\n+  /* Called to notify frontend that instruction is being added (second\n+     parameter == 0) or removed (second parameter == 1).  */     \n+  void (*add_remove_insn) (rtx, int);\n+\n+  /* Called to notify frontend that instruction is being scheduled.\n+     The first parameter - instruction to scheduled, the second parameter -\n+     last scheduled instruction.  */\n+  void (*begin_schedule_ready) (rtx, rtx);\n+\n+  /* Called to notify frontend, that new basic block is being added.\n+     The first parameter - new basic block.\n+     The second parameter - block, after which new basic block is being added,\n+     or EXIT_BLOCK_PTR, if recovery block is being added,\n+     or NULL, if standalone block is being added.  */\n+  void (*add_block) (basic_block, basic_block);\n+\n+  /* If the second parameter is not NULL, return nonnull value, if the\n+     basic block should be advanced.\n+     If the second parameter is NULL, return the next basic block in EBB.\n+     The first parameter is the current basic block in EBB.  */\n+  basic_block (*advance_target_bb) (basic_block, rtx);\n+\n+  /* Called after blocks were rearranged due to movement of jump instruction.\n+     The first parameter - index of basic block, in which jump currently is.\n+     The second parameter - index of basic block, in which jump used\n+     to be.\n+     The third parameter - index of basic block, that follows the second\n+     parameter.  */\n+  void (*fix_recovery_cfg) (int, int, int);\n+\n+#ifdef ENABLE_CHECKING\n+  /* If the second parameter is zero, return nonzero, if block is head of the\n+     region.\n+     If the second parameter is nonzero, return nonzero, if block is leaf of\n+     the region.\n+     global_live_at_start should not change in region heads and\n+     global_live_at_end should not change in region leafs due to scheduling.  */\n+  int (*region_head_or_leaf_p) (basic_block, int);\n+#endif\n+\n   /* ??? FIXME: should use straight bitfields inside sched_info instead of\n      this flag field.  */\n   unsigned int flags;\n };\n \n+/* This structure holds description of the properties for speculative\n+   scheduling.  */\n+struct spec_info_def\n+{\n+  /* Holds types of allowed speculations: BEGIN_{DATA|CONTROL},\n+     BE_IN_{DATA_CONTROL}.  */\n+  int mask;\n+\n+  /* A dump file for additional information on speculative scheduling.  */\n+  FILE *dump;\n+\n+  /* Minimal cumulative weakness of speculative instruction's\n+     dependencies, so that insn will be scheduled.  */\n+  dw_t weakness_cutoff;\n+\n+  /* Flags from the enum SPEC_SCHED_FLAGS.  */\n+  int flags;\n+};\n+typedef struct spec_info_def *spec_info_t;\n+\n extern struct sched_info *current_sched_info;\n \n /* Indexed by INSN_UID, the collection of all data associated with\n@@ -256,9 +320,26 @@ struct haifa_insn_data\n   /* Nonzero if instruction has internal dependence\n      (e.g. add_dependence was invoked with (insn == elem)).  */\n   unsigned int has_internal_dep : 1;\n+  \n+  /* What speculations are neccessary to apply to schedule the instruction.  */\n+  ds_t todo_spec;\n+  /* What speculations were already applied.  */\n+  ds_t done_spec; \n+  /* What speculations are checked by this instruction.  */\n+  ds_t check_spec;\n+\n+  /* Recovery block for speculation checks.  */\n+  basic_block recovery_block;\n+\n+  /* Original pattern of the instruction.  */\n+  rtx orig_pat;\n };\n \n extern struct haifa_insn_data *h_i_d;\n+/* Used only if (current_sched_info->flags & USE_GLAT) != 0.\n+   These regsets store global_live_at_{start, end} information\n+   for each basic block.  */\n+extern regset *glat_start, *glat_end;\n \n /* Accessor macros for h_i_d.  There are more in haifa-sched.c and\n    sched-rgn.c.  */\n@@ -272,6 +353,11 @@ extern struct haifa_insn_data *h_i_d;\n #define INSN_COST(INSN)\t\t(h_i_d[INSN_UID (INSN)].cost)\n #define INSN_REG_WEIGHT(INSN)\t(h_i_d[INSN_UID (INSN)].reg_weight)\n #define HAS_INTERNAL_DEP(INSN)  (h_i_d[INSN_UID (INSN)].has_internal_dep)\n+#define TODO_SPEC(INSN)         (h_i_d[INSN_UID (INSN)].todo_spec)\n+#define DONE_SPEC(INSN)         (h_i_d[INSN_UID (INSN)].done_spec)\n+#define CHECK_SPEC(INSN)        (h_i_d[INSN_UID (INSN)].check_spec)\n+#define RECOVERY_BLOCK(INSN)    (h_i_d[INSN_UID (INSN)].recovery_block)\n+#define ORIG_PAT(INSN)          (h_i_d[INSN_UID (INSN)].orig_pat)\n \n /* DEP_STATUS of the link incapsulates information, that is needed for\n    speculative scheduling.  Namely, it is 4 integers in the range\n@@ -400,9 +486,27 @@ enum SCHED_FLAGS {\n   /* Perform data or control (or both) speculation.\n      Results in generation of data and control speculative dependencies.\n      Requires USE_DEPS_LIST set.  */\n-  DO_SPECULATION = USE_DEPS_LIST << 1\n+  DO_SPECULATION = USE_DEPS_LIST << 1,\n+  SCHED_RGN = DO_SPECULATION << 1,\n+  SCHED_EBB = SCHED_RGN << 1,\n+  /* Detach register live information from basic block headers.\n+     This is necessary to invoke functions, that change CFG (e.g. split_edge).\n+     Requires USE_GLAT.  */\n+  DETACH_LIFE_INFO = SCHED_EBB << 1,\n+  /* Save register live information from basic block headers to\n+     glat_{start, end} arrays.  */\n+  USE_GLAT = DETACH_LIFE_INFO << 1\n+};\n+\n+enum SPEC_SCHED_FLAGS {\n+  COUNT_SPEC_IN_CRITICAL_PATH = 1,\n+  PREFER_NON_DATA_SPEC = COUNT_SPEC_IN_CRITICAL_PATH << 1,\n+  PREFER_NON_CONTROL_SPEC = PREFER_NON_DATA_SPEC << 1\n };\n \n+#define NOTE_NOT_BB_P(NOTE) (NOTE_P (NOTE) && (NOTE_LINE_NUMBER (NOTE)\t\\\n+\t\t\t\t\t       != NOTE_INSN_BASIC_BLOCK))\n+\n extern FILE *sched_dump;\n extern int sched_verbose;\n \n@@ -500,16 +604,19 @@ extern void compute_forward_dependences (rtx, rtx);\n extern rtx find_insn_list (rtx, rtx);\n extern void init_dependency_caches (int);\n extern void free_dependency_caches (void);\n+extern void extend_dependency_caches (int, bool);\n extern enum DEPS_ADJUST_RESULT add_or_update_back_dep (rtx, rtx, \n \t\t\t\t\t\t       enum reg_note, ds_t);\n extern void add_or_update_back_forw_dep (rtx, rtx, enum reg_note, ds_t);\n extern void add_back_forw_dep (rtx, rtx, enum reg_note, ds_t);\n extern void delete_back_forw_dep (rtx, rtx);\n+extern dw_t get_dep_weak (ds_t, ds_t);\n extern ds_t set_dep_weak (ds_t, ds_t, dw_t);\n+extern ds_t ds_merge (ds_t, ds_t);\n \n /* Functions in haifa-sched.c.  */\n extern int haifa_classify_insn (rtx);\n-extern void get_block_head_tail (int, rtx *, rtx *);\n+extern void get_ebb_head_tail (basic_block, basic_block, rtx *, rtx *);\n extern int no_real_insns_p (rtx, rtx);\n \n extern void rm_line_notes (rtx, rtx);\n@@ -521,10 +628,18 @@ extern void rm_other_notes (rtx, rtx);\n extern int insn_cost (rtx, rtx, rtx);\n extern int set_priorities (rtx, rtx);\n \n-extern void schedule_block (int, int);\n+extern void schedule_block (basic_block *, int);\n extern void sched_init (void);\n extern void sched_finish (void);\n \n extern int try_ready (rtx);\n+extern void * xrecalloc (void *, size_t, size_t, size_t);\n+extern void unlink_bb_notes (basic_block, basic_block);\n+extern void add_block (basic_block, basic_block);\n+extern void attach_life_info (void);\n+\n+#ifdef ENABLE_CHECKING\n+extern void check_reg_live (void);\n+#endif\n \n #endif /* GCC_SCHED_INT_H */"}, {"sha": "77eec4b0df0ccf1ce9ff74f27397e598176b253f", "filename": "gcc/sched-rgn.c", "status": "modified", "additions": 450, "deletions": 134, "changes": 584, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fsched-rgn.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Fsched-rgn.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-rgn.c?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -96,8 +96,15 @@ static bool sched_is_disabled_for_current_region_p (void);\n    control flow graph edges, in the 'up' direction.  */\n typedef struct\n {\n-  int rgn_nr_blocks;\t\t/* Number of blocks in region.  */\n-  int rgn_blocks;\t\t/* cblocks in the region (actually index in rgn_bb_table).  */\n+  /* Number of extended basic blocks in region.  */\n+  int rgn_nr_blocks;\n+  /* cblocks in the region (actually index in rgn_bb_table).  */\n+  int rgn_blocks;\n+  /* Dependencies for this region are already computed.  Basically, indicates,\n+     that this is a recovery block.  */\n+  unsigned int dont_calc_deps : 1;\n+  /* This region has at least one non-trivial ebb.  */\n+  unsigned int has_real_ebb : 1;\n }\n region;\n \n@@ -125,6 +132,8 @@ static int min_spec_prob;\n \n #define RGN_NR_BLOCKS(rgn) (rgn_table[rgn].rgn_nr_blocks)\n #define RGN_BLOCKS(rgn) (rgn_table[rgn].rgn_blocks)\n+#define RGN_DONT_CALC_DEPS(rgn) (rgn_table[rgn].dont_calc_deps)\n+#define RGN_HAS_REAL_EBB(rgn) (rgn_table[rgn].has_real_ebb)\n #define BLOCK_TO_BB(block) (block_to_bb[block])\n #define CONTAINING_RGN(block) (containing_rgn[block])\n \n@@ -140,8 +149,15 @@ extern void debug_live (int, int);\n static int current_nr_blocks;\n static int current_blocks;\n \n-/* The mapping from bb to block.  */\n-#define BB_TO_BLOCK(bb) (rgn_bb_table[current_blocks + (bb)])\n+static int rgn_n_insns;\n+\n+/* The mapping from ebb to block.  */\n+/* ebb_head [i] - is index in rgn_bb_table, while\n+   EBB_HEAD (i) - is basic block index.\n+   BASIC_BLOCK (EBB_HEAD (i)) - head of ebb.  */\n+#define BB_TO_BLOCK(ebb) (rgn_bb_table[ebb_head[ebb]])\n+#define EBB_FIRST_BB(ebb) BASIC_BLOCK (BB_TO_BLOCK (ebb))\n+#define EBB_LAST_BB(ebb) BASIC_BLOCK (rgn_bb_table[ebb_head[ebb + 1] - 1])\n \n /* Target info declarations.\n \n@@ -244,6 +260,12 @@ static edgeset *pot_split;\n /* For every bb, a set of its ancestor edges.  */\n static edgeset *ancestor_edges;\n \n+/* Array of EBBs sizes.  Currently we can get a ebb only through \n+   splitting of currently scheduling block, therefore, we don't need\n+   ebb_head array for every region, its sufficient to hold it only\n+   for current one.  */\n+static int *ebb_head;\n+\n static void compute_dom_prob_ps (int);\n \n #define INSN_PROBABILITY(INSN) (SRC_PROB (BLOCK_TO_BB (BLOCK_NUM (INSN))))\n@@ -381,13 +403,12 @@ debug_regions (void)\n \t       rgn_table[rgn].rgn_nr_blocks);\n       fprintf (sched_dump, \";;\\tbb/block: \");\n \n-      for (bb = 0; bb < rgn_table[rgn].rgn_nr_blocks; bb++)\n-\t{\n-\t  current_blocks = RGN_BLOCKS (rgn);\n+      /* We don't have ebb_head initialized yet, so we can't use\n+\t BB_TO_BLOCK ().  */\n+      current_blocks = RGN_BLOCKS (rgn);\n \n-\t  gcc_assert (bb == BLOCK_TO_BB (BB_TO_BLOCK (bb)));\n-\t  fprintf (sched_dump, \" %d/%d \", bb, BB_TO_BLOCK (bb));\n-\t}\n+      for (bb = 0; bb < rgn_table[rgn].rgn_nr_blocks; bb++)\n+\tfprintf (sched_dump, \" %d/%d \", bb, rgn_bb_table[current_blocks + bb]);\n \n       fprintf (sched_dump, \"\\n\\n\");\n     }\n@@ -409,6 +430,8 @@ find_single_block_region (void)\n       rgn_bb_table[nr_regions] = bb->index;\n       RGN_NR_BLOCKS (nr_regions) = 1;\n       RGN_BLOCKS (nr_regions) = nr_regions;\n+      RGN_DONT_CALC_DEPS (nr_regions) = 0;\n+      RGN_HAS_REAL_EBB (nr_regions) = 0;\n       CONTAINING_RGN (bb->index) = nr_regions;\n       BLOCK_TO_BB (bb->index) = 0;\n       nr_regions++;\n@@ -852,6 +875,8 @@ find_rgns (void)\n \t\t  rgn_bb_table[idx] = bb->index;\n \t\t  RGN_NR_BLOCKS (nr_regions) = num_bbs;\n \t\t  RGN_BLOCKS (nr_regions) = idx++;\n+                  RGN_DONT_CALC_DEPS (nr_regions) = 0;\n+\t\t  RGN_HAS_REAL_EBB (nr_regions) = 0;\n \t\t  CONTAINING_RGN (bb->index) = nr_regions;\n \t\t  BLOCK_TO_BB (bb->index) = count = 0;\n \n@@ -921,6 +946,8 @@ find_rgns (void)\n \trgn_bb_table[idx] = bb->index;\n \tRGN_NR_BLOCKS (nr_regions) = 1;\n \tRGN_BLOCKS (nr_regions) = idx++;\n+        RGN_DONT_CALC_DEPS (nr_regions) = 0;\n+\tRGN_HAS_REAL_EBB (nr_regions) = 0;\n \tCONTAINING_RGN (bb->index) = nr_regions++;\n \tBLOCK_TO_BB (bb->index) = 0;\n       }\n@@ -1152,6 +1179,8 @@ extend_rgns (int *degree, int *idxp, sbitmap header, int *loop_hdr)\n \t      degree[bbn] = -1;\n \t      rgn_bb_table[idx] = bbn;\n \t      RGN_BLOCKS (nr_regions) = idx++;\n+\t      RGN_DONT_CALC_DEPS (nr_regions) = 0;\n+\t      RGN_HAS_REAL_EBB (nr_regions) = 0;\n \t      CONTAINING_RGN (bbn) = nr_regions;\n \t      BLOCK_TO_BB (bbn) = 0;\n \n@@ -1205,6 +1234,8 @@ extend_rgns (int *degree, int *idxp, sbitmap header, int *loop_hdr)\n \t\t\t{\n \t\t\t  RGN_BLOCKS (nr_regions) = idx;\n \t\t\t  RGN_NR_BLOCKS (nr_regions) = 1;\n+\t\t\t  RGN_DONT_CALC_DEPS (nr_regions) = 0;\n+\t\t\t  RGN_HAS_REAL_EBB (nr_regions) = 0;\n \t\t\t  nr_regions++;\n \t\t\t}\n \n@@ -1254,6 +1285,9 @@ compute_dom_prob_ps (int bb)\n   edge_iterator in_ei;\n   edge in_edge;\n \n+  /* We shouldn't have any real ebbs yet.  */\n+  gcc_assert (ebb_head [bb] == bb + current_blocks);\n+  \n   if (IS_RGN_ENTRY (bb))\n     {\n       SET_BIT (dom[bb], 0);\n@@ -1519,8 +1553,14 @@ check_live_1 (int src, rtx x)\n \t\t{\n \t\t  basic_block b = candidate_table[src].split_bbs.first_member[i];\n \n-\t\t  if (REGNO_REG_SET_P (b->il.rtl->global_live_at_start,\n-\t\t\t\t       regno + j))\n+\t\t  /* We can have split blocks, that were recently generated.\n+\t\t     such blocks are always outside current region.  */\n+\t\t  gcc_assert (glat_start[b->index]\n+\t\t\t      || CONTAINING_RGN (b->index)\n+\t\t\t      != CONTAINING_RGN (BB_TO_BLOCK (src)));\n+\t\t  if (!glat_start[b->index]\n+\t\t      || REGNO_REG_SET_P (glat_start[b->index],\n+\t\t\t\t\t  regno + j))\n \t\t    {\n \t\t      return 0;\n \t\t    }\n@@ -1534,7 +1574,11 @@ check_live_1 (int src, rtx x)\n \t    {\n \t      basic_block b = candidate_table[src].split_bbs.first_member[i];\n \n-\t      if (REGNO_REG_SET_P (b->il.rtl->global_live_at_start, regno))\n+\t      gcc_assert (glat_start[b->index]\n+\t\t\t  || CONTAINING_RGN (b->index)\n+\t\t\t  != CONTAINING_RGN (BB_TO_BLOCK (src)));\n+\t      if (!glat_start[b->index]\n+\t\t  || REGNO_REG_SET_P (glat_start[b->index], regno))\n \t\t{\n \t\t  return 0;\n \t\t}\n@@ -1593,8 +1637,7 @@ update_live_1 (int src, rtx x)\n \t\t{\n \t\t  basic_block b = candidate_table[src].update_bbs.first_member[i];\n \n-\t\t  SET_REGNO_REG_SET (b->il.rtl->global_live_at_start,\n-\t\t\t\t     regno + j);\n+\t\t  SET_REGNO_REG_SET (glat_start[b->index], regno + j);\n \t\t}\n \t    }\n \t}\n@@ -1604,7 +1647,7 @@ update_live_1 (int src, rtx x)\n \t    {\n \t      basic_block b = candidate_table[src].update_bbs.first_member[i];\n \n-\t      SET_REGNO_REG_SET (b->il.rtl->global_live_at_start, regno);\n+\t      SET_REGNO_REG_SET (glat_start[b->index], regno);\n \t    }\n \t}\n     }\n@@ -1880,25 +1923,35 @@ static int sched_target_n_insns;\n static int target_n_insns;\n /* The number of insns from the entire region scheduled so far.  */\n static int sched_n_insns;\n-/* Nonzero if the last scheduled insn was a jump.  */\n-static int last_was_jump;\n \n /* Implementations of the sched_info functions for region scheduling.  */\n static void init_ready_list (void);\n static int can_schedule_ready_p (rtx);\n-static int new_ready (rtx);\n+static void begin_schedule_ready (rtx, rtx);\n+static ds_t new_ready (rtx, ds_t);\n static int schedule_more_p (void);\n static const char *rgn_print_insn (rtx, int);\n static int rgn_rank (rtx, rtx);\n static int contributes_to_priority (rtx, rtx);\n static void compute_jump_reg_dependencies (rtx, regset, regset, regset);\n \n+/* Functions for speculative scheduling.  */\n+static void add_remove_insn (rtx, int);\n+static void extend_regions (void);\n+static void add_block1 (basic_block, basic_block);\n+static void fix_recovery_cfg (int, int, int);\n+static basic_block advance_target_bb (basic_block, rtx);\n+static void check_dead_notes1 (int, sbitmap);\n+#ifdef ENABLE_CHECKING\n+static int region_head_or_leaf_p (basic_block, int);\n+#endif\n+\n /* Return nonzero if there are more insns that should be scheduled.  */\n \n static int\n schedule_more_p (void)\n {\n-  return ! last_was_jump && sched_target_n_insns < target_n_insns;\n+  return sched_target_n_insns < target_n_insns;\n }\n \n /* Add all insns that are initially ready to the ready list READY.  Called\n@@ -1915,7 +1968,6 @@ init_ready_list (void)\n   target_n_insns = 0;\n   sched_target_n_insns = 0;\n   sched_n_insns = 0;\n-  last_was_jump = 0;\n \n   /* Print debugging information.  */\n   if (sched_verbose >= 5)\n@@ -1946,6 +1998,8 @@ init_ready_list (void)\n     {      \n       try_ready (insn);\n       target_n_insns++;\n+\n+      gcc_assert (!(TODO_SPEC (insn) & BEGIN_CONTROL));\n     }\n \n   /* Add to ready list all 'ready' insns in valid source blocks.\n@@ -1958,7 +2012,8 @@ init_ready_list (void)\n \trtx src_next_tail;\n \trtx tail, head;\n \n-\tget_block_head_tail (BB_TO_BLOCK (bb_src), &head, &tail);\n+\tget_ebb_head_tail (EBB_FIRST_BB (bb_src), EBB_LAST_BB (bb_src),\n+\t\t\t   &head, &tail);\n \tsrc_next_tail = NEXT_INSN (tail);\n \tsrc_head = head;\n \n@@ -1974,18 +2029,29 @@ init_ready_list (void)\n static int\n can_schedule_ready_p (rtx insn)\n {\n-  if (JUMP_P (insn))\n-    last_was_jump = 1;\n+  /* An interblock motion?  */\n+  if (INSN_BB (insn) != target_bb\n+      && IS_SPECULATIVE_INSN (insn)\n+      && !check_live (insn, INSN_BB (insn)))\n+    return 0;          \n+  else\n+    return 1;\n+}\n \n+/* Updates counter and other information.  Splitted from can_schedule_ready_p ()\n+   because when we schedule insn speculatively then insn passed to\n+   can_schedule_ready_p () differs from the one passed to\n+   begin_schedule_ready ().  */\n+static void\n+begin_schedule_ready (rtx insn, rtx last ATTRIBUTE_UNUSED)\n+{\n   /* An interblock motion?  */\n   if (INSN_BB (insn) != target_bb)\n     {\n-      basic_block b1;\n-\n       if (IS_SPECULATIVE_INSN (insn))\n \t{\n-\t  if (!check_live (insn, INSN_BB (insn)))\n-\t    return 0;\n+\t  gcc_assert (check_live (insn, INSN_BB (insn)));\n+\n \t  update_live (insn, INSN_BB (insn));\n \n \t  /* For speculative load, mark insns fed by it.  */\n@@ -1995,61 +2061,51 @@ can_schedule_ready_p (rtx insn)\n \t  nr_spec++;\n \t}\n       nr_inter++;\n-\n-      /* Update source block boundaries.  */\n-      b1 = BLOCK_FOR_INSN (insn);\n-      if (insn == BB_HEAD (b1) && insn == BB_END (b1))\n-\t{\n-\t  /* We moved all the insns in the basic block.\n-\t     Emit a note after the last insn and update the\n-\t     begin/end boundaries to point to the note.  */\n-\t  rtx note = emit_note_after (NOTE_INSN_DELETED, insn);\n-\t  BB_HEAD (b1) = note;\n-\t  BB_END (b1) = note;\n-\t}\n-      else if (insn == BB_END (b1))\n-\t{\n-\t  /* We took insns from the end of the basic block,\n-\t     so update the end of block boundary so that it\n-\t     points to the first insn we did not move.  */\n-\t  BB_END (b1) = PREV_INSN (insn);\n-\t}\n-      else if (insn == BB_HEAD (b1))\n-\t{\n-\t  /* We took insns from the start of the basic block,\n-\t     so update the start of block boundary so that\n-\t     it points to the first insn we did not move.  */\n-\t  BB_HEAD (b1) = NEXT_INSN (insn);\n-\t}\n     }\n   else\n     {\n       /* In block motion.  */\n       sched_target_n_insns++;\n     }\n   sched_n_insns++;\n-\n-  return 1;\n }\n \n-/* Called after INSN has all its dependencies resolved.  Return nonzero\n-   if it should be moved to the ready list or the queue, or zero if we\n-   should silently discard it.  */\n-static int\n-new_ready (rtx next)\n+/* Called after INSN has all its hard dependencies resolved and the speculation\n+   of type TS is enough to overcome them all.\n+   Return nonzero if it should be moved to the ready list or the queue, or zero\n+   if we should silently discard it.  */\n+static ds_t\n+new_ready (rtx next, ds_t ts)\n {\n-  /* For speculative insns, before inserting to ready/queue,\n-     check live, exception-free, and issue-delay.  */\n-  if (INSN_BB (next) != target_bb\n-      && (!IS_VALID (INSN_BB (next))\n+  if (INSN_BB (next) != target_bb)\n+    {\n+      int not_ex_free = 0;\n+\n+      /* For speculative insns, before inserting to ready/queue,\n+\t check live, exception-free, and issue-delay.  */\t\n+      if (!IS_VALID (INSN_BB (next))\n \t  || CANT_MOVE (next)\n \t  || (IS_SPECULATIVE_INSN (next)\n \t      && ((recog_memoized (next) >= 0\n-\t\t   && min_insn_conflict_delay (curr_state, next, next) > 3)\n+\t\t   && min_insn_conflict_delay (curr_state, next, next) \n+                   > PARAM_VALUE (PARAM_MAX_SCHED_INSN_CONFLICT_DELAY))\n+                  || RECOVERY_BLOCK (next)\n \t\t  || !check_live (next, INSN_BB (next))\n-\t\t  || !is_exception_free (next, INSN_BB (next), target_bb)))))\n-    return 0;\n-  return 1;\n+\t\t  || (not_ex_free = !is_exception_free (next, INSN_BB (next),\n+\t\t\t\t\t\t\ttarget_bb)))))\n+\t{\n+\t  if (not_ex_free\n+\t      /* We are here because is_exception_free () == false.\n+\t\t But we possibly can handle that with control speculation.  */\n+\t      && current_sched_info->flags & DO_SPECULATION)\n+            /* Here we got new control-speculative instruction.  */\n+            ts = set_dep_weak (ts, BEGIN_CONTROL, MAX_DEP_WEAK);\n+\t  else\n+            ts = (ts & ~SPECULATIVE) | HARD_DEP;\n+\t}\n+    }\n+  \n+  return ts;\n }\n \n /* Return a string that contains the insn uid and optionally anything else\n@@ -2112,7 +2168,8 @@ rgn_rank (rtx insn1, rtx insn2)\n static int\n contributes_to_priority (rtx next, rtx insn)\n {\n-  return BLOCK_NUM (next) == BLOCK_NUM (insn);\n+  /* NEXT and INSN reside in one ebb.  */\n+  return BLOCK_TO_BB (BLOCK_NUM (next)) == BLOCK_TO_BB (BLOCK_NUM (insn));\n }\n \n /* INSN is a JUMP_INSN, COND_SET is the set of registers that are\n@@ -2148,7 +2205,18 @@ static struct sched_info region_sched_info =\n   NULL, NULL,\n   0, 0, 0,\n \n-  0\n+  add_remove_insn,\n+  begin_schedule_ready,\n+  add_block1,\n+  advance_target_bb,\n+  fix_recovery_cfg,\n+#ifdef ENABLE_CHECKING\n+  region_head_or_leaf_p,\n+#endif\n+  SCHED_RGN | USE_GLAT\n+#ifdef ENABLE_CHECKING\n+  | DETACH_LIFE_INFO\n+#endif\n };\n \n /* Determine if PAT sets a CLASS_LIKELY_SPILLED_P register.  */\n@@ -2447,7 +2515,8 @@ compute_block_backward_dependences (int bb)\n   tmp_deps = bb_deps[bb];\n \n   /* Do the analysis for this block.  */\n-  get_block_head_tail (BB_TO_BLOCK (bb), &head, &tail);\n+  gcc_assert (EBB_FIRST_BB (bb) == EBB_LAST_BB (bb));\n+  get_ebb_head_tail (EBB_FIRST_BB (bb), EBB_LAST_BB (bb), &head, &tail);\n   sched_analyze (&tmp_deps, head, tail);\n   add_branch_dependences (head, tail);\n \n@@ -2489,7 +2558,8 @@ debug_dependencies (void)\n       rtx next_tail;\n       rtx insn;\n \n-      get_block_head_tail (BB_TO_BLOCK (bb), &head, &tail);\n+      gcc_assert (EBB_FIRST_BB (bb) == EBB_LAST_BB (bb));\n+      get_ebb_head_tail (EBB_FIRST_BB (bb), EBB_LAST_BB (bb), &head, &tail);\n       next_tail = NEXT_INSN (tail);\n       fprintf (sched_dump, \"\\n;;   --- Region Dependences --- b %d bb %d \\n\",\n \t       BB_TO_BLOCK (bb), bb);\n@@ -2576,48 +2646,68 @@ schedule_region (int rgn)\n   edge_iterator ei;\n   edge e;\n   int bb;\n-  int rgn_n_insns = 0;\n   int sched_rgn_n_insns = 0;\n \n+  rgn_n_insns = 0;\n   /* Set variables for the current region.  */\n   current_nr_blocks = RGN_NR_BLOCKS (rgn);\n   current_blocks = RGN_BLOCKS (rgn);\n+  \n+  /* See comments in add_block1, for what reasons we allocate +1 element.  */ \n+  ebb_head = xrealloc (ebb_head, (current_nr_blocks + 1) * sizeof (*ebb_head));\n+  for (bb = 0; bb <= current_nr_blocks; bb++)\n+    ebb_head[bb] = current_blocks + bb;\n \n   /* Don't schedule region that is marked by\n      NOTE_DISABLE_SCHED_OF_BLOCK.  */\n   if (sched_is_disabled_for_current_region_p ())\n     return;\n \n-  init_deps_global ();\n+  if (!RGN_DONT_CALC_DEPS (rgn))\n+    {\n+      init_deps_global ();\n \n-  /* Initializations for region data dependence analysis.  */\n-  bb_deps = XNEWVEC (struct deps, current_nr_blocks);\n-  for (bb = 0; bb < current_nr_blocks; bb++)\n-    init_deps (bb_deps + bb);\n+      /* Initializations for region data dependence analysis.  */\n+      bb_deps = XNEWVEC (struct deps, current_nr_blocks);\n+      for (bb = 0; bb < current_nr_blocks; bb++)\n+\tinit_deps (bb_deps + bb);\n \n-  /* Compute LOG_LINKS.  */\n-  for (bb = 0; bb < current_nr_blocks; bb++)\n-    compute_block_backward_dependences (bb);\n+      /* Compute LOG_LINKS.  */\n+      for (bb = 0; bb < current_nr_blocks; bb++)\n+        compute_block_backward_dependences (bb);\n \n-  /* Compute INSN_DEPEND.  */\n-  for (bb = current_nr_blocks - 1; bb >= 0; bb--)\n-    {\n-      rtx head, tail;\n-      get_block_head_tail (BB_TO_BLOCK (bb), &head, &tail);\n+      /* Compute INSN_DEPEND.  */\n+      for (bb = current_nr_blocks - 1; bb >= 0; bb--)\n+        {\n+          rtx head, tail;\n \n-      compute_forward_dependences (head, tail);\n+\t  gcc_assert (EBB_FIRST_BB (bb) == EBB_LAST_BB (bb));\n+          get_ebb_head_tail (EBB_FIRST_BB (bb), EBB_LAST_BB (bb), &head, &tail);\n \n-      if (targetm.sched.dependencies_evaluation_hook)\n-\ttargetm.sched.dependencies_evaluation_hook (head, tail);\n+          compute_forward_dependences (head, tail);\n \n-    }\n+          if (targetm.sched.dependencies_evaluation_hook)\n+            targetm.sched.dependencies_evaluation_hook (head, tail);\n+        }\n \n+      free_pending_lists ();\n+\n+      finish_deps_global ();\n+\n+      free (bb_deps);\n+    }\n+  else\n+    /* This is a recovery block.  It is always a single block region.  */\n+    gcc_assert (current_nr_blocks == 1);\n+      \n   /* Set priorities.  */\n   current_sched_info->sched_max_insns_priority = 0;\n   for (bb = 0; bb < current_nr_blocks; bb++)\n     {\n       rtx head, tail;\n-      get_block_head_tail (BB_TO_BLOCK (bb), &head, &tail);\n+      \n+      gcc_assert (EBB_FIRST_BB (bb) == EBB_LAST_BB (bb));\n+      get_ebb_head_tail (EBB_FIRST_BB (bb), EBB_LAST_BB (bb), &head, &tail);\n \n       rgn_n_insns += set_priorities (head, tail);\n     }\n@@ -2660,18 +2750,36 @@ schedule_region (int rgn)\n       /* Compute probabilities, dominators, split_edges.  */\n       for (bb = 0; bb < current_nr_blocks; bb++)\n \tcompute_dom_prob_ps (bb);\n+\n+      /* Cleanup ->aux used for EDGE_TO_BIT mapping.  */\n+      /* We don't need them anymore.  But we want to avoid dublication of\n+\t aux fields in the newly created edges.  */\n+      FOR_EACH_BB (block)\n+\t{\n+\t  if (CONTAINING_RGN (block->index) != rgn)\n+\t    continue;\n+\t  FOR_EACH_EDGE (e, ei, block->succs)\n+\t    e->aux = NULL;\n+        }\n     }\n \n   /* Now we can schedule all blocks.  */\n   for (bb = 0; bb < current_nr_blocks; bb++)\n     {\n+      basic_block first_bb, last_bb, curr_bb;\n       rtx head, tail;\n       int b = BB_TO_BLOCK (bb);\n \n-      get_block_head_tail (b, &head, &tail);\n+      first_bb = EBB_FIRST_BB (bb);\n+      last_bb = EBB_LAST_BB (bb);\n+\n+      get_ebb_head_tail (first_bb, last_bb, &head, &tail);\n \n       if (no_real_insns_p (head, tail))\n-\tcontinue;\n+\t{\n+\t  gcc_assert (first_bb == last_bb);\n+\t  continue;\n+\t}\n \n       current_sched_info->prev_head = PREV_INSN (head);\n       current_sched_info->next_tail = NEXT_INSN (tail);\n@@ -2696,26 +2804,29 @@ schedule_region (int rgn)\n \t    if (REG_NOTE_KIND (note) == REG_SAVE_NOTE)\n \t      remove_note (head, note);\n \t}\n+      else\n+\t/* This means that first block in ebb is empty.\n+\t   It looks to me as an impossible thing.  There at least should be\n+\t   a recovery check, that caused the splitting.  */\n+\tgcc_unreachable ();\n \n       /* Remove remaining note insns from the block, save them in\n \t note_list.  These notes are restored at the end of\n \t schedule_block ().  */\n       rm_other_notes (head, tail);\n \n+      unlink_bb_notes (first_bb, last_bb);\n+\n       target_bb = bb;\n \n       gcc_assert (flag_schedule_interblock || current_nr_blocks == 1);\n       current_sched_info->queue_must_finish_empty = current_nr_blocks == 1;\n \n-      schedule_block (b, rgn_n_insns);\n+      curr_bb = first_bb;\n+      schedule_block (&curr_bb, rgn_n_insns);\n+      gcc_assert (EBB_FIRST_BB (bb) == first_bb);\n       sched_rgn_n_insns += sched_n_insns;\n \n-      /* Update target block boundaries.  */\n-      if (head == BB_HEAD (BASIC_BLOCK (b)))\n-\tBB_HEAD (BASIC_BLOCK (b)) = current_sched_info->head;\n-      if (tail == BB_END (BASIC_BLOCK (b)))\n-\tBB_END (BASIC_BLOCK (b)) = current_sched_info->tail;\n-\n       /* Clean up.  */\n       if (current_nr_blocks > 1)\n \t{\n@@ -2734,29 +2845,16 @@ schedule_region (int rgn)\n       for (bb = 0; bb < current_nr_blocks; bb++)\n \t{\n \t  rtx head, tail;\n-\t  get_block_head_tail (BB_TO_BLOCK (bb), &head, &tail);\n+\n+\t  get_ebb_head_tail (EBB_FIRST_BB (bb), EBB_LAST_BB (bb), &head, &tail);\n \t  restore_line_notes (head, tail);\n \t}\n     }\n \n   /* Done with this region.  */\n-  free_pending_lists ();\n-\n-  finish_deps_global ();\n-\n-  free (bb_deps);\n \n   if (current_nr_blocks > 1)\n     {\n-      /* Cleanup ->aux used for EDGE_TO_BIT mapping.  */\n-      FOR_EACH_BB (block)\n-\t{\n-\t  if (CONTAINING_RGN (block->index) != rgn)\n-\t    continue;\n-\t  FOR_EACH_EDGE (e, ei, block->succs)\n-\t    e->aux = NULL;\n-\t}\n-\n       free (prob);\n       sbitmap_vector_free (dom);\n       sbitmap_vector_free (pot_split);\n@@ -2778,10 +2876,11 @@ init_regions (void)\n   int rgn;\n \n   nr_regions = 0;\n-  rgn_table = XNEWVEC (region, n_basic_blocks);\n-  rgn_bb_table = XNEWVEC (int, n_basic_blocks);\n-  block_to_bb = XNEWVEC (int, last_basic_block);\n-  containing_rgn = XNEWVEC (int, last_basic_block);\n+  rgn_table = 0;\n+  rgn_bb_table = 0;\n+  block_to_bb = 0;\n+  containing_rgn = 0;\n+  extend_regions ();\n \n   /* Compute regions for scheduling.  */\n   if (reload_completed\n@@ -2806,6 +2905,8 @@ init_regions (void)\n \t to using the cfg code in flow.c.  */\n       free_dominance_info (CDI_DOMINATORS);\n     }\n+  RGN_BLOCKS (nr_regions) = RGN_BLOCKS (nr_regions - 1) +\n+    RGN_NR_BLOCKS (nr_regions - 1);\n \n \n   if (CHECK_DEAD_NOTES)\n@@ -2814,15 +2915,8 @@ init_regions (void)\n       deaths_in_region = XNEWVEC (int, nr_regions);\n       /* Remove all death notes from the subroutine.  */\n       for (rgn = 0; rgn < nr_regions; rgn++)\n-\t{\n-\t  int b;\n+        check_dead_notes1 (rgn, blocks);\n \n-\t  sbitmap_zero (blocks);\n-\t  for (b = RGN_NR_BLOCKS (rgn) - 1; b >= 0; --b)\n-\t    SET_BIT (blocks, rgn_bb_table[RGN_BLOCKS (rgn) + b]);\n-\n-\t  deaths_in_region[rgn] = count_or_remove_death_notes (blocks, 1);\n-\t}\n       sbitmap_free (blocks);\n     }\n   else\n@@ -2858,9 +2952,15 @@ schedule_insns (void)\n \n   init_regions ();\n \n+  /* EBB_HEAD is a region-scope sctructure.  But we realloc it for\n+     each region to save time/memory/something else.  */\n+  ebb_head = 0;\n+  \n   /* Schedule every region in the subroutine.  */\n   for (rgn = 0; rgn < nr_regions; rgn++)\n     schedule_region (rgn);\n+  \n+  free(ebb_head);\n \n   /* Update life analysis for the subroutine.  Do single block regions\n      first so that we can verify that live_at_start didn't change.  Then\n@@ -2875,8 +2975,11 @@ schedule_insns (void)\n      that live_at_start should change at region heads.  Not sure what the\n      best way to test for this kind of thing...  */\n \n+  if (current_sched_info->flags & DETACH_LIFE_INFO)\n+    /* this flag can be set either by the target or by ENABLE_CHECKING.  */\n+    attach_life_info ();\n+\n   allocate_reg_life_data ();\n-  compute_bb_for_insn ();\n \n   any_large_regions = 0;\n   large_region_blocks = sbitmap_alloc (last_basic_block);\n@@ -2891,8 +2994,13 @@ schedule_insns (void)\n      we've possibly done interblock scheduling that affects global liveness.\n      For regions consisting of single blocks we need to do only local\n      liveness.  */\n-  for (rgn = 0; rgn < nr_regions; rgn++)\n-    if (RGN_NR_BLOCKS (rgn) > 1)\n+  for (rgn = 0; rgn < nr_regions; rgn++)    \n+    if (RGN_NR_BLOCKS (rgn) > 1\n+\t/* Or the only block of this region has been splitted.  */\n+\t|| RGN_HAS_REAL_EBB (rgn)\n+\t/* New blocks (e.g. recovery blocks) should be processed\n+\t   as parts of large regions.  */\n+\t|| !glat_start[rgn_bb_table[RGN_BLOCKS (rgn)]])\n       any_large_regions = 1;\n     else\n       {\n@@ -2904,16 +3012,21 @@ schedule_insns (void)\n      regs_ever_live, which should not change after reload.  */\n   update_life_info (blocks, UPDATE_LIFE_LOCAL,\n \t\t    (reload_completed ? PROP_DEATH_NOTES\n-\t\t     : PROP_DEATH_NOTES | PROP_REG_INFO));\n+\t\t     : (PROP_DEATH_NOTES | PROP_REG_INFO)));\n   if (any_large_regions)\n     {\n       update_life_info (large_region_blocks, UPDATE_LIFE_GLOBAL,\n-\t\t\tPROP_DEATH_NOTES | PROP_REG_INFO);\n+\t\t\t(reload_completed ? PROP_DEATH_NOTES\n+\t\t\t : (PROP_DEATH_NOTES | PROP_REG_INFO)));\n+\n+#ifdef ENABLE_CHECKING\n+      check_reg_live ();\n+#endif\n     }\n \n   if (CHECK_DEAD_NOTES)\n     {\n-      /* Verify the counts of basic block notes in single the basic block\n+      /* Verify the counts of basic block notes in single basic block\n          regions.  */\n       for (rgn = 0; rgn < nr_regions; rgn++)\n \tif (RGN_NR_BLOCKS (rgn) == 1)\n@@ -2960,6 +3073,209 @@ schedule_insns (void)\n   sbitmap_free (blocks);\n   sbitmap_free (large_region_blocks);\n }\n+\n+/* INSN has been added to/removed from current region.  */\n+static void\n+add_remove_insn (rtx insn, int remove_p)\n+{\n+  if (!remove_p)\n+    rgn_n_insns++;\n+  else\n+    rgn_n_insns--;\n+\n+  if (INSN_BB (insn) == target_bb)\n+    {\n+      if (!remove_p)\n+\ttarget_n_insns++;\n+      else\n+\ttarget_n_insns--;\n+    }\n+}\n+\n+/* Extend internal data structures.  */\n+static void\n+extend_regions (void)\n+{\n+  rgn_table = XRESIZEVEC (region, rgn_table, n_basic_blocks);\n+  rgn_bb_table = XRESIZEVEC (int, rgn_bb_table, n_basic_blocks);\n+  block_to_bb = XRESIZEVEC (int, block_to_bb, last_basic_block);\n+  containing_rgn = XRESIZEVEC (int, containing_rgn, last_basic_block);\n+}\n+\n+/* BB was added to ebb after AFTER.  */\n+static void\n+add_block1 (basic_block bb, basic_block after)\n+{\n+  extend_regions ();\n+\n+  if (after == 0 || after == EXIT_BLOCK_PTR)\n+    {\n+      int i;\n+      \n+      i = RGN_BLOCKS (nr_regions);\n+      /* I - first free position in rgn_bb_table.  */\n+\n+      rgn_bb_table[i] = bb->index;\n+      RGN_NR_BLOCKS (nr_regions) = 1;\n+      RGN_DONT_CALC_DEPS (nr_regions) = after == EXIT_BLOCK_PTR;\n+      RGN_HAS_REAL_EBB (nr_regions) = 0;\n+      CONTAINING_RGN (bb->index) = nr_regions;\n+      BLOCK_TO_BB (bb->index) = 0;\n+\n+      nr_regions++;\n+      \n+      RGN_BLOCKS (nr_regions) = i + 1;\n+\n+      if (CHECK_DEAD_NOTES)\n+        {\n+          sbitmap blocks = sbitmap_alloc (last_basic_block);\n+          deaths_in_region = xrealloc (deaths_in_region, nr_regions *\n+\t\t\t\t       sizeof (*deaths_in_region));\n+\n+          check_dead_notes1 (nr_regions - 1, blocks);\n+      \n+          sbitmap_free (blocks);\n+        }\n+    }\n+  else\n+    { \n+      int i, pos;\n+\n+      /* We need to fix rgn_table, block_to_bb, containing_rgn\n+\t and ebb_head.  */\n+\n+      BLOCK_TO_BB (bb->index) = BLOCK_TO_BB (after->index);\n+\n+      /* We extend ebb_head to one more position to\n+\t easily find the last position of the last ebb in \n+\t the current region.  Thus, ebb_head[BLOCK_TO_BB (after) + 1]\n+\t is _always_ valid for access.  */\n+\n+      i = BLOCK_TO_BB (after->index) + 1;\n+      for (pos = ebb_head[i]; rgn_bb_table[pos] != after->index; pos--);\n+      pos++;\n+      gcc_assert (pos > ebb_head[i - 1]);\n+      /* i - ebb right after \"AFTER\".  */\n+      /* ebb_head[i] - VALID.  */\n+\n+      /* Source position: ebb_head[i]\n+\t Destination posistion: ebb_head[i] + 1\n+\t Last position: \n+\t   RGN_BLOCKS (nr_regions) - 1\n+\t Number of elements to copy: (last_position) - (source_position) + 1\n+       */\n+      \n+      memmove (rgn_bb_table + pos + 1,\n+\t       rgn_bb_table + pos,\n+\t       ((RGN_BLOCKS (nr_regions) - 1) - (pos) + 1)\n+\t       * sizeof (*rgn_bb_table));\n+\n+      rgn_bb_table[pos] = bb->index;\n+      \n+      for (; i <= current_nr_blocks; i++)\n+\tebb_head [i]++;\n+\n+      i = CONTAINING_RGN (after->index);\n+      CONTAINING_RGN (bb->index) = i;\n+      \n+      RGN_HAS_REAL_EBB (i) = 1;\n+\n+      for (++i; i <= nr_regions; i++)\n+\tRGN_BLOCKS (i)++;\n+\n+      /* We don't need to call check_dead_notes1 () because this new block\n+\t is just a split of the old.  We don't want to count anything twice.  */\n+    }\n+}\n+\n+/* Fix internal data after interblock movement of jump instruction.\n+   For parameter meaning please refer to\n+   sched-int.h: struct sched_info: fix_recovery_cfg.  */\n+static void\n+fix_recovery_cfg (int bbi, int check_bbi, int check_bb_nexti)\n+{\n+  int old_pos, new_pos, i;\n+\n+  BLOCK_TO_BB (check_bb_nexti) = BLOCK_TO_BB (bbi);\n+  \n+  for (old_pos = ebb_head[BLOCK_TO_BB (check_bbi) + 1] - 1;\n+       rgn_bb_table[old_pos] != check_bb_nexti;\n+       old_pos--);\n+  gcc_assert (old_pos > ebb_head[BLOCK_TO_BB (check_bbi)]);\n+\n+  for (new_pos = ebb_head[BLOCK_TO_BB (bbi) + 1] - 1;\n+       rgn_bb_table[new_pos] != bbi;\n+       new_pos--);\n+  new_pos++;\n+  gcc_assert (new_pos > ebb_head[BLOCK_TO_BB (bbi)]);\n+  \n+  gcc_assert (new_pos < old_pos);\n+\n+  memmove (rgn_bb_table + new_pos + 1,\n+\t   rgn_bb_table + new_pos,\n+\t   (old_pos - new_pos) * sizeof (*rgn_bb_table));\n+\n+  rgn_bb_table[new_pos] = check_bb_nexti;\n+\n+  for (i = BLOCK_TO_BB (bbi) + 1; i <= BLOCK_TO_BB (check_bbi); i++)\n+    ebb_head[i]++;\n+}\n+\n+/* Return next block in ebb chain.  For parameter meaning please refer to\n+   sched-int.h: struct sched_info: advance_target_bb.  */\n+static basic_block\n+advance_target_bb (basic_block bb, rtx insn)\n+{\n+  if (insn)\n+    return 0;\n+\n+  gcc_assert (BLOCK_TO_BB (bb->index) == target_bb\n+\t      && BLOCK_TO_BB (bb->next_bb->index) == target_bb);\n+  return bb->next_bb;\n+}\n+\n+/* Count and remove death notes in region RGN, which consists of blocks\n+   with indecies in BLOCKS.  */\n+static void\n+check_dead_notes1 (int rgn, sbitmap blocks)\n+{\n+  int b;\n+\n+  sbitmap_zero (blocks);\n+  for (b = RGN_NR_BLOCKS (rgn) - 1; b >= 0; --b)\n+    SET_BIT (blocks, rgn_bb_table[RGN_BLOCKS (rgn) + b]);\n+\n+  deaths_in_region[rgn] = count_or_remove_death_notes (blocks, 1);\n+}\n+\n+#ifdef ENABLE_CHECKING\n+/* Return non zero, if BB is head or leaf (depending of LEAF_P) block in\n+   current region.  For more information please refer to\n+   sched-int.h: struct sched_info: region_head_or_leaf_p.  */\n+static int\n+region_head_or_leaf_p (basic_block bb, int leaf_p)\n+{\n+  if (!leaf_p)    \n+    return bb->index == rgn_bb_table[RGN_BLOCKS (CONTAINING_RGN (bb->index))];\n+  else\n+    {\n+      int i;\n+      edge e;\n+      edge_iterator ei;\n+      \n+      i = CONTAINING_RGN (bb->index);\n+\n+      FOR_EACH_EDGE (e, ei, bb->succs)\n+\tif (CONTAINING_RGN (e->dest->index) == i\n+\t    /* except self-loop.  */\n+\t    && e->dest != bb)\n+\t  return 0;\n+      \n+      return 1;\n+    }\n+}\n+#endif /* ENABLE_CHECKING  */\n+\n #endif\n \f\n static bool"}, {"sha": "690d0a507670bf441e60e048d5470037c20cee52", "filename": "gcc/target-def.h", "status": "modified", "additions": 16, "deletions": 1, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Ftarget-def.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Ftarget-def.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftarget-def.h?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -288,6 +288,14 @@ Foundation, 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.\n #define TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD 0\n #define TARGET_SCHED_DFA_NEW_CYCLE 0\n #define TARGET_SCHED_IS_COSTLY_DEPENDENCE 0\n+#define TARGET_SCHED_ADJUST_COST_2 0\n+#define TARGET_SCHED_H_I_D_EXTENDED 0\n+#define TARGET_SCHED_SPECULATE_INSN 0\n+#define TARGET_SCHED_NEEDS_BLOCK_P 0\n+#define TARGET_SCHED_GEN_CHECK 0\n+#define TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD_SPEC 0\n+#define TARGET_SCHED_SET_SCHED_FLAGS 0\n+\n \n #define TARGET_SCHED\t\t\t\t\t\t\\\n   {TARGET_SCHED_ADJUST_COST,\t\t\t\t\t\\\n@@ -308,7 +316,14 @@ Foundation, 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.\n    TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD,\t\t\\\n    TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD,\t\\\n    TARGET_SCHED_DFA_NEW_CYCLE,\t\t\t\t\t\\\n-   TARGET_SCHED_IS_COSTLY_DEPENDENCE}\n+   TARGET_SCHED_IS_COSTLY_DEPENDENCE,                           \\\n+   TARGET_SCHED_ADJUST_COST_2,                                  \\\n+   TARGET_SCHED_H_I_D_EXTENDED,\t\t\t\t\t\\\n+   TARGET_SCHED_SPECULATE_INSN,                                 \\\n+   TARGET_SCHED_NEEDS_BLOCK_P,                                  \\\n+   TARGET_SCHED_GEN_CHECK,                                      \\\n+   TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD_SPEC, \\\n+   TARGET_SCHED_SET_SCHED_FLAGS}\n \n #define TARGET_VECTORIZE_BUILTIN_MASK_FOR_LOAD 0\n "}, {"sha": "1b768e4f49c7c35a2df721497b5b727ac54ad580", "filename": "gcc/target.h", "status": "modified", "additions": 53, "deletions": 0, "changes": 53, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Ftarget.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/496d7bb03214b7835638fa14d7275e89d3bec954/gcc%2Ftarget.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftarget.h?ref=496d7bb03214b7835638fa14d7275e89d3bec954", "patch": "@@ -51,6 +51,7 @@ Foundation, 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA.\n #include \"insn-modes.h\"\n \n struct stdarg_info;\n+struct spec_info_def;\n \n /* The struct used by the secondary_reload target hook.  */\n typedef struct secondary_reload_info\n@@ -306,6 +307,58 @@ struct gcc_target\n        between the already scheduled insn (first parameter) and the\n        the second insn (second parameter).  */\n     bool (* is_costly_dependence) (rtx, rtx, rtx, int, int);\n+\n+    /* Given the current cost, COST, of an insn, INSN, calculate and\n+       return a new cost based on its relationship to DEP_INSN through the\n+       dependence of type DEP_TYPE.  The default is to make no adjustment.  */\n+    int (* adjust_cost_2) (rtx insn, int, rtx def_insn, int cost);\n+\n+    /* The following member value is a pointer to a function called\n+       by the insn scheduler. This hook is called to notify the backend\n+       that new instructions were emitted.  */\n+    void (* h_i_d_extended) (void);\n+    \n+    /* The following member value is a pointer to a function called\n+       by the insn scheduler.\n+       The first parameter is an instruction, the second parameter is the type\n+       of the requested speculation, and the third parameter is a pointer to the\n+       speculative pattern of the corresponding type (set if return value == 1).\n+       It should return\n+       -1, if there is no pattern, that will satisfy the requested speculation\n+       type,\n+       0, if current pattern satisfies the requested speculation type,\n+       1, if pattern of the instruction should be changed to the newly\n+       generated one.  */\n+    int (* speculate_insn) (rtx, HOST_WIDE_INT, rtx *);\n+\n+    /* The following member value is a pointer to a function called\n+       by the insn scheduler.  It should return true if the check instruction\n+       corresponding to the instruction passed as the parameter needs a\n+       recovery block.  */\n+    bool (* needs_block_p) (rtx);\n+\n+    /* The following member value is a pointer to a function called\n+       by the insn scheduler.  It should return a pattern for the check\n+       instruction.\n+       The first parameter is a speculative instruction, the second parameter\n+       is the label of the corresponding recovery block (or null, if it is a\n+       simple check).  If the mutation of the check is requested (e.g. from\n+       ld.c to chk.a), the third parameter is true - in this case the first\n+       parameter is the previous check.  */\n+    rtx (* gen_check) (rtx, rtx, bool);\n+\n+    /* The following member value is a pointer to a function controlling\n+       what insns from the ready insn queue will be considered for the\n+       multipass insn scheduling.  If the hook returns zero for the insn\n+       passed as the parameter, the insn will not be chosen to be\n+       issued.  This hook is used to discard speculative instructions,\n+       that stand at the first position of the ready list.  */\n+    bool (* first_cycle_multipass_dfa_lookahead_guard_spec) (rtx);\n+\n+    /* The following member value is a pointer to a function that provides\n+       information about the speculation capabilities of the target.\n+       The parameter is a pointer to spec_info variable.  */\n+    void (* set_sched_flags) (struct spec_info_def *);\n   } sched;\n \n   /* Functions relating to vectorization.  */"}]}