{"sha": "b01819be5d7c47c4682dbb250c124e58bd2c1c9e", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YjAxODE5YmU1ZDdjNDdjNDY4MmRiYjI1MGMxMjRlNThiZDJjMWM5ZQ==", "commit": {"author": {"name": "Andi Kleen", "email": "ak@gcc.gnu.org", "date": "2013-03-24T00:23:20Z"}, "committer": {"name": "Andi Kleen", "email": "ak@gcc.gnu.org", "date": "2013-03-24T00:23:20Z"}, "message": "Avoid nonconst memmodels in libitm's local outdated copy of <atomic> too\n\nThis avoids warnings in libitm for non constant memory models,\nfixing the bootstrap with -Werror\n\nPassed bootstrap and test on x86_64-linux.\n\nlibitm/:\n\n2013-03-23  Andi Kleen  <andi@my.domain.org>\n\n\t* local_atomic (__always_inline): Add.\n\t(__calculate_memory_order, atomic_thread_fence,\n\t atomic_signal_fence, test_and_set, clear, store, load,\n         exchange, compare_exchange_weak, compare_exchange_strong,\n         fetch_add, fetch_sub, fetch_and, fetch_or, fetch_xor):\n\tAdd __always_inline to force inlining.\n\nFrom-SVN: r197018", "tree": {"sha": "9e5d5cde21eb5d22f37f3b5dee6ea682b2f1f929", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/9e5d5cde21eb5d22f37f3b5dee6ea682b2f1f929"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/b01819be5d7c47c4682dbb250c124e58bd2c1c9e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b01819be5d7c47c4682dbb250c124e58bd2c1c9e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b01819be5d7c47c4682dbb250c124e58bd2c1c9e", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b01819be5d7c47c4682dbb250c124e58bd2c1c9e/comments", "author": null, "committer": null, "parents": [{"sha": "94f3ccc8d92b9225626e1f69c31b8cc9d109f74d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/94f3ccc8d92b9225626e1f69c31b8cc9d109f74d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/94f3ccc8d92b9225626e1f69c31b8cc9d109f74d"}], "stats": {"total": 302, "additions": 152, "deletions": 150}, "files": [{"sha": "4cd961af9ba3f2248360153571a36b6fc2cd79cd", "filename": "libitm/local_atomic", "status": "modified", "additions": 152, "deletions": 150, "changes": 302, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b01819be5d7c47c4682dbb250c124e58bd2c1c9e/libitm%2Flocal_atomic", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b01819be5d7c47c4682dbb250c124e58bd2c1c9e/libitm%2Flocal_atomic", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Flocal_atomic?ref=b01819be5d7c47c4682dbb250c124e58bd2c1c9e", "patch": "@@ -41,6 +41,10 @@\n #ifndef _GLIBCXX_ATOMIC\n #define _GLIBCXX_ATOMIC 1\n \n+#ifndef __always_inline\n+#define __always_inline inline __attribute__((always_inline))\n+#endif\n+\n // #pragma GCC system_header\n \n // #ifndef __GXX_EXPERIMENTAL_CXX0X__\n@@ -71,7 +75,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n       memory_order_seq_cst\n     } memory_order;\n \n-  inline memory_order\n+  __always_inline memory_order\n   __calculate_memory_order(memory_order __m) noexcept\n   {\n     const bool __cond1 = __m == memory_order_release;\n@@ -81,13 +85,13 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n     return __mo2;\n   }\n \n-  inline void\n+  __always_inline void\n   atomic_thread_fence(memory_order __m) noexcept\n   {\n     __atomic_thread_fence (__m);\n   }\n \n-  inline void\n+  __always_inline void\n   atomic_signal_fence(memory_order __m) noexcept\n   {\n     __atomic_thread_fence (__m);\n@@ -277,19 +281,19 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n     // Conversion to ATOMIC_FLAG_INIT.\n     atomic_flag(bool __i) noexcept : __atomic_flag_base({ __i }) { }\n \n-    bool\n+    __always_inline bool\n     test_and_set(memory_order __m = memory_order_seq_cst) noexcept\n     {\n       return __atomic_test_and_set (&_M_i, __m);\n     }\n \n-    bool\n+    __always_inline bool\n     test_and_set(memory_order __m = memory_order_seq_cst) volatile noexcept\n     {\n       return __atomic_test_and_set (&_M_i, __m);\n     }\n \n-    void\n+    __always_inline void\n     clear(memory_order __m = memory_order_seq_cst) noexcept\n     {\n       // __glibcxx_assert(__m != memory_order_consume);\n@@ -299,7 +303,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n       __atomic_clear (&_M_i, __m);\n     }\n \n-    void\n+    __always_inline void\n     clear(memory_order __m = memory_order_seq_cst) volatile noexcept\n     {\n       // __glibcxx_assert(__m != memory_order_consume);\n@@ -452,7 +456,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n       is_lock_free() const volatile noexcept\n       { return __atomic_is_lock_free (sizeof (_M_i), &_M_i); }\n \n-      void\n+      __always_inline void\n       store(__int_type __i, memory_order __m = memory_order_seq_cst) noexcept\n       {\n \t// __glibcxx_assert(__m != memory_order_acquire);\n@@ -462,7 +466,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \t__atomic_store_n(&_M_i, __i, __m);\n       }\n \n-      void\n+      __always_inline void\n       store(__int_type __i,\n \t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n@@ -473,7 +477,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \t__atomic_store_n(&_M_i, __i, __m);\n       }\n \n-      __int_type\n+      __always_inline __int_type\n       load(memory_order __m = memory_order_seq_cst) const noexcept\n       {\n \t// __glibcxx_assert(__m != memory_order_release);\n@@ -482,7 +486,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_load_n(&_M_i, __m);\n       }\n \n-      __int_type\n+      __always_inline __int_type\n       load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n       {\n \t// __glibcxx_assert(__m != memory_order_release);\n@@ -491,22 +495,21 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_load_n(&_M_i, __m);\n       }\n \n-      __int_type\n+      __always_inline __int_type\n       exchange(__int_type __i,\n \t       memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn __atomic_exchange_n(&_M_i, __i, __m);\n       }\n \n-\n-      __int_type\n+      __always_inline __int_type\n       exchange(__int_type __i,\n \t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn __atomic_exchange_n(&_M_i, __i, __m);\n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n \t\t\t    memory_order __m1, memory_order __m2) noexcept\n       {\n@@ -517,7 +520,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 1, __m1, __m2);\n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n \t\t\t    memory_order __m1,\n \t\t\t    memory_order __m2) volatile noexcept\n@@ -529,23 +532,23 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 1, __m1, __m2);\n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n \t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn compare_exchange_weak(__i1, __i2, __m,\n \t\t\t\t     __calculate_memory_order(__m));\n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n \t\t   memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn compare_exchange_weak(__i1, __i2, __m,\n \t\t\t\t     __calculate_memory_order(__m));\n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n \t\t\t      memory_order __m1, memory_order __m2) noexcept\n       {\n@@ -556,7 +559,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 0, __m1, __m2);\n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n \t\t\t      memory_order __m1,\n \t\t\t      memory_order __m2) volatile noexcept\n@@ -568,68 +571,68 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_compare_exchange_n(&_M_i, &__i1, __i2, 0, __m1, __m2);\n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n \t\t\t      memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn compare_exchange_strong(__i1, __i2, __m,\n \t\t\t\t       __calculate_memory_order(__m));\n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n \t\t memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn compare_exchange_strong(__i1, __i2, __m,\n \t\t\t\t       __calculate_memory_order(__m));\n       }\n \n-      __int_type\n+      __always_inline __int_type\n       fetch_add(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_add(&_M_i, __i, __m); }\n \n-      __int_type\n+      __always_inline __int_type\n       fetch_add(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_add(&_M_i, __i, __m); }\n \n-      __int_type\n+      __always_inline __int_type\n       fetch_sub(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_sub(&_M_i, __i, __m); }\n \n-      __int_type\n+      __always_inline __int_type\n       fetch_sub(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_sub(&_M_i, __i, __m); }\n \n-      __int_type\n+      __always_inline __int_type\n       fetch_and(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_and(&_M_i, __i, __m); }\n \n-      __int_type\n+      __always_inline __int_type\n       fetch_and(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_and(&_M_i, __i, __m); }\n \n-      __int_type\n+      __always_inline __int_type\n       fetch_or(__int_type __i,\n \t       memory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_or(&_M_i, __i, __m); }\n \n-      __int_type\n+      __always_inline __int_type\n       fetch_or(__int_type __i,\n \t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_or(&_M_i, __i, __m); }\n \n-      __int_type\n+      __always_inline __int_type\n       fetch_xor(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_xor(&_M_i, __i, __m); }\n \n-      __int_type\n+      __always_inline __int_type\n       fetch_xor(__int_type __i,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_xor(&_M_i, __i, __m); }\n@@ -731,7 +734,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n       is_lock_free() const volatile noexcept\n       { return __atomic_is_lock_free (sizeof (_M_p), &_M_p); }\n \n-      void\n+      __always_inline void\n       store(__pointer_type __p,\n \t    memory_order __m = memory_order_seq_cst) noexcept\n       {\n@@ -742,7 +745,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \t__atomic_store_n(&_M_p, __p, __m);\n       }\n \n-      void\n+      __always_inline void\n       store(__pointer_type __p,\n \t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n@@ -753,7 +756,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \t__atomic_store_n(&_M_p, __p, __m);\n       }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       load(memory_order __m = memory_order_seq_cst) const noexcept\n       {\n \t// __glibcxx_assert(__m != memory_order_release);\n@@ -762,7 +765,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_load_n(&_M_p, __m);\n       }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n       {\n \t// __glibcxx_assert(__m != memory_order_release);\n@@ -771,22 +774,21 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_load_n(&_M_p, __m);\n       }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       exchange(__pointer_type __p,\n \t       memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn __atomic_exchange_n(&_M_p, __p, __m);\n       }\n \n-\n-      __pointer_type\n+      __always_inline __pointer_type\n       exchange(__pointer_type __p,\n \t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn __atomic_exchange_n(&_M_p, __p, __m);\n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t      memory_order __m1,\n \t\t\t      memory_order __m2) noexcept\n@@ -798,7 +800,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_compare_exchange_n(&_M_p, &__p1, __p2, 0, __m1, __m2);\n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t      memory_order __m1,\n \t\t\t      memory_order __m2) volatile noexcept\n@@ -810,22 +812,22 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn __atomic_compare_exchange_n(&_M_p, &__p1, __p2, 0, __m1, __m2);\n       }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       fetch_add(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_add(&_M_p, __d, __m); }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       fetch_add(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_add(&_M_p, __d, __m); }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       fetch_sub(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __atomic_fetch_sub(&_M_p, __d, __m); }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       fetch_sub(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __atomic_fetch_sub(&_M_p, __d, __m); }\n@@ -869,67 +871,67 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n     bool\n     is_lock_free() const volatile noexcept { return _M_base.is_lock_free(); }\n \n-    void\n+    __always_inline void\n     store(bool __i, memory_order __m = memory_order_seq_cst) noexcept\n     { _M_base.store(__i, __m); }\n \n-    void\n+    __always_inline void\n     store(bool __i, memory_order __m = memory_order_seq_cst) volatile noexcept\n     { _M_base.store(__i, __m); }\n \n-    bool\n+    __always_inline bool\n     load(memory_order __m = memory_order_seq_cst) const noexcept\n     { return _M_base.load(__m); }\n \n-    bool\n+    __always_inline bool\n     load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n     { return _M_base.load(__m); }\n \n-    bool\n+    __always_inline bool\n     exchange(bool __i, memory_order __m = memory_order_seq_cst) noexcept\n     { return _M_base.exchange(__i, __m); }\n \n-    bool\n+    __always_inline bool\n     exchange(bool __i,\n \t     memory_order __m = memory_order_seq_cst) volatile noexcept\n     { return _M_base.exchange(__i, __m); }\n \n-    bool\n+    __always_inline bool\n     compare_exchange_weak(bool& __i1, bool __i2, memory_order __m1,\n \t\t\t  memory_order __m2) noexcept\n     { return _M_base.compare_exchange_weak(__i1, __i2, __m1, __m2); }\n \n-    bool\n+    __always_inline bool\n     compare_exchange_weak(bool& __i1, bool __i2, memory_order __m1,\n \t\t\t  memory_order __m2) volatile noexcept\n     { return _M_base.compare_exchange_weak(__i1, __i2, __m1, __m2); }\n \n-    bool\n+    __always_inline bool\n     compare_exchange_weak(bool& __i1, bool __i2,\n \t\t\t  memory_order __m = memory_order_seq_cst) noexcept\n     { return _M_base.compare_exchange_weak(__i1, __i2, __m); }\n \n-    bool\n+    __always_inline bool\n     compare_exchange_weak(bool& __i1, bool __i2,\n \t\t     memory_order __m = memory_order_seq_cst) volatile noexcept\n     { return _M_base.compare_exchange_weak(__i1, __i2, __m); }\n \n-    bool\n+    __always_inline bool\n     compare_exchange_strong(bool& __i1, bool __i2, memory_order __m1,\n \t\t\t    memory_order __m2) noexcept\n     { return _M_base.compare_exchange_strong(__i1, __i2, __m1, __m2); }\n \n-    bool\n+    __always_inline bool\n     compare_exchange_strong(bool& __i1, bool __i2, memory_order __m1,\n \t\t\t    memory_order __m2) volatile noexcept\n     { return _M_base.compare_exchange_strong(__i1, __i2, __m1, __m2); }\n \n-    bool\n+    __always_inline bool\n     compare_exchange_strong(bool& __i1, bool __i2,\n \t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n     { return _M_base.compare_exchange_strong(__i1, __i2, __m); }\n \n-    bool\n+    __always_inline bool\n     compare_exchange_strong(bool& __i1, bool __i2,\n \t\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n     { return _M_base.compare_exchange_strong(__i1, __i2, __m); }\n@@ -979,35 +981,35 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n       store(_Tp __i, memory_order _m = memory_order_seq_cst) noexcept\n       { __atomic_store(&_M_i, &__i, _m); }\n \n-      void\n+      __always_inline void\n       store(_Tp __i, memory_order _m = memory_order_seq_cst) volatile noexcept\n       { __atomic_store(&_M_i, &__i, _m); }\n \n-      _Tp\n+      __always_inline _Tp\n       load(memory_order _m = memory_order_seq_cst) const noexcept\n       { \n         _Tp tmp;\n \t__atomic_load(&_M_i, &tmp, _m); \n \treturn tmp;\n       }\n \n-      _Tp\n+      __always_inline _Tp\n       load(memory_order _m = memory_order_seq_cst) const volatile noexcept\n       { \n         _Tp tmp;\n \t__atomic_load(&_M_i, &tmp, _m); \n \treturn tmp;\n       }\n \n-      _Tp\n+      __always_inline _Tp\n       exchange(_Tp __i, memory_order _m = memory_order_seq_cst) noexcept\n       { \n         _Tp tmp;\n \t__atomic_exchange(&_M_i, &__i, &tmp, _m); \n \treturn tmp;\n       }\n \n-      _Tp\n+      __always_inline _Tp\n       exchange(_Tp __i, \n \t       memory_order _m = memory_order_seq_cst) volatile noexcept\n       { \n@@ -1016,50 +1018,50 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \treturn tmp;\n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_weak(_Tp& __e, _Tp __i, memory_order __s, \n \t\t\t    memory_order __f) noexcept\n       {\n \treturn __atomic_compare_exchange(&_M_i, &__e, &__i, true, __s, __f); \n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_weak(_Tp& __e, _Tp __i, memory_order __s, \n \t\t\t    memory_order __f) volatile noexcept\n       {\n \treturn __atomic_compare_exchange(&_M_i, &__e, &__i, true, __s, __f); \n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_weak(_Tp& __e, _Tp __i,\n \t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n       { return compare_exchange_weak(__e, __i, __m, __m); }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_weak(_Tp& __e, _Tp __i,\n \t\t     memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return compare_exchange_weak(__e, __i, __m, __m); }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_strong(_Tp& __e, _Tp __i, memory_order __s, \n \t\t\t      memory_order __f) noexcept\n       {\n \treturn __atomic_compare_exchange(&_M_i, &__e, &__i, false, __s, __f); \n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_strong(_Tp& __e, _Tp __i, memory_order __s, \n \t\t\t      memory_order __f) volatile noexcept\n       {\n \treturn __atomic_compare_exchange(&_M_i, &__e, &__i, false, __s, __f); \n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_strong(_Tp& __e, _Tp __i,\n \t\t\t       memory_order __m = memory_order_seq_cst) noexcept\n       { return compare_exchange_strong(__e, __i, __m, __m); }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_strong(_Tp& __e, _Tp __i,\n \t\t     memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return compare_exchange_strong(__e, __i, __m, __m); }\n@@ -1152,104 +1154,104 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n       is_lock_free() const volatile noexcept\n       { return _M_b.is_lock_free(); }\n \n-      void\n+      __always_inline void\n       store(__pointer_type __p,\n \t    memory_order __m = memory_order_seq_cst) noexcept\n       { return _M_b.store(__p, __m); }\n \n-      void\n+      __always_inline void\n       store(__pointer_type __p,\n \t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _M_b.store(__p, __m); }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       load(memory_order __m = memory_order_seq_cst) const noexcept\n       { return _M_b.load(__m); }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n       { return _M_b.load(__m); }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       exchange(__pointer_type __p,\n \t       memory_order __m = memory_order_seq_cst) noexcept\n       { return _M_b.exchange(__p, __m); }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       exchange(__pointer_type __p,\n \t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _M_b.exchange(__p, __m); }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t    memory_order __m1, memory_order __m2) noexcept\n       { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t    memory_order __m1,\n \t\t\t    memory_order __m2) volatile noexcept\n       { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn compare_exchange_weak(__p1, __p2, __m,\n \t\t\t\t     __calculate_memory_order(__m));\n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n \t\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn compare_exchange_weak(__p1, __p2, __m,\n \t\t\t\t     __calculate_memory_order(__m));\n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t      memory_order __m1, memory_order __m2) noexcept\n       { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t      memory_order __m1,\n \t\t\t      memory_order __m2) volatile noexcept\n       { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n \t\t\t      memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn _M_b.compare_exchange_strong(__p1, __p2, __m,\n \t\t\t\t\t    __calculate_memory_order(__m));\n       }\n \n-      bool\n+      __always_inline bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n \t\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn _M_b.compare_exchange_strong(__p1, __p2, __m,\n \t\t\t\t\t    __calculate_memory_order(__m));\n       }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       fetch_add(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return _M_b.fetch_add(__d, __m); }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       fetch_add(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _M_b.fetch_add(__d, __m); }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       fetch_sub(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return _M_b.fetch_sub(__d, __m); }\n \n-      __pointer_type\n+      __always_inline __pointer_type\n       fetch_sub(ptrdiff_t __d,\n \t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _M_b.fetch_sub(__d, __m); }\n@@ -1543,122 +1545,122 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \n \n   // Function definitions, atomic_flag operations.\n-  inline bool\n+  __always_inline bool\n   atomic_flag_test_and_set_explicit(atomic_flag* __a,\n \t\t\t\t    memory_order __m) noexcept\n   { return __a->test_and_set(__m); }\n \n-  inline bool\n+  __always_inline bool\n   atomic_flag_test_and_set_explicit(volatile atomic_flag* __a,\n \t\t\t\t    memory_order __m) noexcept\n   { return __a->test_and_set(__m); }\n \n-  inline void\n+  __always_inline void\n   atomic_flag_clear_explicit(atomic_flag* __a, memory_order __m) noexcept\n   { __a->clear(__m); }\n \n-  inline void\n+  __always_inline void\n   atomic_flag_clear_explicit(volatile atomic_flag* __a,\n \t\t\t     memory_order __m) noexcept\n   { __a->clear(__m); }\n \n-  inline bool\n+  __always_inline bool\n   atomic_flag_test_and_set(atomic_flag* __a) noexcept\n   { return atomic_flag_test_and_set_explicit(__a, memory_order_seq_cst); }\n \n-  inline bool\n+  __always_inline bool\n   atomic_flag_test_and_set(volatile atomic_flag* __a) noexcept\n   { return atomic_flag_test_and_set_explicit(__a, memory_order_seq_cst); }\n \n-  inline void\n+  __always_inline void\n   atomic_flag_clear(atomic_flag* __a) noexcept\n   { atomic_flag_clear_explicit(__a, memory_order_seq_cst); }\n \n-  inline void\n+  __always_inline void\n   atomic_flag_clear(volatile atomic_flag* __a) noexcept\n   { atomic_flag_clear_explicit(__a, memory_order_seq_cst); }\n \n \n   // Function templates generally applicable to atomic types.\n   template<typename _ITp>\n-    inline bool\n+    __always_inline bool\n     atomic_is_lock_free(const atomic<_ITp>* __a) noexcept\n     { return __a->is_lock_free(); }\n \n   template<typename _ITp>\n-    inline bool\n+    __always_inline bool\n     atomic_is_lock_free(const volatile atomic<_ITp>* __a) noexcept\n     { return __a->is_lock_free(); }\n \n   template<typename _ITp>\n-    inline void\n+    __always_inline void\n     atomic_init(atomic<_ITp>* __a, _ITp __i) noexcept;\n \n   template<typename _ITp>\n-    inline void\n+    __always_inline void\n     atomic_init(volatile atomic<_ITp>* __a, _ITp __i) noexcept;\n \n   template<typename _ITp>\n-    inline void\n+    __always_inline void\n     atomic_store_explicit(atomic<_ITp>* __a, _ITp __i,\n \t\t\t  memory_order __m) noexcept\n     { __a->store(__i, __m); }\n \n   template<typename _ITp>\n-    inline void\n+    __always_inline void\n     atomic_store_explicit(volatile atomic<_ITp>* __a, _ITp __i,\n \t\t\t  memory_order __m) noexcept\n     { __a->store(__i, __m); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_load_explicit(const atomic<_ITp>* __a, memory_order __m) noexcept\n     { return __a->load(__m); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_load_explicit(const volatile atomic<_ITp>* __a,\n \t\t\t memory_order __m) noexcept\n     { return __a->load(__m); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_exchange_explicit(atomic<_ITp>* __a, _ITp __i,\n \t\t\t     memory_order __m) noexcept\n     { return __a->exchange(__i, __m); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_exchange_explicit(volatile atomic<_ITp>* __a, _ITp __i,\n \t\t\t     memory_order __m) noexcept\n     { return __a->exchange(__i, __m); }\n \n   template<typename _ITp>\n-    inline bool\n+    __always_inline bool\n     atomic_compare_exchange_weak_explicit(atomic<_ITp>* __a,\n \t\t\t\t\t  _ITp* __i1, _ITp __i2,\n \t\t\t\t\t  memory_order __m1,\n \t\t\t\t\t  memory_order __m2) noexcept\n     { return __a->compare_exchange_weak(*__i1, __i2, __m1, __m2); }\n \n   template<typename _ITp>\n-    inline bool\n+    __always_inline bool\n     atomic_compare_exchange_weak_explicit(volatile atomic<_ITp>* __a,\n \t\t\t\t\t  _ITp* __i1, _ITp __i2,\n \t\t\t\t\t  memory_order __m1,\n \t\t\t\t\t  memory_order __m2) noexcept\n     { return __a->compare_exchange_weak(*__i1, __i2, __m1, __m2); }\n \n   template<typename _ITp>\n-    inline bool\n+    __always_inline bool\n     atomic_compare_exchange_strong_explicit(atomic<_ITp>* __a,\n \t\t\t\t\t    _ITp* __i1, _ITp __i2,\n \t\t\t\t\t    memory_order __m1,\n \t\t\t\t\t    memory_order __m2) noexcept\n     { return __a->compare_exchange_strong(*__i1, __i2, __m1, __m2); }\n \n   template<typename _ITp>\n-    inline bool\n+    __always_inline bool\n     atomic_compare_exchange_strong_explicit(volatile atomic<_ITp>* __a,\n \t\t\t\t\t    _ITp* __i1, _ITp __i2,\n \t\t\t\t\t    memory_order __m1,\n@@ -1667,37 +1669,37 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n \n \n   template<typename _ITp>\n-    inline void\n+    __always_inline void\n     atomic_store(atomic<_ITp>* __a, _ITp __i) noexcept\n     { atomic_store_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline void\n+    __always_inline void\n     atomic_store(volatile atomic<_ITp>* __a, _ITp __i) noexcept\n     { atomic_store_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_load(const atomic<_ITp>* __a) noexcept\n     { return atomic_load_explicit(__a, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_load(const volatile atomic<_ITp>* __a) noexcept\n     { return atomic_load_explicit(__a, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_exchange(atomic<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_exchange_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_exchange(volatile atomic<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_exchange_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline bool\n+    __always_inline bool\n     atomic_compare_exchange_weak(atomic<_ITp>* __a,\n \t\t\t\t _ITp* __i1, _ITp __i2) noexcept\n     {\n@@ -1707,7 +1709,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n     }\n \n   template<typename _ITp>\n-    inline bool\n+    __always_inline bool\n     atomic_compare_exchange_weak(volatile atomic<_ITp>* __a,\n \t\t\t\t _ITp* __i1, _ITp __i2) noexcept\n     {\n@@ -1717,7 +1719,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n     }\n \n   template<typename _ITp>\n-    inline bool\n+    __always_inline bool\n     atomic_compare_exchange_strong(atomic<_ITp>* __a,\n \t\t\t\t   _ITp* __i1, _ITp __i2) noexcept\n     {\n@@ -1727,7 +1729,7 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n     }\n \n   template<typename _ITp>\n-    inline bool\n+    __always_inline bool\n     atomic_compare_exchange_strong(volatile atomic<_ITp>* __a,\n \t\t\t\t   _ITp* __i1, _ITp __i2) noexcept\n     {\n@@ -1741,158 +1743,158 @@ namespace std // _GLIBCXX_VISIBILITY(default)\n   // intergral types as specified in the standard, excluding address\n   // types.\n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_add_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_add(__i, __m); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_add_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_add(__i, __m); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_sub_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_sub(__i, __m); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_sub_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_sub(__i, __m); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_and_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_and(__i, __m); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_and_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_and(__i, __m); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_or_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t     memory_order __m) noexcept\n     { return __a->fetch_or(__i, __m); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_or_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t     memory_order __m) noexcept\n     { return __a->fetch_or(__i, __m); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_xor_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_xor(__i, __m); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_xor_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_xor(__i, __m); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_add(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_add_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_add(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_add_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_sub(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_sub_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_sub(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_sub_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_and(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_and_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_and(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_and_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_or(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_or_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_or(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_or_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_xor(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_xor_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n-    inline _ITp\n+    __always_inline _ITp\n     atomic_fetch_xor(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_xor_explicit(__a, __i, memory_order_seq_cst); }\n \n \n   // Partial specializations for pointers.\n   template<typename _ITp>\n-    inline _ITp*\n+    __always_inline _ITp*\n     atomic_fetch_add_explicit(atomic<_ITp*>* __a, ptrdiff_t __d,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_add(__d, __m); }\n \n   template<typename _ITp>\n-    inline _ITp*\n+    __always_inline _ITp*\n     atomic_fetch_add_explicit(volatile atomic<_ITp*>* __a, ptrdiff_t __d,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_add(__d, __m); }\n \n   template<typename _ITp>\n-    inline _ITp*\n+    __always_inline _ITp*\n     atomic_fetch_add(volatile atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n     { return __a->fetch_add(__d); }\n \n   template<typename _ITp>\n-    inline _ITp*\n+    __always_inline _ITp*\n     atomic_fetch_add(atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n     { return __a->fetch_add(__d); }\n \n   template<typename _ITp>\n-    inline _ITp*\n+    __always_inline _ITp*\n     atomic_fetch_sub_explicit(volatile atomic<_ITp*>* __a,\n \t\t\t      ptrdiff_t __d, memory_order __m) noexcept\n     { return __a->fetch_sub(__d, __m); }\n \n   template<typename _ITp>\n-    inline _ITp*\n+    __always_inline _ITp*\n     atomic_fetch_sub_explicit(atomic<_ITp*>* __a, ptrdiff_t __d,\n \t\t\t      memory_order __m) noexcept\n     { return __a->fetch_sub(__d, __m); }\n \n   template<typename _ITp>\n-    inline _ITp*\n+    __always_inline _ITp*\n     atomic_fetch_sub(volatile atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n     { return __a->fetch_sub(__d); }\n \n   template<typename _ITp>\n-    inline _ITp*\n+    __always_inline _ITp*\n     atomic_fetch_sub(atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n     { return __a->fetch_sub(__d); }\n   // @} group atomics"}]}