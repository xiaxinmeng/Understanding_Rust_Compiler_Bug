{"sha": "21341cfd7f4f84bfc9c90d6480d4a6b8072279f0", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MjEzNDFjZmQ3ZjRmODRiZmM5YzkwZDY0ODBkNGE2YjgwNzIyNzlmMA==", "commit": {"author": {"name": "Alex Samuel", "email": "samuel@codesourcery.com", "date": "1999-09-23T20:00:57Z"}, "committer": {"name": "Richard Henderson", "email": "rth@gcc.gnu.org", "date": "1999-09-23T20:00:57Z"}, "message": "ggc-page.c: New file.\n\n        * ggc-page.c: New file.\n        * Makefile.in (ggc-page.o): New.\n\nCo-Authored-By: Richard Henderson <rth@cygnus.com>\n\nFrom-SVN: r29632", "tree": {"sha": "09e6ec7b681458bfb09294189b50618f9cb4799a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/09e6ec7b681458bfb09294189b50618f9cb4799a"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/21341cfd7f4f84bfc9c90d6480d4a6b8072279f0", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/21341cfd7f4f84bfc9c90d6480d4a6b8072279f0", "html_url": "https://github.com/Rust-GCC/gccrs/commit/21341cfd7f4f84bfc9c90d6480d4a6b8072279f0", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/21341cfd7f4f84bfc9c90d6480d4a6b8072279f0/comments", "author": null, "committer": null, "parents": [{"sha": "b6d24183dfb4a8194e3b0357ce44723735ad0da1", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b6d24183dfb4a8194e3b0357ce44723735ad0da1", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b6d24183dfb4a8194e3b0357ce44723735ad0da1"}], "stats": {"total": 1094, "additions": 1093, "deletions": 1}, "files": [{"sha": "be99a12754772ba0fdeea5aebb90ad574d293db5", "filename": "gcc/ChangeLog", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/21341cfd7f4f84bfc9c90d6480d4a6b8072279f0/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/21341cfd7f4f84bfc9c90d6480d4a6b8072279f0/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=21341cfd7f4f84bfc9c90d6480d4a6b8072279f0", "patch": "@@ -1,3 +1,9 @@\n+Thu Sep 23 12:59:14 1999  Alex Samuel  <samuel@codesourcery.com>\n+\t\t\t  Richard Henderson  <rth@cygnus.com>\n+\n+\t* ggc-page.c: New file.\n+\t* Makefile.in (ggc-page.o): New.\n+\n Thu Sep 23 13:55:21 1999  Jeffrey A Law  (law@cygnus.com)\n \n \t* invoke.texi: Document -fdelete-null-pointer-checks"}, {"sha": "1653f0505b2a40a0e194013793f1a7b185fe9724", "filename": "gcc/Makefile.in", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/21341cfd7f4f84bfc9c90d6480d4a6b8072279f0/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/21341cfd7f4f84bfc9c90d6480d4a6b8072279f0/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=21341cfd7f4f84bfc9c90d6480d4a6b8072279f0", "patch": "@@ -308,7 +308,7 @@ CLIB=\n OBSTACK=obstack.o\n \n # The GC method to be used on this system.\n-GGC=ggc-simple.o\n+GGC=ggc-page.o\n \n # If a supplementary library is being used for the GC.\n GGC_LIB=\n@@ -1433,6 +1433,9 @@ ggc-common.o: ggc-common.c $(CONFIG_H) $(RTL_BASE_H) $(TREE_H) \\\n ggc-simple.o: ggc-simple.c $(CONFIG_H) $(RTL_BASE_H) $(TREE_H) flags.h \\\n \tggc.h varray.h hash.h\n \n+ggc-page.o: ggc-page.c $(CONFIG_H) $(RTL_BASE_H) $(TREE_H) flags.h \\\n+\tggc.h varray.h hash.h\n+\n ggc-none.o: ggc-none.c $(CONFIG_H) $(RTL_BASE_H) ggc.h\n \n ggc-callbacks.o: ggc-callbacks.c $(CONFIG_H) $(RTL_BASE_H) $(TREE_H) ggc.h"}, {"sha": "b5a28990e423231112c1449ead2c7fc6c9d8f2c6", "filename": "gcc/ggc-page.c", "status": "added", "additions": 1083, "deletions": 0, "changes": 1083, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/21341cfd7f4f84bfc9c90d6480d4a6b8072279f0/gcc%2Fggc-page.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/21341cfd7f4f84bfc9c90d6480d4a6b8072279f0/gcc%2Fggc-page.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fggc-page.c?ref=21341cfd7f4f84bfc9c90d6480d4a6b8072279f0", "patch": "@@ -0,0 +1,1083 @@\n+/* \"Bag-of-pages\" garbage collector for the GNU compiler.\n+   Copyright (C) 1999 Free Software Foundation, Inc.\n+\n+   This file is part of GNU CC.\n+\n+   GNU CC is free software; you can redistribute it and/or modify\n+   it under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 2, or (at your option)\n+   any later version.\n+\n+   GNU CC is distributed in the hope that it will be useful,\n+   but WITHOUT ANY WARRANTY; without even the implied warranty of\n+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+   GNU General Public License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GNU CC; see the file COPYING.  If not, write to\n+   the Free Software Foundation, 59 Temple Place - Suite 330,\n+   Boston, MA 02111-1307, USA.  */\n+\n+#include <unistd.h>\n+#include <stdio.h>\n+#include <sys/mman.h>\n+#include <fcntl.h>\n+\n+#include \"config.h\"\n+#include \"ggc.h\"\n+#include \"rtl.h\"\n+#include \"system.h\"\n+#include \"tree.h\"\n+#include \"varray.h\"\n+#include \"flags.h\"\n+\n+\n+/* Stategy: \n+\n+   This garbage-collecting allocator allocates objects on one of a set\n+   of pages.  Each page can allocate objects of a single size only;\n+   available sizes are powers of two starting at four bytes.  The size\n+   of an allocation request is rounded up to the next power of two\n+   (`order'), and satisfied from the appropriate page.\n+\n+   Each page is recorded in a page-entry, which also maintains an\n+   in-use bitmap of object positions on the page.  This allows the\n+   allocation state of a particular object to be flipped without\n+   touching the page itself.\n+\n+   Each page-entry also has a context depth, which is used to track\n+   pushing and popping of allocation contexts.  Only objects allocated\n+   in the current (highest-numbered) context may be collected.  \n+\n+   Page entries are arranged in an array of singly-linked lists.  The\n+   array is indexed by the allocation size, in bits, of the pages on\n+   it; i.e. all pages on a list allocate objects of the same size.\n+   Pages are ordered on the list such that all non-full pages precede\n+   all full pages, with non-full pages arranged in order of decreasing\n+   context depth.\n+\n+   Empty pages (of all orders) are kept on a single page cache list,\n+   and are considered first when new pages are required; they are\n+   deallocated at the start of the next collection if they haven't\n+   been recycled by then.  */\n+\n+\n+/* Define GGC_POISON to poison memory marked unused by the collector.  */\n+#undef GGC_POISON\n+\n+/* Define GGC_ALWAYS_COLLECT to perform collection every time\n+   ggc_collect is invoked.  Otherwise, collection is performed only\n+   when a significant amount of memory has been allocated since the\n+   last collection.  */\n+#undef GGC_ALWAYS_COLLECT.\n+\n+/* If ENABLE_CHECKING is defined, enable GGC_POISON and\n+   GGC_ALWAYS_COLLECT automatically.  */\n+#ifdef ENABLE_CHECKING\n+#define GGC_POISON\n+#define GGC_ALWAYS_COLLECT\n+#endif\n+\n+/* Define GGC_DEBUG_LEVEL to print debugging information.\n+     0: No debugging output.\n+     1: GC statistics only.\n+     2: Page-entry allocations/deallocations as well.\n+     3: Object allocations as well.\n+     4: Object marks as well.   */\n+#define GGC_DEBUG_LEVEL (0)\n+\f\n+#ifndef HOST_BITS_PER_PTR\n+#define HOST_BITS_PER_PTR  HOST_BITS_PER_LONG\n+#endif\n+\n+/* Timing information for collect execution goes into here.  */\n+extern int gc_time;\n+\n+/* The \"\" allocated string.  */\n+char *empty_string;\n+\f\n+/* A two-level tree is used to look up the page-entry for a given\n+   pointer.  Two chunks of the pointer's bits are extracted to index\n+   the first and second levels of the tree, as follows:\n+\n+\t\t\t\t   HOST_PAGE_SIZE_BITS\n+\t\t\t   32\t\t|      |\n+       msb +----------------+----+------+------+ lsb\n+\t\t\t    |    |      |\n+\t\t\t PAGE_L1_BITS   |\n+\t\t\t\t |      |\n+\t\t\t       PAGE_L2_BITS\n+\n+   The bottommost HOST_PAGE_SIZE_BITS are ignored, since page-entry\n+   pages are aligned on system page boundaries.  The next most\n+   significant PAGE_L2_BITS and PAGE_L1_BITS are the second and first\n+   index values in the lookup table, respectively.  \n+\n+   The topmost leftover bits, if any, are ignored.  For 32-bit\n+   architectures and the settings below, there are no leftover bits.\n+   For architectures with wider pointers, the lookup tree points to a\n+   list of pages, which must be scanned to find the correct one.  */\n+\n+#define PAGE_L1_BITS\t(8)\n+#define PAGE_L2_BITS\t(32 - PAGE_L1_BITS - G.lg_pagesize)\n+#define PAGE_L1_SIZE\t((size_t) 1 << PAGE_L1_BITS)\n+#define PAGE_L2_SIZE\t((size_t) 1 << PAGE_L2_BITS)\n+\n+#define LOOKUP_L1(p) \\\n+  (((size_t) (p) >> (32 - PAGE_L1_BITS)) & ((1 << PAGE_L1_BITS) - 1))\n+\n+#define LOOKUP_L2(p) \\\n+  (((size_t) (p) >> G.lg_pagesize) & ((1 << PAGE_L2_BITS) - 1))\n+\n+\n+/* A page_entry records the status of an allocation page.  This\n+   structure is dynamically sized to fit the bitmap in_use_p.  */\n+typedef struct page_entry \n+{\n+  /* The next page-entry with objects of the same size, or NULL if\n+     this is the last page-entry.  */\n+  struct page_entry *next;\n+\n+  /* The number of bytes allocated.  (This will always be a multiple\n+     of the host system page size.)  */\n+  size_t bytes;\n+\n+  /* The address at which the memory is allocated.  */\n+  char *page;\n+\n+  /* Saved in-use bit vector for pages that aren't in the topmost\n+     context during collection.  */\n+  unsigned long *save_in_use_p;\n+\n+  /* Context depth of this page.  */\n+  unsigned char context_depth;\n+\n+  /* The lg of size of objects allocated from this page.  */\n+  unsigned char order;\n+\n+  /* The number of free objects remaining on this page.  */\n+  unsigned short num_free_objects;\n+\n+  /* A likely candidate for the bit position of a free object for the\n+     next allocation from this page.  */\n+  unsigned short next_bit_hint;\n+\n+  /* Saved number of free objects for pages that aren't in the topmost\n+     context during colleciton.  */\n+  unsigned short save_num_free_objects;\n+\n+  /* A bit vector indicating whether or not objects are in use.  The\n+     Nth bit is one if the Nth object on this page is allocated.  This\n+     array is dynamically sized.  */\n+  unsigned long in_use_p[1];\n+} page_entry;\n+\n+\n+#if HOST_BITS_PER_PTR <= 32\n+\n+/* On 32-bit hosts, we use a two level page table, as pictured above.  */\n+typedef page_entry **page_table[PAGE_L1_SIZE];\n+\n+#else\n+\n+/* On 64-bit hosts, we use two level page tables plus a linked list\n+   that disambiguates the top 32-bits.  There will almost always be\n+   exactly one entry in the list.  */\n+typedef struct page_table_chain\n+{\n+  struct page_table_chain *next;\n+  size_t high_bits;\n+  page_entry **table[PAGE_L1_SIZE];\n+} *page_table;\n+\n+#endif\n+\n+/* The rest of the global variables.  */\n+static struct globals\n+{\n+  /* The Nth element in this array is a page with objects of size 2^N.\n+     If there are any pages with free objects, they will be at the\n+     head of the list.  NULL if there are no page-entries for this\n+     object size.  */\n+  page_entry *pages[HOST_BITS_PER_PTR];\n+\n+  /* The Nth element in this array is the last page with objects of\n+     size 2^N.  NULL if there are no page-entries for this object\n+     size.  */\n+  page_entry *page_tails[HOST_BITS_PER_PTR];\n+\n+  /* Lookup table for associating allocation pages with object addresses.  */\n+  page_table lookup;\n+\n+  /* The system's page size.  */\n+  size_t pagesize;\n+  size_t lg_pagesize;\n+\n+  /* Bytes currently allocated.  */\n+  size_t allocated;\n+\n+  /* Bytes currently allocated at the end of the last collection.  */\n+  size_t allocated_last_gc;\n+\n+  /* The current depth in the context stack.  */\n+  unsigned char context_depth;\n+\n+  /* A file descriptor open to /dev/zero for reading.  */\n+#ifndef MAP_ANONYMOUS\n+  int dev_zero_fd;\n+#endif\n+\n+  /* A cache of free system pages.  */\n+  page_entry *free_pages;\n+\n+  /* The file descriptor for debugging output.  */\n+  FILE *debug_file;\n+} G;\n+\n+\n+/* Compute DIVIDEND / DIVISOR, rounded up.  */\n+#define DIV_ROUND_UP(Dividend, Divisor) \\\n+  ((Dividend + Divisor - 1) / Divisor)\n+\n+/* The number of objects per allocation page, for objects of size\n+   2^ORDER.  */\n+#define OBJECTS_PER_PAGE(Order) \\\n+  ((Order) >= G.lg_pagesize ? 1 : G.pagesize / ((size_t)1 << (Order)))\n+\n+/* The size in bytes required to maintain a bitmap for the objects\n+   on a page-entry.  */\n+#define BITMAP_SIZE(Num_objects) \\\n+  (DIV_ROUND_UP ((Num_objects), HOST_BITS_PER_LONG) * sizeof(long))\n+\n+/* Skip garbage collection if the current allocation is not at least\n+   this factor times the allocation at the end of the last collection.\n+   In other words, total allocation must expand by (this factor minus\n+   one) before collection is performed.  */\n+#define GGC_MIN_EXPAND_FOR_GC (1.3)\n+\n+\f\n+static page_entry *lookup_page_table_entry PROTO ((void *));\n+static void set_page_table_entry PROTO ((void *, page_entry *));\n+static char *alloc_anon PROTO ((char *, size_t));\n+static struct page_entry * alloc_page PROTO ((unsigned));\n+static void free_page PROTO ((struct page_entry *));\n+static void release_pages PROTO ((void));\n+static void *alloc_obj PROTO ((size_t, int));\n+static int mark_obj PROTO ((void *));\n+static void clear_marks PROTO ((void));\n+static void sweep_pages PROTO ((void));\n+\n+#ifdef GGC_POISON\n+static void poison PROTO ((void *, size_t));\n+static void poison_pages PROTO ((void));\n+#endif\n+\n+void debug_print_page_list PROTO ((int));\n+\f\n+/* Traverse the page table and find the entry for a page. \n+   Die (probably) if the object wasn't allocated via GC.  */\n+\n+static inline page_entry *\n+lookup_page_table_entry(p)\n+     void *p;\n+{\n+  page_entry ***base;\n+  size_t L1, L2;\n+\n+#if HOST_BITS_PER_PTR <= 32\n+  base = &G.lookup[0];\n+#else\n+  page_table table = G.lookup;\n+  size_t high_bits = (size_t) p & ~ (size_t) 0xffffffff;\n+  while (table->high_bits != high_bits)\n+    table = table->next;\n+  base = &table->table[0];\n+#endif\n+\n+  /* Extract the level 1 and 2 indicies.  */\n+  L1 = LOOKUP_L1 (p);\n+  L2 = LOOKUP_L2 (p);\n+\n+  return base[L1][L2];\n+}\n+\n+\n+/* Set the page table entry for a page.  */\n+static void\n+set_page_table_entry(p, entry)\n+     void *p;\n+     page_entry *entry;\n+{\n+  page_entry ***base;\n+  size_t L1, L2;\n+\n+#if HOST_BITS_PER_PTR <= 32\n+  base = &G.lookup[0];\n+#else\n+  page_table table;\n+  size_t high_bits = (size_t) p & ~ (size_t) 0xffffffff;\n+  for (table = G.lookup; table; table = table->next)\n+    if (table->high_bits == high_bits)\n+      goto found;\n+\n+  /* Not found -- allocate a new table.  */\n+  table = (page_table) xcalloc (1, sizeof(*table));\n+  table->next = G.lookup;\n+  table->high_bits = high_bits;\n+  G.lookup = table;\n+found:\n+  base = &table->table[0];\n+#endif\n+\n+  /* Extract the level 1 and 2 indicies.  */\n+  L1 = LOOKUP_L1 (p);\n+  L2 = LOOKUP_L2 (p);\n+\n+  if (base[L1] == NULL)\n+    base[L1] = (page_entry **) xcalloc (PAGE_L2_SIZE, sizeof (page_entry *));\n+\n+  base[L1][L2] = entry;\n+}\n+\n+\n+/* Prints the page-entry for object size ORDER, for debugging.  */\n+void\n+debug_print_page_list (order)\n+     int order;\n+{\n+  page_entry *p;\n+  printf (\"Head=%p, Tail=%p:\\n\", G.pages[order], G.page_tails[order]);\n+  p = G.pages[order];\n+  while (p != NULL)\n+    {\n+      printf (\"%p(%1d|%3d) -> \", p, p->context_depth, p->num_free_objects);\n+      p = p->next;\n+    }\n+  printf (\"NULL\\n\");\n+  fflush (stdout);\n+}\n+\n+#ifdef GGC_POISON\n+/* `Poisons' the region of memory starting at START and extending for\n+   LEN bytes.  */\n+static inline void\n+poison (start, len)\n+     void *start;\n+     size_t len;\n+{\n+  memset (start, 0xa5, len);\n+}\n+#endif\n+\n+/* Allocate SIZE bytes of anonymous memory, preferably near PREF,\n+   (if non-null).  */\n+static inline char *\n+alloc_anon (pref, size)\n+     char *pref;\n+     size_t size;\n+{\n+  char *page;\n+\n+#ifdef MAP_ANONYMOUS\n+  page = (char *) mmap (pref, size, PROT_READ | PROT_WRITE,\n+\t\t\tMAP_PRIVATE | MAP_ANONYMOUS, -1, 0);\n+#else\n+  page = (char *) mmap (pref, size, PROT_READ | PROT_WRITE,\n+\t\t\tMAP_PRIVATE, G.dev_zero_fd, 0);\n+#endif\n+  if (page == (char *) MAP_FAILED)\n+    {\n+      fputs (\"Virtual memory exhausted!\\n\", stderr);\n+      exit(1);\n+    }\n+\n+  return page;\n+}\n+\n+/* Allocate a new page for allocating objects of size 2^ORDER,\n+   and return an entry for it.  The entry is not added to the\n+   appropriate page_table list.  */\n+static inline struct page_entry *\n+alloc_page (order)\n+     unsigned order;\n+{\n+  struct page_entry *entry, *p, **pp;\n+  char *page;\n+  size_t num_objects;\n+  size_t bitmap_size;\n+  size_t page_entry_size;\n+  size_t entry_size;\n+\n+  num_objects = OBJECTS_PER_PAGE (order);\n+  bitmap_size = BITMAP_SIZE (num_objects + 1);\n+  page_entry_size = sizeof (page_entry) - sizeof (long) + bitmap_size;\n+  entry_size = num_objects * (1 << order);\n+\n+  entry = NULL;\n+  page = NULL;\n+\n+  /* Check the list of free pages for one we can use.  */\n+  for (pp = &G.free_pages, p = *pp; p ; pp = &p->next, p = *pp)\n+    if (p->bytes == entry_size)\n+      break;\n+\n+  if (p != NULL)\n+    {\n+      /* Recycle the allocated memory from this page ... */\n+      *pp = p->next;\n+      page = p->page;\n+      /* ... and, if possible, the page entry itself.  */\n+      if (p->order == order)\n+\t{\n+\t  entry = p;\n+\t  memset (entry, 0, page_entry_size);\n+\t}\n+      else\n+\tfree (p);\n+    }\n+  else\n+    {\n+      /* Actually allocate the memory, using mmap.  */\n+      page = alloc_anon (NULL, entry_size);\n+    }\n+\n+  if (entry == NULL)\n+    entry = (struct page_entry *) xcalloc (1, page_entry_size);\n+\n+  entry->bytes = entry_size;\n+  entry->page = page;\n+  entry->context_depth = G.context_depth;\n+  entry->order = order;\n+  entry->num_free_objects = num_objects;\n+  entry->next_bit_hint = 1;\n+\n+  /* Set the one-past-the-end in-use bit.  This acts as a sentry as we\n+     increment the hint.  */\n+  entry->in_use_p[num_objects / HOST_BITS_PER_LONG]\n+    = (unsigned long) 1 << (num_objects % HOST_BITS_PER_LONG);\n+\n+  set_page_table_entry (page, entry);\n+\n+  if (GGC_DEBUG_LEVEL >= 2)\n+    fprintf (G.debug_file, \n+\t     \"Allocating page at %p, object size=%d, data %p-%p\\n\", entry,\n+\t     1 << order, page, page + entry_size - 1);\n+\n+  return entry;\n+}\n+\n+\n+/* Free a page when it's no longer needed.  */\n+static inline void\n+free_page (entry)\n+     page_entry *entry;\n+{\n+  if (GGC_DEBUG_LEVEL >= 2)\n+    fprintf (G.debug_file, \n+\t     \"Deallocating page at %p, data %p-%p\\n\", entry,\n+\t     entry->page, entry->page + entry->bytes - 1);\n+\n+  set_page_table_entry (entry->page, NULL);\n+\n+  entry->next = G.free_pages;\n+  G.free_pages = entry;\n+}\n+\n+\n+/* Release the page cache to the system.  */\n+static inline void\n+release_pages ()\n+{\n+  page_entry *p, *next;\n+  char *start;\n+  size_t len;\n+\n+  p = G.free_pages;\n+  if (p == NULL)\n+    return;\n+\n+  next = p->next;\n+  start = p->page;\n+  len = p->bytes;\n+  free (p);\n+  p = next;\n+\n+  while (p)\n+    {\n+      next = p->next;\n+      /* Gather up adjacent pages so they are unmapped together.  */\n+      if (p->page == start + len)\n+\tlen += p->bytes;\n+      else\n+\t{\n+\t  munmap (start, len);\n+\t  start = p->page;\n+\t  len = p->bytes;\n+\t}\n+      free (p);\n+      p = next;\n+    }\n+\n+  munmap (start, len);\n+  G.free_pages = NULL;\n+}\n+\n+\n+/* This table provides a fast way to determine ceil(log_2(size)) for\n+   allocation requests.  The minimum allocation size is four bytes.  */\n+static unsigned char const size_lookup[257] = \n+{ \n+  2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, \n+  4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, \n+  5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, \n+  6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, \n+  6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, \n+  7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, \n+  7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n+  7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, \n+  7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n+  8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n+  8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n+  8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n+  8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n+  8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n+  8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n+  8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n+  8\n+};\n+\n+/* Allocate a chunk of memory of SIZE bytes.  If ZERO is non-zero, the\n+   memory is zeroed; otherwise, its contents are undefined.  */\n+static void *\n+alloc_obj (size, zero)\n+     size_t size;\n+     int zero;\n+{\n+  unsigned order, word, bit, object_offset;\n+  struct page_entry *entry;\n+  void *result;\n+\n+  if (size <= 256)\n+    order = size_lookup[size];\n+  else\n+    {\n+      order = 9;\n+      while (size > ((size_t) 1 << order))\n+\torder++;\n+    }\n+\n+  /* If there are non-full pages for this size allocation, they are at\n+     the head of the list.  */\n+  entry = G.pages[order];\n+\n+  /* If there is no page for this object size, or all pages in this\n+     context are full, allocate a new page.  */\n+  if (entry == NULL \n+      || entry->num_free_objects == 0 \n+      || entry->context_depth != G.context_depth)\n+    {\n+      struct page_entry *new_entry;\n+      new_entry = alloc_page (order);\n+      \n+      /* If this is the only entry, it's also the tail.  */\n+      if (entry == NULL)\n+\tG.page_tails[order] = new_entry;\n+     \n+      /* Put new pages at the head of the page list.  */\n+      new_entry->next = entry;\n+      entry = new_entry;\n+      G.pages[order] = new_entry;\n+\n+      /* For a new page, we know the word and bit positions (in the\n+\t in_use bitmap) of the first available object -- they're zero.  */\n+      new_entry->next_bit_hint = 1;\n+      word = 0;\n+      bit = 0;\n+      object_offset = 0;\n+    }\n+  else\n+    {\n+      /* First try to use the hint left from the previous allocation\n+\t to locate a clear bit in the in-use bitmap.  We've made sure\n+\t that the one-past-the-end bit is always set, so if the hint\n+\t has run over, this test will fail.  */\n+      unsigned hint = entry->next_bit_hint;\n+      word = hint / HOST_BITS_PER_LONG;\n+      bit = hint % HOST_BITS_PER_LONG;\n+      \n+      /* If the hint didn't work, scan the bitmap from the beginning.  */\n+      if ((entry->in_use_p[word] >> bit) & 1)\n+\t{\n+\t  word = bit = 0;\n+\t  while (~entry->in_use_p[word] == 0)\n+\t    ++word;\n+\t  while ((entry->in_use_p[word] >> bit) & 1)\n+\t    ++bit;\n+\t  hint = word * HOST_BITS_PER_LONG + bit;\n+\t}\n+\n+      /* Next time, try the next bit.  */\n+      entry->next_bit_hint = hint + 1;\n+\n+      object_offset = hint << order;\n+    }\n+\n+  /* Set the in-use bit.  */\n+  entry->in_use_p[word] |= ((unsigned long) 1 << bit);\n+\n+  /* Keep a running total of the number of free objects.  If this page\n+     fills up, we may have to move it to the end of the list if the\n+     next page isn't full.  If the next page is full, all subsequent\n+     pages are full, so there's no need to move it.  */\n+  if (--entry->num_free_objects == 0\n+      && entry->next != NULL\n+      && entry->next->num_free_objects > 0)\n+    {\n+      G.pages[order] = entry->next;\n+      entry->next = NULL;\n+      G.page_tails[order]->next = entry;\n+      G.page_tails[order] = entry;\n+    }\n+\n+  /* Calculate the object's address.  */\n+  result = entry->page + object_offset;\n+\n+#ifdef GGC_POISON\n+  /* `Poison' the entire allocated object before zeroing the requested area,\n+     so that bytes beyond the end, if any, will not necessarily be zero.  */\n+  poison (result, 1 << order);\n+#endif\n+  if (zero)\n+    memset (result, 0, size);\n+\n+  /* Keep track of how many bytes are being allocated.  This\n+     information is used in deciding when to collect.  */\n+  G.allocated += (size_t) 1 << order;\n+\n+  if (GGC_DEBUG_LEVEL >= 3)\n+    fprintf (G.debug_file, \n+\t     \"Allocating object, requested size=%d, actual=%d at %p on %p\\n\",\n+\t     (int) size, 1 << order, result, entry);\n+\n+  return result;\n+}\n+\n+\n+/* If P is not marked, marks it and returns 0.  Otherwise returns 1.\n+   P must have been allocated by the GC allocator; it mustn't point to\n+   static objects, stack variables, or memory allocated with malloc.  */\n+static int\n+mark_obj (p)\n+     void *p;\n+{\n+  page_entry *entry;\n+  unsigned bit, word;\n+  unsigned long mask;\n+\n+  /* Look up the page on which the object is alloced.  If the object\n+     wasn't allocated by the collector, we'll probably die.  */\n+  entry = lookup_page_table_entry(p);\n+#ifdef ENABLE_CHECKING\n+  if (entry == NULL)\n+    abort ();\n+#endif\n+\n+  /* Calculate the index of the object on the page; this is its bit\n+     position in the in_use_p bitmap.  */\n+  bit = (((char *) p) - entry->page) >> entry->order;\n+  word = bit / HOST_BITS_PER_LONG;\n+  mask = (unsigned long) 1 << (bit % HOST_BITS_PER_LONG);\n+  \n+  /* If the bit was previously set, skip it. */\n+  if (entry->in_use_p[word] & mask)\n+    return 1;\n+\n+  /* Otherwise set it, and decrement the free object count.  */\n+  entry->in_use_p[word] |= mask;\n+  entry->num_free_objects -= 1;\n+\n+  G.allocated += (size_t) 1 << entry->order;\n+\n+  if (GGC_DEBUG_LEVEL >= 4)\n+    fprintf (G.debug_file, \"Marking %p\\n\", p);\n+\n+  return 0;\n+}\n+\n+\f\n+/* Initialize the ggc-mmap allocator.  */\n+void\n+init_ggc ()\n+{\n+  G.pagesize = getpagesize();\n+  G.lg_pagesize = exact_log2 (G.pagesize);\n+\n+#ifndef MAP_ANONYMOUS\n+  G.dev_zero_fd = open (\"/dev/zero\", O_RDONLY);\n+  if (G.dev_zero_fd == -1)\n+    abort ();\n+#endif\n+\n+#if 0\n+  G.debug_file = fopen (\"ggc-mmap.debug\", \"w\");\n+#else\n+  G.debug_file = stdout;\n+#endif\n+\n+  /* Set the initial allocation to 4MB, so no collection will be\n+     performed until the heap is somewhat larger than 4 MB.  */\n+  G.allocated_last_gc = 4 * 1024 * 1024;\n+\n+  empty_string = ggc_alloc_string (\"\", 0);\n+  ggc_add_string_root (&empty_string, 1);\n+}\n+\n+\n+void\n+ggc_push_context ()\n+{\n+  ++G.context_depth;\n+\n+  /* Die on wrap.  */\n+  if (G.context_depth == 0)\n+    abort ();\n+}\n+\n+\n+void\n+ggc_pop_context ()\n+{\n+  unsigned order, depth;\n+\n+  depth = --G.context_depth;\n+\n+  /* Any remaining pages in the popped context are lowered to the new\n+     current context; i.e. objects allocated in the popped context and\n+     left over are imported into the previous context.  */\n+  for (order = 2; order < HOST_BITS_PER_PTR; order++)\n+    {\n+      size_t num_objects = OBJECTS_PER_PAGE (order);\n+      size_t bitmap_size = BITMAP_SIZE (num_objects);\n+\n+      page_entry *p;\n+\n+      for (p = G.pages[order]; p != NULL; p = p->next)\n+\t{\n+\t  if (p->context_depth > depth)\n+\t    {\n+\t      p->context_depth = depth;\n+\t    }\n+\n+\t  /* If this page is now in the topmost context, and we'd\n+\t     saved its allocation state, restore it.  */\n+\t  else if (p->context_depth == depth && p->save_in_use_p)\n+\t    {\n+\t      memcpy (p->in_use_p, p->save_in_use_p, bitmap_size);\n+\t      free (p->save_in_use_p);\n+\t      p->save_in_use_p = 0;\n+\t      p->num_free_objects = p->save_num_free_objects;\n+\t    }\n+\t}\n+    }\n+}\n+\n+\n+struct rtx_def *\n+ggc_alloc_rtx (nslots)\n+     int nslots;\n+{\n+  return (struct rtx_def *) \n+    alloc_obj (sizeof (struct rtx_def) + (nslots - 1) * sizeof (rtunion), 1);\n+}\n+\n+\n+struct rtvec_def *\n+ggc_alloc_rtvec (nelt)\n+     int nelt;\n+{\n+  return (struct rtvec_def *)\n+    alloc_obj (sizeof (struct rtvec_def) + (nelt - 1) * sizeof (rtunion), 1);\n+}\n+\n+\n+union tree_node *\n+ggc_alloc_tree (length)\n+     int length;\n+{\n+  return (union tree_node *) alloc_obj (length, 1);\n+}\n+\n+\n+char *\n+ggc_alloc_string (contents, length)\n+     const char *contents;\n+     int length;\n+{\n+  char *string;\n+\n+  if (length < 0)\n+    {\n+      if (contents == NULL)\n+\treturn NULL;\n+      length = strlen (contents);\n+    }\n+\n+  string = (char *) alloc_obj (length + 1, 0);\n+  if (contents != NULL)\n+    memcpy (string, contents, length);\n+  string[length] = 0;\n+\n+  return string;\n+}\n+\n+\n+void *\n+ggc_alloc (size)\n+     size_t size;\n+{\n+  return alloc_obj (size, 0);\n+}\n+\n+\f\n+static inline void\n+clear_marks ()\n+{\n+  unsigned order;\n+\n+  for (order = 2; order < HOST_BITS_PER_PTR; order++)\n+    {\n+      size_t num_objects = OBJECTS_PER_PAGE (order);\n+      size_t bitmap_size = BITMAP_SIZE (num_objects);\n+      page_entry *p;\n+\n+      for (p = G.pages[order]; p != NULL; p = p->next)\n+\t{\n+#ifdef ENABLE_CHECKING\n+\t  /* The data should be page-aligned.  */\n+\t  if ((size_t) p->page & (G.pagesize - 1))\n+\t    abort ();\n+#endif\n+\n+\t  /* Pages that aren't in the topmost context are not collected;\n+\t     nevertheless, we need their in-use bit vectors to store GC\n+\t     marks.  So, back them up first.  */\n+\t  if (p->context_depth < G.context_depth\n+\t      && ! p->save_in_use_p)\n+\t    {\n+\t      p->save_in_use_p = xmalloc (bitmap_size);\n+\t      memcpy (p->save_in_use_p, p->in_use_p, bitmap_size);\n+\t      p->save_num_free_objects = p->num_free_objects;\n+\t    }\n+\n+\t  /* Reset reset the number of free objects and clear the\n+             in-use bits.  These will be adjusted by mark_obj.  */\n+\t  p->num_free_objects = num_objects;\n+\t  memset (p->in_use_p, 0, bitmap_size);\n+\n+\t  /* Make sure the one-past-the-end bit is always set.  */\n+\t  p->in_use_p[num_objects / HOST_BITS_PER_LONG] \n+\t    = ((unsigned long) 1 << (num_objects % HOST_BITS_PER_LONG));\n+\t}\n+    }\n+}\n+\n+static inline void\n+sweep_pages ()\n+{\n+  unsigned order;\n+\n+  for (order = 2; order < HOST_BITS_PER_PTR; order++)\n+    {\n+      /* The last page-entry to consider, regardless of entries\n+\t placed at the end of the list.  */\n+      page_entry * const last = G.page_tails[order];\n+\n+      size_t num_objects = OBJECTS_PER_PAGE (order);\n+      page_entry *p, *previous;\n+      int done;\n+\t\n+      p = G.pages[order];\n+      if (p == NULL)\n+\tcontinue;\n+\n+      previous = NULL;\n+      do\n+\t{\n+\t  page_entry *next = p->next;\n+\n+\t  /* Loop until all entries have been examined.  */\n+\t  done = (p == last);\n+\n+\t  /* Only objects on pages in the topmost context should get\n+\t     collected.  */\n+\t  if (p->context_depth < G.context_depth)\n+\t    ;\n+\n+\t  /* Remove the page if it's empty.  */\n+\t  else if (p->num_free_objects == num_objects)\n+\t    {\n+\t      if (! previous)\n+\t\tG.pages[order] = next;\n+\t      else\n+\t\tprevious->next = next;\n+\n+\t      /* Are we removing the last element?  */\n+\t      if (p == G.page_tails[order])\n+\t\tG.page_tails[order] = previous;\n+\t      free_page (p);\n+\t      p = previous;\n+\t    }\n+\n+\t  /* If the page is full, move it to the end.  */\n+\t  else if (p->num_free_objects == 0)\n+\t    {\n+\t      /* Don't move it if it's already at the end.  */\n+\t      if (p != G.page_tails[order])\n+\t\t{\n+\t\t  /* Move p to the end of the list.  */\n+\t\t  p->next = NULL;\n+\t\t  G.page_tails[order]->next = p;\n+\n+\t\t  /* Update the tail pointer...  */\n+\t\t  G.page_tails[order] = p;\n+\n+\t\t  /* ... and the head pointer, if necessary.  */\n+\t\t  if (! previous)\n+\t\t    G.pages[order] = next;\n+\t\t  else\n+\t\t    previous->next = next;\n+\t\t  p = previous;\n+\t\t}\n+\t    }\n+\n+\t  /* If we've fallen through to here, it's a page in the\n+\t     topmost context that is neither full nor empty.  Such a\n+\t     page must precede pages at lesser context depth in the\n+\t     list, so move it to the head.  */\n+\t  else if (p != G.pages[order])\n+\t    {\n+\t      previous->next = p->next;\n+\t      p->next = G.pages[order];\n+\t      G.pages[order] = p;\n+\t      /* Are we moving the last element?  */\n+\t      if (G.page_tails[order] == p)\n+\t        G.page_tails[order] = previous;\n+\t      p = previous;\n+\t    }\n+\n+\t  previous = p;\n+\t  p = next;\n+\t} \n+      while (! done);\n+    }\n+}\n+\n+#ifdef GGC_POISON\n+static inline void\n+poison_pages ()\n+{\n+  unsigned order;\n+\n+  for (order = 2; order < HOST_BITS_PER_PTR; order++)\n+    {\n+      size_t num_objects = OBJECTS_PER_PAGE (order);\n+      size_t size = (size_t) 1 << order;\n+      page_entry *p;\n+\n+      for (p = G.pages[order]; p != NULL; p = p->next)\n+\t{\n+\t  size_t i;\n+\t  for (i = 0; i < num_objects; i++)\n+\t    {\n+\t      size_t word, bit;\n+\t      word = i / HOST_BITS_PER_LONG;\n+\t      bit = i % HOST_BITS_PER_LONG;\n+\t      if (((p->in_use_p[word] >> bit) & 1) == 0)\n+\t\tpoison (p->page + i * size, size);\n+\t    }\n+\t}\n+    }\n+}\n+#endif\n+\n+void\n+ggc_collect ()\n+{\n+  int time;\n+\n+  /* Avoid frequent unnecessary work by skipping collection if the\n+     total allocations haven't expanded much since the last\n+     collection.  */\n+#ifndef GGC_ALWAYS_COLLECT\n+  if (G.allocated < GGC_MIN_EXPAND_FOR_GC * G.allocated_last_gc)\n+    return;\n+#endif\n+\n+  time = get_run_time ();\n+  if (!quiet_flag)\n+    fprintf (stderr, \" {GC %luk -> \", (unsigned long)G.allocated / 1024);\n+\n+  /* Zero the total allocated bytes.  We'll reaccumulate this while\n+     marking.  */\n+  G.allocated = 0;\n+\n+  /* Release the pages we freed the last time we collected, but didn't \n+     reuse in the interim.  */\n+  release_pages ();\n+\n+  clear_marks ();\n+  ggc_mark_roots ();\n+  sweep_pages ();\n+  \n+#ifdef GGC_POISON\n+  poison_pages ();\n+#endif\n+\n+  G.allocated_last_gc = G.allocated;\n+\n+  time = get_run_time () - time;\n+  gc_time += time;\n+\n+  time = (time + 500) / 1000;\n+  if (!quiet_flag)\n+    fprintf (stderr, \"%luk in %d.%03d}\", \n+\t     (unsigned long) G.allocated / 1024, time / 1000, time % 1000);\n+}\n+\n+\n+int\n+ggc_set_mark_rtx (r)\n+     rtx r;\n+{\n+  return mark_obj (r);\n+}\n+\n+int\n+ggc_set_mark_rtvec (v)\n+     rtvec v;\n+{\n+  return mark_obj (v);\n+}\n+\n+int\n+ggc_set_mark_tree (t)\n+     tree t;\n+{\n+  return mark_obj (t);\n+}\n+\n+void\n+ggc_mark_string (s)\n+     char *s;\n+{\n+  if (s)\n+    mark_obj (s);\n+}\n+\n+void \n+ggc_mark (p)\n+     void *p;\n+{\n+  if (p)\n+    mark_obj (p);\n+}"}]}