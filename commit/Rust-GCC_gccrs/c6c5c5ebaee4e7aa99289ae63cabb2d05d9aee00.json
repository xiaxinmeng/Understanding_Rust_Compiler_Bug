{"sha": "c6c5c5ebaee4e7aa99289ae63cabb2d05d9aee00", "node_id": "C_kwDOANBUbNoAKGM2YzVjNWViYWVlNGU3YWE5OTI4OWFlNjNjYWJiMmQwNWQ5YWVlMDA", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2021-11-12T17:33:03Z"}, "committer": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2021-11-12T17:33:03Z"}, "message": "aarch64: Use new hooks for vector comparisons\n\nPreviously we tried to account for the different issue rates of\nthe various vector modes by guessing what the Advanced SIMD version\nof an SVE loop would look like and what its issue rate was likely to be.\nWe'd then increase the cost of the SVE loop if the Advanced SIMD loop\nmight issue more quickly.\n\nThis patch moves that logic to better_main_loop_than_p, so that we\ncan compare loops side-by-side rather than having to guess.  This also\nmeans we can apply the issue rate heuristics to *any* vector loop\ncomparison, rather than just weighting SVE vs. Advanced SIMD.\n\nThe actual heuristics are otherwise unchanged.  We're just\napplying them in a different place.\n\ngcc/\n\t* config/aarch64/aarch64.c (aarch64_vector_costs::m_saw_sve_only_op)\n\t(aarch64_sve_only_stmt_p): Delete.\n\t(aarch64_vector_costs::prefer_unrolled_loop): New function,\n\textracted from adjust_body_cost.\n\t(aarch64_vector_costs::better_main_loop_than_p): New function,\n\tusing heuristics extracted from adjust_body_cost and\n\tadjust_body_cost_sve.\n\t(aarch64_vector_costs::adjust_body_cost_sve): Remove\n\tadvsimd_cycles_per_iter and could_use_advsimd parameters.\n\tUpdate after changes above.\n\t(aarch64_vector_costs::adjust_body_cost): Update after changes above.", "tree": {"sha": "c7627f95923793b9d05fa21eb821f46f8448952c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/c7627f95923793b9d05fa21eb821f46f8448952c"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/c6c5c5ebaee4e7aa99289ae63cabb2d05d9aee00", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c6c5c5ebaee4e7aa99289ae63cabb2d05d9aee00", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c6c5c5ebaee4e7aa99289ae63cabb2d05d9aee00", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c6c5c5ebaee4e7aa99289ae63cabb2d05d9aee00/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "2e1886ea0649885819495bbee4cfc8e004beffc5", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2e1886ea0649885819495bbee4cfc8e004beffc5", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2e1886ea0649885819495bbee4cfc8e004beffc5"}], "stats": {"total": 291, "additions": 145, "deletions": 146}, "files": [{"sha": "1e2f3bf3765f1a60fe660509bec3376b65de6c98", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 145, "deletions": 146, "changes": 291, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/c6c5c5ebaee4e7aa99289ae63cabb2d05d9aee00/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/c6c5c5ebaee4e7aa99289ae63cabb2d05d9aee00/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=c6c5c5ebaee4e7aa99289ae63cabb2d05d9aee00", "patch": "@@ -14909,27 +14909,24 @@ class aarch64_vector_costs : public vector_costs\n \t\t\t      int misalign,\n \t\t\t      vect_cost_model_location where) override;\n   void finish_cost (const vector_costs *) override;\n+  bool better_main_loop_than_p (const vector_costs *other) const override;\n \n private:\n   void record_potential_advsimd_unrolling (loop_vec_info);\n   void analyze_loop_vinfo (loop_vec_info);\n   void count_ops (unsigned int, vect_cost_for_stmt, stmt_vec_info, tree,\n \t\t  aarch64_vec_op_count *, unsigned int);\n   fractional_cost adjust_body_cost_sve (const aarch64_vec_op_count *,\n-\t\t\t\t\tfractional_cost, fractional_cost,\n-\t\t\t\t\tbool, unsigned int, unsigned int *,\n-\t\t\t\t\tbool *);\n+\t\t\t\t\tfractional_cost, unsigned int,\n+\t\t\t\t\tunsigned int *, bool *);\n   unsigned int adjust_body_cost (loop_vec_info, const aarch64_vector_costs *,\n \t\t\t\t unsigned int);\n+  bool prefer_unrolled_loop () const;\n \n   /* True if we have performed one-time initialization based on the\n      vec_info.  */\n   bool m_analyzed_vinfo = false;\n \n-  /* True if we've seen an SVE operation that we cannot currently vectorize\n-     using Advanced SIMD.  */\n-  bool m_saw_sve_only_op = false;\n-\n   /* - If M_VEC_FLAGS is zero then we're costing the original scalar code.\n      - If M_VEC_FLAGS & VEC_ADVSIMD is nonzero then we're costing Advanced\n        SIMD code.\n@@ -15306,42 +15303,6 @@ aarch64_multiply_add_p (vec_info *vinfo, stmt_vec_info stmt_info,\n   return false;\n }\n \n-/* Return true if the vectorized form of STMT_INFO is something that is only\n-   possible when using SVE instead of Advanced SIMD.  VECTYPE is the type of\n-   the vector that STMT_INFO is operating on.  */\n-static bool\n-aarch64_sve_only_stmt_p (stmt_vec_info stmt_info, tree vectype)\n-{\n-  if (!aarch64_sve_mode_p (TYPE_MODE (vectype)))\n-    return false;\n-\n-  if (STMT_VINFO_DATA_REF (stmt_info))\n-    {\n-      /* Check for true gathers and scatters (rather than just strided accesses\n-\t that we've chosen to implement using gathers and scatters).  Although\n-\t in principle we could use elementwise accesses for Advanced SIMD,\n-\t the vectorizer doesn't yet support that.  */\n-      if (STMT_VINFO_GATHER_SCATTER_P (stmt_info))\n-\treturn true;\n-\n-      /* Check for masked loads and stores.  */\n-      if (auto *call = dyn_cast<gcall *> (stmt_info->stmt))\n-\tif (gimple_call_internal_p (call)\n-\t    && internal_fn_mask_index (gimple_call_internal_fn (call)) >= 0)\n-\t  return true;\n-    }\n-\n-  /* Check for 64-bit integer multiplications.  */\n-  auto *assign = dyn_cast<gassign *> (stmt_info->stmt);\n-  if (assign\n-      && gimple_assign_rhs_code (assign) == MULT_EXPR\n-      && GET_MODE_INNER (TYPE_MODE (vectype)) == DImode\n-      && !integer_pow2p (gimple_assign_rhs2 (assign)))\n-    return true;\n-\n-  return false;\n-}\n-\n /* We are considering implementing STMT_INFO using SVE.  If STMT_INFO is an\n    in-loop reduction that SVE supports directly, return its latency in cycles,\n    otherwise return zero.  SVE_COSTS specifies the latencies of the relevant\n@@ -15866,9 +15827,6 @@ aarch64_vector_costs::add_stmt_cost (int count, vect_cost_for_stmt kind,\n      of just looking at KIND.  */\n   if (stmt_info && aarch64_use_new_vector_costs_p ())\n     {\n-      if (vectype && aarch64_sve_only_stmt_p (stmt_info, vectype))\n-\tm_saw_sve_only_op = true;\n-\n       /* If we scalarize a strided store, the vectorizer costs one\n \t vec_to_scalar for each element.  However, we can store the first\n \t element using an FP store without a separate extract step.  */\n@@ -15924,6 +15882,31 @@ aarch64_vector_costs::add_stmt_cost (int count, vect_cost_for_stmt kind,\n   return record_stmt_cost (stmt_info, where, (count * stmt_cost).ceil ());\n }\n \n+/* Return true if (a) we're applying the Advanced SIMD vs. SVE unrolling\n+   heuristic described above m_unrolled_advsimd_niters and (b) the heuristic\n+   says that we should prefer the Advanced SIMD loop.  */\n+bool\n+aarch64_vector_costs::prefer_unrolled_loop () const\n+{\n+  if (!m_unrolled_advsimd_stmts)\n+    return false;\n+\n+  if (dump_enabled_p ())\n+    dump_printf_loc (MSG_NOTE, vect_location, \"Number of insns in\"\n+\t\t     \" unrolled Advanced SIMD loop = %d\\n\",\n+\t\t     m_unrolled_advsimd_stmts);\n+\n+  /* The balance here is tricky.  On the one hand, we can't be sure whether\n+     the code is vectorizable with Advanced SIMD or not.  However, even if\n+     it isn't vectorizable with Advanced SIMD, there's a possibility that\n+     the scalar code could also be unrolled.  Some of the code might then\n+     benefit from SLP, or from using LDP and STP.  We therefore apply\n+     the heuristic regardless of can_use_advsimd_p.  */\n+  return (m_unrolled_advsimd_stmts\n+\t  && (m_unrolled_advsimd_stmts\n+\t      <= (unsigned int) param_max_completely_peeled_insns));\n+}\n+\n /* Subroutine of adjust_body_cost for handling SVE.  Use ISSUE_INFO to work out\n    how fast the SVE code can be issued and compare it to the equivalent value\n    for scalar code (SCALAR_CYCLES_PER_ITER).  If COULD_USE_ADVSIMD is true,\n@@ -15938,15 +15921,12 @@ fractional_cost\n aarch64_vector_costs::\n adjust_body_cost_sve (const aarch64_vec_op_count *ops,\n \t\t      fractional_cost scalar_cycles_per_iter,\n-\t\t      fractional_cost advsimd_cycles_per_iter,\n-\t\t      bool could_use_advsimd, unsigned int orig_body_cost,\n-\t\t      unsigned int *body_cost, bool *should_disparage)\n+\t\t      unsigned int orig_body_cost, unsigned int *body_cost,\n+\t\t      bool *should_disparage)\n {\n   if (dump_enabled_p ())\n     ops->dump ();\n \n-  fractional_cost sve_nonpred_cycles_per_iter\n-    = ops->min_nonpred_cycles_per_iter ();\n   fractional_cost sve_pred_cycles_per_iter = ops->min_pred_cycles_per_iter ();\n   fractional_cost sve_cycles_per_iter = ops->min_cycles_per_iter ();\n \n@@ -15978,43 +15958,6 @@ adjust_body_cost_sve (const aarch64_vec_op_count *ops,\n \t}\n     }\n \n-  /* If it appears that the Advanced SIMD version of a loop could issue\n-     more quickly than the SVE one, increase the SVE cost in proportion\n-     to the difference.  The intention is to make Advanced SIMD preferable\n-     in cases where an Advanced SIMD version exists, without increasing\n-     the costs so much that SVE won't be used at all.\n-\n-     The reasoning is similar to the scalar vs. predicate comparison above:\n-     if the issue rate of the SVE code is limited by predicate operations\n-     (i.e. if sve_pred_cycles_per_iter > sve_nonpred_cycles_per_iter),\n-     and if the Advanced SIMD code could issue within the limit imposed\n-     by the predicate operations, the predicate operations are adding an\n-     overhead that the original code didn't have and so we should prefer\n-     the Advanced SIMD version.  However, if the predicate operations\n-     do not dominate in this way, we should only increase the cost of\n-     the SVE code if sve_cycles_per_iter is strictly greater than\n-     advsimd_cycles_per_iter.  Given rounding effects, this should mean\n-     that Advanced SIMD is either better or at least no worse.  */\n-  if (sve_nonpred_cycles_per_iter >= sve_pred_cycles_per_iter)\n-    sve_estimate = sve_cycles_per_iter;\n-  if (could_use_advsimd && advsimd_cycles_per_iter < sve_estimate)\n-    {\n-      /* This ensures that min_cost > orig_body_cost * 2.  */\n-      unsigned int factor = fractional_cost::scale (1, sve_estimate,\n-\t\t\t\t\t\t    advsimd_cycles_per_iter);\n-      unsigned int min_cost = orig_body_cost * factor + 1;\n-      if (*body_cost < min_cost)\n-\t{\n-\t  if (dump_enabled_p ())\n-\t    dump_printf_loc (MSG_NOTE, vect_location,\n-\t\t\t     \"Increasing body cost to %d because Advanced\"\n-\t\t\t     \" SIMD code could issue as quickly\\n\",\n-\t\t\t     min_cost);\n-\t  *body_cost = min_cost;\n-\t  *should_disparage = true;\n-\t}\n-    }\n-\n   return sve_cycles_per_iter;\n }\n \n@@ -16039,40 +15982,6 @@ adjust_body_cost (loop_vec_info loop_vinfo,\n     dump_printf_loc (MSG_NOTE, vect_location,\n \t\t     \"Original vector body cost = %d\\n\", body_cost);\n \n-  if (m_unrolled_advsimd_stmts)\n-    {\n-      if (dump_enabled_p ())\n-\tdump_printf_loc (MSG_NOTE, vect_location, \"Number of insns in\"\n-\t\t\t \" unrolled Advanced SIMD loop = %d\\n\",\n-\t\t\t m_unrolled_advsimd_stmts);\n-\n-      /* Apply the Advanced SIMD vs. SVE unrolling heuristic described above\n-\t m_unrolled_advsimd_niters.\n-\n-\t The balance here is tricky.  On the one hand, we can't be sure whether\n-\t the code is vectorizable with Advanced SIMD or not.  However, even if\n-\t it isn't vectorizable with Advanced SIMD, there's a possibility that\n-\t the scalar code could also be unrolled.  Some of the code might then\n-\t benefit from SLP, or from using LDP and STP.  We therefore apply\n-\t the heuristic regardless of can_use_advsimd_p.  */\n-      if (m_unrolled_advsimd_stmts\n-\t  && (m_unrolled_advsimd_stmts\n-\t      <= (unsigned int) param_max_completely_peeled_insns))\n-\t{\n-\t  unsigned int estimated_vq = aarch64_estimated_sve_vq ();\n-\t  unsigned int min_cost = (orig_body_cost * estimated_vq) + 1;\n-\t  if (body_cost < min_cost)\n-\t    {\n-\t      if (dump_enabled_p ())\n-\t\tdump_printf_loc (MSG_NOTE, vect_location,\n-\t\t\t\t \"Increasing body cost to %d to account for\"\n-\t\t\t\t \" unrolling\\n\", min_cost);\n-\t      body_cost = min_cost;\n-\t      should_disparage = true;\n-\t    }\n-\t}\n-    }\n-\n   fractional_cost scalar_cycles_per_iter\n     = scalar_ops.min_cycles_per_iter () * estimated_vf;\n \n@@ -16094,30 +16003,10 @@ adjust_body_cost (loop_vec_info loop_vinfo,\n \n   if (vector_ops.sve_issue_info ())\n     {\n-      bool could_use_advsimd\n-\t= (aarch64_autovec_preference != 2\n-\t   && (aarch64_tune_params.extra_tuning_flags\n-\t       & AARCH64_EXTRA_TUNE_MATCHED_VECTOR_THROUGHPUT)\n-\t   && !m_saw_sve_only_op);\n-\n-      fractional_cost advsimd_cycles_per_iter\n-\t= m_advsimd_ops[0].min_cycles_per_iter ();\n       if (dump_enabled_p ())\n-\t{\n-\t  if (could_use_advsimd)\n-\t    {\n-\t      dump_printf_loc (MSG_NOTE, vect_location,\n-\t\t\t       \"Advanced SIMD issue estimate:\\n\");\n-\t      m_advsimd_ops[0].dump ();\n-\t    }\n-\t  else\n-\t    dump_printf_loc (MSG_NOTE, vect_location,\n-\t\t\t     \"Loop could not use Advanced SIMD\\n\");\n-\t  dump_printf_loc (MSG_NOTE, vect_location, \"SVE issue estimate:\\n\");\n-\t}\n+\tdump_printf_loc (MSG_NOTE, vect_location, \"SVE issue estimate:\\n\");\n       vector_cycles_per_iter\n \t= adjust_body_cost_sve (&vector_ops, scalar_cycles_per_iter,\n-\t\t\t\tadvsimd_cycles_per_iter, could_use_advsimd,\n \t\t\t\torig_body_cost, &body_cost, &should_disparage);\n \n       if (aarch64_tune_params.vec_costs == &neoverse512tvb_vector_cost)\n@@ -16130,9 +16019,7 @@ adjust_body_cost (loop_vec_info loop_vinfo,\n \t\t\t     \"Neoverse V1 estimate:\\n\");\n \t  auto vf_factor = m_ops[1].vf_factor ();\n \t  adjust_body_cost_sve (&m_ops[1], scalar_cycles_per_iter * vf_factor,\n-\t\t\t\tadvsimd_cycles_per_iter * vf_factor,\n-\t\t\t\tcould_use_advsimd, orig_body_cost,\n-\t\t\t\t&body_cost, &should_disparage);\n+\t\t\t\torig_body_cost, &body_cost, &should_disparage);\n \t}\n     }\n   else\n@@ -16216,6 +16103,118 @@ aarch64_vector_costs::finish_cost (const vector_costs *uncast_scalar_costs)\n   vector_costs::finish_cost (scalar_costs);\n }\n \n+bool\n+aarch64_vector_costs::\n+better_main_loop_than_p (const vector_costs *uncast_other) const\n+{\n+  auto other = static_cast<const aarch64_vector_costs *> (uncast_other);\n+\n+  auto this_loop_vinfo = as_a<loop_vec_info> (this->m_vinfo);\n+  auto other_loop_vinfo = as_a<loop_vec_info> (other->m_vinfo);\n+\n+  if (dump_enabled_p ())\n+    dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t     \"Comparing two main loops (%s at VF %d vs %s at VF %d)\\n\",\n+\t\t     GET_MODE_NAME (this_loop_vinfo->vector_mode),\n+\t\t     vect_vf_for_cost (this_loop_vinfo),\n+\t\t     GET_MODE_NAME (other_loop_vinfo->vector_mode),\n+\t\t     vect_vf_for_cost (other_loop_vinfo));\n+\n+  /* Apply the unrolling heuristic described above\n+     m_unrolled_advsimd_niters.  */\n+  if (bool (m_unrolled_advsimd_stmts)\n+      != bool (other->m_unrolled_advsimd_stmts))\n+    {\n+      bool this_prefer_unrolled = this->prefer_unrolled_loop ();\n+      bool other_prefer_unrolled = other->prefer_unrolled_loop ();\n+      if (this_prefer_unrolled != other_prefer_unrolled)\n+\t{\n+\t  if (dump_enabled_p ())\n+\t    dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t     \"Preferring Advanced SIMD loop because\"\n+\t\t\t     \" it can be unrolled\\n\");\n+\t  return other_prefer_unrolled;\n+\t}\n+    }\n+\n+  for (unsigned int i = 0; i < m_ops.length (); ++i)\n+    {\n+      if (dump_enabled_p ())\n+\t{\n+\t  if (i)\n+\t    dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t     \"Reconsidering with subtuning %d\\n\", i);\n+\t  dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t   \"Issue info for %s loop:\\n\",\n+\t\t\t   GET_MODE_NAME (this_loop_vinfo->vector_mode));\n+\t  this->m_ops[i].dump ();\n+\t  dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t   \"Issue info for %s loop:\\n\",\n+\t\t\t   GET_MODE_NAME (other_loop_vinfo->vector_mode));\n+\t  other->m_ops[i].dump ();\n+\t}\n+\n+      auto this_estimated_vf = (vect_vf_for_cost (this_loop_vinfo)\n+\t\t\t\t* this->m_ops[i].vf_factor ());\n+      auto other_estimated_vf = (vect_vf_for_cost (other_loop_vinfo)\n+\t\t\t\t * other->m_ops[i].vf_factor ());\n+\n+      /* If it appears that one loop could process the same amount of data\n+\t in fewer cycles, prefer that loop over the other one.  */\n+      fractional_cost this_cost\n+\t= this->m_ops[i].min_cycles_per_iter () * other_estimated_vf;\n+      fractional_cost other_cost\n+\t= other->m_ops[i].min_cycles_per_iter () * this_estimated_vf;\n+      if (dump_enabled_p ())\n+\t{\n+\t  dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t   \"Weighted cycles per iteration of %s loop ~= %f\\n\",\n+\t\t\t   GET_MODE_NAME (this_loop_vinfo->vector_mode),\n+\t\t\t   this_cost.as_double ());\n+\t  dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t   \"Weighted cycles per iteration of %s loop ~= %f\\n\",\n+\t\t\t   GET_MODE_NAME (other_loop_vinfo->vector_mode),\n+\t\t\t   other_cost.as_double ());\n+\t}\n+      if (this_cost != other_cost)\n+\t{\n+\t  if (dump_enabled_p ())\n+\t    dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t     \"Preferring loop with lower cycles\"\n+\t\t\t     \" per iteration\\n\");\n+\t  return this_cost < other_cost;\n+\t}\n+\n+      /* If the issue rate of SVE code is limited by predicate operations\n+\t (i.e. if sve_pred_cycles_per_iter > sve_nonpred_cycles_per_iter),\n+\t and if Advanced SIMD code could issue within the limit imposed\n+\t by the predicate operations, the predicate operations are adding an\n+\t overhead that the original code didn't have and so we should prefer\n+\t the Advanced SIMD version.  */\n+      auto better_pred_limit_p = [](const aarch64_vec_op_count &a,\n+\t\t\t\t    const aarch64_vec_op_count &b) -> bool\n+\t{\n+\t  if (a.pred_ops == 0\n+\t      && (b.min_pred_cycles_per_iter ()\n+\t\t  > b.min_nonpred_cycles_per_iter ()))\n+\t    {\n+\t      if (dump_enabled_p ())\n+\t\tdump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t\t \"Preferring Advanced SIMD loop since\"\n+\t\t\t\t \" SVE loop is predicate-limited\\n\");\n+\t      return true;\n+\t    }\n+\t  return false;\n+\t};\n+      if (better_pred_limit_p (this->m_ops[i], other->m_ops[i]))\n+\treturn true;\n+      if (better_pred_limit_p (other->m_ops[i], this->m_ops[i]))\n+\treturn false;\n+    }\n+\n+  return vector_costs::better_main_loop_than_p (other);\n+}\n+\n static void initialize_aarch64_code_model (struct gcc_options *);\n \n /* Parse the TO_PARSE string and put the architecture struct that it"}]}