{"sha": "2832d07bd187c8298675cf1f6f2020033624e0df", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MjgzMmQwN2JkMTg3YzgyOTg2NzVjZjFmNmYyMDIwMDMzNjI0ZTBkZg==", "commit": {"author": {"name": "Benjamin Kosnik", "email": "bkoz@redhat.com", "date": "2004-06-18T16:52:42Z"}, "committer": {"name": "Benjamin Kosnik", "email": "bkoz@gcc.gnu.org", "date": "2004-06-18T16:52:42Z"}, "message": "pool_allocator.h: Qualify __throw_bad_alloc.\n\n\n2004-06-18  Benjamin Kosnik  <bkoz@redhat.com>\n\n\t* include/ext/pool_allocator.h: Qualify __throw_bad_alloc.\n\t(__pool_base): Remove unused template parameter.  Add\n\tprotected. Move lock data into __pool_base::_Lock. Remove static\n\ton member functions.\n\t(__pool_base::_M_get_free_list): New.\n\t(__pool_alloc): Move _S_force new here.\n\t* src/allocator.cc: Move out of line __pool_base definitions here.\n\t* config/linker-map.gnu: Export bits from __pool_base.\n\nFrom-SVN: r83355", "tree": {"sha": "75d0c5637e84043a4b4cbcbd088941a559af2f3b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/75d0c5637e84043a4b4cbcbd088941a559af2f3b"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/2832d07bd187c8298675cf1f6f2020033624e0df", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2832d07bd187c8298675cf1f6f2020033624e0df", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2832d07bd187c8298675cf1f6f2020033624e0df", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2832d07bd187c8298675cf1f6f2020033624e0df/comments", "author": null, "committer": null, "parents": [{"sha": "64e1e4c4a7825964b7853402053ad5daf95487b3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/64e1e4c4a7825964b7853402053ad5daf95487b3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/64e1e4c4a7825964b7853402053ad5daf95487b3"}], "stats": {"total": 372, "additions": 187, "deletions": 185}, "files": [{"sha": "4561e9e27a987c111fdc02b7ab09fcb2a1c71d52", "filename": "libstdc++-v3/ChangeLog", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2832d07bd187c8298675cf1f6f2020033624e0df/libstdc%2B%2B-v3%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2832d07bd187c8298675cf1f6f2020033624e0df/libstdc%2B%2B-v3%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2FChangeLog?ref=2832d07bd187c8298675cf1f6f2020033624e0df", "patch": "@@ -1,3 +1,14 @@\n+2004-06-18  Benjamin Kosnik  <bkoz@redhat.com>\n+\n+\t* include/ext/pool_allocator.h: Qualify __throw_bad_alloc.\n+\t(__pool_base): Remove unused template parameter.  Add\n+\tprotected. Move lock data into __pool_base::_Lock. Remove static\n+\ton member functions.\n+\t(__pool_base::_M_get_free_list): New.\n+\t(__pool_alloc): Move _S_force new here.\n+\t* src/allocator.cc: Move out of line __pool_base definitions here.\n+\t* config/linker-map.gnu: Export bits from __pool_base.\n+\t\n 2004-06-18  Paolo Carlini  <pcarlini@suse.de>\n \n \t* config/locale/gnu/numeric_members.cc"}, {"sha": "a8f5c02bf6b398c6515352539e2785993c90d73d", "filename": "libstdc++-v3/config/linker-map.gnu", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2832d07bd187c8298675cf1f6f2020033624e0df/libstdc%2B%2B-v3%2Fconfig%2Flinker-map.gnu", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2832d07bd187c8298675cf1f6f2020033624e0df/libstdc%2B%2B-v3%2Fconfig%2Flinker-map.gnu", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Fconfig%2Flinker-map.gnu?ref=2832d07bd187c8298675cf1f6f2020033624e0df", "patch": "@@ -255,6 +255,13 @@ GLIBCXX_3.4.1 {\n  \n } GLIBCXX_3.4;\n  \n+GLIBCXX_3.4.2 {\n+\n+    _ZN9__gnu_cxx11__pool_base5_Lock7_S_lockE;\n+    _ZN9__gnu_cxx11__pool_base9_M_refillEj;\n+    _ZN9__gnu_cxx11__pool_base16_M_get_free_listEj;\n+ \n+} GLIBCXX_3.4.1;\n \n # Symbols in the support library (libsupc++) have their own tag.\n CXXABI_1.3 {"}, {"sha": "e5efac36f5beb7de5dfb1e6e29fe65009458228e", "filename": "libstdc++-v3/include/ext/pool_allocator.h", "status": "modified", "additions": 42, "deletions": 180, "changes": 222, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2832d07bd187c8298675cf1f6f2020033624e0df/libstdc%2B%2B-v3%2Finclude%2Fext%2Fpool_allocator.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2832d07bd187c8298675cf1f6f2020033624e0df/libstdc%2B%2B-v3%2Finclude%2Fext%2Fpool_allocator.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fext%2Fpool_allocator.h?ref=2832d07bd187c8298675cf1f6f2020033624e0df", "patch": "@@ -55,8 +55,6 @@\n \n namespace __gnu_cxx\n {\n-  using std::__throw_bad_alloc;\n-\n   /**\n    *  @if maint\n    *  Uses various allocators to fulfill underlying requests (and makes as\n@@ -71,71 +69,64 @@ namespace __gnu_cxx\n    *     information that we can return the object to the proper free list\n    *     without permanently losing part of the object.\n    *\n-   *  The template parameter specifies whether more than one thread may use\n-   *  this allocator.  It is safe to allocate an object from one instance\n-   *  of the allocator and deallocate it with another one.  This effectively\n-   *  transfers its ownership to the second one.  This may have undesirable\n-   *  effects on reference locality.\n-   *\n    *  @endif\n    *  (See @link Allocators allocators info @endlink for more.)\n    */\n-  template<bool __threads>\n-    struct __pool_base\n+    class __pool_base\n     {\n+    protected:\n+\n       enum { _S_align = 8 };\n       enum { _S_max_bytes = 128 };\n-      enum { _S_freelists = _S_max_bytes / _S_align };\n+      enum { _S_free_list_size = _S_max_bytes / _S_align };\n       \n+      // It would be nice to use _STL_auto_lock here.  But we need a\n+      // test whether threads are in use.\n+      struct _Lock\n+      {\n+\tstatic _STL_mutex_lock        _S_lock;\n+\t_Lock() { _S_lock._M_acquire_lock(); }\n+\t~_Lock() { _S_lock._M_release_lock(); }\n+      };\n+\n       union _Obj\n       {\n \tunion _Obj* _M_free_list_link;\n \tchar        _M_client_data[1];    // The client sees this.\n       };\n       \n-      static _Obj* volatile         _S_free_list[_S_freelists];\n+      static _Obj* volatile         _S_free_list[_S_free_list_size];\n       \n       // Chunk allocation state.\n       static char*                  _S_start_free;\n       static char*                  _S_end_free;\n-      static size_t                 _S_heap_size;\n-      \n-      static _STL_mutex_lock        _S_lock;\n-      static _Atomic_word\t    _S_force_new;\n+      static size_t                 _S_heap_size;     \n       \n-      static size_t\n-      _S_round_up(size_t __bytes)\n+      size_t\n+      _M_round_up(size_t __bytes)\n       { return ((__bytes + (size_t)_S_align - 1) & ~((size_t)_S_align - 1)); }\n       \n-      static size_t\n-      _S_freelist_index(size_t __bytes)\n-      { return ((__bytes + (size_t)_S_align - 1) / (size_t)_S_align - 1); }\n+      _Obj* volatile*\n+      _M_get_free_list(size_t __bytes);\n     \n       // Returns an object of size __n, and optionally adds to size __n\n       // free list.\n-      static void*\n-      _S_refill(size_t __n);\n+      void*\n+      _M_refill(size_t __n);\n       \n       // Allocates a chunk for nobjs of size size.  nobjs may be reduced\n       // if it is inconvenient to allocate the requested number.\n-      static char*\n-      _S_chunk_alloc(size_t __n, int& __nobjs);\n-      \n-      // It would be nice to use _STL_auto_lock here.  But we need a\n-      // test whether threads are in use.\n-      struct _Lock\n-      {\n-\t_Lock() { if (__threads) _S_lock._M_acquire_lock(); }\n-\t~_Lock() { if (__threads) _S_lock._M_release_lock(); }\n-      } __attribute__ ((__unused__));\n-      friend struct _Lock;\n+      char*\n+      _M_allocate_chunk(size_t __n, int& __nobjs);\n     };\n \n-  typedef __pool_base<true> __pool_alloc_base;\n \n   template<typename _Tp>\n-    class __pool_alloc : private __pool_alloc_base\n+    class __pool_alloc : private __pool_base\n     {\n+    private:\n+      static _Atomic_word\t    _S_force_new;\n+\n     public:\n       typedef size_t     size_type;\n       typedef ptrdiff_t  difference_type;\n@@ -194,116 +185,9 @@ namespace __gnu_cxx\n     operator!=(const __pool_alloc<_Tp>&, const __pool_alloc<_Tp>&)\n     { return false; }\n \n-  // Allocate memory in large chunks in order to avoid fragmenting the\n-  // heap too much.  Assume that __n is properly aligned.  We hold\n-  // the allocation lock.\n-  template<bool __threads>\n-    char*\n-    __pool_base<__threads>::_S_chunk_alloc(size_t __n, int& __nobjs)\n-    {\n-      char* __result;\n-      size_t __total_bytes = __n * __nobjs;\n-      size_t __bytes_left = _S_end_free - _S_start_free;\n-      \n-      if (__bytes_left >= __total_bytes)\n-\t{\n-\t  __result = _S_start_free;\n-\t  _S_start_free += __total_bytes;\n-\t  return __result ;\n-\t}\n-      else if (__bytes_left >= __n)\n-\t{\n-\t  __nobjs = (int)(__bytes_left / __n);\n-\t  __total_bytes = __n * __nobjs;\n-\t  __result = _S_start_free;\n-\t  _S_start_free += __total_bytes;\n-\t  return __result;\n-\t}\n-      else\n-\t{\n-\t  size_t __bytes_to_get = (2 * __total_bytes\n-\t\t\t\t   + _S_round_up(_S_heap_size >> 4));\n-\t  // Try to make use of the left-over piece.\n-\t  if (__bytes_left > 0)\n-\t    {\n-\t      _Obj* volatile* __free_list = (_S_free_list\n-\t\t\t\t\t     + _S_freelist_index(__bytes_left));\n-\t      \n-\t      ((_Obj*)(void*)_S_start_free)->_M_free_list_link = *__free_list;\n-\t      *__free_list = (_Obj*)(void*)_S_start_free;\n-\t    }\n-\t  \n-\t  _S_start_free = static_cast<char*>(::operator new(__bytes_to_get));\n-\t  if (_S_start_free == 0)\n-\t    {\n-\t      size_t __i;\n-\t      _Obj* volatile* __free_list;\n-\t      _Obj* __p;\n-\t      // Try to make do with what we have.  That can't hurt.  We\n-\t      // do not try smaller requests, since that tends to result\n-\t      // in disaster on multi-process machines.\n-\t      __i = __n;\n-\t      for (; __i <= (size_t) _S_max_bytes; __i += (size_t) _S_align)\n-\t\t{\n-\t\t  __free_list = _S_free_list + _S_freelist_index(__i);\n-\t\t  __p = *__free_list;\n-\t\t  if (__p != 0)\n-\t\t    {\n-\t\t      *__free_list = __p -> _M_free_list_link;\n-\t\t      _S_start_free = (char*)__p;\n-\t\t      _S_end_free = _S_start_free + __i;\n-\t\t      return _S_chunk_alloc(__n, __nobjs);\n-\t\t      // Any leftover piece will eventually make it to the\n-\t\t      // right free list.\n-\t\t    }\n-\t\t}\n-\t      _S_end_free = 0;        // In case of exception.\n-\t      _S_start_free = static_cast<char*>(::operator new(__bytes_to_get));\n-\t      // This should either throw an exception or remedy the situation.\n-\t      // Thus we assume it succeeded.\n-\t    }\n-\t  _S_heap_size += __bytes_to_get;\n-\t  _S_end_free = _S_start_free + __bytes_to_get;\n-\t  return _S_chunk_alloc(__n, __nobjs);\n-\t}\n-    }\n-  \n-  // Returns an object of size __n, and optionally adds to \"size\n-  // __n\"'s free list.  We assume that __n is properly aligned.  We\n-  // hold the allocation lock.\n-  template<bool __threads>\n-    void*\n-    __pool_base<__threads>::_S_refill(size_t __n)\n-    {\n-      int __nobjs = 20;\n-      char* __chunk = _S_chunk_alloc(__n, __nobjs);\n-      _Obj* volatile* __free_list;\n-      _Obj* __result;\n-      _Obj* __current_obj;\n-      _Obj* __next_obj;\n-      int __i;\n-      \n-      if (1 == __nobjs)\n-\treturn __chunk;\n-      __free_list = _S_free_list + _S_freelist_index(__n);\n-      \n-      // Build free list in chunk.\n-      __result = (_Obj*)(void*)__chunk;\n-      *__free_list = __next_obj = (_Obj*)(void*)(__chunk + __n);\n-      for (__i = 1; ; __i++)\n-\t{\n-\t  __current_obj = __next_obj;\n-\t  __next_obj = (_Obj*)(void*)((char*)__next_obj + __n);\n-\t  if (__nobjs - 1 == __i)\n-\t    {\n-\t      __current_obj -> _M_free_list_link = 0;\n-\t      break;\n-\t    }\n-\t  else\n-\t    __current_obj -> _M_free_list_link = __next_obj;\n-\t}\n-      return __result;\n-    }\n+  template<typename _Tp>\n+    _Atomic_word\n+    __pool_alloc<_Tp>::_S_force_new;\n \n   template<typename _Tp>\n     _Tp*\n@@ -314,7 +198,6 @@ namespace __gnu_cxx\n \t{\n \t  if (__n <= max_size())\n \t    {\n-\t      const size_t __bytes = __n * sizeof(_Tp);\n \t      // If there is a race through here, assume answer from getenv\n \t      // will resolve in same direction.  Inspired by techniques\n \t      // to efficiently support threading found in basic_string.h.\n@@ -325,31 +208,32 @@ namespace __gnu_cxx\n \t\t  else\n \t\t    __atomic_add(&_S_force_new, -1);\n \t\t}\n-\t      \n-\t      if ((__bytes > (size_t) _S_max_bytes) || (_S_force_new > 0))\n+\n+\t      const size_t __bytes = __n * sizeof(_Tp);\t      \n+\t      if (__bytes > size_t(_S_max_bytes) || _S_force_new == 1)\n \t\t__ret = static_cast<_Tp*>(::operator new(__bytes));\n \t      else\n \t\t{\n-\t\t  _Obj* volatile* __free_list = (_S_free_list\n-\t\t\t\t\t\t + _S_freelist_index(__bytes));\n+\t\t  _Obj* volatile* __free_list = _M_get_free_list(__bytes);\n+\n \t\t  // Acquire the lock here with a constructor call.  This\n \t\t  // ensures that it is released in exit or during stack\n \t\t  // unwinding.\n \t\t  _Lock __lock_instance;\n \t\t  _Obj* __restrict__ __result = *__free_list;\n \t\t  if (__builtin_expect(__result == 0, 0))\n-\t\t    __ret = static_cast<_Tp*>(_S_refill(_S_round_up(__bytes)));\n+\t\t    __ret = static_cast<_Tp*>(_M_refill(_M_round_up(__bytes)));\n \t\t  else\n \t\t    {\n \t\t      *__free_list = __result->_M_free_list_link;\n \t\t      __ret = reinterpret_cast<_Tp*>(__result);\n \t\t    }\n \t\t  if (__builtin_expect(__ret == 0, 0))\n-\t\t    __throw_bad_alloc();\n+\t\t    std::__throw_bad_alloc();\n \t\t}\n \t    }\n \t  else\n-\t    __throw_bad_alloc();\n+\t    std::__throw_bad_alloc();\n \t}\n       return __ret;\n     }\n@@ -361,44 +245,22 @@ namespace __gnu_cxx\n       if (__n)\n \t{\n \t  const size_t __bytes = __n * sizeof(_Tp);\n-\t  if ((__bytes > (size_t) _S_max_bytes) || (_S_force_new > 0))\n+\t  if (__bytes > static_cast<size_t>(_S_max_bytes) || _S_force_new == 1)\n \t    ::operator delete(__p);\n \t  else\n \t    {\n-\t      _Obj* volatile* __free_list = (_S_free_list\n-\t\t\t\t\t     + _S_freelist_index(__bytes));\n-\t      _Obj* __q = (_Obj*)__p;\n+\t      _Obj* volatile* __free_list = _M_get_free_list(__bytes);\n+\t      _Obj* __q = reinterpret_cast<_Obj*>(__p);\n \n \t      // Acquire the lock here with a constructor call.  This\n \t      // ensures that it is released in exit or during stack\n \t      // unwinding.\n \t      _Lock __lock_instance;\n-\t      __q -> _M_free_list_link = *__free_list;\n+\t      __q ->_M_free_list_link = *__free_list;\n \t      *__free_list = __q;\n \t    }\n \t}\n     }\n-\n-  template<bool __threads>\n-    typename __pool_base<__threads>::_Obj* volatile\n-    __pool_base<__threads>::_S_free_list[_S_freelists];\n-\n-  template<bool __threads>\n-    char* __pool_base<__threads>::_S_start_free = 0;\n-\n-  template<bool __threads>\n-    char* __pool_base<__threads>::_S_end_free = 0;\n-\n-  template<bool __threads>\n-    size_t __pool_base<__threads>::_S_heap_size = 0;\n-\n-  template<bool __threads>\n-    _STL_mutex_lock\n-    __pool_base<__threads>::_S_lock __STL_MUTEX_INITIALIZER;\n-\n-  template<bool __threads>\n-    _Atomic_word\n-    __pool_base<__threads>::_S_force_new = 0;\n } // namespace __gnu_cxx\n \n #endif"}, {"sha": "d4928735848d29d7a333f9fb90fd85b6abfcb2c0", "filename": "libstdc++-v3/src/allocator.cc", "status": "modified", "additions": 127, "deletions": 5, "changes": 132, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2832d07bd187c8298675cf1f6f2020033624e0df/libstdc%2B%2B-v3%2Fsrc%2Fallocator.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2832d07bd187c8298675cf1f6f2020033624e0df/libstdc%2B%2B-v3%2Fsrc%2Fallocator.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Fsrc%2Fallocator.cc?ref=2832d07bd187c8298675cf1f6f2020033624e0df", "patch": "@@ -36,16 +36,138 @@\n #include <ext/mt_allocator.h>\n #include <ext/pool_allocator.h>\n \n-// Explicitly instantiate the static data members of the underlying\n-// allocator.\n namespace __gnu_cxx\n {\n+  // Instantiations for __mt_alloc.\n   template class __mt_alloc<char>;\n   template class __mt_alloc<wchar_t>;\n \n-  // Static members of __pool_alloc.\n+  // Definitions and instantiations for __pool_alloc and base class.\n+  __pool_base::_Obj* volatile*\n+  __pool_base::_M_get_free_list(size_t __bytes)\n+  { \n+    size_t __i = ((__bytes + (size_t)_S_align - 1) / (size_t)_S_align - 1);\n+    return _S_free_list + __i - 1;\n+  }\n+\n+  // Allocate memory in large chunks in order to avoid fragmenting the\n+  // heap too much.  Assume that __n is properly aligned.  We hold the\n+  // allocation lock.\n+  char*\n+  __pool_base::_M_allocate_chunk(size_t __n, int& __nobjs)\n+  {\n+    char* __result;\n+    size_t __total_bytes = __n * __nobjs;\n+    size_t __bytes_left = _S_end_free - _S_start_free;\n+    \n+    if (__bytes_left >= __total_bytes)\n+      {\n+\t__result = _S_start_free;\n+\t_S_start_free += __total_bytes;\n+\treturn __result ;\n+      }\n+    else if (__bytes_left >= __n)\n+      {\n+\t__nobjs = (int)(__bytes_left / __n);\n+\t__total_bytes = __n * __nobjs;\n+\t__result = _S_start_free;\n+\t_S_start_free += __total_bytes;\n+\treturn __result;\n+      }\n+    else\n+      {\n+\t// Try to make use of the left-over piece.\n+\tif (__bytes_left > 0)\n+\t  {\n+\t    _Obj* volatile* __free_list = _M_get_free_list(__bytes_left);\n+\t    ((_Obj*)(void*)_S_start_free)->_M_free_list_link = *__free_list;\n+\t    *__free_list = (_Obj*)(void*)_S_start_free;\n+\t  }\n+\t\n+\tsize_t __bytes_to_get = (2 * __total_bytes\n+\t\t\t\t + _M_round_up(_S_heap_size >> 4));\n+\t_S_start_free = static_cast<char*>(::operator new(__bytes_to_get));\n+\tif (_S_start_free == 0)\n+\t  {\n+\t    size_t __i;\n+\t    _Obj* volatile* __free_list;\n+\t    _Obj* __p;\n+\n+\t    // Try to make do with what we have.  That can't hurt.  We\n+\t    // do not try smaller requests, since that tends to result\n+\t    // in disaster on multi-process machines.\n+\t    __i = __n;\n+\t    for (; __i <= (size_t) _S_max_bytes; __i += (size_t) _S_align)\n+\t      {\n+\t\t__free_list = _M_get_free_list(__i);\n+\t\t__p = *__free_list;\n+\t\tif (__p != 0)\n+\t\t  {\n+\t\t    *__free_list = __p -> _M_free_list_link;\n+\t\t    _S_start_free = (char*)__p;\n+\t\t    _S_end_free = _S_start_free + __i;\n+\t\t    return _M_allocate_chunk(__n, __nobjs);\n+\t\t    // Any leftover piece will eventually make it to the\n+\t\t    // right free list.\n+\t\t  }\n+\t      }\n+\t    _S_end_free = 0;        // In case of exception.\n+\t    _S_start_free = static_cast<char*>(::operator new(__bytes_to_get));\n+\t    // This should either throw an exception or remedy the situation.\n+\t    // Thus we assume it succeeded.\n+\t  }\n+\t_S_heap_size += __bytes_to_get;\n+\t_S_end_free = _S_start_free + __bytes_to_get;\n+\treturn _M_allocate_chunk(__n, __nobjs);\n+      }\n+  }\n+  \n+  // Returns an object of size __n, and optionally adds to \"size\n+  // __n\"'s free list.  We assume that __n is properly aligned.  We\n+  // hold the allocation lock.\n+  void*\n+  __pool_base::_M_refill(size_t __n)\n+  {\n+    int __nobjs = 20;\n+    char* __chunk = _M_allocate_chunk(__n, __nobjs);\n+    _Obj* volatile* __free_list;\n+    _Obj* __result;\n+    _Obj* __current_obj;\n+    _Obj* __next_obj;\n+    int __i;\n+    \n+    if (1 == __nobjs)\n+      return __chunk;\n+    __free_list = _M_get_free_list(__n);\n+    \n+    // Build free list in chunk.\n+    __result = (_Obj*)(void*)__chunk;\n+    *__free_list = __next_obj = (_Obj*)(void*)(__chunk + __n);\n+    for (__i = 1; ; __i++)\n+      {\n+\t__current_obj = __next_obj;\n+\t__next_obj = (_Obj*)(void*)((char*)__next_obj + __n);\n+\tif (__nobjs - 1 == __i)\n+\t  {\n+\t    __current_obj -> _M_free_list_link = 0;\n+\t    break;\n+\t  }\n+\telse\n+\t  __current_obj -> _M_free_list_link = __next_obj;\n+      }\n+    return __result;\n+  }\n+\n+  __pool_base::_Obj* volatile __pool_base::_S_free_list[_S_free_list_size];\n+  \n+  char* __pool_base::_S_start_free = 0;\n+  \n+  char* __pool_base::_S_end_free = 0;\n+  \n+  size_t __pool_base::_S_heap_size = 0;\n+  \n+  _STL_mutex_lock __pool_base::_Lock::_S_lock __STL_MUTEX_INITIALIZER;\n+  \n   template class __pool_alloc<char>;\n   template class __pool_alloc<wchar_t>;\n-\n-  template class __pool_base<true>;\n } // namespace __gnu_cxx"}]}