{"sha": "3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6M2JiMzQ1YzkzMTNhZDhmNmE2YzI0YWJkN2Q1ZWFhMTE0MTNiYmUyMg==", "commit": {"author": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2021-03-12T13:34:32Z"}, "committer": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2021-03-12T13:34:32Z"}, "message": "i386: Hopefully last set of -mavx512vl -mno-avx512bw fixes [PR99321]\n\nThis is the final patch of the series started with\nhttps://gcc.gnu.org/pipermail/gcc-patches/2021-March/566139.html\nand continued with\nhttps://gcc.gnu.org/pipermail/gcc-patches/2021-March/566356.html\nThis time, I went through all the remaining instructions marked\nby gas as requiring both AVX512BW and AVX512VL and for each checked\ntmp-mddump.md, figure out if it ever could be a problem (e.g. instructions\nthat require AVX512BW+AVX512VL, but didn't exist before AVX512F are usually\nfine, the patterns have the right conditions, the bugs are typically on\npre-AVX512F patterns where we have just blindly added v while they actually\ncan't access those unless AVX512BW+AVX512VL), added test where possible\n(the test doesn't cover MMX though)and fixed md bugs.\n\nFor mmx pextr[bw]/pinsr[bw] patterns it introduces per discussions\na new YW constraint that only requires AVX512BW and not AVX512VL, because\nthose instructions only require the former and not latter when using EVEX\nencoding.\n\nThere are some other interesting details, e.g. most of the 8 interleave\npatterns (vpunck[hl]{bw,wd}) had correctly\n&& <mask_avx512vl_condition> && <mask_avx512bw_condition>\nin the conditions because for masking it needs to be always EVEX encoded\nand then it needs both VL+BW, but 2 of those 8 had just\n&& <mask_avx512vl_condition>\nand so again would run into the -mavx512vl -mno-avx512bw problems.\n\nAnother problem different from others was mmx eq/gt comparisons, that was\nusing Yv constraints, so would happily accept %xmm16+ registers for\n-mavx512vl, but there actually are no such EVEX encoded instructions,\nas AVX512 comparisons work with %k* registers instead.\n\nThe newly added testcase without the patch fails with:\n/tmp/ccVROLo2.s: Assembler messages:\n/tmp/ccVROLo2.s:9: Error: unsupported instruction `vpabsb'\n/tmp/ccVROLo2.s:20: Error: unsupported instruction `vpabsb'\n/tmp/ccVROLo2.s:31: Error: unsupported instruction `vpabsw'\n/tmp/ccVROLo2.s:42: Error: unsupported instruction `vpabsw'\n/tmp/ccVROLo2.s:53: Error: unsupported instruction `vpaddsb'\n/tmp/ccVROLo2.s:64: Error: unsupported instruction `vpaddsb'\n/tmp/ccVROLo2.s:75: Error: unsupported instruction `vpaddsw'\n/tmp/ccVROLo2.s:86: Error: unsupported instruction `vpaddsw'\n/tmp/ccVROLo2.s:97: Error: unsupported instruction `vpsubsb'\n/tmp/ccVROLo2.s:108: Error: unsupported instruction `vpsubsb'\n/tmp/ccVROLo2.s:119: Error: unsupported instruction `vpsubsw'\n/tmp/ccVROLo2.s:130: Error: unsupported instruction `vpsubsw'\n/tmp/ccVROLo2.s:141: Error: unsupported instruction `vpaddusb'\n/tmp/ccVROLo2.s:152: Error: unsupported instruction `vpaddusb'\n/tmp/ccVROLo2.s:163: Error: unsupported instruction `vpaddusw'\n/tmp/ccVROLo2.s:174: Error: unsupported instruction `vpaddusw'\n/tmp/ccVROLo2.s:185: Error: unsupported instruction `vpsubusb'\n/tmp/ccVROLo2.s:196: Error: unsupported instruction `vpsubusb'\n/tmp/ccVROLo2.s:207: Error: unsupported instruction `vpsubusw'\n/tmp/ccVROLo2.s:218: Error: unsupported instruction `vpsubusw'\n/tmp/ccVROLo2.s:258: Error: unsupported instruction `vpaddusw'\n/tmp/ccVROLo2.s:269: Error: unsupported instruction `vpavgb'\n/tmp/ccVROLo2.s:280: Error: unsupported instruction `vpavgb'\n/tmp/ccVROLo2.s:291: Error: unsupported instruction `vpavgw'\n/tmp/ccVROLo2.s:302: Error: unsupported instruction `vpavgw'\n/tmp/ccVROLo2.s:475: Error: unsupported instruction `vpmovsxbw'\n/tmp/ccVROLo2.s:486: Error: unsupported instruction `vpmovsxbw'\n/tmp/ccVROLo2.s:497: Error: unsupported instruction `vpmovzxbw'\n/tmp/ccVROLo2.s:508: Error: unsupported instruction `vpmovzxbw'\n/tmp/ccVROLo2.s:548: Error: unsupported instruction `vpmulhuw'\n/tmp/ccVROLo2.s:559: Error: unsupported instruction `vpmulhuw'\n/tmp/ccVROLo2.s:570: Error: unsupported instruction `vpmulhw'\n/tmp/ccVROLo2.s:581: Error: unsupported instruction `vpmulhw'\n/tmp/ccVROLo2.s:592: Error: unsupported instruction `vpsadbw'\n/tmp/ccVROLo2.s:603: Error: unsupported instruction `vpsadbw'\n/tmp/ccVROLo2.s:643: Error: unsupported instruction `vpshufhw'\n/tmp/ccVROLo2.s:654: Error: unsupported instruction `vpshufhw'\n/tmp/ccVROLo2.s:665: Error: unsupported instruction `vpshuflw'\n/tmp/ccVROLo2.s:676: Error: unsupported instruction `vpshuflw'\n/tmp/ccVROLo2.s:687: Error: unsupported instruction `vpslldq'\n/tmp/ccVROLo2.s:698: Error: unsupported instruction `vpslldq'\n/tmp/ccVROLo2.s:709: Error: unsupported instruction `vpsrldq'\n/tmp/ccVROLo2.s:720: Error: unsupported instruction `vpsrldq'\n/tmp/ccVROLo2.s:899: Error: unsupported instruction `vpunpckhbw'\n/tmp/ccVROLo2.s:910: Error: unsupported instruction `vpunpckhbw'\n/tmp/ccVROLo2.s:921: Error: unsupported instruction `vpunpckhwd'\n/tmp/ccVROLo2.s:932: Error: unsupported instruction `vpunpckhwd'\n/tmp/ccVROLo2.s:943: Error: unsupported instruction `vpunpcklbw'\n/tmp/ccVROLo2.s:954: Error: unsupported instruction `vpunpcklbw'\n/tmp/ccVROLo2.s:965: Error: unsupported instruction `vpunpcklwd'\n/tmp/ccVROLo2.s:976: Error: unsupported instruction `vpunpcklwd'\n\n2021-03-12  Jakub Jelinek  <jakub@redhat.com>\n\n\tPR target/99321\n\t* config/i386/constraints.md (YW): New internal constraint.\n\t* config/i386/sse.md (v_Yw): Add V4TI, V2TI, V1TI and TI cases.\n\t(*<sse2_avx2>_<insn><mode>3<mask_name>,\n\t*<sse2_avx2>_uavg<mode>3<mask_name>, *abs<mode>2,\n\t*<s>mul<mode>3_highpart<mask_name>): Use <v_Yw> instead of v in\n\tconstraints.\n\t(<sse2_avx2>_psadbw): Use YW instead of v in constraints.\n\t(*avx2_pmaddwd, *sse2_pmaddwd, *<code>v8hi3, *<code>v16qi3,\n\tavx2_pmaddubsw256, ssse3_pmaddubsw128): Merge last two alternatives\n\tinto one, use Yw instead of former x,v.\n\t(ashr<mode>3, <insn><mode>3): Use <v_Yw> instead of x in constraints of\n\tthe last alternative.\n\t(<sse2_avx2>_packsswb<mask_name>, <sse2_avx2>_packssdw<mask_name>,\n\t<sse2_avx2>_packuswb<mask_name>, <sse4_1_avx2>_packusdw<mask_name>,\n\t*<ssse3_avx2>_pmulhrsw<mode>3<mask_name>, <ssse3_avx2>_palignr<mode>,\n\t<ssse3_avx2>_pshufb<mode>3<mask_name>): Merge last two alternatives\n\tinto one, use <v_Yw> instead of former x,v.\n\t(avx2_interleave_highv32qi<mask_name>,\n\tvec_interleave_highv16qi<mask_name>): Use Yw instead of v in\n\tconstraints.  Add && <mask_avx512bw_condition> to condition.\n\t(avx2_interleave_lowv32qi<mask_name>,\n\tvec_interleave_lowv16qi<mask_name>,\n\tavx2_interleave_highv16hi<mask_name>,\n\tvec_interleave_highv8hi<mask_name>,\n\tavx2_interleave_lowv16hi<mask_name>, vec_interleave_lowv8hi<mask_name>,\n\tavx2_pshuflw_1<mask_name>, sse2_pshuflw_1<mask_name>,\n\tavx2_pshufhw_1<mask_name>, sse2_pshufhw_1<mask_name>,\n\tavx2_<code>v16qiv16hi2<mask_name>, sse4_1_<code>v8qiv8hi2<mask_name>,\n\t*sse4_1_<code>v8qiv8hi2<mask_name>_1, <sse2_avx2>_<insn><mode>3): Use\n\tYw instead of v in constraints.\n\t* config/i386/mmx.md (Yv_Yw): New define_mode_attr.\n\t(*mmx_<insn><mode>3, mmx_ashr<mode>3, mmx_<insn><mode>3): Use <Yv_Yw>\n\tinstead of Yv in constraints.\n\t(*mmx_<insn><mode>3, *mmx_mulv4hi3, *mmx_smulv4hi3_highpart,\n\t*mmx_umulv4hi3_highpart, *mmx_pmaddwd, *mmx_<code>v4hi3,\n\t*mmx_<code>v8qi3, mmx_pack<s_trunsuffix>swb, mmx_packssdw,\n\tmmx_punpckhbw, mmx_punpcklbw, mmx_punpckhwd, mmx_punpcklwd,\n\t*mmx_uavgv8qi3, *mmx_uavgv4hi3, mmx_psadbw): Use Yw instead of Yv in\n\tconstraints.\n\t(*mmx_pinsrw, *mmx_pinsrb, *mmx_pextrw, *mmx_pextrw_zext, *mmx_pextrb,\n\t*mmx_pextrb_zext): Use YW instead of Yv in constraints.\n\t(*mmx_eq<mode>3, mmx_gt<mode>3): Use x instead of Yv in constraints.\n\t(mmx_andnot<mode>3, *mmx_<code><mode>3): Split last alternative into\n\ttwo, one with just x, another isa avx512vl with v.\n\n\t* gcc.target/i386/avx512vl-pr99321-2.c: New test.", "tree": {"sha": "ebbaa793e577ffbd4e28087df95a8fdf2bfc9e84", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/ebbaa793e577ffbd4e28087df95a8fdf2bfc9e84"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22", "html_url": "https://github.com/Rust-GCC/gccrs/commit/3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22/comments", "author": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "committer": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "425afe1f0c907e6469cef1672160c9c95177e71a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/425afe1f0c907e6469cef1672160c9c95177e71a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/425afe1f0c907e6469cef1672160c9c95177e71a"}], "stats": {"total": 571, "additions": 332, "deletions": 239}, "files": [{"sha": "eaa582d2055699277f3f520ba4b0dff2d31f405b", "filename": "gcc/config/i386/constraints.md", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22/gcc%2Fconfig%2Fi386%2Fconstraints.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22/gcc%2Fconfig%2Fi386%2Fconstraints.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fconstraints.md?ref=3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22", "patch": "@@ -111,6 +111,8 @@\n ;;\totherwise any SSE register\n ;;  w\tany EVEX encodable SSE register for AVX512BW with TARGET_AVX512VL\n ;;\ttarget, otherwise any SSE register.\n+;;  W   any EVEX encodable SSE register for AVX512BW target,\n+;;\totherwise any SSE register.\n \n (define_register_constraint \"Yz\" \"TARGET_SSE ? SSE_FIRST_REG : NO_REGS\"\n  \"First SSE register (@code{%xmm0}).\")\n@@ -151,6 +153,10 @@\n  \"TARGET_AVX512BW && TARGET_AVX512VL ? ALL_SSE_REGS : TARGET_SSE ? SSE_REGS : NO_REGS\"\n  \"@internal Any EVEX encodable SSE register (@code{%xmm0-%xmm31}) for AVX512BW with TARGET_AVX512VL target, otherwise any SSE register.\")\n \n+(define_register_constraint \"YW\"\n+ \"TARGET_AVX512BW ? ALL_SSE_REGS : TARGET_SSE ? SSE_REGS : NO_REGS\"\n+ \"@internal Any EVEX encodable SSE register (@code{%xmm0-%xmm31}) for AVX512BW target, otherwise any SSE register.\")\n+\n ;; We use the B prefix to denote any number of internal operands:\n ;;  f  FLAGS_REG\n ;;  g  GOT memory operand."}, {"sha": "4c2b724dc6f50f5a5a61f76b343f8bd32e8b05d4", "filename": "gcc/config/i386/mmx.md", "status": "modified", "additions": 92, "deletions": 87, "changes": 179, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22/gcc%2Fconfig%2Fi386%2Fmmx.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22/gcc%2Fconfig%2Fi386%2Fmmx.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fmmx.md?ref=3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22", "patch": "@@ -61,6 +61,9 @@\n (define_mode_attr mmxdoublemode\n   [(V8QI \"V8HI\") (V4HI \"V4SI\")])\n \n+(define_mode_attr Yv_Yw\n+  [(V8QI \"Yw\") (V4HI \"Yw\") (V2SI \"Yv\") (V1DI \"Yv\") (V2SF \"Yv\")])\n+\n ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n ;;\n ;; Move patterns\n@@ -1152,10 +1155,10 @@\n   \"ix86_fixup_binary_operands_no_copy (<CODE>, <MODE>mode, operands);\")\n \n (define_insn \"*mmx_<insn><mode>3\"\n-  [(set (match_operand:MMXMODEI8 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:MMXMODEI8 0 \"register_operand\" \"=y,x,<Yv_Yw>\")\n         (plusminus:MMXMODEI8\n-\t  (match_operand:MMXMODEI8 1 \"register_mmxmem_operand\" \"<comm>0,0,Yv\")\n-\t  (match_operand:MMXMODEI8 2 \"register_mmxmem_operand\" \"ym,x,Yv\")))]\n+\t  (match_operand:MMXMODEI8 1 \"register_mmxmem_operand\" \"<comm>0,0,<Yv_Yw>\")\n+\t  (match_operand:MMXMODEI8 2 \"register_mmxmem_operand\" \"ym,x,<Yv_Yw>\")))]\n   \"(TARGET_MMX || TARGET_MMX_WITH_SSE)\n    && ix86_binary_operator_ok (<CODE>, <MODE>mode, operands)\"\n   \"@\n@@ -1176,10 +1179,10 @@\n   \"ix86_fixup_binary_operands_no_copy (<CODE>, <MODE>mode, operands);\")\n \n (define_insn \"*mmx_<insn><mode>3\"\n-  [(set (match_operand:MMXMODE12 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:MMXMODE12 0 \"register_operand\" \"=y,x,Yw\")\n         (sat_plusminus:MMXMODE12\n-\t  (match_operand:MMXMODE12 1 \"register_mmxmem_operand\" \"<comm>0,0,Yv\")\n-\t  (match_operand:MMXMODE12 2 \"register_mmxmem_operand\" \"ym,x,Yv\")))]\n+\t  (match_operand:MMXMODE12 1 \"register_mmxmem_operand\" \"<comm>0,0,Yw\")\n+\t  (match_operand:MMXMODE12 2 \"register_mmxmem_operand\" \"ym,x,Yw\")))]\n   \"(TARGET_MMX || TARGET_MMX_WITH_SSE)\n    && ix86_binary_operator_ok (<CODE>, <MODE>mode, operands)\"\n   \"@\n@@ -1206,9 +1209,9 @@\n   \"ix86_fixup_binary_operands_no_copy (MULT, V4HImode, operands);\")\n \n (define_insn \"*mmx_mulv4hi3\"\n-  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yv\")\n-        (mult:V4HI (match_operand:V4HI 1 \"register_mmxmem_operand\" \"%0,0,Yv\")\n-\t\t   (match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yv\")))]\n+  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yw\")\n+        (mult:V4HI (match_operand:V4HI 1 \"register_mmxmem_operand\" \"%0,0,Yw\")\n+\t\t   (match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yw\")))]\n   \"(TARGET_MMX || TARGET_MMX_WITH_SSE)\n    && ix86_binary_operator_ok (MULT, V4HImode, operands)\"\n   \"@\n@@ -1234,14 +1237,14 @@\n   \"ix86_fixup_binary_operands_no_copy (MULT, V4HImode, operands);\")\n \n (define_insn \"*mmx_smulv4hi3_highpart\"\n-  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yw\")\n \t(truncate:V4HI\n \t  (lshiftrt:V4SI\n \t    (mult:V4SI\n \t      (sign_extend:V4SI\n-\t\t(match_operand:V4HI 1 \"register_mmxmem_operand\" \"%0,0,Yv\"))\n+\t\t(match_operand:V4HI 1 \"register_mmxmem_operand\" \"%0,0,Yw\"))\n \t      (sign_extend:V4SI\n-\t\t(match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yv\")))\n+\t\t(match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yw\")))\n \t    (const_int 16))))]\n   \"(TARGET_MMX || TARGET_MMX_WITH_SSE)\n    && ix86_binary_operator_ok (MULT, V4HImode, operands)\"\n@@ -1269,14 +1272,14 @@\n   \"ix86_fixup_binary_operands_no_copy (MULT, V4HImode, operands);\")\n \n (define_insn \"*mmx_umulv4hi3_highpart\"\n-  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yw\")\n \t(truncate:V4HI\n \t  (lshiftrt:V4SI\n \t    (mult:V4SI\n \t      (zero_extend:V4SI\n-\t\t(match_operand:V4HI 1 \"register_mmxmem_operand\" \"%0,0,Yv\"))\n+\t\t(match_operand:V4HI 1 \"register_mmxmem_operand\" \"%0,0,Yw\"))\n \t      (zero_extend:V4SI\n-\t\t(match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yv\")))\n+\t\t(match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yw\")))\n \t  (const_int 16))))]\n   \"(TARGET_MMX || TARGET_MMX_WITH_SSE)\n    && (TARGET_SSE || TARGET_3DNOW_A)\n@@ -1313,16 +1316,16 @@\n   \"ix86_fixup_binary_operands_no_copy (MULT, V4HImode, operands);\")\n \n (define_insn \"*mmx_pmaddwd\"\n-  [(set (match_operand:V2SI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:V2SI 0 \"register_operand\" \"=y,x,Yw\")\n         (plus:V2SI\n \t  (mult:V2SI\n \t    (sign_extend:V2SI\n \t      (vec_select:V2HI\n-\t\t(match_operand:V4HI 1 \"register_mmxmem_operand\" \"%0,0,Yv\")\n+\t\t(match_operand:V4HI 1 \"register_mmxmem_operand\" \"%0,0,Yw\")\n \t\t(parallel [(const_int 0) (const_int 2)])))\n \t    (sign_extend:V2SI\n \t      (vec_select:V2HI\n-\t\t(match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yv\")\n+\t\t(match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yw\")\n \t\t(parallel [(const_int 0) (const_int 2)]))))\n \t  (mult:V2SI\n \t    (sign_extend:V2SI\n@@ -1432,10 +1435,10 @@\n   \"ix86_fixup_binary_operands_no_copy (<CODE>, V4HImode, operands);\")\n \n (define_insn \"*mmx_<code>v4hi3\"\n-  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yw\")\n         (smaxmin:V4HI\n-\t  (match_operand:V4HI 1 \"register_mmxmem_operand\" \"%0,0,Yv\")\n-\t  (match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yv\")))]\n+\t  (match_operand:V4HI 1 \"register_mmxmem_operand\" \"%0,0,Yw\")\n+\t  (match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yw\")))]\n   \"(TARGET_MMX || TARGET_MMX_WITH_SSE)\n    && (TARGET_SSE || TARGET_3DNOW_A)\n    && ix86_binary_operator_ok (<CODE>, V4HImode, operands)\"\n@@ -1466,10 +1469,10 @@\n   \"ix86_fixup_binary_operands_no_copy (<CODE>, V8QImode, operands);\")\n \n (define_insn \"*mmx_<code>v8qi3\"\n-  [(set (match_operand:V8QI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:V8QI 0 \"register_operand\" \"=y,x,Yw\")\n         (umaxmin:V8QI\n-\t  (match_operand:V8QI 1 \"register_mmxmem_operand\" \"%0,0,Yv\")\n-\t  (match_operand:V8QI 2 \"register_mmxmem_operand\" \"ym,x,Yv\")))]\n+\t  (match_operand:V8QI 1 \"register_mmxmem_operand\" \"%0,0,Yw\")\n+\t  (match_operand:V8QI 2 \"register_mmxmem_operand\" \"ym,x,Yw\")))]\n   \"(TARGET_MMX || TARGET_MMX_WITH_SSE)\n    && (TARGET_SSE || TARGET_3DNOW_A)\n    && ix86_binary_operator_ok (<CODE>, V8QImode, operands)\"\n@@ -1483,10 +1486,10 @@\n    (set_attr \"mode\" \"DI,TI,TI\")])\n \n (define_insn \"mmx_ashr<mode>3\"\n-  [(set (match_operand:MMXMODE24 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:MMXMODE24 0 \"register_operand\" \"=y,x,<Yv_Yw>\")\n         (ashiftrt:MMXMODE24\n-\t  (match_operand:MMXMODE24 1 \"register_operand\" \"0,0,Yv\")\n-\t  (match_operand:DI 2 \"nonmemory_operand\" \"yN,xN,YvN\")))]\n+\t  (match_operand:MMXMODE24 1 \"register_operand\" \"0,0,<Yv_Yw>\")\n+\t  (match_operand:DI 2 \"nonmemory_operand\" \"yN,xN,<Yv_Yw>N\")))]\n   \"TARGET_MMX || TARGET_MMX_WITH_SSE\"\n   \"@\n    psra<mmxvecsize>\\t{%2, %0|%0, %2}\n@@ -1509,10 +1512,10 @@\n   \"TARGET_MMX_WITH_SSE\")\n \n (define_insn \"mmx_<insn><mode>3\"\n-  [(set (match_operand:MMXMODE248 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:MMXMODE248 0 \"register_operand\" \"=y,x,<Yv_Yw>\")\n         (any_lshift:MMXMODE248\n-\t  (match_operand:MMXMODE248 1 \"register_operand\" \"0,0,Yv\")\n-\t  (match_operand:DI 2 \"nonmemory_operand\" \"yN,xN,YvN\")))]\n+\t  (match_operand:MMXMODE248 1 \"register_operand\" \"0,0,<Yv_Yw>\")\n+\t  (match_operand:DI 2 \"nonmemory_operand\" \"yN,xN,<Yv_Yw>N\")))]\n   \"TARGET_MMX || TARGET_MMX_WITH_SSE\"\n   \"@\n    p<vshift><mmxvecsize>\\t{%2, %0|%0, %2}\n@@ -1549,10 +1552,10 @@\n   \"ix86_fixup_binary_operands_no_copy (EQ, <MODE>mode, operands);\")\n \n (define_insn \"*mmx_eq<mode>3\"\n-  [(set (match_operand:MMXMODEI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:MMXMODEI 0 \"register_operand\" \"=y,x,x\")\n         (eq:MMXMODEI\n-\t  (match_operand:MMXMODEI 1 \"register_mmxmem_operand\" \"%0,0,Yv\")\n-\t  (match_operand:MMXMODEI 2 \"register_mmxmem_operand\" \"ym,x,Yv\")))]\n+\t  (match_operand:MMXMODEI 1 \"register_mmxmem_operand\" \"%0,0,x\")\n+\t  (match_operand:MMXMODEI 2 \"register_mmxmem_operand\" \"ym,x,x\")))]\n   \"(TARGET_MMX || TARGET_MMX_WITH_SSE)\n    && ix86_binary_operator_ok (EQ, <MODE>mode, operands)\"\n   \"@\n@@ -1565,10 +1568,10 @@\n    (set_attr \"mode\" \"DI,TI,TI\")])\n \n (define_insn \"mmx_gt<mode>3\"\n-  [(set (match_operand:MMXMODEI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:MMXMODEI 0 \"register_operand\" \"=y,x,x\")\n         (gt:MMXMODEI\n-\t  (match_operand:MMXMODEI 1 \"register_operand\" \"0,0,Yv\")\n-\t  (match_operand:MMXMODEI 2 \"register_mmxmem_operand\" \"ym,x,Yv\")))]\n+\t  (match_operand:MMXMODEI 1 \"register_operand\" \"0,0,x\")\n+\t  (match_operand:MMXMODEI 2 \"register_mmxmem_operand\" \"ym,x,x\")))]\n   \"TARGET_MMX || TARGET_MMX_WITH_SSE\"\n   \"@\n    pcmpgt<mmxvecsize>\\t{%2, %0|%0, %2}\n@@ -1594,19 +1597,20 @@\n   \"operands[2] = force_reg (<MODE>mode, CONSTM1_RTX (<MODE>mode));\")\n \n (define_insn \"mmx_andnot<mode>3\"\n-  [(set (match_operand:MMXMODEI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:MMXMODEI 0 \"register_operand\" \"=y,x,x,v\")\n \t(and:MMXMODEI\n-\t  (not:MMXMODEI (match_operand:MMXMODEI 1 \"register_operand\" \"0,0,Yv\"))\n-\t  (match_operand:MMXMODEI 2 \"register_mmxmem_operand\" \"ym,x,Yv\")))]\n+\t  (not:MMXMODEI (match_operand:MMXMODEI 1 \"register_operand\" \"0,0,x,v\"))\n+\t  (match_operand:MMXMODEI 2 \"register_mmxmem_operand\" \"ym,x,x,v\")))]\n   \"TARGET_MMX || TARGET_MMX_WITH_SSE\"\n   \"@\n    pandn\\t{%2, %0|%0, %2}\n    pandn\\t{%2, %0|%0, %2}\n-   vpandn\\t{%2, %1, %0|%0, %1, %2}\"\n-  [(set_attr \"isa\" \"*,sse2_noavx,avx\")\n-   (set_attr \"mmx_isa\" \"native,*,*\")\n-   (set_attr \"type\" \"mmxadd,sselog,sselog\")\n-   (set_attr \"mode\" \"DI,TI,TI\")])\n+   vpandn\\t{%2, %1, %0|%0, %1, %2}\n+   vpandnd\\t{%2, %1, %0|%0, %1, %2}\"\n+  [(set_attr \"isa\" \"*,sse2_noavx,avx,avx512vl\")\n+   (set_attr \"mmx_isa\" \"native,*,*,*\")\n+   (set_attr \"type\" \"mmxadd,sselog,sselog,sselog\")\n+   (set_attr \"mode\" \"DI,TI,TI,TI\")])\n \n (define_expand \"mmx_<code><mode>3\"\n   [(set (match_operand:MMXMODEI 0 \"register_operand\")\n@@ -1625,20 +1629,21 @@\n   \"ix86_fixup_binary_operands_no_copy (<CODE>, <MODE>mode, operands);\")\n \n (define_insn \"*mmx_<code><mode>3\"\n-  [(set (match_operand:MMXMODEI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:MMXMODEI 0 \"register_operand\" \"=y,x,x,v\")\n         (any_logic:MMXMODEI\n-\t  (match_operand:MMXMODEI 1 \"register_mmxmem_operand\" \"%0,0,Yv\")\n-\t  (match_operand:MMXMODEI 2 \"register_mmxmem_operand\" \"ym,x,Yv\")))]\n+\t  (match_operand:MMXMODEI 1 \"register_mmxmem_operand\" \"%0,0,x,v\")\n+\t  (match_operand:MMXMODEI 2 \"register_mmxmem_operand\" \"ym,x,x,v\")))]\n   \"(TARGET_MMX || TARGET_MMX_WITH_SSE)\n    && ix86_binary_operator_ok (<CODE>, <MODE>mode, operands)\"\n   \"@\n    p<logic>\\t{%2, %0|%0, %2}\n    p<logic>\\t{%2, %0|%0, %2}\n-   vp<logic>\\t{%2, %1, %0|%0, %1, %2}\"\n-  [(set_attr \"isa\" \"*,sse2_noavx,avx\")\n-   (set_attr \"mmx_isa\" \"native,*,*\")\n-   (set_attr \"type\" \"mmxadd,sselog,sselog\")\n-   (set_attr \"mode\" \"DI,TI,TI\")])\n+   vp<logic>\\t{%2, %1, %0|%0, %1, %2}\n+   vp<logic>d\\t{%2, %1, %0|%0, %1, %2}\"\n+  [(set_attr \"isa\" \"*,sse2_noavx,avx,avx512vl\")\n+   (set_attr \"mmx_isa\" \"native,*,*,*\")\n+   (set_attr \"type\" \"mmxadd,sselog,sselog,sselog\")\n+   (set_attr \"mode\" \"DI,TI,TI,TI\")])\n \n ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n ;;\n@@ -1652,12 +1657,12 @@\n (define_code_attr s_trunsuffix [(ss_truncate \"s\") (us_truncate \"u\")])\n \n (define_insn_and_split \"mmx_pack<s_trunsuffix>swb\"\n-  [(set (match_operand:V8QI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:V8QI 0 \"register_operand\" \"=y,x,Yw\")\n \t(vec_concat:V8QI\n \t  (any_s_truncate:V4QI\n-\t    (match_operand:V4HI 1 \"register_operand\" \"0,0,Yv\"))\n+\t    (match_operand:V4HI 1 \"register_operand\" \"0,0,Yw\"))\n \t  (any_s_truncate:V4QI\n-\t    (match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yv\"))))]\n+\t    (match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yw\"))))]\n   \"TARGET_MMX || TARGET_MMX_WITH_SSE\"\n   \"@\n    pack<s_trunsuffix>swb\\t{%2, %0|%0, %2}\n@@ -1672,12 +1677,12 @@\n    (set_attr \"mode\" \"DI,TI,TI\")])\n \n (define_insn_and_split \"mmx_packssdw\"\n-  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yw\")\n \t(vec_concat:V4HI\n \t  (ss_truncate:V2HI\n-\t    (match_operand:V2SI 1 \"register_operand\" \"0,0,Yv\"))\n+\t    (match_operand:V2SI 1 \"register_operand\" \"0,0,Yw\"))\n \t  (ss_truncate:V2HI\n-\t    (match_operand:V2SI 2 \"register_mmxmem_operand\" \"ym,x,Yv\"))))]\n+\t    (match_operand:V2SI 2 \"register_mmxmem_operand\" \"ym,x,Yw\"))))]\n   \"TARGET_MMX || TARGET_MMX_WITH_SSE\"\n   \"@\n    packssdw\\t{%2, %0|%0, %2}\n@@ -1692,11 +1697,11 @@\n    (set_attr \"mode\" \"DI,TI,TI\")])\n \n (define_insn_and_split \"mmx_punpckhbw\"\n-  [(set (match_operand:V8QI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:V8QI 0 \"register_operand\" \"=y,x,Yw\")\n \t(vec_select:V8QI\n \t  (vec_concat:V16QI\n-\t    (match_operand:V8QI 1 \"register_operand\" \"0,0,Yv\")\n-\t    (match_operand:V8QI 2 \"register_mmxmem_operand\" \"ym,x,Yv\"))\n+\t    (match_operand:V8QI 1 \"register_operand\" \"0,0,Yw\")\n+\t    (match_operand:V8QI 2 \"register_mmxmem_operand\" \"ym,x,Yw\"))\n           (parallel [(const_int 4) (const_int 12)\n                      (const_int 5) (const_int 13)\n                      (const_int 6) (const_int 14)\n@@ -1715,11 +1720,11 @@\n    (set_attr \"mode\" \"DI,TI,TI\")])\n \n (define_insn_and_split \"mmx_punpcklbw\"\n-  [(set (match_operand:V8QI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:V8QI 0 \"register_operand\" \"=y,x,Yw\")\n \t(vec_select:V8QI\n \t  (vec_concat:V16QI\n-\t    (match_operand:V8QI 1 \"register_operand\" \"0,0,Yv\")\n-\t    (match_operand:V8QI 2 \"register_mmxmem_operand\" \"ym,x,Yv\"))\n+\t    (match_operand:V8QI 1 \"register_operand\" \"0,0,Yw\")\n+\t    (match_operand:V8QI 2 \"register_mmxmem_operand\" \"ym,x,Yw\"))\n           (parallel [(const_int 0) (const_int 8)\n                      (const_int 1) (const_int 9)\n                      (const_int 2) (const_int 10)\n@@ -1738,11 +1743,11 @@\n    (set_attr \"mode\" \"DI,TI,TI\")])\n \n (define_insn_and_split \"mmx_punpckhwd\"\n-  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yw\")\n \t(vec_select:V4HI\n \t  (vec_concat:V8HI\n-\t    (match_operand:V4HI 1 \"register_operand\" \"0,0,Yv\")\n-\t    (match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yv\"))\n+\t    (match_operand:V4HI 1 \"register_operand\" \"0,0,Yw\")\n+\t    (match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yw\"))\n           (parallel [(const_int 2) (const_int 6)\n                      (const_int 3) (const_int 7)])))]\n   \"TARGET_MMX || TARGET_MMX_WITH_SSE\"\n@@ -1759,11 +1764,11 @@\n    (set_attr \"mode\" \"DI,TI,TI\")])\n \n (define_insn_and_split \"mmx_punpcklwd\"\n-  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yw\")\n \t(vec_select:V4HI\n \t  (vec_concat:V8HI\n-\t    (match_operand:V4HI 1 \"register_operand\" \"0,0,Yv\")\n-\t    (match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yv\"))\n+\t    (match_operand:V4HI 1 \"register_operand\" \"0,0,Yw\")\n+\t    (match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yw\"))\n           (parallel [(const_int 0) (const_int 4)\n                      (const_int 1) (const_int 5)])))]\n   \"TARGET_MMX || TARGET_MMX_WITH_SSE\"\n@@ -1866,11 +1871,11 @@\n })\n \n (define_insn \"*mmx_pinsrw\"\n-  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,YW\")\n         (vec_merge:V4HI\n           (vec_duplicate:V4HI\n             (match_operand:HI 2 \"nonimmediate_operand\" \"rm,rm,rm\"))\n-\t  (match_operand:V4HI 1 \"register_operand\" \"0,0,Yv\")\n+\t  (match_operand:V4HI 1 \"register_operand\" \"0,0,YW\")\n           (match_operand:SI 3 \"const_int_operand\")))]\n   \"(TARGET_MMX || TARGET_MMX_WITH_SSE)\n    && (TARGET_SSE || TARGET_3DNOW_A)\n@@ -1902,11 +1907,11 @@\n    (set_attr \"mode\" \"DI,TI,TI\")])\n \n (define_insn \"*mmx_pinsrb\"\n-  [(set (match_operand:V8QI 0 \"register_operand\" \"=x,Yv\")\n+  [(set (match_operand:V8QI 0 \"register_operand\" \"=x,YW\")\n         (vec_merge:V8QI\n           (vec_duplicate:V8QI\n             (match_operand:QI 2 \"nonimmediate_operand\" \"rm,rm\"))\n-\t  (match_operand:V8QI 1 \"register_operand\" \"0,Yv\")\n+\t  (match_operand:V8QI 1 \"register_operand\" \"0,YW\")\n           (match_operand:SI 3 \"const_int_operand\")))]\n   \"TARGET_MMX_WITH_SSE && TARGET_SSE4_1\n    && ((unsigned) exact_log2 (INTVAL (operands[3]))\n@@ -1940,7 +1945,7 @@\n (define_insn \"*mmx_pextrw\"\n   [(set (match_operand:HI 0 \"register_sse4nonimm_operand\" \"=r,r,m\")\n \t(vec_select:HI\n-\t  (match_operand:V4HI 1 \"register_operand\" \"y,Yv,Yv\")\n+\t  (match_operand:V4HI 1 \"register_operand\" \"y,YW,YW\")\n \t  (parallel [(match_operand:SI 2 \"const_0_to_3_operand\" \"n,n,n\")])))]\n   \"(TARGET_MMX || TARGET_MMX_WITH_SSE)\n    && (TARGET_SSE || TARGET_3DNOW_A)\"\n@@ -1959,7 +1964,7 @@\n   [(set (match_operand:SWI48 0 \"register_operand\" \"=r,r\")\n \t(zero_extend:SWI48\n \t  (vec_select:HI\n-\t    (match_operand:V4HI 1 \"register_operand\" \"y,Yv\")\n+\t    (match_operand:V4HI 1 \"register_operand\" \"y,YW\")\n \t    (parallel [(match_operand:SI 2 \"const_0_to_3_operand\" \"n,n\")]))))]\n   \"(TARGET_MMX || TARGET_MMX_WITH_SSE)\n    && (TARGET_SSE || TARGET_3DNOW_A)\"\n@@ -1976,7 +1981,7 @@\n (define_insn \"*mmx_pextrb\"\n   [(set (match_operand:QI 0 \"nonimmediate_operand\" \"=r,m\")\n \t(vec_select:QI\n-\t  (match_operand:V8QI 1 \"register_operand\" \"Yv,Yv\")\n+\t  (match_operand:V8QI 1 \"register_operand\" \"YW,YW\")\n \t  (parallel [(match_operand:SI 2 \"const_0_to_7_operand\" \"n,n\")])))]\n   \"TARGET_MMX_WITH_SSE && TARGET_SSE4_1\"\n   \"@\n@@ -1993,7 +1998,7 @@\n   [(set (match_operand:SWI248 0 \"register_operand\" \"=r\")\n \t(zero_extend:SWI248\n \t  (vec_select:QI\n-\t    (match_operand:V8QI 1 \"register_operand\" \"Yv\")\n+\t    (match_operand:V8QI 1 \"register_operand\" \"YW\")\n \t    (parallel [(match_operand:SI 2 \"const_0_to_7_operand\" \"n\")]))))]\n   \"TARGET_MMX_WITH_SSE && TARGET_SSE4_1\"\n   \"%vpextrb\\t{%2, %1, %k0|%k0, %1, %2}\"\n@@ -2394,15 +2399,15 @@\n })\n \n (define_insn \"*mmx_uavgv8qi3\"\n-  [(set (match_operand:V8QI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:V8QI 0 \"register_operand\" \"=y,x,Yw\")\n \t(truncate:V8QI\n \t  (lshiftrt:V8HI\n \t    (plus:V8HI\n \t      (plus:V8HI\n \t\t(zero_extend:V8HI\n-\t\t  (match_operand:V8QI 1 \"register_mmxmem_operand\" \"%0,0,Yv\"))\n+\t\t  (match_operand:V8QI 1 \"register_mmxmem_operand\" \"%0,0,Yw\"))\n \t\t(zero_extend:V8HI\n-\t\t  (match_operand:V8QI 2 \"register_mmxmem_operand\" \"ym,x,Yv\")))\n+\t\t  (match_operand:V8QI 2 \"register_mmxmem_operand\" \"ym,x,Yw\")))\n \t      (const_vector:V8HI [(const_int 1) (const_int 1)\n \t\t\t\t  (const_int 1) (const_int 1)\n \t\t\t\t  (const_int 1) (const_int 1)\n@@ -2440,15 +2445,15 @@\n    (set_attr \"mode\" \"DI,TI,TI\")])\n \n (define_insn \"*mmx_uavgv4hi3\"\n-  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yv\")\n+  [(set (match_operand:V4HI 0 \"register_operand\" \"=y,x,Yw\")\n \t(truncate:V4HI\n \t  (lshiftrt:V4SI\n \t    (plus:V4SI\n \t      (plus:V4SI\n \t\t(zero_extend:V4SI\n-\t\t  (match_operand:V4HI 1 \"register_mmxmem_operand\" \"%0,0,Yv\"))\n+\t\t  (match_operand:V4HI 1 \"register_mmxmem_operand\" \"%0,0,Yw\"))\n \t\t(zero_extend:V4SI\n-\t\t  (match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yv\")))\n+\t\t  (match_operand:V4HI 2 \"register_mmxmem_operand\" \"ym,x,Yw\")))\n \t      (const_vector:V4SI [(const_int 1) (const_int 1)\n \t\t\t\t  (const_int 1) (const_int 1)]))\n \t    (const_int 1))))]\n@@ -2483,9 +2488,9 @@\n })\n \n (define_insn \"mmx_psadbw\"\n-  [(set (match_operand:V1DI 0 \"register_operand\" \"=y,x,Yv\")\n-        (unspec:V1DI [(match_operand:V8QI 1 \"register_operand\" \"0,0,Yv\")\n-\t\t      (match_operand:V8QI 2 \"register_mmxmem_operand\" \"ym,x,Yv\")]\n+  [(set (match_operand:V1DI 0 \"register_operand\" \"=y,x,Yw\")\n+        (unspec:V1DI [(match_operand:V8QI 1 \"register_operand\" \"0,0,Yw\")\n+\t\t      (match_operand:V8QI 2 \"register_mmxmem_operand\" \"ym,x,Yw\")]\n \t\t     UNSPEC_PSADBW))]\n   \"(TARGET_MMX || TARGET_MMX_WITH_SSE)\n    && (TARGET_SSE || TARGET_3DNOW_A)\""}, {"sha": "2cd8e04b913a9b84bc0bba572d9efa9f1ac940c0", "filename": "gcc/config/i386/sse.md", "status": "modified", "additions": 140, "deletions": 152, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22/gcc%2Fconfig%2Fi386%2Fsse.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22/gcc%2Fconfig%2Fi386%2Fsse.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fsse.md?ref=3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22", "patch": "@@ -566,7 +566,8 @@\n    (V4SI \"v\") (V8SI \"v\") (V16SI \"v\")\n    (V2DI \"v\") (V4DI \"v\") (V8DI \"v\")\n    (V4SF \"v\") (V8SF \"v\") (V16SF \"v\")\n-   (V2DF \"v\") (V4DF \"v\") (V8DF \"v\")])\n+   (V2DF \"v\") (V4DF \"v\") (V8DF \"v\")\n+   (TI \"Yw\") (V1TI \"Yw\") (V2TI \"Yw\") (V4TI \"v\")])\n \n (define_mode_attr sse2_avx_avx512f\n   [(V16QI \"sse2\") (V32QI \"avx\") (V64QI \"avx512f\")\n@@ -11736,10 +11737,10 @@\n   \"ix86_fixup_binary_operands_no_copy (<CODE>, <MODE>mode, operands);\")\n \n (define_insn \"*<sse2_avx2>_<insn><mode>3<mask_name>\"\n-  [(set (match_operand:VI12_AVX2_AVX512BW 0 \"register_operand\" \"=x,v\")\n+  [(set (match_operand:VI12_AVX2_AVX512BW 0 \"register_operand\" \"=x,<v_Yw>\")\n \t(sat_plusminus:VI12_AVX2_AVX512BW\n-\t  (match_operand:VI12_AVX2_AVX512BW 1 \"vector_operand\" \"<comm>0,v\")\n-\t  (match_operand:VI12_AVX2_AVX512BW 2 \"vector_operand\" \"xBm,vm\")))]\n+\t  (match_operand:VI12_AVX2_AVX512BW 1 \"vector_operand\" \"<comm>0,<v_Yw>\")\n+\t  (match_operand:VI12_AVX2_AVX512BW 2 \"vector_operand\" \"xBm,<v_Yw>m\")))]\n   \"TARGET_SSE2 && <mask_mode512bit_condition> && <mask_avx512bw_condition>\n    && ix86_binary_operator_ok (<CODE>, <MODE>mode, operands)\"\n   \"@\n@@ -11827,14 +11828,14 @@\n   \"ix86_fixup_binary_operands_no_copy (MULT, <MODE>mode, operands);\")\n \n (define_insn \"*<s>mul<mode>3_highpart<mask_name>\"\n-  [(set (match_operand:VI2_AVX2 0 \"register_operand\" \"=x,v\")\n+  [(set (match_operand:VI2_AVX2 0 \"register_operand\" \"=x,<v_Yw>\")\n \t(truncate:VI2_AVX2\n \t  (lshiftrt:<ssedoublemode>\n \t    (mult:<ssedoublemode>\n \t      (any_extend:<ssedoublemode>\n-\t\t(match_operand:VI2_AVX2 1 \"vector_operand\" \"%0,v\"))\n+\t\t(match_operand:VI2_AVX2 1 \"vector_operand\" \"%0,<v_Yw>\"))\n \t      (any_extend:<ssedoublemode>\n-\t\t(match_operand:VI2_AVX2 2 \"vector_operand\" \"xBm,vm\")))\n+\t\t(match_operand:VI2_AVX2 2 \"vector_operand\" \"xBm,<v_Yw>m\")))\n \t    (const_int 16))))]\n   \"TARGET_SSE2 && !(MEM_P (operands[1]) && MEM_P (operands[2]))\n    && <mask_mode512bit_condition> && <mask_avx512bw_condition>\"\n@@ -12128,19 +12129,19 @@\n   \"ix86_fixup_binary_operands_no_copy (MULT, V16HImode, operands);\")\n \n (define_insn \"*avx2_pmaddwd\"\n-  [(set (match_operand:V8SI 0 \"register_operand\" \"=x,v\")\n+  [(set (match_operand:V8SI 0 \"register_operand\" \"=Yw\")\n \t(plus:V8SI\n \t  (mult:V8SI\n \t    (sign_extend:V8SI\n \t      (vec_select:V8HI\n-\t\t(match_operand:V16HI 1 \"nonimmediate_operand\" \"%x,v\")\n+\t\t(match_operand:V16HI 1 \"nonimmediate_operand\" \"%Yw\")\n \t\t(parallel [(const_int 0) (const_int 2)\n \t\t\t   (const_int 4) (const_int 6)\n \t\t\t   (const_int 8) (const_int 10)\n \t\t\t   (const_int 12) (const_int 14)])))\n \t    (sign_extend:V8SI\n \t      (vec_select:V8HI\n-\t\t(match_operand:V16HI 2 \"nonimmediate_operand\" \"xm,vm\")\n+\t\t(match_operand:V16HI 2 \"nonimmediate_operand\" \"Ywm\")\n \t\t(parallel [(const_int 0) (const_int 2)\n \t\t\t   (const_int 4) (const_int 6)\n \t\t\t   (const_int 8) (const_int 10)\n@@ -12161,8 +12162,7 @@\n   \"TARGET_AVX2 && !(MEM_P (operands[1]) && MEM_P (operands[2]))\"\n   \"vpmaddwd\\t{%2, %1, %0|%0, %1, %2}\"\n   [(set_attr \"type\" \"sseiadd\")\n-   (set_attr \"isa\" \"*,avx512bw\")\n-   (set_attr \"prefix\" \"vex,evex\")\n+   (set_attr \"prefix\" \"vex\")\n    (set_attr \"mode\" \"OI\")])\n \n (define_expand \"sse2_pmaddwd\"\n@@ -12192,17 +12192,17 @@\n   \"ix86_fixup_binary_operands_no_copy (MULT, V8HImode, operands);\")\n \n (define_insn \"*sse2_pmaddwd\"\n-  [(set (match_operand:V4SI 0 \"register_operand\" \"=x,x,v\")\n+  [(set (match_operand:V4SI 0 \"register_operand\" \"=x,Yw\")\n \t(plus:V4SI\n \t  (mult:V4SI\n \t    (sign_extend:V4SI\n \t      (vec_select:V4HI\n-\t\t(match_operand:V8HI 1 \"vector_operand\" \"%0,x,v\")\n+\t\t(match_operand:V8HI 1 \"vector_operand\" \"%0,Yw\")\n \t\t(parallel [(const_int 0) (const_int 2)\n \t\t\t   (const_int 4) (const_int 6)])))\n \t    (sign_extend:V4SI\n \t      (vec_select:V4HI\n-\t\t(match_operand:V8HI 2 \"vector_operand\" \"xBm,xm,vm\")\n+\t\t(match_operand:V8HI 2 \"vector_operand\" \"xBm,Ywm\")\n \t\t(parallel [(const_int 0) (const_int 2)\n \t\t\t   (const_int 4) (const_int 6)]))))\n \t  (mult:V4SI\n@@ -12217,13 +12217,12 @@\n   \"TARGET_SSE2 && !(MEM_P (operands[1]) && MEM_P (operands[2]))\"\n   \"@\n    pmaddwd\\t{%2, %0|%0, %2}\n-   vpmaddwd\\t{%2, %1, %0|%0, %1, %2}\n    vpmaddwd\\t{%2, %1, %0|%0, %1, %2}\"\n-  [(set_attr \"isa\" \"noavx,avx,avx512bw\")\n+  [(set_attr \"isa\" \"noavx,avx\")\n    (set_attr \"type\" \"sseiadd\")\n    (set_attr \"atom_unit\" \"simul\")\n-   (set_attr \"prefix_data16\" \"1,*,*\")\n-   (set_attr \"prefix\" \"orig,vex,evex\")\n+   (set_attr \"prefix_data16\" \"1,*\")\n+   (set_attr \"prefix\" \"orig,vex\")\n    (set_attr \"mode\" \"TI\")])\n \n (define_insn \"avx512dq_mul<mode>3<mask_name>\"\n@@ -12449,10 +12448,10 @@\n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n (define_insn \"ashr<mode>3\"\n-  [(set (match_operand:VI24_AVX2 0 \"register_operand\" \"=x,x\")\n+  [(set (match_operand:VI24_AVX2 0 \"register_operand\" \"=x,<v_Yw>\")\n \t(ashiftrt:VI24_AVX2\n-\t  (match_operand:VI24_AVX2 1 \"register_operand\" \"0,x\")\n-\t  (match_operand:DI 2 \"nonmemory_operand\" \"xN,xN\")))]\n+\t  (match_operand:VI24_AVX2 1 \"register_operand\" \"0,<v_Yw>\")\n+\t  (match_operand:DI 2 \"nonmemory_operand\" \"xN,YwN\")))]\n   \"TARGET_SSE2\"\n   \"@\n    psra<ssemodesuffix>\\t{%2, %0|%0, %2}\n@@ -12496,10 +12495,10 @@\n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n (define_insn \"<insn><mode>3\"\n-  [(set (match_operand:VI248_AVX2 0 \"register_operand\" \"=x,x\")\n+  [(set (match_operand:VI248_AVX2 0 \"register_operand\" \"=x,<v_Yw>\")\n \t(any_lshift:VI248_AVX2\n-\t  (match_operand:VI248_AVX2 1 \"register_operand\" \"0,x\")\n-\t  (match_operand:DI 2 \"nonmemory_operand\" \"xN,xN\")))]\n+\t  (match_operand:VI248_AVX2 1 \"register_operand\" \"0,<v_Yw>\")\n+\t  (match_operand:DI 2 \"nonmemory_operand\" \"xN,YwN\")))]\n   \"TARGET_SSE2\"\n   \"@\n    p<vshift><ssemodesuffix>\\t{%2, %0|%0, %2}\n@@ -12571,9 +12570,9 @@\n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n (define_insn \"<sse2_avx2>_<insn><mode>3\"\n-  [(set (match_operand:VIMAX_AVX2 0 \"register_operand\" \"=x,v\")\n+  [(set (match_operand:VIMAX_AVX2 0 \"register_operand\" \"=x,Yw\")\n \t(any_lshift:VIMAX_AVX2\n-\t (match_operand:VIMAX_AVX2 1 \"register_operand\" \"0,v\")\n+\t (match_operand:VIMAX_AVX2 1 \"register_operand\" \"0,Yw\")\n \t (match_operand:SI 2 \"const_0_to_255_mul_8_operand\" \"n,n\")))]\n   \"TARGET_SSE2\"\n {\n@@ -12771,20 +12770,19 @@\n    (set_attr \"mode\" \"TI\")])\n \n (define_insn \"*<code>v8hi3\"\n-  [(set (match_operand:V8HI 0 \"register_operand\" \"=x,x,v\")\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=x,Yw\")\n \t(smaxmin:V8HI\n-\t  (match_operand:V8HI 1 \"vector_operand\" \"%0,x,v\")\n-\t  (match_operand:V8HI 2 \"vector_operand\" \"xBm,xm,vm\")))]\n+\t  (match_operand:V8HI 1 \"vector_operand\" \"%0,Yw\")\n+\t  (match_operand:V8HI 2 \"vector_operand\" \"xBm,Ywm\")))]\n   \"TARGET_SSE2 && !(MEM_P (operands[1]) && MEM_P (operands[2]))\"\n   \"@\n    p<maxmin_int>w\\t{%2, %0|%0, %2}\n-   vp<maxmin_int>w\\t{%2, %1, %0|%0, %1, %2}\n    vp<maxmin_int>w\\t{%2, %1, %0|%0, %1, %2}\"\n-  [(set_attr \"isa\" \"noavx,avx,avx512bw\")\n+  [(set_attr \"isa\" \"noavx,avx\")\n    (set_attr \"type\" \"sseiadd\")\n-   (set_attr \"prefix_data16\" \"1,*,*\")\n-   (set_attr \"prefix_extra\" \"*,1,1\")\n-   (set_attr \"prefix\" \"orig,vex,evex\")\n+   (set_attr \"prefix_data16\" \"1,*\")\n+   (set_attr \"prefix_extra\" \"*,1\")\n+   (set_attr \"prefix\" \"orig,vex\")\n    (set_attr \"mode\" \"TI\")])\n \n (define_expand \"<code><mode>3\"\n@@ -12856,20 +12854,19 @@\n    (set_attr \"mode\" \"TI\")])\n \n (define_insn \"*<code>v16qi3\"\n-  [(set (match_operand:V16QI 0 \"register_operand\" \"=x,x,v\")\n+  [(set (match_operand:V16QI 0 \"register_operand\" \"=x,Yw\")\n \t(umaxmin:V16QI\n-\t  (match_operand:V16QI 1 \"vector_operand\" \"%0,x,v\")\n-\t  (match_operand:V16QI 2 \"vector_operand\" \"xBm,xm,vm\")))]\n+\t  (match_operand:V16QI 1 \"vector_operand\" \"%0,Yw\")\n+\t  (match_operand:V16QI 2 \"vector_operand\" \"xBm,Ywm\")))]\n   \"TARGET_SSE2 && !(MEM_P (operands[1]) && MEM_P (operands[2]))\"\n   \"@\n    p<maxmin_int>b\\t{%2, %0|%0, %2}\n-   vp<maxmin_int>b\\t{%2, %1, %0|%0, %1, %2}\n    vp<maxmin_int>b\\t{%2, %1, %0|%0, %1, %2}\"\n-  [(set_attr \"isa\" \"noavx,avx,avx512bw\")\n+  [(set_attr \"isa\" \"noavx,avx\")\n    (set_attr \"type\" \"sseiadd\")\n-   (set_attr \"prefix_data16\" \"1,*,*\")\n-   (set_attr \"prefix_extra\" \"*,1,1\")\n-   (set_attr \"prefix\" \"orig,vex,evex\")\n+   (set_attr \"prefix_data16\" \"1,*\")\n+   (set_attr \"prefix_extra\" \"*,1\")\n+   (set_attr \"prefix\" \"orig,vex\")\n    (set_attr \"mode\" \"TI\")])\n \n ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n@@ -13888,57 +13885,54 @@\n })\n \n (define_insn \"<sse2_avx2>_packsswb<mask_name>\"\n-  [(set (match_operand:VI1_AVX512 0 \"register_operand\" \"=x,x,v\")\n+  [(set (match_operand:VI1_AVX512 0 \"register_operand\" \"=x,<v_Yw>\")\n \t(vec_concat:VI1_AVX512\n \t  (ss_truncate:<ssehalfvecmode>\n-\t    (match_operand:<sseunpackmode> 1 \"register_operand\" \"0,x,v\"))\n+\t    (match_operand:<sseunpackmode> 1 \"register_operand\" \"0,<v_Yw>\"))\n \t  (ss_truncate:<ssehalfvecmode>\n-\t    (match_operand:<sseunpackmode> 2 \"vector_operand\" \"xBm,xm,vm\"))))]\n+\t    (match_operand:<sseunpackmode> 2 \"vector_operand\" \"xBm,<v_Yw>m\"))))]\n   \"TARGET_SSE2 && <mask_mode512bit_condition> && <mask_avx512bw_condition>\"\n   \"@\n    packsswb\\t{%2, %0|%0, %2}\n-   vpacksswb\\t{%2, %1, %0<mask_operand3>|%0<mask_operand3>, %1, %2}\n    vpacksswb\\t{%2, %1, %0<mask_operand3>|%0<mask_operand3>, %1, %2}\"\n-  [(set_attr \"isa\" \"noavx,avx,avx512bw\")\n+  [(set_attr \"isa\" \"noavx,avx\")\n    (set_attr \"type\" \"sselog\")\n-   (set_attr \"prefix_data16\" \"1,*,*\")\n-   (set_attr \"prefix\" \"orig,<mask_prefix>,evex\")\n+   (set_attr \"prefix_data16\" \"1,*\")\n+   (set_attr \"prefix\" \"orig,<mask_prefix>\")\n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n (define_insn \"<sse2_avx2>_packssdw<mask_name>\"\n-  [(set (match_operand:VI2_AVX2 0 \"register_operand\" \"=x,x,v\")\n+  [(set (match_operand:VI2_AVX2 0 \"register_operand\" \"=x,<v_Yw>\")\n \t(vec_concat:VI2_AVX2\n \t  (ss_truncate:<ssehalfvecmode>\n-\t    (match_operand:<sseunpackmode> 1 \"register_operand\" \"0,x,v\"))\n+\t    (match_operand:<sseunpackmode> 1 \"register_operand\" \"0,<v_Yw>\"))\n \t  (ss_truncate:<ssehalfvecmode>\n-\t    (match_operand:<sseunpackmode> 2 \"vector_operand\" \"xBm,xm,vm\"))))]\n+\t    (match_operand:<sseunpackmode> 2 \"vector_operand\" \"xBm,<v_Yw>m\"))))]\n   \"TARGET_SSE2 && <mask_mode512bit_condition> && <mask_avx512bw_condition>\"\n   \"@\n    packssdw\\t{%2, %0|%0, %2}\n-   vpackssdw\\t{%2, %1, %0<mask_operand3>|%0<mask_operand3>, %1, %2}\n    vpackssdw\\t{%2, %1, %0<mask_operand3>|%0<mask_operand3>, %1, %2}\"\n-  [(set_attr \"isa\" \"noavx,avx,avx512bw\")\n+  [(set_attr \"isa\" \"noavx,avx\")\n    (set_attr \"type\" \"sselog\")\n-   (set_attr \"prefix_data16\" \"1,*,*\")\n-   (set_attr \"prefix\" \"orig,<mask_prefix>,evex\")\n+   (set_attr \"prefix_data16\" \"1,*\")\n+   (set_attr \"prefix\" \"orig,<mask_prefix>\")\n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n (define_insn \"<sse2_avx2>_packuswb<mask_name>\"\n-  [(set (match_operand:VI1_AVX512 0 \"register_operand\" \"=x,x,v\")\n+  [(set (match_operand:VI1_AVX512 0 \"register_operand\" \"=x,<v_Yw>\")\n \t(vec_concat:VI1_AVX512\n \t  (us_truncate:<ssehalfvecmode>\n-\t    (match_operand:<sseunpackmode> 1 \"register_operand\" \"0,x,v\"))\n+\t    (match_operand:<sseunpackmode> 1 \"register_operand\" \"0,<v_Yw>\"))\n \t  (us_truncate:<ssehalfvecmode>\n-\t    (match_operand:<sseunpackmode> 2 \"vector_operand\" \"xBm,xm,vm\"))))]\n+\t    (match_operand:<sseunpackmode> 2 \"vector_operand\" \"xBm,<v_Yw>m\"))))]\n   \"TARGET_SSE2 && <mask_mode512bit_condition> && <mask_avx512bw_condition>\"\n   \"@\n    packuswb\\t{%2, %0|%0, %2}\n-   vpackuswb\\t{%2, %1, %0<mask_operand3>|%0<mask_operand3>, %1, %2}\n    vpackuswb\\t{%2, %1, %0<mask_operand3>|%0<mask_operand3>, %1, %2}\"\n-  [(set_attr \"isa\" \"noavx,avx,avx512bw\")\n+  [(set_attr \"isa\" \"noavx,avx\")\n    (set_attr \"type\" \"sselog\")\n-   (set_attr \"prefix_data16\" \"1,*,*\")\n-   (set_attr \"prefix\" \"orig,<mask_prefix>,evex\")\n+   (set_attr \"prefix_data16\" \"1,*\")\n+   (set_attr \"prefix\" \"orig,<mask_prefix>\")\n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n (define_insn \"avx512bw_interleave_highv64qi<mask_name>\"\n@@ -13986,11 +13980,11 @@\n    (set_attr \"mode\" \"XI\")])\n \n (define_insn \"avx2_interleave_highv32qi<mask_name>\"\n-  [(set (match_operand:V32QI 0 \"register_operand\" \"=v\")\n+  [(set (match_operand:V32QI 0 \"register_operand\" \"=Yw\")\n \t(vec_select:V32QI\n \t  (vec_concat:V64QI\n-\t    (match_operand:V32QI 1 \"register_operand\" \"v\")\n-\t    (match_operand:V32QI 2 \"nonimmediate_operand\" \"vm\"))\n+\t    (match_operand:V32QI 1 \"register_operand\" \"Yw\")\n+\t    (match_operand:V32QI 2 \"nonimmediate_operand\" \"Ywm\"))\n \t  (parallel [(const_int 8)  (const_int 40)\n \t\t     (const_int 9)  (const_int 41)\n \t\t     (const_int 10) (const_int 42)\n@@ -14007,18 +14001,18 @@\n \t\t     (const_int 29) (const_int 61)\n \t\t     (const_int 30) (const_int 62)\n \t\t     (const_int 31) (const_int 63)])))]\n-  \"TARGET_AVX2 && <mask_avx512vl_condition>\"\n+  \"TARGET_AVX2 && <mask_avx512vl_condition> && <mask_avx512bw_condition>\"\n   \"vpunpckhbw\\t{%2, %1, %0<mask_operand3>|%0<mask_operand3>, %1, %2}\"\n   [(set_attr \"type\" \"sselog\")\n    (set_attr \"prefix\" \"<mask_prefix>\")\n    (set_attr \"mode\" \"OI\")])\n \n (define_insn \"vec_interleave_highv16qi<mask_name>\"\n-  [(set (match_operand:V16QI 0 \"register_operand\" \"=x,v\")\n+  [(set (match_operand:V16QI 0 \"register_operand\" \"=x,Yw\")\n \t(vec_select:V16QI\n \t  (vec_concat:V32QI\n-\t    (match_operand:V16QI 1 \"register_operand\" \"0,v\")\n-\t    (match_operand:V16QI 2 \"vector_operand\" \"xBm,vm\"))\n+\t    (match_operand:V16QI 1 \"register_operand\" \"0,Yw\")\n+\t    (match_operand:V16QI 2 \"vector_operand\" \"xBm,Ywm\"))\n \t  (parallel [(const_int 8)  (const_int 24)\n \t\t     (const_int 9)  (const_int 25)\n \t\t     (const_int 10) (const_int 26)\n@@ -14027,7 +14021,7 @@\n \t\t     (const_int 13) (const_int 29)\n \t\t     (const_int 14) (const_int 30)\n \t\t     (const_int 15) (const_int 31)])))]\n-  \"TARGET_SSE2 && <mask_avx512vl_condition>\"\n+  \"TARGET_SSE2 && <mask_avx512vl_condition> && <mask_avx512bw_condition>\"\n   \"@\n    punpckhbw\\t{%2, %0|%0, %2}\n    vpunpckhbw\\t{%2, %1, %0<mask_operand3>|%0<mask_operand3>, %1, %2}\"\n@@ -14082,11 +14076,11 @@\n    (set_attr \"mode\" \"XI\")])\n \n (define_insn \"avx2_interleave_lowv32qi<mask_name>\"\n-  [(set (match_operand:V32QI 0 \"register_operand\" \"=v\")\n+  [(set (match_operand:V32QI 0 \"register_operand\" \"=Yw\")\n \t(vec_select:V32QI\n \t  (vec_concat:V64QI\n-\t    (match_operand:V32QI 1 \"register_operand\" \"v\")\n-\t    (match_operand:V32QI 2 \"nonimmediate_operand\" \"vm\"))\n+\t    (match_operand:V32QI 1 \"register_operand\" \"Yw\")\n+\t    (match_operand:V32QI 2 \"nonimmediate_operand\" \"Ywm\"))\n \t  (parallel [(const_int 0) (const_int 32)\n \t\t     (const_int 1) (const_int 33)\n \t\t     (const_int 2) (const_int 34)\n@@ -14110,11 +14104,11 @@\n    (set_attr \"mode\" \"OI\")])\n \n (define_insn \"vec_interleave_lowv16qi<mask_name>\"\n-  [(set (match_operand:V16QI 0 \"register_operand\" \"=x,v\")\n+  [(set (match_operand:V16QI 0 \"register_operand\" \"=x,Yw\")\n \t(vec_select:V16QI\n \t  (vec_concat:V32QI\n-\t    (match_operand:V16QI 1 \"register_operand\" \"0,v\")\n-\t    (match_operand:V16QI 2 \"vector_operand\" \"xBm,vm\"))\n+\t    (match_operand:V16QI 1 \"register_operand\" \"0,Yw\")\n+\t    (match_operand:V16QI 2 \"vector_operand\" \"xBm,Ywm\"))\n \t  (parallel [(const_int 0) (const_int 16)\n \t\t     (const_int 1) (const_int 17)\n \t\t     (const_int 2) (const_int 18)\n@@ -14162,11 +14156,11 @@\n    (set_attr \"mode\" \"XI\")])\n \n (define_insn \"avx2_interleave_highv16hi<mask_name>\"\n-  [(set (match_operand:V16HI 0 \"register_operand\" \"=v\")\n+  [(set (match_operand:V16HI 0 \"register_operand\" \"=Yw\")\n \t(vec_select:V16HI\n \t  (vec_concat:V32HI\n-\t    (match_operand:V16HI 1 \"register_operand\" \"v\")\n-\t    (match_operand:V16HI 2 \"nonimmediate_operand\" \"vm\"))\n+\t    (match_operand:V16HI 1 \"register_operand\" \"Yw\")\n+\t    (match_operand:V16HI 2 \"nonimmediate_operand\" \"Ywm\"))\n \t  (parallel [(const_int 4) (const_int 20)\n \t\t     (const_int 5) (const_int 21)\n \t\t     (const_int 6) (const_int 22)\n@@ -14182,11 +14176,11 @@\n    (set_attr \"mode\" \"OI\")])\n \n (define_insn \"vec_interleave_highv8hi<mask_name>\"\n-  [(set (match_operand:V8HI 0 \"register_operand\" \"=x,v\")\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=x,Yw\")\n \t(vec_select:V8HI\n \t  (vec_concat:V16HI\n-\t    (match_operand:V8HI 1 \"register_operand\" \"0,v\")\n-\t    (match_operand:V8HI 2 \"vector_operand\" \"xBm,vm\"))\n+\t    (match_operand:V8HI 1 \"register_operand\" \"0,Yw\")\n+\t    (match_operand:V8HI 2 \"vector_operand\" \"xBm,Ywm\"))\n \t  (parallel [(const_int 4) (const_int 12)\n \t\t     (const_int 5) (const_int 13)\n \t\t     (const_int 6) (const_int 14)\n@@ -14230,11 +14224,11 @@\n    (set_attr \"mode\" \"XI\")])\n \n (define_insn \"avx2_interleave_lowv16hi<mask_name>\"\n-  [(set (match_operand:V16HI 0 \"register_operand\" \"=v\")\n+  [(set (match_operand:V16HI 0 \"register_operand\" \"=Yw\")\n \t(vec_select:V16HI\n \t  (vec_concat:V32HI\n-\t    (match_operand:V16HI 1 \"register_operand\" \"v\")\n-\t    (match_operand:V16HI 2 \"nonimmediate_operand\" \"vm\"))\n+\t    (match_operand:V16HI 1 \"register_operand\" \"Yw\")\n+\t    (match_operand:V16HI 2 \"nonimmediate_operand\" \"Ywm\"))\n \t  (parallel [(const_int 0) (const_int 16)\n \t\t     (const_int 1) (const_int 17)\n \t\t     (const_int 2) (const_int 18)\n@@ -14250,11 +14244,11 @@\n    (set_attr \"mode\" \"OI\")])\n \n (define_insn \"vec_interleave_lowv8hi<mask_name>\"\n-  [(set (match_operand:V8HI 0 \"register_operand\" \"=x,v\")\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=x,Yw\")\n \t(vec_select:V8HI\n \t  (vec_concat:V16HI\n-\t    (match_operand:V8HI 1 \"register_operand\" \"0,v\")\n-\t    (match_operand:V8HI 2 \"vector_operand\" \"xBm,vm\"))\n+\t    (match_operand:V8HI 1 \"register_operand\" \"0,Yw\")\n+\t    (match_operand:V8HI 2 \"vector_operand\" \"xBm,Ywm\"))\n \t  (parallel [(const_int 0) (const_int 8)\n \t\t     (const_int 1) (const_int 9)\n \t\t     (const_int 2) (const_int 10)\n@@ -15190,9 +15184,9 @@\n })\n \n (define_insn \"avx2_pshuflw_1<mask_name>\"\n-  [(set (match_operand:V16HI 0 \"register_operand\" \"=v\")\n+  [(set (match_operand:V16HI 0 \"register_operand\" \"=Yw\")\n \t(vec_select:V16HI\n-\t  (match_operand:V16HI 1 \"nonimmediate_operand\" \"vm\")\n+\t  (match_operand:V16HI 1 \"nonimmediate_operand\" \"Ywm\")\n \t  (parallel [(match_operand 2 \"const_0_to_3_operand\")\n \t\t     (match_operand 3 \"const_0_to_3_operand\")\n \t\t     (match_operand 4 \"const_0_to_3_operand\")\n@@ -15264,9 +15258,9 @@\n })\n \n (define_insn \"sse2_pshuflw_1<mask_name>\"\n-  [(set (match_operand:V8HI 0 \"register_operand\" \"=v\")\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=Yw\")\n \t(vec_select:V8HI\n-\t  (match_operand:V8HI 1 \"vector_operand\" \"vBm\")\n+\t  (match_operand:V8HI 1 \"vector_operand\" \"YwBm\")\n \t  (parallel [(match_operand 2 \"const_0_to_3_operand\")\n \t\t     (match_operand 3 \"const_0_to_3_operand\")\n \t\t     (match_operand 4 \"const_0_to_3_operand\")\n@@ -15347,9 +15341,9 @@\n })\n \n (define_insn \"avx2_pshufhw_1<mask_name>\"\n-  [(set (match_operand:V16HI 0 \"register_operand\" \"=v\")\n+  [(set (match_operand:V16HI 0 \"register_operand\" \"=Yw\")\n \t(vec_select:V16HI\n-\t  (match_operand:V16HI 1 \"nonimmediate_operand\" \"vm\")\n+\t  (match_operand:V16HI 1 \"nonimmediate_operand\" \"Ywm\")\n \t  (parallel [(const_int 0)\n \t\t     (const_int 1)\n \t\t     (const_int 2)\n@@ -15421,9 +15415,9 @@\n })\n \n (define_insn \"sse2_pshufhw_1<mask_name>\"\n-  [(set (match_operand:V8HI 0 \"register_operand\" \"=v\")\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=Yw\")\n \t(vec_select:V8HI\n-\t  (match_operand:V8HI 1 \"vector_operand\" \"vBm\")\n+\t  (match_operand:V8HI 1 \"vector_operand\" \"YwBm\")\n \t  (parallel [(const_int 0)\n \t\t     (const_int 1)\n \t\t     (const_int 2)\n@@ -16213,15 +16207,15 @@\n })\n \n (define_insn \"*<sse2_avx2>_uavg<mode>3<mask_name>\"\n-  [(set (match_operand:VI12_AVX2_AVX512BW 0 \"register_operand\" \"=x,v\")\n+  [(set (match_operand:VI12_AVX2_AVX512BW 0 \"register_operand\" \"=x,<v_Yw>\")\n \t(truncate:VI12_AVX2_AVX512BW\n \t  (lshiftrt:<ssedoublemode>\n \t    (plus:<ssedoublemode>\n \t      (plus:<ssedoublemode>\n \t\t(zero_extend:<ssedoublemode>\n-\t\t  (match_operand:VI12_AVX2_AVX512BW 1 \"vector_operand\" \"%0,v\"))\n+\t\t  (match_operand:VI12_AVX2_AVX512BW 1 \"vector_operand\" \"%0,<v_Yw>\"))\n \t\t(zero_extend:<ssedoublemode>\n-\t\t  (match_operand:VI12_AVX2_AVX512BW 2 \"vector_operand\" \"xBm,vm\")))\n+\t\t  (match_operand:VI12_AVX2_AVX512BW 2 \"vector_operand\" \"xBm,<v_Yw>m\")))\n \t      (match_operand:<ssedoublemode> <mask_expand_op3> \"const1_operand\"))\n \t    (const_int 1))))]\n   \"TARGET_SSE2 && <mask_mode512bit_condition> && <mask_avx512bw_condition>\n@@ -16238,10 +16232,10 @@\n ;; The correct representation for this is absolutely enormous, and\n ;; surely not generally useful.\n (define_insn \"<sse2_avx2>_psadbw\"\n-  [(set (match_operand:VI8_AVX2_AVX512BW 0 \"register_operand\" \"=x,v\")\n+  [(set (match_operand:VI8_AVX2_AVX512BW 0 \"register_operand\" \"=x,YW\")\n \t(unspec:VI8_AVX2_AVX512BW\n-\t  [(match_operand:<ssebytemode> 1 \"register_operand\" \"0,v\")\n-\t   (match_operand:<ssebytemode> 2 \"vector_operand\" \"xBm,vm\")]\n+\t  [(match_operand:<ssebytemode> 1 \"register_operand\" \"0,YW\")\n+\t   (match_operand:<ssebytemode> 2 \"vector_operand\" \"xBm,YWm\")]\n \t  UNSPEC_PSADBW))]\n   \"TARGET_SSE2\"\n   \"@\n@@ -16815,12 +16809,12 @@\n    (set_attr \"mode\" \"DI,TI,TI\")])\n \n (define_insn \"avx2_pmaddubsw256\"\n-  [(set (match_operand:V16HI 0 \"register_operand\" \"=x,v\")\n+  [(set (match_operand:V16HI 0 \"register_operand\" \"=Yw\")\n \t(ss_plus:V16HI\n \t  (mult:V16HI\n \t    (zero_extend:V16HI\n \t      (vec_select:V16QI\n-\t\t(match_operand:V32QI 1 \"register_operand\" \"x,v\")\n+\t\t(match_operand:V32QI 1 \"register_operand\" \"Yw\")\n \t\t(parallel [(const_int 0) (const_int 2)\n \t\t\t   (const_int 4) (const_int 6)\n \t\t\t   (const_int 8) (const_int 10)\n@@ -16831,7 +16825,7 @@\n \t\t\t   (const_int 28) (const_int 30)])))\n \t    (sign_extend:V16HI\n \t      (vec_select:V16QI\n-\t\t(match_operand:V32QI 2 \"nonimmediate_operand\" \"xm,vm\")\n+\t\t(match_operand:V32QI 2 \"nonimmediate_operand\" \"Ywm\")\n \t\t(parallel [(const_int 0) (const_int 2)\n \t\t\t   (const_int 4) (const_int 6)\n \t\t\t   (const_int 8) (const_int 10)\n@@ -16863,10 +16857,9 @@\n \t\t\t   (const_int 29) (const_int 31)]))))))]\n   \"TARGET_AVX2\"\n   \"vpmaddubsw\\t{%2, %1, %0|%0, %1, %2}\"\n-  [(set_attr \"isa\" \"*,avx512bw\")\n-   (set_attr \"type\" \"sseiadd\")\n+  [(set_attr \"type\" \"sseiadd\")\n    (set_attr \"prefix_extra\" \"1\")\n-   (set_attr \"prefix\" \"vex,evex\")\n+   (set_attr \"prefix\" \"vex\")\n    (set_attr \"mode\" \"OI\")])\n \n ;; The correct representation for this is absolutely enormous, and\n@@ -16919,19 +16912,19 @@\n    (set_attr \"mode\" \"XI\")])\n \n (define_insn \"ssse3_pmaddubsw128\"\n-  [(set (match_operand:V8HI 0 \"register_operand\" \"=x,x,v\")\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=x,Yw\")\n \t(ss_plus:V8HI\n \t  (mult:V8HI\n \t    (zero_extend:V8HI\n \t      (vec_select:V8QI\n-\t\t(match_operand:V16QI 1 \"register_operand\" \"0,x,v\")\n+\t\t(match_operand:V16QI 1 \"register_operand\" \"0,Yw\")\n \t\t(parallel [(const_int 0) (const_int 2)\n \t\t\t   (const_int 4) (const_int 6)\n \t\t\t   (const_int 8) (const_int 10)\n \t\t\t   (const_int 12) (const_int 14)])))\n \t    (sign_extend:V8HI\n \t      (vec_select:V8QI\n-\t\t(match_operand:V16QI 2 \"vector_operand\" \"xBm,xm,vm\")\n+\t\t(match_operand:V16QI 2 \"vector_operand\" \"xBm,Ywm\")\n \t\t(parallel [(const_int 0) (const_int 2)\n \t\t\t   (const_int 4) (const_int 6)\n \t\t\t   (const_int 8) (const_int 10)\n@@ -16952,14 +16945,13 @@\n   \"TARGET_SSSE3\"\n   \"@\n    pmaddubsw\\t{%2, %0|%0, %2}\n-   vpmaddubsw\\t{%2, %1, %0|%0, %1, %2}\n    vpmaddubsw\\t{%2, %1, %0|%0, %1, %2}\"\n-  [(set_attr \"isa\" \"noavx,avx,avx512bw\")\n+  [(set_attr \"isa\" \"noavx,avx\")\n    (set_attr \"type\" \"sseiadd\")\n    (set_attr \"atom_unit\" \"simul\")\n-   (set_attr \"prefix_data16\" \"1,*,*\")\n+   (set_attr \"prefix_data16\" \"1,*\")\n    (set_attr \"prefix_extra\" \"1\")\n-   (set_attr \"prefix\" \"orig,vex,evex\")\n+   (set_attr \"prefix\" \"orig,vex\")\n    (set_attr \"mode\" \"TI\")])\n \n (define_insn \"ssse3_pmaddubsw\"\n@@ -17065,30 +17057,29 @@\n })\n \n (define_insn \"*<ssse3_avx2>_pmulhrsw<mode>3<mask_name>\"\n-  [(set (match_operand:VI2_AVX2 0 \"register_operand\" \"=x,x,v\")\n+  [(set (match_operand:VI2_AVX2 0 \"register_operand\" \"=x,<v_Yw>\")\n \t(truncate:VI2_AVX2\n \t  (lshiftrt:<ssedoublemode>\n \t    (plus:<ssedoublemode>\n \t      (lshiftrt:<ssedoublemode>\n \t\t(mult:<ssedoublemode>\n \t\t  (sign_extend:<ssedoublemode>\n-\t\t    (match_operand:VI2_AVX2 1 \"vector_operand\" \"%0,x,v\"))\n+\t\t    (match_operand:VI2_AVX2 1 \"vector_operand\" \"%0,<v_Yw>\"))\n \t\t  (sign_extend:<ssedoublemode>\n-\t\t    (match_operand:VI2_AVX2 2 \"vector_operand\" \"xBm,xm,vm\")))\n+\t\t    (match_operand:VI2_AVX2 2 \"vector_operand\" \"xBm,<v_Yw>m\")))\n \t\t(const_int 14))\n \t      (match_operand:VI2_AVX2 3 \"const1_operand\"))\n \t    (const_int 1))))]\n   \"TARGET_SSSE3 && <mask_mode512bit_condition> && <mask_avx512bw_condition>\n    && !(MEM_P (operands[1]) && MEM_P (operands[2]))\"\n   \"@\n    pmulhrsw\\t{%2, %0|%0, %2}\n-   vpmulhrsw\\t{%2, %1, %0<mask_operand4>|%0<mask_operand4>, %1, %2}\n    vpmulhrsw\\t{%2, %1, %0<mask_operand4>|%0<mask_operand4>, %1, %2}\"\n-  [(set_attr \"isa\" \"noavx,avx,avx512bw\")\n+  [(set_attr \"isa\" \"noavx,avx\")\n    (set_attr \"type\" \"sseimul\")\n-   (set_attr \"prefix_data16\" \"1,*,*\")\n+   (set_attr \"prefix_data16\" \"1,*\")\n    (set_attr \"prefix_extra\" \"1\")\n-   (set_attr \"prefix\" \"orig,maybe_evex,evex\")\n+   (set_attr \"prefix\" \"orig,maybe_evex\")\n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n (define_expand \"smulhrsv4hi3\"\n@@ -17160,21 +17151,20 @@\n    (set_attr \"mode\" \"DI,TI,TI\")])\n \n (define_insn \"<ssse3_avx2>_pshufb<mode>3<mask_name>\"\n-  [(set (match_operand:VI1_AVX512 0 \"register_operand\" \"=x,x,v\")\n+  [(set (match_operand:VI1_AVX512 0 \"register_operand\" \"=x,<v_Yw>\")\n \t(unspec:VI1_AVX512\n-\t  [(match_operand:VI1_AVX512 1 \"register_operand\" \"0,x,v\")\n-\t   (match_operand:VI1_AVX512 2 \"vector_operand\" \"xBm,xm,vm\")]\n+\t  [(match_operand:VI1_AVX512 1 \"register_operand\" \"0,<v_Yw>\")\n+\t   (match_operand:VI1_AVX512 2 \"vector_operand\" \"xBm,<v_Yw>m\")]\n \t  UNSPEC_PSHUFB))]\n   \"TARGET_SSSE3 && <mask_mode512bit_condition> && <mask_avx512bw_condition>\"\n   \"@\n    pshufb\\t{%2, %0|%0, %2}\n-   vpshufb\\t{%2, %1, %0<mask_operand3>|%0<mask_operand3>, %1, %2}\n    vpshufb\\t{%2, %1, %0<mask_operand3>|%0<mask_operand3>, %1, %2}\"\n-  [(set_attr \"isa\" \"noavx,avx,avx512bw\")\n+  [(set_attr \"isa\" \"noavx,avx\")\n    (set_attr \"type\" \"sselog1\")\n-   (set_attr \"prefix_data16\" \"1,*,*\")\n+   (set_attr \"prefix_data16\" \"1,*\")\n    (set_attr \"prefix_extra\" \"1\")\n-   (set_attr \"prefix\" \"orig,maybe_evex,evex\")\n+   (set_attr \"prefix\" \"orig,maybe_evex\")\n    (set_attr \"btver2_decode\" \"vector\")\n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n@@ -17274,11 +17264,11 @@\n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n (define_insn \"<ssse3_avx2>_palignr<mode>\"\n-  [(set (match_operand:SSESCALARMODE 0 \"register_operand\" \"=x,x,v\")\n+  [(set (match_operand:SSESCALARMODE 0 \"register_operand\" \"=x,<v_Yw>\")\n \t(unspec:SSESCALARMODE\n-\t  [(match_operand:SSESCALARMODE 1 \"register_operand\" \"0,x,v\")\n-\t   (match_operand:SSESCALARMODE 2 \"vector_operand\" \"xBm,xm,vm\")\n-\t   (match_operand:SI 3 \"const_0_to_255_mul_8_operand\" \"n,n,n\")]\n+\t  [(match_operand:SSESCALARMODE 1 \"register_operand\" \"0,<v_Yw>\")\n+\t   (match_operand:SSESCALARMODE 2 \"vector_operand\" \"xBm,<v_Yw>m\")\n+\t   (match_operand:SI 3 \"const_0_to_255_mul_8_operand\" \"n,n\")]\n \t  UNSPEC_PALIGNR))]\n   \"TARGET_SSSE3\"\n {\n@@ -17289,19 +17279,18 @@\n     case 0:\n       return \"palignr\\t{%3, %2, %0|%0, %2, %3}\";\n     case 1:\n-    case 2:\n       return \"vpalignr\\t{%3, %2, %1, %0|%0, %1, %2, %3}\";\n     default:\n       gcc_unreachable ();\n     }\n }\n-  [(set_attr \"isa\" \"noavx,avx,avx512bw\")\n+  [(set_attr \"isa\" \"noavx,avx\")\n    (set_attr \"type\" \"sseishft\")\n    (set_attr \"atom_unit\" \"sishuf\")\n-   (set_attr \"prefix_data16\" \"1,*,*\")\n+   (set_attr \"prefix_data16\" \"1,*\")\n    (set_attr \"prefix_extra\" \"1\")\n    (set_attr \"length_immediate\" \"1\")\n-   (set_attr \"prefix\" \"orig,vex,evex\")\n+   (set_attr \"prefix\" \"orig,vex\")\n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n (define_insn_and_split \"ssse3_palignrdi\"\n@@ -17367,9 +17356,9 @@\n    (V8DI \"TARGET_AVX512F\") (V4DI \"TARGET_AVX512VL\") (V2DI \"TARGET_AVX512VL\")])\n \n (define_insn \"*abs<mode>2\"\n-  [(set (match_operand:VI1248_AVX512VL_AVX512BW 0 \"register_operand\" \"=v\")\n+  [(set (match_operand:VI1248_AVX512VL_AVX512BW 0 \"register_operand\" \"=<v_Yw>\")\n \t(abs:VI1248_AVX512VL_AVX512BW\n-\t  (match_operand:VI1248_AVX512VL_AVX512BW 1 \"vector_operand\" \"vBm\")))]\n+\t  (match_operand:VI1248_AVX512VL_AVX512BW 1 \"vector_operand\" \"<v_Yw>Bm\")))]\n   \"TARGET_SSSE3\"\n   \"%vpabs<ssemodesuffix>\\t{%1, %0|%0, %1}\"\n   [(set_attr \"type\" \"sselog1\")\n@@ -17731,22 +17720,21 @@\n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n (define_insn \"<sse4_1_avx2>_packusdw<mask_name>\"\n-  [(set (match_operand:VI2_AVX2 0 \"register_operand\" \"=Yr,*x,x,v\")\n+  [(set (match_operand:VI2_AVX2 0 \"register_operand\" \"=Yr,*x,<v_Yw>\")\n \t(vec_concat:VI2_AVX2\n \t  (us_truncate:<ssehalfvecmode>\n-\t    (match_operand:<sseunpackmode> 1 \"register_operand\" \"0,0,x,v\"))\n+\t    (match_operand:<sseunpackmode> 1 \"register_operand\" \"0,0,<v_Yw>\"))\n \t  (us_truncate:<ssehalfvecmode>\n-\t    (match_operand:<sseunpackmode> 2 \"vector_operand\" \"YrBm,*xBm,xm,vm\"))))]\n+\t    (match_operand:<sseunpackmode> 2 \"vector_operand\" \"YrBm,*xBm,<v_Yw>m\"))))]\n   \"TARGET_SSE4_1 && <mask_mode512bit_condition> && <mask_avx512bw_condition>\"\n   \"@\n    packusdw\\t{%2, %0|%0, %2}\n    packusdw\\t{%2, %0|%0, %2}\n-   vpackusdw\\t{%2, %1, %0<mask_operand3>|%0<mask_operand3>, %1, %2}\n    vpackusdw\\t{%2, %1, %0<mask_operand3>|%0<mask_operand3>, %1, %2}\"\n-  [(set_attr \"isa\" \"noavx,noavx,avx,avx512bw\")\n+  [(set_attr \"isa\" \"noavx,noavx,avx\")\n    (set_attr \"type\" \"sselog\")\n    (set_attr \"prefix_extra\" \"1\")\n-   (set_attr \"prefix\" \"orig,orig,<mask_prefix>,evex\")\n+   (set_attr \"prefix\" \"orig,orig,<mask_prefix>\")\n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n (define_insn \"<sse4_1_avx2>_pblendvb\"\n@@ -17867,9 +17855,9 @@\n    (set_attr \"mode\" \"TI\")])\n \n (define_insn \"avx2_<code>v16qiv16hi2<mask_name>\"\n-  [(set (match_operand:V16HI 0 \"register_operand\" \"=v\")\n+  [(set (match_operand:V16HI 0 \"register_operand\" \"=Yw\")\n \t(any_extend:V16HI\n-\t  (match_operand:V16QI 1 \"nonimmediate_operand\" \"vm\")))]\n+\t  (match_operand:V16QI 1 \"nonimmediate_operand\" \"Ywm\")))]\n   \"TARGET_AVX2 && <mask_avx512bw_condition> && <mask_avx512vl_condition>\"\n   \"vpmov<extsuffix>bw\\t{%1, %0<mask_operand2>|%0<mask_operand2>, %1}\"\n   [(set_attr \"type\" \"ssemov\")\n@@ -17935,10 +17923,10 @@\n   \"TARGET_AVX512BW\")\n \n (define_insn \"sse4_1_<code>v8qiv8hi2<mask_name>\"\n-  [(set (match_operand:V8HI 0 \"register_operand\" \"=Yr,*x,v\")\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=Yr,*x,Yw\")\n \t(any_extend:V8HI\n \t  (vec_select:V8QI\n-\t    (match_operand:V16QI 1 \"register_operand\" \"Yr,*x,v\")\n+\t    (match_operand:V16QI 1 \"register_operand\" \"Yr,*x,Yw\")\n \t    (parallel [(const_int 0) (const_int 1)\n \t\t       (const_int 2) (const_int 3)\n \t\t       (const_int 4) (const_int 5)\n@@ -17952,7 +17940,7 @@\n    (set_attr \"mode\" \"TI\")])\n \n (define_insn \"*sse4_1_<code>v8qiv8hi2<mask_name>_1\"\n-  [(set (match_operand:V8HI 0 \"register_operand\" \"=Yr,*x,v\")\n+  [(set (match_operand:V8HI 0 \"register_operand\" \"=Yr,*x,Yw\")\n \t(any_extend:V8HI\n \t  (match_operand:V8QI 1 \"memory_operand\" \"m,m,m\")))]\n   \"TARGET_SSE4_1 && <mask_avx512bw_condition> && <mask_avx512vl_condition>\""}, {"sha": "8bb3a03f5cbe61917fdeebf0edff2d919ad6858d", "filename": "gcc/testsuite/gcc.target/i386/avx512vl-pr99321-2.c", "status": "added", "additions": 94, "deletions": 0, "changes": 94, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx512vl-pr99321-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx512vl-pr99321-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Favx512vl-pr99321-2.c?ref=3bb345c9313ad8f6a6c24abd7d5eaa11413bbe22", "patch": "@@ -0,0 +1,94 @@\n+/* PR target/99321 */\n+/* { dg-do assemble { target lp64 } } */\n+/* { dg-require-effective-target avx512vl } */\n+/* { dg-require-effective-target assembler_march_noavx512bw } */\n+/* { dg-options \"-O2 -mavx512vl -mno-avx512bw -Wa,-march=+noavx512bw\" } */\n+\n+#include <x86intrin.h>\n+\n+typedef unsigned char V1 __attribute__((vector_size (16)));\n+typedef unsigned char V2 __attribute__((vector_size (32)));\n+typedef unsigned short V3 __attribute__((vector_size (16)));\n+typedef unsigned short V4 __attribute__((vector_size (32)));\n+\n+void f1 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_abs_epi8 ((__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f2 (void) { register V2 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_abs_epi8 ((__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f3 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_abs_epi16 ((__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f4 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_abs_epi16 ((__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f5 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_adds_epi8 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f6 (void) { register V2 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_adds_epi8 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f7 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_adds_epi16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f8 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_adds_epi16 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f9 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_subs_epi8 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f10 (void) { register V2 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_subs_epi8 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f11 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_subs_epi16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f12 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_subs_epi16 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f13 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_adds_epu8 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f14 (void) { register V2 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_adds_epu8 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f15 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_adds_epu16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f16 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_adds_epu16 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f17 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_subs_epu8 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f18 (void) { register V2 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_subs_epu8 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f19 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_subs_epu16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f20 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_subs_epu16 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f21 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_alignr_epi8 ((__m128i) a, (__m128i) b, 5); __asm (\"\" : : \"v\" (a)); }\n+void f22 (void) { register V2 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_alignr_epi8 ((__m256i) a, (__m256i) b, 5); __asm (\"\" : : \"v\" (a)); }\n+void f23 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_adds_epu16 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f24 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_avg_epu8 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f25 (void) { register V2 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_avg_epu8 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f26 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_avg_epu16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f27 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_avg_epu16 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f28 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_broadcastb_epi8 ((__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f29 (void) { register V2 a __asm (\"%xmm16\"); register V1 b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_broadcastb_epi8 ((__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f30 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_broadcastw_epi16 ((__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f31 (void) { register V4 a __asm (\"%xmm16\"); register V3 b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_broadcastw_epi16 ((__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+int f32 (void) { register V1 a __asm (\"%xmm16\"); __asm (\"\" : \"=v\" (a)); return _mm_extract_epi8 ((__m128i) a, 3); }\n+int f33 (void) { register V3 a __asm (\"%xmm16\"); __asm (\"\" : \"=v\" (a)); return _mm_extract_epi16 ((__m128i) a, 3); }\n+void f34 (int c) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_insert_epi8 ((__m128i) b, c, 5); __asm (\"\" : : \"v\" (a)); }\n+void f35 (int c) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_insert_epi16 ((__m128i) b, c, 5); __asm (\"\" : : \"v\" (a)); }\n+void f36 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_maddubs_epi16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f37 (void) { register V2 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_maddubs_epi16 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f38 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_madd_epi16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f39 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_madd_epi16 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f40 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_cvtepi8_epi16 ((__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f41 (void) { register V4 a __asm (\"%xmm16\"); register V3 b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_cvtepi8_epi16 ((__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f42 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_cvtepu8_epi16 ((__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f43 (void) { register V4 a __asm (\"%xmm16\"); register V3 b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_cvtepu8_epi16 ((__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f44 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_mulhrs_epi16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f45 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_mulhrs_epi16 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f46 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_mulhi_epu16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f47 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_mulhi_epu16 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f48 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_mulhi_epi16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f49 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_mulhi_epi16 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f50 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_sad_epu8 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f51 (void) { register V2 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_sad_epu8 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f52 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_shuffle_epi8 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f53 (void) { register V2 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_shuffle_epi8 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f54 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_shufflehi_epi16 ((__m128i) b, 0x5b); __asm (\"\" : : \"v\" (a)); }\n+void f55 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_shufflehi_epi16 ((__m256i) b, 0x5b); __asm (\"\" : : \"v\" (a)); }\n+void f56 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_shufflelo_epi16 ((__m128i) b, 0x5b); __asm (\"\" : : \"v\" (a)); }\n+void f57 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_shufflelo_epi16 ((__m256i) b, 0x5b); __asm (\"\" : : \"v\" (a)); }\n+void f58 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_slli_si128 ((__m128i) b, 3); __asm (\"\" : : \"v\" (a)); }\n+void f59 (void) { register V2 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_slli_si256 ((__m256i) b, 3); __asm (\"\" : : \"v\" (a)); }\n+void f60 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_srli_si128 ((__m128i) b, 3); __asm (\"\" : : \"v\" (a)); }\n+void f61 (void) { register V2 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_srli_si256 ((__m256i) b, 3); __asm (\"\" : : \"v\" (a)); }\n+void f62 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_sll_epi16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f63 (void) { register V4 a __asm (\"%xmm16\"); register V3 b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_sll_epi16 ((__m256i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f64 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_slli_epi16 ((__m128i) b, 7); __asm (\"\" : : \"v\" (a)); }\n+void f65 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_slli_epi16 ((__m256i) b, 7); __asm (\"\" : : \"v\" (a)); }\n+void f66 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_srl_epi16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f67 (void) { register V4 a __asm (\"%xmm16\"); register V3 b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_srl_epi16 ((__m256i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f68 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_srli_epi16 ((__m128i) b, 7); __asm (\"\" : : \"v\" (a)); }\n+void f69 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_srli_epi16 ((__m256i) b, 7); __asm (\"\" : : \"v\" (a)); }\n+void f70 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_sra_epi16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f71 (void) { register V4 a __asm (\"%xmm16\"); register V3 b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_sra_epi16 ((__m256i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f72 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_srai_epi16 ((__m128i) b, 7); __asm (\"\" : : \"v\" (a)); }\n+void f73 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_srai_epi16 ((__m256i) b, 7); __asm (\"\" : : \"v\" (a)); }\n+void f74 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_unpackhi_epi8 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f75 (void) { register V2 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_unpackhi_epi8 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f76 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_unpackhi_epi16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f77 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_unpackhi_epi16 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f78 (void) { register V1 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V1) _mm_unpacklo_epi8 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f79 (void) { register V2 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V2) _mm256_unpacklo_epi8 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }\n+void f80 (void) { register V3 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V3) _mm_unpacklo_epi16 ((__m128i) a, (__m128i) b); __asm (\"\" : : \"v\" (a)); }\n+void f81 (void) { register V4 a __asm (\"%xmm16\"), b __asm (\"%xmm17\"); __asm (\"\" : \"=v\" (a), \"=v\" (b)); a = (V4) _mm256_unpacklo_epi16 ((__m256i) a, (__m256i) b); __asm (\"\" : : \"v\" (a)); }"}]}