{"sha": "b28491dc2d79763ecbff4f0b9f1f3e48a443be1d", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YjI4NDkxZGMyZDc5NzYzZWNiZmY0ZjBiOWYxZjNlNDhhNDQzYmUxZA==", "commit": {"author": {"name": "David Malcolm", "email": "dmalcolm@redhat.com", "date": "2020-08-18T22:52:17Z"}, "committer": {"name": "David Malcolm", "email": "dmalcolm@redhat.com", "date": "2020-09-16T23:01:58Z"}, "message": "analyzer: bulk merger/processing of runs of nodes at CFG join points\n\nPrior to this patch the analyzer worklist considered only one node or\ntwo nodes at a time, processing and/or merging state individually or\npairwise.\n\nThis could lead to explosions of merger nodes at CFG join points,\nespecially after switch statements, which could have large numbers\nof in-edges, and thus large numbers of merger exploded_nodes could\nbe created, exceeding the per-point limit and thus stopping analysis\nwith -Wanalyzer-too-complex.\n\nThis patch special-cases the handling for runs of consecutive\nnodes in the worklist at a CFG join point, processing and merging\nthem all together.\n\nThe patch fixes a state explosion seen in bzip2.c seen when attempting\nto reproduce PR analyzer/95188, in a switch statement in a loop for\nargument parsing.  With this patch, the analyzer successfully\nconsolidates the state after the argument parsing to a single exploded\nnode.\n\nIn gcc.dg/analyzer/pr96653.c there is a switch statement with over 300\ncases which leads to hitting the per-point limit.  With this patch\nthe consolidation code doesn't manage to merge all of them due to other\nworklist-ordering bugs, and it still hits the per-point limits, but it\ndoes manage some very long consolidations:\n  merged 2 in-enodes into 2 out-enode(s) at SN: 403\n  merged 2 in-enodes into 2 out-enode(s) at SN: 403\n  merged 2 in-enodes into 1 out-enode(s) at SN: 11\n  merged 29 in-enodes into 1 out-enode(s) at SN: 35\n  merged 6 in-enodes into 1 out-enode(s) at SN: 41\n  merged 31 in-enodes into 1 out-enode(s) at SN: 35\nand with a followup patch to fix an SCC issue it manages:\n  merged 358 in-enodes into 2 out-enode(s) at SN: 402\n\nThe patch appears to fix the failure on non-x86_64 of:\n  gcc.dg/analyzer/pr93032-mztools.c (test for excess errors)\nwhich is PR analyzer/96616.\n\nUnfortunately, the patch introduces a memory leak false positive in\ngcc.dg/analyzer/pr94851-1.c, but this appears to be a pre-existing bug\nthat was hidden by state-merging failures.\n\ngcc/analyzer/ChangeLog:\n\t* engine.cc (exploded_node::dump_dot): Show STATUS_BULK_MERGED.\n\t(exploded_graph::process_worklist): Call\n\tmaybe_process_run_of_before_supernode_enodes.\n\t(exploded_graph::maybe_process_run_of_before_supernode_enodes):\n\tNew.\n\t(exploded_graph_annotator::print_enode): Show STATUS_BULK_MERGED.\n\t* exploded-graph.h (enum exploded_node::status): Add\n\tSTATUS_BULK_MERGED.\n\ngcc/testsuite/ChangeLog:\n\t* gcc.dg/analyzer/bzip2-arg-parse-1.c: New test.\n\t* gcc.dg/analyzer/loop-n-down-to-1-by-1.c: Remove xfail.\n\t* gcc.dg/analyzer/pr94851-1.c: Add xfail.", "tree": {"sha": "e65d9b4badac7ef5e1bfffd03dc6510c6594134b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/e65d9b4badac7ef5e1bfffd03dc6510c6594134b"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/b28491dc2d79763ecbff4f0b9f1f3e48a443be1d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b28491dc2d79763ecbff4f0b9f1f3e48a443be1d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b28491dc2d79763ecbff4f0b9f1f3e48a443be1d", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b28491dc2d79763ecbff4f0b9f1f3e48a443be1d/comments", "author": {"login": "davidmalcolm", "id": 1553248, "node_id": "MDQ6VXNlcjE1NTMyNDg=", "avatar_url": "https://avatars.githubusercontent.com/u/1553248?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidmalcolm", "html_url": "https://github.com/davidmalcolm", "followers_url": "https://api.github.com/users/davidmalcolm/followers", "following_url": "https://api.github.com/users/davidmalcolm/following{/other_user}", "gists_url": "https://api.github.com/users/davidmalcolm/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidmalcolm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidmalcolm/subscriptions", "organizations_url": "https://api.github.com/users/davidmalcolm/orgs", "repos_url": "https://api.github.com/users/davidmalcolm/repos", "events_url": "https://api.github.com/users/davidmalcolm/events{/privacy}", "received_events_url": "https://api.github.com/users/davidmalcolm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "davidmalcolm", "id": 1553248, "node_id": "MDQ6VXNlcjE1NTMyNDg=", "avatar_url": "https://avatars.githubusercontent.com/u/1553248?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidmalcolm", "html_url": "https://github.com/davidmalcolm", "followers_url": "https://api.github.com/users/davidmalcolm/followers", "following_url": "https://api.github.com/users/davidmalcolm/following{/other_user}", "gists_url": "https://api.github.com/users/davidmalcolm/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidmalcolm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidmalcolm/subscriptions", "organizations_url": "https://api.github.com/users/davidmalcolm/orgs", "repos_url": "https://api.github.com/users/davidmalcolm/repos", "events_url": "https://api.github.com/users/davidmalcolm/events{/privacy}", "received_events_url": "https://api.github.com/users/davidmalcolm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "b9b5fc0c2175b34131d9fd0805b1b307f754f4f0", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b9b5fc0c2175b34131d9fd0805b1b307f754f4f0", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b9b5fc0c2175b34131d9fd0805b1b307f754f4f0"}], "stats": {"total": 313, "additions": 310, "deletions": 3}, "files": [{"sha": "53fafb58633cd8380247b6129a4b0137c883b99b", "filename": "gcc/analyzer/engine.cc", "status": "modified", "additions": 207, "deletions": 0, "changes": 207, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b28491dc2d79763ecbff4f0b9f1f3e48a443be1d/gcc%2Fanalyzer%2Fengine.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b28491dc2d79763ecbff4f0b9f1f3e48a443be1d/gcc%2Fanalyzer%2Fengine.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fanalyzer%2Fengine.cc?ref=b28491dc2d79763ecbff4f0b9f1f3e48a443be1d", "patch": "@@ -848,6 +848,8 @@ exploded_node::dump_dot (graphviz_out *gv, const dump_args_t &args) const\n   pp_printf (pp, \"EN: %i\", m_index);\n   if (m_status == STATUS_MERGER)\n     pp_string (pp, \" (merger)\");\n+  else if (m_status == STATUS_BULK_MERGED)\n+    pp_string (pp, \" (bulk merged)\");\n   pp_newline (pp);\n \n   if (args.show_enode_details_p (*this))\n@@ -2211,6 +2213,12 @@ exploded_graph::process_worklist ()\n       if (logger)\n \tlogger->log (\"next to process: EN: %i\", node->m_index);\n \n+      /* If we have a run of nodes that are before-supernode, try merging and\n+\t processing them together, rather than pairwise or individually.  */\n+      if (flag_analyzer_state_merge && node != m_origin)\n+\tif (maybe_process_run_of_before_supernode_enodes (node))\n+\t  goto handle_limit;\n+\n       /* Avoid exponential explosions of nodes by attempting to merge\n \t nodes that are at the same program point and which have\n \t sufficiently similar state.  */\n@@ -2340,6 +2348,7 @@ exploded_graph::process_worklist ()\n \n       process_node (node);\n \n+    handle_limit:\n       /* Impose a hard limit on the number of exploded nodes, to ensure\n \t that the analysis terminates in the face of pathological state\n \t explosion (or bugs).\n@@ -2367,6 +2376,201 @@ exploded_graph::process_worklist ()\n     }\n }\n \n+/* Attempt to process a consecutive run of sufficiently-similar nodes in\n+   the worklist at a CFG join-point (having already popped ENODE from the\n+   head of the worklist).\n+\n+   If ENODE's point is of the form (before-supernode, SNODE) and the next\n+   nodes in the worklist are a consecutive run of enodes of the same form,\n+   for the same supernode as ENODE (but potentially from different in-edges),\n+   process them all together, setting their status to STATUS_BULK_MERGED,\n+   and return true.\n+   Otherwise, return false, in which case ENODE must be processed in the\n+   normal way.\n+\n+   When processing them all together, generate successor states based\n+   on phi nodes for the appropriate CFG edges, and then attempt to merge\n+   these states into a minimal set of merged successor states, partitioning\n+   the inputs by merged successor state.\n+\n+   Create new exploded nodes for all of the merged states, and add edges\n+   connecting the input enodes to the corresponding merger exploded nodes.\n+\n+   We hope we have a much smaller number of merged successor states\n+   compared to the number of input enodes - ideally just one,\n+   if all successor states can be merged.\n+\n+   Processing and merging many together as one operation rather than as\n+   pairs avoids scaling issues where per-pair mergers could bloat the\n+   graph with merger nodes (especially so after switch statements).  */\n+\n+bool\n+exploded_graph::\n+maybe_process_run_of_before_supernode_enodes (exploded_node *enode)\n+{\n+  /* A struct for tracking per-input state.  */\n+  struct item\n+  {\n+    item (exploded_node *input_enode)\n+    : m_input_enode (input_enode),\n+      m_processed_state (input_enode->get_state ()),\n+      m_merger_idx (-1)\n+    {}\n+\n+    exploded_node *m_input_enode;\n+    program_state m_processed_state;\n+    int m_merger_idx;\n+  };\n+\n+  gcc_assert (enode->get_status () == exploded_node::STATUS_WORKLIST);\n+  gcc_assert (enode->m_succs.length () == 0);\n+\n+  const program_point &point = enode->get_point ();\n+\n+  if (point.get_kind () != PK_BEFORE_SUPERNODE)\n+    return false;\n+\n+  const supernode *snode = point.get_supernode ();\n+\n+  logger * const logger = get_logger ();\n+  LOG_SCOPE (logger);\n+\n+  /* Find a run of enodes in the worklist that are before the same supernode,\n+     but potentially from different in-edges.  */\n+  auto_vec <exploded_node *> enodes;\n+  enodes.safe_push (enode);\n+  while (exploded_node *enode_2 = m_worklist.peek_next ())\n+    {\n+      gcc_assert (enode_2->get_status ()\n+\t\t  == exploded_node::STATUS_WORKLIST);\n+      gcc_assert (enode_2->m_succs.length () == 0);\n+\n+      const program_point &point_2 = enode_2->get_point ();\n+\n+      if (point_2.get_kind () == PK_BEFORE_SUPERNODE\n+\t  && point_2.get_supernode () == snode\n+\t  && point_2.get_call_string () == point.get_call_string ())\n+\t{\n+\t  enodes.safe_push (enode_2);\n+\t  m_worklist.take_next ();\n+\t}\n+      else\n+\tbreak;\n+    }\n+\n+  /* If the only node is ENODE, then give up.  */\n+  if (enodes.length () == 1)\n+    return false;\n+\n+  if (logger)\n+    logger->log (\"got run of %i enodes for SN: %i\",\n+\t\t enodes.length (), snode->m_index);\n+\n+  /* All of these enodes have a shared successor point (even if they\n+     were for different in-edges).  */\n+  program_point next_point (point.get_next ());\n+\n+  /* Calculate the successor state for each enode in enodes.  */\n+  auto_delete_vec<item> items (enodes.length ());\n+  unsigned i;\n+  exploded_node *iter_enode;\n+  FOR_EACH_VEC_ELT (enodes, i, iter_enode)\n+    {\n+      item *it = new item (iter_enode);\n+      items.quick_push (it);\n+      const program_state &state = iter_enode->get_state ();\n+      program_state *next_state = &it->m_processed_state;\n+      const program_point &iter_point = iter_enode->get_point ();\n+      if (const superedge *iter_sedge = iter_point.get_from_edge ())\n+\t{\n+\t  impl_region_model_context ctxt (*this, iter_enode,\n+\t\t\t\t\t  &state, next_state, NULL);\n+\t  const cfg_superedge *last_cfg_superedge\n+\t    = iter_sedge->dyn_cast_cfg_superedge ();\n+\t  if (last_cfg_superedge)\n+\t    next_state->m_region_model->update_for_phis\n+\t      (snode, last_cfg_superedge, &ctxt);\n+\t}\n+    }\n+\n+  /* Attempt to partition the items into a set of merged states.\n+     We hope we have a much smaller number of merged states\n+     compared to the number of input enodes - ideally just one,\n+     if all can be merged.  */\n+  auto_delete_vec <program_state> merged_states;\n+  auto_vec<item *> first_item_for_each_merged_state;\n+  item *it;\n+  FOR_EACH_VEC_ELT (items, i, it)\n+    {\n+      const program_state &it_state = it->m_processed_state;\n+      program_state *merged_state;\n+      unsigned iter_merger_idx;\n+      FOR_EACH_VEC_ELT (merged_states, iter_merger_idx, merged_state)\n+\t{\n+\t  program_state merge (m_ext_state);\n+\t  if (it_state.can_merge_with_p (*merged_state, next_point, &merge))\n+\t    {\n+\t      *merged_state = merge;\n+\t      it->m_merger_idx = iter_merger_idx;\n+\t      if (logger)\n+\t\tlogger->log (\"reusing merger state %i for item %i (EN: %i)\",\n+\t\t\t     it->m_merger_idx, i, it->m_input_enode->m_index);\n+\t      goto got_merger;\n+\t    }\n+\t}\n+      /* If it couldn't be merged with any existing merged_states,\n+\t create a new one.  */\n+      if (it->m_merger_idx == -1)\n+\t{\n+\t  it->m_merger_idx = merged_states.length ();\n+\t  merged_states.safe_push (new program_state (it_state));\n+\t  first_item_for_each_merged_state.safe_push (it);\n+\t  if (logger)\n+\t    logger->log (\"using new merger state %i for item %i (EN: %i)\",\n+\t\t\t it->m_merger_idx, i, it->m_input_enode->m_index);\n+\t}\n+    got_merger:\n+      gcc_assert (it->m_merger_idx >= 0);\n+      gcc_assert (it->m_merger_idx < merged_states.length ());\n+    }\n+\n+  /* Create merger nodes.  */\n+  auto_vec<exploded_node *> next_enodes (merged_states.length ());\n+  program_state *merged_state;\n+  FOR_EACH_VEC_ELT (merged_states, i, merged_state)\n+    {\n+      exploded_node *src_enode\n+\t= first_item_for_each_merged_state[i]->m_input_enode;\n+      exploded_node *next\n+\t= get_or_create_node (next_point, *merged_state, src_enode);\n+      /* \"next\" could be NULL; we handle that when adding the edges below.  */\n+      next_enodes.quick_push (next);\n+      if (logger)\n+\t{\n+\t  if (next)\n+\t    logger->log (\"using EN: %i for merger state %i\", next->m_index, i);\n+\t  else\n+\t    logger->log (\"using NULL enode for merger state %i\", i);\n+\t}\n+    }\n+\n+  /* Create edges from each input enode to the appropriate successor enode.\n+     Update the status of the now-processed input enodes.  */\n+  FOR_EACH_VEC_ELT (items, i, it)\n+    {\n+      exploded_node *next = next_enodes[it->m_merger_idx];\n+      if (next)\n+\tadd_edge (it->m_input_enode, next, NULL);\n+      it->m_input_enode->set_status (exploded_node::STATUS_BULK_MERGED);\n+    }\n+\n+  if (logger)\n+    logger->log (\"merged %i in-enodes into %i out-enode(s) at SN: %i\",\n+\t\t items.length (), merged_states.length (), snode->m_index);\n+\n+  return true;\n+}\n+\n /* Return true if STMT must appear at the start of its exploded node, and\n    thus we can't consolidate its effects within a run of other statements,\n    where PREV_STMT was the previous statement.  */\n@@ -3944,6 +4148,9 @@ class exploded_graph_annotator : public dot_annotator\n       case exploded_node::STATUS_MERGER:\n \tpp_string (pp, \"(M)\");\n \tbreak;\n+      case exploded_node::STATUS_BULK_MERGED:\n+\tpp_string (pp, \"(BM)\");\n+\tbreak;\n       }\n     gv->end_tdtr ();\n     /* Dump any saved_diagnostics at this enode.  */"}, {"sha": "5d4c3190283ff99712d03ad5b6135367db0d0149", "filename": "gcc/analyzer/exploded-graph.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b28491dc2d79763ecbff4f0b9f1f3e48a443be1d/gcc%2Fanalyzer%2Fexploded-graph.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b28491dc2d79763ecbff4f0b9f1f3e48a443be1d/gcc%2Fanalyzer%2Fexploded-graph.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fanalyzer%2Fexploded-graph.h?ref=b28491dc2d79763ecbff4f0b9f1f3e48a443be1d", "patch": "@@ -161,6 +161,9 @@ class exploded_node : public dnode<eg_traits>\n     /* Node was left unprocessed due to merger; it won't have had\n        exploded_graph::process_node called on it.  */\n     STATUS_MERGER,\n+\n+    /* Node was processed by maybe_process_run_of_before_supernode_enodes.  */\n+    STATUS_BULK_MERGED\n   };\n \n   exploded_node (const point_and_state &ps, int index);\n@@ -730,6 +733,7 @@ class exploded_graph : public digraph<eg_traits>\n \n   void build_initial_worklist ();\n   void process_worklist ();\n+  bool maybe_process_run_of_before_supernode_enodes (exploded_node *node);\n   void process_node (exploded_node *node);\n \n   exploded_node *get_or_create_node (const program_point &point,"}, {"sha": "1f1d8294c3399aeff263a99b58583454aa70b39c", "filename": "gcc/testsuite/gcc.dg/analyzer/bzip2-arg-parse-1.c", "status": "added", "additions": 95, "deletions": 0, "changes": 95, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b28491dc2d79763ecbff4f0b9f1f3e48a443be1d/gcc%2Ftestsuite%2Fgcc.dg%2Fanalyzer%2Fbzip2-arg-parse-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b28491dc2d79763ecbff4f0b9f1f3e48a443be1d/gcc%2Ftestsuite%2Fgcc.dg%2Fanalyzer%2Fbzip2-arg-parse-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Fanalyzer%2Fbzip2-arg-parse-1.c?ref=b28491dc2d79763ecbff4f0b9f1f3e48a443be1d", "patch": "@@ -0,0 +1,95 @@\n+/* Integration test to verify that we don't explode in this\n+   argument-parsing logic.\n+   Adapted from part of bzip2-1.0.8: bzip2.c: main.  */\n+\n+#include <stdlib.h>\n+#include <stdio.h>\n+#include \"analyzer-decls.h\"\n+\n+/* This test file has been heavily modified from the bzip2.c original,\n+   which has the following license boilerplate.  */\n+/* ------------------------------------------------------------------\n+   This file is part of bzip2/libbzip2, a program and library for\n+   lossless, block-sorting data compression.\n+\n+   bzip2/libbzip2 version 1.0.8 of 13 July 2019\n+   Copyright (C) 1996-2019 Julian Seward <jseward@acm.org>\n+\n+   Please read the WARNING, DISCLAIMER and PATENTS sections in the \n+   README file.\n+\n+   This program is released under the terms of the license contained\n+   in the file LICENSE.\n+   ------------------------------------------------------------------ */\n+\n+typedef char            Char;\n+typedef unsigned char   Bool;\n+typedef int             Int32;\n+\n+#define True  ((Bool)1)\n+#define False ((Bool)0)\n+\n+typedef\n+   struct zzzz {\n+      Char        *name;\n+      struct zzzz *link;\n+   }\n+   Cell;\n+\n+Int32   verbosity;\n+Bool    keepInputFiles, smallMode;\n+Bool    forceOverwrite, noisy;\n+Int32   blockSize100k;\n+Int32   opMode;\n+Int32   srcMode;\n+Char    *progName;\n+\n+extern void license ( void );\n+extern void usage ( Char *fullProgName );\n+\n+void test (Cell   *argList)\n+{\n+   Cell   *aa;\n+   Int32  i, j;\n+\n+   for (aa = argList; aa != NULL; aa = aa->link) {\n+      if (aa->name[0] == '-' && aa->name[1] != '-') {\n+         for (j = 1; aa->name[j] != '\\0'; j++) {\n+            switch (aa->name[j]) {\n+               case 'c': srcMode          = 2; break;\n+               case 'd': opMode           = 2; break;\n+               case 'z': opMode           = 1; break;\n+               case 'f': forceOverwrite   = True; break;\n+               case 't': opMode           = 3; break;\n+               case 'k': keepInputFiles   = True; break;\n+               case 's': smallMode        = True; break;\n+               case 'q': noisy            = False; break;\n+               case '1': blockSize100k    = 1; break;\n+               case '2': blockSize100k    = 2; break;\n+               case '3': blockSize100k    = 3; break;\n+               case '4': blockSize100k    = 4; break;\n+               case '5': blockSize100k    = 5; break;\n+               case '6': blockSize100k    = 6; break;\n+               case '7': blockSize100k    = 7; break;\n+               case '8': blockSize100k    = 8; break;\n+               case '9': blockSize100k    = 9; break;\n+               case 'V':\n+               case 'L': license();            break;\n+               case 'v': verbosity++; break;\n+               case 'h': usage ( progName );\n+                         exit ( 0 );\n+                         break;\n+               default:  fprintf ( stderr, \"%s: Bad flag `%s'\\n\",\n+                                   progName, aa->name );\n+                         usage ( progName );\n+                         exit ( 1 );\n+                         break;\n+            }\n+         }\n+      }\n+   }\n+\n+   /* The analyzer ought to be able to successfully merge all of the\n+      above changes that can reach here into a single state.  */\n+   __analyzer_dump_exploded_nodes (0); /* { dg-warning \"1 processed enode\" } */\n+}"}, {"sha": "553cd2f15dc40746a7e488530865b2227810d640", "filename": "gcc/testsuite/gcc.dg/analyzer/loop-n-down-to-1-by-1.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b28491dc2d79763ecbff4f0b9f1f3e48a443be1d/gcc%2Ftestsuite%2Fgcc.dg%2Fanalyzer%2Floop-n-down-to-1-by-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b28491dc2d79763ecbff4f0b9f1f3e48a443be1d/gcc%2Ftestsuite%2Fgcc.dg%2Fanalyzer%2Floop-n-down-to-1-by-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Fanalyzer%2Floop-n-down-to-1-by-1.c?ref=b28491dc2d79763ecbff4f0b9f1f3e48a443be1d", "patch": "@@ -24,8 +24,8 @@ void test(int n)\n       __analyzer_dump_exploded_nodes (0); /* { dg-warning \"2 processed enodes\" } */\n   }\n \n-  __analyzer_eval (i <= 0); /* { dg-warning \"TRUE\" \"true\" { xfail *-*-* } } */\n-  /* { dg-bogus \"UNKNOWN\" \"unknown\" { xfail *-*-* } .-1 } */\n+  __analyzer_eval (i <= 0); /* { dg-warning \"TRUE\" \"true\" } */\n+\n \n   __analyzer_eval (i == 0); /* { dg-warning \"TRUE\" \"desired\" { xfail *-*-* } } */\n   /* { dg-warning \"UNKNOWN\" \"status quo\" { target *-*-* } .-1 } */"}, {"sha": "da79652c570fe7a6170bdae6e3bc6ec46236ef0a", "filename": "gcc/testsuite/gcc.dg/analyzer/pr94851-1.c", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b28491dc2d79763ecbff4f0b9f1f3e48a443be1d/gcc%2Ftestsuite%2Fgcc.dg%2Fanalyzer%2Fpr94851-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b28491dc2d79763ecbff4f0b9f1f3e48a443be1d/gcc%2Ftestsuite%2Fgcc.dg%2Fanalyzer%2Fpr94851-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Fanalyzer%2Fpr94851-1.c?ref=b28491dc2d79763ecbff4f0b9f1f3e48a443be1d", "patch": "@@ -40,7 +40,8 @@ int pamark(void) {\n       last->m_next = p;\n   }\n \n-  p->m_name = (char)c;\n+  p->m_name = (char)c; /* { dg-bogus \"leak of 'p'\" \"bogus leak\" { xfail *-*-* } } */\n+  // TODO(xfail): related to PR analyzer/97072 and PR analyzer/97074\n \n   return 1;\n }"}]}