{"sha": "175160e7aaf4fd3dda9a76ba86602cf37dd2d23d", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MTc1MTYwZTdhYWY0ZmQzZGRhOWE3NmJhODY2MDJjZjM3ZGQyZDIzZA==", "commit": {"author": {"name": "Michael Tiemann", "email": "tiemann@gnu.org", "date": "1991-12-05T06:17:48Z"}, "committer": {"name": "Michael Tiemann", "email": "tiemann@gnu.org", "date": "1991-12-05T06:17:48Z"}, "message": "Initial revision\n\nFrom-SVN: r100", "tree": {"sha": "8fdb652fb6f2b7499c491a00ab3a61c1c3b677e4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/8fdb652fb6f2b7499c491a00ab3a61c1c3b677e4"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/175160e7aaf4fd3dda9a76ba86602cf37dd2d23d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/175160e7aaf4fd3dda9a76ba86602cf37dd2d23d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/175160e7aaf4fd3dda9a76ba86602cf37dd2d23d", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/175160e7aaf4fd3dda9a76ba86602cf37dd2d23d/comments", "author": null, "committer": null, "parents": [{"sha": "9c7e297806a27fe317e0a2cd0b9cba5360427cc3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9c7e297806a27fe317e0a2cd0b9cba5360427cc3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9c7e297806a27fe317e0a2cd0b9cba5360427cc3"}], "stats": {"total": 2509, "additions": 2509, "deletions": 0}, "files": [{"sha": "faad181bdcb29dfde0578c41fe8043359a9709b0", "filename": "gcc/integrate.c", "status": "added", "additions": 2509, "deletions": 0, "changes": 2509, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/175160e7aaf4fd3dda9a76ba86602cf37dd2d23d/gcc%2Fintegrate.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/175160e7aaf4fd3dda9a76ba86602cf37dd2d23d/gcc%2Fintegrate.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fintegrate.c?ref=175160e7aaf4fd3dda9a76ba86602cf37dd2d23d", "patch": "@@ -0,0 +1,2509 @@\n+/* Procedure integration for GNU CC.\n+   Copyright (C) 1988-1991 Free Software Foundation, Inc.\n+   Contributed by Michael Tiemann (tiemann@cygnus.com)\n+\n+This file is part of GNU CC.\n+\n+GNU CC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 2, or (at your option)\n+any later version.\n+\n+GNU CC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GNU CC; see the file COPYING.  If not, write to\n+the Free Software Foundation, 675 Mass Ave, Cambridge, MA 02139, USA.  */\n+\n+\n+#include <stdio.h>\n+\n+#include \"config.h\"\n+#include \"rtl.h\"\n+#include \"tree.h\"\n+#include \"flags.h\"\n+#include \"insn-config.h\"\n+#include \"insn-flags.h\"\n+#include \"expr.h\"\n+#include \"output.h\"\n+#include \"integrate.h\"\n+#include \"real.h\"\n+#include \"function.h\"\n+\n+#include \"obstack.h\"\n+#define\tobstack_chunk_alloc\txmalloc\n+#define\tobstack_chunk_free\tfree\n+extern int xmalloc ();\n+extern void free ();\n+\n+extern struct obstack *function_maybepermanent_obstack;\n+\n+extern tree pushdecl ();\n+extern tree poplevel ();\n+\n+/* Similar, but round to the next highest integer that meets the\n+   alignment.  */\n+#define CEIL_ROUND(VALUE,ALIGN)\t(((VALUE) + (ALIGN) - 1) & ~((ALIGN)- 1))\n+\n+/* Default max number of insns a function can have and still be inline.\n+   This is overridden on RISC machines.  */\n+#ifndef INTEGRATE_THRESHOLD\n+#define INTEGRATE_THRESHOLD(DECL) \\\n+  (8 * (8 + list_length (DECL_ARGUMENTS (DECL))))\n+#endif\n+\f\n+/* Save any constant pool constants in an insn.  */\n+static void save_constants ();\n+\n+/* Note when parameter registers are the destination of a SET.  */\n+static void note_modified_parmregs ();\n+\n+/* Copy an rtx for save_for_inline_copying.  */\n+static rtx copy_for_inline ();\n+\n+/* Make copies of MEMs in DECL_RTLs.  */\n+static void copy_decl_rtls ();\n+\n+static tree copy_decl_tree ();\n+\n+/* Return the constant equivalent of a given rtx, or 0 if none.  */\n+static rtx const_equiv ();\n+\n+static void integrate_parm_decls ();\n+static void integrate_decl_tree ();\n+\n+static void subst_constants ();\n+static rtx fold_out_const_cc0 ();\n+\f\n+/* Zero if the current function (whose FUNCTION_DECL is FNDECL)\n+   is safe and reasonable to integrate into other functions.\n+   Nonzero means value is a warning message with a single %s\n+   for the function's name.  */\n+\n+char *\n+function_cannot_inline_p (fndecl)\n+     register tree fndecl;\n+{\n+  register rtx insn;\n+  tree last = tree_last (TYPE_ARG_TYPES (TREE_TYPE (fndecl)));\n+  int max_insns = INTEGRATE_THRESHOLD (fndecl);\n+  register int ninsns = 0;\n+  register tree parms;\n+\n+  /* No inlines with varargs.  `grokdeclarator' gives a warning\n+     message about that if `inline' is specified.  This code\n+     it put in to catch the volunteers.  */\n+  if ((last && TREE_VALUE (last) != void_type_node)\n+      || (DECL_ARGUMENTS (fndecl) && DECL_NAME (DECL_ARGUMENTS (fndecl))\n+\t  && ! strcmp (IDENTIFIER_POINTER (DECL_NAME (DECL_ARGUMENTS (fndecl))),\n+\t\t       \"__builtin_va_alist\")))\n+    return \"varargs function cannot be inline\";\n+\n+  if (current_function_calls_alloca)\n+    return \"function using alloca cannot be inline\";\n+\n+  if (current_function_contains_functions)\n+    return \"function with nested functions cannot be inline\";\n+\n+  /* This restriction may be eliminated sometime soon.  But for now, don't\n+     worry about remapping the static chain.  */\n+  if (current_function_needs_context)\n+    return \"nested function cannot be inline\";\n+\n+  /* If its not even close, don't even look.  */\n+  if (!TREE_INLINE (fndecl) && get_max_uid () > 3 * max_insns)\n+    return \"function too large to be inline\";\n+\n+#if 0\n+  /* Large stacks are OK now that inlined functions can share them.  */\n+  /* Don't inline functions with large stack usage,\n+     since they can make other recursive functions burn up stack.  */\n+  if (!TREE_INLINE (fndecl) && get_frame_size () > 100)\n+    return \"function stack frame for inlining\";\n+#endif\n+\n+#if 0\n+  /* Don't inline functions which do not specify a function prototype and\n+     have BLKmode argument or take the address of a parameter.  */\n+  for (parms = DECL_ARGUMENTS (fndecl); parms; parms = TREE_CHAIN (parms))\n+    {\n+      if (TYPE_MODE (TREE_TYPE (parms)) == BLKmode)\n+\tTREE_ADDRESSABLE (parms) = 1;\n+      if (last == NULL_TREE && TREE_ADDRESSABLE (parms))\n+\treturn \"no prototype, and parameter address used; cannot be inline\";\n+    }\n+#endif\n+\n+  /* We can't inline functions that return structures\n+     the old-fashioned PCC way, copying into a static block.  */\n+  if (current_function_returns_pcc_struct)\n+    return \"inline functions not supported for this return value type\";\n+\n+  /* We can't inline functions that return structures of varying size.  */\n+  if (int_size_in_bytes (TREE_TYPE (TREE_TYPE (fndecl))) < 0)\n+    return \"function with varying-size return value cannot be inline\";\n+\n+  /* Cannot inline a function with a varying size argument.  */\n+  for (parms = DECL_ARGUMENTS (fndecl); parms; parms = TREE_CHAIN (parms))\n+    if (int_size_in_bytes (TREE_TYPE (parms)) < 0)\n+      return \"function with varying-size parameter cannot be inline\";\n+\n+  if (!TREE_INLINE (fndecl) && get_max_uid () > max_insns)\n+    {\n+      for (ninsns = 0, insn = get_first_nonparm_insn (); insn && ninsns < max_insns;\n+\t   insn = NEXT_INSN (insn))\n+\t{\n+\t  if (GET_RTX_CLASS (GET_CODE (insn)) == 'i')\n+\t    ninsns++;\n+\t}\n+\n+      if (ninsns >= max_insns)\n+\treturn \"function too large to be inline\";\n+    }\n+\n+  return 0;\n+}\n+\f\n+/* Variables used within save_for_inline.  */\n+\n+/* Mapping from old pseudo-register to new pseudo-registers.\n+   The first element of this map is reg_map[FIRST_PSEUDO_REGISTER].\n+   It is allocated in `save_for_inline' and `expand_inline_function',\n+   and deallocated on exit from each of those routines.  */\n+static rtx *reg_map;\n+\n+/* Mapping from old code-labels to new code-labels.\n+   The first element of this map is label_map[min_labelno].\n+   It is allocated in `save_for_inline' and `expand_inline_function',\n+   and deallocated on exit from each of those routines.  */\n+static rtx *label_map;\n+\n+/* Mapping from old insn uid's to copied insns.\n+   It is allocated in `save_for_inline' and `expand_inline_function',\n+   and deallocated on exit from each of those routines.  */\n+static rtx *insn_map;\n+\n+/* Map pseudo reg number into the PARM_DECL for the parm living in the reg.\n+   Zero for a reg that isn't a parm's home.\n+   Only reg numbers less than max_parm_reg are mapped here.  */\n+static tree *parmdecl_map;\n+\n+/* Keep track of first pseudo-register beyond those that are parms.  */\n+static int max_parm_reg;\n+\n+/* When an insn is being copied by copy_for_inline,\n+   this is nonzero if we have copied an ASM_OPERANDS.\n+   In that case, it is the original input-operand vector.  */\n+static rtvec orig_asm_operands_vector;\n+\n+/* When an insn is being copied by copy_for_inline,\n+   this is nonzero if we have copied an ASM_OPERANDS.\n+   In that case, it is the copied input-operand vector.  */\n+static rtvec copy_asm_operands_vector;\n+\n+/* Likewise, this is the copied constraints vector.  */\n+static rtvec copy_asm_constraints_vector;\n+\n+/* In save_for_inline, nonzero if past the parm-initialization insns.  */\n+static int in_nonparm_insns;\n+\f\n+/* Subroutine for `save_for_inline{copying,nocopy}'.  Performs initialization\n+   needed to save FNDECL's insns and info for future inline expansion.  */\n+   \n+static rtx\n+initialize_for_inline (fndecl, min_labelno, max_labelno, max_reg, copy)\n+     tree fndecl;\n+     int min_labelno;\n+     int max_labelno;\n+     int max_reg;\n+     int copy;\n+{\n+  int function_flags, i;\n+  rtvec arg_vector;\n+  tree parms;\n+\n+  /* Compute the values of any flags we must restore when inlining this.  */\n+\n+  function_flags\n+    = (current_function_calls_alloca * FUNCTION_FLAGS_CALLS_ALLOCA\n+       + current_function_calls_setjmp * FUNCTION_FLAGS_CALLS_SETJMP\n+       + current_function_calls_longjmp * FUNCTION_FLAGS_CALLS_LONGJMP\n+       + current_function_returns_struct * FUNCTION_FLAGS_RETURNS_STRUCT\n+       + current_function_returns_pcc_struct * FUNCTION_FLAGS_RETURNS_PCC_STRUCT\n+       + current_function_needs_context * FUNCTION_FLAGS_NEEDS_CONTEXT\n+       + current_function_has_nonlocal_label * FUNCTION_FLAGS_HAS_NONLOCAL_LABEL\n+       + current_function_returns_pointer * FUNCTION_FLAGS_RETURNS_POINTER\n+       + current_function_uses_const_pool * FUNCTION_FLAGS_USES_CONST_POOL\n+       + current_function_uses_pic_offset_table * FUNCTION_FLAGS_USES_PIC_OFFSET_TABLE);\n+\n+  /* Clear out PARMDECL_MAP.  It was allocated in the caller's frame.  */\n+  bzero (parmdecl_map, max_parm_reg * sizeof (tree));\n+  arg_vector = rtvec_alloc (list_length (DECL_ARGUMENTS (fndecl)));\n+\n+  for (parms = DECL_ARGUMENTS (fndecl), i = 0;\n+       parms;\n+       parms = TREE_CHAIN (parms), i++)\n+    {\n+      rtx p = DECL_RTL (parms);\n+\n+      if (GET_CODE (p) == MEM && copy)\n+\t/* Copy the rtl so that modifications of the address\n+\t   later in compilation won't affect this arg_vector.\n+\t   Virtual register instantiation can screw the address\n+\t   of the rtl.  */\n+\tDECL_RTL (parms) = copy_rtx (p);\n+\n+      RTVEC_ELT (arg_vector, i) = p;\n+\n+      if (GET_CODE (p) == REG)\n+\tparmdecl_map[REGNO (p)] = parms;\n+      TREE_READONLY (parms) = 1;\n+    }\n+\n+  /* Assume we start out in the insns that set up the parameters.  */\n+  in_nonparm_insns = 0;\n+\n+  /* The list of DECL_SAVED_INSNS, starts off with a header which\n+     contains the following information:\n+\n+     the first insn of the function (not including the insns that copy\n+     parameters into registers).\n+     the first parameter insn of the function,\n+     the first label used by that function,\n+     the last label used by that function,\n+     the highest register number used for parameters,\n+     the total number of registers used,\n+     the size of the incoming stack area for parameters,\n+     the number of bytes popped on return,\n+     the stack slot list,\n+     some flags that are used to restore compiler globals,\n+     the value of current_function_outgoing_args_size,\n+     the original argument vector,\n+     and the original DECL_INITIAL.  */\n+\n+  return gen_inline_header_rtx (NULL, NULL, min_labelno, max_labelno,\n+\t\t\t\tmax_parm_reg, max_reg,\n+\t\t\t\tcurrent_function_args_size,\n+\t\t\t\tcurrent_function_pops_args,\n+\t\t\t\tstack_slot_list, function_flags,\n+\t\t\t\tcurrent_function_outgoing_args_size,\n+\t\t\t\targ_vector, (rtx) DECL_INITIAL (fndecl));\n+}\n+\n+/* Subroutine for `save_for_inline{copying,nocopy}'.  Finishes up the\n+   things that must be done to make FNDECL expandable as an inline function.\n+   HEAD contains the chain of insns to which FNDECL will expand.  */\n+   \n+static void\n+finish_inline (fndecl, head)\n+     tree fndecl;\n+     rtx head;\n+{\n+  NEXT_INSN (head) = get_first_nonparm_insn ();\n+  FIRST_PARM_INSN (head) = get_insns ();\n+  DECL_SAVED_INSNS (fndecl) = head;\n+  DECL_FRAME_SIZE (fndecl) = get_frame_size ();\n+  TREE_INLINE (fndecl) = 1;\n+}\n+\n+/* Make the insns and PARM_DECLs of the current function permanent\n+   and record other information in DECL_SAVED_INSNS to allow inlining\n+   of this function in subsequent calls.\n+\n+   This function is called when we are going to immediately compile\n+   the insns for FNDECL.  The insns in maybepermanent_obstack cannot be\n+   modified by the compilation process, so we copy all of them to\n+   new storage and consider the new insns to be the insn chain to be\n+   compiled.  */\n+\n+void\n+save_for_inline_copying (fndecl)\n+     tree fndecl;\n+{\n+  rtx first_insn, last_insn, insn;\n+  rtx head, copy;\n+  int max_labelno, min_labelno, i, len;\n+  int max_reg;\n+  int max_uid;\n+  rtx first_nonparm_insn;\n+\n+  /* Make and emit a return-label if we have not already done so. \n+     Do this before recording the bounds on label numbers. */\n+\n+  if (return_label == 0)\n+    {\n+      return_label = gen_label_rtx ();\n+      emit_label (return_label);\n+    }\n+\n+  /* Get some bounds on the labels and registers used.  */\n+\n+  max_labelno = max_label_num ();\n+  min_labelno = get_first_label_num ();\n+  max_reg = max_reg_num ();\n+\n+  /* Set up PARMDECL_MAP which maps pseudo-reg number to its PARM_DECL.\n+     Later we set TREE_READONLY to 0 if the parm is modified inside the fn.\n+     Also set up ARG_VECTOR, which holds the unmodified DECL_RTX values\n+     for the parms, prior to elimination of virtual registers.\n+     These values are needed for substituting parms properly.  */\n+\n+  max_parm_reg = max_parm_reg_num ();\n+  parmdecl_map = (tree *) alloca (max_parm_reg * sizeof (tree));\n+\n+  head = initialize_for_inline (fndecl, min_labelno, max_labelno, max_reg, 1);\n+\n+  if (current_function_uses_const_pool)\n+    {\n+      /* Replace any constant pool references with the actual constant.  We\n+\t will put the constants back in the copy made below.  */\n+      for (insn = get_insns (); insn; insn = NEXT_INSN (insn))\n+\tif (GET_RTX_CLASS (GET_CODE (insn)) == 'i')\n+\t  {\n+\t    save_constants (&PATTERN (insn));\n+\t    if (REG_NOTES (insn))\n+\t      save_constants (&REG_NOTES (insn));\n+\t  }\n+\n+      /* Clear out the constant pool so that we can recreate it with the\n+\t copied constants below.  */\n+      init_const_rtx_hash_table ();\n+      clear_const_double_mem ();\n+    }\n+\n+  max_uid = INSN_UID (head);\n+\n+  /* We have now allocated all that needs to be allocated permanently\n+     on the rtx obstack.  Set our high-water mark, so that we\n+     can free the rest of this when the time comes.  */\n+\n+  preserve_data ();\n+\n+  /* Copy the chain insns of this function.\n+     Install the copied chain as the insns of this function,\n+     for continued compilation;\n+     the original chain is recorded as the DECL_SAVED_INSNS\n+     for inlining future calls.  */\n+\n+  /* If there are insns that copy parms from the stack into pseudo registers,\n+     those insns are not copied.  `expand_inline_function' must\n+     emit the correct code to handle such things.  */\n+\n+  insn = get_insns ();\n+  if (GET_CODE (insn) != NOTE)\n+    abort ();\n+  first_insn = rtx_alloc (NOTE);\n+  NOTE_SOURCE_FILE (first_insn) = NOTE_SOURCE_FILE (insn);\n+  NOTE_LINE_NUMBER (first_insn) = NOTE_LINE_NUMBER (insn);\n+  INSN_UID (first_insn) = INSN_UID (insn);\n+  PREV_INSN (first_insn) = NULL;\n+  NEXT_INSN (first_insn) = NULL;\n+  last_insn = first_insn;\n+\n+  /* Each pseudo-reg in the old insn chain must have a unique rtx in the copy.\n+     Make these new rtx's now, and install them in regno_reg_rtx, so they\n+     will be the official pseudo-reg rtx's for the rest of compilation.  */\n+\n+  reg_map = (rtx *) alloca ((max_reg + 1) * sizeof (rtx));\n+\n+  len = sizeof (struct rtx_def) + (GET_RTX_LENGTH (REG) - 1) * sizeof (rtunion);\n+  for (i = max_reg - 1; i > LAST_VIRTUAL_REGISTER; i--)\n+    reg_map[i] = (rtx)obstack_copy (function_maybepermanent_obstack,\n+\t\t\t\t    regno_reg_rtx[i], len);\n+\n+  bcopy (reg_map + LAST_VIRTUAL_REGISTER + 1,\n+\t regno_reg_rtx + LAST_VIRTUAL_REGISTER + 1,\n+\t (max_reg - (LAST_VIRTUAL_REGISTER + 1)) * sizeof (rtx));\n+\n+  /* Likewise each label rtx must have a unique rtx as its copy.  */\n+\n+  label_map = (rtx *)alloca ((max_labelno - min_labelno) * sizeof (rtx));\n+  label_map -= min_labelno;\n+\n+  for (i = min_labelno; i < max_labelno; i++)\n+    label_map[i] = gen_label_rtx ();\n+\n+  /* Record the mapping of old insns to copied insns.  */\n+\n+  insn_map = (rtx *) alloca (max_uid * sizeof (rtx));\n+  bzero (insn_map, max_uid * sizeof (rtx));\n+\n+  /* Get the insn which signals the end of parameter setup code.  */\n+  first_nonparm_insn = get_first_nonparm_insn ();\n+\n+  /* Copy any entries in regno_reg_rtx or DECL_RTLs that reference MEM\n+     (the former occurs when a variable has its address taken)\n+     since these may be shared and can be changed by virtual\n+     register instantiation.  DECL_RTL values for our arguments\n+     have already been copied by initialize_for_inline.  */\n+  for (i = LAST_VIRTUAL_REGISTER + 1; i < max_reg; i++)\n+    if (GET_CODE (regno_reg_rtx[i]) == MEM)\n+      XEXP (regno_reg_rtx[i], 0)\n+\t= copy_for_inline (XEXP (regno_reg_rtx[i], 0));\n+\n+  /* Copy the tree of subblocks of the function, and the decls in them.\n+     We will use the copy for compiling this function, then restore the original\n+     subblocks and decls for use when inlining this function.\n+\n+     Several parts of the compiler modify BLOCK trees.  In particular,\n+     instantiate_virtual_regs will instantiate any virtual regs\n+     mentioned in the DECL_RTLs of the decls, and loop\n+     unrolling will replicate any BLOCK trees inside an unrolled loop.\n+\n+     The modified subblocks or DECL_RTLs would be incorrect for the original rtl\n+     which we will use for inlining.  The rtl might even contain pseudoregs\n+     whose space has been freed.  */\n+\n+  DECL_INITIAL (fndecl) = copy_decl_tree (DECL_INITIAL (fndecl));\n+\n+  /* Now copy each DECL_RTL which is a MEM,\n+     so it is safe to modify their addresses.  */\n+  copy_decl_rtls (DECL_INITIAL (fndecl));\n+\n+  /* Now copy the chain of insns.  Do this twice.  The first copy the insn\n+     itself and its body.  The second time copy of REG_NOTES.  This is because\n+     a REG_NOTE may have a forward pointer to another insn.  */\n+\n+  for (insn = NEXT_INSN (insn); insn; insn = NEXT_INSN (insn))\n+    {\n+      orig_asm_operands_vector = 0;\n+\n+      if (insn == first_nonparm_insn)\n+\tin_nonparm_insns = 1;\n+\n+      switch (GET_CODE (insn))\n+\t{\n+\tcase NOTE:\n+\t  /* No need to keep these.  */\n+\t  if (NOTE_LINE_NUMBER (insn) == NOTE_INSN_DELETED)\n+\t    continue;\n+\n+\t  copy = rtx_alloc (NOTE);\n+\t  NOTE_SOURCE_FILE (copy) = NOTE_SOURCE_FILE (insn);\n+\t  NOTE_LINE_NUMBER (copy) = NOTE_LINE_NUMBER (insn);\n+\t  break;\n+\n+\tcase INSN:\n+\tcase CALL_INSN:\n+\tcase JUMP_INSN:\n+\t  copy = rtx_alloc (GET_CODE (insn));\n+\t  PATTERN (copy) = copy_for_inline (PATTERN (insn));\n+\t  INSN_CODE (copy) = -1;\n+\t  LOG_LINKS (copy) = NULL;\n+\t  RTX_INTEGRATED_P (copy) = RTX_INTEGRATED_P (insn);\n+\t  break;\n+\n+\tcase CODE_LABEL:\n+\t  copy = label_map[CODE_LABEL_NUMBER (insn)];\n+\t  break;\n+\n+\tcase BARRIER:\n+\t  copy = rtx_alloc (BARRIER);\n+\t  break;\n+\n+\tdefault:\n+\t  abort ();\n+\t}\n+      INSN_UID (copy) = INSN_UID (insn);\n+      insn_map[INSN_UID (insn)] = copy;\n+      NEXT_INSN (last_insn) = copy;\n+      PREV_INSN (copy) = last_insn;\n+      last_insn = copy;\n+    }\n+\n+  /* Now copy the REG_NOTES.  */\n+  for (insn = NEXT_INSN (get_insns ()); insn; insn = NEXT_INSN (insn))\n+    if (GET_RTX_CLASS (GET_CODE (insn)) == 'i'\n+\t&& insn_map[INSN_UID(insn)])\n+      REG_NOTES (insn_map[INSN_UID (insn)])\n+\t= copy_for_inline (REG_NOTES (insn));\n+\n+  NEXT_INSN (last_insn) = NULL;\n+\n+  finish_inline (fndecl, head);\n+\n+  set_new_first_and_last_insn (first_insn, last_insn);\n+}\n+\n+/* Make a copy of the entire tree of blocks BLOCK, and return it.  */\n+\n+static tree\n+copy_decl_tree (block)\n+     tree block;\n+{\n+  tree t, vars, subblocks;\n+\n+  vars = copy_list (BLOCK_VARS (block));\n+  subblocks = 0;\n+\n+  /* Process all subblocks.  */\n+  for (t = BLOCK_SUBBLOCKS (block); t; t = TREE_CHAIN (t))\n+    {\n+      tree copy = copy_decl_tree (t);\n+      TREE_CHAIN (copy) = subblocks;\n+      subblocks = copy;\n+    }\n+\n+  t = copy_node (block);\n+  BLOCK_VARS (t) = vars;\n+  BLOCK_SUBBLOCKS (t) = nreverse (subblocks);\n+  return t;\n+}\n+\n+/* Copy DECL_RTLs in all decls in the given BLOCK node.  */\n+\n+static void\n+copy_decl_rtls (block)\n+     tree block;\n+{\n+  tree t;\n+\n+  for (t = BLOCK_VARS (block); t; t = TREE_CHAIN (t))\n+    if (DECL_RTL (t) && GET_CODE (DECL_RTL (t)) == MEM)\n+      DECL_RTL (t) = copy_for_inline (DECL_RTL (t));\n+\n+  /* Process all subblocks.  */\n+  for (t = BLOCK_SUBBLOCKS (block); t; t = TREE_CHAIN (t))\n+    copy_decl_rtls (t);\n+}\n+\n+/* Make the insns and PARM_DECLs of the current function permanent\n+   and record other information in DECL_SAVED_INSNS to allow inlining\n+   of this function in subsequent calls.\n+\n+   This routine need not copy any insns because we are not going\n+   to immediately compile the insns in the insn chain.  There\n+   are two cases when we would compile the insns for FNDECL:\n+   (1) when FNDECL is expanded inline, and (2) when FNDECL needs to\n+   be output at the end of other compilation, because somebody took\n+   its address.  In the first case, the insns of FNDECL are copied\n+   as it is expanded inline, so FNDECL's saved insns are not\n+   modified.  In the second case, FNDECL is used for the last time,\n+   so modifying the rtl is not a problem.\n+\n+   ??? Actually, we do not verify that FNDECL is not inline expanded\n+   by other functions which must also be written down at the end\n+   of compilation.  We could set flag_no_inline to nonzero when\n+   the time comes to write down such functions.  */\n+\n+void\n+save_for_inline_nocopy (fndecl)\n+     tree fndecl;\n+{\n+  rtx insn;\n+  rtx head, copy;\n+  tree parms;\n+  int max_labelno, min_labelno, i, len;\n+  int max_reg;\n+  int max_uid;\n+  rtx first_nonparm_insn;\n+  int function_flags;\n+\n+  /* Set up PARMDECL_MAP which maps pseudo-reg number to its PARM_DECL.\n+     Later we set TREE_READONLY to 0 if the parm is modified inside the fn.\n+     Also set up ARG_VECTOR, which holds the unmodified DECL_RTX values\n+     for the parms, prior to elimination of virtual registers.\n+     These values are needed for substituting parms properly.  */\n+\n+  max_parm_reg = max_parm_reg_num ();\n+  parmdecl_map = (tree *) alloca (max_parm_reg * sizeof (tree));\n+\n+  /* Make and emit a return-label if we have not already done so.  */\n+\n+  if (return_label == 0)\n+    {\n+      return_label = gen_label_rtx ();\n+      emit_label (return_label);\n+    }\n+\n+  head = initialize_for_inline (fndecl, get_first_label_num (),\n+\t\t\t\tmax_label_num (), max_reg_num (), 0);\n+\n+  /* If there are insns that copy parms from the stack into pseudo registers,\n+     those insns are not copied.  `expand_inline_function' must\n+     emit the correct code to handle such things.  */\n+\n+  insn = get_insns ();\n+  if (GET_CODE (insn) != NOTE)\n+    abort ();\n+\n+  /* Get the insn which signals the end of parameter setup code.  */\n+  first_nonparm_insn = get_first_nonparm_insn ();\n+\n+  /* Now just scan the chain of insns to see what happens to our\n+     PARM_DECLs.  If a PARM_DECL is used but never modified, we\n+     can substitute its rtl directly when expanding inline (and\n+     perform constant folding when its incoming value is constant).\n+     Otherwise, we have to copy its value into a new register and track\n+     the new register's life.  */\n+\n+  for (insn = NEXT_INSN (insn); insn; insn = NEXT_INSN (insn))\n+    {\n+      if (insn == first_nonparm_insn)\n+\tin_nonparm_insns = 1;\n+\n+      if (GET_RTX_CLASS (GET_CODE (insn)) == 'i')\n+\t{\n+\t  if (current_function_uses_const_pool)\n+\t    {\n+\t      /* Replace any constant pool references with the actual constant.\n+\t\t We will put the constant back if we need to write the\n+\t\t function out after all.  */\n+\t      save_constants (&PATTERN (insn));\n+\t      if (REG_NOTES (insn))\n+\t\tsave_constants (&REG_NOTES (insn));\n+\t    }\n+\n+\t  /* Record what interesting things happen to our parameters.  */\n+\t  note_stores (PATTERN (insn), note_modified_parmregs);\n+\t}\n+    }\n+\n+  /* We have now allocated all that needs to be allocated permanently\n+     on the rtx obstack.  Set our high-water mark, so that we\n+     can free the rest of this when the time comes.  */\n+\n+  preserve_data ();\n+\n+  finish_inline (fndecl, head);\n+}\n+\f\n+/* Given PX, a pointer into an insn, search for references to the constant\n+   pool.  Replace each with a CONST that has the mode of the original\n+   constant, contains the constant, and has RTX_INTEGRATED_P set.\n+   Similarly, constant pool addresses not enclosed in a MEM are replaced\n+   with an ADDRESS rtx which also gives the constant, mode, and has\n+   RTX_INTEGRATED_P set.  */\n+\n+static void\n+save_constants (px)\n+     rtx *px;\n+{\n+  rtx x;\n+  int i, j;\n+\n+ again:\n+  x = *px;\n+\n+  /* If this is a CONST_DOUBLE, don't try to fix things up in \n+     CONST_DOUBLE_MEM, because this is an infinite recursion.  */\n+  if (GET_CODE (x) == CONST_DOUBLE)\n+    return;\n+  else if (GET_CODE (x) == MEM && GET_CODE (XEXP (x, 0)) == SYMBOL_REF\n+\t   && CONSTANT_POOL_ADDRESS_P (XEXP (x,0)))\n+    {\n+      enum machine_mode const_mode = get_pool_mode (XEXP (x, 0));\n+      rtx new = gen_rtx (CONST, const_mode, get_pool_constant (XEXP (x, 0)));\n+      RTX_INTEGRATED_P (new) = 1;\n+\n+      /* If the MEM was in a different mode than the constant (perhaps we\n+\t were only looking at the low-order part), surround it with a \n+\t SUBREG so we can save both modes.  */\n+\n+      if (GET_MODE (x) != const_mode)\n+\t{\n+\t  new = gen_rtx (SUBREG, GET_MODE (x), new, 0);\n+\t  RTX_INTEGRATED_P (new) = 1;\n+\t}\n+\n+      *px = new;\n+      save_constants (&XEXP (*px, 0));\n+    }\n+  else if (GET_CODE (x) == SYMBOL_REF\n+\t   && CONSTANT_POOL_ADDRESS_P (x))\n+    {\n+      *px = gen_rtx (ADDRESS, get_pool_mode (x), get_pool_constant (x));\n+      save_constants (&XEXP (*px, 0));\n+      RTX_INTEGRATED_P (*px) = 1;\n+    }\n+\n+  else\n+    {\n+      char *fmt = GET_RTX_FORMAT (GET_CODE (x));\n+      int len = GET_RTX_LENGTH (GET_CODE (x));\n+\n+      for (i = len-1; i >= 0; i--)\n+\t{\n+\t  switch (fmt[i])\n+\t    {\n+\t    case 'E':\n+\t      for (j = 0; j < XVECLEN (x, i); j++)\n+\t\tsave_constants (&XVECEXP (x, i, j));\n+\t      break;\n+\n+\t    case 'e':\n+\t      if (XEXP (x, i) == 0)\n+\t\tcontinue;\n+\t      if (i == 0)\n+\t\t{\n+\t\t  /* Hack tail-recursion here.  */\n+\t\t  px = &XEXP (x, 0);\n+\t\t  goto again;\n+\t\t}\n+\t      save_constants (&XEXP (x, i));\n+\t      break;\n+\t    }\n+\t}\n+    }\n+}\n+\f\n+/* Note whether a parameter is modified or not.  */\n+\n+static void\n+note_modified_parmregs (reg, x)\n+     rtx reg;\n+     rtx x;\n+{\n+  if (GET_CODE (reg) == REG && in_nonparm_insns\n+      && REGNO (reg) < max_parm_reg\n+      && REGNO (reg) >= FIRST_PSEUDO_REGISTER\n+      && parmdecl_map[REGNO (reg)] != 0)\n+    TREE_READONLY (parmdecl_map[REGNO (reg)]) = 0;\n+}\n+\n+/* Copy the rtx ORIG recursively, replacing pseudo-regs and labels\n+   according to `reg_map' and `label_map'.  The original rtl insns\n+   will be saved for inlining; this is used to make a copy\n+   which is used to finish compiling the inline function itself.\n+\n+   If we find a \"saved\" constant pool entry, one which was replaced with\n+   the value of the constant, convert it back to a constant pool entry.\n+   Since the pool wasn't touched, this should simply restore the old\n+   address.\n+\n+   All other kinds of rtx are copied except those that can never be\n+   changed during compilation.  */\n+\n+static rtx\n+copy_for_inline (orig)\n+     rtx orig;\n+{\n+  register rtx x = orig;\n+  register int i;\n+  register enum rtx_code code;\n+  register char *format_ptr;\n+\n+  if (x == 0)\n+    return x;\n+\n+  code = GET_CODE (x);\n+\n+  /* These types may be freely shared.  */\n+\n+  switch (code)\n+    {\n+    case QUEUED:\n+    case CONST_INT:\n+    case SYMBOL_REF:\n+    case PC:\n+    case CC0:\n+      return x;\n+\n+    case CONST_DOUBLE:\n+      /* We have to make a new CONST_DOUBLE to ensure that we account for\n+\t it correctly.  Using the old CONST_DOUBLE_MEM data is wrong.  */\n+      if (GET_MODE_CLASS (GET_MODE (x)) == MODE_FLOAT)\n+\t{\n+\t  REAL_VALUE_TYPE d;\n+\n+\t  REAL_VALUE_FROM_CONST_DOUBLE (d, x);\n+\t  return immed_real_const_1 (d, GET_MODE (x));\n+\t}\n+      else\n+\treturn immed_double_const (CONST_DOUBLE_LOW (x), CONST_DOUBLE_HIGH (x),\n+\t\t\t\t   VOIDmode);\n+\n+    case CONST:\n+      /* Get constant pool entry for constant in the pool.  */\n+      if (RTX_INTEGRATED_P (x))\n+\treturn validize_mem (force_const_mem (GET_MODE (x),\n+\t\t\t\t\t      copy_for_inline (XEXP (x, 0))));\n+      break;\n+\n+    case SUBREG:\n+      /* Get constant pool entry, but access in different mode.  */\n+      if (RTX_INTEGRATED_P (x))\n+\t{\n+\t  rtx new\n+\t    = force_const_mem (GET_MODE (SUBREG_REG (x)),\n+\t\t\t       copy_for_inline (XEXP (SUBREG_REG (x), 0)));\n+\n+\t  PUT_MODE (new, GET_MODE (x));\n+\t  return validize_mem (new);\n+\t}\n+      break;\n+\n+    case ADDRESS:\n+      /* If not special for constant pool error.  Else get constant pool\n+\t address.  */\n+      if (! RTX_INTEGRATED_P (x))\n+\tabort ();\n+\n+      return XEXP (force_const_mem (GET_MODE (x),\n+\t\t\t\t    copy_for_inline (XEXP (x, 0))), 0);\n+\n+    case ASM_OPERANDS:\n+      /* If a single asm insn contains multiple output operands\n+\t then it contains multiple ASM_OPERANDS rtx's that share operand 3.\n+\t We must make sure that the copied insn continues to share it.  */\n+      if (orig_asm_operands_vector == XVEC (orig, 3))\n+\t{\n+\t  x = rtx_alloc (ASM_OPERANDS);\n+\t  XSTR (x, 0) = XSTR (orig, 0);\n+\t  XSTR (x, 1) = XSTR (orig, 1);\n+\t  XINT (x, 2) = XINT (orig, 2);\n+\t  XVEC (x, 3) = copy_asm_operands_vector;\n+\t  XVEC (x, 4) = copy_asm_constraints_vector;\n+\t  XSTR (x, 5) = XSTR (orig, 5);\n+\t  XINT (x, 6) = XINT (orig, 6);\n+\t  return x;\n+\t}\n+      break;\n+\n+    case MEM:\n+      /* A MEM is usually allowed to be shared if its address is constant\n+\t or is a constant plus one of the special registers.\n+\n+\t We do not allow sharing of addresses that are either a special\n+\t register or the sum of a constant and a special register because\n+\t it is possible for unshare_all_rtl to copy the address, into memory\n+\t that won't be saved.  Although the MEM can safely be shared, and\n+\t won't be copied there, the address itself cannot be shared, and may\n+\t need to be copied. \n+\n+\t There are also two exceptions with constants: The first is if the\n+\t constant is a LABEL_REF or the sum of the LABEL_REF\n+\t and an integer.  This case can happen if we have an inline\n+\t function that supplies a constant operand to the call of another\n+\t inline function that uses it in a switch statement.  In this case,\n+\t we will be replacing the LABEL_REF, so we have to replace this MEM\n+\t as well.\n+\n+\t The second case is if we have a (const (plus (address ..) ...)).\n+\t In that case we need to put back the address of the constant pool\n+\t entry.  */\n+\n+      if (CONSTANT_ADDRESS_P (XEXP (x, 0))\n+\t  && GET_CODE (XEXP (x, 0)) != LABEL_REF\n+\t  && ! (GET_CODE (XEXP (x, 0)) == CONST\n+\t\t&& (GET_CODE (XEXP (XEXP (x, 0), 0)) == PLUS\n+\t\t    && ((GET_CODE (XEXP (XEXP (XEXP (x, 0), 0), 0))\n+\t\t\t== LABEL_REF)\n+\t\t\t|| (GET_CODE (XEXP (XEXP (XEXP (x, 0), 0), 0))\n+\t\t\t    == ADDRESS)))))\n+\treturn x;\n+      break;\n+\n+    case LABEL_REF:\n+      {\n+\t/* Must point to the new insn.  */\n+\treturn gen_rtx (LABEL_REF, GET_MODE (orig),\n+\t\t\tlabel_map[CODE_LABEL_NUMBER (XEXP (orig, 0))]);\n+      }\n+\n+    case REG:\n+      if (REGNO (x) > LAST_VIRTUAL_REGISTER)\n+\treturn reg_map [REGNO (x)];\n+      else\n+\treturn x;\n+\n+    case SET:\n+      /* If a parm that gets modified lives in a pseudo-reg,\n+\t clear its TREE_READONLY to prevent certain optimizations.  */\n+      {\n+\trtx dest = SET_DEST (x);\n+\n+\twhile (GET_CODE (dest) == STRICT_LOW_PART\n+\t       || GET_CODE (dest) == ZERO_EXTRACT\n+\t       || GET_CODE (dest) == SUBREG)\n+\t  dest = XEXP (dest, 0);\n+\n+\tif (GET_CODE (dest) == REG\n+\t    && REGNO (dest) < max_parm_reg\n+\t    && REGNO (dest) >= FIRST_PSEUDO_REGISTER\n+\t    && parmdecl_map[REGNO (dest)] != 0\n+\t    /* The insn to load an arg pseudo from a stack slot\n+\t       does not count as modifying it.  */\n+\t    && in_nonparm_insns)\n+\t  TREE_READONLY (parmdecl_map[REGNO (dest)]) = 0;\n+      }\n+      break;\n+\n+#if 0 /* This is a good idea, but here is the wrong place for it.  */\n+      /* Arrange that CONST_INTs always appear as the second operand\n+\t if they appear, and that `frame_pointer_rtx' or `arg_pointer_rtx'\n+\t always appear as the first.  */\n+    case PLUS:\n+      if (GET_CODE (XEXP (x, 0)) == CONST_INT\n+\t  || (XEXP (x, 1) == frame_pointer_rtx\n+\t      || (ARG_POINTER_REGNUM != FRAME_POINTER_REGNUM\n+\t\t  && XEXP (x, 1) == arg_pointer_rtx)))\n+\t{\n+\t  rtx t = XEXP (x, 0);\n+\t  XEXP (x, 0) = XEXP (x, 1);\n+\t  XEXP (x, 1) = t;\n+\t}\n+      break;\n+#endif\n+    }\n+\n+  /* Replace this rtx with a copy of itself.  */\n+\n+  x = rtx_alloc (code);\n+  bcopy (orig, x, (sizeof (*x) - sizeof (x->fld)\n+\t\t   + sizeof (x->fld[0]) * GET_RTX_LENGTH (code)));\n+\n+  /* Now scan the subexpressions recursively.\n+     We can store any replaced subexpressions directly into X\n+     since we know X is not shared!  Any vectors in X\n+     must be copied if X was copied.  */\n+\n+  format_ptr = GET_RTX_FORMAT (code);\n+\n+  for (i = 0; i < GET_RTX_LENGTH (code); i++)\n+    {\n+      switch (*format_ptr++)\n+\t{\n+\tcase 'e':\n+\t  XEXP (x, i) = copy_for_inline (XEXP (x, i));\n+\t  break;\n+\n+\tcase 'u':\n+\t  /* Change any references to old-insns to point to the\n+\t     corresponding copied insns.  */\n+\t  XEXP (x, i) = insn_map[INSN_UID (XEXP (x, i))];\n+\t  break;\n+\n+\tcase 'E':\n+\t  if (XVEC (x, i) != NULL && XVECLEN (x, i) != 0)\n+\t    {\n+\t      register int j;\n+\n+\t      XVEC (x, i) = gen_rtvec_v (XVECLEN (x, i), &XVECEXP (x, i, 0));\n+\t      for (j = 0; j < XVECLEN (x, i); j++)\n+\t\tXVECEXP (x, i, j)\n+\t\t  = copy_for_inline (XVECEXP (x, i, j));\n+\t    }\n+\t  break;\n+\t}\n+    }\n+\n+  if (code == ASM_OPERANDS && orig_asm_operands_vector == 0)\n+    {\n+      orig_asm_operands_vector = XVEC (orig, 3);\n+      copy_asm_operands_vector = XVEC (x, 3);\n+      copy_asm_constraints_vector = XVEC (x, 4);\n+    }\n+\n+  return x;\n+}\n+\n+/* Unfortunately, we need a global copy of const_equiv map for communication\n+   with a function called from note_stores.  Be *very* careful that this\n+   is used properly in the presence of recursion.  */\n+\n+rtx *global_const_equiv_map;\n+\f\n+#define FIXED_BASE_PLUS_P(X) \\\n+  (GET_CODE (X) == PLUS && GET_CODE (XEXP (X, 1)) == CONST_INT\t\\\n+   && GET_CODE (XEXP (X, 0)) == REG\t\t\t\t\\\n+   && REGNO (XEXP (X, 0)) >= FIRST_VIRTUAL_REGISTER\t\t\\\n+   && REGNO (XEXP (X, 0)) < LAST_VIRTUAL_REGISTER)\n+\n+/* Integrate the procedure defined by FNDECL.  Note that this function\n+   may wind up calling itself.  Since the static variables are not\n+   reentrant, we do not assign them until after the possibility\n+   or recursion is eliminated.\n+\n+   If IGNORE is nonzero, do not produce a value.\n+   Otherwise store the value in TARGET if it is nonzero and that is convenient.\n+\n+   Value is:\n+   (rtx)-1 if we could not substitute the function\n+   0 if we substituted it and it does not produce a value\n+   else an rtx for where the value is stored.  */\n+\n+rtx\n+expand_inline_function (fndecl, parms, target, ignore, type, structure_value_addr)\n+     tree fndecl, parms;\n+     rtx target;\n+     int ignore;\n+     tree type;\n+     rtx structure_value_addr;\n+{\n+  tree formal, actual;\n+  rtx header = DECL_SAVED_INSNS (fndecl);\n+  rtx insns = FIRST_FUNCTION_INSN (header);\n+  rtx parm_insns = FIRST_PARM_INSN (header);\n+  tree *arg_trees;\n+  rtx *arg_vals;\n+  rtx insn;\n+  int max_regno;\n+  int equiv_map_size;\n+  register int i;\n+  int min_labelno = FIRST_LABELNO (header);\n+  int max_labelno = LAST_LABELNO (header);\n+  int nargs;\n+  rtx local_return_label = 0;\n+  rtx loc;\n+  rtx temp;\n+  struct inline_remap *map;\n+  rtx cc0_insn = 0;\n+  rtvec arg_vector = ORIGINAL_ARG_VECTOR (header);\n+\n+  /* Allow for equivalences of the pseudos we make for virtual fp and ap.  */\n+  max_regno = MAX_REGNUM (header) + 3;\n+  if (max_regno < FIRST_PSEUDO_REGISTER)\n+    abort ();\n+\n+  nargs = list_length (DECL_ARGUMENTS (fndecl));\n+\n+  /* We expect PARMS to have the right length; don't crash if not.  */\n+  if (list_length (parms) != nargs)\n+    return (rtx)-1;\n+  /* Also check that the parms type match.  Since the appropriate\n+     conversions or default promotions have already been applied,\n+     the machine modes should match exactly.  */\n+  for (formal = DECL_ARGUMENTS (fndecl),\n+       actual = parms;\n+       formal;\n+       formal = TREE_CHAIN (formal),\n+       actual = TREE_CHAIN (actual))\n+    {\n+      tree arg = TREE_VALUE (actual);\n+      enum machine_mode mode = TYPE_MODE (DECL_ARG_TYPE (formal));\n+      if (mode != TYPE_MODE (TREE_TYPE (arg)))\n+\treturn (rtx)-1;\n+      /* If they are block mode, the types should match exactly.\n+         They don't match exactly if TREE_TYPE (FORMAL) == ERROR_MARK_NODE,\n+\t which could happen if the parameter has incomplete type.  */\n+      if (mode == BLKmode && TREE_TYPE (arg) != TREE_TYPE (formal))\n+\treturn (rtx)-1;\n+    }\n+\n+  /* Make a binding contour to keep inline cleanups called at\n+     outer function-scope level from looking like they are shadowing\n+     parameter declarations.  */\n+  pushlevel (0);\n+\n+  /* Make a fresh binding contour that we can easily remove.  */\n+  pushlevel (0);\n+  expand_start_bindings (0);\n+  if (GET_CODE (parm_insns) == NOTE\n+      && NOTE_LINE_NUMBER (parm_insns) > 0)\n+    emit_note (NOTE_SOURCE_FILE (parm_insns), NOTE_LINE_NUMBER (parm_insns));\n+\n+  /* Expand the function arguments.  Do this first so that any\n+     new registers get created before we allocate the maps.  */\n+\n+  arg_vals = (rtx *) alloca (nargs * sizeof (rtx));\n+  arg_trees = (tree *) alloca (nargs * sizeof (tree));\n+\n+  for (formal = DECL_ARGUMENTS (fndecl), actual = parms, i = 0;\n+       formal;\n+       formal = TREE_CHAIN (formal), actual = TREE_CHAIN (actual), i++)\n+    {\n+      /* Actual parameter, converted to the type of the argument within the\n+\t function.  */\n+      tree arg = convert (TREE_TYPE (formal), TREE_VALUE (actual));\n+      /* Mode of the variable used within the function.  */\n+      enum machine_mode mode = TYPE_MODE (TREE_TYPE (formal));\n+      /* Where parameter is located in the function.  */\n+      rtx copy;\n+\n+      emit_note (DECL_SOURCE_FILE (formal), DECL_SOURCE_LINE (formal));\n+\n+      arg_trees[i] = arg;\n+      loc = RTVEC_ELT (arg_vector, i);\n+\n+      /* If this is an object passed by invisible reference, we copy the\n+\t object into a stack slot and save its address.  If this will go\n+\t into memory, we do nothing now.  Otherwise, we just expand the\n+\t argument.  */\n+      if (GET_CODE (loc) == MEM && GET_CODE (XEXP (loc, 0)) == REG\n+\t  && REGNO (XEXP (loc, 0)) > LAST_VIRTUAL_REGISTER)\n+\t{\n+\t  enum machine_mode mode = TYPE_MODE (TREE_TYPE (arg));\n+\t  rtx stack_slot = assign_stack_temp (mode, int_size_in_bytes (TREE_TYPE (arg)), 1);\n+\n+\t  store_expr (arg, stack_slot, 0);\n+\n+\t  arg_vals[i] = XEXP (stack_slot, 0);\n+\t}\n+      else if (GET_CODE (loc) != MEM)\n+\targ_vals[i] = expand_expr (arg, 0, mode, EXPAND_SUM);\n+      else\n+\targ_vals[i] = 0;\n+\n+      if (arg_vals[i] != 0\n+\t  && (! TREE_READONLY (formal)\n+\t      /* If the parameter is not read-only, copy our argument through\n+\t\t a register.  Also, we cannot use ARG_VALS[I] if it overlaps\n+\t\t TARGET in any way.  In the inline function, they will likely\n+\t\t be two different pseudos, and `safe_from_p' will make all\n+\t\t sorts of smart assumptions about their not conflicting.\n+\t\t But if ARG_VALS[I] overlaps TARGET, these assumptions are\n+\t\t wrong, so put ARG_VALS[I] into a fresh register.  */\n+\t      || (target != 0\n+\t\t  && (GET_CODE (arg_vals[i]) == REG\n+\t\t      || GET_CODE (arg_vals[i]) == SUBREG\n+\t\t      || GET_CODE (arg_vals[i]) == MEM)\n+\t\t  && reg_overlap_mentioned_p (arg_vals[i], target))))\n+\targ_vals[i] = copy_to_mode_reg (mode, arg_vals[i]);\n+    }\n+\t\n+  /* Allocate the structures we use to remap things.  */\n+\n+  map = (struct inline_remap *) alloca (sizeof (struct inline_remap));\n+  map->fndecl = fndecl;\n+\n+  map->reg_map = (rtx *) alloca (max_regno * sizeof (rtx));\n+  bzero (map->reg_map, max_regno * sizeof (rtx));\n+\n+  map->label_map = (rtx *)alloca ((max_labelno - min_labelno) * sizeof (rtx));\n+  map->label_map -= min_labelno;\n+\n+  map->insn_map = (rtx *) alloca (INSN_UID (header) * sizeof (rtx));\n+  bzero (map->insn_map, INSN_UID (header) * sizeof (rtx));\n+  map->min_insnno = 0;\n+  map->max_insnno = INSN_UID (header);\n+\n+  /* const_equiv_map maps pseudos in our routine to constants, so it needs to\n+     be large enough for all our pseudos.  This is the number we are currently\n+     using plus the number in the called routine, plus one for each arg and\n+     one for the return value.  */\n+  equiv_map_size\n+    = max_reg_num () + (max_regno - FIRST_PSEUDO_REGISTER) + nargs + 1;\n+\n+  map->const_equiv_map = (rtx *)alloca (equiv_map_size * sizeof (rtx));\n+  bzero (map->const_equiv_map, equiv_map_size * sizeof (rtx));\n+\n+  map->const_age_map = (unsigned *)alloca (equiv_map_size * sizeof (unsigned));\n+  bzero (map->const_age_map, equiv_map_size * sizeof (unsigned));\n+  map->const_age = 0;\n+\n+  /* Record the current insn in case we have to set up pointers to frame\n+     and argument memory blocks.  */\n+  map->insns_at_start = get_last_insn ();\n+\n+  /* Update the outgoing argument size to allow for those in the inlined\n+     function.  */\n+  if (OUTGOING_ARGS_SIZE (header) > current_function_outgoing_args_size)\n+    current_function_outgoing_args_size = OUTGOING_ARGS_SIZE (header);\n+\n+  /* If the inline function needs to make PIC references, that means\n+     that this function's PIC offset table must be used.  */\n+  if (FUNCTION_FLAGS (header) & FUNCTION_FLAGS_USES_PIC_OFFSET_TABLE)\n+    current_function_uses_pic_offset_table = 1;\n+\n+  /* Process each argument.  For each, set up things so that the function's\n+     reference to the argument will refer to the argument being passed.\n+     We only replace REG with REG here.  Any simplifications are done\n+     via const_equiv_map.\n+\n+     We make two passes:  In the first, we deal with parameters that will\n+     be placed into registers, since we need to ensure that the allocated\n+     register number fits in const_equiv_map.  Then we store all non-register\n+     parameters into their memory location.  */\n+\n+  for (i = 0; i < nargs; i++)\n+    {\n+      rtx copy = arg_vals[i];\n+\n+      loc = RTVEC_ELT (arg_vector, i);\n+\n+      /* There are three cases, each handled separately.  */\n+      if (GET_CODE (loc) == MEM && GET_CODE (XEXP (loc, 0)) == REG\n+\t  && REGNO (XEXP (loc, 0)) > LAST_VIRTUAL_REGISTER)\n+\t{\n+\t  /* This must be an object passed by invisible reference (it could\n+\t     also be a variable-sized object, but we forbid inlining functions\n+\t     with variable-sized arguments).  COPY is the address of the\n+\t     actual value (this computation will cause it to be copied).  We\n+\t     map that address for the register, noting the actual address as\n+\t     an equivalent in case it can be substituted into the insns.  */\n+\n+\t  if (GET_CODE (copy) != REG)\n+\t    {\n+\t      temp = copy_addr_to_reg (copy);\n+\t      if (CONSTANT_P (copy) || FIXED_BASE_PLUS_P (copy))\n+\t\t{\n+\t\t  map->const_equiv_map[REGNO (temp)] = copy;\n+\t\t  map->const_age_map[REGNO (temp)] = CONST_AGE_PARM;\n+\t\t}\n+\t      copy = temp;\n+\t    }\n+\t  map->reg_map[REGNO (XEXP (loc, 0))] = copy;\n+\t}\n+      else if (GET_CODE (loc) == MEM)\n+\t{\n+\t  /* This is the case of a parameter that lives in memory.\n+\t     It will live in the block we allocate in the called routine's\n+\t     frame that simulates the incoming argument area.  Do nothing\n+\t     now; we will call store_expr later.  */\n+\t  ;\n+\t}\n+      else if (GET_CODE (loc) == REG)\n+\t{\n+\t  /* This is the good case where the parameter is in a register.\n+\t     If it is read-only and our argument is a constant, set up the\n+\t     constant equivalence.  */\n+\t  if (GET_CODE (copy) != REG && GET_CODE (copy) != SUBREG)\n+\t    {\n+\t      temp = copy_to_mode_reg (GET_MODE (loc), copy);\n+\t      if (CONSTANT_P (copy) || FIXED_BASE_PLUS_P (copy))\n+\t\t{\n+\t\t  map->const_equiv_map[REGNO (temp)] = copy;\n+\t\t  map->const_age_map[REGNO (temp)] = CONST_AGE_PARM;\n+\t\t}\n+\t      copy = temp;\n+\t    }\n+\t  map->reg_map[REGNO (loc)] = copy;\n+\t}\n+      else\n+\tabort ();\n+\n+      /* Free any temporaries we made setting up this parameter.  */\n+      free_temp_slots ();\n+    }\n+\n+  /* Now do the parameters that will be placed in memory.  */\n+\n+  for (formal = DECL_ARGUMENTS (fndecl), i = 0;\n+       formal; formal = TREE_CHAIN (formal), i++)\n+    {\n+      rtx copy = arg_vals[i];\n+\n+      loc = RTVEC_ELT (arg_vector, i);\n+\n+      if (GET_CODE (loc) == MEM\n+\t  /* Exclude case handled above.  */\n+\t  && ! (GET_CODE (XEXP (loc, 0)) == REG\n+\t\t&& REGNO (XEXP (loc, 0)) > LAST_VIRTUAL_REGISTER))\n+\t{\n+\t  emit_note (DECL_SOURCE_FILE (formal), DECL_SOURCE_LINE (formal));\n+\n+\t  /* Compute the address in the area we reserved and store the\n+\t     value there.  */\n+\t  temp = copy_rtx_and_substitute (loc, map);\n+\t  subst_constants (&temp, 0, map);\n+\t  apply_change_group ();\n+\t  if (! memory_address_p (GET_MODE (temp), XEXP (temp, 0)))\n+\t    temp = change_address (temp, VOIDmode, XEXP (temp, 0));\n+\t  store_expr (arg_trees[i], temp, 0);\n+\n+\t  /* Free any temporaries we made setting up this parameter.  */\n+\t  free_temp_slots ();\n+\t}\n+    }\n+\n+  /* Deal with the places that the function puts its result.\n+     We are driven by what is placed into DECL_RESULT.\n+\n+     Initially, we assume that we don't have anything special handling for\n+     REG_FUNCTION_RETURN_VALUE_P.  */\n+\n+  map->inline_target = 0;\n+  loc = DECL_RTL (DECL_RESULT (fndecl));\n+  if (TYPE_MODE (type) == VOIDmode)\n+    /* There is no return value to worry about.  */\n+    ;\n+  else if (GET_CODE (loc) == MEM)\n+    {\n+      if (! structure_value_addr || ! aggregate_value_p (DECL_RESULT (fndecl)))\n+\tabort ();\n+  \n+      /* Pass the function the address in which to return a structure value.\n+\t Note that a constructor can cause someone to call us with\n+\t STRUCTURE_VALUE_ADDR, but the initialization takes place\n+\t via the first parameter, rather than the struct return address.\n+\n+\t We have two cases:  If the address is a simple register indirect,\n+\t use the mapping mechanism to point that register to our structure\n+\t return address.  Otherwise, store the structure return value into\n+\t the place that it will be referenced from.  */\n+\n+      if (GET_CODE (XEXP (loc, 0)) == REG)\n+\t{\n+\t  temp = force_reg (Pmode, structure_value_addr);\n+\t  map->reg_map[REGNO (XEXP (loc, 0))] = temp;\n+\t  if (CONSTANT_P (structure_value_addr)\n+\t      || (GET_CODE (structure_value_addr) == PLUS\n+\t\t  && XEXP (structure_value_addr, 0) == virtual_stack_vars_rtx\n+\t\t  && GET_CODE (XEXP (structure_value_addr, 1)) == CONST_INT))\n+\t    {\n+\t      map->const_equiv_map[REGNO (temp)] = structure_value_addr;\n+\t      map->const_age_map[REGNO (temp)] = CONST_AGE_PARM;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  temp = copy_rtx_and_substitute (loc, map);\n+\t  subst_constants (&temp, 0, map);\n+\t  apply_change_group ();\n+\t  emit_move_insn (temp, structure_value_addr);\n+\t}\n+    }\n+  else if (ignore)\n+    /* We will ignore the result value, so don't look at its structure.\n+       Note that preparations for an aggregate return value\n+       do need to be made (above) even if it will be ignored.  */\n+    ;\n+  else if (GET_CODE (loc) == REG)\n+    {\n+      /* The function returns an object in a register and we use the return\n+\t value.  Set up our target for remapping.  */\n+\n+      /* Machine mode function was declared to return.   */\n+      enum machine_mode departing_mode = TYPE_MODE (type);\n+      /* (Possibly wider) machine mode it actually computes\n+\t (for the sake of callers that fail to declare it right).  */\n+      enum machine_mode arriving_mode\n+\t= TYPE_MODE (TREE_TYPE (DECL_RESULT (fndecl)));\n+      rtx reg_to_map;\n+\n+      /* Don't use MEMs as direct targets because on some machines\n+\t substituting a MEM for a REG makes invalid insns.\n+\t Let the combiner substitute the MEM if that is valid.  */\n+      if (target == 0 || GET_CODE (target) != REG\n+\t  || GET_MODE (target) != departing_mode)\n+\ttarget = gen_reg_rtx (departing_mode);\n+\n+      /* If function's value was promoted before return,\n+\t avoid machine mode mismatch when we substitute INLINE_TARGET.\n+\t But TARGET is what we will return to the caller.  */\n+      if (arriving_mode != departing_mode)\n+\treg_to_map = gen_rtx (SUBREG, arriving_mode, target, 0);\n+      else\n+\treg_to_map = target;\n+\n+      /* Usually, the result value is the machine's return register.\n+\t Sometimes it may be a pseudo. Handle both cases.  */\n+      if (REG_FUNCTION_VALUE_P (loc))\n+\tmap->inline_target = reg_to_map;\n+      else\n+\tmap->reg_map[REGNO (loc)] = reg_to_map;\n+    }\n+\n+  /* Make new label equivalences for the labels in the called function.  */\n+  for (i = min_labelno; i < max_labelno; i++)\n+    map->label_map[i] = gen_label_rtx ();\n+\n+  /* Perform postincrements before actually calling the function.  */\n+  emit_queue ();\n+\n+  /* Clean up stack so that variables might have smaller offsets.  */\n+  do_pending_stack_adjust ();\n+\n+  /* Save a copy of the location of const_equiv_map for mark_stores, called\n+     via note_stores.  */\n+  global_const_equiv_map = map->const_equiv_map;\n+\n+  /* Now copy the insns one by one.  Do this in two passes, first the insns and\n+     then their REG_NOTES, just like save_for_inline.  */\n+\n+  /* This loop is very similar to the loop in copy_loop_body in unroll.c.  */\n+\n+  for (insn = insns; insn; insn = NEXT_INSN (insn))\n+    {\n+      rtx copy, pattern;\n+\n+      map->orig_asm_operands_vector = 0;\n+\n+      switch (GET_CODE (insn))\n+\t{\n+\tcase INSN:\n+\t  pattern = PATTERN (insn);\n+\t  copy = 0;\n+\t  if (GET_CODE (pattern) == USE\n+\t      && GET_CODE (XEXP (pattern, 0)) == REG\n+\t      && REG_FUNCTION_VALUE_P (XEXP (pattern, 0)))\n+\t    /* The (USE (REG n)) at return from the function should\n+\t       be ignored since we are changing (REG n) into\n+\t       inline_target.  */\n+\t    break;\n+\n+\t  /* Ignore setting a function value that we don't want to use.  */\n+\t  if (map->inline_target == 0\n+\t      && GET_CODE (pattern) == SET\n+\t      && GET_CODE (SET_DEST (pattern)) == REG\n+\t      && REG_FUNCTION_VALUE_P (SET_DEST (pattern)))\n+\t    break;\n+\n+\t  copy = emit_insn (copy_rtx_and_substitute (pattern, map));\n+\t  /* REG_NOTES will be copied later.  */\n+\n+#ifdef HAVE_cc0\n+\t  /* If this insn is setting CC0, it may need to look at\n+\t     the insn that uses CC0 to see what type of insn it is.\n+\t     In that case, the call to recog via validate_change will\n+\t     fail.  So don't substitute constants here.  Instead,\n+\t     do it when we emit the following insn.\n+\n+\t     For example, see the pyr.md file.  That machine has signed and\n+\t     unsigned compares.  The compare patterns must check the\n+\t     following branch insn to see which what kind of compare to\n+\t     emit.\n+\n+\t     If the previous insn set CC0, substitute constants on it as\n+\t     well.  */\n+\t  if (sets_cc0_p (PATTERN (copy)) != 0)\n+\t    cc0_insn = copy;\n+\t  else\n+\t    {\n+\t      if (cc0_insn)\n+\t\ttry_constants (cc0_insn, map);\n+\t      cc0_insn = 0;\n+\t      try_constants (copy, map);\n+\t    }\n+#else\n+\t  try_constants (copy, map);\n+#endif\n+\t  break;\n+\n+\tcase JUMP_INSN:\n+\t  if (GET_CODE (PATTERN (insn)) == RETURN)\n+\t    {\n+\t      if (local_return_label == 0)\n+\t\tlocal_return_label = gen_label_rtx ();\n+\t      pattern = gen_jump (local_return_label);\n+\t    }\n+\t  else\n+\t    pattern = copy_rtx_and_substitute (PATTERN (insn), map);\n+\n+\t  copy = emit_jump_insn (pattern);\n+\n+#ifdef HAVE_cc0\n+\t  if (cc0_insn)\n+\t    try_constants (cc0_insn, map);\n+\t  cc0_insn = 0;\n+#endif\n+\t  try_constants (copy, map);\n+\n+\t  /* If this used to be a conditional jump insn but whose branch\n+\t     direction is now know, we must do something special.  */\n+\t  if (condjump_p (insn) && ! simplejump_p (insn) && map->last_pc_value)\n+\t    {\n+#ifdef HAVE_cc0\n+\t      /* The previous insn set cc0 for us.  So delete it.  */\n+\t      delete_insn (PREV_INSN (copy));\n+#endif\n+\n+\t      /* If this is now a no-op, delete it.  */\n+\t      if (map->last_pc_value == pc_rtx)\n+\t\t{\n+\t\t  delete_insn (copy);\n+\t\t  copy = 0;\n+\t\t}\n+\t      else\n+\t\t/* Otherwise, this is unconditional jump so we must put a\n+\t\t   BARRIER after it.  We could do some dead code elimination\n+\t\t   here, but jump.c will do it just as well.  */\n+\t\temit_barrier ();\n+\t    }\n+\t  break;\n+\n+\tcase CALL_INSN:\n+\t  pattern = copy_rtx_and_substitute (PATTERN (insn), map);\n+\t  copy = emit_call_insn (pattern);\n+\n+#ifdef HAVE_cc0\n+\t  if (cc0_insn)\n+\t    try_constants (cc0_insn, map);\n+\t  cc0_insn = 0;\n+#endif\n+\t  try_constants (copy, map);\n+\n+\t  /* Be lazy and assume CALL_INSNs clobber all hard registers.  */\n+\t  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+\t    map->const_equiv_map[i] = 0;\n+\t  break;\n+\n+\tcase CODE_LABEL:\n+\t  copy = emit_label (map->label_map[CODE_LABEL_NUMBER (insn)]);\n+\t  map->const_age++;\n+\t  break;\n+\n+\tcase BARRIER:\n+\t  copy = emit_barrier ();\n+\t  break;\n+\n+\tcase NOTE:\n+\t  /* It is important to discard function-end and function-beg notes,\n+\t     so we have only one of each in the current function.\n+\t     Also, NOTE_INSN_DELETED notes aren't useful (save_for_inline\n+\t     deleted these in the copy used for continuing compilation,\n+\t     not the copy used for inlining).  */\n+\t  if (NOTE_LINE_NUMBER (insn) != NOTE_INSN_FUNCTION_END\n+\t      && NOTE_LINE_NUMBER (insn) != NOTE_INSN_FUNCTION_BEG\n+\t      && NOTE_LINE_NUMBER (insn) != NOTE_INSN_DELETED)\n+\t    copy = emit_note (NOTE_SOURCE_FILE (insn), NOTE_LINE_NUMBER (insn));\n+\t  else\n+\t    copy = 0;\n+\t  break;\n+\n+\tdefault:\n+\t  abort ();\n+\t  break;\n+\t}\n+\n+      if (copy)\n+\tRTX_INTEGRATED_P (copy) = 1;\n+\n+      map->insn_map[INSN_UID (insn)] = copy;\n+    }\n+\n+  /* Now copy the REG_NOTES.  */\n+  for (insn = insns; insn; insn = NEXT_INSN (insn))\n+    if (GET_RTX_CLASS (GET_CODE (insn)) == 'i'\n+\t&& map->insn_map[INSN_UID (insn)])\n+      REG_NOTES (map->insn_map[INSN_UID (insn)])\n+\t= copy_rtx_and_substitute (REG_NOTES (insn), map);\n+\n+  if (local_return_label)\n+    emit_label (local_return_label);\n+\n+  /* Make copies of the decls of the symbols in the inline function, so that\n+     the copies of the variables get declared in the current function.  Set\n+     up things so that lookup_static_chain knows that to interpret registers\n+     in SAVE_EXPRs for TYPE_SIZEs as local.  */\n+\n+  inline_function_decl = fndecl;\n+  integrate_decl_tree ((tree) ORIGINAL_DECL_INITIAL (header), 0, map, 0);\n+  integrate_parm_decls (DECL_ARGUMENTS (fndecl), map, arg_vector);\n+  inline_function_decl = 0;\n+\n+  /* End the scope containing the copied formal parameter variables.  */\n+\n+  expand_end_bindings (getdecls (), 1, 1);\n+  poplevel (1, 1, 0);\n+  poplevel (0, 0, 0);\n+  emit_line_note (input_filename, lineno);\n+\n+  if (structure_value_addr)\n+    return gen_rtx (MEM, TYPE_MODE (type),\n+\t\t    memory_address (TYPE_MODE (type), structure_value_addr));\n+  return target;\n+}\n+\f\n+/* Given a chain of PARM_DECLs, ARGS, copy each decl into a VAR_DECL,\n+   push all of those decls and give each one the corresponding home.  */\n+\n+static void\n+integrate_parm_decls (args, map, arg_vector)\n+     tree args;\n+     struct inline_remap *map;\n+     rtvec arg_vector;\n+{\n+  register tree tail;\n+  register int i;\n+\n+  for (tail = args, i = 0; tail; tail = TREE_CHAIN (tail), i++)\n+    {\n+      register tree decl = build_decl (VAR_DECL, DECL_NAME (tail),\n+\t\t\t\t       TREE_TYPE (tail));\n+      rtx new_decl_rtl\n+\t= copy_rtx_and_substitute (RTVEC_ELT (arg_vector, i), map);\n+\n+      /* These args would always appear unused, if not for this.  */\n+      TREE_USED (decl) = 1;\n+      /* Prevent warning for shadowing with these.  */\n+      DECL_FROM_INLINE (decl) = 1;\n+      pushdecl (decl);\n+      /* Fully instantiate the address with the equivalent form so that the\n+\t debugging information contains the actual register, instead of the\n+\t virtual register.   Do this by not passing an insn to\n+\t subst_constants.  */\n+      subst_constants (&new_decl_rtl, 0, map);\n+      apply_change_group ();\n+      DECL_RTL (decl) = new_decl_rtl;\n+    }\n+}\n+\n+/* Given a BLOCK node LET, push decls and levels so as to construct in the\n+   current function a tree of contexts isomorphic to the one that is given.\n+\n+   LEVEL indicates how far down into the BLOCK tree is the node we are\n+   currently traversing.  It is always zero for the initial call.\n+\n+   MAP, if nonzero, is a pointer to a inline_remap map which indicates how\n+   registers used in the DECL_RTL field should be remapped.  If it is zero,\n+   no mapping is necessary.\n+\n+   FUNCTIONBODY indicates whether the top level block tree corresponds to\n+   a function body.  This is identical in meaning to the functionbody\n+   argument of poplevel.  */\n+\n+static void\n+integrate_decl_tree (let, level, map, functionbody)\n+     tree let;\n+     int level;\n+     struct inline_remap *map;\n+     int functionbody;\n+{\n+  tree t, node;\n+\n+  pushlevel (0);\n+  \n+  for (t = BLOCK_VARS (let); t; t = TREE_CHAIN (t))\n+    {\n+      tree d = build_decl (TREE_CODE (t), DECL_NAME (t), TREE_TYPE (t));\n+      DECL_SOURCE_LINE (d) = DECL_SOURCE_LINE (t);\n+      DECL_SOURCE_FILE (d) = DECL_SOURCE_FILE (t);\n+      if (! functionbody && DECL_RTL (t) != 0)\n+\t{\n+\t  DECL_RTL (d) = copy_rtx_and_substitute (DECL_RTL (t), map);\n+\t  /* Fully instantiate the address with the equivalent form so that the\n+\t     debugging information contains the actual register, instead of the\n+\t     virtual register.   Do this by not passing an insn to\n+\t     subst_constants.  */\n+\t  subst_constants (&DECL_RTL (d), 0, map);\n+\t  apply_change_group ();\n+\t}\n+      else if (DECL_RTL (t))\n+\tDECL_RTL (d) = copy_rtx (DECL_RTL (t));\n+      TREE_EXTERNAL (d) = TREE_EXTERNAL (t);\n+      TREE_STATIC (d) = TREE_STATIC (t);\n+      TREE_PUBLIC (d) = TREE_PUBLIC (t);\n+      TREE_CONSTANT (d) = TREE_CONSTANT (t);\n+      TREE_ADDRESSABLE (d) = TREE_ADDRESSABLE (t);\n+      TREE_READONLY (d) = TREE_READONLY (t);\n+      TREE_SIDE_EFFECTS (d) = TREE_SIDE_EFFECTS (t);\n+      /* These args would always appear unused, if not for this.  */\n+      TREE_USED (d) = 1;\n+      /* Prevent warning for shadowing with these.  */\n+      DECL_FROM_INLINE (d) = 1;\n+      pushdecl (d);\n+    }\n+\n+  for (t = BLOCK_SUBBLOCKS (let); t; t = TREE_CHAIN (t))\n+    integrate_decl_tree (t, level + 1, map, functionbody);\n+\n+  node = poplevel (level > 0, 0, level == 0 && functionbody);\n+  if (node)\n+    TREE_USED (node) = TREE_USED (let);\n+}\n+\f\n+/* Create a new copy of an rtx.\n+   Recursively copies the operands of the rtx,\n+   except for those few rtx codes that are sharable.\n+\n+   We always return an rtx that is similar to that incoming rtx, with the\n+   exception of possibly changing a REG to a SUBREG or vice versa.  No\n+   rtl is ever emitted.\n+\n+   Handle constants that need to be placed in the constant pool by\n+   calling `force_const_mem'.  */\n+\n+rtx\n+copy_rtx_and_substitute (orig, map)\n+     register rtx orig;\n+     struct inline_remap *map;\n+{\n+  register rtx copy, temp;\n+  register int i, j;\n+  register RTX_CODE code;\n+  register enum machine_mode mode;\n+  register char *format_ptr;\n+  int regno;\n+\n+  if (orig == 0)\n+    return 0;\n+\n+  code = GET_CODE (orig);\n+  mode = GET_MODE (orig);\n+\n+  switch (code)\n+    {\n+    case REG:\n+      /* If the stack pointer register shows up, it must be part of\n+\t stack-adjustments (*not* because we eliminated the frame pointer!).\n+\t Small hard registers are returned as-is.  Pseudo-registers\n+\t go through their `reg_map'.  */\n+      regno = REGNO (orig);\n+      if (regno <= LAST_VIRTUAL_REGISTER)\n+\t{\n+\t  /* Some hard registers are also mapped,\n+\t     but others are not translated.  */\n+\t  if (map->reg_map[regno] != 0)\n+\t    return map->reg_map[regno];\n+\n+\t  /* If this is the virtual frame pointer, make space in current\n+\t     function's stack frame for the stack frame of the inline function.\n+\n+\t     Copy the address of this area into a pseudo.  Map\n+\t     virtual_stack_vars_rtx to this pseudo and set up a constant\n+\t     equivalence for it to be the address.  This will substitute the\n+\t     address into insns where it can be substituted and use the new\n+\t     pseudo where it can't.  */\n+\t  if (regno == VIRTUAL_STACK_VARS_REGNUM)\n+\t    {\n+\t      rtx loc, seq;\n+\t      int size = DECL_FRAME_SIZE (map->fndecl);\n+\t      int rounded;\n+\n+\t      start_sequence ();\n+\t      loc = assign_stack_temp (BLKmode, size, 1);\n+\t      loc = XEXP (loc, 0);\n+#ifdef FRAME_GROWS_DOWNWARD\n+\t      /* In this case, virtual_stack_vars_rtx points to one byte\n+\t\t higher than the top of the frame area.  So compute the offset\n+\t\t to one byte higher than our substitute frame.\n+\t\t Keep the fake frame pointer aligned like a real one.  */\n+\t      rounded = CEIL_ROUND (size, BIGGEST_ALIGNMENT / BITS_PER_UNIT);\n+\t      loc = plus_constant (loc, rounded);\n+#endif\n+\t      map->reg_map[regno] = force_operand (loc, 0);\n+\t      map->const_equiv_map[regno] = loc;\n+\t      map->const_age_map[regno] = CONST_AGE_PARM;\n+\n+\t      seq = gen_sequence ();\n+\t      end_sequence ();\n+\t      emit_insn_after (seq, map->insns_at_start);\n+\t      return map->reg_map[regno];\n+\t    }\n+\t  else if (regno == VIRTUAL_INCOMING_ARGS_REGNUM)\n+\t    {\n+\t      /* Do the same for a block to contain any arguments referenced\n+\t\t in memory. */\n+\t      rtx loc, seq;\n+\t      int size = FUNCTION_ARGS_SIZE (DECL_SAVED_INSNS (map->fndecl));\n+\n+\t      start_sequence ();\n+\t      loc = assign_stack_temp (BLKmode, size, 1);\n+\t      loc = XEXP (loc, 0);\n+\t      map->reg_map[regno] = force_operand (loc, 0);\n+\t      map->const_equiv_map[regno] = loc;\n+\t      map->const_age_map[regno] = CONST_AGE_PARM;\n+\n+\t      seq = gen_sequence ();\n+\t      end_sequence ();\n+\t      emit_insn_after (seq, map->insns_at_start);\n+\t      return map->reg_map[regno];\n+\t    }\n+\t  else if (REG_FUNCTION_VALUE_P (orig))\n+\t    {\n+\t      /* This is a reference to the function return value.  If\n+\t\t the function doesn't have a return value, error.  If the\n+\t\t mode doesn't agree, make a SUBREG.  */\n+\t      if (map->inline_target == 0)\n+\t\t/* Must be unrolling loops or replicating code if we\n+\t\t   reach here, so return the register unchanged.  */\n+\t\treturn orig;\n+\t      else if (mode != GET_MODE (map->inline_target))\n+\t\treturn gen_rtx (SUBREG, mode, map->inline_target, 0);\n+\t      else\n+\t\treturn map->inline_target;\n+\t    }\n+\t  return orig;\n+\t}\n+      if (map->reg_map[regno] == NULL)\n+\t{\n+\t  map->reg_map[regno] = gen_reg_rtx (mode);\n+\t  REG_USERVAR_P (map->reg_map[regno]) = REG_USERVAR_P (orig);\n+\t  REG_LOOP_TEST_P (map->reg_map[regno]) = REG_LOOP_TEST_P (orig);\n+\t  RTX_UNCHANGING_P (map->reg_map[regno]) = RTX_UNCHANGING_P (orig);\n+\t  /* A reg with REG_FUNCTION_VALUE_P true will never reach here.  */\n+\t}\n+      return map->reg_map[regno];\n+\n+    case SUBREG:\n+      copy = copy_rtx_and_substitute (SUBREG_REG (orig), map);\n+      /* SUBREG is ordinary, but don't make nested SUBREGs.  */\n+      if (GET_CODE (copy) == SUBREG)\n+\treturn gen_rtx (SUBREG, GET_MODE (orig), SUBREG_REG (copy),\n+\t\t\tSUBREG_WORD (orig) + SUBREG_WORD (copy));\n+      else\n+\treturn gen_rtx (SUBREG, GET_MODE (orig), copy,\n+\t\t\tSUBREG_WORD (orig));\n+\n+    case USE:\n+    case CLOBBER:\n+      /* USE and CLOBBER are ordinary, but we convert (use (subreg foo))\n+\t to (use foo).  */\n+      copy = copy_rtx_and_substitute (XEXP (orig, 0), map);\n+      if (GET_CODE (copy) == SUBREG)\n+\tcopy = SUBREG_REG (copy);\n+      return gen_rtx (code, VOIDmode, copy);\n+\n+    case CODE_LABEL:\n+      LABEL_PRESERVE_P (map->label_map[CODE_LABEL_NUMBER (orig)])\n+\t= LABEL_PRESERVE_P (orig);\n+      return map->label_map[CODE_LABEL_NUMBER (orig)];\n+\n+    case LABEL_REF:\n+      copy = rtx_alloc (LABEL_REF);\n+      PUT_MODE (copy, mode);\n+      XEXP (copy, 0) = map->label_map[CODE_LABEL_NUMBER (XEXP (orig, 0))];\n+      LABEL_OUTSIDE_LOOP_P (copy) = LABEL_OUTSIDE_LOOP_P (orig);\n+      return copy;\n+\n+    case PC:\n+    case CC0:\n+    case CONST_INT:\n+    case SYMBOL_REF:\n+      return orig;\n+\n+    case CONST_DOUBLE:\n+      /* We have to make a new copy of this CONST_DOUBLE because don't want\n+\t to use the old value of CONST_DOUBLE_MEM.  Also, this may be a\n+\t duplicate of a CONST_DOUBLE we have already seen.  */\n+      if (GET_MODE_CLASS (GET_MODE (orig)) == MODE_FLOAT)\n+\t{\n+\t  REAL_VALUE_TYPE d;\n+\n+\t  REAL_VALUE_FROM_CONST_DOUBLE (d, orig);\n+\t  return immed_real_const_1 (d, GET_MODE (orig));\n+\t}\n+      else\n+\treturn immed_double_const (CONST_DOUBLE_LOW (orig),\n+\t\t\t\t   CONST_DOUBLE_HIGH (orig), VOIDmode);\n+\n+    case CONST:\n+      /* Make new constant pool entry for a constant\n+\t that was in the pool of the inline function.  */\n+      if (RTX_INTEGRATED_P (orig))\n+\t{\n+\t  /* If this was an address of a constant pool entry that itself\n+\t     had to be placed in the constant pool, it might not be a\n+\t     valid address.  So the recursive call below might turn it\n+\t     into a register.  In that case, it isn't a constant any\n+\t     more, so return it.  This has the potential of changing a\n+\t     MEM into a REG, but we'll assume that it safe.  */\n+\t  temp = copy_rtx_and_substitute (XEXP (orig, 0), map);\n+\t  if (! CONSTANT_P (temp))\n+\t    return temp;\n+\t  return validize_mem (force_const_mem (GET_MODE (orig), temp));\n+\t}\n+      break;\n+\n+    case ADDRESS:\n+      /* If from constant pool address, make new constant pool entry and\n+\t return its address.  */\n+      if (! RTX_INTEGRATED_P (orig))\n+\tabort ();\n+\n+      temp = force_const_mem (GET_MODE (orig),\n+\t\t\t      copy_rtx_and_substitute (XEXP (orig, 0), map));\n+\n+#if 0\n+      /* Legitimizing the address here is incorrect.\n+\n+\t The only ADDRESS rtx's that can reach here are ones created by\n+\t save_constants.  Hence the operand of the ADDRESS is always legal\n+\t in this position of the instruction, since the original rtx without\n+\t the ADDRESS was legal.\n+\n+\t The reason we don't legitimize the address here is that on the\n+\t Sparc, the caller may have a (high ...) surrounding this ADDRESS.\n+\t This code forces the operand of the address to a register, which\n+\t fails because we can not take the HIGH part of a register.\n+\n+\t Also, change_address may create new registers.  These registers\n+\t will not have valid reg_map entries.  This can cause try_constants()\n+\t to fail because assumes that all registers in the rtx have valid\n+\t reg_map entries, and it may end up replacing one of these new\n+\t registers with junk. */\n+\n+      if (! memory_address_p (GET_MODE (temp), XEXP (temp, 0)))\n+\ttemp = change_address (temp, GET_MODE (temp), XEXP (temp, 0));\n+#endif\n+\n+      return XEXP (temp, 0);\n+\n+    case ASM_OPERANDS:\n+      /* If a single asm insn contains multiple output operands\n+\t then it contains multiple ASM_OPERANDS rtx's that share operand 3.\n+\t We must make sure that the copied insn continues to share it.  */\n+      if (map->orig_asm_operands_vector == XVEC (orig, 3))\n+\t{\n+\t  copy = rtx_alloc (ASM_OPERANDS);\n+\t  XSTR (copy, 0) = XSTR (orig, 0);\n+\t  XSTR (copy, 1) = XSTR (orig, 1);\n+\t  XINT (copy, 2) = XINT (orig, 2);\n+\t  XVEC (copy, 3) = map->copy_asm_operands_vector;\n+\t  XVEC (copy, 4) = map->copy_asm_constraints_vector;\n+\t  XSTR (copy, 5) = XSTR (orig, 5);\n+\t  XINT (copy, 6) = XINT (orig, 6);\n+\t  return copy;\n+\t}\n+      break;\n+\n+    case CALL:\n+      /* This is given special treatment because the first\n+\t operand of a CALL is a (MEM ...) which may get\n+\t forced into a register for cse.  This is undesirable\n+\t if function-address cse isn't wanted or if we won't do cse.  */\n+#ifndef NO_FUNCTION_CSE\n+      if (! (optimize && ! flag_no_function_cse))\n+#endif\n+\treturn gen_rtx (CALL, GET_MODE (orig),\n+\t\t\tgen_rtx (MEM, GET_MODE (XEXP (orig, 0)),\n+\t\t\t\t copy_rtx_and_substitute (XEXP (XEXP (orig, 0), 0), map)),\n+\t\t\tcopy_rtx_and_substitute (XEXP (orig, 1), map));\n+      break;\n+\n+#if 0\n+      /* Must be ifdefed out for loop unrolling to work.  */\n+    case RETURN:\n+      abort ();\n+#endif\n+\n+    case SET:\n+      /* If this is setting fp or ap, it means that we have a nonlocal goto.\n+\t Don't alter that.\n+\t If the nonlocal goto is into the current function,\n+\t this will result in unnecessarily bad code, but should work.  */\n+      if (SET_DEST (orig) == virtual_stack_vars_rtx\n+\t  || SET_DEST (orig) == virtual_incoming_args_rtx)\n+\treturn gen_rtx (SET, VOIDmode, SET_DEST (orig),\n+\t\t\tcopy_rtx_and_substitute (SET_SRC (orig), map));\n+      break;\n+\n+    case MEM:\n+      copy = rtx_alloc (MEM);\n+      PUT_MODE (copy, mode);\n+      XEXP (copy, 0) = copy_rtx_and_substitute (XEXP (orig, 0), map);\n+      MEM_IN_STRUCT_P (copy) = MEM_IN_STRUCT_P (orig);\n+      MEM_VOLATILE_P (copy) = MEM_VOLATILE_P (orig);\n+      RTX_UNCHANGING_P (copy) = RTX_UNCHANGING_P (orig);\n+      return copy;\n+    }\n+\n+  copy = rtx_alloc (code);\n+  PUT_MODE (copy, mode);\n+  copy->in_struct = orig->in_struct;\n+  copy->volatil = orig->volatil;\n+  copy->unchanging = orig->unchanging;\n+\n+  format_ptr = GET_RTX_FORMAT (GET_CODE (copy));\n+\n+  for (i = 0; i < GET_RTX_LENGTH (GET_CODE (copy)); i++)\n+    {\n+      switch (*format_ptr++)\n+\t{\n+\tcase '0':\n+\t  break;\n+\n+\tcase 'e':\n+\t  XEXP (copy, i) = copy_rtx_and_substitute (XEXP (orig, i), map);\n+\t  break;\n+\n+\tcase 'u':\n+\t  /* Change any references to old-insns to point to the\n+\t     corresponding copied insns.  */\n+\t  XEXP (copy, i) = map->insn_map[INSN_UID (XEXP (orig, i))];\n+\t  break;\n+\n+\tcase 'E':\n+\t  XVEC (copy, i) = XVEC (orig, i);\n+\t  if (XVEC (orig, i) != NULL && XVECLEN (orig, i) != 0)\n+\t    {\n+\t      XVEC (copy, i) = rtvec_alloc (XVECLEN (orig, i));\n+\t      for (j = 0; j < XVECLEN (copy, i); j++)\n+\t\tXVECEXP (copy, i, j)\n+\t\t  = copy_rtx_and_substitute (XVECEXP (orig, i, j), map);\n+\t    }\n+\t  break;\n+\n+\tcase 'i':\n+\t  XINT (copy, i) = XINT (orig, i);\n+\t  break;\n+\n+\tcase 's':\n+\t  XSTR (copy, i) = XSTR (orig, i);\n+\t  break;\n+\n+\tdefault:\n+\t  abort ();\n+\t}\n+    }\n+\n+  if (code == ASM_OPERANDS && map->orig_asm_operands_vector == 0)\n+    {\n+      map->orig_asm_operands_vector = XVEC (orig, 3);\n+      map->copy_asm_operands_vector = XVEC (copy, 3);\n+      map->copy_asm_constraints_vector = XVEC (copy, 4);\n+    }\n+\n+  return copy;\n+}\n+\f\n+/* Substitute known constant values into INSN, if that is valid.  */\n+\n+void\n+try_constants (insn, map)\n+     rtx insn;\n+     struct inline_remap *map;\n+{\n+  int i;\n+\n+  map->num_sets = 0;\n+  subst_constants (&PATTERN (insn), insn, map);\n+\n+  /* Apply the changes if they are valid; otherwise discard them.  */\n+  apply_change_group ();\n+\n+  /* Show we don't know the value of anything stored or clobbered.  */\n+  note_stores (PATTERN (insn), mark_stores);\n+  map->last_pc_value = 0;\n+#ifdef HAVE_cc0\n+  map->last_cc0_value = 0;\n+#endif\n+\n+  /* Set up any constant equivalences made in this insn.  */\n+  for (i = 0; i < map->num_sets; i++)\n+    {\n+      if (GET_CODE (map->equiv_sets[i].dest) == REG)\n+\t{\n+\t  int regno = REGNO (map->equiv_sets[i].dest);\n+\n+\t  if (map->const_equiv_map[regno] == 0\n+\t      /* Following clause is a hack to make case work where GNU C++\n+\t\t reassigns a variable to make cse work right.  */\n+\t      || ! rtx_equal_p (map->const_equiv_map[regno],\n+\t\t\t\tmap->equiv_sets[i].equiv))\n+\t    {\n+\t      map->const_equiv_map[regno] = map->equiv_sets[i].equiv;\n+\t      map->const_age_map[regno] = map->const_age;\n+\t    }\n+\t}\n+      else if (map->equiv_sets[i].dest == pc_rtx)\n+\tmap->last_pc_value = map->equiv_sets[i].equiv;\n+#ifdef HAVE_cc0\n+      else if (map->equiv_sets[i].dest == cc0_rtx)\n+\tmap->last_cc0_value = map->equiv_sets[i].equiv;\n+#endif\n+    }\n+}\n+\f\n+/* Substitute known constants for pseudo regs in the contents of LOC,\n+   which are part of INSN.\n+   If INSN is zero, the substition should always be done (this is used to\n+   update DECL_RTL).\n+   These changes are taken out by try_constants if the result is not valid.\n+\n+   Note that we are more concerned with determining when the result of a SET\n+   is a constant, for further propagation, than actually inserting constants\n+   into insns; cse will do the latter task better.\n+\n+   This function is also used to adjust address of items previously addressed\n+   via the virtual stack variable or virtual incoming arguments registers.  */\n+\n+static void\n+subst_constants (loc, insn, map)\n+     rtx *loc;\n+     rtx insn;\n+     struct inline_remap *map;\n+{\n+  rtx x = *loc;\n+  register int i;\n+  register enum rtx_code code;\n+  register char *format_ptr;\n+  int num_changes = num_validated_changes ();\n+  rtx new = 0;\n+  enum machine_mode op0_mode;\n+\n+  code = GET_CODE (x);\n+\n+  switch (code)\n+    {\n+    case PC:\n+    case CONST_INT:\n+    case CONST_DOUBLE:\n+    case SYMBOL_REF:\n+    case CONST:\n+    case LABEL_REF:\n+    case ADDRESS:\n+      return;\n+\n+#ifdef HAVE_cc0\n+    case CC0:\n+      validate_change (insn, loc, map->last_cc0_value, 1);\n+      return;\n+#endif\n+\n+    case USE:\n+    case CLOBBER:\n+      /* The only thing we can do with a USE or CLOBBER is possibly do\n+\t some substitutions in a MEM within it.  */\n+      if (GET_CODE (XEXP (x, 0)) == MEM)\n+\tsubst_constants (&XEXP (XEXP (x, 0), 0), insn, map);\n+      return;\n+\n+    case REG:\n+      /* Substitute for parms and known constants.  Don't replace\n+\t hard regs used as user variables with constants.  */\n+      {\n+\tint regno = REGNO (x);\n+\tif (! (regno < FIRST_PSEUDO_REGISTER && REG_USERVAR_P (x))\n+\t    && map->const_equiv_map[regno] != 0\n+\t    && map->const_age_map[regno] >= map->const_age)\n+\t  validate_change (insn, loc, map->const_equiv_map[regno], 1);\n+\treturn;\n+      }\n+\n+    case SUBREG:\n+      /* SUBREG is ordinary, but don't make nested SUBREGs and try to simplify\n+\t constants.  */\n+      {\n+\trtx inner = SUBREG_REG (x);\n+\trtx new = 0;\n+\n+\t/* We can't call subst_constants on &SUBREG_REG (x) because any\n+\t   constant or SUBREG wouldn't be valid inside our SUBEG.  Instead,\n+\t   see what is inside, try to form the new SUBREG and see if that is\n+\t   valid.  We handle two cases: extracting a full word in an \n+\t   integral mode and extracting the low part.  */\n+\tsubst_constants (&inner, 0, map);\n+\n+\tif (GET_MODE_CLASS (GET_MODE (x)) == MODE_INT\n+\t    && GET_MODE_SIZE (GET_MODE (x)) == UNITS_PER_WORD\n+\t    && GET_MODE (SUBREG_REG (x)) != VOIDmode)\n+\t  new = operand_subword (inner, SUBREG_WORD (x), 0,\n+\t\t\t\t GET_MODE (SUBREG_REG (x)));\n+\n+\tif (new == 0 && subreg_lowpart_p (x))\n+\t  new = gen_lowpart_common (GET_MODE (x), inner);\n+\n+\tif (new)\n+\t  validate_change (insn, loc, new, 1);\n+\n+\treturn;\n+      }\n+\n+    case MEM:\n+      subst_constants (&XEXP (x, 0), insn, map);\n+\n+      /* If a memory address got spoiled, change it back.  */\n+      if (insn != 0 && num_validated_changes () != num_changes\n+\t  && !memory_address_p (GET_MODE (x), XEXP (x, 0)))\n+\tcancel_changes (num_changes);\n+      return;\n+\n+    case SET:\n+      {\n+\t/* Substitute constants in our source, and in any arguments to a\n+\t   complex (e..g, ZERO_EXTRACT) destination, but not in the destination\n+\t   itself.  */\n+\trtx *dest_loc = &SET_DEST (x);\n+\trtx dest = *dest_loc;\n+\trtx src, tem;\n+\n+\tsubst_constants (&SET_SRC (x), insn, map);\n+\tsrc = SET_SRC (x);\n+\n+\twhile (GET_CODE (*dest_loc) == ZERO_EXTRACT\n+\t       || GET_CODE (*dest_loc) == SIGN_EXTRACT\n+\t       || GET_CODE (*dest_loc) == SUBREG\n+\t       || GET_CODE (*dest_loc) == STRICT_LOW_PART)\n+\t  {\n+\t    if (GET_CODE (*dest_loc) == ZERO_EXTRACT)\n+\t      {\n+\t\tsubst_constants (&XEXP (*dest_loc, 1), insn, map);\n+\t\tsubst_constants (&XEXP (*dest_loc, 2), insn, map);\n+\t      }\n+\t    dest_loc = &XEXP (*dest_loc, 0);\n+\t  }\n+\n+\t/* Check for the case of DEST a SUBREG, both it and the underlying\n+\t   register are less than one word, and the SUBREG has the wider mode.\n+\t   In the case, we are really setting the underlying register to the\n+\t   source converted to the mode of DEST.  So indicate that.  */\n+\tif (GET_CODE (dest) == SUBREG\n+\t    && GET_MODE_SIZE (GET_MODE (dest)) <= UNITS_PER_WORD\n+\t    && GET_MODE_SIZE (GET_MODE (SUBREG_REG (dest))) <= UNITS_PER_WORD\n+\t    && (GET_MODE_SIZE (GET_MODE (SUBREG_REG (dest)))\n+\t\t      <= GET_MODE_SIZE (GET_MODE (dest)))\n+\t    && (tem = gen_lowpart_if_possible (GET_MODE (dest), src)))\n+\t  src = tem, dest = SUBREG_REG (dest);\n+\n+\t/* If storing a recognizable value save it for later recording.  */\n+\tif ((map->num_sets < MAX_RECOG_OPERANDS)\n+\t    && (CONSTANT_P (src)\n+\t\t|| (GET_CODE (src) == PLUS\n+\t\t    && GET_CODE (XEXP (src, 0)) == REG\n+\t\t    && REGNO (XEXP (src, 0)) >= FIRST_VIRTUAL_REGISTER\n+\t\t    && REGNO (XEXP (src, 0)) <= LAST_VIRTUAL_REGISTER\n+\t\t    && CONSTANT_P (XEXP (src, 1)))\n+\t\t|| GET_CODE (src) == COMPARE\n+#ifdef HAVE_cc0\n+\t\t|| dest == cc0_rtx\n+#endif\n+\t\t|| (dest == pc_rtx\n+\t\t    && (src == pc_rtx || GET_CODE (src) == RETURN\n+\t\t\t|| GET_CODE (src) == LABEL_REF))))\n+\t  {\n+\t    /* Normally, this copy won't do anything.  But, if SRC is a COMPARE\n+\t       it will cause us to save the COMPARE with any constants\n+\t       substituted, which is what we want for later.  */\n+\t    map->equiv_sets[map->num_sets].equiv = copy_rtx (src);\n+\t    map->equiv_sets[map->num_sets++].dest = dest;\n+\t  }\n+\n+\treturn;\n+      }\n+    }\n+\n+  format_ptr = GET_RTX_FORMAT (code);\n+  \n+  /* If the first operand is an expression, save its mode for later.  */\n+  if (*format_ptr == 'e')\n+    op0_mode = GET_MODE (XEXP (x, 0));\n+\n+  for (i = 0; i < GET_RTX_LENGTH (code); i++)\n+    {\n+      switch (*format_ptr++)\n+\t{\n+\tcase '0':\n+\t  break;\n+\n+\tcase 'e':\n+\t  if (XEXP (x, i))\n+\t    subst_constants (&XEXP (x, i), insn, map);\n+\t  break;\n+\n+\tcase 'u':\n+\tcase 'i':\n+\tcase 's':\n+\t  break;\n+\n+\tcase 'E':\n+\t  if (XVEC (x, i) != NULL && XVECLEN (x, i) != 0)\n+\t    {\n+\t      int j;\n+\t      for (j = 0; j < XVECLEN (x, i); j++)\n+\t\tsubst_constants (&XVECEXP (x, i, j), insn, map);\n+\t    }\n+\t  break;\n+\n+\tdefault:\n+\t  abort ();\n+\t}\n+    }\n+\n+  /* If this is a commutative operation, move a constant to the second\n+     operand unless the second operand is already a CONST_INT.  */\n+  if ((GET_RTX_CLASS (code) == 'c' || code == NE || code == EQ)\n+      && CONSTANT_P (XEXP (x, 0)) && GET_CODE (XEXP (x, 1)) != CONST_INT)\n+    {\n+      rtx tem = XEXP (x, 0);\n+      validate_change (insn, &XEXP (x, 0), XEXP (x, 1), 1);\n+      validate_change (insn, &XEXP (x, 1), tem, 1);\n+    }\n+\n+  /* Simplify the expression in case we put in some constants.  */\n+  switch (GET_RTX_CLASS (code))\n+    {\n+    case '1':\n+      new = simplify_unary_operation (code, GET_MODE (x),\n+\t\t\t\t      XEXP (x, 0), op0_mode);\n+      break;\n+\n+    case '<':\n+      {\n+\tenum machine_mode op_mode = GET_MODE (XEXP (x, 0));\n+\tif (op_mode == VOIDmode)\n+\t  op_mode = GET_MODE (XEXP (x, 1));\n+\tnew = simplify_relational_operation (code, op_mode,\n+\t\t\t\t\t     XEXP (x, 0), XEXP (x, 1));\n+\tbreak;\n+      }\n+\n+    case '2':\n+    case 'c':\n+      new = simplify_binary_operation (code, GET_MODE (x),\n+\t\t\t\t       XEXP (x, 0), XEXP (x, 1));\n+      break;\n+\n+    case 'b':\n+    case '3':\n+      new = simplify_ternary_operation (code, GET_MODE (x), op0_mode,\n+\t\t\t\t\tXEXP (x, 0), XEXP (x, 1), XEXP (x, 2));\n+      break;\n+    }\n+\n+  if (new)\n+    validate_change (insn, loc, new, 1);\n+}\n+\n+/* Show that register modified no longer contain known constants.  We are\n+   called from note_stores with parts of the new insn.  */\n+\n+void\n+mark_stores (dest, x)\n+     rtx dest;\n+     rtx x;\n+{\n+  if (GET_CODE (dest) == SUBREG)\n+    dest = SUBREG_REG (dest);\n+\n+  if (GET_CODE (dest) == REG)\n+    global_const_equiv_map[REGNO (dest)] = 0;\n+}\n+\f\n+/* If any CONST expressions with RTX_INTEGRATED_P are present in the rtx\n+   pointed to by PX, they represent constants in the constant pool.\n+   Replace these with a new memory reference obtained from force_const_mem.\n+   Similarly, ADDRESS expressions with RTX_INTEGRATED_P represent the\n+   address of a constant pool entry.  Replace them with the address of\n+   a new constant pool entry obtained from force_const_mem.  */\n+\n+static void\n+restore_constants (px)\n+     rtx *px;\n+{\n+  rtx x = *px;\n+  int i, j;\n+  char *fmt;\n+\n+  if (x == 0)\n+    return;\n+\n+  if (GET_CODE (x) == CONST_DOUBLE)\n+    {\n+      /* We have to make a new CONST_DOUBLE to ensure that we account for\n+\t it correctly.  Using the old CONST_DOUBLE_MEM data is wrong.  */\n+      if (GET_MODE_CLASS (GET_MODE (x)) == MODE_FLOAT)\n+\t{\n+\t  REAL_VALUE_TYPE d;\n+\n+\t  REAL_VALUE_FROM_CONST_DOUBLE (d, x);\n+\t  *px = immed_real_const_1 (d, GET_MODE (x));\n+\t}\n+      else\n+\t*px = immed_double_const (CONST_DOUBLE_LOW (x), CONST_DOUBLE_HIGH (x),\n+\t\t\t\t  VOIDmode);\n+    }\n+\n+  else if (RTX_INTEGRATED_P (x) && GET_CODE (x) == CONST)\n+    {\n+      restore_constants (&XEXP (x, 0));\n+      *px = validize_mem (force_const_mem (GET_MODE (x), XEXP (x, 0)));\n+    }\n+  else if (RTX_INTEGRATED_P (x) && GET_CODE (x) == SUBREG)\n+    {\n+      /* This must be (subreg/i:M1 (const/i:M2 ...) 0).  */\n+      rtx new = XEXP (SUBREG_REG (x), 0);\n+\n+      restore_constants (&new);\n+      new = force_const_mem (GET_MODE (SUBREG_REG (x)), new);\n+      PUT_MODE (new, GET_MODE (x));\n+      *px = validize_mem (new);\n+    }\n+  else if (RTX_INTEGRATED_P (x) && GET_CODE (x) == ADDRESS)\n+    {\n+      restore_constants (&XEXP (x, 0));\n+      *px = XEXP (force_const_mem (GET_MODE (x), XEXP (x, 0)), 0);\n+    }\n+  else\n+    {\n+      fmt = GET_RTX_FORMAT (GET_CODE (x));\n+      for (i = 0; i < GET_RTX_LENGTH (GET_CODE (x)); i++)\n+\t{\n+\t  switch (*fmt++)\n+\t    {\n+\t    case 'E':\n+\t      for (j = 0; j < XVECLEN (x, i); j++)\n+\t\trestore_constants (&XVECEXP (x, i, j));\n+\t      break;\n+\n+\t    case 'e':\n+\t      restore_constants (&XEXP (x, i));\n+\t      break;\n+\t    }\n+\t}\n+    }\n+}\n+\f\n+/* Output the assembly language code for the function FNDECL\n+   from its DECL_SAVED_INSNS.  Used for inline functions that are output\n+   at end of compilation instead of where they came in the source.  */\n+\n+void\n+output_inline_function (fndecl)\n+     tree fndecl;\n+{\n+  rtx head = DECL_SAVED_INSNS (fndecl);\n+  rtx last;\n+\n+  temporary_allocation ();\n+\n+  current_function_decl = fndecl;\n+\n+  /* This call is only used to initialize global variables.  */\n+  init_function_start (fndecl, \"lossage\", 1);\n+\n+  /* Redo parameter determinations in case the FUNCTION_...\n+     macros took machine-specific actions that need to be redone.  */\n+  assign_parms (fndecl, 1);\n+\n+  /* Set stack frame size.  */\n+  assign_stack_local (BLKmode, DECL_FRAME_SIZE (fndecl), 0);\n+\n+  restore_reg_data (FIRST_PARM_INSN (head));\n+\n+  stack_slot_list = STACK_SLOT_LIST (head);\n+\n+  if (FUNCTION_FLAGS (head) & FUNCTION_FLAGS_CALLS_ALLOCA)\n+    current_function_calls_alloca = 1;\n+\n+  if (FUNCTION_FLAGS (head) & FUNCTION_FLAGS_CALLS_SETJMP)\n+    current_function_calls_setjmp = 1;\n+\n+  if (FUNCTION_FLAGS (head) & FUNCTION_FLAGS_CALLS_LONGJMP)\n+    current_function_calls_longjmp = 1;\n+\n+  if (FUNCTION_FLAGS (head) & FUNCTION_FLAGS_RETURNS_STRUCT)\n+    current_function_returns_struct = 1;\n+\n+  if (FUNCTION_FLAGS (head) & FUNCTION_FLAGS_RETURNS_PCC_STRUCT)\n+    current_function_returns_pcc_struct = 1;\n+\n+  if (FUNCTION_FLAGS (head) & FUNCTION_FLAGS_NEEDS_CONTEXT)\n+    current_function_needs_context = 1;\n+\n+  if (FUNCTION_FLAGS (head) & FUNCTION_FLAGS_HAS_NONLOCAL_LABEL)\n+    current_function_has_nonlocal_label = 1;\n+\n+  if (FUNCTION_FLAGS (head) & FUNCTION_FLAGS_RETURNS_POINTER)\n+    current_function_returns_pointer = 1;\n+\n+  if (FUNCTION_FLAGS (head) & FUNCTION_FLAGS_USES_CONST_POOL)\n+    current_function_uses_const_pool = 1;\n+\n+  if (FUNCTION_FLAGS (head) & FUNCTION_FLAGS_USES_PIC_OFFSET_TABLE)\n+    current_function_uses_pic_offset_table = 1;\n+\n+  current_function_outgoing_args_size = OUTGOING_ARGS_SIZE (head);\n+  current_function_pops_args = POPS_ARGS (head);\n+\n+  /* There is no need to output a return label again.  */\n+  return_label = 0;\n+\n+  expand_function_end (DECL_SOURCE_FILE (fndecl), DECL_SOURCE_LINE (fndecl));\n+\n+  /* Find last insn and rebuild the constant pool.  */\n+  for (last = FIRST_PARM_INSN (head);\n+       NEXT_INSN (last); last = NEXT_INSN (last))\n+    {\n+      if (GET_RTX_CLASS (GET_CODE (last)) == 'i')\n+\t{\n+\t  restore_constants (&PATTERN (last));\n+\t  restore_constants (&REG_NOTES (last));\n+\t}\n+    }\n+\n+  set_new_first_and_last_insn (FIRST_PARM_INSN (head), last);\n+  set_new_first_and_last_label_num (FIRST_LABELNO (head), LAST_LABELNO (head));\n+\n+  /* Compile this function all the way down to assembly code.  */\n+  rest_of_compilation (fndecl);\n+\n+  current_function_decl = 0;\n+\n+  permanent_allocation ();\n+}"}]}