{"sha": "6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NmMzY2U2M2IwNGIzOGY4NGMwMzU3ZTQ2NDgzODNmMGUzYWIxMWNkOQ==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2020-11-06T16:49:28Z"}, "committer": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2020-11-06T16:49:28Z"}, "message": "aarch64: Support permutes on unpacked SVE vectors\n\nThis patch adds support for permuting unpacked SVE vectors using:\n\n- DUP\n- EXT\n- REV[BHW]\n- REV\n- TRN[12]\n- UZP[12]\n- ZIP[12]\n\nThis involves rewriting the REV[BHW] permute code so that the inputs\nand outputs of the insn pattern have the same mode as the vectors\nbeing permuted.  This is different from the ACLE form, where the\nreversal happens within individual elements rather than within\ngroups of multiple elements.\n\nThe patch does not add a conditional version of REV[BHW].  I'll come\nback to that once we have partial-vector comparisons and selects.\n\nThe patch is really just enablement, adding an extra tool to the\ntoolbox.  It doesn't bring any significant vectorisation opportunities\non its own.  However, the patch does have one artificial example that\nis now vectorised in a better way than before.\n\ngcc/\n\t* config/aarch64/aarch64-modes.def (VNx2BF, VNx4BF): Adjust nunits\n\tand alignment based on the current VG.\n\t* config/aarch64/iterators.md (SVE_ALL, SVE_24, SVE_2, SVE_4): Add\n\tpartial SVE BF modes.\n\t(UNSPEC_REVBHW): New unspec.\n\t(Vetype, Vesize, Vctype, VEL, Vel, vwcore, V_INT_CONTAINER)\n\t(v_int_container, VPRED, vpred): Handle partial SVE BF modes.\n\t(container_bits, Vcwtype): New mode attributes.\n\t* config/aarch64/aarch64-sve.md\n\t(@aarch64_sve_revbhw_<SVE_ALL:mode><PRED_HSD:mode>): New pattern.\n\t(@aarch64_sve_dup_lane<mode>): Extended from SVE_FULL to SVE_ALL.\n\t(@aarch64_sve_rev<mode>, @aarch64_sve_<perm_insn><mode>): Likewise.\n\t(@aarch64_sve_ext<mode>): Likewise.\n\t* config/aarch64/aarch64.c (aarch64_classify_vector_mode): Handle\n\tE_VNx2BFmode and E_VNx4BFmode.\n\t(aarch64_evpc_rev_local): Base the analysis on the container size\n\tinstead of the element size.  Use the new aarch64_sve_revbhw\n\tpatterns for SVE.\n\t(aarch64_evpc_dup): Handle partial SVE data modes.  Use the\n\tcontainer size instead of the element size when applying the\n\tSVE immediate limit.  Fix a previously incorrect bounds check.\n\t(aarch64_expand_vec_perm_const_1): Handle partial SVE data modes.\n\ngcc/testsuite/\n\t* gcc.target/aarch64/sve/dup_lane_2.c: New test.\n\t* gcc.target/aarch64/sve/dup_lane_3.c: Likewise.\n\t* gcc.target/aarch64/sve/ext_4.c: Likewise.\n\t* gcc.target/aarch64/sve/rev_2.c: Likewise.\n\t* gcc.target/aarch64/sve/revhw_1.c: Likewise.\n\t* gcc.target/aarch64/sve/revhw_2.c: Likewise.\n\t* gcc.target/aarch64/sve/slp_perm_8.c: Likewise.\n\t* gcc.target/aarch64/sve/trn1_2.c: Likewise.\n\t* gcc.target/aarch64/sve/trn2_2.c: Likewise.\n\t* gcc.target/aarch64/sve/uzp1_2.c: Likewise.\n\t* gcc.target/aarch64/sve/uzp2_2.c: Likewise.\n\t* gcc.target/aarch64/sve/zip1_2.c: Likewise.\n\t* gcc.target/aarch64/sve/zip2_2.c: Likewise.", "tree": {"sha": "fef5cc0a801d7a562146c02b4a97f79daa4cb14c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/fef5cc0a801d7a562146c02b4a97f79daa4cb14c"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "9b11203e33f999933aaa99bba779ffd7cd329849", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9b11203e33f999933aaa99bba779ffd7cd329849", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9b11203e33f999933aaa99bba779ffd7cd329849"}], "stats": {"total": 3745, "additions": 3685, "deletions": 60}, "files": [{"sha": "f304992e3edd47b5e451d2926766cf1298f55d23", "filename": "gcc/config/aarch64/aarch64-modes.def", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -136,11 +136,13 @@ ADJUST_NUNITS (VNx2QI, aarch64_sve_vg);\n ADJUST_NUNITS (VNx2HI, aarch64_sve_vg);\n ADJUST_NUNITS (VNx2SI, aarch64_sve_vg);\n ADJUST_NUNITS (VNx2HF, aarch64_sve_vg);\n+ADJUST_NUNITS (VNx2BF, aarch64_sve_vg);\n ADJUST_NUNITS (VNx2SF, aarch64_sve_vg);\n \n ADJUST_NUNITS (VNx4QI, aarch64_sve_vg * 2);\n ADJUST_NUNITS (VNx4HI, aarch64_sve_vg * 2);\n ADJUST_NUNITS (VNx4HF, aarch64_sve_vg * 2);\n+ADJUST_NUNITS (VNx4BF, aarch64_sve_vg * 2);\n \n ADJUST_NUNITS (VNx8QI, aarch64_sve_vg * 4);\n \n@@ -151,7 +153,9 @@ ADJUST_ALIGNMENT (VNx8QI, 1);\n ADJUST_ALIGNMENT (VNx2HI, 2);\n ADJUST_ALIGNMENT (VNx4HI, 2);\n ADJUST_ALIGNMENT (VNx2HF, 2);\n+ADJUST_ALIGNMENT (VNx2BF, 2);\n ADJUST_ALIGNMENT (VNx4HF, 2);\n+ADJUST_ALIGNMENT (VNx4BF, 2);\n \n ADJUST_ALIGNMENT (VNx2SI, 4);\n ADJUST_ALIGNMENT (VNx2SF, 4);"}, {"sha": "4b0a1ebe9e1dd8bcbf683c5c136d9458b61dd943", "filename": "gcc/config/aarch64/aarch64-sve.md", "status": "modified", "additions": 37, "deletions": 20, "changes": 57, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Fconfig%2Faarch64%2Faarch64-sve.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Fconfig%2Faarch64%2Faarch64-sve.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-sve.md?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -3009,6 +3009,22 @@\n   \"<sve_int_op>\\t%0.<Vetype>, %1/m, %2.<Vetype>\"\n )\n \n+;; Another way of expressing the REVB, REVH and REVW patterns, with this\n+;; form being easier for permutes.  The predicate mode determines the number\n+;; of lanes and the data mode decides the granularity of the reversal within\n+;; each lane.\n+(define_insn \"@aarch64_sve_revbhw_<SVE_ALL:mode><PRED_HSD:mode>\"\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_ALL\n+\t  [(match_operand:PRED_HSD 1 \"register_operand\" \"Upl\")\n+\t   (unspec:SVE_ALL\n+\t     [(match_operand:SVE_ALL 2 \"register_operand\" \"w\")]\n+\t     UNSPEC_REVBHW)]\n+\t  UNSPEC_PRED_X))]\n+  \"TARGET_SVE && <PRED_HSD:elem_bits> > <SVE_ALL:container_bits>\"\n+  \"rev<SVE_ALL:Vcwtype>\\t%0.<PRED_HSD:Vetype>, %1/m, %2.<PRED_HSD:Vetype>\"\n+)\n+\n ;; Predicated integer unary operations with merging.\n (define_insn \"@cond_<optab><mode>\"\n   [(set (match_operand:SVE_FULL_I 0 \"register_operand\" \"=w, ?&w, ?&w\")\n@@ -8273,14 +8289,14 @@\n \n ;; Duplicate one element of a vector.\n (define_insn \"@aarch64_sve_dup_lane<mode>\"\n-  [(set (match_operand:SVE_FULL 0 \"register_operand\" \"=w\")\n-\t(vec_duplicate:SVE_FULL\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(vec_duplicate:SVE_ALL\n \t  (vec_select:<VEL>\n-\t    (match_operand:SVE_FULL 1 \"register_operand\" \"w\")\n+\t    (match_operand:SVE_ALL 1 \"register_operand\" \"w\")\n \t    (parallel [(match_operand:SI 2 \"const_int_operand\")]))))]\n   \"TARGET_SVE\n-   && IN_RANGE (INTVAL (operands[2]) * GET_MODE_SIZE (<VEL>mode), 0, 63)\"\n-  \"dup\\t%0.<Vetype>, %1.<Vetype>[%2]\"\n+   && IN_RANGE (INTVAL (operands[2]) * <container_bits> / 8, 0, 63)\"\n+  \"dup\\t%0.<Vctype>, %1.<Vctype>[%2]\"\n )\n \n ;; Use DUP.Q to duplicate a 128-bit segment of a register.\n@@ -8321,17 +8337,18 @@\n \n ;; Reverse the order of elements within a full vector.\n (define_insn \"@aarch64_sve_rev<mode>\"\n-  [(set (match_operand:SVE_FULL 0 \"register_operand\" \"=w\")\n-\t(unspec:SVE_FULL\n-\t  [(match_operand:SVE_FULL 1 \"register_operand\" \"w\")]\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_ALL\n+\t  [(match_operand:SVE_ALL 1 \"register_operand\" \"w\")]\n \t  UNSPEC_REV))]\n   \"TARGET_SVE\"\n-  \"rev\\t%0.<Vetype>, %1.<Vetype>\")\n+  \"rev\\t%0.<Vctype>, %1.<Vctype>\")\n \n ;; -------------------------------------------------------------------------\n ;; ---- [INT,FP] Special-purpose binary permutes\n ;; -------------------------------------------------------------------------\n ;; Includes:\n+;; - EXT\n ;; - SPLICE\n ;; - TRN1\n ;; - TRN2\n@@ -8359,13 +8376,13 @@\n ;; Permutes that take half the elements from one vector and half the\n ;; elements from the other.\n (define_insn \"@aarch64_sve_<perm_insn><mode>\"\n-  [(set (match_operand:SVE_FULL 0 \"register_operand\" \"=w\")\n-\t(unspec:SVE_FULL\n-\t  [(match_operand:SVE_FULL 1 \"register_operand\" \"w\")\n-\t   (match_operand:SVE_FULL 2 \"register_operand\" \"w\")]\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_ALL\n+\t  [(match_operand:SVE_ALL 1 \"register_operand\" \"w\")\n+\t   (match_operand:SVE_ALL 2 \"register_operand\" \"w\")]\n \t  PERMUTE))]\n   \"TARGET_SVE\"\n-  \"<perm_insn>\\t%0.<Vetype>, %1.<Vetype>, %2.<Vetype>\"\n+  \"<perm_insn>\\t%0.<Vctype>, %1.<Vctype>, %2.<Vctype>\"\n )\n \n ;; Apply PERMUTE to 128-bit sequences.  The behavior of these patterns\n@@ -8383,16 +8400,16 @@\n ;; Concatenate two vectors and extract a subvector.  Note that the\n ;; immediate (third) operand is the lane index not the byte index.\n (define_insn \"@aarch64_sve_ext<mode>\"\n-  [(set (match_operand:SVE_FULL 0 \"register_operand\" \"=w, ?&w\")\n-\t(unspec:SVE_FULL\n-\t  [(match_operand:SVE_FULL 1 \"register_operand\" \"0, w\")\n-\t   (match_operand:SVE_FULL 2 \"register_operand\" \"w, w\")\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w, ?&w\")\n+\t(unspec:SVE_ALL\n+\t  [(match_operand:SVE_ALL 1 \"register_operand\" \"0, w\")\n+\t   (match_operand:SVE_ALL 2 \"register_operand\" \"w, w\")\n \t   (match_operand:SI 3 \"const_int_operand\")]\n \t  UNSPEC_EXT))]\n   \"TARGET_SVE\n-   && IN_RANGE (INTVAL (operands[3]) * GET_MODE_SIZE (<VEL>mode), 0, 255)\"\n+   && IN_RANGE (INTVAL (operands[3]) * <container_bits> / 8, 0, 255)\"\n   {\n-    operands[3] = GEN_INT (INTVAL (operands[3]) * GET_MODE_SIZE (<VEL>mode));\n+    operands[3] = GEN_INT (INTVAL (operands[3]) * <container_bits> / 8);\n     return (which_alternative == 0\n \t    ? \"ext\\\\t%0.b, %0.b, %2.b, #%3\"\n \t    : \"movprfx\\t%0, %1\\;ext\\\\t%0.b, %0.b, %2.b, #%3\");"}, {"sha": "97cb68980e975dfb2c0c0c0a05f9153beb64a2ad", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 18, "deletions": 27, "changes": 45, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -2226,6 +2226,9 @@ aarch64_classify_vector_mode (machine_mode mode)\n     /* Partial SVE HF vectors.  */\n     case E_VNx2HFmode:\n     case E_VNx4HFmode:\n+    /* Partial SVE BF vectors.  */\n+    case E_VNx2BFmode:\n+    case E_VNx4BFmode:\n     /* Partial SVE SF vector.  */\n     case E_VNx2SFmode:\n       return TARGET_SVE ? VEC_SVE_DATA | VEC_PARTIAL : 0;\n@@ -20468,18 +20471,21 @@ aarch64_evpc_rev_local (struct expand_vec_perm_d *d)\n       || !diff)\n     return false;\n \n-  size = (diff + 1) * GET_MODE_UNIT_SIZE (d->vmode);\n-  if (size == 8)\n+  if (d->vec_flags & VEC_SVE_DATA)\n+    size = (diff + 1) * aarch64_sve_container_bits (d->vmode);\n+  else\n+    size = (diff + 1) * GET_MODE_UNIT_BITSIZE (d->vmode);\n+  if (size == 64)\n     {\n       unspec = UNSPEC_REV64;\n       pred_mode = VNx2BImode;\n     }\n-  else if (size == 4)\n+  else if (size == 32)\n     {\n       unspec = UNSPEC_REV32;\n       pred_mode = VNx4BImode;\n     }\n-  else if (size == 2)\n+  else if (size == 16)\n     {\n       unspec = UNSPEC_REV16;\n       pred_mode = VNx8BImode;\n@@ -20496,28 +20502,11 @@ aarch64_evpc_rev_local (struct expand_vec_perm_d *d)\n   if (d->testing_p)\n     return true;\n \n-  if (d->vec_flags == VEC_SVE_DATA)\n-    {\n-      machine_mode int_mode = aarch64_sve_int_mode (pred_mode);\n-      rtx target = gen_reg_rtx (int_mode);\n-      if (BYTES_BIG_ENDIAN)\n-\t/* The act of taking a subreg between INT_MODE and d->vmode\n-\t   is itself a reversing operation on big-endian targets;\n-\t   see the comment at the head of aarch64-sve.md for details.\n-\t   First reinterpret OP0 as INT_MODE without using a subreg\n-\t   and without changing the contents.  */\n-\temit_insn (gen_aarch64_sve_reinterpret (int_mode, target, d->op0));\n-      else\n-\t{\n-\t  /* For SVE we use REV[BHW] unspecs derived from the element size\n-\t     of v->mode and vector modes whose elements have SIZE bytes.\n-\t     This ensures that the vector modes match the predicate modes.  */\n-\t  int unspec = aarch64_sve_rev_unspec (d->vmode);\n-\t  rtx pred = aarch64_ptrue_reg (pred_mode);\n-\t  emit_insn (gen_aarch64_pred (unspec, int_mode, target, pred,\n-\t\t\t\t       gen_lowpart (int_mode, d->op0)));\n-\t}\n-      emit_move_insn (d->target, gen_lowpart (d->vmode, target));\n+  if (d->vec_flags & VEC_SVE_DATA)\n+    {\n+      rtx pred = aarch64_ptrue_reg (pred_mode);\n+      emit_insn (gen_aarch64_sve_revbhw (d->vmode, pred_mode,\n+\t\t\t\t\t d->target, pred, d->op0));\n       return true;\n     }\n   rtx src = gen_rtx_UNSPEC (d->vmode, gen_rtvec (1, d->op0), unspec);\n@@ -20562,7 +20551,8 @@ aarch64_evpc_dup (struct expand_vec_perm_d *d)\n       || !d->perm[0].is_constant (&elt))\n     return false;\n \n-  if (d->vec_flags == VEC_SVE_DATA && elt >= 64 * GET_MODE_UNIT_SIZE (vmode))\n+  if ((d->vec_flags & VEC_SVE_DATA)\n+      && elt * (aarch64_sve_container_bits (vmode) / 8) >= 64)\n     return false;\n \n   /* Success! */\n@@ -20782,6 +20772,7 @@ aarch64_expand_vec_perm_const_1 (struct expand_vec_perm_d *d)\n \n   if ((d->vec_flags == VEC_ADVSIMD\n        || d->vec_flags == VEC_SVE_DATA\n+       || d->vec_flags == (VEC_SVE_DATA | VEC_PARTIAL)\n        || d->vec_flags == VEC_SVE_PRED)\n       && known_gt (nelt, 1))\n     {"}, {"sha": "fb1426b7752890848cb49722ef7442d96cb1408b", "filename": "gcc/config/aarch64/iterators.md", "status": "modified", "additions": 41, "deletions": 13, "changes": 54, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Fconfig%2Faarch64%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Fconfig%2Faarch64%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fiterators.md?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -400,7 +400,7 @@\n (define_mode_iterator SVE_ALL [VNx16QI VNx8QI VNx4QI VNx2QI\n \t\t\t       VNx8HI VNx4HI VNx2HI\n \t\t\t       VNx8HF VNx4HF VNx2HF\n-\t\t\t       VNx8BF\n+\t\t\t       VNx8BF VNx4BF VNx2BF\n \t\t\t       VNx4SI VNx2SI\n \t\t\t       VNx4SF VNx2SF\n \t\t\t       VNx2DI\n@@ -418,11 +418,13 @@\n \t\t\t\tVNx2DI])\n \n ;; SVE modes with 2 or 4 elements.\n-(define_mode_iterator SVE_24 [VNx2QI VNx2HI VNx2HF VNx2SI VNx2SF VNx2DI VNx2DF\n-\t\t\t      VNx4QI VNx4HI VNx4HF VNx4SI VNx4SF])\n+(define_mode_iterator SVE_24 [VNx2QI VNx2HI VNx2HF VNx2BF VNx2SI VNx2SF\n+\t\t\t      VNx2DI VNx2DF\n+\t\t\t      VNx4QI VNx4HI VNx4HF VNx4BF VNx4SI VNx4SF])\n \n ;; SVE modes with 2 elements.\n-(define_mode_iterator SVE_2 [VNx2QI VNx2HI VNx2HF VNx2SI VNx2SF VNx2DI VNx2DF])\n+(define_mode_iterator SVE_2 [VNx2QI VNx2HI VNx2HF VNx2BF\n+\t\t\t     VNx2SI VNx2SF VNx2DI VNx2DF])\n \n ;; SVE integer modes with 2 elements, excluding the widest element.\n (define_mode_iterator SVE_2BHSI [VNx2QI VNx2HI VNx2SI])\n@@ -431,7 +433,7 @@\n (define_mode_iterator SVE_2HSDI [VNx2HI VNx2SI VNx2DI])\n \n ;; SVE modes with 4 elements.\n-(define_mode_iterator SVE_4 [VNx4QI VNx4HI VNx4HF VNx4SI VNx4SF])\n+(define_mode_iterator SVE_4 [VNx4QI VNx4HI VNx4HF VNx4BF VNx4SI VNx4SF])\n \n ;; SVE integer modes with 4 elements, excluding the widest element.\n (define_mode_iterator SVE_4BHI [VNx4QI VNx4HI])\n@@ -621,6 +623,7 @@\n     UNSPEC_REVB\t\t; Used in aarch64-sve.md.\n     UNSPEC_REVH\t\t; Used in aarch64-sve.md.\n     UNSPEC_REVW\t\t; Used in aarch64-sve.md.\n+    UNSPEC_REVBHW\t; Used in aarch64-sve.md.\n     UNSPEC_SMUL_HIGHPART ; Used in aarch64-sve.md.\n     UNSPEC_UMUL_HIGHPART ; Used in aarch64-sve.md.\n     UNSPEC_FMLA\t\t; Used in aarch64-sve.md.\n@@ -968,6 +971,16 @@\n \t\t\t     (VNx4SI \"32\") (VNx2DI \"64\")\n \t\t\t     (VNx8HF \"16\") (VNx4SF \"32\") (VNx2DF \"64\")])\n \n+;; The number of bits in a vector container.\n+(define_mode_attr container_bits [(VNx16QI \"8\")\n+\t\t\t\t  (VNx8HI \"16\") (VNx8QI \"16\") (VNx8HF \"16\")\n+\t\t\t\t  (VNx8BF \"16\")\n+\t\t\t\t  (VNx4SI \"32\") (VNx4HI \"32\") (VNx4QI \"32\")\n+\t\t\t\t  (VNx4SF \"32\") (VNx4HF \"32\") (VNx4BF \"32\")\n+\t\t\t\t  (VNx2DI \"64\") (VNx2SI \"64\") (VNx2HI \"64\")\n+\t\t\t\t  (VNx2QI \"64\") (VNx2DF \"64\") (VNx2SF \"64\")\n+\t\t\t\t  (VNx2HF \"64\") (VNx2BF \"64\")])\n+\n ;; Attribute to describe constants acceptable in logical operations\n (define_mode_attr lconst [(SI \"K\") (DI \"L\")])\n \n@@ -1029,7 +1042,7 @@\n \t\t\t  (VNx16QI \"b\") (VNx8QI \"b\") (VNx4QI \"b\") (VNx2QI \"b\")\n \t\t\t  (VNx8HI \"h\") (VNx4HI \"h\") (VNx2HI \"h\")\n \t\t\t  (VNx8HF \"h\") (VNx4HF \"h\") (VNx2HF \"h\")\n-\t\t\t  (VNx8BF \"h\")\n+\t\t\t  (VNx8BF \"h\") (VNx4BF \"h\") (VNx2BF \"h\")\n \t\t\t  (VNx4SI \"s\") (VNx2SI \"s\")\n \t\t\t  (VNx4SF \"s\") (VNx2SF \"s\")\n \t\t\t  (VNx2DI \"d\")\n@@ -1047,7 +1060,7 @@\n (define_mode_attr Vesize [(VNx16QI \"b\") (VNx8QI \"b\") (VNx4QI \"b\") (VNx2QI \"b\")\n \t\t\t  (VNx8HI \"h\") (VNx4HI \"h\") (VNx2HI \"h\")\n \t\t\t  (VNx8HF \"h\") (VNx4HF \"h\") (VNx2HF \"h\")\n-\t\t\t  (VNx8BF \"h\")\n+\t\t\t  (VNx8BF \"h\") (VNx4BF \"h\") (VNx2BF \"h\")\n \t\t\t  (VNx4SI \"w\") (VNx2SI \"w\")\n \t\t\t  (VNx4SF \"w\") (VNx2SF \"w\")\n \t\t\t  (VNx2DI \"d\")\n@@ -1066,12 +1079,23 @@\n (define_mode_attr Vctype [(VNx16QI \"b\") (VNx8QI \"h\") (VNx4QI \"s\") (VNx2QI \"d\")\n \t\t\t  (VNx8HI \"h\") (VNx4HI \"s\") (VNx2HI \"d\")\n \t\t\t  (VNx8HF \"h\") (VNx4HF \"s\") (VNx2HF \"d\")\n-\t\t\t  (VNx8BF \"h\")\n+\t\t\t  (VNx8BF \"h\") (VNx4BF \"s\") (VNx2BF \"d\")\n \t\t\t  (VNx4SI \"s\") (VNx2SI \"d\")\n \t\t\t  (VNx4SF \"s\") (VNx2SF \"d\")\n \t\t\t  (VNx2DI \"d\")\n \t\t\t  (VNx2DF \"d\")])\n \n+;; The instruction mnemonic suffix for an SVE mode's element container,\n+;; i.e. the Vewtype of full SVE modes that have the same number of elements.\n+(define_mode_attr Vcwtype [(VNx16QI \"b\") (VNx8QI \"h\") (VNx4QI \"w\") (VNx2QI \"d\")\n+\t\t\t   (VNx8HI \"h\") (VNx4HI \"w\") (VNx2HI \"d\")\n+\t\t\t   (VNx8HF \"h\") (VNx4HF \"w\") (VNx2HF \"d\")\n+\t\t\t   (VNx8BF \"h\") (VNx4BF \"w\") (VNx2BF \"d\")\n+\t\t\t   (VNx4SI \"w\") (VNx2SI \"d\")\n+\t\t\t   (VNx4SF \"w\") (VNx2SF \"d\")\n+\t\t\t   (VNx2DI \"d\")\n+\t\t\t   (VNx2DF \"d\")])\n+\n ;; Vetype is used everywhere in scheduling type and assembly output,\n ;; sometimes they are not the same, for example HF modes on some\n ;; instructions.  stype is defined to represent scheduling type\n@@ -1107,7 +1131,7 @@\n \t\t       (VNx16QI \"QI\") (VNx8QI \"QI\") (VNx4QI \"QI\") (VNx2QI \"QI\")\n \t\t       (VNx8HI \"HI\") (VNx4HI \"HI\") (VNx2HI \"HI\")\n \t\t       (VNx8HF \"HF\") (VNx4HF \"HF\") (VNx2HF \"HF\")\n-\t\t       (VNx8BF \"BF\")\n+\t\t       (VNx8BF \"BF\") (VNx4BF \"BF\") (VNx2BF \"BF\")\n \t\t       (VNx4SI \"SI\") (VNx2SI \"SI\")\n \t\t       (VNx4SF \"SF\") (VNx2SF \"SF\")\n \t\t       (VNx2DI \"DI\")\n@@ -1127,7 +1151,7 @@\n \t\t       (VNx16QI \"qi\") (VNx8QI \"qi\") (VNx4QI \"qi\") (VNx2QI \"qi\")\n \t\t       (VNx8HI \"hi\") (VNx4HI \"hi\") (VNx2HI \"hi\")\n \t\t       (VNx8HF \"hf\") (VNx4HF \"hf\") (VNx2HF \"hf\")\n-\t\t       (VNx8BF \"bf\")\n+\t\t       (VNx8BF \"bf\") (VNx4BF \"bf\") (VNx2BF \"bf\")\n \t\t       (VNx4SI \"si\") (VNx2SI \"si\")\n \t\t       (VNx4SF \"sf\") (VNx2SF \"sf\")\n \t\t       (VNx2DI \"di\")\n@@ -1310,7 +1334,7 @@\n \t\t\t  (VNx16QI \"w\") (VNx8QI \"w\") (VNx4QI \"w\") (VNx2QI \"w\")\n \t\t\t  (VNx8HI \"w\") (VNx4HI \"w\") (VNx2HI \"w\")\n \t\t\t  (VNx8HF \"w\") (VNx4HF \"w\") (VNx2HF \"w\")\n-\t\t\t  (VNx8BF \"w\")\n+\t\t\t  (VNx8BF \"w\") (VNx4BF \"w\") (VNx2BF \"w\")\n \t\t\t  (VNx4SI \"w\") (VNx2SI \"w\")\n \t\t\t  (VNx4SF \"w\") (VNx2SF \"w\")\n \t\t\t  (VNx2DI \"x\")\n@@ -1380,6 +1404,8 @@\n \t\t\t\t   (VNx2DI \"VNx2DI\")\n \t\t\t\t   (VNx8HF \"VNx8HI\") (VNx4HF \"VNx4SI\")\n \t\t\t\t   (VNx2HF \"VNx2DI\")\n+\t\t\t\t   (VNx8BF \"VNx8HI\") (VNx4BF \"VNx4SI\")\n+\t\t\t\t   (VNx2BF \"VNx2DI\")\n \t\t\t\t   (VNx4SF \"VNx4SI\") (VNx2SF \"VNx2DI\")\n \t\t\t\t   (VNx2DF \"VNx2DI\")])\n \n@@ -1392,6 +1418,8 @@\n \t\t\t\t   (VNx2DI \"vnx2di\")\n \t\t\t\t   (VNx8HF \"vnx8hi\") (VNx4HF \"vnx4si\")\n \t\t\t\t   (VNx2HF \"vnx2di\")\n+\t\t\t\t   (VNx8BF \"vnx8hi\") (VNx4BF \"vnx4si\")\n+\t\t\t\t   (VNx2BF \"vnx2di\")\n \t\t\t\t   (VNx4SF \"vnx4si\") (VNx2SF \"vnx2di\")\n \t\t\t\t   (VNx2DF \"vnx2di\")])\n \n@@ -1617,7 +1645,7 @@\n \t\t\t (VNx4QI \"VNx4BI\") (VNx2QI \"VNx2BI\")\n \t\t\t (VNx8HI \"VNx8BI\") (VNx4HI \"VNx4BI\") (VNx2HI \"VNx2BI\")\n \t\t\t (VNx8HF \"VNx8BI\") (VNx4HF \"VNx4BI\") (VNx2HF \"VNx2BI\")\n-\t\t\t (VNx8BF \"VNx8BI\")\n+\t\t\t (VNx8BF \"VNx8BI\") (VNx4BF \"VNx4BI\") (VNx2BF \"VNx2BI\")\n \t\t\t (VNx4SI \"VNx4BI\") (VNx2SI \"VNx2BI\")\n \t\t\t (VNx4SF \"VNx4BI\") (VNx2SF \"VNx2BI\")\n \t\t\t (VNx2DI \"VNx2BI\")\n@@ -1643,7 +1671,7 @@\n \t\t\t (VNx4QI \"vnx4bi\") (VNx2QI \"vnx2bi\")\n \t\t\t (VNx8HI \"vnx8bi\") (VNx4HI \"vnx4bi\") (VNx2HI \"vnx2bi\")\n \t\t\t (VNx8HF \"vnx8bi\") (VNx4HF \"vnx4bi\") (VNx2HF \"vnx2bi\")\n-\t\t\t (VNx8BF \"vnx8bi\")\n+\t\t\t (VNx8BF \"vnx8bi\") (VNx4BF \"vnx4bi\") (VNx2BF \"vnx2bi\")\n \t\t\t (VNx4SI \"vnx4bi\") (VNx2SI \"vnx2bi\")\n \t\t\t (VNx4SF \"vnx4bi\") (VNx2SF \"vnx2bi\")\n \t\t\t (VNx2DI \"vnx2bi\")"}, {"sha": "3d74ff98e6d61789df05827eef89628730b898e3", "filename": "gcc/testsuite/gcc.target/aarch64/sve/dup_lane_2.c", "status": "added", "additions": 331, "deletions": 0, "changes": 331, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fdup_lane_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fdup_lane_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fdup_lane_2.c?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -0,0 +1,331 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O -msve-vector-bits=2048 -mlittle-endian --save-temps\" } */\n+/* { dg-final { check-function-bodies \"**\" \"\" } } */\n+\n+typedef unsigned char v128qi __attribute__((vector_size(128)));\n+typedef unsigned char v64qi __attribute__((vector_size(64)));\n+typedef unsigned char v32qi __attribute__((vector_size(32)));\n+typedef unsigned short v64hi __attribute__((vector_size(128)));\n+typedef unsigned short v32hi __attribute__((vector_size(64)));\n+typedef _Float16 v64hf __attribute__((vector_size(128)));\n+typedef _Float16 v32hf __attribute__((vector_size(64)));\n+typedef __bf16 v64bf __attribute__((vector_size(128)));\n+typedef __bf16 v32bf __attribute__((vector_size(64)));\n+typedef unsigned int v32si __attribute__((vector_size(128)));\n+typedef float v32sf __attribute__((vector_size(128)));\n+\n+#define PERM0(B) B, B\n+#define PERM1(B) PERM0 (B), PERM0 (B)\n+#define PERM2(B) PERM1 (B), PERM1 (B)\n+#define PERM3(B) PERM2 (B), PERM2 (B)\n+#define PERM4(B) PERM3 (B), PERM3 (B)\n+#define PERM5(B) PERM4 (B), PERM4 (B)\n+#define PERM6(B) PERM5 (B), PERM5 (B)\n+\n+/*\n+** qi_dup_h_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.h, \\2\\.h\\[1\\]\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_dup_h_1 (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (1) });\n+}\n+\n+/*\n+** qi_dup_h_31:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.h, \\2\\.h\\[31\\]\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_dup_h_31 (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (31) });\n+}\n+\n+/*\n+** qi_dup_s_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.s, \\2\\.s\\[1\\]\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64qi\n+qi_dup_s_1 (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (1) });\n+}\n+\n+/*\n+** qi_dup_s_15:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.s, \\2\\.s\\[15\\]\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64qi\n+qi_dup_s_15 (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (15) });\n+}\n+\n+/*\n+** qi_dup_d_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.d, \\2\\.d\\[1\\]\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32qi\n+qi_dup_d_1 (v32qi x)\n+{\n+  return __builtin_shuffle (x, x, (v32qi) { PERM4 (1) });\n+}\n+\n+/*\n+** qi_dup_d_7:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.d, \\2\\.d\\[7\\]\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32qi\n+qi_dup_d_7 (v32qi x)\n+{\n+  return __builtin_shuffle (x, x, (v32qi) { PERM4 (7) });\n+}\n+\n+/*\n+** hi_dup_s_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.s, \\2\\.s\\[1\\]\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hi\n+hi_dup_s_1 (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (1) });\n+}\n+\n+/*\n+** hi_dup_s_15:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.s, \\2\\.s\\[15\\]\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hi\n+hi_dup_s_15 (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (15) });\n+}\n+\n+/*\n+** hf_dup_s_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.s, \\2\\.s\\[1\\]\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hf\n+hf_dup_s_1 (v64hf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (1) });\n+}\n+\n+/*\n+** hf_dup_s_11:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.s, \\2\\.s\\[11\\]\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hf\n+hf_dup_s_11 (v64hf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (11) });\n+}\n+\n+/*\n+** bf_dup_s_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.s, \\2\\.s\\[1\\]\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64bf\n+bf_dup_s_1 (v64bf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (1) });\n+}\n+\n+/*\n+** bf_dup_s_13:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.s, \\2\\.s\\[13\\]\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64bf\n+bf_dup_s_13 (v64bf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (13) });\n+}\n+\n+/*\n+** hi_dup_d_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.d, \\2\\.d\\[1\\]\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hi\n+hi_dup_d_1 (v32hi x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (1) });\n+}\n+\n+/*\n+** hi_dup_d_7:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.d, \\2\\.d\\[7\\]\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hi\n+hi_dup_d_7 (v32hi x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (7) });\n+}\n+\n+/*\n+** hf_dup_d_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.d, \\2\\.d\\[1\\]\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hf\n+hf_dup_d_1 (v32hf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (1) });\n+}\n+\n+/*\n+** hf_dup_d_5:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.d, \\2\\.d\\[5\\]\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hf\n+hf_dup_d_5 (v32hf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (5) });\n+}\n+\n+/*\n+** bf_dup_d_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.d, \\2\\.d\\[1\\]\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32bf\n+bf_dup_d_1 (v32bf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (1) });\n+}\n+\n+/*\n+** bf_dup_d_6:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.d, \\2\\.d\\[6\\]\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32bf\n+bf_dup_d_6 (v32bf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (6) });\n+}\n+\n+/*\n+** si_dup_d_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.d, \\2\\.d\\[1\\]\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32si\n+si_dup_d_1 (v32si x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (1) });\n+}\n+\n+/*\n+** si_dup_d_7:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.d, \\2\\.d\\[7\\]\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32si\n+si_dup_d_7 (v32si x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (7) });\n+}\n+\n+/*\n+** sf_dup_d_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.d, \\2\\.d\\[1\\]\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32sf\n+sf_dup_d_1 (v32sf x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (1) });\n+}\n+\n+/*\n+** sf_dup_d_7:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tdup\t(z[0-9]+)\\.d, \\2\\.d\\[7\\]\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32sf\n+sf_dup_d_7 (v32sf x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (7) });\n+}"}, {"sha": "50f73a1aa23afb74bd7b9bc9b4520ad92613e247", "filename": "gcc/testsuite/gcc.target/aarch64/sve/dup_lane_3.c", "status": "added", "additions": 90, "deletions": 0, "changes": 90, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fdup_lane_3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fdup_lane_3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fdup_lane_3.c?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -0,0 +1,90 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O -msve-vector-bits=2048 -mlittle-endian --save-temps\" } */\n+\n+typedef unsigned char v128qi __attribute__((vector_size(128)));\n+typedef unsigned char v64qi __attribute__((vector_size(64)));\n+typedef unsigned char v32qi __attribute__((vector_size(32)));\n+typedef unsigned short v64hi __attribute__((vector_size(128)));\n+typedef unsigned short v32hi __attribute__((vector_size(64)));\n+typedef _Float16 v64hf __attribute__((vector_size(128)));\n+typedef _Float16 v32hf __attribute__((vector_size(64)));\n+typedef __bf16 v64bf __attribute__((vector_size(128)));\n+typedef __bf16 v32bf __attribute__((vector_size(64)));\n+typedef unsigned int v32si __attribute__((vector_size(128)));\n+typedef float v32sf __attribute__((vector_size(128)));\n+\n+#define PERM0(B) B, B\n+#define PERM1(B) PERM0 (B), PERM0 (B)\n+#define PERM2(B) PERM1 (B), PERM1 (B)\n+#define PERM3(B) PERM2 (B), PERM2 (B)\n+#define PERM4(B) PERM3 (B), PERM3 (B)\n+#define PERM5(B) PERM4 (B), PERM4 (B)\n+#define PERM6(B) PERM5 (B), PERM5 (B)\n+\n+v128qi\n+qi_dup_h_32 (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (32) });\n+}\n+\n+v64qi\n+qi_dup_s_16 (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (16) });\n+}\n+\n+v32qi\n+qi_dup_d_8 (v32qi x)\n+{\n+  return __builtin_shuffle (x, x, (v32qi) { PERM4 (8) });\n+}\n+\n+v64hi\n+hi_dup_s_16 (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (16) });\n+}\n+\n+v64hf\n+hf_dup_s_16 (v64hf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (16) });\n+}\n+\n+v64bf\n+bf_dup_s_16 (v64bf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (16) });\n+}\n+\n+v32hi\n+hi_dup_d_8 (v32hi x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (8) });\n+}\n+\n+v32hf\n+hf_dup_d_8 (v32hf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (8) });\n+}\n+\n+v32bf\n+bf_dup_d_8 (v32bf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (8) });\n+}\n+\n+v32si\n+si_dup_d_8 (v32si x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (8) });\n+}\n+\n+v32sf\n+sf_dup_d_8 (v32sf x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (8) });\n+}\n+\n+/* { dg-final { scan-assembler-not {\\tdup\\tz} } } */"}, {"sha": "4637b5cdc7a5b9b956521379e679ff0e9f7b5edc", "filename": "gcc/testsuite/gcc.target/aarch64/sve/ext_4.c", "status": "added", "additions": 353, "deletions": 0, "changes": 353, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fext_4.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fext_4.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fext_4.c?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -0,0 +1,353 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O -msve-vector-bits=2048 -mlittle-endian --save-temps\" } */\n+/* { dg-final { check-function-bodies \"**\" \"\" } } */\n+\n+typedef unsigned char v128qi __attribute__((vector_size(128)));\n+typedef unsigned char v64qi __attribute__((vector_size(64)));\n+typedef unsigned char v32qi __attribute__((vector_size(32)));\n+typedef unsigned short v64hi __attribute__((vector_size(128)));\n+typedef unsigned short v32hi __attribute__((vector_size(64)));\n+typedef _Float16 v64hf __attribute__((vector_size(128)));\n+typedef _Float16 v32hf __attribute__((vector_size(64)));\n+typedef __bf16 v64bf __attribute__((vector_size(128)));\n+typedef __bf16 v32bf __attribute__((vector_size(64)));\n+typedef unsigned int v32si __attribute__((vector_size(128)));\n+typedef float v32sf __attribute__((vector_size(128)));\n+\n+#define PERM0(B) B, B + 1\n+#define PERM1(B) PERM0 (B), PERM0 (B + 2)\n+#define PERM2(B) PERM1 (B), PERM1 (B + 4)\n+#define PERM3(B) PERM2 (B), PERM2 (B + 8)\n+#define PERM4(B) PERM3 (B), PERM3 (B + 16)\n+#define PERM5(B) PERM4 (B), PERM4 (B + 32)\n+#define PERM6(B) PERM5 (B), PERM5 (B + 64)\n+\n+/*\n+** qi_ext_h_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #2\n+**\tst1b\t\\2\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_ext_h_1 (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (1) });\n+}\n+\n+/*\n+** qi_ext_h_1_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\text\t\\3\\.b, \\3\\.b, \\2\\.b, #2\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x1\\]\n+**\text\t\\4\\.b, \\4\\.b, \\5\\.b, #2\n+**\tst1b\t\\4\\.h, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v128qi\n+qi_ext_h_1_two_op (v128qi x, v128qi y)\n+{\n+  return __builtin_shuffle (x, y, (v128qi) { PERM6 (1) });\n+}\n+\n+/*\n+** qi_ext_h_127:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #254\n+**\tst1b\t\\2\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_ext_h_127 (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (127) });\n+}\n+\n+/*\n+** qi_ext_s_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #4\n+**\tst1b\t\\2\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64qi\n+qi_ext_s_1 (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (1) });\n+}\n+\n+/*\n+** qi_ext_s_63:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #252\n+**\tst1b\t\\2\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64qi\n+qi_ext_s_63 (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (63) });\n+}\n+\n+/*\n+** qi_ext_d_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #8\n+**\tst1b\t\\2\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32qi\n+qi_ext_d_1 (v32qi x)\n+{\n+  return __builtin_shuffle (x, x, (v32qi) { PERM4 (1) });\n+}\n+\n+/*\n+** qi_ext_d_31:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #248\n+**\tst1b\t\\2\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32qi\n+qi_ext_d_31 (v32qi x)\n+{\n+  return __builtin_shuffle (x, x, (v32qi) { PERM4 (31) });\n+}\n+\n+/*\n+** hi_ext_s_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #4\n+**\tst1h\t\\2\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hi\n+hi_ext_s_1 (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (1) });\n+}\n+\n+/*\n+** hi_ext_s_63:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #252\n+**\tst1h\t\\2\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hi\n+hi_ext_s_63 (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (63) });\n+}\n+\n+/*\n+** hf_ext_s_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #4\n+**\tst1h\t\\2\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hf\n+hf_ext_s_1 (v64hf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (1) });\n+}\n+\n+/*\n+** hf_ext_s_60:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #240\n+**\tst1h\t\\2\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hf\n+hf_ext_s_60 (v64hf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (60) });\n+}\n+\n+/*\n+** bf_ext_s_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #4\n+**\tst1h\t\\2\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64bf\n+bf_ext_s_1 (v64bf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (1) });\n+}\n+\n+/*\n+** bf_ext_s_40:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #160\n+**\tst1h\t\\2\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64bf\n+bf_ext_s_40 (v64bf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (40) });\n+}\n+\n+/*\n+** hi_ext_d_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #8\n+**\tst1h\t\\2\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hi\n+hi_ext_d_1 (v32hi x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (1) });\n+}\n+\n+/*\n+** hi_ext_d_31:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #248\n+**\tst1h\t\\2\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hi\n+hi_ext_d_31 (v32hi x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (31) });\n+}\n+\n+/*\n+** hf_ext_d_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #8\n+**\tst1h\t\\2\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hf\n+hf_ext_d_1 (v32hf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (1) });\n+}\n+\n+/*\n+** hf_ext_d_18:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #144\n+**\tst1h\t\\2\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hf\n+hf_ext_d_18 (v32hf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (18) });\n+}\n+\n+/*\n+** bf_ext_d_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #8\n+**\tst1h\t\\2\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32bf\n+bf_ext_d_1 (v32bf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (1) });\n+}\n+\n+/*\n+** bf_ext_d_7:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #56\n+**\tst1h\t\\2\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32bf\n+bf_ext_d_7 (v32bf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (7) });\n+}\n+\n+/*\n+** si_ext_d_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #8\n+**\tst1w\t\\2\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32si\n+si_ext_d_1 (v32si x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (1) });\n+}\n+\n+/*\n+** si_ext_d_31:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #248\n+**\tst1w\t\\2\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32si\n+si_ext_d_31 (v32si x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (31) });\n+}\n+\n+/*\n+** sf_ext_d_1:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #8\n+**\tst1w\t\\2\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32sf\n+sf_ext_d_1 (v32sf x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (1) });\n+}\n+\n+/*\n+** sf_ext_d_31:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\text\t\\2\\.b, \\2\\.b, \\2\\.b, #248\n+**\tst1w\t\\2\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32sf\n+sf_ext_d_31 (v32sf x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (31) });\n+}"}, {"sha": "417da37500ad498995c043ab8fcf8df337c74591", "filename": "gcc/testsuite/gcc.target/aarch64/sve/rev_2.c", "status": "added", "additions": 177, "deletions": 0, "changes": 177, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Frev_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Frev_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Frev_2.c?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -0,0 +1,177 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O -msve-vector-bits=2048 -mlittle-endian --save-temps\" } */\n+/* { dg-final { check-function-bodies \"**\" \"\" } } */\n+\n+typedef unsigned char v128qi __attribute__((vector_size(128)));\n+typedef unsigned char v64qi __attribute__((vector_size(64)));\n+typedef unsigned char v32qi __attribute__((vector_size(32)));\n+typedef unsigned short v64hi __attribute__((vector_size(128)));\n+typedef unsigned short v32hi __attribute__((vector_size(64)));\n+typedef _Float16 v64hf __attribute__((vector_size(128)));\n+typedef _Float16 v32hf __attribute__((vector_size(64)));\n+typedef __bf16 v64bf __attribute__((vector_size(128)));\n+typedef __bf16 v32bf __attribute__((vector_size(64)));\n+typedef unsigned int v32si __attribute__((vector_size(128)));\n+typedef float v32sf __attribute__((vector_size(128)));\n+\n+#define PERM0(B) B, B - 1\n+#define PERM1(B) PERM0 (B), PERM0 (B - 2)\n+#define PERM2(B) PERM1 (B), PERM1 (B - 4)\n+#define PERM3(B) PERM2 (B), PERM2 (B - 8)\n+#define PERM4(B) PERM3 (B), PERM3 (B - 16)\n+#define PERM5(B) PERM4 (B), PERM4 (B - 32)\n+#define PERM6(B) PERM5 (B), PERM5 (B - 64)\n+\n+/*\n+** qi_rev_h:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\trev\t(z[0-9]+)\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_rev_h (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (127) });\n+}\n+\n+/*\n+** qi_rev_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\trev\t(z[0-9]+)\\.s, \\2\\.s\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64qi\n+qi_rev_s (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (63) });\n+}\n+\n+/*\n+** qi_rev_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\trev\t(z[0-9]+)\\.d, \\2\\.d\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32qi\n+qi_rev_d (v32qi x)\n+{\n+  return __builtin_shuffle (x, x, (v32qi) { PERM4 (31) });\n+}\n+\n+/*\n+** hi_rev_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\trev\t(z[0-9]+)\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hi\n+hi_rev_s (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (63) });\n+}\n+\n+/*\n+** hf_rev_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\trev\t(z[0-9]+)\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hf\n+hf_rev_s (v64hf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (63) });\n+}\n+\n+/*\n+** bf_rev_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\trev\t(z[0-9]+)\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64bf\n+bf_rev_s (v64bf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (63) });\n+}\n+\n+/*\n+** hi_rev_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\trev\t(z[0-9]+)\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hi\n+hi_rev_d (v32hi x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (31) });\n+}\n+\n+/*\n+** hf_rev_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\trev\t(z[0-9]+)\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hf\n+hf_rev_d (v32hf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (31) });\n+}\n+\n+/*\n+** bf_rev_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\trev\t(z[0-9]+)\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32bf\n+bf_rev_d (v32bf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (31) });\n+}\n+\n+/*\n+** si_rev_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\trev\t(z[0-9]+)\\.d, \\2\\.d\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32si\n+si_rev_d (v32si x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (31) });\n+}\n+\n+/*\n+** sf_rev_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\trev\t(z[0-9]+)\\.d, \\2\\.d\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32sf\n+sf_rev_d (v32sf x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (31) });\n+}"}, {"sha": "62de8127584f1d1913ff6491f50cae4127873e40", "filename": "gcc/testsuite/gcc.target/aarch64/sve/revhw_1.c", "status": "added", "additions": 127, "deletions": 0, "changes": 127, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Frevhw_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Frevhw_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Frevhw_1.c?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -0,0 +1,127 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O -msve-vector-bits=2048 -mlittle-endian --save-temps\" } */\n+/* { dg-final { check-function-bodies \"**\" \"\" } } */\n+\n+typedef unsigned char v128qi __attribute__((vector_size(128)));\n+typedef unsigned char v64qi __attribute__((vector_size(64)));\n+typedef unsigned short v64hi __attribute__((vector_size(128)));\n+typedef _Float16 v64hf __attribute__((vector_size(128)));\n+typedef __bf16 v64bf __attribute__((vector_size(128)));\n+\n+#define PERM0(B) B + 1, B\n+#define PERM1(B) PERM0 (B), PERM0 (B + 2)\n+#define PERM2(B) PERM1 (B), PERM1 (B + 4)\n+#define PERM3(B) PERM2 (B), PERM2 (B + 8)\n+#define PERM4(B) PERM3 (B), PERM3 (B + 16)\n+#define PERM5(B) PERM4 (B), PERM4 (B + 32)\n+#define PERM6(B) PERM5 (B), PERM5 (B + 64)\n+\n+/*\n+** qi_revh_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\trevh\t(z[0-9]+)\\.s, \\1/m, \\2\\.s\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_revh_s (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (0) });\n+}\n+\n+/*\n+** qi_revw_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\trevw\t(z[0-9]+)\\.d, \\1/m, \\2\\.d\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64qi\n+qi_revw_d (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (0) });\n+}\n+\n+/*\n+** hi_revw_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\trevw\t(z[0-9]+)\\.d, \\1/m, \\2\\.d\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hi\n+hi_revw_d (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0) });\n+}\n+\n+/*\n+** hf_revw_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\trevw\t(z[0-9]+)\\.d, \\1/m, \\2\\.d\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hf\n+hf_revw_d (v64hf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0) });\n+}\n+\n+/*\n+** bf_revw_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\trevw\t(z[0-9]+)\\.d, \\1/m, \\2\\.d\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64bf\n+bf_revw_d (v64bf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0) });\n+}\n+\n+#undef PERM1\n+#define PERM1(B) PERM0 (B + 2), PERM0 (B)\n+\n+/*\n+** qi_revh_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\trevh\t(z[0-9]+)\\.d, \\1/m, \\2\\.d\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_revh_d (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (0) });\n+}\n+\n+v64qi\n+qi_revw_q (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (0) });\n+}\n+\n+v64hi\n+hi_revw_q (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0) });\n+}\n+\n+#undef PERM2\n+#define PERM2(B) PERM0 (B + 4), PERM0 (B)\n+\n+v128qi\n+qi_revh_q (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (0) });\n+}\n+\n+/* { dg-final { scan-assembler-times {\\trev.\\t} 6 } } */"}, {"sha": "7634d01b2c42176b22a5f5ebe44ef625a429193a", "filename": "gcc/testsuite/gcc.target/aarch64/sve/revhw_2.c", "status": "added", "additions": 127, "deletions": 0, "changes": 127, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Frevhw_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Frevhw_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Frevhw_2.c?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -0,0 +1,127 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O -msve-vector-bits=2048 -mbig-endian --save-temps\" } */\n+/* { dg-final { check-function-bodies \"**\" \"\" } } */\n+\n+typedef unsigned char v128qi __attribute__((vector_size(128)));\n+typedef unsigned char v64qi __attribute__((vector_size(64)));\n+typedef unsigned short v64hi __attribute__((vector_size(128)));\n+typedef _Float16 v64hf __attribute__((vector_size(128)));\n+typedef __bf16 v64bf __attribute__((vector_size(128)));\n+\n+#define PERM0(B) B + 1, B\n+#define PERM1(B) PERM0 (B), PERM0 (B + 2)\n+#define PERM2(B) PERM1 (B), PERM1 (B + 4)\n+#define PERM3(B) PERM2 (B), PERM2 (B + 8)\n+#define PERM4(B) PERM3 (B), PERM3 (B + 16)\n+#define PERM5(B) PERM4 (B), PERM4 (B + 32)\n+#define PERM6(B) PERM5 (B), PERM5 (B + 64)\n+\n+/*\n+** qi_revh_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\trevh\t(z[0-9]+)\\.s, \\1/m, \\2\\.s\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_revh_s (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (0) });\n+}\n+\n+/*\n+** qi_revw_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\trevw\t(z[0-9]+)\\.d, \\1/m, \\2\\.d\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64qi\n+qi_revw_d (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (0) });\n+}\n+\n+/*\n+** hi_revw_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\trevw\t(z[0-9]+)\\.d, \\1/m, \\2\\.d\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hi\n+hi_revw_d (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0) });\n+}\n+\n+/*\n+** hf_revw_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\trevw\t(z[0-9]+)\\.d, \\1/m, \\2\\.d\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hf\n+hf_revw_d (v64hf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0) });\n+}\n+\n+/*\n+** bf_revw_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\trevw\t(z[0-9]+)\\.d, \\1/m, \\2\\.d\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64bf\n+bf_revw_d (v64bf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0) });\n+}\n+\n+#undef PERM1\n+#define PERM1(B) PERM0 (B + 2), PERM0 (B)\n+\n+/*\n+** qi_revh_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\trevh\t(z[0-9]+)\\.d, \\1/m, \\2\\.d\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_revh_d (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (0) });\n+}\n+\n+v64qi\n+qi_revw_q (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (0) });\n+}\n+\n+v64hi\n+hi_revw_q (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0) });\n+}\n+\n+#undef PERM2\n+#define PERM2(B) PERM0 (B + 4), PERM0 (B)\n+\n+v128qi\n+qi_revh_q (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (0) });\n+}\n+\n+/* { dg-final { scan-assembler-times {\\trev.\\t} 6 } } */"}, {"sha": "fe25000b0bf89a26d0c6328e15daa3f099b18ebd", "filename": "gcc/testsuite/gcc.target/aarch64/sve/slp_perm_8.c", "status": "added", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fslp_perm_8.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fslp_perm_8.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fslp_perm_8.c?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -0,0 +1,18 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-O2 -ftree-vectorize\" } */\n+\n+void\n+f (short *restrict s, signed char *restrict c)\n+{\n+  for (int i = 0; i < 8; i += 2)\n+    {\n+      s[i] = c[i];\n+      s[i + 1] = c[i];\n+    }\n+}\n+\n+/* Ideally this would use LD1SB, but currently we use LD1B and\n+   sign-extend it after the permute.  */\n+/* { dg-final { scan-assembler {\\tptrue\\tp[0-7]\\.h, vl6\\n} } } */\n+/* { dg-final { scan-assembler {\\tld1s?b\\tz[0-9]+\\.h} } } */\n+/* { dg-final { scan-assembler {\\ttrn1\\tz[0-9]+\\.h,} } } */"}, {"sha": "df059ddbc8d98715a58a2d805c5f1ff694510d75", "filename": "gcc/testsuite/gcc.target/aarch64/sve/trn1_2.c", "status": "added", "additions": 403, "deletions": 0, "changes": 403, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Ftrn1_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Ftrn1_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Ftrn1_2.c?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -0,0 +1,403 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O -msve-vector-bits=2048 -mlittle-endian --save-temps\" } */\n+/* { dg-final { check-function-bodies \"**\" \"\" } } */\n+\n+typedef unsigned char v128qi __attribute__((vector_size(128)));\n+typedef unsigned char v64qi __attribute__((vector_size(64)));\n+typedef unsigned char v32qi __attribute__((vector_size(32)));\n+typedef unsigned short v64hi __attribute__((vector_size(128)));\n+typedef unsigned short v32hi __attribute__((vector_size(64)));\n+typedef _Float16 v64hf __attribute__((vector_size(128)));\n+typedef _Float16 v32hf __attribute__((vector_size(64)));\n+typedef __bf16 v64bf __attribute__((vector_size(128)));\n+typedef __bf16 v32bf __attribute__((vector_size(64)));\n+typedef unsigned int v32si __attribute__((vector_size(128)));\n+typedef float v32sf __attribute__((vector_size(128)));\n+\n+#define PERM0(B, C) B, B + C\n+#define PERM1(B, C) PERM0 (B, C), PERM0 (B + 2, C)\n+#define PERM2(B, C) PERM1 (B, C), PERM1 (B + 4, C)\n+#define PERM3(B, C) PERM2 (B, C), PERM2 (B + 8, C)\n+#define PERM4(B, C) PERM3 (B, C), PERM3 (B + 16, C)\n+#define PERM5(B, C) PERM4 (B, C), PERM4 (B + 32, C)\n+#define PERM6(B, C) PERM5 (B, C), PERM5 (B + 64, C)\n+\n+/*\n+** qi_trn1_h_a:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\ttrn1\t(z[0-9]+)\\.h, \\2\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_trn1_h_a (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (0, 0) });\n+}\n+\n+/*\n+** qi_trn1_h_b:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\ttrn1\t(z[0-9]+)\\.h, \\2\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_trn1_h_b (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (0, 128) });\n+}\n+\n+/*\n+** qi_trn1_h_c:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\ttrn1\t(z[0-9]+)\\.h, \\2\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_trn1_h_c (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (128, 0) });\n+}\n+\n+/*\n+** qi_trn1_h_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\ttrn1\t\\3\\.h, \\3\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x1\\]\n+**\ttrn1\t\\4\\.h, \\4\\.h, \\5\\.h\n+**\tst1b\t\\4\\.h, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v128qi\n+qi_trn1_h_two_op (v128qi x, v128qi y)\n+{\n+  return __builtin_shuffle (x, y, (v128qi) { PERM6 (0, 128) });\n+}\n+\n+/*\n+** qi_trn1_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn1\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64qi\n+qi_trn1_s (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** qi_trn1_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn1\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\ttrn1\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1b\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64qi\n+qi_trn1_s_two_op (v64qi x, v64qi y)\n+{\n+  return __builtin_shuffle (x, y, (v64qi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** qi_trn1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32qi\n+qi_trn1_d (v32qi x)\n+{\n+  return __builtin_shuffle (x, x, (v32qi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** qi_trn1_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn1\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\ttrn1\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1b\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32qi\n+qi_trn1_d_two_op (v32qi x, v32qi y)\n+{\n+  return __builtin_shuffle (x, y, (v32qi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** hi_trn1_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn1\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hi\n+hi_trn1_s (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** hi_trn1_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn1\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\ttrn1\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64hi\n+hi_trn1_s_two_op (v64hi x, v64hi y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** hf_trn1_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn1\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hf\n+hf_trn1_s (v64hf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** hf_trn1_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn1\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\ttrn1\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64hf\n+hf_trn1_s_two_op (v64hf x, v64hf y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** bf_trn1_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn1\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64bf\n+bf_trn1_s (v64bf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** bf_trn1_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn1\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\ttrn1\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64bf\n+bf_trn1_s_two_op (v64bf x, v64bf y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** hi_trn1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hi\n+hi_trn1_d (v32hi x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** hi_trn1_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn1\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\ttrn1\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32hi\n+hi_trn1_d_two_op (v32hi x, v32hi y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** hf_trn1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hf\n+hf_trn1_d (v32hf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** hf_trn1_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn1\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\ttrn1\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32hf\n+hf_trn1_d_two_op (v32hf x, v32hf y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** bf_trn1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32bf\n+bf_trn1_d (v32bf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** bf_trn1_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn1\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\ttrn1\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32bf\n+bf_trn1_d_two_op (v32bf x, v32bf y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** si_trn1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32si\n+si_trn1_d (v32si x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** sf_trn1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32sf\n+sf_trn1_d (v32sf x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (0, 32) });\n+}"}, {"sha": "290ce8e980ce320f00274268ab0688683294f569", "filename": "gcc/testsuite/gcc.target/aarch64/sve/trn2_2.c", "status": "added", "additions": 403, "deletions": 0, "changes": 403, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Ftrn2_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Ftrn2_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Ftrn2_2.c?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -0,0 +1,403 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O -msve-vector-bits=2048 -mlittle-endian --save-temps\" } */\n+/* { dg-final { check-function-bodies \"**\" \"\" } } */\n+\n+typedef unsigned char v128qi __attribute__((vector_size(128)));\n+typedef unsigned char v64qi __attribute__((vector_size(64)));\n+typedef unsigned char v32qi __attribute__((vector_size(32)));\n+typedef unsigned short v64hi __attribute__((vector_size(128)));\n+typedef unsigned short v32hi __attribute__((vector_size(64)));\n+typedef _Float16 v64hf __attribute__((vector_size(128)));\n+typedef _Float16 v32hf __attribute__((vector_size(64)));\n+typedef __bf16 v64bf __attribute__((vector_size(128)));\n+typedef __bf16 v32bf __attribute__((vector_size(64)));\n+typedef unsigned int v32si __attribute__((vector_size(128)));\n+typedef float v32sf __attribute__((vector_size(128)));\n+\n+#define PERM0(B, C) B, B + C\n+#define PERM1(B, C) PERM0 (B, C), PERM0 (B + 2, C)\n+#define PERM2(B, C) PERM1 (B, C), PERM1 (B + 4, C)\n+#define PERM3(B, C) PERM2 (B, C), PERM2 (B + 8, C)\n+#define PERM4(B, C) PERM3 (B, C), PERM3 (B + 16, C)\n+#define PERM5(B, C) PERM4 (B, C), PERM4 (B + 32, C)\n+#define PERM6(B, C) PERM5 (B, C), PERM5 (B + 64, C)\n+\n+/*\n+** qi_trn2_h_a:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\ttrn2\t(z[0-9]+)\\.h, \\2\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_trn2_h_a (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (1, 0) });\n+}\n+\n+/*\n+** qi_trn2_h_b:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\ttrn2\t(z[0-9]+)\\.h, \\2\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_trn2_h_b (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (1, 128) });\n+}\n+\n+/*\n+** qi_trn2_h_c:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\ttrn2\t(z[0-9]+)\\.h, \\2\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_trn2_h_c (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (1, 0) });\n+}\n+\n+/*\n+** qi_trn2_h_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\ttrn2\t\\3\\.h, \\3\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x1\\]\n+**\ttrn2\t\\4\\.h, \\4\\.h, \\5\\.h\n+**\tst1b\t\\4\\.h, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v128qi\n+qi_trn2_h_two_op (v128qi x, v128qi y)\n+{\n+  return __builtin_shuffle (x, y, (v128qi) { PERM6 (1, 128) });\n+}\n+\n+/*\n+** qi_trn2_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn2\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64qi\n+qi_trn2_s (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (1, 64) });\n+}\n+\n+/*\n+** qi_trn2_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn2\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\ttrn2\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1b\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64qi\n+qi_trn2_s_two_op (v64qi x, v64qi y)\n+{\n+  return __builtin_shuffle (x, y, (v64qi) { PERM5 (1, 64) });\n+}\n+\n+/*\n+** qi_trn2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32qi\n+qi_trn2_d (v32qi x)\n+{\n+  return __builtin_shuffle (x, x, (v32qi) { PERM4 (1, 32) });\n+}\n+\n+/*\n+** qi_trn2_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn2\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\ttrn2\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1b\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32qi\n+qi_trn2_d_two_op (v32qi x, v32qi y)\n+{\n+  return __builtin_shuffle (x, y, (v32qi) { PERM4 (1, 32) });\n+}\n+\n+/*\n+** hi_trn2_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn2\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hi\n+hi_trn2_s (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (1, 64) });\n+}\n+\n+/*\n+** hi_trn2_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn2\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\ttrn2\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64hi\n+hi_trn2_s_two_op (v64hi x, v64hi y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (1, 64) });\n+}\n+\n+/*\n+** hf_trn2_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn2\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hf\n+hf_trn2_s (v64hf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (1, 64) });\n+}\n+\n+/*\n+** hf_trn2_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn2\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\ttrn2\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64hf\n+hf_trn2_s_two_op (v64hf x, v64hf y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (1, 64) });\n+}\n+\n+/*\n+** bf_trn2_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn2\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64bf\n+bf_trn2_s (v64bf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (1, 64) });\n+}\n+\n+/*\n+** bf_trn2_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\ttrn2\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\ttrn2\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64bf\n+bf_trn2_s_two_op (v64bf x, v64bf y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (1, 64) });\n+}\n+\n+/*\n+** hi_trn2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hi\n+hi_trn2_d (v32hi x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (1, 32) });\n+}\n+\n+/*\n+** hi_trn2_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn2\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\ttrn2\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32hi\n+hi_trn2_d_two_op (v32hi x, v32hi y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (1, 32) });\n+}\n+\n+/*\n+** hf_trn2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hf\n+hf_trn2_d (v32hf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (1, 32) });\n+}\n+\n+/*\n+** hf_trn2_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn2\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\ttrn2\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32hf\n+hf_trn2_d_two_op (v32hf x, v32hf y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (1, 32) });\n+}\n+\n+/*\n+** bf_trn2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32bf\n+bf_trn2_d (v32bf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (1, 32) });\n+}\n+\n+/*\n+** bf_trn2_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn2\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\ttrn2\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32bf\n+bf_trn2_d_two_op (v32bf x, v32bf y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (1, 32) });\n+}\n+\n+/*\n+** si_trn2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32si\n+si_trn2_d (v32si x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (1, 32) });\n+}\n+\n+/*\n+** sf_trn2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\ttrn2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32sf\n+sf_trn2_d (v32sf x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (1, 32) });\n+}"}, {"sha": "e2f2692c7cfeb76275bd52fd8c85c1699c466140", "filename": "gcc/testsuite/gcc.target/aarch64/sve/uzp1_2.c", "status": "added", "additions": 375, "deletions": 0, "changes": 375, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fuzp1_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fuzp1_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fuzp1_2.c?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -0,0 +1,375 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O -msve-vector-bits=2048 -mlittle-endian --save-temps\" } */\n+/* { dg-final { check-function-bodies \"**\" \"\" } } */\n+\n+typedef unsigned char v128qi __attribute__((vector_size(128)));\n+typedef unsigned char v64qi __attribute__((vector_size(64)));\n+typedef unsigned char v32qi __attribute__((vector_size(32)));\n+typedef unsigned short v64hi __attribute__((vector_size(128)));\n+typedef unsigned short v32hi __attribute__((vector_size(64)));\n+typedef _Float16 v64hf __attribute__((vector_size(128)));\n+typedef _Float16 v32hf __attribute__((vector_size(64)));\n+typedef __bf16 v64bf __attribute__((vector_size(128)));\n+typedef __bf16 v32bf __attribute__((vector_size(64)));\n+typedef unsigned int v32si __attribute__((vector_size(128)));\n+typedef float v32sf __attribute__((vector_size(128)));\n+\n+#define PERM0(B) B, B + 2\n+#define PERM1(B) PERM0 (B), PERM0 (B + 4)\n+#define PERM2(B) PERM1 (B), PERM1 (B + 8)\n+#define PERM3(B) PERM2 (B), PERM2 (B + 16)\n+#define PERM4(B) PERM3 (B), PERM3 (B + 32)\n+#define PERM5(B) PERM4 (B), PERM4 (B + 64)\n+#define PERM6(B) PERM5 (B), PERM5 (B + 128)\n+\n+/*\n+** qi_uzp1_h:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tuzp1\t(z[0-9]+)\\.h, \\2\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_uzp1_h (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (0) });\n+}\n+\n+/*\n+** qi_uzp1_h_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tuzp1\t\\3\\.h, \\3\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x1\\]\n+**\tuzp1\t\\4\\.h, \\4\\.h, \\5\\.h\n+**\tst1b\t\\4\\.h, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v128qi\n+qi_uzp1_h_two_op (v128qi x, v128qi y)\n+{\n+  return __builtin_shuffle (x, y, (v128qi) { PERM6 (0) });\n+}\n+\n+/*\n+** qi_uzp1_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp1\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64qi\n+qi_uzp1_s (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (0) });\n+}\n+\n+/*\n+** qi_uzp1_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp1\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tuzp1\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1b\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64qi\n+qi_uzp1_s_two_op (v64qi x, v64qi y)\n+{\n+  return __builtin_shuffle (x, y, (v64qi) { PERM5 (0) });\n+}\n+\n+/*\n+** qi_uzp1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32qi\n+qi_uzp1_d (v32qi x)\n+{\n+  return __builtin_shuffle (x, x, (v32qi) { PERM4 (0) });\n+}\n+\n+/*\n+** qi_uzp1_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp1\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tuzp1\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1b\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32qi\n+qi_uzp1_d_two_op (v32qi x, v32qi y)\n+{\n+  return __builtin_shuffle (x, y, (v32qi) { PERM4 (0) });\n+}\n+\n+/*\n+** hi_uzp1_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp1\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hi\n+hi_uzp1_s (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0) });\n+}\n+\n+/*\n+** hi_uzp1_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp1\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tuzp1\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64hi\n+hi_uzp1_s_two_op (v64hi x, v64hi y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (0) });\n+}\n+\n+/*\n+** hf_uzp1_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp1\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hf\n+hf_uzp1_s (v64hf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0) });\n+}\n+\n+/*\n+** hf_uzp1_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp1\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tuzp1\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64hf\n+hf_uzp1_s_two_op (v64hf x, v64hf y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (0) });\n+}\n+\n+/*\n+** bf_uzp1_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp1\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64bf\n+bf_uzp1_s (v64bf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0) });\n+}\n+\n+/*\n+** bf_uzp1_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp1\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tuzp1\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64bf\n+bf_uzp1_s_two_op (v64bf x, v64bf y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (0) });\n+}\n+\n+/*\n+** hi_uzp1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hi\n+hi_uzp1_d (v32hi x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (0) });\n+}\n+\n+/*\n+** hi_uzp1_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp1\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tuzp1\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32hi\n+hi_uzp1_d_two_op (v32hi x, v32hi y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (0) });\n+}\n+\n+/*\n+** hf_uzp1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hf\n+hf_uzp1_d (v32hf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (0) });\n+}\n+\n+/*\n+** hf_uzp1_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp1\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tuzp1\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32hf\n+hf_uzp1_d_two_op (v32hf x, v32hf y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (0) });\n+}\n+\n+/*\n+** bf_uzp1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32bf\n+bf_uzp1_d (v32bf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (0) });\n+}\n+\n+/*\n+** bf_uzp1_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp1\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tuzp1\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32bf\n+bf_uzp1_d_two_op (v32bf x, v32bf y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (0) });\n+}\n+\n+/*\n+** si_uzp1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32si\n+si_uzp1_d (v32si x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (0) });\n+}\n+\n+/*\n+** sf_uzp1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32sf\n+sf_uzp1_d (v32sf x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (0) });\n+}"}, {"sha": "0d8eda567cf142c22ac50163a66be4678246208b", "filename": "gcc/testsuite/gcc.target/aarch64/sve/uzp2_2.c", "status": "added", "additions": 375, "deletions": 0, "changes": 375, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fuzp2_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fuzp2_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fuzp2_2.c?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -0,0 +1,375 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O -msve-vector-bits=2048 -mlittle-endian --save-temps\" } */\n+/* { dg-final { check-function-bodies \"**\" \"\" } } */\n+\n+typedef unsigned char v128qi __attribute__((vector_size(128)));\n+typedef unsigned char v64qi __attribute__((vector_size(64)));\n+typedef unsigned char v32qi __attribute__((vector_size(32)));\n+typedef unsigned short v64hi __attribute__((vector_size(128)));\n+typedef unsigned short v32hi __attribute__((vector_size(64)));\n+typedef _Float16 v64hf __attribute__((vector_size(128)));\n+typedef _Float16 v32hf __attribute__((vector_size(64)));\n+typedef __bf16 v64bf __attribute__((vector_size(128)));\n+typedef __bf16 v32bf __attribute__((vector_size(64)));\n+typedef unsigned int v32si __attribute__((vector_size(128)));\n+typedef float v32sf __attribute__((vector_size(128)));\n+\n+#define PERM0(B) B, B + 2\n+#define PERM1(B) PERM0 (B), PERM0 (B + 4)\n+#define PERM2(B) PERM1 (B), PERM1 (B + 8)\n+#define PERM3(B) PERM2 (B), PERM2 (B + 16)\n+#define PERM4(B) PERM3 (B), PERM3 (B + 32)\n+#define PERM5(B) PERM4 (B), PERM4 (B + 64)\n+#define PERM6(B) PERM5 (B), PERM5 (B + 128)\n+\n+/*\n+** qi_uzp2_h:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tuzp2\t(z[0-9]+)\\.h, \\2\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_uzp2_h (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (1) });\n+}\n+\n+/*\n+** qi_uzp2_h_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tuzp2\t\\3\\.h, \\3\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x1\\]\n+**\tuzp2\t\\4\\.h, \\4\\.h, \\5\\.h\n+**\tst1b\t\\4\\.h, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v128qi\n+qi_uzp2_h_two_op (v128qi x, v128qi y)\n+{\n+  return __builtin_shuffle (x, y, (v128qi) { PERM6 (1) });\n+}\n+\n+/*\n+** qi_uzp2_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp2\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64qi\n+qi_uzp2_s (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (1) });\n+}\n+\n+/*\n+** qi_uzp2_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp2\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tuzp2\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1b\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64qi\n+qi_uzp2_s_two_op (v64qi x, v64qi y)\n+{\n+  return __builtin_shuffle (x, y, (v64qi) { PERM5 (1) });\n+}\n+\n+/*\n+** qi_uzp2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32qi\n+qi_uzp2_d (v32qi x)\n+{\n+  return __builtin_shuffle (x, x, (v32qi) { PERM4 (1) });\n+}\n+\n+/*\n+** qi_uzp2_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp2\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tuzp2\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1b\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32qi\n+qi_uzp2_d_two_op (v32qi x, v32qi y)\n+{\n+  return __builtin_shuffle (x, y, (v32qi) { PERM4 (1) });\n+}\n+\n+/*\n+** hi_uzp2_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp2\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hi\n+hi_uzp2_s (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (1) });\n+}\n+\n+/*\n+** hi_uzp2_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp2\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tuzp2\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64hi\n+hi_uzp2_s_two_op (v64hi x, v64hi y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (1) });\n+}\n+\n+/*\n+** hf_uzp2_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp2\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hf\n+hf_uzp2_s (v64hf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (1) });\n+}\n+\n+/*\n+** hf_uzp2_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp2\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tuzp2\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64hf\n+hf_uzp2_s_two_op (v64hf x, v64hf y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (1) });\n+}\n+\n+/*\n+** bf_uzp2_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp2\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64bf\n+bf_uzp2_s (v64bf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (1) });\n+}\n+\n+/*\n+** bf_uzp2_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tuzp2\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tuzp2\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64bf\n+bf_uzp2_s_two_op (v64bf x, v64bf y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (1) });\n+}\n+\n+/*\n+** hi_uzp2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hi\n+hi_uzp2_d (v32hi x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (1) });\n+}\n+\n+/*\n+** hi_uzp2_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp2\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tuzp2\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32hi\n+hi_uzp2_d_two_op (v32hi x, v32hi y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (1) });\n+}\n+\n+/*\n+** hf_uzp2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hf\n+hf_uzp2_d (v32hf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (1) });\n+}\n+\n+/*\n+** hf_uzp2_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp2\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tuzp2\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32hf\n+hf_uzp2_d_two_op (v32hf x, v32hf y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (1) });\n+}\n+\n+/*\n+** bf_uzp2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32bf\n+bf_uzp2_d (v32bf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (1) });\n+}\n+\n+/*\n+** bf_uzp2_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp2\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tuzp2\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32bf\n+bf_uzp2_d_two_op (v32bf x, v32bf y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (1) });\n+}\n+\n+/*\n+** si_uzp2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32si\n+si_uzp2_d (v32si x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (1) });\n+}\n+\n+/*\n+** sf_uzp2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tuzp2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32sf\n+sf_uzp2_d (v32sf x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (1) });\n+}"}, {"sha": "395b96f5f0d649f6768a64fac9644e88ba439161", "filename": "gcc/testsuite/gcc.target/aarch64/sve/zip1_2.c", "status": "added", "additions": 403, "deletions": 0, "changes": 403, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fzip1_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fzip1_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fzip1_2.c?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -0,0 +1,403 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O -msve-vector-bits=2048 -mlittle-endian --save-temps\" } */\n+/* { dg-final { check-function-bodies \"**\" \"\" } } */\n+\n+typedef unsigned char v128qi __attribute__((vector_size(128)));\n+typedef unsigned char v64qi __attribute__((vector_size(64)));\n+typedef unsigned char v32qi __attribute__((vector_size(32)));\n+typedef unsigned short v64hi __attribute__((vector_size(128)));\n+typedef unsigned short v32hi __attribute__((vector_size(64)));\n+typedef _Float16 v64hf __attribute__((vector_size(128)));\n+typedef _Float16 v32hf __attribute__((vector_size(64)));\n+typedef __bf16 v64bf __attribute__((vector_size(128)));\n+typedef __bf16 v32bf __attribute__((vector_size(64)));\n+typedef unsigned int v32si __attribute__((vector_size(128)));\n+typedef float v32sf __attribute__((vector_size(128)));\n+\n+#define PERM0(B, C) B, B + C\n+#define PERM1(B, C) PERM0 (B, C), PERM0 (B + 1, C)\n+#define PERM2(B, C) PERM1 (B, C), PERM1 (B + 2, C)\n+#define PERM3(B, C) PERM2 (B, C), PERM2 (B + 4, C)\n+#define PERM4(B, C) PERM3 (B, C), PERM3 (B + 8, C)\n+#define PERM5(B, C) PERM4 (B, C), PERM4 (B + 16, C)\n+#define PERM6(B, C) PERM5 (B, C), PERM5 (B + 32, C)\n+\n+/*\n+** qi_zip1_h_a:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tzip1\t(z[0-9]+)\\.h, \\2\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_zip1_h_a (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (0, 0) });\n+}\n+\n+/*\n+** qi_zip1_h_b:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tzip1\t(z[0-9]+)\\.h, \\2\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_zip1_h_b (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (0, 128) });\n+}\n+\n+/*\n+** qi_zip1_h_c:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tzip1\t(z[0-9]+)\\.h, \\2\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_zip1_h_c (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (128, 0) });\n+}\n+\n+/*\n+** qi_zip1_h_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tzip1\t\\3\\.h, \\3\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x1\\]\n+**\tzip1\t\\4\\.h, \\4\\.h, \\5\\.h\n+**\tst1b\t\\4\\.h, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v128qi\n+qi_zip1_h_two_op (v128qi x, v128qi y)\n+{\n+  return __builtin_shuffle (x, y, (v128qi) { PERM6 (0, 128) });\n+}\n+\n+/*\n+** qi_zip1_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip1\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64qi\n+qi_zip1_s (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** qi_zip1_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip1\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tzip1\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1b\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64qi\n+qi_zip1_s_two_op (v64qi x, v64qi y)\n+{\n+  return __builtin_shuffle (x, y, (v64qi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** qi_zip1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32qi\n+qi_zip1_d (v32qi x)\n+{\n+  return __builtin_shuffle (x, x, (v32qi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** qi_zip1_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip1\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tzip1\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1b\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32qi\n+qi_zip1_d_two_op (v32qi x, v32qi y)\n+{\n+  return __builtin_shuffle (x, y, (v32qi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** hi_zip1_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip1\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hi\n+hi_zip1_s (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** hi_zip1_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip1\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tzip1\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64hi\n+hi_zip1_s_two_op (v64hi x, v64hi y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** hf_zip1_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip1\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hf\n+hf_zip1_s (v64hf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** hf_zip1_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip1\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tzip1\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64hf\n+hf_zip1_s_two_op (v64hf x, v64hf y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** bf_zip1_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip1\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64bf\n+bf_zip1_s (v64bf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** bf_zip1_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip1\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tzip1\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64bf\n+bf_zip1_s_two_op (v64bf x, v64bf y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (0, 64) });\n+}\n+\n+/*\n+** hi_zip1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hi\n+hi_zip1_d (v32hi x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** hi_zip1_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip1\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tzip1\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32hi\n+hi_zip1_d_two_op (v32hi x, v32hi y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** hf_zip1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hf\n+hf_zip1_d (v32hf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** hf_zip1_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip1\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tzip1\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32hf\n+hf_zip1_d_two_op (v32hf x, v32hf y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** bf_zip1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32bf\n+bf_zip1_d (v32bf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** bf_zip1_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip1\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tzip1\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32bf\n+bf_zip1_d_two_op (v32bf x, v32bf y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** si_zip1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32si\n+si_zip1_d (v32si x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (0, 32) });\n+}\n+\n+/*\n+** sf_zip1_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip1\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32sf\n+sf_zip1_d (v32sf x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (0, 32) });\n+}"}, {"sha": "9158ace156447314530af8cb358c92431d00a815", "filename": "gcc/testsuite/gcc.target/aarch64/sve/zip2_2.c", "status": "added", "additions": 403, "deletions": 0, "changes": 403, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fzip2_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6c3ce63b04b38f84c0357e4648383f0e3ab11cd9/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fzip2_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fzip2_2.c?ref=6c3ce63b04b38f84c0357e4648383f0e3ab11cd9", "patch": "@@ -0,0 +1,403 @@\n+/* { dg-do assemble { target aarch64_asm_sve_ok } } */\n+/* { dg-options \"-O -msve-vector-bits=2048 -mlittle-endian --save-temps\" } */\n+/* { dg-final { check-function-bodies \"**\" \"\" } } */\n+\n+typedef unsigned char v128qi __attribute__((vector_size(128)));\n+typedef unsigned char v64qi __attribute__((vector_size(64)));\n+typedef unsigned char v32qi __attribute__((vector_size(32)));\n+typedef unsigned short v64hi __attribute__((vector_size(128)));\n+typedef unsigned short v32hi __attribute__((vector_size(64)));\n+typedef _Float16 v64hf __attribute__((vector_size(128)));\n+typedef _Float16 v32hf __attribute__((vector_size(64)));\n+typedef __bf16 v64bf __attribute__((vector_size(128)));\n+typedef __bf16 v32bf __attribute__((vector_size(64)));\n+typedef unsigned int v32si __attribute__((vector_size(128)));\n+typedef float v32sf __attribute__((vector_size(128)));\n+\n+#define PERM0(B, C) B, B + C\n+#define PERM1(B, C) PERM0 (B, C), PERM0 (B + 1, C)\n+#define PERM2(B, C) PERM1 (B, C), PERM1 (B + 2, C)\n+#define PERM3(B, C) PERM2 (B, C), PERM2 (B + 4, C)\n+#define PERM4(B, C) PERM3 (B, C), PERM3 (B + 8, C)\n+#define PERM5(B, C) PERM4 (B, C), PERM4 (B + 16, C)\n+#define PERM6(B, C) PERM5 (B, C), PERM5 (B + 32, C)\n+\n+/*\n+** qi_zip2_h_a:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tzip2\t(z[0-9]+)\\.h, \\2\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_zip2_h_a (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (64, 128) });\n+}\n+\n+/*\n+** qi_zip2_h_b:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tzip2\t(z[0-9]+)\\.h, \\2\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_zip2_h_b (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (64, 128) });\n+}\n+\n+/*\n+** qi_zip2_h_c:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tzip2\t(z[0-9]+)\\.h, \\2\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+**\tret\n+*/\n+v128qi\n+qi_zip2_h_c (v128qi x)\n+{\n+  return __builtin_shuffle (x, x, (v128qi) { PERM6 (192, 0) });\n+}\n+\n+/*\n+** qi_zip2_h_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tzip2\t\\3\\.h, \\3\\.h, \\2\\.h\n+**\tst1b\t\\3\\.h, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.h, \\1/z, \\[x1\\]\n+**\tzip2\t\\4\\.h, \\4\\.h, \\5\\.h\n+**\tst1b\t\\4\\.h, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v128qi\n+qi_zip2_h_two_op (v128qi x, v128qi y)\n+{\n+  return __builtin_shuffle (x, y, (v128qi) { PERM6 (64, 128) });\n+}\n+\n+/*\n+** qi_zip2_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip2\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64qi\n+qi_zip2_s (v64qi x)\n+{\n+  return __builtin_shuffle (x, x, (v64qi) { PERM5 (32, 64) });\n+}\n+\n+/*\n+** qi_zip2_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip2\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1b\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tzip2\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1b\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64qi\n+qi_zip2_s_two_op (v64qi x, v64qi y)\n+{\n+  return __builtin_shuffle (x, y, (v64qi) { PERM5 (32, 64) });\n+}\n+\n+/*\n+** qi_zip2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32qi\n+qi_zip2_d (v32qi x)\n+{\n+  return __builtin_shuffle (x, x, (v32qi) { PERM4 (16, 32) });\n+}\n+\n+/*\n+** qi_zip2_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip2\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1b\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1b\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tzip2\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1b\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32qi\n+qi_zip2_d_two_op (v32qi x, v32qi y)\n+{\n+  return __builtin_shuffle (x, y, (v32qi) { PERM4 (16, 32) });\n+}\n+\n+/*\n+** hi_zip2_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip2\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hi\n+hi_zip2_s (v64hi x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (32, 64) });\n+}\n+\n+/*\n+** hi_zip2_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip2\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tzip2\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64hi\n+hi_zip2_s_two_op (v64hi x, v64hi y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (32, 64) });\n+}\n+\n+/*\n+** hf_zip2_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip2\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64hf\n+hf_zip2_s (v64hf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (32, 64) });\n+}\n+\n+/*\n+** hf_zip2_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip2\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tzip2\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64hf\n+hf_zip2_s_two_op (v64hf x, v64hf y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (32, 64) });\n+}\n+\n+/*\n+** bf_zip2_s:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip2\t(z[0-9]+)\\.s, \\2\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+**\tret\n+*/\n+v64bf\n+bf_zip2_s (v64bf x)\n+{\n+  return __builtin_shuffle (x, x, (v64hi) { PERM5 (32, 64) });\n+}\n+\n+/*\n+** bf_zip2_s_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tzip2\t\\3\\.s, \\3\\.s, \\2\\.s\n+**\tst1h\t\\3\\.s, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.s, \\1/z, \\[x1\\]\n+**\tzip2\t\\4\\.s, \\4\\.s, \\5\\.s\n+**\tst1h\t\\4\\.s, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v64bf\n+bf_zip2_s_two_op (v64bf x, v64bf y)\n+{\n+  return __builtin_shuffle (x, y, (v64hi) { PERM5 (32, 64) });\n+}\n+\n+/*\n+** hi_zip2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hi\n+hi_zip2_d (v32hi x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (16, 32) });\n+}\n+\n+/*\n+** hi_zip2_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip2\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tzip2\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32hi\n+hi_zip2_d_two_op (v32hi x, v32hi y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (16, 32) });\n+}\n+\n+/*\n+** hf_zip2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32hf\n+hf_zip2_d (v32hf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (16, 32) });\n+}\n+\n+/*\n+** hf_zip2_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip2\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tzip2\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32hf\n+hf_zip2_d_two_op (v32hf x, v32hf y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (16, 32) });\n+}\n+\n+/*\n+** bf_zip2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32bf\n+bf_zip2_d (v32bf x)\n+{\n+  return __builtin_shuffle (x, x, (v32hi) { PERM4 (16, 32) });\n+}\n+\n+/*\n+** bf_zip2_d_two_op:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+** (\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip2\t\\3\\.d, \\3\\.d, \\2\\.d\n+**\tst1h\t\\3\\.d, \\1, \\[x8\\]\n+** |\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tld1h\t(z[0-9]+)\\.d, \\1/z, \\[x1\\]\n+**\tzip2\t\\4\\.d, \\4\\.d, \\5\\.d\n+**\tst1h\t\\4\\.d, \\1, \\[x8\\]\n+** )\n+**\tret\n+*/\n+v32bf\n+bf_zip2_d_two_op (v32bf x, v32bf y)\n+{\n+  return __builtin_shuffle (x, y, (v32hi) { PERM4 (16, 32) });\n+}\n+\n+/*\n+** si_zip2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32si\n+si_zip2_d (v32si x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (16, 32) });\n+}\n+\n+/*\n+** sf_zip2_d:\n+**\tptrue\t(p[0-7])\\.b, vl256\n+**\tld1w\t(z[0-9]+)\\.d, \\1/z, \\[x0\\]\n+**\tzip2\t(z[0-9]+)\\.d, \\2\\.d, \\2\\.d\n+**\tst1w\t\\3\\.d, \\1, \\[x8\\]\n+**\tret\n+*/\n+v32sf\n+sf_zip2_d (v32sf x)\n+{\n+  return __builtin_shuffle (x, x, (v32si) { PERM4 (16, 32) });\n+}"}]}