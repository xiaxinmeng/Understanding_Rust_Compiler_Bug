{"sha": "6a70badb2c1f627cd669f2fcfaeca4a05db50b5b", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NmE3MGJhZGIyYzFmNjI3Y2Q2NjlmMmZjZmFlY2E0YTA1ZGI1MGI1Yg==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@linaro.org", "date": "2018-01-11T13:17:02Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2018-01-11T13:17:02Z"}, "message": "[AArch64] Set NUM_POLY_INT_COEFFS to 2\n\nThis patch switches the AArch64 port to use 2 poly_int coefficients\nand updates code as necessary to keep it compiling.\n\nOne potentially-significant change is to\naarch64_hard_regno_caller_save_mode.  The old implementation\nwas written in a pretty conservative way: it changed the default\nbehaviour for single-register values, but used the default handling\nfor multi-register values.\n\nI don't think that's necessary, since the interesting cases for this\nmacro are usually the single-register ones.  Multi-register modes take\nup the whole of the constituent registers and the move patterns for all\nmulti-register modes should be equally good.\n\nUsing the original mode for multi-register cases stops us from using\nSVE modes to spill multi-register NEON values.  This was caught by\ngcc.c-torture/execute/pr47538.c.\n\nAlso, aarch64_shift_truncation_mask used GET_MODE_BITSIZE - 1.\nGET_MODE_UNIT_BITSIZE - 1 is equivalent for the cases that it handles\n(which are all scalars), and I think it's more obvious, since if we ever\ndo use this for elementwise shifts of vector modes, the mask will depend\non the number of bits in each element rather than the number of bits in\nthe whole vector.\n\n2018-01-11  Richard Sandiford  <richard.sandiford@linaro.org>\n\t    Alan Hayward  <alan.hayward@arm.com>\n\t    David Sherwood  <david.sherwood@arm.com>\n\ngcc/\n\t* config/aarch64/aarch64-modes.def (NUM_POLY_INT_COEFFS): Set to 2.\n\t* config/aarch64/aarch64-protos.h (aarch64_initial_elimination_offset):\n\tReturn a poly_int64 rather than a HOST_WIDE_INT.\n\t(aarch64_offset_7bit_signed_scaled_p): Take the offset as a poly_int64\n\trather than a HOST_WIDE_INT.\n\t* config/aarch64/aarch64.h (aarch64_frame): Protect with\n\tHAVE_POLY_INT_H rather than HOST_WIDE_INT.  Change locals_offset,\n\thard_fp_offset, frame_size, initial_adjust, callee_offset and\n\tfinal_offset from HOST_WIDE_INT to poly_int64.\n\t* config/aarch64/aarch64-builtins.c (aarch64_simd_expand_args): Use\n\tto_constant when getting the number of units in an Advanced SIMD\n\tmode.\n\t(aarch64_builtin_vectorized_function): Check for a constant number\n\tof units.\n\t* config/aarch64/aarch64-simd.md (mov<mode>): Handle polynomial\n\tGET_MODE_SIZE.\n\t(aarch64_ld<VSTRUCT:nregs>_lane<VALLDIF:mode>): Use the nunits\n\tattribute instead of GET_MODE_NUNITS.\n\t* config/aarch64/aarch64.c (aarch64_hard_regno_nregs)\n\t(aarch64_class_max_nregs): Use the constant_lowest_bound of the\n\tGET_MODE_SIZE for fixed-size registers.\n\t(aarch64_const_vec_all_same_in_range_p): Use const_vec_duplicate_p.\n\t(aarch64_hard_regno_call_part_clobbered, aarch64_classify_index)\n\t(aarch64_mode_valid_for_sched_fusion_p, aarch64_classify_address)\n\t(aarch64_legitimize_address_displacement, aarch64_secondary_reload)\n\t(aarch64_print_operand, aarch64_print_address_internal)\n\t(aarch64_address_cost, aarch64_rtx_costs, aarch64_register_move_cost)\n\t(aarch64_short_vector_p, aapcs_vfp_sub_candidate)\n\t(aarch64_simd_attr_length_rglist, aarch64_operands_ok_for_ldpstp):\n\tHandle polynomial GET_MODE_SIZE.\n\t(aarch64_hard_regno_caller_save_mode): Likewise.  Return modes\n\twider than SImode without modification.\n\t(tls_symbolic_operand_type): Use strip_offset instead of split_const.\n\t(aarch64_pass_by_reference, aarch64_layout_arg, aarch64_pad_reg_upward)\n\t(aarch64_gimplify_va_arg_expr): Assert that we don't yet handle\n\tpassing and returning SVE modes.\n\t(aarch64_function_value, aarch64_layout_arg): Use gen_int_mode\n\trather than GEN_INT.\n\t(aarch64_emit_probe_stack_range): Take the size as a poly_int64\n\trather than a HOST_WIDE_INT, but call sorry if it isn't constant.\n\t(aarch64_allocate_and_probe_stack_space): Likewise.\n\t(aarch64_layout_frame): Cope with polynomial offsets.\n\t(aarch64_save_callee_saves, aarch64_restore_callee_saves): Take the\n\tstart_offset as a poly_int64 rather than a HOST_WIDE_INT.  Track\n\tpolynomial offsets.\n\t(offset_9bit_signed_unscaled_p, offset_12bit_unsigned_scaled_p)\n\t(aarch64_offset_7bit_signed_scaled_p): Take the offset as a\n\tpoly_int64 rather than a HOST_WIDE_INT.\n\t(aarch64_get_separate_components, aarch64_process_components)\n\t(aarch64_expand_prologue, aarch64_expand_epilogue)\n\t(aarch64_use_return_insn_p): Handle polynomial frame offsets.\n\t(aarch64_anchor_offset): New function, split out from...\n\t(aarch64_legitimize_address): ...here.\n\t(aarch64_builtin_vectorization_cost): Handle polynomial\n\tTYPE_VECTOR_SUBPARTS.\n\t(aarch64_simd_check_vect_par_cnst_half): Handle polynomial\n\tGET_MODE_NUNITS.\n\t(aarch64_simd_make_constant, aarch64_expand_vector_init): Get the\n\tnumber of elements from the PARALLEL rather than the mode.\n\t(aarch64_shift_truncation_mask): Use GET_MODE_UNIT_BITSIZE\n\trather than GET_MODE_BITSIZE.\n\t(aarch64_evpc_trn, aarch64_evpc_uzp, aarch64_evpc_ext)\n\t(aarch64_evpc_rev, aarch64_evpc_dup, aarch64_evpc_zip)\n\t(aarch64_expand_vec_perm_const_1): Handle polynomial\n\td->perm.length () and d->perm elements.\n\t(aarch64_evpc_tbl): Likewise.  Use nelt rather than GET_MODE_NUNITS.\n\tApply to_constant to d->perm elements.\n\t(aarch64_simd_valid_immediate, aarch64_vec_fpconst_pow_of_2): Handle\n\tpolynomial CONST_VECTOR_NUNITS.\n\t(aarch64_move_pointer): Take amount as a poly_int64 rather\n\tthan an int.\n\t(aarch64_progress_pointer): Avoid temporary variable.\n\t* config/aarch64/aarch64.md (aarch64_<crc_variant>): Use\n\tthe mode attribute instead of GET_MODE.\n\nCo-Authored-By: Alan Hayward <alan.hayward@arm.com>\nCo-Authored-By: David Sherwood <david.sherwood@arm.com>\n\nFrom-SVN: r256533", "tree": {"sha": "ea2114a02027727cf7fe48b60752181543c92f0a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/ea2114a02027727cf7fe48b60752181543c92f0a"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/comments", "author": null, "committer": null, "parents": [{"sha": "f5470a77425a54efebfe1732488c40f05ef176d0", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f5470a77425a54efebfe1732488c40f05ef176d0", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f5470a77425a54efebfe1732488c40f05ef176d0"}], "stats": {"total": 707, "additions": 423, "deletions": 284}, "files": [{"sha": "0ed0a2a27868de58efe30d35429979c511f01f95", "filename": "gcc/ChangeLog", "status": "modified", "additions": 79, "deletions": 0, "changes": 79, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=6a70badb2c1f627cd669f2fcfaeca4a05db50b5b", "patch": "@@ -1,3 +1,82 @@\n+2018-01-11  Richard Sandiford  <richard.sandiford@linaro.org>\n+\t    Alan Hayward  <alan.hayward@arm.com>\n+\t    David Sherwood  <david.sherwood@arm.com>\n+\n+\t* config/aarch64/aarch64-modes.def (NUM_POLY_INT_COEFFS): Set to 2.\n+\t* config/aarch64/aarch64-protos.h (aarch64_initial_elimination_offset):\n+\tReturn a poly_int64 rather than a HOST_WIDE_INT.\n+\t(aarch64_offset_7bit_signed_scaled_p): Take the offset as a poly_int64\n+\trather than a HOST_WIDE_INT.\n+\t* config/aarch64/aarch64.h (aarch64_frame): Protect with\n+\tHAVE_POLY_INT_H rather than HOST_WIDE_INT.  Change locals_offset,\n+\thard_fp_offset, frame_size, initial_adjust, callee_offset and\n+\tfinal_offset from HOST_WIDE_INT to poly_int64.\n+\t* config/aarch64/aarch64-builtins.c (aarch64_simd_expand_args): Use\n+\tto_constant when getting the number of units in an Advanced SIMD\n+\tmode.\n+\t(aarch64_builtin_vectorized_function): Check for a constant number\n+\tof units.\n+\t* config/aarch64/aarch64-simd.md (mov<mode>): Handle polynomial\n+\tGET_MODE_SIZE.\n+\t(aarch64_ld<VSTRUCT:nregs>_lane<VALLDIF:mode>): Use the nunits\n+\tattribute instead of GET_MODE_NUNITS.\n+\t* config/aarch64/aarch64.c (aarch64_hard_regno_nregs)\n+\t(aarch64_class_max_nregs): Use the constant_lowest_bound of the\n+\tGET_MODE_SIZE for fixed-size registers.\n+\t(aarch64_const_vec_all_same_in_range_p): Use const_vec_duplicate_p.\n+\t(aarch64_hard_regno_call_part_clobbered, aarch64_classify_index)\n+\t(aarch64_mode_valid_for_sched_fusion_p, aarch64_classify_address)\n+\t(aarch64_legitimize_address_displacement, aarch64_secondary_reload)\n+\t(aarch64_print_operand, aarch64_print_address_internal)\n+\t(aarch64_address_cost, aarch64_rtx_costs, aarch64_register_move_cost)\n+\t(aarch64_short_vector_p, aapcs_vfp_sub_candidate)\n+\t(aarch64_simd_attr_length_rglist, aarch64_operands_ok_for_ldpstp):\n+\tHandle polynomial GET_MODE_SIZE.\n+\t(aarch64_hard_regno_caller_save_mode): Likewise.  Return modes\n+\twider than SImode without modification.\n+\t(tls_symbolic_operand_type): Use strip_offset instead of split_const.\n+\t(aarch64_pass_by_reference, aarch64_layout_arg, aarch64_pad_reg_upward)\n+\t(aarch64_gimplify_va_arg_expr): Assert that we don't yet handle\n+\tpassing and returning SVE modes.\n+\t(aarch64_function_value, aarch64_layout_arg): Use gen_int_mode\n+\trather than GEN_INT.\n+\t(aarch64_emit_probe_stack_range): Take the size as a poly_int64\n+\trather than a HOST_WIDE_INT, but call sorry if it isn't constant.\n+\t(aarch64_allocate_and_probe_stack_space): Likewise.\n+\t(aarch64_layout_frame): Cope with polynomial offsets.\n+\t(aarch64_save_callee_saves, aarch64_restore_callee_saves): Take the\n+\tstart_offset as a poly_int64 rather than a HOST_WIDE_INT.  Track\n+\tpolynomial offsets.\n+\t(offset_9bit_signed_unscaled_p, offset_12bit_unsigned_scaled_p)\n+\t(aarch64_offset_7bit_signed_scaled_p): Take the offset as a\n+\tpoly_int64 rather than a HOST_WIDE_INT.\n+\t(aarch64_get_separate_components, aarch64_process_components)\n+\t(aarch64_expand_prologue, aarch64_expand_epilogue)\n+\t(aarch64_use_return_insn_p): Handle polynomial frame offsets.\n+\t(aarch64_anchor_offset): New function, split out from...\n+\t(aarch64_legitimize_address): ...here.\n+\t(aarch64_builtin_vectorization_cost): Handle polynomial\n+\tTYPE_VECTOR_SUBPARTS.\n+\t(aarch64_simd_check_vect_par_cnst_half): Handle polynomial\n+\tGET_MODE_NUNITS.\n+\t(aarch64_simd_make_constant, aarch64_expand_vector_init): Get the\n+\tnumber of elements from the PARALLEL rather than the mode.\n+\t(aarch64_shift_truncation_mask): Use GET_MODE_UNIT_BITSIZE\n+\trather than GET_MODE_BITSIZE.\n+\t(aarch64_evpc_trn, aarch64_evpc_uzp, aarch64_evpc_ext)\n+\t(aarch64_evpc_rev, aarch64_evpc_dup, aarch64_evpc_zip)\n+\t(aarch64_expand_vec_perm_const_1): Handle polynomial\n+\td->perm.length () and d->perm elements.\n+\t(aarch64_evpc_tbl): Likewise.  Use nelt rather than GET_MODE_NUNITS.\n+\tApply to_constant to d->perm elements.\n+\t(aarch64_simd_valid_immediate, aarch64_vec_fpconst_pow_of_2): Handle\n+\tpolynomial CONST_VECTOR_NUNITS.\n+\t(aarch64_move_pointer): Take amount as a poly_int64 rather\n+\tthan an int.\n+\t(aarch64_progress_pointer): Avoid temporary variable.\n+\t* config/aarch64/aarch64.md (aarch64_<crc_variant>): Use\n+\tthe mode attribute instead of GET_MODE.\n+\n 2018-01-11  Richard Sandiford  <richard.sandiford@linaro.org>\n \t    Alan Hayward  <alan.hayward@arm.com>\n \t    David Sherwood  <david.sherwood@arm.com>"}, {"sha": "02c6738d2207d19b63f9bd429942068e92c16292", "filename": "gcc/config/aarch64/aarch64-builtins.c", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c?ref=6a70badb2c1f627cd669f2fcfaeca4a05db50b5b", "patch": "@@ -1077,9 +1077,9 @@ aarch64_simd_expand_args (rtx target, int icode, int have_retval,\n \t      gcc_assert (opc > 1);\n \t      if (CONST_INT_P (op[opc]))\n \t\t{\n-\t\t  aarch64_simd_lane_bounds (op[opc], 0,\n-\t\t\t\t\t    GET_MODE_NUNITS (builtin_mode),\n-\t\t\t\t\t    exp);\n+\t\t  unsigned int nunits\n+\t\t    = GET_MODE_NUNITS (builtin_mode).to_constant ();\n+\t\t  aarch64_simd_lane_bounds (op[opc], 0, nunits, exp);\n \t\t  /* Keep to GCC-vector-extension lane indices in the RTL.  */\n \t\t  op[opc] = aarch64_endian_lane_rtx (builtin_mode,\n \t\t\t\t\t\t     INTVAL (op[opc]));\n@@ -1092,8 +1092,9 @@ aarch64_simd_expand_args (rtx target, int icode, int have_retval,\n \t      if (CONST_INT_P (op[opc]))\n \t\t{\n \t\t  machine_mode vmode = insn_data[icode].operand[opc - 1].mode;\n-\t\t  aarch64_simd_lane_bounds (op[opc],\n-\t\t\t\t\t    0, GET_MODE_NUNITS (vmode), exp);\n+\t\t  unsigned int nunits\n+\t\t    = GET_MODE_NUNITS (vmode).to_constant ();\n+\t\t  aarch64_simd_lane_bounds (op[opc], 0, nunits, exp);\n \t\t  /* Keep to GCC-vector-extension lane indices in the RTL.  */\n \t\t  op[opc] = aarch64_endian_lane_rtx (vmode, INTVAL (op[opc]));\n \t\t}\n@@ -1412,16 +1413,17 @@ aarch64_builtin_vectorized_function (unsigned int fn, tree type_out,\n \t\t\t\t     tree type_in)\n {\n   machine_mode in_mode, out_mode;\n-  int in_n, out_n;\n+  unsigned HOST_WIDE_INT in_n, out_n;\n \n   if (TREE_CODE (type_out) != VECTOR_TYPE\n       || TREE_CODE (type_in) != VECTOR_TYPE)\n     return NULL_TREE;\n \n   out_mode = TYPE_MODE (TREE_TYPE (type_out));\n-  out_n = TYPE_VECTOR_SUBPARTS (type_out);\n   in_mode = TYPE_MODE (TREE_TYPE (type_in));\n-  in_n = TYPE_VECTOR_SUBPARTS (type_in);\n+  if (!TYPE_VECTOR_SUBPARTS (type_out).is_constant (&out_n)\n+      || !TYPE_VECTOR_SUBPARTS (type_in).is_constant (&in_n))\n+    return NULL_TREE;\n \n #undef AARCH64_CHECK_BUILTIN_MODE\n #define AARCH64_CHECK_BUILTIN_MODE(C, N) 1"}, {"sha": "de40f72d666c0d9233bea0c342992d1f9ae8ecfc", "filename": "gcc/config/aarch64/aarch64-modes.def", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def?ref=6a70badb2c1f627cd669f2fcfaeca4a05db50b5b", "patch": "@@ -47,3 +47,7 @@ INT_MODE (XI, 64);\n \n /* Quad float: 128-bit floating mode for long doubles.  */\n FLOAT_MODE (TF, 16, ieee_quad_format);\n+\n+/* Coefficient 1 is multiplied by the number of 128-bit chunks in an\n+   SVE vector (referred to as \"VQ\") minus one.  */\n+#define NUM_POLY_INT_COEFFS 2"}, {"sha": "8c3471bdbb87f00ab365092d58e5f9f0f6a605d1", "filename": "gcc/config/aarch64/aarch64-protos.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h?ref=6a70badb2c1f627cd669f2fcfaeca4a05db50b5b", "patch": "@@ -333,7 +333,7 @@ enum simd_immediate_check {\n \n extern struct tune_params aarch64_tune_params;\n \n-HOST_WIDE_INT aarch64_initial_elimination_offset (unsigned, unsigned);\n+poly_int64 aarch64_initial_elimination_offset (unsigned, unsigned);\n int aarch64_get_condition_code (rtx);\n bool aarch64_address_valid_for_prefetch_p (rtx, bool);\n bool aarch64_bitmask_imm (HOST_WIDE_INT val, machine_mode);\n@@ -366,7 +366,7 @@ bool aarch64_zero_extend_const_eq (machine_mode, rtx, machine_mode, rtx);\n bool aarch64_move_imm (HOST_WIDE_INT, machine_mode);\n bool aarch64_mov_operand_p (rtx, machine_mode);\n rtx aarch64_reverse_mask (machine_mode, unsigned int);\n-bool aarch64_offset_7bit_signed_scaled_p (machine_mode, HOST_WIDE_INT);\n+bool aarch64_offset_7bit_signed_scaled_p (machine_mode, poly_int64);\n char *aarch64_output_scalar_simd_mov_immediate (rtx, scalar_int_mode);\n char *aarch64_output_simd_mov_immediate (rtx, unsigned,\n \t\t\tenum simd_immediate_check w = AARCH64_CHECK_MOV);"}, {"sha": "3d1f6a01cb79a093fa3446a03e3c00ea7e4c523a", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=6a70badb2c1f627cd669f2fcfaeca4a05db50b5b", "patch": "@@ -31,9 +31,9 @@\n      normal str, so the check need not apply.  */\n   if (GET_CODE (operands[0]) == MEM\n       && !(aarch64_simd_imm_zero (operands[1], <MODE>mode)\n-\t   && ((GET_MODE_SIZE (<MODE>mode) == 16\n+\t   && ((known_eq (GET_MODE_SIZE (<MODE>mode), 16)\n \t\t&& aarch64_mem_pair_operand (operands[0], DImode))\n-\t       || GET_MODE_SIZE (<MODE>mode) == 8)))\n+\t       || known_eq (GET_MODE_SIZE (<MODE>mode), 8))))\n       operands[1] = force_reg (<MODE>mode, operands[1]);\n   \"\n )\n@@ -5334,9 +5334,7 @@\n   set_mem_size (mem, GET_MODE_SIZE (GET_MODE_INNER (<VALLDIF:MODE>mode))\n \t\t     * <VSTRUCT:nregs>);\n \n-  aarch64_simd_lane_bounds (operands[3], 0,\n-\t\t\t    GET_MODE_NUNITS (<VALLDIF:MODE>mode),\n-\t\t\t    NULL);\n+  aarch64_simd_lane_bounds (operands[3], 0, <VALLDIF:nunits>, NULL);\n   emit_insn (gen_aarch64_vec_load_lanes<VSTRUCT:mode>_lane<VALLDIF:mode> (\n \toperands[0], mem, operands[2], operands[3]));\n   DONE;"}, {"sha": "7ab13773b1d4914ec6271eb26d59e292a0cb1b3a", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 315, "deletions": 259, "changes": 574, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=6a70badb2c1f627cd669f2fcfaeca4a05db50b5b", "patch": "@@ -1139,13 +1139,18 @@ aarch64_array_mode_supported_p (machine_mode mode,\n static unsigned int\n aarch64_hard_regno_nregs (unsigned regno, machine_mode mode)\n {\n+  /* ??? Logically we should only need to provide a value when\n+     HARD_REGNO_MODE_OK says that the combination is valid,\n+     but at the moment we need to handle all modes.  Just ignore\n+     any runtime parts for registers that can't store them.  */\n+  HOST_WIDE_INT lowest_size = constant_lower_bound (GET_MODE_SIZE (mode));\n   switch (aarch64_regno_regclass (regno))\n     {\n     case FP_REGS:\n     case FP_LO_REGS:\n-      return (GET_MODE_SIZE (mode) + UNITS_PER_VREG - 1) / UNITS_PER_VREG;\n+      return CEIL (lowest_size, UNITS_PER_VREG);\n     default:\n-      return (GET_MODE_SIZE (mode) + UNITS_PER_WORD - 1) / UNITS_PER_WORD;\n+      return CEIL (lowest_size, UNITS_PER_WORD);\n     }\n   gcc_unreachable ();\n }\n@@ -1188,25 +1193,17 @@ aarch64_hard_regno_mode_ok (unsigned regno, machine_mode mode)\n static bool\n aarch64_hard_regno_call_part_clobbered (unsigned int regno, machine_mode mode)\n {\n-  return FP_REGNUM_P (regno) && GET_MODE_SIZE (mode) > 8;\n+  return FP_REGNUM_P (regno) && maybe_gt (GET_MODE_SIZE (mode), 8);\n }\n \n /* Implement HARD_REGNO_CALLER_SAVE_MODE.  */\n machine_mode\n-aarch64_hard_regno_caller_save_mode (unsigned regno, unsigned nregs,\n-\t\t\t\t     machine_mode mode)\n+aarch64_hard_regno_caller_save_mode (unsigned, unsigned, machine_mode mode)\n {\n-  /* Handle modes that fit within single registers.  */\n-  if (nregs == 1 && GET_MODE_SIZE (mode) <= 16)\n-    {\n-      if (GET_MODE_SIZE (mode) >= 4)\n-        return mode;\n-      else\n-        return SImode;\n-    }\n-  /* Fall back to generic for multi-reg and very large modes.  */\n+  if (known_ge (GET_MODE_SIZE (mode), 4))\n+    return mode;\n   else\n-    return choose_hard_reg_mode (regno, nregs, false);\n+    return SImode;\n }\n \n /* Implement TARGET_CONSTANT_ALIGNMENT.  Make strings word-aligned so\n@@ -1319,11 +1316,10 @@ static enum tls_model\n tls_symbolic_operand_type (rtx addr)\n {\n   enum tls_model tls_kind = TLS_MODEL_NONE;\n-  rtx sym, addend;\n-\n   if (GET_CODE (addr) == CONST)\n     {\n-      split_const (addr, &sym, &addend);\n+      poly_int64 addend;\n+      rtx sym = strip_offset (addr, &addend);\n       if (GET_CODE (sym) == SYMBOL_REF)\n \ttls_kind = SYMBOL_REF_TLS_MODEL (sym);\n     }\n@@ -2275,8 +2271,12 @@ aarch64_pass_by_reference (cumulative_args_t pcum ATTRIBUTE_UNUSED,\n   int nregs;\n \n   /* GET_MODE_SIZE (BLKmode) is useless since it is 0.  */\n-  size = (mode == BLKmode && type)\n-    ? int_size_in_bytes (type) : (int) GET_MODE_SIZE (mode);\n+  if (mode == BLKmode && type)\n+    size = int_size_in_bytes (type);\n+  else\n+    /* No frontends can create types with variable-sized modes, so we\n+       shouldn't be asked to pass or return them.  */\n+    size = GET_MODE_SIZE (mode).to_constant ();\n \n   /* Aggregates are passed by reference based on their size.  */\n   if (type && AGGREGATE_TYPE_P (type))\n@@ -2373,8 +2373,8 @@ aarch64_function_value (const_tree type, const_tree func,\n \t  for (i = 0; i < count; i++)\n \t    {\n \t      rtx tmp = gen_rtx_REG (ag_mode, V0_REGNUM + i);\n-\t      tmp = gen_rtx_EXPR_LIST (VOIDmode, tmp,\n-\t\t\t\t       GEN_INT (i * GET_MODE_SIZE (ag_mode)));\n+\t      rtx offset = gen_int_mode (i * GET_MODE_SIZE (ag_mode), Pmode);\n+\t      tmp = gen_rtx_EXPR_LIST (VOIDmode, tmp, offset);\n \t      XVECEXP (par, 0, i) = tmp;\n \t    }\n \t  return par;\n@@ -2501,9 +2501,13 @@ aarch64_layout_arg (cumulative_args_t pcum_v, machine_mode mode,\n   pcum->aapcs_arg_processed = true;\n \n   /* Size in bytes, rounded to the nearest multiple of 8 bytes.  */\n-  size\n-    = ROUND_UP (type ? int_size_in_bytes (type) : GET_MODE_SIZE (mode),\n-\t\tUNITS_PER_WORD);\n+  if (type)\n+    size = int_size_in_bytes (type);\n+  else\n+    /* No frontends can create types with variable-sized modes, so we\n+       shouldn't be asked to pass or return them.  */\n+    size = GET_MODE_SIZE (mode).to_constant ();\n+  size = ROUND_UP (size, UNITS_PER_WORD);\n \n   allocate_ncrn = (type) ? !(FLOAT_TYPE_P (type)) : !FLOAT_MODE_P (mode);\n   allocate_nvrn = aarch64_vfp_is_call_candidate (pcum_v,\n@@ -2540,9 +2544,9 @@ aarch64_layout_arg (cumulative_args_t pcum_v, machine_mode mode,\n \t\t{\n \t\t  rtx tmp = gen_rtx_REG (pcum->aapcs_vfp_rmode,\n \t\t\t\t\t V0_REGNUM + nvrn + i);\n-\t\t  tmp = gen_rtx_EXPR_LIST\n-\t\t    (VOIDmode, tmp,\n-\t\t     GEN_INT (i * GET_MODE_SIZE (pcum->aapcs_vfp_rmode)));\n+\t\t  rtx offset = gen_int_mode\n+\t\t    (i * GET_MODE_SIZE (pcum->aapcs_vfp_rmode), Pmode);\n+\t\t  tmp = gen_rtx_EXPR_LIST (VOIDmode, tmp, offset);\n \t\t  XVECEXP (par, 0, i) = tmp;\n \t\t}\n \t      pcum->aapcs_reg = par;\n@@ -2767,8 +2771,13 @@ aarch64_pad_reg_upward (machine_mode mode, const_tree type,\n   /* Small composite types are always padded upward.  */\n   if (BYTES_BIG_ENDIAN && aarch64_composite_type_p (type, mode))\n     {\n-      HOST_WIDE_INT size = (type ? int_size_in_bytes (type)\n-\t\t\t    : GET_MODE_SIZE (mode));\n+      HOST_WIDE_INT size;\n+      if (type)\n+\tsize = int_size_in_bytes (type);\n+      else\n+\t/* No frontends can create types with variable-sized modes, so we\n+\t   shouldn't be asked to pass or return them.  */\n+\tsize = GET_MODE_SIZE (mode).to_constant ();\n       if (size < 2 * UNITS_PER_WORD)\n \treturn true;\n     }\n@@ -2797,12 +2806,19 @@ aarch64_libgcc_cmp_return_mode (void)\n #define PROBE_STACK_FIRST_REG  9\n #define PROBE_STACK_SECOND_REG 10\n \n-/* Emit code to probe a range of stack addresses from FIRST to FIRST+SIZE,\n+/* Emit code to probe a range of stack addresses from FIRST to FIRST+POLY_SIZE,\n    inclusive.  These are offsets from the current stack pointer.  */\n \n static void\n-aarch64_emit_probe_stack_range (HOST_WIDE_INT first, HOST_WIDE_INT size)\n+aarch64_emit_probe_stack_range (HOST_WIDE_INT first, poly_int64 poly_size)\n {\n+  HOST_WIDE_INT size;\n+  if (!poly_size.is_constant (&size))\n+    {\n+      sorry (\"stack probes for SVE frames\");\n+      return;\n+    }\n+\n   rtx reg1 = gen_rtx_REG (Pmode, PROBE_STACK_FIRST_REG);\n \n   /* See the same assertion on PROBE_INTERVAL above.  */\n@@ -3073,13 +3089,16 @@ aarch64_layout_frame (void)\n     = offset + cfun->machine->frame.saved_varargs_size;\n \n   cfun->machine->frame.hard_fp_offset\n-    = ROUND_UP (varargs_and_saved_regs_size + get_frame_size (),\n-\t\tSTACK_BOUNDARY / BITS_PER_UNIT);\n+    = aligned_upper_bound (varargs_and_saved_regs_size\n+\t\t\t   + get_frame_size (),\n+\t\t\t   STACK_BOUNDARY / BITS_PER_UNIT);\n \n+  /* Both these values are already aligned.  */\n+  gcc_assert (multiple_p (crtl->outgoing_args_size,\n+\t\t\t  STACK_BOUNDARY / BITS_PER_UNIT));\n   cfun->machine->frame.frame_size\n-    = ROUND_UP (cfun->machine->frame.hard_fp_offset\n-\t\t+ crtl->outgoing_args_size,\n-\t\tSTACK_BOUNDARY / BITS_PER_UNIT);\n+    = (cfun->machine->frame.hard_fp_offset\n+       + crtl->outgoing_args_size);\n \n   cfun->machine->frame.locals_offset = cfun->machine->frame.saved_varargs_size;\n \n@@ -3094,18 +3113,21 @@ aarch64_layout_frame (void)\n   else if (cfun->machine->frame.wb_candidate1 != INVALID_REGNUM)\n     max_push_offset = 256;\n \n-  if (cfun->machine->frame.frame_size < max_push_offset\n-      && crtl->outgoing_args_size == 0)\n+  HOST_WIDE_INT const_size, const_fp_offset;\n+  if (cfun->machine->frame.frame_size.is_constant (&const_size)\n+      && const_size < max_push_offset\n+      && known_eq (crtl->outgoing_args_size, 0))\n     {\n       /* Simple, small frame with no outgoing arguments:\n \t stp reg1, reg2, [sp, -frame_size]!\n \t stp reg3, reg4, [sp, 16]  */\n-      cfun->machine->frame.callee_adjust = cfun->machine->frame.frame_size;\n+      cfun->machine->frame.callee_adjust = const_size;\n     }\n-  else if ((crtl->outgoing_args_size\n-\t    + cfun->machine->frame.saved_regs_size < 512)\n+  else if (known_lt (crtl->outgoing_args_size\n+\t\t     + cfun->machine->frame.saved_regs_size, 512)\n \t   && !(cfun->calls_alloca\n-\t\t&& cfun->machine->frame.hard_fp_offset < max_push_offset))\n+\t\t&& known_lt (cfun->machine->frame.hard_fp_offset,\n+\t\t\t     max_push_offset)))\n     {\n       /* Frame with small outgoing arguments:\n \t sub sp, sp, frame_size\n@@ -3115,13 +3137,14 @@ aarch64_layout_frame (void)\n       cfun->machine->frame.callee_offset\n \t= cfun->machine->frame.frame_size - cfun->machine->frame.hard_fp_offset;\n     }\n-  else if (cfun->machine->frame.hard_fp_offset < max_push_offset)\n+  else if (cfun->machine->frame.hard_fp_offset.is_constant (&const_fp_offset)\n+\t   && const_fp_offset < max_push_offset)\n     {\n       /* Frame with large outgoing arguments but a small local area:\n \t stp reg1, reg2, [sp, -hard_fp_offset]!\n \t stp reg3, reg4, [sp, 16]\n \t sub sp, sp, outgoing_args_size  */\n-      cfun->machine->frame.callee_adjust = cfun->machine->frame.hard_fp_offset;\n+      cfun->machine->frame.callee_adjust = const_fp_offset;\n       cfun->machine->frame.final_adjust\n \t= cfun->machine->frame.frame_size - cfun->machine->frame.callee_adjust;\n     }\n@@ -3334,7 +3357,7 @@ aarch64_return_address_signing_enabled (void)\n    skipping any write-back candidates if SKIP_WB is true.  */\n \n static void\n-aarch64_save_callee_saves (machine_mode mode, HOST_WIDE_INT start_offset,\n+aarch64_save_callee_saves (machine_mode mode, poly_int64 start_offset,\n \t\t\t   unsigned start, unsigned limit, bool skip_wb)\n {\n   rtx_insn *insn;\n@@ -3346,7 +3369,7 @@ aarch64_save_callee_saves (machine_mode mode, HOST_WIDE_INT start_offset,\n        regno = aarch64_next_callee_save (regno + 1, limit))\n     {\n       rtx reg, mem;\n-      HOST_WIDE_INT offset;\n+      poly_int64 offset;\n \n       if (skip_wb\n \t  && (regno == cfun->machine->frame.wb_candidate1\n@@ -3399,13 +3422,13 @@ aarch64_save_callee_saves (machine_mode mode, HOST_WIDE_INT start_offset,\n \n static void\n aarch64_restore_callee_saves (machine_mode mode,\n-\t\t\t      HOST_WIDE_INT start_offset, unsigned start,\n+\t\t\t      poly_int64 start_offset, unsigned start,\n \t\t\t      unsigned limit, bool skip_wb, rtx *cfi_ops)\n {\n   rtx base_rtx = stack_pointer_rtx;\n   unsigned regno;\n   unsigned regno2;\n-  HOST_WIDE_INT offset;\n+  poly_int64 offset;\n \n   for (regno = aarch64_next_callee_save (start, limit);\n        regno <= limit;\n@@ -3450,25 +3473,27 @@ aarch64_restore_callee_saves (machine_mode mode,\n \n static inline bool\n offset_9bit_signed_unscaled_p (machine_mode mode ATTRIBUTE_UNUSED,\n-\t\t\t       HOST_WIDE_INT offset)\n+\t\t\t       poly_int64 offset)\n {\n-  return offset >= -256 && offset < 256;\n+  HOST_WIDE_INT const_offset;\n+  return (offset.is_constant (&const_offset)\n+\t  && IN_RANGE (const_offset, -256, 255));\n }\n \n static inline bool\n-offset_12bit_unsigned_scaled_p (machine_mode mode, HOST_WIDE_INT offset)\n+offset_12bit_unsigned_scaled_p (machine_mode mode, poly_int64 offset)\n {\n-  return (offset >= 0\n-\t  && offset < 4096 * GET_MODE_SIZE (mode)\n-\t  && offset % GET_MODE_SIZE (mode) == 0);\n+  HOST_WIDE_INT multiple;\n+  return (constant_multiple_p (offset, GET_MODE_SIZE (mode), &multiple)\n+\t  && IN_RANGE (multiple, 0, 4095));\n }\n \n bool\n-aarch64_offset_7bit_signed_scaled_p (machine_mode mode, HOST_WIDE_INT offset)\n+aarch64_offset_7bit_signed_scaled_p (machine_mode mode, poly_int64 offset)\n {\n-  return (offset >= -64 * GET_MODE_SIZE (mode)\n-\t  && offset < 64 * GET_MODE_SIZE (mode)\n-\t  && offset % GET_MODE_SIZE (mode) == 0);\n+  HOST_WIDE_INT multiple;\n+  return (constant_multiple_p (offset, GET_MODE_SIZE (mode), &multiple)\n+\t  && IN_RANGE (multiple, -64, 63));\n }\n \n /* Implement TARGET_SHRINK_WRAP_GET_SEPARATE_COMPONENTS.  */\n@@ -3485,7 +3510,7 @@ aarch64_get_separate_components (void)\n   for (unsigned regno = 0; regno <= LAST_SAVED_REGNUM; regno++)\n     if (aarch64_register_saved_on_entry (regno))\n       {\n-\tHOST_WIDE_INT offset = cfun->machine->frame.reg_offset[regno];\n+\tpoly_int64 offset = cfun->machine->frame.reg_offset[regno];\n \tif (!frame_pointer_needed)\n \t  offset += cfun->machine->frame.frame_size\n \t\t    - cfun->machine->frame.hard_fp_offset;\n@@ -3589,7 +3614,7 @@ aarch64_process_components (sbitmap components, bool prologue_p)\n \t so DFmode for the vector registers is enough.  */\n       machine_mode mode = GP_REGNUM_P (regno) ? E_DImode : E_DFmode;\n       rtx reg = gen_rtx_REG (mode, regno);\n-      HOST_WIDE_INT offset = cfun->machine->frame.reg_offset[regno];\n+      poly_int64 offset = cfun->machine->frame.reg_offset[regno];\n       if (!frame_pointer_needed)\n \toffset += cfun->machine->frame.frame_size\n \t\t  - cfun->machine->frame.hard_fp_offset;\n@@ -3611,13 +3636,13 @@ aarch64_process_components (sbitmap components, bool prologue_p)\n \t  break;\n \t}\n \n-      HOST_WIDE_INT offset2 = cfun->machine->frame.reg_offset[regno2];\n+      poly_int64 offset2 = cfun->machine->frame.reg_offset[regno2];\n       /* The next register is not of the same class or its offset is not\n \t mergeable with the current one into a pair.  */\n       if (!satisfies_constraint_Ump (mem)\n \t  || GP_REGNUM_P (regno) != GP_REGNUM_P (regno2)\n-\t  || (offset2 - cfun->machine->frame.reg_offset[regno])\n-\t\t!= GET_MODE_SIZE (mode))\n+\t  || maybe_ne ((offset2 - cfun->machine->frame.reg_offset[regno]),\n+\t\t       GET_MODE_SIZE (mode)))\n \t{\n \t  insn = emit_insn (set);\n \t  RTX_FRAME_RELATED_P (insn) = 1;\n@@ -3734,11 +3759,11 @@ aarch64_expand_prologue (void)\n {\n   aarch64_layout_frame ();\n \n-  HOST_WIDE_INT frame_size = cfun->machine->frame.frame_size;\n-  HOST_WIDE_INT initial_adjust = cfun->machine->frame.initial_adjust;\n+  poly_int64 frame_size = cfun->machine->frame.frame_size;\n+  poly_int64 initial_adjust = cfun->machine->frame.initial_adjust;\n   HOST_WIDE_INT callee_adjust = cfun->machine->frame.callee_adjust;\n-  HOST_WIDE_INT final_adjust = cfun->machine->frame.final_adjust;\n-  HOST_WIDE_INT callee_offset = cfun->machine->frame.callee_offset;\n+  poly_int64 final_adjust = cfun->machine->frame.final_adjust;\n+  poly_int64 callee_offset = cfun->machine->frame.callee_offset;\n   unsigned reg1 = cfun->machine->frame.wb_candidate1;\n   unsigned reg2 = cfun->machine->frame.wb_candidate2;\n   bool emit_frame_chain = cfun->machine->frame.emit_frame_chain;\n@@ -3753,19 +3778,19 @@ aarch64_expand_prologue (void)\n     }\n \n   if (flag_stack_usage_info)\n-    current_function_static_stack_size = frame_size;\n+    current_function_static_stack_size = constant_lower_bound (frame_size);\n \n   if (flag_stack_check == STATIC_BUILTIN_STACK_CHECK)\n     {\n       if (crtl->is_leaf && !cfun->calls_alloca)\n \t{\n-\t  if (frame_size > PROBE_INTERVAL\n-\t      && frame_size > get_stack_check_protect ())\n+\t  if (maybe_gt (frame_size, PROBE_INTERVAL)\n+\t      && maybe_gt (frame_size, get_stack_check_protect ()))\n \t    aarch64_emit_probe_stack_range (get_stack_check_protect (),\n \t\t\t\t\t    (frame_size\n \t\t\t\t\t     - get_stack_check_protect ()));\n \t}\n-      else if (frame_size > 0)\n+      else if (maybe_gt (frame_size, 0))\n \taarch64_emit_probe_stack_range (get_stack_check_protect (), frame_size);\n     }\n \n@@ -3812,7 +3837,7 @@ aarch64_use_return_insn_p (void)\n \n   aarch64_layout_frame ();\n \n-  return cfun->machine->frame.frame_size == 0;\n+  return known_eq (cfun->machine->frame.frame_size, 0);\n }\n \n /* Generate the epilogue instructions for returning from a function.\n@@ -3825,21 +3850,23 @@ aarch64_expand_epilogue (bool for_sibcall)\n {\n   aarch64_layout_frame ();\n \n-  HOST_WIDE_INT initial_adjust = cfun->machine->frame.initial_adjust;\n+  poly_int64 initial_adjust = cfun->machine->frame.initial_adjust;\n   HOST_WIDE_INT callee_adjust = cfun->machine->frame.callee_adjust;\n-  HOST_WIDE_INT final_adjust = cfun->machine->frame.final_adjust;\n-  HOST_WIDE_INT callee_offset = cfun->machine->frame.callee_offset;\n+  poly_int64 final_adjust = cfun->machine->frame.final_adjust;\n+  poly_int64 callee_offset = cfun->machine->frame.callee_offset;\n   unsigned reg1 = cfun->machine->frame.wb_candidate1;\n   unsigned reg2 = cfun->machine->frame.wb_candidate2;\n   rtx cfi_ops = NULL;\n   rtx_insn *insn;\n \n   /* We need to add memory barrier to prevent read from deallocated stack.  */\n-  bool need_barrier_p = (get_frame_size ()\n-\t\t\t + cfun->machine->frame.saved_varargs_size) != 0;\n+  bool need_barrier_p\n+    = maybe_ne (get_frame_size ()\n+\t\t+ cfun->machine->frame.saved_varargs_size, 0);\n \n   /* Emit a barrier to prevent loads from a deallocated stack.  */\n-  if (final_adjust > crtl->outgoing_args_size || cfun->calls_alloca\n+  if (maybe_gt (final_adjust, crtl->outgoing_args_size)\n+      || cfun->calls_alloca\n       || crtl->calls_eh_return)\n     {\n       emit_insn (gen_stack_tie (stack_pointer_rtx, stack_pointer_rtx));\n@@ -3850,7 +3877,8 @@ aarch64_expand_epilogue (bool for_sibcall)\n      be the same as the stack pointer.  */\n   rtx ip0_rtx = gen_rtx_REG (Pmode, IP0_REGNUM);\n   rtx ip1_rtx = gen_rtx_REG (Pmode, IP1_REGNUM);\n-  if (frame_pointer_needed && (final_adjust || cfun->calls_alloca))\n+  if (frame_pointer_needed\n+      && (maybe_ne (final_adjust, 0) || cfun->calls_alloca))\n     /* If writeback is used when restoring callee-saves, the CFA\n        is restored on the instruction doing the writeback.  */\n     aarch64_add_offset (Pmode, stack_pointer_rtx,\n@@ -3870,7 +3898,7 @@ aarch64_expand_epilogue (bool for_sibcall)\n   if (callee_adjust != 0)\n     aarch64_pop_regs (reg1, reg2, callee_adjust, &cfi_ops);\n \n-  if (callee_adjust != 0 || initial_adjust > 65536)\n+  if (callee_adjust != 0 || maybe_gt (initial_adjust, 65536))\n     {\n       /* Emit delayed restores and set the CFA to be SP + initial_adjust.  */\n       insn = get_last_insn ();\n@@ -4467,9 +4495,9 @@ aarch64_classify_index (struct aarch64_address_info *info, rtx x,\n       && contains_reg_of_mode[GENERAL_REGS][GET_MODE (SUBREG_REG (index))])\n     index = SUBREG_REG (index);\n \n-  if ((shift == 0 ||\n-       (shift > 0 && shift <= 3\n-\t&& (1 << shift) == GET_MODE_SIZE (mode)))\n+  if ((shift == 0\n+       || (shift > 0 && shift <= 3\n+\t   && known_eq (1 << shift, GET_MODE_SIZE (mode))))\n       && REG_P (index)\n       && aarch64_regno_ok_for_index_p (REGNO (index), strict_p))\n     {\n@@ -4491,7 +4519,7 @@ aarch64_mode_valid_for_sched_fusion_p (machine_mode mode)\n   return mode == SImode || mode == DImode\n \t || mode == SFmode || mode == DFmode\n \t || (aarch64_vector_mode_supported_p (mode)\n-\t     && GET_MODE_SIZE (mode) == 8);\n+\t     && known_eq (GET_MODE_SIZE (mode), 8));\n }\n \n /* Return true if REGNO is a virtual pointer register, or an eliminable\n@@ -4517,6 +4545,7 @@ aarch64_classify_address (struct aarch64_address_info *info,\n {\n   enum rtx_code code = GET_CODE (x);\n   rtx op0, op1;\n+  HOST_WIDE_INT const_size;\n \n   /* On BE, we use load/store pair for all large int mode load/stores.\n      TI/TFmode may also use a load/store pair.  */\n@@ -4526,10 +4555,10 @@ aarch64_classify_address (struct aarch64_address_info *info,\n \t\t\t    || (BYTES_BIG_ENDIAN\n \t\t\t\t&& aarch64_vect_struct_mode_p (mode)));\n \n-  bool allow_reg_index_p =\n-    !load_store_pair_p\n-    && (GET_MODE_SIZE (mode) != 16 || aarch64_vector_mode_supported_p (mode))\n-    && !aarch64_vect_struct_mode_p (mode);\n+  bool allow_reg_index_p = (!load_store_pair_p\n+\t\t\t    && (maybe_ne (GET_MODE_SIZE (mode), 16)\n+\t\t\t\t|| aarch64_vector_mode_supported_p (mode))\n+\t\t\t    && !aarch64_vect_struct_mode_p (mode));\n \n   /* On LE, for AdvSIMD, don't support anything other than POST_INC or\n      REG addressing.  */\n@@ -4562,7 +4591,7 @@ aarch64_classify_address (struct aarch64_address_info *info,\n \t  return true;\n \t}\n \n-      if (GET_MODE_SIZE (mode) != 0\n+      if (maybe_ne (GET_MODE_SIZE (mode), 0)\n \t  && CONST_INT_P (op1)\n \t  && aarch64_base_register_rtx_p (op0, strict_p))\n \t{\n@@ -4609,7 +4638,8 @@ aarch64_classify_address (struct aarch64_address_info *info,\n \t\t\t\t\t\t\t    offset + 32));\n \n \t  if (load_store_pair_p)\n-\t    return ((GET_MODE_SIZE (mode) == 4 || GET_MODE_SIZE (mode) == 8)\n+\t    return ((known_eq (GET_MODE_SIZE (mode), 4)\n+\t\t     || known_eq (GET_MODE_SIZE (mode), 8))\n \t\t    && aarch64_offset_7bit_signed_scaled_p (mode, offset));\n \t  else\n \t    return (offset_9bit_signed_unscaled_p (mode, offset)\n@@ -4669,7 +4699,8 @@ aarch64_classify_address (struct aarch64_address_info *info,\n \t\t    && offset_9bit_signed_unscaled_p (mode, offset));\n \n \t  if (load_store_pair_p)\n-\t    return ((GET_MODE_SIZE (mode) == 4 || GET_MODE_SIZE (mode) == 8)\n+\t    return ((known_eq (GET_MODE_SIZE (mode), 4)\n+\t\t     || known_eq (GET_MODE_SIZE (mode), 8))\n \t\t    && aarch64_offset_7bit_signed_scaled_p (mode, offset));\n \t  else\n \t    return offset_9bit_signed_unscaled_p (mode, offset);\n@@ -4683,7 +4714,9 @@ aarch64_classify_address (struct aarch64_address_info *info,\n          for SI mode or larger.  */\n       info->type = ADDRESS_SYMBOLIC;\n \n-      if (!load_store_pair_p && GET_MODE_SIZE (mode) >= 4)\n+      if (!load_store_pair_p\n+\t  && GET_MODE_SIZE (mode).is_constant (&const_size)\n+\t  && const_size >= 4)\n \t{\n \t  rtx sym, addend;\n \n@@ -4709,7 +4742,6 @@ aarch64_classify_address (struct aarch64_address_info *info,\n \t    {\n \t      /* The symbol and offset must be aligned to the access size.  */\n \t      unsigned int align;\n-\t      unsigned int ref_size;\n \n \t      if (CONSTANT_POOL_ADDRESS_P (sym))\n \t\talign = GET_MODE_ALIGNMENT (get_pool_mode (sym));\n@@ -4727,12 +4759,12 @@ aarch64_classify_address (struct aarch64_address_info *info,\n \t      else\n \t\talign = BITS_PER_UNIT;\n \n-\t      ref_size = GET_MODE_SIZE (mode);\n-\t      if (ref_size == 0)\n+\t      poly_int64 ref_size = GET_MODE_SIZE (mode);\n+\t      if (known_eq (ref_size, 0))\n \t\tref_size = GET_MODE_SIZE (DImode);\n \n-\t      return ((INTVAL (offs) & (ref_size - 1)) == 0\n-\t\t      && ((align / BITS_PER_UNIT) & (ref_size - 1)) == 0);\n+\t      return (multiple_p (INTVAL (offs), ref_size)\n+\t\t      && multiple_p (align / BITS_PER_UNIT, ref_size));\n \t    }\n \t}\n       return false;\n@@ -4810,19 +4842,24 @@ aarch64_legitimate_address_p (machine_mode mode, rtx x, bool strict_p,\n static bool\n aarch64_legitimize_address_displacement (rtx *disp, rtx *off, machine_mode mode)\n {\n-  HOST_WIDE_INT offset = INTVAL (*disp);\n-  HOST_WIDE_INT base;\n+  HOST_WIDE_INT size;\n+  if (GET_MODE_SIZE (mode).is_constant (&size))\n+    {\n+      HOST_WIDE_INT offset = INTVAL (*disp);\n+      HOST_WIDE_INT base;\n \n-  if (mode == TImode || mode == TFmode)\n-    base = (offset + 0x100) & ~0x1f8;\n-  else if ((offset & (GET_MODE_SIZE (mode) - 1)) != 0)\n-    base = (offset + 0x100) & ~0x1ff;\n-  else\n-    base = offset & ~(GET_MODE_SIZE (mode) < 4 ? 0xfff : 0x3ffc);\n+      if (mode == TImode || mode == TFmode)\n+\tbase = (offset + 0x100) & ~0x1f8;\n+      else if ((offset & (size - 1)) != 0)\n+\tbase = (offset + 0x100) & ~0x1ff;\n+      else\n+\tbase = offset & ~(size < 4 ? 0xfff : 0x3ffc);\n \n-  *off = GEN_INT (base);\n-  *disp = GEN_INT (offset - base);\n-  return true;\n+      *off = GEN_INT (base);\n+      *disp = GEN_INT (offset - base);\n+      return true;\n+    }\n+  return false;\n }\n \n /* Return the binary representation of floating point constant VALUE in INTVAL.\n@@ -5210,26 +5247,13 @@ aarch64_get_condition_code_1 (machine_mode mode, enum rtx_code comp_code)\n \n bool\n aarch64_const_vec_all_same_in_range_p (rtx x,\n-\t\t\t\t  HOST_WIDE_INT minval,\n-\t\t\t\t  HOST_WIDE_INT maxval)\n+\t\t\t\t       HOST_WIDE_INT minval,\n+\t\t\t\t       HOST_WIDE_INT maxval)\n {\n-  HOST_WIDE_INT firstval;\n-  int count, i;\n-\n-  if (GET_CODE (x) != CONST_VECTOR\n-      || GET_MODE_CLASS (GET_MODE (x)) != MODE_VECTOR_INT)\n-    return false;\n-\n-  firstval = INTVAL (CONST_VECTOR_ELT (x, 0));\n-  if (firstval < minval || firstval > maxval)\n-    return false;\n-\n-  count = CONST_VECTOR_NUNITS (x);\n-  for (i = 1; i < count; i++)\n-    if (INTVAL (CONST_VECTOR_ELT (x, i)) != firstval)\n-      return false;\n-\n-  return true;\n+  rtx elt;\n+  return (const_vec_duplicate_p (x, &elt)\n+\t  && CONST_INT_P (elt)\n+\t  && IN_RANGE (INTVAL (elt), minval, maxval));\n }\n \n bool\n@@ -5671,7 +5695,7 @@ aarch64_print_operand (FILE *f, rtx x, int code)\n \tmachine_mode mode = GET_MODE (x);\n \n \tif (GET_CODE (x) != MEM\n-\t    || (code == 'y' && GET_MODE_SIZE (mode) != 16))\n+\t    || (code == 'y' && maybe_ne (GET_MODE_SIZE (mode), 16)))\n \t  {\n \t    output_operand_lossage (\"invalid operand for '%%%c'\", code);\n \t    return;\n@@ -5702,6 +5726,7 @@ aarch64_print_address_internal (FILE *f, machine_mode mode, rtx x,\n \t\t\t\taarch64_addr_query_type type)\n {\n   struct aarch64_address_info addr;\n+  unsigned int size;\n \n   /* Check all addresses are Pmode - including ILP32.  */\n   gcc_assert (GET_MODE (x) == Pmode);\n@@ -5745,30 +5770,28 @@ aarch64_print_address_internal (FILE *f, machine_mode mode, rtx x,\n \treturn true;\n \n       case ADDRESS_REG_WB:\n+\t/* Writeback is only supported for fixed-width modes.  */\n+\tsize = GET_MODE_SIZE (mode).to_constant ();\n \tswitch (GET_CODE (x))\n \t  {\n \t  case PRE_INC:\n-\t    asm_fprintf (f, \"[%s, %d]!\", reg_names [REGNO (addr.base)],\n-\t\t\t GET_MODE_SIZE (mode));\n+\t    asm_fprintf (f, \"[%s, %d]!\", reg_names [REGNO (addr.base)], size);\n \t    return true;\n \t  case POST_INC:\n-\t    asm_fprintf (f, \"[%s], %d\", reg_names [REGNO (addr.base)],\n-\t\t\t GET_MODE_SIZE (mode));\n+\t    asm_fprintf (f, \"[%s], %d\", reg_names [REGNO (addr.base)], size);\n \t    return true;\n \t  case PRE_DEC:\n-\t    asm_fprintf (f, \"[%s, -%d]!\", reg_names [REGNO (addr.base)],\n-\t\t\t GET_MODE_SIZE (mode));\n+\t    asm_fprintf (f, \"[%s, -%d]!\", reg_names [REGNO (addr.base)], size);\n \t    return true;\n \t  case POST_DEC:\n-\t    asm_fprintf (f, \"[%s], -%d\", reg_names [REGNO (addr.base)],\n-\t\t\t GET_MODE_SIZE (mode));\n+\t    asm_fprintf (f, \"[%s], -%d\", reg_names [REGNO (addr.base)], size);\n \t    return true;\n \t  case PRE_MODIFY:\n-\t    asm_fprintf (f, \"[%s, %wd]!\", reg_names [REGNO (addr.base)],\n+\t    asm_fprintf (f, \"[%s, %wd]!\", reg_names[REGNO (addr.base)],\n \t\t\t INTVAL (addr.offset));\n \t    return true;\n \t  case POST_MODIFY:\n-\t    asm_fprintf (f, \"[%s], %wd\", reg_names [REGNO (addr.base)],\n+\t    asm_fprintf (f, \"[%s], %wd\", reg_names[REGNO (addr.base)],\n \t\t\t INTVAL (addr.offset));\n \t    return true;\n \t  default:\n@@ -5859,6 +5882,39 @@ aarch64_regno_regclass (unsigned regno)\n   return NO_REGS;\n }\n \n+/* OFFSET is an address offset for mode MODE, which has SIZE bytes.\n+   If OFFSET is out of range, return an offset of an anchor point\n+   that is in range.  Return 0 otherwise.  */\n+\n+static HOST_WIDE_INT\n+aarch64_anchor_offset (HOST_WIDE_INT offset, HOST_WIDE_INT size,\n+\t\t       machine_mode mode)\n+{\n+  /* Does it look like we'll need a 16-byte load/store-pair operation?  */\n+  if (size > 16)\n+    return (offset + 0x400) & ~0x7f0;\n+\n+  /* For offsets that aren't a multiple of the access size, the limit is\n+     -256...255.  */\n+  if (offset & (size - 1))\n+    {\n+      /* BLKmode typically uses LDP of X-registers.  */\n+      if (mode == BLKmode)\n+\treturn (offset + 512) & ~0x3ff;\n+      return (offset + 0x100) & ~0x1ff;\n+    }\n+\n+  /* Small negative offsets are supported.  */\n+  if (IN_RANGE (offset, -256, 0))\n+    return 0;\n+\n+  if (mode == TImode || mode == TFmode)\n+    return (offset + 0x100) & ~0x1ff;\n+\n+  /* Use 12-bit offset by access size.  */\n+  return offset & (~0xfff * size);\n+}\n+\n static rtx\n aarch64_legitimize_address (rtx x, rtx /* orig_x  */, machine_mode mode)\n {\n@@ -5908,34 +5964,17 @@ aarch64_legitimize_address (rtx x, rtx /* orig_x  */, machine_mode mode)\n \t  x = gen_rtx_PLUS (Pmode, base, offset_rtx);\n \t}\n \n-      /* Does it look like we'll need a 16-byte load/store-pair operation?  */\n-      HOST_WIDE_INT base_offset;\n-      if (GET_MODE_SIZE (mode) > 16)\n-\tbase_offset = (offset + 0x400) & ~0x7f0;\n-      /* For offsets aren't a multiple of the access size, the limit is\n-\t -256...255.  */\n-      else if (offset & (GET_MODE_SIZE (mode) - 1))\n+      HOST_WIDE_INT size;\n+      if (GET_MODE_SIZE (mode).is_constant (&size))\n \t{\n-\t  base_offset = (offset + 0x100) & ~0x1ff;\n-\n-\t  /* BLKmode typically uses LDP of X-registers.  */\n-\t  if (mode == BLKmode)\n-\t    base_offset = (offset + 512) & ~0x3ff;\n-\t}\n-      /* Small negative offsets are supported.  */\n-      else if (IN_RANGE (offset, -256, 0))\n-\tbase_offset = 0;\n-      else if (mode == TImode || mode == TFmode)\n-\tbase_offset = (offset + 0x100) & ~0x1ff;\n-      /* Use 12-bit offset by access size.  */\n-      else\n-\tbase_offset = offset & (~0xfff * GET_MODE_SIZE (mode));\n-\n-      if (base_offset != 0)\n-\t{\n-\t  base = plus_constant (Pmode, base, base_offset);\n-\t  base = force_operand (base, NULL_RTX);\n-\t  return plus_constant (Pmode, base, offset - base_offset);\n+\t  HOST_WIDE_INT base_offset = aarch64_anchor_offset (offset, size,\n+\t\t\t\t\t\t\t     mode);\n+\t  if (base_offset != 0)\n+\t    {\n+\t      base = plus_constant (Pmode, base, base_offset);\n+\t      base = force_operand (base, NULL_RTX);\n+\t      return plus_constant (Pmode, base, offset - base_offset);\n+\t    }\n \t}\n     }\n \n@@ -6022,7 +6061,7 @@ aarch64_secondary_reload (bool in_p ATTRIBUTE_UNUSED, rtx x,\n      because AArch64 has richer addressing modes for LDR/STR instructions\n      than LDP/STP instructions.  */\n   if (TARGET_FLOAT && rclass == GENERAL_REGS\n-      && GET_MODE_SIZE (mode) == 16 && MEM_P (x))\n+      && known_eq (GET_MODE_SIZE (mode), 16) && MEM_P (x))\n     return FP_REGS;\n \n   if (rclass == FP_REGS && (mode == TImode || mode == TFmode) && CONSTANT_P(x))\n@@ -6043,7 +6082,7 @@ aarch64_can_eliminate (const int from ATTRIBUTE_UNUSED, const int to)\n   return true;\n }\n \n-HOST_WIDE_INT\n+poly_int64\n aarch64_initial_elimination_offset (unsigned from, unsigned to)\n {\n   aarch64_layout_frame ();\n@@ -6129,6 +6168,11 @@ aarch64_trampoline_init (rtx m_tramp, tree fndecl, rtx chain_value)\n static unsigned char\n aarch64_class_max_nregs (reg_class_t regclass, machine_mode mode)\n {\n+  /* ??? Logically we should only need to provide a value when\n+     HARD_REGNO_MODE_OK says that at least one register in REGCLASS\n+     can hold MODE, but at the moment we need to handle all modes.\n+     Just ignore any runtime parts for registers that can't store them.  */\n+  HOST_WIDE_INT lowest_size = constant_lower_bound (GET_MODE_SIZE (mode));\n   switch (regclass)\n     {\n     case CALLER_SAVE_REGS:\n@@ -6138,10 +6182,9 @@ aarch64_class_max_nregs (reg_class_t regclass, machine_mode mode)\n     case POINTER_AND_FP_REGS:\n     case FP_REGS:\n     case FP_LO_REGS:\n-      return\n-\taarch64_vector_mode_p (mode)\n-\t  ? (GET_MODE_SIZE (mode) + UNITS_PER_VREG - 1) / UNITS_PER_VREG\n-\t  : (GET_MODE_SIZE (mode) + UNITS_PER_WORD - 1) / UNITS_PER_WORD;\n+      return (aarch64_vector_mode_p (mode)\n+\t      ? CEIL (lowest_size, UNITS_PER_VREG)\n+\t      : CEIL (lowest_size, UNITS_PER_WORD));\n     case STACK_REG:\n       return 1;\n \n@@ -6692,25 +6735,15 @@ aarch64_address_cost (rtx x,\n     {\n       /* For the sake of calculating the cost of the shifted register\n \t component, we can treat same sized modes in the same way.  */\n-      switch (GET_MODE_BITSIZE (mode))\n-\t{\n-\t  case 16:\n-\t    cost += addr_cost->addr_scale_costs.hi;\n-\t    break;\n-\n-\t  case 32:\n-\t    cost += addr_cost->addr_scale_costs.si;\n-\t    break;\n-\n-\t  case 64:\n-\t    cost += addr_cost->addr_scale_costs.di;\n-\t    break;\n-\n-\t  /* We can't tell, or this is a 128-bit vector.  */\n-\t  default:\n-\t    cost += addr_cost->addr_scale_costs.ti;\n-\t    break;\n-\t}\n+      if (known_eq (GET_MODE_BITSIZE (mode), 16))\n+\tcost += addr_cost->addr_scale_costs.hi;\n+      else if (known_eq (GET_MODE_BITSIZE (mode), 32))\n+\tcost += addr_cost->addr_scale_costs.si;\n+      else if (known_eq (GET_MODE_BITSIZE (mode), 64))\n+\tcost += addr_cost->addr_scale_costs.di;\n+      else\n+\t/* We can't tell, or this is a 128-bit vector.  */\n+\tcost += addr_cost->addr_scale_costs.ti;\n     }\n \n   return cost;\n@@ -7839,7 +7872,8 @@ aarch64_rtx_costs (rtx x, machine_mode mode, int outer ATTRIBUTE_UNUSED,\n \n \t      if (GET_CODE (op1) == AND && REG_P (XEXP (op1, 0))\n \t\t  && CONST_INT_P (XEXP (op1, 1))\n-\t\t  && INTVAL (XEXP (op1, 1)) == GET_MODE_BITSIZE (mode) - 1)\n+\t\t  && known_eq (INTVAL (XEXP (op1, 1)),\n+\t\t\t       GET_MODE_BITSIZE (mode) - 1))\n \t\t{\n \t\t  *cost += rtx_cost (op0, mode, (rtx_code) code, 0, speed);\n \t\t  /* We already demanded XEXP (op1, 0) to be REG_P, so\n@@ -7887,7 +7921,8 @@ aarch64_rtx_costs (rtx x, machine_mode mode, int outer ATTRIBUTE_UNUSED,\n \n \t      if (GET_CODE (op1) == AND && REG_P (XEXP (op1, 0))\n \t\t  && CONST_INT_P (XEXP (op1, 1))\n-\t\t  && INTVAL (XEXP (op1, 1)) == GET_MODE_BITSIZE (mode) - 1)\n+\t\t  && known_eq (INTVAL (XEXP (op1, 1)),\n+\t\t\t       GET_MODE_BITSIZE (mode) - 1))\n \t\t{\n \t\t  *cost += rtx_cost (op0, mode, (rtx_code) code, 0, speed);\n \t\t  /* We already demanded XEXP (op1, 0) to be REG_P, so\n@@ -8313,7 +8348,7 @@ aarch64_register_move_cost (machine_mode mode,\n     return aarch64_register_move_cost (mode, from, GENERAL_REGS)\n             + aarch64_register_move_cost (mode, GENERAL_REGS, to);\n \n-  if (GET_MODE_SIZE (mode) == 16)\n+  if (known_eq (GET_MODE_SIZE (mode), 16))\n     {\n       /* 128-bit operations on general registers require 2 instructions.  */\n       if (from == GENERAL_REGS && to == GENERAL_REGS)\n@@ -8689,7 +8724,7 @@ aarch64_builtin_vectorization_cost (enum vect_cost_for_stmt type_of_cost,\n \treturn fp ? costs->vec_fp_stmt_cost : costs->vec_int_stmt_cost;\n \n       case vec_construct:\n-        elements = TYPE_VECTOR_SUBPARTS (vectype);\n+\telements = estimated_poly_value (TYPE_VECTOR_SUBPARTS (vectype));\n \treturn elements / 2 + 1;\n \n       default:\n@@ -10730,6 +10765,10 @@ aarch64_gimplify_va_arg_expr (tree valist, tree type, gimple_seq *pre_p,\n \t\t\t\t\t       &nregs,\n \t\t\t\t\t       &is_ha))\n     {\n+      /* No frontends can create types with variable-sized modes, so we\n+\t shouldn't be asked to pass or return them.  */\n+      unsigned int ag_size = GET_MODE_SIZE (ag_mode).to_constant ();\n+\n       /* TYPE passed in fp/simd registers.  */\n       if (!TARGET_FLOAT)\n \taarch64_err_no_fpadvsimd (mode, \"varargs\");\n@@ -10743,8 +10782,8 @@ aarch64_gimplify_va_arg_expr (tree valist, tree type, gimple_seq *pre_p,\n \n       if (is_ha)\n \t{\n-\t  if (BYTES_BIG_ENDIAN && GET_MODE_SIZE (ag_mode) < UNITS_PER_VREG)\n-\t    adjust = UNITS_PER_VREG - GET_MODE_SIZE (ag_mode);\n+\t  if (BYTES_BIG_ENDIAN && ag_size < UNITS_PER_VREG)\n+\t    adjust = UNITS_PER_VREG - ag_size;\n \t}\n       else if (BLOCK_REG_PADDING (mode, type, 1) == PAD_DOWNWARD\n \t       && size < UNITS_PER_VREG)\n@@ -11132,8 +11171,8 @@ aapcs_vfp_sub_candidate (const_tree type, machine_mode *modep)\n \t\t      - tree_to_uhwi (TYPE_MIN_VALUE (index)));\n \n \t/* There must be no padding.  */\n-\tif (wi::to_wide (TYPE_SIZE (type))\n-\t    != count * GET_MODE_BITSIZE (*modep))\n+\tif (maybe_ne (wi::to_poly_wide (TYPE_SIZE (type)),\n+\t\t      count * GET_MODE_BITSIZE (*modep)))\n \t  return -1;\n \n \treturn count;\n@@ -11163,8 +11202,8 @@ aapcs_vfp_sub_candidate (const_tree type, machine_mode *modep)\n \t  }\n \n \t/* There must be no padding.  */\n-\tif (wi::to_wide (TYPE_SIZE (type))\n-\t    != count * GET_MODE_BITSIZE (*modep))\n+\tif (maybe_ne (wi::to_poly_wide (TYPE_SIZE (type)),\n+\t\t      count * GET_MODE_BITSIZE (*modep)))\n \t  return -1;\n \n \treturn count;\n@@ -11196,8 +11235,8 @@ aapcs_vfp_sub_candidate (const_tree type, machine_mode *modep)\n \t  }\n \n \t/* There must be no padding.  */\n-\tif (wi::to_wide (TYPE_SIZE (type))\n-\t    != count * GET_MODE_BITSIZE (*modep))\n+\tif (maybe_ne (wi::to_poly_wide (TYPE_SIZE (type)),\n+\t\t      count * GET_MODE_BITSIZE (*modep)))\n \t  return -1;\n \n \treturn count;\n@@ -11219,15 +11258,15 @@ static bool\n aarch64_short_vector_p (const_tree type,\n \t\t\tmachine_mode mode)\n {\n-  HOST_WIDE_INT size = -1;\n+  poly_int64 size = -1;\n \n   if (type && TREE_CODE (type) == VECTOR_TYPE)\n     size = int_size_in_bytes (type);\n   else if (GET_MODE_CLASS (mode) == MODE_VECTOR_INT\n \t    || GET_MODE_CLASS (mode) == MODE_VECTOR_FLOAT)\n     size = GET_MODE_SIZE (mode);\n \n-  return (size == 8 || size == 16);\n+  return known_eq (size, 8) || known_eq (size, 16);\n }\n \n /* Return TRUE if the type, as described by TYPE and MODE, is a composite\n@@ -11679,8 +11718,9 @@ aarch64_simd_valid_immediate (rtx op, simd_immediate_info *info,\n   unsigned int n_elts;\n   if (const_vec_duplicate_p (op, &elt))\n     n_elts = 1;\n-  else if (GET_CODE (op) == CONST_VECTOR)\n-    n_elts = CONST_VECTOR_NUNITS (op);\n+  else if (GET_CODE (op) == CONST_VECTOR\n+\t   && CONST_VECTOR_NUNITS (op).is_constant (&n_elts))\n+    /* N_ELTS set above.  */;\n   else\n     return false;\n \n@@ -11869,11 +11909,11 @@ bool\n aarch64_simd_check_vect_par_cnst_half (rtx op, machine_mode mode,\n \t\t\t\t       bool high)\n {\n-  if (!VECTOR_MODE_P (mode))\n+  int nelts;\n+  if (!VECTOR_MODE_P (mode) || !GET_MODE_NUNITS (mode).is_constant (&nelts))\n     return false;\n \n-  rtx ideal = aarch64_simd_vect_par_cnst_half (mode, GET_MODE_NUNITS (mode),\n-\t\t\t\t\t       high);\n+  rtx ideal = aarch64_simd_vect_par_cnst_half (mode, nelts, high);\n   HOST_WIDE_INT count_op = XVECLEN (op, 0);\n   HOST_WIDE_INT count_ideal = XVECLEN (ideal, 0);\n   int i = 0;\n@@ -11958,7 +11998,8 @@ aarch64_simd_emit_reg_reg_move (rtx *operands, machine_mode mode,\n int\n aarch64_simd_attr_length_rglist (machine_mode mode)\n {\n-  return (GET_MODE_SIZE (mode) / UNITS_PER_VREG) * 4;\n+  /* This is only used (and only meaningful) for Advanced SIMD, not SVE.  */\n+  return (GET_MODE_SIZE (mode).to_constant () / UNITS_PER_VREG) * 4;\n }\n \n /* Implement target hook TARGET_VECTOR_ALIGNMENT.  The AAPCS64 sets the maximum\n@@ -12038,7 +12079,6 @@ aarch64_simd_make_constant (rtx vals)\n   machine_mode mode = GET_MODE (vals);\n   rtx const_dup;\n   rtx const_vec = NULL_RTX;\n-  int n_elts = GET_MODE_NUNITS (mode);\n   int n_const = 0;\n   int i;\n \n@@ -12049,6 +12089,7 @@ aarch64_simd_make_constant (rtx vals)\n       /* A CONST_VECTOR must contain only CONST_INTs and\n \t CONST_DOUBLEs, but CONSTANT_P allows more (e.g. SYMBOL_REF).\n \t Only store valid constants in a CONST_VECTOR.  */\n+      int n_elts = XVECLEN (vals, 0);\n       for (i = 0; i < n_elts; ++i)\n \t{\n \t  rtx x = XVECEXP (vals, 0, i);\n@@ -12087,7 +12128,7 @@ aarch64_expand_vector_init (rtx target, rtx vals)\n   machine_mode mode = GET_MODE (target);\n   scalar_mode inner_mode = GET_MODE_INNER (mode);\n   /* The number of vector elements.  */\n-  int n_elts = GET_MODE_NUNITS (mode);\n+  int n_elts = XVECLEN (vals, 0);\n   /* The number of vector elements which are not constant.  */\n   int n_var = 0;\n   rtx any_const = NULL_RTX;\n@@ -12227,7 +12268,9 @@ aarch64_shift_truncation_mask (machine_mode mode)\n   return\n     (!SHIFT_COUNT_TRUNCATED\n      || aarch64_vector_mode_supported_p (mode)\n-     || aarch64_vect_struct_mode_p (mode)) ? 0 : (GET_MODE_BITSIZE (mode) - 1);\n+     || aarch64_vect_struct_mode_p (mode))\n+    ? 0\n+    : (GET_MODE_UNIT_BITSIZE (mode) - 1);\n }\n \n /* Select a format to encode pointers in exception handling data.  */\n@@ -13350,7 +13393,8 @@ aarch64_expand_vec_perm (rtx target, rtx op0, rtx op1, rtx sel,\n static bool\n aarch64_evpc_trn (struct expand_vec_perm_d *d)\n {\n-  unsigned int odd, nelt = d->perm.length ();\n+  HOST_WIDE_INT odd;\n+  poly_uint64 nelt = d->perm.length ();\n   rtx out, in0, in1, x;\n   machine_mode vmode = d->vmode;\n \n@@ -13359,8 +13403,8 @@ aarch64_evpc_trn (struct expand_vec_perm_d *d)\n \n   /* Note that these are little-endian tests.\n      We correct for big-endian later.  */\n-  odd = d->perm[0];\n-  if ((odd != 0 && odd != 1)\n+  if (!d->perm[0].is_constant (&odd)\n+      || (odd != 0 && odd != 1)\n       || !d->perm.series_p (0, 2, odd, 2)\n       || !d->perm.series_p (1, 2, nelt + odd, 2))\n     return false;\n@@ -13387,7 +13431,7 @@ aarch64_evpc_trn (struct expand_vec_perm_d *d)\n static bool\n aarch64_evpc_uzp (struct expand_vec_perm_d *d)\n {\n-  unsigned int odd;\n+  HOST_WIDE_INT odd;\n   rtx out, in0, in1, x;\n   machine_mode vmode = d->vmode;\n \n@@ -13396,8 +13440,8 @@ aarch64_evpc_uzp (struct expand_vec_perm_d *d)\n \n   /* Note that these are little-endian tests.\n      We correct for big-endian later.  */\n-  odd = d->perm[0];\n-  if ((odd != 0 && odd != 1)\n+  if (!d->perm[0].is_constant (&odd)\n+      || (odd != 0 && odd != 1)\n       || !d->perm.series_p (0, 1, odd, 2))\n     return false;\n \n@@ -13423,7 +13467,8 @@ aarch64_evpc_uzp (struct expand_vec_perm_d *d)\n static bool\n aarch64_evpc_zip (struct expand_vec_perm_d *d)\n {\n-  unsigned int high, nelt = d->perm.length ();\n+  unsigned int high;\n+  poly_uint64 nelt = d->perm.length ();\n   rtx out, in0, in1, x;\n   machine_mode vmode = d->vmode;\n \n@@ -13432,11 +13477,12 @@ aarch64_evpc_zip (struct expand_vec_perm_d *d)\n \n   /* Note that these are little-endian tests.\n      We correct for big-endian later.  */\n-  high = d->perm[0];\n-  if ((high != 0 && high * 2 != nelt)\n-      || !d->perm.series_p (0, 2, high, 1)\n-      || !d->perm.series_p (1, 2, high + nelt, 1))\n+  poly_uint64 first = d->perm[0];\n+  if ((maybe_ne (first, 0U) && maybe_ne (first * 2, nelt))\n+      || !d->perm.series_p (0, 2, first, 1)\n+      || !d->perm.series_p (1, 2, first + nelt, 1))\n     return false;\n+  high = maybe_ne (first, 0U);\n \n   /* Success!  */\n   if (d->testing_p)\n@@ -13461,13 +13507,13 @@ aarch64_evpc_zip (struct expand_vec_perm_d *d)\n static bool\n aarch64_evpc_ext (struct expand_vec_perm_d *d)\n {\n-  unsigned int nelt = d->perm.length ();\n+  HOST_WIDE_INT location;\n   rtx offset;\n \n-  unsigned int location = d->perm[0]; /* Always < nelt.  */\n-\n-  /* Check if the extracted indices are increasing by one.  */\n-  if (!d->perm.series_p (0, 1, location, 1))\n+  /* The first element always refers to the first vector.\n+     Check if the extracted indices are increasing by one.  */\n+  if (!d->perm[0].is_constant (&location)\n+      || !d->perm.series_p (0, 1, location, 1))\n     return false;\n \n   /* Success! */\n@@ -13483,8 +13529,10 @@ aarch64_evpc_ext (struct expand_vec_perm_d *d)\n          at the LSB end of the register), and the low elements of the second\n          vector (stored at the MSB end of the register). So swap.  */\n       std::swap (d->op0, d->op1);\n-      /* location != 0 (above), so safe to assume (nelt - location) < nelt.  */\n-      location = nelt - location;\n+      /* location != 0 (above), so safe to assume (nelt - location) < nelt.\n+\t to_constant () is safe since this is restricted to Advanced SIMD\n+\t vectors.  */\n+      location = d->perm.length ().to_constant () - location;\n     }\n \n   offset = GEN_INT (location);\n@@ -13500,12 +13548,13 @@ aarch64_evpc_ext (struct expand_vec_perm_d *d)\n static bool\n aarch64_evpc_rev (struct expand_vec_perm_d *d)\n {\n-  unsigned int i, diff, size, unspec;\n+  HOST_WIDE_INT diff;\n+  unsigned int i, size, unspec;\n \n-  if (!d->one_vector_p)\n+  if (!d->one_vector_p\n+      || !d->perm[0].is_constant (&diff))\n     return false;\n \n-  diff = d->perm[0];\n   size = (diff + 1) * GET_MODE_UNIT_SIZE (d->vmode);\n   if (size == 8)\n     unspec = UNSPEC_REV64;\n@@ -13535,19 +13584,18 @@ aarch64_evpc_dup (struct expand_vec_perm_d *d)\n {\n   rtx out = d->target;\n   rtx in0;\n+  HOST_WIDE_INT elt;\n   machine_mode vmode = d->vmode;\n-  unsigned int elt;\n   rtx lane;\n \n-  if (d->perm.encoding ().encoded_nelts () != 1)\n+  if (d->perm.encoding ().encoded_nelts () != 1\n+      || !d->perm[0].is_constant (&elt))\n     return false;\n \n   /* Success! */\n   if (d->testing_p)\n     return true;\n \n-  elt = d->perm[0];\n-\n   /* The generic preparation in aarch64_expand_vec_perm_const_1\n      swaps the operand order and the permute indices if it finds\n      d->perm[0] to be in the second operand.  Thus, we can always\n@@ -13567,7 +13615,12 @@ aarch64_evpc_tbl (struct expand_vec_perm_d *d)\n {\n   rtx rperm[MAX_VECT_LEN], sel;\n   machine_mode vmode = d->vmode;\n-  unsigned int i, nelt = d->perm.length ();\n+\n+  /* Make sure that the indices are constant.  */\n+  unsigned int encoded_nelts = d->perm.encoding ().encoded_nelts ();\n+  for (unsigned int i = 0; i < encoded_nelts; ++i)\n+    if (!d->perm[i].is_constant ())\n+      return false;\n \n   if (d->testing_p)\n     return true;\n@@ -13578,16 +13631,17 @@ aarch64_evpc_tbl (struct expand_vec_perm_d *d)\n   if (vmode != V8QImode && vmode != V16QImode)\n     return false;\n \n-  for (i = 0; i < nelt; ++i)\n-    {\n-      int nunits = GET_MODE_NUNITS (vmode);\n+  /* to_constant is safe since this routine is specific to Advanced SIMD\n+     vectors.  */\n+  unsigned int nelt = d->perm.length ().to_constant ();\n+  for (unsigned int i = 0; i < nelt; ++i)\n+    /* If big-endian and two vectors we end up with a weird mixed-endian\n+       mode on NEON.  Reverse the index within each word but not the word\n+       itself.  to_constant is safe because we checked is_constant above.  */\n+    rperm[i] = GEN_INT (BYTES_BIG_ENDIAN\n+\t\t\t? d->perm[i].to_constant () ^ (nelt - 1)\n+\t\t\t: d->perm[i].to_constant ());\n \n-      /* If big-endian and two vectors we end up with a weird mixed-endian\n-\t mode on NEON.  Reverse the index within each word but not the word\n-\t itself.  */\n-      rperm[i] = GEN_INT (BYTES_BIG_ENDIAN ? d->perm[i] ^ (nunits - 1)\n-\t\t\t\t\t   : (HOST_WIDE_INT) d->perm[i]);\n-    }\n   sel = gen_rtx_CONST_VECTOR (vmode, gen_rtvec_v (nelt, rperm));\n   sel = force_reg (vmode, sel);\n \n@@ -13601,14 +13655,14 @@ aarch64_expand_vec_perm_const_1 (struct expand_vec_perm_d *d)\n   /* The pattern matching functions above are written to look for a small\n      number to begin the sequence (0, 1, N/2).  If we begin with an index\n      from the second operand, we can swap the operands.  */\n-  unsigned int nelt = d->perm.length ();\n-  if (d->perm[0] >= nelt)\n+  poly_int64 nelt = d->perm.length ();\n+  if (known_ge (d->perm[0], nelt))\n     {\n       d->perm.rotate_inputs (1);\n       std::swap (d->op0, d->op1);\n     }\n \n-  if (TARGET_SIMD && nelt > 1)\n+  if (TARGET_SIMD && known_gt (nelt, 1))\n     {\n       if (aarch64_evpc_rev (d))\n \treturn true;\n@@ -13724,7 +13778,7 @@ aarch64_modes_tieable_p (machine_mode mode1, machine_mode mode2)\n    AMOUNT bytes.  */\n \n static rtx\n-aarch64_move_pointer (rtx pointer, int amount)\n+aarch64_move_pointer (rtx pointer, poly_int64 amount)\n {\n   rtx next = plus_constant (Pmode, XEXP (pointer, 0), amount);\n \n@@ -13738,9 +13792,7 @@ aarch64_move_pointer (rtx pointer, int amount)\n static rtx\n aarch64_progress_pointer (rtx pointer)\n {\n-  HOST_WIDE_INT amount = GET_MODE_SIZE (GET_MODE (pointer));\n-\n-  return aarch64_move_pointer (pointer, amount);\n+  return aarch64_move_pointer (pointer, GET_MODE_SIZE (GET_MODE (pointer)));\n }\n \n /* Copy one MODE sized block from SRC to DST, then progress SRC and DST by\n@@ -14551,7 +14603,9 @@ aarch64_operands_ok_for_ldpstp (rtx *operands, bool load,\n \n   offval_1 = INTVAL (offset_1);\n   offval_2 = INTVAL (offset_2);\n-  msize = GET_MODE_SIZE (mode);\n+  /* We should only be trying this for fixed-sized modes.  There is no\n+     SVE LDP/STP instruction.  */\n+  msize = GET_MODE_SIZE (mode).to_constant ();\n   /* Check if the offsets are consecutive.  */\n   if (offval_1 != (offval_2 + msize) && offval_2 != (offval_1 + msize))\n     return false;\n@@ -14911,7 +14965,9 @@ aarch64_fpconst_pow_of_2 (rtx x)\n int\n aarch64_vec_fpconst_pow_of_2 (rtx x)\n {\n-  if (GET_CODE (x) != CONST_VECTOR)\n+  int nelts;\n+  if (GET_CODE (x) != CONST_VECTOR\n+      || !CONST_VECTOR_NUNITS (x).is_constant (&nelts))\n     return -1;\n \n   if (GET_MODE_CLASS (GET_MODE (x)) != MODE_VECTOR_FLOAT)\n@@ -14921,7 +14977,7 @@ aarch64_vec_fpconst_pow_of_2 (rtx x)\n   if (firstval <= 0)\n     return -1;\n \n-  for (int i = 1; i < CONST_VECTOR_NUNITS (x); i++)\n+  for (int i = 1; i < nelts; i++)\n     if (aarch64_fpconst_pow_of_2 (CONST_VECTOR_ELT (x, i)) != firstval)\n       return -1;\n "}, {"sha": "98e45171043e8f5c5b5bfe4576185837760bdd23", "filename": "gcc/config/aarch64/aarch64.h", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2Fconfig%2Faarch64%2Faarch64.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2Fconfig%2Faarch64%2Faarch64.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.h?ref=6a70badb2c1f627cd669f2fcfaeca4a05db50b5b", "patch": "@@ -586,7 +586,7 @@ extern enum aarch64_processor aarch64_tune;\n \n #define DEFAULT_PCC_STRUCT_RETURN 0\n \n-#ifdef HOST_WIDE_INT\n+#ifdef HAVE_POLY_INT_H\n struct GTY (()) aarch64_frame\n {\n   HOST_WIDE_INT reg_offset[FIRST_PSEUDO_REGISTER];\n@@ -604,31 +604,31 @@ struct GTY (()) aarch64_frame\n   /* Offset from the base of the frame (incomming SP) to the\n      top of the locals area.  This value is always a multiple of\n      STACK_BOUNDARY.  */\n-  HOST_WIDE_INT locals_offset;\n+  poly_int64 locals_offset;\n \n   /* Offset from the base of the frame (incomming SP) to the\n      hard_frame_pointer.  This value is always a multiple of\n      STACK_BOUNDARY.  */\n-  HOST_WIDE_INT hard_fp_offset;\n+  poly_int64 hard_fp_offset;\n \n   /* The size of the frame.  This value is the offset from base of the\n-   * frame (incomming SP) to the stack_pointer.  This value is always\n-   * a multiple of STACK_BOUNDARY.  */\n-  HOST_WIDE_INT frame_size;\n+     frame (incomming SP) to the stack_pointer.  This value is always\n+     a multiple of STACK_BOUNDARY.  */\n+  poly_int64 frame_size;\n \n   /* The size of the initial stack adjustment before saving callee-saves.  */\n-  HOST_WIDE_INT initial_adjust;\n+  poly_int64 initial_adjust;\n \n   /* The writeback value when pushing callee-save registers.\n      It is zero when no push is used.  */\n   HOST_WIDE_INT callee_adjust;\n \n   /* The offset from SP to the callee-save registers after initial_adjust.\n      It may be non-zero if no push is used (ie. callee_adjust == 0).  */\n-  HOST_WIDE_INT callee_offset;\n+  poly_int64 callee_offset;\n \n   /* The size of the stack adjustment after saving callee-saves.  */\n-  HOST_WIDE_INT final_adjust;\n+  poly_int64 final_adjust;\n \n   /* Store FP,LR and setup a frame pointer.  */\n   bool emit_frame_chain;"}, {"sha": "854c44830e694a960acd3d5d507cfe2623661ad3", "filename": "gcc/config/aarch64/aarch64.md", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2Fconfig%2Faarch64%2Faarch64.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6a70badb2c1f627cd669f2fcfaeca4a05db50b5b/gcc%2Fconfig%2Faarch64%2Faarch64.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.md?ref=6a70badb2c1f627cd669f2fcfaeca4a05db50b5b", "patch": "@@ -3328,7 +3328,7 @@\n          CRC))]\n   \"TARGET_CRC32\"\n   {\n-    if (GET_MODE_BITSIZE (GET_MODE (operands[2])) >= 64)\n+    if (GET_MODE_BITSIZE (<crc_mode>mode) >= 64)\n       return \"<crc_variant>\\\\t%w0, %w1, %x2\";\n     else\n       return \"<crc_variant>\\\\t%w0, %w1, %w2\";"}]}