{"sha": "89c52e5e2ca6e63b1dc0868893d05434d61a0c33", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ODljNTJlNWUyY2E2ZTYzYjFkYzA4Njg4OTNkMDU0MzRkNjFhMGMzMw==", "commit": {"author": {"name": "Tamar Christina", "email": "tamar.christina@arm.com", "date": "2018-07-05T10:31:04Z"}, "committer": {"name": "Tamar Christina", "email": "tnfchris@gcc.gnu.org", "date": "2018-07-05T10:31:04Z"}, "message": "Simplify movmem code by always doing overlapping copies when larger than 8 bytes on AArch64.\n\nThis changes the movmem code in AArch64 that does copy for data between 4 and 7\nbytes to use the smallest possible mode capable of copying the remaining bytes in one\ngo and then overlapping the reads if needed.\n\nThis means that if we're copying 5 bytes we would issue an SImode and QImode\nload instead of two SImode loads.\n\nThis does smaller memory accesses but also gives the mid-end a chance to realise\nthat it can CSE the loads in certain circumstances. e.g. when you have something\nlike\n\nreturn foo;\n\nwhere foo is a struct. This would be transformed by the mid-end into SSA form as\n\nD.XXXX = foo;\n\nreturn D.XXXX;\n\nThis movmem routine will handle the first copy, but it's usually not needed,\nthe mid-end would do SImode and QImode stores into X0 for the 5 bytes example\nbut without the first copies being in the same mode, it doesn't know it doesn't\nneed the stores at all.\n\nFrom-SVN: r262434", "tree": {"sha": "6d840f071e72215c17fb63692d89956f1bba850b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/6d840f071e72215c17fb63692d89956f1bba850b"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/89c52e5e2ca6e63b1dc0868893d05434d61a0c33", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/89c52e5e2ca6e63b1dc0868893d05434d61a0c33", "html_url": "https://github.com/Rust-GCC/gccrs/commit/89c52e5e2ca6e63b1dc0868893d05434d61a0c33", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/89c52e5e2ca6e63b1dc0868893d05434d61a0c33/comments", "author": {"login": "TamarChristinaArm", "id": 48126768, "node_id": "MDQ6VXNlcjQ4MTI2NzY4", "avatar_url": "https://avatars.githubusercontent.com/u/48126768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TamarChristinaArm", "html_url": "https://github.com/TamarChristinaArm", "followers_url": "https://api.github.com/users/TamarChristinaArm/followers", "following_url": "https://api.github.com/users/TamarChristinaArm/following{/other_user}", "gists_url": "https://api.github.com/users/TamarChristinaArm/gists{/gist_id}", "starred_url": "https://api.github.com/users/TamarChristinaArm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TamarChristinaArm/subscriptions", "organizations_url": "https://api.github.com/users/TamarChristinaArm/orgs", "repos_url": "https://api.github.com/users/TamarChristinaArm/repos", "events_url": "https://api.github.com/users/TamarChristinaArm/events{/privacy}", "received_events_url": "https://api.github.com/users/TamarChristinaArm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "bdfc619ed80b29b35aff74731f84915e033a5e84", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/bdfc619ed80b29b35aff74731f84915e033a5e84", "html_url": "https://github.com/Rust-GCC/gccrs/commit/bdfc619ed80b29b35aff74731f84915e033a5e84"}], "stats": {"total": 189, "additions": 112, "deletions": 77}, "files": [{"sha": "f30e83d3c42ec18d1b0e4aa31ca4286727415c1c", "filename": "gcc/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/89c52e5e2ca6e63b1dc0868893d05434d61a0c33/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/89c52e5e2ca6e63b1dc0868893d05434d61a0c33/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=89c52e5e2ca6e63b1dc0868893d05434d61a0c33", "patch": "@@ -1,3 +1,7 @@\n+2018-07-05  Tamar Christina  <tamar.christina@arm.com>\n+\n+\t* config/aarch64/aarch64.c (aarch64_expand_movmem): Fix mode size.\n+\n 2018-07-05  Jakub Jelinek  <jakub@redhat.com>\n \n \tRevert"}, {"sha": "01f35f8e8525adb455780269757452c8c3eb20be", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 35, "deletions": 77, "changes": 112, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/89c52e5e2ca6e63b1dc0868893d05434d61a0c33/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/89c52e5e2ca6e63b1dc0868893d05434d61a0c33/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=89c52e5e2ca6e63b1dc0868893d05434d61a0c33", "patch": "@@ -16137,26 +16137,29 @@ aarch64_copy_one_block_and_progress_pointers (rtx *src, rtx *dst,\n bool\n aarch64_expand_movmem (rtx *operands)\n {\n-  unsigned int n;\n+  int n, mode_bits;\n   rtx dst = operands[0];\n   rtx src = operands[1];\n   rtx base;\n+  machine_mode cur_mode = BLKmode, next_mode;\n   bool speed_p = !optimize_function_for_size_p (cfun);\n \n   /* When optimizing for size, give a better estimate of the length of a\n-     memcpy call, but use the default otherwise.  */\n-  unsigned int max_instructions = (speed_p ? 15 : AARCH64_CALL_RATIO) / 2;\n+     memcpy call, but use the default otherwise.  Moves larger than 8 bytes\n+     will always require an even number of instructions to do now.  And each\n+     operation requires both a load+store, so devide the max number by 2.  */\n+  int max_num_moves = (speed_p ? 16 : AARCH64_CALL_RATIO) / 2;\n \n   /* We can't do anything smart if the amount to copy is not constant.  */\n   if (!CONST_INT_P (operands[2]))\n     return false;\n \n-  n = UINTVAL (operands[2]);\n+  n = INTVAL (operands[2]);\n \n-  /* Try to keep the number of instructions low.  For cases below 16 bytes we\n-     need to make at most two moves.  For cases above 16 bytes it will be one\n-     move for each 16 byte chunk, then at most two additional moves.  */\n-  if (((n / 16) + (n % 16 ? 2 : 0)) > max_instructions)\n+  /* Try to keep the number of instructions low.  For all cases we will do at\n+     most two moves for the residual amount, since we'll always overlap the\n+     remainder.  */\n+  if (((n / 16) + (n % 16 ? 2 : 0)) > max_num_moves)\n     return false;\n \n   base = copy_to_mode_reg (Pmode, XEXP (dst, 0));\n@@ -16165,81 +16168,36 @@ aarch64_expand_movmem (rtx *operands)\n   base = copy_to_mode_reg (Pmode, XEXP (src, 0));\n   src = adjust_automodify_address (src, VOIDmode, base, 0);\n \n-  /* Simple cases.  Copy 0-3 bytes, as (if applicable) a 2-byte, then a\n-     1-byte chunk.  */\n-  if (n < 4)\n-    {\n-      if (n >= 2)\n-\t{\n-\t  aarch64_copy_one_block_and_progress_pointers (&src, &dst, HImode);\n-\t  n -= 2;\n-\t}\n-\n-      if (n == 1)\n-\taarch64_copy_one_block_and_progress_pointers (&src, &dst, QImode);\n-\n-      return true;\n-    }\n+  /* Convert n to bits to make the rest of the code simpler.  */\n+  n = n * BITS_PER_UNIT;\n \n-  /* Copy 4-8 bytes.  First a 4-byte chunk, then (if applicable) a second\n-     4-byte chunk, partially overlapping with the previously copied chunk.  */\n-  if (n < 8)\n+  while (n > 0)\n     {\n-      aarch64_copy_one_block_and_progress_pointers (&src, &dst, SImode);\n-      n -= 4;\n-      if (n > 0)\n-\t{\n-\t  int move = n - 4;\n+      /* Find the largest mode in which to do the copy in without over reading\n+\t or writing.  */\n+      opt_scalar_int_mode mode_iter;\n+      FOR_EACH_MODE_IN_CLASS (mode_iter, MODE_INT)\n+\tif (GET_MODE_BITSIZE (mode_iter.require ()) <= n)\n+\t  cur_mode = mode_iter.require ();\n \n-\t  src = aarch64_move_pointer (src, move);\n-\t  dst = aarch64_move_pointer (dst, move);\n-\t  aarch64_copy_one_block_and_progress_pointers (&src, &dst, SImode);\n-\t}\n-      return true;\n-    }\n+      gcc_assert (cur_mode != BLKmode);\n \n-  /* Copy more than 8 bytes.  Copy chunks of 16 bytes until we run out of\n-     them, then (if applicable) an 8-byte chunk.  */\n-  while (n >= 8)\n-    {\n-      if (n / 16)\n-\t{\n-\t  aarch64_copy_one_block_and_progress_pointers (&src, &dst, TImode);\n-\t  n -= 16;\n-\t}\n-      else\n-\t{\n-\t  aarch64_copy_one_block_and_progress_pointers (&src, &dst, DImode);\n-\t  n -= 8;\n-\t}\n-    }\n+      mode_bits = GET_MODE_BITSIZE (cur_mode).to_constant ();\n+      aarch64_copy_one_block_and_progress_pointers (&src, &dst, cur_mode);\n \n-  /* Finish the final bytes of the copy.  We can always do this in one\n-     instruction.  We either copy the exact amount we need, or partially\n-     overlap with the previous chunk we copied and copy 8-bytes.  */\n-  if (n == 0)\n-    return true;\n-  else if (n == 1)\n-    aarch64_copy_one_block_and_progress_pointers (&src, &dst, QImode);\n-  else if (n == 2)\n-    aarch64_copy_one_block_and_progress_pointers (&src, &dst, HImode);\n-  else if (n == 4)\n-    aarch64_copy_one_block_and_progress_pointers (&src, &dst, SImode);\n-  else\n-    {\n-      if (n == 3)\n-\t{\n-\t  src = aarch64_move_pointer (src, -1);\n-\t  dst = aarch64_move_pointer (dst, -1);\n-\t  aarch64_copy_one_block_and_progress_pointers (&src, &dst, SImode);\n-\t}\n-      else\n-\t{\n-\t  int move = n - 8;\n+      n -= mode_bits;\n \n-\t  src = aarch64_move_pointer (src, move);\n-\t  dst = aarch64_move_pointer (dst, move);\n-\t  aarch64_copy_one_block_and_progress_pointers (&src, &dst, DImode);\n+      /* Do certain trailing copies as overlapping if it's going to be\n+\t cheaper.  i.e. less instructions to do so.  For instance doing a 15\n+\t byte copy it's more efficient to do two overlapping 8 byte copies than\n+\t 8 + 6 + 1.  */\n+      next_mode = smallest_mode_for_size (n, MODE_INT);\n+      int n_bits = GET_MODE_BITSIZE (next_mode).to_constant ();\n+      if (n > 0 && n_bits > n && n_bits <= 8 * BITS_PER_UNIT)\n+\t{\n+\t  src = aarch64_move_pointer (src, (n - n_bits) / BITS_PER_UNIT);\n+\t  dst = aarch64_move_pointer (dst, (n - n_bits) / BITS_PER_UNIT);\n+\t  n = n_bits;\n \t}\n     }\n "}, {"sha": "5ff92a414c6fee1b2248a42e46dc542cc0cf7cfb", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/89c52e5e2ca6e63b1dc0868893d05434d61a0c33/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/89c52e5e2ca6e63b1dc0868893d05434d61a0c33/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=89c52e5e2ca6e63b1dc0868893d05434d61a0c33", "patch": "@@ -1,3 +1,7 @@\n+2018-07-05  Tamar Christina  <tamar.christina@arm.com>\n+\n+\t* gcc.target/aarch64/struct_cpy.c: New.\n+\n 2018-07-05  Christophe Lyon  <christophe.lyon@linaro.org>\n \n \t* c-c++-common/unroll-1.c: Remove 'note:' in matching string."}, {"sha": "4feb3ea990994e0de82b3d54f13ec073cfa7a335", "filename": "gcc/testsuite/gcc.target/aarch64/struct_cpy.c", "status": "added", "additions": 69, "deletions": 0, "changes": 69, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/89c52e5e2ca6e63b1dc0868893d05434d61a0c33/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fstruct_cpy.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/89c52e5e2ca6e63b1dc0868893d05434d61a0c33/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fstruct_cpy.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fstruct_cpy.c?ref=89c52e5e2ca6e63b1dc0868893d05434d61a0c33", "patch": "@@ -0,0 +1,69 @@\n+/* { dg-do compile } */\n+\n+struct struct1 { char a;};\n+struct struct2 { char a, b;};\n+struct struct3 { char a, b, c; };\n+struct struct4 { char a, b, c, d; };\n+struct struct5 { char a, b, c, d, e; };\n+struct struct6 { char a, b, c, d, e, f; };\n+struct struct7 { char a, b, c, d, e, f, g; };\n+struct struct8 { char a, b, c, d, e, f, g, h; };\n+struct struct9 { char a, b, c, d, e, f, g, h, i; };\n+struct struct10 { char a, b, c, d, e, f, g, h, i, j; };\n+struct struct11 { char a, b, c, d, e, f, g, h, i, j, k; };\n+struct struct12 { char a, b, c, d, e, f, g, h, i, j, k, l; };\n+struct struct13 { char a, b, c, d, e, f, g, h, i, j, k, l, m; };\n+struct struct14 { char a, b, c, d, e, f, g, h, i, j, k, l, m, n; };\n+struct struct15 { char a, b, c, d, e, f, g, h, i, j, k, l, m, n, o; };\n+struct struct16 { char a, b, c, d, e, f, g, h, i, j, k, l, m, n, o, p; };\n+\n+struct struct1 foo1 = {'1'};\n+struct struct2 foo2 = { 'a', 'b'};\n+struct struct3 foo3 = { 'A', 'B', 'C'};\n+struct struct4 foo4 = {'1', '2', '3', '4'};\n+struct struct5 foo5 = {'a', 'b', 'c', 'd', 'e'};\n+struct struct6 foo6 = {'A', 'B', 'C', 'D', 'E', 'F'};\n+struct struct7 foo7 = {'1', '2', '3', '4', '5', '6', '7'};\n+struct struct8 foo8 = {'1', '2', '3', '4', '5', '6', '7', '8'};\n+struct struct9 foo9 = {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i'};\n+struct struct10 foo10 = {\n+  'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'};\n+struct struct11 foo11 = {\n+  '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B'};\n+struct struct12 foo12 = {\n+  'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L'};\n+struct struct13 foo13 = {\n+  'a','b','c','d','e','f','g','h','i','j','k','l','m'};\n+struct struct14 foo14 = {\n+  'a','b','c','d','e','f','g','h','i','j','k','l','m','n'};\n+struct struct15 foo15 = {\n+  'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o'};\n+struct struct16 foo16 = {\n+  'a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p'};\n+\n+#define FUN(x) void fun##x ()\\\n+{ \\\n+  volatile struct struct##x var##x = foo##x; \\\n+}\n+\n+FUN(1);\n+FUN(2);\n+FUN(3);\n+FUN(4);\n+FUN(5);\n+FUN(6);\n+FUN(7);\n+FUN(8);\n+FUN(9);\n+FUN(10);\n+FUN(11);\n+FUN(12);\n+FUN(13);\n+FUN(14);\n+FUN(15);\n+FUN(16);\n+\n+/* { dg-final { scan-assembler-times {ldr\\s} 18 } } */\n+/* { dg-final { scan-assembler-times {ldrb} 4 } } */\n+/* { dg-final { scan-assembler-times {ldrh} 4 } } */\n+/* { dg-final { scan-assembler-times {ldp} 1 } } */"}]}