{"sha": "3793ecc10fd4f8be8abd65ba41824f3cc91238e7", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6Mzc5M2VjYzEwZmQ0ZjhiZThhYmQ2NWJhNDE4MjRmM2NjOTEyMzhlNw==", "commit": {"author": {"name": "Andrea Corallo", "email": "andrea.corallo@arm.com", "date": "2020-11-10T11:23:15Z"}, "committer": {"name": "Andrea Corallo", "email": "andrea.corallo@arm.com", "date": "2020-11-13T11:03:43Z"}, "message": "aarch64: Make use of RTL predicates\n\n2020-11-10  Andrea Corallo  <andrea.corallo@arm.com>\n\n\t* config/aarch64/aarch64.c (tls_symbolic_operand_type)\n\t(aarch64_load_symref_appropriately, aarch64_mov128_immediate)\n\t(aarch64_expand_mov_immediate)\n\t(aarch64_maybe_expand_sve_subreg_move)\n\t(aarch64_tls_referenced_p, aarch64_cannot_force_const_mem)\n\t(aarch64_base_register_rtx_p, aarch64_classify_index)\n\t(aarch64_classify_address, aarch64_symbolic_address_p)\n\t(aarch64_reinterpret_float_as_int, aarch64_float_const_rtx_p)\n\t(aarch64_can_const_movi_rtx_p, aarch64_select_cc_mode)\n\t(aarch64_print_operand, aarch64_label_mentioned_p)\n\t(aarch64_secondary_reload, aarch64_preferred_reload_class)\n\t(aarch64_address_cost, aarch64_tls_symbol_p)\n\t(aarch64_classify_symbol, aarch64_legitimate_pic_operand_p)\n\t(aarch64_legitimate_constant_p)\n\t(aarch64_sve_float_arith_immediate_p)\n\t(aarch64_sve_float_mul_immediate_p, aarch64_mov_operand_p)\n\t(fusion_load_store): Use RTL operands where possible.", "tree": {"sha": "e1113c0ec6a4819160473d463ff7ede7b5f426c2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/e1113c0ec6a4819160473d463ff7ede7b5f426c2"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/3793ecc10fd4f8be8abd65ba41824f3cc91238e7", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3793ecc10fd4f8be8abd65ba41824f3cc91238e7", "html_url": "https://github.com/Rust-GCC/gccrs/commit/3793ecc10fd4f8be8abd65ba41824f3cc91238e7", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3793ecc10fd4f8be8abd65ba41824f3cc91238e7/comments", "author": {"login": "AndreaCorallo", "id": 6765576, "node_id": "MDQ6VXNlcjY3NjU1NzY=", "avatar_url": "https://avatars.githubusercontent.com/u/6765576?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AndreaCorallo", "html_url": "https://github.com/AndreaCorallo", "followers_url": "https://api.github.com/users/AndreaCorallo/followers", "following_url": "https://api.github.com/users/AndreaCorallo/following{/other_user}", "gists_url": "https://api.github.com/users/AndreaCorallo/gists{/gist_id}", "starred_url": "https://api.github.com/users/AndreaCorallo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AndreaCorallo/subscriptions", "organizations_url": "https://api.github.com/users/AndreaCorallo/orgs", "repos_url": "https://api.github.com/users/AndreaCorallo/repos", "events_url": "https://api.github.com/users/AndreaCorallo/events{/privacy}", "received_events_url": "https://api.github.com/users/AndreaCorallo/received_events", "type": "User", "site_admin": false}, "committer": {"login": "AndreaCorallo", "id": 6765576, "node_id": "MDQ6VXNlcjY3NjU1NzY=", "avatar_url": "https://avatars.githubusercontent.com/u/6765576?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AndreaCorallo", "html_url": "https://github.com/AndreaCorallo", "followers_url": "https://api.github.com/users/AndreaCorallo/followers", "following_url": "https://api.github.com/users/AndreaCorallo/following{/other_user}", "gists_url": "https://api.github.com/users/AndreaCorallo/gists{/gist_id}", "starred_url": "https://api.github.com/users/AndreaCorallo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AndreaCorallo/subscriptions", "organizations_url": "https://api.github.com/users/AndreaCorallo/orgs", "repos_url": "https://api.github.com/users/AndreaCorallo/repos", "events_url": "https://api.github.com/users/AndreaCorallo/events{/privacy}", "received_events_url": "https://api.github.com/users/AndreaCorallo/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "8c4e33d2032ab150748ea2fe1df2b1c00652a338", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8c4e33d2032ab150748ea2fe1df2b1c00652a338", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8c4e33d2032ab150748ea2fe1df2b1c00652a338"}], "stats": {"total": 80, "additions": 40, "deletions": 40}, "files": [{"sha": "6de51b521bacb0530799c7cbddb5f6b170bf441c", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 40, "deletions": 40, "changes": 80, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3793ecc10fd4f8be8abd65ba41824f3cc91238e7/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3793ecc10fd4f8be8abd65ba41824f3cc91238e7/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=3793ecc10fd4f8be8abd65ba41824f3cc91238e7", "patch": "@@ -2985,7 +2985,7 @@ tls_symbolic_operand_type (rtx addr)\n   enum tls_model tls_kind = TLS_MODEL_NONE;\n   poly_int64 offset;\n   addr = strip_offset_and_salt (addr, &offset);\n-  if (GET_CODE (addr) == SYMBOL_REF)\n+  if (SYMBOL_REF_P (addr))\n     tls_kind = SYMBOL_REF_TLS_MODEL (addr);\n \n   return tls_kind;\n@@ -3126,7 +3126,7 @@ aarch64_load_symref_appropriately (rtx dest, rtx imm,\n \t/* The operand is expected to be MEM.  Whenever the related insn\n \t   pattern changed, above code which calculate mem should be\n \t   updated.  */\n-\tgcc_assert (GET_CODE (mem) == MEM);\n+\tgcc_assert (MEM_P (mem));\n \tMEM_READONLY_P (mem) = 1;\n \tMEM_NOTRAP_P (mem) = 1;\n \temit_insn (insn);\n@@ -3169,7 +3169,7 @@ aarch64_load_symref_appropriately (rtx dest, rtx imm,\n \t    mem = XVECEXP (XEXP (SET_SRC (insn), 0), 0, 0);\n \t  }\n \n-\tgcc_assert (GET_CODE (mem) == MEM);\n+\tgcc_assert (MEM_P (mem));\n \tMEM_READONLY_P (mem) = 1;\n \tMEM_NOTRAP_P (mem) = 1;\n \temit_insn (insn);\n@@ -4235,7 +4235,7 @@ aarch64_internal_mov_immediate (rtx dest, rtx imm, bool generate,\n bool\n aarch64_mov128_immediate (rtx imm)\n {\n-  if (GET_CODE (imm) == CONST_INT)\n+  if (CONST_INT_P (imm))\n     return true;\n \n   gcc_assert (CONST_WIDE_INT_NUNITS (imm) == 2);\n@@ -5099,8 +5099,8 @@ aarch64_expand_mov_immediate (rtx dest, rtx imm)\n \n   /* Check on what type of symbol it is.  */\n   scalar_int_mode int_mode;\n-  if ((GET_CODE (imm) == SYMBOL_REF\n-       || GET_CODE (imm) == LABEL_REF\n+  if ((SYMBOL_REF_P (imm)\n+       || LABEL_REF_P (imm)\n        || GET_CODE (imm) == CONST\n        || GET_CODE (imm) == CONST_POLY_INT)\n       && is_a <scalar_int_mode> (mode, &int_mode))\n@@ -5390,9 +5390,9 @@ bool\n aarch64_maybe_expand_sve_subreg_move (rtx dest, rtx src)\n {\n   gcc_assert (BYTES_BIG_ENDIAN);\n-  if (GET_CODE (dest) == SUBREG)\n+  if (SUBREG_P (dest))\n     dest = SUBREG_REG (dest);\n-  if (GET_CODE (src) == SUBREG)\n+  if (SUBREG_P (src))\n     src = SUBREG_REG (src);\n \n   /* The optimization handles two single SVE REGs with different element\n@@ -8536,7 +8536,7 @@ aarch64_tls_referenced_p (rtx x)\n   FOR_EACH_SUBRTX (iter, array, x, ALL)\n     {\n       const_rtx x = *iter;\n-      if (GET_CODE (x) == SYMBOL_REF && SYMBOL_REF_TLS_MODEL (x) != 0)\n+      if (SYMBOL_REF_P (x) && SYMBOL_REF_TLS_MODEL (x) != 0)\n \treturn true;\n       /* Don't recurse into UNSPEC_TLS looking for TLS symbols; these are\n \t TLS offsets, not real symbol references.  */\n@@ -8762,7 +8762,7 @@ aarch64_cannot_force_const_mem (machine_mode mode ATTRIBUTE_UNUSED, rtx x)\n \n   poly_int64 offset;\n   rtx base = strip_offset_and_salt (x, &offset);\n-  if (GET_CODE (base) == SYMBOL_REF || GET_CODE (base) == LABEL_REF)\n+  if (SYMBOL_REF_P (base) || LABEL_REF_P (base))\n     {\n       /* We checked for POLY_INT_CST offsets above.  */\n       if (aarch64_classify_symbol (base, offset.to_constant ())\n@@ -8848,7 +8848,7 @@ static bool\n aarch64_base_register_rtx_p (rtx x, bool strict_p)\n {\n   if (!strict_p\n-      && GET_CODE (x) == SUBREG\n+      && SUBREG_P (x)\n       && contains_reg_of_mode[GENERAL_REGS][GET_MODE (SUBREG_REG (x))])\n     x = SUBREG_REG (x);\n \n@@ -8867,7 +8867,7 @@ aarch64_classify_index (struct aarch64_address_info *info, rtx x,\n   int shift;\n \n   /* (reg:P) */\n-  if ((REG_P (x) || GET_CODE (x) == SUBREG)\n+  if ((REG_P (x) || SUBREG_P (x))\n       && GET_MODE (x) == Pmode)\n     {\n       type = ADDRESS_REG_REG;\n@@ -8965,7 +8965,7 @@ aarch64_classify_index (struct aarch64_address_info *info, rtx x,\n     return false;\n \n   if (!strict_p\n-      && GET_CODE (index) == SUBREG\n+      && SUBREG_P (index)\n       && contains_reg_of_mode[GENERAL_REGS][GET_MODE (SUBREG_REG (index))])\n     index = SUBREG_REG (index);\n \n@@ -9261,8 +9261,8 @@ aarch64_classify_address (struct aarch64_address_info *info,\n \t{\n \t  poly_int64 offset;\n \t  rtx sym = strip_offset_and_salt (x, &offset);\n-\t  return ((GET_CODE (sym) == LABEL_REF\n-\t\t   || (GET_CODE (sym) == SYMBOL_REF\n+\t  return ((LABEL_REF_P (sym)\n+\t\t   || (SYMBOL_REF_P (sym)\n \t\t       && CONSTANT_POOL_ADDRESS_P (sym)\n \t\t       && aarch64_pcrelative_literal_loads)));\n \t}\n@@ -9278,7 +9278,7 @@ aarch64_classify_address (struct aarch64_address_info *info,\n \t  poly_int64 offset;\n \t  HOST_WIDE_INT const_offset;\n \t  rtx sym = strip_offset_and_salt (info->offset, &offset);\n-\t  if (GET_CODE (sym) == SYMBOL_REF\n+\t  if (SYMBOL_REF_P (sym)\n \t      && offset.is_constant (&const_offset)\n \t      && (aarch64_classify_symbol (sym, const_offset)\n \t\t  == SYMBOL_SMALL_ABSOLUTE))\n@@ -9340,7 +9340,7 @@ aarch64_symbolic_address_p (rtx x)\n {\n   poly_int64 offset;\n   x = strip_offset_and_salt (x, &offset);\n-  return GET_CODE (x) == SYMBOL_REF || GET_CODE (x) == LABEL_REF;\n+  return SYMBOL_REF_P (x) || LABEL_REF_P (x);\n }\n \n /* Classify the base of symbolic expression X.  */\n@@ -9465,7 +9465,7 @@ aarch64_reinterpret_float_as_int (rtx value, unsigned HOST_WIDE_INT *intval)\n     }\n \n   scalar_float_mode mode;\n-  if (GET_CODE (value) != CONST_DOUBLE\n+  if (!CONST_DOUBLE_P (value)\n       || !is_a <scalar_float_mode> (GET_MODE (value), &mode)\n       || GET_MODE_BITSIZE (mode) > HOST_BITS_PER_WIDE_INT\n       /* Only support up to DF mode.  */\n@@ -9505,7 +9505,7 @@ aarch64_float_const_rtx_p (rtx x)\n      mov/movk pairs over ldr/adrp pairs.  */\n   unsigned HOST_WIDE_INT ival;\n \n-  if (GET_CODE (x) == CONST_DOUBLE\n+  if (CONST_DOUBLE_P (x)\n       && SCALAR_FLOAT_MODE_P (mode)\n       && aarch64_reinterpret_float_as_int (x, &ival))\n     {\n@@ -9544,7 +9544,7 @@ aarch64_can_const_movi_rtx_p (rtx x, machine_mode mode)\n   scalar_int_mode imode;\n   unsigned HOST_WIDE_INT ival;\n \n-  if (GET_CODE (x) == CONST_DOUBLE\n+  if (CONST_DOUBLE_P (x)\n       && SCALAR_FLOAT_MODE_P (mode))\n     {\n       if (!aarch64_reinterpret_float_as_int (x, &ival))\n@@ -9556,7 +9556,7 @@ aarch64_can_const_movi_rtx_p (rtx x, machine_mode mode)\n \n       imode = int_mode_for_mode (mode).require ();\n     }\n-  else if (GET_CODE (x) == CONST_INT\n+  else if (CONST_INT_P (x)\n \t   && is_a <scalar_int_mode> (mode, &imode))\n     ival = INTVAL (x);\n   else\n@@ -9707,7 +9707,7 @@ aarch64_select_cc_mode (RTX_CODE code, rtx x, rtx y)\n      the comparison will have to be swapped when we emit the assembly\n      code.  */\n   if ((mode_x == SImode || mode_x == DImode)\n-      && (REG_P (y) || GET_CODE (y) == SUBREG || y == const0_rtx)\n+      && (REG_P (y) || SUBREG_P (y) || y == const0_rtx)\n       && (code_x == ASHIFT || code_x == ASHIFTRT\n \t  || code_x == LSHIFTRT\n \t  || code_x == ZERO_EXTEND || code_x == SIGN_EXTEND))\n@@ -9716,7 +9716,7 @@ aarch64_select_cc_mode (RTX_CODE code, rtx x, rtx y)\n   /* Similarly for a negated operand, but we can only do this for\n      equalities.  */\n   if ((mode_x == SImode || mode_x == DImode)\n-      && (REG_P (y) || GET_CODE (y) == SUBREG)\n+      && (REG_P (y) || SUBREG_P (y))\n       && (code == EQ || code == NE)\n       && code_x == NEG)\n     return CC_Zmode;\n@@ -10510,7 +10510,7 @@ aarch64_print_operand (FILE *f, rtx x, int code)\n       {\n \tmachine_mode mode = GET_MODE (x);\n \n-\tif (GET_CODE (x) != MEM\n+\tif (!MEM_P (x)\n \t    || (code == 'y' && maybe_ne (GET_MODE_SIZE (mode), 16)))\n \t  {\n \t    output_operand_lossage (\"invalid operand for '%%%c'\", code);\n@@ -10673,7 +10673,7 @@ aarch64_label_mentioned_p (rtx x)\n   const char *fmt;\n   int i;\n \n-  if (GET_CODE (x) == LABEL_REF)\n+  if (LABEL_REF_P (x))\n     return true;\n \n   /* UNSPEC_TLS entries for a symbol include a LABEL_REF for the\n@@ -10855,7 +10855,7 @@ aarch64_secondary_reload (bool in_p ATTRIBUTE_UNUSED, rtx x,\n \n   /* If we have to disable direct literal pool loads and stores because the\n      function is too big, then we need a scratch register.  */\n-  if (MEM_P (x) && GET_CODE (x) == SYMBOL_REF && CONSTANT_POOL_ADDRESS_P (x)\n+  if (MEM_P (x) && SYMBOL_REF_P (x) && CONSTANT_POOL_ADDRESS_P (x)\n       && (SCALAR_FLOAT_MODE_P (GET_MODE (x))\n \t  || targetm.vector_mode_supported_p (GET_MODE (x)))\n       && !aarch64_pcrelative_literal_loads)\n@@ -11086,7 +11086,7 @@ aarch64_preferred_reload_class (rtx x, reg_class_t regclass)\n       rtx lhs = XEXP (x, 0);\n \n       /* Look through a possible SUBREG introduced by ILP32.  */\n-      if (GET_CODE (lhs) == SUBREG)\n+      if (SUBREG_P (lhs))\n \tlhs = SUBREG_REG (lhs);\n \n       gcc_assert (REG_P (lhs));\n@@ -11546,7 +11546,7 @@ aarch64_address_cost (rtx x,\n \n   if (!aarch64_classify_address (&info, x, mode, false))\n     {\n-      if (GET_CODE (x) == CONST || GET_CODE (x) == SYMBOL_REF)\n+      if (GET_CODE (x) == CONST || SYMBOL_REF_P (x))\n \t{\n \t  /* This is a CONST or SYMBOL ref which will be split\n \t     in a different way depending on the code model in use.\n@@ -15947,7 +15947,7 @@ aarch64_tls_symbol_p (rtx x)\n     return false;\n \n   x = strip_salt (x);\n-  if (GET_CODE (x) != SYMBOL_REF)\n+  if (!SYMBOL_REF_P (x))\n     return false;\n \n   return SYMBOL_REF_TLS_MODEL (x) != 0;\n@@ -16004,7 +16004,7 @@ aarch64_classify_symbol (rtx x, HOST_WIDE_INT offset)\n {\n   x = strip_salt (x);\n \n-  if (GET_CODE (x) == LABEL_REF)\n+  if (LABEL_REF_P (x))\n     {\n       switch (aarch64_cmodel)\n \t{\n@@ -16025,7 +16025,7 @@ aarch64_classify_symbol (rtx x, HOST_WIDE_INT offset)\n \t}\n     }\n \n-  if (GET_CODE (x) == SYMBOL_REF)\n+  if (SYMBOL_REF_P (x))\n     {\n       if (aarch64_tls_symbol_p (x))\n \treturn aarch64_classify_tls_symbol (x);\n@@ -16105,7 +16105,7 @@ aarch64_legitimate_pic_operand_p (rtx x)\n {\n   poly_int64 offset;\n   x = strip_offset_and_salt (x, &offset);\n-  if (GET_CODE (x) == SYMBOL_REF)\n+  if (SYMBOL_REF_P (x))\n     return false;\n \n   return true;\n@@ -16166,7 +16166,7 @@ aarch64_legitimate_constant_p (machine_mode mode, rtx x)\n     return true;\n \n   /* Label references are always constant.  */\n-  if (GET_CODE (x) == LABEL_REF)\n+  if (LABEL_REF_P (x))\n     return true;\n \n   return false;\n@@ -17609,7 +17609,7 @@ aarch64_sve_float_arith_immediate_p (rtx x, bool negate_p)\n   REAL_VALUE_TYPE r;\n \n   if (!const_vec_duplicate_p (x, &elt)\n-      || GET_CODE (elt) != CONST_DOUBLE)\n+      || !CONST_DOUBLE_P (elt))\n     return false;\n \n   r = *CONST_DOUBLE_REAL_VALUE (elt);\n@@ -17633,7 +17633,7 @@ aarch64_sve_float_mul_immediate_p (rtx x)\n   rtx elt;\n \n   return (const_vec_duplicate_p (x, &elt)\n-\t  && GET_CODE (elt) == CONST_DOUBLE\n+\t  && CONST_DOUBLE_P (elt)\n \t  && (real_equal (CONST_DOUBLE_REAL_VALUE (elt), &dconsthalf)\n \t      || real_equal (CONST_DOUBLE_REAL_VALUE (elt), &dconst2)));\n }\n@@ -18052,7 +18052,7 @@ aarch64_mov_operand_p (rtx x, machine_mode mode)\n     }\n \n   x = strip_salt (x);\n-  if (GET_CODE (x) == SYMBOL_REF && mode == DImode && CONSTANT_ADDRESS_P (x))\n+  if (SYMBOL_REF_P (x) && mode == DImode && CONSTANT_ADDRESS_P (x))\n     return true;\n \n   if (TARGET_SVE && aarch64_sve_cnt_immediate_p (x))\n@@ -22083,20 +22083,20 @@ fusion_load_store (rtx_insn *insn, rtx *base, rtx *offset)\n     {\n       fusion = SCHED_FUSION_LD_SIGN_EXTEND;\n       src = XEXP (src, 0);\n-      if (GET_CODE (src) != MEM || GET_MODE (src) != SImode)\n+      if (!MEM_P (src) || GET_MODE (src) != SImode)\n \treturn SCHED_FUSION_NONE;\n     }\n   else if (GET_CODE (src) == ZERO_EXTEND)\n     {\n       fusion = SCHED_FUSION_LD_ZERO_EXTEND;\n       src = XEXP (src, 0);\n-      if (GET_CODE (src) != MEM || GET_MODE (src) != SImode)\n+      if (!MEM_P (src) || GET_MODE (src) != SImode)\n \treturn SCHED_FUSION_NONE;\n     }\n \n-  if (GET_CODE (src) == MEM && REG_P (dest))\n+  if (MEM_P (src) && REG_P (dest))\n     extract_base_offset_in_addr (src, base, offset);\n-  else if (GET_CODE (dest) == MEM && (REG_P (src) || src == const0_rtx))\n+  else if (MEM_P (dest) && (REG_P (src) || src == const0_rtx))\n     {\n       fusion = SCHED_FUSION_ST;\n       extract_base_offset_in_addr (dest, base, offset);"}]}