{"sha": "b436bf38052013914d21f8833c9b764598bf38af", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YjQzNmJmMzgwNTIwMTM5MTRkMjFmODgzM2M5Yjc2NDU5OGJmMzhhZg==", "commit": {"author": {"name": "Diego Novillo", "email": "dnovillo@google.com", "date": "2012-11-02T19:43:25Z"}, "committer": {"name": "Diego Novillo", "email": "dnovillo@gcc.gnu.org", "date": "2012-11-02T19:43:25Z"}, "message": "Add a new option --clean_build to validate_failures.py\n\nThis is useful when you have two builds of the same compiler.  One with\nyour changes.  The other one, a clean build at the same revision.\nInstead of using a manifest file, --clean_build will compare the\nresults it gather from the patched build against those it gathers from\nthe clean build.\n\nUsage\n\n$ cd /top/of/patched/gcc/bld\n$ validate_failures.py --clean_build=clean/bld-gcc\nSource directory: /usr/local/google/home/dnovillo/gcc/trunk\nBuild target:     x86_64-unknown-linux-gnu\nGetting actual results from build directory .\n        ./x86_64-unknown-linux-gnu/libstdc++-v3/testsuite/libstdc++.sum\n        ./x86_64-unknown-linux-gnu/libffi/testsuite/libffi.sum\n        ./x86_64-unknown-linux-gnu/libgomp/testsuite/libgomp.sum\n        ./x86_64-unknown-linux-gnu/libgo/libgo.sum\n        ./x86_64-unknown-linux-gnu/boehm-gc/testsuite/boehm-gc.sum\n        ./x86_64-unknown-linux-gnu/libatomic/testsuite/libatomic.sum\n        ./x86_64-unknown-linux-gnu/libmudflap/testsuite/libmudflap.sum\n        ./x86_64-unknown-linux-gnu/libitm/testsuite/libitm.sum\n        ./x86_64-unknown-linux-gnu/libjava/testsuite/libjava.sum\n        ./gcc/testsuite/g++/g++.sum\n        ./gcc/testsuite/gnat/gnat.sum\n        ./gcc/testsuite/ada/acats/acats.sum\n        ./gcc/testsuite/gcc/gcc.sum\n        ./gcc/testsuite/gfortran/gfortran.sum\n        ./gcc/testsuite/obj-c++/obj-c++.sum\n        ./gcc/testsuite/go/go.sum\n        ./gcc/testsuite/objc/objc.sum\nGetting actual results from build directory clean/bld-gcc\n        clean/bld-gcc/x86_64-unknown-linux-gnu/libstdc++-v3/testsuite/libstdc++.sum\n        clean/bld-gcc/x86_64-unknown-linux-gnu/libffi/testsuite/libffi.sum\n        clean/bld-gcc/x86_64-unknown-linux-gnu/libgomp/testsuite/libgomp.sum\n        clean/bld-gcc/x86_64-unknown-linux-gnu/libgo/libgo.sum\n        clean/bld-gcc/x86_64-unknown-linux-gnu/boehm-gc/testsuite/boehm-gc.sum\n        clean/bld-gcc/x86_64-unknown-linux-gnu/libatomic/testsuite/libatomic.sum\n        clean/bld-gcc/x86_64-unknown-linux-gnu/libmudflap/testsuite/libmudflap.sum\n        clean/bld-gcc/x86_64-unknown-linux-gnu/libitm/testsuite/libitm.sum\n        clean/bld-gcc/x86_64-unknown-linux-gnu/libjava/testsuite/libjava.sum\n        clean/bld-gcc/gcc/testsuite/g++/g++.sum\n        clean/bld-gcc/gcc/testsuite/gnat/gnat.sum\n        clean/bld-gcc/gcc/testsuite/ada/acats/acats.sum\n        clean/bld-gcc/gcc/testsuite/gcc/gcc.sum\n        clean/bld-gcc/gcc/testsuite/gfortran/gfortran.sum\n        clean/bld-gcc/gcc/testsuite/obj-c++/obj-c++.sum\n        clean/bld-gcc/gcc/testsuite/go/go.sum\n        clean/bld-gcc/gcc/testsuite/objc/objc.sum\n\nSUCCESS: No unexpected failures.\n\n2012-11-02  Diego Novillo  <dnovillo@google.com>\n\n\t* testsuite-management/validate_failures.py: Add option\n\t--clean_build to compare test results against another\n\tbuild.\n\nFrom-SVN: r193105", "tree": {"sha": "bd69ff9dffa72ef8ecfbd18099ab506ae575c2d0", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/bd69ff9dffa72ef8ecfbd18099ab506ae575c2d0"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/b436bf38052013914d21f8833c9b764598bf38af", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b436bf38052013914d21f8833c9b764598bf38af", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b436bf38052013914d21f8833c9b764598bf38af", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b436bf38052013914d21f8833c9b764598bf38af/comments", "author": {"login": "dnovillo", "id": 7295335, "node_id": "MDQ6VXNlcjcyOTUzMzU=", "avatar_url": "https://avatars.githubusercontent.com/u/7295335?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dnovillo", "html_url": "https://github.com/dnovillo", "followers_url": "https://api.github.com/users/dnovillo/followers", "following_url": "https://api.github.com/users/dnovillo/following{/other_user}", "gists_url": "https://api.github.com/users/dnovillo/gists{/gist_id}", "starred_url": "https://api.github.com/users/dnovillo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dnovillo/subscriptions", "organizations_url": "https://api.github.com/users/dnovillo/orgs", "repos_url": "https://api.github.com/users/dnovillo/repos", "events_url": "https://api.github.com/users/dnovillo/events{/privacy}", "received_events_url": "https://api.github.com/users/dnovillo/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "73ddf95bf187bda7f85cac64974c843328a0832d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/73ddf95bf187bda7f85cac64974c843328a0832d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/73ddf95bf187bda7f85cac64974c843328a0832d"}], "stats": {"total": 72, "additions": 53, "deletions": 19}, "files": [{"sha": "6e52ef6a37b970a666ab48cdaa2d791d140feb90", "filename": "contrib/ChangeLog", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b436bf38052013914d21f8833c9b764598bf38af/contrib%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b436bf38052013914d21f8833c9b764598bf38af/contrib%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2FChangeLog?ref=b436bf38052013914d21f8833c9b764598bf38af", "patch": "@@ -1,3 +1,9 @@\n+2012-11-02  Diego Novillo  <dnovillo@google.com>\n+\n+\t* testsuite-management/validate_failures.py: Add option\n+\t--clean_build to compare test results against another\n+\tbuild.\n+\n 2012-10-31  Diego Novillo  <dnovillo@google.com>\n \n \t* testsuite-management/validate_failures.py: Fix parsing"}, {"sha": "739193715cbe8798e0f65aeafc376a5bd47e3878", "filename": "contrib/testsuite-management/validate_failures.py", "status": "modified", "additions": 47, "deletions": 19, "changes": 66, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b436bf38052013914d21f8833c9b764598bf38af/contrib%2Ftestsuite-management%2Fvalidate_failures.py", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b436bf38052013914d21f8833c9b764598bf38af/contrib%2Ftestsuite-management%2Fvalidate_failures.py", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2Ftestsuite-management%2Fvalidate_failures.py?ref=b436bf38052013914d21f8833c9b764598bf38af", "patch": "@@ -292,14 +292,35 @@ def PrintSummary(msg, summary):\n \n def GetSumFiles(results, build_dir):\n   if not results:\n-    print 'Getting actual results from build'\n+    print 'Getting actual results from build directory %s' % build_dir\n     sum_files = CollectSumFiles(build_dir)\n   else:\n     print 'Getting actual results from user-provided results'\n     sum_files = results.split()\n   return sum_files\n \n \n+def PerformComparison(expected, actual, ignore_missing_failures):\n+  actual_vs_expected, expected_vs_actual = CompareResults(expected, actual)\n+\n+  tests_ok = True\n+  if len(actual_vs_expected) > 0:\n+    PrintSummary('Unexpected results in this build (new failures)',\n+                 actual_vs_expected)\n+    tests_ok = False\n+\n+  if not ignore_missing_failures and len(expected_vs_actual) > 0:\n+    PrintSummary('Expected results not present in this build (fixed tests)'\n+                 '\\n\\nNOTE: This is not a failure.  It just means that these '\n+                 'tests were expected\\nto fail, but they worked in this '\n+                 'configuration.\\n', expected_vs_actual)\n+\n+  if tests_ok:\n+    print '\\nSUCCESS: No unexpected failures.'\n+\n+  return tests_ok\n+\n+\n def CheckExpectedResults(options):\n   if not options.manifest:\n     (srcdir, target, valid_build) = GetBuildData(options)\n@@ -320,24 +341,7 @@ def CheckExpectedResults(options):\n     PrintSummary('Tests expected to fail', manifest)\n     PrintSummary('\\nActual test results', actual)\n \n-  actual_vs_manifest, manifest_vs_actual = CompareResults(manifest, actual)\n-\n-  tests_ok = True\n-  if len(actual_vs_manifest) > 0:\n-    PrintSummary('Build results not in the manifest', actual_vs_manifest)\n-    tests_ok = False\n-\n-  if not options.ignore_missing_failures and len(manifest_vs_actual) > 0:\n-    PrintSummary('Manifest results not present in the build'\n-                 '\\n\\nNOTE: This is not a failure.  It just means that the '\n-                 'manifest expected\\nthese tests to fail, '\n-                 'but they worked in this configuration.\\n',\n-                 manifest_vs_actual)\n-\n-  if tests_ok:\n-    print '\\nSUCCESS: No unexpected failures.'\n-\n-  return tests_ok\n+  return PerformComparison(manifest, actual, options.ignore_missing_failures)\n \n \n def ProduceManifest(options):\n@@ -361,13 +365,35 @@ def ProduceManifest(options):\n   return True\n \n \n+def CompareBuilds(options):\n+  (srcdir, target, valid_build) = GetBuildData(options)\n+  if not valid_build:\n+    return False\n+\n+  sum_files = GetSumFiles(options.results, options.build_dir)\n+  actual = GetResults(sum_files)\n+\n+  clean_sum_files = GetSumFiles(None, options.clean_build)\n+  clean = GetResults(clean_sum_files)\n+\n+  return PerformComparison(clean, actual, options.ignore_missing_failures)\n+\n+\n def Main(argv):\n   parser = optparse.OptionParser(usage=__doc__)\n \n   # Keep the following list sorted by option name.\n   parser.add_option('--build_dir', action='store', type='string',\n                     dest='build_dir', default='.',\n                     help='Build directory to check (default = .)')\n+  parser.add_option('--clean_build', action='store', type='string',\n+                    dest='clean_build', default=None,\n+                    help='Compare test results from this build against '\n+                    'those of another (clean) build.  Use this option '\n+                    'when comparing the test results of your patch versus '\n+                    'the test results of a clean build without your patch. '\n+                    'You must provide the path to the top directory of your '\n+                    'clean build.')\n   parser.add_option('--force', action='store_true', dest='force',\n                     default=False, help='When used with --produce_manifest, '\n                     'it will overwrite an existing manifest file '\n@@ -400,6 +426,8 @@ def Main(argv):\n \n   if options.produce_manifest:\n     retval = ProduceManifest(options)\n+  elif options.clean_build:\n+    retval = CompareBuilds(options)\n   else:\n     retval = CheckExpectedResults(options)\n "}]}