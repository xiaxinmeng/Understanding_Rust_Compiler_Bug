{"sha": "1ac09ef2c611d3113665ec8c74e38b125217edb3", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MWFjMDllZjJjNjExZDMxMTM2NjVlYzhjNzRlMzhiMTI1MjE3ZWRiMw==", "commit": {"author": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2019-05-14T14:59:42Z"}, "committer": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2019-05-14T14:59:42Z"}, "message": "libgo: reduce overhead for memory/block/mutex profiling\n    \n    Revise the gccgo version of memory/block/mutex profiling to reduce\n    runtime overhead. The main change is to collect raw stack traces while\n    the profile is on line, then post-process the stacks just prior to the\n    point where we are ready to use the final product. Memory profiling\n    (at a very low sampling rate) is enabled by default, and the overhead\n    of the symbolization / DWARF-reading from backtrace_full was slowing\n    things down relative to the main Go runtime.\n    \n    Reviewed-on: https://go-review.googlesource.com/c/gofrontend/+/171497\n\nFrom-SVN: r271172", "tree": {"sha": "0bed1e11d205c99ef1f13dd4b7aece761779c360", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/0bed1e11d205c99ef1f13dd4b7aece761779c360"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/1ac09ef2c611d3113665ec8c74e38b125217edb3", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1ac09ef2c611d3113665ec8c74e38b125217edb3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/1ac09ef2c611d3113665ec8c74e38b125217edb3", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1ac09ef2c611d3113665ec8c74e38b125217edb3/comments", "author": null, "committer": null, "parents": [{"sha": "ce9f305e44ff0353ee9e6cb07599240354ae9ed2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ce9f305e44ff0353ee9e6cb07599240354ae9ed2", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ce9f305e44ff0353ee9e6cb07599240354ae9ed2"}], "stats": {"total": 453, "additions": 360, "deletions": 93}, "files": [{"sha": "672ad671bb31097d515cbf96b381e79aba82789f", "filename": "gcc/go/gofrontend/MERGE", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ac09ef2c611d3113665ec8c74e38b125217edb3/gcc%2Fgo%2Fgofrontend%2FMERGE", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ac09ef2c611d3113665ec8c74e38b125217edb3/gcc%2Fgo%2Fgofrontend%2FMERGE", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2FMERGE?ref=1ac09ef2c611d3113665ec8c74e38b125217edb3", "patch": "@@ -1,4 +1,4 @@\n-3f015e128bf6d1d9279f3d43e26f60f0927019cb\n+6112f9b8fa9d57d2db8a709cc8b44a94d778d08a\n \n The first line of this file holds the git revision number of the last\n merge done from the gofrontend repository."}, {"sha": "b0506a8e47526289360032a2dda97c3bd68aa9f1", "filename": "libgo/go/runtime/heapdump.go", "status": "modified", "additions": 4, "deletions": 6, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fgo%2Fruntime%2Fheapdump.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fgo%2Fruntime%2Fheapdump.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fheapdump.go?ref=1ac09ef2c611d3113665ec8c74e38b125217edb3", "patch": "@@ -437,17 +437,15 @@ func dumpmemstats() {\n \tdumpint(uint64(memstats.numgc))\n }\n \n-func dumpmemprof_callback(b *bucket, nstk uintptr, pstk *location, size, allocs, frees uintptr) {\n-\tstk := (*[100000]location)(unsafe.Pointer(pstk))\n+func dumpmemprof_callback(b *bucket, nstk uintptr, pstk *uintptr, size, allocs, frees uintptr) {\n+\tstk := (*[100000]uintptr)(unsafe.Pointer(pstk))\n \tdumpint(tagMemProf)\n \tdumpint(uint64(uintptr(unsafe.Pointer(b))))\n \tdumpint(uint64(size))\n \tdumpint(uint64(nstk))\n \tfor i := uintptr(0); i < nstk; i++ {\n-\t\tpc := stk[i].pc\n-\t\tfn := stk[i].function\n-\t\tfile := stk[i].filename\n-\t\tline := stk[i].lineno\n+\t\tpc := stk[i]\n+\t\tfn, file, line, _ := funcfileline(pc, -1)\n \t\tif fn == \"\" {\n \t\t\tvar buf [64]byte\n \t\t\tn := len(buf)"}, {"sha": "1b8a7a3ddd77a416bc67c255d1cf68b2031b05c0", "filename": "libgo/go/runtime/mgcmark.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fgo%2Fruntime%2Fmgcmark.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fgo%2Fruntime%2Fmgcmark.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmgcmark.go?ref=1ac09ef2c611d3113665ec8c74e38b125217edb3", "patch": "@@ -1085,7 +1085,7 @@ func scanstackblockwithmap(pc, b0, n0 uintptr, ptrmask *uint8, gcw *gcWork) {\n \t\t\t\t\t\tspan != nil && span.state != mSpanManual &&\n \t\t\t\t\t\t\t(obj < span.base() || obj >= span.limit || span.state != mSpanInUse) {\n \t\t\t\t\t\tprint(\"runtime: found in object at *(\", hex(b), \"+\", hex(i), \") = \", hex(obj), \", pc=\", hex(pc), \"\\n\")\n-\t\t\t\t\t\tname, file, line := funcfileline(pc, -1)\n+\t\t\t\t\t\tname, file, line, _ := funcfileline(pc, -1)\n \t\t\t\t\t\tprint(name, \"\\n\", file, \":\", line, \"\\n\")\n \t\t\t\t\t\t//gcDumpObject(\"object\", b, i)\n \t\t\t\t\t\tthrow(\"found bad pointer in Go stack (incorrect use of unsafe or cgo?)\")"}, {"sha": "9238e2bb012651521066356cf6fe8fdb351f85c1", "filename": "libgo/go/runtime/mprof.go", "status": "modified", "additions": 256, "deletions": 69, "changes": 325, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fgo%2Fruntime%2Fmprof.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fgo%2Fruntime%2Fmprof.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmprof.go?ref=1ac09ef2c611d3113665ec8c74e38b125217edb3", "patch": "@@ -24,6 +24,10 @@ const (\n \tblockProfile\n \tmutexProfile\n \n+\t// a profile bucket from one of the categories above whose stack\n+\t// trace has been fixed up / pruned.\n+\tprunedProfile\n+\n \t// size of bucket hash table\n \tbuckHashSize = 179999\n \n@@ -138,11 +142,13 @@ type blockRecord struct {\n }\n \n var (\n-\tmbuckets  *bucket // memory profile buckets\n-\tbbuckets  *bucket // blocking profile buckets\n-\txbuckets  *bucket // mutex profile buckets\n-\tbuckhash  *[179999]*bucket\n-\tbucketmem uintptr\n+\tmbuckets    *bucket // memory profile buckets\n+\tbbuckets    *bucket // blocking profile buckets\n+\txbuckets    *bucket // mutex profile buckets\n+\tsbuckets    *bucket // pre-symbolization profile buckets (stacks fixed up)\n+\tfreebuckets *bucket // freelist of unused fixed up profile buckets\n+\tbuckhash    *[179999]*bucket\n+\tbucketmem   uintptr\n \n \tmProf struct {\n \t\t// All fields in mProf are protected by proflock.\n@@ -158,12 +164,35 @@ var (\n \n const mProfCycleWrap = uint32(len(memRecord{}.future)) * (2 << 24)\n \n+// payloadOffset() returns a pointer into the part of a bucket\n+// containing the profile payload (skips past the bucket struct itself\n+// and then the stack trace).\n+func payloadOffset(typ bucketType, nstk uintptr) uintptr {\n+\tif typ == prunedProfile {\n+\t\t// To allow reuse of prunedProfile buckets between different\n+\t\t// collections, allocate them with the max stack size (the portion\n+\t\t// of the stack used will vary from trace to trace).\n+\t\tnstk = maxStack\n+\t}\n+\treturn unsafe.Sizeof(bucket{}) + uintptr(nstk)*unsafe.Sizeof(uintptr)\n+}\n+\n+func max(x, y uintptr) uintptr {\n+\tif x > y {\n+\t\treturn x\n+\t}\n+\treturn y\n+}\n+\n // newBucket allocates a bucket with the given type and number of stack entries.\n func newBucket(typ bucketType, nstk int) *bucket {\n-\tsize := unsafe.Sizeof(bucket{}) + uintptr(nstk)*unsafe.Sizeof(location{})\n+\tsize := payloadOffset(typ, uintptr(nstk))\n \tswitch typ {\n \tdefault:\n \t\tthrow(\"invalid profile bucket type\")\n+\tcase prunedProfile:\n+\t\t// stack-fixed buckets are large enough to accommodate any payload.\n+\t\tsize += max(unsafe.Sizeof(memRecord{}), unsafe.Sizeof(blockRecord{}))\n \tcase memProfile:\n \t\tsize += unsafe.Sizeof(memRecord{})\n \tcase blockProfile, mutexProfile:\n@@ -178,31 +207,29 @@ func newBucket(typ bucketType, nstk int) *bucket {\n }\n \n // stk returns the slice in b holding the stack.\n-func (b *bucket) stk() []location {\n-\tstk := (*[maxStack]location)(add(unsafe.Pointer(b), unsafe.Sizeof(*b)))\n+func (b *bucket) stk() []uintptr {\n+\tstk := (*[maxStack]uintptr)(add(unsafe.Pointer(b), unsafe.Sizeof(*b)))\n \treturn stk[:b.nstk:b.nstk]\n }\n \n // mp returns the memRecord associated with the memProfile bucket b.\n func (b *bucket) mp() *memRecord {\n-\tif b.typ != memProfile {\n+\tif b.typ != memProfile && b.typ != prunedProfile {\n \t\tthrow(\"bad use of bucket.mp\")\n \t}\n-\tdata := add(unsafe.Pointer(b), unsafe.Sizeof(*b)+b.nstk*unsafe.Sizeof(location{}))\n-\treturn (*memRecord)(data)\n+\treturn (*memRecord)(add(unsafe.Pointer(b), payloadOffset(b.typ, b.nstk)))\n }\n \n // bp returns the blockRecord associated with the blockProfile bucket b.\n func (b *bucket) bp() *blockRecord {\n-\tif b.typ != blockProfile && b.typ != mutexProfile {\n+\tif b.typ != blockProfile && b.typ != mutexProfile && b.typ != prunedProfile {\n \t\tthrow(\"bad use of bucket.bp\")\n \t}\n-\tdata := add(unsafe.Pointer(b), unsafe.Sizeof(*b)+b.nstk*unsafe.Sizeof(location{}))\n-\treturn (*blockRecord)(data)\n+\treturn (*blockRecord)(add(unsafe.Pointer(b), payloadOffset(b.typ, b.nstk)))\n }\n \n // Return the bucket for stk[0:nstk], allocating new bucket if needed.\n-func stkbucket(typ bucketType, size uintptr, stk []location, alloc bool) *bucket {\n+func stkbucket(typ bucketType, size uintptr, stk []uintptr, alloc bool) *bucket {\n \tif buckhash == nil {\n \t\tbuckhash = (*[buckHashSize]*bucket)(sysAlloc(unsafe.Sizeof(*buckhash), &memstats.buckhash_sys))\n \t\tif buckhash == nil {\n@@ -212,8 +239,8 @@ func stkbucket(typ bucketType, size uintptr, stk []location, alloc bool) *bucket\n \n \t// Hash stack.\n \tvar h uintptr\n-\tfor _, loc := range stk {\n-\t\th += loc.pc\n+\tfor _, pc := range stk {\n+\t\th += pc\n \t\th += h << 10\n \t\th ^= h >> 6\n \t}\n@@ -249,14 +276,17 @@ func stkbucket(typ bucketType, size uintptr, stk []location, alloc bool) *bucket\n \t} else if typ == mutexProfile {\n \t\tb.allnext = xbuckets\n \t\txbuckets = b\n+\t} else if typ == prunedProfile {\n+\t\tb.allnext = sbuckets\n+\t\tsbuckets = b\n \t} else {\n \t\tb.allnext = bbuckets\n \t\tbbuckets = b\n \t}\n \treturn b\n }\n \n-func eqslice(x, y []location) bool {\n+func eqslice(x, y []uintptr) bool {\n \tif len(x) != len(y) {\n \t\treturn false\n \t}\n@@ -338,8 +368,8 @@ func mProf_PostSweep() {\n \n // Called by malloc to record a profiled block.\n func mProf_Malloc(p unsafe.Pointer, size uintptr) {\n-\tvar stk [maxStack]location\n-\tnstk := callers(4, stk[:])\n+\tvar stk [maxStack]uintptr\n+\tnstk := callersRaw(1, stk[:])\n \tlock(&proflock)\n \tb := stkbucket(memProfile, size, stk[:nstk], true)\n \tc := mProf.cycle\n@@ -414,13 +444,13 @@ func blocksampled(cycles int64) bool {\n func saveblockevent(cycles int64, skip int, which bucketType) {\n \tgp := getg()\n \tvar nstk int\n-\tvar stk [maxStack]location\n+\tvar stk [maxStack]uintptr\n \tif gp.m.curg == nil || gp.m.curg == gp {\n-\t\tnstk = callers(skip, stk[:])\n+\t\tnstk = callersRaw(skip, stk[:])\n \t} else {\n \t\t// FIXME: This should get a traceback of gp.m.curg.\n \t\t// nstk = gcallers(gp.m.curg, skip, stk[:])\n-\t\tnstk = callers(skip, stk[:])\n+\t\tnstk = callersRaw(skip, stk[:])\n \t}\n \tlock(&proflock)\n \tb := stkbucket(which, 0, stk[:nstk], true)\n@@ -521,6 +551,150 @@ func (r *MemProfileRecord) Stack() []uintptr {\n \treturn r.Stack0[0:]\n }\n \n+// reusebucket tries to pick a prunedProfile bucket off\n+// the freebuckets list, returning it if one is available or nil\n+// if the free list is empty.\n+func reusebucket(nstk int) *bucket {\n+\tvar b *bucket\n+\tif freebuckets != nil {\n+\t\tb = freebuckets\n+\t\tfreebuckets = freebuckets.allnext\n+\t\tb.typ = prunedProfile\n+\t\tb.nstk = uintptr(nstk)\n+\t\tmp := b.mp()\n+\t\t// Hack: rely on the fact that memprofile records are\n+\t\t// larger than blockprofile records when clearing.\n+\t\t*mp = memRecord{}\n+\t}\n+\treturn b\n+}\n+\n+// freebucket appends the specified prunedProfile bucket\n+// onto the free list, and removes references to it from the hash.\n+func freebucket(tofree *bucket) *bucket {\n+\t// Thread this bucket into the free list.\n+\tret := tofree.allnext\n+\ttofree.allnext = freebuckets\n+\tfreebuckets = tofree\n+\n+\t// Clean up the hash. The hash may point directly to this bucket...\n+\ti := int(tofree.hash % buckHashSize)\n+\tif buckhash[i] == tofree {\n+\t\tbuckhash[i] = tofree.next\n+\t} else {\n+\t\t// ... or when this bucket was inserted by stkbucket, it may have been\n+\t\t// chained off some other unrelated bucket.\n+\t\tfor b := buckhash[i]; b != nil; b = b.next {\n+\t\t\tif b.next == tofree {\n+\t\t\t\tb.next = tofree.next\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t}\n+\treturn ret\n+}\n+\n+// fixupStack takes a 'raw' stack trace (stack of PCs generated by\n+// callersRaw) and performs pre-symbolization fixup on it, returning\n+// the results in 'canonStack'. For each frame we look at the\n+// file/func/line information, then use that info to decide whether to\n+// include the frame in the final symbolized stack (removing frames\n+// corresponding to 'morestack' routines, for example). We also expand\n+// frames if the PC values to which they refer correponds to inlined\n+// functions to allow for expanded symbolic info to be filled in\n+// later. Note: there is code in go-callers.c's backtrace_full callback()\n+// function that performs very similar fixups; these two code paths\n+// should be kept in sync.\n+func fixupStack(stk []uintptr, canonStack *[maxStack]uintptr, size uintptr) int {\n+\tvar cidx int\n+\tvar termTrace bool\n+\tfor _, pc := range stk {\n+\t\t// Subtract 1 from PC to undo the 1 we added in callback in\n+\t\t// go-callers.c.\n+\t\tfunction, file, _, frames := funcfileline(pc-1, -1)\n+\n+\t\t// Skip split-stack functions (match by function name)\n+\t\tskipFrame := false\n+\t\tif hasPrefix(function, \"_____morestack_\") || hasPrefix(function, \"__morestack_\") {\n+\t\t\tskipFrame = true\n+\t\t}\n+\n+\t\t// Skip split-stack functions (match by file)\n+\t\tif hasSuffix(file, \"/morestack.S\") {\n+\t\t\tskipFrame = true\n+\t\t}\n+\n+\t\t// Skip thunks and recover functions.  There is no equivalent to\n+\t\t// these functions in the gc toolchain.\n+\t\tfcn := function\n+\t\tif hasSuffix(fcn, \"..r\") {\n+\t\t\tskipFrame = true\n+\t\t} else {\n+\t\t\tfor fcn != \"\" && (fcn[len(fcn)-1] >= '0' && fcn[len(fcn)-1] <= '9') {\n+\t\t\t\tfcn = fcn[:len(fcn)-1]\n+\t\t\t}\n+\t\t\tif hasSuffix(fcn, \"..stub\") || hasSuffix(fcn, \"..thunk\") {\n+\t\t\t\tskipFrame = true\n+\t\t\t}\n+\t\t}\n+\t\tif skipFrame {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\t// Terminate the trace if we encounter a frame corresponding to\n+\t\t// runtime.main, runtime.kickoff, makecontext, etc. See the\n+\t\t// corresponding code in go-callers.c, callback function used\n+\t\t// with backtrace_full.\n+\t\tif function == \"makecontext\" {\n+\t\t\ttermTrace = true\n+\t\t}\n+\t\tif hasSuffix(file, \"/proc.c\") && function == \"runtime_mstart\" {\n+\t\t\ttermTrace = true\n+\t\t}\n+\t\tif hasSuffix(file, \"/proc.go\") &&\n+\t\t\t(function == \"runtime.main\" || function == \"runtime.kickoff\") {\n+\t\t\ttermTrace = true\n+\t\t}\n+\n+\t\t// Expand inline frames.\n+\t\tfor i := 0; i < frames; i++ {\n+\t\t\t(*canonStack)[cidx] = pc\n+\t\t\tcidx++\n+\t\t\tif cidx >= maxStack {\n+\t\t\t\ttermTrace = true\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t\tif termTrace {\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\treturn cidx\n+}\n+\n+// fixupBucket takes a raw memprofile bucket and creates a new bucket\n+// in which the stack trace has been fixed up (inline frames expanded,\n+// unwanted frames stripped out). Original bucket is left unmodified;\n+// a new symbolizeProfile bucket may be generated as a side effect.\n+// Payload information from the original bucket is incorporated into\n+// the new bucket.\n+func fixupBucket(b *bucket) {\n+\tvar canonStack [maxStack]uintptr\n+\tframes := fixupStack(b.stk(), &canonStack, b.size)\n+\tcb := stkbucket(prunedProfile, b.size, canonStack[:frames], true)\n+\tswitch b.typ {\n+\tdefault:\n+\t\tthrow(\"invalid profile bucket type\")\n+\tcase memProfile:\n+\t\trawrecord := b.mp()\n+\t\tcb.mp().active.add(&rawrecord.active)\n+\tcase blockProfile, mutexProfile:\n+\t\tbpcount := b.bp().count\n+\t\tcb.bp().count += bpcount\n+\t\tcb.bp().cycles += bpcount\n+\t}\n+}\n+\n // MemProfile returns a profile of memory allocated and freed per allocation\n // site.\n //\n@@ -576,15 +750,31 @@ func MemProfile(p []MemProfileRecord, inuseZero bool) (n int, ok bool) {\n \t\t}\n \t}\n \tif n <= len(p) {\n-\t\tok = true\n-\t\tidx := 0\n-\t\tfor b := mbuckets; b != nil; b = b.allnext {\n+\t\tvar bnext *bucket\n+\n+\t\t// Post-process raw buckets to fix up their stack traces\n+\t\tfor b := mbuckets; b != nil; b = bnext {\n+\t\t\tbnext = b.allnext\n \t\t\tmp := b.mp()\n \t\t\tif inuseZero || mp.active.alloc_bytes != mp.active.free_bytes {\n-\t\t\t\trecord(&p[idx], b)\n-\t\t\t\tidx++\n+\t\t\t\tfixupBucket(b)\n \t\t\t}\n \t\t}\n+\n+\t\t// Record pruned/fixed-up buckets\n+\t\tok = true\n+\t\tidx := 0\n+\t\tfor b := sbuckets; b != nil; b = b.allnext {\n+\t\t\trecord(&p[idx], b)\n+\t\t\tidx++\n+\t\t}\n+\t\tn = idx\n+\n+\t\t// Free up pruned buckets for use in next round\n+\t\tfor b := sbuckets; b != nil; b = bnext {\n+\t\t\tbnext = freebucket(b)\n+\t\t}\n+\t\tsbuckets = nil\n \t}\n \tunlock(&proflock)\n \treturn\n@@ -597,18 +787,18 @@ func record(r *MemProfileRecord, b *bucket) {\n \tr.FreeBytes = int64(mp.active.free_bytes)\n \tr.AllocObjects = int64(mp.active.allocs)\n \tr.FreeObjects = int64(mp.active.frees)\n-\tfor i, loc := range b.stk() {\n+\tfor i, pc := range b.stk() {\n \t\tif i >= len(r.Stack0) {\n \t\t\tbreak\n \t\t}\n-\t\tr.Stack0[i] = loc.pc\n+\t\tr.Stack0[i] = pc\n \t}\n \tfor i := int(b.nstk); i < len(r.Stack0); i++ {\n \t\tr.Stack0[i] = 0\n \t}\n }\n \n-func iterate_memprof(fn func(*bucket, uintptr, *location, uintptr, uintptr, uintptr)) {\n+func iterate_memprof(fn func(*bucket, uintptr, *uintptr, uintptr, uintptr, uintptr)) {\n \tlock(&proflock)\n \tfor b := mbuckets; b != nil; b = b.allnext {\n \t\tmp := b.mp()\n@@ -625,39 +815,59 @@ type BlockProfileRecord struct {\n \tStackRecord\n }\n \n-// BlockProfile returns n, the number of records in the current blocking profile.\n-// If len(p) >= n, BlockProfile copies the profile into p and returns n, true.\n-// If len(p) < n, BlockProfile does not change p and returns n, false.\n-//\n-// Most clients should use the runtime/pprof package or\n-// the testing package's -test.blockprofile flag instead\n-// of calling BlockProfile directly.\n-func BlockProfile(p []BlockProfileRecord) (n int, ok bool) {\n-\tlock(&proflock)\n-\tfor b := bbuckets; b != nil; b = b.allnext {\n+func harvestBlockMutexProfile(buckets *bucket, p []BlockProfileRecord) (n int, ok bool) {\n+\tfor b := buckets; b != nil; b = b.allnext {\n \t\tn++\n \t}\n \tif n <= len(p) {\n+\t\tvar bnext *bucket\n+\n+\t\t// Post-process raw buckets to create pruned/fixed-up buckets\n+\t\tfor b := buckets; b != nil; b = bnext {\n+\t\t\tbnext = b.allnext\n+\t\t\tfixupBucket(b)\n+\t\t}\n+\n+\t\t// Record\n \t\tok = true\n-\t\tfor b := bbuckets; b != nil; b = b.allnext {\n+\t\tfor b := sbuckets; b != nil; b = b.allnext {\n \t\t\tbp := b.bp()\n \t\t\tr := &p[0]\n \t\t\tr.Count = bp.count\n \t\t\tr.Cycles = bp.cycles\n \t\t\ti := 0\n-\t\t\tvar loc location\n-\t\t\tfor i, loc = range b.stk() {\n+\t\t\tvar pc uintptr\n+\t\t\tfor i, pc = range b.stk() {\n \t\t\t\tif i >= len(r.Stack0) {\n \t\t\t\t\tbreak\n \t\t\t\t}\n-\t\t\t\tr.Stack0[i] = loc.pc\n+\t\t\t\tr.Stack0[i] = pc\n \t\t\t}\n \t\t\tfor ; i < len(r.Stack0); i++ {\n \t\t\t\tr.Stack0[i] = 0\n \t\t\t}\n \t\t\tp = p[1:]\n \t\t}\n+\n+\t\t// Free up pruned buckets for use in next round.\n+\t\tfor b := sbuckets; b != nil; b = bnext {\n+\t\t\tbnext = freebucket(b)\n+\t\t}\n+\t\tsbuckets = nil\n \t}\n+\treturn\n+}\n+\n+// BlockProfile returns n, the number of records in the current blocking profile.\n+// If len(p) >= n, BlockProfile copies the profile into p and returns n, true.\n+// If len(p) < n, BlockProfile does not change p and returns n, false.\n+//\n+// Most clients should use the runtime/pprof package or\n+// the testing package's -test.blockprofile flag instead\n+// of calling BlockProfile directly.\n+func BlockProfile(p []BlockProfileRecord) (n int, ok bool) {\n+\tlock(&proflock)\n+\tn, ok = harvestBlockMutexProfile(bbuckets, p)\n \tunlock(&proflock)\n \treturn\n }\n@@ -670,30 +880,7 @@ func BlockProfile(p []BlockProfileRecord) (n int, ok bool) {\n // instead of calling MutexProfile directly.\n func MutexProfile(p []BlockProfileRecord) (n int, ok bool) {\n \tlock(&proflock)\n-\tfor b := xbuckets; b != nil; b = b.allnext {\n-\t\tn++\n-\t}\n-\tif n <= len(p) {\n-\t\tok = true\n-\t\tfor b := xbuckets; b != nil; b = b.allnext {\n-\t\t\tbp := b.bp()\n-\t\t\tr := &p[0]\n-\t\t\tr.Count = int64(bp.count)\n-\t\t\tr.Cycles = bp.cycles\n-\t\t\ti := 0\n-\t\t\tvar loc location\n-\t\t\tfor i, loc = range b.stk() {\n-\t\t\t\tif i >= len(r.Stack0) {\n-\t\t\t\t\tbreak\n-\t\t\t\t}\n-\t\t\t\tr.Stack0[i] = loc.pc\n-\t\t\t}\n-\t\t\tfor ; i < len(r.Stack0); i++ {\n-\t\t\t\tr.Stack0[i] = 0\n-\t\t\t}\n-\t\t\tp = p[1:]\n-\t\t}\n-\t}\n+\tn, ok = harvestBlockMutexProfile(xbuckets, p)\n \tunlock(&proflock)\n \treturn\n }"}, {"sha": "264ad38cc1f26326d424d51463654f9848115b9d", "filename": "libgo/go/runtime/panic.go", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fgo%2Fruntime%2Fpanic.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fgo%2Fruntime%2Fpanic.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fpanic.go?ref=1ac09ef2c611d3113665ec8c74e38b125217edb3", "patch": "@@ -53,7 +53,7 @@ var indexError = error(errorString(\"index out of range\"))\n // entire runtime stack for easier debugging.\n \n func panicindex() {\n-\tname, _, _ := funcfileline(getcallerpc()-1, -1)\n+\tname, _, _, _ := funcfileline(getcallerpc()-1, -1)\n \tif hasPrefix(name, \"runtime.\") {\n \t\tthrow(string(indexError.(errorString)))\n \t}\n@@ -64,7 +64,7 @@ func panicindex() {\n var sliceError = error(errorString(\"slice bounds out of range\"))\n \n func panicslice() {\n-\tname, _, _ := funcfileline(getcallerpc()-1, -1)\n+\tname, _, _, _ := funcfileline(getcallerpc()-1, -1)\n \tif hasPrefix(name, \"runtime.\") {\n \t\tthrow(string(sliceError.(errorString)))\n \t}"}, {"sha": "eac94bf6a8a90993ee946674a69ca31165977ad3", "filename": "libgo/go/runtime/string.go", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fgo%2Fruntime%2Fstring.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fgo%2Fruntime%2Fstring.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fstring.go?ref=1ac09ef2c611d3113665ec8c74e38b125217edb3", "patch": "@@ -360,6 +360,10 @@ func hasPrefix(s, prefix string) bool {\n \treturn len(s) >= len(prefix) && s[:len(prefix)] == prefix\n }\n \n+func hasSuffix(s, suffix string) bool {\n+\treturn len(s) >= len(suffix) && s[len(s)-len(suffix):] == suffix\n+}\n+\n const (\n \tmaxUint = ^uint(0)\n \tmaxInt  = int(maxUint >> 1)"}, {"sha": "8f3c843a78b1e6f0e5b52e37acd7a9d27e23a3bc", "filename": "libgo/go/runtime/symtab.go", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fgo%2Fruntime%2Fsymtab.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fgo%2Fruntime%2Fsymtab.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fsymtab.go?ref=1ac09ef2c611d3113665ec8c74e38b125217edb3", "patch": "@@ -79,7 +79,7 @@ func (ci *Frames) Next() (frame Frame, more bool) {\n \n \t// Subtract 1 from PC to undo the 1 we added in callback in\n \t// go-callers.c.\n-\tfunction, file, line := funcfileline(pc-1, int32(i))\n+\tfunction, file, line, _ := funcfileline(pc-1, int32(i))\n \tif function == \"\" && file == \"\" {\n \t\treturn Frame{}, more\n \t}\n@@ -158,7 +158,7 @@ const (\n // the a *Func describing the innermost function, but with an entry\n // of the outermost function.\n func FuncForPC(pc uintptr) *Func {\n-\tname, _, _ := funcfileline(pc, -1)\n+\tname, _, _, _ := funcfileline(pc, -1)\n \tif name == \"\" {\n \t\treturn nil\n \t}\n@@ -187,7 +187,7 @@ func (f *Func) Entry() uintptr {\n // The result will not be accurate if pc is not a program\n // counter within f.\n func (f *Func) FileLine(pc uintptr) (file string, line int) {\n-\t_, file, line = funcfileline(pc, -1)\n+\t_, file, line, _ = funcfileline(pc, -1)\n \treturn file, line\n }\n \n@@ -261,5 +261,5 @@ func demangleSymbol(s string) string {\n }\n \n // implemented in go-caller.c\n-func funcfileline(uintptr, int32) (string, string, int)\n+func funcfileline(uintptr, int32) (string, string, int, int)\n func funcentry(uintptr) uintptr"}, {"sha": "b0eecf2894e8e6614bb6131c0ad4f95fb47313b2", "filename": "libgo/go/runtime/traceback_gccgo.go", "status": "modified", "additions": 13, "deletions": 3, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fgo%2Fruntime%2Ftraceback_gccgo.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fgo%2Fruntime%2Ftraceback_gccgo.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Ftraceback_gccgo.go?ref=1ac09ef2c611d3113665ec8c74e38b125217edb3", "patch": "@@ -20,7 +20,7 @@ func printcreatedby(gp *g) {\n \tif entry != 0 && tracepc > entry {\n \t\ttracepc -= sys.PCQuantum\n \t}\n-\tfunction, file, line := funcfileline(tracepc, -1)\n+\tfunction, file, line, _ := funcfileline(tracepc, -1)\n \tif function != \"\" && showframe(function, gp, false) && gp.goid != 1 {\n \t\tprintcreatedby1(function, file, line, entry, pc)\n \t}\n@@ -61,6 +61,16 @@ func callers(skip int, locbuf []location) int {\n \treturn int(n)\n }\n \n+//go:noescape\n+//extern runtime_callersRaw\n+func c_callersRaw(skip int32, pcs *uintptr, max int32) int32\n+\n+// callersRaw returns a raw (PCs only) stack trace of the current goroutine.\n+func callersRaw(skip int, pcbuf []uintptr) int {\n+\tn := c_callersRaw(int32(skip)+1, &pcbuf[0], int32(len(pcbuf)))\n+\treturn int(n)\n+}\n+\n // traceback prints a traceback of the current goroutine.\n // This differs from the gc version, which is given pc, sp, lr and g and\n // can print a traceback of any goroutine.\n@@ -83,7 +93,7 @@ func traceback(skip int32) {\n func printAncestorTraceback(ancestor ancestorInfo) {\n \tprint(\"[originating from goroutine \", ancestor.goid, \"]:\\n\")\n \tfor fidx, pc := range ancestor.pcs {\n-\t\tfunction, file, line := funcfileline(pc, -1)\n+\t\tfunction, file, line, _ := funcfileline(pc, -1)\n \t\tif showfuncinfo(function, fidx == 0) {\n \t\t\tprintAncestorTracebackFuncInfo(function, file, line, pc)\n \t\t}\n@@ -92,7 +102,7 @@ func printAncestorTraceback(ancestor ancestorInfo) {\n \t\tprint(\"...additional frames elided...\\n\")\n \t}\n \t// Show what created goroutine, except main goroutine (goid 1).\n-\tfunction, file, line := funcfileline(ancestor.gopc, -1)\n+\tfunction, file, line, _ := funcfileline(ancestor.gopc, -1)\n \tif function != \"\" && showfuncinfo(function, false) && ancestor.goid != 1 {\n \t\tprintcreatedby1(function, file, line, funcentry(ancestor.gopc), ancestor.gopc)\n \t}"}, {"sha": "5e31f912e0ae761ae3274f3a7e453f3284d14fff", "filename": "libgo/runtime/go-caller.c", "status": "modified", "additions": 12, "deletions": 6, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fruntime%2Fgo-caller.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fruntime%2Fgo-caller.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-caller.c?ref=1ac09ef2c611d3113665ec8c74e38b125217edb3", "patch": "@@ -26,18 +26,22 @@ struct caller\n   String file;\n   intgo line;\n   intgo index;\n+  intgo frames;\n };\n \n /* Collect file/line information for a PC value.  If this is called\n-   more than once, due to inlined functions, we use the last call, as\n-   that is usually the most useful one.  */\n+   more than once, due to inlined functions, we record the number of\n+   inlined frames but return file/func/line for the last call, as\n+   that is usually the most useful one.   */\n \n static int\n callback (void *data, uintptr_t pc __attribute__ ((unused)),\n \t  const char *filename, int lineno, const char *function)\n {\n   struct caller *c = (struct caller *) data;\n \n+  c->frames++;\n+\n   /* The libbacktrace library says that these strings might disappear,\n      but with the current implementation they won't.  We can't easily\n      allocate memory here, so for now assume that we can save a\n@@ -125,25 +129,27 @@ __go_get_backtrace_state ()\n   return back_state;\n }\n \n-/* Return function/file/line information for PC.  The index parameter\n+/* Return function/file/line/nframes information for PC.  The index parameter\n    is the entry on the stack of inlined functions; -1 means the last\n-   one.  */\n+   one, with *nframes set to the count of inlined frames for this PC.  */\n \n static _Bool\n-__go_file_line (uintptr pc, int index, String *fn, String *file, intgo *line)\n+__go_file_line (uintptr pc, int index, String *fn, String *file, intgo *line, intgo *nframes)\n {\n   struct caller c;\n   struct backtrace_state *state;\n \n   runtime_memclr (&c, sizeof c);\n   c.index = index;\n+  c.frames = 0;\n   runtime_xadd (&__go_runtime_in_callers, 1);\n   state = __go_get_backtrace_state ();\n   runtime_xadd (&__go_runtime_in_callers, -1);\n   backtrace_pcinfo (state, pc, callback, error_callback, &c);\n   *fn = c.fn;\n   *file = c.file;\n   *line = c.line;\n+  *nframes = c.frames;\n \n   // If backtrace_pcinfo didn't get the function name from the debug\n   // info, try to get it from the symbol table.\n@@ -222,7 +228,7 @@ runtime_funcfileline (uintptr targetpc, int32 index)\n   struct funcfileline_return ret;\n \n   if (!__go_file_line (targetpc, index, &ret.retfn, &ret.retfile,\n-\t\t       &ret.retline))\n+\t\t       &ret.retline, &ret.retframes))\n     runtime_memclr (&ret, sizeof ret);\n   return ret;\n }"}, {"sha": "4a9c1a7b24d2091d7af58ccaca2c847229256983", "filename": "libgo/runtime/go-callers.c", "status": "modified", "additions": 62, "deletions": 1, "changes": 63, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fruntime%2Fgo-callers.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fruntime%2Fgo-callers.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-callers.c?ref=1ac09ef2c611d3113665ec8c74e38b125217edb3", "patch": "@@ -63,7 +63,9 @@ callback (void *data, uintptr_t pc, const char *filename, int lineno,\n \n   /* Skip thunks and recover functions.  There is no equivalent to\n      these functions in the gc toolchain, so returning them here means\n-     significantly different results for runtime.Caller(N).  */\n+     significantly different results for runtime.Caller(N). See also\n+     similar code in runtime/mprof.go that strips out such functions\n+     for block/mutex/memory profiles.  */\n   if (function != NULL && !arg->keep_thunks)\n     {\n       const char *p;\n@@ -262,3 +264,62 @@ Callers (intgo skip, struct __go_open_array pc)\n \n   return ret;\n }\n+\n+struct callersRaw_data\n+{\n+  uintptr* pcbuf;\n+  int skip;\n+  int index;\n+  int max;\n+};\n+\n+// Callback function for backtrace_simple.  Just collect pc's.\n+// Return zero to continue, non-zero to stop.\n+\n+static int callback_raw (void *data, uintptr_t pc)\n+{\n+  struct callersRaw_data *arg = (struct callersRaw_data *) data;\n+\n+  if (arg->skip > 0)\n+    {\n+      --arg->skip;\n+      return 0;\n+    }\n+\n+  /* On the call to backtrace_simple the pc value was most likely\n+     decremented if there was a normal call, since the pc referred to\n+     the instruction where the call returned and not the call itself.\n+     This was done so that the line number referred to the call\n+     instruction.  To make sure the actual pc from the call stack is\n+     used, it is incremented here.\n+\n+     In the case of a signal, the pc was not decremented by\n+     backtrace_full but still incremented here.  That doesn't really\n+     hurt anything since the line number is right and the pc refers to\n+     the same instruction.  */\n+\n+  arg->pcbuf[arg->index] = pc + 1;\n+  arg->index++;\n+  return arg->index >= arg->max;\n+}\n+\n+/* runtime_callersRaw is similar to runtime_callers() above, but\n+   it returns raw PC values as opposed to file/func/line locations. */\n+int32\n+runtime_callersRaw (int32 skip, uintptr *pcbuf, int32 m)\n+{\n+  struct callersRaw_data data;\n+  struct backtrace_state* state;\n+\n+  data.pcbuf = pcbuf;\n+  data.skip = skip + 1;\n+  data.index = 0;\n+  data.max = m;\n+  runtime_xadd (&__go_runtime_in_callers, 1);\n+  state = __go_get_backtrace_state ();\n+  backtrace_simple (state, 0, callback_raw, error_callback, &data);\n+  runtime_xadd (&__go_runtime_in_callers, -1);\n+\n+  return data.index;\n+}\n+"}, {"sha": "71c1a3ee303bfabb0aaabe1eca6454740f37c795", "filename": "libgo/runtime/runtime.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fruntime%2Fruntime.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1ac09ef2c611d3113665ec8c74e38b125217edb3/libgo%2Fruntime%2Fruntime.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime.h?ref=1ac09ef2c611d3113665ec8c74e38b125217edb3", "patch": "@@ -485,6 +485,7 @@ struct funcfileline_return\n   String retfn;\n   String retfile;\n   intgo retline;\n+  intgo retframes;\n };\n \n struct funcfileline_return"}]}