{"sha": "a62b3dc5f96bff4489de781e816e8ff3b257a562", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YTYyYjNkYzVmOTZiZmY0NDg5ZGU3ODFlODE2ZThmZjNiMjU3YTU2Mg==", "commit": {"author": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2017-10-30T11:04:49Z"}, "committer": {"name": "Jakub Jelinek", "email": "jakub@gcc.gnu.org", "date": "2017-10-30T11:04:49Z"}, "message": "re PR middle-end/22141 (Missing optimization when storing structures)\n\n\tPR middle-end/22141\n\t* gimple-ssa-store-merging.c: Include rtl.h and expr.h.\n\t(struct store_immediate_info): Add bitregion_start and bitregion_end\n\tfields.\n\t(store_immediate_info::store_immediate_info): Add brs and bre\n\targuments and initialize bitregion_{start,end} from those.\n\t(struct merged_store_group): Add bitregion_start, bitregion_end,\n\talign_base and mask fields.  Drop unnecessary struct keyword from\n\tstruct store_immediate_info.  Add do_merge method.\n\t(clear_bit_region_be): Use memset instead of loop storing zeros.\n\t(merged_store_group::do_merge): New method.\n\t(merged_store_group::merge_into): Use do_merge.  Allow gaps in between\n\tstores as long as the surrounding bitregions have no gaps.\n\t(merged_store_group::merge_overlapping): Use do_merge.\n\t(merged_store_group::apply_stores): Test that bitregion_{start,end}\n\tis byte aligned, rather than requiring that start and width are\n\tbyte aligned.  Drop unnecessary struct keyword from\n\tstruct store_immediate_info.  Allocate and populate also mask array.\n\tMake start of the arrays relative to bitregion_start rather than\n\tstart and size them according to bitregion_{end,start} difference.\n\t(struct imm_store_chain_info): Drop unnecessary struct keyword from\n\tstruct store_immediate_info.\n\t(pass_store_merging::gate): Punt if BITS_PER_UNIT or CHAR_BIT is not 8.\n\t(pass_store_merging::terminate_all_aliasing_chains): Drop unnecessary\n\tstruct keyword from struct store_immediate_info.\n\t(imm_store_chain_info::coalesce_immediate_stores): Allow gaps in\n\tbetween stores as long as the surrounding bitregions have no gaps.\n\tFormatting fixes.\n\t(struct split_store): Add orig non-static data member.\n\t(split_store::split_store): Initialize orig to false.\n\t(find_constituent_stmts): Return store_immediate_info *, non-NULL\n\tif there is exactly a single original stmt.  Change stmts argument\n\tto pointer from reference, if NULL, don't push anything to it.  Add\n\tfirst argument, use it to optimize skipping over orig stmts that\n\tare known to be before bitpos already.  Simplify.\n\t(split_group): Return unsigned int count how many stores are or\n\twould be needed rather than a bool.  Add allow_unaligned argument.\n\tChange split_stores argument from reference to pointer, if NULL,\n\tonly do a dry run computing how many stores would be produced.\n\tRewritten algorithm to use both alignment and misalign if\n\t!allow_unaligned and handle bitfield stores with gaps.\n\t(imm_store_chain_info::output_merged_store): Set start_byte_pos\n\tfrom bitregion_start instead of start.  Compute allow_unaligned\n\there, if true, do 2 split_group dry runs to compute which one\n\tproduces fewer stores and prefer aligned if equal.  Punt if\n\tnew count is bigger or equal than original before emitting any\n\tstatements, rather than during that.  Remove no longer needed\n\tnew_ssa_names tracking.  Replace num_stmts with\n\tsplit_stores.length ().  Use 32-bit stack allocated entries\n\tin split_stores auto_vec.  Try to reuse original store lhs/rhs1\n\tif possible.  Handle bitfields with gaps.\n\t(pass_store_merging::execute): Ignore bitsize == 0 stores.\n\tCompute bitregion_{start,end} for the stores and construct\n\tstore_immediate_info with that.  Formatting fixes.\n\n\t* gcc.dg/store_merging_10.c: New test.\n\t* gcc.dg/store_merging_11.c: New test.\n\t* gcc.dg/store_merging_12.c: New test.\n\t* g++.dg/pr71694.C: Add -fno-store-merging to dg-options.\n\nFrom-SVN: r254213", "tree": {"sha": "fcd45efb68a0ab5eaab014b7046168d069ee04f8", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/fcd45efb68a0ab5eaab014b7046168d069ee04f8"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/a62b3dc5f96bff4489de781e816e8ff3b257a562", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a62b3dc5f96bff4489de781e816e8ff3b257a562", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a62b3dc5f96bff4489de781e816e8ff3b257a562", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a62b3dc5f96bff4489de781e816e8ff3b257a562/comments", "author": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "5603c1d9db454cd1f2182f5de3a32c9d1c32497e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5603c1d9db454cd1f2182f5de3a32c9d1c32497e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5603c1d9db454cd1f2182f5de3a32c9d1c32497e"}], "stats": {"total": 729, "additions": 568, "deletions": 161}, "files": [{"sha": "680033348f11ca88d85ac35b061a0bce9e6aa07b", "filename": "gcc/ChangeLog", "status": "modified", "additions": 57, "deletions": 0, "changes": 57, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a62b3dc5f96bff4489de781e816e8ff3b257a562/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a62b3dc5f96bff4489de781e816e8ff3b257a562/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=a62b3dc5f96bff4489de781e816e8ff3b257a562", "patch": "@@ -1,3 +1,60 @@\n+2017-10-30  Jakub Jelinek  <jakub@redhat.com>\n+\n+\tPR middle-end/22141\n+\t* gimple-ssa-store-merging.c: Include rtl.h and expr.h.\n+\t(struct store_immediate_info): Add bitregion_start and bitregion_end\n+\tfields.\n+\t(store_immediate_info::store_immediate_info): Add brs and bre\n+\targuments and initialize bitregion_{start,end} from those.\n+\t(struct merged_store_group): Add bitregion_start, bitregion_end,\n+\talign_base and mask fields.  Drop unnecessary struct keyword from\n+\tstruct store_immediate_info.  Add do_merge method.\n+\t(clear_bit_region_be): Use memset instead of loop storing zeros.\n+\t(merged_store_group::do_merge): New method.\n+\t(merged_store_group::merge_into): Use do_merge.  Allow gaps in between\n+\tstores as long as the surrounding bitregions have no gaps.\n+\t(merged_store_group::merge_overlapping): Use do_merge.\n+\t(merged_store_group::apply_stores): Test that bitregion_{start,end}\n+\tis byte aligned, rather than requiring that start and width are\n+\tbyte aligned.  Drop unnecessary struct keyword from\n+\tstruct store_immediate_info.  Allocate and populate also mask array.\n+\tMake start of the arrays relative to bitregion_start rather than\n+\tstart and size them according to bitregion_{end,start} difference.\n+\t(struct imm_store_chain_info): Drop unnecessary struct keyword from\n+\tstruct store_immediate_info.\n+\t(pass_store_merging::gate): Punt if BITS_PER_UNIT or CHAR_BIT is not 8.\n+\t(pass_store_merging::terminate_all_aliasing_chains): Drop unnecessary\n+\tstruct keyword from struct store_immediate_info.\n+\t(imm_store_chain_info::coalesce_immediate_stores): Allow gaps in\n+\tbetween stores as long as the surrounding bitregions have no gaps.\n+\tFormatting fixes.\n+\t(struct split_store): Add orig non-static data member.\n+\t(split_store::split_store): Initialize orig to false.\n+\t(find_constituent_stmts): Return store_immediate_info *, non-NULL\n+\tif there is exactly a single original stmt.  Change stmts argument\n+\tto pointer from reference, if NULL, don't push anything to it.  Add\n+\tfirst argument, use it to optimize skipping over orig stmts that\n+\tare known to be before bitpos already.  Simplify.\n+\t(split_group): Return unsigned int count how many stores are or\n+\twould be needed rather than a bool.  Add allow_unaligned argument.\n+\tChange split_stores argument from reference to pointer, if NULL,\n+\tonly do a dry run computing how many stores would be produced.\n+\tRewritten algorithm to use both alignment and misalign if\n+\t!allow_unaligned and handle bitfield stores with gaps.\n+\t(imm_store_chain_info::output_merged_store): Set start_byte_pos\n+\tfrom bitregion_start instead of start.  Compute allow_unaligned\n+\there, if true, do 2 split_group dry runs to compute which one\n+\tproduces fewer stores and prefer aligned if equal.  Punt if\n+\tnew count is bigger or equal than original before emitting any\n+\tstatements, rather than during that.  Remove no longer needed\n+\tnew_ssa_names tracking.  Replace num_stmts with\n+\tsplit_stores.length ().  Use 32-bit stack allocated entries\n+\tin split_stores auto_vec.  Try to reuse original store lhs/rhs1\n+\tif possible.  Handle bitfields with gaps.\n+\t(pass_store_merging::execute): Ignore bitsize == 0 stores.\n+\tCompute bitregion_{start,end} for the stores and construct\n+\tstore_immediate_info with that.  Formatting fixes.\n+\n 2017-10-30  Uros Bizjak  <ubizjak@gmail.com>\n \n \tPR target/82725"}, {"sha": "fb68a815c87c92563b3ad07bc6b3de7545652f14", "filename": "gcc/gimple-ssa-store-merging.c", "status": "modified", "additions": 388, "deletions": 160, "changes": 548, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a62b3dc5f96bff4489de781e816e8ff3b257a562/gcc%2Fgimple-ssa-store-merging.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a62b3dc5f96bff4489de781e816e8ff3b257a562/gcc%2Fgimple-ssa-store-merging.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgimple-ssa-store-merging.c?ref=a62b3dc5f96bff4489de781e816e8ff3b257a562", "patch": "@@ -126,6 +126,8 @@\n #include \"tree-eh.h\"\n #include \"target.h\"\n #include \"gimplify-me.h\"\n+#include \"rtl.h\"\n+#include \"expr.h\"\t/* For get_bit_range.  */\n #include \"selftest.h\"\n \n /* The maximum size (in bits) of the stores this pass should generate.  */\n@@ -142,17 +144,24 @@ struct store_immediate_info\n {\n   unsigned HOST_WIDE_INT bitsize;\n   unsigned HOST_WIDE_INT bitpos;\n+  unsigned HOST_WIDE_INT bitregion_start;\n+  /* This is one past the last bit of the bit region.  */\n+  unsigned HOST_WIDE_INT bitregion_end;\n   gimple *stmt;\n   unsigned int order;\n   store_immediate_info (unsigned HOST_WIDE_INT, unsigned HOST_WIDE_INT,\n+\t\t\tunsigned HOST_WIDE_INT, unsigned HOST_WIDE_INT,\n \t\t\tgimple *, unsigned int);\n };\n \n store_immediate_info::store_immediate_info (unsigned HOST_WIDE_INT bs,\n \t\t\t\t\t    unsigned HOST_WIDE_INT bp,\n+\t\t\t\t\t    unsigned HOST_WIDE_INT brs,\n+\t\t\t\t\t    unsigned HOST_WIDE_INT bre,\n \t\t\t\t\t    gimple *st,\n \t\t\t\t\t    unsigned int ord)\n-  : bitsize (bs), bitpos (bp), stmt (st), order (ord)\n+  : bitsize (bs), bitpos (bp), bitregion_start (brs), bitregion_end (bre),\n+    stmt (st), order (ord)\n {\n }\n \n@@ -164,26 +173,32 @@ struct merged_store_group\n {\n   unsigned HOST_WIDE_INT start;\n   unsigned HOST_WIDE_INT width;\n-  /* The size of the allocated memory for val.  */\n+  unsigned HOST_WIDE_INT bitregion_start;\n+  unsigned HOST_WIDE_INT bitregion_end;\n+  /* The size of the allocated memory for val and mask.  */\n   unsigned HOST_WIDE_INT buf_size;\n+  unsigned HOST_WIDE_INT align_base;\n \n   unsigned int align;\n   unsigned int first_order;\n   unsigned int last_order;\n \n-  auto_vec<struct store_immediate_info *> stores;\n+  auto_vec<store_immediate_info *> stores;\n   /* We record the first and last original statements in the sequence because\n      we'll need their vuse/vdef and replacement position.  It's easier to keep\n      track of them separately as 'stores' is reordered by apply_stores.  */\n   gimple *last_stmt;\n   gimple *first_stmt;\n   unsigned char *val;\n+  unsigned char *mask;\n \n   merged_store_group (store_immediate_info *);\n   ~merged_store_group ();\n   void merge_into (store_immediate_info *);\n   void merge_overlapping (store_immediate_info *);\n   bool apply_stores ();\n+private:\n+  void do_merge (store_immediate_info *);\n };\n \n /* Debug helper.  Dump LEN elements of byte array PTR to FD in hex.  */\n@@ -287,8 +302,7 @@ clear_bit_region_be (unsigned char *ptr, unsigned int start,\n \t   && len > BITS_PER_UNIT)\n     {\n       unsigned int nbytes = len / BITS_PER_UNIT;\n-      for (unsigned int i = 0; i < nbytes; i++)\n-\tptr[i] = 0U;\n+      memset (ptr, 0, nbytes);\n       if (len % BITS_PER_UNIT != 0)\n \tclear_bit_region_be (ptr + nbytes, BITS_PER_UNIT - 1,\n \t\t\t     len % BITS_PER_UNIT);\n@@ -549,10 +563,16 @@ merged_store_group::merged_store_group (store_immediate_info *info)\n {\n   start = info->bitpos;\n   width = info->bitsize;\n+  bitregion_start = info->bitregion_start;\n+  bitregion_end = info->bitregion_end;\n   /* VAL has memory allocated for it in apply_stores once the group\n      width has been finalized.  */\n   val = NULL;\n-  align = get_object_alignment (gimple_assign_lhs (info->stmt));\n+  mask = NULL;\n+  unsigned HOST_WIDE_INT align_bitpos = 0;\n+  get_object_alignment_1 (gimple_assign_lhs (info->stmt),\n+\t\t\t  &align, &align_bitpos);\n+  align_base = start - align_bitpos;\n   stores.create (1);\n   stores.safe_push (info);\n   last_stmt = info->stmt;\n@@ -568,18 +588,24 @@ merged_store_group::~merged_store_group ()\n     XDELETEVEC (val);\n }\n \n-/* Merge a store recorded by INFO into this merged store.\n-   The store is not overlapping with the existing recorded\n-   stores.  */\n-\n+/* Helper method for merge_into and merge_overlapping to do\n+   the common part.  */\n void\n-merged_store_group::merge_into (store_immediate_info *info)\n+merged_store_group::do_merge (store_immediate_info *info)\n {\n-  unsigned HOST_WIDE_INT wid = info->bitsize;\n-  /* Make sure we're inserting in the position we think we're inserting.  */\n-  gcc_assert (info->bitpos == start + width);\n+  bitregion_start = MIN (bitregion_start, info->bitregion_start);\n+  bitregion_end = MAX (bitregion_end, info->bitregion_end);\n+\n+  unsigned int this_align;\n+  unsigned HOST_WIDE_INT align_bitpos = 0;\n+  get_object_alignment_1 (gimple_assign_lhs (info->stmt),\n+\t\t\t  &this_align, &align_bitpos);\n+  if (this_align > align)\n+    {\n+      align = this_align;\n+      align_base = info->bitpos - align_bitpos;\n+    }\n \n-  width += wid;\n   gimple *stmt = info->stmt;\n   stores.safe_push (info);\n   if (info->order > last_order)\n@@ -594,30 +620,34 @@ merged_store_group::merge_into (store_immediate_info *info)\n     }\n }\n \n+/* Merge a store recorded by INFO into this merged store.\n+   The store is not overlapping with the existing recorded\n+   stores.  */\n+\n+void\n+merged_store_group::merge_into (store_immediate_info *info)\n+{\n+  unsigned HOST_WIDE_INT wid = info->bitsize;\n+  /* Make sure we're inserting in the position we think we're inserting.  */\n+  gcc_assert (info->bitpos >= start + width\n+\t      && info->bitregion_start <= bitregion_end);\n+\n+  width += wid;\n+  do_merge (info);\n+}\n+\n /* Merge a store described by INFO into this merged store.\n    INFO overlaps in some way with the current store (i.e. it's not contiguous\n    which is handled by merged_store_group::merge_into).  */\n \n void\n merged_store_group::merge_overlapping (store_immediate_info *info)\n {\n-  gimple *stmt = info->stmt;\n-  stores.safe_push (info);\n-\n   /* If the store extends the size of the group, extend the width.  */\n-  if ((info->bitpos + info->bitsize) > (start + width))\n+  if (info->bitpos + info->bitsize > start + width)\n     width += info->bitpos + info->bitsize - (start + width);\n \n-  if (info->order > last_order)\n-    {\n-      last_order = info->order;\n-      last_stmt = stmt;\n-    }\n-  else if (info->order < first_order)\n-    {\n-      first_order = info->order;\n-      first_stmt = stmt;\n-    }\n+  do_merge (info);\n }\n \n /* Go through all the recorded stores in this group in program order and\n@@ -627,27 +657,28 @@ merged_store_group::merge_overlapping (store_immediate_info *info)\n bool\n merged_store_group::apply_stores ()\n {\n-  /* The total width of the stores must add up to a whole number of bytes\n-     and start at a byte boundary.  We don't support emitting bitfield\n-     references for now.  Also, make sure we have more than one store\n-     in the group, otherwise we cannot merge anything.  */\n-  if (width % BITS_PER_UNIT != 0\n-      || start % BITS_PER_UNIT != 0\n+  /* Make sure we have more than one store in the group, otherwise we cannot\n+     merge anything.  */\n+  if (bitregion_start % BITS_PER_UNIT != 0\n+      || bitregion_end % BITS_PER_UNIT != 0\n       || stores.length () == 1)\n     return false;\n \n   stores.qsort (sort_by_order);\n-  struct store_immediate_info *info;\n+  store_immediate_info *info;\n   unsigned int i;\n   /* Create a buffer of a size that is 2 times the number of bytes we're\n      storing.  That way native_encode_expr can write power-of-2-sized\n      chunks without overrunning.  */\n-  buf_size = 2 * (ROUND_UP (width, BITS_PER_UNIT) / BITS_PER_UNIT);\n-  val = XCNEWVEC (unsigned char, buf_size);\n+  buf_size = 2 * ((bitregion_end - bitregion_start) / BITS_PER_UNIT);\n+  val = XNEWVEC (unsigned char, 2 * buf_size);\n+  mask = val + buf_size;\n+  memset (val, 0, buf_size);\n+  memset (mask, ~0U, buf_size);\n \n   FOR_EACH_VEC_ELT (stores, i, info)\n     {\n-      unsigned int pos_in_buffer = info->bitpos - start;\n+      unsigned int pos_in_buffer = info->bitpos - bitregion_start;\n       bool ret = encode_tree_to_bitpos (gimple_assign_rhs1 (info->stmt),\n \t\t\t\t\tval, info->bitsize,\n \t\t\t\t\tpos_in_buffer, buf_size);\n@@ -668,6 +699,11 @@ merged_store_group::apply_stores ()\n         }\n       if (!ret)\n \treturn false;\n+      unsigned char *m = mask + (pos_in_buffer / BITS_PER_UNIT);\n+      if (BYTES_BIG_ENDIAN)\n+\tclear_bit_region_be (m, pos_in_buffer % BITS_PER_UNIT, info->bitsize);\n+      else\n+\tclear_bit_region (m, pos_in_buffer % BITS_PER_UNIT, info->bitsize);\n     }\n   return true;\n }\n@@ -682,7 +718,7 @@ struct imm_store_chain_info\n      See pass_store_merging::m_stores_head for more rationale.  */\n   imm_store_chain_info *next, **pnxp;\n   tree base_addr;\n-  auto_vec<struct store_immediate_info *> m_store_info;\n+  auto_vec<store_immediate_info *> m_store_info;\n   auto_vec<merged_store_group *> m_merged_store_groups;\n \n   imm_store_chain_info (imm_store_chain_info *&inspt, tree b_a)\n@@ -730,11 +766,16 @@ class pass_store_merging : public gimple_opt_pass\n   {\n   }\n \n-  /* Pass not supported for PDP-endianness.  */\n+  /* Pass not supported for PDP-endianness, nor for insane hosts\n+     or target character sizes where native_{encode,interpret}_expr\n+     doesn't work properly.  */\n   virtual bool\n   gate (function *)\n   {\n-    return flag_store_merging && (WORDS_BIG_ENDIAN == BYTES_BIG_ENDIAN);\n+    return flag_store_merging\n+\t   && WORDS_BIG_ENDIAN == BYTES_BIG_ENDIAN\n+\t   && CHAR_BIT == 8\n+\t   && BITS_PER_UNIT == 8;\n   }\n \n   virtual unsigned int execute (function *);\n@@ -811,7 +852,7 @@ pass_store_merging::terminate_all_aliasing_chains (imm_store_chain_info\n \t aliases with any of them.  */\n       else\n \t{\n-\t  struct store_immediate_info *info;\n+\t  store_immediate_info *info;\n \t  unsigned int i;\n \t  FOR_EACH_VEC_ELT ((*chain_info)->m_store_info, i, info)\n \t    {\n@@ -926,8 +967,9 @@ imm_store_chain_info::coalesce_immediate_stores ()\n \t}\n \n       /* |---store 1---| <gap> |---store 2---|.\n-\t Gap between stores.  Start a new group.  */\n-      if (start != merged_store->start + merged_store->width)\n+\t Gap between stores.  Start a new group if there are any gaps\n+\t between bitregions.  */\n+      if (info->bitregion_start > merged_store->bitregion_end)\n \t{\n \t  /* Try to apply all the stores recorded for the group to determine\n \t     the bitpattern they write and discard it if that fails.\n@@ -948,11 +990,11 @@ imm_store_chain_info::coalesce_immediate_stores ()\n        merged_store->merge_into (info);\n     }\n \n-    /* Record or discard the last store group.  */\n-    if (!merged_store->apply_stores ())\n-      delete merged_store;\n-    else\n-      m_merged_store_groups.safe_push (merged_store);\n+  /* Record or discard the last store group.  */\n+  if (!merged_store->apply_stores ())\n+    delete merged_store;\n+  else\n+    m_merged_store_groups.safe_push (merged_store);\n \n   gcc_assert (m_merged_store_groups.length () <= m_store_info.length ());\n   bool success\n@@ -961,8 +1003,8 @@ imm_store_chain_info::coalesce_immediate_stores ()\n \n   if (success && dump_file)\n     fprintf (dump_file, \"Coalescing successful!\\n\"\n-\t\t\t \"Merged into %u stores\\n\",\n-\t\tm_merged_store_groups.length ());\n+\t\t\t\"Merged into %u stores\\n\",\n+\t     m_merged_store_groups.length ());\n \n   return success;\n }\n@@ -1016,6 +1058,8 @@ struct split_store\n   unsigned HOST_WIDE_INT size;\n   unsigned HOST_WIDE_INT align;\n   auto_vec<gimple *> orig_stmts;\n+  /* True if there is a single orig stmt covering the whole split store.  */\n+  bool orig;\n   split_store (unsigned HOST_WIDE_INT, unsigned HOST_WIDE_INT,\n \t       unsigned HOST_WIDE_INT);\n };\n@@ -1025,100 +1069,198 @@ struct split_store\n split_store::split_store (unsigned HOST_WIDE_INT bp,\n \t\t\t  unsigned HOST_WIDE_INT sz,\n \t\t\t  unsigned HOST_WIDE_INT al)\n-\t\t\t  : bytepos (bp), size (sz), align (al)\n+\t\t\t  : bytepos (bp), size (sz), align (al), orig (false)\n {\n   orig_stmts.create (0);\n }\n \n /* Record all statements corresponding to stores in GROUP that write to\n    the region starting at BITPOS and is of size BITSIZE.  Record such\n-   statements in STMTS.  The stores in GROUP must be sorted by\n-   bitposition.  */\n+   statements in STMTS if non-NULL.  The stores in GROUP must be sorted by\n+   bitposition.  Return INFO if there is exactly one original store\n+   in the range.  */\n \n-static void\n+static store_immediate_info *\n find_constituent_stmts (struct merged_store_group *group,\n-\t\t\t auto_vec<gimple *> &stmts,\n-\t\t\t unsigned HOST_WIDE_INT bitpos,\n-\t\t\t unsigned HOST_WIDE_INT bitsize)\n+\t\t\tvec<gimple *> *stmts,\n+\t\t\tunsigned int *first,\n+\t\t\tunsigned HOST_WIDE_INT bitpos,\n+\t\t\tunsigned HOST_WIDE_INT bitsize)\n {\n-  struct store_immediate_info *info;\n+  store_immediate_info *info, *ret = NULL;\n   unsigned int i;\n+  bool second = false;\n+  bool update_first = true;\n   unsigned HOST_WIDE_INT end = bitpos + bitsize;\n-  FOR_EACH_VEC_ELT (group->stores, i, info)\n+  for (i = *first; group->stores.iterate (i, &info); ++i)\n     {\n       unsigned HOST_WIDE_INT stmt_start = info->bitpos;\n       unsigned HOST_WIDE_INT stmt_end = stmt_start + info->bitsize;\n-      if (stmt_end < bitpos)\n-\tcontinue;\n+      if (stmt_end <= bitpos)\n+\t{\n+\t  /* BITPOS passed to this function never decreases from within the\n+\t     same split_group call, so optimize and don't scan info records\n+\t     which are known to end before or at BITPOS next time.\n+\t     Only do it if all stores before this one also pass this.  */\n+\t  if (update_first)\n+\t    *first = i + 1;\n+\t  continue;\n+\t}\n+      else\n+\tupdate_first = false;\n+\n       /* The stores in GROUP are ordered by bitposition so if we're past\n-\t  the region for this group return early.  */\n-      if (stmt_start > end)\n-\treturn;\n-\n-      if (IN_RANGE (stmt_start, bitpos, bitpos + bitsize)\n-\t  || IN_RANGE (stmt_end, bitpos, end)\n-\t  /* The statement writes a region that completely encloses the region\n-\t     that this group writes.  Unlikely to occur but let's\n-\t     handle it.  */\n-\t  || IN_RANGE (bitpos, stmt_start, stmt_end))\n-\tstmts.safe_push (info->stmt);\n+\t the region for this group return early.  */\n+      if (stmt_start >= end)\n+\treturn ret;\n+\n+      if (stmts)\n+\t{\n+\t  stmts->safe_push (info->stmt);\n+\t  if (ret)\n+\t    {\n+\t      ret = NULL;\n+\t      second = true;\n+\t    }\n+\t}\n+      else if (ret)\n+\treturn NULL;\n+      if (!second)\n+\tret = info;\n     }\n+  return ret;\n }\n \n /* Split a merged store described by GROUP by populating the SPLIT_STORES\n-   vector with split_store structs describing the byte offset (from the base),\n-   the bit size and alignment of each store as well as the original statements\n-   involved in each such split group.\n+   vector (if non-NULL) with split_store structs describing the byte offset\n+   (from the base), the bit size and alignment of each store as well as the\n+   original statements involved in each such split group.\n    This is to separate the splitting strategy from the statement\n    building/emission/linking done in output_merged_store.\n-   At the moment just start with the widest possible size and keep emitting\n-   the widest we can until we have emitted all the bytes, halving the size\n-   when appropriate.  */\n+   Return number of new stores.\n+   If SPLIT_STORES is NULL, it is just a dry run to count number of\n+   new stores.  */\n \n-static bool\n-split_group (merged_store_group *group,\n-\t     auto_vec<struct split_store *> &split_stores)\n+static unsigned int\n+split_group (merged_store_group *group, bool allow_unaligned,\n+\t     vec<struct split_store *> *split_stores)\n {\n-  unsigned HOST_WIDE_INT pos = group->start;\n-  unsigned HOST_WIDE_INT size = group->width;\n+  unsigned HOST_WIDE_INT pos = group->bitregion_start;\n+  unsigned HOST_WIDE_INT size = group->bitregion_end - pos;\n   unsigned HOST_WIDE_INT bytepos = pos / BITS_PER_UNIT;\n-  unsigned HOST_WIDE_INT align = group->align;\n+  unsigned HOST_WIDE_INT group_align = group->align;\n+  unsigned HOST_WIDE_INT align_base = group->align_base;\n \n-  /* We don't handle partial bitfields for now.  We shouldn't have\n-     reached this far.  */\n   gcc_assert ((size % BITS_PER_UNIT == 0) && (pos % BITS_PER_UNIT == 0));\n \n-  bool allow_unaligned\n-    = !STRICT_ALIGNMENT && PARAM_VALUE (PARAM_STORE_MERGING_ALLOW_UNALIGNED);\n-\n-  unsigned int try_size = MAX_STORE_BITSIZE;\n-  while (try_size > size\n-\t || (!allow_unaligned\n-\t     && try_size > align))\n-    {\n-      try_size /= 2;\n-      if (try_size < BITS_PER_UNIT)\n-\treturn false;\n-    }\n-\n+  unsigned int ret = 0, first = 0;\n   unsigned HOST_WIDE_INT try_pos = bytepos;\n   group->stores.qsort (sort_by_bitpos);\n \n   while (size > 0)\n     {\n-      struct split_store *store = new split_store (try_pos, try_size, align);\n+      if ((allow_unaligned || group_align <= BITS_PER_UNIT)\n+\t  && group->mask[try_pos - bytepos] == (unsigned char) ~0U)\n+\t{\n+\t  /* Skip padding bytes.  */\n+\t  ++try_pos;\n+\t  size -= BITS_PER_UNIT;\n+\t  continue;\n+\t}\n+\n       unsigned HOST_WIDE_INT try_bitpos = try_pos * BITS_PER_UNIT;\n-      find_constituent_stmts (group, store->orig_stmts, try_bitpos, try_size);\n-      split_stores.safe_push (store);\n+      unsigned int try_size = MAX_STORE_BITSIZE, nonmasked;\n+      unsigned HOST_WIDE_INT align_bitpos\n+\t= (try_bitpos - align_base) & (group_align - 1);\n+      unsigned HOST_WIDE_INT align = group_align;\n+      if (align_bitpos)\n+\talign = least_bit_hwi (align_bitpos);\n+      if (!allow_unaligned)\n+\ttry_size = MIN (try_size, align);\n+      store_immediate_info *info\n+\t= find_constituent_stmts (group, NULL, &first, try_bitpos, try_size);\n+      if (info)\n+\t{\n+\t  /* If there is just one original statement for the range, see if\n+\t     we can just reuse the original store which could be even larger\n+\t     than try_size.  */\n+\t  unsigned HOST_WIDE_INT stmt_end\n+\t    = ROUND_UP (info->bitpos + info->bitsize, BITS_PER_UNIT);\n+\t  info = find_constituent_stmts (group, NULL, &first, try_bitpos,\n+\t\t\t\t\t stmt_end - try_bitpos);\n+\t  if (info && info->bitpos >= try_bitpos)\n+\t    {\n+\t      try_size = stmt_end - try_bitpos;\n+\t      goto found;\n+\t    }\n+\t}\n \n-      try_pos += try_size / BITS_PER_UNIT;\n+      /* Approximate store bitsize for the case when there are no padding\n+\t bits.  */\n+      while (try_size > size)\n+\ttry_size /= 2;\n+      /* Now look for whole padding bytes at the end of that bitsize.  */\n+      for (nonmasked = try_size / BITS_PER_UNIT; nonmasked > 0; --nonmasked)\n+\tif (group->mask[try_pos - bytepos + nonmasked - 1]\n+\t    != (unsigned char) ~0U)\n+\t  break;\n+      if (nonmasked == 0)\n+\t{\n+\t  /* If entire try_size range is padding, skip it.  */\n+\t  try_pos += try_size / BITS_PER_UNIT;\n+\t  size -= try_size;\n+\t  continue;\n+\t}\n+      /* Otherwise try to decrease try_size if second half, last 3 quarters\n+\t etc. are padding.  */\n+      nonmasked *= BITS_PER_UNIT;\n+      while (nonmasked <= try_size / 2)\n+\ttry_size /= 2;\n+      if (!allow_unaligned && group_align > BITS_PER_UNIT)\n+\t{\n+\t  /* Now look for whole padding bytes at the start of that bitsize.  */\n+\t  unsigned int try_bytesize = try_size / BITS_PER_UNIT, masked;\n+\t  for (masked = 0; masked < try_bytesize; ++masked)\n+\t    if (group->mask[try_pos - bytepos + masked] != (unsigned char) ~0U)\n+\t      break;\n+\t  masked *= BITS_PER_UNIT;\n+\t  gcc_assert (masked < try_size);\n+\t  if (masked >= try_size / 2)\n+\t    {\n+\t      while (masked >= try_size / 2)\n+\t\t{\n+\t\t  try_size /= 2;\n+\t\t  try_pos += try_size / BITS_PER_UNIT;\n+\t\t  size -= try_size;\n+\t\t  masked -= try_size;\n+\t\t}\n+\t      /* Need to recompute the alignment, so just retry at the new\n+\t\t position.  */\n+\t      continue;\n+\t    }\n+\t}\n+\n+    found:\n+      ++ret;\n \n+      if (split_stores)\n+\t{\n+\t  struct split_store *store\n+\t    = new split_store (try_pos, try_size, align);\n+\t  info = find_constituent_stmts (group, &store->orig_stmts,\n+\t  \t\t\t\t &first, try_bitpos, try_size);\n+\t  if (info\n+\t      && info->bitpos >= try_bitpos\n+\t      && info->bitpos + info->bitsize <= try_bitpos + try_size)\n+\t    store->orig = true;\n+\t  split_stores->safe_push (store);\n+\t}\n+\n+      try_pos += try_size / BITS_PER_UNIT;\n       size -= try_size;\n-      align = try_size;\n-      while (size < try_size)\n-\ttry_size /= 2;\n     }\n-  return true;\n+\n+  return ret;\n }\n \n /* Given a merged store group GROUP output the widened version of it.\n@@ -1132,31 +1274,50 @@ split_group (merged_store_group *group,\n bool\n imm_store_chain_info::output_merged_store (merged_store_group *group)\n {\n-  unsigned HOST_WIDE_INT start_byte_pos = group->start / BITS_PER_UNIT;\n+  unsigned HOST_WIDE_INT start_byte_pos\n+    = group->bitregion_start / BITS_PER_UNIT;\n \n   unsigned int orig_num_stmts = group->stores.length ();\n   if (orig_num_stmts < 2)\n     return false;\n \n-  auto_vec<struct split_store *> split_stores;\n+  auto_vec<struct split_store *, 32> split_stores;\n   split_stores.create (0);\n-  if (!split_group (group, split_stores))\n-    return false;\n+  bool allow_unaligned\n+    = !STRICT_ALIGNMENT && PARAM_VALUE (PARAM_STORE_MERGING_ALLOW_UNALIGNED);\n+  if (allow_unaligned)\n+    {\n+      /* If unaligned stores are allowed, see how many stores we'd emit\n+\t for unaligned and how many stores we'd emit for aligned stores.\n+\t Only use unaligned stores if it allows fewer stores than aligned.  */\n+      unsigned aligned_cnt = split_group (group, false, NULL);\n+      unsigned unaligned_cnt = split_group (group, true, NULL);\n+      if (aligned_cnt <= unaligned_cnt)\n+\tallow_unaligned = false;\n+    }\n+  split_group (group, allow_unaligned, &split_stores);\n+\n+  if (split_stores.length () >= orig_num_stmts)\n+    {\n+      /* We didn't manage to reduce the number of statements.  Bail out.  */\n+      if (dump_file && (dump_flags & TDF_DETAILS))\n+\t{\n+\t  fprintf (dump_file, \"Exceeded original number of stmts (%u).\"\n+\t\t\t      \"  Not profitable to emit new sequence.\\n\",\n+\t\t   orig_num_stmts);\n+\t}\n+      return false;\n+    }\n \n   gimple_stmt_iterator last_gsi = gsi_for_stmt (group->last_stmt);\n   gimple_seq seq = NULL;\n-  unsigned int num_stmts = 0;\n   tree last_vdef, new_vuse;\n   last_vdef = gimple_vdef (group->last_stmt);\n   new_vuse = gimple_vuse (group->last_stmt);\n \n   gimple *stmt = NULL;\n-  /* The new SSA names created.  Keep track of them so that we can free them\n-     if we decide to not use the new sequence.  */\n-  auto_vec<tree> new_ssa_names;\n   split_store *split_store;\n   unsigned int i;\n-  bool fail = false;\n \n   tree addr = force_gimple_operand_1 (unshare_expr (base_addr), &seq,\n \t\t\t\t      is_gimple_mem_ref_addr, NULL_TREE);\n@@ -1165,48 +1326,76 @@ imm_store_chain_info::output_merged_store (merged_store_group *group)\n       unsigned HOST_WIDE_INT try_size = split_store->size;\n       unsigned HOST_WIDE_INT try_pos = split_store->bytepos;\n       unsigned HOST_WIDE_INT align = split_store->align;\n-      tree offset_type = get_alias_type_for_stmts (split_store->orig_stmts);\n-      location_t loc = get_location_for_stmts (split_store->orig_stmts);\n-\n-      tree int_type = build_nonstandard_integer_type (try_size, UNSIGNED);\n-      int_type = build_aligned_type (int_type, align);\n-      tree dest = fold_build2 (MEM_REF, int_type, addr,\n-\t\t\t       build_int_cst (offset_type, try_pos));\n-\n-      tree src = native_interpret_expr (int_type,\n-\t\t\t\t\tgroup->val + try_pos - start_byte_pos,\n-\t\t\t\t\tgroup->buf_size);\n+      tree dest, src;\n+      location_t loc;\n+      if (split_store->orig)\n+\t{\n+\t  /* If there is just a single constituent store which covers\n+\t     the whole area, just reuse the lhs and rhs.  */\n+\t  dest = gimple_assign_lhs (split_store->orig_stmts[0]);\n+\t  src = gimple_assign_rhs1 (split_store->orig_stmts[0]);\n+\t  loc = gimple_location (split_store->orig_stmts[0]);\n+\t}\n+      else\n+\t{\n+\t  tree offset_type\n+\t    = get_alias_type_for_stmts (split_store->orig_stmts);\n+\t  loc = get_location_for_stmts (split_store->orig_stmts);\n+\n+\t  tree int_type = build_nonstandard_integer_type (try_size, UNSIGNED);\n+\t  int_type = build_aligned_type (int_type, align);\n+\t  dest = fold_build2 (MEM_REF, int_type, addr,\n+\t\t\t      build_int_cst (offset_type, try_pos));\n+\t  src = native_interpret_expr (int_type,\n+\t\t\t\t       group->val + try_pos - start_byte_pos,\n+\t\t\t\t       group->buf_size);\n+\t  tree mask\n+\t    = native_interpret_expr (int_type,\n+\t\t\t\t     group->mask + try_pos - start_byte_pos,\n+\t\t\t\t     group->buf_size);\n+\t  if (!integer_zerop (mask))\n+\t    {\n+\t      tree tem = make_ssa_name (int_type);\n+\t      tree load_src = unshare_expr (dest);\n+\t      /* The load might load some or all bits uninitialized,\n+\t\t avoid -W*uninitialized warnings in that case.\n+\t\t As optimization, it would be nice if all the bits are\n+\t\t provably uninitialized (no stores at all yet or previous\n+\t\t store a CLOBBER) we'd optimize away the load and replace\n+\t\t it e.g. with 0.  */\n+\t      TREE_NO_WARNING (load_src) = 1;\n+\t      stmt = gimple_build_assign (tem, load_src);\n+\t      gimple_set_location (stmt, loc);\n+\t      gimple_set_vuse (stmt, new_vuse);\n+\t      gimple_seq_add_stmt_without_update (&seq, stmt);\n+\n+\t      /* FIXME: If there is a single chunk of zero bits in mask,\n+\t\t perhaps use BIT_INSERT_EXPR instead?  */\n+\t      stmt = gimple_build_assign (make_ssa_name (int_type),\n+\t\t\t\t\t  BIT_AND_EXPR, tem, mask);\n+\t      gimple_set_location (stmt, loc);\n+\t      gimple_seq_add_stmt_without_update (&seq, stmt);\n+\t      tem = gimple_assign_lhs (stmt);\n+\n+\t      src = wide_int_to_tree (int_type,\n+\t\t\t\t      wi::bit_and_not (wi::to_wide (src),\n+\t\t\t\t\t\t       wi::to_wide (mask)));\n+\t      stmt = gimple_build_assign (make_ssa_name (int_type),\n+\t\t\t\t\t  BIT_IOR_EXPR, tem, src);\n+\t      gimple_set_location (stmt, loc);\n+\t      gimple_seq_add_stmt_without_update (&seq, stmt);\n+\t      src = gimple_assign_lhs (stmt);\n+\t    }\n+\t}\n \n       stmt = gimple_build_assign (dest, src);\n       gimple_set_location (stmt, loc);\n       gimple_set_vuse (stmt, new_vuse);\n       gimple_seq_add_stmt_without_update (&seq, stmt);\n \n-      /* We didn't manage to reduce the number of statements.  Bail out.  */\n-      if (++num_stmts == orig_num_stmts)\n-\t{\n-\t  if (dump_file && (dump_flags & TDF_DETAILS))\n-\t    {\n-\t      fprintf (dump_file, \"Exceeded original number of stmts (%u).\"\n-\t\t\t\t  \"  Not profitable to emit new sequence.\\n\",\n-\t\t       orig_num_stmts);\n-\t    }\n-\t  unsigned int ssa_count;\n-\t  tree ssa_name;\n-\t  /* Don't forget to cleanup the temporary SSA names.  */\n-\t  FOR_EACH_VEC_ELT (new_ssa_names, ssa_count, ssa_name)\n-\t    release_ssa_name (ssa_name);\n-\n-\t  fail = true;\n-\t  break;\n-\t}\n-\n       tree new_vdef;\n       if (i < split_stores.length () - 1)\n-\t{\n-\t  new_vdef = make_ssa_name (gimple_vop (cfun), stmt);\n-\t  new_ssa_names.safe_push (new_vdef);\n-\t}\n+\tnew_vdef = make_ssa_name (gimple_vop (cfun), stmt);\n       else\n \tnew_vdef = last_vdef;\n \n@@ -1218,15 +1407,12 @@ imm_store_chain_info::output_merged_store (merged_store_group *group)\n   FOR_EACH_VEC_ELT (split_stores, i, split_store)\n     delete split_store;\n \n-  if (fail)\n-    return false;\n-\n   gcc_assert (seq);\n   if (dump_file)\n     {\n       fprintf (dump_file,\n \t       \"New sequence of %u stmts to replace old one of %u stmts\\n\",\n-\t       num_stmts, orig_num_stmts);\n+\t       split_stores.length (), orig_num_stmts);\n       if (dump_flags & TDF_DETAILS)\n \tprint_gimple_seq (dump_file, seq, 0, TDF_VOPS | TDF_MEMSYMS);\n     }\n@@ -1387,12 +1573,25 @@ pass_store_merging::execute (function *fun)\n \t      tree rhs = gimple_assign_rhs1 (stmt);\n \n \t      HOST_WIDE_INT bitsize, bitpos;\n+\t      unsigned HOST_WIDE_INT bitregion_start = 0;\n+\t      unsigned HOST_WIDE_INT bitregion_end = 0;\n \t      machine_mode mode;\n \t      int unsignedp = 0, reversep = 0, volatilep = 0;\n \t      tree offset, base_addr;\n \t      base_addr\n \t\t= get_inner_reference (lhs, &bitsize, &bitpos, &offset, &mode,\n \t\t\t\t       &unsignedp, &reversep, &volatilep);\n+\t      if (TREE_CODE (lhs) == COMPONENT_REF\n+\t\t  && DECL_BIT_FIELD_TYPE (TREE_OPERAND (lhs, 1)))\n+\t\t{\n+\t\t  get_bit_range (&bitregion_start, &bitregion_end, lhs,\n+\t\t\t\t &bitpos, &offset);\n+\t\t  if (bitregion_end)\n+\t\t    ++bitregion_end;\n+\t\t}\n+\t      if (bitsize == 0)\n+\t\tcontinue;\n+\n \t      /* As a future enhancement we could handle stores with the same\n \t\t base and offset.  */\n \t      bool invalid = reversep\n@@ -1414,7 +1613,26 @@ pass_store_merging::execute (function *fun)\n \t\t  bit_off = byte_off << LOG2_BITS_PER_UNIT;\n \t\t  bit_off += bitpos;\n \t\t  if (!wi::neg_p (bit_off) && wi::fits_shwi_p (bit_off))\n-\t\t    bitpos = bit_off.to_shwi ();\n+\t\t    {\n+\t\t      bitpos = bit_off.to_shwi ();\n+\t\t      if (bitregion_end)\n+\t\t\t{\n+\t\t\t  bit_off = byte_off << LOG2_BITS_PER_UNIT;\n+\t\t\t  bit_off += bitregion_start;\n+\t\t\t  if (wi::fits_uhwi_p (bit_off))\n+\t\t\t    {\n+\t\t\t      bitregion_start = bit_off.to_uhwi ();\n+\t\t\t      bit_off = byte_off << LOG2_BITS_PER_UNIT;\n+\t\t\t      bit_off += bitregion_end;\n+\t\t\t      if (wi::fits_uhwi_p (bit_off))\n+\t\t\t\tbitregion_end = bit_off.to_uhwi ();\n+\t\t\t      else\n+\t\t\t\tbitregion_end = 0;\n+\t\t\t    }\n+\t\t\t  else\n+\t\t\t    bitregion_end = 0;\n+\t\t\t}\n+\t\t    }\n \t\t  else\n \t\t    invalid = true;\n \t\t  base_addr = TREE_OPERAND (base_addr, 0);\n@@ -1428,6 +1646,12 @@ pass_store_merging::execute (function *fun)\n \t\t  base_addr = build_fold_addr_expr (base_addr);\n \t\t}\n \n+\t      if (!bitregion_end)\n+\t\t{\n+\t\t  bitregion_start = ROUND_DOWN (bitpos, BITS_PER_UNIT);\n+\t\t  bitregion_end = ROUND_UP (bitpos + bitsize, BITS_PER_UNIT);\n+\t\t}\n+\n \t      if (! invalid\n \t\t  && offset != NULL_TREE)\n \t\t{\n@@ -1457,9 +1681,11 @@ pass_store_merging::execute (function *fun)\n \t\t  store_immediate_info *info;\n \t\t  if (chain_info)\n \t\t    {\n-\t\t      info = new store_immediate_info (\n-\t\t\tbitsize, bitpos, stmt,\n-\t\t\t(*chain_info)->m_store_info.length ());\n+\t\t      unsigned int ord = (*chain_info)->m_store_info.length ();\n+\t\t      info = new store_immediate_info (bitsize, bitpos,\n+\t\t\t\t\t\t       bitregion_start,\n+\t\t\t\t\t\t       bitregion_end,\n+\t\t\t\t\t\t       stmt, ord);\n \t\t      if (dump_file && (dump_flags & TDF_DETAILS))\n \t\t\t{\n \t\t\t  fprintf (dump_file,\n@@ -1488,6 +1714,8 @@ pass_store_merging::execute (function *fun)\n \t\t  struct imm_store_chain_info *new_chain\n \t\t    = new imm_store_chain_info (m_stores_head, base_addr);\n \t\t  info = new store_immediate_info (bitsize, bitpos,\n+\t\t\t\t\t\t   bitregion_start,\n+\t\t\t\t\t\t   bitregion_end,\n \t\t\t\t\t\t   stmt, 0);\n \t\t  new_chain->m_store_info.safe_push (info);\n \t\t  m_stores.put (base_addr, new_chain);"}, {"sha": "ef3e9ae27f0c6d53f23994547988de921f8ae0d6", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a62b3dc5f96bff4489de781e816e8ff3b257a562/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a62b3dc5f96bff4489de781e816e8ff3b257a562/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=a62b3dc5f96bff4489de781e816e8ff3b257a562", "patch": "@@ -1,3 +1,11 @@\n+2017-10-30  Jakub Jelinek  <jakub@redhat.com>\n+\n+\tPR middle-end/22141\n+\t* gcc.dg/store_merging_10.c: New test.\n+\t* gcc.dg/store_merging_11.c: New test.\n+\t* gcc.dg/store_merging_12.c: New test.\n+\t* g++.dg/pr71694.C: Add -fno-store-merging to dg-options.\n+\n 2017-10-30  Uros Bizjak  <ubizjak@gmail.com>\n \n \tPR target/82725"}, {"sha": "0a8baf230bfeec51900b9fdd77d140d7c6fb4986", "filename": "gcc/testsuite/g++.dg/pr71694.C", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a62b3dc5f96bff4489de781e816e8ff3b257a562/gcc%2Ftestsuite%2Fg%2B%2B.dg%2Fpr71694.C", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a62b3dc5f96bff4489de781e816e8ff3b257a562/gcc%2Ftestsuite%2Fg%2B%2B.dg%2Fpr71694.C", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fg%2B%2B.dg%2Fpr71694.C?ref=a62b3dc5f96bff4489de781e816e8ff3b257a562", "patch": "@@ -1,5 +1,5 @@\n /* { dg-do compile } */\n-/* { dg-options \"-O2\" } */\n+/* { dg-options \"-O2 -fno-store-merging\" } */\n \n struct B {\n     B() {}"}, {"sha": "440f6e1f6c3f882611cd422c71c6404b5cf0e2bb", "filename": "gcc/testsuite/gcc.dg/store_merging_10.c", "status": "added", "additions": 56, "deletions": 0, "changes": 56, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a62b3dc5f96bff4489de781e816e8ff3b257a562/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_10.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a62b3dc5f96bff4489de781e816e8ff3b257a562/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_10.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_10.c?ref=a62b3dc5f96bff4489de781e816e8ff3b257a562", "patch": "@@ -0,0 +1,56 @@\n+/* { dg-do compile } */\n+/* { dg-require-effective-target store_merge } */\n+/* { dg-options \"-O2 -fdump-tree-store-merging\" } */\n+\n+struct S {\n+  unsigned int b1:1;\n+  unsigned int b2:1;\n+  unsigned int b3:1;\n+  unsigned int b4:1;\n+  unsigned int b5:1;\n+  unsigned int b6:27;\n+};\n+\n+struct T {\n+  unsigned int b1:1;\n+  unsigned int b2:16;\n+  unsigned int b3:14;\n+  unsigned int b4:1;\n+};\n+\n+__attribute__((noipa)) void\n+foo (struct S *x)\n+{\n+  x->b1 = 1;\n+  x->b2 = 0;\n+  x->b3 = 1;\n+  x->b4 = 1;\n+  x->b5 = 0;\n+}\n+\n+__attribute__((noipa)) void\n+bar (struct T *x)\n+{\n+  x->b1 = 1;\n+  x->b2 = 0;\n+  x->b4 = 0;\n+}\n+\n+struct S s = { 0, 1, 0, 0, 1, 0x3a5f05a };\n+struct T t = { 0, 0xf5af, 0x3a5a, 1 };\n+\n+int\n+main ()\n+{\n+  asm volatile (\"\" : : : \"memory\");\n+  foo (&s);\n+  bar (&t);\n+  asm volatile (\"\" : : : \"memory\");\n+  if (s.b1 != 1 || s.b2 != 0 || s.b3 != 1 || s.b4 != 1 || s.b5 != 0 || s.b6 != 0x3a5f05a)\n+    __builtin_abort ();\n+  if (t.b1 != 1 || t.b2 != 0 || t.b3 != 0x3a5a || t.b4 != 0)\n+    __builtin_abort ();\n+  return 0;\n+}\n+\n+/* { dg-final { scan-tree-dump-times \"Merging successful\" 2 \"store-merging\" } } */"}, {"sha": "399538e522e6bf517460765f0b24935c6dce61f4", "filename": "gcc/testsuite/gcc.dg/store_merging_11.c", "status": "added", "additions": 47, "deletions": 0, "changes": 47, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a62b3dc5f96bff4489de781e816e8ff3b257a562/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_11.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a62b3dc5f96bff4489de781e816e8ff3b257a562/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_11.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_11.c?ref=a62b3dc5f96bff4489de781e816e8ff3b257a562", "patch": "@@ -0,0 +1,47 @@\n+/* { dg-do compile } */\n+/* { dg-require-effective-target store_merge } */\n+/* { dg-options \"-O2 -fdump-tree-store-merging\" } */\n+\n+struct S { unsigned char b[2]; unsigned short c; unsigned char d[4]; unsigned long e; };\n+\n+__attribute__((noipa)) void\n+foo (struct S *p)\n+{\n+  p->b[1] = 1;\n+  p->c = 23;\n+  p->d[0] = 4;\n+  p->d[1] = 5;\n+  p->d[2] = 6;\n+  p->d[3] = 7;\n+  p->e = 8;\n+}\n+\n+__attribute__((noipa)) void\n+bar (struct S *p)\n+{\n+  p->b[1] = 9;\n+  p->c = 112;\n+  p->d[0] = 10;\n+  p->d[1] = 11;\n+}\n+\n+struct S s = { { 30, 31 }, 32, { 33, 34, 35, 36 }, 37 };\n+\n+int\n+main ()\n+{\n+  asm volatile (\"\" : : : \"memory\");\n+  foo (&s);\n+  asm volatile (\"\" : : : \"memory\");\n+  if (s.b[0] != 30 || s.b[1] != 1 || s.c != 23 || s.d[0] != 4 || s.d[1] != 5\n+      || s.d[2] != 6 || s.d[3] != 7 || s.e != 8)\n+    __builtin_abort ();\n+  bar (&s);\n+  asm volatile (\"\" : : : \"memory\");\n+  if (s.b[0] != 30 || s.b[1] != 9 || s.c != 112 || s.d[0] != 10 || s.d[1] != 11\n+      || s.d[2] != 6 || s.d[3] != 7 || s.e != 8)\n+    __builtin_abort ();\n+  return 0;\n+}\n+\n+/* { dg-final { scan-tree-dump-times \"Merging successful\" 2 \"store-merging\" } } */"}, {"sha": "67f23449e9310d60793c6e7e402c82fa959001ef", "filename": "gcc/testsuite/gcc.dg/store_merging_12.c", "status": "added", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a62b3dc5f96bff4489de781e816e8ff3b257a562/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_12.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a62b3dc5f96bff4489de781e816e8ff3b257a562/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_12.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_12.c?ref=a62b3dc5f96bff4489de781e816e8ff3b257a562", "patch": "@@ -0,0 +1,11 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-O2 -Wall\" } */\n+\n+struct S { unsigned int b1:1, b2:1, b3:1, b4:1, b5:1, b6:27; };\n+void bar (struct S *);\n+void foo (int x)\n+{\n+  struct S s;\n+  s.b2 = 1; s.b3 = 0; s.b4 = 1; s.b5 = 0; s.b1 = x; s.b6 = x;\t/* { dg-bogus \"is used uninitialized in this function\" } */\n+  bar (&s);\n+}"}]}