{"sha": "f24f4c15884bf1ee65a10e2f959842eec4198876", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZjI0ZjRjMTU4ODRiZjFlZTY1YTEwZTJmOTU5ODQyZWVjNDE5ODg3Ng==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2019-09-19T09:04:28Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2019-09-19T09:04:28Z"}, "message": "Rework constant subreg folds and handle more variable-length cases\n\nThis patch rewrites the way simplify_subreg handles constants.\nIt uses similar native_encode/native_decode routines to the\ntree-level handling of VIEW_CONVERT_EXPR, meaning that we can\nmove between rtx constants and the target memory image of them.\n\nThe main point of this patch is to support subregs of constant-length\nvectors for VLA vectors, beyond the very simple cases that were already\nhandled.  Many of the new tests failed before the patch for variable-\nlength vectors.\n\nThe boolean side is tested more by the upcoming SVE ACLE work.\n\n2019-09-19  Richard Sandiford  <richard.sandiford@arm.com>\n\ngcc/\n\t* defaults.h (TARGET_UNIT): New macro.\n\t(target_unit): New type.\n\t* rtl.h (native_encode_rtx, native_decode_rtx)\n\t(native_decode_vector_rtx, subreg_size_lsb): Declare.\n\t(subreg_lsb_1): Turn into an inline wrapper around subreg_size_lsb.\n\t* rtlanal.c (subreg_lsb_1): Delete.\n\t(subreg_size_lsb): New function.\n\t* simplify-rtx.c: Include rtx-vector-builder.h\n\t(simplify_immed_subreg): Delete.\n\t(native_encode_rtx, native_decode_vector_rtx, native_decode_rtx)\n\t(simplify_const_vector_byte_offset, simplify_const_vector_subreg): New\n\tfunctions.\n\t(simplify_subreg): Use them.\n\t(test_vector_subregs_modes, test_vector_subregs_repeating)\n\t(test_vector_subregs_fore_back, test_vector_subregs_stepped)\n\t(test_vector_subregs): New functions.\n\t(test_vector_ops): Call test_vector_subregs for integer vector\n\tmodes with at least 2 elements.\n\nFrom-SVN: r275959", "tree": {"sha": "36f90ba89bf92546561ae999a4a48cd9c5a4704f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/36f90ba89bf92546561ae999a4a48cd9c5a4704f"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/f24f4c15884bf1ee65a10e2f959842eec4198876", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f24f4c15884bf1ee65a10e2f959842eec4198876", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f24f4c15884bf1ee65a10e2f959842eec4198876", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f24f4c15884bf1ee65a10e2f959842eec4198876/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "4736041b5aa5681c44cbb9c183b11bbb781492fc", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4736041b5aa5681c44cbb9c183b11bbb781492fc", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4736041b5aa5681c44cbb9c183b11bbb781492fc"}], "stats": {"total": 985, "additions": 663, "deletions": 322}, "files": [{"sha": "c51b6f65720bb24d52ebf713c05b8d711f3e3d2e", "filename": "gcc/ChangeLog", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f24f4c15884bf1ee65a10e2f959842eec4198876/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f24f4c15884bf1ee65a10e2f959842eec4198876/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=f24f4c15884bf1ee65a10e2f959842eec4198876", "patch": "@@ -1,3 +1,24 @@\n+2019-09-19  Richard Sandiford  <richard.sandiford@arm.com>\n+\n+\t* defaults.h (TARGET_UNIT): New macro.\n+\t(target_unit): New type.\n+\t* rtl.h (native_encode_rtx, native_decode_rtx)\n+\t(native_decode_vector_rtx, subreg_size_lsb): Declare.\n+\t(subreg_lsb_1): Turn into an inline wrapper around subreg_size_lsb.\n+\t* rtlanal.c (subreg_lsb_1): Delete.\n+\t(subreg_size_lsb): New function.\n+\t* simplify-rtx.c: Include rtx-vector-builder.h\n+\t(simplify_immed_subreg): Delete.\n+\t(native_encode_rtx, native_decode_vector_rtx, native_decode_rtx)\n+\t(simplify_const_vector_byte_offset, simplify_const_vector_subreg): New\n+\tfunctions.\n+\t(simplify_subreg): Use them.\n+\t(test_vector_subregs_modes, test_vector_subregs_repeating)\n+\t(test_vector_subregs_fore_back, test_vector_subregs_stepped)\n+\t(test_vector_subregs): New functions.\n+\t(test_vector_ops): Call test_vector_subregs for integer vector\n+\tmodes with at least 2 elements.\n+\n 2019-09-19  Richard Biener  <rguenther@suse.de>\n \n \t* tree-parloops.c (parloops_is_slp_reduction): Do not set"}, {"sha": "72d4fba11a629140b28d109b5a9a2c24b65a4ad1", "filename": "gcc/defaults.h", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f24f4c15884bf1ee65a10e2f959842eec4198876/gcc%2Fdefaults.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f24f4c15884bf1ee65a10e2f959842eec4198876/gcc%2Fdefaults.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdefaults.h?ref=f24f4c15884bf1ee65a10e2f959842eec4198876", "patch": "@@ -1459,4 +1459,18 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n #define DWARF_GNAT_ENCODINGS_DEFAULT DWARF_GNAT_ENCODINGS_GDB\n #endif\n \n+#ifndef USED_FOR_TARGET\n+/* Done this way to keep gengtype happy.  */\n+#if BITS_PER_UNIT == 8\n+#define TARGET_UNIT uint8_t\n+#elif BITS_PER_UNIT == 16\n+#define TARGET_UNIT uint16_t\n+#elif BITS_PER_UNIT == 32\n+#define TARGET_UNIT uint32_t\n+#else\n+#error Unknown BITS_PER_UNIT\n+#endif\n+typedef TARGET_UNIT target_unit;\n+#endif\n+\n #endif  /* ! GCC_DEFAULTS_H */"}, {"sha": "9cadac7a9706f0281219f6acd5c62c50e7558f17", "filename": "gcc/rtl.h", "status": "modified", "additions": 19, "deletions": 1, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f24f4c15884bf1ee65a10e2f959842eec4198876/gcc%2Frtl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f24f4c15884bf1ee65a10e2f959842eec4198876/gcc%2Frtl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtl.h?ref=f24f4c15884bf1ee65a10e2f959842eec4198876", "patch": "@@ -2406,12 +2406,30 @@ extern int rtx_cost (rtx, machine_mode, enum rtx_code, int, bool);\n extern int address_cost (rtx, machine_mode, addr_space_t, bool);\n extern void get_full_rtx_cost (rtx, machine_mode, enum rtx_code, int,\n \t\t\t       struct full_rtx_costs *);\n+extern bool native_encode_rtx (machine_mode, rtx, vec<target_unit> &,\n+\t\t\t       unsigned int, unsigned int);\n+extern rtx native_decode_rtx (machine_mode, vec<target_unit>,\n+\t\t\t      unsigned int);\n+extern rtx native_decode_vector_rtx (machine_mode, vec<target_unit>,\n+\t\t\t\t     unsigned int, unsigned int, unsigned int);\n extern poly_uint64 subreg_lsb (const_rtx);\n-extern poly_uint64 subreg_lsb_1 (machine_mode, machine_mode, poly_uint64);\n+extern poly_uint64 subreg_size_lsb (poly_uint64, poly_uint64, poly_uint64);\n extern poly_uint64 subreg_size_offset_from_lsb (poly_uint64, poly_uint64,\n \t\t\t\t\t\tpoly_uint64);\n extern bool read_modify_subreg_p (const_rtx);\n \n+/* Given a subreg's OUTER_MODE, INNER_MODE, and SUBREG_BYTE, return the\n+   bit offset at which the subreg begins (counting from the least significant\n+   bit of the operand).  */\n+\n+inline poly_uint64\n+subreg_lsb_1 (machine_mode outer_mode, machine_mode inner_mode,\n+\t      poly_uint64 subreg_byte)\n+{\n+  return subreg_size_lsb (GET_MODE_SIZE (outer_mode),\n+\t\t\t  GET_MODE_SIZE (inner_mode), subreg_byte);\n+}\n+\n /* Return the subreg byte offset for a subreg whose outer mode is\n    OUTER_MODE, whose inner mode is INNER_MODE, and where there are\n    LSB_SHIFT *bits* between the lsb of the outer value and the lsb of"}, {"sha": "28b399cfc44c390f270ca6763b3b7240e2b0e77c", "filename": "gcc/rtlanal.c", "status": "modified", "additions": 18, "deletions": 10, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f24f4c15884bf1ee65a10e2f959842eec4198876/gcc%2Frtlanal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f24f4c15884bf1ee65a10e2f959842eec4198876/gcc%2Frtlanal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtlanal.c?ref=f24f4c15884bf1ee65a10e2f959842eec4198876", "patch": "@@ -3637,23 +3637,31 @@ loc_mentioned_in_p (rtx *loc, const_rtx in)\n   return 0;\n }\n \n-/* Helper function for subreg_lsb.  Given a subreg's OUTER_MODE, INNER_MODE,\n-   and SUBREG_BYTE, return the bit offset where the subreg begins\n-   (counting from the least significant bit of the operand).  */\n+/* Reinterpret a subreg as a bit extraction from an integer and return\n+   the position of the least significant bit of the extracted value.\n+   In other words, if the extraction were performed as a shift right\n+   and mask, return the number of bits to shift right.\n+\n+   The outer value of the subreg has OUTER_BYTES bytes and starts at\n+   byte offset SUBREG_BYTE within an inner value of INNER_BYTES bytes.  */\n \n poly_uint64\n-subreg_lsb_1 (machine_mode outer_mode,\n-\t      machine_mode inner_mode,\n-\t      poly_uint64 subreg_byte)\n+subreg_size_lsb (poly_uint64 outer_bytes,\n+\t\t poly_uint64 inner_bytes,\n+\t\t poly_uint64 subreg_byte)\n {\n   poly_uint64 subreg_end, trailing_bytes, byte_pos;\n \n   /* A paradoxical subreg begins at bit position 0.  */\n-  if (paradoxical_subreg_p (outer_mode, inner_mode))\n-    return 0;\n+  gcc_checking_assert (ordered_p (outer_bytes, inner_bytes));\n+  if (maybe_gt (outer_bytes, inner_bytes))\n+    {\n+      gcc_checking_assert (known_eq (subreg_byte, 0U));\n+      return 0;\n+    }\n \n-  subreg_end = subreg_byte + GET_MODE_SIZE (outer_mode);\n-  trailing_bytes = GET_MODE_SIZE (inner_mode) - subreg_end;\n+  subreg_end = subreg_byte + outer_bytes;\n+  trailing_bytes = inner_bytes - subreg_end;\n   if (WORDS_BIG_ENDIAN && BYTES_BIG_ENDIAN)\n     byte_pos = trailing_bytes;\n   else if (!WORDS_BIG_ENDIAN && !BYTES_BIG_ENDIAN)"}, {"sha": "87ba337725b5e1ed49ab1ee0894c35586ab0fa49", "filename": "gcc/simplify-rtx.c", "status": "modified", "additions": 591, "deletions": 311, "changes": 902, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f24f4c15884bf1ee65a10e2f959842eec4198876/gcc%2Fsimplify-rtx.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f24f4c15884bf1ee65a10e2f959842eec4198876/gcc%2Fsimplify-rtx.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsimplify-rtx.c?ref=f24f4c15884bf1ee65a10e2f959842eec4198876", "patch": "@@ -6130,342 +6130,466 @@ simplify_ternary_operation (enum rtx_code code, machine_mode mode,\n   return 0;\n }\n \n-/* Evaluate a SUBREG of a CONST_INT or CONST_WIDE_INT or CONST_DOUBLE\n-   or CONST_FIXED or CONST_VECTOR, returning another CONST_INT or\n-   CONST_WIDE_INT or CONST_DOUBLE or CONST_FIXED or CONST_VECTOR.\n+/* Try to calculate NUM_BYTES bytes of the target memory image of X,\n+   starting at byte FIRST_BYTE.  Return true on success and add the\n+   bytes to BYTES, such that each byte has BITS_PER_UNIT bits and such\n+   that the bytes follow target memory order.  Leave BYTES unmodified\n+   on failure.\n \n-   Works by unpacking INNER_BYTES bytes of OP into a collection of 8-bit values\n-   represented as a little-endian array of 'unsigned char', selecting by BYTE,\n-   and then repacking them again for OUTERMODE.  If OP is a CONST_VECTOR,\n-   FIRST_ELEM is the number of the first element to extract, otherwise\n-   FIRST_ELEM is ignored.  */\n+   MODE is the mode of X.  The caller must reserve NUM_BYTES bytes in\n+   BYTES before calling this function.  */\n \n-static rtx\n-simplify_immed_subreg (fixed_size_mode outermode, rtx op,\n-\t\t       machine_mode innermode, unsigned int byte,\n-\t\t       unsigned int first_elem, unsigned int inner_bytes)\n+bool\n+native_encode_rtx (machine_mode mode, rtx x, vec<target_unit> &bytes,\n+\t\t   unsigned int first_byte, unsigned int num_bytes)\n {\n-  enum {\n-    value_bit = 8,\n-    value_mask = (1 << value_bit) - 1\n-  };\n-  unsigned char value[MAX_BITSIZE_MODE_ANY_MODE / value_bit];\n-  int value_start;\n-  int i;\n-  int elem;\n-\n-  int num_elem;\n-  rtx * elems;\n-  int elem_bitsize;\n-  rtx result_s = NULL;\n-  rtvec result_v = NULL;\n-  enum mode_class outer_class;\n-  scalar_mode outer_submode;\n-  int max_bitsize;\n+  /* Check the mode is sensible.  */\n+  gcc_assert (GET_MODE (x) == VOIDmode\n+\t      ? is_a <scalar_int_mode> (mode)\n+\t      : mode == GET_MODE (x));\n \n-  /* Some ports misuse CCmode.  */\n-  if (GET_MODE_CLASS (outermode) == MODE_CC && CONST_INT_P (op))\n-    return op;\n+  if (GET_CODE (x) == CONST_VECTOR)\n+    {\n+      /* CONST_VECTOR_ELT follows target memory order, so no shuffling\n+\t is necessary.  The only complication is that MODE_VECTOR_BOOL\n+\t vectors can have several elements per byte.  */\n+      unsigned int elt_bits = vector_element_size (GET_MODE_BITSIZE (mode),\n+\t\t\t\t\t\t   GET_MODE_NUNITS (mode));\n+      unsigned int elt = first_byte * BITS_PER_UNIT / elt_bits;\n+      if (elt_bits < BITS_PER_UNIT)\n+\t{\n+\t  /* This is the only case in which elements can be smaller than\n+\t     a byte.  */\n+\t  gcc_assert (GET_MODE_CLASS (mode) == MODE_VECTOR_BOOL);\n+\t  for (unsigned int i = 0; i < num_bytes; ++i)\n+\t    {\n+\t      target_unit value = 0;\n+\t      for (unsigned int j = 0; j < BITS_PER_UNIT; j += elt_bits)\n+\t\t{\n+\t\t  value |= (INTVAL (CONST_VECTOR_ELT (x, elt)) & 1) << j;\n+\t\t  elt += 1;\n+\t\t}\n+\t      bytes.quick_push (value);\n+\t    }\n+\t  return true;\n+\t}\n \n-  /* We have no way to represent a complex constant at the rtl level.  */\n-  if (COMPLEX_MODE_P (outermode))\n-    return NULL_RTX;\n+      unsigned int start = bytes.length ();\n+      unsigned int elt_bytes = GET_MODE_UNIT_SIZE (mode);\n+      /* Make FIRST_BYTE relative to ELT.  */\n+      first_byte %= elt_bytes;\n+      while (num_bytes > 0)\n+\t{\n+\t  /* Work out how many bytes we want from element ELT.  */\n+\t  unsigned int chunk_bytes = MIN (num_bytes, elt_bytes - first_byte);\n+\t  if (!native_encode_rtx (GET_MODE_INNER (mode),\n+\t\t\t\t  CONST_VECTOR_ELT (x, elt), bytes,\n+\t\t\t\t  first_byte, chunk_bytes))\n+\t    {\n+\t      bytes.truncate (start);\n+\t      return false;\n+\t    }\n+\t  elt += 1;\n+\t  first_byte = 0;\n+\t  num_bytes -= chunk_bytes;\n+\t}\n+      return true;\n+    }\n \n-  /* We support any size mode.  */\n-  max_bitsize = MAX (GET_MODE_BITSIZE (outermode),\n-\t\t     inner_bytes * BITS_PER_UNIT);\n+  /* All subsequent cases are limited to scalars.  */\n+  scalar_mode smode;\n+  if (!is_a <scalar_mode> (mode, &smode))\n+    return false;\n \n-  /* Unpack the value.  */\n+  /* Make sure that the region is in range.  */\n+  unsigned int end_byte = first_byte + num_bytes;\n+  unsigned int mode_bytes = GET_MODE_SIZE (smode);\n+  gcc_assert (end_byte <= mode_bytes);\n \n-  if (GET_CODE (op) == CONST_VECTOR)\n+  if (CONST_SCALAR_INT_P (x))\n     {\n-      num_elem = CEIL (inner_bytes, GET_MODE_UNIT_SIZE (innermode));\n-      elem_bitsize = GET_MODE_UNIT_BITSIZE (innermode);\n+      /* The target memory layout is affected by both BYTES_BIG_ENDIAN\n+\t and WORDS_BIG_ENDIAN.  Use the subreg machinery to get the lsb\n+\t position of each byte.  */\n+      rtx_mode_t value (x, smode);\n+      wide_int_ref value_wi (value);\n+      for (unsigned int byte = first_byte; byte < end_byte; ++byte)\n+\t{\n+\t  /* Always constant because the inputs are.  */\n+\t  unsigned int lsb\n+\t    = subreg_size_lsb (1, mode_bytes, byte).to_constant ();\n+\t  /* Operate directly on the encoding rather than using\n+\t     wi::extract_uhwi, so that we preserve the sign or zero\n+\t     extension for modes that are not a whole number of bits in\n+\t     size.  (Zero extension is only used for the combination of\n+\t     innermode == BImode && STORE_FLAG_VALUE == 1).  */\n+\t  unsigned int elt = lsb / HOST_BITS_PER_WIDE_INT;\n+\t  unsigned int shift = lsb % HOST_BITS_PER_WIDE_INT;\n+\t  unsigned HOST_WIDE_INT uhwi = value_wi.elt (elt);\n+\t  bytes.quick_push (uhwi >> shift);\n+\t}\n+      return true;\n     }\n-  else\n+\n+  if (CONST_DOUBLE_P (x))\n     {\n-      num_elem = 1;\n-      elem_bitsize = max_bitsize;\n+      /* real_to_target produces an array of integers in target memory order.\n+\t All integers before the last one have 32 bits; the last one may\n+\t have 32 bits or fewer, depending on whether the mode bitsize\n+\t is divisible by 32.  Each of these integers is then laid out\n+\t in target memory as any other integer would be.  */\n+      long el32[MAX_BITSIZE_MODE_ANY_MODE / 32];\n+      real_to_target (el32, CONST_DOUBLE_REAL_VALUE (x), smode);\n+\n+      /* The (maximum) number of target bytes per element of el32.  */\n+      unsigned int bytes_per_el32 = 32 / BITS_PER_UNIT;\n+      gcc_assert (bytes_per_el32 != 0);\n+\n+      /* Build up the integers in a similar way to the CONST_SCALAR_INT_P\n+\t handling above.  */\n+      for (unsigned int byte = first_byte; byte < end_byte; ++byte)\n+\t{\n+\t  unsigned int index = byte / bytes_per_el32;\n+\t  unsigned int subbyte = byte % bytes_per_el32;\n+\t  unsigned int int_bytes = MIN (bytes_per_el32,\n+\t\t\t\t\tmode_bytes - index * bytes_per_el32);\n+\t  /* Always constant because the inputs are.  */\n+\t  unsigned int lsb\n+\t    = subreg_size_lsb (1, int_bytes, subbyte).to_constant ();\n+\t  bytes.quick_push ((unsigned long) el32[index] >> lsb);\n+\t}\n+      return true;\n     }\n-  /* If this asserts, it is too complicated; reducing value_bit may help.  */\n-  gcc_assert (BITS_PER_UNIT % value_bit == 0);\n-  /* I don't know how to handle endianness of sub-units.  */\n-  gcc_assert (elem_bitsize % BITS_PER_UNIT == 0);\n \n-  for (elem = 0; elem < num_elem; elem++)\n+  if (GET_CODE (x) == CONST_FIXED)\n     {\n-      unsigned char * vp;\n-      rtx el = (GET_CODE (op) == CONST_VECTOR\n-\t\t? CONST_VECTOR_ELT (op, first_elem + elem)\n-\t\t: op);\n+      for (unsigned int byte = first_byte; byte < end_byte; ++byte)\n+\t{\n+\t  /* Always constant because the inputs are.  */\n+\t  unsigned int lsb\n+\t    = subreg_size_lsb (1, mode_bytes, byte).to_constant ();\n+\t  unsigned HOST_WIDE_INT piece = CONST_FIXED_VALUE_LOW (x);\n+\t  if (lsb >= HOST_BITS_PER_WIDE_INT)\n+\t    {\n+\t      lsb -= HOST_BITS_PER_WIDE_INT;\n+\t      piece = CONST_FIXED_VALUE_HIGH (x);\n+\t    }\n+\t  bytes.quick_push (piece >> lsb);\n+\t}\n+      return true;\n+    }\n \n-      /* Vectors are kept in target memory order.  (This is probably\n-\t a mistake.)  */\n-      {\n-\tunsigned byte = (elem * elem_bitsize) / BITS_PER_UNIT;\n-\tunsigned ibyte = (((num_elem - 1 - elem) * elem_bitsize)\n-\t\t\t  / BITS_PER_UNIT);\n-\tunsigned word_byte = WORDS_BIG_ENDIAN ? ibyte : byte;\n-\tunsigned subword_byte = BYTES_BIG_ENDIAN ? ibyte : byte;\n-\tunsigned bytele = (subword_byte % UNITS_PER_WORD\n-\t\t\t + (word_byte / UNITS_PER_WORD) * UNITS_PER_WORD);\n-\tvp = value + (bytele * BITS_PER_UNIT) / value_bit;\n-      }\n+  return false;\n+}\n \n-      switch (GET_CODE (el))\n-\t{\n-\tcase CONST_INT:\n-\t  for (i = 0;\n-\t       i < HOST_BITS_PER_WIDE_INT && i < elem_bitsize;\n-\t       i += value_bit)\n-\t    *vp++ = INTVAL (el) >> i;\n-\t  /* CONST_INTs are always logically sign-extended.  */\n-\t  for (; i < elem_bitsize; i += value_bit)\n-\t    *vp++ = INTVAL (el) < 0 ? -1 : 0;\n-\t  break;\n+/* Read a vector of mode MODE from the target memory image given by BYTES,\n+   starting at byte FIRST_BYTE.  The vector is known to be encodable using\n+   NPATTERNS interleaved patterns with NELTS_PER_PATTERN elements each,\n+   and BYTES is known to have enough bytes to supply NPATTERNS *\n+   NELTS_PER_PATTERN vector elements.  Each element of BYTES contains\n+   BITS_PER_UNIT bits and the bytes are in target memory order.\n \n-\tcase CONST_WIDE_INT:\n-\t  {\n-\t    rtx_mode_t val = rtx_mode_t (el, GET_MODE_INNER (innermode));\n-\t    unsigned char extend = wi::sign_mask (val);\n-\t    int prec = wi::get_precision (val);\n-\n-\t    for (i = 0; i < prec && i < elem_bitsize; i += value_bit)\n-\t      *vp++ = wi::extract_uhwi (val, i, value_bit);\n-\t    for (; i < elem_bitsize; i += value_bit)\n-\t      *vp++ = extend;\n-\t  }\n-\t  break;\n+   Return the vector on success, otherwise return NULL_RTX.  */\n \n-\tcase CONST_DOUBLE:\n-\t  if (TARGET_SUPPORTS_WIDE_INT == 0 && GET_MODE (el) == VOIDmode)\n-\t    {\n-\t      unsigned char extend = 0;\n-\t      /* If this triggers, someone should have generated a\n-\t\t CONST_INT instead.  */\n-\t      gcc_assert (elem_bitsize > HOST_BITS_PER_WIDE_INT);\n-\n-\t      for (i = 0; i < HOST_BITS_PER_WIDE_INT; i += value_bit)\n-\t\t*vp++ = CONST_DOUBLE_LOW (el) >> i;\n-\t      while (i < HOST_BITS_PER_DOUBLE_INT && i < elem_bitsize)\n-\t\t{\n-\t\t  *vp++\n-\t\t    = CONST_DOUBLE_HIGH (el) >> (i - HOST_BITS_PER_WIDE_INT);\n-\t\t  i += value_bit;\n-\t\t}\n+rtx\n+native_decode_vector_rtx (machine_mode mode, vec<target_unit> bytes,\n+\t\t\t  unsigned int first_byte, unsigned int npatterns,\n+\t\t\t  unsigned int nelts_per_pattern)\n+{\n+  rtx_vector_builder builder (mode, npatterns, nelts_per_pattern);\n \n-\t      if (CONST_DOUBLE_HIGH (el) >> (HOST_BITS_PER_WIDE_INT - 1))\n-\t\textend = -1;\n-\t      for (; i < elem_bitsize; i += value_bit)\n-\t\t*vp++ = extend;\n-\t    }\n-\t  else\n-\t    {\n-\t      /* This is big enough for anything on the platform.  */\n-\t      long tmp[MAX_BITSIZE_MODE_ANY_MODE / 32];\n-\t      scalar_float_mode el_mode;\n+  unsigned int elt_bits = vector_element_size (GET_MODE_BITSIZE (mode),\n+\t\t\t\t\t       GET_MODE_NUNITS (mode));\n+  if (elt_bits < BITS_PER_UNIT)\n+    {\n+      /* This is the only case in which elements can be smaller than a byte.\n+\t Element 0 is always in the lsb of the containing byte.  */\n+      gcc_assert (GET_MODE_CLASS (mode) == MODE_VECTOR_BOOL);\n+      for (unsigned int i = 0; i < builder.encoded_nelts (); ++i)\n+\t{\n+\t  unsigned int bit_index = first_byte * BITS_PER_UNIT + i * elt_bits;\n+\t  unsigned int byte_index = bit_index / BITS_PER_UNIT;\n+\t  unsigned int lsb = bit_index % BITS_PER_UNIT;\n+\t  builder.quick_push (bytes[byte_index] & (1 << lsb)\n+\t\t\t      ? CONST1_RTX (BImode)\n+\t\t\t      : CONST0_RTX (BImode));\n+\t}\n+    }\n+  else\n+    {\n+      for (unsigned int i = 0; i < builder.encoded_nelts (); ++i)\n+\t{\n+\t  rtx x = native_decode_rtx (GET_MODE_INNER (mode), bytes, first_byte);\n+\t  if (!x)\n+\t    return NULL_RTX;\n+\t  builder.quick_push (x);\n+\t  first_byte += elt_bits / BITS_PER_UNIT;\n+\t}\n+    }\n+  return builder.build ();\n+}\n \n-\t      el_mode = as_a <scalar_float_mode> (GET_MODE (el));\n-\t      int bitsize = GET_MODE_BITSIZE (el_mode);\n+/* Read an rtx of mode MODE from the target memory image given by BYTES,\n+   starting at byte FIRST_BYTE.  Each element of BYTES contains BITS_PER_UNIT\n+   bits and the bytes are in target memory order.  The image has enough\n+   values to specify all bytes of MODE.\n \n-\t      gcc_assert (bitsize <= elem_bitsize);\n-\t      gcc_assert (bitsize % value_bit == 0);\n+   Return the rtx on success, otherwise return NULL_RTX.  */\n \n-\t      real_to_target (tmp, CONST_DOUBLE_REAL_VALUE (el),\n-\t\t\t      GET_MODE (el));\n+rtx\n+native_decode_rtx (machine_mode mode, vec<target_unit> bytes,\n+\t\t   unsigned int first_byte)\n+{\n+  if (VECTOR_MODE_P (mode))\n+    {\n+      /* If we know at compile time how many elements there are,\n+\t pull each element directly from BYTES.  */\n+      unsigned int nelts;\n+      if (GET_MODE_NUNITS (mode).is_constant (&nelts))\n+\treturn native_decode_vector_rtx (mode, bytes, first_byte, nelts, 1);\n+      return NULL_RTX;\n+    }\n \n-\t      /* real_to_target produces its result in words affected by\n-\t\t FLOAT_WORDS_BIG_ENDIAN.  However, we ignore this,\n-\t\t and use WORDS_BIG_ENDIAN instead; see the documentation\n-\t         of SUBREG in rtl.texi.  */\n-\t      for (i = 0; i < bitsize; i += value_bit)\n-\t\t{\n-\t\t  int ibase;\n-\t\t  if (WORDS_BIG_ENDIAN)\n-\t\t    ibase = bitsize - 1 - i;\n-\t\t  else\n-\t\t    ibase = i;\n-\t\t  *vp++ = tmp[ibase / 32] >> i % 32;\n-\t\t}\n+  scalar_int_mode imode;\n+  if (is_a <scalar_int_mode> (mode, &imode)\n+      && GET_MODE_PRECISION (imode) <= MAX_BITSIZE_MODE_ANY_INT)\n+    {\n+      /* Pull the bytes msb first, so that we can use simple\n+\t shift-and-insert wide_int operations.  */\n+      unsigned int size = GET_MODE_SIZE (imode);\n+      wide_int result (wi::zero (GET_MODE_PRECISION (imode)));\n+      for (unsigned int i = 0; i < size; ++i)\n+\t{\n+\t  unsigned int lsb = (size - i - 1) * BITS_PER_UNIT;\n+\t  /* Always constant because the inputs are.  */\n+\t  unsigned int subbyte\n+\t    = subreg_size_offset_from_lsb (1, size, lsb).to_constant ();\n+\t  result <<= BITS_PER_UNIT;\n+\t  result |= bytes[first_byte + subbyte];\n+\t}\n+      return immed_wide_int_const (result, imode);\n+    }\n \n-\t      /* It shouldn't matter what's done here, so fill it with\n-\t\t zero.  */\n-\t      for (; i < elem_bitsize; i += value_bit)\n-\t\t*vp++ = 0;\n-\t    }\n-\t  break;\n+  scalar_float_mode fmode;\n+  if (is_a <scalar_float_mode> (mode, &fmode))\n+    {\n+      /* We need to build an array of integers in target memory order.\n+\t All integers before the last one have 32 bits; the last one may\n+\t have 32 bits or fewer, depending on whether the mode bitsize\n+\t is divisible by 32.  */\n+      long el32[MAX_BITSIZE_MODE_ANY_MODE / 32];\n+      unsigned int num_el32 = CEIL (GET_MODE_BITSIZE (fmode), 32);\n+      memset (el32, 0, num_el32 * sizeof (long));\n+\n+      /* The (maximum) number of target bytes per element of el32.  */\n+      unsigned int bytes_per_el32 = 32 / BITS_PER_UNIT;\n+      gcc_assert (bytes_per_el32 != 0);\n+\n+      unsigned int mode_bytes = GET_MODE_SIZE (fmode);\n+      for (unsigned int byte = 0; byte < mode_bytes; ++byte)\n+\t{\n+\t  unsigned int index = byte / bytes_per_el32;\n+\t  unsigned int subbyte = byte % bytes_per_el32;\n+\t  unsigned int int_bytes = MIN (bytes_per_el32,\n+\t\t\t\t\tmode_bytes - index * bytes_per_el32);\n+\t  /* Always constant because the inputs are.  */\n+\t  unsigned int lsb\n+\t    = subreg_size_lsb (1, int_bytes, subbyte).to_constant ();\n+\t  el32[index] |= (unsigned long) bytes[first_byte + byte] << lsb;\n+\t}\n+      REAL_VALUE_TYPE r;\n+      real_from_target (&r, el32, fmode);\n+      return const_double_from_real_value (r, fmode);\n+    }\n \n-        case CONST_FIXED:\n-\t  if (elem_bitsize <= HOST_BITS_PER_WIDE_INT)\n-\t    {\n-\t      for (i = 0; i < elem_bitsize; i += value_bit)\n-\t\t*vp++ = CONST_FIXED_VALUE_LOW (el) >> i;\n-\t    }\n+  if (ALL_SCALAR_FIXED_POINT_MODE_P (mode))\n+    {\n+      scalar_mode smode = as_a <scalar_mode> (mode);\n+      FIXED_VALUE_TYPE f;\n+      f.data.low = 0;\n+      f.data.high = 0;\n+      f.mode = smode;\n+\n+      unsigned int mode_bytes = GET_MODE_SIZE (smode);\n+      for (unsigned int byte = 0; byte < mode_bytes; ++byte)\n+\t{\n+\t  /* Always constant because the inputs are.  */\n+\t  unsigned int lsb\n+\t    = subreg_size_lsb (1, mode_bytes, byte).to_constant ();\n+\t  unsigned HOST_WIDE_INT unit = bytes[first_byte + byte];\n+\t  if (lsb >= HOST_BITS_PER_WIDE_INT)\n+\t    f.data.high |= unit << (lsb - HOST_BITS_PER_WIDE_INT);\n \t  else\n-\t    {\n-\t      for (i = 0; i < HOST_BITS_PER_WIDE_INT; i += value_bit)\n-\t\t*vp++ = CONST_FIXED_VALUE_LOW (el) >> i;\n-              for (; i < HOST_BITS_PER_DOUBLE_INT && i < elem_bitsize;\n-\t\t   i += value_bit)\n-\t\t*vp++ = CONST_FIXED_VALUE_HIGH (el)\n-\t\t\t>> (i - HOST_BITS_PER_WIDE_INT);\n-\t      for (; i < elem_bitsize; i += value_bit)\n-\t\t*vp++ = 0;\n-\t    }\n-          break;\n-\n-\tdefault:\n-\t  gcc_unreachable ();\n+\t    f.data.low |= unit << lsb;\n \t}\n+      return CONST_FIXED_FROM_FIXED_VALUE (f, mode);\n     }\n \n-  /* Now, pick the right byte to start with.  */\n-  /* Renumber BYTE so that the least-significant byte is byte 0.  A special\n-     case is paradoxical SUBREGs, which shouldn't be adjusted since they\n-     will already have offset 0.  */\n-  if (inner_bytes >= GET_MODE_SIZE (outermode))\n+  return NULL_RTX;\n+}\n+\n+/* Simplify a byte offset BYTE into CONST_VECTOR X.  The main purpose\n+   is to convert a runtime BYTE value into a constant one.  */\n+\n+static poly_uint64\n+simplify_const_vector_byte_offset (rtx x, poly_uint64 byte)\n+{\n+  /* Cope with MODE_VECTOR_BOOL by operating on bits rather than bytes.  */\n+  machine_mode mode = GET_MODE (x);\n+  unsigned int elt_bits = vector_element_size (GET_MODE_BITSIZE (mode),\n+\t\t\t\t\t       GET_MODE_NUNITS (mode));\n+  /* The number of bits needed to encode one element from each pattern.  */\n+  unsigned int sequence_bits = CONST_VECTOR_NPATTERNS (x) * elt_bits;\n+\n+  /* Identify the start point in terms of a sequence number and a byte offset\n+     within that sequence.  */\n+  poly_uint64 first_sequence;\n+  unsigned HOST_WIDE_INT subbit;\n+  if (can_div_trunc_p (byte * BITS_PER_UNIT, sequence_bits,\n+\t\t       &first_sequence, &subbit))\n     {\n-      unsigned ibyte = inner_bytes - GET_MODE_SIZE (outermode) - byte;\n-      unsigned word_byte = WORDS_BIG_ENDIAN ? ibyte : byte;\n-      unsigned subword_byte = BYTES_BIG_ENDIAN ? ibyte : byte;\n-      byte = (subword_byte % UNITS_PER_WORD\n-\t      + (word_byte / UNITS_PER_WORD) * UNITS_PER_WORD);\n+      unsigned int nelts_per_pattern = CONST_VECTOR_NELTS_PER_PATTERN (x);\n+      if (nelts_per_pattern == 1)\n+\t/* This is a duplicated vector, so the value of FIRST_SEQUENCE\n+\t   doesn't matter.  */\n+\tbyte = subbit / BITS_PER_UNIT;\n+      else if (nelts_per_pattern == 2 && known_gt (first_sequence, 0U))\n+\t{\n+\t  /* The subreg drops the first element from each pattern and\n+\t     only uses the second element.  Find the first sequence\n+\t     that starts on a byte boundary.  */\n+\t  subbit += least_common_multiple (sequence_bits, BITS_PER_UNIT);\n+\t  byte = subbit / BITS_PER_UNIT;\n+\t}\n     }\n+  return byte;\n+}\n+\n+/* Subroutine of simplify_subreg in which:\n+\n+   - X is known to be a CONST_VECTOR\n+   - OUTERMODE is known to be a vector mode\n \n-  /* BYTE should still be inside OP.  (Note that BYTE is unsigned,\n-     so if it's become negative it will instead be very large.)  */\n-  gcc_assert (byte < inner_bytes);\n+   Try to handle the subreg by operating on the CONST_VECTOR encoding\n+   rather than on each individual element of the CONST_VECTOR.\n \n-  /* Convert from bytes to chunks of size value_bit.  */\n-  value_start = byte * (BITS_PER_UNIT / value_bit);\n+   Return the simplified subreg on success, otherwise return NULL_RTX.  */\n+\n+static rtx\n+simplify_const_vector_subreg (machine_mode outermode, rtx x,\n+\t\t\t      machine_mode innermode, unsigned int first_byte)\n+{\n+  /* Paradoxical subregs of vectors have dubious semantics.  */\n+  if (paradoxical_subreg_p (outermode, innermode))\n+    return NULL_RTX;\n \n-  /* Re-pack the value.  */\n-  num_elem = GET_MODE_NUNITS (outermode);\n+  /* We can only preserve the semantics of a stepped pattern if the new\n+     vector element is the same as the original one.  */\n+  if (CONST_VECTOR_STEPPED_P (x)\n+      && GET_MODE_INNER (outermode) != GET_MODE_INNER (innermode))\n+    return NULL_RTX;\n \n-  if (VECTOR_MODE_P (outermode))\n+  /* Cope with MODE_VECTOR_BOOL by operating on bits rather than bytes.  */\n+  unsigned int x_elt_bits\n+    = vector_element_size (GET_MODE_BITSIZE (innermode),\n+\t\t\t   GET_MODE_NUNITS (innermode));\n+  unsigned int out_elt_bits\n+    = vector_element_size (GET_MODE_BITSIZE (outermode),\n+\t\t\t   GET_MODE_NUNITS (outermode));\n+\n+  /* The number of bits needed to encode one element from every pattern\n+     of the original vector.  */\n+  unsigned int x_sequence_bits = CONST_VECTOR_NPATTERNS (x) * x_elt_bits;\n+\n+  /* The number of bits needed to encode one element from every pattern\n+     of the result.  */\n+  unsigned int out_sequence_bits\n+    = least_common_multiple (x_sequence_bits, out_elt_bits);\n+\n+  /* Work out the number of interleaved patterns in the output vector\n+     and the number of encoded elements per pattern.  */\n+  unsigned int out_npatterns = out_sequence_bits / out_elt_bits;\n+  unsigned int nelts_per_pattern = CONST_VECTOR_NELTS_PER_PATTERN (x);\n+\n+  /* The encoding scheme requires the number of elements to be a multiple\n+     of the number of patterns, so that each pattern appears at least once\n+     and so that the same number of elements appear from each pattern.  */\n+  bool ok_p = multiple_p (GET_MODE_NUNITS (outermode), out_npatterns);\n+  unsigned int const_nunits;\n+  if (GET_MODE_NUNITS (outermode).is_constant (&const_nunits)\n+      && (!ok_p || out_npatterns * nelts_per_pattern > const_nunits))\n     {\n-      result_v = rtvec_alloc (num_elem);\n-      elems = &RTVEC_ELT (result_v, 0);\n+      /* Either the encoding is invalid, or applying it would give us\n+\t more elements than we need.  Just encode each element directly.  */\n+      out_npatterns = const_nunits;\n+      nelts_per_pattern = 1;\n     }\n-  else\n-    elems = &result_s;\n+  else if (!ok_p)\n+    return NULL_RTX;\n \n-  outer_submode = GET_MODE_INNER (outermode);\n-  outer_class = GET_MODE_CLASS (outer_submode);\n-  elem_bitsize = GET_MODE_BITSIZE (outer_submode);\n+  /* Get enough bytes of X to form the new encoding.  */\n+  unsigned int buffer_bits = out_npatterns * nelts_per_pattern * out_elt_bits;\n+  unsigned int buffer_bytes = CEIL (buffer_bits, BITS_PER_UNIT);\n+  auto_vec<target_unit, 128> buffer (buffer_bytes);\n+  if (!native_encode_rtx (innermode, x, buffer, first_byte, buffer_bytes))\n+    return NULL_RTX;\n \n-  gcc_assert (elem_bitsize % value_bit == 0);\n-  gcc_assert (elem_bitsize + value_start * value_bit <= max_bitsize);\n+  /* Reencode the bytes as OUTERMODE.  */\n+  return native_decode_vector_rtx (outermode, buffer, 0, out_npatterns,\n+\t\t\t\t   nelts_per_pattern);\n+}\n \n-  for (elem = 0; elem < num_elem; elem++)\n-    {\n-      unsigned char *vp;\n+/* Try to simplify a subreg of a constant by encoding the subreg region\n+   as a sequence of target bytes and reading them back in the new mode.\n+   Return the new value on success, otherwise return null.\n \n-      /* Vectors are stored in target memory order.  (This is probably\n-\t a mistake.)  */\n-      {\n-\tunsigned byte = (elem * elem_bitsize) / BITS_PER_UNIT;\n-\tunsigned ibyte = (((num_elem - 1 - elem) * elem_bitsize)\n-\t\t\t  / BITS_PER_UNIT);\n-\tunsigned word_byte = WORDS_BIG_ENDIAN ? ibyte : byte;\n-\tunsigned subword_byte = BYTES_BIG_ENDIAN ? ibyte : byte;\n-\tunsigned bytele = (subword_byte % UNITS_PER_WORD\n-\t\t\t + (word_byte / UNITS_PER_WORD) * UNITS_PER_WORD);\n-\tvp = value + value_start + (bytele * BITS_PER_UNIT) / value_bit;\n-      }\n+   The subreg has outer mode OUTERMODE, inner mode INNERMODE, inner value X\n+   and byte offset FIRST_BYTE.  */\n \n-      switch (outer_class)\n-\t{\n-\tcase MODE_INT:\n-\tcase MODE_PARTIAL_INT:\n-\t  {\n-\t    int u;\n-\t    int base = 0;\n-\t    int units\n-\t      = (GET_MODE_BITSIZE (outer_submode) + HOST_BITS_PER_WIDE_INT - 1)\n-\t      / HOST_BITS_PER_WIDE_INT;\n-\t    HOST_WIDE_INT tmp[MAX_BITSIZE_MODE_ANY_INT / HOST_BITS_PER_WIDE_INT];\n-\t    wide_int r;\n-\n-\t    if (GET_MODE_PRECISION (outer_submode) > MAX_BITSIZE_MODE_ANY_INT)\n-\t      return NULL_RTX;\n-\t    for (u = 0; u < units; u++)\n-\t      {\n-\t\tunsigned HOST_WIDE_INT buf = 0;\n-\t\tfor (i = 0;\n-\t\t     i < HOST_BITS_PER_WIDE_INT && base + i < elem_bitsize;\n-\t\t     i += value_bit)\n-\t\t  buf |= (unsigned HOST_WIDE_INT)(*vp++ & value_mask) << i;\n-\n-\t\ttmp[u] = buf;\n-\t\tbase += HOST_BITS_PER_WIDE_INT;\n-\t      }\n-\t    r = wide_int::from_array (tmp, units,\n-\t\t\t\t      GET_MODE_PRECISION (outer_submode));\n-#if TARGET_SUPPORTS_WIDE_INT == 0\n-\t    /* Make sure r will fit into CONST_INT or CONST_DOUBLE.  */\n-\t    if (wi::min_precision (r, SIGNED) > HOST_BITS_PER_DOUBLE_INT)\n-\t      return NULL_RTX;\n-#endif\n-\t    elems[elem] = immed_wide_int_const (r, outer_submode);\n-\t  }\n-\t  break;\n+static rtx\n+simplify_immed_subreg (fixed_size_mode outermode, rtx x,\n+\t\t       machine_mode innermode, unsigned int first_byte)\n+{\n+  unsigned int buffer_bytes = GET_MODE_SIZE (outermode);\n+  auto_vec<target_unit, 128> buffer (buffer_bytes);\n \n-\tcase MODE_FLOAT:\n-\tcase MODE_DECIMAL_FLOAT:\n-\t  {\n-\t    REAL_VALUE_TYPE r;\n-\t    long tmp[MAX_BITSIZE_MODE_ANY_MODE / 32] = { 0 };\n-\n-\t    /* real_from_target wants its input in words affected by\n-\t       FLOAT_WORDS_BIG_ENDIAN.  However, we ignore this,\n-\t       and use WORDS_BIG_ENDIAN instead; see the documentation\n-\t       of SUBREG in rtl.texi.  */\n-\t    for (i = 0; i < elem_bitsize; i += value_bit)\n-\t      {\n-\t\tint ibase;\n-\t\tif (WORDS_BIG_ENDIAN)\n-\t\t  ibase = elem_bitsize - 1 - i;\n-\t\telse\n-\t\t  ibase = i;\n-\t\ttmp[ibase / 32] |= (*vp++ & value_mask) << i % 32;\n-\t      }\n+  /* Some ports misuse CCmode.  */\n+  if (GET_MODE_CLASS (outermode) == MODE_CC && CONST_INT_P (x))\n+    return x;\n \n-\t    real_from_target (&r, tmp, outer_submode);\n-\t    elems[elem] = const_double_from_real_value (r, outer_submode);\n-\t  }\n-\t  break;\n+  /* Paradoxical subregs read undefined values for bytes outside of the\n+     inner value.  However, we have traditionally always sign-extended\n+     integer constants and zero-extended others.  */\n+  unsigned int inner_bytes = buffer_bytes;\n+  if (paradoxical_subreg_p (outermode, innermode))\n+    {\n+      if (!GET_MODE_SIZE (innermode).is_constant (&inner_bytes))\n+\treturn NULL_RTX;\n \n-\tcase MODE_FRACT:\n-\tcase MODE_UFRACT:\n-\tcase MODE_ACCUM:\n-\tcase MODE_UACCUM:\n-\t  {\n-\t    FIXED_VALUE_TYPE f;\n-\t    f.data.low = 0;\n-\t    f.data.high = 0;\n-\t    f.mode = outer_submode;\n-\n-\t    for (i = 0;\n-\t\t i < HOST_BITS_PER_WIDE_INT && i < elem_bitsize;\n-\t\t i += value_bit)\n-\t      f.data.low |= (unsigned HOST_WIDE_INT)(*vp++ & value_mask) << i;\n-\t    for (; i < elem_bitsize; i += value_bit)\n-\t      f.data.high |= ((unsigned HOST_WIDE_INT)(*vp++ & value_mask)\n-\t\t\t     << (i - HOST_BITS_PER_WIDE_INT));\n-\n-\t    elems[elem] = CONST_FIXED_FROM_FIXED_VALUE (f, outer_submode);\n-          }\n-          break;\n+      target_unit filler = 0;\n+      if (CONST_SCALAR_INT_P (x) && wi::neg_p (rtx_mode_t (x, innermode)))\n+\tfiller = -1;\n \n-\tdefault:\n-\t  gcc_unreachable ();\n-\t}\n+      /* Add any leading bytes due to big-endian layout.  The number of\n+\t bytes must be constant because both modes have constant size.  */\n+      unsigned int leading_bytes\n+\t= -byte_lowpart_offset (outermode, innermode).to_constant ();\n+      for (unsigned int i = 0; i < leading_bytes; ++i)\n+\tbuffer.quick_push (filler);\n+\n+      if (!native_encode_rtx (innermode, x, buffer, first_byte, inner_bytes))\n+\treturn NULL_RTX;\n+\n+      /* Add any trailing bytes due to little-endian layout.  */\n+      while (buffer.length () < buffer_bytes)\n+\tbuffer.quick_push (filler);\n     }\n-  if (VECTOR_MODE_P (outermode))\n-    return gen_rtx_CONST_VECTOR (outermode, result_v);\n   else\n-    return result_s;\n+    {\n+      if (!native_encode_rtx (innermode, x, buffer, first_byte, inner_bytes))\n+\treturn NULL_RTX;\n+      }\n+  return native_decode_rtx (outermode, buffer, 0);\n }\n \n /* Simplify SUBREG:OUTERMODE(OP:INNERMODE, BYTE)\n@@ -6494,6 +6618,9 @@ simplify_subreg (machine_mode outermode, rtx op,\n   if (outermode == innermode && known_eq (byte, 0U))\n     return op;\n \n+  if (GET_CODE (op) == CONST_VECTOR)\n+    byte = simplify_const_vector_byte_offset (op, byte);\n+\n   if (multiple_p (byte, GET_MODE_UNIT_SIZE (innermode)))\n     {\n       rtx elt;\n@@ -6513,30 +6640,21 @@ simplify_subreg (machine_mode outermode, rtx op,\n       || CONST_FIXED_P (op)\n       || GET_CODE (op) == CONST_VECTOR)\n     {\n-      /* simplify_immed_subreg deconstructs OP into bytes and constructs\n-\t the result from bytes, so it only works if the sizes of the modes\n-\t and the value of the offset are known at compile time.  Cases that\n-\t that apply to general modes and offsets should be handled here\n-\t before calling simplify_immed_subreg.  */\n-      fixed_size_mode fs_outermode, fs_innermode;\n       unsigned HOST_WIDE_INT cbyte;\n-      if (is_a <fixed_size_mode> (outermode, &fs_outermode)\n-\t  && is_a <fixed_size_mode> (innermode, &fs_innermode)\n-\t  && byte.is_constant (&cbyte))\n-\treturn simplify_immed_subreg (fs_outermode, op, fs_innermode, cbyte,\n-\t\t\t\t      0, GET_MODE_SIZE (fs_innermode));\n-\n-      /* Handle constant-sized outer modes and variable-sized inner modes.  */\n-      unsigned HOST_WIDE_INT first_elem;\n-      if (GET_CODE (op) == CONST_VECTOR\n-\t  && is_a <fixed_size_mode> (outermode, &fs_outermode)\n-\t  && constant_multiple_p (byte, GET_MODE_UNIT_SIZE (innermode),\n-\t\t\t\t  &first_elem))\n-\treturn simplify_immed_subreg (fs_outermode, op, innermode, 0,\n-\t\t\t\t      first_elem,\n-\t\t\t\t      GET_MODE_SIZE (fs_outermode));\n+      if (byte.is_constant (&cbyte))\n+\t{\n+\t  if (GET_CODE (op) == CONST_VECTOR && VECTOR_MODE_P (outermode))\n+\t    {\n+\t      rtx tmp = simplify_const_vector_subreg (outermode, op,\n+\t\t\t\t\t\t      innermode, cbyte);\n+\t      if (tmp)\n+\t\treturn tmp;\n+\t    }\n \n-      return NULL_RTX;\n+\t  fixed_size_mode fs_outermode;\n+\t  if (is_a <fixed_size_mode> (outermode, &fs_outermode))\n+\t    return simplify_immed_subreg (fs_outermode, op, innermode, cbyte);\n+\t}\n     }\n \n   /* Changing mode twice with SUBREG => just change it once,\n@@ -7179,6 +7297,165 @@ test_vec_merge (machine_mode mode)\n \t\t simplify_rtx (nvm));\n }\n \n+/* Test subregs of integer vector constant X, trying elements in\n+   the range [ELT_BIAS, ELT_BIAS + constant_lower_bound (NELTS)),\n+   where NELTS is the number of elements in X.  Subregs involving\n+   elements [ELT_BIAS, ELT_BIAS + FIRST_VALID) are expected to fail.  */\n+\n+static void\n+test_vector_subregs_modes (rtx x, poly_uint64 elt_bias = 0,\n+\t\t\t   unsigned int first_valid = 0)\n+{\n+  machine_mode inner_mode = GET_MODE (x);\n+  scalar_mode int_mode = GET_MODE_INNER (inner_mode);\n+\n+  for (unsigned int modei = 0; modei < NUM_MACHINE_MODES; ++modei)\n+    {\n+      machine_mode outer_mode = (machine_mode) modei;\n+      if (!VECTOR_MODE_P (outer_mode))\n+\tcontinue;\n+\n+      unsigned int outer_nunits;\n+      if (GET_MODE_INNER (outer_mode) == int_mode\n+\t  && GET_MODE_NUNITS (outer_mode).is_constant (&outer_nunits)\n+\t  && multiple_p (GET_MODE_NUNITS (inner_mode), outer_nunits))\n+\t{\n+\t  /* Test subregs in which the outer mode is a smaller,\n+\t     constant-sized vector of the same element type.  */\n+\t  unsigned int limit\n+\t    = constant_lower_bound (GET_MODE_NUNITS (inner_mode));\n+\t  for (unsigned int elt = 0; elt < limit; elt += outer_nunits)\n+\t    {\n+\t      rtx expected = NULL_RTX;\n+\t      if (elt >= first_valid)\n+\t\t{\n+\t\t  rtx_vector_builder builder (outer_mode, outer_nunits, 1);\n+\t\t  for (unsigned int i = 0; i < outer_nunits; ++i)\n+\t\t    builder.quick_push (CONST_VECTOR_ELT (x, elt + i));\n+\t\t  expected = builder.build ();\n+\t\t}\n+\t      poly_uint64 byte = (elt_bias + elt) * GET_MODE_SIZE (int_mode);\n+\t      ASSERT_RTX_EQ (expected,\n+\t\t\t     simplify_subreg (outer_mode, x,\n+\t\t\t\t\t      inner_mode, byte));\n+\t    }\n+\t}\n+      else if (known_eq (GET_MODE_SIZE (outer_mode),\n+\t\t\t GET_MODE_SIZE (inner_mode))\n+\t       && known_eq (elt_bias, 0U)\n+\t       && (GET_MODE_CLASS (outer_mode) != MODE_VECTOR_BOOL\n+\t\t   || known_eq (GET_MODE_BITSIZE (outer_mode),\n+\t\t\t\tGET_MODE_NUNITS (outer_mode)))\n+\t       && (!FLOAT_MODE_P (outer_mode)\n+\t\t   || (FLOAT_MODE_FORMAT (outer_mode)->ieee_bits\n+\t\t       == GET_MODE_UNIT_PRECISION (outer_mode)))\n+\t       && (GET_MODE_SIZE (inner_mode).is_constant ()\n+\t\t   || !CONST_VECTOR_STEPPED_P (x)))\n+\t{\n+\t  /* Try converting to OUTER_MODE and back.  */\n+\t  rtx outer_x = simplify_subreg (outer_mode, x, inner_mode, 0);\n+\t  ASSERT_TRUE (outer_x != NULL_RTX);\n+\t  ASSERT_RTX_EQ (x, simplify_subreg (inner_mode, outer_x,\n+\t\t\t\t\t     outer_mode, 0));\n+\t}\n+    }\n+\n+  if (BYTES_BIG_ENDIAN == WORDS_BIG_ENDIAN)\n+    {\n+      /* Test each byte in the element range.  */\n+      unsigned int limit\n+\t= constant_lower_bound (GET_MODE_SIZE (inner_mode));\n+      for (unsigned int i = 0; i < limit; ++i)\n+\t{\n+\t  unsigned int elt = i / GET_MODE_SIZE (int_mode);\n+\t  rtx expected = NULL_RTX;\n+\t  if (elt >= first_valid)\n+\t    {\n+\t      unsigned int byte_shift = i % GET_MODE_SIZE (int_mode);\n+\t      if (BYTES_BIG_ENDIAN)\n+\t\tbyte_shift = GET_MODE_SIZE (int_mode) - byte_shift - 1;\n+\t      rtx_mode_t vec_elt (CONST_VECTOR_ELT (x, elt), int_mode);\n+\t      wide_int shifted_elt\n+\t\t= wi::lrshift (vec_elt, byte_shift * BITS_PER_UNIT);\n+\t      expected = immed_wide_int_const (shifted_elt, QImode);\n+\t    }\n+\t  poly_uint64 byte = elt_bias * GET_MODE_SIZE (int_mode) + i;\n+\t  ASSERT_RTX_EQ (expected,\n+\t\t\t simplify_subreg (QImode, x, inner_mode, byte));\n+\t}\n+    }\n+}\n+\n+/* Test constant subregs of integer vector mode INNER_MODE, using 1\n+   element per pattern.  */\n+\n+static void\n+test_vector_subregs_repeating (machine_mode inner_mode)\n+{\n+  poly_uint64 nunits = GET_MODE_NUNITS (inner_mode);\n+  unsigned int min_nunits = constant_lower_bound (nunits);\n+  scalar_mode int_mode = GET_MODE_INNER (inner_mode);\n+  unsigned int count = gcd (min_nunits, 8);\n+\n+  rtx_vector_builder builder (inner_mode, count, 1);\n+  for (unsigned int i = 0; i < count; ++i)\n+    builder.quick_push (gen_int_mode (8 - i, int_mode));\n+  rtx x = builder.build ();\n+\n+  test_vector_subregs_modes (x);\n+  if (!nunits.is_constant ())\n+    test_vector_subregs_modes (x, nunits - min_nunits);\n+}\n+\n+/* Test constant subregs of integer vector mode INNER_MODE, using 2\n+   elements per pattern.  */\n+\n+static void\n+test_vector_subregs_fore_back (machine_mode inner_mode)\n+{\n+  poly_uint64 nunits = GET_MODE_NUNITS (inner_mode);\n+  unsigned int min_nunits = constant_lower_bound (nunits);\n+  scalar_mode int_mode = GET_MODE_INNER (inner_mode);\n+  unsigned int count = gcd (min_nunits, 4);\n+\n+  rtx_vector_builder builder (inner_mode, count, 2);\n+  for (unsigned int i = 0; i < count; ++i)\n+    builder.quick_push (gen_int_mode (i, int_mode));\n+  for (unsigned int i = 0; i < count; ++i)\n+    builder.quick_push (gen_int_mode (-(int) i, int_mode));\n+  rtx x = builder.build ();\n+\n+  test_vector_subregs_modes (x);\n+  if (!nunits.is_constant ())\n+    test_vector_subregs_modes (x, nunits - min_nunits, count);\n+}\n+\n+/* Test constant subregs of integer vector mode INNER_MODE, using 3\n+   elements per pattern.  */\n+\n+static void\n+test_vector_subregs_stepped (machine_mode inner_mode)\n+{\n+  /* Build { 0, 1, 2, 3, ... }.  */\n+  scalar_mode int_mode = GET_MODE_INNER (inner_mode);\n+  rtx_vector_builder builder (inner_mode, 1, 3);\n+  for (unsigned int i = 0; i < 3; ++i)\n+    builder.quick_push (gen_int_mode (i, int_mode));\n+  rtx x = builder.build ();\n+\n+  test_vector_subregs_modes (x);\n+}\n+\n+/* Test constant subregs of integer vector mode INNER_MODE.  */\n+\n+static void\n+test_vector_subregs (machine_mode inner_mode)\n+{\n+  test_vector_subregs_repeating (inner_mode);\n+  test_vector_subregs_fore_back (inner_mode);\n+  test_vector_subregs_stepped (inner_mode);\n+}\n+\n /* Verify some simplifications involving vectors.  */\n \n static void\n@@ -7193,7 +7470,10 @@ test_vector_ops ()\n \t  test_vector_ops_duplicate (mode, scalar_reg);\n \t  if (GET_MODE_CLASS (mode) == MODE_VECTOR_INT\n \t      && maybe_gt (GET_MODE_NUNITS (mode), 2))\n-\t    test_vector_ops_series (mode, scalar_reg);\n+\t    {\n+\t      test_vector_ops_series (mode, scalar_reg);\n+\t      test_vector_subregs (mode);\n+\t    }\n \t  test_vec_merge (mode);\n \t}\n     }"}]}