{"sha": "0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MGRjNjY0NWZjM2IxMGM3OGMwMmQzNTQzZDM0NGI5YjVmYmEwZDBkNQ==", "commit": {"author": {"name": "Aaron Sawdey", "email": "acsawdey@linux.vnet.ibm.com", "date": "2017-07-06T20:20:48Z"}, "committer": {"name": "Aaron Sawdey", "email": "acsawdey@gcc.gnu.org", "date": "2017-07-06T20:20:48Z"}, "message": "rs6000.c (union_defs, [...]): Move all code related to p8 swap optimizations to file rs6000-p8swap.c.\n\n2017-07-06  Aaron Sawdey  <acsawdey@linux.vnet.ibm.com>\n\n\t* config/rs6000/rs6000.c (union_defs, union_uses, insn_is_load_p,\n\tinsn_is_store_p, insn_is_swap_p, const_load_sequence_p, v2df_reduction_p,\n\trtx_is_swappable_p, insn_is_swappable_p, chain_contains_only_swaps,\n\tmark_swaps_for_removal, swap_const_vector_halves, adjust_subreg_index,\n\tpermute_load, permute_store, adjust_extract, adjust_splat,\n\tadjust_xxpermdi, adjust_concat, adjust_vperm, handle_special_swappables,\n\treplace_swap_with_copy, dump_swap_insn_table,\n\talignment_with_canonical_addr, alignment_mask, find_alignment_op,\n\trecombine_lvx_pattern, recombine_stvx_pattern,\n\trecombine_lvx_stvx_patterns, rs6000_analyze_swaps,\n\tmake_pass_analyze_swaps): Move all code related to p8 swap optimizations\n\tto file rs6000-p8swap.c.\n\t* config/rs6000/rs6000-p8swap.c: New file.\n\t* config/rs6000/t-rs6000: Add rule to build rs6000-p8swap.o.\n\t* config.gcc: Add rs6000-p8swap.o to extra_objs for powerpc*-*-*\n\tand rs6000*-*-* targets.\n\nFrom-SVN: r250040", "tree": {"sha": "6c331c017078840d69522cd015c9440eb2071767", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/6c331c017078840d69522cd015c9440eb2071767"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5/comments", "author": null, "committer": null, "parents": [{"sha": "2c6e2eb1b10bb921b739c8733721cc8a0274dccd", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2c6e2eb1b10bb921b739c8733721cc8a0274dccd", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2c6e2eb1b10bb921b739c8733721cc8a0274dccd"}], "stats": {"total": 3775, "additions": 1917, "deletions": 1858}, "files": [{"sha": "933d7152283e80f0a8c1de5473b10d0116b5176c", "filename": "gcc/ChangeLog", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5", "patch": "@@ -1,3 +1,22 @@\n+2017-07-06  Aaron Sawdey  <acsawdey@linux.vnet.ibm.com>\n+\n+\t* config/rs6000/rs6000.c (union_defs, union_uses, insn_is_load_p,\n+\tinsn_is_store_p, insn_is_swap_p, const_load_sequence_p, v2df_reduction_p,\n+\trtx_is_swappable_p, insn_is_swappable_p, chain_contains_only_swaps,\n+\tmark_swaps_for_removal, swap_const_vector_halves, adjust_subreg_index,\n+\tpermute_load, permute_store, adjust_extract, adjust_splat,\n+\tadjust_xxpermdi, adjust_concat, adjust_vperm, handle_special_swappables,\n+\treplace_swap_with_copy, dump_swap_insn_table,\n+\talignment_with_canonical_addr, alignment_mask, find_alignment_op,\n+\trecombine_lvx_pattern, recombine_stvx_pattern,\n+\trecombine_lvx_stvx_patterns, rs6000_analyze_swaps,\n+\tmake_pass_analyze_swaps): Move all code related to p8 swap optimizations\n+\tto file rs6000-p8swap.c.\n+\t* config/rs6000/rs6000-p8swap.c: New file.\n+\t* config/rs6000/t-rs6000: Add rule to build rs6000-p8swap.o.\n+\t* config.gcc: Add rs6000-p8swap.o to extra_objs for powerpc*-*-*\n+\tand rs6000*-*-* targets.\n+\n 2017-07-06  David Malcolm  <dmalcolm@redhat.com>\n \n \t* Makefile.in (selftest): Remove dependency on s-selftest-c++."}, {"sha": "4a729507200954b7e047f1c989532e1556480384", "filename": "gcc/config.gcc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5/gcc%2Fconfig.gcc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5/gcc%2Fconfig.gcc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig.gcc?ref=0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5", "patch": "@@ -454,7 +454,7 @@ powerpc*-*-*spe*)\n \t;;\n powerpc*-*-*)\n \tcpu_type=rs6000\n-\textra_objs=\"rs6000-string.o\"\n+\textra_objs=\"rs6000-string.o rs6000-p8swap.o\"\n \textra_headers=\"ppc-asm.h altivec.h htmintrin.h htmxlintrin.h\"\n \textra_headers=\"${extra_headers} bmi2intrin.h bmiintrin.h x86intrin.h\"\n \textra_headers=\"${extra_headers} ppu_intrinsics.h spu2vmx.h vec_types.h si2vmx.h\"\n@@ -472,7 +472,7 @@ riscv*)\n \t;;\n rs6000*-*-*)\n \textra_options=\"${extra_options} g.opt fused-madd.opt rs6000/rs6000-tables.opt\"\n-\textra_objs=\"rs6000-string.o\"\n+\textra_objs=\"rs6000-string.o rs6000-p8swap.o\"\n \t;;\n sparc*-*-*)\n \tcpu_type=sparc"}, {"sha": "1557f7f587e21aca383e8162fe1d31bed15a6593", "filename": "gcc/config/rs6000/rs6000-p8swap.c", "status": "added", "additions": 1892, "deletions": 0, "changes": 1892, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5/gcc%2Fconfig%2Frs6000%2Frs6000-p8swap.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5/gcc%2Fconfig%2Frs6000%2Frs6000-p8swap.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000-p8swap.c?ref=0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5", "patch": "@@ -0,0 +1,1892 @@\n+/* Subroutines used to remove unnecessary doubleword swaps\n+   for p8 little-endian VSX code.\n+   Copyright (C) 1991-2017 Free Software Foundation, Inc.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published\n+   by the Free Software Foundation; either version 3, or (at your\n+   option) any later version.\n+\n+   GCC is distributed in the hope that it will be useful, but WITHOUT\n+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY\n+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public\n+   License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING3.  If not see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"backend.h\"\n+#include \"rtl.h\"\n+#include \"tree.h\"\n+#include \"memmodel.h\"\n+#include \"df.h\"\n+#include \"tm_p.h\"\n+#include \"ira.h\"\n+#include \"print-tree.h\"\n+#include \"varasm.h\"\n+#include \"explow.h\"\n+#include \"expr.h\"\n+#include \"output.h\"\n+#include \"tree-pass.h\"\n+\n+/* Analyze vector computations and remove unnecessary doubleword\n+   swaps (xxswapdi instructions).  This pass is performed only\n+   for little-endian VSX code generation.\n+\n+   For this specific case, loads and stores of 4x32 and 2x64 vectors\n+   are inefficient.  These are implemented using the lvx2dx and\n+   stvx2dx instructions, which invert the order of doublewords in\n+   a vector register.  Thus the code generation inserts an xxswapdi\n+   after each such load, and prior to each such store.  (For spill\n+   code after register assignment, an additional xxswapdi is inserted\n+   following each store in order to return a hard register to its\n+   unpermuted value.)\n+\n+   The extra xxswapdi instructions reduce performance.  This can be\n+   particularly bad for vectorized code.  The purpose of this pass\n+   is to reduce the number of xxswapdi instructions required for\n+   correctness.\n+\n+   The primary insight is that much code that operates on vectors\n+   does not care about the relative order of elements in a register,\n+   so long as the correct memory order is preserved.  If we have\n+   a computation where all input values are provided by lvxd2x/xxswapdi\n+   sequences, all outputs are stored using xxswapdi/stvxd2x sequences,\n+   and all intermediate computations are pure SIMD (independent of\n+   element order), then all the xxswapdi's associated with the loads\n+   and stores may be removed.\n+\n+   This pass uses some of the infrastructure and logical ideas from\n+   the \"web\" pass in web.c.  We create maximal webs of computations\n+   fitting the description above using union-find.  Each such web is\n+   then optimized by removing its unnecessary xxswapdi instructions.\n+\n+   The pass is placed prior to global optimization so that we can\n+   perform the optimization in the safest and simplest way possible;\n+   that is, by replacing each xxswapdi insn with a register copy insn.\n+   Subsequent forward propagation will remove copies where possible.\n+\n+   There are some operations sensitive to element order for which we\n+   can still allow the operation, provided we modify those operations.\n+   These include CONST_VECTORs, for which we must swap the first and\n+   second halves of the constant vector; and SUBREGs, for which we\n+   must adjust the byte offset to account for the swapped doublewords.\n+   A remaining opportunity would be non-immediate-form splats, for\n+   which we should adjust the selected lane of the input.  We should\n+   also make code generation adjustments for sum-across operations,\n+   since this is a common vectorizer reduction.\n+\n+   Because we run prior to the first split, we can see loads and stores\n+   here that match *vsx_le_perm_{load,store}_<mode>.  These are vanilla\n+   vector loads and stores that have not yet been split into a permuting\n+   load/store and a swap.  (One way this can happen is with a builtin\n+   call to vec_vsx_{ld,st}.)  We can handle these as well, but rather\n+   than deleting a swap, we convert the load/store into a permuting\n+   load/store (which effectively removes the swap).  */\n+\n+/* Notes on Permutes\n+\n+   We do not currently handle computations that contain permutes.  There\n+   is a general transformation that can be performed correctly, but it\n+   may introduce more expensive code than it replaces.  To handle these\n+   would require a cost model to determine when to perform the optimization.\n+   This commentary records how this could be done if desired.\n+\n+   The most general permute is something like this (example for V16QI):\n+\n+   (vec_select:V16QI (vec_concat:V32QI (op1:V16QI) (op2:V16QI))\n+                     (parallel [(const_int a0) (const_int a1)\n+                                 ...\n+                                (const_int a14) (const_int a15)]))\n+\n+   where a0,...,a15 are in [0,31] and select elements from op1 and op2\n+   to produce in the result.\n+\n+   Regardless of mode, we can convert the PARALLEL to a mask of 16\n+   byte-element selectors.  Let's call this M, with M[i] representing\n+   the ith byte-element selector value.  Then if we swap doublewords\n+   throughout the computation, we can get correct behavior by replacing\n+   M with M' as follows:\n+\n+    M'[i] = { (M[i]+8)%16      : M[i] in [0,15]\n+            { ((M[i]+8)%16)+16 : M[i] in [16,31]\n+\n+   This seems promising at first, since we are just replacing one mask\n+   with another.  But certain masks are preferable to others.  If M\n+   is a mask that matches a vmrghh pattern, for example, M' certainly\n+   will not.  Instead of a single vmrghh, we would generate a load of\n+   M' and a vperm.  So we would need to know how many xxswapd's we can\n+   remove as a result of this transformation to determine if it's\n+   profitable; and preferably the logic would need to be aware of all\n+   the special preferable masks.\n+\n+   Another form of permute is an UNSPEC_VPERM, in which the mask is\n+   already in a register.  In some cases, this mask may be a constant\n+   that we can discover with ud-chains, in which case the above\n+   transformation is ok.  However, the common usage here is for the\n+   mask to be produced by an UNSPEC_LVSL, in which case the mask \n+   cannot be known at compile time.  In such a case we would have to\n+   generate several instructions to compute M' as above at run time,\n+   and a cost model is needed again.\n+\n+   However, when the mask M for an UNSPEC_VPERM is loaded from the\n+   constant pool, we can replace M with M' as above at no cost\n+   beyond adding a constant pool entry.  */\n+\n+/* This is based on the union-find logic in web.c.  web_entry_base is\n+   defined in df.h.  */\n+class swap_web_entry : public web_entry_base\n+{\n+ public:\n+  /* Pointer to the insn.  */\n+  rtx_insn *insn;\n+  /* Set if insn contains a mention of a vector register.  All other\n+     fields are undefined if this field is unset.  */\n+  unsigned int is_relevant : 1;\n+  /* Set if insn is a load.  */\n+  unsigned int is_load : 1;\n+  /* Set if insn is a store.  */\n+  unsigned int is_store : 1;\n+  /* Set if insn is a doubleword swap.  This can either be a register swap\n+     or a permuting load or store (test is_load and is_store for this).  */\n+  unsigned int is_swap : 1;\n+  /* Set if the insn has a live-in use of a parameter register.  */\n+  unsigned int is_live_in : 1;\n+  /* Set if the insn has a live-out def of a return register.  */\n+  unsigned int is_live_out : 1;\n+  /* Set if the insn contains a subreg reference of a vector register.  */\n+  unsigned int contains_subreg : 1;\n+  /* Set if the insn contains a 128-bit integer operand.  */\n+  unsigned int is_128_int : 1;\n+  /* Set if this is a call-insn.  */\n+  unsigned int is_call : 1;\n+  /* Set if this insn does not perform a vector operation for which\n+     element order matters, or if we know how to fix it up if it does.\n+     Undefined if is_swap is set.  */\n+  unsigned int is_swappable : 1;\n+  /* A nonzero value indicates what kind of special handling for this\n+     insn is required if doublewords are swapped.  Undefined if\n+     is_swappable is not set.  */\n+  unsigned int special_handling : 4;\n+  /* Set if the web represented by this entry cannot be optimized.  */\n+  unsigned int web_not_optimizable : 1;\n+  /* Set if this insn should be deleted.  */\n+  unsigned int will_delete : 1;\n+};\n+\n+enum special_handling_values {\n+  SH_NONE = 0,\n+  SH_CONST_VECTOR,\n+  SH_SUBREG,\n+  SH_NOSWAP_LD,\n+  SH_NOSWAP_ST,\n+  SH_EXTRACT,\n+  SH_SPLAT,\n+  SH_XXPERMDI,\n+  SH_CONCAT,\n+  SH_VPERM\n+};\n+\n+/* Union INSN with all insns containing definitions that reach USE.\n+   Detect whether USE is live-in to the current function.  */\n+static void\n+union_defs (swap_web_entry *insn_entry, rtx insn, df_ref use)\n+{\n+  struct df_link *link = DF_REF_CHAIN (use);\n+\n+  if (!link)\n+    insn_entry[INSN_UID (insn)].is_live_in = 1;\n+\n+  while (link)\n+    {\n+      if (DF_REF_IS_ARTIFICIAL (link->ref))\n+\tinsn_entry[INSN_UID (insn)].is_live_in = 1;\n+\n+      if (DF_REF_INSN_INFO (link->ref))\n+\t{\n+\t  rtx def_insn = DF_REF_INSN (link->ref);\n+\t  (void)unionfind_union (insn_entry + INSN_UID (insn),\n+\t\t\t\t insn_entry + INSN_UID (def_insn));\n+\t}\n+\n+      link = link->next;\n+    }\n+}\n+\n+/* Union INSN with all insns containing uses reached from DEF.\n+   Detect whether DEF is live-out from the current function.  */\n+static void\n+union_uses (swap_web_entry *insn_entry, rtx insn, df_ref def)\n+{\n+  struct df_link *link = DF_REF_CHAIN (def);\n+\n+  if (!link)\n+    insn_entry[INSN_UID (insn)].is_live_out = 1;\n+\n+  while (link)\n+    {\n+      /* This could be an eh use or some other artificial use;\n+\t we treat these all the same (killing the optimization).  */\n+      if (DF_REF_IS_ARTIFICIAL (link->ref))\n+\tinsn_entry[INSN_UID (insn)].is_live_out = 1;\n+\n+      if (DF_REF_INSN_INFO (link->ref))\n+\t{\n+\t  rtx use_insn = DF_REF_INSN (link->ref);\n+\t  (void)unionfind_union (insn_entry + INSN_UID (insn),\n+\t\t\t\t insn_entry + INSN_UID (use_insn));\n+\t}\n+\n+      link = link->next;\n+    }\n+}\n+\n+/* Return 1 iff INSN is a load insn, including permuting loads that\n+   represent an lvxd2x instruction; else return 0.  */\n+static unsigned int\n+insn_is_load_p (rtx insn)\n+{\n+  rtx body = PATTERN (insn);\n+\n+  if (GET_CODE (body) == SET)\n+    {\n+      if (GET_CODE (SET_SRC (body)) == MEM)\n+\treturn 1;\n+\n+      if (GET_CODE (SET_SRC (body)) == VEC_SELECT\n+\t  && GET_CODE (XEXP (SET_SRC (body), 0)) == MEM)\n+\treturn 1;\n+\n+      return 0;\n+    }\n+\n+  if (GET_CODE (body) != PARALLEL)\n+    return 0;\n+\n+  rtx set = XVECEXP (body, 0, 0);\n+\n+  if (GET_CODE (set) == SET && GET_CODE (SET_SRC (set)) == MEM)\n+    return 1;\n+\n+  return 0;\n+}\n+\n+/* Return 1 iff INSN is a store insn, including permuting stores that\n+   represent an stvxd2x instruction; else return 0.  */\n+static unsigned int\n+insn_is_store_p (rtx insn)\n+{\n+  rtx body = PATTERN (insn);\n+  if (GET_CODE (body) == SET && GET_CODE (SET_DEST (body)) == MEM)\n+    return 1;\n+  if (GET_CODE (body) != PARALLEL)\n+    return 0;\n+  rtx set = XVECEXP (body, 0, 0);\n+  if (GET_CODE (set) == SET && GET_CODE (SET_DEST (set)) == MEM)\n+    return 1;\n+  return 0;\n+}\n+\n+/* Return 1 iff INSN swaps doublewords.  This may be a reg-reg swap,\n+   a permuting load, or a permuting store.  */\n+static unsigned int\n+insn_is_swap_p (rtx insn)\n+{\n+  rtx body = PATTERN (insn);\n+  if (GET_CODE (body) != SET)\n+    return 0;\n+  rtx rhs = SET_SRC (body);\n+  if (GET_CODE (rhs) != VEC_SELECT)\n+    return 0;\n+  rtx parallel = XEXP (rhs, 1);\n+  if (GET_CODE (parallel) != PARALLEL)\n+    return 0;\n+  unsigned int len = XVECLEN (parallel, 0);\n+  if (len != 2 && len != 4 && len != 8 && len != 16)\n+    return 0;\n+  for (unsigned int i = 0; i < len / 2; ++i)\n+    {\n+      rtx op = XVECEXP (parallel, 0, i);\n+      if (GET_CODE (op) != CONST_INT || INTVAL (op) != len / 2 + i)\n+\treturn 0;\n+    }\n+  for (unsigned int i = len / 2; i < len; ++i)\n+    {\n+      rtx op = XVECEXP (parallel, 0, i);\n+      if (GET_CODE (op) != CONST_INT || INTVAL (op) != i - len / 2)\n+\treturn 0;\n+    }\n+  return 1;\n+}\n+\n+/* Return TRUE if insn is a swap fed by a load from the constant pool.  */\n+static bool\n+const_load_sequence_p (swap_web_entry *insn_entry, rtx insn)\n+{\n+  unsigned uid = INSN_UID (insn);\n+  if (!insn_entry[uid].is_swap || insn_entry[uid].is_load)\n+    return false;\n+\n+  const_rtx tocrel_base;\n+\n+  /* Find the unique use in the swap and locate its def.  If the def\n+     isn't unique, punt.  */\n+  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+  df_ref use;\n+  FOR_EACH_INSN_INFO_USE (use, insn_info)\n+    {\n+      struct df_link *def_link = DF_REF_CHAIN (use);\n+      if (!def_link || def_link->next)\n+\treturn false;\n+\n+      rtx def_insn = DF_REF_INSN (def_link->ref);\n+      unsigned uid2 = INSN_UID (def_insn);\n+      if (!insn_entry[uid2].is_load || !insn_entry[uid2].is_swap)\n+\treturn false;\n+\n+      rtx body = PATTERN (def_insn);\n+      if (GET_CODE (body) != SET\n+\t  || GET_CODE (SET_SRC (body)) != VEC_SELECT\n+\t  || GET_CODE (XEXP (SET_SRC (body), 0)) != MEM)\n+\treturn false;\n+\n+      rtx mem = XEXP (SET_SRC (body), 0);\n+      rtx base_reg = XEXP (mem, 0);\n+\n+      df_ref base_use;\n+      insn_info = DF_INSN_INFO_GET (def_insn);\n+      FOR_EACH_INSN_INFO_USE (base_use, insn_info)\n+\t{\n+\t  if (!rtx_equal_p (DF_REF_REG (base_use), base_reg))\n+\t    continue;\n+\n+\t  struct df_link *base_def_link = DF_REF_CHAIN (base_use);\n+\t  if (!base_def_link || base_def_link->next)\n+\t    return false;\n+\n+\t  rtx tocrel_insn = DF_REF_INSN (base_def_link->ref);\n+\t  rtx tocrel_body = PATTERN (tocrel_insn);\n+\t  rtx base, offset;\n+\t  if (GET_CODE (tocrel_body) != SET)\n+\t    return false;\n+\t  /* There is an extra level of indirection for small/large\n+\t     code models.  */\n+\t  rtx tocrel_expr = SET_SRC (tocrel_body);\n+\t  if (GET_CODE (tocrel_expr) == MEM)\n+\t    tocrel_expr = XEXP (tocrel_expr, 0);\n+\t  if (!toc_relative_expr_p (tocrel_expr, false, &tocrel_base, NULL))\n+\t    return false;\n+\t  split_const (XVECEXP (tocrel_base, 0, 0), &base, &offset);\n+\t  if (GET_CODE (base) != SYMBOL_REF || !CONSTANT_POOL_ADDRESS_P (base))\n+\t    return false;\n+\t}\n+    }\n+  return true;\n+}\n+\n+/* Return TRUE iff OP matches a V2DF reduction pattern.  See the\n+   definition of vsx_reduc_<VEC_reduc_name>_v2df in vsx.md.  */\n+static bool\n+v2df_reduction_p (rtx op)\n+{\n+  if (GET_MODE (op) != V2DFmode)\n+    return false;\n+  \n+  enum rtx_code code = GET_CODE (op);\n+  if (code != PLUS && code != SMIN && code != SMAX)\n+    return false;\n+\n+  rtx concat = XEXP (op, 0);\n+  if (GET_CODE (concat) != VEC_CONCAT)\n+    return false;\n+\n+  rtx select0 = XEXP (concat, 0);\n+  rtx select1 = XEXP (concat, 1);\n+  if (GET_CODE (select0) != VEC_SELECT || GET_CODE (select1) != VEC_SELECT)\n+    return false;\n+\n+  rtx reg0 = XEXP (select0, 0);\n+  rtx reg1 = XEXP (select1, 0);\n+  if (!rtx_equal_p (reg0, reg1) || !REG_P (reg0))\n+    return false;\n+\n+  rtx parallel0 = XEXP (select0, 1);\n+  rtx parallel1 = XEXP (select1, 1);\n+  if (GET_CODE (parallel0) != PARALLEL || GET_CODE (parallel1) != PARALLEL)\n+    return false;\n+\n+  if (!rtx_equal_p (XVECEXP (parallel0, 0, 0), const1_rtx)\n+      || !rtx_equal_p (XVECEXP (parallel1, 0, 0), const0_rtx))\n+    return false;\n+\n+  return true;\n+}\n+\n+/* Return 1 iff OP is an operand that will not be affected by having\n+   vector doublewords swapped in memory.  */\n+static unsigned int\n+rtx_is_swappable_p (rtx op, unsigned int *special)\n+{\n+  enum rtx_code code = GET_CODE (op);\n+  int i, j;\n+  rtx parallel;\n+\n+  switch (code)\n+    {\n+    case LABEL_REF:\n+    case SYMBOL_REF:\n+    case CLOBBER:\n+    case REG:\n+      return 1;\n+\n+    case VEC_CONCAT:\n+    case ASM_INPUT:\n+    case ASM_OPERANDS:\n+      return 0;\n+\n+    case CONST_VECTOR:\n+      {\n+\t*special = SH_CONST_VECTOR;\n+\treturn 1;\n+      }\n+\n+    case VEC_DUPLICATE:\n+      /* Opportunity: If XEXP (op, 0) has the same mode as the result,\n+\t and XEXP (op, 1) is a PARALLEL with a single QImode const int,\n+\t it represents a vector splat for which we can do special\n+\t handling.  */\n+      if (GET_CODE (XEXP (op, 0)) == CONST_INT)\n+\treturn 1;\n+      else if (REG_P (XEXP (op, 0))\n+\t       && GET_MODE_INNER (GET_MODE (op)) == GET_MODE (XEXP (op, 0)))\n+\t/* This catches V2DF and V2DI splat, at a minimum.  */\n+\treturn 1;\n+      else if (GET_CODE (XEXP (op, 0)) == TRUNCATE\n+\t       && REG_P (XEXP (XEXP (op, 0), 0))\n+\t       && GET_MODE_INNER (GET_MODE (op)) == GET_MODE (XEXP (op, 0)))\n+\t/* This catches splat of a truncated value.  */\n+\treturn 1;\n+      else if (GET_CODE (XEXP (op, 0)) == VEC_SELECT)\n+\t/* If the duplicated item is from a select, defer to the select\n+\t   processing to see if we can change the lane for the splat.  */\n+\treturn rtx_is_swappable_p (XEXP (op, 0), special);\n+      else\n+\treturn 0;\n+\n+    case VEC_SELECT:\n+      /* A vec_extract operation is ok if we change the lane.  */\n+      if (GET_CODE (XEXP (op, 0)) == REG\n+\t  && GET_MODE_INNER (GET_MODE (XEXP (op, 0))) == GET_MODE (op)\n+\t  && GET_CODE ((parallel = XEXP (op, 1))) == PARALLEL\n+\t  && XVECLEN (parallel, 0) == 1\n+\t  && GET_CODE (XVECEXP (parallel, 0, 0)) == CONST_INT)\n+\t{\n+\t  *special = SH_EXTRACT;\n+\t  return 1;\n+\t}\n+      /* An XXPERMDI is ok if we adjust the lanes.  Note that if the\n+\t XXPERMDI is a swap operation, it will be identified by\n+\t insn_is_swap_p and therefore we won't get here.  */\n+      else if (GET_CODE (XEXP (op, 0)) == VEC_CONCAT\n+\t       && (GET_MODE (XEXP (op, 0)) == V4DFmode\n+\t\t   || GET_MODE (XEXP (op, 0)) == V4DImode)\n+\t       && GET_CODE ((parallel = XEXP (op, 1))) == PARALLEL\n+\t       && XVECLEN (parallel, 0) == 2\n+\t       && GET_CODE (XVECEXP (parallel, 0, 0)) == CONST_INT\n+\t       && GET_CODE (XVECEXP (parallel, 0, 1)) == CONST_INT)\n+\t{\n+\t  *special = SH_XXPERMDI;\n+\t  return 1;\n+\t}\n+      else if (v2df_reduction_p (op))\n+\treturn 1;\n+      else\n+\treturn 0;\n+\n+    case UNSPEC:\n+      {\n+\t/* Various operations are unsafe for this optimization, at least\n+\t   without significant additional work.  Permutes are obviously\n+\t   problematic, as both the permute control vector and the ordering\n+\t   of the target values are invalidated by doubleword swapping.\n+\t   Vector pack and unpack modify the number of vector lanes.\n+\t   Merge-high/low will not operate correctly on swapped operands.\n+\t   Vector shifts across element boundaries are clearly uncool,\n+\t   as are vector select and concatenate operations.  Vector\n+\t   sum-across instructions define one operand with a specific\n+\t   order-dependent element, so additional fixup code would be\n+\t   needed to make those work.  Vector set and non-immediate-form\n+\t   vector splat are element-order sensitive.  A few of these\n+\t   cases might be workable with special handling if required.\n+\t   Adding cost modeling would be appropriate in some cases.  */\n+\tint val = XINT (op, 1);\n+\tswitch (val)\n+\t  {\n+\t  default:\n+\t    break;\n+\t  case UNSPEC_VMRGH_DIRECT:\n+\t  case UNSPEC_VMRGL_DIRECT:\n+\t  case UNSPEC_VPACK_SIGN_SIGN_SAT:\n+\t  case UNSPEC_VPACK_SIGN_UNS_SAT:\n+\t  case UNSPEC_VPACK_UNS_UNS_MOD:\n+\t  case UNSPEC_VPACK_UNS_UNS_MOD_DIRECT:\n+\t  case UNSPEC_VPACK_UNS_UNS_SAT:\n+\t  case UNSPEC_VPERM:\n+\t  case UNSPEC_VPERM_UNS:\n+\t  case UNSPEC_VPERMHI:\n+\t  case UNSPEC_VPERMSI:\n+\t  case UNSPEC_VPKPX:\n+\t  case UNSPEC_VSLDOI:\n+\t  case UNSPEC_VSLO:\n+\t  case UNSPEC_VSRO:\n+\t  case UNSPEC_VSUM2SWS:\n+\t  case UNSPEC_VSUM4S:\n+\t  case UNSPEC_VSUM4UBS:\n+\t  case UNSPEC_VSUMSWS:\n+\t  case UNSPEC_VSUMSWS_DIRECT:\n+\t  case UNSPEC_VSX_CONCAT:\n+\t  case UNSPEC_VSX_SET:\n+\t  case UNSPEC_VSX_SLDWI:\n+\t  case UNSPEC_VUNPACK_HI_SIGN:\n+\t  case UNSPEC_VUNPACK_HI_SIGN_DIRECT:\n+\t  case UNSPEC_VUNPACK_LO_SIGN:\n+\t  case UNSPEC_VUNPACK_LO_SIGN_DIRECT:\n+\t  case UNSPEC_VUPKHPX:\n+\t  case UNSPEC_VUPKHS_V4SF:\n+\t  case UNSPEC_VUPKHU_V4SF:\n+\t  case UNSPEC_VUPKLPX:\n+\t  case UNSPEC_VUPKLS_V4SF:\n+\t  case UNSPEC_VUPKLU_V4SF:\n+\t  case UNSPEC_VSX_CVDPSPN:\n+\t  case UNSPEC_VSX_CVSPDP:\n+\t  case UNSPEC_VSX_CVSPDPN:\n+\t  case UNSPEC_VSX_EXTRACT:\n+\t  case UNSPEC_VSX_VSLO:\n+\t  case UNSPEC_VSX_VEC_INIT:\n+\t    return 0;\n+\t  case UNSPEC_VSPLT_DIRECT:\n+\t  case UNSPEC_VSX_XXSPLTD:\n+\t    *special = SH_SPLAT;\n+\t    return 1;\n+\t  case UNSPEC_REDUC_PLUS:\n+\t  case UNSPEC_REDUC:\n+\t    return 1;\n+\t  }\n+      }\n+\n+    default:\n+      break;\n+    }\n+\n+  const char *fmt = GET_RTX_FORMAT (code);\n+  int ok = 1;\n+\n+  for (i = 0; i < GET_RTX_LENGTH (code); ++i)\n+    if (fmt[i] == 'e' || fmt[i] == 'u')\n+      {\n+\tunsigned int special_op = SH_NONE;\n+\tok &= rtx_is_swappable_p (XEXP (op, i), &special_op);\n+\tif (special_op == SH_NONE)\n+\t  continue;\n+\t/* Ensure we never have two kinds of special handling\n+\t   for the same insn.  */\n+\tif (*special != SH_NONE && *special != special_op)\n+\t  return 0;\n+\t*special = special_op;\n+      }\n+    else if (fmt[i] == 'E')\n+      for (j = 0; j < XVECLEN (op, i); ++j)\n+\t{\n+\t  unsigned int special_op = SH_NONE;\n+\t  ok &= rtx_is_swappable_p (XVECEXP (op, i, j), &special_op);\n+\t  if (special_op == SH_NONE)\n+\t    continue;\n+\t  /* Ensure we never have two kinds of special handling\n+\t     for the same insn.  */\n+\t  if (*special != SH_NONE && *special != special_op)\n+\t    return 0;\n+\t  *special = special_op;\n+\t}\n+\n+  return ok;\n+}\n+\n+/* Return 1 iff INSN is an operand that will not be affected by\n+   having vector doublewords swapped in memory (in which case\n+   *SPECIAL is unchanged), or that can be modified to be correct\n+   if vector doublewords are swapped in memory (in which case\n+   *SPECIAL is changed to a value indicating how).  */\n+static unsigned int\n+insn_is_swappable_p (swap_web_entry *insn_entry, rtx insn,\n+\t\t     unsigned int *special)\n+{\n+  /* Calls are always bad.  */\n+  if (GET_CODE (insn) == CALL_INSN)\n+    return 0;\n+\n+  /* Loads and stores seen here are not permuting, but we can still\n+     fix them up by converting them to permuting ones.  Exceptions:\n+     UNSPEC_LVE, UNSPEC_LVX, and UNSPEC_STVX, which have a PARALLEL\n+     body instead of a SET; and UNSPEC_STVE, which has an UNSPEC\n+     for the SET source.  Also we must now make an exception for lvx\n+     and stvx when they are not in the UNSPEC_LVX/STVX form (with the\n+     explicit \"& -16\") since this leads to unrecognizable insns.  */\n+  rtx body = PATTERN (insn);\n+  int i = INSN_UID (insn);\n+\n+  if (insn_entry[i].is_load)\n+    {\n+      if (GET_CODE (body) == SET)\n+\t{\n+\t  rtx rhs = SET_SRC (body);\n+\t  /* Even without a swap, the RHS might be a vec_select for, say,\n+\t     a byte-reversing load.  */\n+\t  if (GET_CODE (rhs) != MEM)\n+\t    return 0;\n+\t  if (GET_CODE (XEXP (rhs, 0)) == AND)\n+\t    return 0;\n+\n+\t  *special = SH_NOSWAP_LD;\n+\t  return 1;\n+\t}\n+      else\n+\treturn 0;\n+    }\n+\n+  if (insn_entry[i].is_store)\n+    {\n+      if (GET_CODE (body) == SET\n+\t  && GET_CODE (SET_SRC (body)) != UNSPEC)\n+\t{\n+\t  rtx lhs = SET_DEST (body);\n+\t  /* Even without a swap, the LHS might be a vec_select for, say,\n+\t     a byte-reversing store.  */\n+\t  if (GET_CODE (lhs) != MEM)\n+\t    return 0;\n+\t  if (GET_CODE (XEXP (lhs, 0)) == AND)\n+\t    return 0;\n+\t  \n+\t  *special = SH_NOSWAP_ST;\n+\t  return 1;\n+\t}\n+      else\n+\treturn 0;\n+    }\n+\n+  /* A convert to single precision can be left as is provided that\n+     all of its uses are in xxspltw instructions that splat BE element\n+     zero.  */\n+  if (GET_CODE (body) == SET\n+      && GET_CODE (SET_SRC (body)) == UNSPEC\n+      && XINT (SET_SRC (body), 1) == UNSPEC_VSX_CVDPSPN)\n+    {\n+      df_ref def;\n+      struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+\n+      FOR_EACH_INSN_INFO_DEF (def, insn_info)\n+\t{\n+\t  struct df_link *link = DF_REF_CHAIN (def);\n+\t  if (!link)\n+\t    return 0;\n+\n+\t  for (; link; link = link->next) {\n+\t    rtx use_insn = DF_REF_INSN (link->ref);\n+\t    rtx use_body = PATTERN (use_insn);\n+\t    if (GET_CODE (use_body) != SET\n+\t\t|| GET_CODE (SET_SRC (use_body)) != UNSPEC\n+\t\t|| XINT (SET_SRC (use_body), 1) != UNSPEC_VSX_XXSPLTW\n+\t\t|| XVECEXP (SET_SRC (use_body), 0, 1) != const0_rtx)\n+\t      return 0;\n+\t  }\n+\t}\n+\n+      return 1;\n+    }\n+\n+  /* A concatenation of two doublewords is ok if we reverse the\n+     order of the inputs.  */\n+  if (GET_CODE (body) == SET\n+      && GET_CODE (SET_SRC (body)) == VEC_CONCAT\n+      && (GET_MODE (SET_SRC (body)) == V2DFmode\n+\t  || GET_MODE (SET_SRC (body)) == V2DImode))\n+    {\n+      *special = SH_CONCAT;\n+      return 1;\n+    }\n+\n+  /* V2DF reductions are always swappable.  */\n+  if (GET_CODE (body) == PARALLEL)\n+    {\n+      rtx expr = XVECEXP (body, 0, 0);\n+      if (GET_CODE (expr) == SET\n+\t  && v2df_reduction_p (SET_SRC (expr)))\n+\treturn 1;\n+    }\n+\n+  /* An UNSPEC_VPERM is ok if the mask operand is loaded from the\n+     constant pool.  */\n+  if (GET_CODE (body) == SET\n+      && GET_CODE (SET_SRC (body)) == UNSPEC\n+      && XINT (SET_SRC (body), 1) == UNSPEC_VPERM\n+      && XVECLEN (SET_SRC (body), 0) == 3\n+      && GET_CODE (XVECEXP (SET_SRC (body), 0, 2)) == REG)\n+    {\n+      rtx mask_reg = XVECEXP (SET_SRC (body), 0, 2);\n+      struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+      df_ref use;\n+      FOR_EACH_INSN_INFO_USE (use, insn_info)\n+\tif (rtx_equal_p (DF_REF_REG (use), mask_reg))\n+\t  {\n+\t    struct df_link *def_link = DF_REF_CHAIN (use);\n+\t    /* Punt if multiple definitions for this reg.  */\n+\t    if (def_link && !def_link->next &&\n+\t\tconst_load_sequence_p (insn_entry,\n+\t\t\t\t       DF_REF_INSN (def_link->ref)))\n+\t      {\n+\t\t*special = SH_VPERM;\n+\t\treturn 1;\n+\t      }\n+\t  }\n+    }\n+\n+  /* Otherwise check the operands for vector lane violations.  */\n+  return rtx_is_swappable_p (body, special);\n+}\n+\n+enum chain_purpose { FOR_LOADS, FOR_STORES };\n+\n+/* Return true if the UD or DU chain headed by LINK is non-empty,\n+   and every entry on the chain references an insn that is a\n+   register swap.  Furthermore, if PURPOSE is FOR_LOADS, each such\n+   register swap must have only permuting loads as reaching defs.\n+   If PURPOSE is FOR_STORES, each such register swap must have only\n+   register swaps or permuting stores as reached uses.  */\n+static bool\n+chain_contains_only_swaps (swap_web_entry *insn_entry, struct df_link *link,\n+\t\t\t   enum chain_purpose purpose)\n+{\n+  if (!link)\n+    return false;\n+\n+  for (; link; link = link->next)\n+    {\n+      if (!ALTIVEC_OR_VSX_VECTOR_MODE (GET_MODE (DF_REF_REG (link->ref))))\n+\tcontinue;\n+\n+      if (DF_REF_IS_ARTIFICIAL (link->ref))\n+\treturn false;\n+\n+      rtx reached_insn = DF_REF_INSN (link->ref);\n+      unsigned uid = INSN_UID (reached_insn);\n+      struct df_insn_info *insn_info = DF_INSN_INFO_GET (reached_insn);\n+\n+      if (!insn_entry[uid].is_swap || insn_entry[uid].is_load\n+\t  || insn_entry[uid].is_store)\n+\treturn false;\n+\n+      if (purpose == FOR_LOADS)\n+\t{\n+\t  df_ref use;\n+\t  FOR_EACH_INSN_INFO_USE (use, insn_info)\n+\t    {\n+\t      struct df_link *swap_link = DF_REF_CHAIN (use);\n+\n+\t      while (swap_link)\n+\t\t{\n+\t\t  if (DF_REF_IS_ARTIFICIAL (link->ref))\n+\t\t    return false;\n+\n+\t\t  rtx swap_def_insn = DF_REF_INSN (swap_link->ref);\n+\t\t  unsigned uid2 = INSN_UID (swap_def_insn);\n+\n+\t\t  /* Only permuting loads are allowed.  */\n+\t\t  if (!insn_entry[uid2].is_swap || !insn_entry[uid2].is_load)\n+\t\t    return false;\n+\n+\t\t  swap_link = swap_link->next;\n+\t\t}\n+\t    }\n+\t}\n+      else if (purpose == FOR_STORES)\n+\t{\n+\t  df_ref def;\n+\t  FOR_EACH_INSN_INFO_DEF (def, insn_info)\n+\t    {\n+\t      struct df_link *swap_link = DF_REF_CHAIN (def);\n+\n+\t      while (swap_link)\n+\t\t{\n+\t\t  if (DF_REF_IS_ARTIFICIAL (link->ref))\n+\t\t    return false;\n+\n+\t\t  rtx swap_use_insn = DF_REF_INSN (swap_link->ref);\n+\t\t  unsigned uid2 = INSN_UID (swap_use_insn);\n+\n+\t\t  /* Permuting stores or register swaps are allowed.  */\n+\t\t  if (!insn_entry[uid2].is_swap || insn_entry[uid2].is_load)\n+\t\t    return false;\n+\n+\t\t  swap_link = swap_link->next;\n+\t\t}\n+\t    }\n+\t}\n+    }\n+\n+  return true;\n+}\n+\n+/* Mark the xxswapdi instructions associated with permuting loads and\n+   stores for removal.  Note that we only flag them for deletion here,\n+   as there is a possibility of a swap being reached from multiple\n+   loads, etc.  */\n+static void\n+mark_swaps_for_removal (swap_web_entry *insn_entry, unsigned int i)\n+{\n+  rtx insn = insn_entry[i].insn;\n+  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+\n+  if (insn_entry[i].is_load)\n+    {\n+      df_ref def;\n+      FOR_EACH_INSN_INFO_DEF (def, insn_info)\n+\t{\n+\t  struct df_link *link = DF_REF_CHAIN (def);\n+\n+\t  /* We know by now that these are swaps, so we can delete\n+\t     them confidently.  */\n+\t  while (link)\n+\t    {\n+\t      rtx use_insn = DF_REF_INSN (link->ref);\n+\t      insn_entry[INSN_UID (use_insn)].will_delete = 1;\n+\t      link = link->next;\n+\t    }\n+\t}\n+    }\n+  else if (insn_entry[i].is_store)\n+    {\n+      df_ref use;\n+      FOR_EACH_INSN_INFO_USE (use, insn_info)\n+\t{\n+\t  /* Ignore uses for addressability.  */\n+\t  machine_mode mode = GET_MODE (DF_REF_REG (use));\n+\t  if (!ALTIVEC_OR_VSX_VECTOR_MODE (mode))\n+\t    continue;\n+\n+\t  struct df_link *link = DF_REF_CHAIN (use);\n+\n+\t  /* We know by now that these are swaps, so we can delete\n+\t     them confidently.  */\n+\t  while (link)\n+\t    {\n+\t      rtx def_insn = DF_REF_INSN (link->ref);\n+\t      insn_entry[INSN_UID (def_insn)].will_delete = 1;\n+\t      link = link->next;\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* OP is either a CONST_VECTOR or an expression containing one.\n+   Swap the first half of the vector with the second in the first\n+   case.  Recurse to find it in the second.  */\n+static void\n+swap_const_vector_halves (rtx op)\n+{\n+  int i;\n+  enum rtx_code code = GET_CODE (op);\n+  if (GET_CODE (op) == CONST_VECTOR)\n+    {\n+      int half_units = GET_MODE_NUNITS (GET_MODE (op)) / 2;\n+      for (i = 0; i < half_units; ++i)\n+\t{\n+\t  rtx temp = CONST_VECTOR_ELT (op, i);\n+\t  CONST_VECTOR_ELT (op, i) = CONST_VECTOR_ELT (op, i + half_units);\n+\t  CONST_VECTOR_ELT (op, i + half_units) = temp;\n+\t}\n+    }\n+  else\n+    {\n+      int j;\n+      const char *fmt = GET_RTX_FORMAT (code);\n+      for (i = 0; i < GET_RTX_LENGTH (code); ++i)\n+\tif (fmt[i] == 'e' || fmt[i] == 'u')\n+\t  swap_const_vector_halves (XEXP (op, i));\n+\telse if (fmt[i] == 'E')\n+\t  for (j = 0; j < XVECLEN (op, i); ++j)\n+\t    swap_const_vector_halves (XVECEXP (op, i, j));\n+    }\n+}\n+\n+/* Find all subregs of a vector expression that perform a narrowing,\n+   and adjust the subreg index to account for doubleword swapping.  */\n+static void\n+adjust_subreg_index (rtx op)\n+{\n+  enum rtx_code code = GET_CODE (op);\n+  if (code == SUBREG\n+      && (GET_MODE_SIZE (GET_MODE (op))\n+\t  < GET_MODE_SIZE (GET_MODE (XEXP (op, 0)))))\n+    {\n+      unsigned int index = SUBREG_BYTE (op);\n+      if (index < 8)\n+\tindex += 8;\n+      else\n+\tindex -= 8;\n+      SUBREG_BYTE (op) = index;\n+    }\n+\n+  const char *fmt = GET_RTX_FORMAT (code);\n+  int i,j;\n+  for (i = 0; i < GET_RTX_LENGTH (code); ++i)\n+    if (fmt[i] == 'e' || fmt[i] == 'u')\n+      adjust_subreg_index (XEXP (op, i));\n+    else if (fmt[i] == 'E')\n+      for (j = 0; j < XVECLEN (op, i); ++j)\n+\tadjust_subreg_index (XVECEXP (op, i, j));\n+}\n+\n+/* Convert the non-permuting load INSN to a permuting one.  */\n+static void\n+permute_load (rtx_insn *insn)\n+{\n+  rtx body = PATTERN (insn);\n+  rtx mem_op = SET_SRC (body);\n+  rtx tgt_reg = SET_DEST (body);\n+  machine_mode mode = GET_MODE (tgt_reg);\n+  int n_elts = GET_MODE_NUNITS (mode);\n+  int half_elts = n_elts / 2;\n+  rtx par = gen_rtx_PARALLEL (mode, rtvec_alloc (n_elts));\n+  int i, j;\n+  for (i = 0, j = half_elts; i < half_elts; ++i, ++j)\n+    XVECEXP (par, 0, i) = GEN_INT (j);\n+  for (i = half_elts, j = 0; j < half_elts; ++i, ++j)\n+    XVECEXP (par, 0, i) = GEN_INT (j);\n+  rtx sel = gen_rtx_VEC_SELECT (mode, mem_op, par);\n+  SET_SRC (body) = sel;\n+  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n+  df_insn_rescan (insn);\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"Replacing load %d with permuted load\\n\",\n+\t     INSN_UID (insn));\n+}\n+\n+/* Convert the non-permuting store INSN to a permuting one.  */\n+static void\n+permute_store (rtx_insn *insn)\n+{\n+  rtx body = PATTERN (insn);\n+  rtx src_reg = SET_SRC (body);\n+  machine_mode mode = GET_MODE (src_reg);\n+  int n_elts = GET_MODE_NUNITS (mode);\n+  int half_elts = n_elts / 2;\n+  rtx par = gen_rtx_PARALLEL (mode, rtvec_alloc (n_elts));\n+  int i, j;\n+  for (i = 0, j = half_elts; i < half_elts; ++i, ++j)\n+    XVECEXP (par, 0, i) = GEN_INT (j);\n+  for (i = half_elts, j = 0; j < half_elts; ++i, ++j)\n+    XVECEXP (par, 0, i) = GEN_INT (j);\n+  rtx sel = gen_rtx_VEC_SELECT (mode, src_reg, par);\n+  SET_SRC (body) = sel;\n+  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n+  df_insn_rescan (insn);\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"Replacing store %d with permuted store\\n\",\n+\t     INSN_UID (insn));\n+}\n+\n+/* Given OP that contains a vector extract operation, adjust the index\n+   of the extracted lane to account for the doubleword swap.  */\n+static void\n+adjust_extract (rtx_insn *insn)\n+{\n+  rtx pattern = PATTERN (insn);\n+  if (GET_CODE (pattern) == PARALLEL)\n+    pattern = XVECEXP (pattern, 0, 0);\n+  rtx src = SET_SRC (pattern);\n+  /* The vec_select may be wrapped in a vec_duplicate for a splat, so\n+     account for that.  */\n+  rtx sel = GET_CODE (src) == VEC_DUPLICATE ? XEXP (src, 0) : src;\n+  rtx par = XEXP (sel, 1);\n+  int half_elts = GET_MODE_NUNITS (GET_MODE (XEXP (sel, 0))) >> 1;\n+  int lane = INTVAL (XVECEXP (par, 0, 0));\n+  lane = lane >= half_elts ? lane - half_elts : lane + half_elts;\n+  XVECEXP (par, 0, 0) = GEN_INT (lane);\n+  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n+  df_insn_rescan (insn);\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"Changing lane for extract %d\\n\", INSN_UID (insn));\n+}\n+\n+/* Given OP that contains a vector direct-splat operation, adjust the index\n+   of the source lane to account for the doubleword swap.  */\n+static void\n+adjust_splat (rtx_insn *insn)\n+{\n+  rtx body = PATTERN (insn);\n+  rtx unspec = XEXP (body, 1);\n+  int half_elts = GET_MODE_NUNITS (GET_MODE (unspec)) >> 1;\n+  int lane = INTVAL (XVECEXP (unspec, 0, 1));\n+  lane = lane >= half_elts ? lane - half_elts : lane + half_elts;\n+  XVECEXP (unspec, 0, 1) = GEN_INT (lane);\n+  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n+  df_insn_rescan (insn);\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"Changing lane for splat %d\\n\", INSN_UID (insn));\n+}\n+\n+/* Given OP that contains an XXPERMDI operation (that is not a doubleword\n+   swap), reverse the order of the source operands and adjust the indices\n+   of the source lanes to account for doubleword reversal.  */\n+static void\n+adjust_xxpermdi (rtx_insn *insn)\n+{\n+  rtx set = PATTERN (insn);\n+  rtx select = XEXP (set, 1);\n+  rtx concat = XEXP (select, 0);\n+  rtx src0 = XEXP (concat, 0);\n+  XEXP (concat, 0) = XEXP (concat, 1);\n+  XEXP (concat, 1) = src0;\n+  rtx parallel = XEXP (select, 1);\n+  int lane0 = INTVAL (XVECEXP (parallel, 0, 0));\n+  int lane1 = INTVAL (XVECEXP (parallel, 0, 1));\n+  int new_lane0 = 3 - lane1;\n+  int new_lane1 = 3 - lane0;\n+  XVECEXP (parallel, 0, 0) = GEN_INT (new_lane0);\n+  XVECEXP (parallel, 0, 1) = GEN_INT (new_lane1);\n+  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n+  df_insn_rescan (insn);\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"Changing lanes for xxpermdi %d\\n\", INSN_UID (insn));\n+}\n+\n+/* Given OP that contains a VEC_CONCAT operation of two doublewords,\n+   reverse the order of those inputs.  */\n+static void\n+adjust_concat (rtx_insn *insn)\n+{\n+  rtx set = PATTERN (insn);\n+  rtx concat = XEXP (set, 1);\n+  rtx src0 = XEXP (concat, 0);\n+  XEXP (concat, 0) = XEXP (concat, 1);\n+  XEXP (concat, 1) = src0;\n+  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n+  df_insn_rescan (insn);\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"Reversing inputs for concat %d\\n\", INSN_UID (insn));\n+}\n+\n+/* Given an UNSPEC_VPERM insn, modify the mask loaded from the\n+   constant pool to reflect swapped doublewords.  */\n+static void\n+adjust_vperm (rtx_insn *insn)\n+{\n+  /* We previously determined that the UNSPEC_VPERM was fed by a\n+     swap of a swapping load of a TOC-relative constant pool symbol.\n+     Find the MEM in the swapping load and replace it with a MEM for\n+     the adjusted mask constant.  */\n+  rtx set = PATTERN (insn);\n+  rtx mask_reg = XVECEXP (SET_SRC (set), 0, 2);\n+\n+  /* Find the swap.  */\n+  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+  df_ref use;\n+  rtx_insn *swap_insn = 0;\n+  FOR_EACH_INSN_INFO_USE (use, insn_info)\n+    if (rtx_equal_p (DF_REF_REG (use), mask_reg))\n+      {\n+\tstruct df_link *def_link = DF_REF_CHAIN (use);\n+\tgcc_assert (def_link && !def_link->next);\n+\tswap_insn = DF_REF_INSN (def_link->ref);\n+\tbreak;\n+      }\n+  gcc_assert (swap_insn);\n+  \n+  /* Find the load.  */\n+  insn_info = DF_INSN_INFO_GET (swap_insn);\n+  rtx_insn *load_insn = 0;\n+  FOR_EACH_INSN_INFO_USE (use, insn_info)\n+    {\n+      struct df_link *def_link = DF_REF_CHAIN (use);\n+      gcc_assert (def_link && !def_link->next);\n+      load_insn = DF_REF_INSN (def_link->ref);\n+      break;\n+    }\n+  gcc_assert (load_insn);\n+\n+  /* Find the TOC-relative symbol access.  */\n+  insn_info = DF_INSN_INFO_GET (load_insn);\n+  rtx_insn *tocrel_insn = 0;\n+  FOR_EACH_INSN_INFO_USE (use, insn_info)\n+    {\n+      struct df_link *def_link = DF_REF_CHAIN (use);\n+      gcc_assert (def_link && !def_link->next);\n+      tocrel_insn = DF_REF_INSN (def_link->ref);\n+      break;\n+    }\n+  gcc_assert (tocrel_insn);\n+\n+  /* Find the embedded CONST_VECTOR.  We have to call toc_relative_expr_p\n+     to set tocrel_base; otherwise it would be unnecessary as we've\n+     already established it will return true.  */\n+  rtx base, offset;\n+  const_rtx tocrel_base;\n+  rtx tocrel_expr = SET_SRC (PATTERN (tocrel_insn));\n+  /* There is an extra level of indirection for small/large code models.  */\n+  if (GET_CODE (tocrel_expr) == MEM)\n+    tocrel_expr = XEXP (tocrel_expr, 0);\n+  if (!toc_relative_expr_p (tocrel_expr, false, &tocrel_base, NULL))\n+    gcc_unreachable ();\n+  split_const (XVECEXP (tocrel_base, 0, 0), &base, &offset);\n+  rtx const_vector = get_pool_constant (base);\n+  /* With the extra indirection, get_pool_constant will produce the\n+     real constant from the reg_equal expression, so get the real\n+     constant.  */\n+  if (GET_CODE (const_vector) == SYMBOL_REF)\n+    const_vector = get_pool_constant (const_vector);\n+  gcc_assert (GET_CODE (const_vector) == CONST_VECTOR);\n+\n+  /* Create an adjusted mask from the initial mask.  */\n+  unsigned int new_mask[16], i, val;\n+  for (i = 0; i < 16; ++i) {\n+    val = INTVAL (XVECEXP (const_vector, 0, i));\n+    if (val < 16)\n+      new_mask[i] = (val + 8) % 16;\n+    else\n+      new_mask[i] = ((val + 8) % 16) + 16;\n+  }\n+\n+  /* Create a new CONST_VECTOR and a MEM that references it.  */\n+  rtx vals = gen_rtx_PARALLEL (V16QImode, rtvec_alloc (16));\n+  for (i = 0; i < 16; ++i)\n+    XVECEXP (vals, 0, i) = GEN_INT (new_mask[i]);\n+  rtx new_const_vector = gen_rtx_CONST_VECTOR (V16QImode, XVEC (vals, 0));\n+  rtx new_mem = force_const_mem (V16QImode, new_const_vector);\n+  /* This gives us a MEM whose base operand is a SYMBOL_REF, which we\n+     can't recognize.  Force the SYMBOL_REF into a register.  */\n+  if (!REG_P (XEXP (new_mem, 0))) {\n+    rtx base_reg = force_reg (Pmode, XEXP (new_mem, 0));\n+    XEXP (new_mem, 0) = base_reg;\n+    /* Move the newly created insn ahead of the load insn.  */\n+    rtx_insn *force_insn = get_last_insn ();\n+    remove_insn (force_insn);\n+    rtx_insn *before_load_insn = PREV_INSN (load_insn);\n+    add_insn_after (force_insn, before_load_insn, BLOCK_FOR_INSN (load_insn));\n+    df_insn_rescan (before_load_insn);\n+    df_insn_rescan (force_insn);\n+  }\n+\n+  /* Replace the MEM in the load instruction and rescan it.  */\n+  XEXP (SET_SRC (PATTERN (load_insn)), 0) = new_mem;\n+  INSN_CODE (load_insn) = -1; /* Force re-recognition.  */\n+  df_insn_rescan (load_insn);\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"Adjusting mask for vperm %d\\n\", INSN_UID (insn));\n+}\n+\n+/* The insn described by INSN_ENTRY[I] can be swapped, but only\n+   with special handling.  Take care of that here.  */\n+static void\n+handle_special_swappables (swap_web_entry *insn_entry, unsigned i)\n+{\n+  rtx_insn *insn = insn_entry[i].insn;\n+  rtx body = PATTERN (insn);\n+\n+  switch (insn_entry[i].special_handling)\n+    {\n+    default:\n+      gcc_unreachable ();\n+    case SH_CONST_VECTOR:\n+      {\n+\t/* A CONST_VECTOR will only show up somewhere in the RHS of a SET.  */\n+\tgcc_assert (GET_CODE (body) == SET);\n+\trtx rhs = SET_SRC (body);\n+\tswap_const_vector_halves (rhs);\n+\tif (dump_file)\n+\t  fprintf (dump_file, \"Swapping constant halves in insn %d\\n\", i);\n+\tbreak;\n+      }\n+    case SH_SUBREG:\n+      /* A subreg of the same size is already safe.  For subregs that\n+\t select a smaller portion of a reg, adjust the index for\n+\t swapped doublewords.  */\n+      adjust_subreg_index (body);\n+      if (dump_file)\n+\tfprintf (dump_file, \"Adjusting subreg in insn %d\\n\", i);\n+      break;\n+    case SH_NOSWAP_LD:\n+      /* Convert a non-permuting load to a permuting one.  */\n+      permute_load (insn);\n+      break;\n+    case SH_NOSWAP_ST:\n+      /* Convert a non-permuting store to a permuting one.  */\n+      permute_store (insn);\n+      break;\n+    case SH_EXTRACT:\n+      /* Change the lane on an extract operation.  */\n+      adjust_extract (insn);\n+      break;\n+    case SH_SPLAT:\n+      /* Change the lane on a direct-splat operation.  */\n+      adjust_splat (insn);\n+      break;\n+    case SH_XXPERMDI:\n+      /* Change the lanes on an XXPERMDI operation.  */\n+      adjust_xxpermdi (insn);\n+      break;\n+    case SH_CONCAT:\n+      /* Reverse the order of a concatenation operation.  */\n+      adjust_concat (insn);\n+      break;\n+    case SH_VPERM:\n+      /* Change the mask loaded from the constant pool for a VPERM.  */\n+      adjust_vperm (insn);\n+      break;\n+    }\n+}\n+\n+/* Find the insn from the Ith table entry, which is known to be a\n+   register swap Y = SWAP(X).  Replace it with a copy Y = X.  */\n+static void\n+replace_swap_with_copy (swap_web_entry *insn_entry, unsigned i)\n+{\n+  rtx_insn *insn = insn_entry[i].insn;\n+  rtx body = PATTERN (insn);\n+  rtx src_reg = XEXP (SET_SRC (body), 0);\n+  rtx copy = gen_rtx_SET (SET_DEST (body), src_reg);\n+  rtx_insn *new_insn = emit_insn_before (copy, insn);\n+  set_block_for_insn (new_insn, BLOCK_FOR_INSN (insn));\n+  df_insn_rescan (new_insn);\n+\n+  if (dump_file)\n+    {\n+      unsigned int new_uid = INSN_UID (new_insn);\n+      fprintf (dump_file, \"Replacing swap %d with copy %d\\n\", i, new_uid);\n+    }\n+\n+  df_insn_delete (insn);\n+  remove_insn (insn);\n+  insn->set_deleted ();\n+}\n+\n+/* Dump the swap table to DUMP_FILE.  */\n+static void\n+dump_swap_insn_table (swap_web_entry *insn_entry)\n+{\n+  int e = get_max_uid ();\n+  fprintf (dump_file, \"\\nRelevant insns with their flag settings\\n\\n\");\n+\n+  for (int i = 0; i < e; ++i)\n+    if (insn_entry[i].is_relevant)\n+      {\n+\tswap_web_entry *pred_entry = (swap_web_entry *)insn_entry[i].pred ();\n+\tfprintf (dump_file, \"%6d %6d  \", i,\n+\t\t pred_entry && pred_entry->insn\n+\t\t ? INSN_UID (pred_entry->insn) : 0);\n+\tif (insn_entry[i].is_load)\n+\t  fputs (\"load \", dump_file);\n+\tif (insn_entry[i].is_store)\n+\t  fputs (\"store \", dump_file);\n+\tif (insn_entry[i].is_swap)\n+\t  fputs (\"swap \", dump_file);\n+\tif (insn_entry[i].is_live_in)\n+\t  fputs (\"live-in \", dump_file);\n+\tif (insn_entry[i].is_live_out)\n+\t  fputs (\"live-out \", dump_file);\n+\tif (insn_entry[i].contains_subreg)\n+\t  fputs (\"subreg \", dump_file);\n+\tif (insn_entry[i].is_128_int)\n+\t  fputs (\"int128 \", dump_file);\n+\tif (insn_entry[i].is_call)\n+\t  fputs (\"call \", dump_file);\n+\tif (insn_entry[i].is_swappable)\n+\t  {\n+\t    fputs (\"swappable \", dump_file);\n+\t    if (insn_entry[i].special_handling == SH_CONST_VECTOR)\n+\t      fputs (\"special:constvec \", dump_file);\n+\t    else if (insn_entry[i].special_handling == SH_SUBREG)\n+\t      fputs (\"special:subreg \", dump_file);\n+\t    else if (insn_entry[i].special_handling == SH_NOSWAP_LD)\n+\t      fputs (\"special:load \", dump_file);\n+\t    else if (insn_entry[i].special_handling == SH_NOSWAP_ST)\n+\t      fputs (\"special:store \", dump_file);\n+\t    else if (insn_entry[i].special_handling == SH_EXTRACT)\n+\t      fputs (\"special:extract \", dump_file);\n+\t    else if (insn_entry[i].special_handling == SH_SPLAT)\n+\t      fputs (\"special:splat \", dump_file);\n+\t    else if (insn_entry[i].special_handling == SH_XXPERMDI)\n+\t      fputs (\"special:xxpermdi \", dump_file);\n+\t    else if (insn_entry[i].special_handling == SH_CONCAT)\n+\t      fputs (\"special:concat \", dump_file);\n+\t    else if (insn_entry[i].special_handling == SH_VPERM)\n+\t      fputs (\"special:vperm \", dump_file);\n+\t  }\n+\tif (insn_entry[i].web_not_optimizable)\n+\t  fputs (\"unoptimizable \", dump_file);\n+\tif (insn_entry[i].will_delete)\n+\t  fputs (\"delete \", dump_file);\n+\tfputs (\"\\n\", dump_file);\n+      }\n+  fputs (\"\\n\", dump_file);\n+}\n+\n+/* Return RTX with its address canonicalized to (reg) or (+ reg reg).\n+   Here RTX is an (& addr (const_int -16)).  Always return a new copy\n+   to avoid problems with combine.  */\n+static rtx\n+alignment_with_canonical_addr (rtx align)\n+{\n+  rtx canon;\n+  rtx addr = XEXP (align, 0);\n+\n+  if (REG_P (addr))\n+    canon = addr;\n+\n+  else if (GET_CODE (addr) == PLUS)\n+    {\n+      rtx addrop0 = XEXP (addr, 0);\n+      rtx addrop1 = XEXP (addr, 1);\n+\n+      if (!REG_P (addrop0))\n+\taddrop0 = force_reg (GET_MODE (addrop0), addrop0);\n+\n+      if (!REG_P (addrop1))\n+\taddrop1 = force_reg (GET_MODE (addrop1), addrop1);\n+\n+      canon = gen_rtx_PLUS (GET_MODE (addr), addrop0, addrop1);\n+    }\n+\n+  else\n+    canon = force_reg (GET_MODE (addr), addr);\n+\n+  return gen_rtx_AND (GET_MODE (align), canon, GEN_INT (-16));\n+}\n+\n+/* Check whether an rtx is an alignment mask, and if so, return \n+   a fully-expanded rtx for the masking operation.  */\n+static rtx\n+alignment_mask (rtx_insn *insn)\n+{\n+  rtx body = PATTERN (insn);\n+\n+  if (GET_CODE (body) != SET\n+      || GET_CODE (SET_SRC (body)) != AND\n+      || !REG_P (XEXP (SET_SRC (body), 0)))\n+    return 0;\n+\n+  rtx mask = XEXP (SET_SRC (body), 1);\n+\n+  if (GET_CODE (mask) == CONST_INT)\n+    {\n+      if (INTVAL (mask) == -16)\n+\treturn alignment_with_canonical_addr (SET_SRC (body));\n+      else\n+\treturn 0;\n+    }\n+\n+  if (!REG_P (mask))\n+    return 0;\n+\n+  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+  df_ref use;\n+  rtx real_mask = 0;\n+\n+  FOR_EACH_INSN_INFO_USE (use, insn_info)\n+    {\n+      if (!rtx_equal_p (DF_REF_REG (use), mask))\n+\tcontinue;\n+\n+      struct df_link *def_link = DF_REF_CHAIN (use);\n+      if (!def_link || def_link->next)\n+\treturn 0;\n+\n+      rtx_insn *const_insn = DF_REF_INSN (def_link->ref);\n+      rtx const_body = PATTERN (const_insn);\n+      if (GET_CODE (const_body) != SET)\n+\treturn 0;\n+\n+      real_mask = SET_SRC (const_body);\n+\n+      if (GET_CODE (real_mask) != CONST_INT\n+\t  || INTVAL (real_mask) != -16)\n+\treturn 0;\n+    }\n+\n+  if (real_mask == 0)\n+    return 0;\n+\n+  return alignment_with_canonical_addr (SET_SRC (body));\n+}\n+\n+/* Given INSN that's a load or store based at BASE_REG, look for a\n+   feeding computation that aligns its address on a 16-byte boundary.  */\n+static rtx\n+find_alignment_op (rtx_insn *insn, rtx base_reg)\n+{\n+  df_ref base_use;\n+  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+  rtx and_operation = 0;\n+\n+  FOR_EACH_INSN_INFO_USE (base_use, insn_info)\n+    {\n+      if (!rtx_equal_p (DF_REF_REG (base_use), base_reg))\n+\tcontinue;\n+\n+      struct df_link *base_def_link = DF_REF_CHAIN (base_use);\n+      if (!base_def_link || base_def_link->next)\n+\tbreak;\n+\n+      /* With stack-protector code enabled, and possibly in other\n+\t circumstances, there may not be an associated insn for \n+\t the def.  */\n+      if (DF_REF_IS_ARTIFICIAL (base_def_link->ref))\n+\tbreak;\n+\n+      rtx_insn *and_insn = DF_REF_INSN (base_def_link->ref);\n+      and_operation = alignment_mask (and_insn);\n+      if (and_operation != 0)\n+\tbreak;\n+    }\n+\n+  return and_operation;\n+}\n+\n+struct del_info { bool replace; rtx_insn *replace_insn; };\n+\n+/* If INSN is the load for an lvx pattern, put it in canonical form.  */\n+static void\n+recombine_lvx_pattern (rtx_insn *insn, del_info *to_delete)\n+{\n+  rtx body = PATTERN (insn);\n+  gcc_assert (GET_CODE (body) == SET\n+\t      && GET_CODE (SET_SRC (body)) == VEC_SELECT\n+\t      && GET_CODE (XEXP (SET_SRC (body), 0)) == MEM);\n+\n+  rtx mem = XEXP (SET_SRC (body), 0);\n+  rtx base_reg = XEXP (mem, 0);\n+\n+  rtx and_operation = find_alignment_op (insn, base_reg);\n+\n+  if (and_operation != 0)\n+    {\n+      df_ref def;\n+      struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+      FOR_EACH_INSN_INFO_DEF (def, insn_info)\n+\t{\n+\t  struct df_link *link = DF_REF_CHAIN (def);\n+\t  if (!link || link->next)\n+\t    break;\n+\n+\t  rtx_insn *swap_insn = DF_REF_INSN (link->ref);\n+\t  if (!insn_is_swap_p (swap_insn)\n+\t      || insn_is_load_p (swap_insn)\n+\t      || insn_is_store_p (swap_insn))\n+\t    break;\n+\n+\t  /* Expected lvx pattern found.  Change the swap to\n+\t     a copy, and propagate the AND operation into the\n+\t     load.  */\n+\t  to_delete[INSN_UID (swap_insn)].replace = true;\n+\t  to_delete[INSN_UID (swap_insn)].replace_insn = swap_insn;\n+\n+\t  XEXP (mem, 0) = and_operation;\n+\t  SET_SRC (body) = mem;\n+\t  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n+\t  df_insn_rescan (insn);\n+\t\t  \n+\t  if (dump_file)\n+\t    fprintf (dump_file, \"lvx opportunity found at %d\\n\",\n+\t\t     INSN_UID (insn));\n+\t}\n+    }\n+}\n+\n+/* If INSN is the store for an stvx pattern, put it in canonical form.  */\n+static void\n+recombine_stvx_pattern (rtx_insn *insn, del_info *to_delete)\n+{\n+  rtx body = PATTERN (insn);\n+  gcc_assert (GET_CODE (body) == SET\n+\t      && GET_CODE (SET_DEST (body)) == MEM\n+\t      && GET_CODE (SET_SRC (body)) == VEC_SELECT);\n+  rtx mem = SET_DEST (body);\n+  rtx base_reg = XEXP (mem, 0);\n+\n+  rtx and_operation = find_alignment_op (insn, base_reg);\n+\n+  if (and_operation != 0)\n+    {\n+      rtx src_reg = XEXP (SET_SRC (body), 0);\n+      df_ref src_use;\n+      struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+      FOR_EACH_INSN_INFO_USE (src_use, insn_info)\n+\t{\n+\t  if (!rtx_equal_p (DF_REF_REG (src_use), src_reg))\n+\t    continue;\n+\n+\t  struct df_link *link = DF_REF_CHAIN (src_use);\n+\t  if (!link || link->next)\n+\t    break;\n+\n+\t  rtx_insn *swap_insn = DF_REF_INSN (link->ref);\n+\t  if (!insn_is_swap_p (swap_insn)\n+\t      || insn_is_load_p (swap_insn)\n+\t      || insn_is_store_p (swap_insn))\n+\t    break;\n+\n+\t  /* Expected stvx pattern found.  Change the swap to\n+\t     a copy, and propagate the AND operation into the\n+\t     store.  */\n+\t  to_delete[INSN_UID (swap_insn)].replace = true;\n+\t  to_delete[INSN_UID (swap_insn)].replace_insn = swap_insn;\n+\n+\t  XEXP (mem, 0) = and_operation;\n+\t  SET_SRC (body) = src_reg;\n+\t  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n+\t  df_insn_rescan (insn);\n+\t\t  \n+\t  if (dump_file)\n+\t    fprintf (dump_file, \"stvx opportunity found at %d\\n\",\n+\t\t     INSN_UID (insn));\n+\t}\n+    }\n+}\n+\n+/* Look for patterns created from builtin lvx and stvx calls, and\n+   canonicalize them to be properly recognized as such.  */\n+static void\n+recombine_lvx_stvx_patterns (function *fun)\n+{\n+  int i;\n+  basic_block bb;\n+  rtx_insn *insn;\n+\n+  int num_insns = get_max_uid ();\n+  del_info *to_delete = XCNEWVEC (del_info, num_insns);\n+\n+  FOR_ALL_BB_FN (bb, fun)\n+    FOR_BB_INSNS (bb, insn)\n+    {\n+      if (!NONDEBUG_INSN_P (insn))\n+\tcontinue;\n+\n+      if (insn_is_load_p (insn) && insn_is_swap_p (insn))\n+\trecombine_lvx_pattern (insn, to_delete);\n+      else if (insn_is_store_p (insn) && insn_is_swap_p (insn))\n+\trecombine_stvx_pattern (insn, to_delete);\n+    }\n+\n+  /* Turning swaps into copies is delayed until now, to avoid problems\n+     with deleting instructions during the insn walk.  */\n+  for (i = 0; i < num_insns; i++)\n+    if (to_delete[i].replace)\n+      {\n+\trtx swap_body = PATTERN (to_delete[i].replace_insn);\n+\trtx src_reg = XEXP (SET_SRC (swap_body), 0);\n+\trtx copy = gen_rtx_SET (SET_DEST (swap_body), src_reg);\n+\trtx_insn *new_insn = emit_insn_before (copy,\n+\t\t\t\t\t       to_delete[i].replace_insn);\n+\tset_block_for_insn (new_insn,\n+\t\t\t    BLOCK_FOR_INSN (to_delete[i].replace_insn));\n+\tdf_insn_rescan (new_insn);\n+\tdf_insn_delete (to_delete[i].replace_insn);\n+\tremove_insn (to_delete[i].replace_insn);\n+\tto_delete[i].replace_insn->set_deleted ();\n+      }\n+  \n+  free (to_delete);\n+}\n+\n+/* Main entry point for this pass.  */\n+unsigned int\n+rs6000_analyze_swaps (function *fun)\n+{\n+  swap_web_entry *insn_entry;\n+  basic_block bb;\n+  rtx_insn *insn, *curr_insn = 0;\n+\n+  /* Dataflow analysis for use-def chains.  */\n+  df_set_flags (DF_RD_PRUNE_DEAD_DEFS);\n+  df_chain_add_problem (DF_DU_CHAIN | DF_UD_CHAIN);\n+  df_analyze ();\n+  df_set_flags (DF_DEFER_INSN_RESCAN);\n+\n+  /* Pre-pass to recombine lvx and stvx patterns so we don't lose info.  */\n+  recombine_lvx_stvx_patterns (fun);\n+\n+  /* Allocate structure to represent webs of insns.  */\n+  insn_entry = XCNEWVEC (swap_web_entry, get_max_uid ());\n+\n+  /* Walk the insns to gather basic data.  */\n+  FOR_ALL_BB_FN (bb, fun)\n+    FOR_BB_INSNS_SAFE (bb, insn, curr_insn)\n+    {\n+      unsigned int uid = INSN_UID (insn);\n+      if (NONDEBUG_INSN_P (insn))\n+\t{\n+\t  insn_entry[uid].insn = insn;\n+\n+\t  if (GET_CODE (insn) == CALL_INSN)\n+\t    insn_entry[uid].is_call = 1;\n+\n+\t  /* Walk the uses and defs to see if we mention vector regs.\n+\t     Record any constraints on optimization of such mentions.  */\n+\t  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+\t  df_ref mention;\n+\t  FOR_EACH_INSN_INFO_USE (mention, insn_info)\n+\t    {\n+\t      /* We use DF_REF_REAL_REG here to get inside any subregs.  */\n+\t      machine_mode mode = GET_MODE (DF_REF_REAL_REG (mention));\n+\n+\t      /* If a use gets its value from a call insn, it will be\n+\t\t a hard register and will look like (reg:V4SI 3 3).\n+\t\t The df analysis creates two mentions for GPR3 and GPR4,\n+\t\t both DImode.  We must recognize this and treat it as a\n+\t\t vector mention to ensure the call is unioned with this\n+\t\t use.  */\n+\t      if (mode == DImode && DF_REF_INSN_INFO (mention))\n+\t\t{\n+\t\t  rtx feeder = DF_REF_INSN (mention);\n+\t\t  /* FIXME:  It is pretty hard to get from the df mention\n+\t\t     to the mode of the use in the insn.  We arbitrarily\n+\t\t     pick a vector mode here, even though the use might\n+\t\t     be a real DImode.  We can be too conservative\n+\t\t     (create a web larger than necessary) because of\n+\t\t     this, so consider eventually fixing this.  */\n+\t\t  if (GET_CODE (feeder) == CALL_INSN)\n+\t\t    mode = V4SImode;\n+\t\t}\n+\n+\t      if (ALTIVEC_OR_VSX_VECTOR_MODE (mode) || mode == TImode)\n+\t\t{\n+\t\t  insn_entry[uid].is_relevant = 1;\n+\t\t  if (mode == TImode || mode == V1TImode\n+\t\t      || FLOAT128_VECTOR_P (mode))\n+\t\t    insn_entry[uid].is_128_int = 1;\n+\t\t  if (DF_REF_INSN_INFO (mention))\n+\t\t    insn_entry[uid].contains_subreg\n+\t\t      = !rtx_equal_p (DF_REF_REG (mention),\n+\t\t\t\t      DF_REF_REAL_REG (mention));\n+\t\t  union_defs (insn_entry, insn, mention);\n+\t\t}\n+\t    }\n+\t  FOR_EACH_INSN_INFO_DEF (mention, insn_info)\n+\t    {\n+\t      /* We use DF_REF_REAL_REG here to get inside any subregs.  */\n+\t      machine_mode mode = GET_MODE (DF_REF_REAL_REG (mention));\n+\n+\t      /* If we're loading up a hard vector register for a call,\n+\t\t it looks like (set (reg:V4SI 9 9) (...)).  The df\n+\t\t analysis creates two mentions for GPR9 and GPR10, both\n+\t\t DImode.  So relying on the mode from the mentions\n+\t\t isn't sufficient to ensure we union the call into the\n+\t\t web with the parameter setup code.  */\n+\t      if (mode == DImode && GET_CODE (insn) == SET\n+\t\t  && ALTIVEC_OR_VSX_VECTOR_MODE (GET_MODE (SET_DEST (insn))))\n+\t\tmode = GET_MODE (SET_DEST (insn));\n+\n+\t      if (ALTIVEC_OR_VSX_VECTOR_MODE (mode) || mode == TImode)\n+\t\t{\n+\t\t  insn_entry[uid].is_relevant = 1;\n+\t\t  if (mode == TImode || mode == V1TImode\n+\t\t      || FLOAT128_VECTOR_P (mode))\n+\t\t    insn_entry[uid].is_128_int = 1;\n+\t\t  if (DF_REF_INSN_INFO (mention))\n+\t\t    insn_entry[uid].contains_subreg\n+\t\t      = !rtx_equal_p (DF_REF_REG (mention),\n+\t\t\t\t      DF_REF_REAL_REG (mention));\n+\t\t  /* REG_FUNCTION_VALUE_P is not valid for subregs. */\n+\t\t  else if (REG_FUNCTION_VALUE_P (DF_REF_REG (mention)))\n+\t\t    insn_entry[uid].is_live_out = 1;\n+\t\t  union_uses (insn_entry, insn, mention);\n+\t\t}\n+\t    }\n+\n+\t  if (insn_entry[uid].is_relevant)\n+\t    {\n+\t      /* Determine if this is a load or store.  */\n+\t      insn_entry[uid].is_load = insn_is_load_p (insn);\n+\t      insn_entry[uid].is_store = insn_is_store_p (insn);\n+\n+\t      /* Determine if this is a doubleword swap.  If not,\n+\t\t determine whether it can legally be swapped.  */\n+\t      if (insn_is_swap_p (insn))\n+\t\tinsn_entry[uid].is_swap = 1;\n+\t      else\n+\t\t{\n+\t\t  unsigned int special = SH_NONE;\n+\t\t  insn_entry[uid].is_swappable\n+\t\t    = insn_is_swappable_p (insn_entry, insn, &special);\n+\t\t  if (special != SH_NONE && insn_entry[uid].contains_subreg)\n+\t\t    insn_entry[uid].is_swappable = 0;\n+\t\t  else if (special != SH_NONE)\n+\t\t    insn_entry[uid].special_handling = special;\n+\t\t  else if (insn_entry[uid].contains_subreg)\n+\t\t    insn_entry[uid].special_handling = SH_SUBREG;\n+\t\t}\n+\t    }\n+\t}\n+    }\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"\\nSwap insn entry table when first built\\n\");\n+      dump_swap_insn_table (insn_entry);\n+    }\n+\n+  /* Record unoptimizable webs.  */\n+  unsigned e = get_max_uid (), i;\n+  for (i = 0; i < e; ++i)\n+    {\n+      if (!insn_entry[i].is_relevant)\n+\tcontinue;\n+\n+      swap_web_entry *root\n+\t= (swap_web_entry*)(&insn_entry[i])->unionfind_root ();\n+\n+      if (insn_entry[i].is_live_in || insn_entry[i].is_live_out\n+\t  || (insn_entry[i].contains_subreg\n+\t      && insn_entry[i].special_handling != SH_SUBREG)\n+\t  || insn_entry[i].is_128_int || insn_entry[i].is_call\n+\t  || !(insn_entry[i].is_swappable || insn_entry[i].is_swap))\n+\troot->web_not_optimizable = 1;\n+\n+      /* If we have loads or stores that aren't permuting then the\n+\t optimization isn't appropriate.  */\n+      else if ((insn_entry[i].is_load || insn_entry[i].is_store)\n+\t  && !insn_entry[i].is_swap && !insn_entry[i].is_swappable)\n+\troot->web_not_optimizable = 1;\n+\n+      /* If we have permuting loads or stores that are not accompanied\n+\t by a register swap, the optimization isn't appropriate.  */\n+      else if (insn_entry[i].is_load && insn_entry[i].is_swap)\n+\t{\n+\t  rtx insn = insn_entry[i].insn;\n+\t  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+\t  df_ref def;\n+\n+\t  FOR_EACH_INSN_INFO_DEF (def, insn_info)\n+\t    {\n+\t      struct df_link *link = DF_REF_CHAIN (def);\n+\n+\t      if (!chain_contains_only_swaps (insn_entry, link, FOR_LOADS))\n+\t\t{\n+\t\t  root->web_not_optimizable = 1;\n+\t\t  break;\n+\t\t}\n+\t    }\n+\t}\n+      else if (insn_entry[i].is_store && insn_entry[i].is_swap)\n+\t{\n+\t  rtx insn = insn_entry[i].insn;\n+\t  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n+\t  df_ref use;\n+\n+\t  FOR_EACH_INSN_INFO_USE (use, insn_info)\n+\t    {\n+\t      struct df_link *link = DF_REF_CHAIN (use);\n+\n+\t      if (!chain_contains_only_swaps (insn_entry, link, FOR_STORES))\n+\t\t{\n+\t\t  root->web_not_optimizable = 1;\n+\t\t  break;\n+\t\t}\n+\t    }\n+\t}\n+    }\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"\\nSwap insn entry table after web analysis\\n\");\n+      dump_swap_insn_table (insn_entry);\n+    }\n+\n+  /* For each load and store in an optimizable web (which implies\n+     the loads and stores are permuting), find the associated\n+     register swaps and mark them for removal.  Due to various\n+     optimizations we may mark the same swap more than once.  Also\n+     perform special handling for swappable insns that require it.  */\n+  for (i = 0; i < e; ++i)\n+    if ((insn_entry[i].is_load || insn_entry[i].is_store)\n+\t&& insn_entry[i].is_swap)\n+      {\n+\tswap_web_entry* root_entry\n+\t  = (swap_web_entry*)((&insn_entry[i])->unionfind_root ());\n+\tif (!root_entry->web_not_optimizable)\n+\t  mark_swaps_for_removal (insn_entry, i);\n+      }\n+    else if (insn_entry[i].is_swappable && insn_entry[i].special_handling)\n+      {\n+\tswap_web_entry* root_entry\n+\t  = (swap_web_entry*)((&insn_entry[i])->unionfind_root ());\n+\tif (!root_entry->web_not_optimizable)\n+\t  handle_special_swappables (insn_entry, i);\n+      }\n+\n+  /* Now delete the swaps marked for removal.  */\n+  for (i = 0; i < e; ++i)\n+    if (insn_entry[i].will_delete)\n+      replace_swap_with_copy (insn_entry, i);\n+\n+  /* Clean up.  */\n+  free (insn_entry);\n+  return 0;\n+}\n+\n+const pass_data pass_data_analyze_swaps =\n+{\n+  RTL_PASS, /* type */\n+  \"swaps\", /* name */\n+  OPTGROUP_NONE, /* optinfo_flags */\n+  TV_NONE, /* tv_id */\n+  0, /* properties_required */\n+  0, /* properties_provided */\n+  0, /* properties_destroyed */\n+  0, /* todo_flags_start */\n+  TODO_df_finish, /* todo_flags_finish */\n+};\n+\n+class pass_analyze_swaps : public rtl_opt_pass\n+{\n+public:\n+  pass_analyze_swaps(gcc::context *ctxt)\n+    : rtl_opt_pass(pass_data_analyze_swaps, ctxt)\n+  {}\n+\n+  /* opt_pass methods: */\n+  virtual bool gate (function *)\n+    {\n+      return (optimize > 0 && !BYTES_BIG_ENDIAN && TARGET_VSX\n+\t      && !TARGET_P9_VECTOR && rs6000_optimize_swaps);\n+    }\n+\n+  virtual unsigned int execute (function *fun)\n+    {\n+      return rs6000_analyze_swaps (fun);\n+    }\n+\n+  opt_pass *clone ()\n+    {\n+      return new pass_analyze_swaps (m_ctxt);\n+    }\n+\n+}; // class pass_analyze_swaps\n+\n+rtl_opt_pass *\n+make_pass_analyze_swaps (gcc::context *ctxt)\n+{\n+  return new pass_analyze_swaps (ctxt);\n+}\n+"}, {"sha": "200f43acde857e9763df5343bb6c01c8ff9e97fb", "filename": "gcc/config/rs6000/rs6000.c", "status": "modified", "additions": 0, "deletions": 1856, "changes": 1856, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5/gcc%2Fconfig%2Frs6000%2Frs6000.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5/gcc%2Fconfig%2Frs6000%2Frs6000.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000.c?ref=0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5", "patch": "@@ -39030,1862 +39030,6 @@ emit_fusion_p9_store (rtx mem, rtx reg, rtx tmp_reg)\n   return \"\";\n }\n \n-\f\n-/* Analyze vector computations and remove unnecessary doubleword\n-   swaps (xxswapdi instructions).  This pass is performed only\n-   for little-endian VSX code generation.\n-\n-   For this specific case, loads and stores of 4x32 and 2x64 vectors\n-   are inefficient.  These are implemented using the lvx2dx and\n-   stvx2dx instructions, which invert the order of doublewords in\n-   a vector register.  Thus the code generation inserts an xxswapdi\n-   after each such load, and prior to each such store.  (For spill\n-   code after register assignment, an additional xxswapdi is inserted\n-   following each store in order to return a hard register to its\n-   unpermuted value.)\n-\n-   The extra xxswapdi instructions reduce performance.  This can be\n-   particularly bad for vectorized code.  The purpose of this pass\n-   is to reduce the number of xxswapdi instructions required for\n-   correctness.\n-\n-   The primary insight is that much code that operates on vectors\n-   does not care about the relative order of elements in a register,\n-   so long as the correct memory order is preserved.  If we have\n-   a computation where all input values are provided by lvxd2x/xxswapdi\n-   sequences, all outputs are stored using xxswapdi/stvxd2x sequences,\n-   and all intermediate computations are pure SIMD (independent of\n-   element order), then all the xxswapdi's associated with the loads\n-   and stores may be removed.\n-\n-   This pass uses some of the infrastructure and logical ideas from\n-   the \"web\" pass in web.c.  We create maximal webs of computations\n-   fitting the description above using union-find.  Each such web is\n-   then optimized by removing its unnecessary xxswapdi instructions.\n-\n-   The pass is placed prior to global optimization so that we can\n-   perform the optimization in the safest and simplest way possible;\n-   that is, by replacing each xxswapdi insn with a register copy insn.\n-   Subsequent forward propagation will remove copies where possible.\n-\n-   There are some operations sensitive to element order for which we\n-   can still allow the operation, provided we modify those operations.\n-   These include CONST_VECTORs, for which we must swap the first and\n-   second halves of the constant vector; and SUBREGs, for which we\n-   must adjust the byte offset to account for the swapped doublewords.\n-   A remaining opportunity would be non-immediate-form splats, for\n-   which we should adjust the selected lane of the input.  We should\n-   also make code generation adjustments for sum-across operations,\n-   since this is a common vectorizer reduction.\n-\n-   Because we run prior to the first split, we can see loads and stores\n-   here that match *vsx_le_perm_{load,store}_<mode>.  These are vanilla\n-   vector loads and stores that have not yet been split into a permuting\n-   load/store and a swap.  (One way this can happen is with a builtin\n-   call to vec_vsx_{ld,st}.)  We can handle these as well, but rather\n-   than deleting a swap, we convert the load/store into a permuting\n-   load/store (which effectively removes the swap).  */\n-\n-/* Notes on Permutes\n-\n-   We do not currently handle computations that contain permutes.  There\n-   is a general transformation that can be performed correctly, but it\n-   may introduce more expensive code than it replaces.  To handle these\n-   would require a cost model to determine when to perform the optimization.\n-   This commentary records how this could be done if desired.\n-\n-   The most general permute is something like this (example for V16QI):\n-\n-   (vec_select:V16QI (vec_concat:V32QI (op1:V16QI) (op2:V16QI))\n-                     (parallel [(const_int a0) (const_int a1)\n-                                 ...\n-                                (const_int a14) (const_int a15)]))\n-\n-   where a0,...,a15 are in [0,31] and select elements from op1 and op2\n-   to produce in the result.\n-\n-   Regardless of mode, we can convert the PARALLEL to a mask of 16\n-   byte-element selectors.  Let's call this M, with M[i] representing\n-   the ith byte-element selector value.  Then if we swap doublewords\n-   throughout the computation, we can get correct behavior by replacing\n-   M with M' as follows:\n-\n-    M'[i] = { (M[i]+8)%16      : M[i] in [0,15]\n-            { ((M[i]+8)%16)+16 : M[i] in [16,31]\n-\n-   This seems promising at first, since we are just replacing one mask\n-   with another.  But certain masks are preferable to others.  If M\n-   is a mask that matches a vmrghh pattern, for example, M' certainly\n-   will not.  Instead of a single vmrghh, we would generate a load of\n-   M' and a vperm.  So we would need to know how many xxswapd's we can\n-   remove as a result of this transformation to determine if it's\n-   profitable; and preferably the logic would need to be aware of all\n-   the special preferable masks.\n-\n-   Another form of permute is an UNSPEC_VPERM, in which the mask is\n-   already in a register.  In some cases, this mask may be a constant\n-   that we can discover with ud-chains, in which case the above\n-   transformation is ok.  However, the common usage here is for the\n-   mask to be produced by an UNSPEC_LVSL, in which case the mask \n-   cannot be known at compile time.  In such a case we would have to\n-   generate several instructions to compute M' as above at run time,\n-   and a cost model is needed again.\n-\n-   However, when the mask M for an UNSPEC_VPERM is loaded from the\n-   constant pool, we can replace M with M' as above at no cost\n-   beyond adding a constant pool entry.  */\n-\n-/* This is based on the union-find logic in web.c.  web_entry_base is\n-   defined in df.h.  */\n-class swap_web_entry : public web_entry_base\n-{\n- public:\n-  /* Pointer to the insn.  */\n-  rtx_insn *insn;\n-  /* Set if insn contains a mention of a vector register.  All other\n-     fields are undefined if this field is unset.  */\n-  unsigned int is_relevant : 1;\n-  /* Set if insn is a load.  */\n-  unsigned int is_load : 1;\n-  /* Set if insn is a store.  */\n-  unsigned int is_store : 1;\n-  /* Set if insn is a doubleword swap.  This can either be a register swap\n-     or a permuting load or store (test is_load and is_store for this).  */\n-  unsigned int is_swap : 1;\n-  /* Set if the insn has a live-in use of a parameter register.  */\n-  unsigned int is_live_in : 1;\n-  /* Set if the insn has a live-out def of a return register.  */\n-  unsigned int is_live_out : 1;\n-  /* Set if the insn contains a subreg reference of a vector register.  */\n-  unsigned int contains_subreg : 1;\n-  /* Set if the insn contains a 128-bit integer operand.  */\n-  unsigned int is_128_int : 1;\n-  /* Set if this is a call-insn.  */\n-  unsigned int is_call : 1;\n-  /* Set if this insn does not perform a vector operation for which\n-     element order matters, or if we know how to fix it up if it does.\n-     Undefined if is_swap is set.  */\n-  unsigned int is_swappable : 1;\n-  /* A nonzero value indicates what kind of special handling for this\n-     insn is required if doublewords are swapped.  Undefined if\n-     is_swappable is not set.  */\n-  unsigned int special_handling : 4;\n-  /* Set if the web represented by this entry cannot be optimized.  */\n-  unsigned int web_not_optimizable : 1;\n-  /* Set if this insn should be deleted.  */\n-  unsigned int will_delete : 1;\n-};\n-\n-enum special_handling_values {\n-  SH_NONE = 0,\n-  SH_CONST_VECTOR,\n-  SH_SUBREG,\n-  SH_NOSWAP_LD,\n-  SH_NOSWAP_ST,\n-  SH_EXTRACT,\n-  SH_SPLAT,\n-  SH_XXPERMDI,\n-  SH_CONCAT,\n-  SH_VPERM\n-};\n-\n-/* Union INSN with all insns containing definitions that reach USE.\n-   Detect whether USE is live-in to the current function.  */\n-static void\n-union_defs (swap_web_entry *insn_entry, rtx insn, df_ref use)\n-{\n-  struct df_link *link = DF_REF_CHAIN (use);\n-\n-  if (!link)\n-    insn_entry[INSN_UID (insn)].is_live_in = 1;\n-\n-  while (link)\n-    {\n-      if (DF_REF_IS_ARTIFICIAL (link->ref))\n-\tinsn_entry[INSN_UID (insn)].is_live_in = 1;\n-\n-      if (DF_REF_INSN_INFO (link->ref))\n-\t{\n-\t  rtx def_insn = DF_REF_INSN (link->ref);\n-\t  (void)unionfind_union (insn_entry + INSN_UID (insn),\n-\t\t\t\t insn_entry + INSN_UID (def_insn));\n-\t}\n-\n-      link = link->next;\n-    }\n-}\n-\n-/* Union INSN with all insns containing uses reached from DEF.\n-   Detect whether DEF is live-out from the current function.  */\n-static void\n-union_uses (swap_web_entry *insn_entry, rtx insn, df_ref def)\n-{\n-  struct df_link *link = DF_REF_CHAIN (def);\n-\n-  if (!link)\n-    insn_entry[INSN_UID (insn)].is_live_out = 1;\n-\n-  while (link)\n-    {\n-      /* This could be an eh use or some other artificial use;\n-\t we treat these all the same (killing the optimization).  */\n-      if (DF_REF_IS_ARTIFICIAL (link->ref))\n-\tinsn_entry[INSN_UID (insn)].is_live_out = 1;\n-\n-      if (DF_REF_INSN_INFO (link->ref))\n-\t{\n-\t  rtx use_insn = DF_REF_INSN (link->ref);\n-\t  (void)unionfind_union (insn_entry + INSN_UID (insn),\n-\t\t\t\t insn_entry + INSN_UID (use_insn));\n-\t}\n-\n-      link = link->next;\n-    }\n-}\n-\n-/* Return 1 iff INSN is a load insn, including permuting loads that\n-   represent an lvxd2x instruction; else return 0.  */\n-static unsigned int\n-insn_is_load_p (rtx insn)\n-{\n-  rtx body = PATTERN (insn);\n-\n-  if (GET_CODE (body) == SET)\n-    {\n-      if (GET_CODE (SET_SRC (body)) == MEM)\n-\treturn 1;\n-\n-      if (GET_CODE (SET_SRC (body)) == VEC_SELECT\n-\t  && GET_CODE (XEXP (SET_SRC (body), 0)) == MEM)\n-\treturn 1;\n-\n-      return 0;\n-    }\n-\n-  if (GET_CODE (body) != PARALLEL)\n-    return 0;\n-\n-  rtx set = XVECEXP (body, 0, 0);\n-\n-  if (GET_CODE (set) == SET && GET_CODE (SET_SRC (set)) == MEM)\n-    return 1;\n-\n-  return 0;\n-}\n-\n-/* Return 1 iff INSN is a store insn, including permuting stores that\n-   represent an stvxd2x instruction; else return 0.  */\n-static unsigned int\n-insn_is_store_p (rtx insn)\n-{\n-  rtx body = PATTERN (insn);\n-  if (GET_CODE (body) == SET && GET_CODE (SET_DEST (body)) == MEM)\n-    return 1;\n-  if (GET_CODE (body) != PARALLEL)\n-    return 0;\n-  rtx set = XVECEXP (body, 0, 0);\n-  if (GET_CODE (set) == SET && GET_CODE (SET_DEST (set)) == MEM)\n-    return 1;\n-  return 0;\n-}\n-\n-/* Return 1 iff INSN swaps doublewords.  This may be a reg-reg swap,\n-   a permuting load, or a permuting store.  */\n-static unsigned int\n-insn_is_swap_p (rtx insn)\n-{\n-  rtx body = PATTERN (insn);\n-  if (GET_CODE (body) != SET)\n-    return 0;\n-  rtx rhs = SET_SRC (body);\n-  if (GET_CODE (rhs) != VEC_SELECT)\n-    return 0;\n-  rtx parallel = XEXP (rhs, 1);\n-  if (GET_CODE (parallel) != PARALLEL)\n-    return 0;\n-  unsigned int len = XVECLEN (parallel, 0);\n-  if (len != 2 && len != 4 && len != 8 && len != 16)\n-    return 0;\n-  for (unsigned int i = 0; i < len / 2; ++i)\n-    {\n-      rtx op = XVECEXP (parallel, 0, i);\n-      if (GET_CODE (op) != CONST_INT || INTVAL (op) != len / 2 + i)\n-\treturn 0;\n-    }\n-  for (unsigned int i = len / 2; i < len; ++i)\n-    {\n-      rtx op = XVECEXP (parallel, 0, i);\n-      if (GET_CODE (op) != CONST_INT || INTVAL (op) != i - len / 2)\n-\treturn 0;\n-    }\n-  return 1;\n-}\n-\n-/* Return TRUE if insn is a swap fed by a load from the constant pool.  */\n-static bool\n-const_load_sequence_p (swap_web_entry *insn_entry, rtx insn)\n-{\n-  unsigned uid = INSN_UID (insn);\n-  if (!insn_entry[uid].is_swap || insn_entry[uid].is_load)\n-    return false;\n-\n-  const_rtx tocrel_base;\n-\n-  /* Find the unique use in the swap and locate its def.  If the def\n-     isn't unique, punt.  */\n-  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n-  df_ref use;\n-  FOR_EACH_INSN_INFO_USE (use, insn_info)\n-    {\n-      struct df_link *def_link = DF_REF_CHAIN (use);\n-      if (!def_link || def_link->next)\n-\treturn false;\n-\n-      rtx def_insn = DF_REF_INSN (def_link->ref);\n-      unsigned uid2 = INSN_UID (def_insn);\n-      if (!insn_entry[uid2].is_load || !insn_entry[uid2].is_swap)\n-\treturn false;\n-\n-      rtx body = PATTERN (def_insn);\n-      if (GET_CODE (body) != SET\n-\t  || GET_CODE (SET_SRC (body)) != VEC_SELECT\n-\t  || GET_CODE (XEXP (SET_SRC (body), 0)) != MEM)\n-\treturn false;\n-\n-      rtx mem = XEXP (SET_SRC (body), 0);\n-      rtx base_reg = XEXP (mem, 0);\n-\n-      df_ref base_use;\n-      insn_info = DF_INSN_INFO_GET (def_insn);\n-      FOR_EACH_INSN_INFO_USE (base_use, insn_info)\n-\t{\n-\t  if (!rtx_equal_p (DF_REF_REG (base_use), base_reg))\n-\t    continue;\n-\n-\t  struct df_link *base_def_link = DF_REF_CHAIN (base_use);\n-\t  if (!base_def_link || base_def_link->next)\n-\t    return false;\n-\n-\t  rtx tocrel_insn = DF_REF_INSN (base_def_link->ref);\n-\t  rtx tocrel_body = PATTERN (tocrel_insn);\n-\t  rtx base, offset;\n-\t  if (GET_CODE (tocrel_body) != SET)\n-\t    return false;\n-\t  /* There is an extra level of indirection for small/large\n-\t     code models.  */\n-\t  rtx tocrel_expr = SET_SRC (tocrel_body);\n-\t  if (GET_CODE (tocrel_expr) == MEM)\n-\t    tocrel_expr = XEXP (tocrel_expr, 0);\n-\t  if (!toc_relative_expr_p (tocrel_expr, false, &tocrel_base, NULL))\n-\t    return false;\n-\t  split_const (XVECEXP (tocrel_base, 0, 0), &base, &offset);\n-\t  if (GET_CODE (base) != SYMBOL_REF || !CONSTANT_POOL_ADDRESS_P (base))\n-\t    return false;\n-\t}\n-    }\n-  return true;\n-}\n-\n-/* Return TRUE iff OP matches a V2DF reduction pattern.  See the\n-   definition of vsx_reduc_<VEC_reduc_name>_v2df in vsx.md.  */\n-static bool\n-v2df_reduction_p (rtx op)\n-{\n-  if (GET_MODE (op) != V2DFmode)\n-    return false;\n-  \n-  enum rtx_code code = GET_CODE (op);\n-  if (code != PLUS && code != SMIN && code != SMAX)\n-    return false;\n-\n-  rtx concat = XEXP (op, 0);\n-  if (GET_CODE (concat) != VEC_CONCAT)\n-    return false;\n-\n-  rtx select0 = XEXP (concat, 0);\n-  rtx select1 = XEXP (concat, 1);\n-  if (GET_CODE (select0) != VEC_SELECT || GET_CODE (select1) != VEC_SELECT)\n-    return false;\n-\n-  rtx reg0 = XEXP (select0, 0);\n-  rtx reg1 = XEXP (select1, 0);\n-  if (!rtx_equal_p (reg0, reg1) || !REG_P (reg0))\n-    return false;\n-\n-  rtx parallel0 = XEXP (select0, 1);\n-  rtx parallel1 = XEXP (select1, 1);\n-  if (GET_CODE (parallel0) != PARALLEL || GET_CODE (parallel1) != PARALLEL)\n-    return false;\n-\n-  if (!rtx_equal_p (XVECEXP (parallel0, 0, 0), const1_rtx)\n-      || !rtx_equal_p (XVECEXP (parallel1, 0, 0), const0_rtx))\n-    return false;\n-\n-  return true;\n-}\n-\n-/* Return 1 iff OP is an operand that will not be affected by having\n-   vector doublewords swapped in memory.  */\n-static unsigned int\n-rtx_is_swappable_p (rtx op, unsigned int *special)\n-{\n-  enum rtx_code code = GET_CODE (op);\n-  int i, j;\n-  rtx parallel;\n-\n-  switch (code)\n-    {\n-    case LABEL_REF:\n-    case SYMBOL_REF:\n-    case CLOBBER:\n-    case REG:\n-      return 1;\n-\n-    case VEC_CONCAT:\n-    case ASM_INPUT:\n-    case ASM_OPERANDS:\n-      return 0;\n-\n-    case CONST_VECTOR:\n-      {\n-\t*special = SH_CONST_VECTOR;\n-\treturn 1;\n-      }\n-\n-    case VEC_DUPLICATE:\n-      /* Opportunity: If XEXP (op, 0) has the same mode as the result,\n-\t and XEXP (op, 1) is a PARALLEL with a single QImode const int,\n-\t it represents a vector splat for which we can do special\n-\t handling.  */\n-      if (GET_CODE (XEXP (op, 0)) == CONST_INT)\n-\treturn 1;\n-      else if (REG_P (XEXP (op, 0))\n-\t       && GET_MODE_INNER (GET_MODE (op)) == GET_MODE (XEXP (op, 0)))\n-\t/* This catches V2DF and V2DI splat, at a minimum.  */\n-\treturn 1;\n-      else if (GET_CODE (XEXP (op, 0)) == TRUNCATE\n-\t       && REG_P (XEXP (XEXP (op, 0), 0))\n-\t       && GET_MODE_INNER (GET_MODE (op)) == GET_MODE (XEXP (op, 0)))\n-\t/* This catches splat of a truncated value.  */\n-\treturn 1;\n-      else if (GET_CODE (XEXP (op, 0)) == VEC_SELECT)\n-\t/* If the duplicated item is from a select, defer to the select\n-\t   processing to see if we can change the lane for the splat.  */\n-\treturn rtx_is_swappable_p (XEXP (op, 0), special);\n-      else\n-\treturn 0;\n-\n-    case VEC_SELECT:\n-      /* A vec_extract operation is ok if we change the lane.  */\n-      if (GET_CODE (XEXP (op, 0)) == REG\n-\t  && GET_MODE_INNER (GET_MODE (XEXP (op, 0))) == GET_MODE (op)\n-\t  && GET_CODE ((parallel = XEXP (op, 1))) == PARALLEL\n-\t  && XVECLEN (parallel, 0) == 1\n-\t  && GET_CODE (XVECEXP (parallel, 0, 0)) == CONST_INT)\n-\t{\n-\t  *special = SH_EXTRACT;\n-\t  return 1;\n-\t}\n-      /* An XXPERMDI is ok if we adjust the lanes.  Note that if the\n-\t XXPERMDI is a swap operation, it will be identified by\n-\t insn_is_swap_p and therefore we won't get here.  */\n-      else if (GET_CODE (XEXP (op, 0)) == VEC_CONCAT\n-\t       && (GET_MODE (XEXP (op, 0)) == V4DFmode\n-\t\t   || GET_MODE (XEXP (op, 0)) == V4DImode)\n-\t       && GET_CODE ((parallel = XEXP (op, 1))) == PARALLEL\n-\t       && XVECLEN (parallel, 0) == 2\n-\t       && GET_CODE (XVECEXP (parallel, 0, 0)) == CONST_INT\n-\t       && GET_CODE (XVECEXP (parallel, 0, 1)) == CONST_INT)\n-\t{\n-\t  *special = SH_XXPERMDI;\n-\t  return 1;\n-\t}\n-      else if (v2df_reduction_p (op))\n-\treturn 1;\n-      else\n-\treturn 0;\n-\n-    case UNSPEC:\n-      {\n-\t/* Various operations are unsafe for this optimization, at least\n-\t   without significant additional work.  Permutes are obviously\n-\t   problematic, as both the permute control vector and the ordering\n-\t   of the target values are invalidated by doubleword swapping.\n-\t   Vector pack and unpack modify the number of vector lanes.\n-\t   Merge-high/low will not operate correctly on swapped operands.\n-\t   Vector shifts across element boundaries are clearly uncool,\n-\t   as are vector select and concatenate operations.  Vector\n-\t   sum-across instructions define one operand with a specific\n-\t   order-dependent element, so additional fixup code would be\n-\t   needed to make those work.  Vector set and non-immediate-form\n-\t   vector splat are element-order sensitive.  A few of these\n-\t   cases might be workable with special handling if required.\n-\t   Adding cost modeling would be appropriate in some cases.  */\n-\tint val = XINT (op, 1);\n-\tswitch (val)\n-\t  {\n-\t  default:\n-\t    break;\n-\t  case UNSPEC_VMRGH_DIRECT:\n-\t  case UNSPEC_VMRGL_DIRECT:\n-\t  case UNSPEC_VPACK_SIGN_SIGN_SAT:\n-\t  case UNSPEC_VPACK_SIGN_UNS_SAT:\n-\t  case UNSPEC_VPACK_UNS_UNS_MOD:\n-\t  case UNSPEC_VPACK_UNS_UNS_MOD_DIRECT:\n-\t  case UNSPEC_VPACK_UNS_UNS_SAT:\n-\t  case UNSPEC_VPERM:\n-\t  case UNSPEC_VPERM_UNS:\n-\t  case UNSPEC_VPERMHI:\n-\t  case UNSPEC_VPERMSI:\n-\t  case UNSPEC_VPKPX:\n-\t  case UNSPEC_VSLDOI:\n-\t  case UNSPEC_VSLO:\n-\t  case UNSPEC_VSRO:\n-\t  case UNSPEC_VSUM2SWS:\n-\t  case UNSPEC_VSUM4S:\n-\t  case UNSPEC_VSUM4UBS:\n-\t  case UNSPEC_VSUMSWS:\n-\t  case UNSPEC_VSUMSWS_DIRECT:\n-\t  case UNSPEC_VSX_CONCAT:\n-\t  case UNSPEC_VSX_SET:\n-\t  case UNSPEC_VSX_SLDWI:\n-\t  case UNSPEC_VUNPACK_HI_SIGN:\n-\t  case UNSPEC_VUNPACK_HI_SIGN_DIRECT:\n-\t  case UNSPEC_VUNPACK_LO_SIGN:\n-\t  case UNSPEC_VUNPACK_LO_SIGN_DIRECT:\n-\t  case UNSPEC_VUPKHPX:\n-\t  case UNSPEC_VUPKHS_V4SF:\n-\t  case UNSPEC_VUPKHU_V4SF:\n-\t  case UNSPEC_VUPKLPX:\n-\t  case UNSPEC_VUPKLS_V4SF:\n-\t  case UNSPEC_VUPKLU_V4SF:\n-\t  case UNSPEC_VSX_CVDPSPN:\n-\t  case UNSPEC_VSX_CVSPDP:\n-\t  case UNSPEC_VSX_CVSPDPN:\n-\t  case UNSPEC_VSX_EXTRACT:\n-\t  case UNSPEC_VSX_VSLO:\n-\t  case UNSPEC_VSX_VEC_INIT:\n-\t    return 0;\n-\t  case UNSPEC_VSPLT_DIRECT:\n-\t  case UNSPEC_VSX_XXSPLTD:\n-\t    *special = SH_SPLAT;\n-\t    return 1;\n-\t  case UNSPEC_REDUC_PLUS:\n-\t  case UNSPEC_REDUC:\n-\t    return 1;\n-\t  }\n-      }\n-\n-    default:\n-      break;\n-    }\n-\n-  const char *fmt = GET_RTX_FORMAT (code);\n-  int ok = 1;\n-\n-  for (i = 0; i < GET_RTX_LENGTH (code); ++i)\n-    if (fmt[i] == 'e' || fmt[i] == 'u')\n-      {\n-\tunsigned int special_op = SH_NONE;\n-\tok &= rtx_is_swappable_p (XEXP (op, i), &special_op);\n-\tif (special_op == SH_NONE)\n-\t  continue;\n-\t/* Ensure we never have two kinds of special handling\n-\t   for the same insn.  */\n-\tif (*special != SH_NONE && *special != special_op)\n-\t  return 0;\n-\t*special = special_op;\n-      }\n-    else if (fmt[i] == 'E')\n-      for (j = 0; j < XVECLEN (op, i); ++j)\n-\t{\n-\t  unsigned int special_op = SH_NONE;\n-\t  ok &= rtx_is_swappable_p (XVECEXP (op, i, j), &special_op);\n-\tif (special_op == SH_NONE)\n-\t  continue;\n-\t  /* Ensure we never have two kinds of special handling\n-\t     for the same insn.  */\n-\t  if (*special != SH_NONE && *special != special_op)\n-\t    return 0;\n-\t  *special = special_op;\n-\t}\n-\n-  return ok;\n-}\n-\n-/* Return 1 iff INSN is an operand that will not be affected by\n-   having vector doublewords swapped in memory (in which case\n-   *SPECIAL is unchanged), or that can be modified to be correct\n-   if vector doublewords are swapped in memory (in which case\n-   *SPECIAL is changed to a value indicating how).  */\n-static unsigned int\n-insn_is_swappable_p (swap_web_entry *insn_entry, rtx insn,\n-\t\t     unsigned int *special)\n-{\n-  /* Calls are always bad.  */\n-  if (GET_CODE (insn) == CALL_INSN)\n-    return 0;\n-\n-  /* Loads and stores seen here are not permuting, but we can still\n-     fix them up by converting them to permuting ones.  Exceptions:\n-     UNSPEC_LVE, UNSPEC_LVX, and UNSPEC_STVX, which have a PARALLEL\n-     body instead of a SET; and UNSPEC_STVE, which has an UNSPEC\n-     for the SET source.  Also we must now make an exception for lvx\n-     and stvx when they are not in the UNSPEC_LVX/STVX form (with the\n-     explicit \"& -16\") since this leads to unrecognizable insns.  */\n-  rtx body = PATTERN (insn);\n-  int i = INSN_UID (insn);\n-\n-  if (insn_entry[i].is_load)\n-    {\n-      if (GET_CODE (body) == SET)\n-\t{\n-\t  rtx rhs = SET_SRC (body);\n-\t  /* Even without a swap, the RHS might be a vec_select for, say,\n-\t     a byte-reversing load.  */\n-\t  if (GET_CODE (rhs) != MEM)\n-\t    return 0;\n-\t  if (GET_CODE (XEXP (rhs, 0)) == AND)\n-\t    return 0;\n-\n-\t  *special = SH_NOSWAP_LD;\n-\t  return 1;\n-\t}\n-      else\n-\treturn 0;\n-    }\n-\n-  if (insn_entry[i].is_store)\n-    {\n-      if (GET_CODE (body) == SET\n-\t  && GET_CODE (SET_SRC (body)) != UNSPEC)\n-\t{\n-\t  rtx lhs = SET_DEST (body);\n-\t  /* Even without a swap, the LHS might be a vec_select for, say,\n-\t     a byte-reversing store.  */\n-\t  if (GET_CODE (lhs) != MEM)\n-\t    return 0;\n-\t  if (GET_CODE (XEXP (lhs, 0)) == AND)\n-\t    return 0;\n-\t  \n-\t  *special = SH_NOSWAP_ST;\n-\t  return 1;\n-\t}\n-      else\n-\treturn 0;\n-    }\n-\n-  /* A convert to single precision can be left as is provided that\n-     all of its uses are in xxspltw instructions that splat BE element\n-     zero.  */\n-  if (GET_CODE (body) == SET\n-      && GET_CODE (SET_SRC (body)) == UNSPEC\n-      && XINT (SET_SRC (body), 1) == UNSPEC_VSX_CVDPSPN)\n-    {\n-      df_ref def;\n-      struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n-\n-      FOR_EACH_INSN_INFO_DEF (def, insn_info)\n-\t{\n-\t  struct df_link *link = DF_REF_CHAIN (def);\n-\t  if (!link)\n-\t    return 0;\n-\n-\t  for (; link; link = link->next) {\n-\t    rtx use_insn = DF_REF_INSN (link->ref);\n-\t    rtx use_body = PATTERN (use_insn);\n-\t    if (GET_CODE (use_body) != SET\n-\t\t|| GET_CODE (SET_SRC (use_body)) != UNSPEC\n-\t\t|| XINT (SET_SRC (use_body), 1) != UNSPEC_VSX_XXSPLTW\n-\t\t|| XVECEXP (SET_SRC (use_body), 0, 1) != const0_rtx)\n-\t      return 0;\n-\t  }\n-\t}\n-\n-      return 1;\n-    }\n-\n-  /* A concatenation of two doublewords is ok if we reverse the\n-     order of the inputs.  */\n-  if (GET_CODE (body) == SET\n-      && GET_CODE (SET_SRC (body)) == VEC_CONCAT\n-      && (GET_MODE (SET_SRC (body)) == V2DFmode\n-\t  || GET_MODE (SET_SRC (body)) == V2DImode))\n-    {\n-      *special = SH_CONCAT;\n-      return 1;\n-    }\n-\n-  /* V2DF reductions are always swappable.  */\n-  if (GET_CODE (body) == PARALLEL)\n-    {\n-      rtx expr = XVECEXP (body, 0, 0);\n-      if (GET_CODE (expr) == SET\n-\t  && v2df_reduction_p (SET_SRC (expr)))\n-\treturn 1;\n-    }\n-\n-  /* An UNSPEC_VPERM is ok if the mask operand is loaded from the\n-     constant pool.  */\n-  if (GET_CODE (body) == SET\n-      && GET_CODE (SET_SRC (body)) == UNSPEC\n-      && XINT (SET_SRC (body), 1) == UNSPEC_VPERM\n-      && XVECLEN (SET_SRC (body), 0) == 3\n-      && GET_CODE (XVECEXP (SET_SRC (body), 0, 2)) == REG)\n-    {\n-      rtx mask_reg = XVECEXP (SET_SRC (body), 0, 2);\n-      struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n-      df_ref use;\n-      FOR_EACH_INSN_INFO_USE (use, insn_info)\n-\tif (rtx_equal_p (DF_REF_REG (use), mask_reg))\n-\t  {\n-\t    struct df_link *def_link = DF_REF_CHAIN (use);\n-\t    /* Punt if multiple definitions for this reg.  */\n-\t    if (def_link && !def_link->next &&\n-\t\tconst_load_sequence_p (insn_entry,\n-\t\t\t\t       DF_REF_INSN (def_link->ref)))\n-\t      {\n-\t\t*special = SH_VPERM;\n-\t\treturn 1;\n-\t      }\n-\t  }\n-    }\n-\n-  /* Otherwise check the operands for vector lane violations.  */\n-  return rtx_is_swappable_p (body, special);\n-}\n-\n-enum chain_purpose { FOR_LOADS, FOR_STORES };\n-\n-/* Return true if the UD or DU chain headed by LINK is non-empty,\n-   and every entry on the chain references an insn that is a\n-   register swap.  Furthermore, if PURPOSE is FOR_LOADS, each such\n-   register swap must have only permuting loads as reaching defs.\n-   If PURPOSE is FOR_STORES, each such register swap must have only\n-   register swaps or permuting stores as reached uses.  */\n-static bool\n-chain_contains_only_swaps (swap_web_entry *insn_entry, struct df_link *link,\n-\t\t\t   enum chain_purpose purpose)\n-{\n-  if (!link)\n-    return false;\n-\n-  for (; link; link = link->next)\n-    {\n-      if (!ALTIVEC_OR_VSX_VECTOR_MODE (GET_MODE (DF_REF_REG (link->ref))))\n-\tcontinue;\n-\n-      if (DF_REF_IS_ARTIFICIAL (link->ref))\n-\treturn false;\n-\n-      rtx reached_insn = DF_REF_INSN (link->ref);\n-      unsigned uid = INSN_UID (reached_insn);\n-      struct df_insn_info *insn_info = DF_INSN_INFO_GET (reached_insn);\n-\n-      if (!insn_entry[uid].is_swap || insn_entry[uid].is_load\n-\t  || insn_entry[uid].is_store)\n-\treturn false;\n-\n-      if (purpose == FOR_LOADS)\n-\t{\n-\t  df_ref use;\n-\t  FOR_EACH_INSN_INFO_USE (use, insn_info)\n-\t    {\n-\t      struct df_link *swap_link = DF_REF_CHAIN (use);\n-\n-\t      while (swap_link)\n-\t\t{\n-\t\t  if (DF_REF_IS_ARTIFICIAL (link->ref))\n-\t\t    return false;\n-\n-\t\t  rtx swap_def_insn = DF_REF_INSN (swap_link->ref);\n-\t\t  unsigned uid2 = INSN_UID (swap_def_insn);\n-\n-\t\t  /* Only permuting loads are allowed.  */\n-\t\t  if (!insn_entry[uid2].is_swap || !insn_entry[uid2].is_load)\n-\t\t    return false;\n-\n-\t\t  swap_link = swap_link->next;\n-\t\t}\n-\t    }\n-\t}\n-      else if (purpose == FOR_STORES)\n-\t{\n-\t  df_ref def;\n-\t  FOR_EACH_INSN_INFO_DEF (def, insn_info)\n-\t    {\n-\t      struct df_link *swap_link = DF_REF_CHAIN (def);\n-\n-\t      while (swap_link)\n-\t\t{\n-\t\t  if (DF_REF_IS_ARTIFICIAL (link->ref))\n-\t\t    return false;\n-\n-\t\t  rtx swap_use_insn = DF_REF_INSN (swap_link->ref);\n-\t\t  unsigned uid2 = INSN_UID (swap_use_insn);\n-\n-\t\t  /* Permuting stores or register swaps are allowed.  */\n-\t\t  if (!insn_entry[uid2].is_swap || insn_entry[uid2].is_load)\n-\t\t    return false;\n-\n-\t\t  swap_link = swap_link->next;\n-\t\t}\n-\t    }\n-\t}\n-    }\n-\n-  return true;\n-}\n-\n-/* Mark the xxswapdi instructions associated with permuting loads and\n-   stores for removal.  Note that we only flag them for deletion here,\n-   as there is a possibility of a swap being reached from multiple\n-   loads, etc.  */\n-static void\n-mark_swaps_for_removal (swap_web_entry *insn_entry, unsigned int i)\n-{\n-  rtx insn = insn_entry[i].insn;\n-  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n-\n-  if (insn_entry[i].is_load)\n-    {\n-      df_ref def;\n-      FOR_EACH_INSN_INFO_DEF (def, insn_info)\n-\t{\n-\t  struct df_link *link = DF_REF_CHAIN (def);\n-\n-\t  /* We know by now that these are swaps, so we can delete\n-\t     them confidently.  */\n-\t  while (link)\n-\t    {\n-\t      rtx use_insn = DF_REF_INSN (link->ref);\n-\t      insn_entry[INSN_UID (use_insn)].will_delete = 1;\n-\t      link = link->next;\n-\t    }\n-\t}\n-    }\n-  else if (insn_entry[i].is_store)\n-    {\n-      df_ref use;\n-      FOR_EACH_INSN_INFO_USE (use, insn_info)\n-\t{\n-\t  /* Ignore uses for addressability.  */\n-\t  machine_mode mode = GET_MODE (DF_REF_REG (use));\n-\t  if (!ALTIVEC_OR_VSX_VECTOR_MODE (mode))\n-\t    continue;\n-\n-\t  struct df_link *link = DF_REF_CHAIN (use);\n-\n-\t  /* We know by now that these are swaps, so we can delete\n-\t     them confidently.  */\n-\t  while (link)\n-\t    {\n-\t      rtx def_insn = DF_REF_INSN (link->ref);\n-\t      insn_entry[INSN_UID (def_insn)].will_delete = 1;\n-\t      link = link->next;\n-\t    }\n-\t}\n-    }\n-}\n-\n-/* OP is either a CONST_VECTOR or an expression containing one.\n-   Swap the first half of the vector with the second in the first\n-   case.  Recurse to find it in the second.  */\n-static void\n-swap_const_vector_halves (rtx op)\n-{\n-  int i;\n-  enum rtx_code code = GET_CODE (op);\n-  if (GET_CODE (op) == CONST_VECTOR)\n-    {\n-      int half_units = GET_MODE_NUNITS (GET_MODE (op)) / 2;\n-      for (i = 0; i < half_units; ++i)\n-\t{\n-\t  rtx temp = CONST_VECTOR_ELT (op, i);\n-\t  CONST_VECTOR_ELT (op, i) = CONST_VECTOR_ELT (op, i + half_units);\n-\t  CONST_VECTOR_ELT (op, i + half_units) = temp;\n-\t}\n-    }\n-  else\n-    {\n-      int j;\n-      const char *fmt = GET_RTX_FORMAT (code);\n-      for (i = 0; i < GET_RTX_LENGTH (code); ++i)\n-\tif (fmt[i] == 'e' || fmt[i] == 'u')\n-\t  swap_const_vector_halves (XEXP (op, i));\n-\telse if (fmt[i] == 'E')\n-\t  for (j = 0; j < XVECLEN (op, i); ++j)\n-\t    swap_const_vector_halves (XVECEXP (op, i, j));\n-    }\n-}\n-\n-/* Find all subregs of a vector expression that perform a narrowing,\n-   and adjust the subreg index to account for doubleword swapping.  */\n-static void\n-adjust_subreg_index (rtx op)\n-{\n-  enum rtx_code code = GET_CODE (op);\n-  if (code == SUBREG\n-      && (GET_MODE_SIZE (GET_MODE (op))\n-\t  < GET_MODE_SIZE (GET_MODE (XEXP (op, 0)))))\n-    {\n-      unsigned int index = SUBREG_BYTE (op);\n-      if (index < 8)\n-\tindex += 8;\n-      else\n-\tindex -= 8;\n-      SUBREG_BYTE (op) = index;\n-    }\n-\n-  const char *fmt = GET_RTX_FORMAT (code);\n-  int i,j;\n-  for (i = 0; i < GET_RTX_LENGTH (code); ++i)\n-    if (fmt[i] == 'e' || fmt[i] == 'u')\n-      adjust_subreg_index (XEXP (op, i));\n-    else if (fmt[i] == 'E')\n-      for (j = 0; j < XVECLEN (op, i); ++j)\n-\tadjust_subreg_index (XVECEXP (op, i, j));\n-}\n-\n-/* Convert the non-permuting load INSN to a permuting one.  */\n-static void\n-permute_load (rtx_insn *insn)\n-{\n-  rtx body = PATTERN (insn);\n-  rtx mem_op = SET_SRC (body);\n-  rtx tgt_reg = SET_DEST (body);\n-  machine_mode mode = GET_MODE (tgt_reg);\n-  int n_elts = GET_MODE_NUNITS (mode);\n-  int half_elts = n_elts / 2;\n-  rtx par = gen_rtx_PARALLEL (mode, rtvec_alloc (n_elts));\n-  int i, j;\n-  for (i = 0, j = half_elts; i < half_elts; ++i, ++j)\n-    XVECEXP (par, 0, i) = GEN_INT (j);\n-  for (i = half_elts, j = 0; j < half_elts; ++i, ++j)\n-    XVECEXP (par, 0, i) = GEN_INT (j);\n-  rtx sel = gen_rtx_VEC_SELECT (mode, mem_op, par);\n-  SET_SRC (body) = sel;\n-  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n-  df_insn_rescan (insn);\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"Replacing load %d with permuted load\\n\",\n-\t     INSN_UID (insn));\n-}\n-\n-/* Convert the non-permuting store INSN to a permuting one.  */\n-static void\n-permute_store (rtx_insn *insn)\n-{\n-  rtx body = PATTERN (insn);\n-  rtx src_reg = SET_SRC (body);\n-  machine_mode mode = GET_MODE (src_reg);\n-  int n_elts = GET_MODE_NUNITS (mode);\n-  int half_elts = n_elts / 2;\n-  rtx par = gen_rtx_PARALLEL (mode, rtvec_alloc (n_elts));\n-  int i, j;\n-  for (i = 0, j = half_elts; i < half_elts; ++i, ++j)\n-    XVECEXP (par, 0, i) = GEN_INT (j);\n-  for (i = half_elts, j = 0; j < half_elts; ++i, ++j)\n-    XVECEXP (par, 0, i) = GEN_INT (j);\n-  rtx sel = gen_rtx_VEC_SELECT (mode, src_reg, par);\n-  SET_SRC (body) = sel;\n-  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n-  df_insn_rescan (insn);\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"Replacing store %d with permuted store\\n\",\n-\t     INSN_UID (insn));\n-}\n-\n-/* Given OP that contains a vector extract operation, adjust the index\n-   of the extracted lane to account for the doubleword swap.  */\n-static void\n-adjust_extract (rtx_insn *insn)\n-{\n-  rtx pattern = PATTERN (insn);\n-  if (GET_CODE (pattern) == PARALLEL)\n-    pattern = XVECEXP (pattern, 0, 0);\n-  rtx src = SET_SRC (pattern);\n-  /* The vec_select may be wrapped in a vec_duplicate for a splat, so\n-     account for that.  */\n-  rtx sel = GET_CODE (src) == VEC_DUPLICATE ? XEXP (src, 0) : src;\n-  rtx par = XEXP (sel, 1);\n-  int half_elts = GET_MODE_NUNITS (GET_MODE (XEXP (sel, 0))) >> 1;\n-  int lane = INTVAL (XVECEXP (par, 0, 0));\n-  lane = lane >= half_elts ? lane - half_elts : lane + half_elts;\n-  XVECEXP (par, 0, 0) = GEN_INT (lane);\n-  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n-  df_insn_rescan (insn);\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"Changing lane for extract %d\\n\", INSN_UID (insn));\n-}\n-\n-/* Given OP that contains a vector direct-splat operation, adjust the index\n-   of the source lane to account for the doubleword swap.  */\n-static void\n-adjust_splat (rtx_insn *insn)\n-{\n-  rtx body = PATTERN (insn);\n-  rtx unspec = XEXP (body, 1);\n-  int half_elts = GET_MODE_NUNITS (GET_MODE (unspec)) >> 1;\n-  int lane = INTVAL (XVECEXP (unspec, 0, 1));\n-  lane = lane >= half_elts ? lane - half_elts : lane + half_elts;\n-  XVECEXP (unspec, 0, 1) = GEN_INT (lane);\n-  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n-  df_insn_rescan (insn);\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"Changing lane for splat %d\\n\", INSN_UID (insn));\n-}\n-\n-/* Given OP that contains an XXPERMDI operation (that is not a doubleword\n-   swap), reverse the order of the source operands and adjust the indices\n-   of the source lanes to account for doubleword reversal.  */\n-static void\n-adjust_xxpermdi (rtx_insn *insn)\n-{\n-  rtx set = PATTERN (insn);\n-  rtx select = XEXP (set, 1);\n-  rtx concat = XEXP (select, 0);\n-  rtx src0 = XEXP (concat, 0);\n-  XEXP (concat, 0) = XEXP (concat, 1);\n-  XEXP (concat, 1) = src0;\n-  rtx parallel = XEXP (select, 1);\n-  int lane0 = INTVAL (XVECEXP (parallel, 0, 0));\n-  int lane1 = INTVAL (XVECEXP (parallel, 0, 1));\n-  int new_lane0 = 3 - lane1;\n-  int new_lane1 = 3 - lane0;\n-  XVECEXP (parallel, 0, 0) = GEN_INT (new_lane0);\n-  XVECEXP (parallel, 0, 1) = GEN_INT (new_lane1);\n-  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n-  df_insn_rescan (insn);\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"Changing lanes for xxpermdi %d\\n\", INSN_UID (insn));\n-}\n-\n-/* Given OP that contains a VEC_CONCAT operation of two doublewords,\n-   reverse the order of those inputs.  */\n-static void\n-adjust_concat (rtx_insn *insn)\n-{\n-  rtx set = PATTERN (insn);\n-  rtx concat = XEXP (set, 1);\n-  rtx src0 = XEXP (concat, 0);\n-  XEXP (concat, 0) = XEXP (concat, 1);\n-  XEXP (concat, 1) = src0;\n-  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n-  df_insn_rescan (insn);\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"Reversing inputs for concat %d\\n\", INSN_UID (insn));\n-}\n-\n-/* Given an UNSPEC_VPERM insn, modify the mask loaded from the\n-   constant pool to reflect swapped doublewords.  */\n-static void\n-adjust_vperm (rtx_insn *insn)\n-{\n-  /* We previously determined that the UNSPEC_VPERM was fed by a\n-     swap of a swapping load of a TOC-relative constant pool symbol.\n-     Find the MEM in the swapping load and replace it with a MEM for\n-     the adjusted mask constant.  */\n-  rtx set = PATTERN (insn);\n-  rtx mask_reg = XVECEXP (SET_SRC (set), 0, 2);\n-\n-  /* Find the swap.  */\n-  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n-  df_ref use;\n-  rtx_insn *swap_insn = 0;\n-  FOR_EACH_INSN_INFO_USE (use, insn_info)\n-    if (rtx_equal_p (DF_REF_REG (use), mask_reg))\n-      {\n-\tstruct df_link *def_link = DF_REF_CHAIN (use);\n-\tgcc_assert (def_link && !def_link->next);\n-\tswap_insn = DF_REF_INSN (def_link->ref);\n-\tbreak;\n-      }\n-  gcc_assert (swap_insn);\n-  \n-  /* Find the load.  */\n-  insn_info = DF_INSN_INFO_GET (swap_insn);\n-  rtx_insn *load_insn = 0;\n-  FOR_EACH_INSN_INFO_USE (use, insn_info)\n-    {\n-      struct df_link *def_link = DF_REF_CHAIN (use);\n-      gcc_assert (def_link && !def_link->next);\n-      load_insn = DF_REF_INSN (def_link->ref);\n-      break;\n-    }\n-  gcc_assert (load_insn);\n-\n-  /* Find the TOC-relative symbol access.  */\n-  insn_info = DF_INSN_INFO_GET (load_insn);\n-  rtx_insn *tocrel_insn = 0;\n-  FOR_EACH_INSN_INFO_USE (use, insn_info)\n-    {\n-      struct df_link *def_link = DF_REF_CHAIN (use);\n-      gcc_assert (def_link && !def_link->next);\n-      tocrel_insn = DF_REF_INSN (def_link->ref);\n-      break;\n-    }\n-  gcc_assert (tocrel_insn);\n-\n-  /* Find the embedded CONST_VECTOR.  We have to call toc_relative_expr_p\n-     to set tocrel_base; otherwise it would be unnecessary as we've\n-     already established it will return true.  */\n-  rtx base, offset;\n-  const_rtx tocrel_base;\n-  rtx tocrel_expr = SET_SRC (PATTERN (tocrel_insn));\n-  /* There is an extra level of indirection for small/large code models.  */\n-  if (GET_CODE (tocrel_expr) == MEM)\n-    tocrel_expr = XEXP (tocrel_expr, 0);\n-  if (!toc_relative_expr_p (tocrel_expr, false, &tocrel_base, NULL))\n-    gcc_unreachable ();\n-  split_const (XVECEXP (tocrel_base, 0, 0), &base, &offset);\n-  rtx const_vector = get_pool_constant (base);\n-  /* With the extra indirection, get_pool_constant will produce the\n-     real constant from the reg_equal expression, so get the real\n-     constant.  */\n-  if (GET_CODE (const_vector) == SYMBOL_REF)\n-    const_vector = get_pool_constant (const_vector);\n-  gcc_assert (GET_CODE (const_vector) == CONST_VECTOR);\n-\n-  /* Create an adjusted mask from the initial mask.  */\n-  unsigned int new_mask[16], i, val;\n-  for (i = 0; i < 16; ++i) {\n-    val = INTVAL (XVECEXP (const_vector, 0, i));\n-    if (val < 16)\n-      new_mask[i] = (val + 8) % 16;\n-    else\n-      new_mask[i] = ((val + 8) % 16) + 16;\n-  }\n-\n-  /* Create a new CONST_VECTOR and a MEM that references it.  */\n-  rtx vals = gen_rtx_PARALLEL (V16QImode, rtvec_alloc (16));\n-  for (i = 0; i < 16; ++i)\n-    XVECEXP (vals, 0, i) = GEN_INT (new_mask[i]);\n-  rtx new_const_vector = gen_rtx_CONST_VECTOR (V16QImode, XVEC (vals, 0));\n-  rtx new_mem = force_const_mem (V16QImode, new_const_vector);\n-  /* This gives us a MEM whose base operand is a SYMBOL_REF, which we\n-     can't recognize.  Force the SYMBOL_REF into a register.  */\n-  if (!REG_P (XEXP (new_mem, 0))) {\n-    rtx base_reg = force_reg (Pmode, XEXP (new_mem, 0));\n-    XEXP (new_mem, 0) = base_reg;\n-    /* Move the newly created insn ahead of the load insn.  */\n-    rtx_insn *force_insn = get_last_insn ();\n-    remove_insn (force_insn);\n-    rtx_insn *before_load_insn = PREV_INSN (load_insn);\n-    add_insn_after (force_insn, before_load_insn, BLOCK_FOR_INSN (load_insn));\n-    df_insn_rescan (before_load_insn);\n-    df_insn_rescan (force_insn);\n-  }\n-\n-  /* Replace the MEM in the load instruction and rescan it.  */\n-  XEXP (SET_SRC (PATTERN (load_insn)), 0) = new_mem;\n-  INSN_CODE (load_insn) = -1; /* Force re-recognition.  */\n-  df_insn_rescan (load_insn);\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"Adjusting mask for vperm %d\\n\", INSN_UID (insn));\n-}\n-\n-/* The insn described by INSN_ENTRY[I] can be swapped, but only\n-   with special handling.  Take care of that here.  */\n-static void\n-handle_special_swappables (swap_web_entry *insn_entry, unsigned i)\n-{\n-  rtx_insn *insn = insn_entry[i].insn;\n-  rtx body = PATTERN (insn);\n-\n-  switch (insn_entry[i].special_handling)\n-    {\n-    default:\n-      gcc_unreachable ();\n-    case SH_CONST_VECTOR:\n-      {\n-\t/* A CONST_VECTOR will only show up somewhere in the RHS of a SET.  */\n-\tgcc_assert (GET_CODE (body) == SET);\n-\trtx rhs = SET_SRC (body);\n-\tswap_const_vector_halves (rhs);\n-\tif (dump_file)\n-\t  fprintf (dump_file, \"Swapping constant halves in insn %d\\n\", i);\n-\tbreak;\n-      }\n-    case SH_SUBREG:\n-      /* A subreg of the same size is already safe.  For subregs that\n-\t select a smaller portion of a reg, adjust the index for\n-\t swapped doublewords.  */\n-      adjust_subreg_index (body);\n-      if (dump_file)\n-\tfprintf (dump_file, \"Adjusting subreg in insn %d\\n\", i);\n-      break;\n-    case SH_NOSWAP_LD:\n-      /* Convert a non-permuting load to a permuting one.  */\n-      permute_load (insn);\n-      break;\n-    case SH_NOSWAP_ST:\n-      /* Convert a non-permuting store to a permuting one.  */\n-      permute_store (insn);\n-      break;\n-    case SH_EXTRACT:\n-      /* Change the lane on an extract operation.  */\n-      adjust_extract (insn);\n-      break;\n-    case SH_SPLAT:\n-      /* Change the lane on a direct-splat operation.  */\n-      adjust_splat (insn);\n-      break;\n-    case SH_XXPERMDI:\n-      /* Change the lanes on an XXPERMDI operation.  */\n-      adjust_xxpermdi (insn);\n-      break;\n-    case SH_CONCAT:\n-      /* Reverse the order of a concatenation operation.  */\n-      adjust_concat (insn);\n-      break;\n-    case SH_VPERM:\n-      /* Change the mask loaded from the constant pool for a VPERM.  */\n-      adjust_vperm (insn);\n-      break;\n-    }\n-}\n-\n-/* Find the insn from the Ith table entry, which is known to be a\n-   register swap Y = SWAP(X).  Replace it with a copy Y = X.  */\n-static void\n-replace_swap_with_copy (swap_web_entry *insn_entry, unsigned i)\n-{\n-  rtx_insn *insn = insn_entry[i].insn;\n-  rtx body = PATTERN (insn);\n-  rtx src_reg = XEXP (SET_SRC (body), 0);\n-  rtx copy = gen_rtx_SET (SET_DEST (body), src_reg);\n-  rtx_insn *new_insn = emit_insn_before (copy, insn);\n-  set_block_for_insn (new_insn, BLOCK_FOR_INSN (insn));\n-  df_insn_rescan (new_insn);\n-\n-  if (dump_file)\n-    {\n-      unsigned int new_uid = INSN_UID (new_insn);\n-      fprintf (dump_file, \"Replacing swap %d with copy %d\\n\", i, new_uid);\n-    }\n-\n-  df_insn_delete (insn);\n-  remove_insn (insn);\n-  insn->set_deleted ();\n-}\n-\n-/* Dump the swap table to DUMP_FILE.  */\n-static void\n-dump_swap_insn_table (swap_web_entry *insn_entry)\n-{\n-  int e = get_max_uid ();\n-  fprintf (dump_file, \"\\nRelevant insns with their flag settings\\n\\n\");\n-\n-  for (int i = 0; i < e; ++i)\n-    if (insn_entry[i].is_relevant)\n-      {\n-\tswap_web_entry *pred_entry = (swap_web_entry *)insn_entry[i].pred ();\n-\tfprintf (dump_file, \"%6d %6d  \", i,\n-\t\t pred_entry && pred_entry->insn\n-\t\t ? INSN_UID (pred_entry->insn) : 0);\n-\tif (insn_entry[i].is_load)\n-\t  fputs (\"load \", dump_file);\n-\tif (insn_entry[i].is_store)\n-\t  fputs (\"store \", dump_file);\n-\tif (insn_entry[i].is_swap)\n-\t  fputs (\"swap \", dump_file);\n-\tif (insn_entry[i].is_live_in)\n-\t  fputs (\"live-in \", dump_file);\n-\tif (insn_entry[i].is_live_out)\n-\t  fputs (\"live-out \", dump_file);\n-\tif (insn_entry[i].contains_subreg)\n-\t  fputs (\"subreg \", dump_file);\n-\tif (insn_entry[i].is_128_int)\n-\t  fputs (\"int128 \", dump_file);\n-\tif (insn_entry[i].is_call)\n-\t  fputs (\"call \", dump_file);\n-\tif (insn_entry[i].is_swappable)\n-\t  {\n-\t    fputs (\"swappable \", dump_file);\n-\t    if (insn_entry[i].special_handling == SH_CONST_VECTOR)\n-\t      fputs (\"special:constvec \", dump_file);\n-\t    else if (insn_entry[i].special_handling == SH_SUBREG)\n-\t      fputs (\"special:subreg \", dump_file);\n-\t    else if (insn_entry[i].special_handling == SH_NOSWAP_LD)\n-\t      fputs (\"special:load \", dump_file);\n-\t    else if (insn_entry[i].special_handling == SH_NOSWAP_ST)\n-\t      fputs (\"special:store \", dump_file);\n-\t    else if (insn_entry[i].special_handling == SH_EXTRACT)\n-\t      fputs (\"special:extract \", dump_file);\n-\t    else if (insn_entry[i].special_handling == SH_SPLAT)\n-\t      fputs (\"special:splat \", dump_file);\n-\t    else if (insn_entry[i].special_handling == SH_XXPERMDI)\n-\t      fputs (\"special:xxpermdi \", dump_file);\n-\t    else if (insn_entry[i].special_handling == SH_CONCAT)\n-\t      fputs (\"special:concat \", dump_file);\n-\t    else if (insn_entry[i].special_handling == SH_VPERM)\n-\t      fputs (\"special:vperm \", dump_file);\n-\t  }\n-\tif (insn_entry[i].web_not_optimizable)\n-\t  fputs (\"unoptimizable \", dump_file);\n-\tif (insn_entry[i].will_delete)\n-\t  fputs (\"delete \", dump_file);\n-\tfputs (\"\\n\", dump_file);\n-      }\n-  fputs (\"\\n\", dump_file);\n-}\n-\n-/* Return RTX with its address canonicalized to (reg) or (+ reg reg).\n-   Here RTX is an (& addr (const_int -16)).  Always return a new copy\n-   to avoid problems with combine.  */\n-static rtx\n-alignment_with_canonical_addr (rtx align)\n-{\n-  rtx canon;\n-  rtx addr = XEXP (align, 0);\n-\n-  if (REG_P (addr))\n-    canon = addr;\n-\n-  else if (GET_CODE (addr) == PLUS)\n-    {\n-      rtx addrop0 = XEXP (addr, 0);\n-      rtx addrop1 = XEXP (addr, 1);\n-\n-      if (!REG_P (addrop0))\n-\taddrop0 = force_reg (GET_MODE (addrop0), addrop0);\n-\n-      if (!REG_P (addrop1))\n-\taddrop1 = force_reg (GET_MODE (addrop1), addrop1);\n-\n-      canon = gen_rtx_PLUS (GET_MODE (addr), addrop0, addrop1);\n-    }\n-\n-  else\n-    canon = force_reg (GET_MODE (addr), addr);\n-\n-  return gen_rtx_AND (GET_MODE (align), canon, GEN_INT (-16));\n-}\n-\n-/* Check whether an rtx is an alignment mask, and if so, return \n-   a fully-expanded rtx for the masking operation.  */\n-static rtx\n-alignment_mask (rtx_insn *insn)\n-{\n-  rtx body = PATTERN (insn);\n-\n-  if (GET_CODE (body) != SET\n-      || GET_CODE (SET_SRC (body)) != AND\n-      || !REG_P (XEXP (SET_SRC (body), 0)))\n-    return 0;\n-\n-  rtx mask = XEXP (SET_SRC (body), 1);\n-\n-  if (GET_CODE (mask) == CONST_INT)\n-    {\n-      if (INTVAL (mask) == -16)\n-\treturn alignment_with_canonical_addr (SET_SRC (body));\n-      else\n-\treturn 0;\n-    }\n-\n-  if (!REG_P (mask))\n-    return 0;\n-\n-  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n-  df_ref use;\n-  rtx real_mask = 0;\n-\n-  FOR_EACH_INSN_INFO_USE (use, insn_info)\n-    {\n-      if (!rtx_equal_p (DF_REF_REG (use), mask))\n-\tcontinue;\n-\n-      struct df_link *def_link = DF_REF_CHAIN (use);\n-      if (!def_link || def_link->next)\n-\treturn 0;\n-\n-      rtx_insn *const_insn = DF_REF_INSN (def_link->ref);\n-      rtx const_body = PATTERN (const_insn);\n-      if (GET_CODE (const_body) != SET)\n-\treturn 0;\n-\n-      real_mask = SET_SRC (const_body);\n-\n-      if (GET_CODE (real_mask) != CONST_INT\n-\t  || INTVAL (real_mask) != -16)\n-\treturn 0;\n-    }\n-\n-  if (real_mask == 0)\n-    return 0;\n-\n-  return alignment_with_canonical_addr (SET_SRC (body));\n-}\n-\n-/* Given INSN that's a load or store based at BASE_REG, look for a\n-   feeding computation that aligns its address on a 16-byte boundary.  */\n-static rtx\n-find_alignment_op (rtx_insn *insn, rtx base_reg)\n-{\n-  df_ref base_use;\n-  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n-  rtx and_operation = 0;\n-\n-  FOR_EACH_INSN_INFO_USE (base_use, insn_info)\n-    {\n-      if (!rtx_equal_p (DF_REF_REG (base_use), base_reg))\n-\tcontinue;\n-\n-      struct df_link *base_def_link = DF_REF_CHAIN (base_use);\n-      if (!base_def_link || base_def_link->next)\n-\tbreak;\n-\n-      /* With stack-protector code enabled, and possibly in other\n-\t circumstances, there may not be an associated insn for \n-\t the def.  */\n-      if (DF_REF_IS_ARTIFICIAL (base_def_link->ref))\n-\tbreak;\n-\n-      rtx_insn *and_insn = DF_REF_INSN (base_def_link->ref);\n-      and_operation = alignment_mask (and_insn);\n-      if (and_operation != 0)\n-\tbreak;\n-    }\n-\n-  return and_operation;\n-}\n-\n-struct del_info { bool replace; rtx_insn *replace_insn; };\n-\n-/* If INSN is the load for an lvx pattern, put it in canonical form.  */\n-static void\n-recombine_lvx_pattern (rtx_insn *insn, del_info *to_delete)\n-{\n-  rtx body = PATTERN (insn);\n-  gcc_assert (GET_CODE (body) == SET\n-\t      && GET_CODE (SET_SRC (body)) == VEC_SELECT\n-\t      && GET_CODE (XEXP (SET_SRC (body), 0)) == MEM);\n-\n-  rtx mem = XEXP (SET_SRC (body), 0);\n-  rtx base_reg = XEXP (mem, 0);\n-\n-  rtx and_operation = find_alignment_op (insn, base_reg);\n-\n-  if (and_operation != 0)\n-    {\n-      df_ref def;\n-      struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n-      FOR_EACH_INSN_INFO_DEF (def, insn_info)\n-\t{\n-\t  struct df_link *link = DF_REF_CHAIN (def);\n-\t  if (!link || link->next)\n-\t    break;\n-\n-\t  rtx_insn *swap_insn = DF_REF_INSN (link->ref);\n-\t  if (!insn_is_swap_p (swap_insn)\n-\t      || insn_is_load_p (swap_insn)\n-\t      || insn_is_store_p (swap_insn))\n-\t    break;\n-\n-\t  /* Expected lvx pattern found.  Change the swap to\n-\t     a copy, and propagate the AND operation into the\n-\t     load.  */\n-\t  to_delete[INSN_UID (swap_insn)].replace = true;\n-\t  to_delete[INSN_UID (swap_insn)].replace_insn = swap_insn;\n-\n-\t  XEXP (mem, 0) = and_operation;\n-\t  SET_SRC (body) = mem;\n-\t  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n-\t  df_insn_rescan (insn);\n-\t\t  \n-\t  if (dump_file)\n-\t    fprintf (dump_file, \"lvx opportunity found at %d\\n\",\n-\t\t     INSN_UID (insn));\n-\t}\n-    }\n-}\n-\n-/* If INSN is the store for an stvx pattern, put it in canonical form.  */\n-static void\n-recombine_stvx_pattern (rtx_insn *insn, del_info *to_delete)\n-{\n-  rtx body = PATTERN (insn);\n-  gcc_assert (GET_CODE (body) == SET\n-\t      && GET_CODE (SET_DEST (body)) == MEM\n-\t      && GET_CODE (SET_SRC (body)) == VEC_SELECT);\n-  rtx mem = SET_DEST (body);\n-  rtx base_reg = XEXP (mem, 0);\n-\n-  rtx and_operation = find_alignment_op (insn, base_reg);\n-\n-  if (and_operation != 0)\n-    {\n-      rtx src_reg = XEXP (SET_SRC (body), 0);\n-      df_ref src_use;\n-      struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n-      FOR_EACH_INSN_INFO_USE (src_use, insn_info)\n-\t{\n-\t  if (!rtx_equal_p (DF_REF_REG (src_use), src_reg))\n-\t    continue;\n-\n-\t  struct df_link *link = DF_REF_CHAIN (src_use);\n-\t  if (!link || link->next)\n-\t    break;\n-\n-\t  rtx_insn *swap_insn = DF_REF_INSN (link->ref);\n-\t  if (!insn_is_swap_p (swap_insn)\n-\t      || insn_is_load_p (swap_insn)\n-\t      || insn_is_store_p (swap_insn))\n-\t    break;\n-\n-\t  /* Expected stvx pattern found.  Change the swap to\n-\t     a copy, and propagate the AND operation into the\n-\t     store.  */\n-\t  to_delete[INSN_UID (swap_insn)].replace = true;\n-\t  to_delete[INSN_UID (swap_insn)].replace_insn = swap_insn;\n-\n-\t  XEXP (mem, 0) = and_operation;\n-\t  SET_SRC (body) = src_reg;\n-\t  INSN_CODE (insn) = -1; /* Force re-recognition.  */\n-\t  df_insn_rescan (insn);\n-\t\t  \n-\t  if (dump_file)\n-\t    fprintf (dump_file, \"stvx opportunity found at %d\\n\",\n-\t\t     INSN_UID (insn));\n-\t}\n-    }\n-}\n-\n-/* Look for patterns created from builtin lvx and stvx calls, and\n-   canonicalize them to be properly recognized as such.  */\n-static void\n-recombine_lvx_stvx_patterns (function *fun)\n-{\n-  int i;\n-  basic_block bb;\n-  rtx_insn *insn;\n-\n-  int num_insns = get_max_uid ();\n-  del_info *to_delete = XCNEWVEC (del_info, num_insns);\n-\n-  FOR_ALL_BB_FN (bb, fun)\n-    FOR_BB_INSNS (bb, insn)\n-    {\n-      if (!NONDEBUG_INSN_P (insn))\n-\tcontinue;\n-\n-      if (insn_is_load_p (insn) && insn_is_swap_p (insn))\n-\trecombine_lvx_pattern (insn, to_delete);\n-      else if (insn_is_store_p (insn) && insn_is_swap_p (insn))\n-\trecombine_stvx_pattern (insn, to_delete);\n-    }\n-\n-  /* Turning swaps into copies is delayed until now, to avoid problems\n-     with deleting instructions during the insn walk.  */\n-  for (i = 0; i < num_insns; i++)\n-    if (to_delete[i].replace)\n-      {\n-\trtx swap_body = PATTERN (to_delete[i].replace_insn);\n-\trtx src_reg = XEXP (SET_SRC (swap_body), 0);\n-\trtx copy = gen_rtx_SET (SET_DEST (swap_body), src_reg);\n-\trtx_insn *new_insn = emit_insn_before (copy,\n-\t\t\t\t\t       to_delete[i].replace_insn);\n-\tset_block_for_insn (new_insn,\n-\t\t\t    BLOCK_FOR_INSN (to_delete[i].replace_insn));\n-\tdf_insn_rescan (new_insn);\n-\tdf_insn_delete (to_delete[i].replace_insn);\n-\tremove_insn (to_delete[i].replace_insn);\n-\tto_delete[i].replace_insn->set_deleted ();\n-      }\n-  \n-  free (to_delete);\n-}\n-\n-/* Main entry point for this pass.  */\n-unsigned int\n-rs6000_analyze_swaps (function *fun)\n-{\n-  swap_web_entry *insn_entry;\n-  basic_block bb;\n-  rtx_insn *insn, *curr_insn = 0;\n-\n-  /* Dataflow analysis for use-def chains.  */\n-  df_set_flags (DF_RD_PRUNE_DEAD_DEFS);\n-  df_chain_add_problem (DF_DU_CHAIN | DF_UD_CHAIN);\n-  df_analyze ();\n-  df_set_flags (DF_DEFER_INSN_RESCAN);\n-\n-  /* Pre-pass to recombine lvx and stvx patterns so we don't lose info.  */\n-  recombine_lvx_stvx_patterns (fun);\n-\n-  /* Allocate structure to represent webs of insns.  */\n-  insn_entry = XCNEWVEC (swap_web_entry, get_max_uid ());\n-\n-  /* Walk the insns to gather basic data.  */\n-  FOR_ALL_BB_FN (bb, fun)\n-    FOR_BB_INSNS_SAFE (bb, insn, curr_insn)\n-    {\n-      unsigned int uid = INSN_UID (insn);\n-      if (NONDEBUG_INSN_P (insn))\n-\t{\n-\t  insn_entry[uid].insn = insn;\n-\n-\t  if (GET_CODE (insn) == CALL_INSN)\n-\t    insn_entry[uid].is_call = 1;\n-\n-\t  /* Walk the uses and defs to see if we mention vector regs.\n-\t     Record any constraints on optimization of such mentions.  */\n-\t  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n-\t  df_ref mention;\n-\t  FOR_EACH_INSN_INFO_USE (mention, insn_info)\n-\t    {\n-\t      /* We use DF_REF_REAL_REG here to get inside any subregs.  */\n-\t      machine_mode mode = GET_MODE (DF_REF_REAL_REG (mention));\n-\n-\t      /* If a use gets its value from a call insn, it will be\n-\t\t a hard register and will look like (reg:V4SI 3 3).\n-\t\t The df analysis creates two mentions for GPR3 and GPR4,\n-\t\t both DImode.  We must recognize this and treat it as a\n-\t\t vector mention to ensure the call is unioned with this\n-\t\t use.  */\n-\t      if (mode == DImode && DF_REF_INSN_INFO (mention))\n-\t\t{\n-\t\t  rtx feeder = DF_REF_INSN (mention);\n-\t\t  /* FIXME:  It is pretty hard to get from the df mention\n-\t\t     to the mode of the use in the insn.  We arbitrarily\n-\t\t     pick a vector mode here, even though the use might\n-\t\t     be a real DImode.  We can be too conservative\n-\t\t     (create a web larger than necessary) because of\n-\t\t     this, so consider eventually fixing this.  */\n-\t\t  if (GET_CODE (feeder) == CALL_INSN)\n-\t\t    mode = V4SImode;\n-\t\t}\n-\n-\t      if (ALTIVEC_OR_VSX_VECTOR_MODE (mode) || mode == TImode)\n-\t\t{\n-\t\t  insn_entry[uid].is_relevant = 1;\n-\t\t  if (mode == TImode || mode == V1TImode\n-\t\t      || FLOAT128_VECTOR_P (mode))\n-\t\t    insn_entry[uid].is_128_int = 1;\n-\t\t  if (DF_REF_INSN_INFO (mention))\n-\t\t    insn_entry[uid].contains_subreg\n-\t\t      = !rtx_equal_p (DF_REF_REG (mention),\n-\t\t\t\t      DF_REF_REAL_REG (mention));\n-\t\t  union_defs (insn_entry, insn, mention);\n-\t\t}\n-\t    }\n-\t  FOR_EACH_INSN_INFO_DEF (mention, insn_info)\n-\t    {\n-\t      /* We use DF_REF_REAL_REG here to get inside any subregs.  */\n-\t      machine_mode mode = GET_MODE (DF_REF_REAL_REG (mention));\n-\n-\t      /* If we're loading up a hard vector register for a call,\n-\t\t it looks like (set (reg:V4SI 9 9) (...)).  The df\n-\t\t analysis creates two mentions for GPR9 and GPR10, both\n-\t\t DImode.  So relying on the mode from the mentions\n-\t\t isn't sufficient to ensure we union the call into the\n-\t\t web with the parameter setup code.  */\n-\t      if (mode == DImode && GET_CODE (insn) == SET\n-\t\t  && ALTIVEC_OR_VSX_VECTOR_MODE (GET_MODE (SET_DEST (insn))))\n-\t\tmode = GET_MODE (SET_DEST (insn));\n-\n-\t      if (ALTIVEC_OR_VSX_VECTOR_MODE (mode) || mode == TImode)\n-\t\t{\n-\t\t  insn_entry[uid].is_relevant = 1;\n-\t\t  if (mode == TImode || mode == V1TImode\n-\t\t      || FLOAT128_VECTOR_P (mode))\n-\t\t    insn_entry[uid].is_128_int = 1;\n-\t\t  if (DF_REF_INSN_INFO (mention))\n-\t\t    insn_entry[uid].contains_subreg\n-\t\t      = !rtx_equal_p (DF_REF_REG (mention),\n-\t\t\t\t      DF_REF_REAL_REG (mention));\n-\t\t  /* REG_FUNCTION_VALUE_P is not valid for subregs. */\n-\t\t  else if (REG_FUNCTION_VALUE_P (DF_REF_REG (mention)))\n-\t\t    insn_entry[uid].is_live_out = 1;\n-\t\t  union_uses (insn_entry, insn, mention);\n-\t\t}\n-\t    }\n-\n-\t  if (insn_entry[uid].is_relevant)\n-\t    {\n-\t      /* Determine if this is a load or store.  */\n-\t      insn_entry[uid].is_load = insn_is_load_p (insn);\n-\t      insn_entry[uid].is_store = insn_is_store_p (insn);\n-\n-\t      /* Determine if this is a doubleword swap.  If not,\n-\t\t determine whether it can legally be swapped.  */\n-\t      if (insn_is_swap_p (insn))\n-\t\tinsn_entry[uid].is_swap = 1;\n-\t      else\n-\t\t{\n-\t\t  unsigned int special = SH_NONE;\n-\t\t  insn_entry[uid].is_swappable\n-\t\t    = insn_is_swappable_p (insn_entry, insn, &special);\n-\t\t  if (special != SH_NONE && insn_entry[uid].contains_subreg)\n-\t\t    insn_entry[uid].is_swappable = 0;\n-\t\t  else if (special != SH_NONE)\n-\t\t    insn_entry[uid].special_handling = special;\n-\t\t  else if (insn_entry[uid].contains_subreg)\n-\t\t    insn_entry[uid].special_handling = SH_SUBREG;\n-\t\t}\n-\t    }\n-\t}\n-    }\n-\n-  if (dump_file)\n-    {\n-      fprintf (dump_file, \"\\nSwap insn entry table when first built\\n\");\n-      dump_swap_insn_table (insn_entry);\n-    }\n-\n-  /* Record unoptimizable webs.  */\n-  unsigned e = get_max_uid (), i;\n-  for (i = 0; i < e; ++i)\n-    {\n-      if (!insn_entry[i].is_relevant)\n-\tcontinue;\n-\n-      swap_web_entry *root\n-\t= (swap_web_entry*)(&insn_entry[i])->unionfind_root ();\n-\n-      if (insn_entry[i].is_live_in || insn_entry[i].is_live_out\n-\t  || (insn_entry[i].contains_subreg\n-\t      && insn_entry[i].special_handling != SH_SUBREG)\n-\t  || insn_entry[i].is_128_int || insn_entry[i].is_call\n-\t  || !(insn_entry[i].is_swappable || insn_entry[i].is_swap))\n-\troot->web_not_optimizable = 1;\n-\n-      /* If we have loads or stores that aren't permuting then the\n-\t optimization isn't appropriate.  */\n-      else if ((insn_entry[i].is_load || insn_entry[i].is_store)\n-\t  && !insn_entry[i].is_swap && !insn_entry[i].is_swappable)\n-\troot->web_not_optimizable = 1;\n-\n-      /* If we have permuting loads or stores that are not accompanied\n-\t by a register swap, the optimization isn't appropriate.  */\n-      else if (insn_entry[i].is_load && insn_entry[i].is_swap)\n-\t{\n-\t  rtx insn = insn_entry[i].insn;\n-\t  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n-\t  df_ref def;\n-\n-\t  FOR_EACH_INSN_INFO_DEF (def, insn_info)\n-\t    {\n-\t      struct df_link *link = DF_REF_CHAIN (def);\n-\n-\t      if (!chain_contains_only_swaps (insn_entry, link, FOR_LOADS))\n-\t\t{\n-\t\t  root->web_not_optimizable = 1;\n-\t\t  break;\n-\t\t}\n-\t    }\n-\t}\n-      else if (insn_entry[i].is_store && insn_entry[i].is_swap)\n-\t{\n-\t  rtx insn = insn_entry[i].insn;\n-\t  struct df_insn_info *insn_info = DF_INSN_INFO_GET (insn);\n-\t  df_ref use;\n-\n-\t  FOR_EACH_INSN_INFO_USE (use, insn_info)\n-\t    {\n-\t      struct df_link *link = DF_REF_CHAIN (use);\n-\n-\t      if (!chain_contains_only_swaps (insn_entry, link, FOR_STORES))\n-\t\t{\n-\t\t  root->web_not_optimizable = 1;\n-\t\t  break;\n-\t\t}\n-\t    }\n-\t}\n-    }\n-\n-  if (dump_file)\n-    {\n-      fprintf (dump_file, \"\\nSwap insn entry table after web analysis\\n\");\n-      dump_swap_insn_table (insn_entry);\n-    }\n-\n-  /* For each load and store in an optimizable web (which implies\n-     the loads and stores are permuting), find the associated\n-     register swaps and mark them for removal.  Due to various\n-     optimizations we may mark the same swap more than once.  Also\n-     perform special handling for swappable insns that require it.  */\n-  for (i = 0; i < e; ++i)\n-    if ((insn_entry[i].is_load || insn_entry[i].is_store)\n-\t&& insn_entry[i].is_swap)\n-      {\n-\tswap_web_entry* root_entry\n-\t  = (swap_web_entry*)((&insn_entry[i])->unionfind_root ());\n-\tif (!root_entry->web_not_optimizable)\n-\t  mark_swaps_for_removal (insn_entry, i);\n-      }\n-    else if (insn_entry[i].is_swappable && insn_entry[i].special_handling)\n-      {\n-\tswap_web_entry* root_entry\n-\t  = (swap_web_entry*)((&insn_entry[i])->unionfind_root ());\n-\tif (!root_entry->web_not_optimizable)\n-\t  handle_special_swappables (insn_entry, i);\n-      }\n-\n-  /* Now delete the swaps marked for removal.  */\n-  for (i = 0; i < e; ++i)\n-    if (insn_entry[i].will_delete)\n-      replace_swap_with_copy (insn_entry, i);\n-\n-  /* Clean up.  */\n-  free (insn_entry);\n-  return 0;\n-}\n-\n-const pass_data pass_data_analyze_swaps =\n-{\n-  RTL_PASS, /* type */\n-  \"swaps\", /* name */\n-  OPTGROUP_NONE, /* optinfo_flags */\n-  TV_NONE, /* tv_id */\n-  0, /* properties_required */\n-  0, /* properties_provided */\n-  0, /* properties_destroyed */\n-  0, /* todo_flags_start */\n-  TODO_df_finish, /* todo_flags_finish */\n-};\n-\n-class pass_analyze_swaps : public rtl_opt_pass\n-{\n-public:\n-  pass_analyze_swaps(gcc::context *ctxt)\n-    : rtl_opt_pass(pass_data_analyze_swaps, ctxt)\n-  {}\n-\n-  /* opt_pass methods: */\n-  virtual bool gate (function *)\n-    {\n-      return (optimize > 0 && !BYTES_BIG_ENDIAN && TARGET_VSX\n-\t      && !TARGET_P9_VECTOR && rs6000_optimize_swaps);\n-    }\n-\n-  virtual unsigned int execute (function *fun)\n-    {\n-      return rs6000_analyze_swaps (fun);\n-    }\n-\n-  opt_pass *clone ()\n-    {\n-      return new pass_analyze_swaps (m_ctxt);\n-    }\n-\n-}; // class pass_analyze_swaps\n-\n-rtl_opt_pass *\n-make_pass_analyze_swaps (gcc::context *ctxt)\n-{\n-  return new pass_analyze_swaps (ctxt);\n-}\n-\n #ifdef RS6000_GLIBC_ATOMIC_FENV\n /* Function declarations for rs6000_atomic_assign_expand_fenv.  */\n static tree atomic_hold_decl, atomic_clear_decl, atomic_update_decl;"}, {"sha": "304f322f435c449b56cd81bcacfa01514bca300d", "filename": "gcc/config/rs6000/t-rs6000", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5/gcc%2Fconfig%2Frs6000%2Ft-rs6000", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5/gcc%2Fconfig%2Frs6000%2Ft-rs6000", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Ft-rs6000?ref=0dc6645fc3b10c78c02d3543d344b9b5fba0d0d5", "patch": "@@ -30,6 +30,10 @@ rs6000-string.o: $(srcdir)/config/rs6000/rs6000-string.c\n \t$(COMPILE) $<\n \t$(POSTCOMPILE)\n \n+rs6000-p8swap.o: $(srcdir)/config/rs6000/rs6000-p8swap.c\n+\t$(COMPILE) $<\n+\t$(POSTCOMPILE)\n+\n $(srcdir)/config/rs6000/rs6000-tables.opt: $(srcdir)/config/rs6000/genopt.sh \\\n   $(srcdir)/config/rs6000/rs6000-cpus.def\n \t$(SHELL) $(srcdir)/config/rs6000/genopt.sh $(srcdir)/config/rs6000 > \\"}]}