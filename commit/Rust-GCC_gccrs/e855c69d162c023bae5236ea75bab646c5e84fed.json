{"sha": "e855c69d162c023bae5236ea75bab646c5e84fed", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZTg1NWM2OWQxNjJjMDIzYmFlNTIzNmVhNzViYWI2NDZjNWU4NGZlZA==", "commit": {"author": {"name": "Andrey Belevantsev", "email": "abel@ispras.ru", "date": "2008-09-01T08:57:00Z"}, "committer": {"name": "Andrey Belevantsev", "email": "abel@gcc.gnu.org", "date": "2008-09-01T08:57:00Z"}, "message": "sel-sched.h, [...]: New files.\n\n2008-08-31  Andrey Belevantsev  <abel@ispras.ru>\n        Dmitry Melnik  <dm@ispras.ru>\n        Dmitry Zhurikhin  <zhur@ispras.ru>\n        Alexander Monakov  <amonakov@ispras.ru>\n        Maxim Kuvyrkov  <maxim@codesourcery.com>\n\n\t* sel-sched.h, sel-sched-dump.h, sel-sched-ir.h, sel-sched.c,\n\tsel-sched-dump.c, sel-sched-ir.c: New files.\n\t* Makefile.in (OBJS-common): Add selective scheduling object\n\tfiles.\n\t(sel-sched.o, sel-sched-dump.o, sel-sched-ir.o): New entries.\n\t(SEL_SCHED_IR_H, SEL_SCHED_DUMP_H): New entries.\n\t(sched-vis.o): Add dependency on $(INSN_ATTR_H).\n\t* cfghooks.h (get_cfg_hooks, set_cfg_hooks): New prototypes.\n\t* cfghooks.c (get_cfg_hooks, set_cfg_hooks): New functions.\n\t(make_forwarder_block): Update loop latch if we have redirected\n\tthe loop latch edge.\n\t* cfgloop.c (get_loop_body_in_custom_order): New function.\n\t* cfgloop.h (LOOPS_HAVE_FALLTHRU_PREHEADERS): New enum field.\n\t(CP_FALLTHRU_PREHEADERS): Likewise.\n\t(get_loop_body_in_custom_order): Declare.\n\t* cfgloopmanip.c (has_preds_from_loop): New.\n\t(create_preheader): Honor CP_FALLTHRU_PREHEADERS.\n\tAssert that the preheader edge will be fall thru when it is set.\n\t* common.opt (fsel-sched-bookkeeping, fsel-sched-pipelining,\n\tfsel-sched-pipelining-outer-loops, fsel-sched-renaming,\n\tfsel-sched-substitution, fselective-scheduling): New flags.\n    \t* cse.c (hash_rtx_cb): New.\n\t(hash_rtx): Use it.\n\t* dbgcnt.def (sel_sched_cnt, sel_sched_region_cnt,\n\tsel_sched_insn_cnt): New counters. \n\t* final.c (compute_alignments): Export.  Free dominance info after loop_optimizer_finalize.\n\t* genattr.c (main): Output maximal_insn_latency prototype.\n\t* genautomata.c (output_default_latencies): New. Factor its code from ...\n\t(output_internal_insn_latency_func): ... here.\n\t(output_internal_maximal_insn_latency_func): New.\n\t(output_maximal_insn_latency_func): New.\n\t* hard-reg-set.h (UHOST_BITS_PER_WIDE_INT): Define unconditionally.\n\t(struct hard_reg_set_iterator): New.\n\t(hard_reg_set_iter_init, hard_reg_set_iter_set,\n\thard_reg_set_iter_next): New functions.\n\t(EXECUTE_IF_SET_IN_HARD_REG_SET): New macro.\n\t* lists.c (remove_free_INSN_LIST_node,\n\tremove_free_EXPR_LIST_node): New functions.\n\t* loop-init.c (loop_optimizer_init): When LOOPS_HAVE_FALLTHRU_PREHEADERS,\n\tset CP_FALLTHRU_PREHEADERS when calling create_preheaders.\n\t(loop_optimizer_finalize): Do not verify flow info after reload.\n\t* recog.c (validate_replace_rtx_1): New parameter simplify.\n\tDefault it to true.  Update all uses.  Factor out simplifying\n\tcode to ...\n\t(simplify_while_replacing): ... this new function.\n\t(validate_replace_rtx_part,\n\tvalidate_replace_rtx_part_nosimplify): New.\n\t* recog.h (validate_replace_rtx_part,\n\tvalidate_replace_rtx_part_nosimplify): Declare.\n\t* rtl.c (rtx_equal_p_cb): New.\n\t(rtx_equal_p): Use it.\n\t* rtl.h (rtx_equal_p_cb, hash_rtx_cb): Declare.\n\t(remove_free_INSN_LIST_NODE, remove_free_EXPR_LIST_node,\n\tdebug_bb_n_slim, debug_bb_slim,    print_rtl_slim): Likewise.\n\t* vecprim.h: Add a vector type for unsigned int. \n\t* haifa-sched.c: Include vecprim.h and cfgloop.h.\n\t(issue_rate, sched_verbose_param, note_list, dfa_state_size,\n\tready_try, cycle_issued_insns, spec_info): Make global.\n\t(readyp): Initialize.\n\t(dfa_lookahead): New global variable.\n\t(old_max_uid, old_last_basic_block): Remove.\n\t(h_i_d): Make it a vector.\n\t(INSN_TICK, INTER_TICK, QUEUE_INDEX, INSN_COST): Make them work\n\tthrough HID macro.\n\t(after_recovery, adding_bb_to_current_region_p):\n\tNew variables to handle correct insertion of the recovery code.\n\t(struct ready_list): Move declaration to sched-int.h.\n\t(rgn_n_insns): Removed.\n\t(rtx_vec_t): Move to sched-int.h.\n\t(find_insn_reg_weight): Remove.\n\t(find_insn_reg_weight1): Rename to find_insn_reg_weight.\n\t(haifa_init_h_i_d, haifa_finish_h_i_d):\n\tNew functions to initialize / finalize haifa instruction data.\n\t(extend_h_i_d, init_h_i_d): Rewrite.\n\t(unlink_other_notes): Move logic to add_to_note_list.  Handle\n\tselective scheduler.\n\t(ready_lastpos, ready_element, ready_sort, reemit_notes,\n\tfind_fallthru_edge): Make global, remove static prototypes.\n\t(max_issue): Make global.  Add privileged_n and state parameters.  Use\n\tthem.  \n\t(extend_global, extend_all): Removed.\n\t(init_before_recovery): Add new param.  Fix the handling of the case\n\twhen we insert a recovery code before the EXIT which has a predecessor\n\twith a fallthrough edge to it.\n\t(create_recovery_block): Make global.  Rename to\n\tsched_create_recovery_block.  Update.\n\t(change_pattern): Rename to sched_change_pattern.  Make global.\n\t(speculate_insn): Rename to sched_speculate_insn.  Make global.\n\tSplit haifa-specific functionality into ...\n\t(haifa_change_pattern): New static function.\n\t(sched_extend_bb): New static function.\n\t(sched_init_bbs): New function.\n\t(current_sched_info): Change type to struct haifa_sched_info.\n\t(insn_cost): Adjust for selective scheduling.\n\t(dep_cost_1): New function.  Move logic from ...\n\t(dep_cost): ... here.\n\t(dep_cost): Use dep_cost_1.\n\t(contributes_to_priority_p): Use sched_deps_info instead of\n\tcurrent_sched_info.\n\t(priority): Adjust to work with selective scheduling.  Process the\n\tcorner case when all dependencies don't contribute to priority.\n\t(rank_for_schedule): Use ds_weak instead of dep_weak.\n\t(advance_state): New function.  Move logic from ...\n\t(advance_one_cycle): ... here.\n\t(add_to_note_list, concat_note_lists): New functions.\n\t(rm_other_notes): Make static.  Adjust for selective scheduling.\n\t(remove_notes, restore_other_notes): New functions.\n\t(move_insn): Add two arguments.  Update assert.  Don't call\n\treemit_notes.\n\t(choose_ready): Remove lookahead variable, use dfa_lookahead.\n\tRemove more_issue, max_points.  Move the code to initialize\n\tmax_lookahead_tries to max_issue.\n\t(schedule_block): Remove rgn_n_insns1 parameter.  Don't allocate\n\tready.  Adjust use of move_insn.  Call restore_other_notes.\n\t(luid): Remove.\n\t(sched_init, sched_finish): Move Haifa-specific initialization/\n\tfinalization to ...\n\t(haifa_sched_init, haifa_sched_finish): ... respectively.\n\tNew functions.\n\t(setup_sched_dump): New function.\n\t(haifa_init_only_bb): New static function.\n\t(haifa_speculate_insn): New static function.\n\t(try_ready): Use haifa_* instead of speculate_insn and\n\tchange_pattern.\n\t(extend_ready, extend_all): Remove.\n\t(sched_extend_ready_list, sched_finish_ready_list): New functions.\n\t(create_check_block_twin, add_to_speculative_block): Use\n\thaifa_insns_init instead of extend_global.  Update to use new\n\tinitialization functions.  Change parameter.  Factor out code from\n\tcreate_check_block_twin to ...\n\t(sched_create_recovery_edges) ... this new function.\n\t(add_block): Remove.\n\t(sched_scan_info): New.\n\t(extend_bb): Use sched_scan_info.\n\t(init_bb, extend_insn, init_insn, init_insns_in_bb, sched_scan): New\n\tstatic functions for walking through scheduling region.\n\t(sched_luids): New vector variable to replace uid_to_luid.\n\t(luids_extend_insn): New function.\n\t(sched_max_luid): New variable.\n\t(luids_init_insn): New function.\n\t(sched_init_luids, sched_finish_luids): New functions.\n\t(insn_luid): New debug function.\n\t(sched_extend_target): New function.\n\t(haifa_init_insn): New static function.\n\t(sched_init_only_bb): New hook.\n\t(sched_split_block): New hook.\n\t(sched_split_block_1): New function.\n\t(sched_create_empty_bb): New hook.\n\t(sched_create_empty_bb_1): New function.\t\n\t(common_sched_info, ready): New global variables.\n\t(current_sched_info_var): Remove.\n\t(move_block_after_check): Use common_sched_info.\t\t\n\t(haifa_luid_for_non_insn): New static function.\t\n\t(init_before_recovery): Use haifa_init_only_bb instead of\n\tadd_block.\n\t(increase_insn_priority): New.\n\t* modulo-sched.c: (issue_rate): Remove static declaration.\n\t(sms_sched_info): Change type to haifa_sched_info.\n\t(sms_sched_deps_info, sms_common_sched_info): New variables.\n\t(setup_sched_infos): New.\n\t(sms_schedule): Initialize them.  Call haifa_sched_init/finish.\n\tDo not call regstat_free_calls_crossed.\n\t(sms_print_insn): Use const_rtx.\n\t* params.def (PARAM_MAX_PIPELINE_REGION_BLOCKS,\n\tPARAM_MAX_PIPELINE_REGION_INSNS, PARAM_SELSCHED_MAX_LOOKAHEAD,\n\tPARAM_SELSCHED_MAX_SCHED_TIMES, PARAM_SELSCHED_INSNS_TO_RENAME,\n\tPARAM_SCHED_MEM_TRUE_DEP_COST): New.\n\t* sched-deps.c (sched_deps_info): New.  Update all relevant uses of\n\tcurrent_sched_info to use it.\n\t(enum reg_pending_barrier_mode): Move to sched-int.h.\n\t(h_d_i_d): New variable. Initialize to NULL.\n\t({true, output, anti, spec, forward}_dependency_cache): Initialize\n\tto NULL.\n\t(estimate_dep_weak): Remove static declaration.\n\t(sched_has_condition_p): New function.  Adjust users of\n\tsched_get_condition to use it instead.\n\t(conditions_mutex_p): Add arguments indicating which conditions are\n\treversed.  Use them.\n\t(sched_get_condition_with_rev): Rename from sched_get_condition.  Add\n\targument to indicate whether returned condition is reversed.  Do not\n\tgenerate new rtx when condition should be reversed; indicate it by\n\tsetting new argument instead.\n\t(add_dependence_list_and_free): Add deps parameter.\n\tUpdate all users.  Do not free dependence list when\n\tdeps context is readonly.\n\t(add_insn_mem_dependence, flush_pending_lists): Adjust for readonly\n\tcontexts.\n\t(remove_from_dependence_list, remove_from_both_dependence_lists): New.\n\t(remove_from_deps): New. Use the above functions.\t\n\t(cur_insn, can_start_lhs_rhs_p): New static variables.\n\t(add_or_update_back_dep_1): Initialize present_dep_type.\n\t(haifa_start_insn, haifa_finish_insn, haifa_note_reg_set,\n\thaifa_note_reg_clobber, haifa_note_reg_use, haifa_note_mem_dep,\n\thaifa_note_dep): New functions implementing dependence hooks for\n\tthe Haifa scheduler.\n\t(note_reg_use, note_reg_set, note_reg_clobber, note_mem_dep,\n\tnote_dep): New functions.\n\t(ds_to_dt, extend_deps_reg_info, maybe_extend_reg_info_p): New\n\tfunctions.\n\t(init_deps): Initialize last_reg_pending_barrier and deps->readonly.\n\t(free_deps): Initialize deps->reg_last.\n\t(sched_analyze_reg, sched_analyze_1, sched_analyze_2,\n\tsched_analyze_insn): Update to use dependency hooks infrastructure\n\tand readonly contexts.\n\t(deps_analyze_insn): New function.  Move part of logic from ...\n\t(sched_analyze): ... here.  Also move some logic to ...\n\t(deps_start_bb): ... here.  New function.\n\t(add_forw_dep, delete_forw_dep): Guard use of INSN_DEP_COUNT with\n\tsel_sched_p.\n\t(sched_deps_init): New function.  Move code from ...\n\t(init_dependency_caches): ... here.  Remove.\n\t(init_deps_data_vector): New.\n\t(sched_deps_finish): New function.  Move code from ...\n\t(free_dependency_caches): ... here.  Remove.\n\t(init_deps_global, finish_deps_global): Adjust for use with\n\tselective scheduling.\n\t(get_dep_weak): Move logic to ...\n\t(get_dep_weak_1): New function.\n\t(ds_merge): Move logic to ...\n\t(ds_merge_1): New static function.\n\t(ds_full_merge, ds_max_merge, ds_get_speculation_types): New functions.\n\t(ds_get_max_dep_weak): New function.\n\t* sched-ebb.c (sched_n_insns): Rename to sched_rgn_n_insns.\n\t(n_insns): Rename to rgn_n_insns.\n\t(debug_ebb_dependencies): New function.\n\t(init_ready_list): Use it.\n\t(begin_schedule_ready): Use sched_init_only_bb.\n\t(ebb_print_insn): Indicate when an insn starts a new cycle.\n\t(contributes_to_priority, compute_jump_reg_dependencies,\n\tadd_remove_insn, fix_recovery_cfg): Add ebb_ prefix to function names.\n\t(add_block1): Remove to ebb_add_block.\n\t(ebb_sched_deps_info, ebb_common_sched_info): New variables.\n\t(schedule_ebb): Initialize them.  Use remove_notes instead of\n\trm_other_notes.  Use haifa_local_init/finish.\n\t(schedule_ebbs): Use haifa_sched_init/finish.\n\t* sched-int.h: Include vecprim.h, remove rtl.h.\n\t(struct ready_list): Delete declaration.\n\t(sched_verbose_param, enum sched_pass_id_t,\n\tbb_vec_t, insn_vec_t, rtx_vec_t): New.\n\t(struct sched_scan_info_def): New structure.\n\t(sched_scan_info, sched_scan, sched_init_bbs,\n\tsched_init_luids, sched_finish_luids, sched_extend_target,\n\thaifa_init_h_i_d, haifa_finish_h_i_d): Declare.\n\t(struct common_sched_info_def): New.\n\t(common_sched_info, haifa_common_sched_info,\n\tsched_emulate_haifa_p): Declare.\n\t(sel_sched_p): New.\n\t(sched_luids): Declare.\n\t(INSN_LUID, LUID_BY_UID, SET_INSN_LUID): Declare.\n\t(sched_max_luid, insn_luid): Declare.\n\t(note_list, remove_notes, restore_other_notes, bb_note): Declare.\n\t(sched_insns_init, sched_insns_finish, xrecalloc, reemit_notes,\n\tprint_insn, print_pattern, print_value, haifa_classify_insn,\n\tsel_find_rgns, sel_mark_hard_insn, dfa_state_size, advance_state,\n\tsetup_sched_dump, sched_init, sched_finish,\n\tsel_insn_is_speculation_check): Export.\n\t(struct ready_list): Move from haifa-sched.c.\n\t(ready_try, ready, max_issue): Export.\n\t(ebb_compute_jump_reg_dependencies, find_fallthru_edge,\n\tsched_init_only_bb, sched_split_block, sched_split_block_1,\n\tsched_create_empty_bb, sched_create_empty_bb_1,\n\tsched_create_recovery_block, sched_create_recovery_edges): Export.\n\t(enum reg_pending_barrier_mode): Export.\n\t(struct deps): New fields `last_reg_pending_barrier' and `readonly'.\n\t(deps_t): New.\n\t(struct sched_info): Rename to haifa_sched_info.  Use const_rtx for\n\tprint_insn field.  Move add_block and fix_recovery_cfg to\n\tcommon_sched_info_def.  Move compute_jump_reg_dependencies, use_cselib  ...\n\t(struct sched_deps_info_def): ... this new structure.\n\t(sched_deps_info): Declare.\n\t(struct spec_info_def): Remove weakness_cutoff, add\n\tdata_weakness_cutoff and control_weakness_cutoff.\n\t(spec_info): Declare.\n\t(struct _haifa_deps_insn_data): Split from haifa_insn_data.  Add\n\tdep_count field.\n\t(struct haifa_insn_data): Rename to struct _haifa_insn_data.\n\t(haifa_insn_data_def, haifa_insn_data_t): New typedefs.\n\t(current_sched_info): Change type to struct haifa_sched_info.\n\t(haifa_deps_insn_data_def, haifa_deps_insn_data_t): New typedefs.\n\t(h_d_i_d): New variable.\n\t(HDID): New accessor macro.\n\t(h_i_d): Change type to VEC (haifa_insn_data_def, heap) *.\n\t(HID): New accessor macro.  Rewrite h_i_d accessor macros through HID\n\tand HDID.\n\t(IS_SPECULATION_CHECK_P): Update for selective scheduler.\n\t(enum SCHED_FLAGS): Update for selective scheduler.\n\t(enum SPEC_SCHED_FLAGS): New flag SEL_SCHED_SPEC_DONT_CHECK_CONTROL.\n\t(init_dependency_caches, free_dependency_caches): Delete declarations.\n\t(deps_analyze_insn, remove_from_deps, get_dep_weak_1,\n\testimate_dep_weak, ds_full_merge, ds_max_merge, ds_weak,\n\tds_get_speculation_types, ds_get_max_dep_weak, sched_deps_init,\n\tsched_deps_finish, haifa_note_reg_set, haifa_note_reg_use,\n\thaifa_note_reg_clobber, maybe_extend_reg_info_p, deps_start_bb,\n\tds_to_dt): Export.\n\t(rm_other_notes): Delete declaration.\n\t(schedule_block): Remove one argument.\n\t(cycle_issued_insns, issue_rate, dfa_lookahead, ready_sort,\n\tready_element, ready_lastpos, sched_extend_ready_list,\n\tsched_finish_ready_list, sched_change_pattern, sched_speculate_insn,\n\tconcat_note_lists): Export.\n\t(struct region): Move from sched-rgn.h.\n\t(nr_regions, rgn_table, rgn_bb_table, block_to_bb, containing_rgn,\n\tRGN_NR_BLOCKS, RGN_BLOCKS, RGN_DONT_CALC_DEPS, RGN_HAS_REAL_EBB,\n\tBLOCK_TO_BB, CONTAINING_RGN): Export.\n\t(ebb_head, BB_TO_BLOCK, EBB_FIRST_BB, EBB_LAST_BB, INSN_BB): Likewise.\n\t(current_nr_blocks, current_blocks, target_bb): Likewise.\n\t(dep_cost_1, sched_is_disabled_for_current_region_p, sched_rgn_init,\n\tsched_rgn_finish, rgn_setup_region, sched_rgn_compute_dependencies,\n\tsched_rgn_local_init, extend_regions,\n\trgn_make_new_region_out_of_new_block, compute_priorities,\n\tdebug_rgn_dependencies, free_rgn_deps, contributes_to_priority,\n\textend_rgns, deps_join rgn_setup_common_sched_info,\n\trgn_setup_sched_infos, debug_regions, debug_region, dump_region_dot,\n\tdump_region_dot_file, haifa_sched_init, haifa_sched_finish): Export.\n\t(get_rgn_sched_max_insns_priority, sel_add_to_insn_priority, \n\tincrease_insn_priority): Likewise.\n\t* sched-rgn.c: Include sel-sched.h.\n\t(ref_counts): New static variable.  Use it ...\n\t(INSN_REF_COUNT): ... here.  Rewrite and move closer to uses.\n\t(FED_BY_SPEC_LOAD, IS_LOAD_INSN): Rewrite to use HID accessor macro.\n\t(sched_is_disabled_for_current_region_p): Delete static declaration.\n\t(struct region): Move to sched-int.h.\n\t(nr_regions, rgn_table, rgn_bb_table, block_to_bb, containing_rgn,\n\tebb_head): Define and initialize.\n\t(RGN_NR_BLOCKS, RGN_BLOCKS, RGN_DONT_CALC_DEPS, RGN_HAS_REAL_EBB,\n\tBLOCK_TO_BB, CONTAINING_RGN, debug_regions, extend_regions,\n\tBB_TO_BLOCK, EBB_FIRST_BB, EBB_LAST_BB): Move to\n\tsched-int.h.\n\t(find_single_block_region): Add new argument to indicate that EBB\n\tregions should be constructed.\n\t(debug_live): Delete declaration.\n\t(current_nr_blocks, current_blocks, target_bb): Remove static qualifiers.\n\t(compute_dom_prob_ps, check_live, update_live, set_spec_fed): Delete\n\tdeclaration.\n\t(init_regions): Delete declaration.\n\t(debug_region, bb_in_region_p, dump_region_dot_file, dump_region_dot,\n\trgn_estimate_number_of_insns): New.\n\t(too_large): Use estimate_number_of_insns.\n\t(haifa_find_rgns): New. Move the code from ...\n\t(find_rgns): ... here.  Call either sel_find_rgns or haifa_find_rgns.\n\t(free_trg_info): New.\n\t(compute_trg_info): Allocate candidate tables here instead of ...\n\t(init_ready_list): ... here.\n\t(rgn_print_insn): Use const_rtx.\n\t(contributes_to_priority, extend_regions): Delete static declaration.\n\t(add_remove_insn, fix_recovery_cfg): Add rgn_ to function names.\n\t(add_block1): Rename to rgn_add_block.\n\t(debug_rgn_dependencies): Delete static qualifier.\n\t(new_ready): Use sched_deps_info.  Simplify.\n\t(rgn_common_sched_info, rgn_const_sched_deps_info,\n\trgn_const_sel_sched_deps_info, rgn_sched_deps_info, rgn_sched_info): New.\n\t(region_sched_info): Rename to rgn_const_sched_info.\n\t(deps_join): New, extracted from ...\n\t(propagate_deps): ... here.\n\t(compute_block_dependences, debug_dependencies): Update for selective\n\tscheduling.\n\t(free_rgn_deps, compute_priorities): New functions.\n\t(sched_rgn_init, sched_rgn_finish, rgn_setup_region,\n\tsched_rgn_compute_dependencies): New functions.\n\t(schedule_region): Use them.\n\t(sched_rgn_local_init, sched_rgn_local_free, sched_rgn_local_finish,\n\trgn_setup_common_sched_info, rgn_setup_sched_infos):\n\tNew functions.\n\t(schedule_insns): Call new functions that were split out.\n\t(rgn_make_new_region_out_of_new_block): New.\n\t(get_rgn_sched_max_insns_priority): New.\n\t(rest_of_handle_sched, rest_of_handle_sched2): Call selective\n\tscheduling when appropriate.\n\t* sched-vis.c: Include insn-attr.h.\n\t(print_value, print_pattern): Make global.\n\t(print_rtl_slim, debug_bb_slim, debug_bb_n_slim): New functions.\n\t* target-def.h (TARGET_SCHED_ADJUST_COST_2,\n\tTARGET_SCHED_ALLOC_SCHED_CONTEXT, TARGET_SCHED_INIT_SCHED_CONTEXT,\n\tTARGET_SCHED_SET_SCHED_CONTEXT, TARGET_SCHED_CLEAR_SCHED_CONTEXT,\n\tTARGET_SCHED_FREE_SCHED_CONTEXT, TARGET_SCHED_GET_INSN_CHECKED_DS,\n\tTARGET_SCHED_GET_INSN_SPEC_DS, TARGET_SCHED_SKIP_RTX_P): New target\n\thooks.  Initialize them to 0.\n\t(TARGET_SCHED_GEN_CHECK): Rename to TARGET_SCHED_GEN_SPEC_CHECK.\n\t* target.h (struct gcc_target): Add them.  Rename gen_check field to\n\tgen_spec_check.\n\t* flags.h (sel_sched_switch_set): Declare.\n\t* opts.c (sel_sched_switch_set): New variable.\n\t(decode_options): Unset flag_sel_sched_pipelining_outer_loops if\n\tpipelining is disabled from command line.\n\t(common_handle_option): Record whether selective scheduling is\n\trequested from command line.\n\t* doc/invoke.texi: Document new flags and parameters.\n\t* doc/tm.texi: Document new target hooks.\n\t* config/ia64/ia64.c (TARGET_SCHED_GEN_SPEC_CHECK): Define to ia64_gen_check.\n\t(dfa_state_size): Do not declare locally.\n\t* config/ia64/ia64.opt (msched-ar-data-spec): Default to 0.\n\t* config/rs6000/rs6000.c (rs6000_init_sched_context,\n\trs6000_alloc_sched_context, rs6000_set_sched_context,\n\trs6000_free_sched_context): New functions.\n\t(struct _rs6000_sched_context): New.\n\t(rs6000_sched_reorder2): Do not modify INSN_PRIORITY for selective\n\tscheduling.\n\t(rs6000_sched_finish): Do not run for selective scheduling. \n\nCo-Authored-By: Alexander Monakov <amonakov@ispras.ru>\nCo-Authored-By: Dmitry Melnik <dm@ispras.ru>\nCo-Authored-By: Dmitry Zhurikhin <zhur@ispras.ru>\nCo-Authored-By: Maxim Kuvyrkov <maxim@codesourcery.com>\n\nFrom-SVN: r139854", "tree": {"sha": "137062af2fefb2287271b69ca6f0c7e9e8e57e11", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/137062af2fefb2287271b69ca6f0c7e9e8e57e11"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/e855c69d162c023bae5236ea75bab646c5e84fed", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e855c69d162c023bae5236ea75bab646c5e84fed", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e855c69d162c023bae5236ea75bab646c5e84fed", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e855c69d162c023bae5236ea75bab646c5e84fed/comments", "author": {"login": "abonzo", "id": 20396542, "node_id": "MDQ6VXNlcjIwMzk2NTQy", "avatar_url": "https://avatars.githubusercontent.com/u/20396542?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abonzo", "html_url": "https://github.com/abonzo", "followers_url": "https://api.github.com/users/abonzo/followers", "following_url": "https://api.github.com/users/abonzo/following{/other_user}", "gists_url": "https://api.github.com/users/abonzo/gists{/gist_id}", "starred_url": "https://api.github.com/users/abonzo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abonzo/subscriptions", "organizations_url": "https://api.github.com/users/abonzo/orgs", "repos_url": "https://api.github.com/users/abonzo/repos", "events_url": "https://api.github.com/users/abonzo/events{/privacy}", "received_events_url": "https://api.github.com/users/abonzo/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "f57ca1ea5f9ebcd518961e4251dd224524725f11", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f57ca1ea5f9ebcd518961e4251dd224524725f11", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f57ca1ea5f9ebcd518961e4251dd224524725f11"}], "stats": {"total": 22510, "additions": 20841, "deletions": 1669}, "files": [{"sha": "768b2967a819356606c8e6a7d4bcb0a32cd8dfb6", "filename": "gcc/ChangeLog", "status": "modified", "additions": 405, "deletions": 0, "changes": 405, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -1,3 +1,408 @@\n+2008-08-31  Andrey Belevantsev  <abel@ispras.ru>\n+        Dmitry Melnik  <dm@ispras.ru>\n+        Dmitry Zhurikhin  <zhur@ispras.ru>\n+        Alexander Monakov  <amonakov@ispras.ru>\n+        Maxim Kuvyrkov  <maxim@codesourcery.com>\n+\n+\t* sel-sched.h, sel-sched-dump.h, sel-sched-ir.h, sel-sched.c,\n+\tsel-sched-dump.c, sel-sched-ir.c: New files.\n+\t* Makefile.in (OBJS-common): Add selective scheduling object\n+\tfiles.\n+\t(sel-sched.o, sel-sched-dump.o, sel-sched-ir.o): New entries.\n+\t(SEL_SCHED_IR_H, SEL_SCHED_DUMP_H): New entries.\n+\t(sched-vis.o): Add dependency on $(INSN_ATTR_H).\n+\t* cfghooks.h (get_cfg_hooks, set_cfg_hooks): New prototypes.\n+\t* cfghooks.c (get_cfg_hooks, set_cfg_hooks): New functions.\n+\t(make_forwarder_block): Update loop latch if we have redirected\n+\tthe loop latch edge.\n+\t* cfgloop.c (get_loop_body_in_custom_order): New function.\n+\t* cfgloop.h (LOOPS_HAVE_FALLTHRU_PREHEADERS): New enum field.\n+\t(CP_FALLTHRU_PREHEADERS): Likewise.\n+\t(get_loop_body_in_custom_order): Declare.\n+\t* cfgloopmanip.c (has_preds_from_loop): New.\n+\t(create_preheader): Honor CP_FALLTHRU_PREHEADERS.\n+\tAssert that the preheader edge will be fall thru when it is set.\n+\t* common.opt (fsel-sched-bookkeeping, fsel-sched-pipelining,\n+\tfsel-sched-pipelining-outer-loops, fsel-sched-renaming,\n+\tfsel-sched-substitution, fselective-scheduling): New flags.\n+    \t* cse.c (hash_rtx_cb): New.\n+\t(hash_rtx): Use it.\n+\t* dbgcnt.def (sel_sched_cnt, sel_sched_region_cnt,\n+\tsel_sched_insn_cnt): New counters. \n+\t* final.c (compute_alignments): Export.  Free dominance info after loop_optimizer_finalize.\n+\t* genattr.c (main): Output maximal_insn_latency prototype.\n+\t* genautomata.c (output_default_latencies): New. Factor its code from ...\n+\t(output_internal_insn_latency_func): ... here.\n+\t(output_internal_maximal_insn_latency_func): New.\n+\t(output_maximal_insn_latency_func): New.\n+\t* hard-reg-set.h (UHOST_BITS_PER_WIDE_INT): Define unconditionally.\n+\t(struct hard_reg_set_iterator): New.\n+\t(hard_reg_set_iter_init, hard_reg_set_iter_set,\n+\thard_reg_set_iter_next): New functions.\n+\t(EXECUTE_IF_SET_IN_HARD_REG_SET): New macro.\n+\t* lists.c (remove_free_INSN_LIST_node,\n+\tremove_free_EXPR_LIST_node): New functions.\n+\t* loop-init.c (loop_optimizer_init): When LOOPS_HAVE_FALLTHRU_PREHEADERS,\n+\tset CP_FALLTHRU_PREHEADERS when calling create_preheaders.\n+\t(loop_optimizer_finalize): Do not verify flow info after reload.\n+\t* recog.c (validate_replace_rtx_1): New parameter simplify.\n+\tDefault it to true.  Update all uses.  Factor out simplifying\n+\tcode to ...\n+\t(simplify_while_replacing): ... this new function.\n+\t(validate_replace_rtx_part,\n+\tvalidate_replace_rtx_part_nosimplify): New.\n+\t* recog.h (validate_replace_rtx_part,\n+\tvalidate_replace_rtx_part_nosimplify): Declare.\n+\t* rtl.c (rtx_equal_p_cb): New.\n+\t(rtx_equal_p): Use it.\n+\t* rtl.h (rtx_equal_p_cb, hash_rtx_cb): Declare.\n+\t(remove_free_INSN_LIST_NODE, remove_free_EXPR_LIST_node,\n+\tdebug_bb_n_slim, debug_bb_slim,    print_rtl_slim): Likewise.\n+\t* vecprim.h: Add a vector type for unsigned int. \n+\t* haifa-sched.c: Include vecprim.h and cfgloop.h.\n+\t(issue_rate, sched_verbose_param, note_list, dfa_state_size,\n+\tready_try, cycle_issued_insns, spec_info): Make global.\n+\t(readyp): Initialize.\n+\t(dfa_lookahead): New global variable.\n+\t(old_max_uid, old_last_basic_block): Remove.\n+\t(h_i_d): Make it a vector.\n+\t(INSN_TICK, INTER_TICK, QUEUE_INDEX, INSN_COST): Make them work\n+\tthrough HID macro.\n+\t(after_recovery, adding_bb_to_current_region_p):\n+\tNew variables to handle correct insertion of the recovery code.\n+\t(struct ready_list): Move declaration to sched-int.h.\n+\t(rgn_n_insns): Removed.\n+\t(rtx_vec_t): Move to sched-int.h.\n+\t(find_insn_reg_weight): Remove.\n+\t(find_insn_reg_weight1): Rename to find_insn_reg_weight.\n+\t(haifa_init_h_i_d, haifa_finish_h_i_d):\n+\tNew functions to initialize / finalize haifa instruction data.\n+\t(extend_h_i_d, init_h_i_d): Rewrite.\n+\t(unlink_other_notes): Move logic to add_to_note_list.  Handle\n+\tselective scheduler.\n+\t(ready_lastpos, ready_element, ready_sort, reemit_notes,\n+\tfind_fallthru_edge): Make global, remove static prototypes.\n+\t(max_issue): Make global.  Add privileged_n and state parameters.  Use\n+\tthem.  \n+\t(extend_global, extend_all): Removed.\n+\t(init_before_recovery): Add new param.  Fix the handling of the case\n+\twhen we insert a recovery code before the EXIT which has a predecessor\n+\twith a fallthrough edge to it.\n+\t(create_recovery_block): Make global.  Rename to\n+\tsched_create_recovery_block.  Update.\n+\t(change_pattern): Rename to sched_change_pattern.  Make global.\n+\t(speculate_insn): Rename to sched_speculate_insn.  Make global.\n+\tSplit haifa-specific functionality into ...\n+\t(haifa_change_pattern): New static function.\n+\t(sched_extend_bb): New static function.\n+\t(sched_init_bbs): New function.\n+\t(current_sched_info): Change type to struct haifa_sched_info.\n+\t(insn_cost): Adjust for selective scheduling.\n+\t(dep_cost_1): New function.  Move logic from ...\n+\t(dep_cost): ... here.\n+\t(dep_cost): Use dep_cost_1.\n+\t(contributes_to_priority_p): Use sched_deps_info instead of\n+\tcurrent_sched_info.\n+\t(priority): Adjust to work with selective scheduling.  Process the\n+\tcorner case when all dependencies don't contribute to priority.\n+\t(rank_for_schedule): Use ds_weak instead of dep_weak.\n+\t(advance_state): New function.  Move logic from ...\n+\t(advance_one_cycle): ... here.\n+\t(add_to_note_list, concat_note_lists): New functions.\n+\t(rm_other_notes): Make static.  Adjust for selective scheduling.\n+\t(remove_notes, restore_other_notes): New functions.\n+\t(move_insn): Add two arguments.  Update assert.  Don't call\n+\treemit_notes.\n+\t(choose_ready): Remove lookahead variable, use dfa_lookahead.\n+\tRemove more_issue, max_points.  Move the code to initialize\n+\tmax_lookahead_tries to max_issue.\n+\t(schedule_block): Remove rgn_n_insns1 parameter.  Don't allocate\n+\tready.  Adjust use of move_insn.  Call restore_other_notes.\n+\t(luid): Remove.\n+\t(sched_init, sched_finish): Move Haifa-specific initialization/\n+\tfinalization to ...\n+\t(haifa_sched_init, haifa_sched_finish): ... respectively.\n+\tNew functions.\n+\t(setup_sched_dump): New function.\n+\t(haifa_init_only_bb): New static function.\n+\t(haifa_speculate_insn): New static function.\n+\t(try_ready): Use haifa_* instead of speculate_insn and\n+\tchange_pattern.\n+\t(extend_ready, extend_all): Remove.\n+\t(sched_extend_ready_list, sched_finish_ready_list): New functions.\n+\t(create_check_block_twin, add_to_speculative_block): Use\n+\thaifa_insns_init instead of extend_global.  Update to use new\n+\tinitialization functions.  Change parameter.  Factor out code from\n+\tcreate_check_block_twin to ...\n+\t(sched_create_recovery_edges) ... this new function.\n+\t(add_block): Remove.\n+\t(sched_scan_info): New.\n+\t(extend_bb): Use sched_scan_info.\n+\t(init_bb, extend_insn, init_insn, init_insns_in_bb, sched_scan): New\n+\tstatic functions for walking through scheduling region.\n+\t(sched_luids): New vector variable to replace uid_to_luid.\n+\t(luids_extend_insn): New function.\n+\t(sched_max_luid): New variable.\n+\t(luids_init_insn): New function.\n+\t(sched_init_luids, sched_finish_luids): New functions.\n+\t(insn_luid): New debug function.\n+\t(sched_extend_target): New function.\n+\t(haifa_init_insn): New static function.\n+\t(sched_init_only_bb): New hook.\n+\t(sched_split_block): New hook.\n+\t(sched_split_block_1): New function.\n+\t(sched_create_empty_bb): New hook.\n+\t(sched_create_empty_bb_1): New function.\t\n+\t(common_sched_info, ready): New global variables.\n+\t(current_sched_info_var): Remove.\n+\t(move_block_after_check): Use common_sched_info.\t\t\n+\t(haifa_luid_for_non_insn): New static function.\t\n+\t(init_before_recovery): Use haifa_init_only_bb instead of\n+\tadd_block.\n+\t(increase_insn_priority): New.\n+\t* modulo-sched.c: (issue_rate): Remove static declaration.\n+\t(sms_sched_info): Change type to haifa_sched_info.\n+\t(sms_sched_deps_info, sms_common_sched_info): New variables.\n+\t(setup_sched_infos): New.\n+\t(sms_schedule): Initialize them.  Call haifa_sched_init/finish.\n+\tDo not call regstat_free_calls_crossed.\n+\t(sms_print_insn): Use const_rtx.\n+\t* params.def (PARAM_MAX_PIPELINE_REGION_BLOCKS,\n+\tPARAM_MAX_PIPELINE_REGION_INSNS, PARAM_SELSCHED_MAX_LOOKAHEAD,\n+\tPARAM_SELSCHED_MAX_SCHED_TIMES, PARAM_SELSCHED_INSNS_TO_RENAME,\n+\tPARAM_SCHED_MEM_TRUE_DEP_COST): New.\n+\t* sched-deps.c (sched_deps_info): New.  Update all relevant uses of\n+\tcurrent_sched_info to use it.\n+\t(enum reg_pending_barrier_mode): Move to sched-int.h.\n+\t(h_d_i_d): New variable. Initialize to NULL.\n+\t({true, output, anti, spec, forward}_dependency_cache): Initialize\n+\tto NULL.\n+\t(estimate_dep_weak): Remove static declaration.\n+\t(sched_has_condition_p): New function.  Adjust users of\n+\tsched_get_condition to use it instead.\n+\t(conditions_mutex_p): Add arguments indicating which conditions are\n+\treversed.  Use them.\n+\t(sched_get_condition_with_rev): Rename from sched_get_condition.  Add\n+\targument to indicate whether returned condition is reversed.  Do not\n+\tgenerate new rtx when condition should be reversed; indicate it by\n+\tsetting new argument instead.\n+\t(add_dependence_list_and_free): Add deps parameter.\n+\tUpdate all users.  Do not free dependence list when\n+\tdeps context is readonly.\n+\t(add_insn_mem_dependence, flush_pending_lists): Adjust for readonly\n+\tcontexts.\n+\t(remove_from_dependence_list, remove_from_both_dependence_lists): New.\n+\t(remove_from_deps): New. Use the above functions.\t\n+\t(cur_insn, can_start_lhs_rhs_p): New static variables.\n+\t(add_or_update_back_dep_1): Initialize present_dep_type.\n+\t(haifa_start_insn, haifa_finish_insn, haifa_note_reg_set,\n+\thaifa_note_reg_clobber, haifa_note_reg_use, haifa_note_mem_dep,\n+\thaifa_note_dep): New functions implementing dependence hooks for\n+\tthe Haifa scheduler.\n+\t(note_reg_use, note_reg_set, note_reg_clobber, note_mem_dep,\n+\tnote_dep): New functions.\n+\t(ds_to_dt, extend_deps_reg_info, maybe_extend_reg_info_p): New\n+\tfunctions.\n+\t(init_deps): Initialize last_reg_pending_barrier and deps->readonly.\n+\t(free_deps): Initialize deps->reg_last.\n+\t(sched_analyze_reg, sched_analyze_1, sched_analyze_2,\n+\tsched_analyze_insn): Update to use dependency hooks infrastructure\n+\tand readonly contexts.\n+\t(deps_analyze_insn): New function.  Move part of logic from ...\n+\t(sched_analyze): ... here.  Also move some logic to ...\n+\t(deps_start_bb): ... here.  New function.\n+\t(add_forw_dep, delete_forw_dep): Guard use of INSN_DEP_COUNT with\n+\tsel_sched_p.\n+\t(sched_deps_init): New function.  Move code from ...\n+\t(init_dependency_caches): ... here.  Remove.\n+\t(init_deps_data_vector): New.\n+\t(sched_deps_finish): New function.  Move code from ...\n+\t(free_dependency_caches): ... here.  Remove.\n+\t(init_deps_global, finish_deps_global): Adjust for use with\n+\tselective scheduling.\n+\t(get_dep_weak): Move logic to ...\n+\t(get_dep_weak_1): New function.\n+\t(ds_merge): Move logic to ...\n+\t(ds_merge_1): New static function.\n+\t(ds_full_merge, ds_max_merge, ds_get_speculation_types): New functions.\n+\t(ds_get_max_dep_weak): New function.\n+\t* sched-ebb.c (sched_n_insns): Rename to sched_rgn_n_insns.\n+\t(n_insns): Rename to rgn_n_insns.\n+\t(debug_ebb_dependencies): New function.\n+\t(init_ready_list): Use it.\n+\t(begin_schedule_ready): Use sched_init_only_bb.\n+\t(ebb_print_insn): Indicate when an insn starts a new cycle.\n+\t(contributes_to_priority, compute_jump_reg_dependencies,\n+\tadd_remove_insn, fix_recovery_cfg): Add ebb_ prefix to function names.\n+\t(add_block1): Remove to ebb_add_block.\n+\t(ebb_sched_deps_info, ebb_common_sched_info): New variables.\n+\t(schedule_ebb): Initialize them.  Use remove_notes instead of\n+\trm_other_notes.  Use haifa_local_init/finish.\n+\t(schedule_ebbs): Use haifa_sched_init/finish.\n+\t* sched-int.h: Include vecprim.h, remove rtl.h.\n+\t(struct ready_list): Delete declaration.\n+\t(sched_verbose_param, enum sched_pass_id_t,\n+\tbb_vec_t, insn_vec_t, rtx_vec_t): New.\n+\t(struct sched_scan_info_def): New structure.\n+\t(sched_scan_info, sched_scan, sched_init_bbs,\n+\tsched_init_luids, sched_finish_luids, sched_extend_target,\n+\thaifa_init_h_i_d, haifa_finish_h_i_d): Declare.\n+\t(struct common_sched_info_def): New.\n+\t(common_sched_info, haifa_common_sched_info,\n+\tsched_emulate_haifa_p): Declare.\n+\t(sel_sched_p): New.\n+\t(sched_luids): Declare.\n+\t(INSN_LUID, LUID_BY_UID, SET_INSN_LUID): Declare.\n+\t(sched_max_luid, insn_luid): Declare.\n+\t(note_list, remove_notes, restore_other_notes, bb_note): Declare.\n+\t(sched_insns_init, sched_insns_finish, xrecalloc, reemit_notes,\n+\tprint_insn, print_pattern, print_value, haifa_classify_insn,\n+\tsel_find_rgns, sel_mark_hard_insn, dfa_state_size, advance_state,\n+\tsetup_sched_dump, sched_init, sched_finish,\n+\tsel_insn_is_speculation_check): Export.\n+\t(struct ready_list): Move from haifa-sched.c.\n+\t(ready_try, ready, max_issue): Export.\n+\t(ebb_compute_jump_reg_dependencies, find_fallthru_edge,\n+\tsched_init_only_bb, sched_split_block, sched_split_block_1,\n+\tsched_create_empty_bb, sched_create_empty_bb_1,\n+\tsched_create_recovery_block, sched_create_recovery_edges): Export.\n+\t(enum reg_pending_barrier_mode): Export.\n+\t(struct deps): New fields `last_reg_pending_barrier' and `readonly'.\n+\t(deps_t): New.\n+\t(struct sched_info): Rename to haifa_sched_info.  Use const_rtx for\n+\tprint_insn field.  Move add_block and fix_recovery_cfg to\n+\tcommon_sched_info_def.  Move compute_jump_reg_dependencies, use_cselib  ...\n+\t(struct sched_deps_info_def): ... this new structure.\n+\t(sched_deps_info): Declare.\n+\t(struct spec_info_def): Remove weakness_cutoff, add\n+\tdata_weakness_cutoff and control_weakness_cutoff.\n+\t(spec_info): Declare.\n+\t(struct _haifa_deps_insn_data): Split from haifa_insn_data.  Add\n+\tdep_count field.\n+\t(struct haifa_insn_data): Rename to struct _haifa_insn_data.\n+\t(haifa_insn_data_def, haifa_insn_data_t): New typedefs.\n+\t(current_sched_info): Change type to struct haifa_sched_info.\n+\t(haifa_deps_insn_data_def, haifa_deps_insn_data_t): New typedefs.\n+\t(h_d_i_d): New variable.\n+\t(HDID): New accessor macro.\n+\t(h_i_d): Change type to VEC (haifa_insn_data_def, heap) *.\n+\t(HID): New accessor macro.  Rewrite h_i_d accessor macros through HID\n+\tand HDID.\n+\t(IS_SPECULATION_CHECK_P): Update for selective scheduler.\n+\t(enum SCHED_FLAGS): Update for selective scheduler.\n+\t(enum SPEC_SCHED_FLAGS): New flag SEL_SCHED_SPEC_DONT_CHECK_CONTROL.\n+\t(init_dependency_caches, free_dependency_caches): Delete declarations.\n+\t(deps_analyze_insn, remove_from_deps, get_dep_weak_1,\n+\testimate_dep_weak, ds_full_merge, ds_max_merge, ds_weak,\n+\tds_get_speculation_types, ds_get_max_dep_weak, sched_deps_init,\n+\tsched_deps_finish, haifa_note_reg_set, haifa_note_reg_use,\n+\thaifa_note_reg_clobber, maybe_extend_reg_info_p, deps_start_bb,\n+\tds_to_dt): Export.\n+\t(rm_other_notes): Delete declaration.\n+\t(schedule_block): Remove one argument.\n+\t(cycle_issued_insns, issue_rate, dfa_lookahead, ready_sort,\n+\tready_element, ready_lastpos, sched_extend_ready_list,\n+\tsched_finish_ready_list, sched_change_pattern, sched_speculate_insn,\n+\tconcat_note_lists): Export.\n+\t(struct region): Move from sched-rgn.h.\n+\t(nr_regions, rgn_table, rgn_bb_table, block_to_bb, containing_rgn,\n+\tRGN_NR_BLOCKS, RGN_BLOCKS, RGN_DONT_CALC_DEPS, RGN_HAS_REAL_EBB,\n+\tBLOCK_TO_BB, CONTAINING_RGN): Export.\n+\t(ebb_head, BB_TO_BLOCK, EBB_FIRST_BB, EBB_LAST_BB, INSN_BB): Likewise.\n+\t(current_nr_blocks, current_blocks, target_bb): Likewise.\n+\t(dep_cost_1, sched_is_disabled_for_current_region_p, sched_rgn_init,\n+\tsched_rgn_finish, rgn_setup_region, sched_rgn_compute_dependencies,\n+\tsched_rgn_local_init, extend_regions,\n+\trgn_make_new_region_out_of_new_block, compute_priorities,\n+\tdebug_rgn_dependencies, free_rgn_deps, contributes_to_priority,\n+\textend_rgns, deps_join rgn_setup_common_sched_info,\n+\trgn_setup_sched_infos, debug_regions, debug_region, dump_region_dot,\n+\tdump_region_dot_file, haifa_sched_init, haifa_sched_finish): Export.\n+\t(get_rgn_sched_max_insns_priority, sel_add_to_insn_priority, \n+\tincrease_insn_priority): Likewise.\n+\t* sched-rgn.c: Include sel-sched.h.\n+\t(ref_counts): New static variable.  Use it ...\n+\t(INSN_REF_COUNT): ... here.  Rewrite and move closer to uses.\n+\t(FED_BY_SPEC_LOAD, IS_LOAD_INSN): Rewrite to use HID accessor macro.\n+\t(sched_is_disabled_for_current_region_p): Delete static declaration.\n+\t(struct region): Move to sched-int.h.\n+\t(nr_regions, rgn_table, rgn_bb_table, block_to_bb, containing_rgn,\n+\tebb_head): Define and initialize.\n+\t(RGN_NR_BLOCKS, RGN_BLOCKS, RGN_DONT_CALC_DEPS, RGN_HAS_REAL_EBB,\n+\tBLOCK_TO_BB, CONTAINING_RGN, debug_regions, extend_regions,\n+\tBB_TO_BLOCK, EBB_FIRST_BB, EBB_LAST_BB): Move to\n+\tsched-int.h.\n+\t(find_single_block_region): Add new argument to indicate that EBB\n+\tregions should be constructed.\n+\t(debug_live): Delete declaration.\n+\t(current_nr_blocks, current_blocks, target_bb): Remove static qualifiers.\n+\t(compute_dom_prob_ps, check_live, update_live, set_spec_fed): Delete\n+\tdeclaration.\n+\t(init_regions): Delete declaration.\n+\t(debug_region, bb_in_region_p, dump_region_dot_file, dump_region_dot,\n+\trgn_estimate_number_of_insns): New.\n+\t(too_large): Use estimate_number_of_insns.\n+\t(haifa_find_rgns): New. Move the code from ...\n+\t(find_rgns): ... here.  Call either sel_find_rgns or haifa_find_rgns.\n+\t(free_trg_info): New.\n+\t(compute_trg_info): Allocate candidate tables here instead of ...\n+\t(init_ready_list): ... here.\n+\t(rgn_print_insn): Use const_rtx.\n+\t(contributes_to_priority, extend_regions): Delete static declaration.\n+\t(add_remove_insn, fix_recovery_cfg): Add rgn_ to function names.\n+\t(add_block1): Rename to rgn_add_block.\n+\t(debug_rgn_dependencies): Delete static qualifier.\n+\t(new_ready): Use sched_deps_info.  Simplify.\n+\t(rgn_common_sched_info, rgn_const_sched_deps_info,\n+\trgn_const_sel_sched_deps_info, rgn_sched_deps_info, rgn_sched_info): New.\n+\t(region_sched_info): Rename to rgn_const_sched_info.\n+\t(deps_join): New, extracted from ...\n+\t(propagate_deps): ... here.\n+\t(compute_block_dependences, debug_dependencies): Update for selective\n+\tscheduling.\n+\t(free_rgn_deps, compute_priorities): New functions.\n+\t(sched_rgn_init, sched_rgn_finish, rgn_setup_region,\n+\tsched_rgn_compute_dependencies): New functions.\n+\t(schedule_region): Use them.\n+\t(sched_rgn_local_init, sched_rgn_local_free, sched_rgn_local_finish,\n+\trgn_setup_common_sched_info, rgn_setup_sched_infos):\n+\tNew functions.\n+\t(schedule_insns): Call new functions that were split out.\n+\t(rgn_make_new_region_out_of_new_block): New.\n+\t(get_rgn_sched_max_insns_priority): New.\n+\t(rest_of_handle_sched, rest_of_handle_sched2): Call selective\n+\tscheduling when appropriate.\n+\t* sched-vis.c: Include insn-attr.h.\n+\t(print_value, print_pattern): Make global.\n+\t(print_rtl_slim, debug_bb_slim, debug_bb_n_slim): New functions.\n+\t* target-def.h (TARGET_SCHED_ADJUST_COST_2,\n+\tTARGET_SCHED_ALLOC_SCHED_CONTEXT, TARGET_SCHED_INIT_SCHED_CONTEXT,\n+\tTARGET_SCHED_SET_SCHED_CONTEXT, TARGET_SCHED_CLEAR_SCHED_CONTEXT,\n+\tTARGET_SCHED_FREE_SCHED_CONTEXT, TARGET_SCHED_GET_INSN_CHECKED_DS,\n+\tTARGET_SCHED_GET_INSN_SPEC_DS, TARGET_SCHED_SKIP_RTX_P): New target\n+\thooks.  Initialize them to 0.\n+\t(TARGET_SCHED_GEN_CHECK): Rename to TARGET_SCHED_GEN_SPEC_CHECK.\n+\t* target.h (struct gcc_target): Add them.  Rename gen_check field to\n+\tgen_spec_check.\n+\t* flags.h (sel_sched_switch_set): Declare.\n+\t* opts.c (sel_sched_switch_set): New variable.\n+\t(decode_options): Unset flag_sel_sched_pipelining_outer_loops if\n+\tpipelining is disabled from command line.\n+\t(common_handle_option): Record whether selective scheduling is\n+\trequested from command line.\n+\t* doc/invoke.texi: Document new flags and parameters.\n+\t* doc/tm.texi: Document new target hooks.\n+\t* config/ia64/ia64.c (TARGET_SCHED_GEN_SPEC_CHECK): Define to ia64_gen_check.\n+\t(dfa_state_size): Do not declare locally.\n+\t* config/ia64/ia64.opt (msched-ar-data-spec): Default to 0.\n+\t* config/rs6000/rs6000.c (rs6000_init_sched_context,\n+\trs6000_alloc_sched_context, rs6000_set_sched_context,\n+\trs6000_free_sched_context): New functions.\n+\t(struct _rs6000_sched_context): New.\n+\t(rs6000_sched_reorder2): Do not modify INSN_PRIORITY for selective\n+\tscheduling.\n+\t(rs6000_sched_finish): Do not run for selective scheduling. \n+\n 2008-08-31  Jan Hubicka  <jh@suse.cz>\n \n \t* frv.c (frv_rtx_costs): Update forward declaration."}, {"sha": "f5ee1e3fd1e59e51c2a3980a3965e0ec54d17aaa", "filename": "gcc/Makefile.in", "status": "modified", "additions": 26, "deletions": 3, "changes": 29, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -803,7 +803,10 @@ OPTABS_H = optabs.h insn-codes.h\n REGS_H = regs.h varray.h $(MACHMODE_H) $(OBSTACK_H) $(BASIC_BLOCK_H) $(FUNCTION_H)\n RA_H = ra.h $(REGS_H)\n RESOURCE_H = resource.h hard-reg-set.h\n-SCHED_INT_H = sched-int.h $(INSN_ATTR_H) $(BASIC_BLOCK_H) $(RTL_H) $(DF_H)\n+SCHED_INT_H = sched-int.h $(INSN_ATTR_H) $(BASIC_BLOCK_H) $(RTL_H) $(DF_H) vecprim.h\n+SEL_SCHED_IR_H = sel-sched-ir.h $(INSN_ATTR_H) $(BASIC_BLOCK_H) $(RTL_H) \\\n+\t$(GGC_H) $(SCHED_INT_H)\n+SEL_SCHED_DUMP_H = sel-sched-dump.h $(SEL_SCHED_IR_H)\n INTEGRATE_H = integrate.h $(VARRAY_H)\n CFGLAYOUT_H = cfglayout.h $(BASIC_BLOCK_H)\n CFGLOOP_H = cfgloop.h $(BASIC_BLOCK_H) $(RTL_H) vecprim.h double-int.h\n@@ -1163,6 +1166,9 @@ OBJS-common = \\\n \tsched-vis.o \\\n \tsdbout.o \\\n \tsee.o \\\n+\tsel-sched-ir.o \\\n+\tsel-sched-dump.o \\\n+\tsel-sched.o \\\n \tsimplify-rtx.o \\\n \tsparseset.o \\\n \tsreal.o \\\n@@ -2941,14 +2947,31 @@ sched-deps.o : sched-deps.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n sched-rgn.o : sched-rgn.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n    $(RTL_H) $(SCHED_INT_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) insn-config.h \\\n    $(FUNCTION_H) $(INSN_ATTR_H) $(TOPLEV_H) $(RECOG_H) except.h $(PARAMS_H) \\\n-   $(TM_P_H) $(TARGET_H) $(CFGLAYOUT_H) $(TIMEVAR_H) tree-pass.h $(DBGCNT_H)\n+   $(TM_P_H) $(TARGET_H) $(CFGLAYOUT_H) $(TIMEVAR_H) tree-pass.h  \\\n+   $(DBGCNT_H)\n sched-ebb.o : sched-ebb.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n    $(RTL_H) $(SCHED_INT_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) insn-config.h \\\n    $(FUNCTION_H) $(INSN_ATTR_H) $(TOPLEV_H) $(RECOG_H) except.h $(TM_P_H) \\\n    $(PARAMS_H) $(CFGLAYOUT_H) $(TARGET_H) output.h\n sched-vis.o : sched-vis.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n    $(RTL_H) $(SCHED_INT_H) hard-reg-set.h $(BASIC_BLOCK_H) $(OBSTACK_H) \\\n-   $(REAL_H) tree-pass.h\n+   $(REAL_H) tree-pass.h $(INSN_ATTR_H)\n+sel-sched.o : sel-sched.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   $(RTL_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) insn-config.h \\\n+   $(FUNCTION_H) $(INSN_ATTR_H) toplev.h $(RECOG_H) except.h $(PARAMS_H) \\\n+   $(TM_P_H) $(TARGET_H) $(CFGLAYOUT_H) $(TIMEVAR_H) tree-pass.h  \\\n+   $(SCHED_INT_H) $(GGC_H) $(TREE_H) $(LANGHOOKS_DEF_H) \\\n+   $(SEL_SCHED_IR_H) $(SEL_SCHED_DUMP_H) sel-sched.h\n+sel-sched-dump.o : sel-sched-dump.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   $(RTL_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) insn-config.h \\\n+   $(FUNCTION_H) $(INSN_ATTR_H) toplev.h $(RECOG_H) except.h $(PARAMS_H) \\\n+   $(TM_P_H) $(TARGET_H) $(CFGLAYOUT_H) $(TIMEVAR_H) tree-pass.h \\\n+   $(SEL_SCHED_DUMP_H) $(GGC_H) $(TREE_H) $(LANGHOOKS_DEF_H) $(SEL_SCHED_IR_H)\n+sel-sched-ir.o : sel-sched-ir.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   $(RTL_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) insn-config.h \\\n+   $(FUNCTION_H) $(INSN_ATTR_H) toplev.h $(RECOG_H) except.h $(PARAMS_H) \\\n+   $(TM_P_H) $(TARGET_H) $(CFGLAYOUT_H) $(TIMEVAR_H) tree-pass.h \\\n+   $(SCHED_INT_H) $(GGC_H) $(TREE_H) $(LANGHOOKS_DEF_H) $(SEL_SCHED_IR_H)\n final.o : final.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \\\n    $(TREE_H) $(FLAGS_H) intl.h $(REGS_H) $(RECOG_H) conditions.h \\\n    insn-config.h $(INSN_ATTR_H) $(FUNCTION_H) output.h hard-reg-set.h \\"}, {"sha": "0897b0df5653f07a050ee7a3bc04585ec3174a9f", "filename": "gcc/cfghooks.c", "status": "modified", "additions": 28, "deletions": 3, "changes": 31, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fcfghooks.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fcfghooks.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcfghooks.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -56,6 +56,18 @@ gimple_register_cfg_hooks (void)\n   cfg_hooks = &gimple_cfg_hooks;\n }\n \n+struct cfg_hooks\n+get_cfg_hooks (void)\n+{\n+  return *cfg_hooks;\n+}\n+\n+void\n+set_cfg_hooks (struct cfg_hooks new_cfg_hooks)\n+{\n+  *cfg_hooks = new_cfg_hooks;\n+}\n+\n /* Returns current ir type.  */\n \n enum ir_type\n@@ -719,6 +731,8 @@ make_forwarder_block (basic_block bb, bool (*redirect_edge_p) (edge),\n   /* Redirect back edges we want to keep.  */\n   for (ei = ei_start (dummy->preds); (e = ei_safe_edge (ei)); )\n     {\n+      basic_block e_src;\n+\n       if (redirect_edge_p (e))\n \t{\n \t  ei_next (&ei);\n@@ -735,10 +749,21 @@ make_forwarder_block (basic_block bb, bool (*redirect_edge_p) (edge),\n       if (fallthru->count < 0)\n \tfallthru->count = 0;\n \n+      e_src = e->src;\n       jump = redirect_edge_and_branch_force (e, bb);\n-      if (jump != NULL\n-\t  && new_bb_cbk != NULL)\n-\tnew_bb_cbk (jump);\n+      if (jump != NULL)\n+        {\n+          /* If we redirected the loop latch edge, the JUMP block now acts like\n+             the new latch of the loop.  */\n+          if (current_loops != NULL\n+              && dummy->loop_father != NULL\n+              && dummy->loop_father->header == dummy\n+              && dummy->loop_father->latch == e_src)\n+            dummy->loop_father->latch = jump;\n+          \n+          if (new_bb_cbk != NULL)\n+            new_bb_cbk (jump);\n+        }\n     }\n \n   if (dom_info_available_p (CDI_DOMINATORS))"}, {"sha": "cb4a5360f8e8b4be749de4ee640b2d5ce16d50c6", "filename": "gcc/cfghooks.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fcfghooks.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fcfghooks.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcfghooks.h?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -190,5 +190,7 @@ extern enum ir_type current_ir_type (void);\n extern void rtl_register_cfg_hooks (void);\n extern void cfg_layout_rtl_register_cfg_hooks (void);\n extern void gimple_register_cfg_hooks (void);\n+extern struct cfg_hooks get_cfg_hooks (void);\n+extern void set_cfg_hooks (struct cfg_hooks);\n \n #endif  /* GCC_CFGHOOKS_H */"}, {"sha": "0e95323008a3ee1420c432d60d5f90797a839ead", "filename": "gcc/cfgloop.c", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fcfgloop.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fcfgloop.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcfgloop.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -887,6 +887,19 @@ get_loop_body_in_dom_order (const struct loop *loop)\n   return tovisit;\n }\n \n+/* Gets body of a LOOP sorted via provided BB_COMPARATOR.  */\n+\n+basic_block *\n+get_loop_body_in_custom_order (const struct loop *loop, \n+\t\t\t       int (*bb_comparator) (const void *, const void *))\n+{\n+  basic_block *bbs = get_loop_body (loop);\n+\n+  qsort (bbs, loop->num_nodes, sizeof (basic_block), bb_comparator);\n+\n+  return bbs;\n+}\n+\n /* Get body of a LOOP in breadth first sort order.  */\n \n basic_block *"}, {"sha": "aa3e6118ed6b16967c006c89f6b5fedbcf20e051", "filename": "gcc/cfgloop.h", "status": "modified", "additions": 7, "deletions": 2, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fcfgloop.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fcfgloop.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcfgloop.h?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -171,7 +171,8 @@ enum\n   LOOPS_HAVE_RECORDED_EXITS = 8,\n   LOOPS_MAY_HAVE_MULTIPLE_LATCHES = 16,\n   LOOP_CLOSED_SSA = 32,\n-  LOOPS_NEED_FIXUP = 64\n+  LOOPS_NEED_FIXUP = 64,\n+  LOOPS_HAVE_FALLTHRU_PREHEADERS = 128\n };\n \n #define LOOPS_NORMAL (LOOPS_HAVE_PREHEADERS | LOOPS_HAVE_SIMPLE_LATCHES \\\n@@ -235,6 +236,9 @@ extern unsigned get_loop_body_with_size (const struct loop *, basic_block *,\n \t\t\t\t\t unsigned);\n extern basic_block *get_loop_body_in_dom_order (const struct loop *);\n extern basic_block *get_loop_body_in_bfs_order (const struct loop *);\n+extern basic_block *get_loop_body_in_custom_order (const struct loop *, \n+\t\t\t       int (*) (const void *, const void *));\n+\n extern VEC (edge, heap) *get_loop_exit_edges (const struct loop *);\n edge single_exit (const struct loop *);\n extern unsigned num_loop_branches (const struct loop *);\n@@ -250,7 +254,8 @@ extern void delete_loop (struct loop *);\n \n enum\n {\n-  CP_SIMPLE_PREHEADERS = 1\n+  CP_SIMPLE_PREHEADERS = 1,\n+  CP_FALLTHRU_PREHEADERS = 2\n };\n \n basic_block create_preheader (struct loop *, int);"}, {"sha": "025b5be1052aa2706e4478aeb480146ee0acf06d", "filename": "gcc/cfgloopmanip.c", "status": "modified", "additions": 41, "deletions": 6, "changes": 47, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fcfgloopmanip.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fcfgloopmanip.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcfgloopmanip.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -1102,9 +1102,26 @@ mfb_keep_just (edge e)\n   return e != mfb_kj_edge;\n }\n \n+/* True when a candidate preheader BLOCK has predecessors from LOOP.  */\n+\n+static bool\n+has_preds_from_loop (basic_block block, struct loop *loop)\n+{\n+  edge e;\n+  edge_iterator ei;\n+  \n+  FOR_EACH_EDGE (e, ei, block->preds)\n+    if (e->src->loop_father == loop)\n+      return true;\n+  return false;\n+}\n+\n /* Creates a pre-header for a LOOP.  Returns newly created block.  Unless\n    CP_SIMPLE_PREHEADERS is set in FLAGS, we only force LOOP to have single\n    entry; otherwise we also force preheader block to have only one successor.\n+   When CP_FALLTHRU_PREHEADERS is set in FLAGS, we force the preheader block\n+   to be a fallthru predecessor to the loop header and to have only \n+   predecessors from outside of the loop.\n    The function also updates dominators.  */\n \n basic_block\n@@ -1131,13 +1148,27 @@ create_preheader (struct loop *loop, int flags)\n   gcc_assert (nentry);\n   if (nentry == 1)\n     {\n-      if (/* We do not allow entry block to be the loop preheader, since we\n+      bool need_forwarder_block = false;\n+      \n+      /* We do not allow entry block to be the loop preheader, since we\n \t     cannot emit code there.  */\n-\t  single_entry->src != ENTRY_BLOCK_PTR\n-\t  /* If we want simple preheaders, also force the preheader to have\n-\t     just a single successor.  */\n-\t  && !((flags & CP_SIMPLE_PREHEADERS)\n-\t       && !single_succ_p (single_entry->src)))\n+      if (single_entry->src == ENTRY_BLOCK_PTR)\n+        need_forwarder_block = true;\n+      else\n+        {\n+          /* If we want simple preheaders, also force the preheader to have\n+             just a single successor.  */\n+          if ((flags & CP_SIMPLE_PREHEADERS)\n+              && !single_succ_p (single_entry->src))\n+            need_forwarder_block = true;\n+          /* If we want fallthru preheaders, also create forwarder block when\n+             preheader ends with a jump or has predecessors from loop.  */\n+          else if ((flags & CP_FALLTHRU_PREHEADERS)\n+                   && (JUMP_P (BB_END (single_entry->src))\n+                       || has_preds_from_loop (single_entry->src, loop)))\n+            need_forwarder_block = true;\n+        }\n+      if (! need_forwarder_block)\n \treturn NULL;\n     }\n \n@@ -1174,6 +1205,10 @@ create_preheader (struct loop *loop, int flags)\n   if (dump_file)\n     fprintf (dump_file, \"Created preheader block for loop %i\\n\",\n \t     loop->num);\n+  \n+  if (flags & CP_FALLTHRU_PREHEADERS)\n+    gcc_assert ((single_succ_edge (dummy)->flags & EDGE_FALLTHRU)\n+                && !JUMP_P (BB_END (dummy)));\n \n   return dummy;\n }"}, {"sha": "87824f524a7c807f15839f1526f378c160a20ed1", "filename": "gcc/common.opt", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fcommon.opt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fcommon.opt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcommon.opt?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -962,6 +962,29 @@ fschedule-insns2\n Common Report Var(flag_schedule_insns_after_reload) Optimization\n Reschedule instructions after register allocation\n \n+; This flag should be on when a target implements non-trivial\n+; scheduling hooks, maybe saving some information for its own sake.\n+; On IA64, for example, this is used for correct bundling. \n+fselective-scheduling\n+Common Report Var(flag_selective_scheduling) Optimization\n+Schedule instructions using selective scheduling algorithm\n+\n+fselective-scheduling2\n+Common Report Var(flag_selective_scheduling2) Optimization \n+Run selective scheduling after reload\n+\n+fsel-sched-pipelining\n+Common Report Var(flag_sel_sched_pipelining) Init(0) Optimization\n+Perform software pipelining of inner loops during selective scheduling\n+\n+fsel-sched-pipelining-outer-loops\n+Common Report Var(flag_sel_sched_pipelining_outer_loops) Init(0) Optimization\n+Perform software pipelining of outer loops during selective scheduling\n+\n+fsel-sched-reschedule-pipelined\n+Common Report Var(flag_sel_sched_reschedule_pipelined) Init(0) Optimization\n+Reschedule pipelined regions without pipelining\n+\n ; sched_stalled_insns means that insns can be moved prematurely from the queue\n ; of stalled insns into the ready list.\n fsched-stalled-insns"}, {"sha": "1927011920dfa29b95506b9547d23fd1db7b5886", "filename": "gcc/config/ia64/ia64.c", "status": "modified", "additions": 2, "deletions": 6, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fconfig%2Fia64%2Fia64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fconfig%2Fia64%2Fia64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fia64%2Fia64.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -382,8 +382,8 @@ static const struct attribute_spec ia64_attribute_table[] =\n #undef TARGET_SCHED_NEEDS_BLOCK_P\n #define TARGET_SCHED_NEEDS_BLOCK_P ia64_needs_block_p\n \n-#undef TARGET_SCHED_GEN_CHECK\n-#define TARGET_SCHED_GEN_CHECK ia64_gen_check\n+#undef TARGET_SCHED_GEN_SPEC_CHECK\n+#define TARGET_SCHED_GEN_SPEC_CHECK ia64_gen_check\n \n #undef TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD_SPEC\n #define TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD_SPEC\\\n@@ -6278,10 +6278,6 @@ static rtx dfa_stop_insn;\n \n static rtx last_scheduled_insn;\n \n-/* The following variable value is size of the DFA state.  */\n-\n-static size_t dfa_state_size;\n-\n /* The following variable value is pointer to a DFA state used as\n    temporary variable.  */\n "}, {"sha": "5103f908b660111ccb8e3d00cef2835306961132", "filename": "gcc/config/ia64/ia64.opt", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fconfig%2Fia64%2Fia64.opt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fconfig%2Fia64%2Fia64.opt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fia64%2Fia64.opt?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -101,7 +101,7 @@ Target Report Var(mflag_sched_br_data_spec) Init(0)\n Use data speculation before reload\n \n msched-ar-data-spec\n-Target Report Var(mflag_sched_ar_data_spec) Init(1)\n+Target Report Var(mflag_sched_ar_data_spec) Init(0)\n Use data speculation after reload\n \n msched-control-spec"}, {"sha": "6ab34969d02c5caf42acdc6a87856508b91e1335", "filename": "gcc/config/rs6000/rs6000.c", "status": "modified", "additions": 89, "deletions": 6, "changes": 95, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fconfig%2Frs6000%2Frs6000.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fconfig%2Frs6000%2Frs6000.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -857,6 +857,10 @@ static int rs6000_sched_reorder (FILE *, int, rtx *, int *, int);\n static int rs6000_sched_reorder2 (FILE *, int, rtx *, int *, int);\n static int rs6000_use_sched_lookahead (void);\n static int rs6000_use_sched_lookahead_guard (rtx);\n+static void * rs6000_alloc_sched_context (void);\n+static void rs6000_init_sched_context (void *, bool);\n+static void rs6000_set_sched_context (void *);\n+static void rs6000_free_sched_context (void *);\n static tree rs6000_builtin_reciprocal (unsigned int, bool, bool);\n static tree rs6000_builtin_mask_for_load (void);\n static tree rs6000_builtin_mul_widen_even (tree);\n@@ -1131,6 +1135,15 @@ static const char alt_reg_names[][8] =\n #undef TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD\n #define TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD rs6000_use_sched_lookahead_guard\n \n+#undef TARGET_SCHED_ALLOC_SCHED_CONTEXT\n+#define TARGET_SCHED_ALLOC_SCHED_CONTEXT rs6000_alloc_sched_context\n+#undef TARGET_SCHED_INIT_SCHED_CONTEXT\n+#define TARGET_SCHED_INIT_SCHED_CONTEXT rs6000_init_sched_context\n+#undef TARGET_SCHED_SET_SCHED_CONTEXT\n+#define TARGET_SCHED_SET_SCHED_CONTEXT rs6000_set_sched_context\n+#undef TARGET_SCHED_FREE_SCHED_CONTEXT\n+#define TARGET_SCHED_FREE_SCHED_CONTEXT rs6000_free_sched_context\n+\n #undef TARGET_VECTORIZE_BUILTIN_MASK_FOR_LOAD\n #define TARGET_VECTORIZE_BUILTIN_MASK_FOR_LOAD rs6000_builtin_mask_for_load\n #undef TARGET_VECTORIZE_BUILTIN_MUL_WIDEN_EVEN\n@@ -19476,7 +19489,8 @@ rs6000_sched_reorder2 (FILE *dump, int sched_verbose, rtx *ready,\n                   for (i=pos; i<*pn_ready-1; i++)\n                     ready[i] = ready[i + 1];\n                   ready[*pn_ready-1] = tmp;\n-                  if INSN_PRIORITY_KNOWN (tmp)\n+\n+                  if (!sel_sched_p () && INSN_PRIORITY_KNOWN (tmp))\n                     INSN_PRIORITY (tmp)++;\n                   break;\n                 }\n@@ -19493,7 +19507,8 @@ rs6000_sched_reorder2 (FILE *dump, int sched_verbose, rtx *ready,\n           while (pos >= 0)\n             {\n               if (is_load_insn (ready[pos])\n-                  && INSN_PRIORITY_KNOWN (ready[pos]))\n+                  && !sel_sched_p ()\n+\t\t  && INSN_PRIORITY_KNOWN (ready[pos]))\n                 {\n                   INSN_PRIORITY (ready[pos])++;\n \n@@ -19535,8 +19550,10 @@ rs6000_sched_reorder2 (FILE *dump, int sched_verbose, rtx *ready,\n                       for (i=pos; i<*pn_ready-1; i++)\n                         ready[i] = ready[i + 1];\n                       ready[*pn_ready-1] = tmp;\n-                      if INSN_PRIORITY_KNOWN (tmp)\n+\n+                      if (!sel_sched_p () && INSN_PRIORITY_KNOWN (tmp))\n                         INSN_PRIORITY (tmp)++;\n+\n                       first_store_pos = -1;\n \n                       break;\n@@ -19555,7 +19572,7 @@ rs6000_sched_reorder2 (FILE *dump, int sched_verbose, rtx *ready,\n               for (i=first_store_pos; i<*pn_ready-1; i++)\n                 ready[i] = ready[i + 1];\n               ready[*pn_ready-1] = tmp;\n-              if INSN_PRIORITY_KNOWN (tmp)\n+              if (!sel_sched_p () && INSN_PRIORITY_KNOWN (tmp))\n                 INSN_PRIORITY (tmp)++;\n             }\n         }\n@@ -19569,7 +19586,8 @@ rs6000_sched_reorder2 (FILE *dump, int sched_verbose, rtx *ready,\n           while (pos >= 0)\n             {\n               if (is_store_insn (ready[pos])\n-                  && INSN_PRIORITY_KNOWN (ready[pos]))\n+                  && !sel_sched_p ()\n+\t\t  && INSN_PRIORITY_KNOWN (ready[pos]))\n                 {\n                   INSN_PRIORITY (ready[pos])++;\n \n@@ -20071,7 +20089,7 @@ pad_groups (FILE *dump, int sched_verbose, rtx prev_head_insn, rtx tail)\n       if (group_end)\n \t{\n \t  /* If the scheduler had marked group termination at this location\n-\t     (between insn and next_indn), and neither insn nor next_insn will\n+\t     (between insn and next_insn), and neither insn nor next_insn will\n \t     force group termination, pad the group with nops to force group\n \t     termination.  */\n \t  if (can_issue_more\n@@ -20125,6 +20143,10 @@ rs6000_sched_finish (FILE *dump, int sched_verbose)\n \n   if (reload_completed && rs6000_sched_groups)\n     {\n+      /* Do not run sched_finish hook when selective scheduling enabled.  */\n+      if (sel_sched_p ())\n+\treturn;\n+\n       if (rs6000_sched_insert_nops == sched_finish_none)\n \treturn;\n \n@@ -20145,6 +20167,67 @@ rs6000_sched_finish (FILE *dump, int sched_verbose)\n \t}\n     }\n }\n+\n+struct _rs6000_sched_context\n+{\n+  short cached_can_issue_more;\n+  rtx last_scheduled_insn;\n+  int load_store_pendulum;\n+};\n+\n+typedef struct _rs6000_sched_context rs6000_sched_context_def;\n+typedef rs6000_sched_context_def *rs6000_sched_context_t;\n+\n+/* Allocate store for new scheduling context.  */\n+static void *\n+rs6000_alloc_sched_context (void)\n+{\n+  return xmalloc (sizeof (rs6000_sched_context_def));\n+}\n+\n+/* If CLEAN_P is true then initializes _SC with clean data,\n+   and from the global context otherwise.  */\n+static void\n+rs6000_init_sched_context (void *_sc, bool clean_p)\n+{\n+  rs6000_sched_context_t sc = (rs6000_sched_context_t) _sc;\n+\n+  if (clean_p)\n+    {\n+      sc->cached_can_issue_more = 0;\n+      sc->last_scheduled_insn = NULL_RTX;\n+      sc->load_store_pendulum = 0;\n+    }\n+  else\n+    {\n+      sc->cached_can_issue_more = cached_can_issue_more;\n+      sc->last_scheduled_insn = last_scheduled_insn;\n+      sc->load_store_pendulum = load_store_pendulum;\n+    }\n+}\n+\n+/* Sets the global scheduling context to the one pointed to by _SC.  */\n+static void\n+rs6000_set_sched_context (void *_sc)\n+{\n+  rs6000_sched_context_t sc = (rs6000_sched_context_t) _sc;\n+\n+  gcc_assert (sc != NULL);\n+\n+  cached_can_issue_more = sc->cached_can_issue_more;\n+  last_scheduled_insn = sc->last_scheduled_insn;\n+  load_store_pendulum = sc->load_store_pendulum;\n+}\n+\n+/* Free _SC.  */\n+static void\n+rs6000_free_sched_context (void *_sc)\n+{\n+  gcc_assert (_sc != NULL);\n+\n+  free (_sc);\n+}\n+\n \f\n /* Length in units of the trampoline for entering a nested function.  */\n "}, {"sha": "c1effee8f03b02cbedecc4befb9e5f26e028769e", "filename": "gcc/cse.c", "status": "modified", "additions": 64, "deletions": 32, "changes": 96, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fcse.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fcse.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcse.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -574,7 +574,7 @@ static rtx use_related_value (rtx, struct table_elt *);\n \n static inline unsigned canon_hash (rtx, enum machine_mode);\n static inline unsigned safe_hash (rtx, enum machine_mode);\n-static unsigned hash_rtx_string (const char *);\n+static inline unsigned hash_rtx_string (const char *);\n \n static rtx canon_reg (rtx, rtx);\n static enum rtx_code find_comparison_args (enum rtx_code, rtx *, rtx *,\n@@ -2044,6 +2044,7 @@ use_related_value (rtx x, struct table_elt *elt)\n   return plus_constant (q->exp, offset);\n }\n \f\n+\n /* Hash a string.  Just add its bytes up.  */\n static inline unsigned\n hash_rtx_string (const char *ps)\n@@ -2058,27 +2059,20 @@ hash_rtx_string (const char *ps)\n   return hash;\n }\n \n-/* Hash an rtx.  We are careful to make sure the value is never negative.\n-   Equivalent registers hash identically.\n-   MODE is used in hashing for CONST_INTs only;\n-   otherwise the mode of X is used.\n-\n-   Store 1 in DO_NOT_RECORD_P if any subexpression is volatile.\n-\n-   If HASH_ARG_IN_MEMORY_P is not NULL, store 1 in it if X contains\n-   a MEM rtx which does not have the RTX_UNCHANGING_P bit set.\n-\n-   Note that cse_insn knows that the hash code of a MEM expression\n-   is just (int) MEM plus the hash code of the address.  */\n+/* Same as hash_rtx, but call CB on each rtx if it is not NULL.  \n+   When the callback returns true, we continue with the new rtx.  */\n \n unsigned\n-hash_rtx (const_rtx x, enum machine_mode mode, int *do_not_record_p,\n-\t  int *hash_arg_in_memory_p, bool have_reg_qty)\n+hash_rtx_cb (const_rtx x, enum machine_mode mode,\n+             int *do_not_record_p, int *hash_arg_in_memory_p,\n+             bool have_reg_qty, hash_rtx_callback_function cb)\n {\n   int i, j;\n   unsigned hash = 0;\n   enum rtx_code code;\n   const char *fmt;\n+  enum machine_mode newmode;\n+  rtx newx;\n \n   /* Used to turn recursion into iteration.  We can't rely on GCC's\n      tail-recursion elimination since we need to keep accumulating values\n@@ -2087,14 +2081,23 @@ hash_rtx (const_rtx x, enum machine_mode mode, int *do_not_record_p,\n   if (x == 0)\n     return hash;\n \n+  /* Invoke the callback first.  */\n+  if (cb != NULL \n+      && ((*cb) (x, mode, &newx, &newmode)))\n+    {\n+      hash += hash_rtx_cb (newx, newmode, do_not_record_p,\n+                           hash_arg_in_memory_p, have_reg_qty, cb);\n+      return hash;\n+    }\n+\n   code = GET_CODE (x);\n   switch (code)\n     {\n     case REG:\n       {\n \tunsigned int regno = REGNO (x);\n \n-\tif (!reload_completed)\n+\tif (do_not_record_p && !reload_completed)\n \t  {\n \t    /* On some machines, we can't record any non-fixed hard register,\n \t       because extending its life will cause reload problems.  We\n@@ -2188,8 +2191,9 @@ hash_rtx (const_rtx x, enum machine_mode mode, int *do_not_record_p,\n \tfor (i = 0; i < units; ++i)\n \t  {\n \t    elt = CONST_VECTOR_ELT (x, i);\n-\t    hash += hash_rtx (elt, GET_MODE (elt), do_not_record_p,\n-\t\t\t      hash_arg_in_memory_p, have_reg_qty);\n+\t    hash += hash_rtx_cb (elt, GET_MODE (elt),\n+                                 do_not_record_p, hash_arg_in_memory_p, \n+                                 have_reg_qty, cb);\n \t  }\n \n \treturn hash;\n@@ -2223,7 +2227,7 @@ hash_rtx (const_rtx x, enum machine_mode mode, int *do_not_record_p,\n     case MEM:\n       /* We don't record if marked volatile or if BLKmode since we don't\n \t know the size of the move.  */\n-      if (MEM_VOLATILE_P (x) || GET_MODE (x) == BLKmode)\n+      if (do_not_record_p && (MEM_VOLATILE_P (x) || GET_MODE (x) == BLKmode))\n \t{\n \t  *do_not_record_p = 1;\n \t  return 0;\n@@ -2270,11 +2274,16 @@ hash_rtx (const_rtx x, enum machine_mode mode, int *do_not_record_p,\n     case CC0:\n     case CALL:\n     case UNSPEC_VOLATILE:\n-      *do_not_record_p = 1;\n-      return 0;\n+      if (do_not_record_p) {\n+        *do_not_record_p = 1;\n+        return 0;\n+      }\n+      else\n+        return hash;\n+      break;\n \n     case ASM_OPERANDS:\n-      if (MEM_VOLATILE_P (x))\n+      if (do_not_record_p && MEM_VOLATILE_P (x))\n \t{\n \t  *do_not_record_p = 1;\n \t  return 0;\n@@ -2291,12 +2300,12 @@ hash_rtx (const_rtx x, enum machine_mode mode, int *do_not_record_p,\n \t    {\n \t      for (i = 1; i < ASM_OPERANDS_INPUT_LENGTH (x); i++)\n \t\t{\n-\t\t  hash += (hash_rtx (ASM_OPERANDS_INPUT (x, i),\n-\t\t\t\t     GET_MODE (ASM_OPERANDS_INPUT (x, i)),\n-\t\t\t\t     do_not_record_p, hash_arg_in_memory_p,\n-\t\t\t\t     have_reg_qty)\n+\t\t  hash += (hash_rtx_cb (ASM_OPERANDS_INPUT (x, i),\n+                                        GET_MODE (ASM_OPERANDS_INPUT (x, i)),\n+                                        do_not_record_p, hash_arg_in_memory_p,\n+                                        have_reg_qty, cb)\n \t\t\t   + hash_rtx_string\n-\t\t\t\t(ASM_OPERANDS_INPUT_CONSTRAINT (x, i)));\n+                           (ASM_OPERANDS_INPUT_CONSTRAINT (x, i)));\n \t\t}\n \n \t      hash += hash_rtx_string (ASM_OPERANDS_INPUT_CONSTRAINT (x, 0));\n@@ -2329,15 +2338,17 @@ hash_rtx (const_rtx x, enum machine_mode mode, int *do_not_record_p,\n \t      x = XEXP (x, i);\n \t      goto repeat;\n \t    }\n-\n-\t  hash += hash_rtx (XEXP (x, i), 0, do_not_record_p,\n-\t\t\t    hash_arg_in_memory_p, have_reg_qty);\n+          \n+\t  hash += hash_rtx_cb (XEXP (x, i), 0, do_not_record_p,\n+                               hash_arg_in_memory_p,\n+                               have_reg_qty, cb);\n \t  break;\n \n \tcase 'E':\n \t  for (j = 0; j < XVECLEN (x, i); j++)\n-\t    hash += hash_rtx (XVECEXP (x, i, j), 0, do_not_record_p,\n-\t\t\t      hash_arg_in_memory_p, have_reg_qty);\n+\t    hash += hash_rtx_cb (XVECEXP (x, i, j), 0, do_not_record_p,\n+                                 hash_arg_in_memory_p,\n+                                 have_reg_qty, cb);\n \t  break;\n \n \tcase 's':\n@@ -2360,6 +2371,27 @@ hash_rtx (const_rtx x, enum machine_mode mode, int *do_not_record_p,\n   return hash;\n }\n \n+/* Hash an rtx.  We are careful to make sure the value is never negative.\n+   Equivalent registers hash identically.\n+   MODE is used in hashing for CONST_INTs only;\n+   otherwise the mode of X is used.\n+\n+   Store 1 in DO_NOT_RECORD_P if any subexpression is volatile.\n+\n+   If HASH_ARG_IN_MEMORY_P is not NULL, store 1 in it if X contains\n+   a MEM rtx which does not have the RTX_UNCHANGING_P bit set.\n+\n+   Note that cse_insn knows that the hash code of a MEM expression\n+   is just (int) MEM plus the hash code of the address.  */\n+\n+unsigned\n+hash_rtx (const_rtx x, enum machine_mode mode, int *do_not_record_p,\n+\t  int *hash_arg_in_memory_p, bool have_reg_qty)\n+{\n+  return hash_rtx_cb (x, mode, do_not_record_p,\n+                      hash_arg_in_memory_p, have_reg_qty, NULL);\n+}\n+\n /* Hash an rtx X for cse via hash_rtx.\n    Stores 1 in do_not_record if any subexpression is volatile.\n    Stores 1 in hash_arg_in_memory if X contains a mem rtx which"}, {"sha": "18e8bef100a4328e1b31a7fba68134e4445d171d", "filename": "gcc/dbgcnt.def", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fdbgcnt.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fdbgcnt.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdbgcnt.def?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -172,6 +172,9 @@ DEBUG_COUNTER (sched_block)\n DEBUG_COUNTER (sched_func)\n DEBUG_COUNTER (sched_insn)\n DEBUG_COUNTER (sched_region)\n+DEBUG_COUNTER (sel_sched_cnt)\n+DEBUG_COUNTER (sel_sched_region_cnt)\n+DEBUG_COUNTER (sel_sched_insn_cnt)\n DEBUG_COUNTER (sms_sched_loop)\n DEBUG_COUNTER (split_for_sched2)\n DEBUG_COUNTER (tail_call)"}, {"sha": "33070a099ec23c8d6d4c028c5d837573a08ef625", "filename": "gcc/doc/invoke.texi", "status": "modified", "additions": 50, "deletions": 1, "changes": 51, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fdoc%2Finvoke.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fdoc%2Finvoke.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Finvoke.texi?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -303,6 +303,7 @@ Objective-C and Objective-C++ Dialects}.\n -feliminate-unused-debug-symbols -femit-class-debug-always @gol\n -fmem-report -fpre-ipa-mem-report -fpost-ipa-mem-report -fprofile-arcs @gol\n -frandom-seed=@var{string} -fsched-verbose=@var{n} @gol\n+-fsel-sched-verbose -fsel-sched-dump-cfg -fsel-sched-pipelining-verbose @gol\n -ftest-coverage  -ftime-report -fvar-tracking @gol\n -g  -g@var{level}  -gcoff -gdwarf-2 @gol\n -ggdb  -gstabs  -gstabs+  -gvms  -gxcoff  -gxcoff+ @gol\n@@ -356,6 +357,8 @@ Objective-C and Objective-C++ Dialects}.\n -fsched2-use-traces -fsched-spec-load -fsched-spec-load-dangerous @gol\n -fsched-stalled-insns-dep[=@var{n}] -fsched-stalled-insns[=@var{n}] @gol\n -fschedule-insns -fschedule-insns2 -fsection-anchors -fsee @gol\n+-fselective-scheduling -fselective-scheduling2 @gol\n+-fsel-sched-pipelining -fsel-sched-pipelining-outer-loops @gol\n -fsignaling-nans -fsingle-precision-constant -fsplit-ivs-in-unroller @gol\n -fsplit-wide-types -fstack-protector -fstack-protector-all @gol\n -fstrict-aliasing -fstrict-overflow -fthread-jumps -ftracer @gol\n@@ -5829,6 +5832,27 @@ The modulo scheduling comes before the traditional scheduling, if a loop\n was modulo scheduled we may want to prevent the later scheduling passes\n from changing its schedule, we use this option to control that.\n \n+@item -fselective-scheduling\n+@opindex fselective-scheduling\n+Schedule instructions using selective scheduling algorithm.  Selective\n+scheduling runs instead of the first scheduler pass.\n+\n+@item -fselective-scheduling2\n+@opindex fselective-scheduling2\n+Schedule instructions using selective scheduling algorithm.  Selective\n+scheduling runs instead of the second scheduler pass.\n+\n+@item -fsel-sched-pipelining\n+@opindex fsel-sched-pipelining\n+Enable software pipelining of innermost loops during selective scheduling.  \n+This option has no effect until one of @option{-fselective-scheduling} or \n+@option{-fselective-scheduling2} is turned on.\n+\n+@item -fsel-sched-pipelining-outer-loops\n+@opindex fsel-sched-pipelining-outer-loops\n+When pipelining loops during selective scheduling, also pipeline outer loops.\n+This option has no effect until @option{-fsel-sched-pipelining} is turned on.\n+\n @item -fcaller-saves\n @opindex fcaller-saves\n Enable values to be allocated in registers that will be clobbered by\n@@ -7325,10 +7349,18 @@ with probably little benefit.  The default value is 100.\n The maximum number of blocks in a region to be considered for\n interblock scheduling.  The default value is 10.\n \n+@item max-pipeline-region-blocks\n+The maximum number of blocks in a region to be considered for\n+pipelining in the selective scheduler.  The default value is 15.\n+\n @item max-sched-region-insns\n The maximum number of insns in a region to be considered for\n interblock scheduling.  The default value is 100.\n \n+@item max-pipeline-region-insns\n+The maximum number of insns in a region to be considered for\n+pipelining in the selective scheduler.  The default value is 200.\n+\n @item min-spec-prob\n The minimum probability (in percents) of reaching a source block\n for interblock speculative scheduling.  The default value is 40.\n@@ -7348,8 +7380,25 @@ The minimal probability of speculation success (in percents), so that\n speculative insn will be scheduled.\n The default value is 40.\n \n-@item max-last-value-rtl\n+@item sched-mem-true-dep-cost\n+Minimal distance (in CPU cycles) between store and load targeting same\n+memory locations.  The default value is 1.\n+\n+@item selsched-max-lookahead\n+The maximum size of the lookahead window of selective scheduling.  It is a\n+depth of search for available instructions.\n+The default value is 50.\n \n+@item selsched-max-sched-times\n+The maximum number of times that an instruction will be scheduled during \n+selective scheduling.  This is the limit on the number of iterations \n+through which the instruction may be pipelined.  The default value is 2.\n+\n+@item selsched-max-insns-to-rename\n+The maximum number of best instructions in the ready list that are considered\n+for renaming in the selective scheduler.  The default value is 2.\n+\n+@item max-last-value-rtl\n The maximum size measured as number of RTLs that can be recorded in an expression\n in combiner for a pseudo register as last known value of that register.  The default\n is 10000."}, {"sha": "d27dfbf9ab25ac58d8fda424f55dbd5883ee3d77", "filename": "gcc/doc/tm.texi", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fdoc%2Ftm.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fdoc%2Ftm.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Ftm.texi?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -6356,6 +6356,29 @@ the instruction stream.  The hook notifies a target backend to extend its\n per instruction data structures.\n @end deftypefn\n \n+@deftypefn {Target Hook} void * TARGET_SCHED_ALLOC_SCHED_CONTEXT (void)\n+Return a pointer to a store large enough to hold target scheduling context.\n+@end deftypefn\n+\n+@deftypefn {Target Hook} void TARGET_SCHED_INIT_SCHED_CONTEXT (void *@var{tc}, bool @var{clean_p})\n+Initialize store pointed to by @var{tc} to hold target scheduling context.\n+It @var{clean_p} is true then initialize @var{tc} as if scheduler is at the\n+beginning of the block.  Overwise, make a copy of the current context in\n+@var{tc}.\n+@end deftypefn\n+\n+@deftypefn {Target Hook} void TARGET_SCHED_SET_SCHED_CONTEXT (void *@var{tc})\n+Copy target scheduling context pointer to by @var{tc} to the current context.\n+@end deftypefn\n+\n+@deftypefn {Target Hook} void TARGET_SCHED_CLEAR_SCHED_CONTEXT (void *@var{tc})\n+Deallocate internal data in target scheduling context pointed to by @var{tc}.\n+@end deftypefn\n+\n+@deftypefn {Target Hook} void TARGET_SCHED_FREE_SCHED_CONTEXT (void *@var{tc})\n+Deallocate a store for target scheduling context pointed to by @var{tc}.\n+@end deftypefn\n+\n @deftypefn {Target Hook} int TARGET_SCHED_SPECULATE_INSN (rtx @var{insn}, int @var{request}, rtx *@var{new_pat})\n This hook is called by the insn scheduler when @var{insn} has only\n speculative dependencies and therefore can be scheduled speculatively."}, {"sha": "9a887f6d6dff28383206d85b207815e17ce7a2e7", "filename": "gcc/emit-rtl.c", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Femit-rtl.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Femit-rtl.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Femit-rtl.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -3985,6 +3985,7 @@ emit_insn_after_1 (rtx first, rtx after, basic_block bb)\n \n   if (after == last_insn)\n     last_insn = last;\n+\n   return last;\n }\n "}, {"sha": "a9b51caeeca6f0dfabced2e42c8145e89d85cbce", "filename": "gcc/final.c", "status": "modified", "additions": 5, "deletions": 2, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Ffinal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Ffinal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ffinal.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -664,7 +664,7 @@ insn_current_reference_address (rtx branch)\n /* Compute branch alignments based on frequency information in the\n    CFG.  */\n \n-static unsigned int\n+unsigned int\n compute_alignments (void)\n {\n   int log, max_skip, max_log;\n@@ -784,7 +784,10 @@ compute_alignments (void)\n     }\n \n   if (dump_file)\n-    loop_optimizer_finalize ();\n+    {\n+      loop_optimizer_finalize ();\n+      free_dominance_info (CDI_DOMINATORS);\n+    }\n   return 0;\n }\n "}, {"sha": "4a209b32c125c9576cac5d29a8523dd8db9741aa", "filename": "gcc/flags.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fflags.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fflags.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fflags.h?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -228,6 +228,9 @@ extern int flag_evaluation_order;\n extern unsigned HOST_WIDE_INT g_switch_value;\n extern bool g_switch_set;\n \n+/* Same for selective scheduling.  */\n+extern bool sel_sched_switch_set;\n+\n /* Values of the -falign-* flags: how much to align labels in code. \n    0 means `use default', 1 means `don't align'.  \n    For each variable, there is an _log variant which is the power"}, {"sha": "bef41cdc327327b74ed6ff23d44cb0d59a4750f7", "filename": "gcc/genattr.c", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fgenattr.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fgenattr.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgenattr.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -184,6 +184,10 @@ main (int argc, char **argv)\n       printf (\"   Use the function if bypass_p returns nonzero for\\n\");\n       printf (\"   the 1st insn. */\\n\");\n       printf (\"extern int insn_latency (rtx, rtx);\\n\\n\");\n+      printf (\"/* Maximal insn latency time possible of all bypasses for this insn.\\n\");\n+      printf (\"   Use the function if bypass_p returns nonzero for\\n\");\n+      printf (\"   the 1st insn. */\\n\");\n+      printf (\"extern int maximal_insn_latency (rtx);\\n\\n\");\n       printf (\"\\n#if AUTOMATON_ALTS\\n\");\n       printf (\"/* The following function returns number of alternative\\n\");\n       printf (\"   reservations of given insn.  It may be used for better\\n\");"}, {"sha": "d314b8f221c47bc0aa6bd74f83ba1e59949be663", "filename": "gcc/genautomata.c", "status": "modified", "additions": 87, "deletions": 16, "changes": 103, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fgenautomata.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fgenautomata.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgenautomata.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -8076,13 +8076,13 @@ output_min_insn_conflict_delay_func (void)\n   fprintf (output_file, \"}\\n\\n\");\n }\n \n-/* Output function `internal_insn_latency'.  */\n+/* Output the array holding default latency values.  These are used in \n+   insn_latency and maximal_insn_latency function implementations.  */\n static void\n-output_internal_insn_latency_func (void)\n+output_default_latencies (void)\n {\n-  decl_t decl;\n-  struct bypass_decl *bypass;\n   int i, j, col;\n+  decl_t decl;\n   const char *tabletype = \"unsigned char\";\n \n   /* Find the smallest integer type that can hold all the default\n@@ -8098,18 +8098,6 @@ output_internal_insn_latency_func (void)\n \t  tabletype = \"int\";\n       }\n \n-  fprintf (output_file, \"static int\\n%s (int %s ATTRIBUTE_UNUSED,\\n\\tint %s ATTRIBUTE_UNUSED,\\n\\trtx %s ATTRIBUTE_UNUSED,\\n\\trtx %s ATTRIBUTE_UNUSED)\\n\",\n-\t   INTERNAL_INSN_LATENCY_FUNC_NAME, INTERNAL_INSN_CODE_NAME,\n-\t   INTERNAL_INSN2_CODE_NAME, INSN_PARAMETER_NAME,\n-\t   INSN2_PARAMETER_NAME);\n-  fprintf (output_file, \"{\\n\");\n-\n-  if (DECL_INSN_RESERV (advance_cycle_insn_decl)->insn_num == 0)\n-    {\n-      fputs (\"  return 0;\\n}\\n\\n\", output_file);\n-      return;\n-    }\n-\n   fprintf (output_file, \"  static const %s default_latencies[] =\\n    {\",\n \t   tabletype);\n \n@@ -8126,6 +8114,27 @@ output_internal_insn_latency_func (void)\n       }\n   gcc_assert (j == DECL_INSN_RESERV (advance_cycle_insn_decl)->insn_num);\n   fputs (\"\\n    };\\n\", output_file);\n+}\n+\n+/* Output function `internal_insn_latency'.  */\n+static void\n+output_internal_insn_latency_func (void)\n+{\n+  int i;\n+  decl_t decl;\n+  struct bypass_decl *bypass;\n+\n+  fprintf (output_file, \"static int\\n%s (int %s ATTRIBUTE_UNUSED,\\n\\tint %s ATTRIBUTE_UNUSED,\\n\\trtx %s ATTRIBUTE_UNUSED,\\n\\trtx %s ATTRIBUTE_UNUSED)\\n\",\n+\t   INTERNAL_INSN_LATENCY_FUNC_NAME, INTERNAL_INSN_CODE_NAME,\n+\t   INTERNAL_INSN2_CODE_NAME, INSN_PARAMETER_NAME,\n+\t   INSN2_PARAMETER_NAME);\n+  fprintf (output_file, \"{\\n\");\n+\n+  if (DECL_INSN_RESERV (advance_cycle_insn_decl)->insn_num == 0)\n+    {\n+      fputs (\"  return 0;\\n}\\n\\n\", output_file);\n+      return;\n+    }\n \n   fprintf (output_file, \"  if (%s >= %s || %s >= %s)\\n    return 0;\\n\",\n \t   INTERNAL_INSN_CODE_NAME, ADVANCE_CYCLE_VALUE_NAME,\n@@ -8171,6 +8180,50 @@ output_internal_insn_latency_func (void)\n \t   INTERNAL_INSN_CODE_NAME);\n }\n \n+/* Output function `internal_maximum_insn_latency'.  */\n+static void\n+output_internal_maximal_insn_latency_func (void)\n+{\n+  decl_t decl;\n+  struct bypass_decl *bypass;\n+  int i;\n+  int max;\n+\n+  fprintf (output_file, \"static int\\n%s (int %s ATTRIBUTE_UNUSED,\\n\\trtx %s ATTRIBUTE_UNUSED)\\n\",\n+\t   \"internal_maximal_insn_latency\", INTERNAL_INSN_CODE_NAME,\n+\t   INSN_PARAMETER_NAME);\n+  fprintf (output_file, \"{\\n\");\n+\n+  if (DECL_INSN_RESERV (advance_cycle_insn_decl)->insn_num == 0)\n+    {\n+      fputs (\"  return 0;\\n}\\n\\n\", output_file);\n+      return;\n+    }\n+\n+  fprintf (output_file, \"  switch (%s)\\n    {\\n\", INTERNAL_INSN_CODE_NAME);\n+  for (i = 0; i < description->decls_num; i++)\n+    if (description->decls[i]->mode == dm_insn_reserv\n+\t&& DECL_INSN_RESERV (description->decls[i])->bypass_list)\n+      {\n+\tdecl = description->decls [i];\n+        max = DECL_INSN_RESERV (decl)->default_latency;\n+\tfprintf (output_file,\n+\t\t \"    case %d: {\",\n+\t\t DECL_INSN_RESERV (decl)->insn_num);\n+\tfor (bypass = DECL_INSN_RESERV (decl)->bypass_list;\n+\t     bypass != NULL;\n+\t     bypass = bypass->next)\n+\t  {\n+\t    if (bypass->latency > max)\n+              max = bypass->latency;\n+\t  }\n+\tfprintf (output_file, \" return %d; }\\n      break;\\n\", max);\n+      }\n+\n+  fprintf (output_file, \"    }\\n  return default_latencies[%s];\\n}\\n\\n\",\n+\t   INTERNAL_INSN_CODE_NAME);\n+}\n+\n /* The function outputs PHR interface function `insn_latency'.  */\n static void\n output_insn_latency_func (void)\n@@ -8189,6 +8242,21 @@ output_insn_latency_func (void)\n \t   INSN_PARAMETER_NAME, INSN2_PARAMETER_NAME);\n }\n \n+/* The function outputs PHR interface function `maximal_insn_latency'.  */\n+static void\n+output_maximal_insn_latency_func (void)\n+{\n+  fprintf (output_file, \"int\\n%s (rtx %s)\\n\",\n+\t   \"maximal_insn_latency\", INSN_PARAMETER_NAME);\n+  fprintf (output_file, \"{\\n  int %s;\\n\",\n+\t   INTERNAL_INSN_CODE_NAME);\n+  output_internal_insn_code_evaluation (INSN_PARAMETER_NAME,\n+\t\t\t\t\tINTERNAL_INSN_CODE_NAME, 0);\n+  fprintf (output_file, \"  return %s (%s, %s);\\n}\\n\\n\",\n+\t   \"internal_maximal_insn_latency\",\n+\t   INTERNAL_INSN_CODE_NAME, INSN_PARAMETER_NAME);\n+}\n+\n /* The function outputs PHR interface function `print_reservation'.  */\n static void\n output_print_reservation_func (void)\n@@ -9179,8 +9247,11 @@ write_automata (void)\n   output_internal_reset_func ();\n   output_reset_func ();\n   output_min_insn_conflict_delay_func ();\n+  output_default_latencies ();\n   output_internal_insn_latency_func ();\n   output_insn_latency_func ();\n+  output_internal_maximal_insn_latency_func ();\n+  output_maximal_insn_latency_func ();\n   output_print_reservation_func ();\n   /* Output function get_cpu_unit_code.  */\n   fprintf (output_file, \"\\n#if %s\\n\\n\", CPU_UNITS_QUERY_MACRO_NAME);"}, {"sha": "2b5130c66b93a3b03cfef55fcc520931f017072a", "filename": "gcc/haifa-sched.c", "status": "modified", "additions": 1046, "deletions": 549, "changes": 1595, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fhaifa-sched.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fhaifa-sched.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fhaifa-sched.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -144,15 +144,17 @@ along with GCC; see the file COPYING3.  If not see\n #include \"target.h\"\n #include \"output.h\"\n #include \"params.h\"\n+#include \"vecprim.h\"\n #include \"dbgcnt.h\"\n+#include \"cfgloop.h\"\n \n #ifdef INSN_SCHEDULING\n \n /* issue_rate is the number of insns that can be scheduled in the same\n    machine cycle.  It can be defined in the config/mach/mach.h file,\n    otherwise we set it to 1.  */\n \n-static int issue_rate;\n+int issue_rate;\n \n /* sched-verbose controls the amount of debugging output the\n    scheduler prints.  It is controlled by -fsched-verbose=N:\n@@ -170,9 +172,6 @@ int sched_verbose = 0;\n    either to stderr, or to the dump listing file (-dRS).  */\n FILE *sched_dump = 0;\n \n-/* Highest uid before scheduling.  */\n-static int old_max_uid;\n-\n /* fix_sched_param() is called from toplev.c upon detection\n    of the -fsched-verbose=N option.  */\n \n@@ -185,10 +184,12 @@ fix_sched_param (const char *param, const char *val)\n     warning (0, \"fix_sched_param: unknown param: %s\", param);\n }\n \n-struct haifa_insn_data *h_i_d;\n+/* This is a placeholder for the scheduler parameters common \n+   to all schedulers.  */\n+struct common_sched_info_def *common_sched_info;\n \n-#define INSN_TICK(INSN)\t\t(h_i_d[INSN_UID (INSN)].tick)\n-#define INTER_TICK(INSN)        (h_i_d[INSN_UID (INSN)].inter_tick)\n+#define INSN_TICK(INSN)\t(HID (INSN)->tick)\n+#define INTER_TICK(INSN) (HID (INSN)->inter_tick)\n \n /* If INSN_TICK of an instruction is equal to INVALID_TICK,\n    then it should be recalculated from scratch.  */\n@@ -202,12 +203,12 @@ struct haifa_insn_data *h_i_d;\n \n /* List of important notes we must keep around.  This is a pointer to the\n    last element in the list.  */\n-static rtx note_list;\n+rtx note_list;\n \n static struct spec_info_def spec_info_var;\n /* Description of the speculative part of the scheduling.\n    If NULL - no speculation.  */\n-spec_info_t spec_info;\n+spec_info_t spec_info = NULL;\n \n /* True, if recovery block was added during scheduling of current block.\n    Used to determine, if we need to fix INSN_TICKs.  */\n@@ -224,12 +225,16 @@ static int nr_begin_data, nr_be_in_data, nr_begin_control, nr_be_in_control;\n /* Array used in {unlink, restore}_bb_notes.  */\n static rtx *bb_header = 0;\n \n-/* Number of basic_blocks.  */\n-static int old_last_basic_block;\n-\n /* Basic block after which recovery blocks will be created.  */\n static basic_block before_recovery;\n \n+/* Basic block just before the EXIT_BLOCK and after recovery, if we have\n+   created it.  */\n+basic_block after_recovery;\n+\n+/* FALSE if we add bb to another region, so we don't need to initialize it.  */\n+bool adding_bb_to_current_region_p = true;\n+\n /* Queues, etc.  */\n \n /* An instruction is ready to be scheduled when all insns preceding it\n@@ -290,45 +295,29 @@ static int q_size = 0;\n    QUEUE_READY     - INSN is in ready list.\n    N >= 0 - INSN queued for X [where NEXT_Q_AFTER (q_ptr, X) == N] cycles.  */\n    \n-#define QUEUE_INDEX(INSN) (h_i_d[INSN_UID (INSN)].queue_index)\n+#define QUEUE_INDEX(INSN) (HID (INSN)->queue_index)\n \n /* The following variable value refers for all current and future\n    reservations of the processor units.  */\n state_t curr_state;\n \n /* The following variable value is size of memory representing all\n    current and future reservations of the processor units.  */\n-static size_t dfa_state_size;\n+size_t dfa_state_size;\n \n /* The following array is used to find the best insn from ready when\n    the automaton pipeline interface is used.  */\n-static char *ready_try;\n+char *ready_try = NULL;\n \n-/* Describe the ready list of the scheduler.\n-   VEC holds space enough for all insns in the current region.  VECLEN\n-   says how many exactly.\n-   FIRST is the index of the element with the highest priority; i.e. the\n-   last one in the ready list, since elements are ordered by ascending\n-   priority.\n-   N_READY determines how many insns are on the ready list.  */\n+/* The ready list.  */\n+struct ready_list ready = {NULL, 0, 0, 0};\n \n-struct ready_list\n-{\n-  rtx *vec;\n-  int veclen;\n-  int first;\n-  int n_ready;\n-};\n-\n-/* The pointer to the ready list.  */\n-static struct ready_list *readyp;\n+/* The pointer to the ready list (to be removed).  */\n+static struct ready_list *readyp = &ready;\n \n /* Scheduling clock.  */\n static int clock_var;\n \n-/* Number of instructions in current scheduling region.  */\n-static int rgn_n_insns;\n-\n static int may_trap_exp (const_rtx, int);\n \n /* Nonzero iff the address is comprised from at most 1 register.  */\n@@ -342,6 +331,39 @@ static int may_trap_exp (const_rtx, int);\n /* Returns a class that insn with GET_DEST(insn)=x may belong to,\n    as found by analyzing insn's expression.  */\n \n+\f\n+static int haifa_luid_for_non_insn (rtx x);\n+\n+/* Haifa version of sched_info hooks common to all headers.  */\n+const struct common_sched_info_def haifa_common_sched_info = \n+  {\n+    NULL, /* fix_recovery_cfg */\n+    NULL, /* add_block */\n+    NULL, /* estimate_number_of_insns */\n+    haifa_luid_for_non_insn, /* luid_for_non_insn */\n+    SCHED_PASS_UNKNOWN /* sched_pass_id */\n+  };\n+\n+const struct sched_scan_info_def *sched_scan_info;\n+\n+/* Mapping from instruction UID to its Logical UID.  */\n+VEC (int, heap) *sched_luids = NULL;\n+\n+/* Next LUID to assign to an instruction.  */\n+int sched_max_luid = 1;\n+\n+/* Haifa Instruction Data.  */\n+VEC (haifa_insn_data_def, heap) *h_i_d = NULL;\n+\n+void (* sched_init_only_bb) (basic_block, basic_block);\n+\n+/* Split block function.  Different schedulers might use different functions\n+   to handle their internal data consistent.  */\n+basic_block (* sched_split_block) (basic_block, rtx);\n+\n+/* Create empty basic block after the specified block.  */\n+basic_block (* sched_create_empty_bb) (basic_block);\n+\n static int\n may_trap_exp (const_rtx x, int is_store)\n {\n@@ -478,10 +500,6 @@ haifa_classify_insn (const_rtx insn)\n   return haifa_classify_rtx (PATTERN (insn));\n }\n \n-\n-/* A typedef for rtx vector.  */\n-typedef VEC(rtx, heap) *rtx_vec_t;\n-\n /* Forward declarations.  */\n \n static int priority (rtx);\n@@ -490,10 +508,11 @@ static void swap_sort (rtx *, int);\n static void queue_insn (rtx, int);\n static int schedule_insn (rtx);\n static int find_set_reg_weight (const_rtx);\n-static void find_insn_reg_weight (basic_block);\n-static void find_insn_reg_weight1 (rtx);\n+static void find_insn_reg_weight (const_rtx);\n static void adjust_priority (rtx);\n static void advance_one_cycle (void);\n+static void extend_h_i_d (void);\n+\n \n /* Notes handling mechanism:\n    =========================\n@@ -511,27 +530,18 @@ static void advance_one_cycle (void);\n    unlink_other_notes ()).  After scheduling the block, these notes are\n    inserted at the beginning of the block (in schedule_block()).  */\n \n-static rtx unlink_other_notes (rtx, rtx);\n-static void reemit_notes (rtx);\n-\n-static rtx *ready_lastpos (struct ready_list *);\n static void ready_add (struct ready_list *, rtx, bool);\n-static void ready_sort (struct ready_list *);\n static rtx ready_remove_first (struct ready_list *);\n \n static void queue_to_ready (struct ready_list *);\n static int early_queue_to_ready (state_t, struct ready_list *);\n \n static void debug_ready_list (struct ready_list *);\n \n-static void move_insn (rtx);\n-\n /* The following functions are used to implement multi-pass scheduling\n    on the first cycle.  */\n-static rtx ready_element (struct ready_list *, int);\n static rtx ready_remove (struct ready_list *, int);\n static void ready_remove_insn (rtx);\n-static int max_issue (struct ready_list *, int *, int);\n \n static int choose_ready (struct ready_list *, rtx *);\n \n@@ -543,23 +553,17 @@ static void change_queue_index (rtx, int);\n    speculative instructions.  */\n \n static void extend_h_i_d (void);\n-static void extend_ready (int);\n static void init_h_i_d (rtx);\n static void generate_recovery_code (rtx);\n static void process_insn_forw_deps_be_in_spec (rtx, rtx, ds_t);\n static void begin_speculative_block (rtx);\n static void add_to_speculative_block (rtx);\n-static dw_t dep_weak (ds_t);\n-static edge find_fallthru_edge (basic_block);\n-static void init_before_recovery (void);\n-static basic_block create_recovery_block (void);\n+static void init_before_recovery (basic_block *);\n static void create_check_block_twin (rtx, bool);\n static void fix_recovery_deps (basic_block);\n-static void change_pattern (rtx, rtx);\n-static int speculate_insn (rtx, ds_t, rtx *);\n+static void haifa_change_pattern (rtx, rtx);\n static void dump_new_block_header (int, basic_block, rtx, rtx);\n static void restore_bb_notes (basic_block);\n-static void extend_bb (void);\n static void fix_jump_move (rtx);\n static void move_block_after_check (rtx);\n static void move_succs (VEC(edge,gc) **, basic_block);\n@@ -575,7 +579,7 @@ static void check_cfg (rtx, rtx);\n #endif /* INSN_SCHEDULING */\n \f\n /* Point to state used for the current scheduling pass.  */\n-struct sched_info *current_sched_info;\n+struct haifa_sched_info *current_sched_info;\n \f\n #ifndef INSN_SCHEDULING\n void\n@@ -584,9 +588,6 @@ schedule_insns (void)\n }\n #else\n \n-/* Working copy of frontend's sched_info variable.  */\n-static struct sched_info current_sched_info_var;\n-\n /* Pointer to the last instruction scheduled.  Used by rank_for_schedule,\n    so that insns independent of the last scheduled insn will be preferred\n    over dependent instructions.  */\n@@ -595,15 +596,29 @@ static rtx last_scheduled_insn;\n \n /* Cached cost of the instruction.  Use below function to get cost of the\n    insn.  -1 here means that the field is not initialized.  */\n-#define INSN_COST(INSN)\t\t(h_i_d[INSN_UID (INSN)].cost)\n+#define INSN_COST(INSN)\t(HID (INSN)->cost)\n \n /* Compute cost of executing INSN.\n    This is the number of cycles between instruction issue and\n    instruction results.  */\n HAIFA_INLINE int\n insn_cost (rtx insn)\n {\n-  int cost = INSN_COST (insn);\n+  int cost;\n+\n+  if (sel_sched_p ())\n+    {\n+      if (recog_memoized (insn) < 0)\n+\treturn 0;\n+\n+      cost = insn_default_latency (insn);\n+      if (cost < 0)\n+\tcost = 0;\n+\n+      return cost;\n+    }\n+\n+  cost = INSN_COST (insn);\n \n   if (cost < 0)\n     {\n@@ -633,7 +648,7 @@ insn_cost (rtx insn)\n    This is the number of cycles between instruction issue and\n    instruction results.  */\n int\n-dep_cost (dep_t link)\n+dep_cost_1 (dep_t link, dw_t dw)\n {\n   rtx used = DEP_CON (link);\n   int cost;\n@@ -664,8 +679,14 @@ dep_cost (dep_t link)\n \t  else if (bypass_p (insn))\n \t    cost = insn_latency (insn, used);\n \t}\n+\t\n \n-      if (targetm.sched.adjust_cost != NULL)\n+      if (targetm.sched.adjust_cost_2)\n+\t{\n+\t  cost = targetm.sched.adjust_cost_2 (used, (int) dep_type, insn, cost,\n+\t\t\t\t\t      dw);\n+\t}\n+      else if (targetm.sched.adjust_cost != NULL)\n \t{\n \t  /* This variable is used for backward compatibility with the\n \t     targets.  */\n@@ -691,6 +712,34 @@ dep_cost (dep_t link)\n   return cost;\n }\n \n+/* Compute cost of dependence LINK.\n+   This is the number of cycles between instruction issue and\n+   instruction results.  */\n+int\n+dep_cost (dep_t link)\n+{\n+  return dep_cost_1 (link, 0);\n+}\n+\n+/* Use this sel-sched.c friendly function in reorder2 instead of increasing\n+   INSN_PRIORITY explicitly.  */\n+void\n+increase_insn_priority (rtx insn, int amount)\n+{\n+  if (!sel_sched_p ())\n+    {\n+      /* We're dealing with haifa-sched.c INSN_PRIORITY.  */\n+      if (INSN_PRIORITY_KNOWN (insn))\n+\t  INSN_PRIORITY (insn) += amount;\n+    }\n+  else\n+    {\n+      /* In sel-sched.c INSN_PRIORITY is not kept up to date.  \n+\t Use EXPR_PRIORITY instead. */\n+      sel_add_to_insn_priority (insn, amount);\n+    }\n+}\n+\n /* Return 'true' if DEP should be included in priority calculations.  */\n static bool\n contributes_to_priority_p (dep_t dep)\n@@ -706,7 +755,7 @@ contributes_to_priority_p (dep_t dep)\n      their producers will increase, and, thus, the\n      producers will more likely be scheduled, thus,\n      resolving the dependence.  */\n-  if ((current_sched_info->flags & DO_SPECULATION)\n+  if (sched_deps_info->generate_spec_deps\n       && !(spec_info->flags & COUNT_SPEC_IN_CRITICAL_PATH)\n       && (DEP_STATUS (dep) & SPECULATIVE))\n     return false;\n@@ -726,7 +775,7 @@ priority (rtx insn)\n \n   if (!INSN_PRIORITY_KNOWN (insn))\n     {\n-      int this_priority = 0;\n+      int this_priority = -1;\n \n       if (sd_lists_empty_p (insn, SD_LIST_FORW))\n \t/* ??? We should set INSN_PRIORITY to insn_cost when and insn has\n@@ -745,7 +794,8 @@ priority (rtx insn)\n \t     INSN_FORW_DEPS list of each instruction in the corresponding\n \t     recovery block.  */ \n \n-\t  rec = RECOVERY_BLOCK (insn);\n+          /* Selective scheduling does not define RECOVERY_BLOCK macro.  */\n+\t  rec = sel_sched_p () ? NULL : RECOVERY_BLOCK (insn);\n \t  if (!rec || rec == EXIT_BLOCK_PTR)\n \t    {\n \t      prev_first = PREV_INSN (insn);\n@@ -798,6 +848,14 @@ priority (rtx insn)\n \t    }\n \t  while (twin != prev_first);\n \t}\n+\n+      if (this_priority < 0)\n+\t{\n+\t  gcc_assert (this_priority == -1);\n+\n+\t  this_priority = insn_cost (insn);\n+\t}\n+\n       INSN_PRIORITY (insn) = this_priority;\n       INSN_PRIORITY_STATUS (insn) = 1;\n     }\n@@ -849,13 +907,13 @@ rank_for_schedule (const void *x, const void *y)\n \n       ds1 = TODO_SPEC (tmp) & SPECULATIVE;\n       if (ds1)\n-\tdw1 = dep_weak (ds1);\n+\tdw1 = ds_weak (ds1);\n       else\n \tdw1 = NO_DEP_WEAK;\n       \n       ds2 = TODO_SPEC (tmp2) & SPECULATIVE;\n       if (ds2)\n-\tdw2 = dep_weak (ds2);\n+\tdw2 = ds_weak (ds2);\n       else\n \tdw2 = NO_DEP_WEAK;\n \n@@ -962,7 +1020,7 @@ queue_insn (rtx insn, int n_cycles)\n \n       fprintf (sched_dump, \"queued for %d cycles.\\n\", n_cycles);\n     }\n-  \n+\n   QUEUE_INDEX (insn) = next_q;\n }\n \n@@ -979,7 +1037,7 @@ queue_remove (rtx insn)\n /* Return a pointer to the bottom of the ready list, i.e. the insn\n    with the lowest priority.  */\n \n-HAIFA_INLINE static rtx *\n+rtx *\n ready_lastpos (struct ready_list *ready)\n {\n   gcc_assert (ready->n_ready >= 1);\n@@ -1052,7 +1110,7 @@ ready_remove_first (struct ready_list *ready)\n    insn with the highest priority is 0, and the lowest priority has\n    N_READY - 1.  */\n \n-HAIFA_INLINE static rtx\n+rtx\n ready_element (struct ready_list *ready, int index)\n {\n   gcc_assert (ready->n_ready && index < ready->n_ready);\n@@ -1099,7 +1157,7 @@ ready_remove_insn (rtx insn)\n /* Sort the ready list READY by ascending priority, using the SCHED_SORT\n    macro.  */\n \n-HAIFA_INLINE static void\n+void\n ready_sort (struct ready_list *ready)\n {\n   rtx *first = ready_lastpos (ready);\n@@ -1125,27 +1183,36 @@ adjust_priority (rtx prev)\n       targetm.sched.adjust_priority (prev, INSN_PRIORITY (prev));\n }\n \n-/* Advance time on one cycle.  */\n-HAIFA_INLINE static void\n-advance_one_cycle (void)\n+/* Advance DFA state STATE on one cycle.  */\n+void\n+advance_state (state_t state)\n {\n   if (targetm.sched.dfa_pre_advance_cycle)\n     targetm.sched.dfa_pre_advance_cycle ();\n \n   if (targetm.sched.dfa_pre_cycle_insn)\n-    state_transition (curr_state,\n+    state_transition (state,\n \t\t      targetm.sched.dfa_pre_cycle_insn ());\n \n-  state_transition (curr_state, NULL);\n+  state_transition (state, NULL);\n   \n   if (targetm.sched.dfa_post_cycle_insn)\n-    state_transition (curr_state,\n+    state_transition (state,\n \t\t      targetm.sched.dfa_post_cycle_insn ());\n \n   if (targetm.sched.dfa_post_advance_cycle)\n     targetm.sched.dfa_post_advance_cycle ();\n }\n \n+/* Advance time on one cycle.  */\n+HAIFA_INLINE static void\n+advance_one_cycle (void)\n+{\n+  advance_state (curr_state);\n+  if (sched_verbose >= 6)\n+    fprintf (sched_dump, \"\\n;;\\tAdvanced a state.\\n\");\n+}\n+\n /* Clock at which the previous instruction was issued.  */\n static int last_clock_var;\n \n@@ -1256,10 +1323,45 @@ schedule_insn (rtx insn)\n \n /* Functions for handling of notes.  */\n \n+/* Insert the INSN note at the end of the notes list.  */\n+static void \n+add_to_note_list (rtx insn, rtx *note_list_end_p)\n+{\n+  PREV_INSN (insn) = *note_list_end_p;\n+  if (*note_list_end_p)\n+    NEXT_INSN (*note_list_end_p) = insn;\n+  *note_list_end_p = insn;\n+}\n+\n+/* Add note list that ends on FROM_END to the end of TO_ENDP.  */\n+void\n+concat_note_lists (rtx from_end, rtx *to_endp)\n+{\n+  rtx from_start;\n+\n+  if (from_end == NULL)\n+    /* It's easy when have nothing to concat.  */\n+    return;\n+\n+  if (*to_endp == NULL)\n+    /* It's also easy when destination is empty.  */\n+    {\n+      *to_endp = from_end;\n+      return;\n+    }\n+\n+  from_start = from_end;\n+  /* A note list should be traversed via PREV_INSN.  */\n+  while (PREV_INSN (from_start) != NULL) \n+    from_start = PREV_INSN (from_start);\n+\n+  add_to_note_list (from_start, to_endp);\n+  *to_endp = from_end;\n+}\n+\n /* Delete notes beginning with INSN and put them in the chain\n    of notes ended by NOTE_LIST.\n    Returns the insn following the notes.  */\n-\n static rtx\n unlink_other_notes (rtx insn, rtx tail)\n {\n@@ -1290,22 +1392,22 @@ unlink_other_notes (rtx insn, rtx tail)\n       /* See sched_analyze to see how these are handled.  */\n       if (NOTE_KIND (insn) != NOTE_INSN_EH_REGION_BEG\n \t  && NOTE_KIND (insn) != NOTE_INSN_EH_REGION_END)\n-\t{\n-\t  /* Insert the note at the end of the notes list.  */\n-\t  PREV_INSN (insn) = note_list;\n-\t  if (note_list)\n-\t    NEXT_INSN (note_list) = insn;\n-\t  note_list = insn;\n-\t}\n+        add_to_note_list (insn, &note_list);\n \n       insn = next;\n     }\n+\n+  if (insn == tail)\n+    {\n+      gcc_assert (sel_sched_p ());\n+      return prev;\n+    }\n+\n   return insn;\n }\n \n /* Return the head and tail pointers of ebb starting at BEG and ending\n    at END.  */\n-\n void\n get_ebb_head_tail (basic_block beg, basic_block end, rtx *headp, rtx *tailp)\n {\n@@ -1358,8 +1460,7 @@ no_real_insns_p (const_rtx head, const_rtx tail)\n \n /* Delete notes between HEAD and TAIL and put them in the chain\n    of notes ended by NOTE_LIST.  */\n-\n-void\n+static void\n rm_other_notes (rtx head, rtx tail)\n {\n   rtx next_tail;\n@@ -1380,20 +1481,80 @@ rm_other_notes (rtx head, rtx tail)\n       if (NOTE_NOT_BB_P (insn))\n \t{\n \t  prev = insn;\n-\n \t  insn = unlink_other_notes (insn, next_tail);\n \n-\t  gcc_assert (prev != tail && prev != head && insn != next_tail);\n+\t  gcc_assert ((sel_sched_p ()\n+\t\t       || prev != tail) && prev != head && insn != next_tail);\n+\t}\n+    }\n+}\n+\n+/* Same as above, but also process REG_SAVE_NOTEs of HEAD.  */\n+void\n+remove_notes (rtx head, rtx tail)\n+{\n+  /* rm_other_notes only removes notes which are _inside_ the\n+     block---that is, it won't remove notes before the first real insn\n+     or after the last real insn of the block.  So if the first insn\n+     has a REG_SAVE_NOTE which would otherwise be emitted before the\n+     insn, it is redundant with the note before the start of the\n+     block, and so we have to take it out.  */\n+  if (INSN_P (head))\n+    {\n+      rtx note;\n+\n+      for (note = REG_NOTES (head); note; note = XEXP (note, 1))\n+\tif (REG_NOTE_KIND (note) == REG_SAVE_NOTE)\n+\t  remove_note (head, note);\n+    }\n+\n+  /* Remove remaining note insns from the block, save them in\n+     note_list.  These notes are restored at the end of\n+     schedule_block ().  */\n+  rm_other_notes (head, tail);\n+}\n+\n+/* Restore-other-notes: NOTE_LIST is the end of a chain of notes\n+   previously found among the insns.  Insert them just before HEAD.  */\n+rtx\n+restore_other_notes (rtx head, basic_block head_bb)\n+{\n+  if (note_list != 0)\n+    {\n+      rtx note_head = note_list;\n+\n+      if (head)\n+\thead_bb = BLOCK_FOR_INSN (head);\n+      else\n+\thead = NEXT_INSN (bb_note (head_bb));\n+\n+      while (PREV_INSN (note_head))\n+\t{\n+\t  set_block_for_insn (note_head, head_bb);\n+\t  note_head = PREV_INSN (note_head);\n \t}\n+      /* In the above cycle we've missed this note.  */\n+      set_block_for_insn (note_head, head_bb);\n+\n+      PREV_INSN (note_head) = PREV_INSN (head);\n+      NEXT_INSN (PREV_INSN (head)) = note_head;\n+      PREV_INSN (head) = note_list;\n+      NEXT_INSN (note_list) = head;\n+\n+      if (BLOCK_FOR_INSN (head) != head_bb)\n+\tBB_END (head_bb) = note_list;\n+\n+      head = note_head;\n     }\n+\n+  return head;\n }\n \n /* Functions for computation of registers live/usage info.  */\n \n /* This function looks for a new register being defined.\n    If the destination register is already used by the source,\n    a new register is not needed.  */\n-\n static int\n find_set_reg_weight (const_rtx x)\n {\n@@ -1415,25 +1576,9 @@ find_set_reg_weight (const_rtx x)\n   return 0;\n }\n \n-/* Calculate INSN_REG_WEIGHT for all insns of a block.  */\n-\n-static void\n-find_insn_reg_weight (basic_block bb)\n-{\n-  rtx insn, next_tail, head, tail;\n-\n-  get_ebb_head_tail (bb, bb, &head, &tail);\n-  next_tail = NEXT_INSN (tail);\n-\n-  for (insn = head; insn != next_tail; insn = NEXT_INSN (insn))\n-    find_insn_reg_weight1 (insn);    \n-}\n-\n-/* Calculate INSN_REG_WEIGHT for single instruction.\n-   Separated from find_insn_reg_weight because of need\n-   to initialize new instruction in generate_recovery_code.  */\n+/* Calculate INSN_REG_WEIGHT for INSN.  */\n static void\n-find_insn_reg_weight1 (rtx insn)\n+find_insn_reg_weight (const_rtx insn)\n {\n   int reg_weight = 0;\n   rtx x;\n@@ -1739,8 +1884,7 @@ debug_ready_list (struct ready_list *ready)\n    NOTEs.  The REG_SAVE_NOTE note following first one is contains the\n    saved value for NOTE_BLOCK_NUMBER which is useful for\n    NOTE_INSN_EH_REGION_{BEG,END} NOTEs.  */\n-\n-static void\n+void\n reemit_notes (rtx insn)\n {\n   rtx note, last = insn;\n@@ -1759,10 +1903,8 @@ reemit_notes (rtx insn)\n \n /* Move INSN.  Reemit notes if needed.  Update CFG, if needed.  */\n static void\n-move_insn (rtx insn)\n+move_insn (rtx insn, rtx last, rtx nt)\n {\n-  rtx last = last_scheduled_insn;\n-\n   if (PREV_INSN (insn) != last)\n     {\n       basic_block bb;\n@@ -1782,9 +1924,10 @@ move_insn (rtx insn)\n \t  jump_p = control_flow_insn_p (insn);\n \n \t  gcc_assert (!jump_p\n-\t\t      || ((current_sched_info->flags & SCHED_RGN)\n+\t\t      || ((common_sched_info->sched_pass_id == SCHED_RGN_PASS)\n \t\t\t  && IS_SPECULATION_BRANCHY_CHECK_P (insn))\n-\t\t      || (current_sched_info->flags & SCHED_EBB));\n+\t\t      || (common_sched_info->sched_pass_id\n+\t\t\t  == SCHED_EBB_PASS));\n \t  \n \t  gcc_assert (BLOCK_FOR_INSN (PREV_INSN (insn)) == bb);\n \n@@ -1796,8 +1939,7 @@ move_insn (rtx insn)\n       if (jump_p)\n \t/* We move the block note along with jump.  */\n \t{\n-\t  /* NT is needed for assertion below.  */\n-\t  rtx nt = current_sched_info->next_tail;\n+\t  gcc_assert (nt);\n \n \t  note = NEXT_INSN (insn);\n \t  while (NOTE_NOT_BB_P (note) && note != nt)\n@@ -1840,8 +1982,6 @@ move_insn (rtx insn)\n       if (BB_END (bb) == last)\n \tBB_END (bb) = insn;  \n     }\n-  \n-  reemit_notes (insn);\n \n   SCHED_GROUP_P (insn) = 0;  \n }\n@@ -1866,7 +2006,10 @@ static struct choice_entry *choice_stack;\n /* The following variable value is number of essential insns issued on\n    the current cycle.  An insn is essential one if it changes the\n    processors state.  */\n-static int cycle_issued_insns;\n+int cycle_issued_insns;\n+\n+/* This holds the value of the target dfa_lookahead hook.  */\n+int dfa_lookahead;\n \n /* The following variable value is maximal number of tries of issuing\n    insns for the first cycle multipass insn scheduling.  We define\n@@ -1897,89 +2040,157 @@ static int cached_issue_rate = 0;\n    of all instructions in READY.  The function stops immediately,\n    if it reached the such a solution, that all instruction can be issued.\n    INDEX will contain index of the best insn in READY.  The following\n-   function is used only for first cycle multipass scheduling.  */\n-static int\n-max_issue (struct ready_list *ready, int *index, int max_points)\n+   function is used only for first cycle multipass scheduling.\n+\n+   PRIVILEGED_N >= 0\n+\n+   This function expects recognized insns only.  All USEs,\n+   CLOBBERs, etc must be filtered elsewhere.  */\n+int\n+max_issue (struct ready_list *ready, int privileged_n, state_t state,\n+\t   int *index)\n {\n-  int n, i, all, n_ready, best, delay, tries_num, points = -1;\n+  int n, i, all, n_ready, best, delay, tries_num, points = -1, max_points;\n+  int more_issue;\n   struct choice_entry *top;\n   rtx insn;\n \n+  n_ready = ready->n_ready;\n+  gcc_assert (dfa_lookahead >= 1 && privileged_n >= 0\n+\t      && privileged_n <= n_ready);\n+\n+  /* Init MAX_LOOKAHEAD_TRIES.  */\n+  if (cached_first_cycle_multipass_dfa_lookahead != dfa_lookahead)\n+    {\n+      cached_first_cycle_multipass_dfa_lookahead = dfa_lookahead;\n+      max_lookahead_tries = 100;\n+      for (i = 0; i < issue_rate; i++)\n+\tmax_lookahead_tries *= dfa_lookahead;\n+    }\n+\n+  /* Init max_points.  */\n+  max_points = 0;\n+  more_issue = issue_rate - cycle_issued_insns;\n+  gcc_assert (more_issue >= 0);\n+\n+  for (i = 0; i < n_ready; i++)\n+    if (!ready_try [i])\n+      {\n+\tif (more_issue-- > 0)\n+\t  max_points += ISSUE_POINTS (ready_element (ready, i));\n+\telse\n+\t  break;\n+      }\n+\n+  /* The number of the issued insns in the best solution.  */\n   best = 0;\n-  memcpy (choice_stack->state, curr_state, dfa_state_size);\n+\n   top = choice_stack;\n-  top->rest = cached_first_cycle_multipass_dfa_lookahead;\n+\n+  /* Set initial state of the search.  */\n+  memcpy (top->state, state, dfa_state_size);\n+  top->rest = dfa_lookahead;\n   top->n = 0;\n-  n_ready = ready->n_ready;\n+\n+  /* Count the number of the insns to search among.  */\n   for (all = i = 0; i < n_ready; i++)\n     if (!ready_try [i])\n       all++;\n+\n+  /* I is the index of the insn to try next.  */\n   i = 0;\n   tries_num = 0;\n   for (;;)\n     {\n-      if (top->rest == 0 || i >= n_ready)\n+      if (/* If we've reached a dead end or searched enough of what we have\n+\t     been asked...  */\n+\t  top->rest == 0\n+\t  /* Or have nothing else to try.  */\n+\t  || i >= n_ready)\n \t{\n+\t  /* ??? (... || i == n_ready).  */\n+\t  gcc_assert (i <= n_ready);\n+\n \t  if (top == choice_stack)\n \t    break;\n-\t  if (best < top - choice_stack && ready_try [0])\n+\n+\t  if (best < top - choice_stack)\n \t    {\n-\t      best = top - choice_stack;\n-\t      *index = choice_stack [1].index;\n-\t      points = top->n;\n-\t      if (top->n == max_points || best == all)\n-\t\tbreak;\n+\t      if (privileged_n)\n+\t\t{\n+\t\t  n = privileged_n;\n+\t\t  /* Try to find issued privileged insn.  */\n+\t\t  while (n && !ready_try[--n]);\n+\t\t}\n+\n+\t      if (/* If all insns are equally good...  */\n+\t\t  privileged_n == 0\n+\t\t  /* Or a privileged insn will be issued.  */\n+\t\t  || ready_try[n])\n+\t\t/* Then we have a solution.  */\n+\t\t{\n+\t\t  best = top - choice_stack;\n+\t\t  /* This is the index of the insn issued first in this\n+\t\t     solution.  */\n+\t\t  *index = choice_stack [1].index;\n+\t\t  points = top->n;\n+\t\t  if (top->n == max_points || best == all)\n+\t\t    break;\n+\t\t}\n \t    }\n+\n+\t  /* Set ready-list index to point to the last insn\n+\t     ('i++' below will advance it to the next insn).  */\n \t  i = top->index;\n+\n+\t  /* Backtrack.  */\n \t  ready_try [i] = 0;\n \t  top--;\n-\t  memcpy (curr_state, top->state, dfa_state_size);\n+\t  memcpy (state, top->state, dfa_state_size);\n \t}\n       else if (!ready_try [i])\n \t{\n \t  tries_num++;\n \t  if (tries_num > max_lookahead_tries)\n \t    break;\n \t  insn = ready_element (ready, i);\n-\t  delay = state_transition (curr_state, insn);\n+\t  delay = state_transition (state, insn);\n \t  if (delay < 0)\n \t    {\n-\t      if (state_dead_lock_p (curr_state))\n+\t      if (state_dead_lock_p (state))\n \t\ttop->rest = 0;\n \t      else\n \t\ttop->rest--;\n+\n \t      n = top->n;\n-\t      if (memcmp (top->state, curr_state, dfa_state_size) != 0)\n+\t      if (memcmp (top->state, state, dfa_state_size) != 0)\n \t\tn += ISSUE_POINTS (insn);\n+\n+\t      /* Advance to the next choice_entry.  */\n \t      top++;\n-\t      top->rest = cached_first_cycle_multipass_dfa_lookahead;\n+\t      /* Initialize it.  */\n+\t      top->rest = dfa_lookahead;\n \t      top->index = i;\n \t      top->n = n;\n-\t      memcpy (top->state, curr_state, dfa_state_size);\n+\t      memcpy (top->state, state, dfa_state_size);\n+\n \t      ready_try [i] = 1;\n \t      i = -1;\n \t    }\n \t}\n+\n+      /* Increase ready-list index.  */\n       i++;\n     }\n-  while (top != choice_stack)\n-    {\n-      ready_try [top->index] = 0;\n-      top--;\n-    }\n-  memcpy (curr_state, choice_stack->state, dfa_state_size);  \n \n-  if (sched_verbose >= 4)    \n-    fprintf (sched_dump, \";;\\t\\tChoosed insn : %s; points: %d/%d\\n\",\n-\t     (*current_sched_info->print_insn) (ready_element (ready, *index),\n-\t\t\t\t\t\t0), \n-\t     points, max_points);\n-  \n+  /* Restore the original state of the DFA.  */\n+  memcpy (state, choice_stack->state, dfa_state_size);  \n+\n   return best;\n }\n \n /* The following function chooses insn from READY and modifies\n-   *N_READY and READY.  The following function is used only for first\n+   READY.  The following function is used only for first\n    cycle multipass scheduling.\n    Return:\n    -1 if cycle should be advanced,\n@@ -2022,15 +2233,9 @@ choose_ready (struct ready_list *ready, rtx *insn_ptr)\n       /* Try to choose the better insn.  */\n       int index = 0, i, n;\n       rtx insn;\n-      int more_issue, max_points, try_data = 1, try_control = 1;\n+      int try_data = 1, try_control = 1;\n+      ds_t ts;\n       \n-      if (cached_first_cycle_multipass_dfa_lookahead != lookahead)\n-\t{\n-\t  cached_first_cycle_multipass_dfa_lookahead = lookahead;\n-\t  max_lookahead_tries = 100;\n-\t  for (i = 0; i < issue_rate; i++)\n-\t    max_lookahead_tries *= lookahead;\n-\t}\n       insn = ready_element (ready, 0);\n       if (INSN_CODE (insn) < 0)\n \t{\n@@ -2069,43 +2274,63 @@ choose_ready (struct ready_list *ready, rtx *insn_ptr)\n \t    }\n \t}\n \n-      if ((!try_data && (TODO_SPEC (insn) & DATA_SPEC))\n-\t  || (!try_control && (TODO_SPEC (insn) & CONTROL_SPEC))\n-\t  || (targetm.sched.first_cycle_multipass_dfa_lookahead_guard_spec\n-\t      && !targetm.sched.first_cycle_multipass_dfa_lookahead_guard_spec\n-\t      (insn)))\n+      ts = TODO_SPEC (insn);\n+      if ((ts & SPECULATIVE)\n+\t  && (((!try_data && (ts & DATA_SPEC))\n+\t       || (!try_control && (ts & CONTROL_SPEC)))\n+\t      || (targetm.sched.first_cycle_multipass_dfa_lookahead_guard_spec\n+\t\t  && !targetm.sched\n+\t\t  .first_cycle_multipass_dfa_lookahead_guard_spec (insn))))\n \t/* Discard speculative instruction that stands first in the ready\n \t   list.  */\n \t{\n \t  change_queue_index (insn, 1);\n \t  return 1;\n \t}\n \n-      max_points = ISSUE_POINTS (insn);\n-      more_issue = issue_rate - cycle_issued_insns - 1;\n+      ready_try[0] = 0;\n \n       for (i = 1; i < ready->n_ready; i++)\n \t{\n \t  insn = ready_element (ready, i);\n+\n \t  ready_try [i]\n-\t    = (INSN_CODE (insn) < 0\n-               || (!try_data && (TODO_SPEC (insn) & DATA_SPEC))\n-               || (!try_control && (TODO_SPEC (insn) & CONTROL_SPEC))\n-\t       || (targetm.sched.first_cycle_multipass_dfa_lookahead_guard\n-\t\t   && !targetm.sched.first_cycle_multipass_dfa_lookahead_guard\n-\t\t   (insn)));\n-\n-\t  if (!ready_try [i] && more_issue-- > 0)\n-\t    max_points += ISSUE_POINTS (insn);\n+\t    = ((!try_data && (TODO_SPEC (insn) & DATA_SPEC))\n+               || (!try_control && (TODO_SPEC (insn) & CONTROL_SPEC)));\n \t}\n \n-      if (max_issue (ready, &index, max_points) == 0)\n+      /* Let the target filter the search space.  */\n+      for (i = 1; i < ready->n_ready; i++)\n+\tif (!ready_try[i])\n+\t  {\n+\t    insn = ready_element (ready, i);\n+\n+\t    gcc_assert (INSN_CODE (insn) >= 0\n+\t\t\t|| recog_memoized (insn) < 0);\n+\n+\t    ready_try [i]\n+\t      = (/* INSN_CODE check can be omitted here as it is also done later\n+\t\t    in max_issue ().  */\n+\t\t INSN_CODE (insn) < 0\n+\t\t || (targetm.sched.first_cycle_multipass_dfa_lookahead_guard\n+\t\t     && !targetm.sched.first_cycle_multipass_dfa_lookahead_guard\n+\t\t     (insn)));\n+\t  }\n+\n+      if (max_issue (ready, 1, curr_state, &index) == 0)\n \t{\n+\t  if (sched_verbose >= 4)\n+\t    fprintf (sched_dump, \";;\\t\\tChosen none\\n\");\n \t  *insn_ptr = ready_remove_first (ready);\n \t  return 0;\n \t}\n       else\n \t{\n+\t  if (sched_verbose >= 4)    \n+\t    fprintf (sched_dump, \";;\\t\\tChosen insn : %s\\n\",\n+\t\t     (*current_sched_info->print_insn)\n+\t\t     (ready_element (ready, index), 0));\n+          \n \t  *insn_ptr = ready_remove (ready, index);\n \t  return 0;\n \t}\n@@ -2117,9 +2342,8 @@ choose_ready (struct ready_list *ready, rtx *insn_ptr)\n    region.  */\n \n void\n-schedule_block (basic_block *target_bb, int rgn_n_insns1)\n+schedule_block (basic_block *target_bb)\n {\n-  struct ready_list ready;\n   int i, first_cycle_insn_p;\n   int can_issue_more;\n   state_t temp_state = NULL;  /* It is used for multipass scheduling.  */\n@@ -2148,15 +2372,7 @@ schedule_block (basic_block *target_bb, int rgn_n_insns1)\n \n   state_reset (curr_state);\n \n-  /* Allocate the ready list.  */\n-  readyp = &ready;\n-  ready.vec = NULL;\n-  ready_try = NULL;\n-  choice_stack = NULL;\n-\n-  rgn_n_insns = -1;\n-  extend_ready (rgn_n_insns1 + 1);\n-\n+  /* Clear the ready list.  */\n   ready.first = ready.veclen - 1;\n   ready.n_ready = 0;\n \n@@ -2443,7 +2659,8 @@ schedule_block (basic_block *target_bb, int rgn_n_insns1)\n \t  (*current_sched_info->begin_schedule_ready) (insn,\n \t\t\t\t\t\t       last_scheduled_insn);\n  \n-\t  move_insn (insn);\n+\t  move_insn (insn, last_scheduled_insn, current_sched_info->next_tail);\n+\t  reemit_notes (insn);\n \t  last_scheduled_insn = insn;\n \t  \n \t  if (memcmp (curr_state, temp_state, dfa_state_size) != 0)\n@@ -2530,6 +2747,9 @@ schedule_block (basic_block *target_bb, int rgn_n_insns1)\n \t  }\n     }\n \n+  if (sched_verbose)\n+    fprintf (sched_dump, \";;   total time = %d\\n\", clock_var);\n+\n   if (!current_sched_info->queue_must_finish_empty\n       || haifa_recovery_bb_recently_added_p)\n     {\n@@ -2545,59 +2765,25 @@ schedule_block (basic_block *target_bb, int rgn_n_insns1)\n   if (targetm.sched.md_finish)\n     {\n       targetm.sched.md_finish (sched_dump, sched_verbose);\n-\n       /* Target might have added some instructions to the scheduled block\n \t in its md_finish () hook.  These new insns don't have any data\n \t initialized and to identify them we extend h_i_d so that they'll\n-\t get zero luids.*/\n-      extend_h_i_d ();\n+\t get zero luids.  */\n+      sched_init_luids (NULL, NULL, NULL, NULL);\n     }\n \n+  if (sched_verbose)\n+    fprintf (sched_dump, \";;   new head = %d\\n;;   new tail = %d\\n\\n\",\n+\t     INSN_UID (head), INSN_UID (tail));\n+\n   /* Update head/tail boundaries.  */\n   head = NEXT_INSN (prev_head);\n   tail = last_scheduled_insn;\n \n-  /* Restore-other-notes: NOTE_LIST is the end of a chain of notes\n-     previously found among the insns.  Insert them at the beginning\n-     of the insns.  */\n-  if (note_list != 0)\n-    {\n-      basic_block head_bb = BLOCK_FOR_INSN (head);\n-      rtx note_head = note_list;\n-\n-      while (PREV_INSN (note_head))\n-\t{\n-\t  set_block_for_insn (note_head, head_bb);\n-\t  note_head = PREV_INSN (note_head);\n-\t}\n-      /* In the above cycle we've missed this note:  */\n-      set_block_for_insn (note_head, head_bb);\n-\n-      PREV_INSN (note_head) = PREV_INSN (head);\n-      NEXT_INSN (PREV_INSN (head)) = note_head;\n-      PREV_INSN (head) = note_list;\n-      NEXT_INSN (note_list) = head;\n-      head = note_head;\n-    }\n-\n-  /* Debugging.  */\n-  if (sched_verbose)\n-    {\n-      fprintf (sched_dump, \";;   total time = %d\\n;;   new head = %d\\n\",\n-\t       clock_var, INSN_UID (head));\n-      fprintf (sched_dump, \";;   new tail = %d\\n\\n\",\n-\t       INSN_UID (tail));\n-    }\n+  head = restore_other_notes (head, NULL);\n \n   current_sched_info->head = head;\n   current_sched_info->tail = tail;\n-\n-  free (ready.vec);\n-\n-  free (ready_try);\n-  for (i = 0; i <= rgn_n_insns; i++)\n-    free (choice_stack [i].state);\n-  free (choice_stack);\n }\n \f\n /* Set_priorities: compute priority of each insn in the block.  */\n@@ -2636,48 +2822,49 @@ set_priorities (rtx head, rtx tail)\n   return n_insn;\n }\n \n-/* Next LUID to assign to an instruction.  */\n-static int luid;\n+/* Set dump and sched_verbose for the desired debugging output.  If no\n+   dump-file was specified, but -fsched-verbose=N (any N), print to stderr.\n+   For -fsched-verbose=N, N>=10, print everything to stderr.  */\n+void\n+setup_sched_dump (void)\n+{\n+  sched_verbose = sched_verbose_param;\n+  if (sched_verbose_param == 0 && dump_file)\n+    sched_verbose = 1;\n+  sched_dump = ((sched_verbose_param >= 10 || !dump_file)\n+\t\t? stderr : dump_file);\n+}\n \n-/* Initialize some global state for the scheduler.  */\n+/* Initialize some global state for the scheduler.  This function works \n+   with the common data shared between all the schedulers.  It is called\n+   from the scheduler specific initialization routine.  */\n \n void\n sched_init (void)\n {\n-  basic_block b;\n-  rtx insn;\n-  int i;\n-\n-  /* Switch to working copy of sched_info.  */\n-  memcpy (&current_sched_info_var, current_sched_info,\n-\t  sizeof (current_sched_info_var));\n-  current_sched_info = &current_sched_info_var;\n-      \n   /* Disable speculative loads in their presence if cc0 defined.  */\n #ifdef HAVE_cc0\n   flag_schedule_speculative_load = 0;\n #endif\n \n-  /* Set dump and sched_verbose for the desired debugging output.  If no\n-     dump-file was specified, but -fsched-verbose=N (any N), print to stderr.\n-     For -fsched-verbose=N, N>=10, print everything to stderr.  */\n-  sched_verbose = sched_verbose_param;\n-  if (sched_verbose_param == 0 && dump_file)\n-    sched_verbose = 1;\n-  sched_dump = ((sched_verbose_param >= 10 || !dump_file)\n-\t\t? stderr : dump_file);\n-\n   /* Initialize SPEC_INFO.  */\n   if (targetm.sched.set_sched_flags)\n     {\n       spec_info = &spec_info_var;\n       targetm.sched.set_sched_flags (spec_info);\n-      if (current_sched_info->flags & DO_SPECULATION)\n-\tspec_info->weakness_cutoff =\n-\t  (PARAM_VALUE (PARAM_SCHED_SPEC_PROB_CUTOFF) * MAX_DEP_WEAK) / 100;\n+\n+      if (spec_info->mask != 0)\n+        {\n+          spec_info->data_weakness_cutoff =\n+            (PARAM_VALUE (PARAM_SCHED_SPEC_PROB_CUTOFF) * MAX_DEP_WEAK) / 100;\n+          spec_info->control_weakness_cutoff =\n+            (PARAM_VALUE (PARAM_SCHED_SPEC_PROB_CUTOFF)\n+             * REG_BR_PROB_BASE) / 100;\n+        }\n       else\n \t/* So we won't read anything accidentally.  */\n-\tspec_info = 0;\n+\tspec_info = NULL;\n+\n     }\n   else\n     /* So we won't read anything accidentally.  */\n@@ -2696,18 +2883,10 @@ sched_init (void)\n       cached_first_cycle_multipass_dfa_lookahead = 0;\n     }\n \n-  old_max_uid = 0;\n-  h_i_d = 0;\n-  extend_h_i_d ();\n-\n-  for (i = 0; i < old_max_uid; i++)\n-    {\n-      h_i_d[i].cost = -1;\n-      h_i_d[i].todo_spec = HARD_DEP;\n-      h_i_d[i].queue_index = QUEUE_NOWHERE;\n-      h_i_d[i].tick = INVALID_TICK;\n-      h_i_d[i].inter_tick = INVALID_TICK;\n-    }\n+  if (targetm.sched.first_cycle_multipass_dfa_lookahead)\n+    dfa_lookahead = targetm.sched.first_cycle_multipass_dfa_lookahead ();\n+  else\n+    dfa_lookahead = 0;\n \n   if (targetm.sched.init_dfa_pre_cycle_insn)\n     targetm.sched.init_dfa_pre_cycle_insn ();\n@@ -2717,67 +2896,93 @@ sched_init (void)\n \n   dfa_start ();\n   dfa_state_size = state_size ();\n-  curr_state = xmalloc (dfa_state_size);\n \n-  h_i_d[0].luid = 0;\n-  luid = 1;\n-  FOR_EACH_BB (b)\n-    for (insn = BB_HEAD (b); ; insn = NEXT_INSN (insn))\n-      {\n-\tINSN_LUID (insn) = luid;\n+  init_alias_analysis ();\n \n-\t/* Increment the next luid, unless this is a note.  We don't\n-\t   really need separate IDs for notes and we don't want to\n-\t   schedule differently depending on whether or not there are\n-\t   line-number notes, i.e., depending on whether or not we're\n-\t   generating debugging information.  */\n-\tif (!NOTE_P (insn))\n-\t  ++luid;\n+  df_set_flags (DF_LR_RUN_DCE);\n+  df_note_add_problem ();\n \n-\tif (insn == BB_END (b))\n-\t  break;\n-      }\n+  /* More problems needed for interloop dep calculation in SMS.  */\n+  if (common_sched_info->sched_pass_id == SCHED_SMS_PASS)\n+    {\n+      df_rd_add_problem ();\n+      df_chain_add_problem (DF_DU_CHAIN + DF_UD_CHAIN);\n+    }\n \n-  init_dependency_caches (luid);\n+  df_analyze ();\n+  \n+  /* Do not run DCE after reload, as this can kill nops inserted \n+     by bundling.  */\n+  if (reload_completed)\n+    df_clear_flags (DF_LR_RUN_DCE);\n \n-  init_alias_analysis ();\n+  regstat_compute_calls_crossed ();\n \n-  old_last_basic_block = 0;\n-  extend_bb ();\n+  if (targetm.sched.md_init_global)\n+    targetm.sched.md_init_global (sched_dump, sched_verbose,\n+\t\t\t\t  get_max_uid () + 1);\n \n-  /* Compute INSN_REG_WEIGHT for all blocks.  We must do this before\n-     removing death notes.  */\n-  FOR_EACH_BB_REVERSE (b)\n-    find_insn_reg_weight (b);\n+  curr_state = xmalloc (dfa_state_size);\n+}\n \n-  if (targetm.sched.md_init_global)\n-      targetm.sched.md_init_global (sched_dump, sched_verbose, old_max_uid);\n+static void haifa_init_only_bb (basic_block, basic_block);\n \n-  nr_begin_data = nr_begin_control = nr_be_in_data = nr_be_in_control = 0;\n-  before_recovery = 0;\n+/* Initialize data structures specific to the Haifa scheduler.  */\n+void\n+haifa_sched_init (void)\n+{\n+  setup_sched_dump ();\n+  sched_init ();\n+\n+  if (spec_info != NULL)\n+    {\n+      sched_deps_info->use_deps_list = 1;\n+      sched_deps_info->generate_spec_deps = 1;\n+    }\n+\n+  /* Initialize luids, dependency caches, target and h_i_d for the\n+     whole function.  */\n+  {\n+    bb_vec_t bbs = VEC_alloc (basic_block, heap, n_basic_blocks);\n+    basic_block bb;\n \n+    sched_init_bbs ();\n+\n+    FOR_EACH_BB (bb)\n+      VEC_quick_push (basic_block, bbs, bb);\n+    sched_init_luids (bbs, NULL, NULL, NULL);\n+    sched_deps_init (true);\n+    sched_extend_target ();\n+    haifa_init_h_i_d (bbs, NULL, NULL, NULL);\n+\n+    VEC_free (basic_block, heap, bbs);\n+  }\n+\n+  sched_init_only_bb = haifa_init_only_bb;\n+  sched_split_block = sched_split_block_1;\n+  sched_create_empty_bb = sched_create_empty_bb_1;\n   haifa_recovery_bb_ever_added_p = false;\n \n #ifdef ENABLE_CHECKING\n-  /* This is used preferably for finding bugs in check_cfg () itself.  */\n+  /* This is used preferably for finding bugs in check_cfg () itself.\n+     We must call sched_bbs_init () before check_cfg () because check_cfg ()\n+     assumes that the last insn in the last bb has a non-null successor.  */\n   check_cfg (0, 0);\n #endif\n-}\n \n-/* Free global data used during insn scheduling.  */\n+  nr_begin_data = nr_begin_control = nr_be_in_data = nr_be_in_control = 0;\n+  before_recovery = 0;\n+  after_recovery = 0;\n+}\n \n+/* Finish work with the data specific to the Haifa scheduler.  */\n void\n-sched_finish (void)\n+haifa_sched_finish (void)\n {\n-  free (h_i_d);\n-  free (curr_state);\n-  dfa_finish ();\n-  free_dependency_caches ();\n-  end_alias_analysis ();\n+  sched_create_empty_bb = NULL;\n+  sched_split_block = NULL;\n+  sched_init_only_bb = NULL;\n \n-  if (targetm.sched.md_finish_global)\n-    targetm.sched.md_finish_global (sched_dump, sched_verbose);\n-  \n   if (spec_info && spec_info->dump)\n     {\n       char c = reload_completed ? 'a' : 'b';\n@@ -2799,13 +3004,37 @@ sched_finish (void)\n                c, nr_be_in_control);\n     }\n \n+  /* Finalize h_i_d, dependency caches, and luids for the whole\n+     function.  Target will be finalized in md_global_finish ().  */\n+  sched_deps_finish ();\n+  sched_finish_luids ();\n+  current_sched_info = NULL;\n+  sched_finish ();\n+}\n+\n+/* Free global data used during insn scheduling.  This function works with \n+   the common data shared between the schedulers.  */\n+\n+void\n+sched_finish (void)\n+{\n+  haifa_finish_h_i_d ();\n+  free (curr_state);\n+\n+  if (targetm.sched.md_finish_global)\n+    targetm.sched.md_finish_global (sched_dump, sched_verbose);\n+\n+  end_alias_analysis ();\n+\n+  regstat_free_calls_crossed ();\n+\n+  dfa_finish ();\n+\n #ifdef ENABLE_CHECKING\n   /* After reload ia64 backend clobbers CFG, so can't check anything.  */\n   if (!reload_completed)\n     check_cfg (0, 0);\n #endif\n-\n-  current_sched_info = NULL;\n }\n \n /* Fix INSN_TICKs of the instructions in the current block as well as\n@@ -2881,6 +3110,8 @@ fix_inter_tick (rtx head, rtx tail)\n     }\n   bitmap_clear (&processed);\n }\n+\n+static int haifa_speculate_insn (rtx, ds_t, rtx *);\n   \n /* Check if NEXT is ready to be added to the ready or queue list.\n    If \"yes\", add it to the proper list.\n@@ -2940,7 +3171,7 @@ try_ready (rtx next)\n \t\t*ts = ds_merge (*ts, ds);\n \t    }\n \n-\t  if (dep_weak (*ts) < spec_info->weakness_cutoff)\n+\t  if (ds_weak (*ts) < spec_info->data_weakness_cutoff)\n \t    /* Too few points.  */\n \t    *ts = (*ts & ~SPECULATIVE) | HARD_DEP;\n \t}\n@@ -2974,7 +3205,7 @@ try_ready (rtx next)\n       \n       gcc_assert ((*ts & SPECULATIVE) && !(*ts & ~SPECULATIVE));\n       \n-      res = speculate_insn (next, *ts, &new_pat);\n+      res = haifa_speculate_insn (next, *ts, &new_pat);\n \t\n       switch (res)\n \t{\n@@ -2998,7 +3229,7 @@ try_ready (rtx next)\n \t       save it.  */\n \t    ORIG_PAT (next) = PATTERN (next);\n \t  \n-\t  change_pattern (next, new_pat);\n+\t  haifa_change_pattern (next, new_pat);\n \t  break;\n \t  \n \tdefault:\n@@ -3029,7 +3260,7 @@ try_ready (rtx next)\n        ORIG_PAT field.  Except one case - speculation checks have ORIG_PAT\n        pat too, so skip them.  */\n     {\n-      change_pattern (next, ORIG_PAT (next));\n+      haifa_change_pattern (next, ORIG_PAT (next));\n       ORIG_PAT (next) = 0;\n     }\n \n@@ -3148,98 +3379,70 @@ change_queue_index (rtx next, int delay)\n     }\n }\n \n-/* Extend H_I_D data.  */\n-static void\n-extend_h_i_d (void)\n-{\n-  /* We use LUID 0 for the fake insn (UID 0) which holds dependencies for\n-     pseudos which do not cross calls.  */\n-  int new_max_uid = get_max_uid () + 1;  \n-\n-  h_i_d = (struct haifa_insn_data *)\n-    xrecalloc (h_i_d, new_max_uid, old_max_uid, sizeof (*h_i_d));\n-  old_max_uid = new_max_uid;\n-\n-  if (targetm.sched.h_i_d_extended)\n-    targetm.sched.h_i_d_extended ();\n-}\n+static int sched_ready_n_insns = -1;\n \n-/* Extend READY, READY_TRY and CHOICE_STACK arrays.\n-   N_NEW_INSNS is the number of additional elements to allocate.  */\n-static void\n-extend_ready (int n_new_insns)\n+/* Initialize per region data structures.  */\n+void\n+sched_extend_ready_list (int new_sched_ready_n_insns)\n {\n   int i;\n \n-  readyp->veclen = rgn_n_insns + n_new_insns + 1 + issue_rate;\n-  readyp->vec = XRESIZEVEC (rtx, readyp->vec, readyp->veclen);\n- \n-  ready_try = (char *) xrecalloc (ready_try, rgn_n_insns + n_new_insns + 1,\n-\t\t\t\t  rgn_n_insns + 1, sizeof (char));\n+  if (sched_ready_n_insns == -1)\n+    /* At the first call we need to initialize one more choice_stack\n+       entry.  */\n+    {\n+      i = 0;\n+      sched_ready_n_insns = 0;\n+    }\n+  else\n+    i = sched_ready_n_insns + 1;\n \n-  rgn_n_insns += n_new_insns;\n+  ready.veclen = new_sched_ready_n_insns + issue_rate;\n+  ready.vec = XRESIZEVEC (rtx, ready.vec, ready.veclen);\n \n-  choice_stack = XRESIZEVEC (struct choice_entry, choice_stack,\n-\t\t\t     rgn_n_insns + 1);\n+  gcc_assert (new_sched_ready_n_insns >= sched_ready_n_insns);\n \n-  for (i = rgn_n_insns; n_new_insns--; i--)\n-    choice_stack[i].state = xmalloc (dfa_state_size);\n-}\n+  ready_try = (char *) xrecalloc (ready_try, new_sched_ready_n_insns,\n+                                  sched_ready_n_insns, sizeof (*ready_try));\n \n-/* Extend global-scope scheduler data structures\n-   (those, that live within one call to schedule_insns)\n-   to include information about just emitted INSN.  */\n-static void\n-extend_global_data (rtx insn)\n-{\n-  gcc_assert (INSN_P (insn));\n+  /* We allocate +1 element to save initial state in the choice_stack[0]\n+     entry.  */\n+  choice_stack = XRESIZEVEC (struct choice_entry, choice_stack,\n+\t\t\t     new_sched_ready_n_insns + 1);\n \n-  /* Init h_i_d.  */\n-  extend_h_i_d ();\n-  init_h_i_d (insn);\n+  for (; i <= new_sched_ready_n_insns; i++)\n+    choice_stack[i].state = xmalloc (dfa_state_size);\n \n-  /* Extend dependency caches by one element.  */\n-  extend_dependency_caches (1, false);\n+  sched_ready_n_insns = new_sched_ready_n_insns;\n }\n \n-/* Extend global- and region-scope scheduler data structures\n-   (those, that live within one call to schedule_region)\n-   to include information about just emitted INSN.  */\n-static void\n-extend_region_data (rtx insn)\n+/* Free per region data structures.  */\n+void\n+sched_finish_ready_list (void)\n {\n-  extend_global_data (insn);\n+  int i;\n \n-  /* Init dependency data.  */\n-  sd_init_insn (insn);\n-}\n+  free (ready.vec);\n+  ready.vec = NULL;\n+  ready.veclen = 0;\n \n-/* Extend global-, region- and block-scope scheduler data structures\n-   (those, that live within one call to schedule_block)\n-   to include information about just emitted INSN.  */\n-static void\n-extend_block_data (rtx insn)\n-{\n-  extend_region_data (insn);\n+  free (ready_try);\n+  ready_try = NULL;\n \n-  /* These structures have block scope.  */\n-  extend_ready (1);\n+  for (i = 0; i <= sched_ready_n_insns; i++)\n+    free (choice_stack [i].state);\n+  free (choice_stack);\n+  choice_stack = NULL;\n \n-  (*current_sched_info->add_remove_insn) (insn, 0);\n+  sched_ready_n_insns = -1;\n }\n \n-/* Initialize h_i_d entry of the new INSN with default values.\n-   Values, that are not explicitly initialized here, hold zero.  */\n-static void\n-init_h_i_d (rtx insn)\n+static int\n+haifa_luid_for_non_insn (rtx x)\n {\n-  INSN_LUID (insn) = luid++;\n-  INSN_COST (insn) = -1;\n-  TODO_SPEC (insn) = HARD_DEP;\n-  QUEUE_INDEX (insn) = QUEUE_NOWHERE;\n-  INSN_TICK (insn) = INVALID_TICK;\n-  INTER_TICK (insn) = INVALID_TICK;\n-  find_insn_reg_weight1 (insn);\n+  gcc_assert (NOTE_P (x) || LABEL_P (x));\n+\n+  return 0;\n }\n \n /* Generates recovery code for INSN.  */\n@@ -3290,7 +3493,7 @@ process_insn_forw_deps_be_in_spec (rtx insn, rtx twin, ds_t fs)\n \t\t     it can be removed from the ready (or queue) list only\n \t\t     due to backend decision.  Hence we can't let the\n \t\t     probability of the speculative dep to decrease.  */\n-\t\t  dep_weak (ds) <= dep_weak (fs))\n+\t\t  ds_weak (ds) <= ds_weak (fs))\n \t\t{\n \t\t  ds_t new_ds;\n \n@@ -3331,6 +3534,8 @@ begin_speculative_block (rtx insn)\n   TODO_SPEC (insn) &= ~BEGIN_SPEC;\n }\n \n+static void haifa_init_insn (rtx);\n+\n /* Generates recovery code for BE_IN speculative INSN.  */\n static void\n add_to_speculative_block (rtx insn)\n@@ -3398,7 +3603,7 @@ add_to_speculative_block (rtx insn)\n       rec = BLOCK_FOR_INSN (check);\n \n       twin = emit_insn_before (copy_insn (PATTERN (insn)), BB_END (rec));\n-      extend_region_data (twin);\n+      haifa_init_insn (twin);\n \n       sd_copy_back_deps (twin, insn, true);\n \n@@ -3479,44 +3684,9 @@ xrecalloc (void *p, size_t new_nmemb, size_t old_nmemb, size_t size)\n   return p;\n }\n \n-/* Return the probability of speculation success for the speculation\n-   status DS.  */\n-static dw_t\n-dep_weak (ds_t ds)\n-{\n-  ds_t res = 1, dt;\n-  int n = 0;\n-\n-  dt = FIRST_SPEC_TYPE;\n-  do\n-    {\n-      if (ds & dt)\n-\t{\n-\t  res *= (ds_t) get_dep_weak (ds, dt);\n-\t  n++;\n-\t}\n-\n-      if (dt == LAST_SPEC_TYPE)\n-\tbreak;\n-      dt <<= SPEC_TYPE_SHIFT;\n-    }\n-  while (1);\n-\n-  gcc_assert (n);\n-  while (--n)\n-    res /= MAX_DEP_WEAK;\n-\n-  if (res < MIN_DEP_WEAK)\n-    res = MIN_DEP_WEAK;\n-\n-  gcc_assert (res <= MAX_DEP_WEAK);\n-\n-  return (dw_t) res;\n-}\n-\n /* Helper function.\n    Find fallthru edge from PRED.  */\n-static edge\n+edge\n find_fallthru_edge (basic_block pred)\n {\n   edge e;\n@@ -3548,9 +3718,37 @@ find_fallthru_edge (basic_block pred)\n   return NULL;\n }\n \n+/* Extend per basic block data structures.  */\n+static void\n+sched_extend_bb (void)\n+{\n+  rtx insn;\n+\n+  /* The following is done to keep current_sched_info->next_tail non null.  */\n+  insn = BB_END (EXIT_BLOCK_PTR->prev_bb);\n+  if (NEXT_INSN (insn) == 0\n+      || (!NOTE_P (insn)\n+\t  && !LABEL_P (insn)\n+\t  /* Don't emit a NOTE if it would end up before a BARRIER.  */\n+\t  && !BARRIER_P (NEXT_INSN (insn))))\n+    {\n+      rtx note = emit_note_after (NOTE_INSN_DELETED, insn);\n+      /* Make insn appear outside BB.  */\n+      set_block_for_insn (note, NULL);\n+      BB_END (EXIT_BLOCK_PTR->prev_bb) = insn;\n+    }\n+}\n+\n+/* Init per basic block data structures.  */\n+void\n+sched_init_bbs (void)\n+{\n+  sched_extend_bb ();\n+}\n+\n /* Initialize BEFORE_RECOVERY variable.  */\n static void\n-init_before_recovery (void)\n+init_before_recovery (basic_block *before_recovery_ptr)\n {\n   basic_block last;\n   edge e;\n@@ -3569,10 +3767,24 @@ init_before_recovery (void)\n       basic_block single, empty;\n       rtx x, label;\n \n-      single = create_empty_bb (last);\n-      empty = create_empty_bb (single);            \n+      /* If the fallthrough edge to exit we've found is from the block we've \n+\t created before, don't do anything more.  */\n+      if (last == after_recovery)\n+\treturn;\n \n-      single->count = last->count;     \n+      adding_bb_to_current_region_p = false;\n+\n+      single = sched_create_empty_bb (last);\n+      empty = sched_create_empty_bb (single);\n+\n+      /* Add new blocks to the root loop.  */\n+      if (current_loops != NULL)\n+\t{\n+\t  add_bb_to_loop (single, VEC_index (loop_p, current_loops->larray, 0));\n+\t  add_bb_to_loop (empty, VEC_index (loop_p, current_loops->larray, 0));\n+\t}\n+\n+      single->count = last->count;\n       empty->count = last->count;\n       single->frequency = last->frequency;\n       empty->frequency = last->frequency;\n@@ -3588,14 +3800,20 @@ init_before_recovery (void)\n       x = emit_jump_insn_after (gen_jump (label), BB_END (single));\n       JUMP_LABEL (x) = label;\n       LABEL_NUSES (label)++;\n-      extend_global_data (x);\n+      haifa_init_insn (x);\n           \n       emit_barrier_after (x);\n \n-      add_block (empty, 0);\n-      add_block (single, 0);\n+      sched_init_only_bb (empty, NULL);\n+      sched_init_only_bb (single, NULL);\n+      sched_extend_bb ();\n \n+      adding_bb_to_current_region_p = true;\n       before_recovery = single;\n+      after_recovery = empty;\n+\n+      if (before_recovery_ptr)\n+        *before_recovery_ptr = before_recovery;\n \n       if (sched_verbose >= 2 && spec_info->dump)\n         fprintf (spec_info->dump,\n@@ -3607,8 +3825,8 @@ init_before_recovery (void)\n }\n \n /* Returns new recovery block.  */\n-static basic_block\n-create_recovery_block (void)\n+basic_block\n+sched_create_recovery_block (basic_block *before_recovery_ptr)\n {\n   rtx label;\n   rtx barrier;\n@@ -3617,8 +3835,7 @@ create_recovery_block (void)\n   haifa_recovery_bb_recently_added_p = true;\n   haifa_recovery_bb_ever_added_p = true;\n \n-  if (!before_recovery)\n-    init_before_recovery ();\n+  init_before_recovery (before_recovery_ptr);\n \n   barrier = get_last_bb_insn (before_recovery);\n   gcc_assert (BARRIER_P (barrier));\n@@ -3627,7 +3844,7 @@ create_recovery_block (void)\n \n   rec = create_basic_block (label, label, before_recovery);\n \n-  /* Recovery block always end with an unconditional jump.  */\n+  /* A recovery block always ends with an unconditional jump.  */\n   emit_barrier_after (BB_END (rec));\n \n   if (BB_PARTITION (before_recovery) != BB_UNPARTITIONED)\n@@ -3637,11 +3854,55 @@ create_recovery_block (void)\n     fprintf (spec_info->dump, \";;\\t\\tGenerated recovery block rec%d\\n\",\n              rec->index);\n \n-  before_recovery = rec;\n-\n   return rec;\n }\n \n+/* Create edges: FIRST_BB -> REC; FIRST_BB -> SECOND_BB; REC -> SECOND_BB\n+   and emit necessary jumps.  */\n+void\n+sched_create_recovery_edges (basic_block first_bb, basic_block rec,\n+\t\t\t     basic_block second_bb)\n+{\n+  rtx label;\n+  rtx jump;\n+  edge e;\n+  int edge_flags;\n+\n+  /* This is fixing of incoming edge.  */\n+  /* ??? Which other flags should be specified?  */      \n+  if (BB_PARTITION (first_bb) != BB_PARTITION (rec))\n+    /* Partition type is the same, if it is \"unpartitioned\".  */\n+    edge_flags = EDGE_CROSSING;\n+  else\n+    edge_flags = 0;\n+      \n+  e = make_edge (first_bb, rec, edge_flags);\n+  label = block_label (second_bb);\n+  jump = emit_jump_insn_after (gen_jump (label), BB_END (rec));\n+  JUMP_LABEL (jump) = label;\n+  LABEL_NUSES (label)++;\n+\n+  if (BB_PARTITION (second_bb) != BB_PARTITION (rec))\n+    /* Partition type is the same, if it is \"unpartitioned\".  */\n+    {\n+      /* Rewritten from cfgrtl.c.  */\n+      if (flag_reorder_blocks_and_partition\n+\t  && targetm.have_named_sections)\n+\t/* We don't need the same note for the check because\n+\t   any_condjump_p (check) == true.  */\n+\t{\n+\t  REG_NOTES (jump) = gen_rtx_EXPR_LIST (REG_CROSSING_JUMP,\n+\t\t\t\t\t\tNULL_RTX,\n+\t\t\t\t\t\tREG_NOTES (jump));\n+\t}\n+      edge_flags = EDGE_CROSSING;\n+    }\n+  else\n+    edge_flags = 0;  \n+\n+  make_single_succ_edge (rec, second_bb, edge_flags);  \n+}\n+\n /* This function creates recovery code for INSN.  If MUTATE_P is nonzero,\n    INSN is a simple check, that should be converted to branchy one.  */\n static void\n@@ -3653,26 +3914,36 @@ create_check_block_twin (rtx insn, bool mutate_p)\n   sd_iterator_def sd_it;\n   dep_t dep;\n   dep_def _new_dep, *new_dep = &_new_dep;\n+  ds_t todo_spec;\n \n-  gcc_assert (ORIG_PAT (insn)\n-\t      && (!mutate_p \n-\t\t  || (IS_SPECULATION_SIMPLE_CHECK_P (insn)\n-\t\t      && !(TODO_SPEC (insn) & SPECULATIVE))));\n+  gcc_assert (ORIG_PAT (insn) != NULL_RTX);\n+\n+  if (!mutate_p)\n+    todo_spec = TODO_SPEC (insn);\n+  else\n+    {\n+      gcc_assert (IS_SPECULATION_SIMPLE_CHECK_P (insn)\n+\t\t  && (TODO_SPEC (insn) & SPECULATIVE) == 0);\n+\n+      todo_spec = CHECK_SPEC (insn);\n+    }\n+\n+  todo_spec &= SPECULATIVE;\n \n   /* Create recovery block.  */\n   if (mutate_p || targetm.sched.needs_block_p (insn))\n     {\n-      rec = create_recovery_block ();\n+      rec = sched_create_recovery_block (NULL);\n       label = BB_HEAD (rec);\n     }\n   else\n     {\n       rec = EXIT_BLOCK_PTR;\n-      label = 0;\n+      label = NULL_RTX;\n     }\n \n   /* Emit CHECK.  */\n-  check = targetm.sched.gen_check (insn, label, mutate_p);\n+  check = targetm.sched.gen_spec_check (insn, label, mutate_p);\n \n   if (rec != EXIT_BLOCK_PTR)\n     {\n@@ -3688,7 +3959,15 @@ create_check_block_twin (rtx insn, bool mutate_p)\n     check = emit_insn_before (check, insn);\n \n   /* Extend data structures.  */\n-  extend_block_data (check);\n+  haifa_init_insn (check);\n+\n+  /* CHECK is being added to current region.  Extend ready list.  */\n+  gcc_assert (sched_ready_n_insns != -1);\n+  sched_extend_ready_list (sched_ready_n_insns + 1);\n+\n+  if (current_sched_info->add_remove_insn)\n+    current_sched_info->add_remove_insn (insn, 0);\n+\n   RECOVERY_BLOCK (check) = rec;\n \n   if (sched_verbose && spec_info->dump)\n@@ -3715,7 +3994,7 @@ create_check_block_twin (rtx insn, bool mutate_p)\n \t  }\n \n       twin = emit_insn_after (ORIG_PAT (insn), BB_END (rec));\n-      extend_region_data (twin);\n+      haifa_init_insn (twin);\n \n       if (sched_verbose && spec_info->dump)\n \t/* INSN_BB (insn) isn't determined for twin insns yet.\n@@ -3741,54 +4020,17 @@ create_check_block_twin (rtx insn, bool mutate_p)\n     {\n       basic_block first_bb, second_bb;\n       rtx jump;\n-      edge e;\n-      int edge_flags;\n \n       first_bb = BLOCK_FOR_INSN (check);\n-      e = split_block (first_bb, check);\n-      /* split_block emits note if *check == BB_END.  Probably it \n-\t is better to rip that note off.  */\n-      gcc_assert (e->src == first_bb);\n-      second_bb = e->dest;\n-\n-      /* This is fixing of incoming edge.  */\n-      /* ??? Which other flags should be specified?  */      \n-      if (BB_PARTITION (first_bb) != BB_PARTITION (rec))\n-\t/* Partition type is the same, if it is \"unpartitioned\".  */\n-\tedge_flags = EDGE_CROSSING;\n-      else\n-\tedge_flags = 0;\n-      \n-      e = make_edge (first_bb, rec, edge_flags);\n+      second_bb = sched_split_block (first_bb, check);\n \n-      add_block (second_bb, first_bb);\n-      \n-      gcc_assert (NOTE_INSN_BASIC_BLOCK_P (BB_HEAD (second_bb)));\n-      label = block_label (second_bb);\n-      jump = emit_jump_insn_after (gen_jump (label), BB_END (rec));\n-      JUMP_LABEL (jump) = label;\n-      LABEL_NUSES (label)++;\n-      extend_region_data (jump);\n+      sched_create_recovery_edges (first_bb, rec, second_bb);\n \n-      if (BB_PARTITION (second_bb) != BB_PARTITION (rec))\n-\t/* Partition type is the same, if it is \"unpartitioned\".  */\n-\t{\n-\t  /* Rewritten from cfgrtl.c.  */\n-\t  if (flag_reorder_blocks_and_partition\n-\t      && targetm.have_named_sections\n-\t      /*&& !any_condjump_p (jump)*/)\n-\t    /* any_condjump_p (jump) == false.\n-\t       We don't need the same note for the check because\n-\t       any_condjump_p (check) == true.  */\n-\t    add_reg_note (jump, REG_CROSSING_JUMP, NULL_RTX);\n-\t  edge_flags = EDGE_CROSSING;\n-\t}\n-      else\n-\tedge_flags = 0;  \n-      \n-      make_single_succ_edge (rec, second_bb, edge_flags);  \n-      \n-      add_block (rec, EXIT_BLOCK_PTR);\n+      sched_init_only_bb (second_bb, first_bb);      \n+      sched_init_only_bb (rec, EXIT_BLOCK_PTR);\n+\n+      jump = BB_END (rec);\n+      haifa_init_insn (jump);\n     }\n \n   /* Move backward dependences from INSN to CHECK and \n@@ -4004,27 +4246,36 @@ fix_recovery_deps (basic_block rec)\n   add_jump_dependencies (insn, jump);\n }\n \n-/* Changes pattern of the INSN to NEW_PAT.  */\n-static void\n-change_pattern (rtx insn, rtx new_pat)\n+/* Change pattern of INSN to NEW_PAT.  */\n+void\n+sched_change_pattern (rtx insn, rtx new_pat)\n {\n   int t;\n \n   t = validate_change (insn, &PATTERN (insn), new_pat, 0);\n   gcc_assert (t);\n+  dfa_clear_single_insn_cache (insn);\n+}\n+\n+/* Change pattern of INSN to NEW_PAT.  Invalidate cached haifa\n+   instruction data.  */\n+static void\n+haifa_change_pattern (rtx insn, rtx new_pat)\n+{\n+  sched_change_pattern (insn, new_pat);\n+\n   /* Invalidate INSN_COST, so it'll be recalculated.  */\n   INSN_COST (insn) = -1;\n   /* Invalidate INSN_TICK, so it'll be recalculated.  */\n   INSN_TICK (insn) = INVALID_TICK;\n-  dfa_clear_single_insn_cache (insn);\n }\n \n /* -1 - can't speculate,\n    0 - for speculation with REQUEST mode it is OK to use\n    current instruction pattern,\n    1 - need to change pattern for *NEW_PAT to be speculative.  */\n-static int\n-speculate_insn (rtx insn, ds_t request, rtx *new_pat)\n+int\n+sched_speculate_insn (rtx insn, ds_t request, rtx *new_pat)\n {\n   gcc_assert (current_sched_info->flags & DO_SPECULATION\n               && (request & SPECULATIVE)\n@@ -4037,7 +4288,20 @@ speculate_insn (rtx insn, ds_t request, rtx *new_pat)\n       && !(request & BEGIN_SPEC))\n     return 0;\n \n-  return targetm.sched.speculate_insn (insn, request & BEGIN_SPEC, new_pat);\n+  return targetm.sched.speculate_insn (insn, request, new_pat);\n+}\n+\n+static int\n+haifa_speculate_insn (rtx insn, ds_t request, rtx *new_pat)\n+{\n+  gcc_assert (sched_deps_info->generate_spec_deps\n+\t      && !IS_SPECULATION_CHECK_P (insn));\n+\n+  if (HAS_INTERNAL_DEP (insn)\n+      || SCHED_GROUP_P (insn))\n+    return -1;\n+\n+  return sched_speculate_insn (insn, request, new_pat);\n }\n \n /* Print some information about block BB, which starts with HEAD and\n@@ -4151,47 +4415,6 @@ restore_bb_notes (basic_block first)\n   bb_header = 0;\n }\n \n-/* Extend per basic block data structures of the scheduler.\n-   If BB is NULL, initialize structures for the whole CFG.\n-   Otherwise, initialize them for the just created BB.  */\n-static void\n-extend_bb (void)\n-{\n-  rtx insn;\n-\n-  old_last_basic_block = last_basic_block;\n-\n-  /* The following is done to keep current_sched_info->next_tail non null.  */\n-\n-  insn = BB_END (EXIT_BLOCK_PTR->prev_bb);\n-  if (NEXT_INSN (insn) == 0\n-      || (!NOTE_P (insn)\n-\t  && !LABEL_P (insn)\n-\t  /* Don't emit a NOTE if it would end up before a BARRIER.  */\n-\t  && !BARRIER_P (NEXT_INSN (insn))))\n-    {\n-      rtx note = emit_note_after (NOTE_INSN_DELETED, insn);\n-      /* Make insn appear outside BB.  */\n-      set_block_for_insn (note, NULL);\n-      BB_END (EXIT_BLOCK_PTR->prev_bb) = insn;\n-    }\n-}\n-\n-/* Add a basic block BB to extended basic block EBB.\n-   If EBB is EXIT_BLOCK_PTR, then BB is recovery block.\n-   If EBB is NULL, then BB should be a new region.  */\n-void\n-add_block (basic_block bb, basic_block ebb)\n-{\n-  gcc_assert (current_sched_info->flags & NEW_BBS);\n-\n-  extend_bb ();\n-\n-  if (current_sched_info->add_block)\n-    /* This changes only data structures of the front-end.  */\n-    current_sched_info->add_block (bb, ebb);\n-}\n-\n /* Helper function.\n    Fix CFG after both in- and inter-block movement of\n    control_flow_insn_p JUMP.  */\n@@ -4204,7 +4427,7 @@ fix_jump_move (rtx jump)\n   jump_bb = BLOCK_FOR_INSN (jump);\n   jump_bb_next = jump_bb->next_bb;\n \n-  gcc_assert (current_sched_info->flags & SCHED_EBB\n+  gcc_assert (common_sched_info->sched_pass_id == SCHED_EBB_PASS\n \t      || IS_SPECULATION_BRANCHY_CHECK_P (jump));\n   \n   if (!NOTE_INSN_BASIC_BLOCK_P (BB_END (jump_bb_next)))\n@@ -4252,9 +4475,8 @@ move_block_after_check (rtx jump)\n \n   df_mark_solutions_dirty ();\n   \n-  if (current_sched_info->fix_recovery_cfg)\n-    current_sched_info->fix_recovery_cfg \n-      (bb->index, jump_bb->index, jump_bb_next->index);\n+  common_sched_info->fix_recovery_cfg\n+    (bb->index, jump_bb->index, jump_bb_next->index);\n }\n \n /* Helper function for move_block_after_check.\n@@ -4480,6 +4702,281 @@ check_cfg (rtx head, rtx tail)\n \n   gcc_assert (bb == 0);\n }\n+\n #endif /* ENABLE_CHECKING */\n \n+const struct sched_scan_info_def *sched_scan_info;\n+\n+/* Extend per basic block data structures.  */\n+static void\n+extend_bb (void)\n+{\n+  if (sched_scan_info->extend_bb)\n+    sched_scan_info->extend_bb ();\n+}\n+\n+/* Init data for BB.  */\n+static void\n+init_bb (basic_block bb)\n+{\n+  if (sched_scan_info->init_bb)\n+    sched_scan_info->init_bb (bb);\n+}\n+\n+/* Extend per insn data structures.  */\n+static void\n+extend_insn (void)\n+{\n+  if (sched_scan_info->extend_insn)\n+    sched_scan_info->extend_insn ();\n+}\n+\n+/* Init data structures for INSN.  */\n+static void\n+init_insn (rtx insn)\n+{\n+  if (sched_scan_info->init_insn)\n+    sched_scan_info->init_insn (insn);\n+}\n+\n+/* Init all insns in BB.  */\n+static void\n+init_insns_in_bb (basic_block bb)\n+{\n+  rtx insn;\n+\n+  FOR_BB_INSNS (bb, insn)\n+    init_insn (insn);\n+}\n+\n+/* A driver function to add a set of basic blocks (BBS),\n+   a single basic block (BB), a set of insns (INSNS) or a single insn (INSN)\n+   to the scheduling region.  */\n+void\n+sched_scan (const struct sched_scan_info_def *ssi,\n+\t    bb_vec_t bbs, basic_block bb, insn_vec_t insns, rtx insn)\n+{\n+  sched_scan_info = ssi;\n+\n+  if (bbs != NULL || bb != NULL)\n+    {\n+      extend_bb ();\n+\n+      if (bbs != NULL)\n+\t{\n+\t  unsigned i;\n+\t  basic_block x;\n+\n+\t  for (i = 0; VEC_iterate (basic_block, bbs, i, x); i++)\n+\t    init_bb (x);\n+\t}\n+\n+      if (bb != NULL)\n+\tinit_bb (bb);\n+    }\n+\n+  extend_insn ();\n+\n+  if (bbs != NULL)\n+    {      \n+      unsigned i;\n+      basic_block x;\n+\n+      for (i = 0; VEC_iterate (basic_block, bbs, i, x); i++)\n+\tinit_insns_in_bb (x);\n+    }\n+\n+  if (bb != NULL)\n+    init_insns_in_bb (bb);\n+\n+  if (insns != NULL)\n+    {\n+      unsigned i;\n+      rtx x;\n+\n+      for (i = 0; VEC_iterate (rtx, insns, i, x); i++)\n+\tinit_insn (x);\n+    }\n+\n+  if (insn != NULL)\n+    init_insn (insn);\n+}\n+\n+\n+/* Extend data structures for logical insn UID.  */\n+static void\n+luids_extend_insn (void)\n+{\n+  int new_luids_max_uid = get_max_uid () + 1;\n+\n+  VEC_safe_grow_cleared (int, heap, sched_luids, new_luids_max_uid);\n+}\n+\n+/* Initialize LUID for INSN.  */\n+static void\n+luids_init_insn (rtx insn)\n+{\n+  int i = INSN_P (insn) ? 1 : common_sched_info->luid_for_non_insn (insn);\n+  int luid;\n+\n+  if (i >= 0)\n+    {\n+      luid = sched_max_luid;\n+      sched_max_luid += i;\n+    }\n+  else\n+    luid = -1;\n+\n+  SET_INSN_LUID (insn, luid);\n+}\n+\n+/* Initialize luids for BBS, BB, INSNS and INSN.\n+   The hook common_sched_info->luid_for_non_insn () is used to determine\n+   if notes, labels, etc. need luids.  */\n+void\n+sched_init_luids (bb_vec_t bbs, basic_block bb, insn_vec_t insns, rtx insn)\n+{\n+  const struct sched_scan_info_def ssi =\n+    {\n+      NULL, /* extend_bb */\n+      NULL, /* init_bb */\n+      luids_extend_insn, /* extend_insn */\n+      luids_init_insn /* init_insn */\n+    };\n+\n+  sched_scan (&ssi, bbs, bb, insns, insn);\n+}\n+\n+/* Free LUIDs.  */\n+void\n+sched_finish_luids (void)\n+{\n+  VEC_free (int, heap, sched_luids);\n+  sched_max_luid = 1;\n+}\n+\n+/* Return logical uid of INSN.  Helpful while debugging.  */\n+int\n+insn_luid (rtx insn)\n+{\n+  return INSN_LUID (insn);\n+}\n+\n+/* Extend per insn data in the target.  */\n+void\n+sched_extend_target (void)\n+{\n+  if (targetm.sched.h_i_d_extended)\n+    targetm.sched.h_i_d_extended ();\n+}\n+\n+/* Extend global scheduler structures (those, that live across calls to\n+   schedule_block) to include information about just emitted INSN.  */\n+static void\n+extend_h_i_d (void)\n+{\n+  int reserve = (get_max_uid () + 1 \n+                 - VEC_length (haifa_insn_data_def, h_i_d));\n+  if (reserve > 0 \n+      && ! VEC_space (haifa_insn_data_def, h_i_d, reserve))\n+    {\n+      VEC_safe_grow_cleared (haifa_insn_data_def, heap, h_i_d, \n+                             3 * get_max_uid () / 2);\n+      sched_extend_target ();\n+    }\n+}\n+\n+/* Initialize h_i_d entry of the INSN with default values.\n+   Values, that are not explicitly initialized here, hold zero.  */\n+static void\n+init_h_i_d (rtx insn)\n+{\n+  if (INSN_LUID (insn) > 0)\n+    {\n+      INSN_COST (insn) = -1;\n+      find_insn_reg_weight (insn);\n+      QUEUE_INDEX (insn) = QUEUE_NOWHERE;\n+      INSN_TICK (insn) = INVALID_TICK;\n+      INTER_TICK (insn) = INVALID_TICK;\n+      TODO_SPEC (insn) = HARD_DEP;\n+    }\n+}\n+\n+/* Initialize haifa_insn_data for BBS, BB, INSNS and INSN.  */\n+void\n+haifa_init_h_i_d (bb_vec_t bbs, basic_block bb, insn_vec_t insns, rtx insn)\n+{\n+  const struct sched_scan_info_def ssi =\n+    {\n+      NULL, /* extend_bb */\n+      NULL, /* init_bb */\n+      extend_h_i_d, /* extend_insn */\n+      init_h_i_d /* init_insn */\n+    };\n+\n+  sched_scan (&ssi, bbs, bb, insns, insn);\n+}\n+\n+/* Finalize haifa_insn_data.  */\n+void\n+haifa_finish_h_i_d (void)\n+{\n+  VEC_free (haifa_insn_data_def, heap, h_i_d);\n+}\n+\n+/* Init data for the new insn INSN.  */\n+static void\n+haifa_init_insn (rtx insn)\n+{\n+  gcc_assert (insn != NULL);\n+\n+  sched_init_luids (NULL, NULL, NULL, insn);\n+  sched_extend_target ();\n+  sched_deps_init (false);\n+  haifa_init_h_i_d (NULL, NULL, NULL, insn);\n+\n+  if (adding_bb_to_current_region_p)\n+    {\n+      sd_init_insn (insn);\n+\n+      /* Extend dependency caches by one element.  */\n+      extend_dependency_caches (1, false);\n+    }\n+}\n+\n+/* Init data for the new basic block BB which comes after AFTER.  */\n+static void\n+haifa_init_only_bb (basic_block bb, basic_block after)\n+{\n+  gcc_assert (bb != NULL);\n+\n+  sched_init_bbs ();\n+\n+  if (common_sched_info->add_block)\n+    /* This changes only data structures of the front-end.  */\n+    common_sched_info->add_block (bb, after);\n+}\n+\n+/* A generic version of sched_split_block ().  */\n+basic_block\n+sched_split_block_1 (basic_block first_bb, rtx after)\n+{\n+  edge e;\n+\n+  e = split_block (first_bb, after);\n+  gcc_assert (e->src == first_bb);\n+\n+  /* sched_split_block emits note if *check == BB_END.  Probably it \n+     is better to rip that note off.  */\n+\n+  return e->dest;\n+}\n+\n+/* A generic version of sched_create_empty_bb ().  */\n+basic_block\n+sched_create_empty_bb_1 (basic_block after)\n+{\n+  return create_empty_bb (after);\n+}\n+\n #endif /* INSN_SCHEDULING */"}, {"sha": "7e17caba9df939f5eb5d9fbebf58997dcbe094bd", "filename": "gcc/hard-reg-set.h", "status": "modified", "additions": 96, "deletions": 2, "changes": 98, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fhard-reg-set.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fhard-reg-set.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fhard-reg-set.h?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -89,6 +89,8 @@ typedef HARD_REG_ELT_TYPE HARD_REG_SET[HARD_REG_SET_LONGS];\n    hard_reg_set_intersect_p (X, Y), which returns true if X and Y intersect.\n    hard_reg_set_empty_p (X), which returns true if X is empty.  */\n \n+#define UHOST_BITS_PER_WIDE_INT ((unsigned) HOST_BITS_PER_WIDEST_FAST_INT)\n+\n #ifdef HARD_REG_SET\n \n #define SET_HARD_REG_BIT(SET, BIT)  \\\n@@ -135,8 +137,6 @@ hard_reg_set_empty_p (const HARD_REG_SET x)\n \n #else\n \n-#define UHOST_BITS_PER_WIDE_INT ((unsigned) HOST_BITS_PER_WIDEST_FAST_INT)\n-\n #define SET_HARD_REG_BIT(SET, BIT)\t\t\\\n   ((SET)[(BIT) / UHOST_BITS_PER_WIDE_INT]\t\\\n    |= HARD_CONST (1) << ((BIT) % UHOST_BITS_PER_WIDE_INT))\n@@ -479,6 +479,100 @@ hard_reg_set_empty_p (const HARD_REG_SET x)\n #endif\n #endif\n \n+/* Iterator for hard register sets.  */\n+\n+typedef struct\n+{\n+  /* Pointer to the current element.  */\n+  HARD_REG_ELT_TYPE *pelt;\n+\n+  /* The length of the set.  */\n+  unsigned short length;\n+\n+  /* Word within the current element.  */\n+  unsigned short word_no;\n+\n+  /* Contents of the actually processed word.  When finding next bit\n+     it is shifted right, so that the actual bit is always the least\n+     significant bit of ACTUAL.  */\n+  HARD_REG_ELT_TYPE bits;\n+} hard_reg_set_iterator;\n+\n+#define HARD_REG_ELT_BITS UHOST_BITS_PER_WIDE_INT\n+\n+/* The implementation of the iterator functions is fully analogous to \n+   the bitmap iterators.  */\n+static inline void\n+hard_reg_set_iter_init (hard_reg_set_iterator *iter, HARD_REG_SET set, \n+                        unsigned min, unsigned *regno)\n+{\n+#ifdef HARD_REG_SET_LONGS\n+  iter->pelt = set;\n+  iter->length = HARD_REG_SET_LONGS;\n+#else\n+  iter->pelt = &set;\n+  iter->length = 1;\n+#endif\n+  iter->word_no = min / HARD_REG_ELT_BITS;\n+  if (iter->word_no < iter->length)\n+    {\n+      iter->bits = iter->pelt[iter->word_no];\n+      iter->bits >>= min % HARD_REG_ELT_BITS;\n+\n+      /* This is required for correct search of the next bit.  */\n+      min += !iter->bits;\n+    }\n+  *regno = min;\n+}\n+\n+static inline bool \n+hard_reg_set_iter_set (hard_reg_set_iterator *iter, unsigned *regno)\n+{\n+  while (1)\n+    {\n+      /* Return false when we're advanced past the end of the set.  */\n+      if (iter->word_no >= iter->length)\n+        return false;\n+\n+      if (iter->bits)\n+        {\n+          /* Find the correct bit and return it.  */\n+          while (!(iter->bits & 1))\n+            {\n+              iter->bits >>= 1;\n+              *regno += 1;\n+            }\n+          return (*regno < FIRST_PSEUDO_REGISTER);\n+        }\n+  \n+      /* Round to the beginning of the next word.  */\n+      *regno = (*regno + HARD_REG_ELT_BITS - 1);\n+      *regno -= *regno % HARD_REG_ELT_BITS;\n+\n+      /* Find the next non-zero word.  */\n+      while (++iter->word_no < iter->length)\n+        {\n+          iter->bits = iter->pelt[iter->word_no];\n+          if (iter->bits)\n+            break;\n+          *regno += HARD_REG_ELT_BITS;\n+        }\n+    }\n+}\n+\n+static inline void\n+hard_reg_set_iter_next (hard_reg_set_iterator *iter, unsigned *regno)\n+{\n+  iter->bits >>= 1;\n+  *regno += 1;\n+}\n+\n+#define EXECUTE_IF_SET_IN_HARD_REG_SET(SET, MIN, REGNUM, ITER)          \\\n+  for (hard_reg_set_iter_init (&(ITER), (SET), (MIN), &(REGNUM));       \\\n+       hard_reg_set_iter_set (&(ITER), &(REGNUM));                      \\\n+       hard_reg_set_iter_next (&(ITER), &(REGNUM)))\n+\n+\n /* Define some standard sets of registers.  */\n \n /* Indexed by hard register number, contains 1 for registers"}, {"sha": "77aeac3a09d81ebb07332408306a9ab710e3e590", "filename": "gcc/lists.c", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Flists.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Flists.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Flists.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -188,4 +188,30 @@ remove_free_INSN_LIST_elem (rtx elem, rtx *listp)\n   free_INSN_LIST_node (remove_list_elem (elem, listp));\n }\n \n+/* Remove and free the first node in the INSN_LIST pointed to by LISTP.  */\n+rtx\n+remove_free_INSN_LIST_node (rtx *listp)\n+{\n+  rtx node = *listp;\n+  rtx elem = XEXP (node, 0);\n+\n+  remove_list_node (listp);\n+  free_INSN_LIST_node (node);\n+\n+  return elem;\n+}\n+\n+/* Remove and free the first node in the EXPR_LIST pointed to by LISTP.  */\n+rtx\n+remove_free_EXPR_LIST_node (rtx *listp)\n+{\n+  rtx node = *listp;\n+  rtx elem = XEXP (node, 0);\n+\n+  remove_list_node (listp);\n+  free_EXPR_LIST_node (node);\n+\n+  return elem;\n+}\n+\n #include \"gt-lists.h\""}, {"sha": "cb93eca6b610f4c4a3357e46ffb95464b1917e92", "filename": "gcc/loop-init.c", "status": "modified", "additions": 12, "deletions": 2, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Floop-init.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Floop-init.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Floop-init.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -66,7 +66,14 @@ loop_optimizer_init (unsigned flags)\n \n   /* Create pre-headers.  */\n   if (flags & LOOPS_HAVE_PREHEADERS)\n-    create_preheaders (CP_SIMPLE_PREHEADERS);\n+    {\n+      int cp_flags = CP_SIMPLE_PREHEADERS;\n+\n+      if (flags & LOOPS_HAVE_FALLTHRU_PREHEADERS)\n+        cp_flags |= CP_FALLTHRU_PREHEADERS;\n+      \n+      create_preheaders (cp_flags);\n+    }\n \n   /* Force all latches to have only single successor.  */\n   if (flags & LOOPS_HAVE_SIMPLE_LATCHES)\n@@ -118,7 +125,10 @@ loop_optimizer_finalize (void)\n \n   /* Checking.  */\n #ifdef ENABLE_CHECKING\n-  verify_flow_info ();\n+  /* FIXME: no point to verify flow info after bundling on ia64.  Use this \n+     hack for achieving this.  */\n+  if (!reload_completed)\n+    verify_flow_info ();\n #endif\n }\n "}, {"sha": "7134bfc0d00c5fe658769b0cdacfc9a4b6df7b4c", "filename": "gcc/modulo-sched.c", "status": "modified", "additions": 30, "deletions": 25, "changes": 55, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fmodulo-sched.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fmodulo-sched.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fmodulo-sched.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -187,13 +187,6 @@ static int compute_split_row (sbitmap, int, int, int, ddg_node_ptr);\n /* This page defines constants and structures for the modulo scheduling\n    driver.  */\n \n-/* As in haifa-sched.c:  */\n-/* issue_rate is the number of insns that can be scheduled in the same\n-   machine cycle.  It can be defined in the config/mach/mach.h file,\n-   otherwise we set it to 1.  */\n-\n-static int issue_rate;\n-\n static int sms_order_nodes (ddg_ptr, int, int *, int *);\n static void set_node_sched_params (ddg_ptr);\n static partial_schedule_ptr sms_schedule_by_order (ddg_ptr, int, int, int *);\n@@ -242,7 +235,7 @@ typedef struct node_sched_params\n    code in order to use sched_analyze() for computing the dependencies.\n    They are used when initializing the sched_info structure.  */\n static const char *\n-sms_print_insn (rtx insn, int aligned ATTRIBUTE_UNUSED)\n+sms_print_insn (const_rtx insn, int aligned ATTRIBUTE_UNUSED)\n {\n   static char tmp[80];\n \n@@ -258,7 +251,17 @@ compute_jump_reg_dependencies (rtx insn ATTRIBUTE_UNUSED,\n {\n }\n \n-static struct sched_info sms_sched_info =\n+static struct common_sched_info_def sms_common_sched_info;\n+\n+static struct sched_deps_info_def sms_sched_deps_info =\n+  {\n+    compute_jump_reg_dependencies,\n+    NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,\n+    NULL,\n+    0, 0, 0\n+  };\n+\n+static struct haifa_sched_info sms_sched_info =\n {\n   NULL,\n   NULL,\n@@ -267,16 +270,14 @@ static struct sched_info sms_sched_info =\n   NULL,\n   sms_print_insn,\n   NULL,\n-  compute_jump_reg_dependencies,\n   NULL, NULL,\n   NULL, NULL,\n-  0, 0, 0,\n+  0, 0,\n \n-  NULL, NULL, NULL, NULL, NULL,\n+  NULL, NULL, NULL, \n   0\n };\n \n-\n /* Given HEAD and TAIL which are the first and last insns in a loop;\n    return the register which controls the loop.  Return zero if it has\n    more than one occurrence in the loop besides the control part or the\n@@ -856,6 +857,19 @@ canon_loop (struct loop *loop)\n     }\n }\n \n+/* Setup infos.  */\n+static void\n+setup_sched_infos (void)\n+{\n+  memcpy (&sms_common_sched_info, &haifa_common_sched_info,\n+\t  sizeof (sms_common_sched_info));\n+  sms_common_sched_info.sched_pass_id = SCHED_SMS_PASS;\n+  common_sched_info = &sms_common_sched_info;\n+\n+  sched_deps_info = &sms_sched_deps_info;\n+  current_sched_info = &sms_sched_info;\n+}\n+\n /* Probability in % that the sms-ed loop rolls enough so that optimized\n    version may be entered.  Just a guess.  */\n #define PROB_SMS_ENOUGH_ITERATIONS 80\n@@ -901,16 +915,8 @@ sms_schedule (void)\n     issue_rate = 1;\n \n   /* Initialize the scheduler.  */\n-  current_sched_info = &sms_sched_info;\n-\n-  /* Init Data Flow analysis, to be used in interloop dep calculation.  */\n-  df_set_flags (DF_LR_RUN_DCE);\n-  df_rd_add_problem ();\n-  df_note_add_problem ();\n-  df_chain_add_problem (DF_DU_CHAIN + DF_UD_CHAIN);\n-  df_analyze ();\n-  regstat_compute_calls_crossed ();\n-  sched_init ();\n+  setup_sched_infos ();\n+  haifa_sched_init ();\n \n   /* Allocate memory to hold the DDG array one entry for each loop.\n      We use loop->num as index into this array.  */\n@@ -1242,11 +1248,10 @@ sms_schedule (void)\n       free_ddg (g);\n     }\n \n-  regstat_free_calls_crossed ();\n   free (g_arr);\n \n   /* Release scheduler data, needed until now because of DFA.  */\n-  sched_finish ();\n+  haifa_sched_finish ();\n   loop_optimizer_finalize ();\n }\n "}, {"sha": "0c0f0a3152bff7861a8b31ceb7152383441f1076", "filename": "gcc/opts.c", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fopts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fopts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fopts.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -47,6 +47,9 @@ along with GCC; see the file COPYING3.  If not see\n unsigned HOST_WIDE_INT g_switch_value;\n bool g_switch_set;\n \n+/* Same for selective scheduling.  */\n+bool sel_sched_switch_set;\n+\n /* True if we should exit after parsing options.  */\n bool exit_after_options;\n \n@@ -1087,6 +1090,11 @@ decode_options (unsigned int argc, const char **argv)\n       flag_reorder_blocks = 1;\n     }\n \n+  /* Pipelining of outer loops is only possible when general pipelining\n+     capabilities are requested.  */\n+  if (!flag_sel_sched_pipelining)\n+    flag_sel_sched_pipelining_outer_loops = 0;\n+\n #ifndef IRA_COVER_CLASSES\n   if (flag_ira)\n     {\n@@ -1870,6 +1878,11 @@ common_handle_option (size_t scode, const char *arg, int value,\n       set_random_seed (arg);\n       break;\n \n+    case OPT_fselective_scheduling:\n+    case OPT_fselective_scheduling2:\n+      sel_sched_switch_set = true;\n+      break;\n+\n     case OPT_fsched_verbose_:\n #ifdef INSN_SCHEDULING\n       fix_sched_param (\"verbose\", arg);"}, {"sha": "8d30a24a6026295cb9a840d9c8653f5b402a9060", "filename": "gcc/params.def", "status": "modified", "additions": 30, "deletions": 0, "changes": 30, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fparams.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fparams.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fparams.def?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -557,6 +557,16 @@ DEFPARAM(PARAM_MAX_SCHED_REGION_INSNS,\n \t \"The maximum number of insns in a region to be considered for interblock scheduling\",\n \t 100, 0, 0)\n \n+DEFPARAM(PARAM_MAX_PIPELINE_REGION_BLOCKS,\n+\t \"max-pipeline-region-blocks\",\n+\t \"The maximum number of blocks in a region to be considered for interblock scheduling\",\n+\t 15, 0, 0)\n+\n+DEFPARAM(PARAM_MAX_PIPELINE_REGION_INSNS,\n+\t \"max-pipeline-region-insns\",\n+\t \"The maximum number of insns in a region to be considered for interblock scheduling\",\n+\t 200, 0, 0)\n+\n DEFPARAM(PARAM_MIN_SPEC_PROB,\n          \"min-spec-prob\",\n          \"The minimum probability of reaching a source block for interblock speculative scheduling\",\n@@ -577,6 +587,26 @@ DEFPARAM(PARAM_SCHED_SPEC_PROB_CUTOFF,\n          \"The minimal probability of speculation success (in percents), so that speculative insn will be scheduled.\",\n          40, 0, 100)\n \n+DEFPARAM(PARAM_SELSCHED_MAX_LOOKAHEAD,\n+         \"selsched-max-lookahead\",\n+         \"The maximum size of the lookahead window of selective scheduling\",\n+         50, 0, 0)\n+\n+DEFPARAM(PARAM_SELSCHED_MAX_SCHED_TIMES,\n+         \"selsched-max-sched-times\",\n+         \"Maximum number of times that an insn could be scheduled\",\n+         2, 0, 0)\n+\n+DEFPARAM(PARAM_SELSCHED_INSNS_TO_RENAME,\n+         \"selsched-insns-to-rename\",\n+         \"Maximum number of instructions in the ready list that are considered eligible for renaming\",\n+         2, 0, 0)\n+\n+DEFPARAM (PARAM_SCHED_MEM_TRUE_DEP_COST,\n+\t  \"sched-mem-true-dep-cost\",\n+\t  \"Minimal distance between possibly conflicting store and load\",\n+\t  1, 0, 0)\n+\n DEFPARAM(PARAM_MAX_LAST_VALUE_RTL,\n \t \"max-last-value-rtl\",\n \t \"The maximum number of RTL nodes that can be recorded as combiner's last value\","}, {"sha": "7ab6a1de4e0d675c22f455c94ad88bcf4f246c5e", "filename": "gcc/recog.c", "status": "modified", "additions": 119, "deletions": 80, "changes": 199, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Frecog.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Frecog.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frecog.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -69,7 +69,7 @@ get_attr_enabled (rtx insn ATTRIBUTE_UNUSED)\n }\n #endif\n \n-static void validate_replace_rtx_1 (rtx *, rtx, rtx, rtx);\n+static void validate_replace_rtx_1 (rtx *, rtx, rtx, rtx, bool);\n static void validate_replace_src_1 (rtx *, void *);\n static rtx split_insn (rtx);\n \n@@ -513,88 +513,17 @@ cancel_changes (int num)\n   num_changes = num;\n }\n \n-/* Replace every occurrence of FROM in X with TO.  Mark each change with\n-   validate_change passing OBJECT.  */\n+/* A subroutine of validate_replace_rtx_1 that tries to simplify the resulting\n+   rtx.  */\n \n static void\n-validate_replace_rtx_1 (rtx *loc, rtx from, rtx to, rtx object)\n+simplify_while_replacing (rtx *loc, rtx to, rtx object, \n+                          enum machine_mode op0_mode)\n {\n-  int i, j;\n-  const char *fmt;\n   rtx x = *loc;\n-  enum rtx_code code;\n-  enum machine_mode op0_mode = VOIDmode;\n-  int prev_changes = num_changes;\n+  enum rtx_code code = GET_CODE (x);\n   rtx new_rtx;\n \n-  if (!x)\n-    return;\n-\n-  code = GET_CODE (x);\n-  fmt = GET_RTX_FORMAT (code);\n-  if (fmt[0] == 'e')\n-    op0_mode = GET_MODE (XEXP (x, 0));\n-\n-  /* X matches FROM if it is the same rtx or they are both referring to the\n-     same register in the same mode.  Avoid calling rtx_equal_p unless the\n-     operands look similar.  */\n-\n-  if (x == from\n-      || (REG_P (x) && REG_P (from)\n-\t  && GET_MODE (x) == GET_MODE (from)\n-\t  && REGNO (x) == REGNO (from))\n-      || (GET_CODE (x) == GET_CODE (from) && GET_MODE (x) == GET_MODE (from)\n-\t  && rtx_equal_p (x, from)))\n-    {\n-      validate_unshare_change (object, loc, to, 1);\n-      return;\n-    }\n-\n-  /* Call ourself recursively to perform the replacements.\n-     We must not replace inside already replaced expression, otherwise we\n-     get infinite recursion for replacements like (reg X)->(subreg (reg X))\n-     done by regmove, so we must special case shared ASM_OPERANDS.  */\n-\n-  if (GET_CODE (x) == PARALLEL)\n-    {\n-      for (j = XVECLEN (x, 0) - 1; j >= 0; j--)\n-\t{\n-\t  if (j && GET_CODE (XVECEXP (x, 0, j)) == SET\n-\t      && GET_CODE (SET_SRC (XVECEXP (x, 0, j))) == ASM_OPERANDS)\n-\t    {\n-\t      /* Verify that operands are really shared.  */\n-\t      gcc_assert (ASM_OPERANDS_INPUT_VEC (SET_SRC (XVECEXP (x, 0, 0)))\n-\t\t\t  == ASM_OPERANDS_INPUT_VEC (SET_SRC (XVECEXP\n-\t\t\t\t\t\t\t      (x, 0, j))));\n-\t      validate_replace_rtx_1 (&SET_DEST (XVECEXP (x, 0, j)),\n-\t\t\t\t      from, to, object);\n-\t    }\n-\t  else\n-\t    validate_replace_rtx_1 (&XVECEXP (x, 0, j), from, to, object);\n-\t}\n-    }\n-  else\n-    for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n-      {\n-\tif (fmt[i] == 'e')\n-\t  validate_replace_rtx_1 (&XEXP (x, i), from, to, object);\n-\telse if (fmt[i] == 'E')\n-\t  for (j = XVECLEN (x, i) - 1; j >= 0; j--)\n-\t    validate_replace_rtx_1 (&XVECEXP (x, i, j), from, to, object);\n-      }\n-\n-  /* If we didn't substitute, there is nothing more to do.  */\n-  if (num_changes == prev_changes)\n-    return;\n-\n-  /* Allow substituted expression to have different mode.  This is used by\n-     regmove to change mode of pseudo register.  */\n-  if (fmt[0] == 'e' && GET_MODE (XEXP (x, 0)) != VOIDmode)\n-    op0_mode = GET_MODE (XEXP (x, 0));\n-\n-  /* Do changes needed to keep rtx consistent.  Don't do any other\n-     simplifications, as it is not our job.  */\n-\n   if (SWAPPABLE_OPERANDS_P (x)\n       && swap_commutative_operands_p (XEXP (x, 0), XEXP (x, 1)))\n     {\n@@ -715,22 +644,132 @@ validate_replace_rtx_1 (rtx *loc, rtx from, rtx to, rtx object)\n     }\n }\n \n+/* Replace every occurrence of FROM in X with TO.  Mark each change with\n+   validate_change passing OBJECT.  */\n+\n+static void\n+validate_replace_rtx_1 (rtx *loc, rtx from, rtx to, rtx object, \n+                        bool simplify)\n+{\n+  int i, j;\n+  const char *fmt;\n+  rtx x = *loc;\n+  enum rtx_code code;\n+  enum machine_mode op0_mode = VOIDmode;\n+  int prev_changes = num_changes;\n+\n+  if (!x)\n+    return;\n+\n+  code = GET_CODE (x);\n+  fmt = GET_RTX_FORMAT (code);\n+  if (fmt[0] == 'e')\n+    op0_mode = GET_MODE (XEXP (x, 0));\n+\n+  /* X matches FROM if it is the same rtx or they are both referring to the\n+     same register in the same mode.  Avoid calling rtx_equal_p unless the\n+     operands look similar.  */\n+\n+  if (x == from\n+      || (REG_P (x) && REG_P (from)\n+\t  && GET_MODE (x) == GET_MODE (from)\n+\t  && REGNO (x) == REGNO (from))\n+      || (GET_CODE (x) == GET_CODE (from) && GET_MODE (x) == GET_MODE (from)\n+\t  && rtx_equal_p (x, from)))\n+    {\n+      validate_unshare_change (object, loc, to, 1);\n+      return;\n+    }\n+\n+  /* Call ourself recursively to perform the replacements.\n+     We must not replace inside already replaced expression, otherwise we\n+     get infinite recursion for replacements like (reg X)->(subreg (reg X))\n+     done by regmove, so we must special case shared ASM_OPERANDS.  */\n+\n+  if (GET_CODE (x) == PARALLEL)\n+    {\n+      for (j = XVECLEN (x, 0) - 1; j >= 0; j--)\n+\t{\n+\t  if (j && GET_CODE (XVECEXP (x, 0, j)) == SET\n+\t      && GET_CODE (SET_SRC (XVECEXP (x, 0, j))) == ASM_OPERANDS)\n+\t    {\n+\t      /* Verify that operands are really shared.  */\n+\t      gcc_assert (ASM_OPERANDS_INPUT_VEC (SET_SRC (XVECEXP (x, 0, 0)))\n+\t\t\t  == ASM_OPERANDS_INPUT_VEC (SET_SRC (XVECEXP\n+\t\t\t\t\t\t\t      (x, 0, j))));\n+\t      validate_replace_rtx_1 (&SET_DEST (XVECEXP (x, 0, j)),\n+\t\t\t\t      from, to, object, simplify);\n+\t    }\n+\t  else\n+\t    validate_replace_rtx_1 (&XVECEXP (x, 0, j), from, to, object, \n+                                    simplify);\n+\t}\n+    }\n+  else\n+    for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n+      {\n+\tif (fmt[i] == 'e')\n+\t  validate_replace_rtx_1 (&XEXP (x, i), from, to, object, simplify);\n+\telse if (fmt[i] == 'E')\n+\t  for (j = XVECLEN (x, i) - 1; j >= 0; j--)\n+\t    validate_replace_rtx_1 (&XVECEXP (x, i, j), from, to, object, \n+                                    simplify);\n+      }\n+\n+  /* If we didn't substitute, there is nothing more to do.  */\n+  if (num_changes == prev_changes)\n+    return;\n+\n+  /* Allow substituted expression to have different mode.  This is used by\n+     regmove to change mode of pseudo register.  */\n+  if (fmt[0] == 'e' && GET_MODE (XEXP (x, 0)) != VOIDmode)\n+    op0_mode = GET_MODE (XEXP (x, 0));\n+\n+  /* Do changes needed to keep rtx consistent.  Don't do any other\n+     simplifications, as it is not our job.  */\n+  if (simplify)\n+    simplify_while_replacing (loc, to, object, op0_mode);\n+}\n+\n /* Try replacing every occurrence of FROM in INSN with TO.  After all\n    changes have been made, validate by seeing if INSN is still valid.  */\n \n int\n validate_replace_rtx (rtx from, rtx to, rtx insn)\n {\n-  validate_replace_rtx_1 (&PATTERN (insn), from, to, insn);\n+  validate_replace_rtx_1 (&PATTERN (insn), from, to, insn, true);\n   return apply_change_group ();\n }\n \n+/* Try replacing every occurrence of FROM in WHERE with TO.  Assume that WHERE\n+   is a part of INSN.  After all changes have been made, validate by seeing if \n+   INSN is still valid.  \n+   validate_replace_rtx (from, to, insn) is equivalent to \n+   validate_replace_rtx_part (from, to, &PATTERN (insn), insn).  */\n+\n+int\n+validate_replace_rtx_part (rtx from, rtx to, rtx *where, rtx insn)\n+{\n+  validate_replace_rtx_1 (where, from, to, insn, true);\n+  return apply_change_group ();\n+}\n+\n+/* Same as above, but do not simplify rtx afterwards.  */\n+int \n+validate_replace_rtx_part_nosimplify (rtx from, rtx to, rtx *where, \n+                                      rtx insn)\n+{\n+  validate_replace_rtx_1 (where, from, to, insn, false);\n+  return apply_change_group ();\n+\n+}\n+\n /* Try replacing every occurrence of FROM in INSN with TO.  */\n \n void\n validate_replace_rtx_group (rtx from, rtx to, rtx insn)\n {\n-  validate_replace_rtx_1 (&PATTERN (insn), from, to, insn);\n+  validate_replace_rtx_1 (&PATTERN (insn), from, to, insn, true);\n }\n \n /* Function called by note_uses to replace used subexpressions.  */\n@@ -747,7 +786,7 @@ validate_replace_src_1 (rtx *x, void *data)\n   struct validate_replace_src_data *d\n     = (struct validate_replace_src_data *) data;\n \n-  validate_replace_rtx_1 (x, d->from, d->to, d->insn);\n+  validate_replace_rtx_1 (x, d->from, d->to, d->insn, true);\n }\n \n /* Try replacing every occurrence of FROM in INSN with TO, avoiding"}, {"sha": "9b73211e8744d02c6b792764ea924d4e60024ae9", "filename": "gcc/recog.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Frecog.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Frecog.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frecog.h?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -87,6 +87,8 @@ extern int constrain_operands_cached (int);\n extern int memory_address_p (enum machine_mode, rtx);\n extern int strict_memory_address_p (enum machine_mode, rtx);\n extern int validate_replace_rtx (rtx, rtx, rtx);\n+extern int validate_replace_rtx_part (rtx, rtx, rtx *, rtx);\n+extern int validate_replace_rtx_part_nosimplify (rtx, rtx, rtx *, rtx);\n extern void validate_replace_rtx_group (rtx, rtx, rtx);\n extern void validate_replace_src_group (rtx, rtx, rtx);\n extern bool validate_simplify_insn (rtx insn);"}, {"sha": "97debd3c1d5cda99caaaec94afae321c56b10275", "filename": "gcc/rtl.c", "status": "modified", "additions": 22, "deletions": 5, "changes": 27, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Frtl.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Frtl.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtl.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -333,22 +333,29 @@ int generating_concat_p;\n int currently_expanding_to_rtl;\n \n \f\n-/* Return 1 if X and Y are identical-looking rtx's.\n-   This is the Lisp function EQUAL for rtx arguments.  */\n+\n+/* Same as rtx_equal_p, but call CB on each pair of rtx if CB is not NULL.  \n+   When the callback returns true, we continue with the new pair.  */\n \n int\n-rtx_equal_p (const_rtx x, const_rtx y)\n+rtx_equal_p_cb (const_rtx x, const_rtx y, rtx_equal_p_callback_function cb)\n {\n   int i;\n   int j;\n   enum rtx_code code;\n   const char *fmt;\n+  rtx nx, ny;\n \n   if (x == y)\n     return 1;\n   if (x == 0 || y == 0)\n     return 0;\n \n+  /* Invoke the callback first.  */\n+  if (cb != NULL\n+      && ((*cb) (&x, &y, &nx, &ny)))\n+    return rtx_equal_p_cb (nx, ny, cb);\n+\n   code = GET_CODE (x);\n   /* Rtx's of different codes cannot be equal.  */\n   if (code != GET_CODE (y))\n@@ -409,12 +416,13 @@ rtx_equal_p (const_rtx x, const_rtx y)\n \n \t  /* And the corresponding elements must match.  */\n \t  for (j = 0; j < XVECLEN (x, i); j++)\n-\t    if (rtx_equal_p (XVECEXP (x, i, j), XVECEXP (y, i, j)) == 0)\n+\t    if (rtx_equal_p_cb (XVECEXP (x, i, j), \n+                                XVECEXP (y, i, j), cb) == 0)\n \t      return 0;\n \t  break;\n \n \tcase 'e':\n-\t  if (rtx_equal_p (XEXP (x, i), XEXP (y, i)) == 0)\n+\t  if (rtx_equal_p_cb (XEXP (x, i), XEXP (y, i), cb) == 0)\n \t    return 0;\n \t  break;\n \n@@ -444,6 +452,15 @@ rtx_equal_p (const_rtx x, const_rtx y)\n   return 1;\n }\n \n+/* Return 1 if X and Y are identical-looking rtx's.\n+   This is the Lisp function EQUAL for rtx arguments.  */\n+\n+int\n+rtx_equal_p (const_rtx x, const_rtx y)\n+{\n+  return rtx_equal_p_cb (x, y, NULL);\n+}\n+\n void\n dump_rtx_statistics (void)\n {"}, {"sha": "4b04d286927c9635728bf359042df4a4ecafd3d9", "filename": "gcc/rtl.h", "status": "modified", "additions": 32, "deletions": 9, "changes": 41, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Frtl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Frtl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtl.h?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -1774,8 +1774,20 @@ extern int replace_label (rtx *, void *);\n extern int rtx_referenced_p (rtx, rtx);\n extern bool tablejump_p (const_rtx, rtx *, rtx *);\n extern int computed_jump_p (const_rtx);\n+\n typedef int (*rtx_function) (rtx *, void *);\n extern int for_each_rtx (rtx *, rtx_function, void *);\n+\n+typedef int (*rtx_equal_p_callback_function) (const_rtx *, const_rtx *,\n+                                              rtx *, rtx *);\n+extern int rtx_equal_p_cb (const_rtx, const_rtx,\n+                           rtx_equal_p_callback_function);\n+\n+typedef int (*hash_rtx_callback_function) (const_rtx, enum machine_mode, rtx *,\n+                                           enum machine_mode *);\n+extern unsigned hash_rtx_cb (const_rtx, enum machine_mode, int *, int *,\n+                             bool, hash_rtx_callback_function);\n+\n extern rtx regno_use_in (unsigned int, rtx);\n extern int auto_inc_p (const_rtx);\n extern int in_expr_list_p (const_rtx, const_rtx);\n@@ -1796,14 +1808,17 @@ extern rtx get_condition (rtx, rtx *, int, int);\n \n /* lists.c */\n \n-void free_EXPR_LIST_list\t\t(rtx *);\n-void free_INSN_LIST_list\t\t(rtx *);\n-void free_EXPR_LIST_node\t\t(rtx);\n-void free_INSN_LIST_node\t\t(rtx);\n-rtx alloc_INSN_LIST\t\t\t(rtx, rtx);\n-rtx alloc_EXPR_LIST\t\t\t(int, rtx, rtx);\n-void remove_free_INSN_LIST_elem (rtx, rtx *);\n-rtx remove_list_elem (rtx, rtx *);\n+extern void free_EXPR_LIST_list\t\t(rtx *);\n+extern void free_INSN_LIST_list\t\t(rtx *);\n+extern void free_EXPR_LIST_node\t\t(rtx);\n+extern void free_INSN_LIST_node\t\t(rtx);\n+extern rtx alloc_INSN_LIST\t\t\t(rtx, rtx);\n+extern rtx alloc_EXPR_LIST\t\t\t(int, rtx, rtx);\n+extern void remove_free_INSN_LIST_elem (rtx, rtx *);\n+extern rtx remove_list_elem (rtx, rtx *);\n+extern rtx remove_free_INSN_LIST_node (rtx *);\n+extern rtx remove_free_EXPR_LIST_node (rtx *);\n+\n \n /* regclass.c */\n \n@@ -2133,7 +2148,9 @@ extern void dump_combine_total_stats (FILE *);\n extern void delete_dead_jumptables (void);\n \n /* In sched-vis.c.  */\n-extern void print_insn (char *, rtx, int);\n+extern void debug_bb_n_slim (int);\n+extern void debug_bb_slim (struct basic_block_def *);\n+extern void print_rtl_slim (FILE *, rtx, rtx, int, int);\n extern void print_rtl_slim_with_bb (FILE *, rtx, int);\n extern void dump_insn_slim (FILE *f, rtx x);\n extern void debug_insn_slim (rtx x);\n@@ -2147,6 +2164,9 @@ extern void schedule_ebbs (void);\n /* In haifa-sched.c.  */\n extern void fix_sched_param (const char *, const char *);\n \n+/* In sel-sched-dump.c.  */\n+extern void sel_sched_fix_param (const char *param, const char *val);\n+\n /* In print-rtl.c */\n extern const char *print_rtx_head;\n extern void debug_rtx (const_rtx);\n@@ -2311,6 +2331,9 @@ extern rtx compare_and_jump_seq (rtx, rtx, enum rtx_code, rtx, int, rtx);\n /* In loop-iv.c  */\n extern rtx canon_condition (rtx);\n extern void simplify_using_condition (rtx, rtx *, struct bitmap_head_def *);\n+\n+/* In final.c  */\n+extern unsigned int compute_alignments (void);\n \f\n struct rtl_hooks\n {"}, {"sha": "623d6abe6eb538ed5acff434563999e898f26e77", "filename": "gcc/rtlhooks-def.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Frtlhooks-def.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Frtlhooks-def.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtlhooks-def.h?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -34,7 +34,7 @@ along with GCC; see the file COPYING3.  If not see\n   RTL_HOOKS_GEN_LOWPART_NO_EMIT,\t\t\\\n   RTL_HOOKS_REG_NONZERO_REG_BITS,\t\t\\\n   RTL_HOOKS_REG_NUM_SIGN_BIT_COPIES,\t\t\\\n-  RTL_HOOKS_REG_TRUNCATED_TO_MODE,\t\t\\\n+  RTL_HOOKS_REG_TRUNCATED_TO_MODE\t\t\\\n }\n \n extern rtx gen_lowpart_general (enum machine_mode, rtx);"}, {"sha": "b7aa6b4d9ebb0dcebf5efc8a019b9338b39ba9bb", "filename": "gcc/sched-deps.c", "status": "modified", "additions": 1017, "deletions": 349, "changes": 1366, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsched-deps.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsched-deps.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-deps.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -50,6 +50,12 @@ along with GCC; see the file COPYING3.  If not see\n #define CHECK (false)\n #endif\n \n+/* Holds current parameters for the dependency analyzer.  */\n+struct sched_deps_info_def *sched_deps_info;\n+\n+/* The data is specific to the Haifa scheduler.  */\n+VEC(haifa_deps_insn_data_def, heap) *h_d_i_d = NULL;\n+\n /* Return the major type present in the DS.  */\n enum reg_note\n ds_to_dk (ds_t ds)\n@@ -388,17 +394,6 @@ clear_deps_list (deps_list_t l)\n static regset reg_pending_sets;\n static regset reg_pending_clobbers;\n static regset reg_pending_uses;\n-\n-/* The following enumeration values tell us what dependencies we\n-   should use to implement the barrier.  We use true-dependencies for\n-   TRUE_BARRIER and anti-dependencies for MOVE_BARRIER.  */\n-enum reg_pending_barrier_mode\n-{\n-  NOT_A_BARRIER = 0,\n-  MOVE_BARRIER,\n-  TRUE_BARRIER\n-};\n-\n static enum reg_pending_barrier_mode reg_pending_barrier;\n \n /* To speed up the test for duplicate dependency links we keep a\n@@ -414,15 +409,16 @@ static enum reg_pending_barrier_mode reg_pending_barrier;\n    has enough entries to represent a dependency on any other insn in\n    the insn chain.  All bitmap for true dependencies cache is\n    allocated then the rest two ones are also allocated.  */\n-static bitmap_head *true_dependency_cache;\n-static bitmap_head *output_dependency_cache;\n-static bitmap_head *anti_dependency_cache;\n-static bitmap_head *spec_dependency_cache;\n+static bitmap_head *true_dependency_cache = NULL;\n+static bitmap_head *output_dependency_cache = NULL;\n+static bitmap_head *anti_dependency_cache = NULL;\n+static bitmap_head *spec_dependency_cache = NULL;\n static int cache_size;\n \n static int deps_may_trap_p (const_rtx);\n static void add_dependence_list (rtx, rtx, int, enum reg_note);\n-static void add_dependence_list_and_free (rtx, rtx *, int, enum reg_note);\n+static void add_dependence_list_and_free (struct deps *, rtx, \n+                                          rtx *, int, enum reg_note);\n static void delete_all_dependences (rtx);\n static void fixup_sched_groups (rtx);\n \n@@ -431,14 +427,13 @@ static void sched_analyze_1 (struct deps *, rtx, rtx);\n static void sched_analyze_2 (struct deps *, rtx, rtx);\n static void sched_analyze_insn (struct deps *, rtx, rtx);\n \n-static rtx sched_get_condition (const_rtx);\n-static int conditions_mutex_p (const_rtx, const_rtx);\n+static bool sched_has_condition_p (const_rtx);\n+static int conditions_mutex_p (const_rtx, const_rtx, bool, bool);\n \n static enum DEPS_ADJUST_RESULT maybe_add_or_update_dep_1 (dep_t, bool,\n \t\t\t\t\t\t\t  rtx, rtx);\n static enum DEPS_ADJUST_RESULT add_or_update_dep_1 (dep_t, bool, rtx, rtx);\n \n-static dw_t estimate_dep_weak (rtx, rtx);\n #ifdef ENABLE_CHECKING\n static void check_dep (dep_t, bool);\n #endif\n@@ -459,17 +454,22 @@ deps_may_trap_p (const_rtx mem)\n   return rtx_addr_can_trap_p (addr);\n }\n \f\n-/* Find the condition under which INSN is executed.  */\n \n+/* Find the condition under which INSN is executed.  If REV is not NULL,\n+   it is set to TRUE when the returned comparison should be reversed\n+   to get the actual condition.  */\n static rtx\n-sched_get_condition (const_rtx insn)\n+sched_get_condition_with_rev (const_rtx insn, bool *rev)\n {\n   rtx pat = PATTERN (insn);\n   rtx src;\n \n   if (pat == 0)\n     return 0;\n \n+  if (rev)\n+    *rev = false;\n+\n   if (GET_CODE (pat) == COND_EXEC)\n     return COND_EXEC_TEST (pat);\n \n@@ -487,22 +487,34 @@ sched_get_condition (const_rtx insn)\n \n       if (revcode == UNKNOWN)\n \treturn 0;\n-      return gen_rtx_fmt_ee (revcode, GET_MODE (cond), XEXP (cond, 0),\n-\t\t\t     XEXP (cond, 1));\n+\n+      if (rev)\n+\t*rev = true;\n+      return cond;\n     }\n \n   return 0;\n }\n \n+/* True when we can find a condition under which INSN is executed.  */\n+static bool\n+sched_has_condition_p (const_rtx insn)\n+{\n+  return !! sched_get_condition_with_rev (insn, NULL);\n+}\n+\n \f\n-/* Return nonzero if conditions COND1 and COND2 can never be both true.  */\n \n+/* Return nonzero if conditions COND1 and COND2 can never be both true.  */\n static int\n-conditions_mutex_p (const_rtx cond1, const_rtx cond2)\n+conditions_mutex_p (const_rtx cond1, const_rtx cond2, bool rev1, bool rev2)\n {\n   if (COMPARISON_P (cond1)\n       && COMPARISON_P (cond2)\n-      && GET_CODE (cond1) == reversed_comparison_code (cond2, NULL)\n+      && GET_CODE (cond1) ==\n+\t  (rev1==rev2\n+\t  ? reversed_comparison_code (cond2, NULL)\n+\t  : GET_CODE (cond2))\n       && XEXP (cond1, 0) == XEXP (cond2, 0)\n       && XEXP (cond1, 1) == XEXP (cond2, 1))\n     return 1;\n@@ -515,15 +527,16 @@ bool\n sched_insns_conditions_mutex_p (const_rtx insn1, const_rtx insn2)\n {\n   rtx cond1, cond2;\n+  bool rev1, rev2;\n \n   /* df doesn't handle conditional lifetimes entirely correctly;\n      calls mess up the conditional lifetimes.  */\n   if (!CALL_P (insn1) && !CALL_P (insn2))\n     {\n-      cond1 = sched_get_condition (insn1);\n-      cond2 = sched_get_condition (insn2);\n+      cond1 = sched_get_condition_with_rev (insn1, &rev1);\n+      cond2 = sched_get_condition_with_rev (insn2, &rev2);\n       if (cond1 && cond2\n-\t  && conditions_mutex_p (cond1, cond2)\n+\t  && conditions_mutex_p (cond1, cond2, rev1, rev2)\n \t  /* Make sure first instruction doesn't affect condition of second\n \t     instruction if switched.  */\n \t  && !modified_in_p (cond1, insn2)\n@@ -549,7 +562,7 @@ sched_insn_is_legitimate_for_speculation_p (const_rtx insn, ds_t ds)\n   if (SCHED_GROUP_P (insn))\n     return false;\n \n-  if (IS_SPECULATION_CHECK_P (insn))\n+  if (IS_SPECULATION_CHECK_P (CONST_CAST_RTX (insn)))\n     return false;\n \n   if (side_effects_p (PATTERN (insn)))\n@@ -567,7 +580,7 @@ sched_insn_is_legitimate_for_speculation_p (const_rtx insn, ds_t ds)\n \treturn false;\n \n       if ((ds & BE_IN_DATA)\n-\t  && sched_get_condition (insn) != NULL_RTX)\n+\t  && sched_has_condition_p (insn))\n \t/* If this is a predicated instruction, then it cannot be\n \t   speculatively scheduled.  See PR35659.  */\n \treturn false;\n@@ -792,7 +805,7 @@ maybe_add_or_update_dep_1 (dep_t dep, bool resolved_p, rtx mem1, rtx mem2)\n   /* Don't depend an insn on itself.  */\n   if (insn == elem)\n     {\n-      if (current_sched_info->flags & DO_SPECULATION)\n+      if (sched_deps_info->generate_spec_deps)\n         /* INSN has an internal dependence, which we can't overcome.  */\n         HAS_INTERNAL_DEP (insn) = 1;\n \n@@ -1119,7 +1132,7 @@ add_or_update_dep_1 (dep_t new_dep, bool resolved_p,\n \n   if (mem1 != NULL_RTX)\n     {\n-      gcc_assert (current_sched_info->flags & DO_SPECULATION);\n+      gcc_assert (sched_deps_info->generate_spec_deps);\n       DEP_STATUS (new_dep) = set_dep_weak (DEP_STATUS (new_dep), BEGIN_DATA,\n \t\t\t\t\t   estimate_dep_weak (mem1, mem2));\n     }\n@@ -1339,13 +1352,21 @@ add_dependence_list (rtx insn, rtx list, int uncond, enum reg_note dep_type)\n     }\n }\n \n-/* Similar, but free *LISTP at the same time.  */\n+/* Similar, but free *LISTP at the same time, when the context \n+   is not readonly.  */\n \n static void\n-add_dependence_list_and_free (rtx insn, rtx *listp, int uncond,\n-\t\t\t      enum reg_note dep_type)\n+add_dependence_list_and_free (struct deps *deps, rtx insn, rtx *listp, \n+                              int uncond, enum reg_note dep_type)\n {\n   rtx list, next;\n+\n+  if (deps->readonly)\n+    {\n+      add_dependence_list (insn, *listp, uncond, dep_type);\n+      return;\n+    }\n+\n   for (list = *listp, *listp = NULL; list ; list = next)\n     {\n       next = XEXP (list, 1);\n@@ -1355,6 +1376,52 @@ add_dependence_list_and_free (rtx insn, rtx *listp, int uncond,\n     }\n }\n \n+/* Remove all occurences of INSN from LIST.  Return the number of \n+   occurences removed.  */\n+\n+static int\n+remove_from_dependence_list (rtx insn, rtx* listp)\n+{\n+  int removed = 0;\n+  \n+  while (*listp)\n+    {\n+      if (XEXP (*listp, 0) == insn)\n+        {\n+          remove_free_INSN_LIST_node (listp);\n+          removed++;\n+          continue;\n+        }\n+      \n+      listp = &XEXP (*listp, 1);\n+    }\n+  \n+  return removed;\n+}\n+\n+/* Same as above, but process two lists at once.  */\n+static int \n+remove_from_both_dependence_lists (rtx insn, rtx *listp, rtx *exprp)\n+{\n+  int removed = 0;\n+  \n+  while (*listp)\n+    {\n+      if (XEXP (*listp, 0) == insn)\n+        {\n+          remove_free_INSN_LIST_node (listp);\n+          remove_free_EXPR_LIST_node (exprp);\n+          removed++;\n+          continue;\n+        }\n+      \n+      listp = &XEXP (*listp, 1);\n+      exprp = &XEXP (*exprp, 1);\n+    }\n+  \n+  return removed;\n+}\n+\n /* Clear all dependencies for an insn.  */\n static void\n delete_all_dependences (rtx insn)\n@@ -1433,6 +1500,7 @@ add_insn_mem_dependence (struct deps *deps, bool read_p,\n   rtx *mem_list;\n   rtx link;\n \n+  gcc_assert (!deps->readonly);\n   if (read_p)\n     {\n       insn_list = &deps->pending_read_insns;\n@@ -1449,7 +1517,7 @@ add_insn_mem_dependence (struct deps *deps, bool read_p,\n   link = alloc_INSN_LIST (insn, *insn_list);\n   *insn_list = link;\n \n-  if (current_sched_info->use_cselib)\n+  if (sched_deps_info->use_cselib)\n     {\n       mem = shallow_copy_rtx (mem);\n       XEXP (mem, 0) = cselib_subst_to_values (XEXP (mem, 0));\n@@ -1468,23 +1536,202 @@ flush_pending_lists (struct deps *deps, rtx insn, int for_read,\n {\n   if (for_write)\n     {\n-      add_dependence_list_and_free (insn, &deps->pending_read_insns, 1,\n-\t\t\t\t    REG_DEP_ANTI);\n-      free_EXPR_LIST_list (&deps->pending_read_mems);\n-      deps->pending_read_list_length = 0;\n+      add_dependence_list_and_free (deps, insn, &deps->pending_read_insns, \n+                                    1, REG_DEP_ANTI);\n+      if (!deps->readonly)\n+        {\n+          free_EXPR_LIST_list (&deps->pending_read_mems);\n+          deps->pending_read_list_length = 0;\n+        }\n     }\n \n-  add_dependence_list_and_free (insn, &deps->pending_write_insns, 1,\n+  add_dependence_list_and_free (deps, insn, &deps->pending_write_insns, 1,\n \t\t\t\tfor_read ? REG_DEP_ANTI : REG_DEP_OUTPUT);\n-  free_EXPR_LIST_list (&deps->pending_write_mems);\n-  deps->pending_write_list_length = 0;\n \n-  add_dependence_list_and_free (insn, &deps->last_pending_memory_flush, 1,\n-\t\t\t\tfor_read ? REG_DEP_ANTI : REG_DEP_OUTPUT);\n-  deps->last_pending_memory_flush = alloc_INSN_LIST (insn, NULL_RTX);\n-  deps->pending_flush_length = 1;\n+  add_dependence_list_and_free (deps, insn, \n+                                &deps->last_pending_memory_flush, 1,\n+                                for_read ? REG_DEP_ANTI : REG_DEP_OUTPUT);\n+  if (!deps->readonly)\n+    {\n+      free_EXPR_LIST_list (&deps->pending_write_mems);\n+      deps->pending_write_list_length = 0;\n+\n+      deps->last_pending_memory_flush = alloc_INSN_LIST (insn, NULL_RTX);\n+      deps->pending_flush_length = 1;\n+    }\n+}\n+\f\n+/* Instruction which dependencies we are analyzing.  */\n+static rtx cur_insn = NULL_RTX;\n+\n+/* Implement hooks for haifa scheduler.  */\n+\n+static void\n+haifa_start_insn (rtx insn)\n+{\n+  gcc_assert (insn && !cur_insn);\n+\n+  cur_insn = insn;\n+}\n+\n+static void\n+haifa_finish_insn (void)\n+{\n+  cur_insn = NULL;\n+}\n+\n+void\n+haifa_note_reg_set (int regno)\n+{\n+  SET_REGNO_REG_SET (reg_pending_sets, regno);\n+}\n+\n+void\n+haifa_note_reg_clobber (int regno)\n+{\n+  SET_REGNO_REG_SET (reg_pending_clobbers, regno);\n+}\n+\n+void\n+haifa_note_reg_use (int regno)\n+{\n+  SET_REGNO_REG_SET (reg_pending_uses, regno);\n+}\n+\n+static void\n+haifa_note_mem_dep (rtx mem, rtx pending_mem, rtx pending_insn, ds_t ds)\n+{\n+  if (!(ds & SPECULATIVE))\n+    {\n+      mem = NULL_RTX;\n+      pending_mem = NULL_RTX;\n+    }\n+  else\n+    gcc_assert (ds & BEGIN_DATA);\n+\n+  {\n+    dep_def _dep, *dep = &_dep;\n+    \n+    init_dep_1 (dep, pending_insn, cur_insn, ds_to_dt (ds), \n+                current_sched_info->flags & USE_DEPS_LIST ? ds : -1);\n+    maybe_add_or_update_dep_1 (dep, false, pending_mem, mem);\n+  }\n+\n+}\n+\n+static void\n+haifa_note_dep (rtx elem, ds_t ds)\n+{\n+  dep_def _dep;\n+  dep_t dep = &_dep;\n+\n+  init_dep (dep, elem, cur_insn, ds_to_dt (ds));\n+  maybe_add_or_update_dep_1 (dep, false, NULL_RTX, NULL_RTX);\n+}\n+\n+static void\n+note_reg_use (int r)\n+{\n+  if (sched_deps_info->note_reg_use)\n+    sched_deps_info->note_reg_use (r);\n+}\n+\n+static void\n+note_reg_set (int r)\n+{\n+  if (sched_deps_info->note_reg_set)\n+    sched_deps_info->note_reg_set (r);\n+}\n+\n+static void\n+note_reg_clobber (int r)\n+{\n+  if (sched_deps_info->note_reg_clobber)\n+    sched_deps_info->note_reg_clobber (r);\n+}\n+\n+static void\n+note_mem_dep (rtx m1, rtx m2, rtx e, ds_t ds)\n+{\n+  if (sched_deps_info->note_mem_dep)\n+    sched_deps_info->note_mem_dep (m1, m2, e, ds);\n+}\n+\n+static void\n+note_dep (rtx e, ds_t ds)\n+{\n+  if (sched_deps_info->note_dep)\n+    sched_deps_info->note_dep (e, ds);\n+}\n+\n+/* Return corresponding to DS reg_note.  */\n+enum reg_note\n+ds_to_dt (ds_t ds)\n+{\n+  if (ds & DEP_TRUE)\n+    return REG_DEP_TRUE;\n+  else if (ds & DEP_OUTPUT)\n+    return REG_DEP_OUTPUT;\n+  else\n+    {\n+      gcc_assert (ds & DEP_ANTI);\n+      return REG_DEP_ANTI;\n+    }\n }\n \f\n+\n+/* Internal variable for sched_analyze_[12] () functions.\n+   If it is nonzero, this means that sched_analyze_[12] looks\n+   at the most toplevel SET.  */\n+static bool can_start_lhs_rhs_p;\n+\n+/* Extend reg info for the deps context DEPS given that \n+   we have just generated a register numbered REGNO.  */\n+static void\n+extend_deps_reg_info (struct deps *deps, int regno)\n+{\n+  int max_regno = regno + 1;\n+\n+  gcc_assert (!reload_completed);\n+\n+  /* In a readonly context, it would not hurt to extend info,\n+     but it should not be needed.  */\n+  if (reload_completed && deps->readonly)\n+    {\n+      deps->max_reg = max_regno;\n+      return;\n+    }\n+\n+  if (max_regno > deps->max_reg)\n+    {\n+      deps->reg_last = XRESIZEVEC (struct deps_reg, deps->reg_last, \n+                                   max_regno);\n+      memset (&deps->reg_last[deps->max_reg],\n+              0, (max_regno - deps->max_reg) \n+              * sizeof (struct deps_reg));\n+      deps->max_reg = max_regno;\n+    }\n+}\n+\n+/* Extends REG_INFO_P if needed.  */\n+void\n+maybe_extend_reg_info_p (void)\n+{\n+  /* Extend REG_INFO_P, if needed.  */\n+  if ((unsigned int)max_regno - 1 >= reg_info_p_size)\n+    {\n+      size_t new_reg_info_p_size = max_regno + 128;\n+\n+      gcc_assert (!reload_completed && sel_sched_p ());\n+\n+      reg_info_p = (struct reg_info_t *) xrecalloc (reg_info_p,\n+                                                    new_reg_info_p_size,\n+                                                    reg_info_p_size,\n+                                                    sizeof (*reg_info_p));\n+      reg_info_p_size = new_reg_info_p_size;\n+    }\n+}\n+\n /* Analyze a single reference to register (reg:MODE REGNO) in INSN.\n    The type of the reference is specified by REF and can be SET,\n    CLOBBER, PRE_DEC, POST_DEC, PRE_INC, POST_INC or USE.  */\n@@ -1493,6 +1740,13 @@ static void\n sched_analyze_reg (struct deps *deps, int regno, enum machine_mode mode,\n \t\t   enum rtx_code ref, rtx insn)\n {\n+  /* We could emit new pseudos in renaming.  Extend the reg structures.  */\n+  if (!reload_completed && sel_sched_p ()\n+      && (regno >= max_reg_num () - 1 || regno >= deps->max_reg))\n+    extend_deps_reg_info (deps, regno);\n+\n+  maybe_extend_reg_info_p ();\n+\n   /* A hard reg in a wide mode may really be multiple registers.\n      If so, mark all of them just like the first.  */\n   if (regno < FIRST_PSEUDO_REGISTER)\n@@ -1501,17 +1755,17 @@ sched_analyze_reg (struct deps *deps, int regno, enum machine_mode mode,\n       if (ref == SET)\n \t{\n \t  while (--i >= 0)\n-\t    SET_REGNO_REG_SET (reg_pending_sets, regno + i);\n+\t    note_reg_set (regno + i);\n \t}\n       else if (ref == USE)\n \t{\n \t  while (--i >= 0)\n-\t    SET_REGNO_REG_SET (reg_pending_uses, regno + i);\n+\t    note_reg_use (regno + i);\n \t}\n       else\n \t{\n \t  while (--i >= 0)\n-\t    SET_REGNO_REG_SET (reg_pending_clobbers, regno + i);\n+\t    note_reg_clobber (regno + i);\n \t}\n     }\n \n@@ -1527,11 +1781,11 @@ sched_analyze_reg (struct deps *deps, int regno, enum machine_mode mode,\n   else\n     {\n       if (ref == SET)\n-\tSET_REGNO_REG_SET (reg_pending_sets, regno);\n+\tnote_reg_set (regno);\n       else if (ref == USE)\n-\tSET_REGNO_REG_SET (reg_pending_uses, regno);\n+\tnote_reg_use (regno);\n       else\n-\tSET_REGNO_REG_SET (reg_pending_clobbers, regno);\n+\tnote_reg_clobber (regno);\n \n       /* Pseudos that are REG_EQUIV to something may be replaced\n \t by that during reloading.  We need only add dependencies for\n@@ -1547,7 +1801,8 @@ sched_analyze_reg (struct deps *deps, int regno, enum machine_mode mode,\n \t already cross one.  */\n       if (REG_N_CALLS_CROSSED (regno) == 0)\n \t{\n-\t  if (ref == USE)\n+\t  if (!deps->readonly \n+              && ref == USE)\n \t    deps->sched_before_next_call\n \t      = alloc_INSN_LIST (insn, deps->sched_before_next_call);\n \t  else\n@@ -1566,10 +1821,17 @@ sched_analyze_1 (struct deps *deps, rtx x, rtx insn)\n {\n   rtx dest = XEXP (x, 0);\n   enum rtx_code code = GET_CODE (x);\n+  bool cslr_p = can_start_lhs_rhs_p;\n \n+  can_start_lhs_rhs_p = false;\n+\n+  gcc_assert (dest);\n   if (dest == 0)\n     return;\n \n+  if (cslr_p && sched_deps_info->start_lhs)\n+    sched_deps_info->start_lhs (dest);\n+\n   if (GET_CODE (dest) == PARALLEL)\n     {\n       int i;\n@@ -1581,8 +1843,18 @@ sched_analyze_1 (struct deps *deps, rtx x, rtx insn)\n \t\t\t\t\t    XEXP (XVECEXP (dest, 0, i), 0)),\n \t\t\t   insn);\n \n-      if (GET_CODE (x) == SET)\n-\tsched_analyze_2 (deps, SET_SRC (x), insn);\n+      if (cslr_p && sched_deps_info->finish_lhs)\n+\tsched_deps_info->finish_lhs ();\n+\n+      if (code == SET)\n+\t{\n+\t  can_start_lhs_rhs_p = cslr_p;\n+\n+\t  sched_analyze_2 (deps, SET_SRC (x), insn);\n+\n+\t  can_start_lhs_rhs_p = false;\n+\t}\n+\n       return;\n     }\n \n@@ -1633,16 +1905,18 @@ sched_analyze_1 (struct deps *deps, rtx x, rtx insn)\n       /* Writing memory.  */\n       rtx t = dest;\n \n-      if (current_sched_info->use_cselib)\n+      if (sched_deps_info->use_cselib)\n \t{\n \t  t = shallow_copy_rtx (dest);\n \t  cselib_lookup (XEXP (t, 0), Pmode, 1);\n \t  XEXP (t, 0) = cselib_subst_to_values (XEXP (t, 0));\n \t}\n       t = canon_rtx (t);\n \n-      if ((deps->pending_read_list_length + deps->pending_write_list_length)\n-\t  > MAX_PENDING_LIST_LENGTH)\n+      /* Pending lists can't get larger with a readonly context.  */\n+      if (!deps->readonly\n+          && ((deps->pending_read_list_length + deps->pending_write_list_length)\n+              > MAX_PENDING_LIST_LENGTH))\n \t{\n \t  /* Flush all pending reads and writes to prevent the pending lists\n \t     from getting any larger.  Insn scheduling runs too slowly when\n@@ -1661,7 +1935,8 @@ sched_analyze_1 (struct deps *deps, rtx x, rtx insn)\n \t    {\n \t      if (anti_dependence (XEXP (pending_mem, 0), t)\n \t\t  && ! sched_insns_conditions_mutex_p (insn, XEXP (pending, 0)))\n-\t\tadd_dependence (insn, XEXP (pending, 0), REG_DEP_ANTI);\n+\t\tnote_mem_dep (t, XEXP (pending_mem, 0), XEXP (pending, 0),\n+\t\t\t      DEP_ANTI);\n \n \t      pending = XEXP (pending, 1);\n \t      pending_mem = XEXP (pending_mem, 1);\n@@ -1673,7 +1948,8 @@ sched_analyze_1 (struct deps *deps, rtx x, rtx insn)\n \t    {\n \t      if (output_dependence (XEXP (pending_mem, 0), t)\n \t\t  && ! sched_insns_conditions_mutex_p (insn, XEXP (pending, 0)))\n-\t\tadd_dependence (insn, XEXP (pending, 0), REG_DEP_OUTPUT);\n+\t\tnote_mem_dep (t, XEXP (pending_mem, 0), XEXP (pending, 0),\n+\t\t\t      DEP_OUTPUT);\n \n \t      pending = XEXP (pending, 1);\n \t      pending_mem = XEXP (pending_mem, 1);\n@@ -1682,29 +1958,45 @@ sched_analyze_1 (struct deps *deps, rtx x, rtx insn)\n \t  add_dependence_list (insn, deps->last_pending_memory_flush, 1,\n \t\t\t       REG_DEP_ANTI);\n \n-\t  add_insn_mem_dependence (deps, false, insn, dest);\n+          if (!deps->readonly)\n+            add_insn_mem_dependence (deps, false, insn, dest);\n \t}\n       sched_analyze_2 (deps, XEXP (dest, 0), insn);\n     }\n \n+  if (cslr_p && sched_deps_info->finish_lhs)\n+    sched_deps_info->finish_lhs ();\n+\n   /* Analyze reads.  */\n   if (GET_CODE (x) == SET)\n-    sched_analyze_2 (deps, SET_SRC (x), insn);\n+    {\n+      can_start_lhs_rhs_p = cslr_p;\n+\n+      sched_analyze_2 (deps, SET_SRC (x), insn);\n+\n+      can_start_lhs_rhs_p = false;\n+    }\n }\n \n /* Analyze the uses of memory and registers in rtx X in INSN.  */\n-\n static void\n sched_analyze_2 (struct deps *deps, rtx x, rtx insn)\n {\n   int i;\n   int j;\n   enum rtx_code code;\n   const char *fmt;\n+  bool cslr_p = can_start_lhs_rhs_p;\n+\n+  can_start_lhs_rhs_p = false;\n \n+  gcc_assert (x);\n   if (x == 0)\n     return;\n \n+  if (cslr_p && sched_deps_info->start_rhs)\n+    sched_deps_info->start_rhs (x);\n+\n   code = GET_CODE (x);\n \n   switch (code)\n@@ -1719,6 +2011,10 @@ sched_analyze_2 (struct deps *deps, rtx x, rtx insn)\n       /* Ignore constants.  Note that we must handle CONST_DOUBLE here\n          because it may have a cc0_rtx in its CONST_DOUBLE_CHAIN field, but\n          this does not mean that this insn is using cc0.  */\n+\n+      if (cslr_p && sched_deps_info->finish_rhs)\n+\tsched_deps_info->finish_rhs ();\n+\n       return;\n \n #ifdef HAVE_cc0\n@@ -1728,6 +2024,10 @@ sched_analyze_2 (struct deps *deps, rtx x, rtx insn)\n        /* Don't move CC0 setter to another block (it can set up the\n         same flag for previous CC0 users which is safe).  */\n       CANT_MOVE (prev_nonnote_insn (insn)) = 1;\n+\n+      if (cslr_p && sched_deps_info->finish_rhs)\n+\tsched_deps_info->finish_rhs ();\n+\n       return;\n #endif\n \n@@ -1748,6 +2048,10 @@ sched_analyze_2 (struct deps *deps, rtx x, rtx insn)\n \t  sched_analyze_reg (deps, FIRST_STACK_REG, mode, SET, insn);\n \t}\n #endif\n+\n+\tif (cslr_p && sched_deps_info->finish_rhs)\n+\t  sched_deps_info->finish_rhs ();\n+\n \treturn;\n       }\n \n@@ -1758,7 +2062,7 @@ sched_analyze_2 (struct deps *deps, rtx x, rtx insn)\n \trtx pending, pending_mem;\n \trtx t = x;\n \n-\tif (current_sched_info->use_cselib)\n+\tif (sched_deps_info->use_cselib)\n \t  {\n \t    t = shallow_copy_rtx (t);\n \t    cselib_lookup (XEXP (t, 0), Pmode, 1);\n@@ -1771,7 +2075,8 @@ sched_analyze_2 (struct deps *deps, rtx x, rtx insn)\n \t  {\n \t    if (read_dependence (XEXP (pending_mem, 0), t)\n \t\t&& ! sched_insns_conditions_mutex_p (insn, XEXP (pending, 0)))\n-\t      add_dependence (insn, XEXP (pending, 0), REG_DEP_ANTI);\n+\t      note_mem_dep (t, XEXP (pending_mem, 0), XEXP (pending, 0),\n+\t\t\t    DEP_ANTI);\n \n \t    pending = XEXP (pending, 1);\n \t    pending_mem = XEXP (pending_mem, 1);\n@@ -1784,38 +2089,44 @@ sched_analyze_2 (struct deps *deps, rtx x, rtx insn)\n \t    if (true_dependence (XEXP (pending_mem, 0), VOIDmode,\n \t\t\t\t t, rtx_varies_p)\n \t\t&& ! sched_insns_conditions_mutex_p (insn, XEXP (pending, 0)))\n-              {\n-                if ((current_sched_info->flags & DO_SPECULATION)\n-\t\t    && (spec_info->mask & BEGIN_DATA))\n-\t\t  /* Create a data-speculative dependence between producer\n-\t\t     and consumer.  */\n-\t\t  {\n-\t\t    dep_def _dep, *dep = &_dep;\n-\n-\t\t    init_dep_1 (dep, XEXP (pending, 0), insn, REG_DEP_TRUE,\n-\t\t\t\tBEGIN_DATA | DEP_TRUE);\n-\n-\t\t    maybe_add_or_update_dep_1 (dep, false,\n-\t\t\t\t\t       XEXP (pending_mem, 0), t);\n-\t\t  }\n-                else\n-                  add_dependence (insn, XEXP (pending, 0), REG_DEP_TRUE);\n-              }\n+\t      note_mem_dep (t, XEXP (pending_mem, 0), XEXP (pending, 0),\n+\t\t\t    sched_deps_info->generate_spec_deps\n+\t\t\t    ? BEGIN_DATA | DEP_TRUE : DEP_TRUE);\n \n \t    pending = XEXP (pending, 1);\n \t    pending_mem = XEXP (pending_mem, 1);\n \t  }\n \n \tfor (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n-\t  if (! JUMP_P (XEXP (u, 0)) || deps_may_trap_p (x))\n-\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\t  {\n+\t    if (! JUMP_P (XEXP (u, 0)))\n+\t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\t    else if (deps_may_trap_p (x))\n+\t      {\n+\t\tif ((sched_deps_info->generate_spec_deps)\n+\t\t    && sel_sched_p () && (spec_info->mask & BEGIN_CONTROL))\n+\t\t  {\n+\t\t    ds_t ds = set_dep_weak (DEP_ANTI, BEGIN_CONTROL,\n+\t\t\t\t\t    MAX_DEP_WEAK);\n+\n+\t\t    note_dep (XEXP (u, 0), ds);\n+\t\t  }\n+\t\telse\n+\t\t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\t      }\n+\t  }\n \n \t/* Always add these dependencies to pending_reads, since\n \t   this insn may be followed by a write.  */\n-\tadd_insn_mem_dependence (deps, true, insn, x);\n+        if (!deps->readonly)\n+          add_insn_mem_dependence (deps, true, insn, x);\n \n \t/* Take advantage of tail recursion here.  */\n \tsched_analyze_2 (deps, XEXP (x, 0), insn);\n+\n+\tif (cslr_p && sched_deps_info->finish_rhs)\n+\t  sched_deps_info->finish_rhs ();\n+\n \treturn;\n       }\n \n@@ -1847,6 +2158,10 @@ sched_analyze_2 (struct deps *deps, rtx x, rtx insn)\n \t  {\n \t    for (j = 0; j < ASM_OPERANDS_INPUT_LENGTH (x); j++)\n \t      sched_analyze_2 (deps, ASM_OPERANDS_INPUT (x, j), insn);\n+\n+\t    if (cslr_p && sched_deps_info->finish_rhs)\n+\t      sched_deps_info->finish_rhs ();\n+\n \t    return;\n \t  }\n \tbreak;\n@@ -1864,6 +2179,10 @@ sched_analyze_2 (struct deps *deps, rtx x, rtx insn)\n          to get the proper antecedent for the read.  */\n       sched_analyze_2 (deps, XEXP (x, 0), insn);\n       sched_analyze_1 (deps, x, insn);\n+\n+      if (cslr_p && sched_deps_info->finish_rhs)\n+\tsched_deps_info->finish_rhs ();\n+\n       return;\n \n     case POST_MODIFY:\n@@ -1872,6 +2191,10 @@ sched_analyze_2 (struct deps *deps, rtx x, rtx insn)\n       sched_analyze_2 (deps, XEXP (x, 0), insn);\n       sched_analyze_2 (deps, XEXP (x, 1), insn);\n       sched_analyze_1 (deps, x, insn);\n+\n+      if (cslr_p && sched_deps_info->finish_rhs)\n+\tsched_deps_info->finish_rhs ();\n+\n       return;\n \n     default:\n@@ -1888,10 +2211,12 @@ sched_analyze_2 (struct deps *deps, rtx x, rtx insn)\n \tfor (j = 0; j < XVECLEN (x, i); j++)\n \t  sched_analyze_2 (deps, XVECEXP (x, i, j), insn);\n     }\n+\n+  if (cslr_p && sched_deps_info->finish_rhs)\n+    sched_deps_info->finish_rhs ();\n }\n \n /* Analyze an INSN with pattern X to find all dependencies.  */\n-\n static void\n sched_analyze_insn (struct deps *deps, rtx x, rtx insn)\n {\n@@ -1900,6 +2225,9 @@ sched_analyze_insn (struct deps *deps, rtx x, rtx insn)\n   unsigned i;\n   reg_set_iterator rsi;\n \n+  can_start_lhs_rhs_p = (NONJUMP_INSN_P (insn)\n+\t\t\t && code == SET);\n+\n   if (code == COND_EXEC)\n     {\n       sched_analyze_2 (deps, COND_EXEC_TEST (x), insn);\n@@ -1964,25 +2292,34 @@ sched_analyze_insn (struct deps *deps, rtx x, rtx insn)\n       else\n \t{\n \t  rtx pending, pending_mem;\n-\t  regset_head tmp_uses, tmp_sets;\n-\t  INIT_REG_SET (&tmp_uses);\n-\t  INIT_REG_SET (&tmp_sets);\n-\n-\t  (*current_sched_info->compute_jump_reg_dependencies)\n-\t    (insn, &deps->reg_conditional_sets, &tmp_uses, &tmp_sets);\n-\t  /* Make latency of jump equal to 0 by using anti-dependence.  */\n-\t  EXECUTE_IF_SET_IN_REG_SET (&tmp_uses, 0, i, rsi)\n-\t    {\n-\t      struct deps_reg *reg_last = &deps->reg_last[i];\n-\t      add_dependence_list (insn, reg_last->sets, 0, REG_DEP_ANTI);\n-\t      add_dependence_list (insn, reg_last->clobbers, 0, REG_DEP_ANTI);\n-\t      reg_last->uses_length++;\n-\t      reg_last->uses = alloc_INSN_LIST (insn, reg_last->uses);\n-\t    }\n-\t  IOR_REG_SET (reg_pending_sets, &tmp_sets);\n \n-\t  CLEAR_REG_SET (&tmp_uses);\n-\t  CLEAR_REG_SET (&tmp_sets);\n+          if (sched_deps_info->compute_jump_reg_dependencies)\n+            {\n+              regset_head tmp_uses, tmp_sets;\n+              INIT_REG_SET (&tmp_uses);\n+              INIT_REG_SET (&tmp_sets);\n+\n+              (*sched_deps_info->compute_jump_reg_dependencies)\n+                (insn, &deps->reg_conditional_sets, &tmp_uses, &tmp_sets);\n+              /* Make latency of jump equal to 0 by using anti-dependence.  */\n+              EXECUTE_IF_SET_IN_REG_SET (&tmp_uses, 0, i, rsi)\n+                {\n+                  struct deps_reg *reg_last = &deps->reg_last[i];\n+                  add_dependence_list (insn, reg_last->sets, 0, REG_DEP_ANTI);\n+                  add_dependence_list (insn, reg_last->clobbers, 0,\n+\t\t\t\t       REG_DEP_ANTI);\n+\n+                  if (!deps->readonly)\n+                    {\n+                      reg_last->uses_length++;\n+                      reg_last->uses = alloc_INSN_LIST (insn, reg_last->uses);\n+                    }\n+                }\n+              IOR_REG_SET (reg_pending_sets, &tmp_sets);\n+\n+              CLEAR_REG_SET (&tmp_uses);\n+              CLEAR_REG_SET (&tmp_sets);\n+            }\n \n \t  /* All memory writes and volatile reads must happen before the\n \t     jump.  Non-volatile reads must happen before the jump iff\n@@ -2024,89 +2361,123 @@ sched_analyze_insn (struct deps *deps, rtx x, rtx insn)\n       || (NONJUMP_INSN_P (insn) && control_flow_insn_p (insn)))\n     reg_pending_barrier = MOVE_BARRIER;\n \n-  /* Add register dependencies for insn.\n-     If the current insn is conditional, we can't free any of the lists.  */\n-  if (sched_get_condition (insn))\n+  /* If the current insn is conditional, we can't free any\n+     of the lists.  */\n+  if (sched_has_condition_p (insn))\n     {\n       EXECUTE_IF_SET_IN_REG_SET (reg_pending_uses, 0, i, rsi)\n-\t{\n-\t  struct deps_reg *reg_last = &deps->reg_last[i];\n-\t  add_dependence_list (insn, reg_last->sets, 0, REG_DEP_TRUE);\n-\t  add_dependence_list (insn, reg_last->clobbers, 0, REG_DEP_TRUE);\n-\t  reg_last->uses = alloc_INSN_LIST (insn, reg_last->uses);\n-\t  reg_last->uses_length++;\n-\t}\n+        {\n+          struct deps_reg *reg_last = &deps->reg_last[i];\n+          add_dependence_list (insn, reg_last->sets, 0, REG_DEP_TRUE);\n+          add_dependence_list (insn, reg_last->clobbers, 0, REG_DEP_TRUE);\n+              \n+          if (!deps->readonly)\n+            {\n+              reg_last->uses = alloc_INSN_LIST (insn, reg_last->uses);\n+              reg_last->uses_length++;\n+            }\n+        }\n       EXECUTE_IF_SET_IN_REG_SET (reg_pending_clobbers, 0, i, rsi)\n-\t{\n-\t  struct deps_reg *reg_last = &deps->reg_last[i];\n-\t  add_dependence_list (insn, reg_last->sets, 0, REG_DEP_OUTPUT);\n-\t  add_dependence_list (insn, reg_last->uses, 0, REG_DEP_ANTI);\n-\t  reg_last->clobbers = alloc_INSN_LIST (insn, reg_last->clobbers);\n-\t  reg_last->clobbers_length++;\n-\t}\n+        {\n+          struct deps_reg *reg_last = &deps->reg_last[i];\n+          add_dependence_list (insn, reg_last->sets, 0, REG_DEP_OUTPUT);\n+          add_dependence_list (insn, reg_last->uses, 0, REG_DEP_ANTI);\n+\n+          if (!deps->readonly)\n+            {\n+              reg_last->clobbers = alloc_INSN_LIST (insn, reg_last->clobbers);\n+              reg_last->clobbers_length++;\n+            }\n+        }\n       EXECUTE_IF_SET_IN_REG_SET (reg_pending_sets, 0, i, rsi)\n-\t{\n-\t  struct deps_reg *reg_last = &deps->reg_last[i];\n-\t  add_dependence_list (insn, reg_last->sets, 0, REG_DEP_OUTPUT);\n-\t  add_dependence_list (insn, reg_last->clobbers, 0, REG_DEP_OUTPUT);\n-\t  add_dependence_list (insn, reg_last->uses, 0, REG_DEP_ANTI);\n-\t  reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);\n-\t  SET_REGNO_REG_SET (&deps->reg_conditional_sets, i);\n-\t}\n+        {\n+          struct deps_reg *reg_last = &deps->reg_last[i];\n+          add_dependence_list (insn, reg_last->sets, 0, REG_DEP_OUTPUT);\n+          add_dependence_list (insn, reg_last->clobbers, 0, REG_DEP_OUTPUT);\n+          add_dependence_list (insn, reg_last->uses, 0, REG_DEP_ANTI);\n+\n+          if (!deps->readonly)\n+            {\n+              reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);\n+              SET_REGNO_REG_SET (&deps->reg_conditional_sets, i);\n+            }\n+        }\n     }\n   else\n     {\n       EXECUTE_IF_SET_IN_REG_SET (reg_pending_uses, 0, i, rsi)\n-\t{\n-\t  struct deps_reg *reg_last = &deps->reg_last[i];\n-\t  add_dependence_list (insn, reg_last->sets, 0, REG_DEP_TRUE);\n-\t  add_dependence_list (insn, reg_last->clobbers, 0, REG_DEP_TRUE);\n-\t  reg_last->uses_length++;\n-\t  reg_last->uses = alloc_INSN_LIST (insn, reg_last->uses);\n-\t}\n+        {\n+          struct deps_reg *reg_last = &deps->reg_last[i];\n+          add_dependence_list (insn, reg_last->sets, 0, REG_DEP_TRUE);\n+          add_dependence_list (insn, reg_last->clobbers, 0, REG_DEP_TRUE);\n+\n+          if (!deps->readonly)\n+            {\n+              reg_last->uses_length++;\n+              reg_last->uses = alloc_INSN_LIST (insn, reg_last->uses);\n+            }\n+        }\n       EXECUTE_IF_SET_IN_REG_SET (reg_pending_clobbers, 0, i, rsi)\n-\t{\n-\t  struct deps_reg *reg_last = &deps->reg_last[i];\n-\t  if (reg_last->uses_length > MAX_PENDING_LIST_LENGTH\n-\t      || reg_last->clobbers_length > MAX_PENDING_LIST_LENGTH)\n-\t    {\n-\t      add_dependence_list_and_free (insn, &reg_last->sets, 0,\n-\t\t\t\t\t    REG_DEP_OUTPUT);\n-\t      add_dependence_list_and_free (insn, &reg_last->uses, 0,\n-\t\t\t\t\t    REG_DEP_ANTI);\n-\t      add_dependence_list_and_free (insn, &reg_last->clobbers, 0,\n-\t\t\t\t\t    REG_DEP_OUTPUT);\n-\t      reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);\n-\t      reg_last->clobbers_length = 0;\n-\t      reg_last->uses_length = 0;\n-\t    }\n-\t  else\n-\t    {\n-\t      add_dependence_list (insn, reg_last->sets, 0, REG_DEP_OUTPUT);\n-\t      add_dependence_list (insn, reg_last->uses, 0, REG_DEP_ANTI);\n-\t    }\n-\t  reg_last->clobbers_length++;\n-\t  reg_last->clobbers = alloc_INSN_LIST (insn, reg_last->clobbers);\n-\t}\n+        {\n+          struct deps_reg *reg_last = &deps->reg_last[i];\n+          if (reg_last->uses_length > MAX_PENDING_LIST_LENGTH\n+              || reg_last->clobbers_length > MAX_PENDING_LIST_LENGTH)\n+            {\n+              add_dependence_list_and_free (deps, insn, &reg_last->sets, 0,\n+                                            REG_DEP_OUTPUT);\n+              add_dependence_list_and_free (deps, insn, &reg_last->uses, 0,\n+                                            REG_DEP_ANTI);\n+              add_dependence_list_and_free (deps, insn, &reg_last->clobbers, 0,\n+                                            REG_DEP_OUTPUT);\n+\n+              if (!deps->readonly)\n+                {\n+                  reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);\n+                  reg_last->clobbers_length = 0;\n+                  reg_last->uses_length = 0;\n+                }\n+            }\n+          else\n+            {\n+              add_dependence_list (insn, reg_last->sets, 0, REG_DEP_OUTPUT);\n+              add_dependence_list (insn, reg_last->uses, 0, REG_DEP_ANTI);\n+            }\n+\n+          if (!deps->readonly)\n+            {\n+              reg_last->clobbers_length++;\n+              reg_last->clobbers = alloc_INSN_LIST (insn, reg_last->clobbers);\n+            }\n+        }\n       EXECUTE_IF_SET_IN_REG_SET (reg_pending_sets, 0, i, rsi)\n-\t{\n-\t  struct deps_reg *reg_last = &deps->reg_last[i];\n-\t  add_dependence_list_and_free (insn, &reg_last->sets, 0,\n-\t\t\t\t\tREG_DEP_OUTPUT);\n-\t  add_dependence_list_and_free (insn, &reg_last->clobbers, 0,\n-\t\t\t\t\tREG_DEP_OUTPUT);\n-\t  add_dependence_list_and_free (insn, &reg_last->uses, 0,\n-\t\t\t\t\tREG_DEP_ANTI);\n-\t  reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);\n-\t  reg_last->uses_length = 0;\n-\t  reg_last->clobbers_length = 0;\n-\t  CLEAR_REGNO_REG_SET (&deps->reg_conditional_sets, i);\n-\t}\n+        {\n+          struct deps_reg *reg_last = &deps->reg_last[i];\n+          add_dependence_list_and_free (deps, insn, &reg_last->sets, 0,\n+                                        REG_DEP_OUTPUT);\n+          add_dependence_list_and_free (deps, insn, &reg_last->clobbers, 0,\n+                                        REG_DEP_OUTPUT);\n+          add_dependence_list_and_free (deps, insn, &reg_last->uses, 0,\n+                                        REG_DEP_ANTI);\n+\n+          if (!deps->readonly)\n+            {\n+              reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);\n+              reg_last->uses_length = 0;\n+              reg_last->clobbers_length = 0;\n+              CLEAR_REGNO_REG_SET (&deps->reg_conditional_sets, i);\n+            }\n+        }\n     }\n \n-  IOR_REG_SET (&deps->reg_last_in_use, reg_pending_uses);\n-  IOR_REG_SET (&deps->reg_last_in_use, reg_pending_clobbers);\n-  IOR_REG_SET (&deps->reg_last_in_use, reg_pending_sets);\n+  if (!deps->readonly)\n+    {\n+      IOR_REG_SET (&deps->reg_last_in_use, reg_pending_uses);\n+      IOR_REG_SET (&deps->reg_last_in_use, reg_pending_clobbers);\n+      IOR_REG_SET (&deps->reg_last_in_use, reg_pending_sets);\n+\n+      /* Set up the pending barrier found.  */\n+      deps->last_reg_pending_barrier = reg_pending_barrier;\n+    }\n \n   CLEAR_REG_SET (reg_pending_uses);\n   CLEAR_REG_SET (reg_pending_clobbers);\n@@ -2117,7 +2488,7 @@ sched_analyze_insn (struct deps *deps, rtx x, rtx insn)\n     {\n       /* In the case of barrier the most added dependencies are not\n          real, so we use anti-dependence here.  */\n-      if (sched_get_condition (insn))\n+      if (sched_has_condition_p (insn))\n \t{\n \t  EXECUTE_IF_SET_IN_REG_SET (&deps->reg_last_in_use, 0, i, rsi)\n \t    {\n@@ -2136,28 +2507,38 @@ sched_analyze_insn (struct deps *deps, rtx x, rtx insn)\n \t  EXECUTE_IF_SET_IN_REG_SET (&deps->reg_last_in_use, 0, i, rsi)\n \t    {\n \t      struct deps_reg *reg_last = &deps->reg_last[i];\n-\t      add_dependence_list_and_free (insn, &reg_last->uses, 0,\n+\t      add_dependence_list_and_free (deps, insn, &reg_last->uses, 0,\n \t\t\t\t\t    REG_DEP_ANTI);\n \t      add_dependence_list_and_free\n-\t\t(insn, &reg_last->sets, 0,\n+\t\t(deps, insn, &reg_last->sets, 0,\n \t\t reg_pending_barrier == TRUE_BARRIER ? REG_DEP_TRUE : REG_DEP_ANTI);\n \t      add_dependence_list_and_free\n-\t\t(insn, &reg_last->clobbers, 0,\n+\t\t(deps, insn, &reg_last->clobbers, 0,\n \t\t reg_pending_barrier == TRUE_BARRIER ? REG_DEP_TRUE : REG_DEP_ANTI);\n-\t      reg_last->uses_length = 0;\n-\t      reg_last->clobbers_length = 0;\n-\t    }\n-\t}\n \n-      for (i = 0; i < (unsigned)deps->max_reg; i++)\n-\t{\n-\t  struct deps_reg *reg_last = &deps->reg_last[i];\n-\t  reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);\n-\t  SET_REGNO_REG_SET (&deps->reg_last_in_use, i);\n+              if (!deps->readonly)\n+                {\n+                  reg_last->uses_length = 0;\n+                  reg_last->clobbers_length = 0;\n+                }\n+\t    }\n \t}\n \n-      flush_pending_lists (deps, insn, true, true);\n-      CLEAR_REG_SET (&deps->reg_conditional_sets);\n+      if (!deps->readonly)\n+        for (i = 0; i < (unsigned)deps->max_reg; i++)\n+          {\n+            struct deps_reg *reg_last = &deps->reg_last[i];\n+            reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);\n+            SET_REGNO_REG_SET (&deps->reg_last_in_use, i);\n+          }\n+\n+      /* Flush pending lists on jumps, but not on speculative checks.  */\n+      if (JUMP_P (insn) && !(sel_sched_p () \n+                             && sel_insn_is_speculation_check (insn)))\n+\tflush_pending_lists (deps, insn, true, true);\n+      \n+      if (!deps->readonly)\n+        CLEAR_REG_SET (&deps->reg_conditional_sets);\n       reg_pending_barrier = NOT_A_BARRIER;\n     }\n \n@@ -2203,160 +2584,211 @@ sched_analyze_insn (struct deps *deps, rtx x, rtx insn)\n       if (src_regno < FIRST_PSEUDO_REGISTER\n \t  || dest_regno < FIRST_PSEUDO_REGISTER)\n \t{\n-\t  if (deps->in_post_call_group_p == post_call_initial)\n+\t  if (!deps->readonly\n+              && deps->in_post_call_group_p == post_call_initial)\n \t    deps->in_post_call_group_p = post_call;\n \n-\t  SCHED_GROUP_P (insn) = 1;\n-\t  CANT_MOVE (insn) = 1;\n+          if (!sel_sched_p () || sched_emulate_haifa_p) \n+            {\n+              SCHED_GROUP_P (insn) = 1;\n+              CANT_MOVE (insn) = 1;\n+            }\n \t}\n       else\n \t{\n \tend_call_group:\n-\t  deps->in_post_call_group_p = not_post_call;\n+          if (!deps->readonly)\n+            deps->in_post_call_group_p = not_post_call;\n \t}\n     }\n \n-  /* Fixup the dependencies in the sched group.  */\n-  if (SCHED_GROUP_P (insn))\n-    fixup_sched_groups (insn);\n-\n   if ((current_sched_info->flags & DO_SPECULATION)\n       && !sched_insn_is_legitimate_for_speculation_p (insn, 0))\n     /* INSN has an internal dependency (e.g. r14 = [r14]) and thus cannot\n        be speculated.  */\n     {\n-      sd_iterator_def sd_it;\n-      dep_t dep;\n-\n-      for (sd_it = sd_iterator_start (insn, SD_LIST_SPEC_BACK);\n-\t   sd_iterator_cond (&sd_it, &dep);)\n-\tchange_spec_dep_to_hard (sd_it);\n+      if (sel_sched_p ())\n+        sel_mark_hard_insn (insn);\n+      else\n+        {\n+          sd_iterator_def sd_it;\n+          dep_t dep;\n+          \n+          for (sd_it = sd_iterator_start (insn, SD_LIST_SPEC_BACK);\n+               sd_iterator_cond (&sd_it, &dep);)\n+            change_spec_dep_to_hard (sd_it);\n+        }\n     }\n }\n \n-/* Analyze every insn between HEAD and TAIL inclusive, creating backward\n-   dependencies for each insn.  */\n-\n+/* Analyze INSN with DEPS as a context.  */\n void\n-sched_analyze (struct deps *deps, rtx head, rtx tail)\n+deps_analyze_insn (struct deps *deps, rtx insn)\n {\n-  rtx insn;\n+  if (sched_deps_info->start_insn)\n+    sched_deps_info->start_insn (insn);\n \n-  if (current_sched_info->use_cselib)\n-    cselib_init (true);\n+  if (NONJUMP_INSN_P (insn) || JUMP_P (insn))\n+    {\n+      /* Make each JUMP_INSN (but not a speculative check) \n+         a scheduling barrier for memory references.  */\n+      if (!deps->readonly\n+          && JUMP_P (insn) \n+          && !(sel_sched_p () \n+               && sel_insn_is_speculation_check (insn)))\n+        {\n+          /* Keep the list a reasonable size.  */\n+          if (deps->pending_flush_length++ > MAX_PENDING_LIST_LENGTH)\n+            flush_pending_lists (deps, insn, true, true);\n+          else\n+            deps->last_pending_memory_flush\n+              = alloc_INSN_LIST (insn, deps->last_pending_memory_flush);\n+        }\n+\n+      sched_analyze_insn (deps, PATTERN (insn), insn);\n+    }\n+  else if (CALL_P (insn))\n+    {\n+      int i;\n+\n+      CANT_MOVE (insn) = 1;\n+\n+      if (find_reg_note (insn, REG_SETJMP, NULL))\n+        {\n+          /* This is setjmp.  Assume that all registers, not just\n+             hard registers, may be clobbered by this call.  */\n+          reg_pending_barrier = MOVE_BARRIER;\n+        }\n+      else\n+        {\n+          for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+            /* A call may read and modify global register variables.  */\n+            if (global_regs[i])\n+              {\n+                SET_REGNO_REG_SET (reg_pending_sets, i);\n+                SET_REGNO_REG_SET (reg_pending_uses, i);\n+              }\n+          /* Other call-clobbered hard regs may be clobbered.\n+             Since we only have a choice between 'might be clobbered'\n+             and 'definitely not clobbered', we must include all\n+             partly call-clobbered registers here.  */\n+            else if (HARD_REGNO_CALL_PART_CLOBBERED (i, reg_raw_mode[i])\n+                     || TEST_HARD_REG_BIT (regs_invalidated_by_call, i))\n+              SET_REGNO_REG_SET (reg_pending_clobbers, i);\n+          /* We don't know what set of fixed registers might be used\n+             by the function, but it is certain that the stack pointer\n+             is among them, but be conservative.  */\n+            else if (fixed_regs[i])\n+              SET_REGNO_REG_SET (reg_pending_uses, i);\n+          /* The frame pointer is normally not used by the function\n+             itself, but by the debugger.  */\n+          /* ??? MIPS o32 is an exception.  It uses the frame pointer\n+             in the macro expansion of jal but does not represent this\n+             fact in the call_insn rtl.  */\n+            else if (i == FRAME_POINTER_REGNUM\n+                     || (i == HARD_FRAME_POINTER_REGNUM\n+                         && (! reload_completed || frame_pointer_needed)))\n+              SET_REGNO_REG_SET (reg_pending_uses, i);\n+        }\n+\n+      /* For each insn which shouldn't cross a call, add a dependence\n+         between that insn and this call insn.  */\n+      add_dependence_list_and_free (deps, insn, \n+                                    &deps->sched_before_next_call, 1,\n+                                    REG_DEP_ANTI);\n+\n+      sched_analyze_insn (deps, PATTERN (insn), insn);\n+\n+      /* If CALL would be in a sched group, then this will violate\n+\t convention that sched group insns have dependencies only on the\n+\t previous instruction.\n+\n+\t Of course one can say: \"Hey!  What about head of the sched group?\"\n+\t And I will answer: \"Basic principles (one dep per insn) are always\n+\t the same.\"  */\n+      gcc_assert (!SCHED_GROUP_P (insn));\n+\n+      /* In the absence of interprocedural alias analysis, we must flush\n+         all pending reads and writes, and start new dependencies starting\n+         from here.  But only flush writes for constant calls (which may\n+         be passed a pointer to something we haven't written yet).  */\n+      flush_pending_lists (deps, insn, true, ! RTL_CONST_OR_PURE_CALL_P (insn));\n+\n+      if (!deps->readonly)\n+        {\n+          /* Remember the last function call for limiting lifetimes.  */\n+          free_INSN_LIST_list (&deps->last_function_call);\n+          deps->last_function_call = alloc_INSN_LIST (insn, NULL_RTX);\n+          \n+          /* Before reload, begin a post-call group, so as to keep the\n+             lifetimes of hard registers correct.  */\n+          if (! reload_completed)\n+            deps->in_post_call_group_p = post_call;\n+        }\n+    }\n+\n+  if (sched_deps_info->use_cselib)\n+    cselib_process_insn (insn);\n+\n+  /* EH_REGION insn notes can not appear until well after we complete\n+     scheduling.  */\n+  if (NOTE_P (insn))\n+    gcc_assert (NOTE_KIND (insn) != NOTE_INSN_EH_REGION_BEG\n+\t\t&& NOTE_KIND (insn) != NOTE_INSN_EH_REGION_END);\n+\n+  if (sched_deps_info->finish_insn)\n+    sched_deps_info->finish_insn ();\n+\n+  /* Fixup the dependencies in the sched group.  */\n+  if ((NONJUMP_INSN_P (insn) || JUMP_P (insn)) \n+      && SCHED_GROUP_P (insn) && !sel_sched_p ())\n+    fixup_sched_groups (insn);\n+}\n+\n+/* Initialize DEPS for the new block beginning with HEAD.  */\n+void\n+deps_start_bb (struct deps *deps, rtx head)\n+{\n+  gcc_assert (!deps->readonly);\n \n   /* Before reload, if the previous block ended in a call, show that\n      we are inside a post-call group, so as to keep the lifetimes of\n      hard registers correct.  */\n   if (! reload_completed && !LABEL_P (head))\n     {\n-      insn = prev_nonnote_insn (head);\n+      rtx insn = prev_nonnote_insn (head);\n+\n       if (insn && CALL_P (insn))\n \tdeps->in_post_call_group_p = post_call_initial;\n     }\n+}\n+\n+/* Analyze every insn between HEAD and TAIL inclusive, creating backward\n+   dependencies for each insn.  */\n+void\n+sched_analyze (struct deps *deps, rtx head, rtx tail)\n+{\n+  rtx insn;\n+\n+  if (sched_deps_info->use_cselib)\n+    cselib_init (true);\n+\n+  deps_start_bb (deps, head);\n+\n   for (insn = head;; insn = NEXT_INSN (insn))\n     {\n+\n       if (INSN_P (insn))\n \t{\n \t  /* And initialize deps_lists.  */\n \t  sd_init_insn (insn);\n \t}\n \n-      if (NONJUMP_INSN_P (insn) || JUMP_P (insn))\n-\t{\n-\t  /* Make each JUMP_INSN a scheduling barrier for memory\n-             references.  */\n-\t  if (JUMP_P (insn))\n-\t    {\n-\t      /* Keep the list a reasonable size.  */\n-\t      if (deps->pending_flush_length++ > MAX_PENDING_LIST_LENGTH)\n-\t\tflush_pending_lists (deps, insn, true, true);\n-\t      else\n-\t\tdeps->last_pending_memory_flush\n-\t\t  = alloc_INSN_LIST (insn, deps->last_pending_memory_flush);\n-\t    }\n-\t  sched_analyze_insn (deps, PATTERN (insn), insn);\n-\t}\n-      else if (CALL_P (insn))\n-\t{\n-\t  int i;\n-\n-\t  CANT_MOVE (insn) = 1;\n-\n-\t  if (find_reg_note (insn, REG_SETJMP, NULL))\n-\t    {\n-\t      /* This is setjmp.  Assume that all registers, not just\n-\t\t hard registers, may be clobbered by this call.  */\n-\t      reg_pending_barrier = MOVE_BARRIER;\n-\t    }\n-\t  else\n-\t    {\n-\t      for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n-\t\t/* A call may read and modify global register variables.  */\n-\t\tif (global_regs[i])\n-\t\t  {\n-\t\t    SET_REGNO_REG_SET (reg_pending_sets, i);\n-\t\t    SET_REGNO_REG_SET (reg_pending_uses, i);\n-\t\t  }\n-\t\t/* Other call-clobbered hard regs may be clobbered.\n-\t\t   Since we only have a choice between 'might be clobbered'\n-\t\t   and 'definitely not clobbered', we must include all\n-\t\t   partly call-clobbered registers here.  */\n-\t\telse if (HARD_REGNO_CALL_PART_CLOBBERED (i, reg_raw_mode[i])\n-\t\t\t || TEST_HARD_REG_BIT (regs_invalidated_by_call, i))\n-\t\t  SET_REGNO_REG_SET (reg_pending_clobbers, i);\n-\t\t/* We don't know what set of fixed registers might be used\n-\t\t   by the function, but it is certain that the stack pointer\n-\t\t   is among them, but be conservative.  */\n-\t\telse if (fixed_regs[i])\n-\t\t  SET_REGNO_REG_SET (reg_pending_uses, i);\n-\t\t/* The frame pointer is normally not used by the function\n-\t\t   itself, but by the debugger.  */\n-\t\t/* ??? MIPS o32 is an exception.  It uses the frame pointer\n-\t\t   in the macro expansion of jal but does not represent this\n-\t\t   fact in the call_insn rtl.  */\n-\t\telse if (i == FRAME_POINTER_REGNUM\n-\t\t\t || (i == HARD_FRAME_POINTER_REGNUM\n-\t\t\t     && (! reload_completed || frame_pointer_needed)))\n-\t\t  SET_REGNO_REG_SET (reg_pending_uses, i);\n-\t    }\n-\n-\t  /* For each insn which shouldn't cross a call, add a dependence\n-\t     between that insn and this call insn.  */\n-\t  add_dependence_list_and_free (insn, &deps->sched_before_next_call, 1,\n-\t\t\t\t\tREG_DEP_ANTI);\n-\n-\t  sched_analyze_insn (deps, PATTERN (insn), insn);\n-\n-\t  /* In the absence of interprocedural alias analysis, we must flush\n-\t     all pending reads and writes, and start new dependencies starting\n-\t     from here.  But only flush writes for constant calls (which may\n-\t     be passed a pointer to something we haven't written yet).  */\n-\t  flush_pending_lists (deps, insn, true, \n-\t\t\t       ! RTL_CONST_OR_PURE_CALL_P (insn));\n-\n-\t  /* Remember the last function call for limiting lifetimes.  */\n-\t  free_INSN_LIST_list (&deps->last_function_call);\n-\t  deps->last_function_call = alloc_INSN_LIST (insn, NULL_RTX);\n-\n-\t  /* Before reload, begin a post-call group, so as to keep the\n-\t     lifetimes of hard registers correct.  */\n-\t  if (! reload_completed)\n-\t    deps->in_post_call_group_p = post_call;\n-\t}\n-\n-      /* EH_REGION insn notes can not appear until well after we complete\n-\t scheduling.  */\n-      if (NOTE_P (insn))\n-\tgcc_assert (NOTE_KIND (insn) != NOTE_INSN_EH_REGION_BEG\n-\t\t    && NOTE_KIND (insn) != NOTE_INSN_EH_REGION_END);\n-\n-      if (current_sched_info->use_cselib)\n-\tcselib_process_insn (insn);\n+      deps_analyze_insn (deps, insn);\n \n       if (insn == tail)\n \t{\n-\t  if (current_sched_info->use_cselib)\n+\t  if (sched_deps_info->use_cselib)\n \t    cselib_finish ();\n \t  return;\n \t}\n@@ -2441,6 +2873,8 @@ init_deps (struct deps *deps)\n   deps->last_function_call = 0;\n   deps->sched_before_next_call = 0;\n   deps->in_post_call_group_p = not_post_call;\n+  deps->last_reg_pending_barrier = NOT_A_BARRIER;\n+  deps->readonly = 0;\n }\n \n /* Free insn lists found in DEPS.  */\n@@ -2474,42 +2908,98 @@ free_deps (struct deps *deps)\n   CLEAR_REG_SET (&deps->reg_conditional_sets);\n \n   free (deps->reg_last);\n+  deps->reg_last = NULL;\n+\n+  deps = NULL;\n }\n \n-/* If it is profitable to use them, initialize caches for tracking\n-   dependency information.  LUID is the number of insns to be scheduled,\n-   it is used in the estimate of profitability.  */\n+/* Remove INSN from dependence contexts DEPS.  Caution: reg_conditional_sets\n+   is not handled.  */\n+void\n+remove_from_deps (struct deps *deps, rtx insn)\n+{\n+  int removed;\n+  unsigned i;\n+  reg_set_iterator rsi;\n+  \n+  removed = remove_from_both_dependence_lists (insn, &deps->pending_read_insns,\n+                                               &deps->pending_read_mems);\n+  deps->pending_read_list_length -= removed;\n+  removed = remove_from_both_dependence_lists (insn, &deps->pending_write_insns,\n+                                               &deps->pending_write_mems);\n+  deps->pending_write_list_length -= removed;\n+  removed = remove_from_dependence_list (insn, &deps->last_pending_memory_flush);\n+  deps->pending_flush_length -= removed;\n \n+  EXECUTE_IF_SET_IN_REG_SET (&deps->reg_last_in_use, 0, i, rsi)\n+    {\n+      struct deps_reg *reg_last = &deps->reg_last[i];\n+      if (reg_last->uses)\n+\tremove_from_dependence_list (insn, &reg_last->uses);\n+      if (reg_last->sets)\n+\tremove_from_dependence_list (insn, &reg_last->sets);\n+      if (reg_last->clobbers)\n+\tremove_from_dependence_list (insn, &reg_last->clobbers);\n+      if (!reg_last->uses && !reg_last->sets && !reg_last->clobbers)\n+        CLEAR_REGNO_REG_SET (&deps->reg_last_in_use, i);\n+    }\n+\n+  if (CALL_P (insn))\n+    remove_from_dependence_list (insn, &deps->last_function_call);\n+  remove_from_dependence_list (insn, &deps->sched_before_next_call);\n+}\n+\n+/* Init deps data vector.  */\n+static void\n+init_deps_data_vector (void)\n+{\n+  int reserve = (sched_max_luid + 1 \n+                 - VEC_length (haifa_deps_insn_data_def, h_d_i_d));\n+  if (reserve > 0 \n+      && ! VEC_space (haifa_deps_insn_data_def, h_d_i_d, reserve))\n+    VEC_safe_grow_cleared (haifa_deps_insn_data_def, heap, h_d_i_d,\n+                           3 * sched_max_luid / 2);\n+}\n+\n+/* If it is profitable to use them, initialize or extend (depending on\n+   GLOBAL_P) dependency data.  */\n void\n-init_dependency_caches (int luid)\n+sched_deps_init (bool global_p)\n {\n   /* Average number of insns in the basic block.\n      '+ 1' is used to make it nonzero.  */\n-  int insns_in_block = luid / n_basic_blocks + 1;\n+  int insns_in_block = sched_max_luid / n_basic_blocks + 1;\n \n-  /* ?!? We could save some memory by computing a per-region luid mapping\n-     which could reduce both the number of vectors in the cache and the size\n-     of each vector.  Instead we just avoid the cache entirely unless the\n-     average number of instructions in a basic block is very high.  See\n-     the comment before the declaration of true_dependency_cache for\n-     what we consider \"very high\".  */\n-  if (insns_in_block > 100 * 5)\n-    {\n+  init_deps_data_vector ();\n+  \n+  /* We use another caching mechanism for selective scheduling, so \n+     we don't use this one.  */\n+  if (!sel_sched_p () && global_p && insns_in_block > 100 * 5)\n+    {\n+      /* ?!? We could save some memory by computing a per-region luid mapping\n+         which could reduce both the number of vectors in the cache and the\n+         size of each vector.  Instead we just avoid the cache entirely unless\n+         the average number of instructions in a basic block is very high.  See\n+         the comment before the declaration of true_dependency_cache for\n+         what we consider \"very high\".  */\n       cache_size = 0;\n-      extend_dependency_caches (luid, true);\n+      extend_dependency_caches (sched_max_luid, true);\n     }\n \n-  dl_pool = create_alloc_pool (\"deps_list\", sizeof (struct _deps_list),\n-\t\t\t       /* Allocate lists for one block at a time.  */\n-\t\t\t       insns_in_block);\n-\n-  dn_pool = create_alloc_pool (\"dep_node\", sizeof (struct _dep_node),\n-\t\t\t       /* Allocate nodes for one block at a time.\n-\t\t\t\t  We assume that average insn has\n-\t\t\t\t  5 producers.  */\n-\t\t\t       5 * insns_in_block);\n+  if (global_p) \n+    {\n+      dl_pool = create_alloc_pool (\"deps_list\", sizeof (struct _deps_list),\n+                                   /* Allocate lists for one block at a time.  */\n+                                   insns_in_block);\n+      dn_pool = create_alloc_pool (\"dep_node\", sizeof (struct _dep_node),\n+                                   /* Allocate nodes for one block at a time.\n+                                      We assume that average insn has\n+                                      5 producers.  */\n+                                   5 * insns_in_block);\n+    }\n }\n \n+\n /* Create or extend (depending on CREATE_P) dependency caches to\n    size N.  */\n void\n@@ -2543,16 +3033,18 @@ extend_dependency_caches (int n, bool create_p)\n     }\n }\n \n-/* Free the caches allocated in init_dependency_caches.  */\n-\n+/* Finalize dependency information for the whole function.  */\n void\n-free_dependency_caches (void)\n+sched_deps_finish (void)\n {\n   gcc_assert (deps_pools_are_empty_p ());\n   free_alloc_pool_if_empty (&dn_pool);\n   free_alloc_pool_if_empty (&dl_pool);\n   gcc_assert (dn_pool == NULL && dl_pool == NULL);\n \n+  VEC_free (haifa_deps_insn_data_def, heap, h_d_i_d);\n+  cache_size = 0;\n+  \n   if (true_dependency_cache)\n     {\n       int i;\n@@ -2563,7 +3055,7 @@ free_dependency_caches (void)\n \t  bitmap_clear (&output_dependency_cache[i]);\n \t  bitmap_clear (&anti_dependency_cache[i]);\n \n-          if (current_sched_info->flags & DO_SPECULATION)\n+          if (sched_deps_info->generate_spec_deps)\n             bitmap_clear (&spec_dependency_cache[i]);\n \t}\n       free (true_dependency_cache);\n@@ -2573,11 +3065,12 @@ free_dependency_caches (void)\n       free (anti_dependency_cache);\n       anti_dependency_cache = NULL;\n \n-      if (current_sched_info->flags & DO_SPECULATION)\n+      if (sched_deps_info->generate_spec_deps)\n         {\n           free (spec_dependency_cache);\n           spec_dependency_cache = NULL;\n         }\n+\n     }\n }\n \n@@ -2591,6 +3084,19 @@ init_deps_global (void)\n   reg_pending_clobbers = ALLOC_REG_SET (&reg_obstack);\n   reg_pending_uses = ALLOC_REG_SET (&reg_obstack);\n   reg_pending_barrier = NOT_A_BARRIER;\n+\n+  if (!sel_sched_p () || sched_emulate_haifa_p)\n+    {\n+      sched_deps_info->start_insn = haifa_start_insn;\n+      sched_deps_info->finish_insn = haifa_finish_insn;\n+\n+      sched_deps_info->note_reg_set = haifa_note_reg_set;\n+      sched_deps_info->note_reg_clobber = haifa_note_reg_clobber;\n+      sched_deps_info->note_reg_use = haifa_note_reg_use;\n+\n+      sched_deps_info->note_mem_dep = haifa_note_mem_dep;\n+      sched_deps_info->note_dep = haifa_note_dep;\n+   }\n }\n \n /* Free everything used by the dependency analysis code.  */\n@@ -2604,7 +3110,7 @@ finish_deps_global (void)\n }\n \n /* Estimate the weakness of dependence between MEM1 and MEM2.  */\n-static dw_t\n+dw_t\n estimate_dep_weak (rtx mem1, rtx mem2)\n {\n   rtx r1, r2;\n@@ -2637,17 +3143,38 @@ estimate_dep_weak (rtx mem1, rtx mem2)\n void\n add_dependence (rtx insn, rtx elem, enum reg_note dep_type)\n {\n-  dep_def _dep, *dep = &_dep;\n+  ds_t ds;\n+  bool internal;\n \n-  init_dep (dep, elem, insn, dep_type);\n-  maybe_add_or_update_dep_1 (dep, false, NULL_RTX, NULL_RTX);\n+  if (dep_type == REG_DEP_TRUE)\n+    ds = DEP_TRUE;\n+  else if (dep_type == REG_DEP_OUTPUT)\n+    ds = DEP_OUTPUT;\n+  else\n+    {\n+      gcc_assert (dep_type == REG_DEP_ANTI);\n+      ds = DEP_ANTI;\n+    }\n+\n+  /* When add_dependence is called from inside sched-deps.c, we expect\n+     cur_insn to be non-null.  */\n+  internal = cur_insn != NULL;\n+  if (internal)\n+    gcc_assert (insn == cur_insn);\n+  else\n+    cur_insn = insn;\n+      \n+  note_dep (elem, ds);\n+  if (!internal)\n+    cur_insn = NULL;\n }\n \n /* Return weakness of speculative type TYPE in the dep_status DS.  */\n-static dw_t\n+dw_t\n get_dep_weak_1 (ds_t ds, ds_t type)\n {\n   ds = ds & type;\n+\n   switch (type)\n     {\n     case BEGIN_DATA: ds >>= BEGIN_DATA_BITS_OFFSET; break;\n@@ -2660,14 +3187,12 @@ get_dep_weak_1 (ds_t ds, ds_t type)\n   return (dw_t) ds;\n }\n \n-/* Return weakness of speculative type TYPE in the dep_status DS.  */\n dw_t\n get_dep_weak (ds_t ds, ds_t type)\n {\n   dw_t dw = get_dep_weak_1 (ds, type);\n \n   gcc_assert (MIN_DEP_WEAK <= dw && dw <= MAX_DEP_WEAK);\n-\n   return dw;\n }\n \n@@ -2690,9 +3215,12 @@ set_dep_weak (ds_t ds, ds_t type, dw_t dw)\n   return ds;\n }\n \n-/* Return the join of two dep_statuses DS1 and DS2.  */\n-ds_t\n-ds_merge (ds_t ds1, ds_t ds2)\n+/* Return the join of two dep_statuses DS1 and DS2.\n+   If MAX_P is true then choose the greater probability,\n+   otherwise multiply probabilities.\n+   This function assumes that both DS1 and DS2 contain speculative bits.  */\n+static ds_t\n+ds_merge_1 (ds_t ds1, ds_t ds2, bool max_p)\n {\n   ds_t ds, t;\n \n@@ -2709,12 +3237,24 @@ ds_merge (ds_t ds1, ds_t ds2)\n \tds |= ds2 & t;\n       else if ((ds1 & t) && (ds2 & t))\n \t{\n+\t  dw_t dw1 = get_dep_weak (ds1, t);\n+\t  dw_t dw2 = get_dep_weak (ds2, t);\n \t  ds_t dw;\n \n-\t  dw = ((ds_t) get_dep_weak (ds1, t)) * ((ds_t) get_dep_weak (ds2, t));\n-\t  dw /= MAX_DEP_WEAK;\n-\t  if (dw < MIN_DEP_WEAK)\n-\t    dw = MIN_DEP_WEAK;\n+\t  if (!max_p)\n+\t    {\n+\t      dw = ((ds_t) dw1) * ((ds_t) dw2);\n+\t      dw /= MAX_DEP_WEAK;\n+\t      if (dw < MIN_DEP_WEAK)\n+\t\tdw = MIN_DEP_WEAK;\n+\t    }\n+\t  else\n+\t    {\n+\t      if (dw1 >= dw2)\n+\t\tdw = dw1;\n+\t      else\n+\t\tdw = dw2;\n+\t    }\n \n \t  ds = set_dep_weak (ds, t, (dw_t) dw);\n \t}\n@@ -2728,6 +3268,134 @@ ds_merge (ds_t ds1, ds_t ds2)\n   return ds;\n }\n \n+/* Return the join of two dep_statuses DS1 and DS2.\n+   This function assumes that both DS1 and DS2 contain speculative bits.  */\n+ds_t\n+ds_merge (ds_t ds1, ds_t ds2)\n+{\n+  return ds_merge_1 (ds1, ds2, false);\n+}\n+\n+/* Return the join of two dep_statuses DS1 and DS2.  */\n+ds_t\n+ds_full_merge (ds_t ds, ds_t ds2, rtx mem1, rtx mem2)\n+{\n+  ds_t new_status = ds | ds2;\n+\n+  if (new_status & SPECULATIVE)\n+    {\n+      if ((ds && !(ds & SPECULATIVE))\n+\t  || (ds2 && !(ds2 & SPECULATIVE)))\n+\t/* Then this dep can't be speculative.  */\n+\tnew_status &= ~SPECULATIVE;\n+      else\n+\t{\n+\t  /* Both are speculative.  Merging probabilities.  */\n+\t  if (mem1)\n+\t    {\n+\t      dw_t dw;\n+\n+\t      dw = estimate_dep_weak (mem1, mem2);\n+\t      ds = set_dep_weak (ds, BEGIN_DATA, dw);\n+\t    }\n+\n+\t  if (!ds)\n+\t    new_status = ds2;\n+\t  else if (!ds2)\n+\t    new_status = ds;\n+\t  else\n+\t    new_status = ds_merge (ds2, ds);\n+\t}\n+    }\n+\n+  return new_status;\n+}\n+\n+/* Return the join of DS1 and DS2.  Use maximum instead of multiplying\n+   probabilities.  */\n+ds_t\n+ds_max_merge (ds_t ds1, ds_t ds2)\n+{\n+  if (ds1 == 0 && ds2 == 0)\n+    return 0;\n+\n+  if (ds1 == 0 && ds2 != 0)\n+    return ds2;\n+\n+  if (ds1 != 0 && ds2 == 0)\n+    return ds1;\n+\n+  return ds_merge_1 (ds1, ds2, true);\n+}\n+\n+/* Return the probability of speculation success for the speculation\n+   status DS.  */\n+dw_t\n+ds_weak (ds_t ds)\n+{\n+  ds_t res = 1, dt;\n+  int n = 0;\n+\n+  dt = FIRST_SPEC_TYPE;\n+  do\n+    {\n+      if (ds & dt)\n+\t{\n+\t  res *= (ds_t) get_dep_weak (ds, dt);\n+\t  n++;\n+\t}\n+\n+      if (dt == LAST_SPEC_TYPE)\n+\tbreak;\n+      dt <<= SPEC_TYPE_SHIFT;\n+    }\n+  while (1);\n+\n+  gcc_assert (n);\n+  while (--n)\n+    res /= MAX_DEP_WEAK;\n+\n+  if (res < MIN_DEP_WEAK)\n+    res = MIN_DEP_WEAK;\n+\n+  gcc_assert (res <= MAX_DEP_WEAK);\n+\n+  return (dw_t) res;\n+}\n+\n+/* Return a dep status that contains all speculation types of DS.  */\n+ds_t\n+ds_get_speculation_types (ds_t ds)\n+{\n+  if (ds & BEGIN_DATA)\n+    ds |= BEGIN_DATA;\n+  if (ds & BE_IN_DATA)\n+    ds |= BE_IN_DATA;\n+  if (ds & BEGIN_CONTROL)\n+    ds |= BEGIN_CONTROL;\n+  if (ds & BE_IN_CONTROL)\n+    ds |= BE_IN_CONTROL;\n+\n+  return ds & SPECULATIVE;\n+}\n+\n+/* Return a dep status that contains maximal weakness for each speculation\n+   type present in DS.  */\n+ds_t\n+ds_get_max_dep_weak (ds_t ds)\n+{\n+  if (ds & BEGIN_DATA)\n+    ds = set_dep_weak (ds, BEGIN_DATA, MAX_DEP_WEAK);\n+  if (ds & BE_IN_DATA)\n+    ds = set_dep_weak (ds, BE_IN_DATA, MAX_DEP_WEAK);\n+  if (ds & BEGIN_CONTROL)\n+    ds = set_dep_weak (ds, BEGIN_CONTROL, MAX_DEP_WEAK);\n+  if (ds & BE_IN_CONTROL)\n+    ds = set_dep_weak (ds, BE_IN_CONTROL, MAX_DEP_WEAK);\n+\n+  return ds;\n+}\n+\n /* Dump information about the dependence status S.  */\n static void\n dump_ds (FILE *f, ds_t s)\n@@ -2796,7 +3464,7 @@ check_dep (dep_t dep, bool relaxed_p)\n \n   /* Check that dependence status is set correctly when speculation is not\n      supported.  */\n-  if (!(current_sched_info->flags & DO_SPECULATION))\n+  if (!sched_deps_info->generate_spec_deps)\n     gcc_assert (!(ds & SPECULATIVE));\n   else if (ds & SPECULATIVE)\n     {"}, {"sha": "5c4d2df3acaab540da0fd806c4ae884f0dcaf1f5", "filename": "gcc/sched-ebb.c", "status": "modified", "additions": 72, "deletions": 69, "changes": 141, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsched-ebb.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsched-ebb.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-ebb.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -46,11 +46,11 @@ along with GCC; see the file COPYING3.  If not see\n \f\n #ifdef INSN_SCHEDULING\n \n-/* The number of insns scheduled so far.  */\n-static int sched_n_insns;\n-\n /* The number of insns to be scheduled in total.  */\n-static int n_insns;\n+static int rgn_n_insns;\n+\n+/* The number of insns scheduled so far.  */\n+static int sched_rgn_n_insns;\n \n /* Set of blocks, that already have their dependencies calculated.  */\n static bitmap_head dont_calc_deps;\n@@ -62,25 +62,25 @@ static basic_block last_bb;\n static void init_ready_list (void);\n static void begin_schedule_ready (rtx, rtx);\n static int schedule_more_p (void);\n-static const char *ebb_print_insn (rtx, int);\n+static const char *ebb_print_insn (const_rtx, int);\n static int rank (rtx, rtx);\n-static int contributes_to_priority (rtx, rtx);\n-static void compute_jump_reg_dependencies (rtx, regset, regset, regset);\n+static int ebb_contributes_to_priority (rtx, rtx);\n static basic_block earliest_block_with_similiar_load (basic_block, rtx);\n static void add_deps_for_risky_insns (rtx, rtx);\n static basic_block schedule_ebb (rtx, rtx);\n+static void debug_ebb_dependencies (rtx, rtx);\n \n-static void add_remove_insn (rtx, int);\n-static void add_block1 (basic_block, basic_block);\n+static void ebb_add_remove_insn (rtx, int);\n+static void ebb_add_block (basic_block, basic_block);\n static basic_block advance_target_bb (basic_block, rtx);\n-static void fix_recovery_cfg (int, int, int);\n+static void ebb_fix_recovery_cfg (int, int, int);\n \n /* Return nonzero if there are more insns that should be scheduled.  */\n \n static int\n schedule_more_p (void)\n {\n-  return sched_n_insns < n_insns;\n+  return sched_rgn_n_insns < rgn_n_insns;\n }\n \n /* Print dependency information about ebb between HEAD and TAIL.  */\n@@ -107,7 +107,7 @@ init_ready_list (void)\n   rtx next_tail = current_sched_info->next_tail;\n   rtx insn;\n \n-  sched_n_insns = 0;\n+  sched_rgn_n_insns = 0;\n \n   /* Print debugging information.  */\n   if (sched_verbose >= 5)\n@@ -121,14 +121,14 @@ init_ready_list (void)\n       n++;\n     }\n \n-  gcc_assert (n == n_insns);\n+  gcc_assert (n == rgn_n_insns);\n }\n \n /* INSN is being scheduled after LAST.  Update counters.  */\n static void\n begin_schedule_ready (rtx insn, rtx last)\n {\n-  sched_n_insns++;\n+  sched_rgn_n_insns++;\n \n   if (BLOCK_FOR_INSN (insn) == last_bb\n       /* INSN is a jump in the last block, ...  */\n@@ -184,7 +184,8 @@ begin_schedule_ready (rtx insn, rtx last)\n       current_sched_info->next_tail = NEXT_INSN (BB_END (bb));\n       gcc_assert (current_sched_info->next_tail);\n \n-      add_block (bb, last_bb);\n+      /* Append new basic block to the end of the ebb.  */\n+      sched_init_only_bb (bb, last_bb);\n       gcc_assert (last_bb == bb);\n     }\n }\n@@ -195,11 +196,16 @@ begin_schedule_ready (rtx insn, rtx last)\n    to be formatted so that multiple output lines will line up nicely.  */\n \n static const char *\n-ebb_print_insn (rtx insn, int aligned ATTRIBUTE_UNUSED)\n+ebb_print_insn (const_rtx insn, int aligned ATTRIBUTE_UNUSED)\n {\n   static char tmp[80];\n \n-  sprintf (tmp, \"%4d\", INSN_UID (insn));\n+  /* '+' before insn means it is a new cycle start.  */\n+  if (GET_MODE (insn) == TImode)\n+    sprintf (tmp, \"+ %4d\", INSN_UID (insn));\n+  else\n+    sprintf (tmp, \"  %4d\", INSN_UID (insn));\n+\n   return tmp;\n }\n \n@@ -227,8 +233,8 @@ rank (rtx insn1, rtx insn2)\n    calculations.  */\n \n static int\n-contributes_to_priority (rtx next ATTRIBUTE_UNUSED,\n-\t\t\t rtx insn ATTRIBUTE_UNUSED)\n+ebb_contributes_to_priority (rtx next ATTRIBUTE_UNUSED,\n+                             rtx insn ATTRIBUTE_UNUSED)\n {\n   return 1;\n }\n@@ -238,9 +244,9 @@ contributes_to_priority (rtx next ATTRIBUTE_UNUSED,\n     must be considered as used by this jump in USED and that of\n     registers that must be considered as set in SET.  */\n \n-static void\n-compute_jump_reg_dependencies (rtx insn, regset cond_set, regset used,\n-\t\t\t       regset set)\n+void\n+ebb_compute_jump_reg_dependencies (rtx insn, regset cond_set, regset used,\n+\t\t\t\t   regset set)\n {\n   basic_block b = BLOCK_FOR_INSN (insn);\n   edge e;\n@@ -261,26 +267,33 @@ compute_jump_reg_dependencies (rtx insn, regset cond_set, regset used,\n /* Used in schedule_insns to initialize current_sched_info for scheduling\n    regions (or single basic blocks).  */\n \n-static struct sched_info ebb_sched_info =\n+static struct common_sched_info_def ebb_common_sched_info;\n+\n+static struct sched_deps_info_def ebb_sched_deps_info =\n+  {\n+    ebb_compute_jump_reg_dependencies,\n+    NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,\n+    NULL,\n+    1, 0, 0\n+  };\n+\n+static struct haifa_sched_info ebb_sched_info =\n {\n   init_ready_list,\n   NULL,\n   schedule_more_p,\n   NULL,\n   rank,\n   ebb_print_insn,\n-  contributes_to_priority,\n-  compute_jump_reg_dependencies,\n+  ebb_contributes_to_priority,\n \n   NULL, NULL,\n   NULL, NULL,\n-  0, 1, 0,\n+  1, 0,\n \n-  add_remove_insn,\n+  ebb_add_remove_insn,\n   begin_schedule_ready,\n-  add_block1,\n   advance_target_bb,\n-  fix_recovery_cfg,\n   SCHED_EBB\n   /* We can create new blocks in begin_schedule_ready ().  */\n   | NEW_BBS\n@@ -482,44 +495,29 @@ schedule_ebb (rtx head, rtx tail)\n \n   /* Set priorities.  */\n   current_sched_info->sched_max_insns_priority = 0;\n-  n_insns = set_priorities (head, tail);\n+  rgn_n_insns = set_priorities (head, tail);\n   current_sched_info->sched_max_insns_priority++;\n \n   current_sched_info->prev_head = PREV_INSN (head);\n   current_sched_info->next_tail = NEXT_INSN (tail);\n \n-  /* rm_other_notes only removes notes which are _inside_ the\n-     block---that is, it won't remove notes before the first real insn\n-     or after the last real insn of the block.  So if the first insn\n-     has a REG_SAVE_NOTE which would otherwise be emitted before the\n-     insn, it is redundant with the note before the start of the\n-     block, and so we have to take it out.  */\n-  if (INSN_P (head))\n-    {\n-      rtx note;\n-\n-      for (note = REG_NOTES (head); note; note = XEXP (note, 1))\n-\tif (REG_NOTE_KIND (note) == REG_SAVE_NOTE)\n-\t  remove_note (head, note);\n-    }\n-\n-  /* Remove remaining note insns from the block, save them in\n-     note_list.  These notes are restored at the end of\n-     schedule_block ().  */\n-  rm_other_notes (head, tail);\n+  remove_notes (head, tail);\n \n   unlink_bb_notes (first_bb, last_bb);\n \n-  current_sched_info->queue_must_finish_empty = 1;\n-\n   target_bb = first_bb;\n-  schedule_block (&target_bb, n_insns);\n+\n+  /* Make ready list big enough to hold all the instructions from the ebb.  */\n+  sched_extend_ready_list (rgn_n_insns);\n+  schedule_block (&target_bb);\n+  /* Free ready list.  */\n+  sched_finish_ready_list ();\n \n   /* We might pack all instructions into fewer blocks,\n      so we may made some of them empty.  Can't assert (b == last_bb).  */\n   \n   /* Sanity check: verify that all region insns were scheduled.  */\n-  gcc_assert (sched_n_insns == n_insns);\n+  gcc_assert (sched_rgn_n_insns == rgn_n_insns);\n \n   /* Free dependencies.  */\n   sched_free_deps (current_sched_info->head, current_sched_info->tail, true);\n@@ -559,16 +557,21 @@ schedule_ebbs (void)\n   if (n_basic_blocks == NUM_FIXED_BLOCKS)\n     return;\n \n-  /* We need current_sched_info in init_dependency_caches, which is\n-     invoked via sched_init.  */\n-  current_sched_info = &ebb_sched_info;\n+  /* Setup infos.  */\n+  {\n+    memcpy (&ebb_common_sched_info, &haifa_common_sched_info,\n+\t    sizeof (ebb_common_sched_info));\n+\n+    ebb_common_sched_info.fix_recovery_cfg = ebb_fix_recovery_cfg;\n+    ebb_common_sched_info.add_block = ebb_add_block;\n+    ebb_common_sched_info.sched_pass_id = SCHED_EBB_PASS;\n+\n+    common_sched_info = &ebb_common_sched_info;\n+    sched_deps_info = &ebb_sched_deps_info;\n+    current_sched_info = &ebb_sched_info;\n+  }\n \n-  df_set_flags (DF_LR_RUN_DCE);\n-  df_note_add_problem ();\n-  df_analyze ();\n-  df_clear_flags (DF_LR_RUN_DCE);\n-  regstat_compute_calls_crossed ();\n-  sched_init ();\n+  haifa_sched_init ();\n \n   compute_bb_for_insn ();\n \n@@ -622,23 +625,22 @@ schedule_ebbs (void)\n   if (reload_completed)\n     reposition_prologue_and_epilogue_notes ();\n \n-  sched_finish ();\n-  regstat_free_calls_crossed ();\n+  haifa_sched_finish ();\n }\n \n /* INSN has been added to/removed from current ebb.  */\n static void\n-add_remove_insn (rtx insn ATTRIBUTE_UNUSED, int remove_p)\n+ebb_add_remove_insn (rtx insn ATTRIBUTE_UNUSED, int remove_p)\n {\n   if (!remove_p)\n-    n_insns++;\n+    rgn_n_insns++;\n   else\n-    n_insns--;\n+    rgn_n_insns--;\n }\n \n /* BB was added to ebb after AFTER.  */\n static void\n-add_block1 (basic_block bb, basic_block after)\n+ebb_add_block (basic_block bb, basic_block after)\n {\n   /* Recovery blocks are always bounded by BARRIERS, \n      therefore, they always form single block EBB,\n@@ -691,7 +693,8 @@ advance_target_bb (basic_block bb, rtx insn)\n    For parameter meaning please refer to\n    sched-int.h: struct sched_info: fix_recovery_cfg.  */\n static void\n-fix_recovery_cfg (int bbi ATTRIBUTE_UNUSED, int jump_bbi, int jump_bb_nexti)\n+ebb_fix_recovery_cfg (int bbi ATTRIBUTE_UNUSED, int jump_bbi,\n+\t\t      int jump_bb_nexti)\n {\n   gcc_assert (last_bb->index != bbi);\n "}, {"sha": "2f9b7818eaaf2f8ce11da02ea824c1a972df799a", "filename": "gcc/sched-int.h", "status": "modified", "additions": 464, "deletions": 83, "changes": 547, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsched-int.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsched-int.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-int.h?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -26,18 +26,196 @@ along with GCC; see the file COPYING3.  If not see\n \n /* For state_t.  */\n #include \"insn-attr.h\"\n-/* For regset_head.  */\n-#include \"basic-block.h\"\n-/* For reg_note.  */\n-#include \"rtl.h\"\n #include \"df.h\"\n+#include \"basic-block.h\"\n+\n+/* For VEC (int, heap).  */\n+#include \"vecprim.h\"\n+\n+/* Identificator of a scheduler pass.  */\n+enum sched_pass_id_t { SCHED_PASS_UNKNOWN, SCHED_RGN_PASS, SCHED_EBB_PASS,\n+\t\t       SCHED_SMS_PASS, SCHED_SEL_PASS };\n+\n+typedef VEC (basic_block, heap) *bb_vec_t;\n+typedef VEC (rtx, heap) *insn_vec_t;\n+typedef VEC(rtx, heap) *rtx_vec_t;\n+\n+struct sched_scan_info_def\n+{\n+  /* This hook notifies scheduler frontend to extend its internal per basic\n+     block data structures.  This hook should be called once before a series of\n+     calls to bb_init ().  */\n+  void (*extend_bb) (void);\n+\n+  /* This hook makes scheduler frontend to initialize its internal data\n+     structures for the passed basic block.  */\n+  void (*init_bb) (basic_block);\n+\n+  /* This hook notifies scheduler frontend to extend its internal per insn data\n+     structures.  This hook should be called once before a series of calls to\n+     insn_init ().  */\n+  void (*extend_insn) (void);\n+\n+  /* This hook makes scheduler frontend to initialize its internal data\n+     structures for the passed insn.  */\n+  void (*init_insn) (rtx);\n+};\n+\n+extern const struct sched_scan_info_def *sched_scan_info;\n+\n+extern void sched_scan (const struct sched_scan_info_def *,\n+\t\t\tbb_vec_t, basic_block, insn_vec_t, rtx);\n+\n+extern void sched_init_bbs (void);\n+\n+extern void sched_init_luids (bb_vec_t, basic_block, insn_vec_t, rtx);\n+extern void sched_finish_luids (void);\n+\n+extern void sched_extend_target (void);\n+\n+extern void haifa_init_h_i_d (bb_vec_t, basic_block, insn_vec_t, rtx);\n+extern void haifa_finish_h_i_d (void);\n+\n+/* Hooks that are common to all the schedulers.  */\n+struct common_sched_info_def\n+{\n+  /* Called after blocks were rearranged due to movement of jump instruction.\n+     The first parameter - index of basic block, in which jump currently is.\n+     The second parameter - index of basic block, in which jump used\n+     to be.\n+     The third parameter - index of basic block, that follows the second\n+     parameter.  */\n+  void (*fix_recovery_cfg) (int, int, int);\n+\n+  /* Called to notify frontend, that new basic block is being added.\n+     The first parameter - new basic block.\n+     The second parameter - block, after which new basic block is being added,\n+     or EXIT_BLOCK_PTR, if recovery block is being added,\n+     or NULL, if standalone block is being added.  */\n+  void (*add_block) (basic_block, basic_block);\n+\n+  /* Estimate number of insns in the basic block.  */\n+  int (*estimate_number_of_insns) (basic_block);\n+\n+  /* Given a non-insn (!INSN_P (x)) return\n+     -1 - if this rtx don't need a luid.\n+     0 - if it should have the same luid as the previous insn.\n+     1 - if it needs a separate luid.  */\n+  int (*luid_for_non_insn) (rtx);\n+\n+  /* Scheduler pass identifier.  It is preferably used in assertions.  */\n+  enum sched_pass_id_t sched_pass_id;\n+};\n+\n+extern struct common_sched_info_def *common_sched_info;\n+\n+extern const struct common_sched_info_def haifa_common_sched_info;\n+\n+/* Return true if selective scheduling pass is working.  */\n+static inline bool\n+sel_sched_p (void)\n+{\n+  return common_sched_info->sched_pass_id == SCHED_SEL_PASS;\n+}\n+\n+/* Returns maximum priority that an insn was assigned to.  */\n+extern int get_rgn_sched_max_insns_priority (void);\n+\n+/* Increases effective priority for INSN by AMOUNT.  */\n+extern void sel_add_to_insn_priority (rtx, int);\n+\n+/* True if during selective scheduling we need to emulate some of haifa\n+   scheduler behaviour.  */\n+extern int sched_emulate_haifa_p;\n+\n+/* Mapping from INSN_UID to INSN_LUID.  In the end all other per insn data\n+   structures should be indexed by luid.  */\n+extern VEC (int, heap) *sched_luids;\n+#define INSN_LUID(INSN) (VEC_index (int, sched_luids, INSN_UID (INSN)))\n+#define LUID_BY_UID(UID) (VEC_index (int, sched_luids, UID))\n+\n+#define SET_INSN_LUID(INSN, LUID) \\\n+(VEC_replace (int, sched_luids, INSN_UID (INSN), (LUID)))\n+\n+/* The highest INSN_LUID.  */\n+extern int sched_max_luid;\n+\n+extern int insn_luid (rtx);\n+\n+/* This list holds ripped off notes from the current block.  These notes will\n+   be attached to the beginning of the block when its scheduling is\n+   finished.  */\n+extern rtx note_list;\n+\n+extern void remove_notes (rtx, rtx);\n+extern rtx restore_other_notes (rtx, basic_block);\n+extern void sched_insns_init (rtx);\n+extern void sched_insns_finish (void);\n+\n+extern void *xrecalloc (void *, size_t, size_t, size_t);\n+extern rtx bb_note (basic_block);\n+\n+extern void reemit_notes (rtx);\n+\n+/* Functions in sched-vis.c.  */\n+extern void print_insn (char *, const_rtx, int);\n+extern void print_pattern (char *, const_rtx, int);\n+extern void print_value (char *, const_rtx, int);\n+\n+/* Functions in haifa-sched.c.  */\n+extern int haifa_classify_insn (const_rtx);\n+\n+/* Functions in sel-sched-ir.c.  */\n+extern void sel_find_rgns (void);\n+extern void sel_mark_hard_insn (rtx);\n+\n+extern size_t dfa_state_size;\n+\n+extern void advance_state (state_t);\n+\n+extern void setup_sched_dump (void);\n+extern void sched_init (void);\n+extern void sched_finish (void);\n+\n+extern bool sel_insn_is_speculation_check (rtx);\n+\n+/* Describe the ready list of the scheduler.\n+   VEC holds space enough for all insns in the current region.  VECLEN\n+   says how many exactly.\n+   FIRST is the index of the element with the highest priority; i.e. the\n+   last one in the ready list, since elements are ordered by ascending\n+   priority.\n+   N_READY determines how many insns are on the ready list.  */\n+struct ready_list\n+{\n+  rtx *vec;\n+  int veclen;\n+  int first;\n+  int n_ready;\n+};\n+\n+extern char *ready_try;\n+extern struct ready_list ready;\n+\n+extern int max_issue (struct ready_list *, int, state_t, int *);\n+\n+extern void ebb_compute_jump_reg_dependencies (rtx, regset, regset, regset);\n+\n+extern edge find_fallthru_edge (basic_block);\n+\n+extern void (* sched_init_only_bb) (basic_block, basic_block);\n+extern basic_block (* sched_split_block) (basic_block, rtx);\n+extern basic_block sched_split_block_1 (basic_block, rtx);\n+extern basic_block (* sched_create_empty_bb) (basic_block);\n+extern basic_block sched_create_empty_bb_1 (basic_block);\n+\n+extern basic_block sched_create_recovery_block (basic_block *);\n+extern void sched_create_recovery_edges (basic_block, basic_block,\n+\t\t\t\t\t basic_block);\n \n /* Pointer to data describing the current DFA state.  */\n extern state_t curr_state;\n \n-/* Forward declaration.  */\n-struct ready_list;\n-\n /* Type to represent status of a dependence.  */\n typedef int ds_t;\n \n@@ -242,6 +420,16 @@ struct _dep_node\n #define DEP_NODE_DEP(N) (&(N)->dep)\n #define DEP_NODE_FORW(N) (&(N)->forw)\n \n+/* The following enumeration values tell us what dependencies we\n+   should use to implement the barrier.  We use true-dependencies for\n+   TRUE_BARRIER and anti-dependencies for MOVE_BARRIER.  */\n+enum reg_pending_barrier_mode\n+{\n+  NOT_A_BARRIER = 0,\n+  MOVE_BARRIER,\n+  TRUE_BARRIER\n+};\n+\n /* Describe state of dependencies used during sched_analyze phase.  */\n struct deps\n {\n@@ -330,14 +518,23 @@ struct deps\n \n   /* Element N is set for each register that is conditionally set.  */\n   regset_head reg_conditional_sets;\n+\n+  /* Shows the last value of reg_pending_barrier associated with the insn.  */\n+  enum reg_pending_barrier_mode last_reg_pending_barrier;\n+\n+  /* True when this context should be treated as a readonly by \n+     the analysis.  */\n+  BOOL_BITFIELD readonly : 1;\n };\n \n+typedef struct deps *deps_t;\n+\n /* This structure holds some state of the current scheduling pass, and\n    contains some function pointers that abstract out some of the non-generic\n    functionality from functions such as schedule_block or schedule_insn.\n    There is one global variable, current_sched_info, which points to the\n    sched_info structure currently in use.  */\n-struct sched_info\n+struct haifa_sched_info\n {\n   /* Add all insns that are initially ready to the ready list.  Called once\n      before scheduling a set of insns.  */\n@@ -361,14 +558,10 @@ struct sched_info\n      necessary to identify this insn in an output.  It's valid to use a\n      static buffer for this.  The ALIGNED parameter should cause the string\n      to be formatted so that multiple output lines will line up nicely.  */\n-  const char *(*print_insn) (rtx, int);\n+  const char *(*print_insn) (const_rtx, int);\n   /* Return nonzero if an insn should be included in priority\n      calculations.  */\n   int (*contributes_to_priority) (rtx, rtx);\n-  /* Called when computing dependencies for a JUMP_INSN.  This function\n-     should store the set of registers that must be considered as set by\n-     the jump in the regset.  */\n-  void (*compute_jump_reg_dependencies) (rtx, regset, regset, regset);\n \n   /* The boundaries of the set of insns to be scheduled.  */\n   rtx prev_head, next_tail;\n@@ -379,11 +572,6 @@ struct sched_info\n \n   /* If nonzero, enables an additional sanity check in schedule_block.  */\n   unsigned int queue_must_finish_empty:1;\n-  /* Nonzero if we should use cselib for better alias analysis.  This\n-     must be 0 if the dependency information is used after sched_analyze\n-     has completed, e.g. if we're using it to initialize state for successor\n-     blocks in region scheduling.  */\n-  unsigned int use_cselib:1;\n \n   /* Maximum priority that has been assigned to an insn.  */\n   int sched_max_insns_priority;\n@@ -399,27 +587,12 @@ struct sched_info\n      last scheduled instruction.  */\n   void (*begin_schedule_ready) (rtx, rtx);\n \n-  /* Called to notify frontend, that new basic block is being added.\n-     The first parameter - new basic block.\n-     The second parameter - block, after which new basic block is being added,\n-     or EXIT_BLOCK_PTR, if recovery block is being added,\n-     or NULL, if standalone block is being added.  */\n-  void (*add_block) (basic_block, basic_block);\n-\n   /* If the second parameter is not NULL, return nonnull value, if the\n      basic block should be advanced.\n      If the second parameter is NULL, return the next basic block in EBB.\n      The first parameter is the current basic block in EBB.  */\n   basic_block (*advance_target_bb) (basic_block, rtx);\n \n-  /* Called after blocks were rearranged due to movement of jump instruction.\n-     The first parameter - index of basic block, in which jump currently is.\n-     The second parameter - index of basic block, in which jump used\n-     to be.\n-     The third parameter - index of basic block, that follows the second\n-     parameter.  */\n-  void (*fix_recovery_cfg) (int, int, int);\n-\n   /* ??? FIXME: should use straight bitfields inside sched_info instead of\n      this flag field.  */\n   unsigned int flags;\n@@ -438,23 +611,40 @@ struct spec_info_def\n \n   /* Minimal cumulative weakness of speculative instruction's\n      dependencies, so that insn will be scheduled.  */\n-  dw_t weakness_cutoff;\n+  dw_t data_weakness_cutoff;\n+\n+  /* Minimal usefulness of speculative instruction to be considered for\n+     scheduling.  */\n+  int control_weakness_cutoff;\n \n   /* Flags from the enum SPEC_SCHED_FLAGS.  */\n   int flags;\n };\n typedef struct spec_info_def *spec_info_t;\n \n-extern struct sched_info *current_sched_info;\n+extern spec_info_t spec_info;\n+\n+extern struct haifa_sched_info *current_sched_info;\n \n /* Indexed by INSN_UID, the collection of all data associated with\n    a single instruction.  */\n \n-struct haifa_insn_data\n+struct _haifa_deps_insn_data\n {\n-  /* We can't place 'struct _deps_list' into h_i_d instead of deps_list_t\n-     because when h_i_d extends, addresses of the deps_list->first\n-     change without updating deps_list->first->next->prev_nextp.  */\n+  /* The number of incoming edges in the forward dependency graph.\n+     As scheduling proceeds, counts are decreased.  An insn moves to\n+     the ready queue when its counter reaches zero.  */\n+  int dep_count;\n+\n+  /* Nonzero if instruction has internal dependence\n+     (e.g. add_dependence was invoked with (insn == elem)).  */\n+  unsigned int has_internal_dep;\n+\n+  /* NB: We can't place 'struct _deps_list' here instead of deps_list_t into\n+     h_i_d because when h_i_d extends, addresses of the deps_list->first\n+     change without updating deps_list->first->next->prev_nextp.  Thus\n+     BACK_DEPS and RESOLVED_BACK_DEPS are allocated on the heap and FORW_DEPS\n+     list is allocated on the obstack.  */\n \n   /* A list of hard backward dependencies.  The insn is a consumer of all the\n      deps mentioned here.  */\n@@ -476,7 +666,17 @@ struct haifa_insn_data\n      from 'forw_deps' to 'resolved_forw_deps' while scheduling to fasten the\n      search in 'forw_deps'.  */\n   deps_list_t resolved_forw_deps;\n- \n+\n+  /* Some insns (e.g. call) are not allowed to move across blocks.  */\n+  unsigned int cant_move : 1;\n+};\n+\n+struct _haifa_insn_data\n+{\n+  /* We can't place 'struct _deps_list' into h_i_d instead of deps_list_t\n+     because when h_i_d extends, addresses of the deps_list->first\n+     change without updating deps_list->first->next->prev_nextp.  */\n+\n   /* Logical uid gives the original ordering of the insns.  */\n   int luid;\n \n@@ -503,9 +703,6 @@ struct haifa_insn_data\n      register pressure.  */\n   short reg_weight;\n \n-  /* Some insns (e.g. call) are not allowed to move across blocks.  */\n-  unsigned int cant_move : 1;\n-\n   /* Set if there's DEF-USE dependence between some speculatively\n      moved load insn and this one.  */\n   unsigned int fed_by_spec_load : 1;\n@@ -516,14 +713,12 @@ struct haifa_insn_data\n      '< 0' if priority in invalid and should be recomputed.  */\n   signed char priority_status;\n \n-  /* Nonzero if instruction has internal dependence\n-     (e.g. add_dependence was invoked with (insn == elem)).  */\n-  unsigned int has_internal_dep : 1;\n-  \n   /* What speculations are necessary to apply to schedule the instruction.  */\n   ds_t todo_spec;\n+\n   /* What speculations were already applied.  */\n   ds_t done_spec; \n+\n   /* What speculations are checked by this instruction.  */\n   ds_t check_spec;\n \n@@ -534,33 +729,56 @@ struct haifa_insn_data\n   rtx orig_pat;\n };\n \n-extern struct haifa_insn_data *h_i_d;\n+typedef struct _haifa_insn_data haifa_insn_data_def;\n+typedef haifa_insn_data_def *haifa_insn_data_t;\n+\n+DEF_VEC_O (haifa_insn_data_def);\n+DEF_VEC_ALLOC_O (haifa_insn_data_def, heap);\n+\n+extern VEC(haifa_insn_data_def, heap) *h_i_d;\n+\n+#define HID(INSN) (VEC_index (haifa_insn_data_def, h_i_d, INSN_UID (INSN)))\n \n /* Accessor macros for h_i_d.  There are more in haifa-sched.c and\n    sched-rgn.c.  */\n-\n-#define INSN_HARD_BACK_DEPS(INSN) (h_i_d[INSN_UID (INSN)].hard_back_deps)\n-#define INSN_SPEC_BACK_DEPS(INSN) (h_i_d[INSN_UID (INSN)].spec_back_deps)\n-#define INSN_FORW_DEPS(INSN) (h_i_d[INSN_UID (INSN)].forw_deps)\n-#define INSN_RESOLVED_BACK_DEPS(INSN) \\\n-  (h_i_d[INSN_UID (INSN)].resolved_back_deps)\n-#define INSN_RESOLVED_FORW_DEPS(INSN) \\\n-  (h_i_d[INSN_UID (INSN)].resolved_forw_deps)\n-#define INSN_LUID(INSN)\t\t(h_i_d[INSN_UID (INSN)].luid)\n-#define CANT_MOVE(insn)\t\t(h_i_d[INSN_UID (insn)].cant_move)\n-#define INSN_PRIORITY(INSN)\t(h_i_d[INSN_UID (INSN)].priority)\n-#define INSN_PRIORITY_STATUS(INSN) (h_i_d[INSN_UID (INSN)].priority_status)\n+#define INSN_PRIORITY(INSN) (HID (INSN)->priority)\n+#define INSN_REG_WEIGHT(INSN) (HID (INSN)->reg_weight)\n+#define INSN_PRIORITY_STATUS(INSN) (HID (INSN)->priority_status)\n+\n+typedef struct _haifa_deps_insn_data haifa_deps_insn_data_def;\n+typedef haifa_deps_insn_data_def *haifa_deps_insn_data_t;\n+\n+DEF_VEC_O (haifa_deps_insn_data_def);\n+DEF_VEC_ALLOC_O (haifa_deps_insn_data_def, heap);\n+\n+extern VEC(haifa_deps_insn_data_def, heap) *h_d_i_d;\n+\n+#define HDID(INSN) (VEC_index (haifa_deps_insn_data_def, h_d_i_d,\t\\\n+\t\t\t       INSN_LUID (INSN)))\n+#define INSN_DEP_COUNT(INSN)\t(HDID (INSN)->dep_count)\n+#define HAS_INTERNAL_DEP(INSN)  (HDID (INSN)->has_internal_dep)\n+#define INSN_FORW_DEPS(INSN) (HDID (INSN)->forw_deps)\n+#define INSN_RESOLVED_BACK_DEPS(INSN) (HDID (INSN)->resolved_back_deps)\n+#define INSN_RESOLVED_FORW_DEPS(INSN) (HDID (INSN)->resolved_forw_deps)\n+#define INSN_HARD_BACK_DEPS(INSN) (HDID (INSN)->hard_back_deps)\n+#define INSN_SPEC_BACK_DEPS(INSN) (HDID (INSN)->spec_back_deps)\n+#define CANT_MOVE(INSN)\t(HDID (INSN)->cant_move)\n+#define CANT_MOVE_BY_LUID(LUID)\t(VEC_index (haifa_deps_insn_data_def, h_d_i_d, \\\n+                                            LUID)->cant_move)\n+\n+\n+#define INSN_PRIORITY(INSN)\t(HID (INSN)->priority)\n+#define INSN_PRIORITY_STATUS(INSN) (HID (INSN)->priority_status)\n #define INSN_PRIORITY_KNOWN(INSN) (INSN_PRIORITY_STATUS (INSN) > 0)\n-#define INSN_REG_WEIGHT(INSN)\t(h_i_d[INSN_UID (INSN)].reg_weight)\n-#define HAS_INTERNAL_DEP(INSN)  (h_i_d[INSN_UID (INSN)].has_internal_dep)\n-#define TODO_SPEC(INSN)         (h_i_d[INSN_UID (INSN)].todo_spec)\n-#define DONE_SPEC(INSN)         (h_i_d[INSN_UID (INSN)].done_spec)\n-#define CHECK_SPEC(INSN)        (h_i_d[INSN_UID (INSN)].check_spec)\n-#define RECOVERY_BLOCK(INSN)    (h_i_d[INSN_UID (INSN)].recovery_block)\n-#define ORIG_PAT(INSN)          (h_i_d[INSN_UID (INSN)].orig_pat)\n+#define TODO_SPEC(INSN) (HID (INSN)->todo_spec)\n+#define DONE_SPEC(INSN) (HID (INSN)->done_spec)\n+#define CHECK_SPEC(INSN) (HID (INSN)->check_spec)\n+#define RECOVERY_BLOCK(INSN) (HID (INSN)->recovery_block)\n+#define ORIG_PAT(INSN) (HID (INSN)->orig_pat)\n \n /* INSN is either a simple or a branchy speculation check.  */\n-#define IS_SPECULATION_CHECK_P(INSN) (RECOVERY_BLOCK (INSN) != NULL)\n+#define IS_SPECULATION_CHECK_P(INSN) \\\n+  (sel_sched_p () ? sel_insn_is_speculation_check (INSN) : RECOVERY_BLOCK (INSN) != NULL)\n \n /* INSN is a speculation check that will simply reexecute the speculatively\n    scheduled instruction if the speculation fails.  */\n@@ -706,14 +924,16 @@ enum SCHED_FLAGS {\n   DO_SPECULATION = USE_DEPS_LIST << 1,\n   SCHED_RGN = DO_SPECULATION << 1,\n   SCHED_EBB = SCHED_RGN << 1,\n-  /* Scheduler can possible create new basic blocks.  Used for assertions.  */\n-  NEW_BBS = SCHED_EBB << 1\n+  /* Scheduler can possibly create new basic blocks.  Used for assertions.  */\n+  NEW_BBS = SCHED_EBB << 1,\n+  SEL_SCHED = NEW_BBS << 1\n };\n \n enum SPEC_SCHED_FLAGS {\n   COUNT_SPEC_IN_CRITICAL_PATH = 1,\n   PREFER_NON_DATA_SPEC = COUNT_SPEC_IN_CRITICAL_PATH << 1,\n-  PREFER_NON_CONTROL_SPEC = PREFER_NON_DATA_SPEC << 1\n+  PREFER_NON_CONTROL_SPEC = PREFER_NON_DATA_SPEC << 1,\n+  SEL_SCHED_SPEC_DONT_CHECK_CONTROL = PREFER_NON_CONTROL_SPEC << 1\n };\n \n #define NOTE_NOT_BB_P(NOTE) (NOTE_P (NOTE) && (NOTE_KIND (NOTE)\t\\\n@@ -803,49 +1023,210 @@ enum INSN_TRAP_CLASS\n #define HAIFA_INLINE __inline\n #endif\n \n+struct sched_deps_info_def\n+{\n+  /* Called when computing dependencies for a JUMP_INSN.  This function\n+     should store the set of registers that must be considered as set by\n+     the jump in the regset.  */\n+  void (*compute_jump_reg_dependencies) (rtx, regset, regset, regset);\n+\n+  /* Start analyzing insn.  */\n+  void (*start_insn) (rtx);\n+\n+  /* Finish analyzing insn.  */\n+  void (*finish_insn) (void);\n+\n+  /* Start analyzing insn LHS (Left Hand Side).  */\n+  void (*start_lhs) (rtx);\n+\n+  /* Finish analyzing insn LHS.  */\n+  void (*finish_lhs) (void);\n+\n+  /* Start analyzing insn RHS (Right Hand Side).  */\n+  void (*start_rhs) (rtx);\n+\n+  /* Finish analyzing insn RHS.  */\n+  void (*finish_rhs) (void);\n+\n+  /* Note set of the register.  */\n+  void (*note_reg_set) (int);\n+\n+  /* Note clobber of the register.  */\n+  void (*note_reg_clobber) (int);\n+\n+  /* Note use of the register.  */\n+  void (*note_reg_use) (int);\n+\n+  /* Note memory dependence of type DS between MEM1 and MEM2 (which is\n+     in the INSN2).  */\n+  void (*note_mem_dep) (rtx mem1, rtx mem2, rtx insn2, ds_t ds);\n+\n+  /* Note a dependence of type DS from the INSN.  */\n+  void (*note_dep) (rtx insn, ds_t ds);\n+\n+  /* Nonzero if we should use cselib for better alias analysis.  This\n+     must be 0 if the dependency information is used after sched_analyze\n+     has completed, e.g. if we're using it to initialize state for successor\n+     blocks in region scheduling.  */\n+  unsigned int use_cselib : 1;\n+\n+  /* If set, generate links between instruction as DEPS_LIST.\n+     Otherwise, generate usual INSN_LIST links.  */\n+  unsigned int use_deps_list : 1;\n+\n+  /* Generate data and control speculative dependencies.\n+     Requires USE_DEPS_LIST set.  */\n+  unsigned int generate_spec_deps : 1;\n+};\n+\n+extern struct sched_deps_info_def *sched_deps_info;\n+\n+\n /* Functions in sched-deps.c.  */\n extern bool sched_insns_conditions_mutex_p (const_rtx, const_rtx);\n extern bool sched_insn_is_legitimate_for_speculation_p (const_rtx, ds_t);\n extern void add_dependence (rtx, rtx, enum reg_note);\n extern void sched_analyze (struct deps *, rtx, rtx);\n-extern bool deps_pools_are_empty_p (void);\n-extern void sched_free_deps (rtx, rtx, bool);\n extern void init_deps (struct deps *);\n extern void free_deps (struct deps *);\n extern void init_deps_global (void);\n extern void finish_deps_global (void);\n-extern void init_dependency_caches (int);\n-extern void free_dependency_caches (void);\n-extern void extend_dependency_caches (int, bool);\n+extern void deps_analyze_insn (struct deps *, rtx);\n+extern void remove_from_deps (struct deps *, rtx);\n+\n+extern dw_t get_dep_weak_1 (ds_t, ds_t);\n extern dw_t get_dep_weak (ds_t, ds_t);\n extern ds_t set_dep_weak (ds_t, ds_t, dw_t);\n+extern dw_t estimate_dep_weak (rtx, rtx);\n extern ds_t ds_merge (ds_t, ds_t);\n+extern ds_t ds_full_merge (ds_t, ds_t, rtx, rtx);\n+extern ds_t ds_max_merge (ds_t, ds_t);\n+extern dw_t ds_weak (ds_t);\n+extern ds_t ds_get_speculation_types (ds_t);\n+extern ds_t ds_get_max_dep_weak (ds_t);\n+\n+extern void sched_deps_init (bool);\n+extern void sched_deps_finish (void);\n+\n+extern void haifa_note_reg_set (int);\n+extern void haifa_note_reg_clobber (int);\n+extern void haifa_note_reg_use (int);\n+\n+extern void maybe_extend_reg_info_p (void);\n+\n+extern void deps_start_bb (struct deps *, rtx);\n+extern enum reg_note ds_to_dt (ds_t);\n+\n+extern bool deps_pools_are_empty_p (void);\n+extern void sched_free_deps (rtx, rtx, bool);\n+extern void extend_dependency_caches (int, bool);\n+\n extern void debug_ds (ds_t);\n \n /* Functions in haifa-sched.c.  */\n extern int haifa_classify_insn (const_rtx);\n extern void get_ebb_head_tail (basic_block, basic_block, rtx *, rtx *);\n extern int no_real_insns_p (const_rtx, const_rtx);\n \n-extern void rm_other_notes (rtx, rtx);\n-\n extern int insn_cost (rtx);\n+extern int dep_cost_1 (dep_t, dw_t);\n extern int dep_cost (dep_t);\n extern int set_priorities (rtx, rtx);\n \n-extern void schedule_block (basic_block *, int);\n-extern void sched_init (void);\n-extern void sched_finish (void);\n+extern void schedule_block (basic_block *);\n+\n+extern int cycle_issued_insns;\n+extern int issue_rate;\n+extern int dfa_lookahead;\n+\n+extern void ready_sort (struct ready_list *);\n+extern rtx ready_element (struct ready_list *, int);\n+extern rtx *ready_lastpos (struct ready_list *);\n \n extern int try_ready (rtx);\n-extern void * xrecalloc (void *, size_t, size_t, size_t);\n+extern void sched_extend_ready_list (int);\n+extern void sched_finish_ready_list (void);\n+extern void sched_change_pattern (rtx, rtx);\n+extern int sched_speculate_insn (rtx, ds_t, rtx *);\n extern void unlink_bb_notes (basic_block, basic_block);\n extern void add_block (basic_block, basic_block);\n extern rtx bb_note (basic_block);\n+extern void concat_note_lists (rtx, rtx *);\n+\f\n \n-/* Functions in sched-rgn.c.  */\n+/* Types and functions in sched-rgn.c.  */\n \n+/* A region is the main entity for interblock scheduling: insns\n+   are allowed to move between blocks in the same region, along\n+   control flow graph edges, in the 'up' direction.  */\n+typedef struct\n+{\n+  /* Number of extended basic blocks in region.  */\n+  int rgn_nr_blocks;\n+  /* cblocks in the region (actually index in rgn_bb_table).  */\n+  int rgn_blocks;\n+  /* Dependencies for this region are already computed.  Basically, indicates,\n+     that this is a recovery block.  */\n+  unsigned int dont_calc_deps : 1;\n+  /* This region has at least one non-trivial ebb.  */\n+  unsigned int has_real_ebb : 1;\n+}\n+region;\n+\n+extern int nr_regions;\n+extern region *rgn_table;\n+extern int *rgn_bb_table;\n+extern int *block_to_bb;\n+extern int *containing_rgn;\n+\n+#define RGN_NR_BLOCKS(rgn) (rgn_table[rgn].rgn_nr_blocks)\n+#define RGN_BLOCKS(rgn) (rgn_table[rgn].rgn_blocks)\n+#define RGN_DONT_CALC_DEPS(rgn) (rgn_table[rgn].dont_calc_deps)\n+#define RGN_HAS_REAL_EBB(rgn) (rgn_table[rgn].has_real_ebb)\n+#define BLOCK_TO_BB(block) (block_to_bb[block])\n+#define CONTAINING_RGN(block) (containing_rgn[block])\n+\n+/* The mapping from ebb to block.  */\n+extern int *ebb_head;\n+#define BB_TO_BLOCK(ebb) (rgn_bb_table[ebb_head[ebb]])\n+#define EBB_FIRST_BB(ebb) BASIC_BLOCK (BB_TO_BLOCK (ebb))\n+#define EBB_LAST_BB(ebb) BASIC_BLOCK (rgn_bb_table[ebb_head[ebb + 1] - 1])\n+#define INSN_BB(INSN) (BLOCK_TO_BB (BLOCK_NUM (INSN)))\n+\n+extern int current_nr_blocks;\n+extern int current_blocks;\n+extern int target_bb;\n+\n+extern bool sched_is_disabled_for_current_region_p (void);\n+extern void sched_rgn_init (bool);\n+extern void sched_rgn_finish (void);\n+extern void rgn_setup_region (int);\n+extern void sched_rgn_compute_dependencies (int);\n+extern void sched_rgn_local_init (int);\n+extern void sched_rgn_local_finish (void);\n+extern void sched_rgn_local_free (void);\n+extern void extend_regions (void);\n+extern void rgn_make_new_region_out_of_new_block (basic_block);\n+\n+extern void compute_priorities (void);\n+extern void increase_insn_priority (rtx, int);\n+extern void debug_rgn_dependencies (int);\n extern void debug_dependencies (rtx, rtx);\n+extern void free_rgn_deps (void);          \n+extern int contributes_to_priority (rtx, rtx);\n+extern void extend_rgns (int *, int *, sbitmap, int *);\n+extern void deps_join (struct deps *, struct deps *);\n+\n+extern void rgn_setup_common_sched_info (void);\n+extern void rgn_setup_sched_infos (void);\n+\n+extern void debug_regions (void);\n+extern void debug_region (int);\n+extern void dump_region_dot (FILE *, int);\n+extern void dump_region_dot_file (const char *, int);\n+\n+extern void haifa_sched_init (void);\n+extern void haifa_sched_finish (void);\n \n /* sched-deps.c interface to walk, add, search, update, resolve, delete\n    and debug instruction dependencies.  */"}, {"sha": "8ea3d098dcce0200221ac8bddc4c6043ae59ca88", "filename": "gcc/sched-rgn.c", "status": "modified", "additions": 672, "deletions": 399, "changes": 1071, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsched-rgn.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsched-rgn.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-rgn.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -64,90 +64,68 @@ along with GCC; see the file COPYING3.  If not see\n #include \"cfglayout.h\"\n #include \"params.h\"\n #include \"sched-int.h\"\n+#include \"sel-sched.h\"\n #include \"target.h\"\n #include \"timevar.h\"\n #include \"tree-pass.h\"\n #include \"dbgcnt.h\"\n \n #ifdef INSN_SCHEDULING\n+\n /* Some accessor macros for h_i_d members only used within this file.  */\n-#define INSN_REF_COUNT(INSN)\t(h_i_d[INSN_UID (INSN)].ref_count)\n-#define FED_BY_SPEC_LOAD(insn)\t(h_i_d[INSN_UID (insn)].fed_by_spec_load)\n-#define IS_LOAD_INSN(insn)\t(h_i_d[INSN_UID (insn)].is_load_insn)\n+#define FED_BY_SPEC_LOAD(INSN) (HID (INSN)->fed_by_spec_load)\n+#define IS_LOAD_INSN(INSN) (HID (insn)->is_load_insn)\n \n /* nr_inter/spec counts interblock/speculative motion for the function.  */\n static int nr_inter, nr_spec;\n \n static int is_cfg_nonregular (void);\n-static bool sched_is_disabled_for_current_region_p (void);\n-\n-/* A region is the main entity for interblock scheduling: insns\n-   are allowed to move between blocks in the same region, along\n-   control flow graph edges, in the 'up' direction.  */\n-typedef struct\n-{\n-  /* Number of extended basic blocks in region.  */\n-  int rgn_nr_blocks;\n-  /* cblocks in the region (actually index in rgn_bb_table).  */\n-  int rgn_blocks;\n-  /* Dependencies for this region are already computed.  Basically, indicates,\n-     that this is a recovery block.  */\n-  unsigned int dont_calc_deps : 1;\n-  /* This region has at least one non-trivial ebb.  */\n-  unsigned int has_real_ebb : 1;\n-}\n-region;\n \n /* Number of regions in the procedure.  */\n-static int nr_regions;\n+int nr_regions = 0;\n \n /* Table of region descriptions.  */\n-static region *rgn_table;\n+region *rgn_table = NULL;\n \n /* Array of lists of regions' blocks.  */\n-static int *rgn_bb_table;\n+int *rgn_bb_table = NULL;\n \n /* Topological order of blocks in the region (if b2 is reachable from\n    b1, block_to_bb[b2] > block_to_bb[b1]).  Note: A basic block is\n    always referred to by either block or b, while its topological\n    order name (in the region) is referred to by bb.  */\n-static int *block_to_bb;\n+int *block_to_bb = NULL;\n \n /* The number of the region containing a block.  */\n-static int *containing_rgn;\n+int *containing_rgn = NULL;\n+\n+/* ebb_head [i] - is index in rgn_bb_table of the head basic block of i'th ebb.\n+   Currently we can get a ebb only through splitting of currently\n+   scheduling block, therefore, we don't need ebb_head array for every region,\n+   hence, its sufficient to hold it for current one only.  */\n+int *ebb_head = NULL;\n \n /* The minimum probability of reaching a source block so that it will be\n    considered for speculative scheduling.  */\n static int min_spec_prob;\n \n-#define RGN_NR_BLOCKS(rgn) (rgn_table[rgn].rgn_nr_blocks)\n-#define RGN_BLOCKS(rgn) (rgn_table[rgn].rgn_blocks)\n-#define RGN_DONT_CALC_DEPS(rgn) (rgn_table[rgn].dont_calc_deps)\n-#define RGN_HAS_REAL_EBB(rgn) (rgn_table[rgn].has_real_ebb)\n-#define BLOCK_TO_BB(block) (block_to_bb[block])\n-#define CONTAINING_RGN(block) (containing_rgn[block])\n-\n-void debug_regions (void);\n-static void find_single_block_region (void);\n+static void find_single_block_region (bool);\n static void find_rgns (void);\n-static void extend_rgns (int *, int *, sbitmap, int *);\n static bool too_large (int, int *, int *);\n \n-extern void debug_live (int, int);\n-\n /* Blocks of the current region being scheduled.  */\n-static int current_nr_blocks;\n-static int current_blocks;\n+int current_nr_blocks;\n+int current_blocks;\n \n-static int rgn_n_insns;\n+/* A speculative motion requires checking live information on the path\n+   from 'source' to 'target'.  The split blocks are those to be checked.\n+   After a speculative motion, live information should be modified in\n+   the 'update' blocks.\n \n-/* The mapping from ebb to block.  */\n-/* ebb_head [i] - is index in rgn_bb_table, while\n-   EBB_HEAD (i) - is basic block index.\n-   BASIC_BLOCK (EBB_HEAD (i)) - head of ebb.  */\n-#define BB_TO_BLOCK(ebb) (rgn_bb_table[ebb_head[ebb]])\n-#define EBB_FIRST_BB(ebb) BASIC_BLOCK (BB_TO_BLOCK (ebb))\n-#define EBB_LAST_BB(ebb) BASIC_BLOCK (rgn_bb_table[ebb_head[ebb + 1] - 1])\n+   Lists of split and update blocks for each candidate of the current\n+   target are in array bblst_table.  */\n+static basic_block *bblst_table;\n+static int bblst_size, bblst_last;\n \n /* Target info declarations.\n \n@@ -173,23 +151,14 @@ typedef struct\n candidate;\n \n static candidate *candidate_table;\n-\n-/* A speculative motion requires checking live information on the path\n-   from 'source' to 'target'.  The split blocks are those to be checked.\n-   After a speculative motion, live information should be modified in\n-   the 'update' blocks.\n-\n-   Lists of split and update blocks for each candidate of the current\n-   target are in array bblst_table.  */\n-static basic_block *bblst_table;\n-static int bblst_size, bblst_last;\n-\n-#define IS_VALID(src) ( candidate_table[src].is_valid )\n-#define IS_SPECULATIVE(src) ( candidate_table[src].is_speculative )\n+#define IS_VALID(src) (candidate_table[src].is_valid)\n+#define IS_SPECULATIVE(src) (candidate_table[src].is_speculative)\n+#define IS_SPECULATIVE_INSN(INSN)\t\t\t\\\n+  (IS_SPECULATIVE (BLOCK_TO_BB (BLOCK_NUM (INSN))))\n #define SRC_PROB(src) ( candidate_table[src].src_prob )\n \n /* The bb being currently scheduled.  */\n-static int target_bb;\n+int target_bb;\n \n /* List of edges.  */\n typedef struct\n@@ -204,7 +173,6 @@ static int edgelst_last;\n \n static void extract_edgelst (sbitmap, edgelst *);\n \n-\n /* Target info functions.  */\n static void split_edges (int, int, edgelst *);\n static void compute_trg_info (int);\n@@ -250,24 +218,11 @@ static edgeset *pot_split;\n /* For every bb, a set of its ancestor edges.  */\n static edgeset *ancestor_edges;\n \n-/* Array of EBBs sizes.  Currently we can get a ebb only through \n-   splitting of currently scheduling block, therefore, we don't need\n-   ebb_head array for every region, its sufficient to hold it only\n-   for current one.  */\n-static int *ebb_head;\n-\n-static void compute_dom_prob_ps (int);\n-\n #define INSN_PROBABILITY(INSN) (SRC_PROB (BLOCK_TO_BB (BLOCK_NUM (INSN))))\n-#define IS_SPECULATIVE_INSN(INSN) (IS_SPECULATIVE (BLOCK_TO_BB (BLOCK_NUM (INSN))))\n-#define INSN_BB(INSN) (BLOCK_TO_BB (BLOCK_NUM (INSN)))\n \n /* Speculative scheduling functions.  */\n static int check_live_1 (int, rtx);\n static void update_live_1 (int, rtx);\n-static int check_live (rtx, int);\n-static void update_live (rtx, int);\n-static void set_spec_fed (rtx);\n static int is_pfree (rtx, int, int);\n static int find_conditional_protection (rtx, int);\n static int is_conditionally_protected (rtx, int, int);\n@@ -279,7 +234,6 @@ static void sets_likely_spilled_1 (rtx, const_rtx, void *);\n static void add_branch_dependences (rtx, rtx);\n static void compute_block_dependences (int);\n \n-static void init_regions (void);\n static void schedule_region (int);\n static rtx concat_INSN_LIST (rtx, rtx);\n static void concat_insn_mem_list (rtx, rtx, rtx *, rtx *);\n@@ -423,28 +377,160 @@ debug_regions (void)\n     }\n }\n \n+/* Print the region's basic blocks.  */\n+\n+void\n+debug_region (int rgn)\n+{\n+  int bb;\n+\n+  fprintf (stderr, \"\\n;;   ------------ REGION %d ----------\\n\\n\", rgn);\n+  fprintf (stderr, \";;\\trgn %d nr_blocks %d:\\n\", rgn,\n+\t   rgn_table[rgn].rgn_nr_blocks);\n+  fprintf (stderr, \";;\\tbb/block: \");\n+\n+  /* We don't have ebb_head initialized yet, so we can't use\n+     BB_TO_BLOCK ().  */\n+  current_blocks = RGN_BLOCKS (rgn);\n+\n+  for (bb = 0; bb < rgn_table[rgn].rgn_nr_blocks; bb++)\n+    fprintf (stderr, \" %d/%d \", bb, rgn_bb_table[current_blocks + bb]);\n+\n+  fprintf (stderr, \"\\n\\n\");\n+\n+  for (bb = 0; bb < rgn_table[rgn].rgn_nr_blocks; bb++)\n+    {\n+      debug_bb_n_slim (rgn_bb_table[current_blocks + bb]);\n+      fprintf (stderr, \"\\n\");\n+    }\n+\n+  fprintf (stderr, \"\\n\");\n+\n+}\n+\n+/* True when a bb with index BB_INDEX contained in region RGN.  */\n+static bool\n+bb_in_region_p (int bb_index, int rgn)\n+{\n+  int i;\n+\n+  for (i = 0; i < rgn_table[rgn].rgn_nr_blocks; i++)\n+    if (rgn_bb_table[current_blocks + i] == bb_index)\n+      return true;\n+\n+  return false;\n+}\n+\n+/* Dump region RGN to file F using dot syntax.  */\n+void\n+dump_region_dot (FILE *f, int rgn)\n+{\n+  int i;\n+\n+  fprintf (f, \"digraph Region_%d {\\n\", rgn);\n+\n+  /* We don't have ebb_head initialized yet, so we can't use\n+     BB_TO_BLOCK ().  */\n+  current_blocks = RGN_BLOCKS (rgn);\n+\n+  for (i = 0; i < rgn_table[rgn].rgn_nr_blocks; i++)\n+    {\n+      edge e;\n+      edge_iterator ei;\n+      int src_bb_num = rgn_bb_table[current_blocks + i];\n+      struct basic_block_def *bb = BASIC_BLOCK (src_bb_num);\n+\n+      FOR_EACH_EDGE (e, ei, bb->succs)\n+        if (bb_in_region_p (e->dest->index, rgn))\n+\t  fprintf (f, \"\\t%d -> %d\\n\", src_bb_num, e->dest->index);\n+    }\n+  fprintf (f, \"}\\n\");\n+}\n+\n+/* The same, but first open a file specified by FNAME.  */\n+void \n+dump_region_dot_file (const char *fname, int rgn)\n+{\n+  FILE *f = fopen (fname, \"wt\");\n+  dump_region_dot (f, rgn);\n+  fclose (f);\n+}\n+\n /* Build a single block region for each basic block in the function.\n    This allows for using the same code for interblock and basic block\n    scheduling.  */\n \n static void\n-find_single_block_region (void)\n+find_single_block_region (bool ebbs_p)\n {\n-  basic_block bb;\n+  basic_block bb, ebb_start;\n+  int i = 0;\n \n   nr_regions = 0;\n \n-  FOR_EACH_BB (bb)\n-    {\n-      rgn_bb_table[nr_regions] = bb->index;\n-      RGN_NR_BLOCKS (nr_regions) = 1;\n-      RGN_BLOCKS (nr_regions) = nr_regions;\n-      RGN_DONT_CALC_DEPS (nr_regions) = 0;\n-      RGN_HAS_REAL_EBB (nr_regions) = 0;\n-      CONTAINING_RGN (bb->index) = nr_regions;\n-      BLOCK_TO_BB (bb->index) = 0;\n-      nr_regions++;\n-    }\n+  if (ebbs_p) {\n+    int probability_cutoff;\n+    if (profile_info && flag_branch_probabilities)\n+      probability_cutoff = PARAM_VALUE (TRACER_MIN_BRANCH_PROBABILITY_FEEDBACK);\n+    else\n+      probability_cutoff = PARAM_VALUE (TRACER_MIN_BRANCH_PROBABILITY);\n+    probability_cutoff = REG_BR_PROB_BASE / 100 * probability_cutoff;\n+\n+    FOR_EACH_BB (ebb_start)\n+      {\n+        RGN_NR_BLOCKS (nr_regions) = 0;\n+        RGN_BLOCKS (nr_regions) = i;\n+        RGN_DONT_CALC_DEPS (nr_regions) = 0;\n+        RGN_HAS_REAL_EBB (nr_regions) = 0;\n+\n+        for (bb = ebb_start; ; bb = bb->next_bb)\n+          {\n+            edge e;\n+            edge_iterator ei;\n+\n+            rgn_bb_table[i] = bb->index;\n+            RGN_NR_BLOCKS (nr_regions)++;\n+            CONTAINING_RGN (bb->index) = nr_regions;\n+            BLOCK_TO_BB (bb->index) = i - RGN_BLOCKS (nr_regions);\n+            i++;\n+\n+            if (bb->next_bb == EXIT_BLOCK_PTR\n+                || LABEL_P (BB_HEAD (bb->next_bb)))\n+              break;\n+            \n+            FOR_EACH_EDGE (e, ei, bb->succs)\n+             if ((e->flags & EDGE_FALLTHRU) != 0)\n+               break;\n+            if (! e)\n+              break;\n+            if (e->probability <= probability_cutoff)\n+              break;\n+          }\n+\n+        ebb_start = bb;\n+        nr_regions++;\n+      }\n+  }\n+  else\n+    FOR_EACH_BB (bb)\n+      {\n+        rgn_bb_table[nr_regions] = bb->index;\n+        RGN_NR_BLOCKS (nr_regions) = 1;\n+        RGN_BLOCKS (nr_regions) = nr_regions;\n+        RGN_DONT_CALC_DEPS (nr_regions) = 0;\n+        RGN_HAS_REAL_EBB (nr_regions) = 0;\n+\n+        CONTAINING_RGN (bb->index) = nr_regions;\n+        BLOCK_TO_BB (bb->index) = 0;\n+        nr_regions++;\n+      }\n+}\n+\n+/* Estimate number of the insns in the BB.  */\n+static int\n+rgn_estimate_number_of_insns (basic_block bb)\n+{\n+  return INSN_LUID (BB_END (bb)) - INSN_LUID (BB_HEAD (bb));\n }\n \n /* Update number of blocks and the estimate for number of insns\n@@ -455,8 +541,8 @@ static bool\n too_large (int block, int *num_bbs, int *num_insns)\n {\n   (*num_bbs)++;\n-  (*num_insns) += (INSN_LUID (BB_END (BASIC_BLOCK (block)))\n-\t\t   - INSN_LUID (BB_HEAD (BASIC_BLOCK (block))));\n+  (*num_insns) += (common_sched_info->estimate_number_of_insns\n+                   (BASIC_BLOCK (block)));\n \n   return ((*num_bbs > PARAM_VALUE (PARAM_MAX_SCHED_REGION_BLOCKS))\n \t  || (*num_insns > PARAM_VALUE (PARAM_MAX_SCHED_REGION_INSNS)));\n@@ -509,7 +595,7 @@ too_large (int block, int *num_bbs, int *num_insns)\n    of edge tables.  That would simplify it somewhat.  */\n \n static void\n-find_rgns (void)\n+haifa_find_rgns (void)\n {\n   int *max_hdr, *dfs_nr, *degree;\n   char no_loops = 1;\n@@ -765,8 +851,7 @@ find_rgns (void)\n \n \t      /* Estimate # insns, and count # blocks in the region.  */\n \t      num_bbs = 1;\n-\t      num_insns = (INSN_LUID (BB_END (bb))\n-\t\t\t   - INSN_LUID (BB_HEAD (bb)));\n+\t      num_insns = common_sched_info->estimate_number_of_insns (bb);\n \n \t      /* Find all loop latches (blocks with back edges to the loop\n \t\t header) or all the leaf blocks in the cfg has no loops.\n@@ -970,6 +1055,19 @@ find_rgns (void)\n   sbitmap_free (in_stack);\n }\n \n+\n+/* Wrapper function.\n+   If FLAG_SEL_SCHED_PIPELINING is set, then use custom function to form\n+   regions.  Otherwise just call find_rgns_haifa.  */\n+static void\n+find_rgns (void)\n+{\n+  if (sel_sched_p () && flag_sel_sched_pipelining)\n+    sel_find_rgns ();\n+  else\n+    haifa_find_rgns ();\n+}\n+\n static int gather_region_statistics (int **);\n static void print_region_statistics (int *, int, int *, int);\n \n@@ -1039,7 +1137,7 @@ print_region_statistics (int *s1, int s1_sz, int *s2, int s2_sz)\n    LOOP_HDR - mapping from block to the containing loop\n    (two blocks can reside within one region if they have\n    the same loop header).  */\n-static void\n+void\n extend_rgns (int *degree, int *idxp, sbitmap header, int *loop_hdr)\n {\n   int *order, i, rescan = 0, idx = *idxp, iter = 0, max_iter, *max_hdr;\n@@ -1073,7 +1171,8 @@ extend_rgns (int *degree, int *idxp, sbitmap header, int *loop_hdr)\n      CFG should be traversed until no further changes are made.  On each \n      iteration the set of the region heads is extended (the set of those \n      blocks that have max_hdr[bbi] == bbi).  This set is upper bounded by the \n-     set of all basic blocks, thus the algorithm is guaranteed to terminate.  */\n+     set of all basic blocks, thus the algorithm is guaranteed to\n+     terminate.  */\n \n   while (rescan && iter < max_iter)\n     {\n@@ -1372,6 +1471,19 @@ compute_trg_info (int trg)\n   edge_iterator ei;\n   edge e;\n \n+  candidate_table = XNEWVEC (candidate, current_nr_blocks);\n+\n+  bblst_last = 0;\n+  /* bblst_table holds split blocks and update blocks for each block after\n+     the current one in the region.  split blocks and update blocks are\n+     the TO blocks of region edges, so there can be at most rgn_nr_edges\n+     of them.  */\n+  bblst_size = (current_nr_blocks - target_bb) * rgn_nr_edges;\n+  bblst_table = XNEWVEC (basic_block, bblst_size);\n+\n+  edgelst_last = 0;\n+  edgelst_table = XNEWVEC (edge, rgn_nr_edges);\n+\n   /* Define some of the fields for the target bb as well.  */\n   sp = candidate_table + trg;\n   sp->is_valid = 1;\n@@ -1458,6 +1570,15 @@ compute_trg_info (int trg)\n   sbitmap_free (visited);\n }\n \n+/* Free the computed target info.  */\n+static void\n+free_trg_info (void)\n+{\n+  free (candidate_table);\n+  free (bblst_table);\n+  free (edgelst_table);\n+}\n+\n /* Print candidates info, for debugging purposes.  Callable from debugger.  */\n \n void\n@@ -1941,20 +2062,16 @@ static int can_schedule_ready_p (rtx);\n static void begin_schedule_ready (rtx, rtx);\n static ds_t new_ready (rtx, ds_t);\n static int schedule_more_p (void);\n-static const char *rgn_print_insn (rtx, int);\n+static const char *rgn_print_insn (const_rtx, int);\n static int rgn_rank (rtx, rtx);\n-static int contributes_to_priority (rtx, rtx);\n static void compute_jump_reg_dependencies (rtx, regset, regset, regset);\n \n /* Functions for speculative scheduling.  */\n-static void add_remove_insn (rtx, int);\n-static void extend_regions (void);\n-static void add_block1 (basic_block, basic_block);\n-static void fix_recovery_cfg (int, int, int);\n+static void rgn_add_remove_insn (rtx, int);\n+static void rgn_add_block (basic_block, basic_block);\n+static void rgn_fix_recovery_cfg (int, int, int);\n static basic_block advance_target_bb (basic_block, rtx);\n \n-static void debug_rgn_dependencies (int);\n-\n /* Return nonzero if there are more insns that should be scheduled.  */\n \n static int\n@@ -1984,22 +2101,7 @@ init_ready_list (void)\n \n   /* Prepare current target block info.  */\n   if (current_nr_blocks > 1)\n-    {\n-      candidate_table = XNEWVEC (candidate, current_nr_blocks);\n-\n-      bblst_last = 0;\n-      /* bblst_table holds split blocks and update blocks for each block after\n-\t the current one in the region.  split blocks and update blocks are\n-\t the TO blocks of region edges, so there can be at most rgn_nr_edges\n-\t of them.  */\n-      bblst_size = (current_nr_blocks - target_bb) * rgn_nr_edges;\n-      bblst_table = XNEWVEC (basic_block, bblst_size);\n-\n-      edgelst_last = 0;\n-      edgelst_table = XNEWVEC (edge, rgn_nr_edges);\n-\n-      compute_trg_info (target_bb);\n-    }\n+    compute_trg_info (target_bb);\n \n   /* Initialize ready list with all 'ready' insns in target block.\n      Count number of insns in the target block being scheduled.  */\n@@ -2106,8 +2208,8 @@ new_ready (rtx next, ds_t ts)\n \t  if (not_ex_free\n \t      /* We are here because is_exception_free () == false.\n \t\t But we possibly can handle that with control speculation.  */\n-\t      && (current_sched_info->flags & DO_SPECULATION)\n-\t      && (spec_info->mask & BEGIN_CONTROL))\n+\t      && sched_deps_info->generate_spec_deps\n+\t      && spec_info->mask & BEGIN_CONTROL)\n \t    {\n \t      ds_t new_ds;\n \n@@ -2137,7 +2239,7 @@ new_ready (rtx next, ds_t ts)\n    to be formatted so that multiple output lines will line up nicely.  */\n \n static const char *\n-rgn_print_insn (rtx insn, int aligned)\n+rgn_print_insn (const_rtx insn, int aligned)\n {\n   static char tmp[80];\n \n@@ -2188,7 +2290,7 @@ rgn_rank (rtx insn1, rtx insn2)\n    return nonzero if we should include this dependence in priority\n    calculations.  */\n \n-static int\n+int\n contributes_to_priority (rtx next, rtx insn)\n {\n   /* NEXT and INSN reside in one ebb.  */\n@@ -2210,10 +2312,36 @@ compute_jump_reg_dependencies (rtx insn ATTRIBUTE_UNUSED,\n      add_branch_dependences.  */\n }\n \n+/* This variable holds common_sched_info hooks and data relevant to \n+   the interblock scheduler.  */\n+static struct common_sched_info_def rgn_common_sched_info;\n+\n+\n+/* This holds data for the dependence analysis relevant to\n+   the interblock scheduler.  */\n+static struct sched_deps_info_def rgn_sched_deps_info;\n+\n+/* This holds constant data used for initializing the above structure\n+   for the Haifa scheduler.  */\n+static const struct sched_deps_info_def rgn_const_sched_deps_info =\n+  {\n+    compute_jump_reg_dependencies,\n+    NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,\n+    0, 0, 0\n+  };\n+\n+/* Same as above, but for the selective scheduler.  */\n+static const struct sched_deps_info_def rgn_const_sel_sched_deps_info =\n+  {\n+    compute_jump_reg_dependencies,\n+    NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,\n+    0, 0, 0\n+  };\n+\n /* Used in schedule_insns to initialize current_sched_info for scheduling\n    regions (or single basic blocks).  */\n \n-static struct sched_info region_sched_info =\n+static const struct haifa_sched_info rgn_const_sched_info =\n {\n   init_ready_list,\n   can_schedule_ready_p,\n@@ -2222,20 +2350,29 @@ static struct sched_info region_sched_info =\n   rgn_rank,\n   rgn_print_insn,\n   contributes_to_priority,\n-  compute_jump_reg_dependencies,\n \n   NULL, NULL,\n   NULL, NULL,\n-  0, 0, 0,\n+  0, 0,\n \n-  add_remove_insn,\n+  rgn_add_remove_insn,\n   begin_schedule_ready,\n-  add_block1,\n   advance_target_bb,\n-  fix_recovery_cfg,\n   SCHED_RGN\n };\n \n+/* This variable holds the data and hooks needed to the Haifa scheduler backend\n+   for the interblock scheduler frontend.  */\n+static struct haifa_sched_info rgn_sched_info;\n+\n+/* Returns maximum priority that an insn was assigned to.  */\n+\n+int\n+get_rgn_sched_max_insns_priority (void)\n+{\n+  return rgn_sched_info.sched_max_insns_priority;\n+}\n+\n /* Determine if PAT sets a CLASS_LIKELY_SPILLED_P register.  */\n \n static bool\n@@ -2258,9 +2395,12 @@ sets_likely_spilled_1 (rtx x, const_rtx pat, void *data)\n     *ret = true;\n }\n \n+/* An array used to hold the number of dependencies in which insn \n+   participates.  Used in add_branch_dependences.  */\n+static int *ref_counts;\n+\n /* Add dependences so that branches are scheduled to run last in their\n    block.  */\n-\n static void\n add_branch_dependences (rtx head, rtx tail)\n {\n@@ -2284,6 +2424,8 @@ add_branch_dependences (rtx head, rtx tail)\n      are not moved before reload because we can wind up with register\n      allocation failures.  */\n \n+#define INSN_REF_COUNT(INSN) (ref_counts[INSN_UID (INSN)])\n+\n   insn = tail;\n   last = 0;\n   while (CALL_P (insn)\n@@ -2426,6 +2568,57 @@ concat_insn_mem_list (rtx copy_insns, rtx copy_mems, rtx *old_insns_p,\n   *old_mems_p = new_mems;\n }\n \n+/* Join PRED_DEPS to the SUCC_DEPS.  */\n+void\n+deps_join (struct deps *succ_deps, struct deps *pred_deps)\n+{\n+  unsigned reg;\n+  reg_set_iterator rsi;\n+\n+  /* The reg_last lists are inherited by successor.  */\n+  EXECUTE_IF_SET_IN_REG_SET (&pred_deps->reg_last_in_use, 0, reg, rsi)\n+    {\n+      struct deps_reg *pred_rl = &pred_deps->reg_last[reg];\n+      struct deps_reg *succ_rl = &succ_deps->reg_last[reg];\n+\n+      succ_rl->uses = concat_INSN_LIST (pred_rl->uses, succ_rl->uses);\n+      succ_rl->sets = concat_INSN_LIST (pred_rl->sets, succ_rl->sets);\n+      succ_rl->clobbers = concat_INSN_LIST (pred_rl->clobbers,\n+                                            succ_rl->clobbers);\n+      succ_rl->uses_length += pred_rl->uses_length;\n+      succ_rl->clobbers_length += pred_rl->clobbers_length;\n+    }\n+  IOR_REG_SET (&succ_deps->reg_last_in_use, &pred_deps->reg_last_in_use);\n+\n+  /* Mem read/write lists are inherited by successor.  */\n+  concat_insn_mem_list (pred_deps->pending_read_insns,\n+                        pred_deps->pending_read_mems,\n+                        &succ_deps->pending_read_insns,\n+                        &succ_deps->pending_read_mems);\n+  concat_insn_mem_list (pred_deps->pending_write_insns,\n+                        pred_deps->pending_write_mems,\n+                        &succ_deps->pending_write_insns,\n+                        &succ_deps->pending_write_mems);\n+\n+  succ_deps->last_pending_memory_flush\n+    = concat_INSN_LIST (pred_deps->last_pending_memory_flush,\n+                        succ_deps->last_pending_memory_flush);\n+\n+  succ_deps->pending_read_list_length += pred_deps->pending_read_list_length;\n+  succ_deps->pending_write_list_length += pred_deps->pending_write_list_length;\n+  succ_deps->pending_flush_length += pred_deps->pending_flush_length;\n+\n+  /* last_function_call is inherited by successor.  */\n+  succ_deps->last_function_call\n+    = concat_INSN_LIST (pred_deps->last_function_call,\n+                        succ_deps->last_function_call);\n+\n+  /* sched_before_next_call is inherited by successor.  */\n+  succ_deps->sched_before_next_call\n+    = concat_INSN_LIST (pred_deps->sched_before_next_call,\n+                        succ_deps->sched_before_next_call);\n+}\n+\n /* After computing the dependencies for block BB, propagate the dependencies\n    found in TMP_DEPS to the successors of the block.  */\n static void\n@@ -2438,62 +2631,13 @@ propagate_deps (int bb, struct deps *pred_deps)\n   /* bb's structures are inherited by its successors.  */\n   FOR_EACH_EDGE (e, ei, block->succs)\n     {\n-      struct deps *succ_deps;\n-      unsigned reg;\n-      reg_set_iterator rsi;\n-\n       /* Only bbs \"below\" bb, in the same region, are interesting.  */\n       if (e->dest == EXIT_BLOCK_PTR\n \t  || CONTAINING_RGN (block->index) != CONTAINING_RGN (e->dest->index)\n \t  || BLOCK_TO_BB (e->dest->index) <= bb)\n \tcontinue;\n \n-      succ_deps = bb_deps + BLOCK_TO_BB (e->dest->index);\n-\n-      /* The reg_last lists are inherited by successor.  */\n-      EXECUTE_IF_SET_IN_REG_SET (&pred_deps->reg_last_in_use, 0, reg, rsi)\n-\t{\n-\t  struct deps_reg *pred_rl = &pred_deps->reg_last[reg];\n-\t  struct deps_reg *succ_rl = &succ_deps->reg_last[reg];\n-\n-\t  succ_rl->uses = concat_INSN_LIST (pred_rl->uses, succ_rl->uses);\n-\t  succ_rl->sets = concat_INSN_LIST (pred_rl->sets, succ_rl->sets);\n-\t  succ_rl->clobbers = concat_INSN_LIST (pred_rl->clobbers,\n-\t\t\t\t\t\tsucc_rl->clobbers);\n-\t  succ_rl->uses_length += pred_rl->uses_length;\n-\t  succ_rl->clobbers_length += pred_rl->clobbers_length;\n-\t}\n-      IOR_REG_SET (&succ_deps->reg_last_in_use, &pred_deps->reg_last_in_use);\n-\n-      /* Mem read/write lists are inherited by successor.  */\n-      concat_insn_mem_list (pred_deps->pending_read_insns,\n-\t\t\t    pred_deps->pending_read_mems,\n-\t\t\t    &succ_deps->pending_read_insns,\n-\t\t\t    &succ_deps->pending_read_mems);\n-      concat_insn_mem_list (pred_deps->pending_write_insns,\n-\t\t\t    pred_deps->pending_write_mems,\n-\t\t\t    &succ_deps->pending_write_insns,\n-\t\t\t    &succ_deps->pending_write_mems);\n-\n-      succ_deps->last_pending_memory_flush\n-\t= concat_INSN_LIST (pred_deps->last_pending_memory_flush,\n-\t\t\t    succ_deps->last_pending_memory_flush);\n-\n-      succ_deps->pending_read_list_length\n-\t+= pred_deps->pending_read_list_length;\n-      succ_deps->pending_write_list_length\n-\t+= pred_deps->pending_write_list_length;\n-      succ_deps->pending_flush_length += pred_deps->pending_flush_length;\n-\n-      /* last_function_call is inherited by successor.  */\n-      succ_deps->last_function_call\n-\t= concat_INSN_LIST (pred_deps->last_function_call,\n-\t\t\t      succ_deps->last_function_call);\n-\n-      /* sched_before_next_call is inherited by successor.  */\n-      succ_deps->sched_before_next_call\n-\t= concat_INSN_LIST (pred_deps->sched_before_next_call,\n-\t\t\t    succ_deps->sched_before_next_call);\n+      deps_join (bb_deps + BLOCK_TO_BB (e->dest->index), pred_deps);\n     }\n \n   /* These lists should point to the right place, for correct\n@@ -2540,7 +2684,10 @@ compute_block_dependences (int bb)\n   get_ebb_head_tail (EBB_FIRST_BB (bb), EBB_LAST_BB (bb), &head, &tail);\n \n   sched_analyze (&tmp_deps, head, tail);\n-  add_branch_dependences (head, tail);\n+\n+  /* Selective scheduling handles control dependencies by itself.  */\n+  if (!sel_sched_p ())\n+    add_branch_dependences (head, tail);\n \n   if (current_nr_blocks > 1)\n     propagate_deps (bb, &tmp_deps);\n@@ -2641,9 +2788,13 @@ void debug_dependencies (rtx head, rtx tail)\n \t       INSN_UID (insn),\n \t       INSN_CODE (insn),\n \t       BLOCK_NUM (insn),\n-\t       sd_lists_size (insn, SD_LIST_BACK),\n-\t       INSN_PRIORITY (insn),\n-\t       insn_cost (insn));\n+\t       sched_emulate_haifa_p ? -1 : sd_lists_size (insn, SD_LIST_BACK),\n+\t       (sel_sched_p () ? (sched_emulate_haifa_p ? -1\n+\t\t\t       : INSN_PRIORITY (insn))\n+\t\t: INSN_PRIORITY (insn)),\n+\t       (sel_sched_p () ? (sched_emulate_haifa_p ? -1\n+\t\t\t       : insn_cost (insn))\n+\t\t: insn_cost (insn)));\n \n       if (recog_memoized (insn) < 0)\n \tfprintf (sched_dump, \"nothing\");\n@@ -2666,7 +2817,7 @@ void debug_dependencies (rtx head, rtx tail)\n \f\n /* Returns true if all the basic blocks of the current region have\n    NOTE_DISABLE_SCHED_OF_BLOCK which means not to schedule that region.  */\n-static bool\n+bool\n sched_is_disabled_for_current_region_p (void)\n {\n   int bb;\n@@ -2678,58 +2829,34 @@ sched_is_disabled_for_current_region_p (void)\n   return true;\n }\n \n-/* Schedule a region.  A region is either an inner loop, a loop-free\n-   subroutine, or a single basic block.  Each bb in the region is\n-   scheduled after its flow predecessors.  */\n-\n-static void\n-schedule_region (int rgn)\n+/* Free all region dependencies saved in INSN_BACK_DEPS and \n+   INSN_RESOLVED_BACK_DEPS.  The Haifa scheduler does this on the fly\n+   when scheduling, so this function is supposed to be called from \n+   the selective scheduling only.  */\n+void\n+free_rgn_deps (void)\n {\n-  basic_block block;\n-  edge_iterator ei;\n-  edge e;\n   int bb;\n-  int sched_rgn_n_insns = 0;\n-\n-  rgn_n_insns = 0;\n-  /* Set variables for the current region.  */\n-  current_nr_blocks = RGN_NR_BLOCKS (rgn);\n-  current_blocks = RGN_BLOCKS (rgn);\n-  \n-  /* See comments in add_block1, for what reasons we allocate +1 element.  */ \n-  ebb_head = XRESIZEVEC (int, ebb_head, current_nr_blocks + 1);\n-  for (bb = 0; bb <= current_nr_blocks; bb++)\n-    ebb_head[bb] = current_blocks + bb;\n-\n-  /* Don't schedule region that is marked by\n-     NOTE_DISABLE_SCHED_OF_BLOCK.  */\n-  if (sched_is_disabled_for_current_region_p ())\n-    return;\n \n-  if (!RGN_DONT_CALC_DEPS (rgn))\n+  for (bb = 0; bb < current_nr_blocks; bb++)\n     {\n-      init_deps_global ();\n-\n-      /* Initializations for region data dependence analysis.  */\n-      bb_deps = XNEWVEC (struct deps, current_nr_blocks);\n-      for (bb = 0; bb < current_nr_blocks; bb++)\n-\tinit_deps (bb_deps + bb);\n+      rtx head, tail;\n+      \n+      gcc_assert (EBB_FIRST_BB (bb) == EBB_LAST_BB (bb));\n+      get_ebb_head_tail (EBB_FIRST_BB (bb), EBB_LAST_BB (bb), &head, &tail);\n \n-      /* Compute dependencies.  */\n-      for (bb = 0; bb < current_nr_blocks; bb++)\n-\tcompute_block_dependences (bb);\n+      sched_free_deps (head, tail, false);\n+    }\n+}\n \n-      free_pending_lists ();\n+static int rgn_n_insns;\n \n-      finish_deps_global ();\n+/* Compute insn priority for a current region.  */\n+void\n+compute_priorities (void) \n+{\n+  int bb;\n \n-      free (bb_deps);\n-    }\n-  else\n-    /* This is a recovery block.  It is always a single block region.  */\n-    gcc_assert (current_nr_blocks == 1);\n-      \n-  /* Set priorities.  */\n   current_sched_info->sched_max_insns_priority = 0;\n   for (bb = 0; bb < current_nr_blocks; bb++)\n     {\n@@ -2741,56 +2868,35 @@ schedule_region (int rgn)\n       rgn_n_insns += set_priorities (head, tail);\n     }\n   current_sched_info->sched_max_insns_priority++;\n+}\n \n-  /* Compute interblock info: probabilities, split-edges, dominators, etc.  */\n-  if (current_nr_blocks > 1)\n-    {\n-      prob = XNEWVEC (int, current_nr_blocks);\n+/* Schedule a region.  A region is either an inner loop, a loop-free\n+   subroutine, or a single basic block.  Each bb in the region is\n+   scheduled after its flow predecessors.  */\n \n-      dom = sbitmap_vector_alloc (current_nr_blocks, current_nr_blocks);\n-      sbitmap_vector_zero (dom, current_nr_blocks);\n+static void\n+schedule_region (int rgn)\n+{\n+  int bb;\n+  int sched_rgn_n_insns = 0;\n \n-      /* Use ->aux to implement EDGE_TO_BIT mapping.  */\n-      rgn_nr_edges = 0;\n-      FOR_EACH_BB (block)\n-\t{\n-\t  if (CONTAINING_RGN (block->index) != rgn)\n-\t    continue;\n-\t  FOR_EACH_EDGE (e, ei, block->succs)\n-\t    SET_EDGE_TO_BIT (e, rgn_nr_edges++);\n-\t}\n+  rgn_n_insns = 0;\n \n-      rgn_edges = XNEWVEC (edge, rgn_nr_edges);\n-      rgn_nr_edges = 0;\n-      FOR_EACH_BB (block)\n-\t{\n-\t  if (CONTAINING_RGN (block->index) != rgn)\n-\t    continue;\n-\t  FOR_EACH_EDGE (e, ei, block->succs)\n-\t    rgn_edges[rgn_nr_edges++] = e;\n-\t}\n+  rgn_setup_region (rgn);\n \n-      /* Split edges.  */\n-      pot_split = sbitmap_vector_alloc (current_nr_blocks, rgn_nr_edges);\n-      sbitmap_vector_zero (pot_split, current_nr_blocks);\n-      ancestor_edges = sbitmap_vector_alloc (current_nr_blocks, rgn_nr_edges);\n-      sbitmap_vector_zero (ancestor_edges, current_nr_blocks);\n+  /* Don't schedule region that is marked by\n+     NOTE_DISABLE_SCHED_OF_BLOCK.  */\n+  if (sched_is_disabled_for_current_region_p ())\n+    return;\n \n-      /* Compute probabilities, dominators, split_edges.  */\n-      for (bb = 0; bb < current_nr_blocks; bb++)\n-\tcompute_dom_prob_ps (bb);\n+  sched_rgn_compute_dependencies (rgn);\n \n-      /* Cleanup ->aux used for EDGE_TO_BIT mapping.  */\n-      /* We don't need them anymore.  But we want to avoid duplication of\n-\t aux fields in the newly created edges.  */\n-      FOR_EACH_BB (block)\n-\t{\n-\t  if (CONTAINING_RGN (block->index) != rgn)\n-\t    continue;\n-\t  FOR_EACH_EDGE (e, ei, block->succs)\n-\t    e->aux = NULL;\n-        }\n-    }\n+  sched_rgn_local_init (rgn);\n+\n+  /* Set priorities.  */\n+  compute_priorities ();\n+\n+  sched_extend_ready_list (rgn_n_insns);\n \n   /* Now we can schedule all blocks.  */\n   for (bb = 0; bb < current_nr_blocks; bb++)\n@@ -2812,31 +2918,7 @@ schedule_region (int rgn)\n       current_sched_info->prev_head = PREV_INSN (head);\n       current_sched_info->next_tail = NEXT_INSN (tail);\n \n-\n-      /* rm_other_notes only removes notes which are _inside_ the\n-\t block---that is, it won't remove notes before the first real insn\n-\t or after the last real insn of the block.  So if the first insn\n-\t has a REG_SAVE_NOTE which would otherwise be emitted before the\n-\t insn, it is redundant with the note before the start of the\n-\t block, and so we have to take it out.  */\n-      if (INSN_P (head))\n-\t{\n-\t  rtx note;\n-\n-\t  for (note = REG_NOTES (head); note; note = XEXP (note, 1))\n-\t    if (REG_NOTE_KIND (note) == REG_SAVE_NOTE)\n-\t      remove_note (head, note);\n-\t}\n-      else\n-\t/* This means that first block in ebb is empty.\n-\t   It looks to me as an impossible thing.  There at least should be\n-\t   a recovery check, that caused the splitting.  */\n-\tgcc_unreachable ();\n-\n-      /* Remove remaining note insns from the block, save them in\n-\t note_list.  These notes are restored at the end of\n-\t schedule_block ().  */\n-      rm_other_notes (head, tail);\n+      remove_notes (head, tail);\n \n       unlink_bb_notes (first_bb, last_bb);\n \n@@ -2848,7 +2930,7 @@ schedule_region (int rgn)\n       curr_bb = first_bb;\n       if (dbg_cnt (sched_block))\n         {\n-          schedule_block (&curr_bb, rgn_n_insns);\n+          schedule_block (&curr_bb);\n           gcc_assert (EBB_FIRST_BB (bb) == first_bb);\n           sched_rgn_n_insns += sched_n_insns;\n         }\n@@ -2859,26 +2941,16 @@ schedule_region (int rgn)\n \n       /* Clean up.  */\n       if (current_nr_blocks > 1)\n-\t{\n-\t  free (candidate_table);\n-\t  free (bblst_table);\n-\t  free (edgelst_table);\n-\t}\n+\tfree_trg_info ();\n     }\n \n   /* Sanity check: verify that all region insns were scheduled.  */\n   gcc_assert (sched_rgn_n_insns == rgn_n_insns);\n \n-  /* Done with this region.  */\n+  sched_finish_ready_list ();\n \n-  if (current_nr_blocks > 1)\n-    {\n-      free (prob);\n-      sbitmap_vector_free (dom);\n-      sbitmap_vector_free (pot_split);\n-      sbitmap_vector_free (ancestor_edges);\n-      free (rgn_edges);\n-    }\n+  /* Done with this region.  */\n+  sched_rgn_local_finish ();\n \n   /* Free dependencies.  */\n   for (bb = 0; bb < current_nr_blocks; ++bb)\n@@ -2890,28 +2962,33 @@ schedule_region (int rgn)\n \n /* Initialize data structures for region scheduling.  */\n \n-static void\n-init_regions (void)\n+void\n+sched_rgn_init (bool single_blocks_p)\n {\n-  nr_regions = 0;\n-  rgn_table = 0;\n-  rgn_bb_table = 0;\n-  block_to_bb = 0;\n-  containing_rgn = 0;\n+  min_spec_prob = ((PARAM_VALUE (PARAM_MIN_SPEC_PROB) * REG_BR_PROB_BASE)\n+\t\t    / 100);\n+\n+  nr_inter = 0;\n+  nr_spec = 0;\n+\n   extend_regions ();\n \n+  CONTAINING_RGN (ENTRY_BLOCK) = -1;\n+  CONTAINING_RGN (EXIT_BLOCK) = -1;\n+\n   /* Compute regions for scheduling.  */\n-  if (reload_completed\n+  if (single_blocks_p\n       || n_basic_blocks == NUM_FIXED_BLOCKS + 1\n       || !flag_schedule_interblock\n       || is_cfg_nonregular ())\n     {\n-      find_single_block_region ();\n+      find_single_block_region (sel_sched_p ());\n     }\n   else\n     {\n       /* Compute the dominators and post dominators.  */\n-      calculate_dominance_info (CDI_DOMINATORS);\n+      if (!sel_sched_p ())\n+\tcalculate_dominance_info (CDI_DOMINATORS);\n \n       /* Find regions.  */\n       find_rgns ();\n@@ -2921,64 +2998,29 @@ init_regions (void)\n \n       /* For now.  This will move as more and more of haifa is converted\n \t to using the cfg code.  */\n-      free_dominance_info (CDI_DOMINATORS);\n+      if (!sel_sched_p ())\n+\tfree_dominance_info (CDI_DOMINATORS);\n     }\n-  RGN_BLOCKS (nr_regions) = RGN_BLOCKS (nr_regions - 1) +\n-    RGN_NR_BLOCKS (nr_regions - 1);\n-}\n \n-/* The one entry point in this file.  */\n+  gcc_assert (0 < nr_regions && nr_regions <= n_basic_blocks);\n \n+  RGN_BLOCKS (nr_regions) = (RGN_BLOCKS (nr_regions - 1) +\n+\t\t\t     RGN_NR_BLOCKS (nr_regions - 1));\n+}\n+\n+/* Free data structures for region scheduling.  */\n void\n-schedule_insns (void)\n+sched_rgn_finish (void)\n {\n-  int rgn;\n-\n-  /* Taking care of this degenerate case makes the rest of\n-     this code simpler.  */\n-  if (n_basic_blocks == NUM_FIXED_BLOCKS)\n-    return;\n-\n-  nr_inter = 0;\n-  nr_spec = 0;\n-\n-  /* We need current_sched_info in init_dependency_caches, which is\n-     invoked via sched_init.  */\n-  current_sched_info = &region_sched_info;\n-\n-  df_set_flags (DF_LR_RUN_DCE);\n-  df_note_add_problem ();\n-  df_analyze ();\n-  regstat_compute_calls_crossed ();\n-\n-  sched_init ();\n-\n-  bitmap_initialize (&not_in_df, 0);\n-  bitmap_clear (&not_in_df);\n-\n-  min_spec_prob = ((PARAM_VALUE (PARAM_MIN_SPEC_PROB) * REG_BR_PROB_BASE)\n-\t\t    / 100);\n-\n-  init_regions ();\n-\n-  /* EBB_HEAD is a region-scope structure.  But we realloc it for\n-     each region to save time/memory/something else.  */\n-  ebb_head = 0;\n-  \n-  /* Schedule every region in the subroutine.  */\n-  for (rgn = 0; rgn < nr_regions; rgn++)\n-    if (dbg_cnt (sched_region))\n-      schedule_region (rgn);\n-  \n-  free(ebb_head);\n   /* Reposition the prologue and epilogue notes in case we moved the\n      prologue/epilogue insns.  */\n   if (reload_completed)\n     reposition_prologue_and_epilogue_notes ();\n \n   if (sched_verbose)\n     {\n-      if (reload_completed == 0 && flag_schedule_interblock)\n+      if (reload_completed == 0\n+\t  && flag_schedule_interblock)\n \t{\n \t  fprintf (sched_dump,\n \t\t   \"\\n;; Procedure interblock/speculative motions == %d/%d \\n\",\n@@ -2989,22 +3031,237 @@ schedule_insns (void)\n       fprintf (sched_dump, \"\\n\\n\");\n     }\n \n-  /* Clean up.  */\n+  nr_regions = 0;\n+\n   free (rgn_table);\n+  rgn_table = NULL;\n+\n   free (rgn_bb_table);\n+  rgn_bb_table = NULL;\n+\n   free (block_to_bb);\n+  block_to_bb = NULL;\n+\n   free (containing_rgn);\n+  containing_rgn = NULL;\n+\n+  free (ebb_head);\n+  ebb_head = NULL;\n+}\n+\n+/* Setup global variables like CURRENT_BLOCKS and CURRENT_NR_BLOCK to\n+   point to the region RGN.  */\n+void\n+rgn_setup_region (int rgn)\n+{\n+  int bb;\n+\n+  /* Set variables for the current region.  */\n+  current_nr_blocks = RGN_NR_BLOCKS (rgn);\n+  current_blocks = RGN_BLOCKS (rgn);\n+  \n+  /* EBB_HEAD is a region-scope structure.  But we realloc it for\n+     each region to save time/memory/something else.\n+     See comments in add_block1, for what reasons we allocate +1 element.  */\n+  ebb_head = XRESIZEVEC (int, ebb_head, current_nr_blocks + 1);\n+  for (bb = 0; bb <= current_nr_blocks; bb++)\n+    ebb_head[bb] = current_blocks + bb;\n+}\n+\n+/* Compute instruction dependencies in region RGN.  */\n+void\n+sched_rgn_compute_dependencies (int rgn)\n+{\n+  if (!RGN_DONT_CALC_DEPS (rgn))\n+    {\n+      int bb;\n+\n+      if (sel_sched_p ())\n+\tsched_emulate_haifa_p = 1;\n+\n+      init_deps_global ();\n+\n+      /* Initializations for region data dependence analysis.  */\n+      bb_deps = XNEWVEC (struct deps, current_nr_blocks);\n+      for (bb = 0; bb < current_nr_blocks; bb++)\n+\tinit_deps (bb_deps + bb);\n+\n+      /* Initialize array used in add_branch_dependencies ().  */\n+      ref_counts = XCNEWVEC (int, get_max_uid () + 1);\n+      \n+      /* Compute backward dependencies.  */\n+      for (bb = 0; bb < current_nr_blocks; bb++)\n+\tcompute_block_dependences (bb);\n+      \n+      free (ref_counts);\n+      free_pending_lists ();\n+      finish_deps_global ();\n+      free (bb_deps);\n \n-  regstat_free_calls_crossed ();\n+      /* We don't want to recalculate this twice.  */\n+      RGN_DONT_CALC_DEPS (rgn) = 1;\n \n+      if (sel_sched_p ())\n+\tsched_emulate_haifa_p = 0;\n+    }\n+  else\n+    /* (This is a recovery block.  It is always a single block region.)\n+       OR (We use selective scheduling.)  */\n+    gcc_assert (current_nr_blocks == 1 || sel_sched_p ());\n+}\n+\n+/* Init region data structures.  Returns true if this region should\n+   not be scheduled.  */\n+void\n+sched_rgn_local_init (int rgn)\n+{\n+  int bb;\n+      \n+  /* Compute interblock info: probabilities, split-edges, dominators, etc.  */\n+  if (current_nr_blocks > 1)\n+    {\n+      basic_block block;\n+      edge e;\n+      edge_iterator ei;\n+\n+      prob = XNEWVEC (int, current_nr_blocks);\n+\n+      dom = sbitmap_vector_alloc (current_nr_blocks, current_nr_blocks);\n+      sbitmap_vector_zero (dom, current_nr_blocks);\n+\n+      /* Use ->aux to implement EDGE_TO_BIT mapping.  */\n+      rgn_nr_edges = 0;\n+      FOR_EACH_BB (block)\n+\t{\n+\t  if (CONTAINING_RGN (block->index) != rgn)\n+\t    continue;\n+\t  FOR_EACH_EDGE (e, ei, block->succs)\n+\t    SET_EDGE_TO_BIT (e, rgn_nr_edges++);\n+\t}\n+\n+      rgn_edges = XNEWVEC (edge, rgn_nr_edges);\n+      rgn_nr_edges = 0;\n+      FOR_EACH_BB (block)\n+\t{\n+\t  if (CONTAINING_RGN (block->index) != rgn)\n+\t    continue;\n+\t  FOR_EACH_EDGE (e, ei, block->succs)\n+\t    rgn_edges[rgn_nr_edges++] = e;\n+\t}\n+\n+      /* Split edges.  */\n+      pot_split = sbitmap_vector_alloc (current_nr_blocks, rgn_nr_edges);\n+      sbitmap_vector_zero (pot_split, current_nr_blocks);\n+      ancestor_edges = sbitmap_vector_alloc (current_nr_blocks, rgn_nr_edges);\n+      sbitmap_vector_zero (ancestor_edges, current_nr_blocks);\n+\n+      /* Compute probabilities, dominators, split_edges.  */\n+      for (bb = 0; bb < current_nr_blocks; bb++)\n+\tcompute_dom_prob_ps (bb);\n+\n+      /* Cleanup ->aux used for EDGE_TO_BIT mapping.  */\n+      /* We don't need them anymore.  But we want to avoid duplication of\n+\t aux fields in the newly created edges.  */\n+      FOR_EACH_BB (block)\n+\t{\n+\t  if (CONTAINING_RGN (block->index) != rgn)\n+\t    continue;\n+\t  FOR_EACH_EDGE (e, ei, block->succs)\n+\t    e->aux = NULL;\n+        }\n+    }\n+}\n+\n+/* Free data computed for the finished region.  */\n+void \n+sched_rgn_local_free (void)\n+{\n+  free (prob);\n+  sbitmap_vector_free (dom);\n+  sbitmap_vector_free (pot_split);\n+  sbitmap_vector_free (ancestor_edges);\n+  free (rgn_edges);\n+}\n+\n+/* Free data computed for the finished region.  */\n+void\n+sched_rgn_local_finish (void)\n+{\n+  if (current_nr_blocks > 1 && !sel_sched_p ())\n+    {\n+      sched_rgn_local_free ();\n+    }\n+}\n+\n+/* Setup scheduler infos.  */\n+void\n+rgn_setup_common_sched_info (void)\n+{\n+  memcpy (&rgn_common_sched_info, &haifa_common_sched_info,\n+\t  sizeof (rgn_common_sched_info));\n+\n+  rgn_common_sched_info.fix_recovery_cfg = rgn_fix_recovery_cfg;\n+  rgn_common_sched_info.add_block = rgn_add_block;\n+  rgn_common_sched_info.estimate_number_of_insns\n+    = rgn_estimate_number_of_insns;\n+  rgn_common_sched_info.sched_pass_id = SCHED_RGN_PASS;\n+\n+  common_sched_info = &rgn_common_sched_info;\n+}\n+\n+/* Setup all *_sched_info structures (for the Haifa frontend\n+   and for the dependence analysis) in the interblock scheduler.  */\n+void\n+rgn_setup_sched_infos (void)\n+{\n+  if (!sel_sched_p ())\n+    memcpy (&rgn_sched_deps_info, &rgn_const_sched_deps_info,\n+\t    sizeof (rgn_sched_deps_info));\n+  else\n+    memcpy (&rgn_sched_deps_info, &rgn_const_sel_sched_deps_info,\n+\t    sizeof (rgn_sched_deps_info));\n+\n+  sched_deps_info = &rgn_sched_deps_info;\n+\n+  memcpy (&rgn_sched_info, &rgn_const_sched_info, sizeof (rgn_sched_info));\n+  current_sched_info = &rgn_sched_info;\n+}\n+\n+/* The one entry point in this file.  */\n+void\n+schedule_insns (void)\n+{\n+  int rgn;\n+\n+  /* Taking care of this degenerate case makes the rest of\n+     this code simpler.  */\n+  if (n_basic_blocks == NUM_FIXED_BLOCKS)\n+    return;\n+\n+  rgn_setup_common_sched_info ();\n+  rgn_setup_sched_infos ();\n+\n+  haifa_sched_init ();\n+  sched_rgn_init (reload_completed);\n+\n+  bitmap_initialize (&not_in_df, 0);\n   bitmap_clear (&not_in_df);\n \n-  sched_finish ();\n+  /* Schedule every region in the subroutine.  */\n+  for (rgn = 0; rgn < nr_regions; rgn++)\n+    if (dbg_cnt (sched_region))\n+      schedule_region (rgn);\n+\n+  /* Clean up.  */\n+  sched_rgn_finish ();\n+  bitmap_clear (&not_in_df);\n+\n+  haifa_sched_finish ();\n }\n \n /* INSN has been added to/removed from current region.  */\n static void\n-add_remove_insn (rtx insn, int remove_p)\n+rgn_add_remove_insn (rtx insn, int remove_p)\n {\n   if (!remove_p)\n     rgn_n_insns++;\n@@ -3021,7 +3278,7 @@ add_remove_insn (rtx insn, int remove_p)\n }\n \n /* Extend internal data structures.  */\n-static void\n+void\n extend_regions (void)\n {\n   rgn_table = XRESIZEVEC (region, rgn_table, n_basic_blocks);\n@@ -3030,31 +3287,37 @@ extend_regions (void)\n   containing_rgn = XRESIZEVEC (int, containing_rgn, last_basic_block);\n }\n \n+void\n+rgn_make_new_region_out_of_new_block (basic_block bb)\n+{\n+  int i;\n+\n+  i = RGN_BLOCKS (nr_regions);\n+  /* I - first free position in rgn_bb_table.  */\n+\n+  rgn_bb_table[i] = bb->index;\n+  RGN_NR_BLOCKS (nr_regions) = 1;\n+  RGN_HAS_REAL_EBB (nr_regions) = 0;\n+  RGN_DONT_CALC_DEPS (nr_regions) = 0;\n+  CONTAINING_RGN (bb->index) = nr_regions;\n+  BLOCK_TO_BB (bb->index) = 0;\n+\n+  nr_regions++;\n+      \n+  RGN_BLOCKS (nr_regions) = i + 1;\n+}\n+\n /* BB was added to ebb after AFTER.  */\n static void\n-add_block1 (basic_block bb, basic_block after)\n+rgn_add_block (basic_block bb, basic_block after)\n {\n   extend_regions ();\n-\n   bitmap_set_bit (&not_in_df, bb->index);\n \n   if (after == 0 || after == EXIT_BLOCK_PTR)\n     {\n-      int i;\n-      \n-      i = RGN_BLOCKS (nr_regions);\n-      /* I - first free position in rgn_bb_table.  */\n-\n-      rgn_bb_table[i] = bb->index;\n-      RGN_NR_BLOCKS (nr_regions) = 1;\n-      RGN_DONT_CALC_DEPS (nr_regions) = after == EXIT_BLOCK_PTR;\n-      RGN_HAS_REAL_EBB (nr_regions) = 0;\n-      CONTAINING_RGN (bb->index) = nr_regions;\n-      BLOCK_TO_BB (bb->index) = 0;\n-\n-      nr_regions++;\n-      \n-      RGN_BLOCKS (nr_regions) = i + 1;\n+      rgn_make_new_region_out_of_new_block (bb);\n+      RGN_DONT_CALC_DEPS (nr_regions - 1) = (after == EXIT_BLOCK_PTR);\n     }\n   else\n     { \n@@ -3114,7 +3377,7 @@ add_block1 (basic_block bb, basic_block after)\n    For parameter meaning please refer to\n    sched-int.h: struct sched_info: fix_recovery_cfg.  */\n static void\n-fix_recovery_cfg (int bbi, int check_bbi, int check_bb_nexti)\n+rgn_fix_recovery_cfg (int bbi, int check_bbi, int check_bb_nexti)\n {\n   int old_pos, new_pos, i;\n \n@@ -3173,7 +3436,11 @@ static unsigned int\n rest_of_handle_sched (void)\n {\n #ifdef INSN_SCHEDULING\n-  schedule_insns ();\n+  if (flag_selective_scheduling\n+      && ! maybe_skip_selective_scheduling ())\n+    run_selective_scheduling ();\n+  else\n+    schedule_insns ();\n #endif\n   return 0;\n }\n@@ -3194,12 +3461,18 @@ static unsigned int\n rest_of_handle_sched2 (void)\n {\n #ifdef INSN_SCHEDULING\n-  /* Do control and data sched analysis again,\n-     and write some more of the results to dump file.  */\n-  if (flag_sched2_use_superblocks || flag_sched2_use_traces)\n-    schedule_ebbs ();\n+  if (flag_selective_scheduling2\n+      && ! maybe_skip_selective_scheduling ())\n+    run_selective_scheduling ();\n   else\n-    schedule_insns ();\n+    {\n+      /* Do control and data sched analysis again,\n+\t and write some more of the results to dump file.  */\n+      if (flag_sched2_use_superblocks || flag_sched2_use_traces)\n+\tschedule_ebbs ();\n+      else\n+\tschedule_insns ();\n+    }\n #endif\n   return 0;\n }"}, {"sha": "2544338d646b1566285601f1cf0ef0ff2e848297", "filename": "gcc/sched-vis.c", "status": "modified", "additions": 34, "deletions": 11, "changes": 45, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsched-vis.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsched-vis.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-vis.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -29,13 +29,11 @@ along with GCC; see the file COPYING3.  If not see\n #include \"hard-reg-set.h\"\n #include \"basic-block.h\"\n #include \"real.h\"\n+#include \"insn-attr.h\"\n #include \"sched-int.h\"\n #include \"tree-pass.h\"\n \n static char *safe_concat (char *, char *, const char *);\n-static void print_exp (char *, const_rtx, int);\n-static void print_value (char *, const_rtx, int);\n-static void print_pattern (char *, const_rtx, int);\n \n #define BUF_LEN 2048\n \n@@ -425,7 +423,7 @@ print_exp (char *buf, const_rtx x, int verbose)\n /* Prints rtxes, I customarily classified as values.  They're constants,\n    registers, labels, symbols and memory accesses.  */\n \n-static void\n+void\n print_value (char *buf, const_rtx x, int verbose)\n {\n   char t[BUF_LEN];\n@@ -532,7 +530,7 @@ print_value (char *buf, const_rtx x, int verbose)\n \n /* The next step in insn detalization, its pattern recognition.  */\n \n-static void\n+void\n print_pattern (char *buf, const_rtx x, int verbose)\n {\n   char t1[BUF_LEN], t2[BUF_LEN], t3[BUF_LEN];\n@@ -643,10 +641,10 @@ print_pattern (char *buf, const_rtx x, int verbose)\n    depends now on sched.c inner variables ...)  */\n \n void\n-print_insn (char *buf, rtx x, int verbose)\n+print_insn (char *buf, const_rtx x, int verbose)\n {\n   char t[BUF_LEN];\n-  rtx insn = x;\n+  const_rtx insn = x;\n \n   switch (GET_CODE (x))\n     {\n@@ -681,7 +679,7 @@ print_insn (char *buf, rtx x, int verbose)\n \tstrcpy (t, \"call <...>\");\n #ifdef INSN_SCHEDULING\n       if (verbose && current_sched_info)\n-\tsprintf (buf, \"%s: %s\", (*current_sched_info->print_insn) (x, 1), t);\n+\tsprintf (buf, \"%s: %s\", (*current_sched_info->print_insn) (insn, 1), t);\n       else\n #endif\n \tsprintf (buf, \" %4d %s\", INSN_UID (insn), t);\n@@ -702,7 +700,6 @@ print_insn (char *buf, rtx x, int verbose)\n     }\n }\t\t\t\t/* print_insn */\n \n-\n /* Emit a slim dump of X (an insn) to the file F, including any register\n    note attached to the instruction.  */\n void\n@@ -735,11 +732,22 @@ debug_insn_slim (rtx x)\n    include more information on the basic blocks.  */\n void\n print_rtl_slim_with_bb (FILE *f, rtx first, int flags)\n+{\n+  print_rtl_slim (f, first, NULL, -1, flags);\n+}\n+\n+/* Same as above, but stop at LAST or when COUNT == 0.  \n+   If COUNT < 0 it will stop only at LAST or NULL rtx.  */\n+void\n+print_rtl_slim (FILE *f, rtx first, rtx last, int count, int flags)\n {\n   basic_block current_bb = NULL;\n-  rtx insn;\n+  rtx insn, tail;\n \n-  for (insn = first; NULL != insn; insn = NEXT_INSN (insn))\n+  tail = last ? NEXT_INSN (last) : NULL_RTX;\n+  for (insn = first; \n+       (insn != NULL) && (insn != tail) && (count != 0); \n+       insn = NEXT_INSN (insn))\n     {\n       if ((flags & TDF_BLOCKS)\n \t  && (INSN_P (insn) || GET_CODE (insn) == NOTE)\n@@ -759,6 +767,21 @@ print_rtl_slim_with_bb (FILE *f, rtx first, int flags)\n \t  dump_bb_info (current_bb, false, true, flags, \";; \", f);\n \t  current_bb = NULL;\n \t}\n+      if (count > 0)\n+        count--;\n     }\n }\n \n+void \n+debug_bb_slim (struct basic_block_def *bb)\n+{\n+  print_rtl_slim (stderr, BB_HEAD (bb), BB_END (bb), -1, 32);\n+}\n+\n+void\n+debug_bb_n_slim (int n)\n+{\n+  struct basic_block_def *bb = BASIC_BLOCK (n);\n+  debug_bb_slim (bb);\n+}\n+"}, {"sha": "7d56d3ba078eee01648c1031217ac5155bfee602", "filename": "gcc/sel-sched-dump.c", "status": "added", "additions": 945, "deletions": 0, "changes": 945, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsel-sched-dump.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsel-sched-dump.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsel-sched-dump.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -0,0 +1,945 @@\n+/* Instruction scheduling pass.   Log dumping infrastructure.\n+   Copyright (C) 2006, 2007, 2008 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"toplev.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"hard-reg-set.h\"\n+#include \"regs.h\"\n+#include \"function.h\"\n+#include \"flags.h\"\n+#include \"insn-config.h\"\n+#include \"insn-attr.h\"\n+#include \"params.h\"\n+#include \"output.h\"\n+#include \"basic-block.h\"\n+#include \"cselib.h\"\n+#include \"sel-sched-ir.h\"\n+#include \"sel-sched-dump.h\"\n+\f\n+\n+/* These variables control high-level pretty printing.  */\n+static int sel_dump_cfg_flags = SEL_DUMP_CFG_FLAGS;\n+static int sel_debug_cfg_flags = SEL_DUMP_CFG_FLAGS;\n+\n+/* True when a cfg should be dumped.  */\n+static bool sel_dump_cfg_p;\n+\n+/* Variables that are used to build the cfg dump file name.  */\n+static const char * const sel_debug_cfg_root = \"./\";\n+static const char * const sel_debug_cfg_root_postfix_default = \"\";\n+static const char *sel_debug_cfg_root_postfix = \"\";\n+static int sel_dump_cfg_fileno = -1;\n+static int sel_debug_cfg_fileno = -1;\n+\n+/* When this flag is on, we are dumping to the .dot file.\n+   When it is off, we are dumping to log.\n+   This is useful to differentiate formatting between log and .dot\n+   files.  */\n+bool sched_dump_to_dot_p = false;\n+\n+/* Controls how insns from a fence list should be dumped.  */\n+static int dump_flist_insn_flags = (DUMP_INSN_UID | DUMP_INSN_BBN\n+                                    | DUMP_INSN_SEQNO);\n+\f\n+\n+/* The variable used to hold the value of sched_dump when temporarily\n+   switching dump output to the other source, e.g. the .dot file.  */\n+static FILE *saved_sched_dump = NULL;\n+\n+/* Switch sched_dump to TO.  It must not be called twice.  */\n+static void\n+switch_dump (FILE *to)\n+{\n+  gcc_assert (saved_sched_dump == NULL);\n+  \n+  saved_sched_dump = sched_dump;\n+  sched_dump = to;\n+}\n+\n+/* Restore previously switched dump.  */\n+static void\n+restore_dump (void)\n+{\n+  sched_dump = saved_sched_dump;\n+  saved_sched_dump = NULL;\n+}\n+\f\n+\n+/* Functions for dumping instructions, av sets, and exprs.  */ \n+\n+/* Default flags for dumping insns.  */\n+static int dump_insn_rtx_flags = DUMP_INSN_RTX_PATTERN;\n+\n+/* Default flags for dumping vinsns.  */\n+static int dump_vinsn_flags = (DUMP_VINSN_INSN_RTX | DUMP_VINSN_TYPE\n+\t\t\t       | DUMP_VINSN_COUNT);\n+\n+/* Default flags for dumping expressions.  */\n+static int dump_expr_flags = DUMP_EXPR_ALL;\n+\n+/* Default flags for dumping insns when debugging.  */\n+static int debug_insn_rtx_flags = DUMP_INSN_RTX_ALL;\n+\n+/* Default flags for dumping vinsns when debugging.  */\n+static int debug_vinsn_flags = DUMP_VINSN_ALL;\n+\n+/* Default flags for dumping expressions when debugging.  */\n+static int debug_expr_flags = DUMP_EXPR_ALL;\n+\n+/* Controls how an insn from stream should be dumped when debugging.  */\n+static int debug_insn_flags = DUMP_INSN_ALL;\n+\n+/* Print an rtx X.  */\n+void\n+sel_print_rtl (rtx x)\n+{\n+  print_rtl_single (sched_dump, x);\n+}\n+\n+/* Dump insn INSN honoring FLAGS.  */\n+void\n+dump_insn_rtx_1 (rtx insn, int flags)\n+{\n+  int all;\n+\n+  /* flags == -1 also means dumping all.  */\n+  all = (flags & 1);;\n+  if (all)\n+    flags |= DUMP_INSN_RTX_ALL;\n+\n+  sel_print (\"(\");\n+\n+  if (flags & DUMP_INSN_RTX_UID)\n+    sel_print (\"%d;\", INSN_UID (insn));\n+\n+  if (flags & DUMP_INSN_RTX_PATTERN)\n+    {\n+      char buf[2048];\n+\n+      print_insn (buf, insn, 0);\n+      sel_print (\"%s;\", buf);\n+    }\n+\n+  if (flags & DUMP_INSN_RTX_BBN)\n+    {\n+      basic_block bb = BLOCK_FOR_INSN (insn);\n+\n+      sel_print (\"bb:%d;\", bb != NULL ? bb->index : -1);\n+    }\n+\n+  sel_print (\")\");\n+}\n+\n+\n+/* Dump INSN with default flags.  */\n+void\n+dump_insn_rtx (rtx insn)\n+{\n+  dump_insn_rtx_1 (insn, dump_insn_rtx_flags);\n+}\n+\n+\n+/* Dump INSN to stderr.  */\n+void\n+debug_insn_rtx (rtx insn)\n+{\n+  switch_dump (stderr);\n+  dump_insn_rtx_1 (insn, debug_insn_rtx_flags);\n+  sel_print (\"\\n\");\n+  restore_dump ();\n+}\n+\n+/* Dump vinsn VI honoring flags.  */\n+void\n+dump_vinsn_1 (vinsn_t vi, int flags)\n+{\n+  int all;\n+\n+  /* flags == -1 also means dumping all.  */\n+  all = flags & 1;\n+  if (all)\n+    flags |= DUMP_VINSN_ALL;\n+\n+  sel_print (\"(\");\n+\n+  if (flags & DUMP_VINSN_INSN_RTX)\n+    dump_insn_rtx_1 (VINSN_INSN_RTX (vi), dump_insn_rtx_flags | all);\n+\n+  if (flags & DUMP_VINSN_TYPE)\n+    sel_print (\"type:%s;\", GET_RTX_NAME (VINSN_TYPE (vi)));\n+\n+  if (flags & DUMP_VINSN_COUNT)\n+    sel_print (\"count:%d;\", VINSN_COUNT (vi));\n+\n+  if (flags & DUMP_VINSN_COST)\n+    {\n+      int cost = vi->cost;\n+\n+      if (cost != -1)\n+\tsel_print (\"cost:%d;\", cost);\n+    }\n+\n+  sel_print (\")\");\n+}\n+\n+/* Dump vinsn VI with default flags.  */\n+void\n+dump_vinsn (vinsn_t vi)\n+{\n+  dump_vinsn_1 (vi, dump_vinsn_flags);\n+}\n+\n+/* Dump vinsn VI to stderr.  */\n+void\n+debug_vinsn (vinsn_t vi)\n+{\n+  switch_dump (stderr);\n+  dump_vinsn_1 (vi, debug_vinsn_flags);\n+  sel_print (\"\\n\"); \n+  restore_dump ();\n+}\n+\n+/* Dump EXPR honoring flags.  */\n+void\n+dump_expr_1 (expr_t expr, int flags)\n+{\n+  int all;\n+\n+  /* flags == -1 also means dumping all.  */\n+  all = flags & 1;\n+  if (all)\n+    flags |= DUMP_EXPR_ALL;\n+\n+  sel_print (\"[\");\n+\n+  if (flags & DUMP_EXPR_VINSN)\n+    dump_vinsn_1 (EXPR_VINSN (expr), dump_vinsn_flags | all);\n+\n+  if (flags & DUMP_EXPR_SPEC)\n+    {\n+      int spec = EXPR_SPEC (expr);\n+\n+      if (spec != 0)\n+\tsel_print (\"spec:%d;\", spec);\n+    }\n+\n+  if (flags & DUMP_EXPR_USEFULNESS)\n+    {\n+      int use = EXPR_USEFULNESS (expr);\n+\n+      if (use != REG_BR_PROB_BASE)\n+        sel_print (\"use:%d;\", use);\n+    }\n+\n+  if (flags & DUMP_EXPR_PRIORITY)\n+    sel_print (\"prio:%d;\", EXPR_PRIORITY (expr));\n+\n+  if (flags & DUMP_EXPR_SCHED_TIMES)\n+    {\n+      int times = EXPR_SCHED_TIMES (expr);\n+\n+      if (times != 0)\n+\tsel_print (\"times:%d;\", times);\n+    }\n+\n+  if (flags & DUMP_EXPR_SPEC_DONE_DS)\n+    {\n+      ds_t spec_done_ds = EXPR_SPEC_DONE_DS (expr);\n+\n+      if (spec_done_ds != 0)\n+\tsel_print (\"ds:%d;\", spec_done_ds);\n+    }\n+\n+  if (flags & DUMP_EXPR_ORIG_BB)\n+    {\n+      int orig_bb = EXPR_ORIG_BB_INDEX (expr);\n+\n+      if (orig_bb != 0)\n+\tsel_print (\"orig_bb:%d;\", orig_bb);\n+    }\n+  \n+  if (EXPR_TARGET_AVAILABLE (expr) < 1)\n+    sel_print (\"target:%d;\", EXPR_TARGET_AVAILABLE (expr));\n+  sel_print (\"]\");\n+}\n+\n+/* Dump expression EXPR with default flags.  */\n+void\n+dump_expr (expr_t expr)\n+{\n+  dump_expr_1 (expr, dump_expr_flags);\n+}\n+\n+/* Dump expression EXPR to stderr.  */\n+void\n+debug_expr (expr_t expr)\n+{\n+  switch_dump (stderr);\n+  dump_expr_1 (expr, debug_expr_flags);\n+  sel_print (\"\\n\");\n+  restore_dump ();\n+}\n+\n+/* Dump insn I honoring FLAGS.  */\n+void\n+dump_insn_1 (insn_t i, int flags)\n+{\n+  int all;\n+\n+  all = flags & 1;\n+  if (all)\n+    flags |= DUMP_INSN_ALL;\n+\n+  if (!sched_dump_to_dot_p)\n+    sel_print (\"(\");\n+\n+  if (flags & DUMP_INSN_EXPR)\n+    {\n+      dump_expr_1 (INSN_EXPR (i), dump_expr_flags | all);\n+      sel_print (\";\");\n+    }\n+  else if (flags & DUMP_INSN_PATTERN)\n+    {\n+      dump_insn_rtx_1 (i, DUMP_INSN_RTX_PATTERN | all);\n+      sel_print (\";\");\n+    }\n+  else if (flags & DUMP_INSN_UID)\n+    sel_print (\"uid:%d;\", INSN_UID (i));\n+\n+  if (flags & DUMP_INSN_SEQNO)\n+    sel_print (\"seqno:%d;\", INSN_SEQNO (i));\n+\n+  if (flags & DUMP_INSN_SCHED_CYCLE)\n+    {\n+      int cycle = INSN_SCHED_CYCLE (i);\n+\n+      if (cycle != 0)\n+\tsel_print (\"cycle:%d;\", cycle);\n+    }\n+\n+  if (!sched_dump_to_dot_p)\n+    sel_print (\")\");\n+}\n+\n+/* Dump insn I with default flags.  */\n+void\n+dump_insn (insn_t i)\n+{\n+  dump_insn_1 (i, DUMP_INSN_EXPR | DUMP_INSN_SCHED_CYCLE);\n+}\n+\n+/* Dump INSN to stderr.  */\n+void\n+debug_insn (insn_t insn)\n+{\n+  switch_dump (stderr);\n+  dump_insn_1 (insn, debug_insn_flags);\n+  sel_print (\"\\n\");\n+  restore_dump ();\n+}\n+\n+/* Dumps av_set AV.  */\n+void\n+dump_av_set (av_set_t av)\n+{\n+  av_set_iterator i;\n+  expr_t expr;\n+  \n+  if (!sched_dump_to_dot_p)\n+    sel_print (\"{\");\n+  \n+  FOR_EACH_EXPR (expr, i, av)\n+    {\n+      dump_expr (expr);\n+      if (!sched_dump_to_dot_p)\n+        sel_print (\" \");\n+      else\n+        sel_print (\"\\n\");\n+    }\n+  \n+  if (!sched_dump_to_dot_p)\n+    sel_print (\"}\");\n+}\n+\n+/* Dumps lvset LV.  */\n+void\n+dump_lv_set (regset lv)\n+{\n+  sel_print (\"{\");\n+\n+  /* This code was adapted from flow.c: dump_regset ().  */\n+  if (lv == NULL)\n+    sel_print (\"nil\");\n+  else\n+    {\n+      unsigned i;\n+      reg_set_iterator rsi;\n+      int count = 0;\n+      \n+      EXECUTE_IF_SET_IN_REG_SET (lv, 0, i, rsi)\n+        {\n+          sel_print (\" %d\", i);\n+          if (i < FIRST_PSEUDO_REGISTER)\n+            {\n+              sel_print (\" [%s]\", reg_names[i]);\n+              ++count;\n+            }\n+          \n+          ++count;\n+          \n+          if (sched_dump_to_dot_p && count == 12)\n+            {\n+              count = 0;\n+              sel_print (\"\\n\");\n+            }\n+        }\n+    }\n+  \n+  sel_print (\"}\\n\");\n+}\n+\n+/* Dumps a list of instructions pointed to by P.  */\n+static void\n+dump_ilist (ilist_t p)\n+{\n+  while (p)\n+    {\n+      dump_insn (ILIST_INSN (p));\n+      p = ILIST_NEXT (p);\n+    }\n+}\n+\n+/* Dumps a list of boundaries pointed to by BNDS.  */\n+void\n+dump_blist (blist_t bnds)\n+{\n+  for (; bnds; bnds = BLIST_NEXT (bnds))\n+    {\n+      bnd_t bnd = BLIST_BND (bnds);\n+      \n+      sel_print (\"[to: %d; ptr: \", INSN_UID (BND_TO (bnd)));\n+      dump_ilist (BND_PTR (bnd));\n+      sel_print (\"] \");\n+    }\n+}\n+\n+/* Dumps a list of fences pointed to by L.  */\n+void\n+dump_flist (flist_t l)\n+{\n+  while (l)\n+    {\n+      dump_insn_1 (FENCE_INSN (FLIST_FENCE (l)), dump_flist_insn_flags);\n+      sel_print (\" \");\n+      l = FLIST_NEXT (l);\n+    }\n+}\n+\n+/* Dumps an insn vector SUCCS.  */\n+void\n+dump_insn_vector (rtx_vec_t succs)\n+{\n+  int i;\n+  rtx succ;\n+  \n+  for (i = 0; VEC_iterate (rtx, succs, i, succ); i++)\n+    if (succ)\n+      dump_insn (succ);\n+    else\n+      sel_print (\"NULL \");\n+}\n+\n+/* Dumps a hard reg set SET to FILE using PREFIX.  */\n+static void\n+print_hard_reg_set (FILE *file, const char *prefix, HARD_REG_SET set)\n+{\n+  int i;\n+\n+  fprintf (file, \"%s{ \", prefix);\n+  for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n+    {\n+      if (TEST_HARD_REG_BIT (set, i))\n+\tfprintf (file, \"%d \", i);\n+    }\n+  fprintf (file, \"}\\n\");\n+}\n+\n+/* Dumps a hard reg set SET using PREFIX.  */\n+void\n+dump_hard_reg_set (const char *prefix, HARD_REG_SET set)\n+{\n+  print_hard_reg_set (sched_dump, prefix, set);\n+}\n+\n+/* Pretty print INSN.  This is used as a hook.  */\n+const char *\n+sel_print_insn (const_rtx insn, int aligned ATTRIBUTE_UNUSED)\n+{\n+  static char buf[80];\n+\n+  /* '+' before insn means it is a new cycle start and it's not been \n+     scheduled yet.  '>' - has been scheduled.  */\n+  if (s_i_d && INSN_LUID (insn) > 0)\n+    if (GET_MODE (insn) == TImode)\n+      sprintf (buf, \"%s %4d\", \n+               INSN_SCHED_TIMES (insn) > 0 ? \"> \" : \"< \", \n+               INSN_UID (insn));\n+    else\n+      sprintf (buf, \"%s %4d\", \n+               INSN_SCHED_TIMES (insn) > 0 ? \"! \" : \"  \", \n+               INSN_UID (insn));\n+  else\n+    if (GET_MODE (insn) == TImode)\n+      sprintf (buf, \"+ %4d\", INSN_UID (insn));\n+    else\n+      sprintf (buf, \"  %4d\", INSN_UID (insn));\n+\n+  return buf;\n+}\n+\f\n+\n+/* Functions for pretty printing of CFG.  */\n+\n+/* Replace all occurencies of STR1 to STR2 in BUF.\n+   The BUF must be large enough to hold the result.  */\n+static void\n+replace_str_in_buf (char *buf, const char *str1, const char *str2)\n+{\n+  int buf_len = strlen (buf);\n+  int str1_len = strlen (str1);\n+  int str2_len = strlen (str2);\n+  int diff = str2_len - str1_len;\n+\n+  char *p = buf;\n+  do\n+    {\n+      p = strstr (p, str1);\n+      if (p) \n+\t{\n+\t  char *p1 = p + str1_len;\n+\t  /* Copy the rest of buf and '\\0'.  */\n+\t  int n = buf + buf_len - p1;\n+\t  int i;\n+\n+\t  /* Shift str by DIFF chars.  */\n+\t  if (diff > 0)\n+            for (i = n; i >= 0; i--)\n+              p1[i + diff] = p1[i];\n+\t  else\n+            for (i = 0; i <= n; i++)\n+              p1[i + diff] = p1[i];\n+\n+\t  /* Copy str2.  */\n+\t  for (i = 0; i < str2_len; i++)\n+\t    p[i] = str2[i];\n+\t    \n+\t  p += str2_len;\n+\t  buf_len += diff;\n+\t}\n+\n+    }\n+  while (p);\n+}\n+\n+/* Replace characters in BUF that have special meaning in .dot file.  */\n+void\n+sel_prepare_string_for_dot_label (char *buf)\n+{\n+  static char specials_from[7][2] = { \"<\", \">\", \"{\", \"|\", \"}\", \"\\\"\",\n+                                      \"\\n\" };\n+  static char specials_to[7][3] = { \"\\\\<\", \"\\\\>\", \"\\\\{\", \"\\\\|\", \"\\\\}\", \n+                                    \"\\\\\\\"\", \"\\\\l\" };\n+  unsigned i;\n+\n+  for (i = 0; i < 7; i++)\n+    replace_str_in_buf (buf, specials_from[i], specials_to[i]);\n+}\n+\n+/* Dump INSN with FLAGS.  */\n+static void\n+sel_dump_cfg_insn (insn_t insn, int flags)\n+{\n+  int insn_flags = DUMP_INSN_UID | DUMP_INSN_PATTERN;\n+\n+  if (sched_luids != NULL && INSN_LUID (insn) > 0)\n+    {\n+      if (flags & SEL_DUMP_CFG_INSN_SEQNO)\n+\tinsn_flags |= DUMP_INSN_SEQNO | DUMP_INSN_SCHED_CYCLE | DUMP_INSN_EXPR;\n+    }\n+\n+  dump_insn_1 (insn, insn_flags);\n+}\n+\n+/* Dump E to the dot file F.  */\n+static void\n+sel_dump_cfg_edge (FILE *f, edge e)\n+{\n+  int w;\n+  const char *color;\n+\n+  if (e->flags & EDGE_FALLTHRU)\n+    {\n+      w = 10;\n+      color = \", color = red\";\n+    }\n+  else if (e->src->next_bb == e->dest)\n+    {\n+      w = 3;\n+      color = \", color = blue\";\n+    }\n+  else\n+    {\n+      w = 1;\n+      color = \"\";\n+    }\n+\n+  fprintf (f, \"\\tbb%d -> bb%d [weight = %d%s];\\n\",\n+\t   e->src->index, e->dest->index, w, color);\n+}\n+\n+\n+/* Return true if BB has a predesessor from current region.\n+   TODO: Either make this function to trace back through empty block\n+   or just remove those empty blocks.  */\n+static bool\n+has_preds_in_current_region_p (basic_block bb)\n+{\n+  edge e;\n+  edge_iterator ei;\n+\n+  gcc_assert (!in_current_region_p (bb));\n+\n+  FOR_EACH_EDGE (e, ei, bb->preds)\n+    if (in_current_region_p (e->src))\n+      return true;\n+\n+  return false;\n+}\n+\n+/* Dump a cfg region to the dot file F honoring FLAGS.  */\n+static void\n+sel_dump_cfg_2 (FILE *f, int flags)\n+{\n+  basic_block bb;\n+\n+  sched_dump_to_dot_p = true;\n+  switch_dump (f);\n+\n+  fprintf (f, \"digraph G {\\n\"\n+\t   \"\\tratio = 2.25;\\n\"\n+\t   \"\\tnode [shape = record, fontsize = 9];\\n\");\n+\n+  if (flags & SEL_DUMP_CFG_FUNCTION_NAME)\n+    fprintf (f, \"function [label = \\\"%s\\\"];\\n\", current_function_name ());\n+\n+  FOR_EACH_BB (bb)\n+    {\n+      insn_t insn = BB_HEAD (bb);\n+      insn_t next_tail = NEXT_INSN (BB_END (bb));\n+      edge e;\n+      edge_iterator ei;\n+      bool in_region_p = ((flags & SEL_DUMP_CFG_CURRENT_REGION)\n+\t\t\t  && in_current_region_p (bb));\n+      bool full_p = (!(flags & SEL_DUMP_CFG_CURRENT_REGION)\n+\t\t     || in_region_p);\n+      bool some_p = full_p || has_preds_in_current_region_p (bb);\n+      const char *color;\n+      const char *style;\n+\n+      if (!some_p)\n+\tcontinue;\n+\n+      if ((flags & SEL_DUMP_CFG_CURRENT_REGION)\n+\t  && in_current_region_p (bb)\n+\t  && BLOCK_TO_BB (bb->index) == 0)\n+\tcolor = \"color = green, \";\n+      else\n+\tcolor = \"\";\n+\n+      if ((flags & SEL_DUMP_CFG_FENCES)\n+\t  && in_region_p)\n+\t{\n+\t  style = \"\";\n+\n+\t  if (!sel_bb_empty_p (bb))\n+\t    {\n+\t      bool first_p = true;\n+\t      insn_t tail = BB_END (bb);\n+\t      insn_t cur_insn;\n+\n+\t      cur_insn = bb_note (bb);\n+\n+\t      do\n+\t\t{\n+\t\t  fence_t fence;\n+\n+\t\t  cur_insn = NEXT_INSN (cur_insn);\n+\t\t  fence = flist_lookup (fences, cur_insn);\n+\n+\t\t  if (fence != NULL)\n+\t\t    {\n+\t\t      if (!FENCE_SCHEDULED_P (fence))\n+\t\t\t{\n+\t\t\t  if (first_p)\n+\t\t\t    color = \"color = red, \";\n+\t\t\t  else\n+\t\t\t    color = \"color = yellow, \";\n+\t\t\t}\n+\t\t      else\n+\t\t\tcolor = \"color = blue, \";\n+\t\t    }\n+\n+\t\t  first_p = false;\n+\t\t}\n+\t      while (cur_insn != tail);\n+\t    }\n+\t}\n+      else if (!full_p)\n+\tstyle = \"style = dashed, \";\n+      else\n+\tstyle = \"\";\n+\n+      fprintf (f, \"\\tbb%d [%s%slabel = \\\"{Basic block %d\", bb->index,\n+\t       style, color, bb->index);\n+\n+      if ((flags & SEL_DUMP_CFG_BB_LOOP)\n+\t  && bb->loop_father != NULL)\n+\tfprintf (f, \", loop %d\", bb->loop_father->num);\n+\n+      if (full_p\n+\t  && (flags & SEL_DUMP_CFG_BB_NOTES_LIST))\n+\t{\n+\t  insn_t notes = BB_NOTE_LIST (bb);\n+\n+\t  if (notes != NULL_RTX)\n+\t    {\n+\t      fprintf (f, \"|\");\n+\n+\t      /* For simplicity, we dump notes from note_list in reversed order\n+\t\t to that what they will appear in the code.  */\n+\t      while (notes != NULL_RTX)\n+\t\t{\n+\t\t  sel_dump_cfg_insn (notes, flags);\n+\t\t  fprintf (f, \"\\\\l\");\n+\n+\t\t  notes = PREV_INSN (notes);\n+\t\t}\n+\t    }\n+\t}\n+\n+      if (full_p\n+\t  && (flags & SEL_DUMP_CFG_AV_SET)\n+\t  && in_current_region_p (bb)\n+\t  && !sel_bb_empty_p (bb))\n+\t{\n+\t  fprintf (f, \"|\");\n+\n+\t  if (BB_AV_SET_VALID_P (bb))\n+\t    dump_av_set (BB_AV_SET (bb));\n+\t  else if (BB_AV_LEVEL (bb) == -1)\n+\t    fprintf (f, \"AV_SET needs update\");\n+\t}\n+\n+      if ((flags & SEL_DUMP_CFG_LV_SET)\n+\t  && !sel_bb_empty_p (bb))\n+ \t{\n+\t  fprintf (f, \"|\");\n+\n+\t  if (BB_LV_SET_VALID_P (bb))\n+\t    dump_lv_set (BB_LV_SET (bb));\n+\t  else\n+\t    fprintf (f, \"LV_SET needs update\");\n+\t}\n+\n+      if (full_p\n+\t  && (flags & SEL_DUMP_CFG_BB_INSNS))\n+\t{\n+\t  fprintf (f, \"|\");\n+\t  while (insn != next_tail)\n+\t    {\n+\t      sel_dump_cfg_insn (insn, flags);\n+\t      fprintf (f, \"\\\\l\");\n+\n+\t      insn = NEXT_INSN (insn);\n+\t    }\n+\t}\n+\n+      fprintf (f, \"}\\\"];\\n\");\n+\n+      FOR_EACH_EDGE (e, ei, bb->succs)\n+\tif (full_p || in_current_region_p (e->dest))\n+\t  sel_dump_cfg_edge (f, e);\n+    }\n+\n+  fprintf (f, \"}\");\n+\n+  restore_dump ();\n+  sched_dump_to_dot_p = false;\n+}\n+\n+/* Dump a cfg region to the file specified by TAG honoring flags.  \n+   The file is created by the function.  */\n+static void\n+sel_dump_cfg_1 (const char *tag, int flags)\n+{\n+  char *buf;\n+  int i;\n+  FILE *f;\n+\n+  ++sel_dump_cfg_fileno;\n+\n+  if (!sel_dump_cfg_p)\n+    return;\n+\n+  i = 1 + snprintf (NULL, 0, \"%s/%s%05d-%s.dot\", sel_debug_cfg_root,\n+\t\t    sel_debug_cfg_root_postfix, sel_dump_cfg_fileno, tag);\n+  buf = XNEWVEC (char, i);\n+  snprintf (buf, i, \"%s/%s%05d-%s.dot\", sel_debug_cfg_root,\n+\t    sel_debug_cfg_root_postfix, sel_dump_cfg_fileno, tag);\n+\n+  f = fopen (buf, \"w\");\n+\n+  if (f == NULL)\n+    fprintf (stderr, \"Can't create file: %s.\\n\", buf);\n+  else\n+    {\n+      sel_dump_cfg_2 (f, flags);\n+\n+      fclose (f);\n+    }\n+\n+  free (buf);\n+}\n+\n+/* Setup cfg dumping flags.  Used for debugging.  */\n+void\n+setup_dump_cfg_params (void)\n+{\n+  sel_dump_cfg_flags = SEL_DUMP_CFG_FLAGS;\n+  sel_dump_cfg_p = 0;\n+  sel_debug_cfg_root_postfix = sel_debug_cfg_root_postfix_default;\n+}\n+\n+/* Debug a cfg region with FLAGS.  */\n+void\n+sel_debug_cfg_1 (int flags)\n+{\n+  bool t1 = sel_dump_cfg_p;\n+  int t2 = sel_dump_cfg_fileno;\n+\n+  sel_dump_cfg_p = true;\n+  sel_dump_cfg_fileno = ++sel_debug_cfg_fileno;\n+\n+  sel_dump_cfg_1 (\"sel-debug-cfg\", flags);\n+\n+  sel_dump_cfg_fileno = t2;\n+  sel_dump_cfg_p = t1;\n+}\n+\f\n+/* Dumps av_set AV to stderr.  */\n+void\n+debug_av_set (av_set_t av)\n+{\n+  switch_dump (stderr);\n+  dump_av_set (av);\n+  sel_print (\"\\n\");\n+  restore_dump ();\n+}\n+\n+/* Dump LV to stderr.  */\n+void\n+debug_lv_set (regset lv)\n+{\n+  switch_dump (stderr);\n+  dump_lv_set (lv);\n+  sel_print (\"\\n\");\n+  restore_dump ();\n+}\n+\n+/* Dump an instruction list P to stderr.  */\n+void\n+debug_ilist (ilist_t p)\n+{\n+  switch_dump (stderr);\n+  dump_ilist (p);\n+  sel_print (\"\\n\");\n+  restore_dump ();\n+}\n+\n+/* Dump a boundary list BNDS to stderr.  */\n+void\n+debug_blist (blist_t bnds)\n+{\n+  switch_dump (stderr);\n+  dump_blist (bnds);\n+  sel_print (\"\\n\");\n+  restore_dump ();\n+}\n+\n+/* Dump an insn vector SUCCS.  */\n+void\n+debug_insn_vector (rtx_vec_t succs)\n+{\n+  switch_dump (stderr);\n+  dump_insn_vector (succs);\n+  sel_print (\"\\n\");\n+  restore_dump ();\n+}\n+\n+/* Dump a hard reg set SET to stderr.  */\n+void\n+debug_hard_reg_set (HARD_REG_SET set)\n+{\n+  switch_dump (stderr);\n+  dump_hard_reg_set (\"\", set);\n+  sel_print (\"\\n\");\n+  restore_dump ();\n+}\n+\n+/* Debug a cfg region with default flags.  */\n+void\n+sel_debug_cfg (void)\n+{\n+  sel_debug_cfg_1 (sel_debug_cfg_flags);\n+}\n+\n+/* Print a current cselib value for X's address to stderr.  */\n+rtx\n+debug_mem_addr_value (rtx x)\n+{\n+  rtx t, addr;\n+\n+  gcc_assert (MEM_P (x));\n+  t = shallow_copy_rtx (x);\n+  if (cselib_lookup (XEXP (t, 0), Pmode, 0))\n+    XEXP (t, 0) = cselib_subst_to_values (XEXP (t, 0));\n+\n+  t = canon_rtx (t);\n+  addr = get_addr (XEXP (t, 0));\n+  debug_rtx (t);\n+  debug_rtx (addr);\n+  return t;\n+}\n+\n+"}, {"sha": "70750f9cdcd22709e50e32e5a6e0f91c62d1fe4b", "filename": "gcc/sel-sched-dump.h", "status": "added", "additions": 241, "deletions": 0, "changes": 241, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsel-sched-dump.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsel-sched-dump.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsel-sched-dump.h?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -0,0 +1,241 @@\n+/* Instruction scheduling pass.  Log dumping infrastructure.  \n+   Copyright (C) 2006, 2007, 2008 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+\n+#ifndef GCC_SEL_SCHED_DUMP_H\n+#define GCC_SEL_SCHED_DUMP_H\n+\n+#include \"sel-sched-ir.h\"\n+\n+\n+/* These values control the dumping of control flow graph to the .dot file.  */\n+enum sel_dump_cfg_def\n+  {\n+    /* Dump only current region.  */\n+    SEL_DUMP_CFG_CURRENT_REGION = 2,\n+\n+    /* Dump note_list for this bb.  */\n+    SEL_DUMP_CFG_BB_NOTES_LIST = 4,\n+\n+    /* Dump availability set from the bb header.  */\n+    SEL_DUMP_CFG_AV_SET = 8,\n+\n+    /* Dump liveness set from the bb header.  */\n+    SEL_DUMP_CFG_LV_SET = 16,\n+\n+    /* Dump insns of the given block.  */\n+    SEL_DUMP_CFG_BB_INSNS = 32,\n+\n+    /* Show current fences when dumping cfg.  */\n+    SEL_DUMP_CFG_FENCES = 64,\n+\n+    /* Show insn's seqnos when dumping cfg.  */\n+    SEL_DUMP_CFG_INSN_SEQNO = 128,\n+\n+    /* Dump function name when dumping cfg.  */\n+    SEL_DUMP_CFG_FUNCTION_NAME = 256,\n+\n+    /* Dump loop father number of the given bb.  */\n+    SEL_DUMP_CFG_BB_LOOP = 512,\n+\n+    /* The default flags for cfg dumping.  */\n+    SEL_DUMP_CFG_FLAGS = (SEL_DUMP_CFG_CURRENT_REGION   \n+                          | SEL_DUMP_CFG_BB_NOTES_LIST   \n+                          | SEL_DUMP_CFG_AV_SET         \n+                          | SEL_DUMP_CFG_LV_SET         \n+                          | SEL_DUMP_CFG_BB_INSNS       \n+                          | SEL_DUMP_CFG_FENCES         \n+                          | SEL_DUMP_CFG_INSN_SEQNO     \n+                          | SEL_DUMP_CFG_BB_LOOP)\n+  };\n+\n+/* These values control the dumping of insns containing in expressions.  */\n+enum dump_insn_rtx_def\n+  {\n+    /* Dump insn's UID.  */\n+    DUMP_INSN_RTX_UID = 2,\n+\n+    /* Dump insn's pattern.  */\n+    DUMP_INSN_RTX_PATTERN = 4,\n+\n+    /* Dump insn's basic block number.  */\n+    DUMP_INSN_RTX_BBN = 8,\n+\n+    /* Dump all of the above.  */\n+    DUMP_INSN_RTX_ALL = (DUMP_INSN_RTX_UID | DUMP_INSN_RTX_PATTERN\n+\t\t\t | DUMP_INSN_RTX_BBN)\n+  };\n+\n+extern void dump_insn_rtx_1 (rtx, int);\n+extern void dump_insn_rtx (rtx);\n+extern void debug_insn_rtx (rtx);\n+\n+/* These values control dumping of vinsns.  The meaning of different fields\n+   of a vinsn is explained in sel-sched-ir.h.  */\n+enum dump_vinsn_def\n+  {\n+    /* Dump the insn behind this vinsn.  */\n+    DUMP_VINSN_INSN_RTX = 2,\n+\n+    /* Dump vinsn's type.  */\n+    DUMP_VINSN_TYPE = 4,\n+\n+    /* Dump vinsn's count.  */\n+    DUMP_VINSN_COUNT = 8,\n+\n+    /* Dump the cost (default latency) of the insn behind this vinsn.  */\n+    DUMP_VINSN_COST = 16,\n+\n+    /* Dump all of the above.  */\n+    DUMP_VINSN_ALL = (DUMP_VINSN_INSN_RTX | DUMP_VINSN_TYPE | DUMP_VINSN_COUNT\n+\t\t      | DUMP_VINSN_COST)\n+  };\n+\n+extern void dump_vinsn_1 (vinsn_t, int);\n+extern void dump_vinsn (vinsn_t);\n+extern void debug_vinsn (vinsn_t);\n+\n+/* These values control dumping of expressions.  The meaning of the fields\n+   is explained in sel-sched-ir.h.  */\n+enum dump_expr_def\n+  {\n+    /* Dump the vinsn behind this expression.  */\n+    DUMP_EXPR_VINSN = 2,\n+    \n+    /* Dump expression's SPEC parameter.  */\n+    DUMP_EXPR_SPEC = 4,\n+\n+    /* Dump expression's priority.  */\n+    DUMP_EXPR_PRIORITY = 8,\n+\n+    /* Dump the number of times this expression was scheduled.  */\n+    DUMP_EXPR_SCHED_TIMES = 16,\n+\n+    /* Dump speculative status of the expression.  */\n+    DUMP_EXPR_SPEC_DONE_DS = 32,\n+\n+    /* Dump the basic block number which originated this expression.  */\n+    DUMP_EXPR_ORIG_BB = 64,\n+\n+    /* Dump expression's usefulness.  */\n+    DUMP_EXPR_USEFULNESS = 128,\n+\n+    /* Dump all of the above.  */\n+    DUMP_EXPR_ALL = (DUMP_EXPR_VINSN | DUMP_EXPR_SPEC | DUMP_EXPR_PRIORITY\n+\t\t     | DUMP_EXPR_SCHED_TIMES | DUMP_EXPR_SPEC_DONE_DS\n+\t\t     | DUMP_EXPR_ORIG_BB | DUMP_EXPR_USEFULNESS)\n+  };\n+\n+extern void dump_expr_1 (expr_t, int);\n+extern void dump_expr (expr_t);\n+extern void debug_expr (expr_t);\n+\n+/* A enumeration for dumping flags of an insn.  The difference from \n+   dump_insn_rtx_def is that these fields are for insns in stream only.  */\n+enum dump_insn_def\n+{\n+  /* Dump expression of this insn.  */\n+  DUMP_INSN_EXPR = 2,\n+\n+  /* Dump insn's seqno.  */\n+  DUMP_INSN_SEQNO = 4,\n+\n+  /* Dump the cycle on which insn was scheduled.  */\n+  DUMP_INSN_SCHED_CYCLE = 8,\n+\n+  /* Dump insn's UID.  */\n+  DUMP_INSN_UID = 16,\n+\n+  /* Dump insn's pattern.  */\n+  DUMP_INSN_PATTERN = 32,\n+\n+  /* Dump insn's basic block number.  */\n+  DUMP_INSN_BBN = 64,\n+\n+  /* Dump all of the above.  */\n+  DUMP_INSN_ALL = (DUMP_INSN_EXPR | DUMP_INSN_SEQNO | DUMP_INSN_BBN\n+\t\t   | DUMP_INSN_SCHED_CYCLE | DUMP_INSN_UID | DUMP_INSN_PATTERN)\n+};\n+\n+extern void dump_insn_1 (insn_t, int);\n+extern void dump_insn (insn_t);\n+extern void debug_insn (insn_t);\n+\n+extern void sel_prepare_string_for_dot_label (char *);\n+\n+/* When this flag is on, we are dumping to the .dot file.\n+   When it is off, we are dumping to log.  */\n+extern bool sched_dump_to_dot_p;\n+\n+/* This macro acts like printf but dumps information to the .dot file.  \n+   Used when dumping control flow.  */\n+#define sel_print_to_dot(...)                           \\\n+  do {                                                  \\\n+    int __j = 1 + 2 * snprintf (NULL, 0, __VA_ARGS__);  \\\n+    char *__s = XALLOCAVEC (char, __j);                 \\\n+    snprintf (__s, __j, __VA_ARGS__);                   \\\n+    sel_prepare_string_for_dot_label (__s);             \\\n+    fprintf (sched_dump, \"%s\", __s);                    \\\n+  } while (0)\n+\n+/* This macro acts like printf but dumps to the sched_dump file.  */\n+#define sel_print(...)\t\t\t\t\t\\\n+  do {\t\t\t\t\t\t\t\\\n+    if (sched_dump_to_dot_p)                            \\\n+      sel_print_to_dot (__VA_ARGS__);                   \\\n+    else                                                \\\n+      fprintf (sched_dump, __VA_ARGS__);                \\\n+  } while (0)\n+\f\n+\n+/* Functions from sel-sched-dump.c.  */\n+extern const char * sel_print_insn (const_rtx, int);\n+extern void free_sel_dump_data (void);\n+\n+extern void block_start (void);\n+extern void block_finish (void);\n+extern int get_print_blocks_num (void);\n+extern void line_start (void);\n+extern void line_finish (void);\n+\n+extern void sel_print_rtl (rtx x);\n+extern void dump_insn_1 (insn_t, int);\n+extern void dump_insn (insn_t);\n+extern void dump_insn_vector (rtx_vec_t);\n+extern void dump_expr (expr_t);\n+extern void dump_used_regs (bitmap);\n+extern void dump_av_set (av_set_t);\n+extern void dump_lv_set (regset);\n+extern void dump_blist (blist_t);\n+extern void dump_flist (flist_t);\n+extern void dump_hard_reg_set (const char *, HARD_REG_SET);\n+extern void sel_debug_cfg_1 (int);\n+extern void sel_debug_cfg (void);\n+extern void setup_dump_cfg_params (void);\n+\n+/* Debug functions.  */\n+extern void debug_expr (expr_t);\n+extern void debug_av_set (av_set_t);\n+extern void debug_lv_set (regset);\n+extern void debug_ilist (ilist_t);\n+extern void debug_blist (blist_t);\n+extern void debug_insn_vector (rtx_vec_t);\n+extern void debug_hard_reg_set (HARD_REG_SET);\n+extern rtx debug_mem_addr_value (rtx);\n+#endif"}, {"sha": "856fb4259c7510c0820c5fe0f004bd0dd20200a1", "filename": "gcc/sel-sched-ir.c", "status": "added", "additions": 6049, "deletions": 0, "changes": 6049, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsel-sched-ir.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsel-sched-ir.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsel-sched-ir.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed"}, {"sha": "4bf21b21263bba13f556de1748bd4cf9c588304c", "filename": "gcc/sel-sched-ir.h", "status": "added", "additions": 1627, "deletions": 0, "changes": 1627, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsel-sched-ir.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsel-sched-ir.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsel-sched-ir.h?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -0,0 +1,1627 @@\n+/* Instruction scheduling pass.  This file contains definitions used\n+   internally in the scheduler.\n+   Copyright (C) 2006, 2007, 2008 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#ifndef GCC_SEL_SCHED_IR_H\n+#define GCC_SEL_SCHED_IR_H\n+\n+/* For state_t.  */\n+#include \"insn-attr.h\"\n+/* For regset_head.  */\n+#include \"basic-block.h\"\n+/* For reg_note.  */\n+#include \"rtl.h\"\n+#include \"ggc.h\"\n+#include \"bitmap.h\"\n+#include \"vecprim.h\"\n+#include \"sched-int.h\"\n+#include \"cfgloop.h\"\n+\n+/* tc_t is a short for target context.  This is a state of the target\n+   backend.  */\n+typedef void *tc_t;\n+\n+/* List data types used for av sets, fences, paths, and boundaries.  */\n+\n+/* Forward declarations for types that are part of some list nodes.  */\n+struct _list_node;\n+\n+/* List backend.  */\n+typedef struct _list_node *_list_t;\n+#define _LIST_NEXT(L) ((L)->next)\n+\n+/* Instruction data that is part of vinsn type.  */\n+struct idata_def;\n+typedef struct idata_def *idata_t;\n+\n+/* A virtual instruction, i.e. an instruction as seen by the scheduler.  */\n+struct vinsn_def;\n+typedef struct vinsn_def *vinsn_t;\n+\n+/* RTX list.\n+   This type is the backend for ilist.  */\n+typedef _list_t _xlist_t;\n+#define _XLIST_X(L) ((L)->u.x)\n+#define _XLIST_NEXT(L) (_LIST_NEXT (L))\n+\n+/* Instruction.  */\n+typedef rtx insn_t;\n+\n+/* List of insns.  */\n+typedef _xlist_t ilist_t;\n+#define ILIST_INSN(L) (_XLIST_X (L))\n+#define ILIST_NEXT(L) (_XLIST_NEXT (L))\n+\n+/* This lists possible transformations that done locally, i.e. in \n+   moveup_expr.  */\n+enum local_trans_type\n+  {\n+    TRANS_SUBSTITUTION,\n+    TRANS_SPECULATION\n+  };\n+\n+/* This struct is used to record the history of expression's \n+   transformations.  */\n+struct expr_history_def_1\n+{\n+  /* UID of the insn.  */\n+  unsigned uid;\n+\n+  /* How the expression looked like.  */\n+  vinsn_t old_expr_vinsn;\n+\n+  /* How the expression looks after the transformation.  */\n+  vinsn_t new_expr_vinsn;\n+\n+  /* And its speculative status.  */\n+  ds_t spec_ds;\n+\n+  /* Type of the transformation.  */\n+  enum local_trans_type type;\n+};\n+\n+typedef struct expr_history_def_1 expr_history_def;\n+\n+DEF_VEC_O (expr_history_def);\n+DEF_VEC_ALLOC_O (expr_history_def, heap);\n+\n+/* Expression information.  */\n+struct _expr\n+{\n+  /* Insn description.  */\n+  vinsn_t vinsn;\n+\n+  /* SPEC is the degree of speculativeness.\n+     FIXME: now spec is increased when an rhs is moved through a\n+     conditional, thus showing only control speculativeness.  In the\n+     future we'd like to count data spec separately to allow a better\n+     control on scheduling.  */\n+  int spec;\n+\n+  /* Degree of speculativeness measured as probability of executing \n+     instruction's original basic block given relative to \n+     the current scheduling point.  */\n+  int usefulness;\n+\n+  /* A priority of this expression.  */\n+  int priority;\n+\n+  /* A priority adjustment of this expression.  */\n+  int priority_adj;\n+\n+  /* Number of times the insn was scheduled.  */\n+  int sched_times;\n+\n+  /* A basic block index this was originated from.  Zero when there is \n+     more than one originator.  */\n+  int orig_bb_index;\n+\n+  /* Instruction should be of SPEC_DONE_DS type in order to be moved to this\n+     point.  */\n+  ds_t spec_done_ds;\n+\n+  /* SPEC_TO_CHECK_DS hold speculation types that should be checked\n+     (used only during move_op ()).  */\n+  ds_t spec_to_check_ds;\n+\n+  /* Cycle on which original insn was scheduled.  Zero when it has not yet \n+     been scheduled or more than one originator.  */\n+  int orig_sched_cycle;\n+\n+  /* This vector contains the history of insn's transformations.  */\n+  VEC(expr_history_def, heap) *history_of_changes;\n+\n+  /* True (1) when original target (register or memory) of this instruction \n+     is available for scheduling, false otherwise.  -1 means we're not sure;\n+     please run find_used_regs to clarify.  */\n+  signed char target_available;\n+\n+  /* True when this expression needs a speculation check to be scheduled.  \n+     This is used during find_used_regs.  */\n+  BOOL_BITFIELD needs_spec_check_p : 1;\n+\n+  /* True when the expression was substituted.  Used for statistical \n+     purposes.  */\n+  BOOL_BITFIELD was_substituted : 1;\n+\n+  /* True when the expression was renamed.  */\n+  BOOL_BITFIELD was_renamed : 1;\n+\n+  /* True when expression can't be moved.  */\n+  BOOL_BITFIELD cant_move : 1;\n+};\n+\n+typedef struct _expr expr_def;\n+typedef expr_def *expr_t;\n+\n+#define EXPR_VINSN(EXPR) ((EXPR)->vinsn)\n+#define EXPR_INSN_RTX(EXPR) (VINSN_INSN_RTX (EXPR_VINSN (EXPR)))\n+#define EXPR_PATTERN(EXPR) (VINSN_PATTERN (EXPR_VINSN (EXPR)))\n+#define EXPR_LHS(EXPR) (VINSN_LHS (EXPR_VINSN (EXPR)))\n+#define EXPR_RHS(EXPR) (VINSN_RHS (EXPR_VINSN (EXPR)))\n+#define EXPR_TYPE(EXPR) (VINSN_TYPE (EXPR_VINSN (EXPR)))\n+#define EXPR_SEPARABLE_P(EXPR) (VINSN_SEPARABLE_P (EXPR_VINSN (EXPR)))\n+\n+#define EXPR_SPEC(EXPR) ((EXPR)->spec)\n+#define EXPR_USEFULNESS(EXPR) ((EXPR)->usefulness)\n+#define EXPR_PRIORITY(EXPR) ((EXPR)->priority)\n+#define EXPR_PRIORITY_ADJ(EXPR) ((EXPR)->priority_adj)\n+#define EXPR_SCHED_TIMES(EXPR) ((EXPR)->sched_times)\n+#define EXPR_ORIG_BB_INDEX(EXPR) ((EXPR)->orig_bb_index)\n+#define EXPR_ORIG_SCHED_CYCLE(EXPR) ((EXPR)->orig_sched_cycle)\n+#define EXPR_SPEC_DONE_DS(EXPR) ((EXPR)->spec_done_ds)\n+#define EXPR_SPEC_TO_CHECK_DS(EXPR) ((EXPR)->spec_to_check_ds)\n+#define EXPR_HISTORY_OF_CHANGES(EXPR) ((EXPR)->history_of_changes)\n+#define EXPR_TARGET_AVAILABLE(EXPR) ((EXPR)->target_available)\n+#define EXPR_NEEDS_SPEC_CHECK_P(EXPR) ((EXPR)->needs_spec_check_p)\n+#define EXPR_WAS_SUBSTITUTED(EXPR) ((EXPR)->was_substituted)\n+#define EXPR_WAS_RENAMED(EXPR) ((EXPR)->was_renamed)\n+#define EXPR_CANT_MOVE(EXPR) ((EXPR)->cant_move)\n+\n+#define EXPR_WAS_CHANGED(EXPR) (VEC_length (expr_history_def, \\\n+                                            EXPR_HISTORY_OF_CHANGES (EXPR)) > 0) \n+\n+/* Insn definition for list of original insns in find_used_regs.  */\n+struct _def\n+{\n+  insn_t orig_insn;\n+\n+  /* FIXME: Get rid of CROSSES_CALL in each def, since if we're moving up\n+     rhs from two different places, but only one of the code motion paths\n+     crosses a call, we can't use any of the call_used_regs, no matter which \n+     path or whether all paths crosses a call.  Thus we should move CROSSES_CALL\n+     to static params.  */\n+  bool crosses_call;\n+};\n+typedef struct _def *def_t;\n+\n+\n+/* Availability sets are sets of expressions we're scheduling.  */\n+typedef _list_t av_set_t;\n+#define _AV_SET_EXPR(L) (&(L)->u.expr)\n+#define _AV_SET_NEXT(L) (_LIST_NEXT (L))\n+\n+\n+/* Boundary of the current fence group.  */\n+struct _bnd\n+{\n+  /* The actual boundary instruction.  */\n+  insn_t to;\n+\n+  /* Its path to the fence.  */\n+  ilist_t ptr;\n+\n+  /* Availability set at the boundary.  */\n+  av_set_t av;\n+\n+  /* This set moved to the fence.  */\n+  av_set_t av1;\n+  \n+  /* Deps context at this boundary.  As long as we have one boundary per fence,\n+     this is just a pointer to the same deps context as in the corresponding\n+     fence.  */\n+  deps_t dc;\n+};\n+typedef struct _bnd *bnd_t;\n+#define BND_TO(B) ((B)->to)\n+\n+/* PTR stands not for pointer as you might think, but as a Path To Root of the\n+   current instruction group from boundary B.  */\n+#define BND_PTR(B) ((B)->ptr)\n+#define BND_AV(B) ((B)->av)\n+#define BND_AV1(B) ((B)->av1)\n+#define BND_DC(B) ((B)->dc)\n+\n+/* List of boundaries.  */\n+typedef _list_t blist_t;\n+#define BLIST_BND(L) (&(L)->u.bnd)\n+#define BLIST_NEXT(L) (_LIST_NEXT (L))\n+\n+\n+/* Fence information.  A fence represents current scheduling point and also\n+   blocks code motion through it when pipelining.  */\n+struct _fence\n+{\n+  /* Insn before which we gather an instruction group.*/\n+  insn_t insn;\n+\n+  /* Modeled state of the processor pipeline.  */\n+  state_t state;\n+\n+  /* Current cycle that is being scheduled on this fence.  */\n+  int cycle;\n+\n+  /* Number of insns that were scheduled on the current cycle.\n+     This information has to be local to a fence.  */\n+  int cycle_issued_insns;\n+\n+  /* At the end of fill_insns () this field holds the list of the instructions\n+     that are inner boundaries of the scheduled parallel group.  */\n+  ilist_t bnds;\n+\n+  /* Deps context at this fence.  It is used to model dependencies at the\n+     fence so that insn ticks can be properly evaluated.  */\n+  deps_t dc;\n+\n+  /* Target context at this fence.  Used to save and load any local target\n+     scheduling information when changing fences.  */\n+  tc_t tc;\n+\n+  /* A vector of insns that are scheduled but not yet completed.  */\n+  VEC (rtx,gc) *executing_insns;\n+\n+  /* A vector indexed by UIDs that caches the earliest cycle on which \n+     an insn can be scheduled on this fence.  */\n+  int *ready_ticks;\n+\n+  /* Its size.  */\n+  int ready_ticks_size;\n+\n+  /* Insn, which has been scheduled last on this fence.  */\n+  rtx last_scheduled_insn;\n+\n+  /* If non-NULL force the next scheduled insn to be SCHED_NEXT.  */\n+  rtx sched_next;\n+\n+  /* True if fill_insns processed this fence.  */\n+  BOOL_BITFIELD processed_p : 1;\n+\n+  /* True if fill_insns actually scheduled something on this fence.  */\n+  BOOL_BITFIELD scheduled_p : 1;\n+\n+  /* True when the next insn scheduled here would start a cycle.  */\n+  BOOL_BITFIELD starts_cycle_p : 1;\n+\n+  /* True when the next insn scheduled here would be scheduled after a stall.  */\n+  BOOL_BITFIELD after_stall_p : 1;\n+};\n+typedef struct _fence *fence_t;\n+\n+#define FENCE_INSN(F) ((F)->insn)\n+#define FENCE_STATE(F) ((F)->state)\n+#define FENCE_BNDS(F) ((F)->bnds)\n+#define FENCE_PROCESSED_P(F) ((F)->processed_p)\n+#define FENCE_SCHEDULED_P(F) ((F)->scheduled_p)\n+#define FENCE_ISSUED_INSNS(F) ((F)->cycle_issued_insns)\n+#define FENCE_CYCLE(F) ((F)->cycle)\n+#define FENCE_STARTS_CYCLE_P(F) ((F)->starts_cycle_p)\n+#define FENCE_AFTER_STALL_P(F) ((F)->after_stall_p)\n+#define FENCE_DC(F) ((F)->dc)\n+#define FENCE_TC(F) ((F)->tc)\n+#define FENCE_LAST_SCHEDULED_INSN(F) ((F)->last_scheduled_insn)\n+#define FENCE_EXECUTING_INSNS(F) ((F)->executing_insns)\n+#define FENCE_READY_TICKS(F) ((F)->ready_ticks)\n+#define FENCE_READY_TICKS_SIZE(F) ((F)->ready_ticks_size)\n+#define FENCE_SCHED_NEXT(F) ((F)->sched_next)\n+\n+/* List of fences.  */\n+typedef _list_t flist_t;\n+#define FLIST_FENCE(L) (&(L)->u.fence)\n+#define FLIST_NEXT(L) (_LIST_NEXT (L))\n+\n+/* List of fences with pointer to the tail node.  */\n+struct flist_tail_def\n+{\n+  flist_t head;\n+  flist_t *tailp;\n+};\n+\n+typedef struct flist_tail_def *flist_tail_t;\n+#define FLIST_TAIL_HEAD(L) ((L)->head)\n+#define FLIST_TAIL_TAILP(L) ((L)->tailp)\n+\n+/* List node information.  A list node can be any of the types above.  */\n+struct _list_node\n+{\n+  _list_t next;\n+\n+  union\n+  {\n+    rtx x;\n+    struct _bnd bnd;\n+    expr_def expr;\n+    struct _fence fence;\n+    struct _def def;\n+    void *data;\n+  } u;\n+};\n+\f\n+\n+/* _list_t functions.\n+   All of _*list_* functions are used through accessor macros, thus\n+   we can't move them in sel-sched-ir.c.  */\n+extern alloc_pool sched_lists_pool;\n+\n+static inline _list_t\n+_list_alloc (void)\n+{\n+  return (_list_t) pool_alloc (sched_lists_pool);\n+}\n+\n+static inline void\n+_list_add (_list_t *lp)\n+{\n+  _list_t l = _list_alloc ();\n+\n+  _LIST_NEXT (l) = *lp;\n+  *lp = l;\n+}\n+\n+static inline void\n+_list_remove_nofree (_list_t *lp)\n+{\n+  _list_t n = *lp;\n+\n+  *lp = _LIST_NEXT (n);\n+}\n+\n+static inline void\n+_list_remove (_list_t *lp)\n+{\n+  _list_t n = *lp;\n+\n+  *lp = _LIST_NEXT (n);\n+  pool_free (sched_lists_pool, n);\n+}\n+\n+static inline void\n+_list_clear (_list_t *l)\n+{\n+  while (*l)\n+    _list_remove (l);\n+}\n+\f\n+\n+/* List iterator backend.  */\n+typedef struct\n+{\n+  /* The list we're iterating.  */\n+  _list_t *lp;\n+\n+  /* True when this iterator supprts removing.  */\n+  bool can_remove_p;\n+\n+  /* True when we've actually removed something.  */\n+  bool removed_p;\n+} _list_iterator;\n+\n+static inline void\n+_list_iter_start (_list_iterator *ip, _list_t *lp, bool can_remove_p)\n+{\n+  ip->lp = lp;\n+  ip->can_remove_p = can_remove_p;\n+  ip->removed_p = false;\n+}\n+\n+static inline void\n+_list_iter_next (_list_iterator *ip)\n+{\n+  if (!ip->removed_p)\n+    ip->lp = &_LIST_NEXT (*ip->lp);\n+  else\n+    ip->removed_p = false;\n+}\n+\n+static inline void\n+_list_iter_remove (_list_iterator *ip)\n+{\n+  gcc_assert (!ip->removed_p && ip->can_remove_p);\n+  _list_remove (ip->lp);\n+  ip->removed_p = true;\n+}\n+\n+static inline void\n+_list_iter_remove_nofree (_list_iterator *ip)\n+{\n+  gcc_assert (!ip->removed_p && ip->can_remove_p);\n+  _list_remove_nofree (ip->lp);\n+  ip->removed_p = true;\n+}\n+\n+/* General macros to traverse a list.  FOR_EACH_* interfaces are\n+   implemented using these.  */\n+#define _FOR_EACH(TYPE, ELEM, I, L)\t\t\t\t\\\n+  for (_list_iter_start (&(I), &(L), false);\t\t\t\\\n+       _list_iter_cond_##TYPE (*(I).lp, &(ELEM));\t\t\\\n+       _list_iter_next (&(I)))\n+\n+#define _FOR_EACH_1(TYPE, ELEM, I, LP)                              \\\n+  for (_list_iter_start (&(I), (LP), true);                         \\\n+       _list_iter_cond_##TYPE (*(I).lp, &(ELEM));                   \\\n+       _list_iter_next (&(I))) \n+\f\n+\n+/* _xlist_t functions.  */\n+\n+static inline void\n+_xlist_add (_xlist_t *lp, rtx x)\n+{\n+  _list_add (lp);\n+  _XLIST_X (*lp) = x;\n+}\n+\n+#define _xlist_remove(LP) (_list_remove (LP))\n+#define _xlist_clear(LP) (_list_clear (LP))\n+\n+static inline bool\n+_xlist_is_in_p (_xlist_t l, rtx x)\n+{\n+  while (l)\n+    {\n+      if (_XLIST_X (l) == x)\n+        return true;\n+      l = _XLIST_NEXT (l);\n+    }\n+\n+  return false;\n+}\n+\n+/* Used through _FOR_EACH.  */\n+static inline bool\n+_list_iter_cond_x (_xlist_t l, rtx *xp)\n+{\n+  if (l)\n+    {\n+      *xp = _XLIST_X (l);\n+      return true;\n+    }\n+\n+  return false;\n+}\n+\n+#define _xlist_iter_remove(IP) (_list_iter_remove (IP))\n+\n+typedef _list_iterator _xlist_iterator;\n+#define _FOR_EACH_X(X, I, L) _FOR_EACH (x, (X), (I), (L))\n+#define _FOR_EACH_X_1(X, I, LP) _FOR_EACH_1 (x, (X), (I), (LP))\n+\f\n+\n+/* ilist_t functions.  Instruction lists are simply RTX lists.  */\n+\n+#define ilist_add(LP, INSN) (_xlist_add ((LP), (INSN)))\n+#define ilist_remove(LP) (_xlist_remove (LP))\n+#define ilist_clear(LP) (_xlist_clear (LP))\n+#define ilist_is_in_p(L, INSN) (_xlist_is_in_p ((L), (INSN)))\n+#define ilist_iter_remove(IP) (_xlist_iter_remove (IP))\n+\n+typedef _xlist_iterator ilist_iterator;\n+#define FOR_EACH_INSN(INSN, I, L) _FOR_EACH_X (INSN, I, L)\n+#define FOR_EACH_INSN_1(INSN, I, LP) _FOR_EACH_X_1 (INSN, I, LP)\n+\f\n+\n+/* Av set iterators.  */\n+typedef _list_iterator av_set_iterator;\n+#define FOR_EACH_EXPR(EXPR, I, AV) _FOR_EACH (expr, (EXPR), (I), (AV))\n+#define FOR_EACH_EXPR_1(EXPR, I, AV) _FOR_EACH_1 (expr, (EXPR), (I), (AV))\n+\n+static bool\n+_list_iter_cond_expr (av_set_t av, expr_t *exprp)\n+{\n+  if (av)\n+    {\n+      *exprp = _AV_SET_EXPR (av);\n+      return true;\n+    }\n+\n+  return false;\n+}\n+\f\n+\n+/* Def list iterators.  */\n+typedef _list_t def_list_t;\n+typedef _list_iterator def_list_iterator;\n+\n+#define DEF_LIST_NEXT(L) (_LIST_NEXT (L))\n+#define DEF_LIST_DEF(L) (&(L)->u.def)\n+\n+#define FOR_EACH_DEF(DEF, I, DEF_LIST) _FOR_EACH (def, (DEF), (I), (DEF_LIST))\n+\n+static inline bool\n+_list_iter_cond_def (def_list_t def_list, def_t *def)\n+{\n+  if (def_list)\n+    {\n+      *def = DEF_LIST_DEF (def_list);\n+      return true;\n+    }\n+\n+  return false;\n+}\n+\f\n+\n+/* InstructionData.  Contains information about insn pattern.  */\n+struct idata_def\n+{\n+  /* Type of the insn.\n+     o CALL_INSN - Call insn\n+     o JUMP_INSN - Jump insn\n+     o INSN - INSN that cannot be cloned\n+     o USE - INSN that can be cloned\n+     o SET - INSN that can be cloned and separable into lhs and rhs\n+     o PC - simplejump.  Insns that simply redirect control flow should not\n+     have any dependencies.  Sched-deps.c, though, might consider them as\n+     producers or consumers of certain registers.  To avoid that we handle\n+     dependency for simple jumps ourselves.  */\n+  int type;\n+\n+  /* If insn is a SET, this is its left hand side.  */\n+  rtx lhs;\n+\n+  /* If insn is a SET, this is its right hand side.  */\n+  rtx rhs;\n+\n+  /* Registers that are set/used by this insn.  This info is now gathered\n+     via sched-deps.c.  The downside of this is that we also use live info\n+     from flow that is accumulated in the basic blocks.  These two infos\n+     can be slightly inconsistent, hence in the beginning we make a pass\n+     through CFG and calculating the conservative solution for the info in\n+     basic blocks.  When this scheduler will be switched to use dataflow,\n+     this can be unified as df gives us both per basic block and per\n+     instruction info.  Actually, we don't do that pass and just hope\n+     for the best.  */\n+  regset reg_sets;\n+\n+  regset reg_clobbers;\n+\n+  regset reg_uses;\n+};\n+\n+#define IDATA_TYPE(ID) ((ID)->type)\n+#define IDATA_LHS(ID) ((ID)->lhs)\n+#define IDATA_RHS(ID) ((ID)->rhs)\n+#define IDATA_REG_SETS(ID) ((ID)->reg_sets)\n+#define IDATA_REG_USES(ID) ((ID)->reg_uses)\n+#define IDATA_REG_CLOBBERS(ID) ((ID)->reg_clobbers)\n+\n+/* Type to represent all needed info to emit an insn.\n+   This is a virtual equivalent of the insn.\n+   Every insn in the stream has an associated vinsn.  This is used\n+   to reduce memory consumption basing on the fact that many insns\n+   don't change through the scheduler.\n+\n+   vinsn can be either normal or unique.\n+   * Normal vinsn is the one, that can be cloned multiple times and typically\n+   corresponds to normal instruction.\n+\n+   * Unique vinsn derivates from CALL, ASM, JUMP (for a while) and other\n+   unusual stuff.  Such a vinsn is described by its INSN field, which is a\n+   reference to the original instruction.  */\n+struct vinsn_def\n+{\n+  /* Associated insn.  */\n+  rtx insn_rtx;\n+\n+  /* Its description.  */\n+  struct idata_def id;\n+\n+  /* Hash of vinsn.  It is computed either from pattern or from rhs using\n+     hash_rtx.  It is not placed in ID for faster compares.  */\n+  unsigned hash;\n+\n+  /* Hash of the insn_rtx pattern.  */\n+  unsigned hash_rtx;\n+\n+  /* Smart pointer counter.  */\n+  int count;\n+\n+  /* Cached cost of the vinsn.  To access it please use vinsn_cost ().  */\n+  int cost;\n+\n+  /* Mark insns that may trap so we don't move them through jumps.  */\n+  bool may_trap_p;\n+};\n+\n+#define VINSN_INSN_RTX(VI) ((VI)->insn_rtx)\n+#define VINSN_PATTERN(VI) (PATTERN (VINSN_INSN_RTX (VI)))\n+\n+#define VINSN_ID(VI) (&((VI)->id))\n+#define VINSN_HASH(VI) ((VI)->hash)\n+#define VINSN_HASH_RTX(VI) ((VI)->hash_rtx)\n+#define VINSN_TYPE(VI) (IDATA_TYPE (VINSN_ID (VI)))\n+#define VINSN_SEPARABLE_P(VI) (VINSN_TYPE (VI) == SET)\n+#define VINSN_CLONABLE_P(VI) (VINSN_SEPARABLE_P (VI) || VINSN_TYPE (VI) == USE)\n+#define VINSN_UNIQUE_P(VI) (!VINSN_CLONABLE_P (VI))\n+#define VINSN_LHS(VI) (IDATA_LHS (VINSN_ID (VI)))\n+#define VINSN_RHS(VI) (IDATA_RHS (VINSN_ID (VI)))\n+#define VINSN_REG_SETS(VI) (IDATA_REG_SETS (VINSN_ID (VI)))\n+#define VINSN_REG_USES(VI) (IDATA_REG_USES (VINSN_ID (VI)))\n+#define VINSN_REG_CLOBBERS(VI) (IDATA_REG_CLOBBERS (VINSN_ID (VI)))\n+#define VINSN_COUNT(VI) ((VI)->count)\n+#define VINSN_MAY_TRAP_P(VI) ((VI)->may_trap_p)\n+\f\n+\n+/* An entry of the hashtable describing transformations happened when \n+   moving up through an insn.  */\n+struct transformed_insns\n+{\n+  /* Previous vinsn.  Used to find the proper element.  */\n+  vinsn_t vinsn_old;\n+\n+  /* A new vinsn.  */\n+  vinsn_t vinsn_new;\n+\n+  /* Speculative status.  */\n+  ds_t ds;\n+\n+  /* Type of transformation happened.  */\n+  enum local_trans_type type;\n+\n+  /* Whether a conflict on the target register happened.  */\n+  BOOL_BITFIELD was_target_conflict : 1;\n+\n+  /* Whether a check was needed.  */\n+  BOOL_BITFIELD needs_check : 1;\n+};\n+\n+/* Indexed by INSN_LUID, the collection of all data associated with\n+   a single instruction that is in the stream.  */\n+struct _sel_insn_data\n+{\n+  /* The expression that contains vinsn for this insn and some\n+     flow-sensitive data like priority.  */\n+  expr_def expr;\n+\n+  /* If (WS_LEVEL == GLOBAL_LEVEL) then AV is empty.  */\n+  int ws_level;\n+\n+  /* A number that helps in defining a traversing order for a region.  */\n+  int seqno;\n+\n+  /* A liveness data computed above this insn.  */\n+  regset live;\n+\n+  /* An INSN_UID bit is set when deps analysis result is already known.  */\n+  bitmap analyzed_deps;\n+\n+  /* An INSN_UID bit is set when a hard dep was found, not set when \n+     no dependence is found.  This is meaningful only when the analyzed_deps\n+     bitmap has its bit set.  */\n+  bitmap found_deps;\n+\n+  /* An INSN_UID bit is set when this is a bookkeeping insn generated from \n+     a parent with this uid.  */\n+  bitmap originators;\n+\n+  /* A hashtable caching the result of insn transformations through this one.  */\n+  htab_t transformed_insns;\n+  \n+  /* A context incapsulating this insn.  */\n+  struct deps deps_context;\n+\n+  /* This field is initialized at the beginning of scheduling and is used\n+     to handle sched group instructions.  If it is non-null, then it points\n+     to the instruction, which should be forced to schedule next.  Such\n+     instructions are unique.  */\n+  insn_t sched_next;\n+\n+  /* Cycle at which insn was scheduled.  It is greater than zero if insn was\n+     scheduled.  This is used for bundling.  */\n+  int sched_cycle;\n+\n+  /* Cycle at which insn's data will be fully ready.  */\n+  int ready_cycle;\n+\n+  /* Speculations that are being checked by this insn.  */\n+  ds_t spec_checked_ds;\n+\n+  /* Whether the live set valid or not.  */\n+  BOOL_BITFIELD live_valid_p : 1;\n+  /* Insn is an ASM.  */\n+  BOOL_BITFIELD asm_p : 1;\n+\n+  /* True when an insn is scheduled after we've determined that a stall is\n+     required.\n+     This is used when emulating the Haifa scheduler for bundling.  */\n+  BOOL_BITFIELD after_stall_p : 1;\n+};\n+\n+typedef struct _sel_insn_data sel_insn_data_def;\n+typedef sel_insn_data_def *sel_insn_data_t;\n+\n+DEF_VEC_O (sel_insn_data_def);\n+DEF_VEC_ALLOC_O (sel_insn_data_def, heap);\n+extern VEC (sel_insn_data_def, heap) *s_i_d;\n+\n+/* Accessor macros for s_i_d.  */\n+#define SID(INSN) (VEC_index (sel_insn_data_def, s_i_d,\tINSN_LUID (INSN)))\n+#define SID_BY_UID(UID) (VEC_index (sel_insn_data_def, s_i_d,\tLUID_BY_UID (UID)))\n+\n+extern sel_insn_data_def insn_sid (insn_t);\n+\n+#define INSN_ASM_P(INSN) (SID (INSN)->asm_p)\n+#define INSN_SCHED_NEXT(INSN) (SID (INSN)->sched_next)\n+#define INSN_ANALYZED_DEPS(INSN) (SID (INSN)->analyzed_deps)\n+#define INSN_FOUND_DEPS(INSN) (SID (INSN)->found_deps) \n+#define INSN_DEPS_CONTEXT(INSN) (SID (INSN)->deps_context) \n+#define INSN_ORIGINATORS(INSN) (SID (INSN)->originators)\n+#define INSN_ORIGINATORS_BY_UID(UID) (SID_BY_UID (UID)->originators)\n+#define INSN_TRANSFORMED_INSNS(INSN) (SID (INSN)->transformed_insns)\n+\n+#define INSN_EXPR(INSN) (&SID (INSN)->expr)\n+#define INSN_LIVE(INSN) (SID (INSN)->live)\n+#define INSN_LIVE_VALID_P(INSN) (SID (INSN)->live_valid_p)\n+#define INSN_VINSN(INSN) (EXPR_VINSN (INSN_EXPR (INSN)))\n+#define INSN_TYPE(INSN) (VINSN_TYPE (INSN_VINSN (INSN)))\n+#define INSN_SIMPLEJUMP_P(INSN) (INSN_TYPE (INSN) == PC)\n+#define INSN_LHS(INSN) (VINSN_LHS (INSN_VINSN (INSN)))\n+#define INSN_RHS(INSN) (VINSN_RHS (INSN_VINSN (INSN)))\n+#define INSN_REG_SETS(INSN) (VINSN_REG_SETS (INSN_VINSN (INSN)))\n+#define INSN_REG_CLOBBERS(INSN) (VINSN_REG_CLOBBERS (INSN_VINSN (INSN)))\n+#define INSN_REG_USES(INSN) (VINSN_REG_USES (INSN_VINSN (INSN)))\n+#define INSN_SCHED_TIMES(INSN) (EXPR_SCHED_TIMES (INSN_EXPR (INSN)))\n+#define INSN_SEQNO(INSN) (SID (INSN)->seqno)\n+#define INSN_AFTER_STALL_P(INSN) (SID (INSN)->after_stall_p)\n+#define INSN_SCHED_CYCLE(INSN) (SID (INSN)->sched_cycle)\n+#define INSN_READY_CYCLE(INSN) (SID (INSN)->ready_cycle)\n+#define INSN_SPEC_CHECKED_DS(INSN) (SID (INSN)->spec_checked_ds)\n+\n+/* A global level shows whether an insn is valid or not.  */\n+extern int global_level;\n+\n+#define INSN_WS_LEVEL(INSN) (SID (INSN)->ws_level)\n+\n+extern av_set_t get_av_set (insn_t);\n+extern int get_av_level (insn_t);\n+\n+#define AV_SET(INSN) (get_av_set (INSN))\n+#define AV_LEVEL(INSN) (get_av_level (INSN))\n+#define AV_SET_VALID_P(INSN) (AV_LEVEL (INSN) == global_level)\n+\n+/* A list of fences currently in the works.  */\n+extern flist_t fences;\n+\n+/* A NOP pattern used as a placeholder for real insns.  */\n+extern rtx nop_pattern;\n+\n+/* An insn that 'contained' in EXIT block.  */\n+extern rtx exit_insn;\n+\n+/* Provide a separate luid for the insn.  */\n+#define INSN_INIT_TODO_LUID (1)\n+\n+/* Initialize s_s_i_d.  */\n+#define INSN_INIT_TODO_SSID (2)\n+\n+/* Initialize data for simplejump.  */\n+#define INSN_INIT_TODO_SIMPLEJUMP (4)\n+\n+/* Return true if INSN is a local NOP.  The nop is local in the sense that\n+   it was emitted by the scheduler as a temporary insn and will soon be\n+   deleted.  These nops are identified by their pattern.  */\n+#define INSN_NOP_P(INSN) (PATTERN (INSN) == nop_pattern)\n+\n+/* Return true if INSN is linked into instruction stream.\n+   NB: It is impossible for INSN to have one field null and the other not\n+   null: gcc_assert ((PREV_INSN (INSN) == NULL_RTX)\n+   == (NEXT_INSN (INSN) == NULL_RTX)) is valid.  */\n+#define INSN_IN_STREAM_P(INSN) (PREV_INSN (INSN) && NEXT_INSN (INSN))\n+\n+/* Return true if INSN is in current fence.  */\n+#define IN_CURRENT_FENCE_P(INSN) (flist_lookup (fences, INSN) != NULL)\n+\n+/* Marks loop as being considered for pipelining.  */\n+#define MARK_LOOP_FOR_PIPELINING(LOOP) ((LOOP)->aux = (void *)(size_t)(1))\n+#define LOOP_MARKED_FOR_PIPELINING_P(LOOP) ((size_t)((LOOP)->aux))\n+\n+/* Saved loop preheader to transfer when scheduling the loop.  */\n+#define LOOP_PREHEADER_BLOCKS(LOOP) ((size_t)((LOOP)->aux) == 1         \\\n+                                     ? NULL                             \\\n+                                     : ((VEC(basic_block, heap) *) (LOOP)->aux))\n+#define SET_LOOP_PREHEADER_BLOCKS(LOOP,BLOCKS) ((LOOP)->aux             \\\n+                                                = (BLOCKS != NULL       \\\n+                                                   ? BLOCKS             \\\n+                                                   : (LOOP)->aux))\n+\n+extern bitmap blocks_to_reschedule;\n+\f\n+\n+/* A variable to track which part of rtx we are scanning in\n+   sched-deps.c: sched_analyze_insn ().  */\n+enum deps_where_def\n+  {\n+    DEPS_IN_INSN,\n+    DEPS_IN_LHS,\n+    DEPS_IN_RHS,\n+    DEPS_IN_NOWHERE\n+  };\n+typedef enum deps_where_def deps_where_t;\n+\f\n+\n+/* Per basic block data for the whole CFG.  */\n+typedef struct\n+{\n+  /* For each bb header this field contains a set of live registers.\n+     For all other insns this field has a NULL.\n+     We also need to know LV sets for the instructions, that are immediatly\n+     after the border of the region.  */\n+  regset lv_set;\n+\n+  /* Status of LV_SET.\n+     true - block has usable LV_SET.\n+     false - block's LV_SET should be recomputed.  */\n+  bool lv_set_valid_p;\n+} sel_global_bb_info_def;\n+\n+typedef sel_global_bb_info_def *sel_global_bb_info_t;\n+\n+DEF_VEC_O (sel_global_bb_info_def);\n+DEF_VEC_ALLOC_O (sel_global_bb_info_def, heap);\n+\n+/* Per basic block data.  This array is indexed by basic block index.  */\n+extern VEC (sel_global_bb_info_def, heap) *sel_global_bb_info;\n+\n+extern void sel_extend_global_bb_info (void);\n+extern void sel_finish_global_bb_info (void);\n+\n+/* Get data for BB.  */\n+#define SEL_GLOBAL_BB_INFO(BB)\t\t\t\t\t\\\n+  (VEC_index (sel_global_bb_info_def, sel_global_bb_info, (BB)->index))\n+\n+/* Access macros.  */\n+#define BB_LV_SET(BB) (SEL_GLOBAL_BB_INFO (BB)->lv_set)\n+#define BB_LV_SET_VALID_P(BB) (SEL_GLOBAL_BB_INFO (BB)->lv_set_valid_p)\n+\n+/* Per basic block data for the region.  */\n+typedef struct \n+{\n+  /* This insn stream is constructed in such a way that it should be\n+     traversed by PREV_INSN field - (*not* NEXT_INSN).  */\n+  rtx note_list;\n+\n+  /* Cached availability set at the beginning of a block.\n+     See also AV_LEVEL () for conditions when this av_set can be used.  */\n+  av_set_t av_set;\n+\n+  /* If (AV_LEVEL == GLOBAL_LEVEL) then AV is valid.  */\n+  int av_level;\n+} sel_region_bb_info_def;\n+\n+typedef sel_region_bb_info_def *sel_region_bb_info_t;\n+\n+DEF_VEC_O (sel_region_bb_info_def);\n+DEF_VEC_ALLOC_O (sel_region_bb_info_def, heap);\n+\n+/* Per basic block data.  This array is indexed by basic block index.  */\n+extern VEC (sel_region_bb_info_def, heap) *sel_region_bb_info;\n+\n+/* Get data for BB.  */\n+#define SEL_REGION_BB_INFO(BB) (VEC_index (sel_region_bb_info_def,\t\\\n+\t\t\t\t\t   sel_region_bb_info, (BB)->index))\n+\n+/* Get BB's note_list.\n+   A note_list is a list of various notes that was scattered across BB\n+   before scheduling, and will be appended at the beginning of BB after\n+   scheduling is finished.  */\n+#define BB_NOTE_LIST(BB) (SEL_REGION_BB_INFO (BB)->note_list)\n+\n+#define BB_AV_SET(BB) (SEL_REGION_BB_INFO (BB)->av_set)\n+#define BB_AV_LEVEL(BB) (SEL_REGION_BB_INFO (BB)->av_level)\n+#define BB_AV_SET_VALID_P(BB) (BB_AV_LEVEL (BB) == global_level)\n+\n+/* Used in bb_in_ebb_p.  */\n+extern bitmap_head *forced_ebb_heads;\n+\n+/* The loop nest being pipelined.  */\n+extern struct loop *current_loop_nest;\n+\n+/* Saves pipelined blocks.  Bitmap is indexed by bb->index.  */\n+extern sbitmap bbs_pipelined;\n+\n+/* Various flags.  */\n+extern bool enable_moveup_set_path_p;\n+extern bool pipelining_p;\n+extern bool bookkeeping_p;\n+extern int max_insns_to_rename;  \n+extern bool preheader_removed;\n+\n+/* Software lookahead window size.\n+   According to the results in Nakatani and Ebcioglu [1993], window size of 16 \n+   is enough to extract most ILP in integer code.  */\n+#define MAX_WS (PARAM_VALUE (PARAM_SELSCHED_MAX_LOOKAHEAD))\n+\n+extern regset sel_all_regs;\n+\f\n+\n+/* Successor iterator backend.  */\n+typedef struct\n+{\n+  /* True if we're at BB end.  */\n+  bool bb_end;\n+\n+  /* An edge on which we're iterating.  */\n+  edge e1;\n+\n+  /* The previous edge saved after skipping empty blocks.  */\n+  edge e2;\n+  \n+  /* Edge iterator used when there are successors in other basic blocks.  */\n+  edge_iterator ei;\n+\n+  /* Successor block we're traversing.  */\n+  basic_block bb;\n+\n+  /* Flags that are passed to the iterator.  We return only successors\n+     that comply to these flags.  */\n+  short flags;\n+  \n+  /* When flags include SUCCS_ALL, this will be set to the exact type \n+     of the sucessor we're traversing now.  */\n+  short current_flags;\n+\n+  /* If skip to loop exits, save here information about loop exits.  */\n+  int current_exit;\n+  VEC (edge, heap) *loop_exits;\n+} succ_iterator;\n+\n+/* A structure returning all successor's information.  */\n+struct succs_info\n+{\n+  /* Flags that these succcessors were computed with.  */\n+  short flags;\n+\n+  /* Successors that correspond to the flags.  */\n+  insn_vec_t succs_ok;\n+\n+  /* Their probabilities.  As of now, we don't need this for other \n+     successors.  */\n+  VEC(int,heap) *probs_ok;\n+\n+  /* Other successors.  */\n+  insn_vec_t succs_other;\n+\n+  /* Probability of all successors.  */\n+  int all_prob;\n+\n+  /* The number of all successors.  */\n+  int all_succs_n;\n+\n+  /* The number of good successors.  */\n+  int succs_ok_n;\n+};\n+\n+/* Some needed definitions.  */\n+extern basic_block after_recovery;\n+\n+extern insn_t sel_bb_head (basic_block);\n+extern bool sel_bb_empty_p (basic_block);\n+extern bool in_current_region_p (basic_block);\n+\n+/* True when BB is a header of the inner loop.  */\n+static inline bool\n+inner_loop_header_p (basic_block bb)\n+{\n+  struct loop *inner_loop; \n+\n+  if (!current_loop_nest)\n+    return false;\n+\n+  if (bb == EXIT_BLOCK_PTR)\n+    return false;\n+\n+  inner_loop = bb->loop_father;\n+  if (inner_loop == current_loop_nest)\n+    return false;\n+\n+  /* If successor belongs to another loop.  */\n+  if (bb == inner_loop->header\n+      && flow_bb_inside_loop_p (current_loop_nest, bb))\n+    {\n+      /* Could be '=' here because of wrong loop depths.  */\n+      gcc_assert (loop_depth (inner_loop) >= loop_depth (current_loop_nest));\n+      return true;\n+    }\n+\n+  return false;  \n+}\n+\n+/* Return exit edges of LOOP, filtering out edges with the same dest bb.  */\n+static inline VEC (edge, heap) *\n+get_loop_exit_edges_unique_dests (const struct loop *loop)\n+{\n+  VEC (edge, heap) *edges = NULL;\n+  struct loop_exit *exit;\n+\n+  gcc_assert (loop->latch != EXIT_BLOCK_PTR\n+              && current_loops->state & LOOPS_HAVE_RECORDED_EXITS);\n+\n+  for (exit = loop->exits->next; exit->e; exit = exit->next)\n+    {\n+      int i;\n+      edge e;\n+      bool was_dest = false;\n+      \n+      for (i = 0; VEC_iterate (edge, edges, i, e); i++)\n+        if (e->dest == exit->e->dest)\n+          {\n+            was_dest = true;\n+            break;\n+          }\n+\n+      if (!was_dest)\n+        VEC_safe_push (edge, heap, edges, exit->e);\n+    }\n+  return edges;\n+}\n+\n+/* Collect all loop exits recursively, skipping empty BBs between them.  \n+   E.g. if BB is a loop header which has several loop exits,\n+   traverse all of them and if any of them turns out to be another loop header\n+   (after skipping empty BBs), add its loop exits to the resulting vector \n+   as well.  */\n+static inline VEC(edge, heap) *\n+get_all_loop_exits (basic_block bb)\n+{\n+  VEC(edge, heap) *exits = NULL;\n+\n+  /* If bb is empty, and we're skipping to loop exits, then\n+     consider bb as a possible gate to the inner loop now.  */\n+  while (sel_bb_empty_p (bb) \n+\t && in_current_region_p (bb))\n+    {\n+      bb = single_succ (bb);\n+\n+      /* This empty block could only lead outside the region.  */\n+      gcc_assert (! in_current_region_p (bb));\n+    }\n+\n+  /* And now check whether we should skip over inner loop.  */\n+  if (inner_loop_header_p (bb))\n+    {\n+      struct loop *this_loop;\n+      struct loop *pred_loop = NULL;\n+      int i;\n+      edge e;\n+      \n+      for (this_loop = bb->loop_father;\n+           this_loop && this_loop != current_loop_nest;\n+           this_loop = loop_outer (this_loop))\n+        pred_loop = this_loop;\n+      \n+      this_loop = pred_loop;\n+      gcc_assert (this_loop != NULL);\n+\n+      exits = get_loop_exit_edges_unique_dests (this_loop);\n+\n+      /* Traverse all loop headers.  */\n+      for (i = 0; VEC_iterate (edge, exits, i, e); i++)\n+\tif (in_current_region_p (e->dest))\n+\t  {\n+\t    VEC(edge, heap) *next_exits = get_all_loop_exits (e->dest);\n+  \n+\t    if (next_exits)\n+\t      {\n+\t\tint j;\n+\t\tedge ne;\n+  \n+\t\t/* Add all loop exits for the current edge into the\n+\t\t   resulting vector.  */\n+\t\tfor (j = 0; VEC_iterate (edge, next_exits, j, ne); j++)\n+\t\t  VEC_safe_push (edge, heap, exits, ne);\n+  \n+\t\t/* Remove the original edge.  */\n+\t\tVEC_ordered_remove (edge, exits, i);\n+\n+\t\t/*  Decrease the loop counter so we won't skip anything.  */\n+\t\ti--;\n+\t\tcontinue;\n+\t      }\n+\t  }\n+    }\n+\n+  return exits;\n+}\n+\n+/* Flags to pass to compute_succs_info and FOR_EACH_SUCC.\n+   Any successor will fall into exactly one category.   */\n+\n+/* Include normal successors.  */\n+#define SUCCS_NORMAL (1)\n+\n+/* Include back-edge successors.  */\n+#define SUCCS_BACK (2)\n+\n+/* Include successors that are outside of the current region.  */\n+#define SUCCS_OUT (4)\n+\n+/* When pipelining of the outer loops is enabled, skip innermost loops \n+   to their exits.  */\n+#define SUCCS_SKIP_TO_LOOP_EXITS (8)\n+\n+/* Include all successors.  */\n+#define SUCCS_ALL (SUCCS_NORMAL | SUCCS_BACK | SUCCS_OUT)\n+\n+/* We need to return a succ_iterator to avoid 'unitialized' warning\n+   during bootstrap.  */\n+static inline succ_iterator\n+_succ_iter_start (insn_t *succp, insn_t insn, int flags)\n+{\n+  succ_iterator i;\n+\n+  basic_block bb = BLOCK_FOR_INSN (insn);\n+\n+  gcc_assert (INSN_P (insn) || NOTE_INSN_BASIC_BLOCK_P (insn));\n+\n+  i.flags = flags;\n+\n+  /* Avoid 'uninitialized' warning.  */\n+  *succp = NULL;\n+  i.e1 = NULL;\n+  i.e2 = NULL;\n+  i.bb = bb;\n+  i.current_flags = 0;\n+  i.current_exit = -1;\n+  i.loop_exits = NULL;\n+\n+  if (bb != EXIT_BLOCK_PTR && BB_END (bb) != insn)\n+    {\n+      i.bb_end = false;\n+\n+      /* Avoid 'uninitialized' warning.  */\n+      i.ei.index = 0;\n+      i.ei.container = NULL;\n+    }\n+  else\n+    {\n+      i.ei = ei_start (bb->succs);\n+      i.bb_end = true;\n+    }\n+\n+  return i;\n+}\n+\n+static inline bool\n+_succ_iter_cond (succ_iterator *ip, rtx *succp, rtx insn,\n+                 bool check (edge, succ_iterator *))\n+{\n+  if (!ip->bb_end)\n+    {\n+      /* When we're in a middle of a basic block, return\n+         the next insn immediately, but only when SUCCS_NORMAL is set.  */\n+      if (*succp != NULL || (ip->flags & SUCCS_NORMAL) == 0)\n+        return false;\n+\n+      *succp = NEXT_INSN (insn);\n+      ip->current_flags = SUCCS_NORMAL;\n+      return true;\n+    }\n+  else\n+    {\n+      while (1) \n+        {\n+          edge e_tmp = NULL;\n+\n+          /* First, try loop exits, if we have them.  */\n+          if (ip->loop_exits)\n+            {\n+              do\n+                {\n+                  VEC_iterate (edge, ip->loop_exits, \n+                               ip->current_exit, e_tmp);\n+                  ip->current_exit++;\n+                }\n+\t      while (e_tmp && !check (e_tmp, ip));\n+              \n+              if (!e_tmp)\n+                VEC_free (edge, heap, ip->loop_exits);\n+            }\n+\n+          /* If we have found a successor, then great.  */\n+          if (e_tmp)\n+            {\n+              ip->e1 = e_tmp;\n+              break;\n+            }\n+\n+          /* If not, then try the next edge.  */\n+          while (ei_cond (ip->ei, &(ip->e1)))\n+            {\n+              basic_block bb = ip->e1->dest;\n+\n+              /* Consider bb as a possible loop header.  */\n+              if ((ip->flags & SUCCS_SKIP_TO_LOOP_EXITS)\n+                  && flag_sel_sched_pipelining_outer_loops\n+\t\t  && (!in_current_region_p (bb) \n+\t\t      || BLOCK_TO_BB (ip->bb->index) \n+\t\t\t < BLOCK_TO_BB (bb->index)))\n+                {\n+\t\t  /* Get all loop exits recursively.  */\n+\t\t  ip->loop_exits = get_all_loop_exits (bb);\n+\n+\t\t  if (ip->loop_exits)\n+\t\t    {\n+  \t\t      ip->current_exit = 0;\n+\t\t      /* Move the iterator now, because we won't do \n+\t\t\t succ_iter_next until loop exits will end.  */\n+\t\t      ei_next (&(ip->ei));\n+\t\t      break;\n+\t\t    }\n+                }\n+\n+              /* bb is not a loop header, check as usual.  */\n+              if (check (ip->e1, ip))\n+                break;\n+\n+              ei_next (&(ip->ei));\n+            }\n+\n+          /* If loop_exits are non null, we have found an inner loop;\n+\t     do one more iteration to fetch an edge from these exits.  */\n+          if (ip->loop_exits)\n+            continue;\n+\n+          /* Otherwise, we've found an edge in a usual way.  Break now.  */\n+          break;\n+        }\n+\n+      if (ip->e1)\n+\t{\n+\t  basic_block bb = ip->e2->dest;\n+\n+\t  if (bb == EXIT_BLOCK_PTR || bb == after_recovery)\n+\t    *succp = exit_insn;\n+\t  else\n+\t    {\n+              *succp = sel_bb_head (bb);\n+\n+              gcc_assert (ip->flags != SUCCS_NORMAL\n+                          || *succp == NEXT_INSN (bb_note (bb)));\n+\t      gcc_assert (BLOCK_FOR_INSN (*succp) == bb);\n+\t    }\n+\n+\t  return true;\n+\t}\n+      else\n+\treturn false;\n+    }\n+}\n+\n+static inline void\n+_succ_iter_next (succ_iterator *ip)\n+{\n+  gcc_assert (!ip->e2 || ip->e1);\n+\n+  if (ip->bb_end && ip->e1 && !ip->loop_exits)\n+    ei_next (&(ip->ei));\n+}\n+\n+/* Returns true when E1 is an eligible successor edge, possibly skipping\n+   empty blocks.  When E2P is not null, the resulting edge is written there.\n+   FLAGS are used to specify whether back edges and out-of-region edges\n+   should be considered.  */\n+static inline bool\n+_eligible_successor_edge_p (edge e1, succ_iterator *ip)\n+{\n+  edge e2 = e1;\n+  basic_block bb;\n+  int flags = ip->flags;\n+  bool src_outside_rgn = !in_current_region_p (e1->src);\n+\n+  gcc_assert (flags != 0);\n+\n+  if (src_outside_rgn)\n+    {\n+      /* Any successor of the block that is outside current region is\n+         ineligible, except when we're skipping to loop exits.  */\n+      gcc_assert (flags & (SUCCS_OUT | SUCCS_SKIP_TO_LOOP_EXITS));\n+\n+      if (flags & SUCCS_OUT)\n+\treturn false;\n+    }\n+\n+  bb = e2->dest;\n+\n+  /* Skip empty blocks, but be careful not to leave the region.  */\n+  while (1)\n+    {\n+      if (!sel_bb_empty_p (bb))\n+        break;\n+        \n+      if (!in_current_region_p (bb) \n+          && !(flags & SUCCS_OUT))\n+        return false;\n+\n+      e2 = EDGE_SUCC (bb, 0);\n+      bb = e2->dest;\n+      \n+      /* This couldn't happen inside a region.  */\n+      gcc_assert (! in_current_region_p (bb)\n+                  || (flags & SUCCS_OUT));\n+    }\n+  \n+  /* Save the second edge for later checks.  */\n+  ip->e2 = e2;\n+\n+  if (in_current_region_p (bb))\n+    {\n+      /* BLOCK_TO_BB sets topological order of the region here.  \n+         It is important to use real predecessor here, which is ip->bb, \n+         as we may well have e1->src outside current region, \n+         when skipping to loop exits.  */\n+      bool succeeds_in_top_order = (BLOCK_TO_BB (ip->bb->index)\n+\t\t\t\t    < BLOCK_TO_BB (bb->index));\n+\n+      /* This is true for the all cases except the last one.  */\n+      ip->current_flags = SUCCS_NORMAL;\n+      \n+      /* We are advancing forward in the region, as usual.  */\n+      if (succeeds_in_top_order)\n+        {\n+          /* We are skipping to loop exits here.  */\n+          gcc_assert (!src_outside_rgn\n+                      || flag_sel_sched_pipelining_outer_loops);\n+          return !!(flags & SUCCS_NORMAL);\n+        }\n+\n+      /* This is a back edge.  During pipelining we ignore back edges, \n+         but only when it leads to the same loop.  It can lead to the header\n+         of the outer loop, which will also be the preheader of \n+         the current loop.  */\n+      if (pipelining_p\n+           && e1->src->loop_father == bb->loop_father)\n+        return !!(flags & SUCCS_NORMAL);\n+\n+      /* A back edge should be requested explicitly.  */\n+      ip->current_flags = SUCCS_BACK;\n+      return !!(flags & SUCCS_BACK);\n+    }\n+\n+  ip->current_flags = SUCCS_OUT;\n+  return !!(flags & SUCCS_OUT);\n+}\n+\n+#define FOR_EACH_SUCC_1(SUCC, ITER, INSN, FLAGS)                        \\\n+  for ((ITER) = _succ_iter_start (&(SUCC), (INSN), (FLAGS));            \\\n+       _succ_iter_cond (&(ITER), &(SUCC), (INSN), _eligible_successor_edge_p); \\\n+       _succ_iter_next (&(ITER)))\n+\n+#define FOR_EACH_SUCC(SUCC, ITER, INSN)                 \\\n+  FOR_EACH_SUCC_1 (SUCC, ITER, INSN, SUCCS_NORMAL)\n+\n+/* Return the current edge along which a successor was built.  */\n+#define SUCC_ITER_EDGE(ITER) ((ITER)->e1)\n+\n+/* Return the next block of BB not running into inconsistencies.  */\n+static inline basic_block\n+bb_next_bb (basic_block bb)\n+{\n+  switch (EDGE_COUNT (bb->succs))\n+    {\n+    case 0:\n+      return bb->next_bb;\n+\n+    case 1: \n+      return single_succ (bb);\n+\n+    case 2:\n+      return FALLTHRU_EDGE (bb)->dest;\n+      \n+    default:\n+      return bb->next_bb;\n+    }\n+\n+  gcc_unreachable ();\n+}\n+\n+\f\n+\n+/* Functions that are used in sel-sched.c.  */\n+\n+/* List functions.  */\n+extern ilist_t ilist_copy (ilist_t);\n+extern ilist_t ilist_invert (ilist_t);\n+extern void blist_add (blist_t *, insn_t, ilist_t, deps_t);\n+extern void blist_remove (blist_t *);\n+extern void flist_tail_init (flist_tail_t);\n+\n+extern fence_t flist_lookup (flist_t, insn_t);\n+extern void flist_clear (flist_t *);\n+extern void def_list_add (def_list_t *, insn_t, bool);\n+\n+/* Target context functions.  */\n+extern tc_t create_target_context (bool);\n+extern void set_target_context (tc_t);\n+extern void reset_target_context (tc_t, bool);\n+\n+/* Deps context functions.  */\n+extern void advance_deps_context (deps_t, insn_t);\n+\n+/* Fences functions.  */\n+extern void init_fences (insn_t);\n+extern void add_clean_fence_to_fences (flist_tail_t, insn_t, fence_t);\n+extern void add_dirty_fence_to_fences (flist_tail_t, insn_t, fence_t);\n+extern void move_fence_to_fences (flist_t, flist_tail_t);\n+\n+/* Pool functions.  */\n+extern regset get_regset_from_pool (void);\n+extern regset get_clear_regset_from_pool (void);\n+extern void return_regset_to_pool (regset);\n+extern void free_regset_pool (void);\n+\n+extern insn_t get_nop_from_pool (insn_t);\n+extern void return_nop_to_pool (insn_t);\n+extern void free_nop_pool (void);\n+\n+/* Vinsns functions.  */\n+extern bool vinsn_separable_p (vinsn_t);\n+extern bool vinsn_cond_branch_p (vinsn_t);\n+extern void recompute_vinsn_lhs_rhs (vinsn_t);\n+extern int sel_vinsn_cost (vinsn_t);\n+extern insn_t sel_gen_insn_from_rtx_after (rtx, expr_t, int, insn_t);\n+extern insn_t sel_gen_recovery_insn_from_rtx_after (rtx, expr_t, int, insn_t);\n+extern insn_t sel_gen_insn_from_expr_after (expr_t, vinsn_t, int, insn_t);\n+extern insn_t  sel_move_insn (expr_t, int, insn_t);\n+extern void vinsn_attach (vinsn_t);\n+extern void vinsn_detach (vinsn_t);\n+extern vinsn_t vinsn_copy (vinsn_t, bool);\n+extern bool vinsn_equal_p (vinsn_t, vinsn_t);\n+\n+/* EXPR functions.  */\n+extern void copy_expr (expr_t, expr_t);\n+extern void copy_expr_onside (expr_t, expr_t);\n+extern void merge_expr_data (expr_t, expr_t, insn_t);\n+extern void merge_expr (expr_t, expr_t, insn_t);\n+extern void clear_expr (expr_t);\n+extern unsigned expr_dest_regno (expr_t);\n+extern rtx expr_dest_reg (expr_t); \n+extern int find_in_history_vect (VEC(expr_history_def, heap) *, \n+                                 rtx, vinsn_t, bool);\n+extern void insert_in_history_vect (VEC(expr_history_def, heap) **, \n+                                    unsigned, enum local_trans_type, \n+                                    vinsn_t, vinsn_t, ds_t);\n+extern void mark_unavailable_targets (av_set_t, av_set_t, regset);\n+extern int speculate_expr (expr_t, ds_t);\n+\n+/* Av set functions.  */\n+extern void av_set_add (av_set_t *, expr_t);\n+extern void av_set_iter_remove (av_set_iterator *);\n+extern expr_t av_set_lookup (av_set_t, vinsn_t);\n+extern expr_t merge_with_other_exprs (av_set_t *, av_set_iterator *, expr_t);\n+extern bool av_set_is_in_p (av_set_t, vinsn_t);\n+extern av_set_t av_set_copy (av_set_t);\n+extern void av_set_union_and_clear (av_set_t *, av_set_t *, insn_t);\n+extern void av_set_union_and_live (av_set_t *, av_set_t *, regset, regset, insn_t);\n+extern void av_set_clear (av_set_t *);\n+extern void av_set_leave_one_nonspec (av_set_t *);\n+extern expr_t av_set_element (av_set_t, int);\n+extern void av_set_substract_cond_branches (av_set_t *);\n+extern void av_set_split_usefulness (av_set_t, int, int);\n+extern void av_set_intersect (av_set_t *, av_set_t);\n+\n+extern void sel_save_haifa_priorities (void);\n+\n+extern void sel_init_global_and_expr (bb_vec_t);\n+extern void sel_finish_global_and_expr (void);\n+\n+extern regset compute_live (insn_t);\n+\n+/* Dependence analysis functions.  */\n+extern void sel_clear_has_dependence (void);\n+extern ds_t has_dependence_p (expr_t, insn_t, ds_t **);\n+\n+extern int tick_check_p (expr_t, deps_t, fence_t);\n+\n+/* Functions to work with insns.  */\n+extern bool lhs_of_insn_equals_to_dest_p (insn_t, rtx);\n+extern bool insn_eligible_for_subst_p (insn_t);\n+extern void get_dest_and_mode (rtx, rtx *, enum machine_mode *);\n+\n+extern bool bookkeeping_can_be_created_if_moved_through_p (insn_t);\n+extern bool sel_remove_insn (insn_t, bool, bool);\n+extern bool bb_header_p (insn_t);\n+extern void sel_init_invalid_data_sets (insn_t);\n+extern bool insn_at_boundary_p (insn_t);\n+extern bool jump_leads_only_to_bb_p (insn_t, basic_block);\n+\n+/* Basic block and CFG functions.  */\n+\n+extern insn_t sel_bb_head (basic_block);\n+extern bool sel_bb_head_p (insn_t);\n+extern insn_t sel_bb_end (basic_block);\n+extern bool sel_bb_end_p (insn_t);\n+extern bool sel_bb_empty_p (basic_block);\n+\n+extern bool in_current_region_p (basic_block);\n+extern basic_block fallthru_bb_of_jump (rtx);\n+\n+extern void sel_init_bbs (bb_vec_t, basic_block);\n+extern void sel_finish_bbs (void);\n+\n+extern struct succs_info * compute_succs_info (insn_t, short);\n+extern void free_succs_info (struct succs_info *);\n+extern bool sel_insn_has_single_succ_p (insn_t, int);\n+extern bool sel_num_cfg_preds_gt_1 (insn_t);\n+extern int get_seqno_by_preds (rtx);\n+\n+extern bool bb_ends_ebb_p (basic_block);\n+extern bool in_same_ebb_p (insn_t, insn_t);\n+\n+extern bool tidy_control_flow (basic_block, bool);\n+extern void free_bb_note_pool (void);\n+\n+extern void sel_remove_empty_bb (basic_block, bool, bool);\n+extern bool maybe_tidy_empty_bb (basic_block bb);\n+extern basic_block sel_split_edge (edge);\n+extern basic_block sel_create_recovery_block (insn_t);\n+extern void sel_merge_blocks (basic_block, basic_block);\n+extern void sel_redirect_edge_and_branch (edge, basic_block);\n+extern void sel_redirect_edge_and_branch_force (edge, basic_block);\n+extern void sel_init_pipelining (void);\n+extern void sel_finish_pipelining (void);\n+extern void sel_sched_region (int);\n+extern void sel_find_rgns (void);\n+extern loop_p get_loop_nest_for_rgn (unsigned int);\n+extern bool considered_for_pipelining_p (struct loop *);\n+extern void make_region_from_loop_preheader (VEC(basic_block, heap) **);\n+extern void sel_add_loop_preheaders (void);\n+extern bool sel_is_loop_preheader_p (basic_block);\n+extern void clear_outdated_rtx_info (basic_block);\n+extern void free_data_sets (basic_block);\n+extern void exchange_data_sets (basic_block, basic_block);\n+extern void copy_data_sets (basic_block, basic_block);\n+\n+extern void sel_register_cfg_hooks (void);\n+extern void sel_unregister_cfg_hooks (void);\n+\n+/* Expression transformation routines.  */\n+extern rtx create_insn_rtx_from_pattern (rtx, rtx);\n+extern vinsn_t create_vinsn_from_insn_rtx (rtx, bool);\n+extern rtx create_copy_of_insn_rtx (rtx);\n+extern void change_vinsn_in_expr (expr_t, vinsn_t);\n+\n+/* Various initialization functions.  */\n+extern void init_lv_sets (void);\n+extern void free_lv_sets (void);\n+extern void setup_nop_and_exit_insns (void);\n+extern void free_nop_and_exit_insns (void);\n+extern void setup_nop_vinsn (void);\n+extern void free_nop_vinsn (void);\n+extern void sel_set_sched_flags (void);\n+extern void sel_setup_sched_infos (void);\n+extern void alloc_sched_pools (void);\n+extern void free_sched_pools (void);\n+\n+#endif /* GCC_SEL_SCHED_IR_H */\n+\n+\n+\n+\n+\n+\n+\n+"}, {"sha": "e2edb0ab4c40a8eb6834e94fe39fdc00dbc52ed2", "filename": "gcc/sel-sched.c", "status": "added", "additions": 7327, "deletions": 0, "changes": 7327, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsel-sched.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsel-sched.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsel-sched.c?ref=e855c69d162c023bae5236ea75bab646c5e84fed"}, {"sha": "8e0b5d50c98c598885165287af9892ab7ddac970", "filename": "gcc/sel-sched.h", "status": "added", "additions": 27, "deletions": 0, "changes": 27, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsel-sched.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fsel-sched.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsel-sched.h?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -0,0 +1,27 @@\n+/* Instruction scheduling pass.  \n+   Copyright (C) 2006, 2007, 2008 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#ifndef GCC_SEL_SCHED_H\n+#define GCC_SEL_SCHED_H\n+\n+/* The main entry point.  */\n+extern void run_selective_scheduling (void);\n+extern bool maybe_skip_selective_scheduling (void);\n+\n+#endif /* GCC_SEL_SCHED_H */"}, {"sha": "07b7f33a2ebaeda25cc4d0d5003decd3bb84cc0c", "filename": "gcc/target-def.h", "status": "modified", "additions": 20, "deletions": 2, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Ftarget-def.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Ftarget-def.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftarget-def.h?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -316,12 +316,21 @@\n #define TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD 0\n #define TARGET_SCHED_DFA_NEW_CYCLE 0\n #define TARGET_SCHED_IS_COSTLY_DEPENDENCE 0\n+#define TARGET_SCHED_ADJUST_COST_2 0\n #define TARGET_SCHED_H_I_D_EXTENDED 0\n+#define TARGET_SCHED_ALLOC_SCHED_CONTEXT 0\n+#define TARGET_SCHED_INIT_SCHED_CONTEXT 0\n+#define TARGET_SCHED_SET_SCHED_CONTEXT 0\n+#define TARGET_SCHED_CLEAR_SCHED_CONTEXT 0\n+#define TARGET_SCHED_FREE_SCHED_CONTEXT 0\n #define TARGET_SCHED_SPECULATE_INSN 0\n #define TARGET_SCHED_NEEDS_BLOCK_P 0\n-#define TARGET_SCHED_GEN_CHECK 0\n+#define TARGET_SCHED_GEN_SPEC_CHECK 0\n #define TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD_SPEC 0\n #define TARGET_SCHED_SET_SCHED_FLAGS 0\n+#define TARGET_SCHED_GET_INSN_SPEC_DS 0\n+#define TARGET_SCHED_GET_INSN_CHECKED_DS 0\n+#define TARGET_SCHED_SKIP_RTX_P 0\n #define TARGET_SCHED_SMS_RES_MII 0\n \n #define TARGET_SCHED\t\t\t\t\t\t\\\n@@ -346,12 +355,21 @@\n    TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD,\t\\\n    TARGET_SCHED_DFA_NEW_CYCLE,\t\t\t\t\t\\\n    TARGET_SCHED_IS_COSTLY_DEPENDENCE,                           \\\n+   TARGET_SCHED_ADJUST_COST_2,                                  \\\n    TARGET_SCHED_H_I_D_EXTENDED,\t\t\t\t\t\\\n+   TARGET_SCHED_ALLOC_SCHED_CONTEXT,                            \\\n+   TARGET_SCHED_INIT_SCHED_CONTEXT,                             \\\n+   TARGET_SCHED_SET_SCHED_CONTEXT,                              \\\n+   TARGET_SCHED_CLEAR_SCHED_CONTEXT,                            \\\n+   TARGET_SCHED_FREE_SCHED_CONTEXT,                             \\\n    TARGET_SCHED_SPECULATE_INSN,                                 \\\n    TARGET_SCHED_NEEDS_BLOCK_P,                                  \\\n-   TARGET_SCHED_GEN_CHECK,                                      \\\n+   TARGET_SCHED_GEN_SPEC_CHECK,\t\t\t\t        \\\n    TARGET_SCHED_FIRST_CYCLE_MULTIPASS_DFA_LOOKAHEAD_GUARD_SPEC, \\\n    TARGET_SCHED_SET_SCHED_FLAGS,                                \\\n+   TARGET_SCHED_GET_INSN_SPEC_DS,                               \\\n+   TARGET_SCHED_GET_INSN_CHECKED_DS,                            \\\n+   TARGET_SCHED_SKIP_RTX_P,\t\t\t\t\t\\\n    TARGET_SCHED_SMS_RES_MII}\n \n #define TARGET_VECTORIZE_BUILTIN_MASK_FOR_LOAD 0"}, {"sha": "de8150eb9baa1267c0b483be1c7ca4b673c7b3e2", "filename": "gcc/target.h", "status": "modified", "additions": 37, "deletions": 5, "changes": 42, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Ftarget.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Ftarget.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftarget.h?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -274,7 +274,7 @@ struct gcc_target\n     /* Finalize machine-dependent scheduling code.  */\n     void (* md_finish) (FILE *, int);\n \n-    /* Initialize machine-dependent function while scheduling code.  */\n+    /* Initialize machine-dependent function wide scheduling code.  */\n     void (* md_init_global) (FILE *, int, int);\n \n     /* Finalize machine-dependent function wide scheduling code.  */\n@@ -354,11 +354,33 @@ struct gcc_target\n        second insn (second parameter).  */\n     bool (* is_costly_dependence) (struct _dep *_dep, int, int);\n \n+    /* Given the current cost, COST, of an insn, INSN, calculate and\n+       return a new cost based on its relationship to DEP_INSN through the\n+       dependence of type DEP_TYPE.  The default is to make no adjustment.  */\n+    int (* adjust_cost_2) (rtx insn, int, rtx dep_insn, int cost, int dw);\n+\n     /* The following member value is a pointer to a function called\n        by the insn scheduler. This hook is called to notify the backend\n        that new instructions were emitted.  */\n     void (* h_i_d_extended) (void);\n-    \n+\n+    /* Next 5 functions are for multi-point scheduling.  */\n+\n+    /* Allocate memory for scheduler context.  */\n+    void *(* alloc_sched_context) (void);\n+\n+    /* Fills the context from the local machine scheduler context.  */\n+    void (* init_sched_context) (void *, bool);\n+\n+    /* Sets local machine scheduler context to a saved value.  */\n+    void (* set_sched_context) (void *);\n+\n+    /* Clears a scheduler context so it becomes like after init.  */\n+    void (* clear_sched_context) (void *);\n+\n+    /* Frees the scheduler context.  */\n+    void (* free_sched_context) (void *);\n+\n     /* The following member value is a pointer to a function called\n        by the insn scheduler.\n        The first parameter is an instruction, the second parameter is the type\n@@ -374,8 +396,7 @@ struct gcc_target\n \n     /* The following member value is a pointer to a function called\n        by the insn scheduler.  It should return true if the check instruction\n-       corresponding to the instruction passed as the parameter needs a\n-       recovery block.  */\n+       passed as the parameter needs a recovery block.  */\n     bool (* needs_block_p) (const_rtx);\n \n     /* The following member value is a pointer to a function called\n@@ -386,7 +407,7 @@ struct gcc_target\n        simple check).  If the mutation of the check is requested (e.g. from\n        ld.c to chk.a), the third parameter is true - in this case the first\n        parameter is the previous check.  */\n-    rtx (* gen_check) (rtx, rtx, bool);\n+    rtx (* gen_spec_check) (rtx, rtx, bool);\n \n     /* The following member value is a pointer to a function controlling\n        what insns from the ready insn queue will be considered for the\n@@ -401,6 +422,17 @@ struct gcc_target\n        The parameter is a pointer to spec_info variable.  */\n     void (* set_sched_flags) (struct spec_info_def *);\n \n+    /* Return speculation types of the instruction passed as the parameter.  */\n+    int (* get_insn_spec_ds) (rtx);\n+\n+    /* Return speculation types that are checked for the instruction passed as\n+       the parameter.  */\n+    int (* get_insn_checked_ds) (rtx);\n+\n+    /* Return bool if rtx scanning should just skip current layer and\n+       advance to the inner rtxes.  */\n+    bool (* skip_rtx_p) (const_rtx);\n+\n     /* The following member value is a pointer to a function that provides\n        information about the target resource-based lower bound which is\n        used by the swing modulo scheduler.  The parameter is a pointer"}, {"sha": "750dadabfb4f763efab0f077a8e2d47eb777bc36", "filename": "gcc/vecprim.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fvecprim.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e855c69d162c023bae5236ea75bab646c5e84fed/gcc%2Fvecprim.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fvecprim.h?ref=e855c69d162c023bae5236ea75bab646c5e84fed", "patch": "@@ -26,4 +26,7 @@ DEF_VEC_ALLOC_I(char,heap);\n DEF_VEC_I(int);\n DEF_VEC_ALLOC_I(int,heap);\n \n+DEF_VEC_I(unsigned);\n+DEF_VEC_ALLOC_I(unsigned,heap);\n+\n #endif /* GCC_VECPRIM_H */"}]}