{"sha": "bac07a1da24fbc40720bd4080caaea6fa7769672", "node_id": "C_kwDOANBUbNoAKGJhYzA3YTFkYTI0ZmJjNDA3MjBiZDQwODBjYWFlYTZmYTc3Njk2NzI", "commit": {"author": {"name": "Richard Biener", "email": "rguenther@suse.de", "date": "2022-08-15T13:38:09Z"}, "committer": {"name": "Richard Biener", "email": "rguenther@suse.de", "date": "2022-08-17T08:40:43Z"}, "message": "Refactor back_threader_profitability\n\nThe following refactors profitable_path_p in the backward threader,\nsplitting out parts that can be computed once the exit block is known,\nparts that contiguously update and that can be checked allowing\nfor the path to be later identified as FSM with larger limits,\npossibly_profitable_path_p, and final checks done when the whole\npath is known, profitable_path_p.\n\nI've removed the back_threader_profitability instance from the\nback_threader class and instead instantiate it once per path\ndiscovery.  I've kept the size compute non-incremental to simplify\nthe patch and not worry about unwinding.\n\nThere's key changes to previous behavior - namely we apply\nthe param_max_jump_thread_duplication_stmts early only when\nwe know the path cannot become an FSM one (multiway + thread through\nlatch) but make sure to elide the path query when we we didn't\nyet discover that but are over this limit.  Similarly the\nspeed limit is now used even when we did not yet discover a\nhot BB on the path.  Basically the idea is to only stop path\ndiscovery when we know the path will never become profitable\nbut avoid the expensive path range query when we know it's\ncurrently not.\n\nI've done a few cleanups, merging functions, on the way.\n\n\t* tree-ssa-threadbackward.cc\n\t(back_threader_profitability): Split profitable_path_p\n\tinto possibly_profitable_path_p and itself, keep state\n\tas new members.\n\t(back_threader::m_profit): Remove.\n\t(back_threader::find_paths): Likewise.\n\t(back_threader::maybe_register_path): Take profitability\n\tinstance as parameter.\n\t(back_threader::find_paths_to_names): Likewise.  Use\n\tpossibly_profitable_path_p and avoid the path range query\n\twhen the path is currently too large.\n\t(back_threader::find_paths): Fold into ...\n\t(back_threader::maybe_thread_block): ... this.\n\t(get_gimple_control_stmt): Remove.\n\t(back_threader_profitability::possibly_profitable_path_p):\n\tSplit out from profitable_path_p, do early profitability\n\tchecks.\n\t(back_threader_profitability::profitable_path_p): Do final\n\tprofitability path after the taken edge has been determined.", "tree": {"sha": "f257b97ac867dafcd95d8c0b84843a7035c6b7ae", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/f257b97ac867dafcd95d8c0b84843a7035c6b7ae"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/bac07a1da24fbc40720bd4080caaea6fa7769672", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/bac07a1da24fbc40720bd4080caaea6fa7769672", "html_url": "https://github.com/Rust-GCC/gccrs/commit/bac07a1da24fbc40720bd4080caaea6fa7769672", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/bac07a1da24fbc40720bd4080caaea6fa7769672/comments", "author": {"login": "rguenth", "id": 2046526, "node_id": "MDQ6VXNlcjIwNDY1MjY=", "avatar_url": "https://avatars.githubusercontent.com/u/2046526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rguenth", "html_url": "https://github.com/rguenth", "followers_url": "https://api.github.com/users/rguenth/followers", "following_url": "https://api.github.com/users/rguenth/following{/other_user}", "gists_url": "https://api.github.com/users/rguenth/gists{/gist_id}", "starred_url": "https://api.github.com/users/rguenth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rguenth/subscriptions", "organizations_url": "https://api.github.com/users/rguenth/orgs", "repos_url": "https://api.github.com/users/rguenth/repos", "events_url": "https://api.github.com/users/rguenth/events{/privacy}", "received_events_url": "https://api.github.com/users/rguenth/received_events", "type": "User", "site_admin": false}, "committer": {"login": "rguenth", "id": 2046526, "node_id": "MDQ6VXNlcjIwNDY1MjY=", "avatar_url": "https://avatars.githubusercontent.com/u/2046526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rguenth", "html_url": "https://github.com/rguenth", "followers_url": "https://api.github.com/users/rguenth/followers", "following_url": "https://api.github.com/users/rguenth/following{/other_user}", "gists_url": "https://api.github.com/users/rguenth/gists{/gist_id}", "starred_url": "https://api.github.com/users/rguenth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rguenth/subscriptions", "organizations_url": "https://api.github.com/users/rguenth/orgs", "repos_url": "https://api.github.com/users/rguenth/repos", "events_url": "https://api.github.com/users/rguenth/events{/privacy}", "received_events_url": "https://api.github.com/users/rguenth/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "5bc2042df437cd8aeebcdf5bbb858678e3733ca4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5bc2042df437cd8aeebcdf5bbb858678e3733ca4", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5bc2042df437cd8aeebcdf5bbb858678e3733ca4"}], "stats": {"total": 357, "additions": 199, "deletions": 158}, "files": [{"sha": "7698e1f8b11fd17170628905466da032e2c86ace", "filename": "gcc/tree-ssa-threadbackward.cc", "status": "modified", "additions": 199, "deletions": 158, "changes": 357, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bac07a1da24fbc40720bd4080caaea6fa7769672/gcc%2Ftree-ssa-threadbackward.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bac07a1da24fbc40720bd4080caaea6fa7769672/gcc%2Ftree-ssa-threadbackward.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-threadbackward.cc?ref=bac07a1da24fbc40720bd4080caaea6fa7769672", "patch": "@@ -61,15 +61,33 @@ class back_threader_registry : public back_jt_path_registry\n class back_threader_profitability\n {\n public:\n-  back_threader_profitability (bool speed_p)\n-    : m_speed_p (speed_p)\n-  { }\n-  bool profitable_path_p (const vec<basic_block> &, tree name, edge taken,\n-\t\t\t  bool *irreducible_loop = NULL);\n+  back_threader_profitability (bool speed_p, gimple *stmt);\n+  bool possibly_profitable_path_p (const vec<basic_block> &, tree, bool *);\n+  bool profitable_path_p (const vec<basic_block> &,\n+\t\t\t  edge taken, bool *irreducible_loop);\n private:\n   const bool m_speed_p;\n+  int m_exit_jump_benefit;\n+  bool m_threaded_multiway_branch;\n+  // The following are computed by possibly_profitable_path_p\n+  bool m_threaded_through_latch;\n+  bool m_multiway_branch_in_path;\n+  bool m_contains_hot_bb;\n+  int m_n_insns;\n };\n \n+back_threader_profitability::back_threader_profitability (bool speed_p,\n+\t\t\t\t\t\t\t  gimple *last)\n+  : m_speed_p (speed_p)\n+{\n+  m_threaded_multiway_branch = (gimple_code (last) == GIMPLE_SWITCH\n+\t\t\t\t|| gimple_code (last) == GIMPLE_GOTO);\n+  // The forward threader has estimate_threading_killed_stmts, in\n+  // particular it estimates further DCE from eliminating the exit\n+  // control stmt.\n+  m_exit_jump_benefit = estimate_num_insns (last, &eni_size_weights);\n+}\n+\n // Back threader flags.\n #define BT_NONE 0\n // Generate fast code at the expense of code size.\n@@ -86,19 +104,18 @@ class back_threader\n   unsigned thread_blocks ();\n private:\n   void maybe_thread_block (basic_block bb);\n-  void find_paths (basic_block bb, tree name);\n   bool debug_counter ();\n-  edge maybe_register_path ();\n+  edge maybe_register_path (back_threader_profitability &);\n   void maybe_register_path_dump (edge taken_edge);\n-  void find_paths_to_names (basic_block bb, bitmap imports, unsigned);\n+  void find_paths_to_names (basic_block bb, bitmap imports, unsigned,\n+\t\t\t    back_threader_profitability &);\n   edge find_taken_edge (const vec<basic_block> &path);\n   edge find_taken_edge_cond (const vec<basic_block> &path, gcond *);\n   edge find_taken_edge_switch (const vec<basic_block> &path, gswitch *);\n   virtual void debug ();\n   virtual void dump (FILE *out);\n \n   back_threader_registry m_registry;\n-  back_threader_profitability m_profit;\n   path_range_query *m_solver;\n \n   // Current path being analyzed.\n@@ -131,8 +148,7 @@ class back_threader\n const edge back_threader::UNREACHABLE_EDGE = (edge) -1;\n \n back_threader::back_threader (function *fun, unsigned flags, bool first)\n-  : m_profit (flags & BT_SPEED),\n-    m_first (first)\n+  : m_first (first)\n {\n   if (flags & BT_SPEED)\n     loop_optimizer_init (LOOPS_HAVE_PREHEADERS | LOOPS_HAVE_SIMPLE_LATCHES);\n@@ -226,7 +242,7 @@ back_threader::maybe_register_path_dump (edge taken)\n // unreachable.\n \n edge\n-back_threader::maybe_register_path ()\n+back_threader::maybe_register_path (back_threader_profitability &profit)\n {\n   edge taken_edge = find_taken_edge (m_path);\n \n@@ -241,8 +257,7 @@ back_threader::maybe_register_path ()\n       else\n \t{\n \t  bool irreducible = false;\n-\t  if (m_profit.profitable_path_p (m_path, m_name, taken_edge,\n-\t\t\t\t\t  &irreducible)\n+\t  if (profit.profitable_path_p (m_path, taken_edge, &irreducible)\n \t      && debug_counter ()\n \t      && m_registry.register_path (m_path, taken_edge))\n \t    {\n@@ -342,17 +357,26 @@ back_threader::find_taken_edge_cond (const vec<basic_block> &path,\n \n void\n back_threader::find_paths_to_names (basic_block bb, bitmap interesting,\n-\t\t\t\t    unsigned overall_paths)\n+\t\t\t\t    unsigned overall_paths,\n+\t\t\t\t    back_threader_profitability &profit)\n {\n   if (m_visited_bbs.add (bb))\n     return;\n \n   m_path.safe_push (bb);\n \n-  // Try to resolve the path without looking back.\n+  // Try to resolve the path without looking back.  Avoid resolving paths\n+  // we know are large but are not (yet) recognized as Finite State Machine.\n+  // ???  Ideally we'd explore the cheapest path to the loop backedge here,\n+  // avoiding the exponential greedy search and only start that from there.\n+  // Precomputing a path-size-to-immediate-dominator-of-successor for each\n+  // edge might help here.  Alternatively copying divergent control flow\n+  // on the way to the backedge could be worthwhile.\n+  bool large_non_fsm;\n   if (m_path.length () > 1\n-      && (!m_profit.profitable_path_p (m_path, m_name, NULL)\n-\t  || maybe_register_path ()))\n+      && (!profit.possibly_profitable_path_p (m_path, m_name, &large_non_fsm)\n+\t  || (!large_non_fsm\n+\t      && maybe_register_path (profit))))\n     ;\n \n   // The backwards thread copier cannot copy blocks that do not belong\n@@ -474,7 +498,8 @@ back_threader::find_paths_to_names (basic_block bb, bitmap interesting,\n \t\t\t}\n \t\t    }\n \t\t}\n-\t      find_paths_to_names (e->src, new_interesting, overall_paths);\n+\t      find_paths_to_names (e->src, new_interesting, overall_paths,\n+\t\t\t\t   profit);\n \t      // Restore new_interesting.\n \t      for (int def : unwind)\n \t\tbitmap_clear_bit (new_interesting, def);\n@@ -499,74 +524,17 @@ back_threader::find_paths_to_names (basic_block bb, bitmap interesting,\n   m_visited_bbs.remove (bb);\n }\n \n-// Search backwards from BB looking for paths where the final\n-// conditional out of BB can be determined.  NAME is the LHS of the\n-// final conditional.  Register such paths for jump threading.\n-\n-void\n-back_threader::find_paths (basic_block bb, tree name)\n-{\n-  gimple *stmt = last_stmt (bb);\n-  if (!stmt\n-      || (gimple_code (stmt) != GIMPLE_COND\n-\t  && gimple_code (stmt) != GIMPLE_SWITCH))\n-    return;\n-\n-  if (EDGE_COUNT (bb->succs) > 1)\n-    {\n-      m_last_stmt = stmt;\n-      m_visited_bbs.empty ();\n-      m_path.truncate (0);\n-      m_name = name;\n-\n-      // We compute imports of the path during discovery starting\n-      // just with names used in the conditional.\n-      bitmap_clear (m_imports);\n-      ssa_op_iter iter;\n-      FOR_EACH_SSA_TREE_OPERAND (name, stmt, iter, SSA_OP_USE)\n-\t{\n-\t  if (!gimple_range_ssa_p (name))\n-\t    return;\n-\t  bitmap_set_bit (m_imports, SSA_NAME_VERSION (name));\n-\t}\n-\n-      // Interesting is the set of imports we still not have see\n-      // the definition of.  So while imports only grow, the\n-      // set of interesting defs dwindles and once empty we can\n-      // stop searching.\n-      auto_bitmap interesting;\n-      bitmap_copy (interesting, m_imports);\n-      find_paths_to_names (bb, interesting, 1);\n-    }\n-}\n-\n-// Simple helper to get the last statement from BB, which is assumed\n-// to be a control statement.  Return NULL if the last statement is\n-// not a control statement.\n-\n-static gimple *\n-get_gimple_control_stmt (basic_block bb)\n-{\n-  gimple_stmt_iterator gsi = gsi_last_nondebug_bb (bb);\n-\n-  if (gsi_end_p (gsi))\n-    return NULL;\n-\n-  gimple *stmt = gsi_stmt (gsi);\n-  enum gimple_code code = gimple_code (stmt);\n-  if (code == GIMPLE_COND || code == GIMPLE_SWITCH || code == GIMPLE_GOTO)\n-    return stmt;\n-  return NULL;\n-}\n-\n // Search backwards from BB looking for paths where the final\n // conditional maybe threaded to a successor block.  Record such paths\n // for jump threading.\n \n void\n back_threader::maybe_thread_block (basic_block bb)\n {\n-  gimple *stmt = get_gimple_control_stmt (bb);\n+  if (EDGE_COUNT (bb->succs) <= 1)\n+    return;\n+\n+  gimple *stmt = last_stmt (bb);\n   if (!stmt)\n     return;\n \n@@ -576,12 +544,33 @@ back_threader::maybe_thread_block (basic_block bb)\n     name = gimple_switch_index (as_a <gswitch *> (stmt));\n   else if (code == GIMPLE_COND)\n     name = gimple_cond_lhs (stmt);\n-  else if (code == GIMPLE_GOTO)\n-    name = gimple_goto_dest (stmt);\n   else\n-    name = NULL;\n+    return;\n \n-  find_paths (bb, name);\n+  m_last_stmt = stmt;\n+  m_visited_bbs.empty ();\n+  m_path.truncate (0);\n+  m_name = name;\n+\n+  // We compute imports of the path during discovery starting\n+  // just with names used in the conditional.\n+  bitmap_clear (m_imports);\n+  ssa_op_iter iter;\n+  FOR_EACH_SSA_TREE_OPERAND (name, stmt, iter, SSA_OP_USE)\n+    {\n+      if (!gimple_range_ssa_p (name))\n+\treturn;\n+      bitmap_set_bit (m_imports, SSA_NAME_VERSION (name));\n+    }\n+\n+  // Interesting is the set of imports we still not have see\n+  // the definition of.  So while imports only grow, the\n+  // set of interesting defs dwindles and once empty we can\n+  // stop searching.\n+  auto_bitmap interesting;\n+  bitmap_copy (interesting, m_imports);\n+  back_threader_profitability profit (m_flags & BT_SPEED, stmt);\n+  find_paths_to_names (bb, interesting, 1, profit);\n }\n \n DEBUG_FUNCTION void\n@@ -615,26 +604,23 @@ back_threader::debug ()\n   dump (stderr);\n }\n \n-/* Examine jump threading path PATH and return TRUE if it is profitable to\n-   thread it, otherwise return FALSE.\n+/* Examine jump threading path PATH and return TRUE if it is possibly\n+   profitable to thread it, otherwise return FALSE.  If this function\n+   returns TRUE profitable_path_p might not be satisfied but when\n+   the path is extended it might be.  In particular indicate in\n+   *LARGE_NON_FSM whether the thread is too large for a non-FSM thread\n+   but would be OK if we extend the path to cover the loop backedge.\n \n    NAME is the SSA_NAME of the variable we found to have a constant\n    value on PATH.  If unknown, SSA_NAME is NULL.\n \n-   If the taken edge out of the path is known ahead of time it is passed in\n-   TAKEN_EDGE, otherwise it is NULL.\n-\n-   CREATES_IRREDUCIBLE_LOOP, if non-null is set to TRUE if threading this path\n-   would create an irreducible loop.\n-\n    ?? It seems we should be able to loosen some of the restrictions in\n    this function after loop optimizations have run.  */\n \n bool\n-back_threader_profitability::profitable_path_p (const vec<basic_block> &m_path,\n-\t\t\t\t\t\ttree name,\n-\t\t\t\t\t\tedge taken_edge,\n-\t\t\t\t\t\tbool *creates_irreducible_loop)\n+back_threader_profitability::possibly_profitable_path_p\n+\t\t\t\t  (const vec<basic_block> &m_path, tree name,\n+\t\t\t\t   bool *large_non_fsm)\n {\n   gcc_checking_assert (!m_path.is_empty ());\n \n@@ -650,13 +636,15 @@ back_threader_profitability::profitable_path_p (const vec<basic_block> &m_path,\n   if (m_path.length () <= 1)\n       return false;\n \n-  int n_insns = 0;\n   gimple_stmt_iterator gsi;\n   loop_p loop = m_path[0]->loop_father;\n-  bool threaded_through_latch = false;\n-  bool multiway_branch_in_path = false;\n-  bool threaded_multiway_branch = false;\n-  bool contains_hot_bb = false;\n+\n+  // We recompute the following, when we rewrite possibly_profitable_path_p\n+  // to work incrementally on added BBs we have to unwind them on backtracking\n+  m_n_insns = 0;\n+  m_threaded_through_latch = false;\n+  m_multiway_branch_in_path = false;\n+  m_contains_hot_bb = false;\n \n   if (dump_file && (dump_flags & TDF_DETAILS))\n     fprintf (dump_file, \"Checking profitability of path (backwards): \");\n@@ -679,7 +667,7 @@ back_threader_profitability::profitable_path_p (const vec<basic_block> &m_path,\n \t it ends in a multiway branch.  */\n       if (j < m_path.length () - 1)\n \t{\n-\t  int orig_n_insns = n_insns;\n+\t  int orig_n_insns = m_n_insns;\n \t  /* PHIs in the path will create degenerate PHIS in the\n \t     copied path which will then get propagated away, so\n \t     looking at just the duplicate path the PHIs would\n@@ -714,12 +702,12 @@ back_threader_profitability::profitable_path_p (const vec<basic_block> &m_path,\n \t\t      && (SSA_NAME_VAR (dst) != SSA_NAME_VAR (name)\n \t\t\t  || !SSA_NAME_VAR (dst))\n \t\t      && !virtual_operand_p (dst))\n-\t\t    ++n_insns;\n+\t\t    ++m_n_insns;\n \t\t}\n \t    }\n \n-\t  if (!contains_hot_bb && m_speed_p)\n-\t    contains_hot_bb |= optimize_bb_for_speed_p (bb);\n+\t  if (!m_contains_hot_bb && m_speed_p)\n+\t    m_contains_hot_bb |= optimize_bb_for_speed_p (bb);\n \t  for (gsi = gsi_after_labels (bb);\n \t       !gsi_end_p (gsi);\n \t       gsi_next_nondebug (&gsi))\n@@ -735,10 +723,10 @@ back_threader_profitability::profitable_path_p (const vec<basic_block> &m_path,\n \t      /* Do not count empty statements and labels.  */\n \t      if (gimple_code (stmt) != GIMPLE_NOP\n \t\t  && !is_gimple_debug (stmt))\n-\t\tn_insns += estimate_num_insns (stmt, &eni_size_weights);\n+\t\tm_n_insns += estimate_num_insns (stmt, &eni_size_weights);\n \t    }\n \t  if (dump_file && (dump_flags & TDF_DETAILS))\n-\t    fprintf (dump_file, \" (%i insns)\", n_insns-orig_n_insns);\n+\t    fprintf (dump_file, \" (%i insns)\", m_n_insns-orig_n_insns);\n \n \t  /* We do not look at the block with the threaded branch\n \t     in this loop.  So if any block with a last statement that\n@@ -747,14 +735,13 @@ back_threader_profitability::profitable_path_p (const vec<basic_block> &m_path,\n \n \t     The block in PATH[0] is special, it's the block were we're\n \t     going to be able to eliminate its branch.  */\n-\t  gimple *last = last_stmt (bb);\n-\t  if (last && (gimple_code (last) == GIMPLE_SWITCH\n-\t\t       || gimple_code (last) == GIMPLE_GOTO))\n+\t  if (j > 0)\n \t    {\n-\t      if (j == 0)\n-\t\tthreaded_multiway_branch = true;\n-\t      else\n-\t\tmultiway_branch_in_path = true;\n+\t      gimple *last = last_stmt (bb);\n+\t      if (last\n+\t\t  && (gimple_code (last) == GIMPLE_SWITCH\n+\t\t      || gimple_code (last) == GIMPLE_GOTO))\n+\t\tm_multiway_branch_in_path = true;\n \t    }\n \t}\n \n@@ -763,80 +750,134 @@ back_threader_profitability::profitable_path_p (const vec<basic_block> &m_path,\n \t through the loop latch.  */\n       if (loop->latch == bb)\n \t{\n-\t  threaded_through_latch = true;\n+\t  m_threaded_through_latch = true;\n \t  if (dump_file && (dump_flags & TDF_DETAILS))\n \t    fprintf (dump_file, \" (latch)\");\n \t}\n     }\n \n-  gimple *stmt = get_gimple_control_stmt (m_path[0]);\n-  gcc_assert (stmt);\n-\n   /* We are going to remove the control statement at the end of the\n      last block in the threading path.  So don't count it against our\n      statement count.  */\n-\n-  int stmt_insns = estimate_num_insns (stmt, &eni_size_weights);\n-  n_insns-= stmt_insns;\n+  m_n_insns -= m_exit_jump_benefit;\n \n   if (dump_file && (dump_flags & TDF_DETAILS))\n     fprintf (dump_file, \"\\n  Control statement insns: %i\\n\"\n \t     \"  Overall: %i insns\\n\",\n-\t     stmt_insns, n_insns);\n-\n-  if (creates_irreducible_loop)\n-    {\n-      /* If this path threaded through the loop latch back into the\n-\t same loop and the destination does not dominate the loop\n-\t latch, then this thread would create an irreducible loop.  */\n-      *creates_irreducible_loop = false;\n-      if (taken_edge\n-\t  && threaded_through_latch\n-\t  && loop == taken_edge->dest->loop_father\n-\t  && (determine_bb_domination_status (loop, taken_edge->dest)\n-\t      == DOMST_NONDOMINATING))\n-\t*creates_irreducible_loop = true;\n-    }\n+\t     m_exit_jump_benefit, m_n_insns);\n \n   /* Threading is profitable if the path duplicated is hot but also\n      in a case we separate cold path from hot path and permit optimization\n      of the hot path later.  Be on the agressive side here. In some testcases,\n      as in PR 78407 this leads to noticeable improvements.  */\n-  if (m_speed_p\n-      && ((taken_edge && optimize_edge_for_speed_p (taken_edge))\n-\t  || contains_hot_bb))\n+  if (m_speed_p)\n     {\n-      if (n_insns >= param_max_fsm_thread_path_insns)\n+      if (m_n_insns >= param_max_fsm_thread_path_insns)\n \t{\n \t  if (dump_file && (dump_flags & TDF_DETAILS))\n \t    fprintf (dump_file, \"  FAIL: Jump-thread path not considered: \"\n \t\t     \"the number of instructions on the path \"\n \t\t     \"exceeds PARAM_MAX_FSM_THREAD_PATH_INSNS.\\n\");\n \t  return false;\n \t}\n-      if (taken_edge && probably_never_executed_edge_p (cfun, taken_edge))\n+      edge entry = find_edge (m_path[m_path.length () - 1],\n+\t\t\t      m_path[m_path.length () - 2]);\n+      if (probably_never_executed_edge_p (cfun, entry))\n \t{\n \t  if (dump_file && (dump_flags & TDF_DETAILS))\n \t    fprintf (dump_file, \"  FAIL: Jump-thread path not considered: \"\n-\t\t     \"path leads to probably never executed edge.\\n\");\n+\t\t     \"path entry is probably never executed.\\n\");\n \t  return false;\n \t}\n-      edge entry = find_edge (m_path[m_path.length () - 1],\n-\t\t\t      m_path[m_path.length () - 2]);\n-      if (probably_never_executed_edge_p (cfun, entry))\n+    }\n+  else if (m_n_insns > 1)\n+    {\n+      if (dump_file && (dump_flags & TDF_DETAILS))\n+\tfprintf (dump_file, \"  FAIL: Jump-thread path not considered: \"\n+\t\t \"duplication of %i insns is needed and optimizing for size.\\n\",\n+\t\t m_n_insns);\n+      return false;\n+    }\n+\n+  /* The generic copier used by the backthreader does not re-use an\n+     existing threading path to reduce code duplication.  So for that\n+     case, drastically reduce the number of statements we are allowed\n+     to copy.  We don't know yet whether we will thread through the latch\n+     so we have to be permissive and continue threading, but indicate\n+     to the caller the thread, if final, wouldn't be profitable.  */\n+  if ((!m_threaded_multiway_branch\n+       || !loop->latch\n+       || loop->latch->index == EXIT_BLOCK)\n+      && (m_n_insns * param_fsm_scale_path_stmts\n+\t  >= param_max_jump_thread_duplication_stmts))\n+    {\n+      if (dump_file && (dump_flags & TDF_DETAILS))\n+\tfprintf (dump_file,\n+\t\t \"  FAIL: Did not thread around loop and would copy too \"\n+\t\t \"many statements.\\n\");\n+      return false;\n+    }\n+  *large_non_fsm = (!(m_threaded_through_latch && m_threaded_multiway_branch)\n+\t\t    && (m_n_insns * param_fsm_scale_path_stmts\n+\t\t\t>= param_max_jump_thread_duplication_stmts));\n+\n+  return true;\n+}\n+\n+/* Examine jump threading path PATH and return TRUE if it is profitable to\n+   thread it, otherwise return FALSE.\n+\n+   The taken edge out of the path is TAKEN_EDGE.\n+\n+   CREATES_IRREDUCIBLE_LOOP is set to TRUE if threading this path\n+   would create an irreducible loop.\n+\n+   ?? It seems we should be able to loosen some of the restrictions in\n+   this function after loop optimizations have run.  */\n+\n+bool\n+back_threader_profitability::profitable_path_p (const vec<basic_block> &m_path,\n+\t\t\t\t\t\tedge taken_edge,\n+\t\t\t\t\t\tbool *creates_irreducible_loop)\n+{\n+  // We can assume that possibly_profitable_path_p holds here\n+\n+  loop_p loop = m_path[0]->loop_father;\n+\n+  if (dump_file && (dump_flags & TDF_DETAILS))\n+    fprintf (dump_file, \"Checking profitability of path (backwards): \");\n+\n+  /* If this path threaded through the loop latch back into the\n+     same loop and the destination does not dominate the loop\n+     latch, then this thread would create an irreducible loop.  */\n+  *creates_irreducible_loop = false;\n+  if (m_threaded_through_latch\n+      && loop == taken_edge->dest->loop_father\n+      && (determine_bb_domination_status (loop, taken_edge->dest)\n+\t  == DOMST_NONDOMINATING))\n+    *creates_irreducible_loop = true;\n+\n+  /* Threading is profitable if the path duplicated is hot but also\n+     in a case we separate cold path from hot path and permit optimization\n+     of the hot path later.  Be on the agressive side here. In some testcases,\n+     as in PR 78407 this leads to noticeable improvements.  */\n+  if (m_speed_p\n+      && (optimize_edge_for_speed_p (taken_edge) || m_contains_hot_bb))\n+    {\n+      if (probably_never_executed_edge_p (cfun, taken_edge))\n \t{\n \t  if (dump_file && (dump_flags & TDF_DETAILS))\n \t    fprintf (dump_file, \"  FAIL: Jump-thread path not considered: \"\n-\t\t     \"path entry is probably never executed.\\n\");\n+\t\t     \"path leads to probably never executed edge.\\n\");\n \t  return false;\n \t}\n     }\n-  else if (n_insns > 1)\n+  else if (m_n_insns > 1)\n     {\n       if (dump_file && (dump_flags & TDF_DETAILS))\n \tfprintf (dump_file, \"  FAIL: Jump-thread path not considered: \"\n \t\t \"duplication of %i insns is needed and optimizing for size.\\n\",\n-\t\t n_insns);\n+\t\t m_n_insns);\n       return false;\n     }\n \n@@ -849,10 +890,9 @@ back_threader_profitability::profitable_path_p (const vec<basic_block> &m_path,\n      the path -- in that case there's little the traditional loop\n      optimizer would have done anyway, so an irreducible loop is not\n      so bad.  */\n-  if (!threaded_multiway_branch\n-      && creates_irreducible_loop\n+  if (!m_threaded_multiway_branch\n       && *creates_irreducible_loop\n-      && (n_insns * (unsigned) param_fsm_scale_path_stmts\n+      && (m_n_insns * (unsigned) param_fsm_scale_path_stmts\n \t  > (m_path.length () *\n \t     (unsigned) param_fsm_scale_path_blocks)))\n \n@@ -861,15 +901,16 @@ back_threader_profitability::profitable_path_p (const vec<basic_block> &m_path,\n \tfprintf (dump_file,\n \t\t \"  FAIL: Would create irreducible loop without threading \"\n \t\t \"multiway branch.\\n\");\n+      /* We compute creates_irreducible_loop only late.  */\n       return false;\n     }\n \n   /* The generic copier used by the backthreader does not re-use an\n      existing threading path to reduce code duplication.  So for that\n      case, drastically reduce the number of statements we are allowed\n      to copy.  */\n-  if (!(threaded_through_latch && threaded_multiway_branch)\n-      && (n_insns * param_fsm_scale_path_stmts\n+  if (!(m_threaded_through_latch && m_threaded_multiway_branch)\n+      && (m_n_insns * param_fsm_scale_path_stmts\n \t  >= param_max_jump_thread_duplication_stmts))\n     {\n       if (dump_file && (dump_flags & TDF_DETAILS))\n@@ -883,7 +924,7 @@ back_threader_profitability::profitable_path_p (const vec<basic_block> &m_path,\n      explode the CFG due to duplicating the edges for that multi-way\n      branch.  So like above, only allow a multi-way branch on the path\n      if we actually thread a multi-way branch.  */\n-  if (!threaded_multiway_branch && multiway_branch_in_path)\n+  if (!m_threaded_multiway_branch && m_multiway_branch_in_path)\n     {\n       if (dump_file && (dump_flags & TDF_DETAILS))\n \tfprintf (dump_file,\n@@ -896,20 +937,20 @@ back_threader_profitability::profitable_path_p (const vec<basic_block> &m_path,\n      the latch.  This could alter the loop form sufficiently to cause\n      loop optimizations to fail.  Disable these threads until after\n      loop optimizations have run.  */\n-  if ((threaded_through_latch\n-       || (taken_edge && taken_edge->dest == loop->latch))\n+  if ((m_threaded_through_latch || taken_edge->dest == loop->latch)\n       && !(cfun->curr_properties & PROP_loop_opts_done)\n       && empty_block_p (loop->latch))\n     {\n       if (dump_file && (dump_flags & TDF_DETAILS))\n \tfprintf (dump_file,\n-\t\t \"  FAIL: Thread through latch before loop opts would create non-empty latch\\n\");\n+\t\t \"  FAIL: Thread through latch before loop opts would create \"\n+\t\t \"non-empty latch\\n\");\n       return false;\n-\n     }\n   return true;\n }\n \n+\n /* The current path PATH is a vector of blocks forming a jump threading\n    path in reverse order.  TAKEN_EDGE is the edge taken from path[0].\n "}]}