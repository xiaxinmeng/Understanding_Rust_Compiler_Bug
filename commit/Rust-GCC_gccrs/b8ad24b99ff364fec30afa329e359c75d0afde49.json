{"sha": "b8ad24b99ff364fec30afa329e359c75d0afde49", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YjhhZDI0Yjk5ZmYzNjRmZWMzMGFmYTMyOWUzNTljNzVkMGFmZGU0OQ==", "commit": {"author": {"name": "Dhruv Matani", "email": "dhruvbird@gmx.net", "date": "2004-12-19T11:04:48Z"}, "committer": {"name": "Paolo Carlini", "email": "paolo@gcc.gnu.org", "date": "2004-12-19T11:04:48Z"}, "message": "allocator.html: Correct link.\n\n2004-12-19  Dhruv Matani  <dhruvbird@gmx.net>\n\n\t* docs/html/20_util/allocator.html: Correct link.\n\t* docs/html/ext/ballocator_doc.txt: Remove.\n\t* docs/html/ext/ballocator_doc.html: Add.\n\nFrom-SVN: r92375", "tree": {"sha": "4941df885867fc977dbdac7ce294e23ae54ccfd7", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/4941df885867fc977dbdac7ce294e23ae54ccfd7"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/b8ad24b99ff364fec30afa329e359c75d0afde49", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b8ad24b99ff364fec30afa329e359c75d0afde49", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b8ad24b99ff364fec30afa329e359c75d0afde49", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b8ad24b99ff364fec30afa329e359c75d0afde49/comments", "author": null, "committer": null, "parents": [{"sha": "3600c6dbda496240a8caa5dfe30b4b394fa4622c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3600c6dbda496240a8caa5dfe30b4b394fa4622c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/3600c6dbda496240a8caa5dfe30b4b394fa4622c"}], "stats": {"total": 808, "additions": 433, "deletions": 375}, "files": [{"sha": "d49da6cb480f6dcce55f19fbcdc78df8fcadecb3", "filename": "libstdc++-v3/ChangeLog", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b8ad24b99ff364fec30afa329e359c75d0afde49/libstdc%2B%2B-v3%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b8ad24b99ff364fec30afa329e359c75d0afde49/libstdc%2B%2B-v3%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2FChangeLog?ref=b8ad24b99ff364fec30afa329e359c75d0afde49", "patch": "@@ -1,3 +1,9 @@\n+2004-12-19  Dhruv Matani  <dhruvbird@gmx.net>\n+\t\n+\t* docs/html/20_util/allocator.html: Correct link.\n+\t* docs/html/ext/ballocator_doc.txt: Remove.\n+\t* docs/html/ext/ballocator_doc.html: Add.\n+\n 2004-12-16  Danny Smith  <dannysmith@users.sourceforge.net>\n \n \tPR target/18997"}, {"sha": "d847fc0afc927e06e047a0c6a12183cc5ef1f324", "filename": "libstdc++-v3/docs/html/20_util/allocator.html", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b8ad24b99ff364fec30afa329e359c75d0afde49/libstdc%2B%2B-v3%2Fdocs%2Fhtml%2F20_util%2Fallocator.html", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b8ad24b99ff364fec30afa329e359c75d0afde49/libstdc%2B%2B-v3%2Fdocs%2Fhtml%2F20_util%2Fallocator.html", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Fdocs%2Fhtml%2F20_util%2Fallocator.html?ref=b8ad24b99ff364fec30afa329e359c75d0afde49", "patch": "@@ -434,7 +434,7 @@ <h3 class=\"left\">\n      <p>A high-performance allocator that uses a bit-map to keep track\n      of the used and unused memory locations. It has its own\n      documentation, found <a\n-     href=\"../ext/ballocator_doc.txt\">here</a>.\n+     href=\"../ext/ballocator_doc.html\">here</a>.\n      </p>\n      </li>\n    </ul>"}, {"sha": "7b1f4d23ede60c9c53741eeeb6d2bb806461a651", "filename": "libstdc++-v3/docs/html/ext/ballocator_doc.html", "status": "added", "additions": 426, "deletions": 0, "changes": 426, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b8ad24b99ff364fec30afa329e359c75d0afde49/libstdc%2B%2B-v3%2Fdocs%2Fhtml%2Fext%2Fballocator_doc.html", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b8ad24b99ff364fec30afa329e359c75d0afde49/libstdc%2B%2B-v3%2Fdocs%2Fhtml%2Fext%2Fballocator_doc.html", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Fdocs%2Fhtml%2Fext%2Fballocator_doc.html?ref=b8ad24b99ff364fec30afa329e359c75d0afde49", "patch": "@@ -0,0 +1,426 @@\n+<!DOCTYPE html PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">\n+<html>\n+<head>\n+  <meta content=\"text/html; charset=ISO-8859-1\"\n+ http-equiv=\"content-type\">\n+  <title>Bitmap Allocator</title>\n+  <meta content=\"Dhruv Matani\" name=\"author\">\n+  <meta content=\"Bitmap Allocator\" name=\"description\">\n+</head>\n+<body>\n+<h1 style=\"text-align: center;\">Bitmap Allocator</h1>\n+<em><br>\n+<small><small>The latest version of this document is always available\n+at <a\n+ href=\"http://gcc.gnu.org/onlinedocs/libstdc++/ext/ballocator_doc.html\">\n+http://gcc.gnu.org/onlinedocs/libstdc++/ext/ballocator_doc.html</a>.</small></small></em><br>\n+<br>\n+<em> To the <a href=\"http://gcc.gnu.org/libstdc++/\">libstdc++-v3\n+homepage</a>.</em><br>\n+<br>\n+<hr style=\"width: 100%; height: 2px;\"><br>\n+As this name suggests, this allocator uses a bit-map to keep track of\n+the used and unused memory locations for it's book-keeping purposes.<br>\n+<br>\n+This allocator will make use of 1 single bit to keep track of whether\n+it has been allocated or not. A bit 1 indicates free, while 0 indicates\n+allocated. This has been done so that you can easily check a collection\n+of bits for a free block. This kind of Bitmapped strategy works best\n+for single object allocations, and with the STL type parameterized\n+allocators, we do not need to choose any size for the block which will\n+be represented by a single bit. This will be the size of the parameter\n+around which the allocator has been parameterized. Thus, close to\n+optimal performance will result. Hence, this should be used for node\n+based containers which call the allocate function with an argument of 1.<br>\n+<br>\n+The bitmapped allocator's internal pool is exponentially growing.\n+Meaning that internally, the blocks acquired from the Free List Store\n+will double every time the bitmapped allocator runs out of memory.<br>\n+<br>\n+<hr style=\"width: 100%; height: 2px;\"><br>\n+The macro __GTHREADS decides whether to use Mutex Protection around\n+every allocation/deallocation. The state of the macro is picked up\n+automatically from the gthr abstration layer.<br>\n+<br>\n+<hr style=\"width: 100%; height: 2px;\">\n+<h3 style=\"text-align: center;\">What is the Free List Store?</h3>\n+<br>\n+The Free List Store (referred to as FLS for the remaining part of this\n+document) is the Global memory pool that is shared by all instances of\n+the bitmapped allocator instantiated for any type. This maintains a\n+sorted order of all free memory blocks given back to it by the\n+bitmapped allocator, and is also responsible for giving memory to the\n+bitmapped allocator when it asks for more.<br>\n+<br>\n+Internally, there is a Free List threshold which indicates the Maximum\n+number of free lists that the FLS can hold internally (cache).\n+Currently, this value is set at 64. So, if there are more than 64 free\n+lists coming in, then some of them will be given back to the OS using\n+operator delete so that at any given time the Free List's size does not\n+exceed 64 entries. This is done because a Binary Search is used to\n+locate an entry in a free list when a request for memory comes along.\n+Thus, the run-time complexity of the search would go up given an\n+increasing size, for 64 entries however, lg(64) == 6 comparisons are\n+enough to locate the correct free list if it exists.<br>\n+<br>\n+Suppose the free list size has reached it's threshold, then the largest\n+block from among those in the list and the new block will be selected\n+and given back to the OS. This is done because it reduces external\n+fragmentation, and allows the OS to use the larger blocks later in an\n+orderly fashion, possibly merging them later. Also, on some systems,\n+large blocks are obtained via calls to mmap, so giving them back to\n+free system resources becomes most important.<br>\n+<br>\n+The function _S_should_i_give decides the policy that determines\n+whether the current block of memory should be given to the allocator\n+for the request that it has made. That's because we may not always have\n+exact fits for the memory size that the allocator requests. We do this\n+mainly to prevent external fragmentation at the cost of a little\n+internal fragmentation. Now, the value of this internal fragmentation\n+has to be decided by this function. I can see 3 possibilities right\n+now. Please add more as and when you find better strategies.<br>\n+<br>\n+<ol>\n+  <li>Equal size check. Return true only when the 2 blocks are of equal\n+size.</li>\n+  <li>Difference Threshold: Return true only when the _block_size is\n+greater than or equal to the _required_size, and if the _BS is &gt; _RS\n+by a difference of less than some THRESHOLD value, then return true,\n+else return false. </li>\n+  <li>Percentage Threshold. Return true only when the _block_size is\n+greater than or equal to the _required_size, and if the _BS is &gt; _RS\n+by a percentage of less than some THRESHOLD value, then return true,\n+else return false.</li>\n+</ol>\n+<br>\n+Currently, (3) is being used with a value of 36% Maximum wastage per\n+Super Block.<br>\n+<br>\n+<hr style=\"width: 100%; height: 2px;\"><span style=\"font-weight: bold;\">1)\n+What is a super block? Why is it needed?</span><br>\n+<br>\n+A super block is the block of memory acquired from the FLS from which\n+the bitmap allocator carves out memory for single objects and satisfies\n+the user's requests. These super blocks come in sizes that are powers\n+of 2 and multiples of 32 (_Bits_Per_Block). Yes both at the same time!\n+That's because the next super block acquired will be 2 times the\n+previous one, and also all super blocks have to be multiples of the\n+_Bits_Per_Block value. <br>\n+<br>\n+<span style=\"font-weight: bold;\">2) How does it interact with the free\n+list store?</span><br>\n+<br>\n+The super block is contained in the FLS, and the FLS is responsible for\n+getting / returning Super Bocks to and from the OS using operator new\n+as defined by the C++ standard.<br>\n+<br>\n+<hr style=\"width: 100%; height: 2px;\">\n+<h3 style=\"text-align: center;\">How does the allocate function Work?</h3>\n+<br>\n+The allocate function is specialized for single object allocation ONLY.\n+Thus, ONLY if n == 1, will the bitmap_allocator's specialized algorithm\n+be used. Otherwise, the request is satisfied directly by calling\n+operator new.<br>\n+<br>\n+Suppose n == 1, then the allocator does the following:<br>\n+<br>\n+<ol>\n+  <li>Checks to see whether the a free block exists somewhere in a\n+region of memory close to the last satisfied request. If so, then that\n+block is marked as allocated in the bit map and given to the user. If\n+not, then (2) is executed.</li>\n+  <li>Is there a free block anywhere after the current block right upto\n+the end of the memory that we have? If so, that block is found, and the\n+same procedure is applied as above, and returned to the user. If not,\n+then (3) is executed.</li>\n+  <li>Is there any block in whatever region of memory that we own free?\n+This is done by checking <br>\n+    <div style=\"margin-left: 40px;\">\n+    <ul>\n+      <li>The use count for each super block, and if that fails then </li>\n+      <li>The individual bit-maps for each super block. </li>\n+    </ul>\n+    </div>\n+Note: Here we are never touching any of the memory that the user will\n+be given, and we are confining all memory accesses to a small region of\n+memory! This helps reduce cache misses. If this succeeds then we apply\n+the same procedure on that bit-map as (1), and return that block of\n+memory to the user. However, if this process fails, then we resort to\n+(4).</li>\n+  <li>This process involves Refilling the internal exponentially\n+growing memory pool. The said effect is achieved by calling\n+_S_refill_pool which does the following: <br>\n+    <div style=\"margin-left: 40px;\">\n+    <ul>\n+      <li>Gets more memory from the Global Free List of the Required\n+size. </li>\n+      <li>Adjusts the size for the next call to itself. </li>\n+      <li>Writes the appropriate headers in the bit-maps.</li>\n+      <li>Sets the use count for that super-block just allocated to 0\n+(zero). </li>\n+      <li>All of the above accounts to maintaining the basic invariant\n+for the allocator. If the invariant is maintained, we are sure that all\n+is well. Now, the same process is applied on the newly acquired free\n+blocks, which are dispatched accordingly.</li>\n+    </ul>\n+    </div>\n+  </li>\n+</ol>\n+<br>\n+Thus, you can clearly see that the allocate function is nothing but a\n+combination of the next-fit and first-fit algorithm optimized ONLY for\n+single object allocations.<br>\n+<br>\n+<br>\n+<hr style=\"width: 100%; height: 2px;\">\n+<h3 style=\"text-align: center;\">How does the deallocate function work?</h3>\n+<br>\n+The deallocate function again is specialized for single objects ONLY.\n+For all n belonging to &gt; 1, the operator delete is called without\n+further ado, and the deallocate function returns.<br>\n+<br>\n+However for n == 1, a series of steps are performed:<br>\n+<br>\n+<ol>\n+  <li>We first need to locate that super-block which holds the memory\n+location given to us by the user. For that purpose, we maintain a\n+static variable _S_last_dealloc_index, which holds the index into the\n+vector of block pairs which indicates the index of the last super-block\n+from which memory was freed. We use this strategy in the hope that the\n+user will deallocate memory in a region close to what he/she\n+deallocated the last time around. If the check for belongs_to succeeds,\n+then we determine the bit-map for the given pointer, and locate the\n+index into that bit-map, and mark that bit as free by setting it.</li>\n+  <li>If the _S_last_dealloc_index does not point to the memory block\n+that we're looking for, then we do a linear search on the block stored\n+in the vector of Block Pairs. This vector in code is called\n+_S_mem_blocks. When the corresponding super-block is found, we apply\n+the same procedure as we did for (1) to mark the block as free in the\n+bit-map.</li>\n+</ol>\n+<br>\n+Now, whenever a block is freed, the use count of that particular super\n+block goes down by 1. When this use count hits 0, we remove that super\n+block from the list of all valid super blocks stored in the vector.\n+While doing this, we also make sure that the basic invariant is\n+maintained by making sure that _S_last_request and\n+_S_last_dealloc_index point to valid locations within the vector.<br>\n+<br>\n+<hr style=\"width: 100%; height: 2px;\"><br>\n+<h3 style=\"text-align: center;\">Data Layout for a Super Block:</h3>\n+<br>\n+Each Super Block will be of some size that is a multiple of the number\n+of Bits Per Block. Typically, this value is chosen as Bits_Per_Byte x\n+sizeof(size_t). On an x86 system, this gives the figure &nbsp;8 x\n+4 = 32. Thus, each Super Block will be of size 32 x Some_Value. This\n+Some_Value is sizeof(value_type). For now, let it be called 'K'. Thus,\n+finally, Super Block size is 32 x K bytes.<br>\n+<br>\n+This value of 32 has been chosen because each size_t has 32-bits\n+and Maximum use of these can be made with such a figure.<br>\n+<br>\n+Consider a block of size 64 ints. In memory, it would look like this:\n+(assume a 32-bit system where, size_t is a 32-bit entity).<br>\n+<br>\n+<table cellpadding=\"0\" cellspacing=\"0\" border=\"1\"\n+ style=\"text-align: left; width: 763px; height: 21px;\">\n+  <tbody>\n+    <tr>\n+      <td style=\"vertical-align: top; text-align: center;\">268<br>\n+      </td>\n+      <td style=\"vertical-align: top; text-align: center;\">0<br>\n+      </td>\n+      <td style=\"vertical-align: top; text-align: center;\">4294967295<br>\n+      </td>\n+      <td style=\"vertical-align: top; text-align: center;\">4294967295<br>\n+      </td>\n+      <td style=\"vertical-align: top; text-align: center;\">Data -&gt;\n+Space for 64 ints<br>\n+      </td>\n+    </tr>\n+  </tbody>\n+</table>\n+<br>\n+<br>\n+The first Column(268) represents the size of the Block in bytes as seen\n+by\n+the Bitmap Allocator. Internally, a global free list is used to keep\n+track of the free blocks used and given back by the bitmap allocator.\n+It is this Free List Store that is responsible for writing and managing\n+this information. Actually the number of bytes allocated in this case\n+would be: 4 + 4 + (4x2) + (64x4) = 272 bytes, but the first 4 bytes are\n+an\n+addition by the Free List Store, so the Bitmap Allocator sees only 268\n+bytes. These first 4 bytes about which the bitmapped allocator is not\n+aware hold the value 268.<br>\n+<br>\n+<span style=\"font-weight: bold;\">What do the remaining values represent?</span><br>\n+<br>\n+The 2nd 4 in the expression is the sizeof(size_t) because the\n+Bitmapped Allocator maintains a used count for each Super Block, which\n+is initially set to 0 (as indicated in the diagram). This is\n+incremented every time a block is removed from this super block\n+(allocated), and decremented whenever it is given back. So, when the\n+used count falls to 0, the whole super block will be given back to the\n+Free List Store.<br>\n+<br>\n+The value 4294967295 represents the integer corresponding to the bit\n+representation of all bits set: 11111111111111111111111111111111.<br>\n+<br>\n+The 3rd 4x2 is size of the bitmap itself, which is the size of 32-bits\n+x 2,\n+which is 8-bytes, or 2 x sizeof(size_t).<br>\n+<br>\n+<hr style=\"width: 100%; height: 2px;\"><br>\n+Another issue would be whether to keep the all bitmaps in a separate\n+area in memory, or to keep them near the actual blocks that will be\n+given out or allocated for the client. After some testing, I've decided\n+to keep these bitmaps close to the actual blocks. this will help in 2\n+ways. <br>\n+<br>\n+<ol>\n+  <li>Constant time access for the bitmap themselves, since no kind of\n+look up will be needed to find the correct bitmap list or it's\n+equivalent.</li>\n+  <li>And also this would preserve the cache as far as possible.</li>\n+</ol>\n+<br>\n+So in effect, this kind of an allocator might prove beneficial from a\n+purely cache point of view. But this allocator has been made to try and\n+roll out the defects of the node_allocator, wherein the nodes get\n+skewed about in memory, if they are not returned in the exact reverse\n+order or in the same order in which they were allocated. Also, the\n+new_allocator's book keeping overhead is too much for small objects and\n+single object allocations, though it preserves the locality of blocks\n+very well when they are returned back to the allocator.<br>\n+<br>\n+<hr style=\"width: 100%; height: 2px;\"><br>\n+Expected overhead per block would be 1 bit in memory. Also, once the\n+address of the free list has been found, the cost for\n+allocation/deallocation would be negligible, and is supposed to be\n+constant time. For these very reasons, it is very important to minimize\n+the linear time costs, which include finding a free list with a free\n+block while allocating, and finding the corresponding free list for a\n+block while deallocating. Therefore, I have decided that the growth of\n+the internal pool for this allocator will be exponential as compared to\n+linear for node_allocator. There, linear time works well, because we\n+are mainly concerned with speed of allocation/deallocation and memory\n+consumption, whereas here, the allocation/deallocation part does have\n+some linear/logarithmic complexity components in it. Thus, to try and\n+minimize them would be a good thing to do at the cost of a little bit\n+of memory.<br>\n+<br>\n+Another thing to be noted is the the pool size will double every time\n+the internal pool gets exhausted, and all the free blocks have been\n+given away. The initial size of the pool would be sizeof(size_t) x 8\n+which is the number of bits in an integer, which can fit exactly\n+in a CPU register. Hence, the term given is exponential growth of the\n+internal pool.<br>\n+<br>\n+<hr style=\"width: 100%; height: 2px;\">After reading all this, you may\n+still have a few questions about the internal working of this\n+allocator, like my friend had!<br>\n+<br>\n+Well here are the exact questions that he posed:<br>\n+<br>\n+<span style=\"font-weight: bold;\">Q1) The \"Data Layout\" section is\n+cryptic. I have no idea of what you are trying to say. Layout of what?\n+The free-list? Each bitmap? The Super Block?</span><br>\n+<br>\n+<div style=\"margin-left: 40px;\"> The layout of a Super Block of a given\n+size. In the example, a super block of size 32 x 1 is taken. The\n+general formula for calculating the size of a super block is\n+32 x sizeof(value_type) x 2^n, where n ranges from 0 to 32 for 32-bit\n+systems.<br>\n+</div>\n+<br>\n+<span style=\"font-weight: bold;\">Q2) And since I just mentioned the\n+term `each bitmap', what in the world is meant by it? What does each\n+bitmap manage? How does it relate to the super block? Is the Super\n+Block a bitmap as well?</span><br style=\"font-weight: bold;\">\n+<br>\n+<div style=\"margin-left: 40px;\"> Good question! Each bitmap is part of\n+a\n+Super Block which is made up of 3 parts as I have mentioned earlier.\n+Re-iterating, 1. The use count, 2. The bit-map for that Super Block. 3.\n+The actual memory that will be eventually given to the user. Each\n+bitmap is a multiple of 32 in size. If there are 32 x (2^3) blocks of\n+single objects to be given, there will be '32 x (2^3)' bits present.\n+Each\n+32 bits managing the allocated / free status for 32 blocks. Since each\n+size_t contains 32-bits, one size_t can manage upto 32\n+blocks' status. Each bit-map is made up of a number of size_t,\n+whose exact number for a super-block of a given size I have just\n+mentioned.<br>\n+</div>\n+<br>\n+<span style=\"font-weight: bold;\">Q3) How do the allocate and deallocate\n+functions work in regard to bitmaps?</span><br>\n+<br>\n+<div style=\"margin-left: 40px;\"> The allocate and deallocate functions\n+manipulate the bitmaps and have nothing to do with the memory that is\n+given to the user. As I have earlier mentioned, a 1 in the bitmap's bit\n+field indicates free, while a 0 indicates allocated. This lets us check\n+32 bits at a time to check whether there is at lease one free block in\n+those 32 blocks by testing for equality with (0). Now, the allocate\n+function will given a memory block find the corresponding bit in the\n+bitmap, and will reset it (ie. make it re-set (0)). And when the\n+deallocate function is called, it will again set that bit after\n+locating it to indicate that that particular block corresponding to\n+this bit in the bit-map is not being used by anyone, and may be used to\n+satisfy future requests.<br>\n+<br>\n+eg: Consider a bit-map of 64-bits as represented below:<br>\n+1111111111111111111111111111111111111111111111111111111111111111<br>\n+<br>\n+Now, when the first request for allocation of a single object comes\n+along, the first block in address order is returned. And since the\n+bit-maps in the reverse order to that of the address order, the last\n+bit(LSB if the bit-map is considered as a binary word of 64-bits) is\n+re-set to 0.<br>\n+<br>\n+The bit-map now looks like this:<br>\n+1111111111111111111111111111111111111111111111111111111111111110<br>\n+</div>\n+<br>\n+<br>\n+<hr style=\"width: 100%; height: 2px;\"><br>\n+(Tech-Stuff, Please stay out if you are not interested in the selection\n+of certain constants. This has nothing to do with the algorithm per-se,\n+only with some vales that must be chosen correctly to ensure that the\n+allocator performs well in a real word scenario, and maintains a good\n+balance between the memory consumption and the allocation/deallocation\n+speed).<br>\n+<br>\n+The formula for calculating the maximum wastage as a percentage:<br>\n+<br>\n+(32 x k + 1) / (2 x (32 x k + 1 + 32 x c)) x 100.<br>\n+<br>\n+Where,<br>\n+k =&gt; The constant overhead per node. eg. for list, it is 8 bytes,\n+and for map it is 12 bytes.<br>\n+c =&gt; The size of the base type on which the map/list is\n+instantiated. Thus, suppose the the type1 is int and type2 is double,\n+they are related by the relation sizeof(double) == 2*sizeof(int). Thus,\n+all types must have this double size relation for this formula to work\n+properly.<br>\n+<br>\n+Plugging-in: For List: k = 8 and c = 4 (int and double), we get:<br>\n+33.376%<br>\n+<br>\n+For map/multimap: k = 12, and c = 4 (int and double), we get:<br>\n+37.524%<br>\n+<br>\n+Thus, knowing these values, and based on the sizeof(value_type), we may\n+create a function that returns the Max_Wastage_Percentage for us to use.<br>\n+<br>\n+<hr style=\"width: 100%; height: 2px;\"><small><small><em> See <a\n+ href=\"file:///home/dhruv/projects/libstdc++-v3/gcc/libstdc++-v3/docs/html/17_intro/license.html\">license.html</a>\n+for copying conditions. Comments and suggestions are welcome, and may\n+be\n+sent to <a href=\"mailto:libstdc++@gcc.gnu.org\">the libstdc++ mailing\n+list</a>.</em><br>\n+</small></small><br>\n+<br>\n+</body>\n+</html>"}, {"sha": "2173b618f4f8836f35ed833c308a8b0a95a89879", "filename": "libstdc++-v3/docs/html/ext/ballocator_doc.txt", "status": "removed", "additions": 0, "deletions": 374, "changes": 374, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3600c6dbda496240a8caa5dfe30b4b394fa4622c/libstdc%2B%2B-v3%2Fdocs%2Fhtml%2Fext%2Fballocator_doc.txt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3600c6dbda496240a8caa5dfe30b4b394fa4622c/libstdc%2B%2B-v3%2Fdocs%2Fhtml%2Fext%2Fballocator_doc.txt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Fdocs%2Fhtml%2Fext%2Fballocator_doc.txt?ref=3600c6dbda496240a8caa5dfe30b4b394fa4622c", "patch": "@@ -1,374 +0,0 @@\n-\t\t\tBITMAPPED ALLOCATOR\n-\t\t\t===================\n-\n-2004-03-11  Dhruv Matani  <dhruvbird@HotPOP.com>\n-\n----------------------------------------------------------------------\n-\n-As this name suggests, this allocator uses a bit-map to keep track of\n-the used and unused memory locations for it's book-keeping purposes.\n-\n-This allocator will make use of 1 single bit to keep track of whether\n-it has been allocated or not. A bit 1 indicates free, while 0\n-indicates allocated. This has been done so that you can easily check a\n-collection of bits for a free block. This kind of Bitmapped strategy\n-works best for single object allocations, and with the STL type\n-parameterized allocators, we do not need to choose any size for the\n-block which will be represented by a single bit. This will be the size\n-of the parameter around which the allocator has been\n-parameterized. Thus, close to optimal performance will result. Hence,\n-this should be used for node based containers which call the allocate\n-function with an argument of 1.\n-\n-The bitmapped allocator's internal pool is exponentially\n-growing. Meaning that internally, the blocks acquired from the Free\n-List Store will double every time the bitmapped allocator runs out of\n-memory.\n-\n---------------------------------------------------------------------\n-\n-The macro __GTHREADS decides whether to use Mutex Protection around\n-every allocation/deallocation.  The state of the macro is picked up\n-automatically from the gthr abstration layer.\n-\n-----------------------------------------------------------------------\n-\n-What is the Free List Store?\n-----------------------------\n-\n-The Free List Store (referred to as FLS for the remaining part of this\n-document) is the Global memory pool that is shared by all instances of\n-the bitmapped allocator instantiated for any type. This maintains a\n-sorted order of all free memory blocks given back to it by the\n-bitmapped allocator, and is also responsible for giving memory to the\n-bitmapped allocator when it asks for more.\n-\n-Internally, there is a Free List threshold which indicates the Maximum\n-number of free lists that the FLS can hold internally\n-(cache). Currently, this value is set at 64. So, if there are more\n-than 64 free lists coming in, then some of them will be given back to\n-the OS using operator delete so that at any given time the Free List's\n-size does not exceed 64 entries. This is done because a Binary Search\n-is used to locate an entry in a free list when a request for memory\n-comes along. Thus, the run-time complexity of the search would go up\n-given an increasing size, for 64 entries however, lg(64) == 6\n-comparisons are enough to locate the correct free list if it exists.\n-\n-Suppose the free list size has reached it's threshold, then the\n-largest block from among those in the list and the new block will be\n-selected and given back to the OS. This is done because it reduces\n-external fragmentation, and allows the OS to use the larger blocks\n-later in an orderly fashion, possibly merging them later. Also, on\n-some systems, large blocks are obtained via calls to mmap, so giving\n-them back to free system resources becomes most important.\n-\n-The function _S_should_i_give decides the policy that determines\n-whether the current block of memory should be given to the allocator\n-for the request that it has made. That's because we may not always\n-have exact fits for the memory size that the allocator requests. We do\n-this mainly to prevent external fragmentation at the cost of a little\n-internal fragmentation. Now, the value of this internal fragmentation\n-has to be decided by this function. I can see 3 possibilities right\n-now. Please add more as and when you find better strategies.\n-\n-1. Equal size check. Return true only when the 2 blocks are of equal\n-   size.\n-\n-2. Difference Threshold: Return true only when the _block_size is\n-   greater than or equal to the _required_size, and if the _BS is >\n-   _RS by a difference of less than some THRESHOLD value, then return\n-   true, else return false.  \n-\n-3. Percentage Threshold. Return true only when the _block_size is\n-   greater than or equal to the _required_size, and if the _BS is >\n-   _RS by a percentage of less than some THRESHOLD value, then return\n-   true, else return false.\n-\n-Currently, (3) is being used with a value of 36% Maximum wastage per\n-Super Block.\n-\n---------------------------------------------------------------------\n-\n-1) What is a super block? Why is it needed?\n-\n-   A super block is the block of memory acquired from the FLS from\n-   which the bitmap allocator carves out memory for single objects and\n-   satisfies the user's requests. These super blocks come in sizes that\n-   are powers of 2 and multiples of 32 (_Bits_Per_Block). Yes both at\n-   the same time! That's because the next super block acquired will be\n-   2 times the previous one, and also all super blocks have to be\n-   multiples of the _Bits_Per_Block value.\n-\n-2) How does it interact with the free list store?\n-\n-   The super block is contained in the FLS, and the FLS is responsible\n-   for getting / returning Super Bocks to and from the OS using\n-   operator new as defined by the C++ standard.\n-\n----------------------------------------------------------------------\n-\n-How does the allocate function Work?\n-------------------------------------\n-\n-The allocate function is specialized for single object allocation\n-ONLY. Thus, ONLY if n == 1, will the bitmap_allocator's specialized\n-algorithm be used. Otherwise, the request is satisfied directly by\n-calling operator new.\n-\n-Suppose n == 1, then the allocator does the following:\n-\n-1. Checks to see whether the a free block exists somewhere in a region\n-   of memory close to the last satisfied request. If so, then that\n-   block is marked as allocated in the bit map and given to the\n-   user. If not, then (2) is executed.\n-\n-2. Is there a free block anywhere after the current block right upto\n-   the end of the memory that we have? If so, that block is found, and\n-   the same procedure is applied as above, and returned to the\n-   user. If not, then (3) is executed.\n-\n-3. Is there any block in whatever region of memory that we own free?\n-   This is done by checking (a) The use count for each super block,\n-   and if that fails then (b) The individual bit-maps for each super\n-   block. Note: Here we are never touching any of the memory that the\n-   user will be given, and we are confining all memory accesses to a\n-   small region of memory! This helps reduce cache misses. If this\n-   succeeds then we apply the same procedure on that bit-map as (1),\n-   and return that block of memory to the user. However, if this\n-   process fails, then we resort to (4).\n-\n-4. This process involves Refilling the internal exponentially growing\n-   memory pool. The said effect is achieved by calling _S_refill_pool\n-   which does the following:\n-\t (a). Gets more memory from the Global Free List of the\n-\t      Required size.\n-\t (b). Adjusts the size for the next call to itself.\n-\t (c). Writes the appropriate headers in the bit-maps.\n-\t (d). Sets the use count for that super-block just allocated\n-\t      to 0 (zero).\n-\t (e). All of the above accounts to maintaining the basic\n-\t      invariant for the allocator. If the invariant is\n-\t      maintained, we are sure that all is well.\n-   Now, the same process is applied on the newly acquired free blocks,\n-   which are dispatched accordingly.\n-\n-Thus, you can clearly see that the allocate function is nothing but a\n-combination of the next-fit and first-fit algorithm optimized ONLY for\n-single object allocations.\n-\n-\n--------------------------------------------------------------------------\n-\n-How does the deallocate function work?\n---------------------------------------\n-\n-The deallocate function again is specialized for single objects ONLY.\n-For all n belonging to > 1, the operator delete is called without\n-further ado, and the deallocate function returns.\n-\n-However for n == 1, a series of steps are performed:\n-\n-1. We first need to locate that super-block which holds the memory\n-   location given to us by the user. For that purpose, we maintain a\n-   static variable _S_last_dealloc_index, which holds the index into\n-   the vector of block pairs which indicates the index of the last\n-   super-block from which memory was freed. We use this strategy in\n-   the hope that the user will deallocate memory in a region close to\n-   what he/she deallocated the last time around. If the check for\n-   belongs_to succeeds, then we determine the bit-map for the given\n-   pointer, and locate the index into that bit-map, and mark that bit\n-   as free by setting it.\n-\n-2. If the _S_last_dealloc_index does not point to the memory block\n-   that we're looking for, then we do a linear search on the block\n-   stored in the vector of Block Pairs. This vector in code is called\n-   _S_mem_blocks. When the corresponding super-block is found, we\n-   apply the same procedure as we did for (1) to mark the block as\n-   free in the bit-map.\n-\n-Now, whenever a block is freed, the use count of that particular super\n-block goes down by 1. When this use count hits 0, we remove that super\n-block from the list of all valid super blocks stored in the\n-vector. While doing this, we also make sure that the basic invariant\n-is maintained by making sure that _S_last_request and\n-_S_last_dealloc_index point to valid locations within the vector.\n-\n---------------------------------------------------------------------\n-\n-\n-Data Layout for a Super Block:\n-==============================\n-\n-Each Super Block will be of some size that is a multiple of the number\n-of Bits Per Block. Typically, this value is chosen as Bits_Per_Byte X\n-sizeof(unsigned int). On an X86 system, this gives the figure \n-8 X 4 = 32. Thus, each Super Block will be of size 32 X Some_Value.\n-This Some_Value is sizeof(value_type). For now, let it be called 'K'.\n-Thus, finally, Super Block size is 32 X K bytes.\n-\n-This value of 32 has been chosen because each unsigned int has 32-bits\n-and Maximum use of these can be made with such a figure.\n-\n-Consider a block of size 32 ints.\n-In memory, it would look like this:\n-\n----------------------------------------------------------------------\n-|   136   |    0    | 4294967295 |      Data-> Space for 32-ints    |\n----------------------------------------------------------------------\n-\n-The first Columns represents the size of the Block in bytes as seen by\n-the Bitmap Allocator. Internally, a global free list is used to keep\n-track of the free blocks used and given back by the bitmap\n-allocator. It is this Free List Store that is responsible for writing\n-and managing this information. Actually the number of bytes allocated\n-in this case would be: 4 + 4 + 4 + 32*4 = 140 bytes, but the first 4\n-bytes are an addition by the Free List Store, so the Bitmap Allocator\n-sees only 136 bytes. These first 4 bytes about which the bitmapped\n-allocator is not aware hold the value 136.\n-\n-What do the remaining values represent?\n----------------------------------------\n-\n-The 2nd 4 in the expression is the sizeof(unsigned int) because the\n-Bitmapped Allocator maintains a used count for each Super Block, which\n-is initially set to 0 (as indicated in the diagram). This is\n-incremented every time a block is removed from this super block\n-(allocated), and decremented whenever it is given back. So, when the\n-used count falls to 0, the whole super block will be given back to the\n-Free List Store.\n-\n-The value 4294967295 represents the integer corresponding to the\n-bit representation of all bits set: 11111111111111111111111111111111.\n-\n-The 3rd 4 is size of the bitmap itself, which is the size of 32-bits,\n-which is 4-bytes, or 1 X sizeof(unsigned int).\n-\n-\n---------------------------------------------------------------------\n-\n-Another issue would be whether to keep the all bitmaps in a separate\n-area in memory, or to keep them near the actual blocks that will be\n-given out or allocated for the client. After some testing, I've\n-decided to keep these bitmaps close to the actual blocks. this will\n-help in 2 ways. \n-\n-1. Constant time access for the bitmap themselves, since no kind of\n-   look up will be needed to find the correct bitmap list or it's\n-   equivalent.\n-\n-2. And also this would preserve the cache as far as possible.\n-\n-So in effect, this kind of an allocator might prove beneficial from a\n-purely cache point of view. But this allocator has been made to try\n-and roll out the defects of the node_allocator, wherein the nodes get\n-skewed about in memory, if they are not returned in the exact reverse\n-order or in the same order in which they were allocated. Also, the\n-new_allocator's book keeping overhead is too much for small objects\n-and single object allocations, though it preserves the locality of\n-blocks very well when they are returned back to the allocator.\n-\n--------------------------------------------------------------------\n-\n-Expected overhead per block would be 1 bit in memory. Also, once\n-the address of the free list has been found, the cost for\n-allocation/deallocation would be negligible, and is supposed to be\n-constant time. For these very reasons, it is very important to\n-minimize the linear time costs, which include finding a free list\n-with a free block while allocating, and finding the corresponding\n-free list for a block while deallocating. Therefore, I have decided\n-that the growth of the internal pool for this allocator will be\n-exponential as compared to linear for node_allocator. There, linear\n-time works well, because we are mainly concerned with speed of\n-allocation/deallocation and memory consumption, whereas here, the\n-allocation/deallocation part does have some linear/logarithmic\n-complexity components in it. Thus, to try and minimize them would\n-be a good thing to do at the cost of a little bit of memory.\n-\n-Another thing to be noted is the the pool size will double every time\n-the internal pool gets exhausted, and all the free blocks have been\n-given away. The initial size of the pool would be sizeof(unsigned\n-int)*8 which is the number of bits in an integer, which can fit\n-exactly in a CPU register. Hence, the term given is exponential growth\n-of the internal pool.\n-\n----------------------------------------------------------------------\n-\n-After reading all this, you may still have a few questions about the\n-internal working of this allocator, like my friend had!\n-\n-Well here are the exact questions that he posed:\n-\n-1) The \"Data Layout\" section is cryptic. I have no idea of what you\n-   are trying to say. Layout of what? The free-list? Each bitmap? The\n-   Super Block?\n-\n-   The layout of a Super Block of a given size. In the example, a super\n-   block of size 32 X 1 is taken. The general formula for calculating\n-   the size of a super block is 32*sizeof(value_type)*2^n, where n\n-   ranges from 0 to 32 for 32-bit systems.\n-\n-2) And since I just mentioned the term `each bitmap', what in the\n-   world is meant by it? What does each bitmap manage? How does it\n-   relate to the super block? Is the Super Block a bitmap as well?\n-\n-   Good question! Each bitmap is part of a Super Block which is made up\n-   of 3 parts as I have mentioned earlier. Re-iterating, 1. The use\n-   count, 2. The bit-map for that Super Block. 3. The actual memory\n-   that will be eventually given to the user. Each bitmap is a multiple\n-   of 32 in size. If there are 32*(2^3) blocks of single objects to be\n-   given, there will be '32*(2^3)' bits present. Each 32 bits managing\n-   the allocated / free status for 32 blocks. Since each unsigned int\n-   contains 32-bits, one unsigned int can manage upto 32 blocks'\n-   status. Each bit-map is made up of a number of unsigned ints, whose\n-   exact number for a super-block of a given size I have just\n-   mentioned.\n-\n-3) How do the allocate and deallocate functions work in regard to\n-   bitmaps?\n-\n-   The allocate and deallocate functions manipulate the bitmaps and have\n-   nothing to do with the memory that is given to the user. As I have\n-   earlier mentioned, a 1 in the bitmap's bit field indicates free,\n-   while a 0 indicates allocated. This lets us check 32 bits at a time\n-   to check whether there is at lease one free block in those 32 blocks\n-   by testing for equality with (0). Now, the allocate function will\n-   given a memory block find the corresponding bit in the bitmap, and\n-   will reset it (ie. make it re-set (0)). And when the deallocate\n-   function is called, it will again set that bit after locating it to\n-   indicate that that particular block corresponding to this bit in the\n-   bit-map is not being used by anyone, and may be used to satisfy\n-   future requests.\n-\n-----------------------------------------------------------------------\n-\n-(Tech-Stuff, Please stay out if you are not interested in the\n-selection of certain constants. This has nothing to do with the\n-algorithm per-se, only with some vales that must be chosen correctly\n-to ensure that the allocator performs well in a real word scenario,\n-and maintains a good balance between the memory consumption and the\n-allocation/deallocation speed).\n-\n-The formula for calculating the maximum wastage as a percentage:\n-\n-(32 X k + 1) / (2 X (32 X k + 1 + 32 X c)) X 100.\n-\n-Where,\n-\tk => The constant overhead per node. eg. for list, it is 8\n-\tbytes, and for map it is 12 bytes.\n-\tc => The size of the base type on which the map/list is\n-\tinstantiated. Thus, suppose the the type1 is int and type2 is\n-\tdouble, they are related by the relation sizeof(double) ==\n-\t2*sizeof(int). Thus, all types must have this double size\n-\trelation for this formula to work properly.\n-\n-Plugging-in: For List: k = 8 and c = 4 (int and double), we get:\n-33.376%\n-\n-For map/multimap: k = 12, and c = 4 (int and double), we get:\n-37.524%\n-\n-Thus, knowing these values, and based on the sizeof(value_type), we\n-may create a function that returns the Max_Wastage_Percentage for us\n-to use.\n-\n-"}]}