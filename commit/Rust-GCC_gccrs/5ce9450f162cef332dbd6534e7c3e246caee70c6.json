{"sha": "5ce9450f162cef332dbd6534e7c3e246caee70c6", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NWNlOTQ1MGYxNjJjZWYzMzJkYmQ2NTM0ZTdjM2UyNDZjYWVlNzBjNg==", "commit": {"author": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2013-12-10T11:46:01Z"}, "committer": {"name": "Jakub Jelinek", "email": "jakub@gcc.gnu.org", "date": "2013-12-10T11:46:01Z"}, "message": "tree-vectorizer.h (struct _loop_vec_info): Add scalar_loop field.\n\n\t* tree-vectorizer.h (struct _loop_vec_info): Add scalar_loop field.\n\t(LOOP_VINFO_SCALAR_LOOP): Define.\n\t(slpeel_tree_duplicate_loop_to_edge_cfg): Add scalar_loop argument.\n\t* config/i386/sse.md (maskload<mode>, maskstore<mode>): New expanders.\n\t* tree-data-ref.c (get_references_in_stmt): Handle MASK_LOAD and\n\tMASK_STORE.\n\t* internal-fn.def (LOOP_VECTORIZED, MASK_LOAD, MASK_STORE): New\n\tinternal fns.\n\t* tree-if-conv.c: Include expr.h, optabs.h, tree-ssa-loop-ivopts.h and\n\ttree-ssa-address.h.\n\t(release_bb_predicate): New function.\n\t(free_bb_predicate): Use it.\n\t(reset_bb_predicate): Likewise.  Don't unallocate bb->aux\n\tjust to immediately allocate it again.\n\t(add_to_predicate_list): Add loop argument.  If basic blocks that\n\tdominate loop->latch don't insert any predicate.\n\t(add_to_dst_predicate_list): Adjust caller.\n\t(if_convertible_phi_p): Add any_mask_load_store argument, if true,\n\thandle it like flag_tree_loop_if_convert_stores.\n\t(insert_gimplified_predicates): Likewise.\n\t(ifcvt_can_use_mask_load_store): New function.\n\t(if_convertible_gimple_assign_stmt_p): Add any_mask_load_store\n\targument, check if some conditional loads or stores can't be\n\tconverted into MASK_LOAD or MASK_STORE.\n\t(if_convertible_stmt_p): Add any_mask_load_store argument,\n\tpass it down to if_convertible_gimple_assign_stmt_p.\n\t(predicate_bbs): Don't return bool, only check if the last stmt\n\tof a basic block is GIMPLE_COND and handle that.  Adjust\n\tadd_to_predicate_list caller.\n\t(if_convertible_loop_p_1): Only call predicate_bbs if\n\tflag_tree_loop_if_convert_stores and free_bb_predicate in that case\n\tafterwards, check gimple_code of stmts here.  Replace is_predicated\n\tcheck with dominance check.  Add any_mask_load_store argument,\n\tpass it down to if_convertible_stmt_p and if_convertible_phi_p,\n\tcall if_convertible_phi_p only after all if_convertible_stmt_p\n\tcalls.\n\t(if_convertible_loop_p): Add any_mask_load_store argument,\n\tpass it down to if_convertible_loop_p_1.\n\t(predicate_mem_writes): Emit MASK_LOAD and/or MASK_STORE calls.\n\t(combine_blocks): Add any_mask_load_store argument, pass\n\tit down to insert_gimplified_predicates and call predicate_mem_writes\n\tif it is set.  Call predicate_bbs.\n\t(version_loop_for_if_conversion): New function.\n\t(tree_if_conversion): Adjust if_convertible_loop_p and combine_blocks\n\tcalls.  Return todo flags instead of bool, call\n\tversion_loop_for_if_conversion if if-conversion should be just\n\tfor the vectorized loops and nothing else.\n\t(main_tree_if_conversion): Adjust caller.  Don't call\n\ttree_if_conversion for dont_vectorize loops if if-conversion\n\tisn't explicitly enabled.\n\t* tree-vect-data-refs.c (vect_check_gather): Handle\n\tMASK_LOAD/MASK_STORE.\n\t(vect_analyze_data_refs, vect_supportable_dr_alignment): Likewise.\n\t* gimple.h (gimple_expr_type): Handle MASK_STORE.\n\t* internal-fn.c (expand_LOOP_VECTORIZED, expand_MASK_LOAD,\n\texpand_MASK_STORE): New functions.\n\t* tree-vectorizer.c: Include tree-cfg.h and gimple-fold.h.\n\t(vect_loop_vectorized_call, fold_loop_vectorized_call): New functions.\n\t(vectorize_loops): Don't try to vectorize loops with\n\tloop->dont_vectorize set.  Set LOOP_VINFO_SCALAR_LOOP for if-converted\n\tloops, fold LOOP_VECTORIZED internal call depending on if loop\n\thas been vectorized or not.\n\t* tree-vect-loop-manip.c (slpeel_duplicate_current_defs_from_edges):\n\tNew function.\n\t(slpeel_tree_duplicate_loop_to_edge_cfg): Add scalar_loop argument.\n\tIf non-NULL, copy basic blocks from scalar_loop instead of loop, but\n\tstill to loop's entry or exit edge.\n\t(slpeel_tree_peel_loop_to_edge): Add scalar_loop argument, pass it\n\tdown to slpeel_tree_duplicate_loop_to_edge_cfg.\n\t(vect_do_peeling_for_loop_bound, vect_do_peeling_for_loop_alignment):\n\tAdjust callers.\n\t(vect_loop_versioning): If LOOP_VINFO_SCALAR_LOOP, perform loop\n\tversioning from that loop instead of LOOP_VINFO_LOOP, move it to the\n\tright place in the CFG afterwards.\n\t* tree-vect-loop.c (vect_determine_vectorization_factor): Handle\n\tMASK_STORE.\n\t* cfgloop.h (struct loop): Add dont_vectorize field.\n\t* tree-loop-distribution.c (copy_loop_before): Adjust\n\tslpeel_tree_duplicate_loop_to_edge_cfg caller.\n\t* optabs.def (maskload_optab, maskstore_optab): New optabs.\n\t* passes.def: Add a note that pass_vectorize must immediately follow\n\tpass_if_conversion.\n\t* tree-predcom.c (split_data_refs_to_components): Give up if\n\tDR_STMT is a call.\n\t* tree-vect-stmts.c (vect_mark_relevant): Don't crash if lhs\n\tis NULL.\n\t(exist_non_indexing_operands_for_use_p): Handle MASK_LOAD\n\tand MASK_STORE.\n\t(vectorizable_mask_load_store): New function.\n\t(vectorizable_call): Call it for MASK_LOAD or MASK_STORE.\n\t(vect_transform_stmt): Handle MASK_STORE.\n\t* tree-ssa-phiopt.c (cond_if_else_store_replacement): Ignore\n\tDR_STMT where lhs is NULL.\n\t* optabs.h (can_vec_perm_p): Fix up comment typo.\n\t(can_vec_mask_load_store_p): New prototype.\n\t* optabs.c (can_vec_mask_load_store_p): New function.\n\n\t* gcc.dg/vect/vect-cond-11.c: New test.\n\t* gcc.target/i386/vect-cond-1.c: New test.\n\t* gcc.target/i386/avx2-gather-5.c: New test.\n\t* gcc.target/i386/avx2-gather-6.c: New test.\n\t* gcc.dg/vect/vect-mask-loadstore-1.c: New test.\n\t* gcc.dg/vect/vect-mask-load-1.c: New test.\n\nFrom-SVN: r205856", "tree": {"sha": "638f17e2a1b480505958dcbd6cca009ca6ade040", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/638f17e2a1b480505958dcbd6cca009ca6ade040"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/5ce9450f162cef332dbd6534e7c3e246caee70c6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5ce9450f162cef332dbd6534e7c3e246caee70c6", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5ce9450f162cef332dbd6534e7c3e246caee70c6", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5ce9450f162cef332dbd6534e7c3e246caee70c6/comments", "author": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "ae3df2dc0a108b120304fe7725f58efc316fdd9a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ae3df2dc0a108b120304fe7725f58efc316fdd9a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ae3df2dc0a108b120304fe7725f58efc316fdd9a"}], "stats": {"total": 1578, "additions": 1391, "deletions": 187}, "files": [{"sha": "b2604b652c7dd78e7782b0366323dcc9b4bb3d62", "filename": "gcc/ChangeLog", "status": "modified", "additions": 99, "deletions": 0, "changes": 99, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -1,3 +1,102 @@\n+2013-12-10  Jakub Jelinek  <jakub@redhat.com>\n+\n+\t* tree-vectorizer.h (struct _loop_vec_info): Add scalar_loop field.\n+\t(LOOP_VINFO_SCALAR_LOOP): Define.\n+\t(slpeel_tree_duplicate_loop_to_edge_cfg): Add scalar_loop argument.\n+\t* config/i386/sse.md (maskload<mode>, maskstore<mode>): New expanders.\n+\t* tree-data-ref.c (get_references_in_stmt): Handle MASK_LOAD and\n+\tMASK_STORE.\n+\t* internal-fn.def (LOOP_VECTORIZED, MASK_LOAD, MASK_STORE): New\n+\tinternal fns.\n+\t* tree-if-conv.c: Include expr.h, optabs.h, tree-ssa-loop-ivopts.h and\n+\ttree-ssa-address.h.\n+\t(release_bb_predicate): New function.\n+\t(free_bb_predicate): Use it.\n+\t(reset_bb_predicate): Likewise.  Don't unallocate bb->aux\n+\tjust to immediately allocate it again.\n+\t(add_to_predicate_list): Add loop argument.  If basic blocks that\n+\tdominate loop->latch don't insert any predicate.\n+\t(add_to_dst_predicate_list): Adjust caller.\n+\t(if_convertible_phi_p): Add any_mask_load_store argument, if true,\n+\thandle it like flag_tree_loop_if_convert_stores.\n+\t(insert_gimplified_predicates): Likewise.\n+\t(ifcvt_can_use_mask_load_store): New function.\n+\t(if_convertible_gimple_assign_stmt_p): Add any_mask_load_store\n+\targument, check if some conditional loads or stores can't be\n+\tconverted into MASK_LOAD or MASK_STORE.\n+\t(if_convertible_stmt_p): Add any_mask_load_store argument,\n+\tpass it down to if_convertible_gimple_assign_stmt_p.\n+\t(predicate_bbs): Don't return bool, only check if the last stmt\n+\tof a basic block is GIMPLE_COND and handle that.  Adjust\n+\tadd_to_predicate_list caller.\n+\t(if_convertible_loop_p_1): Only call predicate_bbs if\n+\tflag_tree_loop_if_convert_stores and free_bb_predicate in that case\n+\tafterwards, check gimple_code of stmts here.  Replace is_predicated\n+\tcheck with dominance check.  Add any_mask_load_store argument,\n+\tpass it down to if_convertible_stmt_p and if_convertible_phi_p,\n+\tcall if_convertible_phi_p only after all if_convertible_stmt_p\n+\tcalls.\n+\t(if_convertible_loop_p): Add any_mask_load_store argument,\n+\tpass it down to if_convertible_loop_p_1.\n+\t(predicate_mem_writes): Emit MASK_LOAD and/or MASK_STORE calls.\n+\t(combine_blocks): Add any_mask_load_store argument, pass\n+\tit down to insert_gimplified_predicates and call predicate_mem_writes\n+\tif it is set.  Call predicate_bbs.\n+\t(version_loop_for_if_conversion): New function.\n+\t(tree_if_conversion): Adjust if_convertible_loop_p and combine_blocks\n+\tcalls.  Return todo flags instead of bool, call\n+\tversion_loop_for_if_conversion if if-conversion should be just\n+\tfor the vectorized loops and nothing else.\n+\t(main_tree_if_conversion): Adjust caller.  Don't call\n+\ttree_if_conversion for dont_vectorize loops if if-conversion\n+\tisn't explicitly enabled.\n+\t* tree-vect-data-refs.c (vect_check_gather): Handle\n+\tMASK_LOAD/MASK_STORE.\n+\t(vect_analyze_data_refs, vect_supportable_dr_alignment): Likewise.\n+\t* gimple.h (gimple_expr_type): Handle MASK_STORE.\n+\t* internal-fn.c (expand_LOOP_VECTORIZED, expand_MASK_LOAD,\n+\texpand_MASK_STORE): New functions.\n+\t* tree-vectorizer.c: Include tree-cfg.h and gimple-fold.h.\n+\t(vect_loop_vectorized_call, fold_loop_vectorized_call): New functions.\n+\t(vectorize_loops): Don't try to vectorize loops with\n+\tloop->dont_vectorize set.  Set LOOP_VINFO_SCALAR_LOOP for if-converted\n+\tloops, fold LOOP_VECTORIZED internal call depending on if loop\n+\thas been vectorized or not.\n+\t* tree-vect-loop-manip.c (slpeel_duplicate_current_defs_from_edges):\n+\tNew function.\n+\t(slpeel_tree_duplicate_loop_to_edge_cfg): Add scalar_loop argument.\n+\tIf non-NULL, copy basic blocks from scalar_loop instead of loop, but\n+\tstill to loop's entry or exit edge.\n+\t(slpeel_tree_peel_loop_to_edge): Add scalar_loop argument, pass it\n+\tdown to slpeel_tree_duplicate_loop_to_edge_cfg.\n+\t(vect_do_peeling_for_loop_bound, vect_do_peeling_for_loop_alignment):\n+\tAdjust callers.\n+\t(vect_loop_versioning): If LOOP_VINFO_SCALAR_LOOP, perform loop\n+\tversioning from that loop instead of LOOP_VINFO_LOOP, move it to the\n+\tright place in the CFG afterwards.\n+\t* tree-vect-loop.c (vect_determine_vectorization_factor): Handle\n+\tMASK_STORE.\n+\t* cfgloop.h (struct loop): Add dont_vectorize field.\n+\t* tree-loop-distribution.c (copy_loop_before): Adjust\n+\tslpeel_tree_duplicate_loop_to_edge_cfg caller.\n+\t* optabs.def (maskload_optab, maskstore_optab): New optabs.\n+\t* passes.def: Add a note that pass_vectorize must immediately follow\n+\tpass_if_conversion.\n+\t* tree-predcom.c (split_data_refs_to_components): Give up if\n+\tDR_STMT is a call.\n+\t* tree-vect-stmts.c (vect_mark_relevant): Don't crash if lhs\n+\tis NULL.\n+\t(exist_non_indexing_operands_for_use_p): Handle MASK_LOAD\n+\tand MASK_STORE.\n+\t(vectorizable_mask_load_store): New function.\n+\t(vectorizable_call): Call it for MASK_LOAD or MASK_STORE.\n+\t(vect_transform_stmt): Handle MASK_STORE.\n+\t* tree-ssa-phiopt.c (cond_if_else_store_replacement): Ignore\n+\tDR_STMT where lhs is NULL.\n+\t* optabs.h (can_vec_perm_p): Fix up comment typo.\n+\t(can_vec_mask_load_store_p): New prototype.\n+\t* optabs.c (can_vec_mask_load_store_p): New function.\n+\n 2013-12-10  Eric Botcazou  <ebotcazou@adacore.com>\n \n \t* expr.c (expand_expr_real_1) <normal_inner_ref>: Always return 0 for"}, {"sha": "5dbefaf119401aa1e0fb6a569df2a1389ecfcef9", "filename": "gcc/cfgloop.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Fcfgloop.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Fcfgloop.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcfgloop.h?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -176,6 +176,9 @@ struct GTY ((chain_next (\"%h.next\"))) loop {\n   /* True if we should try harder to vectorize this loop.  */\n   bool force_vect;\n \n+  /* True if this loop should never be vectorized.  */\n+  bool dont_vectorize;\n+\n   /* For SIMD loops, this is a unique identifier of the loop, referenced\n      by IFN_GOMP_SIMD_VF, IFN_GOMP_SIMD_LANE and IFN_GOMP_SIMD_LAST_LANE\n      builtins.  */"}, {"sha": "30895c67c0929e1149dff30444c2c8b2993ef566", "filename": "gcc/config/i386/sse.md", "status": "modified", "additions": 17, "deletions": 0, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Fconfig%2Fi386%2Fsse.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Fconfig%2Fi386%2Fsse.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fsse.md?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -14253,6 +14253,23 @@\n    (set_attr \"btver2_decode\" \"vector\") \n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n+(define_expand \"maskload<mode>\"\n+  [(set (match_operand:V48_AVX2 0 \"register_operand\")\n+\t(unspec:V48_AVX2\n+\t  [(match_operand:<sseintvecmode> 2 \"register_operand\")\n+\t   (match_operand:V48_AVX2 1 \"memory_operand\")]\n+\t  UNSPEC_MASKMOV))]\n+  \"TARGET_AVX\")\n+\n+(define_expand \"maskstore<mode>\"\n+  [(set (match_operand:V48_AVX2 0 \"memory_operand\")\n+\t(unspec:V48_AVX2\n+\t  [(match_operand:<sseintvecmode> 2 \"register_operand\")\n+\t   (match_operand:V48_AVX2 1 \"register_operand\")\n+\t   (match_dup 0)]\n+\t  UNSPEC_MASKMOV))]\n+  \"TARGET_AVX\")\n+\n (define_insn_and_split \"avx_<castmode><avxsizesuffix>_<castmode>\"\n   [(set (match_operand:AVX256MODE2P 0 \"nonimmediate_operand\" \"=x,m\")\n \t(unspec:AVX256MODE2P"}, {"sha": "a49016fcc6c71f9d54e41642922714b434996abb", "filename": "gcc/gimple.h", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Fgimple.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Fgimple.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgimple.h?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -5624,7 +5624,13 @@ gimple_expr_type (const_gimple stmt)\n \t useless conversion involved.  That means returning the\n \t original RHS type as far as we can reconstruct it.  */\n       if (code == GIMPLE_CALL)\n-\ttype = gimple_call_return_type (stmt);\n+\t{\n+\t  if (gimple_call_internal_p (stmt)\n+\t      && gimple_call_internal_fn (stmt) == IFN_MASK_STORE)\n+\t    type = TREE_TYPE (gimple_call_arg (stmt, 3));\n+\t  else\n+\t    type = gimple_call_return_type (stmt);\n+\t}\n       else\n \tswitch (gimple_assign_rhs_code (stmt))\n \t  {"}, {"sha": "ad9c9475265521db35fc658177f1a2850efc0c86", "filename": "gcc/internal-fn.c", "status": "modified", "additions": 54, "deletions": 0, "changes": 54, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Finternal-fn.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Finternal-fn.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Finternal-fn.c?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -461,6 +461,60 @@ expand_UBSAN_CHECK_MUL (gimple stmt)\n   ubsan_expand_si_overflow_mul_check (stmt);\n }\n \n+/* This should get folded in tree-vectorizer.c.  */\n+\n+static void\n+expand_LOOP_VECTORIZED (gimple stmt ATTRIBUTE_UNUSED)\n+{\n+  gcc_unreachable ();\n+}\n+\n+static void\n+expand_MASK_LOAD (gimple stmt)\n+{\n+  struct expand_operand ops[3];\n+  tree type, lhs, rhs, maskt;\n+  rtx mem, target, mask;\n+\n+  maskt = gimple_call_arg (stmt, 2);\n+  lhs = gimple_call_lhs (stmt);\n+  type = TREE_TYPE (lhs);\n+  rhs = fold_build2 (MEM_REF, type, gimple_call_arg (stmt, 0),\n+\t\t     gimple_call_arg (stmt, 1));\n+\n+  mem = expand_expr (rhs, NULL_RTX, VOIDmode, EXPAND_WRITE);\n+  gcc_assert (MEM_P (mem));\n+  mask = expand_normal (maskt);\n+  target = expand_expr (lhs, NULL_RTX, VOIDmode, EXPAND_WRITE);\n+  create_output_operand (&ops[0], target, TYPE_MODE (type));\n+  create_fixed_operand (&ops[1], mem);\n+  create_input_operand (&ops[2], mask, TYPE_MODE (TREE_TYPE (maskt)));\n+  expand_insn (optab_handler (maskload_optab, TYPE_MODE (type)), 3, ops);\n+}\n+\n+static void\n+expand_MASK_STORE (gimple stmt)\n+{\n+  struct expand_operand ops[3];\n+  tree type, lhs, rhs, maskt;\n+  rtx mem, reg, mask;\n+\n+  maskt = gimple_call_arg (stmt, 2);\n+  rhs = gimple_call_arg (stmt, 3);\n+  type = TREE_TYPE (rhs);\n+  lhs = fold_build2 (MEM_REF, type, gimple_call_arg (stmt, 0),\n+\t\t     gimple_call_arg (stmt, 1));\n+\n+  mem = expand_expr (lhs, NULL_RTX, VOIDmode, EXPAND_WRITE);\n+  gcc_assert (MEM_P (mem));\n+  mask = expand_normal (maskt);\n+  reg = expand_normal (rhs);\n+  create_fixed_operand (&ops[0], mem);\n+  create_input_operand (&ops[1], reg, TYPE_MODE (type));\n+  create_input_operand (&ops[2], mask, TYPE_MODE (TREE_TYPE (maskt)));\n+  expand_insn (optab_handler (maskstore_optab, TYPE_MODE (type)), 3, ops);\n+}\n+\n /* Routines to expand each internal function, indexed by function number.\n    Each routine has the prototype:\n "}, {"sha": "fdb1812e430967c24f6370c446929814cd9beb03", "filename": "gcc/internal-fn.def", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Finternal-fn.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Finternal-fn.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Finternal-fn.def?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -43,6 +43,9 @@ DEF_INTERNAL_FN (STORE_LANES, ECF_CONST | ECF_LEAF)\n DEF_INTERNAL_FN (GOMP_SIMD_LANE, ECF_NOVOPS | ECF_LEAF | ECF_NOTHROW)\n DEF_INTERNAL_FN (GOMP_SIMD_VF, ECF_CONST | ECF_LEAF | ECF_NOTHROW)\n DEF_INTERNAL_FN (GOMP_SIMD_LAST_LANE, ECF_CONST | ECF_LEAF | ECF_NOTHROW)\n+DEF_INTERNAL_FN (LOOP_VECTORIZED, ECF_NOVOPS | ECF_LEAF | ECF_NOTHROW)\n+DEF_INTERNAL_FN (MASK_LOAD, ECF_PURE | ECF_LEAF)\n+DEF_INTERNAL_FN (MASK_STORE, ECF_LEAF)\n DEF_INTERNAL_FN (ANNOTATE,  ECF_CONST | ECF_LEAF | ECF_NOTHROW)\n DEF_INTERNAL_FN (UBSAN_NULL, ECF_LEAF | ECF_NOTHROW)\n DEF_INTERNAL_FN (UBSAN_CHECK_ADD, ECF_CONST | ECF_LEAF | ECF_NOTHROW)"}, {"sha": "e034b751aa4d56ec59d4a549ad35bcc90628000a", "filename": "gcc/optabs.c", "status": "modified", "additions": 39, "deletions": 0, "changes": 39, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Foptabs.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Foptabs.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Foptabs.c?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -6916,6 +6916,45 @@ expand_mult_highpart (enum machine_mode mode, rtx op0, rtx op1,\n \n   return expand_vec_perm (mode, m1, m2, perm, target);\n }\n+\n+/* Return true if target supports vector masked load/store for mode.  */\n+bool\n+can_vec_mask_load_store_p (enum machine_mode mode, bool is_load)\n+{\n+  optab op = is_load ? maskload_optab : maskstore_optab;\n+  enum machine_mode vmode;\n+  unsigned int vector_sizes;\n+\n+  /* If mode is vector mode, check it directly.  */\n+  if (VECTOR_MODE_P (mode))\n+    return optab_handler (op, mode) != CODE_FOR_nothing;\n+\n+  /* Otherwise, return true if there is some vector mode with\n+     the mask load/store supported.  */\n+\n+  /* See if there is any chance the mask load or store might be\n+     vectorized.  If not, punt.  */\n+  vmode = targetm.vectorize.preferred_simd_mode (mode);\n+  if (!VECTOR_MODE_P (vmode))\n+    return false;\n+\n+  if (optab_handler (op, vmode) != CODE_FOR_nothing)\n+    return true;\n+\n+  vector_sizes = targetm.vectorize.autovectorize_vector_sizes ();\n+  while (vector_sizes != 0)\n+    {\n+      unsigned int cur = 1 << floor_log2 (vector_sizes);\n+      vector_sizes &= ~cur;\n+      if (cur <= GET_MODE_SIZE (mode))\n+\tcontinue;\n+      vmode = mode_for_vector (mode, cur / GET_MODE_SIZE (mode));\n+      if (VECTOR_MODE_P (vmode)\n+\t  && optab_handler (op, vmode) != CODE_FOR_nothing)\n+\treturn true;\n+    }\n+  return false;\n+}\n \f\n /* Return true if there is a compare_and_swap pattern.  */\n "}, {"sha": "f19ceba474627df955e46006b753f290830e8d3a", "filename": "gcc/optabs.def", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Foptabs.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Foptabs.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Foptabs.def?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -252,6 +252,8 @@ OPTAB_D (sdot_prod_optab, \"sdot_prod$I$a\")\n OPTAB_D (ssum_widen_optab, \"widen_ssum$I$a3\")\n OPTAB_D (udot_prod_optab, \"udot_prod$I$a\")\n OPTAB_D (usum_widen_optab, \"widen_usum$I$a3\")\n+OPTAB_D (maskload_optab, \"maskload$a\")\n+OPTAB_D (maskstore_optab, \"maskstore$a\")\n OPTAB_D (vec_extract_optab, \"vec_extract$a\")\n OPTAB_D (vec_init_optab, \"vec_init$a\")\n OPTAB_D (vec_pack_sfix_trunc_optab, \"vec_pack_sfix_trunc_$a\")"}, {"sha": "3c40b4a0e94127548dfa35ada0a27eecff156f2b", "filename": "gcc/optabs.h", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Foptabs.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Foptabs.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Foptabs.h?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -236,7 +236,7 @@ extern rtx expand_vec_cond_expr (tree, tree, tree, tree, rtx);\n /* Generate code for VEC_LSHIFT_EXPR and VEC_RSHIFT_EXPR.  */\n extern rtx expand_vec_shift_expr (sepops, rtx);\n \n-/* Return tree if target supports vector operations for VEC_PERM_EXPR.  */\n+/* Return true if target supports vector operations for VEC_PERM_EXPR.  */\n extern bool can_vec_perm_p (enum machine_mode, bool, const unsigned char *);\n \n /* Generate code for VEC_PERM_EXPR.  */\n@@ -248,6 +248,9 @@ extern int can_mult_highpart_p (enum machine_mode, bool);\n /* Generate code for MULT_HIGHPART_EXPR.  */\n extern rtx expand_mult_highpart (enum machine_mode, rtx, rtx, rtx, bool);\n \n+/* Return true if target supports vector masked load/store for mode.  */\n+extern bool can_vec_mask_load_store_p (enum machine_mode, bool);\n+\n /* Return the insn used to implement mode MODE of OP, or CODE_FOR_nothing\n    if the target does not have such an insn.  */\n "}, {"sha": "1fe2003cbca9c5f9dc01572441be91b3b3a47e2c", "filename": "gcc/passes.def", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Fpasses.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Fpasses.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fpasses.def?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -217,6 +217,8 @@ along with GCC; see the file COPYING3.  If not see\n \t  NEXT_PASS (pass_iv_canon);\n \t  NEXT_PASS (pass_parallelize_loops);\n \t  NEXT_PASS (pass_if_conversion);\n+\t  /* pass_vectorize must immediately follow pass_if_conversion.\n+\t     Please do not add any other passes in between.  */\n \t  NEXT_PASS (pass_vectorize);\n           PUSH_INSERT_PASSES_WITHIN (pass_vectorize)\n \t      NEXT_PASS (pass_dce_loop);"}, {"sha": "d91e279a180333c6f7340bf8dabd403cc74c2df7", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -1,3 +1,12 @@\n+2013-12-10  Jakub Jelinek  <jakub@redhat.com>\n+\n+\t* gcc.dg/vect/vect-cond-11.c: New test.\n+\t* gcc.target/i386/vect-cond-1.c: New test.\n+\t* gcc.target/i386/avx2-gather-5.c: New test.\n+\t* gcc.target/i386/avx2-gather-6.c: New test.\n+\t* gcc.dg/vect/vect-mask-loadstore-1.c: New test.\n+\t* gcc.dg/vect/vect-mask-load-1.c: New test.\n+\n 2013-12-09  Marek Polacek  <polacek@redhat.com>\n \n \tPR sanitizer/59437"}, {"sha": "94c7917344a591a66325a1b503d990c34d164d03", "filename": "gcc/tree-data-ref.c", "status": "modified", "additions": 38, "deletions": 9, "changes": 47, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-data-ref.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-data-ref.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-data-ref.c?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -4346,16 +4346,26 @@ get_references_in_stmt (gimple stmt, vec<data_ref_loc, va_heap> *references)\n       && !(gimple_call_flags (stmt) & ECF_CONST))\n     {\n       /* Allow IFN_GOMP_SIMD_LANE in their own loops.  */\n-      if (gimple_call_internal_p (stmt)\n-\t  && gimple_call_internal_fn (stmt) == IFN_GOMP_SIMD_LANE)\n-\t{\n-\t  struct loop *loop = gimple_bb (stmt)->loop_father;\n-\t  tree uid = gimple_call_arg (stmt, 0);\n-\t  gcc_assert (TREE_CODE (uid) == SSA_NAME);\n-\t  if (loop == NULL\n-\t      || loop->simduid != SSA_NAME_VAR (uid))\n+      if (gimple_call_internal_p (stmt))\n+\tswitch (gimple_call_internal_fn (stmt))\n+\t  {\n+\t  case IFN_GOMP_SIMD_LANE:\n+\t    {\n+\t      struct loop *loop = gimple_bb (stmt)->loop_father;\n+\t      tree uid = gimple_call_arg (stmt, 0);\n+\t      gcc_assert (TREE_CODE (uid) == SSA_NAME);\n+\t      if (loop == NULL\n+\t\t  || loop->simduid != SSA_NAME_VAR (uid))\n+\t\tclobbers_memory = true;\n+\t      break;\n+\t    }\n+\t  case IFN_MASK_LOAD:\n+\t  case IFN_MASK_STORE:\n+\t    break;\n+\t  default:\n \t    clobbers_memory = true;\n-\t}\n+\t    break;\n+\t  }\n       else\n \tclobbers_memory = true;\n     }\n@@ -4386,6 +4396,25 @@ get_references_in_stmt (gimple stmt, vec<data_ref_loc, va_heap> *references)\n     {\n       unsigned i, n;\n \n+      ref.is_read = false;\n+      if (gimple_call_internal_p (stmt))\n+\tswitch (gimple_call_internal_fn (stmt))\n+\t  {\n+\t  case IFN_MASK_LOAD:\n+\t    ref.is_read = true;\n+\t  case IFN_MASK_STORE:\n+\t    ref.ref = fold_build2 (MEM_REF,\n+\t\t\t\t   ref.is_read\n+\t\t\t\t   ? TREE_TYPE (gimple_call_lhs (stmt))\n+\t\t\t\t   : TREE_TYPE (gimple_call_arg (stmt, 3)),\n+\t\t\t\t   gimple_call_arg (stmt, 0),\n+\t\t\t\t   gimple_call_arg (stmt, 1));\n+\t    references->safe_push (ref);\n+\t    return false;\n+\t  default:\n+\t    break;\n+\t  }\n+\n       op0 = gimple_call_lhs (stmt);\n       n = gimple_call_num_args (stmt);\n       for (i = 0; i < n; i++)"}, {"sha": "59404ec14e2f43a13f190c250acb1c6376b91de9", "filename": "gcc/tree-if-conv.c", "status": "modified", "additions": 303, "deletions": 113, "changes": 416, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-if-conv.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-if-conv.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-if-conv.c?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -110,8 +110,12 @@ along with GCC; see the file COPYING3.  If not see\n #include \"tree-chrec.h\"\n #include \"tree-data-ref.h\"\n #include \"tree-scalar-evolution.h\"\n+#include \"tree-ssa-loop-ivopts.h\"\n+#include \"tree-ssa-address.h\"\n #include \"tree-pass.h\"\n #include \"dbgcnt.h\"\n+#include \"expr.h\"\n+#include \"optabs.h\"\n \n /* List of basic blocks in if-conversion-suitable order.  */\n static basic_block *ifc_bbs;\n@@ -194,39 +198,48 @@ init_bb_predicate (basic_block bb)\n   set_bb_predicate (bb, boolean_true_node);\n }\n \n-/* Free the predicate of basic block BB.  */\n+/* Release the SSA_NAMEs associated with the predicate of basic block BB,\n+   but don't actually free it.  */\n \n static inline void\n-free_bb_predicate (basic_block bb)\n+release_bb_predicate (basic_block bb)\n {\n-  gimple_seq stmts;\n-\n-  if (!bb_has_predicate (bb))\n-    return;\n-\n-  /* Release the SSA_NAMEs created for the gimplification of the\n-     predicate.  */\n-  stmts = bb_predicate_gimplified_stmts (bb);\n+  gimple_seq stmts = bb_predicate_gimplified_stmts (bb);\n   if (stmts)\n     {\n       gimple_stmt_iterator i;\n \n       for (i = gsi_start (stmts); !gsi_end_p (i); gsi_next (&i))\n \tfree_stmt_operands (cfun, gsi_stmt (i));\n+      set_bb_predicate_gimplified_stmts (bb, NULL);\n     }\n+}\n+\n+/* Free the predicate of basic block BB.  */\n \n+static inline void\n+free_bb_predicate (basic_block bb)\n+{\n+  if (!bb_has_predicate (bb))\n+    return;\n+\n+  release_bb_predicate (bb);\n   free (bb->aux);\n   bb->aux = NULL;\n }\n \n-/* Free the predicate of BB and reinitialize it with the true\n-   predicate.  */\n+/* Reinitialize predicate of BB with the true predicate.  */\n \n static inline void\n reset_bb_predicate (basic_block bb)\n {\n-  free_bb_predicate (bb);\n-  init_bb_predicate (bb);\n+  if (!bb_has_predicate (bb))\n+    init_bb_predicate (bb);\n+  else\n+    {\n+      release_bb_predicate (bb);\n+      set_bb_predicate (bb, boolean_true_node);\n+    }\n }\n \n /* Returns a new SSA_NAME of type TYPE that is assigned the value of\n@@ -382,18 +395,26 @@ fold_build_cond_expr (tree type, tree cond, tree rhs, tree lhs)\n   return build3 (COND_EXPR, type, cond, rhs, lhs);\n }\n \n-/* Add condition NC to the predicate list of basic block BB.  */\n+/* Add condition NC to the predicate list of basic block BB.  LOOP is\n+   the loop to be if-converted.  */\n \n static inline void\n-add_to_predicate_list (basic_block bb, tree nc)\n+add_to_predicate_list (struct loop *loop, basic_block bb, tree nc)\n {\n   tree bc, *tp;\n \n   if (is_true_predicate (nc))\n     return;\n \n   if (!is_predicated (bb))\n-    bc = nc;\n+    {\n+      /* If dominance tells us this basic block is always executed, don't\n+\t record any predicates for it.  */\n+      if (dominated_by_p (CDI_DOMINATORS, loop->latch, bb))\n+\treturn;\n+\n+      bc = nc;\n+    }\n   else\n     {\n       bc = bb_predicate (bb);\n@@ -434,7 +455,7 @@ add_to_dst_predicate_list (struct loop *loop, edge e,\n     cond = fold_build2 (TRUTH_AND_EXPR, boolean_type_node,\n \t\t\tprev_cond, cond);\n \n-  add_to_predicate_list (e->dest, cond);\n+  add_to_predicate_list (loop, e->dest, cond);\n }\n \n /* Return true if one of the successor edges of BB exits LOOP.  */\n@@ -464,7 +485,8 @@ bb_with_exit_edge_p (struct loop *loop, basic_block bb)\n    - there is a virtual PHI in a BB other than the loop->header.  */\n \n static bool\n-if_convertible_phi_p (struct loop *loop, basic_block bb, gimple phi)\n+if_convertible_phi_p (struct loop *loop, basic_block bb, gimple phi,\n+\t\t      bool any_mask_load_store)\n {\n   if (dump_file && (dump_flags & TDF_DETAILS))\n     {\n@@ -479,7 +501,7 @@ if_convertible_phi_p (struct loop *loop, basic_block bb, gimple phi)\n       return false;\n     }\n \n-  if (flag_tree_loop_if_convert_stores)\n+  if (flag_tree_loop_if_convert_stores || any_mask_load_store)\n     return true;\n \n   /* When the flag_tree_loop_if_convert_stores is not set, check\n@@ -695,6 +717,56 @@ ifcvt_could_trap_p (gimple stmt, vec<data_reference_p> refs)\n   return gimple_could_trap_p (stmt);\n }\n \n+/* Return true if STMT could be converted into a masked load or store\n+   (conditional load or store based on a mask computed from bb predicate).  */\n+\n+static bool\n+ifcvt_can_use_mask_load_store (gimple stmt)\n+{\n+  tree lhs, ref;\n+  enum machine_mode mode;\n+  basic_block bb = gimple_bb (stmt);\n+  bool is_load;\n+\n+  if (!(flag_tree_loop_vectorize || bb->loop_father->force_vect)\n+      || bb->loop_father->dont_vectorize\n+      || !gimple_assign_single_p (stmt)\n+      || gimple_has_volatile_ops (stmt))\n+    return false;\n+\n+  /* Check whether this is a load or store.  */\n+  lhs = gimple_assign_lhs (stmt);\n+  if (gimple_store_p (stmt))\n+    {\n+      if (!is_gimple_val (gimple_assign_rhs1 (stmt)))\n+\treturn false;\n+      is_load = false;\n+      ref = lhs;\n+    }\n+  else if (gimple_assign_load_p (stmt))\n+    {\n+      is_load = true;\n+      ref = gimple_assign_rhs1 (stmt);\n+    }\n+  else\n+    return false;\n+\n+  if (may_be_nonaddressable_p (ref))\n+    return false;\n+\n+  /* Mask should be integer mode of the same size as the load/store\n+     mode.  */\n+  mode = TYPE_MODE (TREE_TYPE (lhs));\n+  if (int_mode_for_mode (mode) == BLKmode\n+      || VECTOR_MODE_P (mode))\n+    return false;\n+\n+  if (can_vec_mask_load_store_p (mode, is_load))\n+    return true;\n+\n+  return false;\n+}\n+\n /* Return true when STMT is if-convertible.\n \n    GIMPLE_ASSIGN statement is not if-convertible if,\n@@ -704,7 +776,8 @@ ifcvt_could_trap_p (gimple stmt, vec<data_reference_p> refs)\n \n static bool\n if_convertible_gimple_assign_stmt_p (gimple stmt,\n-\t\t\t\t     vec<data_reference_p> refs)\n+\t\t\t\t     vec<data_reference_p> refs,\n+\t\t\t\t     bool *any_mask_load_store)\n {\n   tree lhs = gimple_assign_lhs (stmt);\n   basic_block bb;\n@@ -730,10 +803,21 @@ if_convertible_gimple_assign_stmt_p (gimple stmt,\n       return false;\n     }\n \n+  /* tree-into-ssa.c uses GF_PLF_1, so avoid it, because\n+     in between if_convertible_loop_p and combine_blocks\n+     we can perform loop versioning.  */\n+  gimple_set_plf (stmt, GF_PLF_2, false);\n+\n   if (flag_tree_loop_if_convert_stores)\n     {\n       if (ifcvt_could_trap_p (stmt, refs))\n \t{\n+\t  if (ifcvt_can_use_mask_load_store (stmt))\n+\t    {\n+\t      gimple_set_plf (stmt, GF_PLF_2, true);\n+\t      *any_mask_load_store = true;\n+\t      return true;\n+\t    }\n \t  if (dump_file && (dump_flags & TDF_DETAILS))\n \t    fprintf (dump_file, \"tree could trap...\\n\");\n \t  return false;\n@@ -743,6 +827,12 @@ if_convertible_gimple_assign_stmt_p (gimple stmt,\n \n   if (gimple_assign_rhs_could_trap_p (stmt))\n     {\n+      if (ifcvt_can_use_mask_load_store (stmt))\n+\t{\n+\t  gimple_set_plf (stmt, GF_PLF_2, true);\n+\t  *any_mask_load_store = true;\n+\t  return true;\n+\t}\n       if (dump_file && (dump_flags & TDF_DETAILS))\n \tfprintf (dump_file, \"tree could trap...\\n\");\n       return false;\n@@ -754,6 +844,12 @@ if_convertible_gimple_assign_stmt_p (gimple stmt,\n       && bb != bb->loop_father->header\n       && !bb_with_exit_edge_p (bb->loop_father, bb))\n     {\n+      if (ifcvt_can_use_mask_load_store (stmt))\n+\t{\n+\t  gimple_set_plf (stmt, GF_PLF_2, true);\n+\t  *any_mask_load_store = true;\n+\t  return true;\n+\t}\n       if (dump_file && (dump_flags & TDF_DETAILS))\n \t{\n \t  fprintf (dump_file, \"LHS is not var\\n\");\n@@ -772,7 +868,8 @@ if_convertible_gimple_assign_stmt_p (gimple stmt,\n    - it is a GIMPLE_LABEL or a GIMPLE_COND.  */\n \n static bool\n-if_convertible_stmt_p (gimple stmt, vec<data_reference_p> refs)\n+if_convertible_stmt_p (gimple stmt, vec<data_reference_p> refs,\n+\t\t       bool *any_mask_load_store)\n {\n   switch (gimple_code (stmt))\n     {\n@@ -782,7 +879,8 @@ if_convertible_stmt_p (gimple stmt, vec<data_reference_p> refs)\n       return true;\n \n     case GIMPLE_ASSIGN:\n-      return if_convertible_gimple_assign_stmt_p (stmt, refs);\n+      return if_convertible_gimple_assign_stmt_p (stmt, refs,\n+\t\t\t\t\t\t  any_mask_load_store);\n \n     case GIMPLE_CALL:\n       {\n@@ -984,7 +1082,7 @@ get_loop_body_in_if_conv_order (const struct loop *loop)\n    S1 will be predicated with \"x\", and\n    S2 will be predicated with \"!x\".  */\n \n-static bool\n+static void\n predicate_bbs (loop_p loop)\n {\n   unsigned int i;\n@@ -996,7 +1094,7 @@ predicate_bbs (loop_p loop)\n     {\n       basic_block bb = ifc_bbs[i];\n       tree cond;\n-      gimple_stmt_iterator itr;\n+      gimple stmt;\n \n       /* The loop latch is always executed and has no extra conditions\n \t to be processed: skip it.  */\n@@ -1007,52 +1105,32 @@ predicate_bbs (loop_p loop)\n \t}\n \n       cond = bb_predicate (bb);\n-\n-      for (itr = gsi_start_bb (bb); !gsi_end_p (itr); gsi_next (&itr))\n+      stmt = last_stmt (bb);\n+      if (stmt && gimple_code (stmt) == GIMPLE_COND)\n \t{\n-\t  gimple stmt = gsi_stmt (itr);\n-\n-\t  switch (gimple_code (stmt))\n-\t    {\n-\t    case GIMPLE_LABEL:\n-\t    case GIMPLE_ASSIGN:\n-\t    case GIMPLE_CALL:\n-\t    case GIMPLE_DEBUG:\n-\t      break;\n-\n-\t    case GIMPLE_COND:\n-\t      {\n-\t\ttree c2;\n-\t\tedge true_edge, false_edge;\n-\t\tlocation_t loc = gimple_location (stmt);\n-\t\ttree c = fold_build2_loc (loc, gimple_cond_code (stmt),\n-\t\t\t\t\t  boolean_type_node,\n-\t\t\t\t\t  gimple_cond_lhs (stmt),\n-\t\t\t\t\t  gimple_cond_rhs (stmt));\n-\n-\t\t/* Add new condition into destination's predicate list.  */\n-\t\textract_true_false_edges_from_block (gimple_bb (stmt),\n-\t\t\t\t\t\t     &true_edge, &false_edge);\n-\n-\t\t/* If C is true, then TRUE_EDGE is taken.  */\n-\t\tadd_to_dst_predicate_list (loop, true_edge,\n-\t\t\t\t\t   unshare_expr (cond),\n-\t\t\t\t\t   unshare_expr (c));\n-\n-\t\t/* If C is false, then FALSE_EDGE is taken.  */\n-\t\tc2 = build1_loc (loc, TRUTH_NOT_EXPR,\n-\t\t\t\t boolean_type_node, unshare_expr (c));\n-\t\tadd_to_dst_predicate_list (loop, false_edge,\n-\t\t\t\t\t   unshare_expr (cond), c2);\n-\n-\t\tcond = NULL_TREE;\n-\t\tbreak;\n-\t      }\n-\n-\t    default:\n-\t      /* Not handled yet in if-conversion.  */\n-\t      return false;\n-\t    }\n+\t  tree c2;\n+\t  edge true_edge, false_edge;\n+\t  location_t loc = gimple_location (stmt);\n+\t  tree c = fold_build2_loc (loc, gimple_cond_code (stmt),\n+\t\t\t\t    boolean_type_node,\n+\t\t\t\t    gimple_cond_lhs (stmt),\n+\t\t\t\t    gimple_cond_rhs (stmt));\n+\n+\t  /* Add new condition into destination's predicate list.  */\n+\t  extract_true_false_edges_from_block (gimple_bb (stmt),\n+\t\t\t\t\t       &true_edge, &false_edge);\n+\n+\t  /* If C is true, then TRUE_EDGE is taken.  */\n+\t  add_to_dst_predicate_list (loop, true_edge, unshare_expr (cond),\n+\t\t\t\t     unshare_expr (c));\n+\n+\t  /* If C is false, then FALSE_EDGE is taken.  */\n+\t  c2 = build1_loc (loc, TRUTH_NOT_EXPR, boolean_type_node,\n+\t\t\t   unshare_expr (c));\n+\t  add_to_dst_predicate_list (loop, false_edge,\n+\t\t\t\t     unshare_expr (cond), c2);\n+\n+\t  cond = NULL_TREE;\n \t}\n \n       /* If current bb has only one successor, then consider it as an\n@@ -1067,16 +1145,14 @@ predicate_bbs (loop_p loop)\n \t  if (cond == NULL_TREE)\n \t    cond = boolean_true_node;\n \n-\t  add_to_predicate_list (bb_n, cond);\n+\t  add_to_predicate_list (loop, bb_n, cond);\n \t}\n     }\n \n   /* The loop header is always executed.  */\n   reset_bb_predicate (loop->header);\n   gcc_assert (bb_predicate_gimplified_stmts (loop->header) == NULL\n \t      && bb_predicate_gimplified_stmts (loop->latch) == NULL);\n-\n-  return true;\n }\n \n /* Return true when LOOP is if-convertible.  This is a helper function\n@@ -1087,7 +1163,7 @@ static bool\n if_convertible_loop_p_1 (struct loop *loop,\n \t\t\t vec<loop_p> *loop_nest,\n \t\t\t vec<data_reference_p> *refs,\n-\t\t\t vec<ddr_p> *ddrs)\n+\t\t\t vec<ddr_p> *ddrs, bool *any_mask_load_store)\n {\n   bool res;\n   unsigned int i;\n@@ -1121,9 +1197,24 @@ if_convertible_loop_p_1 (struct loop *loop,\n \texit_bb = bb;\n     }\n \n-  res = predicate_bbs (loop);\n-  if (!res)\n-    return false;\n+  for (i = 0; i < loop->num_nodes; i++)\n+    {\n+      basic_block bb = ifc_bbs[i];\n+      gimple_stmt_iterator gsi;\n+\n+      for (gsi = gsi_start_bb (bb); !gsi_end_p (gsi); gsi_next (&gsi))\n+\tswitch (gimple_code (gsi_stmt (gsi)))\n+\t  {\n+\t  case GIMPLE_LABEL:\n+\t  case GIMPLE_ASSIGN:\n+\t  case GIMPLE_CALL:\n+\t  case GIMPLE_DEBUG:\n+\t  case GIMPLE_COND:\n+\t    break;\n+\t  default:\n+\t    return false;\n+\t  }\n+    }\n \n   if (flag_tree_loop_if_convert_stores)\n     {\n@@ -1135,24 +1226,39 @@ if_convertible_loop_p_1 (struct loop *loop,\n \t  DR_WRITTEN_AT_LEAST_ONCE (dr) = -1;\n \t  DR_RW_UNCONDITIONALLY (dr) = -1;\n \t}\n+      predicate_bbs (loop);\n     }\n \n   for (i = 0; i < loop->num_nodes; i++)\n     {\n       basic_block bb = ifc_bbs[i];\n       gimple_stmt_iterator itr;\n \n-      for (itr = gsi_start_phis (bb); !gsi_end_p (itr); gsi_next (&itr))\n-\tif (!if_convertible_phi_p (loop, bb, gsi_stmt (itr)))\n-\t  return false;\n-\n       /* Check the if-convertibility of statements in predicated BBs.  */\n-      if (is_predicated (bb))\n+      if (!dominated_by_p (CDI_DOMINATORS, loop->latch, bb))\n \tfor (itr = gsi_start_bb (bb); !gsi_end_p (itr); gsi_next (&itr))\n-\t  if (!if_convertible_stmt_p (gsi_stmt (itr), *refs))\n+\t  if (!if_convertible_stmt_p (gsi_stmt (itr), *refs,\n+\t\t\t\t      any_mask_load_store))\n \t    return false;\n     }\n \n+  if (flag_tree_loop_if_convert_stores)\n+    for (i = 0; i < loop->num_nodes; i++)\n+      free_bb_predicate (ifc_bbs[i]);\n+\n+  /* Checking PHIs needs to be done after stmts, as the fact whether there\n+     are any masked loads or stores affects the tests.  */\n+  for (i = 0; i < loop->num_nodes; i++)\n+    {\n+      basic_block bb = ifc_bbs[i];\n+      gimple_stmt_iterator itr;\n+\n+      for (itr = gsi_start_phis (bb); !gsi_end_p (itr); gsi_next (&itr))\n+\tif (!if_convertible_phi_p (loop, bb, gsi_stmt (itr),\n+\t\t\t\t   *any_mask_load_store))\n+\t  return false;\n+    }\n+\n   if (dump_file)\n     fprintf (dump_file, \"Applying if-conversion\\n\");\n \n@@ -1168,7 +1274,7 @@ if_convertible_loop_p_1 (struct loop *loop,\n    - if its basic blocks and phi nodes are if convertible.  */\n \n static bool\n-if_convertible_loop_p (struct loop *loop)\n+if_convertible_loop_p (struct loop *loop, bool *any_mask_load_store)\n {\n   edge e;\n   edge_iterator ei;\n@@ -1209,7 +1315,8 @@ if_convertible_loop_p (struct loop *loop)\n   refs.create (5);\n   ddrs.create (25);\n   stack_vec<loop_p, 3> loop_nest;\n-  res = if_convertible_loop_p_1 (loop, &loop_nest, &refs, &ddrs);\n+  res = if_convertible_loop_p_1 (loop, &loop_nest, &refs, &ddrs,\n+\t\t\t\t any_mask_load_store);\n \n   if (flag_tree_loop_if_convert_stores)\n     {\n@@ -1395,7 +1502,7 @@ predicate_all_scalar_phis (struct loop *loop)\n    gimplification of the predicates.  */\n \n static void\n-insert_gimplified_predicates (loop_p loop)\n+insert_gimplified_predicates (loop_p loop, bool any_mask_load_store)\n {\n   unsigned int i;\n \n@@ -1416,7 +1523,8 @@ insert_gimplified_predicates (loop_p loop)\n       stmts = bb_predicate_gimplified_stmts (bb);\n       if (stmts)\n \t{\n-\t  if (flag_tree_loop_if_convert_stores)\n+\t  if (flag_tree_loop_if_convert_stores\n+\t      || any_mask_load_store)\n \t    {\n \t      /* Insert the predicate of the BB just after the label,\n \t\t as the if-conversion of memory writes will use this\n@@ -1575,9 +1683,49 @@ predicate_mem_writes (loop_p loop)\n \t}\n \n       for (gsi = gsi_start_bb (bb); !gsi_end_p (gsi); gsi_next (&gsi))\n-\tif ((stmt = gsi_stmt (gsi))\n-\t    && gimple_assign_single_p (stmt)\n-\t    && gimple_vdef (stmt))\n+\tif (!gimple_assign_single_p (stmt = gsi_stmt (gsi)))\n+\t  continue;\n+\telse if (gimple_plf (stmt, GF_PLF_2))\n+\t  {\n+\t    tree lhs = gimple_assign_lhs (stmt);\n+\t    tree rhs = gimple_assign_rhs1 (stmt);\n+\t    tree ref, addr, ptr, masktype, mask_op0, mask_op1, mask;\n+\t    gimple new_stmt;\n+\t    int bitsize = GET_MODE_BITSIZE (TYPE_MODE (TREE_TYPE (lhs)));\n+\n+\t    masktype = build_nonstandard_integer_type (bitsize, 1);\n+\t    mask_op0 = build_int_cst (masktype, swap ? 0 : -1);\n+\t    mask_op1 = build_int_cst (masktype, swap ? -1 : 0);\n+\t    ref = TREE_CODE (lhs) == SSA_NAME ? rhs : lhs;\n+\t    mark_addressable (ref);\n+\t    addr = force_gimple_operand_gsi (&gsi, build_fold_addr_expr (ref),\n+\t\t\t\t\t     true, NULL_TREE, true,\n+\t\t\t\t\t     GSI_SAME_STMT);\n+\t    cond = force_gimple_operand_gsi_1 (&gsi, unshare_expr (cond),\n+\t\t\t\t\t       is_gimple_condexpr, NULL_TREE,\n+\t\t\t\t\t       true, GSI_SAME_STMT);\n+\t    mask = fold_build_cond_expr (masktype, unshare_expr (cond),\n+\t\t\t\t\t mask_op0, mask_op1);\n+\t    mask = ifc_temp_var (masktype, mask, &gsi);\n+\t    ptr = build_int_cst (reference_alias_ptr_type (ref), 0);\n+\t    /* Copy points-to info if possible.  */\n+\t    if (TREE_CODE (addr) == SSA_NAME && !SSA_NAME_PTR_INFO (addr))\n+\t      copy_ref_info (build2 (MEM_REF, TREE_TYPE (ref), addr, ptr),\n+\t\t\t     ref);\n+\t    if (TREE_CODE (lhs) == SSA_NAME)\n+\t      {\n+\t\tnew_stmt\n+\t\t  = gimple_build_call_internal (IFN_MASK_LOAD, 3, addr,\n+\t\t\t\t\t\tptr, mask);\n+\t\tgimple_call_set_lhs (new_stmt, lhs);\n+\t      }\n+\t    else\n+\t      new_stmt\n+\t\t= gimple_build_call_internal (IFN_MASK_STORE, 4, addr, ptr,\n+\t\t\t\t\t      mask, rhs);\n+\t    gsi_replace (&gsi, new_stmt, false);\n+\t  }\n+\telse if (gimple_vdef (stmt))\n \t  {\n \t    tree lhs = gimple_assign_lhs (stmt);\n \t    tree rhs = gimple_assign_rhs1 (stmt);\n@@ -1647,19 +1795,20 @@ remove_conditions_and_labels (loop_p loop)\n    blocks.  Replace PHI nodes with conditional modify expressions.  */\n \n static void\n-combine_blocks (struct loop *loop)\n+combine_blocks (struct loop *loop, bool any_mask_load_store)\n {\n   basic_block bb, exit_bb, merge_target_bb;\n   unsigned int orig_loop_num_nodes = loop->num_nodes;\n   unsigned int i;\n   edge e;\n   edge_iterator ei;\n \n+  predicate_bbs (loop);\n   remove_conditions_and_labels (loop);\n-  insert_gimplified_predicates (loop);\n+  insert_gimplified_predicates (loop, any_mask_load_store);\n   predicate_all_scalar_phis (loop);\n \n-  if (flag_tree_loop_if_convert_stores)\n+  if (flag_tree_loop_if_convert_stores || any_mask_load_store)\n     predicate_mem_writes (loop);\n \n   /* Merge basic blocks: first remove all the edges in the loop,\n@@ -1749,28 +1898,76 @@ combine_blocks (struct loop *loop)\n   ifc_bbs = NULL;\n }\n \n-/* If-convert LOOP when it is legal.  For the moment this pass has no\n-   profitability analysis.  Returns true when something changed.  */\n+/* Version LOOP before if-converting it, the original loop\n+   will be then if-converted, the new copy of the loop will not,\n+   and the LOOP_VECTORIZED internal call will be guarding which\n+   loop to execute.  The vectorizer pass will fold this\n+   internal call into either true or false.  */\n \n static bool\n+version_loop_for_if_conversion (struct loop *loop)\n+{\n+  basic_block cond_bb;\n+  tree cond = make_ssa_name (boolean_type_node, NULL);\n+  struct loop *new_loop;\n+  gimple g;\n+  gimple_stmt_iterator gsi;\n+\n+  g = gimple_build_call_internal (IFN_LOOP_VECTORIZED, 2,\n+\t\t\t\t  build_int_cst (integer_type_node, loop->num),\n+\t\t\t\t  integer_zero_node);\n+  gimple_call_set_lhs (g, cond);\n+\n+  initialize_original_copy_tables ();\n+  new_loop = loop_version (loop, cond, &cond_bb,\n+\t\t\t   REG_BR_PROB_BASE, REG_BR_PROB_BASE,\n+\t\t\t   REG_BR_PROB_BASE, true);\n+  free_original_copy_tables ();\n+  if (new_loop == NULL)\n+    return false;\n+  new_loop->dont_vectorize = true;\n+  new_loop->force_vect = false;\n+  gsi = gsi_last_bb (cond_bb);\n+  gimple_call_set_arg (g, 1, build_int_cst (integer_type_node, new_loop->num));\n+  gsi_insert_before (&gsi, g, GSI_SAME_STMT);\n+  update_ssa (TODO_update_ssa);\n+  return true;\n+}\n+\n+/* If-convert LOOP when it is legal.  For the moment this pass has no\n+   profitability analysis.  Returns non-zero todo flags when something\n+   changed.  */\n+\n+static unsigned int\n tree_if_conversion (struct loop *loop)\n {\n-  bool changed = false;\n+  unsigned int todo = 0;\n   ifc_bbs = NULL;\n+  bool any_mask_load_store = false;\n \n-  if (!if_convertible_loop_p (loop)\n+  if (!if_convertible_loop_p (loop, &any_mask_load_store)\n       || !dbg_cnt (if_conversion_tree))\n     goto cleanup;\n \n+  if (any_mask_load_store\n+      && ((!flag_tree_loop_vectorize && !loop->force_vect)\n+\t  || loop->dont_vectorize))\n+    goto cleanup;\n+\n+  if (any_mask_load_store && !version_loop_for_if_conversion (loop))\n+    goto cleanup;\n+\n   /* Now all statements are if-convertible.  Combine all the basic\n      blocks into one huge basic block doing the if-conversion\n      on-the-fly.  */\n-  combine_blocks (loop);\n-\n-  if (flag_tree_loop_if_convert_stores)\n-    mark_virtual_operands_for_renaming (cfun);\n+  combine_blocks (loop, any_mask_load_store);\n \n-  changed = true;\n+  todo |= TODO_cleanup_cfg;\n+  if (flag_tree_loop_if_convert_stores || any_mask_load_store)\n+    {\n+      mark_virtual_operands_for_renaming (cfun);\n+      todo |= TODO_update_ssa_only_virtuals;\n+    }\n \n  cleanup:\n   if (ifc_bbs)\n@@ -1784,7 +1981,7 @@ tree_if_conversion (struct loop *loop)\n       ifc_bbs = NULL;\n     }\n \n-  return changed;\n+  return todo;\n }\n \n /* Tree if-conversion pass management.  */\n@@ -1793,7 +1990,6 @@ static unsigned int\n main_tree_if_conversion (void)\n {\n   struct loop *loop;\n-  bool changed = false;\n   unsigned todo = 0;\n \n   if (number_of_loops (cfun) <= 1)\n@@ -1802,15 +1998,9 @@ main_tree_if_conversion (void)\n   FOR_EACH_LOOP (loop, 0)\n     if (flag_tree_loop_if_convert == 1\n \t|| flag_tree_loop_if_convert_stores == 1\n-\t|| flag_tree_loop_vectorize\n-\t|| loop->force_vect)\n-    changed |= tree_if_conversion (loop);\n-\n-  if (changed)\n-    todo |= TODO_cleanup_cfg;\n-\n-  if (changed && flag_tree_loop_if_convert_stores)\n-    todo |= TODO_update_ssa_only_virtuals;\n+\t|| ((flag_tree_loop_vectorize || loop->force_vect)\n+\t    && !loop->dont_vectorize))\n+      todo |= tree_if_conversion (loop);\n \n #ifdef ENABLE_CHECKING\n   {"}, {"sha": "c16e51fb7c7f291cbb2f495354bdfc201f6f509b", "filename": "gcc/tree-loop-distribution.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-loop-distribution.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-loop-distribution.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-loop-distribution.c?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -588,7 +588,7 @@ copy_loop_before (struct loop *loop)\n   edge preheader = loop_preheader_edge (loop);\n \n   initialize_original_copy_tables ();\n-  res = slpeel_tree_duplicate_loop_to_edge_cfg (loop, preheader);\n+  res = slpeel_tree_duplicate_loop_to_edge_cfg (loop, NULL, preheader);\n   gcc_assert (res != NULL);\n   free_original_copy_tables ();\n   delete_update_ssa ();"}, {"sha": "ea31302101fbeb1b8897e73dfb765bf15b75390d", "filename": "gcc/tree-predcom.c", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-predcom.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-predcom.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-predcom.c?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -732,6 +732,9 @@ split_data_refs_to_components (struct loop *loop,\n \t     just fail.  */\n \t  goto end;\n \t}\n+      /* predcom pass isn't prepared to handle calls with data references.  */\n+      if (is_gimple_call (DR_STMT (dr)))\n+\tgoto end;\n       dr->aux = (void *) (size_t) i;\n       comp_father[i] = i;\n       comp_size[i] = 1;"}, {"sha": "4c3fe509f20d95c3f599ec68b16175c0f6fc6fe8", "filename": "gcc/tree-ssa-phiopt.c", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-ssa-phiopt.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-ssa-phiopt.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-phiopt.c?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -1706,7 +1706,7 @@ cond_if_else_store_replacement (basic_block then_bb, basic_block else_bb,\n         == chrec_dont_know)\n       || !then_datarefs.length ()\n       || (find_data_references_in_bb (NULL, else_bb, &else_datarefs)\n-        == chrec_dont_know)\n+\t  == chrec_dont_know)\n       || !else_datarefs.length ())\n     {\n       free_data_refs (then_datarefs);\n@@ -1723,6 +1723,8 @@ cond_if_else_store_replacement (basic_block then_bb, basic_block else_bb,\n \n       then_store = DR_STMT (then_dr);\n       then_lhs = gimple_get_lhs (then_store);\n+      if (then_lhs == NULL_TREE)\n+\tcontinue;\n       found = false;\n \n       FOR_EACH_VEC_ELT (else_datarefs, j, else_dr)\n@@ -1732,6 +1734,8 @@ cond_if_else_store_replacement (basic_block then_bb, basic_block else_bb,\n \n           else_store = DR_STMT (else_dr);\n           else_lhs = gimple_get_lhs (else_store);\n+\t  if (else_lhs == NULL_TREE)\n+\t    continue;\n \n           if (operand_equal_p (then_lhs, else_lhs, 0))\n             {"}, {"sha": "51c08b9b03accc87a5d9f31a1eca996a902efad3", "filename": "gcc/tree-vect-data-refs.c", "status": "modified", "additions": 31, "deletions": 2, "changes": 33, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-vect-data-refs.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-vect-data-refs.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-data-refs.c?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -2919,6 +2919,24 @@ vect_check_gather (gimple stmt, loop_vec_info loop_vinfo, tree *basep,\n   enum machine_mode pmode;\n   int punsignedp, pvolatilep;\n \n+  base = DR_REF (dr);\n+  /* For masked loads/stores, DR_REF (dr) is an artificial MEM_REF,\n+     see if we can use the def stmt of the address.  */\n+  if (is_gimple_call (stmt)\n+      && gimple_call_internal_p (stmt)\n+      && (gimple_call_internal_fn (stmt) == IFN_MASK_LOAD\n+\t  || gimple_call_internal_fn (stmt) == IFN_MASK_STORE)\n+      && TREE_CODE (base) == MEM_REF\n+      && TREE_CODE (TREE_OPERAND (base, 0)) == SSA_NAME\n+      && integer_zerop (TREE_OPERAND (base, 1))\n+      && !expr_invariant_in_loop_p (loop, TREE_OPERAND (base, 0)))\n+    {\n+      gimple def_stmt = SSA_NAME_DEF_STMT (TREE_OPERAND (base, 0));\n+      if (is_gimple_assign (def_stmt)\n+\t  && gimple_assign_rhs_code (def_stmt) == ADDR_EXPR)\n+\tbase = TREE_OPERAND (gimple_assign_rhs1 (def_stmt), 0);\n+    }\n+\n   /* The gather builtins need address of the form\n      loop_invariant + vector * {1, 2, 4, 8}\n      or\n@@ -2931,7 +2949,7 @@ vect_check_gather (gimple stmt, loop_vec_info loop_vinfo, tree *basep,\n      vectorized.  The following code attempts to find such a preexistng\n      SSA_NAME OFF and put the loop invariants into a tree BASE\n      that can be gimplified before the loop.  */\n-  base = get_inner_reference (DR_REF (dr), &pbitsize, &pbitpos, &off,\n+  base = get_inner_reference (base, &pbitsize, &pbitpos, &off,\n \t\t\t      &pmode, &punsignedp, &pvolatilep, false);\n   gcc_assert (base != NULL_TREE && (pbitpos % BITS_PER_UNIT) == 0);\n \n@@ -3428,7 +3446,10 @@ vect_analyze_data_refs (loop_vec_info loop_vinfo,\n       offset = unshare_expr (DR_OFFSET (dr));\n       init = unshare_expr (DR_INIT (dr));\n \n-      if (is_gimple_call (stmt))\n+      if (is_gimple_call (stmt)\n+\t  && (!gimple_call_internal_p (stmt)\n+\t      || (gimple_call_internal_fn (stmt) != IFN_MASK_LOAD\n+\t\t  && gimple_call_internal_fn (stmt) != IFN_MASK_STORE)))\n \t{\n \t  if (dump_enabled_p ())\n \t    {\n@@ -5079,6 +5100,14 @@ vect_supportable_dr_alignment (struct data_reference *dr,\n   if (aligned_access_p (dr) && !check_aligned_accesses)\n     return dr_aligned;\n \n+  /* For now assume all conditional loads/stores support unaligned\n+     access without any special code.  */\n+  if (is_gimple_call (stmt)\n+      && gimple_call_internal_p (stmt)\n+      && (gimple_call_internal_fn (stmt) == IFN_MASK_LOAD\n+\t  || gimple_call_internal_fn (stmt) == IFN_MASK_STORE))\n+    return dr_unaligned_supported;\n+\n   if (loop_vinfo)\n     {\n       vect_loop = LOOP_VINFO_LOOP (loop_vinfo);"}, {"sha": "c5a3c72bc361d5ca24215aaccca3672389e7dfde", "filename": "gcc/tree-vect-loop-manip.c", "status": "modified", "additions": 196, "deletions": 47, "changes": 243, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-vect-loop-manip.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-vect-loop-manip.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-loop-manip.c?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -703,12 +703,42 @@ slpeel_make_loop_iterate_ntimes (struct loop *loop, tree niters)\n   loop->nb_iterations = niters;\n }\n \n+/* Helper routine of slpeel_tree_duplicate_loop_to_edge_cfg.\n+   For all PHI arguments in FROM->dest and TO->dest from those\n+   edges ensure that TO->dest PHI arguments have current_def\n+   to that in from.  */\n+\n+static void\n+slpeel_duplicate_current_defs_from_edges (edge from, edge to)\n+{\n+  gimple_stmt_iterator gsi_from, gsi_to;\n+\n+  for (gsi_from = gsi_start_phis (from->dest),\n+       gsi_to = gsi_start_phis (to->dest);\n+       !gsi_end_p (gsi_from) && !gsi_end_p (gsi_to);\n+       gsi_next (&gsi_from), gsi_next (&gsi_to))\n+    {\n+      gimple from_phi = gsi_stmt (gsi_from);\n+      gimple to_phi = gsi_stmt (gsi_to);\n+      tree from_arg = PHI_ARG_DEF_FROM_EDGE (from_phi, from);\n+      tree to_arg = PHI_ARG_DEF_FROM_EDGE (to_phi, to);\n+      if (TREE_CODE (from_arg) == SSA_NAME\n+\t  && TREE_CODE (to_arg) == SSA_NAME\n+\t  && get_current_def (to_arg) == NULL_TREE)\n+\tset_current_def (to_arg, get_current_def (from_arg));\n+    }\n+}\n+\n \n /* Given LOOP this function generates a new copy of it and puts it\n-   on E which is either the entry or exit of LOOP.  */\n+   on E which is either the entry or exit of LOOP.  If SCALAR_LOOP is\n+   non-NULL, assume LOOP and SCALAR_LOOP are equivalent and copy the\n+   basic blocks from SCALAR_LOOP instead of LOOP, but to either the\n+   entry or exit of LOOP.  */\n \n struct loop *\n-slpeel_tree_duplicate_loop_to_edge_cfg (struct loop *loop, edge e)\n+slpeel_tree_duplicate_loop_to_edge_cfg (struct loop *loop,\n+\t\t\t\t\tstruct loop *scalar_loop, edge e)\n {\n   struct loop *new_loop;\n   basic_block *new_bbs, *bbs;\n@@ -722,19 +752,22 @@ slpeel_tree_duplicate_loop_to_edge_cfg (struct loop *loop, edge e)\n   if (!at_exit && e != loop_preheader_edge (loop))\n     return NULL;\n \n-  bbs = XNEWVEC (basic_block, loop->num_nodes + 1);\n-  get_loop_body_with_size (loop, bbs, loop->num_nodes);\n+  if (scalar_loop == NULL)\n+    scalar_loop = loop;\n+\n+  bbs = XNEWVEC (basic_block, scalar_loop->num_nodes + 1);\n+  get_loop_body_with_size (scalar_loop, bbs, scalar_loop->num_nodes);\n \n   /* Check whether duplication is possible.  */\n-  if (!can_copy_bbs_p (bbs, loop->num_nodes))\n+  if (!can_copy_bbs_p (bbs, scalar_loop->num_nodes))\n     {\n       free (bbs);\n       return NULL;\n     }\n \n   /* Generate new loop structure.  */\n-  new_loop = duplicate_loop (loop, loop_outer (loop));\n-  duplicate_subloops (loop, new_loop);\n+  new_loop = duplicate_loop (scalar_loop, loop_outer (scalar_loop));\n+  duplicate_subloops (scalar_loop, new_loop);\n \n   exit_dest = exit->dest;\n   was_imm_dom = (get_immediate_dominator (CDI_DOMINATORS,\n@@ -744,35 +777,80 @@ slpeel_tree_duplicate_loop_to_edge_cfg (struct loop *loop, edge e)\n   /* Also copy the pre-header, this avoids jumping through hoops to\n      duplicate the loop entry PHI arguments.  Create an empty\n      pre-header unconditionally for this.  */\n-  basic_block preheader = split_edge (loop_preheader_edge (loop));\n+  basic_block preheader = split_edge (loop_preheader_edge (scalar_loop));\n   edge entry_e = single_pred_edge (preheader);\n-  bbs[loop->num_nodes] = preheader;\n-  new_bbs = XNEWVEC (basic_block, loop->num_nodes + 1);\n+  bbs[scalar_loop->num_nodes] = preheader;\n+  new_bbs = XNEWVEC (basic_block, scalar_loop->num_nodes + 1);\n \n-  copy_bbs (bbs, loop->num_nodes + 1, new_bbs,\n+  exit = single_exit (scalar_loop);\n+  copy_bbs (bbs, scalar_loop->num_nodes + 1, new_bbs,\n \t    &exit, 1, &new_exit, NULL,\n \t    e->src, true);\n-  basic_block new_preheader = new_bbs[loop->num_nodes];\n+  exit = single_exit (loop);\n+  basic_block new_preheader = new_bbs[scalar_loop->num_nodes];\n \n-  add_phi_args_after_copy (new_bbs, loop->num_nodes + 1, NULL);\n+  add_phi_args_after_copy (new_bbs, scalar_loop->num_nodes + 1, NULL);\n+\n+  if (scalar_loop != loop)\n+    {\n+      /* If we copied from SCALAR_LOOP rather than LOOP, SSA_NAMEs from\n+\t SCALAR_LOOP will have current_def set to SSA_NAMEs in the new_loop,\n+\t but LOOP will not.  slpeel_update_phi_nodes_for_guard{1,2} expects\n+\t the LOOP SSA_NAMEs (on the exit edge and edge from latch to\n+\t header) to have current_def set, so copy them over.  */\n+      slpeel_duplicate_current_defs_from_edges (single_exit (scalar_loop),\n+\t\t\t\t\t\texit);\n+      slpeel_duplicate_current_defs_from_edges (EDGE_SUCC (scalar_loop->latch,\n+\t\t\t\t\t\t\t   0),\n+\t\t\t\t\t\tEDGE_SUCC (loop->latch, 0));\n+    }\n \n   if (at_exit) /* Add the loop copy at exit.  */\n     {\n+      if (scalar_loop != loop)\n+\t{\n+\t  gimple_stmt_iterator gsi;\n+\t  new_exit = redirect_edge_and_branch (new_exit, exit_dest);\n+\n+\t  for (gsi = gsi_start_phis (exit_dest); !gsi_end_p (gsi);\n+\t       gsi_next (&gsi))\n+\t    {\n+\t      gimple phi = gsi_stmt (gsi);\n+\t      tree orig_arg = PHI_ARG_DEF_FROM_EDGE (phi, e);\n+\t      location_t orig_locus\n+\t\t= gimple_phi_arg_location_from_edge (phi, e);\n+\n+\t      add_phi_arg (phi, orig_arg, new_exit, orig_locus);\n+\t    }\n+\t}\n       redirect_edge_and_branch_force (e, new_preheader);\n       flush_pending_stmts (e);\n       set_immediate_dominator (CDI_DOMINATORS, new_preheader, e->src);\n       if (was_imm_dom)\n-\tset_immediate_dominator (CDI_DOMINATORS, exit_dest, new_loop->header);\n+\tset_immediate_dominator (CDI_DOMINATORS, exit_dest, new_exit->src);\n \n       /* And remove the non-necessary forwarder again.  Keep the other\n          one so we have a proper pre-header for the loop at the exit edge.  */\n-      redirect_edge_pred (single_succ_edge (preheader), single_pred (preheader));\n+      redirect_edge_pred (single_succ_edge (preheader),\n+\t\t\t  single_pred (preheader));\n       delete_basic_block (preheader);\n-      set_immediate_dominator (CDI_DOMINATORS, loop->header,\n-\t\t\t       loop_preheader_edge (loop)->src);\n+      set_immediate_dominator (CDI_DOMINATORS, scalar_loop->header,\n+\t\t\t       loop_preheader_edge (scalar_loop)->src);\n     }\n   else /* Add the copy at entry.  */\n     {\n+      if (scalar_loop != loop)\n+\t{\n+\t  /* Remove the non-necessary forwarder of scalar_loop again.  */\n+\t  redirect_edge_pred (single_succ_edge (preheader),\n+\t\t\t      single_pred (preheader));\n+\t  delete_basic_block (preheader);\n+\t  set_immediate_dominator (CDI_DOMINATORS, scalar_loop->header,\n+\t\t\t\t   loop_preheader_edge (scalar_loop)->src);\n+\t  preheader = split_edge (loop_preheader_edge (loop));\n+\t  entry_e = single_pred_edge (preheader);\n+\t}\n+\n       redirect_edge_and_branch_force (entry_e, new_preheader);\n       flush_pending_stmts (entry_e);\n       set_immediate_dominator (CDI_DOMINATORS, new_preheader, entry_e->src);\n@@ -783,15 +861,39 @@ slpeel_tree_duplicate_loop_to_edge_cfg (struct loop *loop, edge e)\n \n       /* And remove the non-necessary forwarder again.  Keep the other\n          one so we have a proper pre-header for the loop at the exit edge.  */\n-      redirect_edge_pred (single_succ_edge (new_preheader), single_pred (new_preheader));\n+      redirect_edge_pred (single_succ_edge (new_preheader),\n+\t\t\t  single_pred (new_preheader));\n       delete_basic_block (new_preheader);\n       set_immediate_dominator (CDI_DOMINATORS, new_loop->header,\n \t\t\t       loop_preheader_edge (new_loop)->src);\n     }\n \n-  for (unsigned i = 0; i < loop->num_nodes+1; i++)\n+  for (unsigned i = 0; i < scalar_loop->num_nodes + 1; i++)\n     rename_variables_in_bb (new_bbs[i]);\n \n+  if (scalar_loop != loop)\n+    {\n+      /* Update new_loop->header PHIs, so that on the preheader\n+\t edge they are the ones from loop rather than scalar_loop.  */\n+      gimple_stmt_iterator gsi_orig, gsi_new;\n+      edge orig_e = loop_preheader_edge (loop);\n+      edge new_e = loop_preheader_edge (new_loop);\n+\n+      for (gsi_orig = gsi_start_phis (loop->header),\n+\t   gsi_new = gsi_start_phis (new_loop->header);\n+\t   !gsi_end_p (gsi_orig) && !gsi_end_p (gsi_new);\n+\t   gsi_next (&gsi_orig), gsi_next (&gsi_new))\n+\t{\n+\t  gimple orig_phi = gsi_stmt (gsi_orig);\n+\t  gimple new_phi = gsi_stmt (gsi_new);\n+\t  tree orig_arg = PHI_ARG_DEF_FROM_EDGE (orig_phi, orig_e);\n+\t  location_t orig_locus\n+\t    = gimple_phi_arg_location_from_edge (orig_phi, orig_e);\n+\n+\t  add_phi_arg (new_phi, orig_arg, new_e, orig_locus);\n+\t}\n+    }\n+\n   free (new_bbs);\n   free (bbs);\n \n@@ -1002,6 +1104,8 @@ set_prologue_iterations (basic_block bb_before_first_loop,\n \n    Input:\n    - LOOP: the loop to be peeled.\n+   - SCALAR_LOOP: if non-NULL, the alternate loop from which basic blocks\n+\tshould be copied.\n    - E: the exit or entry edge of LOOP.\n         If it is the entry edge, we peel the first iterations of LOOP. In this\n         case first-loop is LOOP, and second-loop is the newly created loop.\n@@ -1043,8 +1147,8 @@ set_prologue_iterations (basic_block bb_before_first_loop,\n    FORNOW the resulting code will not be in loop-closed-ssa form.\n */\n \n-static struct loop*\n-slpeel_tree_peel_loop_to_edge (struct loop *loop,\n+static struct loop *\n+slpeel_tree_peel_loop_to_edge (struct loop *loop, struct loop *scalar_loop,\n \t\t\t       edge e, tree *first_niters,\n \t\t\t       tree niters, bool update_first_loop_count,\n \t\t\t       unsigned int th, bool check_profitability,\n@@ -1128,7 +1232,8 @@ slpeel_tree_peel_loop_to_edge (struct loop *loop,\n         orig_exit_bb:\n    */\n \n-  if (!(new_loop = slpeel_tree_duplicate_loop_to_edge_cfg (loop, e)))\n+  if (!(new_loop = slpeel_tree_duplicate_loop_to_edge_cfg (loop, scalar_loop,\n+\t\t\t\t\t\t\t   e)))\n     {\n       loop_loc = find_loop_location (loop);\n       dump_printf_loc (MSG_MISSED_OPTIMIZATION, loop_loc,\n@@ -1620,6 +1725,7 @@ vect_do_peeling_for_loop_bound (loop_vec_info loop_vinfo,\n \t\t\t\tunsigned int th, bool check_profitability)\n {\n   struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  struct loop *scalar_loop = LOOP_VINFO_SCALAR_LOOP (loop_vinfo);\n   struct loop *new_loop;\n   edge update_e;\n   basic_block preheader;\n@@ -1636,11 +1742,12 @@ vect_do_peeling_for_loop_bound (loop_vec_info loop_vinfo,\n \n   loop_num  = loop->num;\n \n-  new_loop = slpeel_tree_peel_loop_to_edge (loop, single_exit (loop),\n-                                            &ratio_mult_vf_name, ni_name, false,\n-                                            th, check_profitability,\n-\t\t\t\t\t    cond_expr, cond_expr_stmt_list,\n-\t\t\t\t\t    0, LOOP_VINFO_VECT_FACTOR (loop_vinfo));\n+  new_loop\n+    = slpeel_tree_peel_loop_to_edge (loop, scalar_loop, single_exit (loop),\n+\t\t\t\t     &ratio_mult_vf_name, ni_name, false,\n+\t\t\t\t     th, check_profitability,\n+\t\t\t\t     cond_expr, cond_expr_stmt_list,\n+\t\t\t\t     0, LOOP_VINFO_VECT_FACTOR (loop_vinfo));\n   gcc_assert (new_loop);\n   gcc_assert (loop_num == loop->num);\n #ifdef ENABLE_CHECKING\n@@ -1873,6 +1980,7 @@ vect_do_peeling_for_alignment (loop_vec_info loop_vinfo, tree ni_name,\n \t\t\t       unsigned int th, bool check_profitability)\n {\n   struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  struct loop *scalar_loop = LOOP_VINFO_SCALAR_LOOP (loop_vinfo);\n   tree niters_of_prolog_loop;\n   tree wide_prolog_niters;\n   struct loop *new_loop;\n@@ -1894,11 +2002,11 @@ vect_do_peeling_for_alignment (loop_vec_info loop_vinfo, tree ni_name,\n \n   /* Peel the prolog loop and iterate it niters_of_prolog_loop.  */\n   new_loop =\n-    slpeel_tree_peel_loop_to_edge (loop, loop_preheader_edge (loop),\n+    slpeel_tree_peel_loop_to_edge (loop, scalar_loop,\n+\t\t\t\t   loop_preheader_edge (loop),\n \t\t\t\t   &niters_of_prolog_loop, ni_name, true,\n \t\t\t\t   th, check_profitability, NULL_TREE, NULL,\n-\t\t\t\t   bound,\n-\t\t\t\t   0);\n+\t\t\t\t   bound, 0);\n \n   gcc_assert (new_loop);\n #ifdef ENABLE_CHECKING\n@@ -2185,6 +2293,7 @@ vect_loop_versioning (loop_vec_info loop_vinfo,\n \t\t      unsigned int th, bool check_profitability)\n {\n   struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  struct loop *scalar_loop = LOOP_VINFO_SCALAR_LOOP (loop_vinfo);\n   basic_block condition_bb;\n   gimple_stmt_iterator gsi, cond_exp_gsi;\n   basic_block merge_bb;\n@@ -2220,8 +2329,43 @@ vect_loop_versioning (loop_vec_info loop_vinfo,\n   gimple_seq_add_seq (&cond_expr_stmt_list, gimplify_stmt_list);\n \n   initialize_original_copy_tables ();\n-  loop_version (loop, cond_expr, &condition_bb,\n-\t\tprob, prob, REG_BR_PROB_BASE - prob, true);\n+  if (scalar_loop)\n+    {\n+      edge scalar_e;\n+      basic_block preheader, scalar_preheader;\n+\n+      /* We don't want to scale SCALAR_LOOP's frequencies, we need to\n+\t scale LOOP's frequencies instead.  */\n+      loop_version (scalar_loop, cond_expr, &condition_bb,\n+\t\t    prob, REG_BR_PROB_BASE, REG_BR_PROB_BASE - prob, true);\n+      scale_loop_frequencies (loop, prob, REG_BR_PROB_BASE);\n+      /* CONDITION_BB was created above SCALAR_LOOP's preheader,\n+\t while we need to move it above LOOP's preheader.  */\n+      e = loop_preheader_edge (loop);\n+      scalar_e = loop_preheader_edge (scalar_loop);\n+      gcc_assert (empty_block_p (e->src)\n+\t\t  && single_pred_p (e->src));\n+      gcc_assert (empty_block_p (scalar_e->src)\n+\t\t  && single_pred_p (scalar_e->src));\n+      gcc_assert (single_pred_p (condition_bb));\n+      preheader = e->src;\n+      scalar_preheader = scalar_e->src;\n+      scalar_e = find_edge (condition_bb, scalar_preheader);\n+      e = single_pred_edge (preheader);\n+      redirect_edge_and_branch_force (single_pred_edge (condition_bb),\n+\t\t\t\t      scalar_preheader);\n+      redirect_edge_and_branch_force (scalar_e, preheader);\n+      redirect_edge_and_branch_force (e, condition_bb);\n+      set_immediate_dominator (CDI_DOMINATORS, condition_bb,\n+\t\t\t       single_pred (condition_bb));\n+      set_immediate_dominator (CDI_DOMINATORS, scalar_preheader,\n+\t\t\t       single_pred (scalar_preheader));\n+      set_immediate_dominator (CDI_DOMINATORS, preheader,\n+\t\t\t       condition_bb);\n+    }\n+  else\n+    loop_version (loop, cond_expr, &condition_bb,\n+\t\t  prob, prob, REG_BR_PROB_BASE - prob, true);\n \n   if (LOCATION_LOCUS (vect_location) != UNKNOWN_LOCATION\n       && dump_enabled_p ())\n@@ -2244,24 +2388,29 @@ vect_loop_versioning (loop_vec_info loop_vinfo,\n      basic block (i.e. it has two predecessors). Just in order to simplify\n      following transformations in the vectorizer, we fix this situation\n      here by adding a new (empty) block on the exit-edge of the loop,\n-     with the proper loop-exit phis to maintain loop-closed-form.  */\n+     with the proper loop-exit phis to maintain loop-closed-form.\n+     If loop versioning wasn't done from loop, but scalar_loop instead,\n+     merge_bb will have already just a single successor.  */\n \n   merge_bb = single_exit (loop)->dest;\n-  gcc_assert (EDGE_COUNT (merge_bb->preds) == 2);\n-  new_exit_bb = split_edge (single_exit (loop));\n-  new_exit_e = single_exit (loop);\n-  e = EDGE_SUCC (new_exit_bb, 0);\n-\n-  for (gsi = gsi_start_phis (merge_bb); !gsi_end_p (gsi); gsi_next (&gsi))\n+  if (scalar_loop == NULL || EDGE_COUNT (merge_bb->preds) >= 2)\n     {\n-      tree new_res;\n-      orig_phi = gsi_stmt (gsi);\n-      new_res = copy_ssa_name (PHI_RESULT (orig_phi), NULL);\n-      new_phi = create_phi_node (new_res, new_exit_bb);\n-      arg = PHI_ARG_DEF_FROM_EDGE (orig_phi, e);\n-      add_phi_arg (new_phi, arg, new_exit_e,\n-\t\t   gimple_phi_arg_location_from_edge (orig_phi, e));\n-      adjust_phi_and_debug_stmts (orig_phi, e, PHI_RESULT (new_phi));\n+      gcc_assert (EDGE_COUNT (merge_bb->preds) >= 2);\n+      new_exit_bb = split_edge (single_exit (loop));\n+      new_exit_e = single_exit (loop);\n+      e = EDGE_SUCC (new_exit_bb, 0);\n+\n+      for (gsi = gsi_start_phis (merge_bb); !gsi_end_p (gsi); gsi_next (&gsi))\n+\t{\n+\t  tree new_res;\n+\t  orig_phi = gsi_stmt (gsi);\n+\t  new_res = copy_ssa_name (PHI_RESULT (orig_phi), NULL);\n+\t  new_phi = create_phi_node (new_res, new_exit_bb);\n+\t  arg = PHI_ARG_DEF_FROM_EDGE (orig_phi, e);\n+\t  add_phi_arg (new_phi, arg, new_exit_e,\n+\t\t       gimple_phi_arg_location_from_edge (orig_phi, e));\n+\t  adjust_phi_and_debug_stmts (orig_phi, e, PHI_RESULT (new_phi));\n+\t}\n     }\n \n "}, {"sha": "fc377477648030c6a5e28895bbdfb62950551f6b", "filename": "gcc/tree-vect-loop.c", "status": "modified", "additions": 11, "deletions": 2, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-vect-loop.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-vect-loop.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-loop.c?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -374,7 +374,11 @@ vect_determine_vectorization_factor (loop_vec_info loop_vinfo)\n \t\tanalyze_pattern_stmt = false;\n \t    }\n \n-\t  if (gimple_get_lhs (stmt) == NULL_TREE)\n+\t  if (gimple_get_lhs (stmt) == NULL_TREE\n+\t      /* MASK_STORE has no lhs, but is ok.  */\n+\t      && (!is_gimple_call (stmt)\n+\t\t  || !gimple_call_internal_p (stmt)\n+\t\t  || gimple_call_internal_fn (stmt) != IFN_MASK_STORE))\n \t    {\n \t      if (is_gimple_call (stmt))\n \t\t{\n@@ -426,7 +430,12 @@ vect_determine_vectorization_factor (loop_vec_info loop_vinfo)\n \t  else\n \t    {\n \t      gcc_assert (!STMT_VINFO_DATA_REF (stmt_info));\n-\t      scalar_type = TREE_TYPE (gimple_get_lhs (stmt));\n+\t      if (is_gimple_call (stmt)\n+\t\t  && gimple_call_internal_p (stmt)\n+\t\t  && gimple_call_internal_fn (stmt) == IFN_MASK_STORE)\n+\t\tscalar_type = TREE_TYPE (gimple_call_arg (stmt, 3));\n+\t      else\n+\t\tscalar_type = TREE_TYPE (gimple_get_lhs (stmt));\n \t      if (dump_enabled_p ())\n \t\t{\n \t\t  dump_printf_loc (MSG_NOTE, vect_location,"}, {"sha": "99f6b1f1ccb1f444fd1d1ca46605ae7910cdc1ba", "filename": "gcc/tree-vect-stmts.c", "status": "modified", "additions": 439, "deletions": 6, "changes": 445, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-vect-stmts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-vect-stmts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-stmts.c?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -235,7 +235,7 @@ vect_mark_relevant (vec<gimple> *worklist, gimple stmt,\n           /* This use is out of pattern use, if LHS has other uses that are\n              pattern uses, we should mark the stmt itself, and not the pattern\n              stmt.  */\n-\t  if (TREE_CODE (lhs) == SSA_NAME)\n+\t  if (lhs && TREE_CODE (lhs) == SSA_NAME)\n \t    FOR_EACH_IMM_USE_FAST (use_p, imm_iter, lhs)\n \t      {\n \t\tif (is_gimple_debug (USE_STMT (use_p)))\n@@ -393,7 +393,27 @@ exist_non_indexing_operands_for_use_p (tree use, gimple stmt)\n      first case, and whether var corresponds to USE.  */\n \n   if (!gimple_assign_copy_p (stmt))\n-    return false;\n+    {\n+      if (is_gimple_call (stmt)\n+\t  && gimple_call_internal_p (stmt))\n+\tswitch (gimple_call_internal_fn (stmt))\n+\t  {\n+\t  case IFN_MASK_STORE:\n+\t    operand = gimple_call_arg (stmt, 3);\n+\t    if (operand == use)\n+\t      return true;\n+\t    /* FALLTHRU */\n+\t  case IFN_MASK_LOAD:\n+\t    operand = gimple_call_arg (stmt, 2);\n+\t    if (operand == use)\n+\t      return true;\n+\t    break;\n+\t  default:\n+\t    break;\n+\t  }\n+      return false;\n+    }\n+\n   if (TREE_CODE (gimple_assign_lhs (stmt)) == SSA_NAME)\n     return false;\n   operand = gimple_assign_rhs1 (stmt);\n@@ -1696,6 +1716,413 @@ vectorizable_function (gimple call, tree vectype_out, tree vectype_in)\n \t\t\t\t\t\t        vectype_in);\n }\n \n+\n+static tree permute_vec_elements (tree, tree, tree, gimple,\n+\t\t\t\t  gimple_stmt_iterator *);\n+\n+\n+/* Function vectorizable_mask_load_store.\n+\n+   Check if STMT performs a conditional load or store that can be vectorized.\n+   If VEC_STMT is also passed, vectorize the STMT: create a vectorized\n+   stmt to replace it, put it in VEC_STMT, and insert it at GSI.\n+   Return FALSE if not a vectorizable STMT, TRUE otherwise.  */\n+\n+static bool\n+vectorizable_mask_load_store (gimple stmt, gimple_stmt_iterator *gsi,\n+\t\t\t      gimple *vec_stmt, slp_tree slp_node)\n+{\n+  tree vec_dest = NULL;\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  stmt_vec_info prev_stmt_info;\n+  loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  bool nested_in_vect_loop = nested_in_vect_loop_p (loop, stmt);\n+  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n+  tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n+  tree elem_type;\n+  gimple new_stmt;\n+  tree dummy;\n+  tree dataref_ptr = NULL_TREE;\n+  gimple ptr_incr;\n+  int nunits = TYPE_VECTOR_SUBPARTS (vectype);\n+  int ncopies;\n+  int i, j;\n+  bool inv_p;\n+  tree gather_base = NULL_TREE, gather_off = NULL_TREE;\n+  tree gather_off_vectype = NULL_TREE, gather_decl = NULL_TREE;\n+  int gather_scale = 1;\n+  enum vect_def_type gather_dt = vect_unknown_def_type;\n+  bool is_store;\n+  tree mask;\n+  gimple def_stmt;\n+  tree def;\n+  enum vect_def_type dt;\n+\n+  if (slp_node != NULL)\n+    return false;\n+\n+  ncopies = LOOP_VINFO_VECT_FACTOR (loop_vinfo) / nunits;\n+  gcc_assert (ncopies >= 1);\n+\n+  is_store = gimple_call_internal_fn (stmt) == IFN_MASK_STORE;\n+  mask = gimple_call_arg (stmt, 2);\n+  if (TYPE_PRECISION (TREE_TYPE (mask))\n+      != GET_MODE_BITSIZE (TYPE_MODE (TREE_TYPE (vectype))))\n+    return false;\n+\n+  /* FORNOW. This restriction should be relaxed.  */\n+  if (nested_in_vect_loop && ncopies > 1)\n+    {\n+      if (dump_enabled_p ())\n+\tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n+\t\t\t \"multiple types in nested loop.\");\n+      return false;\n+    }\n+\n+  if (!STMT_VINFO_RELEVANT_P (stmt_info))\n+    return false;\n+\n+  if (STMT_VINFO_DEF_TYPE (stmt_info) != vect_internal_def)\n+    return false;\n+\n+  if (!STMT_VINFO_DATA_REF (stmt_info))\n+    return false;\n+\n+  elem_type = TREE_TYPE (vectype);\n+\n+  if (STMT_VINFO_GROUPED_ACCESS (stmt_info))\n+    return false;\n+\n+  if (STMT_VINFO_STRIDE_LOAD_P (stmt_info))\n+    return false;\n+\n+  if (STMT_VINFO_GATHER_P (stmt_info))\n+    {\n+      gimple def_stmt;\n+      tree def;\n+      gather_decl = vect_check_gather (stmt, loop_vinfo, &gather_base,\n+\t\t\t\t       &gather_off, &gather_scale);\n+      gcc_assert (gather_decl);\n+      if (!vect_is_simple_use_1 (gather_off, NULL, loop_vinfo, NULL,\n+\t\t\t\t &def_stmt, &def, &gather_dt,\n+\t\t\t\t &gather_off_vectype))\n+\t{\n+\t  if (dump_enabled_p ())\n+\t    dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n+\t\t\t     \"gather index use not simple.\");\n+\t  return false;\n+\t}\n+    }\n+  else if (tree_int_cst_compare (nested_in_vect_loop\n+\t\t\t\t ? STMT_VINFO_DR_STEP (stmt_info)\n+\t\t\t\t : DR_STEP (dr), size_zero_node) <= 0)\n+    return false;\n+  else if (!VECTOR_MODE_P (TYPE_MODE (vectype))\n+\t   || !can_vec_mask_load_store_p (TYPE_MODE (vectype), !is_store))\n+    return false;\n+\n+  if (TREE_CODE (mask) != SSA_NAME)\n+    return false;\n+\n+  if (!vect_is_simple_use (mask, stmt, loop_vinfo, NULL,\n+\t\t\t   &def_stmt, &def, &dt))\n+    return false;\n+\n+  if (is_store)\n+    {\n+      tree rhs = gimple_call_arg (stmt, 3);\n+      if (!vect_is_simple_use (rhs, stmt, loop_vinfo, NULL,\n+\t\t\t       &def_stmt, &def, &dt))\n+\treturn false;\n+    }\n+\n+  if (!vec_stmt) /* transformation not required.  */\n+    {\n+      STMT_VINFO_TYPE (stmt_info) = call_vec_info_type;\n+      if (is_store)\n+\tvect_model_store_cost (stmt_info, ncopies, false, dt,\n+\t\t\t       NULL, NULL, NULL);\n+      else\n+\tvect_model_load_cost (stmt_info, ncopies, false, NULL, NULL, NULL);\n+      return true;\n+    }\n+\n+  /** Transform.  **/\n+\n+  if (STMT_VINFO_GATHER_P (stmt_info))\n+    {\n+      tree vec_oprnd0 = NULL_TREE, op;\n+      tree arglist = TYPE_ARG_TYPES (TREE_TYPE (gather_decl));\n+      tree rettype, srctype, ptrtype, idxtype, masktype, scaletype;\n+      tree ptr, vec_mask = NULL_TREE, mask_op, var, scale;\n+      tree perm_mask = NULL_TREE, prev_res = NULL_TREE;\n+      edge pe = loop_preheader_edge (loop);\n+      gimple_seq seq;\n+      basic_block new_bb;\n+      enum { NARROW, NONE, WIDEN } modifier;\n+      int gather_off_nunits = TYPE_VECTOR_SUBPARTS (gather_off_vectype);\n+\n+      if (nunits == gather_off_nunits)\n+\tmodifier = NONE;\n+      else if (nunits == gather_off_nunits / 2)\n+\t{\n+\t  unsigned char *sel = XALLOCAVEC (unsigned char, gather_off_nunits);\n+\t  modifier = WIDEN;\n+\n+\t  for (i = 0; i < gather_off_nunits; ++i)\n+\t    sel[i] = i | nunits;\n+\n+\t  perm_mask = vect_gen_perm_mask (gather_off_vectype, sel);\n+\t  gcc_assert (perm_mask != NULL_TREE);\n+\t}\n+      else if (nunits == gather_off_nunits * 2)\n+\t{\n+\t  unsigned char *sel = XALLOCAVEC (unsigned char, nunits);\n+\t  modifier = NARROW;\n+\n+\t  for (i = 0; i < nunits; ++i)\n+\t    sel[i] = i < gather_off_nunits\n+\t\t     ? i : i + nunits - gather_off_nunits;\n+\n+\t  perm_mask = vect_gen_perm_mask (vectype, sel);\n+\t  gcc_assert (perm_mask != NULL_TREE);\n+\t  ncopies *= 2;\n+\t}\n+      else\n+\tgcc_unreachable ();\n+\n+      rettype = TREE_TYPE (TREE_TYPE (gather_decl));\n+      srctype = TREE_VALUE (arglist); arglist = TREE_CHAIN (arglist);\n+      ptrtype = TREE_VALUE (arglist); arglist = TREE_CHAIN (arglist);\n+      idxtype = TREE_VALUE (arglist); arglist = TREE_CHAIN (arglist);\n+      masktype = TREE_VALUE (arglist); arglist = TREE_CHAIN (arglist);\n+      scaletype = TREE_VALUE (arglist);\n+      gcc_checking_assert (types_compatible_p (srctype, rettype)\n+\t\t\t   && types_compatible_p (srctype, masktype));\n+\n+      vec_dest = vect_create_destination_var (gimple_call_lhs (stmt), vectype);\n+\n+      ptr = fold_convert (ptrtype, gather_base);\n+      if (!is_gimple_min_invariant (ptr))\n+\t{\n+\t  ptr = force_gimple_operand (ptr, &seq, true, NULL_TREE);\n+\t  new_bb = gsi_insert_seq_on_edge_immediate (pe, seq);\n+\t  gcc_assert (!new_bb);\n+\t}\n+\n+      scale = build_int_cst (scaletype, gather_scale);\n+\n+      prev_stmt_info = NULL;\n+      for (j = 0; j < ncopies; ++j)\n+\t{\n+\t  if (modifier == WIDEN && (j & 1))\n+\t    op = permute_vec_elements (vec_oprnd0, vec_oprnd0,\n+\t\t\t\t       perm_mask, stmt, gsi);\n+\t  else if (j == 0)\n+\t    op = vec_oprnd0\n+\t      = vect_get_vec_def_for_operand (gather_off, stmt, NULL);\n+\t  else\n+\t    op = vec_oprnd0\n+\t      = vect_get_vec_def_for_stmt_copy (gather_dt, vec_oprnd0);\n+\n+\t  if (!useless_type_conversion_p (idxtype, TREE_TYPE (op)))\n+\t    {\n+\t      gcc_assert (TYPE_VECTOR_SUBPARTS (TREE_TYPE (op))\n+\t\t\t  == TYPE_VECTOR_SUBPARTS (idxtype));\n+\t      var = vect_get_new_vect_var (idxtype, vect_simple_var, NULL);\n+\t      var = make_ssa_name (var, NULL);\n+\t      op = build1 (VIEW_CONVERT_EXPR, idxtype, op);\n+\t      new_stmt\n+\t\t= gimple_build_assign_with_ops (VIEW_CONVERT_EXPR, var,\n+\t\t\t\t\t\top, NULL_TREE);\n+\t      vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t      op = var;\n+\t    }\n+\n+\t  if (j == 0)\n+\t    vec_mask = vect_get_vec_def_for_operand (mask, stmt, NULL);\n+\t  else\n+\t    {\n+\t      vect_is_simple_use (vec_mask, NULL, loop_vinfo, NULL, &def_stmt,\n+\t\t\t\t  &def, &dt);\n+\t      vec_mask = vect_get_vec_def_for_stmt_copy (dt, vec_mask);\n+\t    }\n+\n+\t  mask_op = vec_mask;\n+\t  if (!useless_type_conversion_p (masktype, TREE_TYPE (vec_mask)))\n+\t    {\n+\t      gcc_assert (TYPE_VECTOR_SUBPARTS (TREE_TYPE (mask_op))\n+\t\t\t  == TYPE_VECTOR_SUBPARTS (masktype));\n+\t      var = vect_get_new_vect_var (masktype, vect_simple_var, NULL);\n+\t      var = make_ssa_name (var, NULL);\n+\t      mask_op = build1 (VIEW_CONVERT_EXPR, masktype, mask_op);\n+\t      new_stmt\n+\t\t= gimple_build_assign_with_ops (VIEW_CONVERT_EXPR, var,\n+\t\t\t\t\t\tmask_op, NULL_TREE);\n+\t      vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t      mask_op = var;\n+\t    }\n+\n+\t  new_stmt\n+\t    = gimple_build_call (gather_decl, 5, mask_op, ptr, op, mask_op,\n+\t\t\t\t scale);\n+\n+\t  if (!useless_type_conversion_p (vectype, rettype))\n+\t    {\n+\t      gcc_assert (TYPE_VECTOR_SUBPARTS (vectype)\n+\t\t\t  == TYPE_VECTOR_SUBPARTS (rettype));\n+\t      var = vect_get_new_vect_var (rettype, vect_simple_var, NULL);\n+\t      op = make_ssa_name (var, new_stmt);\n+\t      gimple_call_set_lhs (new_stmt, op);\n+\t      vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t      var = make_ssa_name (vec_dest, NULL);\n+\t      op = build1 (VIEW_CONVERT_EXPR, vectype, op);\n+\t      new_stmt\n+\t\t= gimple_build_assign_with_ops (VIEW_CONVERT_EXPR, var, op,\n+\t\t\t\t\t\tNULL_TREE);\n+\t    }\n+\t  else\n+\t    {\n+\t      var = make_ssa_name (vec_dest, new_stmt);\n+\t      gimple_call_set_lhs (new_stmt, var);\n+\t    }\n+\n+\t  vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\n+\t  if (modifier == NARROW)\n+\t    {\n+\t      if ((j & 1) == 0)\n+\t\t{\n+\t\t  prev_res = var;\n+\t\t  continue;\n+\t\t}\n+\t      var = permute_vec_elements (prev_res, var,\n+\t\t\t\t\t  perm_mask, stmt, gsi);\n+\t      new_stmt = SSA_NAME_DEF_STMT (var);\n+\t    }\n+\n+\t  if (prev_stmt_info == NULL)\n+\t    STMT_VINFO_VEC_STMT (stmt_info) = *vec_stmt = new_stmt;\n+\t  else\n+\t    STMT_VINFO_RELATED_STMT (prev_stmt_info) = new_stmt;\n+\t  prev_stmt_info = vinfo_for_stmt (new_stmt);\n+\t}\n+      return true;\n+    }\n+  else if (is_store)\n+    {\n+      tree vec_rhs = NULL_TREE, vec_mask = NULL_TREE;\n+      prev_stmt_info = NULL;\n+      for (i = 0; i < ncopies; i++)\n+\t{\n+\t  unsigned align, misalign;\n+\n+\t  if (i == 0)\n+\t    {\n+\t      tree rhs = gimple_call_arg (stmt, 3);\n+\t      vec_rhs = vect_get_vec_def_for_operand (rhs, stmt, NULL);\n+\t      vec_mask = vect_get_vec_def_for_operand (mask, stmt, NULL);\n+\t      /* We should have catched mismatched types earlier.  */\n+\t      gcc_assert (useless_type_conversion_p (vectype,\n+\t\t\t\t\t\t     TREE_TYPE (vec_rhs)));\n+\t      dataref_ptr = vect_create_data_ref_ptr (stmt, vectype, NULL,\n+\t\t\t\t\t\t      NULL_TREE, &dummy, gsi,\n+\t\t\t\t\t\t      &ptr_incr, false, &inv_p);\n+\t      gcc_assert (!inv_p);\n+\t    }\n+\t  else\n+\t    {\n+\t      vect_is_simple_use (vec_rhs, NULL, loop_vinfo, NULL, &def_stmt,\n+\t\t\t\t  &def, &dt);\n+\t      vec_rhs = vect_get_vec_def_for_stmt_copy (dt, vec_rhs);\n+\t      vect_is_simple_use (vec_mask, NULL, loop_vinfo, NULL, &def_stmt,\n+\t\t\t\t  &def, &dt);\n+\t      vec_mask = vect_get_vec_def_for_stmt_copy (dt, vec_mask);\n+\t      dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi, stmt,\n+\t\t\t\t\t     TYPE_SIZE_UNIT (vectype));\n+\t    }\n+\n+\t  align = TYPE_ALIGN_UNIT (vectype);\n+\t  if (aligned_access_p (dr))\n+\t    misalign = 0;\n+\t  else if (DR_MISALIGNMENT (dr) == -1)\n+\t    {\n+\t      align = TYPE_ALIGN_UNIT (elem_type);\n+\t      misalign = 0;\n+\t    }\n+\t  else\n+\t    misalign = DR_MISALIGNMENT (dr);\n+\t  set_ptr_info_alignment (get_ptr_info (dataref_ptr), align,\n+\t\t\t\t  misalign);\n+\t  new_stmt\n+\t    = gimple_build_call_internal (IFN_MASK_STORE, 4, dataref_ptr,\n+\t\t\t\t\t  gimple_call_arg (stmt, 1),\n+\t\t\t\t\t  vec_mask, vec_rhs);\n+\t  vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t  if (i == 0)\n+\t    STMT_VINFO_VEC_STMT (stmt_info) = *vec_stmt = new_stmt;\n+\t  else\n+\t    STMT_VINFO_RELATED_STMT (prev_stmt_info) = new_stmt;\n+\t  prev_stmt_info = vinfo_for_stmt (new_stmt);\n+\t}\n+    }\n+  else\n+    {\n+      tree vec_mask = NULL_TREE;\n+      prev_stmt_info = NULL;\n+      vec_dest = vect_create_destination_var (gimple_call_lhs (stmt), vectype);\n+      for (i = 0; i < ncopies; i++)\n+\t{\n+\t  unsigned align, misalign;\n+\n+\t  if (i == 0)\n+\t    {\n+\t      vec_mask = vect_get_vec_def_for_operand (mask, stmt, NULL);\n+\t      dataref_ptr = vect_create_data_ref_ptr (stmt, vectype, NULL,\n+\t\t\t\t\t\t      NULL_TREE, &dummy, gsi,\n+\t\t\t\t\t\t      &ptr_incr, false, &inv_p);\n+\t      gcc_assert (!inv_p);\n+\t    }\n+\t  else\n+\t    {\n+\t      vect_is_simple_use (vec_mask, NULL, loop_vinfo, NULL, &def_stmt,\n+\t\t\t\t  &def, &dt);\n+\t      vec_mask = vect_get_vec_def_for_stmt_copy (dt, vec_mask);\n+\t      dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi, stmt,\n+\t\t\t\t\t     TYPE_SIZE_UNIT (vectype));\n+\t    }\n+\n+\t  align = TYPE_ALIGN_UNIT (vectype);\n+\t  if (aligned_access_p (dr))\n+\t    misalign = 0;\n+\t  else if (DR_MISALIGNMENT (dr) == -1)\n+\t    {\n+\t      align = TYPE_ALIGN_UNIT (elem_type);\n+\t      misalign = 0;\n+\t    }\n+\t  else\n+\t    misalign = DR_MISALIGNMENT (dr);\n+\t  set_ptr_info_alignment (get_ptr_info (dataref_ptr), align,\n+\t\t\t\t  misalign);\n+\t  new_stmt\n+\t    = gimple_build_call_internal (IFN_MASK_LOAD, 3, dataref_ptr,\n+\t\t\t\t\t  gimple_call_arg (stmt, 1),\n+\t\t\t\t\t  vec_mask);\n+\t  gimple_call_set_lhs (new_stmt, make_ssa_name (vec_dest, NULL));\n+\t  vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\t  if (i == 0)\n+\t    STMT_VINFO_VEC_STMT (stmt_info) = *vec_stmt = new_stmt;\n+\t  else\n+\t    STMT_VINFO_RELATED_STMT (prev_stmt_info) = new_stmt;\n+\t  prev_stmt_info = vinfo_for_stmt (new_stmt);\n+\t}\n+    }\n+\n+  return true;\n+}\n+\n+\n /* Function vectorizable_call.\n \n    Check if STMT performs a function call that can be vectorized.\n@@ -1738,6 +2165,12 @@ vectorizable_call (gimple stmt, gimple_stmt_iterator *gsi, gimple *vec_stmt,\n   if (!is_gimple_call (stmt))\n     return false;\n \n+  if (gimple_call_internal_p (stmt)\n+      && (gimple_call_internal_fn (stmt) == IFN_MASK_LOAD\n+\t  || gimple_call_internal_fn (stmt) == IFN_MASK_STORE))\n+    return vectorizable_mask_load_store (stmt, gsi, vec_stmt,\n+\t\t\t\t\t slp_node);\n+\n   if (gimple_call_lhs (stmt) == NULL_TREE\n       || TREE_CODE (gimple_call_lhs (stmt)) != SSA_NAME)\n     return false;\n@@ -4051,10 +4484,6 @@ vectorizable_shift (gimple stmt, gimple_stmt_iterator *gsi,\n }\n \n \n-static tree permute_vec_elements (tree, tree, tree, gimple,\n-\t\t\t\t  gimple_stmt_iterator *);\n-\n-\n /* Function vectorizable_operation.\n \n    Check if STMT performs a binary, unary or ternary operation that can\n@@ -6567,6 +6996,10 @@ vect_transform_stmt (gimple stmt, gimple_stmt_iterator *gsi,\n     case call_vec_info_type:\n       done = vectorizable_call (stmt, gsi, &vec_stmt, slp_node);\n       stmt = gsi_stmt (*gsi);\n+      if (is_gimple_call (stmt)\n+\t  && gimple_call_internal_p (stmt)\n+\t  && gimple_call_internal_fn (stmt) == IFN_MASK_STORE)\n+\tis_store = true;\n       break;\n \n     case call_simd_clone_vec_info_type:"}, {"sha": "1c411c4dec65f64372dc69e130f754c760bd03e0", "filename": "gcc/tree-vectorizer.c", "status": "modified", "additions": 118, "deletions": 3, "changes": 121, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-vectorizer.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-vectorizer.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.c?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -75,11 +75,13 @@ along with GCC; see the file COPYING3.  If not see\n #include \"tree-phinodes.h\"\n #include \"ssa-iterators.h\"\n #include \"tree-ssa-loop-manip.h\"\n+#include \"tree-cfg.h\"\n #include \"cfgloop.h\"\n #include \"tree-vectorizer.h\"\n #include \"tree-pass.h\"\n #include \"tree-ssa-propagate.h\"\n #include \"dbgcnt.h\"\n+#include \"gimple-fold.h\"\n \n /* Loop or bb location.  */\n source_location vect_location;\n@@ -317,6 +319,60 @@ vect_destroy_datarefs (loop_vec_info loop_vinfo, bb_vec_info bb_vinfo)\n }\n \n \n+/* If LOOP has been versioned during ifcvt, return the internal call\n+   guarding it.  */\n+\n+static gimple\n+vect_loop_vectorized_call (struct loop *loop)\n+{\n+  basic_block bb = loop_preheader_edge (loop)->src;\n+  gimple g;\n+  do\n+    {\n+      g = last_stmt (bb);\n+      if (g)\n+\tbreak;\n+      if (!single_pred_p (bb))\n+\tbreak;\n+      bb = single_pred (bb);\n+    }\n+  while (1);\n+  if (g && gimple_code (g) == GIMPLE_COND)\n+    {\n+      gimple_stmt_iterator gsi = gsi_for_stmt (g);\n+      gsi_prev (&gsi);\n+      if (!gsi_end_p (gsi))\n+\t{\n+\t  g = gsi_stmt (gsi);\n+\t  if (is_gimple_call (g)\n+\t      && gimple_call_internal_p (g)\n+\t      && gimple_call_internal_fn (g) == IFN_LOOP_VECTORIZED\n+\t      && (tree_to_shwi (gimple_call_arg (g, 0)) == loop->num\n+\t\t  || tree_to_shwi (gimple_call_arg (g, 1)) == loop->num))\n+\t    return g;\n+\t}\n+    }\n+  return NULL;\n+}\n+\n+/* Fold LOOP_VECTORIZED internal call G to VALUE and\n+   update any immediate uses of it's LHS.  */\n+\n+static void\n+fold_loop_vectorized_call (gimple g, tree value)\n+{\n+  tree lhs = gimple_call_lhs (g);\n+  use_operand_p use_p;\n+  imm_use_iterator iter;\n+  gimple use_stmt;\n+  gimple_stmt_iterator gsi = gsi_for_stmt (g);\n+\n+  update_call_from_tree (&gsi, value);\n+  FOR_EACH_IMM_USE_STMT (use_stmt, iter, lhs)\n+    FOR_EACH_IMM_USE_ON_STMT (use_p, iter)\n+      SET_USE (use_p, value);\n+}\n+\n /* Function vectorize_loops.\n \n    Entry point to loop vectorization phase.  */\n@@ -330,6 +386,8 @@ vectorize_loops (void)\n   struct loop *loop;\n   hash_table <simduid_to_vf> simduid_to_vf_htab;\n   hash_table <simd_array_to_simduid> simd_array_to_simduid_htab;\n+  bool any_ifcvt_loops = false;\n+  unsigned ret = 0;\n \n   vect_loops_num = number_of_loops (cfun);\n \n@@ -352,8 +410,11 @@ vectorize_loops (void)\n      than all previously defined loops.  This fact allows us to run\n      only over initial loops skipping newly generated ones.  */\n   FOR_EACH_LOOP (loop, 0)\n-    if ((flag_tree_loop_vectorize && optimize_loop_nest_for_speed_p (loop))\n-\t|| loop->force_vect)\n+    if (loop->dont_vectorize)\n+      any_ifcvt_loops = true;\n+    else if ((flag_tree_loop_vectorize\n+\t      && optimize_loop_nest_for_speed_p (loop))\n+\t     || loop->force_vect)\n       {\n \tloop_vec_info loop_vinfo;\n \tvect_location = find_loop_location (loop);\n@@ -372,6 +433,39 @@ vectorize_loops (void)\n         if (!dbg_cnt (vect_loop))\n \t  break;\n \n+\tgimple loop_vectorized_call = vect_loop_vectorized_call (loop);\n+\tif (loop_vectorized_call)\n+\t  {\n+\t    tree arg = gimple_call_arg (loop_vectorized_call, 1);\n+\t    basic_block *bbs;\n+\t    unsigned int i;\n+\t    struct loop *scalar_loop = get_loop (cfun, tree_to_shwi (arg));\n+\n+\t    LOOP_VINFO_SCALAR_LOOP (loop_vinfo) = scalar_loop;\n+\t    gcc_checking_assert (vect_loop_vectorized_call\n+\t\t\t\t\t(LOOP_VINFO_SCALAR_LOOP (loop_vinfo))\n+\t\t\t\t == loop_vectorized_call);\n+\t    bbs = get_loop_body (scalar_loop);\n+\t    for (i = 0; i < scalar_loop->num_nodes; i++)\n+\t      {\n+\t\tbasic_block bb = bbs[i];\n+\t\tgimple_stmt_iterator gsi;\n+\t\tfor (gsi = gsi_start_phis (bb); !gsi_end_p (gsi);\n+\t\t     gsi_next (&gsi))\n+\t\t  {\n+\t\t    gimple phi = gsi_stmt (gsi);\n+\t\t    gimple_set_uid (phi, 0);\n+\t\t  }\n+\t\tfor (gsi = gsi_start_bb (bb); !gsi_end_p (gsi);\n+\t\t     gsi_next (&gsi))\n+\t\t  {\n+\t\t    gimple stmt = gsi_stmt (gsi);\n+\t\t    gimple_set_uid (stmt, 0);\n+\t\t  }\n+\t      }\n+\t    free (bbs);\n+\t  }\n+\n         if (LOCATION_LOCUS (vect_location) != UNKNOWN_LOCATION\n \t    && dump_enabled_p ())\n           dump_printf_loc (MSG_OPTIMIZED_LOCATIONS, vect_location,\n@@ -392,6 +486,12 @@ vectorize_loops (void)\n \t    *simduid_to_vf_htab.find_slot (simduid_to_vf_data, INSERT)\n \t      = simduid_to_vf_data;\n \t  }\n+\n+\tif (loop_vectorized_call)\n+\t  {\n+\t    fold_loop_vectorized_call (loop_vectorized_call, boolean_true_node);\n+\t    ret |= TODO_cleanup_cfg;\n+\t  }\n       }\n \n   vect_location = UNKNOWN_LOCATION;\n@@ -405,6 +505,21 @@ vectorize_loops (void)\n \n   /*  ----------- Finalize. -----------  */\n \n+  if (any_ifcvt_loops)\n+    for (i = 1; i < vect_loops_num; i++)\n+      {\n+\tloop = get_loop (cfun, i);\n+\tif (loop && loop->dont_vectorize)\n+\t  {\n+\t    gimple g = vect_loop_vectorized_call (loop);\n+\t    if (g)\n+\t      {\n+\t\tfold_loop_vectorized_call (g, boolean_false_node);\n+\t\tret |= TODO_cleanup_cfg;\n+\t      }\n+\t  }\n+      }\n+\n   for (i = 1; i < vect_loops_num; i++)\n     {\n       loop_vec_info loop_vinfo;\n@@ -462,7 +577,7 @@ vectorize_loops (void)\n       return TODO_cleanup_cfg;\n     }\n \n-  return 0;\n+  return ret;\n }\n \n "}, {"sha": "54e73c8c9a06d69a02f3e0f131695f7cb2236731", "filename": "gcc/tree-vectorizer.h", "status": "modified", "additions": 7, "deletions": 1, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-vectorizer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ce9450f162cef332dbd6534e7c3e246caee70c6/gcc%2Ftree-vectorizer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.h?ref=5ce9450f162cef332dbd6534e7c3e246caee70c6", "patch": "@@ -347,6 +347,10 @@ typedef struct _loop_vec_info {\n      fix it up.  */\n   bool operands_swapped;\n \n+  /* If if-conversion versioned this loop before conversion, this is the\n+     loop version without if-conversion.  */\n+  struct loop *scalar_loop;\n+\n } *loop_vec_info;\n \n /* Access Functions.  */\n@@ -381,6 +385,7 @@ typedef struct _loop_vec_info {\n #define LOOP_VINFO_PEELING_FOR_GAPS(L)     (L)->peeling_for_gaps\n #define LOOP_VINFO_OPERANDS_SWAPPED(L)     (L)->operands_swapped\n #define LOOP_VINFO_PEELING_FOR_NITER(L)    (L)->peeling_for_niter\n+#define LOOP_VINFO_SCALAR_LOOP(L)\t   (L)->scalar_loop\n \n #define LOOP_REQUIRES_VERSIONING_FOR_ALIGNMENT(L) \\\n   (L)->may_misalign_stmts.length () > 0\n@@ -939,7 +944,8 @@ extern source_location vect_location;\n    in tree-vect-loop-manip.c.  */\n extern void slpeel_make_loop_iterate_ntimes (struct loop *, tree);\n extern bool slpeel_can_duplicate_loop_p (const struct loop *, const_edge);\n-struct loop *slpeel_tree_duplicate_loop_to_edge_cfg (struct loop *, edge);\n+struct loop *slpeel_tree_duplicate_loop_to_edge_cfg (struct loop *,\n+\t\t\t\t\t\t     struct loop *, edge);\n extern void vect_loop_versioning (loop_vec_info, unsigned int, bool);\n extern void vect_do_peeling_for_loop_bound (loop_vec_info, tree, tree,\n \t\t\t\t\t    unsigned int, bool);"}]}