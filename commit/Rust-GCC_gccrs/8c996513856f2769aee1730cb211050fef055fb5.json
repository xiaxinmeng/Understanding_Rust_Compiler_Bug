{"sha": "8c996513856f2769aee1730cb211050fef055fb5", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OGM5OTY1MTM4NTZmMjc2OWFlZTE3MzBjYjIxMTA1MGZlZjA1NWZiNQ==", "commit": {"author": {"name": "Jan Hubicka", "email": "jh@suse.cz", "date": "2006-11-27T16:00:26Z"}, "committer": {"name": "Jan Hubicka", "email": "hubicka@gcc.gnu.org", "date": "2006-11-27T16:00:26Z"}, "message": "expr.c (emit_block_move_via_libcall): Export.\n\n\n\t* expr.c (emit_block_move_via_libcall): Export.\n\t(clear_storage_via_libcall): Rename to ...\n\t(set_storage_via_libcall): ... this one; handle arbitrary value to set.\n\t(clear_storage): Update to set_storage_via_libcall.\n\t* expr.h (emit_block_move_via_libcall): Declare\n\t* config/i386/i386.h (stringop_alg): New enum.\n\t(MAX_STRINGOP_ALGS): New constant.\n\t(stringop_algs): New struct.\n\t(processor_costs): Add memcpy/memset descriptors.\n\t(TARGET_REP_MOVEL_OPTIMAL): Remove.\n\t* config/i386/i386.md (movsi_insv_1_rex64): New pattern.\n\t(strmovsi, strmovdi, strsetsi, strsetdi): Update; accept all operands\n\tfor memset.\n\t* config/i386/i386.opt (minline-stringops-dynamically,\n\tmstringop-strategy): New\n\tparameters.\n\t* config/i386/i386-prostos.h (ix86_expand_movmem): Update prototype.\n\t(ix86_expand-clrmem): Rename to ...\n\t(ix86_expand_setmem): ... this one; update prototype.\n\t* config/i386/i386.c (DUMMY_STRINGOP_ALGS): New constant.\n\t(size_cost, i386_cost, i486_cost, pentium_cost, pentiumpro_cost,\n\tgeode_cost, k6_cost, athlon_cost, k8_cost, pentium4_cost,\n\tnocona_cost, core2_cost, generic_cost, generic64_cost,\n\tgeneric32_cost): Add memcpy/memset descriptors.\n\t(x86_rep_movl_optimal): Remove.\n\t(stringop_alg): New static variable.\n\t(ix86_expand_aligntest): Handle predictions.\n\t(override_options): Add strgop_alg handling.\n\t(predict_jump): New function.\n\t(scale_counter): New function.\n\t(expand_set_or_movmem_via_loop): New function.\n\t(expand_movmem_via_rep_mov): New function.\n\t(expand_setmem_via_rep_stots): New function.\n\t(emit_strmov): New function.\n\t(expand_movmem_epilogue): New function.\n\t(expand_setmem_epilogue_via_loop): New function.\n\t(expand_setmem_epilogue): New function.\n\t(expand_movmem_prologue): New function.\n\t(expand_setmem_prologue): New function.\n\t(decide_alg): New function.\n\t(decide_alignment): New function.\n\t(ix86_exand_movmem): Rewrite.\n\t(promote_duplicated_reg): New function.\n\t(ix86_expand_clrmem): Rename to ...\n\t(ix86_expand_setmem): ... this one. Rewrite.\n\n\t* invoke.texi (minline-stringops-dynamically): New command line option.\n\t(mstringop-strategy): Likewise.\n\nFrom-SVN: r119252", "tree": {"sha": "0fffda932600cccb90f66739f0c0ad7403474356", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/0fffda932600cccb90f66739f0c0ad7403474356"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/8c996513856f2769aee1730cb211050fef055fb5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8c996513856f2769aee1730cb211050fef055fb5", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8c996513856f2769aee1730cb211050fef055fb5", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8c996513856f2769aee1730cb211050fef055fb5/comments", "author": null, "committer": null, "parents": [{"sha": "d3d3d8986db75c95a43be00918c9888ca7569d41", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d3d3d8986db75c95a43be00918c9888ca7569d41", "html_url": "https://github.com/Rust-GCC/gccrs/commit/d3d3d8986db75c95a43be00918c9888ca7569d41"}], "stats": {"total": 1880, "additions": 1386, "deletions": 494}, "files": [{"sha": "5e1c96767c15c0a79c88e042c22039a8fce3b27b", "filename": "gcc/ChangeLog", "status": "modified", "additions": 51, "deletions": 0, "changes": 51, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8c996513856f2769aee1730cb211050fef055fb5/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8c996513856f2769aee1730cb211050fef055fb5/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=8c996513856f2769aee1730cb211050fef055fb5", "patch": "@@ -1,3 +1,54 @@\n+2006-11-27  Jan Hubicka  <jh@suse.cz>\n+\n+\t* expr.c (emit_block_move_via_libcall): Export.\n+\t(clear_storage_via_libcall): Rename to ...\n+\t(set_storage_via_libcall): ... this one; handle arbitrary value to set.\n+\t(clear_storage): Update to set_storage_via_libcall.\n+\t* expr.h (emit_block_move_via_libcall): Declare\n+\t* config/i386/i386.h (stringop_alg): New enum.\n+\t(MAX_STRINGOP_ALGS): New constant.\n+\t(stringop_algs): New struct.\n+\t(processor_costs): Add memcpy/memset descriptors.\n+\t(TARGET_REP_MOVEL_OPTIMAL): Remove.\n+\t* config/i386/i386.md (movsi_insv_1_rex64): New pattern.\n+\t(strmovsi, strmovdi, strsetsi, strsetdi): Update; accept all operands\n+\tfor memset.\n+\t* config/i386/i386.opt (minline-stringops-dynamically,\n+\tmstringop-strategy): New\n+\tparameters.\n+\t* config/i386/i386-prostos.h (ix86_expand_movmem): Update prototype.\n+\t(ix86_expand-clrmem): Rename to ...\n+\t(ix86_expand_setmem): ... this one; update prototype.\n+\t* config/i386/i386.c (DUMMY_STRINGOP_ALGS): New constant.\n+\t(size_cost, i386_cost, i486_cost, pentium_cost, pentiumpro_cost,\n+\tgeode_cost, k6_cost, athlon_cost, k8_cost, pentium4_cost,\n+\tnocona_cost, core2_cost, generic_cost, generic64_cost,\n+\tgeneric32_cost): Add memcpy/memset descriptors.\n+\t(x86_rep_movl_optimal): Remove.\n+\t(stringop_alg): New static variable.\n+\t(ix86_expand_aligntest): Handle predictions.\n+\t(override_options): Add strgop_alg handling.\n+\t(predict_jump): New function.\n+\t(scale_counter): New function.\n+\t(expand_set_or_movmem_via_loop): New function.\n+\t(expand_movmem_via_rep_mov): New function.\n+\t(expand_setmem_via_rep_stots): New function.\n+\t(emit_strmov): New function.\n+\t(expand_movmem_epilogue): New function.\n+\t(expand_setmem_epilogue_via_loop): New function.\n+\t(expand_setmem_epilogue): New function.\n+\t(expand_movmem_prologue): New function.\n+\t(expand_setmem_prologue): New function.\n+\t(decide_alg): New function.\n+\t(decide_alignment): New function.\n+\t(ix86_exand_movmem): Rewrite.\n+\t(promote_duplicated_reg): New function.\n+\t(ix86_expand_clrmem): Rename to ...\n+\t(ix86_expand_setmem): ... this one. Rewrite.\n+\n+\t* invoke.texi (minline-stringops-dynamically): New command line option.\n+\t(mstringop-strategy): Likewise.\n+\n 2006-11-27  Jan Hubicka  <jh@suse.cz>\n \n \t* cfgexpand.c (construct_exit_block): Don't disturb end of last BB."}, {"sha": "184b5b21d3e59909334a11f504fcb578be902d14", "filename": "gcc/config/i386/i386-protos.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fconfig%2Fi386%2Fi386-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fconfig%2Fi386%2Fi386-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-protos.h?ref=8c996513856f2769aee1730cb211050fef055fb5", "patch": "@@ -50,8 +50,8 @@ extern bool x86_extended_QIreg_mentioned_p (rtx);\n extern bool x86_extended_reg_mentioned_p (rtx);\n extern enum machine_mode ix86_cc_mode (enum rtx_code, rtx, rtx);\n \n-extern int ix86_expand_movmem (rtx, rtx, rtx, rtx);\n-extern int ix86_expand_clrmem (rtx, rtx, rtx);\n+extern int ix86_expand_movmem (rtx, rtx, rtx, rtx, rtx, rtx);\n+extern int ix86_expand_setmem (rtx, rtx, rtx, rtx, rtx, rtx);\n extern int ix86_expand_strlen (rtx, rtx, rtx, rtx);\n \n extern bool legitimate_constant_p (rtx);"}, {"sha": "067c192bce47e930d958705a9237ff246ce01e3f", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 1230, "deletions": 463, "changes": 1693, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=8c996513856f2769aee1730cb211050fef055fb5", "patch": "@@ -68,6 +68,8 @@ Boston, MA 02110-1301, USA.  */\n /* We assume COSTS_N_INSNS is defined as (N)*4 and an addition is 2 bytes.  */\n #define COSTS_N_BYTES(N) ((N) * 2)\n \n+#define DUMMY_STRINGOP_ALGS {libcall, {{-1, libcall}}}\n+\n static const\n struct processor_costs size_cost = {\t/* costs for tuning for size */\n   COSTS_N_BYTES (2),\t\t\t/* cost of an add instruction */\n@@ -119,6 +121,10 @@ struct processor_costs size_cost = {\t/* costs for tuning for size */\n   COSTS_N_BYTES (2),\t\t\t/* cost of FABS instruction.  */\n   COSTS_N_BYTES (2),\t\t\t/* cost of FCHS instruction.  */\n   COSTS_N_BYTES (2),\t\t\t/* cost of FSQRT instruction.  */\n+  {{rep_prefix_1_byte, {{-1, rep_prefix_1_byte}}},\n+   {rep_prefix_1_byte, {{-1, rep_prefix_1_byte}}}},\n+  {{rep_prefix_1_byte, {{-1, rep_prefix_1_byte}}},\n+   {rep_prefix_1_byte, {{-1, rep_prefix_1_byte}}}}\n };\n \n /* Processor costs (relative to an add) */\n@@ -173,6 +179,10 @@ struct processor_costs i386_cost = {\t/* 386 specific costs */\n   COSTS_N_INSNS (22),\t\t\t/* cost of FABS instruction.  */\n   COSTS_N_INSNS (24),\t\t\t/* cost of FCHS instruction.  */\n   COSTS_N_INSNS (122),\t\t\t/* cost of FSQRT instruction.  */\n+  {{rep_prefix_1_byte, {{-1, rep_prefix_1_byte}}},\n+   DUMMY_STRINGOP_ALGS},\n+  {{rep_prefix_1_byte, {{-1, rep_prefix_1_byte}}},\n+   DUMMY_STRINGOP_ALGS},\n };\n \n static const\n@@ -226,6 +236,10 @@ struct processor_costs i486_cost = {\t/* 486 specific costs */\n   COSTS_N_INSNS (3),\t\t\t/* cost of FABS instruction.  */\n   COSTS_N_INSNS (3),\t\t\t/* cost of FCHS instruction.  */\n   COSTS_N_INSNS (83),\t\t\t/* cost of FSQRT instruction.  */\n+  {{rep_prefix_4_byte, {{-1, rep_prefix_4_byte}}},\n+   DUMMY_STRINGOP_ALGS},\n+  {{rep_prefix_4_byte, {{-1, rep_prefix_4_byte}}},\n+   DUMMY_STRINGOP_ALGS}\n };\n \n static const\n@@ -279,6 +293,10 @@ struct processor_costs pentium_cost = {\n   COSTS_N_INSNS (1),\t\t\t/* cost of FABS instruction.  */\n   COSTS_N_INSNS (1),\t\t\t/* cost of FCHS instruction.  */\n   COSTS_N_INSNS (70),\t\t\t/* cost of FSQRT instruction.  */\n+  {{libcall, {{256, rep_prefix_4_byte}, {-1, libcall}}},\n+   DUMMY_STRINGOP_ALGS},\n+  {{libcall, {{-1, rep_prefix_4_byte}}},\n+   DUMMY_STRINGOP_ALGS}\n };\n \n static const\n@@ -332,6 +350,17 @@ struct processor_costs pentiumpro_cost = {\n   COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n   COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n   COSTS_N_INSNS (56),\t\t\t/* cost of FSQRT instruction.  */\n+  /* PentiumPro has optimized rep instructions for blocks aligned by 8 bytes (we ensure\n+     the alignment).  For small blocks inline loop is still a noticeable win, for bigger\n+     blocks either rep movsl or rep movsb is way to go.  Rep movsb has apparently\n+     more expensive startup time in CPU, but after 4K the difference is down in the noise.\n+   */\n+  {{rep_prefix_4_byte, {{128, loop}, {1024, unrolled_loop},\n+\t\t\t{8192, rep_prefix_4_byte}, {-1, rep_prefix_1_byte}}},\n+   DUMMY_STRINGOP_ALGS},\n+  {{rep_prefix_4_byte, {{1024, unrolled_loop},\n+  \t\t        {8192, rep_prefix_4_byte}, {-1, libcall}}},\n+   DUMMY_STRINGOP_ALGS}\n };\n \n static const\n@@ -386,6 +415,10 @@ struct processor_costs geode_cost = {\n   COSTS_N_INSNS (1),\t\t\t/* cost of FABS instruction.  */\n   COSTS_N_INSNS (1),\t\t\t/* cost of FCHS instruction.  */\n   COSTS_N_INSNS (54),\t\t\t/* cost of FSQRT instruction.  */\n+  {{libcall, {{256, rep_prefix_4_byte}, {-1, libcall}}},\n+   DUMMY_STRINGOP_ALGS},\n+  {{libcall, {{256, rep_prefix_4_byte}, {-1, libcall}}},\n+   DUMMY_STRINGOP_ALGS}\n };\n \n static const\n@@ -439,6 +472,10 @@ struct processor_costs k6_cost = {\n   COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n   COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n   COSTS_N_INSNS (56),\t\t\t/* cost of FSQRT instruction.  */\n+  {{libcall, {{256, rep_prefix_4_byte}, {-1, libcall}}},\n+   DUMMY_STRINGOP_ALGS},\n+  {{libcall, {{256, rep_prefix_4_byte}, {-1, libcall}}},\n+   DUMMY_STRINGOP_ALGS}\n };\n \n static const\n@@ -492,6 +529,13 @@ struct processor_costs athlon_cost = {\n   COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n   COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n   COSTS_N_INSNS (35),\t\t\t/* cost of FSQRT instruction.  */\n+  /* For some reason, Athlon deals better with REP prefix (relative to loops)\n+     comopared to K8. Alignment becomes important after 8 bytes for mempcy and\n+     128 bytes for memset.  */\n+  {{libcall, {{2048, rep_prefix_4_byte}, {-1, libcall}}},\n+   DUMMY_STRINGOP_ALGS},\n+  {{libcall, {{2048, rep_prefix_4_byte}, {-1, libcall}}},\n+   DUMMY_STRINGOP_ALGS}\n };\n \n static const\n@@ -550,6 +594,14 @@ struct processor_costs k8_cost = {\n   COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n   COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n   COSTS_N_INSNS (35),\t\t\t/* cost of FSQRT instruction.  */\n+  /* K8 has optimized REP instruction for medium sized blocks, but for very small\n+     blocks it is better to use loop. For large blocks, libcall can do\n+     nontemporary accesses and beat inline considerably.  */\n+  {{libcall, {{6, loop}, {14, unrolled_loop}, {-1, rep_prefix_4_byte}}},\n+   {libcall, {{16, loop}, {8192, rep_prefix_8_byte}, {-1, libcall}}}},\n+  {{libcall, {{8, loop}, {24, unrolled_loop},\n+\t      {2048, rep_prefix_4_byte}, {-1, libcall}}},\n+   {libcall, {{48, unrolled_loop}, {8192, rep_prefix_8_byte}, {-1, libcall}}}}\n };\n \n static const\n@@ -603,6 +655,10 @@ struct processor_costs pentium4_cost = {\n   COSTS_N_INSNS (2),\t\t\t/* cost of FABS instruction.  */\n   COSTS_N_INSNS (2),\t\t\t/* cost of FCHS instruction.  */\n   COSTS_N_INSNS (43),\t\t\t/* cost of FSQRT instruction.  */\n+  {{libcall, {{256, rep_prefix_4_byte}, {-1, libcall}}},\n+   {libcall, {{256, rep_prefix_4_byte}, {-1, libcall}}}},\n+  {{libcall, {{256, rep_prefix_4_byte}, {-1, libcall}}},\n+   {libcall, {{256, rep_prefix_4_byte}, {-1, libcall}}}}\n };\n \n static const\n@@ -656,6 +712,12 @@ struct processor_costs nocona_cost = {\n   COSTS_N_INSNS (3),\t\t\t/* cost of FABS instruction.  */\n   COSTS_N_INSNS (3),\t\t\t/* cost of FCHS instruction.  */\n   COSTS_N_INSNS (44),\t\t\t/* cost of FSQRT instruction.  */\n+  {{libcall, {{256, rep_prefix_4_byte}, {-1, libcall}}},\n+   {libcall, {{32, loop}, {20000, rep_prefix_8_byte},\n+\t      {100000, unrolled_loop}, {-1, libcall}}}},\n+  {{libcall, {{256, rep_prefix_4_byte}, {-1, libcall}}},\n+   {libcall, {{24, loop}, {64, unrolled_loop},\n+\t      {8192, rep_prefix_8_byte}, {-1, libcall}}}}\n };\n \n static const\n@@ -708,6 +770,13 @@ struct processor_costs core2_cost = {\n   COSTS_N_INSNS (1),\t\t\t/* cost of FABS instruction.  */\n   COSTS_N_INSNS (1),\t\t\t/* cost of FCHS instruction.  */\n   COSTS_N_INSNS (58),\t\t\t/* cost of FSQRT instruction.  */\n+  {{libcall, {{11, loop}, {-1, rep_prefix_4_byte}}},\n+   {libcall, {{32, loop}, {64, rep_prefix_4_byte},\n+\t      {8192, rep_prefix_8_byte}, {-1, libcall}}}},\n+  {{libcall, {{8, loop}, {15, unrolled_loop},\n+\t      {2048, rep_prefix_4_byte}, {-1, libcall}}},\n+   {libcall, {{24, loop}, {32, unrolled_loop},\n+\t      {8192, rep_prefix_8_byte}, {-1, libcall}}}}\n };\n \n /* Generic64 should produce code tuned for Nocona and K8.  */\n@@ -768,6 +837,10 @@ struct processor_costs generic64_cost = {\n   COSTS_N_INSNS (8),\t\t\t/* cost of FABS instruction.  */\n   COSTS_N_INSNS (8),\t\t\t/* cost of FCHS instruction.  */\n   COSTS_N_INSNS (40),\t\t\t/* cost of FSQRT instruction.  */\n+  {DUMMY_STRINGOP_ALGS,\n+   {libcall, {{32, loop}, {8192, rep_prefix_8_byte}, {-1, libcall}}}},\n+  {DUMMY_STRINGOP_ALGS,\n+   {libcall, {{32, loop}, {8192, rep_prefix_8_byte}, {-1, libcall}}}}\n };\n \n /* Generic32 should produce code tuned for Athlon, PPro, Pentium4, Nocona and K8.  */\n@@ -822,6 +895,10 @@ struct processor_costs generic32_cost = {\n   COSTS_N_INSNS (8),\t\t\t/* cost of FABS instruction.  */\n   COSTS_N_INSNS (8),\t\t\t/* cost of FCHS instruction.  */\n   COSTS_N_INSNS (40),\t\t\t/* cost of FSQRT instruction.  */\n+  {{libcall, {{32, loop}, {8192, rep_prefix_4_byte}, {-1, libcall}}},\n+   DUMMY_STRINGOP_ALGS},\n+  {{libcall, {{32, loop}, {8192, rep_prefix_4_byte}, {-1, libcall}}},\n+   DUMMY_STRINGOP_ALGS},\n };\n \n const struct processor_costs *ix86_cost = &pentium_cost;\n@@ -923,7 +1000,6 @@ const int x86_sse_split_regs = m_ATHLON_K8;\n const int x86_sse_typeless_stores = m_ATHLON_K8;\n const int x86_sse_load0_by_pxor = m_PPRO | m_PENT4 | m_NOCONA;\n const int x86_use_ffreep = m_ATHLON_K8;\n-const int x86_rep_movl_optimal = m_386 | m_PENT | m_PPRO | m_K6_GEODE | m_CORE2;\n const int x86_use_incdec = ~(m_PENT4 | m_NOCONA | m_CORE2 | m_GENERIC);\n \n /* ??? Allowing interunit moves makes it all too easy for the compiler to put\n@@ -948,6 +1024,8 @@ const int x86_xadd = ~m_386;\n const int x86_bswap = ~m_386;\n const int x86_pad_returns = m_ATHLON_K8 | m_CORE2 | m_GENERIC;\n \n+static enum stringop_alg stringop_alg = no_stringop;\n+\n /* In case the average insn count for single function invocation is\n    lower than this constant, emit fast (but longer) prologue and\n    epilogue code.  */\n@@ -1212,7 +1290,6 @@ static void ix86_emit_restore_regs_using_mov (rtx, HOST_WIDE_INT, int);\n static void ix86_output_function_epilogue (FILE *, HOST_WIDE_INT);\n static HOST_WIDE_INT ix86_GOT_alias_set (void);\n static void ix86_adjust_counter (rtx, HOST_WIDE_INT);\n-static rtx ix86_expand_aligntest (rtx, int);\n static void ix86_expand_strlensi_unroll_1 (rtx, rtx, rtx);\n static int ix86_issue_rate (void);\n static int ix86_adjust_cost (rtx, rtx, rtx, int);\n@@ -1732,6 +1809,25 @@ override_options (void)\n \t    ix86_tune_string = \"generic32\";\n \t}\n     }\n+  if (ix86_stringop_string)\n+    {\n+      if (!strcmp (ix86_stringop_string, \"rep_byte\"))\n+\tstringop_alg = rep_prefix_1_byte;\n+      else if (!strcmp (ix86_stringop_string, \"libcall\"))\n+\tstringop_alg = libcall;\n+      else if (!strcmp (ix86_stringop_string, \"rep_4byte\"))\n+\tstringop_alg = rep_prefix_4_byte;\n+      else if (!strcmp (ix86_stringop_string, \"rep_8byte\"))\n+\tstringop_alg = rep_prefix_8_byte;\n+      else if (!strcmp (ix86_stringop_string, \"byte_loop\"))\n+\tstringop_alg = loop_1_byte;\n+      else if (!strcmp (ix86_stringop_string, \"loop\"))\n+\tstringop_alg = loop;\n+      else if (!strcmp (ix86_stringop_string, \"unrolled_loop\"))\n+\tstringop_alg = unrolled_loop;\n+      else\n+\terror (\"bad value (%s) for -mstringop-strategy= switch\", ix86_stringop_string);\n+    }\n   if (!strcmp (ix86_tune_string, \"x86-64\"))\n     warning (OPT_Wdeprecated, \"-mtune=x86-64 is deprecated.  Use -mtune=k8 or \"\n \t     \"-mtune=generic instead as appropriate.\");\n@@ -12725,10 +12821,22 @@ ix86_split_lshr (rtx *operands, rtx scratch, enum machine_mode mode)\n     }\n }\n \n+/* Predict just emitted jump instruction to be taken with probability PROB.  */\n+static void\n+predict_jump (int prob)\n+{\n+  rtx insn = get_last_insn ();\n+  gcc_assert (GET_CODE (insn) == JUMP_INSN);\n+  REG_NOTES (insn)\n+    = gen_rtx_EXPR_LIST (REG_BR_PROB,\n+\t\t\t GEN_INT (prob),\n+\t\t\t REG_NOTES (insn));\n+}\n+\n /* Helper function for the string operations below.  Dest VARIABLE whether\n    it is aligned to VALUE bytes.  If true, jump to the label.  */\n static rtx\n-ix86_expand_aligntest (rtx variable, int value)\n+ix86_expand_aligntest (rtx variable, int value, bool epilogue)\n {\n   rtx label = gen_label_rtx ();\n   rtx tmpcount = gen_reg_rtx (GET_MODE (variable));\n@@ -12738,6 +12846,10 @@ ix86_expand_aligntest (rtx variable, int value)\n     emit_insn (gen_andsi3 (tmpcount, variable, GEN_INT (value)));\n   emit_cmp_and_jump_insns (tmpcount, const0_rtx, EQ, 0, GET_MODE (variable),\n \t\t\t   1, label);\n+  if (epilogue)\n+    predict_jump (REG_BR_PROB_BASE * 50 / 100);\n+  else\n+    predict_jump (REG_BR_PROB_BASE * 90 / 100);\n   return label;\n }\n \n@@ -12765,581 +12877,1236 @@ ix86_zero_extend_to_Pmode (rtx exp)\n   return r;\n }\n \n-/* Expand string move (memcpy) operation.  Use i386 string operations when\n-   profitable.  expand_clrmem contains similar code.  */\n-int\n-ix86_expand_movmem (rtx dst, rtx src, rtx count_exp, rtx align_exp)\n+/* Divide COUNTREG by SCALE.  */\n+static rtx\n+scale_counter (rtx countreg, int scale)\n {\n-  rtx srcreg, destreg, countreg, srcexp, destexp;\n-  enum machine_mode counter_mode;\n-  HOST_WIDE_INT align = 0;\n-  unsigned HOST_WIDE_INT count = 0;\n+  rtx sc;\n+  rtx piece_size_mask;\n \n-  if (GET_CODE (align_exp) == CONST_INT)\n-    align = INTVAL (align_exp);\n+  if (scale == 1)\n+    return countreg;\n+  if (GET_CODE (countreg) == CONST_INT)\n+    return GEN_INT (INTVAL (countreg) / scale);\n+  gcc_assert (REG_P (countreg));\n \n-  /* Can't use any of this if the user has appropriated esi or edi.  */\n-  if (global_regs[4] || global_regs[5])\n-    return 0;\n+  piece_size_mask = GEN_INT (scale - 1);\n+  sc = expand_simple_binop (GET_MODE (countreg), LSHIFTRT, countreg,\n+\t\t\t    GEN_INT (exact_log2 (scale)),\n+\t\t\t    NULL, 1, OPTAB_DIRECT);\n+  return sc;\n+}\n \n-  /* This simple hack avoids all inlining code and simplifies code below.  */\n-  if (!TARGET_ALIGN_STRINGOPS)\n-    align = 64;\n+/* When SRCPTR is non-NULL, output simple loop to move memory\n+   pointer to SRCPTR to DESTPTR via chunks of MODE unrolled UNROLL times,\n+   overall size is COUNT specified in bytes.  When SRCPTR is NULL, output the\n+   equivalent loop to set memory by VALUE (supposed to be in MODE).\n \n-  if (GET_CODE (count_exp) == CONST_INT)\n+   The size is rounded down to whole number of chunk size moved at once.\n+   SRCMEM and DESTMEM provide MEMrtx to feed proper aliasing info.  */\n+  \n+\n+static void\n+expand_set_or_movmem_via_loop (rtx destmem, rtx srcmem,\n+\t\t\t       rtx destptr, rtx srcptr, rtx value,\n+\t\t\t       rtx count, enum machine_mode mode, int unroll,\n+\t\t\t       int expected_size)\n+{\n+  rtx out_label, top_label, iter, tmp;\n+  enum machine_mode iter_mode;\n+  rtx piece_size = GEN_INT (GET_MODE_SIZE (mode) * unroll);\n+  rtx piece_size_mask = GEN_INT (~((GET_MODE_SIZE (mode) * unroll) - 1));\n+  rtx size;\n+  rtx x_addr;\n+  rtx y_addr;\n+  int i;\n+\n+  iter_mode = GET_MODE (count);\n+  if (iter_mode == VOIDmode)\n+    iter_mode = word_mode;\n+\n+  top_label = gen_label_rtx ();\n+  out_label = gen_label_rtx ();\n+  iter = gen_reg_rtx (iter_mode);\n+\n+  size = expand_simple_binop (iter_mode, AND, count, piece_size_mask,\n+\t\t\t      NULL, 1, OPTAB_DIRECT);\n+  /* Those two should combine.  */\n+  if (piece_size == const1_rtx)\n     {\n-      count = INTVAL (count_exp);\n-      if (!TARGET_INLINE_ALL_STRINGOPS && count > 64)\n-\treturn 0;\n+      emit_cmp_and_jump_insns (size, const0_rtx, EQ, NULL_RTX, iter_mode,\n+\t\t\t       true, out_label);\n+      predict_jump (REG_BR_PROB_BASE * 10 / 100);\n     }\n+  emit_move_insn (iter, const0_rtx);\n \n-  /* Figure out proper mode for counter.  For 32bits it is always SImode,\n-     for 64bits use SImode when possible, otherwise DImode.\n-     Set count to number of bytes copied when known at compile time.  */\n-  if (!TARGET_64BIT\n-      || GET_MODE (count_exp) == SImode\n-      || x86_64_zext_immediate_operand (count_exp, VOIDmode))\n-    counter_mode = SImode;\n+  emit_label (top_label);\n+\n+  tmp = convert_modes (Pmode, iter_mode, iter, true);\n+  x_addr = gen_rtx_PLUS (Pmode, destptr, tmp);\n+  destmem = change_address (destmem, mode, x_addr);\n+\n+  if (srcmem)\n+    {\n+      y_addr = gen_rtx_PLUS (Pmode, srcptr, copy_rtx (tmp));\n+      srcmem = change_address (srcmem, mode, y_addr);\n+\n+      /* When unrolling for chips that reorder memory reads and writes,\n+\t we can save registers by using single temporary.  \n+\t Also using 4 temporaries is overkill in 32bit mode.  */\n+      if (!TARGET_64BIT && 0)\n+\t{\n+\t  for (i = 0; i < unroll; i++)\n+\t    {\n+\t      if (i)\n+\t\t{\n+\t\t  destmem =\n+\t\t    adjust_address (copy_rtx (destmem), mode, GET_MODE_SIZE (mode));\n+\t\t  srcmem =\n+\t\t    adjust_address (copy_rtx (srcmem), mode, GET_MODE_SIZE (mode));\n+\t\t}\n+\t      emit_move_insn (destmem, srcmem);\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  rtx tmpreg[4];\n+\t  gcc_assert (unroll <= 4);\n+\t  for (i = 0; i < unroll; i++)\n+\t    {\n+\t      tmpreg[i] = gen_reg_rtx (mode);\n+\t      if (i)\n+\t\t{\n+\t\t  srcmem =\n+\t\t    adjust_address (copy_rtx (srcmem), mode, GET_MODE_SIZE (mode));\n+\t\t}\n+\t      emit_move_insn (tmpreg[i], srcmem);\n+\t    }\n+\t  for (i = 0; i < unroll; i++)\n+\t    {\n+\t      if (i)\n+\t\t{\n+\t\t  destmem =\n+\t\t    adjust_address (copy_rtx (destmem), mode, GET_MODE_SIZE (mode));\n+\t\t}\n+\t      emit_move_insn (destmem, tmpreg[i]);\n+\t    }\n+\t}\n+    }\n   else\n-    counter_mode = DImode;\n+    for (i = 0; i < unroll; i++)\n+      {\n+\tif (i)\n+\t  destmem =\n+\t    adjust_address (copy_rtx (destmem), mode, GET_MODE_SIZE (mode));\n+\temit_move_insn (destmem, value);\n+      }\n \n-  gcc_assert (counter_mode == SImode || counter_mode == DImode);\n+  tmp = expand_simple_binop (iter_mode, PLUS, iter, piece_size, iter,\n+\t\t\t     true, OPTAB_LIB_WIDEN);\n+  if (tmp != iter)\n+    emit_move_insn (iter, tmp);\n \n-  destreg = copy_to_mode_reg (Pmode, XEXP (dst, 0));\n-  if (destreg != XEXP (dst, 0))\n-    dst = replace_equiv_address_nv (dst, destreg);\n-  srcreg = copy_to_mode_reg (Pmode, XEXP (src, 0));\n-  if (srcreg != XEXP (src, 0))\n-    src = replace_equiv_address_nv (src, srcreg);\n-\n-  /* When optimizing for size emit simple rep ; movsb instruction for\n-     counts not divisible by 4, except when (movsl;)*(movsw;)?(movsb;)?\n-     sequence is shorter than mov{b,l} $count, %{ecx,cl}; rep; movsb.\n-     Sice of (movsl;)*(movsw;)?(movsb;)? sequence is\n-     count / 4 + (count & 3), the other sequence is either 4 or 7 bytes,\n-     but we don't know whether upper 24 (resp. 56) bits of %ecx will be\n-     known to be zero or not.  The rep; movsb sequence causes higher\n-     register pressure though, so take that into account.  */\n-\n-  if ((!optimize || optimize_size)\n-      && (count == 0\n-\t  || ((count & 0x03)\n-\t      && (!optimize_size\n-\t\t  || count > 5 * 4\n-\t\t  || (count & 3) + count / 4 > 6))))\n+  emit_cmp_and_jump_insns (iter, size, LT, NULL_RTX, iter_mode,\n+\t\t\t   true, top_label);\n+  if (expected_size != -1)\n     {\n-      emit_insn (gen_cld ());\n-      countreg = ix86_zero_extend_to_Pmode (count_exp);\n-      destexp = gen_rtx_PLUS (Pmode, destreg, countreg);\n-      srcexp = gen_rtx_PLUS (Pmode, srcreg, countreg);\n-      emit_insn (gen_rep_mov (destreg, dst, srcreg, src, countreg,\n-\t\t\t      destexp, srcexp));\n+      expected_size /= GET_MODE_SIZE (mode) * unroll;\n+      if (expected_size == 0)\n+\tpredict_jump (0);\n+      else if (expected_size > REG_BR_PROB_BASE)\n+\tpredict_jump (REG_BR_PROB_BASE - 1);\n+      else\n+        predict_jump (REG_BR_PROB_BASE - (REG_BR_PROB_BASE + expected_size / 2) / expected_size);\n+    }\n+  else\n+    predict_jump (REG_BR_PROB_BASE * 80 / 100);\n+  iter = ix86_zero_extend_to_Pmode (iter);\n+  tmp = expand_simple_binop (Pmode, PLUS, destptr, iter, destptr,\n+\t\t\t     true, OPTAB_LIB_WIDEN);\n+  if (tmp != destptr)\n+    emit_move_insn (destptr, tmp);\n+  if (srcptr)\n+    {\n+      tmp = expand_simple_binop (Pmode, PLUS, srcptr, iter, srcptr,\n+\t\t\t\t true, OPTAB_LIB_WIDEN);\n+      if (tmp != srcptr)\n+\temit_move_insn (srcptr, tmp);\n+    }\n+  emit_label (out_label);\n+}\n+\n+/* Output \"rep; mov\" instruction.  \n+   Arguments have same meaning as for previous function */\n+static void\n+expand_movmem_via_rep_mov (rtx destmem, rtx srcmem,\n+\t\t\t   rtx destptr, rtx srcptr,\n+\t\t\t   rtx count,\n+\t\t\t   enum machine_mode mode)\n+{\n+  rtx destexp;\n+  rtx srcexp;\n+  rtx countreg;\n+\n+  /* If the size is known, it is shorter to use rep movs.  */\n+  if (mode == QImode && GET_CODE (count) == CONST_INT\n+      && !(INTVAL (count) & 3))\n+    mode = SImode;\n+\n+  if (destptr != XEXP (destmem, 0) || GET_MODE (destmem) != BLKmode)\n+    destmem = adjust_automodify_address_nv (destmem, BLKmode, destptr, 0);\n+  if (srcptr != XEXP (srcmem, 0) || GET_MODE (srcmem) != BLKmode)\n+    srcmem = adjust_automodify_address_nv (srcmem, BLKmode, srcptr, 0);\n+  countreg = ix86_zero_extend_to_Pmode (scale_counter (count, GET_MODE_SIZE (mode)));\n+  if (mode != QImode)\n+    {\n+      destexp = gen_rtx_ASHIFT (Pmode, countreg,\n+\t\t\t\tGEN_INT (exact_log2 (GET_MODE_SIZE (mode))));\n+      destexp = gen_rtx_PLUS (Pmode, destexp, destptr);\n+      srcexp = gen_rtx_ASHIFT (Pmode, countreg,\n+\t\t\t       GEN_INT (exact_log2 (GET_MODE_SIZE (mode))));\n+      srcexp = gen_rtx_PLUS (Pmode, srcexp, srcptr);\n+    }\n+  else\n+    {\n+      destexp = gen_rtx_PLUS (Pmode, destptr, countreg);\n+      srcexp = gen_rtx_PLUS (Pmode, srcptr, countreg);\n     }\n+  emit_insn (gen_rep_mov (destptr, destmem, srcptr, srcmem, countreg,\n+\t\t\t  destexp, srcexp));\n+}\n \n-  /* For constant aligned (or small unaligned) copies use rep movsl\n-     followed by code copying the rest.  For PentiumPro ensure 8 byte\n-     alignment to allow rep movsl acceleration.  */\n+/* Output \"rep; stos\" instruction.  \n+   Arguments have same meaning as for previous function */\n+static void\n+expand_setmem_via_rep_stos (rtx destmem, rtx destptr, rtx value,\n+\t\t\t    rtx count,\n+\t\t\t    enum machine_mode mode)\n+{\n+  rtx destexp;\n+  rtx countreg;\n \n-  else if (count != 0\n-\t   && (align >= 8\n-\t       || (!TARGET_PENTIUMPRO && !TARGET_64BIT && align >= 4)\n-\t       || optimize_size || count < (unsigned int) 64))\n+  if (destptr != XEXP (destmem, 0) || GET_MODE (destmem) != BLKmode)\n+    destmem = adjust_automodify_address_nv (destmem, BLKmode, destptr, 0);\n+  value = force_reg (mode, gen_lowpart (mode, value));\n+  countreg = ix86_zero_extend_to_Pmode (scale_counter (count, GET_MODE_SIZE (mode)));\n+  if (mode != QImode)\n     {\n-      unsigned HOST_WIDE_INT offset = 0;\n-      int size = TARGET_64BIT && !optimize_size ? 8 : 4;\n-      rtx srcmem, dstmem;\n+      destexp = gen_rtx_ASHIFT (Pmode, countreg,\n+\t\t\t\tGEN_INT (exact_log2 (GET_MODE_SIZE (mode))));\n+      destexp = gen_rtx_PLUS (Pmode, destexp, destptr);\n+    }\n+  else\n+    destexp = gen_rtx_PLUS (Pmode, destptr, countreg);\n+  emit_insn (gen_rep_stos (destptr, countreg, destmem, value, destexp));\n+}\n \n-      emit_insn (gen_cld ());\n-      if (count & ~(size - 1))\n+static void\n+emit_strmov (rtx destmem, rtx srcmem,\n+\t     rtx destptr, rtx srcptr, enum machine_mode mode, int offset)\n+{\n+  rtx src = adjust_automodify_address_nv (srcmem, mode, srcptr, offset);\n+  rtx dest = adjust_automodify_address_nv (destmem, mode, destptr, offset);\n+  emit_insn (gen_strmov (destptr, dest, srcptr, src));\n+}\n+\n+/* Output code to copy at most count & (max_size - 1) bytes from SRC to DEST.  */\n+static void\n+expand_movmem_epilogue (rtx destmem, rtx srcmem,\n+\t\t\trtx destptr, rtx srcptr, rtx count, int max_size)\n+{\n+  rtx src, dest;\n+  if (GET_CODE (count) == CONST_INT)\n+    {\n+      HOST_WIDE_INT countval = INTVAL (count);\n+      int offset = 0;\n+\n+      if ((countval & 0x16) && max_size > 16)\n \t{\n-\t  if ((TARGET_SINGLE_STRINGOP || optimize_size) && count < 5 * 4)\n+\t  if (TARGET_64BIT)\n \t    {\n-\t      enum machine_mode movs_mode = size == 4 ? SImode : DImode;\n-\n-\t      while (offset < (count & ~(size - 1)))\n-\t\t{\n-\t\t  srcmem = adjust_automodify_address_nv (src, movs_mode,\n-\t\t\t\t\t\t\t srcreg, offset);\n-\t\t  dstmem = adjust_automodify_address_nv (dst, movs_mode,\n-\t\t\t\t\t\t\t destreg, offset);\n-\t\t  emit_insn (gen_strmov (destreg, dstmem, srcreg, srcmem));\n-\t\t  offset += size;\n-\t\t}\n+\t      emit_strmov (destmem, srcmem, destptr, srcptr, DImode, offset);\n+\t      emit_strmov (destmem, srcmem, destptr, srcptr, DImode, offset + 8);\n \t    }\n+\t  else\n+\t    gcc_unreachable ();\n+\t  offset += 16;\n+\t}\n+      if ((countval & 0x08) && max_size > 8)\n+\t{\n+\t  if (TARGET_64BIT)\n+\t    emit_strmov (destmem, srcmem, destptr, srcptr, DImode, offset);\n \t  else\n \t    {\n-\t      countreg = GEN_INT ((count >> (size == 4 ? 2 : 3))\n-\t\t\t\t  & (TARGET_64BIT ? -1 : 0x3fffffff));\n-\t      countreg = copy_to_mode_reg (counter_mode, countreg);\n-\t      countreg = ix86_zero_extend_to_Pmode (countreg);\n-\n-\t      destexp = gen_rtx_ASHIFT (Pmode, countreg,\n-\t\t\t\t\tGEN_INT (size == 4 ? 2 : 3));\n-\t      srcexp = gen_rtx_PLUS (Pmode, destexp, srcreg);\n-\t      destexp = gen_rtx_PLUS (Pmode, destexp, destreg);\n-\n-\t      emit_insn (gen_rep_mov (destreg, dst, srcreg, src,\n-\t\t\t\t      countreg, destexp, srcexp));\n-\t      offset = count & ~(size - 1);\n+\t      emit_strmov (destmem, srcmem, destptr, srcptr, DImode, offset);\n+\t      emit_strmov (destmem, srcmem, destptr, srcptr, DImode, offset + 4);\n \t    }\n+\t  offset += 8;\n \t}\n-      if (size == 8 && (count & 0x04))\n+      if ((countval & 0x04) && max_size > 4)\n \t{\n-\t  srcmem = adjust_automodify_address_nv (src, SImode, srcreg,\n-\t\t\t\t\t\t offset);\n-\t  dstmem = adjust_automodify_address_nv (dst, SImode, destreg,\n-\t\t\t\t\t\t offset);\n-\t  emit_insn (gen_strmov (destreg, dstmem, srcreg, srcmem));\n+          emit_strmov (destmem, srcmem, destptr, srcptr, SImode, offset);\n \t  offset += 4;\n \t}\n-      if (count & 0x02)\n+      if ((countval & 0x02) && max_size > 2)\n \t{\n-\t  srcmem = adjust_automodify_address_nv (src, HImode, srcreg,\n-\t\t\t\t\t\t offset);\n-\t  dstmem = adjust_automodify_address_nv (dst, HImode, destreg,\n-\t\t\t\t\t\t offset);\n-\t  emit_insn (gen_strmov (destreg, dstmem, srcreg, srcmem));\n+          emit_strmov (destmem, srcmem, destptr, srcptr, HImode, offset);\n \t  offset += 2;\n \t}\n-      if (count & 0x01)\n+      if ((countval & 0x01) && max_size > 1)\n \t{\n-\t  srcmem = adjust_automodify_address_nv (src, QImode, srcreg,\n-\t\t\t\t\t\t offset);\n-\t  dstmem = adjust_automodify_address_nv (dst, QImode, destreg,\n-\t\t\t\t\t\t offset);\n-\t  emit_insn (gen_strmov (destreg, dstmem, srcreg, srcmem));\n+          emit_strmov (destmem, srcmem, destptr, srcptr, QImode, offset);\n+\t  offset += 1;\n \t}\n+      return;\n     }\n-  /* The generic code based on the glibc implementation:\n-     - align destination to 4 bytes (8 byte alignment is used for PentiumPro\n-     allowing accelerated copying there)\n-     - copy the data using rep movsl\n-     - copy the rest.  */\n-  else\n+  if (max_size > 8)\n     {\n-      rtx countreg2;\n-      rtx label = NULL;\n-      rtx srcmem, dstmem;\n-      int desired_alignment = (TARGET_PENTIUMPRO\n-\t\t\t       && (count == 0 || count >= (unsigned int) 260)\n-\t\t\t       ? 8 : UNITS_PER_WORD);\n-      /* Get rid of MEM_OFFSETs, they won't be accurate.  */\n-      dst = change_address (dst, BLKmode, destreg);\n-      src = change_address (src, BLKmode, srcreg);\n-\n-      /* In case we don't know anything about the alignment, default to\n-         library version, since it is usually equally fast and result in\n-         shorter code.\n-\n-\t Also emit call when we know that the count is large and call overhead\n-\t will not be important.  */\n-      if (!TARGET_INLINE_ALL_STRINGOPS\n-\t  && (align < UNITS_PER_WORD || !TARGET_REP_MOVL_OPTIMAL))\n-\treturn 0;\n-\n-      if (TARGET_SINGLE_STRINGOP)\n-\temit_insn (gen_cld ());\n-\n-      countreg2 = gen_reg_rtx (Pmode);\n-      countreg = copy_to_mode_reg (counter_mode, count_exp);\n-\n-      /* We don't use loops to align destination and to copy parts smaller\n-         than 4 bytes, because gcc is able to optimize such code better (in\n-         the case the destination or the count really is aligned, gcc is often\n-         able to predict the branches) and also it is friendlier to the\n-         hardware branch prediction.\n-\n-         Using loops is beneficial for generic case, because we can\n-         handle small counts using the loops.  Many CPUs (such as Athlon)\n-         have large REP prefix setup costs.\n-\n-         This is quite costly.  Maybe we can revisit this decision later or\n-         add some customizability to this code.  */\n+      count = expand_simple_binop (GET_MODE (count), AND, count, GEN_INT (max_size - 1),\n+\t\t\t\t    count, 1, OPTAB_DIRECT);\n+      expand_set_or_movmem_via_loop (destmem, srcmem, destptr, srcptr, NULL,\n+\t\t\t\t     count, QImode, 1, 4);\n+      return;\n+    }\n \n-      if (count == 0 && align < desired_alignment)\n+  /* When there are stringops, we can cheaply increase dest and src pointers.\n+     Otherwise we save code size by maintaining offset (zero is readily\n+     available from preceeding rep operation) and using x86 addressing modes.\n+   */\n+  if (TARGET_SINGLE_STRINGOP)\n+    {\n+      if (max_size > 4)\n \t{\n-\t  label = gen_label_rtx ();\n-\t  emit_cmp_and_jump_insns (countreg, GEN_INT (desired_alignment - 1),\n-\t\t\t\t   LEU, 0, counter_mode, 1, label);\n+\t  rtx label = ix86_expand_aligntest (count, 4, true);\n+\t  src = change_address (srcmem, SImode, srcptr);\n+\t  dest = change_address (destmem, SImode, destptr);\n+\t  emit_insn (gen_strmov (destptr, dest, srcptr, src));\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n \t}\n-      if (align <= 1)\n+      if (max_size > 2)\n \t{\n-\t  rtx label = ix86_expand_aligntest (destreg, 1);\n-\t  srcmem = change_address (src, QImode, srcreg);\n-\t  dstmem = change_address (dst, QImode, destreg);\n-\t  emit_insn (gen_strmov (destreg, dstmem, srcreg, srcmem));\n-\t  ix86_adjust_counter (countreg, 1);\n+\t  rtx label = ix86_expand_aligntest (count, 2, true);\n+\t  src = change_address (srcmem, HImode, srcptr);\n+\t  dest = change_address (destmem, HImode, destptr);\n+\t  emit_insn (gen_strmov (destptr, dest, srcptr, src));\n \t  emit_label (label);\n \t  LABEL_NUSES (label) = 1;\n \t}\n-      if (align <= 2)\n+      if (max_size > 1)\n \t{\n-\t  rtx label = ix86_expand_aligntest (destreg, 2);\n-\t  srcmem = change_address (src, HImode, srcreg);\n-\t  dstmem = change_address (dst, HImode, destreg);\n-\t  emit_insn (gen_strmov (destreg, dstmem, srcreg, srcmem));\n-\t  ix86_adjust_counter (countreg, 2);\n+\t  rtx label = ix86_expand_aligntest (count, 1, true);\n+\t  src = change_address (srcmem, QImode, srcptr);\n+\t  dest = change_address (destmem, QImode, destptr);\n+\t  emit_insn (gen_strmov (destptr, dest, srcptr, src));\n \t  emit_label (label);\n \t  LABEL_NUSES (label) = 1;\n \t}\n-      if (align <= 4 && desired_alignment > 4)\n+    }\n+  else\n+    {\n+      rtx offset = force_reg (Pmode, const0_rtx);\n+      rtx tmp;\n+\n+      if (max_size > 4)\n \t{\n-\t  rtx label = ix86_expand_aligntest (destreg, 4);\n-\t  srcmem = change_address (src, SImode, srcreg);\n-\t  dstmem = change_address (dst, SImode, destreg);\n-\t  emit_insn (gen_strmov (destreg, dstmem, srcreg, srcmem));\n-\t  ix86_adjust_counter (countreg, 4);\n+\t  rtx label = ix86_expand_aligntest (count, 4, true);\n+\t  src = change_address (srcmem, SImode, srcptr);\n+\t  dest = change_address (destmem, SImode, destptr);\n+\t  emit_move_insn (dest, src);\n+\t  tmp = expand_simple_binop (Pmode, PLUS, offset, GEN_INT (4), NULL,\n+\t\t\t\t     true, OPTAB_LIB_WIDEN);\n+\t  if (tmp != offset)\n+\t    emit_move_insn (offset, tmp);\n \t  emit_label (label);\n \t  LABEL_NUSES (label) = 1;\n \t}\n-\n-      if (label && desired_alignment > 4 && !TARGET_64BIT)\n+      if (max_size > 2)\n+\t{\n+\t  rtx label = ix86_expand_aligntest (count, 2, true);\n+\t  tmp = gen_rtx_PLUS (Pmode, srcptr, offset);\n+\t  src = change_address (srcmem, HImode, tmp);\n+\t  tmp = gen_rtx_PLUS (Pmode, destptr, offset);\n+\t  dest = change_address (destmem, HImode, tmp);\n+\t  emit_move_insn (dest, src);\n+\t  tmp = expand_simple_binop (Pmode, PLUS, offset, GEN_INT (2), tmp,\n+\t\t\t\t     true, OPTAB_LIB_WIDEN);\n+\t  if (tmp != offset)\n+\t    emit_move_insn (offset, tmp);\n+\t  emit_label (label);\n+\t  LABEL_NUSES (label) = 1;\n+\t}\n+      if (max_size > 1)\n \t{\n+\t  rtx label = ix86_expand_aligntest (count, 1, true);\n+\t  tmp = gen_rtx_PLUS (Pmode, srcptr, offset);\n+\t  src = change_address (srcmem, QImode, tmp);\n+\t  tmp = gen_rtx_PLUS (Pmode, destptr, offset);\n+\t  dest = change_address (destmem, QImode, tmp);\n+\t  emit_move_insn (dest, src);\n \t  emit_label (label);\n \t  LABEL_NUSES (label) = 1;\n-\t  label = NULL_RTX;\n \t}\n-      if (!TARGET_SINGLE_STRINGOP)\n-\temit_insn (gen_cld ());\n-      if (TARGET_64BIT)\n+    }\n+}\n+\n+/* Output code to set at most count & (max_size - 1) bytes starting by DEST.  */\n+static void\n+expand_setmem_epilogue_via_loop (rtx destmem, rtx destptr, rtx value,\n+\t\t\t\t rtx count, int max_size)\n+{\n+  count =\n+    expand_simple_binop (GET_MODE (count), AND, count, GEN_INT (max_size - 1),\n+\t\t\t count, 1, OPTAB_DIRECT);\n+  expand_set_or_movmem_via_loop (destmem, NULL, destptr, NULL,\n+\t\t\t\t gen_lowpart (QImode, value), count, QImode,\n+\t\t\t\t 1, max_size / 2);\n+}\n+\n+/* Output code to set at most count & (max_size - 1) bytes starting by DEST.  */\n+static void\n+expand_setmem_epilogue (rtx destmem, rtx destptr, rtx value, rtx count, int max_size)\n+{\n+  rtx dest;\n+  if (GET_CODE (count) == CONST_INT)\n+    {\n+      HOST_WIDE_INT countval = INTVAL (count);\n+      int offset = 0;\n+\n+      if ((countval & 0x16) && max_size > 16)\n \t{\n-\t  emit_insn (gen_lshrdi3 (countreg2, ix86_zero_extend_to_Pmode (countreg),\n-\t\t\t\t  GEN_INT (3)));\n-\t  destexp = gen_rtx_ASHIFT (Pmode, countreg2, GEN_INT (3));\n+\t  if (TARGET_64BIT)\n+\t    {\n+\t      dest = adjust_automodify_address_nv (destmem, DImode, destptr, offset);\n+\t      emit_insn (gen_strset (destptr, dest, value));\n+\t      dest = adjust_automodify_address_nv (destmem, DImode, destptr, offset + 8);\n+\t      emit_insn (gen_strset (destptr, dest, value));\n+\t    }\n+\t  else\n+\t    gcc_unreachable ();\n+\t  offset += 16;\n \t}\n-      else\n+      if ((countval & 0x08) && max_size > 8)\n \t{\n-\t  emit_insn (gen_lshrsi3 (countreg2, countreg, const2_rtx));\n-\t  destexp = gen_rtx_ASHIFT (Pmode, countreg2, const2_rtx);\n+\t  if (TARGET_64BIT)\n+\t    {\n+\t      dest = adjust_automodify_address_nv (destmem, DImode, destptr, offset);\n+\t      emit_insn (gen_strset (destptr, dest, value));\n+\t    }\n+\t  else\n+\t    {\n+\t      dest = adjust_automodify_address_nv (destmem, SImode, destptr, offset);\n+\t      emit_insn (gen_strset (destptr, dest, value));\n+\t      dest = adjust_automodify_address_nv (destmem, SImode, destptr, offset + 4);\n+\t      emit_insn (gen_strset (destptr, dest, value));\n+\t    }\n+\t  offset += 8;\n \t}\n-      srcexp = gen_rtx_PLUS (Pmode, destexp, srcreg);\n-      destexp = gen_rtx_PLUS (Pmode, destexp, destreg);\n-      emit_insn (gen_rep_mov (destreg, dst, srcreg, src,\n-\t\t\t      countreg2, destexp, srcexp));\n-\n-      if (label)\n+      if ((countval & 0x04) && max_size > 4)\n \t{\n-\t  emit_label (label);\n-\t  LABEL_NUSES (label) = 1;\n+\t  dest = adjust_automodify_address_nv (destmem, SImode, destptr, offset);\n+\t  emit_insn (gen_strset (destptr, dest, gen_lowpart (SImode, value)));\n+\t  offset += 4;\n \t}\n-      if (TARGET_64BIT && align > 4 && count != 0 && (count & 4))\n+      if ((countval & 0x02) && max_size > 2)\n \t{\n-\t  srcmem = change_address (src, SImode, srcreg);\n-\t  dstmem = change_address (dst, SImode, destreg);\n-\t  emit_insn (gen_strmov (destreg, dstmem, srcreg, srcmem));\n+\t  dest = adjust_automodify_address_nv (destmem, HImode, destptr, offset);\n+\t  emit_insn (gen_strset (destptr, dest, gen_lowpart (HImode, value)));\n+\t  offset += 2;\n \t}\n-      if ((align <= 4 || count == 0) && TARGET_64BIT)\n+      if ((countval & 0x01) && max_size > 1)\n \t{\n-\t  rtx label = ix86_expand_aligntest (countreg, 4);\n-\t  srcmem = change_address (src, SImode, srcreg);\n-\t  dstmem = change_address (dst, SImode, destreg);\n-\t  emit_insn (gen_strmov (destreg, dstmem, srcreg, srcmem));\n-\t  emit_label (label);\n-\t  LABEL_NUSES (label) = 1;\n+\t  dest = adjust_automodify_address_nv (destmem, QImode, destptr, offset);\n+\t  emit_insn (gen_strset (destptr, dest, gen_lowpart (QImode, value)));\n+\t  offset += 1;\n \t}\n-      if (align > 2 && count != 0 && (count & 2))\n+      return;\n+    }\n+  if (max_size > 32)\n+    {\n+      expand_setmem_epilogue_via_loop (destmem, destptr, value, count, max_size);\n+      return;\n+    }\n+  if (max_size > 16)\n+    {\n+      rtx label = ix86_expand_aligntest (count, 16, true);\n+      if (TARGET_64BIT)\n \t{\n-\t  srcmem = change_address (src, HImode, srcreg);\n-\t  dstmem = change_address (dst, HImode, destreg);\n-\t  emit_insn (gen_strmov (destreg, dstmem, srcreg, srcmem));\n+\t  dest = change_address (destmem, DImode, destptr);\n+\t  emit_insn (gen_strset (destptr, dest, value));\n+\t  emit_insn (gen_strset (destptr, dest, value));\n \t}\n-      if (align <= 2 || count == 0)\n+      else\n \t{\n-\t  rtx label = ix86_expand_aligntest (countreg, 2);\n-\t  srcmem = change_address (src, HImode, srcreg);\n-\t  dstmem = change_address (dst, HImode, destreg);\n-\t  emit_insn (gen_strmov (destreg, dstmem, srcreg, srcmem));\n-\t  emit_label (label);\n-\t  LABEL_NUSES (label) = 1;\n+\t  dest = change_address (destmem, SImode, destptr);\n+\t  emit_insn (gen_strset (destptr, dest, value));\n+\t  emit_insn (gen_strset (destptr, dest, value));\n+\t  emit_insn (gen_strset (destptr, dest, value));\n+\t  emit_insn (gen_strset (destptr, dest, value));\n \t}\n-      if (align > 1 && count != 0 && (count & 1))\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n+    }\n+  if (max_size > 8)\n+    {\n+      rtx label = ix86_expand_aligntest (count, 8, true);\n+      if (TARGET_64BIT)\n \t{\n-\t  srcmem = change_address (src, QImode, srcreg);\n-\t  dstmem = change_address (dst, QImode, destreg);\n-\t  emit_insn (gen_strmov (destreg, dstmem, srcreg, srcmem));\n+\t  dest = change_address (destmem, DImode, destptr);\n+\t  emit_insn (gen_strset (destptr, dest, value));\n \t}\n-      if (align <= 1 || count == 0)\n+      else\n \t{\n-\t  rtx label = ix86_expand_aligntest (countreg, 1);\n-\t  srcmem = change_address (src, QImode, srcreg);\n-\t  dstmem = change_address (dst, QImode, destreg);\n-\t  emit_insn (gen_strmov (destreg, dstmem, srcreg, srcmem));\n-\t  emit_label (label);\n-\t  LABEL_NUSES (label) = 1;\n+\t  dest = change_address (destmem, SImode, destptr);\n+\t  emit_insn (gen_strset (destptr, dest, value));\n+\t  emit_insn (gen_strset (destptr, dest, value));\n \t}\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n+    }\n+  if (max_size > 4)\n+    {\n+      rtx label = ix86_expand_aligntest (count, 4, true);\n+      dest = change_address (destmem, SImode, destptr);\n+      emit_insn (gen_strset (destptr, dest, gen_lowpart (SImode, value)));\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n     }\n+  if (max_size > 2)\n+    {\n+      rtx label = ix86_expand_aligntest (count, 2, true);\n+      dest = change_address (destmem, HImode, destptr);\n+      emit_insn (gen_strset (destptr, dest, gen_lowpart (HImode, value)));\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n+    }\n+  if (max_size > 1)\n+    {\n+      rtx label = ix86_expand_aligntest (count, 1, true);\n+      dest = change_address (destmem, QImode, destptr);\n+      emit_insn (gen_strset (destptr, dest, gen_lowpart (QImode, value)));\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n+    }\n+}\n \n-  return 1;\n+/* Copy enough from DEST to SRC to align DEST known to by aligned by ALIGN to\n+   DESIRED_ALIGNMENT.  */\n+static void\n+expand_movmem_prologue (rtx destmem, rtx srcmem,\n+\t\t\trtx destptr, rtx srcptr, rtx count,\n+\t\t\tint align, int desired_alignment)\n+{\n+  if (align <= 1 && desired_alignment > 1)\n+    {\n+      rtx label = ix86_expand_aligntest (destptr, 1, false);\n+      srcmem = change_address (srcmem, QImode, srcptr);\n+      destmem = change_address (destmem, QImode, destptr);\n+      emit_insn (gen_strmov (destptr, destmem, srcptr, srcmem));\n+      ix86_adjust_counter (count, 1);\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n+    }\n+  if (align <= 2 && desired_alignment > 2)\n+    {\n+      rtx label = ix86_expand_aligntest (destptr, 2, false);\n+      srcmem = change_address (srcmem, HImode, srcptr);\n+      destmem = change_address (destmem, HImode, destptr);\n+      emit_insn (gen_strmov (destptr, destmem, srcptr, srcmem));\n+      ix86_adjust_counter (count, 2);\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n+    }\n+  if (align <= 4 && desired_alignment > 4)\n+    {\n+      rtx label = ix86_expand_aligntest (destptr, 4, false);\n+      srcmem = change_address (srcmem, SImode, srcptr);\n+      destmem = change_address (destmem, SImode, destptr);\n+      emit_insn (gen_strmov (destptr, destmem, srcptr, srcmem));\n+      ix86_adjust_counter (count, 4);\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n+    }\n+  gcc_assert (desired_alignment <= 8);\n }\n \n-/* Expand string clear operation (bzero).  Use i386 string operations when\n-   profitable.  expand_movmem contains similar code.  */\n+/* Set enough from DEST to align DEST known to by aligned by ALIGN to\n+   DESIRED_ALIGNMENT.  */\n+static void\n+expand_setmem_prologue (rtx destmem, rtx destptr, rtx value, rtx count,\n+\t\t\tint align, int desired_alignment)\n+{\n+  if (align <= 1 && desired_alignment > 1)\n+    {\n+      rtx label = ix86_expand_aligntest (destptr, 1, false);\n+      destmem = change_address (destmem, QImode, destptr);\n+      emit_insn (gen_strset (destptr, destmem, gen_lowpart (QImode, value)));\n+      ix86_adjust_counter (count, 1);\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n+    }\n+  if (align <= 2 && desired_alignment > 2)\n+    {\n+      rtx label = ix86_expand_aligntest (destptr, 2, false);\n+      destmem = change_address (destmem, HImode, destptr);\n+      emit_insn (gen_strset (destptr, destmem, gen_lowpart (HImode, value)));\n+      ix86_adjust_counter (count, 2);\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n+    }\n+  if (align <= 4 && desired_alignment > 4)\n+    {\n+      rtx label = ix86_expand_aligntest (destptr, 4, false);\n+      destmem = change_address (destmem, SImode, destptr);\n+      emit_insn (gen_strset (destptr, destmem, gen_lowpart (SImode, value)));\n+      ix86_adjust_counter (count, 4);\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n+    }\n+  gcc_assert (desired_alignment <= 8);\n+}\n+\n+/* Given COUNT and EXPECTED_SIZE, decide on codegen of string operation.  */\n+static enum stringop_alg\n+decide_alg (HOST_WIDE_INT count, HOST_WIDE_INT expected_size, bool memset,\n+\t    int *dynamic_check)\n+{\n+  const struct stringop_algs * algs;\n+\n+  *dynamic_check = -1;\n+  if (memset)\n+    algs = &ix86_cost->memset[TARGET_64BIT != 0];\n+  else\n+    algs = &ix86_cost->memcpy[TARGET_64BIT != 0];\n+  if (stringop_alg != no_stringop)\n+    return stringop_alg;\n+  /* rep; movq or rep; movl is the smallest variant.  */\n+  else if (optimize_size)\n+    {\n+      if (!count || (count & 3))\n+\treturn rep_prefix_1_byte;\n+      else\n+\treturn rep_prefix_4_byte;\n+    }\n+  /* Very tiny blocks are best handled via the loop, REP is expensive to setup.\n+   */\n+  else if (expected_size != -1 && expected_size < 4)\n+    return loop_1_byte;\n+  else if (expected_size != -1)\n+    {\n+      unsigned int i;\n+      enum stringop_alg alg = libcall;\n+      for (i = 0; i < NAX_STRINGOP_ALGS; i++)\n+\t{\n+\t  gcc_assert (algs->size[i].max);\n+\t  if (algs->size[i].max >= expected_size || algs->size[i].max == -1)\n+\t    {\n+\t      if (algs->size[i].alg != libcall)\n+\t\talg = algs->size[i].alg;\n+\t      /* Honor TARGET_INLINE_ALL_STRINGOPS by picking\n+\t         last non-libcall inline algorithm.  */\n+\t      if (TARGET_INLINE_ALL_STRINGOPS)\n+\t\t{\n+\t\t  gcc_assert (alg != libcall);\n+\t\t  return alg;\n+\t\t}\n+\t      else\n+\t\treturn algs->size[i].alg;\n+\t    }\n+\t}\n+      gcc_unreachable ();\n+    }\n+  /* When asked to inline the call anyway, try to pick meaningful choice.\n+     We look for maximal size of block that is faster to copy by hand and\n+     take blocks of at most of that size guessing that average size will\n+     be roughly half of the block.  \n+\n+     If this turns out to be bad, we might simply specify the preferred\n+     choice in ix86_costs.  */\n+  if ((TARGET_INLINE_ALL_STRINGOPS || TARGET_INLINE_STRINGOPS_DYNAMICALLY)\n+      && algs->unknown_size == libcall)\n+    {\n+      int max = -1;\n+      enum stringop_alg alg;\n+      int i;\n+\n+      for (i = 0; i < NAX_STRINGOP_ALGS; i++)\n+\tif (algs->size[i].alg != libcall && algs->size[i].alg)\n+\t  max = algs->size[i].max;\n+      if (max == -1)\n+\tmax = 4096;\n+      alg = decide_alg (count, max / 2, memset, dynamic_check);\n+      gcc_assert (*dynamic_check == -1);\n+      gcc_assert (alg != libcall);\n+      if (TARGET_INLINE_STRINGOPS_DYNAMICALLY)\n+\t*dynamic_check = max;\n+      return alg;\n+    }\n+  return algs->unknown_size;\n+}\n+\n+/* Decide on alignment.  We know that the operand is already aligned to ALIGN\n+   (ALIGN can be based on profile feedback and thus it is not 100% guaranteed).  */\n+static int\n+decide_alignment (int align,\n+\t\t  enum stringop_alg alg,\n+\t\t  int expected_size)\n+{\n+  int desired_align = 0;\n+  switch (alg)\n+    {\n+      case no_stringop:\n+\tgcc_unreachable ();\n+      case loop:\n+      case unrolled_loop:\n+\tdesired_align = GET_MODE_SIZE (Pmode);\n+\tbreak;\n+      case rep_prefix_8_byte:\n+\tdesired_align = 8;\n+\tbreak;\n+      case rep_prefix_4_byte:\n+\t/* PentiumPro has special logic triggering for 8 byte aligned blocks.\n+\t   copying whole cacheline at once.  */\n+\tif (TARGET_PENTIUMPRO)\n+\t  desired_align = 8;\n+\telse\n+\t  desired_align = 4;\n+\tbreak;\n+      case rep_prefix_1_byte:\n+\t/* PentiumPro has special logic triggering for 8 byte aligned blocks.\n+\t   copying whole cacheline at once.  */\n+\tif (TARGET_PENTIUMPRO)\n+\t  desired_align = 8;\n+\telse\n+\t  desired_align = 1;\n+\tbreak;\n+      case loop_1_byte:\n+\tdesired_align = 1;\n+\tbreak;\n+      case libcall:\n+\treturn 0;\n+    }\n+\n+  if (optimize_size)\n+    desired_align = 1;\n+  if (desired_align < align)\n+    desired_align = align;\n+  if (expected_size != -1 && expected_size < 4)\n+    desired_align = align;\n+  return desired_align;\n+}\n+\n+/* Expand string move (memcpy) operation.  Use i386 string operations when\n+   profitable.  expand_clrmem contains similar code.  */\n int\n-ix86_expand_clrmem (rtx dst, rtx count_exp, rtx align_exp)\n+ix86_expand_movmem (rtx dst, rtx src, rtx count_exp, rtx align_exp,\n+\t\t    rtx expected_align_exp, rtx expected_size_exp)\n {\n-  rtx destreg, zeroreg, countreg, destexp;\n-  enum machine_mode counter_mode;\n-  HOST_WIDE_INT align = 0;\n+  rtx destreg;\n+  rtx srcreg;\n+  rtx label = NULL;\n+  rtx tmp;\n+  rtx jump_around_label = NULL;\n+  HOST_WIDE_INT align = 1;\n   unsigned HOST_WIDE_INT count = 0;\n+  HOST_WIDE_INT expected_size = -1;\n+  int size_needed = 0;\n+  int desired_align = 0;\n+  enum stringop_alg alg;\n+  int dynamic_check;\n+  /* Precise placement on cld depends whether stringops will be emit in\n+     prologue, main copying body or epilogue.  This variable keeps track\n+     if cld was already needed.  */\n+  bool cld_done = false;\n \n   if (GET_CODE (align_exp) == CONST_INT)\n     align = INTVAL (align_exp);\n+  /* i386 can do missaligned access on resonably increased cost.  */\n+  if (GET_CODE (expected_align_exp) == CONST_INT\n+      && INTVAL (expected_align_exp) > align)\n+    align = INTVAL (expected_align_exp);\n+  if (GET_CODE (count_exp) == CONST_INT)\n+    count = expected_size = INTVAL (count_exp);\n+  if (GET_CODE (expected_size_exp) == CONST_INT && count == 0)\n+    {\n+      expected_size = INTVAL (expected_size_exp);\n+    }\n \n-  /* Can't use any of this if the user has appropriated esi.  */\n-  if (global_regs[4])\n-    return 0;\n+  alg = decide_alg (count, expected_size, false, &dynamic_check);\n+  desired_align = decide_alignment (align, alg, expected_size);\n \n-  /* This simple hack avoids all inlining code and simplifies code below.  */\n   if (!TARGET_ALIGN_STRINGOPS)\n-    align = 32;\n+    align = desired_align;\n \n-  if (GET_CODE (count_exp) == CONST_INT)\n+  if (alg == libcall)\n+    return 0;\n+  gcc_assert (alg != no_stringop);\n+  if (!count)\n+    count_exp = copy_to_mode_reg (GET_MODE (count_exp), count_exp);\n+  destreg = copy_to_mode_reg (Pmode, XEXP (dst, 0));\n+  srcreg = copy_to_mode_reg (Pmode, XEXP (src, 0));\n+  switch (alg)\n     {\n-      count = INTVAL (count_exp);\n-      if (!TARGET_INLINE_ALL_STRINGOPS && count > 64)\n-\treturn 0;\n+    case libcall:\n+    case no_stringop:\n+      gcc_unreachable ();\n+    case loop:\n+      size_needed = GET_MODE_SIZE (Pmode);\n+      break;\n+    case unrolled_loop:\n+      size_needed = GET_MODE_SIZE (Pmode) * (TARGET_64BIT ? 4 : 2);\n+      break;\n+    case rep_prefix_8_byte:\n+      size_needed = 8;\n+      break;\n+    case rep_prefix_4_byte:\n+      size_needed = 4;\n+      break;\n+    case rep_prefix_1_byte:\n+    case loop_1_byte:\n+      size_needed = 1;\n+      break;\n     }\n-  /* Figure out proper mode for counter.  For 32bits it is always SImode,\n-     for 64bits use SImode when possible, otherwise DImode.\n-     Set count to number of bytes copied when known at compile time.  */\n-  if (!TARGET_64BIT\n-      || GET_MODE (count_exp) == SImode\n-      || x86_64_zext_immediate_operand (count_exp, VOIDmode))\n-    counter_mode = SImode;\n-  else\n-    counter_mode = DImode;\n \n-  destreg = copy_to_mode_reg (Pmode, XEXP (dst, 0));\n-  if (destreg != XEXP (dst, 0))\n-    dst = replace_equiv_address_nv (dst, destreg);\n+  /* Alignment code needs count to be in register.  */\n+  if (GET_CODE (count_exp) == CONST_INT && desired_align > align)\n+    {\n+      enum machine_mode mode = SImode;\n+      if (TARGET_64BIT && (count & ~0xffffffff))\n+\tmode = DImode;\n+      count_exp = force_reg (mode, count_exp);\n+    }\n+  gcc_assert (desired_align >= 1 && align >= 1);\n+  /* Ensure that alignment prologue won't copy past end of block.  */\n+  if ((size_needed > 1 || (desired_align > 1 && desired_align > align))\n+      && !count)\n+    {\n+      int size = MAX (size_needed - 1, desired_align - align);\n+      if (TARGET_SINGLE_STRINGOP)\n+\temit_insn (gen_cld ()), cld_done = true;\n+      label = gen_label_rtx ();\n+      emit_cmp_and_jump_insns (count_exp,\n+\t\t\t       GEN_INT (size),\n+\t\t\t       LEU, 0, GET_MODE (count_exp), 1, label);\n+      if (expected_size == -1 || expected_size < size)\n+\tpredict_jump (REG_BR_PROB_BASE * 60 / 100);\n+      else\n+\tpredict_jump (REG_BR_PROB_BASE * 20 / 100);\n+    }\n+  /* Emit code to decide on runtime whether library call or inline should be\n+     used.  */\n+  if (dynamic_check != -1)\n+    {\n+      rtx hot_label = gen_label_rtx ();\n+      jump_around_label = gen_label_rtx ();\n+      emit_cmp_and_jump_insns (count_exp, GEN_INT (dynamic_check - 1),\n+\t\t\t       LEU, 0, GET_MODE (count_exp), 1, hot_label);\n+      predict_jump (REG_BR_PROB_BASE * 90 / 100);\n+      emit_block_move_via_libcall (dst, src, count_exp, false);\n+      emit_jump (jump_around_label);\n+      emit_label (hot_label);\n+    }\n \n \n-  /* When optimizing for size emit simple rep ; movsb instruction for\n-     counts not divisible by 4.  The movl $N, %ecx; rep; stosb\n-     sequence is 7 bytes long, so if optimizing for size and count is\n-     small enough that some stosl, stosw and stosb instructions without\n-     rep are shorter, fall back into the next if.  */\n+  /* Alignment prologue.  */\n+  if (desired_align > align)\n+    {\n+      /* Except for the first move in epilogue, we no longer know\n+         constant offset in aliasing info.  It don't seems to worth\n+\t the pain to maintain it for the first move, so throw away\n+\t the info early.  */\n+      src = change_address (src, BLKmode, srcreg);\n+      dst = change_address (dst, BLKmode, destreg);\n+      if (TARGET_SINGLE_STRINGOP && !cld_done)\n+\temit_insn (gen_cld ()), cld_done = true;\n+      expand_movmem_prologue (dst, src, destreg, srcreg, count_exp, align,\n+\t\t\t      desired_align);\n+    }\n+  if (label && size_needed == 1)\n+    {\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n+      label = NULL;\n+    }\n \n-  if ((!optimize || optimize_size)\n-      && (count == 0\n-\t  || ((count & 0x03)\n-\t      && (!optimize_size || (count & 0x03) + (count >> 2) > 7))))\n+  /* Main body.  */\n+  switch (alg)\n     {\n-      emit_insn (gen_cld ());\n+    case libcall:\n+    case no_stringop:\n+      gcc_unreachable ();\n+    case loop_1_byte:\n+      expand_set_or_movmem_via_loop (dst, src, destreg, srcreg, NULL,\n+\t\t\t\t     count_exp, QImode, 1, expected_size);\n+      break;\n+    case loop:\n+      expand_set_or_movmem_via_loop (dst, src, destreg, srcreg, NULL,\n+\t\t\t\t     count_exp, Pmode, 1, expected_size);\n+      break;\n+    case unrolled_loop:\n+      /* Unroll only by factor of 2 in 32bit mode, since we don't have enough\n+\t registers for 4 temporaries anyway.  */\n+      expand_set_or_movmem_via_loop (dst, src, destreg, srcreg, NULL,\n+\t\t\t\t     count_exp, Pmode, TARGET_64BIT ? 4 : 2,\n+\t\t\t\t     expected_size);\n+      break;\n+    case rep_prefix_8_byte:\n+      if (!cld_done)\n+\temit_insn (gen_cld ()), cld_done = true;\n+      expand_movmem_via_rep_mov (dst, src, destreg, srcreg, count_exp,\n+\t\t\t\t DImode);\n+      break;\n+    case rep_prefix_4_byte:\n+      if (!cld_done)\n+\temit_insn (gen_cld ()), cld_done = true;\n+      expand_movmem_via_rep_mov (dst, src, destreg, srcreg, count_exp,\n+\t\t\t\t SImode);\n+      break;\n+    case rep_prefix_1_byte:\n+      if (!cld_done)\n+\temit_insn (gen_cld ()), cld_done = true;\n+      expand_movmem_via_rep_mov (dst, src, destreg, srcreg, count_exp,\n+\t\t\t\t QImode);\n+      break;\n+    }\n+  /* Adjust properly the offset of src and dest memory for aliasing.  */\n+  if (GET_CODE (count_exp) == CONST_INT)\n+    {\n+      src = adjust_automodify_address_nv (src, BLKmode, srcreg,\n+\t\t\t\t\t  (count / size_needed) * size_needed);\n+      dst = adjust_automodify_address_nv (dst, BLKmode, destreg,\n+\t\t\t\t\t  (count / size_needed) * size_needed);\n+    }\n+  else\n+    {\n+      src = change_address (src, BLKmode, srcreg);\n+      dst = change_address (dst, BLKmode, destreg);\n+    }\n \n-      countreg = ix86_zero_extend_to_Pmode (count_exp);\n-      zeroreg = copy_to_mode_reg (QImode, const0_rtx);\n-      destexp = gen_rtx_PLUS (Pmode, destreg, countreg);\n-      emit_insn (gen_rep_stos (destreg, countreg, dst, zeroreg, destexp));\n+  /* Epologue to copy the remaining bytes.  */\n+  if (label)\n+    {\n+      if (size_needed < desired_align - align)\n+\t{\n+\t  tmp =\n+\t    expand_simple_binop (GET_MODE (count_exp), AND, count_exp,\n+\t\t\t\t GEN_INT (size_needed - 1), count_exp, 1,\n+\t\t\t\t OPTAB_DIRECT);\n+\t  size_needed = desired_align - align + 1;\n+\t  if (tmp != count_exp)\n+\t    emit_move_insn (count_exp, tmp);\n+\t}\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n     }\n-  else if (count != 0\n-\t   && (align >= 8\n-\t       || (!TARGET_PENTIUMPRO && !TARGET_64BIT && align >= 4)\n-\t       || optimize_size || count < (unsigned int) 64))\n+  if (count_exp != const0_rtx && size_needed > 1)\n     {\n-      int size = TARGET_64BIT && !optimize_size ? 8 : 4;\n-      unsigned HOST_WIDE_INT offset = 0;\n+      if (TARGET_SINGLE_STRINGOP && !cld_done)\n+\temit_insn (gen_cld ()), cld_done = true;\n+      expand_movmem_epilogue (dst, src, destreg, srcreg, count_exp,\n+\t\t\t      size_needed);\n+    }\n+  if (jump_around_label)\n+    emit_label (jump_around_label);\n+  return 1;\n+}\n \n-      emit_insn (gen_cld ());\n+/* Helper function for memcpy.  For QImode value 0xXY produce\n+   0xXYXYXYXY of wide specified by MODE.  This is essentially\n+   a * 0x10101010, but we can do slightly better than\n+   synth_mult by unwinding the sequence by hand on CPUs with\n+   slow multiply.  */\n+static rtx\n+promote_duplicated_reg (enum machine_mode mode, rtx val)\n+{\n+  enum machine_mode valmode = GET_MODE (val);\n+  rtx tmp;\n+  int nops = mode == DImode ? 3 : 2;\n \n-      zeroreg = copy_to_mode_reg (size == 4 ? SImode : DImode, const0_rtx);\n-      if (count & ~(size - 1))\n-\t{\n-\t  unsigned HOST_WIDE_INT repcount;\n-\t  unsigned int max_nonrep;\n+  gcc_assert (mode == SImode || mode == DImode);\n+  if (val == const0_rtx)\n+    return copy_to_mode_reg (mode, const0_rtx);\n+  if (GET_CODE (val) == CONST_INT)\n+    {\n+      HOST_WIDE_INT v = INTVAL (val) & 255;\n \n-\t  repcount = count >> (size == 4 ? 2 : 3);\n-\t  if (!TARGET_64BIT)\n-\t    repcount &= 0x3fffffff;\n+      v |= v << 8;\n+      v |= v << 16;\n+      if (mode == DImode)\n+        v |= (v << 16) << 16;\n+      return copy_to_mode_reg (mode, gen_int_mode (v, mode));\n+    }\n \n-\t  /* movl $N, %ecx; rep; stosl is 7 bytes, while N x stosl is N bytes.\n-\t     movl $N, %ecx; rep; stosq is 8 bytes, while N x stosq is 2xN\n-\t     bytes.  In both cases the latter seems to be faster for small\n-\t     values of N.  */\n-\t  max_nonrep = size == 4 ? 7 : 4;\n-\t  if (!optimize_size)\n-\t    switch (ix86_tune)\n-\t      {\n-\t      case PROCESSOR_PENTIUM4:\n-\t      case PROCESSOR_NOCONA:\n-\t        max_nonrep = 3;\n-\t        break;\n-\t      default:\n-\t        break;\n-\t      }\n+  if (valmode == VOIDmode)\n+    valmode = QImode;\n+  if (valmode != QImode)\n+    val = gen_lowpart (QImode, val);\n+  if (mode == QImode)\n+    return val;\n+  if (!TARGET_PARTIAL_REG_STALL)\n+    nops--;\n+  if (ix86_cost->mult_init[mode == DImode ? 3 : 2]\n+      + ix86_cost->mult_bit * (mode == DImode ? 8 : 4)\n+      <= (ix86_cost->shift_const + ix86_cost->add) * nops\n+          + (COSTS_N_INSNS (TARGET_PARTIAL_REG_STALL == 0)))\n+    {\n+      rtx reg = convert_modes (mode, QImode, val, true);\n+      tmp = promote_duplicated_reg (mode, const1_rtx);\n+      return expand_simple_binop (mode, MULT, reg, tmp, NULL, 1,\n+\t\t\t\t  OPTAB_DIRECT);\n+    }\n+  else\n+    {\n+      rtx reg = convert_modes (mode, QImode, val, true);\n \n-\t  if (repcount <= max_nonrep)\n-\t    while (repcount-- > 0)\n-\t      {\n-\t\trtx mem = adjust_automodify_address_nv (dst,\n-\t\t\t\t\t\t\tGET_MODE (zeroreg),\n-\t\t\t\t\t\t\tdestreg, offset);\n-\t\temit_insn (gen_strset (destreg, mem, zeroreg));\n-\t\toffset += size;\n-\t      }\n-\t  else\n-\t    {\n-\t      countreg = copy_to_mode_reg (counter_mode, GEN_INT (repcount));\n-\t      countreg = ix86_zero_extend_to_Pmode (countreg);\n-\t      destexp = gen_rtx_ASHIFT (Pmode, countreg,\n-\t\t\t\t\tGEN_INT (size == 4 ? 2 : 3));\n-\t      destexp = gen_rtx_PLUS (Pmode, destexp, destreg);\n-\t      emit_insn (gen_rep_stos (destreg, countreg, dst, zeroreg,\n-\t\t\t\t       destexp));\n-\t      offset = count & ~(size - 1);\n-\t    }\n-\t}\n-      if (size == 8 && (count & 0x04))\n-\t{\n-\t  rtx mem = adjust_automodify_address_nv (dst, SImode, destreg,\n-\t\t\t\t\t\t  offset);\n-\t  emit_insn (gen_strset (destreg, mem,\n-\t\t\t\t gen_rtx_SUBREG (SImode, zeroreg, 0)));\n-\t  offset += 4;\n-\t}\n-      if (count & 0x02)\n-\t{\n-\t  rtx mem = adjust_automodify_address_nv (dst, HImode, destreg,\n-\t\t\t\t\t\t  offset);\n-\t  emit_insn (gen_strset (destreg, mem,\n-\t\t\t\t gen_rtx_SUBREG (HImode, zeroreg, 0)));\n-\t  offset += 2;\n-\t}\n-      if (count & 0x01)\n+      if (!TARGET_PARTIAL_REG_STALL)\n+\tif (mode == SImode)\n+\t  emit_insn (gen_movsi_insv_1 (reg, reg));\n+\telse\n+\t  emit_insn (gen_movdi_insv_1_rex64 (reg, reg));\n+      else\n \t{\n-\t  rtx mem = adjust_automodify_address_nv (dst, QImode, destreg,\n-\t\t\t\t\t\t  offset);\n-\t  emit_insn (gen_strset (destreg, mem,\n-\t\t\t\t gen_rtx_SUBREG (QImode, zeroreg, 0)));\n+\t  tmp = expand_simple_binop (mode, ASHIFT, reg, GEN_INT (8),\n+\t\t\t\t     NULL, 1, OPTAB_DIRECT);\n+\t  reg =\n+\t    expand_simple_binop (mode, IOR, reg, tmp, reg, 1, OPTAB_DIRECT);\n \t}\n+      tmp = expand_simple_binop (mode, ASHIFT, reg, GEN_INT (16),\n+\t\t\t         NULL, 1, OPTAB_DIRECT);\n+      reg = expand_simple_binop (mode, IOR, reg, tmp, reg, 1, OPTAB_DIRECT);\n+      if (mode == SImode)\n+\treturn reg;\n+      tmp = expand_simple_binop (mode, ASHIFT, reg, GEN_INT (32),\n+\t\t\t\t NULL, 1, OPTAB_DIRECT);\n+      reg = expand_simple_binop (mode, IOR, reg, tmp, reg, 1, OPTAB_DIRECT);\n+      return reg;\n     }\n-  else\n-    {\n-      rtx countreg2;\n-      rtx label = NULL;\n-      /* Compute desired alignment of the string operation.  */\n-      int desired_alignment = (TARGET_PENTIUMPRO\n-\t\t\t       && (count == 0 || count >= (unsigned int) 260)\n-\t\t\t       ? 8 : UNITS_PER_WORD);\n-\n-      /* In case we don't know anything about the alignment, default to\n-         library version, since it is usually equally fast and result in\n-         shorter code.\n-\n-\t Also emit call when we know that the count is large and call overhead\n-\t will not be important.  */\n-      if (!TARGET_INLINE_ALL_STRINGOPS\n-\t  && (align < UNITS_PER_WORD || !TARGET_REP_MOVL_OPTIMAL))\n-\treturn 0;\n+}\n \n-      if (TARGET_SINGLE_STRINGOP)\n-\temit_insn (gen_cld ());\n+/* Expand string clear operation (bzero).  Use i386 string operations when\n+   profitable.  expand_movmem contains similar code.  */\n+int\n+ix86_expand_setmem (rtx dst, rtx count_exp, rtx val_exp, rtx align_exp,\n+\t\t    rtx expected_align_exp, rtx expected_size_exp)\n+{\n+  rtx destreg;\n+  rtx label = NULL;\n+  rtx tmp;\n+  rtx jump_around_label = NULL;\n+  HOST_WIDE_INT align = 1;\n+  unsigned HOST_WIDE_INT count = 0;\n+  HOST_WIDE_INT expected_size = -1;\n+  int size_needed = 0;\n+  int desired_align = 0;\n+  enum stringop_alg alg;\n+  /* Precise placement on cld depends whether stringops will be emit in\n+     prologue, main copying body or epilogue.  This variable keeps track\n+     if cld was already needed.  */\n+  bool cld_done = false;\n+  rtx promoted_val = val_exp;\n+  bool force_loopy_epilogue = false;\n+  int dynamic_check;\n \n-      countreg2 = gen_reg_rtx (Pmode);\n-      countreg = copy_to_mode_reg (counter_mode, count_exp);\n-      zeroreg = copy_to_mode_reg (Pmode, const0_rtx);\n-      /* Get rid of MEM_OFFSET, it won't be accurate.  */\n-      dst = change_address (dst, BLKmode, destreg);\n+  if (GET_CODE (align_exp) == CONST_INT)\n+    align = INTVAL (align_exp);\n+  /* i386 can do missaligned access on resonably increased cost.  */\n+  if (GET_CODE (expected_align_exp) == CONST_INT\n+      && INTVAL (expected_align_exp) > align)\n+    align = INTVAL (expected_align_exp);\n+  if (GET_CODE (count_exp) == CONST_INT)\n+    count = expected_size = INTVAL (count_exp);\n+  if (GET_CODE (expected_size_exp) == CONST_INT && count == 0)\n+    expected_size = INTVAL (expected_size_exp);\n \n-      if (count == 0 && align < desired_alignment)\n-\t{\n-\t  label = gen_label_rtx ();\n-\t  emit_cmp_and_jump_insns (countreg, GEN_INT (desired_alignment - 1),\n-\t\t\t\t   LEU, 0, counter_mode, 1, label);\n-\t}\n-      if (align <= 1)\n-\t{\n-\t  rtx label = ix86_expand_aligntest (destreg, 1);\n-\t  emit_insn (gen_strset (destreg, dst,\n-\t\t\t\t gen_rtx_SUBREG (QImode, zeroreg, 0)));\n-\t  ix86_adjust_counter (countreg, 1);\n-\t  emit_label (label);\n-\t  LABEL_NUSES (label) = 1;\n-\t}\n-      if (align <= 2)\n-\t{\n-\t  rtx label = ix86_expand_aligntest (destreg, 2);\n-\t  emit_insn (gen_strset (destreg, dst,\n-\t\t\t\t gen_rtx_SUBREG (HImode, zeroreg, 0)));\n-\t  ix86_adjust_counter (countreg, 2);\n-\t  emit_label (label);\n-\t  LABEL_NUSES (label) = 1;\n-\t}\n-      if (align <= 4 && desired_alignment > 4)\n-\t{\n-\t  rtx label = ix86_expand_aligntest (destreg, 4);\n-\t  emit_insn (gen_strset (destreg, dst,\n-\t\t\t\t (TARGET_64BIT\n-\t\t\t\t  ? gen_rtx_SUBREG (SImode, zeroreg, 0)\n-\t\t\t\t  : zeroreg)));\n-\t  ix86_adjust_counter (countreg, 4);\n-\t  emit_label (label);\n-\t  LABEL_NUSES (label) = 1;\n-\t}\n+  alg = decide_alg (count, expected_size, true, &dynamic_check);\n+  desired_align = decide_alignment (align, alg, expected_size);\n \n-      if (label && desired_alignment > 4 && !TARGET_64BIT)\n-\t{\n-\t  emit_label (label);\n-\t  LABEL_NUSES (label) = 1;\n-\t  label = NULL_RTX;\n-\t}\n+  if (!TARGET_ALIGN_STRINGOPS)\n+    align = desired_align;\n \n-      if (!TARGET_SINGLE_STRINGOP)\n-\temit_insn (gen_cld ());\n-      if (TARGET_64BIT)\n-\t{\n-\t  emit_insn (gen_lshrdi3 (countreg2, ix86_zero_extend_to_Pmode (countreg),\n-\t\t\t\t  GEN_INT (3)));\n-\t  destexp = gen_rtx_ASHIFT (Pmode, countreg2, GEN_INT (3));\n-\t}\n+  if (alg == libcall)\n+    return 0;\n+  gcc_assert (alg != no_stringop);\n+  if (!count)\n+    count_exp = copy_to_mode_reg (GET_MODE (count_exp), count_exp);\n+  destreg = copy_to_mode_reg (Pmode, XEXP (dst, 0));\n+  switch (alg)\n+    {\n+    case libcall:\n+    case no_stringop:\n+      gcc_unreachable ();\n+    case loop:\n+      size_needed = GET_MODE_SIZE (Pmode);\n+      break;\n+    case unrolled_loop:\n+      size_needed = GET_MODE_SIZE (Pmode) * 4;\n+      break;\n+    case rep_prefix_8_byte:\n+      size_needed = 8;\n+      break;\n+    case rep_prefix_4_byte:\n+      size_needed = 4;\n+      break;\n+    case rep_prefix_1_byte:\n+    case loop_1_byte:\n+      size_needed = 1;\n+      break;\n+    }\n+  /* Alignment code needs count to be in register.  */\n+  if (GET_CODE (count_exp) == CONST_INT && desired_align > align)\n+    {\n+      enum machine_mode mode = SImode;\n+      if (TARGET_64BIT && (count & ~0xffffffff))\n+\tmode = DImode;\n+      count_exp = force_reg (mode, count_exp);\n+    }\n+  /* Ensure that alignment prologue won't copy past end of block.  */\n+  if ((size_needed > 1 || (desired_align > 1 && desired_align > align))\n+      && !count)\n+    {\n+      int size = MAX (size_needed - 1, desired_align - align);\n+      /* To improve performance of small blocks, we jump around the promoting\n+         code, so we need to use QImode accesses in epilogue.  */\n+      if (GET_CODE (val_exp) != CONST_INT && size_needed > 1)\n+\tforce_loopy_epilogue = true;\n+      else if (TARGET_SINGLE_STRINGOP)\n+\temit_insn (gen_cld ()), cld_done = true;\n+      label = gen_label_rtx ();\n+      emit_cmp_and_jump_insns (count_exp,\n+\t\t\t       GEN_INT (size),\n+\t\t\t       LEU, 0, GET_MODE (count_exp), 1, label);\n+      if (expected_size == -1 || expected_size <= size)\n+\tpredict_jump (REG_BR_PROB_BASE * 60 / 100);\n       else\n-\t{\n-\t  emit_insn (gen_lshrsi3 (countreg2, countreg, const2_rtx));\n-\t  destexp = gen_rtx_ASHIFT (Pmode, countreg2, const2_rtx);\n-\t}\n-      destexp = gen_rtx_PLUS (Pmode, destexp, destreg);\n-      emit_insn (gen_rep_stos (destreg, countreg2, dst, zeroreg, destexp));\n-\n-      if (label)\n-\t{\n-\t  emit_label (label);\n-\t  LABEL_NUSES (label) = 1;\n-\t}\n+\tpredict_jump (REG_BR_PROB_BASE * 20 / 100);\n+    }\n+  if (dynamic_check != -1)\n+    {\n+      rtx hot_label = gen_label_rtx ();\n+      jump_around_label = gen_label_rtx ();\n+      emit_cmp_and_jump_insns (count_exp, GEN_INT (dynamic_check - 1),\n+\t\t\t       LEU, 0, GET_MODE (count_exp), 1, hot_label);\n+      predict_jump (REG_BR_PROB_BASE * 90 / 100);\n+      set_storage_via_libcall (dst, count_exp, val_exp, false);\n+      emit_jump (jump_around_label);\n+      emit_label (hot_label);\n+    }\n+  if (TARGET_64BIT\n+      && (size_needed > 4 || (desired_align > align && desired_align > 4)))\n+    promoted_val = promote_duplicated_reg (DImode, val_exp);\n+  else if (size_needed > 2 || (desired_align > align && desired_align > 2))\n+    promoted_val = promote_duplicated_reg (SImode, val_exp);\n+  else if (size_needed > 1 || (desired_align > align && desired_align > 1))\n+    promoted_val = promote_duplicated_reg (HImode, val_exp);\n+  else\n+    promoted_val = val_exp;\n+  gcc_assert (desired_align >= 1 && align >= 1);\n+  if ((size_needed > 1 || (desired_align > 1 && desired_align > align))\n+      && !count && !label)\n+    {\n+      int size = MAX (size_needed - 1, desired_align - align);\n+      if (TARGET_SINGLE_STRINGOP)\n+\temit_insn (gen_cld ()), cld_done = true;\n+      label = gen_label_rtx ();\n+      emit_cmp_and_jump_insns (count_exp,\n+\t\t\t       GEN_INT (size),\n+\t\t\t       LEU, 0, GET_MODE (count_exp), 1, label);\n+      if (expected_size == -1 || expected_size <= size)\n+\tpredict_jump (REG_BR_PROB_BASE * 60 / 100);\n+      else\n+\tpredict_jump (REG_BR_PROB_BASE * 20 / 100);\n+    }\n+  if (desired_align > align)\n+    {\n+      /* Except for the first move in epilogue, we no longer know\n+         constant offset in aliasing info.  It don't seems to worth\n+\t the pain to maintain it for the first move, so throw away\n+\t the info early.  */\n+      dst = change_address (dst, BLKmode, destreg);\n+      if (TARGET_SINGLE_STRINGOP && !cld_done)\n+\temit_insn (gen_cld ()), cld_done = true;\n+      expand_setmem_prologue (dst, destreg, promoted_val, count_exp, align,\n+\t\t\t      desired_align);\n+    }\n+  if (label && size_needed == 1)\n+    {\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n+      label = NULL;\n+    }\n+  switch (alg)\n+    {\n+    case libcall:\n+    case no_stringop:\n+      gcc_unreachable ();\n+    case loop_1_byte:\n+      expand_set_or_movmem_via_loop (dst, NULL, destreg, NULL, promoted_val,\n+\t\t\t\t     count_exp, QImode, 1, expected_size);\n+      break;\n+    case loop:\n+      expand_set_or_movmem_via_loop (dst, NULL, destreg, NULL, promoted_val,\n+\t\t\t\t     count_exp, Pmode, 1, expected_size);\n+      break;\n+    case unrolled_loop:\n+      expand_set_or_movmem_via_loop (dst, NULL, destreg, NULL, promoted_val,\n+\t\t\t\t     count_exp, Pmode, 4, expected_size);\n+      break;\n+    case rep_prefix_8_byte:\n+      if (!cld_done)\n+\temit_insn (gen_cld ()), cld_done = true;\n+      expand_setmem_via_rep_stos (dst, destreg, promoted_val, count_exp,\n+\t\t\t\t  DImode);\n+      break;\n+    case rep_prefix_4_byte:\n+      if (!cld_done)\n+\temit_insn (gen_cld ()), cld_done = true;\n+      expand_setmem_via_rep_stos (dst, destreg, promoted_val, count_exp,\n+\t\t\t\t  SImode);\n+      break;\n+    case rep_prefix_1_byte:\n+      if (!cld_done)\n+\temit_insn (gen_cld ()), cld_done = true;\n+      expand_setmem_via_rep_stos (dst, destreg, promoted_val, count_exp,\n+\t\t\t\t  QImode);\n+      break;\n+    }\n+  /* Adjust properly the offset of src and dest memory for aliasing.  */\n+  if (GET_CODE (count_exp) == CONST_INT)\n+    dst = adjust_automodify_address_nv (dst, BLKmode, destreg,\n+\t\t\t\t\t(count / size_needed) * size_needed);\n+  else\n+    dst = change_address (dst, BLKmode, destreg);\n \n-      if (TARGET_64BIT && align > 4 && count != 0 && (count & 4))\n-\temit_insn (gen_strset (destreg, dst,\n-\t\t\t       gen_rtx_SUBREG (SImode, zeroreg, 0)));\n-      if (TARGET_64BIT && (align <= 4 || count == 0))\n+  if (label)\n+    {\n+      if (size_needed < desired_align - align)\n \t{\n-\t  rtx label = ix86_expand_aligntest (countreg, 4);\n-\t  emit_insn (gen_strset (destreg, dst,\n-\t\t\t\t gen_rtx_SUBREG (SImode, zeroreg, 0)));\n-\t  emit_label (label);\n-\t  LABEL_NUSES (label) = 1;\n+\t  tmp =\n+\t    expand_simple_binop (GET_MODE (count_exp), AND, count_exp,\n+\t\t\t\t GEN_INT (size_needed - 1), count_exp, 1,\n+\t\t\t\t OPTAB_DIRECT);\n+\t  size_needed = desired_align - align + 1;\n+\t  if (tmp != count_exp)\n+\t    emit_move_insn (count_exp, tmp);\n \t}\n-      if (align > 2 && count != 0 && (count & 2))\n-\temit_insn (gen_strset (destreg, dst,\n-\t\t\t       gen_rtx_SUBREG (HImode, zeroreg, 0)));\n-      if (align <= 2 || count == 0)\n-\t{\n-\t  rtx label = ix86_expand_aligntest (countreg, 2);\n-\t  emit_insn (gen_strset (destreg, dst,\n-\t\t\t\t gen_rtx_SUBREG (HImode, zeroreg, 0)));\n-\t  emit_label (label);\n-\t  LABEL_NUSES (label) = 1;\n-\t}\n-      if (align > 1 && count != 0 && (count & 1))\n-\temit_insn (gen_strset (destreg, dst,\n-\t\t\t       gen_rtx_SUBREG (QImode, zeroreg, 0)));\n-      if (align <= 1 || count == 0)\n+      emit_label (label);\n+      LABEL_NUSES (label) = 1;\n+    }\n+  if (count_exp != const0_rtx && size_needed > 1)\n+    {\n+      if (force_loopy_epilogue)\n+\texpand_setmem_epilogue_via_loop (dst, destreg, val_exp, count_exp,\n+\t\t\t\t\t size_needed);\n+      else\n \t{\n-\t  rtx label = ix86_expand_aligntest (countreg, 1);\n-\t  emit_insn (gen_strset (destreg, dst,\n-\t\t\t\t gen_rtx_SUBREG (QImode, zeroreg, 0)));\n-\t  emit_label (label);\n-\t  LABEL_NUSES (label) = 1;\n+\t  if (TARGET_SINGLE_STRINGOP && !cld_done)\n+\t    emit_insn (gen_cld ()), cld_done = true;\n+\t  expand_setmem_epilogue (dst, destreg, promoted_val, count_exp,\n+\t\t\t\t  size_needed);\n \t}\n     }\n+  if (jump_around_label)\n+    emit_label (jump_around_label);\n   return 1;\n }\n "}, {"sha": "52118f98ab6f9a697ca1bd9fd774f97f737044d0", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 35, "deletions": 1, "changes": 36, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=8c996513856f2769aee1730cb211050fef055fb5", "patch": "@@ -19,6 +19,38 @@ along with GCC; see the file COPYING.  If not, write to\n the Free Software Foundation, 51 Franklin Street, Fifth Floor,\n Boston, MA 02110-1301, USA.  */\n \n+/* Algorithm to expand string function with.  */\n+enum stringop_alg\n+{\n+   no_stringop,\n+   libcall,\n+   rep_prefix_1_byte,\n+   rep_prefix_4_byte,\n+   rep_prefix_8_byte,\n+   loop_1_byte,\n+   loop,\n+   unrolled_loop\n+};\n+#define NAX_STRINGOP_ALGS 4\n+/* Specify what algorithm to use for stringops on known size.\n+   When size is unknown, the UNKNOWN_SIZE alg is used.  When size is\n+   known at compile time or estimated via feedback, the SIZE array\n+   is walked in order until MAX is greater then the estimate (or -1\n+   means infinity).  Corresponding ALG is used then.  \n+   For example initializer:\n+    {{256, loop}, {-1, rep_prefix_4_byte}}\t\t\n+   will use loop for blocks smaller or equal to 256 bytes, rep prefix will\n+   be used otherwise.\n+*/\n+struct stringop_algs\n+{\n+  const enum stringop_alg unknown_size;\n+  const struct stringop_strategy {\n+    const int max;\n+    const enum stringop_alg alg;\n+  } size [NAX_STRINGOP_ALGS];\n+};\n+\n /* The purpose of this file is to define the characteristics of the i386,\n    independent of assembler syntax or operating system.\n \n@@ -84,6 +116,9 @@ struct processor_costs {\n   const int fabs;\t\t/* cost of FABS instruction.  */\n   const int fchs;\t\t/* cost of FCHS instruction.  */\n   const int fsqrt;\t\t/* cost of FSQRT instruction.  */\n+\t\t\t\t/* Specify what algorithm\n+\t\t\t\t   to use for stringops on unknown size.  */\n+  struct stringop_algs memcpy[2], memset[2];\n };\n \n extern const struct processor_costs *ix86_cost;\n@@ -217,7 +252,6 @@ extern int x86_prefetch_sse;\n #define TARGET_PREFETCH_SSE (x86_prefetch_sse)\n #define TARGET_SHIFT1 (x86_shift1 & TUNEMASK)\n #define TARGET_USE_FFREEP (x86_use_ffreep & TUNEMASK)\n-#define TARGET_REP_MOVL_OPTIMAL (x86_rep_movl_optimal & TUNEMASK)\n #define TARGET_INTER_UNIT_MOVES (x86_inter_unit_moves & TUNEMASK)\n #define TARGET_FOUR_JUMP_LIMIT (x86_four_jump_limit & TUNEMASK)\n #define TARGET_SCHEDULE (x86_schedule & TUNEMASK)"}, {"sha": "f237abe99567c5198fdcbaf67c73ab9c58474e59", "filename": "gcc/config/i386/i386.md", "status": "modified", "additions": 24, "deletions": 14, "changes": 38, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fconfig%2Fi386%2Fi386.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fconfig%2Fi386%2Fi386.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.md?ref=8c996513856f2769aee1730cb211050fef055fb5", "patch": "@@ -1805,6 +1805,16 @@\n   [(set_attr \"type\" \"imov\")\n    (set_attr \"mode\" \"QI\")])\n \n+(define_insn \"*movsi_insv_1_rex64\"\n+  [(set (zero_extract:SI (match_operand 0 \"ext_register_operand\" \"+Q\")\n+\t\t\t (const_int 8)\n+\t\t\t (const_int 8))\n+\t(match_operand:SI 1 \"nonmemory_operand\" \"Qn\"))]\n+  \"TARGET_64BIT\"\n+  \"mov{b}\\t{%b1, %h0|%h0, %b1}\"\n+  [(set_attr \"type\" \"imov\")\n+   (set_attr \"mode\" \"QI\")])\n+\n (define_insn \"movdi_insv_1_rex64\"\n   [(set (zero_extract:DI (match_operand 0 \"ext_register_operand\" \"+Q\")\n \t\t\t (const_int 8)\n@@ -18160,9 +18170,10 @@\n    (use (match_operand:BLK 1 \"memory_operand\" \"\"))\n    (use (match_operand:SI 2 \"nonmemory_operand\" \"\"))\n    (use (match_operand:SI 3 \"const_int_operand\" \"\"))]\n-  \"! optimize_size || TARGET_INLINE_ALL_STRINGOPS\"\n+  \"\"\n {\n- if (ix86_expand_movmem (operands[0], operands[1], operands[2], operands[3]))\n+ if (ix86_expand_movmem (operands[0], operands[1], operands[2], operands[3],\n+\t\t\t operands[3], constm1_rtx))\n    DONE;\n  else\n    FAIL;\n@@ -18175,7 +18186,8 @@\n    (use (match_operand:DI 3 \"const_int_operand\" \"\"))]\n   \"TARGET_64BIT\"\n {\n- if (ix86_expand_movmem (operands[0], operands[1], operands[2], operands[3]))\n+ if (ix86_expand_movmem (operands[0], operands[1], operands[2], operands[3],\n+\t\t\t operands[3], constm1_rtx))\n    DONE;\n  else\n    FAIL;\n@@ -18450,11 +18462,9 @@\n     (use (match_operand 3 \"const_int_operand\" \"\"))]\n   \"\"\n {\n- /* If value to set is not zero, use the library routine.  */\n- if (operands[2] != const0_rtx)\n-   FAIL;\n-\n- if (ix86_expand_clrmem (operands[0], operands[1], operands[3]))\n+ if (ix86_expand_setmem (operands[0], operands[1],\n+\t\t\t operands[2], operands[3],\n+\t\t\t operands[3], constm1_rtx))\n    DONE;\n  else\n    FAIL;\n@@ -18464,14 +18474,14 @@\n    [(use (match_operand:BLK 0 \"memory_operand\" \"\"))\n     (use (match_operand:DI 1 \"nonmemory_operand\" \"\"))\n     (use (match_operand 2 \"const_int_operand\" \"\"))\n-    (use (match_operand 3 \"const_int_operand\" \"\"))]\n+    (use (match_operand 3 \"const_int_operand\" \"\"))\n+    (use (match_operand 4 \"const_int_operand\" \"\"))\n+    (use (match_operand 5 \"const_int_operand\" \"\"))]\n   \"TARGET_64BIT\"\n {\n- /* If value to set is not zero, use the library routine.  */\n- if (operands[2] != const0_rtx)\n-   FAIL;\n-\n- if (ix86_expand_clrmem (operands[0], operands[1], operands[3]))\n+ if (ix86_expand_setmem (operands[0], operands[1],\n+\t\t\t operands[2], operands[3],\n+\t\t\t operands[3], constm1_rtx))\n    DONE;\n  else\n    FAIL;"}, {"sha": "aba2ee7440dc12ac0a9adebd149b15808adf68d3", "filename": "gcc/config/i386/i386.opt", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fconfig%2Fi386%2Fi386.opt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fconfig%2Fi386%2Fi386.opt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.opt?ref=8c996513856f2769aee1730cb211050fef055fb5", "patch": "@@ -125,6 +125,10 @@ minline-all-stringops\n Target Report Mask(INLINE_ALL_STRINGOPS)\n Inline all known string operations\n \n+minline-stringops-dynamically\n+Target Report Mask(INLINE_STRINGOPS_DYNAMICALLY)\n+Inline memset/memcpy string operations, but perform inline version only for small blocks\n+\n mintel-syntax\n Target Undocumented\n ;; Deprecated\n@@ -221,6 +225,10 @@ mstack-arg-probe\n Target Report Mask(STACK_PROBE)\n Enable stack probing\n \n+mstringop-strategy=\n+Target RejectNegative Joined Var(ix86_stringop_string)\n+Chose strategy to generate stringop using\n+\n mtls-dialect=\n Target RejectNegative Joined Var(ix86_tls_dialect_string)\n Use given thread-local storage dialect"}, {"sha": "51d8fb4ffa3d9fa89f6b1bbacc6bd53de0cef0e3", "filename": "gcc/doc/invoke.texi", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fdoc%2Finvoke.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fdoc%2Finvoke.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Finvoke.texi?ref=8c996513856f2769aee1730cb211050fef055fb5", "patch": "@@ -9721,6 +9721,19 @@ aligned at least to 4 byte boundary.  This enables more inlining, increase code\n size, but may improve performance of code that depends on fast memcpy, strlen\n and memset for short lengths.\n \n+@item -minline-stringops-dynamically\n+@opindex minline-stringops-dynamically\n+For string operation of unknown size, inline runtime checks so for small\n+blocks inline code is used, while for large blocks librarly call is used.\n+\n+@item -mstringop-strategy=@var{alg}\n+@optindex mstringop-strategy=@var{alg}\n+Overwrite internal decision heuristic about particular algorithm to inline\n+string opteration with.  The allowed values are @code{rep_byte},\n+@code{rep_4byte}, @code{rep_8byte} for expanding using i386 @code{rep} prefix\n+of specified size, @code{loop}, @code{unrolled_loop} for expanding inline loop,\n+@code{libcall} for always expanding library call.\n+\n @item -momit-leaf-frame-pointer\n @opindex momit-leaf-frame-pointer\n Don't keep the frame pointer in a register for leaf functions.  This"}, {"sha": "7182b57442350af311db9dbefba68215cce70638", "filename": "gcc/expr.c", "status": "modified", "additions": 11, "deletions": 10, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fexpr.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fexpr.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpr.c?ref=8c996513856f2769aee1730cb211050fef055fb5", "patch": "@@ -127,15 +127,13 @@ static void move_by_pieces_1 (rtx (*) (rtx, ...), enum machine_mode,\n \t\t\t      struct move_by_pieces *);\n static bool block_move_libcall_safe_for_call_parm (void);\n static bool emit_block_move_via_movmem (rtx, rtx, rtx, unsigned);\n-static rtx emit_block_move_via_libcall (rtx, rtx, rtx, bool);\n static tree emit_block_move_libcall_fn (int);\n static void emit_block_move_via_loop (rtx, rtx, rtx, unsigned);\n static rtx clear_by_pieces_1 (void *, HOST_WIDE_INT, enum machine_mode);\n static void clear_by_pieces (rtx, unsigned HOST_WIDE_INT, unsigned int);\n static void store_by_pieces_1 (struct store_by_pieces *, unsigned int);\n static void store_by_pieces_2 (rtx (*) (rtx, ...), enum machine_mode,\n \t\t\t       struct store_by_pieces *);\n-static rtx clear_storage_via_libcall (rtx, rtx, bool);\n static tree clear_storage_libcall_fn (int);\n static rtx compress_float_constant (rtx, rtx);\n static rtx get_subtarget (rtx);\n@@ -1336,7 +1334,7 @@ emit_block_move_via_movmem (rtx x, rtx y, rtx size, unsigned int align)\n /* A subroutine of emit_block_move.  Expand a call to memcpy.\n    Return the return value from memcpy, 0 otherwise.  */\n \n-static rtx\n+rtx\n emit_block_move_via_libcall (rtx dst, rtx src, rtx size, bool tailcall)\n {\n   rtx dst_addr, src_addr;\n@@ -2540,19 +2538,19 @@ clear_storage (rtx object, rtx size, enum block_op_methods method)\n   else if (set_storage_via_setmem (object, size, const0_rtx, align))\n     ;\n   else\n-    return clear_storage_via_libcall (object, size,\n-\t\t\t\t      method == BLOCK_OP_TAILCALL);\n+    return set_storage_via_libcall (object, size, const0_rtx,\n+\t\t\t\t    method == BLOCK_OP_TAILCALL);\n \n   return NULL;\n }\n \n /* A subroutine of clear_storage.  Expand a call to memset.\n    Return the return value of memset, 0 otherwise.  */\n \n-static rtx\n-clear_storage_via_libcall (rtx object, rtx size, bool tailcall)\n+rtx\n+set_storage_via_libcall (rtx object, rtx size, rtx val, bool tailcall)\n {\n-  tree call_expr, arg_list, fn, object_tree, size_tree;\n+  tree call_expr, arg_list, fn, object_tree, size_tree, val_tree;\n   enum machine_mode size_mode;\n   rtx retval;\n \n@@ -2572,11 +2570,14 @@ clear_storage_via_libcall (rtx object, rtx size, bool tailcall)\n      for returning pointers, we could end up generating incorrect code.  */\n \n   object_tree = make_tree (ptr_type_node, object);\n+  if (GET_CODE (val) != CONST_INT)\n+    val = convert_to_mode (TYPE_MODE (integer_type_node), val, 1);\n   size_tree = make_tree (sizetype, size);\n+  val_tree = make_tree (integer_type_node, val);\n \n   fn = clear_storage_libcall_fn (true);\n   arg_list = tree_cons (NULL_TREE, size_tree, NULL_TREE);\n-  arg_list = tree_cons (NULL_TREE, integer_zero_node, arg_list);\n+  arg_list = tree_cons (NULL_TREE, val_tree, arg_list);\n   arg_list = tree_cons (NULL_TREE, object_tree, arg_list);\n \n   /* Now we have to build up the CALL_EXPR itself.  */\n@@ -2590,7 +2591,7 @@ clear_storage_via_libcall (rtx object, rtx size, bool tailcall)\n   return retval;\n }\n \n-/* A subroutine of clear_storage_via_libcall.  Create the tree node\n+/* A subroutine of set_storage_via_libcall.  Create the tree node\n    for the function we use for block clears.  The first time FOR_CALL\n    is true, we call assemble_external.  */\n "}, {"sha": "beb8ea388d1efc6cc191a2b3a1e7bfdf872ca76c", "filename": "gcc/expr.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fexpr.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Fexpr.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpr.h?ref=8c996513856f2769aee1730cb211050fef055fb5", "patch": "@@ -377,6 +377,7 @@ extern void init_block_move_fn (const char *);\n extern void init_block_clear_fn (const char *);\n \n extern rtx emit_block_move (rtx, rtx, rtx, enum block_op_methods);\n+extern rtx emit_block_move_via_libcall (rtx, rtx, rtx, bool);\n \n /* Copy all or part of a value X into registers starting at REGNO.\n    The number of registers to be filled is NREGS.  */\n@@ -423,6 +424,8 @@ extern void use_group_regs (rtx *, rtx);\n /* Write zeros through the storage of OBJECT.\n    If OBJECT has BLKmode, SIZE is its length in bytes.  */\n extern rtx clear_storage (rtx, rtx, enum block_op_methods);\n+/* The same, but always output an library call.  */\n+rtx set_storage_via_libcall (rtx, rtx, rtx, bool);\n \n /* Expand a setmem pattern; return true if successful.  */\n extern bool set_storage_via_setmem (rtx, rtx, rtx, unsigned int);"}, {"sha": "374a945d7e2605c0f511e8c2fe5114fabf71c458", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=8c996513856f2769aee1730cb211050fef055fb5", "patch": "@@ -1,3 +1,8 @@\n+2006-11-27  Jan Hubicka  <jh@suse.cz>\n+\n+\t* gcc.target/i386/memcpy-1.c: Adjust size.\n+\t* testsuite/gcc.dg/visibility-11.c: Likewise.\n+\n 2006-11-27  Richard Guenther  <rguenther@suse.de>\n \n \tPR middle-end/25620"}, {"sha": "ec5c9449b8cc6cd4fb3fb9da92f3251888b12883", "filename": "gcc/testsuite/gcc.dg/visibility-11.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Ftestsuite%2Fgcc.dg%2Fvisibility-11.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Ftestsuite%2Fgcc.dg%2Fvisibility-11.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Fvisibility-11.c?ref=8c996513856f2769aee1730cb211050fef055fb5", "patch": "@@ -13,7 +13,7 @@\n extern void* memcpy (void *, const void *, __SIZE_TYPE__);\n #pragma GCC visibility pop\n \n-struct a { int a[1024]; };\n+struct a { int a[4096]; };\n \n extern void *bar (struct a *, struct a *, int);\n "}, {"sha": "a3999214cdf316651ca88f0f546ed83cb4c12f14", "filename": "gcc/testsuite/gcc.target/i386/memcpy-1.c", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fmemcpy-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8c996513856f2769aee1730cb211050fef055fb5/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fmemcpy-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fmemcpy-1.c?ref=8c996513856f2769aee1730cb211050fef055fb5", "patch": "@@ -9,9 +9,9 @@\n /* A and B are aligned, but we used to lose track of it.\n    Ensure that memcpy is inlined and alignment prologue is missing.  */\n \n-char a[900];\n-char b[900];\n+char a[2048];\n+char b[2048];\n t()\n {\n-  __builtin_memcpy (a,b,900);\n+  __builtin_memcpy (a,b,2048);\n }"}]}