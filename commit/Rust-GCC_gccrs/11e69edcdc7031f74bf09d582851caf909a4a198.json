{"sha": "11e69edcdc7031f74bf09d582851caf909a4a198", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MTFlNjllZGNkYzcwMzFmNzRiZjA5ZDU4Mjg1MWNhZjkwOWE0YTE5OA==", "commit": {"author": {"name": "Bernd Schmidt", "email": "bernds@codesourcery.com", "date": "2011-09-30T15:37:43Z"}, "committer": {"name": "Bernd Schmidt", "email": "bernds@gcc.gnu.org", "date": "2011-09-30T15:37:43Z"}, "message": "c6x-common.c (c6x_option_optimization_table): Enable -fmodulo-sched at -O2 and above.\n\n\t* common/config/c6x/c6x-common.c (c6x_option_optimization_table):\n\tEnable -fmodulo-sched at -O2 and above.\n\t* config/c6x/c6x.md (doloop_end): New expander.\n\t(mvilc, sploop, spkernel, loop_end): New patterns.\n\t(loop_end with memory destination splitter): New.\n\t* config/c6x/c6x.c: Include \"hw-doloop.h\".\n\t(enum unitreqs): New.\n\t(unit_req_table): New typedef.\n\t(unit_reqs): New static variable.\n\t(unit_req_factor, get_unit_reqs, count_unit_reqs, merge_unit_reqs,\n\tres_mii, split_delayed_nonbranch, undo_split_delayed_nonbranch,\n\thwloop_pattern_reg, bb_earliest_end_cycle, filter_insns_above,\n\thwloop_optimize, hwloop_fail, c6x_hwloops): New static functions.\n\t(struct c6x_sched_context): New member last_scheduled_iter0.\n\t(init_sched_state): Initialize it.\n\t(c6x_variable_issue): Update it.\n\t(sploop_max_uid_iter0): New static variable.\n\t(c6x_sched_reorder_1): Be careful about issuing sploop.\n\t(c6x_reorg): Call c6x_hwlooops before the final schedule.\n\nFrom-SVN: r179393", "tree": {"sha": "6d433974ec952d60a7a61ed475ee648cc78c8fb6", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/6d433974ec952d60a7a61ed475ee648cc78c8fb6"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/11e69edcdc7031f74bf09d582851caf909a4a198", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/11e69edcdc7031f74bf09d582851caf909a4a198", "html_url": "https://github.com/Rust-GCC/gccrs/commit/11e69edcdc7031f74bf09d582851caf909a4a198", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/11e69edcdc7031f74bf09d582851caf909a4a198/comments", "author": null, "committer": null, "parents": [{"sha": "fe780c134a209c1bc665883636d6b068e1abd97b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fe780c134a209c1bc665883636d6b068e1abd97b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/fe780c134a209c1bc665883636d6b068e1abd97b"}], "stats": {"total": 905, "additions": 898, "deletions": 7}, "files": [{"sha": "485e839129de1b591fe5de01b99822e7f3607fbb", "filename": "gcc/ChangeLog", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/11e69edcdc7031f74bf09d582851caf909a4a198/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/11e69edcdc7031f74bf09d582851caf909a4a198/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=11e69edcdc7031f74bf09d582851caf909a4a198", "patch": "@@ -1,3 +1,25 @@\n+2011-09-30  Bernd Schmidt  <bernds@codesourcery.com>\n+\n+\t* common/config/c6x/c6x-common.c (c6x_option_optimization_table):\n+\tEnable -fmodulo-sched at -O2 and above.\n+\t* config/c6x/c6x.md (doloop_end): New expander.\n+\t(mvilc, sploop, spkernel, loop_end): New patterns.\n+\t(loop_end with memory destination splitter): New.\n+\t* config/c6x/c6x.c: Include \"hw-doloop.h\".\n+\t(enum unitreqs): New.\n+\t(unit_req_table): New typedef.\n+\t(unit_reqs): New static variable.\n+\t(unit_req_factor, get_unit_reqs, count_unit_reqs, merge_unit_reqs,\n+\tres_mii, split_delayed_nonbranch, undo_split_delayed_nonbranch,\n+\thwloop_pattern_reg, bb_earliest_end_cycle, filter_insns_above,\n+\thwloop_optimize, hwloop_fail, c6x_hwloops): New static functions.\n+\t(struct c6x_sched_context): New member last_scheduled_iter0.\n+\t(init_sched_state): Initialize it.\n+\t(c6x_variable_issue): Update it.\n+\t(sploop_max_uid_iter0): New static variable.\n+\t(c6x_sched_reorder_1): Be careful about issuing sploop.\n+\t(c6x_reorg): Call c6x_hwlooops before the final schedule.\n+\n 2011-09-30  Georg-Johann Lay  <avr@gjlay.de>\n \n \tPR target/50566"}, {"sha": "a9c98af22663fb160fc60cc22090be9be8525744", "filename": "gcc/common/config/c6x/c6x-common.c", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/11e69edcdc7031f74bf09d582851caf909a4a198/gcc%2Fcommon%2Fconfig%2Fc6x%2Fc6x-common.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/11e69edcdc7031f74bf09d582851caf909a4a198/gcc%2Fcommon%2Fconfig%2Fc6x%2Fc6x-common.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcommon%2Fconfig%2Fc6x%2Fc6x-common.c?ref=11e69edcdc7031f74bf09d582851caf909a4a198", "patch": "@@ -33,6 +33,7 @@ static const struct default_options c6x_option_optimization_table[] =\n   {\n     { OPT_LEVELS_1_PLUS, OPT_fomit_frame_pointer, NULL, 1 },\n     { OPT_LEVELS_1_PLUS, OPT_frename_registers, NULL, 1 },\n+    { OPT_LEVELS_2_PLUS, OPT_fmodulo_sched, NULL, 1 },\n     { OPT_LEVELS_ALL, OPT_freciprocal_math, NULL, 1 },\n     { OPT_LEVELS_NONE, 0, NULL, 0 }\n   };"}, {"sha": "57e62d9b98de7001da97a229bdc5e6b8872a10cc", "filename": "gcc/config/c6x/c6x.c", "status": "modified", "additions": 775, "deletions": 7, "changes": 782, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/11e69edcdc7031f74bf09d582851caf909a4a198/gcc%2Fconfig%2Fc6x%2Fc6x.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/11e69edcdc7031f74bf09d582851caf909a4a198/gcc%2Fconfig%2Fc6x%2Fc6x.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fc6x%2Fc6x.c?ref=11e69edcdc7031f74bf09d582851caf909a4a198", "patch": "@@ -50,6 +50,7 @@\n #include \"sel-sched.h\"\n #include \"debug.h\"\n #include \"opts.h\"\n+#include \"hw-doloop.h\"\n \n /* Table of supported architecture variants.  */\n typedef struct\n@@ -160,6 +161,27 @@ static int c6x_unit_codes[ARRAY_SIZE (c6x_unit_names)];\n \n #define RESERVATION_S1 2\n #define RESERVATION_S2 10\n+\n+/* An enum for the unit requirements we count in the UNIT_REQS table.  */\n+enum unitreqs\n+{\n+  UNIT_REQ_D,\n+  UNIT_REQ_L,\n+  UNIT_REQ_S,\n+  UNIT_REQ_M,\n+  UNIT_REQ_DL,\n+  UNIT_REQ_DS,\n+  UNIT_REQ_LS,\n+  UNIT_REQ_DLS,\n+  UNIT_REQ_T,\n+  UNIT_REQ_X,\n+  UNIT_REQ_MAX\n+};\n+\n+/* A table used to count unit requirements.  Used when computing minimum\n+   iteration intervals.  */\n+typedef int unit_req_table[2][UNIT_REQ_MAX];\n+static unit_req_table unit_reqs;\n \f\n /* Register map for debugging.  */\n int const dbx_register_map[FIRST_PSEUDO_REGISTER] =\n@@ -3164,6 +3186,149 @@ assign_reservations (rtx head, rtx end)\n \t  }\n     }\n }\n+\n+/* Return a factor by which to weight unit imbalances for a reservation\n+   R.  */\n+static int\n+unit_req_factor (enum unitreqs r)\n+{\n+  switch (r)\n+    {\n+    case UNIT_REQ_D:\n+    case UNIT_REQ_L:\n+    case UNIT_REQ_S:\n+    case UNIT_REQ_M:\n+    case UNIT_REQ_X:\n+    case UNIT_REQ_T:\n+      return 1;\n+    case UNIT_REQ_DL:\n+    case UNIT_REQ_LS:\n+    case UNIT_REQ_DS:\n+      return 2;\n+    case UNIT_REQ_DLS:\n+      return 3;\n+    default:\n+      gcc_unreachable ();\n+    }\n+}\n+\n+/* Examine INSN, and store in REQ1/SIDE1 and REQ2/SIDE2 the unit\n+   requirements.  Returns zero if INSN can't be handled, otherwise\n+   either one or two to show how many of the two pairs are in use.\n+   REQ1 is always used, it holds what is normally thought of as the\n+   instructions reservation, e.g. UNIT_REQ_DL.  REQ2 is used to either\n+   describe a cross path, or for loads/stores, the T unit.  */\n+static int\n+get_unit_reqs (rtx insn, int *req1, int *side1, int *req2, int *side2)\n+{\n+  enum attr_units units;\n+  enum attr_cross cross;\n+  int side, req;\n+\n+  if (!NONDEBUG_INSN_P (insn) || recog_memoized (insn) < 0)\n+    return 0;\n+  units = get_attr_units (insn);\n+  if (units == UNITS_UNKNOWN)\n+    return 0;\n+  side = get_insn_side (insn, units);\n+  cross = get_attr_cross (insn);\n+\n+  req = (units == UNITS_D ? UNIT_REQ_D\n+\t : units == UNITS_D_ADDR ? UNIT_REQ_D\n+\t : units == UNITS_DL ? UNIT_REQ_DL\n+\t : units == UNITS_DS ? UNIT_REQ_DS\n+\t : units == UNITS_L ? UNIT_REQ_L\n+\t : units == UNITS_LS ? UNIT_REQ_LS\n+\t : units == UNITS_S ? UNIT_REQ_S\n+\t : units == UNITS_M ? UNIT_REQ_M\n+\t : units == UNITS_DLS ? UNIT_REQ_DLS\n+\t : -1);\n+  gcc_assert (req != -1);\n+  *req1 = req;\n+  *side1 = side;\n+  if (units == UNITS_D_ADDR)\n+    {\n+      *req2 = UNIT_REQ_T;\n+      *side2 = side ^ (cross == CROSS_Y ? 1 : 0);\n+      return 2;\n+    }\n+  else if (cross == CROSS_Y)\n+    {\n+      *req2 = UNIT_REQ_X;\n+      *side2 = side;\n+      return 2;\n+    }\n+  return 1;\n+}\n+\n+/* Walk the insns between and including HEAD and TAIL, and mark the\n+   resource requirements in the unit_reqs table.  */\n+static void\n+count_unit_reqs (unit_req_table reqs, rtx head, rtx tail)\n+{\n+  rtx insn;\n+\n+  memset (reqs, 0, sizeof (unit_req_table));\n+\n+  for (insn = head; insn != NEXT_INSN (tail); insn = NEXT_INSN (insn))\n+    {\n+      int side1, side2, req1, req2;\n+\n+      switch (get_unit_reqs (insn, &req1, &side1, &req2, &side2))\n+\t{\n+\tcase 2:\n+\t  reqs[side2][req2]++;\n+\t  /* fall through */\n+\tcase 1:\n+\t  reqs[side1][req1]++;\n+\t  break;\n+\t}\n+    }\n+}\n+\n+/* Update the table REQS by merging more specific unit reservations into\n+   more general ones, i.e. counting (for example) UNIT_REQ_D also in\n+   UNIT_REQ_DL, DS, and DLS.  */\n+static void\n+merge_unit_reqs (unit_req_table reqs)\n+{\n+  int side;\n+  for (side = 0; side < 2; side++)\n+    {\n+      int d = reqs[side][UNIT_REQ_D];\n+      int l = reqs[side][UNIT_REQ_L];\n+      int s = reqs[side][UNIT_REQ_S];\n+      int dl = reqs[side][UNIT_REQ_DL];\n+      int ls = reqs[side][UNIT_REQ_LS];\n+      int ds = reqs[side][UNIT_REQ_DS];\n+\n+      reqs[side][UNIT_REQ_DL] += d;\n+      reqs[side][UNIT_REQ_DL] += l;\n+      reqs[side][UNIT_REQ_DS] += d;\n+      reqs[side][UNIT_REQ_DS] += s;\n+      reqs[side][UNIT_REQ_LS] += l;\n+      reqs[side][UNIT_REQ_LS] += s;\n+      reqs[side][UNIT_REQ_DLS] += ds + dl + ls + d + l + s;\n+    }\n+}\n+\n+/* Return the resource-constrained minimum iteration interval given the\n+   data in the REQS table.  This must have been processed with\n+   merge_unit_reqs already.  */\n+static int\n+res_mii (unit_req_table reqs)\n+{\n+  int side, req;\n+  int worst = 1;\n+  for (side = 0; side < 2; side++)\n+    for (req = 0; req < UNIT_REQ_MAX; req++)\n+      {\n+\tint factor = unit_req_factor (req);\n+\tworst = MAX ((reqs[side][UNIT_REQ_D] + factor - 1) / factor, worst);\n+      }\n+\n+  return worst;\n+}\n \f\n /* Backend scheduling state.  */\n typedef struct c6x_sched_context\n@@ -3196,13 +3361,15 @@ typedef struct c6x_sched_context\n \n   /* The following variable value is the last issued insn.  */\n   rtx last_scheduled_insn;\n+  /* The last issued insn that isn't a shadow of another.  */\n+  rtx last_scheduled_iter0;\n \n   /* The following variable value is DFA state before issuing the\n      first insn in the current clock cycle.  We do not use this member\n      of the structure directly; we copy the data in and out of\n      prev_cycle_state.  */\n   state_t prev_cycle_state_ctx;\n-  \n+\n   int reg_n_accesses[FIRST_PSEUDO_REGISTER];\n   int reg_n_xaccesses[FIRST_PSEUDO_REGISTER];\n   int reg_set_in_cycle[FIRST_PSEUDO_REGISTER];\n@@ -3223,6 +3390,10 @@ static state_t prev_cycle_state;\n    many accesses of the same register.  */\n static bool reg_access_stall;\n \n+/* The highest insn uid after delayed insns were split, but before loop bodies\n+   were copied by the modulo scheduling code.  */\n+static int sploop_max_uid_iter0;\n+\n /* Look up the jump cycle with index N.  For an out-of-bounds N, we return 0,\n    so the caller does not specifically have to test for it.  */\n static int\n@@ -3421,6 +3592,7 @@ static void\n init_sched_state (c6x_sched_context_t sc)\n {\n   sc->last_scheduled_insn = NULL_RTX;\n+  sc->last_scheduled_iter0 = NULL_RTX;\n   sc->issued_this_cycle = 0;\n   memset (sc->jump_cycles, 0, sizeof sc->jump_cycles);\n   memset (sc->jump_cond, 0, sizeof sc->jump_cond);\n@@ -3481,7 +3653,7 @@ c6x_clear_sched_context (void *_sc)\n   c6x_sched_context_t sc = (c6x_sched_context_t) _sc;\n   gcc_assert (_sc != NULL);\n \n-  free (sc->prev_cycle_state_ctx); \n+  free (sc->prev_cycle_state_ctx);\n }\n \n /* Free _SC.  */\n@@ -3716,7 +3888,7 @@ c6x_sched_reorder_1 (rtx *ready, int *pn_ready, int clock_var)\n       bool is_asm = (icode < 0\n \t\t     && (GET_CODE (PATTERN (insn)) == ASM_INPUT\n \t\t\t || asm_noperands (PATTERN (insn)) >= 0));\n-      bool no_parallel = (is_asm\n+      bool no_parallel = (is_asm || icode == CODE_FOR_sploop\n \t\t\t  || (icode >= 0\n \t\t\t      && get_attr_type (insn) == TYPE_ATOMIC));\n \n@@ -3725,7 +3897,8 @@ c6x_sched_reorder_1 (rtx *ready, int *pn_ready, int clock_var)\n \t code always assumes at least 1 cycle, which may be wrong.  */\n       if ((no_parallel\n \t   && (ss.issued_this_cycle > 0 || clock_var < ss.delays_finished_at))\n-\t  || c6x_registers_update (insn))\n+\t  || c6x_registers_update (insn)\n+\t  || (ss.issued_this_cycle > 0 && icode == CODE_FOR_sploop))\n \t{\n \t  memmove (ready + 1, ready, (insnp - ready) * sizeof (rtx));\n \t  *ready = insn;\n@@ -3924,6 +4097,8 @@ c6x_variable_issue (FILE *dump ATTRIBUTE_UNUSED,\n \t\t    rtx insn, int can_issue_more ATTRIBUTE_UNUSED)\n {\n   ss.last_scheduled_insn = insn;\n+  if (INSN_UID (insn) < sploop_max_uid_iter0 && !JUMP_P (insn))\n+    ss.last_scheduled_iter0 = insn;\n   if (GET_CODE (PATTERN (insn)) != USE && GET_CODE (PATTERN (insn)) != CLOBBER)\n     ss.issued_this_cycle++;\n   if (insn_info)\n@@ -4813,6 +4988,117 @@ split_delayed_branch (rtx insn)\n   record_delay_slot_pair (i1, insn, 5, 0);\n }\n \n+/* If INSN is a multi-cycle insn that should be handled properly in\n+   modulo-scheduling, split it into a real insn and a shadow.\n+   Return true if we made a change.\n+\n+   It is valid for us to fail to split an insn; the caller has to deal\n+   with the possibility.  Currently we handle loads and most mpy2 and\n+   mpy4 insns.  */\n+static bool\n+split_delayed_nonbranch (rtx insn)\n+{\n+  int code = recog_memoized (insn);\n+  enum attr_type type;\n+  rtx i1, newpat, src, dest;\n+  rtx pat = PATTERN (insn);\n+  rtvec rtv;\n+  int delay;\n+\n+  if (GET_CODE (pat) == COND_EXEC)\n+    pat = COND_EXEC_CODE (pat);\n+\n+  if (code < 0 || GET_CODE (pat) != SET)\n+    return false;\n+  src = SET_SRC (pat);\n+  dest = SET_DEST (pat);\n+  if (!REG_P (dest))\n+    return false;\n+\n+  type = get_attr_type (insn);\n+  if (code >= 0\n+      && (type == TYPE_LOAD\n+\t  || type == TYPE_LOADN))\n+    {\n+      if (!MEM_P (src)\n+\t  && (GET_CODE (src) != ZERO_EXTEND\n+\t      || !MEM_P (XEXP (src, 0))))\n+\treturn false;\n+\n+      if (GET_MODE_SIZE (GET_MODE (dest)) > 4\n+\t  && (GET_MODE_SIZE (GET_MODE (dest)) != 8 || !TARGET_LDDW))\n+\treturn false;\n+\n+      rtv = gen_rtvec (2, GEN_INT (REGNO (SET_DEST (pat))),\n+\t\t       SET_SRC (pat));\n+      newpat = gen_load_shadow (SET_DEST (pat));\n+      pat = gen_rtx_UNSPEC (VOIDmode, rtv, UNSPEC_REAL_LOAD);\n+      delay = 4;\n+    }\n+  else if (code >= 0\n+\t   && (type == TYPE_MPY2\n+\t       || type == TYPE_MPY4))\n+    {\n+      /* We don't handle floating point multiplies yet.  */\n+      if (GET_MODE (dest) == SFmode)\n+\treturn false;\n+\n+      rtv = gen_rtvec (2, GEN_INT (REGNO (SET_DEST (pat))),\n+\t\t       SET_SRC (pat));\n+      newpat = gen_mult_shadow (SET_DEST (pat));\n+      pat = gen_rtx_UNSPEC (VOIDmode, rtv, UNSPEC_REAL_MULT);\n+      delay = type == TYPE_MPY2 ? 1 : 3;\n+    }\n+  else\n+    return false;\n+\n+  pat = duplicate_cond (pat, insn);\n+  newpat = duplicate_cond (newpat, insn);\n+  i1 = emit_insn_before (pat, insn);\n+  PATTERN (insn) = newpat;\n+  INSN_CODE (insn) = -1;\n+  recog_memoized (insn);\n+  recog_memoized (i1);\n+  record_delay_slot_pair (i1, insn, delay, 0);\n+  return true;\n+}\n+\n+/* Examine if INSN is the result of splitting a load into a real load and a\n+   shadow, and if so, undo the transformation.  */\n+static void\n+undo_split_delayed_nonbranch (rtx insn)\n+{\n+  int icode = recog_memoized (insn);\n+  enum attr_type type;\n+  rtx prev_pat, insn_pat, prev;\n+\n+  if (icode < 0)\n+    return;\n+  type = get_attr_type (insn);\n+  if (type != TYPE_LOAD_SHADOW && type != TYPE_MULT_SHADOW)\n+    return;\n+  prev = PREV_INSN (insn);\n+  prev_pat = PATTERN (prev);\n+  insn_pat = PATTERN (insn);\n+  if (GET_CODE (prev_pat) == COND_EXEC)\n+    {\n+      prev_pat = COND_EXEC_CODE (prev_pat);\n+      insn_pat = COND_EXEC_CODE (insn_pat);\n+    }\n+\n+  gcc_assert (GET_CODE (prev_pat) == UNSPEC\n+\t      && ((XINT (prev_pat, 1) == UNSPEC_REAL_LOAD\n+\t\t   && type == TYPE_LOAD_SHADOW)\n+\t\t  || (XINT (prev_pat, 1) == UNSPEC_REAL_MULT\n+\t\t      && type == TYPE_MULT_SHADOW)));\n+  insn_pat = gen_rtx_SET (VOIDmode, SET_DEST (insn_pat),\n+\t\t\t  XVECEXP (prev_pat, 0, 1));\n+  insn_pat = duplicate_cond (insn_pat, prev);\n+  PATTERN (insn) = insn_pat;\n+  INSN_CODE (insn) = -1;\n+  delete_insn (prev);\n+}\n+\n /* Split every insn (i.e. jumps and calls) which can have delay slots into\n    two parts: the first one is scheduled normally and emits the instruction,\n    while the second one is a shadow insn which shows the side effect taking\n@@ -4853,6 +5139,481 @@ conditionalize_after_sched (void)\n       }\n }\n \n+/* A callback for the hw-doloop pass.  This function examines INSN; if\n+   it is a loop_end pattern we recognize, return the reg rtx for the\n+   loop counter.  Otherwise, return NULL_RTX.  */\n+\n+static rtx\n+hwloop_pattern_reg (rtx insn)\n+{\n+  rtx pat, reg;\n+\n+  if (!JUMP_P (insn) || recog_memoized (insn) != CODE_FOR_loop_end)\n+    return NULL_RTX;\n+\n+  pat = PATTERN (insn);\n+  reg = SET_DEST (XVECEXP (pat, 0, 1));\n+  if (!REG_P (reg))\n+    return NULL_RTX;\n+  return reg;\n+}\n+\n+/* Return the number of cycles taken by BB, as computed by scheduling,\n+   including the latencies of all insns with delay slots.  IGNORE is\n+   an insn we should ignore in the calculation, usually the final\n+   branch.  */\n+static int\n+bb_earliest_end_cycle (basic_block bb, rtx ignore)\n+{\n+  int earliest = 0;\n+  rtx insn;\n+\n+  FOR_BB_INSNS (bb, insn)\n+    {\n+      int cycles, this_clock;\n+\n+      if (LABEL_P (insn) || NOTE_P (insn) || DEBUG_INSN_P (insn)\n+\t  || GET_CODE (PATTERN (insn)) == USE\n+\t  || GET_CODE (PATTERN (insn)) == CLOBBER\n+\t  || insn == ignore)\n+\tcontinue;\n+\n+      this_clock = insn_get_clock (insn);\n+      cycles = get_attr_cycles (insn);\n+\n+      if (earliest < this_clock + cycles)\n+\tearliest = this_clock + cycles;\n+    }\n+  return earliest;\n+}\n+\n+/* Examine the insns in BB and remove all which have a uid greater or\n+   equal to MAX_UID.  */\n+static void\n+filter_insns_above (basic_block bb, int max_uid)\n+{\n+  rtx insn, next;\n+  bool prev_ti = false;\n+  int prev_cycle = -1;\n+\n+  FOR_BB_INSNS_SAFE (bb, insn, next)\n+    {\n+      int this_cycle;\n+      if (!NONDEBUG_INSN_P (insn))\n+\tcontinue;\n+      if (insn == BB_END (bb))\n+\treturn;\n+      this_cycle = insn_get_clock (insn);\n+      if (prev_ti && this_cycle == prev_cycle)\n+\t{\n+\t  gcc_assert (GET_MODE (insn) != TImode);\n+\t  PUT_MODE (insn, TImode);\n+\t}\n+      prev_ti = false;\n+      if (INSN_UID (insn) >= max_uid)\n+\t{\n+\t  if (GET_MODE (insn) == TImode)\n+\t    {\n+\t      prev_ti = true;\n+\t      prev_cycle = this_cycle;\n+\t    }\n+\t  delete_insn (insn);\n+\t}\n+    }\n+}\n+\n+/* A callback for the hw-doloop pass.  Called to optimize LOOP in a\n+   machine-specific fashion; returns true if successful and false if\n+   the hwloop_fail function should be called.  */\n+\n+static bool\n+hwloop_optimize (hwloop_info loop)\n+{\n+  basic_block entry_bb, bb;\n+  rtx seq, insn, prev, entry_after, end_packet;\n+  rtx head_insn, tail_insn, new_insns, last_insn;\n+  int loop_earliest, entry_earliest, entry_end_cycle;\n+  int n_execute_packets;\n+  edge entry_edge;\n+  unsigned ix;\n+  int max_uid_before, delayed_splits;\n+  int i, sp_ii, min_ii, max_ii, max_parallel, n_insns, n_real_insns, stages;\n+  rtx *orig_vec;\n+  rtx *copies;\n+  rtx **insn_copies;\n+\n+  if (!c6x_flag_modulo_sched || !c6x_flag_schedule_insns2\n+      || !TARGET_INSNS_64PLUS)\n+    return false;\n+\n+  if (loop->iter_reg_used || loop->depth > 1)\n+    return false;\n+  if (loop->has_call || loop->has_asm)\n+    return false;\n+\n+  if (loop->head != loop->tail)\n+    return false;\n+\n+  gcc_assert (loop->incoming_dest == loop->head);\n+\n+  entry_edge = NULL;\n+  FOR_EACH_VEC_ELT (edge, loop->incoming, i, entry_edge)\n+    if (entry_edge->flags & EDGE_FALLTHRU)\n+      break;\n+  if (entry_edge == NULL)\n+    return false;\n+\n+  schedule_ebbs_init ();\n+  schedule_ebb (BB_HEAD (loop->tail), loop->loop_end, true);\n+  schedule_ebbs_finish ();\n+\n+  bb = loop->head;\n+  loop_earliest = bb_earliest_end_cycle (bb, loop->loop_end) + 1;\n+\n+  max_uid_before = get_max_uid ();\n+\n+  /* Split all multi-cycle operations, such as loads.  For normal\n+     scheduling, we only do this for branches, as the generated code\n+     would otherwise not be interrupt-safe.  When using sploop, it is\n+     safe and beneficial to split them.  If any multi-cycle operations\n+     remain after splitting (because we don't handle them yet), we\n+     cannot pipeline the loop.  */\n+  delayed_splits = 0;\n+  FOR_BB_INSNS (bb, insn)\n+    {\n+      if (NONDEBUG_INSN_P (insn))\n+\t{\n+\t  recog_memoized (insn);\n+\t  if (split_delayed_nonbranch (insn))\n+\t    delayed_splits++;\n+\t  else if (INSN_CODE (insn) >= 0\n+\t\t   && get_attr_cycles (insn) > 1)\n+\t    goto undo_splits;\n+\t}\n+    }\n+\n+  /* Count the number of insns as well as the number real insns, and save\n+     the original sequence of insns in case we must restore it later.  */\n+  n_insns = n_real_insns = 0;\n+  FOR_BB_INSNS (bb, insn)\n+    {\n+      n_insns++;\n+      if (NONDEBUG_INSN_P (insn) && insn != loop->loop_end)\n+\tn_real_insns++;\n+    }\n+  orig_vec = XNEWVEC (rtx, n_insns);\n+  n_insns = 0;\n+  FOR_BB_INSNS (bb, insn)\n+    orig_vec[n_insns++] = insn;\n+\n+  /* Count the unit reservations, and compute a minimum II from that\n+     table.  */\n+  count_unit_reqs (unit_reqs, loop->start_label,\n+\t\t   PREV_INSN (loop->loop_end));\n+  merge_unit_reqs (unit_reqs);\n+\n+  min_ii = res_mii (unit_reqs);\n+  max_ii = loop_earliest < 15 ? loop_earliest : 14;\n+\n+  /* Make copies of the loop body, up to a maximum number of stages we want\n+     to handle.  */\n+  max_parallel = loop_earliest / min_ii + 1;\n+\n+  copies = XCNEWVEC (rtx, (max_parallel + 1) * n_real_insns);\n+  insn_copies = XNEWVEC (rtx *, max_parallel + 1);\n+  for (i = 0; i < max_parallel + 1; i++)\n+    insn_copies[i] = copies + i * n_real_insns;\n+\n+  head_insn = next_nonnote_nondebug_insn (loop->start_label);\n+  tail_insn = prev_real_insn (BB_END (bb));\n+\n+  i = 0;\n+  FOR_BB_INSNS (bb, insn)\n+    if (NONDEBUG_INSN_P (insn) && insn != loop->loop_end)\n+      insn_copies[0][i++] = insn;\n+\n+  sploop_max_uid_iter0 = get_max_uid ();\n+\n+  /* Generate the copies of the loop body, and save them in the\n+     INSN_COPIES array.  */\n+  start_sequence ();\n+  for (i = 0; i < max_parallel; i++)\n+    {\n+      int j;\n+      rtx this_iter;\n+\n+      this_iter = duplicate_insn_chain (head_insn, tail_insn);\n+      j = 0;\n+      while (this_iter)\n+\t{\n+\t  rtx prev_stage_insn = insn_copies[i][j];\n+\t  gcc_assert (INSN_CODE (this_iter) == INSN_CODE (prev_stage_insn));\n+\n+\t  if (INSN_CODE (this_iter) >= 0\n+\t      && (get_attr_type (this_iter) == TYPE_LOAD_SHADOW\n+\t\t  || get_attr_type (this_iter) == TYPE_MULT_SHADOW))\n+\t    {\n+\t      rtx prev = PREV_INSN (this_iter);\n+\t      record_delay_slot_pair (prev, this_iter,\n+\t\t\t\t      get_attr_cycles (prev) - 1, 0);\n+\t    }\n+\t  else\n+\t    record_delay_slot_pair (prev_stage_insn, this_iter, i, 1);\n+\n+\t  insn_copies[i + 1][j] = this_iter;\n+\t  j++;\n+\t  this_iter = next_nonnote_nondebug_insn (this_iter);\n+\t}\n+    }\n+  new_insns = get_insns ();\n+  last_insn = insn_copies[max_parallel][n_real_insns - 1];\n+  end_sequence ();\n+  emit_insn_before (new_insns, BB_END (bb));\n+\n+  /* Try to schedule the loop using varying initiation intervals,\n+     starting with the smallest possible and incrementing it\n+     on failure.  */\n+  for (sp_ii = min_ii; sp_ii <= max_ii; sp_ii++)\n+    {\n+      basic_block tmp_bb;\n+      if (dump_file)\n+\tfprintf (dump_file, \"Trying to schedule for II %d\\n\", sp_ii);\n+\n+      df_clear_flags (DF_LR_RUN_DCE);\n+\n+      schedule_ebbs_init ();\n+      set_modulo_params (sp_ii, max_parallel, n_real_insns,\n+\t\t\t sploop_max_uid_iter0);\n+      tmp_bb = schedule_ebb (BB_HEAD (bb), last_insn, true);\n+      schedule_ebbs_finish ();\n+\n+      if (tmp_bb)\n+\t{\n+\t  if (dump_file)\n+\t    fprintf (dump_file, \"Found schedule with II %d\\n\", sp_ii);\n+\t  break;\n+\t}\n+    }\n+\n+  discard_delay_pairs_above (max_uid_before);\n+\n+  if (sp_ii > max_ii)\n+    goto restore_loop;\n+\n+  stages = insn_get_clock (ss.last_scheduled_iter0) / sp_ii + 1;\n+\n+  if (stages == 1 && sp_ii > 5)\n+    goto restore_loop;\n+\n+  /* At this point, we know we've been successful, unless we find later that\n+     there are too many execute packets for the loop buffer to hold.  */\n+\n+  /* Assign reservations to the instructions in the loop.  We must find\n+     the stage that contains the full loop kernel, and transfer the\n+     reservations of the instructions contained in it to the corresponding\n+     instructions from iteration 0, which are the only ones we'll keep.  */\n+  assign_reservations (BB_HEAD (bb), ss.last_scheduled_insn);\n+  PREV_INSN (BB_END (bb)) = ss.last_scheduled_iter0;\n+  NEXT_INSN (ss.last_scheduled_iter0) = BB_END (bb);\n+  filter_insns_above (bb, sploop_max_uid_iter0);\n+\n+  for (i = 0; i < n_real_insns; i++)\n+    {\n+      rtx insn = insn_copies[0][i];\n+      int uid = INSN_UID (insn);\n+      int stage = insn_uid_get_clock (uid) / sp_ii;\n+\n+      if (stage + 1 < stages)\n+\t{\n+\t  int copy_uid;\n+\t  stage = stages - stage - 1;\n+\t  copy_uid = INSN_UID (insn_copies[stage][i]);\n+\t  INSN_INFO_ENTRY (uid).reservation\n+\t    = INSN_INFO_ENTRY (copy_uid).reservation;\n+\t}\n+    }\n+  if (stages == 1)\n+    stages++;\n+\n+  /* Compute the number of execute packets the pipelined form of the loop will\n+     require.  */\n+  prev = NULL_RTX;\n+  n_execute_packets = 0;\n+  for (insn = loop->start_label; insn != loop->loop_end; insn = NEXT_INSN (insn))\n+    {\n+      if (NONDEBUG_INSN_P (insn) && GET_MODE (insn) == TImode\n+\t  && !shadow_p (insn))\n+\t{\n+\t  n_execute_packets++;\n+\t  if (prev && insn_get_clock (prev) + 1 != insn_get_clock (insn))\n+\t    /* We need an extra NOP instruction.  */\n+\t    n_execute_packets++;\n+\n+\t  prev = insn;\n+\t}\n+    }\n+\n+  end_packet = ss.last_scheduled_iter0;\n+  while (!NONDEBUG_INSN_P (end_packet) || GET_MODE (end_packet) != TImode)\n+    end_packet = PREV_INSN (end_packet);\n+\n+  /* The earliest cycle in which we can emit the SPKERNEL instruction.  */\n+  loop_earliest = (stages - 1) * sp_ii;\n+  if (loop_earliest > insn_get_clock (end_packet))\n+    {\n+      n_execute_packets++;\n+      end_packet = loop->loop_end;\n+    }\n+  else\n+    loop_earliest = insn_get_clock (end_packet);\n+\n+  if (n_execute_packets > 14)\n+    goto restore_loop;\n+\n+  /* Generate the spkernel instruction, and place it at the appropriate\n+     spot.  */\n+  PUT_MODE (end_packet, VOIDmode);\n+\n+  insn = gen_spkernel (GEN_INT (stages - 1),\n+\t\t       const0_rtx, JUMP_LABEL (loop->loop_end));\n+  insn = emit_jump_insn_before (insn, end_packet);\n+  JUMP_LABEL (insn) = JUMP_LABEL (loop->loop_end);\n+  insn_set_clock (insn, loop_earliest);\n+  PUT_MODE (insn, TImode);\n+  INSN_INFO_ENTRY (INSN_UID (insn)).ebb_start = false;\n+  delete_insn (loop->loop_end);\n+\n+  /* Place the mvc and sploop instructions before the loop.  */\n+  entry_bb = entry_edge->src;\n+\n+  start_sequence ();\n+\n+  insn = emit_insn (gen_mvilc (loop->iter_reg));\n+  insn = emit_insn (gen_sploop (GEN_INT (sp_ii)));\n+\n+  seq = get_insns ();\n+\n+  if (!single_succ_p (entry_bb) || VEC_length (edge, loop->incoming) > 1)\n+    {\n+      basic_block new_bb;\n+      edge e;\n+      edge_iterator ei;\n+\n+      emit_insn_before (seq, BB_HEAD (loop->head));\n+      seq = emit_label_before (gen_label_rtx (), seq);\n+\n+      new_bb = create_basic_block (seq, insn, entry_bb);\n+      FOR_EACH_EDGE (e, ei, loop->incoming)\n+\t{\n+\t  if (!(e->flags & EDGE_FALLTHRU))\n+\t    redirect_edge_and_branch_force (e, new_bb);\n+\t  else\n+\t    redirect_edge_succ (e, new_bb);\n+\t}\n+      make_edge (new_bb, loop->head, 0);\n+    }\n+  else\n+    {\n+      entry_after = BB_END (entry_bb);\n+      while (DEBUG_INSN_P (entry_after)\n+\t     || (NOTE_P (entry_after)\n+\t\t && NOTE_KIND (entry_after) != NOTE_INSN_BASIC_BLOCK))\n+\tentry_after = PREV_INSN (entry_after);\n+      emit_insn_after (seq, entry_after);\n+    }\n+\n+  end_sequence ();\n+\n+  /* Make sure we don't try to schedule this loop again.  */\n+  for (ix = 0; VEC_iterate (basic_block, loop->blocks, ix, bb); ix++)\n+    bb->flags |= BB_DISABLE_SCHEDULE;\n+\n+  return true;\n+\n+ restore_loop:\n+  if (dump_file)\n+    fprintf (dump_file, \"Unable to pipeline loop.\\n\");\n+\n+  for (i = 1; i < n_insns; i++)\n+    {\n+      NEXT_INSN (orig_vec[i - 1]) = orig_vec[i];\n+      PREV_INSN (orig_vec[i]) = orig_vec[i - 1];\n+    }\n+  PREV_INSN (orig_vec[0]) = PREV_INSN (BB_HEAD (bb));\n+  NEXT_INSN (PREV_INSN (BB_HEAD (bb))) = orig_vec[0];\n+  NEXT_INSN (orig_vec[n_insns - 1]) = NEXT_INSN (BB_END (bb));\n+  PREV_INSN (NEXT_INSN (BB_END (bb))) = orig_vec[n_insns - 1];\n+  BB_HEAD (bb) = orig_vec[0];\n+  BB_END (bb) = orig_vec[n_insns - 1];\n+ undo_splits:\n+  free_delay_pairs ();\n+  FOR_BB_INSNS (bb, insn)\n+    if (NONDEBUG_INSN_P (insn))\n+      undo_split_delayed_nonbranch (insn);\n+  return false;\n+}\n+\n+/* A callback for the hw-doloop pass.  Called when a loop we have discovered\n+   turns out not to be optimizable; we have to split the doloop_end pattern\n+   into a subtract and a test.  */\n+static void\n+hwloop_fail (hwloop_info loop)\n+{\n+  rtx insn, test, testreg;\n+\n+  if (dump_file)\n+    fprintf (dump_file, \"splitting doloop insn %d\\n\",\n+\t     INSN_UID (loop->loop_end));\n+  insn = gen_addsi3 (loop->iter_reg, loop->iter_reg, constm1_rtx);\n+  /* See if we can emit the add at the head of the loop rather than at the\n+     end.  */\n+  if (loop->head == NULL\n+      || loop->iter_reg_used_outside\n+      || loop->iter_reg_used\n+      || TEST_HARD_REG_BIT (loop->regs_set_in_loop, REGNO (loop->iter_reg))\n+      || loop->incoming_dest != loop->head\n+      || EDGE_COUNT (loop->head->preds) != 2)\n+    emit_insn_before (insn, loop->loop_end);\n+  else\n+    {\n+      rtx t = loop->start_label;\n+      while (!NOTE_P (t) || NOTE_KIND (t) != NOTE_INSN_BASIC_BLOCK)\n+\tt = NEXT_INSN (t);\n+      emit_insn_after (insn, t);\n+    }\n+\n+  testreg = SET_DEST (XVECEXP (PATTERN (loop->loop_end), 0, 2));\n+  if (GET_CODE (testreg) == SCRATCH)\n+    testreg = loop->iter_reg;\n+  else\n+    emit_insn_before (gen_movsi (testreg, loop->iter_reg), loop->loop_end);\n+\n+  test = gen_rtx_NE (VOIDmode, testreg, const0_rtx);\n+  insn = emit_jump_insn_before (gen_cbranchsi4 (test, testreg, const0_rtx,\n+\t\t\t\t\t\tloop->start_label),\n+\t\t\t\tloop->loop_end);\n+\n+  JUMP_LABEL (insn) = loop->start_label;\n+  LABEL_NUSES (loop->start_label)++;\n+  delete_insn (loop->loop_end);\n+}\n+\n+static struct hw_doloop_hooks c6x_doloop_hooks =\n+{\n+  hwloop_pattern_reg,\n+  hwloop_optimize,\n+  hwloop_fail\n+};\n+\n+/* Run the hw-doloop pass to modulo-schedule hardware loops, or split the\n+   doloop_end patterns where such optimizations are impossible.  */\n+static void\n+c6x_hwloops (void)\n+{\n+  if (optimize)\n+    reorg_loops (true, &c6x_doloop_hooks);\n+}\n+\n /* Implement the TARGET_MACHINE_DEPENDENT_REORG pass.  We split call insns here\n    into a sequence that loads the return register and performs the call,\n    and emit the return label.\n@@ -4881,10 +5642,17 @@ c6x_reorg (void)\n       int sz = get_max_uid () * 3 / 2 + 1;\n \n       insn_info = VEC_alloc (c6x_sched_insn_info, heap, sz);\n+    }\n+\n+  /* Make sure the real-jump insns we create are not deleted.  When modulo-\n+     scheduling, situations where a reg is only stored in a loop can also\n+     cause dead code when doing the initial unrolling.  */\n+  sched_no_dce = true;\n \n-      /* Make sure the real-jump insns we create are not deleted.  */\n-      sched_no_dce = true;\n+  c6x_hwloops ();\n \n+  if (c6x_flag_schedule_insns2)\n+    {\n       split_delayed_insns ();\n       timevar_push (TV_SCHED2);\n       if (do_selsched)\n@@ -4895,8 +5663,8 @@ c6x_reorg (void)\n       timevar_pop (TV_SCHED2);\n \n       free_delay_pairs ();\n-      sched_no_dce = false;\n     }\n+  sched_no_dce = false;\n \n   call_labels = XCNEWVEC (rtx, get_max_uid () + 1);\n "}, {"sha": "c2196ac6fe29f75a50b4d66e0bad40c5cb6c1075", "filename": "gcc/config/c6x/c6x.md", "status": "modified", "additions": 100, "deletions": 0, "changes": 100, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/11e69edcdc7031f74bf09d582851caf909a4a198/gcc%2Fconfig%2Fc6x%2Fc6x.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/11e69edcdc7031f74bf09d582851caf909a4a198/gcc%2Fconfig%2Fc6x%2Fc6x.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fc6x%2Fc6x.md?ref=11e69edcdc7031f74bf09d582851caf909a4a198", "patch": "@@ -1390,6 +1390,106 @@\n   }\"\n )\n \n+;; -------------------------------------------------------------------------\n+;; Doloop\n+;; -------------------------------------------------------------------------\n+\n+; operand 0 is the loop count pseudo register\n+; operand 1 is the number of loop iterations or 0 if it is unknown\n+; operand 2 is the maximum number of loop iterations\n+; operand 3 is the number of levels of enclosed loops\n+; operand 4 is the label to jump to at the top of the loop\n+(define_expand \"doloop_end\"\n+  [(parallel [(set (pc) (if_then_else\n+\t\t\t  (ne (match_operand:SI 0 \"\" \"\")\n+\t\t\t      (const_int 1))\n+\t\t\t  (label_ref (match_operand 4 \"\" \"\"))\n+\t\t\t  (pc)))\n+\t      (set (match_dup 0)\n+\t\t   (plus:SI (match_dup 0)\n+\t\t\t    (const_int -1)))\n+\t      (clobber (match_scratch:SI 5 \"\"))])]\n+  \"TARGET_INSNS_64PLUS && optimize\"\n+{\n+  /* The loop optimizer doesn't check the predicates... */\n+  if (GET_MODE (operands[0]) != SImode)\n+    FAIL;\n+})\n+\n+(define_insn \"mvilc\"\n+  [(set (reg:SI REG_ILC)\n+\t(unspec [(match_operand:SI 0 \"register_operand\" \"a,b\")] UNSPEC_MVILC))]\n+  \"TARGET_INSNS_64PLUS\"\n+  \"%|%.\\\\tmvc\\\\t%$\\\\t%0, ILC\"\n+  [(set_attr \"predicable\" \"no\")\n+   (set_attr \"cross\" \"y,n\")\n+   (set_attr \"units\" \"s\")\n+   (set_attr \"dest_regfile\" \"b\")\n+   (set_attr \"type\" \"mvilc\")])\n+  \n+(define_insn \"sploop\"\n+  [(unspec_volatile [(match_operand:SI 0 \"const_int_operand\" \"i\")\n+\t\t     (reg:SI REG_ILC)]\n+\t\t    UNSPECV_SPLOOP)]\n+  \"TARGET_INSNS_64PLUS\"\n+  \"%|%.\\\\tsploop\\t%0\"\n+  [(set_attr \"predicable\" \"no\")\n+   (set_attr \"type\" \"sploop\")])\n+  \n+(define_insn \"spkernel\"\n+  [(set (pc)\n+\t(if_then_else\n+\t (ne (unspec_volatile:SI\n+\t      [(match_operand:SI 0 \"const_int_operand\" \"i\")\n+\t       (match_operand:SI 1 \"const_int_operand\" \"i\")]\n+\t      UNSPECV_SPKERNEL)\n+\t     (const_int 1))\n+\t (label_ref (match_operand 2 \"\" \"\"))\n+\t (pc)))]\n+  \"TARGET_INSNS_64PLUS\"\n+  \"%|%.\\\\tspkernel\\t%0, %1\"\n+  [(set_attr \"predicable\" \"no\")\n+   (set_attr \"type\" \"spkernel\")])\n+  \n+(define_insn \"loop_end\"\n+  [(set (pc)\n+\t(if_then_else (ne (match_operand:SI 3 \"nonimmediate_operand\" \"0,0,0,*r\")\n+\t\t\t  (const_int 1))\n+\t\t      (label_ref (match_operand 1 \"\" \"\"))\n+\t\t      (pc)))\n+   (set (match_operand:SI 0 \"nonimmediate_operand\" \"=AB,*r,m,m\")\n+\t(plus:SI (match_dup 3)\n+\t\t (const_int -1)))\n+   (clobber (match_scratch:SI 2 \"=X,&AB,&AB,&AB\"))]\n+  \"TARGET_INSNS_64PLUS && optimize\"\n+  \"#\"\n+  [(set_attr \"type\" \"spkernel\")])\n+\n+(define_split\n+  [(set (pc)\n+\t(if_then_else (ne (match_operand:SI 3 \"nonimmediate_operand\" \"\")\n+\t\t\t  (const_int 1))\n+\t\t      (label_ref (match_operand 1 \"\" \"\"))\n+\t\t      (pc)))\n+   (set (match_operand:SI 0 \"memory_operand\" \"\")\n+\t(plus:SI (match_dup 3)\n+\t\t (const_int -1)))\n+   (clobber (match_scratch 2))]\n+  \"\"\n+  [(set (match_dup 2) (plus:SI (match_dup 3) (const_int -1)))\n+   (set (match_dup 0) (match_dup 2))\n+   (set (pc)\n+\t(if_then_else (ne (match_dup 2) (const_int 0))\n+\t\t      (label_ref (match_dup 1))\n+\t\t      (pc)))]\n+{\n+  if (!REG_P (operands[3]))\n+    {\n+      emit_move_insn (operands[2], operands[3]);\n+      operands[3] = operands[2];\n+    }\n+})\n+\n ;; -------------------------------------------------------------------------\n ;; Delayed-branch real jumps and shadows\n ;; -------------------------------------------------------------------------"}]}