{"sha": "f6e93b7b48195037d6c545104c952b97e05ad381", "node_id": "C_kwDOANBUbNoAKGY2ZTkzYjdiNDgxOTUwMzdkNmM1NDUxMDRjOTUyYjk3ZTA1YWQzODE", "commit": {"author": {"name": "Jeff Law", "email": "jlaw@ventanamicro.com", "date": "2022-10-17T23:33:52Z"}, "committer": {"name": "Jeff Law", "email": "jlaw@ventanamicro.com", "date": "2022-10-17T23:36:42Z"}, "message": "Remove accidential commits\n\ngcc/\n\t* config/i386/cet.c: Remove accidental commit.\n\t* config/i386/driver-mingw32.c: Likewise.\n\t* config/i386/i386-builtins.c: Likewise.\n\t* config/i386/i386-d.c:  Likewise.\n\t* config/i386/i386-expand.c: Likewise.\n\t* config/i386/i386-features.c: Likewise.\n\t* config/i386/i386-options.c: Likewise.\n\t* config/i386/t-cet: Likewise.\n\t* config/i386/x86-tune-sched-atom.c: Likewise.\n\t* config/i386/x86-tune-sched-bd.c: Likewise.\n\t* config/i386/x86-tune-sched-core.c: Likewise.\n\t* config/i386/x86-tune-sched.c: Likewise.", "tree": {"sha": "059db5d7abb81d2d77f98cd0970aada4b5813e29", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/059db5d7abb81d2d77f98cd0970aada4b5813e29"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/f6e93b7b48195037d6c545104c952b97e05ad381", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f6e93b7b48195037d6c545104c952b97e05ad381", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f6e93b7b48195037d6c545104c952b97e05ad381", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f6e93b7b48195037d6c545104c952b97e05ad381/comments", "author": null, "committer": null, "parents": [{"sha": "566c5f1aaae120d2283103e68ecf1c1a83dd4459", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/566c5f1aaae120d2283103e68ecf1c1a83dd4459", "html_url": "https://github.com/Rust-GCC/gccrs/commit/566c5f1aaae120d2283103e68ecf1c1a83dd4459"}], "stats": {"total": 31671, "additions": 0, "deletions": 31671}, "files": [{"sha": "5450ac307d59b85df53d5e2a67f8625684964bf3", "filename": "gcc/config/i386/cet.c", "status": "removed", "additions": 0, "deletions": 76, "changes": 76, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fcet.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fcet.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fcet.c?ref=566c5f1aaae120d2283103e68ecf1c1a83dd4459", "patch": "@@ -1,76 +0,0 @@\n-/* Functions for CET/x86.\n-   Copyright (C) 2017-2020 Free Software Foundation, Inc.\n-\n-This file is part of GCC.\n-\n-GCC is free software; you can redistribute it and/or modify\n-it under the terms of the GNU General Public License as published by\n-the Free Software Foundation; either version 3, or (at your option)\n-any later version.\n-\n-GCC is distributed in the hope that it will be useful,\n-but WITHOUT ANY WARRANTY; without even the implied warranty of\n-MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n-GNU General Public License for more details.\n-\n-You should have received a copy of the GNU General Public License\n-along with GCC; see the file COPYING3.  If not see\n-<http://www.gnu.org/licenses/>.  */\n-\n-#include \"config.h\"\n-#include \"system.h\"\n-#include \"coretypes.h\"\n-#include \"tm.h\"\n-#include \"output.h\"\n-#include \"linux-common.h\"\n-\n-void\n-file_end_indicate_exec_stack_and_cet (void)\n-{\n-  file_end_indicate_exec_stack ();\n-\n-  if (flag_cf_protection == CF_NONE)\n-    return;\n-\n-  unsigned int feature_1 = 0;\n-\n-  if (flag_cf_protection & CF_BRANCH)\n-    /* GNU_PROPERTY_X86_FEATURE_1_IBT.  */\n-    feature_1 |= 0x1;\n-\n-  if (flag_cf_protection & CF_RETURN)\n-    /* GNU_PROPERTY_X86_FEATURE_1_SHSTK.  */\n-    feature_1 |= 0x2;\n-\n-  if (feature_1)\n-    {\n-      int p2align = ptr_mode == SImode ? 2 : 3;\n-\n-      /* Generate GNU_PROPERTY_X86_FEATURE_1_XXX.  */\n-      switch_to_section (get_section (\".note.gnu.property\",\n-\t\t\t\t      SECTION_NOTYPE, NULL));\n-\n-      ASM_OUTPUT_ALIGN (asm_out_file, p2align);\n-      /* name length.  */\n-      fprintf (asm_out_file, ASM_LONG \" 1f - 0f\\n\");\n-      /* data length.  */\n-      fprintf (asm_out_file, ASM_LONG \" 4f - 1f\\n\");\n-      /* note type: NT_GNU_PROPERTY_TYPE_0.  */\n-      fprintf (asm_out_file, ASM_LONG \" 5\\n\");\n-      fprintf (asm_out_file, \"0:\\n\");\n-      /* vendor name: \"GNU\".  */\n-      fprintf (asm_out_file, STRING_ASM_OP \" \\\"GNU\\\"\\n\");\n-      fprintf (asm_out_file, \"1:\\n\");\n-      ASM_OUTPUT_ALIGN (asm_out_file, p2align);\n-      /* pr_type: GNU_PROPERTY_X86_FEATURE_1_AND.  */\n-      fprintf (asm_out_file, ASM_LONG \" 0xc0000002\\n\");\n-      /* pr_datasz.  */\\\n-      fprintf (asm_out_file, ASM_LONG \" 3f - 2f\\n\");\n-      fprintf (asm_out_file, \"2:\\n\");\n-      /* GNU_PROPERTY_X86_FEATURE_1_XXX.  */\n-      fprintf (asm_out_file, ASM_LONG \" 0x%x\\n\", feature_1);\n-      fprintf (asm_out_file, \"3:\\n\");\n-      ASM_OUTPUT_ALIGN (asm_out_file, p2align);\n-      fprintf (asm_out_file, \"4:\\n\");\n-    }\n-}"}, {"sha": "d0517e6759d751e88d019e379bb33643c0b8f0f2", "filename": "gcc/config/i386/driver-mingw32.c", "status": "removed", "additions": 0, "deletions": 28, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fdriver-mingw32.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fdriver-mingw32.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fdriver-mingw32.c?ref=566c5f1aaae120d2283103e68ecf1c1a83dd4459", "patch": "@@ -1,28 +0,0 @@\n-/* Host OS specific configuration for the gcc driver.\n-   Copyright (C) 2017-2020 Free Software Foundation, Inc.\n-\n-This file is part of GCC.\n-\n-GCC is free software; you can redistribute it and/or modify\n-it under the terms of the GNU General Public License as published by\n-the Free Software Foundation; either version 3, or (at your option)\n-any later version.\n-\n-GCC is distributed in the hope that it will be useful,\n-but WITHOUT ANY WARRANTY; without even the implied warranty of\n-MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n-GNU General Public License for more details.\n-\n-You should have received a copy of the GNU General Public License\n-along with GCC; see the file COPYING3.  If not see\n-<http://www.gnu.org/licenses/>.  */\n-\n-#define IN_TARGET_CODE 1\n-\n-#include \"config.h\"\n-\n-/* When defined, force the use (if non null) or not (otherwise) of CLI\n-   globbing.  */\n-#ifdef MINGW_DOWILDCARD\n-int _dowildcard = MINGW_DOWILDCARD;\n-#endif"}, {"sha": "be3ed0158f21352a2f00de7cfbb12eaa7ed03a9b", "filename": "gcc/config/i386/i386-builtins.c", "status": "removed", "additions": 0, "deletions": 2546, "changes": 2546, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fi386-builtins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fi386-builtins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-builtins.c?ref=566c5f1aaae120d2283103e68ecf1c1a83dd4459", "patch": "@@ -1,2546 +0,0 @@\n-/* Copyright (C) 1988-2020 Free Software Foundation, Inc.\n-\n-This file is part of GCC.\n-\n-GCC is free software; you can redistribute it and/or modify\n-it under the terms of the GNU General Public License as published by\n-the Free Software Foundation; either version 3, or (at your option)\n-any later version.\n-\n-GCC is distributed in the hope that it will be useful,\n-but WITHOUT ANY WARRANTY; without even the implied warranty of\n-MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n-GNU General Public License for more details.\n-\n-You should have received a copy of the GNU General Public License\n-along with GCC; see the file COPYING3.  If not see\n-<http://www.gnu.org/licenses/>.  */\n-\n-#define IN_TARGET_CODE 1\n-\n-#include \"config.h\"\n-#include \"system.h\"\n-#include \"coretypes.h\"\n-#include \"backend.h\"\n-#include \"rtl.h\"\n-#include \"tree.h\"\n-#include \"memmodel.h\"\n-#include \"gimple.h\"\n-#include \"cfghooks.h\"\n-#include \"cfgloop.h\"\n-#include \"df.h\"\n-#include \"tm_p.h\"\n-#include \"stringpool.h\"\n-#include \"expmed.h\"\n-#include \"optabs.h\"\n-#include \"regs.h\"\n-#include \"emit-rtl.h\"\n-#include \"recog.h\"\n-#include \"cgraph.h\"\n-#include \"diagnostic.h\"\n-#include \"cfgbuild.h\"\n-#include \"alias.h\"\n-#include \"fold-const.h\"\n-#include \"attribs.h\"\n-#include \"calls.h\"\n-#include \"stor-layout.h\"\n-#include \"varasm.h\"\n-#include \"output.h\"\n-#include \"insn-attr.h\"\n-#include \"flags.h\"\n-#include \"except.h\"\n-#include \"explow.h\"\n-#include \"expr.h\"\n-#include \"cfgrtl.h\"\n-#include \"common/common-target.h\"\n-#include \"langhooks.h\"\n-#include \"reload.h\"\n-#include \"gimplify.h\"\n-#include \"dwarf2.h\"\n-#include \"tm-constrs.h\"\n-#include \"cselib.h\"\n-#include \"sched-int.h\"\n-#include \"opts.h\"\n-#include \"tree-pass.h\"\n-#include \"context.h\"\n-#include \"pass_manager.h\"\n-#include \"target-globals.h\"\n-#include \"gimple-iterator.h\"\n-#include \"tree-vectorizer.h\"\n-#include \"shrink-wrap.h\"\n-#include \"builtins.h\"\n-#include \"rtl-iter.h\"\n-#include \"tree-iterator.h\"\n-#include \"dbgcnt.h\"\n-#include \"case-cfn-macros.h\"\n-#include \"dojump.h\"\n-#include \"fold-const-call.h\"\n-#include \"tree-vrp.h\"\n-#include \"tree-ssanames.h\"\n-#include \"selftest.h\"\n-#include \"selftest-rtl.h\"\n-#include \"print-rtl.h\"\n-#include \"intl.h\"\n-#include \"ifcvt.h\"\n-#include \"symbol-summary.h\"\n-#include \"ipa-prop.h\"\n-#include \"ipa-fnsummary.h\"\n-#include \"wide-int-bitmask.h\"\n-#include \"tree-vector-builder.h\"\n-#include \"debug.h\"\n-#include \"dwarf2out.h\"\n-#include \"i386-builtins.h\"\n-\n-#undef BDESC\n-#undef BDESC_FIRST\n-#undef BDESC_END\n-\n-/* Macros for verification of enum ix86_builtins order.  */\n-#define BDESC_VERIFY(x, y, z) \\\n-  gcc_checking_assert ((x) == (enum ix86_builtins) ((y) + (z)))\n-#define BDESC_VERIFYS(x, y, z) \\\n-  STATIC_ASSERT ((x) == (enum ix86_builtins) ((y) + (z)))\n-\n-BDESC_VERIFYS (IX86_BUILTIN__BDESC_PCMPESTR_FIRST,\n-\t       IX86_BUILTIN__BDESC_COMI_LAST, 1);\n-BDESC_VERIFYS (IX86_BUILTIN__BDESC_PCMPISTR_FIRST,\n-\t       IX86_BUILTIN__BDESC_PCMPESTR_LAST, 1);\n-BDESC_VERIFYS (IX86_BUILTIN__BDESC_SPECIAL_ARGS_FIRST,\n-\t       IX86_BUILTIN__BDESC_PCMPISTR_LAST, 1);\n-BDESC_VERIFYS (IX86_BUILTIN__BDESC_ARGS_FIRST,\n-\t       IX86_BUILTIN__BDESC_SPECIAL_ARGS_LAST, 1);\n-BDESC_VERIFYS (IX86_BUILTIN__BDESC_ROUND_ARGS_FIRST,\n-\t       IX86_BUILTIN__BDESC_ARGS_LAST, 1);\n-BDESC_VERIFYS (IX86_BUILTIN__BDESC_MULTI_ARG_FIRST,\n-\t       IX86_BUILTIN__BDESC_ROUND_ARGS_LAST, 1);\n-BDESC_VERIFYS (IX86_BUILTIN__BDESC_CET_FIRST,\n-\t       IX86_BUILTIN__BDESC_MULTI_ARG_LAST, 1);\n-BDESC_VERIFYS (IX86_BUILTIN__BDESC_CET_NORMAL_FIRST,\n-\t       IX86_BUILTIN__BDESC_CET_LAST, 1);\n-BDESC_VERIFYS (IX86_BUILTIN_MAX,\n-\t       IX86_BUILTIN__BDESC_CET_NORMAL_LAST, 1);\n-\n-\n-/* Table for the ix86 builtin non-function types.  */\n-static GTY(()) tree ix86_builtin_type_tab[(int) IX86_BT_LAST_CPTR + 1];\n-\n-/* Retrieve an element from the above table, building some of\n-   the types lazily.  */\n-\n-static tree\n-ix86_get_builtin_type (enum ix86_builtin_type tcode)\n-{\n-  unsigned int index;\n-  tree type, itype;\n-\n-  gcc_assert ((unsigned)tcode < ARRAY_SIZE(ix86_builtin_type_tab));\n-\n-  type = ix86_builtin_type_tab[(int) tcode];\n-  if (type != NULL)\n-    return type;\n-\n-  gcc_assert (tcode > IX86_BT_LAST_PRIM);\n-  if (tcode <= IX86_BT_LAST_VECT)\n-    {\n-      machine_mode mode;\n-\n-      index = tcode - IX86_BT_LAST_PRIM - 1;\n-      itype = ix86_get_builtin_type (ix86_builtin_type_vect_base[index]);\n-      mode = ix86_builtin_type_vect_mode[index];\n-\n-      type = build_vector_type_for_mode (itype, mode);\n-    }\n-  else\n-    {\n-      int quals;\n-\n-      index = tcode - IX86_BT_LAST_VECT - 1;\n-      if (tcode <= IX86_BT_LAST_PTR)\n-\tquals = TYPE_UNQUALIFIED;\n-      else\n-\tquals = TYPE_QUAL_CONST;\n-\n-      itype = ix86_get_builtin_type (ix86_builtin_type_ptr_base[index]);\n-      if (quals != TYPE_UNQUALIFIED)\n-\titype = build_qualified_type (itype, quals);\n-\n-      type = build_pointer_type (itype);\n-    }\n-\n-  ix86_builtin_type_tab[(int) tcode] = type;\n-  return type;\n-}\n-\n-/* Table for the ix86 builtin function types.  */\n-static GTY(()) tree ix86_builtin_func_type_tab[(int) IX86_BT_LAST_ALIAS + 1];\n-\n-/* Retrieve an element from the above table, building some of\n-   the types lazily.  */\n-\n-static tree\n-ix86_get_builtin_func_type (enum ix86_builtin_func_type tcode)\n-{\n-  tree type;\n-\n-  gcc_assert ((unsigned)tcode < ARRAY_SIZE (ix86_builtin_func_type_tab));\n-\n-  type = ix86_builtin_func_type_tab[(int) tcode];\n-  if (type != NULL)\n-    return type;\n-\n-  if (tcode <= IX86_BT_LAST_FUNC)\n-    {\n-      unsigned start = ix86_builtin_func_start[(int) tcode];\n-      unsigned after = ix86_builtin_func_start[(int) tcode + 1];\n-      tree rtype, atype, args = void_list_node;\n-      unsigned i;\n-\n-      rtype = ix86_get_builtin_type (ix86_builtin_func_args[start]);\n-      for (i = after - 1; i > start; --i)\n-\t{\n-\t  atype = ix86_get_builtin_type (ix86_builtin_func_args[i]);\n-\t  args = tree_cons (NULL, atype, args);\n-\t}\n-\n-      type = build_function_type (rtype, args);\n-    }\n-  else\n-    {\n-      unsigned index = tcode - IX86_BT_LAST_FUNC - 1;\n-      enum ix86_builtin_func_type icode;\n-\n-      icode = ix86_builtin_func_alias_base[index];\n-      type = ix86_get_builtin_func_type (icode);\n-    }\n-\n-  ix86_builtin_func_type_tab[(int) tcode] = type;\n-  return type;\n-}\n-\n-/* Table for the ix86 builtin decls.  */\n-static GTY(()) tree ix86_builtins[(int) IX86_BUILTIN_MAX];\n-\n-struct builtin_isa ix86_builtins_isa[(int) IX86_BUILTIN_MAX];\n-\n-tree get_ix86_builtin (enum ix86_builtins c)\n-{\n-  return ix86_builtins[c];\n-}\n-\n-/* Bits that can still enable any inclusion of a builtin.  */\n-HOST_WIDE_INT deferred_isa_values = 0;\n-HOST_WIDE_INT deferred_isa_values2 = 0;\n-\n-/* Add an ix86 target builtin function with CODE, NAME and TYPE.  Save the\n-   MASK and MASK2 of which isa_flags and ix86_isa_flags2 to use in the\n-   ix86_builtins_isa array.  Stores the function decl in the ix86_builtins\n-   array.  Returns the function decl or NULL_TREE, if the builtin was not\n-   added.\n-\n-   If the front end has a special hook for builtin functions, delay adding\n-   builtin functions that aren't in the current ISA until the ISA is changed\n-   with function specific optimization.  Doing so, can save about 300K for the\n-   default compiler.  When the builtin is expanded, check at that time whether\n-   it is valid.\n-\n-   If the front end doesn't have a special hook, record all builtins, even if\n-   it isn't an instruction set in the current ISA in case the user uses\n-   function specific options for a different ISA, so that we don't get scope\n-   errors if a builtin is added in the middle of a function scope.  */\n-\n-static inline tree\n-def_builtin (HOST_WIDE_INT mask, HOST_WIDE_INT mask2,\n-\t     const char *name,\n-\t     enum ix86_builtin_func_type tcode,\n-\t     enum ix86_builtins code)\n-{\n-  tree decl = NULL_TREE;\n-\n-  /* An instruction may be 64bit only regardless of ISAs.  */\n-  if (!(mask & OPTION_MASK_ISA_64BIT) || TARGET_64BIT)\n-    {\n-      ix86_builtins_isa[(int) code].isa = mask;\n-      ix86_builtins_isa[(int) code].isa2 = mask2;\n-\n-      mask &= ~OPTION_MASK_ISA_64BIT;\n-\n-      /* Filter out the masks most often ored together with others.  */\n-      if ((mask & ix86_isa_flags & OPTION_MASK_ISA_AVX512VL)\n-\t  && mask != OPTION_MASK_ISA_AVX512VL)\n-\tmask &= ~OPTION_MASK_ISA_AVX512VL;\n-      if ((mask & ix86_isa_flags & OPTION_MASK_ISA_AVX512BW)\n-\t  && mask != OPTION_MASK_ISA_AVX512BW)\n-\tmask &= ~OPTION_MASK_ISA_AVX512BW;\n-\n-      if (((mask2 == 0 || (mask2 & ix86_isa_flags2) != 0)\n-\t   && (mask == 0 || (mask & ix86_isa_flags) != 0))\n-\t  || ((mask & OPTION_MASK_ISA_MMX) != 0 && TARGET_MMX_WITH_SSE)\n-\t  || (lang_hooks.builtin_function\n-\t      == lang_hooks.builtin_function_ext_scope))\n-\t{\n-\t  tree type = ix86_get_builtin_func_type (tcode);\n-\t  decl = add_builtin_function (name, type, code, BUILT_IN_MD,\n-\t\t\t\t       NULL, NULL_TREE);\n-\t  ix86_builtins[(int) code] = decl;\n-\t  ix86_builtins_isa[(int) code].set_and_not_built_p = false;\n-\t}\n-      else\n-\t{\n-\t  /* Just MASK and MASK2 where set_and_not_built_p == true can potentially\n-\t     include a builtin.  */\n-\t  deferred_isa_values |= mask;\n-\t  deferred_isa_values2 |= mask2;\n-\t  ix86_builtins[(int) code] = NULL_TREE;\n-\t  ix86_builtins_isa[(int) code].tcode = tcode;\n-\t  ix86_builtins_isa[(int) code].name = name;\n-\t  ix86_builtins_isa[(int) code].const_p = false;\n-\t  ix86_builtins_isa[(int) code].pure_p = false;\n-\t  ix86_builtins_isa[(int) code].set_and_not_built_p = true;\n-\t}\n-    }\n-\n-  return decl;\n-}\n-\n-/* Like def_builtin, but also marks the function decl \"const\".  */\n-\n-static inline tree\n-def_builtin_const (HOST_WIDE_INT mask, HOST_WIDE_INT mask2, const char *name,\n-\t\t   enum ix86_builtin_func_type tcode, enum ix86_builtins code)\n-{\n-  tree decl = def_builtin (mask, mask2, name, tcode, code);\n-  if (decl)\n-    TREE_READONLY (decl) = 1;\n-  else\n-    ix86_builtins_isa[(int) code].const_p = true;\n-\n-  return decl;\n-}\n-\n-/* Like def_builtin, but also marks the function decl \"pure\".  */\n-\n-static inline tree\n-def_builtin_pure (HOST_WIDE_INT mask, HOST_WIDE_INT mask2, const char *name,\n-\t\t  enum ix86_builtin_func_type tcode, enum ix86_builtins code)\n-{\n-  tree decl = def_builtin (mask, mask2, name, tcode, code);\n-  if (decl)\n-    DECL_PURE_P (decl) = 1;\n-  else\n-    ix86_builtins_isa[(int) code].pure_p = true;\n-\n-  return decl;\n-}\n-\n-/* Add any new builtin functions for a given ISA that may not have been\n-   declared.  This saves a bit of space compared to adding all of the\n-   declarations to the tree, even if we didn't use them.  */\n-\n-void\n-ix86_add_new_builtins (HOST_WIDE_INT isa, HOST_WIDE_INT isa2)\n-{\n-  isa &= ~OPTION_MASK_ISA_64BIT;\n-\n-  if ((isa & deferred_isa_values) == 0\n-      && (isa2 & deferred_isa_values2) == 0\n-      && ((deferred_isa_values & OPTION_MASK_ISA_MMX) == 0\n-\t  || !(TARGET_64BIT && (isa & OPTION_MASK_ISA_SSE2) != 0)))\n-    return;\n-\n-  /* Bits in ISA value can be removed from potential isa values.  */\n-  deferred_isa_values &= ~isa;\n-  deferred_isa_values2 &= ~isa2;\n-  if (TARGET_64BIT && (isa & OPTION_MASK_ISA_SSE2) != 0)\n-    deferred_isa_values &= ~OPTION_MASK_ISA_MMX;\n-\n-  int i;\n-  tree saved_current_target_pragma = current_target_pragma;\n-  current_target_pragma = NULL_TREE;\n-\n-  for (i = 0; i < (int)IX86_BUILTIN_MAX; i++)\n-    {\n-      if (((ix86_builtins_isa[i].isa & isa) != 0\n-\t   || (ix86_builtins_isa[i].isa2 & isa2) != 0\n-\t   || ((ix86_builtins_isa[i].isa & OPTION_MASK_ISA_MMX) != 0\n-\t       && TARGET_64BIT\n-\t       && (isa & OPTION_MASK_ISA_SSE2) != 0))\n-\t  && ix86_builtins_isa[i].set_and_not_built_p)\n-\t{\n-\t  tree decl, type;\n-\n-\t  /* Don't define the builtin again.  */\n-\t  ix86_builtins_isa[i].set_and_not_built_p = false;\n-\n-\t  type = ix86_get_builtin_func_type (ix86_builtins_isa[i].tcode);\n-\t  decl = add_builtin_function_ext_scope (ix86_builtins_isa[i].name,\n-\t\t\t\t\t\t type, i, BUILT_IN_MD, NULL,\n-\t\t\t\t\t\t NULL_TREE);\n-\n-\t  ix86_builtins[i] = decl;\n-\t  if (ix86_builtins_isa[i].const_p)\n-\t    TREE_READONLY (decl) = 1;\n-\t}\n-    }\n-\n-  current_target_pragma = saved_current_target_pragma;\n-}\n-\f\n-/* TM vector builtins.  */\n-\n-/* Reuse the existing x86-specific `struct builtin_description' cause\n-   we're lazy.  Add casts to make them fit.  */\n-static const struct builtin_description bdesc_tm[] =\n-{\n-  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_WM64\", (enum ix86_builtins) BUILT_IN_TM_STORE_M64, UNKNOWN, VOID_FTYPE_PV2SI_V2SI },\n-  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_WaRM64\", (enum ix86_builtins) BUILT_IN_TM_STORE_WAR_M64, UNKNOWN, VOID_FTYPE_PV2SI_V2SI },\n-  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_WaWM64\", (enum ix86_builtins) BUILT_IN_TM_STORE_WAW_M64, UNKNOWN, VOID_FTYPE_PV2SI_V2SI },\n-  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_RM64\", (enum ix86_builtins) BUILT_IN_TM_LOAD_M64, UNKNOWN, V2SI_FTYPE_PCV2SI },\n-  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_RaRM64\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RAR_M64, UNKNOWN, V2SI_FTYPE_PCV2SI },\n-  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_RaWM64\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RAW_M64, UNKNOWN, V2SI_FTYPE_PCV2SI },\n-  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_RfWM64\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RFW_M64, UNKNOWN, V2SI_FTYPE_PCV2SI },\n-\n-  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_WM128\", (enum ix86_builtins) BUILT_IN_TM_STORE_M128, UNKNOWN, VOID_FTYPE_PV4SF_V4SF },\n-  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_WaRM128\", (enum ix86_builtins) BUILT_IN_TM_STORE_WAR_M128, UNKNOWN, VOID_FTYPE_PV4SF_V4SF },\n-  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_WaWM128\", (enum ix86_builtins) BUILT_IN_TM_STORE_WAW_M128, UNKNOWN, VOID_FTYPE_PV4SF_V4SF },\n-  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_RM128\", (enum ix86_builtins) BUILT_IN_TM_LOAD_M128, UNKNOWN, V4SF_FTYPE_PCV4SF },\n-  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_RaRM128\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RAR_M128, UNKNOWN, V4SF_FTYPE_PCV4SF },\n-  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_RaWM128\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RAW_M128, UNKNOWN, V4SF_FTYPE_PCV4SF },\n-  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_RfWM128\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RFW_M128, UNKNOWN, V4SF_FTYPE_PCV4SF },\n-\n-  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_WM256\", (enum ix86_builtins) BUILT_IN_TM_STORE_M256, UNKNOWN, VOID_FTYPE_PV8SF_V8SF },\n-  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_WaRM256\", (enum ix86_builtins) BUILT_IN_TM_STORE_WAR_M256, UNKNOWN, VOID_FTYPE_PV8SF_V8SF },\n-  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_WaWM256\", (enum ix86_builtins) BUILT_IN_TM_STORE_WAW_M256, UNKNOWN, VOID_FTYPE_PV8SF_V8SF },\n-  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_RM256\", (enum ix86_builtins) BUILT_IN_TM_LOAD_M256, UNKNOWN, V8SF_FTYPE_PCV8SF },\n-  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_RaRM256\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RAR_M256, UNKNOWN, V8SF_FTYPE_PCV8SF },\n-  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_RaWM256\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RAW_M256, UNKNOWN, V8SF_FTYPE_PCV8SF },\n-  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_RfWM256\", (enum ix86_builtins) BUILT_IN_TM_LOAD_RFW_M256, UNKNOWN, V8SF_FTYPE_PCV8SF },\n-\n-  { OPTION_MASK_ISA_MMX, 0, CODE_FOR_nothing, \"__builtin__ITM_LM64\", (enum ix86_builtins) BUILT_IN_TM_LOG_M64, UNKNOWN, VOID_FTYPE_PCVOID },\n-  { OPTION_MASK_ISA_SSE, 0, CODE_FOR_nothing, \"__builtin__ITM_LM128\", (enum ix86_builtins) BUILT_IN_TM_LOG_M128, UNKNOWN, VOID_FTYPE_PCVOID },\n-  { OPTION_MASK_ISA_AVX, 0, CODE_FOR_nothing, \"__builtin__ITM_LM256\", (enum ix86_builtins) BUILT_IN_TM_LOG_M256, UNKNOWN, VOID_FTYPE_PCVOID },\n-};\n-\n-/* Initialize the transactional memory vector load/store builtins.  */\n-\n-static void\n-ix86_init_tm_builtins (void)\n-{\n-  enum ix86_builtin_func_type ftype;\n-  const struct builtin_description *d;\n-  size_t i;\n-  tree decl;\n-  tree attrs_load, attrs_type_load, attrs_store, attrs_type_store;\n-  tree attrs_log, attrs_type_log;\n-\n-  if (!flag_tm)\n-    return;\n-\n-  /* If there are no builtins defined, we must be compiling in a\n-     language without trans-mem support.  */\n-  if (!builtin_decl_explicit_p (BUILT_IN_TM_LOAD_1))\n-    return;\n-\n-  /* Use whatever attributes a normal TM load has.  */\n-  decl = builtin_decl_explicit (BUILT_IN_TM_LOAD_1);\n-  attrs_load = DECL_ATTRIBUTES (decl);\n-  attrs_type_load = TYPE_ATTRIBUTES (TREE_TYPE (decl));\n-  /* Use whatever attributes a normal TM store has.  */\n-  decl = builtin_decl_explicit (BUILT_IN_TM_STORE_1);\n-  attrs_store = DECL_ATTRIBUTES (decl);\n-  attrs_type_store = TYPE_ATTRIBUTES (TREE_TYPE (decl));\n-  /* Use whatever attributes a normal TM log has.  */\n-  decl = builtin_decl_explicit (BUILT_IN_TM_LOG);\n-  attrs_log = DECL_ATTRIBUTES (decl);\n-  attrs_type_log = TYPE_ATTRIBUTES (TREE_TYPE (decl));\n-\n-  for (i = 0, d = bdesc_tm;\n-       i < ARRAY_SIZE (bdesc_tm);\n-       i++, d++)\n-    {\n-      if ((d->mask & ix86_isa_flags) != 0\n-\t  || ((d->mask & OPTION_MASK_ISA_MMX) != 0 && TARGET_MMX_WITH_SSE)\n-\t  || (lang_hooks.builtin_function\n-\t      == lang_hooks.builtin_function_ext_scope))\n-\t{\n-\t  tree type, attrs, attrs_type;\n-\t  enum built_in_function code = (enum built_in_function) d->code;\n-\n-\t  ftype = (enum ix86_builtin_func_type) d->flag;\n-\t  type = ix86_get_builtin_func_type (ftype);\n-\n-\t  if (BUILTIN_TM_LOAD_P (code))\n-\t    {\n-\t      attrs = attrs_load;\n-\t      attrs_type = attrs_type_load;\n-\t    }\n-\t  else if (BUILTIN_TM_STORE_P (code))\n-\t    {\n-\t      attrs = attrs_store;\n-\t      attrs_type = attrs_type_store;\n-\t    }\n-\t  else\n-\t    {\n-\t      attrs = attrs_log;\n-\t      attrs_type = attrs_type_log;\n-\t    }\n-\t  decl = add_builtin_function (d->name, type, code, BUILT_IN_NORMAL,\n-\t\t\t\t       /* The builtin without the prefix for\n-\t\t\t\t\t  calling it directly.  */\n-\t\t\t\t       d->name + strlen (\"__builtin_\"),\n-\t\t\t\t       attrs);\n-\t  /* add_builtin_function() will set the DECL_ATTRIBUTES, now\n-\t     set the TYPE_ATTRIBUTES.  */\n-\t  decl_attributes (&TREE_TYPE (decl), attrs_type, ATTR_FLAG_BUILT_IN);\n-\n-\t  set_builtin_decl (code, decl, false);\n-\t}\n-    }\n-}\n-\n-/* Set up all the MMX/SSE builtins, even builtins for instructions that are not\n-   in the current target ISA to allow the user to compile particular modules\n-   with different target specific options that differ from the command line\n-   options.  */\n-static void\n-ix86_init_mmx_sse_builtins (void)\n-{\n-  const struct builtin_description * d;\n-  enum ix86_builtin_func_type ftype;\n-  size_t i;\n-\n-  /* Add all special builtins with variable number of operands.  */\n-  for (i = 0, d = bdesc_special_args;\n-       i < ARRAY_SIZE (bdesc_special_args);\n-       i++, d++)\n-    {\n-      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_SPECIAL_ARGS_FIRST, i);\n-      if (d->name == 0)\n-\tcontinue;\n-\n-      ftype = (enum ix86_builtin_func_type) d->flag;\n-      def_builtin (d->mask, d->mask2, d->name, ftype, d->code);\n-    }\n-  BDESC_VERIFYS (IX86_BUILTIN__BDESC_SPECIAL_ARGS_LAST,\n-\t\t IX86_BUILTIN__BDESC_SPECIAL_ARGS_FIRST,\n-\t\t ARRAY_SIZE (bdesc_special_args) - 1);\n-\n-  /* Add all builtins with variable number of operands.  */\n-  for (i = 0, d = bdesc_args;\n-       i < ARRAY_SIZE (bdesc_args);\n-       i++, d++)\n-    {\n-      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_ARGS_FIRST, i);\n-      if (d->name == 0)\n-\tcontinue;\n-\n-      ftype = (enum ix86_builtin_func_type) d->flag;\n-      def_builtin_const (d->mask, d->mask2, d->name, ftype, d->code);\n-    }\n-  BDESC_VERIFYS (IX86_BUILTIN__BDESC_ARGS_LAST,\n-\t\t IX86_BUILTIN__BDESC_ARGS_FIRST,\n-\t\t ARRAY_SIZE (bdesc_args) - 1);\n-\n-  /* Add all builtins with rounding.  */\n-  for (i = 0, d = bdesc_round_args;\n-       i < ARRAY_SIZE (bdesc_round_args);\n-       i++, d++)\n-    {\n-      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_ROUND_ARGS_FIRST, i);\n-      if (d->name == 0)\n-\tcontinue;\n-\n-      ftype = (enum ix86_builtin_func_type) d->flag;\n-      def_builtin_const (d->mask, d->mask2, d->name, ftype, d->code);\n-    }\n-  BDESC_VERIFYS (IX86_BUILTIN__BDESC_ROUND_ARGS_LAST,\n-\t\t IX86_BUILTIN__BDESC_ROUND_ARGS_FIRST,\n-\t\t ARRAY_SIZE (bdesc_round_args) - 1);\n-\n-  /* pcmpestr[im] insns.  */\n-  for (i = 0, d = bdesc_pcmpestr;\n-       i < ARRAY_SIZE (bdesc_pcmpestr);\n-       i++, d++)\n-    {\n-      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_PCMPESTR_FIRST, i);\n-      if (d->code == IX86_BUILTIN_PCMPESTRM128)\n-\tftype = V16QI_FTYPE_V16QI_INT_V16QI_INT_INT;\n-      else\n-\tftype = INT_FTYPE_V16QI_INT_V16QI_INT_INT;\n-      def_builtin_const (d->mask, d->mask2, d->name, ftype, d->code);\n-    }\n-  BDESC_VERIFYS (IX86_BUILTIN__BDESC_PCMPESTR_LAST,\n-\t\t IX86_BUILTIN__BDESC_PCMPESTR_FIRST,\n-\t\t ARRAY_SIZE (bdesc_pcmpestr) - 1);\n-\n-  /* pcmpistr[im] insns.  */\n-  for (i = 0, d = bdesc_pcmpistr;\n-       i < ARRAY_SIZE (bdesc_pcmpistr);\n-       i++, d++)\n-    {\n-      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_PCMPISTR_FIRST, i);\n-      if (d->code == IX86_BUILTIN_PCMPISTRM128)\n-\tftype = V16QI_FTYPE_V16QI_V16QI_INT;\n-      else\n-\tftype = INT_FTYPE_V16QI_V16QI_INT;\n-      def_builtin_const (d->mask, d->mask2, d->name, ftype, d->code);\n-    }\n-  BDESC_VERIFYS (IX86_BUILTIN__BDESC_PCMPISTR_LAST,\n-\t\t IX86_BUILTIN__BDESC_PCMPISTR_FIRST,\n-\t\t ARRAY_SIZE (bdesc_pcmpistr) - 1);\n-\n-  /* comi/ucomi insns.  */\n-  for (i = 0, d = bdesc_comi; i < ARRAY_SIZE (bdesc_comi); i++, d++)\n-    {\n-      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_COMI_FIRST, i);\n-      if (d->mask == OPTION_MASK_ISA_SSE2)\n-\tftype = INT_FTYPE_V2DF_V2DF;\n-      else\n-\tftype = INT_FTYPE_V4SF_V4SF;\n-      def_builtin_const (d->mask, d->mask2, d->name, ftype, d->code);\n-    }\n-  BDESC_VERIFYS (IX86_BUILTIN__BDESC_COMI_LAST,\n-\t\t IX86_BUILTIN__BDESC_COMI_FIRST,\n-\t\t ARRAY_SIZE (bdesc_comi) - 1);\n-\n-  /* SSE */\n-  def_builtin (OPTION_MASK_ISA_SSE, 0,  \"__builtin_ia32_ldmxcsr\",\n-\t       VOID_FTYPE_UNSIGNED, IX86_BUILTIN_LDMXCSR);\n-  def_builtin_pure (OPTION_MASK_ISA_SSE, 0, \"__builtin_ia32_stmxcsr\",\n-\t\t    UNSIGNED_FTYPE_VOID, IX86_BUILTIN_STMXCSR);\n-\n-  /* SSE or 3DNow!A */\n-  def_builtin (OPTION_MASK_ISA_SSE | OPTION_MASK_ISA_3DNOW_A\n-\t       /* As it uses V4HImode, we have to require -mmmx too.  */\n-\t       | OPTION_MASK_ISA_MMX, 0,\n-\t       \"__builtin_ia32_maskmovq\", VOID_FTYPE_V8QI_V8QI_PCHAR,\n-\t       IX86_BUILTIN_MASKMOVQ);\n-\n-  /* SSE2 */\n-  def_builtin (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_maskmovdqu\",\n-\t       VOID_FTYPE_V16QI_V16QI_PCHAR, IX86_BUILTIN_MASKMOVDQU);\n-\n-  def_builtin (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_clflush\",\n-\t       VOID_FTYPE_PCVOID, IX86_BUILTIN_CLFLUSH);\n-  x86_mfence = def_builtin (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_mfence\",\n-\t\t\t    VOID_FTYPE_VOID, IX86_BUILTIN_MFENCE);\n-\n-  /* SSE3.  */\n-  def_builtin (OPTION_MASK_ISA_SSE3, 0, \"__builtin_ia32_monitor\",\n-\t       VOID_FTYPE_PCVOID_UNSIGNED_UNSIGNED, IX86_BUILTIN_MONITOR);\n-  def_builtin (OPTION_MASK_ISA_SSE3, 0, \"__builtin_ia32_mwait\",\n-\t       VOID_FTYPE_UNSIGNED_UNSIGNED, IX86_BUILTIN_MWAIT);\n-\n-  /* AES */\n-  def_builtin_const (OPTION_MASK_ISA_AES | OPTION_MASK_ISA_SSE2, 0,\n-\t\t     \"__builtin_ia32_aesenc128\",\n-\t\t     V2DI_FTYPE_V2DI_V2DI, IX86_BUILTIN_AESENC128);\n-  def_builtin_const (OPTION_MASK_ISA_AES | OPTION_MASK_ISA_SSE2, 0,\n-\t\t     \"__builtin_ia32_aesenclast128\",\n-\t\t     V2DI_FTYPE_V2DI_V2DI, IX86_BUILTIN_AESENCLAST128);\n-  def_builtin_const (OPTION_MASK_ISA_AES | OPTION_MASK_ISA_SSE2, 0,\n-\t\t     \"__builtin_ia32_aesdec128\",\n-\t\t     V2DI_FTYPE_V2DI_V2DI, IX86_BUILTIN_AESDEC128);\n-  def_builtin_const (OPTION_MASK_ISA_AES | OPTION_MASK_ISA_SSE2, 0,\n-\t\t     \"__builtin_ia32_aesdeclast128\",\n-\t\t     V2DI_FTYPE_V2DI_V2DI, IX86_BUILTIN_AESDECLAST128);\n-  def_builtin_const (OPTION_MASK_ISA_AES | OPTION_MASK_ISA_SSE2, 0,\n-\t\t     \"__builtin_ia32_aesimc128\",\n-\t\t     V2DI_FTYPE_V2DI, IX86_BUILTIN_AESIMC128);\n-  def_builtin_const (OPTION_MASK_ISA_AES | OPTION_MASK_ISA_SSE2, 0,\n-\t\t     \"__builtin_ia32_aeskeygenassist128\",\n-\t\t     V2DI_FTYPE_V2DI_INT, IX86_BUILTIN_AESKEYGENASSIST128);\n-\n-  /* PCLMUL */\n-  def_builtin_const (OPTION_MASK_ISA_PCLMUL | OPTION_MASK_ISA_SSE2, 0,\n-\t\t     \"__builtin_ia32_pclmulqdq128\",\n-\t\t     V2DI_FTYPE_V2DI_V2DI_INT, IX86_BUILTIN_PCLMULQDQ128);\n-\n-  /* RDRND */\n-  def_builtin (OPTION_MASK_ISA_RDRND, 0, \"__builtin_ia32_rdrand16_step\",\n-\t       INT_FTYPE_PUSHORT, IX86_BUILTIN_RDRAND16_STEP);\n-  def_builtin (OPTION_MASK_ISA_RDRND, 0,  \"__builtin_ia32_rdrand32_step\",\n-\t       INT_FTYPE_PUNSIGNED, IX86_BUILTIN_RDRAND32_STEP);\n-  def_builtin (OPTION_MASK_ISA_RDRND | OPTION_MASK_ISA_64BIT, 0,\n-\t       \"__builtin_ia32_rdrand64_step\", INT_FTYPE_PULONGLONG,\n-\t       IX86_BUILTIN_RDRAND64_STEP);\n-\n-  /* AVX2 */\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv2df\",\n-\t\t    V2DF_FTYPE_V2DF_PCDOUBLE_V4SI_V2DF_INT,\n-\t\t    IX86_BUILTIN_GATHERSIV2DF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv4df\",\n-\t\t    V4DF_FTYPE_V4DF_PCDOUBLE_V4SI_V4DF_INT,\n-\t\t    IX86_BUILTIN_GATHERSIV4DF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv2df\",\n-\t\t    V2DF_FTYPE_V2DF_PCDOUBLE_V2DI_V2DF_INT,\n-\t\t    IX86_BUILTIN_GATHERDIV2DF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv4df\",\n-\t\t    V4DF_FTYPE_V4DF_PCDOUBLE_V4DI_V4DF_INT,\n-\t\t    IX86_BUILTIN_GATHERDIV4DF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv4sf\",\n-\t\t    V4SF_FTYPE_V4SF_PCFLOAT_V4SI_V4SF_INT,\n-\t\t    IX86_BUILTIN_GATHERSIV4SF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv8sf\",\n-\t\t    V8SF_FTYPE_V8SF_PCFLOAT_V8SI_V8SF_INT,\n-\t\t    IX86_BUILTIN_GATHERSIV8SF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv4sf\",\n-\t\t    V4SF_FTYPE_V4SF_PCFLOAT_V2DI_V4SF_INT,\n-\t\t    IX86_BUILTIN_GATHERDIV4SF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv4sf256\",\n-\t\t    V4SF_FTYPE_V4SF_PCFLOAT_V4DI_V4SF_INT,\n-\t\t    IX86_BUILTIN_GATHERDIV8SF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv2di\",\n-\t\t    V2DI_FTYPE_V2DI_PCINT64_V4SI_V2DI_INT,\n-\t\t    IX86_BUILTIN_GATHERSIV2DI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv4di\",\n-\t\t    V4DI_FTYPE_V4DI_PCINT64_V4SI_V4DI_INT,\n-\t\t    IX86_BUILTIN_GATHERSIV4DI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv2di\",\n-\t\t    V2DI_FTYPE_V2DI_PCINT64_V2DI_V2DI_INT,\n-\t\t    IX86_BUILTIN_GATHERDIV2DI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv4di\",\n-\t\t    V4DI_FTYPE_V4DI_PCINT64_V4DI_V4DI_INT,\n-\t\t    IX86_BUILTIN_GATHERDIV4DI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv4si\",\n-\t\t    V4SI_FTYPE_V4SI_PCINT_V4SI_V4SI_INT,\n-\t\t    IX86_BUILTIN_GATHERSIV4SI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gathersiv8si\",\n-\t\t    V8SI_FTYPE_V8SI_PCINT_V8SI_V8SI_INT,\n-\t\t    IX86_BUILTIN_GATHERSIV8SI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv4si\",\n-\t\t    V4SI_FTYPE_V4SI_PCINT_V2DI_V4SI_INT,\n-\t\t    IX86_BUILTIN_GATHERDIV4SI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatherdiv4si256\",\n-\t\t    V4SI_FTYPE_V4SI_PCINT_V4DI_V4SI_INT,\n-\t\t    IX86_BUILTIN_GATHERDIV8SI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatheraltsiv4df \",\n-\t\t    V4DF_FTYPE_V4DF_PCDOUBLE_V8SI_V4DF_INT,\n-\t\t    IX86_BUILTIN_GATHERALTSIV4DF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatheraltdiv8sf \",\n-\t\t    V8SF_FTYPE_V8SF_PCFLOAT_V4DI_V8SF_INT,\n-\t\t    IX86_BUILTIN_GATHERALTDIV8SF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatheraltsiv4di \",\n-\t\t    V4DI_FTYPE_V4DI_PCINT64_V8SI_V4DI_INT,\n-\t\t    IX86_BUILTIN_GATHERALTSIV4DI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX2, 0, \"__builtin_ia32_gatheraltdiv8si \",\n-\t\t    V8SI_FTYPE_V8SI_PCINT_V4DI_V8SI_INT,\n-\t\t    IX86_BUILTIN_GATHERALTDIV8SI);\n-\n-  /* AVX512F */\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gathersiv16sf\",\n-\t\t    V16SF_FTYPE_V16SF_PCVOID_V16SI_HI_INT,\n-\t\t    IX86_BUILTIN_GATHER3SIV16SF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gathersiv8df\",\n-\t\t    V8DF_FTYPE_V8DF_PCVOID_V8SI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3SIV8DF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gatherdiv16sf\",\n-\t\t    V8SF_FTYPE_V8SF_PCVOID_V8DI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3DIV16SF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gatherdiv8df\",\n-\t\t    V8DF_FTYPE_V8DF_PCVOID_V8DI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3DIV8DF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gathersiv16si\",\n-\t\t    V16SI_FTYPE_V16SI_PCVOID_V16SI_HI_INT,\n-\t\t    IX86_BUILTIN_GATHER3SIV16SI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gathersiv8di\",\n-\t\t    V8DI_FTYPE_V8DI_PCVOID_V8SI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3SIV8DI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gatherdiv16si\",\n-\t\t    V8SI_FTYPE_V8SI_PCVOID_V8DI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3DIV16SI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gatherdiv8di\",\n-\t\t    V8DI_FTYPE_V8DI_PCVOID_V8DI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3DIV8DI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gather3altsiv8df \",\n-\t\t    V8DF_FTYPE_V8DF_PCDOUBLE_V16SI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3ALTSIV8DF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gather3altdiv16sf \",\n-\t\t    V16SF_FTYPE_V16SF_PCFLOAT_V8DI_HI_INT,\n-\t\t    IX86_BUILTIN_GATHER3ALTDIV16SF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gather3altsiv8di \",\n-\t\t    V8DI_FTYPE_V8DI_PCINT64_V16SI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3ALTSIV8DI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_gather3altdiv16si \",\n-\t\t    V16SI_FTYPE_V16SI_PCINT_V8DI_HI_INT,\n-\t\t    IX86_BUILTIN_GATHER3ALTDIV16SI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scattersiv16sf\",\n-\t       VOID_FTYPE_PVOID_HI_V16SI_V16SF_INT,\n-\t       IX86_BUILTIN_SCATTERSIV16SF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scattersiv8df\",\n-\t       VOID_FTYPE_PVOID_QI_V8SI_V8DF_INT,\n-\t       IX86_BUILTIN_SCATTERSIV8DF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatterdiv16sf\",\n-\t       VOID_FTYPE_PVOID_QI_V8DI_V8SF_INT,\n-\t       IX86_BUILTIN_SCATTERDIV16SF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatterdiv8df\",\n-\t       VOID_FTYPE_PVOID_QI_V8DI_V8DF_INT,\n-\t       IX86_BUILTIN_SCATTERDIV8DF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scattersiv16si\",\n-\t       VOID_FTYPE_PVOID_HI_V16SI_V16SI_INT,\n-\t       IX86_BUILTIN_SCATTERSIV16SI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scattersiv8di\",\n-\t       VOID_FTYPE_PVOID_QI_V8SI_V8DI_INT,\n-\t       IX86_BUILTIN_SCATTERSIV8DI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatterdiv16si\",\n-\t       VOID_FTYPE_PVOID_QI_V8DI_V8SI_INT,\n-\t       IX86_BUILTIN_SCATTERDIV16SI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatterdiv8di\",\n-\t       VOID_FTYPE_PVOID_QI_V8DI_V8DI_INT,\n-\t       IX86_BUILTIN_SCATTERDIV8DI);\n-\n-  /* AVX512VL */\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv2df\",\n-\t\t    V2DF_FTYPE_V2DF_PCVOID_V4SI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3SIV2DF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv4df\",\n-\t\t    V4DF_FTYPE_V4DF_PCVOID_V4SI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3SIV4DF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div2df\",\n-\t\t    V2DF_FTYPE_V2DF_PCVOID_V2DI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3DIV2DF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div4df\",\n-\t\t    V4DF_FTYPE_V4DF_PCVOID_V4DI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3DIV4DF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv4sf\",\n-\t\t    V4SF_FTYPE_V4SF_PCVOID_V4SI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3SIV4SF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv8sf\",\n-\t\t    V8SF_FTYPE_V8SF_PCVOID_V8SI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3SIV8SF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div4sf\",\n-\t\t    V4SF_FTYPE_V4SF_PCVOID_V2DI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3DIV4SF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div8sf\",\n-\t\t    V4SF_FTYPE_V4SF_PCVOID_V4DI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3DIV8SF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv2di\",\n-\t\t    V2DI_FTYPE_V2DI_PCVOID_V4SI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3SIV2DI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv4di\",\n-\t\t    V4DI_FTYPE_V4DI_PCVOID_V4SI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3SIV4DI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div2di\",\n-\t\t    V2DI_FTYPE_V2DI_PCVOID_V2DI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3DIV2DI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div4di\",\n-\t\t    V4DI_FTYPE_V4DI_PCVOID_V4DI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3DIV4DI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv4si\",\n-\t\t    V4SI_FTYPE_V4SI_PCVOID_V4SI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3SIV4SI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3siv8si\",\n-\t\t    V8SI_FTYPE_V8SI_PCVOID_V8SI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3SIV8SI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div4si\",\n-\t\t    V4SI_FTYPE_V4SI_PCVOID_V2DI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3DIV4SI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3div8si\",\n-\t\t    V4SI_FTYPE_V4SI_PCVOID_V4DI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3DIV8SI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3altsiv4df \",\n-\t\t    V4DF_FTYPE_V4DF_PCDOUBLE_V8SI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3ALTSIV4DF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3altdiv8sf \",\n-\t\t    V8SF_FTYPE_V8SF_PCFLOAT_V4DI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3ALTDIV8SF);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3altsiv4di \",\n-\t\t    V4DI_FTYPE_V4DI_PCINT64_V8SI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3ALTSIV4DI);\n-\n-  def_builtin_pure (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_gather3altdiv8si \",\n-\t\t    V8SI_FTYPE_V8SI_PCINT_V4DI_QI_INT,\n-\t\t    IX86_BUILTIN_GATHER3ALTDIV8SI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv8sf\",\n-\t       VOID_FTYPE_PVOID_QI_V8SI_V8SF_INT,\n-\t       IX86_BUILTIN_SCATTERSIV8SF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv4sf\",\n-\t       VOID_FTYPE_PVOID_QI_V4SI_V4SF_INT,\n-\t       IX86_BUILTIN_SCATTERSIV4SF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv4df\",\n-\t       VOID_FTYPE_PVOID_QI_V4SI_V4DF_INT,\n-\t       IX86_BUILTIN_SCATTERSIV4DF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv2df\",\n-\t       VOID_FTYPE_PVOID_QI_V4SI_V2DF_INT,\n-\t       IX86_BUILTIN_SCATTERSIV2DF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv8sf\",\n-\t       VOID_FTYPE_PVOID_QI_V4DI_V4SF_INT,\n-\t       IX86_BUILTIN_SCATTERDIV8SF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv4sf\",\n-\t       VOID_FTYPE_PVOID_QI_V2DI_V4SF_INT,\n-\t       IX86_BUILTIN_SCATTERDIV4SF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv4df\",\n-\t       VOID_FTYPE_PVOID_QI_V4DI_V4DF_INT,\n-\t       IX86_BUILTIN_SCATTERDIV4DF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv2df\",\n-\t       VOID_FTYPE_PVOID_QI_V2DI_V2DF_INT,\n-\t       IX86_BUILTIN_SCATTERDIV2DF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv8si\",\n-\t       VOID_FTYPE_PVOID_QI_V8SI_V8SI_INT,\n-\t       IX86_BUILTIN_SCATTERSIV8SI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv4si\",\n-\t       VOID_FTYPE_PVOID_QI_V4SI_V4SI_INT,\n-\t       IX86_BUILTIN_SCATTERSIV4SI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv4di\",\n-\t       VOID_FTYPE_PVOID_QI_V4SI_V4DI_INT,\n-\t       IX86_BUILTIN_SCATTERSIV4DI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scattersiv2di\",\n-\t       VOID_FTYPE_PVOID_QI_V4SI_V2DI_INT,\n-\t       IX86_BUILTIN_SCATTERSIV2DI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv8si\",\n-\t       VOID_FTYPE_PVOID_QI_V4DI_V4SI_INT,\n-\t       IX86_BUILTIN_SCATTERDIV8SI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv4si\",\n-\t       VOID_FTYPE_PVOID_QI_V2DI_V4SI_INT,\n-\t       IX86_BUILTIN_SCATTERDIV4SI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv4di\",\n-\t       VOID_FTYPE_PVOID_QI_V4DI_V4DI_INT,\n-\t       IX86_BUILTIN_SCATTERDIV4DI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatterdiv2di\",\n-\t       VOID_FTYPE_PVOID_QI_V2DI_V2DI_INT,\n-\t       IX86_BUILTIN_SCATTERDIV2DI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatteraltsiv8df \",\n-\t       VOID_FTYPE_PDOUBLE_QI_V16SI_V8DF_INT,\n-\t       IX86_BUILTIN_SCATTERALTSIV8DF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatteraltdiv16sf \",\n-\t       VOID_FTYPE_PFLOAT_HI_V8DI_V16SF_INT,\n-\t       IX86_BUILTIN_SCATTERALTDIV16SF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatteraltsiv8di \",\n-\t       VOID_FTYPE_PLONGLONG_QI_V16SI_V8DI_INT,\n-\t       IX86_BUILTIN_SCATTERALTSIV8DI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512F, 0, \"__builtin_ia32_scatteraltdiv16si \",\n-\t       VOID_FTYPE_PINT_HI_V8DI_V16SI_INT,\n-\t       IX86_BUILTIN_SCATTERALTDIV16SI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltsiv4df \",\n-\t       VOID_FTYPE_PDOUBLE_QI_V8SI_V4DF_INT,\n-\t       IX86_BUILTIN_SCATTERALTSIV4DF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltdiv8sf \",\n-\t       VOID_FTYPE_PFLOAT_QI_V4DI_V8SF_INT,\n-\t       IX86_BUILTIN_SCATTERALTDIV8SF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltsiv4di \",\n-\t       VOID_FTYPE_PLONGLONG_QI_V8SI_V4DI_INT,\n-\t       IX86_BUILTIN_SCATTERALTSIV4DI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltdiv8si \",\n-\t       VOID_FTYPE_PINT_QI_V4DI_V8SI_INT,\n-\t       IX86_BUILTIN_SCATTERALTDIV8SI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltsiv2df \",\n-\t       VOID_FTYPE_PDOUBLE_QI_V4SI_V2DF_INT,\n-\t       IX86_BUILTIN_SCATTERALTSIV2DF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltdiv4sf \",\n-\t       VOID_FTYPE_PFLOAT_QI_V2DI_V4SF_INT,\n-\t       IX86_BUILTIN_SCATTERALTDIV4SF);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltsiv2di \",\n-\t       VOID_FTYPE_PLONGLONG_QI_V4SI_V2DI_INT,\n-\t       IX86_BUILTIN_SCATTERALTSIV2DI);\n-\n-  def_builtin (OPTION_MASK_ISA_AVX512VL, 0, \"__builtin_ia32_scatteraltdiv4si \",\n-\t       VOID_FTYPE_PINT_QI_V2DI_V4SI_INT,\n-\t       IX86_BUILTIN_SCATTERALTDIV4SI);\n-\n-  /* AVX512PF */\n-  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_gatherpfdpd\",\n-\t       VOID_FTYPE_QI_V8SI_PCVOID_INT_INT,\n-\t       IX86_BUILTIN_GATHERPFDPD);\n-  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_gatherpfdps\",\n-\t       VOID_FTYPE_HI_V16SI_PCVOID_INT_INT,\n-\t       IX86_BUILTIN_GATHERPFDPS);\n-  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_gatherpfqpd\",\n-\t       VOID_FTYPE_QI_V8DI_PCVOID_INT_INT,\n-\t       IX86_BUILTIN_GATHERPFQPD);\n-  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_gatherpfqps\",\n-\t       VOID_FTYPE_QI_V8DI_PCVOID_INT_INT,\n-\t       IX86_BUILTIN_GATHERPFQPS);\n-  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_scatterpfdpd\",\n-\t       VOID_FTYPE_QI_V8SI_PCVOID_INT_INT,\n-\t       IX86_BUILTIN_SCATTERPFDPD);\n-  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_scatterpfdps\",\n-\t       VOID_FTYPE_HI_V16SI_PCVOID_INT_INT,\n-\t       IX86_BUILTIN_SCATTERPFDPS);\n-  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_scatterpfqpd\",\n-\t       VOID_FTYPE_QI_V8DI_PCVOID_INT_INT,\n-\t       IX86_BUILTIN_SCATTERPFQPD);\n-  def_builtin (OPTION_MASK_ISA_AVX512PF, 0, \"__builtin_ia32_scatterpfqps\",\n-\t       VOID_FTYPE_QI_V8DI_PCVOID_INT_INT,\n-\t       IX86_BUILTIN_SCATTERPFQPS);\n-\n-  /* SHA */\n-  def_builtin_const (OPTION_MASK_ISA_SHA, 0, \"__builtin_ia32_sha1msg1\",\n-\t\t     V4SI_FTYPE_V4SI_V4SI, IX86_BUILTIN_SHA1MSG1);\n-  def_builtin_const (OPTION_MASK_ISA_SHA, 0, \"__builtin_ia32_sha1msg2\",\n-\t\t     V4SI_FTYPE_V4SI_V4SI, IX86_BUILTIN_SHA1MSG2);\n-  def_builtin_const (OPTION_MASK_ISA_SHA, 0, \"__builtin_ia32_sha1nexte\",\n-\t\t     V4SI_FTYPE_V4SI_V4SI, IX86_BUILTIN_SHA1NEXTE);\n-  def_builtin_const (OPTION_MASK_ISA_SHA, 0, \"__builtin_ia32_sha1rnds4\",\n-\t\t     V4SI_FTYPE_V4SI_V4SI_INT, IX86_BUILTIN_SHA1RNDS4);\n-  def_builtin_const (OPTION_MASK_ISA_SHA, 0, \"__builtin_ia32_sha256msg1\",\n-\t\t     V4SI_FTYPE_V4SI_V4SI, IX86_BUILTIN_SHA256MSG1);\n-  def_builtin_const (OPTION_MASK_ISA_SHA, 0, \"__builtin_ia32_sha256msg2\",\n-\t\t     V4SI_FTYPE_V4SI_V4SI, IX86_BUILTIN_SHA256MSG2);\n-  def_builtin_const (OPTION_MASK_ISA_SHA, 0, \"__builtin_ia32_sha256rnds2\",\n-\t\t     V4SI_FTYPE_V4SI_V4SI_V4SI, IX86_BUILTIN_SHA256RNDS2);\n-\n-  /* RTM.  */\n-  def_builtin (OPTION_MASK_ISA_RTM, 0, \"__builtin_ia32_xabort\",\n-\t       VOID_FTYPE_UNSIGNED, IX86_BUILTIN_XABORT);\n-\n-  /* MMX access to the vec_init patterns.  */\n-  def_builtin_const (OPTION_MASK_ISA_MMX, 0,\n-\t\t     \"__builtin_ia32_vec_init_v2si\",\n-\t\t     V2SI_FTYPE_INT_INT, IX86_BUILTIN_VEC_INIT_V2SI);\n-\n-  def_builtin_const (OPTION_MASK_ISA_MMX, 0,\n-\t\t     \"__builtin_ia32_vec_init_v4hi\",\n-\t\t     V4HI_FTYPE_HI_HI_HI_HI,\n-\t\t     IX86_BUILTIN_VEC_INIT_V4HI);\n-\n-  def_builtin_const (OPTION_MASK_ISA_MMX, 0,\n-\t\t     \"__builtin_ia32_vec_init_v8qi\",\n-\t\t     V8QI_FTYPE_QI_QI_QI_QI_QI_QI_QI_QI,\n-\t\t     IX86_BUILTIN_VEC_INIT_V8QI);\n-\n-  /* Access to the vec_extract patterns.  */\n-  def_builtin_const (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_vec_ext_v2df\",\n-\t\t     DOUBLE_FTYPE_V2DF_INT, IX86_BUILTIN_VEC_EXT_V2DF);\n-  def_builtin_const (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_vec_ext_v2di\",\n-\t\t     DI_FTYPE_V2DI_INT, IX86_BUILTIN_VEC_EXT_V2DI);\n-  def_builtin_const (OPTION_MASK_ISA_SSE, 0, \"__builtin_ia32_vec_ext_v4sf\",\n-\t\t     FLOAT_FTYPE_V4SF_INT, IX86_BUILTIN_VEC_EXT_V4SF);\n-  def_builtin_const (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_vec_ext_v4si\",\n-\t\t     SI_FTYPE_V4SI_INT, IX86_BUILTIN_VEC_EXT_V4SI);\n-  def_builtin_const (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_vec_ext_v8hi\",\n-\t\t     HI_FTYPE_V8HI_INT, IX86_BUILTIN_VEC_EXT_V8HI);\n-\n-  def_builtin_const (OPTION_MASK_ISA_SSE | OPTION_MASK_ISA_3DNOW_A\n-\t\t     /* As it uses V4HImode, we have to require -mmmx too.  */\n-\t\t     | OPTION_MASK_ISA_MMX, 0,\n-\t\t     \"__builtin_ia32_vec_ext_v4hi\",\n-\t\t     HI_FTYPE_V4HI_INT, IX86_BUILTIN_VEC_EXT_V4HI);\n-\n-  def_builtin_const (OPTION_MASK_ISA_MMX, 0,\n-\t\t     \"__builtin_ia32_vec_ext_v2si\",\n-\t\t     SI_FTYPE_V2SI_INT, IX86_BUILTIN_VEC_EXT_V2SI);\n-\n-  def_builtin_const (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_vec_ext_v16qi\",\n-\t\t     QI_FTYPE_V16QI_INT, IX86_BUILTIN_VEC_EXT_V16QI);\n-\n-  /* Access to the vec_set patterns.  */\n-  def_builtin_const (OPTION_MASK_ISA_SSE4_1 | OPTION_MASK_ISA_64BIT, 0,\n-\t\t     \"__builtin_ia32_vec_set_v2di\",\n-\t\t     V2DI_FTYPE_V2DI_DI_INT, IX86_BUILTIN_VEC_SET_V2DI);\n-\n-  def_builtin_const (OPTION_MASK_ISA_SSE4_1, 0, \"__builtin_ia32_vec_set_v4sf\",\n-\t\t     V4SF_FTYPE_V4SF_FLOAT_INT, IX86_BUILTIN_VEC_SET_V4SF);\n-\n-  def_builtin_const (OPTION_MASK_ISA_SSE4_1, 0, \"__builtin_ia32_vec_set_v4si\",\n-\t\t     V4SI_FTYPE_V4SI_SI_INT, IX86_BUILTIN_VEC_SET_V4SI);\n-\n-  def_builtin_const (OPTION_MASK_ISA_SSE2, 0, \"__builtin_ia32_vec_set_v8hi\",\n-\t\t     V8HI_FTYPE_V8HI_HI_INT, IX86_BUILTIN_VEC_SET_V8HI);\n-\n-  def_builtin_const (OPTION_MASK_ISA_SSE | OPTION_MASK_ISA_3DNOW_A\n-\t\t     /* As it uses V4HImode, we have to require -mmmx too.  */\n-\t\t     | OPTION_MASK_ISA_MMX, 0,\n-\t\t     \"__builtin_ia32_vec_set_v4hi\",\n-\t\t     V4HI_FTYPE_V4HI_HI_INT, IX86_BUILTIN_VEC_SET_V4HI);\n-\n-  def_builtin_const (OPTION_MASK_ISA_SSE4_1, 0, \"__builtin_ia32_vec_set_v16qi\",\n-\t\t     V16QI_FTYPE_V16QI_QI_INT, IX86_BUILTIN_VEC_SET_V16QI);\n-\n-  /* RDSEED */\n-  def_builtin (OPTION_MASK_ISA_RDSEED, 0, \"__builtin_ia32_rdseed_hi_step\",\n-\t       INT_FTYPE_PUSHORT, IX86_BUILTIN_RDSEED16_STEP);\n-  def_builtin (OPTION_MASK_ISA_RDSEED, 0, \"__builtin_ia32_rdseed_si_step\",\n-\t       INT_FTYPE_PUNSIGNED, IX86_BUILTIN_RDSEED32_STEP);\n-  def_builtin (OPTION_MASK_ISA_RDSEED | OPTION_MASK_ISA_64BIT, 0,\n-\t       \"__builtin_ia32_rdseed_di_step\",\n-\t       INT_FTYPE_PULONGLONG, IX86_BUILTIN_RDSEED64_STEP);\n-\n-  /* ADCX */\n-  def_builtin (0, 0, \"__builtin_ia32_addcarryx_u32\",\n-\t       UCHAR_FTYPE_UCHAR_UINT_UINT_PUNSIGNED, IX86_BUILTIN_ADDCARRYX32);\n-  def_builtin (OPTION_MASK_ISA_64BIT, 0,\n-\t       \"__builtin_ia32_addcarryx_u64\",\n-\t       UCHAR_FTYPE_UCHAR_ULONGLONG_ULONGLONG_PULONGLONG,\n-\t       IX86_BUILTIN_ADDCARRYX64);\n-\n-  /* SBB */\n-  def_builtin (0, 0, \"__builtin_ia32_sbb_u32\",\n-\t       UCHAR_FTYPE_UCHAR_UINT_UINT_PUNSIGNED, IX86_BUILTIN_SBB32);\n-  def_builtin (OPTION_MASK_ISA_64BIT, 0,\n-\t       \"__builtin_ia32_sbb_u64\",\n-\t       UCHAR_FTYPE_UCHAR_ULONGLONG_ULONGLONG_PULONGLONG,\n-\t       IX86_BUILTIN_SBB64);\n-\n-  /* Read/write FLAGS.  */\n-  if (TARGET_64BIT)\n-    {\n-      def_builtin (OPTION_MASK_ISA_64BIT, 0, \"__builtin_ia32_readeflags_u64\",\n-\t\t   UINT64_FTYPE_VOID, IX86_BUILTIN_READ_FLAGS);\n-      def_builtin (OPTION_MASK_ISA_64BIT, 0, \"__builtin_ia32_writeeflags_u64\",\n-\t\t   VOID_FTYPE_UINT64, IX86_BUILTIN_WRITE_FLAGS);\n-    }\n-  else\n-    {\n-      def_builtin (0, 0, \"__builtin_ia32_readeflags_u32\",\n-\t\t   UNSIGNED_FTYPE_VOID, IX86_BUILTIN_READ_FLAGS);\n-      def_builtin (0, 0, \"__builtin_ia32_writeeflags_u32\",\n-\t\t   VOID_FTYPE_UNSIGNED, IX86_BUILTIN_WRITE_FLAGS);\n-    }\n-\n-  /* CLFLUSHOPT.  */\n-  def_builtin (OPTION_MASK_ISA_CLFLUSHOPT, 0, \"__builtin_ia32_clflushopt\",\n-\t       VOID_FTYPE_PCVOID, IX86_BUILTIN_CLFLUSHOPT);\n-\n-  /* CLWB.  */\n-  def_builtin (OPTION_MASK_ISA_CLWB, 0, \"__builtin_ia32_clwb\",\n-\t       VOID_FTYPE_PCVOID, IX86_BUILTIN_CLWB);\n-\n-  /* MONITORX and MWAITX.  */\n-  def_builtin (0, OPTION_MASK_ISA2_MWAITX, \"__builtin_ia32_monitorx\",\n-\t\tVOID_FTYPE_PCVOID_UNSIGNED_UNSIGNED, IX86_BUILTIN_MONITORX);\n-  def_builtin (0, OPTION_MASK_ISA2_MWAITX, \"__builtin_ia32_mwaitx\",\n-\t\tVOID_FTYPE_UNSIGNED_UNSIGNED_UNSIGNED, IX86_BUILTIN_MWAITX);\n-\n-  /* CLZERO.  */\n-  def_builtin (0, OPTION_MASK_ISA2_CLZERO, \"__builtin_ia32_clzero\",\n-\t\tVOID_FTYPE_PCVOID, IX86_BUILTIN_CLZERO);\n-\n-  /* WAITPKG.  */\n-  def_builtin (0, OPTION_MASK_ISA2_WAITPKG, \"__builtin_ia32_umonitor\",\n-\t       VOID_FTYPE_PVOID, IX86_BUILTIN_UMONITOR);\n-  def_builtin (0, OPTION_MASK_ISA2_WAITPKG, \"__builtin_ia32_umwait\",\n-\t       UINT8_FTYPE_UNSIGNED_UINT64, IX86_BUILTIN_UMWAIT);\n-  def_builtin (0, OPTION_MASK_ISA2_WAITPKG, \"__builtin_ia32_tpause\",\n-\t       UINT8_FTYPE_UNSIGNED_UINT64, IX86_BUILTIN_TPAUSE);\n-\n-  /* CLDEMOTE.  */\n-  def_builtin (0, OPTION_MASK_ISA2_CLDEMOTE, \"__builtin_ia32_cldemote\",\n-\t       VOID_FTYPE_PCVOID, IX86_BUILTIN_CLDEMOTE);\n-\n-  /* Add FMA4 multi-arg argument instructions */\n-  for (i = 0, d = bdesc_multi_arg; i < ARRAY_SIZE (bdesc_multi_arg); i++, d++)\n-    {\n-      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_MULTI_ARG_FIRST, i);\n-      if (d->name == 0)\n-\tcontinue;\n-\n-      ftype = (enum ix86_builtin_func_type) d->flag;\n-      def_builtin_const (d->mask, d->mask2, d->name, ftype, d->code);\n-    }\n-  BDESC_VERIFYS (IX86_BUILTIN__BDESC_MULTI_ARG_LAST,\n-\t\t IX86_BUILTIN__BDESC_MULTI_ARG_FIRST,\n-\t\t ARRAY_SIZE (bdesc_multi_arg) - 1);\n-\n-  /* Add CET inrinsics.  */\n-  for (i = 0, d = bdesc_cet; i < ARRAY_SIZE (bdesc_cet); i++, d++)\n-    {\n-      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_CET_FIRST, i);\n-      if (d->name == 0)\n-\tcontinue;\n-\n-      ftype = (enum ix86_builtin_func_type) d->flag;\n-      def_builtin (d->mask, d->mask2, d->name, ftype, d->code);\n-    }\n-  BDESC_VERIFYS (IX86_BUILTIN__BDESC_CET_LAST,\n-\t\t IX86_BUILTIN__BDESC_CET_FIRST,\n-\t\t ARRAY_SIZE (bdesc_cet) - 1);\n-\n-  for (i = 0, d = bdesc_cet_rdssp;\n-       i < ARRAY_SIZE (bdesc_cet_rdssp);\n-       i++, d++)\n-    {\n-      BDESC_VERIFY (d->code, IX86_BUILTIN__BDESC_CET_NORMAL_FIRST, i);\n-      if (d->name == 0)\n-\tcontinue;\n-\n-      ftype = (enum ix86_builtin_func_type) d->flag;\n-      def_builtin (d->mask, d->mask2, d->name, ftype, d->code);\n-    }\n-  BDESC_VERIFYS (IX86_BUILTIN__BDESC_CET_NORMAL_LAST,\n-\t\t IX86_BUILTIN__BDESC_CET_NORMAL_FIRST,\n-\t\t ARRAY_SIZE (bdesc_cet_rdssp) - 1);\n-}\n-\n-#undef BDESC_VERIFY\n-#undef BDESC_VERIFYS\n-\n-/* Make builtins to detect cpu type and features supported.  NAME is\n-   the builtin name, CODE is the builtin code, and FTYPE is the function\n-   type of the builtin.  */\n-\n-static void\n-make_cpu_type_builtin (const char* name, int code,\n-\t\t       enum ix86_builtin_func_type ftype, bool is_const)\n-{\n-  tree decl;\n-  tree type;\n-\n-  type = ix86_get_builtin_func_type (ftype);\n-  decl = add_builtin_function (name, type, code, BUILT_IN_MD,\n-\t\t\t       NULL, NULL_TREE);\n-  gcc_assert (decl != NULL_TREE);\n-  ix86_builtins[(int) code] = decl;\n-  TREE_READONLY (decl) = is_const;\n-}\n-\n-/* Make builtins to get CPU type and features supported.  The created\n-   builtins are :\n-\n-   __builtin_cpu_init (), to detect cpu type and features,\n-   __builtin_cpu_is (\"<CPUNAME>\"), to check if cpu is of type <CPUNAME>,\n-   __builtin_cpu_supports (\"<FEATURE>\"), to check if cpu supports <FEATURE>\n-   */\n-\n-static void\n-ix86_init_platform_type_builtins (void)\n-{\n-  make_cpu_type_builtin (\"__builtin_cpu_init\", IX86_BUILTIN_CPU_INIT,\n-\t\t\t INT_FTYPE_VOID, false);\n-  make_cpu_type_builtin (\"__builtin_cpu_is\", IX86_BUILTIN_CPU_IS,\n-\t\t\t INT_FTYPE_PCCHAR, true);\n-  make_cpu_type_builtin (\"__builtin_cpu_supports\", IX86_BUILTIN_CPU_SUPPORTS,\n-\t\t\t INT_FTYPE_PCCHAR, true);\n-}\n-\n-/* Internal method for ix86_init_builtins.  */\n-\n-static void\n-ix86_init_builtins_va_builtins_abi (void)\n-{\n-  tree ms_va_ref, sysv_va_ref;\n-  tree fnvoid_va_end_ms, fnvoid_va_end_sysv;\n-  tree fnvoid_va_start_ms, fnvoid_va_start_sysv;\n-  tree fnvoid_va_copy_ms, fnvoid_va_copy_sysv;\n-  tree fnattr_ms = NULL_TREE, fnattr_sysv = NULL_TREE;\n-\n-  if (!TARGET_64BIT)\n-    return;\n-  fnattr_ms = build_tree_list (get_identifier (\"ms_abi\"), NULL_TREE);\n-  fnattr_sysv = build_tree_list (get_identifier (\"sysv_abi\"), NULL_TREE);\n-  ms_va_ref = build_reference_type (ms_va_list_type_node);\n-  sysv_va_ref = build_pointer_type (TREE_TYPE (sysv_va_list_type_node));\n-\n-  fnvoid_va_end_ms = build_function_type_list (void_type_node, ms_va_ref,\n-\t\t\t\t\t       NULL_TREE);\n-  fnvoid_va_start_ms\n-    = build_varargs_function_type_list (void_type_node, ms_va_ref, NULL_TREE);\n-  fnvoid_va_end_sysv\n-    = build_function_type_list (void_type_node, sysv_va_ref, NULL_TREE);\n-  fnvoid_va_start_sysv\n-    = build_varargs_function_type_list (void_type_node, sysv_va_ref,\n-\t\t\t\t\tNULL_TREE);\n-  fnvoid_va_copy_ms\n-    = build_function_type_list (void_type_node, ms_va_ref,\n-\t\t\t\tms_va_list_type_node, NULL_TREE);\n-  fnvoid_va_copy_sysv\n-    = build_function_type_list (void_type_node, sysv_va_ref,\n-\t\t\t\tsysv_va_ref, NULL_TREE);\n-\n-  add_builtin_function (\"__builtin_ms_va_start\", fnvoid_va_start_ms,\n-  \t\t\tBUILT_IN_VA_START, BUILT_IN_NORMAL, NULL, fnattr_ms);\n-  add_builtin_function (\"__builtin_ms_va_end\", fnvoid_va_end_ms,\n-  \t\t\tBUILT_IN_VA_END, BUILT_IN_NORMAL, NULL, fnattr_ms);\n-  add_builtin_function (\"__builtin_ms_va_copy\", fnvoid_va_copy_ms,\n-\t\t\tBUILT_IN_VA_COPY, BUILT_IN_NORMAL, NULL, fnattr_ms);\n-  add_builtin_function (\"__builtin_sysv_va_start\", fnvoid_va_start_sysv,\n-  \t\t\tBUILT_IN_VA_START, BUILT_IN_NORMAL, NULL, fnattr_sysv);\n-  add_builtin_function (\"__builtin_sysv_va_end\", fnvoid_va_end_sysv,\n-  \t\t\tBUILT_IN_VA_END, BUILT_IN_NORMAL, NULL, fnattr_sysv);\n-  add_builtin_function (\"__builtin_sysv_va_copy\", fnvoid_va_copy_sysv,\n-\t\t\tBUILT_IN_VA_COPY, BUILT_IN_NORMAL, NULL, fnattr_sysv);\n-}\n-\n-static void\n-ix86_init_builtin_types (void)\n-{\n-  tree float80_type_node, const_string_type_node;\n-\n-  /* The __float80 type.  */\n-  float80_type_node = long_double_type_node;\n-  if (TYPE_MODE (float80_type_node) != XFmode)\n-    {\n-      if (float64x_type_node != NULL_TREE\n-\t  && TYPE_MODE (float64x_type_node) == XFmode)\n-\tfloat80_type_node = float64x_type_node;\n-      else\n-\t{\n-\t  /* The __float80 type.  */\n-\t  float80_type_node = make_node (REAL_TYPE);\n-\n-\t  TYPE_PRECISION (float80_type_node) = 80;\n-\t  layout_type (float80_type_node);\n-\t}\n-    }\n-  lang_hooks.types.register_builtin_type (float80_type_node, \"__float80\");\n-\n-  /* The __float128 type.  The node has already been created as\n-     _Float128, so we only need to register the __float128 name for\n-     it.  */\n-  lang_hooks.types.register_builtin_type (float128_type_node, \"__float128\");\n-\n-  const_string_type_node\n-    = build_pointer_type (build_qualified_type\n-\t\t\t  (char_type_node, TYPE_QUAL_CONST));\n-\n-  /* This macro is built by i386-builtin-types.awk.  */\n-  DEFINE_BUILTIN_PRIMITIVE_TYPES;\n-}\n-\n-void\n-ix86_init_builtins (void)\n-{\n-  tree ftype, decl;\n-\n-  ix86_init_builtin_types ();\n-\n-  /* Builtins to get CPU type and features. */\n-  ix86_init_platform_type_builtins ();\n-\n-  /* TFmode support builtins.  */\n-  def_builtin_const (0, 0, \"__builtin_infq\",\n-\t\t     FLOAT128_FTYPE_VOID, IX86_BUILTIN_INFQ);\n-  def_builtin_const (0, 0, \"__builtin_huge_valq\",\n-\t\t     FLOAT128_FTYPE_VOID, IX86_BUILTIN_HUGE_VALQ);\n-\n-  ftype = ix86_get_builtin_func_type (FLOAT128_FTYPE_CONST_STRING);\n-  decl = add_builtin_function (\"__builtin_nanq\", ftype, IX86_BUILTIN_NANQ,\n-\t\t\t       BUILT_IN_MD, \"nanq\", NULL_TREE);\n-  TREE_READONLY (decl) = 1;\n-  ix86_builtins[(int) IX86_BUILTIN_NANQ] = decl;\n-\n-  decl = add_builtin_function (\"__builtin_nansq\", ftype, IX86_BUILTIN_NANSQ,\n-\t\t\t       BUILT_IN_MD, \"nansq\", NULL_TREE);\n-  TREE_READONLY (decl) = 1;\n-  ix86_builtins[(int) IX86_BUILTIN_NANSQ] = decl;\n-\n-  /* We will expand them to normal call if SSE isn't available since\n-     they are used by libgcc. */\n-  ftype = ix86_get_builtin_func_type (FLOAT128_FTYPE_FLOAT128);\n-  decl = add_builtin_function (\"__builtin_fabsq\", ftype, IX86_BUILTIN_FABSQ,\n-\t\t\t       BUILT_IN_MD, \"__fabstf2\", NULL_TREE);\n-  TREE_READONLY (decl) = 1;\n-  ix86_builtins[(int) IX86_BUILTIN_FABSQ] = decl;\n-\n-  ftype = ix86_get_builtin_func_type (FLOAT128_FTYPE_FLOAT128_FLOAT128);\n-  decl = add_builtin_function (\"__builtin_copysignq\", ftype,\n-\t\t\t       IX86_BUILTIN_COPYSIGNQ, BUILT_IN_MD,\n-\t\t\t       \"__copysigntf3\", NULL_TREE);\n-  TREE_READONLY (decl) = 1;\n-  ix86_builtins[(int) IX86_BUILTIN_COPYSIGNQ] = decl;\n-\n-  ix86_init_tm_builtins ();\n-  ix86_init_mmx_sse_builtins ();\n-\n-  if (TARGET_LP64)\n-    ix86_init_builtins_va_builtins_abi ();\n-\n-#ifdef SUBTARGET_INIT_BUILTINS\n-  SUBTARGET_INIT_BUILTINS;\n-#endif\n-}\n-\n-/* Return the ix86 builtin for CODE.  */\n-\n-tree\n-ix86_builtin_decl (unsigned code, bool)\n-{\n-  if (code >= IX86_BUILTIN_MAX)\n-    return error_mark_node;\n-\n-  return ix86_builtins[code];\n-}\n-\n-/* This returns the target-specific builtin with code CODE if\n-   current_function_decl has visibility on this builtin, which is checked\n-   using isa flags.  Returns NULL_TREE otherwise.  */\n-\n-static tree ix86_get_builtin (enum ix86_builtins code)\n-{\n-  struct cl_target_option *opts;\n-  tree target_tree = NULL_TREE;\n-\n-  /* Determine the isa flags of current_function_decl.  */\n-\n-  if (current_function_decl)\n-    target_tree = DECL_FUNCTION_SPECIFIC_TARGET (current_function_decl);\n-\n-  if (target_tree == NULL)\n-    target_tree = target_option_default_node;\n-\n-  opts = TREE_TARGET_OPTION (target_tree);\n-\n-  if ((ix86_builtins_isa[(int) code].isa & opts->x_ix86_isa_flags)\n-      || (ix86_builtins_isa[(int) code].isa2 & opts->x_ix86_isa_flags2))\n-    return ix86_builtin_decl (code, true);\n-  else\n-    return NULL_TREE;\n-}\n-\n-/* Vectorization library interface and handlers.  */\n-tree (*ix86_veclib_handler) (combined_fn, tree, tree);\n-\n-/* Returns a function decl for a vectorized version of the combined function\n-   with combined_fn code FN and the result vector type TYPE, or NULL_TREE\n-   if it is not available.  */\n-\n-tree\n-ix86_builtin_vectorized_function (unsigned int fn, tree type_out,\n-\t\t\t\t  tree type_in)\n-{\n-  machine_mode in_mode, out_mode;\n-  int in_n, out_n;\n-\n-  if (TREE_CODE (type_out) != VECTOR_TYPE\n-      || TREE_CODE (type_in) != VECTOR_TYPE)\n-    return NULL_TREE;\n-\n-  out_mode = TYPE_MODE (TREE_TYPE (type_out));\n-  out_n = TYPE_VECTOR_SUBPARTS (type_out);\n-  in_mode = TYPE_MODE (TREE_TYPE (type_in));\n-  in_n = TYPE_VECTOR_SUBPARTS (type_in);\n-\n-  switch (fn)\n-    {\n-    CASE_CFN_EXP2:\n-      if (out_mode == SFmode && in_mode == SFmode)\n-\t{\n-\t  if (out_n == 16 && in_n == 16)\n-\t    return ix86_get_builtin (IX86_BUILTIN_EXP2PS);\n-\t}\n-      break;\n-\n-    CASE_CFN_IFLOOR:\n-    CASE_CFN_LFLOOR:\n-    CASE_CFN_LLFLOOR:\n-      /* The round insn does not trap on denormals.  */\n-      if (flag_trapping_math || !TARGET_SSE4_1)\n-\tbreak;\n-\n-      if (out_mode == SImode && in_mode == DFmode)\n-\t{\n-\t  if (out_n == 4 && in_n == 2)\n-\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD_VEC_PACK_SFIX);\n-\t  else if (out_n == 8 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD_VEC_PACK_SFIX256);\n-\t  else if (out_n == 16 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD_VEC_PACK_SFIX512);\n-\t}\n-      if (out_mode == SImode && in_mode == SFmode)\n-\t{\n-\t  if (out_n == 4 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPS_SFIX);\n-\t  else if (out_n == 8 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPS_SFIX256);\n-\t  else if (out_n == 16 && in_n == 16)\n-\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPS_SFIX512);\n-\t}\n-      break;\n-\n-    CASE_CFN_ICEIL:\n-    CASE_CFN_LCEIL:\n-    CASE_CFN_LLCEIL:\n-      /* The round insn does not trap on denormals.  */\n-      if (flag_trapping_math || !TARGET_SSE4_1)\n-\tbreak;\n-\n-      if (out_mode == SImode && in_mode == DFmode)\n-\t{\n-\t  if (out_n == 4 && in_n == 2)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CEILPD_VEC_PACK_SFIX);\n-\t  else if (out_n == 8 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CEILPD_VEC_PACK_SFIX256);\n-\t  else if (out_n == 16 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CEILPD_VEC_PACK_SFIX512);\n-\t}\n-      if (out_mode == SImode && in_mode == SFmode)\n-\t{\n-\t  if (out_n == 4 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CEILPS_SFIX);\n-\t  else if (out_n == 8 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CEILPS_SFIX256);\n-\t  else if (out_n == 16 && in_n == 16)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CEILPS_SFIX512);\n-\t}\n-      break;\n-\n-    CASE_CFN_IRINT:\n-    CASE_CFN_LRINT:\n-    CASE_CFN_LLRINT:\n-      if (out_mode == SImode && in_mode == DFmode)\n-\t{\n-\t  if (out_n == 4 && in_n == 2)\n-\t    return ix86_get_builtin (IX86_BUILTIN_VEC_PACK_SFIX);\n-\t  else if (out_n == 8 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_VEC_PACK_SFIX256);\n-\t  else if (out_n == 16 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_VEC_PACK_SFIX512);\n-\t}\n-      if (out_mode == SImode && in_mode == SFmode)\n-\t{\n-\t  if (out_n == 4 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CVTPS2DQ);\n-\t  else if (out_n == 8 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CVTPS2DQ256);\n-\t  else if (out_n == 16 && in_n == 16)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CVTPS2DQ512);\n-\t}\n-      break;\n-\n-    CASE_CFN_IROUND:\n-    CASE_CFN_LROUND:\n-    CASE_CFN_LLROUND:\n-      /* The round insn does not trap on denormals.  */\n-      if (flag_trapping_math || !TARGET_SSE4_1)\n-\tbreak;\n-\n-      if (out_mode == SImode && in_mode == DFmode)\n-\t{\n-\t  if (out_n == 4 && in_n == 2)\n-\t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPD_AZ_VEC_PACK_SFIX);\n-\t  else if (out_n == 8 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPD_AZ_VEC_PACK_SFIX256);\n-\t  else if (out_n == 16 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPD_AZ_VEC_PACK_SFIX512);\n-\t}\n-      if (out_mode == SImode && in_mode == SFmode)\n-\t{\n-\t  if (out_n == 4 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPS_AZ_SFIX);\n-\t  else if (out_n == 8 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPS_AZ_SFIX256);\n-\t  else if (out_n == 16 && in_n == 16)\n-\t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPS_AZ_SFIX512);\n-\t}\n-      break;\n-\n-    CASE_CFN_FLOOR:\n-      /* The round insn does not trap on denormals.  */\n-      if (flag_trapping_math || !TARGET_SSE4_1)\n-\tbreak;\n-\n-      if (out_mode == DFmode && in_mode == DFmode)\n-\t{\n-\t  if (out_n == 2 && in_n == 2)\n-\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD);\n-\t  else if (out_n == 4 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD256);\n-\t  else if (out_n == 8 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD512);\n-\t}\n-      if (out_mode == SFmode && in_mode == SFmode)\n-\t{\n-\t  if (out_n == 4 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPS);\n-\t  else if (out_n == 8 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPS256);\n-\t  else if (out_n == 16 && in_n == 16)\n-\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPS512);\n-\t}\n-      break;\n-\n-    CASE_CFN_CEIL:\n-      /* The round insn does not trap on denormals.  */\n-      if (flag_trapping_math || !TARGET_SSE4_1)\n-\tbreak;\n-\n-      if (out_mode == DFmode && in_mode == DFmode)\n-\t{\n-\t  if (out_n == 2 && in_n == 2)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CEILPD);\n-\t  else if (out_n == 4 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CEILPD256);\n-\t  else if (out_n == 8 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CEILPD512);\n-\t}\n-      if (out_mode == SFmode && in_mode == SFmode)\n-\t{\n-\t  if (out_n == 4 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CEILPS);\n-\t  else if (out_n == 8 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CEILPS256);\n-\t  else if (out_n == 16 && in_n == 16)\n-\t    return ix86_get_builtin (IX86_BUILTIN_CEILPS512);\n-\t}\n-      break;\n-\n-    CASE_CFN_TRUNC:\n-      /* The round insn does not trap on denormals.  */\n-      if (flag_trapping_math || !TARGET_SSE4_1)\n-\tbreak;\n-\n-      if (out_mode == DFmode && in_mode == DFmode)\n-\t{\n-\t  if (out_n == 2 && in_n == 2)\n-\t    return ix86_get_builtin (IX86_BUILTIN_TRUNCPD);\n-\t  else if (out_n == 4 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_TRUNCPD256);\n-\t  else if (out_n == 8 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_TRUNCPD512);\n-\t}\n-      if (out_mode == SFmode && in_mode == SFmode)\n-\t{\n-\t  if (out_n == 4 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_TRUNCPS);\n-\t  else if (out_n == 8 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_TRUNCPS256);\n-\t  else if (out_n == 16 && in_n == 16)\n-\t    return ix86_get_builtin (IX86_BUILTIN_TRUNCPS512);\n-\t}\n-      break;\n-\n-    CASE_CFN_FMA:\n-      if (out_mode == DFmode && in_mode == DFmode)\n-\t{\n-\t  if (out_n == 2 && in_n == 2)\n-\t    return ix86_get_builtin (IX86_BUILTIN_VFMADDPD);\n-\t  if (out_n == 4 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_VFMADDPD256);\n-\t}\n-      if (out_mode == SFmode && in_mode == SFmode)\n-\t{\n-\t  if (out_n == 4 && in_n == 4)\n-\t    return ix86_get_builtin (IX86_BUILTIN_VFMADDPS);\n-\t  if (out_n == 8 && in_n == 8)\n-\t    return ix86_get_builtin (IX86_BUILTIN_VFMADDPS256);\n-\t}\n-      break;\n-\n-    default:\n-      break;\n-    }\n-\n-  /* Dispatch to a handler for a vectorization library.  */\n-  if (ix86_veclib_handler)\n-    return ix86_veclib_handler (combined_fn (fn), type_out, type_in);\n-\n-  return NULL_TREE;\n-}\n-\n-/* Returns a decl of a function that implements gather load with\n-   memory type MEM_VECTYPE and index type INDEX_VECTYPE and SCALE.\n-   Return NULL_TREE if it is not available.  */\n-\n-tree\n-ix86_vectorize_builtin_gather (const_tree mem_vectype,\n-\t\t\t       const_tree index_type, int scale)\n-{\n-  bool si;\n-  enum ix86_builtins code;\n-\n-  if (! TARGET_AVX2 || !TARGET_USE_GATHER)\n-    return NULL_TREE;\n-\n-  if ((TREE_CODE (index_type) != INTEGER_TYPE\n-       && !POINTER_TYPE_P (index_type))\n-      || (TYPE_MODE (index_type) != SImode\n-\t  && TYPE_MODE (index_type) != DImode))\n-    return NULL_TREE;\n-\n-  if (TYPE_PRECISION (index_type) > POINTER_SIZE)\n-    return NULL_TREE;\n-\n-  /* v*gather* insn sign extends index to pointer mode.  */\n-  if (TYPE_PRECISION (index_type) < POINTER_SIZE\n-      && TYPE_UNSIGNED (index_type))\n-    return NULL_TREE;\n-\n-  if (scale <= 0\n-      || scale > 8\n-      || (scale & (scale - 1)) != 0)\n-    return NULL_TREE;\n-\n-  si = TYPE_MODE (index_type) == SImode;\n-  switch (TYPE_MODE (mem_vectype))\n-    {\n-    case E_V2DFmode:\n-      if (TARGET_AVX512VL)\n-\tcode = si ? IX86_BUILTIN_GATHER3SIV2DF : IX86_BUILTIN_GATHER3DIV2DF;\n-      else\n-\tcode = si ? IX86_BUILTIN_GATHERSIV2DF : IX86_BUILTIN_GATHERDIV2DF;\n-      break;\n-    case E_V4DFmode:\n-      if (TARGET_AVX512VL)\n-\tcode = si ? IX86_BUILTIN_GATHER3ALTSIV4DF : IX86_BUILTIN_GATHER3DIV4DF;\n-      else\n-\tcode = si ? IX86_BUILTIN_GATHERALTSIV4DF : IX86_BUILTIN_GATHERDIV4DF;\n-      break;\n-    case E_V2DImode:\n-      if (TARGET_AVX512VL)\n-\tcode = si ? IX86_BUILTIN_GATHER3SIV2DI : IX86_BUILTIN_GATHER3DIV2DI;\n-      else\n-\tcode = si ? IX86_BUILTIN_GATHERSIV2DI : IX86_BUILTIN_GATHERDIV2DI;\n-      break;\n-    case E_V4DImode:\n-      if (TARGET_AVX512VL)\n-\tcode = si ? IX86_BUILTIN_GATHER3ALTSIV4DI : IX86_BUILTIN_GATHER3DIV4DI;\n-      else\n-\tcode = si ? IX86_BUILTIN_GATHERALTSIV4DI : IX86_BUILTIN_GATHERDIV4DI;\n-      break;\n-    case E_V4SFmode:\n-      if (TARGET_AVX512VL)\n-\tcode = si ? IX86_BUILTIN_GATHER3SIV4SF : IX86_BUILTIN_GATHER3DIV4SF;\n-      else\n-\tcode = si ? IX86_BUILTIN_GATHERSIV4SF : IX86_BUILTIN_GATHERDIV4SF;\n-      break;\n-    case E_V8SFmode:\n-      if (TARGET_AVX512VL)\n-\tcode = si ? IX86_BUILTIN_GATHER3SIV8SF : IX86_BUILTIN_GATHER3ALTDIV8SF;\n-      else\n-\tcode = si ? IX86_BUILTIN_GATHERSIV8SF : IX86_BUILTIN_GATHERALTDIV8SF;\n-      break;\n-    case E_V4SImode:\n-      if (TARGET_AVX512VL)\n-\tcode = si ? IX86_BUILTIN_GATHER3SIV4SI : IX86_BUILTIN_GATHER3DIV4SI;\n-      else\n-\tcode = si ? IX86_BUILTIN_GATHERSIV4SI : IX86_BUILTIN_GATHERDIV4SI;\n-      break;\n-    case E_V8SImode:\n-      if (TARGET_AVX512VL)\n-\tcode = si ? IX86_BUILTIN_GATHER3SIV8SI : IX86_BUILTIN_GATHER3ALTDIV8SI;\n-      else\n-\tcode = si ? IX86_BUILTIN_GATHERSIV8SI : IX86_BUILTIN_GATHERALTDIV8SI;\n-      break;\n-    case E_V8DFmode:\n-      if (TARGET_AVX512F)\n-\tcode = si ? IX86_BUILTIN_GATHER3ALTSIV8DF : IX86_BUILTIN_GATHER3DIV8DF;\n-      else\n-\treturn NULL_TREE;\n-      break;\n-    case E_V8DImode:\n-      if (TARGET_AVX512F)\n-\tcode = si ? IX86_BUILTIN_GATHER3ALTSIV8DI : IX86_BUILTIN_GATHER3DIV8DI;\n-      else\n-\treturn NULL_TREE;\n-      break;\n-    case E_V16SFmode:\n-      if (TARGET_AVX512F)\n-\tcode = si ? IX86_BUILTIN_GATHER3SIV16SF : IX86_BUILTIN_GATHER3ALTDIV16SF;\n-      else\n-\treturn NULL_TREE;\n-      break;\n-    case E_V16SImode:\n-      if (TARGET_AVX512F)\n-\tcode = si ? IX86_BUILTIN_GATHER3SIV16SI : IX86_BUILTIN_GATHER3ALTDIV16SI;\n-      else\n-\treturn NULL_TREE;\n-      break;\n-    default:\n-      return NULL_TREE;\n-    }\n-\n-  return ix86_get_builtin (code);\n-}\n-\n-/* Returns a code for a target-specific builtin that implements\n-   reciprocal of the function, or NULL_TREE if not available.  */\n-\n-tree\n-ix86_builtin_reciprocal (tree fndecl)\n-{\n-  enum ix86_builtins fn_code\n-    = (enum ix86_builtins) DECL_MD_FUNCTION_CODE (fndecl);\n-  switch (fn_code)\n-    {\n-      /* Vectorized version of sqrt to rsqrt conversion.  */\n-    case IX86_BUILTIN_SQRTPS_NR:\n-      return ix86_get_builtin (IX86_BUILTIN_RSQRTPS_NR);\n-\n-    case IX86_BUILTIN_SQRTPS_NR256:\n-      return ix86_get_builtin (IX86_BUILTIN_RSQRTPS_NR256);\n-\n-    default:\n-      return NULL_TREE;\n-    }\n-}\n-\n-/* Priority of i386 features, greater value is higher priority.   This is\n-   used to decide the order in which function dispatch must happen.  For\n-   instance, a version specialized for SSE4.2 should be checked for dispatch\n-   before a version for SSE3, as SSE4.2 implies SSE3.  */\n-enum feature_priority\n-{\n-  P_ZERO = 0,\n-  P_MMX,\n-  P_SSE,\n-  P_SSE2,\n-  P_SSE3,\n-  P_SSSE3,\n-  P_PROC_SSSE3,\n-  P_SSE4_A,\n-  P_PROC_SSE4_A,\n-  P_SSE4_1,\n-  P_SSE4_2,\n-  P_PROC_SSE4_2,\n-  P_POPCNT,\n-  P_AES,\n-  P_PCLMUL,\n-  P_AVX,\n-  P_PROC_AVX,\n-  P_BMI,\n-  P_PROC_BMI,\n-  P_FMA4,\n-  P_XOP,\n-  P_PROC_XOP,\n-  P_FMA,\n-  P_PROC_FMA,\n-  P_BMI2,\n-  P_AVX2,\n-  P_PROC_AVX2,\n-  P_AVX512F,\n-  P_PROC_AVX512F\n-};\n-\n-/* This is the order of bit-fields in __processor_features in cpuinfo.c */\n-enum processor_features\n-{\n-  F_CMOV = 0,\n-  F_MMX,\n-  F_POPCNT,\n-  F_SSE,\n-  F_SSE2,\n-  F_SSE3,\n-  F_SSSE3,\n-  F_SSE4_1,\n-  F_SSE4_2,\n-  F_AVX,\n-  F_AVX2,\n-  F_SSE4_A,\n-  F_FMA4,\n-  F_XOP,\n-  F_FMA,\n-  F_AVX512F,\n-  F_BMI,\n-  F_BMI2,\n-  F_AES,\n-  F_PCLMUL,\n-  F_AVX512VL,\n-  F_AVX512BW,\n-  F_AVX512DQ,\n-  F_AVX512CD,\n-  F_AVX512ER,\n-  F_AVX512PF,\n-  F_AVX512VBMI,\n-  F_AVX512IFMA,\n-  F_AVX5124VNNIW,\n-  F_AVX5124FMAPS,\n-  F_AVX512VPOPCNTDQ,\n-  F_AVX512VBMI2,\n-  F_GFNI,\n-  F_VPCLMULQDQ,\n-  F_AVX512VNNI,\n-  F_AVX512BITALG,\n-  F_AVX512BF16,\n-  F_AVX512VP2INTERSECT,\n-  F_MAX\n-};\n-\n-/* These are the values for vendor types and cpu types  and subtypes\n-   in cpuinfo.c.  Cpu types and subtypes should be subtracted by\n-   the corresponding start value.  */\n-enum processor_model\n-{\n-  M_INTEL = 1,\n-  M_AMD,\n-  M_CPU_TYPE_START,\n-  M_INTEL_BONNELL,\n-  M_INTEL_CORE2,\n-  M_INTEL_COREI7,\n-  M_AMDFAM10H,\n-  M_AMDFAM15H,\n-  M_INTEL_SILVERMONT,\n-  M_INTEL_KNL,\n-  M_AMD_BTVER1,\n-  M_AMD_BTVER2,\n-  M_AMDFAM17H,\n-  M_INTEL_KNM,\n-  M_INTEL_GOLDMONT,\n-  M_INTEL_GOLDMONT_PLUS,\n-  M_INTEL_TREMONT,\n-  M_CPU_SUBTYPE_START,\n-  M_INTEL_COREI7_NEHALEM,\n-  M_INTEL_COREI7_WESTMERE,\n-  M_INTEL_COREI7_SANDYBRIDGE,\n-  M_AMDFAM10H_BARCELONA,\n-  M_AMDFAM10H_SHANGHAI,\n-  M_AMDFAM10H_ISTANBUL,\n-  M_AMDFAM15H_BDVER1,\n-  M_AMDFAM15H_BDVER2,\n-  M_AMDFAM15H_BDVER3,\n-  M_AMDFAM15H_BDVER4,\n-  M_AMDFAM17H_ZNVER1,\n-  M_INTEL_COREI7_IVYBRIDGE,\n-  M_INTEL_COREI7_HASWELL,\n-  M_INTEL_COREI7_BROADWELL,\n-  M_INTEL_COREI7_SKYLAKE,\n-  M_INTEL_COREI7_SKYLAKE_AVX512,\n-  M_INTEL_COREI7_CANNONLAKE,\n-  M_INTEL_COREI7_ICELAKE_CLIENT,\n-  M_INTEL_COREI7_ICELAKE_SERVER,\n-  M_AMDFAM17H_ZNVER2,\n-  M_INTEL_COREI7_CASCADELAKE,\n-  M_INTEL_COREI7_TIGERLAKE,\n-  M_INTEL_COREI7_COOPERLAKE\n-};\n-\n-struct _arch_names_table\n-{\n-  const char *const name;\n-  const enum processor_model model;\n-};\n-\n-static const _arch_names_table arch_names_table[] =\n-{\n-  {\"amd\", M_AMD},\n-  {\"intel\", M_INTEL},\n-  {\"atom\", M_INTEL_BONNELL},\n-  {\"slm\", M_INTEL_SILVERMONT},\n-  {\"core2\", M_INTEL_CORE2},\n-  {\"corei7\", M_INTEL_COREI7},\n-  {\"nehalem\", M_INTEL_COREI7_NEHALEM},\n-  {\"westmere\", M_INTEL_COREI7_WESTMERE},\n-  {\"sandybridge\", M_INTEL_COREI7_SANDYBRIDGE},\n-  {\"ivybridge\", M_INTEL_COREI7_IVYBRIDGE},\n-  {\"haswell\", M_INTEL_COREI7_HASWELL},\n-  {\"broadwell\", M_INTEL_COREI7_BROADWELL},\n-  {\"skylake\", M_INTEL_COREI7_SKYLAKE},\n-  {\"skylake-avx512\", M_INTEL_COREI7_SKYLAKE_AVX512},\n-  {\"cannonlake\", M_INTEL_COREI7_CANNONLAKE},\n-  {\"icelake-client\", M_INTEL_COREI7_ICELAKE_CLIENT},\n-  {\"icelake-server\", M_INTEL_COREI7_ICELAKE_SERVER},\n-  {\"cascadelake\", M_INTEL_COREI7_CASCADELAKE},\n-  {\"tigerlake\", M_INTEL_COREI7_TIGERLAKE},\n-  {\"cooperlake\", M_INTEL_COREI7_COOPERLAKE},\n-  {\"bonnell\", M_INTEL_BONNELL},\n-  {\"silvermont\", M_INTEL_SILVERMONT},\n-  {\"goldmont\", M_INTEL_GOLDMONT},\n-  {\"goldmont-plus\", M_INTEL_GOLDMONT_PLUS},\n-  {\"tremont\", M_INTEL_TREMONT},\n-  {\"knl\", M_INTEL_KNL},\n-  {\"knm\", M_INTEL_KNM},\n-  {\"amdfam10h\", M_AMDFAM10H},\n-  {\"barcelona\", M_AMDFAM10H_BARCELONA},\n-  {\"shanghai\", M_AMDFAM10H_SHANGHAI},\n-  {\"istanbul\", M_AMDFAM10H_ISTANBUL},\n-  {\"btver1\", M_AMD_BTVER1},\n-  {\"amdfam15h\", M_AMDFAM15H},\n-  {\"bdver1\", M_AMDFAM15H_BDVER1},\n-  {\"bdver2\", M_AMDFAM15H_BDVER2},\n-  {\"bdver3\", M_AMDFAM15H_BDVER3},\n-  {\"bdver4\", M_AMDFAM15H_BDVER4},\n-  {\"btver2\", M_AMD_BTVER2},\n-  {\"amdfam17h\", M_AMDFAM17H},\n-  {\"znver1\", M_AMDFAM17H_ZNVER1},\n-  {\"znver2\", M_AMDFAM17H_ZNVER2},\n-};\n-\n-/* These are the target attribute strings for which a dispatcher is\n-   available, from fold_builtin_cpu.  */\n-struct _isa_names_table\n-{\n-  const char *const name;\n-  const enum processor_features feature;\n-  const enum feature_priority priority;\n-};\n-\n-static const _isa_names_table isa_names_table[] =\n-{\n-  {\"cmov\",    F_CMOV,\tP_ZERO},\n-  {\"mmx\",     F_MMX,\tP_MMX},\n-  {\"popcnt\",  F_POPCNT,\tP_POPCNT},\n-  {\"sse\",     F_SSE,\tP_SSE},\n-  {\"sse2\",    F_SSE2,\tP_SSE2},\n-  {\"sse3\",    F_SSE3,\tP_SSE3},\n-  {\"ssse3\",   F_SSSE3,\tP_SSSE3},\n-  {\"sse4a\",   F_SSE4_A,\tP_SSE4_A},\n-  {\"sse4.1\",  F_SSE4_1,\tP_SSE4_1},\n-  {\"sse4.2\",  F_SSE4_2,\tP_SSE4_2},\n-  {\"avx\",     F_AVX,\tP_AVX},\n-  {\"fma4\",    F_FMA4,\tP_FMA4},\n-  {\"xop\",     F_XOP,\tP_XOP},\n-  {\"fma\",     F_FMA,\tP_FMA},\n-  {\"avx2\",    F_AVX2,\tP_AVX2},\n-  {\"avx512f\", F_AVX512F, P_AVX512F},\n-  {\"bmi\",     F_BMI,\tP_BMI},\n-  {\"bmi2\",    F_BMI2,\tP_BMI2},\n-  {\"aes\",     F_AES,\tP_AES},\n-  {\"pclmul\",  F_PCLMUL,\tP_PCLMUL},\n-  {\"avx512vl\",F_AVX512VL, P_ZERO},\n-  {\"avx512bw\",F_AVX512BW, P_ZERO},\n-  {\"avx512dq\",F_AVX512DQ, P_ZERO},\n-  {\"avx512cd\",F_AVX512CD, P_ZERO},\n-  {\"avx512er\",F_AVX512ER, P_ZERO},\n-  {\"avx512pf\",F_AVX512PF, P_ZERO},\n-  {\"avx512vbmi\",F_AVX512VBMI, P_ZERO},\n-  {\"avx512ifma\",F_AVX512IFMA, P_ZERO},\n-  {\"avx5124vnniw\",F_AVX5124VNNIW, P_ZERO},\n-  {\"avx5124fmaps\",F_AVX5124FMAPS, P_ZERO},\n-  {\"avx512vpopcntdq\",F_AVX512VPOPCNTDQ,\tP_ZERO},\n-  {\"avx512vbmi2\", F_AVX512VBMI2, P_ZERO},\n-  {\"gfni\",\tF_GFNI,\tP_ZERO},\n-  {\"vpclmulqdq\", F_VPCLMULQDQ, P_ZERO},\n-  {\"avx512vnni\", F_AVX512VNNI, P_ZERO},\n-  {\"avx512bitalg\", F_AVX512BITALG, P_ZERO},\n-  {\"avx512bf16\", F_AVX512BF16, P_ZERO},\n-  {\"avx512vp2intersect\",F_AVX512VP2INTERSECT, P_ZERO}\n-};\n-\n-/* This parses the attribute arguments to target in DECL and determines\n-   the right builtin to use to match the platform specification.\n-   It returns the priority value for this version decl.  If PREDICATE_LIST\n-   is not NULL, it stores the list of cpu features that need to be checked\n-   before dispatching this function.  */\n-\n-unsigned int\n-get_builtin_code_for_version (tree decl, tree *predicate_list)\n-{\n-  tree attrs;\n-  struct cl_target_option cur_target;\n-  tree target_node;\n-  struct cl_target_option *new_target;\n-  const char *arg_str = NULL;\n-  const char *attrs_str = NULL;\n-  char *tok_str = NULL;\n-  char *token;\n-\n-  enum feature_priority priority = P_ZERO;\n-\n-  static unsigned int NUM_FEATURES\n-    = sizeof (isa_names_table) / sizeof (_isa_names_table);\n-\n-  unsigned int i;\n-\n-  tree predicate_chain = NULL_TREE;\n-  tree predicate_decl, predicate_arg;\n-\n-  attrs = lookup_attribute (\"target\", DECL_ATTRIBUTES (decl));\n-  gcc_assert (attrs != NULL);\n-\n-  attrs = TREE_VALUE (TREE_VALUE (attrs));\n-\n-  gcc_assert (TREE_CODE (attrs) == STRING_CST);\n-  attrs_str = TREE_STRING_POINTER (attrs);\n-\n-  /* Return priority zero for default function.  */\n-  if (strcmp (attrs_str, \"default\") == 0)\n-    return 0;\n-\n-  /* Handle arch= if specified.  For priority, set it to be 1 more than\n-     the best instruction set the processor can handle.  For instance, if\n-     there is a version for atom and a version for ssse3 (the highest ISA\n-     priority for atom), the atom version must be checked for dispatch\n-     before the ssse3 version. */\n-  if (strstr (attrs_str, \"arch=\") != NULL)\n-    {\n-      cl_target_option_save (&cur_target, &global_options);\n-      target_node\n-\t= ix86_valid_target_attribute_tree (decl, attrs, &global_options,\n-\t\t\t\t\t    &global_options_set, 0);\n-    \n-      gcc_assert (target_node);\n-      if (target_node == error_mark_node)\n-\treturn 0;\n-      new_target = TREE_TARGET_OPTION (target_node);\n-      gcc_assert (new_target);\n-      \n-      if (new_target->arch_specified && new_target->arch > 0)\n-\t{\n-\t  switch (new_target->arch)\n-\t    {\n-\t    case PROCESSOR_CORE2:\n-\t      arg_str = \"core2\";\n-\t      priority = P_PROC_SSSE3;\n-\t      break;\n-\t    case PROCESSOR_NEHALEM:\n-\t      if (new_target->x_ix86_isa_flags & OPTION_MASK_ISA_PCLMUL)\n-\t\t{\n-\t\t  arg_str = \"westmere\";\n-\t\t  priority = P_PCLMUL;\n-\t\t}\n-\t      else\n-\t\t{\n-\t\t  /* We translate \"arch=corei7\" and \"arch=nehalem\" to\n-\t\t     \"corei7\" so that it will be mapped to M_INTEL_COREI7\n-\t\t     as cpu type to cover all M_INTEL_COREI7_XXXs.  */\n-\t\t  arg_str = \"corei7\";\n-\t\t  priority = P_PROC_SSE4_2;\n-\t\t}\n-\t      break;\n-\t    case PROCESSOR_SANDYBRIDGE:\n-\t      if (new_target->x_ix86_isa_flags & OPTION_MASK_ISA_F16C)\n-\t\targ_str = \"ivybridge\";\n-\t      else\n-\t\targ_str = \"sandybridge\";\n-\t      priority = P_PROC_AVX;\n-\t      break;\n-\t    case PROCESSOR_HASWELL:\n-\t      if (new_target->x_ix86_isa_flags & OPTION_MASK_ISA_ADX)\n-\t\targ_str = \"broadwell\";\n-\t      else\n-\t\targ_str = \"haswell\";\n-\t      priority = P_PROC_AVX2;\n-\t      break;\n-\t    case PROCESSOR_SKYLAKE:\n-\t      arg_str = \"skylake\";\n-\t      priority = P_PROC_AVX2;\n-\t      break;\n-\t    case PROCESSOR_SKYLAKE_AVX512:\n-\t      arg_str = \"skylake-avx512\";\n-\t      priority = P_PROC_AVX512F;\n-\t      break;\n-\t    case PROCESSOR_CANNONLAKE:\n-\t      arg_str = \"cannonlake\";\n-\t      priority = P_PROC_AVX512F;\n-\t      break;\n-\t    case PROCESSOR_ICELAKE_CLIENT:\n-\t      arg_str = \"icelake-client\";\n-\t      priority = P_PROC_AVX512F;\n-\t      break;\n-\t    case PROCESSOR_ICELAKE_SERVER:\n-\t      arg_str = \"icelake-server\";\n-\t      priority = P_PROC_AVX512F;\n-\t      break;\n-\t    case PROCESSOR_CASCADELAKE:\n-\t      arg_str = \"cascadelake\";\n-\t      priority = P_PROC_AVX512F;\n-\t      break;\n-\t    case PROCESSOR_TIGERLAKE:\n-\t      arg_str = \"tigerlake\";\n-\t      priority = P_PROC_AVX512F;\n-\t      break;\n-\t    case PROCESSOR_COOPERLAKE:\n-\t      arg_str = \"cooperlake\";\n-\t      priority = P_PROC_AVX512F;\n-\t      break;\n-\t    case PROCESSOR_BONNELL:\n-\t      arg_str = \"bonnell\";\n-\t      priority = P_PROC_SSSE3;\n-\t      break;\n-\t    case PROCESSOR_KNL:\n-\t      arg_str = \"knl\";\n-\t      priority = P_PROC_AVX512F;\n-\t      break;\n-\t    case PROCESSOR_KNM:\n-\t      arg_str = \"knm\";\n-\t      priority = P_PROC_AVX512F;\n-\t      break;\n-\t    case PROCESSOR_SILVERMONT:\n-\t      arg_str = \"silvermont\";\n-\t      priority = P_PROC_SSE4_2;\n-\t      break;\n-\t    case PROCESSOR_GOLDMONT:\n-\t      arg_str = \"goldmont\";\n-\t      priority = P_PROC_SSE4_2;\n-\t      break;\n-\t    case PROCESSOR_GOLDMONT_PLUS:\n-\t      arg_str = \"goldmont-plus\";\n-\t      priority = P_PROC_SSE4_2;\n-\t      break;\n-\t    case PROCESSOR_TREMONT:\n-\t      arg_str = \"tremont\";\n-\t      priority = P_PROC_SSE4_2;\n-\t      break;\n-\t    case PROCESSOR_AMDFAM10:\n-\t      arg_str = \"amdfam10h\";\n-\t      priority = P_PROC_SSE4_A;\n-\t      break;\n-\t    case PROCESSOR_BTVER1:\n-\t      arg_str = \"btver1\";\n-\t      priority = P_PROC_SSE4_A;\n-\t      break;\n-\t    case PROCESSOR_BTVER2:\n-\t      arg_str = \"btver2\";\n-\t      priority = P_PROC_BMI;\n-\t      break;\n-\t    case PROCESSOR_BDVER1:\n-\t      arg_str = \"bdver1\";\n-\t      priority = P_PROC_XOP;\n-\t      break;\n-\t    case PROCESSOR_BDVER2:\n-\t      arg_str = \"bdver2\";\n-\t      priority = P_PROC_FMA;\n-\t      break;\n-\t    case PROCESSOR_BDVER3:\n-\t      arg_str = \"bdver3\";\n-\t      priority = P_PROC_FMA;\n-\t      break;\n-\t    case PROCESSOR_BDVER4:\n-\t      arg_str = \"bdver4\";\n-\t      priority = P_PROC_AVX2;\n-\t      break;\n-\t    case PROCESSOR_ZNVER1:\n-\t      arg_str = \"znver1\";\n-\t      priority = P_PROC_AVX2;\n-\t      break;\n-\t    case PROCESSOR_ZNVER2:\n-\t      arg_str = \"znver2\";\n-\t      priority = P_PROC_AVX2;\n-\t      break;\n-\t    }\n-\t}\n-\n-      cl_target_option_restore (&global_options, &cur_target);\n-\t\n-      if (predicate_list && arg_str == NULL)\n-\t{\n-\t  error_at (DECL_SOURCE_LOCATION (decl),\n-\t\t    \"no dispatcher found for the versioning attributes\");\n-\t  return 0;\n-\t}\n-    \n-      if (predicate_list)\n-\t{\n-          predicate_decl = ix86_builtins [(int) IX86_BUILTIN_CPU_IS];\n-          /* For a C string literal the length includes the trailing NULL.  */\n-          predicate_arg = build_string_literal (strlen (arg_str) + 1, arg_str);\n-          predicate_chain = tree_cons (predicate_decl, predicate_arg,\n-\t\t\t\t       predicate_chain);\n-\t}\n-    }\n-\n-  /* Process feature name.  */\n-  tok_str =  (char *) xmalloc (strlen (attrs_str) + 1);\n-  strcpy (tok_str, attrs_str);\n-  token = strtok (tok_str, \",\");\n-  predicate_decl = ix86_builtins [(int) IX86_BUILTIN_CPU_SUPPORTS];\n-\n-  while (token != NULL)\n-    {\n-      /* Do not process \"arch=\"  */\n-      if (strncmp (token, \"arch=\", 5) == 0)\n-\t{\n-\t  token = strtok (NULL, \",\");\n-\t  continue;\n-\t}\n-      for (i = 0; i < NUM_FEATURES; ++i)\n-\t{\n-\t  if (strcmp (token, isa_names_table[i].name) == 0)\n-\t    {\n-\t      if (predicate_list)\n-\t\t{\n-\t\t  predicate_arg = build_string_literal (\n-\t\t\t\t  strlen (isa_names_table[i].name) + 1,\n-\t\t\t\t  isa_names_table[i].name);\n-\t\t  predicate_chain = tree_cons (predicate_decl, predicate_arg,\n-\t\t\t\t\t       predicate_chain);\n-\t\t}\n-\t      /* Find the maximum priority feature.  */\n-\t      if (isa_names_table[i].priority > priority)\n-\t\tpriority = isa_names_table[i].priority;\n-\n-\t      break;\n-\t    }\n-\t}\n-      if (predicate_list && priority == P_ZERO)\n-\t{\n-\t  error_at (DECL_SOURCE_LOCATION (decl),\n-\t\t    \"ISA %qs is not supported in %<target%> attribute, \"\n-\t\t    \"use %<arch=%> syntax\", token);\n-\t  return 0;\n-\t}\n-      token = strtok (NULL, \",\");\n-    }\n-  free (tok_str);\n-\n-  if (predicate_list && predicate_chain == NULL_TREE)\n-    {\n-      error_at (DECL_SOURCE_LOCATION (decl),\n-\t        \"no dispatcher found for the versioning attributes: %s\",\n-\t        attrs_str);\n-      return 0;\n-    }\n-  else if (predicate_list)\n-    {\n-      predicate_chain = nreverse (predicate_chain);\n-      *predicate_list = predicate_chain;\n-    }\n-\n-  return priority; \n-}\n-\n-/* This builds the processor_model struct type defined in\n-   libgcc/config/i386/cpuinfo.c  */\n-\n-static tree\n-build_processor_model_struct (void)\n-{\n-  const char *field_name[] = {\"__cpu_vendor\", \"__cpu_type\", \"__cpu_subtype\",\n-\t\t\t      \"__cpu_features\"};\n-  tree field = NULL_TREE, field_chain = NULL_TREE;\n-  int i;\n-  tree type = make_node (RECORD_TYPE);\n-\n-  /* The first 3 fields are unsigned int.  */\n-  for (i = 0; i < 3; ++i)\n-    {\n-      field = build_decl (UNKNOWN_LOCATION, FIELD_DECL,\n-\t\t\t  get_identifier (field_name[i]), unsigned_type_node);\n-      if (field_chain != NULL_TREE)\n-\tDECL_CHAIN (field) = field_chain;\n-      field_chain = field;\n-    }\n-\n-  /* The last field is an array of unsigned integers of size one.  */\n-  field = build_decl (UNKNOWN_LOCATION, FIELD_DECL,\n-\t\t      get_identifier (field_name[3]),\n-\t\t      build_array_type (unsigned_type_node,\n-\t\t\t\t\tbuild_index_type (size_one_node)));\n-  if (field_chain != NULL_TREE)\n-    DECL_CHAIN (field) = field_chain;\n-  field_chain = field;\n-\n-  finish_builtin_struct (type, \"__processor_model\", field_chain, NULL_TREE);\n-  return type;\n-}\n-\n-/* Returns a extern, comdat VAR_DECL of type TYPE and name NAME. */\n-\n-static tree\n-make_var_decl (tree type, const char *name)\n-{\n-  tree new_decl;\n-\n-  new_decl = build_decl (UNKNOWN_LOCATION,\n-\t                 VAR_DECL,\n-\t  \t         get_identifier(name),\n-\t\t         type);\n-\n-  DECL_EXTERNAL (new_decl) = 1;\n-  TREE_STATIC (new_decl) = 1;\n-  TREE_PUBLIC (new_decl) = 1;\n-  DECL_INITIAL (new_decl) = 0;\n-  DECL_ARTIFICIAL (new_decl) = 0;\n-  DECL_PRESERVE_P (new_decl) = 1;\n-\n-  make_decl_one_only (new_decl, DECL_ASSEMBLER_NAME (new_decl));\n-  assemble_variable (new_decl, 0, 0, 0);\n-\n-  return new_decl;\n-}\n-\n-/* FNDECL is a __builtin_cpu_is or a __builtin_cpu_supports call that is folded\n-   into an integer defined in libgcc/config/i386/cpuinfo.c */\n-\n-tree\n-fold_builtin_cpu (tree fndecl, tree *args)\n-{\n-  unsigned int i;\n-  enum ix86_builtins fn_code\n-    = (enum ix86_builtins) DECL_MD_FUNCTION_CODE (fndecl);\n-  tree param_string_cst = NULL;\n-\n-  tree __processor_model_type = build_processor_model_struct ();\n-  tree __cpu_model_var = make_var_decl (__processor_model_type,\n-\t\t\t\t\t\"__cpu_model\");\n-\n-\n-  varpool_node::add (__cpu_model_var);\n-\n-  gcc_assert ((args != NULL) && (*args != NULL));\n-\n-  param_string_cst = *args;\n-  while (param_string_cst\n-\t && TREE_CODE (param_string_cst) !=  STRING_CST)\n-    {\n-      /* *args must be a expr that can contain other EXPRS leading to a\n-\t STRING_CST.   */\n-      if (!EXPR_P (param_string_cst))\n- \t{\n-\t  error (\"parameter to builtin must be a string constant or literal\");\n-\t  return integer_zero_node;\n-\t}\n-      param_string_cst = TREE_OPERAND (EXPR_CHECK (param_string_cst), 0);\n-    }\n-\n-  gcc_assert (param_string_cst);\n-\n-  if (fn_code == IX86_BUILTIN_CPU_IS)\n-    {\n-      tree ref;\n-      tree field;\n-      tree final;\n-\n-      unsigned int field_val = 0;\n-      unsigned int NUM_ARCH_NAMES\n-\t= sizeof (arch_names_table) / sizeof (struct _arch_names_table);\n-\n-      for (i = 0; i < NUM_ARCH_NAMES; i++)\n-\tif (strcmp (arch_names_table[i].name,\n-\t    TREE_STRING_POINTER (param_string_cst)) == 0)\n-\t  break;\n-\n-      if (i == NUM_ARCH_NAMES)\n-\t{\n-\t  error (\"parameter to builtin not valid: %s\",\n-\t         TREE_STRING_POINTER (param_string_cst));\n-\t  return integer_zero_node;\n-\t}\n-\n-      field = TYPE_FIELDS (__processor_model_type);\n-      field_val = arch_names_table[i].model;\n-\n-      /* CPU types are stored in the next field.  */\n-      if (field_val > M_CPU_TYPE_START\n-\t  && field_val < M_CPU_SUBTYPE_START)\n-\t{\n-\t  field = DECL_CHAIN (field);\n-\t  field_val -= M_CPU_TYPE_START;\n-\t}\n-\n-      /* CPU subtypes are stored in the next field.  */\n-      if (field_val > M_CPU_SUBTYPE_START)\n-\t{\n-\t  field = DECL_CHAIN ( DECL_CHAIN (field));\n-\t  field_val -= M_CPU_SUBTYPE_START;\n-\t}\n-\n-      /* Get the appropriate field in __cpu_model.  */\n-      ref = build3 (COMPONENT_REF, TREE_TYPE (field), __cpu_model_var,\n-\t\t    field, NULL_TREE);\n-\n-      /* Check the value.  */\n-      final = build2 (EQ_EXPR, unsigned_type_node, ref,\n-\t\t      build_int_cstu (unsigned_type_node, field_val));\n-      return build1 (CONVERT_EXPR, integer_type_node, final);\n-    }\n-  else if (fn_code == IX86_BUILTIN_CPU_SUPPORTS)\n-    {\n-      tree ref;\n-      tree array_elt;\n-      tree field;\n-      tree final;\n-\n-      unsigned int field_val = 0;\n-      unsigned int NUM_ISA_NAMES\n-\t= sizeof (isa_names_table) / sizeof (struct _isa_names_table);\n-\n-      for (i = 0; i < NUM_ISA_NAMES; i++)\n-\tif (strcmp (isa_names_table[i].name,\n-\t    TREE_STRING_POINTER (param_string_cst)) == 0)\n-\t  break;\n-\n-      if (i == NUM_ISA_NAMES)\n-\t{\n-\t  error (\"parameter to builtin not valid: %s\",\n-\t       \t TREE_STRING_POINTER (param_string_cst));\n-\t  return integer_zero_node;\n-\t}\n-\n-      if (isa_names_table[i].feature >= 32)\n-\t{\n-\t  tree __cpu_features2_var = make_var_decl (unsigned_type_node,\n-\t\t\t\t\t\t    \"__cpu_features2\");\n-\n-\t  varpool_node::add (__cpu_features2_var);\n-\t  field_val = (1U << (isa_names_table[i].feature - 32));\n-\t  /* Return __cpu_features2 & field_val  */\n-\t  final = build2 (BIT_AND_EXPR, unsigned_type_node,\n-\t\t\t  __cpu_features2_var,\n-\t\t\t  build_int_cstu (unsigned_type_node, field_val));\n-\t  return build1 (CONVERT_EXPR, integer_type_node, final);\n-\t}\n-\n-      field = TYPE_FIELDS (__processor_model_type);\n-      /* Get the last field, which is __cpu_features.  */\n-      while (DECL_CHAIN (field))\n-        field = DECL_CHAIN (field);\n-\n-      /* Get the appropriate field: __cpu_model.__cpu_features  */\n-      ref = build3 (COMPONENT_REF, TREE_TYPE (field), __cpu_model_var,\n-\t\t    field, NULL_TREE);\n-\n-      /* Access the 0th element of __cpu_features array.  */\n-      array_elt = build4 (ARRAY_REF, unsigned_type_node, ref,\n-\t\t\t  integer_zero_node, NULL_TREE, NULL_TREE);\n-\n-      field_val = (1U << isa_names_table[i].feature);\n-      /* Return __cpu_model.__cpu_features[0] & field_val  */\n-      final = build2 (BIT_AND_EXPR, unsigned_type_node, array_elt,\n-\t\t      build_int_cstu (unsigned_type_node, field_val));\n-      return build1 (CONVERT_EXPR, integer_type_node, final);\n-    }\n-  gcc_unreachable ();\n-}\n-\n-#include \"gt-i386-builtins.h\""}, {"sha": "56fec11846e734b26e7697c8451f74974766fa5c", "filename": "gcc/config/i386/i386-d.c", "status": "removed", "additions": 0, "deletions": 44, "changes": 44, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fi386-d.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fi386-d.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-d.c?ref=566c5f1aaae120d2283103e68ecf1c1a83dd4459", "patch": "@@ -1,44 +0,0 @@\n-/* Subroutines for the D front end on the x86 architecture.\n-   Copyright (C) 2017-2020 Free Software Foundation, Inc.\n-\n-GCC is free software; you can redistribute it and/or modify\n-it under the terms of the GNU General Public License as published by\n-the Free Software Foundation; either version 3, or (at your option)\n-any later version.\n-\n-GCC is distributed in the hope that it will be useful,\n-but WITHOUT ANY WARRANTY; without even the implied warranty of\n-MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n-GNU General Public License for more details.\n-\n-You should have received a copy of the GNU General Public License\n-along with GCC; see the file COPYING3.  If not see\n-<http://www.gnu.org/licenses/>.  */\n-\n-#include \"config.h\"\n-#include \"system.h\"\n-#include \"coretypes.h\"\n-#include \"tm.h\"\n-#include \"d/d-target.h\"\n-#include \"d/d-target-def.h\"\n-\n-/* Implement TARGET_D_CPU_VERSIONS for x86 targets.  */\n-\n-void\n-ix86_d_target_versions (void)\n-{\n-  if (TARGET_64BIT)\n-    {\n-      d_add_builtin_version (\"X86_64\");\n-\n-      if (TARGET_X32)\n-\td_add_builtin_version (\"D_X32\");\n-    }\n-  else\n-    d_add_builtin_version (\"X86\");\n-\n-  if (TARGET_80387)\n-    d_add_builtin_version (\"D_HardFloat\");\n-  else\n-    d_add_builtin_version (\"D_SoftFloat\");\n-}"}, {"sha": "270585decb2cb951d6cc32048d2fb0e22f19b21b", "filename": "gcc/config/i386/i386-expand.c", "status": "removed", "additions": 0, "deletions": 20310, "changes": 20310, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fi386-expand.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fi386-expand.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-expand.c?ref=566c5f1aaae120d2283103e68ecf1c1a83dd4459"}, {"sha": "b9b764c092af95884777644e50ef7086f97ba915", "filename": "gcc/config/i386/i386-features.c", "status": "removed", "additions": 0, "deletions": 2884, "changes": 2884, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fi386-features.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fi386-features.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-features.c?ref=566c5f1aaae120d2283103e68ecf1c1a83dd4459", "patch": "@@ -1,2884 +0,0 @@\n-/* Copyright (C) 1988-2020 Free Software Foundation, Inc.\n-\n-This file is part of GCC.\n-\n-GCC is free software; you can redistribute it and/or modify\n-it under the terms of the GNU General Public License as published by\n-the Free Software Foundation; either version 3, or (at your option)\n-any later version.\n-\n-GCC is distributed in the hope that it will be useful,\n-but WITHOUT ANY WARRANTY; without even the implied warranty of\n-MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n-GNU General Public License for more details.\n-\n-You should have received a copy of the GNU General Public License\n-along with GCC; see the file COPYING3.  If not see\n-<http://www.gnu.org/licenses/>.  */\n-\n-#define IN_TARGET_CODE 1\n-\n-#include \"config.h\"\n-#include \"system.h\"\n-#include \"coretypes.h\"\n-#include \"backend.h\"\n-#include \"rtl.h\"\n-#include \"tree.h\"\n-#include \"memmodel.h\"\n-#include \"gimple.h\"\n-#include \"cfghooks.h\"\n-#include \"cfgloop.h\"\n-#include \"df.h\"\n-#include \"tm_p.h\"\n-#include \"stringpool.h\"\n-#include \"expmed.h\"\n-#include \"optabs.h\"\n-#include \"regs.h\"\n-#include \"emit-rtl.h\"\n-#include \"recog.h\"\n-#include \"cgraph.h\"\n-#include \"diagnostic.h\"\n-#include \"cfgbuild.h\"\n-#include \"alias.h\"\n-#include \"fold-const.h\"\n-#include \"attribs.h\"\n-#include \"calls.h\"\n-#include \"stor-layout.h\"\n-#include \"varasm.h\"\n-#include \"output.h\"\n-#include \"insn-attr.h\"\n-#include \"flags.h\"\n-#include \"except.h\"\n-#include \"explow.h\"\n-#include \"expr.h\"\n-#include \"cfgrtl.h\"\n-#include \"common/common-target.h\"\n-#include \"langhooks.h\"\n-#include \"reload.h\"\n-#include \"gimplify.h\"\n-#include \"dwarf2.h\"\n-#include \"tm-constrs.h\"\n-#include \"cselib.h\"\n-#include \"sched-int.h\"\n-#include \"opts.h\"\n-#include \"tree-pass.h\"\n-#include \"context.h\"\n-#include \"pass_manager.h\"\n-#include \"target-globals.h\"\n-#include \"gimple-iterator.h\"\n-#include \"tree-vectorizer.h\"\n-#include \"shrink-wrap.h\"\n-#include \"builtins.h\"\n-#include \"rtl-iter.h\"\n-#include \"tree-iterator.h\"\n-#include \"dbgcnt.h\"\n-#include \"case-cfn-macros.h\"\n-#include \"dojump.h\"\n-#include \"fold-const-call.h\"\n-#include \"tree-vrp.h\"\n-#include \"tree-ssanames.h\"\n-#include \"selftest.h\"\n-#include \"selftest-rtl.h\"\n-#include \"print-rtl.h\"\n-#include \"intl.h\"\n-#include \"ifcvt.h\"\n-#include \"symbol-summary.h\"\n-#include \"ipa-prop.h\"\n-#include \"ipa-fnsummary.h\"\n-#include \"wide-int-bitmask.h\"\n-#include \"tree-vector-builder.h\"\n-#include \"debug.h\"\n-#include \"dwarf2out.h\"\n-#include \"i386-builtins.h\"\n-#include \"i386-features.h\"\n-\n-const char * const xlogue_layout::STUB_BASE_NAMES[XLOGUE_STUB_COUNT] = {\n-  \"savms64\",\n-  \"resms64\",\n-  \"resms64x\",\n-  \"savms64f\",\n-  \"resms64f\",\n-  \"resms64fx\"\n-};\n-\n-const unsigned xlogue_layout::REG_ORDER[xlogue_layout::MAX_REGS] = {\n-/* The below offset values are where each register is stored for the layout\n-   relative to incoming stack pointer.  The value of each m_regs[].offset will\n-   be relative to the incoming base pointer (rax or rsi) used by the stub.\n-\n-    s_instances:   0\t\t1\t\t2\t\t3\n-    Offset:\t\t\t\t\trealigned or\taligned + 8\n-    Register\t   aligned\taligned + 8\taligned w/HFP\tw/HFP\t*/\n-    XMM15_REG,\t/* 0x10\t\t0x18\t\t0x10\t\t0x18\t*/\n-    XMM14_REG,\t/* 0x20\t\t0x28\t\t0x20\t\t0x28\t*/\n-    XMM13_REG,\t/* 0x30\t\t0x38\t\t0x30\t\t0x38\t*/\n-    XMM12_REG,\t/* 0x40\t\t0x48\t\t0x40\t\t0x48\t*/\n-    XMM11_REG,\t/* 0x50\t\t0x58\t\t0x50\t\t0x58\t*/\n-    XMM10_REG,\t/* 0x60\t\t0x68\t\t0x60\t\t0x68\t*/\n-    XMM9_REG,\t/* 0x70\t\t0x78\t\t0x70\t\t0x78\t*/\n-    XMM8_REG,\t/* 0x80\t\t0x88\t\t0x80\t\t0x88\t*/\n-    XMM7_REG,\t/* 0x90\t\t0x98\t\t0x90\t\t0x98\t*/\n-    XMM6_REG,\t/* 0xa0\t\t0xa8\t\t0xa0\t\t0xa8\t*/\n-    SI_REG,\t/* 0xa8\t\t0xb0\t\t0xa8\t\t0xb0\t*/\n-    DI_REG,\t/* 0xb0\t\t0xb8\t\t0xb0\t\t0xb8\t*/\n-    BX_REG,\t/* 0xb8\t\t0xc0\t\t0xb8\t\t0xc0\t*/\n-    BP_REG,\t/* 0xc0\t\t0xc8\t\tN/A\t\tN/A\t*/\n-    R12_REG,\t/* 0xc8\t\t0xd0\t\t0xc0\t\t0xc8\t*/\n-    R13_REG,\t/* 0xd0\t\t0xd8\t\t0xc8\t\t0xd0\t*/\n-    R14_REG,\t/* 0xd8\t\t0xe0\t\t0xd0\t\t0xd8\t*/\n-    R15_REG,\t/* 0xe0\t\t0xe8\t\t0xd8\t\t0xe0\t*/\n-};\n-\n-/* Instantiate static const values.  */\n-const HOST_WIDE_INT xlogue_layout::STUB_INDEX_OFFSET;\n-const unsigned xlogue_layout::MIN_REGS;\n-const unsigned xlogue_layout::MAX_REGS;\n-const unsigned xlogue_layout::MAX_EXTRA_REGS;\n-const unsigned xlogue_layout::VARIANT_COUNT;\n-const unsigned xlogue_layout::STUB_NAME_MAX_LEN;\n-\n-/* Initialize xlogue_layout::s_stub_names to zero.  */\n-char xlogue_layout::s_stub_names[2][XLOGUE_STUB_COUNT][VARIANT_COUNT]\n-\t\t\t\t[STUB_NAME_MAX_LEN];\n-\n-/* Instantiates all xlogue_layout instances.  */\n-const xlogue_layout xlogue_layout::s_instances[XLOGUE_SET_COUNT] = {\n-  xlogue_layout (0, false),\n-  xlogue_layout (8, false),\n-  xlogue_layout (0, true),\n-  xlogue_layout (8, true)\n-};\n-\n-/* Return an appropriate const instance of xlogue_layout based upon values\n-   in cfun->machine and crtl.  */\n-const class xlogue_layout &\n-xlogue_layout::get_instance ()\n-{\n-  enum xlogue_stub_sets stub_set;\n-  bool aligned_plus_8 = cfun->machine->call_ms2sysv_pad_in;\n-\n-  if (stack_realign_fp)\n-    stub_set = XLOGUE_SET_HFP_ALIGNED_OR_REALIGN;\n-  else if (frame_pointer_needed)\n-    stub_set = aligned_plus_8\n-\t      ? XLOGUE_SET_HFP_ALIGNED_PLUS_8\n-\t      : XLOGUE_SET_HFP_ALIGNED_OR_REALIGN;\n-  else\n-    stub_set = aligned_plus_8 ? XLOGUE_SET_ALIGNED_PLUS_8 : XLOGUE_SET_ALIGNED;\n-\n-  return s_instances[stub_set];\n-}\n-\n-/* Determine how many clobbered registers can be saved by the stub.\n-   Returns the count of registers the stub will save and restore.  */\n-unsigned\n-xlogue_layout::count_stub_managed_regs ()\n-{\n-  bool hfp = frame_pointer_needed || stack_realign_fp;\n-  unsigned i, count;\n-  unsigned regno;\n-\n-  for (count = i = MIN_REGS; i < MAX_REGS; ++i)\n-    {\n-      regno = REG_ORDER[i];\n-      if (regno == BP_REG && hfp)\n-\tcontinue;\n-      if (!ix86_save_reg (regno, false, false))\n-\tbreak;\n-      ++count;\n-    }\n-  return count;\n-}\n-\n-/* Determine if register REGNO is a stub managed register given the\n-   total COUNT of stub managed registers.  */\n-bool\n-xlogue_layout::is_stub_managed_reg (unsigned regno, unsigned count)\n-{\n-  bool hfp = frame_pointer_needed || stack_realign_fp;\n-  unsigned i;\n-\n-  for (i = 0; i < count; ++i)\n-    {\n-      gcc_assert (i < MAX_REGS);\n-      if (REG_ORDER[i] == BP_REG && hfp)\n-\t++count;\n-      else if (REG_ORDER[i] == regno)\n-\treturn true;\n-    }\n-  return false;\n-}\n-\n-/* Constructor for xlogue_layout.  */\n-xlogue_layout::xlogue_layout (HOST_WIDE_INT stack_align_off_in, bool hfp)\n-  : m_hfp (hfp) , m_nregs (hfp ? 17 : 18),\n-    m_stack_align_off_in (stack_align_off_in)\n-{\n-  HOST_WIDE_INT offset = stack_align_off_in;\n-  unsigned i, j;\n-\n-  for (i = j = 0; i < MAX_REGS; ++i)\n-    {\n-      unsigned regno = REG_ORDER[i];\n-\n-      if (regno == BP_REG && hfp)\n-\tcontinue;\n-      if (SSE_REGNO_P (regno))\n-\t{\n-\t  offset += 16;\n-\t  /* Verify that SSE regs are always aligned.  */\n-\t  gcc_assert (!((stack_align_off_in + offset) & 15));\n-\t}\n-      else\n-\toffset += 8;\n-\n-      m_regs[j].regno    = regno;\n-      m_regs[j++].offset = offset - STUB_INDEX_OFFSET;\n-    }\n-  gcc_assert (j == m_nregs);\n-}\n-\n-const char *\n-xlogue_layout::get_stub_name (enum xlogue_stub stub,\n-\t\t\t      unsigned n_extra_regs)\n-{\n-  const int have_avx = TARGET_AVX;\n-  char *name = s_stub_names[!!have_avx][stub][n_extra_regs];\n-\n-  /* Lazy init */\n-  if (!*name)\n-    {\n-      int res = snprintf (name, STUB_NAME_MAX_LEN, \"__%s_%s_%u\",\n-\t\t\t  (have_avx ? \"avx\" : \"sse\"),\n-\t\t\t  STUB_BASE_NAMES[stub],\n-\t\t\t  MIN_REGS + n_extra_regs);\n-      gcc_checking_assert (res < (int)STUB_NAME_MAX_LEN);\n-    }\n-\n-  return name;\n-}\n-\n-/* Return rtx of a symbol ref for the entry point (based upon\n-   cfun->machine->call_ms2sysv_extra_regs) of the specified stub.  */\n-rtx\n-xlogue_layout::get_stub_rtx (enum xlogue_stub stub)\n-{\n-  const unsigned n_extra_regs = cfun->machine->call_ms2sysv_extra_regs;\n-  gcc_checking_assert (n_extra_regs <= MAX_EXTRA_REGS);\n-  gcc_assert (stub < XLOGUE_STUB_COUNT);\n-  gcc_assert (crtl->stack_realign_finalized);\n-\n-  return gen_rtx_SYMBOL_REF (Pmode, get_stub_name (stub, n_extra_regs));\n-}\n-\n-unsigned scalar_chain::max_id = 0;\n-\n-namespace {\n-\n-/* Initialize new chain.  */\n-\n-scalar_chain::scalar_chain (enum machine_mode smode_, enum machine_mode vmode_)\n-{\n-  smode = smode_;\n-  vmode = vmode_;\n-\n-  chain_id = ++max_id;\n-\n-   if (dump_file)\n-    fprintf (dump_file, \"Created a new instruction chain #%d\\n\", chain_id);\n-\n-  bitmap_obstack_initialize (NULL);\n-  insns = BITMAP_ALLOC (NULL);\n-  defs = BITMAP_ALLOC (NULL);\n-  defs_conv = BITMAP_ALLOC (NULL);\n-  queue = NULL;\n-}\n-\n-/* Free chain's data.  */\n-\n-scalar_chain::~scalar_chain ()\n-{\n-  BITMAP_FREE (insns);\n-  BITMAP_FREE (defs);\n-  BITMAP_FREE (defs_conv);\n-  bitmap_obstack_release (NULL);\n-}\n-\n-/* Add instruction into chains' queue.  */\n-\n-void\n-scalar_chain::add_to_queue (unsigned insn_uid)\n-{\n-  if (bitmap_bit_p (insns, insn_uid)\n-      || bitmap_bit_p (queue, insn_uid))\n-    return;\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"  Adding insn %d into chain's #%d queue\\n\",\n-\t     insn_uid, chain_id);\n-  bitmap_set_bit (queue, insn_uid);\n-}\n-\n-general_scalar_chain::general_scalar_chain (enum machine_mode smode_,\n-\t\t\t\t\t    enum machine_mode vmode_)\n-     : scalar_chain (smode_, vmode_)\n-{\n-  insns_conv = BITMAP_ALLOC (NULL);\n-  n_sse_to_integer = 0;\n-  n_integer_to_sse = 0;\n-}\n-\n-general_scalar_chain::~general_scalar_chain ()\n-{\n-  BITMAP_FREE (insns_conv);\n-}\n-\n-/* For DImode conversion, mark register defined by DEF as requiring\n-   conversion.  */\n-\n-void\n-general_scalar_chain::mark_dual_mode_def (df_ref def)\n-{\n-  gcc_assert (DF_REF_REG_DEF_P (def));\n-\n-  /* Record the def/insn pair so we can later efficiently iterate over\n-     the defs to convert on insns not in the chain.  */\n-  bool reg_new = bitmap_set_bit (defs_conv, DF_REF_REGNO (def));\n-  if (!bitmap_bit_p (insns, DF_REF_INSN_UID (def)))\n-    {\n-      if (!bitmap_set_bit (insns_conv, DF_REF_INSN_UID (def))\n-\t  && !reg_new)\n-\treturn;\n-      n_integer_to_sse++;\n-    }\n-  else\n-    {\n-      if (!reg_new)\n-\treturn;\n-      n_sse_to_integer++;\n-    }\n- \n-  if (dump_file)\n-    fprintf (dump_file,\n-\t     \"  Mark r%d def in insn %d as requiring both modes in chain #%d\\n\",\n-\t     DF_REF_REGNO (def), DF_REF_INSN_UID (def), chain_id);\n-}\n-\n-/* For TImode conversion, it is unused.  */\n-\n-void\n-timode_scalar_chain::mark_dual_mode_def (df_ref)\n-{\n-  gcc_unreachable ();\n-}\n-\n-/* Check REF's chain to add new insns into a queue\n-   and find registers requiring conversion.  */\n-\n-void\n-scalar_chain::analyze_register_chain (bitmap candidates, df_ref ref)\n-{\n-  df_link *chain;\n-\n-  gcc_assert (bitmap_bit_p (insns, DF_REF_INSN_UID (ref))\n-\t      || bitmap_bit_p (candidates, DF_REF_INSN_UID (ref)));\n-  add_to_queue (DF_REF_INSN_UID (ref));\n-\n-  for (chain = DF_REF_CHAIN (ref); chain; chain = chain->next)\n-    {\n-      unsigned uid = DF_REF_INSN_UID (chain->ref);\n-\n-      if (!NONDEBUG_INSN_P (DF_REF_INSN (chain->ref)))\n-\tcontinue;\n-\n-      if (!DF_REF_REG_MEM_P (chain->ref))\n-\t{\n-\t  if (bitmap_bit_p (insns, uid))\n-\t    continue;\n-\n-\t  if (bitmap_bit_p (candidates, uid))\n-\t    {\n-\t      add_to_queue (uid);\n-\t      continue;\n-\t    }\n-\t}\n-\n-      if (DF_REF_REG_DEF_P (chain->ref))\n-\t{\n-\t  if (dump_file)\n-\t    fprintf (dump_file, \"  r%d def in insn %d isn't convertible\\n\",\n-\t\t     DF_REF_REGNO (chain->ref), uid);\n-\t  mark_dual_mode_def (chain->ref);\n-\t}\n-      else\n-\t{\n-\t  if (dump_file)\n-\t    fprintf (dump_file, \"  r%d use in insn %d isn't convertible\\n\",\n-\t\t     DF_REF_REGNO (chain->ref), uid);\n-\t  mark_dual_mode_def (ref);\n-\t}\n-    }\n-}\n-\n-/* Add instruction into a chain.  */\n-\n-void\n-scalar_chain::add_insn (bitmap candidates, unsigned int insn_uid)\n-{\n-  if (bitmap_bit_p (insns, insn_uid))\n-    return;\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"  Adding insn %d to chain #%d\\n\", insn_uid, chain_id);\n-\n-  bitmap_set_bit (insns, insn_uid);\n-\n-  rtx_insn *insn = DF_INSN_UID_GET (insn_uid)->insn;\n-  rtx def_set = single_set (insn);\n-  if (def_set && REG_P (SET_DEST (def_set))\n-      && !HARD_REGISTER_P (SET_DEST (def_set)))\n-    bitmap_set_bit (defs, REGNO (SET_DEST (def_set)));\n-\n-  /* ???  The following is quadratic since analyze_register_chain\n-     iterates over all refs to look for dual-mode regs.  Instead this\n-     should be done separately for all regs mentioned in the chain once.  */\n-  df_ref ref;\n-  for (ref = DF_INSN_UID_DEFS (insn_uid); ref; ref = DF_REF_NEXT_LOC (ref))\n-    if (!HARD_REGISTER_P (DF_REF_REG (ref)))\n-      analyze_register_chain (candidates, ref);\n-  for (ref = DF_INSN_UID_USES (insn_uid); ref; ref = DF_REF_NEXT_LOC (ref))\n-    if (!DF_REF_REG_MEM_P (ref))\n-      analyze_register_chain (candidates, ref);\n-}\n-\n-/* Build new chain starting from insn INSN_UID recursively\n-   adding all dependent uses and definitions.  */\n-\n-void\n-scalar_chain::build (bitmap candidates, unsigned insn_uid)\n-{\n-  queue = BITMAP_ALLOC (NULL);\n-  bitmap_set_bit (queue, insn_uid);\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"Building chain #%d...\\n\", chain_id);\n-\n-  while (!bitmap_empty_p (queue))\n-    {\n-      insn_uid = bitmap_first_set_bit (queue);\n-      bitmap_clear_bit (queue, insn_uid);\n-      bitmap_clear_bit (candidates, insn_uid);\n-      add_insn (candidates, insn_uid);\n-    }\n-\n-  if (dump_file)\n-    {\n-      fprintf (dump_file, \"Collected chain #%d...\\n\", chain_id);\n-      fprintf (dump_file, \"  insns: \");\n-      dump_bitmap (dump_file, insns);\n-      if (!bitmap_empty_p (defs_conv))\n-\t{\n-\t  bitmap_iterator bi;\n-\t  unsigned id;\n-\t  const char *comma = \"\";\n-\t  fprintf (dump_file, \"  defs to convert: \");\n-\t  EXECUTE_IF_SET_IN_BITMAP (defs_conv, 0, id, bi)\n-\t    {\n-\t      fprintf (dump_file, \"%sr%d\", comma, id);\n-\t      comma = \", \";\n-\t    }\n-\t  fprintf (dump_file, \"\\n\");\n-\t}\n-    }\n-\n-  BITMAP_FREE (queue);\n-}\n-\n-/* Return a cost of building a vector costant\n-   instead of using a scalar one.  */\n-\n-int\n-general_scalar_chain::vector_const_cost (rtx exp)\n-{\n-  gcc_assert (CONST_INT_P (exp));\n-\n-  if (standard_sse_constant_p (exp, vmode))\n-    return ix86_cost->sse_op;\n-  /* We have separate costs for SImode and DImode, use SImode costs\n-     for smaller modes.  */\n-  return ix86_cost->sse_load[smode == DImode ? 1 : 0];\n-}\n-\n-/* Compute a gain for chain conversion.  */\n-\n-int\n-general_scalar_chain::compute_convert_gain ()\n-{\n-  bitmap_iterator bi;\n-  unsigned insn_uid;\n-  int gain = 0;\n-  int cost = 0;\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"Computing gain for chain #%d...\\n\", chain_id);\n-\n-  /* SSE costs distinguish between SImode and DImode loads/stores, for\n-     int costs factor in the number of GPRs involved.  When supporting\n-     smaller modes than SImode the int load/store costs need to be\n-     adjusted as well.  */\n-  unsigned sse_cost_idx = smode == DImode ? 1 : 0;\n-  unsigned m = smode == DImode ? (TARGET_64BIT ? 1 : 2) : 1;\n-\n-  EXECUTE_IF_SET_IN_BITMAP (insns, 0, insn_uid, bi)\n-    {\n-      rtx_insn *insn = DF_INSN_UID_GET (insn_uid)->insn;\n-      rtx def_set = single_set (insn);\n-      rtx src = SET_SRC (def_set);\n-      rtx dst = SET_DEST (def_set);\n-      int igain = 0;\n-\n-      if (REG_P (src) && REG_P (dst))\n-\tigain += 2 * m - ix86_cost->xmm_move;\n-      else if (REG_P (src) && MEM_P (dst))\n-\tigain\n-\t  += m * ix86_cost->int_store[2] - ix86_cost->sse_store[sse_cost_idx];\n-      else if (MEM_P (src) && REG_P (dst))\n-\tigain += m * ix86_cost->int_load[2] - ix86_cost->sse_load[sse_cost_idx];\n-      else if (GET_CODE (src) == ASHIFT\n-\t       || GET_CODE (src) == ASHIFTRT\n-\t       || GET_CODE (src) == LSHIFTRT)\n-\t{\n-\t  if (m == 2)\n-\t    {\n-\t      if (INTVAL (XEXP (src, 1)) >= 32)\n-\t\tigain += ix86_cost->add;\n-\t      else\n-\t\tigain += ix86_cost->shift_const;\n-\t    }\n-\n-\t  igain += ix86_cost->shift_const - ix86_cost->sse_op;\n-\n-\t  if (CONST_INT_P (XEXP (src, 0)))\n-\t    igain -= vector_const_cost (XEXP (src, 0));\n-\t}\n-      else if (GET_CODE (src) == PLUS\n-\t       || GET_CODE (src) == MINUS\n-\t       || GET_CODE (src) == IOR\n-\t       || GET_CODE (src) == XOR\n-\t       || GET_CODE (src) == AND)\n-\t{\n-\t  igain += m * ix86_cost->add - ix86_cost->sse_op;\n-\t  /* Additional gain for andnot for targets without BMI.  */\n-\t  if (GET_CODE (XEXP (src, 0)) == NOT\n-\t      && !TARGET_BMI)\n-\t    igain += m * ix86_cost->add;\n-\n-\t  if (CONST_INT_P (XEXP (src, 0)))\n-\t    igain -= vector_const_cost (XEXP (src, 0));\n-\t  if (CONST_INT_P (XEXP (src, 1)))\n-\t    igain -= vector_const_cost (XEXP (src, 1));\n-\t}\n-      else if (GET_CODE (src) == NEG\n-\t       || GET_CODE (src) == NOT)\n-\tigain += m * ix86_cost->add - ix86_cost->sse_op - COSTS_N_INSNS (1);\n-      else if (GET_CODE (src) == SMAX\n-\t       || GET_CODE (src) == SMIN\n-\t       || GET_CODE (src) == UMAX\n-\t       || GET_CODE (src) == UMIN)\n-\t{\n-\t  /* We do not have any conditional move cost, estimate it as a\n-\t     reg-reg move.  Comparisons are costed as adds.  */\n-\t  igain += m * (COSTS_N_INSNS (2) + ix86_cost->add);\n-\t  /* Integer SSE ops are all costed the same.  */\n-\t  igain -= ix86_cost->sse_op;\n-\t}\n-      else if (GET_CODE (src) == COMPARE)\n-\t{\n-\t  /* Assume comparison cost is the same.  */\n-\t}\n-      else if (CONST_INT_P (src))\n-\t{\n-\t  if (REG_P (dst))\n-\t    /* DImode can be immediate for TARGET_64BIT and SImode always.  */\n-\t    igain += m * COSTS_N_INSNS (1);\n-\t  else if (MEM_P (dst))\n-\t    igain += (m * ix86_cost->int_store[2]\n-\t\t     - ix86_cost->sse_store[sse_cost_idx]);\n-\t  igain -= vector_const_cost (src);\n-\t}\n-      else\n-\tgcc_unreachable ();\n-\n-      if (igain != 0 && dump_file)\n-\t{\n-\t  fprintf (dump_file, \"  Instruction gain %d for \", igain);\n-\t  dump_insn_slim (dump_file, insn);\n-\t}\n-      gain += igain;\n-    }\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"  Instruction conversion gain: %d\\n\", gain);\n-\n-  /* Cost the integer to sse and sse to integer moves.  */\n-  cost += n_sse_to_integer * ix86_cost->sse_to_integer;\n-  /* ???  integer_to_sse but we only have that in the RA cost table.\n-     Assume sse_to_integer/integer_to_sse are the same which they\n-     are at the moment.  */\n-  cost += n_integer_to_sse * ix86_cost->sse_to_integer;\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"  Registers conversion cost: %d\\n\", cost);\n-\n-  gain -= cost;\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"  Total gain: %d\\n\", gain);\n-\n-  return gain;\n-}\n-\n-/* Insert generated conversion instruction sequence INSNS\n-   after instruction AFTER.  New BB may be required in case\n-   instruction has EH region attached.  */\n-\n-void\n-scalar_chain::emit_conversion_insns (rtx insns, rtx_insn *after)\n-{\n-  if (!control_flow_insn_p (after))\n-    {\n-      emit_insn_after (insns, after);\n-      return;\n-    }\n-\n-  basic_block bb = BLOCK_FOR_INSN (after);\n-  edge e = find_fallthru_edge (bb->succs);\n-  gcc_assert (e);\n-\n-  basic_block new_bb = split_edge (e);\n-  emit_insn_after (insns, BB_HEAD (new_bb));\n-}\n-\n-} // anon namespace\n-\n-/* Generate the canonical SET_SRC to move GPR to a VMODE vector register,\n-   zeroing the upper parts.  */\n-\n-static rtx\n-gen_gpr_to_xmm_move_src (enum machine_mode vmode, rtx gpr)\n-{\n-  switch (GET_MODE_NUNITS (vmode))\n-    {\n-    case 1:\n-      /* We are not using this case currently.  */\n-      gcc_unreachable ();\n-    case 2:\n-      return gen_rtx_VEC_CONCAT (vmode, gpr,\n-\t\t\t\t CONST0_RTX (GET_MODE_INNER (vmode)));\n-    default:\n-      return gen_rtx_VEC_MERGE (vmode, gen_rtx_VEC_DUPLICATE (vmode, gpr),\n-\t\t\t\tCONST0_RTX (vmode), GEN_INT (HOST_WIDE_INT_1U));\n-    }\n-}\n-\n-/* Make vector copies for all register REGNO definitions\n-   and replace its uses in a chain.  */\n-\n-void\n-general_scalar_chain::make_vector_copies (rtx_insn *insn, rtx reg)\n-{\n-  rtx vreg = *defs_map.get (reg);\n-\n-  start_sequence ();\n-  if (!TARGET_INTER_UNIT_MOVES_TO_VEC)\n-    {\n-      rtx tmp = assign_386_stack_local (smode, SLOT_STV_TEMP);\n-      if (smode == DImode && !TARGET_64BIT)\n-\t{\n-\t  emit_move_insn (adjust_address (tmp, SImode, 0),\n-\t\t\t  gen_rtx_SUBREG (SImode, reg, 0));\n-\t  emit_move_insn (adjust_address (tmp, SImode, 4),\n-\t\t\t  gen_rtx_SUBREG (SImode, reg, 4));\n-\t}\n-      else\n-\temit_move_insn (copy_rtx (tmp), reg);\n-      emit_insn (gen_rtx_SET (gen_rtx_SUBREG (vmode, vreg, 0),\n-\t\t\t      gen_gpr_to_xmm_move_src (vmode, tmp)));\n-    }\n-  else if (!TARGET_64BIT && smode == DImode)\n-    {\n-      if (TARGET_SSE4_1)\n-\t{\n-\t  emit_insn (gen_sse2_loadld (gen_rtx_SUBREG (V4SImode, vreg, 0),\n-\t\t\t\t      CONST0_RTX (V4SImode),\n-\t\t\t\t      gen_rtx_SUBREG (SImode, reg, 0)));\n-\t  emit_insn (gen_sse4_1_pinsrd (gen_rtx_SUBREG (V4SImode, vreg, 0),\n-\t\t\t\t\tgen_rtx_SUBREG (V4SImode, vreg, 0),\n-\t\t\t\t\tgen_rtx_SUBREG (SImode, reg, 4),\n-\t\t\t\t\tGEN_INT (2)));\n-\t}\n-      else\n-\t{\n-\t  rtx tmp = gen_reg_rtx (DImode);\n-\t  emit_insn (gen_sse2_loadld (gen_rtx_SUBREG (V4SImode, vreg, 0),\n-\t\t\t\t      CONST0_RTX (V4SImode),\n-\t\t\t\t      gen_rtx_SUBREG (SImode, reg, 0)));\n-\t  emit_insn (gen_sse2_loadld (gen_rtx_SUBREG (V4SImode, tmp, 0),\n-\t\t\t\t      CONST0_RTX (V4SImode),\n-\t\t\t\t      gen_rtx_SUBREG (SImode, reg, 4)));\n-\t  emit_insn (gen_vec_interleave_lowv4si\n-\t\t     (gen_rtx_SUBREG (V4SImode, vreg, 0),\n-\t\t      gen_rtx_SUBREG (V4SImode, vreg, 0),\n-\t\t      gen_rtx_SUBREG (V4SImode, tmp, 0)));\n-\t}\n-    }\n-  else\n-    emit_insn (gen_rtx_SET (gen_rtx_SUBREG (vmode, vreg, 0),\n-\t\t\t    gen_gpr_to_xmm_move_src (vmode, reg)));\n-  rtx_insn *seq = get_insns ();\n-  end_sequence ();\n-  emit_conversion_insns (seq, insn);\n-\n-  if (dump_file)\n-    fprintf (dump_file,\n-\t     \"  Copied r%d to a vector register r%d for insn %d\\n\",\n-\t     REGNO (reg), REGNO (vreg), INSN_UID (insn));\n-}\n-\n-/* Copy the definition SRC of INSN inside the chain to DST for\n-   scalar uses outside of the chain.  */\n-\n-void\n-general_scalar_chain::convert_reg (rtx_insn *insn, rtx dst, rtx src)\n-{\n-  start_sequence ();\n-  if (!TARGET_INTER_UNIT_MOVES_FROM_VEC)\n-    {\n-      rtx tmp = assign_386_stack_local (smode, SLOT_STV_TEMP);\n-      emit_move_insn (tmp, src);\n-      if (!TARGET_64BIT && smode == DImode)\n-\t{\n-\t  emit_move_insn (gen_rtx_SUBREG (SImode, dst, 0),\n-\t\t\t  adjust_address (tmp, SImode, 0));\n-\t  emit_move_insn (gen_rtx_SUBREG (SImode, dst, 4),\n-\t\t\t  adjust_address (tmp, SImode, 4));\n-\t}\n-      else\n-\temit_move_insn (dst, copy_rtx (tmp));\n-    }\n-  else if (!TARGET_64BIT && smode == DImode)\n-    {\n-      if (TARGET_SSE4_1)\n-\t{\n-\t  rtx tmp = gen_rtx_PARALLEL (VOIDmode,\n-\t\t\t\t      gen_rtvec (1, const0_rtx));\n-\t  emit_insn\n-\t      (gen_rtx_SET\n-\t       (gen_rtx_SUBREG (SImode, dst, 0),\n-\t\tgen_rtx_VEC_SELECT (SImode,\n-\t\t\t\t    gen_rtx_SUBREG (V4SImode, src, 0),\n-\t\t\t\t    tmp)));\n-\n-\t  tmp = gen_rtx_PARALLEL (VOIDmode, gen_rtvec (1, const1_rtx));\n-\t  emit_insn\n-\t      (gen_rtx_SET\n-\t       (gen_rtx_SUBREG (SImode, dst, 4),\n-\t\tgen_rtx_VEC_SELECT (SImode,\n-\t\t\t\t    gen_rtx_SUBREG (V4SImode, src, 0),\n-\t\t\t\t    tmp)));\n-\t}\n-      else\n-\t{\n-\t  rtx vcopy = gen_reg_rtx (V2DImode);\n-\t  emit_move_insn (vcopy, gen_rtx_SUBREG (V2DImode, src, 0));\n-\t  emit_move_insn (gen_rtx_SUBREG (SImode, dst, 0),\n-\t\t\t  gen_rtx_SUBREG (SImode, vcopy, 0));\n-\t  emit_move_insn (vcopy,\n-\t\t\t  gen_rtx_LSHIFTRT (V2DImode,\n-\t\t\t\t\t    vcopy, GEN_INT (32)));\n-\t  emit_move_insn (gen_rtx_SUBREG (SImode, dst, 4),\n-\t\t\t  gen_rtx_SUBREG (SImode, vcopy, 0));\n-\t}\n-    }\n-  else\n-    emit_move_insn (dst, src);\n-\n-  rtx_insn *seq = get_insns ();\n-  end_sequence ();\n-  emit_conversion_insns (seq, insn);\n-\n-  if (dump_file)\n-    fprintf (dump_file,\n-\t     \"  Copied r%d to a scalar register r%d for insn %d\\n\",\n-\t     REGNO (src), REGNO (dst), INSN_UID (insn));\n-}\n-\n-/* Convert operand OP in INSN.  We should handle\n-   memory operands and uninitialized registers.\n-   All other register uses are converted during\n-   registers conversion.  */\n-\n-void\n-general_scalar_chain::convert_op (rtx *op, rtx_insn *insn)\n-{\n-  *op = copy_rtx_if_shared (*op);\n-\n-  if (GET_CODE (*op) == NOT)\n-    {\n-      convert_op (&XEXP (*op, 0), insn);\n-      PUT_MODE (*op, vmode);\n-    }\n-  else if (MEM_P (*op))\n-    {\n-      rtx tmp = gen_reg_rtx (GET_MODE (*op));\n-\n-      /* Handle movabs.  */\n-      if (!memory_operand (*op, GET_MODE (*op)))\n-\t{\n-\t  rtx tmp2 = gen_reg_rtx (GET_MODE (*op));\n-\n-\t  emit_insn_before (gen_rtx_SET (tmp2, *op), insn);\n-\t  *op = tmp2;\n-\t}\n-\n-      emit_insn_before (gen_rtx_SET (gen_rtx_SUBREG (vmode, tmp, 0),\n-\t\t\t\t     gen_gpr_to_xmm_move_src (vmode, *op)),\n-\t\t\tinsn);\n-      *op = gen_rtx_SUBREG (vmode, tmp, 0);\n-\n-      if (dump_file)\n-\tfprintf (dump_file, \"  Preloading operand for insn %d into r%d\\n\",\n-\t\t INSN_UID (insn), REGNO (tmp));\n-    }\n-  else if (REG_P (*op))\n-    {\n-      *op = gen_rtx_SUBREG (vmode, *op, 0);\n-    }\n-  else if (CONST_INT_P (*op))\n-    {\n-      rtx vec_cst;\n-      rtx tmp = gen_rtx_SUBREG (vmode, gen_reg_rtx (smode), 0);\n-\n-      /* Prefer all ones vector in case of -1.  */\n-      if (constm1_operand (*op, GET_MODE (*op)))\n-\tvec_cst = CONSTM1_RTX (vmode);\n-      else\n-\t{\n-\t  unsigned n = GET_MODE_NUNITS (vmode);\n-\t  rtx *v = XALLOCAVEC (rtx, n);\n-\t  v[0] = *op;\n-\t  for (unsigned i = 1; i < n; ++i)\n-\t    v[i] = const0_rtx;\n-\t  vec_cst = gen_rtx_CONST_VECTOR (vmode, gen_rtvec_v (n, v));\n-\t}\n-\n-      if (!standard_sse_constant_p (vec_cst, vmode))\n-\t{\n-\t  start_sequence ();\n-\t  vec_cst = validize_mem (force_const_mem (vmode, vec_cst));\n-\t  rtx_insn *seq = get_insns ();\n-\t  end_sequence ();\n-\t  emit_insn_before (seq, insn);\n-\t}\n-\n-      emit_insn_before (gen_move_insn (copy_rtx (tmp), vec_cst), insn);\n-      *op = tmp;\n-    }\n-  else\n-    {\n-      gcc_assert (SUBREG_P (*op));\n-      gcc_assert (GET_MODE (*op) == vmode);\n-    }\n-}\n-\n-/* Convert INSN to vector mode.  */\n-\n-void\n-general_scalar_chain::convert_insn (rtx_insn *insn)\n-{\n-  /* Generate copies for out-of-chain uses of defs and adjust debug uses.  */\n-  for (df_ref ref = DF_INSN_DEFS (insn); ref; ref = DF_REF_NEXT_LOC (ref))\n-    if (bitmap_bit_p (defs_conv, DF_REF_REGNO (ref)))\n-      {\n-\tdf_link *use;\n-\tfor (use = DF_REF_CHAIN (ref); use; use = use->next)\n-\t  if (NONDEBUG_INSN_P (DF_REF_INSN (use->ref))\n-\t      && (DF_REF_REG_MEM_P (use->ref)\n-\t\t  || !bitmap_bit_p (insns, DF_REF_INSN_UID (use->ref))))\n-\t    break;\n-\tif (use)\n-\t  convert_reg (insn, DF_REF_REG (ref),\n-\t\t       *defs_map.get (regno_reg_rtx [DF_REF_REGNO (ref)]));\n-\telse if (MAY_HAVE_DEBUG_BIND_INSNS)\n-\t  {\n-\t    /* If we generated a scalar copy we can leave debug-insns\n-\t       as-is, if not, we have to adjust them.  */\n-\t    auto_vec<rtx_insn *, 5> to_reset_debug_insns;\n-\t    for (use = DF_REF_CHAIN (ref); use; use = use->next)\n-\t      if (DEBUG_INSN_P (DF_REF_INSN (use->ref)))\n-\t\t{\n-\t\t  rtx_insn *debug_insn = DF_REF_INSN (use->ref);\n-\t\t  /* If there's a reaching definition outside of the\n-\t\t     chain we have to reset.  */\n-\t\t  df_link *def;\n-\t\t  for (def = DF_REF_CHAIN (use->ref); def; def = def->next)\n-\t\t    if (!bitmap_bit_p (insns, DF_REF_INSN_UID (def->ref)))\n-\t\t      break;\n-\t\t  if (def)\n-\t\t    to_reset_debug_insns.safe_push (debug_insn);\n-\t\t  else\n-\t\t    {\n-\t\t      *DF_REF_REAL_LOC (use->ref)\n-\t\t\t= *defs_map.get (regno_reg_rtx [DF_REF_REGNO (ref)]);\n-\t\t      df_insn_rescan (debug_insn);\n-\t\t    }\n-\t\t}\n-\t    /* Have to do the reset outside of the DF_CHAIN walk to not\n-\t       disrupt it.  */\n-\t    while (!to_reset_debug_insns.is_empty ())\n-\t      {\n-\t\trtx_insn *debug_insn = to_reset_debug_insns.pop ();\n-\t\tINSN_VAR_LOCATION_LOC (debug_insn) = gen_rtx_UNKNOWN_VAR_LOC ();\n-\t\tdf_insn_rescan_debug_internal (debug_insn);\n-\t      }\n-\t  }\n-      }\n-\n-  /* Replace uses in this insn with the defs we use in the chain.  */\n-  for (df_ref ref = DF_INSN_USES (insn); ref; ref = DF_REF_NEXT_LOC (ref))\n-    if (!DF_REF_REG_MEM_P (ref))\n-      if (rtx *vreg = defs_map.get (regno_reg_rtx[DF_REF_REGNO (ref)]))\n-\t{\n-\t  /* Also update a corresponding REG_DEAD note.  */\n-\t  rtx note = find_reg_note (insn, REG_DEAD, DF_REF_REG (ref));\n-\t  if (note)\n-\t    XEXP (note, 0) = *vreg;\n-\t  *DF_REF_REAL_LOC (ref) = *vreg;\n-\t}\n-\n-  rtx def_set = single_set (insn);\n-  rtx src = SET_SRC (def_set);\n-  rtx dst = SET_DEST (def_set);\n-  rtx subreg;\n-\n-  if (MEM_P (dst) && !REG_P (src))\n-    {\n-      /* There are no scalar integer instructions and therefore\n-\t temporary register usage is required.  */\n-      rtx tmp = gen_reg_rtx (smode);\n-      emit_conversion_insns (gen_move_insn (dst, tmp), insn);\n-      dst = gen_rtx_SUBREG (vmode, tmp, 0);\n-    }\n-  else if (REG_P (dst))\n-    {\n-      /* Replace the definition with a SUBREG to the definition we\n-         use inside the chain.  */\n-      rtx *vdef = defs_map.get (dst);\n-      if (vdef)\n-\tdst = *vdef;\n-      dst = gen_rtx_SUBREG (vmode, dst, 0);\n-      /* IRA doesn't like to have REG_EQUAL/EQUIV notes when the SET_DEST\n-         is a non-REG_P.  So kill those off.  */\n-      rtx note = find_reg_equal_equiv_note (insn);\n-      if (note)\n-\tremove_note (insn, note);\n-    }\n-\n-  switch (GET_CODE (src))\n-    {\n-    case ASHIFT:\n-    case ASHIFTRT:\n-    case LSHIFTRT:\n-      convert_op (&XEXP (src, 0), insn);\n-      PUT_MODE (src, vmode);\n-      break;\n-\n-    case PLUS:\n-    case MINUS:\n-    case IOR:\n-    case XOR:\n-    case AND:\n-    case SMAX:\n-    case SMIN:\n-    case UMAX:\n-    case UMIN:\n-      convert_op (&XEXP (src, 0), insn);\n-      convert_op (&XEXP (src, 1), insn);\n-      PUT_MODE (src, vmode);\n-      break;\n-\n-    case NEG:\n-      src = XEXP (src, 0);\n-      convert_op (&src, insn);\n-      subreg = gen_reg_rtx (vmode);\n-      emit_insn_before (gen_move_insn (subreg, CONST0_RTX (vmode)), insn);\n-      src = gen_rtx_MINUS (vmode, subreg, src);\n-      break;\n-\n-    case NOT:\n-      src = XEXP (src, 0);\n-      convert_op (&src, insn);\n-      subreg = gen_reg_rtx (vmode);\n-      emit_insn_before (gen_move_insn (subreg, CONSTM1_RTX (vmode)), insn);\n-      src = gen_rtx_XOR (vmode, src, subreg);\n-      break;\n-\n-    case MEM:\n-      if (!REG_P (dst))\n-\tconvert_op (&src, insn);\n-      break;\n-\n-    case REG:\n-      if (!MEM_P (dst))\n-\tconvert_op (&src, insn);\n-      break;\n-\n-    case SUBREG:\n-      gcc_assert (GET_MODE (src) == vmode);\n-      break;\n-\n-    case COMPARE:\n-      src = SUBREG_REG (XEXP (XEXP (src, 0), 0));\n-\n-      gcc_assert (REG_P (src) && GET_MODE (src) == DImode);\n-      subreg = gen_rtx_SUBREG (V2DImode, src, 0);\n-      emit_insn_before (gen_vec_interleave_lowv2di (copy_rtx_if_shared (subreg),\n-\t\t\t\t\t\t    copy_rtx_if_shared (subreg),\n-\t\t\t\t\t\t    copy_rtx_if_shared (subreg)),\n-\t\t\tinsn);\n-      dst = gen_rtx_REG (CCmode, FLAGS_REG);\n-      src = gen_rtx_UNSPEC (CCmode, gen_rtvec (2, copy_rtx_if_shared (subreg),\n-\t\t\t\t\t       copy_rtx_if_shared (subreg)),\n-\t\t\t    UNSPEC_PTEST);\n-      break;\n-\n-    case CONST_INT:\n-      convert_op (&src, insn);\n-      break;\n-\n-    default:\n-      gcc_unreachable ();\n-    }\n-\n-  SET_SRC (def_set) = src;\n-  SET_DEST (def_set) = dst;\n-\n-  /* Drop possible dead definitions.  */\n-  PATTERN (insn) = def_set;\n-\n-  INSN_CODE (insn) = -1;\n-  int patt = recog_memoized (insn);\n-  if  (patt == -1)\n-    fatal_insn_not_found (insn);\n-  df_insn_rescan (insn);\n-}\n-\n-/* Fix uses of converted REG in debug insns.  */\n-\n-void\n-timode_scalar_chain::fix_debug_reg_uses (rtx reg)\n-{\n-  if (!flag_var_tracking)\n-    return;\n-\n-  df_ref ref, next;\n-  for (ref = DF_REG_USE_CHAIN (REGNO (reg)); ref; ref = next)\n-    {\n-      rtx_insn *insn = DF_REF_INSN (ref);\n-      /* Make sure the next ref is for a different instruction,\n-         so that we're not affected by the rescan.  */\n-      next = DF_REF_NEXT_REG (ref);\n-      while (next && DF_REF_INSN (next) == insn)\n-\tnext = DF_REF_NEXT_REG (next);\n-\n-      if (DEBUG_INSN_P (insn))\n-\t{\n-\t  /* It may be a debug insn with a TImode variable in\n-\t     register.  */\n-\t  bool changed = false;\n-\t  for (; ref != next; ref = DF_REF_NEXT_REG (ref))\n-\t    {\n-\t      rtx *loc = DF_REF_LOC (ref);\n-\t      if (REG_P (*loc) && GET_MODE (*loc) == V1TImode)\n-\t\t{\n-\t\t  *loc = gen_rtx_SUBREG (TImode, *loc, 0);\n-\t\t  changed = true;\n-\t\t}\n-\t    }\n-\t  if (changed)\n-\t    df_insn_rescan (insn);\n-\t}\n-    }\n-}\n-\n-/* Convert INSN from TImode to V1T1mode.  */\n-\n-void\n-timode_scalar_chain::convert_insn (rtx_insn *insn)\n-{\n-  rtx def_set = single_set (insn);\n-  rtx src = SET_SRC (def_set);\n-  rtx dst = SET_DEST (def_set);\n-\n-  switch (GET_CODE (dst))\n-    {\n-    case REG:\n-      {\n-\trtx tmp = find_reg_equal_equiv_note (insn);\n-\tif (tmp)\n-\t  PUT_MODE (XEXP (tmp, 0), V1TImode);\n-\tPUT_MODE (dst, V1TImode);\n-\tfix_debug_reg_uses (dst);\n-      }\n-      break;\n-    case MEM:\n-      PUT_MODE (dst, V1TImode);\n-      break;\n-\n-    default:\n-      gcc_unreachable ();\n-    }\n-\n-  switch (GET_CODE (src))\n-    {\n-    case REG:\n-      PUT_MODE (src, V1TImode);\n-      /* Call fix_debug_reg_uses only if SRC is never defined.  */\n-      if (!DF_REG_DEF_CHAIN (REGNO (src)))\n-\tfix_debug_reg_uses (src);\n-      break;\n-\n-    case MEM:\n-      PUT_MODE (src, V1TImode);\n-      break;\n-\n-    case CONST_WIDE_INT:\n-      if (NONDEBUG_INSN_P (insn))\n-\t{\n-\t  /* Since there are no instructions to store 128-bit constant,\n-\t     temporary register usage is required.  */\n-\t  rtx tmp = gen_reg_rtx (V1TImode);\n-\t  start_sequence ();\n-\t  src = gen_rtx_CONST_VECTOR (V1TImode, gen_rtvec (1, src));\n-\t  src = validize_mem (force_const_mem (V1TImode, src));\n-\t  rtx_insn *seq = get_insns ();\n-\t  end_sequence ();\n-\t  if (seq)\n-\t    emit_insn_before (seq, insn);\n-\t  emit_conversion_insns (gen_rtx_SET (dst, tmp), insn);\n-\t  dst = tmp;\n-\t}\n-      break;\n-\n-    case CONST_INT:\n-      switch (standard_sse_constant_p (src, TImode))\n-\t{\n-\tcase 1:\n-\t  src = CONST0_RTX (GET_MODE (dst));\n-\t  break;\n-\tcase 2:\n-\t  src = CONSTM1_RTX (GET_MODE (dst));\n-\t  break;\n-\tdefault:\n-\t  gcc_unreachable ();\n-\t}\n-      if (NONDEBUG_INSN_P (insn))\n-\t{\n-\t  rtx tmp = gen_reg_rtx (V1TImode);\n-\t  /* Since there are no instructions to store standard SSE\n-\t     constant, temporary register usage is required.  */\n-\t  emit_conversion_insns (gen_rtx_SET (dst, tmp), insn);\n-\t  dst = tmp;\n-\t}\n-      break;\n-\n-    default:\n-      gcc_unreachable ();\n-    }\n-\n-  SET_SRC (def_set) = src;\n-  SET_DEST (def_set) = dst;\n-\n-  /* Drop possible dead definitions.  */\n-  PATTERN (insn) = def_set;\n-\n-  INSN_CODE (insn) = -1;\n-  recog_memoized (insn);\n-  df_insn_rescan (insn);\n-}\n-\n-/* Generate copies from defs used by the chain but not defined therein.\n-   Also populates defs_map which is used later by convert_insn.  */\n-\n-void\n-general_scalar_chain::convert_registers ()\n-{\n-  bitmap_iterator bi;\n-  unsigned id;\n-  EXECUTE_IF_SET_IN_BITMAP (defs_conv, 0, id, bi)\n-    {\n-      rtx chain_reg = gen_reg_rtx (smode);\n-      defs_map.put (regno_reg_rtx[id], chain_reg);\n-    }\n-  EXECUTE_IF_SET_IN_BITMAP (insns_conv, 0, id, bi)\n-    for (df_ref ref = DF_INSN_UID_DEFS (id); ref; ref = DF_REF_NEXT_LOC (ref))\n-      if (bitmap_bit_p (defs_conv, DF_REF_REGNO (ref)))\n-\tmake_vector_copies (DF_REF_INSN (ref), DF_REF_REAL_REG (ref));\n-}\n-\n-/* Convert whole chain creating required register\n-   conversions and copies.  */\n-\n-int\n-scalar_chain::convert ()\n-{\n-  bitmap_iterator bi;\n-  unsigned id;\n-  int converted_insns = 0;\n-\n-  if (!dbg_cnt (stv_conversion))\n-    return 0;\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"Converting chain #%d...\\n\", chain_id);\n-\n-  convert_registers ();\n-\n-  EXECUTE_IF_SET_IN_BITMAP (insns, 0, id, bi)\n-    {\n-      convert_insn (DF_INSN_UID_GET (id)->insn);\n-      converted_insns++;\n-    }\n-\n-  return converted_insns;\n-}\n-\n-/* Return the SET expression if INSN doesn't reference hard register.\n-   Return NULL if INSN uses or defines a hard register, excluding\n-   pseudo register pushes, hard register uses in a memory address,\n-   clobbers and flags definitions.  */\n-\n-static rtx\n-pseudo_reg_set (rtx_insn *insn)\n-{\n-  rtx set = single_set (insn);\n-  if (!set)\n-    return NULL;\n-\n-  /* Check pseudo register push first. */\n-  if (REG_P (SET_SRC (set))\n-      && !HARD_REGISTER_P (SET_SRC (set))\n-      && push_operand (SET_DEST (set), GET_MODE (SET_DEST (set))))\n-    return set;\n-\n-  df_ref ref;\n-  FOR_EACH_INSN_DEF (ref, insn)\n-    if (HARD_REGISTER_P (DF_REF_REAL_REG (ref))\n-\t&& !DF_REF_FLAGS_IS_SET (ref, DF_REF_MUST_CLOBBER)\n-\t&& DF_REF_REGNO (ref) != FLAGS_REG)\n-      return NULL;\n-\n-  FOR_EACH_INSN_USE (ref, insn)\n-    if (!DF_REF_REG_MEM_P (ref) && HARD_REGISTER_P (DF_REF_REAL_REG (ref)))\n-      return NULL;\n-\n-  return set;\n-}\n-\n-/* Check if comparison INSN may be transformed\n-   into vector comparison.  Currently we transform\n-   zero checks only which look like:\n-\n-   (set (reg:CCZ 17 flags)\n-        (compare:CCZ (ior:SI (subreg:SI (reg:DI x) 4)\n-                             (subreg:SI (reg:DI x) 0))\n-\t\t     (const_int 0 [0])))  */\n-\n-static bool\n-convertible_comparison_p (rtx_insn *insn, enum machine_mode mode)\n-{\n-  /* ??? Currently convertible for double-word DImode chain only.  */\n-  if (TARGET_64BIT || mode != DImode)\n-    return false;\n-\n-  if (!TARGET_SSE4_1)\n-    return false;\n-\n-  rtx def_set = single_set (insn);\n-\n-  gcc_assert (def_set);\n-\n-  rtx src = SET_SRC (def_set);\n-  rtx dst = SET_DEST (def_set);\n-\n-  gcc_assert (GET_CODE (src) == COMPARE);\n-\n-  if (GET_CODE (dst) != REG\n-      || REGNO (dst) != FLAGS_REG\n-      || GET_MODE (dst) != CCZmode)\n-    return false;\n-\n-  rtx op1 = XEXP (src, 0);\n-  rtx op2 = XEXP (src, 1);\n-\n-  if (op2 != CONST0_RTX (GET_MODE (op2)))\n-    return false;\n-\n-  if (GET_CODE (op1) != IOR)\n-    return false;\n-\n-  op2 = XEXP (op1, 1);\n-  op1 = XEXP (op1, 0);\n-\n-  if (!SUBREG_P (op1)\n-      || !SUBREG_P (op2)\n-      || GET_MODE (op1) != SImode\n-      || GET_MODE (op2) != SImode\n-      || ((SUBREG_BYTE (op1) != 0\n-\t   || SUBREG_BYTE (op2) != GET_MODE_SIZE (SImode))\n-\t  && (SUBREG_BYTE (op2) != 0\n-\t      || SUBREG_BYTE (op1) != GET_MODE_SIZE (SImode))))\n-    return false;\n-\n-  op1 = SUBREG_REG (op1);\n-  op2 = SUBREG_REG (op2);\n-\n-  if (op1 != op2\n-      || !REG_P (op1)\n-      || GET_MODE (op1) != DImode)\n-    return false;\n-\n-  return true;\n-}\n-\n-/* The general version of scalar_to_vector_candidate_p.  */\n-\n-static bool\n-general_scalar_to_vector_candidate_p (rtx_insn *insn, enum machine_mode mode)\n-{\n-  rtx def_set = pseudo_reg_set (insn);\n-\n-  if (!def_set)\n-    return false;\n-\n-  rtx src = SET_SRC (def_set);\n-  rtx dst = SET_DEST (def_set);\n-\n-  if (GET_CODE (src) == COMPARE)\n-    return convertible_comparison_p (insn, mode);\n-\n-  /* We are interested in \"mode\" only.  */\n-  if ((GET_MODE (src) != mode\n-       && !CONST_INT_P (src))\n-      || GET_MODE (dst) != mode)\n-    return false;\n-\n-  if (!REG_P (dst) && !MEM_P (dst))\n-    return false;\n-\n-  switch (GET_CODE (src))\n-    {\n-    case ASHIFTRT:\n-      if (!TARGET_AVX512VL)\n-\treturn false;\n-      /* FALLTHRU */\n-\n-    case ASHIFT:\n-    case LSHIFTRT:\n-      if (!CONST_INT_P (XEXP (src, 1))\n-\t  || !IN_RANGE (INTVAL (XEXP (src, 1)), 0, GET_MODE_BITSIZE (mode)-1))\n-\treturn false;\n-      break;\n-\n-    case SMAX:\n-    case SMIN:\n-    case UMAX:\n-    case UMIN:\n-      if ((mode == DImode && !TARGET_AVX512VL)\n-\t  || (mode == SImode && !TARGET_SSE4_1))\n-\treturn false;\n-      /* Fallthru.  */\n-\n-    case PLUS:\n-    case MINUS:\n-    case IOR:\n-    case XOR:\n-    case AND:\n-      if (!REG_P (XEXP (src, 1))\n-\t  && !MEM_P (XEXP (src, 1))\n-\t  && !CONST_INT_P (XEXP (src, 1)))\n-\treturn false;\n-\n-      if (GET_MODE (XEXP (src, 1)) != mode\n-\t  && !CONST_INT_P (XEXP (src, 1)))\n-\treturn false;\n-      break;\n-\n-    case NEG:\n-    case NOT:\n-      break;\n-\n-    case REG:\n-      return true;\n-\n-    case MEM:\n-    case CONST_INT:\n-      return REG_P (dst);\n-\n-    default:\n-      return false;\n-    }\n-\n-  if (!REG_P (XEXP (src, 0))\n-      && !MEM_P (XEXP (src, 0))\n-      && !CONST_INT_P (XEXP (src, 0))\n-      /* Check for andnot case.  */\n-      && (GET_CODE (src) != AND\n-\t  || GET_CODE (XEXP (src, 0)) != NOT\n-\t  || !REG_P (XEXP (XEXP (src, 0), 0))))\n-      return false;\n-\n-  if (GET_MODE (XEXP (src, 0)) != mode\n-      && !CONST_INT_P (XEXP (src, 0)))\n-    return false;\n-\n-  return true;\n-}\n-\n-/* The TImode version of scalar_to_vector_candidate_p.  */\n-\n-static bool\n-timode_scalar_to_vector_candidate_p (rtx_insn *insn)\n-{\n-  rtx def_set = pseudo_reg_set (insn);\n-\n-  if (!def_set)\n-    return false;\n-\n-  rtx src = SET_SRC (def_set);\n-  rtx dst = SET_DEST (def_set);\n-\n-  /* Only TImode load and store are allowed.  */\n-  if (GET_MODE (dst) != TImode)\n-    return false;\n-\n-  if (MEM_P (dst))\n-    {\n-      /* Check for store.  Memory must be aligned or unaligned store\n-\t is optimal.  Only support store from register, standard SSE\n-\t constant or CONST_WIDE_INT generated from piecewise store.\n-\n-\t ??? Verify performance impact before enabling CONST_INT for\n-\t __int128 store.  */\n-      if (misaligned_operand (dst, TImode)\n-\t  && !TARGET_SSE_UNALIGNED_STORE_OPTIMAL)\n-\treturn false;\n-\n-      switch (GET_CODE (src))\n-\t{\n-\tdefault:\n-\t  return false;\n-\n-\tcase REG:\n-\tcase CONST_WIDE_INT:\n-\t  return true;\n-\n-\tcase CONST_INT:\n-\t  return standard_sse_constant_p (src, TImode);\n-\t}\n-    }\n-  else if (MEM_P (src))\n-    {\n-      /* Check for load.  Memory must be aligned or unaligned load is\n-\t optimal.  */\n-      return (REG_P (dst)\n-\t      && (!misaligned_operand (src, TImode)\n-\t\t  || TARGET_SSE_UNALIGNED_LOAD_OPTIMAL));\n-    }\n-\n-  return false;\n-}\n-\n-/* For a register REGNO, scan instructions for its defs and uses.\n-   Put REGNO in REGS if a def or use isn't in CANDIDATES.  */\n-\n-static void\n-timode_check_non_convertible_regs (bitmap candidates, bitmap regs,\n-\t\t\t\t   unsigned int regno)\n-{\n-  for (df_ref def = DF_REG_DEF_CHAIN (regno);\n-       def;\n-       def = DF_REF_NEXT_REG (def))\n-    {\n-      if (!bitmap_bit_p (candidates, DF_REF_INSN_UID (def)))\n-\t{\n-\t  if (dump_file)\n-\t    fprintf (dump_file,\n-\t\t     \"r%d has non convertible def in insn %d\\n\",\n-\t\t     regno, DF_REF_INSN_UID (def));\n-\n-\t  bitmap_set_bit (regs, regno);\n-\t  break;\n-\t}\n-    }\n-\n-  for (df_ref ref = DF_REG_USE_CHAIN (regno);\n-       ref;\n-       ref = DF_REF_NEXT_REG (ref))\n-    {\n-      /* Debug instructions are skipped.  */\n-      if (NONDEBUG_INSN_P (DF_REF_INSN (ref))\n-\t  && !bitmap_bit_p (candidates, DF_REF_INSN_UID (ref)))\n-\t{\n-\t  if (dump_file)\n-\t    fprintf (dump_file,\n-\t\t     \"r%d has non convertible use in insn %d\\n\",\n-\t\t     regno, DF_REF_INSN_UID (ref));\n-\n-\t  bitmap_set_bit (regs, regno);\n-\t  break;\n-\t}\n-    }\n-}\n-\n-/* The TImode version of remove_non_convertible_regs.  */\n-\n-static void\n-timode_remove_non_convertible_regs (bitmap candidates)\n-{\n-  bitmap_iterator bi;\n-  unsigned id;\n-  bitmap regs = BITMAP_ALLOC (NULL);\n-\n-  EXECUTE_IF_SET_IN_BITMAP (candidates, 0, id, bi)\n-    {\n-      rtx def_set = single_set (DF_INSN_UID_GET (id)->insn);\n-      rtx dest = SET_DEST (def_set);\n-      rtx src = SET_SRC (def_set);\n-\n-      if ((!REG_P (dest)\n-\t   || bitmap_bit_p (regs, REGNO (dest))\n-\t   || HARD_REGISTER_P (dest))\n-\t  && (!REG_P (src)\n-\t      || bitmap_bit_p (regs, REGNO (src))\n-\t      || HARD_REGISTER_P (src)))\n-\tcontinue;\n-\n-      if (REG_P (dest))\n-\ttimode_check_non_convertible_regs (candidates, regs,\n-\t\t\t\t\t   REGNO (dest));\n-\n-      if (REG_P (src))\n-\ttimode_check_non_convertible_regs (candidates, regs,\n-\t\t\t\t\t   REGNO (src));\n-    }\n-\n-  EXECUTE_IF_SET_IN_BITMAP (regs, 0, id, bi)\n-    {\n-      for (df_ref def = DF_REG_DEF_CHAIN (id);\n-\t   def;\n-\t   def = DF_REF_NEXT_REG (def))\n-\tif (bitmap_bit_p (candidates, DF_REF_INSN_UID (def)))\n-\t  {\n-\t    if (dump_file)\n-\t      fprintf (dump_file, \"Removing insn %d from candidates list\\n\",\n-\t\t       DF_REF_INSN_UID (def));\n-\n-\t    bitmap_clear_bit (candidates, DF_REF_INSN_UID (def));\n-\t  }\n-\n-      for (df_ref ref = DF_REG_USE_CHAIN (id);\n-\t   ref;\n-\t   ref = DF_REF_NEXT_REG (ref))\n-\tif (bitmap_bit_p (candidates, DF_REF_INSN_UID (ref)))\n-\t  {\n-\t    if (dump_file)\n-\t      fprintf (dump_file, \"Removing insn %d from candidates list\\n\",\n-\t\t       DF_REF_INSN_UID (ref));\n-\n-\t    bitmap_clear_bit (candidates, DF_REF_INSN_UID (ref));\n-\t  }\n-    }\n-\n-  BITMAP_FREE (regs);\n-}\n-\n-/* Main STV pass function.  Find and convert scalar\n-   instructions into vector mode when profitable.  */\n-\n-static unsigned int\n-convert_scalars_to_vector (bool timode_p)\n-{\n-  basic_block bb;\n-  int converted_insns = 0;\n-\n-  bitmap_obstack_initialize (NULL);\n-  const machine_mode cand_mode[3] = { SImode, DImode, TImode };\n-  const machine_mode cand_vmode[3] = { V4SImode, V2DImode, V1TImode };\n-  bitmap_head candidates[3];  /* { SImode, DImode, TImode } */\n-  for (unsigned i = 0; i < 3; ++i)\n-    bitmap_initialize (&candidates[i], &bitmap_default_obstack);\n-\n-  calculate_dominance_info (CDI_DOMINATORS);\n-  df_set_flags (DF_DEFER_INSN_RESCAN);\n-  df_chain_add_problem (DF_DU_CHAIN | DF_UD_CHAIN);\n-  df_analyze ();\n-\n-  /* Find all instructions we want to convert into vector mode.  */\n-  if (dump_file)\n-    fprintf (dump_file, \"Searching for mode conversion candidates...\\n\");\n-\n-  FOR_EACH_BB_FN (bb, cfun)\n-    {\n-      rtx_insn *insn;\n-      FOR_BB_INSNS (bb, insn)\n-\tif (timode_p\n-\t    && timode_scalar_to_vector_candidate_p (insn))\n-\t  {\n-\t    if (dump_file)\n-\t      fprintf (dump_file, \"  insn %d is marked as a TImode candidate\\n\",\n-\t\t       INSN_UID (insn));\n-\n-\t    bitmap_set_bit (&candidates[2], INSN_UID (insn));\n-\t  }\n-\telse if (!timode_p)\n-\t  {\n-\t    /* Check {SI,DI}mode.  */\n-\t    for (unsigned i = 0; i <= 1; ++i)\n-\t      if (general_scalar_to_vector_candidate_p (insn, cand_mode[i]))\n-\t\t{\n-\t\t  if (dump_file)\n-\t\t    fprintf (dump_file, \"  insn %d is marked as a %s candidate\\n\",\n-\t\t\t     INSN_UID (insn), i == 0 ? \"SImode\" : \"DImode\");\n-\n-\t\t  bitmap_set_bit (&candidates[i], INSN_UID (insn));\n-\t\t  break;\n-\t\t}\n-\t  }\n-    }\n-\n-  if (timode_p)\n-    timode_remove_non_convertible_regs (&candidates[2]);\n-\n-  for (unsigned i = 0; i <= 2; ++i)\n-    if (!bitmap_empty_p (&candidates[i]))\n-      break;\n-    else if (i == 2 && dump_file)\n-      fprintf (dump_file, \"There are no candidates for optimization.\\n\");\n-\n-  for (unsigned i = 0; i <= 2; ++i)\n-    while (!bitmap_empty_p (&candidates[i]))\n-      {\n-\tunsigned uid = bitmap_first_set_bit (&candidates[i]);\n-\tscalar_chain *chain;\n-\n-\tif (cand_mode[i] == TImode)\n-\t  chain = new timode_scalar_chain;\n-\telse\n-\t  chain = new general_scalar_chain (cand_mode[i], cand_vmode[i]);\n-\n-\t/* Find instructions chain we want to convert to vector mode.\n-\t   Check all uses and definitions to estimate all required\n-\t   conversions.  */\n-\tchain->build (&candidates[i], uid);\n-\n-\tif (chain->compute_convert_gain () > 0)\n-\t  converted_insns += chain->convert ();\n-\telse\n-\t  if (dump_file)\n-\t    fprintf (dump_file, \"Chain #%d conversion is not profitable\\n\",\n-\t\t     chain->chain_id);\n-\n-\tdelete chain;\n-      }\n-\n-  if (dump_file)\n-    fprintf (dump_file, \"Total insns converted: %d\\n\", converted_insns);\n-\n-  for (unsigned i = 0; i <= 2; ++i)\n-    bitmap_release (&candidates[i]);\n-  bitmap_obstack_release (NULL);\n-  df_process_deferred_rescans ();\n-\n-  /* Conversion means we may have 128bit register spills/fills\n-     which require aligned stack.  */\n-  if (converted_insns)\n-    {\n-      if (crtl->stack_alignment_needed < 128)\n-\tcrtl->stack_alignment_needed = 128;\n-      if (crtl->stack_alignment_estimated < 128)\n-\tcrtl->stack_alignment_estimated = 128;\n-\n-      crtl->stack_realign_needed\n-\t= INCOMING_STACK_BOUNDARY < crtl->stack_alignment_estimated;\n-      crtl->stack_realign_tried = crtl->stack_realign_needed;\n-\n-      crtl->stack_realign_processed = true;\n-\n-      if (!crtl->drap_reg)\n-\t{\n-\t  rtx drap_rtx = targetm.calls.get_drap_rtx ();\n-\n-\t  /* stack_realign_drap and drap_rtx must match.  */\n-\t  gcc_assert ((stack_realign_drap != 0) == (drap_rtx != NULL));\n-\n-\t  /* Do nothing if NULL is returned,\n-\t     which means DRAP is not needed.  */\n-\t  if (drap_rtx != NULL)\n-\t    {\n-\t      crtl->args.internal_arg_pointer = drap_rtx;\n-\n-\t      /* Call fixup_tail_calls to clean up\n-\t\t REG_EQUIV note if DRAP is needed. */\n-\t      fixup_tail_calls ();\n-\t    }\n-\t}\n-\n-      /* Fix up DECL_RTL/DECL_INCOMING_RTL of arguments.  */\n-      if (TARGET_64BIT)\n-\tfor (tree parm = DECL_ARGUMENTS (current_function_decl);\n-\t     parm; parm = DECL_CHAIN (parm))\n-\t  {\n-\t    if (TYPE_MODE (TREE_TYPE (parm)) != TImode)\n-\t      continue;\n-\t    if (DECL_RTL_SET_P (parm)\n-\t\t&& GET_MODE (DECL_RTL (parm)) == V1TImode)\n-\t      {\n-\t\trtx r = DECL_RTL (parm);\n-\t\tif (REG_P (r))\n-\t\t  SET_DECL_RTL (parm, gen_rtx_SUBREG (TImode, r, 0));\n-\t      }\n-\t    if (DECL_INCOMING_RTL (parm)\n-\t\t&& GET_MODE (DECL_INCOMING_RTL (parm)) == V1TImode)\n-\t      {\n-\t\trtx r = DECL_INCOMING_RTL (parm);\n-\t\tif (REG_P (r))\n-\t\t  DECL_INCOMING_RTL (parm) = gen_rtx_SUBREG (TImode, r, 0);\n-\t      }\n-\t  }\n-    }\n-\n-  return 0;\n-}\n-\n-/* Modify the vzeroupper pattern in INSN so that it describes the effect\n-   that the instruction has on the SSE registers.  LIVE_REGS are the set\n-   of registers that are live across the instruction.\n-\n-   For a live register R we use:\n-\n-     (set (reg:V2DF R) (reg:V2DF R))\n-\n-   which preserves the low 128 bits but clobbers the upper bits.  */\n-\n-static void\n-ix86_add_reg_usage_to_vzeroupper (rtx_insn *insn, bitmap live_regs)\n-{\n-  rtx pattern = PATTERN (insn);\n-  unsigned int nregs = TARGET_64BIT ? 16 : 8;\n-  unsigned int npats = nregs;\n-  for (unsigned int i = 0; i < nregs; ++i)\n-    {\n-      unsigned int regno = GET_SSE_REGNO (i);\n-      if (!bitmap_bit_p (live_regs, regno))\n-\tnpats--;\n-    }\n-  if (npats == 0)\n-    return;\n-  rtvec vec = rtvec_alloc (npats + 1);\n-  RTVEC_ELT (vec, 0) = XVECEXP (pattern, 0, 0);\n-  for (unsigned int i = 0, j = 0; i < nregs; ++i)\n-    {\n-      unsigned int regno = GET_SSE_REGNO (i);\n-      if (!bitmap_bit_p (live_regs, regno))\n-\tcontinue;\n-      rtx reg = gen_rtx_REG (V2DImode, regno);\n-      ++j;\n-      RTVEC_ELT (vec, j) = gen_rtx_SET (reg, reg);\n-    }\n-  XVEC (pattern, 0) = vec;\n-  INSN_CODE (insn) = -1;\n-  df_insn_rescan (insn);\n-}\n-\n-/* Walk the vzeroupper instructions in the function and annotate them\n-   with the effect that they have on the SSE registers.  */\n-\n-static void\n-ix86_add_reg_usage_to_vzerouppers (void)\n-{\n-  basic_block bb;\n-  rtx_insn *insn;\n-  auto_bitmap live_regs;\n-\n-  df_analyze ();\n-  FOR_EACH_BB_FN (bb, cfun)\n-    {\n-      bitmap_copy (live_regs, df_get_live_out (bb));\n-      df_simulate_initialize_backwards (bb, live_regs);\n-      FOR_BB_INSNS_REVERSE (bb, insn)\n-\t{\n-\t  if (!NONDEBUG_INSN_P (insn))\n-\t    continue;\n-\t  if (vzeroupper_pattern (PATTERN (insn), VOIDmode))\n-\t    ix86_add_reg_usage_to_vzeroupper (insn, live_regs);\n-\t  df_simulate_one_insn_backwards (bb, insn, live_regs);\n-\t}\n-    }\n-}\n-\n-static unsigned int\n-rest_of_handle_insert_vzeroupper (void)\n-{\n-  int i;\n-\n-  /* vzeroupper instructions are inserted immediately after reload to\n-     account for possible spills from 256bit or 512bit registers.  The pass\n-     reuses mode switching infrastructure by re-running mode insertion\n-     pass, so disable entities that have already been processed.  */\n-  for (i = 0; i < MAX_386_ENTITIES; i++)\n-    ix86_optimize_mode_switching[i] = 0;\n-\n-  ix86_optimize_mode_switching[AVX_U128] = 1;\n-\n-  /* Call optimize_mode_switching.  */\n-  g->get_passes ()->execute_pass_mode_switching ();\n-  ix86_add_reg_usage_to_vzerouppers ();\n-  return 0;\n-}\n-\n-namespace {\n-\n-const pass_data pass_data_insert_vzeroupper =\n-{\n-  RTL_PASS, /* type */\n-  \"vzeroupper\", /* name */\n-  OPTGROUP_NONE, /* optinfo_flags */\n-  TV_MACH_DEP, /* tv_id */\n-  0, /* properties_required */\n-  0, /* properties_provided */\n-  0, /* properties_destroyed */\n-  0, /* todo_flags_start */\n-  TODO_df_finish, /* todo_flags_finish */\n-};\n-\n-class pass_insert_vzeroupper : public rtl_opt_pass\n-{\n-public:\n-  pass_insert_vzeroupper(gcc::context *ctxt)\n-    : rtl_opt_pass(pass_data_insert_vzeroupper, ctxt)\n-  {}\n-\n-  /* opt_pass methods: */\n-  virtual bool gate (function *)\n-    {\n-      return TARGET_AVX\n-\t     && TARGET_VZEROUPPER && flag_expensive_optimizations\n-\t     && !optimize_size;\n-    }\n-\n-  virtual unsigned int execute (function *)\n-    {\n-      return rest_of_handle_insert_vzeroupper ();\n-    }\n-\n-}; // class pass_insert_vzeroupper\n-\n-const pass_data pass_data_stv =\n-{\n-  RTL_PASS, /* type */\n-  \"stv\", /* name */\n-  OPTGROUP_NONE, /* optinfo_flags */\n-  TV_MACH_DEP, /* tv_id */\n-  0, /* properties_required */\n-  0, /* properties_provided */\n-  0, /* properties_destroyed */\n-  0, /* todo_flags_start */\n-  TODO_df_finish, /* todo_flags_finish */\n-};\n-\n-class pass_stv : public rtl_opt_pass\n-{\n-public:\n-  pass_stv (gcc::context *ctxt)\n-    : rtl_opt_pass (pass_data_stv, ctxt),\n-      timode_p (false)\n-  {}\n-\n-  /* opt_pass methods: */\n-  virtual bool gate (function *)\n-    {\n-      return ((!timode_p || TARGET_64BIT)\n-\t      && TARGET_STV && TARGET_SSE2 && optimize > 1);\n-    }\n-\n-  virtual unsigned int execute (function *)\n-    {\n-      return convert_scalars_to_vector (timode_p);\n-    }\n-\n-  opt_pass *clone ()\n-    {\n-      return new pass_stv (m_ctxt);\n-    }\n-\n-  void set_pass_param (unsigned int n, bool param)\n-    {\n-      gcc_assert (n == 0);\n-      timode_p = param;\n-    }\n-\n-private:\n-  bool timode_p;\n-}; // class pass_stv\n-\n-} // anon namespace\n-\n-rtl_opt_pass *\n-make_pass_insert_vzeroupper (gcc::context *ctxt)\n-{\n-  return new pass_insert_vzeroupper (ctxt);\n-}\n-\n-rtl_opt_pass *\n-make_pass_stv (gcc::context *ctxt)\n-{\n-  return new pass_stv (ctxt);\n-}\n-\n-/* Inserting ENDBRANCH instructions.  */\n-\n-static unsigned int\n-rest_of_insert_endbranch (void)\n-{\n-  timevar_push (TV_MACH_DEP);\n-\n-  rtx cet_eb;\n-  rtx_insn *insn;\n-  basic_block bb;\n-\n-  /* Currently emit EB if it's a tracking function, i.e. 'nocf_check' is\n-     absent among function attributes.  Later an optimization will be\n-     introduced to make analysis if an address of a static function is\n-     taken.  A static function whose address is not taken will get a\n-     nocf_check attribute.  This will allow to reduce the number of EB.  */\n-\n-  if (!lookup_attribute (\"nocf_check\",\n-\t\t\t TYPE_ATTRIBUTES (TREE_TYPE (cfun->decl)))\n-      && (!flag_manual_endbr\n-\t  || lookup_attribute (\"cf_check\",\n-\t\t\t       DECL_ATTRIBUTES (cfun->decl)))\n-      && (!cgraph_node::get (cfun->decl)->only_called_directly_p ()\n-\t  || ix86_cmodel == CM_LARGE\n-\t  || ix86_cmodel == CM_LARGE_PIC\n-\t  || flag_force_indirect_call\n-\t  || (TARGET_DLLIMPORT_DECL_ATTRIBUTES\n-\t      && DECL_DLLIMPORT_P (cfun->decl))))\n-    {\n-      /* Queue ENDBR insertion to x86_function_profiler.  */\n-      if (crtl->profile && flag_fentry)\n-\tcfun->machine->endbr_queued_at_entrance = true;\n-      else\n-\t{\n-\t  cet_eb = gen_nop_endbr ();\n-\n-\t  bb = ENTRY_BLOCK_PTR_FOR_FN (cfun)->next_bb;\n-\t  insn = BB_HEAD (bb);\n-\t  emit_insn_before (cet_eb, insn);\n-\t}\n-    }\n-\n-  bb = 0;\n-  FOR_EACH_BB_FN (bb, cfun)\n-    {\n-      for (insn = BB_HEAD (bb); insn != NEXT_INSN (BB_END (bb));\n-\t   insn = NEXT_INSN (insn))\n-\t{\n-\t  if (CALL_P (insn))\n-\t    {\n-\t      bool need_endbr;\n-\t      need_endbr = find_reg_note (insn, REG_SETJMP, NULL) != NULL;\n-\t      if (!need_endbr && !SIBLING_CALL_P (insn))\n-\t\t{\n-\t\t  rtx call = get_call_rtx_from (insn);\n-\t\t  rtx fnaddr = XEXP (call, 0);\n-\t\t  tree fndecl = NULL_TREE;\n-\n-\t\t  /* Also generate ENDBRANCH for non-tail call which\n-\t\t     may return via indirect branch.  */\n-\t\t  if (GET_CODE (XEXP (fnaddr, 0)) == SYMBOL_REF)\n-\t\t    fndecl = SYMBOL_REF_DECL (XEXP (fnaddr, 0));\n-\t\t  if (fndecl == NULL_TREE)\n-\t\t    fndecl = MEM_EXPR (fnaddr);\n-\t\t  if (fndecl\n-\t\t      && TREE_CODE (TREE_TYPE (fndecl)) != FUNCTION_TYPE\n-\t\t      && TREE_CODE (TREE_TYPE (fndecl)) != METHOD_TYPE)\n-\t\t    fndecl = NULL_TREE;\n-\t\t  if (fndecl && TYPE_ARG_TYPES (TREE_TYPE (fndecl)))\n-\t\t    {\n-\t\t      tree fntype = TREE_TYPE (fndecl);\n-\t\t      if (lookup_attribute (\"indirect_return\",\n-\t\t\t\t\t    TYPE_ATTRIBUTES (fntype)))\n-\t\t\tneed_endbr = true;\n-\t\t    }\n-\t\t}\n-\t      if (!need_endbr)\n-\t\tcontinue;\n-\t      /* Generate ENDBRANCH after CALL, which can return more than\n-\t\t twice, setjmp-like functions.  */\n-\n-\t      cet_eb = gen_nop_endbr ();\n-\t      emit_insn_after_setloc (cet_eb, insn, INSN_LOCATION (insn));\n-\t      continue;\n-\t    }\n-\n-\t  if (JUMP_P (insn) && flag_cet_switch)\n-\t    {\n-\t      rtx target = JUMP_LABEL (insn);\n-\t      if (target == NULL_RTX || ANY_RETURN_P (target))\n-\t\tcontinue;\n-\n-\t      /* Check the jump is a switch table.  */\n-\t      rtx_insn *label = as_a<rtx_insn *> (target);\n-\t      rtx_insn *table = next_insn (label);\n-\t      if (table == NULL_RTX || !JUMP_TABLE_DATA_P (table))\n-\t\tcontinue;\n-\n-\t      /* For the indirect jump find out all places it jumps and insert\n-\t\t ENDBRANCH there.  It should be done under a special flag to\n-\t\t control ENDBRANCH generation for switch stmts.  */\n-\t      edge_iterator ei;\n-\t      edge e;\n-\t      basic_block dest_blk;\n-\n-\t      FOR_EACH_EDGE (e, ei, bb->succs)\n-\t\t{\n-\t\t  rtx_insn *insn;\n-\n-\t\t  dest_blk = e->dest;\n-\t\t  insn = BB_HEAD (dest_blk);\n-\t\t  gcc_assert (LABEL_P (insn));\n-\t\t  cet_eb = gen_nop_endbr ();\n-\t\t  emit_insn_after (cet_eb, insn);\n-\t\t}\n-\t      continue;\n-\t    }\n-\n-\t  if (LABEL_P (insn) && LABEL_PRESERVE_P (insn))\n-\t    {\n-\t      cet_eb = gen_nop_endbr ();\n-\t      emit_insn_after (cet_eb, insn);\n-\t      continue;\n-\t    }\n-\t}\n-    }\n-\n-  timevar_pop (TV_MACH_DEP);\n-  return 0;\n-}\n-\n-namespace {\n-\n-const pass_data pass_data_insert_endbranch =\n-{\n-  RTL_PASS, /* type.  */\n-  \"cet\", /* name.  */\n-  OPTGROUP_NONE, /* optinfo_flags.  */\n-  TV_MACH_DEP, /* tv_id.  */\n-  0, /* properties_required.  */\n-  0, /* properties_provided.  */\n-  0, /* properties_destroyed.  */\n-  0, /* todo_flags_start.  */\n-  0, /* todo_flags_finish.  */\n-};\n-\n-class pass_insert_endbranch : public rtl_opt_pass\n-{\n-public:\n-  pass_insert_endbranch (gcc::context *ctxt)\n-    : rtl_opt_pass (pass_data_insert_endbranch, ctxt)\n-  {}\n-\n-  /* opt_pass methods: */\n-  virtual bool gate (function *)\n-    {\n-      return ((flag_cf_protection & CF_BRANCH));\n-    }\n-\n-  virtual unsigned int execute (function *)\n-    {\n-      return rest_of_insert_endbranch ();\n-    }\n-\n-}; // class pass_insert_endbranch\n-\n-} // anon namespace\n-\n-rtl_opt_pass *\n-make_pass_insert_endbranch (gcc::context *ctxt)\n-{\n-  return new pass_insert_endbranch (ctxt);\n-}\n-\n-/* At entry of the nearest common dominator for basic blocks with\n-   conversions, generate a single\n-\tvxorps %xmmN, %xmmN, %xmmN\n-   for all\n-\tvcvtss2sd  op, %xmmN, %xmmX\n-\tvcvtsd2ss  op, %xmmN, %xmmX\n-\tvcvtsi2ss  op, %xmmN, %xmmX\n-\tvcvtsi2sd  op, %xmmN, %xmmX\n-\n-   NB: We want to generate only a single vxorps to cover the whole\n-   function.  The LCM algorithm isn't appropriate here since it may\n-   place a vxorps inside the loop.  */\n-\n-static unsigned int\n-remove_partial_avx_dependency (void)\n-{\n-  timevar_push (TV_MACH_DEP);\n-\n-  bitmap_obstack_initialize (NULL);\n-  bitmap convert_bbs = BITMAP_ALLOC (NULL);\n-\n-  basic_block bb;\n-  rtx_insn *insn, *set_insn;\n-  rtx set;\n-  rtx v4sf_const0 = NULL_RTX;\n-\n-  auto_vec<rtx_insn *> control_flow_insns;\n-\n-  FOR_EACH_BB_FN (bb, cfun)\n-    {\n-      FOR_BB_INSNS (bb, insn)\n-\t{\n-\t  if (!NONDEBUG_INSN_P (insn))\n-\t    continue;\n-\n-\t  set = single_set (insn);\n-\t  if (!set)\n-\t    continue;\n-\n-\t  if (get_attr_avx_partial_xmm_update (insn)\n-\t      != AVX_PARTIAL_XMM_UPDATE_TRUE)\n-\t    continue;\n-\n-\t  if (!v4sf_const0)\n-\t    {\n-\t      calculate_dominance_info (CDI_DOMINATORS);\n-\t      df_set_flags (DF_DEFER_INSN_RESCAN);\n-\t      df_chain_add_problem (DF_DU_CHAIN | DF_UD_CHAIN);\n-\t      df_md_add_problem ();\n-\t      df_analyze ();\n-\t      v4sf_const0 = gen_reg_rtx (V4SFmode);\n-\t    }\n-\n-\t  /* Convert PARTIAL_XMM_UPDATE_TRUE insns, DF -> SF, SF -> DF,\n-\t     SI -> SF, SI -> DF, DI -> SF, DI -> DF, to vec_dup and\n-\t     vec_merge with subreg.  */\n-\t  rtx src = SET_SRC (set);\n-\t  rtx dest = SET_DEST (set);\n-\t  machine_mode dest_mode = GET_MODE (dest);\n-\n-\t  rtx zero;\n-\t  machine_mode dest_vecmode;\n-\t  if (dest_mode == E_SFmode)\n-\t    {\n-\t      dest_vecmode = V4SFmode;\n-\t      zero = v4sf_const0;\n-\t    }\n-\t  else\n-\t    {\n-\t      dest_vecmode = V2DFmode;\n-\t      zero = gen_rtx_SUBREG (V2DFmode, v4sf_const0, 0);\n-\t    }\n-\n-\t  /* Change source to vector mode.  */\n-\t  src = gen_rtx_VEC_DUPLICATE (dest_vecmode, src);\n-\t  src = gen_rtx_VEC_MERGE (dest_vecmode, src, zero,\n-\t\t\t\t   GEN_INT (HOST_WIDE_INT_1U));\n-\t  /* Change destination to vector mode.  */\n-\t  rtx vec = gen_reg_rtx (dest_vecmode);\n-\t  /* Generate an XMM vector SET.  */\n-\t  set = gen_rtx_SET (vec, src);\n-\t  set_insn = emit_insn_before (set, insn);\n-\t  df_insn_rescan (set_insn);\n-\n-\t  if (cfun->can_throw_non_call_exceptions)\n-\t    {\n-\t      /* Handle REG_EH_REGION note.  */\n-\t      rtx note = find_reg_note (insn, REG_EH_REGION, NULL_RTX);\n-\t      if (note)\n-\t\t{\n-\t\t  control_flow_insns.safe_push (set_insn);\n-\t\t  add_reg_note (set_insn, REG_EH_REGION, XEXP (note, 0));\n-\t\t}\n-\t    }\n-\n-\t  src = gen_rtx_SUBREG (dest_mode, vec, 0);\n-\t  set = gen_rtx_SET (dest, src);\n-\n-\t  /* Drop possible dead definitions.  */\n-\t  PATTERN (insn) = set;\n-\n-\t  INSN_CODE (insn) = -1;\n-\t  recog_memoized (insn);\n-\t  df_insn_rescan (insn);\n-\t  bitmap_set_bit (convert_bbs, bb->index);\n-\t}\n-    }\n-\n-  if (v4sf_const0)\n-    {\n-      /* (Re-)discover loops so that bb->loop_father can be used in the\n-\t analysis below.  */\n-      loop_optimizer_init (AVOID_CFG_MODIFICATIONS);\n-\n-      /* Generate a vxorps at entry of the nearest dominator for basic\n-\t blocks with conversions, which is in the fake loop that\n-\t contains the whole function, so that there is only a single\n-\t vxorps in the whole function.   */\n-      bb = nearest_common_dominator_for_set (CDI_DOMINATORS,\n-\t\t\t\t\t     convert_bbs);\n-      while (bb->loop_father->latch\n-\t     != EXIT_BLOCK_PTR_FOR_FN (cfun))\n-\tbb = get_immediate_dominator (CDI_DOMINATORS,\n-\t\t\t\t      bb->loop_father->header);\n-\n-      set = gen_rtx_SET (v4sf_const0, CONST0_RTX (V4SFmode));\n-\n-      insn = BB_HEAD (bb);\n-      while (insn && !NONDEBUG_INSN_P (insn))\n-\t{\n-\t  if (insn == BB_END (bb))\n-\t    {\n-\t      insn = NULL;\n-\t      break;\n-\t    }\n-\t  insn = NEXT_INSN (insn);\n-\t}\n-      if (insn == BB_HEAD (bb))\n-        set_insn = emit_insn_before (set, insn);\n-      else\n-\tset_insn = emit_insn_after (set,\n-\t\t\t\t    insn ? PREV_INSN (insn) : BB_END (bb));\n-      df_insn_rescan (set_insn);\n-      df_process_deferred_rescans ();\n-      loop_optimizer_finalize ();\n-\n-      if (!control_flow_insns.is_empty ())\n-\t{\n-\t  free_dominance_info (CDI_DOMINATORS);\n-\n-\t  unsigned int i;\n-\t  FOR_EACH_VEC_ELT (control_flow_insns, i, insn)\n-\t    if (control_flow_insn_p (insn))\n-\t      {\n-\t\t/* Split the block after insn.  There will be a fallthru\n-\t\t   edge, which is OK so we keep it.  We have to create\n-\t\t   the exception edges ourselves.  */\n-\t\tbb = BLOCK_FOR_INSN (insn);\n-\t\tsplit_block (bb, insn);\n-\t\trtl_make_eh_edge (NULL, bb, BB_END (bb));\n-\t      }\n-\t}\n-    }\n-\n-  bitmap_obstack_release (NULL);\n-  BITMAP_FREE (convert_bbs);\n-\n-  timevar_pop (TV_MACH_DEP);\n-  return 0;\n-}\n-\n-namespace {\n-\n-const pass_data pass_data_remove_partial_avx_dependency =\n-{\n-  RTL_PASS, /* type */\n-  \"rpad\", /* name */\n-  OPTGROUP_NONE, /* optinfo_flags */\n-  TV_MACH_DEP, /* tv_id */\n-  0, /* properties_required */\n-  0, /* properties_provided */\n-  0, /* properties_destroyed */\n-  0, /* todo_flags_start */\n-  TODO_df_finish, /* todo_flags_finish */\n-};\n-\n-class pass_remove_partial_avx_dependency : public rtl_opt_pass\n-{\n-public:\n-  pass_remove_partial_avx_dependency (gcc::context *ctxt)\n-    : rtl_opt_pass (pass_data_remove_partial_avx_dependency, ctxt)\n-  {}\n-\n-  /* opt_pass methods: */\n-  virtual bool gate (function *)\n-    {\n-      return (TARGET_AVX\n-\t      && TARGET_SSE_PARTIAL_REG_DEPENDENCY\n-\t      && TARGET_SSE_MATH\n-\t      && optimize\n-\t      && optimize_function_for_speed_p (cfun));\n-    }\n-\n-  virtual unsigned int execute (function *)\n-    {\n-      return remove_partial_avx_dependency ();\n-    }\n-}; // class pass_rpad\n-\n-} // anon namespace\n-\n-rtl_opt_pass *\n-make_pass_remove_partial_avx_dependency (gcc::context *ctxt)\n-{\n-  return new pass_remove_partial_avx_dependency (ctxt);\n-}\n-\n-/* This compares the priority of target features in function DECL1\n-   and DECL2.  It returns positive value if DECL1 is higher priority,\n-   negative value if DECL2 is higher priority and 0 if they are the\n-   same.  */\n-\n-int\n-ix86_compare_version_priority (tree decl1, tree decl2)\n-{\n-  unsigned int priority1 = get_builtin_code_for_version (decl1, NULL);\n-  unsigned int priority2 = get_builtin_code_for_version (decl2, NULL);\n-\n-  return (int)priority1 - (int)priority2;\n-}\n-\n-/* V1 and V2 point to function versions with different priorities\n-   based on the target ISA.  This function compares their priorities.  */\n- \n-static int\n-feature_compare (const void *v1, const void *v2)\n-{\n-  typedef struct _function_version_info\n-    {\n-      tree version_decl;\n-      tree predicate_chain;\n-      unsigned int dispatch_priority;\n-    } function_version_info;\n-\n-  const function_version_info c1 = *(const function_version_info *)v1;\n-  const function_version_info c2 = *(const function_version_info *)v2;\n-  return (c2.dispatch_priority - c1.dispatch_priority);\n-}\n-\n-/* This adds a condition to the basic_block NEW_BB in function FUNCTION_DECL\n-   to return a pointer to VERSION_DECL if the outcome of the expression\n-   formed by PREDICATE_CHAIN is true.  This function will be called during\n-   version dispatch to decide which function version to execute.  It returns\n-   the basic block at the end, to which more conditions can be added.  */\n-\n-static basic_block\n-add_condition_to_bb (tree function_decl, tree version_decl,\n-\t\t     tree predicate_chain, basic_block new_bb)\n-{\n-  gimple *return_stmt;\n-  tree convert_expr, result_var;\n-  gimple *convert_stmt;\n-  gimple *call_cond_stmt;\n-  gimple *if_else_stmt;\n-\n-  basic_block bb1, bb2, bb3;\n-  edge e12, e23;\n-\n-  tree cond_var, and_expr_var = NULL_TREE;\n-  gimple_seq gseq;\n-\n-  tree predicate_decl, predicate_arg;\n-\n-  push_cfun (DECL_STRUCT_FUNCTION (function_decl));\n-\n-  gcc_assert (new_bb != NULL);\n-  gseq = bb_seq (new_bb);\n-\n-\n-  convert_expr = build1 (CONVERT_EXPR, ptr_type_node,\n-\t     \t\t build_fold_addr_expr (version_decl));\n-  result_var = create_tmp_var (ptr_type_node);\n-  convert_stmt = gimple_build_assign (result_var, convert_expr); \n-  return_stmt = gimple_build_return (result_var);\n-\n-  if (predicate_chain == NULL_TREE)\n-    {\n-      gimple_seq_add_stmt (&gseq, convert_stmt);\n-      gimple_seq_add_stmt (&gseq, return_stmt);\n-      set_bb_seq (new_bb, gseq);\n-      gimple_set_bb (convert_stmt, new_bb);\n-      gimple_set_bb (return_stmt, new_bb);\n-      pop_cfun ();\n-      return new_bb;\n-    }\n-\n-  while (predicate_chain != NULL)\n-    {\n-      cond_var = create_tmp_var (integer_type_node);\n-      predicate_decl = TREE_PURPOSE (predicate_chain);\n-      predicate_arg = TREE_VALUE (predicate_chain);\n-      call_cond_stmt = gimple_build_call (predicate_decl, 1, predicate_arg);\n-      gimple_call_set_lhs (call_cond_stmt, cond_var);\n-\n-      gimple_set_block (call_cond_stmt, DECL_INITIAL (function_decl));\n-      gimple_set_bb (call_cond_stmt, new_bb);\n-      gimple_seq_add_stmt (&gseq, call_cond_stmt);\n-\n-      predicate_chain = TREE_CHAIN (predicate_chain);\n-      \n-      if (and_expr_var == NULL)\n-        and_expr_var = cond_var;\n-      else\n-\t{\n-\t  gimple *assign_stmt;\n-\t  /* Use MIN_EXPR to check if any integer is zero?.\n-\t     and_expr_var = min_expr <cond_var, and_expr_var>  */\n-\t  assign_stmt = gimple_build_assign (and_expr_var,\n-\t\t\t  build2 (MIN_EXPR, integer_type_node,\n-\t\t\t\t  cond_var, and_expr_var));\n-\n-\t  gimple_set_block (assign_stmt, DECL_INITIAL (function_decl));\n-\t  gimple_set_bb (assign_stmt, new_bb);\n-\t  gimple_seq_add_stmt (&gseq, assign_stmt);\n-\t}\n-    }\n-\n-  if_else_stmt = gimple_build_cond (GT_EXPR, and_expr_var,\n-\t  \t\t            integer_zero_node,\n-\t\t\t\t    NULL_TREE, NULL_TREE);\n-  gimple_set_block (if_else_stmt, DECL_INITIAL (function_decl));\n-  gimple_set_bb (if_else_stmt, new_bb);\n-  gimple_seq_add_stmt (&gseq, if_else_stmt);\n-\n-  gimple_seq_add_stmt (&gseq, convert_stmt);\n-  gimple_seq_add_stmt (&gseq, return_stmt);\n-  set_bb_seq (new_bb, gseq);\n-\n-  bb1 = new_bb;\n-  e12 = split_block (bb1, if_else_stmt);\n-  bb2 = e12->dest;\n-  e12->flags &= ~EDGE_FALLTHRU;\n-  e12->flags |= EDGE_TRUE_VALUE;\n-\n-  e23 = split_block (bb2, return_stmt);\n-\n-  gimple_set_bb (convert_stmt, bb2);\n-  gimple_set_bb (return_stmt, bb2);\n-\n-  bb3 = e23->dest;\n-  make_edge (bb1, bb3, EDGE_FALSE_VALUE); \n-\n-  remove_edge (e23);\n-  make_edge (bb2, EXIT_BLOCK_PTR_FOR_FN (cfun), 0);\n-\n-  pop_cfun ();\n-\n-  return bb3;\n-}\n-\n-/* This function generates the dispatch function for\n-   multi-versioned functions.  DISPATCH_DECL is the function which will\n-   contain the dispatch logic.  FNDECLS are the function choices for\n-   dispatch, and is a tree chain.  EMPTY_BB is the basic block pointer\n-   in DISPATCH_DECL in which the dispatch code is generated.  */\n-\n-static int\n-dispatch_function_versions (tree dispatch_decl,\n-\t\t\t    void *fndecls_p,\n-\t\t\t    basic_block *empty_bb)\n-{\n-  tree default_decl;\n-  gimple *ifunc_cpu_init_stmt;\n-  gimple_seq gseq;\n-  int ix;\n-  tree ele;\n-  vec<tree> *fndecls;\n-  unsigned int num_versions = 0;\n-  unsigned int actual_versions = 0;\n-  unsigned int i;\n-\n-  struct _function_version_info\n-    {\n-      tree version_decl;\n-      tree predicate_chain;\n-      unsigned int dispatch_priority;\n-    }*function_version_info;\n-\n-  gcc_assert (dispatch_decl != NULL\n-\t      && fndecls_p != NULL\n-\t      && empty_bb != NULL);\n-\n-  /*fndecls_p is actually a vector.  */\n-  fndecls = static_cast<vec<tree> *> (fndecls_p);\n-\n-  /* At least one more version other than the default.  */\n-  num_versions = fndecls->length ();\n-  gcc_assert (num_versions >= 2);\n-\n-  function_version_info = (struct _function_version_info *)\n-    XNEWVEC (struct _function_version_info, (num_versions - 1));\n-\n-  /* The first version in the vector is the default decl.  */\n-  default_decl = (*fndecls)[0];\n-\n-  push_cfun (DECL_STRUCT_FUNCTION (dispatch_decl));\n-\n-  gseq = bb_seq (*empty_bb);\n-  /* Function version dispatch is via IFUNC.  IFUNC resolvers fire before\n-     constructors, so explicity call __builtin_cpu_init here.  */\n-  ifunc_cpu_init_stmt\n-    = gimple_build_call_vec (get_ix86_builtin (IX86_BUILTIN_CPU_INIT), vNULL);\n-  gimple_seq_add_stmt (&gseq, ifunc_cpu_init_stmt);\n-  gimple_set_bb (ifunc_cpu_init_stmt, *empty_bb);\n-  set_bb_seq (*empty_bb, gseq);\n-\n-  pop_cfun ();\n-\n-\n-  for (ix = 1; fndecls->iterate (ix, &ele); ++ix)\n-    {\n-      tree version_decl = ele;\n-      tree predicate_chain = NULL_TREE;\n-      unsigned int priority;\n-      /* Get attribute string, parse it and find the right predicate decl.\n-         The predicate function could be a lengthy combination of many\n-\t features, like arch-type and various isa-variants.  */\n-      priority = get_builtin_code_for_version (version_decl,\n-\t \t\t\t               &predicate_chain);\n-\n-      if (predicate_chain == NULL_TREE)\n-\tcontinue;\n-\n-      function_version_info [actual_versions].version_decl = version_decl;\n-      function_version_info [actual_versions].predicate_chain\n-\t = predicate_chain;\n-      function_version_info [actual_versions].dispatch_priority = priority;\n-      actual_versions++;\n-    }\n-\n-  /* Sort the versions according to descending order of dispatch priority.  The\n-     priority is based on the ISA.  This is not a perfect solution.  There\n-     could still be ambiguity.  If more than one function version is suitable\n-     to execute,  which one should be dispatched?  In future, allow the user\n-     to specify a dispatch  priority next to the version.  */\n-  qsort (function_version_info, actual_versions,\n-         sizeof (struct _function_version_info), feature_compare);\n-\n-  for  (i = 0; i < actual_versions; ++i)\n-    *empty_bb = add_condition_to_bb (dispatch_decl,\n-\t\t\t\t     function_version_info[i].version_decl,\n-\t\t\t\t     function_version_info[i].predicate_chain,\n-\t\t\t\t     *empty_bb);\n-\n-  /* dispatch default version at the end.  */\n-  *empty_bb = add_condition_to_bb (dispatch_decl, default_decl,\n-\t\t\t\t   NULL, *empty_bb);\n-\n-  free (function_version_info);\n-  return 0;\n-}\n-\n-/* This function changes the assembler name for functions that are\n-   versions.  If DECL is a function version and has a \"target\"\n-   attribute, it appends the attribute string to its assembler name.  */\n-\n-static tree\n-ix86_mangle_function_version_assembler_name (tree decl, tree id)\n-{\n-  tree version_attr;\n-  const char *orig_name, *version_string;\n-  char *attr_str, *assembler_name;\n-\n-  if (DECL_DECLARED_INLINE_P (decl)\n-      && lookup_attribute (\"gnu_inline\",\n-\t\t\t   DECL_ATTRIBUTES (decl)))\n-    error_at (DECL_SOURCE_LOCATION (decl),\n-\t      \"function versions cannot be marked as %<gnu_inline%>,\"\n-\t      \" bodies have to be generated\");\n-\n-  if (DECL_VIRTUAL_P (decl)\n-      || DECL_VINDEX (decl))\n-    sorry (\"virtual function multiversioning not supported\");\n-\n-  version_attr = lookup_attribute (\"target\", DECL_ATTRIBUTES (decl));\n-\n-  /* target attribute string cannot be NULL.  */\n-  gcc_assert (version_attr != NULL_TREE);\n-\n-  orig_name = IDENTIFIER_POINTER (id);\n-  version_string\n-    = TREE_STRING_POINTER (TREE_VALUE (TREE_VALUE (version_attr)));\n-\n-  if (strcmp (version_string, \"default\") == 0)\n-    return id;\n-\n-  attr_str = sorted_attr_string (TREE_VALUE (version_attr));\n-  assembler_name = XNEWVEC (char, strlen (orig_name) + strlen (attr_str) + 2);\n-\n-  sprintf (assembler_name, \"%s.%s\", orig_name, attr_str);\n-\n-  /* Allow assembler name to be modified if already set.  */\n-  if (DECL_ASSEMBLER_NAME_SET_P (decl))\n-    SET_DECL_RTL (decl, NULL);\n-\n-  tree ret = get_identifier (assembler_name);\n-  XDELETEVEC (attr_str);\n-  XDELETEVEC (assembler_name);\n-  return ret;\n-}\n-\n-tree \n-ix86_mangle_decl_assembler_name (tree decl, tree id)\n-{\n-  /* For function version, add the target suffix to the assembler name.  */\n-  if (TREE_CODE (decl) == FUNCTION_DECL\n-      && DECL_FUNCTION_VERSIONED (decl))\n-    id = ix86_mangle_function_version_assembler_name (decl, id);\n-#ifdef SUBTARGET_MANGLE_DECL_ASSEMBLER_NAME\n-  id = SUBTARGET_MANGLE_DECL_ASSEMBLER_NAME (decl, id);\n-#endif\n-\n-  return id;\n-}\n-\n-/* Make a dispatcher declaration for the multi-versioned function DECL.\n-   Calls to DECL function will be replaced with calls to the dispatcher\n-   by the front-end.  Returns the decl of the dispatcher function.  */\n-\n-tree\n-ix86_get_function_versions_dispatcher (void *decl)\n-{\n-  tree fn = (tree) decl;\n-  struct cgraph_node *node = NULL;\n-  struct cgraph_node *default_node = NULL;\n-  struct cgraph_function_version_info *node_v = NULL;\n-  struct cgraph_function_version_info *first_v = NULL;\n-\n-  tree dispatch_decl = NULL;\n-\n-  struct cgraph_function_version_info *default_version_info = NULL;\n- \n-  gcc_assert (fn != NULL && DECL_FUNCTION_VERSIONED (fn));\n-\n-  node = cgraph_node::get (fn);\n-  gcc_assert (node != NULL);\n-\n-  node_v = node->function_version ();\n-  gcc_assert (node_v != NULL);\n- \n-  if (node_v->dispatcher_resolver != NULL)\n-    return node_v->dispatcher_resolver;\n-\n-  /* Find the default version and make it the first node.  */\n-  first_v = node_v;\n-  /* Go to the beginning of the chain.  */\n-  while (first_v->prev != NULL)\n-    first_v = first_v->prev;\n-  default_version_info = first_v;\n-  while (default_version_info != NULL)\n-    {\n-      if (is_function_default_version\n-\t    (default_version_info->this_node->decl))\n-        break;\n-      default_version_info = default_version_info->next;\n-    }\n-\n-  /* If there is no default node, just return NULL.  */\n-  if (default_version_info == NULL)\n-    return NULL;\n-\n-  /* Make default info the first node.  */\n-  if (first_v != default_version_info)\n-    {\n-      default_version_info->prev->next = default_version_info->next;\n-      if (default_version_info->next)\n-        default_version_info->next->prev = default_version_info->prev;\n-      first_v->prev = default_version_info;\n-      default_version_info->next = first_v;\n-      default_version_info->prev = NULL;\n-    }\n-\n-  default_node = default_version_info->this_node;\n-\n-#if defined (ASM_OUTPUT_TYPE_DIRECTIVE)\n-  if (targetm.has_ifunc_p ())\n-    {\n-      struct cgraph_function_version_info *it_v = NULL;\n-      struct cgraph_node *dispatcher_node = NULL;\n-      struct cgraph_function_version_info *dispatcher_version_info = NULL;\n-\n-      /* Right now, the dispatching is done via ifunc.  */\n-      dispatch_decl = make_dispatcher_decl (default_node->decl);\n-\n-      dispatcher_node = cgraph_node::get_create (dispatch_decl);\n-      gcc_assert (dispatcher_node != NULL);\n-      dispatcher_node->dispatcher_function = 1;\n-      dispatcher_version_info\n-\t= dispatcher_node->insert_new_function_version ();\n-      dispatcher_version_info->next = default_version_info;\n-      dispatcher_node->definition = 1;\n-\n-      /* Set the dispatcher for all the versions.  */\n-      it_v = default_version_info;\n-      while (it_v != NULL)\n-\t{\n-\t  it_v->dispatcher_resolver = dispatch_decl;\n-\t  it_v = it_v->next;\n-\t}\n-    }\n-  else\n-#endif\n-    {\n-      error_at (DECL_SOURCE_LOCATION (default_node->decl),\n-\t\t\"multiversioning needs %<ifunc%> which is not supported \"\n-\t\t\"on this target\");\n-    }\n-\n-  return dispatch_decl;\n-}\n-\n-/* Make the resolver function decl to dispatch the versions of\n-   a multi-versioned function,  DEFAULT_DECL.  IFUNC_ALIAS_DECL is\n-   ifunc alias that will point to the created resolver.  Create an\n-   empty basic block in the resolver and store the pointer in\n-   EMPTY_BB.  Return the decl of the resolver function.  */\n-\n-static tree\n-make_resolver_func (const tree default_decl,\n-\t\t    const tree ifunc_alias_decl,\n-\t\t    basic_block *empty_bb)\n-{\n-  tree decl, type, t;\n-\n-  /* Create resolver function name based on default_decl.  */\n-  tree decl_name = clone_function_name (default_decl, \"resolver\");\n-  const char *resolver_name = IDENTIFIER_POINTER (decl_name);\n-\n-  /* The resolver function should return a (void *). */\n-  type = build_function_type_list (ptr_type_node, NULL_TREE);\n-\n-  decl = build_fn_decl (resolver_name, type);\n-  SET_DECL_ASSEMBLER_NAME (decl, decl_name);\n-\n-  DECL_NAME (decl) = decl_name;\n-  TREE_USED (decl) = 1;\n-  DECL_ARTIFICIAL (decl) = 1;\n-  DECL_IGNORED_P (decl) = 1;\n-  TREE_PUBLIC (decl) = 0;\n-  DECL_UNINLINABLE (decl) = 1;\n-\n-  /* Resolver is not external, body is generated.  */\n-  DECL_EXTERNAL (decl) = 0;\n-  DECL_EXTERNAL (ifunc_alias_decl) = 0;\n-\n-  DECL_CONTEXT (decl) = NULL_TREE;\n-  DECL_INITIAL (decl) = make_node (BLOCK);\n-  DECL_STATIC_CONSTRUCTOR (decl) = 0;\n-\n-  if (DECL_COMDAT_GROUP (default_decl)\n-      || TREE_PUBLIC (default_decl))\n-    {\n-      /* In this case, each translation unit with a call to this\n-\t versioned function will put out a resolver.  Ensure it\n-\t is comdat to keep just one copy.  */\n-      DECL_COMDAT (decl) = 1;\n-      make_decl_one_only (decl, DECL_ASSEMBLER_NAME (decl));\n-    }\n-  else\n-    TREE_PUBLIC (ifunc_alias_decl) = 0;\n-\n-  /* Build result decl and add to function_decl. */\n-  t = build_decl (UNKNOWN_LOCATION, RESULT_DECL, NULL_TREE, ptr_type_node);\n-  DECL_CONTEXT (t) = decl;\n-  DECL_ARTIFICIAL (t) = 1;\n-  DECL_IGNORED_P (t) = 1;\n-  DECL_RESULT (decl) = t;\n-\n-  gimplify_function_tree (decl);\n-  push_cfun (DECL_STRUCT_FUNCTION (decl));\n-  *empty_bb = init_lowered_empty_function (decl, false,\n-\t\t\t\t\t   profile_count::uninitialized ());\n-\n-  cgraph_node::add_new_function (decl, true);\n-  symtab->call_cgraph_insertion_hooks (cgraph_node::get_create (decl));\n-\n-  pop_cfun ();\n-\n-  gcc_assert (ifunc_alias_decl != NULL);\n-  /* Mark ifunc_alias_decl as \"ifunc\" with resolver as resolver_name.  */\n-  DECL_ATTRIBUTES (ifunc_alias_decl)\n-    = make_attribute (\"ifunc\", resolver_name,\n-\t\t      DECL_ATTRIBUTES (ifunc_alias_decl));\n-\n-  /* Create the alias for dispatch to resolver here.  */\n-  cgraph_node::create_same_body_alias (ifunc_alias_decl, decl);\n-  return decl;\n-}\n-\n-/* Generate the dispatching code body to dispatch multi-versioned function\n-   DECL.  The target hook is called to process the \"target\" attributes and\n-   provide the code to dispatch the right function at run-time.  NODE points\n-   to the dispatcher decl whose body will be created.  */\n-\n-tree \n-ix86_generate_version_dispatcher_body (void *node_p)\n-{\n-  tree resolver_decl;\n-  basic_block empty_bb;\n-  tree default_ver_decl;\n-  struct cgraph_node *versn;\n-  struct cgraph_node *node;\n-\n-  struct cgraph_function_version_info *node_version_info = NULL;\n-  struct cgraph_function_version_info *versn_info = NULL;\n-\n-  node = (cgraph_node *)node_p;\n-\n-  node_version_info = node->function_version ();\n-  gcc_assert (node->dispatcher_function\n-\t      && node_version_info != NULL);\n-\n-  if (node_version_info->dispatcher_resolver)\n-    return node_version_info->dispatcher_resolver;\n-\n-  /* The first version in the chain corresponds to the default version.  */\n-  default_ver_decl = node_version_info->next->this_node->decl;\n-\n-  /* node is going to be an alias, so remove the finalized bit.  */\n-  node->definition = false;\n-\n-  resolver_decl = make_resolver_func (default_ver_decl,\n-\t\t\t\t      node->decl, &empty_bb);\n-\n-  node_version_info->dispatcher_resolver = resolver_decl;\n-\n-  push_cfun (DECL_STRUCT_FUNCTION (resolver_decl));\n-\n-  auto_vec<tree, 2> fn_ver_vec;\n-\n-  for (versn_info = node_version_info->next; versn_info;\n-       versn_info = versn_info->next)\n-    {\n-      versn = versn_info->this_node;\n-      /* Check for virtual functions here again, as by this time it should\n-\t have been determined if this function needs a vtable index or\n-\t not.  This happens for methods in derived classes that override\n-\t virtual methods in base classes but are not explicitly marked as\n-\t virtual.  */\n-      if (DECL_VINDEX (versn->decl))\n-\tsorry (\"virtual function multiversioning not supported\");\n-\n-      fn_ver_vec.safe_push (versn->decl);\n-    }\n-\n-  dispatch_function_versions (resolver_decl, &fn_ver_vec, &empty_bb);\n-  cgraph_edge::rebuild_edges ();\n-  pop_cfun ();\n-  return resolver_decl;\n-}\n-\n-"}, {"sha": "67480b2deea75e4c1549ddc5b04dd125c0520463", "filename": "gcc/config/i386/i386-options.c", "status": "removed", "additions": 0, "deletions": 3799, "changes": 3799, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fi386-options.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fi386-options.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-options.c?ref=566c5f1aaae120d2283103e68ecf1c1a83dd4459"}, {"sha": "d685d31ebcddd5c01662b7ddc65a22614fb8cbb3", "filename": "gcc/config/i386/t-cet", "status": "removed", "additions": 0, "deletions": 21, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Ft-cet", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Ft-cet", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Ft-cet?ref=566c5f1aaae120d2283103e68ecf1c1a83dd4459", "patch": "@@ -1,21 +0,0 @@\n-# Copyright (C) 2017-2020 Free Software Foundation, Inc.\n-#\n-# This file is part of GCC.\n-#\n-# GCC is free software; you can redistribute it and/or modify\n-# it under the terms of the GNU General Public License as published by\n-# the Free Software Foundation; either version 3, or (at your option)\n-# any later version.\n-#\n-# GCC is distributed in the hope that it will be useful,\n-# but WITHOUT ANY WARRANTY; without even the implied warranty of\n-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n-# GNU General Public License for more details.\n-#\n-# You should have received a copy of the GNU General Public License\n-# along with GCC; see the file COPYING3.  If not see\n-# <http://www.gnu.org/licenses/>.\n-\n-cet.o: $(srcdir)/config/i386/cet.c\n-\t  $(COMPILE) $<\n-\t  $(POSTCOMPILE)"}, {"sha": "1318efa6be44b9ada0d161256618af14a550aef8", "filename": "gcc/config/i386/x86-tune-sched-atom.c", "status": "removed", "additions": 0, "deletions": 246, "changes": 246, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-atom.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-atom.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-atom.c?ref=566c5f1aaae120d2283103e68ecf1c1a83dd4459", "patch": "@@ -1,246 +0,0 @@\n-/* Scheduler hooks for IA-32 which implement atom+ specific logic.\n-   Copyright (C) 1988-2020 Free Software Foundation, Inc.\n-\n-This file is part of GCC.\n-\n-GCC is free software; you can redistribute it and/or modify\n-it under the terms of the GNU General Public License as published by\n-the Free Software Foundation; either version 3, or (at your option)\n-any later version.\n-\n-GCC is distributed in the hope that it will be useful,\n-but WITHOUT ANY WARRANTY; without even the implied warranty of\n-MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n-GNU General Public License for more details.\n-\n-You should have received a copy of the GNU General Public License\n-along with GCC; see the file COPYING3.  If not see\n-<http://www.gnu.org/licenses/>.  */\n-\n-#define IN_TARGET_CODE 1\n-\n-#include \"config.h\"\n-#include \"system.h\"\n-#include \"coretypes.h\"\n-#include \"backend.h\"\n-#include \"rtl.h\"\n-#include \"tree.h\"\n-#include \"cfghooks.h\"\n-#include \"tm_p.h\"\n-#include \"insn-config.h\"\n-#include \"insn-attr.h\"\n-#include \"recog.h\"\n-#include \"target.h\"\n-#include \"rtl-iter.h\"\n-#include \"regset.h\"\n-#include \"sched-int.h\"\n-\n-/* Try to reorder ready list to take advantage of Atom pipelined IMUL\n-   execution. It is applied if\n-   (1) IMUL instruction is on the top of list;\n-   (2) There exists the only producer of independent IMUL instruction in\n-       ready list.\n-   Return index of IMUL producer if it was found and -1 otherwise.  */\n-static int\n-do_reorder_for_imul (rtx_insn **ready, int n_ready)\n-{\n-  rtx_insn *insn;\n-  rtx set, insn1, insn2;\n-  sd_iterator_def sd_it;\n-  dep_t dep;\n-  int index = -1;\n-  int i;\n-\n-  if (!TARGET_BONNELL)\n-    return index;\n-\n-  /* Check that IMUL instruction is on the top of ready list.  */\n-  insn = ready[n_ready - 1];\n-  set = single_set (insn);\n-  if (!set)\n-    return index;\n-  if (!(GET_CODE (SET_SRC (set)) == MULT\n-      && GET_MODE (SET_SRC (set)) == SImode))\n-    return index;\n-\n-  /* Search for producer of independent IMUL instruction.  */\n-  for (i = n_ready - 2; i >= 0; i--)\n-    {\n-      insn = ready[i];\n-      if (!NONDEBUG_INSN_P (insn))\n-\tcontinue;\n-      /* Skip IMUL instruction.  */\n-      insn2 = PATTERN (insn);\n-      if (GET_CODE (insn2) == PARALLEL)\n-\tinsn2 = XVECEXP (insn2, 0, 0);\n-      if (GET_CODE (insn2) == SET\n-\t  && GET_CODE (SET_SRC (insn2)) == MULT\n-\t  && GET_MODE (SET_SRC (insn2)) == SImode)\n-\tcontinue;\n-\n-      FOR_EACH_DEP (insn, SD_LIST_FORW, sd_it, dep)\n-\t{\n-\t  rtx con;\n-\t  con = DEP_CON (dep);\n-\t  if (!NONDEBUG_INSN_P (con))\n-\t    continue;\n-\t  insn1 = PATTERN (con);\n-\t  if (GET_CODE (insn1) == PARALLEL)\n-\t    insn1 = XVECEXP (insn1, 0, 0);\n-\n-\t  if (GET_CODE (insn1) == SET\n-\t      && GET_CODE (SET_SRC (insn1)) == MULT\n-\t      && GET_MODE (SET_SRC (insn1)) == SImode)\n-\t    {\n-\t      sd_iterator_def sd_it1;\n-\t      dep_t dep1;\n-\t      /* Check if there is no other dependee for IMUL.  */\n-\t      index = i;\n-\t      FOR_EACH_DEP (con, SD_LIST_BACK, sd_it1, dep1)\n-\t\t{\n-\t\t  rtx pro;\n-\t\t  pro = DEP_PRO (dep1);\n-\t\t  if (!NONDEBUG_INSN_P (pro))\n-\t\t    continue;\n-\t\t  if (pro != insn)\n-\t\t    index = -1;\n-\t\t}\n-\t      if (index >= 0)\n-\t\tbreak;\n-\t    }\n-\t}\n-      if (index >= 0)\n-\tbreak;\n-    }\n-  return index;\n-}\n-\n-/* Try to find the best candidate on the top of ready list if two insns\n-   have the same priority - candidate is best if its dependees were\n-   scheduled earlier. Applied for Silvermont only.\n-   Return true if top 2 insns must be interchanged.  */\n-static bool\n-swap_top_of_ready_list (rtx_insn **ready, int n_ready)\n-{\n-  rtx_insn *top = ready[n_ready - 1];\n-  rtx_insn *next = ready[n_ready - 2];\n-  rtx set;\n-  sd_iterator_def sd_it;\n-  dep_t dep;\n-  int clock1 = -1;\n-  int clock2 = -1;\n-  #define INSN_TICK(INSN) (HID (INSN)->tick)\n-\n-  if (!TARGET_SILVERMONT && !TARGET_INTEL)\n-    return false;\n-\n-  if (!NONDEBUG_INSN_P (top))\n-    return false;\n-  if (!NONJUMP_INSN_P (top))\n-    return false;\n-  if (!NONDEBUG_INSN_P (next))\n-    return false;\n-  if (!NONJUMP_INSN_P (next))\n-    return false;\n-  set = single_set (top);\n-  if (!set)\n-    return false;\n-  set = single_set (next);\n-  if (!set)\n-    return false;\n-\n-  if (INSN_PRIORITY_KNOWN (top) && INSN_PRIORITY_KNOWN (next))\n-    {\n-      if (INSN_PRIORITY (top) != INSN_PRIORITY (next))\n-\treturn false;\n-      /* Determine winner more precise.  */\n-      FOR_EACH_DEP (top, SD_LIST_RES_BACK, sd_it, dep)\n-\t{\n-\t  rtx pro;\n-\t  pro = DEP_PRO (dep);\n-\t  if (!NONDEBUG_INSN_P (pro))\n-\t    continue;\n-\t  if (INSN_TICK (pro) > clock1)\n-\t    clock1 = INSN_TICK (pro);\n-\t}\n-      FOR_EACH_DEP (next, SD_LIST_RES_BACK, sd_it, dep)\n-\t{\n-\t  rtx pro;\n-\t  pro = DEP_PRO (dep);\n-\t  if (!NONDEBUG_INSN_P (pro))\n-\t    continue;\n-\t  if (INSN_TICK (pro) > clock2)\n-\t    clock2 = INSN_TICK (pro);\n-\t}\n-\n-      if (clock1 == clock2)\n-\t{\n-\t  /* Determine winner - load must win. */\n-\t  enum attr_memory memory1, memory2;\n-\t  memory1 = get_attr_memory (top);\n-\t  memory2 = get_attr_memory (next);\n-\t  if (memory2 == MEMORY_LOAD && memory1 != MEMORY_LOAD)\n-\t    return true;\n-\t}\n-\treturn (bool) (clock2 < clock1);\n-    }\n-  return false;\n-  #undef INSN_TICK\n-}\n-\n-/* Perform possible reodering of ready list for Atom/Silvermont only.\n-   Return issue rate.  */\n-int\n-ix86_atom_sched_reorder (FILE *dump, int sched_verbose, rtx_insn **ready,\n-\t\t         int *pn_ready, int clock_var)\n-{\n-  int issue_rate = -1;\n-  int n_ready = *pn_ready;\n-  int i;\n-  rtx_insn *insn;\n-  int index = -1;\n-\n-  /* Set up issue rate.  */\n-  issue_rate = ix86_issue_rate ();\n-\n-  /* Do reodering for BONNELL/SILVERMONT only.  */\n-  if (!TARGET_BONNELL && !TARGET_SILVERMONT && !TARGET_INTEL)\n-    return issue_rate;\n-\n-  /* Nothing to do if ready list contains only 1 instruction.  */\n-  if (n_ready <= 1)\n-    return issue_rate;\n-\n-  /* Do reodering for post-reload scheduler only.  */\n-  if (!reload_completed)\n-    return issue_rate;\n-\n-  if ((index = do_reorder_for_imul (ready, n_ready)) >= 0)\n-    {\n-      if (sched_verbose > 1)\n-\tfprintf (dump, \";;\\tatom sched_reorder: put %d insn on top\\n\",\n-\t\t INSN_UID (ready[index]));\n-\n-      /* Put IMUL producer (ready[index]) at the top of ready list.  */\n-      insn = ready[index];\n-      for (i = index; i < n_ready - 1; i++)\n-\tready[i] = ready[i + 1];\n-      ready[n_ready - 1] = insn;\n-      return issue_rate;\n-    }\n-\n-  /* Skip selective scheduling since HID is not populated in it.  */\n-  if (clock_var != 0\n-      && !sel_sched_p ()\n-      && swap_top_of_ready_list (ready, n_ready))\n-    {\n-      if (sched_verbose > 1)\n-\tfprintf (dump, \";;\\tslm sched_reorder: swap %d and %d insns\\n\",\n-\t\t INSN_UID (ready[n_ready - 1]), INSN_UID (ready[n_ready - 2]));\n-      /* Swap 2 top elements of ready list.  */\n-      insn = ready[n_ready - 1];\n-      ready[n_ready - 1] = ready[n_ready - 2];\n-      ready[n_ready - 2] = insn;\n-    }\n-  return issue_rate;\n-}"}, {"sha": "8c2abc4e8af38bf3c349bba56c60eabe9650eb88", "filename": "gcc/config/i386/x86-tune-sched-bd.c", "status": "removed", "additions": 0, "deletions": 824, "changes": 824, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-bd.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-bd.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-bd.c?ref=566c5f1aaae120d2283103e68ecf1c1a83dd4459", "patch": "@@ -1,824 +0,0 @@\n-/* Scheduler hooks for IA-32 which implement bdver1-4 specific logic.\n-   Copyright (C) 1988-2020 Free Software Foundation, Inc.\n-\n-This file is part of GCC.\n-\n-GCC is free software; you can redistribute it and/or modify\n-it under the terms of the GNU General Public License as published by\n-the Free Software Foundation; either version 3, or (at your option)\n-any later version.\n-\n-GCC is distributed in the hope that it will be useful,\n-but WITHOUT ANY WARRANTY; without even the implied warranty of\n-MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n-GNU General Public License for more details.\n-\n-You should have received a copy of the GNU General Public License\n-along with GCC; see the file COPYING3.  If not see\n-<http://www.gnu.org/licenses/>.  */\n-\n-#define IN_TARGET_CODE 1\n-\n-#include \"config.h\"\n-#include \"system.h\"\n-#include \"coretypes.h\"\n-#include \"backend.h\"\n-#include \"rtl.h\"\n-#include \"tree.h\"\n-#include \"cfghooks.h\"\n-#include \"tm_p.h\"\n-#include \"insn-config.h\"\n-#include \"insn-attr.h\"\n-#include \"recog.h\"\n-#include \"target.h\"\n-#include \"rtl-iter.h\"\n-#include \"regset.h\"\n-#include \"sched-int.h\"\n-\n-/* The size of the dispatch window is the total number of bytes of\n-   object code allowed in a window.  */\n-#define DISPATCH_WINDOW_SIZE 16\n-\n-/* Number of dispatch windows considered for scheduling.  */\n-#define MAX_DISPATCH_WINDOWS 3\n-\n-/* Maximum number of instructions in a window.  */\n-#define MAX_INSN 4\n-\n-/* Maximum number of immediate operands in a window.  */\n-#define MAX_IMM 4\n-\n-/* Maximum number of immediate bits allowed in a window.  */\n-#define MAX_IMM_SIZE 128\n-\n-/* Maximum number of 32 bit immediates allowed in a window.  */\n-#define MAX_IMM_32 4\n-\n-/* Maximum number of 64 bit immediates allowed in a window.  */\n-#define MAX_IMM_64 2\n-\n-/* Maximum total of loads or prefetches allowed in a window.  */\n-#define MAX_LOAD 2\n-\n-/* Maximum total of stores allowed in a window.  */\n-#define MAX_STORE 1\n-\n-#undef BIG\n-#define BIG 100\n-\n-\n-/* Dispatch groups.  Istructions that affect the mix in a dispatch window.  */\n-enum dispatch_group {\n-  disp_no_group = 0,\n-  disp_load,\n-  disp_store,\n-  disp_load_store,\n-  disp_prefetch,\n-  disp_imm,\n-  disp_imm_32,\n-  disp_imm_64,\n-  disp_branch,\n-  disp_cmp,\n-  disp_jcc,\n-  disp_last\n-};\n-\n-/* Number of allowable groups in a dispatch window.  It is an array\n-   indexed by dispatch_group enum.  100 is used as a big number,\n-   because the number of these kind of operations does not have any\n-   effect in dispatch window, but we need them for other reasons in\n-   the table.  */\n-static unsigned int num_allowable_groups[disp_last] = {\n-  0, 2, 1, 1, 2, 4, 4, 2, 1, BIG, BIG\n-};\n-\n-char group_name[disp_last + 1][16] = {\n-  \"disp_no_group\", \"disp_load\", \"disp_store\", \"disp_load_store\",\n-  \"disp_prefetch\", \"disp_imm\", \"disp_imm_32\", \"disp_imm_64\",\n-  \"disp_branch\", \"disp_cmp\", \"disp_jcc\", \"disp_last\"\n-};\n-\n-/* Instruction path.  */\n-enum insn_path {\n-  no_path = 0,\n-  path_single, /* Single micro op.  */\n-  path_double, /* Double micro op.  */\n-  path_multi,  /* Instructions with more than 2 micro op..  */\n-  last_path\n-};\n-\n-/* sched_insn_info defines a window to the instructions scheduled in\n-   the basic block.  It contains a pointer to the insn_info table and\n-   the instruction scheduled.\n-\n-   Windows are allocated for each basic block and are linked\n-   together.  */\n-typedef struct sched_insn_info_s {\n-  rtx insn;\n-  enum dispatch_group group;\n-  enum insn_path path;\n-  int byte_len;\n-  int imm_bytes;\n-} sched_insn_info;\n-\n-/* Linked list of dispatch windows.  This is a two way list of\n-   dispatch windows of a basic block.  It contains information about\n-   the number of uops in the window and the total number of\n-   instructions and of bytes in the object code for this dispatch\n-   window.  */\n-typedef struct dispatch_windows_s {\n-  int num_insn;            /* Number of insn in the window.  */\n-  int num_uops;            /* Number of uops in the window.  */\n-  int window_size;         /* Number of bytes in the window.  */\n-  int window_num;          /* Window number between 0 or 1.  */\n-  int num_imm;             /* Number of immediates in an insn.  */\n-  int num_imm_32;          /* Number of 32 bit immediates in an insn.  */\n-  int num_imm_64;          /* Number of 64 bit immediates in an insn.  */\n-  int imm_size;            /* Total immediates in the window.  */\n-  int num_loads;           /* Total memory loads in the window.  */\n-  int num_stores;          /* Total memory stores in the window.  */\n-  int violation;          /* Violation exists in window.  */\n-  sched_insn_info *window; /* Pointer to the window.  */\n-  struct dispatch_windows_s *next;\n-  struct dispatch_windows_s *prev;\n-} dispatch_windows;\n-\n-/* Immediate valuse used in an insn.  */\n-typedef struct imm_info_s\n-  {\n-    int imm;\n-    int imm32;\n-    int imm64;\n-  } imm_info;\n-\n-static dispatch_windows *dispatch_window_list;\n-static dispatch_windows *dispatch_window_list1;\n-\n-/* Get dispatch group of insn.  */\n-\n-static enum dispatch_group\n-get_mem_group (rtx_insn *insn)\n-{\n-  enum attr_memory memory;\n-\n-  if (INSN_CODE (insn) < 0)\n-    return disp_no_group;\n-  memory = get_attr_memory (insn);\n-  if (memory == MEMORY_STORE)\n-    return disp_store;\n-\n-  if (memory == MEMORY_LOAD)\n-    return disp_load;\n-\n-  if (memory == MEMORY_BOTH)\n-    return disp_load_store;\n-\n-  return disp_no_group;\n-}\n-\n-/* Return true if insn is a compare instruction.  */\n-\n-static bool\n-is_cmp (rtx_insn *insn)\n-{\n-  enum attr_type type;\n-\n-  type = get_attr_type (insn);\n-  return (type == TYPE_TEST\n-\t  || type == TYPE_ICMP\n-\t  || type == TYPE_FCMP\n-\t  || GET_CODE (PATTERN (insn)) == COMPARE);\n-}\n-\n-/* Return true if a dispatch violation encountered.  */\n-\n-static bool\n-dispatch_violation (void)\n-{\n-  if (dispatch_window_list->next)\n-    return dispatch_window_list->next->violation;\n-  return dispatch_window_list->violation;\n-}\n-\n-/* Return true if insn is a branch instruction.  */\n-\n-static bool\n-is_branch (rtx_insn *insn)\n-{\n-  return (CALL_P (insn) || JUMP_P (insn));\n-}\n-\n-/* Return true if insn is a prefetch instruction.  */\n-\n-static bool\n-is_prefetch (rtx_insn *insn)\n-{\n-  return NONJUMP_INSN_P (insn) && GET_CODE (PATTERN (insn)) == PREFETCH;\n-}\n-\n-/* This function initializes a dispatch window and the list container holding a\n-   pointer to the window.  */\n-\n-static void\n-init_window (int window_num)\n-{\n-  int i;\n-  dispatch_windows *new_list;\n-\n-  if (window_num == 0)\n-    new_list = dispatch_window_list;\n-  else\n-    new_list = dispatch_window_list1;\n-\n-  new_list->num_insn = 0;\n-  new_list->num_uops = 0;\n-  new_list->window_size = 0;\n-  new_list->next = NULL;\n-  new_list->prev = NULL;\n-  new_list->window_num = window_num;\n-  new_list->num_imm = 0;\n-  new_list->num_imm_32 = 0;\n-  new_list->num_imm_64 = 0;\n-  new_list->imm_size = 0;\n-  new_list->num_loads = 0;\n-  new_list->num_stores = 0;\n-  new_list->violation = false;\n-\n-  for (i = 0; i < MAX_INSN; i++)\n-    {\n-      new_list->window[i].insn = NULL;\n-      new_list->window[i].group = disp_no_group;\n-      new_list->window[i].path = no_path;\n-      new_list->window[i].byte_len = 0;\n-      new_list->window[i].imm_bytes = 0;\n-    }\n-  return;\n-}\n-\n-/* This function allocates and initializes a dispatch window and the\n-   list container holding a pointer to the window.  */\n-\n-static dispatch_windows *\n-allocate_window (void)\n-{\n-  dispatch_windows *new_list = XNEW (struct dispatch_windows_s);\n-  new_list->window = XNEWVEC (struct sched_insn_info_s, MAX_INSN + 1);\n-\n-  return new_list;\n-}\n-\n-/* This routine initializes the dispatch scheduling information.  It\n-   initiates building dispatch scheduler tables and constructs the\n-   first dispatch window.  */\n-\n-static void\n-init_dispatch_sched (void)\n-{\n-  /* Allocate a dispatch list and a window.  */\n-  dispatch_window_list = allocate_window ();\n-  dispatch_window_list1 = allocate_window ();\n-  init_window (0);\n-  init_window (1);\n-}\n-\n-/* This function returns true if a branch is detected.  End of a basic block\n-   does not have to be a branch, but here we assume only branches end a\n-   window.  */\n-\n-static bool\n-is_end_basic_block (enum dispatch_group group)\n-{\n-  return group == disp_branch;\n-}\n-\n-/* This function is called when the end of a window processing is reached.  */\n-\n-static void\n-process_end_window (void)\n-{\n-  gcc_assert (dispatch_window_list->num_insn <= MAX_INSN);\n-  if (dispatch_window_list->next)\n-    {\n-      gcc_assert (dispatch_window_list1->num_insn <= MAX_INSN);\n-      gcc_assert (dispatch_window_list->window_size\n-\t\t  + dispatch_window_list1->window_size <= 48);\n-      init_window (1);\n-    }\n-  init_window (0);\n-}\n-\n-/* Allocates a new dispatch window and adds it to WINDOW_LIST.\n-   WINDOW_NUM is either 0 or 1.  A maximum of two windows are generated\n-   for 48 bytes of instructions.  Note that these windows are not dispatch\n-   windows that their sizes are DISPATCH_WINDOW_SIZE.  */\n-\n-static dispatch_windows *\n-allocate_next_window (int window_num)\n-{\n-  if (window_num == 0)\n-    {\n-      if (dispatch_window_list->next)\n-\t  init_window (1);\n-      init_window (0);\n-      return dispatch_window_list;\n-    }\n-\n-  dispatch_window_list->next = dispatch_window_list1;\n-  dispatch_window_list1->prev = dispatch_window_list;\n-\n-  return dispatch_window_list1;\n-}\n-\n-/* Compute number of immediate operands of an instruction.  */\n-\n-static void\n-find_constant (rtx in_rtx, imm_info *imm_values)\n-{\n-  if (INSN_P (in_rtx))\n-    in_rtx = PATTERN (in_rtx);\n-  subrtx_iterator::array_type array;\n-  FOR_EACH_SUBRTX (iter, array, in_rtx, ALL)\n-    if (const_rtx x = *iter)\n-      switch (GET_CODE (x))\n-\t{\n-\tcase CONST:\n-\tcase SYMBOL_REF:\n-\tcase CONST_INT:\n-\t  (imm_values->imm)++;\n-\t  if (x86_64_immediate_operand (CONST_CAST_RTX (x), SImode))\n-\t    (imm_values->imm32)++;\n-\t  else\n-\t    (imm_values->imm64)++;\n-\t  break;\n-\n-\tcase CONST_DOUBLE:\n-\tcase CONST_WIDE_INT:\n-\t  (imm_values->imm)++;\n-\t  (imm_values->imm64)++;\n-\t  break;\n-\n-\tcase CODE_LABEL:\n-\t  if (LABEL_KIND (x) == LABEL_NORMAL)\n-\t    {\n-\t      (imm_values->imm)++;\n-\t      (imm_values->imm32)++;\n-\t    }\n-\t  break;\n-\n-\tdefault:\n-\t  break;\n-\t}\n-}\n-\n-/* Return total size of immediate operands of an instruction along with number\n-   of corresponding immediate-operands.  It initializes its parameters to zero\n-   befor calling FIND_CONSTANT.\n-   INSN is the input instruction.  IMM is the total of immediates.\n-   IMM32 is the number of 32 bit immediates.  IMM64 is the number of 64\n-   bit immediates.  */\n-\n-static int\n-get_num_immediates (rtx_insn *insn, int *imm, int *imm32, int *imm64)\n-{\n-  imm_info imm_values = {0, 0, 0};\n-\n-  find_constant (insn, &imm_values);\n-  *imm = imm_values.imm;\n-  *imm32 = imm_values.imm32;\n-  *imm64 = imm_values.imm64;\n-  return imm_values.imm32 * 4 + imm_values.imm64 * 8;\n-}\n-\n-/* This function indicates if an operand of an instruction is an\n-   immediate.  */\n-\n-static bool\n-has_immediate (rtx_insn *insn)\n-{\n-  int num_imm_operand;\n-  int num_imm32_operand;\n-  int num_imm64_operand;\n-\n-  if (insn)\n-    return get_num_immediates (insn, &num_imm_operand, &num_imm32_operand,\n-\t\t\t       &num_imm64_operand);\n-  return false;\n-}\n-\n-/* Return single or double path for instructions.  */\n-\n-static enum insn_path\n-get_insn_path (rtx_insn *insn)\n-{\n-  enum attr_amdfam10_decode path = get_attr_amdfam10_decode (insn);\n-\n-  if ((int)path == 0)\n-    return path_single;\n-\n-  if ((int)path == 1)\n-    return path_double;\n-\n-  return path_multi;\n-}\n-\n-/* Return insn dispatch group.  */\n-\n-static enum dispatch_group\n-get_insn_group (rtx_insn *insn)\n-{\n-  enum dispatch_group group = get_mem_group (insn);\n-  if (group)\n-    return group;\n-\n-  if (is_branch (insn))\n-    return disp_branch;\n-\n-  if (is_cmp (insn))\n-    return disp_cmp;\n-\n-  if (has_immediate (insn))\n-    return disp_imm;\n-\n-  if (is_prefetch (insn))\n-    return disp_prefetch;\n-\n-  return disp_no_group;\n-}\n-\n-/* Count number of GROUP restricted instructions in a dispatch\n-   window WINDOW_LIST.  */\n-\n-static int\n-count_num_restricted (rtx_insn *insn, dispatch_windows *window_list)\n-{\n-  enum dispatch_group group = get_insn_group (insn);\n-  int imm_size;\n-  int num_imm_operand;\n-  int num_imm32_operand;\n-  int num_imm64_operand;\n-\n-  if (group == disp_no_group)\n-    return 0;\n-\n-  if (group == disp_imm)\n-    {\n-      imm_size = get_num_immediates (insn, &num_imm_operand, &num_imm32_operand,\n-\t\t\t      &num_imm64_operand);\n-      if (window_list->imm_size + imm_size > MAX_IMM_SIZE\n-\t  || num_imm_operand + window_list->num_imm > MAX_IMM\n-\t  || (num_imm32_operand > 0\n-\t      && (window_list->num_imm_32 + num_imm32_operand > MAX_IMM_32\n-\t\t  || window_list->num_imm_64 * 2 + num_imm32_operand > MAX_IMM_32))\n-\t  || (num_imm64_operand > 0\n-\t      && (window_list->num_imm_64 + num_imm64_operand > MAX_IMM_64\n-\t\t  || window_list->num_imm_32 + num_imm64_operand * 2 > MAX_IMM_32))\n-\t  || (window_list->imm_size + imm_size == MAX_IMM_SIZE\n-\t      && num_imm64_operand > 0\n-\t      && ((window_list->num_imm_64 > 0\n-\t\t   && window_list->num_insn >= 2)\n-\t\t  || window_list->num_insn >= 3)))\n-\treturn BIG;\n-\n-      return 1;\n-    }\n-\n-  if ((group == disp_load_store\n-       && (window_list->num_loads >= MAX_LOAD\n-\t   || window_list->num_stores >= MAX_STORE))\n-      || ((group == disp_load\n-\t   || group == disp_prefetch)\n-\t  && window_list->num_loads >= MAX_LOAD)\n-      || (group == disp_store\n-\t  && window_list->num_stores >= MAX_STORE))\n-    return BIG;\n-\n-  return 1;\n-}\n-\n-/* This function returns true if insn satisfies dispatch rules on the\n-   last window scheduled.  */\n-\n-static bool\n-fits_dispatch_window (rtx_insn *insn)\n-{\n-  dispatch_windows *window_list = dispatch_window_list;\n-  dispatch_windows *window_list_next = dispatch_window_list->next;\n-  unsigned int num_restrict;\n-  enum dispatch_group group = get_insn_group (insn);\n-  enum insn_path path = get_insn_path (insn);\n-  int sum;\n-\n-  /* Make disp_cmp and disp_jcc get scheduled at the latest.  These\n-     instructions should be given the lowest priority in the\n-     scheduling process in Haifa scheduler to make sure they will be\n-     scheduled in the same dispatch window as the reference to them.  */\n-  if (group == disp_jcc || group == disp_cmp)\n-    return false;\n-\n-  /* Check nonrestricted.  */\n-  if (group == disp_no_group || group == disp_branch)\n-    return true;\n-\n-  /* Get last dispatch window.  */\n-  if (window_list_next)\n-    window_list = window_list_next;\n-\n-  if (window_list->window_num == 1)\n-    {\n-      sum = window_list->prev->window_size + window_list->window_size;\n-\n-      if (sum == 32\n-\t  || (ix86_min_insn_size (insn) + sum) >= 48)\n-\t/* Window 1 is full.  Go for next window.  */\n-\treturn true;\n-    }\n-\n-  num_restrict = count_num_restricted (insn, window_list);\n-\n-  if (num_restrict > num_allowable_groups[group])\n-    return false;\n-\n-  /* See if it fits in the first window.  */\n-  if (window_list->window_num == 0)\n-    {\n-      /* The first widow should have only single and double path\n-\t uops.  */\n-      if (path == path_double\n-\t  && (window_list->num_uops + 2) > MAX_INSN)\n-\treturn false;\n-      else if (path != path_single)\n-        return false;\n-    }\n-  return true;\n-}\n-\n-/* Add an instruction INSN with NUM_UOPS micro-operations to the\n-   dispatch window WINDOW_LIST.  */\n-\n-static void\n-add_insn_window (rtx_insn *insn, dispatch_windows *window_list, int num_uops)\n-{\n-  int byte_len = ix86_min_insn_size (insn);\n-  int num_insn = window_list->num_insn;\n-  int imm_size;\n-  sched_insn_info *window = window_list->window;\n-  enum dispatch_group group = get_insn_group (insn);\n-  enum insn_path path = get_insn_path (insn);\n-  int num_imm_operand;\n-  int num_imm32_operand;\n-  int num_imm64_operand;\n-\n-  if (!window_list->violation && group != disp_cmp\n-      && !fits_dispatch_window (insn))\n-    window_list->violation = true;\n-\n-  imm_size = get_num_immediates (insn, &num_imm_operand, &num_imm32_operand,\n-\t\t\t\t &num_imm64_operand);\n-\n-  /* Initialize window with new instruction.  */\n-  window[num_insn].insn = insn;\n-  window[num_insn].byte_len = byte_len;\n-  window[num_insn].group = group;\n-  window[num_insn].path = path;\n-  window[num_insn].imm_bytes = imm_size;\n-\n-  window_list->window_size += byte_len;\n-  window_list->num_insn = num_insn + 1;\n-  window_list->num_uops = window_list->num_uops + num_uops;\n-  window_list->imm_size += imm_size;\n-  window_list->num_imm += num_imm_operand;\n-  window_list->num_imm_32 += num_imm32_operand;\n-  window_list->num_imm_64 += num_imm64_operand;\n-\n-  if (group == disp_store)\n-    window_list->num_stores += 1;\n-  else if (group == disp_load\n-\t   || group == disp_prefetch)\n-    window_list->num_loads += 1;\n-  else if (group == disp_load_store)\n-    {\n-      window_list->num_stores += 1;\n-      window_list->num_loads += 1;\n-    }\n-}\n-\n-/* Adds a scheduled instruction, INSN, to the current dispatch window.\n-   If the total bytes of instructions or the number of instructions in\n-   the window exceed allowable, it allocates a new window.  */\n-\n-static void\n-add_to_dispatch_window (rtx_insn *insn)\n-{\n-  int byte_len;\n-  dispatch_windows *window_list;\n-  dispatch_windows *next_list;\n-  dispatch_windows *window0_list;\n-  enum insn_path path;\n-  enum dispatch_group insn_group;\n-  bool insn_fits;\n-  int num_insn;\n-  int num_uops;\n-  int window_num;\n-  int insn_num_uops;\n-  int sum;\n-\n-  if (INSN_CODE (insn) < 0)\n-    return;\n-\n-  byte_len = ix86_min_insn_size (insn);\n-  window_list = dispatch_window_list;\n-  next_list = window_list->next;\n-  path = get_insn_path (insn);\n-  insn_group = get_insn_group (insn);\n-\n-  /* Get the last dispatch window.  */\n-  if (next_list)\n-      window_list = dispatch_window_list->next;\n-\n-  if (path == path_single)\n-    insn_num_uops = 1;\n-  else if (path == path_double)\n-    insn_num_uops = 2;\n-  else\n-    insn_num_uops = (int) path;\n-\n-  /* If current window is full, get a new window.\n-     Window number zero is full, if MAX_INSN uops are scheduled in it.\n-     Window number one is full, if window zero's bytes plus window\n-     one's bytes is 32, or if the bytes of the new instruction added\n-     to the total makes it greater than 48, or it has already MAX_INSN\n-     instructions in it.  */\n-  num_insn = window_list->num_insn;\n-  num_uops = window_list->num_uops;\n-  window_num = window_list->window_num;\n-  insn_fits = fits_dispatch_window (insn);\n-\n-  if (num_insn >= MAX_INSN\n-      || num_uops + insn_num_uops > MAX_INSN\n-      || !(insn_fits))\n-    {\n-      window_num = ~window_num & 1;\n-      window_list = allocate_next_window (window_num);\n-    }\n-\n-  if (window_num == 0)\n-    {\n-      add_insn_window (insn, window_list, insn_num_uops);\n-      if (window_list->num_insn >= MAX_INSN\n-\t  && insn_group == disp_branch)\n-\t{\n-\t  process_end_window ();\n-\t  return;\n-\t}\n-    }\n-  else if (window_num == 1)\n-    {\n-      window0_list = window_list->prev;\n-      sum = window0_list->window_size + window_list->window_size;\n-      if (sum == 32\n-\t  || (byte_len + sum) >= 48)\n-\t{\n-\t  process_end_window ();\n-\t  window_list = dispatch_window_list;\n-\t}\n-\n-      add_insn_window (insn, window_list, insn_num_uops);\n-    }\n-  else\n-    gcc_unreachable ();\n-\n-  if (is_end_basic_block (insn_group))\n-    {\n-      /* End of basic block is reached do end-basic-block process.  */\n-      process_end_window ();\n-      return;\n-    }\n-}\n-\n-/* Print the dispatch window, WINDOW_NUM, to FILE.  */\n-\n-DEBUG_FUNCTION static void\n-debug_dispatch_window_file (FILE *file, int window_num)\n-{\n-  dispatch_windows *list;\n-  int i;\n-\n-  if (window_num == 0)\n-    list = dispatch_window_list;\n-  else\n-    list = dispatch_window_list1;\n-\n-  fprintf (file, \"Window #%d:\\n\", list->window_num);\n-  fprintf (file, \"  num_insn = %d, num_uops = %d, window_size = %d\\n\",\n-\t  list->num_insn, list->num_uops, list->window_size);\n-  fprintf (file, \"  num_imm = %d, num_imm_32 = %d, num_imm_64 = %d, imm_size = %d\\n\",\n-\t   list->num_imm, list->num_imm_32, list->num_imm_64, list->imm_size);\n-\n-  fprintf (file, \"  num_loads = %d, num_stores = %d\\n\", list->num_loads,\n-\t  list->num_stores);\n-  fprintf (file, \" insn info:\\n\");\n-\n-  for (i = 0; i < MAX_INSN; i++)\n-    {\n-      if (!list->window[i].insn)\n-\tbreak;\n-      fprintf (file, \"    group[%d] = %s, insn[%d] = %p, path[%d] = %d byte_len[%d] = %d, imm_bytes[%d] = %d\\n\",\n-\t      i, group_name[list->window[i].group],\n-\t      i, (void *)list->window[i].insn,\n-\t      i, list->window[i].path,\n-\t      i, list->window[i].byte_len,\n-\t      i, list->window[i].imm_bytes);\n-    }\n-}\n-\n-/* Print to stdout a dispatch window.  */\n-\n-DEBUG_FUNCTION void\n-debug_dispatch_window (int window_num)\n-{\n-  debug_dispatch_window_file (stdout, window_num);\n-}\n-\n-/* Print INSN dispatch information to FILE.  */\n-\n-DEBUG_FUNCTION static void\n-debug_insn_dispatch_info_file (FILE *file, rtx_insn *insn)\n-{\n-  int byte_len;\n-  enum insn_path path;\n-  enum dispatch_group group;\n-  int imm_size;\n-  int num_imm_operand;\n-  int num_imm32_operand;\n-  int num_imm64_operand;\n-\n-  if (INSN_CODE (insn) < 0)\n-    return;\n-\n-  byte_len = ix86_min_insn_size (insn);\n-  path = get_insn_path (insn);\n-  group = get_insn_group (insn);\n-  imm_size = get_num_immediates (insn, &num_imm_operand, &num_imm32_operand,\n-\t\t\t\t &num_imm64_operand);\n-\n-  fprintf (file, \" insn info:\\n\");\n-  fprintf (file, \"  group = %s, path = %d, byte_len = %d\\n\",\n-\t   group_name[group], path, byte_len);\n-  fprintf (file, \"  num_imm = %d, num_imm_32 = %d, num_imm_64 = %d, imm_size = %d\\n\",\n-\t   num_imm_operand, num_imm32_operand, num_imm64_operand, imm_size);\n-}\n-\n-/* Print to STDERR the status of the ready list with respect to\n-   dispatch windows.  */\n-\n-DEBUG_FUNCTION void\n-debug_ready_dispatch (void)\n-{\n-  int i;\n-  int no_ready = number_in_ready ();\n-\n-  fprintf (stdout, \"Number of ready: %d\\n\", no_ready);\n-\n-  for (i = 0; i < no_ready; i++)\n-    debug_insn_dispatch_info_file (stdout, get_ready_element (i));\n-}\n-\n-/* This routine is the driver of the dispatch scheduler.  */\n-\n-void\n-ix86_bd_do_dispatch (rtx_insn *insn, int mode)\n-{\n-  if (mode == DISPATCH_INIT)\n-    init_dispatch_sched ();\n-  else if (mode == ADD_TO_DISPATCH_WINDOW)\n-    add_to_dispatch_window (insn);\n-}\n-\n-/* Return TRUE if Dispatch Scheduling is supported.  */\n-\n-bool\n-ix86_bd_has_dispatch (rtx_insn *insn, int action)\n-{\n-  /* Current implementation of dispatch scheduler models buldozer only.  */\n-  if ((TARGET_BDVER1 || TARGET_BDVER2 || TARGET_BDVER3\n-      || TARGET_BDVER4) && flag_dispatch_scheduler)\n-    switch (action)\n-      {\n-      default:\n-\treturn false;\n-\n-      case IS_DISPATCH_ON:\n-\treturn true;\n-\n-      case IS_CMP:\n-\treturn is_cmp (insn);\n-\n-      case DISPATCH_VIOLATION:\n-\treturn dispatch_violation ();\n-\n-      case FITS_DISPATCH_WINDOW:\n-\treturn fits_dispatch_window (insn);\n-      }\n-\n-  return false;\n-}"}, {"sha": "076368c74bd282101c14416e9ad7bd84a700c7da", "filename": "gcc/config/i386/x86-tune-sched-core.c", "status": "removed", "additions": 0, "deletions": 257, "changes": 257, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-core.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-core.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fx86-tune-sched-core.c?ref=566c5f1aaae120d2283103e68ecf1c1a83dd4459", "patch": "@@ -1,257 +0,0 @@\n-/* Scheduler hooks for IA-32 which implement bdver1-4 specific logic.\n-   Copyright (C) 1988-2020 Free Software Foundation, Inc.\n-\n-This file is part of GCC.\n-\n-GCC is free software; you can redistribute it and/or modify\n-it under the terms of the GNU General Public License as published by\n-the Free Software Foundation; either version 3, or (at your option)\n-any later version.\n-\n-GCC is distributed in the hope that it will be useful,\n-but WITHOUT ANY WARRANTY; without even the implied warranty of\n-MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n-GNU General Public License for more details.\n-\n-You should have received a copy of the GNU General Public License\n-along with GCC; see the file COPYING3.  If not see\n-<http://www.gnu.org/licenses/>.  */\n-\n-#define IN_TARGET_CODE 1\n-\n-#include \"config.h\"\n-#include \"system.h\"\n-#include \"coretypes.h\"\n-#include \"backend.h\"\n-#include \"rtl.h\"\n-#include \"tree.h\"\n-#include \"cfghooks.h\"\n-#include \"tm_p.h\"\n-#include \"insn-config.h\"\n-#include \"insn-attr.h\"\n-#include \"recog.h\"\n-#include \"target.h\"\n-#include \"rtl-iter.h\"\n-#include \"regset.h\"\n-#include \"sched-int.h\"\n-\n-\n-/* Model decoder of Core 2/i7.\n-   Below hooks for multipass scheduling (see haifa-sched.c:max_issue)\n-   track the instruction fetch block boundaries and make sure that long\n-   (9+ bytes) instructions are assigned to D0.  */\n-\n-/* Maximum length of an insn that can be handled by\n-   a secondary decoder unit.  '8' for Core 2/i7.  */\n-static int core2i7_secondary_decoder_max_insn_size;\n-\n-/* Ifetch block size, i.e., number of bytes decoder reads per cycle.\n-   '16' for Core 2/i7.  */\n-static int core2i7_ifetch_block_size;\n-\n-/* Maximum number of instructions decoder can handle per cycle.\n-   '6' for Core 2/i7.  */\n-static int core2i7_ifetch_block_max_insns;\n-\n-typedef struct ix86_first_cycle_multipass_data_ *\n-  ix86_first_cycle_multipass_data_t;\n-typedef const struct ix86_first_cycle_multipass_data_ *\n-  const_ix86_first_cycle_multipass_data_t;\n-\n-/* A variable to store target state across calls to max_issue within\n-   one cycle.  */\n-static struct ix86_first_cycle_multipass_data_ _ix86_first_cycle_multipass_data,\n-  *ix86_first_cycle_multipass_data = &_ix86_first_cycle_multipass_data;\n-\n-/* Initialize DATA.  */\n-static void\n-core2i7_first_cycle_multipass_init (void *_data)\n-{\n-  ix86_first_cycle_multipass_data_t data\n-    = (ix86_first_cycle_multipass_data_t) _data;\n-\n-  data->ifetch_block_len = 0;\n-  data->ifetch_block_n_insns = 0;\n-  data->ready_try_change = NULL;\n-  data->ready_try_change_size = 0;\n-}\n-\n-/* Advancing the cycle; reset ifetch block counts.  */\n-static void\n-core2i7_dfa_post_advance_cycle (void)\n-{\n-  ix86_first_cycle_multipass_data_t data = ix86_first_cycle_multipass_data;\n-\n-  gcc_assert (data->ifetch_block_n_insns <= core2i7_ifetch_block_max_insns);\n-\n-  data->ifetch_block_len = 0;\n-  data->ifetch_block_n_insns = 0;\n-}\n-\n-/* Filter out insns from ready_try that the core will not be able to issue\n-   on current cycle due to decoder.  */\n-static void\n-core2i7_first_cycle_multipass_filter_ready_try\n-(const_ix86_first_cycle_multipass_data_t data,\n- signed char *ready_try, int n_ready, bool first_cycle_insn_p)\n-{\n-  while (n_ready--)\n-    {\n-      rtx_insn *insn;\n-      int insn_size;\n-\n-      if (ready_try[n_ready])\n-\tcontinue;\n-\n-      insn = get_ready_element (n_ready);\n-      insn_size = ix86_min_insn_size (insn);\n-\n-      if (/* If this is a too long an insn for a secondary decoder ...  */\n-\t  (!first_cycle_insn_p\n-\t   && insn_size > core2i7_secondary_decoder_max_insn_size)\n-\t  /* ... or it would not fit into the ifetch block ...  */\n-\t  || data->ifetch_block_len + insn_size > core2i7_ifetch_block_size\n-\t  /* ... or the decoder is full already ...  */\n-\t  || data->ifetch_block_n_insns + 1 > core2i7_ifetch_block_max_insns)\n-\t/* ... mask the insn out.  */\n-\t{\n-\t  ready_try[n_ready] = 1;\n-\n-\t  if (data->ready_try_change)\n-\t    bitmap_set_bit (data->ready_try_change, n_ready);\n-\t}\n-    }\n-}\n-\n-/* Prepare for a new round of multipass lookahead scheduling.  */\n-static void\n-core2i7_first_cycle_multipass_begin (void *_data,\n-\t\t\t\t     signed char *ready_try, int n_ready,\n-\t\t\t\t     bool first_cycle_insn_p)\n-{\n-  ix86_first_cycle_multipass_data_t data\n-    = (ix86_first_cycle_multipass_data_t) _data;\n-  const_ix86_first_cycle_multipass_data_t prev_data\n-    = ix86_first_cycle_multipass_data;\n-\n-  /* Restore the state from the end of the previous round.  */\n-  data->ifetch_block_len = prev_data->ifetch_block_len;\n-  data->ifetch_block_n_insns = prev_data->ifetch_block_n_insns;\n-\n-  /* Filter instructions that cannot be issued on current cycle due to\n-     decoder restrictions.  */\n-  core2i7_first_cycle_multipass_filter_ready_try (data, ready_try, n_ready,\n-\t\t\t\t\t\t  first_cycle_insn_p);\n-}\n-\n-/* INSN is being issued in current solution.  Account for its impact on\n-   the decoder model.  */\n-static void\n-core2i7_first_cycle_multipass_issue (void *_data,\n-\t\t\t\t     signed char *ready_try, int n_ready,\n-\t\t\t\t     rtx_insn *insn, const void *_prev_data)\n-{\n-  ix86_first_cycle_multipass_data_t data\n-    = (ix86_first_cycle_multipass_data_t) _data;\n-  const_ix86_first_cycle_multipass_data_t prev_data\n-    = (const_ix86_first_cycle_multipass_data_t) _prev_data;\n-\n-  int insn_size = ix86_min_insn_size (insn);\n-\n-  data->ifetch_block_len = prev_data->ifetch_block_len + insn_size;\n-  data->ifetch_block_n_insns = prev_data->ifetch_block_n_insns + 1;\n-  gcc_assert (data->ifetch_block_len <= core2i7_ifetch_block_size\n-\t      && data->ifetch_block_n_insns <= core2i7_ifetch_block_max_insns);\n-\n-  /* Allocate or resize the bitmap for storing INSN's effect on ready_try.  */\n-  if (!data->ready_try_change)\n-    {\n-      data->ready_try_change = sbitmap_alloc (n_ready);\n-      data->ready_try_change_size = n_ready;\n-    }\n-  else if (data->ready_try_change_size < n_ready)\n-    {\n-      data->ready_try_change = sbitmap_resize (data->ready_try_change,\n-\t\t\t\t\t       n_ready, 0);\n-      data->ready_try_change_size = n_ready;\n-    }\n-  bitmap_clear (data->ready_try_change);\n-\n-  /* Filter out insns from ready_try that the core will not be able to issue\n-     on current cycle due to decoder.  */\n-  core2i7_first_cycle_multipass_filter_ready_try (data, ready_try, n_ready,\n-\t\t\t\t\t\t  false);\n-}\n-\n-/* Revert the effect on ready_try.  */\n-static void\n-core2i7_first_cycle_multipass_backtrack (const void *_data,\n-\t\t\t\t\t signed char *ready_try,\n-\t\t\t\t\t int n_ready ATTRIBUTE_UNUSED)\n-{\n-  const_ix86_first_cycle_multipass_data_t data\n-    = (const_ix86_first_cycle_multipass_data_t) _data;\n-  unsigned int i = 0;\n-  sbitmap_iterator sbi;\n-\n-  gcc_assert (bitmap_last_set_bit (data->ready_try_change) < n_ready);\n-  EXECUTE_IF_SET_IN_BITMAP (data->ready_try_change, 0, i, sbi)\n-    {\n-      ready_try[i] = 0;\n-    }\n-}\n-\n-/* Save the result of multipass lookahead scheduling for the next round.  */\n-static void\n-core2i7_first_cycle_multipass_end (const void *_data)\n-{\n-  const_ix86_first_cycle_multipass_data_t data\n-    = (const_ix86_first_cycle_multipass_data_t) _data;\n-  ix86_first_cycle_multipass_data_t next_data\n-    = ix86_first_cycle_multipass_data;\n-\n-  if (data != NULL)\n-    {\n-      next_data->ifetch_block_len = data->ifetch_block_len;\n-      next_data->ifetch_block_n_insns = data->ifetch_block_n_insns;\n-    }\n-}\n-\n-/* Deallocate target data.  */\n-static void\n-core2i7_first_cycle_multipass_fini (void *_data)\n-{\n-  ix86_first_cycle_multipass_data_t data\n-    = (ix86_first_cycle_multipass_data_t) _data;\n-\n-  if (data->ready_try_change)\n-    {\n-      sbitmap_free (data->ready_try_change);\n-      data->ready_try_change = NULL;\n-      data->ready_try_change_size = 0;\n-    }\n-}\n-\n-void\n-ix86_core2i7_init_hooks (void)\n-{\n-  targetm.sched.dfa_post_advance_cycle\n-    = core2i7_dfa_post_advance_cycle;\n-  targetm.sched.first_cycle_multipass_init\n-    = core2i7_first_cycle_multipass_init;\n-  targetm.sched.first_cycle_multipass_begin\n-    = core2i7_first_cycle_multipass_begin;\n-  targetm.sched.first_cycle_multipass_issue\n-    = core2i7_first_cycle_multipass_issue;\n-  targetm.sched.first_cycle_multipass_backtrack\n-    = core2i7_first_cycle_multipass_backtrack;\n-  targetm.sched.first_cycle_multipass_end\n-    = core2i7_first_cycle_multipass_end;\n-  targetm.sched.first_cycle_multipass_fini\n-    = core2i7_first_cycle_multipass_fini;\n-\n-  /* Set decoder parameters.  */\n-  core2i7_secondary_decoder_max_insn_size = 8;\n-  core2i7_ifetch_block_size = 16;\n-  core2i7_ifetch_block_max_insns = 6;\n-}"}, {"sha": "d4d8a127b4163758d2eece2250c8d7d3db818c1f", "filename": "gcc/config/i386/x86-tune-sched.c", "status": "removed", "additions": 0, "deletions": 636, "changes": 636, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fx86-tune-sched.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/566c5f1aaae120d2283103e68ecf1c1a83dd4459/gcc%2Fconfig%2Fi386%2Fx86-tune-sched.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fx86-tune-sched.c?ref=566c5f1aaae120d2283103e68ecf1c1a83dd4459", "patch": "@@ -1,636 +0,0 @@\n-/* Scheduler hooks for IA-32 which implement CPU specific logic.\n-   Copyright (C) 1988-2020 Free Software Foundation, Inc.\n-\n-This file is part of GCC.\n-\n-GCC is free software; you can redistribute it and/or modify\n-it under the terms of the GNU General Public License as published by\n-the Free Software Foundation; either version 3, or (at your option)\n-any later version.\n-\n-GCC is distributed in the hope that it will be useful,\n-but WITHOUT ANY WARRANTY; without even the implied warranty of\n-MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n-GNU General Public License for more details.\n-\n-You should have received a copy of the GNU General Public License\n-along with GCC; see the file COPYING3.  If not see\n-<http://www.gnu.org/licenses/>.  */\n-\n-#define IN_TARGET_CODE 1\n-\n-#include \"config.h\"\n-#include \"system.h\"\n-#include \"coretypes.h\"\n-#include \"backend.h\"\n-#include \"rtl.h\"\n-#include \"tree.h\"\n-#include \"cfghooks.h\"\n-#include \"tm_p.h\"\n-#include \"target.h\"\n-#include \"insn-config.h\"\n-#include \"insn-attr.h\"\n-#include \"insn-opinit.h\"\n-#include \"recog.h\"\n-\n-/* Return the maximum number of instructions a cpu can issue.  */\n-\n-int\n-ix86_issue_rate (void)\n-{\n-  switch (ix86_tune)\n-    {\n-    case PROCESSOR_PENTIUM:\n-    case PROCESSOR_LAKEMONT:\n-    case PROCESSOR_BONNELL:\n-    case PROCESSOR_SILVERMONT:\n-    case PROCESSOR_KNL:\n-    case PROCESSOR_KNM:\n-    case PROCESSOR_INTEL:\n-    case PROCESSOR_K6:\n-    case PROCESSOR_BTVER2:\n-    case PROCESSOR_PENTIUM4:\n-    case PROCESSOR_NOCONA:\n-      return 2;\n-\n-    case PROCESSOR_PENTIUMPRO:\n-    case PROCESSOR_ATHLON:\n-    case PROCESSOR_K8:\n-    case PROCESSOR_AMDFAM10:\n-    case PROCESSOR_BTVER1:\n-      return 3;\n-\n-    case PROCESSOR_BDVER1:\n-    case PROCESSOR_BDVER2:\n-    case PROCESSOR_BDVER3:\n-    case PROCESSOR_BDVER4:\n-    case PROCESSOR_ZNVER1:\n-    case PROCESSOR_ZNVER2:\n-    case PROCESSOR_CORE2:\n-    case PROCESSOR_NEHALEM:\n-    case PROCESSOR_SANDYBRIDGE:\n-    case PROCESSOR_HASWELL:\n-    case PROCESSOR_GENERIC:\n-      return 4;\n-\n-    default:\n-      return 1;\n-    }\n-}\n-\n-/* Return true iff USE_INSN has a memory address with operands set by\n-   SET_INSN.  */\n-\n-bool\n-ix86_agi_dependent (rtx_insn *set_insn, rtx_insn *use_insn)\n-{\n-  int i;\n-  extract_insn_cached (use_insn);\n-  for (i = recog_data.n_operands - 1; i >= 0; --i)\n-    if (MEM_P (recog_data.operand[i]))\n-      {\n-\trtx addr = XEXP (recog_data.operand[i], 0);\n-\tif (modified_in_p (addr, set_insn) != 0)\n-\t  {\n-\t    /* No AGI stall if SET_INSN is a push or pop and USE_INSN\n-\t       has SP based memory (unless index reg is modified in a pop).  */\n-\t    rtx set = single_set (set_insn);\n-\t    if (set\n-\t\t&& (push_operand (SET_DEST (set), GET_MODE (SET_DEST (set)))\n-\t\t    || pop_operand (SET_SRC (set), GET_MODE (SET_SRC (set)))))\n-\t      {\n-\t\tstruct ix86_address parts;\n-\t\tif (ix86_decompose_address (addr, &parts)\n-\t\t    && parts.base == stack_pointer_rtx\n-\t\t    && (parts.index == NULL_RTX\n-\t\t\t|| MEM_P (SET_DEST (set))\n-\t\t\t|| !modified_in_p (parts.index, set_insn)))\n-\t\t  return false;\n-\t      }\n-\t    return true;\n-\t  }\n-\treturn false;\n-      }\n-  return false;\n-}\n-\n-/* A subroutine of ix86_adjust_cost -- return TRUE iff INSN reads flags set\n-   by DEP_INSN and nothing set by DEP_INSN.  */\n-\n-static bool\n-ix86_flags_dependent (rtx_insn *insn, rtx_insn *dep_insn, enum attr_type insn_type)\n-{\n-  rtx set, set2;\n-\n-  /* Simplify the test for uninteresting insns.  */\n-  if (insn_type != TYPE_SETCC\n-      && insn_type != TYPE_ICMOV\n-      && insn_type != TYPE_FCMOV\n-      && insn_type != TYPE_IBR)\n-    return false;\n-\n-  if ((set = single_set (dep_insn)) != 0)\n-    {\n-      set = SET_DEST (set);\n-      set2 = NULL_RTX;\n-    }\n-  else if (GET_CODE (PATTERN (dep_insn)) == PARALLEL\n-\t   && XVECLEN (PATTERN (dep_insn), 0) == 2\n-\t   && GET_CODE (XVECEXP (PATTERN (dep_insn), 0, 0)) == SET\n-\t   && GET_CODE (XVECEXP (PATTERN (dep_insn), 0, 1)) == SET)\n-    {\n-      set = SET_DEST (XVECEXP (PATTERN (dep_insn), 0, 0));\n-      set2 = SET_DEST (XVECEXP (PATTERN (dep_insn), 0, 0));\n-    }\n-  else\n-    return false;\n-\n-  if (!REG_P (set) || REGNO (set) != FLAGS_REG)\n-    return false;\n-\n-  /* This test is true if the dependent insn reads the flags but\n-     not any other potentially set register.  */\n-  if (!reg_overlap_mentioned_p (set, PATTERN (insn)))\n-    return false;\n-\n-  if (set2 && reg_overlap_mentioned_p (set2, PATTERN (insn)))\n-    return false;\n-\n-  return true;\n-}\n-\n-/* Helper function for exact_store_load_dependency.\n-   Return true if addr is found in insn.  */\n-static bool\n-exact_dependency_1 (rtx addr, rtx insn)\n-{\n-  enum rtx_code code;\n-  const char *format_ptr;\n-  int i, j;\n-\n-  code = GET_CODE (insn);\n-  switch (code)\n-    {\n-    case MEM:\n-      if (rtx_equal_p (addr, insn))\n-\treturn true;\n-      break;\n-    case REG:\n-    CASE_CONST_ANY:\n-    case SYMBOL_REF:\n-    case CODE_LABEL:\n-    case PC:\n-    case CC0:\n-    case EXPR_LIST:\n-      return false;\n-    default:\n-      break;\n-    }\n-\n-  format_ptr = GET_RTX_FORMAT (code);\n-  for (i = 0; i < GET_RTX_LENGTH (code); i++)\n-    {\n-      switch (*format_ptr++)\n-\t{\n-\tcase 'e':\n-\t  if (exact_dependency_1 (addr, XEXP (insn, i)))\n-\t    return true;\n-\t  break;\n-\tcase 'E':\n-\t  for (j = 0; j < XVECLEN (insn, i); j++)\n-\t    if (exact_dependency_1 (addr, XVECEXP (insn, i, j)))\n-\t      return true;\n-\t  break;\n-\t}\n-    }\n-  return false;\n-}\n-\n-/* Return true if there exists exact dependency for store & load, i.e.\n-   the same memory address is used in them.  */\n-static bool\n-exact_store_load_dependency (rtx_insn *store, rtx_insn *load)\n-{\n-  rtx set1, set2;\n-\n-  set1 = single_set (store);\n-  if (!set1)\n-    return false;\n-  if (!MEM_P (SET_DEST (set1)))\n-    return false;\n-  set2 = single_set (load);\n-  if (!set2)\n-    return false;\n-  if (exact_dependency_1 (SET_DEST (set1), SET_SRC (set2)))\n-    return true;\n-  return false;\n-}\n-\n-\n-/* This function corrects the value of COST (latency) based on the relationship\n-   between INSN and DEP_INSN through a dependence of type DEP_TYPE, and strength\n-   DW.  It should return the new value.\n-\n-   On x86 CPUs this is most commonly used to model the fact that valus of\n-   registers used to compute address of memory operand  needs to be ready\n-   earlier than values of registers used in the actual operation.  */\n-\n-int\n-ix86_adjust_cost (rtx_insn *insn, int dep_type, rtx_insn *dep_insn, int cost,\n-\t\t  unsigned int)\n-{\n-  enum attr_type insn_type, dep_insn_type;\n-  enum attr_memory memory;\n-  rtx set, set2;\n-  int dep_insn_code_number;\n-\n-  /* Anti and output dependencies have zero cost on all CPUs.  */\n-  if (dep_type != 0)\n-    return 0;\n-\n-  dep_insn_code_number = recog_memoized (dep_insn);\n-\n-  /* If we can't recognize the insns, we can't really do anything.  */\n-  if (dep_insn_code_number < 0 || recog_memoized (insn) < 0)\n-    return cost;\n-\n-  insn_type = get_attr_type (insn);\n-  dep_insn_type = get_attr_type (dep_insn);\n-\n-  switch (ix86_tune)\n-    {\n-    case PROCESSOR_PENTIUM:\n-    case PROCESSOR_LAKEMONT:\n-      /* Address Generation Interlock adds a cycle of latency.  */\n-      if (insn_type == TYPE_LEA)\n-\t{\n-\t  rtx addr = PATTERN (insn);\n-\n-\t  if (GET_CODE (addr) == PARALLEL)\n-\t    addr = XVECEXP (addr, 0, 0);\n-\n-\t  gcc_assert (GET_CODE (addr) == SET);\n-\n-\t  addr = SET_SRC (addr);\n-\t  if (modified_in_p (addr, dep_insn))\n-\t    cost += 1;\n-\t}\n-      else if (ix86_agi_dependent (dep_insn, insn))\n-\tcost += 1;\n-\n-      /* ??? Compares pair with jump/setcc.  */\n-      if (ix86_flags_dependent (insn, dep_insn, insn_type))\n-\tcost = 0;\n-\n-      /* Floating point stores require value to be ready one cycle earlier.  */\n-      if (insn_type == TYPE_FMOV\n-\t  && get_attr_memory (insn) == MEMORY_STORE\n-\t  && !ix86_agi_dependent (dep_insn, insn))\n-\tcost += 1;\n-      break;\n-\n-    case PROCESSOR_PENTIUMPRO:\n-      /* INT->FP conversion is expensive.  */\n-      if (get_attr_fp_int_src (dep_insn))\n-\tcost += 5;\n-\n-      /* There is one cycle extra latency between an FP op and a store.  */\n-      if (insn_type == TYPE_FMOV\n-\t  && (set = single_set (dep_insn)) != NULL_RTX\n-\t  && (set2 = single_set (insn)) != NULL_RTX\n-\t  && rtx_equal_p (SET_DEST (set), SET_SRC (set2))\n-\t  && MEM_P (SET_DEST (set2)))\n-\tcost += 1;\n-\n-      memory = get_attr_memory (insn);\n-\n-      /* Show ability of reorder buffer to hide latency of load by executing\n-\t in parallel with previous instruction in case\n-\t previous instruction is not needed to compute the address.  */\n-      if ((memory == MEMORY_LOAD || memory == MEMORY_BOTH)\n-\t  && !ix86_agi_dependent (dep_insn, insn))\n-\t{\n-\t  /* Claim moves to take one cycle, as core can issue one load\n-\t     at time and the next load can start cycle later.  */\n-\t  if (dep_insn_type == TYPE_IMOV\n-\t      || dep_insn_type == TYPE_FMOV)\n-\t    cost = 1;\n-\t  else if (cost > 1)\n-\t    cost--;\n-\t}\n-      break;\n-\n-    case PROCESSOR_K6:\n-     /* The esp dependency is resolved before\n-\tthe instruction is really finished.  */\n-      if ((insn_type == TYPE_PUSH || insn_type == TYPE_POP)\n-\t  && (dep_insn_type == TYPE_PUSH || dep_insn_type == TYPE_POP))\n-\treturn 1;\n-\n-      /* INT->FP conversion is expensive.  */\n-      if (get_attr_fp_int_src (dep_insn))\n-\tcost += 5;\n-\n-      memory = get_attr_memory (insn);\n-\n-      /* Show ability of reorder buffer to hide latency of load by executing\n-\t in parallel with previous instruction in case\n-\t previous instruction is not needed to compute the address.  */\n-      if ((memory == MEMORY_LOAD || memory == MEMORY_BOTH)\n-\t  && !ix86_agi_dependent (dep_insn, insn))\n-\t{\n-\t  /* Claim moves to take one cycle, as core can issue one load\n-\t     at time and the next load can start cycle later.  */\n-\t  if (dep_insn_type == TYPE_IMOV\n-\t      || dep_insn_type == TYPE_FMOV)\n-\t    cost = 1;\n-\t  else if (cost > 2)\n-\t    cost -= 2;\n-\t  else\n-\t    cost = 1;\n-\t}\n-      break;\n-\n-    case PROCESSOR_AMDFAM10:\n-    case PROCESSOR_BDVER1:\n-    case PROCESSOR_BDVER2:\n-    case PROCESSOR_BDVER3:\n-    case PROCESSOR_BDVER4:\n-    case PROCESSOR_BTVER1:\n-    case PROCESSOR_BTVER2:\n-      /* Stack engine allows to execute push&pop instructions in parall.  */\n-      if ((insn_type == TYPE_PUSH || insn_type == TYPE_POP)\n-\t  && (dep_insn_type == TYPE_PUSH || dep_insn_type == TYPE_POP))\n-\treturn 0;\n-      /* FALLTHRU */\n-\n-    case PROCESSOR_ATHLON:\n-    case PROCESSOR_K8:\n-      memory = get_attr_memory (insn);\n-\n-      /* Show ability of reorder buffer to hide latency of load by executing\n-\t in parallel with previous instruction in case\n-\t previous instruction is not needed to compute the address.  */\n-      if ((memory == MEMORY_LOAD || memory == MEMORY_BOTH)\n-\t  && !ix86_agi_dependent (dep_insn, insn))\n-\t{\n-\t  enum attr_unit unit = get_attr_unit (insn);\n-\t  int loadcost = 3;\n-\n-\t  /* Because of the difference between the length of integer and\n-\t     floating unit pipeline preparation stages, the memory operands\n-\t     for floating point are cheaper.\n-\n-\t     ??? For Athlon it the difference is most probably 2.  */\n-\t  if (unit == UNIT_INTEGER || unit == UNIT_UNKNOWN)\n-\t    loadcost = 3;\n-\t  else\n-\t    loadcost = TARGET_ATHLON ? 2 : 0;\n-\n-\t  if (cost >= loadcost)\n-\t    cost -= loadcost;\n-\t  else\n-\t    cost = 0;\n-\t}\n-      break;\n-\n-    case PROCESSOR_ZNVER1:\n-    case PROCESSOR_ZNVER2:\n-      /* Stack engine allows to execute push&pop instructions in parall.  */\n-      if ((insn_type == TYPE_PUSH || insn_type == TYPE_POP)\n-\t  && (dep_insn_type == TYPE_PUSH || dep_insn_type == TYPE_POP))\n-\treturn 0;\n-\n-      memory = get_attr_memory (insn);\n-\n-      /* Show ability of reorder buffer to hide latency of load by executing\n-\t in parallel with previous instruction in case\n-\t previous instruction is not needed to compute the address.  */\n-      if ((memory == MEMORY_LOAD || memory == MEMORY_BOTH)\n-\t  && !ix86_agi_dependent (dep_insn, insn))\n-\t{\n-\t  enum attr_unit unit = get_attr_unit (insn);\n-\t  int loadcost;\n-\n-\t  if (unit == UNIT_INTEGER || unit == UNIT_UNKNOWN)\n-\t    loadcost = 4;\n-\t  else\n-\t    loadcost = 7;\n-\n-\t  if (cost >= loadcost)\n-\t    cost -= loadcost;\n-\t  else\n-\t    cost = 0;\n-\t}\n-      break;\n-\n-    case PROCESSOR_CORE2:\n-    case PROCESSOR_NEHALEM:\n-    case PROCESSOR_SANDYBRIDGE:\n-    case PROCESSOR_HASWELL:\n-    case PROCESSOR_GENERIC:\n-      /* Stack engine allows to execute push&pop instructions in parall.  */\n-      if ((insn_type == TYPE_PUSH || insn_type == TYPE_POP)\n-\t  && (dep_insn_type == TYPE_PUSH || dep_insn_type == TYPE_POP))\n-\treturn 0;\n-\n-      memory = get_attr_memory (insn);\n-\n-      /* Show ability of reorder buffer to hide latency of load by executing\n-\t in parallel with previous instruction in case\n-\t previous instruction is not needed to compute the address.  */\n-      if ((memory == MEMORY_LOAD || memory == MEMORY_BOTH)\n-\t  && !ix86_agi_dependent (dep_insn, insn))\n-\t{\n-\t  if (cost >= 4)\n-\t    cost -= 4;\n-\t  else\n-\t    cost = 0;\n-\t}\n-      break;\n-\n-    case PROCESSOR_SILVERMONT:\n-    case PROCESSOR_KNL:\n-    case PROCESSOR_KNM:\n-    case PROCESSOR_INTEL:\n-      if (!reload_completed)\n-\treturn cost;\n-\n-      /* Increase cost of integer loads.  */\n-      memory = get_attr_memory (dep_insn);\n-      if (memory == MEMORY_LOAD || memory == MEMORY_BOTH)\n-\t{\n-\t  enum attr_unit unit = get_attr_unit (dep_insn);\n-\t  if (unit == UNIT_INTEGER && cost == 1)\n-\t    {\n-\t      if (memory == MEMORY_LOAD)\n-\t\tcost = 3;\n-\t      else\n-\t\t{\n-\t\t  /* Increase cost of ld/st for short int types only\n-\t\t     because of store forwarding issue.  */\n-\t\t  rtx set = single_set (dep_insn);\n-\t\t  if (set && (GET_MODE (SET_DEST (set)) == QImode\n-\t\t\t      || GET_MODE (SET_DEST (set)) == HImode))\n-\t\t    {\n-\t\t      /* Increase cost of store/load insn if exact\n-\t\t\t dependence exists and it is load insn.  */\n-\t\t      enum attr_memory insn_memory = get_attr_memory (insn);\n-\t\t      if (insn_memory == MEMORY_LOAD\n-\t\t\t  && exact_store_load_dependency (dep_insn, insn))\n-\t\t\tcost = 3;\n-\t\t    }\n-\t\t}\n-\t    }\n-\t}\n-\n-    default:\n-      break;\n-    }\n-\n-  return cost;\n-}\n-\n-/* How many alternative schedules to try.  This should be as wide as the\n-   scheduling freedom in the DFA, but no wider.  Making this value too\n-   large results extra work for the scheduler.  */\n-\n-int\n-ia32_multipass_dfa_lookahead (void)\n-{\n-  /* Generally, we want haifa-sched:max_issue() to look ahead as far\n-     as many instructions can be executed on a cycle, i.e.,\n-     issue_rate.  */\n-  if (reload_completed)\n-    return ix86_issue_rate ();\n-  /* Don't use lookahead for pre-reload schedule to save compile time.  */\n-  return 0;\n-}\n-\n-/* Return true if target platform supports macro-fusion.  */\n-\n-bool\n-ix86_macro_fusion_p ()\n-{\n-  return TARGET_FUSE_CMP_AND_BRANCH;\n-}\n-\n-/* Check whether current microarchitecture support macro fusion\n-   for insn pair \"CONDGEN + CONDJMP\". Refer to\n-   \"Intel Architectures Optimization Reference Manual\". */\n-\n-bool\n-ix86_macro_fusion_pair_p (rtx_insn *condgen, rtx_insn *condjmp)\n-{\n-  rtx src, dest;\n-  enum rtx_code ccode;\n-  rtx compare_set = NULL_RTX, test_if, cond;\n-  rtx alu_set = NULL_RTX, addr = NULL_RTX;\n-  enum attr_type condgen_type;\n-\n-  if (!any_condjump_p (condjmp))\n-    return false;\n-\n-  unsigned int condreg1, condreg2;\n-  rtx cc_reg_1;\n-  targetm.fixed_condition_code_regs (&condreg1, &condreg2);\n-  cc_reg_1 = gen_rtx_REG (CCmode, condreg1);\n-  if (!reg_referenced_p (cc_reg_1, PATTERN (condjmp))\n-      || !condgen\n-      || !modified_in_p (cc_reg_1, condgen))\n-    return false;\n-\n-  condgen_type = get_attr_type (condgen);\n-  if (condgen_type == TYPE_MULTI\n-      && INSN_CODE (condgen) == code_for_stack_protect_test_1 (ptr_mode)\n-      && TARGET_FUSE_ALU_AND_BRANCH)\n-    {\n-      /* stack_protect_test_<mode> ends with a sub, which subtracts\n-\t a non-rip special memory operand from a GPR.  */\n-      src = NULL_RTX;\n-      alu_set = XVECEXP (PATTERN (condgen), 0, 1);\n-      goto handle_stack_protect_test;\n-    }\n-  else if (condgen_type != TYPE_TEST\n-\t   && condgen_type != TYPE_ICMP\n-\t   && condgen_type != TYPE_INCDEC\n-\t   && condgen_type != TYPE_ALU)\n-    return false;\n-\n-  compare_set = single_set (condgen);\n-  if (compare_set == NULL_RTX && !TARGET_FUSE_ALU_AND_BRANCH)\n-    return false;\n-\n-  if (compare_set == NULL_RTX)\n-    {\n-      int i;\n-      rtx pat = PATTERN (condgen);\n-      for (i = 0; i < XVECLEN (pat, 0); i++)\n-\tif (GET_CODE (XVECEXP (pat, 0, i)) == SET)\n-\t  {\n-\t    rtx set_src = SET_SRC (XVECEXP (pat, 0, i));\n-\t    if (GET_CODE (set_src) == COMPARE)\n-\t      compare_set = XVECEXP (pat, 0, i);\n-\t    else\n-\t      alu_set = XVECEXP (pat, 0, i);\n-\t  }\n-    }\n-  if (compare_set == NULL_RTX)\n-    return false;\n-  src = SET_SRC (compare_set);\n-  if (GET_CODE (src) != COMPARE)\n-    return false;\n-\n-  /* Macro-fusion for cmp/test MEM-IMM + conditional jmp is not\n-     supported.  */\n-  if ((MEM_P (XEXP (src, 0)) && CONST_INT_P (XEXP (src, 1)))\n-      || (MEM_P (XEXP (src, 1)) && CONST_INT_P (XEXP (src, 0))))\n-    return false;\n-\n-  /* No fusion for RIP-relative address.  */\n-  if (MEM_P (XEXP (src, 0)))\n-    addr = XEXP (XEXP (src, 0), 0);\n-  else if (MEM_P (XEXP (src, 1)))\n-    addr = XEXP (XEXP (src, 1), 0);\n-\n-  if (addr)\n-    {\n-      ix86_address parts;\n-      int ok = ix86_decompose_address (addr, &parts);\n-      gcc_assert (ok);\n-\n-      if (ix86_rip_relative_addr_p (&parts))\n-\treturn false;\n-    }\n-\n- handle_stack_protect_test:\n-  test_if = SET_SRC (pc_set (condjmp));\n-  cond = XEXP (test_if, 0);\n-  ccode = GET_CODE (cond);\n-  /* Check whether conditional jump use Sign or Overflow Flags.  */\n-  if (!TARGET_FUSE_CMP_AND_BRANCH_SOFLAGS\n-      && (ccode == GE || ccode == GT || ccode == LE || ccode == LT))\n-    return false;\n-\n-  /* Return true for TYPE_TEST and TYPE_ICMP.  */\n-  if (condgen_type == TYPE_TEST || condgen_type == TYPE_ICMP)\n-    return true;\n-\n-  /* The following is the case that macro-fusion for alu + jmp.  */\n-  if (!TARGET_FUSE_ALU_AND_BRANCH || !alu_set)\n-    return false;\n-\n-  /* No fusion for alu op with memory destination operand.  */\n-  dest = SET_DEST (alu_set);\n-  if (MEM_P (dest))\n-    return false;\n-\n-  /* Macro-fusion for inc/dec + unsigned conditional jump is not\n-     supported.  */\n-  if (condgen_type == TYPE_INCDEC\n-      && (ccode == GEU || ccode == GTU || ccode == LEU || ccode == LTU))\n-    return false;\n-\n-  return true;\n-}\n-"}]}