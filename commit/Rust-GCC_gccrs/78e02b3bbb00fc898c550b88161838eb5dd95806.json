{"sha": "78e02b3bbb00fc898c550b88161838eb5dd95806", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NzhlMDJiM2JiYjAwZmM4OThjNTUwYjg4MTYxODM4ZWI1ZGQ5NTgwNg==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2018-07-31T14:23:25Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2018-07-31T14:23:25Z"}, "message": "[19/46] Make vect_dr_stmt return a stmt_vec_info\n\nThis patch makes vect_dr_stmt return a stmt_vec_info instead of a\ngimple stmt.  Rather than retain a separate gimple stmt variable\nin cases where both existed, the patch replaces uses of the gimple\nvariable with the uses of the stmt_vec_info.  Later patches do this\nmore generally.\n\nMany things that are keyed off a data_reference would these days\nbe better keyed off a stmt_vec_info, but it's more convenient\nto do that later in the series.  The vect_dr_size calls that are\nleft over do still benefit from this patch.\n\n2018-07-31  Richard Sandiford  <richard.sandiford@arm.com>\n\ngcc/\n\t* tree-vectorizer.h (vect_dr_stmt): Return a stmt_vec_info rather\n\tthan a gimple stmt.\n\t* tree-vect-data-refs.c (vect_analyze_data_ref_dependence)\n\t(vect_slp_analyze_data_ref_dependence, vect_record_base_alignments)\n\t(vect_calculate_target_alignmentm, vect_compute_data_ref_alignment)\n\t(vect_update_misalignment_for_peel, vect_verify_datarefs_alignment)\n\t(vector_alignment_reachable_p, vect_get_data_access_cost)\n\t(vect_get_peeling_costs_all_drs, vect_peeling_hash_get_lowest_cost)\n\t(vect_peeling_supportable, vect_enhance_data_refs_alignment)\n\t(vect_find_same_alignment_drs, vect_analyze_data_refs_alignment)\n\t(vect_analyze_group_access_1, vect_analyze_group_access)\n\t(vect_analyze_data_ref_access, vect_analyze_data_ref_accesses)\n\t(vect_vfa_access_size, vect_small_gap_p, vect_analyze_data_refs)\n\t(vect_supportable_dr_alignment): Remove vinfo_for_stmt from the\n\tresult of vect_dr_stmt and use the stmt_vec_info instead of\n\tthe associated gimple stmt.\n\t* tree-vect-loop-manip.c (get_misalign_in_elems): Likewise.\n\t(vect_gen_prolog_loop_niters): Likewise.\n\t* tree-vect-loop.c (vect_analyze_loop_2): Likewise.\n\nFrom-SVN: r263134", "tree": {"sha": "b8c2aa33b7aef2b96b2f17210fec46a79c3dabc3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/b8c2aa33b7aef2b96b2f17210fec46a79c3dabc3"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/78e02b3bbb00fc898c550b88161838eb5dd95806", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/78e02b3bbb00fc898c550b88161838eb5dd95806", "html_url": "https://github.com/Rust-GCC/gccrs/commit/78e02b3bbb00fc898c550b88161838eb5dd95806", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/78e02b3bbb00fc898c550b88161838eb5dd95806/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "b978758186fa187d52d2c4a02cdc8474d361a0dd", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b978758186fa187d52d2c4a02cdc8474d361a0dd", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b978758186fa187d52d2c4a02cdc8474d361a0dd"}], "stats": {"total": 277, "additions": 139, "deletions": 138}, "files": [{"sha": "fed88c304fb2f58ca1279f835d87e8b8c135a90e", "filename": "gcc/ChangeLog", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/78e02b3bbb00fc898c550b88161838eb5dd95806/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/78e02b3bbb00fc898c550b88161838eb5dd95806/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=78e02b3bbb00fc898c550b88161838eb5dd95806", "patch": "@@ -1,3 +1,25 @@\n+2018-07-31  Richard Sandiford  <richard.sandiford@arm.com>\n+\n+\t* tree-vectorizer.h (vect_dr_stmt): Return a stmt_vec_info rather\n+\tthan a gimple stmt.\n+\t* tree-vect-data-refs.c (vect_analyze_data_ref_dependence)\n+\t(vect_slp_analyze_data_ref_dependence, vect_record_base_alignments)\n+\t(vect_calculate_target_alignmentm, vect_compute_data_ref_alignment)\n+\t(vect_update_misalignment_for_peel, vect_verify_datarefs_alignment)\n+\t(vector_alignment_reachable_p, vect_get_data_access_cost)\n+\t(vect_get_peeling_costs_all_drs, vect_peeling_hash_get_lowest_cost)\n+\t(vect_peeling_supportable, vect_enhance_data_refs_alignment)\n+\t(vect_find_same_alignment_drs, vect_analyze_data_refs_alignment)\n+\t(vect_analyze_group_access_1, vect_analyze_group_access)\n+\t(vect_analyze_data_ref_access, vect_analyze_data_ref_accesses)\n+\t(vect_vfa_access_size, vect_small_gap_p, vect_analyze_data_refs)\n+\t(vect_supportable_dr_alignment): Remove vinfo_for_stmt from the\n+\tresult of vect_dr_stmt and use the stmt_vec_info instead of\n+\tthe associated gimple stmt.\n+\t* tree-vect-loop-manip.c (get_misalign_in_elems): Likewise.\n+\t(vect_gen_prolog_loop_niters): Likewise.\n+\t* tree-vect-loop.c (vect_analyze_loop_2): Likewise.\n+\n 2018-07-31  Richard Sandiford  <richard.sandiford@arm.com>\n \n \t* tree-vectorizer.h (_slp_tree::stmts): Change from a vec<gimple *>"}, {"sha": "72465fd081773f24d1cd08ba3a79cacf5c25989f", "filename": "gcc/tree-vect-data-refs.c", "status": "modified", "additions": 111, "deletions": 129, "changes": 240, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/78e02b3bbb00fc898c550b88161838eb5dd95806/gcc%2Ftree-vect-data-refs.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/78e02b3bbb00fc898c550b88161838eb5dd95806/gcc%2Ftree-vect-data-refs.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-data-refs.c?ref=78e02b3bbb00fc898c550b88161838eb5dd95806", "patch": "@@ -294,8 +294,8 @@ vect_analyze_data_ref_dependence (struct data_dependence_relation *ddr,\n   struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n   struct data_reference *dra = DDR_A (ddr);\n   struct data_reference *drb = DDR_B (ddr);\n-  stmt_vec_info stmtinfo_a = vinfo_for_stmt (vect_dr_stmt (dra));\n-  stmt_vec_info stmtinfo_b = vinfo_for_stmt (vect_dr_stmt (drb));\n+  stmt_vec_info stmtinfo_a = vect_dr_stmt (dra);\n+  stmt_vec_info stmtinfo_b = vect_dr_stmt (drb);\n   lambda_vector dist_v;\n   unsigned int loop_depth;\n \n@@ -627,9 +627,9 @@ vect_slp_analyze_data_ref_dependence (struct data_dependence_relation *ddr)\n \n   /* If dra and drb are part of the same interleaving chain consider\n      them independent.  */\n-  if (STMT_VINFO_GROUPED_ACCESS (vinfo_for_stmt (vect_dr_stmt (dra)))\n-      && (DR_GROUP_FIRST_ELEMENT (vinfo_for_stmt (vect_dr_stmt (dra)))\n-\t  == DR_GROUP_FIRST_ELEMENT (vinfo_for_stmt (vect_dr_stmt (drb)))))\n+  if (STMT_VINFO_GROUPED_ACCESS (vect_dr_stmt (dra))\n+      && (DR_GROUP_FIRST_ELEMENT (vect_dr_stmt (dra))\n+\t  == DR_GROUP_FIRST_ELEMENT (vect_dr_stmt (drb))))\n     return false;\n \n   /* Unknown data dependence.  */\n@@ -841,19 +841,18 @@ vect_record_base_alignments (vec_info *vinfo)\n   unsigned int i;\n   FOR_EACH_VEC_ELT (vinfo->shared->datarefs, i, dr)\n     {\n-      gimple *stmt = vect_dr_stmt (dr);\n-      stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+      stmt_vec_info stmt_info = vect_dr_stmt (dr);\n       if (!DR_IS_CONDITIONAL_IN_STMT (dr)\n \t  && STMT_VINFO_VECTORIZABLE (stmt_info)\n \t  && !STMT_VINFO_GATHER_SCATTER_P (stmt_info))\n \t{\n-\t  vect_record_base_alignment (vinfo, stmt, &DR_INNERMOST (dr));\n+\t  vect_record_base_alignment (vinfo, stmt_info, &DR_INNERMOST (dr));\n \n \t  /* If DR is nested in the loop that is being vectorized, we can also\n \t     record the alignment of the base wrt the outer loop.  */\n-\t  if (loop && nested_in_vect_loop_p (loop, stmt))\n+\t  if (loop && nested_in_vect_loop_p (loop, stmt_info))\n \t    vect_record_base_alignment\n-\t\t(vinfo, stmt, &STMT_VINFO_DR_WRT_VEC_LOOP (stmt_info));\n+\t\t(vinfo, stmt_info, &STMT_VINFO_DR_WRT_VEC_LOOP (stmt_info));\n \t}\n     }\n }\n@@ -863,8 +862,7 @@ vect_record_base_alignments (vec_info *vinfo)\n static unsigned int\n vect_calculate_target_alignment (struct data_reference *dr)\n {\n-  gimple *stmt = vect_dr_stmt (dr);\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n   tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n   return targetm.vectorize.preferred_vector_alignment (vectype);\n }\n@@ -882,8 +880,7 @@ vect_calculate_target_alignment (struct data_reference *dr)\n static void\n vect_compute_data_ref_alignment (struct data_reference *dr)\n {\n-  gimple *stmt = vect_dr_stmt (dr);\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n   vec_base_alignments *base_alignments = &stmt_info->vinfo->base_alignments;\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   struct loop *loop = NULL;\n@@ -923,7 +920,7 @@ vect_compute_data_ref_alignment (struct data_reference *dr)\n      stays the same throughout the execution of the inner-loop, which is why\n      we have to check that the stride of the dataref in the inner-loop evenly\n      divides by the vector alignment.  */\n-  else if (nested_in_vect_loop_p (loop, stmt))\n+  else if (nested_in_vect_loop_p (loop, stmt_info))\n     {\n       step_preserves_misalignment_p\n \t= (DR_STEP_ALIGNMENT (dr) % vector_alignment) == 0;\n@@ -1074,8 +1071,8 @@ vect_update_misalignment_for_peel (struct data_reference *dr,\n   struct data_reference *current_dr;\n   int dr_size = vect_get_scalar_dr_size (dr);\n   int dr_peel_size = vect_get_scalar_dr_size (dr_peel);\n-  stmt_vec_info stmt_info = vinfo_for_stmt (vect_dr_stmt (dr));\n-  stmt_vec_info peel_stmt_info = vinfo_for_stmt (vect_dr_stmt (dr_peel));\n+  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+  stmt_vec_info peel_stmt_info = vect_dr_stmt (dr_peel);\n \n  /* For interleaved data accesses the step in the loop must be multiplied by\n      the size of the interleaving group.  */\n@@ -1086,8 +1083,7 @@ vect_update_misalignment_for_peel (struct data_reference *dr,\n \n   /* It can be assumed that the data refs with the same alignment as dr_peel\n      are aligned in the vector loop.  */\n-  same_aligned_drs\n-    = STMT_VINFO_SAME_ALIGN_REFS (vinfo_for_stmt (vect_dr_stmt (dr_peel)));\n+  same_aligned_drs = STMT_VINFO_SAME_ALIGN_REFS (vect_dr_stmt (dr_peel));\n   FOR_EACH_VEC_ELT (same_aligned_drs, i, current_dr)\n     {\n       if (current_dr != dr)\n@@ -1167,15 +1163,14 @@ vect_verify_datarefs_alignment (loop_vec_info vinfo)\n \n   FOR_EACH_VEC_ELT (datarefs, i, dr)\n     {\n-      gimple *stmt = vect_dr_stmt (dr);\n-      stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+      stmt_vec_info stmt_info = vect_dr_stmt (dr);\n \n       if (!STMT_VINFO_RELEVANT_P (stmt_info))\n \tcontinue;\n \n       /* For interleaving, only the alignment of the first access matters.   */\n       if (STMT_VINFO_GROUPED_ACCESS (stmt_info)\n-\t  && DR_GROUP_FIRST_ELEMENT (stmt_info) != stmt)\n+\t  && DR_GROUP_FIRST_ELEMENT (stmt_info) != stmt_info)\n \tcontinue;\n \n       /* Strided accesses perform only component accesses, alignment is\n@@ -1212,8 +1207,7 @@ not_size_aligned (tree exp)\n static bool\n vector_alignment_reachable_p (struct data_reference *dr)\n {\n-  gimple *stmt = vect_dr_stmt (dr);\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n   tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n \n   if (STMT_VINFO_GROUPED_ACCESS (stmt_info))\n@@ -1282,8 +1276,7 @@ vect_get_data_access_cost (struct data_reference *dr,\n \t\t\t   stmt_vector_for_cost *body_cost_vec,\n \t\t\t   stmt_vector_for_cost *prologue_cost_vec)\n {\n-  gimple *stmt = vect_dr_stmt (dr);\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   int ncopies;\n \n@@ -1412,16 +1405,15 @@ vect_get_peeling_costs_all_drs (vec<data_reference_p> datarefs,\n \n   FOR_EACH_VEC_ELT (datarefs, i, dr)\n     {\n-      gimple *stmt = vect_dr_stmt (dr);\n-      stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+      stmt_vec_info stmt_info = vect_dr_stmt (dr);\n       if (!STMT_VINFO_RELEVANT_P (stmt_info))\n \tcontinue;\n \n       /* For interleaving, only the alignment of the first access\n          matters.  */\n       if (STMT_VINFO_GROUPED_ACCESS (stmt_info)\n-          && DR_GROUP_FIRST_ELEMENT (stmt_info) != stmt)\n-        continue;\n+\t  && DR_GROUP_FIRST_ELEMENT (stmt_info) != stmt_info)\n+\tcontinue;\n \n       /* Strided accesses perform only component accesses, alignment is\n          irrelevant for them.  */\n@@ -1453,8 +1445,7 @@ vect_peeling_hash_get_lowest_cost (_vect_peel_info **slot,\n   vect_peel_info elem = *slot;\n   int dummy;\n   unsigned int inside_cost = 0, outside_cost = 0;\n-  gimple *stmt = vect_dr_stmt (elem->dr);\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  stmt_vec_info stmt_info = vect_dr_stmt (elem->dr);\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   stmt_vector_for_cost prologue_cost_vec, body_cost_vec,\n \t\t       epilogue_cost_vec;\n@@ -1537,8 +1528,6 @@ vect_peeling_supportable (loop_vec_info loop_vinfo, struct data_reference *dr0,\n   unsigned i;\n   struct data_reference *dr = NULL;\n   vec<data_reference_p> datarefs = LOOP_VINFO_DATAREFS (loop_vinfo);\n-  gimple *stmt;\n-  stmt_vec_info stmt_info;\n   enum dr_alignment_support supportable_dr_alignment;\n \n   /* Ensure that all data refs can be vectorized after the peel.  */\n@@ -1549,12 +1538,11 @@ vect_peeling_supportable (loop_vec_info loop_vinfo, struct data_reference *dr0,\n       if (dr == dr0)\n \tcontinue;\n \n-      stmt = vect_dr_stmt (dr);\n-      stmt_info = vinfo_for_stmt (stmt);\n+      stmt_vec_info stmt_info = vect_dr_stmt (dr);\n       /* For interleaving, only the alignment of the first access\n \t matters.  */\n       if (STMT_VINFO_GROUPED_ACCESS (stmt_info)\n-\t  && DR_GROUP_FIRST_ELEMENT (stmt_info) != stmt)\n+\t  && DR_GROUP_FIRST_ELEMENT (stmt_info) != stmt_info)\n \tcontinue;\n \n       /* Strided accesses perform only component accesses, alignment is\n@@ -1678,8 +1666,6 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n   bool do_peeling = false;\n   bool do_versioning = false;\n   bool stat;\n-  gimple *stmt;\n-  stmt_vec_info stmt_info;\n   unsigned int npeel = 0;\n   bool one_misalignment_known = false;\n   bool one_misalignment_unknown = false;\n@@ -1731,17 +1717,16 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \n   FOR_EACH_VEC_ELT (datarefs, i, dr)\n     {\n-      stmt = vect_dr_stmt (dr);\n-      stmt_info = vinfo_for_stmt (stmt);\n+      stmt_vec_info stmt_info = vect_dr_stmt (dr);\n \n       if (!STMT_VINFO_RELEVANT_P (stmt_info))\n \tcontinue;\n \n       /* For interleaving, only the alignment of the first access\n          matters.  */\n       if (STMT_VINFO_GROUPED_ACCESS (stmt_info)\n-          && DR_GROUP_FIRST_ELEMENT (stmt_info) != stmt)\n-        continue;\n+\t  && DR_GROUP_FIRST_ELEMENT (stmt_info) != stmt_info)\n+\tcontinue;\n \n       /* For scatter-gather or invariant accesses there is nothing\n \t to enhance.  */\n@@ -1943,8 +1928,7 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n       epilogue_cost_vec.release ();\n \n       peel_for_unknown_alignment.peel_info.count = 1\n-\t+ STMT_VINFO_SAME_ALIGN_REFS\n-\t(vinfo_for_stmt (vect_dr_stmt (dr0))).length ();\n+\t+ STMT_VINFO_SAME_ALIGN_REFS (vect_dr_stmt (dr0)).length ();\n     }\n \n   peel_for_unknown_alignment.peel_info.npeel = 0;\n@@ -2025,8 +2009,7 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \n   if (do_peeling)\n     {\n-      stmt = vect_dr_stmt (dr0);\n-      stmt_info = vinfo_for_stmt (stmt);\n+      stmt_vec_info stmt_info = vect_dr_stmt (dr0);\n       vectype = STMT_VINFO_VECTYPE (stmt_info);\n \n       if (known_alignment_for_access_p (dr0))\n@@ -2049,7 +2032,7 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \t  /* For interleaved data access every iteration accesses all the\n \t     members of the group, therefore we divide the number of iterations\n \t     by the group size.  */\n-\t  stmt_info = vinfo_for_stmt (vect_dr_stmt (dr0));\n+\t  stmt_info = vect_dr_stmt (dr0);\n \t  if (STMT_VINFO_GROUPED_ACCESS (stmt_info))\n \t    npeel /= DR_GROUP_SIZE (stmt_info);\n \n@@ -2123,7 +2106,7 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \t      {\n \t\t/* Strided accesses perform only component accesses, alignment\n \t\t   is irrelevant for them.  */\n-\t\tstmt_info = vinfo_for_stmt (vect_dr_stmt (dr));\n+\t\tstmt_info = vect_dr_stmt (dr);\n \t\tif (STMT_VINFO_STRIDED_P (stmt_info)\n \t\t    && !STMT_VINFO_GROUPED_ACCESS (stmt_info))\n \t\t  continue;\n@@ -2172,14 +2155,13 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n     {\n       FOR_EACH_VEC_ELT (datarefs, i, dr)\n         {\n-\t  stmt = vect_dr_stmt (dr);\n-\t  stmt_info = vinfo_for_stmt (stmt);\n+\t  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n \n \t  /* For interleaving, only the alignment of the first access\n \t     matters.  */\n \t  if (aligned_access_p (dr)\n \t      || (STMT_VINFO_GROUPED_ACCESS (stmt_info)\n-\t\t  && DR_GROUP_FIRST_ELEMENT (stmt_info) != stmt))\n+\t\t  && DR_GROUP_FIRST_ELEMENT (stmt_info) != stmt_info))\n \t    continue;\n \n \t  if (STMT_VINFO_STRIDED_P (stmt_info))\n@@ -2196,7 +2178,6 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \n           if (!supportable_dr_alignment)\n             {\n-\t      gimple *stmt;\n               int mask;\n               tree vectype;\n \n@@ -2208,9 +2189,9 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n                   break;\n                 }\n \n-              stmt = vect_dr_stmt (dr);\n-              vectype = STMT_VINFO_VECTYPE (vinfo_for_stmt (stmt));\n-              gcc_assert (vectype);\n+\t      stmt_info = vect_dr_stmt (dr);\n+\t      vectype = STMT_VINFO_VECTYPE (stmt_info);\n+\t      gcc_assert (vectype);\n \n \t      /* At present we don't support versioning for alignment\n \t\t with variable VF, since there's no guarantee that the\n@@ -2237,8 +2218,7 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n               gcc_assert (!LOOP_VINFO_PTR_MASK (loop_vinfo)\n                           || LOOP_VINFO_PTR_MASK (loop_vinfo) == mask);\n               LOOP_VINFO_PTR_MASK (loop_vinfo) = mask;\n-              LOOP_VINFO_MAY_MISALIGN_STMTS (loop_vinfo).safe_push (\n-\t\t      vect_dr_stmt (dr));\n+\t      LOOP_VINFO_MAY_MISALIGN_STMTS (loop_vinfo).safe_push (stmt_info);\n             }\n         }\n \n@@ -2298,8 +2278,8 @@ vect_find_same_alignment_drs (struct data_dependence_relation *ddr)\n {\n   struct data_reference *dra = DDR_A (ddr);\n   struct data_reference *drb = DDR_B (ddr);\n-  stmt_vec_info stmtinfo_a = vinfo_for_stmt (vect_dr_stmt (dra));\n-  stmt_vec_info stmtinfo_b = vinfo_for_stmt (vect_dr_stmt (drb));\n+  stmt_vec_info stmtinfo_a = vect_dr_stmt (dra);\n+  stmt_vec_info stmtinfo_b = vect_dr_stmt (drb);\n \n   if (DDR_ARE_DEPENDENT (ddr) == chrec_known)\n     return;\n@@ -2372,7 +2352,7 @@ vect_analyze_data_refs_alignment (loop_vec_info vinfo)\n   vect_record_base_alignments (vinfo);\n   FOR_EACH_VEC_ELT (datarefs, i, dr)\n     {\n-      stmt_vec_info stmt_info = vinfo_for_stmt (vect_dr_stmt (dr));\n+      stmt_vec_info stmt_info = vect_dr_stmt (dr);\n       if (STMT_VINFO_VECTORIZABLE (stmt_info))\n \tvect_compute_data_ref_alignment (dr);\n     }\n@@ -2451,8 +2431,7 @@ vect_analyze_group_access_1 (struct data_reference *dr)\n   tree step = DR_STEP (dr);\n   tree scalar_type = TREE_TYPE (DR_REF (dr));\n   HOST_WIDE_INT type_size = TREE_INT_CST_LOW (TYPE_SIZE_UNIT (scalar_type));\n-  gimple *stmt = vect_dr_stmt (dr);\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   bb_vec_info bb_vinfo = STMT_VINFO_BB_VINFO (stmt_info);\n   HOST_WIDE_INT dr_step = -1;\n@@ -2491,7 +2470,7 @@ vect_analyze_group_access_1 (struct data_reference *dr)\n     groupsize = 0;\n \n   /* Not consecutive access is possible only if it is a part of interleaving.  */\n-  if (!DR_GROUP_FIRST_ELEMENT (vinfo_for_stmt (stmt)))\n+  if (!DR_GROUP_FIRST_ELEMENT (stmt_info))\n     {\n       /* Check if it this DR is a part of interleaving, and is a single\n \t element of the group that is accessed in the loop.  */\n@@ -2502,8 +2481,8 @@ vect_analyze_group_access_1 (struct data_reference *dr)\n \t  && (dr_step % type_size) == 0\n \t  && groupsize > 0)\n \t{\n-\t  DR_GROUP_FIRST_ELEMENT (vinfo_for_stmt (stmt)) = stmt;\n-\t  DR_GROUP_SIZE (vinfo_for_stmt (stmt)) = groupsize;\n+\t  DR_GROUP_FIRST_ELEMENT (stmt_info) = stmt_info;\n+\t  DR_GROUP_SIZE (stmt_info) = groupsize;\n \t  DR_GROUP_GAP (stmt_info) = groupsize - 1;\n \t  if (dump_enabled_p ())\n \t    {\n@@ -2522,29 +2501,30 @@ vect_analyze_group_access_1 (struct data_reference *dr)\n         {\n  \t  dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n \t                   \"not consecutive access \");\n-\t  dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM, stmt, 0);\n+\t  dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM,\n+\t\t\t    stmt_info->stmt, 0);\n         }\n \n       if (bb_vinfo)\n-        {\n-          /* Mark the statement as unvectorizable.  */\n-          STMT_VINFO_VECTORIZABLE (vinfo_for_stmt (vect_dr_stmt (dr))) = false;\n-          return true;\n-        }\n+\t{\n+\t  /* Mark the statement as unvectorizable.  */\n+\t  STMT_VINFO_VECTORIZABLE (vect_dr_stmt (dr)) = false;\n+\t  return true;\n+\t}\n \n       dump_printf_loc (MSG_NOTE, vect_location, \"using strided accesses\\n\");\n       STMT_VINFO_STRIDED_P (stmt_info) = true;\n       return true;\n     }\n \n-  if (DR_GROUP_FIRST_ELEMENT (vinfo_for_stmt (stmt)) == stmt)\n+  if (DR_GROUP_FIRST_ELEMENT (stmt_info) == stmt_info)\n     {\n       /* First stmt in the interleaving chain. Check the chain.  */\n-      gimple *next = DR_GROUP_NEXT_ELEMENT (vinfo_for_stmt (stmt));\n+      gimple *next = DR_GROUP_NEXT_ELEMENT (stmt_info);\n       struct data_reference *data_ref = dr;\n       unsigned int count = 1;\n       tree prev_init = DR_INIT (data_ref);\n-      gimple *prev = stmt;\n+      gimple *prev = stmt_info;\n       HOST_WIDE_INT diff, gaps = 0;\n \n       /* By construction, all group members have INTEGER_CST DR_INITs.  */\n@@ -2643,9 +2623,9 @@ vect_analyze_group_access_1 (struct data_reference *dr)\n \t difference between the groupsize and the last accessed\n \t element.\n \t When there is no gap, this difference should be 0.  */\n-      DR_GROUP_GAP (vinfo_for_stmt (stmt)) = groupsize - last_accessed_element;\n+      DR_GROUP_GAP (stmt_info) = groupsize - last_accessed_element;\n \n-      DR_GROUP_SIZE (vinfo_for_stmt (stmt)) = groupsize;\n+      DR_GROUP_SIZE (stmt_info) = groupsize;\n       if (dump_enabled_p ())\n \t{\n \t  dump_printf_loc (MSG_NOTE, vect_location,\n@@ -2656,22 +2636,22 @@ vect_analyze_group_access_1 (struct data_reference *dr)\n \t    dump_printf (MSG_NOTE, \"store \");\n \t  dump_printf (MSG_NOTE, \"of size %u starting with \",\n \t\t       (unsigned)groupsize);\n-\t  dump_gimple_stmt (MSG_NOTE, TDF_SLIM, stmt, 0);\n-\t  if (DR_GROUP_GAP (vinfo_for_stmt (stmt)) != 0)\n+\t  dump_gimple_stmt (MSG_NOTE, TDF_SLIM, stmt_info->stmt, 0);\n+\t  if (DR_GROUP_GAP (stmt_info) != 0)\n \t    dump_printf_loc (MSG_NOTE, vect_location,\n \t\t\t     \"There is a gap of %u elements after the group\\n\",\n-\t\t\t     DR_GROUP_GAP (vinfo_for_stmt (stmt)));\n+\t\t\t     DR_GROUP_GAP (stmt_info));\n \t}\n \n       /* SLP: create an SLP data structure for every interleaving group of\n \t stores for further analysis in vect_analyse_slp.  */\n       if (DR_IS_WRITE (dr) && !slp_impossible)\n-        {\n-          if (loop_vinfo)\n-            LOOP_VINFO_GROUPED_STORES (loop_vinfo).safe_push (stmt);\n-          if (bb_vinfo)\n-            BB_VINFO_GROUPED_STORES (bb_vinfo).safe_push (stmt);\n-        }\n+\t{\n+\t  if (loop_vinfo)\n+\t    LOOP_VINFO_GROUPED_STORES (loop_vinfo).safe_push (stmt_info);\n+\t  if (bb_vinfo)\n+\t    BB_VINFO_GROUPED_STORES (bb_vinfo).safe_push (stmt_info);\n+\t}\n     }\n \n   return true;\n@@ -2689,7 +2669,7 @@ vect_analyze_group_access (struct data_reference *dr)\n     {\n       /* Dissolve the group if present.  */\n       gimple *next;\n-      gimple *stmt = DR_GROUP_FIRST_ELEMENT (vinfo_for_stmt (vect_dr_stmt (dr)));\n+      gimple *stmt = DR_GROUP_FIRST_ELEMENT (vect_dr_stmt (dr));\n       while (stmt)\n \t{\n \t  stmt_vec_info vinfo = vinfo_for_stmt (stmt);\n@@ -2712,8 +2692,7 @@ vect_analyze_data_ref_access (struct data_reference *dr)\n {\n   tree step = DR_STEP (dr);\n   tree scalar_type = TREE_TYPE (DR_REF (dr));\n-  gimple *stmt = vect_dr_stmt (dr);\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   struct loop *loop = NULL;\n \n@@ -2734,8 +2713,8 @@ vect_analyze_data_ref_access (struct data_reference *dr)\n   /* Allow loads with zero step in inner-loop vectorization.  */\n   if (loop_vinfo && integer_zerop (step))\n     {\n-      DR_GROUP_FIRST_ELEMENT (vinfo_for_stmt (stmt)) = NULL;\n-      if (!nested_in_vect_loop_p (loop, stmt))\n+      DR_GROUP_FIRST_ELEMENT (stmt_info) = NULL;\n+      if (!nested_in_vect_loop_p (loop, stmt_info))\n \treturn DR_IS_READ (dr);\n       /* Allow references with zero step for outer loops marked\n \t with pragma omp simd only - it guarantees absence of\n@@ -2749,11 +2728,11 @@ vect_analyze_data_ref_access (struct data_reference *dr)\n \t}\n     }\n \n-  if (loop && nested_in_vect_loop_p (loop, stmt))\n+  if (loop && nested_in_vect_loop_p (loop, stmt_info))\n     {\n       /* Interleaved accesses are not yet supported within outer-loop\n         vectorization for references in the inner-loop.  */\n-      DR_GROUP_FIRST_ELEMENT (vinfo_for_stmt (stmt)) = NULL;\n+      DR_GROUP_FIRST_ELEMENT (stmt_info) = NULL;\n \n       /* For the rest of the analysis we use the outer-loop step.  */\n       step = STMT_VINFO_DR_STEP (stmt_info);\n@@ -2775,12 +2754,12 @@ vect_analyze_data_ref_access (struct data_reference *dr)\n \t      && !compare_tree_int (TYPE_SIZE_UNIT (scalar_type), -dr_step)))\n \t{\n \t  /* Mark that it is not interleaving.  */\n-\t  DR_GROUP_FIRST_ELEMENT (vinfo_for_stmt (stmt)) = NULL;\n+\t  DR_GROUP_FIRST_ELEMENT (stmt_info) = NULL;\n \t  return true;\n \t}\n     }\n \n-  if (loop && nested_in_vect_loop_p (loop, stmt))\n+  if (loop && nested_in_vect_loop_p (loop, stmt_info))\n     {\n       if (dump_enabled_p ())\n \tdump_printf_loc (MSG_NOTE, vect_location,\n@@ -2939,7 +2918,7 @@ vect_analyze_data_ref_accesses (vec_info *vinfo)\n   for (i = 0; i < datarefs_copy.length () - 1;)\n     {\n       data_reference_p dra = datarefs_copy[i];\n-      stmt_vec_info stmtinfo_a = vinfo_for_stmt (vect_dr_stmt (dra));\n+      stmt_vec_info stmtinfo_a = vect_dr_stmt (dra);\n       stmt_vec_info lastinfo = NULL;\n       if (!STMT_VINFO_VECTORIZABLE (stmtinfo_a)\n \t  || STMT_VINFO_GATHER_SCATTER_P (stmtinfo_a))\n@@ -2950,7 +2929,7 @@ vect_analyze_data_ref_accesses (vec_info *vinfo)\n       for (i = i + 1; i < datarefs_copy.length (); ++i)\n \t{\n \t  data_reference_p drb = datarefs_copy[i];\n-\t  stmt_vec_info stmtinfo_b = vinfo_for_stmt (vect_dr_stmt (drb));\n+\t  stmt_vec_info stmtinfo_b = vect_dr_stmt (drb);\n \t  if (!STMT_VINFO_VECTORIZABLE (stmtinfo_b)\n \t      || STMT_VINFO_GATHER_SCATTER_P (stmtinfo_b))\n \t    break;\n@@ -3073,19 +3052,19 @@ vect_analyze_data_ref_accesses (vec_info *vinfo)\n     }\n \n   FOR_EACH_VEC_ELT (datarefs_copy, i, dr)\n-    if (STMT_VINFO_VECTORIZABLE (vinfo_for_stmt (vect_dr_stmt (dr))) \n+    if (STMT_VINFO_VECTORIZABLE (vect_dr_stmt (dr))\n         && !vect_analyze_data_ref_access (dr))\n       {\n \tif (dump_enabled_p ())\n \t  dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n \t                   \"not vectorized: complicated access pattern.\\n\");\n \n         if (is_a <bb_vec_info> (vinfo))\n-          {\n-            /* Mark the statement as not vectorizable.  */\n-            STMT_VINFO_VECTORIZABLE (vinfo_for_stmt (vect_dr_stmt (dr))) = false;\n-            continue;\n-          }\n+\t  {\n+\t    /* Mark the statement as not vectorizable.  */\n+\t    STMT_VINFO_VECTORIZABLE (vect_dr_stmt (dr)) = false;\n+\t    continue;\n+\t  }\n         else\n \t  {\n \t    datarefs_copy.release ();\n@@ -3124,7 +3103,7 @@ vect_vfa_segment_size (struct data_reference *dr, tree length_factor)\n static unsigned HOST_WIDE_INT\n vect_vfa_access_size (data_reference *dr)\n {\n-  stmt_vec_info stmt_vinfo = vinfo_for_stmt (vect_dr_stmt (dr));\n+  stmt_vec_info stmt_vinfo = vect_dr_stmt (dr);\n   tree ref_type = TREE_TYPE (DR_REF (dr));\n   unsigned HOST_WIDE_INT ref_size = tree_to_uhwi (TYPE_SIZE_UNIT (ref_type));\n   unsigned HOST_WIDE_INT access_size = ref_size;\n@@ -3298,7 +3277,7 @@ vect_check_lower_bound (loop_vec_info loop_vinfo, tree expr, bool unsigned_p,\n static bool\n vect_small_gap_p (loop_vec_info loop_vinfo, data_reference *dr, poly_int64 gap)\n {\n-  stmt_vec_info stmt_info = vinfo_for_stmt (vect_dr_stmt (dr));\n+  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n   HOST_WIDE_INT count\n     = estimated_poly_value (LOOP_VINFO_VECT_FACTOR (loop_vinfo));\n   if (DR_GROUP_FIRST_ELEMENT (stmt_info))\n@@ -4141,14 +4120,11 @@ vect_analyze_data_refs (vec_info *vinfo, poly_uint64 *min_vf)\n   vec<data_reference_p> datarefs = vinfo->shared->datarefs;\n   FOR_EACH_VEC_ELT (datarefs, i, dr)\n     {\n-      gimple *stmt;\n-      stmt_vec_info stmt_info;\n       enum { SG_NONE, GATHER, SCATTER } gatherscatter = SG_NONE;\n       poly_uint64 vf;\n \n       gcc_assert (DR_REF (dr));\n-      stmt = vect_dr_stmt (dr);\n-      stmt_info = vinfo_for_stmt (stmt);\n+      stmt_vec_info stmt_info = vect_dr_stmt (dr);\n \n       /* Check that analysis of the data-ref succeeded.  */\n       if (!DR_BASE_ADDRESS (dr) || !DR_OFFSET (dr) || !DR_INIT (dr)\n@@ -4168,7 +4144,7 @@ vect_analyze_data_refs (vec_info *vinfo, poly_uint64 *min_vf)\n \t  /* If target supports vector gather loads or scatter stores,\n \t     see if they can't be used.  */\n \t  if (is_a <loop_vec_info> (vinfo)\n-\t      && !nested_in_vect_loop_p (loop, stmt))\n+\t      && !nested_in_vect_loop_p (loop, stmt_info))\n \t    {\n \t      if (maybe_gather || maybe_scatter)\n \t\t{\n@@ -4186,7 +4162,8 @@ vect_analyze_data_refs (vec_info *vinfo, poly_uint64 *min_vf)\n \t\t  dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n                                    \"not vectorized: data ref analysis \"\n                                    \"failed \");\n-\t\t  dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM, stmt, 0);\n+\t\t  dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM,\n+\t\t\t\t    stmt_info->stmt, 0);\n \t\t}\n \t      if (is_a <bb_vec_info> (vinfo))\n \t\t{\n@@ -4202,14 +4179,15 @@ vect_analyze_data_refs (vec_info *vinfo, poly_uint64 *min_vf)\n       /* See if this was detected as SIMD lane access.  */\n       if (dr->aux == (void *)-1)\n \t{\n-\t  if (nested_in_vect_loop_p (loop, stmt))\n+\t  if (nested_in_vect_loop_p (loop, stmt_info))\n \t    {\n \t      if (dump_enabled_p ())\n \t\t{\n \t\t  dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n \t\t\t\t   \"not vectorized: data ref analysis \"\n \t\t\t\t   \"failed \");\n-\t\t  dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM, stmt, 0);\n+\t\t  dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM,\n+\t\t\t\t    stmt_info->stmt, 0);\n \t\t}\n \t      return false;\n \t    }\n@@ -4224,7 +4202,8 @@ vect_analyze_data_refs (vec_info *vinfo, poly_uint64 *min_vf)\n               dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n                                \"not vectorized: base object not addressable \"\n \t\t\t       \"for stmt: \");\n-              dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM, stmt, 0);\n+\t      dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM,\n+\t\t\t\tstmt_info->stmt, 0);\n             }\n           if (is_a <bb_vec_info> (vinfo))\n \t    {\n@@ -4240,14 +4219,15 @@ vect_analyze_data_refs (vec_info *vinfo, poly_uint64 *min_vf)\n \t  && DR_STEP (dr)\n \t  && TREE_CODE (DR_STEP (dr)) != INTEGER_CST)\n \t{\n-\t  if (nested_in_vect_loop_p (loop, stmt))\n+\t  if (nested_in_vect_loop_p (loop, stmt_info))\n \t    {\n \t      if (dump_enabled_p ())\n \t\t{\n \t\t  dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location, \n                                    \"not vectorized: not suitable for strided \"\n                                    \"load \");\n-\t\t  dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM, stmt, 0);\n+\t\t  dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM,\n+\t\t\t\t    stmt_info->stmt, 0);\n \t\t}\n \t      return false;\n \t    }\n@@ -4262,7 +4242,7 @@ vect_analyze_data_refs (vec_info *vinfo, poly_uint64 *min_vf)\n \t inner-most enclosing loop).  We do that by building a reference to the\n \t first location accessed by the inner-loop, and analyze it relative to\n \t the outer-loop.  */\n-      if (loop && nested_in_vect_loop_p (loop, stmt))\n+      if (loop && nested_in_vect_loop_p (loop, stmt_info))\n \t{\n \t  /* Build a reference to the first location accessed by the\n \t     inner loop: *(BASE + INIT + OFFSET).  By construction,\n@@ -4329,7 +4309,8 @@ vect_analyze_data_refs (vec_info *vinfo, poly_uint64 *min_vf)\n             {\n               dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n                                \"not vectorized: no vectype for stmt: \");\n-              dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM, stmt, 0);\n+\t      dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM,\n+\t\t\t\tstmt_info->stmt, 0);\n               dump_printf (MSG_MISSED_OPTIMIZATION, \" scalar_type: \");\n               dump_generic_expr (MSG_MISSED_OPTIMIZATION, TDF_DETAILS,\n                                  scalar_type);\n@@ -4351,7 +4332,7 @@ vect_analyze_data_refs (vec_info *vinfo, poly_uint64 *min_vf)\n \t    {\n \t      dump_printf_loc (MSG_NOTE, vect_location,\n \t\t\t       \"got vectype for stmt: \");\n-\t      dump_gimple_stmt (MSG_NOTE, TDF_SLIM, stmt, 0);\n+\t      dump_gimple_stmt (MSG_NOTE, TDF_SLIM, stmt_info->stmt, 0);\n \t      dump_generic_expr (MSG_NOTE, TDF_SLIM,\n \t\t\t\t STMT_VINFO_VECTYPE (stmt_info));\n \t      dump_printf (MSG_NOTE, \"\\n\");\n@@ -4366,7 +4347,8 @@ vect_analyze_data_refs (vec_info *vinfo, poly_uint64 *min_vf)\n       if (gatherscatter != SG_NONE)\n \t{\n \t  gather_scatter_info gs_info;\n-\t  if (!vect_check_gather_scatter (stmt, as_a <loop_vec_info> (vinfo),\n+\t  if (!vect_check_gather_scatter (stmt_info,\n+\t\t\t\t\t  as_a <loop_vec_info> (vinfo),\n \t\t\t\t\t  &gs_info)\n \t      || !get_vectype_for_scalar_type (TREE_TYPE (gs_info.offset)))\n \t    {\n@@ -4378,7 +4360,8 @@ vect_analyze_data_refs (vec_info *vinfo, poly_uint64 *min_vf)\n \t\t\t\t   \"load \" :\n \t\t\t\t   \"not vectorized: not suitable for scatter \"\n \t\t\t\t   \"store \");\n-\t\t  dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM, stmt, 0);\n+\t\t  dump_gimple_stmt (MSG_MISSED_OPTIMIZATION, TDF_SLIM,\n+\t\t\t\t    stmt_info->stmt, 0);\n \t\t}\n \t      return false;\n \t    }\n@@ -6459,8 +6442,7 @@ enum dr_alignment_support\n vect_supportable_dr_alignment (struct data_reference *dr,\n                                bool check_aligned_accesses)\n {\n-  gimple *stmt = vect_dr_stmt (dr);\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n   tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n   machine_mode mode = TYPE_MODE (vectype);\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n@@ -6472,16 +6454,16 @@ vect_supportable_dr_alignment (struct data_reference *dr,\n \n   /* For now assume all conditional loads/stores support unaligned\n      access without any special code.  */\n-  if (is_gimple_call (stmt)\n-      && gimple_call_internal_p (stmt)\n-      && (gimple_call_internal_fn (stmt) == IFN_MASK_LOAD\n-\t  || gimple_call_internal_fn (stmt) == IFN_MASK_STORE))\n-    return dr_unaligned_supported;\n+  if (gcall *stmt = dyn_cast <gcall *> (stmt_info->stmt))\n+    if (gimple_call_internal_p (stmt)\n+\t&& (gimple_call_internal_fn (stmt) == IFN_MASK_LOAD\n+\t    || gimple_call_internal_fn (stmt) == IFN_MASK_STORE))\n+      return dr_unaligned_supported;\n \n   if (loop_vinfo)\n     {\n       vect_loop = LOOP_VINFO_LOOP (loop_vinfo);\n-      nested_in_vect_loop = nested_in_vect_loop_p (vect_loop, stmt);\n+      nested_in_vect_loop = nested_in_vect_loop_p (vect_loop, stmt_info);\n     }\n \n   /* Possibly unaligned access.  */"}, {"sha": "4ff6d2dafa4ad2bb9dd60afded71aa3c086f3f46", "filename": "gcc/tree-vect-loop-manip.c", "status": "modified", "additions": 3, "deletions": 5, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/78e02b3bbb00fc898c550b88161838eb5dd95806/gcc%2Ftree-vect-loop-manip.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/78e02b3bbb00fc898c550b88161838eb5dd95806/gcc%2Ftree-vect-loop-manip.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-loop-manip.c?ref=78e02b3bbb00fc898c550b88161838eb5dd95806", "patch": "@@ -1560,8 +1560,7 @@ static tree\n get_misalign_in_elems (gimple **seq, loop_vec_info loop_vinfo)\n {\n   struct data_reference *dr = LOOP_VINFO_UNALIGNED_DR (loop_vinfo);\n-  gimple *dr_stmt = vect_dr_stmt (dr);\n-  stmt_vec_info stmt_info = vinfo_for_stmt (dr_stmt);\n+  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n   tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n \n   unsigned int target_align = DR_TARGET_ALIGNMENT (dr);\n@@ -1571,7 +1570,7 @@ get_misalign_in_elems (gimple **seq, loop_vec_info loop_vinfo)\n   tree offset = (negative\n \t\t ? size_int (-TYPE_VECTOR_SUBPARTS (vectype) + 1)\n \t\t : size_zero_node);\n-  tree start_addr = vect_create_addr_base_for_vector_ref (dr_stmt, seq,\n+  tree start_addr = vect_create_addr_base_for_vector_ref (stmt_info, seq,\n \t\t\t\t\t\t\t  offset);\n   tree type = unsigned_type_for (TREE_TYPE (start_addr));\n   tree target_align_minus_1 = build_int_cst (type, target_align - 1);\n@@ -1631,8 +1630,7 @@ vect_gen_prolog_loop_niters (loop_vec_info loop_vinfo,\n   tree niters_type = TREE_TYPE (LOOP_VINFO_NITERS (loop_vinfo));\n   gimple_seq stmts = NULL, new_stmts = NULL;\n   tree iters, iters_name;\n-  gimple *dr_stmt = vect_dr_stmt (dr);\n-  stmt_vec_info stmt_info = vinfo_for_stmt (dr_stmt);\n+  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n   tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n   unsigned int target_align = DR_TARGET_ALIGNMENT (dr);\n "}, {"sha": "3ac4aee02b3e6d612ddbe0fcce8f3745279256b3", "filename": "gcc/tree-vect-loop.c", "status": "modified", "additions": 1, "deletions": 2, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/78e02b3bbb00fc898c550b88161838eb5dd95806/gcc%2Ftree-vect-loop.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/78e02b3bbb00fc898c550b88161838eb5dd95806/gcc%2Ftree-vect-loop.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-loop.c?ref=78e02b3bbb00fc898c550b88161838eb5dd95806", "patch": "@@ -2145,8 +2145,7 @@ vect_analyze_loop_2 (loop_vec_info loop_vinfo, bool &fatal, unsigned *n_stmts)\n \t  if (LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo) < 0)\n \t    {\n \t      struct data_reference *dr = LOOP_VINFO_UNALIGNED_DR (loop_vinfo);\n-\t      tree vectype\n-\t\t= STMT_VINFO_VECTYPE (vinfo_for_stmt (vect_dr_stmt (dr)));\n+\t      tree vectype = STMT_VINFO_VECTYPE (vect_dr_stmt (dr));\n \t      niters_th += TYPE_VECTOR_SUBPARTS (vectype) - 1;\n \t    }\n \t  else"}, {"sha": "9bb9afdd023c0233e27eb4d6d5b9a62eade813d8", "filename": "gcc/tree-vectorizer.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/78e02b3bbb00fc898c550b88161838eb5dd95806/gcc%2Ftree-vectorizer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/78e02b3bbb00fc898c550b88161838eb5dd95806/gcc%2Ftree-vectorizer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.h?ref=78e02b3bbb00fc898c550b88161838eb5dd95806", "patch": "@@ -1370,7 +1370,7 @@ vect_dr_behavior (data_reference *dr)\n    a pattern this returns the corresponding pattern stmt.  Otherwise\n    DR_STMT is returned.  */\n \n-inline gimple *\n+inline stmt_vec_info\n vect_dr_stmt (data_reference *dr)\n {\n   gimple *stmt = DR_STMT (dr);\n@@ -1379,7 +1379,7 @@ vect_dr_stmt (data_reference *dr)\n     return STMT_VINFO_RELATED_STMT (stmt_info);\n   /* DR_STMT should never refer to a stmt in a pattern replacement.  */\n   gcc_checking_assert (!STMT_VINFO_RELATED_STMT (stmt_info));\n-  return stmt;\n+  return stmt_info;\n }\n \n /* Return true if the vect cost model is unlimited.  */"}]}