{"sha": "923fcec3d8427c6169979294ea5f2a5e11cfd4cf", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OTIzZmNlYzNkODQyN2M2MTY5OTc5Mjk0ZWE1ZjJhNWUxMWNmZDRjZg==", "commit": {"author": {"name": "Alan Lawrence", "email": "alan.lawrence@arm.com", "date": "2014-06-03T11:28:55Z"}, "committer": {"name": "Alan Lawrence", "email": "alalaw01@gcc.gnu.org", "date": "2014-06-03T11:28:55Z"}, "message": "Recognize shuffle patterns for REV instructions on AArch64, rewrite intrinsics.\n\n        * config/aarch64/aarch64-simd.md (aarch64_rev<REVERSE:rev-op><mode>):\n        New pattern.\n        * config/aarch64/aarch64.c (aarch64_evpc_rev): New function.\n        (aarch64_expand_vec_perm_const_1): Add call to aarch64_evpc_rev.\n        * config/aarch64/iterators.md (REVERSE): New iterator.\n        (UNSPEC_REV64, UNSPEC_REV32, UNSPEC_REV16): New enum elements.\n        (rev_op): New int_attribute.\n        * config/aarch64/arm_neon.h (vrev16_p8, vrev16_s8, vrev16_u8,\n        vrev16q_p8, vrev16q_s8, vrev16q_u8, vrev32_p8, vrev32_p16, vrev32_s8,\n        vrev32_s16, vrev32_u8, vrev32_u16, vrev32q_p8, vrev32q_p16, vrev32q_s8,\n        vrev32q_s16, vrev32q_u8, vrev32q_u16, vrev64_f32, vrev64_p8,\n        vrev64_p16, vrev64_s8, vrev64_s16, vrev64_s32, vrev64_u8, vrev64_u16,\n        vrev64_u32, vrev64q_f32, vrev64q_p8, vrev64q_p16, vrev64q_s8,\n        vrev64q_s16, vrev64q_s32, vrev64q_u8, vrev64q_u16, vrev64q_u32):\n        Replace temporary __asm__ with __builtin_shuffle.\n\nFrom-SVN: r211174", "tree": {"sha": "cea9f6dffba394b50862fffc14e64513e44bfcf8", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/cea9f6dffba394b50862fffc14e64513e44bfcf8"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/923fcec3d8427c6169979294ea5f2a5e11cfd4cf", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/923fcec3d8427c6169979294ea5f2a5e11cfd4cf", "html_url": "https://github.com/Rust-GCC/gccrs/commit/923fcec3d8427c6169979294ea5f2a5e11cfd4cf", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/923fcec3d8427c6169979294ea5f2a5e11cfd4cf/comments", "author": null, "committer": null, "parents": [{"sha": "2b3bd04055774268843ff094d5995e31ac52afa0", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2b3bd04055774268843ff094d5995e31ac52afa0", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2b3bd04055774268843ff094d5995e31ac52afa0"}], "stats": {"total": 738, "additions": 341, "deletions": 397}, "files": [{"sha": "1f09b046bb5fd255d0ea53180744d56deee7d40f", "filename": "gcc/ChangeLog", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/923fcec3d8427c6169979294ea5f2a5e11cfd4cf/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/923fcec3d8427c6169979294ea5f2a5e11cfd4cf/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=923fcec3d8427c6169979294ea5f2a5e11cfd4cf", "patch": "@@ -1,3 +1,21 @@\n+2014-06-03  Alan Lawrence  <alan.lawrence@arm.com>\n+\n+\t* config/aarch64/aarch64-simd.md (aarch64_rev<REVERSE:rev-op><mode>):\n+\tNew pattern.\n+\t* config/aarch64/aarch64.c (aarch64_evpc_rev): New function.\n+\t(aarch64_expand_vec_perm_const_1): Add call to aarch64_evpc_rev.\n+\t* config/aarch64/iterators.md (REVERSE): New iterator.\n+\t(UNSPEC_REV64, UNSPEC_REV32, UNSPEC_REV16): New enum elements.\n+\t(rev_op): New int_attribute.\n+\t* config/aarch64/arm_neon.h (vrev16_p8, vrev16_s8, vrev16_u8,\n+\tvrev16q_p8, vrev16q_s8, vrev16q_u8, vrev32_p8, vrev32_p16, vrev32_s8,\n+\tvrev32_s16, vrev32_u8, vrev32_u16, vrev32q_p8, vrev32q_p16, vrev32q_s8,\n+\tvrev32q_s16, vrev32q_u8, vrev32q_u16, vrev64_f32, vrev64_p8,\n+\tvrev64_p16, vrev64_s8, vrev64_s16, vrev64_s32, vrev64_u8, vrev64_u16,\n+\tvrev64_u32, vrev64q_f32, vrev64q_p8, vrev64q_p16, vrev64q_s8,\n+\tvrev64q_s16, vrev64q_s32, vrev64q_u8, vrev64q_u16, vrev64q_u32):\n+\tReplace temporary __asm__ with __builtin_shuffle.\n+\n 2014-06-03  Andrew Bennett  <andrew.bennett@imgtec.com> \n \n \t* config/mips/mips-cpus.def: Add mips32r3, mips32r5, mips64r3 and"}, {"sha": "a1f0ff53eb3858466b274d29056221b131255329", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/923fcec3d8427c6169979294ea5f2a5e11cfd4cf/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/923fcec3d8427c6169979294ea5f2a5e11cfd4cf/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=923fcec3d8427c6169979294ea5f2a5e11cfd4cf", "patch": "@@ -4196,6 +4196,15 @@\n }\n )\n \n+(define_insn \"aarch64_rev<REVERSE:rev_op><mode>\"\n+  [(set (match_operand:VALL 0 \"register_operand\" \"=w\")\n+\t(unspec:VALL [(match_operand:VALL 1 \"register_operand\" \"w\")]\n+                    REVERSE))]\n+  \"TARGET_SIMD\"\n+  \"rev<REVERSE:rev_op>\\\\t%0.<Vtype>, %1.<Vtype>\"\n+  [(set_attr \"type\" \"neon_rev<q>\")]\n+)\n+\n (define_insn \"aarch64_st2<mode>_dreg\"\n   [(set (match_operand:TI 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n \t(unspec:TI [(match_operand:OI 1 \"register_operand\" \"w\")"}, {"sha": "65ef84a8ffa294cf9f31af4726f7586e58abd053", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 77, "deletions": 1, "changes": 78, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/923fcec3d8427c6169979294ea5f2a5e11cfd4cf/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/923fcec3d8427c6169979294ea5f2a5e11cfd4cf/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=923fcec3d8427c6169979294ea5f2a5e11cfd4cf", "patch": "@@ -9058,6 +9058,80 @@ aarch64_evpc_ext (struct expand_vec_perm_d *d)\n   return true;\n }\n \n+/* Recognize patterns for the REV insns.  */\n+\n+static bool\n+aarch64_evpc_rev (struct expand_vec_perm_d *d)\n+{\n+  unsigned int i, j, diff, nelt = d->nelt;\n+  rtx (*gen) (rtx, rtx);\n+\n+  if (!d->one_vector_p)\n+    return false;\n+\n+  diff = d->perm[0];\n+  switch (diff)\n+    {\n+    case 7:\n+      switch (d->vmode)\n+\t{\n+\tcase V16QImode: gen = gen_aarch64_rev64v16qi; break;\n+\tcase V8QImode: gen = gen_aarch64_rev64v8qi;  break;\n+\tdefault:\n+\t  return false;\n+\t}\n+      break;\n+    case 3:\n+      switch (d->vmode)\n+\t{\n+\tcase V16QImode: gen = gen_aarch64_rev32v16qi; break;\n+\tcase V8QImode: gen = gen_aarch64_rev32v8qi;  break;\n+\tcase V8HImode: gen = gen_aarch64_rev64v8hi;  break;\n+\tcase V4HImode: gen = gen_aarch64_rev64v4hi;  break;\n+\tdefault:\n+\t  return false;\n+\t}\n+      break;\n+    case 1:\n+      switch (d->vmode)\n+\t{\n+\tcase V16QImode: gen = gen_aarch64_rev16v16qi; break;\n+\tcase V8QImode: gen = gen_aarch64_rev16v8qi;  break;\n+\tcase V8HImode: gen = gen_aarch64_rev32v8hi;  break;\n+\tcase V4HImode: gen = gen_aarch64_rev32v4hi;  break;\n+\tcase V4SImode: gen = gen_aarch64_rev64v4si;  break;\n+\tcase V2SImode: gen = gen_aarch64_rev64v2si;  break;\n+\tcase V4SFmode: gen = gen_aarch64_rev64v4sf;  break;\n+\tcase V2SFmode: gen = gen_aarch64_rev64v2sf;  break;\n+\tdefault:\n+\t  return false;\n+\t}\n+      break;\n+    default:\n+      return false;\n+    }\n+\n+  for (i = 0; i < nelt ; i += diff + 1)\n+    for (j = 0; j <= diff; j += 1)\n+      {\n+\t/* This is guaranteed to be true as the value of diff\n+\t   is 7, 3, 1 and we should have enough elements in the\n+\t   queue to generate this.  Getting a vector mask with a\n+\t   value of diff other than these values implies that\n+\t   something is wrong by the time we get here.  */\n+\tgcc_assert (i + j < nelt);\n+\tif (d->perm[i + j] != i + diff - j)\n+\t  return false;\n+      }\n+\n+  /* Success! */\n+  if (d->testing_p)\n+    return true;\n+\n+  emit_insn (gen (d->target, d->op0));\n+  return true;\n+}\n+\n static bool\n aarch64_evpc_dup (struct expand_vec_perm_d *d)\n {\n@@ -9162,7 +9236,9 @@ aarch64_expand_vec_perm_const_1 (struct expand_vec_perm_d *d)\n \n   if (TARGET_SIMD)\n     {\n-      if (aarch64_evpc_ext (d))\n+      if (aarch64_evpc_rev (d))\n+\treturn true;\n+      else if (aarch64_evpc_ext (d))\n \treturn true;\n       else if (aarch64_evpc_zip (d))\n \treturn true;"}, {"sha": "60b28447002824219bdcb224d301066ee7e77b99", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 228, "deletions": 396, "changes": 624, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/923fcec3d8427c6169979294ea5f2a5e11cfd4cf/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/923fcec3d8427c6169979294ea5f2a5e11cfd4cf/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=923fcec3d8427c6169979294ea5f2a5e11cfd4cf", "patch": "@@ -10563,402 +10563,6 @@ vrecpeq_u32 (uint32x4_t a)\n   return result;\n }\n \n-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n-vrev16_p8 (poly8x8_t a)\n-{\n-  poly8x8_t result;\n-  __asm__ (\"rev16 %0.8b,%1.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n-vrev16_s8 (int8x8_t a)\n-{\n-  int8x8_t result;\n-  __asm__ (\"rev16 %0.8b,%1.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vrev16_u8 (uint8x8_t a)\n-{\n-  uint8x8_t result;\n-  __asm__ (\"rev16 %0.8b,%1.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n-vrev16q_p8 (poly8x16_t a)\n-{\n-  poly8x16_t result;\n-  __asm__ (\"rev16 %0.16b,%1.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n-vrev16q_s8 (int8x16_t a)\n-{\n-  int8x16_t result;\n-  __asm__ (\"rev16 %0.16b,%1.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n-vrev16q_u8 (uint8x16_t a)\n-{\n-  uint8x16_t result;\n-  __asm__ (\"rev16 %0.16b,%1.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n-vrev32_p8 (poly8x8_t a)\n-{\n-  poly8x8_t result;\n-  __asm__ (\"rev32 %0.8b,%1.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))\n-vrev32_p16 (poly16x4_t a)\n-{\n-  poly16x4_t result;\n-  __asm__ (\"rev32 %0.4h,%1.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n-vrev32_s8 (int8x8_t a)\n-{\n-  int8x8_t result;\n-  __asm__ (\"rev32 %0.8b,%1.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n-vrev32_s16 (int16x4_t a)\n-{\n-  int16x4_t result;\n-  __asm__ (\"rev32 %0.4h,%1.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vrev32_u8 (uint8x8_t a)\n-{\n-  uint8x8_t result;\n-  __asm__ (\"rev32 %0.8b,%1.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n-vrev32_u16 (uint16x4_t a)\n-{\n-  uint16x4_t result;\n-  __asm__ (\"rev32 %0.4h,%1.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n-vrev32q_p8 (poly8x16_t a)\n-{\n-  poly8x16_t result;\n-  __asm__ (\"rev32 %0.16b,%1.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))\n-vrev32q_p16 (poly16x8_t a)\n-{\n-  poly16x8_t result;\n-  __asm__ (\"rev32 %0.8h,%1.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n-vrev32q_s8 (int8x16_t a)\n-{\n-  int8x16_t result;\n-  __asm__ (\"rev32 %0.16b,%1.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n-vrev32q_s16 (int16x8_t a)\n-{\n-  int16x8_t result;\n-  __asm__ (\"rev32 %0.8h,%1.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n-vrev32q_u8 (uint8x16_t a)\n-{\n-  uint8x16_t result;\n-  __asm__ (\"rev32 %0.16b,%1.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vrev32q_u16 (uint16x8_t a)\n-{\n-  uint16x8_t result;\n-  __asm__ (\"rev32 %0.8h,%1.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n-vrev64_f32 (float32x2_t a)\n-{\n-  float32x2_t result;\n-  __asm__ (\"rev64 %0.2s,%1.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n-vrev64_p8 (poly8x8_t a)\n-{\n-  poly8x8_t result;\n-  __asm__ (\"rev64 %0.8b,%1.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))\n-vrev64_p16 (poly16x4_t a)\n-{\n-  poly16x4_t result;\n-  __asm__ (\"rev64 %0.4h,%1.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n-vrev64_s8 (int8x8_t a)\n-{\n-  int8x8_t result;\n-  __asm__ (\"rev64 %0.8b,%1.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n-vrev64_s16 (int16x4_t a)\n-{\n-  int16x4_t result;\n-  __asm__ (\"rev64 %0.4h,%1.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n-vrev64_s32 (int32x2_t a)\n-{\n-  int32x2_t result;\n-  __asm__ (\"rev64 %0.2s,%1.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vrev64_u8 (uint8x8_t a)\n-{\n-  uint8x8_t result;\n-  __asm__ (\"rev64 %0.8b,%1.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n-vrev64_u16 (uint16x4_t a)\n-{\n-  uint16x4_t result;\n-  __asm__ (\"rev64 %0.4h,%1.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vrev64_u32 (uint32x2_t a)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"rev64 %0.2s,%1.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n-vrev64q_f32 (float32x4_t a)\n-{\n-  float32x4_t result;\n-  __asm__ (\"rev64 %0.4s,%1.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n-vrev64q_p8 (poly8x16_t a)\n-{\n-  poly8x16_t result;\n-  __asm__ (\"rev64 %0.16b,%1.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))\n-vrev64q_p16 (poly16x8_t a)\n-{\n-  poly16x8_t result;\n-  __asm__ (\"rev64 %0.8h,%1.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n-vrev64q_s8 (int8x16_t a)\n-{\n-  int8x16_t result;\n-  __asm__ (\"rev64 %0.16b,%1.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n-vrev64q_s16 (int16x8_t a)\n-{\n-  int16x8_t result;\n-  __asm__ (\"rev64 %0.8h,%1.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n-vrev64q_s32 (int32x4_t a)\n-{\n-  int32x4_t result;\n-  __asm__ (\"rev64 %0.4s,%1.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n-vrev64q_u8 (uint8x16_t a)\n-{\n-  uint8x16_t result;\n-  __asm__ (\"rev64 %0.16b,%1.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vrev64q_u16 (uint16x8_t a)\n-{\n-  uint16x8_t result;\n-  __asm__ (\"rev64 %0.8h,%1.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vrev64q_u32 (uint32x4_t a)\n-{\n-  uint32x4_t result;\n-  __asm__ (\"rev64 %0.4s,%1.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n #define vrshrn_high_n_s16(a, b, c)                                      \\\n   __extension__                                                         \\\n     ({                                                                  \\\n@@ -21414,6 +21018,234 @@ vrecpxd_f64 (float64_t __a)\n   return __builtin_aarch64_frecpxdf (__a);\n }\n \n+\n+/* vrev  */\n+\n+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n+vrev16_p8 (poly8x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint8x8_t) { 1, 0, 3, 2, 5, 4, 7, 6 });\n+}\n+\n+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n+vrev16_s8 (int8x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint8x8_t) { 1, 0, 3, 2, 5, 4, 7, 6 });\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vrev16_u8 (uint8x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint8x8_t) { 1, 0, 3, 2, 5, 4, 7, 6 });\n+}\n+\n+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n+vrev16q_p8 (poly8x16_t a)\n+{\n+  return __builtin_shuffle (a,\n+      (uint8x16_t) { 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14 });\n+}\n+\n+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n+vrev16q_s8 (int8x16_t a)\n+{\n+  return __builtin_shuffle (a,\n+      (uint8x16_t) { 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14 });\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vrev16q_u8 (uint8x16_t a)\n+{\n+  return __builtin_shuffle (a,\n+      (uint8x16_t) { 1, 0, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14 });\n+}\n+\n+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n+vrev32_p8 (poly8x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint8x8_t) { 3, 2, 1, 0, 7, 6, 5, 4 });\n+}\n+\n+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))\n+vrev32_p16 (poly16x4_t a)\n+{\n+  return __builtin_shuffle (a, (uint16x4_t) { 1, 0, 3, 2 });\n+}\n+\n+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n+vrev32_s8 (int8x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint8x8_t) { 3, 2, 1, 0, 7, 6, 5, 4 });\n+}\n+\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vrev32_s16 (int16x4_t a)\n+{\n+  return __builtin_shuffle (a, (uint16x4_t) { 1, 0, 3, 2 });\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vrev32_u8 (uint8x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint8x8_t) { 3, 2, 1, 0, 7, 6, 5, 4 });\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vrev32_u16 (uint16x4_t a)\n+{\n+  return __builtin_shuffle (a, (uint16x4_t) { 1, 0, 3, 2 });\n+}\n+\n+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n+vrev32q_p8 (poly8x16_t a)\n+{\n+  return __builtin_shuffle (a,\n+      (uint8x16_t) { 3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12 });\n+}\n+\n+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))\n+vrev32q_p16 (poly16x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint16x8_t) { 1, 0, 3, 2, 5, 4, 7, 6 });\n+}\n+\n+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n+vrev32q_s8 (int8x16_t a)\n+{\n+  return __builtin_shuffle (a,\n+      (uint8x16_t) { 3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12 });\n+}\n+\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vrev32q_s16 (int16x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint16x8_t) { 1, 0, 3, 2, 5, 4, 7, 6 });\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vrev32q_u8 (uint8x16_t a)\n+{\n+  return __builtin_shuffle (a,\n+      (uint8x16_t) { 3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12 });\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vrev32q_u16 (uint16x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint16x8_t) { 1, 0, 3, 2, 5, 4, 7, 6 });\n+}\n+\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vrev64_f32 (float32x2_t a)\n+{\n+  return __builtin_shuffle (a, (uint32x2_t) { 1, 0 });\n+}\n+\n+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n+vrev64_p8 (poly8x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint8x8_t) { 7, 6, 5, 4, 3, 2, 1, 0 });\n+}\n+\n+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))\n+vrev64_p16 (poly16x4_t a)\n+{\n+  return __builtin_shuffle (a, (uint16x4_t) { 3, 2, 1, 0 });\n+}\n+\n+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n+vrev64_s8 (int8x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint8x8_t) { 7, 6, 5, 4, 3, 2, 1, 0 });\n+}\n+\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vrev64_s16 (int16x4_t a)\n+{\n+  return __builtin_shuffle (a, (uint16x4_t) { 3, 2, 1, 0 });\n+}\n+\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vrev64_s32 (int32x2_t a)\n+{\n+  return __builtin_shuffle (a, (uint32x2_t) { 1, 0 });\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vrev64_u8 (uint8x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint8x8_t) { 7, 6, 5, 4, 3, 2, 1, 0 });\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vrev64_u16 (uint16x4_t a)\n+{\n+  return __builtin_shuffle (a, (uint16x4_t) { 3, 2, 1, 0 });\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vrev64_u32 (uint32x2_t a)\n+{\n+  return __builtin_shuffle (a, (uint32x2_t) { 1, 0 });\n+}\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vrev64q_f32 (float32x4_t a)\n+{\n+  return __builtin_shuffle (a, (uint32x4_t) { 1, 0, 3, 2 });\n+}\n+\n+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n+vrev64q_p8 (poly8x16_t a)\n+{\n+  return __builtin_shuffle (a,\n+      (uint8x16_t) { 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8 });\n+}\n+\n+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))\n+vrev64q_p16 (poly16x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint16x8_t) { 3, 2, 1, 0, 7, 6, 5, 4 });\n+}\n+\n+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n+vrev64q_s8 (int8x16_t a)\n+{\n+  return __builtin_shuffle (a,\n+      (uint8x16_t) { 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8 });\n+}\n+\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vrev64q_s16 (int16x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint16x8_t) { 3, 2, 1, 0, 7, 6, 5, 4 });\n+}\n+\n+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n+vrev64q_s32 (int32x4_t a)\n+{\n+  return __builtin_shuffle (a, (uint32x4_t) { 1, 0, 3, 2 });\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vrev64q_u8 (uint8x16_t a)\n+{\n+  return __builtin_shuffle (a,\n+      (uint8x16_t) { 7, 6, 5, 4, 3, 2, 1, 0, 15, 14, 13, 12, 11, 10, 9, 8 });\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vrev64q_u16 (uint16x8_t a)\n+{\n+  return __builtin_shuffle (a, (uint16x8_t) { 3, 2, 1, 0, 7, 6, 5, 4 });\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vrev64q_u32 (uint32x4_t a)\n+{\n+  return __builtin_shuffle (a, (uint32x4_t) { 1, 0, 3, 2 });\n+}\n+\n /* vrnd  */\n \n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))"}, {"sha": "05c4f7ea543a40e14ee0b1c02e16cc927aafd99c", "filename": "gcc/config/aarch64/iterators.md", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/923fcec3d8427c6169979294ea5f2a5e11cfd4cf/gcc%2Fconfig%2Faarch64%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/923fcec3d8427c6169979294ea5f2a5e11cfd4cf/gcc%2Fconfig%2Faarch64%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fiterators.md?ref=923fcec3d8427c6169979294ea5f2a5e11cfd4cf", "patch": "@@ -271,6 +271,9 @@\n     UNSPEC_TRN1\t\t; Used in vector permute patterns.\n     UNSPEC_TRN2\t\t; Used in vector permute patterns.\n     UNSPEC_EXT\t\t; Used in aarch64-simd.md.\n+    UNSPEC_REV64\t; Used in vector reverse patterns (permute).\n+    UNSPEC_REV32\t; Used in vector reverse patterns (permute).\n+    UNSPEC_REV16\t; Used in vector reverse patterns (permute).\n     UNSPEC_AESE\t\t; Used in aarch64-simd.md.\n     UNSPEC_AESD         ; Used in aarch64-simd.md.\n     UNSPEC_AESMC        ; Used in aarch64-simd.md.\n@@ -896,6 +899,8 @@\n \t\t\t      UNSPEC_TRN1 UNSPEC_TRN2\n \t\t\t      UNSPEC_UZP1 UNSPEC_UZP2])\n \n+(define_int_iterator REVERSE [UNSPEC_REV64 UNSPEC_REV32 UNSPEC_REV16])\n+\n (define_int_iterator FRINT [UNSPEC_FRINTZ UNSPEC_FRINTP UNSPEC_FRINTM\n \t\t\t     UNSPEC_FRINTN UNSPEC_FRINTI UNSPEC_FRINTX\n \t\t\t     UNSPEC_FRINTA])\n@@ -1023,6 +1028,10 @@\n \t\t\t    (UNSPEC_TRN1 \"trn\") (UNSPEC_TRN2 \"trn\")\n \t\t\t    (UNSPEC_UZP1 \"uzp\") (UNSPEC_UZP2 \"uzp\")])\n \n+; op code for REV instructions (size within which elements are reversed).\n+(define_int_attr rev_op [(UNSPEC_REV64 \"64\") (UNSPEC_REV32 \"32\")\n+\t\t\t (UNSPEC_REV16 \"16\")])\n+\n (define_int_attr perm_hilo [(UNSPEC_ZIP1 \"1\") (UNSPEC_ZIP2 \"2\")\n \t\t\t    (UNSPEC_TRN1 \"1\") (UNSPEC_TRN2 \"2\")\n \t\t\t    (UNSPEC_UZP1 \"1\") (UNSPEC_UZP2 \"2\")])"}]}