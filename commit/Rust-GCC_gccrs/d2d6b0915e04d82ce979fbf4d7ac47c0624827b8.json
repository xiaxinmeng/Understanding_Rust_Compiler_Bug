{"sha": "d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "node_id": "C_kwDOANBUbNoAKGQyZDZiMDkxNWUwNGQ4MmNlOTc5ZmJmNGQ3YWM0N2MwNjI0ODI3Yjg", "commit": {"author": {"name": "Ju-Zhe Zhong", "email": "juzhe.zhong@rivai.ai", "date": "2023-02-03T07:08:18Z"}, "committer": {"name": "Kito Cheng", "email": "kito.cheng@sifive.com", "date": "2023-02-10T11:27:02Z"}, "message": "RISC-V: Add vrsub.vx C API tests\n\ngcc/testsuite/ChangeLog:\n\n\t* gcc.target/riscv/rvv/base/vrsub_vx_m_rv32-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_m_rv32-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_m_rv32-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_m_rv64-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_m_rv64-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_m_rv64-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_mu_rv32-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_mu_rv32-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_mu_rv32-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_mu_rv64-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_mu_rv64-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_mu_rv64-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_rv32-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_rv32-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_rv32-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_rv64-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_rv64-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_rv64-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tu_rv32-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tu_rv32-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tu_rv32-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tu_rv64-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tu_rv64-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tu_rv64-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tum_rv32-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tum_rv32-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tum_rv32-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tum_rv64-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tum_rv64-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tum_rv64-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tumu_rv32-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tumu_rv32-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tumu_rv32-3.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tumu_rv64-1.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tumu_rv64-2.c: New test.\n\t* gcc.target/riscv/rvv/base/vrsub_vx_tumu_rv64-3.c: New test.", "tree": {"sha": "59a936989a29e0ff33e40ef03b9671c45584ec9a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/59a936989a29e0ff33e40ef03b9671c45584ec9a"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "html_url": "https://github.com/Rust-GCC/gccrs/commit/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/comments", "author": {"login": "zhongjuzhe", "id": 66454988, "node_id": "MDQ6VXNlcjY2NDU0OTg4", "avatar_url": "https://avatars.githubusercontent.com/u/66454988?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhongjuzhe", "html_url": "https://github.com/zhongjuzhe", "followers_url": "https://api.github.com/users/zhongjuzhe/followers", "following_url": "https://api.github.com/users/zhongjuzhe/following{/other_user}", "gists_url": "https://api.github.com/users/zhongjuzhe/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhongjuzhe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhongjuzhe/subscriptions", "organizations_url": "https://api.github.com/users/zhongjuzhe/orgs", "repos_url": "https://api.github.com/users/zhongjuzhe/repos", "events_url": "https://api.github.com/users/zhongjuzhe/events{/privacy}", "received_events_url": "https://api.github.com/users/zhongjuzhe/received_events", "type": "User", "site_admin": false}, "committer": {"login": "kito-cheng", "id": 2723185, "node_id": "MDQ6VXNlcjI3MjMxODU=", "avatar_url": "https://avatars.githubusercontent.com/u/2723185?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kito-cheng", "html_url": "https://github.com/kito-cheng", "followers_url": "https://api.github.com/users/kito-cheng/followers", "following_url": "https://api.github.com/users/kito-cheng/following{/other_user}", "gists_url": "https://api.github.com/users/kito-cheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/kito-cheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kito-cheng/subscriptions", "organizations_url": "https://api.github.com/users/kito-cheng/orgs", "repos_url": "https://api.github.com/users/kito-cheng/repos", "events_url": "https://api.github.com/users/kito-cheng/events{/privacy}", "received_events_url": "https://api.github.com/users/kito-cheng/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "fe9e2eccb9e9e82779fa60a8a2f5cb0f62ac399e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fe9e2eccb9e9e82779fa60a8a2f5cb0f62ac399e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/fe9e2eccb9e9e82779fa60a8a2f5cb0f62ac399e"}], "stats": {"total": 10458, "additions": 10458, "deletions": 0}, "files": [{"sha": "87627d73b6c364b61e3070ac6a1fec18f33aaefa", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_m_rv32-1.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv32-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv32-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv32-1.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_m(vbool64_t mask,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_m(vbool32_t mask,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_m(vbool16_t mask,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_m(vbool8_t mask,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_m(vbool4_t mask,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_m(vbool2_t mask,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_m(vbool1_t mask,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_m(vbool64_t mask,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_m(vbool32_t mask,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_m(vbool16_t mask,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_m(vbool8_t mask,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_m(vbool4_t mask,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_m(vbool2_t mask,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_m(vbool64_t mask,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_m(vbool32_t mask,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_m(vbool16_t mask,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_m(vbool8_t mask,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_m(vbool4_t mask,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_m(vbool64_t mask,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_m(vbool32_t mask,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_m(vbool16_t mask,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_m(vbool8_t mask,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_m(vbool64_t mask,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_m(vbool32_t mask,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_m(vbool16_t mask,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_m(vbool8_t mask,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_m(vbool4_t mask,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_m(vbool2_t mask,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_m(vbool1_t mask,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_m(vbool64_t mask,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_m(vbool32_t mask,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_m(vbool16_t mask,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_m(vbool8_t mask,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_m(vbool4_t mask,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_m(vbool2_t mask,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_m(vbool64_t mask,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_m(vbool32_t mask,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_m(vbool16_t mask,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_m(vbool8_t mask,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_m(vbool4_t mask,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_m(vbool64_t mask,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_m(vbool32_t mask,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_m(vbool16_t mask,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_m(vbool8_t mask,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "ba9658a320eba14bdfb5ec84c3f8cc4537c1cd61", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_m_rv32-2.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv32-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv32-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv32-2.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_m(vbool64_t mask,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_m(vbool32_t mask,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_m(vbool16_t mask,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_m(vbool8_t mask,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_m(vbool4_t mask,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_m(vbool2_t mask,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_m(vbool1_t mask,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_m(vbool64_t mask,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_m(vbool32_t mask,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_m(vbool16_t mask,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_m(vbool8_t mask,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_m(vbool4_t mask,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_m(vbool2_t mask,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_m(vbool64_t mask,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_m(vbool32_t mask,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_m(vbool16_t mask,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_m(vbool8_t mask,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_m(vbool4_t mask,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_m(vbool64_t mask,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_m(vbool32_t mask,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_m(vbool16_t mask,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_m(vbool8_t mask,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_m(vbool64_t mask,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_m(vbool32_t mask,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_m(vbool16_t mask,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_m(vbool8_t mask,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_m(vbool4_t mask,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_m(vbool2_t mask,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_m(vbool1_t mask,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_m(vbool64_t mask,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_m(vbool32_t mask,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_m(vbool16_t mask,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_m(vbool8_t mask,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_m(vbool4_t mask,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_m(vbool2_t mask,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_m(vbool64_t mask,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_m(vbool32_t mask,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_m(vbool16_t mask,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_m(vbool8_t mask,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_m(vbool4_t mask,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_m(vbool64_t mask,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_m(vbool32_t mask,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_m(vbool16_t mask,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_m(vbool8_t mask,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_m(mask,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "53829b66916cceb75feba9a1b2a0b2944922d856", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_m_rv32-3.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv32-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv32-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv32-3.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_m(vbool64_t mask,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_m(vbool32_t mask,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_m(vbool16_t mask,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_m(vbool8_t mask,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_m(vbool4_t mask,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_m(vbool2_t mask,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_m(vbool1_t mask,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_m(vbool64_t mask,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_m(vbool32_t mask,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_m(vbool16_t mask,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_m(vbool8_t mask,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_m(vbool4_t mask,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_m(vbool2_t mask,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_m(vbool64_t mask,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_m(vbool32_t mask,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_m(vbool16_t mask,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_m(vbool8_t mask,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_m(vbool4_t mask,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_m(vbool64_t mask,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_m(vbool32_t mask,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_m(vbool16_t mask,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_m(vbool8_t mask,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_m(vbool64_t mask,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_m(vbool32_t mask,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_m(vbool16_t mask,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_m(vbool8_t mask,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_m(vbool4_t mask,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_m(vbool2_t mask,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_m(vbool1_t mask,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_m(vbool64_t mask,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_m(vbool32_t mask,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_m(vbool16_t mask,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_m(vbool8_t mask,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_m(vbool4_t mask,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_m(vbool2_t mask,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_m(vbool64_t mask,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_m(vbool32_t mask,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_m(vbool16_t mask,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_m(vbool8_t mask,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_m(vbool4_t mask,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_m(vbool64_t mask,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_m(vbool32_t mask,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_m(vbool16_t mask,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_m(vbool8_t mask,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_m(mask,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "5c011c11ad54a681ed3e82ea4061d2c42bf516db", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_m_rv64-1.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv64-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv64-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv64-1.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_m(vbool64_t mask,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_m(vbool32_t mask,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_m(vbool16_t mask,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_m(vbool8_t mask,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_m(vbool4_t mask,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_m(vbool2_t mask,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_m(vbool1_t mask,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_m(vbool64_t mask,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_m(vbool32_t mask,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_m(vbool16_t mask,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_m(vbool8_t mask,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_m(vbool4_t mask,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_m(vbool2_t mask,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_m(vbool64_t mask,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_m(vbool32_t mask,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_m(vbool16_t mask,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_m(vbool8_t mask,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_m(vbool4_t mask,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_m(vbool64_t mask,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_m(vbool32_t mask,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_m(vbool16_t mask,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_m(vbool8_t mask,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_m(vbool64_t mask,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_m(vbool32_t mask,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_m(vbool16_t mask,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_m(vbool8_t mask,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_m(vbool4_t mask,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_m(vbool2_t mask,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_m(vbool1_t mask,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_m(vbool64_t mask,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_m(vbool32_t mask,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_m(vbool16_t mask,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_m(vbool8_t mask,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_m(vbool4_t mask,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_m(vbool2_t mask,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_m(vbool64_t mask,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_m(vbool32_t mask,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_m(vbool16_t mask,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_m(vbool8_t mask,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_m(vbool4_t mask,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_m(vbool64_t mask,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_m(vbool32_t mask,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_m(vbool16_t mask,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_m(mask,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_m(vbool8_t mask,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_m(mask,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "ca47a60deeb14d33625fc3f0c92d1ba061d08e68", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_m_rv64-2.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv64-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv64-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv64-2.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_m(vbool64_t mask,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_m(vbool32_t mask,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_m(vbool16_t mask,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_m(vbool8_t mask,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_m(vbool4_t mask,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_m(vbool2_t mask,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_m(vbool1_t mask,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_m(vbool64_t mask,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_m(vbool32_t mask,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_m(vbool16_t mask,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_m(vbool8_t mask,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_m(vbool4_t mask,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_m(vbool2_t mask,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_m(vbool64_t mask,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_m(vbool32_t mask,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_m(vbool16_t mask,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_m(vbool8_t mask,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_m(vbool4_t mask,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_m(vbool64_t mask,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_m(vbool32_t mask,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_m(vbool16_t mask,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_m(vbool8_t mask,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_m(vbool64_t mask,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_m(vbool32_t mask,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_m(vbool16_t mask,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_m(vbool8_t mask,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_m(vbool4_t mask,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_m(vbool2_t mask,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_m(vbool1_t mask,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_m(vbool64_t mask,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_m(vbool32_t mask,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_m(vbool16_t mask,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_m(vbool8_t mask,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_m(vbool4_t mask,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_m(vbool2_t mask,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_m(vbool64_t mask,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_m(vbool32_t mask,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_m(vbool16_t mask,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_m(vbool8_t mask,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_m(vbool4_t mask,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_m(vbool64_t mask,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_m(vbool32_t mask,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_m(vbool16_t mask,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_m(mask,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_m(vbool8_t mask,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_m(mask,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "8c1016f572def894499bb7fc21cda620bb4f0a45", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_m_rv64-3.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv64-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv64-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_m_rv64-3.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_m(vbool64_t mask,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_m(vbool32_t mask,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_m(vbool16_t mask,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_m(vbool8_t mask,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_m(vbool4_t mask,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_m(vbool2_t mask,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_m(vbool1_t mask,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_m(vbool64_t mask,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_m(vbool32_t mask,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_m(vbool16_t mask,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_m(vbool8_t mask,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_m(vbool4_t mask,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_m(vbool2_t mask,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_m(vbool64_t mask,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_m(vbool32_t mask,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_m(vbool16_t mask,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_m(vbool8_t mask,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_m(vbool4_t mask,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_m(vbool64_t mask,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_m(vbool32_t mask,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_m(vbool16_t mask,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_m(vbool8_t mask,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_m(vbool64_t mask,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_m(vbool32_t mask,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_m(vbool16_t mask,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_m(vbool8_t mask,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_m(vbool4_t mask,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_m(vbool2_t mask,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_m(vbool1_t mask,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_m(vbool64_t mask,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_m(vbool32_t mask,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_m(vbool16_t mask,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_m(vbool8_t mask,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_m(vbool4_t mask,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_m(vbool2_t mask,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_m(vbool64_t mask,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_m(vbool32_t mask,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_m(vbool16_t mask,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_m(vbool8_t mask,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_m(vbool4_t mask,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_m(vbool64_t mask,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_m(vbool32_t mask,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_m(vbool16_t mask,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_m(mask,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_m(vbool8_t mask,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_m(mask,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "9495ad55a83495fe40480ab07d4c3452a0acb2c2", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_mu_rv32-1.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv32-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv32-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv32-1.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_mu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_mu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_mu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_mu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_mu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_mu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_mu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_mu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_mu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_mu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_mu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_mu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_mu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_mu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_mu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_mu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_mu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_mu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_mu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_mu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_mu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_mu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_mu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_mu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_mu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_mu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_mu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_mu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_mu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_mu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_mu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_mu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_mu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_mu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_mu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_mu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_mu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_mu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_mu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_mu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_mu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_mu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_mu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_mu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "58f3406fbc8497a0d2210ab5686e56777d407cb7", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_mu_rv32-2.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv32-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv32-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv32-2.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_mu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_mu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_mu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_mu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_mu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_mu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_mu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_mu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_mu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_mu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_mu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_mu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_mu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_mu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_mu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_mu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_mu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_mu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_mu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_mu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_mu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_mu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_mu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_mu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_mu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_mu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_mu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_mu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_mu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_mu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_mu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_mu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_mu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_mu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_mu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_mu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_mu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_mu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_mu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_mu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_mu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_mu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_mu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_mu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "5fcb9b0ac9bd043dca99b4f11ee7960e41b02371", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_mu_rv32-3.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv32-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv32-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv32-3.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_mu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_mu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_mu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_mu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_mu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_mu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_mu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_mu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_mu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_mu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_mu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_mu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_mu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_mu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_mu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_mu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_mu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_mu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_mu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_mu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_mu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_mu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_mu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_mu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_mu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_mu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_mu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_mu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_mu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_mu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_mu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_mu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_mu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_mu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_mu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_mu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_mu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_mu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_mu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_mu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_mu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_mu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_mu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_mu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "c5616f4354e0dc0d6eef68139f59c93e9623b508", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_mu_rv64-1.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv64-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv64-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv64-1.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_mu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_mu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_mu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_mu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_mu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_mu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_mu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_mu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_mu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_mu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_mu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_mu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_mu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_mu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_mu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_mu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_mu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_mu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_mu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_mu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_mu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_mu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_mu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_mu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_mu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_mu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_mu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_mu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_mu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_mu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_mu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_mu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_mu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_mu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_mu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_mu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_mu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_mu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_mu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_mu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_mu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_mu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_mu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_mu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_mu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "5ae1d0c2410046c1bd4be47abf4b3dfcc9753371", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_mu_rv64-2.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv64-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv64-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv64-2.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_mu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_mu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_mu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_mu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_mu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_mu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_mu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_mu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_mu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_mu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_mu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_mu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_mu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_mu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_mu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_mu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_mu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_mu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_mu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_mu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_mu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_mu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_mu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_mu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_mu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_mu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_mu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_mu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_mu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_mu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_mu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_mu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_mu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_mu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_mu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_mu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_mu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_mu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_mu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_mu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_mu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_mu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_mu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_mu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_mu(mask,merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "00fcc9786f7ea803b9b713a3997ced24cb9c5c18", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_mu_rv64-3.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv64-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv64-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_mu_rv64-3.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_mu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_mu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_mu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_mu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_mu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_mu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_mu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_mu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_mu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_mu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_mu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_mu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_mu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_mu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_mu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_mu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_mu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_mu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_mu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_mu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_mu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_mu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_mu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_mu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_mu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_mu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_mu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_mu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_mu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_mu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_mu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_mu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_mu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_mu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_mu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_mu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_mu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_mu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_mu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_mu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_mu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_mu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_mu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_mu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_mu(mask,merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*t[au],\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "4004e7563df7d75a593446fd5edd89c1e34449b6", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_rv32-1.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv32-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv32-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv32-1.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8(vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8(op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4(vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4(op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2(vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2(op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1(vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1(op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2(vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2(op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4(vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4(op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8(vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8(op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4(vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4(op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2(vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2(op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1(vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1(op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2(vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2(op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4(vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4(op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8(vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8(op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2(vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2(op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1(vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1(op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2(vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2(op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4(vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4(op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8(vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8(op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1(vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1(op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2(vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2(op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4(vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4(op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8(vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8(op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8(vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8(op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4(vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4(op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2(vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2(op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1(vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1(op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2(vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2(op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4(vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4(op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8(vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8(op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4(vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4(op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2(vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2(op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1(vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1(op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2(vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2(op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4(vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4(op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8(vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8(op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2(vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2(op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1(vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1(op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2(vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2(op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4(vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4(op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8(vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8(op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1(vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1(op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2(vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2(op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4(vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4(op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8(vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8(op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+} 8 } } */"}, {"sha": "383b1be388808cb5b6748c23789bc8bae6817665", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_rv32-2.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv32-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv32-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv32-2.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8(vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8(op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4(vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4(op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2(vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2(op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1(vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1(op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2(vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2(op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4(vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4(op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8(vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8(op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4(vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4(op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2(vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2(op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1(vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1(op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2(vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2(op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4(vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4(op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8(vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8(op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2(vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2(op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1(vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1(op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2(vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2(op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4(vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4(op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8(vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8(op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1(vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1(op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2(vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2(op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4(vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4(op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8(vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8(op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8(vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8(op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4(vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4(op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2(vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2(op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1(vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1(op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2(vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2(op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4(vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4(op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8(vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8(op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4(vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4(op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2(vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2(op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1(vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1(op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2(vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2(op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4(vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4(op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8(vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8(op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2(vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2(op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1(vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1(op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2(vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2(op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4(vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4(op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8(vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8(op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1(vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1(op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2(vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2(op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4(vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4(op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8(vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8(op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+} 8 } } */"}, {"sha": "42c5bec37b8424995c3f568bfda6462d337fc92f", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_rv32-3.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv32-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv32-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv32-3.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8(vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8(op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4(vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4(op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2(vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2(op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1(vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1(op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2(vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2(op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4(vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4(op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8(vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8(op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4(vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4(op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2(vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2(op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1(vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1(op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2(vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2(op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4(vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4(op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8(vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8(op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2(vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2(op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1(vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1(op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2(vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2(op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4(vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4(op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8(vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8(op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1(vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1(op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2(vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2(op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4(vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4(op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8(vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8(op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8(vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8(op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4(vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4(op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2(vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2(op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1(vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1(op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2(vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2(op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4(vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4(op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8(vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8(op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4(vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4(op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2(vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2(op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1(vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1(op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2(vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2(op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4(vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4(op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8(vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8(op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2(vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2(op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1(vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1(op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2(vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2(op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4(vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4(op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8(vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8(op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1(vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1(op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2(vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2(op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4(vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4(op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8(vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8(op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+} 8 } } */"}, {"sha": "a110de3c4702c05f0ce84a99d9b44ee6a3611be0", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_rv64-1.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv64-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv64-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv64-1.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8(vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8(op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4(vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4(op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2(vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2(op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1(vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1(op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2(vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2(op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4(vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4(op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8(vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8(op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4(vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4(op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2(vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2(op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1(vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1(op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2(vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2(op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4(vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4(op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8(vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8(op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2(vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2(op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1(vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1(op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2(vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2(op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4(vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4(op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8(vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8(op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1(vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1(op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2(vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2(op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4(vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4(op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8(vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8(op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8(vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8(op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4(vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4(op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2(vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2(op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1(vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1(op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2(vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2(op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4(vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4(op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8(vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8(op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4(vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4(op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2(vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2(op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1(vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1(op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2(vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2(op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4(vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4(op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8(vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8(op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2(vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2(op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1(vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1(op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2(vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2(op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4(vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4(op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8(vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8(op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1(vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1(op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2(vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2(op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4(vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4(op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8(vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8(op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */"}, {"sha": "55cae6db56a1ba1fa7449fde6623488d1ae269ca", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_rv64-2.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv64-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv64-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv64-2.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8(vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8(op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4(vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4(op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2(vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2(op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1(vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1(op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2(vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2(op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4(vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4(op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8(vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8(op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4(vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4(op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2(vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2(op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1(vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1(op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2(vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2(op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4(vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4(op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8(vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8(op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2(vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2(op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1(vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1(op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2(vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2(op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4(vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4(op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8(vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8(op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1(vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1(op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2(vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2(op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4(vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4(op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8(vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8(op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8(vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8(op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4(vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4(op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2(vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2(op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1(vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1(op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2(vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2(op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4(vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4(op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8(vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8(op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4(vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4(op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2(vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2(op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1(vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1(op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2(vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2(op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4(vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4(op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8(vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8(op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2(vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2(op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1(vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1(op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2(vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2(op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4(vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4(op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8(vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8(op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1(vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1(op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2(vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2(op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4(vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4(op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8(vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8(op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */"}, {"sha": "951cfed6af660728021d385f1c53c184903cd61b", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_rv64-3.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv64-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv64-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_rv64-3.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8(vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8(op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4(vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4(op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2(vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2(op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1(vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1(op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2(vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2(op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4(vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4(op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8(vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8(op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4(vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4(op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2(vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2(op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1(vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1(op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2(vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2(op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4(vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4(op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8(vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8(op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2(vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2(op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1(vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1(op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2(vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2(op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4(vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4(op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8(vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8(op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1(vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1(op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2(vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2(op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4(vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4(op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8(vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8(op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8(vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8(op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4(vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4(op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2(vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2(op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1(vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1(op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2(vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2(op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4(vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4(op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8(vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8(op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4(vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4(op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2(vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2(op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1(vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1(op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2(vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2(op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4(vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4(op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8(vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8(op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2(vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2(op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1(vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1(op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2(vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2(op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4(vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4(op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8(vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8(op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1(vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1(op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2(vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2(op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4(vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4(op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8(vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8(op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*t[au],\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */"}, {"sha": "44517845ad5c68367647729393a9f98d67c77ca9", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tu_rv32-1.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv32-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv32-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv32-1.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tu(vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tu(vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tu(vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tu(vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tu(vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tu(vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tu(vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tu(vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tu(vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tu(vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tu(vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tu(vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tu(vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tu(vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tu(vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tu(vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tu(vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tu(vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tu(vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tu(vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tu(vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tu(vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tu(vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tu(vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tu(vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tu(vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tu(vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tu(vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tu(vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tu(vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tu(vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tu(vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tu(vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tu(vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tu(vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tu(vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tu(vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tu(vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tu(vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tu(vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tu(vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tu(vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tu(vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tu(vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+} 8 } } */"}, {"sha": "e80729963a35b6d8340927e9f368cc2285906ac5", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tu_rv32-2.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv32-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv32-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv32-2.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tu(vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tu(vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tu(vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tu(vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tu(vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tu(vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tu(vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tu(vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tu(vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tu(vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tu(vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tu(vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tu(vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tu(vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tu(vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tu(vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tu(vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tu(vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tu(vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tu(vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tu(vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tu(vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tu(vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tu(vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tu(vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tu(vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tu(vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tu(vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tu(vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tu(vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tu(vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tu(vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tu(vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tu(vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tu(vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tu(vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tu(vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tu(vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tu(vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tu(vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tu(vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tu(vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tu(vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tu(vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+} 8 } } */"}, {"sha": "361368bb5471b055c398ee1f45111875ef045b64", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tu_rv32-3.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv32-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv32-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv32-3.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tu(vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tu(vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tu(vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tu(vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tu(vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tu(vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tu(vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tu(vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tu(vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tu(vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tu(vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tu(vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tu(vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tu(vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tu(vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tu(vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tu(vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tu(vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tu(vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tu(vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tu(vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tu(vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tu(vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tu(vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tu(vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tu(vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tu(vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tu(vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tu(vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tu(vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tu(vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tu(vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tu(vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tu(vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tu(vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tu(vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tu(vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tu(vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tu(vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tu(vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tu(vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tu(vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tu(vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tu(vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+} 8 } } */"}, {"sha": "5aa7f06f324d1a355c31f9c8688a8e81a6ad1688", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tu_rv64-1.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv64-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv64-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv64-1.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tu(vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tu(vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tu(vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tu(vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tu(vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tu(vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tu(vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tu(vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tu(vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tu(vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tu(vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tu(vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tu(vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tu(vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tu(vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tu(vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tu(vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tu(vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tu(vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tu(vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tu(vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tu(vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tu(vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tu(vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tu(vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tu(vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tu(vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tu(vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tu(vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tu(vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tu(vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tu(vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tu(vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tu(vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tu(vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tu(vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tu(vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tu(vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tu(vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tu(vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tu(vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tu(vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tu(vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tu(merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tu(vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tu(merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */"}, {"sha": "10c5a250390c4158b090562407d7e9f71e5271ff", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tu_rv64-2.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv64-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv64-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv64-2.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tu(vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tu(vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tu(vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tu(vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tu(vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tu(vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tu(vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tu(vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tu(vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tu(vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tu(vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tu(vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tu(vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tu(vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tu(vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tu(vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tu(vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tu(vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tu(vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tu(vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tu(vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tu(vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tu(vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tu(vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tu(vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tu(vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tu(vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tu(vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tu(vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tu(vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tu(vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tu(vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tu(vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tu(vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tu(vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tu(vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tu(vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tu(vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tu(vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tu(vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tu(vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tu(vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tu(vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tu(merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tu(vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tu(merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */"}, {"sha": "133625685a40bdf54646bc47a9794a64c61fa1d9", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tu_rv64-3.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv64-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv64-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tu_rv64-3.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tu(vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tu(vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tu(vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tu(vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tu(vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tu(vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tu(vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tu(vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tu(vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tu(vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tu(vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tu(vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tu(vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tu(vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tu(vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tu(vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tu(vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tu(vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tu(vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tu(vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tu(vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tu(vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tu(vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tu(vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tu(vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tu(vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tu(vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tu(vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tu(vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tu(vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tu(vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tu(vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tu(vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tu(vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tu(vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tu(vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tu(vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tu(vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tu(vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tu(vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tu(vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tu(vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tu(vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tu(merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tu(vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tu(merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+} 2 } } */"}, {"sha": "bf8692f34acdb7bb88d2682ecebcd390a59c9615", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tum_rv32-1.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv32-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv32-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv32-1.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tum(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tum(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tum(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tum(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tum(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tum(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tum(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tum(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tum(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tum(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tum(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tum(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tum(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tum(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tum(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tum(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tum(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tum(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tum(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tum(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tum(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tum(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tum(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tum(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tum(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tum(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tum(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tum(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tum(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tum(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tum(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tum(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tum(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tum(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tum(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tum(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tum(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tum(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tum(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tum(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tum(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tum(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tum(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tum(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "284e731434535e3c09b420816b4e8d0ba53e7359", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tum_rv32-2.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv32-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv32-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv32-2.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tum(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tum(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tum(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tum(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tum(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tum(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tum(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tum(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tum(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tum(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tum(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tum(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tum(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tum(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tum(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tum(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tum(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tum(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tum(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tum(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tum(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tum(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tum(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tum(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tum(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tum(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tum(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tum(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tum(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tum(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tum(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tum(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tum(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tum(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tum(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tum(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tum(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tum(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tum(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tum(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tum(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tum(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tum(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tum(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "37b82b3f01ad41adda99e226b6360118c90513c4", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tum_rv32-3.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv32-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv32-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv32-3.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tum(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tum(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tum(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tum(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tum(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tum(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tum(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tum(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tum(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tum(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tum(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tum(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tum(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tum(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tum(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tum(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tum(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tum(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tum(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tum(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tum(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tum(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tum(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tum(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tum(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tum(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tum(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tum(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tum(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tum(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tum(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tum(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tum(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tum(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tum(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tum(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tum(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tum(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tum(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tum(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tum(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tum(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tum(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tum(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "f60bd185a758f4475a8b16a4167cc35bb46b1dae", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tum_rv64-1.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv64-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv64-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv64-1.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tum(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tum(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tum(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tum(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tum(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tum(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tum(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tum(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tum(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tum(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tum(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tum(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tum(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tum(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tum(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tum(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tum(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tum(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tum(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tum(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tum(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tum(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tum(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tum(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tum(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tum(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tum(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tum(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tum(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tum(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tum(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tum(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tum(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tum(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tum(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tum(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tum(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tum(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tum(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tum(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tum(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tum(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tum(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tum(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tum(mask,merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "73ae66f0d9ae53ce570524e7a63f5fb5c272be8f", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tum_rv64-2.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv64-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv64-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv64-2.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tum(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tum(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tum(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tum(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tum(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tum(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tum(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tum(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tum(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tum(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tum(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tum(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tum(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tum(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tum(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tum(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tum(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tum(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tum(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tum(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tum(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tum(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tum(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tum(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tum(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tum(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tum(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tum(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tum(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tum(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tum(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tum(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tum(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tum(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tum(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tum(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tum(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tum(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tum(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tum(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tum(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tum(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tum(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tum(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tum(mask,merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "ba715a09fe73857f7fe13087c1b2070dfc92029e", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tum_rv64-3.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv64-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv64-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tum_rv64-3.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tum(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tum(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tum(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tum(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tum(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tum(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tum(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tum(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tum(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tum(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tum(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tum(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tum(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tum(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tum(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tum(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tum(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tum(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tum(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tum(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tum(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tum(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tum(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tum(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tum(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tum(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tum(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tum(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tum(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tum(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tum(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tum(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tum(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tum(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tum(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tum(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tum(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tum(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tum(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tum(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tum(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tum(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tum(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tum(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tum(mask,merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*tu,\\s*m[au]\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "54e57beedc51cbbbca463d01d65c9ef69aa223e5", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tumu_rv32-1.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv32-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv32-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv32-1.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tumu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tumu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tumu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tumu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tumu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tumu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tumu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tumu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tumu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tumu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tumu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tumu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tumu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tumu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tumu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tumu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tumu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tumu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tumu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tumu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tumu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tumu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tumu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tumu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tumu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tumu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tumu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tumu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tumu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tumu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tumu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tumu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tumu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tumu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tumu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tumu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tumu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tumu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tumu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tumu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tumu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tumu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tumu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tumu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "b8ca8cf1485ea3c89a8732ed73f6be0413213e6d", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tumu_rv32-2.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv32-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv32-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv32-2.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tumu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tumu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tumu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tumu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tumu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tumu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tumu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tumu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tumu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tumu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tumu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tumu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tumu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tumu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tumu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tumu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tumu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tumu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tumu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tumu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tumu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tumu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tumu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tumu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tumu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tumu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tumu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tumu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tumu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tumu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tumu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tumu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tumu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tumu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tumu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tumu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tumu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tumu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tumu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tumu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tumu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tumu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tumu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tumu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "f5dc22b77d888df56b5cfc753e874b7be453b89d", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tumu_rv32-3.c", "status": "added", "additions": 289, "deletions": 0, "changes": 289, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv32-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv32-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv32-3.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,289 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tumu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tumu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tumu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tumu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tumu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tumu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tumu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tumu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tumu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tumu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tumu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tumu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tumu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tumu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tumu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tumu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tumu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tumu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tumu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tumu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tumu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tumu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tumu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tumu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tumu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tumu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tumu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tumu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tumu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tumu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tumu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tumu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tumu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tumu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tumu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tumu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tumu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tumu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tumu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tumu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tumu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tumu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tumu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tumu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsub\\.vv\\s+v[0-9]+,\\s*v[0-9]+,\\s*v[0-9]+,\\s*v0.t} 8 } } */"}, {"sha": "100a8fe2aecd494c87a098fcb409710cc24dc1d1", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tumu_rv64-1.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv64-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv64-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv64-1.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tumu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tumu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tumu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tumu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tumu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tumu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tumu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tumu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tumu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tumu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tumu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tumu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tumu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tumu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tumu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tumu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tumu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tumu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tumu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tumu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tumu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tumu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tumu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tumu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tumu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tumu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tumu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tumu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tumu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tumu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tumu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tumu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tumu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tumu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tumu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tumu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tumu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tumu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tumu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tumu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tumu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tumu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tumu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tumu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tumu(mask,merge,op1,op2,vl);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "487ccf3516adc9dcf592ef7a88c95d5abd765e30", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tumu_rv64-2.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv64-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv64-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv64-2.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tumu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tumu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tumu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tumu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tumu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tumu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tumu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tumu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tumu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tumu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tumu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tumu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tumu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tumu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tumu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tumu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tumu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tumu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tumu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tumu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tumu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tumu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tumu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tumu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tumu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tumu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tumu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tumu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tumu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tumu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tumu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tumu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tumu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tumu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tumu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tumu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tumu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tumu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tumu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tumu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tumu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tumu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tumu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tumu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tumu(mask,merge,op1,op2,31);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e8,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e16,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e32,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetivli\\s+zero,\\s*31,\\s*e64,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}, {"sha": "8be1abab9ea2b6bc99eec76aacdf02446777ca13", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/vrsub_vx_tumu_rv64-3.c", "status": "added", "additions": 292, "deletions": 0, "changes": 292, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv64-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d2d6b0915e04d82ce979fbf4d7ac47c0624827b8/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv64-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fvrsub_vx_tumu_rv64-3.c?ref=d2d6b0915e04d82ce979fbf4d7ac47c0624827b8", "patch": "@@ -0,0 +1,292 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv64gcv -mabi=lp64d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+vint8mf8_t test___riscv_vrsub_vx_i8mf8_tumu(vbool64_t mask,vint8mf8_t merge,vint8mf8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf4_t test___riscv_vrsub_vx_i8mf4_tumu(vbool32_t mask,vint8mf4_t merge,vint8mf4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8mf2_t test___riscv_vrsub_vx_i8mf2_tumu(vbool16_t mask,vint8mf2_t merge,vint8mf2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m1_t test___riscv_vrsub_vx_i8m1_tumu(vbool8_t mask,vint8m1_t merge,vint8m1_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m2_t test___riscv_vrsub_vx_i8m2_tumu(vbool4_t mask,vint8m2_t merge,vint8m2_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m4_t test___riscv_vrsub_vx_i8m4_tumu(vbool2_t mask,vint8m4_t merge,vint8m4_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint8m8_t test___riscv_vrsub_vx_i8m8_tumu(vbool1_t mask,vint8m8_t merge,vint8m8_t op1,int8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i8m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf4_t test___riscv_vrsub_vx_i16mf4_tumu(vbool64_t mask,vint16mf4_t merge,vint16mf4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16mf2_t test___riscv_vrsub_vx_i16mf2_tumu(vbool32_t mask,vint16mf2_t merge,vint16mf2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m1_t test___riscv_vrsub_vx_i16m1_tumu(vbool16_t mask,vint16m1_t merge,vint16m1_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m2_t test___riscv_vrsub_vx_i16m2_tumu(vbool8_t mask,vint16m2_t merge,vint16m2_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m4_t test___riscv_vrsub_vx_i16m4_tumu(vbool4_t mask,vint16m4_t merge,vint16m4_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint16m8_t test___riscv_vrsub_vx_i16m8_tumu(vbool2_t mask,vint16m8_t merge,vint16m8_t op1,int16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i16m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32mf2_t test___riscv_vrsub_vx_i32mf2_tumu(vbool64_t mask,vint32mf2_t merge,vint32mf2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m1_t test___riscv_vrsub_vx_i32m1_tumu(vbool32_t mask,vint32m1_t merge,vint32m1_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m2_t test___riscv_vrsub_vx_i32m2_tumu(vbool16_t mask,vint32m2_t merge,vint32m2_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m4_t test___riscv_vrsub_vx_i32m4_tumu(vbool8_t mask,vint32m4_t merge,vint32m4_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint32m8_t test___riscv_vrsub_vx_i32m8_tumu(vbool4_t mask,vint32m8_t merge,vint32m8_t op1,int32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i32m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m1_t test___riscv_vrsub_vx_i64m1_tumu(vbool64_t mask,vint64m1_t merge,vint64m1_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m2_t test___riscv_vrsub_vx_i64m2_tumu(vbool32_t mask,vint64m2_t merge,vint64m2_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m4_t test___riscv_vrsub_vx_i64m4_tumu(vbool16_t mask,vint64m4_t merge,vint64m4_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vint64m8_t test___riscv_vrsub_vx_i64m8_tumu(vbool8_t mask,vint64m8_t merge,vint64m8_t op1,int64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_i64m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf8_t test___riscv_vrsub_vx_u8mf8_tumu(vbool64_t mask,vuint8mf8_t merge,vuint8mf8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf4_t test___riscv_vrsub_vx_u8mf4_tumu(vbool32_t mask,vuint8mf4_t merge,vuint8mf4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8mf2_t test___riscv_vrsub_vx_u8mf2_tumu(vbool16_t mask,vuint8mf2_t merge,vuint8mf2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m1_t test___riscv_vrsub_vx_u8m1_tumu(vbool8_t mask,vuint8m1_t merge,vuint8m1_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m2_t test___riscv_vrsub_vx_u8m2_tumu(vbool4_t mask,vuint8m2_t merge,vuint8m2_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m4_t test___riscv_vrsub_vx_u8m4_tumu(vbool2_t mask,vuint8m4_t merge,vuint8m4_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint8m8_t test___riscv_vrsub_vx_u8m8_tumu(vbool1_t mask,vuint8m8_t merge,vuint8m8_t op1,uint8_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u8m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf4_t test___riscv_vrsub_vx_u16mf4_tumu(vbool64_t mask,vuint16mf4_t merge,vuint16mf4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16mf2_t test___riscv_vrsub_vx_u16mf2_tumu(vbool32_t mask,vuint16mf2_t merge,vuint16mf2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m1_t test___riscv_vrsub_vx_u16m1_tumu(vbool16_t mask,vuint16m1_t merge,vuint16m1_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m2_t test___riscv_vrsub_vx_u16m2_tumu(vbool8_t mask,vuint16m2_t merge,vuint16m2_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m4_t test___riscv_vrsub_vx_u16m4_tumu(vbool4_t mask,vuint16m4_t merge,vuint16m4_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint16m8_t test___riscv_vrsub_vx_u16m8_tumu(vbool2_t mask,vuint16m8_t merge,vuint16m8_t op1,uint16_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u16m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32mf2_t test___riscv_vrsub_vx_u32mf2_tumu(vbool64_t mask,vuint32mf2_t merge,vuint32mf2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32mf2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m1_t test___riscv_vrsub_vx_u32m1_tumu(vbool32_t mask,vuint32m1_t merge,vuint32m1_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m2_t test___riscv_vrsub_vx_u32m2_tumu(vbool16_t mask,vuint32m2_t merge,vuint32m2_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m4_t test___riscv_vrsub_vx_u32m4_tumu(vbool8_t mask,vuint32m4_t merge,vuint32m4_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint32m8_t test___riscv_vrsub_vx_u32m8_tumu(vbool4_t mask,vuint32m8_t merge,vuint32m8_t op1,uint32_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u32m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m1_t test___riscv_vrsub_vx_u64m1_tumu(vbool64_t mask,vuint64m1_t merge,vuint64m1_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m1_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m2_t test___riscv_vrsub_vx_u64m2_tumu(vbool32_t mask,vuint64m2_t merge,vuint64m2_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m2_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m4_t test___riscv_vrsub_vx_u64m4_tumu(vbool16_t mask,vuint64m4_t merge,vuint64m4_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m4_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+vuint64m8_t test___riscv_vrsub_vx_u64m8_tumu(vbool8_t mask,vuint64m8_t merge,vuint64m8_t op1,uint64_t op2,size_t vl)\n+{\n+    return __riscv_vrsub_vx_u64m8_tumu(mask,merge,op1,op2,32);\n+}\n+\n+\n+\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e8,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e16,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*mf2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e32,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m1,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m2,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m4,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */\n+/* { dg-final { scan-assembler-times {vsetvli\\s+zero,\\s*[a-x0-9]+,\\s*e64,\\s*m8,\\s*tu,\\s*mu\\s+vrsub\\.vx\\s+v[0-9]+,\\s*v[0-9]+,\\s*[a-x0-9]+,\\s*v0.t} 2 } } */"}]}