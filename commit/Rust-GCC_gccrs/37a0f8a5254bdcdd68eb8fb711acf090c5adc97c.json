{"sha": "37a0f8a5254bdcdd68eb8fb711acf090c5adc97c", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MzdhMGY4YTUyNTRiZGNkZDY4ZWI4ZmI3MTFhY2YwOTBjNWFkYzk3Yw==", "commit": {"author": {"name": "Richard Henderson", "email": "rth@redhat.com", "date": "2002-01-27T04:46:53Z"}, "committer": {"name": "Richard Henderson", "email": "rth@gcc.gnu.org", "date": "2002-01-27T04:46:53Z"}, "message": "sched-deps.c (reg_pending_uses_head): New.\n\n        * sched-deps.c (reg_pending_uses_head): New.\n        (reg_pending_barrier): Rename from reg_pending_sets_all.\n        (find_insn_list): Don't mark inline.\n        (find_insn_mem_list): Remove.\n        (add_dependence_list, add_dependence_list_and_free): New.\n        (flush_pending_lists): Replace only_write param with separate\n        for_read and for_write parameters.  Update all callers.  Use\n        add_dependence_list_and_free.\n        (sched_analyze_1): Do not add reg dependencies here; just set\n        the pending bits.  Use add_dependence_list.\n        (sched_analyze_2): Likewise.\n        (sched_analyze_insn): Replace schedule_barrier_found with\n        reg_pending_barrier.  Add all dependencies for pending reg\n        uses, sets, and clobbers.\n        (sched_analyze): Don't add reg dependencies for calls, just\n        set pending bits.  Use regs_invalidated_by_call.  Treat\n        sched_before_next_call as a normal list, not a fake insn.\n        (init_deps): No funny init for sched_before_next_call.\n        (free_deps): Free pending mems lists.  Don't zero reg_last.\n        (init_deps_global): Init reg_pending_uses.\n        (finish_deps_global): Free it.\n        * sched-int.h (deps): Make in_post_call_group_p boolean.  Update docs.\n        (find_insn_mem_list): Remove.\n        * sched-rgn.c (concat_INSN_LIST, concat_insn_mem_list): New.\n        (propagate_deps): Use them.  Zero temp mem lists.\n\nFrom-SVN: r49262", "tree": {"sha": "0550590bfcb89e31ecc9e480084d2d4bceb518e9", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/0550590bfcb89e31ecc9e480084d2d4bceb518e9"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/37a0f8a5254bdcdd68eb8fb711acf090c5adc97c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/37a0f8a5254bdcdd68eb8fb711acf090c5adc97c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/37a0f8a5254bdcdd68eb8fb711acf090c5adc97c", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/37a0f8a5254bdcdd68eb8fb711acf090c5adc97c/comments", "author": null, "committer": null, "parents": [{"sha": "cea3bd3e5a0a40eb90809bf90063da4911ba23b0", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/cea3bd3e5a0a40eb90809bf90063da4911ba23b0", "html_url": "https://github.com/Rust-GCC/gccrs/commit/cea3bd3e5a0a40eb90809bf90063da4911ba23b0"}], "stats": {"total": 721, "additions": 318, "deletions": 403}, "files": [{"sha": "1241b5eab7b90016b4d0c2a89f26fa8ca9ef3773", "filename": "gcc/ChangeLog", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/37a0f8a5254bdcdd68eb8fb711acf090c5adc97c/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/37a0f8a5254bdcdd68eb8fb711acf090c5adc97c/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=37a0f8a5254bdcdd68eb8fb711acf090c5adc97c", "patch": "@@ -1,3 +1,31 @@\n+2002-01-26  Richard Henderson  <rth@redhat.com>\n+\n+\t* sched-deps.c (reg_pending_uses_head): New.\n+\t(reg_pending_barrier): Rename from reg_pending_sets_all.\n+\t(find_insn_list): Don't mark inline.\n+\t(find_insn_mem_list): Remove.\n+\t(add_dependence_list, add_dependence_list_and_free): New.\n+\t(flush_pending_lists): Replace only_write param with separate\n+\tfor_read and for_write parameters.  Update all callers.  Use\n+\tadd_dependence_list_and_free.\n+\t(sched_analyze_1): Do not add reg dependencies here; just set\n+\tthe pending bits.  Use add_dependence_list.\n+\t(sched_analyze_2): Likewise.\n+\t(sched_analyze_insn): Replace schedule_barrier_found with\n+\treg_pending_barrier.  Add all dependencies for pending reg\n+\tuses, sets, and clobbers.\n+\t(sched_analyze): Don't add reg dependencies for calls, just\n+\tset pending bits.  Use regs_invalidated_by_call.  Treat\n+\tsched_before_next_call as a normal list, not a fake insn.\n+\t(init_deps): No funny init for sched_before_next_call.\n+\t(free_deps): Free pending mems lists.  Don't zero reg_last.\n+\t(init_deps_global): Init reg_pending_uses.\n+\t(finish_deps_global): Free it.\n+\t* sched-int.h (deps): Make in_post_call_group_p boolean.  Update docs.\n+\t(find_insn_mem_list): Remove.\n+\t* sched-rgn.c (concat_INSN_LIST, concat_insn_mem_list): New.\n+\t(propagate_deps): Use them.  Zero temp mem lists.\n+\n 2002-01-26  Richard Henderson  <rth@redhat.com>\n \n \t* Makefile.in (CRTSTUFF_CFLAGS): New."}, {"sha": "ef38b8bc0cdbd4e0ed5fed8d36a53ea25a04edb1", "filename": "gcc/sched-deps.c", "status": "modified", "additions": 173, "deletions": 284, "changes": 457, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/37a0f8a5254bdcdd68eb8fb711acf090c5adc97c/gcc%2Fsched-deps.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/37a0f8a5254bdcdd68eb8fb711acf090c5adc97c/gcc%2Fsched-deps.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-deps.c?ref=37a0f8a5254bdcdd68eb8fb711acf090c5adc97c", "patch": "@@ -46,10 +46,12 @@ extern rtx *reg_known_value;\n \n static regset_head reg_pending_sets_head;\n static regset_head reg_pending_clobbers_head;\n+static regset_head reg_pending_uses_head;\n \n static regset reg_pending_sets;\n static regset reg_pending_clobbers;\n-static int reg_pending_sets_all;\n+static regset reg_pending_uses;\n+static bool reg_pending_barrier;\n \n /* To speed up the test for duplicate dependency links we keep a\n    record of dependencies created by add_dependence when the average\n@@ -77,10 +79,12 @@ static sbitmap *forward_dependency_cache;\n #endif\n \n static int deps_may_trap_p PARAMS ((rtx));\n+static void add_dependence_list PARAMS ((rtx, rtx, enum reg_note));\n+static void add_dependence_list_and_free PARAMS ((rtx, rtx *, enum reg_note));\n static void remove_dependence PARAMS ((rtx, rtx));\n static void set_sched_group_p PARAMS ((rtx));\n \n-static void flush_pending_lists PARAMS ((struct deps *, rtx, int));\n+static void flush_pending_lists PARAMS ((struct deps *, rtx, int, int));\n static void sched_analyze_1 PARAMS ((struct deps *, rtx, rtx));\n static void sched_analyze_2 PARAMS ((struct deps *, rtx, rtx));\n static void sched_analyze_insn PARAMS ((struct deps *, rtx, rtx, rtx));\n@@ -107,7 +111,7 @@ deps_may_trap_p (mem)\n /* Return the INSN_LIST containing INSN in LIST, or NULL\n    if LIST does not contain INSN.  */\n \n-HAIFA_INLINE rtx\n+rtx\n find_insn_list (insn, list)\n      rtx insn;\n      rtx list;\n@@ -120,25 +124,6 @@ find_insn_list (insn, list)\n     }\n   return 0;\n }\n-\n-/* Return 1 if the pair (insn, x) is found in (LIST, LIST1), or 0\n-   otherwise.  */\n-\n-HAIFA_INLINE int\n-find_insn_mem_list (insn, x, list, list1)\n-     rtx insn, x;\n-     rtx list, list1;\n-{\n-  while (list)\n-    {\n-      if (XEXP (list, 0) == insn\n-\t  && XEXP (list1, 0) == x)\n-\treturn 1;\n-      list = XEXP (list, 1);\n-      list1 = XEXP (list1, 1);\n-    }\n-  return 0;\n-}\n \f\n /* Find the condition under which INSN is executed.  */\n \n@@ -370,6 +355,34 @@ add_dependence (insn, elem, dep_type)\n #endif\n }\n \n+/* A convenience wrapper to operate on an entire list.  */\n+\n+static void\n+add_dependence_list (insn, list, dep_type)\n+     rtx insn, list;\n+     enum reg_note dep_type;\n+{\n+  for (; list; list = XEXP (list, 1))\n+    add_dependence (insn, XEXP (list, 0), dep_type);\n+}\n+\n+/* Similar, but free *LISTP at the same time.  */\n+\n+static void\n+add_dependence_list_and_free (insn, listp, dep_type)\n+     rtx insn;\n+     rtx *listp;\n+     enum reg_note dep_type;\n+{\n+  rtx list, next;\n+  for (list = *listp, *listp = NULL; list ; list = next)\n+    {\n+      next = XEXP (list, 1);\n+      add_dependence (insn, XEXP (list, 0), dep_type);\n+      free_INSN_LIST_node (list);\n+    }\n+}\n+\n /* Remove ELEM wrapped in an INSN_LIST from the LOG_LINKS\n    of INSN.  Abort if not found.  */\n \n@@ -505,51 +518,29 @@ add_insn_mem_dependence (deps, insn_list, mem_list, insn, mem)\n }\n \n /* Make a dependency between every memory reference on the pending lists\n-   and INSN, thus flushing the pending lists.  If ONLY_WRITE, don't flush\n-   the read list.  */\n+   and INSN, thus flushing the pending lists.  FOR_READ is true if emitting\n+   dependencies for a read operation, similarly with FOR_WRITE.  */\n \n static void\n-flush_pending_lists (deps, insn, only_write)\n+flush_pending_lists (deps, insn, for_read, for_write)\n      struct deps *deps;\n      rtx insn;\n-     int only_write;\n+     int for_read, for_write;\n {\n-  rtx u;\n-  rtx link;\n-\n-  while (deps->pending_read_insns && ! only_write)\n+  if (for_write)\n     {\n-      add_dependence (insn, XEXP (deps->pending_read_insns, 0),\n-\t\t      REG_DEP_ANTI);\n-\n-      link = deps->pending_read_insns;\n-      deps->pending_read_insns = XEXP (deps->pending_read_insns, 1);\n-      free_INSN_LIST_node (link);\n-\n-      link = deps->pending_read_mems;\n-      deps->pending_read_mems = XEXP (deps->pending_read_mems, 1);\n-      free_EXPR_LIST_node (link);\n+      add_dependence_list_and_free (insn, &deps->pending_read_insns,\n+\t\t\t\t    REG_DEP_ANTI);\n+      free_EXPR_LIST_list (&deps->pending_read_mems);\n     }\n-  while (deps->pending_write_insns)\n-    {\n-      add_dependence (insn, XEXP (deps->pending_write_insns, 0),\n-\t\t      REG_DEP_ANTI);\n \n-      link = deps->pending_write_insns;\n-      deps->pending_write_insns = XEXP (deps->pending_write_insns, 1);\n-      free_INSN_LIST_node (link);\n-\n-      link = deps->pending_write_mems;\n-      deps->pending_write_mems = XEXP (deps->pending_write_mems, 1);\n-      free_EXPR_LIST_node (link);\n-    }\n+  add_dependence_list_and_free (insn, &deps->pending_write_insns,\n+\t\t\t\tfor_read ? REG_DEP_ANTI : REG_DEP_OUTPUT);\n+  free_EXPR_LIST_list (&deps->pending_write_mems);\n   deps->pending_lists_length = 0;\n \n-  /* last_pending_memory_flush is now a list of insns.  */\n-  for (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n-    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\n-  free_INSN_LIST_list (&deps->last_pending_memory_flush);\n+  add_dependence_list_and_free (insn, &deps->last_pending_memory_flush,\n+\t\t\t\tfor_read ? REG_DEP_ANTI : REG_DEP_OUTPUT);\n   deps->last_pending_memory_flush = alloc_INSN_LIST (insn, NULL_RTX);\n   deps->pending_flush_length = 1;\n }\n@@ -601,46 +592,22 @@ sched_analyze_1 (deps, x, insn)\n \n   if (GET_CODE (dest) == REG)\n     {\n-      int i;\n-\n       regno = REGNO (dest);\n \n       /* A hard reg in a wide mode may really be multiple registers.\n          If so, mark all of them just like the first.  */\n       if (regno < FIRST_PSEUDO_REGISTER)\n \t{\n-\t  i = HARD_REGNO_NREGS (regno, GET_MODE (dest));\n-\t  while (--i >= 0)\n+\t  int i = HARD_REGNO_NREGS (regno, GET_MODE (dest));\n+\t  if (code == SET)\n \t    {\n-\t      int r = regno + i;\n-\t      rtx u;\n-\n-\t      for (u = deps->reg_last[r].uses; u; u = XEXP (u, 1))\n-\t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\n-\t      for (u = deps->reg_last[r].sets; u; u = XEXP (u, 1))\n-\t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n-\n-\t      /* Clobbers need not be ordered with respect to one\n-\t\t another, but sets must be ordered with respect to a\n-\t\t pending clobber.  */\n-\t      if (code == SET)\n-\t\t{\n-\t\t  if (GET_CODE (PATTERN (insn)) != COND_EXEC)\n-\t\t    free_INSN_LIST_list (&deps->reg_last[r].uses);\n-\t\t  for (u = deps->reg_last[r].clobbers; u; u = XEXP (u, 1))\n-\t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n-\t\t  SET_REGNO_REG_SET (reg_pending_sets, r);\n-\t\t}\n-\t      else\n-\t\tSET_REGNO_REG_SET (reg_pending_clobbers, r);\n-\n-\t      /* Function calls clobber all call_used regs.  */\n-\t      if (global_regs[r]\n-\t\t  || (code == SET\n-\t\t      && TEST_HARD_REG_BIT (regs_invalidated_by_call, r)))\n-\t\tfor (u = deps->last_function_call; u; u = XEXP (u, 1))\n-\t\t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\t      while (--i >= 0)\n+\t\tSET_REGNO_REG_SET (reg_pending_sets, regno + i);\n+\t    }\n+\t  else\n+\t    {\n+\t      while (--i >= 0)\n+\t\tSET_REGNO_REG_SET (reg_pending_clobbers, regno + i);\n \t    }\n \t}\n       /* ??? Reload sometimes emits USEs and CLOBBERs of pseudos that\n@@ -654,22 +621,8 @@ sched_analyze_1 (deps, x, insn)\n \t}\n       else\n \t{\n-\t  rtx u;\n-\n-\t  for (u = deps->reg_last[regno].uses; u; u = XEXP (u, 1))\n-\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\n-\t  for (u = deps->reg_last[regno].sets; u; u = XEXP (u, 1))\n-\t    add_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n-\n \t  if (code == SET)\n-\t    {\n-\t      if (GET_CODE (PATTERN (insn)) != COND_EXEC)\n-\t\tfree_INSN_LIST_list (&deps->reg_last[regno].uses);\n-\t      for (u = deps->reg_last[regno].clobbers; u; u = XEXP (u, 1))\n-\t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_OUTPUT);\n-\t      SET_REGNO_REG_SET (reg_pending_sets, regno);\n-\t    }\n+\t    SET_REGNO_REG_SET (reg_pending_sets, regno);\n \t  else\n \t    SET_REGNO_REG_SET (reg_pending_clobbers, regno);\n \n@@ -683,10 +636,8 @@ sched_analyze_1 (deps, x, insn)\n \n \t  /* Don't let it cross a call after scheduling if it doesn't\n \t     already cross one.  */\n-\n \t  if (REG_N_CALLS_CROSSED (regno) == 0)\n-\t    for (u = deps->last_function_call; u; u = XEXP (u, 1))\n-\t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\t    add_dependence_list (insn, deps->last_function_call, REG_DEP_ANTI);\n \t}\n     }\n   else if (GET_CODE (dest) == MEM)\n@@ -708,11 +659,10 @@ sched_analyze_1 (deps, x, insn)\n \t     these lists get long.  When compiling GCC with itself,\n \t     this flush occurs 8 times for sparc, and 10 times for m88k using\n \t     the default value of 32.  */\n-\t  flush_pending_lists (deps, insn, 0);\n+\t  flush_pending_lists (deps, insn, false, true);\n \t}\n       else\n \t{\n-\t  rtx u;\n \t  rtx pending, pending_mem;\n \n \t  pending = deps->pending_read_insns;\n@@ -737,8 +687,8 @@ sched_analyze_1 (deps, x, insn)\n \t      pending_mem = XEXP (pending_mem, 1);\n \t    }\n \n-\t  for (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n-\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\t  add_dependence_list (insn, deps->last_pending_memory_flush,\n+\t\t\t       REG_DEP_ANTI);\n \n \t  add_insn_mem_dependence (deps, &deps->pending_write_insns,\n \t\t\t\t   &deps->pending_write_mems, insn, dest);\n@@ -790,32 +740,12 @@ sched_analyze_2 (deps, x, insn)\n \n     case REG:\n       {\n-\trtx u;\n \tint regno = REGNO (x);\n \tif (regno < FIRST_PSEUDO_REGISTER)\n \t  {\n-\t    int i;\n-\n-\t    i = HARD_REGNO_NREGS (regno, GET_MODE (x));\n+\t    int i = HARD_REGNO_NREGS (regno, GET_MODE (x));\n \t    while (--i >= 0)\n-\t      {\n-\t\tint r = regno + i;\n-\t\tdeps->reg_last[r].uses\n-\t\t  = alloc_INSN_LIST (insn, deps->reg_last[r].uses);\n-\t\tSET_REGNO_REG_SET (&deps->reg_last_in_use, r);\n-\n-\t\tfor (u = deps->reg_last[r].sets; u; u = XEXP (u, 1))\n-\t\t  add_dependence (insn, XEXP (u, 0), 0);\n-\n-\t\t/* ??? This should never happen.  */\n-\t\tfor (u = deps->reg_last[r].clobbers; u; u = XEXP (u, 1))\n-\t\t  add_dependence (insn, XEXP (u, 0), 0);\n-\n-\t\tif (call_used_regs[r] || global_regs[r])\n-\t\t  /* Function calls clobber all call_used regs.  */\n-\t\t  for (u = deps->last_function_call; u; u = XEXP (u, 1))\n-\t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t      }\n+\t      SET_REGNO_REG_SET (reg_pending_uses, regno + i);\n \t  }\n \t/* ??? Reload sometimes emits USEs and CLOBBERs of pseudos that\n \t   it does not reload.  Ignore these as they have served their\n@@ -828,16 +758,7 @@ sched_analyze_2 (deps, x, insn)\n \t  }\n \telse\n \t  {\n-\t    deps->reg_last[regno].uses\n-\t      = alloc_INSN_LIST (insn, deps->reg_last[regno].uses);\n-\t    SET_REGNO_REG_SET (&deps->reg_last_in_use, regno);\n-\n-\t    for (u = deps->reg_last[regno].sets; u; u = XEXP (u, 1))\n-\t      add_dependence (insn, XEXP (u, 0), 0);\n-\n-\t    /* ??? This should never happen.  */\n-\t    for (u = deps->reg_last[regno].clobbers; u; u = XEXP (u, 1))\n-\t      add_dependence (insn, XEXP (u, 0), 0);\n+\t    SET_REGNO_REG_SET (reg_pending_uses, regno);\n \n \t    /* Pseudos that are REG_EQUIV to something may be replaced\n \t       by that during reloading.  We need only add dependencies for\n@@ -851,8 +772,8 @@ sched_analyze_2 (deps, x, insn)\n \t       insn to the sched_before_next_call list so that it will still\n \t       not cross calls after scheduling.  */\n \t    if (REG_N_CALLS_CROSSED (regno) == 0)\n-\t      add_dependence (deps->sched_before_next_call, insn,\n-\t\t\t      REG_DEP_ANTI);\n+\t      deps->sched_before_next_call\n+\t\t= alloc_INSN_LIST (insn, deps->sched_before_next_call);\n \t  }\n \treturn;\n       }\n@@ -910,15 +831,13 @@ sched_analyze_2 (deps, x, insn)\n \n     /* Force pending stores to memory in case a trap handler needs them.  */\n     case TRAP_IF:\n-      flush_pending_lists (deps, insn, 1);\n+      flush_pending_lists (deps, insn, true, false);\n       break;\n \n     case ASM_OPERANDS:\n     case ASM_INPUT:\n     case UNSPEC_VOLATILE:\n       {\n-\trtx u;\n-\n \t/* Traditional and volatile asm instructions must be considered to use\n \t   and clobber all hard registers, all pseudo-registers and all of\n \t   memory.  So must TRAP_IF and UNSPEC_VOLATILE operations.\n@@ -927,25 +846,7 @@ sched_analyze_2 (deps, x, insn)\n \t   mode.  An insn should not be moved across this even if it only uses\n \t   pseudo-regs because it might give an incorrectly rounded result.  */\n \tif (code != ASM_OPERANDS || MEM_VOLATILE_P (x))\n-\t  {\n-\t    for (i = 0; i < deps->max_reg; i++)\n-\t      {\n-\t\tstruct deps_reg *reg_last = &deps->reg_last[i];\n-\n-\t\tfor (u = reg_last->uses; u; u = XEXP (u, 1))\n-\t\t  add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t\tfor (u = reg_last->sets; u; u = XEXP (u, 1))\n-\t\t  add_dependence (insn, XEXP (u, 0), 0);\n-\t\tfor (u = reg_last->clobbers; u; u = XEXP (u, 1))\n-\t\t  add_dependence (insn, XEXP (u, 0), 0);\n-\n-\t\tif (GET_CODE (PATTERN (insn)) != COND_EXEC)\n-\t\t  free_INSN_LIST_list (&reg_last->uses);\n-\t      }\n-\t    reg_pending_sets_all = 1;\n-\n-\t    flush_pending_lists (deps, insn, 0);\n-\t  }\n+\t  reg_pending_barrier = true;\n \n \t/* For all ASM_OPERANDS, we must traverse the vector of input operands.\n \t   We can not just fall through here since then we would be confused\n@@ -1008,7 +909,6 @@ sched_analyze_insn (deps, x, insn, loop_notes)\n      rtx loop_notes;\n {\n   RTX_CODE code = GET_CODE (x);\n-  int schedule_barrier_found = 0;\n   rtx link;\n   int i;\n \n@@ -1057,31 +957,23 @@ sched_analyze_insn (deps, x, insn, loop_notes)\n \t    sched_analyze_2 (deps, XEXP (link, 0), insn);\n \t}\n       if (find_reg_note (insn, REG_SETJMP, NULL))\n-\tschedule_barrier_found = 1;\n+\treg_pending_barrier = true;\n     }\n \n   if (GET_CODE (insn) == JUMP_INSN)\n     {\n       rtx next;\n       next = next_nonnote_insn (insn);\n       if (next && GET_CODE (next) == BARRIER)\n-\tschedule_barrier_found = 1;\n+\treg_pending_barrier = true;\n       else\n \t{\n-\t  rtx pending, pending_mem, u;\n+\t  rtx pending, pending_mem;\n \t  regset_head tmp;\n \t  INIT_REG_SET (&tmp);\n \n \t  (*current_sched_info->compute_jump_reg_dependencies) (insn, &tmp);\n-\t  EXECUTE_IF_SET_IN_REG_SET (&tmp, 0, i,\n-\t    {\n-\t      struct deps_reg *reg_last = &deps->reg_last[i];\n-\t      for (u = reg_last->sets; u; u = XEXP (u, 1))\n-\t\tadd_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t      reg_last->uses = alloc_INSN_LIST (insn, reg_last->uses);\n-\t      SET_REGNO_REG_SET (&deps->reg_last_in_use, i);\n-\t    });\n-\n+\t  IOR_REG_SET (reg_pending_uses, &tmp);\n \t  CLEAR_REG_SET (&tmp);\n \n \t  /* All memory writes and volatile reads must happen before the\n@@ -1107,8 +999,8 @@ sched_analyze_insn (deps, x, insn, loop_notes)\n \t      pending_mem = XEXP (pending_mem, 1);\n \t    }\n \n-\t  for (u = deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n-\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n+\t  add_dependence_list (insn, deps->last_pending_memory_flush,\n+\t\t\t       REG_DEP_ANTI);\n \t}\n     }\n \n@@ -1130,7 +1022,7 @@ sched_analyze_insn (deps, x, insn, loop_notes)\n \t      || INTVAL (XEXP (link, 0)) == NOTE_INSN_LOOP_END\n \t      || INTVAL (XEXP (link, 0)) == NOTE_INSN_EH_REGION_BEG\n \t      || INTVAL (XEXP (link, 0)) == NOTE_INSN_EH_REGION_END)\n-\t    schedule_barrier_found = 1;\n+\t    reg_pending_barrier = true;\n \n \t  link = XEXP (link, 1);\n \t}\n@@ -1142,72 +1034,95 @@ sched_analyze_insn (deps, x, insn, loop_notes)\n      where block boundaries fall.  This is mighty confusing elsewhere. \n      Therefore, prevent such an instruction from being moved.  */\n   if (can_throw_internal (insn))\n-    schedule_barrier_found = 1;\n+    reg_pending_barrier = true;\n \n   /* Add dependencies if a scheduling barrier was found.  */\n-  if (schedule_barrier_found)\n+  if (reg_pending_barrier)\n     {\n-      rtx u;\n-\n-      for (i = 0; i < deps->max_reg; i++)\n+      if (GET_CODE (PATTERN (insn)) == COND_EXEC)\n \t{\n-\t  struct deps_reg *reg_last = &deps->reg_last[i];\n-\n-\t  for (u = reg_last->uses; u; u = XEXP (u, 1))\n-\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t  for (u = reg_last->sets; u; u = XEXP (u, 1))\n-\t    add_dependence (insn, XEXP (u, 0), 0);\n-\t  for (u = reg_last->clobbers; u; u = XEXP (u, 1))\n-\t    add_dependence (insn, XEXP (u, 0), 0);\n-\n-\t  if (GET_CODE (PATTERN (insn)) != COND_EXEC)\n-\t    free_INSN_LIST_list (&reg_last->uses);\n+\t  EXECUTE_IF_SET_IN_REG_SET (&deps->reg_last_in_use, 0, i,\n+\t    {\n+\t      struct deps_reg *reg_last = &deps->reg_last[i];\n+\t      add_dependence_list (insn, reg_last->uses, REG_DEP_ANTI);\n+\t      add_dependence_list (insn, reg_last->sets, 0);\n+\t      add_dependence_list (insn, reg_last->clobbers, 0);\n+\t    });\n+\t}\n+      else\n+\t{\n+\t  EXECUTE_IF_SET_IN_REG_SET (&deps->reg_last_in_use, 0, i,\n+\t    {\n+\t      struct deps_reg *reg_last = &deps->reg_last[i];\n+\t      add_dependence_list_and_free (insn, &reg_last->uses,\n+\t\t\t\t\t    REG_DEP_ANTI);\n+\t      add_dependence_list_and_free (insn, &reg_last->sets, 0);\n+\t      add_dependence_list_and_free (insn, &reg_last->clobbers, 0);\n+\t    });\n \t}\n-      flush_pending_lists (deps, insn, 0);\n-\n-      reg_pending_sets_all = 1;\n-    }\n \n-  /* Accumulate clobbers until the next set so that it will be output\n-     dependent on all of them.  At the next set we can clear the clobber\n-     list, since subsequent sets will be output dependent on it.  */\n-  if (reg_pending_sets_all)\n-    {\n-      reg_pending_sets_all = 0;\n       for (i = 0; i < deps->max_reg; i++)\n \t{\n \t  struct deps_reg *reg_last = &deps->reg_last[i];\n-\t  if (GET_CODE (PATTERN (insn)) != COND_EXEC)\n-\t    {\n-\t      free_INSN_LIST_list (&reg_last->sets);\n-\t      free_INSN_LIST_list (&reg_last->clobbers);\n-\t    }\n \t  reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);\n \t  SET_REGNO_REG_SET (&deps->reg_last_in_use, i);\n \t}\n+\n+      flush_pending_lists (deps, insn, true, true);\n+      reg_pending_barrier = false;\n     }\n   else\n     {\n-      EXECUTE_IF_SET_IN_REG_SET (reg_pending_sets, 0, i,\n+      EXECUTE_IF_SET_IN_REG_SET (reg_pending_uses, 0, i,\n \t{\n \t  struct deps_reg *reg_last = &deps->reg_last[i];\n-\t  if (GET_CODE (PATTERN (insn)) != COND_EXEC)\n-\t    {\n-\t      free_INSN_LIST_list (&reg_last->sets);\n-\t      free_INSN_LIST_list (&reg_last->clobbers);\n-\t    }\n-\t  reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);\n-\t  SET_REGNO_REG_SET (&deps->reg_last_in_use, i);\n+\t  add_dependence_list (insn, reg_last->sets, 0);\n+\t  add_dependence_list (insn, reg_last->clobbers, 0);\n+\t  reg_last->uses = alloc_INSN_LIST (insn, reg_last->uses);\n \t});\n       EXECUTE_IF_SET_IN_REG_SET (reg_pending_clobbers, 0, i,\n \t{\n \t  struct deps_reg *reg_last = &deps->reg_last[i];\n+\t  add_dependence_list (insn, reg_last->sets, REG_DEP_OUTPUT);\n+\t  add_dependence_list (insn, reg_last->uses, REG_DEP_ANTI);\n \t  reg_last->clobbers = alloc_INSN_LIST (insn, reg_last->clobbers);\n-\t  SET_REGNO_REG_SET (&deps->reg_last_in_use, i);\n \t});\n+\n+      /* If the current insn is conditional, we can't free any\n+\t of the lists.  */\n+      if (GET_CODE (PATTERN (insn)) == COND_EXEC)\n+\t{\n+\t  EXECUTE_IF_SET_IN_REG_SET (reg_pending_sets, 0, i,\n+\t    {\n+\t      struct deps_reg *reg_last = &deps->reg_last[i];\n+\t      add_dependence_list (insn, reg_last->sets, REG_DEP_OUTPUT);\n+\t      add_dependence_list (insn, reg_last->clobbers, REG_DEP_OUTPUT);\n+\t      add_dependence_list (insn, reg_last->uses, REG_DEP_ANTI);\n+\t      reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);\n+\t    });\n+\t}\n+      else\n+\t{\n+\t  EXECUTE_IF_SET_IN_REG_SET (reg_pending_sets, 0, i,\n+\t    {\n+\t      struct deps_reg *reg_last = &deps->reg_last[i];\n+\t      add_dependence_list_and_free (insn, &reg_last->sets,\n+\t\t\t\t\t    REG_DEP_OUTPUT);\n+\t      add_dependence_list_and_free (insn, &reg_last->clobbers,\n+\t\t\t\t\t    REG_DEP_OUTPUT);\n+\t      add_dependence_list_and_free (insn, &reg_last->uses,\n+\t\t\t\t\t    REG_DEP_ANTI);\n+\t      reg_last->sets = alloc_INSN_LIST (insn, reg_last->sets);\n+\t    });\n+\t}\n+\n+      IOR_REG_SET (&deps->reg_last_in_use, reg_pending_uses);\n+      IOR_REG_SET (&deps->reg_last_in_use, reg_pending_clobbers);\n+      IOR_REG_SET (&deps->reg_last_in_use, reg_pending_sets);\n     }\n-  CLEAR_REG_SET (reg_pending_sets);\n+  CLEAR_REG_SET (reg_pending_uses);\n   CLEAR_REG_SET (reg_pending_clobbers);\n+  CLEAR_REG_SET (reg_pending_sets);\n \n   /* If a post-call group is still open, see if it should remain so.\n      This insn must be a simple move of a hard reg to a pseudo or\n@@ -1251,7 +1166,7 @@ sched_analyze_insn (deps, x, insn, loop_notes)\n       else\n \t{\n \tend_call_group:\n-\t  deps->in_post_call_group_p = 0;\n+\t  deps->in_post_call_group_p = false;\n \t}\n     }\n }\n@@ -1265,7 +1180,6 @@ sched_analyze (deps, head, tail)\n      rtx head, tail;\n {\n   rtx insn;\n-  rtx u;\n   rtx loop_notes = 0;\n \n   if (current_sched_info->use_cselib)\n@@ -1287,7 +1201,7 @@ sched_analyze (deps, head, tail)\n \t    {\n \t      /* Keep the list a reasonable size.  */\n \t      if (deps->pending_flush_length++ > MAX_PENDING_LIST_LENGTH)\n-\t\tflush_pending_lists (deps, insn, 0);\n+\t\tflush_pending_lists (deps, insn, true, true);\n \t      else\n \t\tdeps->last_pending_memory_flush\n \t\t  = alloc_INSN_LIST (insn, deps->last_pending_memory_flush);\n@@ -1297,7 +1211,6 @@ sched_analyze (deps, head, tail)\n \t}\n       else if (GET_CODE (insn) == CALL_INSN)\n \t{\n-\t  rtx x;\n \t  int i;\n \n \t  /* Clear out stale SCHED_GROUP_P.  */\n@@ -1308,59 +1221,35 @@ sched_analyze (deps, head, tail)\n \t  /* Clear out the stale LOG_LINKS from flow.  */\n \t  free_INSN_LIST_list (&LOG_LINKS (insn));\n \n-\t  /* Any instruction using a hard register which may get clobbered\n-\t     by a call needs to be marked as dependent on this call.\n-\t     This prevents a use of a hard return reg from being moved\n-\t     past a void call (i.e. it does not explicitly set the hard\n-\t     return reg).  */\n-\n-\t  /* If this call has REG_SETJMP, then assume that\n-\t     all registers, not just hard registers, may be clobbered by this\n-\t     call.  */\n-\n-\t  /* Insn, being a CALL_INSN, magically depends on\n-\t     `last_function_call' already.  */\n-\n \t  if (find_reg_note (insn, REG_SETJMP, NULL))\n \t    {\n-\t      for (i = 0; i < deps->max_reg; i++)\n-\t\t{\n-\t\t  struct deps_reg *reg_last = &deps->reg_last[i];\n-\t\t\n-\t\t  for (u = reg_last->uses; u; u = XEXP (u, 1))\n-\t\t    add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t\t  for (u = reg_last->sets; u; u = XEXP (u, 1))\n-\t\t    add_dependence (insn, XEXP (u, 0), 0);\n-\t\t  for (u = reg_last->clobbers; u; u = XEXP (u, 1))\n-\t\t    add_dependence (insn, XEXP (u, 0), 0);\n-\n-\t\t  free_INSN_LIST_list (&reg_last->uses);\n-\t\t}\n-\t      reg_pending_sets_all = 1;\n+\t      /* This is setjmp.  Assume that all registers, not just\n+\t\t hard registers, may be clobbered by this call.  */\n+\t      reg_pending_barrier = true;\n \t    }\n \t  else\n \t    {\n+\t      /* A call may read and modify global register variables.\n+\t\t Other call-clobbered hard regs may be clobbered.  We\n+\t\t don't know what set of fixed registers might be used\n+\t\t by the function.  It is certain that the stack pointer\n+\t\t is among them, but be conservative.  */\n \t      for (i = 0; i < FIRST_PSEUDO_REGISTER; i++)\n-\t\tif (call_used_regs[i] || global_regs[i])\n+\t\tif (global_regs[i])\n \t\t  {\n-\t\t    for (u = deps->reg_last[i].uses; u; u = XEXP (u, 1))\n-\t\t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\t\t    for (u = deps->reg_last[i].sets; u; u = XEXP (u, 1))\n-\t\t      add_dependence (insn, XEXP (u, 0), REG_DEP_ANTI);\n-\n-\t\t    SET_REGNO_REG_SET (reg_pending_clobbers, i);\n+\t\t    SET_REGNO_REG_SET (reg_pending_sets, i);\n+\t\t    SET_REGNO_REG_SET (reg_pending_uses, i);\n \t\t  }\n+\t\telse if (TEST_HARD_REG_BIT (regs_invalidated_by_call, i))\n+\t\t  SET_REGNO_REG_SET (reg_pending_clobbers, i);\n+\t\telse if (fixed_regs[i])\n+\t\t  SET_REGNO_REG_SET (reg_pending_uses, i);\n \t    }\n \n \t  /* For each insn which shouldn't cross a call, add a dependence\n \t     between that insn and this call insn.  */\n-\t  x = LOG_LINKS (deps->sched_before_next_call);\n-\t  while (x)\n-\t    {\n-\t      add_dependence (insn, XEXP (x, 0), REG_DEP_ANTI);\n-\t      x = XEXP (x, 1);\n-\t    }\n-\t  free_INSN_LIST_list (&LOG_LINKS (deps->sched_before_next_call));\n+\t  add_dependence_list_and_free (insn, &deps->sched_before_next_call,\n+\t\t\t\t\tREG_DEP_ANTI);\n \n \t  sched_analyze_insn (deps, PATTERN (insn), insn, loop_notes);\n \t  loop_notes = 0;\n@@ -1369,19 +1258,16 @@ sched_analyze (deps, head, tail)\n \t     all pending reads and writes, and start new dependencies starting\n \t     from here.  But only flush writes for constant calls (which may\n \t     be passed a pointer to something we haven't written yet).  */\n-\t  flush_pending_lists (deps, insn, CONST_OR_PURE_CALL_P (insn));\n-\n-\t  /* Depend this function call (actually, the user of this\n-\t     function call) on all hard register clobberage.  */\n+\t  flush_pending_lists (deps, insn, true, !CONST_OR_PURE_CALL_P (insn));\n \n-\t  /* last_function_call is now a list of insns.  */\n+\t  /* Remember the last function call for limiting lifetimes.  */\n \t  free_INSN_LIST_list (&deps->last_function_call);\n \t  deps->last_function_call = alloc_INSN_LIST (insn, NULL_RTX);\n \n \t  /* Before reload, begin a post-call group, so as to keep the\n \t     lifetimes of hard registers correct.  */\n \t  if (! reload_completed)\n-\t    deps->in_post_call_group_p = 1;\n+\t    deps->in_post_call_group_p = true;\n \t}\n \n       /* See comments on reemit_notes as to why we do this.\n@@ -1513,12 +1399,8 @@ init_deps (deps)\n   deps->pending_flush_length = 0;\n   deps->last_pending_memory_flush = 0;\n   deps->last_function_call = 0;\n-  deps->in_post_call_group_p = 0;\n-\n-  deps->sched_before_next_call\n-    = gen_rtx_INSN (VOIDmode, 0, NULL_RTX, NULL_RTX,\n-\t\t    NULL_RTX, 0, NULL_RTX, NULL_RTX);\n-  LOG_LINKS (deps->sched_before_next_call) = 0;\n+  deps->sched_before_next_call = 0;\n+  deps->in_post_call_group_p = false;\n }\n \n /* Free insn lists found in DEPS.  */\n@@ -1529,6 +1411,12 @@ free_deps (deps)\n {\n   int i;\n \n+  free_INSN_LIST_list (&deps->pending_read_insns);\n+  free_EXPR_LIST_list (&deps->pending_read_mems);\n+  free_INSN_LIST_list (&deps->pending_write_insns);\n+  free_EXPR_LIST_list (&deps->pending_write_mems);\n+  free_INSN_LIST_list (&deps->last_pending_memory_flush);\n+\n   /* Without the EXECUTE_IF_SET, this loop is executed max_reg * nr_regions\n      times.  For a test case with 42000 regs and 8000 small basic blocks,\n      this loop accounted for nearly 60% (84 sec) of the total -O2 runtime.  */\n@@ -1542,7 +1430,6 @@ free_deps (deps)\n   CLEAR_REG_SET (&deps->reg_last_in_use);\n \n   free (deps->reg_last);\n-  deps->reg_last = NULL;\n }\n \n /* If it is profitable to use them, initialize caches for tracking\n@@ -1602,7 +1489,8 @@ init_deps_global ()\n {\n   reg_pending_sets = INITIALIZE_REG_SET (reg_pending_sets_head);\n   reg_pending_clobbers = INITIALIZE_REG_SET (reg_pending_clobbers_head);\n-  reg_pending_sets_all = 0;\n+  reg_pending_uses = INITIALIZE_REG_SET (reg_pending_uses_head);\n+  reg_pending_barrier = false;\n }\n \n /* Free everything used by the dependency analysis code.  */\n@@ -1612,4 +1500,5 @@ finish_deps_global ()\n {\n   FREE_REG_SET (reg_pending_sets);\n   FREE_REG_SET (reg_pending_clobbers);\n+  FREE_REG_SET (reg_pending_uses);\n }"}, {"sha": "851d058c74d0ef9dceb714aef0fee59fe900e804", "filename": "gcc/sched-int.h", "status": "modified", "additions": 10, "deletions": 10, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/37a0f8a5254bdcdd68eb8fb711acf090c5adc97c/gcc%2Fsched-int.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/37a0f8a5254bdcdd68eb8fb711acf090c5adc97c/gcc%2Fsched-int.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-int.h?ref=37a0f8a5254bdcdd68eb8fb711acf090c5adc97c", "patch": "@@ -68,19 +68,20 @@ struct deps\n      too large.  */\n   rtx last_pending_memory_flush;\n \n-  /* The last function call we have seen.  All hard regs, and, of course,\n-     the last function call, must depend on this.  */\n+  /* A list of the last function calls we have seen.  We use a list to\n+     represent last function calls from multiple predecessor blocks.\n+     Used to prevent register lifetimes from expanding unnecessarily.  */\n   rtx last_function_call;\n \n+  /* A list of insns which use a pseudo register that does not already\n+     cross a call.  We create dependencies between each of those insn\n+     and the next call insn, to ensure that they won't cross a call after\n+     scheduling is done.  */\n+  rtx sched_before_next_call;\n+\n   /* Used to keep post-call psuedo/hard reg movements together with\n      the call.  */\n-  int in_post_call_group_p;\n-\n-  /* The LOG_LINKS field of this is a list of insns which use a pseudo\n-     register that does not already cross a call.  We create\n-     dependencies between each of those insn and the next call insn,\n-     to ensure that they won't cross a call after scheduling is done.  */\n-  rtx sched_before_next_call;\n+  bool in_post_call_group_p;\n \n   /* The maximum register number for the following arrays.  Before reload\n      this is max_reg_num; after reload it is FIRST_PSEUDO_REGISTER.  */\n@@ -274,7 +275,6 @@ extern void free_deps PARAMS ((struct deps *));\n extern void init_deps_global PARAMS ((void));\n extern void finish_deps_global PARAMS ((void));\n extern void compute_forward_dependences PARAMS ((rtx, rtx));\n-extern int find_insn_mem_list PARAMS ((rtx, rtx, rtx, rtx));\n extern rtx find_insn_list PARAMS ((rtx, rtx));\n extern void init_dependency_caches PARAMS ((int));\n extern void free_dependency_caches PARAMS ((void));"}, {"sha": "b7193aaf604cf7ab66fb5527977858e36f938f18", "filename": "gcc/sched-rgn.c", "status": "modified", "additions": 107, "deletions": 109, "changes": 216, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/37a0f8a5254bdcdd68eb8fb711acf090c5adc97c/gcc%2Fsched-rgn.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/37a0f8a5254bdcdd68eb8fb711acf090c5adc97c/gcc%2Fsched-rgn.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsched-rgn.c?ref=37a0f8a5254bdcdd68eb8fb711acf090c5adc97c", "patch": "@@ -300,6 +300,8 @@ void debug_dependencies PARAMS ((void));\n \n static void init_regions PARAMS ((void));\n static void schedule_region PARAMS ((int));\n+static rtx concat_INSN_LIST PARAMS ((rtx, rtx));\n+static void concat_insn_mem_list PARAMS ((rtx, rtx, rtx *, rtx *));\n static void propagate_deps PARAMS ((int, struct deps *));\n static void free_pending_lists PARAMS ((void));\n \n@@ -2299,8 +2301,7 @@ add_branch_dependences (head, tail)\n     {\n       if (GET_CODE (insn) != NOTE)\n \t{\n-\t  if (last != 0\n-\t      && !find_insn_list (insn, LOG_LINKS (last)))\n+\t  if (last != 0 && !find_insn_list (insn, LOG_LINKS (last)))\n \t    {\n \t      add_dependence (last, insn, REG_DEP_ANTI);\n \t      INSN_REF_COUNT (insn)++;\n@@ -2356,125 +2357,122 @@ add_branch_dependences (head, tail)\n \n static struct deps *bb_deps;\n \n+/* Duplicate the INSN_LIST elements of COPY and prepend them to OLD.  */\n+\n+static rtx\n+concat_INSN_LIST (copy, old)\n+     rtx copy, old;\n+{\n+  rtx new = old;\n+  for (; copy ; copy = XEXP (copy, 1))\n+    new = alloc_INSN_LIST (XEXP (copy, 0), new);\n+  return new;\n+}\n+\n+static void\n+concat_insn_mem_list (copy_insns, copy_mems, old_insns_p, old_mems_p)\n+     rtx copy_insns, copy_mems;\n+     rtx *old_insns_p, *old_mems_p;\n+{\n+  rtx new_insns = *old_insns_p;\n+  rtx new_mems = *old_mems_p;\n+\n+  while (copy_insns)\n+    {\n+      new_insns = alloc_INSN_LIST (XEXP (copy_insns, 0), new_insns);\n+      new_mems = alloc_EXPR_LIST (VOIDmode, XEXP (copy_mems, 0), new_mems);\n+      copy_insns = XEXP (copy_insns, 1);\n+      copy_mems = XEXP (copy_mems, 1);\n+    }\n+\n+  *old_insns_p = new_insns;\n+  *old_mems_p = new_mems;\n+}\n+\n /* After computing the dependencies for block BB, propagate the dependencies\n    found in TMP_DEPS to the successors of the block.  */\n static void\n-propagate_deps (bb, tmp_deps)\n+propagate_deps (bb, pred_deps)\n      int bb;\n-     struct deps *tmp_deps;\n+     struct deps *pred_deps;\n {\n   int b = BB_TO_BLOCK (bb);\n   int e, first_edge;\n-  int reg;\n-  rtx link_insn, link_mem;\n-  rtx u;\n-\n-  /* These lists should point to the right place, for correct\n-     freeing later.  */\n-  bb_deps[bb].pending_read_insns = tmp_deps->pending_read_insns;\n-  bb_deps[bb].pending_read_mems = tmp_deps->pending_read_mems;\n-  bb_deps[bb].pending_write_insns = tmp_deps->pending_write_insns;\n-  bb_deps[bb].pending_write_mems = tmp_deps->pending_write_mems;\n \n   /* bb's structures are inherited by its successors.  */\n   first_edge = e = OUT_EDGES (b);\n-  if (e <= 0)\n-    return;\n-\n-  do\n-    {\n-      rtx x;\n-      int b_succ = TO_BLOCK (e);\n-      int bb_succ = BLOCK_TO_BB (b_succ);\n-      struct deps *succ_deps = bb_deps + bb_succ;\n-\n-      /* Only bbs \"below\" bb, in the same region, are interesting.  */\n-      if (CONTAINING_RGN (b) != CONTAINING_RGN (b_succ)\n-\t  || bb_succ <= bb)\n-\t{\n-\t  e = NEXT_OUT (e);\n-\t  continue;\n-\t}\n-\n-      /* The reg_last lists are inherited by bb_succ.  */\n-      EXECUTE_IF_SET_IN_REG_SET (&tmp_deps->reg_last_in_use, 0, reg,\n-\t{\n-\t  struct deps_reg *tmp_deps_reg = &tmp_deps->reg_last[reg];\n-\t  struct deps_reg *succ_deps_reg = &succ_deps->reg_last[reg];\n-\n-\t  for (u = tmp_deps_reg->uses; u; u = XEXP (u, 1))\n-\t    if (! find_insn_list (XEXP (u, 0), succ_deps_reg->uses))\n-\t      succ_deps_reg->uses\n-\t\t= alloc_INSN_LIST (XEXP (u, 0), succ_deps_reg->uses);\n-\n-\t  for (u = tmp_deps_reg->sets; u; u = XEXP (u, 1))\n-\t    if (! find_insn_list (XEXP (u, 0), succ_deps_reg->sets))\n-\t      succ_deps_reg->sets\n-\t\t= alloc_INSN_LIST (XEXP (u, 0), succ_deps_reg->sets);\n-\n-\t  for (u = tmp_deps_reg->clobbers; u; u = XEXP (u, 1))\n-\t    if (! find_insn_list (XEXP (u, 0), succ_deps_reg->clobbers))\n-\t      succ_deps_reg->clobbers\n-\t\t= alloc_INSN_LIST (XEXP (u, 0), succ_deps_reg->clobbers);\n-\t});\n-      IOR_REG_SET (&succ_deps->reg_last_in_use, &tmp_deps->reg_last_in_use);\n-\n-      /* Mem read/write lists are inherited by bb_succ.  */\n-      link_insn = tmp_deps->pending_read_insns;\n-      link_mem = tmp_deps->pending_read_mems;\n-      while (link_insn)\n-\t{\n-\t  if (!(find_insn_mem_list (XEXP (link_insn, 0),\n-\t\t\t\t    XEXP (link_mem, 0),\n-\t\t\t\t    succ_deps->pending_read_insns,\n-\t\t\t\t    succ_deps->pending_read_mems)))\n-\t    add_insn_mem_dependence (succ_deps, &succ_deps->pending_read_insns,\n-\t\t\t\t     &succ_deps->pending_read_mems,\n-\t\t\t\t     XEXP (link_insn, 0), XEXP (link_mem, 0));\n-\t  link_insn = XEXP (link_insn, 1);\n-\t  link_mem = XEXP (link_mem, 1);\n-\t}\n+  if (e > 0)\n+    do\n+      {\n+\tint b_succ = TO_BLOCK (e);\n+\tint bb_succ = BLOCK_TO_BB (b_succ);\n+\tstruct deps *succ_deps = bb_deps + bb_succ;\n+\tint reg;\n+\n+\t/* Only bbs \"below\" bb, in the same region, are interesting.  */\n+\tif (CONTAINING_RGN (b) != CONTAINING_RGN (b_succ)\n+\t    || bb_succ <= bb)\n+\t  {\n+\t    e = NEXT_OUT (e);\n+\t    continue;\n+\t  }\n \n-      link_insn = tmp_deps->pending_write_insns;\n-      link_mem = tmp_deps->pending_write_mems;\n-      while (link_insn)\n-\t{\n-\t  if (!(find_insn_mem_list (XEXP (link_insn, 0),\n-\t\t\t\t    XEXP (link_mem, 0),\n-\t\t\t\t    succ_deps->pending_write_insns,\n-\t\t\t\t    succ_deps->pending_write_mems)))\n-\t    add_insn_mem_dependence (succ_deps,\n-\t\t\t\t     &succ_deps->pending_write_insns,\n-\t\t\t\t     &succ_deps->pending_write_mems,\n-\t\t\t\t     XEXP (link_insn, 0), XEXP (link_mem, 0));\n-\n-\t  link_insn = XEXP (link_insn, 1);\n-\t  link_mem = XEXP (link_mem, 1);\n-\t}\n+\t/* The reg_last lists are inherited by bb_succ.  */\n+\tEXECUTE_IF_SET_IN_REG_SET (&pred_deps->reg_last_in_use, 0, reg,\n+\t  {\n+\t    struct deps_reg *pred_rl = &pred_deps->reg_last[reg];\n+\t    struct deps_reg *succ_rl = &succ_deps->reg_last[reg];\n+\n+\t    succ_rl->uses = concat_INSN_LIST (pred_rl->uses, succ_rl->uses);\n+\t    succ_rl->sets = concat_INSN_LIST (pred_rl->sets, succ_rl->sets);\n+\t    succ_rl->clobbers = concat_INSN_LIST (pred_rl->clobbers,\n+\t\t\t\t\t\t  succ_rl->clobbers);\n+\t  });\n+\tIOR_REG_SET (&succ_deps->reg_last_in_use, &pred_deps->reg_last_in_use);\n+\n+\t/* Mem read/write lists are inherited by bb_succ.  */\n+\tconcat_insn_mem_list (pred_deps->pending_read_insns,\n+\t\t\t      pred_deps->pending_read_mems,\n+\t\t\t      &succ_deps->pending_read_insns,\n+\t\t\t      &succ_deps->pending_read_mems);\n+\tconcat_insn_mem_list (pred_deps->pending_write_insns,\n+\t\t\t      pred_deps->pending_write_mems,\n+\t\t\t      &succ_deps->pending_write_insns,\n+\t\t\t      &succ_deps->pending_write_mems);\n+\n+\tsucc_deps->last_pending_memory_flush\n+\t  = concat_INSN_LIST (pred_deps->last_pending_memory_flush,\n+\t\t\t      succ_deps->last_pending_memory_flush);\n+\t\n+\tsucc_deps->pending_lists_length += pred_deps->pending_lists_length;\n+\tsucc_deps->pending_flush_length += pred_deps->pending_flush_length;\n+\n+\t/* last_function_call is inherited by bb_succ.  */\n+\tsucc_deps->last_function_call\n+\t  = concat_INSN_LIST (pred_deps->last_function_call,\n+\t\t\t      succ_deps->last_function_call);\n+\n+\t/* sched_before_next_call is inherited by bb_succ.  */\n+\tsucc_deps->sched_before_next_call\n+\t  = concat_INSN_LIST (pred_deps->sched_before_next_call,\n+\t\t\t      succ_deps->sched_before_next_call);\n+\n+\te = NEXT_OUT (e);\n+      }\n+    while (e != first_edge);\n \n-      /* last_function_call is inherited by bb_succ.  */\n-      for (u = tmp_deps->last_function_call; u; u = XEXP (u, 1))\n-\tif (! find_insn_list (XEXP (u, 0), succ_deps->last_function_call))\n-\t  succ_deps->last_function_call\n-\t    = alloc_INSN_LIST (XEXP (u, 0), succ_deps->last_function_call);\n-\n-      /* last_pending_memory_flush is inherited by bb_succ.  */\n-      for (u = tmp_deps->last_pending_memory_flush; u; u = XEXP (u, 1))\n-\tif (! find_insn_list (XEXP (u, 0),\n-\t\t\t      succ_deps->last_pending_memory_flush))\n-\t  succ_deps->last_pending_memory_flush\n-\t    = alloc_INSN_LIST (XEXP (u, 0),\n-\t\t\t       succ_deps->last_pending_memory_flush);\n-\n-      /* sched_before_next_call is inherited by bb_succ.  */\n-      x = LOG_LINKS (tmp_deps->sched_before_next_call);\n-      for (; x; x = XEXP (x, 1))\n-\tadd_dependence (succ_deps->sched_before_next_call,\n-\t\t\tXEXP (x, 0), REG_DEP_ANTI);\n-\n-      e = NEXT_OUT (e);\n-    }\n-  while (e != first_edge);\n+  /* These lists should point to the right place, for correct\n+     freeing later.  */\n+  bb_deps[bb].pending_read_insns = pred_deps->pending_read_insns;\n+  bb_deps[bb].pending_read_mems = pred_deps->pending_read_mems;\n+  bb_deps[bb].pending_write_insns = pred_deps->pending_write_insns;\n+  bb_deps[bb].pending_write_mems = pred_deps->pending_write_mems;\n+\n+  /* Can't allow these to be freed twice.  */\n+  pred_deps->pending_read_insns = 0;\n+  pred_deps->pending_read_mems = 0;\n+  pred_deps->pending_write_insns = 0;\n+  pred_deps->pending_write_mems = 0;\n }\n \n /* Compute backward dependences inside bb.  In a multiple blocks region:"}]}