{"sha": "d458f806afe07d1e06bdf275e94d05a716f41bf6", "node_id": "C_kwDOANBUbNoAKGQ0NThmODA2YWZlMDdkMWUwNmJkZjI3NWU5NGQwNWE3MTZmNDFiZjY", "commit": {"author": {"name": "Thomas Neumann", "email": "tneumann@users.sourceforge.net", "date": "2022-09-18T09:31:01Z"}, "committer": {"name": "Thomas Neumann", "email": "tneumann@users.sourceforge.net", "date": "2022-09-18T09:34:39Z"}, "message": "Remove dependency on uintptr_t in libgcc\n\nuintptr_t is no available for all targets, use __UINTPTR_TYPE__\ninstead.\n\nlibgcc/ChangeLog:\n\n\t* unwind-dw2-fde.c: Replace uintptr_t with typedef\n\tfor __UINTPTR_TYPE__.\n\t* unwind-dw2-btree.h: Likewise.", "tree": {"sha": "9e6017a2ef44148594a9cdaf66f5d8fb9b1d3a75", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/9e6017a2ef44148594a9cdaf66f5d8fb9b1d3a75"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/d458f806afe07d1e06bdf275e94d05a716f41bf6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d458f806afe07d1e06bdf275e94d05a716f41bf6", "html_url": "https://github.com/Rust-GCC/gccrs/commit/d458f806afe07d1e06bdf275e94d05a716f41bf6", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d458f806afe07d1e06bdf275e94d05a716f41bf6/comments", "author": {"login": "neumannt", "id": 25097991, "node_id": "MDQ6VXNlcjI1MDk3OTkx", "avatar_url": "https://avatars.githubusercontent.com/u/25097991?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neumannt", "html_url": "https://github.com/neumannt", "followers_url": "https://api.github.com/users/neumannt/followers", "following_url": "https://api.github.com/users/neumannt/following{/other_user}", "gists_url": "https://api.github.com/users/neumannt/gists{/gist_id}", "starred_url": "https://api.github.com/users/neumannt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neumannt/subscriptions", "organizations_url": "https://api.github.com/users/neumannt/orgs", "repos_url": "https://api.github.com/users/neumannt/repos", "events_url": "https://api.github.com/users/neumannt/events{/privacy}", "received_events_url": "https://api.github.com/users/neumannt/received_events", "type": "User", "site_admin": false}, "committer": {"login": "neumannt", "id": 25097991, "node_id": "MDQ6VXNlcjI1MDk3OTkx", "avatar_url": "https://avatars.githubusercontent.com/u/25097991?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neumannt", "html_url": "https://github.com/neumannt", "followers_url": "https://api.github.com/users/neumannt/followers", "following_url": "https://api.github.com/users/neumannt/following{/other_user}", "gists_url": "https://api.github.com/users/neumannt/gists{/gist_id}", "starred_url": "https://api.github.com/users/neumannt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neumannt/subscriptions", "organizations_url": "https://api.github.com/users/neumannt/orgs", "repos_url": "https://api.github.com/users/neumannt/repos", "events_url": "https://api.github.com/users/neumannt/events{/privacy}", "received_events_url": "https://api.github.com/users/neumannt/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "e3c4a86e6b98b334b1a20f2529e6c59f6e19f73d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e3c4a86e6b98b334b1a20f2529e6c59f6e19f73d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e3c4a86e6b98b334b1a20f2529e6c59f6e19f73d"}], "stats": {"total": 87, "additions": 45, "deletions": 42}, "files": [{"sha": "ace507d9ffbdffb739d6bda4b285e0b0809212c0", "filename": "libgcc/unwind-dw2-btree.h", "status": "modified", "additions": 35, "deletions": 34, "changes": 69, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d458f806afe07d1e06bdf275e94d05a716f41bf6/libgcc%2Funwind-dw2-btree.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d458f806afe07d1e06bdf275e94d05a716f41bf6/libgcc%2Funwind-dw2-btree.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgcc%2Funwind-dw2-btree.h?ref=d458f806afe07d1e06bdf275e94d05a716f41bf6", "patch": "@@ -39,7 +39,7 @@ struct version_lock\n   // range. Even on 32 bit platforms that would require 1 billion\n   // frame registrations within the time span of a few assembler\n   // instructions.\n-  uintptr_t version_lock;\n+  uintptr_type version_lock;\n };\n \n #ifdef __GTHREAD_HAS_COND\n@@ -60,7 +60,7 @@ version_lock_initialize_locked_exclusive (struct version_lock *vl)\n static inline bool\n version_lock_try_lock_exclusive (struct version_lock *vl)\n {\n-  uintptr_t state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n+  uintptr_type state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n   if (state & 1)\n     return false;\n   return __atomic_compare_exchange_n (&(vl->version_lock), &state, state | 1,\n@@ -78,7 +78,7 @@ version_lock_lock_exclusive (struct version_lock *vl)\n \n   // We should virtually never get contention here, as frame\n   // changes are rare.\n-  uintptr_t state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n+  uintptr_type state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n   if (!(state & 1))\n     {\n       if (__atomic_compare_exchange_n (&(vl->version_lock), &state, state | 1,\n@@ -134,8 +134,8 @@ static void\n version_lock_unlock_exclusive (struct version_lock *vl)\n {\n   // increase version, reset exclusive lock bits\n-  uintptr_t state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n-  uintptr_t ns = (state + 4) & (~((uintptr_t) 3));\n+  uintptr_type state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n+  uintptr_type ns = (state + 4) & (~((uintptr_type) 3));\n   state = __atomic_exchange_n (&(vl->version_lock), ns, __ATOMIC_SEQ_CST);\n \n #ifdef __GTHREAD_HAS_COND\n@@ -152,9 +152,9 @@ version_lock_unlock_exclusive (struct version_lock *vl)\n // Acquire an optimistic \"lock\". Note that this does not lock at all, it\n // only allows for validation later.\n static inline bool\n-version_lock_lock_optimistic (const struct version_lock *vl, uintptr_t *lock)\n+version_lock_lock_optimistic (const struct version_lock *vl, uintptr_type *lock)\n {\n-  uintptr_t state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n+  uintptr_type state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n   *lock = state;\n \n   // Acquiring the lock fails when there is currently an exclusive lock.\n@@ -163,34 +163,34 @@ version_lock_lock_optimistic (const struct version_lock *vl, uintptr_t *lock)\n \n // Validate a previously acquired \"lock\".\n static inline bool\n-version_lock_validate (const struct version_lock *vl, uintptr_t lock)\n+version_lock_validate (const struct version_lock *vl, uintptr_type lock)\n {\n   // Prevent the reordering of non-atomic loads behind the atomic load.\n   // Hans Boehm, Can Seqlocks Get Along with Programming Language Memory\n   // Models?, Section 4.\n   __atomic_thread_fence (__ATOMIC_ACQUIRE);\n \n   // Check that the node is still in the same state.\n-  uintptr_t state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n+  uintptr_type state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n   return (state == lock);\n }\n \n // The largest possible separator value.\n-static const uintptr_t max_separator = ~((uintptr_t) (0));\n+static const uintptr_type max_separator = ~((uintptr_type) (0));\n \n struct btree_node;\n \n // Inner entry. The child tree contains all entries <= separator.\n struct inner_entry\n {\n-  uintptr_t separator;\n+  uintptr_type separator;\n   struct btree_node *child;\n };\n \n // Leaf entry. Stores an object entry.\n struct leaf_entry\n {\n-  uintptr_t base, size;\n+  uintptr_type base, size;\n   struct object *ob;\n };\n \n@@ -248,7 +248,7 @@ btree_node_needs_merge (const struct btree_node *n)\n }\n \n // Get the fence key for inner nodes.\n-static inline uintptr_t\n+static inline uintptr_type\n btree_node_get_fence_key (const struct btree_node *n)\n {\n   // For inner nodes we just return our right-most entry.\n@@ -257,7 +257,7 @@ btree_node_get_fence_key (const struct btree_node *n)\n \n // Find the position for a slot in an inner node.\n static unsigned\n-btree_node_find_inner_slot (const struct btree_node *n, uintptr_t value)\n+btree_node_find_inner_slot (const struct btree_node *n, uintptr_type value)\n {\n   for (unsigned index = 0, ec = n->entry_count; index != ec; ++index)\n     if (n->content.children[index].separator >= value)\n@@ -267,7 +267,7 @@ btree_node_find_inner_slot (const struct btree_node *n, uintptr_t value)\n \n // Find the position for a slot in a leaf node.\n static unsigned\n-btree_node_find_leaf_slot (const struct btree_node *n, uintptr_t value)\n+btree_node_find_leaf_slot (const struct btree_node *n, uintptr_type value)\n {\n   for (unsigned index = 0, ec = n->entry_count; index != ec; ++index)\n     if (n->content.entries[index].base + n->content.entries[index].size > value)\n@@ -299,23 +299,23 @@ btree_node_unlock_exclusive (struct btree_node *n)\n // Acquire an optimistic \"lock\". Note that this does not lock at all, it\n // only allows for validation later.\n static inline bool\n-btree_node_lock_optimistic (const struct btree_node *n, uintptr_t *lock)\n+btree_node_lock_optimistic (const struct btree_node *n, uintptr_type *lock)\n {\n   return version_lock_lock_optimistic (&(n->version_lock), lock);\n }\n \n // Validate a previously acquire lock.\n static inline bool\n-btree_node_validate (const struct btree_node *n, uintptr_t lock)\n+btree_node_validate (const struct btree_node *n, uintptr_type lock)\n {\n   return version_lock_validate (&(n->version_lock), lock);\n }\n \n // Insert a new separator after splitting.\n static void\n btree_node_update_separator_after_split (struct btree_node *n,\n-\t\t\t\t\t uintptr_t old_separator,\n-\t\t\t\t\t uintptr_t new_separator,\n+\t\t\t\t\t uintptr_type old_separator,\n+\t\t\t\t\t uintptr_type new_separator,\n \t\t\t\t\t struct btree_node *new_right)\n {\n   unsigned slot = btree_node_find_inner_slot (n, old_separator);\n@@ -474,13 +474,13 @@ btree_handle_root_split (struct btree *t, struct btree_node **node,\n // Split an inner node.\n static void\n btree_split_inner (struct btree *t, struct btree_node **inner,\n-\t\t   struct btree_node **parent, uintptr_t target)\n+\t\t   struct btree_node **parent, uintptr_type target)\n {\n   // Check for the root.\n   btree_handle_root_split (t, inner, parent);\n \n   // Create two inner node.\n-  uintptr_t right_fence = btree_node_get_fence_key (*inner);\n+  uintptr_type right_fence = btree_node_get_fence_key (*inner);\n   struct btree_node *left_inner = *inner;\n   struct btree_node *right_inner = btree_allocate_node (t, true);\n   unsigned split = left_inner->entry_count / 2;\n@@ -489,7 +489,7 @@ btree_split_inner (struct btree *t, struct btree_node **inner,\n     right_inner->content.children[index]\n       = left_inner->content.children[split + index];\n   left_inner->entry_count = split;\n-  uintptr_t left_fence = btree_node_get_fence_key (left_inner);\n+  uintptr_type left_fence = btree_node_get_fence_key (left_inner);\n   btree_node_update_separator_after_split (*parent, right_fence, left_fence,\n \t\t\t\t\t   right_inner);\n   if (target <= left_fence)\n@@ -507,13 +507,14 @@ btree_split_inner (struct btree *t, struct btree_node **inner,\n // Split a leaf node.\n static void\n btree_split_leaf (struct btree *t, struct btree_node **leaf,\n-\t\t  struct btree_node **parent, uintptr_t fence, uintptr_t target)\n+\t\t  struct btree_node **parent, uintptr_type fence,\n+\t\t  uintptr_type target)\n {\n   // Check for the root.\n   btree_handle_root_split (t, leaf, parent);\n \n   // Create two leaf nodes.\n-  uintptr_t right_fence = fence;\n+  uintptr_type right_fence = fence;\n   struct btree_node *left_leaf = *leaf;\n   struct btree_node *right_leaf = btree_allocate_node (t, false);\n   unsigned split = left_leaf->entry_count / 2;\n@@ -522,7 +523,7 @@ btree_split_leaf (struct btree *t, struct btree_node **leaf,\n     right_leaf->content.entries[index]\n       = left_leaf->content.entries[split + index];\n   left_leaf->entry_count = split;\n-  uintptr_t left_fence = right_leaf->content.entries[0].base - 1;\n+  uintptr_type left_fence = right_leaf->content.entries[0].base - 1;\n   btree_node_update_separator_after_split (*parent, right_fence, left_fence,\n \t\t\t\t\t   right_leaf);\n   if (target <= left_fence)\n@@ -540,7 +541,7 @@ btree_split_leaf (struct btree *t, struct btree_node **leaf,\n // Merge (or balance) child nodes.\n static struct btree_node *\n btree_merge_node (struct btree *t, unsigned child_slot,\n-\t\t  struct btree_node *parent, uintptr_t target)\n+\t\t  struct btree_node *parent, uintptr_type target)\n {\n   // Choose the emptiest neighbor and lock both. The target child is already\n   // locked.\n@@ -693,7 +694,7 @@ btree_merge_node (struct btree *t, unsigned child_slot,\n       left_node->entry_count += to_shift;\n       right_node->entry_count -= to_shift;\n     }\n-  uintptr_t left_fence;\n+  uintptr_type left_fence;\n   if (btree_node_is_leaf (left_node))\n     {\n       left_fence = right_node->content.entries[0].base - 1;\n@@ -718,7 +719,7 @@ btree_merge_node (struct btree *t, unsigned child_slot,\n \n // Insert an entry.\n static bool\n-btree_insert (struct btree *t, uintptr_t base, uintptr_t size,\n+btree_insert (struct btree *t, uintptr_type base, uintptr_type size,\n \t      struct object *ob)\n {\n   // Sanity check.\n@@ -747,7 +748,7 @@ btree_insert (struct btree *t, uintptr_t base, uintptr_t size,\n   // But that is more difficult to implement and frame registration is\n   // rare anyway, we use simple locking for now.\n \n-  uintptr_t fence = max_separator;\n+  uintptr_type fence = max_separator;\n   while (btree_node_is_inner (iter))\n     {\n       // Use eager splits to avoid lock coupling up.\n@@ -790,7 +791,7 @@ btree_insert (struct btree *t, uintptr_t base, uintptr_t size,\n \n // Remove an entry.\n static struct object *\n-btree_remove (struct btree *t, uintptr_t base)\n+btree_remove (struct btree *t, uintptr_type base)\n {\n   // Access the root.\n   version_lock_lock_exclusive (&(t->root_lock));\n@@ -838,7 +839,7 @@ btree_remove (struct btree *t, uintptr_t base)\n \n // Find the corresponding entry for the given address.\n static struct object *\n-btree_lookup (const struct btree *t, uintptr_t target_addr)\n+btree_lookup (const struct btree *t, uintptr_type target_addr)\n {\n   // Within this function many loads are relaxed atomic loads.\n   // Use a macro to keep the code reasonable.\n@@ -867,7 +868,7 @@ btree_lookup (const struct btree *t, uintptr_t target_addr)\n \n restart:\n   struct btree_node *iter;\n-  uintptr_t lock;\n+  uintptr_type lock;\n   {\n     // Accessing the root node requires defending against concurrent pointer\n     // changes Thus we couple rootLock -> lock on root node -> validate rootLock\n@@ -878,7 +879,7 @@ btree_lookup (const struct btree *t, uintptr_t target_addr)\n       goto restart;\n     if (!iter)\n       return NULL;\n-    uintptr_t child_lock;\n+    uintptr_type child_lock;\n     if ((!btree_node_lock_optimistic (iter, &child_lock))\n \t|| (!version_lock_validate (&(t->root_lock), lock)))\n       goto restart;\n@@ -910,7 +911,7 @@ btree_lookup (const struct btree *t, uintptr_t target_addr)\n \n \t  // The node content can change at any point in time, thus we must\n \t  // interleave parent and child checks.\n-\t  uintptr_t child_lock;\n+\t  uintptr_type child_lock;\n \t  if (!btree_node_lock_optimistic (child, &child_lock))\n \t    goto restart;\n \t  if (!btree_node_validate (iter, lock))"}, {"sha": "919abfe06642ad13affa68f780136edc608bc094", "filename": "libgcc/unwind-dw2-fde.c", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d458f806afe07d1e06bdf275e94d05a716f41bf6/libgcc%2Funwind-dw2-fde.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d458f806afe07d1e06bdf275e94d05a716f41bf6/libgcc%2Funwind-dw2-fde.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgcc%2Funwind-dw2-fde.c?ref=d458f806afe07d1e06bdf275e94d05a716f41bf6", "patch": "@@ -42,6 +42,8 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n #endif\n #endif\n \n+typedef __UINTPTR_TYPE__ uintptr_type;\n+\n #ifdef ATOMIC_FDE_FAST_PATH\n #include \"unwind-dw2-btree.h\"\n \n@@ -58,7 +60,7 @@ release_registered_frames (void)\n }\n \n static void\n-get_pc_range (const struct object *ob, uintptr_t *range);\n+get_pc_range (const struct object *ob, uintptr_type *range);\n static void\n init_object (struct object *ob);\n \n@@ -124,7 +126,7 @@ __register_frame_info_bases (const void *begin, struct object *ob,\n   init_object (ob);\n \n   // And register the frame\n-  uintptr_t range[2];\n+  uintptr_type range[2];\n   get_pc_range (ob, range);\n   btree_insert (&registered_frames, range[0], range[1] - range[0], ob);\n #else\n@@ -178,7 +180,7 @@ __register_frame_info_table_bases (void *begin, struct object *ob,\n   init_object (ob);\n \n   // And register the frame\n-  uintptr_t range[2];\n+  uintptr_type range[2];\n   get_pc_range (ob, range);\n   btree_insert (&registered_frames, range[0], range[1] - range[0], ob);\n #else\n@@ -237,7 +239,7 @@ __deregister_frame_info_bases (const void *begin)\n #ifdef DWARF2_OBJECT_END_PTR_EXTENSION\n   lookupob.fde_end = NULL;\n #endif\n-  uintptr_t range[2];\n+  uintptr_type range[2];\n   get_pc_range (&lookupob, range);\n \n   // And remove\n@@ -677,7 +679,7 @@ end_fde_sort (struct object *ob, struct fde_accumulator *accu, size_t count)\n \n static size_t\n classify_object_over_fdes (struct object *ob, const fde *this_fde,\n-\t\t\t   uintptr_t *range)\n+\t\t\t   uintptr_type *range)\n {\n   const struct dwarf_cie *last_cie = 0;\n   size_t count = 0;\n@@ -892,11 +894,11 @@ init_object (struct object* ob)\n #ifdef ATOMIC_FDE_FAST_PATH\n /* Get the PC range for lookup */\n static void\n-get_pc_range (const struct object *ob, uintptr_t *range)\n+get_pc_range (const struct object *ob, uintptr_type *range)\n {\n   // It is safe to cast to non-const object* here as\n   // classify_object_over_fdes does not modify ob in query mode.\n-  struct object *ncob = (struct object *) (uintptr_t) ob;\n+  struct object *ncob = (struct object *) (uintptr_type) ob;\n   range[0] = range[1] = 0;\n   if (ob->s.b.sorted)\n     {\n@@ -1131,7 +1133,7 @@ _Unwind_Find_FDE (void *pc, struct dwarf_eh_bases *bases)\n   const fde *f = NULL;\n \n #ifdef ATOMIC_FDE_FAST_PATH\n-  ob = btree_lookup (&registered_frames, (uintptr_t) pc);\n+  ob = btree_lookup (&registered_frames, (uintptr_type) pc);\n   if (!ob)\n     return NULL;\n "}]}