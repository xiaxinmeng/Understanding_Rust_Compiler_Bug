{"sha": "7cb0403f41978641d6e0954761fa0600bc2ad10c", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6N2NiMDQwM2Y0MTk3ODY0MWQ2ZTA5NTQ3NjFmYTA2MDBiYzJhZDEwYw==", "commit": {"author": {"name": "Alan Lawrence", "email": "alan.lawrence@arm.com", "date": "2014-04-29T16:55:48Z"}, "committer": {"name": "Alan Lawrence", "email": "alalaw01@gcc.gnu.org", "date": "2014-04-29T16:55:48Z"}, "message": "arm_neon.h (vzip1_f32, [...]): Replace inline __asm__ with __builtin_shuffle.\n\n\t* config/aarch64/arm_neon.h (vzip1_f32, vzip1_p8, vzip1_p16, vzip1_s8,\n\tvzip1_s16, vzip1_s32, vzip1_u8, vzip1_u16, vzip1_u32, vzip1q_f32,\n\tvzip1q_f64, vzip1q_p8, vzip1q_p16, vzip1q_s8, vzip1q_s16, vzip1q_s32,\n\tvzip1q_s64, vzip1q_u8, vzip1q_u16, vzip1q_u32, vzip1q_u64, vzip2_f32,\n\tvzip2_p8, vzip2_p16, vzip2_s8, vzip2_s16, vzip2_s32, vzip2_u8,\n\tvzip2_u16, vzip2_u32, vzip2q_f32, vzip2q_f64, vzip2q_p8, vzip2q_p16,\n\tvzip2q_s8, vzip2q_s16, vzip2q_s32, vzip2q_s64, vzip2q_u8, vzip2q_u16,\n\tvzip2q_u32, vzip2q_u64): Replace inline __asm__ with __builtin_shuffle.\n\nFrom-SVN: r209906", "tree": {"sha": "252ec37adb99a940c1248810e01333a91bb3efd2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/252ec37adb99a940c1248810e01333a91bb3efd2"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/7cb0403f41978641d6e0954761fa0600bc2ad10c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7cb0403f41978641d6e0954761fa0600bc2ad10c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/7cb0403f41978641d6e0954761fa0600bc2ad10c", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7cb0403f41978641d6e0954761fa0600bc2ad10c/comments", "author": null, "committer": null, "parents": [{"sha": "8933ee48c5fb753eb987a9a4270961fb3d35dad3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8933ee48c5fb753eb987a9a4270961fb3d35dad3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8933ee48c5fb753eb987a9a4270961fb3d35dad3"}], "stats": {"total": 911, "additions": 449, "deletions": 462}, "files": [{"sha": "7b4bb549d8ab58c81cd2468aec2e765492f01a7a", "filename": "gcc/ChangeLog", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7cb0403f41978641d6e0954761fa0600bc2ad10c/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7cb0403f41978641d6e0954761fa0600bc2ad10c/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=7cb0403f41978641d6e0954761fa0600bc2ad10c", "patch": "@@ -1,3 +1,14 @@\n+2014-04-29  Alan Lawrence  <alan.lawrence@arm.com>\n+\n+\t* config/aarch64/arm_neon.h (vzip1_f32, vzip1_p8, vzip1_p16, vzip1_s8,\n+\tvzip1_s16, vzip1_s32, vzip1_u8, vzip1_u16, vzip1_u32, vzip1q_f32,\n+\tvzip1q_f64, vzip1q_p8, vzip1q_p16, vzip1q_s8, vzip1q_s16, vzip1q_s32,\n+\tvzip1q_s64, vzip1q_u8, vzip1q_u16, vzip1q_u32, vzip1q_u64, vzip2_f32,\n+\tvzip2_p8, vzip2_p16, vzip2_s8, vzip2_s16, vzip2_s32, vzip2_u8,\n+\tvzip2_u16, vzip2_u32, vzip2q_f32, vzip2q_f64, vzip2q_p8, vzip2q_p16,\n+\tvzip2q_s8, vzip2q_s16, vzip2q_s32, vzip2q_s64, vzip2q_u8, vzip2q_u16,\n+\tvzip2q_u32, vzip2q_u64): Replace inline __asm__ with __builtin_shuffle.\n+\n 2014-04-29  David Malcolm  <dmalcolm@redhat.com>\n \n \t* tree-cfg.c (dump_function_to_file): Dump the return type of"}, {"sha": "fa5766787e9b490292a835834db98a1c3a5556e4", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 438, "deletions": 462, "changes": 900, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7cb0403f41978641d6e0954761fa0600bc2ad10c/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7cb0403f41978641d6e0954761fa0600bc2ad10c/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=7cb0403f41978641d6e0954761fa0600bc2ad10c", "patch": "@@ -13661,468 +13661,6 @@ vuzp2q_u64 (uint64x2_t a, uint64x2_t b)\n   return result;\n }\n \n-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n-vzip1_f32 (float32x2_t a, float32x2_t b)\n-{\n-  float32x2_t result;\n-  __asm__ (\"zip1 %0.2s,%1.2s,%2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n-vzip1_p8 (poly8x8_t a, poly8x8_t b)\n-{\n-  poly8x8_t result;\n-  __asm__ (\"zip1 %0.8b,%1.8b,%2.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))\n-vzip1_p16 (poly16x4_t a, poly16x4_t b)\n-{\n-  poly16x4_t result;\n-  __asm__ (\"zip1 %0.4h,%1.4h,%2.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n-vzip1_s8 (int8x8_t a, int8x8_t b)\n-{\n-  int8x8_t result;\n-  __asm__ (\"zip1 %0.8b,%1.8b,%2.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n-vzip1_s16 (int16x4_t a, int16x4_t b)\n-{\n-  int16x4_t result;\n-  __asm__ (\"zip1 %0.4h,%1.4h,%2.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n-vzip1_s32 (int32x2_t a, int32x2_t b)\n-{\n-  int32x2_t result;\n-  __asm__ (\"zip1 %0.2s,%1.2s,%2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vzip1_u8 (uint8x8_t a, uint8x8_t b)\n-{\n-  uint8x8_t result;\n-  __asm__ (\"zip1 %0.8b,%1.8b,%2.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n-vzip1_u16 (uint16x4_t a, uint16x4_t b)\n-{\n-  uint16x4_t result;\n-  __asm__ (\"zip1 %0.4h,%1.4h,%2.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vzip1_u32 (uint32x2_t a, uint32x2_t b)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"zip1 %0.2s,%1.2s,%2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n-vzip1q_f32 (float32x4_t a, float32x4_t b)\n-{\n-  float32x4_t result;\n-  __asm__ (\"zip1 %0.4s,%1.4s,%2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n-vzip1q_f64 (float64x2_t a, float64x2_t b)\n-{\n-  float64x2_t result;\n-  __asm__ (\"zip1 %0.2d,%1.2d,%2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n-vzip1q_p8 (poly8x16_t a, poly8x16_t b)\n-{\n-  poly8x16_t result;\n-  __asm__ (\"zip1 %0.16b,%1.16b,%2.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))\n-vzip1q_p16 (poly16x8_t a, poly16x8_t b)\n-{\n-  poly16x8_t result;\n-  __asm__ (\"zip1 %0.8h,%1.8h,%2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n-vzip1q_s8 (int8x16_t a, int8x16_t b)\n-{\n-  int8x16_t result;\n-  __asm__ (\"zip1 %0.16b,%1.16b,%2.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n-vzip1q_s16 (int16x8_t a, int16x8_t b)\n-{\n-  int16x8_t result;\n-  __asm__ (\"zip1 %0.8h,%1.8h,%2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n-vzip1q_s32 (int32x4_t a, int32x4_t b)\n-{\n-  int32x4_t result;\n-  __asm__ (\"zip1 %0.4s,%1.4s,%2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))\n-vzip1q_s64 (int64x2_t a, int64x2_t b)\n-{\n-  int64x2_t result;\n-  __asm__ (\"zip1 %0.2d,%1.2d,%2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n-vzip1q_u8 (uint8x16_t a, uint8x16_t b)\n-{\n-  uint8x16_t result;\n-  __asm__ (\"zip1 %0.16b,%1.16b,%2.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vzip1q_u16 (uint16x8_t a, uint16x8_t b)\n-{\n-  uint16x8_t result;\n-  __asm__ (\"zip1 %0.8h,%1.8h,%2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vzip1q_u32 (uint32x4_t a, uint32x4_t b)\n-{\n-  uint32x4_t result;\n-  __asm__ (\"zip1 %0.4s,%1.4s,%2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n-vzip1q_u64 (uint64x2_t a, uint64x2_t b)\n-{\n-  uint64x2_t result;\n-  __asm__ (\"zip1 %0.2d,%1.2d,%2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n-vzip2_f32 (float32x2_t a, float32x2_t b)\n-{\n-  float32x2_t result;\n-  __asm__ (\"zip2 %0.2s,%1.2s,%2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n-vzip2_p8 (poly8x8_t a, poly8x8_t b)\n-{\n-  poly8x8_t result;\n-  __asm__ (\"zip2 %0.8b,%1.8b,%2.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))\n-vzip2_p16 (poly16x4_t a, poly16x4_t b)\n-{\n-  poly16x4_t result;\n-  __asm__ (\"zip2 %0.4h,%1.4h,%2.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n-vzip2_s8 (int8x8_t a, int8x8_t b)\n-{\n-  int8x8_t result;\n-  __asm__ (\"zip2 %0.8b,%1.8b,%2.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n-vzip2_s16 (int16x4_t a, int16x4_t b)\n-{\n-  int16x4_t result;\n-  __asm__ (\"zip2 %0.4h,%1.4h,%2.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n-vzip2_s32 (int32x2_t a, int32x2_t b)\n-{\n-  int32x2_t result;\n-  __asm__ (\"zip2 %0.2s,%1.2s,%2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n-vzip2_u8 (uint8x8_t a, uint8x8_t b)\n-{\n-  uint8x8_t result;\n-  __asm__ (\"zip2 %0.8b,%1.8b,%2.8b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n-vzip2_u16 (uint16x4_t a, uint16x4_t b)\n-{\n-  uint16x4_t result;\n-  __asm__ (\"zip2 %0.4h,%1.4h,%2.4h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n-vzip2_u32 (uint32x2_t a, uint32x2_t b)\n-{\n-  uint32x2_t result;\n-  __asm__ (\"zip2 %0.2s,%1.2s,%2.2s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n-vzip2q_f32 (float32x4_t a, float32x4_t b)\n-{\n-  float32x4_t result;\n-  __asm__ (\"zip2 %0.4s,%1.4s,%2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n-vzip2q_f64 (float64x2_t a, float64x2_t b)\n-{\n-  float64x2_t result;\n-  __asm__ (\"zip2 %0.2d,%1.2d,%2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n-vzip2q_p8 (poly8x16_t a, poly8x16_t b)\n-{\n-  poly8x16_t result;\n-  __asm__ (\"zip2 %0.16b,%1.16b,%2.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))\n-vzip2q_p16 (poly16x8_t a, poly16x8_t b)\n-{\n-  poly16x8_t result;\n-  __asm__ (\"zip2 %0.8h,%1.8h,%2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n-vzip2q_s8 (int8x16_t a, int8x16_t b)\n-{\n-  int8x16_t result;\n-  __asm__ (\"zip2 %0.16b,%1.16b,%2.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n-vzip2q_s16 (int16x8_t a, int16x8_t b)\n-{\n-  int16x8_t result;\n-  __asm__ (\"zip2 %0.8h,%1.8h,%2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n-vzip2q_s32 (int32x4_t a, int32x4_t b)\n-{\n-  int32x4_t result;\n-  __asm__ (\"zip2 %0.4s,%1.4s,%2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))\n-vzip2q_s64 (int64x2_t a, int64x2_t b)\n-{\n-  int64x2_t result;\n-  __asm__ (\"zip2 %0.2d,%1.2d,%2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n-vzip2q_u8 (uint8x16_t a, uint8x16_t b)\n-{\n-  uint8x16_t result;\n-  __asm__ (\"zip2 %0.16b,%1.16b,%2.16b\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n-vzip2q_u16 (uint16x8_t a, uint16x8_t b)\n-{\n-  uint16x8_t result;\n-  __asm__ (\"zip2 %0.8h,%1.8h,%2.8h\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n-vzip2q_u32 (uint32x4_t a, uint32x4_t b)\n-{\n-  uint32x4_t result;\n-  __asm__ (\"zip2 %0.4s,%1.4s,%2.4s\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n-__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n-vzip2q_u64 (uint64x2_t a, uint64x2_t b)\n-{\n-  uint64x2_t result;\n-  __asm__ (\"zip2 %0.2d,%1.2d,%2.2d\"\n-           : \"=w\"(result)\n-           : \"w\"(a), \"w\"(b)\n-           : /* No clobbers */);\n-  return result;\n-}\n-\n /* End of temporary inline asm implementations.  */\n \n /* Start of temporary inline asm for vldn, vstn and friends.  */\n@@ -25711,6 +25249,444 @@ __INTERLEAVE_LIST (uzp)\n \n /* vzip */\n \n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vzip1_f32 (float32x2_t __a, float32x2_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {3, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {0, 2});\n+#endif\n+}\n+\n+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n+vzip1_p8 (poly8x8_t __a, poly8x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {12, 4, 13, 5, 14, 6, 15, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {0, 8, 1, 9, 2, 10, 3, 11});\n+#endif\n+}\n+\n+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))\n+vzip1_p16 (poly16x4_t __a, poly16x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {6, 2, 7, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {0, 4, 1, 5});\n+#endif\n+}\n+\n+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n+vzip1_s8 (int8x8_t __a, int8x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {12, 4, 13, 5, 14, 6, 15, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {0, 8, 1, 9, 2, 10, 3, 11});\n+#endif\n+}\n+\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vzip1_s16 (int16x4_t __a, int16x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {6, 2, 7, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {0, 4, 1, 5});\n+#endif\n+}\n+\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vzip1_s32 (int32x2_t __a, int32x2_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {3, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {0, 2});\n+#endif\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vzip1_u8 (uint8x8_t __a, uint8x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {12, 4, 13, 5, 14, 6, 15, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {0, 8, 1, 9, 2, 10, 3, 11});\n+#endif\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vzip1_u16 (uint16x4_t __a, uint16x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {6, 2, 7, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {0, 4, 1, 5});\n+#endif\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vzip1_u32 (uint32x2_t __a, uint32x2_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {3, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {0, 2});\n+#endif\n+}\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vzip1q_f32 (float32x4_t __a, float32x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {6, 2, 7, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {0, 4, 1, 5});\n+#endif\n+}\n+\n+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n+vzip1q_f64 (float64x2_t __a, float64x2_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {3, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {0, 2});\n+#endif\n+}\n+\n+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n+vzip1q_p8 (poly8x16_t __a, poly8x16_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {24, 8, 25, 9, 26, 10, 27, 11, 28, 12, 29, 13, 30, 14, 31, 15});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23});\n+#endif\n+}\n+\n+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))\n+vzip1q_p16 (poly16x8_t __a, poly16x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t)\n+      {12, 4, 13, 5, 14, 6, 15, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {0, 8, 1, 9, 2, 10, 3, 11});\n+#endif\n+}\n+\n+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n+vzip1q_s8 (int8x16_t __a, int8x16_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {24, 8, 25, 9, 26, 10, 27, 11, 28, 12, 29, 13, 30, 14, 31, 15});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23});\n+#endif\n+}\n+\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vzip1q_s16 (int16x8_t __a, int16x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t)\n+      {12, 4, 13, 5, 14, 6, 15, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {0, 8, 1, 9, 2, 10, 3, 11});\n+#endif\n+}\n+\n+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n+vzip1q_s32 (int32x4_t __a, int32x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {6, 2, 7, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {0, 4, 1, 5});\n+#endif\n+}\n+\n+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))\n+vzip1q_s64 (int64x2_t __a, int64x2_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {3, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {0, 2});\n+#endif\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vzip1q_u8 (uint8x16_t __a, uint8x16_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {24, 8, 25, 9, 26, 10, 27, 11, 28, 12, 29, 13, 30, 14, 31, 15});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {0, 16, 1, 17, 2, 18, 3, 19, 4, 20, 5, 21, 6, 22, 7, 23});\n+#endif\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vzip1q_u16 (uint16x8_t __a, uint16x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t)\n+      {12, 4, 13, 5, 14, 6, 15, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {0, 8, 1, 9, 2, 10, 3, 11});\n+#endif\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vzip1q_u32 (uint32x4_t __a, uint32x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {6, 2, 7, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {0, 4, 1, 5});\n+#endif\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vzip1q_u64 (uint64x2_t __a, uint64x2_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {3, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {0, 2});\n+#endif\n+}\n+\n+__extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n+vzip2_f32 (float32x2_t __a, float32x2_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {2, 0});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {1, 3});\n+#endif\n+}\n+\n+__extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n+vzip2_p8 (poly8x8_t __a, poly8x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {8, 0, 9, 1, 10, 2, 11, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {4, 12, 5, 13, 6, 14, 7, 15});\n+#endif\n+}\n+\n+__extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))\n+vzip2_p16 (poly16x4_t __a, poly16x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {4, 0, 5, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {2, 6, 3, 7});\n+#endif\n+}\n+\n+__extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n+vzip2_s8 (int8x8_t __a, int8x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {8, 0, 9, 1, 10, 2, 11, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {4, 12, 5, 13, 6, 14, 7, 15});\n+#endif\n+}\n+\n+__extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n+vzip2_s16 (int16x4_t __a, int16x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {4, 0, 5, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {2, 6, 3, 7});\n+#endif\n+}\n+\n+__extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n+vzip2_s32 (int32x2_t __a, int32x2_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {2, 0});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {1, 3});\n+#endif\n+}\n+\n+__extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n+vzip2_u8 (uint8x8_t __a, uint8x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {8, 0, 9, 1, 10, 2, 11, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x8_t) {4, 12, 5, 13, 6, 14, 7, 15});\n+#endif\n+}\n+\n+__extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n+vzip2_u16 (uint16x4_t __a, uint16x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {4, 0, 5, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x4_t) {2, 6, 3, 7});\n+#endif\n+}\n+\n+__extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n+vzip2_u32 (uint32x2_t __a, uint32x2_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {2, 0});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x2_t) {1, 3});\n+#endif\n+}\n+\n+__extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n+vzip2q_f32 (float32x4_t __a, float32x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {4, 0, 5, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {2, 6, 3, 7});\n+#endif\n+}\n+\n+__extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n+vzip2q_f64 (float64x2_t __a, float64x2_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {2, 0});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {1, 3});\n+#endif\n+}\n+\n+__extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n+vzip2q_p8 (poly8x16_t __a, poly8x16_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {16, 0, 17, 1, 18, 2, 19, 3, 20, 4, 21, 5, 22, 6, 23, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31});\n+#endif\n+}\n+\n+__extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))\n+vzip2q_p16 (poly16x8_t __a, poly16x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {8, 0, 9, 1, 10, 2, 11, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t)\n+      {4, 12, 5, 13, 6, 14, 7, 15});\n+#endif\n+}\n+\n+__extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n+vzip2q_s8 (int8x16_t __a, int8x16_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {16, 0, 17, 1, 18, 2, 19, 3, 20, 4, 21, 5, 22, 6, 23, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31});\n+#endif\n+}\n+\n+__extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n+vzip2q_s16 (int16x8_t __a, int16x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {8, 0, 9, 1, 10, 2, 11, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t)\n+      {4, 12, 5, 13, 6, 14, 7, 15});\n+#endif\n+}\n+\n+__extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n+vzip2q_s32 (int32x4_t __a, int32x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {4, 0, 5, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {2, 6, 3, 7});\n+#endif\n+}\n+\n+__extension__ static __inline int64x2_t __attribute__ ((__always_inline__))\n+vzip2q_s64 (int64x2_t __a, int64x2_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {2, 0});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {1, 3});\n+#endif\n+}\n+\n+__extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n+vzip2q_u8 (uint8x16_t __a, uint8x16_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {16, 0, 17, 1, 18, 2, 19, 3, 20, 4, 21, 5, 22, 6, 23, 7});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint8x16_t)\n+      {8, 24, 9, 25, 10, 26, 11, 27, 12, 28, 13, 29, 14, 30, 15, 31});\n+#endif\n+}\n+\n+__extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n+vzip2q_u16 (uint16x8_t __a, uint16x8_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint16x8_t) {8, 0, 9, 1, 10, 2, 11, 3});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint16x8_t)\n+      {4, 12, 5, 13, 6, 14, 7, 15});\n+#endif\n+}\n+\n+__extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n+vzip2q_u32 (uint32x4_t __a, uint32x4_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {4, 0, 5, 1});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint32x4_t) {2, 6, 3, 7});\n+#endif\n+}\n+\n+__extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n+vzip2q_u64 (uint64x2_t __a, uint64x2_t __b)\n+{\n+#ifdef __AARCH64EB__\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {2, 0});\n+#else\n+  return __builtin_shuffle (__a, __b, (uint64x2_t) {1, 3});\n+#endif\n+}\n+\n __INTERLEAVE_LIST (zip)\n \n #undef __INTERLEAVE_LIST"}]}