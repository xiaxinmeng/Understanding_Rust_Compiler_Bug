{"sha": "1359ef39755c2a648c44e410e974450d91a4bd99", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MTM1OWVmMzk3NTVjMmE2NDhjNDRlNDEwZTk3NDQ1MGQ5MWE0YmQ5OQ==", "commit": {"author": {"name": "Uros Bizjak", "email": "ubizjak@gmail.com", "date": "2008-03-13T13:33:47Z"}, "committer": {"name": "Uros Bizjak", "email": "uros@gcc.gnu.org", "date": "2008-03-13T13:33:47Z"}, "message": "re PR target/34000 (GCC pedwarns about use of static inline functions from system headers in extern inline functions)\n\n\tPR target/34000\n\tPR target/35553\n\t* config/i386/xmmintrin.h: Change all static inline functions to\n\textern inline and add __gnu_inline__ attribute.\n\t* config/i386/bmintrin.h: Ditto.\n\t* config/i386/smmintrin.h: Ditto.\n\t* config/i386/tmmintrin.h: Ditto.\n\t* config/i386/mmintrin-common.h: Ditto.\n\t* config/i386/ammintrin.h: Ditto.\n\t* config/i386/emmintrin.h: Ditto.\n\t* config/i386/pmmintrin.h: Ditto.\n\t* config/i386/mmintrin.h: Ditto.\n\t* config/i386/mm3dnow.h: Ditto.\n\ntestsuite/ChangeLog:\n\n\tPR target/34000\n\tPR target/35553\n\t* g++.dg/other/i386-3.C: New test.\n\t* gcc.target/i386/sse-13.c: Redefine extern instead of static.\n\t* gcc.target/i386/sse-14.c: Ditto.\n\t* gcc.target/i386/mmx-1.c: Ditto.\n\t* gcc.target/i386/mmx-2.c: Ditto.\n\t* gcc.target/i386/3dnow-1.c: Ditto.\n\t* gcc.target/i386/3dnow-2.c: Ditto.\n\t* gcc.target/i386/3dnowA-1.c: Ditto.\n\t* gcc.target/i386/3dnowA-2.c: Ditto.\n\nFrom-SVN: r133169", "tree": {"sha": "0ca19fa288a9af69b989a25372e7675c5943fbe1", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/0ca19fa288a9af69b989a25372e7675c5943fbe1"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/1359ef39755c2a648c44e410e974450d91a4bd99", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1359ef39755c2a648c44e410e974450d91a4bd99", "html_url": "https://github.com/Rust-GCC/gccrs/commit/1359ef39755c2a648c44e410e974450d91a4bd99", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1359ef39755c2a648c44e410e974450d91a4bd99/comments", "author": {"login": "ubizjak", "id": 55479990, "node_id": "MDQ6VXNlcjU1NDc5OTkw", "avatar_url": "https://avatars.githubusercontent.com/u/55479990?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ubizjak", "html_url": "https://github.com/ubizjak", "followers_url": "https://api.github.com/users/ubizjak/followers", "following_url": "https://api.github.com/users/ubizjak/following{/other_user}", "gists_url": "https://api.github.com/users/ubizjak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ubizjak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ubizjak/subscriptions", "organizations_url": "https://api.github.com/users/ubizjak/orgs", "repos_url": "https://api.github.com/users/ubizjak/repos", "events_url": "https://api.github.com/users/ubizjak/events{/privacy}", "received_events_url": "https://api.github.com/users/ubizjak/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "5d2edb29da34b654ab66139c12280eae688970bb", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5d2edb29da34b654ab66139c12280eae688970bb", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5d2edb29da34b654ab66139c12280eae688970bb"}], "stats": {"total": 1782, "additions": 909, "deletions": 873}, "files": [{"sha": "a13353456c34084f991c8b5cb4c998019f381ba1", "filename": "gcc/ChangeLog", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -1,3 +1,19 @@\n+2008-03-13  Uros Bizjak  <ubizjak@gmail.com>\n+\n+\tPR target/34000\n+\tPR target/35553\n+\t* config/i386/xmmintrin.h:  Change all static inline functions to\n+\textern inline and add __gnu_inline__ attribute.\n+\t* config/i386/bmintrin.h: Ditto.\n+\t* config/i386/smmintrin.h: Ditto.\n+\t* config/i386/tmmintrin.h: Ditto.\n+\t* config/i386/mmintrin-common.h: Ditto.\n+\t* config/i386/ammintrin.h: Ditto.\n+\t* config/i386/emmintrin.h: Ditto.\n+\t* config/i386/pmmintrin.h: Ditto.\n+\t* config/i386/mmintrin.h: Ditto.\n+\t* config/i386/mm3dnow.h: Ditto.\n+\n 2008-03-13  Jakub Jelinek  <jakub@redhat.com>\n \n \tPR middle-end/35185"}, {"sha": "8866e37ad3893a2b9e16f3104684b4de384c4adc", "filename": "gcc/config/i386/ammintrin.h", "status": "modified", "additions": 6, "deletions": 6, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fammintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fammintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fammintrin.h?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -37,26 +37,26 @@\n /* We need definitions from the SSE3, SSE2 and SSE header files*/\n #include <pmmintrin.h>\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_stream_sd (double * __P, __m128d __Y)\n {\n   __builtin_ia32_movntsd (__P, (__v2df) __Y);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_stream_ss (float * __P, __m128 __Y)\n {\n   __builtin_ia32_movntss (__P, (__v4sf) __Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_extract_si64 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_extrq ((__v2di) __X, (__v16qi) __Y);\n }\n \n #ifdef __OPTIMIZE__\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_extracti_si64 (__m128i __X, unsigned const int __I, unsigned const int __L)\n {\n   return (__m128i) __builtin_ia32_extrqi ((__v2di) __X, __I, __L);\n@@ -67,14 +67,14 @@ _mm_extracti_si64 (__m128i __X, unsigned const int __I, unsigned const int __L)\n \t\t\t\t    (unsigned int)(I), (unsigned int)(L)))\n #endif\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_insert_si64 (__m128i __X,__m128i __Y)\n {\n   return (__m128i) __builtin_ia32_insertq ((__v2di)__X, (__v2di)__Y);\n }\n \n #ifdef __OPTIMIZE__\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_inserti_si64(__m128i __X, __m128i __Y, unsigned const int __I, unsigned const int __L)\n {\n   return (__m128i) __builtin_ia32_insertqi ((__v2di)__X, (__v2di)__Y, __I, __L);"}, {"sha": "d7c69e576a9e4622ab8108bcbcc0ebcfd09ab5c4", "filename": "gcc/config/i386/bmmintrin.h", "status": "modified", "additions": 195, "deletions": 195, "changes": 390, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fbmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fbmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fbmmintrin.h?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -36,286 +36,286 @@\n #include <mmintrin-common.h>\n \n /* Floating point multiply/add type instructions */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_macc_ps(__m128 __A, __m128 __B, __m128 __C)\n {\n   return (__m128) __builtin_ia32_fmaddps ((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_macc_pd(__m128d __A, __m128d __B, __m128d __C)\n {\n   return (__m128d) __builtin_ia32_fmaddpd ((__v2df)__A, (__v2df)__B, (__v2df)__C);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_macc_ss(__m128 __A, __m128 __B, __m128 __C)\n {\n   return  (__m128) __builtin_ia32_fmaddss ((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_macc_sd(__m128d __A, __m128d __B, __m128d __C)\n {\n   return (__m128d) __builtin_ia32_fmaddsd ((__v2df)__A, (__v2df)__B, (__v2df)__C);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_msub_ps(__m128 __A, __m128 __B, __m128 __C)\n {\n   return (__m128) __builtin_ia32_fmsubps ((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_msub_pd(__m128d __A, __m128d __B, __m128d __C)\n {\n   return (__m128d) __builtin_ia32_fmsubpd ((__v2df)__A, (__v2df)__B, (__v2df)__C);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_msub_ss(__m128 __A, __m128 __B, __m128 __C)\n {\n   return (__m128) __builtin_ia32_fmsubss ((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_msub_sd(__m128d __A, __m128d __B, __m128d __C)\n {\n   return (__m128d) __builtin_ia32_fmsubsd ((__v2df)__A, (__v2df)__B, (__v2df)__C);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_nmacc_ps(__m128 __A, __m128 __B, __m128 __C)\n {\n   return (__m128) __builtin_ia32_fnmaddps ((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_nmacc_pd(__m128d __A, __m128d __B, __m128d __C)\n {\n   return (__m128d) __builtin_ia32_fnmaddpd ((__v2df)__A, (__v2df)__B, (__v2df)__C);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_nmacc_ss(__m128 __A, __m128 __B, __m128 __C)\n {\n   return (__m128) __builtin_ia32_fnmaddss ((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_nmacc_sd(__m128d __A, __m128d __B, __m128d __C)\n {\n   return (__m128d) __builtin_ia32_fnmaddsd ((__v2df)__A, (__v2df)__B, (__v2df)__C);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_nmsub_ps(__m128 __A, __m128 __B, __m128 __C)\n {\n   return (__m128) __builtin_ia32_fnmsubps ((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_nmsub_pd(__m128d __A, __m128d __B, __m128d __C)\n {\n   return (__m128d) __builtin_ia32_fnmsubpd ((__v2df)__A, (__v2df)__B, (__v2df)__C);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_nmsub_ss(__m128 __A, __m128 __B, __m128 __C)\n {\n   return (__m128) __builtin_ia32_fnmsubss ((__v4sf)__A, (__v4sf)__B, (__v4sf)__C);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_nmsub_sd(__m128d __A, __m128d __B, __m128d __C)\n {\n   return (__m128d) __builtin_ia32_fnmsubsd ((__v2df)__A, (__v2df)__B, (__v2df)__C);\n }\n \n /* Integer multiply/add intructions. */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maccs_epi16(__m128i __A, __m128i __B, __m128i __C)\n {\n   return (__m128i) __builtin_ia32_pmacssww ((__v8hi)__A,(__v8hi)__B, (__v8hi)__C);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_macc_epi16(__m128i __A, __m128i __B, __m128i __C)\n {\n   return (__m128i) __builtin_ia32_pmacsww ((__v8hi)__A, (__v8hi)__B, (__v8hi)__C);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maccsd_epi16(__m128i __A, __m128i __B, __m128i __C)\n {\n   return  (__m128i) __builtin_ia32_pmacsswd ((__v8hi)__A, (__v8hi)__B, (__v4si)__C);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maccd_epi16(__m128i __A, __m128i __B, __m128i __C)\n {\n   return  (__m128i) __builtin_ia32_pmacswd ((__v8hi)__A, (__v8hi)__B, (__v4si)__C);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maccs_epi32(__m128i __A, __m128i __B, __m128i __C)\n {\n   return  (__m128i) __builtin_ia32_pmacssdd ((__v4si)__A, (__v4si)__B, (__v4si)__C);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_macc_epi32(__m128i __A, __m128i __B, __m128i __C)\n {\n   return  (__m128i) __builtin_ia32_pmacsdd ((__v4si)__A, (__v4si)__B, (__v4si)__C);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maccslo_epi32(__m128i __A, __m128i __B, __m128i __C)\n {\n   return  (__m128i) __builtin_ia32_pmacssdql ((__v4si)__A, (__v4si)__B, (__v2di)__C);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_macclo_epi32(__m128i __A, __m128i __B, __m128i __C)\n {\n   return  (__m128i) __builtin_ia32_pmacsdql ((__v4si)__A, (__v4si)__B, (__v2di)__C);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maccshi_epi32(__m128i __A, __m128i __B, __m128i __C)\n {\n   return  (__m128i) __builtin_ia32_pmacssdqh ((__v4si)__A, (__v4si)__B, (__v2di)__C);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_macchi_epi32(__m128i __A, __m128i __B, __m128i __C)\n {\n   return  (__m128i) __builtin_ia32_pmacsdqh ((__v4si)__A, (__v4si)__B, (__v2di)__C);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maddsd_epi16(__m128i __A, __m128i __B, __m128i __C)\n {\n   return  (__m128i) __builtin_ia32_pmadcsswd ((__v8hi)__A,(__v8hi)__B,(__v4si)__C);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maddd_epi16(__m128i __A, __m128i __B, __m128i __C)\n {\n   return  (__m128i) __builtin_ia32_pmadcswd ((__v8hi)__A,(__v8hi)__B,(__v4si)__C);\n }\n \n /* Packed Integer Horizontal Add and Subtract */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_haddw_epi8(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phaddbw ((__v16qi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_haddd_epi8(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phaddbd ((__v16qi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_haddq_epi8(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phaddbq ((__v16qi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_haddd_epi16(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phaddwd ((__v8hi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_haddq_epi16(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phaddwq ((__v8hi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_haddq_epi32(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phadddq ((__v4si)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_haddw_epu8(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phaddubw ((__v16qi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_haddd_epu8(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phaddubd ((__v16qi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_haddq_epu8(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phaddubq ((__v16qi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_haddd_epu16(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phadduwd ((__v8hi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_haddq_epu16(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phadduwq ((__v8hi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_haddq_epu32(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phaddudq ((__v4si)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hsubw_epi8(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phsubbw ((__v16qi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hsubd_epi16(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phsubwd ((__v8hi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hsubq_epi32(__m128i __A)\n {\n   return  (__m128i) __builtin_ia32_phsubdq ((__v4si)__A);\n }\n \n /* Vector conditional move and permute */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmov_si128(__m128i __A, __m128i __B, __m128i __C)\n {\n   return  (__m128i) __builtin_ia32_pcmov (__A, __B, __C);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_perm_epi8(__m128i __A, __m128i __B, __m128i __C)\n {\n   return  (__m128i) __builtin_ia32_pperm ((__v16qi)__A, (__v16qi)__B, (__v16qi)__C);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_perm_ps(__m128 __A, __m128 __B, __m128i __C)\n {\n   return  (__m128) __builtin_ia32_permps ((__m128)__A, (__m128)__B, (__v16qi)__C);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_perm_pd(__m128d __A, __m128d __B, __m128i __C)\n {\n   return  (__m128d) __builtin_ia32_permpd ((__m128d)__A, (__m128d)__B, (__v16qi)__C);\n@@ -324,25 +324,25 @@ _mm_perm_pd(__m128d __A, __m128d __B, __m128i __C)\n /* Packed Integer Rotates and Shifts */\n \n /* Rotates - Non-Immediate form */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_rot_epi8(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_protb ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_rot_epi16(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_protw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_rot_epi32(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_protd ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_rot_epi64(__m128i __A,  __m128i __B)\n {\n   return (__m128i)  __builtin_ia32_protq ((__v2di)__A, (__v2di)__B);\n@@ -351,25 +351,25 @@ _mm_rot_epi64(__m128i __A,  __m128i __B)\n \n /* Rotates - Immediate form */\n #ifdef __OPTIMIZE__\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_roti_epi8(__m128i __A, const int __B)\n {\n   return  (__m128i) __builtin_ia32_protbi ((__v16qi)__A, __B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_roti_epi16(__m128i __A, const int __B)\n {\n   return  (__m128i) __builtin_ia32_protwi ((__v8hi)__A, __B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_roti_epi32(__m128i __A, const int __B)\n {\n   return  (__m128i) __builtin_ia32_protdi ((__v4si)__A, __B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_roti_epi64(__m128i __A, const int __B)\n {\n   return  (__m128i) __builtin_ia32_protqi ((__v2di)__A, __B);\n@@ -387,50 +387,50 @@ _mm_roti_epi64(__m128i __A, const int __B)\n \n /* pshl */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shl_epi8(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshlb ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shl_epi16(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshlw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shl_epi32(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshld ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shl_epi64(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshlq ((__v2di)__A, (__v2di)__B);\n }\n \n /* psha */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sha_epi8(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshab ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sha_epi16(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshaw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sha_epi32(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshad ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sha_epi64(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshaq ((__v2di)__A, (__v2di)__B);\n@@ -439,395 +439,395 @@ _mm_sha_epi64(__m128i __A,  __m128i __B)\n /* Compare and Predicate Generation */\n \n /* com (floating point, packed single) */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comeq_ps(__m128 __A, __m128 __B)\n {\n   return  (__m128) __builtin_ia32_comeqps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comlt_ps(__m128 __A, __m128 __B)\n {\n   return  (__m128) __builtin_ia32_comltps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comle_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comleps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comunord_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comunordps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comneq_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comuneqps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comnlt_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comunltps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comnle_ps(__m128 __A, __m128 __B)\n {\n   return (__m128)  __builtin_ia32_comunleps ((__v4sf)__A, (__v4sf)__B);\n }\n \n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comord_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comordps ((__v4sf)__A, (__v4sf)__B);\n }\n \n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comueq_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comueqps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comnge_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comungeps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comngt_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comungtps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comfalse_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comfalseps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comoneq_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comneqps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comge_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comgeps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comgt_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comgtps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comtrue_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comtrueps ((__v4sf)__A, (__v4sf)__B);\n }\n \n /* com (floating point, packed double) */\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comeq_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comeqpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comlt_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comltpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comle_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comlepd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comunord_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comunordpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comneq_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comuneqpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comnlt_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comunltpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comnle_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comunlepd ((__v2df)__A, (__v2df)__B);\n }\n \n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comord_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comordpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comueq_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comueqpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comnge_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comungepd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comngt_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comungtpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comfalse_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comfalsepd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comoneq_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comneqpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comge_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comgepd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comgt_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comgtpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comtrue_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comtruepd ((__v2df)__A, (__v2df)__B);\n }\n \n /* com (floating point, scalar single) */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comeq_ss(__m128 __A, __m128 __B)\n {\n   return (__m128)  __builtin_ia32_comeqss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comlt_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comltss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comle_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comless ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comunord_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comunordss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comneq_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comuneqss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comnlt_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comunltss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comnle_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comunless ((__v4sf)__A, (__v4sf)__B);\n }\n \n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comord_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comordss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comueq_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comueqss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comnge_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comungess ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comngt_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comungtss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comfalse_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comfalsess ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comoneq_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comneqss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comge_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comgess ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comgt_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comgtss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comtrue_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comtruess ((__v4sf)__A, (__v4sf)__B);\n }\n \n /* com (floating point, scalar double) */\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comeq_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comeqsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comlt_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comltsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comle_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comlesd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comunord_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comunordsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comneq_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comuneqsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comnlt_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comunltsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comnle_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comunlesd ((__v2df)__A, (__v2df)__B);\n }\n \n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comord_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comordsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comueq_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comueqsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comnge_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comungesd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comngt_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comungtsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comfalse_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comfalsesd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comoneq_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comneqsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comge_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comgesd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comgt_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comgtsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comtrue_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comtruesd ((__v2df)__A, (__v2df)__B);\n@@ -836,424 +836,424 @@ _mm_comtrue_sd(__m128d __A, __m128d __B)\n \n /*pcom (integer, unsinged bytes) */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comlt_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltub ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comle_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomleub ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comgt_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtub ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comge_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgeub ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comeq_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomequb ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comneq_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomnequb ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comfalse_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalseub ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comtrue_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtrueub ((__v16qi)__A, (__v16qi)__B);\n }\n \n /*pcom (integer, unsinged words) */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comlt_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltuw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comle_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomleuw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comgt_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtuw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comge_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgeuw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comeq_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomequw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comneq_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomnequw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comfalse_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalseuw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comtrue_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtrueuw ((__v8hi)__A, (__v8hi)__B);\n }\n \n /*pcom (integer, unsinged double words) */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comlt_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltud ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comle_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomleud ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comgt_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtud ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comge_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgeud ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comeq_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomequd ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comneq_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomnequd ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comfalse_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalseud ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comtrue_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtrueud ((__v4si)__A, (__v4si)__B);\n }\n \n /*pcom (integer, unsinged quad words) */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comlt_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltuq ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comle_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomleuq ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comgt_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtuq ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comge_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgeuq ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comeq_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomequq ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comneq_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomnequq ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comfalse_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalseuq ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comtrue_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtrueuq ((__v2di)__A, (__v2di)__B);\n }\n \n /*pcom (integer, signed bytes) */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comlt_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltb ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comle_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomleb ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comgt_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtb ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comge_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgeb ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comeq_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomeqb ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comneq_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomneqb ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comfalse_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalseb ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comtrue_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtrueb ((__v16qi)__A, (__v16qi)__B);\n }\n \n /*pcom (integer, signed words) */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comlt_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comle_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomlew ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comgt_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comge_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgew ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comeq_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomeqw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comneq_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomneqw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comfalse_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalsew ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comtrue_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtruew ((__v8hi)__A, (__v8hi)__B);\n }\n \n /*pcom (integer, signed double words) */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comlt_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltd ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comle_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomled ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comgt_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtd ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comge_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomged ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comeq_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomeqd ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comneq_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomneqd ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comfalse_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalsed ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comtrue_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtrued ((__v4si)__A, (__v4si)__B);\n }\n \n /*pcom (integer, signed quad words) */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comlt_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltq ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comle_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomleq ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comgt_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtq ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comge_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgeq ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comeq_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomeqq ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comneq_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomneqq ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comfalse_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalseq ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comtrue_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtrueq ((__v2di)__A, (__v2di)__B);\n }\n \n /* FRCZ */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_frcz_ps (__m128 __A)\n {\n   return (__m128) __builtin_ia32_frczps ((__v4sf)__A);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_frcz_pd (__m128d __A)\n {\n   return (__m128d) __builtin_ia32_frczpd ((__v2df)__A);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_frcz_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_frczss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_frcz_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_frczsd ((__v2df)__A, (__v2df)__B);"}, {"sha": "933dcd61e63f2d5908a29466877ae270a2b05759", "filename": "gcc/config/i386/emmintrin.h", "status": "modified", "additions": 224, "deletions": 224, "changes": 448, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Femmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Femmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Femmintrin.h?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -54,379 +54,379 @@ typedef double __m128d __attribute__ ((__vector_size__ (16), __may_alias__));\n  (((fp1) << 1) | (fp0))\n \n /* Create a vector with element 0 as F and the rest zero.  */\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_sd (double __F)\n {\n   return __extension__ (__m128d){ __F, 0.0 };\n }\n \n /* Create a vector with both elements equal to F.  */\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set1_pd (double __F)\n {\n   return __extension__ (__m128d){ __F, __F };\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_pd1 (double __F)\n {\n   return _mm_set1_pd (__F);\n }\n \n /* Create a vector with the lower value X and upper value W.  */\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_pd (double __W, double __X)\n {\n   return __extension__ (__m128d){ __X, __W };\n }\n \n /* Create a vector with the lower value W and upper value X.  */\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setr_pd (double __W, double __X)\n {\n   return __extension__ (__m128d){ __W, __X };\n }\n \n /* Create a vector of zeros.  */\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setzero_pd (void)\n {\n   return __extension__ (__m128d){ 0.0, 0.0 };\n }\n \n /* Sets the low DPFP value of A from the low value of B.  */\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_move_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_movsd ((__v2df)__A, (__v2df)__B);\n }\n \n /* Load two DPFP values from P.  The address must be 16-byte aligned.  */\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_load_pd (double const *__P)\n {\n   return *(__m128d *)__P;\n }\n \n /* Load two DPFP values from P.  The address need not be 16-byte aligned.  */\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_loadu_pd (double const *__P)\n {\n   return __builtin_ia32_loadupd (__P);\n }\n \n /* Create a vector with all two elements equal to *P.  */\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_load1_pd (double const *__P)\n {\n   return _mm_set1_pd (*__P);\n }\n \n /* Create a vector with element 0 as *P and the rest zero.  */\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_load_sd (double const *__P)\n {\n   return _mm_set_sd (*__P);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_load_pd1 (double const *__P)\n {\n   return _mm_load1_pd (__P);\n }\n \n /* Load two DPFP values in reverse order.  The address must be aligned.  */\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_loadr_pd (double const *__P)\n {\n   __m128d __tmp = _mm_load_pd (__P);\n   return __builtin_ia32_shufpd (__tmp, __tmp, _MM_SHUFFLE2 (0,1));\n }\n \n /* Store two DPFP values.  The address must be 16-byte aligned.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_store_pd (double *__P, __m128d __A)\n {\n   *(__m128d *)__P = __A;\n }\n \n /* Store two DPFP values.  The address need not be 16-byte aligned.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_storeu_pd (double *__P, __m128d __A)\n {\n   __builtin_ia32_storeupd (__P, __A);\n }\n \n /* Stores the lower DPFP value.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_store_sd (double *__P, __m128d __A)\n {\n   *__P = __builtin_ia32_vec_ext_v2df (__A, 0);\n }\n \n-static __inline double __attribute__((__always_inline__, __artificial__))\n+extern __inline double __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsd_f64 (__m128d __A)\n {\n   return __builtin_ia32_vec_ext_v2df (__A, 0);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_storel_pd (double *__P, __m128d __A)\n {\n   _mm_store_sd (__P, __A);\n }\n \n /* Stores the upper DPFP value.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_storeh_pd (double *__P, __m128d __A)\n {\n   *__P = __builtin_ia32_vec_ext_v2df (__A, 1);\n }\n \n /* Store the lower DPFP value across two words.\n    The address must be 16-byte aligned.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_store1_pd (double *__P, __m128d __A)\n {\n   _mm_store_pd (__P, __builtin_ia32_shufpd (__A, __A, _MM_SHUFFLE2 (0,0)));\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_store_pd1 (double *__P, __m128d __A)\n {\n   _mm_store1_pd (__P, __A);\n }\n \n /* Store two DPFP values in reverse order.  The address must be aligned.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_storer_pd (double *__P, __m128d __A)\n {\n   _mm_store_pd (__P, __builtin_ia32_shufpd (__A, __A, _MM_SHUFFLE2 (0,1)));\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi128_si32 (__m128i __A)\n {\n   return __builtin_ia32_vec_ext_v4si ((__v4si)__A, 0);\n }\n \n #ifdef __x86_64__\n /* Intel intrinsic.  */\n-static __inline long long __attribute__((__always_inline__, __artificial__))\n+extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi128_si64 (__m128i __A)\n {\n   return __builtin_ia32_vec_ext_v2di ((__v2di)__A, 0);\n }\n \n /* Microsoft intrinsic.  */\n-static __inline long long __attribute__((__always_inline__, __artificial__))\n+extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi128_si64x (__m128i __A)\n {\n   return __builtin_ia32_vec_ext_v2di ((__v2di)__A, 0);\n }\n #endif\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_add_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_addpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_add_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_addsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sub_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_subpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sub_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_subsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mul_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_mulpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mul_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_mulsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_div_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_divpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_div_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_divsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sqrt_pd (__m128d __A)\n {\n   return (__m128d)__builtin_ia32_sqrtpd ((__v2df)__A);\n }\n \n /* Return pair {sqrt (A[0), B[1]}.  */\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sqrt_sd (__m128d __A, __m128d __B)\n {\n   __v2df __tmp = __builtin_ia32_movsd ((__v2df)__A, (__v2df)__B);\n   return (__m128d)__builtin_ia32_sqrtsd ((__v2df)__tmp);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_minpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_minsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_maxpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_maxsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_and_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_andpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_andnot_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_andnpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_or_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_orpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_xor_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_xorpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpeq_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpeqpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmplt_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpltpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmple_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmplepd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpgt_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpgtpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpge_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpgepd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpneq_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpneqpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnlt_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpnltpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnle_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpnlepd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpngt_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpngtpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnge_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpngepd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpord_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpordpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpunord_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpunordpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpeq_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpeqsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmplt_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpltsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmple_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmplesd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpgt_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_movsd ((__v2df) __A,\n@@ -436,7 +436,7 @@ _mm_cmpgt_sd (__m128d __A, __m128d __B)\n \t\t\t\t\t\t\t\t __A));\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpge_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_movsd ((__v2df) __A,\n@@ -446,25 +446,25 @@ _mm_cmpge_sd (__m128d __A, __m128d __B)\n \t\t\t\t\t\t\t\t __A));\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpneq_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpneqsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnlt_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpnltsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnle_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpnlesd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpngt_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_movsd ((__v2df) __A,\n@@ -474,7 +474,7 @@ _mm_cmpngt_sd (__m128d __A, __m128d __B)\n \t\t\t\t\t\t\t\t  __A));\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnge_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_movsd ((__v2df) __A,\n@@ -484,119 +484,119 @@ _mm_cmpnge_sd (__m128d __A, __m128d __B)\n \t\t\t\t\t\t\t\t  __A));\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpord_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpordsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpunord_sd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_cmpunordsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comieq_sd (__m128d __A, __m128d __B)\n {\n   return __builtin_ia32_comisdeq ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comilt_sd (__m128d __A, __m128d __B)\n {\n   return __builtin_ia32_comisdlt ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comile_sd (__m128d __A, __m128d __B)\n {\n   return __builtin_ia32_comisdle ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comigt_sd (__m128d __A, __m128d __B)\n {\n   return __builtin_ia32_comisdgt ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comige_sd (__m128d __A, __m128d __B)\n {\n   return __builtin_ia32_comisdge ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comineq_sd (__m128d __A, __m128d __B)\n {\n   return __builtin_ia32_comisdneq ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomieq_sd (__m128d __A, __m128d __B)\n {\n   return __builtin_ia32_ucomisdeq ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomilt_sd (__m128d __A, __m128d __B)\n {\n   return __builtin_ia32_ucomisdlt ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomile_sd (__m128d __A, __m128d __B)\n {\n   return __builtin_ia32_ucomisdle ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomigt_sd (__m128d __A, __m128d __B)\n {\n   return __builtin_ia32_ucomisdgt ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomige_sd (__m128d __A, __m128d __B)\n {\n   return __builtin_ia32_ucomisdge ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomineq_sd (__m128d __A, __m128d __B)\n {\n   return __builtin_ia32_ucomisdneq ((__v2df)__A, (__v2df)__B);\n }\n \n /* Create a vector of Qi, where i is the element number.  */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_epi64x (long long __q1, long long __q0)\n {\n   return __extension__ (__m128i)(__v2di){ __q0, __q1 };\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_epi64 (__m64 __q1,  __m64 __q0)\n {\n   return _mm_set_epi64x ((long long)__q1, (long long)__q0);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_epi32 (int __q3, int __q2, int __q1, int __q0)\n {\n   return __extension__ (__m128i)(__v4si){ __q0, __q1, __q2, __q3 };\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_epi16 (short __q7, short __q6, short __q5, short __q4,\n \t       short __q3, short __q2, short __q1, short __q0)\n {\n   return __extension__ (__m128i)(__v8hi){\n     __q0, __q1, __q2, __q3, __q4, __q5, __q6, __q7 };\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_epi8 (char __q15, char __q14, char __q13, char __q12,\n \t      char __q11, char __q10, char __q09, char __q08,\n \t      char __q07, char __q06, char __q05, char __q04,\n@@ -610,31 +610,31 @@ _mm_set_epi8 (char __q15, char __q14, char __q13, char __q12,\n \n /* Set all of the elements of the vector to A.  */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set1_epi64x (long long __A)\n {\n   return _mm_set_epi64x (__A, __A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set1_epi64 (__m64 __A)\n {\n   return _mm_set_epi64 (__A, __A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set1_epi32 (int __A)\n {\n   return _mm_set_epi32 (__A, __A, __A, __A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set1_epi16 (short __A)\n {\n   return _mm_set_epi16 (__A, __A, __A, __A, __A, __A, __A, __A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set1_epi8 (char __A)\n {\n   return _mm_set_epi8 (__A, __A, __A, __A, __A, __A, __A, __A,\n@@ -644,26 +644,26 @@ _mm_set1_epi8 (char __A)\n /* Create a vector of Qi, where i is the element number.\n    The parameter order is reversed from the _mm_set_epi* functions.  */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setr_epi64 (__m64 __q0, __m64 __q1)\n {\n   return _mm_set_epi64 (__q1, __q0);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setr_epi32 (int __q0, int __q1, int __q2, int __q3)\n {\n   return _mm_set_epi32 (__q3, __q2, __q1, __q0);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setr_epi16 (short __q0, short __q1, short __q2, short __q3,\n \t        short __q4, short __q5, short __q6, short __q7)\n {\n   return _mm_set_epi16 (__q7, __q6, __q5, __q4, __q3, __q2, __q1, __q0);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setr_epi8 (char __q00, char __q01, char __q02, char __q03,\n \t       char __q04, char __q05, char __q06, char __q07,\n \t       char __q08, char __q09, char __q10, char __q11,\n@@ -675,213 +675,213 @@ _mm_setr_epi8 (char __q00, char __q01, char __q02, char __q03,\n \n /* Create a vector with element 0 as *P and the rest zero.  */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_load_si128 (__m128i const *__P)\n {\n   return *__P;\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_loadu_si128 (__m128i const *__P)\n {\n   return (__m128i) __builtin_ia32_loaddqu ((char const *)__P);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_loadl_epi64 (__m128i const *__P)\n {\n   return _mm_set_epi64 ((__m64)0LL, *(__m64 *)__P);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_store_si128 (__m128i *__P, __m128i __B)\n {\n   *__P = __B;\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_storeu_si128 (__m128i *__P, __m128i __B)\n {\n   __builtin_ia32_storedqu ((char *)__P, (__v16qi)__B);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_storel_epi64 (__m128i *__P, __m128i __B)\n {\n   *(long long *)__P = __builtin_ia32_vec_ext_v2di ((__v2di)__B, 0);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_movepi64_pi64 (__m128i __B)\n {\n   return (__m64) __builtin_ia32_vec_ext_v2di ((__v2di)__B, 0);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_movpi64_epi64 (__m64 __A)\n {\n   return _mm_set_epi64 ((__m64)0LL, __A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_move_epi64 (__m128i __A)\n {\n   return _mm_set_epi64 ((__m64)0LL, _mm_movepi64_pi64 (__A));\n }\n \n /* Create a vector of zeros.  */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setzero_si128 (void)\n {\n   return __extension__ (__m128i)(__v4si){ 0, 0, 0, 0 };\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepi32_pd (__m128i __A)\n {\n   return (__m128d)__builtin_ia32_cvtdq2pd ((__v4si) __A);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepi32_ps (__m128i __A)\n {\n   return (__m128)__builtin_ia32_cvtdq2ps ((__v4si) __A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpd_epi32 (__m128d __A)\n {\n   return (__m128i)__builtin_ia32_cvtpd2dq ((__v2df) __A);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpd_pi32 (__m128d __A)\n {\n   return (__m64)__builtin_ia32_cvtpd2pi ((__v2df) __A);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpd_ps (__m128d __A)\n {\n   return (__m128)__builtin_ia32_cvtpd2ps ((__v2df) __A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttpd_epi32 (__m128d __A)\n {\n   return (__m128i)__builtin_ia32_cvttpd2dq ((__v2df) __A);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttpd_pi32 (__m128d __A)\n {\n   return (__m64)__builtin_ia32_cvttpd2pi ((__v2df) __A);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpi32_pd (__m64 __A)\n {\n   return (__m128d)__builtin_ia32_cvtpi2pd ((__v2si) __A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtps_epi32 (__m128 __A)\n {\n   return (__m128i)__builtin_ia32_cvtps2dq ((__v4sf) __A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttps_epi32 (__m128 __A)\n {\n   return (__m128i)__builtin_ia32_cvttps2dq ((__v4sf) __A);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtps_pd (__m128 __A)\n {\n   return (__m128d)__builtin_ia32_cvtps2pd ((__v4sf) __A);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsd_si32 (__m128d __A)\n {\n   return __builtin_ia32_cvtsd2si ((__v2df) __A);\n }\n \n #ifdef __x86_64__\n /* Intel intrinsic.  */\n-static __inline long long __attribute__((__always_inline__, __artificial__))\n+extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsd_si64 (__m128d __A)\n {\n   return __builtin_ia32_cvtsd2si64 ((__v2df) __A);\n }\n \n /* Microsoft intrinsic.  */\n-static __inline long long __attribute__((__always_inline__, __artificial__))\n+extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsd_si64x (__m128d __A)\n {\n   return __builtin_ia32_cvtsd2si64 ((__v2df) __A);\n }\n #endif\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttsd_si32 (__m128d __A)\n {\n   return __builtin_ia32_cvttsd2si ((__v2df) __A);\n }\n \n #ifdef __x86_64__\n /* Intel intrinsic.  */\n-static __inline long long __attribute__((__always_inline__, __artificial__))\n+extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttsd_si64 (__m128d __A)\n {\n   return __builtin_ia32_cvttsd2si64 ((__v2df) __A);\n }\n \n /* Microsoft intrinsic.  */\n-static __inline long long __attribute__((__always_inline__, __artificial__))\n+extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttsd_si64x (__m128d __A)\n {\n   return __builtin_ia32_cvttsd2si64 ((__v2df) __A);\n }\n #endif\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsd_ss (__m128 __A, __m128d __B)\n {\n   return (__m128)__builtin_ia32_cvtsd2ss ((__v4sf) __A, (__v2df) __B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi32_sd (__m128d __A, int __B)\n {\n   return (__m128d)__builtin_ia32_cvtsi2sd ((__v2df) __A, __B);\n }\n \n #ifdef __x86_64__\n /* Intel intrinsic.  */\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi64_sd (__m128d __A, long long __B)\n {\n   return (__m128d)__builtin_ia32_cvtsi642sd ((__v2df) __A, __B);\n }\n \n /* Microsoft intrinsic.  */\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi64x_sd (__m128d __A, long long __B)\n {\n   return (__m128d)__builtin_ia32_cvtsi642sd ((__v2df) __A, __B);\n }\n #endif\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtss_sd (__m128d __A, __m128 __B)\n {\n   return (__m128d)__builtin_ia32_cvtss2sd ((__v2df) __A, (__v4sf)__B);\n }\n \n #ifdef __OPTIMIZE__\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shuffle_pd(__m128d __A, __m128d __B, const int __mask)\n {\n   return (__m128d)__builtin_ia32_shufpd ((__v2df)__A, (__v2df)__B, __mask);\n@@ -892,266 +892,266 @@ _mm_shuffle_pd(__m128d __A, __m128d __B, const int __mask)\n \t\t\t\t   (__v2df)(__m128d)(B), (int)(N)))\n #endif\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpackhi_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_unpckhpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpacklo_pd (__m128d __A, __m128d __B)\n {\n   return (__m128d)__builtin_ia32_unpcklpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_loadh_pd (__m128d __A, double const *__B)\n {\n   return (__m128d)__builtin_ia32_loadhpd ((__v2df)__A, __B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_loadl_pd (__m128d __A, double const *__B)\n {\n   return (__m128d)__builtin_ia32_loadlpd ((__v2df)__A, __B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_movemask_pd (__m128d __A)\n {\n   return __builtin_ia32_movmskpd ((__v2df)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_packs_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_packsswb128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_packs_epi32 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_packssdw128 ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_packus_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_packuswb128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpackhi_epi8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_punpckhbw128 ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpackhi_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_punpckhwd128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpackhi_epi32 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_punpckhdq128 ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpackhi_epi64 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_punpckhqdq128 ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpacklo_epi8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_punpcklbw128 ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpacklo_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_punpcklwd128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpacklo_epi32 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_punpckldq128 ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpacklo_epi64 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_punpcklqdq128 ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_add_epi8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_paddb128 ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_add_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_paddw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_add_epi32 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_paddd128 ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_add_epi64 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_paddq128 ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_adds_epi8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_paddsb128 ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_adds_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_paddsw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_adds_epu8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_paddusb128 ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_adds_epu16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_paddusw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sub_epi8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psubb128 ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sub_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psubw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sub_epi32 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psubd128 ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sub_epi64 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psubq128 ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_subs_epi8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psubsb128 ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_subs_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psubsw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_subs_epu8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psubusb128 ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_subs_epu16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psubusw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_madd_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pmaddwd128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mulhi_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pmulhw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mullo_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pmullw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mul_su32 (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pmuludq ((__v2si)__A, (__v2si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mul_epu32 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pmuludq128 ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_slli_epi16 (__m128i __A, int __B)\n {\n   return (__m128i)__builtin_ia32_psllwi128 ((__v8hi)__A, __B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_slli_epi32 (__m128i __A, int __B)\n {\n   return (__m128i)__builtin_ia32_pslldi128 ((__v4si)__A, __B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_slli_epi64 (__m128i __A, int __B)\n {\n   return (__m128i)__builtin_ia32_psllqi128 ((__v2di)__A, __B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srai_epi16 (__m128i __A, int __B)\n {\n   return (__m128i)__builtin_ia32_psrawi128 ((__v8hi)__A, __B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srai_epi32 (__m128i __A, int __B)\n {\n   return (__m128i)__builtin_ia32_psradi128 ((__v4si)__A, __B);\n }\n \n #ifdef __OPTIMIZE__\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srli_si128 (__m128i __A, const int __N)\n {\n   return (__m128i)__builtin_ia32_psrldqi128 (__A, __N * 8);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_slli_si128 (__m128i __A, const int __N)\n {\n   return (__m128i)__builtin_ia32_pslldqi128 (__A, __N * 8);\n@@ -1163,158 +1163,158 @@ _mm_slli_si128 (__m128i __A, const int __N)\n   ((__m128i)__builtin_ia32_pslldqi128 ((__m128i)(A), (int)(N) * 8))\n #endif\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srli_epi16 (__m128i __A, int __B)\n {\n   return (__m128i)__builtin_ia32_psrlwi128 ((__v8hi)__A, __B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srli_epi32 (__m128i __A, int __B)\n {\n   return (__m128i)__builtin_ia32_psrldi128 ((__v4si)__A, __B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srli_epi64 (__m128i __A, int __B)\n {\n   return (__m128i)__builtin_ia32_psrlqi128 ((__v2di)__A, __B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sll_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psllw128((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sll_epi32 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pslld128((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sll_epi64 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psllq128((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sra_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psraw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sra_epi32 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psrad128 ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srl_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psrlw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srl_epi32 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psrld128 ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srl_epi64 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psrlq128 ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_and_si128 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pand128 ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_andnot_si128 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pandn128 ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_or_si128 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_por128 ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_xor_si128 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pxor128 ((__v2di)__A, (__v2di)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpeq_epi8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pcmpeqb128 ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpeq_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pcmpeqw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpeq_epi32 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pcmpeqd128 ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmplt_epi8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pcmpgtb128 ((__v16qi)__B, (__v16qi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmplt_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pcmpgtw128 ((__v8hi)__B, (__v8hi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmplt_epi32 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pcmpgtd128 ((__v4si)__B, (__v4si)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpgt_epi8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pcmpgtb128 ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpgt_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pcmpgtw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpgt_epi32 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pcmpgtd128 ((__v4si)__A, (__v4si)__B);\n }\n \n #ifdef __OPTIMIZE__\n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_extract_epi16 (__m128i const __A, int const __N)\n {\n   return __builtin_ia32_vec_ext_v8hi ((__v8hi)__A, __N);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_insert_epi16 (__m128i const __A, int const __D, int const __N)\n {\n   return (__m128i) __builtin_ia32_vec_set_v8hi ((__v8hi)__A, __D, __N);\n@@ -1327,56 +1327,56 @@ _mm_insert_epi16 (__m128i const __A, int const __D, int const __N)\n \t\t\t\t\t  (int)(D), (int)(N)))\n #endif\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pmaxsw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_epu8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pmaxub128 ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_epi16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pminsw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_epu8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pminub128 ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_movemask_epi8 (__m128i __A)\n {\n   return __builtin_ia32_pmovmskb128 ((__v16qi)__A);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mulhi_epu16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pmulhuw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n #ifdef __OPTIMIZE__\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shufflehi_epi16 (__m128i __A, const int __mask)\n {\n   return (__m128i)__builtin_ia32_pshufhw ((__v8hi)__A, __mask);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shufflelo_epi16 (__m128i __A, const int __mask)\n {\n   return (__m128i)__builtin_ia32_pshuflw ((__v8hi)__A, __mask);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shuffle_epi32 (__m128i __A, const int __mask)\n {\n   return (__m128i)__builtin_ia32_pshufd ((__v4si)__A, __mask);\n@@ -1390,82 +1390,82 @@ _mm_shuffle_epi32 (__m128i __A, const int __mask)\n   ((__m128i)__builtin_ia32_pshufd ((__v4si)(__m128i)(A), (int)(N)))\n #endif\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskmoveu_si128 (__m128i __A, __m128i __B, char *__C)\n {\n   __builtin_ia32_maskmovdqu ((__v16qi)__A, (__v16qi)__B, __C);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_avg_epu8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pavgb128 ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_avg_epu16 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_pavgw128 ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sad_epu8 (__m128i __A, __m128i __B)\n {\n   return (__m128i)__builtin_ia32_psadbw128 ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_stream_si32 (int *__A, int __B)\n {\n   __builtin_ia32_movnti (__A, __B);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_stream_si128 (__m128i *__A, __m128i __B)\n {\n   __builtin_ia32_movntdq ((__v2di *)__A, (__v2di)__B);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_stream_pd (double *__A, __m128d __B)\n {\n   __builtin_ia32_movntpd (__A, (__v2df)__B);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_clflush (void const *__A)\n {\n   __builtin_ia32_clflush (__A);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_lfence (void)\n {\n   __builtin_ia32_lfence ();\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mfence (void)\n {\n   __builtin_ia32_mfence ();\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi32_si128 (int __A)\n {\n   return _mm_set_epi32 (0, 0, 0, __A);\n }\n \n #ifdef __x86_64__\n /* Intel intrinsic.  */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi64_si128 (long long __A)\n {\n   return _mm_set_epi64x (0, __A);\n }\n \n /* Microsoft intrinsic.  */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi64x_si128 (long long __A)\n {\n   return _mm_set_epi64x (0, __A);\n@@ -1474,37 +1474,37 @@ _mm_cvtsi64x_si128 (long long __A)\n \n /* Casts between various SP, DP, INT vector types.  Note that these do no\n    conversion of values, they just change the type.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_castpd_ps(__m128d __A)\n {\n   return (__m128) __A;\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_castpd_si128(__m128d __A)\n {\n   return (__m128i) __A;\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_castps_pd(__m128 __A)\n {\n   return (__m128d) __A;\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_castps_si128(__m128 __A)\n {\n   return (__m128i) __A;\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_castsi128_ps(__m128i __A)\n {\n   return (__m128) __A;\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_castsi128_pd(__m128i __A)\n {\n   return (__m128d) __A;"}, {"sha": "dd1c871936efd7e70580e50a1790cde1118f4984", "filename": "gcc/config/i386/mm3dnow.h", "status": "modified", "additions": 29, "deletions": 29, "changes": 58, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fmm3dnow.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fmm3dnow.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fmm3dnow.h?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -37,145 +37,145 @@\n /* Internal data types for implementing the intrinsics.  */\n typedef float __v2sf __attribute__ ((__vector_size__ (8)));\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_femms (void)\n {\n   __builtin_ia32_femms();\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pavgusb (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pavgusb ((__v8qi)__A, (__v8qi)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pf2id (__m64 __A)\n {\n   return (__m64)__builtin_ia32_pf2id ((__v2sf)__A);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfacc (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfacc ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfadd (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfadd ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfcmpeq (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfcmpeq ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfcmpge (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfcmpge ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfcmpgt (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfcmpgt ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfmax (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfmax ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfmin (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfmin ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfmul (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfmul ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfrcp (__m64 __A)\n {\n   return (__m64)__builtin_ia32_pfrcp ((__v2sf)__A);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfrcpit1 (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfrcpit1 ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfrcpit2 (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfrcpit2 ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfrsqrt (__m64 __A)\n {\n   return (__m64)__builtin_ia32_pfrsqrt ((__v2sf)__A);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfrsqit1 (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfrsqit1 ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfsub (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfsub ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfsubr (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfsubr ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pi2fd (__m64 __A)\n {\n   return (__m64)__builtin_ia32_pi2fd ((__v2si)__A);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pmulhrw (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pmulhrw ((__v4hi)__A, (__v4hi)__B);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_prefetch (void *__P)\n {\n   __builtin_prefetch (__P, 0, 3 /* _MM_HINT_T0 */);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_prefetchw (void *__P)\n {\n   __builtin_prefetch (__P, 1, 3 /* _MM_HINT_T0 */);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_from_float (float __A)\n {\n   return __extension__ (__m64)(__v2sf){ __A, 0.0f };\n }\n \n-static __inline float __attribute__((__always_inline__, __artificial__))\n+extern __inline float __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_to_float (__m64 __A)\n {\n   union { __v2sf v; float a[2]; } __tmp;\n@@ -185,31 +185,31 @@ _m_to_float (__m64 __A)\n \n #ifdef __3dNOW_A__\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pf2iw (__m64 __A)\n {\n   return (__m64)__builtin_ia32_pf2iw ((__v2sf)__A);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfnacc (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfnacc ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pfpnacc (__m64 __A, __m64 __B)\n {\n   return (__m64)__builtin_ia32_pfpnacc ((__v2sf)__A, (__v2sf)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pi2fw (__m64 __A)\n {\n   return (__m64)__builtin_ia32_pi2fw ((__v2si)__A);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pswapd (__m64 __A)\n {\n   return (__m64)__builtin_ia32_pswapdsf ((__v2sf)__A);"}, {"sha": "b906d5c07d2ce2a49d56172cb61d6286577ad087", "filename": "gcc/config/i386/mmintrin-common.h", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fmmintrin-common.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fmmintrin-common.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fmmintrin-common.h?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -60,23 +60,23 @@\n /* Test Instruction */\n /* Packed integer 128-bit bitwise comparison. Return 1 if\n    (__V & __M) == 0.  */\n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_testz_si128 (__m128i __M, __m128i __V)\n {\n   return __builtin_ia32_ptestz128 ((__v2di)__M, (__v2di)__V);\n }\n \n /* Packed integer 128-bit bitwise comparison. Return 1 if\n    (__V & ~__M) == 0.  */\n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_testc_si128 (__m128i __M, __m128i __V)\n {\n   return __builtin_ia32_ptestc128 ((__v2di)__M, (__v2di)__V);\n }\n \n /* Packed integer 128-bit bitwise comparison. Return 1 if\n    (__V & __M) != 0 && (__V & ~__M) != 0.  */\n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_testnzc_si128 (__m128i __M, __m128i __V)\n {\n   return __builtin_ia32_ptestnzc128 ((__v2di)__M, (__v2di)__V);\n@@ -93,13 +93,13 @@ _mm_testnzc_si128 (__m128i __M, __m128i __V)\n /* Packed/scalar double precision floating point rounding.  */\n \n #ifdef __OPTIMIZE__\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_round_pd (__m128d __V, const int __M)\n {\n   return (__m128d) __builtin_ia32_roundpd ((__v2df)__V, __M);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_round_sd(__m128d __D, __m128d __V, const int __M)\n {\n   return (__m128d) __builtin_ia32_roundsd ((__v2df)__D,\n@@ -118,13 +118,13 @@ _mm_round_sd(__m128d __D, __m128d __V, const int __M)\n /* Packed/scalar single precision floating point rounding.  */\n \n #ifdef __OPTIMIZE__\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_round_ps (__m128 __V, const int __M)\n {\n   return (__m128) __builtin_ia32_roundps ((__v4sf)__V, __M);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_round_ss (__m128 __D, __m128 __V, const int __M)\n {\n   return (__m128) __builtin_ia32_roundss ((__v4sf)__D,"}, {"sha": "1c09be30e1ae2c2150a4054020afc184d20a9b6d", "filename": "gcc/config/i386/mmintrin.h", "status": "modified", "additions": 129, "deletions": 129, "changes": 258, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fmmintrin.h?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -45,26 +45,26 @@ typedef char __v8qi __attribute__ ((__vector_size__ (8)));\n typedef long long __v1di __attribute__ ((__vector_size__ (8)));\n \n /* Empty the multimedia state.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_empty (void)\n {\n   __builtin_ia32_emms ();\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_empty (void)\n {\n   _mm_empty ();\n }\n \n /* Convert I to a __m64 object.  The integer is zero-extended to 64-bits.  */\n-static __inline __m64  __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi32_si64 (int __i)\n {\n   return (__m64) __builtin_ia32_vec_init_v2si (__i, 0);\n }\n \n-static __inline __m64  __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_from_int (int __i)\n {\n   return _mm_cvtsi32_si64 (__i);\n@@ -74,40 +74,40 @@ _m_from_int (int __i)\n /* Convert I to a __m64 object.  */\n \n /* Intel intrinsic.  */\n-static __inline __m64  __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_from_int64 (long long __i)\n {\n   return (__m64) __i;\n }\n \n-static __inline __m64  __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi64_m64 (long long __i)\n {\n   return (__m64) __i;\n }\n \n /* Microsoft intrinsic.  */\n-static __inline __m64  __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi64x_si64 (long long __i)\n {\n   return (__m64) __i;\n }\n \n-static __inline __m64  __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_pi64x (long long __i)\n {\n   return (__m64) __i;\n }\n #endif\n \n /* Convert the lower 32 bits of the __m64 object into an integer.  */\n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi64_si32 (__m64 __i)\n {\n   return __builtin_ia32_vec_ext_v2si ((__v2si)__i, 0);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_to_int (__m64 __i)\n {\n   return _mm_cvtsi64_si32 (__i);\n@@ -117,20 +117,20 @@ _m_to_int (__m64 __i)\n /* Convert the __m64 object to a 64bit integer.  */\n \n /* Intel intrinsic.  */\n-static __inline long long __attribute__((__always_inline__, __artificial__))\n+extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_to_int64 (__m64 __i)\n {\n   return (long long)__i;\n }\n \n-static __inline long long __attribute__((__always_inline__, __artificial__))\n+extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtm64_si64 (__m64 __i)\n {\n   return (long long)__i;\n }\n \n /* Microsoft intrinsic.  */\n-static __inline long long __attribute__((__always_inline__, __artificial__))\n+extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi64_si64x (__m64 __i)\n {\n   return (long long)__i;\n@@ -140,13 +140,13 @@ _mm_cvtsi64_si64x (__m64 __i)\n /* Pack the four 16-bit values from M1 into the lower four 8-bit values of\n    the result, and the four 16-bit values from M2 into the upper four 8-bit\n    values of the result, all with signed saturation.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_packs_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_packsswb ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_packsswb (__m64 __m1, __m64 __m2)\n {\n   return _mm_packs_pi16 (__m1, __m2);\n@@ -155,13 +155,13 @@ _m_packsswb (__m64 __m1, __m64 __m2)\n /* Pack the two 32-bit values from M1 in to the lower two 16-bit values of\n    the result, and the two 32-bit values from M2 into the upper two 16-bit\n    values of the result, all with signed saturation.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_packs_pi32 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_packssdw ((__v2si)__m1, (__v2si)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_packssdw (__m64 __m1, __m64 __m2)\n {\n   return _mm_packs_pi32 (__m1, __m2);\n@@ -170,144 +170,144 @@ _m_packssdw (__m64 __m1, __m64 __m2)\n /* Pack the four 16-bit values from M1 into the lower four 8-bit values of\n    the result, and the four 16-bit values from M2 into the upper four 8-bit\n    values of the result, all with unsigned saturation.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_packs_pu16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_packuswb ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_packuswb (__m64 __m1, __m64 __m2)\n {\n   return _mm_packs_pu16 (__m1, __m2);\n }\n \n /* Interleave the four 8-bit values from the high half of M1 with the four\n    8-bit values from the high half of M2.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpackhi_pi8 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_punpckhbw ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_punpckhbw (__m64 __m1, __m64 __m2)\n {\n   return _mm_unpackhi_pi8 (__m1, __m2);\n }\n \n /* Interleave the two 16-bit values from the high half of M1 with the two\n    16-bit values from the high half of M2.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpackhi_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_punpckhwd ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_punpckhwd (__m64 __m1, __m64 __m2)\n {\n   return _mm_unpackhi_pi16 (__m1, __m2);\n }\n \n /* Interleave the 32-bit value from the high half of M1 with the 32-bit\n    value from the high half of M2.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpackhi_pi32 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_punpckhdq ((__v2si)__m1, (__v2si)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_punpckhdq (__m64 __m1, __m64 __m2)\n {\n   return _mm_unpackhi_pi32 (__m1, __m2);\n }\n \n /* Interleave the four 8-bit values from the low half of M1 with the four\n    8-bit values from the low half of M2.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpacklo_pi8 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_punpcklbw ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_punpcklbw (__m64 __m1, __m64 __m2)\n {\n   return _mm_unpacklo_pi8 (__m1, __m2);\n }\n \n /* Interleave the two 16-bit values from the low half of M1 with the two\n    16-bit values from the low half of M2.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpacklo_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_punpcklwd ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_punpcklwd (__m64 __m1, __m64 __m2)\n {\n   return _mm_unpacklo_pi16 (__m1, __m2);\n }\n \n /* Interleave the 32-bit value from the low half of M1 with the 32-bit\n    value from the low half of M2.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpacklo_pi32 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_punpckldq ((__v2si)__m1, (__v2si)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_punpckldq (__m64 __m1, __m64 __m2)\n {\n   return _mm_unpacklo_pi32 (__m1, __m2);\n }\n \n /* Add the 8-bit values in M1 to the 8-bit values in M2.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_add_pi8 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_paddb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_paddb (__m64 __m1, __m64 __m2)\n {\n   return _mm_add_pi8 (__m1, __m2);\n }\n \n /* Add the 16-bit values in M1 to the 16-bit values in M2.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_add_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_paddw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_paddw (__m64 __m1, __m64 __m2)\n {\n   return _mm_add_pi16 (__m1, __m2);\n }\n \n /* Add the 32-bit values in M1 to the 32-bit values in M2.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_add_pi32 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_paddd ((__v2si)__m1, (__v2si)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_paddd (__m64 __m1, __m64 __m2)\n {\n   return _mm_add_pi32 (__m1, __m2);\n }\n \n /* Add the 64-bit values in M1 to the 64-bit values in M2.  */\n #ifdef __SSE2__\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_add_si64 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_paddq ((__v1di)__m1, (__v1di)__m2);\n@@ -316,102 +316,102 @@ _mm_add_si64 (__m64 __m1, __m64 __m2)\n \n /* Add the 8-bit values in M1 to the 8-bit values in M2 using signed\n    saturated arithmetic.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_adds_pi8 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_paddsb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_paddsb (__m64 __m1, __m64 __m2)\n {\n   return _mm_adds_pi8 (__m1, __m2);\n }\n \n /* Add the 16-bit values in M1 to the 16-bit values in M2 using signed\n    saturated arithmetic.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_adds_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_paddsw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_paddsw (__m64 __m1, __m64 __m2)\n {\n   return _mm_adds_pi16 (__m1, __m2);\n }\n \n /* Add the 8-bit values in M1 to the 8-bit values in M2 using unsigned\n    saturated arithmetic.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_adds_pu8 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_paddusb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_paddusb (__m64 __m1, __m64 __m2)\n {\n   return _mm_adds_pu8 (__m1, __m2);\n }\n \n /* Add the 16-bit values in M1 to the 16-bit values in M2 using unsigned\n    saturated arithmetic.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_adds_pu16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_paddusw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_paddusw (__m64 __m1, __m64 __m2)\n {\n   return _mm_adds_pu16 (__m1, __m2);\n }\n \n /* Subtract the 8-bit values in M2 from the 8-bit values in M1.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sub_pi8 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_psubb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psubb (__m64 __m1, __m64 __m2)\n {\n   return _mm_sub_pi8 (__m1, __m2);\n }\n \n /* Subtract the 16-bit values in M2 from the 16-bit values in M1.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sub_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_psubw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psubw (__m64 __m1, __m64 __m2)\n {\n   return _mm_sub_pi16 (__m1, __m2);\n }\n \n /* Subtract the 32-bit values in M2 from the 32-bit values in M1.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sub_pi32 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_psubd ((__v2si)__m1, (__v2si)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psubd (__m64 __m1, __m64 __m2)\n {\n   return _mm_sub_pi32 (__m1, __m2);\n }\n \n /* Add the 64-bit values in M1 to the 64-bit values in M2.  */\n #ifdef __SSE2__\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sub_si64 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_psubq ((__v1di)__m1, (__v1di)__m2);\n@@ -420,55 +420,55 @@ _mm_sub_si64 (__m64 __m1, __m64 __m2)\n \n /* Subtract the 8-bit values in M2 from the 8-bit values in M1 using signed\n    saturating arithmetic.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_subs_pi8 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_psubsb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psubsb (__m64 __m1, __m64 __m2)\n {\n   return _mm_subs_pi8 (__m1, __m2);\n }\n \n /* Subtract the 16-bit values in M2 from the 16-bit values in M1 using\n    signed saturating arithmetic.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_subs_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_psubsw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psubsw (__m64 __m1, __m64 __m2)\n {\n   return _mm_subs_pi16 (__m1, __m2);\n }\n \n /* Subtract the 8-bit values in M2 from the 8-bit values in M1 using\n    unsigned saturating arithmetic.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_subs_pu8 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_psubusb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psubusb (__m64 __m1, __m64 __m2)\n {\n   return _mm_subs_pu8 (__m1, __m2);\n }\n \n /* Subtract the 16-bit values in M2 from the 16-bit values in M1 using\n    unsigned saturating arithmetic.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_subs_pu16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_psubusw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psubusw (__m64 __m1, __m64 __m2)\n {\n   return _mm_subs_pu16 (__m1, __m2);\n@@ -477,400 +477,400 @@ _m_psubusw (__m64 __m1, __m64 __m2)\n /* Multiply four 16-bit values in M1 by four 16-bit values in M2 producing\n    four 32-bit intermediate results, which are then summed by pairs to\n    produce two 32-bit results.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_madd_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_pmaddwd ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pmaddwd (__m64 __m1, __m64 __m2)\n {\n   return _mm_madd_pi16 (__m1, __m2);\n }\n \n /* Multiply four signed 16-bit values in M1 by four signed 16-bit values in\n    M2 and produce the high 16 bits of the 32-bit results.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mulhi_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_pmulhw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pmulhw (__m64 __m1, __m64 __m2)\n {\n   return _mm_mulhi_pi16 (__m1, __m2);\n }\n \n /* Multiply four 16-bit values in M1 by four 16-bit values in M2 and produce\n    the low 16 bits of the results.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mullo_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_pmullw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pmullw (__m64 __m1, __m64 __m2)\n {\n   return _mm_mullo_pi16 (__m1, __m2);\n }\n \n /* Shift four 16-bit values in M left by COUNT.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sll_pi16 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_psllw ((__v4hi)__m, (__v4hi)__count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psllw (__m64 __m, __m64 __count)\n {\n   return _mm_sll_pi16 (__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_slli_pi16 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_psllwi ((__v4hi)__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psllwi (__m64 __m, int __count)\n {\n   return _mm_slli_pi16 (__m, __count);\n }\n \n /* Shift two 32-bit values in M left by COUNT.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sll_pi32 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_pslld ((__v2si)__m, (__v2si)__count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pslld (__m64 __m, __m64 __count)\n {\n   return _mm_sll_pi32 (__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_slli_pi32 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_pslldi ((__v2si)__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pslldi (__m64 __m, int __count)\n {\n   return _mm_slli_pi32 (__m, __count);\n }\n \n /* Shift the 64-bit value in M left by COUNT.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sll_si64 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_psllq ((__v1di)__m, (__v1di)__count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psllq (__m64 __m, __m64 __count)\n {\n   return _mm_sll_si64 (__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_slli_si64 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_psllqi ((__v1di)__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psllqi (__m64 __m, int __count)\n {\n   return _mm_slli_si64 (__m, __count);\n }\n \n /* Shift four 16-bit values in M right by COUNT; shift in the sign bit.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sra_pi16 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_psraw ((__v4hi)__m, (__v4hi)__count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psraw (__m64 __m, __m64 __count)\n {\n   return _mm_sra_pi16 (__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srai_pi16 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_psrawi ((__v4hi)__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psrawi (__m64 __m, int __count)\n {\n   return _mm_srai_pi16 (__m, __count);\n }\n \n /* Shift two 32-bit values in M right by COUNT; shift in the sign bit.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sra_pi32 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_psrad ((__v2si)__m, (__v2si)__count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psrad (__m64 __m, __m64 __count)\n {\n   return _mm_sra_pi32 (__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srai_pi32 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_psradi ((__v2si)__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psradi (__m64 __m, int __count)\n {\n   return _mm_srai_pi32 (__m, __count);\n }\n \n /* Shift four 16-bit values in M right by COUNT; shift in zeros.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srl_pi16 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_psrlw ((__v4hi)__m, (__v4hi)__count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psrlw (__m64 __m, __m64 __count)\n {\n   return _mm_srl_pi16 (__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srli_pi16 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_psrlwi ((__v4hi)__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psrlwi (__m64 __m, int __count)\n {\n   return _mm_srli_pi16 (__m, __count);\n }\n \n /* Shift two 32-bit values in M right by COUNT; shift in zeros.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srl_pi32 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_psrld ((__v2si)__m, (__v2si)__count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psrld (__m64 __m, __m64 __count)\n {\n   return _mm_srl_pi32 (__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srli_pi32 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_psrldi ((__v2si)__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psrldi (__m64 __m, int __count)\n {\n   return _mm_srli_pi32 (__m, __count);\n }\n \n /* Shift the 64-bit value in M left by COUNT; shift in zeros.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srl_si64 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_psrlq ((__v1di)__m, (__v1di)__count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psrlq (__m64 __m, __m64 __count)\n {\n   return _mm_srl_si64 (__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_srli_si64 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_psrlqi ((__v1di)__m, __count);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psrlqi (__m64 __m, int __count)\n {\n   return _mm_srli_si64 (__m, __count);\n }\n \n /* Bit-wise AND the 64-bit values in M1 and M2.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_and_si64 (__m64 __m1, __m64 __m2)\n {\n   return __builtin_ia32_pand (__m1, __m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pand (__m64 __m1, __m64 __m2)\n {\n   return _mm_and_si64 (__m1, __m2);\n }\n \n /* Bit-wise complement the 64-bit value in M1 and bit-wise AND it with the\n    64-bit value in M2.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_andnot_si64 (__m64 __m1, __m64 __m2)\n {\n   return __builtin_ia32_pandn (__m1, __m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pandn (__m64 __m1, __m64 __m2)\n {\n   return _mm_andnot_si64 (__m1, __m2);\n }\n \n /* Bit-wise inclusive OR the 64-bit values in M1 and M2.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_or_si64 (__m64 __m1, __m64 __m2)\n {\n   return __builtin_ia32_por (__m1, __m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_por (__m64 __m1, __m64 __m2)\n {\n   return _mm_or_si64 (__m1, __m2);\n }\n \n /* Bit-wise exclusive OR the 64-bit values in M1 and M2.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_xor_si64 (__m64 __m1, __m64 __m2)\n {\n   return __builtin_ia32_pxor (__m1, __m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pxor (__m64 __m1, __m64 __m2)\n {\n   return _mm_xor_si64 (__m1, __m2);\n }\n \n /* Compare eight 8-bit values.  The result of the comparison is 0xFF if the\n    test is true and zero if false.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpeq_pi8 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_pcmpeqb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pcmpeqb (__m64 __m1, __m64 __m2)\n {\n   return _mm_cmpeq_pi8 (__m1, __m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpgt_pi8 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_pcmpgtb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pcmpgtb (__m64 __m1, __m64 __m2)\n {\n   return _mm_cmpgt_pi8 (__m1, __m2);\n }\n \n /* Compare four 16-bit values.  The result of the comparison is 0xFFFF if\n    the test is true and zero if false.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpeq_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_pcmpeqw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pcmpeqw (__m64 __m1, __m64 __m2)\n {\n   return _mm_cmpeq_pi16 (__m1, __m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpgt_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_pcmpgtw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pcmpgtw (__m64 __m1, __m64 __m2)\n {\n   return _mm_cmpgt_pi16 (__m1, __m2);\n }\n \n /* Compare two 32-bit values.  The result of the comparison is 0xFFFFFFFF if\n    the test is true and zero if false.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpeq_pi32 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_pcmpeqd ((__v2si)__m1, (__v2si)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pcmpeqd (__m64 __m1, __m64 __m2)\n {\n   return _mm_cmpeq_pi32 (__m1, __m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpgt_pi32 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_pcmpgtd ((__v2si)__m1, (__v2si)__m2);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pcmpgtd (__m64 __m1, __m64 __m2)\n {\n   return _mm_cmpgt_pi32 (__m1, __m2);\n }\n \n /* Creates a 64-bit zero.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setzero_si64 (void)\n {\n   return (__m64)0LL;\n }\n \n /* Creates a vector of two 32-bit values; I0 is least significant.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_pi32 (int __i1, int __i0)\n {\n   return (__m64) __builtin_ia32_vec_init_v2si (__i0, __i1);\n }\n \n /* Creates a vector of four 16-bit values; W0 is least significant.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_pi16 (short __w3, short __w2, short __w1, short __w0)\n {\n   return (__m64) __builtin_ia32_vec_init_v4hi (__w0, __w1, __w2, __w3);\n }\n \n /* Creates a vector of eight 8-bit values; B0 is least significant.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_pi8 (char __b7, char __b6, char __b5, char __b4,\n \t     char __b3, char __b2, char __b1, char __b0)\n {\n@@ -879,41 +879,41 @@ _mm_set_pi8 (char __b7, char __b6, char __b5, char __b4,\n }\n \n /* Similar, but with the arguments in reverse order.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setr_pi32 (int __i0, int __i1)\n {\n   return _mm_set_pi32 (__i1, __i0);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setr_pi16 (short __w0, short __w1, short __w2, short __w3)\n {\n   return _mm_set_pi16 (__w3, __w2, __w1, __w0);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setr_pi8 (char __b0, char __b1, char __b2, char __b3,\n \t      char __b4, char __b5, char __b6, char __b7)\n {\n   return _mm_set_pi8 (__b7, __b6, __b5, __b4, __b3, __b2, __b1, __b0);\n }\n \n /* Creates a vector of two 32-bit values, both elements containing I.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set1_pi32 (int __i)\n {\n   return _mm_set_pi32 (__i, __i);\n }\n \n /* Creates a vector of four 16-bit values, all elements containing W.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set1_pi16 (short __w)\n {\n   return _mm_set_pi16 (__w, __w, __w, __w);\n }\n \n /* Creates a vector of eight 8-bit values, all elements containing B.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set1_pi8 (char __b)\n {\n   return _mm_set_pi8 (__b, __b, __b, __b, __b, __b, __b, __b);"}, {"sha": "809799a93aa4df31964597e2971ecf7bb2cb11c1", "filename": "gcc/config/i386/pmmintrin.h", "status": "modified", "additions": 13, "deletions": 13, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fpmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fpmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fpmmintrin.h?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -47,79 +47,79 @@\n #define _MM_GET_DENORMALS_ZERO_MODE() \\\n   (_mm_getcsr() & _MM_DENORMALS_ZERO_MASK)\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_addsub_ps (__m128 __X, __m128 __Y)\n {\n   return (__m128) __builtin_ia32_addsubps ((__v4sf)__X, (__v4sf)__Y);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hadd_ps (__m128 __X, __m128 __Y)\n {\n   return (__m128) __builtin_ia32_haddps ((__v4sf)__X, (__v4sf)__Y);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hsub_ps (__m128 __X, __m128 __Y)\n {\n   return (__m128) __builtin_ia32_hsubps ((__v4sf)__X, (__v4sf)__Y);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_movehdup_ps (__m128 __X)\n {\n   return (__m128) __builtin_ia32_movshdup ((__v4sf)__X);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_moveldup_ps (__m128 __X)\n {\n   return (__m128) __builtin_ia32_movsldup ((__v4sf)__X);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_addsub_pd (__m128d __X, __m128d __Y)\n {\n   return (__m128d) __builtin_ia32_addsubpd ((__v2df)__X, (__v2df)__Y);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hadd_pd (__m128d __X, __m128d __Y)\n {\n   return (__m128d) __builtin_ia32_haddpd ((__v2df)__X, (__v2df)__Y);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hsub_pd (__m128d __X, __m128d __Y)\n {\n   return (__m128d) __builtin_ia32_hsubpd ((__v2df)__X, (__v2df)__Y);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_loaddup_pd (double const *__P)\n {\n   return _mm_load1_pd (__P);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_movedup_pd (__m128d __X)\n {\n   return _mm_shuffle_pd (__X, __X, _MM_SHUFFLE2 (0,0));\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_lddqu_si128 (__m128i const *__P)\n {\n   return (__m128i) __builtin_ia32_lddqu ((char const *)__P);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_monitor (void const * __P, unsigned int __E, unsigned int __H)\n {\n   __builtin_ia32_monitor (__P, __E, __H);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mwait (unsigned int __E, unsigned int __H)\n {\n   __builtin_ia32_mwait (__E, __H);"}, {"sha": "d6423b48aa515c55d921f9af054c9dd12e7f28aa", "filename": "gcc/config/i386/smmintrin.h", "status": "modified", "additions": 64, "deletions": 64, "changes": 128, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fsmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fsmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fsmmintrin.h?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -45,7 +45,7 @@\n    constant/variable mask.  */\n \n #ifdef __OPTIMIZE__\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_blend_epi16 (__m128i __X, __m128i __Y, const int __M)\n {\n   return (__m128i) __builtin_ia32_pblendw128 ((__v8hi)__X,\n@@ -58,7 +58,7 @@ _mm_blend_epi16 (__m128i __X, __m128i __Y, const int __M)\n \t\t\t\t\t(__v8hi)(__m128i)(Y), (int)(M)))\n #endif\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_blendv_epi8 (__m128i __X, __m128i __Y, __m128i __M)\n {\n   return (__m128i) __builtin_ia32_pblendvb128 ((__v16qi)__X,\n@@ -70,7 +70,7 @@ _mm_blendv_epi8 (__m128i __X, __m128i __Y, __m128i __M)\n    from 2 sources using constant/variable mask.  */\n \n #ifdef __OPTIMIZE__\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_blend_ps (__m128 __X, __m128 __Y, const int __M)\n {\n   return (__m128) __builtin_ia32_blendps ((__v4sf)__X,\n@@ -83,7 +83,7 @@ _mm_blend_ps (__m128 __X, __m128 __Y, const int __M)\n \t\t\t\t    (__v4sf)(__m128)(Y), (int)(M)))\n #endif\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_blendv_ps (__m128 __X, __m128 __Y, __m128 __M)\n {\n   return (__m128) __builtin_ia32_blendvps ((__v4sf)__X,\n@@ -95,7 +95,7 @@ _mm_blendv_ps (__m128 __X, __m128 __Y, __m128 __M)\n    from 2 sources using constant/variable mask.  */\n \n #ifdef __OPTIMIZE__\n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_blend_pd (__m128d __X, __m128d __Y, const int __M)\n {\n   return (__m128d) __builtin_ia32_blendpd ((__v2df)__X,\n@@ -108,7 +108,7 @@ _mm_blend_pd (__m128d __X, __m128d __Y, const int __M)\n \t\t\t\t     (__v2df)(__m128d)(Y), (int)(M)))\n #endif\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_blendv_pd (__m128d __X, __m128d __Y, __m128d __M)\n {\n   return (__m128d) __builtin_ia32_blendvpd ((__v2df)__X,\n@@ -120,15 +120,15 @@ _mm_blendv_pd (__m128d __X, __m128d __Y, __m128d __M)\n    of result.  */\n \n #ifdef __OPTIMIZE__\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_dp_ps (__m128 __X, __m128 __Y, const int __M)\n {\n   return (__m128) __builtin_ia32_dpps ((__v4sf)__X,\n \t\t\t\t       (__v4sf)__Y,\n \t\t\t\t       __M);\n }\n \n-static __inline __m128d __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_dp_pd (__m128d __X, __m128d __Y, const int __M)\n {\n   return (__m128d) __builtin_ia32_dppd ((__v2df)__X,\n@@ -147,73 +147,73 @@ _mm_dp_pd (__m128d __X, __m128d __Y, const int __M)\n \n /* Packed integer 64-bit comparison, zeroing or filling with ones\n    corresponding parts of result.  */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpeq_epi64 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pcmpeqq ((__v2di)__X, (__v2di)__Y);\n }\n \n /*  Min/max packed integer instructions.  */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_epi8 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pminsb128 ((__v16qi)__X, (__v16qi)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_epi8 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pmaxsb128 ((__v16qi)__X, (__v16qi)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_epu16 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pminuw128 ((__v8hi)__X, (__v8hi)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_epu16 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pmaxuw128 ((__v8hi)__X, (__v8hi)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_epi32 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pminsd128 ((__v4si)__X, (__v4si)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_epi32 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pmaxsd128 ((__v4si)__X, (__v4si)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_epu32 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pminud128 ((__v4si)__X, (__v4si)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_epu32 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pmaxud128 ((__v4si)__X, (__v4si)__Y);\n }\n \n /* Packed integer 32-bit multiplication with truncation of upper\n    halves of results.  */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mullo_epi32 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pmulld128 ((__v4si)__X, (__v4si)__Y);\n }\n \n /* Packed integer 32-bit multiplication of 2 pairs of operands\n    with two 64-bit results.  */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mul_epi32 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pmuldq128 ((__v4si)__X, (__v4si)__Y);\n@@ -225,7 +225,7 @@ _mm_mul_epi32 (__m128i __X, __m128i __Y)\n    zeroing mask for D.  */\n \n #ifdef __OPTIMIZE__\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_insert_ps (__m128 __D, __m128 __S, const int __N)\n {\n   return (__m128) __builtin_ia32_insertps128 ((__v4sf)__D,\n@@ -245,7 +245,7 @@ _mm_insert_ps (__m128 __D, __m128 __S, const int __N)\n    single precision array element of X selected by index N.  */\n \n #ifdef __OPTIMIZE__\n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_extract_ps (__m128 __X, const int __N)\n {\n   union { int i; float f; } __tmp;\n@@ -278,22 +278,22 @@ _mm_extract_ps (__m128 __X, const int __N)\n    selected by index N.  */\n \n #ifdef __OPTIMIZE__\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_insert_epi8 (__m128i __D, int __S, const int __N)\n {\n   return (__m128i) __builtin_ia32_vec_set_v16qi ((__v16qi)__D,\n \t\t\t\t\t\t __S, __N);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_insert_epi32 (__m128i __D, int __S, const int __N)\n {\n   return (__m128i) __builtin_ia32_vec_set_v4si ((__v4si)__D,\n \t\t\t\t\t\t __S, __N);\n }\n \n #ifdef __x86_64__\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_insert_epi64 (__m128i __D, long long __S, const int __N)\n {\n   return (__m128i) __builtin_ia32_vec_set_v2di ((__v2di)__D,\n@@ -320,20 +320,20 @@ _mm_insert_epi64 (__m128i __D, long long __S, const int __N)\n    index N.  */\n \n #ifdef __OPTIMIZE__\n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_extract_epi8 (__m128i __X, const int __N)\n {\n    return __builtin_ia32_vec_ext_v16qi ((__v16qi)__X, __N);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_extract_epi32 (__m128i __X, const int __N)\n {\n    return __builtin_ia32_vec_ext_v4si ((__v4si)__X, __N);\n }\n \n #ifdef __x86_64__\n-static __inline long long  __attribute__((__always_inline__, __artificial__))\n+extern __inline long long  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_extract_epi64 (__m128i __X, const int __N)\n {\n   return __builtin_ia32_vec_ext_v2di ((__v2di)__X, __N);\n@@ -353,91 +353,91 @@ _mm_extract_epi64 (__m128i __X, const int __N)\n \n /* Return horizontal packed word minimum and its index in bits [15:0]\n    and bits [18:16] respectively.  */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_minpos_epu16 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_phminposuw128 ((__v8hi)__X);\n }\n \n /* Packed integer sign-extension.  */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepi8_epi32 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pmovsxbd128 ((__v16qi)__X);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepi16_epi32 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pmovsxwd128 ((__v8hi)__X);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepi8_epi64 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pmovsxbq128 ((__v16qi)__X);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepi32_epi64 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pmovsxdq128 ((__v4si)__X);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepi16_epi64 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pmovsxwq128 ((__v8hi)__X);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepi8_epi16 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pmovsxbw128 ((__v16qi)__X);\n }\n \n /* Packed integer zero-extension. */\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepu8_epi32 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pmovzxbd128 ((__v16qi)__X);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepu16_epi32 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pmovzxwd128 ((__v8hi)__X);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepu8_epi64 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pmovzxbq128 ((__v16qi)__X);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepu32_epi64 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pmovzxdq128 ((__v4si)__X);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepu16_epi64 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pmovzxwq128 ((__v8hi)__X);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtepu8_epi16 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pmovzxbw128 ((__v16qi)__X);\n }\n \n /* Pack 8 double words from 2 operands into 8 words of result with\n    unsigned saturation. */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_packus_epi32 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_packusdw128 ((__v4si)__X, (__v4si)__Y);\n@@ -448,7 +448,7 @@ _mm_packus_epi32 (__m128i __X, __m128i __Y)\n    operands are determined by the 3rd mask operand.  */\n \n #ifdef __OPTIMIZE__\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mpsadbw_epu8 (__m128i __X, __m128i __Y, const int __M)\n {\n   return (__m128i) __builtin_ia32_mpsadbw128 ((__v16qi)__X,\n@@ -461,7 +461,7 @@ _mm_mpsadbw_epu8 (__m128i __X, __m128i __Y, const int __M)\n #endif\n \n /* Load double quadword using non-temporal aligned hint.  */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_stream_load_si128 (__m128i *__X)\n {\n   return (__m128i) __builtin_ia32_movntdqa ((__v2di *) __X);\n@@ -498,31 +498,31 @@ _mm_stream_load_si128 (__m128i *__X)\n /* Intrinsics for text/string processing.  */\n \n #ifdef __OPTIMIZE__\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpistrm (__m128i __X, __m128i __Y, const int __M)\n {\n   return (__m128i) __builtin_ia32_pcmpistrm128 ((__v16qi)__X,\n \t\t\t\t\t\t(__v16qi)__Y,\n \t\t\t\t\t\t__M);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpistri (__m128i __X, __m128i __Y, const int __M)\n {\n   return __builtin_ia32_pcmpistri128 ((__v16qi)__X,\n \t\t\t\t      (__v16qi)__Y,\n \t\t\t\t      __M);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpestrm (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)\n {\n   return (__m128i) __builtin_ia32_pcmpestrm128 ((__v16qi)__X, __LX,\n \t\t\t\t\t\t(__v16qi)__Y, __LY,\n \t\t\t\t\t\t__M);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpestri (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)\n {\n   return __builtin_ia32_pcmpestri128 ((__v16qi)__X, __LX,\n@@ -551,79 +551,79 @@ _mm_cmpestri (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)\n    EFlags.  */\n \n #ifdef __OPTIMIZE__\n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpistra (__m128i __X, __m128i __Y, const int __M)\n {\n   return __builtin_ia32_pcmpistria128 ((__v16qi)__X,\n \t\t\t\t       (__v16qi)__Y,\n \t\t\t\t       __M);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpistrc (__m128i __X, __m128i __Y, const int __M)\n {\n   return __builtin_ia32_pcmpistric128 ((__v16qi)__X,\n \t\t\t\t       (__v16qi)__Y,\n \t\t\t\t       __M);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpistro (__m128i __X, __m128i __Y, const int __M)\n {\n   return __builtin_ia32_pcmpistrio128 ((__v16qi)__X,\n \t\t\t\t       (__v16qi)__Y,\n \t\t\t\t       __M);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpistrs (__m128i __X, __m128i __Y, const int __M)\n {\n   return __builtin_ia32_pcmpistris128 ((__v16qi)__X,\n \t\t\t\t       (__v16qi)__Y,\n \t\t\t\t       __M);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpistrz (__m128i __X, __m128i __Y, const int __M)\n {\n   return __builtin_ia32_pcmpistriz128 ((__v16qi)__X,\n \t\t\t\t       (__v16qi)__Y,\n \t\t\t\t       __M);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpestra (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)\n {\n   return __builtin_ia32_pcmpestria128 ((__v16qi)__X, __LX,\n \t\t\t\t       (__v16qi)__Y, __LY,\n \t\t\t\t       __M);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpestrc (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)\n {\n   return __builtin_ia32_pcmpestric128 ((__v16qi)__X, __LX,\n \t\t\t\t       (__v16qi)__Y, __LY,\n \t\t\t\t       __M);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpestro (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)\n {\n   return __builtin_ia32_pcmpestrio128 ((__v16qi)__X, __LX,\n \t\t\t\t       (__v16qi)__Y, __LY,\n \t\t\t\t       __M);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpestrs (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)\n {\n   return __builtin_ia32_pcmpestris128 ((__v16qi)__X, __LX,\n \t\t\t\t       (__v16qi)__Y, __LY,\n \t\t\t\t       __M);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpestrz (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)\n {\n   return __builtin_ia32_pcmpestriz128 ((__v16qi)__X, __LX,\n@@ -671,48 +671,48 @@ _mm_cmpestrz (__m128i __X, int __LX, __m128i __Y, int __LY, const int __M)\n \n /* Packed integer 64-bit comparison, zeroing or filling with ones\n    corresponding parts of result.  */\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpgt_epi64 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pcmpgtq ((__v2di)__X, (__v2di)__Y);\n }\n \n /* Calculate a number of bits set to 1.  */\n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_popcnt_u32 (unsigned int __X)\n {\n   return __builtin_popcount (__X);\n }\n \n #ifdef __x86_64__\n-static __inline long long  __attribute__((__always_inline__, __artificial__))\n+extern __inline long long  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_popcnt_u64 (unsigned long long __X)\n {\n   return __builtin_popcountll (__X);\n }\n #endif\n \n /* Accumulate CRC32 (polynomial 0x11EDC6F41) value.  */\n-static __inline unsigned int __attribute__((__always_inline__, __artificial__))\n+extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_crc32_u8 (unsigned int __C, unsigned char __V)\n {\n   return __builtin_ia32_crc32qi (__C, __V);\n }\n \n-static __inline unsigned int __attribute__((__always_inline__, __artificial__))\n+extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_crc32_u16 (unsigned int __C, unsigned short __V)\n {\n   return __builtin_ia32_crc32hi (__C, __V);\n }\n \n-static __inline unsigned int __attribute__((__always_inline__, __artificial__))\n+extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_crc32_u32 (unsigned int __C, unsigned int __V)\n {\n   return __builtin_ia32_crc32si (__C, __V);\n }\n \n #ifdef __x86_64__\n-static __inline unsigned long long __attribute__((__always_inline__, __artificial__))\n+extern __inline unsigned long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_crc32_u64 (unsigned long long __C, unsigned long long __V)\n {\n   return __builtin_ia32_crc32di (__C, __V);"}, {"sha": "827d8b95b764619743e645649b5c53a3abf5c393", "filename": "gcc/config/i386/tmmintrin.h", "status": "modified", "additions": 32, "deletions": 32, "changes": 64, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Ftmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Ftmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Ftmmintrin.h?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -37,159 +37,159 @@\n /* We need definitions from the SSE3, SSE2 and SSE header files*/\n #include <pmmintrin.h>\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hadd_epi16 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_phaddw128 ((__v8hi)__X, (__v8hi)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hadd_epi32 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_phaddd128 ((__v4si)__X, (__v4si)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hadds_epi16 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_phaddsw128 ((__v8hi)__X, (__v8hi)__Y);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hadd_pi16 (__m64 __X, __m64 __Y)\n {\n   return (__m64) __builtin_ia32_phaddw ((__v4hi)__X, (__v4hi)__Y);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hadd_pi32 (__m64 __X, __m64 __Y)\n {\n   return (__m64) __builtin_ia32_phaddd ((__v2si)__X, (__v2si)__Y);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hadds_pi16 (__m64 __X, __m64 __Y)\n {\n   return (__m64) __builtin_ia32_phaddsw ((__v4hi)__X, (__v4hi)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hsub_epi16 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_phsubw128 ((__v8hi)__X, (__v8hi)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hsub_epi32 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_phsubd128 ((__v4si)__X, (__v4si)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hsubs_epi16 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_phsubsw128 ((__v8hi)__X, (__v8hi)__Y);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hsub_pi16 (__m64 __X, __m64 __Y)\n {\n   return (__m64) __builtin_ia32_phsubw ((__v4hi)__X, (__v4hi)__Y);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hsub_pi32 (__m64 __X, __m64 __Y)\n {\n   return (__m64) __builtin_ia32_phsubd ((__v2si)__X, (__v2si)__Y);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_hsubs_pi16 (__m64 __X, __m64 __Y)\n {\n   return (__m64) __builtin_ia32_phsubsw ((__v4hi)__X, (__v4hi)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maddubs_epi16 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pmaddubsw128 ((__v16qi)__X, (__v16qi)__Y);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maddubs_pi16 (__m64 __X, __m64 __Y)\n {\n   return (__m64) __builtin_ia32_pmaddubsw ((__v8qi)__X, (__v8qi)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mulhrs_epi16 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pmulhrsw128 ((__v8hi)__X, (__v8hi)__Y);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mulhrs_pi16 (__m64 __X, __m64 __Y)\n {\n   return (__m64) __builtin_ia32_pmulhrsw ((__v4hi)__X, (__v4hi)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shuffle_epi8 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_pshufb128 ((__v16qi)__X, (__v16qi)__Y);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shuffle_pi8 (__m64 __X, __m64 __Y)\n {\n   return (__m64) __builtin_ia32_pshufb ((__v8qi)__X, (__v8qi)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sign_epi8 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_psignb128 ((__v16qi)__X, (__v16qi)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sign_epi16 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_psignw128 ((__v8hi)__X, (__v8hi)__Y);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sign_epi32 (__m128i __X, __m128i __Y)\n {\n   return (__m128i) __builtin_ia32_psignd128 ((__v4si)__X, (__v4si)__Y);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sign_pi8 (__m64 __X, __m64 __Y)\n {\n   return (__m64) __builtin_ia32_psignb ((__v8qi)__X, (__v8qi)__Y);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sign_pi16 (__m64 __X, __m64 __Y)\n {\n   return (__m64) __builtin_ia32_psignw ((__v4hi)__X, (__v4hi)__Y);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sign_pi32 (__m64 __X, __m64 __Y)\n {\n   return (__m64) __builtin_ia32_psignd ((__v2si)__X, (__v2si)__Y);\n }\n \n #ifdef __OPTIMIZE__\n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_alignr_epi8(__m128i __X, __m128i __Y, const int __N)\n {\n   return (__m128i) __builtin_ia32_palignr128 ((__v2di)__X,\n \t\t\t\t\t      (__v2di)__Y, __N * 8);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_alignr_pi8(__m64 __X, __m64 __Y, const int __N)\n {\n   return (__m64) __builtin_ia32_palignr ((long long)__X,\n@@ -206,37 +206,37 @@ _mm_alignr_pi8(__m64 __X, __m64 __Y, const int __N)\n \t\t\t\t   (int)(N) * 8))\n #endif\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_abs_epi8 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pabsb128 ((__v16qi)__X);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_abs_epi16 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pabsw128 ((__v8hi)__X);\n }\n \n-static __inline __m128i __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_abs_epi32 (__m128i __X)\n {\n   return (__m128i) __builtin_ia32_pabsd128 ((__v4si)__X);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_abs_pi8 (__m64 __X)\n {\n   return (__m64) __builtin_ia32_pabsb ((__v8qi)__X);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_abs_pi16 (__m64 __X)\n {\n   return (__m64) __builtin_ia32_pabsw ((__v4hi)__X);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_abs_pi32 (__m64 __X)\n {\n   return (__m64) __builtin_ia32_pabsd ((__v2si)__X);"}, {"sha": "f176d741f74075765628d33a43600c9dda516cf9", "filename": "gcc/config/i386/xmmintrin.h", "status": "modified", "additions": 154, "deletions": 154, "changes": 308, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fxmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Fconfig%2Fi386%2Fxmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fxmmintrin.h?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -89,7 +89,7 @@ enum _mm_hint\n #define _MM_FLUSH_ZERO_OFF    0x0000\n \n /* Create a vector of zeros.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setzero_ps (void)\n {\n   return __extension__ (__m128){ 0.0f, 0.0f, 0.0f, 0.0f };\n@@ -99,137 +99,137 @@ _mm_setzero_ps (void)\n    floating-point) values of A and B; the upper three SPFP values are\n    passed through from A.  */\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_add_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_addss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sub_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_subss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mul_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_mulss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_div_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_divss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sqrt_ss (__m128 __A)\n {\n   return (__m128) __builtin_ia32_sqrtss ((__v4sf)__A);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_rcp_ss (__m128 __A)\n {\n   return (__m128) __builtin_ia32_rcpss ((__v4sf)__A);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_rsqrt_ss (__m128 __A)\n {\n   return (__m128) __builtin_ia32_rsqrtss ((__v4sf)__A);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_minss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_maxss ((__v4sf)__A, (__v4sf)__B);\n }\n \n /* Perform the respective operation on the four SPFP values in A and B.  */\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_add_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_addps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sub_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_subps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mul_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_mulps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_div_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_divps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sqrt_ps (__m128 __A)\n {\n   return (__m128) __builtin_ia32_sqrtps ((__v4sf)__A);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_rcp_ps (__m128 __A)\n {\n   return (__m128) __builtin_ia32_rcpps ((__v4sf)__A);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_rsqrt_ps (__m128 __A)\n {\n   return (__m128) __builtin_ia32_rsqrtps ((__v4sf)__A);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_minps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_maxps ((__v4sf)__A, (__v4sf)__B);\n }\n \n /* Perform logical bit-wise operations on 128-bit values.  */\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_and_ps (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_andps (__A, __B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_andnot_ps (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_andnps (__A, __B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_or_ps (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_orps (__A, __B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_xor_ps (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_xorps (__A, __B);\n@@ -239,25 +239,25 @@ _mm_xor_ps (__m128 __A, __m128 __B)\n    comparison is true, place a mask of all ones in the result, otherwise a\n    mask of zeros.  The upper three SPFP values are passed through from A.  */\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpeq_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpeqss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmplt_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpltss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmple_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpless ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpgt_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_movss ((__v4sf) __A,\n@@ -267,7 +267,7 @@ _mm_cmpgt_ss (__m128 __A, __m128 __B)\n \t\t\t\t\t\t\t\t__A));\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpge_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_movss ((__v4sf) __A,\n@@ -277,25 +277,25 @@ _mm_cmpge_ss (__m128 __A, __m128 __B)\n \t\t\t\t\t\t\t\t__A));\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpneq_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpneqss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnlt_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpnltss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnle_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpnless ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpngt_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_movss ((__v4sf) __A,\n@@ -305,7 +305,7 @@ _mm_cmpngt_ss (__m128 __A, __m128 __B)\n \t\t\t\t\t\t\t\t __A));\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnge_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_movss ((__v4sf) __A,\n@@ -315,13 +315,13 @@ _mm_cmpnge_ss (__m128 __A, __m128 __B)\n \t\t\t\t\t\t\t\t __A));\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpord_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpordss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpunord_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpunordss ((__v4sf)__A, (__v4sf)__B);\n@@ -331,73 +331,73 @@ _mm_cmpunord_ss (__m128 __A, __m128 __B)\n    element, if the comparison is true, place a mask of all ones in the\n    result, otherwise a mask of zeros.  */\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpeq_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpeqps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmplt_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpltps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmple_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpleps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpgt_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpgtps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpge_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpgeps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpneq_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpneqps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnlt_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpnltps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnle_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpnleps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpngt_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpngtps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpnge_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpngeps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpord_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpordps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cmpunord_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_cmpunordps ((__v4sf)__A, (__v4sf)__B);\n@@ -406,87 +406,87 @@ _mm_cmpunord_ps (__m128 __A, __m128 __B)\n /* Compare the lower SPFP values of A and B and return 1 if true\n    and 0 if false.  */\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comieq_ss (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_comieq ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comilt_ss (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_comilt ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comile_ss (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_comile ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comigt_ss (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_comigt ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comige_ss (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_comige ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_comineq_ss (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_comineq ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomieq_ss (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_ucomieq ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomilt_ss (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_ucomilt ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomile_ss (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_ucomile ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomigt_ss (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_ucomigt ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomige_ss (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_ucomige ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_ucomineq_ss (__m128 __A, __m128 __B)\n {\n   return __builtin_ia32_ucomineq ((__v4sf)__A, (__v4sf)__B);\n }\n \n /* Convert the lower SPFP value to a 32-bit integer according to the current\n    rounding mode.  */\n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtss_si32 (__m128 __A)\n {\n   return __builtin_ia32_cvtss2si ((__v4sf) __A);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvt_ss2si (__m128 __A)\n {\n   return _mm_cvtss_si32 (__A);\n@@ -497,14 +497,14 @@ _mm_cvt_ss2si (__m128 __A)\n    current rounding mode.  */\n \n /* Intel intrinsic.  */\n-static __inline long long __attribute__((__always_inline__, __artificial__))\n+extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtss_si64 (__m128 __A)\n {\n   return __builtin_ia32_cvtss2si64 ((__v4sf) __A);\n }\n \n /* Microsoft intrinsic.  */\n-static __inline long long __attribute__((__always_inline__, __artificial__))\n+extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtss_si64x (__m128 __A)\n {\n   return __builtin_ia32_cvtss2si64 ((__v4sf) __A);\n@@ -513,26 +513,26 @@ _mm_cvtss_si64x (__m128 __A)\n \n /* Convert the two lower SPFP values to 32-bit integers according to the\n    current rounding mode.  Return the integers in packed form.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtps_pi32 (__m128 __A)\n {\n   return (__m64) __builtin_ia32_cvtps2pi ((__v4sf) __A);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvt_ps2pi (__m128 __A)\n {\n   return _mm_cvtps_pi32 (__A);\n }\n \n /* Truncate the lower SPFP value to a 32-bit integer.  */\n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttss_si32 (__m128 __A)\n {\n   return __builtin_ia32_cvttss2si ((__v4sf) __A);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtt_ss2si (__m128 __A)\n {\n   return _mm_cvttss_si32 (__A);\n@@ -542,14 +542,14 @@ _mm_cvtt_ss2si (__m128 __A)\n /* Truncate the lower SPFP value to a 32-bit integer.  */\n \n /* Intel intrinsic.  */\n-static __inline long long __attribute__((__always_inline__, __artificial__))\n+extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttss_si64 (__m128 __A)\n {\n   return __builtin_ia32_cvttss2si64 ((__v4sf) __A);\n }\n \n /* Microsoft intrinsic.  */\n-static __inline long long __attribute__((__always_inline__, __artificial__))\n+extern __inline long long __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttss_si64x (__m128 __A)\n {\n   return __builtin_ia32_cvttss2si64 ((__v4sf) __A);\n@@ -558,26 +558,26 @@ _mm_cvttss_si64x (__m128 __A)\n \n /* Truncate the two lower SPFP values to 32-bit integers.  Return the\n    integers in packed form.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvttps_pi32 (__m128 __A)\n {\n   return (__m64) __builtin_ia32_cvttps2pi ((__v4sf) __A);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtt_ps2pi (__m128 __A)\n {\n   return _mm_cvttps_pi32 (__A);\n }\n \n /* Convert B to a SPFP value and insert it as element zero in A.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi32_ss (__m128 __A, int __B)\n {\n   return (__m128) __builtin_ia32_cvtsi2ss ((__v4sf) __A, __B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvt_si2ss (__m128 __A, int __B)\n {\n   return _mm_cvtsi32_ss (__A, __B);\n@@ -587,14 +587,14 @@ _mm_cvt_si2ss (__m128 __A, int __B)\n /* Convert B to a SPFP value and insert it as element zero in A.  */\n \n /* Intel intrinsic.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi64_ss (__m128 __A, long long __B)\n {\n   return (__m128) __builtin_ia32_cvtsi642ss ((__v4sf) __A, __B);\n }\n \n /* Microsoft intrinsic.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtsi64x_ss (__m128 __A, long long __B)\n {\n   return (__m128) __builtin_ia32_cvtsi642ss ((__v4sf) __A, __B);\n@@ -603,20 +603,20 @@ _mm_cvtsi64x_ss (__m128 __A, long long __B)\n \n /* Convert the two 32-bit values in B to SPFP form and insert them\n    as the two lower elements in A.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpi32_ps (__m128 __A, __m64 __B)\n {\n   return (__m128) __builtin_ia32_cvtpi2ps ((__v4sf) __A, (__v2si)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvt_pi2ps (__m128 __A, __m64 __B)\n {\n   return _mm_cvtpi32_ps (__A, __B);\n }\n \n /* Convert the four signed 16-bit values in A to SPFP form.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpi16_ps (__m64 __A)\n {\n   __v4hi __sign;\n@@ -642,7 +642,7 @@ _mm_cvtpi16_ps (__m64 __A)\n }\n \n /* Convert the four unsigned 16-bit values in A to SPFP form.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpu16_ps (__m64 __A)\n {\n   __v2si __hisi, __losi;\n@@ -662,7 +662,7 @@ _mm_cvtpu16_ps (__m64 __A)\n }\n \n /* Convert the low four signed 8-bit values in A to SPFP form.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpi8_ps (__m64 __A)\n {\n   __v8qi __sign;\n@@ -679,15 +679,15 @@ _mm_cvtpi8_ps (__m64 __A)\n }\n \n /* Convert the low four unsigned 8-bit values in A to SPFP form.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpu8_ps(__m64 __A)\n {\n   __A = (__m64) __builtin_ia32_punpcklbw ((__v8qi)__A, (__v8qi)0LL);\n   return _mm_cvtpu16_ps(__A);\n }\n \n /* Convert the four signed 32-bit values in A and B to SPFP form.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtpi32x2_ps(__m64 __A, __m64 __B)\n {\n   __v4sf __zero = (__v4sf) _mm_setzero_ps ();\n@@ -697,7 +697,7 @@ _mm_cvtpi32x2_ps(__m64 __A, __m64 __B)\n }\n \n /* Convert the four SPFP values in A to four signed 16-bit integers.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtps_pi16(__m128 __A)\n {\n   __v4sf __hisf = (__v4sf)__A;\n@@ -708,7 +708,7 @@ _mm_cvtps_pi16(__m128 __A)\n }\n \n /* Convert the four SPFP values in A to four signed 8-bit integers.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtps_pi8(__m128 __A)\n {\n   __v4hi __tmp = (__v4hi) _mm_cvtps_pi16 (__A);\n@@ -717,7 +717,7 @@ _mm_cvtps_pi8(__m128 __A)\n \n /* Selects four specific SPFP values from A and B based on MASK.  */\n #ifdef __OPTIMIZE__\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shuffle_ps (__m128 __A, __m128 __B, int const __mask)\n {\n   return (__m128) __builtin_ia32_shufps ((__v4sf)__A, (__v4sf)__B, __mask);\n@@ -729,254 +729,254 @@ _mm_shuffle_ps (__m128 __A, __m128 __B, int const __mask)\n #endif\n \n /* Selects and interleaves the upper two SPFP values from A and B.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpackhi_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_unpckhps ((__v4sf)__A, (__v4sf)__B);\n }\n \n /* Selects and interleaves the lower two SPFP values from A and B.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_unpacklo_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_unpcklps ((__v4sf)__A, (__v4sf)__B);\n }\n \n /* Sets the upper two SPFP values with 64-bits of data loaded from P;\n    the lower two values are passed through from A.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_loadh_pi (__m128 __A, __m64 const *__P)\n {\n   return (__m128) __builtin_ia32_loadhps ((__v4sf)__A, (__v2si *)__P);\n }\n \n /* Stores the upper two SPFP values of A into P.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_storeh_pi (__m64 *__P, __m128 __A)\n {\n   __builtin_ia32_storehps ((__v2si *)__P, (__v4sf)__A);\n }\n \n /* Moves the upper two values of B into the lower two values of A.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_movehl_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_movhlps ((__v4sf)__A, (__v4sf)__B);\n }\n \n /* Moves the lower two values of B into the upper two values of A.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_movelh_ps (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_movlhps ((__v4sf)__A, (__v4sf)__B);\n }\n \n /* Sets the lower two SPFP values with 64-bits of data loaded from P;\n    the upper two values are passed through from A.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_loadl_pi (__m128 __A, __m64 const *__P)\n {\n   return (__m128) __builtin_ia32_loadlps ((__v4sf)__A, (__v2si *)__P);\n }\n \n /* Stores the lower two SPFP values of A into P.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_storel_pi (__m64 *__P, __m128 __A)\n {\n   __builtin_ia32_storelps ((__v2si *)__P, (__v4sf)__A);\n }\n \n /* Creates a 4-bit mask from the most significant bits of the SPFP values.  */\n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_movemask_ps (__m128 __A)\n {\n   return __builtin_ia32_movmskps ((__v4sf)__A);\n }\n \n /* Return the contents of the control register.  */\n-static __inline unsigned int __attribute__((__always_inline__, __artificial__))\n+extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_getcsr (void)\n {\n   return __builtin_ia32_stmxcsr ();\n }\n \n /* Read exception bits from the control register.  */\n-static __inline unsigned int __attribute__((__always_inline__, __artificial__))\n+extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _MM_GET_EXCEPTION_STATE (void)\n {\n   return _mm_getcsr() & _MM_EXCEPT_MASK;\n }\n \n-static __inline unsigned int __attribute__((__always_inline__, __artificial__))\n+extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _MM_GET_EXCEPTION_MASK (void)\n {\n   return _mm_getcsr() & _MM_MASK_MASK;\n }\n \n-static __inline unsigned int __attribute__((__always_inline__, __artificial__))\n+extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _MM_GET_ROUNDING_MODE (void)\n {\n   return _mm_getcsr() & _MM_ROUND_MASK;\n }\n \n-static __inline unsigned int __attribute__((__always_inline__, __artificial__))\n+extern __inline unsigned int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _MM_GET_FLUSH_ZERO_MODE (void)\n {\n   return _mm_getcsr() & _MM_FLUSH_ZERO_MASK;\n }\n \n /* Set the control register to I.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setcsr (unsigned int __I)\n {\n   __builtin_ia32_ldmxcsr (__I);\n }\n \n /* Set exception bits in the control register.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _MM_SET_EXCEPTION_STATE(unsigned int __mask)\n {\n   _mm_setcsr((_mm_getcsr() & ~_MM_EXCEPT_MASK) | __mask);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _MM_SET_EXCEPTION_MASK (unsigned int __mask)\n {\n   _mm_setcsr((_mm_getcsr() & ~_MM_MASK_MASK) | __mask);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _MM_SET_ROUNDING_MODE (unsigned int __mode)\n {\n   _mm_setcsr((_mm_getcsr() & ~_MM_ROUND_MASK) | __mode);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _MM_SET_FLUSH_ZERO_MODE (unsigned int __mode)\n {\n   _mm_setcsr((_mm_getcsr() & ~_MM_FLUSH_ZERO_MASK) | __mode);\n }\n \n /* Create a vector with element 0 as F and the rest zero.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_ss (float __F)\n {\n   return __extension__ (__m128)(__v4sf){ __F, 0.0f, 0.0f, 0.0f };\n }\n \n /* Create a vector with all four elements equal to F.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set1_ps (float __F)\n {\n   return __extension__ (__m128)(__v4sf){ __F, __F, __F, __F };\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_ps1 (float __F)\n {\n   return _mm_set1_ps (__F);\n }\n \n /* Create a vector with element 0 as *P and the rest zero.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_load_ss (float const *__P)\n {\n   return _mm_set_ss (*__P);\n }\n \n /* Create a vector with all four elements equal to *P.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_load1_ps (float const *__P)\n {\n   return _mm_set1_ps (*__P);\n }\n \n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_load_ps1 (float const *__P)\n {\n   return _mm_load1_ps (__P);\n }\n \n /* Load four SPFP values from P.  The address must be 16-byte aligned.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_load_ps (float const *__P)\n {\n   return (__m128) *(__v4sf *)__P;\n }\n \n /* Load four SPFP values from P.  The address need not be 16-byte aligned.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_loadu_ps (float const *__P)\n {\n   return (__m128) __builtin_ia32_loadups (__P);\n }\n \n /* Load four SPFP values in reverse order.  The address must be aligned.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_loadr_ps (float const *__P)\n {\n   __v4sf __tmp = *(__v4sf *)__P;\n   return (__m128) __builtin_ia32_shufps (__tmp, __tmp, _MM_SHUFFLE (0,1,2,3));\n }\n \n /* Create the vector [Z Y X W].  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_set_ps (const float __Z, const float __Y, const float __X, const float __W)\n {\n   return __extension__ (__m128)(__v4sf){ __W, __X, __Y, __Z };\n }\n \n /* Create the vector [W X Y Z].  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_setr_ps (float __Z, float __Y, float __X, float __W)\n {\n   return __extension__ (__m128)(__v4sf){ __Z, __Y, __X, __W };\n }\n \n /* Stores the lower SPFP value.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_store_ss (float *__P, __m128 __A)\n {\n   *__P = __builtin_ia32_vec_ext_v4sf ((__v4sf)__A, 0);\n }\n \n-static __inline float __attribute__((__always_inline__, __artificial__))\n+extern __inline float __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_cvtss_f32 (__m128 __A)\n {\n   return __builtin_ia32_vec_ext_v4sf ((__v4sf)__A, 0);\n }\n \n /* Store four SPFP values.  The address must be 16-byte aligned.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_store_ps (float *__P, __m128 __A)\n {\n   *(__v4sf *)__P = (__v4sf)__A;\n }\n \n /* Store four SPFP values.  The address need not be 16-byte aligned.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_storeu_ps (float *__P, __m128 __A)\n {\n   __builtin_ia32_storeups (__P, (__v4sf)__A);\n }\n \n /* Store the lower SPFP value across four words.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_store1_ps (float *__P, __m128 __A)\n {\n   __v4sf __va = (__v4sf)__A;\n   __v4sf __tmp = __builtin_ia32_shufps (__va, __va, _MM_SHUFFLE (0,0,0,0));\n   _mm_storeu_ps (__P, __tmp);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_store_ps1 (float *__P, __m128 __A)\n {\n   _mm_store1_ps (__P, __A);\n }\n \n /* Store four SPFP values in reverse order.  The address must be aligned.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_storer_ps (float *__P, __m128 __A)\n {\n   __v4sf __va = (__v4sf)__A;\n@@ -985,21 +985,21 @@ _mm_storer_ps (float *__P, __m128 __A)\n }\n \n /* Sets the low SPFP value of A from the low value of B.  */\n-static __inline __m128 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_move_ss (__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_movss ((__v4sf)__A, (__v4sf)__B);\n }\n \n /* Extracts one of the four words of A.  The selector N must be immediate.  */\n #ifdef __OPTIMIZE__\n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_extract_pi16 (__m64 const __A, int const __N)\n {\n   return __builtin_ia32_vec_ext_v4hi ((__v4hi)__A, __N);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pextrw (__m64 const __A, int const __N)\n {\n   return _mm_extract_pi16 (__A, __N);\n@@ -1014,13 +1014,13 @@ _m_pextrw (__m64 const __A, int const __N)\n /* Inserts word D into one of four words of A.  The selector N must be\n    immediate.  */\n #ifdef __OPTIMIZE__\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_insert_pi16 (__m64 const __A, int const __D, int const __N)\n {\n   return (__m64) __builtin_ia32_vec_set_v4hi ((__v4hi)__A, __D, __N);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pinsrw (__m64 const __A, int const __D, int const __N)\n {\n   return _mm_insert_pi16 (__A, __D, __N);\n@@ -1034,79 +1034,79 @@ _m_pinsrw (__m64 const __A, int const __D, int const __N)\n #endif\n \n /* Compute the element-wise maximum of signed 16-bit values.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_pi16 (__m64 __A, __m64 __B)\n {\n   return (__m64) __builtin_ia32_pmaxsw ((__v4hi)__A, (__v4hi)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pmaxsw (__m64 __A, __m64 __B)\n {\n   return _mm_max_pi16 (__A, __B);\n }\n \n /* Compute the element-wise maximum of unsigned 8-bit values.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_max_pu8 (__m64 __A, __m64 __B)\n {\n   return (__m64) __builtin_ia32_pmaxub ((__v8qi)__A, (__v8qi)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pmaxub (__m64 __A, __m64 __B)\n {\n   return _mm_max_pu8 (__A, __B);\n }\n \n /* Compute the element-wise minimum of signed 16-bit values.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_pi16 (__m64 __A, __m64 __B)\n {\n   return (__m64) __builtin_ia32_pminsw ((__v4hi)__A, (__v4hi)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pminsw (__m64 __A, __m64 __B)\n {\n   return _mm_min_pi16 (__A, __B);\n }\n \n /* Compute the element-wise minimum of unsigned 8-bit values.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_min_pu8 (__m64 __A, __m64 __B)\n {\n   return (__m64) __builtin_ia32_pminub ((__v8qi)__A, (__v8qi)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pminub (__m64 __A, __m64 __B)\n {\n   return _mm_min_pu8 (__A, __B);\n }\n \n /* Create an 8-bit mask of the signs of 8-bit values.  */\n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_movemask_pi8 (__m64 __A)\n {\n   return __builtin_ia32_pmovmskb ((__v8qi)__A);\n }\n \n-static __inline int __attribute__((__always_inline__, __artificial__))\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pmovmskb (__m64 __A)\n {\n   return _mm_movemask_pi8 (__A);\n }\n \n /* Multiply four unsigned 16-bit values in A by four unsigned 16-bit values\n    in B and produce the high 16 bits of the 32-bit results.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_mulhi_pu16 (__m64 __A, __m64 __B)\n {\n   return (__m64) __builtin_ia32_pmulhuw ((__v4hi)__A, (__v4hi)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pmulhuw (__m64 __A, __m64 __B)\n {\n   return _mm_mulhi_pu16 (__A, __B);\n@@ -1115,13 +1115,13 @@ _m_pmulhuw (__m64 __A, __m64 __B)\n /* Return a combination of the four 16-bit values in A.  The selector\n    must be an immediate.  */\n #ifdef __OPTIMIZE__\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_shuffle_pi16 (__m64 __A, int const __N)\n {\n   return (__m64) __builtin_ia32_pshufw ((__v4hi)__A, __N);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pshufw (__m64 __A, int const __N)\n {\n   return _mm_shuffle_pi16 (__A, __N);\n@@ -1136,39 +1136,39 @@ _m_pshufw (__m64 __A, int const __N)\n /* Conditionally store byte elements of A into P.  The high bit of each\n    byte in the selector N determines whether the corresponding byte from\n    A is stored.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_maskmove_si64 (__m64 __A, __m64 __N, char *__P)\n {\n   __builtin_ia32_maskmovq ((__v8qi)__A, (__v8qi)__N, __P);\n }\n \n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_maskmovq (__m64 __A, __m64 __N, char *__P)\n {\n   _mm_maskmove_si64 (__A, __N, __P);\n }\n \n /* Compute the rounded averages of the unsigned 8-bit values in A and B.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_avg_pu8 (__m64 __A, __m64 __B)\n {\n   return (__m64) __builtin_ia32_pavgb ((__v8qi)__A, (__v8qi)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pavgb (__m64 __A, __m64 __B)\n {\n   return _mm_avg_pu8 (__A, __B);\n }\n \n /* Compute the rounded averages of the unsigned 16-bit values in A and B.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_avg_pu16 (__m64 __A, __m64 __B)\n {\n   return (__m64) __builtin_ia32_pavgw ((__v4hi)__A, (__v4hi)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_pavgw (__m64 __A, __m64 __B)\n {\n   return _mm_avg_pu16 (__A, __B);\n@@ -1177,13 +1177,13 @@ _m_pavgw (__m64 __A, __m64 __B)\n /* Compute the sum of the absolute differences of the unsigned 8-bit\n    values in A and B.  Return the value in the lower 16-bit word; the\n    upper words are cleared.  */\n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sad_pu8 (__m64 __A, __m64 __B)\n {\n   return (__m64) __builtin_ia32_psadbw ((__v8qi)__A, (__v8qi)__B);\n }\n \n-static __inline __m64 __attribute__((__always_inline__, __artificial__))\n+extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _m_psadbw (__m64 __A, __m64 __B)\n {\n   return _mm_sad_pu8 (__A, __B);\n@@ -1192,7 +1192,7 @@ _m_psadbw (__m64 __A, __m64 __B)\n /* Loads one cache line from address P to a location \"closer\" to the\n    processor.  The selector I specifies the type of prefetch operation.  */\n #ifdef __OPTIMIZE__\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_prefetch (const void *__P, enum _mm_hint __I)\n {\n   __builtin_prefetch (__P, 0, __I);\n@@ -1203,22 +1203,22 @@ _mm_prefetch (const void *__P, enum _mm_hint __I)\n #endif\n \n /* Stores the data in A to the address P without polluting the caches.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_stream_pi (__m64 *__P, __m64 __A)\n {\n   __builtin_ia32_movntq ((unsigned long long *)__P, (unsigned long long)__A);\n }\n \n /* Likewise.  The address must be 16-byte aligned.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_stream_ps (float *__P, __m128 __A)\n {\n   __builtin_ia32_movntps (__P, (__v4sf)__A);\n }\n \n /* Guarantees that every preceding store is globally visible before\n    any subsequent store.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_sfence (void)\n {\n   __builtin_ia32_sfence ();\n@@ -1227,7 +1227,7 @@ _mm_sfence (void)\n /* The execution of the next instruction is delayed by an implementation\n    specific amount of time.  The instruction does not modify the\n    architectural state.  */\n-static __inline void __attribute__((__always_inline__, __artificial__))\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n _mm_pause (void)\n {\n   __asm__ __volatile__ (\"rep; nop\" : : );"}, {"sha": "dbd8e8bfca3fbc0e3484f3f776802a3c96952006", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 14, "deletions": 0, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -1,3 +1,17 @@\n+2008-03-13  Uros Bizjak  <ubizjak@gmail.com>\n+\n+\tPR target/34000\n+\tPR target/35553\n+\t* g++.dg/other/i386-3.C: New test.\n+\t* gcc.target/i386/sse-13.c: Redefine extern instead of static.\n+\t* gcc.target/i386/sse-14.c: Ditto.\n+\t* gcc.target/i386/mmx-1.c: Ditto.\n+\t* gcc.target/i386/mmx-2.c: Ditto.\n+\t* gcc.target/i386/3dnow-1.c: Ditto.\n+\t* gcc.target/i386/3dnow-2.c: Ditto.\n+\t* gcc.target/i386/3dnowA-1.c: Ditto.\n+\t* gcc.target/i386/3dnowA-2.c: Ditto.\n+\n 2008-03-13  Paolo Bonzini  <bonzini@gnu.org>\n \n \tPR tree-opt/35422"}, {"sha": "8ebc74e628cf6ab148823fe2eb8b5483515e79ac", "filename": "gcc/testsuite/g++.dg/other/i386-3.C", "status": "added", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fg%2B%2B.dg%2Fother%2Fi386-3.C", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fg%2B%2B.dg%2Fother%2Fi386-3.C", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fg%2B%2B.dg%2Fother%2Fi386-3.C?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -0,0 +1,8 @@\n+/* Test that {,x,e,p,t,s,a,b}mmintrin.h, mm3dnow.h and mm_malloc.h are\n+   usable with -O -fkeep-inline-functions.  */\n+/* { dg-do compile { target i?86-*-* x86_64-*-* } } */\n+/* { dg-options \"-O -fkeep-inline-functions -march=k8 -m3dnow -msse4 -msse5\" } */\n+\n+#include <bmmintrin.h>\n+#include <smmintrin.h>\n+#include <mm3dnow.h>"}, {"sha": "de5a2c3f9357393fd70e9ceba3afe1900ea9ebca", "filename": "gcc/testsuite/gcc.target/i386/3dnow-1.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2F3dnow-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2F3dnow-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2F3dnow-1.c?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -3,10 +3,10 @@\n \n /* Test that the intrinsics compile with optimization.  All of them are\n    defined as inline functions in mmintrin.h that reference the proper\n-   builtin functions.  Defining away \"static\" and \"__inline\" results in\n+   builtin functions.  Defining away \"extern\" and \"__inline\" results in\n    all of them being compiled as proper functions.  */\n \n-#define static\n+#define extern\n #define __inline\n \n #include <mm3dnow.h>"}, {"sha": "4b4d7472265d9cbfabe6e8cecf667b4300fa6418", "filename": "gcc/testsuite/gcc.target/i386/3dnow-2.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2F3dnow-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2F3dnow-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2F3dnow-2.c?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -3,10 +3,10 @@\n \n /* Test that the intrinsics compile without optimization.  All of them are\n    defined as inline functions in mmintrin.h that reference the proper\n-   builtin functions.  Defining away \"static\" and \"__inline\" results in\n+   builtin functions.  Defining away \"extern\" and \"__inline\" results in\n    all of them being compiled as proper functions.  */\n \n-#define static\n+#define extern\n #define __inline\n \n #include <mm3dnow.h>"}, {"sha": "6d4f32532bcff0c376d6bfd4accc9e41a9b67c2a", "filename": "gcc/testsuite/gcc.target/i386/3dnowA-1.c", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2F3dnowA-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2F3dnowA-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2F3dnowA-1.c?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -1,13 +1,12 @@\n /* { dg-do assemble } */\n-/* { dg-require-effective-target ilp32 } */\n-/* { dg-options \"-O2 -Werror-implicit-function-declaration -m3dnow -march=athlon\" } */\n+/* { dg-options \"-O2 -Werror-implicit-function-declaration -march=k8 -m3dnow\" } */\n \n /* Test that the intrinsics compile with optimization.  All of them are\n    defined as inline functions in mmintrin.h that reference the proper\n-   builtin functions.  Defining away \"static\" and \"__inline\" results in\n+   builtin functions.  Defining away \"extern\" and \"__inline\" results in\n    all of them being compiled as proper functions.  */\n \n-#define static\n+#define extern\n #define __inline\n \n #include <mm3dnow.h>"}, {"sha": "0a30d61c9e6f764542276879ffa149d9045a4e68", "filename": "gcc/testsuite/gcc.target/i386/3dnowA-2.c", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2F3dnowA-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2F3dnowA-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2F3dnowA-2.c?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -1,13 +1,12 @@\n /* { dg-do assemble } */\n-/* { dg-require-effective-target ilp32 } */\n-/* { dg-options \"-O0 -Werror-implicit-function-declaration -m3dnow -march=athlon\" } */\n+/* { dg-options \"-O0 -Werror-implicit-function-declaration -march=k8 -m3dnow\" } */\n \n /* Test that the intrinsics compile without optimization.  All of them are\n    defined as inline functions in mmintrin.h that reference the proper\n-   builtin functions.  Defining away \"static\" and \"__inline\" results in\n+   builtin functions.  Defining away \"extern\" and \"__inline\" results in\n    all of them being compiled as proper functions.  */\n \n-#define static\n+#define extern\n #define __inline\n \n #include <mm3dnow.h>"}, {"sha": "0b31a531a41153f7efe42143991de2183a1cf2d2", "filename": "gcc/testsuite/gcc.target/i386/mmx-1.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fmmx-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fmmx-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fmmx-1.c?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -3,10 +3,10 @@\n \n /* Test that the intrinsics compile with optimization.  All of them are\n    defined as inline functions in mmintrin.h that reference the proper\n-   builtin functions.  Defining away \"static\" and \"__inline\" results in\n+   builtin functions.  Defining away \"extern\" and \"__inline\" results in\n    all of them being compiled as proper functions.  */\n \n-#define static\n+#define extern\n #define __inline\n \n #include <mmintrin.h>"}, {"sha": "d15ceb18513764b614a803050f423dec281906ea", "filename": "gcc/testsuite/gcc.target/i386/mmx-2.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fmmx-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fmmx-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fmmx-2.c?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -3,10 +3,10 @@\n \n /* Test that the intrinsics compile without optimization.  All of them are\n    defined as inline functions in mmintrin.h that reference the proper\n-   builtin functions.  Defining away \"static\" and \"__inline\" results in\n+   builtin functions.  Defining away \"extern\" and \"__inline\" results in\n    all of them being compiled as proper functions.  */\n \n-#define static\n+#define extern\n #define __inline\n \n #include <mmintrin.h>"}, {"sha": "baf5f16396036ef66d5e7c79c5ed30f6e4562f9e", "filename": "gcc/testsuite/gcc.target/i386/sse-13.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fsse-13.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fsse-13.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fsse-13.c?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -3,10 +3,10 @@\n \n /* Test that the intrinsics compile with optimization.  All of them are\n    defined as inline functions in {,x,e,p,t,s,a,b}mmintrin.h and mm3dnow.h\n-   that reference the proper builtin functions.  Defining away \"static\" and\n+   that reference the proper builtin functions.  Defining away \"extern\" and\n    \"__inline\" results in all of them being compiled as proper functions.  */\n \n-#define static\n+#define extern\n #define __inline\n \n /* Following intrinsics require immediate arguments. */"}, {"sha": "708313dab846a66ace5aa8cf3c6143f81afa2588", "filename": "gcc/testsuite/gcc.target/i386/sse-14.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fsse-14.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/1359ef39755c2a648c44e410e974450d91a4bd99/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fsse-14.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fsse-14.c?ref=1359ef39755c2a648c44e410e974450d91a4bd99", "patch": "@@ -3,10 +3,10 @@\n \n /* Test that the intrinsics compile without optimization.  All of them are\n    defined as inline functions in {,x,e,p,t,s,a,b}mmintrin.h  and mm3dnow.h\n-   that reference the proper builtin functions.  Defining away \"static\" and\n+   that reference the proper builtin functions.  Defining away \"extern\" and\n    \"__inline\" results in all of them being compiled as proper functions.  */\n \n-#define static\n+#define extern\n #define __inline\n \n #include <bmmintrin.h>"}]}