{"sha": "f163907e0afadfa09f23320c51b90927c771c106", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZjE2MzkwN2UwYWZhZGZhMDlmMjMzMjBjNTFiOTA5MjdjNzcxYzEwNg==", "commit": {"author": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2017-05-10T17:26:09Z"}, "committer": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2017-05-10T17:26:09Z"}, "message": "Big merge of changes to gofrontend repo that were postponed due to the\nGCC release freeze.\n\n\t* go-backend.c: Include \"go-c.h\".\n\t* go-gcc.cc (Gcc_backend::write_export_data): New method.\n\n\t* go-gcc.cc (Gcc_backend::Gcc_backend): Declare\n\t__builtin_prefetch.\n\t* Make-lang.in (GO_OBJS): Add go/wb.o.\n\ncommit 884c9f2cafb3fc1decaca70f1817ae269e4c6889\nAuthor: Than McIntosh <thanm@google.com>\nDate:   Mon Jan 23 15:07:07 2017 -0500\n\n    compiler: insert additional conversion for type desc ptr expr\n    \n    Change the method Type::type_descriptor_pointer to apply an additional\n    type conversion to its result Bexpression, to avoid type clashes in\n    the back end. The backend expression for a given type descriptor var\n    is given a type of \"_type\", however the virtual calls that create the\n    variable use types derived from _type, hence the need to force a\n    conversion.\n    \n    Reviewed-on: https://go-review.googlesource.com/35506\n\n\ncommit 5f0647c71e3b29eddcd0eecc44e7ba44ae7fc8dd\nAuthor: Than McIntosh <thanm@google.com>\nDate:   Mon Jan 23 15:22:26 2017 -0500\n\n    compiler: insure tree integrity in Call_expression::set_result\n    \n    Depending on the back end, it can be problematic to reuse Bexpressions\n    (passing the same Bexpression to more than one Backend call to create\n    additional Bexpressions or Bstatements). The Call_expression::set_result\n    method was reusing its Bexpression input in more than one tree\n    context; the fix is to pass in an Expression instead and generate\n    multiple Bexpression references to it within the method.\n    \n    Reviewed-on: https://go-review.googlesource.com/35505\n\n\ncommit 7a8e49870885af898c3c790275e513d1764a2828\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Tue Jan 24 21:19:06 2017 -0800\n\n    runtime: copy more of the scheduler from the Go 1.8 runtime\n    \n    Copies mstart, newm, m0, g0, and friends.\n    \n    Reviewed-on: https://go-review.googlesource.com/35645\n\n\ncommit 3546e2f002d0277d805ec59c5403bc1d4eda4ed9\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Thu Jan 26 19:47:37 2017 -0800\n\n    runtime: remove a few C functions that are no longer used\n    \n    Reviewed-on: https://go-review.googlesource.com/35849\n\n\ncommit a71b835254f6d3164a0e6beaf54f2b175d1a6a92\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Thu Jan 26 16:51:16 2017 -0800\n\n    runtime: copy over more of the Go 1.8 scheduler\n    \n    In particular __go_go (aka newproc) and goexit[01].\n    \n    Reviewed-on: https://go-review.googlesource.com/35847\n\n\ncommit c3ffff725adbe54d8283c373b6aa7dc95d6fc27f\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Fri Jan 27 16:58:20 2017 -0800\n\n    runtime: copy syscall handling from Go 1.8 runtime\n    \n    Entering a syscall still has to start in C, to save the registers.\n    Fix entersyscallblock to save them more reliably.\n    \n    This copies over the tracing code for syscalls, which we previously\n    weren't doing, and lets us turn on runtime/trace/check.\n    \n    Reviewed-on: https://go-review.googlesource.com/35912\n\n\ncommit d5b921de4a28b04000fc4c8dac7f529a4a624dfc\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Fri Jan 27 18:34:11 2017 -0800\n\n    runtime: copy SIGPROF handling from Go 1.8 runtime\n    \n    Also copy over Breakpoint.\n    \n    Fix Func.Name and Func.Entry to not crash on a nil Func.\n    \n    Reviewed-on: https://go-review.googlesource.com/35913\n\n\ncommit cc60235e55aef14b15c3d2114030245beb3adfef\nAuthor: Than McIntosh <thanm@google.com>\nDate:   Mon Feb 6 11:12:12 2017 -0500\n\n    compiler: convert go_write_export_data to Backend method.\n    \n    Convert the helper function 'go_write_export_data' into a Backend\n    class method, to allow for an implementation of this function that\n    needs to access backend state.\n    \n    Reviewed-on: https://go-review.googlesource.com/36357\n\n\ncommit e387439bfd24d5e142874b8e68e7039f74c744d7\nAuthor: Than McIntosh <thanm@google.com>\nDate:   Wed Feb 8 11:13:46 2017 -0500\n\n    compiler: insert backend conversion in temporary statement init\n    \n    Insert an additional type conversion in Temporary_statement::do_get_backend\n    when assigning a Bexpression initializer to the temporary variable, to\n    avoid potential clashes in the back end. This can come up when assigning\n    something of concrete pointer-to-function type to a variable of generic\n    pointer-to-function type.\n    \n    Reviewed-on: https://go-review.googlesource.com/36591\n\n\ncommit c5acf0ce09e61ff623847a35a99da465b8571609\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Wed Mar 1 17:57:53 2017 +0100\n\n    libgo: build tags for aix\n    \n    Build tags for the libgo source files required to build\n    libgo on AIX.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/37633\n\n\ncommit 67ed19616898ea18a101ec9325b82d028cd395d9\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Thu Mar 2 15:41:31 2017 +0100\n\n    libgo: handle AIX tag in match.sh and gotest\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/37638\n\n\ncommit 83ea2d694c10b2dd83fc8620c43da13d20db754e\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Wed Mar 1 17:48:16 2017 +0100\n\n    libgo: add AIX support in configure and Makefile\n    \n    - support for GOOS=aix\n    - CFLAGS/GOCFLAGS/LDFLAGS for AIX\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/37632\n\n\ncommit 35d577fe22ffa16a3ccaadf5dae9f6f425c8ec8c\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Mon Mar 6 15:00:15 2017 +0100\n\n    runtime: adapt memory management to AIX mmap\n    \n    On AIX:\n    * mmap does not allow to map an already mapped range,\n    * mmap range start at 0x30000000 for 32 bits processes,\n    * mmap range start at 0x70000000_00000000 for 64 bits processes\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/37845\n\n\ncommit 4e49e56a5fd4072b4ca7fcefe4158d6885d9ee62\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Mon Mar 6 13:42:26 2017 +0100\n\n    runtime: add getproccount implementation for AIX\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/37844\n\n\ncommit ff626470294237ac664127894826614edc46a3d0\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Mon Mar 6 17:31:21 2017 +0100\n\n    runtime: handle ERESTART errno with AIX's wait4\n    \n    On AIX, wait4 may return with errno set to ERESTART, which causes unexepected\n    behavior (for instance, go build may exit with the message \"wait: restart\n    system call\" after running a command, even if it was successfull).\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/37846\n\n\ncommit 37daabbfc83d533b826ef9ab10e2dee7406e7198\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Mon Mar 6 11:02:58 2017 +0100\n\n    runtime: support for AIX's procfs tree\n    \n    On AIX, the process executable file is available under /proc/<pid>/object/a.out\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/37842\n\n\ncommit a0275c039d56acf4bf48151978c1a4ec5758cc2c\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Mar 8 07:00:05 2017 -0800\n\n    libgo/Makefile.am: don't use nonportable \\n or \\t in sed expression\n    \n    The resulting zstdpktlist.go is less pretty, but it works.\n    \n    Reviewed-on: https://go-review.googlesource.com/37940\n\n\ncommit 29b190f76105aafa2b50b48249afdafecc97a4be\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Thu Mar 9 16:02:34 2017 +0100\n\n    runtime: netpoll and semaphores for AIX\n    \n    semaphore implementation based on Solaris implementation in\n    libgo/go/runtime/os_solaris.go\n    \n    netpoll is just a stub to avoid build failure on AIX.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/37966\n\n\ncommit 55ca6d3f3cddf0ff9ccb074b2694da9fc54de7ec\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Thu Mar 9 15:38:30 2017 +0100\n\n    libmain: ensure initfn is called when loading a go library\n    \n    AIX does not support .init_array.\n    The alterative is to export the __go_init function and tell the linker\n    it is an init function with the -Wl,-binitfini:__go_init option.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/37965\n\n\ncommit 349a30d17d880ac8bc1a35e1a2ffee6d6e870ae9\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Fri Mar 10 11:15:08 2017 +0100\n\n    libgo: use an import list for missing symbols\n    \n    libgo depends on symbols provided by Go programs at runtime. On AIX,\n    this requires either to build libgo with -Wl,-berok linker option and\n    the programs with -Wl,-brtl, or to provide a list of imported symbols\n    when building libgo. The second options seems preferable, to avoid\n    requiring an additional option for every Go program.\n    \n    There are also some symbols that are specific to GNU ld and do not\n    exist when linking with AIX ld (__data_start, __edata, __etext and\n    __bss_start).\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/37969\n\n\ncommit 91db0ea1ff068ca1d97b9c99612100ea5b96ddb2\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Wed Mar 8 15:34:45 2017 +0100\n\n    crypto/x509: add certificate files locations for AIX\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/37952\n\n\ncommit 92e521c854e91709b949548c47e267377850f26a\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Fri Mar 10 14:10:11 2017 -0800\n\n    compiler: fix check for pointer in Temporary_reference_expression\n    \n    The check for an unrepresentable pointer in\n    Temporary_reference_expression::do_get_backend was incorrectly\n    translated from C to Go in https://golang.org/cl/14346043.  Fix the\n    check to use points_to rather than has_pointer and deref.  This should\n    not make any difference in practice as either way the condition will\n    only be true for a pointer to void, but points_to is correct and more\n    efficient.\n    \n    Reviewed-on: https://go-review.googlesource.com/38009\n\n\ncommit 9a0b676e59e7171a630c48fdc3d4de6712bad0ca\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Thu Mar 16 16:51:53 2017 +0100\n\n    libgo: add missing _arpcom struct to *sysinfo.go\n    \n    This struct is filtered due to having a field of type _in6_addr,\n    but other types exported to *sysinfo.go are depending on it.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38251\n\n\ncommit 61262a757bdd3d9a595ab6a90f68c0c4ebed7bc1\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Thu Mar 16 18:27:46 2017 +0100\n\n    syscall: raw_ptrace stub for AIX\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38253\n\n\ncommit 8029632b50880fd9b5e39299c738b38e3386595f\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Wed Mar 15 16:58:37 2017 +0100\n\n    libgo: adapt runtime.inc to AIX\n    \n    * Two AIX types are wrongfully exported to runtime.inc as their names\n      make them look like a Go type.\n    * The sigset go type conflicts with a system sigset type.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38192\n\n\ncommit 25f3a90d14bc268479369ecc0eada72791612f86\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Wed Mar 15 16:58:37 2017 +0100\n\n    libgo: update Makefile.in, accidentally omitted from last change\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38310\n\n\ncommit d52b4895616b66f93b460366527e74336829aaa5\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Thu Mar 16 18:39:26 2017 +0100\n\n    syscall: TIOCSCTTY does not exist on AIX\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38254\n\n\ncommit ff1ec3847a4472008e5d53a98b6694b1e54ca322\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Thu Mar 16 18:07:34 2017 +0100\n\n    syscall: syscall does not exist on AIX\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38252\n\n\ncommit c1ee60dabf0b243a0b0286215481a5d326c34596\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Fri Mar 17 17:18:18 2017 +0100\n\n    net: EAI_OVERFLOW does not exist on AIX\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38266\n\n\ncommit ad4ad29aed9f70b14b39b488bfeb9ee745382ec4\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Fri Mar 17 17:23:56 2017 +0100\n\n    net: sockopt/sockoptip stubs for AIX\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38267\n\n\ncommit 5d7db2d7542fe7082f426d42f8c2ce14aad6df55\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Fri Mar 17 16:35:05 2017 +0100\n\n    os/user: add listgroups stub for AIX\n    \n    This is required to build os/user.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38263\n\n\ncommit 4e57a7973e9fa4cb5ab977c6d792e62a8f7c5795\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Wed Mar 22 11:11:30 2017 +0100\n\n    os: fix readdirnames for AIX\n    \n    Largefile implementation should be used on AIX.\n    \n    readdir64_r function returns 9 and sets result to NULL when\n    reaching end of directory, so this return code should not\n    always be considered as an error.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38359\n\n\ncommit b34036967d1ec57b25e3debe077439b4210a1d4a\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Fri Mar 17 17:39:31 2017 +0100\n\n    libgo: adapt sigtab.go to AIX\n    \n    On AIX, _NSIG is not directly defined to its integer value in\n    gen-sysinfo.go.\n    The real value is _SIGMAX32+1 or _SIGMAX64+1, depending if we are\n    building a 32bit ligbo or a 64bit libgo, so we need to read one of\n    those constants to set nsig value in mksigtab.sh\n    \n    This change also ensures that all signal numbers from 0 to nsig-1\n    are referenced in sigtable.\n    \n    Reviewed-on: https://go-review.googlesource.com/38268\n\n\ncommit 20991c32671a183ec859b4f285df37fdd4634247\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Thu Mar 23 17:28:09 2017 +0100\n\n    syscall: missing import in socket_bsd.go\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38369\n\n\ncommit c34754bd9adf5496c4c26257eaa50793553c11e8\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Wed Mar 22 17:57:01 2017 +0100\n\n    sycall: WCOREDUMP macro is not defined on AIX\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38363\n\n\ncommit 4f38813482227b12ea0ac6ac1b981ff9ef9853ef\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Thu Mar 23 17:44:43 2017 +0100\n\n    libgo: additional build tags for AIX\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38510\n\n\ncommit d117ede6ff5a7083e9c40eba28a0f94f3535d773\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Thu Mar 23 17:48:46 2017 +0100\n\n    go/build: add AIX to \"go build\" command known OS\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38511\n\n\ncommit 7b0ddaa6a6a71f9eb1c374122d29775b13c2cac5\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Thu Mar 23 09:57:01 2017 -0700\n\n    compiler: don't crash if imported package imports this one\n    \n    When building a test it's OK if test code imports a package that\n    imports this one. The go tool is supposed to catch cases where this\n    creates an impossible initialization order. The compiler already has\n    code to permit this in Gogo::add_import_init_fn. This CL avoids a\n    compiler crash on a similar case when writing out the export data.\n    \n    I have no test case for this. Basically it pushes a compiler crash\n    into an error reported elsewhere.\n    \n    Problem was reported by Tony Reix.\n    \n    Reviewed-on: https://go-review.googlesource.com/38462\n\n\ncommit 925636975d075e3e3353823b09db3f933f23cb03\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Mar 29 14:14:18 2017 -0700\n\n    runtime: copy finalizer support from Go 1.8 runtime\n    \n    Reviewed-on: https://go-review.googlesource.com/38794\n\n\ncommit 1ccb22b96cb3b1011db0e427877d9ddecb577fa9\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Thu Mar 30 15:21:06 2017 +0200\n\n    runtime: initcontext and setcontext stubs for AIX\n    \n    Further investigations are required to understand the clobbering\n    issue and implement a proper fix. Until then, those stubs are\n    required to allow the build to complete.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38930\n\n\ncommit 27db481f369b54256063c72b911d22390c59199c\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Wed Mar 29 18:07:25 2017 +0200\n\n    os: fix Readlink failure on AIX\n    \n    AIX readlink routine returns an error if the link is longer\n    than the buffer, instead of truncating the link.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38700\n\n\ncommit c93babbf48eddd0bc34d4179ffb302dc60087299\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Wed Mar 29 17:26:35 2017 +0200\n\n    compiler: implement support for reading AIX big archives\n    \n    This is required to read go export from a Go library.\n    \n    Code courtesy of Damien Bergamini from Atos Infog\u00e9rance.\n    \n    Issue golang/go#19200\n    Reviewed-on: https://go-review.googlesource.com/38698\n\n\ncommit 930dd53482bdee3a9074850d168d0b9d7819c135\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Thu Apr 6 18:50:11 2017 -0700\n\n    compiler: fix whether conversions are static initializers\n    \n    The compiler was incorrectly treating type conversions from string to\n    int or vice-versa as static initializers.  That doesn't work, as those\n    conversions are implemented via a function call.\n    \n    This case may never actually arise but it seems like the right thing to do.\n    \n    Reviewed-on: https://go-review.googlesource.com/39872\n\n\ncommit f02691e4195728dbf06f4dde0853c6bccc922183\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Thu Apr 6 17:24:08 2017 -0700\n\n    compiler, runtime: don't let slices point past end of memory block\n    \n    When Go code uses a slice expression like [:len(str)] or [:cap(slice)],\n    it's natural for the resulting pointer to point just past the end of\n    the memory block.  If the next memory block is not live, we now have a\n    live pointer to a dead block, which will unnecessarily keep the block\n    alive.  That wastes space, and with the new Go 1.8 GC (not yet\n    committed) will trigger an error when using GODEBUG=gccheckmark=1.\n    \n    This changes the implementation of slice expressions to not move the\n    pointer if the resulting string length or slice capacity is 0.  When\n    the length/capacity is zero, the pointer is never used anyhow.\n    \n    Reviewed-on: https://go-review.googlesource.com/39870\n\n\ncommit 17527c35b027e1afcc318faf5563909e1e9d44a6\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Thu Apr 6 15:30:11 2017 -0700\n\n    compiler: emit write barriers\n    \n    The Go 1.8 concurrent GC requires optional write barriers for all\n    assignments that may change pointer values in the heap or in a global\n    variable.  For details see https://blog.golang.org/go15gc.\n    \n    This changes the gofrontend code to emit write barriers as needed.\n    This is in preparation for future changes.  At the moment the write\n    barriers will do nothing.  They test runtime.writeBarrier.enabled,\n    which will never be non-zero.  They call simple functions which just\n    do a move without doing any of the other operations required by the\n    write barrier.\n    \n    Reviewed-on: https://go-review.googlesource.com/39852\n\n\ncommit c0b00f072bf34b2c288e1271ec8118b88c4f6f6f\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Tue Apr 11 17:47:29 2017 +0200\n\n    libgo: allow building gox files from PIC objects\n    \n    libtool builds non-PIC objects in the same directory as .lo files\n    and PIC objects in a .libs subdirectory.\n    BUILDGOX rule uses the non-PIC objects to build the gox files,\n    but on AIX only the PIC objects are built.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/40355\n\n\ncommit ea0f3da174c5503a209043f14ddda34871cfec52\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Thu Apr 6 19:06:14 2017 -0700\n\n    compiler: add code to generate a ptrmask for a type\n    \n    The Go 1.8 garbage collector uses a ptrmask for all types below a\n    certain size.  A ptrmask is simply a bit vector with a single bit for\n    each pointer-sized word in the value.  The bit is 1 if the type has a\n    pointer in that position, 0 if it does not.\n    \n    This change adds code to the compiler to generate a ptrmask.  The code\n    is not used by anything yet, it is just compiled.  It will be used\n    when we switch over to the Go 1.8 garbage collector.\n    \n    The new Array_type::int_length method, and the new memory_size\n    methods, will also be used by other patches coming later.\n    \n    Reviewed-on: https://go-review.googlesource.com/39873\n\n\ncommit 3029e1df3be3614d196a03c15e50e68ff850aa4c\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Fri Apr 7 10:31:39 2017 -0700\n\n    compiler: add code to generate a gcprog for a type\n    \n    The Go 1.8 garbage collector uses a gcprog for all types above a\n    certain size.  A gcprog describes where the pointers are in the type,\n    using a simple bytecode machine that supports repeating bits.  The\n    effect is to permit using much less space to describe arrays.  The\n    format is described in runtime/mbitmap.go in the docs for runGCProg.\n    This is not yet added to the gofrontend, but can be seen in the gc sources.\n    \n    This change adds code to the compiler to generate a gcprog.  The code\n    is not used by anything yet, it is just compiled.  It will be used\n    when we switch over to the Go 1.8 garbage collector.\n    \n    Reviewed-on: https://go-review.googlesource.com/39923\n\n\ncommit 8b01ef1e9176d20f4c9e667972fe031069a4d057\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Thu Apr 13 07:00:35 2017 -0700\n\n    compiler: add ptrdata computations and expressions\n    \n    For the upcoming Go 1.8 GC we need to compute the \"ptrdata\" of a type:\n    the number of bytes in the type that can contain pointers.  For types\n    that do not contain pointers this number is zero.  For many types it\n    is a number larger than zero but smaller than the total size of the\n    type.  The Go 1.8 GC uses this number to make loops looking for\n    pointers run faster by not scanning the suffix of a value that can not\n    contain a pointer.\n    \n    Unfortunately there are two subtly different definitions of ptrdata,\n    and we need both.  The first is the simple one: the prefix that can\n    contain pointers.  The second is the number of bytes described by the\n    gcprog for the type.  Recall that we describe the exact position of\n    pointers in a type using either a ptrmask or a gcprog.  The ptrmask is\n    simpler, the gcprog uses less space.  We use the gcprog for large\n    types, currently defined as types that are more than 2048 bytes.  When\n    the Go 1.8 runtime expands a gcprog, it verifies that the gcprog\n    describes exactly the same number of bytes as the ptrdata field in the\n    type descriptor.  If the last pointer-containing portion of a type is\n    an array, and if the elements of the array have a ptrdata that is less\n    than the size of the element type, then the simple definition of the\n    ptrdata will not include the final non-pointer-containing bytes of the\n    last element of the array.  However, the gcprog will define the array\n    using a repeat count, and will therefore include the full size of the\n    last element of the array.  So for a type that needs a gcprog, the\n    ptrdata field in the type descriptor must be the size of the data\n    described by the gcprog, and that is not necessarily the same as the\n    simple ptrdata.\n    \n    It might seem that we can always use the gcprog version of the ptrdata\n    calculation, since that is what will appear in a type descriptor, but\n    it turns out that for global variables we always use a ptrmask, not a\n    gcprog, even if the global variable is large.  This is because gcprogs\n    are handled by expanding them into a ptrmask at runtime, and for a\n    global variable there is no natural place to put the ptrmask.  Simpler\n    to always use the ptrmask.  That means that we need to describe the\n    size of the ptrmask, and that means that we need an expression for the\n    simple form of the ptrdata.\n    \n    This CL implements the ptrdata calculation.  This code is not actually\n    used yet.  It will be used later when the Go 1.8 GC is committed.\n    \n    Reviewed-on: https://go-review.googlesource.com/40573\n\n\ncommit 7a37331303b572412179a08141f1dd35339d40c8\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Fri Apr 14 06:55:48 2017 -0700\n\n    compiler: zero length arrays never contain pointers\n    \n    Reviewed-on: https://go-review.googlesource.com/40696\n\n\ncommit c242f0508a64d3d74a28d498cbaeda785ff76258\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Fri Apr 14 07:26:54 2017 -0700\n\n    bytes: disable allocations test on gccgo\n    \n    It turns out that testing.AllocsPerRun has not been producing correct\n    results with the current gccgo memory allocator.  When we update to\n    the Go 1.8 memory allocator, testing.AllocsPerRun will work again, and\n    this test will fail due to lack of escape analysis.\n    \n    Reviewed-on: https://go-review.googlesource.com/40700\n\n\ncommit 0dc369f1d63376a36bfb0999a1b0377fd444bfab\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Tue Apr 11 16:22:38 2017 +0200\n\n    os: alternative way to find executable path, using Args[0]\n    \n    AIX does not provide a proper way to find the original\n    executable path from procfs, which contains just an\n    hardlink.\n    Executable path can be found using Args[0], Getcwd and\n    $PATH.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/40353\n\n\ncommit f9bad1342569b338e3b2ea9f12ffc6d3d3fa3028\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Fri Apr 14 08:01:19 2017 -0700\n\n    compiler: don't write struct with multiple sink fields to C header file\n    \n    When writing a struct to the C header file used by the C runtime code,\n    a single sink field is fine: it will be called \"_\", which is valid C.\n    There are structs with single sink fields that we want to write out,\n    such as finblock.  As it happens, though, the Go 1.8 runtime has a\n    struct with two sink fields, gcControllerState, which will produce a C\n    definition with two fields named \"_\", which will fail.  Since we don't\n    need to know that struct in C, rather than fix the general case, just\n    punt if the struct has multiple sink fields.\n    \n    After the conversion to the Go 1.8 GC, we may be able to get rid of\n    the C header file anyhow.  I'm not sure yet.\n    \n    Reviewed-on: https://go-review.googlesource.com/40701\n\n\ncommit cfc28901a572aeb15b2f10a38f79eec04c64dfb2\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Fri Apr 14 10:07:23 2017 -0700\n\n    runtime: disable allocations test on gccgo\n    \n    It turns out that testing.AllocsPerRun has not been producing correct\n    results with the current gccgo memory allocator.  When we update to\n    the Go 1.8 memory allocator, testing.AllocsPerRun will work again, and\n    these tests will fail due to lack of escape analysis.\n    \n    Reviewed-on: https://go-review.googlesource.com/40703\n\n\ncommit 36fedd76edaa48b9ec09709a70d9e4abaddf0caf\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Fri Apr 14 10:47:06 2017 -0700\n\n    runtime: remove unused size argument from hash/equal fns\n    \n    The size argument was removed from hash and equal functions in CL\n    34983.  Somehow I missed removing them from three of the predefined\n    functions.\n    \n    Reviewed-on: https://go-review.googlesource.com/40770\n\n\ncommit 90f6accb48d2e78cad8955b9292933f6ce3fe4c8\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Fri Apr 14 13:23:05 2017 -0700\n\n    runtime: remove unused stack.go\n    \n    We're never going to use stack.go for gccgo.  Although a build tag\n    keeps it from being built, even having it around can be confusing.\n    Remove it.\n    \n    Reviewed-on: https://go-review.googlesource.com/40774\n\n\ncommit befa71603fc66a214e01ac219f2bba36e19f136f\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Fri Apr 14 13:18:34 2017 -0700\n\n    runtime: build fastlog\n    \n    Take out the build tags which were preventing fastlog2 from being\n    built.  It's used by the upcoming Go 1.8 GC.\n    \n    Reviewed-on: https://go-review.googlesource.com/40773\n\n\ncommit b7e19e9be4ab4c3cd8f4c9506d79a8cd56bace40\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Fri Apr 14 10:04:23 2017 -0700\n\n    runtime: add tests from Go 1.8\n    \n    Some runtime package tests never made it into the gofrontend repo for\n    some reason.  Add them now.\n    Reviewed-on: https://go-review.googlesource.com/40869\n\n\ncommit 1feef185aebd71bc2a09b9a04287461806096610\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Mon Apr 17 16:26:11 2017 -0700\n\n    runtime: change mcall to take a Go function value\n    \n    For future work in bringing in the Go 1.8 GC, change the mcall\n    function to take a Go function value, which means that mcall can take\n    a closure rather than just a straight C function pointer.\n    \n    As part of this change move kickoff from C to Go, which we want to do\n    anyhow so that we run the write barriers that it generates.\n    \n    Reviewed-on: https://go-review.googlesource.com/40935\n\n\ncommit c3db34f4efc2d610f74a01dd2ad7775f48889b29\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Tue Apr 11 16:11:26 2017 +0200\n\n    runtime: netpoll implementation for AIX\n    \n    Code courtesy of Damien Bergamini from Atos Infog\u00e9rance.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/40352\n\n\ncommit f5634dff40e53ad9ce61afd67fd07334e3af9d1f\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Tue Apr 18 22:06:07 2017 -0700\n\n    runtime: move mstart from Go to C\n    \n    The assignments done in mstart must be done without write barriers, as\n    mstart is running without an m or p.  In the gc toolchain the\n    equivalent code to intialize g and g->m is written in assembler;\n    on GNU/Linux, it's in the clone function.\n    \n    Reviewed-on: https://go-review.googlesource.com/40989\n\n\ncommit 671d7c74592f4b6fe3665af279482ba0ea47ca2d\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Tue Apr 18 17:47:28 2017 -0700\n\n    compiler: varargs slices do not escape in runtime\n    \n    Also, don't try to allocate an empty slice on the stack, as it will\n    confuse the GCC backend.\n    \n    Also add a few trivial style, code formatting, and debug output fixes.\n    \n    Updates golang/go#17431\n    \n    Reviewed-on: https://go-review.googlesource.com/40983\n\n\ncommit 94699d25f31353bf03419eda56b15993a39f3275\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Tue Apr 18 17:30:09 2017 -0700\n\n    compiler: add Ptrmask_symbol_expression\n    \n    Add an expression to evaluate to the ptrmask for a type.  This will be\n    used for global variables, which always use a ptrmask no matter how\n    large they are.\n    \n    Reviewed-on: https://go-review.googlesource.com/40981\n\n\ncommit bfff1654eac5b9288fa6c431e66cba8c9da6a660\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Mon Apr 17 10:51:16 2017 -0700\n\n    runtime: change g's in systemstack\n    \n    The systemstack function in the gc toolchain changes to a different g.\n    This is often used to get more stack space; the gofrontend uses a\n    different stack growth mechanism that does not require changing g's,\n    so we've been running with a version of systemstack that keeps the\n    same g.  However, the garbage collector has various tests to verify\n    that it is running on g0 rather than on a normal g.  For simplicity,\n    change the gofrontend version of systemstack to change to a different\n    g just as the gc toolchain does.\n    \n    This permits us to uncomment some sanity checks in notetsleep.\n    Doing that requires us to fix up a couple of places where C code calls\n    {start,stop}TheWorldWithSema while not on g0.\n    \n    Note that this does slow down some code in the runtime package unnecessarily.\n    It may be useful to find some places where the runtime calls\n    systemstack only to get more stack space and change it to use some\n    other function.  That other function would act like systemstack in the\n    gc toolchain but simply call the argument in the gofrontend.\n    \n    Reviewed-on: https://go-review.googlesource.com/40973\n\n\ncommit b2ccc7601ce71a7c5732154cf9b2eeea64681469\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Apr 19 10:36:12 2017 -0700\n\n    compiler, runtime: include ptrmask in GC roots\n    \n    Change the list of registered GC roots to include a ptrmask,\n    and change the data structures to be easily used from Go code.\n    The new ptrmask will be used by the Go 1.8 GC to only scan pointers.\n    Tweak the current GC to use the new structures, but ignore the new\n    ptrmask information for now.\n    \n    The new GC root data includes the size of the variable.  The size is\n    not currently used, but will be used later by the cgo checking code.\n    \n    Reviewed-on: https://go-review.googlesource.com/41075\n\n\ncommit 9e065149970bc180e4ca83bb99c74d9c4f43b47b\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Apr 19 12:23:16 2017 -0700\n\n    compiler, runtime: don't pass size to __go_new\n    \n    There is no reason to pass the size to __go_new, as the type\n    descriptor includes the size anyhow.  This makes the function\n    correspond to the Go 1.8 function runtime.newobject, which is what we\n    will use when we update to the Go 1.8 memory allocator.\n    \n    Reviewed-on: https://go-review.googlesource.com/41080\n\n\ncommit c321de7b738c4a3387c1842919c9305acfa04c57\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Apr 19 13:13:56 2017 -0700\n\n    compiler, runtime, reflect: make type descriptors more like Go 1.8\n    \n    Change the type descriptor structure to be more like the one in the Go\n    1.8 runtime.  Specifically we add the ptrdata field, rename the gc\n    field to gcdata and change the type to *byte, and rearrange a few of\n    the fields.  The structure is still not identical to the Go 1.8\n    structure--we don't use any of the tricks to reduce overall executable\n    size--but it is more similar.\n    \n    For now we don't use the new ptrdata field, and the gcdata field is\n    still the old format rather than the new Go 1.8 ptrmask/gcprog format.\n    \n    Reviewed-on: https://go-review.googlesource.com/41081\n\n\ncommit 7b70c52cddeebea9ebeac003f8c6aad59497e5f0\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Apr 19 14:54:29 2017 -0700\n\n    reflect: make sure to clear unusable hash/equal function\n    \n    Otherwise we wind up copying the one from the prototype, which is wrong.\n    \n    Also rewrite the hash/equal functions to look like the ones in Go 1.8,\n    mainly a matter of changing names and using arrayAt.\n    \n    Reviewed-on: https://go-review.googlesource.com/41133\n\n\ncommit 84d26f467f7de8bdbb0d230458135fe1b6b2a99d\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Apr 19 14:59:13 2017 -0700\n\n    runtime: remove duplicate declarations of SetFinalizer/KeepAlive\n    \n    These should have been removed in CL 38794.  It's a bug that the\n    compiler even permits these duplicate declarations.\n    \n    Reviewed-on: https://go-review.googlesource.com/41134\n\n\ncommit f85ff7e64c24031f6d0bd7c9c426b6176cb95160\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Apr 19 15:56:32 2017 -0700\n\n    runtime: don't crash if panicstring called with no m\n    \n    It's possible for runtime_panicstring to be called with no m if a\n    signal handler, or scheduler innards, do something wrong.  If that\n    happens carry on with the panic rather than crashing.\n    \n    Reviewed-on: https://go-review.googlesource.com/41137\n\n\ncommit 5b362b04f642afb8b20715930416fc3b7d91bb12\nAuthor: Than McIntosh <thanm@google.com>\nDate:   Fri Mar 31 14:35:48 2017 -0400\n\n    compiler: fix for expr sharing introduced by Order_eval::statement.\n    \n    When processing an expression statement with a top-level call\n    that returns multiple results, Order_eval::statement can wind up\n    creating a tree that has multiple references to the same call,\n    which results in a confusing AST dump. Change the implementation\n    to avoid introducing this unwanted sharing.\n    \n    Reviewed-on: https://go-review.googlesource.com/39210\n\n\ncommit b05b4260a68695bf9c9cc29e14ae86ca2699458a\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Apr 19 16:00:28 2017 -0700\n\n    runtime: restore correct m in gtraceback\n    \n    If gtraceback is used to get a stack trace of a g running in the same m,\n    as can happen if we collect a stack trace from a g0, then restore the\n    old m value, don't clear it.\n    \n    Reviewed-on: https://go-review.googlesource.com/41138\n\n\ncommit ca8bbf4dfac19b3f4f7ce21a688b96a418c75031\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Apr 19 16:03:24 2017 -0700\n\n    runtime: set startpc field when starting a new goroutine\n    \n    This puts the right value in a trace--previously it was always zero.\n    \n    Reviewed-on: https://go-review.googlesource.com/41139\n\n\ncommit ca8bbf4dfac19b3f4f7ce21a688b96a418c75031\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Apr 19 16:03:24 2017 -0700\n\n    runtime: set startpc field when starting a new goroutine\n    \n    This puts the right value in a trace--previously it was always zero.\n    \n    Reviewed-on: https://go-review.googlesource.com/41139\n\n\ncommit 887690dce42d7bf8f711f8ea082e4928fb70f2a5\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Apr 19 17:06:11 2017 -0700\n\n    runtime: add prefetch functions\n    \n    The Go 1.8 GC uses prefetch functions.  Add versions for gccgo that\n    call __builtin_prefetch.  Uncomment the test for them in testAtomic64.\n    Don't force the check function to return early, as now taking the\n    address of a local variable in the runtime package does not force it\n    onto the heap.\n    \n    Reviewed-on: https://go-review.googlesource.com/41144\n\n\ncommit 4269db69f9184e5a45c54aaee7352425a1f88bff\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Apr 19 17:55:21 2017 -0700\n\n    runtime: split up ticks to get correct alignment\n    \n    On 32-bit x86 a uint64 variable by itself is aligned to an 8-byte boundary.\n    A uint64 field in a struct is aligned to a 4-byte boundary.\n    The runtime.ticks variable has a uint64 field that must be aligned\n    to an 8-byte boundary.  Rather than rely on luck, split up the struct\n    into separate vars so that the required alignment happens reliably.\n    \n    It would be much nicer if issue golang/go#19057 were fixed somehow,\n    but that is for another day.\n    \n    Reviewed-on: https://go-review.googlesource.com/41143\n\n\ncommit 66926cabdbdbf3431b4f172f7756e195c1c6c513\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Thu Apr 20 17:15:38 2017 +0200\n\n    libgo: fix bad value for O_CLOEXEC on AIX 7.1\n    \n    On AIX 7.1, O_CLOEXEC is defined as 0x0000001000000000, which\n    creates an integer constant overflow error when building libgo.\n    \n    This affects only 7.1, O_CLOEXEC is not defined on 6.1 (and\n    defaults to O in sysinfo.go) and is defined as 0x00800000 on\n    AIX 7.2.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/41214\n\n\ncommit af288ff10aeafc47651f5def327ed56425d5be19\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Thu Apr 20 17:15:02 2017 -0700\n\n    runtime: preserve stack context in tracebackothers\n    \n    The tracebackothers function works by saving the current stack context\n    in the goroutine's context field and then calling gogo to switch to a\n    new goroutine.  The new goroutine will collect its own stack trace and\n    then call gogo to switch back to the original goroutine.  This works\n    fine, but if the original goroutine was called by mcall then the\n    contents of its context field are needed to return from the mcall.\n    Fix this by saving the stack context across the calls to the other\n    goroutines.\n    \n    Reviewed-on: https://go-review.googlesource.com/41293\n\n\ncommit 43101e5956e793f1b4de05c15d7738c785e927df\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Fri Apr 21 10:58:52 2017 +0200\n\n    os/user: use _posix_* libc functions\n    \n    libc getpwnam_r function has a different signature, we must use\n    _posix_getpwnam_r instead (by default, the pwd.h system include\n     file defines getpwnam_r as a static function calling\n    _posix_getpwnam_r, so a C program calling getpwnam_r will indeed\n    reference the _posix_getpwnam_r symbol).\n    \n    Idem for getpwuid_r, getgrnam_r and getgrgid_r.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/41334\n\n\ncommit 71e1fec4d2a536591ea6657a06916a17b5127071\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Apr 19 21:24:48 2017 -0700\n\n    runtime: don't use pointers in g_ucontext_t or stackcontext\n    \n    The g_ucontext_t type holds registers saved for a goroutine.  We have\n    to scan it for pointers, but since registers don't necessarily hold\n    pointers we have to scan it conservatively.  That means that it should\n    not have a pointer type, since the GC will always scan pointers.\n    Instead it needs special treatment to be scanned conservatively.\n    The current GC doesn't care when a pointer type holds a non-pointer,\n    but the Go 1.8 GC does.\n    \n    For the current GC this means we have to explicitly scan the\n    g_ucontext_t values in a G.\n    \n    While we're at it change stackcontext to be uintptr too.  The entries\n    in stackcontext never hold pointers that the Go GC cares about.\n    \n    Reviewed-on: https://go-review.googlesource.com/41270\n\n\ncommit eab2960aee91d3e3a6baa5b1bce01262d24c714f\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Thu Apr 20 17:08:19 2017 -0700\n\n    runtime/internal/sys: define Goexperiment\n    \n    The gc toolchain defines Goexperiment based on the environment\n    variable GOEXPERIMENT when the toolchain is built.  We just always set\n    Goexperiment to the empty string.\n    \n    Reviewed-on: https://go-review.googlesource.com/41292\n\n\ncommit be4a751943265c0637da859d15a4faf162f5c478\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Thu Apr 20 14:04:35 2017 +0200\n\n    net: sockopt implementation for AIX\n    \n    This is a copy of the Linux implementation, it allows to\n    run some simple client/server applications on AIX, while\n    the current sockopt stubs don't.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/41213\n\n\ncommit 46a669c4ca5b80fd6f6a0a42095804d9f704611d\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Wed Mar 29 17:55:06 2017 +0200\n\n    math: fix sign for atan/expm1/log1p(-0)\n    \n    AIX libc returns +0 for atan(-0), expm1(-0) and log1p(-0),\n    while matching Go functions must return -0.\n    \n    Code courtesy of Tony Reix.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/38699\n\n\ncommit 53b0e809130038a46f0a3d2870e3905f44ab888d\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Wed Apr 26 17:29:22 2017 +0200\n\n    runtime: fix context clobbering on AIX\n    \n    On AIX 64-bits, r13 is a pointer to thread data.\n    setcontext() overwrites r13 with the value saved by getcontext().\n    So, when a goroutine is scheduled on a new thread, r13 will point\n    to the old thread data after calling setcontext().\n    \n    Code courtesy of Damien Bergamini.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/41854\n\n\ncommit f8d5ebd71c71e6e777200530d8204b92619157f8\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Wed Apr 26 18:01:19 2017 +0200\n\n    runtime: fix wrong time calculation in semasleep\n    \n    tv_nsec is added twice when calculating the sleep end time.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/41855\n\n\ncommit ef56097f4ea848d48fbf61eba1c757fe7fce99d3\nAuthor: Matthieu Sarter <matthieu.sarter.external@atos.net>\nDate:   Fri Apr 28 10:27:32 2017 +0200\n\n    libgo: pass $(NM) value when running benchmarks\n    \n    On AIX, we need to use \"nm -B\" instead of \"nm\", to have the\n    epxected output format, so the configured $(NM) value from\n    the Makefile should be exported before running gotest, which\n    defaults to \"nm\" if $NM is not set.\n    \n    Issue golang/go#19200\n    \n    Reviewed-on: https://go-review.googlesource.com/42051\n\n\ncommit 0fb550083ae474fb964435927b899ec8e4b62771\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Wed Nov 16 21:12:53 2016 -0800\n\n    runtime: copy garbage collector from Go 1.8 runtime\n    \n    This giant patch replaces the old Go 1.4 memory allocator and garbage\n    collector with the new Go 1.8 code.  The memory allocator is fairly\n    similar, though now written in Go rather than C.  The garbage\n    collector is completely different.  It now uses ptrmask and gcprog\n    information, which requires changes in the compiler and the reflect\n    package as well as the runtime.  And, of course, the garbage collector\n    now runs concurrently with program execution.\n    \n    In the gc toolchain the garbage collector is strict and precise at all\n    levels.  In the gofrontend we do not have stack maps, so stacks, and\n    register values, are collected conservatively.  That means that an\n    old, no longer used, pointer on a stack or in a register can cause a\n    memory object to live longer than it should.  That in turns means that\n    we must disable some checks for invalid pointers in the garbage\n    collection code.  Not only can we get an invalid pointer on the stack;\n    the concurrent nature of the collector means that we can in effect\n    resurrect a block that was already unmarked but that the collector had\n    not yet gotten around to freeing, and that block can in turn point to\n    other blocks that the collector had managed to already free.  So we\n    must disable pointer checks in general.  In effect we are relying on\n    the fact that the strict pointer checks in the gc toolchain ensure\n    that the garbage collector is correct, and we just assume that it is\n    correct for the gofrontend since we are using the same code.\n    \n    Reviewed-on: https://go-review.googlesource.com/41307\n\n\ncommit a95078d501175240d095500a8c5fbfb21bec65cb\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Mon Apr 24 16:33:47 2017 -0700\n\n    libgo/Makefile: clean more files\n    \n    Fix up the mostlyclean, clean, and distclean targets to better follow\n    https://www.gnu.org/prep/standards/html_node/Standard-Targets.html.\n    \n    Reviewed-on: https://go-review.googlesource.com/41625\n\n\ncommit 5956bf1055451cf4239cdfeca259c23b1ded54d8\nAuthor: Ian Lance Taylor <iant@golang.org>\nDate:   Mon May 8 13:35:11 2017 -0700\n\n    libgo: delete goc2c\n    \n    The last .goc file has been removed, so remove goc2c.\n    \n    The goc2c program was my first contribution to the gc repository that\n    was more than 100 lines:\n    https://github.com/golang/go/commit/2b57a1124e87b0dc8bc1ff6899297b4d7d6e74f2\n    The program was used in gc for a few years under various guises but\n    was finally removed in https://golang.org/cl/132680043.  Now we can\n    remove it from gofrontend as well.\n    \n    Reviewed-on: https://go-review.googlesource.com/42911\n\n\ncommit a222e35d041de0cd42506b61c93b8209e07702b9\nAuthor: Than McIntosh <thanm@google.com>\nDate:   Tue May 9 10:33:10 2017 -0400\n\n    compiler: set \"need_init_fn\" when adding gc root\n    \n    Variables that back slice initializers in certain cases have to be\n    added to the gc roots list, since they can be modified at runtime. The\n    code that was doing this addition did not update the flag that tracks\n    whether the package being compiled needs an initializer function,\n    which resulted in the call in question being left out of the final\n    generated code in certain cases. Fix is to change Gogo::add_gc_root()\n    to update the \"needs init\" flag.\n    \n    Reviewed-on: https://go-review.googlesource.com/43030\n\n\ncommit 822ab419bf7d1c705cdce1c12133e7a11f56be2e\nAuthor: Than McIntosh <thanm@google.com>\nDate:   Tue May 9 11:36:51 2017 -0400\n\n    compiler: fix variable context nit in write barrier generation\n    \n    Update the write barrier generation code to insure that the \"lvalue\n    context\" tag on the space var expression is set only in the case where\n    the expr feeds directly into an assignment. This is somewhat\n    counter-intuitive, but needed in the case where the backend looks at\n    context tags.\n    \n    Reviewed-on: https://go-review.googlesource.com/43031\n\nFrom-SVN: r247848", "tree": {"sha": "2fdfc586b7f8d427f4f4efcaf91b88dcac9d244f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/2fdfc586b7f8d427f4f4efcaf91b88dcac9d244f"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/f163907e0afadfa09f23320c51b90927c771c106", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f163907e0afadfa09f23320c51b90927c771c106", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f163907e0afadfa09f23320c51b90927c771c106", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f163907e0afadfa09f23320c51b90927c771c106/comments", "author": null, "committer": null, "parents": [{"sha": "0f3744176fe1c1f739c8c53c1f6627ee8791eb03", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0f3744176fe1c1f739c8c53c1f6627ee8791eb03", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}], "stats": {"total": 29023, "additions": 18043, "deletions": 10980}, "files": [{"sha": "6b37acd189427f7a34596ad8a91dc27493767777", "filename": "gcc/go/ChangeLog", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2FChangeLog?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -1,3 +1,14 @@\n+2017-05-10  Than McIntosh  <thanm@google.com>\n+\n+\t* go-backend.c: Include \"go-c.h\".\n+\t* go-gcc.cc (Gcc_backend::write_export_data): New method.\n+\n+2017-05-10  Ian Lance Taylor  <iant@google.com>\n+\n+\t* go-gcc.cc (Gcc_backend::Gcc_backend): Declare\n+\t__builtin_prefetch.\n+\t* Make-lang.in (GO_OBJS): Add go/wb.o.\n+\n 2017-03-28  Than McIntosh  <thanm@google.com>\n \n \tPR go/80226"}, {"sha": "b65d347f93a2fd465b5bfd63f224e235515171ba", "filename": "gcc/go/Make-lang.in", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2FMake-lang.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2FMake-lang.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2FMake-lang.in?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -72,7 +72,8 @@ GO_OBJS = \\\n \tgo/runtime.o \\\n \tgo/statements.o \\\n \tgo/types.o \\\n-\tgo/unsafe.o\n+\tgo/unsafe.o \\\n+\tgo/wb.o\n \n go_OBJS = $(GO_OBJS) go/gospec.o\n "}, {"sha": "2f8d2f405da1fcf35cb2a3723a1fe0462c079c39", "filename": "gcc/go/go-backend.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgo-backend.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgo-backend.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgo-backend.c?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -30,7 +30,7 @@ along with GCC; see the file COPYING3.  If not see\n #include \"intl.h\"\n #include \"output.h\"\t/* for assemble_string */\n #include \"common/common-target.h\"\n-\n+#include \"go-c.h\"\n \n /* The segment name we pass to simple_object_start_read to find Go\n    export data.  */"}, {"sha": "7c6147a8e2d90e6176d3f50845f6128b2480c5f2", "filename": "gcc/go/go-gcc.cc", "status": "modified", "additions": 18, "deletions": 0, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgo-gcc.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgo-gcc.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgo-gcc.cc?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -506,6 +506,10 @@ class Gcc_backend : public Backend\n                            const std::vector<Bfunction*>&,\n                            const std::vector<Bvariable*>&);\n \n+  void\n+  write_export_data(const char* bytes, unsigned int size);\n+\n+\n  private:\n   // Make a Bexpression from a tree.\n   Bexpression*\n@@ -748,6 +752,13 @@ Gcc_backend::Gcc_backend()\n   this->define_builtin(BUILT_IN_TRAP, \"__builtin_trap\", NULL,\n \t\t       build_function_type(void_type_node, void_list_node),\n \t\t       false, true);\n+\n+  // The runtime uses __builtin_prefetch.\n+  this->define_builtin(BUILT_IN_PREFETCH, \"__builtin_prefetch\", NULL,\n+\t\t       build_varargs_function_type_list(void_type_node,\n+\t\t\t\t\t\t\tconst_ptr_type_node,\n+\t\t\t\t\t\t\tNULL_TREE),\n+\t\t       false, false);\n }\n \n // Get an unnamed integer type.\n@@ -3212,6 +3223,13 @@ Gcc_backend::write_global_definitions(\n   delete[] defs;\n }\n \n+void\n+Gcc_backend::write_export_data(const char* bytes, unsigned int size)\n+{\n+  go_write_export_data(bytes, size);\n+}\n+\n+\n // Define a builtin function.  BCODE is the builtin function code\n // defined by builtins.def.  NAME is the name of the builtin function.\n // LIBNAME is the name of the corresponding library function, and is"}, {"sha": "1082abd40b5cf00898d327d0ca6c9f0454847f1c", "filename": "gcc/go/gofrontend/MERGE", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2FMERGE", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2FMERGE", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2FMERGE?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -1,4 +1,4 @@\n-a4f445e18fb06a032a4399859f432e03245f1a7d\n+822ab419bf7d1c705cdce1c12133e7a11f56be2e\n \n The first line of this file holds the git revision number of the last\n merge done from the gofrontend repository."}, {"sha": "e51efe4ef237d82b7876c3faba9bd9584f06fe2a", "filename": "gcc/go/gofrontend/backend.h", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fbackend.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fbackend.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fbackend.h?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -750,6 +750,11 @@ class Backend\n                            const std::vector<Bexpression*>& constant_decls,\n                            const std::vector<Bfunction*>& function_decls,\n                            const std::vector<Bvariable*>& variable_decls) = 0;\n+\n+  // Write SIZE bytes of export data from BYTES to the proper\n+  // section in the output object file.\n+  virtual void\n+  write_export_data(const char* bytes, unsigned int size) = 0;\n };\n \n #endif // !defined(GO_BACKEND_H)"}, {"sha": "27b76801d5cd9e18d3e255a78167c62353eeb8c2", "filename": "gcc/go/gofrontend/export.cc", "status": "modified", "additions": 10, "deletions": 2, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fexport.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fexport.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fexport.cc?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -14,6 +14,9 @@\n #include \"statements.h\"\n #include \"export.h\"\n \n+#include \"go-linemap.h\"\n+#include \"backend.h\"\n+\n // This file handles exporting global declarations.\n \n // Class Export.\n@@ -359,6 +362,10 @@ Export::write_imported_init_fns(const std::string& package_name,\n        ++p)\n     {\n       const Import_init* ii = *p;\n+\n+      if (ii->init_name() == import_init_fn)\n+\tcontinue;\n+\n       this->write_c_string(\" \");\n       this->write_string(ii->package_name());\n       this->write_c_string(\" \");\n@@ -727,7 +734,8 @@ Export::Stream::write_checksum(const std::string& s)\n \n // Class Stream_to_section.\n \n-Stream_to_section::Stream_to_section()\n+Stream_to_section::Stream_to_section(Backend* backend)\n+    : backend_(backend)\n {\n }\n \n@@ -736,5 +744,5 @@ Stream_to_section::Stream_to_section()\n void\n Stream_to_section::do_write(const char* bytes, size_t length)\n {\n-  go_write_export_data (bytes, length);\n+  this->backend_->write_export_data (bytes, length);\n }"}, {"sha": "136567725a5148ad3de0d02aa9dfe81f5823540d", "filename": "gcc/go/gofrontend/export.h", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fexport.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fexport.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fexport.h?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -16,6 +16,7 @@ class Bindings;\n class Type;\n class Package;\n class Import_init_set;\n+class Backend;\n \n // Codes used for the builtin types.  These are all negative to make\n // them easily distinct from the codes assigned by Export::write_type.\n@@ -236,11 +237,14 @@ class Export : public String_dump\n class Stream_to_section : public Export::Stream\n {\n  public:\n-  Stream_to_section();\n+  Stream_to_section(Backend*);\n \n  protected:\n   void\n   do_write(const char*, size_t);\n+\n+ private:\n+  Backend* backend_;\n };\n \n #endif // !defined(GO_EXPORT_H)"}, {"sha": "ecafe1658b67f4150101e00532c798b21a20d46d", "filename": "gcc/go/gofrontend/expressions.cc", "status": "modified", "additions": 331, "deletions": 64, "changes": 395, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fexpressions.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fexpressions.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fexpressions.cc?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -900,8 +900,8 @@ Temporary_reference_expression::do_get_backend(Translate_context* context)\n   // the circularity down one level.\n   Type* stype = this->statement_->type();\n   if (!this->is_lvalue_\n-      && stype->has_pointer()\n-      && stype->deref()->is_void_type())\n+      && stype->points_to() != NULL\n+      && stype->points_to()->is_void_type())\n     {\n       Btype* btype = this->type()->base()->get_backend(gogo);\n       ret = gogo->backend()->convert_expression(btype, ret, this->location());\n@@ -3311,7 +3311,18 @@ Type_conversion_expression::do_is_static_initializer() const\n   if (Type::are_identical(type, expr_type, false, NULL))\n     return true;\n \n-  return type->is_basic_type() && expr_type->is_basic_type();\n+  if (type->is_string_type() && expr_type->is_string_type())\n+    return true;\n+\n+  if ((type->is_numeric_type()\n+       || type->is_boolean_type()\n+       || type->points_to() != NULL)\n+      && (expr_type->is_numeric_type()\n+\t  || expr_type->is_boolean_type()\n+\t  || expr_type->points_to() != NULL))\n+    return true;\n+\n+  return false;\n }\n \n // Return the constant numeric value if there is one.\n@@ -3570,7 +3581,18 @@ Unsafe_type_conversion_expression::do_is_static_initializer() const\n   if (Type::are_convertible(type, expr_type, NULL))\n     return true;\n \n-  return type->is_basic_type() && expr_type->is_basic_type();\n+  if (type->is_string_type() && expr_type->is_string_type())\n+    return true;\n+\n+  if ((type->is_numeric_type()\n+       || type->is_boolean_type()\n+       || type->points_to() != NULL)\n+      && (expr_type->is_numeric_type()\n+\t  || expr_type->is_boolean_type()\n+\t  || expr_type->points_to() != NULL))\n+    return true;\n+\n+  return false;\n }\n \n // Convert to backend representation.\n@@ -3669,6 +3691,48 @@ Expression::make_unsafe_cast(Type* type, Expression* expr,\n \n // Class Unary_expression.\n \n+// Call the address_taken method of the operand if needed.  This is\n+// called after escape analysis but before inserting write barriers.\n+\n+void\n+Unary_expression::check_operand_address_taken(Gogo* gogo)\n+{\n+  if (this->op_ != OPERATOR_AND)\n+    return;\n+\n+  // If this->escapes_ is false at this point, then it was set to\n+  // false by an explicit call to set_does_not_escape, and the value\n+  // does not escape.  If this->escapes_ is true, we may be able to\n+  // set it to false if taking the address of a variable that does not\n+  // escape.\n+  Node* n = Node::make_node(this);\n+  if ((n->encoding() & ESCAPE_MASK) == int(Node::ESCAPE_NONE))\n+    this->escapes_ = false;\n+\n+  // When compiling the runtime, the address operator does not cause\n+  // local variables to escape.  When escape analysis becomes the\n+  // default, this should be changed to make it an error if we have an\n+  // address operator that escapes.\n+  if (gogo->compiling_runtime() && gogo->package_name() == \"runtime\")\n+    this->escapes_ = false;\n+\n+  Named_object* var = NULL;\n+  if (this->expr_->var_expression() != NULL)\n+    var = this->expr_->var_expression()->named_object();\n+  else if (this->expr_->enclosed_var_expression() != NULL)\n+    var = this->expr_->enclosed_var_expression()->variable();\n+\n+  if (this->escapes_ && var != NULL)\n+    {\n+      if (var->is_variable())\n+\tthis->escapes_ = var->var_value()->escapes();\n+      if (var->is_result_variable())\n+\tthis->escapes_ = var->result_var_value()->escapes();\n+    }\n+\n+  this->expr_->address_taken(this->escapes_);\n+}\n+\n // If we are taking the address of a composite literal, and the\n // contents are not constant, then we want to make a heap expression\n // instead.\n@@ -3795,40 +3859,6 @@ Unary_expression::do_flatten(Gogo* gogo, Named_object*,\n         }\n     }\n \n-  if (this->op_ == OPERATOR_AND)\n-    {\n-      // If this->escapes_ is false at this point, then it was set to\n-      // false by an explicit call to set_does_not_escape, and the\n-      // value does not escape.  If this->escapes_ is true, we may be\n-      // able to set it to false if taking the address of a variable\n-      // that does not escape.\n-      Node* n = Node::make_node(this);\n-      if ((n->encoding() & ESCAPE_MASK) == int(Node::ESCAPE_NONE))\n-\tthis->escapes_ = false;\n-\n-      // When compiling the runtime, the address operator does not\n-      // cause local variables to escape.  When escape analysis\n-      // becomes the default, this should be changed to make it an\n-      // error if we have an address operator that escapes.\n-      if (gogo->compiling_runtime() && gogo->package_name() == \"runtime\")\n-\tthis->escapes_ = false;\n-\n-      Named_object* var = NULL;\n-      if (this->expr_->var_expression() != NULL)\n-\tvar = this->expr_->var_expression()->named_object();\n-      else if (this->expr_->enclosed_var_expression() != NULL)\n-\tvar = this->expr_->enclosed_var_expression()->variable();\n-\n-      if (this->escapes_ && var != NULL)\n-\t{\n-\t  if (var->is_variable())\n-\t    this->escapes_ = var->var_value()->escapes();\n-\t  if (var->is_result_variable())\n-\t    this->escapes_ = var->result_var_value()->escapes();\n-\t}\n-      this->expr_->address_taken(this->escapes_);\n-    }\n-\n   if (this->create_temp_ && !this->expr_->is_variable())\n     {\n       Temporary_statement* temp =\n@@ -7759,7 +7789,16 @@ Builtin_call_expression::flatten_append(Gogo* gogo, Named_object* function,\n \t  lhs = Expression::make_index(ref, ref2, NULL, NULL, loc);\n \t  gogo->lower_expression(function, inserter, &lhs);\n \t  gogo->flatten_expression(function, inserter, &lhs);\n-\t  assign = Statement::make_assignment(lhs, *pa, loc);\n+\t  // The flatten pass runs after the write barrier pass, so we\n+\t  // need to insert a write barrier here if necessary.\n+\t  if (!gogo->assign_needs_write_barrier(lhs))\n+\t    assign = Statement::make_assignment(lhs, *pa, loc);\n+\t  else\n+\t    {\n+\t      Function* f = function == NULL ? NULL : function->func_value();\n+\t      assign = gogo->assign_with_write_barrier(f, NULL, inserter,\n+\t\t\t\t\t\t       lhs, *pa, loc);\n+\t    }\n \t  inserter->insert(assign);\n \t}\n     }\n@@ -9539,6 +9578,12 @@ Call_expression::lower_varargs(Gogo* gogo, Named_object* function,\n \t\t\t       Type* varargs_type, size_t param_count,\n                                Slice_storage_escape_disp escape_disp)\n {\n+  // When compiling the runtime, varargs slices do not escape.  When\n+  // escape analysis becomes the default, this should be changed to\n+  // make it an error if we have a varargs slice that escapes.\n+  if (gogo->compiling_runtime() && gogo->package_name() == \"runtime\")\n+    escape_disp = SLICE_STORAGE_DOES_NOT_ESCAPE;\n+\n   if (this->varargs_are_lowered_)\n     return;\n \n@@ -10250,16 +10295,13 @@ Call_expression::do_get_backend(Translate_context* context)\n \n   if (this->results_ != NULL)\n     {\n-      go_assert(this->call_temp_ != NULL);\n-      Expression* call_ref =\n-          Expression::make_temporary_reference(this->call_temp_, location);\n-      Bexpression* bcall_ref = call_ref->get_backend(context);\n+      Bexpression* bcall_ref = this->call_result_ref(context);\n       Bfunction* bfunction = context->function()->func_value()->get_decl();\n       Bstatement* assn_stmt =\n           gogo->backend()->assignment_statement(bfunction,\n                                                 bcall_ref, call, location);\n \n-      this->call_ = this->set_results(context, bcall_ref);\n+      this->call_ = this->set_results(context);\n \n       Bexpression* set_and_call =\n           gogo->backend()->compound_expression(assn_stmt, this->call_,\n@@ -10271,16 +10313,32 @@ Call_expression::do_get_backend(Translate_context* context)\n   return this->call_;\n }\n \n+// Return the backend representation of a reference to the struct used\n+// to capture the result of a multiple-output call.\n+\n+Bexpression*\n+Call_expression::call_result_ref(Translate_context* context)\n+{\n+  go_assert(this->call_temp_ != NULL);\n+  Location location = this->location();\n+  Expression* call_ref =\n+      Expression::make_temporary_reference(this->call_temp_, location);\n+  Bexpression* bcall_ref = call_ref->get_backend(context);\n+  return bcall_ref;\n+}\n+\n // Set the result variables if this call returns multiple results.\n \n Bexpression*\n-Call_expression::set_results(Translate_context* context, Bexpression* call)\n+Call_expression::set_results(Translate_context* context)\n {\n   Gogo* gogo = context->gogo();\n \n   Bexpression* results = NULL;\n   Location loc = this->location();\n \n+  go_assert(this->call_temp_ != NULL);\n+\n   size_t rc = this->result_count();\n   for (size_t i = 0; i < rc; ++i)\n     {\n@@ -10296,12 +10354,15 @@ Call_expression::set_results(Translate_context* context, Bexpression* call)\n \n       Bfunction* bfunction = context->function()->func_value()->get_decl();\n       Bexpression* result_ref = ref->get_backend(context);\n+      Bexpression* bcall_ref = this->call_result_ref(context);\n       Bexpression* call_result =\n-          gogo->backend()->struct_field_expression(call, i, loc);\n+          gogo->backend()->struct_field_expression(bcall_ref, i, loc);\n       Bstatement* assn_stmt =\n           gogo->backend()->assignment_statement(bfunction,\n                                                 result_ref, call_result, loc);\n \n+      bcall_ref = this->call_result_ref(context);\n+      call_result = gogo->backend()->struct_field_expression(bcall_ref, i, loc);\n       Bexpression* result =\n           gogo->backend()->compound_expression(assn_stmt, call_result, loc);\n \n@@ -10860,7 +10921,7 @@ Array_index_expression::do_flatten(Gogo*, Named_object*,\n       inserter->insert(temp);\n       this->end_ = Expression::make_temporary_reference(temp, loc);\n     }\n-  if (cap!= NULL && !cap->is_variable())\n+  if (cap != NULL && !cap->is_variable())\n     {\n       temp = Statement::make_temporary(NULL, cap, loc);\n       inserter->insert(temp);\n@@ -11047,16 +11108,28 @@ Array_index_expression::do_get_backend(Translate_context* context)\n \t\t\t\t\t\t     bad_index, loc);\n     }\n \n-  Expression* valptr = array_type->get_value_pointer(gogo, this->array_);\n-  Bexpression* val = valptr->get_backend(context);\n-  val = gogo->backend()->pointer_offset_expression(val, start, loc);\n-\n   Bexpression* result_length =\n     gogo->backend()->binary_expression(OPERATOR_MINUS, end, start, loc);\n \n   Bexpression* result_capacity =\n     gogo->backend()->binary_expression(OPERATOR_MINUS, cap_arg, start, loc);\n \n+  // If the new capacity is zero, don't change val.  Otherwise we can\n+  // get a pointer to the next object in memory, keeping it live\n+  // unnecessarily.  When the capacity is zero, the actual pointer\n+  // value doesn't matter.\n+  Bexpression* zero =\n+    Expression::make_integer_ul(0, int_type, loc)->get_backend(context);\n+  Bexpression* cond =\n+    gogo->backend()->binary_expression(OPERATOR_EQEQ, result_capacity, zero,\n+\t\t\t\t       loc);\n+  Bexpression* offset = gogo->backend()->conditional_expression(bfn, int_btype,\n+\t\t\t\t\t\t\t\tcond, zero,\n+\t\t\t\t\t\t\t\tstart, loc);\n+  Expression* valptr = array_type->get_value_pointer(gogo, this->array_);\n+  Bexpression* val = valptr->get_backend(context);\n+  val = gogo->backend()->pointer_offset_expression(val, offset, loc);\n+\n   Btype* struct_btype = this->type()->get_backend(gogo);\n   std::vector<Bexpression*> init;\n   init.push_back(val);\n@@ -13065,10 +13138,12 @@ Slice_construction_expression::do_flatten(Gogo* gogo, Named_object* no,\n   this->Array_construction_expression::do_flatten(gogo, no, inserter);\n \n   // Create a stack-allocated storage temp if storage won't escape\n-  if (!this->storage_escapes_ && this->slice_storage_ == NULL)\n+  if (!this->storage_escapes_\n+      && this->slice_storage_ == NULL\n+      && this->element_count() > 0)\n     {\n       Location loc = this->location();\n-      this->array_val_ = create_array_val();\n+      this->array_val_ = this->create_array_val();\n       go_assert(this->array_val_);\n       Temporary_statement* temp =\n           Statement::make_temporary(this->valtype_, this->array_val_, loc);\n@@ -13098,7 +13173,7 @@ Bexpression*\n Slice_construction_expression::do_get_backend(Translate_context* context)\n {\n   if (this->array_val_ == NULL)\n-    this->array_val_ = create_array_val();\n+    this->array_val_ = this->create_array_val();\n   if (this->array_val_ == NULL)\n     {\n       go_assert(this->type()->is_error());\n@@ -14216,14 +14291,15 @@ Heap_expression::do_type()\n Bexpression*\n Heap_expression::do_get_backend(Translate_context* context)\n {\n-  if (this->expr_->is_error_expression() || this->expr_->type()->is_error())\n+  Type* etype = this->expr_->type();\n+  if (this->expr_->is_error_expression() || etype->is_error())\n     return context->backend()->error_expression();\n \n   Location loc = this->location();\n   Gogo* gogo = context->gogo();\n   Btype* btype = this->type()->get_backend(gogo);\n \n-  Expression* alloc = Expression::make_allocation(this->expr_->type(), loc);\n+  Expression* alloc = Expression::make_allocation(etype, loc);\n   Node* n = Node::make_node(this);\n   if ((n->encoding() & ESCAPE_MASK) == int(Node::ESCAPE_NONE))\n     alloc->allocation_expression()->set_allocate_on_stack();\n@@ -14236,14 +14312,43 @@ Heap_expression::do_get_backend(Translate_context* context)\n   Bvariable* space_temp =\n     gogo->backend()->temporary_variable(fndecl, context->bblock(), btype,\n \t\t\t\t\tspace, true, loc, &decl);\n-  space = gogo->backend()->var_expression(space_temp, VE_lvalue, loc);\n-  Btype* expr_btype = this->expr_->type()->get_backend(gogo);\n-  Bexpression* ref =\n-    gogo->backend()->indirect_expression(expr_btype, space, true, loc);\n+  Btype* expr_btype = etype->get_backend(gogo);\n \n   Bexpression* bexpr = this->expr_->get_backend(context);\n-  Bstatement* assn = gogo->backend()->assignment_statement(fndecl, ref,\n-                                                           bexpr, loc);\n+\n+  // If this assignment needs a write barrier, call typedmemmove.  We\n+  // don't do this in the write barrier pass because in some cases\n+  // backend conversion can introduce new Heap_expression values.\n+  Bstatement* assn;\n+  if (!etype->has_pointer())\n+    {\n+      space = gogo->backend()->var_expression(space_temp, VE_lvalue, loc);\n+      Bexpression* ref =\n+\tgogo->backend()->indirect_expression(expr_btype, space, true, loc);\n+      assn = gogo->backend()->assignment_statement(fndecl, ref, bexpr, loc);\n+    }\n+  else\n+    {\n+      Bstatement* edecl;\n+      Bvariable* btemp =\n+\tgogo->backend()->temporary_variable(fndecl, context->bblock(),\n+\t\t\t\t\t    expr_btype, bexpr, true, loc,\n+\t\t\t\t\t    &edecl);\n+      Bexpression* btempref = gogo->backend()->var_expression(btemp,\n+\t\t\t\t\t\t\t      VE_lvalue, loc);\n+      Bexpression* addr = gogo->backend()->address_expression(btempref, loc);\n+\n+      Expression* td = Expression::make_type_descriptor(etype, loc);\n+      Type* etype_ptr = Type::make_pointer_type(etype);\n+      space = gogo->backend()->var_expression(space_temp, VE_rvalue, loc);\n+      Expression* elhs = Expression::make_backend(space, etype_ptr, loc);\n+      Expression* erhs = Expression::make_backend(addr, etype_ptr, loc);\n+      Expression* call = Runtime::make_call(Runtime::TYPEDMEMMOVE, loc, 3,\n+\t\t\t\t\t    td, elhs, erhs);\n+      Bexpression* bcall = call->get_backend(context);\n+      Bstatement* s = gogo->backend()->expression_statement(fndecl, bcall);\n+      assn = gogo->backend()->compound_statement(edecl, s);\n+    }\n   decl = gogo->backend()->compound_statement(decl, assn);\n   space = gogo->backend()->var_expression(space_temp, VE_rvalue, loc);\n   return gogo->backend()->compound_expression(decl, space, loc);\n@@ -14468,7 +14573,7 @@ class GC_symbol_expression : public Expression\n  protected:\n   Type*\n   do_type()\n-  { return Type::lookup_integer_type(\"uintptr\"); }\n+  { return Type::make_pointer_type(Type::lookup_integer_type(\"uint8\")); }\n \n   bool\n   do_is_static_initializer() const\n@@ -14513,6 +14618,91 @@ Expression::make_gc_symbol(Type* type)\n   return new GC_symbol_expression(type);\n }\n \n+// An expression that evaluates to a pointer to a symbol holding the\n+// ptrmask data of a type.\n+\n+class Ptrmask_symbol_expression : public Expression\n+{\n+ public:\n+  Ptrmask_symbol_expression(Type* type)\n+    : Expression(EXPRESSION_PTRMASK_SYMBOL, Linemap::predeclared_location()),\n+      type_(type)\n+  {}\n+\n+ protected:\n+  Type*\n+  do_type()\n+  { return Type::make_pointer_type(Type::lookup_integer_type(\"uint8\")); }\n+\n+  bool\n+  do_is_static_initializer() const\n+  { return true; }\n+\n+  void\n+  do_determine_type(const Type_context*)\n+  { }\n+\n+  Expression*\n+  do_copy()\n+  { return this; }\n+\n+  Bexpression*\n+  do_get_backend(Translate_context*);\n+\n+  void\n+  do_dump_expression(Ast_dump_context*) const;\n+\n+ private:\n+  // The type that this ptrmask symbol describes.\n+  Type* type_;\n+};\n+\n+// Return the ptrmask variable.\n+\n+Bexpression*\n+Ptrmask_symbol_expression::do_get_backend(Translate_context* context)\n+{\n+  Gogo* gogo = context->gogo();\n+\n+  // If this type does not need a gcprog, then we can use the standard\n+  // GC symbol.\n+  int64_t ptrsize, ptrdata;\n+  if (!this->type_->needs_gcprog(gogo, &ptrsize, &ptrdata))\n+    return this->type_->gc_symbol_pointer(gogo);\n+\n+  // Otherwise we have to build a ptrmask variable, and return a\n+  // pointer to it.\n+\n+  Bvariable* bvar = this->type_->gc_ptrmask_var(gogo, ptrsize, ptrdata);\n+  Location bloc = Linemap::predeclared_location();\n+  Bexpression* bref = gogo->backend()->var_expression(bvar, VE_rvalue, bloc);\n+  Bexpression* baddr = gogo->backend()->address_expression(bref, bloc);\n+\n+  Type* uint8_type = Type::lookup_integer_type(\"uint8\");\n+  Type* pointer_uint8_type = Type::make_pointer_type(uint8_type);\n+  Btype* ubtype = pointer_uint8_type->get_backend(gogo);\n+  return gogo->backend()->convert_expression(ubtype, baddr, bloc);\n+}\n+\n+// Dump AST for a ptrmask symbol expression.\n+\n+void\n+Ptrmask_symbol_expression::do_dump_expression(\n+    Ast_dump_context* ast_dump_context) const\n+{\n+  ast_dump_context->ostream() << \"ptrmask(\";\n+  ast_dump_context->dump_type(this->type_);\n+  ast_dump_context->ostream() << \")\";\n+}\n+\n+// Make a ptrmask symbol expression.\n+\n+Expression*\n+Expression::make_ptrmask_symbol(Type* type)\n+{\n+  return new Ptrmask_symbol_expression(type);\n+}\n+\n // An expression which evaluates to some characteristic of a type.\n // This is only used to initialize fields of a type descriptor.  Using\n // a new expression class is slightly inefficient but gives us a good\n@@ -14565,6 +14755,8 @@ Type_info_expression::do_type()\n   switch (this->type_info_)\n     {\n     case TYPE_INFO_SIZE:\n+    case TYPE_INFO_BACKEND_PTRDATA:\n+    case TYPE_INFO_DESCRIPTOR_PTRDATA:\n       return Type::lookup_integer_type(\"uintptr\");\n     case TYPE_INFO_ALIGNMENT:\n     case TYPE_INFO_FIELD_ALIGNMENT:\n@@ -14593,6 +14785,12 @@ Type_info_expression::do_get_backend(Translate_context* context)\n     case TYPE_INFO_FIELD_ALIGNMENT:\n       ok = this->type_->backend_type_field_align(gogo, &val);\n       break;\n+    case TYPE_INFO_BACKEND_PTRDATA:\n+      ok = this->type_->backend_type_ptrdata(gogo, &val);\n+      break;\n+    case TYPE_INFO_DESCRIPTOR_PTRDATA:\n+      ok = this->type_->descriptor_ptrdata(gogo, &val);\n+      break;\n     default:\n       go_unreachable();\n     }\n@@ -14618,7 +14816,9 @@ Type_info_expression::do_dump_expression(\n   ast_dump_context->ostream() << \n     (this->type_info_ == TYPE_INFO_ALIGNMENT ? \"alignment\" \n     : this->type_info_ == TYPE_INFO_FIELD_ALIGNMENT ? \"field alignment\"\n-    : this->type_info_ == TYPE_INFO_SIZE ? \"size \"\n+    : this->type_info_ == TYPE_INFO_SIZE ? \"size\"\n+    : this->type_info_ == TYPE_INFO_BACKEND_PTRDATA ? \"backend_ptrdata\"\n+    : this->type_info_ == TYPE_INFO_DESCRIPTOR_PTRDATA ? \"descriptor_ptrdata\"\n     : \"unknown\");\n   ast_dump_context->ostream() << \")\";\n }\n@@ -15924,6 +16124,73 @@ Numeric_constant::mpfr_to_unsigned_long(const mpfr_t fval,\n   return ret;\n }\n \n+// Express value as memory size if possible.\n+\n+bool\n+Numeric_constant::to_memory_size(int64_t* val) const\n+{\n+  switch (this->classification_)\n+    {\n+    case NC_INT:\n+    case NC_RUNE:\n+      return this->mpz_to_memory_size(this->u_.int_val, val);\n+    case NC_FLOAT:\n+      return this->mpfr_to_memory_size(this->u_.float_val, val);\n+    case NC_COMPLEX:\n+      if (!mpfr_zero_p(mpc_imagref(this->u_.complex_val)))\n+\treturn false;\n+      return this->mpfr_to_memory_size(mpc_realref(this->u_.complex_val), val);\n+    default:\n+      go_unreachable();\n+    }\n+}\n+\n+// Express integer as memory size if possible.\n+\n+bool\n+Numeric_constant::mpz_to_memory_size(const mpz_t ival, int64_t* val) const\n+{\n+  if (mpz_sgn(ival) < 0)\n+    return false;\n+  if (mpz_fits_slong_p(ival))\n+    {\n+      *val = static_cast<int64_t>(mpz_get_si(ival));\n+      return true;\n+    }\n+\n+  // Test >= 64, not > 64, because an int64_t can hold 63 bits of a\n+  // positive value.\n+  if (mpz_sizeinbase(ival, 2) >= 64)\n+    return false;\n+\n+  mpz_t q, r;\n+  mpz_init(q);\n+  mpz_init(r);\n+  mpz_tdiv_q_2exp(q, ival, 32);\n+  mpz_tdiv_r_2exp(r, ival, 32);\n+  go_assert(mpz_fits_ulong_p(q) && mpz_fits_ulong_p(r));\n+  *val = ((static_cast<int64_t>(mpz_get_ui(q)) << 32)\n+\t  + static_cast<int64_t>(mpz_get_ui(r)));\n+  mpz_clear(r);\n+  mpz_clear(q);\n+  return true;\n+}\n+\n+// Express floating point value as memory size if possible.\n+\n+bool\n+Numeric_constant::mpfr_to_memory_size(const mpfr_t fval, int64_t* val) const\n+{\n+  if (!mpfr_integer_p(fval))\n+    return false;\n+  mpz_t ival;\n+  mpz_init(ival);\n+  mpfr_get_z(ival, fval, GMP_RNDN);\n+  bool ret = this->mpz_to_memory_size(ival, val);\n+  mpz_clear(ival);\n+  return ret;\n+}\n+\n // Convert value to integer if possible.\n \n bool"}, {"sha": "03bb08531f3cc85c4a714caa40302ec4f2646c70", "filename": "gcc/go/gofrontend/expressions.h", "status": "modified", "additions": 38, "deletions": 2, "changes": 40, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fexpressions.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fexpressions.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fexpressions.h?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -128,6 +128,7 @@ class Expression\n     EXPRESSION_RECEIVE,\n     EXPRESSION_TYPE_DESCRIPTOR,\n     EXPRESSION_GC_SYMBOL,\n+    EXPRESSION_PTRMASK_SYMBOL,\n     EXPRESSION_TYPE_INFO,\n     EXPRESSION_SLICE_INFO,\n     EXPRESSION_SLICE_VALUE,\n@@ -402,6 +403,13 @@ class Expression\n   static Expression*\n   make_gc_symbol(Type* type);\n \n+  // Make an expression that evaluates to the address of a ptrmask\n+  // symbol for TYPE.  For most types this will be the same as\n+  // make_gc_symbol, but for larger types make_gc_symbol will return a\n+  // gcprog while this will return a ptrmask.\n+  static Expression*\n+  make_ptrmask_symbol(Type* type);\n+\n   // Make an expression which evaluates to some characteristic of a\n   // type.  These are only used for type descriptors, so there is no\n   // location parameter.\n@@ -413,7 +421,15 @@ class Expression\n       TYPE_INFO_ALIGNMENT,\n       // The required alignment of a value of the type when used as a\n       // field in a struct.\n-      TYPE_INFO_FIELD_ALIGNMENT\n+      TYPE_INFO_FIELD_ALIGNMENT,\n+      // The size of the prefix of a value of the type that contains\n+      // all the pointers.  This is 0 for a type that contains no\n+      // pointers.  It is always <= TYPE_INFO_SIZE.\n+      TYPE_INFO_BACKEND_PTRDATA,\n+      // Like TYPE_INFO_BACKEND_PTRDATA, but the ptrdata value that we\n+      // want to store in a type descriptor.  They are the same for\n+      // most types, but can differ for a type that uses a gcprog.\n+      TYPE_INFO_DESCRIPTOR_PTRDATA\n     };\n \n   static Expression*\n@@ -1774,6 +1790,10 @@ class Unary_expression : public Expression\n     this->is_slice_init_ = true;\n   }\n \n+  // Call the address_taken method on the operand if necessary.\n+  void\n+  check_operand_address_taken(Gogo*);\n+\n   // Apply unary opcode OP to UNC, setting NC.  Return true if this\n   // could be done, false if not.  On overflow, issues an error and\n   // sets *ISSUED_ERROR.\n@@ -2270,7 +2290,10 @@ class Call_expression : public Expression\n \t\t\t    Expression**);\n \n   Bexpression*\n-  set_results(Translate_context*, Bexpression*);\n+  set_results(Translate_context*);\n+\n+  Bexpression*\n+  call_result_ref(Translate_context* context);\n \n   // The function to call.\n   Expression* fn_;\n@@ -4011,6 +4034,13 @@ class Numeric_constant\n   To_unsigned_long\n   to_unsigned_long(unsigned long* val) const;\n \n+  // If the value can be expressed as an integer that describes the\n+  // size of an object in memory, set *VAL and return true.\n+  // Otherwise, return false.  Currently we use int64_t to represent a\n+  // memory size, as in Type::backend_type_size.\n+  bool\n+  to_memory_size(int64_t* val) const;\n+\n   // If the value can be expressed as an int, return true and\n   // initialize and set VAL.  This will return false for a value with\n   // an explicit float or complex type, even if the value is integral.\n@@ -4052,6 +4082,12 @@ class Numeric_constant\n   To_unsigned_long\n   mpfr_to_unsigned_long(const mpfr_t fval, unsigned long *val) const;\n \n+  bool\n+  mpz_to_memory_size(const mpz_t ival, int64_t* val) const;\n+\n+  bool\n+  mpfr_to_memory_size(const mpfr_t fval, int64_t* val) const;\n+\n   bool\n   check_int_type(Integer_type*, bool, Location);\n "}, {"sha": "e3f17bccbefc8d99983eba6084579de2488f8904", "filename": "gcc/go/gofrontend/go.cc", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fgo.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fgo.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fgo.cc?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -158,6 +158,9 @@ go_parse_input_files(const char** filenames, unsigned int filename_count,\n   // Write out queued up functions for hash and comparison of types.\n   ::gogo->write_specific_type_functions();\n \n+  // Add write barriers.\n+  ::gogo->add_write_barriers();\n+\n   // Flatten the parse tree.\n   ::gogo->flatten();\n "}, {"sha": "12135d7e9a6c87e39dae6b1322eef8af124d300c", "filename": "gcc/go/gofrontend/gogo.cc", "status": "modified", "additions": 116, "deletions": 68, "changes": 184, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fgogo.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fgogo.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fgogo.cc?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -720,15 +720,18 @@ Gogo::init_imports(std::vector<Bstatement*>& init_stmts, Bfunction *bfunction)\n // roots during the mark phase.  We build a struct that is easy to\n // hook into a list of roots.\n \n-// struct __go_gc_root_list\n-// {\n-//   struct __go_gc_root_list* __next;\n-//   struct __go_gc_root\n-//   {\n-//     void* __decl;\n-//     size_t __size;\n-//   } __roots[];\n-// };\n+// type gcRoot struct {\n+// \tdecl    unsafe.Pointer // Pointer to variable.\n+//\tsize    uintptr        // Total size of variable.\n+// \tptrdata uintptr        // Length of variable's gcdata.\n+// \tgcdata  *byte          // Pointer mask.\n+// }\n+//\n+// type gcRootList struct {\n+// \tnext  *gcRootList\n+// \tcount int\n+// \troots [...]gcRoot\n+// }\n \n // The last entry in the roots array has a NULL decl field.\n \n@@ -737,28 +740,35 @@ Gogo::register_gc_vars(const std::vector<Named_object*>& var_gc,\n \t\t       std::vector<Bstatement*>& init_stmts,\n                        Bfunction* init_bfn)\n {\n-  if (var_gc.empty())\n+  if (var_gc.empty() && this->gc_roots_.empty())\n     return;\n \n   Type* pvt = Type::make_pointer_type(Type::make_void_type());\n-  Type* uint_type = Type::lookup_integer_type(\"uint\");\n-  Struct_type* root_type = Type::make_builtin_struct_type(2,\n-                                                          \"__decl\", pvt,\n-                                                          \"__size\", uint_type);\n+  Type* uintptr_type = Type::lookup_integer_type(\"uintptr\");\n+  Type* byte_type = this->lookup_global(\"byte\")->type_value();\n+  Type* pointer_byte_type = Type::make_pointer_type(byte_type);\n+  Struct_type* root_type =\n+    Type::make_builtin_struct_type(4,\n+\t\t\t\t   \"decl\", pvt,\n+\t\t\t\t   \"size\", uintptr_type,\n+\t\t\t\t   \"ptrdata\", uintptr_type,\n+\t\t\t\t   \"gcdata\", pointer_byte_type);\n \n   Location builtin_loc = Linemap::predeclared_location();\n-  unsigned roots_len = var_gc.size() + this->gc_roots_.size() + 1;\n+  unsigned long roots_len = var_gc.size() + this->gc_roots_.size();\n   Expression* length = Expression::make_integer_ul(roots_len, NULL,\n                                                    builtin_loc);\n   Array_type* root_array_type = Type::make_array_type(root_type, length);\n   root_array_type->set_is_array_incomparable();\n-  Type* ptdt = Type::make_type_descriptor_ptr_type();\n+\n+  Type* int_type = Type::lookup_integer_type(\"int\");\n   Struct_type* root_list_type =\n-      Type::make_builtin_struct_type(2,\n-                                     \"__next\", ptdt,\n-                                     \"__roots\", root_array_type);\n+      Type::make_builtin_struct_type(3,\n+                                     \"next\", pvt,\n+\t\t\t\t     \"count\", int_type,\n+                                     \"roots\", root_array_type);\n \n-  // Build an initializer for the __roots array.\n+  // Build an initializer for the roots array.\n \n   Expression_list* roots_init = new Expression_list();\n \n@@ -772,11 +782,22 @@ Gogo::register_gc_vars(const std::vector<Named_object*>& var_gc,\n       Expression* decl = Expression::make_var_reference(*p, no_loc);\n       Expression* decl_addr =\n           Expression::make_unary(OPERATOR_AND, decl, no_loc);\n+      decl_addr->unary_expression()->set_does_not_escape();\n+      decl_addr = Expression::make_cast(pvt, decl_addr, no_loc);\n       init->push_back(decl_addr);\n \n-      Expression* decl_size =\n-          Expression::make_type_info(decl->type(), Expression::TYPE_INFO_SIZE);\n-      init->push_back(decl_size);\n+      Expression* size =\n+\tExpression::make_type_info(decl->type(),\n+\t\t\t\t   Expression::TYPE_INFO_SIZE);\n+      init->push_back(size);\n+\n+      Expression* ptrdata =\n+\tExpression::make_type_info(decl->type(),\n+\t\t\t\t   Expression::TYPE_INFO_BACKEND_PTRDATA);\n+      init->push_back(ptrdata);\n+\n+      Expression* gcdata = Expression::make_ptrmask_symbol(decl->type());\n+      init->push_back(gcdata);\n \n       Expression* root_ctor =\n           Expression::make_struct_composite_literal(root_type, init, no_loc);\n@@ -791,37 +812,35 @@ Gogo::register_gc_vars(const std::vector<Named_object*>& var_gc,\n \n       Expression* expr = *p;\n       Location eloc = expr->location();\n-      init->push_back(expr);\n+      init->push_back(Expression::make_cast(pvt, expr, eloc));\n \n       Type* type = expr->type()->points_to();\n       go_assert(type != NULL);\n+\n       Expression* size =\n-\tExpression::make_type_info(type, Expression::TYPE_INFO_SIZE);\n+\tExpression::make_type_info(type,\n+\t\t\t\t   Expression::TYPE_INFO_SIZE);\n       init->push_back(size);\n \n+      Expression* ptrdata =\n+\tExpression::make_type_info(type,\n+\t\t\t\t   Expression::TYPE_INFO_BACKEND_PTRDATA);\n+      init->push_back(ptrdata);\n+\n+      Expression* gcdata = Expression::make_ptrmask_symbol(type);\n+      init->push_back(gcdata);\n+\n       Expression* root_ctor =\n \tExpression::make_struct_composite_literal(root_type, init, eloc);\n       roots_init->push_back(root_ctor);\n     }\n \n-  // The list ends with a NULL entry.\n-\n-  Expression_list* null_init = new Expression_list();\n-  Expression* nil = Expression::make_nil(builtin_loc);\n-  null_init->push_back(nil);\n-\n-  Expression *zero = Expression::make_integer_ul(0, NULL, builtin_loc);\n-  null_init->push_back(zero);\n-\n-  Expression* null_root_ctor =\n-      Expression::make_struct_composite_literal(root_type, null_init,\n-                                                builtin_loc);\n-  roots_init->push_back(null_root_ctor);\n-\n   // Build a constructor for the struct.\n \n   Expression_list* root_list_init = new Expression_list();\n-  root_list_init->push_back(nil);\n+  root_list_init->push_back(Expression::make_nil(builtin_loc));\n+  root_list_init->push_back(Expression::make_integer_ul(roots_len, int_type,\n+\t\t\t\t\t\t\tbuiltin_loc));\n \n   Expression* roots_ctor =\n       Expression::make_array_composite_literal(root_array_type, roots_init,\n@@ -1216,24 +1235,31 @@ sort_var_inits(Gogo* gogo, Var_inits* var_inits)\n     }\n \n   // VAR_INITS is in the correct order.  For each VAR in VAR_INITS,\n-  // check for a loop of VAR on itself.  We only do this if\n-  // INIT is not NULL and there is no dependency; when INIT is\n-  // NULL, it means that PREINIT sets VAR, which we will\n+  // check for a loop of VAR on itself.\n   // interpret as a loop.\n   for (Var_inits::const_iterator p = var_inits->begin();\n        p != var_inits->end();\n        ++p)\n-    {\n-      Named_object* var = p->var();\n-      Expression* init = var->var_value()->init();\n-      Block* preinit = var->var_value()->preinit();\n-      Named_object* dep = gogo->var_depends_on(var->var_value());\n-      if (init != NULL && dep == NULL\n-\t  && expression_requires(init, preinit, NULL, var))\n-\tgo_error_at(var->location(),\n-\t\t    \"initialization expression for %qs depends upon itself\",\n-\t\t    var->message_name().c_str());\n-    }\n+    gogo->check_self_dep(p->var());\n+}\n+\n+// Give an error if the initialization expression for VAR depends on\n+// itself.  We only check if INIT is not NULL and there is no\n+// dependency; when INIT is NULL, it means that PREINIT sets VAR,\n+// which we will interpret as a loop.\n+\n+void\n+Gogo::check_self_dep(Named_object* var)\n+{\n+  Expression* init = var->var_value()->init();\n+  Block* preinit = var->var_value()->preinit();\n+  Named_object* dep = this->var_depends_on(var->var_value());\n+  if (init != NULL\n+      && dep == NULL\n+      && expression_requires(init, preinit, NULL, var))\n+    go_error_at(var->location(),\n+\t\t\"initialization expression for %qs depends upon itself\",\n+\t\tvar->message_name().c_str());\n }\n \n // Write out the global definitions.\n@@ -1425,8 +1451,18 @@ Gogo::write_globals()\n \t      var_inits.push_back(Var_init(no, zero_stmt));\n \t    }\n \n+\t  // Collect a list of all global variables with pointers,\n+\t  // to register them for the garbage collector.\n \t  if (!is_sink && var->type()->has_pointer())\n-\t    var_gc.push_back(no);\n+\t    {\n+\t      // Avoid putting runtime.gcRoots itself on the list.\n+\t      if (this->compiling_runtime()\n+\t\t  && this->package_name() == \"runtime\"\n+\t\t  && Gogo::unpack_hidden_name(no->name()) == \"gcRoots\")\n+\t\t;\n+\t      else\n+\t\tvar_gc.push_back(no);\n+\t    }\n \t}\n     }\n \n@@ -3584,26 +3620,26 @@ class Order_eval : public Traverse\n // Implement the order of evaluation rules for a statement.\n \n int\n-Order_eval::statement(Block* block, size_t* pindex, Statement* s)\n+Order_eval::statement(Block* block, size_t* pindex, Statement* stmt)\n {\n   // FIXME: This approach doesn't work for switch statements, because\n   // we add the new statements before the whole switch when we need to\n   // instead add them just before the switch expression.  The right\n   // fix is probably to lower switch statements with nonconstant cases\n   // to a series of conditionals.\n-  if (s->switch_statement() != NULL)\n+  if (stmt->switch_statement() != NULL)\n     return TRAVERSE_CONTINUE;\n \n   Find_eval_ordering find_eval_ordering;\n \n   // If S is a variable declaration, then ordinary traversal won't do\n   // anything.  We want to explicitly traverse the initialization\n   // expression if there is one.\n-  Variable_declaration_statement* vds = s->variable_declaration_statement();\n+  Variable_declaration_statement* vds = stmt->variable_declaration_statement();\n   Expression* init = NULL;\n   Expression* orig_init = NULL;\n   if (vds == NULL)\n-    s->traverse_contents(&find_eval_ordering);\n+    stmt->traverse_contents(&find_eval_ordering);\n   else\n     {\n       init = vds->var()->var_value()->init();\n@@ -3636,7 +3672,7 @@ Order_eval::statement(Block* block, size_t* pindex, Statement* s)\n   // usually leave it in place.\n   if (c == 1)\n     {\n-      switch (s->classification())\n+      switch (stmt->classification())\n \t{\n \tcase Statement::STATEMENT_ASSIGNMENT:\n \t  // For an assignment statement, we need to evaluate an\n@@ -3653,7 +3689,7 @@ Order_eval::statement(Block* block, size_t* pindex, Statement* s)\n \t    // move.  We need to move any subexpressions in case they\n \t    // are themselves call statements that require passing a\n \t    // closure.\n-\t    Expression* expr = s->expression_statement()->expr();\n+\t    Expression* expr = stmt->expression_statement()->expr();\n \t    if (expr->call_expression() != NULL\n \t\t&& expr->call_expression()->result_count() == 0)\n \t      break;\n@@ -3666,7 +3702,8 @@ Order_eval::statement(Block* block, size_t* pindex, Statement* s)\n \t}\n     }\n \n-  bool is_thunk = s->thunk_statement() != NULL;\n+  bool is_thunk = stmt->thunk_statement() != NULL;\n+  Expression_statement* es = stmt->expression_statement();\n   for (Find_eval_ordering::const_iterator p = find_eval_ordering.begin();\n        p != find_eval_ordering.end();\n        ++p)\n@@ -3697,9 +3734,14 @@ Order_eval::statement(Block* block, size_t* pindex, Statement* s)\n           //\n           // Since a given call expression can be shared by multiple\n           // Call_result_expressions, avoid hoisting the call the\n-          // second time we see it here.\n+          // second time we see it here. In addition, don't try to\n+          // hoist the top-level multi-return call in the statement,\n+          // since doing this would result a tree with more than one copy\n+          // of the call.\n           if (this->remember_expression(*pexpr))\n             s = NULL;\n+          else if (es != NULL && *pexpr == es->expr())\n+            s = NULL;\n           else\n             s = Statement::make_statement(*pexpr, true);\n         }\n@@ -4448,9 +4490,7 @@ Expression*\n Gogo::allocate_memory(Type* type, Location location)\n {\n   Expression* td = Expression::make_type_descriptor(type, location);\n-  Expression* size =\n-    Expression::make_type_info(type, Expression::TYPE_INFO_SIZE);\n-  return Runtime::make_call(Runtime::NEW, location, 2, td, size);\n+  return Runtime::make_call(Runtime::NEW, location, 1, td);\n }\n \n // Traversal class used to check for return statements.\n@@ -4502,7 +4542,7 @@ Gogo::do_exports()\n {\n   // For now we always stream to a section.  Later we may want to\n   // support streaming to a separate file.\n-  Stream_to_section stream;\n+  Stream_to_section stream(this->backend());\n \n   // Write out either the prefix or pkgpath depending on how we were\n   // invoked.\n@@ -6696,11 +6736,19 @@ Variable::get_backend_variable(Gogo* gogo, Named_object* function,\n                   asm_name.append(n);\n                 }\n \t      asm_name = go_encode_id(asm_name);\n+\n+\t      bool is_hidden = Gogo::is_hidden_name(name);\n+\t      // Hack to export runtime.writeBarrier.  FIXME.\n+\t      // This is because go:linkname doesn't work on variables.\n+\t      if (gogo->compiling_runtime()\n+\t\t  && var_name == \"runtime.writeBarrier\")\n+\t\tis_hidden = false;\n+\n \t      bvar = backend->global_variable(var_name,\n \t\t\t\t\t      asm_name,\n \t\t\t\t\t      btype,\n \t\t\t\t\t      package != NULL,\n-\t\t\t\t\t      Gogo::is_hidden_name(name),\n+\t\t\t\t\t      is_hidden,\n \t\t\t\t\t      this->in_unique_section_,\n \t\t\t\t\t      this->location_);\n \t    }"}, {"sha": "994f23399417af61a5b3778254288002a6fb7669", "filename": "gcc/go/gofrontend/gogo.h", "status": "modified", "additions": 37, "deletions": 2, "changes": 39, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fgogo.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fgogo.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fgogo.h?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -585,7 +585,10 @@ class Gogo\n   // variable initializers that would otherwise not be seen.\n   void\n   add_gc_root(Expression* expr)\n-  { this->gc_roots_.push_back(expr); }\n+  {\n+    this->set_need_init_fn();\n+    this->gc_roots_.push_back(expr);\n+  }\n \n   // Traverse the tree.  See the Traverse class.\n   void\n@@ -693,6 +696,23 @@ class Gogo\n   void\n   order_evaluations();\n \n+  // Add write barriers as needed.\n+  void\n+  add_write_barriers();\n+\n+  // Return whether an assignment that sets LHS to RHS needs a write\n+  // barrier.\n+  bool\n+  assign_needs_write_barrier(Expression* lhs);\n+\n+  // Return an assignment that sets LHS to RHS using a write barrier.\n+  // This returns an if statement that checks whether write barriers\n+  // are enabled.  If not, it does LHS = RHS, otherwise it calls the\n+  // appropriate write barrier function.\n+  Statement*\n+  assign_with_write_barrier(Function*, Block*, Statement_inserter*,\n+\t\t\t    Expression* lhs, Expression* rhs, Location);\n+\n   // Flatten parse tree.\n   void\n   flatten();\n@@ -737,6 +757,10 @@ class Gogo\n   named_types_are_converted() const\n   { return this->named_types_are_converted_; }\n \n+  // Give an error if the initialization of VAR depends on itself.\n+  void\n+  check_self_dep(Named_object*);\n+\n   // Write out the global values.\n   void\n   write_globals();\n@@ -805,6 +829,12 @@ class Gogo\n                    std::vector<Bstatement*>&,\n                    Bfunction* init_bfunction);\n \n+  Named_object*\n+  write_barrier_variable();\n+\n+  Statement*\n+  check_write_barrier(Block*, Statement*, Statement*);\n+\n   // Type used to map import names to packages.\n   typedef std::map<std::string, Package*> Imports;\n \n@@ -1096,6 +1126,11 @@ class Function\n   set_asm_name(const std::string& asm_name)\n   { this->asm_name_ = asm_name; }\n \n+  // Return the pragmas for this function.\n+  unsigned int\n+  pragmas() const\n+  { return this->pragmas_; }\n+\n   // Set the pragmas for this function.\n   void\n   set_pragmas(unsigned int pragmas)\n@@ -1648,7 +1683,7 @@ class Variable\n   set_is_used()\n   { this->is_used_ = true; }\n \n-  // Clear the initial value; used for error handling.\n+  // Clear the initial value; used for error handling and write barriers.\n   void\n   clear_init()\n   { this->init_ = NULL; }"}, {"sha": "a6d54037ce788c731893533c558320baf7b7538b", "filename": "gcc/go/gofrontend/import-archive.cc", "status": "modified", "additions": 314, "deletions": 61, "changes": 375, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fimport-archive.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fimport-archive.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fimport-archive.cc?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -25,8 +25,33 @@ static const char armagt[] =\n   '!', '<', 't', 'h', 'i', 'n', '>', '\\n'\n };\n \n+static const char armagb[] =\n+{\n+  '<', 'b', 'i', 'g', 'a', 'f', '>', '\\n'\n+};\n+\n static const char arfmag[2] = { '`', '\\n' };\n \n+// Archive fixed length header for AIX big format.\n+\n+struct Archive_fl_header\n+{\n+  // Archive magic string.\n+  char fl_magic[8];\n+  // Offset to member table.\n+  char fl_memoff[20];\n+  // Offset to global symbol table.\n+  char fl_gstoff[20];\n+  // Offset to global symbol table for 64-bit objects.\n+  char fl_gst64off[20];\n+  // Offset to first archive member.\n+  char fl_fstmoff[20];\n+  // Offset to last archive member.\n+  char fl_lstmoff[20];\n+  // Offset to first member on free list.\n+  char fl_freeoff[20];\n+};\n+\n // The header of an entry in an archive.  This is all readable text,\n // padded with spaces where necesary.\n \n@@ -48,6 +73,29 @@ struct Archive_header\n   char ar_fmag[2];\n };\n \n+// The header of an entry in an AIX big archive.\n+// This is followed by ar_namlen bytes + 2 bytes for arfmag.\n+\n+struct Archive_big_header\n+{\n+  // The file size in decimal.\n+  char ar_size[20];\n+  // The next member offset in decimal.\n+  char ar_nxtmem[20];\n+  // The previous member offset in decimal.\n+  char ar_prvmem[20];\n+  // The file modification time in decimal.\n+  char ar_date[12];\n+  // The user's UID in decimal.\n+  char ar_uid[12];\n+  // The user's GID in decimal.\n+  char ar_gid[12];\n+  // The file mode in octal.\n+  char ar_mode[12];\n+  // The file name length in decimal.\n+  char ar_namlen[4];\n+};\n+\n // The functions in this file extract Go export data from an archive.\n \n const int Import::archive_magic_len;\n@@ -59,7 +107,8 @@ bool\n Import::is_archive_magic(const char* bytes)\n {\n   return (memcmp(bytes, armag, Import::archive_magic_len) == 0\n-\t  || memcmp(bytes, armagt, Import::archive_magic_len) == 0);\n+\t  || memcmp(bytes, armagt, Import::archive_magic_len) == 0\n+\t  || memcmp(bytes, armagb, Import::archive_magic_len) == 0);\n }\n \n // An object used to read an archive file.\n@@ -68,8 +117,9 @@ class Archive_file\n {\n  public:\n   Archive_file(const std::string& filename, int fd, Location location)\n-    : filename_(filename), fd_(fd), filesize_(-1), extended_names_(),\n-      is_thin_archive_(false), location_(location), nested_archives_()\n+    : filename_(filename), fd_(fd), filesize_(-1), first_member_offset_(0),\n+      extended_names_(), is_thin_archive_(false), is_big_archive_(false),\n+      location_(location), nested_archives_()\n   { }\n \n   // Initialize.\n@@ -86,11 +136,21 @@ class Archive_file\n   filesize() const\n   { return this->filesize_; }\n \n+  // Return the offset of the first member.\n+  off_t\n+  first_member_offset() const\n+  { return this->first_member_offset_; }\n+\n   // Return whether this is a thin archive.\n   bool\n   is_thin_archive() const\n   { return this->is_thin_archive_; }\n \n+  // Return whether this is a big archive.\n+  bool\n+  is_big_archive() const\n+  { return this->is_big_archive_; }\n+\n   // Return the location of the import statement.\n   Location\n   location() const\n@@ -100,10 +160,15 @@ class Archive_file\n   bool\n   read(off_t offset, off_t size, char*);\n \n-  // Read the archive header at OFF, setting *PNAME, *SIZE, and\n-  // *NESTED_OFF.\n+  // Parse a decimal in readable text.\n   bool\n-  read_header(off_t off, std::string* pname, off_t* size, off_t* nested_off);\n+  parse_decimal(const char* str, off_t size, long* res) const;\n+\n+  // Read the archive header at OFF, setting *PNAME, *SIZE,\n+  // *NESTED_OFF and *NEXT_OFF.\n+  bool\n+  read_header(off_t off, std::string* pname, off_t* size, off_t* nested_off,\n+              off_t* next_off);\n \n   // Interpret the header of HDR, the header of the archive member at\n   // file offset OFF.  Return whether it succeeded.  Set *SIZE to the\n@@ -120,6 +185,25 @@ class Archive_file\n \t\t      std::string* memname);\n \n  private:\n+  // Initialize a big archive (AIX)\n+  bool\n+  initialize_big_archive();\n+\n+  // Initialize a normal archive\n+  bool\n+  initialize_archive();\n+\n+  // Read the big archive header at OFF, setting *PNAME, *SIZE and *NEXT_OFF.\n+  bool\n+  read_big_archive_header(off_t off, std::string* pname,\n+                          off_t* size, off_t* next_off);\n+\n+  // Read the normal archive header at OFF, setting *PNAME, *SIZE,\n+  // *NESTED_OFF and *NEXT_OFF.\n+  bool\n+  read_archive_header(off_t off, std::string* pname, off_t* size,\n+                      off_t* nested_off, off_t* next_off);\n+\n   // For keeping track of open nested archives in a thin archive file.\n   typedef std::map<std::string, Archive_file*> Nested_archive_table;\n \n@@ -129,10 +213,14 @@ class Archive_file\n   int fd_;\n   // The file size;\n   off_t filesize_;\n+  // The first member offset;\n+  off_t first_member_offset_;\n   // The extended name table.\n   std::string extended_names_;\n   // Whether this is a thin archive.\n   bool is_thin_archive_;\n+  // Whether this is a big archive.\n+  bool is_big_archive_;\n   // The location of the import statements.\n   Location location_;\n   // Table of nested archives.\n@@ -157,9 +245,60 @@ Archive_file::initialize()\n       go_error_at(this->location_, \"%s: %m\", this->filename_.c_str());\n       return false;\n     }\n-  this->is_thin_archive_ = memcmp(buf, armagt, sizeof(armagt)) == 0;\n+  if (memcmp(buf, armagt, sizeof(armagt)) == 0)\n+    this->is_thin_archive_ = true;\n+  else if (memcmp(buf, armagb, sizeof(armagb)) == 0)\n+    this->is_big_archive_ = true;\n \n-  if (this->filesize_ == sizeof(armag))\n+  if (this->is_big_archive_)\n+    return this->initialize_big_archive();\n+  else\n+    return this->initialize_archive();\n+}\n+\n+// Initialize a big archive (AIX).\n+\n+bool\n+Archive_file::initialize_big_archive()\n+{\n+  Archive_fl_header flhdr;\n+\n+  // Read the fixed length header.\n+  if (::lseek(this->fd_, 0, SEEK_SET) < 0\n+      || ::read(this->fd_, &flhdr, sizeof(flhdr)) != sizeof(flhdr))\n+    {\n+      go_error_at(this->location_, \"%s: could not read archive header\",\n+                  this->filename_.c_str());\n+      return false;\n+    }\n+\n+  // Parse offset of the first member.\n+  long off;\n+  if (!this->parse_decimal(flhdr.fl_fstmoff, sizeof(flhdr.fl_fstmoff), &off))\n+    {\n+      char* buf = new char[sizeof(flhdr.fl_fstmoff) + 1];\n+      memcpy(buf, flhdr.fl_fstmoff, sizeof(flhdr.fl_fstmoff));\n+      go_error_at(this->location_,\n+                  (\"%s: malformed first member offset in archive header\"\n+                   \" (expected decimal, got %s)\"),\n+                  this->filename_.c_str(), buf);\n+      delete[] buf;\n+      return false;\n+    }\n+  if (off == 0) // Empty archive.\n+    this->first_member_offset_ = this->filesize_;\n+  else\n+    this->first_member_offset_ = off;\n+  return true;\n+}\n+\n+// Initialize a normal archive.\n+\n+bool\n+Archive_file::initialize_archive()\n+{\n+  this->first_member_offset_ = sizeof(armag);\n+  if (this->first_member_offset_ == this->filesize_)\n     {\n       // Empty archive.\n       return true;\n@@ -168,15 +307,14 @@ Archive_file::initialize()\n   // Look for the extended name table.\n   std::string filename;\n   off_t size;\n-  if (!this->read_header(sizeof(armagt), &filename, &size, NULL))\n+  off_t next_off;\n+  if (!this->read_header(this->first_member_offset_, &filename,\n+                         &size, NULL, &next_off))\n     return false;\n   if (filename.empty())\n     {\n       // We found the symbol table.\n-      off_t off = sizeof(armagt) + sizeof(Archive_header) + size;\n-      if ((off & 1) != 0)\n-\t++off;\n-      if (!this->read_header(off, &filename, &size, NULL))\n+      if (!this->read_header(next_off, &filename, &size, NULL, NULL))\n \tfilename.clear();\n     }\n   if (filename == \"/\")\n@@ -210,19 +348,142 @@ Archive_file::read(off_t offset, off_t size, char* buf)\n   return true;\n }\n \n+// Parse a decimal in readable text.\n+\n+bool\n+Archive_file::parse_decimal(const char* str, off_t size, long* res) const\n+{\n+  char* buf = new char[size + 1];\n+  memcpy(buf, str, size);\n+  char* ps = buf + size;\n+  while (ps > buf && ps[-1] == ' ')\n+    --ps;\n+  *ps = '\\0';\n+\n+  errno = 0;\n+  char* end;\n+  *res = strtol(buf, &end, 10);\n+  if (*end != '\\0'\n+      || *res < 0\n+      || (*res == LONG_MAX && errno == ERANGE))\n+    {\n+      delete[] buf;\n+      return false;\n+    }\n+  delete[] buf;\n+  return true;\n+}\n+\n // Read the header at OFF.  Set *PNAME to the name, *SIZE to the size,\n-// and *NESTED_OFF to the nested offset.\n+// *NESTED_OFF to the nested offset, and *NEXT_OFF to the next member offset.\n \n bool\n Archive_file::read_header(off_t off, std::string* pname, off_t* size,\n-\t\t\t  off_t* nested_off)\n+\t\t\t  off_t* nested_off, off_t* next_off)\n {\n-  Archive_header hdr;\n   if (::lseek(this->fd_, off, SEEK_SET) < 0)\n     {\n       go_error_at(this->location_, \"%s: %m\", this->filename_.c_str());\n       return false;\n     }\n+  if (this->is_big_archive_)\n+    return this->read_big_archive_header(off, pname, size, next_off);\n+  else\n+    return this->read_archive_header(off, pname, size, nested_off, next_off);\n+}\n+\n+// Read the big archive header at OFF, setting *PNAME, *SIZE and *NEXT_OFF.\n+\n+bool\n+Archive_file::read_big_archive_header(off_t off, std::string* pname,\n+                                      off_t* size, off_t* next_off)\n+{\n+  Archive_big_header hdr;\n+  ssize_t got;\n+\n+  got = ::read(this->fd_, &hdr, sizeof hdr);\n+  if (got != sizeof hdr)\n+    {\n+      if (got < 0)\n+        go_error_at(this->location_, \"%s: %m\", this->filename_.c_str());\n+      else if (got > 0)\n+        go_error_at(this->location_, \"%s: short entry header at %ld\",\n+                    this->filename_.c_str(), static_cast<long>(off));\n+      else\n+        go_error_at(this->location_, \"%s: unexpected EOF at %ld\",\n+                    this->filename_.c_str(), static_cast<long>(off));\n+    }\n+\n+  long local_size;\n+  if (!this->parse_decimal(hdr.ar_size, sizeof(hdr.ar_size), &local_size))\n+    {\n+      char* buf = new char[sizeof(hdr.ar_size) + 1];\n+      memcpy(buf, hdr.ar_size, sizeof(hdr.ar_size));\n+      go_error_at(this->location_,\n+                  (\"%s: malformed ar_size in entry header at %ld\"\n+                   \" (expected decimal, got %s)\"),\n+                  this->filename_.c_str(), static_cast<long>(off), buf);\n+      delete[] buf;\n+      return false;\n+    }\n+  *size = local_size;\n+\n+  long namlen;\n+  if (!this->parse_decimal(hdr.ar_namlen, sizeof(hdr.ar_namlen), &namlen))\n+    {\n+      char* buf = new char[sizeof(hdr.ar_namlen) + 1];\n+      memcpy(buf, hdr.ar_namlen, sizeof(hdr.ar_namlen));\n+      go_error_at(this->location_,\n+                  (\"%s: malformed ar_namlen in entry header at %ld\"\n+                   \" (expected decimal, got %s)\"),\n+                  this->filename_.c_str(), static_cast<long>(off), buf);\n+      delete[] buf;\n+      return false;\n+    }\n+  // Read member name following member header.\n+  char* rdbuf = new char[namlen];\n+  got = ::read(this->fd_, rdbuf, namlen);\n+  if (got != namlen)\n+    {\n+      go_error_at(this->location_,\n+                  \"%s: malformed member name in entry header at %ld\",\n+                  this->filename_.c_str(), static_cast<long>(off));\n+      delete[] rdbuf;\n+      return false;\n+    }\n+  pname->assign(rdbuf, namlen);\n+  delete[] rdbuf;\n+\n+  long local_next_off;\n+  if (!this->parse_decimal(hdr.ar_nxtmem, sizeof(hdr.ar_nxtmem), &local_next_off))\n+    {\n+      char* buf = new char[sizeof(hdr.ar_nxtmem) + 1];\n+      memcpy(buf, hdr.ar_nxtmem, sizeof(hdr.ar_nxtmem));\n+      go_error_at(this->location_,\n+                  (\"%s: malformed ar_nxtmem in entry header at %ld\"\n+                   \" (expected decimal, got %s)\"),\n+                  this->filename_.c_str(), static_cast<long>(off), buf);\n+      delete[] buf;\n+      return false;\n+    }\n+  if (next_off != NULL)\n+    {\n+      if (local_next_off == 0) // Last member.\n+        *next_off = this->filesize_;\n+      else\n+        *next_off = local_next_off;\n+    }\n+  return true;\n+}\n+\n+// Read the normal archive header at OFF, setting *PNAME, *SIZE,\n+// *NESTED_OFF and *NEXT_OFF.\n+\n+bool\n+Archive_file::read_archive_header(off_t off, std::string* pname, off_t* size,\n+                                  off_t* nested_off, off_t* next_off)\n+{\n+  Archive_header hdr;\n   ssize_t got = ::read(this->fd_, &hdr, sizeof hdr);\n   if (got != sizeof hdr)\n     {\n@@ -240,6 +501,17 @@ Archive_file::read_header(off_t off, std::string* pname, off_t* size,\n     return false;\n   if (nested_off != NULL)\n     *nested_off = local_nested_off;\n+\n+  off_t local_next_off;\n+  local_next_off = off + sizeof(Archive_header);\n+  if (!this->is_thin_archive_ || pname->empty() || *pname == \"/\")\n+    local_next_off += *size;\n+  if ((local_next_off & 1) != 0)\n+    ++local_next_off;\n+  if (local_next_off > this->filesize_) // Last member.\n+    local_next_off = this->filesize_;\n+  if (next_off != NULL)\n+    *next_off = local_next_off;\n   return true;\n }\n \n@@ -258,25 +530,14 @@ Archive_file::interpret_header(const Archive_header* hdr, off_t off,\n       return false;\n     }\n \n-  const int size_string_size = sizeof hdr->ar_size;\n-  char size_string[size_string_size + 1];\n-  memcpy(size_string, hdr->ar_size, size_string_size);\n-  char* ps = size_string + size_string_size;\n-  while (ps > size_string && ps[-1] == ' ')\n-    --ps;\n-  *ps = '\\0';\n-\n-  errno = 0;\n-  char* end;\n-  *size = strtol(size_string, &end, 10);\n-  if (*end != '\\0'\n-      || *size < 0\n-      || (*size == LONG_MAX && errno == ERANGE))\n+  long local_size;\n+  if (!this->parse_decimal(hdr->ar_size, sizeof hdr->ar_size, &local_size))\n     {\n       go_error_at(this->location_, \"%s: malformed archive header size at %lu\",\n \t\t  this->filename_.c_str(), static_cast<unsigned long>(off));\n       return false;\n     }\n+  *size = local_size;\n \n   *nested_off = 0;\n   if (hdr->ar_name[0] != '/')\n@@ -313,6 +574,7 @@ Archive_file::interpret_header(const Archive_header* hdr, off_t off,\n     }\n   else\n     {\n+      char* end;\n       errno = 0;\n       long x = strtol(hdr->ar_name + 1, &end, 10);\n       long y = 0;\n@@ -352,7 +614,17 @@ Archive_file::get_file_and_offset(off_t off, const std::string& hdrname,\n \t\t\t\t  off_t nested_off, int* memfd, off_t* memoff,\n \t\t\t\t  std::string* memname)\n {\n-  if (!this->is_thin_archive_)\n+  if (this->is_big_archive_)\n+    {\n+      *memfd = this->fd_;\n+      *memoff = (off + sizeof(Archive_big_header) + hdrname.length()\n+                 + sizeof(arfmag));\n+      if ((*memoff & 1) != 0)\n+        ++*memoff;\n+      *memname = this->filename_ + '(' + hdrname + ')';\n+      return true;\n+    }\n+  else if (!this->is_thin_archive_)\n     {\n       *memfd = this->fd_;\n       *memoff = off + sizeof(Archive_header);\n@@ -399,7 +671,7 @@ Archive_file::get_file_and_offset(off_t off, const std::string& hdrname,\n       std::string nname;\n       off_t nsize;\n       off_t nnested_off;\n-      if (!nfile->read_header(nested_off, &nname, &nsize, &nnested_off))\n+      if (!nfile->read_header(nested_off, &nname, &nsize, &nnested_off, NULL))\n \treturn false;\n       return nfile->get_file_and_offset(nested_off, nname, nnested_off,\n \t\t\t\t\tmemfd, memoff, memname);\n@@ -453,11 +725,7 @@ class Archive_iterator\n   {\n     if (this->off_ == this->afile_->filesize())\n       return *this;\n-    this->off_ += sizeof(Archive_header);\n-    if (!this->afile_->is_thin_archive())\n-      this->off_ += this->header_.size;\n-    if ((this->off_ & 1) != 0)\n-      ++this->off_;\n+    this->off_ = this->next_off_;\n     this->read_next_header();\n     return *this;\n   }\n@@ -486,6 +754,8 @@ class Archive_iterator\n   Archive_file* afile_;\n   // The current offset in the file.\n   off_t off_;\n+  // The offset of the next member.\n+  off_t next_off_;\n   // The current archive header.\n   Header header_;\n };\n@@ -498,31 +768,16 @@ Archive_iterator::read_next_header()\n   off_t filesize = this->afile_->filesize();\n   while (true)\n     {\n-      if (filesize - this->off_ < static_cast<off_t>(sizeof(Archive_header)))\n-\t{\n-\t  if (filesize != this->off_)\n-\t    {\n-\t      go_error_at(this->afile_->location(),\n-\t\t\t  \"%s: short archive header at %lu\",\n-\t\t\t  this->afile_->filename().c_str(),\n-\t\t\t  static_cast<unsigned long>(this->off_));\n-\t      this->off_ = filesize;\n-\t    }\n-\t  this->header_.off = filesize;\n-\t  return;\n-\t}\n-\n-      char buf[sizeof(Archive_header)];\n-      if (!this->afile_->read(this->off_, sizeof(Archive_header), buf))\n+      if (this->off_ == filesize)\n \t{\n \t  this->header_.off = filesize;\n \t  return;\n \t}\n \n-      const Archive_header* hdr = reinterpret_cast<const Archive_header*>(buf);\n-      if (!this->afile_->interpret_header(hdr, this->off_, &this->header_.name,\n-\t\t\t\t\t  &this->header_.size,\n-\t\t\t\t\t  &this->header_.nested_off))\n+      if (!this->afile_->read_header(this->off_, &this->header_.name,\n+                                     &this->header_.size,\n+                                     &this->header_.nested_off,\n+                                     &this->next_off_))\n \t{\n \t  this->header_.off = filesize;\n \t  return;\n@@ -533,9 +788,7 @@ Archive_iterator::read_next_header()\n       if (!this->header_.name.empty() && this->header_.name != \"/\")\n \treturn;\n \n-      this->off_ += sizeof(Archive_header) + this->header_.size;\n-      if ((this->off_ & 1) != 0)\n-\t++this->off_;\n+      this->off_ = this->next_off_;\n     }\n }\n \n@@ -544,7 +797,7 @@ Archive_iterator::read_next_header()\n Archive_iterator\n archive_begin(Archive_file* afile)\n {\n-  return Archive_iterator(afile, sizeof(armag));\n+  return Archive_iterator(afile, afile->first_member_offset());\n }\n \n // Final iterator."}, {"sha": "635b7febd7732ca36c9b78cdaba8ccdff72d5116", "filename": "gcc/go/gofrontend/runtime.def", "status": "modified", "additions": 11, "deletions": 2, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fruntime.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fruntime.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fruntime.def?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -220,11 +220,11 @@ DEF_GO_RUNTIME(GROWSLICE, \"runtime.growslice\", P3(TYPE, SLICE, INT), R1(SLICE))\n \n \n // Register roots (global variables) for the garbage collector.\n-DEF_GO_RUNTIME(REGISTER_GC_ROOTS, \"__go_register_gc_roots\", P1(POINTER), R0())\n+DEF_GO_RUNTIME(REGISTER_GC_ROOTS, \"runtime.registerGCRoots\", P1(POINTER), R0())\n \n \n // Allocate memory.\n-DEF_GO_RUNTIME(NEW, \"__go_new\", P2(TYPE, UINTPTR), R1(POINTER))\n+DEF_GO_RUNTIME(NEW, \"runtime.newobject\", P1(TYPE), R1(POINTER))\n \n // Start a new goroutine.\n DEF_GO_RUNTIME(GO, \"__go_go\", P2(FUNC_PTR, POINTER), R0())\n@@ -315,6 +315,15 @@ DEF_GO_RUNTIME(IFACEEFACEEQ, \"runtime.ifaceefaceeq\", P2(IFACE, EFACE),\n \t       R1(BOOL))\n \n \n+// Set *dst = src where dst is a pointer to a pointer and src is a pointer.\n+DEF_GO_RUNTIME(WRITEBARRIERPTR, \"runtime.writebarrierptr\",\n+\t       P2(POINTER, POINTER), R0())\n+\n+// Set *dst = *src for an arbitrary type.\n+DEF_GO_RUNTIME(TYPEDMEMMOVE, \"runtime.typedmemmove\",\n+\t       P3(TYPE, POINTER, POINTER), R0())\n+\n+\n // Lock the printer (for print/println).\n DEF_GO_RUNTIME(PRINTLOCK, \"runtime.printlock\", P0(), R0())\n "}, {"sha": "00367ef0553973395b00eed89195ef5eadc06dd1", "filename": "gcc/go/gofrontend/statements.cc", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fstatements.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fstatements.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fstatements.cc?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -510,6 +510,10 @@ Temporary_statement::do_get_backend(Translate_context* context)\n       binit = init->get_backend(context);\n     }\n \n+  if (binit != NULL)\n+    binit = context->backend()->convert_expression(btype, binit,\n+                                                   this->location());\n+\n   Bstatement* statement;\n   this->bvariable_ =\n     context->backend()->temporary_variable(bfunction, context->bblock(),"}, {"sha": "f2056aa62083b0afcf0a82b0fa268e310755d1be", "filename": "gcc/go/gofrontend/types.cc", "status": "modified", "additions": 968, "deletions": 418, "changes": 1386, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Ftypes.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Ftypes.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Ftypes.cc?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -1177,7 +1177,12 @@ Type::type_descriptor_pointer(Gogo* gogo, Location location)\n   Bexpression* var_expr =\n       gogo->backend()->var_expression(t->type_descriptor_var_,\n                                       VE_rvalue, location);\n-  return gogo->backend()->address_expression(var_expr, location);\n+  Bexpression* var_addr =\n+      gogo->backend()->address_expression(var_expr, location);\n+  Type* td_type = Type::make_type_descriptor_type();\n+  Btype* td_btype = td_type->get_backend(gogo);\n+  Btype* ptd_btype = gogo->backend()->pointer_type(td_btype);\n+  return gogo->backend()->convert_expression(ptd_btype, var_addr, location);\n }\n \n // A mapping from unnamed types to type descriptor variables.\n@@ -1395,18 +1400,6 @@ Type::named_type_descriptor(Gogo* gogo, Type* type, Named_type* name)\n   return type->do_type_descriptor(gogo, name);\n }\n \n-// Generate the GC symbol for this TYPE.  VALS is the data so far in this\n-// symbol; extra values will be appended in do_gc_symbol.  OFFSET is the\n-// offset into the symbol where the GC data is located.  STACK_SIZE is the\n-// size of the GC stack when dealing with array types.\n-\n-void\n-Type::gc_symbol(Gogo* gogo, Type* type, Expression_list** vals,\n-\t\tExpression** offset, int stack_size)\n-{\n-  type->do_gc_symbol(gogo, vals, offset, stack_size);\n-}\n-\n // Make a builtin struct type from a list of fields.  The fields are\n // pairs of a name and a type.\n \n@@ -1477,6 +1470,7 @@ Type::make_type_descriptor_type()\n       Location bloc = Linemap::predeclared_location();\n \n       Type* uint8_type = Type::lookup_integer_type(\"uint8\");\n+      Type* pointer_uint8_type = Type::make_pointer_type(uint8_type);\n       Type* uint32_type = Type::lookup_integer_type(\"uint32\");\n       Type* uintptr_type = Type::lookup_integer_type(\"uintptr\");\n       Type* string_type = Type::lookup_string_type();\n@@ -1543,15 +1537,16 @@ Type::make_type_descriptor_type()\n       // The type descriptor type.\n \n       Struct_type* type_descriptor_type =\n-\tType::make_builtin_struct_type(11,\n+\tType::make_builtin_struct_type(12,\n+\t\t\t\t       \"size\", uintptr_type,\n+\t\t\t\t       \"ptrdata\", uintptr_type,\n+\t\t\t\t       \"hash\", uint32_type,\n \t\t\t\t       \"kind\", uint8_type,\n \t\t\t\t       \"align\", uint8_type,\n \t\t\t\t       \"fieldAlign\", uint8_type,\n-\t\t\t\t       \"size\", uintptr_type,\n-\t\t\t\t       \"hash\", uint32_type,\n \t\t\t\t       \"hashfn\", hash_fntype,\n \t\t\t\t       \"equalfn\", equal_fntype,\n-\t\t\t\t       \"gc\", uintptr_type,\n+\t\t\t\t       \"gcdata\", pointer_uint8_type,\n \t\t\t\t       \"string\", pointer_string_type,\n \t\t\t\t       \"\", pointer_uncommon_type,\n \t\t\t\t       \"ptrToThis\",\n@@ -2307,30 +2302,25 @@ Type::type_descriptor_constructor(Gogo* gogo, int runtime_type_kind,\n   const Struct_field_list* fields = td_type->struct_type()->fields();\n \n   Expression_list* vals = new Expression_list();\n-  vals->reserve(9);\n+  vals->reserve(12);\n \n   if (!this->has_pointer())\n     runtime_type_kind |= RUNTIME_TYPE_KIND_NO_POINTERS;\n   if (this->points_to() != NULL)\n     runtime_type_kind |= RUNTIME_TYPE_KIND_DIRECT_IFACE;\n-  Struct_field_list::const_iterator p = fields->begin();\n-  go_assert(p->is_field_name(\"kind\"));\n-  vals->push_back(Expression::make_integer_ul(runtime_type_kind, p->type(),\n-\t\t\t\t\t      bloc));\n-\n-  ++p;\n-  go_assert(p->is_field_name(\"align\"));\n-  Expression::Type_info type_info = Expression::TYPE_INFO_ALIGNMENT;\n-  vals->push_back(Expression::make_type_info(this, type_info));\n+  int64_t ptrsize;\n+  int64_t ptrdata;\n+  if (this->needs_gcprog(gogo, &ptrsize, &ptrdata))\n+    runtime_type_kind |= RUNTIME_TYPE_KIND_GC_PROG;\n \n-  ++p;\n-  go_assert(p->is_field_name(\"fieldAlign\"));\n-  type_info = Expression::TYPE_INFO_FIELD_ALIGNMENT;\n+  Struct_field_list::const_iterator p = fields->begin();\n+  go_assert(p->is_field_name(\"size\"));\n+  Expression::Type_info type_info = Expression::TYPE_INFO_SIZE;\n   vals->push_back(Expression::make_type_info(this, type_info));\n \n   ++p;\n-  go_assert(p->is_field_name(\"size\"));\n-  type_info = Expression::TYPE_INFO_SIZE;\n+  go_assert(p->is_field_name(\"ptrdata\"));\n+  type_info = Expression::TYPE_INFO_DESCRIPTOR_PTRDATA;\n   vals->push_back(Expression::make_type_info(this, type_info));\n \n   ++p;\n@@ -2342,6 +2332,21 @@ Type::type_descriptor_constructor(Gogo* gogo, int runtime_type_kind,\n     h = this->hash_for_method(gogo);\n   vals->push_back(Expression::make_integer_ul(h, p->type(), bloc));\n \n+  ++p;\n+  go_assert(p->is_field_name(\"kind\"));\n+  vals->push_back(Expression::make_integer_ul(runtime_type_kind, p->type(),\n+\t\t\t\t\t      bloc));\n+\n+  ++p;\n+  go_assert(p->is_field_name(\"align\"));\n+  type_info = Expression::TYPE_INFO_ALIGNMENT;\n+  vals->push_back(Expression::make_type_info(this, type_info));\n+\n+  ++p;\n+  go_assert(p->is_field_name(\"fieldAlign\"));\n+  type_info = Expression::TYPE_INFO_FIELD_ALIGNMENT;\n+  vals->push_back(Expression::make_type_info(this, type_info));\n+\n   ++p;\n   go_assert(p->is_field_name(\"hashfn\"));\n   Function_type* hash_fntype = p->type()->function_type();\n@@ -2368,7 +2373,7 @@ Type::type_descriptor_constructor(Gogo* gogo, int runtime_type_kind,\n     vals->push_back(Expression::make_func_reference(equal_fn, NULL, bloc));\n \n   ++p;\n-  go_assert(p->is_field_name(\"gc\"));\n+  go_assert(p->is_field_name(\"gcdata\"));\n   vals->push_back(Expression::make_gc_symbol(this));\n \n   ++p;\n@@ -2413,6 +2418,12 @@ Type::type_descriptor_constructor(Gogo* gogo, int runtime_type_kind,\n   return Expression::make_struct_composite_literal(td_type, vals, bloc);\n }\n \n+// The maximum length of a GC ptrmask bitmap.  This corresponds to the\n+// length used by the gc toolchain, and also appears in\n+// libgo/go/reflect/type.go.\n+\n+static const int64_t max_ptrmask_bytes = 2048;\n+\n // Return a pointer to the Garbage Collection information for this type.\n \n Bexpression*\n@@ -2421,6 +2432,10 @@ Type::gc_symbol_pointer(Gogo* gogo)\n   Type* t = this->forwarded();\n   while (t->named_type() != NULL && t->named_type()->is_alias())\n     t = t->named_type()->real_type()->forwarded();\n+\n+  if (!t->has_pointer())\n+    return gogo->backend()->nil_pointer_expression();\n+\n   if (t->gc_symbol_var_ == NULL)\n     {\n       t->make_gc_symbol_var(gogo);\n@@ -2431,7 +2446,10 @@ Type::gc_symbol_pointer(Gogo* gogo)\n       gogo->backend()->var_expression(t->gc_symbol_var_, VE_rvalue, bloc);\n   Bexpression* addr_expr =\n       gogo->backend()->address_expression(var_expr, bloc);\n-  Btype* ubtype = Type::lookup_integer_type(\"uintptr\")->get_backend(gogo);\n+\n+  Type* uint8_type = Type::lookup_integer_type(\"uint8\");\n+  Type* pointer_uint8_type = Type::make_pointer_type(uint8_type);\n+  Btype* ubtype = pointer_uint8_type->get_backend(gogo);\n   return gogo->backend()->convert_expression(ubtype, addr_expr, bloc);\n }\n \n@@ -2466,10 +2484,20 @@ Type::make_gc_symbol_var(Gogo* gogo)\n       phash = &ins.first->second;\n     }\n \n+  int64_t ptrsize;\n+  int64_t ptrdata;\n+  if (!this->needs_gcprog(gogo, &ptrsize, &ptrdata))\n+    {\n+      this->gc_symbol_var_ = this->gc_ptrmask_var(gogo, ptrsize, ptrdata);\n+      if (phash != NULL)\n+\t*phash = this->gc_symbol_var_;\n+      return;\n+    }\n+\n   std::string sym_name = this->type_descriptor_var_name(gogo, nt) + \"$gc\";\n \n   // Build the contents of the gc symbol.\n-  Expression* sym_init = this->gc_symbol_constructor(gogo);\n+  Expression* sym_init = this->gcprog_constructor(gogo, ptrsize, ptrdata);\n   Btype* sym_btype = sym_init->type()->get_backend(gogo);\n \n   // If the type descriptor for this type is defined somewhere else, so is the\n@@ -2502,28 +2530,13 @@ Type::make_gc_symbol_var(Gogo* gogo)\n       is_common = true;\n     }\n \n-  // The current garbage collector requires that the GC symbol be\n-  // aligned to at least a four byte boundary.  See the use of PRECISE\n-  // and LOOP in libgo/runtime/mgc0.c.\n-  int64_t align;\n-  if (!sym_init->type()->backend_type_align(gogo, &align))\n-    go_assert(saw_errors());\n-  if (align < 4)\n-    align = 4;\n-  else\n-    {\n-      // Use default alignment.\n-      align = 0;\n-    }\n-\n   // Since we are building the GC symbol in this package, we must create the\n   // variable before converting the initializer to its backend representation\n   // because the initializer may refer to the GC symbol for this type.\n   std::string asm_name(go_selectively_encode_id(sym_name));\n   this->gc_symbol_var_ =\n       gogo->backend()->implicit_variable(sym_name, asm_name,\n-\t\t\t\t\t sym_btype, false, true, is_common,\n-\t\t\t\t\t align);\n+\t\t\t\t\t sym_btype, false, true, is_common, 0);\n   if (phash != NULL)\n     *phash = this->gc_symbol_var_;\n \n@@ -2535,50 +2548,658 @@ Type::make_gc_symbol_var(Gogo* gogo)\n \t\t\t\t\t      sym_binit);\n }\n \n-// Return an array literal for the Garbage Collection information for this type.\n+// Return whether this type needs a GC program, and set *PTRDATA to\n+// the size of the pointer data in bytes and *PTRSIZE to the size of a\n+// pointer.\n+\n+bool\n+Type::needs_gcprog(Gogo* gogo, int64_t* ptrsize, int64_t* ptrdata)\n+{\n+  if (!this->backend_type_ptrdata(gogo, ptrdata))\n+    {\n+      go_assert(saw_errors());\n+      return false;\n+    }\n+\n+  Type* voidptr = Type::make_pointer_type(Type::make_void_type());\n+  if (!voidptr->backend_type_size(gogo, ptrsize))\n+    go_unreachable();\n+\n+  return *ptrdata / *ptrsize > max_ptrmask_bytes;\n+}\n+\n+// A simple class used to build a GC ptrmask for a type.\n+\n+class Ptrmask\n+{\n+ public:\n+  Ptrmask(size_t count)\n+    : bits_((count + 7) / 8, 0)\n+  {}\n+\n+  void\n+  set_from(Gogo*, Type*, int64_t ptrsize, int64_t offset);\n+\n+  std::string\n+  symname() const;\n+\n+  Expression*\n+  constructor(Gogo* gogo) const;\n+\n+ private:\n+  void\n+  set(size_t index)\n+  { this->bits_.at(index / 8) |= 1 << (index % 8); }\n+\n+  // The actual bits.\n+  std::vector<unsigned char> bits_;\n+};\n+\n+// Set bits in ptrmask starting from OFFSET based on TYPE.  OFFSET\n+// counts in bytes.  PTRSIZE is the size of a pointer on the target\n+// system.\n+\n+void\n+Ptrmask::set_from(Gogo* gogo, Type* type, int64_t ptrsize, int64_t offset)\n+{\n+  switch (type->base()->classification())\n+    {\n+    default:\n+    case Type::TYPE_NIL:\n+    case Type::TYPE_CALL_MULTIPLE_RESULT:\n+    case Type::TYPE_NAMED:\n+    case Type::TYPE_FORWARD:\n+      go_unreachable();\n+\n+    case Type::TYPE_ERROR:\n+    case Type::TYPE_VOID:\n+    case Type::TYPE_BOOLEAN:\n+    case Type::TYPE_INTEGER:\n+    case Type::TYPE_FLOAT:\n+    case Type::TYPE_COMPLEX:\n+    case Type::TYPE_SINK:\n+      break;\n+\n+    case Type::TYPE_FUNCTION:\n+    case Type::TYPE_POINTER:\n+    case Type::TYPE_MAP:\n+    case Type::TYPE_CHANNEL:\n+      // These types are all a single pointer.\n+      go_assert((offset % ptrsize) == 0);\n+      this->set(offset / ptrsize);\n+      break;\n+\n+    case Type::TYPE_STRING:\n+      // A string starts with a single pointer.\n+      go_assert((offset % ptrsize) == 0);\n+      this->set(offset / ptrsize);\n+      break;\n+\n+    case Type::TYPE_INTERFACE:\n+      // An interface is two pointers.\n+      go_assert((offset % ptrsize) == 0);\n+      this->set(offset / ptrsize);\n+      this->set((offset / ptrsize) + 1);\n+      break;\n+\n+    case Type::TYPE_STRUCT:\n+      {\n+\tif (!type->has_pointer())\n+\t  return;\n+\n+\tconst Struct_field_list* fields = type->struct_type()->fields();\n+\tint64_t soffset = 0;\n+\tfor (Struct_field_list::const_iterator pf = fields->begin();\n+\t     pf != fields->end();\n+\t     ++pf)\n+\t  {\n+\t    int64_t field_align;\n+\t    if (!pf->type()->backend_type_field_align(gogo, &field_align))\n+\t      {\n+\t\tgo_assert(saw_errors());\n+\t\treturn;\n+\t      }\n+\t    soffset = (soffset + (field_align - 1)) &~ (field_align - 1);\n+\n+\t    this->set_from(gogo, pf->type(), ptrsize, offset + soffset);\n+\n+\t    int64_t field_size;\n+\t    if (!pf->type()->backend_type_size(gogo, &field_size))\n+\t      {\n+\t\tgo_assert(saw_errors());\n+\t\treturn;\n+\t      }\n+\t    soffset += field_size;\n+\t  }\n+      }\n+      break;\n+\n+    case Type::TYPE_ARRAY:\n+      if (type->is_slice_type())\n+\t{\n+\t  // A slice starts with a single pointer.\n+\t  go_assert((offset % ptrsize) == 0);\n+\t  this->set(offset / ptrsize);\n+\t  break;\n+\t}\n+      else\n+\t{\n+\t  if (!type->has_pointer())\n+\t    return;\n+\n+\t  int64_t len;\n+\t  if (!type->array_type()->int_length(&len))\n+\t    {\n+\t      go_assert(saw_errors());\n+\t      return;\n+\t    }\n+\n+\t  Type* element_type = type->array_type()->element_type();\n+\t  int64_t ele_size;\n+\t  if (!element_type->backend_type_size(gogo, &ele_size))\n+\t    {\n+\t      go_assert(saw_errors());\n+\t      return;\n+\t    }\n+\n+\t  int64_t eoffset = 0;\n+\t  for (int64_t i = 0; i < len; i++, eoffset += ele_size)\n+\t    this->set_from(gogo, element_type, ptrsize, offset + eoffset);\n+\t  break;\n+\t}\n+    }\n+}\n+\n+// Return a symbol name for this ptrmask.  This is used to coalesce\n+// identical ptrmasks, which are common.  The symbol name must use\n+// only characters that are valid in symbols.  It's nice if it's\n+// short.  We convert it to a base64 string.\n+\n+std::string\n+Ptrmask::symname() const\n+{\n+  const char chars[65] =\n+    \"0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_.\";\n+  go_assert(chars[64] == '\\0');\n+  std::string ret;\n+  unsigned int b = 0;\n+  int remaining = 0;\n+  for (std::vector<unsigned char>::const_iterator p = this->bits_.begin();\n+       p != this->bits_.end();\n+       ++p)\n+    {\n+      b |= *p << remaining;\n+      remaining += 8;\n+      while (remaining >= 6)\n+\t{\n+\t  ret += chars[b & 0x3f];\n+\t  b >>= 6;\n+\t  remaining -= 6;\n+\t}\n+    }\n+  while (remaining > 0)\n+    {\n+      ret += chars[b & 0x3f];\n+      b >>= 6;\n+      remaining -= 6;\n+    }\n+  return ret;\n+}\n+\n+// Return a constructor for this ptrmask.  This will be used to\n+// initialize the runtime ptrmask value.\n \n Expression*\n-Type::gc_symbol_constructor(Gogo* gogo)\n+Ptrmask::constructor(Gogo* gogo) const\n {\n   Location bloc = Linemap::predeclared_location();\n+  Type* byte_type = gogo->lookup_global(\"byte\")->type_value();\n+  Expression* len = Expression::make_integer_ul(this->bits_.size(), NULL,\n+\t\t\t\t\t\tbloc);\n+  Array_type* at = Type::make_array_type(byte_type, len);\n+  Expression_list* vals = new Expression_list();\n+  vals->reserve(this->bits_.size());\n+  for (std::vector<unsigned char>::const_iterator p = this->bits_.begin();\n+       p != this->bits_.end();\n+       ++p)\n+    vals->push_back(Expression::make_integer_ul(*p, byte_type, bloc));\n+  return Expression::make_array_composite_literal(at, vals, bloc);\n+}\n+\n+// The hash table mapping a ptrmask symbol name to the ptrmask variable.\n+Type::GC_gcbits_vars Type::gc_gcbits_vars;\n+\n+// Return a ptrmask variable for a type.  For a type descriptor this\n+// is only used for variables that are small enough to not need a\n+// gcprog, but for a global variable this is used for a variable of\n+// any size.  PTRDATA is the number of bytes of the type that contain\n+// pointer data.  PTRSIZE is the size of a pointer on the target\n+// system.\n+\n+Bvariable*\n+Type::gc_ptrmask_var(Gogo* gogo, int64_t ptrsize, int64_t ptrdata)\n+{\n+  Ptrmask ptrmask(ptrdata / ptrsize);\n+  ptrmask.set_from(gogo, this, ptrsize, 0);\n+  std::string sym_name = \"runtime.gcbits.\" + ptrmask.symname();\n+  Bvariable* bvnull = NULL;\n+  std::pair<GC_gcbits_vars::iterator, bool> ins =\n+    Type::gc_gcbits_vars.insert(std::make_pair(sym_name, bvnull));\n+  if (!ins.second)\n+    {\n+      // We've already built a GC symbol for this set of gcbits.\n+      return ins.first->second;\n+    }\n+\n+  Expression* val = ptrmask.constructor(gogo);\n+  Translate_context context(gogo, NULL, NULL, NULL);\n+  context.set_is_const();\n+  Bexpression* bval = val->get_backend(&context);\n+\n+  std::string asm_name(go_selectively_encode_id(sym_name));\n+  Btype *btype = val->type()->get_backend(gogo);\n+  Bvariable* ret = gogo->backend()->implicit_variable(sym_name, asm_name,\n+\t\t\t\t\t\t      btype, false, true,\n+\t\t\t\t\t\t      true, 0);\n+  gogo->backend()->implicit_variable_set_init(ret, sym_name, btype, false,\n+\t\t\t\t\t      true, true, bval);\n+  ins.first->second = ret;\n+  return ret;\n+}\n \n-  // The common GC Symbol data starts with the width of the type and ends\n-  // with the GC Opcode GC_END.\n-  // However, for certain types, the GC symbol may include extra information\n-  // before the ending opcode, so we pass the expression list into\n-  // Type::gc_symbol to allow it to add extra information as is necessary.\n-  Expression_list* vals = new Expression_list;\n+// A GCProg is used to build a program for the garbage collector.\n+// This is used for types with a lot of pointer data, to reduce the\n+// size of the data in the compiled program.  The program is expanded\n+// at runtime.  For the format, see runGCProg in libgo/go/runtime/mbitmap.go.\n \n-  Type* uintptr_t = Type::lookup_integer_type(\"uintptr\");\n-  // width\n-  vals->push_back(Expression::make_type_info(this,\n-\t\t\t\t\t     Expression::TYPE_INFO_SIZE));\n+class GCProg\n+{\n+ public:\n+  GCProg()\n+    : bytes_(), index_(0), nb_(0)\n+  {}\n \n-  Expression* offset = Expression::make_integer_ul(0, uintptr_t, bloc);\n+  // The number of bits described so far.\n+  int64_t\n+  bit_index() const\n+  { return this->index_; }\n \n-  this->do_gc_symbol(gogo, &vals, &offset, 0);\n+  void\n+  set_from(Gogo*, Type*, int64_t ptrsize, int64_t offset);\n \n-  vals->push_back(Expression::make_integer_ul(GC_END, uintptr_t, bloc));\n+  void\n+  end();\n \n-  Expression* len = Expression::make_integer_ul(vals->size(), NULL,\n-\t\t\t\t\t\tbloc);\n-  Array_type* gc_symbol_type = Type::make_array_type(uintptr_t, len);\n-  gc_symbol_type->set_is_array_incomparable();\n-  return Expression::make_array_composite_literal(gc_symbol_type, vals, bloc);\n+  Expression*\n+  constructor(Gogo* gogo) const;\n+\n+ private:\n+  void\n+  ptr(int64_t);\n+\n+  bool\n+  should_repeat(int64_t, int64_t);\n+\n+  void\n+  repeat(int64_t, int64_t);\n+\n+  void\n+  zero_until(int64_t);\n+\n+  void\n+  lit(unsigned char);\n+\n+  void\n+  varint(int64_t);\n+\n+  void\n+  flushlit();\n+\n+  // Add a byte to the program.\n+  void\n+  byte(unsigned char x)\n+  { this->bytes_.push_back(x); }\n+\n+  // The maximum number of bytes of literal bits.\n+  static const int max_literal = 127;\n+\n+  // The program.\n+  std::vector<unsigned char> bytes_;\n+  // The index of the last bit described.\n+  int64_t index_;\n+  // The current set of literal bits.\n+  unsigned char b_[max_literal];\n+  // The current number of literal bits.\n+  int nb_;\n+};\n+\n+// Set data in gcprog starting from OFFSET based on TYPE.  OFFSET\n+// counts in bytes.  PTRSIZE is the size of a pointer on the target\n+// system.\n+\n+void\n+GCProg::set_from(Gogo* gogo, Type* type, int64_t ptrsize, int64_t offset)\n+{\n+  switch (type->base()->classification())\n+    {\n+    default:\n+    case Type::TYPE_NIL:\n+    case Type::TYPE_CALL_MULTIPLE_RESULT:\n+    case Type::TYPE_NAMED:\n+    case Type::TYPE_FORWARD:\n+      go_unreachable();\n+\n+    case Type::TYPE_ERROR:\n+    case Type::TYPE_VOID:\n+    case Type::TYPE_BOOLEAN:\n+    case Type::TYPE_INTEGER:\n+    case Type::TYPE_FLOAT:\n+    case Type::TYPE_COMPLEX:\n+    case Type::TYPE_SINK:\n+      break;\n+\n+    case Type::TYPE_FUNCTION:\n+    case Type::TYPE_POINTER:\n+    case Type::TYPE_MAP:\n+    case Type::TYPE_CHANNEL:\n+      // These types are all a single pointer.\n+      go_assert((offset % ptrsize) == 0);\n+      this->ptr(offset / ptrsize);\n+      break;\n+\n+    case Type::TYPE_STRING:\n+      // A string starts with a single pointer.\n+      go_assert((offset % ptrsize) == 0);\n+      this->ptr(offset / ptrsize);\n+      break;\n+\n+    case Type::TYPE_INTERFACE:\n+      // An interface is two pointers.\n+      go_assert((offset % ptrsize) == 0);\n+      this->ptr(offset / ptrsize);\n+      this->ptr((offset / ptrsize) + 1);\n+      break;\n+\n+    case Type::TYPE_STRUCT:\n+      {\n+\tif (!type->has_pointer())\n+\t  return;\n+\n+\tconst Struct_field_list* fields = type->struct_type()->fields();\n+\tint64_t soffset = 0;\n+\tfor (Struct_field_list::const_iterator pf = fields->begin();\n+\t     pf != fields->end();\n+\t     ++pf)\n+\t  {\n+\t    int64_t field_align;\n+\t    if (!pf->type()->backend_type_field_align(gogo, &field_align))\n+\t      {\n+\t\tgo_assert(saw_errors());\n+\t\treturn;\n+\t      }\n+\t    soffset = (soffset + (field_align - 1)) &~ (field_align - 1);\n+\n+\t    this->set_from(gogo, pf->type(), ptrsize, offset + soffset);\n+\n+\t    int64_t field_size;\n+\t    if (!pf->type()->backend_type_size(gogo, &field_size))\n+\t      {\n+\t\tgo_assert(saw_errors());\n+\t\treturn;\n+\t      }\n+\t    soffset += field_size;\n+\t  }\n+      }\n+      break;\n+\n+    case Type::TYPE_ARRAY:\n+      if (type->is_slice_type())\n+\t{\n+\t  // A slice starts with a single pointer.\n+\t  go_assert((offset % ptrsize) == 0);\n+\t  this->ptr(offset / ptrsize);\n+\t  break;\n+\t}\n+      else\n+\t{\n+\t  if (!type->has_pointer())\n+\t    return;\n+\n+\t  int64_t len;\n+\t  if (!type->array_type()->int_length(&len))\n+\t    {\n+\t      go_assert(saw_errors());\n+\t      return;\n+\t    }\n+\n+\t  Type* element_type = type->array_type()->element_type();\n+\n+\t  // Flatten array of array to a big array by multiplying counts.\n+\t  while (element_type->array_type() != NULL\n+\t\t && !element_type->is_slice_type())\n+\t    {\n+\t      int64_t ele_len;\n+\t      if (!element_type->array_type()->int_length(&ele_len))\n+\t\t{\n+\t\t  go_assert(saw_errors());\n+\t\t  return;\n+\t\t}\n+\n+\t      len *= ele_len;\n+\t      element_type = element_type->array_type()->element_type();\n+\t    }\n+\n+\t  int64_t ele_size;\n+\t  if (!element_type->backend_type_size(gogo, &ele_size))\n+\t    {\n+\t      go_assert(saw_errors());\n+\t      return;\n+\t    }\n+\n+\t  go_assert(len > 0 && ele_size > 0);\n+\n+\t  if (!this->should_repeat(ele_size / ptrsize, len))\n+\t    {\n+\t      // Cheaper to just emit the bits.\n+\t      int64_t eoffset = 0;\n+\t      for (int64_t i = 0; i < len; i++, eoffset += ele_size)\n+\t\tthis->set_from(gogo, element_type, ptrsize, offset + eoffset);\n+\t    }\n+\t  else\n+\t    {\n+\t      go_assert((offset % ptrsize) == 0);\n+\t      go_assert((ele_size % ptrsize) == 0);\n+\t      this->set_from(gogo, element_type, ptrsize, offset);\n+\t      this->zero_until((offset + ele_size) / ptrsize);\n+\t      this->repeat(ele_size / ptrsize, len - 1);\n+\t    }\n+\n+\t  break;\n+\t}\n+    }\n }\n \n-// Advance the OFFSET of the GC symbol by this type's width.\n+// Emit a 1 into the bit stream of a GC program at the given bit index.\n \n void\n-Type::advance_gc_offset(Expression** offset)\n+GCProg::ptr(int64_t index)\n {\n-  if (this->is_error_type())\n+  go_assert(index >= this->index_);\n+  this->zero_until(index);\n+  this->lit(1);\n+}\n+\n+// Return whether it is worthwhile to use a repeat to describe c\n+// elements of n bits each, compared to just emitting c copies of the\n+// n-bit description.\n+\n+bool\n+GCProg::should_repeat(int64_t n, int64_t c)\n+{\n+  // Repeat if there is more than 1 item and if the total data doesn't\n+  // fit into four bytes.\n+  return c > 1 && c * n > 4 * 8;\n+}\n+\n+// Emit an instruction to repeat the description of the last n words c\n+// times (including the initial description, so c + 1 times in total).\n+\n+void\n+GCProg::repeat(int64_t n, int64_t c)\n+{\n+  if (n == 0 || c == 0)\n+    return;\n+  this->flushlit();\n+  if (n < 128)\n+    this->byte(0x80 | static_cast<unsigned char>(n & 0x7f));\n+  else\n+    {\n+      this->byte(0x80);\n+      this->varint(n);\n+    }\n+  this->varint(c);\n+  this->index_ += n * c;\n+}\n+\n+// Add zeros to the bit stream up to the given index.\n+\n+void\n+GCProg::zero_until(int64_t index)\n+{\n+  go_assert(index >= this->index_);\n+  int64_t skip = index - this->index_;\n+  if (skip == 0)\n     return;\n+  if (skip < 4 * 8)\n+    {\n+      for (int64_t i = 0; i < skip; ++i)\n+\tthis->lit(0);\n+      return;\n+    }\n+  this->lit(0);\n+  this->flushlit();\n+  this->repeat(1, skip - 1);\n+}\n+\n+// Add a single literal bit to the program.\n+\n+void\n+GCProg::lit(unsigned char x)\n+{\n+  if (this->nb_ == GCProg::max_literal)\n+    this->flushlit();\n+  this->b_[this->nb_] = x;\n+  ++this->nb_;\n+  ++this->index_;\n+}\n+\n+// Emit the varint encoding of x.\n+\n+void\n+GCProg::varint(int64_t x)\n+{\n+  go_assert(x >= 0);\n+  while (x >= 0x80)\n+    {\n+      this->byte(0x80 | static_cast<unsigned char>(x & 0x7f));\n+      x >>= 7;\n+    }\n+  this->byte(static_cast<unsigned char>(x & 0x7f));\n+}\n \n+// Flush any pending literal bits.\n+\n+void\n+GCProg::flushlit()\n+{\n+  if (this->nb_ == 0)\n+    return;\n+  this->byte(static_cast<unsigned char>(this->nb_));\n+  unsigned char bits = 0;\n+  for (int i = 0; i < this->nb_; ++i)\n+    {\n+      bits |= this->b_[i] << (i % 8);\n+      if ((i + 1) % 8 == 0)\n+\t{\n+\t  this->byte(bits);\n+\t  bits = 0;\n+\t}\n+    }\n+  if (this->nb_ % 8 != 0)\n+    this->byte(bits);\n+  this->nb_ = 0;\n+}\n+\n+// Mark the end of a GC program.\n+\n+void\n+GCProg::end()\n+{\n+  this->flushlit();\n+  this->byte(0);\n+}\n+\n+// Return an Expression for the bytes in a GC program.\n+\n+Expression*\n+GCProg::constructor(Gogo* gogo) const\n+{\n+  Location bloc = Linemap::predeclared_location();\n+\n+  // The first four bytes are the length of the program in target byte\n+  // order.  Build a struct whose first type is uint32 to make this\n+  // work.\n+\n+  Type* uint32_type = Type::lookup_integer_type(\"uint32\");\n+\n+  Type* byte_type = gogo->lookup_global(\"byte\")->type_value();\n+  Expression* len = Expression::make_integer_ul(this->bytes_.size(), NULL,\n+\t\t\t\t\t\tbloc);\n+  Array_type* at = Type::make_array_type(byte_type, len);\n+\n+  Struct_type* st = Type::make_builtin_struct_type(2, \"len\", uint32_type,\n+\t\t\t\t\t\t   \"bytes\", at);\n+\n+  Expression_list* vals = new Expression_list();\n+  vals->reserve(this->bytes_.size());\n+  for (std::vector<unsigned char>::const_iterator p = this->bytes_.begin();\n+       p != this->bytes_.end();\n+       ++p)\n+    vals->push_back(Expression::make_integer_ul(*p, byte_type, bloc));\n+  Expression* bytes = Expression::make_array_composite_literal(at, vals, bloc);\n+\n+  vals = new Expression_list();\n+  vals->push_back(Expression::make_integer_ul(this->bytes_.size(), uint32_type,\n+\t\t\t\t\t      bloc));\n+  vals->push_back(bytes);\n+\n+  return Expression::make_struct_composite_literal(st, vals, bloc);\n+}\n+\n+// Return a composite literal for the garbage collection program for\n+// this type.  This is only used for types that are too large to use a\n+// ptrmask.\n+\n+Expression*\n+Type::gcprog_constructor(Gogo* gogo, int64_t ptrsize, int64_t ptrdata)\n+{\n   Location bloc = Linemap::predeclared_location();\n-  Expression* width =\n-    Expression::make_type_info(this, Expression::TYPE_INFO_SIZE);\n-  *offset = Expression::make_binary(OPERATOR_PLUS, *offset, width, bloc);\n+\n+  GCProg prog;\n+  prog.set_from(gogo, this, ptrsize, 0);\n+  int64_t offset = prog.bit_index() * ptrsize;\n+  prog.end();\n+\n+  int64_t type_size;\n+  if (!this->backend_type_size(gogo, &type_size))\n+    {\n+      go_assert(saw_errors());\n+      return Expression::make_error(bloc);\n+    }\n+\n+  go_assert(offset >= ptrdata && offset <= type_size);\n+\n+  return prog.constructor(gogo);\n }\n \n // Return a composite literal for the uncommon type information for\n@@ -2862,87 +3483,245 @@ Type::is_backend_type_size_known(Gogo* gogo)\n \t    Numeric_constant nc;\n \t    if (!at->length()->numeric_constant_value(&nc))\n \t      return false;\n-\t    mpz_t ival;\n-\t    if (!nc.to_int(&ival))\n+\t    mpz_t ival;\n+\t    if (!nc.to_int(&ival))\n+\t      return false;\n+\t    mpz_clear(ival);\n+\t    return at->element_type()->is_backend_type_size_known(gogo);\n+\t  }\n+      }\n+\n+    case TYPE_NAMED:\n+      this->named_type()->convert(gogo);\n+      return this->named_type()->is_named_backend_type_size_known();\n+\n+    case TYPE_FORWARD:\n+      {\n+\tForward_declaration_type* fdt = this->forward_declaration_type();\n+\treturn fdt->real_type()->is_backend_type_size_known(gogo);\n+      }\n+\n+    case TYPE_SINK:\n+    case TYPE_CALL_MULTIPLE_RESULT:\n+      go_unreachable();\n+\n+    default:\n+      go_unreachable();\n+    }\n+}\n+\n+// If the size of the type can be determined, set *PSIZE to the size\n+// in bytes and return true.  Otherwise, return false.  This queries\n+// the backend.\n+\n+bool\n+Type::backend_type_size(Gogo* gogo, int64_t *psize)\n+{\n+  if (!this->is_backend_type_size_known(gogo))\n+    return false;\n+  if (this->is_error_type())\n+    return false;\n+  Btype* bt = this->get_backend_placeholder(gogo);\n+  *psize = gogo->backend()->type_size(bt);\n+  if (*psize == -1)\n+    {\n+      if (this->named_type() != NULL)\n+\tgo_error_at(this->named_type()->location(),\n+\t\t \"type %s larger than address space\",\n+\t\t Gogo::message_name(this->named_type()->name()).c_str());\n+      else\n+\tgo_error_at(Linemap::unknown_location(),\n+\t\t    \"type %s larger than address space\",\n+\t\t    this->reflection(gogo).c_str());\n+\n+      // Make this an error type to avoid knock-on errors.\n+      this->classification_ = TYPE_ERROR;\n+      return false;\n+    }\n+  return true;\n+}\n+\n+// If the alignment of the type can be determined, set *PALIGN to\n+// the alignment in bytes and return true.  Otherwise, return false.\n+\n+bool\n+Type::backend_type_align(Gogo* gogo, int64_t *palign)\n+{\n+  if (!this->is_backend_type_size_known(gogo))\n+    return false;\n+  Btype* bt = this->get_backend_placeholder(gogo);\n+  *palign = gogo->backend()->type_alignment(bt);\n+  return true;\n+}\n+\n+// Like backend_type_align, but return the alignment when used as a\n+// field.\n+\n+bool\n+Type::backend_type_field_align(Gogo* gogo, int64_t *palign)\n+{\n+  if (!this->is_backend_type_size_known(gogo))\n+    return false;\n+  Btype* bt = this->get_backend_placeholder(gogo);\n+  *palign = gogo->backend()->type_field_alignment(bt);\n+  return true;\n+}\n+\n+// Get the ptrdata value for a type.  This is the size of the prefix\n+// of the type that contains all pointers.  Store the ptrdata in\n+// *PPTRDATA and return whether we found it.\n+\n+bool\n+Type::backend_type_ptrdata(Gogo* gogo, int64_t* pptrdata)\n+{\n+  *pptrdata = 0;\n+\n+  if (!this->has_pointer())\n+    return true;\n+\n+  if (!this->is_backend_type_size_known(gogo))\n+    return false;\n+\n+  switch (this->classification_)\n+    {\n+    case TYPE_ERROR:\n+      return true;\n+\n+    case TYPE_FUNCTION:\n+    case TYPE_POINTER:\n+    case TYPE_MAP:\n+    case TYPE_CHANNEL:\n+      // These types are nothing but a pointer.\n+      return this->backend_type_size(gogo, pptrdata);\n+\n+    case TYPE_INTERFACE:\n+      // An interface is a struct of two pointers.\n+      return this->backend_type_size(gogo, pptrdata);\n+\n+    case TYPE_STRING:\n+      {\n+\t// A string is a struct whose first field is a pointer, and\n+\t// whose second field is not.\n+\tType* uint8_type = Type::lookup_integer_type(\"uint8\");\n+\tType* ptr = Type::make_pointer_type(uint8_type);\n+\treturn ptr->backend_type_size(gogo, pptrdata);\n+      }\n+\n+    case TYPE_NAMED:\n+    case TYPE_FORWARD:\n+      return this->base()->backend_type_ptrdata(gogo, pptrdata);\n+\n+    case TYPE_STRUCT:\n+      {\n+\tconst Struct_field_list* fields = this->struct_type()->fields();\n+\tint64_t offset = 0;\n+\tconst Struct_field *ptr = NULL;\n+\tint64_t ptr_offset = 0;\n+\tfor (Struct_field_list::const_iterator pf = fields->begin();\n+\t     pf != fields->end();\n+\t     ++pf)\n+\t  {\n+\t    int64_t field_align;\n+\t    if (!pf->type()->backend_type_field_align(gogo, &field_align))\n+\t      return false;\n+\t    offset = (offset + (field_align - 1)) &~ (field_align - 1);\n+\n+\t    if (pf->type()->has_pointer())\n+\t      {\n+\t\tptr = &*pf;\n+\t\tptr_offset = offset;\n+\t      }\n+\n+\t    int64_t field_size;\n+\t    if (!pf->type()->backend_type_size(gogo, &field_size))\n+\t      return false;\n+\t    offset += field_size;\n+\t  }\n+\n+\tif (ptr != NULL)\n+\t  {\n+\t    int64_t ptr_ptrdata;\n+\t    if (!ptr->type()->backend_type_ptrdata(gogo, &ptr_ptrdata))\n \t      return false;\n-\t    mpz_clear(ival);\n-\t    return at->element_type()->is_backend_type_size_known(gogo);\n+\t    *pptrdata = ptr_offset + ptr_ptrdata;\n \t  }\n+\treturn true;\n       }\n \n-    case TYPE_NAMED:\n-      this->named_type()->convert(gogo);\n-      return this->named_type()->is_named_backend_type_size_known();\n+    case TYPE_ARRAY:\n+      if (this->is_slice_type())\n+\t{\n+\t  // A slice is a struct whose first field is a pointer, and\n+\t  // whose remaining fields are not.\n+\t  Type* element_type = this->array_type()->element_type();\n+\t  Type* ptr = Type::make_pointer_type(element_type);\n+\t  return ptr->backend_type_size(gogo, pptrdata);\n+\t}\n+      else\n+\t{\n+\t  Numeric_constant nc;\n+\t  if (!this->array_type()->length()->numeric_constant_value(&nc))\n+\t    return false;\n+\t  int64_t len;\n+\t  if (!nc.to_memory_size(&len))\n+\t    return false;\n \n-    case TYPE_FORWARD:\n-      {\n-\tForward_declaration_type* fdt = this->forward_declaration_type();\n-\treturn fdt->real_type()->is_backend_type_size_known(gogo);\n-      }\n+\t  Type* element_type = this->array_type()->element_type();\n+\t  int64_t ele_size;\n+\t  int64_t ele_ptrdata;\n+\t  if (!element_type->backend_type_size(gogo, &ele_size)\n+\t      || !element_type->backend_type_ptrdata(gogo, &ele_ptrdata))\n+\t    return false;\n+\t  go_assert(ele_size > 0 && ele_ptrdata > 0);\n \n-    case TYPE_SINK:\n-    case TYPE_CALL_MULTIPLE_RESULT:\n-      go_unreachable();\n+\t  *pptrdata = (len - 1) * ele_size + ele_ptrdata;\n+\t  return true;\n+\t}\n \n     default:\n+    case TYPE_VOID:\n+    case TYPE_BOOLEAN:\n+    case TYPE_INTEGER:\n+    case TYPE_FLOAT:\n+    case TYPE_COMPLEX:\n+    case TYPE_SINK:\n+    case TYPE_NIL:\n+    case TYPE_CALL_MULTIPLE_RESULT:\n       go_unreachable();\n     }\n }\n \n-// If the size of the type can be determined, set *PSIZE to the size\n-// in bytes and return true.  Otherwise, return false.  This queries\n-// the backend.\n+// Get the ptrdata value to store in a type descriptor.  This is\n+// normally the same as backend_type_ptrdata, but for a type that is\n+// large enough to use a gcprog we may need to store a different value\n+// if it ends with an array.  If the gcprog uses a repeat descriptor\n+// for the array, and if the array element ends with non-pointer data,\n+// then the gcprog will produce a value that describes the complete\n+// array where the backend ptrdata will omit the non-pointer elements\n+// of the final array element.  This is a subtle difference but the\n+// run time code checks it to verify that it has expanded a gcprog as\n+// expected.\n \n bool\n-Type::backend_type_size(Gogo* gogo, int64_t *psize)\n+Type::descriptor_ptrdata(Gogo* gogo, int64_t* pptrdata)\n {\n-  if (!this->is_backend_type_size_known(gogo))\n-    return false;\n-  if (this->is_error_type())\n+  int64_t backend_ptrdata;\n+  if (!this->backend_type_ptrdata(gogo, &backend_ptrdata))\n     return false;\n-  Btype* bt = this->get_backend_placeholder(gogo);\n-  *psize = gogo->backend()->type_size(bt);\n-  if (*psize == -1)\n-    {\n-      if (this->named_type() != NULL)\n-\tgo_error_at(this->named_type()->location(),\n-\t\t \"type %s larger than address space\",\n-\t\t Gogo::message_name(this->named_type()->name()).c_str());\n-      else\n-\tgo_error_at(Linemap::unknown_location(),\n-\t\t    \"type %s larger than address space\",\n-\t\t    this->reflection(gogo).c_str());\n \n-      // Make this an error type to avoid knock-on errors.\n-      this->classification_ = TYPE_ERROR;\n-      return false;\n+  int64_t ptrsize;\n+  if (!this->needs_gcprog(gogo, &ptrsize, &backend_ptrdata))\n+    {\n+      *pptrdata = backend_ptrdata;\n+      return true;\n     }\n-  return true;\n-}\n-\n-// If the alignment of the type can be determined, set *PALIGN to\n-// the alignment in bytes and return true.  Otherwise, return false.\n-\n-bool\n-Type::backend_type_align(Gogo* gogo, int64_t *palign)\n-{\n-  if (!this->is_backend_type_size_known(gogo))\n-    return false;\n-  Btype* bt = this->get_backend_placeholder(gogo);\n-  *palign = gogo->backend()->type_alignment(bt);\n-  return true;\n-}\n \n-// Like backend_type_align, but return the alignment when used as a\n-// field.\n+  GCProg prog;\n+  prog.set_from(gogo, this, ptrsize, 0);\n+  int64_t offset = prog.bit_index() * ptrsize;\n \n-bool\n-Type::backend_type_field_align(Gogo* gogo, int64_t *palign)\n-{\n-  if (!this->is_backend_type_size_known(gogo))\n-    return false;\n-  Btype* bt = this->get_backend_placeholder(gogo);\n-  *palign = gogo->backend()->type_field_alignment(bt);\n+  go_assert(offset >= backend_ptrdata);\n+  *pptrdata = offset;\n   return true;\n }\n \n@@ -3007,10 +3786,6 @@ class Error_type : public Type\n   do_reflection(Gogo*, std::string*) const\n   { go_assert(saw_errors()); }\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int)\n-  { go_assert(saw_errors()); }\n-\n   void\n   do_mangled_name(Gogo*, std::string* ret) const\n   { ret->push_back('E'); }\n@@ -3049,10 +3824,6 @@ class Void_type : public Type\n   do_reflection(Gogo*, std::string*) const\n   { }\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int)\n-  { }\n-\n   void\n   do_mangled_name(Gogo*, std::string* ret) const\n   { ret->push_back('v'); }\n@@ -3091,9 +3862,6 @@ class Boolean_type : public Type\n   do_reflection(Gogo*, std::string* ret) const\n   { ret->append(\"bool\"); }\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int);\n-\n   void\n   do_mangled_name(Gogo*, std::string* ret) const\n   { ret->push_back('b'); }\n@@ -3114,12 +3882,6 @@ Boolean_type::do_type_descriptor(Gogo* gogo, Named_type* name)\n     }\n }\n \n-// Update the offset of the GC symbol.\n-\n-void\n-Boolean_type::do_gc_symbol(Gogo*, Expression_list**, Expression** offset, int)\n-{ this->advance_gc_offset(offset); }\n-\n Type*\n Type::make_boolean_type()\n {\n@@ -3629,20 +4391,6 @@ String_type::do_reflection(Gogo*, std::string* ret) const\n   ret->append(\"string\");\n }\n \n-// Generate GC symbol for strings.\n-\n-void\n-String_type::do_gc_symbol(Gogo*, Expression_list** vals,\n-\t\t\t  Expression** offset, int)\n-{\n-  Location bloc = Linemap::predeclared_location();\n-  Type* uintptr_type = Type::lookup_integer_type(\"uintptr\");\n-  (*vals)->push_back(Expression::make_integer_ul(GC_STRING, uintptr_type,\n-\t\t\t\t\t\t bloc));\n-  (*vals)->push_back(*offset);\n-  this->advance_gc_offset(offset);\n-}\n-\n // Mangled name of a string type.\n \n void\n@@ -3713,10 +4461,6 @@ class Sink_type : public Type\n   do_reflection(Gogo*, std::string*) const\n   { go_unreachable(); }\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int)\n-  { go_unreachable(); }\n-\n   void\n   do_mangled_name(Gogo*, std::string*) const\n   { go_unreachable(); }\n@@ -4300,22 +5044,6 @@ Function_type::do_reflection(Gogo* gogo, std::string* ret) const\n     }\n }\n \n-// Generate GC symbol for a function type.\n-\n-void\n-Function_type::do_gc_symbol(Gogo*, Expression_list** vals,\n-\t\t\t    Expression** offset, int)\n-{\n-  Location bloc = Linemap::predeclared_location();\n-  Type* uintptr_type = Type::lookup_integer_type(\"uintptr\");\n-\n-  // We use GC_APTR here because we do not currently have a way to describe the\n-  // the type of the possible function closure.  FIXME.\n-  (*vals)->push_back(Expression::make_integer_ul(GC_APTR, uintptr_type, bloc));\n-  (*vals)->push_back(*offset);\n-  this->advance_gc_offset(offset);\n-}\n-\n // Mangled name.\n \n void\n@@ -4718,26 +5446,6 @@ Pointer_type::do_reflection(Gogo* gogo, std::string* ret) const\n   this->append_reflection(this->to_type_, gogo, ret);\n }\n \n-// Generate GC symbol for pointer types.\n-\n-void\n-Pointer_type::do_gc_symbol(Gogo*, Expression_list** vals,\n-\t\t\t   Expression** offset, int)\n-{\n-  Location loc = Linemap::predeclared_location();\n-  Type* uintptr_type = Type::lookup_integer_type(\"uintptr\");\n-\n-  unsigned long opval = this->to_type_->has_pointer() ? GC_PTR : GC_APTR;\n-  (*vals)->push_back(Expression::make_integer_ul(opval, uintptr_type, loc));\n-  (*vals)->push_back(*offset);\n-\n-  if (this->to_type_->has_pointer())\n-    (*vals)->push_back(Expression::make_gc_symbol(this->to_type_));\n-  this->advance_gc_offset(offset);\n-}\n-\n-// Mangled name.\n-\n void\n Pointer_type::do_mangled_name(Gogo* gogo, std::string* ret) const\n {\n@@ -4815,10 +5523,6 @@ class Nil_type : public Type\n   do_reflection(Gogo*, std::string*) const\n   { go_unreachable(); }\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int)\n-  { go_unreachable(); }\n-\n   void\n   do_mangled_name(Gogo*, std::string* ret) const\n   { ret->push_back('n'); }\n@@ -4873,10 +5577,6 @@ class Call_multiple_result_type : public Type\n   do_reflection(Gogo*, std::string*) const\n   { go_assert(saw_errors()); }\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int)\n-  { go_unreachable(); }\n-\n   void\n   do_mangled_name(Gogo*, std::string*) const\n   { go_assert(saw_errors()); }\n@@ -5890,27 +6590,6 @@ Struct_type::do_reflection(Gogo* gogo, std::string* ret) const\n   ret->push_back('}');\n }\n \n-// Generate GC symbol for struct types.\n-\n-void\n-Struct_type::do_gc_symbol(Gogo* gogo, Expression_list** vals,\n-\t\t\t  Expression** offset, int stack_size)\n-{\n-  Location bloc = Linemap::predeclared_location();\n-  const Struct_field_list* sfl = this->fields();\n-  for (Struct_field_list::const_iterator p = sfl->begin();\n-       p != sfl->end();\n-       ++p)\n-    {\n-      Expression* field_offset =\n-  \tExpression::make_struct_field_offset(this, &*p);\n-      Expression* o =\n-  \tExpression::make_binary(OPERATOR_PLUS, *offset, field_offset, bloc);\n-      Type::gc_symbol(gogo, p->type(), vals, &o, stack_size);\n-    }\n-  this->advance_gc_offset(offset);\n-}\n-\n // Mangled name.\n \n void\n@@ -6085,6 +6764,7 @@ Struct_type::can_write_to_c_header(\n   const Struct_field_list* fields = this->fields_;\n   if (fields == NULL || fields->empty())\n     return false;\n+  int sinks = 0;\n   for (Struct_field_list::const_iterator p = fields->begin();\n        p != fields->end();\n        ++p)\n@@ -6093,7 +6773,11 @@ Struct_type::can_write_to_c_header(\n \treturn false;\n       if (!this->can_write_type_to_c_header(p->type(), requires, declare))\n \treturn false;\n+      if (Gogo::message_name(p->field_name()) == \"_\")\n+\tsinks++;\n     }\n+  if (sinks > 1)\n+    return false;\n   return true;\n }\n \n@@ -6360,6 +7044,20 @@ Type::make_struct_type(Struct_field_list* fields,\n \n // Class Array_type.\n \n+// Store the length of an array as an int64_t into *PLEN.  Return\n+// false if the length can not be determined.  This will assert if\n+// called for a slice.\n+\n+bool\n+Array_type::int_length(int64_t* plen)\n+{\n+  go_assert(this->length_ != NULL);\n+  Numeric_constant nc;\n+  if (!this->length_->numeric_constant_value(&nc))\n+    return false;\n+  return nc.to_memory_size(plen);\n+}\n+\n // Whether two array types are identical.\n \n bool\n@@ -6504,6 +7202,38 @@ Array_type::do_verify()\n   return true;\n }\n \n+// Whether the type contains pointers.  This is always true for a\n+// slice.  For an array it is true if the element type has pointers\n+// and the length is greater than zero.\n+\n+bool\n+Array_type::do_has_pointer() const\n+{\n+  if (this->length_ == NULL)\n+    return true;\n+  if (!this->element_type_->has_pointer())\n+    return false;\n+\n+  Numeric_constant nc;\n+  if (!this->length_->numeric_constant_value(&nc))\n+    {\n+      // Error reported elsewhere.\n+      return false;\n+    }\n+\n+  unsigned long val;\n+  switch (nc.to_unsigned_long(&val))\n+    {\n+    case Numeric_constant::NC_UL_VALID:\n+      return val > 0;\n+    case Numeric_constant::NC_UL_BIG:\n+      return true;\n+    default:\n+      // Error reported elsewhere.\n+      return false;\n+    }\n+}\n+\n // Whether we can use memcmp to compare this array.\n \n bool\n@@ -7093,120 +7823,6 @@ Array_type::do_reflection(Gogo* gogo, std::string* ret) const\n   this->append_reflection(this->element_type_, gogo, ret);\n }\n \n-// GC Symbol construction for array types.\n-\n-void\n-Array_type::do_gc_symbol(Gogo* gogo, Expression_list** vals,\n-\t\t\t Expression** offset, int stack_size)\n-{\n-  if (this->length_ == NULL)\n-    this->slice_gc_symbol(gogo, vals, offset, stack_size);\n-  else\n-    this->array_gc_symbol(gogo, vals, offset, stack_size);\n-}\n-\n-// Generate the GC Symbol for a slice.\n-\n-void\n-Array_type::slice_gc_symbol(Gogo* gogo, Expression_list** vals,\n-\t\t\t    Expression** offset, int)\n-{\n-  Location bloc = Linemap::predeclared_location();\n-\n-  // Differentiate between slices with zero-length and non-zero-length values.\n-  Type* element_type = this->element_type();\n-  int64_t element_size;\n-  bool ok = element_type->backend_type_size(gogo, &element_size);\n-  if (!ok) {\n-    go_assert(saw_errors());\n-    element_size = 4;\n-  }\n-\n-  Type* uintptr_type = Type::lookup_integer_type(\"uintptr\");\n-  unsigned long opval = element_size == 0 ? GC_APTR : GC_SLICE;\n-  (*vals)->push_back(Expression::make_integer_ul(opval, uintptr_type, bloc));\n-  (*vals)->push_back(*offset);\n-\n-  if (element_size != 0 && ok)\n-    (*vals)->push_back(Expression::make_gc_symbol(element_type));\n-  this->advance_gc_offset(offset);\n-}\n-\n-// Generate the GC symbol for an array.\n-\n-void\n-Array_type::array_gc_symbol(Gogo* gogo, Expression_list** vals,\n-\t\t\t    Expression** offset, int stack_size)\n-{\n-  Location bloc = Linemap::predeclared_location();\n-\n-  Numeric_constant nc;\n-  unsigned long bound;\n-  if (!this->length_->numeric_constant_value(&nc)\n-      || nc.to_unsigned_long(&bound) == Numeric_constant::NC_UL_NOTINT)\n-    {\n-      go_assert(saw_errors());\n-      return;\n-    }\n-\n-  Btype* pbtype = gogo->backend()->pointer_type(gogo->backend()->void_type());\n-  int64_t pwidth = gogo->backend()->type_size(pbtype);\n-  int64_t iwidth;\n-  bool ok = this->backend_type_size(gogo, &iwidth);\n-  if (!ok)\n-    {\n-      go_assert(saw_errors());\n-      iwidth = 4;\n-    }\n-\n-  Type* element_type = this->element_type();\n-  if (bound < 1 || !element_type->has_pointer())\n-    this->advance_gc_offset(offset);\n-  else if (ok && (bound == 1 || iwidth <= 4 * pwidth))\n-    {\n-      for (unsigned int i = 0; i < bound; ++i)\n-\tType::gc_symbol(gogo, element_type, vals, offset, stack_size);\n-    }\n-  else\n-    {\n-      Type* uintptr_type = Type::lookup_integer_type(\"uintptr\");\n-\n-      if (stack_size < GC_STACK_CAPACITY)\n-  \t{\n-\t  (*vals)->push_back(Expression::make_integer_ul(GC_ARRAY_START,\n-\t\t\t\t\t\t\t uintptr_type, bloc));\n-  \t  (*vals)->push_back(*offset);\n-\t  Expression* uintptr_len =\n-\t    Expression::make_cast(uintptr_type, this->length_, bloc);\n-  \t  (*vals)->push_back(uintptr_len);\n-\n-\t  Expression* width =\n-\t    Expression::make_type_info(element_type,\n-\t\t\t\t       Expression::TYPE_INFO_SIZE);\n-  \t  (*vals)->push_back(width);\n-\n-\t  Expression* offset2 = Expression::make_integer_ul(0, uintptr_type,\n-\t\t\t\t\t\t\t    bloc);\n-\n-\t  Type::gc_symbol(gogo, element_type, vals, &offset2, stack_size + 1);\n-\t  (*vals)->push_back(Expression::make_integer_ul(GC_ARRAY_NEXT,\n-\t\t\t\t\t\t\t uintptr_type, bloc));\n-  \t}\n-      else\n-  \t{\n-\t  (*vals)->push_back(Expression::make_integer_ul(GC_REGION,\n-\t\t\t\t\t\t\t uintptr_type, bloc));\n-\t  (*vals)->push_back(*offset);\n-\n-\t  Expression* width =\n-\t    Expression::make_type_info(this, Expression::TYPE_INFO_SIZE);\n-  \t  (*vals)->push_back(width);\n-\t  (*vals)->push_back(Expression::make_gc_symbol(this));\n-  \t}\n-      this->advance_gc_offset(offset);\n-    }\n-}\n-\n // Mangled name.\n \n void\n@@ -7832,21 +8448,6 @@ Map_type::do_reflection(Gogo* gogo, std::string* ret) const\n   this->append_reflection(this->val_type_, gogo, ret);\n }\n \n-// Generate GC symbol for a map.\n-\n-void\n-Map_type::do_gc_symbol(Gogo*, Expression_list** vals,\n-\t\t       Expression** offset, int)\n-{\n-  // TODO(cmang): Generate GC data for the Map elements.\n-  Location bloc = Linemap::predeclared_location();\n-  Type* uintptr_type = Type::lookup_integer_type(\"uintptr\");\n-\n-  (*vals)->push_back(Expression::make_integer_ul(GC_APTR, uintptr_type, bloc));\n-  (*vals)->push_back(*offset);\n-  this->advance_gc_offset(offset);\n-}\n-\n // Mangled name for a map.\n \n void\n@@ -8017,28 +8618,6 @@ Channel_type::do_reflection(Gogo* gogo, std::string* ret) const\n   this->append_reflection(this->element_type_, gogo, ret);\n }\n \n-// Generate GC symbol for channels.\n-\n-void\n-Channel_type::do_gc_symbol(Gogo*, Expression_list** vals,\n-\t\t\t   Expression** offset, int)\n-{\n-  Location bloc = Linemap::predeclared_location();\n-  Type* uintptr_type = Type::lookup_integer_type(\"uintptr\");\n-\n-  (*vals)->push_back(Expression::make_integer_ul(GC_CHAN_PTR, uintptr_type,\n-\t\t\t\t\t\t bloc));\n-  (*vals)->push_back(*offset);\n- \n-  Type* unsafeptr_type = Type::make_pointer_type(Type::make_void_type());\n-  Expression* type_descriptor =\n-    Expression::make_type_descriptor(this, bloc);\n-  type_descriptor =\n-    Expression::make_unsafe_cast(unsafeptr_type, type_descriptor, bloc);\n-  (*vals)->push_back(type_descriptor);\n-  this->advance_gc_offset(offset);\n-}\n-\n // Mangled name.\n \n void\n@@ -8975,21 +9554,6 @@ Interface_type::do_reflection(Gogo* gogo, std::string* ret) const\n   ret->append(\"}\");\n }\n \n-// Generate GC symbol for interface types.\n-\n-void\n-Interface_type::do_gc_symbol(Gogo*, Expression_list** vals,\n-\t\t\t     Expression** offset, int)\n-{\n-  Location bloc = Linemap::predeclared_location();\n-  Type* uintptr_type = Type::lookup_integer_type(\"uintptr\");\n-\n-  unsigned long opval = this->is_empty() ? GC_EFACE : GC_IFACE;\n-  (*vals)->push_back(Expression::make_integer_ul(opval, uintptr_type, bloc));\n-  (*vals)->push_back(*offset);\n-  this->advance_gc_offset(offset);\n-}\n-\n // Mangled name.\n \n void\n@@ -10464,20 +11028,6 @@ Named_type::append_reflection_type_name(Gogo* gogo, bool use_alias,\n   ret->append(Gogo::unpack_hidden_name(this->named_object_->name()));\n }\n \n-// Generate GC symbol for named types.\n-\n-void\n-Named_type::do_gc_symbol(Gogo* gogo, Expression_list** vals,\n-\t\t\t Expression** offset, int stack)\n-{\n-  if (!this->seen_)\n-    {\n-      this->seen_ = true;\n-      Type::gc_symbol(gogo, this->real_type(), vals, offset, stack);\n-      this->seen_ = false;\n-    }\n-}\n-\n // Get the mangled name.\n \n void"}, {"sha": "e0fcf0cc86ac3d011d0c634e58c7f0dbe1ea3ddf", "filename": "gcc/go/gofrontend/types.h", "status": "modified", "additions": 40, "deletions": 96, "changes": 136, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Ftypes.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Ftypes.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Ftypes.h?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -89,28 +89,6 @@ static const int RUNTIME_TYPE_KIND_DIRECT_IFACE = (1 << 5);\n static const int RUNTIME_TYPE_KIND_GC_PROG = (1 << 6);\n static const int RUNTIME_TYPE_KIND_NO_POINTERS = (1 << 7);\n \n-// GC instruction opcodes.  These must match the values in libgo/runtime/mgc0.h.\n-enum GC_Opcode\n-{\n-  GC_END = 0,     // End of object, loop or subroutine.\n-  GC_PTR,         // A typed pointer.\n-  GC_APTR,        // Pointer to an arbitrary object.\n-  GC_ARRAY_START, // Start an array with a fixed length.\n-  GC_ARRAY_NEXT,  // The next element of an array.\n-  GC_CALL,        // Call a subroutine.\n-  GC_CHAN_PTR,    // Go channel.\n-  GC_STRING,      // Go string.\n-  GC_EFACE,       // interface{}.\n-  GC_IFACE,       // interface{...}.\n-  GC_SLICE,       // Go slice.\n-  GC_REGION,      // A region/part of the current object.\n-\n-  GC_NUM_INSTR    // Number of instruction opcodes\n-};\n-\n-// The GC Stack Capacity must match the value in libgo/runtime/mgc0.h.\n-static const int GC_STACK_CAPACITY = 8;\n-\n // To build the complete list of methods for a named type we need to\n // gather all methods from anonymous fields.  Those methods may\n // require an arbitrary set of indirections and field offsets.  There\n@@ -944,6 +922,15 @@ class Type\n   Bexpression*\n   gc_symbol_pointer(Gogo* gogo);\n \n+  // Return whether this type needs a garbage collection program.\n+  // Sets *PTRSIZE and *PTRDATA.\n+  bool\n+  needs_gcprog(Gogo*, int64_t* ptrsize, int64_t* ptrdata);\n+\n+  // Return a ptrmask variable for this type.\n+  Bvariable*\n+  gc_ptrmask_var(Gogo*, int64_t ptrsize, int64_t ptrdata);\n+\n   // Return the type reflection string for this type.\n   std::string\n   reflection(Gogo*) const;\n@@ -971,6 +958,20 @@ class Type\n   bool\n   backend_type_field_align(Gogo*, int64_t* palign);\n \n+  // Determine the ptrdata size for the backend version of this type:\n+  // the length of the prefix of the type that can contain a pointer\n+  // value.  If it can be determined, set *PPTRDATA to the value in\n+  // bytes and return true.  Otherwise, return false.\n+  bool\n+  backend_type_ptrdata(Gogo*, int64_t* pptrdata);\n+\n+  // Determine the ptrdata size that we are going to set in the type\n+  // descriptor.  This is normally the same as backend_type_ptrdata,\n+  // but differs if we use a gcprog for an array.  The arguments and\n+  // results are as for backend_type_ptrdata.\n+  bool\n+  descriptor_ptrdata(Gogo*, int64_t* pptrdata);\n+\n   // Whether the backend size is known.\n   bool\n   is_backend_type_size_known(Gogo*);\n@@ -1043,9 +1044,6 @@ class Type\n   virtual Expression*\n   do_type_descriptor(Gogo*, Named_type* name) = 0;\n \n-  virtual void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int) = 0;\n-\n   virtual void\n   do_reflection(Gogo*, std::string*) const = 0;\n \n@@ -1101,22 +1099,6 @@ class Type\n   type_descriptor_constructor(Gogo*, int runtime_type_kind, Named_type*,\n \t\t\t      const Methods*, bool only_value_methods);\n \n-  // Generate the GC symbol for this TYPE.  VALS is the data so far in this\n-  // symbol; extra values will be appended in do_gc_symbol.  OFFSET is the\n-  // offset into the symbol where the GC data is located.  STACK_SIZE is the\n-  // size of the GC stack when dealing with array types.\n-  static void\n-  gc_symbol(Gogo*, Type* type, Expression_list** vals, Expression** offset,\n-\t    int stack_size);\n-\n-  // Build a composite literal for the GC symbol of this type.\n-  Expression*\n-  gc_symbol_constructor(Gogo*);\n-\n-  // Advance the OFFSET of the GC symbol by the size of this type.\n-  void\n-  advance_gc_offset(Expression** offset);\n-\n   // For the benefit of child class reflection string generation.\n   void\n   append_reflection(const Type* type, Gogo* gogo, std::string* ret) const\n@@ -1194,6 +1176,11 @@ class Type\n \n   static GC_symbol_vars gc_symbol_vars;\n \n+  // Map ptrmask symbol names to the ptrmask variable.\n+  typedef Unordered_map(std::string, Bvariable*) GC_gcbits_vars;\n+\n+  static GC_gcbits_vars gc_gcbits_vars;\n+\n   // Build the GC symbol for this type.\n   void\n   make_gc_symbol_var(Gogo*);\n@@ -1210,6 +1197,11 @@ class Type\n   bool\n   type_descriptor_defined_elsewhere(Named_type* name, const Package** package);\n \n+  // Make a composite literal for the garbage collection program for\n+  // this type.\n+  Expression*\n+  gcprog_constructor(Gogo*, int64_t ptrsize, int64_t ptrdata);\n+\n   // Build the hash and equality type functions for a type which needs\n   // specific functions.\n   void\n@@ -1605,10 +1597,6 @@ class Integer_type : public Type\n   void\n   do_reflection(Gogo*, std::string*) const;\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression** offset, int)\n-  { this->advance_gc_offset(offset); }\n-\n   void\n   do_mangled_name(Gogo*, std::string*) const;\n \n@@ -1695,10 +1683,6 @@ class Float_type : public Type\n   void\n   do_reflection(Gogo*, std::string*) const;\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression** offset, int)\n-  { this->advance_gc_offset(offset); }\n-\n   void\n   do_mangled_name(Gogo*, std::string*) const;\n \n@@ -1777,10 +1761,6 @@ class Complex_type : public Type\n   void\n   do_reflection(Gogo*, std::string*) const;\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression** offset, int)\n-  { this->advance_gc_offset(offset); }\n-\n   void\n   do_mangled_name(Gogo*, std::string*) const;\n \n@@ -1835,9 +1815,6 @@ class String_type : public Type\n   void\n   do_reflection(Gogo*, std::string*) const;\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int);\n-\n   void\n   do_mangled_name(Gogo*, std::string* ret) const;\n \n@@ -1993,9 +1970,6 @@ class Function_type : public Type\n   void\n   do_reflection(Gogo*, std::string*) const;\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int);\n-\n   void\n   do_mangled_name(Gogo*, std::string*) const;\n \n@@ -2119,9 +2093,6 @@ class Pointer_type : public Type\n   void\n   do_reflection(Gogo*, std::string*) const;\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int);\n-\n   void\n   do_mangled_name(Gogo*, std::string*) const;\n \n@@ -2441,9 +2412,6 @@ class Struct_type : public Type\n   void\n   do_reflection(Gogo*, std::string*) const;\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int);\n-\n   void\n   do_mangled_name(Gogo*, std::string*) const;\n \n@@ -2519,6 +2487,12 @@ class Array_type : public Type\n   length() const\n   { return this->length_; }\n \n+  // Store the length as an int64_t into *PLEN.  Return false if the\n+  // length can not be determined.  This will assert if called for a\n+  // slice.\n+  bool\n+  int_length(int64_t* plen);\n+\n   // Whether this type is identical with T.\n   bool\n   is_identical(const Array_type* t, bool errors_are_identical) const;\n@@ -2583,10 +2557,7 @@ class Array_type : public Type\n   do_verify();\n \n   bool\n-  do_has_pointer() const\n-  {\n-    return this->length_ == NULL || this->element_type_->has_pointer();\n-  }\n+  do_has_pointer() const;\n \n   bool\n   do_compare_is_identity(Gogo*);\n@@ -2613,9 +2584,6 @@ class Array_type : public Type\n   void\n   do_reflection(Gogo*, std::string*) const;\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int);\n-\n   void\n   do_mangled_name(Gogo*, std::string*) const;\n \n@@ -2632,12 +2600,6 @@ class Array_type : public Type\n   Expression*\n   slice_type_descriptor(Gogo*, Named_type*);\n \n-  void\n-  slice_gc_symbol(Gogo*, Expression_list**, Expression**, int);\n-\n-  void\n-  array_gc_symbol(Gogo*, Expression_list**, Expression**, int);\n-\n   // The type of elements of the array.\n   Type* element_type_;\n   // The number of elements.  This may be NULL.\n@@ -2736,9 +2698,6 @@ class Map_type : public Type\n   void\n   do_reflection(Gogo*, std::string*) const;\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int);\n-\n   void\n   do_mangled_name(Gogo*, std::string*) const;\n \n@@ -2851,9 +2810,6 @@ class Channel_type : public Type\n   void\n   do_reflection(Gogo*, std::string*) const;\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int);\n-\n   void\n   do_mangled_name(Gogo*, std::string*) const;\n \n@@ -2998,9 +2954,6 @@ class Interface_type : public Type\n   void\n   do_reflection(Gogo*, std::string*) const;\n \n-  void\n-  do_gc_symbol(Gogo*, Expression_list**, Expression**, int);\n-\n   void\n   do_mangled_name(Gogo*, std::string*) const;\n \n@@ -3306,10 +3259,6 @@ class Named_type : public Type\n   void\n   do_reflection(Gogo*, std::string*) const;\n \n-  void\n-  do_gc_symbol(Gogo* gogo, Expression_list** vals, Expression** offset,\n-\t       int stack);\n-\n   void\n   do_mangled_name(Gogo*, std::string* ret) const;\n \n@@ -3466,11 +3415,6 @@ class Forward_declaration_type : public Type\n   void\n   do_reflection(Gogo*, std::string*) const;\n \n-  void\n-  do_gc_symbol(Gogo* gogo, Expression_list** vals, Expression** offset,\n-\t       int stack_size)\n-  { Type::gc_symbol(gogo, this->real_type(), vals, offset, stack_size); }\n-\n   void\n   do_mangled_name(Gogo*, std::string* ret) const;\n "}, {"sha": "5a49961aba8cf1bfaafa4e1f59509df440e2b061", "filename": "gcc/go/gofrontend/wb.cc", "status": "added", "additions": 465, "deletions": 0, "changes": 465, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fwb.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Fgo%2Fgofrontend%2Fwb.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgo%2Fgofrontend%2Fwb.cc?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,465 @@\n+// wb.cc -- Add write barriers as needed.\n+\n+// Copyright 2017 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+#include \"go-system.h\"\n+\n+#include \"go-c.h\"\n+#include \"go-diagnostics.h\"\n+#include \"operator.h\"\n+#include \"lex.h\"\n+#include \"types.h\"\n+#include \"expressions.h\"\n+#include \"statements.h\"\n+#include \"runtime.h\"\n+#include \"gogo.h\"\n+\n+// Mark variables whose addresses are taken.  This has to be done\n+// before the write barrier pass and after the escape analysis pass.\n+// It would be nice to do this elsewhere but there isn't an obvious\n+// place.\n+\n+class Mark_address_taken : public Traverse\n+{\n+ public:\n+  Mark_address_taken(Gogo* gogo)\n+    : Traverse(traverse_expressions),\n+      gogo_(gogo)\n+  { }\n+\n+  int\n+  expression(Expression**);\n+\n+ private:\n+  Gogo* gogo_;\n+};\n+\n+// Mark variable addresses taken.\n+\n+int\n+Mark_address_taken::expression(Expression** pexpr)\n+{\n+  Expression* expr = *pexpr;\n+  Unary_expression* ue = expr->unary_expression();\n+  if (ue != NULL)\n+    ue->check_operand_address_taken(this->gogo_);\n+  return TRAVERSE_CONTINUE;\n+}\n+\n+// Add write barriers to the IR.  This are required by the concurrent\n+// garbage collector.  A write barrier is needed for any write of a\n+// pointer into memory controlled by the garbage collector.  Write\n+// barriers are not required for writes to local variables that live\n+// on the stack.  Write barriers are only required when the runtime\n+// enables them, which can be checked using a run time check on\n+// runtime.writeBarrier.enabled.\n+//\n+// Essentially, for each assignment A = B, where A is or contains a\n+// pointer, and where A is not, or at any rate may not be, a stack\n+// variable, we rewrite it into\n+//     if runtime.writeBarrier.enabled {\n+//         typedmemmove(typeof(A), &A, &B)\n+//     } else {\n+//         A = B\n+//     }\n+//\n+// The test of runtime.writeBarrier.Enabled is implemented by treating\n+// the variable as a *uint32, and testing *runtime.writeBarrier != 0.\n+// This is compatible with the definition in the runtime package.\n+//\n+// For types that are pointer shared (pointers, maps, chans, funcs),\n+// we replaced the call to typedmemmove with writebarrierptr(&A, B).\n+// As far as the GC is concerned, all pointers are the same, so it\n+// doesn't need the type descriptor.\n+//\n+// There are possible optimizations that are not implemented.\n+//\n+// runtime.writeBarrier can only change when the goroutine is\n+// preempted, which in practice means when a call is made into the\n+// runtime package, so we could optimize by only testing it once\n+// between function calls.\n+//\n+// A slice could be handled with a call to writebarrierptr plus two\n+// integer moves.\n+\n+// Traverse the IR adding write barriers.\n+\n+class Write_barriers : public Traverse\n+{\n+ public:\n+  Write_barriers(Gogo* gogo)\n+    : Traverse(traverse_functions | traverse_variables | traverse_statements),\n+      gogo_(gogo), function_(NULL)\n+  { }\n+\n+  int\n+  function(Named_object*);\n+\n+  int\n+  variable(Named_object*);\n+\n+  int\n+  statement(Block*, size_t* pindex, Statement*);\n+\n+ private:\n+  // General IR.\n+  Gogo* gogo_;\n+  // Current function.\n+  Function* function_;\n+};\n+\n+// Traverse a function.  Just record it for later.\n+\n+int\n+Write_barriers::function(Named_object* no)\n+{\n+  go_assert(this->function_ == NULL);\n+  this->function_ = no->func_value();\n+  int t = this->function_->traverse(this);\n+  this->function_ = NULL;\n+\n+  if (t == TRAVERSE_EXIT)\n+    return t;\n+  return TRAVERSE_SKIP_COMPONENTS;\n+}\n+\n+// Insert write barriers for a global variable: ensure that variable\n+// initialization is handled correctly.  This is rarely needed, since\n+// we currently don't enable background GC until after all global\n+// variables are initialized.  But we do need this if an init function\n+// calls runtime.GC.\n+\n+int\n+Write_barriers::variable(Named_object* no)\n+{\n+  // We handle local variables in the variable declaration statement.\n+  // We only have to handle global variables here.\n+  if (!no->is_variable())\n+    return TRAVERSE_CONTINUE;\n+  Variable* var = no->var_value();\n+  if (!var->is_global())\n+    return TRAVERSE_CONTINUE;\n+\n+  // Nothing to do if there is no initializer.\n+  Expression* init = var->init();\n+  if (init == NULL)\n+    return TRAVERSE_CONTINUE;\n+\n+  // Nothing to do for variables that do not contain any pointers.\n+  if (!var->type()->has_pointer())\n+    return TRAVERSE_CONTINUE;\n+\n+  // Nothing to do if the initializer is static.\n+  init = Expression::make_cast(var->type(), init, var->location());\n+  if (!var->has_pre_init() && init->is_static_initializer())\n+    return TRAVERSE_CONTINUE;\n+\n+  // Otherwise change the initializer into a pre_init assignment\n+  // statement with a write barrier.\n+\n+  // We can't check for a dependency of the variable on itself after\n+  // we make this change, because the preinit statement will always\n+  // depend on the variable (since it assigns to it).  So check for a\n+  // self-dependency now.\n+  this->gogo_->check_self_dep(no);\n+\n+  // Replace the initializer.\n+  Location loc = init->location();\n+  Expression* ref = Expression::make_var_reference(no, loc);\n+  ref->var_expression()->set_in_lvalue_pos();\n+\n+  Statement_inserter inserter(this->gogo_, var);\n+  Statement* s = this->gogo_->assign_with_write_barrier(NULL, NULL, &inserter,\n+\t\t\t\t\t\t\tref, init, loc);\n+\n+  var->add_preinit_statement(this->gogo_, s);\n+  var->clear_init();\n+\n+  return TRAVERSE_CONTINUE;\n+}\n+\n+// Insert write barriers for statements.\n+\n+int\n+Write_barriers::statement(Block* block, size_t* pindex, Statement* s)\n+{\n+  switch (s->classification())\n+    {\n+    default:\n+      break;\n+\n+    case Statement::STATEMENT_VARIABLE_DECLARATION:\n+      {\n+\tVariable_declaration_statement* vds =\n+\t  s->variable_declaration_statement();\n+\tNamed_object* no = vds->var();\n+\tVariable* var = no->var_value();\n+\n+\t// We may need to emit a write barrier for the initialization\n+\t// of the variable.\n+\n+\t// Nothing to do for a variable with no initializer.\n+\tExpression* init = var->init();\n+\tif (init == NULL)\n+\t  break;\n+\n+\t// Nothing to do if the variable is not in the heap.  Only\n+\t// local variables get declaration statements, and local\n+\t// variables on the stack do not require write barriers.\n+\tif (!var->is_in_heap())\n+\t  break;\n+\n+\t// Nothing to do if the variable does not contain any pointers.\n+\tif (!var->type()->has_pointer())\n+\t  break;\n+\n+\t// Otherwise initialize the variable with a write barrier.\n+\n+\tFunction* function = this->function_;\n+\tLocation loc = init->location();\n+\tStatement_inserter inserter(block, pindex);\n+\n+\t// Insert the variable declaration statement with no\n+\t// initializer, so that the variable exists.\n+\tvar->clear_init();\n+\tinserter.insert(s);\n+\n+\t// Create a statement that initializes the variable with a\n+\t// write barrier.\n+\tExpression* ref = Expression::make_var_reference(no, loc);\n+\tStatement* assign = this->gogo_->assign_with_write_barrier(function,\n+\t\t\t\t\t\t\t\t   block,\n+\t\t\t\t\t\t\t\t   &inserter,\n+\t\t\t\t\t\t\t\t   ref, init,\n+\t\t\t\t\t\t\t\t   loc);\n+\n+\t// Replace the old variable declaration statement with the new\n+\t// initialization.\n+\tblock->replace_statement(*pindex, assign);\n+      }\n+      break;\n+\n+    case Statement::STATEMENT_ASSIGNMENT:\n+      {\n+\tAssignment_statement* as = s->assignment_statement();\n+\tExpression* lhs = as->lhs();\n+\tExpression* rhs = as->rhs();\n+\n+\t// We may need to emit a write barrier for the assignment.\n+\n+\tif (!this->gogo_->assign_needs_write_barrier(lhs))\n+\t  break;\n+\n+\t// Change the assignment to use a write barrier.\n+\tFunction* function = this->function_;\n+\tLocation loc = as->location();\n+\tStatement_inserter inserter = Statement_inserter(block, pindex);\n+\tStatement* assign = this->gogo_->assign_with_write_barrier(function,\n+\t\t\t\t\t\t\t\t   block,\n+\t\t\t\t\t\t\t\t   &inserter,\n+\t\t\t\t\t\t\t\t   lhs, rhs,\n+\t\t\t\t\t\t\t\t   loc);\n+\tblock->replace_statement(*pindex, assign);\n+      }\n+      break;\n+    }\n+\n+  return TRAVERSE_CONTINUE;\n+}\n+\n+// The write barrier pass.\n+\n+void\n+Gogo::add_write_barriers()\n+{\n+  Mark_address_taken mat(this);\n+  this->traverse(&mat);\n+\n+  Write_barriers wb(this);\n+  this->traverse(&wb);\n+}\n+\n+// Return the runtime.writeBarrier variable.\n+\n+Named_object*\n+Gogo::write_barrier_variable()\n+{\n+  static Named_object* write_barrier_var;\n+  if (write_barrier_var == NULL)\n+    {\n+      Location bloc = Linemap::predeclared_location();\n+\n+      // We pretend that writeBarrier is a uint32, so that we do a\n+      // 32-bit load.  That is what the gc toolchain does.\n+      Type* uint32_type = Type::lookup_integer_type(\"uint32\");\n+      Variable* var = new Variable(uint32_type, NULL, true, false, false,\n+\t\t\t\t   bloc);\n+\n+      bool add_to_globals;\n+      Package* package = this->add_imported_package(\"runtime\", \"_\", false,\n+\t\t\t\t\t\t    \"runtime\", \"runtime\",\n+\t\t\t\t\t\t    bloc, &add_to_globals);\n+      write_barrier_var = Named_object::make_variable(\"writeBarrier\",\n+\t\t\t\t\t\t      package, var);\n+    }\n+\n+  return write_barrier_var;\n+}\n+\n+// Return whether an assignment that sets LHS needs a write barrier.\n+\n+bool\n+Gogo::assign_needs_write_barrier(Expression* lhs)\n+{\n+  // Nothing to do if the variable does not contain any pointers.\n+  if (!lhs->type()->has_pointer())\n+    return false;\n+\n+  // Nothing to do for an assignment to a temporary.\n+  if (lhs->temporary_reference_expression() != NULL)\n+    return false;\n+\n+  // Nothing to do for an assignment to a sink.\n+  if (lhs->is_sink_expression())\n+    return false;\n+\n+  // Nothing to do for an assignment to a local variable that is not\n+  // on the heap.\n+  Var_expression* ve = lhs->var_expression();\n+  if (ve != NULL)\n+    {\n+      Named_object* no = ve->named_object();\n+      if (no->is_variable())\n+\t{\n+\t  Variable* var = no->var_value();\n+\t  if (!var->is_global() && !var->is_in_heap())\n+\t    return false;\n+\t}\n+      else if (no->is_result_variable())\n+\t{\n+\t  Result_variable* rvar = no->result_var_value();\n+\t  if (!rvar->is_in_heap())\n+\t    return false;\n+\t}\n+    }\n+\n+  // Write barrier needed in other cases.\n+  return true;\n+}\n+\n+// Return a statement that sets LHS to RHS using a write barrier.\n+// ENCLOSING is the enclosing block.\n+\n+Statement*\n+Gogo::assign_with_write_barrier(Function* function, Block* enclosing,\n+\t\t\t\tStatement_inserter* inserter, Expression* lhs,\n+\t\t\t\tExpression* rhs, Location loc)\n+{\n+  if (function != NULL\n+      && ((function->pragmas() & GOPRAGMA_NOWRITEBARRIER) != 0\n+\t  || (function->pragmas() & GOPRAGMA_NOWRITEBARRIERREC) != 0))\n+    go_error_at(loc, \"write barrier prohibited\");\n+\n+  Type* type = lhs->type();\n+  go_assert(type->has_pointer());\n+\n+  Expression* addr;\n+  if (lhs->unary_expression() != NULL\n+      && lhs->unary_expression()->op() == OPERATOR_MULT)\n+    addr = lhs->unary_expression()->operand();\n+  else\n+    {\n+      addr = Expression::make_unary(OPERATOR_AND, lhs, loc);\n+      addr->unary_expression()->set_does_not_escape();\n+    }\n+  Temporary_statement* lhs_temp = Statement::make_temporary(NULL, addr, loc);\n+  inserter->insert(lhs_temp);\n+  lhs = Expression::make_temporary_reference(lhs_temp, loc);\n+\n+  if (!Type::are_identical(type, rhs->type(), false, NULL)\n+      && rhs->type()->interface_type() != NULL\n+      && !rhs->is_variable())\n+    {\n+      // May need a temporary for interface conversion.\n+      Temporary_statement* temp = Statement::make_temporary(NULL, rhs, loc);\n+      inserter->insert(temp);\n+      rhs = Expression::make_temporary_reference(temp, loc);\n+    }\n+  rhs = Expression::convert_for_assignment(this, type, rhs, loc);\n+  Temporary_statement* rhs_temp = NULL;\n+  if (!rhs->is_variable() && !rhs->is_constant())\n+    {\n+      rhs_temp = Statement::make_temporary(NULL, rhs, loc);\n+      inserter->insert(rhs_temp);\n+      rhs = Expression::make_temporary_reference(rhs_temp, loc);\n+    }\n+\n+  Expression* indir = Expression::make_unary(OPERATOR_MULT, lhs, loc);\n+  Statement* assign = Statement::make_assignment(indir, rhs, loc);\n+\n+  lhs = Expression::make_temporary_reference(lhs_temp, loc);\n+  if (rhs_temp != NULL)\n+    rhs = Expression::make_temporary_reference(rhs_temp, loc);\n+\n+  Type* unsafe_ptr_type = Type::make_pointer_type(Type::make_void_type());\n+  lhs = Expression::make_unsafe_cast(unsafe_ptr_type, lhs, loc);\n+\n+  Expression* call;\n+  switch (type->base()->classification())\n+    {\n+    default:\n+      go_unreachable();\n+\n+    case Type::TYPE_ERROR:\n+      return assign;\n+\n+    case Type::TYPE_POINTER:\n+    case Type::TYPE_FUNCTION:\n+    case Type::TYPE_MAP:\n+    case Type::TYPE_CHANNEL:\n+      // These types are all represented by a single pointer.\n+      call = Runtime::make_call(Runtime::WRITEBARRIERPTR, loc, 2, lhs, rhs);\n+      break;\n+\n+    case Type::TYPE_STRING:\n+    case Type::TYPE_STRUCT:\n+    case Type::TYPE_ARRAY:\n+    case Type::TYPE_INTERFACE:\n+      {\n+\trhs = Expression::make_unary(OPERATOR_AND, rhs, loc);\n+\trhs->unary_expression()->set_does_not_escape();\n+\tcall = Runtime::make_call(Runtime::TYPEDMEMMOVE, loc, 3,\n+\t\t\t\t  Expression::make_type_descriptor(type, loc),\n+\t\t\t\t  lhs, rhs);\n+      }\n+      break;\n+    }\n+\n+  return this->check_write_barrier(enclosing, assign,\n+\t\t\t\t   Statement::make_statement(call, false));\n+}\n+\n+// Return a statement that tests whether write barriers are enabled\n+// and executes either the efficient code or the write barrier\n+// function call, depending.\n+\n+Statement*\n+Gogo::check_write_barrier(Block* enclosing, Statement* without,\n+\t\t\t  Statement* with)\n+{\n+  Location loc = without->location();\n+  Named_object* wb = this->write_barrier_variable();\n+  Expression* ref = Expression::make_var_reference(wb, loc);\n+  Expression* zero = Expression::make_integer_ul(0, ref->type(), loc);\n+  Expression* cond = Expression::make_binary(OPERATOR_EQEQ, ref, zero, loc);\n+\n+  Block* then_block = new Block(enclosing, loc);\n+  then_block->add_statement(without);\n+\n+  Block* else_block = new Block(enclosing, loc);\n+  else_block->add_statement(with);\n+\n+  return Statement::make_if_statement(cond, then_block, else_block, loc);\n+}"}, {"sha": "8a184d1c6ad5753ed8d6beefb2c96a452daa8e64", "filename": "gcc/testsuite/go.test/test/slice3.go", "status": "modified", "additions": 12, "deletions": 8, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Ftestsuite%2Fgo.test%2Ftest%2Fslice3.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/gcc%2Ftestsuite%2Fgo.test%2Ftest%2Fslice3.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgo.test%2Ftest%2Fslice3.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -1,6 +1,6 @@\n // runoutput\n \n-// Copyright 2013 The Go Authors.  All rights reserved.\n+// Copyright 2013 The Go Authors. All rights reserved.\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n@@ -19,10 +19,10 @@ var bout *bufio.Writer\n \n func main() {\n \tbout = bufio.NewWriter(os.Stdout)\n-\t\n+\n \tfmt.Fprintf(bout, \"%s\", programTop)\n \tfmt.Fprintf(bout, \"func main() {\\n\")\n-\t\n+\n \tindex := []string{\n \t\t\"0\",\n \t\t\"1\",\n@@ -38,7 +38,7 @@ func main() {\n \t\t\"v10\",\n \t\t\"v20\",\n \t}\n-\t\n+\n \tparse := func(s string) (n int, isconst bool) {\n \t\tif s == \"vminus1\" {\n \t\t\treturn -1, false\n@@ -69,7 +69,7 @@ func main() {\n \t\t\t\t\t\ticonst && kconst && iv > kv,\n \t\t\t\t\t\ticonst && base == \"array\" && iv > Cap,\n \t\t\t\t\t\tjconst && base == \"array\" && jv > Cap,\n-\t\t\t\t\t\tkconst && base == \"array\" && kv > Cap:\t\t\t\t\n+\t\t\t\t\t\tkconst && base == \"array\" && kv > Cap:\n \t\t\t\t\t\tcontinue\n \t\t\t\t\t}\n \n@@ -82,7 +82,7 @@ func main() {\n \t\t\t\t\t\txlen = jv - iv\n \t\t\t\t\t\txcap = kv - iv\n \t\t\t\t\t}\n-\t\t\t\t\tfmt.Fprintf(bout, \"\\tcheckSlice(%q, func() []byte { return %s }, %d, %d, %d)\\n\", expr, expr, xbase, xlen, xcap)\t\t\t\t\t\t\t\t\t\n+\t\t\t\t\tfmt.Fprintf(bout, \"\\tcheckSlice(%q, func() []byte { return %s }, %d, %d, %d)\\n\", expr, expr, xbase, xlen, xcap)\n \t\t\t\t}\n \t\t\t}\n \t\t}\n@@ -147,9 +147,13 @@ func checkSlice(desc string, f func() []byte, xbase, xlen, xcap int) {\n \t\tprintln(desc, \"=\", base, len, cap, \"want panic\")\n \t\treturn\n \t}\n-\tif base != uintptr(xbase) || len != uintptr(xlen) || cap != uintptr(xcap) {\n+\tif cap != 0 && base != uintptr(xbase) || base >= 10 || len != uintptr(xlen) || cap != uintptr(xcap) {\n \t\tnotOK()\n-\t\tprintln(desc, \"=\", base, len, cap, \"want\", xbase, xlen, xcap)\n+\t\tif cap == 0 {\n+\t\t\tprintln(desc, \"=\", base, len, cap, \"want\", \"0-9\", xlen, xcap)\n+\t\t} else {\n+\t\t\tprintln(desc, \"=\", base, len, cap, \"want\", xbase, xlen, xcap)\n+\t\t}\n \t}\n }\n "}, {"sha": "d18f1b89a31c3b7f2d072d729e35f1b87c993eff", "filename": "libgo/Makefile.am", "status": "modified", "additions": 52, "deletions": 44, "changes": 96, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2FMakefile.am", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2FMakefile.am", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2FMakefile.am?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -46,8 +46,17 @@ AM_CFLAGS = -fexceptions -fnon-call-exceptions -fplan9-extensions \\\n \t-I $(srcdir)/../libgcc -I $(srcdir)/../libbacktrace \\\n \t-I $(MULTIBUILDTOP)../../gcc/include\n \n+AM_LDFLAGS =\n+\n if USING_SPLIT_STACK\n-AM_LDFLAGS = -XCClinker $(SPLIT_STACK)\n+AM_LDFLAGS += -XCClinker $(SPLIT_STACK)\n+endif\n+\n+if LIBGO_IS_AIX\n+# Using an import file for libgo avoid requiring to use the -brtl flag\n+# when builing a go program\n+AM_LDFLAGS += -Wl,-bbigtoc -Wl,-bI:$(srcdir)/libgo.imp\n+EXTRA_libgo_la_DEPENDENCIES = libgo.imp\n endif\n \n # Multilib support.\n@@ -383,12 +392,6 @@ toolexeclibgounicode_DATA = \\\n \tunicode/utf16.gox \\\n \tunicode/utf8.gox\n \n-if HAVE_SYS_MMAN_H\n-runtime_mem_file = runtime/mem.c\n-else\n-runtime_mem_file = runtime/mem_posix_memalign.c\n-endif\n-\n if LIBGO_IS_RTEMS\n rtems_task_variable_add_file = runtime/rtems-task-variable-add.c\n else\n@@ -419,18 +422,21 @@ else\n if LIBGO_IS_NETBSD\n runtime_getncpu_file = runtime/getncpu-bsd.c\n else\n+if LIBGO_IS_AIX\n+runtime_getncpu_file = runtime/getncpu-aix.c\n+else\n runtime_getncpu_file = runtime/getncpu-none.c\n endif\n endif\n endif\n endif\n endif\n endif\n+endif\n \n runtime_files = \\\n \truntime/aeshash.c \\\n \truntime/go-assert.c \\\n-\truntime/go-breakpoint.c \\\n \truntime/go-caller.c \\\n \truntime/go-callers.c \\\n \truntime/go-cdiv.c \\\n@@ -445,61 +451,36 @@ runtime_files = \\\n \truntime/go-memmove.c \\\n \truntime/go-nanotime.c \\\n \truntime/go-now.c \\\n-\truntime/go-new.c \\\n \truntime/go-nosys.c \\\n \truntime/go-reflect-call.c \\\n \truntime/go-runtime-error.c \\\n \truntime/go-setenv.c \\\n \truntime/go-signal.c \\\n \truntime/go-strslice.c \\\n \truntime/go-typedesc-equal.c \\\n-\truntime/go-unsafe-new.c \\\n-\truntime/go-unsafe-newarray.c \\\n \truntime/go-unsafe-pointer.c \\\n \truntime/go-unsetenv.c \\\n \truntime/go-unwind.c \\\n \truntime/go-varargs.c \\\n \truntime/env_posix.c \\\n-\truntime/heapdump.c \\\n-\truntime/mcache.c \\\n-\truntime/mcentral.c \\\n-\t$(runtime_mem_file) \\\n-\truntime/mfixalloc.c \\\n-\truntime/mgc0.c \\\n-\truntime/mheap.c \\\n-\truntime/msize.c \\\n \truntime/panic.c \\\n-\truntime/parfor.c \\\n \truntime/print.c \\\n \truntime/proc.c \\\n \truntime/runtime_c.c \\\n+\truntime/stack.c \\\n \truntime/thread.c \\\n \t$(runtime_thread_files) \\\n \truntime/yield.c \\\n \t$(rtems_task_variable_add_file) \\\n-\tmalloc.c \\\n \t$(runtime_getncpu_file)\n \n-goc2c.$(OBJEXT): runtime/goc2c.c\n-\t$(CC_FOR_BUILD) -c $(CFLAGS_FOR_BUILD) $<\n-\n-goc2c: goc2c.$(OBJEXT)\n-\t$(CC_FOR_BUILD) $(CFLAGS_FOR_BUILD) $(LDFLAGS_FOR_BUILD) -o $@ $<\n-\n-malloc.c: $(srcdir)/runtime/malloc.goc goc2c\n-\t./goc2c $< > $@.tmp\n-\tmv -f $@.tmp $@\n-\n-%.c: $(srcdir)/runtime/%.goc goc2c\n-\t./goc2c $< > $@.tmp\n-\tmv -f $@.tmp $@\n-\n version.go: s-version; @true\n s-version: Makefile\n \trm -f version.go.tmp\n \techo \"package sys\" > version.go.tmp\n \techo 'const DefaultGoroot = \"$(prefix)\"' >> version.go.tmp\n \techo 'const TheVersion = \"'`cat $(srcdir)/VERSION | sed 1q`' '`$(GOC) --version | sed 1q`'\"' >> version.go.tmp\n+\techo 'const Goexperiment = ``' >> version.go.tmp\n \techo 'const GOARCH = \"'$(GOARCH)'\"' >> version.go.tmp\n \techo 'const GOOS = \"'$(GOOS)'\"' >> version.go.tmp\n \techo 'const GccgoToolDir = \"$(libexecsubdir)\"' >> version.go.tmp\n@@ -550,7 +531,7 @@ s-version: Makefile\n \n runtime_sysinfo.go: s-runtime_sysinfo; @true\n s-runtime_sysinfo: $(srcdir)/mkrsysinfo.sh gen-sysinfo.go\n-\t$(SHELL) $(srcdir)/mkrsysinfo.sh\n+\tGOOS=$(GOOS) $(SHELL) $(srcdir)/mkrsysinfo.sh\n \t$(SHELL) $(srcdir)/mvifdiff.sh tmp-runtime_sysinfo.go runtime_sysinfo.go\n \t$(STAMP) $@\n \n@@ -560,14 +541,23 @@ s-sigtab: $(srcdir)/mksigtab.sh gen-sysinfo.go\n \t$(SHELL) $(srcdir)/mvifdiff.sh tmp-sigtab.go sigtab.go\n \t$(STAMP) $@\n \n+# _Complex_lock and _Reader_lock are Go translations of some AIX system\n+# types and should not be exported back to C\n+# sigset conflicts with system type sigset on AIX, so we need to rename it\n runtime.inc: s-runtime-inc; @true\n s-runtime-inc: runtime.lo Makefile\n-\trm -f runtime.inc.tmp2\n-\tgrep -v \"#define _\" runtime.inc.tmp | grep -v \"#define [cm][01234] \" > runtime.inc.tmp2\n+\trm -f runtime.inc.tmp2 runtime.inc.tmp3\n+\tgrep -v \"#define _\" runtime.inc.tmp | grep -v \"#define [cm][01234] \" | grep -v \"#define empty \" > runtime.inc.tmp2\n \tfor pattern in '_[GP][a-z]' _Max _Lock _Sig _Trace _MHeap _Num; do \\\n \t  grep \"#define $$pattern\" runtime.inc.tmp >> runtime.inc.tmp2; \\\n \tdone\n-\t$(SHELL) $(srcdir)/mvifdiff.sh runtime.inc.tmp2 runtime.inc\n+\tfor TYPE in _Complex_lock _Reader_lock; do \\\n+\t  sed -e '/struct '$${TYPE}' {/,/^}/s/^.*$$//' runtime.inc.tmp2 > runtime.inc.tmp3; \\\n+\t  mv runtime.inc.tmp3 runtime.inc.tmp2; \\\n+\tdone\n+\tsed -e 's/sigset/sigset_go/' runtime.inc.tmp2 > runtime.inc.tmp3\n+\t$(SHELL) $(srcdir)/mvifdiff.sh runtime.inc.tmp3 runtime.inc\n+\trm -f runtime.inc.tmp2 runtime.inc.tmp3\n \t$(STAMP) $@\n \n noinst_DATA = zstdpkglist.go\n@@ -579,7 +569,7 @@ s-zstdpkglist: Makefile\n \techo 'package main' > zstdpkglist.go.tmp\n \techo \"\" >> zstdpkglist.go.tmp\n \techo 'var stdpkg = map[string]bool{' >> zstdpkglist.go.tmp\n-\techo $(libgo_go_objs) 'unsafe.lo' 'runtime/cgo.lo' | sed 's/\\.lo /\\\": true,\\n/g' | grep -v _c | sed 's/\\.lo/\\\": true,/' | sed 's/^/\\t\\\"/' | sort -u >> zstdpkglist.go.tmp\n+\techo $(libgo_go_objs) 'unsafe.lo' 'runtime/cgo.lo' | sed 's|[a-z0-9_/]*_c\\.lo||g' | sed 's|\\([a-z0-9_/]*\\)\\.lo|\"\\1\": true,|g' >> zstdpkglist.go.tmp\n \techo '}' >> zstdpkglist.go.tmp\n \t$(SHELL) $(srcdir)/mvifdiff.sh zstdpkglist.go.tmp zstdpkglist.go\n \t$(STAMP) $@\n@@ -632,7 +622,7 @@ s-errno:\n \n sysinfo.go: s-sysinfo; @true\n s-sysinfo: $(srcdir)/mksysinfo.sh gen-sysinfo.go errno.i\n-\t$(SHELL) $(srcdir)/mksysinfo.sh\n+\tGOOS=$(GOOS) $(SHELL) $(srcdir)/mksysinfo.sh\n \t$(SHELL) $(srcdir)/mvifdiff.sh tmp-sysinfo.go sysinfo.go\n \t$(STAMP) $@\n \n@@ -923,8 +913,13 @@ BUILDPACKAGE = \\\n \t$(LTGOCOMPILE) -I . -c -fgo-pkgpath=`echo $@ | sed -e 's/.lo$$//'` $($(subst -,_,$(subst .,_,$(subst /,_,$@)))_GOCFLAGS) -o $@ $$files\n \n # How to build a .gox file from a .lo file.\n+# Matching .o file can either be in the same directory as the .lo (non-PIC\n+# object) or in the .libs directory (PIC object).\n BUILDGOX = \\\n-\tf=`echo $< | sed -e 's/.lo$$/.o/'`; \\\n+\tf=\"$(basename $<).o\"; \\\n+\tif test ! -f $$f; then \\\n+\t  f=\"$(basename $(<D)/.libs/$(<F)).o\"; \\\n+\tfi; \\\n \t$(OBJCOPY) -j .go_export $$f $@.tmp; \\\n \t$(SHELL) $(srcdir)/mvifdiff.sh $@.tmp `echo $@ | sed -e 's/s-gox/gox/'`\n \n@@ -941,6 +936,8 @@ CHECK = \\\n \texport RUNTESTFLAGS; \\\n \tMAKE=\"$(MAKE)\"; \\\n \texport MAKE; \\\n+\tNM=\"$(NM)\"; \\\n+\texport NM; \\\n \tlibgccdir=`${GOC} -print-libgcc-file-name | sed -e 's|/[^/]*$$||'`; \\\n \tLD_LIBRARY_PATH=\"`${PWD_COMMAND}`/.libs:$${libgccdir}:${LD_LIBRARY_PATH}\"; \\\n \tLD_LIBRARY_PATH=`echo $${LD_LIBRARY_PATH} | sed 's,::*,:,g;s,^:*,,;s,:*$$,,'`; \\\n@@ -1296,6 +1293,7 @@ TEST_PACKAGES = \\\n \truntime/internal/sys/check \\\n \truntime/pprof/check \\\n \truntime/pprof/internal/protopprof/check \\\n+\truntime/trace/check \\\n \tsync/atomic/check \\\n \ttext/scanner/check \\\n \ttext/tabwriter/check \\\n@@ -1407,16 +1405,26 @@ check-multi:\n bench:\n \t-@$(MAKE) -k $(TEST_PACKAGES) GOBENCH=.\n \n-MOSTLYCLEAN_FILES = libgo.head libgo.sum.sep libgo.log.sep\n+MOSTLYCLEANFILES = \\\n+\ts-runtime_sysinfo s-sigtab s-runtime-inc s-zstdpkglist \\\n+\ts-libcalls s-libcalls-list s-syscall_arch s-gen-sysinfo s-sysinfo \\\n+\ts-errno s-epoll \\\n+\tlibgo.head libgo.sum.sep libgo.log.sep libgo.var \\\n+\tlibcalls-list runtime.inc runtime.inc.tmp2 runtime.inc.tmp3\n \n mostlyclean-local:\n \tfind . -name '*.lo' -print | xargs $(LIBTOOL) --mode=clean rm -f\n \tfind . -name '*.$(OBJEXT)' -print | xargs rm -f\n \tfind . -name '*-testsum' -print | xargs rm -f\n \tfind . -name '*-testlog' -print | xargs rm -f\n \n-CLEANFILES = *.go *.gox goc2c *.c s-version libgo.sum libgo.log\n+CLEANFILES = *.go *.c s-version libgo.sum libgo.log runtime.inc\n \n clean-local:\n \tfind . -name '*.la' -print | xargs $(LIBTOOL) --mode=clean rm -f\n \tfind . -name '*.a' -print | xargs rm -f\n+\tfind . -name '*.gox' -print | xargs rm -f\n+\tfind . -name '*.s-gox' -print | xargs rm -f\n+\n+distclean-local:\n+\tfind . -name '*.lo.dep' -print | xargs rm -f"}, {"sha": "0a06038768464b507a35cb2f42e8d0204f0a3f63", "filename": "libgo/Makefile.in", "status": "modified", "additions": 100, "deletions": 193, "changes": 293, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2FMakefile.in?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -62,8 +62,13 @@ POST_UNINSTALL = :\n build_triplet = @build@\n host_triplet = @host@\n target_triplet = @target@\n-@GOC_IS_LLGO_TRUE@am__append_1 = libgo-llgo.la libgobegin-llgo.a\n-@GOC_IS_LLGO_FALSE@am__append_2 = libgo.la libgobegin.a\n+@USING_SPLIT_STACK_TRUE@am__append_1 = -XCClinker $(SPLIT_STACK)\n+\n+# Using an import file for libgo avoid requiring to use the -brtl flag\n+# when builing a go program\n+@LIBGO_IS_AIX_TRUE@am__append_2 = -Wl,-bbigtoc -Wl,-bI:$(srcdir)/libgo.imp\n+@GOC_IS_LLGO_TRUE@am__append_3 = libgo-llgo.la libgobegin-llgo.a\n+@GOC_IS_LLGO_FALSE@am__append_4 = libgo.la libgobegin.a\n subdir = .\n DIST_COMMON = README $(srcdir)/Makefile.in $(srcdir)/Makefile.am \\\n \t$(top_srcdir)/configure $(am__configure_deps) \\\n@@ -178,40 +183,36 @@ am__DEPENDENCIES_4 = $(am__DEPENDENCIES_2) \\\n \t$(am__DEPENDENCIES_3) $(am__DEPENDENCIES_3) \\\n \t$(am__DEPENDENCIES_3) $(am__DEPENDENCIES_3)\n libgo_llgo_la_DEPENDENCIES = $(am__DEPENDENCIES_4)\n-@HAVE_SYS_MMAN_H_FALSE@am__objects_1 = mem_posix_memalign.lo\n-@HAVE_SYS_MMAN_H_TRUE@am__objects_1 = mem.lo\n-@LIBGO_IS_LINUX_FALSE@am__objects_2 = thread-sema.lo\n-@LIBGO_IS_LINUX_TRUE@am__objects_2 = thread-linux.lo\n-@LIBGO_IS_RTEMS_TRUE@am__objects_3 = rtems-task-variable-add.lo\n-@LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_FREEBSD_FALSE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_NETBSD_FALSE@@LIBGO_IS_SOLARIS_FALSE@am__objects_4 = getncpu-none.lo\n-@LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_FREEBSD_FALSE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_NETBSD_TRUE@@LIBGO_IS_SOLARIS_FALSE@am__objects_4 = getncpu-bsd.lo\n-@LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_FREEBSD_TRUE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_SOLARIS_FALSE@am__objects_4 = getncpu-bsd.lo\n-@LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_SOLARIS_TRUE@am__objects_4 = getncpu-solaris.lo\n-@LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_IRIX_TRUE@@LIBGO_IS_LINUX_FALSE@am__objects_4 = getncpu-irix.lo\n-@LIBGO_IS_DARWIN_TRUE@@LIBGO_IS_LINUX_FALSE@am__objects_4 =  \\\n+@LIBGO_IS_LINUX_FALSE@am__objects_1 = thread-sema.lo\n+@LIBGO_IS_LINUX_TRUE@am__objects_1 = thread-linux.lo\n+@LIBGO_IS_RTEMS_TRUE@am__objects_2 = rtems-task-variable-add.lo\n+@LIBGO_IS_AIX_FALSE@@LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_FREEBSD_FALSE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_NETBSD_FALSE@@LIBGO_IS_SOLARIS_FALSE@am__objects_3 = getncpu-none.lo\n+@LIBGO_IS_AIX_TRUE@@LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_FREEBSD_FALSE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_NETBSD_FALSE@@LIBGO_IS_SOLARIS_FALSE@am__objects_3 = getncpu-aix.lo\n+@LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_FREEBSD_FALSE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_NETBSD_TRUE@@LIBGO_IS_SOLARIS_FALSE@am__objects_3 = getncpu-bsd.lo\n+@LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_FREEBSD_TRUE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_SOLARIS_FALSE@am__objects_3 = getncpu-bsd.lo\n+@LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_SOLARIS_TRUE@am__objects_3 = getncpu-solaris.lo\n+@LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_IRIX_TRUE@@LIBGO_IS_LINUX_FALSE@am__objects_3 = getncpu-irix.lo\n+@LIBGO_IS_DARWIN_TRUE@@LIBGO_IS_LINUX_FALSE@am__objects_3 =  \\\n @LIBGO_IS_DARWIN_TRUE@@LIBGO_IS_LINUX_FALSE@\tgetncpu-bsd.lo\n-@LIBGO_IS_LINUX_TRUE@am__objects_4 = getncpu-linux.lo\n-am__objects_5 = aeshash.lo go-assert.lo go-breakpoint.lo go-caller.lo \\\n-\tgo-callers.lo go-cdiv.lo go-cgo.lo go-construct-map.lo \\\n-\tgo-ffi.lo go-fieldtrack.lo go-matherr.lo go-memclr.lo \\\n-\tgo-memcmp.lo go-memequal.lo go-memmove.lo go-nanotime.lo \\\n-\tgo-now.lo go-new.lo go-nosys.lo go-reflect-call.lo \\\n-\tgo-runtime-error.lo go-setenv.lo go-signal.lo go-strslice.lo \\\n-\tgo-typedesc-equal.lo go-unsafe-new.lo go-unsafe-newarray.lo \\\n+@LIBGO_IS_LINUX_TRUE@am__objects_3 = getncpu-linux.lo\n+am__objects_4 = aeshash.lo go-assert.lo go-caller.lo go-callers.lo \\\n+\tgo-cdiv.lo go-cgo.lo go-construct-map.lo go-ffi.lo \\\n+\tgo-fieldtrack.lo go-matherr.lo go-memclr.lo go-memcmp.lo \\\n+\tgo-memequal.lo go-memmove.lo go-nanotime.lo go-now.lo \\\n+\tgo-nosys.lo go-reflect-call.lo go-runtime-error.lo \\\n+\tgo-setenv.lo go-signal.lo go-strslice.lo go-typedesc-equal.lo \\\n \tgo-unsafe-pointer.lo go-unsetenv.lo go-unwind.lo go-varargs.lo \\\n-\tenv_posix.lo heapdump.lo mcache.lo mcentral.lo \\\n-\t$(am__objects_1) mfixalloc.lo mgc0.lo mheap.lo msize.lo \\\n-\tpanic.lo parfor.lo print.lo proc.lo runtime_c.lo thread.lo \\\n-\t$(am__objects_2) yield.lo $(am__objects_3) malloc.lo \\\n-\t$(am__objects_4)\n-am_libgo_llgo_la_OBJECTS = $(am__objects_5)\n+\tenv_posix.lo panic.lo print.lo proc.lo runtime_c.lo stack.lo \\\n+\tthread.lo $(am__objects_1) yield.lo $(am__objects_2) \\\n+\t$(am__objects_3)\n+am_libgo_llgo_la_OBJECTS = $(am__objects_4)\n libgo_llgo_la_OBJECTS = $(am_libgo_llgo_la_OBJECTS)\n libgo_llgo_la_LINK = $(LIBTOOL) --tag=CC $(AM_LIBTOOLFLAGS) \\\n \t$(LIBTOOLFLAGS) --mode=link $(CCLD) $(AM_CFLAGS) $(CFLAGS) \\\n \t$(libgo_llgo_la_LDFLAGS) $(LDFLAGS) -o $@\n @GOC_IS_LLGO_TRUE@am_libgo_llgo_la_rpath = -rpath $(toolexeclibdir)\n libgo_la_DEPENDENCIES = $(am__DEPENDENCIES_4)\n-am_libgo_la_OBJECTS = $(am__objects_5)\n+am_libgo_la_OBJECTS = $(am__objects_4)\n libgo_la_OBJECTS = $(am_libgo_la_OBJECTS)\n libgo_la_LINK = $(LIBTOOL) --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) \\\n \t--mode=link $(CCLD) $(AM_CFLAGS) $(CFLAGS) $(libgo_la_LDFLAGS) \\\n@@ -463,7 +464,8 @@ AM_CFLAGS = -fexceptions -fnon-call-exceptions -fplan9-extensions \\\n \t-I $(srcdir)/../libgcc -I $(srcdir)/../libbacktrace \\\n \t-I $(MULTIBUILDTOP)../../gcc/include\n \n-@USING_SPLIT_STACK_TRUE@AM_LDFLAGS = -XCClinker $(SPLIT_STACK)\n+AM_LDFLAGS = $(am__append_1) $(am__append_2)\n+@LIBGO_IS_AIX_TRUE@EXTRA_libgo_la_DEPENDENCIES = libgo.imp\n \n # Multilib support.\n MAKEOVERRIDES = \n@@ -761,13 +763,12 @@ toolexeclibgounicode_DATA = \\\n \tunicode/utf16.gox \\\n \tunicode/utf8.gox\n \n-@HAVE_SYS_MMAN_H_FALSE@runtime_mem_file = runtime/mem_posix_memalign.c\n-@HAVE_SYS_MMAN_H_TRUE@runtime_mem_file = runtime/mem.c\n @LIBGO_IS_RTEMS_FALSE@rtems_task_variable_add_file = \n @LIBGO_IS_RTEMS_TRUE@rtems_task_variable_add_file = runtime/rtems-task-variable-add.c\n @LIBGO_IS_LINUX_FALSE@runtime_thread_files = runtime/thread-sema.c\n @LIBGO_IS_LINUX_TRUE@runtime_thread_files = runtime/thread-linux.c\n-@LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_FREEBSD_FALSE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_NETBSD_FALSE@@LIBGO_IS_SOLARIS_FALSE@runtime_getncpu_file = runtime/getncpu-none.c\n+@LIBGO_IS_AIX_FALSE@@LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_FREEBSD_FALSE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_NETBSD_FALSE@@LIBGO_IS_SOLARIS_FALSE@runtime_getncpu_file = runtime/getncpu-none.c\n+@LIBGO_IS_AIX_TRUE@@LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_FREEBSD_FALSE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_NETBSD_FALSE@@LIBGO_IS_SOLARIS_FALSE@runtime_getncpu_file = runtime/getncpu-aix.c\n @LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_FREEBSD_FALSE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_NETBSD_TRUE@@LIBGO_IS_SOLARIS_FALSE@runtime_getncpu_file = runtime/getncpu-bsd.c\n @LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_FREEBSD_TRUE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_SOLARIS_FALSE@runtime_getncpu_file = runtime/getncpu-bsd.c\n @LIBGO_IS_DARWIN_FALSE@@LIBGO_IS_IRIX_FALSE@@LIBGO_IS_LINUX_FALSE@@LIBGO_IS_SOLARIS_TRUE@runtime_getncpu_file = runtime/getncpu-solaris.c\n@@ -777,7 +778,6 @@ toolexeclibgounicode_DATA = \\\n runtime_files = \\\n \truntime/aeshash.c \\\n \truntime/go-assert.c \\\n-\truntime/go-breakpoint.c \\\n \truntime/go-caller.c \\\n \truntime/go-callers.c \\\n \truntime/go-cdiv.c \\\n@@ -792,39 +792,27 @@ runtime_files = \\\n \truntime/go-memmove.c \\\n \truntime/go-nanotime.c \\\n \truntime/go-now.c \\\n-\truntime/go-new.c \\\n \truntime/go-nosys.c \\\n \truntime/go-reflect-call.c \\\n \truntime/go-runtime-error.c \\\n \truntime/go-setenv.c \\\n \truntime/go-signal.c \\\n \truntime/go-strslice.c \\\n \truntime/go-typedesc-equal.c \\\n-\truntime/go-unsafe-new.c \\\n-\truntime/go-unsafe-newarray.c \\\n \truntime/go-unsafe-pointer.c \\\n \truntime/go-unsetenv.c \\\n \truntime/go-unwind.c \\\n \truntime/go-varargs.c \\\n \truntime/env_posix.c \\\n-\truntime/heapdump.c \\\n-\truntime/mcache.c \\\n-\truntime/mcentral.c \\\n-\t$(runtime_mem_file) \\\n-\truntime/mfixalloc.c \\\n-\truntime/mgc0.c \\\n-\truntime/mheap.c \\\n-\truntime/msize.c \\\n \truntime/panic.c \\\n-\truntime/parfor.c \\\n \truntime/print.c \\\n \truntime/proc.c \\\n \truntime/runtime_c.c \\\n+\truntime/stack.c \\\n \truntime/thread.c \\\n \t$(runtime_thread_files) \\\n \truntime/yield.c \\\n \t$(rtems_task_variable_add_file) \\\n-\tmalloc.c \\\n \t$(runtime_getncpu_file)\n \n noinst_DATA = zstdpkglist.go\n@@ -1079,8 +1067,13 @@ BUILDPACKAGE = \\\n \n \n # How to build a .gox file from a .lo file.\n+# Matching .o file can either be in the same directory as the .lo (non-PIC\n+# object) or in the .libs directory (PIC object).\n BUILDGOX = \\\n-\tf=`echo $< | sed -e 's/.lo$$/.o/'`; \\\n+\tf=\"$(basename $<).o\"; \\\n+\tif test ! -f $$f; then \\\n+\t  f=\"$(basename $(<D)/.libs/$(<F)).o\"; \\\n+\tfi; \\\n \t$(OBJCOPY) -j .go_export $$f $@.tmp; \\\n \t$(SHELL) $(srcdir)/mvifdiff.sh $@.tmp `echo $@ | sed -e 's/s-gox/gox/'`\n \n@@ -1097,6 +1090,8 @@ CHECK = \\\n \texport RUNTESTFLAGS; \\\n \tMAKE=\"$(MAKE)\"; \\\n \texport MAKE; \\\n+\tNM=\"$(NM)\"; \\\n+\texport NM; \\\n \tlibgccdir=`${GOC} -print-libgcc-file-name | sed -e 's|/[^/]*$$||'`; \\\n \tLD_LIBRARY_PATH=\"`${PWD_COMMAND}`/.libs:$${libgccdir}:${LD_LIBRARY_PATH}\"; \\\n \tLD_LIBRARY_PATH=`echo $${LD_LIBRARY_PATH} | sed 's,::*,:,g;s,^:*,,;s,:*$$,,'`; \\\n@@ -1136,7 +1131,7 @@ CHECK_DEPS = $(toolexeclibgo_DATA) $(toolexeclibgoarchive_DATA) \\\n \t$(toolexeclibgorpc_DATA) $(toolexeclibgoruntime_DATA) \\\n \t$(toolexeclibgosync_DATA) $(toolexeclibgotesting_DATA) \\\n \t$(toolexeclibgotext_DATA) $(toolexeclibgotexttemplate_DATA) \\\n-\t$(toolexeclibgounicode_DATA) $(am__append_1) $(am__append_2)\n+\t$(toolexeclibgounicode_DATA) $(am__append_3) $(am__append_4)\n \n # Pass -ffp-contract=off, or 386-specific options, when building the\n # math package.  MATH_FLAG is defined in configure.ac.\n@@ -1329,6 +1324,7 @@ TEST_PACKAGES = \\\n \truntime/internal/sys/check \\\n \truntime/pprof/check \\\n \truntime/pprof/internal/protopprof/check \\\n+\truntime/trace/check \\\n \tsync/atomic/check \\\n \ttext/scanner/check \\\n \ttext/tabwriter/check \\\n@@ -1338,8 +1334,14 @@ TEST_PACKAGES = \\\n \tunicode/utf16/check \\\n \tunicode/utf8/check\n \n-MOSTLYCLEAN_FILES = libgo.head libgo.sum.sep libgo.log.sep\n-CLEANFILES = *.go *.gox goc2c *.c s-version libgo.sum libgo.log\n+MOSTLYCLEANFILES = \\\n+\ts-runtime_sysinfo s-sigtab s-runtime-inc s-zstdpkglist \\\n+\ts-libcalls s-libcalls-list s-syscall_arch s-gen-sysinfo s-sysinfo \\\n+\ts-errno s-epoll \\\n+\tlibgo.head libgo.sum.sep libgo.log.sep libgo.var \\\n+\tlibcalls-list runtime.inc runtime.inc.tmp2 runtime.inc.tmp3\n+\n+CLEANFILES = *.go *.c s-version libgo.sum libgo.log runtime.inc\n all: config.h\n \t$(MAKE) $(AM_MAKEFLAGS) all-recursive\n \n@@ -1482,13 +1484,13 @@ distclean-compile:\n \n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/aeshash.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/env_posix.Plo@am__quote@\n+@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/getncpu-aix.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/getncpu-bsd.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/getncpu-irix.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/getncpu-linux.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/getncpu-none.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/getncpu-solaris.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-assert.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-breakpoint.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-caller.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-callers.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-cdiv.Plo@am__quote@\n@@ -1502,7 +1504,6 @@ distclean-compile:\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-memequal.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-memmove.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-nanotime.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-new.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-nosys.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-now.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-reflect-call.Plo@am__quote@\n@@ -1511,31 +1512,19 @@ distclean-compile:\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-signal.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-strslice.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-typedesc-equal.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-unsafe-new.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-unsafe-newarray.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-unsafe-pointer.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-unsetenv.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-unwind.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/go-varargs.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/heapdump.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/libgobegin_a-go-main.Po@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/libgobegin_llgo_a-go-main.Po@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/libgolibbegin_a-go-libmain.Po@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/malloc.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/mcache.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/mcentral.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/mem.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/mem_posix_memalign.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/mfixalloc.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/mgc0.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/mheap.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/msize.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/panic.Plo@am__quote@\n-@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/parfor.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/print.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/proc.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/rtems-task-variable-add.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/runtime_c.Plo@am__quote@\n+@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/stack.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/thread-linux.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/thread-sema.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/thread.Plo@am__quote@\n@@ -1618,13 +1607,6 @@ go-assert.lo: runtime/go-assert.c\n @AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n @am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o go-assert.lo `test -f 'runtime/go-assert.c' || echo '$(srcdir)/'`runtime/go-assert.c\n \n-go-breakpoint.lo: runtime/go-breakpoint.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT go-breakpoint.lo -MD -MP -MF $(DEPDIR)/go-breakpoint.Tpo -c -o go-breakpoint.lo `test -f 'runtime/go-breakpoint.c' || echo '$(srcdir)/'`runtime/go-breakpoint.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/go-breakpoint.Tpo $(DEPDIR)/go-breakpoint.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/go-breakpoint.c' object='go-breakpoint.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o go-breakpoint.lo `test -f 'runtime/go-breakpoint.c' || echo '$(srcdir)/'`runtime/go-breakpoint.c\n-\n go-caller.lo: runtime/go-caller.c\n @am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT go-caller.lo -MD -MP -MF $(DEPDIR)/go-caller.Tpo -c -o go-caller.lo `test -f 'runtime/go-caller.c' || echo '$(srcdir)/'`runtime/go-caller.c\n @am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/go-caller.Tpo $(DEPDIR)/go-caller.Plo\n@@ -1723,13 +1705,6 @@ go-now.lo: runtime/go-now.c\n @AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n @am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o go-now.lo `test -f 'runtime/go-now.c' || echo '$(srcdir)/'`runtime/go-now.c\n \n-go-new.lo: runtime/go-new.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT go-new.lo -MD -MP -MF $(DEPDIR)/go-new.Tpo -c -o go-new.lo `test -f 'runtime/go-new.c' || echo '$(srcdir)/'`runtime/go-new.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/go-new.Tpo $(DEPDIR)/go-new.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/go-new.c' object='go-new.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o go-new.lo `test -f 'runtime/go-new.c' || echo '$(srcdir)/'`runtime/go-new.c\n-\n go-nosys.lo: runtime/go-nosys.c\n @am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT go-nosys.lo -MD -MP -MF $(DEPDIR)/go-nosys.Tpo -c -o go-nosys.lo `test -f 'runtime/go-nosys.c' || echo '$(srcdir)/'`runtime/go-nosys.c\n @am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/go-nosys.Tpo $(DEPDIR)/go-nosys.Plo\n@@ -1779,20 +1754,6 @@ go-typedesc-equal.lo: runtime/go-typedesc-equal.c\n @AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n @am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o go-typedesc-equal.lo `test -f 'runtime/go-typedesc-equal.c' || echo '$(srcdir)/'`runtime/go-typedesc-equal.c\n \n-go-unsafe-new.lo: runtime/go-unsafe-new.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT go-unsafe-new.lo -MD -MP -MF $(DEPDIR)/go-unsafe-new.Tpo -c -o go-unsafe-new.lo `test -f 'runtime/go-unsafe-new.c' || echo '$(srcdir)/'`runtime/go-unsafe-new.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/go-unsafe-new.Tpo $(DEPDIR)/go-unsafe-new.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/go-unsafe-new.c' object='go-unsafe-new.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o go-unsafe-new.lo `test -f 'runtime/go-unsafe-new.c' || echo '$(srcdir)/'`runtime/go-unsafe-new.c\n-\n-go-unsafe-newarray.lo: runtime/go-unsafe-newarray.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT go-unsafe-newarray.lo -MD -MP -MF $(DEPDIR)/go-unsafe-newarray.Tpo -c -o go-unsafe-newarray.lo `test -f 'runtime/go-unsafe-newarray.c' || echo '$(srcdir)/'`runtime/go-unsafe-newarray.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/go-unsafe-newarray.Tpo $(DEPDIR)/go-unsafe-newarray.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/go-unsafe-newarray.c' object='go-unsafe-newarray.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o go-unsafe-newarray.lo `test -f 'runtime/go-unsafe-newarray.c' || echo '$(srcdir)/'`runtime/go-unsafe-newarray.c\n-\n go-unsafe-pointer.lo: runtime/go-unsafe-pointer.c\n @am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT go-unsafe-pointer.lo -MD -MP -MF $(DEPDIR)/go-unsafe-pointer.Tpo -c -o go-unsafe-pointer.lo `test -f 'runtime/go-unsafe-pointer.c' || echo '$(srcdir)/'`runtime/go-unsafe-pointer.c\n @am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/go-unsafe-pointer.Tpo $(DEPDIR)/go-unsafe-pointer.Plo\n@@ -1828,83 +1789,13 @@ env_posix.lo: runtime/env_posix.c\n @AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n @am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o env_posix.lo `test -f 'runtime/env_posix.c' || echo '$(srcdir)/'`runtime/env_posix.c\n \n-heapdump.lo: runtime/heapdump.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT heapdump.lo -MD -MP -MF $(DEPDIR)/heapdump.Tpo -c -o heapdump.lo `test -f 'runtime/heapdump.c' || echo '$(srcdir)/'`runtime/heapdump.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/heapdump.Tpo $(DEPDIR)/heapdump.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/heapdump.c' object='heapdump.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o heapdump.lo `test -f 'runtime/heapdump.c' || echo '$(srcdir)/'`runtime/heapdump.c\n-\n-mcache.lo: runtime/mcache.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT mcache.lo -MD -MP -MF $(DEPDIR)/mcache.Tpo -c -o mcache.lo `test -f 'runtime/mcache.c' || echo '$(srcdir)/'`runtime/mcache.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/mcache.Tpo $(DEPDIR)/mcache.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/mcache.c' object='mcache.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o mcache.lo `test -f 'runtime/mcache.c' || echo '$(srcdir)/'`runtime/mcache.c\n-\n-mcentral.lo: runtime/mcentral.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT mcentral.lo -MD -MP -MF $(DEPDIR)/mcentral.Tpo -c -o mcentral.lo `test -f 'runtime/mcentral.c' || echo '$(srcdir)/'`runtime/mcentral.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/mcentral.Tpo $(DEPDIR)/mcentral.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/mcentral.c' object='mcentral.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o mcentral.lo `test -f 'runtime/mcentral.c' || echo '$(srcdir)/'`runtime/mcentral.c\n-\n-mem_posix_memalign.lo: runtime/mem_posix_memalign.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT mem_posix_memalign.lo -MD -MP -MF $(DEPDIR)/mem_posix_memalign.Tpo -c -o mem_posix_memalign.lo `test -f 'runtime/mem_posix_memalign.c' || echo '$(srcdir)/'`runtime/mem_posix_memalign.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/mem_posix_memalign.Tpo $(DEPDIR)/mem_posix_memalign.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/mem_posix_memalign.c' object='mem_posix_memalign.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o mem_posix_memalign.lo `test -f 'runtime/mem_posix_memalign.c' || echo '$(srcdir)/'`runtime/mem_posix_memalign.c\n-\n-mem.lo: runtime/mem.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT mem.lo -MD -MP -MF $(DEPDIR)/mem.Tpo -c -o mem.lo `test -f 'runtime/mem.c' || echo '$(srcdir)/'`runtime/mem.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/mem.Tpo $(DEPDIR)/mem.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/mem.c' object='mem.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o mem.lo `test -f 'runtime/mem.c' || echo '$(srcdir)/'`runtime/mem.c\n-\n-mfixalloc.lo: runtime/mfixalloc.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT mfixalloc.lo -MD -MP -MF $(DEPDIR)/mfixalloc.Tpo -c -o mfixalloc.lo `test -f 'runtime/mfixalloc.c' || echo '$(srcdir)/'`runtime/mfixalloc.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/mfixalloc.Tpo $(DEPDIR)/mfixalloc.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/mfixalloc.c' object='mfixalloc.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o mfixalloc.lo `test -f 'runtime/mfixalloc.c' || echo '$(srcdir)/'`runtime/mfixalloc.c\n-\n-mgc0.lo: runtime/mgc0.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT mgc0.lo -MD -MP -MF $(DEPDIR)/mgc0.Tpo -c -o mgc0.lo `test -f 'runtime/mgc0.c' || echo '$(srcdir)/'`runtime/mgc0.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/mgc0.Tpo $(DEPDIR)/mgc0.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/mgc0.c' object='mgc0.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o mgc0.lo `test -f 'runtime/mgc0.c' || echo '$(srcdir)/'`runtime/mgc0.c\n-\n-mheap.lo: runtime/mheap.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT mheap.lo -MD -MP -MF $(DEPDIR)/mheap.Tpo -c -o mheap.lo `test -f 'runtime/mheap.c' || echo '$(srcdir)/'`runtime/mheap.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/mheap.Tpo $(DEPDIR)/mheap.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/mheap.c' object='mheap.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o mheap.lo `test -f 'runtime/mheap.c' || echo '$(srcdir)/'`runtime/mheap.c\n-\n-msize.lo: runtime/msize.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT msize.lo -MD -MP -MF $(DEPDIR)/msize.Tpo -c -o msize.lo `test -f 'runtime/msize.c' || echo '$(srcdir)/'`runtime/msize.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/msize.Tpo $(DEPDIR)/msize.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/msize.c' object='msize.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o msize.lo `test -f 'runtime/msize.c' || echo '$(srcdir)/'`runtime/msize.c\n-\n panic.lo: runtime/panic.c\n @am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT panic.lo -MD -MP -MF $(DEPDIR)/panic.Tpo -c -o panic.lo `test -f 'runtime/panic.c' || echo '$(srcdir)/'`runtime/panic.c\n @am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/panic.Tpo $(DEPDIR)/panic.Plo\n @AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/panic.c' object='panic.lo' libtool=yes @AMDEPBACKSLASH@\n @AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n @am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o panic.lo `test -f 'runtime/panic.c' || echo '$(srcdir)/'`runtime/panic.c\n \n-parfor.lo: runtime/parfor.c\n-@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT parfor.lo -MD -MP -MF $(DEPDIR)/parfor.Tpo -c -o parfor.lo `test -f 'runtime/parfor.c' || echo '$(srcdir)/'`runtime/parfor.c\n-@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/parfor.Tpo $(DEPDIR)/parfor.Plo\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/parfor.c' object='parfor.lo' libtool=yes @AMDEPBACKSLASH@\n-@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n-@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o parfor.lo `test -f 'runtime/parfor.c' || echo '$(srcdir)/'`runtime/parfor.c\n-\n print.lo: runtime/print.c\n @am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT print.lo -MD -MP -MF $(DEPDIR)/print.Tpo -c -o print.lo `test -f 'runtime/print.c' || echo '$(srcdir)/'`runtime/print.c\n @am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/print.Tpo $(DEPDIR)/print.Plo\n@@ -1926,6 +1817,13 @@ runtime_c.lo: runtime/runtime_c.c\n @AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n @am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o runtime_c.lo `test -f 'runtime/runtime_c.c' || echo '$(srcdir)/'`runtime/runtime_c.c\n \n+stack.lo: runtime/stack.c\n+@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT stack.lo -MD -MP -MF $(DEPDIR)/stack.Tpo -c -o stack.lo `test -f 'runtime/stack.c' || echo '$(srcdir)/'`runtime/stack.c\n+@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/stack.Tpo $(DEPDIR)/stack.Plo\n+@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/stack.c' object='stack.lo' libtool=yes @AMDEPBACKSLASH@\n+@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n+@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o stack.lo `test -f 'runtime/stack.c' || echo '$(srcdir)/'`runtime/stack.c\n+\n thread.lo: runtime/thread.c\n @am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT thread.lo -MD -MP -MF $(DEPDIR)/thread.Tpo -c -o thread.lo `test -f 'runtime/thread.c' || echo '$(srcdir)/'`runtime/thread.c\n @am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/thread.Tpo $(DEPDIR)/thread.Plo\n@@ -1968,6 +1866,13 @@ getncpu-none.lo: runtime/getncpu-none.c\n @AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n @am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o getncpu-none.lo `test -f 'runtime/getncpu-none.c' || echo '$(srcdir)/'`runtime/getncpu-none.c\n \n+getncpu-aix.lo: runtime/getncpu-aix.c\n+@am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT getncpu-aix.lo -MD -MP -MF $(DEPDIR)/getncpu-aix.Tpo -c -o getncpu-aix.lo `test -f 'runtime/getncpu-aix.c' || echo '$(srcdir)/'`runtime/getncpu-aix.c\n+@am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/getncpu-aix.Tpo $(DEPDIR)/getncpu-aix.Plo\n+@AMDEP_TRUE@@am__fastdepCC_FALSE@\tsource='runtime/getncpu-aix.c' object='getncpu-aix.lo' libtool=yes @AMDEPBACKSLASH@\n+@AMDEP_TRUE@@am__fastdepCC_FALSE@\tDEPDIR=$(DEPDIR) $(CCDEPMODE) $(depcomp) @AMDEPBACKSLASH@\n+@am__fastdepCC_FALSE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -c -o getncpu-aix.lo `test -f 'runtime/getncpu-aix.c' || echo '$(srcdir)/'`runtime/getncpu-aix.c\n+\n getncpu-bsd.lo: runtime/getncpu-bsd.c\n @am__fastdepCC_TRUE@\t$(LIBTOOL)  --tag=CC $(AM_LIBTOOLFLAGS) $(LIBTOOLFLAGS) --mode=compile $(CC) $(DEFS) $(DEFAULT_INCLUDES) $(INCLUDES) $(AM_CPPFLAGS) $(CPPFLAGS) $(AM_CFLAGS) $(CFLAGS) -MT getncpu-bsd.lo -MD -MP -MF $(DEPDIR)/getncpu-bsd.Tpo -c -o getncpu-bsd.lo `test -f 'runtime/getncpu-bsd.c' || echo '$(srcdir)/'`runtime/getncpu-bsd.c\n @am__fastdepCC_TRUE@\t$(am__mv) $(DEPDIR)/getncpu-bsd.Tpo $(DEPDIR)/getncpu-bsd.Plo\n@@ -2899,6 +2804,7 @@ install-strip:\n \t    \"INSTALL_PROGRAM_ENV=STRIPPROG='$(STRIP)'\" install; \\\n \tfi\n mostlyclean-generic:\n+\t-test -z \"$(MOSTLYCLEANFILES)\" || rm -f $(MOSTLYCLEANFILES)\n \n clean-generic:\n \t-test -z \"$(CLEANFILES)\" || rm -f $(CLEANFILES)\n@@ -2921,7 +2827,7 @@ distclean: distclean-multi distclean-recursive\n \t-rm -rf ./$(DEPDIR)\n \t-rm -f Makefile\n distclean-am: clean-am distclean-compile distclean-generic \\\n-\tdistclean-hdr distclean-libtool distclean-tags\n+\tdistclean-hdr distclean-libtool distclean-local distclean-tags\n \n dvi: dvi-recursive\n \n@@ -3049,15 +2955,15 @@ uninstall-am: uninstall-toolexeclibLIBRARIES \\\n \tclean-generic clean-libtool clean-local clean-multi \\\n \tclean-toolexeclibLIBRARIES clean-toolexeclibLTLIBRARIES ctags \\\n \tctags-recursive distclean distclean-compile distclean-generic \\\n-\tdistclean-hdr distclean-libtool distclean-multi distclean-tags \\\n-\tdvi dvi-am html html-am info info-am install install-am \\\n-\tinstall-data install-data-am install-dvi install-dvi-am \\\n-\tinstall-exec install-exec-am install-html install-html-am \\\n-\tinstall-info install-info-am install-man install-multi \\\n-\tinstall-pdf install-pdf-am install-ps install-ps-am \\\n-\tinstall-strip install-toolexeclibLIBRARIES \\\n-\tinstall-toolexeclibLTLIBRARIES install-toolexeclibgoDATA \\\n-\tinstall-toolexeclibgoarchiveDATA \\\n+\tdistclean-hdr distclean-libtool distclean-local \\\n+\tdistclean-multi distclean-tags dvi dvi-am html html-am info \\\n+\tinfo-am install install-am install-data install-data-am \\\n+\tinstall-dvi install-dvi-am install-exec install-exec-am \\\n+\tinstall-html install-html-am install-info install-info-am \\\n+\tinstall-man install-multi install-pdf install-pdf-am \\\n+\tinstall-ps install-ps-am install-strip \\\n+\tinstall-toolexeclibLIBRARIES install-toolexeclibLTLIBRARIES \\\n+\tinstall-toolexeclibgoDATA install-toolexeclibgoarchiveDATA \\\n \tinstall-toolexeclibgocompressDATA \\\n \tinstall-toolexeclibgocontainerDATA \\\n \tinstall-toolexeclibgocryptoDATA \\\n@@ -3118,26 +3024,13 @@ uninstall-am: uninstall-toolexeclibLIBRARIES \\\n \tuninstall-toolexeclibgounicodeDATA\n \n \n-goc2c.$(OBJEXT): runtime/goc2c.c\n-\t$(CC_FOR_BUILD) -c $(CFLAGS_FOR_BUILD) $<\n-\n-goc2c: goc2c.$(OBJEXT)\n-\t$(CC_FOR_BUILD) $(CFLAGS_FOR_BUILD) $(LDFLAGS_FOR_BUILD) -o $@ $<\n-\n-malloc.c: $(srcdir)/runtime/malloc.goc goc2c\n-\t./goc2c $< > $@.tmp\n-\tmv -f $@.tmp $@\n-\n-%.c: $(srcdir)/runtime/%.goc goc2c\n-\t./goc2c $< > $@.tmp\n-\tmv -f $@.tmp $@\n-\n version.go: s-version; @true\n s-version: Makefile\n \trm -f version.go.tmp\n \techo \"package sys\" > version.go.tmp\n \techo 'const DefaultGoroot = \"$(prefix)\"' >> version.go.tmp\n \techo 'const TheVersion = \"'`cat $(srcdir)/VERSION | sed 1q`' '`$(GOC) --version | sed 1q`'\"' >> version.go.tmp\n+\techo 'const Goexperiment = ``' >> version.go.tmp\n \techo 'const GOARCH = \"'$(GOARCH)'\"' >> version.go.tmp\n \techo 'const GOOS = \"'$(GOOS)'\"' >> version.go.tmp\n \techo 'const GccgoToolDir = \"$(libexecsubdir)\"' >> version.go.tmp\n@@ -3188,7 +3081,7 @@ s-version: Makefile\n \n runtime_sysinfo.go: s-runtime_sysinfo; @true\n s-runtime_sysinfo: $(srcdir)/mkrsysinfo.sh gen-sysinfo.go\n-\t$(SHELL) $(srcdir)/mkrsysinfo.sh\n+\tGOOS=$(GOOS) $(SHELL) $(srcdir)/mkrsysinfo.sh\n \t$(SHELL) $(srcdir)/mvifdiff.sh tmp-runtime_sysinfo.go runtime_sysinfo.go\n \t$(STAMP) $@\n \n@@ -3198,14 +3091,23 @@ s-sigtab: $(srcdir)/mksigtab.sh gen-sysinfo.go\n \t$(SHELL) $(srcdir)/mvifdiff.sh tmp-sigtab.go sigtab.go\n \t$(STAMP) $@\n \n+# _Complex_lock and _Reader_lock are Go translations of some AIX system\n+# types and should not be exported back to C\n+# sigset conflicts with system type sigset on AIX, so we need to rename it\n runtime.inc: s-runtime-inc; @true\n s-runtime-inc: runtime.lo Makefile\n-\trm -f runtime.inc.tmp2\n-\tgrep -v \"#define _\" runtime.inc.tmp | grep -v \"#define [cm][01234] \" > runtime.inc.tmp2\n+\trm -f runtime.inc.tmp2 runtime.inc.tmp3\n+\tgrep -v \"#define _\" runtime.inc.tmp | grep -v \"#define [cm][01234] \" | grep -v \"#define empty \" > runtime.inc.tmp2\n \tfor pattern in '_[GP][a-z]' _Max _Lock _Sig _Trace _MHeap _Num; do \\\n \t  grep \"#define $$pattern\" runtime.inc.tmp >> runtime.inc.tmp2; \\\n \tdone\n-\t$(SHELL) $(srcdir)/mvifdiff.sh runtime.inc.tmp2 runtime.inc\n+\tfor TYPE in _Complex_lock _Reader_lock; do \\\n+\t  sed -e '/struct '$${TYPE}' {/,/^}/s/^.*$$//' runtime.inc.tmp2 > runtime.inc.tmp3; \\\n+\t  mv runtime.inc.tmp3 runtime.inc.tmp2; \\\n+\tdone\n+\tsed -e 's/sigset/sigset_go/' runtime.inc.tmp2 > runtime.inc.tmp3\n+\t$(SHELL) $(srcdir)/mvifdiff.sh runtime.inc.tmp3 runtime.inc\n+\trm -f runtime.inc.tmp2 runtime.inc.tmp3\n \t$(STAMP) $@\n \n # Generate the list of go std packages that were included in libgo\n@@ -3215,7 +3117,7 @@ s-zstdpkglist: Makefile\n \techo 'package main' > zstdpkglist.go.tmp\n \techo \"\" >> zstdpkglist.go.tmp\n \techo 'var stdpkg = map[string]bool{' >> zstdpkglist.go.tmp\n-\techo $(libgo_go_objs) 'unsafe.lo' 'runtime/cgo.lo' | sed 's/\\.lo /\\\": true,\\n/g' | grep -v _c | sed 's/\\.lo/\\\": true,/' | sed 's/^/\\t\\\"/' | sort -u >> zstdpkglist.go.tmp\n+\techo $(libgo_go_objs) 'unsafe.lo' 'runtime/cgo.lo' | sed 's|[a-z0-9_/]*_c\\.lo||g' | sed 's|\\([a-z0-9_/]*\\)\\.lo|\"\\1\": true,|g' >> zstdpkglist.go.tmp\n \techo '}' >> zstdpkglist.go.tmp\n \t$(SHELL) $(srcdir)/mvifdiff.sh zstdpkglist.go.tmp zstdpkglist.go\n \t$(STAMP) $@\n@@ -3258,7 +3160,7 @@ s-errno:\n \n sysinfo.go: s-sysinfo; @true\n s-sysinfo: $(srcdir)/mksysinfo.sh gen-sysinfo.go errno.i\n-\t$(SHELL) $(srcdir)/mksysinfo.sh\n+\tGOOS=$(GOOS) $(SHELL) $(srcdir)/mksysinfo.sh\n \t$(SHELL) $(srcdir)/mvifdiff.sh tmp-sysinfo.go sysinfo.go\n \t$(STAMP) $@\n \n@@ -3496,6 +3398,11 @@ mostlyclean-local:\n clean-local:\n \tfind . -name '*.la' -print | xargs $(LIBTOOL) --mode=clean rm -f\n \tfind . -name '*.a' -print | xargs rm -f\n+\tfind . -name '*.gox' -print | xargs rm -f\n+\tfind . -name '*.s-gox' -print | xargs rm -f\n+\n+distclean-local:\n+\tfind . -name '*.lo.dep' -print | xargs rm -f\n \n # Tell versions [3.59,3.63) of GNU make to not export all variables.\n # Otherwise a system limit (for SysV at least) may be exceeded."}, {"sha": "17afd76ca6c5020e405d16c28fc7769839e4b528", "filename": "libgo/config.h.in", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fconfig.h.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fconfig.h.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fconfig.h.in?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -259,6 +259,9 @@\n /* Define to 1 if you have the `sync_file_range' function. */\n #undef HAVE_SYNC_FILE_RANGE\n \n+/* Define to 1 if you have the `syscall' function. */\n+#undef HAVE_SYSCALL\n+\n /* Define to 1 if you have the <syscall.h> header file. */\n #undef HAVE_SYSCALL_H\n "}, {"sha": "556387a180f810fff5e2b2510657c4b40ce75bdf", "filename": "libgo/configure", "status": "modified", "additions": 28, "deletions": 4, "changes": 32, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fconfigure", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fconfigure", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fconfigure?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -646,6 +646,8 @@ ALLGOOS\n GOOS\n LIBGO_IS_BSD_FALSE\n LIBGO_IS_BSD_TRUE\n+LIBGO_IS_AIX_FALSE\n+LIBGO_IS_AIX_TRUE\n LIBGO_IS_SOLARIS_FALSE\n LIBGO_IS_SOLARIS_TRUE\n LIBGO_IS_RTEMS_FALSE\n@@ -4327,6 +4329,14 @@ ac_compiler_gnu=$ac_cv_c_compiler_gnu\n \n \n \n+case ${host} in\n+  *-*-aix*)\n+    # static hash tables crashes on AIX when libgo is built with O2\n+    CFLAGS=\"$CFLAGS -fno-section-anchors\"\n+    GOCFLAGS=\"$GOCFLAGS -fno-section-anchors\"\n+    ;;\n+esac\n+\n \n { $as_echo \"$as_me:${as_lineno-$LINENO}: checking whether to enable maintainer-specific portions of Makefiles\" >&5\n $as_echo_n \"checking whether to enable maintainer-specific portions of Makefiles... \" >&6; }\n@@ -11104,7 +11114,7 @@ else\n   lt_dlunknown=0; lt_dlno_uscore=1; lt_dlneed_uscore=2\n   lt_status=$lt_dlunknown\n   cat > conftest.$ac_ext <<_LT_EOF\n-#line 11107 \"configure\"\n+#line 11117 \"configure\"\n #include \"confdefs.h\"\n \n #if HAVE_DLFCN_H\n@@ -11210,7 +11220,7 @@ else\n   lt_dlunknown=0; lt_dlno_uscore=1; lt_dlneed_uscore=2\n   lt_status=$lt_dlunknown\n   cat > conftest.$ac_ext <<_LT_EOF\n-#line 11213 \"configure\"\n+#line 11223 \"configure\"\n #include \"confdefs.h\"\n \n #if HAVE_DLFCN_H\n@@ -13507,7 +13517,7 @@ go_include=\"-include\"\n # All known GOOS values.  This is the union of all operating systems\n # supported by the gofrontend and all operating systems supported by\n # the gc toolchain.\n-ALLGOOS=\"android darwin dragonfly freebsd irix linux netbsd openbsd plan9 rtems solaris windows\"\n+ALLGOOS=\"aix android darwin dragonfly freebsd irix linux netbsd openbsd plan9 rtems solaris windows\"\n \n is_darwin=no\n is_freebsd=no\n@@ -13518,6 +13528,7 @@ is_openbsd=no\n is_dragonfly=no\n is_rtems=no\n is_solaris=no\n+is_aix=no\n GOOS=unknown\n case ${host} in\n   *-*-darwin*)   is_darwin=yes;  GOOS=darwin ;;\n@@ -13529,6 +13540,7 @@ case ${host} in\n   *-*-dragonfly*) is_dragonfly=yes; GOOS=dragonfly ;;\n   *-*-rtems*)    is_rtems=yes;   GOOS=rtems ;;\n   *-*-solaris2*) is_solaris=yes; GOOS=solaris ;;\n+  *-*-aix*)      is_aix=yes;     GOOS=aix ;;\n esac\n  if test $is_darwin = yes; then\n   LIBGO_IS_DARWIN_TRUE=\n@@ -13602,6 +13614,14 @@ else\n   LIBGO_IS_SOLARIS_FALSE=\n fi\n \n+ if test $is_aix = yes; then\n+  LIBGO_IS_AIX_TRUE=\n+  LIBGO_IS_AIX_FALSE='#'\n+else\n+  LIBGO_IS_AIX_TRUE='#'\n+  LIBGO_IS_AIX_FALSE=\n+fi\n+\n  if test $is_darwin = yes -o $is_dragonfly = yes -o $is_freebsd = yes -o $is_netbsd = yes -o $is_openbsd = yes; then\n   LIBGO_IS_BSD_TRUE=\n   LIBGO_IS_BSD_FALSE='#'\n@@ -14868,7 +14888,7 @@ else\n fi\n \n \n-for ac_func in accept4 dup3 epoll_create1 faccessat fallocate fchmodat fchownat futimesat getxattr inotify_add_watch inotify_init inotify_init1 inotify_rm_watch listxattr mkdirat mknodat open64 openat pipe2 removexattr renameat setxattr sync_file_range splice tee unlinkat unshare utimensat\n+for ac_func in accept4 dup3 epoll_create1 faccessat fallocate fchmodat fchownat futimesat getxattr inotify_add_watch inotify_init inotify_init1 inotify_rm_watch listxattr mkdirat mknodat open64 openat pipe2 removexattr renameat setxattr sync_file_range splice syscall tee unlinkat unshare utimensat\n do :\n   as_ac_var=`$as_echo \"ac_cv_func_$ac_func\" | $as_tr_sh`\n ac_fn_c_check_func \"$LINENO\" \"$ac_func\" \"$as_ac_var\"\n@@ -15753,6 +15773,10 @@ if test -z \"${LIBGO_IS_SOLARIS_TRUE}\" && test -z \"${LIBGO_IS_SOLARIS_FALSE}\"; th\n   as_fn_error \"conditional \\\"LIBGO_IS_SOLARIS\\\" was never defined.\n Usually this means the macro was only invoked conditionally.\" \"$LINENO\" 5\n fi\n+if test -z \"${LIBGO_IS_AIX_TRUE}\" && test -z \"${LIBGO_IS_AIX_FALSE}\"; then\n+  as_fn_error \"conditional \\\"LIBGO_IS_AIX\\\" was never defined.\n+Usually this means the macro was only invoked conditionally.\" \"$LINENO\" 5\n+fi\n if test -z \"${LIBGO_IS_BSD_TRUE}\" && test -z \"${LIBGO_IS_BSD_FALSE}\"; then\n   as_fn_error \"conditional \\\"LIBGO_IS_BSD\\\" was never defined.\n Usually this means the macro was only invoked conditionally.\" \"$LINENO\" 5"}, {"sha": "5a1032f4dab379d296ad35a21dcd05998a32fa4e", "filename": "libgo/configure.ac", "status": "modified", "additions": 13, "deletions": 2, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fconfigure.ac", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fconfigure.ac", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fconfigure.ac?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -31,6 +31,14 @@ m4_rename_force([glibgo_PRECIOUS],[_AC_ARG_VAR_PRECIOUS])\n \n AC_SUBST(CFLAGS)\n \n+case ${host} in\n+  *-*-aix*)\n+    # static hash tables crashes on AIX when libgo is built with O2\n+    CFLAGS=\"$CFLAGS -fno-section-anchors\"\n+    GOCFLAGS=\"$GOCFLAGS -fno-section-anchors\"\n+    ;;\n+esac\n+\n AM_MAINTAINER_MODE\n \n AC_PROG_LD\n@@ -146,7 +154,7 @@ AC_SUBST(go_include)\n # All known GOOS values.  This is the union of all operating systems\n # supported by the gofrontend and all operating systems supported by\n # the gc toolchain.\n-ALLGOOS=\"android darwin dragonfly freebsd irix linux netbsd openbsd plan9 rtems solaris windows\"\n+ALLGOOS=\"aix android darwin dragonfly freebsd irix linux netbsd openbsd plan9 rtems solaris windows\"\n \n is_darwin=no\n is_freebsd=no\n@@ -157,6 +165,7 @@ is_openbsd=no\n is_dragonfly=no\n is_rtems=no\n is_solaris=no\n+is_aix=no\n GOOS=unknown\n case ${host} in\n   *-*-darwin*)   is_darwin=yes;  GOOS=darwin ;;\n@@ -168,6 +177,7 @@ case ${host} in\n   *-*-dragonfly*) is_dragonfly=yes; GOOS=dragonfly ;;\n   *-*-rtems*)    is_rtems=yes;   GOOS=rtems ;;\n   *-*-solaris2*) is_solaris=yes; GOOS=solaris ;;\n+  *-*-aix*)      is_aix=yes;     GOOS=aix ;;\n esac\n AM_CONDITIONAL(LIBGO_IS_DARWIN, test $is_darwin = yes)\n AM_CONDITIONAL(LIBGO_IS_FREEBSD, test $is_freebsd = yes)\n@@ -178,6 +188,7 @@ AM_CONDITIONAL(LIBGO_IS_OPENBSD, test $is_openbsd = yes)\n AM_CONDITIONAL(LIBGO_IS_DRAGONFLY, test $is_dragonfly = yes)\n AM_CONDITIONAL(LIBGO_IS_RTEMS, test $is_rtems = yes)\n AM_CONDITIONAL(LIBGO_IS_SOLARIS, test $is_solaris = yes)\n+AM_CONDITIONAL(LIBGO_IS_AIX, test $is_aix = yes)\n AM_CONDITIONAL(LIBGO_IS_BSD, test $is_darwin = yes -o $is_dragonfly = yes -o $is_freebsd = yes -o $is_netbsd = yes -o $is_openbsd = yes)\n AC_SUBST(GOOS)\n AC_SUBST(ALLGOOS)\n@@ -601,7 +612,7 @@ AC_CHECK_FUNCS(strerror_r strsignal wait4 mincore setenv unsetenv dl_iterate_phd\n AM_CONDITIONAL(HAVE_STRERROR_R, test \"$ac_cv_func_strerror_r\" = yes)\n AM_CONDITIONAL(HAVE_WAIT4, test \"$ac_cv_func_wait4\" = yes)\n \n-AC_CHECK_FUNCS(accept4 dup3 epoll_create1 faccessat fallocate fchmodat fchownat futimesat getxattr inotify_add_watch inotify_init inotify_init1 inotify_rm_watch listxattr mkdirat mknodat open64 openat pipe2 removexattr renameat setxattr sync_file_range splice tee unlinkat unshare utimensat)\n+AC_CHECK_FUNCS(accept4 dup3 epoll_create1 faccessat fallocate fchmodat fchownat futimesat getxattr inotify_add_watch inotify_init inotify_init1 inotify_rm_watch listxattr mkdirat mknodat open64 openat pipe2 removexattr renameat setxattr sync_file_range splice syscall tee unlinkat unshare utimensat)\n AC_TYPE_OFF_T\n AC_CHECK_TYPES([loff_t])\n "}, {"sha": "ad01952bb734784c20f049126358277f4b6f3990", "filename": "libgo/go/bytes/bytes_test.go", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fbytes%2Fbytes_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fbytes%2Fbytes_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fbytes%2Fbytes_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -10,6 +10,7 @@ import (\n \t\"internal/testenv\"\n \t\"math/rand\"\n \t\"reflect\"\n+\t\"runtime\"\n \t\"strings\"\n \t\"testing\"\n \t\"unicode\"\n@@ -392,7 +393,11 @@ func TestIndexRune(t *testing.T) {\n \t\t}\n \t})\n \tif allocs != 0 {\n-\t\tt.Errorf(\"expected no allocations, got %f\", allocs)\n+\t\tif runtime.Compiler == \"gccgo\" {\n+\t\t\tt.Log(\"does not work on gccgo without better escape analysis\")\n+\t\t} else {\n+\t\t\tt.Errorf(\"expected no allocations, got %f\", allocs)\n+\t\t}\n \t}\n }\n "}, {"sha": "045d037d209f314302a006ba321221db6ab89ec4", "filename": "libgo/go/crypto/rand/eagain.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fcrypto%2Frand%2Feagain.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fcrypto%2Frand%2Feagain.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fcrypto%2Frand%2Feagain.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package rand\n "}, {"sha": "ec474d36ccfa31c0de18632a8b64ba1e4e68ede9", "filename": "libgo/go/crypto/rand/rand_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fcrypto%2Frand%2Frand_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fcrypto%2Frand%2Frand_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fcrypto%2Frand%2Frand_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd plan9 solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd plan9 solaris\n \n // Unix cryptographically secure pseudorandom number\n // generator."}, {"sha": "de5702de35d874aa2253e7674b6f826e171039f3", "filename": "libgo/go/crypto/x509/root_aix.go", "status": "added", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fcrypto%2Fx509%2Froot_aix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fcrypto%2Fx509%2Froot_aix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fcrypto%2Fx509%2Froot_aix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,8 @@\n+// Copyright 2017 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package x509\n+\n+// Possible certificate files; stop after finding one.\n+var certFiles []string"}, {"sha": "c44a5241685fa9ee1ed6b93e354fdcfa8ad1300e", "filename": "libgo/go/crypto/x509/root_unix.go", "status": "modified", "additions": 2, "deletions": 1, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fcrypto%2Fx509%2Froot_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fcrypto%2Fx509%2Froot_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fcrypto%2Fx509%2Froot_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package x509\n \n@@ -16,6 +16,7 @@ import (\n var certDirectories = []string{\n \t\"/etc/ssl/certs\",               // SLES10/SLES11, https://golang.org/issue/12139\n \t\"/system/etc/security/cacerts\", // Android\n+\t\"/var/ssl/certs\",               // AIX\n }\n \n func (c *Certificate) systemVerify(opts *VerifyOptions) (chains [][]*Certificate, err error) {"}, {"sha": "fe9820ee66959ec0bb442973dc28bdb90a90d13e", "filename": "libgo/go/go/build/syslist.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fgo%2Fbuild%2Fsyslist.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fgo%2Fbuild%2Fsyslist.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fgo%2Fbuild%2Fsyslist.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -4,5 +4,5 @@\n \n package build\n \n-const goosList = \"android darwin dragonfly freebsd linux nacl netbsd openbsd plan9 solaris windows zos \"\n+const goosList = \"aix android darwin dragonfly freebsd linux nacl netbsd openbsd plan9 solaris windows zos \"\n const goarchList = \"386 amd64 amd64p32 arm armbe arm64 arm64be alpha m68k ppc64 ppc64le mips mipsle mips64 mips64le mips64p32 mips64p32le mipso32 mipsn32 mipsn64 mipso64 ppc s390 s390x sparc sparc64 \""}, {"sha": "4c9eda4d7313a18022d6f4ff9c70fc6d2a1a70a7", "filename": "libgo/go/math/atan.go", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fmath%2Fatan.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fmath%2Fatan.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fmath%2Fatan.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -97,6 +97,9 @@ func satan(x float64) float64 {\n func libc_atan(float64) float64\n \n func Atan(x float64) float64 {\n+\tif x == 0 {\n+\t\treturn x\n+\t}\n \treturn libc_atan(x)\n }\n "}, {"sha": "74940436f92c1553e9ce62cf11355a1e4bbb018a", "filename": "libgo/go/math/expm1.go", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fmath%2Fexpm1.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fmath%2Fexpm1.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fmath%2Fexpm1.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -126,6 +126,9 @@ package math\n func libc_expm1(float64) float64\n \n func Expm1(x float64) float64 {\n+\tif x == 0 {\n+\t\treturn x\n+\t}\n \treturn libc_expm1(x)\n }\n "}, {"sha": "044495aaf2322f51ae12f4b53f65cf7af065161a", "filename": "libgo/go/math/log1p.go", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fmath%2Flog1p.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fmath%2Flog1p.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fmath%2Flog1p.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -97,6 +97,9 @@ package math\n func libc_log1p(float64) float64\n \n func Log1p(x float64) float64 {\n+\tif x == 0 {\n+\t\treturn x\n+\t}\n \treturn libc_log1p(x)\n }\n "}, {"sha": "8e177ca14ee9d776a39998e4650565c79983a6d3", "filename": "libgo/go/mime/type_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fmime%2Ftype_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fmime%2Ftype_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fmime%2Ftype_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package mime\n "}, {"sha": "7c0dfe261c4c15486a7e144b07a381353b7677e0", "filename": "libgo/go/net/addrselect.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Faddrselect.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Faddrselect.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Faddrselect.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris\n \n // Minimal RFC 6724 address selection.\n "}, {"sha": "4f23d9b1771ba3cc9f2516c4e2e9ff3d485141a0", "filename": "libgo/go/net/cgo_aix.go", "status": "added", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fcgo_aix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fcgo_aix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fcgo_aix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,13 @@\n+// Copyright 2011 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// +build cgo,!netgo\n+\n+package net\n+\n+import (\n+\t\"syscall\"\n+)\n+\n+const cgoAddrInfoFlags = syscall.AI_CANONNAME"}, {"sha": "81b39c9c77a1f90a850669c3b5da3002c8491046", "filename": "libgo/go/net/cgo_resnew.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fcgo_resnew.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fcgo_resnew.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fcgo_resnew.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -3,7 +3,7 @@\n // license that can be found in the LICENSE file.\n \n // +build cgo,!netgo\n-// +build darwin linux,!android netbsd solaris\n+// +build aix darwin linux,!android netbsd solaris\n \n package net\n "}, {"sha": "25d4f67b4cbe8d820be863cc4ce5b4728e0985ea", "filename": "libgo/go/net/cgo_sockold.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fcgo_sockold.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fcgo_sockold.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fcgo_sockold.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -3,7 +3,7 @@\n // license that can be found in the LICENSE file.\n \n // +build cgo,!netgo\n-// +build darwin dragonfly freebsd netbsd openbsd\n+// +build aix darwin dragonfly freebsd netbsd openbsd\n \n package net\n "}, {"sha": "09cfb2a71ae2f2b5a30d8832b9de3d6dd1542237", "filename": "libgo/go/net/cgo_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fcgo_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fcgo_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fcgo_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -3,7 +3,7 @@\n // license that can be found in the LICENSE file.\n \n // +build cgo,!netgo\n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris\n \n package net\n "}, {"sha": "e579198e09a111cb443565fc44b35710d92536e9", "filename": "libgo/go/net/cgo_unix_test.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fcgo_unix_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fcgo_unix_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fcgo_unix_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -3,7 +3,7 @@\n // license that can be found in the LICENSE file.\n \n // +build cgo,!netgo\n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris\n \n package net\n "}, {"sha": "a79869981ad7b287bfa4dd59650890beec4ff8ee", "filename": "libgo/go/net/conf.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fconf.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fconf.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fconf.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris\n \n package net\n "}, {"sha": "d5c6dde28183210fdf1defad1a3c21b4be4ac225", "filename": "libgo/go/net/dial_unix_test.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fdial_unix_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fdial_unix_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fdial_unix_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris\n \n package net\n "}, {"sha": "0647b9c305211bc9bf7703e7accbebe37b580f24", "filename": "libgo/go/net/dnsclient_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fdnsclient_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fdnsclient_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fdnsclient_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris\n \n // DNS client: see RFC 1035.\n // Has to be linked into package net for Dial."}, {"sha": "c66d2d196b214e6eae578cf076667e3c21b8b3a1", "filename": "libgo/go/net/dnsclient_unix_test.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fdnsclient_unix_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fdnsclient_unix_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fdnsclient_unix_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris\n \n package net\n "}, {"sha": "24487af33b5f1240c38e4cf0e802922e36d55414", "filename": "libgo/go/net/dnsconfig_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fdnsconfig_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fdnsconfig_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fdnsconfig_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris\n \n // Read system DNS config from /etc/resolv.conf\n "}, {"sha": "0797559d1a208a604581051300556a9fc09762d6", "filename": "libgo/go/net/dnsconfig_unix_test.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fdnsconfig_unix_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fdnsconfig_unix_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fdnsconfig_unix_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris\n \n package net\n "}, {"sha": "4ea92cb1f87435717b1d3b72fa459b5dc0c1bd33", "filename": "libgo/go/net/fd_poll_runtime.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Ffd_poll_runtime.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Ffd_poll_runtime.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Ffd_poll_runtime.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd windows solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd windows solaris\n \n package net\n "}, {"sha": "72304796e4d72b4c1e24b6c9f6dd8628225eb7c9", "filename": "libgo/go/net/fd_posix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Ffd_posix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Ffd_posix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Ffd_posix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n \n package net\n "}, {"sha": "b6ee05976dd415b62ece23396ad0cc1ecbbffe20", "filename": "libgo/go/net/fd_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Ffd_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Ffd_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Ffd_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package net\n "}, {"sha": "b47a6143e9a76e27aae49aff970f8b7066d668ab", "filename": "libgo/go/net/file_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Ffile_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Ffile_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Ffile_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris\n \n package net\n "}, {"sha": "b2522a2ed2329d40b18e157b2a9b96f4e5468e7f", "filename": "libgo/go/net/hook_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fhook_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fhook_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fhook_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package net\n "}, {"sha": "6d7147e87b8d935f0b2d50b129b57bc3a01fe78a", "filename": "libgo/go/net/interface_stub.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Finterface_stub.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Finterface_stub.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Finterface_stub.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build nacl\n+// +build aix nacl\n \n package net\n "}, {"sha": "8fb15f343fa33a5a407945dee7651feca22c8e13", "filename": "libgo/go/net/internal/socktest/switch_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Finternal%2Fsocktest%2Fswitch_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Finternal%2Fsocktest%2Fswitch_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Finternal%2Fsocktest%2Fswitch_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package socktest\n "}, {"sha": "16e65dcc2a864c0047595c39d72497a734e4debb", "filename": "libgo/go/net/iprawsock_posix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fiprawsock_posix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fiprawsock_posix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fiprawsock_posix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n \n package net\n "}, {"sha": "05bf93956bf89946b2e135b0444c916438e3d209", "filename": "libgo/go/net/ipsock_posix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fipsock_posix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fipsock_posix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fipsock_posix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n \n package net\n "}, {"sha": "f96c8beb6148cf90ce477e90481d0359ac3b8cf5", "filename": "libgo/go/net/lookup_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Flookup_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Flookup_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Flookup_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris\n \n package net\n "}, {"sha": "8c8f94479de863635a192bce8730b14c7819be66", "filename": "libgo/go/net/main_unix_test.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fmain_unix_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fmain_unix_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fmain_unix_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package net\n "}, {"sha": "1650f5e301d2888823d6328553293b0027a77ca5", "filename": "libgo/go/net/nss.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fnss.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fnss.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fnss.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris\n \n package net\n "}, {"sha": "3120ba1c7e2d62ebb038b2d914a4521d845ecdbc", "filename": "libgo/go/net/port_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fport_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fport_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fport_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris nacl\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris nacl\n \n // Read system port mappings from /etc/services\n "}, {"sha": "f043062df5820b4d3af475861afc1d85b73e2430", "filename": "libgo/go/net/sendfile_stub.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsendfile_stub.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsendfile_stub.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fsendfile_stub.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin nacl netbsd openbsd\n+// +build aix darwin nacl netbsd openbsd\n \n package net\n "}, {"sha": "6bbfd1208ee807ae42a539880d4f67e10256e952", "filename": "libgo/go/net/sock_posix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsock_posix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsock_posix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fsock_posix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n \n package net\n "}, {"sha": "d1ec02923f52c1d57e02cdfeb3d52200e1dc791f", "filename": "libgo/go/net/sock_stub.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsock_stub.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsock_stub.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fsock_stub.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build nacl solaris\n+// +build aix nacl solaris\n \n package net\n "}, {"sha": "7aef64b48e432b90952d4457f39d020a02cad7c7", "filename": "libgo/go/net/sockopt_aix.go", "status": "added", "additions": 34, "deletions": 0, "changes": 34, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsockopt_aix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsockopt_aix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fsockopt_aix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,34 @@\n+// Copyright 2017 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package net\n+\n+import (\n+\t\"os\"\n+\t\"syscall\"\n+)\n+\n+// This was copied from sockopt_linux.go\n+\n+func setDefaultSockopts(s, family, sotype int, ipv6only bool) error {\n+\tif family == syscall.AF_INET6 && sotype != syscall.SOCK_RAW {\n+\t\t// Allow both IP versions even if the OS default\n+\t\t// is otherwise. Note that some operating systems\n+\t\t// never admit this option.\n+\t\tsyscall.SetsockoptInt(s, syscall.IPPROTO_IPV6, syscall.IPV6_V6ONLY, boolint(ipv6only))\n+\t}\n+\t// Allow broadcast.\n+\treturn os.NewSyscallError(\"setsockopt\", syscall.SetsockoptInt(s, syscall.SOL_SOCKET, syscall.SO_BROADCAST, 1))\n+}\n+\n+func setDefaultListenerSockopts(s int) error {\n+\t// Allow reuse of recently-used addresses.\n+\treturn os.NewSyscallError(\"setsockopt\", syscall.SetsockoptInt(s, syscall.SOL_SOCKET, syscall.SO_REUSEADDR, 1))\n+}\n+\n+func setDefaultMulticastSockopts(s int) error {\n+\t// Allow multicast UDP and raw IP datagram sockets to listen\n+\t// concurrently across multiple listeners.\n+\treturn os.NewSyscallError(\"setsockopt\", syscall.SetsockoptInt(s, syscall.SOL_SOCKET, syscall.SO_REUSEADDR, 1))\n+}"}, {"sha": "cacd04889d93b7c83e2efb46b6dc7a4f2af86dde", "filename": "libgo/go/net/sockopt_posix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsockopt_posix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsockopt_posix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fsockopt_posix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris windows\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris windows\n \n package net\n "}, {"sha": "1e28fe6aadeb6dfb181bf2b628adc92980821575", "filename": "libgo/go/net/sockoptip_aix.go", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsockoptip_aix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsockoptip_aix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fsockoptip_aix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,15 @@\n+// Copyright 2017 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package net\n+\n+import \"syscall\"\n+\n+func setIPv4MulticastInterface(fd *netFD, ifi *Interface) error {\n+\treturn syscall.ENOPROTOOPT\n+}\n+\n+func setIPv4MulticastLoopback(fd *netFD, v bool) error {\n+\treturn syscall.ENOPROTOOPT\n+}"}, {"sha": "4afd4c8ea3f208003b16494f1d0a2819691ea70a", "filename": "libgo/go/net/sockoptip_posix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsockoptip_posix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsockoptip_posix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fsockoptip_posix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd windows\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd windows\n \n package net\n "}, {"sha": "f2ea8425493ccc9b0e4f4160d762f15ad7120435", "filename": "libgo/go/net/sys_cloexec.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsys_cloexec.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fsys_cloexec.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fsys_cloexec.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -5,7 +5,7 @@\n // This file implements sysSocket and accept for platforms that do not\n // provide a fast path for setting SetNonblock and CloseOnExec.\n \n-// +build darwin dragonfly nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly nacl netbsd openbsd solaris\n \n package net\n "}, {"sha": "7533c24d12291405c496f5864aeb85978a99ff44", "filename": "libgo/go/net/tcpsock_posix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Ftcpsock_posix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Ftcpsock_posix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Ftcpsock_posix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n \n package net\n "}, {"sha": "36866ac0418e00f987cf4cc84ddc2b9081171929", "filename": "libgo/go/net/tcpsockopt_posix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Ftcpsockopt_posix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Ftcpsockopt_posix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Ftcpsockopt_posix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris windows\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris windows\n \n package net\n "}, {"sha": "46e5e6d23feb54ddc0c481dfa67e8e68955082a3", "filename": "libgo/go/net/tcpsockopt_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Ftcpsockopt_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Ftcpsockopt_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Ftcpsockopt_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build freebsd linux netbsd\n+// +build aix freebsd linux netbsd\n \n package net\n "}, {"sha": "0c905afafcc7355c3b9f14f0c2c26fcc5864cbe5", "filename": "libgo/go/net/udpsock_posix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fudpsock_posix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Fudpsock_posix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Fudpsock_posix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n \n package net\n "}, {"sha": "945aa031fac6836f0f471d01a962c4959fd4b8a7", "filename": "libgo/go/net/unixsock_posix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Funixsock_posix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fnet%2Funixsock_posix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fnet%2Funixsock_posix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n \n package net\n "}, {"sha": "8923f0e47ae45e42778d022c447560b266edfd20", "filename": "libgo/go/os/dir_gccgo.go", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fdir_gccgo.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fdir_gccgo.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fdir_gccgo.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -6,6 +6,7 @@ package os\n \n import (\n \t\"io\"\n+\t\"runtime\"\n \t\"sync/atomic\"\n \t\"syscall\"\n \t\"unsafe\"\n@@ -81,6 +82,11 @@ func (file *File) readdirnames(n int) (names []string, err error) {\n \t\tsyscall.Entersyscall()\n \t\ti := libc_readdir_r(file.dirinfo.dir, entryDirent, pr)\n \t\tsyscall.Exitsyscall()\n+\t\t// On AIX when readdir_r hits EOF it sets dirent to nil and returns 9.\n+\t\t//  https://www.ibm.com/support/knowledgecenter/ssw_aix_71/com.ibm.aix.basetrf2/readdir_r.htm\n+\t\tif runtime.GOOS == \"aix\" && i == 9 && dirent == nil {\n+\t\t\tbreak\n+\t\t}\n \t\tif i != 0 {\n \t\t\treturn names, NewSyscallError(\"readdir_r\", i)\n \t\t}"}, {"sha": "75df6a4361ca224958162073b14a508c21de47c5", "filename": "libgo/go/os/dir_largefile.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fdir_largefile.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fdir_largefile.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fdir_largefile.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -5,7 +5,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build linux solaris,386 solaris,sparc\n+// +build aix linux solaris,386 solaris,sparc\n \n package os\n "}, {"sha": "02ddd7b9ffa46834212dce6ab1a0f346b1ace16f", "filename": "libgo/go/os/dir_regfile.go", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fdir_regfile.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fdir_regfile.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fdir_regfile.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -5,6 +5,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n+// +build !aix\n // +build !linux\n // +build !solaris !386\n // +build !solaris !sparc"}, {"sha": "2dc6a897a859b74a6c94a48b5bc2209af947224e", "filename": "libgo/go/os/dir_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fdir_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fdir_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fdir_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package os\n "}, {"sha": "2349851b84aa4214bf5d5d4a60d93cda37042a01", "filename": "libgo/go/os/error_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Ferror_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Ferror_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Ferror_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package os\n "}, {"sha": "20ce7a41e030acc0fe51ddb871d3c41bdfe61641", "filename": "libgo/go/os/exec/lp_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fexec%2Flp_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fexec%2Flp_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fexec%2Flp_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package exec\n "}, {"sha": "9e792b4b34f3504903805e6137c2624d11173e79", "filename": "libgo/go/os/exec_posix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fexec_posix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fexec_posix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fexec_posix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n \n package os\n "}, {"sha": "d6433bf0109a4c94473eb56b55901e09fbc55023", "filename": "libgo/go/os/exec_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fexec_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fexec_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fexec_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package os\n "}, {"sha": "117320d7949452b5eb11bf32d418db0f57f4ff99", "filename": "libgo/go/os/executable_path.go", "status": "added", "additions": 104, "deletions": 0, "changes": 104, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fexecutable_path.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fexecutable_path.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fexecutable_path.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,104 @@\n+// Copyright 2017 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// +build aix\n+\n+package os\n+\n+// We query the working directory at init, to use it later to search for the\n+// executable file\n+// errWd will be checked later, if we need to use initWd\n+var initWd, errWd = Getwd()\n+\n+func executable() (string, error) {\n+\tvar err error\n+\tvar exePath string\n+\tif len(Args) == 0 || Args[0] == \"\" {\n+\t\treturn \"\", ErrNotExist\n+\t}\n+\t// Args[0] is an absolute path : this is the executable\n+\tif IsPathSeparator(Args[0][0]) {\n+\t\texePath = Args[0]\n+\t} else {\n+\t\tfor i := 1; i < len(Args[0]); i++ {\n+\t\t\t// Args[0] is a relative path : append current directory\n+\t\t\tif IsPathSeparator(Args[0][i]) {\n+\t\t\t\tif errWd != nil {\n+\t\t\t\t\treturn \"\", errWd\n+\t\t\t\t}\n+\t\t\t\texePath = initWd + string(PathSeparator) + Args[0]\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t}\n+\tif exePath != \"\" {\n+\t\terr = isExecutable(exePath)\n+\t\tif err == nil {\n+\t\t\treturn exePath, nil\n+\t\t}\n+\t\t// File does not exist or is not executable,\n+\t\t// this is an unexpected situation !\n+\t\treturn \"\", err\n+\t}\n+\t// Search for executable in $PATH\n+\tfor _, dir := range splitPathList(Getenv(\"PATH\")) {\n+\t\tif len(dir) == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\tif !IsPathSeparator(dir[0]) {\n+\t\t\tif errWd != nil {\n+\t\t\t\treturn \"\", errWd\n+\t\t\t}\n+\t\t\tdir = initWd + string(PathSeparator) + dir\n+\t\t}\n+\t\texePath = dir + string(PathSeparator) + Args[0]\n+\t\terr = isExecutable(exePath)\n+\t\tif err == nil {\n+\t\t\treturn exePath, nil\n+\t\t}\n+\t\tif err == ErrPermission {\n+\t\t\treturn \"\", err\n+\t\t}\n+\t}\n+\treturn \"\", ErrNotExist\n+}\n+\n+// isExecutable returns an error if a given file is not an executable.\n+func isExecutable(path string) error {\n+\tstat, err := Stat(path)\n+\tif err != nil {\n+\t\treturn err\n+\t}\n+\tmode := stat.Mode()\n+\tif !mode.IsRegular() {\n+\t\treturn ErrPermission\n+\t}\n+\tif (mode & 0111) != 0 {\n+\t\treturn nil\n+\t}\n+\treturn ErrPermission\n+}\n+\n+// splitPathList splits a path list.\n+// This is based on genSplit from strings/strings.go\n+func splitPathList(pathList string) []string {\n+\tn := 1\n+\tfor i := 0; i < len(pathList); i++ {\n+\t\tif pathList[i] == PathListSeparator {\n+\t\t\tn++\n+\t\t}\n+\t}\n+\tstart := 0\n+\ta := make([]string, n)\n+\tna := 0\n+\tfor i := 0; i+1 <= len(pathList) && na+1 < n; i++ {\n+\t\tif pathList[i] == PathListSeparator {\n+\t\t\ta[na] = pathList[start:i]\n+\t\t\tna++\n+\t\t\tstart = i + 1\n+\t\t}\n+\t}\n+\ta[na] = pathList[start:]\n+\treturn a[:na+1]\n+}"}, {"sha": "6634112efce6cafb2d7b41c1247aa949fabcfe0a", "filename": "libgo/go/os/file_posix.go", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Ffile_posix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Ffile_posix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Ffile_posix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n \n package os\n \n@@ -19,6 +19,10 @@ func Readlink(name string) (string, error) {\n \tfor len := 128; ; len *= 2 {\n \t\tb := make([]byte, len)\n \t\tn, e := fixCount(syscall.Readlink(fixLongPath(name), b))\n+\t\t// buffer too small\n+\t\tif e == syscall.ERANGE {\n+\t\t\tcontinue\n+\t\t}\n \t\tif e != nil {\n \t\t\treturn \"\", &PathError{\"readlink\", name, e}\n \t\t}"}, {"sha": "1bba4ed9d63d9a2edfc3e3723fef1e86488db99a", "filename": "libgo/go/os/file_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Ffile_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Ffile_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Ffile_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package os\n "}, {"sha": "bc0f23911972868e5551d30a1da7e5b35ee6c1c3", "filename": "libgo/go/os/path_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fpath_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fpath_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fpath_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package os\n "}, {"sha": "ebe198bb6a8f00487daaab495c9c09770ebbbcc6", "filename": "libgo/go/os/pipe_bsd.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fpipe_bsd.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fpipe_bsd.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fpipe_bsd.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd nacl netbsd openbsd solaris\n \n package os\n "}, {"sha": "c8409e736974b82fada66e2df66228bf425b8a89", "filename": "libgo/go/os/signal/signal_test.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fsignal%2Fsignal_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fsignal%2Fsignal_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fsignal%2Fsignal_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris\n \n package signal\n "}, {"sha": "5ec7e97f3bd90c0ad7020a0cb07b8a7d0031c97f", "filename": "libgo/go/os/signal/signal_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fsignal%2Fsignal_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fsignal%2Fsignal_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fsignal%2Fsignal_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n \n package signal\n "}, {"sha": "564215bcb73b76c9f13733737c123c9a723d61a2", "filename": "libgo/go/os/stat.go", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fstat.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fstat.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fstat.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,6 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n+// +build !aix\n // +build !darwin\n // +build !freebsd\n // +build !linux"}, {"sha": "82481c04a7bfa822f60d7b71147d468bcfd3257e", "filename": "libgo/go/os/stat_atim.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fstat_atim.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fstat_atim.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fstat_atim.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build linux openbsd solaristag\n+// +build aix linux openbsd solaristag\n \n package os\n "}, {"sha": "043aefe8fad31bc8835f96551fe536c497c962a3", "filename": "libgo/go/os/stat_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fstat_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fstat_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fstat_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package os\n "}, {"sha": "89fbff8e077f765dabecb443d867a56b3a775ec1", "filename": "libgo/go/os/sys_uname.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fsys_uname.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fsys_uname.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fsys_uname.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -4,7 +4,7 @@\n \n // For systems which only store the hostname in uname (Solaris).\n \n-// +build solaris irix rtems\n+// +build aix solaris irix rtems\n \n package os\n "}, {"sha": "4caf8bdb7d0e3d7106fd6649c88f950a791eca22", "filename": "libgo/go/os/sys_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fsys_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fsys_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fsys_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build dragonfly linux netbsd openbsd solaris\n+// +build aix dragonfly linux netbsd openbsd solaris\n \n package os\n "}, {"sha": "64455b5c01b78f24fd8d7bf90a3b37f465fd9e0a", "filename": "libgo/go/os/user/decls_aix.go", "status": "added", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fuser%2Fdecls_aix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fuser%2Fdecls_aix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fuser%2Fdecls_aix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,24 @@\n+// Copyright 2017 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package user\n+\n+import \"syscall\"\n+\n+// Declarations for the libc functions on AIX.\n+\n+//extern _posix_getpwnam_r\n+func libc_getpwnam_r(name *byte, pwd *syscall.Passwd, buf *byte, buflen syscall.Size_t, result **syscall.Passwd) int\n+\n+//extern _posix_getpwuid_r\n+func libc_getpwuid_r(uid syscall.Uid_t, pwd *syscall.Passwd, buf *byte, buflen syscall.Size_t, result **syscall.Passwd) int\n+\n+//extern _posix_getgrnam_r\n+func libc_getgrnam_r(name *byte, grp *syscall.Group, buf *byte, buflen syscall.Size_t, result **syscall.Group) int\n+\n+//extern _posix_getgrgid_r\n+func libc_getgrgid_r(gid syscall.Gid_t, grp *syscall.Group, buf *byte, buflen syscall.Size_t, result **syscall.Group) int\n+\n+//extern getgrset\n+func libc_getgrset(user *byte) *byte"}, {"sha": "5b9f3f9a1d01a00b10fdcae485f24f569c3cf41f", "filename": "libgo/go/os/user/listgroups_aix.go", "status": "added", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fuser%2Flistgroups_aix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fuser%2Flistgroups_aix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fuser%2Flistgroups_aix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,11 @@\n+// Copyright 2017 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package user\n+\n+import \"fmt\"\n+\n+func listGroups(u *User) ([]string, error) {\n+\treturn nil, fmt.Errorf(\"user: list groups for %s: not supported on AIX\", u.Username)\n+}"}, {"sha": "9670ada49425d15d14907eeb1c114097becde27f", "filename": "libgo/go/os/user/lookup_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fuser%2Flookup_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fuser%2Flookup_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fuser%2Flookup_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd !android,linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd !android,linux netbsd openbsd solaris\n // +build cgo\n \n package user"}, {"sha": "0378b830b76f588b5219654727ede01d6c97c205", "filename": "libgo/go/os/wait_unimp.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fwait_unimp.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fos%2Fwait_unimp.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fos%2Fwait_unimp.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build dragonfly nacl netbsd openbsd solaris\n+// +build aix dragonfly nacl netbsd openbsd solaris\n \n package os\n "}, {"sha": "2d407a8bf0af34a133f01699d183bf40a7a74eb8", "filename": "libgo/go/path/filepath/path_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fpath%2Ffilepath%2Fpath_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fpath%2Ffilepath%2Fpath_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fpath%2Ffilepath%2Fpath_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris\n \n package filepath\n "}, {"sha": "3ae0f182b3add839925ad2777a563a84eda364a8", "filename": "libgo/go/reflect/type.go", "status": "modified", "additions": 275, "deletions": 264, "changes": 539, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Freflect%2Ftype.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Freflect%2Ftype.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Freflect%2Ftype.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -259,20 +259,21 @@ const (\n // with a unique tag like `reflect:\"array\"` or `reflect:\"ptr\"`\n // so that code cannot convert from, say, *arrayType to *ptrType.\n type rtype struct {\n-\tkind       uint8 // enumeration for C\n-\talign      int8  // alignment of variable with this type\n-\tfieldAlign uint8 // alignment of struct field with this type\n-\t_          uint8 // unused/padding\n \tsize       uintptr\n-\thash       uint32 // hash of type; avoids computation in hash tables\n+\tptrdata    uintptr // size of memory prefix holding all pointers\n+\thash       uint32  // hash of type; avoids computation in hash tables\n+\tkind       uint8   // enumeration for C\n+\talign      int8    // alignment of variable with this type\n+\tfieldAlign uint8   // alignment of struct field with this type\n+\t_          uint8   // unused/padding\n \n \thashfn  func(unsafe.Pointer, uintptr) uintptr     // hash function\n \tequalfn func(unsafe.Pointer, unsafe.Pointer) bool // equality function\n \n-\tgc            unsafe.Pointer // garbage collection data\n-\tstring        *string        // string form; unnecessary  but undeniably useful\n-\t*uncommonType                // (relatively) uncommon fields\n-\tptrToThis     *rtype         // type for pointer to this type, if used in binary or has methods\n+\tgcdata        *byte   // garbage collection data\n+\tstring        *string // string form; unnecessary  but undeniably useful\n+\t*uncommonType         // (relatively) uncommon fields\n+\tptrToThis     *rtype  // type for pointer to this type, if used in binary or has methods\n }\n \n // Method on non-interface type\n@@ -382,24 +383,6 @@ type structType struct {\n \tfields []structField // sorted by offset\n }\n \n-// NOTE: These are copied from ../runtime/mgc0.h.\n-// They must be kept in sync.\n-const (\n-\t_GC_END = iota\n-\t_GC_PTR\n-\t_GC_APTR\n-\t_GC_ARRAY_START\n-\t_GC_ARRAY_NEXT\n-\t_GC_CALL\n-\t_GC_CHAN_PTR\n-\t_GC_STRING\n-\t_GC_EFACE\n-\t_GC_IFACE\n-\t_GC_SLICE\n-\t_GC_REGION\n-\t_GC_NUM_INSTR\n-)\n-\n /*\n  * The compiler knows the exact layout of all the data structures above.\n  * The compiler does not know about the data structures and methods below.\n@@ -1098,32 +1081,6 @@ var ptrMap struct {\n \tm map[*rtype]*ptrType\n }\n \n-// garbage collection bytecode program for pointer to memory without pointers.\n-// See ../../cmd/gc/reflect.c:/^dgcsym1 and :/^dgcsym.\n-type ptrDataGC struct {\n-\twidth uintptr // sizeof(ptr)\n-\top    uintptr // _GC_APTR\n-\toff   uintptr // 0\n-\tend   uintptr // _GC_END\n-}\n-\n-var ptrDataGCProg = ptrDataGC{\n-\twidth: unsafe.Sizeof((*byte)(nil)),\n-\top:    _GC_APTR,\n-\toff:   0,\n-\tend:   _GC_END,\n-}\n-\n-// garbage collection bytecode program for pointer to memory with pointers.\n-// See ../../cmd/gc/reflect.c:/^dgcsym1 and :/^dgcsym.\n-type ptrGC struct {\n-\twidth  uintptr        // sizeof(ptr)\n-\top     uintptr        // _GC_PTR\n-\toff    uintptr        // 0\n-\telemgc unsafe.Pointer // element gc type\n-\tend    uintptr        // _GC_END\n-}\n-\n // PtrTo returns the pointer type with element t.\n // For example, if t represents type Foo, PtrTo(t) represents *Foo.\n func PtrTo(t Type) Type {\n@@ -1189,18 +1146,6 @@ func (t *rtype) ptrTo() *rtype {\n \tpp.ptrToThis = nil\n \tpp.elem = t\n \n-\tif t.kind&kindNoPointers != 0 {\n-\t\tpp.gc = unsafe.Pointer(&ptrDataGCProg)\n-\t} else {\n-\t\tpp.gc = unsafe.Pointer(&ptrGC{\n-\t\t\twidth:  pp.size,\n-\t\t\top:     _GC_PTR,\n-\t\t\toff:    0,\n-\t\t\telemgc: t.gc,\n-\t\t\tend:    _GC_END,\n-\t\t})\n-\t}\n-\n \tq := canonicalize(&pp.rtype)\n \tp = (*ptrType)(unsafe.Pointer(q.(*rtype)))\n \n@@ -1507,16 +1452,6 @@ func cachePut(k cacheKey, t *rtype) Type {\n \treturn t\n }\n \n-// garbage collection bytecode program for chan.\n-// See ../../cmd/gc/reflect.c:/^dgcsym1 and :/^dgcsym.\n-type chanGC struct {\n-\twidth uintptr // sizeof(map)\n-\top    uintptr // _GC_CHAN_PTR\n-\toff   uintptr // 0\n-\ttyp   *rtype  // map type\n-\tend   uintptr // _GC_END\n-}\n-\n // The funcLookupCache caches FuncOf lookups.\n // FuncOf does not share the common lookupCache since cacheKey is not\n // sufficient to represent functions unambiguously.\n@@ -1584,17 +1519,6 @@ func ChanOf(dir ChanDir, t Type) Type {\n \tch.uncommonType = nil\n \tch.ptrToThis = nil\n \n-\tch.gc = unsafe.Pointer(&chanGC{\n-\t\twidth: ch.size,\n-\t\top:    _GC_CHAN_PTR,\n-\t\toff:   0,\n-\t\ttyp:   &ch.rtype,\n-\t\tend:   _GC_END,\n-\t})\n-\n-\t// INCORRECT. Uncomment to check that TestChanOfGC fails when ch.gc is wrong.\n-\t// ch.gc = unsafe.Pointer(&badGC{width: ch.size, end: _GC_END})\n-\n \treturn cachePut(ckey, &ch.rtype)\n }\n \n@@ -1733,9 +1657,6 @@ func FuncOf(in, out []Type, variadic bool) Type {\n \tft.uncommonType = nil\n \tft.ptrToThis = nil\n \n-\t// TODO(cmang): Generate GC data for funcs.\n-\tft.gc = unsafe.Pointer(&ptrDataGCProg)\n-\n \tfuncLookupCache.m[hash] = append(funcLookupCache.m[hash], &ft.rtype)\n \n \treturn toType(&ft.rtype)\n@@ -1859,8 +1780,8 @@ func bucketOf(ktyp, etyp *rtype) *rtype {\n \t// and it's easier to generate a pointer bitmap than a GC program.\n \t// Note that since the key and value are known to be <= 128 bytes,\n \t// they're guaranteed to have bitmaps instead of GC programs.\n-\t// var gcdata *byte\n-\t// var ptrdata uintptr\n+\tvar gcdata *byte\n+\tvar ptrdata uintptr\n \n \tsize := bucketSize\n \tsize = align(size, uintptr(ktyp.fieldAlign))\n@@ -1875,140 +1796,78 @@ func bucketOf(ktyp, etyp *rtype) *rtype {\n \tif maxAlign > ptrSize {\n \t\tsize = align(size, maxAlign)\n \t\tsize += align(ptrSize, maxAlign) - ptrSize\n+\t} else if maxAlign < ptrSize {\n+\t\tsize = align(size, ptrSize)\n+\t\tmaxAlign = ptrSize\n \t}\n \n \tovoff := size\n \tsize += ptrSize\n-\tif maxAlign < ptrSize {\n-\t\tmaxAlign = ptrSize\n-\t}\n \n-\tvar gcPtr unsafe.Pointer\n \tif kind != kindNoPointers {\n-\t\tgc := []uintptr{size}\n-\t\tbase := bucketSize\n-\t\tbase = align(base, uintptr(ktyp.fieldAlign))\n+\t\tnptr := size / ptrSize\n+\t\tmask := make([]byte, (nptr+7)/8)\n+\t\tpsize := bucketSize\n+\t\tpsize = align(psize, uintptr(ktyp.fieldAlign))\n+\t\tbase := psize / ptrSize\n+\n \t\tif ktyp.kind&kindNoPointers == 0 {\n-\t\t\tgc = append(gc, _GC_ARRAY_START, base, bucketSize, ktyp.size)\n-\t\t\tgc = appendGCProgram(gc, ktyp, 0)\n-\t\t\tgc = append(gc, _GC_ARRAY_NEXT)\n+\t\t\tif ktyp.kind&kindGCProg != 0 {\n+\t\t\t\tpanic(\"reflect: unexpected GC program in MapOf\")\n+\t\t\t}\n+\t\t\tkmask := (*[16]byte)(unsafe.Pointer(ktyp.gcdata))\n+\t\t\tfor i := uintptr(0); i < ktyp.size/ptrSize; i++ {\n+\t\t\t\tif (kmask[i/8]>>(i%8))&1 != 0 {\n+\t\t\t\t\tfor j := uintptr(0); j < bucketSize; j++ {\n+\t\t\t\t\t\tword := base + j*ktyp.size/ptrSize + i\n+\t\t\t\t\t\tmask[word/8] |= 1 << (word % 8)\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n \t\t}\n-\t\tbase += ktyp.size * bucketSize\n-\t\tbase = align(base, uintptr(etyp.fieldAlign))\n+\t\tpsize += bucketSize * ktyp.size\n+\t\tpsize = align(psize, uintptr(etyp.fieldAlign))\n+\t\tbase = psize / ptrSize\n+\n \t\tif etyp.kind&kindNoPointers == 0 {\n-\t\t\tgc = append(gc, _GC_ARRAY_START, base, bucketSize, etyp.size)\n-\t\t\tgc = appendGCProgram(gc, etyp, 0)\n-\t\t\tgc = append(gc, _GC_ARRAY_NEXT)\n+\t\t\tif etyp.kind&kindGCProg != 0 {\n+\t\t\t\tpanic(\"reflect: unexpected GC program in MapOf\")\n+\t\t\t}\n+\t\t\temask := (*[16]byte)(unsafe.Pointer(etyp.gcdata))\n+\t\t\tfor i := uintptr(0); i < etyp.size/ptrSize; i++ {\n+\t\t\t\tif (emask[i/8]>>(i%8))&1 != 0 {\n+\t\t\t\t\tfor j := uintptr(0); j < bucketSize; j++ {\n+\t\t\t\t\t\tword := base + j*etyp.size/ptrSize + i\n+\t\t\t\t\t\tmask[word/8] |= 1 << (word % 8)\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\tword := ovoff / ptrSize\n+\t\tmask[word/8] |= 1 << (word % 8)\n+\t\tgcdata = &mask[0]\n+\t\tptrdata = (word + 1) * ptrSize\n+\n+\t\t// overflow word must be last\n+\t\tif ptrdata != size {\n+\t\t\tpanic(\"reflect: bad layout computation in MapOf\")\n \t\t}\n-\t\tgc = append(gc, _GC_APTR, ovoff, _GC_END)\n-\t\tgcPtr = unsafe.Pointer(&gc[0])\n-\t} else {\n-\t\t// No pointers in bucket.\n-\t\tgc := [...]uintptr{size, _GC_END}\n-\t\tgcPtr = unsafe.Pointer(&gc[0])\n \t}\n \n \tb := &rtype{\n \t\talign:      int8(maxAlign),\n \t\tfieldAlign: uint8(maxAlign),\n \t\tsize:       size,\n \t\tkind:       kind,\n-\t\tgc:         gcPtr,\n+\t\tptrdata:    ptrdata,\n+\t\tgcdata:     gcdata,\n \t}\n \ts := \"bucket(\" + *ktyp.string + \",\" + *etyp.string + \")\"\n \tb.string = &s\n \treturn b\n }\n \n-// Take the GC program for \"t\" and append it to the GC program \"gc\".\n-func appendGCProgram(gc []uintptr, t *rtype, offset uintptr) []uintptr {\n-\tp := t.gc\n-\tp = unsafe.Pointer(uintptr(p) + unsafe.Sizeof(uintptr(0))) // skip size\n-loop:\n-\tfor {\n-\t\tvar argcnt int\n-\t\tswitch *(*uintptr)(p) {\n-\t\tcase _GC_END:\n-\t\t\t// Note: _GC_END not included in append\n-\t\t\tbreak loop\n-\t\tcase _GC_ARRAY_NEXT:\n-\t\t\targcnt = 0\n-\t\tcase _GC_APTR, _GC_STRING, _GC_EFACE, _GC_IFACE:\n-\t\t\targcnt = 1\n-\t\tcase _GC_PTR, _GC_CALL, _GC_CHAN_PTR, _GC_SLICE:\n-\t\t\targcnt = 2\n-\t\tcase _GC_ARRAY_START, _GC_REGION:\n-\t\t\targcnt = 3\n-\t\tdefault:\n-\t\t\tpanic(\"unknown GC program op for \" + *t.string + \": \" + strconv.FormatUint(*(*uint64)(p), 10))\n-\t\t}\n-\t\tfor i := 0; i < argcnt+1; i++ {\n-\t\t\tv := *(*uintptr)(p)\n-\t\t\tif i == 1 {\n-\t\t\t\tv += offset\n-\t\t\t}\n-\t\t\tgc = append(gc, v)\n-\t\t\tp = unsafe.Pointer(uintptr(p) + unsafe.Sizeof(uintptr(0)))\n-\t\t}\n-\t}\n-\treturn gc\n-}\n-func hMapOf(bucket *rtype) *rtype {\n-\tptrsize := unsafe.Sizeof(uintptr(0))\n-\n-\t// make gc program & compute hmap size\n-\tgc := make([]uintptr, 1)           // first entry is size, filled in at the end\n-\toffset := unsafe.Sizeof(uint(0))   // count\n-\toffset += unsafe.Sizeof(uint32(0)) // flags\n-\toffset += unsafe.Sizeof(uint32(0)) // hash0\n-\toffset += unsafe.Sizeof(uint8(0))  // B\n-\toffset += unsafe.Sizeof(uint8(0))  // keysize\n-\toffset += unsafe.Sizeof(uint8(0))  // valuesize\n-\toffset = (offset + 1) / 2 * 2\n-\toffset += unsafe.Sizeof(uint16(0)) // bucketsize\n-\toffset = (offset + ptrsize - 1) / ptrsize * ptrsize\n-\t// gc = append(gc, _GC_PTR, offset, uintptr(bucket.gc)) // buckets\n-\toffset += ptrsize\n-\t// gc = append(gc, _GC_PTR, offset, uintptr(bucket.gc)) // oldbuckets\n-\toffset += ptrsize\n-\toffset += ptrsize // nevacuate\n-\tgc = append(gc, _GC_END)\n-\tgc[0] = offset\n-\n-\th := new(rtype)\n-\th.size = offset\n-\t// h.gc = unsafe.Pointer(&gc[0])\n-\ts := \"hmap(\" + *bucket.string + \")\"\n-\th.string = &s\n-\treturn h\n-}\n-\n-// garbage collection bytecode program for slice of non-zero-length values.\n-// See ../../cmd/gc/reflect.c:/^dgcsym1 and :/^dgcsym.\n-type sliceGC struct {\n-\twidth  uintptr        // sizeof(slice)\n-\top     uintptr        // _GC_SLICE\n-\toff    uintptr        // 0\n-\telemgc unsafe.Pointer // element gc program\n-\tend    uintptr        // _GC_END\n-}\n-\n-// garbage collection bytecode program for slice of zero-length values.\n-// See ../../cmd/gc/reflect.c:/^dgcsym1 and :/^dgcsym.\n-type sliceEmptyGC struct {\n-\twidth uintptr // sizeof(slice)\n-\top    uintptr // _GC_APTR\n-\toff   uintptr // 0\n-\tend   uintptr // _GC_END\n-}\n-\n-var sliceEmptyGCProg = sliceEmptyGC{\n-\twidth: unsafe.Sizeof([]byte(nil)),\n-\top:    _GC_APTR,\n-\toff:   0,\n-\tend:   _GC_END,\n-}\n-\n // SliceOf returns the slice type with element type t.\n // For example, if t represents int, SliceOf(t) represents []int.\n func SliceOf(t Type) Type {\n@@ -2037,21 +1896,6 @@ func SliceOf(t Type) Type {\n \tslice.uncommonType = nil\n \tslice.ptrToThis = nil\n \n-\tif typ.size == 0 {\n-\t\tslice.gc = unsafe.Pointer(&sliceEmptyGCProg)\n-\t} else {\n-\t\tslice.gc = unsafe.Pointer(&sliceGC{\n-\t\t\twidth:  slice.size,\n-\t\t\top:     _GC_SLICE,\n-\t\t\toff:    0,\n-\t\t\telemgc: typ.gc,\n-\t\t\tend:    _GC_END,\n-\t\t})\n-\t}\n-\n-\t// INCORRECT. Uncomment to check that TestSliceOfOfGC fails when slice.gc is wrong.\n-\t// slice.gc = unsafe.Pointer(&badGC{width: slice.size, end: _GC_END})\n-\n \treturn cachePut(ckey, &slice.rtype)\n }\n \n@@ -2073,15 +1917,18 @@ var structLookupCache struct {\n // This limitation may be lifted in a future version.\n func StructOf(fields []StructField) Type {\n \tvar (\n-\t\thash     = uint32(0)\n-\t\tsize     uintptr\n-\t\ttypalign int8\n+\t\thash       = uint32(0)\n+\t\tsize       uintptr\n+\t\ttypalign   int8\n+\t\tcomparable = true\n+\t\thashable   = true\n \n \t\tfs   = make([]structField, len(fields))\n \t\trepr = make([]byte, 0, 64)\n \t\tfset = map[string]struct{}{} // fields' names\n \n-\t\thasPtr = false // records whether at least one struct-field is a pointer\n+\t\thasPtr    = false // records whether at least one struct-field is a pointer\n+\t\thasGCProg = false // records whether a struct-field type has a GCProg\n \t)\n \n \tlastzero := uintptr(0)\n@@ -2092,6 +1939,9 @@ func StructOf(fields []StructField) Type {\n \t\t}\n \t\tf := runtimeStructField(field)\n \t\tft := f.typ\n+\t\tif ft.kind&kindGCProg != 0 {\n+\t\t\thasGCProg = true\n+\t\t}\n \t\tif ft.pointers() {\n \t\t\thasPtr = true\n \t\t}\n@@ -2156,6 +2006,9 @@ func StructOf(fields []StructField) Type {\n \t\t\trepr = append(repr, ';')\n \t\t}\n \n+\t\tcomparable = comparable && (ft.equalfn != nil)\n+\t\thashable = hashable && (ft.hashfn != nil)\n+\n \t\tf.offset = align(size, uintptr(ft.fieldAlign))\n \t\tif int8(ft.fieldAlign) > typalign {\n \t\t\ttypalign = int8(ft.fieldAlign)\n@@ -2228,36 +2081,95 @@ func StructOf(fields []StructField) Type {\n \ttyp.fieldAlign = uint8(typalign)\n \tif !hasPtr {\n \t\ttyp.kind |= kindNoPointers\n-\t\tgc := [...]uintptr{size, _GC_END}\n-\t\ttyp.gc = unsafe.Pointer(&gc[0])\n \t} else {\n \t\ttyp.kind &^= kindNoPointers\n-\t\tgc := []uintptr{size}\n-\t\tfor _, ft := range fs {\n-\t\t\tgc = appendGCProgram(gc, ft.typ, ft.offset)\n+\t}\n+\n+\tif hasGCProg {\n+\t\tlastPtrField := 0\n+\t\tfor i, ft := range fs {\n+\t\t\tif ft.typ.pointers() {\n+\t\t\t\tlastPtrField = i\n+\t\t\t}\n+\t\t}\n+\t\tprog := []byte{0, 0, 0, 0} // will be length of prog\n+\t\tfor i, ft := range fs {\n+\t\t\tif i > lastPtrField {\n+\t\t\t\t// gcprog should not include anything for any field after\n+\t\t\t\t// the last field that contains pointer data\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\t// FIXME(sbinet) handle padding, fields smaller than a word\n+\t\t\telemGC := (*[1 << 30]byte)(unsafe.Pointer(ft.typ.gcdata))[:]\n+\t\t\telemPtrs := ft.typ.ptrdata / ptrSize\n+\t\t\tswitch {\n+\t\t\tcase ft.typ.kind&kindGCProg == 0 && ft.typ.ptrdata != 0:\n+\t\t\t\t// Element is small with pointer mask; use as literal bits.\n+\t\t\t\tmask := elemGC\n+\t\t\t\t// Emit 120-bit chunks of full bytes (max is 127 but we avoid using partial bytes).\n+\t\t\t\tvar n uintptr\n+\t\t\t\tfor n := elemPtrs; n > 120; n -= 120 {\n+\t\t\t\t\tprog = append(prog, 120)\n+\t\t\t\t\tprog = append(prog, mask[:15]...)\n+\t\t\t\t\tmask = mask[15:]\n+\t\t\t\t}\n+\t\t\t\tprog = append(prog, byte(n))\n+\t\t\t\tprog = append(prog, mask[:(n+7)/8]...)\n+\t\t\tcase ft.typ.kind&kindGCProg != 0:\n+\t\t\t\t// Element has GC program; emit one element.\n+\t\t\t\telemProg := elemGC[4 : 4+*(*uint32)(unsafe.Pointer(&elemGC[0]))-1]\n+\t\t\t\tprog = append(prog, elemProg...)\n+\t\t\t}\n+\t\t\t// Pad from ptrdata to size.\n+\t\t\telemWords := ft.typ.size / ptrSize\n+\t\t\tif elemPtrs < elemWords {\n+\t\t\t\t// Emit literal 0 bit, then repeat as needed.\n+\t\t\t\tprog = append(prog, 0x01, 0x00)\n+\t\t\t\tif elemPtrs+1 < elemWords {\n+\t\t\t\t\tprog = append(prog, 0x81)\n+\t\t\t\t\tprog = appendVarint(prog, elemWords-elemPtrs-1)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\t*(*uint32)(unsafe.Pointer(&prog[0])) = uint32(len(prog) - 4)\n+\t\ttyp.kind |= kindGCProg\n+\t\ttyp.gcdata = &prog[0]\n+\t} else {\n+\t\ttyp.kind &^= kindGCProg\n+\t\tbv := new(bitVector)\n+\t\taddTypeBits(bv, 0, typ.common())\n+\t\tif len(bv.data) > 0 {\n+\t\t\ttyp.gcdata = &bv.data[0]\n \t\t}\n-\t\tgc = append(gc, _GC_END)\n-\t\ttyp.gc = unsafe.Pointer(&gc[0])\n \t}\n+\ttyp.ptrdata = typeptrdata(typ.common())\n \n-\ttyp.hashfn = func(p unsafe.Pointer, seed uintptr) uintptr {\n-\t\tret := seed\n-\t\tfor _, ft := range typ.fields {\n-\t\t\to := unsafe.Pointer(uintptr(p) + ft.offset)\n-\t\t\tret = ft.typ.hashfn(o, ret)\n+\tif hashable {\n+\t\ttyp.hashfn = func(p unsafe.Pointer, seed uintptr) uintptr {\n+\t\t\to := seed\n+\t\t\tfor _, ft := range typ.fields {\n+\t\t\t\tpi := unsafe.Pointer(uintptr(p) + ft.offset)\n+\t\t\t\to = ft.typ.hashfn(pi, o)\n+\t\t\t}\n+\t\t\treturn o\n \t\t}\n-\t\treturn ret\n+\t} else {\n+\t\ttyp.hashfn = nil\n \t}\n \n-\ttyp.equalfn = func(p, q unsafe.Pointer) bool {\n-\t\tfor _, ft := range typ.fields {\n-\t\t\tpi := unsafe.Pointer(uintptr(p) + ft.offset)\n-\t\t\tqi := unsafe.Pointer(uintptr(q) + ft.offset)\n-\t\t\tif !ft.typ.equalfn(pi, qi) {\n-\t\t\t\treturn false\n+\tif comparable {\n+\t\ttyp.equalfn = func(p, q unsafe.Pointer) bool {\n+\t\t\tfor _, ft := range typ.fields {\n+\t\t\t\tpi := unsafe.Pointer(uintptr(p) + ft.offset)\n+\t\t\t\tqi := unsafe.Pointer(uintptr(q) + ft.offset)\n+\t\t\t\tif !ft.typ.equalfn(pi, qi) {\n+\t\t\t\t\treturn false\n+\t\t\t\t}\n \t\t\t}\n+\t\t\treturn true\n \t\t}\n-\t\treturn true\n+\t} else {\n+\t\ttyp.equalfn = nil\n \t}\n \n \ttyp.kind &^= kindDirectIface\n@@ -2308,6 +2220,35 @@ func runtimeStructField(field StructField) structField {\n \t}\n }\n \n+// typeptrdata returns the length in bytes of the prefix of t\n+// containing pointer data. Anything after this offset is scalar data.\n+// keep in sync with ../cmd/compile/internal/gc/reflect.go\n+func typeptrdata(t *rtype) uintptr {\n+\tif !t.pointers() {\n+\t\treturn 0\n+\t}\n+\tswitch t.Kind() {\n+\tcase Struct:\n+\t\tst := (*structType)(unsafe.Pointer(t))\n+\t\t// find the last field that has pointers.\n+\t\tfield := 0\n+\t\tfor i := range st.fields {\n+\t\t\tft := st.fields[i].typ\n+\t\t\tif ft.pointers() {\n+\t\t\t\tfield = i\n+\t\t\t}\n+\t\t}\n+\t\tf := st.fields[field]\n+\t\treturn f.offset + f.typ.ptrdata\n+\n+\tdefault:\n+\t\tpanic(\"reflect.typeptrdata: unexpected type, \" + t.String())\n+\t}\n+}\n+\n+// See cmd/compile/internal/gc/reflect.go for derivation of constant.\n+const maxPtrmaskBytes = 2048\n+\n // ArrayOf returns the array type with the given count and element type.\n // For example, if t represents int, ArrayOf(5, t) represents [5]int.\n //\n@@ -2350,9 +2291,9 @@ func ArrayOf(count int, elem Type) Type {\n \t\tpanic(\"reflect.ArrayOf: array size would exceed virtual address space\")\n \t}\n \tarray.size = typ.size * uintptr(count)\n-\t// if count > 0 && typ.ptrdata != 0 {\n-\t// \tarray.ptrdata = typ.size*uintptr(count-1) + typ.ptrdata\n-\t// }\n+\tif count > 0 && typ.ptrdata != 0 {\n+\t\tarray.ptrdata = typ.size*uintptr(count-1) + typ.ptrdata\n+\t}\n \tarray.align = typ.align\n \tarray.fieldAlign = typ.fieldAlign\n \tarray.uncommonType = nil\n@@ -2364,41 +2305,111 @@ func ArrayOf(count int, elem Type) Type {\n \tcase typ.kind&kindNoPointers != 0 || array.size == 0:\n \t\t// No pointers.\n \t\tarray.kind |= kindNoPointers\n-\t\tgc := [...]uintptr{array.size, _GC_END}\n-\t\tarray.gc = unsafe.Pointer(&gc[0])\n+\t\tarray.gcdata = nil\n+\t\tarray.ptrdata = 0\n \n \tcase count == 1:\n \t\t// In memory, 1-element array looks just like the element.\n \t\tarray.kind |= typ.kind & kindGCProg\n-\t\tarray.gc = typ.gc\n+\t\tarray.gcdata = typ.gcdata\n+\t\tarray.ptrdata = typ.ptrdata\n+\n+\tcase typ.kind&kindGCProg == 0 && array.size <= maxPtrmaskBytes*8*ptrSize:\n+\t\t// Element is small with pointer mask; array is still small.\n+\t\t// Create direct pointer mask by turning each 1 bit in elem\n+\t\t// into count 1 bits in larger mask.\n+\t\tmask := make([]byte, (array.ptrdata/ptrSize+7)/8)\n+\t\telemMask := (*[1 << 30]byte)(unsafe.Pointer(typ.gcdata))[:]\n+\t\telemWords := typ.size / ptrSize\n+\t\tfor j := uintptr(0); j < typ.ptrdata/ptrSize; j++ {\n+\t\t\tif (elemMask[j/8]>>(j%8))&1 != 0 {\n+\t\t\t\tfor i := uintptr(0); i < array.len; i++ {\n+\t\t\t\t\tk := i*elemWords + j\n+\t\t\t\t\tmask[k/8] |= 1 << (k % 8)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tarray.gcdata = &mask[0]\n \n \tdefault:\n-\t\tgc := []uintptr{array.size, _GC_ARRAY_START, 0, uintptr(count), typ.size}\n-\t\tgc = appendGCProgram(gc, typ, 0)\n-\t\tgc = append(gc, _GC_ARRAY_NEXT, _GC_END)\n-\t\tarray.gc = unsafe.Pointer(&gc[0])\n+\t\t// Create program that emits one element\n+\t\t// and then repeats to make the array.\n+\t\tprog := []byte{0, 0, 0, 0} // will be length of prog\n+\t\telemGC := (*[1 << 30]byte)(unsafe.Pointer(typ.gcdata))[:]\n+\t\telemPtrs := typ.ptrdata / ptrSize\n+\t\tif typ.kind&kindGCProg == 0 {\n+\t\t\t// Element is small with pointer mask; use as literal bits.\n+\t\t\tmask := elemGC\n+\t\t\t// Emit 120-bit chunks of full bytes (max is 127 but we avoid using partial bytes).\n+\t\t\tvar n uintptr\n+\t\t\tfor n = elemPtrs; n > 120; n -= 120 {\n+\t\t\t\tprog = append(prog, 120)\n+\t\t\t\tprog = append(prog, mask[:15]...)\n+\t\t\t\tmask = mask[15:]\n+\t\t\t}\n+\t\t\tprog = append(prog, byte(n))\n+\t\t\tprog = append(prog, mask[:(n+7)/8]...)\n+\t\t} else {\n+\t\t\t// Element has GC program; emit one element.\n+\t\t\telemProg := elemGC[4 : 4+*(*uint32)(unsafe.Pointer(&elemGC[0]))-1]\n+\t\t\tprog = append(prog, elemProg...)\n+\t\t}\n+\t\t// Pad from ptrdata to size.\n+\t\telemWords := typ.size / ptrSize\n+\t\tif elemPtrs < elemWords {\n+\t\t\t// Emit literal 0 bit, then repeat as needed.\n+\t\t\tprog = append(prog, 0x01, 0x00)\n+\t\t\tif elemPtrs+1 < elemWords {\n+\t\t\t\tprog = append(prog, 0x81)\n+\t\t\t\tprog = appendVarint(prog, elemWords-elemPtrs-1)\n+\t\t\t}\n+\t\t}\n+\t\t// Repeat count-1 times.\n+\t\tif elemWords < 0x80 {\n+\t\t\tprog = append(prog, byte(elemWords|0x80))\n+\t\t} else {\n+\t\t\tprog = append(prog, 0x80)\n+\t\t\tprog = appendVarint(prog, elemWords)\n+\t\t}\n+\t\tprog = appendVarint(prog, uintptr(count)-1)\n+\t\tprog = append(prog, 0)\n+\t\t*(*uint32)(unsafe.Pointer(&prog[0])) = uint32(len(prog) - 4)\n+\t\tarray.kind |= kindGCProg\n+\t\tarray.gcdata = &prog[0]\n+\t\tarray.ptrdata = array.size // overestimate but ok; must match program\n \t}\n \n \tarray.kind &^= kindDirectIface\n \n-\tarray.hashfn = func(p unsafe.Pointer, seed uintptr) uintptr {\n-\t\tret := seed\n-\t\tfor i := 0; i < count; i++ {\n-\t\t\tret = typ.hashfn(p, ret)\n-\t\t\tp = unsafe.Pointer(uintptr(p) + typ.size)\n+\tesize := typ.size\n+\n+\tif typ.equalfn == nil {\n+\t\tarray.equalfn = nil\n+\t} else {\n+\t\teequal := typ.equalfn\n+\t\tarray.equalfn = func(p, q unsafe.Pointer) bool {\n+\t\t\tfor i := 0; i < count; i++ {\n+\t\t\t\tpi := arrayAt(p, i, esize)\n+\t\t\t\tqi := arrayAt(q, i, esize)\n+\t\t\t\tif !eequal(pi, qi) {\n+\t\t\t\t\treturn false\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn true\n \t\t}\n-\t\treturn ret\n \t}\n \n-\tarray.equalfn = func(p1, p2 unsafe.Pointer) bool {\n-\t\tfor i := 0; i < count; i++ {\n-\t\t\tif !typ.equalfn(p1, p2) {\n-\t\t\t\treturn false\n+\tif typ.hashfn == nil {\n+\t\tarray.hashfn = nil\n+\t} else {\n+\t\tehash := typ.hashfn\n+\t\tarray.hashfn = func(ptr unsafe.Pointer, seed uintptr) uintptr {\n+\t\t\to := seed\n+\t\t\tfor i := 0; i < count; i++ {\n+\t\t\t\to = ehash(arrayAt(ptr, i, esize), o)\n \t\t\t}\n-\t\t\tp1 = unsafe.Pointer(uintptr(p1) + typ.size)\n-\t\t\tp2 = unsafe.Pointer(uintptr(p2) + typ.size)\n+\t\t\treturn o\n \t\t}\n-\t\treturn true\n \t}\n \n \treturn cachePut(ckey, &array.rtype)"}, {"sha": "174320fe85a7a6119c6795f9d06797fb0eb49e68", "filename": "libgo/go/runtime/alg.go", "status": "modified", "additions": 31, "deletions": 3, "changes": 34, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Falg.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Falg.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Falg.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -131,7 +131,7 @@ func c128hash(p unsafe.Pointer, h uintptr) uintptr {\n \treturn f64hash(unsafe.Pointer(&x[1]), f64hash(unsafe.Pointer(&x[0]), h))\n }\n \n-func interhash(p unsafe.Pointer, h uintptr, size uintptr) uintptr {\n+func interhash(p unsafe.Pointer, h uintptr) uintptr {\n \ta := (*iface)(p)\n \ttab := a.tab\n \tif tab == nil {\n@@ -199,10 +199,10 @@ func c128equal(p, q unsafe.Pointer) bool {\n func strequal(p, q unsafe.Pointer) bool {\n \treturn *(*string)(p) == *(*string)(q)\n }\n-func interequal(p, q unsafe.Pointer, size uintptr) bool {\n+func interequal(p, q unsafe.Pointer) bool {\n \treturn ifaceeq(*(*iface)(p), *(*iface)(q))\n }\n-func nilinterequal(p, q unsafe.Pointer, size uintptr) bool {\n+func nilinterequal(p, q unsafe.Pointer) bool {\n \treturn efaceeq(*(*eface)(p), *(*eface)(q))\n }\n func efaceeq(x, y eface) bool {\n@@ -361,6 +361,34 @@ var _ = nilinterequal\n var _ = pointerhash\n var _ = pointerequal\n \n+// Testing adapters for hash quality tests (see hash_test.go)\n+func stringHash(s string, seed uintptr) uintptr {\n+\treturn strhash(noescape(unsafe.Pointer(&s)), seed)\n+}\n+\n+func bytesHash(b []byte, seed uintptr) uintptr {\n+\ts := (*slice)(unsafe.Pointer(&b))\n+\treturn memhash(s.array, seed, uintptr(s.len))\n+}\n+\n+func int32Hash(i uint32, seed uintptr) uintptr {\n+\treturn memhash32(noescape(unsafe.Pointer(&i)), seed)\n+}\n+\n+func int64Hash(i uint64, seed uintptr) uintptr {\n+\treturn memhash64(noescape(unsafe.Pointer(&i)), seed)\n+}\n+\n+func efaceHash(i interface{}, seed uintptr) uintptr {\n+\treturn nilinterhash(noescape(unsafe.Pointer(&i)), seed)\n+}\n+\n+func ifaceHash(i interface {\n+\tF()\n+}, seed uintptr) uintptr {\n+\treturn interhash(noescape(unsafe.Pointer(&i)), seed)\n+}\n+\n const hashRandomBytes = sys.PtrSize / 4 * 64\n \n // used in asm_{386,amd64}.s to seed the hash function"}, {"sha": "2e0e591138a326ce31164e568fad8abd6508109d", "filename": "libgo/go/runtime/cgocall.go", "status": "added", "additions": 307, "deletions": 0, "changes": 307, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fcgocall.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fcgocall.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fcgocall.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,307 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Cgo call and callback support.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/sys\"\n+\t\"unsafe\"\n+)\n+\n+// Pointer checking for cgo code.\n+\n+// We want to detect all cases where a program that does not use\n+// unsafe makes a cgo call passing a Go pointer to memory that\n+// contains a Go pointer. Here a Go pointer is defined as a pointer\n+// to memory allocated by the Go runtime. Programs that use unsafe\n+// can evade this restriction easily, so we don't try to catch them.\n+// The cgo program will rewrite all possibly bad pointer arguments to\n+// call cgoCheckPointer, where we can catch cases of a Go pointer\n+// pointing to a Go pointer.\n+\n+// Complicating matters, taking the address of a slice or array\n+// element permits the C program to access all elements of the slice\n+// or array. In that case we will see a pointer to a single element,\n+// but we need to check the entire data structure.\n+\n+// The cgoCheckPointer call takes additional arguments indicating that\n+// it was called on an address expression. An additional argument of\n+// true means that it only needs to check a single element. An\n+// additional argument of a slice or array means that it needs to\n+// check the entire slice/array, but nothing else. Otherwise, the\n+// pointer could be anything, and we check the entire heap object,\n+// which is conservative but safe.\n+\n+// When and if we implement a moving garbage collector,\n+// cgoCheckPointer will pin the pointer for the duration of the cgo\n+// call.  (This is necessary but not sufficient; the cgo program will\n+// also have to change to pin Go pointers that cannot point to Go\n+// pointers.)\n+\n+// cgoCheckPointer checks if the argument contains a Go pointer that\n+// points to a Go pointer, and panics if it does.\n+func cgoCheckPointer(ptr interface{}, args ...interface{}) {\n+\tif debug.cgocheck == 0 {\n+\t\treturn\n+\t}\n+\n+\tep := (*eface)(unsafe.Pointer(&ptr))\n+\tt := ep._type\n+\n+\ttop := true\n+\tif len(args) > 0 && (t.kind&kindMask == kindPtr || t.kind&kindMask == kindUnsafePointer) {\n+\t\tp := ep.data\n+\t\tif t.kind&kindDirectIface == 0 {\n+\t\t\tp = *(*unsafe.Pointer)(p)\n+\t\t}\n+\t\tif !cgoIsGoPointer(p) {\n+\t\t\treturn\n+\t\t}\n+\t\taep := (*eface)(unsafe.Pointer(&args[0]))\n+\t\tswitch aep._type.kind & kindMask {\n+\t\tcase kindBool:\n+\t\t\tif t.kind&kindMask == kindUnsafePointer {\n+\t\t\t\t// We don't know the type of the element.\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tpt := (*ptrtype)(unsafe.Pointer(t))\n+\t\t\tcgoCheckArg(pt.elem, p, true, false, cgoCheckPointerFail)\n+\t\t\treturn\n+\t\tcase kindSlice:\n+\t\t\t// Check the slice rather than the pointer.\n+\t\t\tep = aep\n+\t\t\tt = ep._type\n+\t\tcase kindArray:\n+\t\t\t// Check the array rather than the pointer.\n+\t\t\t// Pass top as false since we have a pointer\n+\t\t\t// to the array.\n+\t\t\tep = aep\n+\t\t\tt = ep._type\n+\t\t\ttop = false\n+\t\tdefault:\n+\t\t\tthrow(\"can't happen\")\n+\t\t}\n+\t}\n+\n+\tcgoCheckArg(t, ep.data, t.kind&kindDirectIface == 0, top, cgoCheckPointerFail)\n+}\n+\n+const cgoCheckPointerFail = \"cgo argument has Go pointer to Go pointer\"\n+const cgoResultFail = \"cgo result has Go pointer\"\n+\n+// cgoCheckArg is the real work of cgoCheckPointer. The argument p\n+// is either a pointer to the value (of type t), or the value itself,\n+// depending on indir. The top parameter is whether we are at the top\n+// level, where Go pointers are allowed.\n+func cgoCheckArg(t *_type, p unsafe.Pointer, indir, top bool, msg string) {\n+\tif t.kind&kindNoPointers != 0 {\n+\t\t// If the type has no pointers there is nothing to do.\n+\t\treturn\n+\t}\n+\n+\tswitch t.kind & kindMask {\n+\tdefault:\n+\t\tthrow(\"can't happen\")\n+\tcase kindArray:\n+\t\tat := (*arraytype)(unsafe.Pointer(t))\n+\t\tif !indir {\n+\t\t\tif at.len != 1 {\n+\t\t\t\tthrow(\"can't happen\")\n+\t\t\t}\n+\t\t\tcgoCheckArg(at.elem, p, at.elem.kind&kindDirectIface == 0, top, msg)\n+\t\t\treturn\n+\t\t}\n+\t\tfor i := uintptr(0); i < at.len; i++ {\n+\t\t\tcgoCheckArg(at.elem, p, true, top, msg)\n+\t\t\tp = add(p, at.elem.size)\n+\t\t}\n+\tcase kindChan, kindMap:\n+\t\t// These types contain internal pointers that will\n+\t\t// always be allocated in the Go heap. It's never OK\n+\t\t// to pass them to C.\n+\t\tpanic(errorString(msg))\n+\tcase kindFunc:\n+\t\tif indir {\n+\t\t\tp = *(*unsafe.Pointer)(p)\n+\t\t}\n+\t\tif !cgoIsGoPointer(p) {\n+\t\t\treturn\n+\t\t}\n+\t\tpanic(errorString(msg))\n+\tcase kindInterface:\n+\t\tit := *(**_type)(p)\n+\t\tif it == nil {\n+\t\t\treturn\n+\t\t}\n+\t\t// A type known at compile time is OK since it's\n+\t\t// constant. A type not known at compile time will be\n+\t\t// in the heap and will not be OK.\n+\t\tif inheap(uintptr(unsafe.Pointer(it))) {\n+\t\t\tpanic(errorString(msg))\n+\t\t}\n+\t\tp = *(*unsafe.Pointer)(add(p, sys.PtrSize))\n+\t\tif !cgoIsGoPointer(p) {\n+\t\t\treturn\n+\t\t}\n+\t\tif !top {\n+\t\t\tpanic(errorString(msg))\n+\t\t}\n+\t\tcgoCheckArg(it, p, it.kind&kindDirectIface == 0, false, msg)\n+\tcase kindSlice:\n+\t\tst := (*slicetype)(unsafe.Pointer(t))\n+\t\ts := (*slice)(p)\n+\t\tp = s.array\n+\t\tif !cgoIsGoPointer(p) {\n+\t\t\treturn\n+\t\t}\n+\t\tif !top {\n+\t\t\tpanic(errorString(msg))\n+\t\t}\n+\t\tif st.elem.kind&kindNoPointers != 0 {\n+\t\t\treturn\n+\t\t}\n+\t\tfor i := 0; i < s.cap; i++ {\n+\t\t\tcgoCheckArg(st.elem, p, true, false, msg)\n+\t\t\tp = add(p, st.elem.size)\n+\t\t}\n+\tcase kindString:\n+\t\tss := (*stringStruct)(p)\n+\t\tif !cgoIsGoPointer(ss.str) {\n+\t\t\treturn\n+\t\t}\n+\t\tif !top {\n+\t\t\tpanic(errorString(msg))\n+\t\t}\n+\tcase kindStruct:\n+\t\tst := (*structtype)(unsafe.Pointer(t))\n+\t\tif !indir {\n+\t\t\tif len(st.fields) != 1 {\n+\t\t\t\tthrow(\"can't happen\")\n+\t\t\t}\n+\t\t\tcgoCheckArg(st.fields[0].typ, p, st.fields[0].typ.kind&kindDirectIface == 0, top, msg)\n+\t\t\treturn\n+\t\t}\n+\t\tfor _, f := range st.fields {\n+\t\t\tcgoCheckArg(f.typ, add(p, f.offset), true, top, msg)\n+\t\t}\n+\tcase kindPtr, kindUnsafePointer:\n+\t\tif indir {\n+\t\t\tp = *(*unsafe.Pointer)(p)\n+\t\t}\n+\n+\t\tif !cgoIsGoPointer(p) {\n+\t\t\treturn\n+\t\t}\n+\t\tif !top {\n+\t\t\tpanic(errorString(msg))\n+\t\t}\n+\n+\t\tcgoCheckUnknownPointer(p, msg)\n+\t}\n+}\n+\n+// cgoCheckUnknownPointer is called for an arbitrary pointer into Go\n+// memory. It checks whether that Go memory contains any other\n+// pointer into Go memory. If it does, we panic.\n+// The return values are unused but useful to see in panic tracebacks.\n+func cgoCheckUnknownPointer(p unsafe.Pointer, msg string) (base, i uintptr) {\n+\tif cgoInRange(p, mheap_.arena_start, mheap_.arena_used) {\n+\t\tif !inheap(uintptr(p)) {\n+\t\t\t// On 32-bit systems it is possible for C's allocated memory\n+\t\t\t// to have addresses between arena_start and arena_used.\n+\t\t\t// Either this pointer is a stack or an unused span or it's\n+\t\t\t// a C allocation. Escape analysis should prevent the first,\n+\t\t\t// garbage collection should prevent the second,\n+\t\t\t// and the third is completely OK.\n+\t\t\treturn\n+\t\t}\n+\n+\t\tb, hbits, span, _ := heapBitsForObject(uintptr(p), 0, 0, false)\n+\t\tbase = b\n+\t\tif base == 0 {\n+\t\t\treturn\n+\t\t}\n+\t\tn := span.elemsize\n+\t\tfor i = uintptr(0); i < n; i += sys.PtrSize {\n+\t\t\tif i != 1*sys.PtrSize && !hbits.morePointers() {\n+\t\t\t\t// No more possible pointers.\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tif hbits.isPointer() {\n+\t\t\t\tif cgoIsGoPointer(*(*unsafe.Pointer)(unsafe.Pointer(base + i))) {\n+\t\t\t\t\tpanic(errorString(msg))\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\thbits = hbits.next()\n+\t\t}\n+\n+\t\treturn\n+\t}\n+\n+\troots := gcRoots\n+\tfor roots != nil {\n+\t\tfor j := 0; j < roots.count; j++ {\n+\t\t\tpr := roots.roots[j]\n+\t\t\taddr := uintptr(pr.decl)\n+\t\t\tif cgoInRange(p, addr, addr+pr.size) {\n+\t\t\t\tcgoCheckBits(pr.decl, pr.gcdata, 0, pr.ptrdata)\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t}\n+\t\troots = roots.next\n+\t}\n+\n+\treturn\n+}\n+\n+// cgoIsGoPointer returns whether the pointer is a Go pointer--a\n+// pointer to Go memory. We only care about Go memory that might\n+// contain pointers.\n+//go:nosplit\n+//go:nowritebarrierrec\n+func cgoIsGoPointer(p unsafe.Pointer) bool {\n+\tif p == nil {\n+\t\treturn false\n+\t}\n+\n+\tif inHeapOrStack(uintptr(p)) {\n+\t\treturn true\n+\t}\n+\n+\troots := gcRoots\n+\tfor roots != nil {\n+\t\tfor i := 0; i < roots.count; i++ {\n+\t\t\tpr := roots.roots[i]\n+\t\t\taddr := uintptr(pr.decl)\n+\t\t\tif cgoInRange(p, addr, addr+pr.size) {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t}\n+\t\troots = roots.next\n+\t}\n+\n+\treturn false\n+}\n+\n+// cgoInRange returns whether p is between start and end.\n+//go:nosplit\n+//go:nowritebarrierrec\n+func cgoInRange(p unsafe.Pointer, start, end uintptr) bool {\n+\treturn start <= uintptr(p) && uintptr(p) < end\n+}\n+\n+// cgoCheckResult is called to check the result parameter of an\n+// exported Go function. It panics if the result is or contains a Go\n+// pointer.\n+func cgoCheckResult(val interface{}) {\n+\tif debug.cgocheck == 0 {\n+\t\treturn\n+\t}\n+\n+\tep := (*eface)(unsafe.Pointer(&val))\n+\tt := ep._type\n+\tcgoCheckArg(t, ep.data, t.kind&kindDirectIface == 0, false, cgoResultFail)\n+}"}, {"sha": "09d444dbd1aaaf63a9c2d359f2d505e99e844540", "filename": "libgo/go/runtime/cgocheck.go", "status": "modified", "additions": 11, "deletions": 12, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fcgocheck.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fcgocheck.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fcgocheck.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,8 +2,6 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build ignore\n-\n // Code to check that pointer writes follow the cgo rules.\n // These functions are invoked via the write barrier when debug.cgocheck > 1.\n \n@@ -110,17 +108,18 @@ func cgoCheckTypedBlock(typ *_type, src unsafe.Pointer, off, size uintptr) {\n \t}\n \n \t// The type has a GC program. Try to find GC bits somewhere else.\n-\tfor _, datap := range activeModules() {\n-\t\tif cgoInRange(src, datap.data, datap.edata) {\n-\t\t\tdoff := uintptr(src) - datap.data\n-\t\t\tcgoCheckBits(add(src, -doff), datap.gcdatamask.bytedata, off+doff, size)\n-\t\t\treturn\n-\t\t}\n-\t\tif cgoInRange(src, datap.bss, datap.ebss) {\n-\t\t\tboff := uintptr(src) - datap.bss\n-\t\t\tcgoCheckBits(add(src, -boff), datap.gcbssmask.bytedata, off+boff, size)\n-\t\t\treturn\n+\troots := gcRoots\n+\tfor roots != nil {\n+\t\tfor i := 0; i < roots.count; i++ {\n+\t\t\tpr := roots.roots[i]\n+\t\t\taddr := uintptr(pr.decl)\n+\t\t\tif cgoInRange(src, addr, addr+pr.size) {\n+\t\t\t\tdoff := uintptr(src) - addr\n+\t\t\t\tcgoCheckBits(add(src, -doff), pr.gcdata, off+doff, size)\n+\t\t\t\treturn\n+\t\t\t}\n \t\t}\n+\t\troots = roots.next\n \t}\n \n \taoff := uintptr(src) - mheap_.arena_start"}, {"sha": "67ef3342946ac1be5347d1f3ca3ccb0269f876ca", "filename": "libgo/go/runtime/crash_unix_test.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fcrash_unix_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fcrash_unix_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fcrash_unix_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux netbsd openbsd solaris\n+// +build aix darwin dragonfly freebsd linux netbsd openbsd solaris\n \n package runtime_test\n "}, {"sha": "6a9efcd7da7a59cbb26fe4c50b13a7000705707f", "filename": "libgo/go/runtime/debug.go", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fdebug.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fdebug.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fdebug.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -39,7 +39,9 @@ func GOMAXPROCS(n int) int {\n // The set of available CPUs is checked by querying the operating system\n // at process startup. Changes to operating system CPU allocation after\n // process startup are not reflected.\n-func NumCPU() int\n+func NumCPU() int {\n+\treturn int(ncpu)\n+}\n \n // NumCgoCall returns the number of cgo calls made by the current process.\n func NumCgoCall() int64 {"}, {"sha": "9bf7ddcc535c21a5800ba1b5d510bc56705d79a7", "filename": "libgo/go/runtime/env_posix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fenv_posix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fenv_posix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fenv_posix.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n \n package runtime\n "}, {"sha": "bf435f447e0b8b7d6a445e88e98cf17ea3f33846", "filename": "libgo/go/runtime/export_test.go", "status": "modified", "additions": 9, "deletions": 12, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fexport_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fexport_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fexport_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -26,10 +26,11 @@ import (\n var Entersyscall = entersyscall\n var Exitsyscall = exitsyscall\n var LockedOSThread = lockedOSThread\n+var Xadduintptr = atomic.Xadduintptr\n \n-// var Xadduintptr = xadduintptr\n+var FuncPC = funcPC\n \n-// var FuncPC = funcPC\n+var Fastlog2 = fastlog2\n \n var Atoi = atoi\n var Atoi32 = atoi32\n@@ -148,12 +149,12 @@ func RunSchedLocalQueueEmptyTest(iters int) {\n \t}\n }\n \n-//var StringHash = stringHash\n-//var BytesHash = bytesHash\n-//var Int32Hash = int32Hash\n-//var Int64Hash = int64Hash\n-//var EfaceHash = efaceHash\n-//var IfaceHash = ifaceHash\n+var StringHash = stringHash\n+var BytesHash = bytesHash\n+var Int32Hash = int32Hash\n+var Int64Hash = int64Hash\n+var EfaceHash = efaceHash\n+var IfaceHash = ifaceHash\n \n func MemclrBytes(b []byte) {\n \ts := (*slice)(unsafe.Pointer(&b))\n@@ -182,7 +183,6 @@ func SetEnvs(e []string) { envs = e }\n \n // For benchmarking.\n \n-/*\n func BenchSetType(n int, x interface{}) {\n \te := *efaceOf(&x)\n \tt := e._type\n@@ -213,7 +213,6 @@ func BenchSetType(n int, x interface{}) {\n const PtrSize = sys.PtrSize\n \n var ForceGCPeriod = &forcegcperiod\n-*/\n \n // SetTracebackEnv is like runtime/debug.SetTraceback, but it raises\n // the \"environment\" traceback level, so later calls to\n@@ -223,7 +222,6 @@ func SetTracebackEnv(level string) {\n \ttraceback_env = traceback_cache\n }\n \n-/*\n var ReadUnaligned32 = readUnaligned32\n var ReadUnaligned64 = readUnaligned64\n \n@@ -242,7 +240,6 @@ func CountPagesInUse() (pagesInUse, counted uintptr) {\n \n \treturn\n }\n-*/\n \n // BlockOnSystemStack switches to the system stack, prints \"x\\n\" to\n // stderr, and blocks in a stack containing"}, {"sha": "5c50760b8b8049d52a12fd2a919096cc0714b301", "filename": "libgo/go/runtime/extern.go", "status": "modified", "additions": 0, "deletions": 87, "changes": 87, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fextern.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fextern.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fextern.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -183,93 +183,6 @@ func Caller(skip int) (pc uintptr, file string, line int, ok bool)\n // It returns the number of entries written to pc.\n func Callers(skip int, pc []uintptr) int\n \n-// SetFinalizer sets the finalizer associated with obj to the provided\n-// finalizer function. When the garbage collector finds an unreachable block\n-// with an associated finalizer, it clears the association and runs\n-// finalizer(obj) in a separate goroutine. This makes obj reachable again,\n-// but now without an associated finalizer. Assuming that SetFinalizer\n-// is not called again, the next time the garbage collector sees\n-// that obj is unreachable, it will free obj.\n-//\n-// SetFinalizer(obj, nil) clears any finalizer associated with obj.\n-//\n-// The argument obj must be a pointer to an object allocated by\n-// calling new or by taking the address of a composite literal.\n-// The argument finalizer must be a function that takes a single argument\n-// to which obj's type can be assigned, and can have arbitrary ignored return\n-// values. If either of these is not true, SetFinalizer aborts the\n-// program.\n-//\n-// Finalizers are run in dependency order: if A points at B, both have\n-// finalizers, and they are otherwise unreachable, only the finalizer\n-// for A runs; once A is freed, the finalizer for B can run.\n-// If a cyclic structure includes a block with a finalizer, that\n-// cycle is not guaranteed to be garbage collected and the finalizer\n-// is not guaranteed to run, because there is no ordering that\n-// respects the dependencies.\n-//\n-// The finalizer for obj is scheduled to run at some arbitrary time after\n-// obj becomes unreachable.\n-// There is no guarantee that finalizers will run before a program exits,\n-// so typically they are useful only for releasing non-memory resources\n-// associated with an object during a long-running program.\n-// For example, an os.File object could use a finalizer to close the\n-// associated operating system file descriptor when a program discards\n-// an os.File without calling Close, but it would be a mistake\n-// to depend on a finalizer to flush an in-memory I/O buffer such as a\n-// bufio.Writer, because the buffer would not be flushed at program exit.\n-//\n-// It is not guaranteed that a finalizer will run if the size of *obj is\n-// zero bytes.\n-//\n-// It is not guaranteed that a finalizer will run for objects allocated\n-// in initializers for package-level variables. Such objects may be\n-// linker-allocated, not heap-allocated.\n-//\n-// A finalizer may run as soon as an object becomes unreachable.\n-// In order to use finalizers correctly, the program must ensure that\n-// the object is reachable until it is no longer required.\n-// Objects stored in global variables, or that can be found by tracing\n-// pointers from a global variable, are reachable. For other objects,\n-// pass the object to a call of the KeepAlive function to mark the\n-// last point in the function where the object must be reachable.\n-//\n-// For example, if p points to a struct that contains a file descriptor d,\n-// and p has a finalizer that closes that file descriptor, and if the last\n-// use of p in a function is a call to syscall.Write(p.d, buf, size), then\n-// p may be unreachable as soon as the program enters syscall.Write. The\n-// finalizer may run at that moment, closing p.d, causing syscall.Write\n-// to fail because it is writing to a closed file descriptor (or, worse,\n-// to an entirely different file descriptor opened by a different goroutine).\n-// To avoid this problem, call runtime.KeepAlive(p) after the call to\n-// syscall.Write.\n-//\n-// A single goroutine runs all finalizers for a program, sequentially.\n-// If a finalizer must run for a long time, it should do so by starting\n-// a new goroutine.\n-func SetFinalizer(obj interface{}, finalizer interface{})\n-\n-// KeepAlive marks its argument as currently reachable.\n-// This ensures that the object is not freed, and its finalizer is not run,\n-// before the point in the program where KeepAlive is called.\n-//\n-// A very simplified example showing where KeepAlive is required:\n-// \ttype File struct { d int }\n-// \td, err := syscall.Open(\"/file/path\", syscall.O_RDONLY, 0)\n-// \t// ... do something if err != nil ...\n-// \tp := &File{d}\n-// \truntime.SetFinalizer(p, func(p *File) { syscall.Close(p.d) })\n-// \tvar buf [10]byte\n-// \tn, err := syscall.Read(p.d, buf[:])\n-// \t// Ensure p is not finalized until Read returns.\n-// \truntime.KeepAlive(p)\n-// \t// No more uses of p after this point.\n-//\n-// Without the KeepAlive call, the finalizer could run at the start of\n-// syscall.Read, closing the file descriptor before syscall.Read makes\n-// the actual system call.\n-func KeepAlive(interface{})\n-\n // GOROOT returns the root of the Go tree.\n // It uses the GOROOT environment variable, if set,\n // or else the root used during the Go build."}, {"sha": "5f3fb53423297be3356292429fee7543eaf284ed", "filename": "libgo/go/runtime/fastlog2.go", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Ffastlog2.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Ffastlog2.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Ffastlog2.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,8 +2,6 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build ignore\n-\n package runtime\n \n import \"unsafe\""}, {"sha": "ae0f40b2bb7772a8eae364e02fb4660194928092", "filename": "libgo/go/runtime/fastlog2_test.go", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Ffastlog2_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Ffastlog2_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Ffastlog2_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,8 +2,6 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build ignore\n-\n package runtime_test\n \n import ("}, {"sha": "c36d5835f64b3838cc5443fefc93312dab3190c4", "filename": "libgo/go/runtime/fastlog2table.go", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Ffastlog2table.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Ffastlog2table.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Ffastlog2table.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,8 +2,6 @@\n // Run go generate from src/runtime to update.\n // See mkfastlog2table.go for comments.\n \n-// +build ignore\n-\n package runtime\n \n const fastlogNumBits = 5"}, {"sha": "ec043ed45bef709e3e59ca492e2c62493f8b6096", "filename": "libgo/go/runtime/gc_test.go", "status": "modified", "additions": 0, "deletions": 4, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fgc_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fgc_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fgc_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -434,8 +434,6 @@ func testIfaceEqual(x interface{}) {\n \t}\n }\n \n-/*\n-\n func TestPageAccounting(t *testing.T) {\n \t// Grow the heap in small increments. This used to drop the\n \t// pages-in-use count below zero because of a rounding\n@@ -452,5 +450,3 @@ func TestPageAccounting(t *testing.T) {\n \t\tt.Fatalf(\"mheap_.pagesInUse is %d, but direct count is %d\", pagesInUse, counted)\n \t}\n }\n-\n-*/"}, {"sha": "167c49eb5f59a286185c21485e804416942c0def", "filename": "libgo/go/runtime/hash_test.go", "status": "added", "additions": 710, "deletions": 0, "changes": 710, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fhash_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fhash_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fhash_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,710 @@\n+// Copyright 2013 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package runtime_test\n+\n+import (\n+\t\"fmt\"\n+\t\"math\"\n+\t\"math/rand\"\n+\t. \"runtime\"\n+\t\"strings\"\n+\t\"testing\"\n+\t\"unsafe\"\n+)\n+\n+// Smhasher is a torture test for hash functions.\n+// https://code.google.com/p/smhasher/\n+// This code is a port of some of the Smhasher tests to Go.\n+//\n+// The current AES hash function passes Smhasher. Our fallback\n+// hash functions don't, so we only enable the difficult tests when\n+// we know the AES implementation is available.\n+\n+// Sanity checks.\n+// hash should not depend on values outside key.\n+// hash should not depend on alignment.\n+func TestSmhasherSanity(t *testing.T) {\n+\tr := rand.New(rand.NewSource(1234))\n+\tconst REP = 10\n+\tconst KEYMAX = 128\n+\tconst PAD = 16\n+\tconst OFFMAX = 16\n+\tfor k := 0; k < REP; k++ {\n+\t\tfor n := 0; n < KEYMAX; n++ {\n+\t\t\tfor i := 0; i < OFFMAX; i++ {\n+\t\t\t\tvar b [KEYMAX + OFFMAX + 2*PAD]byte\n+\t\t\t\tvar c [KEYMAX + OFFMAX + 2*PAD]byte\n+\t\t\t\trandBytes(r, b[:])\n+\t\t\t\trandBytes(r, c[:])\n+\t\t\t\tcopy(c[PAD+i:PAD+i+n], b[PAD:PAD+n])\n+\t\t\t\tif BytesHash(b[PAD:PAD+n], 0) != BytesHash(c[PAD+i:PAD+i+n], 0) {\n+\t\t\t\t\tt.Errorf(\"hash depends on bytes outside key\")\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+type HashSet struct {\n+\tm map[uintptr]struct{} // set of hashes added\n+\tn int                  // number of hashes added\n+}\n+\n+func newHashSet() *HashSet {\n+\treturn &HashSet{make(map[uintptr]struct{}), 0}\n+}\n+func (s *HashSet) add(h uintptr) {\n+\ts.m[h] = struct{}{}\n+\ts.n++\n+}\n+func (s *HashSet) addS(x string) {\n+\ts.add(StringHash(x, 0))\n+}\n+func (s *HashSet) addB(x []byte) {\n+\ts.add(BytesHash(x, 0))\n+}\n+func (s *HashSet) addS_seed(x string, seed uintptr) {\n+\ts.add(StringHash(x, seed))\n+}\n+func (s *HashSet) check(t *testing.T) {\n+\tconst SLOP = 10.0\n+\tcollisions := s.n - len(s.m)\n+\t//fmt.Printf(\"%d/%d\\n\", len(s.m), s.n)\n+\tpairs := int64(s.n) * int64(s.n-1) / 2\n+\texpected := float64(pairs) / math.Pow(2.0, float64(hashSize))\n+\tstddev := math.Sqrt(expected)\n+\tif float64(collisions) > expected+SLOP*(3*stddev+1) {\n+\t\tt.Errorf(\"unexpected number of collisions: got=%d mean=%f stddev=%f\", collisions, expected, stddev)\n+\t}\n+}\n+\n+// a string plus adding zeros must make distinct hashes\n+func TestSmhasherAppendedZeros(t *testing.T) {\n+\ts := \"hello\" + strings.Repeat(\"\\x00\", 256)\n+\th := newHashSet()\n+\tfor i := 0; i <= len(s); i++ {\n+\t\th.addS(s[:i])\n+\t}\n+\th.check(t)\n+}\n+\n+// All 0-3 byte strings have distinct hashes.\n+func TestSmhasherSmallKeys(t *testing.T) {\n+\th := newHashSet()\n+\tvar b [3]byte\n+\tfor i := 0; i < 256; i++ {\n+\t\tb[0] = byte(i)\n+\t\th.addB(b[:1])\n+\t\tfor j := 0; j < 256; j++ {\n+\t\t\tb[1] = byte(j)\n+\t\t\th.addB(b[:2])\n+\t\t\tif !testing.Short() {\n+\t\t\t\tfor k := 0; k < 256; k++ {\n+\t\t\t\t\tb[2] = byte(k)\n+\t\t\t\t\th.addB(b[:3])\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\th.check(t)\n+}\n+\n+// Different length strings of all zeros have distinct hashes.\n+func TestSmhasherZeros(t *testing.T) {\n+\tN := 256 * 1024\n+\tif testing.Short() {\n+\t\tN = 1024\n+\t}\n+\th := newHashSet()\n+\tb := make([]byte, N)\n+\tfor i := 0; i <= N; i++ {\n+\t\th.addB(b[:i])\n+\t}\n+\th.check(t)\n+}\n+\n+// Strings with up to two nonzero bytes all have distinct hashes.\n+func TestSmhasherTwoNonzero(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"Skipping in short mode\")\n+\t}\n+\th := newHashSet()\n+\tfor n := 2; n <= 16; n++ {\n+\t\ttwoNonZero(h, n)\n+\t}\n+\th.check(t)\n+}\n+func twoNonZero(h *HashSet, n int) {\n+\tb := make([]byte, n)\n+\n+\t// all zero\n+\th.addB(b[:])\n+\n+\t// one non-zero byte\n+\tfor i := 0; i < n; i++ {\n+\t\tfor x := 1; x < 256; x++ {\n+\t\t\tb[i] = byte(x)\n+\t\t\th.addB(b[:])\n+\t\t\tb[i] = 0\n+\t\t}\n+\t}\n+\n+\t// two non-zero bytes\n+\tfor i := 0; i < n; i++ {\n+\t\tfor x := 1; x < 256; x++ {\n+\t\t\tb[i] = byte(x)\n+\t\t\tfor j := i + 1; j < n; j++ {\n+\t\t\t\tfor y := 1; y < 256; y++ {\n+\t\t\t\t\tb[j] = byte(y)\n+\t\t\t\t\th.addB(b[:])\n+\t\t\t\t\tb[j] = 0\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tb[i] = 0\n+\t\t}\n+\t}\n+}\n+\n+// Test strings with repeats, like \"abcdabcdabcdabcd...\"\n+func TestSmhasherCyclic(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"Skipping in short mode\")\n+\t}\n+\tr := rand.New(rand.NewSource(1234))\n+\tconst REPEAT = 8\n+\tconst N = 1000000\n+\tfor n := 4; n <= 12; n++ {\n+\t\th := newHashSet()\n+\t\tb := make([]byte, REPEAT*n)\n+\t\tfor i := 0; i < N; i++ {\n+\t\t\tb[0] = byte(i * 79 % 97)\n+\t\t\tb[1] = byte(i * 43 % 137)\n+\t\t\tb[2] = byte(i * 151 % 197)\n+\t\t\tb[3] = byte(i * 199 % 251)\n+\t\t\trandBytes(r, b[4:n])\n+\t\t\tfor j := n; j < n*REPEAT; j++ {\n+\t\t\t\tb[j] = b[j-n]\n+\t\t\t}\n+\t\t\th.addB(b)\n+\t\t}\n+\t\th.check(t)\n+\t}\n+}\n+\n+// Test strings with only a few bits set\n+func TestSmhasherSparse(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"Skipping in short mode\")\n+\t}\n+\tsparse(t, 32, 6)\n+\tsparse(t, 40, 6)\n+\tsparse(t, 48, 5)\n+\tsparse(t, 56, 5)\n+\tsparse(t, 64, 5)\n+\tsparse(t, 96, 4)\n+\tsparse(t, 256, 3)\n+\tsparse(t, 2048, 2)\n+}\n+func sparse(t *testing.T, n int, k int) {\n+\tb := make([]byte, n/8)\n+\th := newHashSet()\n+\tsetbits(h, b, 0, k)\n+\th.check(t)\n+}\n+\n+// set up to k bits at index i and greater\n+func setbits(h *HashSet, b []byte, i int, k int) {\n+\th.addB(b)\n+\tif k == 0 {\n+\t\treturn\n+\t}\n+\tfor j := i; j < len(b)*8; j++ {\n+\t\tb[j/8] |= byte(1 << uint(j&7))\n+\t\tsetbits(h, b, j+1, k-1)\n+\t\tb[j/8] &= byte(^(1 << uint(j&7)))\n+\t}\n+}\n+\n+// Test all possible combinations of n blocks from the set s.\n+// \"permutation\" is a bad name here, but it is what Smhasher uses.\n+func TestSmhasherPermutation(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"Skipping in short mode\")\n+\t}\n+\tpermutation(t, []uint32{0, 1, 2, 3, 4, 5, 6, 7}, 8)\n+\tpermutation(t, []uint32{0, 1 << 29, 2 << 29, 3 << 29, 4 << 29, 5 << 29, 6 << 29, 7 << 29}, 8)\n+\tpermutation(t, []uint32{0, 1}, 20)\n+\tpermutation(t, []uint32{0, 1 << 31}, 20)\n+\tpermutation(t, []uint32{0, 1, 2, 3, 4, 5, 6, 7, 1 << 29, 2 << 29, 3 << 29, 4 << 29, 5 << 29, 6 << 29, 7 << 29}, 6)\n+}\n+func permutation(t *testing.T, s []uint32, n int) {\n+\tb := make([]byte, n*4)\n+\th := newHashSet()\n+\tgenPerm(h, b, s, 0)\n+\th.check(t)\n+}\n+func genPerm(h *HashSet, b []byte, s []uint32, n int) {\n+\th.addB(b[:n])\n+\tif n == len(b) {\n+\t\treturn\n+\t}\n+\tfor _, v := range s {\n+\t\tb[n] = byte(v)\n+\t\tb[n+1] = byte(v >> 8)\n+\t\tb[n+2] = byte(v >> 16)\n+\t\tb[n+3] = byte(v >> 24)\n+\t\tgenPerm(h, b, s, n+4)\n+\t}\n+}\n+\n+type Key interface {\n+\tclear()              // set bits all to 0\n+\trandom(r *rand.Rand) // set key to something random\n+\tbits() int           // how many bits key has\n+\tflipBit(i int)       // flip bit i of the key\n+\thash() uintptr       // hash the key\n+\tname() string        // for error reporting\n+}\n+\n+type BytesKey struct {\n+\tb []byte\n+}\n+\n+func (k *BytesKey) clear() {\n+\tfor i := range k.b {\n+\t\tk.b[i] = 0\n+\t}\n+}\n+func (k *BytesKey) random(r *rand.Rand) {\n+\trandBytes(r, k.b)\n+}\n+func (k *BytesKey) bits() int {\n+\treturn len(k.b) * 8\n+}\n+func (k *BytesKey) flipBit(i int) {\n+\tk.b[i>>3] ^= byte(1 << uint(i&7))\n+}\n+func (k *BytesKey) hash() uintptr {\n+\treturn BytesHash(k.b, 0)\n+}\n+func (k *BytesKey) name() string {\n+\treturn fmt.Sprintf(\"bytes%d\", len(k.b))\n+}\n+\n+type Int32Key struct {\n+\ti uint32\n+}\n+\n+func (k *Int32Key) clear() {\n+\tk.i = 0\n+}\n+func (k *Int32Key) random(r *rand.Rand) {\n+\tk.i = r.Uint32()\n+}\n+func (k *Int32Key) bits() int {\n+\treturn 32\n+}\n+func (k *Int32Key) flipBit(i int) {\n+\tk.i ^= 1 << uint(i)\n+}\n+func (k *Int32Key) hash() uintptr {\n+\treturn Int32Hash(k.i, 0)\n+}\n+func (k *Int32Key) name() string {\n+\treturn \"int32\"\n+}\n+\n+type Int64Key struct {\n+\ti uint64\n+}\n+\n+func (k *Int64Key) clear() {\n+\tk.i = 0\n+}\n+func (k *Int64Key) random(r *rand.Rand) {\n+\tk.i = uint64(r.Uint32()) + uint64(r.Uint32())<<32\n+}\n+func (k *Int64Key) bits() int {\n+\treturn 64\n+}\n+func (k *Int64Key) flipBit(i int) {\n+\tk.i ^= 1 << uint(i)\n+}\n+func (k *Int64Key) hash() uintptr {\n+\treturn Int64Hash(k.i, 0)\n+}\n+func (k *Int64Key) name() string {\n+\treturn \"int64\"\n+}\n+\n+type EfaceKey struct {\n+\ti interface{}\n+}\n+\n+func (k *EfaceKey) clear() {\n+\tk.i = nil\n+}\n+func (k *EfaceKey) random(r *rand.Rand) {\n+\tk.i = uint64(r.Int63())\n+}\n+func (k *EfaceKey) bits() int {\n+\t// use 64 bits. This tests inlined interfaces\n+\t// on 64-bit targets and indirect interfaces on\n+\t// 32-bit targets.\n+\treturn 64\n+}\n+func (k *EfaceKey) flipBit(i int) {\n+\tk.i = k.i.(uint64) ^ uint64(1)<<uint(i)\n+}\n+func (k *EfaceKey) hash() uintptr {\n+\treturn EfaceHash(k.i, 0)\n+}\n+func (k *EfaceKey) name() string {\n+\treturn \"Eface\"\n+}\n+\n+type IfaceKey struct {\n+\ti interface {\n+\t\tF()\n+\t}\n+}\n+type fInter uint64\n+\n+func (x fInter) F() {\n+}\n+\n+func (k *IfaceKey) clear() {\n+\tk.i = nil\n+}\n+func (k *IfaceKey) random(r *rand.Rand) {\n+\tk.i = fInter(r.Int63())\n+}\n+func (k *IfaceKey) bits() int {\n+\t// use 64 bits. This tests inlined interfaces\n+\t// on 64-bit targets and indirect interfaces on\n+\t// 32-bit targets.\n+\treturn 64\n+}\n+func (k *IfaceKey) flipBit(i int) {\n+\tk.i = k.i.(fInter) ^ fInter(1)<<uint(i)\n+}\n+func (k *IfaceKey) hash() uintptr {\n+\treturn IfaceHash(k.i, 0)\n+}\n+func (k *IfaceKey) name() string {\n+\treturn \"Iface\"\n+}\n+\n+// Flipping a single bit of a key should flip each output bit with 50% probability.\n+func TestSmhasherAvalanche(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"Skipping in short mode\")\n+\t}\n+\tavalancheTest1(t, &BytesKey{make([]byte, 2)})\n+\tavalancheTest1(t, &BytesKey{make([]byte, 4)})\n+\tavalancheTest1(t, &BytesKey{make([]byte, 8)})\n+\tavalancheTest1(t, &BytesKey{make([]byte, 16)})\n+\tavalancheTest1(t, &BytesKey{make([]byte, 32)})\n+\tavalancheTest1(t, &BytesKey{make([]byte, 200)})\n+\tavalancheTest1(t, &Int32Key{})\n+\tavalancheTest1(t, &Int64Key{})\n+\tavalancheTest1(t, &EfaceKey{})\n+\tavalancheTest1(t, &IfaceKey{})\n+}\n+func avalancheTest1(t *testing.T, k Key) {\n+\tconst REP = 100000\n+\tr := rand.New(rand.NewSource(1234))\n+\tn := k.bits()\n+\n+\t// grid[i][j] is a count of whether flipping\n+\t// input bit i affects output bit j.\n+\tgrid := make([][hashSize]int, n)\n+\n+\tfor z := 0; z < REP; z++ {\n+\t\t// pick a random key, hash it\n+\t\tk.random(r)\n+\t\th := k.hash()\n+\n+\t\t// flip each bit, hash & compare the results\n+\t\tfor i := 0; i < n; i++ {\n+\t\t\tk.flipBit(i)\n+\t\t\td := h ^ k.hash()\n+\t\t\tk.flipBit(i)\n+\n+\t\t\t// record the effects of that bit flip\n+\t\t\tg := &grid[i]\n+\t\t\tfor j := 0; j < hashSize; j++ {\n+\t\t\t\tg[j] += int(d & 1)\n+\t\t\t\td >>= 1\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Each entry in the grid should be about REP/2.\n+\t// More precisely, we did N = k.bits() * hashSize experiments where\n+\t// each is the sum of REP coin flips. We want to find bounds on the\n+\t// sum of coin flips such that a truly random experiment would have\n+\t// all sums inside those bounds with 99% probability.\n+\tN := n * hashSize\n+\tvar c float64\n+\t// find c such that Prob(mean-c*stddev < x < mean+c*stddev)^N > .9999\n+\tfor c = 0.0; math.Pow(math.Erf(c/math.Sqrt(2)), float64(N)) < .9999; c += .1 {\n+\t}\n+\tc *= 4.0 // allowed slack - we don't need to be perfectly random\n+\tmean := .5 * REP\n+\tstddev := .5 * math.Sqrt(REP)\n+\tlow := int(mean - c*stddev)\n+\thigh := int(mean + c*stddev)\n+\tfor i := 0; i < n; i++ {\n+\t\tfor j := 0; j < hashSize; j++ {\n+\t\t\tx := grid[i][j]\n+\t\t\tif x < low || x > high {\n+\t\t\t\tt.Errorf(\"bad bias for %s bit %d -> bit %d: %d/%d\\n\", k.name(), i, j, x, REP)\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+// All bit rotations of a set of distinct keys\n+func TestSmhasherWindowed(t *testing.T) {\n+\twindowed(t, &Int32Key{})\n+\twindowed(t, &Int64Key{})\n+\twindowed(t, &BytesKey{make([]byte, 128)})\n+}\n+func windowed(t *testing.T, k Key) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"Skipping in short mode\")\n+\t}\n+\tconst BITS = 16\n+\n+\tfor r := 0; r < k.bits(); r++ {\n+\t\th := newHashSet()\n+\t\tfor i := 0; i < 1<<BITS; i++ {\n+\t\t\tk.clear()\n+\t\t\tfor j := 0; j < BITS; j++ {\n+\t\t\t\tif i>>uint(j)&1 != 0 {\n+\t\t\t\t\tk.flipBit((j + r) % k.bits())\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\th.add(k.hash())\n+\t\t}\n+\t\th.check(t)\n+\t}\n+}\n+\n+// All keys of the form prefix + [A-Za-z0-9]*N + suffix.\n+func TestSmhasherText(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"Skipping in short mode\")\n+\t}\n+\ttext(t, \"Foo\", \"Bar\")\n+\ttext(t, \"FooBar\", \"\")\n+\ttext(t, \"\", \"FooBar\")\n+}\n+func text(t *testing.T, prefix, suffix string) {\n+\tconst N = 4\n+\tconst S = \"ABCDEFGHIJKLMNOPQRSTabcdefghijklmnopqrst0123456789\"\n+\tconst L = len(S)\n+\tb := make([]byte, len(prefix)+N+len(suffix))\n+\tcopy(b, prefix)\n+\tcopy(b[len(prefix)+N:], suffix)\n+\th := newHashSet()\n+\tc := b[len(prefix):]\n+\tfor i := 0; i < L; i++ {\n+\t\tc[0] = S[i]\n+\t\tfor j := 0; j < L; j++ {\n+\t\t\tc[1] = S[j]\n+\t\t\tfor k := 0; k < L; k++ {\n+\t\t\t\tc[2] = S[k]\n+\t\t\t\tfor x := 0; x < L; x++ {\n+\t\t\t\t\tc[3] = S[x]\n+\t\t\t\t\th.addB(b)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\th.check(t)\n+}\n+\n+// Make sure different seed values generate different hashes.\n+func TestSmhasherSeed(t *testing.T) {\n+\th := newHashSet()\n+\tconst N = 100000\n+\ts := \"hello\"\n+\tfor i := 0; i < N; i++ {\n+\t\th.addS_seed(s, uintptr(i))\n+\t}\n+\th.check(t)\n+}\n+\n+// size of the hash output (32 or 64 bits)\n+const hashSize = 32 + int(^uintptr(0)>>63<<5)\n+\n+func randBytes(r *rand.Rand, b []byte) {\n+\tfor i := range b {\n+\t\tb[i] = byte(r.Uint32())\n+\t}\n+}\n+\n+func benchmarkHash(b *testing.B, n int) {\n+\ts := strings.Repeat(\"A\", n)\n+\n+\tfor i := 0; i < b.N; i++ {\n+\t\tStringHash(s, 0)\n+\t}\n+\tb.SetBytes(int64(n))\n+}\n+\n+func BenchmarkHash5(b *testing.B)     { benchmarkHash(b, 5) }\n+func BenchmarkHash16(b *testing.B)    { benchmarkHash(b, 16) }\n+func BenchmarkHash64(b *testing.B)    { benchmarkHash(b, 64) }\n+func BenchmarkHash1024(b *testing.B)  { benchmarkHash(b, 1024) }\n+func BenchmarkHash65536(b *testing.B) { benchmarkHash(b, 65536) }\n+\n+func TestArrayHash(t *testing.T) {\n+\tif Compiler == \"gccgo\" {\n+\t\tt.Skip(\"does not work on gccgo without better escape analysis\")\n+\t}\n+\n+\t// Make sure that \"\" in arrays hash correctly. The hash\n+\t// should at least scramble the input seed so that, e.g.,\n+\t// {\"\",\"foo\"} and {\"foo\",\"\"} have different hashes.\n+\n+\t// If the hash is bad, then all (8 choose 4) = 70 keys\n+\t// have the same hash. If so, we allocate 70/8 = 8\n+\t// overflow buckets. If the hash is good we don't\n+\t// normally allocate any overflow buckets, and the\n+\t// probability of even one or two overflows goes down rapidly.\n+\t// (There is always 1 allocation of the bucket array. The map\n+\t// header is allocated on the stack.)\n+\tf := func() {\n+\t\t// Make the key type at most 128 bytes. Otherwise,\n+\t\t// we get an allocation per key.\n+\t\ttype key [8]string\n+\t\tm := make(map[key]bool, 70)\n+\n+\t\t// fill m with keys that have 4 \"foo\"s and 4 \"\"s.\n+\t\tfor i := 0; i < 256; i++ {\n+\t\t\tvar k key\n+\t\t\tcnt := 0\n+\t\t\tfor j := uint(0); j < 8; j++ {\n+\t\t\t\tif i>>j&1 != 0 {\n+\t\t\t\t\tk[j] = \"foo\"\n+\t\t\t\t\tcnt++\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tif cnt == 4 {\n+\t\t\t\tm[k] = true\n+\t\t\t}\n+\t\t}\n+\t\tif len(m) != 70 {\n+\t\t\tt.Errorf(\"bad test: (8 choose 4) should be 70, not %d\", len(m))\n+\t\t}\n+\t}\n+\tif n := testing.AllocsPerRun(10, f); n > 6 {\n+\t\tt.Errorf(\"too many allocs %f - hash not balanced\", n)\n+\t}\n+}\n+func TestStructHash(t *testing.T) {\n+\t// See the comment in TestArrayHash.\n+\tf := func() {\n+\t\ttype key struct {\n+\t\t\ta, b, c, d, e, f, g, h string\n+\t\t}\n+\t\tm := make(map[key]bool, 70)\n+\n+\t\t// fill m with keys that have 4 \"foo\"s and 4 \"\"s.\n+\t\tfor i := 0; i < 256; i++ {\n+\t\t\tvar k key\n+\t\t\tcnt := 0\n+\t\t\tif i&1 != 0 {\n+\t\t\t\tk.a = \"foo\"\n+\t\t\t\tcnt++\n+\t\t\t}\n+\t\t\tif i&2 != 0 {\n+\t\t\t\tk.b = \"foo\"\n+\t\t\t\tcnt++\n+\t\t\t}\n+\t\t\tif i&4 != 0 {\n+\t\t\t\tk.c = \"foo\"\n+\t\t\t\tcnt++\n+\t\t\t}\n+\t\t\tif i&8 != 0 {\n+\t\t\t\tk.d = \"foo\"\n+\t\t\t\tcnt++\n+\t\t\t}\n+\t\t\tif i&16 != 0 {\n+\t\t\t\tk.e = \"foo\"\n+\t\t\t\tcnt++\n+\t\t\t}\n+\t\t\tif i&32 != 0 {\n+\t\t\t\tk.f = \"foo\"\n+\t\t\t\tcnt++\n+\t\t\t}\n+\t\t\tif i&64 != 0 {\n+\t\t\t\tk.g = \"foo\"\n+\t\t\t\tcnt++\n+\t\t\t}\n+\t\t\tif i&128 != 0 {\n+\t\t\t\tk.h = \"foo\"\n+\t\t\t\tcnt++\n+\t\t\t}\n+\t\t\tif cnt == 4 {\n+\t\t\t\tm[k] = true\n+\t\t\t}\n+\t\t}\n+\t\tif len(m) != 70 {\n+\t\t\tt.Errorf(\"bad test: (8 choose 4) should be 70, not %d\", len(m))\n+\t\t}\n+\t}\n+\tif n := testing.AllocsPerRun(10, f); n > 6 {\n+\t\tt.Errorf(\"too many allocs %f - hash not balanced\", n)\n+\t}\n+}\n+\n+var sink uint64\n+\n+func BenchmarkAlignedLoad(b *testing.B) {\n+\tvar buf [16]byte\n+\tp := unsafe.Pointer(&buf[0])\n+\tvar s uint64\n+\tfor i := 0; i < b.N; i++ {\n+\t\ts += ReadUnaligned64(p)\n+\t}\n+\tsink = s\n+}\n+\n+func BenchmarkUnalignedLoad(b *testing.B) {\n+\tvar buf [16]byte\n+\tp := unsafe.Pointer(&buf[1])\n+\tvar s uint64\n+\tfor i := 0; i < b.N; i++ {\n+\t\ts += ReadUnaligned64(p)\n+\t}\n+\tsink = s\n+}\n+\n+func TestCollisions(t *testing.T) {\n+\tif testing.Short() {\n+\t\tt.Skip(\"Skipping in short mode\")\n+\t}\n+\tfor i := 0; i < 16; i++ {\n+\t\tfor j := 0; j < 16; j++ {\n+\t\t\tif j == i {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tvar a [16]byte\n+\t\t\tm := make(map[uint16]struct{}, 1<<16)\n+\t\t\tfor n := 0; n < 1<<16; n++ {\n+\t\t\t\ta[i] = byte(n)\n+\t\t\t\ta[j] = byte(n >> 8)\n+\t\t\t\tm[uint16(BytesHash(a[:], 0))] = struct{}{}\n+\t\t\t}\n+\t\t\tif len(m) <= 1<<15 {\n+\t\t\t\tt.Errorf(\"too many collisions i=%d j=%d outputs=%d out of 65536\\n\", i, j, len(m))\n+\t\t\t}\n+\t\t}\n+\t}\n+}"}, {"sha": "0db53f544a5303b1971e8e1e68f400cf6722db7f", "filename": "libgo/go/runtime/heapdump.go", "status": "added", "additions": 594, "deletions": 0, "changes": 594, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fheapdump.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fheapdump.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fheapdump.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,594 @@\n+// Copyright 2014 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Implementation of runtime/debug.WriteHeapDump. Writes all\n+// objects in the heap plus additional info (roots, threads,\n+// finalizers, etc.) to a file.\n+\n+// The format of the dumped file is described at\n+// https://golang.org/s/go15heapdump.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/sys\"\n+\t\"unsafe\"\n+)\n+\n+//go:linkname runtime_debug_WriteHeapDump runtime_debug.WriteHeapDump\n+func runtime_debug_WriteHeapDump(fd uintptr) {\n+\tstopTheWorld(\"write heap dump\")\n+\n+\tsystemstack(func() {\n+\t\twriteheapdump_m(fd)\n+\t})\n+\n+\tstartTheWorld()\n+}\n+\n+const (\n+\tfieldKindEol       = 0\n+\tfieldKindPtr       = 1\n+\tfieldKindIface     = 2\n+\tfieldKindEface     = 3\n+\ttagEOF             = 0\n+\ttagObject          = 1\n+\ttagOtherRoot       = 2\n+\ttagType            = 3\n+\ttagGoroutine       = 4\n+\ttagStackFrame      = 5\n+\ttagParams          = 6\n+\ttagFinalizer       = 7\n+\ttagItab            = 8\n+\ttagOSThread        = 9\n+\ttagMemStats        = 10\n+\ttagQueuedFinalizer = 11\n+\ttagData            = 12\n+\ttagBSS             = 13\n+\ttagDefer           = 14\n+\ttagPanic           = 15\n+\ttagMemProf         = 16\n+\ttagAllocSample     = 17\n+)\n+\n+var dumpfd uintptr // fd to write the dump to.\n+var tmpbuf []byte\n+\n+// buffer of pending write data\n+const (\n+\tbufSize = 4096\n+)\n+\n+var buf [bufSize]byte\n+var nbuf uintptr\n+\n+func dwrite(data unsafe.Pointer, len uintptr) {\n+\tif len == 0 {\n+\t\treturn\n+\t}\n+\tif nbuf+len <= bufSize {\n+\t\tcopy(buf[nbuf:], (*[bufSize]byte)(data)[:len])\n+\t\tnbuf += len\n+\t\treturn\n+\t}\n+\n+\twrite(dumpfd, unsafe.Pointer(&buf), int32(nbuf))\n+\tif len >= bufSize {\n+\t\twrite(dumpfd, data, int32(len))\n+\t\tnbuf = 0\n+\t} else {\n+\t\tcopy(buf[:], (*[bufSize]byte)(data)[:len])\n+\t\tnbuf = len\n+\t}\n+}\n+\n+func dwritebyte(b byte) {\n+\tdwrite(unsafe.Pointer(&b), 1)\n+}\n+\n+func flush() {\n+\twrite(dumpfd, unsafe.Pointer(&buf), int32(nbuf))\n+\tnbuf = 0\n+}\n+\n+// Cache of types that have been serialized already.\n+// We use a type's hash field to pick a bucket.\n+// Inside a bucket, we keep a list of types that\n+// have been serialized so far, most recently used first.\n+// Note: when a bucket overflows we may end up\n+// serializing a type more than once. That's ok.\n+const (\n+\ttypeCacheBuckets = 256\n+\ttypeCacheAssoc   = 4\n+)\n+\n+type typeCacheBucket struct {\n+\tt [typeCacheAssoc]*_type\n+}\n+\n+var typecache [typeCacheBuckets]typeCacheBucket\n+\n+// dump a uint64 in a varint format parseable by encoding/binary\n+func dumpint(v uint64) {\n+\tvar buf [10]byte\n+\tvar n int\n+\tfor v >= 0x80 {\n+\t\tbuf[n] = byte(v | 0x80)\n+\t\tn++\n+\t\tv >>= 7\n+\t}\n+\tbuf[n] = byte(v)\n+\tn++\n+\tdwrite(unsafe.Pointer(&buf), uintptr(n))\n+}\n+\n+func dumpbool(b bool) {\n+\tif b {\n+\t\tdumpint(1)\n+\t} else {\n+\t\tdumpint(0)\n+\t}\n+}\n+\n+// dump varint uint64 length followed by memory contents\n+func dumpmemrange(data unsafe.Pointer, len uintptr) {\n+\tdumpint(uint64(len))\n+\tdwrite(data, len)\n+}\n+\n+func dumpslice(b []byte) {\n+\tdumpint(uint64(len(b)))\n+\tif len(b) > 0 {\n+\t\tdwrite(unsafe.Pointer(&b[0]), uintptr(len(b)))\n+\t}\n+}\n+\n+func dumpstr(s string) {\n+\tsp := stringStructOf(&s)\n+\tdumpmemrange(sp.str, uintptr(sp.len))\n+}\n+\n+// dump information for a type\n+func dumptype(t *_type) {\n+\tif t == nil {\n+\t\treturn\n+\t}\n+\n+\t// If we've definitely serialized the type before,\n+\t// no need to do it again.\n+\tb := &typecache[t.hash&(typeCacheBuckets-1)]\n+\tif t == b.t[0] {\n+\t\treturn\n+\t}\n+\tfor i := 1; i < typeCacheAssoc; i++ {\n+\t\tif t == b.t[i] {\n+\t\t\t// Move-to-front\n+\t\t\tfor j := i; j > 0; j-- {\n+\t\t\t\tb.t[j] = b.t[j-1]\n+\t\t\t}\n+\t\t\tb.t[0] = t\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\t// Might not have been dumped yet. Dump it and\n+\t// remember we did so.\n+\tfor j := typeCacheAssoc - 1; j > 0; j-- {\n+\t\tb.t[j] = b.t[j-1]\n+\t}\n+\tb.t[0] = t\n+\n+\t// dump the type\n+\tdumpint(tagType)\n+\tdumpint(uint64(uintptr(unsafe.Pointer(t))))\n+\tdumpint(uint64(t.size))\n+\tif x := t.uncommontype; x == nil || t.pkgPath == nil || *t.pkgPath == \"\" {\n+\t\tdumpstr(*t.string)\n+\t} else {\n+\t\tpkgpathstr := *t.pkgPath\n+\t\tpkgpath := stringStructOf(&pkgpathstr)\n+\t\tnamestr := *t.name\n+\t\tname := stringStructOf(&namestr)\n+\t\tdumpint(uint64(uintptr(pkgpath.len) + 1 + uintptr(name.len)))\n+\t\tdwrite(pkgpath.str, uintptr(pkgpath.len))\n+\t\tdwritebyte('.')\n+\t\tdwrite(name.str, uintptr(name.len))\n+\t}\n+\tdumpbool(t.kind&kindDirectIface == 0 || t.kind&kindNoPointers == 0)\n+}\n+\n+// dump an object\n+func dumpobj(obj unsafe.Pointer, size uintptr, bv bitvector) {\n+\tdumpbvtypes(&bv, obj)\n+\tdumpint(tagObject)\n+\tdumpint(uint64(uintptr(obj)))\n+\tdumpmemrange(obj, size)\n+\tdumpfields(bv)\n+}\n+\n+func dumpotherroot(description string, to unsafe.Pointer) {\n+\tdumpint(tagOtherRoot)\n+\tdumpstr(description)\n+\tdumpint(uint64(uintptr(to)))\n+}\n+\n+func dumpfinalizer(obj unsafe.Pointer, fn *funcval, ft *functype, ot *ptrtype) {\n+\tdumpint(tagFinalizer)\n+\tdumpint(uint64(uintptr(obj)))\n+\tdumpint(uint64(uintptr(unsafe.Pointer(fn))))\n+\tdumpint(uint64(uintptr(unsafe.Pointer(fn.fn))))\n+\tdumpint(uint64(uintptr(unsafe.Pointer(ft))))\n+\tdumpint(uint64(uintptr(unsafe.Pointer(ot))))\n+}\n+\n+type childInfo struct {\n+\t// Information passed up from the callee frame about\n+\t// the layout of the outargs region.\n+\targoff uintptr   // where the arguments start in the frame\n+\targlen uintptr   // size of args region\n+\targs   bitvector // if args.n >= 0, pointer map of args region\n+\tsp     *uint8    // callee sp\n+\tdepth  uintptr   // depth in call stack (0 == most recent)\n+}\n+\n+// dump kinds & offsets of interesting fields in bv\n+func dumpbv(cbv *bitvector, offset uintptr) {\n+\tbv := gobv(*cbv)\n+\tfor i := uintptr(0); i < bv.n; i++ {\n+\t\tif bv.bytedata[i/8]>>(i%8)&1 == 1 {\n+\t\t\tdumpint(fieldKindPtr)\n+\t\t\tdumpint(uint64(offset + i*sys.PtrSize))\n+\t\t}\n+\t}\n+}\n+\n+func dumpgoroutine(gp *g) {\n+\tsp := gp.syscallsp\n+\n+\tdumpint(tagGoroutine)\n+\tdumpint(uint64(uintptr(unsafe.Pointer(gp))))\n+\tdumpint(uint64(sp))\n+\tdumpint(uint64(gp.goid))\n+\tdumpint(uint64(gp.gopc))\n+\tdumpint(uint64(readgstatus(gp)))\n+\tdumpbool(isSystemGoroutine(gp))\n+\tdumpbool(false) // isbackground\n+\tdumpint(uint64(gp.waitsince))\n+\tdumpstr(gp.waitreason)\n+\tdumpint(0)\n+\tdumpint(uint64(uintptr(unsafe.Pointer(gp.m))))\n+\tdumpint(uint64(uintptr(unsafe.Pointer(gp._defer))))\n+\tdumpint(uint64(uintptr(unsafe.Pointer(gp._panic))))\n+\n+\t// dump defer & panic records\n+\tfor d := gp._defer; d != nil; d = d.link {\n+\t\tdumpint(tagDefer)\n+\t\tdumpint(uint64(uintptr(unsafe.Pointer(d))))\n+\t\tdumpint(uint64(uintptr(unsafe.Pointer(gp))))\n+\t\tdumpint(0)\n+\t\tdumpint(0)\n+\t\tdumpint(uint64(uintptr(unsafe.Pointer(d.pfn))))\n+\t\tdumpint(0)\n+\t\tdumpint(uint64(uintptr(unsafe.Pointer(d.link))))\n+\t}\n+\tfor p := gp._panic; p != nil; p = p.link {\n+\t\tdumpint(tagPanic)\n+\t\tdumpint(uint64(uintptr(unsafe.Pointer(p))))\n+\t\tdumpint(uint64(uintptr(unsafe.Pointer(gp))))\n+\t\teface := efaceOf(&p.arg)\n+\t\tdumpint(uint64(uintptr(unsafe.Pointer(eface._type))))\n+\t\tdumpint(uint64(uintptr(unsafe.Pointer(eface.data))))\n+\t\tdumpint(0) // was p->defer, no longer recorded\n+\t\tdumpint(uint64(uintptr(unsafe.Pointer(p.link))))\n+\t}\n+}\n+\n+func dumpgs() {\n+\t// goroutines & stacks\n+\tfor i := 0; uintptr(i) < allglen; i++ {\n+\t\tgp := allgs[i]\n+\t\tstatus := readgstatus(gp) // The world is stopped so gp will not be in a scan state.\n+\t\tswitch status {\n+\t\tdefault:\n+\t\t\tprint(\"runtime: unexpected G.status \", hex(status), \"\\n\")\n+\t\t\tthrow(\"dumpgs in STW - bad status\")\n+\t\tcase _Gdead:\n+\t\t\t// ok\n+\t\tcase _Grunnable,\n+\t\t\t_Gsyscall,\n+\t\t\t_Gwaiting:\n+\t\t\tdumpgoroutine(gp)\n+\t\t}\n+\t}\n+}\n+\n+func finq_callback(fn *funcval, obj unsafe.Pointer, ft *functype, ot *ptrtype) {\n+\tdumpint(tagQueuedFinalizer)\n+\tdumpint(uint64(uintptr(obj)))\n+\tdumpint(uint64(uintptr(unsafe.Pointer(fn))))\n+\tdumpint(uint64(uintptr(unsafe.Pointer(fn.fn))))\n+\tdumpint(uint64(uintptr(unsafe.Pointer(ft))))\n+\tdumpint(uint64(uintptr(unsafe.Pointer(ot))))\n+}\n+\n+func dumproots() {\n+\t// MSpan.types\n+\tfor _, s := range mheap_.allspans {\n+\t\tif s.state == _MSpanInUse {\n+\t\t\t// Finalizers\n+\t\t\tfor sp := s.specials; sp != nil; sp = sp.next {\n+\t\t\t\tif sp.kind != _KindSpecialFinalizer {\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\tspf := (*specialfinalizer)(unsafe.Pointer(sp))\n+\t\t\t\tp := unsafe.Pointer(s.base() + uintptr(spf.special.offset))\n+\t\t\t\tdumpfinalizer(p, spf.fn, spf.ft, spf.ot)\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Finalizer queue\n+\titerate_finq(finq_callback)\n+}\n+\n+// Bit vector of free marks.\n+// Needs to be as big as the largest number of objects per span.\n+var freemark [_PageSize / 8]bool\n+\n+func dumpobjs() {\n+\tfor _, s := range mheap_.allspans {\n+\t\tif s.state != _MSpanInUse {\n+\t\t\tcontinue\n+\t\t}\n+\t\tp := s.base()\n+\t\tsize := s.elemsize\n+\t\tn := (s.npages << _PageShift) / size\n+\t\tif n > uintptr(len(freemark)) {\n+\t\t\tthrow(\"freemark array doesn't have enough entries\")\n+\t\t}\n+\n+\t\tfor freeIndex := uintptr(0); freeIndex < s.nelems; freeIndex++ {\n+\t\t\tif s.isFree(freeIndex) {\n+\t\t\t\tfreemark[freeIndex] = true\n+\t\t\t}\n+\t\t}\n+\n+\t\tfor j := uintptr(0); j < n; j, p = j+1, p+size {\n+\t\t\tif freemark[j] {\n+\t\t\t\tfreemark[j] = false\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tdumpobj(unsafe.Pointer(p), size, makeheapobjbv(p, size))\n+\t\t}\n+\t}\n+}\n+\n+func dumpparams() {\n+\tdumpint(tagParams)\n+\tx := uintptr(1)\n+\tif *(*byte)(unsafe.Pointer(&x)) == 1 {\n+\t\tdumpbool(false) // little-endian ptrs\n+\t} else {\n+\t\tdumpbool(true) // big-endian ptrs\n+\t}\n+\tdumpint(sys.PtrSize)\n+\tdumpint(uint64(mheap_.arena_start))\n+\tdumpint(uint64(mheap_.arena_used))\n+\tdumpstr(sys.GOARCH)\n+\tdumpstr(sys.Goexperiment)\n+\tdumpint(uint64(ncpu))\n+}\n+\n+func dumpms() {\n+\tfor mp := allm; mp != nil; mp = mp.alllink {\n+\t\tdumpint(tagOSThread)\n+\t\tdumpint(uint64(uintptr(unsafe.Pointer(mp))))\n+\t\tdumpint(uint64(mp.id))\n+\t\tdumpint(mp.procid)\n+\t}\n+}\n+\n+func dumpmemstats() {\n+\tdumpint(tagMemStats)\n+\tdumpint(memstats.alloc)\n+\tdumpint(memstats.total_alloc)\n+\tdumpint(memstats.sys)\n+\tdumpint(memstats.nlookup)\n+\tdumpint(memstats.nmalloc)\n+\tdumpint(memstats.nfree)\n+\tdumpint(memstats.heap_alloc)\n+\tdumpint(memstats.heap_sys)\n+\tdumpint(memstats.heap_idle)\n+\tdumpint(memstats.heap_inuse)\n+\tdumpint(memstats.heap_released)\n+\tdumpint(memstats.heap_objects)\n+\tdumpint(memstats.stacks_inuse)\n+\tdumpint(memstats.stacks_sys)\n+\tdumpint(memstats.mspan_inuse)\n+\tdumpint(memstats.mspan_sys)\n+\tdumpint(memstats.mcache_inuse)\n+\tdumpint(memstats.mcache_sys)\n+\tdumpint(memstats.buckhash_sys)\n+\tdumpint(memstats.gc_sys)\n+\tdumpint(memstats.other_sys)\n+\tdumpint(memstats.next_gc)\n+\tdumpint(memstats.last_gc)\n+\tdumpint(memstats.pause_total_ns)\n+\tfor i := 0; i < 256; i++ {\n+\t\tdumpint(memstats.pause_ns[i])\n+\t}\n+\tdumpint(uint64(memstats.numgc))\n+}\n+\n+func dumpmemprof_callback(b *bucket, nstk uintptr, pstk *location, size, allocs, frees uintptr) {\n+\tstk := (*[100000]location)(unsafe.Pointer(pstk))\n+\tdumpint(tagMemProf)\n+\tdumpint(uint64(uintptr(unsafe.Pointer(b))))\n+\tdumpint(uint64(size))\n+\tdumpint(uint64(nstk))\n+\tfor i := uintptr(0); i < nstk; i++ {\n+\t\tpc := stk[i].pc\n+\t\tfn := stk[i].function\n+\t\tfile := stk[i].filename\n+\t\tline := stk[i].lineno\n+\t\tif fn == \"\" {\n+\t\t\tvar buf [64]byte\n+\t\t\tn := len(buf)\n+\t\t\tn--\n+\t\t\tbuf[n] = ')'\n+\t\t\tif pc == 0 {\n+\t\t\t\tn--\n+\t\t\t\tbuf[n] = '0'\n+\t\t\t} else {\n+\t\t\t\tfor pc > 0 {\n+\t\t\t\t\tn--\n+\t\t\t\t\tbuf[n] = \"0123456789abcdef\"[pc&15]\n+\t\t\t\t\tpc >>= 4\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tn--\n+\t\t\tbuf[n] = 'x'\n+\t\t\tn--\n+\t\t\tbuf[n] = '0'\n+\t\t\tn--\n+\t\t\tbuf[n] = '('\n+\t\t\tdumpslice(buf[n:])\n+\t\t\tdumpstr(\"?\")\n+\t\t\tdumpint(0)\n+\t\t} else {\n+\t\t\tdumpstr(fn)\n+\t\t\tdumpstr(file)\n+\t\t\tdumpint(uint64(line))\n+\t\t}\n+\t}\n+\tdumpint(uint64(allocs))\n+\tdumpint(uint64(frees))\n+}\n+\n+func dumpmemprof() {\n+\titerate_memprof(dumpmemprof_callback)\n+\tfor _, s := range mheap_.allspans {\n+\t\tif s.state != _MSpanInUse {\n+\t\t\tcontinue\n+\t\t}\n+\t\tfor sp := s.specials; sp != nil; sp = sp.next {\n+\t\t\tif sp.kind != _KindSpecialProfile {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tspp := (*specialprofile)(unsafe.Pointer(sp))\n+\t\t\tp := s.base() + uintptr(spp.special.offset)\n+\t\t\tdumpint(tagAllocSample)\n+\t\t\tdumpint(uint64(p))\n+\t\t\tdumpint(uint64(uintptr(unsafe.Pointer(spp.b))))\n+\t\t}\n+\t}\n+}\n+\n+var dumphdr = []byte(\"go1.7 heap dump\\n\")\n+\n+func mdump() {\n+\t// make sure we're done sweeping\n+\tfor _, s := range mheap_.allspans {\n+\t\tif s.state == _MSpanInUse {\n+\t\t\ts.ensureSwept()\n+\t\t}\n+\t}\n+\tmemclrNoHeapPointers(unsafe.Pointer(&typecache), unsafe.Sizeof(typecache))\n+\tdwrite(unsafe.Pointer(&dumphdr[0]), uintptr(len(dumphdr)))\n+\tdumpparams()\n+\tdumpobjs()\n+\tdumpgs()\n+\tdumpms()\n+\tdumproots()\n+\tdumpmemstats()\n+\tdumpmemprof()\n+\tdumpint(tagEOF)\n+\tflush()\n+}\n+\n+func writeheapdump_m(fd uintptr) {\n+\t_g_ := getg()\n+\tcasgstatus(_g_.m.curg, _Grunning, _Gwaiting)\n+\t_g_.waitreason = \"dumping heap\"\n+\n+\t// Update stats so we can dump them.\n+\t// As a side effect, flushes all the MCaches so the MSpan.freelist\n+\t// lists contain all the free objects.\n+\tupdatememstats(nil)\n+\n+\t// Set dump file.\n+\tdumpfd = fd\n+\n+\t// Call dump routine.\n+\tmdump()\n+\n+\t// Reset dump file.\n+\tdumpfd = 0\n+\tif tmpbuf != nil {\n+\t\tsysFree(unsafe.Pointer(&tmpbuf[0]), uintptr(len(tmpbuf)), &memstats.other_sys)\n+\t\ttmpbuf = nil\n+\t}\n+\n+\tcasgstatus(_g_.m.curg, _Gwaiting, _Grunning)\n+}\n+\n+// dumpint() the kind & offset of each field in an object.\n+func dumpfields(bv bitvector) {\n+\tdumpbv(&bv, 0)\n+\tdumpint(fieldKindEol)\n+}\n+\n+// The heap dump reader needs to be able to disambiguate\n+// Eface entries. So it needs to know every type that might\n+// appear in such an entry. The following routine accomplishes that.\n+// TODO(rsc, khr): Delete - no longer possible.\n+\n+// Dump all the types that appear in the type field of\n+// any Eface described by this bit vector.\n+func dumpbvtypes(bv *bitvector, base unsafe.Pointer) {\n+}\n+\n+func makeheapobjbv(p uintptr, size uintptr) bitvector {\n+\t// Extend the temp buffer if necessary.\n+\tnptr := size / sys.PtrSize\n+\tif uintptr(len(tmpbuf)) < nptr/8+1 {\n+\t\tif tmpbuf != nil {\n+\t\t\tsysFree(unsafe.Pointer(&tmpbuf[0]), uintptr(len(tmpbuf)), &memstats.other_sys)\n+\t\t}\n+\t\tn := nptr/8 + 1\n+\t\tp := sysAlloc(n, &memstats.other_sys)\n+\t\tif p == nil {\n+\t\t\tthrow(\"heapdump: out of memory\")\n+\t\t}\n+\t\ttmpbuf = (*[1 << 30]byte)(p)[:n]\n+\t}\n+\t// Convert heap bitmap to pointer bitmap.\n+\tfor i := uintptr(0); i < nptr/8+1; i++ {\n+\t\ttmpbuf[i] = 0\n+\t}\n+\ti := uintptr(0)\n+\thbits := heapBitsForAddr(p)\n+\tfor ; i < nptr; i++ {\n+\t\tif i != 1 && !hbits.morePointers() {\n+\t\t\tbreak // end of object\n+\t\t}\n+\t\tif hbits.isPointer() {\n+\t\t\ttmpbuf[i/8] |= 1 << (i % 8)\n+\t\t}\n+\t\thbits = hbits.next()\n+\t}\n+\treturn bitvector{int32(i), &tmpbuf[0]}\n+}\n+\n+type gobitvector struct {\n+\tn        uintptr\n+\tbytedata []uint8\n+}\n+\n+func gobv(bv bitvector) gobitvector {\n+\treturn gobitvector{\n+\t\tuintptr(bv.n),\n+\t\t(*[1 << 30]byte)(unsafe.Pointer(bv.bytedata))[:(bv.n+7)/8],\n+\t}\n+}"}, {"sha": "3744a4f9700582d25a8973f42621a3c5a154e29a", "filename": "libgo/go/runtime/iface_test.go", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fiface_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fiface_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fiface_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -223,6 +223,10 @@ func BenchmarkAssertE2E2Blank(b *testing.B) {\n }\n \n func TestNonEscapingConvT2E(t *testing.T) {\n+\tif runtime.Compiler == \"gccgo\" {\n+\t\tt.Skip(\"does not work on gccgo without better escape analysis\")\n+\t}\n+\n \tm := make(map[interface{}]bool)\n \tm[42] = true\n \tif !m[42] {\n@@ -243,6 +247,10 @@ func TestNonEscapingConvT2E(t *testing.T) {\n }\n \n func TestNonEscapingConvT2I(t *testing.T) {\n+\tif runtime.Compiler == \"gccgo\" {\n+\t\tt.Skip(\"does not work on gccgo without better escape analysis\")\n+\t}\n+\n \tm := make(map[I1]bool)\n \tm[TM(42)] = true\n \tif !m[TM(42)] {"}, {"sha": "9877bc35606fea533d47129cce105550b6998040", "filename": "libgo/go/runtime/lock_futex.go", "status": "modified", "additions": 4, "deletions": 7, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Flock_futex.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Flock_futex.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Flock_futex.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -198,13 +198,10 @@ func notetsleep_internal(n *note, ns int64) bool {\n }\n \n func notetsleep(n *note, ns int64) bool {\n-\t// Currently OK to sleep in non-g0 for gccgo.  It happens in\n-\t// stoptheworld because our version of systemstack does not\n-\t// change to g0.\n-\t// gp := getg()\n-\t// if gp != gp.m.g0 && gp.m.preemptoff != \"\" {\n-\t//\tthrow(\"notetsleep not on g0\")\n-\t// }\n+\tgp := getg()\n+\tif gp != gp.m.g0 && gp.m.preemptoff != \"\" {\n+\t\tthrow(\"notetsleep not on g0\")\n+\t}\n \n \treturn notetsleep_internal(n, ns)\n }"}, {"sha": "57fee1985e3a41a6a78bd86b2d7655b2dac36fac", "filename": "libgo/go/runtime/lock_sema.go", "status": "modified", "additions": 4, "deletions": 9, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Flock_sema.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Flock_sema.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Flock_sema.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin nacl netbsd openbsd plan9 solaris windows\n+// +build aix darwin nacl netbsd openbsd plan9 solaris windows\n \n package runtime\n \n@@ -251,14 +251,9 @@ func notetsleep_internal(n *note, ns int64, gp *g, deadline int64) bool {\n \n func notetsleep(n *note, ns int64) bool {\n \tgp := getg()\n-\n-\t// Currently OK to sleep in non-g0 for gccgo.  It happens in\n-\t// stoptheworld because our version of systemstack does not\n-\t// change to g0.\n-\t// if gp != gp.m.g0 && gp.m.preemptoff != \"\" {\n-\t//\tthrow(\"notetsleep not on g0\")\n-\t// }\n-\n+\tif gp != gp.m.g0 && gp.m.preemptoff != \"\" {\n+\t\tthrow(\"notetsleep not on g0\")\n+\t}\n \tsemacreate(gp.m)\n \treturn notetsleep_internal(n, ns, nil, 0)\n }"}, {"sha": "ed2578233fc02692a22b7c0f04a85fc04acf316b", "filename": "libgo/go/runtime/malloc.go", "status": "added", "additions": 998, "deletions": 0, "changes": 998, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmalloc.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmalloc.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmalloc.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,998 @@\n+// Copyright 2014 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Memory allocator.\n+//\n+// This was originally based on tcmalloc, but has diverged quite a bit.\n+// http://goog-perftools.sourceforge.net/doc/tcmalloc.html\n+\n+// The main allocator works in runs of pages.\n+// Small allocation sizes (up to and including 32 kB) are\n+// rounded to one of about 70 size classes, each of which\n+// has its own free set of objects of exactly that size.\n+// Any free page of memory can be split into a set of objects\n+// of one size class, which are then managed using a free bitmap.\n+//\n+// The allocator's data structures are:\n+//\n+//\tfixalloc: a free-list allocator for fixed-size off-heap objects,\n+//\t\tused to manage storage used by the allocator.\n+//\tmheap: the malloc heap, managed at page (8192-byte) granularity.\n+//\tmspan: a run of pages managed by the mheap.\n+//\tmcentral: collects all spans of a given size class.\n+//\tmcache: a per-P cache of mspans with free space.\n+//\tmstats: allocation statistics.\n+//\n+// Allocating a small object proceeds up a hierarchy of caches:\n+//\n+//\t1. Round the size up to one of the small size classes\n+//\t   and look in the corresponding mspan in this P's mcache.\n+//\t   Scan the mspan's free bitmap to find a free slot.\n+//\t   If there is a free slot, allocate it.\n+//\t   This can all be done without acquiring a lock.\n+//\n+//\t2. If the mspan has no free slots, obtain a new mspan\n+//\t   from the mcentral's list of mspans of the required size\n+//\t   class that have free space.\n+//\t   Obtaining a whole span amortizes the cost of locking\n+//\t   the mcentral.\n+//\n+//\t3. If the mcentral's mspan list is empty, obtain a run\n+//\t   of pages from the mheap to use for the mspan.\n+//\n+//\t4. If the mheap is empty or has no page runs large enough,\n+//\t   allocate a new group of pages (at least 1MB) from the\n+//\t   operating system. Allocating a large run of pages\n+//\t   amortizes the cost of talking to the operating system.\n+//\n+// Sweeping an mspan and freeing objects on it proceeds up a similar\n+// hierarchy:\n+//\n+//\t1. If the mspan is being swept in response to allocation, it\n+//\t   is returned to the mcache to satisfy the allocation.\n+//\n+//\t2. Otherwise, if the mspan still has allocated objects in it,\n+//\t   it is placed on the mcentral free list for the mspan's size\n+//\t   class.\n+//\n+//\t3. Otherwise, if all objects in the mspan are free, the mspan\n+//\t   is now \"idle\", so it is returned to the mheap and no longer\n+//\t   has a size class.\n+//\t   This may coalesce it with adjacent idle mspans.\n+//\n+//\t4. If an mspan remains idle for long enough, return its pages\n+//\t   to the operating system.\n+//\n+// Allocating and freeing a large object uses the mheap\n+// directly, bypassing the mcache and mcentral.\n+//\n+// Free object slots in an mspan are zeroed only if mspan.needzero is\n+// false. If needzero is true, objects are zeroed as they are\n+// allocated. There are various benefits to delaying zeroing this way:\n+//\n+//\t1. Stack frame allocation can avoid zeroing altogether.\n+//\n+//\t2. It exhibits better temporal locality, since the program is\n+//\t   probably about to write to the memory.\n+//\n+//\t3. We don't zero pages that never get reused.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/sys\"\n+\t\"unsafe\"\n+)\n+\n+// C function to get the end of the program's memory.\n+func getEnd() uintptr\n+\n+// For gccgo, use go:linkname to rename compiler-called functions to\n+// themselves, so that the compiler will export them.\n+//\n+//go:linkname newobject runtime.newobject\n+\n+// Functions called by C code.\n+//go:linkname mallocgc runtime.mallocgc\n+\n+const (\n+\tdebugMalloc = false\n+\n+\tmaxTinySize   = _TinySize\n+\ttinySizeClass = _TinySizeClass\n+\tmaxSmallSize  = _MaxSmallSize\n+\n+\tpageShift = _PageShift\n+\tpageSize  = _PageSize\n+\tpageMask  = _PageMask\n+\t// By construction, single page spans of the smallest object class\n+\t// have the most objects per span.\n+\tmaxObjsPerSpan = pageSize / 8\n+\n+\tmSpanInUse = _MSpanInUse\n+\n+\tconcurrentSweep = _ConcurrentSweep\n+\n+\t_PageSize = 1 << _PageShift\n+\t_PageMask = _PageSize - 1\n+\n+\t// _64bit = 1 on 64-bit systems, 0 on 32-bit systems\n+\t_64bit = 1 << (^uintptr(0) >> 63) / 2\n+\n+\t// Tiny allocator parameters, see \"Tiny allocator\" comment in malloc.go.\n+\t_TinySize      = 16\n+\t_TinySizeClass = 2\n+\n+\t_FixAllocChunk  = 16 << 10               // Chunk size for FixAlloc\n+\t_MaxMHeapList   = 1 << (20 - _PageShift) // Maximum page length for fixed-size list in MHeap.\n+\t_HeapAllocChunk = 1 << 20                // Chunk size for heap growth\n+\n+\t// Per-P, per order stack segment cache size.\n+\t_StackCacheSize = 32 * 1024\n+\n+\t// Number of orders that get caching. Order 0 is FixedStack\n+\t// and each successive order is twice as large.\n+\t// We want to cache 2KB, 4KB, 8KB, and 16KB stacks. Larger stacks\n+\t// will be allocated directly.\n+\t// Since FixedStack is different on different systems, we\n+\t// must vary NumStackOrders to keep the same maximum cached size.\n+\t//   OS               | FixedStack | NumStackOrders\n+\t//   -----------------+------------+---------------\n+\t//   linux/darwin/bsd | 2KB        | 4\n+\t//   windows/32       | 4KB        | 3\n+\t//   windows/64       | 8KB        | 2\n+\t//   plan9            | 4KB        | 3\n+\t_NumStackOrders = 4 - sys.PtrSize/4*sys.GoosWindows - 1*sys.GoosPlan9\n+\n+\t// Number of bits in page to span calculations (4k pages).\n+\t// On Windows 64-bit we limit the arena to 32GB or 35 bits.\n+\t// Windows counts memory used by page table into committed memory\n+\t// of the process, so we can't reserve too much memory.\n+\t// See https://golang.org/issue/5402 and https://golang.org/issue/5236.\n+\t// On other 64-bit platforms, we limit the arena to 512GB, or 39 bits.\n+\t// On 32-bit, we don't bother limiting anything, so we use the full 32-bit address.\n+\t// The only exception is mips32 which only has access to low 2GB of virtual memory.\n+\t// On Darwin/arm64, we cannot reserve more than ~5GB of virtual memory,\n+\t// but as most devices have less than 4GB of physical memory anyway, we\n+\t// try to be conservative here, and only ask for a 2GB heap.\n+\t_MHeapMap_TotalBits = (_64bit*sys.GoosWindows)*35 + (_64bit*(1-sys.GoosWindows)*(1-sys.GoosDarwin*sys.GoarchArm64))*39 + sys.GoosDarwin*sys.GoarchArm64*31 + (1-_64bit)*(32-(sys.GoarchMips+sys.GoarchMipsle))\n+\t_MHeapMap_Bits      = _MHeapMap_TotalBits - _PageShift\n+\n+\t_MaxMem = uintptr(1<<_MHeapMap_TotalBits - 1)\n+\n+\t// Max number of threads to run garbage collection.\n+\t// 2, 3, and 4 are all plausible maximums depending\n+\t// on the hardware details of the machine. The garbage\n+\t// collector scales well to 32 cpus.\n+\t_MaxGcproc = 32\n+\n+\t_MaxArena32 = 1<<32 - 1\n+\n+\t// minLegalPointer is the smallest possible legal pointer.\n+\t// This is the smallest possible architectural page size,\n+\t// since we assume that the first page is never mapped.\n+\t//\n+\t// This should agree with minZeroPage in the compiler.\n+\tminLegalPointer uintptr = 4096\n+)\n+\n+// physPageSize is the size in bytes of the OS's physical pages.\n+// Mapping and unmapping operations must be done at multiples of\n+// physPageSize.\n+//\n+// This must be set by the OS init code (typically in osinit) before\n+// mallocinit.\n+var physPageSize uintptr\n+\n+// OS-defined helpers:\n+//\n+// sysAlloc obtains a large chunk of zeroed memory from the\n+// operating system, typically on the order of a hundred kilobytes\n+// or a megabyte.\n+// NOTE: sysAlloc returns OS-aligned memory, but the heap allocator\n+// may use larger alignment, so the caller must be careful to realign the\n+// memory obtained by sysAlloc.\n+//\n+// SysUnused notifies the operating system that the contents\n+// of the memory region are no longer needed and can be reused\n+// for other purposes.\n+// SysUsed notifies the operating system that the contents\n+// of the memory region are needed again.\n+//\n+// SysFree returns it unconditionally; this is only used if\n+// an out-of-memory error has been detected midway through\n+// an allocation. It is okay if SysFree is a no-op.\n+//\n+// SysReserve reserves address space without allocating memory.\n+// If the pointer passed to it is non-nil, the caller wants the\n+// reservation there, but SysReserve can still choose another\n+// location if that one is unavailable. On some systems and in some\n+// cases SysReserve will simply check that the address space is\n+// available and not actually reserve it. If SysReserve returns\n+// non-nil, it sets *reserved to true if the address space is\n+// reserved, false if it has merely been checked.\n+// NOTE: SysReserve returns OS-aligned memory, but the heap allocator\n+// may use larger alignment, so the caller must be careful to realign the\n+// memory obtained by sysAlloc.\n+//\n+// SysMap maps previously reserved address space for use.\n+// The reserved argument is true if the address space was really\n+// reserved, not merely checked.\n+//\n+// SysFault marks a (already sysAlloc'd) region to fault\n+// if accessed. Used only for debugging the runtime.\n+\n+func mallocinit() {\n+\tif class_to_size[_TinySizeClass] != _TinySize {\n+\t\tthrow(\"bad TinySizeClass\")\n+\t}\n+\n+\t// Not used for gccgo.\n+\t// testdefersizes()\n+\n+\t// Copy class sizes out for statistics table.\n+\tfor i := range class_to_size {\n+\t\tmemstats.by_size[i].size = uint32(class_to_size[i])\n+\t}\n+\n+\t// Check physPageSize.\n+\tif physPageSize == 0 {\n+\t\t// The OS init code failed to fetch the physical page size.\n+\t\tthrow(\"failed to get system page size\")\n+\t}\n+\tif physPageSize < minPhysPageSize {\n+\t\tprint(\"system page size (\", physPageSize, \") is smaller than minimum page size (\", minPhysPageSize, \")\\n\")\n+\t\tthrow(\"bad system page size\")\n+\t}\n+\tif physPageSize&(physPageSize-1) != 0 {\n+\t\tprint(\"system page size (\", physPageSize, \") must be a power of 2\\n\")\n+\t\tthrow(\"bad system page size\")\n+\t}\n+\n+\tvar p, bitmapSize, spansSize, pSize, limit uintptr\n+\tvar reserved bool\n+\n+\t// limit = runtime.memlimit();\n+\t// See https://golang.org/issue/5049\n+\t// TODO(rsc): Fix after 1.1.\n+\tlimit = 0\n+\n+\t// Set up the allocation arena, a contiguous area of memory where\n+\t// allocated data will be found. The arena begins with a bitmap large\n+\t// enough to hold 2 bits per allocated word.\n+\tif sys.PtrSize == 8 && (limit == 0 || limit > 1<<30) {\n+\t\t// On a 64-bit machine, allocate from a single contiguous reservation.\n+\t\t// 512 GB (MaxMem) should be big enough for now.\n+\t\t//\n+\t\t// The code will work with the reservation at any address, but ask\n+\t\t// SysReserve to use 0x0000XXc000000000 if possible (XX=00...7f).\n+\t\t// Allocating a 512 GB region takes away 39 bits, and the amd64\n+\t\t// doesn't let us choose the top 17 bits, so that leaves the 9 bits\n+\t\t// in the middle of 0x00c0 for us to choose. Choosing 0x00c0 means\n+\t\t// that the valid memory addresses will begin 0x00c0, 0x00c1, ..., 0x00df.\n+\t\t// In little-endian, that's c0 00, c1 00, ..., df 00. None of those are valid\n+\t\t// UTF-8 sequences, and they are otherwise as far away from\n+\t\t// ff (likely a common byte) as possible. If that fails, we try other 0xXXc0\n+\t\t// addresses. An earlier attempt to use 0x11f8 caused out of memory errors\n+\t\t// on OS X during thread allocations.  0x00c0 causes conflicts with\n+\t\t// AddressSanitizer which reserves all memory up to 0x0100.\n+\t\t// These choices are both for debuggability and to reduce the\n+\t\t// odds of a conservative garbage collector (as is still used in gccgo)\n+\t\t// not collecting memory because some non-pointer block of memory\n+\t\t// had a bit pattern that matched a memory address.\n+\t\t//\n+\t\t// Actually we reserve 544 GB (because the bitmap ends up being 32 GB)\n+\t\t// but it hardly matters: e0 00 is not valid UTF-8 either.\n+\t\t//\n+\t\t// If this fails we fall back to the 32 bit memory mechanism\n+\t\t//\n+\t\t// However, on arm64, we ignore all this advice above and slam the\n+\t\t// allocation at 0x40 << 32 because when using 4k pages with 3-level\n+\t\t// translation buffers, the user address space is limited to 39 bits\n+\t\t// On darwin/arm64, the address space is even smaller.\n+\t\tarenaSize := round(_MaxMem, _PageSize)\n+\t\tbitmapSize = arenaSize / (sys.PtrSize * 8 / 2)\n+\t\tspansSize = arenaSize / _PageSize * sys.PtrSize\n+\t\tspansSize = round(spansSize, _PageSize)\n+\t\tfor i := 0; i <= 0x7f; i++ {\n+\t\t\tswitch {\n+\t\t\tcase GOARCH == \"arm64\" && GOOS == \"darwin\":\n+\t\t\t\tp = uintptr(i)<<40 | uintptrMask&(0x0013<<28)\n+\t\t\tcase GOARCH == \"arm64\":\n+\t\t\t\tp = uintptr(i)<<40 | uintptrMask&(0x0040<<32)\n+\t\t\tdefault:\n+\t\t\t\tp = uintptr(i)<<40 | uintptrMask&(0x00c0<<32)\n+\t\t\t}\n+\t\t\tpSize = bitmapSize + spansSize + arenaSize + _PageSize\n+\t\t\tp = uintptr(sysReserve(unsafe.Pointer(p), pSize, &reserved))\n+\t\t\tif p != 0 {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif p == 0 {\n+\t\t// On a 32-bit machine, we can't typically get away\n+\t\t// with a giant virtual address space reservation.\n+\t\t// Instead we map the memory information bitmap\n+\t\t// immediately after the data segment, large enough\n+\t\t// to handle the entire 4GB address space (256 MB),\n+\t\t// along with a reservation for an initial arena.\n+\t\t// When that gets used up, we'll start asking the kernel\n+\t\t// for any memory anywhere.\n+\n+\t\t// If we fail to allocate, try again with a smaller arena.\n+\t\t// This is necessary on Android L where we share a process\n+\t\t// with ART, which reserves virtual memory aggressively.\n+\t\t// In the worst case, fall back to a 0-sized initial arena,\n+\t\t// in the hope that subsequent reservations will succeed.\n+\t\tarenaSizes := [...]uintptr{\n+\t\t\t512 << 20,\n+\t\t\t256 << 20,\n+\t\t\t128 << 20,\n+\t\t\t0,\n+\t\t}\n+\n+\t\tfor _, arenaSize := range &arenaSizes {\n+\t\t\tbitmapSize = (_MaxArena32 + 1) / (sys.PtrSize * 8 / 2)\n+\t\t\tspansSize = (_MaxArena32 + 1) / _PageSize * sys.PtrSize\n+\t\t\tif limit > 0 && arenaSize+bitmapSize+spansSize > limit {\n+\t\t\t\tbitmapSize = (limit / 9) &^ ((1 << _PageShift) - 1)\n+\t\t\t\tarenaSize = bitmapSize * 8\n+\t\t\t\tspansSize = arenaSize / _PageSize * sys.PtrSize\n+\t\t\t}\n+\t\t\tspansSize = round(spansSize, _PageSize)\n+\n+\t\t\t// SysReserve treats the address we ask for, end, as a hint,\n+\t\t\t// not as an absolute requirement. If we ask for the end\n+\t\t\t// of the data segment but the operating system requires\n+\t\t\t// a little more space before we can start allocating, it will\n+\t\t\t// give out a slightly higher pointer. Except QEMU, which\n+\t\t\t// is buggy, as usual: it won't adjust the pointer upward.\n+\t\t\t// So adjust it upward a little bit ourselves: 1/4 MB to get\n+\t\t\t// away from the running binary image and then round up\n+\t\t\t// to a MB boundary.\n+\t\t\tp = round(getEnd()+(1<<18), 1<<20)\n+\t\t\tpSize = bitmapSize + spansSize + arenaSize + _PageSize\n+\t\t\tp = uintptr(sysReserve(unsafe.Pointer(p), pSize, &reserved))\n+\t\t\tif p != 0 {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t\tif p == 0 {\n+\t\t\tthrow(\"runtime: cannot reserve arena virtual address space\")\n+\t\t}\n+\t}\n+\n+\t// PageSize can be larger than OS definition of page size,\n+\t// so SysReserve can give us a PageSize-unaligned pointer.\n+\t// To overcome this we ask for PageSize more and round up the pointer.\n+\tp1 := round(p, _PageSize)\n+\n+\tspansStart := p1\n+\tmheap_.bitmap = p1 + spansSize + bitmapSize\n+\tif sys.PtrSize == 4 {\n+\t\t// Set arena_start such that we can accept memory\n+\t\t// reservations located anywhere in the 4GB virtual space.\n+\t\tmheap_.arena_start = 0\n+\t} else {\n+\t\tmheap_.arena_start = p1 + (spansSize + bitmapSize)\n+\t}\n+\tmheap_.arena_end = p + pSize\n+\tmheap_.arena_used = p1 + (spansSize + bitmapSize)\n+\tmheap_.arena_reserved = reserved\n+\n+\tif mheap_.arena_start&(_PageSize-1) != 0 {\n+\t\tprintln(\"bad pagesize\", hex(p), hex(p1), hex(spansSize), hex(bitmapSize), hex(_PageSize), \"start\", hex(mheap_.arena_start))\n+\t\tthrow(\"misrounded allocation in mallocinit\")\n+\t}\n+\n+\t// Initialize the rest of the allocator.\n+\tmheap_.init(spansStart, spansSize)\n+\t_g_ := getg()\n+\t_g_.m.mcache = allocmcache()\n+}\n+\n+// sysAlloc allocates the next n bytes from the heap arena. The\n+// returned pointer is always _PageSize aligned and between\n+// h.arena_start and h.arena_end. sysAlloc returns nil on failure.\n+// There is no corresponding free function.\n+func (h *mheap) sysAlloc(n uintptr) unsafe.Pointer {\n+\tif n > h.arena_end-h.arena_used {\n+\t\t// We are in 32-bit mode, maybe we didn't use all possible address space yet.\n+\t\t// Reserve some more space.\n+\t\tp_size := round(n+_PageSize, 256<<20)\n+\t\tnew_end := h.arena_end + p_size // Careful: can overflow\n+\t\tif h.arena_end <= new_end && new_end-h.arena_start-1 <= _MaxArena32 {\n+\t\t\t// TODO: It would be bad if part of the arena\n+\t\t\t// is reserved and part is not.\n+\t\t\tvar reserved bool\n+\t\t\tp := uintptr(sysReserve(unsafe.Pointer(h.arena_end), p_size, &reserved))\n+\t\t\tif p == 0 {\n+\t\t\t\treturn nil\n+\t\t\t}\n+\t\t\tif p == h.arena_end {\n+\t\t\t\th.arena_end = new_end\n+\t\t\t\th.arena_reserved = reserved\n+\t\t\t} else if h.arena_start <= p && p+p_size-h.arena_start-1 <= _MaxArena32 {\n+\t\t\t\t// Keep everything page-aligned.\n+\t\t\t\t// Our pages are bigger than hardware pages.\n+\t\t\t\th.arena_end = p + p_size\n+\t\t\t\tused := p + (-p & (_PageSize - 1))\n+\t\t\t\th.mapBits(used)\n+\t\t\t\th.mapSpans(used)\n+\t\t\t\th.arena_used = used\n+\t\t\t\th.arena_reserved = reserved\n+\t\t\t} else {\n+\t\t\t\t// We haven't added this allocation to\n+\t\t\t\t// the stats, so subtract it from a\n+\t\t\t\t// fake stat (but avoid underflow).\n+\t\t\t\tstat := uint64(p_size)\n+\t\t\t\tsysFree(unsafe.Pointer(p), p_size, &stat)\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif n <= h.arena_end-h.arena_used {\n+\t\t// Keep taking from our reservation.\n+\t\tp := h.arena_used\n+\t\tsysMap(unsafe.Pointer(p), n, h.arena_reserved, &memstats.heap_sys)\n+\t\th.mapBits(p + n)\n+\t\th.mapSpans(p + n)\n+\t\th.arena_used = p + n\n+\t\tif raceenabled {\n+\t\t\tracemapshadow(unsafe.Pointer(p), n)\n+\t\t}\n+\n+\t\tif p&(_PageSize-1) != 0 {\n+\t\t\tthrow(\"misrounded allocation in MHeap_SysAlloc\")\n+\t\t}\n+\t\treturn unsafe.Pointer(p)\n+\t}\n+\n+\t// If using 64-bit, our reservation is all we have.\n+\tif h.arena_end-h.arena_start > _MaxArena32 {\n+\t\treturn nil\n+\t}\n+\n+\t// On 32-bit, once the reservation is gone we can\n+\t// try to get memory at a location chosen by the OS.\n+\tp_size := round(n, _PageSize) + _PageSize\n+\tp := uintptr(sysAlloc(p_size, &memstats.heap_sys))\n+\tif p == 0 {\n+\t\treturn nil\n+\t}\n+\n+\tif p < h.arena_start || p+p_size-h.arena_start > _MaxArena32 {\n+\t\ttop := ^uintptr(0)\n+\t\tif top-h.arena_start-1 > _MaxArena32 {\n+\t\t\ttop = h.arena_start + _MaxArena32 + 1\n+\t\t}\n+\t\tprint(\"runtime: memory allocated by OS (\", hex(p), \") not in usable range [\", hex(h.arena_start), \",\", hex(top), \")\\n\")\n+\t\tsysFree(unsafe.Pointer(p), p_size, &memstats.heap_sys)\n+\t\treturn nil\n+\t}\n+\n+\tp_end := p + p_size\n+\tp += -p & (_PageSize - 1)\n+\tif p+n > h.arena_used {\n+\t\th.mapBits(p + n)\n+\t\th.mapSpans(p + n)\n+\t\th.arena_used = p + n\n+\t\tif p_end > h.arena_end {\n+\t\t\th.arena_end = p_end\n+\t\t}\n+\t\tif raceenabled {\n+\t\t\tracemapshadow(unsafe.Pointer(p), n)\n+\t\t}\n+\t}\n+\n+\tif p&(_PageSize-1) != 0 {\n+\t\tthrow(\"misrounded allocation in MHeap_SysAlloc\")\n+\t}\n+\treturn unsafe.Pointer(p)\n+}\n+\n+// base address for all 0-byte allocations\n+var zerobase uintptr\n+\n+// nextFreeFast returns the next free object if one is quickly available.\n+// Otherwise it returns 0.\n+func nextFreeFast(s *mspan) gclinkptr {\n+\ttheBit := sys.Ctz64(s.allocCache) // Is there a free object in the allocCache?\n+\tif theBit < 64 {\n+\t\tresult := s.freeindex + uintptr(theBit)\n+\t\tif result < s.nelems {\n+\t\t\tfreeidx := result + 1\n+\t\t\tif freeidx%64 == 0 && freeidx != s.nelems {\n+\t\t\t\treturn 0\n+\t\t\t}\n+\t\t\ts.allocCache >>= (theBit + 1)\n+\t\t\ts.freeindex = freeidx\n+\t\t\tv := gclinkptr(result*s.elemsize + s.base())\n+\t\t\ts.allocCount++\n+\t\t\treturn v\n+\t\t}\n+\t}\n+\treturn 0\n+}\n+\n+// nextFree returns the next free object from the cached span if one is available.\n+// Otherwise it refills the cache with a span with an available object and\n+// returns that object along with a flag indicating that this was a heavy\n+// weight allocation. If it is a heavy weight allocation the caller must\n+// determine whether a new GC cycle needs to be started or if the GC is active\n+// whether this goroutine needs to assist the GC.\n+func (c *mcache) nextFree(sizeclass uint8) (v gclinkptr, s *mspan, shouldhelpgc bool) {\n+\ts = c.alloc[sizeclass]\n+\tshouldhelpgc = false\n+\tfreeIndex := s.nextFreeIndex()\n+\tif freeIndex == s.nelems {\n+\t\t// The span is full.\n+\t\tif uintptr(s.allocCount) != s.nelems {\n+\t\t\tprintln(\"runtime: s.allocCount=\", s.allocCount, \"s.nelems=\", s.nelems)\n+\t\t\tthrow(\"s.allocCount != s.nelems && freeIndex == s.nelems\")\n+\t\t}\n+\t\tsystemstack(func() {\n+\t\t\tc.refill(int32(sizeclass))\n+\t\t})\n+\t\tshouldhelpgc = true\n+\t\ts = c.alloc[sizeclass]\n+\n+\t\tfreeIndex = s.nextFreeIndex()\n+\t}\n+\n+\tif freeIndex >= s.nelems {\n+\t\tthrow(\"freeIndex is not valid\")\n+\t}\n+\n+\tv = gclinkptr(freeIndex*s.elemsize + s.base())\n+\ts.allocCount++\n+\tif uintptr(s.allocCount) > s.nelems {\n+\t\tprintln(\"s.allocCount=\", s.allocCount, \"s.nelems=\", s.nelems)\n+\t\tthrow(\"s.allocCount > s.nelems\")\n+\t}\n+\treturn\n+}\n+\n+// Allocate an object of size bytes.\n+// Small objects are allocated from the per-P cache's free lists.\n+// Large objects (> 32 kB) are allocated straight from the heap.\n+func mallocgc(size uintptr, typ *_type, needzero bool) unsafe.Pointer {\n+\tif gcphase == _GCmarktermination {\n+\t\tthrow(\"mallocgc called with gcphase == _GCmarktermination\")\n+\t}\n+\n+\tif size == 0 {\n+\t\treturn unsafe.Pointer(&zerobase)\n+\t}\n+\n+\tif debug.sbrk != 0 {\n+\t\talign := uintptr(16)\n+\t\tif typ != nil {\n+\t\t\talign = uintptr(typ.align)\n+\t\t}\n+\t\treturn persistentalloc(size, align, &memstats.other_sys)\n+\t}\n+\n+\t// When using gccgo, when a cgo or SWIG function has an\n+\t// interface return type and the function returns a\n+\t// non-pointer, memory allocation occurs after syscall.Cgocall\n+\t// but before syscall.CgocallDone. Treat this allocation as a\n+\t// callback.\n+\tincallback := false\n+\tif gomcache() == nil && getg().m.ncgo > 0 {\n+\t\texitsyscall(0)\n+\t\tincallback = true\n+\t}\n+\n+\t// assistG is the G to charge for this allocation, or nil if\n+\t// GC is not currently active.\n+\tvar assistG *g\n+\tif gcBlackenEnabled != 0 {\n+\t\t// Charge the current user G for this allocation.\n+\t\tassistG = getg()\n+\t\tif assistG.m.curg != nil {\n+\t\t\tassistG = assistG.m.curg\n+\t\t}\n+\t\t// Charge the allocation against the G. We'll account\n+\t\t// for internal fragmentation at the end of mallocgc.\n+\t\tassistG.gcAssistBytes -= int64(size)\n+\n+\t\tif assistG.gcAssistBytes < 0 {\n+\t\t\t// This G is in debt. Assist the GC to correct\n+\t\t\t// this before allocating. This must happen\n+\t\t\t// before disabling preemption.\n+\t\t\tgcAssistAlloc(assistG)\n+\t\t}\n+\t}\n+\n+\t// Set mp.mallocing to keep from being preempted by GC.\n+\tmp := acquirem()\n+\tif mp.mallocing != 0 {\n+\t\tthrow(\"malloc deadlock\")\n+\t}\n+\tif mp.gsignal == getg() {\n+\t\tthrow(\"malloc during signal\")\n+\t}\n+\tmp.mallocing = 1\n+\n+\tshouldhelpgc := false\n+\tdataSize := size\n+\tc := gomcache()\n+\tvar x unsafe.Pointer\n+\tnoscan := typ == nil || typ.kind&kindNoPointers != 0\n+\tif size <= maxSmallSize {\n+\t\tif noscan && size < maxTinySize {\n+\t\t\t// Tiny allocator.\n+\t\t\t//\n+\t\t\t// Tiny allocator combines several tiny allocation requests\n+\t\t\t// into a single memory block. The resulting memory block\n+\t\t\t// is freed when all subobjects are unreachable. The subobjects\n+\t\t\t// must be noscan (don't have pointers), this ensures that\n+\t\t\t// the amount of potentially wasted memory is bounded.\n+\t\t\t//\n+\t\t\t// Size of the memory block used for combining (maxTinySize) is tunable.\n+\t\t\t// Current setting is 16 bytes, which relates to 2x worst case memory\n+\t\t\t// wastage (when all but one subobjects are unreachable).\n+\t\t\t// 8 bytes would result in no wastage at all, but provides less\n+\t\t\t// opportunities for combining.\n+\t\t\t// 32 bytes provides more opportunities for combining,\n+\t\t\t// but can lead to 4x worst case wastage.\n+\t\t\t// The best case winning is 8x regardless of block size.\n+\t\t\t//\n+\t\t\t// Objects obtained from tiny allocator must not be freed explicitly.\n+\t\t\t// So when an object will be freed explicitly, we ensure that\n+\t\t\t// its size >= maxTinySize.\n+\t\t\t//\n+\t\t\t// SetFinalizer has a special case for objects potentially coming\n+\t\t\t// from tiny allocator, it such case it allows to set finalizers\n+\t\t\t// for an inner byte of a memory block.\n+\t\t\t//\n+\t\t\t// The main targets of tiny allocator are small strings and\n+\t\t\t// standalone escaping variables. On a json benchmark\n+\t\t\t// the allocator reduces number of allocations by ~12% and\n+\t\t\t// reduces heap size by ~20%.\n+\t\t\toff := c.tinyoffset\n+\t\t\t// Align tiny pointer for required (conservative) alignment.\n+\t\t\tif size&7 == 0 {\n+\t\t\t\toff = round(off, 8)\n+\t\t\t} else if size&3 == 0 {\n+\t\t\t\toff = round(off, 4)\n+\t\t\t} else if size&1 == 0 {\n+\t\t\t\toff = round(off, 2)\n+\t\t\t}\n+\t\t\tif off+size <= maxTinySize && c.tiny != 0 {\n+\t\t\t\t// The object fits into existing tiny block.\n+\t\t\t\tx = unsafe.Pointer(c.tiny + off)\n+\t\t\t\tc.tinyoffset = off + size\n+\t\t\t\tc.local_tinyallocs++\n+\t\t\t\tmp.mallocing = 0\n+\t\t\t\treleasem(mp)\n+\t\t\t\tif incallback {\n+\t\t\t\t\tentersyscall(0)\n+\t\t\t\t}\n+\t\t\t\treturn x\n+\t\t\t}\n+\t\t\t// Allocate a new maxTinySize block.\n+\t\t\tspan := c.alloc[tinySizeClass]\n+\t\t\tv := nextFreeFast(span)\n+\t\t\tif v == 0 {\n+\t\t\t\tv, _, shouldhelpgc = c.nextFree(tinySizeClass)\n+\t\t\t}\n+\t\t\tx = unsafe.Pointer(v)\n+\t\t\t(*[2]uint64)(x)[0] = 0\n+\t\t\t(*[2]uint64)(x)[1] = 0\n+\t\t\t// See if we need to replace the existing tiny block with the new one\n+\t\t\t// based on amount of remaining free space.\n+\t\t\tif size < c.tinyoffset || c.tiny == 0 {\n+\t\t\t\tc.tiny = uintptr(x)\n+\t\t\t\tc.tinyoffset = size\n+\t\t\t}\n+\t\t\tsize = maxTinySize\n+\t\t} else {\n+\t\t\tvar sizeclass uint8\n+\t\t\tif size <= smallSizeMax-8 {\n+\t\t\t\tsizeclass = size_to_class8[(size+smallSizeDiv-1)/smallSizeDiv]\n+\t\t\t} else {\n+\t\t\t\tsizeclass = size_to_class128[(size-smallSizeMax+largeSizeDiv-1)/largeSizeDiv]\n+\t\t\t}\n+\t\t\tsize = uintptr(class_to_size[sizeclass])\n+\t\t\tspan := c.alloc[sizeclass]\n+\t\t\tv := nextFreeFast(span)\n+\t\t\tif v == 0 {\n+\t\t\t\tv, span, shouldhelpgc = c.nextFree(sizeclass)\n+\t\t\t}\n+\t\t\tx = unsafe.Pointer(v)\n+\t\t\tif needzero && span.needzero != 0 {\n+\t\t\t\tmemclrNoHeapPointers(unsafe.Pointer(v), size)\n+\t\t\t}\n+\t\t}\n+\t} else {\n+\t\tvar s *mspan\n+\t\tshouldhelpgc = true\n+\t\tsystemstack(func() {\n+\t\t\ts = largeAlloc(size, needzero)\n+\t\t})\n+\t\ts.freeindex = 1\n+\t\ts.allocCount = 1\n+\t\tx = unsafe.Pointer(s.base())\n+\t\tsize = s.elemsize\n+\t}\n+\n+\tvar scanSize uintptr\n+\tif noscan {\n+\t\theapBitsSetTypeNoScan(uintptr(x))\n+\t} else {\n+\t\theapBitsSetType(uintptr(x), size, dataSize, typ)\n+\t\tif dataSize > typ.size {\n+\t\t\t// Array allocation. If there are any\n+\t\t\t// pointers, GC has to scan to the last\n+\t\t\t// element.\n+\t\t\tif typ.ptrdata != 0 {\n+\t\t\t\tscanSize = dataSize - typ.size + typ.ptrdata\n+\t\t\t}\n+\t\t} else {\n+\t\t\tscanSize = typ.ptrdata\n+\t\t}\n+\t\tc.local_scan += scanSize\n+\t}\n+\n+\t// Ensure that the stores above that initialize x to\n+\t// type-safe memory and set the heap bits occur before\n+\t// the caller can make x observable to the garbage\n+\t// collector. Otherwise, on weakly ordered machines,\n+\t// the garbage collector could follow a pointer to x,\n+\t// but see uninitialized memory or stale heap bits.\n+\tpublicationBarrier()\n+\n+\t// Allocate black during GC.\n+\t// All slots hold nil so no scanning is needed.\n+\t// This may be racing with GC so do it atomically if there can be\n+\t// a race marking the bit.\n+\tif gcphase != _GCoff {\n+\t\tgcmarknewobject(uintptr(x), size, scanSize)\n+\t}\n+\n+\tif raceenabled {\n+\t\tracemalloc(x, size)\n+\t}\n+\n+\tif msanenabled {\n+\t\tmsanmalloc(x, size)\n+\t}\n+\n+\tmp.mallocing = 0\n+\treleasem(mp)\n+\n+\tif debug.allocfreetrace != 0 {\n+\t\ttracealloc(x, size, typ)\n+\t}\n+\n+\tif rate := MemProfileRate; rate > 0 {\n+\t\tif size < uintptr(rate) && int32(size) < c.next_sample {\n+\t\t\tc.next_sample -= int32(size)\n+\t\t} else {\n+\t\t\tmp := acquirem()\n+\t\t\tprofilealloc(mp, x, size)\n+\t\t\treleasem(mp)\n+\t\t}\n+\t}\n+\n+\tif assistG != nil {\n+\t\t// Account for internal fragmentation in the assist\n+\t\t// debt now that we know it.\n+\t\tassistG.gcAssistBytes -= int64(size - dataSize)\n+\t}\n+\n+\tif shouldhelpgc && gcShouldStart(false) {\n+\t\tgcStart(gcBackgroundMode, false)\n+\t}\n+\n+\tif getg().preempt {\n+\t\tcheckPreempt()\n+\t}\n+\n+\tif incallback {\n+\t\tentersyscall(0)\n+\t}\n+\n+\treturn x\n+}\n+\n+func largeAlloc(size uintptr, needzero bool) *mspan {\n+\t// print(\"largeAlloc size=\", size, \"\\n\")\n+\n+\tif size+_PageSize < size {\n+\t\tthrow(\"out of memory\")\n+\t}\n+\tnpages := size >> _PageShift\n+\tif size&_PageMask != 0 {\n+\t\tnpages++\n+\t}\n+\n+\t// Deduct credit for this span allocation and sweep if\n+\t// necessary. mHeap_Alloc will also sweep npages, so this only\n+\t// pays the debt down to npage pages.\n+\tdeductSweepCredit(npages*_PageSize, npages)\n+\n+\ts := mheap_.alloc(npages, 0, true, needzero)\n+\tif s == nil {\n+\t\tthrow(\"out of memory\")\n+\t}\n+\ts.limit = s.base() + size\n+\theapBitsForSpan(s.base()).initSpan(s)\n+\treturn s\n+}\n+\n+// implementation of new builtin\n+// compiler (both frontend and SSA backend) knows the signature\n+// of this function\n+func newobject(typ *_type) unsafe.Pointer {\n+\treturn mallocgc(typ.size, typ, true)\n+}\n+\n+//go:linkname reflect_unsafe_New reflect.unsafe_New\n+func reflect_unsafe_New(typ *_type) unsafe.Pointer {\n+\treturn newobject(typ)\n+}\n+\n+// newarray allocates an array of n elements of type typ.\n+func newarray(typ *_type, n int) unsafe.Pointer {\n+\tif n < 0 || uintptr(n) > maxSliceCap(typ.size) {\n+\t\tpanic(plainError(\"runtime: allocation size out of range\"))\n+\t}\n+\treturn mallocgc(typ.size*uintptr(n), typ, true)\n+}\n+\n+//go:linkname reflect_unsafe_NewArray reflect.unsafe_NewArray\n+func reflect_unsafe_NewArray(typ *_type, n int) unsafe.Pointer {\n+\treturn newarray(typ, n)\n+}\n+\n+func profilealloc(mp *m, x unsafe.Pointer, size uintptr) {\n+\tmp.mcache.next_sample = nextSample()\n+\tmProf_Malloc(x, size)\n+}\n+\n+// nextSample returns the next sampling point for heap profiling.\n+// It produces a random variable with a geometric distribution and\n+// mean MemProfileRate. This is done by generating a uniformly\n+// distributed random number and applying the cumulative distribution\n+// function for an exponential.\n+func nextSample() int32 {\n+\tif GOOS == \"plan9\" {\n+\t\t// Plan 9 doesn't support floating point in note handler.\n+\t\tif g := getg(); g == g.m.gsignal {\n+\t\t\treturn nextSampleNoFP()\n+\t\t}\n+\t}\n+\n+\tperiod := MemProfileRate\n+\n+\t// make nextSample not overflow. Maximum possible step is\n+\t// -ln(1/(1<<kRandomBitCount)) * period, approximately 20 * period.\n+\tswitch {\n+\tcase period > 0x7000000:\n+\t\tperiod = 0x7000000\n+\tcase period == 0:\n+\t\treturn 0\n+\t}\n+\n+\t// Let m be the sample rate,\n+\t// the probability distribution function is m*exp(-mx), so the CDF is\n+\t// p = 1 - exp(-mx), so\n+\t// q = 1 - p == exp(-mx)\n+\t// log_e(q) = -mx\n+\t// -log_e(q)/m = x\n+\t// x = -log_e(q) * period\n+\t// x = log_2(q) * (-log_e(2)) * period    ; Using log_2 for efficiency\n+\tconst randomBitCount = 26\n+\tq := fastrand()%(1<<randomBitCount) + 1\n+\tqlog := fastlog2(float64(q)) - randomBitCount\n+\tif qlog > 0 {\n+\t\tqlog = 0\n+\t}\n+\tconst minusLog2 = -0.6931471805599453 // -ln(2)\n+\treturn int32(qlog*(minusLog2*float64(period))) + 1\n+}\n+\n+// nextSampleNoFP is similar to nextSample, but uses older,\n+// simpler code to avoid floating point.\n+func nextSampleNoFP() int32 {\n+\t// Set first allocation sample size.\n+\trate := MemProfileRate\n+\tif rate > 0x3fffffff { // make 2*rate not overflow\n+\t\trate = 0x3fffffff\n+\t}\n+\tif rate != 0 {\n+\t\treturn int32(int(fastrand()) % (2 * rate))\n+\t}\n+\treturn 0\n+}\n+\n+type persistentAlloc struct {\n+\tbase unsafe.Pointer\n+\toff  uintptr\n+}\n+\n+var globalAlloc struct {\n+\tmutex\n+\tpersistentAlloc\n+}\n+\n+// Wrapper around sysAlloc that can allocate small chunks.\n+// There is no associated free operation.\n+// Intended for things like function/type/debug-related persistent data.\n+// If align is 0, uses default align (currently 8).\n+// The returned memory will be zeroed.\n+//\n+// Consider marking persistentalloc'd types go:notinheap.\n+func persistentalloc(size, align uintptr, sysStat *uint64) unsafe.Pointer {\n+\tvar p unsafe.Pointer\n+\tsystemstack(func() {\n+\t\tp = persistentalloc1(size, align, sysStat)\n+\t})\n+\treturn p\n+}\n+\n+// Must run on system stack because stack growth can (re)invoke it.\n+// See issue 9174.\n+//go:systemstack\n+func persistentalloc1(size, align uintptr, sysStat *uint64) unsafe.Pointer {\n+\tconst (\n+\t\tchunk    = 256 << 10\n+\t\tmaxBlock = 64 << 10 // VM reservation granularity is 64K on windows\n+\t)\n+\n+\tif size == 0 {\n+\t\tthrow(\"persistentalloc: size == 0\")\n+\t}\n+\tif align != 0 {\n+\t\tif align&(align-1) != 0 {\n+\t\t\tthrow(\"persistentalloc: align is not a power of 2\")\n+\t\t}\n+\t\tif align > _PageSize {\n+\t\t\tthrow(\"persistentalloc: align is too large\")\n+\t\t}\n+\t} else {\n+\t\talign = 8\n+\t}\n+\n+\tif size >= maxBlock {\n+\t\treturn sysAlloc(size, sysStat)\n+\t}\n+\n+\tmp := acquirem()\n+\tvar persistent *persistentAlloc\n+\tif mp != nil && mp.p != 0 {\n+\t\tpersistent = &mp.p.ptr().palloc\n+\t} else {\n+\t\tlock(&globalAlloc.mutex)\n+\t\tpersistent = &globalAlloc.persistentAlloc\n+\t}\n+\tpersistent.off = round(persistent.off, align)\n+\tif persistent.off+size > chunk || persistent.base == nil {\n+\t\tpersistent.base = sysAlloc(chunk, &memstats.other_sys)\n+\t\tif persistent.base == nil {\n+\t\t\tif persistent == &globalAlloc.persistentAlloc {\n+\t\t\t\tunlock(&globalAlloc.mutex)\n+\t\t\t}\n+\t\t\tthrow(\"runtime: cannot allocate memory\")\n+\t\t}\n+\t\tpersistent.off = 0\n+\t}\n+\tp := add(persistent.base, persistent.off)\n+\tpersistent.off += size\n+\treleasem(mp)\n+\tif persistent == &globalAlloc.persistentAlloc {\n+\t\tunlock(&globalAlloc.mutex)\n+\t}\n+\n+\tif sysStat != &memstats.other_sys {\n+\t\tmSysStatInc(sysStat, size)\n+\t\tmSysStatDec(&memstats.other_sys, size)\n+\t}\n+\treturn p\n+}"}, {"sha": "3a463c8b1bab52a6f3ddfd1e3c8630e410f69022", "filename": "libgo/go/runtime/mbarrier.go", "status": "added", "additions": 418, "deletions": 0, "changes": 418, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmbarrier.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmbarrier.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmbarrier.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,418 @@\n+// Copyright 2015 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Garbage collector: write barriers.\n+//\n+// For the concurrent garbage collector, the Go compiler implements\n+// updates to pointer-valued fields that may be in heap objects by\n+// emitting calls to write barriers. This file contains the actual write barrier\n+// implementation, gcmarkwb_m, and the various wrappers called by the\n+// compiler to implement pointer assignment, slice assignment,\n+// typed memmove, and so on.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/sys\"\n+\t\"unsafe\"\n+)\n+\n+// For gccgo, use go:linkname to rename compiler-called functions to\n+// themselves, so that the compiler will export them.\n+//\n+//go:linkname writebarrierptr runtime.writebarrierptr\n+//go:linkname typedmemmove runtime.typedmemmove\n+//go:linkname typedslicecopy runtime.typedslicecopy\n+\n+// gcmarkwb_m is the mark-phase write barrier, the only barrier we have.\n+// The rest of this file exists only to make calls to this function.\n+//\n+// This is a hybrid barrier that combines a Yuasa-style deletion\n+// barrier\u2014which shades the object whose reference is being\n+// overwritten\u2014with Dijkstra insertion barrier\u2014which shades the object\n+// whose reference is being written. The insertion part of the barrier\n+// is necessary while the calling goroutine's stack is grey. In\n+// pseudocode, the barrier is:\n+//\n+//     writePointer(slot, ptr):\n+//         shade(*slot)\n+//         if current stack is grey:\n+//             shade(ptr)\n+//         *slot = ptr\n+//\n+// slot is the destination in Go code.\n+// ptr is the value that goes into the slot in Go code.\n+//\n+// Shade indicates that it has seen a white pointer by adding the referent\n+// to wbuf as well as marking it.\n+//\n+// The two shades and the condition work together to prevent a mutator\n+// from hiding an object from the garbage collector:\n+//\n+// 1. shade(*slot) prevents a mutator from hiding an object by moving\n+// the sole pointer to it from the heap to its stack. If it attempts\n+// to unlink an object from the heap, this will shade it.\n+//\n+// 2. shade(ptr) prevents a mutator from hiding an object by moving\n+// the sole pointer to it from its stack into a black object in the\n+// heap. If it attempts to install the pointer into a black object,\n+// this will shade it.\n+//\n+// 3. Once a goroutine's stack is black, the shade(ptr) becomes\n+// unnecessary. shade(ptr) prevents hiding an object by moving it from\n+// the stack to the heap, but this requires first having a pointer\n+// hidden on the stack. Immediately after a stack is scanned, it only\n+// points to shaded objects, so it's not hiding anything, and the\n+// shade(*slot) prevents it from hiding any other pointers on its\n+// stack.\n+//\n+// For a detailed description of this barrier and proof of\n+// correctness, see https://github.com/golang/proposal/blob/master/design/17503-eliminate-rescan.md\n+//\n+//\n+//\n+// Dealing with memory ordering:\n+//\n+// Both the Yuasa and Dijkstra barriers can be made conditional on the\n+// color of the object containing the slot. We chose not to make these\n+// conditional because the cost of ensuring that the object holding\n+// the slot doesn't concurrently change color without the mutator\n+// noticing seems prohibitive.\n+//\n+// Consider the following example where the mutator writes into\n+// a slot and then loads the slot's mark bit while the GC thread\n+// writes to the slot's mark bit and then as part of scanning reads\n+// the slot.\n+//\n+// Initially both [slot] and [slotmark] are 0 (nil)\n+// Mutator thread          GC thread\n+// st [slot], ptr          st [slotmark], 1\n+//\n+// ld r1, [slotmark]       ld r2, [slot]\n+//\n+// Without an expensive memory barrier between the st and the ld, the final\n+// result on most HW (including 386/amd64) can be r1==r2==0. This is a classic\n+// example of what can happen when loads are allowed to be reordered with older\n+// stores (avoiding such reorderings lies at the heart of the classic\n+// Peterson/Dekker algorithms for mutual exclusion). Rather than require memory\n+// barriers, which will slow down both the mutator and the GC, we always grey\n+// the ptr object regardless of the slot's color.\n+//\n+// Another place where we intentionally omit memory barriers is when\n+// accessing mheap_.arena_used to check if a pointer points into the\n+// heap. On relaxed memory machines, it's possible for a mutator to\n+// extend the size of the heap by updating arena_used, allocate an\n+// object from this new region, and publish a pointer to that object,\n+// but for tracing running on another processor to observe the pointer\n+// but use the old value of arena_used. In this case, tracing will not\n+// mark the object, even though it's reachable. However, the mutator\n+// is guaranteed to execute a write barrier when it publishes the\n+// pointer, so it will take care of marking the object. A general\n+// consequence of this is that the garbage collector may cache the\n+// value of mheap_.arena_used. (See issue #9984.)\n+//\n+//\n+// Stack writes:\n+//\n+// The compiler omits write barriers for writes to the current frame,\n+// but if a stack pointer has been passed down the call stack, the\n+// compiler will generate a write barrier for writes through that\n+// pointer (because it doesn't know it's not a heap pointer).\n+//\n+// One might be tempted to ignore the write barrier if slot points\n+// into to the stack. Don't do it! Mark termination only re-scans\n+// frames that have potentially been active since the concurrent scan,\n+// so it depends on write barriers to track changes to pointers in\n+// stack frames that have not been active.\n+//\n+//\n+// Global writes:\n+//\n+// The Go garbage collector requires write barriers when heap pointers\n+// are stored in globals. Many garbage collectors ignore writes to\n+// globals and instead pick up global -> heap pointers during\n+// termination. This increases pause time, so we instead rely on write\n+// barriers for writes to globals so that we don't have to rescan\n+// global during mark termination.\n+//\n+//\n+// Publication ordering:\n+//\n+// The write barrier is *pre-publication*, meaning that the write\n+// barrier happens prior to the *slot = ptr write that may make ptr\n+// reachable by some goroutine that currently cannot reach it.\n+//\n+//\n+//go:nowritebarrierrec\n+//go:systemstack\n+func gcmarkwb_m(slot *uintptr, ptr uintptr) {\n+\tif writeBarrier.needed {\n+\t\t// Note: This turns bad pointer writes into bad\n+\t\t// pointer reads, which could be confusing. We avoid\n+\t\t// reading from obviously bad pointers, which should\n+\t\t// take care of the vast majority of these. We could\n+\t\t// patch this up in the signal handler, or use XCHG to\n+\t\t// combine the read and the write. Checking inheap is\n+\t\t// insufficient since we need to track changes to\n+\t\t// roots outside the heap.\n+\t\tif slot1 := uintptr(unsafe.Pointer(slot)); slot1 >= minPhysPageSize {\n+\t\t\tif optr := *slot; optr != 0 {\n+\t\t\t\tshade(optr)\n+\t\t\t}\n+\t\t}\n+\t\t// TODO: Make this conditional on the caller's stack color.\n+\t\tif ptr != 0 && inheap(ptr) {\n+\t\t\tshade(ptr)\n+\t\t}\n+\t}\n+}\n+\n+// writebarrierptr_prewrite1 invokes a write barrier for *dst = src\n+// prior to the write happening.\n+//\n+// Write barrier calls must not happen during critical GC and scheduler\n+// related operations. In particular there are times when the GC assumes\n+// that the world is stopped but scheduler related code is still being\n+// executed, dealing with syscalls, dealing with putting gs on runnable\n+// queues and so forth. This code cannot execute write barriers because\n+// the GC might drop them on the floor. Stopping the world involves removing\n+// the p associated with an m. We use the fact that m.p == nil to indicate\n+// that we are in one these critical section and throw if the write is of\n+// a pointer to a heap object.\n+//go:nosplit\n+func writebarrierptr_prewrite1(dst *uintptr, src uintptr) {\n+\tmp := acquirem()\n+\tif mp.inwb || mp.dying > 0 {\n+\t\treleasem(mp)\n+\t\treturn\n+\t}\n+\tsystemstack(func() {\n+\t\tif mp.p == 0 && memstats.enablegc && !mp.inwb && inheap(src) {\n+\t\t\tthrow(\"writebarrierptr_prewrite1 called with mp.p == nil\")\n+\t\t}\n+\t\tmp.inwb = true\n+\t\tgcmarkwb_m(dst, src)\n+\t})\n+\tmp.inwb = false\n+\treleasem(mp)\n+}\n+\n+// NOTE: Really dst *unsafe.Pointer, src unsafe.Pointer,\n+// but if we do that, Go inserts a write barrier on *dst = src.\n+//go:nosplit\n+func writebarrierptr(dst *uintptr, src uintptr) {\n+\tif writeBarrier.cgo {\n+\t\tcgoCheckWriteBarrier(dst, src)\n+\t}\n+\tif !writeBarrier.needed {\n+\t\t*dst = src\n+\t\treturn\n+\t}\n+\tif src != 0 && src < minPhysPageSize {\n+\t\tsystemstack(func() {\n+\t\t\tprint(\"runtime: writebarrierptr *\", dst, \" = \", hex(src), \"\\n\")\n+\t\t\tthrow(\"bad pointer in write barrier\")\n+\t\t})\n+\t}\n+\twritebarrierptr_prewrite1(dst, src)\n+\t*dst = src\n+}\n+\n+// writebarrierptr_prewrite is like writebarrierptr, but the store\n+// will be performed by the caller after this call. The caller must\n+// not allow preemption between this call and the write.\n+//\n+//go:nosplit\n+func writebarrierptr_prewrite(dst *uintptr, src uintptr) {\n+\tif writeBarrier.cgo {\n+\t\tcgoCheckWriteBarrier(dst, src)\n+\t}\n+\tif !writeBarrier.needed {\n+\t\treturn\n+\t}\n+\tif src != 0 && src < minPhysPageSize {\n+\t\tsystemstack(func() { throw(\"bad pointer in write barrier\") })\n+\t}\n+\twritebarrierptr_prewrite1(dst, src)\n+}\n+\n+// typedmemmove copies a value of type t to dst from src.\n+//go:nosplit\n+func typedmemmove(typ *_type, dst, src unsafe.Pointer) {\n+\tif typ.kind&kindNoPointers == 0 {\n+\t\tbulkBarrierPreWrite(uintptr(dst), uintptr(src), typ.size)\n+\t}\n+\t// There's a race here: if some other goroutine can write to\n+\t// src, it may change some pointer in src after we've\n+\t// performed the write barrier but before we perform the\n+\t// memory copy. This safe because the write performed by that\n+\t// other goroutine must also be accompanied by a write\n+\t// barrier, so at worst we've unnecessarily greyed the old\n+\t// pointer that was in src.\n+\tmemmove(dst, src, typ.size)\n+\tif writeBarrier.cgo {\n+\t\tcgoCheckMemmove(typ, dst, src, 0, typ.size)\n+\t}\n+}\n+\n+//go:linkname reflect_typedmemmove reflect.typedmemmove\n+func reflect_typedmemmove(typ *_type, dst, src unsafe.Pointer) {\n+\tif raceenabled {\n+\t\traceWriteObjectPC(typ, dst, getcallerpc(unsafe.Pointer(&typ)), funcPC(reflect_typedmemmove))\n+\t\traceReadObjectPC(typ, src, getcallerpc(unsafe.Pointer(&typ)), funcPC(reflect_typedmemmove))\n+\t}\n+\tif msanenabled {\n+\t\tmsanwrite(dst, typ.size)\n+\t\tmsanread(src, typ.size)\n+\t}\n+\ttypedmemmove(typ, dst, src)\n+}\n+\n+// typedmemmovepartial is like typedmemmove but assumes that\n+// dst and src point off bytes into the value and only copies size bytes.\n+//go:linkname reflect_typedmemmovepartial reflect.typedmemmovepartial\n+func reflect_typedmemmovepartial(typ *_type, dst, src unsafe.Pointer, off, size uintptr) {\n+\tif writeBarrier.needed && typ.kind&kindNoPointers == 0 && size >= sys.PtrSize {\n+\t\t// Pointer-align start address for bulk barrier.\n+\t\tadst, asrc, asize := dst, src, size\n+\t\tif frag := -off & (sys.PtrSize - 1); frag != 0 {\n+\t\t\tadst = add(dst, frag)\n+\t\t\tasrc = add(src, frag)\n+\t\t\tasize -= frag\n+\t\t}\n+\t\tbulkBarrierPreWrite(uintptr(adst), uintptr(asrc), asize&^(sys.PtrSize-1))\n+\t}\n+\n+\tmemmove(dst, src, size)\n+\tif writeBarrier.cgo {\n+\t\tcgoCheckMemmove(typ, dst, src, off, size)\n+\t}\n+}\n+\n+//go:nosplit\n+func typedslicecopy(typ *_type, dst, src slice) int {\n+\t// TODO(rsc): If typedslicecopy becomes faster than calling\n+\t// typedmemmove repeatedly, consider using during func growslice.\n+\tn := dst.len\n+\tif n > src.len {\n+\t\tn = src.len\n+\t}\n+\tif n == 0 {\n+\t\treturn 0\n+\t}\n+\tdstp := dst.array\n+\tsrcp := src.array\n+\n+\tif raceenabled {\n+\t\tcallerpc := getcallerpc(unsafe.Pointer(&typ))\n+\t\tpc := funcPC(slicecopy)\n+\t\tracewriterangepc(dstp, uintptr(n)*typ.size, callerpc, pc)\n+\t\tracereadrangepc(srcp, uintptr(n)*typ.size, callerpc, pc)\n+\t}\n+\tif msanenabled {\n+\t\tmsanwrite(dstp, uintptr(n)*typ.size)\n+\t\tmsanread(srcp, uintptr(n)*typ.size)\n+\t}\n+\n+\tif writeBarrier.cgo {\n+\t\tcgoCheckSliceCopy(typ, dst, src, n)\n+\t}\n+\n+\t// Note: No point in checking typ.kind&kindNoPointers here:\n+\t// compiler only emits calls to typedslicecopy for types with pointers,\n+\t// and growslice and reflect_typedslicecopy check for pointers\n+\t// before calling typedslicecopy.\n+\tif !writeBarrier.needed {\n+\t\tmemmove(dstp, srcp, uintptr(n)*typ.size)\n+\t\treturn n\n+\t}\n+\n+\tsystemstack(func() {\n+\t\tif uintptr(srcp) < uintptr(dstp) && uintptr(srcp)+uintptr(n)*typ.size > uintptr(dstp) {\n+\t\t\t// Overlap with src before dst.\n+\t\t\t// Copy backward, being careful not to move dstp/srcp\n+\t\t\t// out of the array they point into.\n+\t\t\tdstp = add(dstp, uintptr(n-1)*typ.size)\n+\t\t\tsrcp = add(srcp, uintptr(n-1)*typ.size)\n+\t\t\ti := 0\n+\t\t\tfor {\n+\t\t\t\ttypedmemmove(typ, dstp, srcp)\n+\t\t\t\tif i++; i >= n {\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t\tdstp = add(dstp, -typ.size)\n+\t\t\t\tsrcp = add(srcp, -typ.size)\n+\t\t\t}\n+\t\t} else {\n+\t\t\t// Copy forward, being careful not to move dstp/srcp\n+\t\t\t// out of the array they point into.\n+\t\t\ti := 0\n+\t\t\tfor {\n+\t\t\t\ttypedmemmove(typ, dstp, srcp)\n+\t\t\t\tif i++; i >= n {\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t\tdstp = add(dstp, typ.size)\n+\t\t\t\tsrcp = add(srcp, typ.size)\n+\t\t\t}\n+\t\t}\n+\t})\n+\treturn n\n+}\n+\n+//go:linkname reflect_typedslicecopy reflect.typedslicecopy\n+func reflect_typedslicecopy(elemType *_type, dst, src slice) int {\n+\tif elemType.kind&kindNoPointers != 0 {\n+\t\tn := dst.len\n+\t\tif n > src.len {\n+\t\t\tn = src.len\n+\t\t}\n+\t\tif n == 0 {\n+\t\t\treturn 0\n+\t\t}\n+\n+\t\tsize := uintptr(n) * elemType.size\n+\t\tif raceenabled {\n+\t\t\tcallerpc := getcallerpc(unsafe.Pointer(&elemType))\n+\t\t\tpc := funcPC(reflect_typedslicecopy)\n+\t\t\tracewriterangepc(dst.array, size, callerpc, pc)\n+\t\t\tracereadrangepc(src.array, size, callerpc, pc)\n+\t\t}\n+\t\tif msanenabled {\n+\t\t\tmsanwrite(dst.array, size)\n+\t\t\tmsanread(src.array, size)\n+\t\t}\n+\n+\t\tmemmove(dst.array, src.array, size)\n+\t\treturn n\n+\t}\n+\treturn typedslicecopy(elemType, dst, src)\n+}\n+\n+// typedmemclr clears the typed memory at ptr with type typ. The\n+// memory at ptr must already be initialized (and hence in type-safe\n+// state). If the memory is being initialized for the first time, see\n+// memclrNoHeapPointers.\n+//\n+// If the caller knows that typ has pointers, it can alternatively\n+// call memclrHasPointers.\n+//\n+//go:nosplit\n+func typedmemclr(typ *_type, ptr unsafe.Pointer) {\n+\tif typ.kind&kindNoPointers == 0 {\n+\t\tbulkBarrierPreWrite(uintptr(ptr), 0, typ.size)\n+\t}\n+\tmemclrNoHeapPointers(ptr, typ.size)\n+}\n+\n+// memclrHasPointers clears n bytes of typed memory starting at ptr.\n+// The caller must ensure that the type of the object at ptr has\n+// pointers, usually by checking typ.kind&kindNoPointers. However, ptr\n+// does not have to point to the start of the allocation.\n+//\n+//go:nosplit\n+func memclrHasPointers(ptr unsafe.Pointer, n uintptr) {\n+\tbulkBarrierPreWrite(uintptr(ptr), 0, n)\n+\tmemclrNoHeapPointers(ptr, n)\n+}"}, {"sha": "2b00493d43c3172f55b2520764b301868c95e2de", "filename": "libgo/go/runtime/mbitmap.go", "status": "added", "additions": 1874, "deletions": 0, "changes": 1874, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmbitmap.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmbitmap.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmbitmap.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,1874 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Garbage collector: type and heap bitmaps.\n+//\n+// Stack, data, and bss bitmaps\n+//\n+// Stack frames and global variables in the data and bss sections are described\n+// by 1-bit bitmaps in which 0 means uninteresting and 1 means live pointer\n+// to be visited during GC. The bits in each byte are consumed starting with\n+// the low bit: 1<<0, 1<<1, and so on.\n+//\n+// Heap bitmap\n+//\n+// The allocated heap comes from a subset of the memory in the range [start, used),\n+// where start == mheap_.arena_start and used == mheap_.arena_used.\n+// The heap bitmap comprises 2 bits for each pointer-sized word in that range,\n+// stored in bytes indexed backward in memory from start.\n+// That is, the byte at address start-1 holds the 2-bit entries for the four words\n+// start through start+3*ptrSize, the byte at start-2 holds the entries for\n+// start+4*ptrSize through start+7*ptrSize, and so on.\n+//\n+// In each 2-bit entry, the lower bit holds the same information as in the 1-bit\n+// bitmaps: 0 means uninteresting and 1 means live pointer to be visited during GC.\n+// The meaning of the high bit depends on the position of the word being described\n+// in its allocated object. In all words *except* the second word, the\n+// high bit indicates that the object is still being described. In\n+// these words, if a bit pair with a high bit 0 is encountered, the\n+// low bit can also be assumed to be 0, and the object description is\n+// over. This 00 is called the ``dead'' encoding: it signals that the\n+// rest of the words in the object are uninteresting to the garbage\n+// collector.\n+//\n+// In the second word, the high bit is the GC ``checkmarked'' bit (see below).\n+//\n+// The 2-bit entries are split when written into the byte, so that the top half\n+// of the byte contains 4 high bits and the bottom half contains 4 low (pointer)\n+// bits.\n+// This form allows a copy from the 1-bit to the 4-bit form to keep the\n+// pointer bits contiguous, instead of having to space them out.\n+//\n+// The code makes use of the fact that the zero value for a heap bitmap\n+// has no live pointer bit set and is (depending on position), not used,\n+// not checkmarked, and is the dead encoding.\n+// These properties must be preserved when modifying the encoding.\n+//\n+// Checkmarks\n+//\n+// In a concurrent garbage collector, one worries about failing to mark\n+// a live object due to mutations without write barriers or bugs in the\n+// collector implementation. As a sanity check, the GC has a 'checkmark'\n+// mode that retraverses the object graph with the world stopped, to make\n+// sure that everything that should be marked is marked.\n+// In checkmark mode, in the heap bitmap, the high bit of the 2-bit entry\n+// for the second word of the object holds the checkmark bit.\n+// When not in checkmark mode, this bit is set to 1.\n+//\n+// The smallest possible allocation is 8 bytes. On a 32-bit machine, that\n+// means every allocated object has two words, so there is room for the\n+// checkmark bit. On a 64-bit machine, however, the 8-byte allocation is\n+// just one word, so the second bit pair is not available for encoding the\n+// checkmark. However, because non-pointer allocations are combined\n+// into larger 16-byte (maxTinySize) allocations, a plain 8-byte allocation\n+// must be a pointer, so the type bit in the first word is not actually needed.\n+// It is still used in general, except in checkmark the type bit is repurposed\n+// as the checkmark bit and then reinitialized (to 1) as the type bit when\n+// finished.\n+//\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/atomic\"\n+\t\"runtime/internal/sys\"\n+\t\"unsafe\"\n+)\n+\n+const (\n+\tbitPointer = 1 << 0\n+\tbitScan    = 1 << 4\n+\n+\theapBitsShift   = 1                     // shift offset between successive bitPointer or bitScan entries\n+\theapBitmapScale = sys.PtrSize * (8 / 2) // number of data bytes described by one heap bitmap byte\n+\n+\t// all scan/pointer bits in a byte\n+\tbitScanAll    = bitScan | bitScan<<heapBitsShift | bitScan<<(2*heapBitsShift) | bitScan<<(3*heapBitsShift)\n+\tbitPointerAll = bitPointer | bitPointer<<heapBitsShift | bitPointer<<(2*heapBitsShift) | bitPointer<<(3*heapBitsShift)\n+)\n+\n+// addb returns the byte pointer p+n.\n+//go:nowritebarrier\n+//go:nosplit\n+func addb(p *byte, n uintptr) *byte {\n+\t// Note: wrote out full expression instead of calling add(p, n)\n+\t// to reduce the number of temporaries generated by the\n+\t// compiler for this trivial expression during inlining.\n+\treturn (*byte)(unsafe.Pointer(uintptr(unsafe.Pointer(p)) + n))\n+}\n+\n+// subtractb returns the byte pointer p-n.\n+// subtractb is typically used when traversing the pointer tables referred to by hbits\n+// which are arranged in reverse order.\n+//go:nowritebarrier\n+//go:nosplit\n+func subtractb(p *byte, n uintptr) *byte {\n+\t// Note: wrote out full expression instead of calling add(p, -n)\n+\t// to reduce the number of temporaries generated by the\n+\t// compiler for this trivial expression during inlining.\n+\treturn (*byte)(unsafe.Pointer(uintptr(unsafe.Pointer(p)) - n))\n+}\n+\n+// add1 returns the byte pointer p+1.\n+//go:nowritebarrier\n+//go:nosplit\n+func add1(p *byte) *byte {\n+\t// Note: wrote out full expression instead of calling addb(p, 1)\n+\t// to reduce the number of temporaries generated by the\n+\t// compiler for this trivial expression during inlining.\n+\treturn (*byte)(unsafe.Pointer(uintptr(unsafe.Pointer(p)) + 1))\n+}\n+\n+// subtract1 returns the byte pointer p-1.\n+// subtract1 is typically used when traversing the pointer tables referred to by hbits\n+// which are arranged in reverse order.\n+//go:nowritebarrier\n+//\n+// nosplit because it is used during write barriers and must not be preempted.\n+//go:nosplit\n+func subtract1(p *byte) *byte {\n+\t// Note: wrote out full expression instead of calling subtractb(p, 1)\n+\t// to reduce the number of temporaries generated by the\n+\t// compiler for this trivial expression during inlining.\n+\treturn (*byte)(unsafe.Pointer(uintptr(unsafe.Pointer(p)) - 1))\n+}\n+\n+// mHeap_MapBits is called each time arena_used is extended.\n+// It maps any additional bitmap memory needed for the new arena memory.\n+// It must be called with the expected new value of arena_used,\n+// *before* h.arena_used has been updated.\n+// Waiting to update arena_used until after the memory has been mapped\n+// avoids faults when other threads try access the bitmap immediately\n+// after observing the change to arena_used.\n+//\n+//go:nowritebarrier\n+func (h *mheap) mapBits(arena_used uintptr) {\n+\t// Caller has added extra mappings to the arena.\n+\t// Add extra mappings of bitmap words as needed.\n+\t// We allocate extra bitmap pieces in chunks of bitmapChunk.\n+\tconst bitmapChunk = 8192\n+\n+\tn := (arena_used - mheap_.arena_start) / heapBitmapScale\n+\tn = round(n, bitmapChunk)\n+\tn = round(n, physPageSize)\n+\tif h.bitmap_mapped >= n {\n+\t\treturn\n+\t}\n+\n+\tsysMap(unsafe.Pointer(h.bitmap-n), n-h.bitmap_mapped, h.arena_reserved, &memstats.gc_sys)\n+\th.bitmap_mapped = n\n+}\n+\n+// heapBits provides access to the bitmap bits for a single heap word.\n+// The methods on heapBits take value receivers so that the compiler\n+// can more easily inline calls to those methods and registerize the\n+// struct fields independently.\n+type heapBits struct {\n+\tbitp  *uint8\n+\tshift uint32\n+}\n+\n+// markBits provides access to the mark bit for an object in the heap.\n+// bytep points to the byte holding the mark bit.\n+// mask is a byte with a single bit set that can be &ed with *bytep\n+// to see if the bit has been set.\n+// *m.byte&m.mask != 0 indicates the mark bit is set.\n+// index can be used along with span information to generate\n+// the address of the object in the heap.\n+// We maintain one set of mark bits for allocation and one for\n+// marking purposes.\n+type markBits struct {\n+\tbytep *uint8\n+\tmask  uint8\n+\tindex uintptr\n+}\n+\n+//go:nosplit\n+func (s *mspan) allocBitsForIndex(allocBitIndex uintptr) markBits {\n+\twhichByte := allocBitIndex / 8\n+\twhichBit := allocBitIndex % 8\n+\tbytePtr := addb(s.allocBits, whichByte)\n+\treturn markBits{bytePtr, uint8(1 << whichBit), allocBitIndex}\n+}\n+\n+// refillaCache takes 8 bytes s.allocBits starting at whichByte\n+// and negates them so that ctz (count trailing zeros) instructions\n+// can be used. It then places these 8 bytes into the cached 64 bit\n+// s.allocCache.\n+func (s *mspan) refillAllocCache(whichByte uintptr) {\n+\tbytes := (*[8]uint8)(unsafe.Pointer(addb(s.allocBits, whichByte)))\n+\taCache := uint64(0)\n+\taCache |= uint64(bytes[0])\n+\taCache |= uint64(bytes[1]) << (1 * 8)\n+\taCache |= uint64(bytes[2]) << (2 * 8)\n+\taCache |= uint64(bytes[3]) << (3 * 8)\n+\taCache |= uint64(bytes[4]) << (4 * 8)\n+\taCache |= uint64(bytes[5]) << (5 * 8)\n+\taCache |= uint64(bytes[6]) << (6 * 8)\n+\taCache |= uint64(bytes[7]) << (7 * 8)\n+\ts.allocCache = ^aCache\n+}\n+\n+// nextFreeIndex returns the index of the next free object in s at\n+// or after s.freeindex.\n+// There are hardware instructions that can be used to make this\n+// faster if profiling warrants it.\n+func (s *mspan) nextFreeIndex() uintptr {\n+\tsfreeindex := s.freeindex\n+\tsnelems := s.nelems\n+\tif sfreeindex == snelems {\n+\t\treturn sfreeindex\n+\t}\n+\tif sfreeindex > snelems {\n+\t\tthrow(\"s.freeindex > s.nelems\")\n+\t}\n+\n+\taCache := s.allocCache\n+\n+\tbitIndex := sys.Ctz64(aCache)\n+\tfor bitIndex == 64 {\n+\t\t// Move index to start of next cached bits.\n+\t\tsfreeindex = (sfreeindex + 64) &^ (64 - 1)\n+\t\tif sfreeindex >= snelems {\n+\t\t\ts.freeindex = snelems\n+\t\t\treturn snelems\n+\t\t}\n+\t\twhichByte := sfreeindex / 8\n+\t\t// Refill s.allocCache with the next 64 alloc bits.\n+\t\ts.refillAllocCache(whichByte)\n+\t\taCache = s.allocCache\n+\t\tbitIndex = sys.Ctz64(aCache)\n+\t\t// nothing available in cached bits\n+\t\t// grab the next 8 bytes and try again.\n+\t}\n+\tresult := sfreeindex + uintptr(bitIndex)\n+\tif result >= snelems {\n+\t\ts.freeindex = snelems\n+\t\treturn snelems\n+\t}\n+\n+\ts.allocCache >>= (bitIndex + 1)\n+\tsfreeindex = result + 1\n+\n+\tif sfreeindex%64 == 0 && sfreeindex != snelems {\n+\t\t// We just incremented s.freeindex so it isn't 0.\n+\t\t// As each 1 in s.allocCache was encountered and used for allocation\n+\t\t// it was shifted away. At this point s.allocCache contains all 0s.\n+\t\t// Refill s.allocCache so that it corresponds\n+\t\t// to the bits at s.allocBits starting at s.freeindex.\n+\t\twhichByte := sfreeindex / 8\n+\t\ts.refillAllocCache(whichByte)\n+\t}\n+\ts.freeindex = sfreeindex\n+\treturn result\n+}\n+\n+// isFree returns whether the index'th object in s is unallocated.\n+func (s *mspan) isFree(index uintptr) bool {\n+\tif index < s.freeindex {\n+\t\treturn false\n+\t}\n+\twhichByte := index / 8\n+\twhichBit := index % 8\n+\tbyteVal := *addb(s.allocBits, whichByte)\n+\treturn byteVal&uint8(1<<whichBit) == 0\n+}\n+\n+func (s *mspan) objIndex(p uintptr) uintptr {\n+\tbyteOffset := p - s.base()\n+\tif byteOffset == 0 {\n+\t\treturn 0\n+\t}\n+\tif s.baseMask != 0 {\n+\t\t// s.baseMask is 0, elemsize is a power of two, so shift by s.divShift\n+\t\treturn byteOffset >> s.divShift\n+\t}\n+\treturn uintptr(((uint64(byteOffset) >> s.divShift) * uint64(s.divMul)) >> s.divShift2)\n+}\n+\n+func markBitsForAddr(p uintptr) markBits {\n+\ts := spanOf(p)\n+\tobjIndex := s.objIndex(p)\n+\treturn s.markBitsForIndex(objIndex)\n+}\n+\n+func (s *mspan) markBitsForIndex(objIndex uintptr) markBits {\n+\twhichByte := objIndex / 8\n+\tbitMask := uint8(1 << (objIndex % 8)) // low 3 bits hold the bit index\n+\tbytePtr := addb(s.gcmarkBits, whichByte)\n+\treturn markBits{bytePtr, bitMask, objIndex}\n+}\n+\n+func (s *mspan) markBitsForBase() markBits {\n+\treturn markBits{s.gcmarkBits, uint8(1), 0}\n+}\n+\n+// isMarked reports whether mark bit m is set.\n+func (m markBits) isMarked() bool {\n+\treturn *m.bytep&m.mask != 0\n+}\n+\n+// setMarked sets the marked bit in the markbits, atomically. Some compilers\n+// are not able to inline atomic.Or8 function so if it appears as a hot spot consider\n+// inlining it manually.\n+func (m markBits) setMarked() {\n+\t// Might be racing with other updates, so use atomic update always.\n+\t// We used to be clever here and use a non-atomic update in certain\n+\t// cases, but it's not worth the risk.\n+\tatomic.Or8(m.bytep, m.mask)\n+}\n+\n+// setMarkedNonAtomic sets the marked bit in the markbits, non-atomically.\n+func (m markBits) setMarkedNonAtomic() {\n+\t*m.bytep |= m.mask\n+}\n+\n+// clearMarked clears the marked bit in the markbits, atomically.\n+func (m markBits) clearMarked() {\n+\t// Might be racing with other updates, so use atomic update always.\n+\t// We used to be clever here and use a non-atomic update in certain\n+\t// cases, but it's not worth the risk.\n+\tatomic.And8(m.bytep, ^m.mask)\n+}\n+\n+// clearMarkedNonAtomic clears the marked bit non-atomically.\n+func (m markBits) clearMarkedNonAtomic() {\n+\t*m.bytep ^= m.mask\n+}\n+\n+// markBitsForSpan returns the markBits for the span base address base.\n+func markBitsForSpan(base uintptr) (mbits markBits) {\n+\tif base < mheap_.arena_start || base >= mheap_.arena_used {\n+\t\tthrow(\"markBitsForSpan: base out of range\")\n+\t}\n+\tmbits = markBitsForAddr(base)\n+\tif mbits.mask != 1 {\n+\t\tthrow(\"markBitsForSpan: unaligned start\")\n+\t}\n+\treturn mbits\n+}\n+\n+// advance advances the markBits to the next object in the span.\n+func (m *markBits) advance() {\n+\tif m.mask == 1<<7 {\n+\t\tm.bytep = (*uint8)(unsafe.Pointer(uintptr(unsafe.Pointer(m.bytep)) + 1))\n+\t\tm.mask = 1\n+\t} else {\n+\t\tm.mask = m.mask << 1\n+\t}\n+\tm.index++\n+}\n+\n+// heapBitsForAddr returns the heapBits for the address addr.\n+// The caller must have already checked that addr is in the range [mheap_.arena_start, mheap_.arena_used).\n+//\n+// nosplit because it is used during write barriers and must not be preempted.\n+//go:nosplit\n+func heapBitsForAddr(addr uintptr) heapBits {\n+\t// 2 bits per work, 4 pairs per byte, and a mask is hard coded.\n+\toff := (addr - mheap_.arena_start) / sys.PtrSize\n+\treturn heapBits{(*uint8)(unsafe.Pointer(mheap_.bitmap - off/4 - 1)), uint32(off & 3)}\n+}\n+\n+// heapBitsForSpan returns the heapBits for the span base address base.\n+func heapBitsForSpan(base uintptr) (hbits heapBits) {\n+\tif base < mheap_.arena_start || base >= mheap_.arena_used {\n+\t\tthrow(\"heapBitsForSpan: base out of range\")\n+\t}\n+\treturn heapBitsForAddr(base)\n+}\n+\n+// heapBitsForObject returns the base address for the heap object\n+// containing the address p, the heapBits for base,\n+// the object's span, and of the index of the object in s.\n+// If p does not point into a heap object,\n+// return base == 0\n+// otherwise return the base of the object.\n+//\n+// For gccgo, the forStack parameter is true if the value came from the stack.\n+// The stack is collected conservatively and may contain invalid pointers.\n+//\n+// refBase and refOff optionally give the base address of the object\n+// in which the pointer p was found and the byte offset at which it\n+// was found. These are used for error reporting.\n+func heapBitsForObject(p, refBase, refOff uintptr, forStack bool) (base uintptr, hbits heapBits, s *mspan, objIndex uintptr) {\n+\tarenaStart := mheap_.arena_start\n+\tif p < arenaStart || p >= mheap_.arena_used {\n+\t\treturn\n+\t}\n+\toff := p - arenaStart\n+\tidx := off >> _PageShift\n+\t// p points into the heap, but possibly to the middle of an object.\n+\t// Consult the span table to find the block beginning.\n+\ts = mheap_.spans[idx]\n+\tif s == nil || p < s.base() || p >= s.limit || s.state != mSpanInUse {\n+\t\tif s == nil || s.state == _MSpanStack || forStack {\n+\t\t\t// If s is nil, the virtual address has never been part of the heap.\n+\t\t\t// This pointer may be to some mmap'd region, so we allow it.\n+\t\t\t// Pointers into stacks are also ok, the runtime manages these explicitly.\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// The following ensures that we are rigorous about what data\n+\t\t// structures hold valid pointers.\n+\t\tif debug.invalidptr != 0 {\n+\t\t\t// Typically this indicates an incorrect use\n+\t\t\t// of unsafe or cgo to store a bad pointer in\n+\t\t\t// the Go heap. It may also indicate a runtime\n+\t\t\t// bug.\n+\t\t\t//\n+\t\t\t// TODO(austin): We could be more aggressive\n+\t\t\t// and detect pointers to unallocated objects\n+\t\t\t// in allocated spans.\n+\t\t\tprintlock()\n+\t\t\tprint(\"runtime: pointer \", hex(p))\n+\t\t\tif s.state != mSpanInUse {\n+\t\t\t\tprint(\" to unallocated span\")\n+\t\t\t} else {\n+\t\t\t\tprint(\" to unused region of span\")\n+\t\t\t}\n+\t\t\tprint(\" idx=\", hex(idx), \" span.base()=\", hex(s.base()), \" span.limit=\", hex(s.limit), \" span.state=\", s.state, \"\\n\")\n+\t\t\tif refBase != 0 {\n+\t\t\t\tprint(\"runtime: found in object at *(\", hex(refBase), \"+\", hex(refOff), \")\\n\")\n+\t\t\t\tgcDumpObject(\"object\", refBase, refOff)\n+\t\t\t}\n+\t\t\tthrow(\"found bad pointer in Go heap (incorrect use of unsafe or cgo?)\")\n+\t\t}\n+\t\treturn\n+\t}\n+\n+\tif forStack {\n+\t\t// A span can be entered in mheap_.spans, and be set\n+\t\t// to mSpanInUse, before it is fully initialized.\n+\t\t// All we need in practice is allocBits and gcmarkBits,\n+\t\t// so make sure they are set.\n+\t\tif s.allocBits == nil || s.gcmarkBits == nil {\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\t// If this span holds object of a power of 2 size, just mask off the bits to\n+\t// the interior of the object. Otherwise use the size to get the base.\n+\tif s.baseMask != 0 {\n+\t\t// optimize for power of 2 sized objects.\n+\t\tbase = s.base()\n+\t\tbase = base + (p-base)&uintptr(s.baseMask)\n+\t\tobjIndex = (base - s.base()) >> s.divShift\n+\t\t// base = p & s.baseMask is faster for small spans,\n+\t\t// but doesn't work for large spans.\n+\t\t// Overall, it's faster to use the more general computation above.\n+\t} else {\n+\t\tbase = s.base()\n+\t\tif p-base >= s.elemsize {\n+\t\t\t// n := (p - base) / s.elemsize, using division by multiplication\n+\t\t\tobjIndex = uintptr(p-base) >> s.divShift * uintptr(s.divMul) >> s.divShift2\n+\t\t\tbase += objIndex * s.elemsize\n+\t\t}\n+\t}\n+\t// Now that we know the actual base, compute heapBits to return to caller.\n+\thbits = heapBitsForAddr(base)\n+\treturn\n+}\n+\n+// prefetch the bits.\n+func (h heapBits) prefetch() {\n+\tprefetchnta(uintptr(unsafe.Pointer((h.bitp))))\n+}\n+\n+// next returns the heapBits describing the next pointer-sized word in memory.\n+// That is, if h describes address p, h.next() describes p+ptrSize.\n+// Note that next does not modify h. The caller must record the result.\n+//\n+// nosplit because it is used during write barriers and must not be preempted.\n+//go:nosplit\n+func (h heapBits) next() heapBits {\n+\tif h.shift < 3*heapBitsShift {\n+\t\treturn heapBits{h.bitp, h.shift + heapBitsShift}\n+\t}\n+\treturn heapBits{subtract1(h.bitp), 0}\n+}\n+\n+// forward returns the heapBits describing n pointer-sized words ahead of h in memory.\n+// That is, if h describes address p, h.forward(n) describes p+n*ptrSize.\n+// h.forward(1) is equivalent to h.next(), just slower.\n+// Note that forward does not modify h. The caller must record the result.\n+// bits returns the heap bits for the current word.\n+func (h heapBits) forward(n uintptr) heapBits {\n+\tn += uintptr(h.shift) / heapBitsShift\n+\treturn heapBits{subtractb(h.bitp, n/4), uint32(n%4) * heapBitsShift}\n+}\n+\n+// The caller can test morePointers and isPointer by &-ing with bitScan and bitPointer.\n+// The result includes in its higher bits the bits for subsequent words\n+// described by the same bitmap byte.\n+func (h heapBits) bits() uint32 {\n+\t// The (shift & 31) eliminates a test and conditional branch\n+\t// from the generated code.\n+\treturn uint32(*h.bitp) >> (h.shift & 31)\n+}\n+\n+// morePointers returns true if this word and all remaining words in this object\n+// are scalars.\n+// h must not describe the second word of the object.\n+func (h heapBits) morePointers() bool {\n+\treturn h.bits()&bitScan != 0\n+}\n+\n+// isPointer reports whether the heap bits describe a pointer word.\n+//\n+// nosplit because it is used during write barriers and must not be preempted.\n+//go:nosplit\n+func (h heapBits) isPointer() bool {\n+\treturn h.bits()&bitPointer != 0\n+}\n+\n+// hasPointers reports whether the given object has any pointers.\n+// It must be told how large the object at h is for efficiency.\n+// h must describe the initial word of the object.\n+func (h heapBits) hasPointers(size uintptr) bool {\n+\tif size == sys.PtrSize { // 1-word objects are always pointers\n+\t\treturn true\n+\t}\n+\treturn (*h.bitp>>h.shift)&bitScan != 0\n+}\n+\n+// isCheckmarked reports whether the heap bits have the checkmarked bit set.\n+// It must be told how large the object at h is, because the encoding of the\n+// checkmark bit varies by size.\n+// h must describe the initial word of the object.\n+func (h heapBits) isCheckmarked(size uintptr) bool {\n+\tif size == sys.PtrSize {\n+\t\treturn (*h.bitp>>h.shift)&bitPointer != 0\n+\t}\n+\t// All multiword objects are 2-word aligned,\n+\t// so we know that the initial word's 2-bit pair\n+\t// and the second word's 2-bit pair are in the\n+\t// same heap bitmap byte, *h.bitp.\n+\treturn (*h.bitp>>(heapBitsShift+h.shift))&bitScan != 0\n+}\n+\n+// setCheckmarked sets the checkmarked bit.\n+// It must be told how large the object at h is, because the encoding of the\n+// checkmark bit varies by size.\n+// h must describe the initial word of the object.\n+func (h heapBits) setCheckmarked(size uintptr) {\n+\tif size == sys.PtrSize {\n+\t\tatomic.Or8(h.bitp, bitPointer<<h.shift)\n+\t\treturn\n+\t}\n+\tatomic.Or8(h.bitp, bitScan<<(heapBitsShift+h.shift))\n+}\n+\n+// bulkBarrierPreWrite executes writebarrierptr_prewrite1\n+// for every pointer slot in the memory range [src, src+size),\n+// using pointer/scalar information from [dst, dst+size).\n+// This executes the write barriers necessary before a memmove.\n+// src, dst, and size must be pointer-aligned.\n+// The range [dst, dst+size) must lie within a single object.\n+//\n+// As a special case, src == 0 indicates that this is being used for a\n+// memclr. bulkBarrierPreWrite will pass 0 for the src of each write\n+// barrier.\n+//\n+// Callers should call bulkBarrierPreWrite immediately before\n+// calling memmove(dst, src, size). This function is marked nosplit\n+// to avoid being preempted; the GC must not stop the goroutine\n+// between the memmove and the execution of the barriers.\n+// The caller is also responsible for cgo pointer checks if this\n+// may be writing Go pointers into non-Go memory.\n+//\n+// The pointer bitmap is not maintained for allocations containing\n+// no pointers at all; any caller of bulkBarrierPreWrite must first\n+// make sure the underlying allocation contains pointers, usually\n+// by checking typ.kind&kindNoPointers.\n+//\n+//go:nosplit\n+func bulkBarrierPreWrite(dst, src, size uintptr) {\n+\tif (dst|src|size)&(sys.PtrSize-1) != 0 {\n+\t\tthrow(\"bulkBarrierPreWrite: unaligned arguments\")\n+\t}\n+\tif !writeBarrier.needed {\n+\t\treturn\n+\t}\n+\tif !inheap(dst) {\n+\t\t// If dst is a global, use the data or BSS bitmaps to\n+\t\t// execute write barriers.\n+\t\troots := gcRoots\n+\t\tfor roots != nil {\n+\t\t\tfor i := 0; i < roots.count; i++ {\n+\t\t\t\tpr := roots.roots[i]\n+\t\t\t\taddr := uintptr(pr.decl)\n+\t\t\t\tif addr <= dst && dst < addr+pr.size {\n+\t\t\t\t\tif dst < addr+pr.ptrdata {\n+\t\t\t\t\t\tbulkBarrierBitmap(dst, src, size, dst-addr, pr.gcdata)\n+\t\t\t\t\t}\n+\t\t\t\t\treturn\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\troots = roots.next\n+\t\t}\n+\t\treturn\n+\t}\n+\n+\th := heapBitsForAddr(dst)\n+\tif src == 0 {\n+\t\tfor i := uintptr(0); i < size; i += sys.PtrSize {\n+\t\t\tif h.isPointer() {\n+\t\t\t\tdstx := (*uintptr)(unsafe.Pointer(dst + i))\n+\t\t\t\twritebarrierptr_prewrite1(dstx, 0)\n+\t\t\t}\n+\t\t\th = h.next()\n+\t\t}\n+\t} else {\n+\t\tfor i := uintptr(0); i < size; i += sys.PtrSize {\n+\t\t\tif h.isPointer() {\n+\t\t\t\tdstx := (*uintptr)(unsafe.Pointer(dst + i))\n+\t\t\t\tsrcx := (*uintptr)(unsafe.Pointer(src + i))\n+\t\t\t\twritebarrierptr_prewrite1(dstx, *srcx)\n+\t\t\t}\n+\t\t\th = h.next()\n+\t\t}\n+\t}\n+}\n+\n+// bulkBarrierBitmap executes write barriers for copying from [src,\n+// src+size) to [dst, dst+size) using a 1-bit pointer bitmap. src is\n+// assumed to start maskOffset bytes into the data covered by the\n+// bitmap in bits (which may not be a multiple of 8).\n+//\n+// This is used by bulkBarrierPreWrite for writes to data and BSS.\n+//\n+//go:nosplit\n+func bulkBarrierBitmap(dst, src, size, maskOffset uintptr, bits *uint8) {\n+\tword := maskOffset / sys.PtrSize\n+\tbits = addb(bits, word/8)\n+\tmask := uint8(1) << (word % 8)\n+\n+\tfor i := uintptr(0); i < size; i += sys.PtrSize {\n+\t\tif mask == 0 {\n+\t\t\tbits = addb(bits, 1)\n+\t\t\tif *bits == 0 {\n+\t\t\t\t// Skip 8 words.\n+\t\t\t\ti += 7 * sys.PtrSize\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tmask = 1\n+\t\t}\n+\t\tif *bits&mask != 0 {\n+\t\t\tdstx := (*uintptr)(unsafe.Pointer(dst + i))\n+\t\t\tif src == 0 {\n+\t\t\t\twritebarrierptr_prewrite1(dstx, 0)\n+\t\t\t} else {\n+\t\t\t\tsrcx := (*uintptr)(unsafe.Pointer(src + i))\n+\t\t\t\twritebarrierptr_prewrite1(dstx, *srcx)\n+\t\t\t}\n+\t\t}\n+\t\tmask <<= 1\n+\t}\n+}\n+\n+// typeBitsBulkBarrier executes writebarrierptr_prewrite for every\n+// pointer that would be copied from [src, src+size) to [dst,\n+// dst+size) by a memmove using the type bitmap to locate those\n+// pointer slots.\n+//\n+// The type typ must correspond exactly to [src, src+size) and [dst, dst+size).\n+// dst, src, and size must be pointer-aligned.\n+// The type typ must have a plain bitmap, not a GC program.\n+// The only use of this function is in channel sends, and the\n+// 64 kB channel element limit takes care of this for us.\n+//\n+// Must not be preempted because it typically runs right before memmove,\n+// and the GC must observe them as an atomic action.\n+//\n+//go:nosplit\n+func typeBitsBulkBarrier(typ *_type, dst, src, size uintptr) {\n+\tif typ == nil {\n+\t\tthrow(\"runtime: typeBitsBulkBarrier without type\")\n+\t}\n+\tif typ.size != size {\n+\t\tprintln(\"runtime: typeBitsBulkBarrier with type \", *typ.string, \" of size \", typ.size, \" but memory size\", size)\n+\t\tthrow(\"runtime: invalid typeBitsBulkBarrier\")\n+\t}\n+\tif typ.kind&kindGCProg != 0 {\n+\t\tprintln(\"runtime: typeBitsBulkBarrier with type \", *typ.string, \" with GC prog\")\n+\t\tthrow(\"runtime: invalid typeBitsBulkBarrier\")\n+\t}\n+\tif !writeBarrier.needed {\n+\t\treturn\n+\t}\n+\tptrmask := typ.gcdata\n+\tvar bits uint32\n+\tfor i := uintptr(0); i < typ.ptrdata; i += sys.PtrSize {\n+\t\tif i&(sys.PtrSize*8-1) == 0 {\n+\t\t\tbits = uint32(*ptrmask)\n+\t\t\tptrmask = addb(ptrmask, 1)\n+\t\t} else {\n+\t\t\tbits = bits >> 1\n+\t\t}\n+\t\tif bits&1 != 0 {\n+\t\t\tdstx := (*uintptr)(unsafe.Pointer(dst + i))\n+\t\t\tsrcx := (*uintptr)(unsafe.Pointer(src + i))\n+\t\t\twritebarrierptr_prewrite(dstx, *srcx)\n+\t\t}\n+\t}\n+}\n+\n+// The methods operating on spans all require that h has been returned\n+// by heapBitsForSpan and that size, n, total are the span layout description\n+// returned by the mspan's layout method.\n+// If total > size*n, it means that there is extra leftover memory in the span,\n+// usually due to rounding.\n+//\n+// TODO(rsc): Perhaps introduce a different heapBitsSpan type.\n+\n+// initSpan initializes the heap bitmap for a span.\n+// It clears all checkmark bits.\n+// If this is a span of pointer-sized objects, it initializes all\n+// words to pointer/scan.\n+// Otherwise, it initializes all words to scalar/dead.\n+func (h heapBits) initSpan(s *mspan) {\n+\tsize, n, total := s.layout()\n+\n+\t// Init the markbit structures\n+\ts.freeindex = 0\n+\ts.allocCache = ^uint64(0) // all 1s indicating all free.\n+\ts.nelems = n\n+\ts.allocBits = nil\n+\ts.gcmarkBits = nil\n+\ts.gcmarkBits = newMarkBits(s.nelems)\n+\ts.allocBits = newAllocBits(s.nelems)\n+\n+\t// Clear bits corresponding to objects.\n+\tif total%heapBitmapScale != 0 {\n+\t\tthrow(\"initSpan: unaligned length\")\n+\t}\n+\tnbyte := total / heapBitmapScale\n+\tif sys.PtrSize == 8 && size == sys.PtrSize {\n+\t\tend := h.bitp\n+\t\tbitp := subtractb(end, nbyte-1)\n+\t\tfor {\n+\t\t\t*bitp = bitPointerAll | bitScanAll\n+\t\t\tif bitp == end {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tbitp = add1(bitp)\n+\t\t}\n+\t\treturn\n+\t}\n+\tmemclrNoHeapPointers(unsafe.Pointer(subtractb(h.bitp, nbyte-1)), nbyte)\n+}\n+\n+// initCheckmarkSpan initializes a span for being checkmarked.\n+// It clears the checkmark bits, which are set to 1 in normal operation.\n+func (h heapBits) initCheckmarkSpan(size, n, total uintptr) {\n+\t// The ptrSize == 8 is a compile-time constant false on 32-bit and eliminates this code entirely.\n+\tif sys.PtrSize == 8 && size == sys.PtrSize {\n+\t\t// Checkmark bit is type bit, bottom bit of every 2-bit entry.\n+\t\t// Only possible on 64-bit system, since minimum size is 8.\n+\t\t// Must clear type bit (checkmark bit) of every word.\n+\t\t// The type bit is the lower of every two-bit pair.\n+\t\tbitp := h.bitp\n+\t\tfor i := uintptr(0); i < n; i += 4 {\n+\t\t\t*bitp &^= bitPointerAll\n+\t\t\tbitp = subtract1(bitp)\n+\t\t}\n+\t\treturn\n+\t}\n+\tfor i := uintptr(0); i < n; i++ {\n+\t\t*h.bitp &^= bitScan << (heapBitsShift + h.shift)\n+\t\th = h.forward(size / sys.PtrSize)\n+\t}\n+}\n+\n+// clearCheckmarkSpan undoes all the checkmarking in a span.\n+// The actual checkmark bits are ignored, so the only work to do\n+// is to fix the pointer bits. (Pointer bits are ignored by scanobject\n+// but consulted by typedmemmove.)\n+func (h heapBits) clearCheckmarkSpan(size, n, total uintptr) {\n+\t// The ptrSize == 8 is a compile-time constant false on 32-bit and eliminates this code entirely.\n+\tif sys.PtrSize == 8 && size == sys.PtrSize {\n+\t\t// Checkmark bit is type bit, bottom bit of every 2-bit entry.\n+\t\t// Only possible on 64-bit system, since minimum size is 8.\n+\t\t// Must clear type bit (checkmark bit) of every word.\n+\t\t// The type bit is the lower of every two-bit pair.\n+\t\tbitp := h.bitp\n+\t\tfor i := uintptr(0); i < n; i += 4 {\n+\t\t\t*bitp |= bitPointerAll\n+\t\t\tbitp = subtract1(bitp)\n+\t\t}\n+\t}\n+}\n+\n+// oneBitCount is indexed by byte and produces the\n+// number of 1 bits in that byte. For example 128 has 1 bit set\n+// and oneBitCount[128] will holds 1.\n+var oneBitCount = [256]uint8{\n+\t0, 1, 1, 2, 1, 2, 2, 3,\n+\t1, 2, 2, 3, 2, 3, 3, 4,\n+\t1, 2, 2, 3, 2, 3, 3, 4,\n+\t2, 3, 3, 4, 3, 4, 4, 5,\n+\t1, 2, 2, 3, 2, 3, 3, 4,\n+\t2, 3, 3, 4, 3, 4, 4, 5,\n+\t2, 3, 3, 4, 3, 4, 4, 5,\n+\t3, 4, 4, 5, 4, 5, 5, 6,\n+\t1, 2, 2, 3, 2, 3, 3, 4,\n+\t2, 3, 3, 4, 3, 4, 4, 5,\n+\t2, 3, 3, 4, 3, 4, 4, 5,\n+\t3, 4, 4, 5, 4, 5, 5, 6,\n+\t2, 3, 3, 4, 3, 4, 4, 5,\n+\t3, 4, 4, 5, 4, 5, 5, 6,\n+\t3, 4, 4, 5, 4, 5, 5, 6,\n+\t4, 5, 5, 6, 5, 6, 6, 7,\n+\t1, 2, 2, 3, 2, 3, 3, 4,\n+\t2, 3, 3, 4, 3, 4, 4, 5,\n+\t2, 3, 3, 4, 3, 4, 4, 5,\n+\t3, 4, 4, 5, 4, 5, 5, 6,\n+\t2, 3, 3, 4, 3, 4, 4, 5,\n+\t3, 4, 4, 5, 4, 5, 5, 6,\n+\t3, 4, 4, 5, 4, 5, 5, 6,\n+\t4, 5, 5, 6, 5, 6, 6, 7,\n+\t2, 3, 3, 4, 3, 4, 4, 5,\n+\t3, 4, 4, 5, 4, 5, 5, 6,\n+\t3, 4, 4, 5, 4, 5, 5, 6,\n+\t4, 5, 5, 6, 5, 6, 6, 7,\n+\t3, 4, 4, 5, 4, 5, 5, 6,\n+\t4, 5, 5, 6, 5, 6, 6, 7,\n+\t4, 5, 5, 6, 5, 6, 6, 7,\n+\t5, 6, 6, 7, 6, 7, 7, 8}\n+\n+// countFree runs through the mark bits in a span and counts the number of free objects\n+// in the span.\n+// TODO:(rlh) Use popcount intrinsic.\n+func (s *mspan) countFree() int {\n+\tcount := 0\n+\tmaxIndex := s.nelems / 8\n+\tfor i := uintptr(0); i < maxIndex; i++ {\n+\t\tmrkBits := *addb(s.gcmarkBits, i)\n+\t\tcount += int(oneBitCount[mrkBits])\n+\t}\n+\tif bitsInLastByte := s.nelems % 8; bitsInLastByte != 0 {\n+\t\tmrkBits := *addb(s.gcmarkBits, maxIndex)\n+\t\tmask := uint8((1 << bitsInLastByte) - 1)\n+\t\tbits := mrkBits & mask\n+\t\tcount += int(oneBitCount[bits])\n+\t}\n+\treturn int(s.nelems) - count\n+}\n+\n+// heapBitsSetType records that the new allocation [x, x+size)\n+// holds in [x, x+dataSize) one or more values of type typ.\n+// (The number of values is given by dataSize / typ.size.)\n+// If dataSize < size, the fragment [x+dataSize, x+size) is\n+// recorded as non-pointer data.\n+// It is known that the type has pointers somewhere;\n+// malloc does not call heapBitsSetType when there are no pointers,\n+// because all free objects are marked as noscan during\n+// heapBitsSweepSpan.\n+//\n+// There can only be one allocation from a given span active at a time,\n+// and the bitmap for a span always falls on byte boundaries,\n+// so there are no write-write races for access to the heap bitmap.\n+// Hence, heapBitsSetType can access the bitmap without atomics.\n+//\n+// There can be read-write races between heapBitsSetType and things\n+// that read the heap bitmap like scanobject. However, since\n+// heapBitsSetType is only used for objects that have not yet been\n+// made reachable, readers will ignore bits being modified by this\n+// function. This does mean this function cannot transiently modify\n+// bits that belong to neighboring objects. Also, on weakly-ordered\n+// machines, callers must execute a store/store (publication) barrier\n+// between calling this function and making the object reachable.\n+func heapBitsSetType(x, size, dataSize uintptr, typ *_type) {\n+\tconst doubleCheck = false // slow but helpful; enable to test modifications to this code\n+\n+\t// dataSize is always size rounded up to the next malloc size class,\n+\t// except in the case of allocating a defer block, in which case\n+\t// size is sizeof(_defer{}) (at least 6 words) and dataSize may be\n+\t// arbitrarily larger.\n+\t//\n+\t// The checks for size == sys.PtrSize and size == 2*sys.PtrSize can therefore\n+\t// assume that dataSize == size without checking it explicitly.\n+\n+\tif sys.PtrSize == 8 && size == sys.PtrSize {\n+\t\t// It's one word and it has pointers, it must be a pointer.\n+\t\t// Since all allocated one-word objects are pointers\n+\t\t// (non-pointers are aggregated into tinySize allocations),\n+\t\t// initSpan sets the pointer bits for us. Nothing to do here.\n+\t\tif doubleCheck {\n+\t\t\th := heapBitsForAddr(x)\n+\t\t\tif !h.isPointer() {\n+\t\t\t\tthrow(\"heapBitsSetType: pointer bit missing\")\n+\t\t\t}\n+\t\t\tif !h.morePointers() {\n+\t\t\t\tthrow(\"heapBitsSetType: scan bit missing\")\n+\t\t\t}\n+\t\t}\n+\t\treturn\n+\t}\n+\n+\th := heapBitsForAddr(x)\n+\tptrmask := typ.gcdata // start of 1-bit pointer mask (or GC program, handled below)\n+\n+\t// Heap bitmap bits for 2-word object are only 4 bits,\n+\t// so also shared with objects next to it.\n+\t// This is called out as a special case primarily for 32-bit systems,\n+\t// so that on 32-bit systems the code below can assume all objects\n+\t// are 4-word aligned (because they're all 16-byte aligned).\n+\tif size == 2*sys.PtrSize {\n+\t\tif typ.size == sys.PtrSize {\n+\t\t\t// We're allocating a block big enough to hold two pointers.\n+\t\t\t// On 64-bit, that means the actual object must be two pointers,\n+\t\t\t// or else we'd have used the one-pointer-sized block.\n+\t\t\t// On 32-bit, however, this is the 8-byte block, the smallest one.\n+\t\t\t// So it could be that we're allocating one pointer and this was\n+\t\t\t// just the smallest block available. Distinguish by checking dataSize.\n+\t\t\t// (In general the number of instances of typ being allocated is\n+\t\t\t// dataSize/typ.size.)\n+\t\t\tif sys.PtrSize == 4 && dataSize == sys.PtrSize {\n+\t\t\t\t// 1 pointer object. On 32-bit machines clear the bit for the\n+\t\t\t\t// unused second word.\n+\t\t\t\t*h.bitp &^= (bitPointer | bitScan | ((bitPointer | bitScan) << heapBitsShift)) << h.shift\n+\t\t\t\t*h.bitp |= (bitPointer | bitScan) << h.shift\n+\t\t\t} else {\n+\t\t\t\t// 2-element slice of pointer.\n+\t\t\t\t*h.bitp |= (bitPointer | bitScan | bitPointer<<heapBitsShift) << h.shift\n+\t\t\t}\n+\t\t\treturn\n+\t\t}\n+\t\t// Otherwise typ.size must be 2*sys.PtrSize,\n+\t\t// and typ.kind&kindGCProg == 0.\n+\t\tif doubleCheck {\n+\t\t\tif typ.size != 2*sys.PtrSize || typ.kind&kindGCProg != 0 {\n+\t\t\t\tprint(\"runtime: heapBitsSetType size=\", size, \" but typ.size=\", typ.size, \" gcprog=\", typ.kind&kindGCProg != 0, \"\\n\")\n+\t\t\t\tthrow(\"heapBitsSetType\")\n+\t\t\t}\n+\t\t}\n+\t\tb := uint32(*ptrmask)\n+\t\thb := (b & 3) | bitScan\n+\t\t// bitPointer == 1, bitScan is 1 << 4, heapBitsShift is 1.\n+\t\t// 110011 is shifted h.shift and complemented.\n+\t\t// This clears out the bits that are about to be\n+\t\t// ored into *h.hbitp in the next instructions.\n+\t\t*h.bitp &^= (bitPointer | bitScan | ((bitPointer | bitScan) << heapBitsShift)) << h.shift\n+\t\t*h.bitp |= uint8(hb << h.shift)\n+\t\treturn\n+\t}\n+\n+\t// Copy from 1-bit ptrmask into 2-bit bitmap.\n+\t// The basic approach is to use a single uintptr as a bit buffer,\n+\t// alternating between reloading the buffer and writing bitmap bytes.\n+\t// In general, one load can supply two bitmap byte writes.\n+\t// This is a lot of lines of code, but it compiles into relatively few\n+\t// machine instructions.\n+\n+\tvar (\n+\t\t// Ptrmask input.\n+\t\tp     *byte   // last ptrmask byte read\n+\t\tb     uintptr // ptrmask bits already loaded\n+\t\tnb    uintptr // number of bits in b at next read\n+\t\tendp  *byte   // final ptrmask byte to read (then repeat)\n+\t\tendnb uintptr // number of valid bits in *endp\n+\t\tpbits uintptr // alternate source of bits\n+\n+\t\t// Heap bitmap output.\n+\t\tw     uintptr // words processed\n+\t\tnw    uintptr // number of words to process\n+\t\thbitp *byte   // next heap bitmap byte to write\n+\t\thb    uintptr // bits being prepared for *hbitp\n+\t)\n+\n+\thbitp = h.bitp\n+\n+\t// Handle GC program. Delayed until this part of the code\n+\t// so that we can use the same double-checking mechanism\n+\t// as the 1-bit case. Nothing above could have encountered\n+\t// GC programs: the cases were all too small.\n+\tif typ.kind&kindGCProg != 0 {\n+\t\theapBitsSetTypeGCProg(h, typ.ptrdata, typ.size, dataSize, size, addb(typ.gcdata, 4))\n+\t\tif doubleCheck {\n+\t\t\t// Double-check the heap bits written by GC program\n+\t\t\t// by running the GC program to create a 1-bit pointer mask\n+\t\t\t// and then jumping to the double-check code below.\n+\t\t\t// This doesn't catch bugs shared between the 1-bit and 4-bit\n+\t\t\t// GC program execution, but it does catch mistakes specific\n+\t\t\t// to just one of those and bugs in heapBitsSetTypeGCProg's\n+\t\t\t// implementation of arrays.\n+\t\t\tlock(&debugPtrmask.lock)\n+\t\t\tif debugPtrmask.data == nil {\n+\t\t\t\tdebugPtrmask.data = (*byte)(persistentalloc(1<<20, 1, &memstats.other_sys))\n+\t\t\t}\n+\t\t\tptrmask = debugPtrmask.data\n+\t\t\trunGCProg(addb(typ.gcdata, 4), nil, ptrmask, 1)\n+\t\t\tgoto Phase4\n+\t\t}\n+\t\treturn\n+\t}\n+\n+\t// Note about sizes:\n+\t//\n+\t// typ.size is the number of words in the object,\n+\t// and typ.ptrdata is the number of words in the prefix\n+\t// of the object that contains pointers. That is, the final\n+\t// typ.size - typ.ptrdata words contain no pointers.\n+\t// This allows optimization of a common pattern where\n+\t// an object has a small header followed by a large scalar\n+\t// buffer. If we know the pointers are over, we don't have\n+\t// to scan the buffer's heap bitmap at all.\n+\t// The 1-bit ptrmasks are sized to contain only bits for\n+\t// the typ.ptrdata prefix, zero padded out to a full byte\n+\t// of bitmap. This code sets nw (below) so that heap bitmap\n+\t// bits are only written for the typ.ptrdata prefix; if there is\n+\t// more room in the allocated object, the next heap bitmap\n+\t// entry is a 00, indicating that there are no more pointers\n+\t// to scan. So only the ptrmask for the ptrdata bytes is needed.\n+\t//\n+\t// Replicated copies are not as nice: if there is an array of\n+\t// objects with scalar tails, all but the last tail does have to\n+\t// be initialized, because there is no way to say \"skip forward\".\n+\t// However, because of the possibility of a repeated type with\n+\t// size not a multiple of 4 pointers (one heap bitmap byte),\n+\t// the code already must handle the last ptrmask byte specially\n+\t// by treating it as containing only the bits for endnb pointers,\n+\t// where endnb <= 4. We represent large scalar tails that must\n+\t// be expanded in the replication by setting endnb larger than 4.\n+\t// This will have the effect of reading many bits out of b,\n+\t// but once the real bits are shifted out, b will supply as many\n+\t// zero bits as we try to read, which is exactly what we need.\n+\n+\tp = ptrmask\n+\tif typ.size < dataSize {\n+\t\t// Filling in bits for an array of typ.\n+\t\t// Set up for repetition of ptrmask during main loop.\n+\t\t// Note that ptrmask describes only a prefix of\n+\t\tconst maxBits = sys.PtrSize*8 - 7\n+\t\tif typ.ptrdata/sys.PtrSize <= maxBits {\n+\t\t\t// Entire ptrmask fits in uintptr with room for a byte fragment.\n+\t\t\t// Load into pbits and never read from ptrmask again.\n+\t\t\t// This is especially important when the ptrmask has\n+\t\t\t// fewer than 8 bits in it; otherwise the reload in the middle\n+\t\t\t// of the Phase 2 loop would itself need to loop to gather\n+\t\t\t// at least 8 bits.\n+\n+\t\t\t// Accumulate ptrmask into b.\n+\t\t\t// ptrmask is sized to describe only typ.ptrdata, but we record\n+\t\t\t// it as describing typ.size bytes, since all the high bits are zero.\n+\t\t\tnb = typ.ptrdata / sys.PtrSize\n+\t\t\tfor i := uintptr(0); i < nb; i += 8 {\n+\t\t\t\tb |= uintptr(*p) << i\n+\t\t\t\tp = add1(p)\n+\t\t\t}\n+\t\t\tnb = typ.size / sys.PtrSize\n+\n+\t\t\t// Replicate ptrmask to fill entire pbits uintptr.\n+\t\t\t// Doubling and truncating is fewer steps than\n+\t\t\t// iterating by nb each time. (nb could be 1.)\n+\t\t\t// Since we loaded typ.ptrdata/sys.PtrSize bits\n+\t\t\t// but are pretending to have typ.size/sys.PtrSize,\n+\t\t\t// there might be no replication necessary/possible.\n+\t\t\tpbits = b\n+\t\t\tendnb = nb\n+\t\t\tif nb+nb <= maxBits {\n+\t\t\t\tfor endnb <= sys.PtrSize*8 {\n+\t\t\t\t\tpbits |= pbits << endnb\n+\t\t\t\t\tendnb += endnb\n+\t\t\t\t}\n+\t\t\t\t// Truncate to a multiple of original ptrmask.\n+\t\t\t\tendnb = maxBits / nb * nb\n+\t\t\t\tpbits &= 1<<endnb - 1\n+\t\t\t\tb = pbits\n+\t\t\t\tnb = endnb\n+\t\t\t}\n+\n+\t\t\t// Clear p and endp as sentinel for using pbits.\n+\t\t\t// Checked during Phase 2 loop.\n+\t\t\tp = nil\n+\t\t\tendp = nil\n+\t\t} else {\n+\t\t\t// Ptrmask is larger. Read it multiple times.\n+\t\t\tn := (typ.ptrdata/sys.PtrSize+7)/8 - 1\n+\t\t\tendp = addb(ptrmask, n)\n+\t\t\tendnb = typ.size/sys.PtrSize - n*8\n+\t\t}\n+\t}\n+\tif p != nil {\n+\t\tb = uintptr(*p)\n+\t\tp = add1(p)\n+\t\tnb = 8\n+\t}\n+\n+\tif typ.size == dataSize {\n+\t\t// Single entry: can stop once we reach the non-pointer data.\n+\t\tnw = typ.ptrdata / sys.PtrSize\n+\t} else {\n+\t\t// Repeated instances of typ in an array.\n+\t\t// Have to process first N-1 entries in full, but can stop\n+\t\t// once we reach the non-pointer data in the final entry.\n+\t\tnw = ((dataSize/typ.size-1)*typ.size + typ.ptrdata) / sys.PtrSize\n+\t}\n+\tif nw == 0 {\n+\t\t// No pointers! Caller was supposed to check.\n+\t\tprintln(\"runtime: invalid type \", *typ.string)\n+\t\tthrow(\"heapBitsSetType: called with non-pointer type\")\n+\t\treturn\n+\t}\n+\tif nw < 2 {\n+\t\t// Must write at least 2 words, because the \"no scan\"\n+\t\t// encoding doesn't take effect until the third word.\n+\t\tnw = 2\n+\t}\n+\n+\t// Phase 1: Special case for leading byte (shift==0) or half-byte (shift==4).\n+\t// The leading byte is special because it contains the bits for word 1,\n+\t// which does not have the scan bit set.\n+\t// The leading half-byte is special because it's a half a byte,\n+\t// so we have to be careful with the bits already there.\n+\tswitch {\n+\tdefault:\n+\t\tthrow(\"heapBitsSetType: unexpected shift\")\n+\n+\tcase h.shift == 0:\n+\t\t// Ptrmask and heap bitmap are aligned.\n+\t\t// Handle first byte of bitmap specially.\n+\t\t//\n+\t\t// The first byte we write out covers the first four\n+\t\t// words of the object. The scan/dead bit on the first\n+\t\t// word must be set to scan since there are pointers\n+\t\t// somewhere in the object. The scan/dead bit on the\n+\t\t// second word is the checkmark, so we don't set it.\n+\t\t// In all following words, we set the scan/dead\n+\t\t// appropriately to indicate that the object contains\n+\t\t// to the next 2-bit entry in the bitmap.\n+\t\t//\n+\t\t// TODO: It doesn't matter if we set the checkmark, so\n+\t\t// maybe this case isn't needed any more.\n+\t\thb = b & bitPointerAll\n+\t\thb |= bitScan | bitScan<<(2*heapBitsShift) | bitScan<<(3*heapBitsShift)\n+\t\tif w += 4; w >= nw {\n+\t\t\tgoto Phase3\n+\t\t}\n+\t\t*hbitp = uint8(hb)\n+\t\thbitp = subtract1(hbitp)\n+\t\tb >>= 4\n+\t\tnb -= 4\n+\n+\tcase sys.PtrSize == 8 && h.shift == 2:\n+\t\t// Ptrmask and heap bitmap are misaligned.\n+\t\t// The bits for the first two words are in a byte shared\n+\t\t// with another object, so we must be careful with the bits\n+\t\t// already there.\n+\t\t// We took care of 1-word and 2-word objects above,\n+\t\t// so this is at least a 6-word object.\n+\t\thb = (b & (bitPointer | bitPointer<<heapBitsShift)) << (2 * heapBitsShift)\n+\t\t// This is not noscan, so set the scan bit in the\n+\t\t// first word.\n+\t\thb |= bitScan << (2 * heapBitsShift)\n+\t\tb >>= 2\n+\t\tnb -= 2\n+\t\t// Note: no bitScan for second word because that's\n+\t\t// the checkmark.\n+\t\t*hbitp &^= uint8((bitPointer | bitScan | (bitPointer << heapBitsShift)) << (2 * heapBitsShift))\n+\t\t*hbitp |= uint8(hb)\n+\t\thbitp = subtract1(hbitp)\n+\t\tif w += 2; w >= nw {\n+\t\t\t// We know that there is more data, because we handled 2-word objects above.\n+\t\t\t// This must be at least a 6-word object. If we're out of pointer words,\n+\t\t\t// mark no scan in next bitmap byte and finish.\n+\t\t\thb = 0\n+\t\t\tw += 4\n+\t\t\tgoto Phase3\n+\t\t}\n+\t}\n+\n+\t// Phase 2: Full bytes in bitmap, up to but not including write to last byte (full or partial) in bitmap.\n+\t// The loop computes the bits for that last write but does not execute the write;\n+\t// it leaves the bits in hb for processing by phase 3.\n+\t// To avoid repeated adjustment of nb, we subtract out the 4 bits we're going to\n+\t// use in the first half of the loop right now, and then we only adjust nb explicitly\n+\t// if the 8 bits used by each iteration isn't balanced by 8 bits loaded mid-loop.\n+\tnb -= 4\n+\tfor {\n+\t\t// Emit bitmap byte.\n+\t\t// b has at least nb+4 bits, with one exception:\n+\t\t// if w+4 >= nw, then b has only nw-w bits,\n+\t\t// but we'll stop at the break and then truncate\n+\t\t// appropriately in Phase 3.\n+\t\thb = b & bitPointerAll\n+\t\thb |= bitScanAll\n+\t\tif w += 4; w >= nw {\n+\t\t\tbreak\n+\t\t}\n+\t\t*hbitp = uint8(hb)\n+\t\thbitp = subtract1(hbitp)\n+\t\tb >>= 4\n+\n+\t\t// Load more bits. b has nb right now.\n+\t\tif p != endp {\n+\t\t\t// Fast path: keep reading from ptrmask.\n+\t\t\t// nb unmodified: we just loaded 8 bits,\n+\t\t\t// and the next iteration will consume 8 bits,\n+\t\t\t// leaving us with the same nb the next time we're here.\n+\t\t\tif nb < 8 {\n+\t\t\t\tb |= uintptr(*p) << nb\n+\t\t\t\tp = add1(p)\n+\t\t\t} else {\n+\t\t\t\t// Reduce the number of bits in b.\n+\t\t\t\t// This is important if we skipped\n+\t\t\t\t// over a scalar tail, since nb could\n+\t\t\t\t// be larger than the bit width of b.\n+\t\t\t\tnb -= 8\n+\t\t\t}\n+\t\t} else if p == nil {\n+\t\t\t// Almost as fast path: track bit count and refill from pbits.\n+\t\t\t// For short repetitions.\n+\t\t\tif nb < 8 {\n+\t\t\t\tb |= pbits << nb\n+\t\t\t\tnb += endnb\n+\t\t\t}\n+\t\t\tnb -= 8 // for next iteration\n+\t\t} else {\n+\t\t\t// Slow path: reached end of ptrmask.\n+\t\t\t// Process final partial byte and rewind to start.\n+\t\t\tb |= uintptr(*p) << nb\n+\t\t\tnb += endnb\n+\t\t\tif nb < 8 {\n+\t\t\t\tb |= uintptr(*ptrmask) << nb\n+\t\t\t\tp = add1(ptrmask)\n+\t\t\t} else {\n+\t\t\t\tnb -= 8\n+\t\t\t\tp = ptrmask\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Emit bitmap byte.\n+\t\thb = b & bitPointerAll\n+\t\thb |= bitScanAll\n+\t\tif w += 4; w >= nw {\n+\t\t\tbreak\n+\t\t}\n+\t\t*hbitp = uint8(hb)\n+\t\thbitp = subtract1(hbitp)\n+\t\tb >>= 4\n+\t}\n+\n+Phase3:\n+\t// Phase 3: Write last byte or partial byte and zero the rest of the bitmap entries.\n+\tif w > nw {\n+\t\t// Counting the 4 entries in hb not yet written to memory,\n+\t\t// there are more entries than possible pointer slots.\n+\t\t// Discard the excess entries (can't be more than 3).\n+\t\tmask := uintptr(1)<<(4-(w-nw)) - 1\n+\t\thb &= mask | mask<<4 // apply mask to both pointer bits and scan bits\n+\t}\n+\n+\t// Change nw from counting possibly-pointer words to total words in allocation.\n+\tnw = size / sys.PtrSize\n+\n+\t// Write whole bitmap bytes.\n+\t// The first is hb, the rest are zero.\n+\tif w <= nw {\n+\t\t*hbitp = uint8(hb)\n+\t\thbitp = subtract1(hbitp)\n+\t\thb = 0 // for possible final half-byte below\n+\t\tfor w += 4; w <= nw; w += 4 {\n+\t\t\t*hbitp = 0\n+\t\t\thbitp = subtract1(hbitp)\n+\t\t}\n+\t}\n+\n+\t// Write final partial bitmap byte if any.\n+\t// We know w > nw, or else we'd still be in the loop above.\n+\t// It can be bigger only due to the 4 entries in hb that it counts.\n+\t// If w == nw+4 then there's nothing left to do: we wrote all nw entries\n+\t// and can discard the 4 sitting in hb.\n+\t// But if w == nw+2, we need to write first two in hb.\n+\t// The byte is shared with the next object, so be careful with\n+\t// existing bits.\n+\tif w == nw+2 {\n+\t\t*hbitp = *hbitp&^(bitPointer|bitScan|(bitPointer|bitScan)<<heapBitsShift) | uint8(hb)\n+\t}\n+\n+Phase4:\n+\t// Phase 4: all done, but perhaps double check.\n+\tif doubleCheck {\n+\t\tend := heapBitsForAddr(x + size)\n+\t\tif typ.kind&kindGCProg == 0 && (hbitp != end.bitp || (w == nw+2) != (end.shift == 2)) {\n+\t\t\tprintln(\"ended at wrong bitmap byte for\", *typ.string, \"x\", dataSize/typ.size)\n+\t\t\tprint(\"typ.size=\", typ.size, \" typ.ptrdata=\", typ.ptrdata, \" dataSize=\", dataSize, \" size=\", size, \"\\n\")\n+\t\t\tprint(\"w=\", w, \" nw=\", nw, \" b=\", hex(b), \" nb=\", nb, \" hb=\", hex(hb), \"\\n\")\n+\t\t\th0 := heapBitsForAddr(x)\n+\t\t\tprint(\"initial bits h0.bitp=\", h0.bitp, \" h0.shift=\", h0.shift, \"\\n\")\n+\t\t\tprint(\"ended at hbitp=\", hbitp, \" but next starts at bitp=\", end.bitp, \" shift=\", end.shift, \"\\n\")\n+\t\t\tthrow(\"bad heapBitsSetType\")\n+\t\t}\n+\n+\t\t// Double-check that bits to be written were written correctly.\n+\t\t// Does not check that other bits were not written, unfortunately.\n+\t\th := heapBitsForAddr(x)\n+\t\tnptr := typ.ptrdata / sys.PtrSize\n+\t\tndata := typ.size / sys.PtrSize\n+\t\tcount := dataSize / typ.size\n+\t\ttotalptr := ((count-1)*typ.size + typ.ptrdata) / sys.PtrSize\n+\t\tfor i := uintptr(0); i < size/sys.PtrSize; i++ {\n+\t\t\tj := i % ndata\n+\t\t\tvar have, want uint8\n+\t\t\thave = (*h.bitp >> h.shift) & (bitPointer | bitScan)\n+\t\t\tif i >= totalptr {\n+\t\t\t\twant = 0 // deadmarker\n+\t\t\t\tif typ.kind&kindGCProg != 0 && i < (totalptr+3)/4*4 {\n+\t\t\t\t\twant = bitScan\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tif j < nptr && (*addb(ptrmask, j/8)>>(j%8))&1 != 0 {\n+\t\t\t\t\twant |= bitPointer\n+\t\t\t\t}\n+\t\t\t\tif i != 1 {\n+\t\t\t\t\twant |= bitScan\n+\t\t\t\t} else {\n+\t\t\t\t\thave &^= bitScan\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tif have != want {\n+\t\t\t\tprintln(\"mismatch writing bits for\", *typ.string, \"x\", dataSize/typ.size)\n+\t\t\t\tprint(\"typ.size=\", typ.size, \" typ.ptrdata=\", typ.ptrdata, \" dataSize=\", dataSize, \" size=\", size, \"\\n\")\n+\t\t\t\tprint(\"kindGCProg=\", typ.kind&kindGCProg != 0, \"\\n\")\n+\t\t\t\tprint(\"w=\", w, \" nw=\", nw, \" b=\", hex(b), \" nb=\", nb, \" hb=\", hex(hb), \"\\n\")\n+\t\t\t\th0 := heapBitsForAddr(x)\n+\t\t\t\tprint(\"initial bits h0.bitp=\", h0.bitp, \" h0.shift=\", h0.shift, \"\\n\")\n+\t\t\t\tprint(\"current bits h.bitp=\", h.bitp, \" h.shift=\", h.shift, \" *h.bitp=\", hex(*h.bitp), \"\\n\")\n+\t\t\t\tprint(\"ptrmask=\", ptrmask, \" p=\", p, \" endp=\", endp, \" endnb=\", endnb, \" pbits=\", hex(pbits), \" b=\", hex(b), \" nb=\", nb, \"\\n\")\n+\t\t\t\tprintln(\"at word\", i, \"offset\", i*sys.PtrSize, \"have\", have, \"want\", want)\n+\t\t\t\tif typ.kind&kindGCProg != 0 {\n+\t\t\t\t\tprintln(\"GC program:\")\n+\t\t\t\t\tdumpGCProg(addb(typ.gcdata, 4))\n+\t\t\t\t}\n+\t\t\t\tthrow(\"bad heapBitsSetType\")\n+\t\t\t}\n+\t\t\th = h.next()\n+\t\t}\n+\t\tif ptrmask == debugPtrmask.data {\n+\t\t\tunlock(&debugPtrmask.lock)\n+\t\t}\n+\t}\n+}\n+\n+// heapBitsSetTypeNoScan marks x as noscan by setting the first word\n+// of x in the heap bitmap to scalar/dead.\n+func heapBitsSetTypeNoScan(x uintptr) {\n+\th := heapBitsForAddr(uintptr(x))\n+\t*h.bitp &^= (bitPointer | bitScan) << h.shift\n+}\n+\n+var debugPtrmask struct {\n+\tlock mutex\n+\tdata *byte\n+}\n+\n+// heapBitsSetTypeGCProg implements heapBitsSetType using a GC program.\n+// progSize is the size of the memory described by the program.\n+// elemSize is the size of the element that the GC program describes (a prefix of).\n+// dataSize is the total size of the intended data, a multiple of elemSize.\n+// allocSize is the total size of the allocated memory.\n+//\n+// GC programs are only used for large allocations.\n+// heapBitsSetType requires that allocSize is a multiple of 4 words,\n+// so that the relevant bitmap bytes are not shared with surrounding\n+// objects.\n+func heapBitsSetTypeGCProg(h heapBits, progSize, elemSize, dataSize, allocSize uintptr, prog *byte) {\n+\tif sys.PtrSize == 8 && allocSize%(4*sys.PtrSize) != 0 {\n+\t\t// Alignment will be wrong.\n+\t\tthrow(\"heapBitsSetTypeGCProg: small allocation\")\n+\t}\n+\tvar totalBits uintptr\n+\tif elemSize == dataSize {\n+\t\ttotalBits = runGCProg(prog, nil, h.bitp, 2)\n+\t\tif totalBits*sys.PtrSize != progSize {\n+\t\t\tprintln(\"runtime: heapBitsSetTypeGCProg: total bits\", totalBits, \"but progSize\", progSize)\n+\t\t\tthrow(\"heapBitsSetTypeGCProg: unexpected bit count\")\n+\t\t}\n+\t} else {\n+\t\tcount := dataSize / elemSize\n+\n+\t\t// Piece together program trailer to run after prog that does:\n+\t\t//\tliteral(0)\n+\t\t//\trepeat(1, elemSize-progSize-1) // zeros to fill element size\n+\t\t//\trepeat(elemSize, count-1) // repeat that element for count\n+\t\t// This zero-pads the data remaining in the first element and then\n+\t\t// repeats that first element to fill the array.\n+\t\tvar trailer [40]byte // 3 varints (max 10 each) + some bytes\n+\t\ti := 0\n+\t\tif n := elemSize/sys.PtrSize - progSize/sys.PtrSize; n > 0 {\n+\t\t\t// literal(0)\n+\t\t\ttrailer[i] = 0x01\n+\t\t\ti++\n+\t\t\ttrailer[i] = 0\n+\t\t\ti++\n+\t\t\tif n > 1 {\n+\t\t\t\t// repeat(1, n-1)\n+\t\t\t\ttrailer[i] = 0x81\n+\t\t\t\ti++\n+\t\t\t\tn--\n+\t\t\t\tfor ; n >= 0x80; n >>= 7 {\n+\t\t\t\t\ttrailer[i] = byte(n | 0x80)\n+\t\t\t\t\ti++\n+\t\t\t\t}\n+\t\t\t\ttrailer[i] = byte(n)\n+\t\t\t\ti++\n+\t\t\t}\n+\t\t}\n+\t\t// repeat(elemSize/ptrSize, count-1)\n+\t\ttrailer[i] = 0x80\n+\t\ti++\n+\t\tn := elemSize / sys.PtrSize\n+\t\tfor ; n >= 0x80; n >>= 7 {\n+\t\t\ttrailer[i] = byte(n | 0x80)\n+\t\t\ti++\n+\t\t}\n+\t\ttrailer[i] = byte(n)\n+\t\ti++\n+\t\tn = count - 1\n+\t\tfor ; n >= 0x80; n >>= 7 {\n+\t\t\ttrailer[i] = byte(n | 0x80)\n+\t\t\ti++\n+\t\t}\n+\t\ttrailer[i] = byte(n)\n+\t\ti++\n+\t\ttrailer[i] = 0\n+\t\ti++\n+\n+\t\trunGCProg(prog, &trailer[0], h.bitp, 2)\n+\n+\t\t// Even though we filled in the full array just now,\n+\t\t// record that we only filled in up to the ptrdata of the\n+\t\t// last element. This will cause the code below to\n+\t\t// memclr the dead section of the final array element,\n+\t\t// so that scanobject can stop early in the final element.\n+\t\ttotalBits = (elemSize*(count-1) + progSize) / sys.PtrSize\n+\t}\n+\tendProg := unsafe.Pointer(subtractb(h.bitp, (totalBits+3)/4))\n+\tendAlloc := unsafe.Pointer(subtractb(h.bitp, allocSize/heapBitmapScale))\n+\tmemclrNoHeapPointers(add(endAlloc, 1), uintptr(endProg)-uintptr(endAlloc))\n+}\n+\n+// progToPointerMask returns the 1-bit pointer mask output by the GC program prog.\n+// size the size of the region described by prog, in bytes.\n+// The resulting bitvector will have no more than size/sys.PtrSize bits.\n+func progToPointerMask(prog *byte, size uintptr) bitvector {\n+\tn := (size/sys.PtrSize + 7) / 8\n+\tx := (*[1 << 30]byte)(persistentalloc(n+1, 1, &memstats.buckhash_sys))[:n+1]\n+\tx[len(x)-1] = 0xa1 // overflow check sentinel\n+\tn = runGCProg(prog, nil, &x[0], 1)\n+\tif x[len(x)-1] != 0xa1 {\n+\t\tthrow(\"progToPointerMask: overflow\")\n+\t}\n+\treturn bitvector{int32(n), &x[0]}\n+}\n+\n+// Packed GC pointer bitmaps, aka GC programs.\n+//\n+// For large types containing arrays, the type information has a\n+// natural repetition that can be encoded to save space in the\n+// binary and in the memory representation of the type information.\n+//\n+// The encoding is a simple Lempel-Ziv style bytecode machine\n+// with the following instructions:\n+//\n+//\t00000000: stop\n+//\t0nnnnnnn: emit n bits copied from the next (n+7)/8 bytes\n+//\t10000000 n c: repeat the previous n bits c times; n, c are varints\n+//\t1nnnnnnn c: repeat the previous n bits c times; c is a varint\n+\n+// runGCProg executes the GC program prog, and then trailer if non-nil,\n+// writing to dst with entries of the given size.\n+// If size == 1, dst is a 1-bit pointer mask laid out moving forward from dst.\n+// If size == 2, dst is the 2-bit heap bitmap, and writes move backward\n+// starting at dst (because the heap bitmap does). In this case, the caller guarantees\n+// that only whole bytes in dst need to be written.\n+//\n+// runGCProg returns the number of 1- or 2-bit entries written to memory.\n+func runGCProg(prog, trailer, dst *byte, size int) uintptr {\n+\tdstStart := dst\n+\n+\t// Bits waiting to be written to memory.\n+\tvar bits uintptr\n+\tvar nbits uintptr\n+\n+\tp := prog\n+Run:\n+\tfor {\n+\t\t// Flush accumulated full bytes.\n+\t\t// The rest of the loop assumes that nbits <= 7.\n+\t\tfor ; nbits >= 8; nbits -= 8 {\n+\t\t\tif size == 1 {\n+\t\t\t\t*dst = uint8(bits)\n+\t\t\t\tdst = add1(dst)\n+\t\t\t\tbits >>= 8\n+\t\t\t} else {\n+\t\t\t\tv := bits&bitPointerAll | bitScanAll\n+\t\t\t\t*dst = uint8(v)\n+\t\t\t\tdst = subtract1(dst)\n+\t\t\t\tbits >>= 4\n+\t\t\t\tv = bits&bitPointerAll | bitScanAll\n+\t\t\t\t*dst = uint8(v)\n+\t\t\t\tdst = subtract1(dst)\n+\t\t\t\tbits >>= 4\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Process one instruction.\n+\t\tinst := uintptr(*p)\n+\t\tp = add1(p)\n+\t\tn := inst & 0x7F\n+\t\tif inst&0x80 == 0 {\n+\t\t\t// Literal bits; n == 0 means end of program.\n+\t\t\tif n == 0 {\n+\t\t\t\t// Program is over; continue in trailer if present.\n+\t\t\t\tif trailer != nil {\n+\t\t\t\t\t//println(\"trailer\")\n+\t\t\t\t\tp = trailer\n+\t\t\t\t\ttrailer = nil\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t\t//println(\"done\")\n+\t\t\t\tbreak Run\n+\t\t\t}\n+\t\t\t//println(\"lit\", n, dst)\n+\t\t\tnbyte := n / 8\n+\t\t\tfor i := uintptr(0); i < nbyte; i++ {\n+\t\t\t\tbits |= uintptr(*p) << nbits\n+\t\t\t\tp = add1(p)\n+\t\t\t\tif size == 1 {\n+\t\t\t\t\t*dst = uint8(bits)\n+\t\t\t\t\tdst = add1(dst)\n+\t\t\t\t\tbits >>= 8\n+\t\t\t\t} else {\n+\t\t\t\t\tv := bits&0xf | bitScanAll\n+\t\t\t\t\t*dst = uint8(v)\n+\t\t\t\t\tdst = subtract1(dst)\n+\t\t\t\t\tbits >>= 4\n+\t\t\t\t\tv = bits&0xf | bitScanAll\n+\t\t\t\t\t*dst = uint8(v)\n+\t\t\t\t\tdst = subtract1(dst)\n+\t\t\t\t\tbits >>= 4\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tif n %= 8; n > 0 {\n+\t\t\t\tbits |= uintptr(*p) << nbits\n+\t\t\t\tp = add1(p)\n+\t\t\t\tnbits += n\n+\t\t\t}\n+\t\t\tcontinue Run\n+\t\t}\n+\n+\t\t// Repeat. If n == 0, it is encoded in a varint in the next bytes.\n+\t\tif n == 0 {\n+\t\t\tfor off := uint(0); ; off += 7 {\n+\t\t\t\tx := uintptr(*p)\n+\t\t\t\tp = add1(p)\n+\t\t\t\tn |= (x & 0x7F) << off\n+\t\t\t\tif x&0x80 == 0 {\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Count is encoded in a varint in the next bytes.\n+\t\tc := uintptr(0)\n+\t\tfor off := uint(0); ; off += 7 {\n+\t\t\tx := uintptr(*p)\n+\t\t\tp = add1(p)\n+\t\t\tc |= (x & 0x7F) << off\n+\t\t\tif x&0x80 == 0 {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t\tc *= n // now total number of bits to copy\n+\n+\t\t// If the number of bits being repeated is small, load them\n+\t\t// into a register and use that register for the entire loop\n+\t\t// instead of repeatedly reading from memory.\n+\t\t// Handling fewer than 8 bits here makes the general loop simpler.\n+\t\t// The cutoff is sys.PtrSize*8 - 7 to guarantee that when we add\n+\t\t// the pattern to a bit buffer holding at most 7 bits (a partial byte)\n+\t\t// it will not overflow.\n+\t\tsrc := dst\n+\t\tconst maxBits = sys.PtrSize*8 - 7\n+\t\tif n <= maxBits {\n+\t\t\t// Start with bits in output buffer.\n+\t\t\tpattern := bits\n+\t\t\tnpattern := nbits\n+\n+\t\t\t// If we need more bits, fetch them from memory.\n+\t\t\tif size == 1 {\n+\t\t\t\tsrc = subtract1(src)\n+\t\t\t\tfor npattern < n {\n+\t\t\t\t\tpattern <<= 8\n+\t\t\t\t\tpattern |= uintptr(*src)\n+\t\t\t\t\tsrc = subtract1(src)\n+\t\t\t\t\tnpattern += 8\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tsrc = add1(src)\n+\t\t\t\tfor npattern < n {\n+\t\t\t\t\tpattern <<= 4\n+\t\t\t\t\tpattern |= uintptr(*src) & 0xf\n+\t\t\t\t\tsrc = add1(src)\n+\t\t\t\t\tnpattern += 4\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\t// We started with the whole bit output buffer,\n+\t\t\t// and then we loaded bits from whole bytes.\n+\t\t\t// Either way, we might now have too many instead of too few.\n+\t\t\t// Discard the extra.\n+\t\t\tif npattern > n {\n+\t\t\t\tpattern >>= npattern - n\n+\t\t\t\tnpattern = n\n+\t\t\t}\n+\n+\t\t\t// Replicate pattern to at most maxBits.\n+\t\t\tif npattern == 1 {\n+\t\t\t\t// One bit being repeated.\n+\t\t\t\t// If the bit is 1, make the pattern all 1s.\n+\t\t\t\t// If the bit is 0, the pattern is already all 0s,\n+\t\t\t\t// but we can claim that the number of bits\n+\t\t\t\t// in the word is equal to the number we need (c),\n+\t\t\t\t// because right shift of bits will zero fill.\n+\t\t\t\tif pattern == 1 {\n+\t\t\t\t\tpattern = 1<<maxBits - 1\n+\t\t\t\t\tnpattern = maxBits\n+\t\t\t\t} else {\n+\t\t\t\t\tnpattern = c\n+\t\t\t\t}\n+\t\t\t} else {\n+\t\t\t\tb := pattern\n+\t\t\t\tnb := npattern\n+\t\t\t\tif nb+nb <= maxBits {\n+\t\t\t\t\t// Double pattern until the whole uintptr is filled.\n+\t\t\t\t\tfor nb <= sys.PtrSize*8 {\n+\t\t\t\t\t\tb |= b << nb\n+\t\t\t\t\t\tnb += nb\n+\t\t\t\t\t}\n+\t\t\t\t\t// Trim away incomplete copy of original pattern in high bits.\n+\t\t\t\t\t// TODO(rsc): Replace with table lookup or loop on systems without divide?\n+\t\t\t\t\tnb = maxBits / npattern * npattern\n+\t\t\t\t\tb &= 1<<nb - 1\n+\t\t\t\t\tpattern = b\n+\t\t\t\t\tnpattern = nb\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\t// Add pattern to bit buffer and flush bit buffer, c/npattern times.\n+\t\t\t// Since pattern contains >8 bits, there will be full bytes to flush\n+\t\t\t// on each iteration.\n+\t\t\tfor ; c >= npattern; c -= npattern {\n+\t\t\t\tbits |= pattern << nbits\n+\t\t\t\tnbits += npattern\n+\t\t\t\tif size == 1 {\n+\t\t\t\t\tfor nbits >= 8 {\n+\t\t\t\t\t\t*dst = uint8(bits)\n+\t\t\t\t\t\tdst = add1(dst)\n+\t\t\t\t\t\tbits >>= 8\n+\t\t\t\t\t\tnbits -= 8\n+\t\t\t\t\t}\n+\t\t\t\t} else {\n+\t\t\t\t\tfor nbits >= 4 {\n+\t\t\t\t\t\t*dst = uint8(bits&0xf | bitScanAll)\n+\t\t\t\t\t\tdst = subtract1(dst)\n+\t\t\t\t\t\tbits >>= 4\n+\t\t\t\t\t\tnbits -= 4\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\t\t// Add final fragment to bit buffer.\n+\t\t\tif c > 0 {\n+\t\t\t\tpattern &= 1<<c - 1\n+\t\t\t\tbits |= pattern << nbits\n+\t\t\t\tnbits += c\n+\t\t\t}\n+\t\t\tcontinue Run\n+\t\t}\n+\n+\t\t// Repeat; n too large to fit in a register.\n+\t\t// Since nbits <= 7, we know the first few bytes of repeated data\n+\t\t// are already written to memory.\n+\t\toff := n - nbits // n > nbits because n > maxBits and nbits <= 7\n+\t\tif size == 1 {\n+\t\t\t// Leading src fragment.\n+\t\t\tsrc = subtractb(src, (off+7)/8)\n+\t\t\tif frag := off & 7; frag != 0 {\n+\t\t\t\tbits |= uintptr(*src) >> (8 - frag) << nbits\n+\t\t\t\tsrc = add1(src)\n+\t\t\t\tnbits += frag\n+\t\t\t\tc -= frag\n+\t\t\t}\n+\t\t\t// Main loop: load one byte, write another.\n+\t\t\t// The bits are rotating through the bit buffer.\n+\t\t\tfor i := c / 8; i > 0; i-- {\n+\t\t\t\tbits |= uintptr(*src) << nbits\n+\t\t\t\tsrc = add1(src)\n+\t\t\t\t*dst = uint8(bits)\n+\t\t\t\tdst = add1(dst)\n+\t\t\t\tbits >>= 8\n+\t\t\t}\n+\t\t\t// Final src fragment.\n+\t\t\tif c %= 8; c > 0 {\n+\t\t\t\tbits |= (uintptr(*src) & (1<<c - 1)) << nbits\n+\t\t\t\tnbits += c\n+\t\t\t}\n+\t\t} else {\n+\t\t\t// Leading src fragment.\n+\t\t\tsrc = addb(src, (off+3)/4)\n+\t\t\tif frag := off & 3; frag != 0 {\n+\t\t\t\tbits |= (uintptr(*src) & 0xf) >> (4 - frag) << nbits\n+\t\t\t\tsrc = subtract1(src)\n+\t\t\t\tnbits += frag\n+\t\t\t\tc -= frag\n+\t\t\t}\n+\t\t\t// Main loop: load one byte, write another.\n+\t\t\t// The bits are rotating through the bit buffer.\n+\t\t\tfor i := c / 4; i > 0; i-- {\n+\t\t\t\tbits |= (uintptr(*src) & 0xf) << nbits\n+\t\t\t\tsrc = subtract1(src)\n+\t\t\t\t*dst = uint8(bits&0xf | bitScanAll)\n+\t\t\t\tdst = subtract1(dst)\n+\t\t\t\tbits >>= 4\n+\t\t\t}\n+\t\t\t// Final src fragment.\n+\t\t\tif c %= 4; c > 0 {\n+\t\t\t\tbits |= (uintptr(*src) & (1<<c - 1)) << nbits\n+\t\t\t\tnbits += c\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Write any final bits out, using full-byte writes, even for the final byte.\n+\tvar totalBits uintptr\n+\tif size == 1 {\n+\t\ttotalBits = (uintptr(unsafe.Pointer(dst))-uintptr(unsafe.Pointer(dstStart)))*8 + nbits\n+\t\tnbits += -nbits & 7\n+\t\tfor ; nbits > 0; nbits -= 8 {\n+\t\t\t*dst = uint8(bits)\n+\t\t\tdst = add1(dst)\n+\t\t\tbits >>= 8\n+\t\t}\n+\t} else {\n+\t\ttotalBits = (uintptr(unsafe.Pointer(dstStart))-uintptr(unsafe.Pointer(dst)))*4 + nbits\n+\t\tnbits += -nbits & 3\n+\t\tfor ; nbits > 0; nbits -= 4 {\n+\t\t\tv := bits&0xf | bitScanAll\n+\t\t\t*dst = uint8(v)\n+\t\t\tdst = subtract1(dst)\n+\t\t\tbits >>= 4\n+\t\t}\n+\t}\n+\treturn totalBits\n+}\n+\n+func dumpGCProg(p *byte) {\n+\tnptr := 0\n+\tfor {\n+\t\tx := *p\n+\t\tp = add1(p)\n+\t\tif x == 0 {\n+\t\t\tprint(\"\\t\", nptr, \" end\\n\")\n+\t\t\tbreak\n+\t\t}\n+\t\tif x&0x80 == 0 {\n+\t\t\tprint(\"\\t\", nptr, \" lit \", x, \":\")\n+\t\t\tn := int(x+7) / 8\n+\t\t\tfor i := 0; i < n; i++ {\n+\t\t\t\tprint(\" \", hex(*p))\n+\t\t\t\tp = add1(p)\n+\t\t\t}\n+\t\t\tprint(\"\\n\")\n+\t\t\tnptr += int(x)\n+\t\t} else {\n+\t\t\tnbit := int(x &^ 0x80)\n+\t\t\tif nbit == 0 {\n+\t\t\t\tfor nb := uint(0); ; nb += 7 {\n+\t\t\t\t\tx := *p\n+\t\t\t\t\tp = add1(p)\n+\t\t\t\t\tnbit |= int(x&0x7f) << nb\n+\t\t\t\t\tif x&0x80 == 0 {\n+\t\t\t\t\t\tbreak\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tcount := 0\n+\t\t\tfor nb := uint(0); ; nb += 7 {\n+\t\t\t\tx := *p\n+\t\t\t\tp = add1(p)\n+\t\t\t\tcount |= int(x&0x7f) << nb\n+\t\t\t\tif x&0x80 == 0 {\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tprint(\"\\t\", nptr, \" repeat \", nbit, \" \u00d7 \", count, \"\\n\")\n+\t\t\tnptr += nbit * count\n+\t\t}\n+\t}\n+}\n+\n+// Testing.\n+\n+// gcbits returns the GC type info for x, for testing.\n+// The result is the bitmap entries (0 or 1), one entry per byte.\n+//go:linkname reflect_gcbits reflect.gcbits\n+func reflect_gcbits(x interface{}) []byte {\n+\tret := getgcmask(x)\n+\ttyp := (*ptrtype)(unsafe.Pointer(efaceOf(&x)._type)).elem\n+\tnptr := typ.ptrdata / sys.PtrSize\n+\tfor uintptr(len(ret)) > nptr && ret[len(ret)-1] == 0 {\n+\t\tret = ret[:len(ret)-1]\n+\t}\n+\treturn ret\n+}\n+\n+// Returns GC type info for object p for testing.\n+func getgcmask(ep interface{}) (mask []byte) {\n+\te := *efaceOf(&ep)\n+\tp := e.data\n+\tt := e._type\n+\t// data or bss\n+\troots := gcRoots\n+\tfor roots != nil {\n+\t\tfor i := 0; i < roots.count; i++ {\n+\t\t\tpr := roots.roots[i]\n+\t\t\taddr := uintptr(pr.decl)\n+\t\t\tif addr <= uintptr(p) && uintptr(p) < addr+pr.size {\n+\t\t\t\tn := (*ptrtype)(unsafe.Pointer(t)).elem.size\n+\t\t\t\tmask = make([]byte, n/sys.PtrSize)\n+\t\t\t\tcopy(mask, (*[1 << 29]uint8)(unsafe.Pointer(pr.gcdata))[:pr.ptrdata])\n+\t\t\t}\n+\t\t\treturn\n+\t\t}\n+\t\troots = roots.next\n+\t}\n+\n+\t// heap\n+\tvar n uintptr\n+\tvar base uintptr\n+\tif mlookup(uintptr(p), &base, &n, nil) != 0 {\n+\t\tmask = make([]byte, n/sys.PtrSize)\n+\t\tfor i := uintptr(0); i < n; i += sys.PtrSize {\n+\t\t\thbits := heapBitsForAddr(base + i)\n+\t\t\tif hbits.isPointer() {\n+\t\t\t\tmask[i/sys.PtrSize] = 1\n+\t\t\t}\n+\t\t\tif i != 1*sys.PtrSize && !hbits.morePointers() {\n+\t\t\t\tmask = mask[:i/sys.PtrSize]\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t}\n+\t\treturn\n+\t}\n+\n+\t// otherwise, not something the GC knows about.\n+\t// possibly read-only data, like malloc(0).\n+\t// must not have pointers\n+\t// For gccgo, may live on the stack, which is collected conservatively.\n+\treturn\n+}"}, {"sha": "92dabeffd93dcec1c9a5c1a544fd706c2a689435", "filename": "libgo/go/runtime/mcache.go", "status": "modified", "additions": 94, "deletions": 50, "changes": 144, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmcache.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmcache.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmcache.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -4,16 +4,8 @@\n \n package runtime\n \n-// This is a temporary mcache.go for gccgo.\n-// At some point it will be replaced by the one in the gc runtime package.\n-\n import \"unsafe\"\n \n-type mcachelist struct {\n-\tlist  *mlink\n-\tnlist uint32\n-}\n-\n // Per-thread (in Go, per-P) cache for small objects.\n // No locking needed because it is per-thread (per-P).\n //\n@@ -24,8 +16,8 @@ type mcachelist struct {\n type mcache struct {\n \t// The following members are accessed on every malloc,\n \t// so they are grouped here for better caching.\n-\tnext_sample      int32   // trigger heap sample after allocating this many bytes\n-\tlocal_cachealloc uintptr // bytes allocated (or freed) from cache since last lock of heap\n+\tnext_sample int32   // trigger heap sample after allocating this many bytes\n+\tlocal_scan  uintptr // bytes of scannable heap allocated\n \n \t// Allocator cache for tiny objects w/o pointers.\n \t// See \"Tiny allocator\" comment in malloc.go.\n@@ -36,12 +28,12 @@ type mcache struct {\n \t// tiny is a heap pointer. Since mcache is in non-GC'd memory,\n \t// we handle it by clearing it in releaseAll during mark\n \t// termination.\n-\ttiny     unsafe.Pointer\n-\ttinysize uintptr\n+\ttiny             uintptr\n+\ttinyoffset       uintptr\n+\tlocal_tinyallocs uintptr // number of tiny allocs not counted in other stats\n \n \t// The rest is not accessed on every malloc.\n-\talloc [_NumSizeClasses]*mspan     // spans to allocate from\n-\tfree  [_NumSizeClasses]mcachelist // lists of explicitly freed objects\n+\talloc [_NumSizeClasses]*mspan // spans to allocate from\n \n \t// Local allocator stats, flushed during GC.\n \tlocal_nlookup    uintptr                  // number of pointer lookups\n@@ -50,46 +42,98 @@ type mcache struct {\n \tlocal_nsmallfree [_NumSizeClasses]uintptr // number of frees for small objects (<=maxsmallsize)\n }\n \n-type mtypes struct {\n-\tcompression byte\n-\tdata        uintptr\n+// A gclink is a node in a linked list of blocks, like mlink,\n+// but it is opaque to the garbage collector.\n+// The GC does not trace the pointers during collection,\n+// and the compiler does not emit write barriers for assignments\n+// of gclinkptr values. Code should store references to gclinks\n+// as gclinkptr, not as *gclink.\n+type gclink struct {\n+\tnext gclinkptr\n }\n \n-type special struct {\n-\tnext   *special\n-\toffset uint16\n-\tkind   byte\n+// A gclinkptr is a pointer to a gclink, but it is opaque\n+// to the garbage collector.\n+type gclinkptr uintptr\n+\n+// ptr returns the *gclink form of p.\n+// The result should be used for accessing fields, not stored\n+// in other data structures.\n+func (p gclinkptr) ptr() *gclink {\n+\treturn (*gclink)(unsafe.Pointer(p))\n }\n \n-type mspan struct {\n-\tnext     *mspan // next span in list, or nil if none\n-\tprev     *mspan // previous span's next field, or list head's first field if none\n-\tstart    uintptr\n-\tnpages   uintptr // number of pages in span\n-\tfreelist *mlink\n-\n-\t// sweep generation:\n-\t// if sweepgen == h->sweepgen - 2, the span needs sweeping\n-\t// if sweepgen == h->sweepgen - 1, the span is currently being swept\n-\t// if sweepgen == h->sweepgen, the span is swept and ready to use\n-\t// h->sweepgen is incremented by 2 after every GC\n-\n-\tsweepgen    uint32\n-\tref         uint16\n-\tsizeclass   uint8   // size class\n-\tincache     bool    // being used by an mcache\n-\tstate       uint8   // mspaninuse etc\n-\tneedzero    uint8   // needs to be zeroed before allocation\n-\telemsize    uintptr // computed from sizeclass or from npages\n-\tunusedsince int64   // first time spotted by gc in mspanfree state\n-\tnpreleased  uintptr // number of pages released to the os\n-\tlimit       uintptr // end of data in span\n-\ttypes       mtypes\n-\tspeciallock mutex    // guards specials list\n-\tspecials    *special // linked list of special records sorted by offset.\n-\tfreebuf     *mlink\n+// dummy MSpan that contains no free objects.\n+var emptymspan mspan\n+\n+func allocmcache() *mcache {\n+\tlock(&mheap_.lock)\n+\tc := (*mcache)(mheap_.cachealloc.alloc())\n+\tunlock(&mheap_.lock)\n+\tfor i := 0; i < _NumSizeClasses; i++ {\n+\t\tc.alloc[i] = &emptymspan\n+\t}\n+\tc.next_sample = nextSample()\n+\treturn c\n+}\n+\n+func freemcache(c *mcache) {\n+\tsystemstack(func() {\n+\t\tc.releaseAll()\n+\n+\t\t// NOTE(rsc,rlh): If gcworkbuffree comes back, we need to coordinate\n+\t\t// with the stealing of gcworkbufs during garbage collection to avoid\n+\t\t// a race where the workbuf is double-freed.\n+\t\t// gcworkbuffree(c.gcworkbuf)\n+\n+\t\tlock(&mheap_.lock)\n+\t\tpurgecachedstats(c)\n+\t\tmheap_.cachealloc.free(unsafe.Pointer(c))\n+\t\tunlock(&mheap_.lock)\n+\t})\n+}\n+\n+// Gets a span that has a free object in it and assigns it\n+// to be the cached span for the given sizeclass. Returns this span.\n+func (c *mcache) refill(sizeclass int32) *mspan {\n+\t_g_ := getg()\n+\n+\t_g_.m.locks++\n+\t// Return the current cached span to the central lists.\n+\ts := c.alloc[sizeclass]\n+\n+\tif uintptr(s.allocCount) != s.nelems {\n+\t\tthrow(\"refill of span with free space remaining\")\n+\t}\n+\n+\tif s != &emptymspan {\n+\t\ts.incache = false\n+\t}\n+\n+\t// Get a new cached span from the central lists.\n+\ts = mheap_.central[sizeclass].mcentral.cacheSpan()\n+\tif s == nil {\n+\t\tthrow(\"out of memory\")\n+\t}\n+\n+\tif uintptr(s.allocCount) == s.nelems {\n+\t\tthrow(\"span has no free space\")\n+\t}\n+\n+\tc.alloc[sizeclass] = s\n+\t_g_.m.locks--\n+\treturn s\n }\n \n-type mlink struct {\n-\tnext *mlink\n+func (c *mcache) releaseAll() {\n+\tfor i := 0; i < _NumSizeClasses; i++ {\n+\t\ts := c.alloc[i]\n+\t\tif s != &emptymspan {\n+\t\t\tmheap_.central[i].mcentral.uncacheSpan(s)\n+\t\t\tc.alloc[i] = &emptymspan\n+\t\t}\n+\t}\n+\t// Clear tinyalloc pool.\n+\tc.tiny = 0\n+\tc.tinyoffset = 0\n }"}, {"sha": "ddcf81ebb18ddb0f351a44d95d7ccf8eb0822b8d", "filename": "libgo/go/runtime/mcentral.go", "status": "added", "additions": 222, "deletions": 0, "changes": 222, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmcentral.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmcentral.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmcentral.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,222 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Central free lists.\n+//\n+// See malloc.go for an overview.\n+//\n+// The MCentral doesn't actually contain the list of free objects; the MSpan does.\n+// Each MCentral is two lists of MSpans: those with free objects (c->nonempty)\n+// and those that are completely allocated (c->empty).\n+\n+package runtime\n+\n+import \"runtime/internal/atomic\"\n+\n+// Central list of free objects of a given size.\n+//\n+//go:notinheap\n+type mcentral struct {\n+\tlock      mutex\n+\tsizeclass int32\n+\tnonempty  mSpanList // list of spans with a free object, ie a nonempty free list\n+\tempty     mSpanList // list of spans with no free objects (or cached in an mcache)\n+}\n+\n+// Initialize a single central free list.\n+func (c *mcentral) init(sizeclass int32) {\n+\tc.sizeclass = sizeclass\n+\tc.nonempty.init()\n+\tc.empty.init()\n+}\n+\n+// Allocate a span to use in an MCache.\n+func (c *mcentral) cacheSpan() *mspan {\n+\t// Deduct credit for this span allocation and sweep if necessary.\n+\tspanBytes := uintptr(class_to_allocnpages[c.sizeclass]) * _PageSize\n+\tdeductSweepCredit(spanBytes, 0)\n+\n+\tlock(&c.lock)\n+\tsg := mheap_.sweepgen\n+retry:\n+\tvar s *mspan\n+\tfor s = c.nonempty.first; s != nil; s = s.next {\n+\t\tif s.sweepgen == sg-2 && atomic.Cas(&s.sweepgen, sg-2, sg-1) {\n+\t\t\tc.nonempty.remove(s)\n+\t\t\tc.empty.insertBack(s)\n+\t\t\tunlock(&c.lock)\n+\t\t\ts.sweep(true)\n+\t\t\tgoto havespan\n+\t\t}\n+\t\tif s.sweepgen == sg-1 {\n+\t\t\t// the span is being swept by background sweeper, skip\n+\t\t\tcontinue\n+\t\t}\n+\t\t// we have a nonempty span that does not require sweeping, allocate from it\n+\t\tc.nonempty.remove(s)\n+\t\tc.empty.insertBack(s)\n+\t\tunlock(&c.lock)\n+\t\tgoto havespan\n+\t}\n+\n+\tfor s = c.empty.first; s != nil; s = s.next {\n+\t\tif s.sweepgen == sg-2 && atomic.Cas(&s.sweepgen, sg-2, sg-1) {\n+\t\t\t// we have an empty span that requires sweeping,\n+\t\t\t// sweep it and see if we can free some space in it\n+\t\t\tc.empty.remove(s)\n+\t\t\t// swept spans are at the end of the list\n+\t\t\tc.empty.insertBack(s)\n+\t\t\tunlock(&c.lock)\n+\t\t\ts.sweep(true)\n+\t\t\tfreeIndex := s.nextFreeIndex()\n+\t\t\tif freeIndex != s.nelems {\n+\t\t\t\ts.freeindex = freeIndex\n+\t\t\t\tgoto havespan\n+\t\t\t}\n+\t\t\tlock(&c.lock)\n+\t\t\t// the span is still empty after sweep\n+\t\t\t// it is already in the empty list, so just retry\n+\t\t\tgoto retry\n+\t\t}\n+\t\tif s.sweepgen == sg-1 {\n+\t\t\t// the span is being swept by background sweeper, skip\n+\t\t\tcontinue\n+\t\t}\n+\t\t// already swept empty span,\n+\t\t// all subsequent ones must also be either swept or in process of sweeping\n+\t\tbreak\n+\t}\n+\tunlock(&c.lock)\n+\n+\t// Replenish central list if empty.\n+\ts = c.grow()\n+\tif s == nil {\n+\t\treturn nil\n+\t}\n+\tlock(&c.lock)\n+\tc.empty.insertBack(s)\n+\tunlock(&c.lock)\n+\n+\t// At this point s is a non-empty span, queued at the end of the empty list,\n+\t// c is unlocked.\n+havespan:\n+\tcap := int32((s.npages << _PageShift) / s.elemsize)\n+\tn := cap - int32(s.allocCount)\n+\tif n == 0 || s.freeindex == s.nelems || uintptr(s.allocCount) == s.nelems {\n+\t\tthrow(\"span has no free objects\")\n+\t}\n+\tusedBytes := uintptr(s.allocCount) * s.elemsize\n+\tif usedBytes > 0 {\n+\t\treimburseSweepCredit(usedBytes)\n+\t}\n+\tatomic.Xadd64(&memstats.heap_live, int64(spanBytes)-int64(usedBytes))\n+\tif trace.enabled {\n+\t\t// heap_live changed.\n+\t\ttraceHeapAlloc()\n+\t}\n+\tif gcBlackenEnabled != 0 {\n+\t\t// heap_live changed.\n+\t\tgcController.revise()\n+\t}\n+\ts.incache = true\n+\tfreeByteBase := s.freeindex &^ (64 - 1)\n+\twhichByte := freeByteBase / 8\n+\t// Init alloc bits cache.\n+\ts.refillAllocCache(whichByte)\n+\n+\t// Adjust the allocCache so that s.freeindex corresponds to the low bit in\n+\t// s.allocCache.\n+\ts.allocCache >>= s.freeindex % 64\n+\n+\treturn s\n+}\n+\n+// Return span from an MCache.\n+func (c *mcentral) uncacheSpan(s *mspan) {\n+\tlock(&c.lock)\n+\n+\ts.incache = false\n+\n+\tif s.allocCount == 0 {\n+\t\tthrow(\"uncaching span but s.allocCount == 0\")\n+\t}\n+\n+\tcap := int32((s.npages << _PageShift) / s.elemsize)\n+\tn := cap - int32(s.allocCount)\n+\tif n > 0 {\n+\t\tc.empty.remove(s)\n+\t\tc.nonempty.insert(s)\n+\t\t// mCentral_CacheSpan conservatively counted\n+\t\t// unallocated slots in heap_live. Undo this.\n+\t\tatomic.Xadd64(&memstats.heap_live, -int64(n)*int64(s.elemsize))\n+\t}\n+\tunlock(&c.lock)\n+}\n+\n+// freeSpan updates c and s after sweeping s.\n+// It sets s's sweepgen to the latest generation,\n+// and, based on the number of free objects in s,\n+// moves s to the appropriate list of c or returns it\n+// to the heap.\n+// freeSpan returns true if s was returned to the heap.\n+// If preserve=true, it does not move s (the caller\n+// must take care of it).\n+func (c *mcentral) freeSpan(s *mspan, preserve bool, wasempty bool) bool {\n+\tif s.incache {\n+\t\tthrow(\"freeSpan given cached span\")\n+\t}\n+\ts.needzero = 1\n+\n+\tif preserve {\n+\t\t// preserve is set only when called from MCentral_CacheSpan above,\n+\t\t// the span must be in the empty list.\n+\t\tif !s.inList() {\n+\t\t\tthrow(\"can't preserve unlinked span\")\n+\t\t}\n+\t\tatomic.Store(&s.sweepgen, mheap_.sweepgen)\n+\t\treturn false\n+\t}\n+\n+\tlock(&c.lock)\n+\n+\t// Move to nonempty if necessary.\n+\tif wasempty {\n+\t\tc.empty.remove(s)\n+\t\tc.nonempty.insert(s)\n+\t}\n+\n+\t// delay updating sweepgen until here. This is the signal that\n+\t// the span may be used in an MCache, so it must come after the\n+\t// linked list operations above (actually, just after the\n+\t// lock of c above.)\n+\tatomic.Store(&s.sweepgen, mheap_.sweepgen)\n+\n+\tif s.allocCount != 0 {\n+\t\tunlock(&c.lock)\n+\t\treturn false\n+\t}\n+\n+\tc.nonempty.remove(s)\n+\tunlock(&c.lock)\n+\tmheap_.freeSpan(s, 0)\n+\treturn true\n+}\n+\n+// grow allocates a new empty span from the heap and initializes it for c's size class.\n+func (c *mcentral) grow() *mspan {\n+\tnpages := uintptr(class_to_allocnpages[c.sizeclass])\n+\tsize := uintptr(class_to_size[c.sizeclass])\n+\tn := (npages << _PageShift) / size\n+\n+\ts := mheap_.alloc(npages, c.sizeclass, false, true)\n+\tif s == nil {\n+\t\treturn nil\n+\t}\n+\n+\tp := s.base()\n+\ts.limit = p + size*n\n+\n+\theapBitsForSpan(s.base()).initSpan(s)\n+\treturn s\n+}"}, {"sha": "161ff26b137efba24ae16b0e8c3d270978fcbb56", "filename": "libgo/go/runtime/mem_gccgo.go", "status": "added", "additions": 280, "deletions": 0, "changes": 280, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmem_gccgo.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmem_gccgo.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmem_gccgo.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,280 @@\n+// Copyright 2016 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// The gccgo version of mem_*.go.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/sys\"\n+\t\"unsafe\"\n+)\n+\n+// Functions called by C code.\n+//go:linkname sysAlloc runtime.sysAlloc\n+\n+//extern mmap\n+func mmap(addr unsafe.Pointer, n uintptr, prot, flags, fd int32, off uintptr) unsafe.Pointer\n+\n+//extern munmap\n+func munmap(addr unsafe.Pointer, length uintptr) int32\n+\n+//extern mincore\n+func mincore(addr unsafe.Pointer, n uintptr, dst *byte) int32\n+\n+//extern madvise\n+func madvise(addr unsafe.Pointer, n uintptr, flags int32) int32\n+\n+var mmapFD = int32(-1)\n+\n+var devZero = []byte(\"/dev/zero\\x00\")\n+\n+func init() {\n+\tif _MAP_ANON == 0 {\n+\t\tmmapFD = open(&devZero[0], 0 /* O_RDONLY */, 0)\n+\t\tif mmapFD < 0 {\n+\t\t\tprintln(\"open /dev/zero: errno=\", errno())\n+\t\t\texit(2)\n+\t\t}\n+\t}\n+}\n+\n+// NOTE: vec must be just 1 byte long here.\n+// Mincore returns ENOMEM if any of the pages are unmapped,\n+// but we want to know that all of the pages are unmapped.\n+// To make these the same, we can only ask about one page\n+// at a time. See golang.org/issue/7476.\n+var addrspace_vec [1]byte\n+\n+func addrspace_free(v unsafe.Pointer, n uintptr) bool {\n+\tfor off := uintptr(0); off < n; off += physPageSize {\n+\t\t// Use a length of 1 byte, which the kernel will round\n+\t\t// up to one physical page regardless of the true\n+\t\t// physical page size.\n+\t\terrval := 0\n+\t\tif mincore(unsafe.Pointer(uintptr(v)+off), 1, &addrspace_vec[0]) < 0 {\n+\t\t\terrval = errno()\n+\t\t}\n+\t\tif errval == _ENOSYS {\n+\t\t\t// mincore is not available on this system.\n+\t\t\t// Assume the address is available.\n+\t\t\treturn true\n+\t\t}\n+\t\tif errval == _EINVAL {\n+\t\t\t// Address is not a multiple of the physical\n+\t\t\t// page size. Shouldn't happen, but just ignore it.\n+\t\t\tcontinue\n+\t\t}\n+\t\t// ENOMEM means unmapped, which is what we want.\n+\t\t// Anything else we assume means the pages are mapped.\n+\t\tif errval != _ENOMEM {\n+\t\t\treturn false\n+\t\t}\n+\t}\n+\treturn true\n+}\n+\n+func mmap_fixed(v unsafe.Pointer, n uintptr, prot, flags, fd int32, offset uintptr) unsafe.Pointer {\n+\tp := mmap(v, n, prot, flags, fd, offset)\n+\t// On some systems, mmap ignores v without\n+\t// MAP_FIXED, so retry if the address space is free.\n+\tif p != v && addrspace_free(v, n) {\n+\t\tif uintptr(p) != _MAP_FAILED {\n+\t\t\tmunmap(p, n)\n+\t\t}\n+\t\tp = mmap(v, n, prot, flags|_MAP_FIXED, fd, offset)\n+\t}\n+\treturn p\n+}\n+\n+// Don't split the stack as this method may be invoked without a valid G, which\n+// prevents us from allocating more stack.\n+//go:nosplit\n+func sysAlloc(n uintptr, sysStat *uint64) unsafe.Pointer {\n+\tp := mmap(nil, n, _PROT_READ|_PROT_WRITE, _MAP_ANON|_MAP_PRIVATE, mmapFD, 0)\n+\tif uintptr(p) == _MAP_FAILED {\n+\t\terrval := errno()\n+\t\tif errval == _EACCES {\n+\t\t\tprint(\"runtime: mmap: access denied\\n\")\n+\t\t\texit(2)\n+\t\t}\n+\t\tif errval == _EAGAIN {\n+\t\t\tprint(\"runtime: mmap: too much locked memory (check 'ulimit -l').\\n\")\n+\t\t\texit(2)\n+\t\t}\n+\t\treturn nil\n+\t}\n+\tmSysStatInc(sysStat, n)\n+\treturn p\n+}\n+\n+func sysUnused(v unsafe.Pointer, n uintptr) {\n+\t// By default, Linux's \"transparent huge page\" support will\n+\t// merge pages into a huge page if there's even a single\n+\t// present regular page, undoing the effects of the DONTNEED\n+\t// below. On amd64, that means khugepaged can turn a single\n+\t// 4KB page to 2MB, bloating the process's RSS by as much as\n+\t// 512X. (See issue #8832 and Linux kernel bug\n+\t// https://bugzilla.kernel.org/show_bug.cgi?id=93111)\n+\t//\n+\t// To work around this, we explicitly disable transparent huge\n+\t// pages when we release pages of the heap. However, we have\n+\t// to do this carefully because changing this flag tends to\n+\t// split the VMA (memory mapping) containing v in to three\n+\t// VMAs in order to track the different values of the\n+\t// MADV_NOHUGEPAGE flag in the different regions. There's a\n+\t// default limit of 65530 VMAs per address space (sysctl\n+\t// vm.max_map_count), so we must be careful not to create too\n+\t// many VMAs (see issue #12233).\n+\t//\n+\t// Since huge pages are huge, there's little use in adjusting\n+\t// the MADV_NOHUGEPAGE flag on a fine granularity, so we avoid\n+\t// exploding the number of VMAs by only adjusting the\n+\t// MADV_NOHUGEPAGE flag on a large granularity. This still\n+\t// gets most of the benefit of huge pages while keeping the\n+\t// number of VMAs under control. With hugePageSize = 2MB, even\n+\t// a pessimal heap can reach 128GB before running out of VMAs.\n+\tif sys.HugePageSize != 0 && _MADV_NOHUGEPAGE != 0 {\n+\t\tvar s uintptr = sys.HugePageSize // division by constant 0 is a compile-time error :(\n+\n+\t\t// If it's a large allocation, we want to leave huge\n+\t\t// pages enabled. Hence, we only adjust the huge page\n+\t\t// flag on the huge pages containing v and v+n-1, and\n+\t\t// only if those aren't aligned.\n+\t\tvar head, tail uintptr\n+\t\tif uintptr(v)%s != 0 {\n+\t\t\t// Compute huge page containing v.\n+\t\t\thead = uintptr(v) &^ (s - 1)\n+\t\t}\n+\t\tif (uintptr(v)+n)%s != 0 {\n+\t\t\t// Compute huge page containing v+n-1.\n+\t\t\ttail = (uintptr(v) + n - 1) &^ (s - 1)\n+\t\t}\n+\n+\t\t// Note that madvise will return EINVAL if the flag is\n+\t\t// already set, which is quite likely. We ignore\n+\t\t// errors.\n+\t\tif head != 0 && head+sys.HugePageSize == tail {\n+\t\t\t// head and tail are different but adjacent,\n+\t\t\t// so do this in one call.\n+\t\t\tmadvise(unsafe.Pointer(head), 2*sys.HugePageSize, _MADV_NOHUGEPAGE)\n+\t\t} else {\n+\t\t\t// Advise the huge pages containing v and v+n-1.\n+\t\t\tif head != 0 {\n+\t\t\t\tmadvise(unsafe.Pointer(head), sys.HugePageSize, _MADV_NOHUGEPAGE)\n+\t\t\t}\n+\t\t\tif tail != 0 && tail != head {\n+\t\t\t\tmadvise(unsafe.Pointer(tail), sys.HugePageSize, _MADV_NOHUGEPAGE)\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\tif uintptr(v)&(physPageSize-1) != 0 || n&(physPageSize-1) != 0 {\n+\t\t// madvise will round this to any physical page\n+\t\t// *covered* by this range, so an unaligned madvise\n+\t\t// will release more memory than intended.\n+\t\tthrow(\"unaligned sysUnused\")\n+\t}\n+\n+\tif _MADV_DONTNEED != 0 {\n+\t\tmadvise(v, n, _MADV_DONTNEED)\n+\t} else if _MADV_FREE != 0 {\n+\t\tmadvise(v, n, _MADV_FREE)\n+\t}\n+}\n+\n+func sysUsed(v unsafe.Pointer, n uintptr) {\n+\tif sys.HugePageSize != 0 && _MADV_HUGEPAGE != 0 {\n+\t\t// Partially undo the NOHUGEPAGE marks from sysUnused\n+\t\t// for whole huge pages between v and v+n. This may\n+\t\t// leave huge pages off at the end points v and v+n\n+\t\t// even though allocations may cover these entire huge\n+\t\t// pages. We could detect this and undo NOHUGEPAGE on\n+\t\t// the end points as well, but it's probably not worth\n+\t\t// the cost because when neighboring allocations are\n+\t\t// freed sysUnused will just set NOHUGEPAGE again.\n+\t\tvar s uintptr = sys.HugePageSize\n+\n+\t\t// Round v up to a huge page boundary.\n+\t\tbeg := (uintptr(v) + (s - 1)) &^ (s - 1)\n+\t\t// Round v+n down to a huge page boundary.\n+\t\tend := (uintptr(v) + n) &^ (s - 1)\n+\n+\t\tif beg < end {\n+\t\t\tmadvise(unsafe.Pointer(beg), end-beg, _MADV_HUGEPAGE)\n+\t\t}\n+\t}\n+}\n+\n+// Don't split the stack as this function may be invoked without a valid G,\n+// which prevents us from allocating more stack.\n+//go:nosplit\n+func sysFree(v unsafe.Pointer, n uintptr, sysStat *uint64) {\n+\tmSysStatDec(sysStat, n)\n+\tmunmap(v, n)\n+}\n+\n+func sysFault(v unsafe.Pointer, n uintptr) {\n+\tmmap(v, n, _PROT_NONE, _MAP_ANON|_MAP_PRIVATE|_MAP_FIXED, mmapFD, 0)\n+}\n+\n+func sysReserve(v unsafe.Pointer, n uintptr, reserved *bool) unsafe.Pointer {\n+\t// On 64-bit, people with ulimit -v set complain if we reserve too\n+\t// much address space. Instead, assume that the reservation is okay\n+\t// if we can reserve at least 64K and check the assumption in SysMap.\n+\t// Only user-mode Linux (UML) rejects these requests.\n+\tif sys.PtrSize == 8 && uint64(n) > 1<<32 {\n+\t\tp := mmap_fixed(v, 64<<10, _PROT_NONE, _MAP_ANON|_MAP_PRIVATE, mmapFD, 0)\n+\t\tif p != v {\n+\t\t\tif uintptr(p) != _MAP_FAILED {\n+\t\t\t\tmunmap(p, 64<<10)\n+\t\t\t}\n+\t\t\treturn nil\n+\t\t}\n+\t\tmunmap(p, 64<<10)\n+\t\t*reserved = false\n+\t\treturn v\n+\t}\n+\n+\tp := mmap(v, n, _PROT_NONE, _MAP_ANON|_MAP_PRIVATE, mmapFD, 0)\n+\tif uintptr(p) == _MAP_FAILED {\n+\t\treturn nil\n+\t}\n+\t*reserved = true\n+\treturn p\n+}\n+\n+func sysMap(v unsafe.Pointer, n uintptr, reserved bool, sysStat *uint64) {\n+\tmSysStatInc(sysStat, n)\n+\n+\t// On 64-bit, we don't actually have v reserved, so tread carefully.\n+\tif !reserved {\n+\t\tflags := int32(_MAP_ANON | _MAP_PRIVATE)\n+\t\tif GOOS == \"dragonfly\" {\n+\t\t\t// TODO(jsing): For some reason DragonFly seems to return\n+\t\t\t// memory at a different address than we requested, even when\n+\t\t\t// there should be no reason for it to do so. This can be\n+\t\t\t// avoided by using MAP_FIXED, but I'm not sure we should need\n+\t\t\t// to do this - we do not on other platforms.\n+\t\t\tflags |= _MAP_FIXED\n+\t\t}\n+\t\tp := mmap_fixed(v, n, _PROT_READ|_PROT_WRITE, flags, mmapFD, 0)\n+\t\tif uintptr(p) == _MAP_FAILED && errno() == _ENOMEM {\n+\t\t\tthrow(\"runtime: out of memory\")\n+\t\t}\n+\t\tif p != v {\n+\t\t\tprint(\"runtime: address space conflict: map(\", v, \") = \", p, \"\\n\")\n+\t\t\tthrow(\"runtime: address space conflict\")\n+\t\t}\n+\t\treturn\n+\t}\n+\n+\tp := mmap(v, n, _PROT_READ|_PROT_WRITE, _MAP_ANON|_MAP_FIXED|_MAP_PRIVATE, mmapFD, 0)\n+\tif uintptr(p) == _MAP_FAILED && errno() == _ENOMEM {\n+\t\tthrow(\"runtime: out of memory\")\n+\t}\n+\tif p != v {\n+\t\tthrow(\"runtime: cannot map pages in arena address space\")\n+\t}\n+}"}, {"sha": "d0e8b42a5a708da2f0add38609e4150a36c18b22", "filename": "libgo/go/runtime/memmove_linux_amd64_test.go", "status": "added", "additions": 62, "deletions": 0, "changes": 62, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmemmove_linux_amd64_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmemmove_linux_amd64_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmemmove_linux_amd64_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,62 @@\n+// Copyright 2013 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package runtime_test\n+\n+import (\n+\t\"io/ioutil\"\n+\t\"os\"\n+\t\"reflect\"\n+\t\"syscall\"\n+\t\"testing\"\n+\t\"unsafe\"\n+)\n+\n+// TestMemmoveOverflow maps 3GB of memory and calls memmove on\n+// the corresponding slice.\n+func TestMemmoveOverflow(t *testing.T) {\n+\tt.Parallel()\n+\t// Create a temporary file.\n+\ttmp, err := ioutil.TempFile(\"\", \"go-memmovetest\")\n+\tif err != nil {\n+\t\tt.Fatal(err)\n+\t}\n+\t_, err = tmp.Write(make([]byte, 65536))\n+\tif err != nil {\n+\t\tt.Fatal(err)\n+\t}\n+\tdefer os.Remove(tmp.Name())\n+\tdefer tmp.Close()\n+\n+\t// Set up mappings.\n+\tbase, _, errno := syscall.Syscall6(syscall.SYS_MMAP,\n+\t\t0xa0<<32, 3<<30, syscall.PROT_READ|syscall.PROT_WRITE, syscall.MAP_PRIVATE|syscall.MAP_ANONYMOUS, ^uintptr(0), 0)\n+\tif errno != 0 {\n+\t\tt.Skipf(\"could not create memory mapping: %s\", errno)\n+\t}\n+\tsyscall.Syscall(syscall.SYS_MUNMAP, base, 3<<30, 0)\n+\n+\tfor off := uintptr(0); off < 3<<30; off += 65536 {\n+\t\t_, _, errno := syscall.Syscall6(syscall.SYS_MMAP,\n+\t\t\tbase+off, 65536, syscall.PROT_READ|syscall.PROT_WRITE, syscall.MAP_SHARED|syscall.MAP_FIXED, tmp.Fd(), 0)\n+\t\tif errno != 0 {\n+\t\t\tt.Skipf(\"could not map a page at requested 0x%x: %s\", base+off, errno)\n+\t\t}\n+\t\tdefer syscall.Syscall(syscall.SYS_MUNMAP, base+off, 65536, 0)\n+\t}\n+\n+\tvar s []byte\n+\tsp := (*reflect.SliceHeader)(unsafe.Pointer(&s))\n+\tsp.Data = base\n+\tsp.Len, sp.Cap = 3<<30, 3<<30\n+\n+\tn := copy(s[1:], s)\n+\tif n != 3<<30-1 {\n+\t\tt.Fatalf(\"copied %d bytes, expected %d\", n, 3<<30-1)\n+\t}\n+\tn = copy(s, s[1:])\n+\tif n != 3<<30-1 {\n+\t\tt.Fatalf(\"copied %d bytes, expected %d\", n, 3<<30-1)\n+\t}\n+}"}, {"sha": "74b8753b5f78f4f22f53346c2ce0b3f9113f2980", "filename": "libgo/go/runtime/memmove_test.go", "status": "added", "additions": 469, "deletions": 0, "changes": 469, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmemmove_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmemmove_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmemmove_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,469 @@\n+// Copyright 2013 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package runtime_test\n+\n+import (\n+\t\"crypto/rand\"\n+\t\"encoding/binary\"\n+\t\"fmt\"\n+\t\"internal/race\"\n+\t. \"runtime\"\n+\t\"testing\"\n+)\n+\n+func TestMemmove(t *testing.T) {\n+\tt.Parallel()\n+\tsize := 256\n+\tif testing.Short() {\n+\t\tsize = 128 + 16\n+\t}\n+\tsrc := make([]byte, size)\n+\tdst := make([]byte, size)\n+\tfor i := 0; i < size; i++ {\n+\t\tsrc[i] = byte(128 + (i & 127))\n+\t}\n+\tfor i := 0; i < size; i++ {\n+\t\tdst[i] = byte(i & 127)\n+\t}\n+\tfor n := 0; n <= size; n++ {\n+\t\tfor x := 0; x <= size-n; x++ { // offset in src\n+\t\t\tfor y := 0; y <= size-n; y++ { // offset in dst\n+\t\t\t\tcopy(dst[y:y+n], src[x:x+n])\n+\t\t\t\tfor i := 0; i < y; i++ {\n+\t\t\t\t\tif dst[i] != byte(i&127) {\n+\t\t\t\t\t\tt.Fatalf(\"prefix dst[%d] = %d\", i, dst[i])\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tfor i := y; i < y+n; i++ {\n+\t\t\t\t\tif dst[i] != byte(128+((i-y+x)&127)) {\n+\t\t\t\t\t\tt.Fatalf(\"copied dst[%d] = %d\", i, dst[i])\n+\t\t\t\t\t}\n+\t\t\t\t\tdst[i] = byte(i & 127) // reset dst\n+\t\t\t\t}\n+\t\t\t\tfor i := y + n; i < size; i++ {\n+\t\t\t\t\tif dst[i] != byte(i&127) {\n+\t\t\t\t\t\tt.Fatalf(\"suffix dst[%d] = %d\", i, dst[i])\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+func TestMemmoveAlias(t *testing.T) {\n+\tt.Parallel()\n+\tsize := 256\n+\tif testing.Short() {\n+\t\tsize = 128 + 16\n+\t}\n+\tbuf := make([]byte, size)\n+\tfor i := 0; i < size; i++ {\n+\t\tbuf[i] = byte(i)\n+\t}\n+\tfor n := 0; n <= size; n++ {\n+\t\tfor x := 0; x <= size-n; x++ { // src offset\n+\t\t\tfor y := 0; y <= size-n; y++ { // dst offset\n+\t\t\t\tcopy(buf[y:y+n], buf[x:x+n])\n+\t\t\t\tfor i := 0; i < y; i++ {\n+\t\t\t\t\tif buf[i] != byte(i) {\n+\t\t\t\t\t\tt.Fatalf(\"prefix buf[%d] = %d\", i, buf[i])\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t\tfor i := y; i < y+n; i++ {\n+\t\t\t\t\tif buf[i] != byte(i-y+x) {\n+\t\t\t\t\t\tt.Fatalf(\"copied buf[%d] = %d\", i, buf[i])\n+\t\t\t\t\t}\n+\t\t\t\t\tbuf[i] = byte(i) // reset buf\n+\t\t\t\t}\n+\t\t\t\tfor i := y + n; i < size; i++ {\n+\t\t\t\t\tif buf[i] != byte(i) {\n+\t\t\t\t\t\tt.Fatalf(\"suffix buf[%d] = %d\", i, buf[i])\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+func TestMemmoveLarge0x180000(t *testing.T) {\n+\tt.Parallel()\n+\tif race.Enabled {\n+\t\tt.Skip(\"skipping large memmove test under race detector\")\n+\t}\n+\ttestSize(t, 0x180000)\n+}\n+\n+func TestMemmoveOverlapLarge0x120000(t *testing.T) {\n+\tt.Parallel()\n+\tif race.Enabled {\n+\t\tt.Skip(\"skipping large memmove test under race detector\")\n+\t}\n+\ttestOverlap(t, 0x120000)\n+}\n+\n+func testSize(t *testing.T, size int) {\n+\tsrc := make([]byte, size)\n+\tdst := make([]byte, size)\n+\t_, _ = rand.Read(src)\n+\t_, _ = rand.Read(dst)\n+\n+\tref := make([]byte, size)\n+\tcopyref(ref, dst)\n+\n+\tfor n := size - 50; n > 1; n >>= 1 {\n+\t\tfor x := 0; x <= size-n; x = x*7 + 1 { // offset in src\n+\t\t\tfor y := 0; y <= size-n; y = y*9 + 1 { // offset in dst\n+\t\t\t\tcopy(dst[y:y+n], src[x:x+n])\n+\t\t\t\tcopyref(ref[y:y+n], src[x:x+n])\n+\t\t\t\tp := cmpb(dst, ref)\n+\t\t\t\tif p >= 0 {\n+\t\t\t\t\tt.Fatalf(\"Copy failed, copying from src[%d:%d] to dst[%d:%d].\\nOffset %d is different, %v != %v\", x, x+n, y, y+n, p, dst[p], ref[p])\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+func testOverlap(t *testing.T, size int) {\n+\tsrc := make([]byte, size)\n+\ttest := make([]byte, size)\n+\tref := make([]byte, size)\n+\t_, _ = rand.Read(src)\n+\n+\tfor n := size - 50; n > 1; n >>= 1 {\n+\t\tfor x := 0; x <= size-n; x = x*7 + 1 { // offset in src\n+\t\t\tfor y := 0; y <= size-n; y = y*9 + 1 { // offset in dst\n+\t\t\t\t// Reset input\n+\t\t\t\tcopyref(test, src)\n+\t\t\t\tcopyref(ref, src)\n+\t\t\t\tcopy(test[y:y+n], test[x:x+n])\n+\t\t\t\tif y <= x {\n+\t\t\t\t\tcopyref(ref[y:y+n], ref[x:x+n])\n+\t\t\t\t} else {\n+\t\t\t\t\tcopybw(ref[y:y+n], ref[x:x+n])\n+\t\t\t\t}\n+\t\t\t\tp := cmpb(test, ref)\n+\t\t\t\tif p >= 0 {\n+\t\t\t\t\tt.Fatalf(\"Copy failed, copying from src[%d:%d] to dst[%d:%d].\\nOffset %d is different, %v != %v\", x, x+n, y, y+n, p, test[p], ref[p])\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+}\n+\n+// Forward copy.\n+func copyref(dst, src []byte) {\n+\tfor i, v := range src {\n+\t\tdst[i] = v\n+\t}\n+}\n+\n+// Backwards copy\n+func copybw(dst, src []byte) {\n+\tif len(src) == 0 {\n+\t\treturn\n+\t}\n+\tfor i := len(src) - 1; i >= 0; i-- {\n+\t\tdst[i] = src[i]\n+\t}\n+}\n+\n+// Returns offset of difference\n+func matchLen(a, b []byte, max int) int {\n+\ta = a[:max]\n+\tb = b[:max]\n+\tfor i, av := range a {\n+\t\tif b[i] != av {\n+\t\t\treturn i\n+\t\t}\n+\t}\n+\treturn max\n+}\n+\n+func cmpb(a, b []byte) int {\n+\tl := matchLen(a, b, len(a))\n+\tif l == len(a) {\n+\t\treturn -1\n+\t}\n+\treturn l\n+}\n+\n+func benchmarkSizes(b *testing.B, sizes []int, fn func(b *testing.B, n int)) {\n+\tfor _, n := range sizes {\n+\t\tb.Run(fmt.Sprint(n), func(b *testing.B) {\n+\t\t\tb.SetBytes(int64(n))\n+\t\t\tfn(b, n)\n+\t\t})\n+\t}\n+}\n+\n+var bufSizes = []int{\n+\t0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,\n+\t32, 64, 128, 256, 512, 1024, 2048, 4096,\n+}\n+\n+func BenchmarkMemmove(b *testing.B) {\n+\tbenchmarkSizes(b, bufSizes, func(b *testing.B, n int) {\n+\t\tx := make([]byte, n)\n+\t\ty := make([]byte, n)\n+\t\tfor i := 0; i < b.N; i++ {\n+\t\t\tcopy(x, y)\n+\t\t}\n+\t})\n+}\n+\n+func BenchmarkMemmoveUnalignedDst(b *testing.B) {\n+\tbenchmarkSizes(b, bufSizes, func(b *testing.B, n int) {\n+\t\tx := make([]byte, n+1)\n+\t\ty := make([]byte, n)\n+\t\tfor i := 0; i < b.N; i++ {\n+\t\t\tcopy(x[1:], y)\n+\t\t}\n+\t})\n+}\n+\n+func BenchmarkMemmoveUnalignedSrc(b *testing.B) {\n+\tbenchmarkSizes(b, bufSizes, func(b *testing.B, n int) {\n+\t\tx := make([]byte, n)\n+\t\ty := make([]byte, n+1)\n+\t\tfor i := 0; i < b.N; i++ {\n+\t\t\tcopy(x, y[1:])\n+\t\t}\n+\t})\n+}\n+\n+func TestMemclr(t *testing.T) {\n+\tsize := 512\n+\tif testing.Short() {\n+\t\tsize = 128 + 16\n+\t}\n+\tmem := make([]byte, size)\n+\tfor i := 0; i < size; i++ {\n+\t\tmem[i] = 0xee\n+\t}\n+\tfor n := 0; n < size; n++ {\n+\t\tfor x := 0; x <= size-n; x++ { // offset in mem\n+\t\t\tMemclrBytes(mem[x : x+n])\n+\t\t\tfor i := 0; i < x; i++ {\n+\t\t\t\tif mem[i] != 0xee {\n+\t\t\t\t\tt.Fatalf(\"overwrite prefix mem[%d] = %d\", i, mem[i])\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tfor i := x; i < x+n; i++ {\n+\t\t\t\tif mem[i] != 0 {\n+\t\t\t\t\tt.Fatalf(\"failed clear mem[%d] = %d\", i, mem[i])\n+\t\t\t\t}\n+\t\t\t\tmem[i] = 0xee\n+\t\t\t}\n+\t\t\tfor i := x + n; i < size; i++ {\n+\t\t\t\tif mem[i] != 0xee {\n+\t\t\t\t\tt.Fatalf(\"overwrite suffix mem[%d] = %d\", i, mem[i])\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+func BenchmarkMemclr(b *testing.B) {\n+\tfor _, n := range []int{5, 16, 64, 256, 4096, 65536} {\n+\t\tx := make([]byte, n)\n+\t\tb.Run(fmt.Sprint(n), func(b *testing.B) {\n+\t\t\tb.SetBytes(int64(n))\n+\t\t\tfor i := 0; i < b.N; i++ {\n+\t\t\t\tMemclrBytes(x)\n+\t\t\t}\n+\t\t})\n+\t}\n+\tfor _, m := range []int{1, 4, 8, 16, 64} {\n+\t\tx := make([]byte, m<<20)\n+\t\tb.Run(fmt.Sprint(m, \"M\"), func(b *testing.B) {\n+\t\t\tb.SetBytes(int64(m << 20))\n+\t\t\tfor i := 0; i < b.N; i++ {\n+\t\t\t\tMemclrBytes(x)\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+func BenchmarkGoMemclr(b *testing.B) {\n+\tbenchmarkSizes(b, []int{5, 16, 64, 256}, func(b *testing.B, n int) {\n+\t\tx := make([]byte, n)\n+\t\tfor i := 0; i < b.N; i++ {\n+\t\t\tfor j := range x {\n+\t\t\t\tx[j] = 0\n+\t\t\t}\n+\t\t}\n+\t})\n+}\n+\n+func BenchmarkClearFat8(b *testing.B) {\n+\tfor i := 0; i < b.N; i++ {\n+\t\tvar x [8 / 4]uint32\n+\t\t_ = x\n+\t}\n+}\n+func BenchmarkClearFat12(b *testing.B) {\n+\tfor i := 0; i < b.N; i++ {\n+\t\tvar x [12 / 4]uint32\n+\t\t_ = x\n+\t}\n+}\n+func BenchmarkClearFat16(b *testing.B) {\n+\tfor i := 0; i < b.N; i++ {\n+\t\tvar x [16 / 4]uint32\n+\t\t_ = x\n+\t}\n+}\n+func BenchmarkClearFat24(b *testing.B) {\n+\tfor i := 0; i < b.N; i++ {\n+\t\tvar x [24 / 4]uint32\n+\t\t_ = x\n+\t}\n+}\n+func BenchmarkClearFat32(b *testing.B) {\n+\tfor i := 0; i < b.N; i++ {\n+\t\tvar x [32 / 4]uint32\n+\t\t_ = x\n+\t}\n+}\n+func BenchmarkClearFat40(b *testing.B) {\n+\tfor i := 0; i < b.N; i++ {\n+\t\tvar x [40 / 4]uint32\n+\t\t_ = x\n+\t}\n+}\n+func BenchmarkClearFat48(b *testing.B) {\n+\tfor i := 0; i < b.N; i++ {\n+\t\tvar x [48 / 4]uint32\n+\t\t_ = x\n+\t}\n+}\n+func BenchmarkClearFat56(b *testing.B) {\n+\tfor i := 0; i < b.N; i++ {\n+\t\tvar x [56 / 4]uint32\n+\t\t_ = x\n+\t}\n+}\n+func BenchmarkClearFat64(b *testing.B) {\n+\tfor i := 0; i < b.N; i++ {\n+\t\tvar x [64 / 4]uint32\n+\t\t_ = x\n+\t}\n+}\n+func BenchmarkClearFat128(b *testing.B) {\n+\tfor i := 0; i < b.N; i++ {\n+\t\tvar x [128 / 4]uint32\n+\t\t_ = x\n+\t}\n+}\n+func BenchmarkClearFat256(b *testing.B) {\n+\tfor i := 0; i < b.N; i++ {\n+\t\tvar x [256 / 4]uint32\n+\t\t_ = x\n+\t}\n+}\n+func BenchmarkClearFat512(b *testing.B) {\n+\tfor i := 0; i < b.N; i++ {\n+\t\tvar x [512 / 4]uint32\n+\t\t_ = x\n+\t}\n+}\n+func BenchmarkClearFat1024(b *testing.B) {\n+\tfor i := 0; i < b.N; i++ {\n+\t\tvar x [1024 / 4]uint32\n+\t\t_ = x\n+\t}\n+}\n+\n+func BenchmarkCopyFat8(b *testing.B) {\n+\tvar x [8 / 4]uint32\n+\tfor i := 0; i < b.N; i++ {\n+\t\ty := x\n+\t\t_ = y\n+\t}\n+}\n+func BenchmarkCopyFat12(b *testing.B) {\n+\tvar x [12 / 4]uint32\n+\tfor i := 0; i < b.N; i++ {\n+\t\ty := x\n+\t\t_ = y\n+\t}\n+}\n+func BenchmarkCopyFat16(b *testing.B) {\n+\tvar x [16 / 4]uint32\n+\tfor i := 0; i < b.N; i++ {\n+\t\ty := x\n+\t\t_ = y\n+\t}\n+}\n+func BenchmarkCopyFat24(b *testing.B) {\n+\tvar x [24 / 4]uint32\n+\tfor i := 0; i < b.N; i++ {\n+\t\ty := x\n+\t\t_ = y\n+\t}\n+}\n+func BenchmarkCopyFat32(b *testing.B) {\n+\tvar x [32 / 4]uint32\n+\tfor i := 0; i < b.N; i++ {\n+\t\ty := x\n+\t\t_ = y\n+\t}\n+}\n+func BenchmarkCopyFat64(b *testing.B) {\n+\tvar x [64 / 4]uint32\n+\tfor i := 0; i < b.N; i++ {\n+\t\ty := x\n+\t\t_ = y\n+\t}\n+}\n+func BenchmarkCopyFat128(b *testing.B) {\n+\tvar x [128 / 4]uint32\n+\tfor i := 0; i < b.N; i++ {\n+\t\ty := x\n+\t\t_ = y\n+\t}\n+}\n+func BenchmarkCopyFat256(b *testing.B) {\n+\tvar x [256 / 4]uint32\n+\tfor i := 0; i < b.N; i++ {\n+\t\ty := x\n+\t\t_ = y\n+\t}\n+}\n+func BenchmarkCopyFat512(b *testing.B) {\n+\tvar x [512 / 4]uint32\n+\tfor i := 0; i < b.N; i++ {\n+\t\ty := x\n+\t\t_ = y\n+\t}\n+}\n+func BenchmarkCopyFat1024(b *testing.B) {\n+\tvar x [1024 / 4]uint32\n+\tfor i := 0; i < b.N; i++ {\n+\t\ty := x\n+\t\t_ = y\n+\t}\n+}\n+\n+func BenchmarkIssue18740(b *testing.B) {\n+\t// This tests that memmove uses one 4-byte load/store to move 4 bytes.\n+\t// It used to do 2 2-byte load/stores, which leads to a pipeline stall\n+\t// when we try to read the result with one 4-byte load.\n+\tvar buf [4]byte\n+\tfor j := 0; j < b.N; j++ {\n+\t\ts := uint32(0)\n+\t\tfor i := 0; i < 4096; i += 4 {\n+\t\t\tcopy(buf[:], g[i:])\n+\t\t\ts += binary.LittleEndian.Uint32(buf[:])\n+\t\t}\n+\t\tsink = uint64(s)\n+\t}\n+}\n+\n+// TODO: 2 byte and 8 byte benchmarks also.\n+\n+var g [4096]byte"}, {"sha": "f0123b399f25607261813cd1fa615a14bf9bda9f", "filename": "libgo/go/runtime/mfinal.go", "status": "added", "additions": 424, "deletions": 0, "changes": 424, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmfinal.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmfinal.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmfinal.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,424 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Garbage collector: finalizers and block profiling.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/atomic\"\n+\t\"runtime/internal/sys\"\n+\t\"unsafe\"\n+)\n+\n+// finblock is allocated from non-GC'd memory, so any heap pointers\n+// must be specially handled.\n+//\n+//go:notinheap\n+type finblock struct {\n+\talllink *finblock\n+\tnext    *finblock\n+\tcnt     uint32\n+\t_       int32\n+\tfin     [(_FinBlockSize - 2*sys.PtrSize - 2*4) / unsafe.Sizeof(finalizer{})]finalizer\n+}\n+\n+var finlock mutex  // protects the following variables\n+var fing *g        // goroutine that runs finalizers\n+var finq *finblock // list of finalizers that are to be executed\n+var finc *finblock // cache of free blocks\n+var finptrmask [_FinBlockSize / sys.PtrSize / 8]byte\n+var fingwait bool\n+var fingwake bool\n+var allfin *finblock // list of all blocks\n+\n+// NOTE: Layout known to queuefinalizer.\n+type finalizer struct {\n+\tfn  *funcval       // function to call (may be a heap pointer)\n+\targ unsafe.Pointer // ptr to object (may be a heap pointer)\n+\tft  *functype      // type of fn (unlikely, but may be a heap pointer)\n+\tot  *ptrtype       // type of ptr to object (may be a heap pointer)\n+}\n+\n+func queuefinalizer(p unsafe.Pointer, fn *funcval, ft *functype, ot *ptrtype) {\n+\tlock(&finlock)\n+\tif finq == nil || finq.cnt == uint32(len(finq.fin)) {\n+\t\tif finc == nil {\n+\t\t\tfinc = (*finblock)(persistentalloc(_FinBlockSize, 0, &memstats.gc_sys))\n+\t\t\tfinc.alllink = allfin\n+\t\t\tallfin = finc\n+\t\t\tif finptrmask[0] == 0 {\n+\t\t\t\t// Build pointer mask for Finalizer array in block.\n+\t\t\t\t// We allocate values of type finalizer in\n+\t\t\t\t// finblock values. Since these values are\n+\t\t\t\t// allocated by persistentalloc, they require\n+\t\t\t\t// special scanning during GC. finptrmask is a\n+\t\t\t\t// pointer mask to use while scanning.\n+\t\t\t\t// Since all the values in finalizer are\n+\t\t\t\t// pointers, just turn all bits on.\n+\t\t\t\tfor i := range finptrmask {\n+\t\t\t\t\tfinptrmask[i] = 0xff\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t\tblock := finc\n+\t\tfinc = block.next\n+\t\tblock.next = finq\n+\t\tfinq = block\n+\t}\n+\tf := &finq.fin[finq.cnt]\n+\tatomic.Xadd(&finq.cnt, +1) // Sync with markroots\n+\tf.fn = fn\n+\tf.ft = ft\n+\tf.ot = ot\n+\tf.arg = p\n+\tfingwake = true\n+\tunlock(&finlock)\n+}\n+\n+//go:nowritebarrier\n+func iterate_finq(callback func(*funcval, unsafe.Pointer, *functype, *ptrtype)) {\n+\tfor fb := allfin; fb != nil; fb = fb.alllink {\n+\t\tfor i := uint32(0); i < fb.cnt; i++ {\n+\t\t\tf := &fb.fin[i]\n+\t\t\tcallback(f.fn, f.arg, f.ft, f.ot)\n+\t\t}\n+\t}\n+}\n+\n+func wakefing() *g {\n+\tvar res *g\n+\tlock(&finlock)\n+\tif fingwait && fingwake {\n+\t\tfingwait = false\n+\t\tfingwake = false\n+\t\tres = fing\n+\t}\n+\tunlock(&finlock)\n+\treturn res\n+}\n+\n+var (\n+\tfingCreate  uint32\n+\tfingRunning bool\n+)\n+\n+func createfing() {\n+\t// start the finalizer goroutine exactly once\n+\tif fingCreate == 0 && atomic.Cas(&fingCreate, 0, 1) {\n+\t\tgo runfinq()\n+\t}\n+}\n+\n+// This is the goroutine that runs all of the finalizers\n+func runfinq() {\n+\tvar (\n+\t\tef   eface\n+\t\tifac iface\n+\t)\n+\n+\tfor {\n+\t\tlock(&finlock)\n+\t\tfb := finq\n+\t\tfinq = nil\n+\t\tif fb == nil {\n+\t\t\tgp := getg()\n+\t\t\tfing = gp\n+\t\t\tfingwait = true\n+\t\t\tgoparkunlock(&finlock, \"finalizer wait\", traceEvGoBlock, 1)\n+\t\t\tcontinue\n+\t\t}\n+\t\tunlock(&finlock)\n+\t\tfor fb != nil {\n+\t\t\tfor i := fb.cnt; i > 0; i-- {\n+\t\t\t\tf := &fb.fin[i-1]\n+\n+\t\t\t\tif f.ft == nil {\n+\t\t\t\t\tthrow(\"missing type in runfinq\")\n+\t\t\t\t}\n+\t\t\t\tfint := f.ft.in[0]\n+\t\t\t\tvar param unsafe.Pointer\n+\t\t\t\tswitch fint.kind & kindMask {\n+\t\t\t\tcase kindPtr:\n+\t\t\t\t\t// direct use of pointer\n+\t\t\t\t\tparam = unsafe.Pointer(&f.arg)\n+\t\t\t\tcase kindInterface:\n+\t\t\t\t\tityp := (*interfacetype)(unsafe.Pointer(fint))\n+\t\t\t\t\tif len(ityp.methods) == 0 {\n+\t\t\t\t\t\t// set up with empty interface\n+\t\t\t\t\t\tef._type = &f.ot.typ\n+\t\t\t\t\t\tef.data = f.arg\n+\t\t\t\t\t\tparam = unsafe.Pointer(&ef)\n+\t\t\t\t\t} else {\n+\t\t\t\t\t\t// convert to interface with methods\n+\t\t\t\t\t\t// this conversion is guaranteed to succeed - we checked in SetFinalizer\n+\t\t\t\t\t\tifac.tab = getitab(fint, &f.ot.typ, true)\n+\t\t\t\t\t\tifac.data = f.arg\n+\t\t\t\t\t\tparam = unsafe.Pointer(&ifac)\n+\t\t\t\t\t}\n+\t\t\t\tdefault:\n+\t\t\t\t\tthrow(\"bad kind in runfinq\")\n+\t\t\t\t}\n+\t\t\t\tfingRunning = true\n+\t\t\t\treflectcall(f.ft, f.fn, false, false, &param, nil)\n+\t\t\t\tfingRunning = false\n+\n+\t\t\t\t// Drop finalizer queue heap references\n+\t\t\t\t// before hiding them from markroot.\n+\t\t\t\t// This also ensures these will be\n+\t\t\t\t// clear if we reuse the finalizer.\n+\t\t\t\tf.fn = nil\n+\t\t\t\tf.arg = nil\n+\t\t\t\tf.ot = nil\n+\t\t\t\tatomic.Store(&fb.cnt, i-1)\n+\t\t\t}\n+\t\t\tnext := fb.next\n+\t\t\tlock(&finlock)\n+\t\t\tfb.next = finc\n+\t\t\tfinc = fb\n+\t\t\tunlock(&finlock)\n+\t\t\tfb = next\n+\t\t}\n+\t}\n+}\n+\n+// SetFinalizer sets the finalizer associated with obj to the provided\n+// finalizer function. When the garbage collector finds an unreachable block\n+// with an associated finalizer, it clears the association and runs\n+// finalizer(obj) in a separate goroutine. This makes obj reachable again,\n+// but now without an associated finalizer. Assuming that SetFinalizer\n+// is not called again, the next time the garbage collector sees\n+// that obj is unreachable, it will free obj.\n+//\n+// SetFinalizer(obj, nil) clears any finalizer associated with obj.\n+//\n+// The argument obj must be a pointer to an object allocated by calling\n+// new, by taking the address of a composite literal, or by taking the\n+// address of a local variable.\n+// The argument finalizer must be a function that takes a single argument\n+// to which obj's type can be assigned, and can have arbitrary ignored return\n+// values. If either of these is not true, SetFinalizer may abort the\n+// program.\n+//\n+// Finalizers are run in dependency order: if A points at B, both have\n+// finalizers, and they are otherwise unreachable, only the finalizer\n+// for A runs; once A is freed, the finalizer for B can run.\n+// If a cyclic structure includes a block with a finalizer, that\n+// cycle is not guaranteed to be garbage collected and the finalizer\n+// is not guaranteed to run, because there is no ordering that\n+// respects the dependencies.\n+//\n+// The finalizer for obj is scheduled to run at some arbitrary time after\n+// obj becomes unreachable.\n+// There is no guarantee that finalizers will run before a program exits,\n+// so typically they are useful only for releasing non-memory resources\n+// associated with an object during a long-running program.\n+// For example, an os.File object could use a finalizer to close the\n+// associated operating system file descriptor when a program discards\n+// an os.File without calling Close, but it would be a mistake\n+// to depend on a finalizer to flush an in-memory I/O buffer such as a\n+// bufio.Writer, because the buffer would not be flushed at program exit.\n+//\n+// It is not guaranteed that a finalizer will run if the size of *obj is\n+// zero bytes.\n+//\n+// It is not guaranteed that a finalizer will run for objects allocated\n+// in initializers for package-level variables. Such objects may be\n+// linker-allocated, not heap-allocated.\n+//\n+// A finalizer may run as soon as an object becomes unreachable.\n+// In order to use finalizers correctly, the program must ensure that\n+// the object is reachable until it is no longer required.\n+// Objects stored in global variables, or that can be found by tracing\n+// pointers from a global variable, are reachable. For other objects,\n+// pass the object to a call of the KeepAlive function to mark the\n+// last point in the function where the object must be reachable.\n+//\n+// For example, if p points to a struct that contains a file descriptor d,\n+// and p has a finalizer that closes that file descriptor, and if the last\n+// use of p in a function is a call to syscall.Write(p.d, buf, size), then\n+// p may be unreachable as soon as the program enters syscall.Write. The\n+// finalizer may run at that moment, closing p.d, causing syscall.Write\n+// to fail because it is writing to a closed file descriptor (or, worse,\n+// to an entirely different file descriptor opened by a different goroutine).\n+// To avoid this problem, call runtime.KeepAlive(p) after the call to\n+// syscall.Write.\n+//\n+// A single goroutine runs all finalizers for a program, sequentially.\n+// If a finalizer must run for a long time, it should do so by starting\n+// a new goroutine.\n+func SetFinalizer(obj interface{}, finalizer interface{}) {\n+\tif debug.sbrk != 0 {\n+\t\t// debug.sbrk never frees memory, so no finalizers run\n+\t\t// (and we don't have the data structures to record them).\n+\t\treturn\n+\t}\n+\te := efaceOf(&obj)\n+\tetyp := e._type\n+\tif etyp == nil {\n+\t\tthrow(\"runtime.SetFinalizer: first argument is nil\")\n+\t}\n+\tif etyp.kind&kindMask != kindPtr {\n+\t\tthrow(\"runtime.SetFinalizer: first argument is \" + *etyp.string + \", not pointer\")\n+\t}\n+\tot := (*ptrtype)(unsafe.Pointer(etyp))\n+\tif ot.elem == nil {\n+\t\tthrow(\"nil elem type!\")\n+\t}\n+\n+\t// find the containing object\n+\t_, base, _ := findObject(e.data)\n+\n+\tif base == nil {\n+\t\t// 0-length objects are okay.\n+\t\tif e.data == unsafe.Pointer(&zerobase) {\n+\t\t\treturn\n+\t\t}\n+\n+\t\t// Global initializers might be linker-allocated.\n+\t\t//\tvar Foo = &Object{}\n+\t\t//\tfunc main() {\n+\t\t//\t\truntime.SetFinalizer(Foo, nil)\n+\t\t//\t}\n+\t\t// The relevant segments are: noptrdata, data, bss, noptrbss.\n+\t\t// We cannot assume they are in any order or even contiguous,\n+\t\t// due to external linking.\n+\t\t//\n+\t\t// For gccgo we have no reliable way to detect them,\n+\t\t// so we just return.\n+\t\treturn\n+\t}\n+\n+\tif e.data != base {\n+\t\t// As an implementation detail we allow to set finalizers for an inner byte\n+\t\t// of an object if it could come from tiny alloc (see mallocgc for details).\n+\t\tif ot.elem == nil || ot.elem.kind&kindNoPointers == 0 || ot.elem.size >= maxTinySize {\n+\t\t\tthrow(\"runtime.SetFinalizer: pointer not at beginning of allocated block\")\n+\t\t}\n+\t}\n+\n+\tf := efaceOf(&finalizer)\n+\tftyp := f._type\n+\tif ftyp == nil {\n+\t\t// switch to system stack and remove finalizer\n+\t\tsystemstack(func() {\n+\t\t\tremovefinalizer(e.data)\n+\t\t})\n+\t\treturn\n+\t}\n+\n+\tif ftyp.kind&kindMask != kindFunc {\n+\t\tthrow(\"runtime.SetFinalizer: second argument is \" + *ftyp.string + \", not a function\")\n+\t}\n+\tft := (*functype)(unsafe.Pointer(ftyp))\n+\tif ft.dotdotdot {\n+\t\tthrow(\"runtime.SetFinalizer: cannot pass \" + *etyp.string + \" to finalizer \" + *ftyp.string + \" because dotdotdot\")\n+\t}\n+\tif len(ft.in) != 1 {\n+\t\tthrow(\"runtime.SetFinalizer: cannot pass \" + *etyp.string + \" to finalizer \" + *ftyp.string)\n+\t}\n+\tfint := ft.in[0]\n+\tswitch {\n+\tcase fint == etyp:\n+\t\t// ok - same type\n+\t\tgoto okarg\n+\tcase fint.kind&kindMask == kindPtr:\n+\t\tif (fint.uncommontype == nil || etyp.uncommontype == nil) && (*ptrtype)(unsafe.Pointer(fint)).elem == ot.elem {\n+\t\t\t// ok - not same type, but both pointers,\n+\t\t\t// one or the other is unnamed, and same element type, so assignable.\n+\t\t\tgoto okarg\n+\t\t}\n+\tcase fint.kind&kindMask == kindInterface:\n+\t\tityp := (*interfacetype)(unsafe.Pointer(fint))\n+\t\tif len(ityp.methods) == 0 {\n+\t\t\t// ok - satisfies empty interface\n+\t\t\tgoto okarg\n+\t\t}\n+\t\tif getitab(fint, etyp, true) == nil {\n+\t\t\tgoto okarg\n+\t\t}\n+\t}\n+\tthrow(\"runtime.SetFinalizer: cannot pass \" + *etyp.string + \" to finalizer \" + *ftyp.string)\n+okarg:\n+\t// make sure we have a finalizer goroutine\n+\tcreatefing()\n+\n+\tsystemstack(func() {\n+\t\tdata := f.data\n+\t\tif !isDirectIface(ftyp) {\n+\t\t\tdata = *(*unsafe.Pointer)(data)\n+\t\t}\n+\t\tif !addfinalizer(e.data, (*funcval)(data), ft, ot) {\n+\t\t\tthrow(\"runtime.SetFinalizer: finalizer already set\")\n+\t\t}\n+\t})\n+}\n+\n+// Look up pointer v in heap. Return the span containing the object,\n+// the start of the object, and the size of the object. If the object\n+// does not exist, return nil, nil, 0.\n+func findObject(v unsafe.Pointer) (s *mspan, x unsafe.Pointer, n uintptr) {\n+\tc := gomcache()\n+\tc.local_nlookup++\n+\tif sys.PtrSize == 4 && c.local_nlookup >= 1<<30 {\n+\t\t// purge cache stats to prevent overflow\n+\t\tlock(&mheap_.lock)\n+\t\tpurgecachedstats(c)\n+\t\tunlock(&mheap_.lock)\n+\t}\n+\n+\t// find span\n+\tarena_start := mheap_.arena_start\n+\tarena_used := mheap_.arena_used\n+\tif uintptr(v) < arena_start || uintptr(v) >= arena_used {\n+\t\treturn\n+\t}\n+\tp := uintptr(v) >> pageShift\n+\tq := p - arena_start>>pageShift\n+\ts = mheap_.spans[q]\n+\tif s == nil {\n+\t\treturn\n+\t}\n+\tx = unsafe.Pointer(s.base())\n+\n+\tif uintptr(v) < uintptr(x) || uintptr(v) >= uintptr(unsafe.Pointer(s.limit)) || s.state != mSpanInUse {\n+\t\ts = nil\n+\t\tx = nil\n+\t\treturn\n+\t}\n+\n+\tn = s.elemsize\n+\tif s.sizeclass != 0 {\n+\t\tx = add(x, (uintptr(v)-uintptr(x))/n*n)\n+\t}\n+\treturn\n+}\n+\n+// Mark KeepAlive as noinline so that the current compiler will ensure\n+// that the argument is alive at the point of the function call.\n+// If it were inlined, it would disappear, and there would be nothing\n+// keeping the argument alive. Perhaps a future compiler will recognize\n+// runtime.KeepAlive specially and do something more efficient.\n+//go:noinline\n+\n+// KeepAlive marks its argument as currently reachable.\n+// This ensures that the object is not freed, and its finalizer is not run,\n+// before the point in the program where KeepAlive is called.\n+//\n+// A very simplified example showing where KeepAlive is required:\n+// \ttype File struct { d int }\n+// \td, err := syscall.Open(\"/file/path\", syscall.O_RDONLY, 0)\n+// \t// ... do something if err != nil ...\n+// \tp := &File{d}\n+// \truntime.SetFinalizer(p, func(p *File) { syscall.Close(p.d) })\n+// \tvar buf [10]byte\n+// \tn, err := syscall.Read(p.d, buf[:])\n+// \t// Ensure p is not finalized until Read returns.\n+// \truntime.KeepAlive(p)\n+// \t// No more uses of p after this point.\n+//\n+// Without the KeepAlive call, the finalizer could run at the start of\n+// syscall.Read, closing the file descriptor before syscall.Read makes\n+// the actual system call.\n+func KeepAlive(interface{}) {}"}, {"sha": "fe4b0fcf2a947a75b06b0109fa907a7279757c88", "filename": "libgo/go/runtime/mfixalloc.go", "status": "added", "additions": 99, "deletions": 0, "changes": 99, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmfixalloc.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmfixalloc.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmfixalloc.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,99 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Fixed-size object allocator. Returned memory is not zeroed.\n+//\n+// See malloc.go for overview.\n+\n+package runtime\n+\n+import \"unsafe\"\n+\n+// FixAlloc is a simple free-list allocator for fixed size objects.\n+// Malloc uses a FixAlloc wrapped around sysAlloc to manages its\n+// MCache and MSpan objects.\n+//\n+// Memory returned by fixalloc.alloc is zeroed by default, but the\n+// caller may take responsibility for zeroing allocations by setting\n+// the zero flag to false. This is only safe if the memory never\n+// contains heap pointers.\n+//\n+// The caller is responsible for locking around FixAlloc calls.\n+// Callers can keep state in the object but the first word is\n+// smashed by freeing and reallocating.\n+//\n+// Consider marking fixalloc'd types go:notinheap.\n+type fixalloc struct {\n+\tsize   uintptr\n+\tfirst  func(arg, p unsafe.Pointer) // called first time p is returned\n+\targ    unsafe.Pointer\n+\tlist   *mlink\n+\tchunk  unsafe.Pointer\n+\tnchunk uint32\n+\tinuse  uintptr // in-use bytes now\n+\tstat   *uint64\n+\tzero   bool // zero allocations\n+}\n+\n+// A generic linked list of blocks.  (Typically the block is bigger than sizeof(MLink).)\n+// Since assignments to mlink.next will result in a write barrier being performed\n+// this cannot be used by some of the internal GC structures. For example when\n+// the sweeper is placing an unmarked object on the free list it does not want the\n+// write barrier to be called since that could result in the object being reachable.\n+//\n+//go:notinheap\n+type mlink struct {\n+\tnext *mlink\n+}\n+\n+// Initialize f to allocate objects of the given size,\n+// using the allocator to obtain chunks of memory.\n+func (f *fixalloc) init(size uintptr, first func(arg, p unsafe.Pointer), arg unsafe.Pointer, stat *uint64) {\n+\tf.size = size\n+\tf.first = first\n+\tf.arg = arg\n+\tf.list = nil\n+\tf.chunk = nil\n+\tf.nchunk = 0\n+\tf.inuse = 0\n+\tf.stat = stat\n+\tf.zero = true\n+}\n+\n+func (f *fixalloc) alloc() unsafe.Pointer {\n+\tif f.size == 0 {\n+\t\tprint(\"runtime: use of FixAlloc_Alloc before FixAlloc_Init\\n\")\n+\t\tthrow(\"runtime: internal error\")\n+\t}\n+\n+\tif f.list != nil {\n+\t\tv := unsafe.Pointer(f.list)\n+\t\tf.list = f.list.next\n+\t\tf.inuse += f.size\n+\t\tif f.zero {\n+\t\t\tmemclrNoHeapPointers(v, f.size)\n+\t\t}\n+\t\treturn v\n+\t}\n+\tif uintptr(f.nchunk) < f.size {\n+\t\tf.chunk = persistentalloc(_FixAllocChunk, 0, f.stat)\n+\t\tf.nchunk = _FixAllocChunk\n+\t}\n+\n+\tv := f.chunk\n+\tif f.first != nil {\n+\t\tf.first(f.arg, v)\n+\t}\n+\tf.chunk = add(f.chunk, f.size)\n+\tf.nchunk -= uint32(f.size)\n+\tf.inuse += f.size\n+\treturn v\n+}\n+\n+func (f *fixalloc) free(p unsafe.Pointer) {\n+\tf.inuse -= f.size\n+\tv := (*mlink)(p)\n+\tv.next = f.list\n+\tf.list = v\n+}"}, {"sha": "abec9d3cd247b0f8a816437cdda83345f1c9c147", "filename": "libgo/go/runtime/mgc.go", "status": "added", "additions": 1963, "deletions": 0, "changes": 1963, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmgc.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmgc.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmgc.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,1963 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Garbage collector (GC).\n+//\n+// The GC runs concurrently with mutator threads, is type accurate (aka precise), allows multiple\n+// GC thread to run in parallel. It is a concurrent mark and sweep that uses a write barrier. It is\n+// non-generational and non-compacting. Allocation is done using size segregated per P allocation\n+// areas to minimize fragmentation while eliminating locks in the common case.\n+//\n+// The algorithm decomposes into several steps.\n+// This is a high level description of the algorithm being used. For an overview of GC a good\n+// place to start is Richard Jones' gchandbook.org.\n+//\n+// The algorithm's intellectual heritage includes Dijkstra's on-the-fly algorithm, see\n+// Edsger W. Dijkstra, Leslie Lamport, A. J. Martin, C. S. Scholten, and E. F. M. Steffens. 1978.\n+// On-the-fly garbage collection: an exercise in cooperation. Commun. ACM 21, 11 (November 1978),\n+// 966-975.\n+// For journal quality proofs that these steps are complete, correct, and terminate see\n+// Hudson, R., and Moss, J.E.B. Copying Garbage Collection without stopping the world.\n+// Concurrency and Computation: Practice and Experience 15(3-5), 2003.\n+//\n+// 1. GC performs sweep termination.\n+//\n+//    a. Stop the world. This causes all Ps to reach a GC safe-point.\n+//\n+//    b. Sweep any unswept spans. There will only be unswept spans if\n+//    this GC cycle was forced before the expected time.\n+//\n+// 2. GC performs the \"mark 1\" sub-phase. In this sub-phase, Ps are\n+// allowed to locally cache parts of the work queue.\n+//\n+//    a. Prepare for the mark phase by setting gcphase to _GCmark\n+//    (from _GCoff), enabling the write barrier, enabling mutator\n+//    assists, and enqueueing root mark jobs. No objects may be\n+//    scanned until all Ps have enabled the write barrier, which is\n+//    accomplished using STW.\n+//\n+//    b. Start the world. From this point, GC work is done by mark\n+//    workers started by the scheduler and by assists performed as\n+//    part of allocation. The write barrier shades both the\n+//    overwritten pointer and the new pointer value for any pointer\n+//    writes (see mbarrier.go for details). Newly allocated objects\n+//    are immediately marked black.\n+//\n+//    c. GC performs root marking jobs. This includes scanning all\n+//    stacks, shading all globals, and shading any heap pointers in\n+//    off-heap runtime data structures. Scanning a stack stops a\n+//    goroutine, shades any pointers found on its stack, and then\n+//    resumes the goroutine.\n+//\n+//    d. GC drains the work queue of grey objects, scanning each grey\n+//    object to black and shading all pointers found in the object\n+//    (which in turn may add those pointers to the work queue).\n+//\n+// 3. Once the global work queue is empty (but local work queue caches\n+// may still contain work), GC performs the \"mark 2\" sub-phase.\n+//\n+//    a. GC stops all workers, disables local work queue caches,\n+//    flushes each P's local work queue cache to the global work queue\n+//    cache, and reenables workers.\n+//\n+//    b. GC again drains the work queue, as in 2d above.\n+//\n+// 4. Once the work queue is empty, GC performs mark termination.\n+//\n+//    a. Stop the world.\n+//\n+//    b. Set gcphase to _GCmarktermination, and disable workers and\n+//    assists.\n+//\n+//    c. Drain any remaining work from the work queue (typically there\n+//    will be none).\n+//\n+//    d. Perform other housekeeping like flushing mcaches.\n+//\n+// 5. GC performs the sweep phase.\n+//\n+//    a. Prepare for the sweep phase by setting gcphase to _GCoff,\n+//    setting up sweep state and disabling the write barrier.\n+//\n+//    b. Start the world. From this point on, newly allocated objects\n+//    are white, and allocating sweeps spans before use if necessary.\n+//\n+//    c. GC does concurrent sweeping in the background and in response\n+//    to allocation. See description below.\n+//\n+// 6. When sufficient allocation has taken place, replay the sequence\n+// starting with 1 above. See discussion of GC rate below.\n+\n+// Concurrent sweep.\n+//\n+// The sweep phase proceeds concurrently with normal program execution.\n+// The heap is swept span-by-span both lazily (when a goroutine needs another span)\n+// and concurrently in a background goroutine (this helps programs that are not CPU bound).\n+// At the end of STW mark termination all spans are marked as \"needs sweeping\".\n+//\n+// The background sweeper goroutine simply sweeps spans one-by-one.\n+//\n+// To avoid requesting more OS memory while there are unswept spans, when a\n+// goroutine needs another span, it first attempts to reclaim that much memory\n+// by sweeping. When a goroutine needs to allocate a new small-object span, it\n+// sweeps small-object spans for the same object size until it frees at least\n+// one object. When a goroutine needs to allocate large-object span from heap,\n+// it sweeps spans until it frees at least that many pages into heap. There is\n+// one case where this may not suffice: if a goroutine sweeps and frees two\n+// nonadjacent one-page spans to the heap, it will allocate a new two-page\n+// span, but there can still be other one-page unswept spans which could be\n+// combined into a two-page span.\n+//\n+// It's critical to ensure that no operations proceed on unswept spans (that would corrupt\n+// mark bits in GC bitmap). During GC all mcaches are flushed into the central cache,\n+// so they are empty. When a goroutine grabs a new span into mcache, it sweeps it.\n+// When a goroutine explicitly frees an object or sets a finalizer, it ensures that\n+// the span is swept (either by sweeping it, or by waiting for the concurrent sweep to finish).\n+// The finalizer goroutine is kicked off only when all spans are swept.\n+// When the next GC starts, it sweeps all not-yet-swept spans (if any).\n+\n+// GC rate.\n+// Next GC is after we've allocated an extra amount of memory proportional to\n+// the amount already in use. The proportion is controlled by GOGC environment variable\n+// (100 by default). If GOGC=100 and we're using 4M, we'll GC again when we get to 8M\n+// (this mark is tracked in next_gc variable). This keeps the GC cost in linear\n+// proportion to the allocation cost. Adjusting GOGC just changes the linear constant\n+// (and also the amount of extra memory used).\n+\n+// Oblets\n+//\n+// In order to prevent long pauses while scanning large objects and to\n+// improve parallelism, the garbage collector breaks up scan jobs for\n+// objects larger than maxObletBytes into \"oblets\" of at most\n+// maxObletBytes. When scanning encounters the beginning of a large\n+// object, it scans only the first oblet and enqueues the remaining\n+// oblets as new scan jobs.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/atomic\"\n+\t\"runtime/internal/sys\"\n+\t\"unsafe\"\n+)\n+\n+const (\n+\t_DebugGC         = 0\n+\t_ConcurrentSweep = true\n+\t_FinBlockSize    = 4 * 1024\n+\n+\t// sweepMinHeapDistance is a lower bound on the heap distance\n+\t// (in bytes) reserved for concurrent sweeping between GC\n+\t// cycles. This will be scaled by gcpercent/100.\n+\tsweepMinHeapDistance = 1024 * 1024\n+)\n+\n+// heapminimum is the minimum heap size at which to trigger GC.\n+// For small heaps, this overrides the usual GOGC*live set rule.\n+//\n+// When there is a very small live set but a lot of allocation, simply\n+// collecting when the heap reaches GOGC*live results in many GC\n+// cycles and high total per-GC overhead. This minimum amortizes this\n+// per-GC overhead while keeping the heap reasonably small.\n+//\n+// During initialization this is set to 4MB*GOGC/100. In the case of\n+// GOGC==0, this will set heapminimum to 0, resulting in constant\n+// collection even when the heap size is small, which is useful for\n+// debugging.\n+var heapminimum uint64 = defaultHeapMinimum\n+\n+// defaultHeapMinimum is the value of heapminimum for GOGC==100.\n+const defaultHeapMinimum = 4 << 20\n+\n+// Initialized from $GOGC.  GOGC=off means no GC.\n+var gcpercent int32\n+\n+func gcinit() {\n+\tif unsafe.Sizeof(workbuf{}) != _WorkbufSize {\n+\t\tthrow(\"size of Workbuf is suboptimal\")\n+\t}\n+\n+\t_ = setGCPercent(readgogc())\n+\tmemstats.gc_trigger = heapminimum\n+\t// Compute the goal heap size based on the trigger:\n+\t//   trigger = marked * (1 + triggerRatio)\n+\t//   marked = trigger / (1 + triggerRatio)\n+\t//   goal = marked * (1 + GOGC/100)\n+\t//        = trigger / (1 + triggerRatio) * (1 + GOGC/100)\n+\tmemstats.next_gc = uint64(float64(memstats.gc_trigger) / (1 + gcController.triggerRatio) * (1 + float64(gcpercent)/100))\n+\tif gcpercent < 0 {\n+\t\tmemstats.next_gc = ^uint64(0)\n+\t}\n+\twork.startSema = 1\n+\twork.markDoneSema = 1\n+}\n+\n+func readgogc() int32 {\n+\tp := gogetenv(\"GOGC\")\n+\tif p == \"off\" {\n+\t\treturn -1\n+\t}\n+\tif n, ok := atoi32(p); ok {\n+\t\treturn n\n+\t}\n+\treturn 100\n+}\n+\n+// gcenable is called after the bulk of the runtime initialization,\n+// just before we're about to start letting user code run.\n+// It kicks off the background sweeper goroutine and enables GC.\n+func gcenable() {\n+\tc := make(chan int, 1)\n+\tgo bgsweep(c)\n+\t<-c\n+\tmemstats.enablegc = true // now that runtime is initialized, GC is okay\n+}\n+\n+//go:linkname setGCPercent runtime_debug.setGCPercent\n+func setGCPercent(in int32) (out int32) {\n+\tlock(&mheap_.lock)\n+\tout = gcpercent\n+\tif in < 0 {\n+\t\tin = -1\n+\t}\n+\tgcpercent = in\n+\theapminimum = defaultHeapMinimum * uint64(gcpercent) / 100\n+\tif gcController.triggerRatio > float64(gcpercent)/100 {\n+\t\tgcController.triggerRatio = float64(gcpercent) / 100\n+\t}\n+\t// This is either in gcinit or followed by a STW GC, both of\n+\t// which will reset other stats like memstats.gc_trigger and\n+\t// memstats.next_gc to appropriate values.\n+\tunlock(&mheap_.lock)\n+\treturn out\n+}\n+\n+// Garbage collector phase.\n+// Indicates to write barrier and synchronization task to perform.\n+var gcphase uint32\n+\n+// The compiler knows about this variable.\n+// If you change it, you must change the compiler too.\n+var writeBarrier struct {\n+\tenabled bool    // compiler emits a check of this before calling write barrier\n+\tpad     [3]byte // compiler uses 32-bit load for \"enabled\" field\n+\tneeded  bool    // whether we need a write barrier for current GC phase\n+\tcgo     bool    // whether we need a write barrier for a cgo check\n+\talignme uint64  // guarantee alignment so that compiler can use a 32 or 64-bit load\n+}\n+\n+// gcBlackenEnabled is 1 if mutator assists and background mark\n+// workers are allowed to blacken objects. This must only be set when\n+// gcphase == _GCmark.\n+var gcBlackenEnabled uint32\n+\n+// gcBlackenPromptly indicates that optimizations that may\n+// hide work from the global work queue should be disabled.\n+//\n+// If gcBlackenPromptly is true, per-P gcWork caches should\n+// be flushed immediately and new objects should be allocated black.\n+//\n+// There is a tension between allocating objects white and\n+// allocating them black. If white and the objects die before being\n+// marked they can be collected during this GC cycle. On the other\n+// hand allocating them black will reduce _GCmarktermination latency\n+// since more work is done in the mark phase. This tension is resolved\n+// by allocating white until the mark phase is approaching its end and\n+// then allocating black for the remainder of the mark phase.\n+var gcBlackenPromptly bool\n+\n+const (\n+\t_GCoff             = iota // GC not running; sweeping in background, write barrier disabled\n+\t_GCmark                   // GC marking roots and workbufs: allocate black, write barrier ENABLED\n+\t_GCmarktermination        // GC mark termination: allocate black, P's help GC, write barrier ENABLED\n+)\n+\n+//go:nosplit\n+func setGCPhase(x uint32) {\n+\tatomic.Store(&gcphase, x)\n+\twriteBarrier.needed = gcphase == _GCmark || gcphase == _GCmarktermination\n+\twriteBarrier.enabled = writeBarrier.needed || writeBarrier.cgo\n+}\n+\n+// gcMarkWorkerMode represents the mode that a concurrent mark worker\n+// should operate in.\n+//\n+// Concurrent marking happens through four different mechanisms. One\n+// is mutator assists, which happen in response to allocations and are\n+// not scheduled. The other three are variations in the per-P mark\n+// workers and are distinguished by gcMarkWorkerMode.\n+type gcMarkWorkerMode int\n+\n+const (\n+\t// gcMarkWorkerDedicatedMode indicates that the P of a mark\n+\t// worker is dedicated to running that mark worker. The mark\n+\t// worker should run without preemption.\n+\tgcMarkWorkerDedicatedMode gcMarkWorkerMode = iota\n+\n+\t// gcMarkWorkerFractionalMode indicates that a P is currently\n+\t// running the \"fractional\" mark worker. The fractional worker\n+\t// is necessary when GOMAXPROCS*gcGoalUtilization is not an\n+\t// integer. The fractional worker should run until it is\n+\t// preempted and will be scheduled to pick up the fractional\n+\t// part of GOMAXPROCS*gcGoalUtilization.\n+\tgcMarkWorkerFractionalMode\n+\n+\t// gcMarkWorkerIdleMode indicates that a P is running the mark\n+\t// worker because it has nothing else to do. The idle worker\n+\t// should run until it is preempted and account its time\n+\t// against gcController.idleMarkTime.\n+\tgcMarkWorkerIdleMode\n+)\n+\n+// gcMarkWorkerModeStrings are the strings labels of gcMarkWorkerModes\n+// to use in execution traces.\n+var gcMarkWorkerModeStrings = [...]string{\n+\t\"GC (dedicated)\",\n+\t\"GC (fractional)\",\n+\t\"GC (idle)\",\n+}\n+\n+// gcController implements the GC pacing controller that determines\n+// when to trigger concurrent garbage collection and how much marking\n+// work to do in mutator assists and background marking.\n+//\n+// It uses a feedback control algorithm to adjust the memstats.gc_trigger\n+// trigger based on the heap growth and GC CPU utilization each cycle.\n+// This algorithm optimizes for heap growth to match GOGC and for CPU\n+// utilization between assist and background marking to be 25% of\n+// GOMAXPROCS. The high-level design of this algorithm is documented\n+// at https://golang.org/s/go15gcpacing.\n+var gcController = gcControllerState{\n+\t// Initial trigger ratio guess.\n+\ttriggerRatio: 7 / 8.0,\n+}\n+\n+type gcControllerState struct {\n+\t// scanWork is the total scan work performed this cycle. This\n+\t// is updated atomically during the cycle. Updates occur in\n+\t// bounded batches, since it is both written and read\n+\t// throughout the cycle. At the end of the cycle, this is how\n+\t// much of the retained heap is scannable.\n+\t//\n+\t// Currently this is the bytes of heap scanned. For most uses,\n+\t// this is an opaque unit of work, but for estimation the\n+\t// definition is important.\n+\tscanWork int64\n+\n+\t// bgScanCredit is the scan work credit accumulated by the\n+\t// concurrent background scan. This credit is accumulated by\n+\t// the background scan and stolen by mutator assists. This is\n+\t// updated atomically. Updates occur in bounded batches, since\n+\t// it is both written and read throughout the cycle.\n+\tbgScanCredit int64\n+\n+\t// assistTime is the nanoseconds spent in mutator assists\n+\t// during this cycle. This is updated atomically. Updates\n+\t// occur in bounded batches, since it is both written and read\n+\t// throughout the cycle.\n+\tassistTime int64\n+\n+\t// dedicatedMarkTime is the nanoseconds spent in dedicated\n+\t// mark workers during this cycle. This is updated atomically\n+\t// at the end of the concurrent mark phase.\n+\tdedicatedMarkTime int64\n+\n+\t// fractionalMarkTime is the nanoseconds spent in the\n+\t// fractional mark worker during this cycle. This is updated\n+\t// atomically throughout the cycle and will be up-to-date if\n+\t// the fractional mark worker is not currently running.\n+\tfractionalMarkTime int64\n+\n+\t// idleMarkTime is the nanoseconds spent in idle marking\n+\t// during this cycle. This is updated atomically throughout\n+\t// the cycle.\n+\tidleMarkTime int64\n+\n+\t// markStartTime is the absolute start time in nanoseconds\n+\t// that assists and background mark workers started.\n+\tmarkStartTime int64\n+\n+\t// dedicatedMarkWorkersNeeded is the number of dedicated mark\n+\t// workers that need to be started. This is computed at the\n+\t// beginning of each cycle and decremented atomically as\n+\t// dedicated mark workers get started.\n+\tdedicatedMarkWorkersNeeded int64\n+\n+\t// assistWorkPerByte is the ratio of scan work to allocated\n+\t// bytes that should be performed by mutator assists. This is\n+\t// computed at the beginning of each cycle and updated every\n+\t// time heap_scan is updated.\n+\tassistWorkPerByte float64\n+\n+\t// assistBytesPerWork is 1/assistWorkPerByte.\n+\tassistBytesPerWork float64\n+\n+\t// fractionalUtilizationGoal is the fraction of wall clock\n+\t// time that should be spent in the fractional mark worker.\n+\t// For example, if the overall mark utilization goal is 25%\n+\t// and GOMAXPROCS is 6, one P will be a dedicated mark worker\n+\t// and this will be set to 0.5 so that 50% of the time some P\n+\t// is in a fractional mark worker. This is computed at the\n+\t// beginning of each cycle.\n+\tfractionalUtilizationGoal float64\n+\n+\t// triggerRatio is the heap growth ratio at which the garbage\n+\t// collection cycle should start. E.g., if this is 0.6, then\n+\t// GC should start when the live heap has reached 1.6 times\n+\t// the heap size marked by the previous cycle. This should be\n+\t// \u2264 GOGC/100 so the trigger heap size is less than the goal\n+\t// heap size. This is updated at the end of of each cycle.\n+\ttriggerRatio float64\n+\n+\t_ [sys.CacheLineSize]byte\n+\n+\t// fractionalMarkWorkersNeeded is the number of fractional\n+\t// mark workers that need to be started. This is either 0 or\n+\t// 1. This is potentially updated atomically at every\n+\t// scheduling point (hence it gets its own cache line).\n+\tfractionalMarkWorkersNeeded int64\n+\n+\t_ [sys.CacheLineSize]byte\n+}\n+\n+// startCycle resets the GC controller's state and computes estimates\n+// for a new GC cycle. The caller must hold worldsema.\n+func (c *gcControllerState) startCycle() {\n+\tc.scanWork = 0\n+\tc.bgScanCredit = 0\n+\tc.assistTime = 0\n+\tc.dedicatedMarkTime = 0\n+\tc.fractionalMarkTime = 0\n+\tc.idleMarkTime = 0\n+\n+\t// If this is the first GC cycle or we're operating on a very\n+\t// small heap, fake heap_marked so it looks like gc_trigger is\n+\t// the appropriate growth from heap_marked, even though the\n+\t// real heap_marked may not have a meaningful value (on the\n+\t// first cycle) or may be much smaller (resulting in a large\n+\t// error response).\n+\tif memstats.gc_trigger <= heapminimum {\n+\t\tmemstats.heap_marked = uint64(float64(memstats.gc_trigger) / (1 + c.triggerRatio))\n+\t}\n+\n+\t// Re-compute the heap goal for this cycle in case something\n+\t// changed. This is the same calculation we use elsewhere.\n+\tmemstats.next_gc = memstats.heap_marked + memstats.heap_marked*uint64(gcpercent)/100\n+\tif gcpercent < 0 {\n+\t\tmemstats.next_gc = ^uint64(0)\n+\t}\n+\n+\t// Ensure that the heap goal is at least a little larger than\n+\t// the current live heap size. This may not be the case if GC\n+\t// start is delayed or if the allocation that pushed heap_live\n+\t// over gc_trigger is large or if the trigger is really close to\n+\t// GOGC. Assist is proportional to this distance, so enforce a\n+\t// minimum distance, even if it means going over the GOGC goal\n+\t// by a tiny bit.\n+\tif memstats.next_gc < memstats.heap_live+1024*1024 {\n+\t\tmemstats.next_gc = memstats.heap_live + 1024*1024\n+\t}\n+\n+\t// Compute the total mark utilization goal and divide it among\n+\t// dedicated and fractional workers.\n+\ttotalUtilizationGoal := float64(gomaxprocs) * gcGoalUtilization\n+\tc.dedicatedMarkWorkersNeeded = int64(totalUtilizationGoal)\n+\tc.fractionalUtilizationGoal = totalUtilizationGoal - float64(c.dedicatedMarkWorkersNeeded)\n+\tif c.fractionalUtilizationGoal > 0 {\n+\t\tc.fractionalMarkWorkersNeeded = 1\n+\t} else {\n+\t\tc.fractionalMarkWorkersNeeded = 0\n+\t}\n+\n+\t// Clear per-P state\n+\tfor _, p := range &allp {\n+\t\tif p == nil {\n+\t\t\tbreak\n+\t\t}\n+\t\tp.gcAssistTime = 0\n+\t}\n+\n+\t// Compute initial values for controls that are updated\n+\t// throughout the cycle.\n+\tc.revise()\n+\n+\tif debug.gcpacertrace > 0 {\n+\t\tprint(\"pacer: assist ratio=\", c.assistWorkPerByte,\n+\t\t\t\" (scan \", memstats.heap_scan>>20, \" MB in \",\n+\t\t\twork.initialHeapLive>>20, \"->\",\n+\t\t\tmemstats.next_gc>>20, \" MB)\",\n+\t\t\t\" workers=\", c.dedicatedMarkWorkersNeeded,\n+\t\t\t\"+\", c.fractionalMarkWorkersNeeded, \"\\n\")\n+\t}\n+}\n+\n+// revise updates the assist ratio during the GC cycle to account for\n+// improved estimates. This should be called either under STW or\n+// whenever memstats.heap_scan or memstats.heap_live is updated (with\n+// mheap_.lock held).\n+//\n+// It should only be called when gcBlackenEnabled != 0 (because this\n+// is when assists are enabled and the necessary statistics are\n+// available).\n+//\n+// TODO: Consider removing the periodic controller update altogether.\n+// Since we switched to allocating black, in theory we shouldn't have\n+// to change the assist ratio. However, this is still a useful hook\n+// that we've found many uses for when experimenting.\n+func (c *gcControllerState) revise() {\n+\t// Compute the expected scan work remaining.\n+\t//\n+\t// Note that we currently count allocations during GC as both\n+\t// scannable heap (heap_scan) and scan work completed\n+\t// (scanWork), so this difference won't be changed by\n+\t// allocations during GC.\n+\t//\n+\t// This particular estimate is a strict upper bound on the\n+\t// possible remaining scan work for the current heap.\n+\t// You might consider dividing this by 2 (or by\n+\t// (100+GOGC)/100) to counter this over-estimation, but\n+\t// benchmarks show that this has almost no effect on mean\n+\t// mutator utilization, heap size, or assist time and it\n+\t// introduces the danger of under-estimating and letting the\n+\t// mutator outpace the garbage collector.\n+\tscanWorkExpected := int64(memstats.heap_scan) - c.scanWork\n+\tif scanWorkExpected < 1000 {\n+\t\t// We set a somewhat arbitrary lower bound on\n+\t\t// remaining scan work since if we aim a little high,\n+\t\t// we can miss by a little.\n+\t\t//\n+\t\t// We *do* need to enforce that this is at least 1,\n+\t\t// since marking is racy and double-scanning objects\n+\t\t// may legitimately make the expected scan work\n+\t\t// negative.\n+\t\tscanWorkExpected = 1000\n+\t}\n+\n+\t// Compute the heap distance remaining.\n+\theapDistance := int64(memstats.next_gc) - int64(memstats.heap_live)\n+\tif heapDistance <= 0 {\n+\t\t// This shouldn't happen, but if it does, avoid\n+\t\t// dividing by zero or setting the assist negative.\n+\t\theapDistance = 1\n+\t}\n+\n+\t// Compute the mutator assist ratio so by the time the mutator\n+\t// allocates the remaining heap bytes up to next_gc, it will\n+\t// have done (or stolen) the remaining amount of scan work.\n+\tc.assistWorkPerByte = float64(scanWorkExpected) / float64(heapDistance)\n+\tc.assistBytesPerWork = float64(heapDistance) / float64(scanWorkExpected)\n+}\n+\n+// endCycle updates the GC controller state at the end of the\n+// concurrent part of the GC cycle.\n+func (c *gcControllerState) endCycle() {\n+\th_t := c.triggerRatio // For debugging\n+\n+\t// Proportional response gain for the trigger controller. Must\n+\t// be in [0, 1]. Lower values smooth out transient effects but\n+\t// take longer to respond to phase changes. Higher values\n+\t// react to phase changes quickly, but are more affected by\n+\t// transient changes. Values near 1 may be unstable.\n+\tconst triggerGain = 0.5\n+\n+\t// Compute next cycle trigger ratio. First, this computes the\n+\t// \"error\" for this cycle; that is, how far off the trigger\n+\t// was from what it should have been, accounting for both heap\n+\t// growth and GC CPU utilization. We compute the actual heap\n+\t// growth during this cycle and scale that by how far off from\n+\t// the goal CPU utilization we were (to estimate the heap\n+\t// growth if we had the desired CPU utilization). The\n+\t// difference between this estimate and the GOGC-based goal\n+\t// heap growth is the error.\n+\tgoalGrowthRatio := float64(gcpercent) / 100\n+\tactualGrowthRatio := float64(memstats.heap_live)/float64(memstats.heap_marked) - 1\n+\tassistDuration := nanotime() - c.markStartTime\n+\n+\t// Assume background mark hit its utilization goal.\n+\tutilization := gcGoalUtilization\n+\t// Add assist utilization; avoid divide by zero.\n+\tif assistDuration > 0 {\n+\t\tutilization += float64(c.assistTime) / float64(assistDuration*int64(gomaxprocs))\n+\t}\n+\n+\ttriggerError := goalGrowthRatio - c.triggerRatio - utilization/gcGoalUtilization*(actualGrowthRatio-c.triggerRatio)\n+\n+\t// Finally, we adjust the trigger for next time by this error,\n+\t// damped by the proportional gain.\n+\tc.triggerRatio += triggerGain * triggerError\n+\tif c.triggerRatio < 0 {\n+\t\t// This can happen if the mutator is allocating very\n+\t\t// quickly or the GC is scanning very slowly.\n+\t\tc.triggerRatio = 0\n+\t} else if c.triggerRatio > goalGrowthRatio*0.95 {\n+\t\t// Ensure there's always a little margin so that the\n+\t\t// mutator assist ratio isn't infinity.\n+\t\tc.triggerRatio = goalGrowthRatio * 0.95\n+\t}\n+\n+\tif debug.gcpacertrace > 0 {\n+\t\t// Print controller state in terms of the design\n+\t\t// document.\n+\t\tH_m_prev := memstats.heap_marked\n+\t\tH_T := memstats.gc_trigger\n+\t\th_a := actualGrowthRatio\n+\t\tH_a := memstats.heap_live\n+\t\th_g := goalGrowthRatio\n+\t\tH_g := int64(float64(H_m_prev) * (1 + h_g))\n+\t\tu_a := utilization\n+\t\tu_g := gcGoalUtilization\n+\t\tW_a := c.scanWork\n+\t\tprint(\"pacer: H_m_prev=\", H_m_prev,\n+\t\t\t\" h_t=\", h_t, \" H_T=\", H_T,\n+\t\t\t\" h_a=\", h_a, \" H_a=\", H_a,\n+\t\t\t\" h_g=\", h_g, \" H_g=\", H_g,\n+\t\t\t\" u_a=\", u_a, \" u_g=\", u_g,\n+\t\t\t\" W_a=\", W_a,\n+\t\t\t\" goal\u0394=\", goalGrowthRatio-h_t,\n+\t\t\t\" actual\u0394=\", h_a-h_t,\n+\t\t\t\" u_a/u_g=\", u_a/u_g,\n+\t\t\t\"\\n\")\n+\t}\n+}\n+\n+// enlistWorker encourages another dedicated mark worker to start on\n+// another P if there are spare worker slots. It is used by putfull\n+// when more work is made available.\n+//\n+//go:nowritebarrier\n+func (c *gcControllerState) enlistWorker() {\n+\t// If there are idle Ps, wake one so it will run an idle worker.\n+\t// NOTE: This is suspected of causing deadlocks. See golang.org/issue/19112.\n+\t//\n+\t//\tif atomic.Load(&sched.npidle) != 0 && atomic.Load(&sched.nmspinning) == 0 {\n+\t//\t\twakep()\n+\t//\t\treturn\n+\t//\t}\n+\n+\t// There are no idle Ps. If we need more dedicated workers,\n+\t// try to preempt a running P so it will switch to a worker.\n+\tif c.dedicatedMarkWorkersNeeded <= 0 {\n+\t\treturn\n+\t}\n+\t// Pick a random other P to preempt.\n+\tif gomaxprocs <= 1 {\n+\t\treturn\n+\t}\n+\tgp := getg()\n+\tif gp == nil || gp.m == nil || gp.m.p == 0 {\n+\t\treturn\n+\t}\n+\tmyID := gp.m.p.ptr().id\n+\tfor tries := 0; tries < 5; tries++ {\n+\t\tid := int32(fastrand() % uint32(gomaxprocs-1))\n+\t\tif id >= myID {\n+\t\t\tid++\n+\t\t}\n+\t\tp := allp[id]\n+\t\tif p.status != _Prunning {\n+\t\t\tcontinue\n+\t\t}\n+\t\tif preemptone(p) {\n+\t\t\treturn\n+\t\t}\n+\t}\n+}\n+\n+// findRunnableGCWorker returns the background mark worker for _p_ if it\n+// should be run. This must only be called when gcBlackenEnabled != 0.\n+func (c *gcControllerState) findRunnableGCWorker(_p_ *p) *g {\n+\tif gcBlackenEnabled == 0 {\n+\t\tthrow(\"gcControllerState.findRunnable: blackening not enabled\")\n+\t}\n+\tif _p_.gcBgMarkWorker == 0 {\n+\t\t// The mark worker associated with this P is blocked\n+\t\t// performing a mark transition. We can't run it\n+\t\t// because it may be on some other run or wait queue.\n+\t\treturn nil\n+\t}\n+\n+\tif !gcMarkWorkAvailable(_p_) {\n+\t\t// No work to be done right now. This can happen at\n+\t\t// the end of the mark phase when there are still\n+\t\t// assists tapering off. Don't bother running a worker\n+\t\t// now because it'll just return immediately.\n+\t\treturn nil\n+\t}\n+\n+\tdecIfPositive := func(ptr *int64) bool {\n+\t\tif *ptr > 0 {\n+\t\t\tif atomic.Xaddint64(ptr, -1) >= 0 {\n+\t\t\t\treturn true\n+\t\t\t}\n+\t\t\t// We lost a race\n+\t\t\tatomic.Xaddint64(ptr, +1)\n+\t\t}\n+\t\treturn false\n+\t}\n+\n+\tif decIfPositive(&c.dedicatedMarkWorkersNeeded) {\n+\t\t// This P is now dedicated to marking until the end of\n+\t\t// the concurrent mark phase.\n+\t\t_p_.gcMarkWorkerMode = gcMarkWorkerDedicatedMode\n+\t\t// TODO(austin): This P isn't going to run anything\n+\t\t// else for a while, so kick everything out of its run\n+\t\t// queue.\n+\t} else {\n+\t\tif !decIfPositive(&c.fractionalMarkWorkersNeeded) {\n+\t\t\t// No more workers are need right now.\n+\t\t\treturn nil\n+\t\t}\n+\n+\t\t// This P has picked the token for the fractional worker.\n+\t\t// Is the GC currently under or at the utilization goal?\n+\t\t// If so, do more work.\n+\t\t//\n+\t\t// We used to check whether doing one time slice of work\n+\t\t// would remain under the utilization goal, but that has the\n+\t\t// effect of delaying work until the mutator has run for\n+\t\t// enough time slices to pay for the work. During those time\n+\t\t// slices, write barriers are enabled, so the mutator is running slower.\n+\t\t// Now instead we do the work whenever we're under or at the\n+\t\t// utilization work and pay for it by letting the mutator run later.\n+\t\t// This doesn't change the overall utilization averages, but it\n+\t\t// front loads the GC work so that the GC finishes earlier and\n+\t\t// write barriers can be turned off sooner, effectively giving\n+\t\t// the mutator a faster machine.\n+\t\t//\n+\t\t// The old, slower behavior can be restored by setting\n+\t\t//\tgcForcePreemptNS = forcePreemptNS.\n+\t\tconst gcForcePreemptNS = 0\n+\n+\t\t// TODO(austin): We could fast path this and basically\n+\t\t// eliminate contention on c.fractionalMarkWorkersNeeded by\n+\t\t// precomputing the minimum time at which it's worth\n+\t\t// next scheduling the fractional worker. Then Ps\n+\t\t// don't have to fight in the window where we've\n+\t\t// passed that deadline and no one has started the\n+\t\t// worker yet.\n+\t\t//\n+\t\t// TODO(austin): Shorter preemption interval for mark\n+\t\t// worker to improve fairness and give this\n+\t\t// finer-grained control over schedule?\n+\t\tnow := nanotime() - gcController.markStartTime\n+\t\tthen := now + gcForcePreemptNS\n+\t\ttimeUsed := c.fractionalMarkTime + gcForcePreemptNS\n+\t\tif then > 0 && float64(timeUsed)/float64(then) > c.fractionalUtilizationGoal {\n+\t\t\t// Nope, we'd overshoot the utilization goal\n+\t\t\tatomic.Xaddint64(&c.fractionalMarkWorkersNeeded, +1)\n+\t\t\treturn nil\n+\t\t}\n+\t\t_p_.gcMarkWorkerMode = gcMarkWorkerFractionalMode\n+\t}\n+\n+\t// Run the background mark worker\n+\tgp := _p_.gcBgMarkWorker.ptr()\n+\tcasgstatus(gp, _Gwaiting, _Grunnable)\n+\tif trace.enabled {\n+\t\ttraceGoUnpark(gp, 0)\n+\t}\n+\treturn gp\n+}\n+\n+// gcGoalUtilization is the goal CPU utilization for background\n+// marking as a fraction of GOMAXPROCS.\n+const gcGoalUtilization = 0.25\n+\n+// gcCreditSlack is the amount of scan work credit that can can\n+// accumulate locally before updating gcController.scanWork and,\n+// optionally, gcController.bgScanCredit. Lower values give a more\n+// accurate assist ratio and make it more likely that assists will\n+// successfully steal background credit. Higher values reduce memory\n+// contention.\n+const gcCreditSlack = 2000\n+\n+// gcAssistTimeSlack is the nanoseconds of mutator assist time that\n+// can accumulate on a P before updating gcController.assistTime.\n+const gcAssistTimeSlack = 5000\n+\n+// gcOverAssistWork determines how many extra units of scan work a GC\n+// assist does when an assist happens. This amortizes the cost of an\n+// assist by pre-paying for this many bytes of future allocations.\n+const gcOverAssistWork = 64 << 10\n+\n+var work struct {\n+\tfull  uint64                   // lock-free list of full blocks workbuf\n+\tempty uint64                   // lock-free list of empty blocks workbuf\n+\tpad0  [sys.CacheLineSize]uint8 // prevents false-sharing between full/empty and nproc/nwait\n+\n+\t// bytesMarked is the number of bytes marked this cycle. This\n+\t// includes bytes blackened in scanned objects, noscan objects\n+\t// that go straight to black, and permagrey objects scanned by\n+\t// markroot during the concurrent scan phase. This is updated\n+\t// atomically during the cycle. Updates may be batched\n+\t// arbitrarily, since the value is only read at the end of the\n+\t// cycle.\n+\t//\n+\t// Because of benign races during marking, this number may not\n+\t// be the exact number of marked bytes, but it should be very\n+\t// close.\n+\t//\n+\t// Put this field here because it needs 64-bit atomic access\n+\t// (and thus 8-byte alignment even on 32-bit architectures).\n+\tbytesMarked uint64\n+\n+\tmarkrootNext uint32 // next markroot job\n+\tmarkrootJobs uint32 // number of markroot jobs\n+\n+\tnproc   uint32\n+\ttstart  int64\n+\tnwait   uint32\n+\tndone   uint32\n+\talldone note\n+\n+\t// helperDrainBlock indicates that GC mark termination helpers\n+\t// should pass gcDrainBlock to gcDrain to block in the\n+\t// getfull() barrier. Otherwise, they should pass gcDrainNoBlock.\n+\t//\n+\t// TODO: This is a temporary fallback to support\n+\t// debug.gcrescanstacks > 0 and to work around some known\n+\t// races. Remove this when we remove the debug option and fix\n+\t// the races.\n+\thelperDrainBlock bool\n+\n+\t// Number of roots of various root types. Set by gcMarkRootPrepare.\n+\tnFlushCacheRoots                                  int\n+\tnDataRoots, nSpanRoots, nStackRoots, nRescanRoots int\n+\n+\t// markrootDone indicates that roots have been marked at least\n+\t// once during the current GC cycle. This is checked by root\n+\t// marking operations that have to happen only during the\n+\t// first root marking pass, whether that's during the\n+\t// concurrent mark phase in current GC or mark termination in\n+\t// STW GC.\n+\tmarkrootDone bool\n+\n+\t// Each type of GC state transition is protected by a lock.\n+\t// Since multiple threads can simultaneously detect the state\n+\t// transition condition, any thread that detects a transition\n+\t// condition must acquire the appropriate transition lock,\n+\t// re-check the transition condition and return if it no\n+\t// longer holds or perform the transition if it does.\n+\t// Likewise, any transition must invalidate the transition\n+\t// condition before releasing the lock. This ensures that each\n+\t// transition is performed by exactly one thread and threads\n+\t// that need the transition to happen block until it has\n+\t// happened.\n+\t//\n+\t// startSema protects the transition from \"off\" to mark or\n+\t// mark termination.\n+\tstartSema uint32\n+\t// markDoneSema protects transitions from mark 1 to mark 2 and\n+\t// from mark 2 to mark termination.\n+\tmarkDoneSema uint32\n+\n+\tbgMarkReady note   // signal background mark worker has started\n+\tbgMarkDone  uint32 // cas to 1 when at a background mark completion point\n+\t// Background mark completion signaling\n+\n+\t// mode is the concurrency mode of the current GC cycle.\n+\tmode gcMode\n+\n+\t// totaltime is the CPU nanoseconds spent in GC since the\n+\t// program started if debug.gctrace > 0.\n+\ttotaltime int64\n+\n+\t// initialHeapLive is the value of memstats.heap_live at the\n+\t// beginning of this GC cycle.\n+\tinitialHeapLive uint64\n+\n+\t// assistQueue is a queue of assists that are blocked because\n+\t// there was neither enough credit to steal or enough work to\n+\t// do.\n+\tassistQueue struct {\n+\t\tlock       mutex\n+\t\thead, tail guintptr\n+\t}\n+\n+\t// rescan is a list of G's that need to be rescanned during\n+\t// mark termination. A G adds itself to this list when it\n+\t// first invalidates its stack scan.\n+\trescan struct {\n+\t\tlock mutex\n+\t\tlist []guintptr\n+\t}\n+\n+\t// Timing/utilization stats for this cycle.\n+\tstwprocs, maxprocs                 int32\n+\ttSweepTerm, tMark, tMarkTerm, tEnd int64 // nanotime() of phase start\n+\n+\tpauseNS    int64 // total STW time this cycle\n+\tpauseStart int64 // nanotime() of last STW\n+\n+\t// debug.gctrace heap sizes for this cycle.\n+\theap0, heap1, heap2, heapGoal uint64\n+}\n+\n+// GC runs a garbage collection and blocks the caller until the\n+// garbage collection is complete. It may also block the entire\n+// program.\n+func GC() {\n+\tgcStart(gcForceBlockMode, false)\n+}\n+\n+// gcMode indicates how concurrent a GC cycle should be.\n+type gcMode int\n+\n+const (\n+\tgcBackgroundMode gcMode = iota // concurrent GC and sweep\n+\tgcForceMode                    // stop-the-world GC now, concurrent sweep\n+\tgcForceBlockMode               // stop-the-world GC now and STW sweep (forced by user)\n+)\n+\n+// gcShouldStart returns true if the exit condition for the _GCoff\n+// phase has been met. The exit condition should be tested when\n+// allocating.\n+//\n+// If forceTrigger is true, it ignores the current heap size, but\n+// checks all other conditions. In general this should be false.\n+func gcShouldStart(forceTrigger bool) bool {\n+\treturn gcphase == _GCoff && (forceTrigger || memstats.heap_live >= memstats.gc_trigger) && memstats.enablegc && panicking == 0 && gcpercent >= 0\n+}\n+\n+// gcStart transitions the GC from _GCoff to _GCmark (if mode ==\n+// gcBackgroundMode) or _GCmarktermination (if mode !=\n+// gcBackgroundMode) by performing sweep termination and GC\n+// initialization.\n+//\n+// This may return without performing this transition in some cases,\n+// such as when called on a system stack or with locks held.\n+func gcStart(mode gcMode, forceTrigger bool) {\n+\t// Since this is called from malloc and malloc is called in\n+\t// the guts of a number of libraries that might be holding\n+\t// locks, don't attempt to start GC in non-preemptible or\n+\t// potentially unstable situations.\n+\tmp := acquirem()\n+\tif gp := getg(); gp == mp.g0 || mp.locks > 1 || mp.preemptoff != \"\" {\n+\t\treleasem(mp)\n+\t\treturn\n+\t}\n+\treleasem(mp)\n+\tmp = nil\n+\n+\t// Pick up the remaining unswept/not being swept spans concurrently\n+\t//\n+\t// This shouldn't happen if we're being invoked in background\n+\t// mode since proportional sweep should have just finished\n+\t// sweeping everything, but rounding errors, etc, may leave a\n+\t// few spans unswept. In forced mode, this is necessary since\n+\t// GC can be forced at any point in the sweeping cycle.\n+\t//\n+\t// We check the transition condition continuously here in case\n+\t// this G gets delayed in to the next GC cycle.\n+\tfor (mode != gcBackgroundMode || gcShouldStart(forceTrigger)) && gosweepone() != ^uintptr(0) {\n+\t\tsweep.nbgsweep++\n+\t}\n+\n+\t// Perform GC initialization and the sweep termination\n+\t// transition.\n+\t//\n+\t// If this is a forced GC, don't acquire the transition lock\n+\t// or re-check the transition condition because we\n+\t// specifically *don't* want to share the transition with\n+\t// another thread.\n+\tuseStartSema := mode == gcBackgroundMode\n+\tif useStartSema {\n+\t\tsemacquire(&work.startSema, 0)\n+\t\t// Re-check transition condition under transition lock.\n+\t\tif !gcShouldStart(forceTrigger) {\n+\t\t\tsemrelease(&work.startSema)\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\t// For stats, check if this GC was forced by the user.\n+\tforced := mode != gcBackgroundMode\n+\n+\t// In gcstoptheworld debug mode, upgrade the mode accordingly.\n+\t// We do this after re-checking the transition condition so\n+\t// that multiple goroutines that detect the heap trigger don't\n+\t// start multiple STW GCs.\n+\tif mode == gcBackgroundMode {\n+\t\tif debug.gcstoptheworld == 1 {\n+\t\t\tmode = gcForceMode\n+\t\t} else if debug.gcstoptheworld == 2 {\n+\t\t\tmode = gcForceBlockMode\n+\t\t}\n+\t}\n+\n+\t// Ok, we're doing it!  Stop everybody else\n+\tsemacquire(&worldsema, 0)\n+\n+\tif trace.enabled {\n+\t\ttraceGCStart()\n+\t}\n+\n+\tif mode == gcBackgroundMode {\n+\t\tgcBgMarkStartWorkers()\n+\t}\n+\n+\tgcResetMarkState()\n+\n+\tnow := nanotime()\n+\twork.stwprocs, work.maxprocs = gcprocs(), gomaxprocs\n+\twork.tSweepTerm = now\n+\twork.heap0 = memstats.heap_live\n+\twork.pauseNS = 0\n+\twork.mode = mode\n+\n+\twork.pauseStart = now\n+\tsystemstack(stopTheWorldWithSema)\n+\t// Finish sweep before we start concurrent scan.\n+\tsystemstack(func() {\n+\t\tfinishsweep_m()\n+\t})\n+\t// clearpools before we start the GC. If we wait they memory will not be\n+\t// reclaimed until the next GC cycle.\n+\tclearpools()\n+\n+\tif mode == gcBackgroundMode { // Do as much work concurrently as possible\n+\t\tgcController.startCycle()\n+\t\twork.heapGoal = memstats.next_gc\n+\n+\t\t// Enter concurrent mark phase and enable\n+\t\t// write barriers.\n+\t\t//\n+\t\t// Because the world is stopped, all Ps will\n+\t\t// observe that write barriers are enabled by\n+\t\t// the time we start the world and begin\n+\t\t// scanning.\n+\t\t//\n+\t\t// It's necessary to enable write barriers\n+\t\t// during the scan phase for several reasons:\n+\t\t//\n+\t\t// They must be enabled for writes to higher\n+\t\t// stack frames before we scan stacks and\n+\t\t// install stack barriers because this is how\n+\t\t// we track writes to inactive stack frames.\n+\t\t// (Alternatively, we could not install stack\n+\t\t// barriers over frame boundaries with\n+\t\t// up-pointers).\n+\t\t//\n+\t\t// They must be enabled before assists are\n+\t\t// enabled because they must be enabled before\n+\t\t// any non-leaf heap objects are marked. Since\n+\t\t// allocations are blocked until assists can\n+\t\t// happen, we want enable assists as early as\n+\t\t// possible.\n+\t\tsetGCPhase(_GCmark)\n+\n+\t\tgcBgMarkPrepare() // Must happen before assist enable.\n+\t\tgcMarkRootPrepare()\n+\n+\t\t// Mark all active tinyalloc blocks. Since we're\n+\t\t// allocating from these, they need to be black like\n+\t\t// other allocations. The alternative is to blacken\n+\t\t// the tiny block on every allocation from it, which\n+\t\t// would slow down the tiny allocator.\n+\t\tgcMarkTinyAllocs()\n+\n+\t\t// At this point all Ps have enabled the write\n+\t\t// barrier, thus maintaining the no white to\n+\t\t// black invariant. Enable mutator assists to\n+\t\t// put back-pressure on fast allocating\n+\t\t// mutators.\n+\t\tatomic.Store(&gcBlackenEnabled, 1)\n+\n+\t\t// Assists and workers can start the moment we start\n+\t\t// the world.\n+\t\tgcController.markStartTime = now\n+\n+\t\t// Concurrent mark.\n+\t\tsystemstack(startTheWorldWithSema)\n+\t\tnow = nanotime()\n+\t\twork.pauseNS += now - work.pauseStart\n+\t\twork.tMark = now\n+\t} else {\n+\t\tt := nanotime()\n+\t\twork.tMark, work.tMarkTerm = t, t\n+\t\twork.heapGoal = work.heap0\n+\n+\t\tif forced {\n+\t\t\tmemstats.numforcedgc++\n+\t\t}\n+\n+\t\t// Perform mark termination. This will restart the world.\n+\t\tgcMarkTermination()\n+\t}\n+\n+\tif useStartSema {\n+\t\tsemrelease(&work.startSema)\n+\t}\n+}\n+\n+// gcMarkDone transitions the GC from mark 1 to mark 2 and from mark 2\n+// to mark termination.\n+//\n+// This should be called when all mark work has been drained. In mark\n+// 1, this includes all root marking jobs, global work buffers, and\n+// active work buffers in assists and background workers; however,\n+// work may still be cached in per-P work buffers. In mark 2, per-P\n+// caches are disabled.\n+//\n+// The calling context must be preemptible.\n+//\n+// Note that it is explicitly okay to have write barriers in this\n+// function because completion of concurrent mark is best-effort\n+// anyway. Any work created by write barriers here will be cleaned up\n+// by mark termination.\n+func gcMarkDone() {\n+top:\n+\tsemacquire(&work.markDoneSema, 0)\n+\n+\t// Re-check transition condition under transition lock.\n+\tif !(gcphase == _GCmark && work.nwait == work.nproc && !gcMarkWorkAvailable(nil)) {\n+\t\tsemrelease(&work.markDoneSema)\n+\t\treturn\n+\t}\n+\n+\t// Disallow starting new workers so that any remaining workers\n+\t// in the current mark phase will drain out.\n+\t//\n+\t// TODO(austin): Should dedicated workers keep an eye on this\n+\t// and exit gcDrain promptly?\n+\tatomic.Xaddint64(&gcController.dedicatedMarkWorkersNeeded, -0xffffffff)\n+\tatomic.Xaddint64(&gcController.fractionalMarkWorkersNeeded, -0xffffffff)\n+\n+\tif !gcBlackenPromptly {\n+\t\t// Transition from mark 1 to mark 2.\n+\t\t//\n+\t\t// The global work list is empty, but there can still be work\n+\t\t// sitting in the per-P work caches.\n+\t\t// Flush and disable work caches.\n+\n+\t\t// Disallow caching workbufs and indicate that we're in mark 2.\n+\t\tgcBlackenPromptly = true\n+\n+\t\t// Prevent completion of mark 2 until we've flushed\n+\t\t// cached workbufs.\n+\t\tatomic.Xadd(&work.nwait, -1)\n+\n+\t\t// GC is set up for mark 2. Let Gs blocked on the\n+\t\t// transition lock go while we flush caches.\n+\t\tsemrelease(&work.markDoneSema)\n+\n+\t\tsystemstack(func() {\n+\t\t\t// Flush all currently cached workbufs and\n+\t\t\t// ensure all Ps see gcBlackenPromptly. This\n+\t\t\t// also blocks until any remaining mark 1\n+\t\t\t// workers have exited their loop so we can\n+\t\t\t// start new mark 2 workers.\n+\t\t\tforEachP(func(_p_ *p) {\n+\t\t\t\t_p_.gcw.dispose()\n+\t\t\t})\n+\t\t})\n+\n+\t\t// Check that roots are marked. We should be able to\n+\t\t// do this before the forEachP, but based on issue\n+\t\t// #16083 there may be a (harmless) race where we can\n+\t\t// enter mark 2 while some workers are still scanning\n+\t\t// stacks. The forEachP ensures these scans are done.\n+\t\t//\n+\t\t// TODO(austin): Figure out the race and fix this\n+\t\t// properly.\n+\t\tgcMarkRootCheck()\n+\n+\t\t// Now we can start up mark 2 workers.\n+\t\tatomic.Xaddint64(&gcController.dedicatedMarkWorkersNeeded, 0xffffffff)\n+\t\tatomic.Xaddint64(&gcController.fractionalMarkWorkersNeeded, 0xffffffff)\n+\n+\t\tincnwait := atomic.Xadd(&work.nwait, +1)\n+\t\tif incnwait == work.nproc && !gcMarkWorkAvailable(nil) {\n+\t\t\t// This loop will make progress because\n+\t\t\t// gcBlackenPromptly is now true, so it won't\n+\t\t\t// take this same \"if\" branch.\n+\t\t\tgoto top\n+\t\t}\n+\t} else {\n+\t\t// Transition to mark termination.\n+\t\tnow := nanotime()\n+\t\twork.tMarkTerm = now\n+\t\twork.pauseStart = now\n+\t\tgetg().m.preemptoff = \"gcing\"\n+\t\tsystemstack(stopTheWorldWithSema)\n+\t\t// The gcphase is _GCmark, it will transition to _GCmarktermination\n+\t\t// below. The important thing is that the wb remains active until\n+\t\t// all marking is complete. This includes writes made by the GC.\n+\n+\t\t// Record that one root marking pass has completed.\n+\t\twork.markrootDone = true\n+\n+\t\t// Disable assists and background workers. We must do\n+\t\t// this before waking blocked assists.\n+\t\tatomic.Store(&gcBlackenEnabled, 0)\n+\n+\t\t// Wake all blocked assists. These will run when we\n+\t\t// start the world again.\n+\t\tgcWakeAllAssists()\n+\n+\t\t// Likewise, release the transition lock. Blocked\n+\t\t// workers and assists will run when we start the\n+\t\t// world again.\n+\t\tsemrelease(&work.markDoneSema)\n+\n+\t\t// endCycle depends on all gcWork cache stats being\n+\t\t// flushed. This is ensured by mark 2.\n+\t\tgcController.endCycle()\n+\n+\t\t// Perform mark termination. This will restart the world.\n+\t\tgcMarkTermination()\n+\t}\n+}\n+\n+func gcMarkTermination() {\n+\t// World is stopped.\n+\t// Start marktermination which includes enabling the write barrier.\n+\tatomic.Store(&gcBlackenEnabled, 0)\n+\tgcBlackenPromptly = false\n+\tsetGCPhase(_GCmarktermination)\n+\n+\twork.heap1 = memstats.heap_live\n+\tstartTime := nanotime()\n+\n+\tmp := acquirem()\n+\tmp.preemptoff = \"gcing\"\n+\t_g_ := getg()\n+\t_g_.m.traceback = 2\n+\tgp := _g_.m.curg\n+\tcasgstatus(gp, _Grunning, _Gwaiting)\n+\tgp.waitreason = \"garbage collection\"\n+\n+\t// Run gc on the g0 stack. We do this so that the g stack\n+\t// we're currently running on will no longer change. Cuts\n+\t// the root set down a bit (g0 stacks are not scanned, and\n+\t// we don't need to scan gc's internal state).  We also\n+\t// need to switch to g0 so we can shrink the stack.\n+\tsystemstack(func() {\n+\t\tgcMark(startTime)\n+\t\t// Must return immediately.\n+\t\t// The outer function's stack may have moved\n+\t\t// during gcMark (it shrinks stacks, including the\n+\t\t// outer function's stack), so we must not refer\n+\t\t// to any of its variables. Return back to the\n+\t\t// non-system stack to pick up the new addresses\n+\t\t// before continuing.\n+\t})\n+\n+\tsystemstack(func() {\n+\t\twork.heap2 = work.bytesMarked\n+\t\tif debug.gccheckmark > 0 {\n+\t\t\t// Run a full stop-the-world mark using checkmark bits,\n+\t\t\t// to check that we didn't forget to mark anything during\n+\t\t\t// the concurrent mark process.\n+\t\t\tgcResetMarkState()\n+\t\t\tinitCheckmarks()\n+\t\t\tgcMark(startTime)\n+\t\t\tclearCheckmarks()\n+\t\t}\n+\n+\t\t// marking is complete so we can turn the write barrier off\n+\t\tsetGCPhase(_GCoff)\n+\t\tgcSweep(work.mode)\n+\n+\t\tif debug.gctrace > 1 {\n+\t\t\tstartTime = nanotime()\n+\t\t\t// The g stacks have been scanned so\n+\t\t\t// they have gcscanvalid==true and gcworkdone==true.\n+\t\t\t// Reset these so that all stacks will be rescanned.\n+\t\t\tgcResetMarkState()\n+\t\t\tfinishsweep_m()\n+\n+\t\t\t// Still in STW but gcphase is _GCoff, reset to _GCmarktermination\n+\t\t\t// At this point all objects will be found during the gcMark which\n+\t\t\t// does a complete STW mark and object scan.\n+\t\t\tsetGCPhase(_GCmarktermination)\n+\t\t\tgcMark(startTime)\n+\t\t\tsetGCPhase(_GCoff) // marking is done, turn off wb.\n+\t\t\tgcSweep(work.mode)\n+\t\t}\n+\t})\n+\n+\t_g_.m.traceback = 0\n+\tcasgstatus(gp, _Gwaiting, _Grunning)\n+\n+\tif trace.enabled {\n+\t\ttraceGCDone()\n+\t}\n+\n+\t// all done\n+\tmp.preemptoff = \"\"\n+\n+\tif gcphase != _GCoff {\n+\t\tthrow(\"gc done but gcphase != _GCoff\")\n+\t}\n+\n+\t// Update timing memstats\n+\tnow, unixNow := nanotime(), unixnanotime()\n+\twork.pauseNS += now - work.pauseStart\n+\twork.tEnd = now\n+\tatomic.Store64(&memstats.last_gc, uint64(unixNow)) // must be Unix time to make sense to user\n+\tmemstats.pause_ns[memstats.numgc%uint32(len(memstats.pause_ns))] = uint64(work.pauseNS)\n+\tmemstats.pause_end[memstats.numgc%uint32(len(memstats.pause_end))] = uint64(unixNow)\n+\tmemstats.pause_total_ns += uint64(work.pauseNS)\n+\n+\t// Update work.totaltime.\n+\tsweepTermCpu := int64(work.stwprocs) * (work.tMark - work.tSweepTerm)\n+\t// We report idle marking time below, but omit it from the\n+\t// overall utilization here since it's \"free\".\n+\tmarkCpu := gcController.assistTime + gcController.dedicatedMarkTime + gcController.fractionalMarkTime\n+\tmarkTermCpu := int64(work.stwprocs) * (work.tEnd - work.tMarkTerm)\n+\tcycleCpu := sweepTermCpu + markCpu + markTermCpu\n+\twork.totaltime += cycleCpu\n+\n+\t// Compute overall GC CPU utilization.\n+\ttotalCpu := sched.totaltime + (now-sched.procresizetime)*int64(gomaxprocs)\n+\tmemstats.gc_cpu_fraction = float64(work.totaltime) / float64(totalCpu)\n+\n+\tmemstats.numgc++\n+\n+\t// Reset sweep state.\n+\tsweep.nbgsweep = 0\n+\tsweep.npausesweep = 0\n+\n+\tsystemstack(startTheWorldWithSema)\n+\n+\t// Update heap profile stats if gcSweep didn't do it. This is\n+\t// relatively expensive, so we don't want to do it while the\n+\t// world is stopped, but it needs to happen ASAP after\n+\t// starting the world to prevent too many allocations from the\n+\t// next cycle leaking in. It must happen before releasing\n+\t// worldsema since there are applications that do a\n+\t// runtime.GC() to update the heap profile and then\n+\t// immediately collect the profile.\n+\tif _ConcurrentSweep && work.mode != gcForceBlockMode {\n+\t\tmProf_GC()\n+\t}\n+\n+\t// Print gctrace before dropping worldsema. As soon as we drop\n+\t// worldsema another cycle could start and smash the stats\n+\t// we're trying to print.\n+\tif debug.gctrace > 0 {\n+\t\tutil := int(memstats.gc_cpu_fraction * 100)\n+\n+\t\tvar sbuf [24]byte\n+\t\tprintlock()\n+\t\tprint(\"gc \", memstats.numgc,\n+\t\t\t\" @\", string(itoaDiv(sbuf[:], uint64(work.tSweepTerm-runtimeInitTime)/1e6, 3)), \"s \",\n+\t\t\tutil, \"%: \")\n+\t\tprev := work.tSweepTerm\n+\t\tfor i, ns := range []int64{work.tMark, work.tMarkTerm, work.tEnd} {\n+\t\t\tif i != 0 {\n+\t\t\t\tprint(\"+\")\n+\t\t\t}\n+\t\t\tprint(string(fmtNSAsMS(sbuf[:], uint64(ns-prev))))\n+\t\t\tprev = ns\n+\t\t}\n+\t\tprint(\" ms clock, \")\n+\t\tfor i, ns := range []int64{sweepTermCpu, gcController.assistTime, gcController.dedicatedMarkTime + gcController.fractionalMarkTime, gcController.idleMarkTime, markTermCpu} {\n+\t\t\tif i == 2 || i == 3 {\n+\t\t\t\t// Separate mark time components with /.\n+\t\t\t\tprint(\"/\")\n+\t\t\t} else if i != 0 {\n+\t\t\t\tprint(\"+\")\n+\t\t\t}\n+\t\t\tprint(string(fmtNSAsMS(sbuf[:], uint64(ns))))\n+\t\t}\n+\t\tprint(\" ms cpu, \",\n+\t\t\twork.heap0>>20, \"->\", work.heap1>>20, \"->\", work.heap2>>20, \" MB, \",\n+\t\t\twork.heapGoal>>20, \" MB goal, \",\n+\t\t\twork.maxprocs, \" P\")\n+\t\tif work.mode != gcBackgroundMode {\n+\t\t\tprint(\" (forced)\")\n+\t\t}\n+\t\tprint(\"\\n\")\n+\t\tprintunlock()\n+\t}\n+\n+\tsemrelease(&worldsema)\n+\t// Careful: another GC cycle may start now.\n+\n+\treleasem(mp)\n+\tmp = nil\n+\n+\t// now that gc is done, kick off finalizer thread if needed\n+\tif !concurrentSweep {\n+\t\t// give the queued finalizers, if any, a chance to run\n+\t\tGosched()\n+\t}\n+}\n+\n+// gcBgMarkStartWorkers prepares background mark worker goroutines.\n+// These goroutines will not run until the mark phase, but they must\n+// be started while the work is not stopped and from a regular G\n+// stack. The caller must hold worldsema.\n+func gcBgMarkStartWorkers() {\n+\t// Background marking is performed by per-P G's. Ensure that\n+\t// each P has a background GC G.\n+\tfor _, p := range &allp {\n+\t\tif p == nil || p.status == _Pdead {\n+\t\t\tbreak\n+\t\t}\n+\t\tif p.gcBgMarkWorker == 0 {\n+\t\t\tgo gcBgMarkWorker(p)\n+\t\t\tnotetsleepg(&work.bgMarkReady, -1)\n+\t\t\tnoteclear(&work.bgMarkReady)\n+\t\t}\n+\t}\n+}\n+\n+// gcBgMarkPrepare sets up state for background marking.\n+// Mutator assists must not yet be enabled.\n+func gcBgMarkPrepare() {\n+\t// Background marking will stop when the work queues are empty\n+\t// and there are no more workers (note that, since this is\n+\t// concurrent, this may be a transient state, but mark\n+\t// termination will clean it up). Between background workers\n+\t// and assists, we don't really know how many workers there\n+\t// will be, so we pretend to have an arbitrarily large number\n+\t// of workers, almost all of which are \"waiting\". While a\n+\t// worker is working it decrements nwait. If nproc == nwait,\n+\t// there are no workers.\n+\twork.nproc = ^uint32(0)\n+\twork.nwait = ^uint32(0)\n+}\n+\n+func gcBgMarkWorker(_p_ *p) {\n+\tgp := getg()\n+\n+\ttype parkInfo struct {\n+\t\tm      muintptr // Release this m on park.\n+\t\tattach puintptr // If non-nil, attach to this p on park.\n+\t}\n+\t// We pass park to a gopark unlock function, so it can't be on\n+\t// the stack (see gopark). Prevent deadlock from recursively\n+\t// starting GC by disabling preemption.\n+\tgp.m.preemptoff = \"GC worker init\"\n+\tpark := new(parkInfo)\n+\tgp.m.preemptoff = \"\"\n+\n+\tpark.m.set(acquirem())\n+\tpark.attach.set(_p_)\n+\t// Inform gcBgMarkStartWorkers that this worker is ready.\n+\t// After this point, the background mark worker is scheduled\n+\t// cooperatively by gcController.findRunnable. Hence, it must\n+\t// never be preempted, as this would put it into _Grunnable\n+\t// and put it on a run queue. Instead, when the preempt flag\n+\t// is set, this puts itself into _Gwaiting to be woken up by\n+\t// gcController.findRunnable at the appropriate time.\n+\tnotewakeup(&work.bgMarkReady)\n+\n+\tfor {\n+\t\t// Go to sleep until woken by gcController.findRunnable.\n+\t\t// We can't releasem yet since even the call to gopark\n+\t\t// may be preempted.\n+\t\tgopark(func(g *g, parkp unsafe.Pointer) bool {\n+\t\t\tpark := (*parkInfo)(parkp)\n+\n+\t\t\t// The worker G is no longer running, so it's\n+\t\t\t// now safe to allow preemption.\n+\t\t\treleasem(park.m.ptr())\n+\n+\t\t\t// If the worker isn't attached to its P,\n+\t\t\t// attach now. During initialization and after\n+\t\t\t// a phase change, the worker may have been\n+\t\t\t// running on a different P. As soon as we\n+\t\t\t// attach, the owner P may schedule the\n+\t\t\t// worker, so this must be done after the G is\n+\t\t\t// stopped.\n+\t\t\tif park.attach != 0 {\n+\t\t\t\tp := park.attach.ptr()\n+\t\t\t\tpark.attach.set(nil)\n+\t\t\t\t// cas the worker because we may be\n+\t\t\t\t// racing with a new worker starting\n+\t\t\t\t// on this P.\n+\t\t\t\tif !p.gcBgMarkWorker.cas(0, guintptr(unsafe.Pointer(g))) {\n+\t\t\t\t\t// The P got a new worker.\n+\t\t\t\t\t// Exit this worker.\n+\t\t\t\t\treturn false\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\treturn true\n+\t\t}, unsafe.Pointer(park), \"GC worker (idle)\", traceEvGoBlock, 0)\n+\n+\t\t// Loop until the P dies and disassociates this\n+\t\t// worker (the P may later be reused, in which case\n+\t\t// it will get a new worker) or we failed to associate.\n+\t\tif _p_.gcBgMarkWorker.ptr() != gp {\n+\t\t\tbreak\n+\t\t}\n+\n+\t\t// Disable preemption so we can use the gcw. If the\n+\t\t// scheduler wants to preempt us, we'll stop draining,\n+\t\t// dispose the gcw, and then preempt.\n+\t\tpark.m.set(acquirem())\n+\n+\t\tif gcBlackenEnabled == 0 {\n+\t\t\tthrow(\"gcBgMarkWorker: blackening not enabled\")\n+\t\t}\n+\n+\t\tstartTime := nanotime()\n+\n+\t\tdecnwait := atomic.Xadd(&work.nwait, -1)\n+\t\tif decnwait == work.nproc {\n+\t\t\tprintln(\"runtime: work.nwait=\", decnwait, \"work.nproc=\", work.nproc)\n+\t\t\tthrow(\"work.nwait was > work.nproc\")\n+\t\t}\n+\n+\t\tsystemstack(func() {\n+\t\t\t// Mark our goroutine preemptible so its stack\n+\t\t\t// can be scanned. This lets two mark workers\n+\t\t\t// scan each other (otherwise, they would\n+\t\t\t// deadlock). We must not modify anything on\n+\t\t\t// the G stack. However, stack shrinking is\n+\t\t\t// disabled for mark workers, so it is safe to\n+\t\t\t// read from the G stack.\n+\t\t\tcasgstatus(gp, _Grunning, _Gwaiting)\n+\t\t\tswitch _p_.gcMarkWorkerMode {\n+\t\t\tdefault:\n+\t\t\t\tthrow(\"gcBgMarkWorker: unexpected gcMarkWorkerMode\")\n+\t\t\tcase gcMarkWorkerDedicatedMode:\n+\t\t\t\tgcDrain(&_p_.gcw, gcDrainNoBlock|gcDrainFlushBgCredit)\n+\t\t\tcase gcMarkWorkerFractionalMode:\n+\t\t\t\tgcDrain(&_p_.gcw, gcDrainUntilPreempt|gcDrainFlushBgCredit)\n+\t\t\tcase gcMarkWorkerIdleMode:\n+\t\t\t\tgcDrain(&_p_.gcw, gcDrainIdle|gcDrainUntilPreempt|gcDrainFlushBgCredit)\n+\t\t\t}\n+\t\t\tcasgstatus(gp, _Gwaiting, _Grunning)\n+\t\t})\n+\n+\t\t// If we are nearing the end of mark, dispose\n+\t\t// of the cache promptly. We must do this\n+\t\t// before signaling that we're no longer\n+\t\t// working so that other workers can't observe\n+\t\t// no workers and no work while we have this\n+\t\t// cached, and before we compute done.\n+\t\tif gcBlackenPromptly {\n+\t\t\t_p_.gcw.dispose()\n+\t\t}\n+\n+\t\t// Account for time.\n+\t\tduration := nanotime() - startTime\n+\t\tswitch _p_.gcMarkWorkerMode {\n+\t\tcase gcMarkWorkerDedicatedMode:\n+\t\t\tatomic.Xaddint64(&gcController.dedicatedMarkTime, duration)\n+\t\t\tatomic.Xaddint64(&gcController.dedicatedMarkWorkersNeeded, 1)\n+\t\tcase gcMarkWorkerFractionalMode:\n+\t\t\tatomic.Xaddint64(&gcController.fractionalMarkTime, duration)\n+\t\t\tatomic.Xaddint64(&gcController.fractionalMarkWorkersNeeded, 1)\n+\t\tcase gcMarkWorkerIdleMode:\n+\t\t\tatomic.Xaddint64(&gcController.idleMarkTime, duration)\n+\t\t}\n+\n+\t\t// Was this the last worker and did we run out\n+\t\t// of work?\n+\t\tincnwait := atomic.Xadd(&work.nwait, +1)\n+\t\tif incnwait > work.nproc {\n+\t\t\tprintln(\"runtime: p.gcMarkWorkerMode=\", _p_.gcMarkWorkerMode,\n+\t\t\t\t\"work.nwait=\", incnwait, \"work.nproc=\", work.nproc)\n+\t\t\tthrow(\"work.nwait > work.nproc\")\n+\t\t}\n+\n+\t\t// If this worker reached a background mark completion\n+\t\t// point, signal the main GC goroutine.\n+\t\tif incnwait == work.nproc && !gcMarkWorkAvailable(nil) {\n+\t\t\t// Make this G preemptible and disassociate it\n+\t\t\t// as the worker for this P so\n+\t\t\t// findRunnableGCWorker doesn't try to\n+\t\t\t// schedule it.\n+\t\t\t_p_.gcBgMarkWorker.set(nil)\n+\t\t\treleasem(park.m.ptr())\n+\n+\t\t\tgcMarkDone()\n+\n+\t\t\t// Disable preemption and prepare to reattach\n+\t\t\t// to the P.\n+\t\t\t//\n+\t\t\t// We may be running on a different P at this\n+\t\t\t// point, so we can't reattach until this G is\n+\t\t\t// parked.\n+\t\t\tpark.m.set(acquirem())\n+\t\t\tpark.attach.set(_p_)\n+\t\t}\n+\t}\n+}\n+\n+// gcMarkWorkAvailable returns true if executing a mark worker\n+// on p is potentially useful. p may be nil, in which case it only\n+// checks the global sources of work.\n+func gcMarkWorkAvailable(p *p) bool {\n+\tif p != nil && !p.gcw.empty() {\n+\t\treturn true\n+\t}\n+\tif atomic.Load64(&work.full) != 0 {\n+\t\treturn true // global work available\n+\t}\n+\tif work.markrootNext < work.markrootJobs {\n+\t\treturn true // root scan work available\n+\t}\n+\treturn false\n+}\n+\n+// gcMark runs the mark (or, for concurrent GC, mark termination)\n+// All gcWork caches must be empty.\n+// STW is in effect at this point.\n+//TODO go:nowritebarrier\n+func gcMark(start_time int64) {\n+\tif debug.allocfreetrace > 0 {\n+\t\ttracegc()\n+\t}\n+\n+\tif gcphase != _GCmarktermination {\n+\t\tthrow(\"in gcMark expecting to see gcphase as _GCmarktermination\")\n+\t}\n+\twork.tstart = start_time\n+\n+\t// Queue root marking jobs.\n+\tgcMarkRootPrepare()\n+\n+\twork.nwait = 0\n+\twork.ndone = 0\n+\twork.nproc = uint32(gcprocs())\n+\n+\tif debug.gcrescanstacks == 0 && work.full == 0 && work.nDataRoots+work.nSpanRoots+work.nStackRoots+work.nRescanRoots == 0 {\n+\t\t// There's no work on the work queue and no root jobs\n+\t\t// that can produce work, so don't bother entering the\n+\t\t// getfull() barrier.\n+\t\t//\n+\t\t// With the hybrid barrier enabled, this will be the\n+\t\t// situation the vast majority of the time after\n+\t\t// concurrent mark. However, we still need a fallback\n+\t\t// for STW GC and because there are some known races\n+\t\t// that occasionally leave work around for mark\n+\t\t// termination.\n+\t\t//\n+\t\t// We're still hedging our bets here: if we do\n+\t\t// accidentally produce some work, we'll still process\n+\t\t// it, just not necessarily in parallel.\n+\t\t//\n+\t\t// TODO(austin): When we eliminate\n+\t\t// debug.gcrescanstacks: fix the races, and remove\n+\t\t// work draining from mark termination so we don't\n+\t\t// need the fallback path.\n+\t\twork.helperDrainBlock = false\n+\t} else {\n+\t\twork.helperDrainBlock = true\n+\t}\n+\n+\tif trace.enabled {\n+\t\ttraceGCScanStart()\n+\t}\n+\n+\tif work.nproc > 1 {\n+\t\tnoteclear(&work.alldone)\n+\t\thelpgc(int32(work.nproc))\n+\t}\n+\n+\tgchelperstart()\n+\n+\tgcw := &getg().m.p.ptr().gcw\n+\tif work.helperDrainBlock {\n+\t\tgcDrain(gcw, gcDrainBlock)\n+\t} else {\n+\t\tgcDrain(gcw, gcDrainNoBlock)\n+\t}\n+\tgcw.dispose()\n+\n+\tif debug.gccheckmark > 0 {\n+\t\t// This is expensive when there's a large number of\n+\t\t// Gs, so only do it if checkmark is also enabled.\n+\t\tgcMarkRootCheck()\n+\t}\n+\tif work.full != 0 {\n+\t\tthrow(\"work.full != 0\")\n+\t}\n+\n+\tif work.nproc > 1 {\n+\t\tnotesleep(&work.alldone)\n+\t}\n+\n+\t// Record that at least one root marking pass has completed.\n+\twork.markrootDone = true\n+\n+\t// Double-check that all gcWork caches are empty. This should\n+\t// be ensured by mark 2 before we enter mark termination.\n+\tfor i := 0; i < int(gomaxprocs); i++ {\n+\t\tgcw := &allp[i].gcw\n+\t\tif !gcw.empty() {\n+\t\t\tthrow(\"P has cached GC work at end of mark termination\")\n+\t\t}\n+\t\tif gcw.scanWork != 0 || gcw.bytesMarked != 0 {\n+\t\t\tthrow(\"P has unflushed stats at end of mark termination\")\n+\t\t}\n+\t}\n+\n+\tif trace.enabled {\n+\t\ttraceGCScanDone()\n+\t}\n+\n+\tcachestats()\n+\n+\t// Update the marked heap stat.\n+\tmemstats.heap_marked = work.bytesMarked\n+\n+\t// Trigger the next GC cycle when the allocated heap has grown\n+\t// by triggerRatio over the marked heap size. Assume that\n+\t// we're in steady state, so the marked heap size is the\n+\t// same now as it was at the beginning of the GC cycle.\n+\tmemstats.gc_trigger = uint64(float64(memstats.heap_marked) * (1 + gcController.triggerRatio))\n+\tif memstats.gc_trigger < heapminimum {\n+\t\tmemstats.gc_trigger = heapminimum\n+\t}\n+\tif int64(memstats.gc_trigger) < 0 {\n+\t\tprint(\"next_gc=\", memstats.next_gc, \" bytesMarked=\", work.bytesMarked, \" heap_live=\", memstats.heap_live, \" initialHeapLive=\", work.initialHeapLive, \"\\n\")\n+\t\tthrow(\"gc_trigger underflow\")\n+\t}\n+\n+\t// Update other GC heap size stats. This must happen after\n+\t// cachestats (which flushes local statistics to these) and\n+\t// flushallmcaches (which modifies heap_live).\n+\tmemstats.heap_live = work.bytesMarked\n+\tmemstats.heap_scan = uint64(gcController.scanWork)\n+\n+\tminTrigger := memstats.heap_live + sweepMinHeapDistance*uint64(gcpercent)/100\n+\tif memstats.gc_trigger < minTrigger {\n+\t\t// The allocated heap is already past the trigger.\n+\t\t// This can happen if the triggerRatio is very low and\n+\t\t// the marked heap is less than the live heap size.\n+\t\t//\n+\t\t// Concurrent sweep happens in the heap growth from\n+\t\t// heap_live to gc_trigger, so bump gc_trigger up to ensure\n+\t\t// that concurrent sweep has some heap growth in which\n+\t\t// to perform sweeping before we start the next GC\n+\t\t// cycle.\n+\t\tmemstats.gc_trigger = minTrigger\n+\t}\n+\n+\t// The next GC cycle should finish before the allocated heap\n+\t// has grown by GOGC/100.\n+\tmemstats.next_gc = memstats.heap_marked + memstats.heap_marked*uint64(gcpercent)/100\n+\tif gcpercent < 0 {\n+\t\tmemstats.next_gc = ^uint64(0)\n+\t}\n+\tif memstats.next_gc < memstats.gc_trigger {\n+\t\tmemstats.next_gc = memstats.gc_trigger\n+\t}\n+\n+\tif trace.enabled {\n+\t\ttraceHeapAlloc()\n+\t\ttraceNextGC()\n+\t}\n+}\n+\n+func gcSweep(mode gcMode) {\n+\tif gcphase != _GCoff {\n+\t\tthrow(\"gcSweep being done but phase is not GCoff\")\n+\t}\n+\n+\tlock(&mheap_.lock)\n+\tmheap_.sweepgen += 2\n+\tmheap_.sweepdone = 0\n+\tif mheap_.sweepSpans[mheap_.sweepgen/2%2].index != 0 {\n+\t\t// We should have drained this list during the last\n+\t\t// sweep phase. We certainly need to start this phase\n+\t\t// with an empty swept list.\n+\t\tthrow(\"non-empty swept list\")\n+\t}\n+\tunlock(&mheap_.lock)\n+\n+\tif !_ConcurrentSweep || mode == gcForceBlockMode {\n+\t\t// Special case synchronous sweep.\n+\t\t// Record that no proportional sweeping has to happen.\n+\t\tlock(&mheap_.lock)\n+\t\tmheap_.sweepPagesPerByte = 0\n+\t\tmheap_.pagesSwept = 0\n+\t\tunlock(&mheap_.lock)\n+\t\t// Sweep all spans eagerly.\n+\t\tfor sweepone() != ^uintptr(0) {\n+\t\t\tsweep.npausesweep++\n+\t\t}\n+\t\t// Do an additional mProf_GC, because all 'free' events are now real as well.\n+\t\tmProf_GC()\n+\t\tmProf_GC()\n+\t\treturn\n+\t}\n+\n+\t// Concurrent sweep needs to sweep all of the in-use pages by\n+\t// the time the allocated heap reaches the GC trigger. Compute\n+\t// the ratio of in-use pages to sweep per byte allocated.\n+\theapDistance := int64(memstats.gc_trigger) - int64(memstats.heap_live)\n+\t// Add a little margin so rounding errors and concurrent\n+\t// sweep are less likely to leave pages unswept when GC starts.\n+\theapDistance -= 1024 * 1024\n+\tif heapDistance < _PageSize {\n+\t\t// Avoid setting the sweep ratio extremely high\n+\t\theapDistance = _PageSize\n+\t}\n+\tlock(&mheap_.lock)\n+\tmheap_.sweepPagesPerByte = float64(mheap_.pagesInUse) / float64(heapDistance)\n+\tmheap_.pagesSwept = 0\n+\tmheap_.spanBytesAlloc = 0\n+\tunlock(&mheap_.lock)\n+\n+\t// Background sweep.\n+\tlock(&sweep.lock)\n+\tif sweep.parked {\n+\t\tsweep.parked = false\n+\t\tready(sweep.g, 0, true)\n+\t}\n+\tunlock(&sweep.lock)\n+}\n+\n+// gcResetMarkState resets global state prior to marking (concurrent\n+// or STW) and resets the stack scan state of all Gs.\n+//\n+// This is safe to do without the world stopped because any Gs created\n+// during or after this will start out in the reset state.\n+func gcResetMarkState() {\n+\t// This may be called during a concurrent phase, so make sure\n+\t// allgs doesn't change.\n+\tif !(gcphase == _GCoff || gcphase == _GCmarktermination) {\n+\t\t// Accessing gcRescan is unsafe.\n+\t\tthrow(\"bad GC phase\")\n+\t}\n+\tlock(&allglock)\n+\tfor _, gp := range allgs {\n+\t\tgp.gcscandone = false  // set to true in gcphasework\n+\t\tgp.gcscanvalid = false // stack has not been scanned\n+\t\tgp.gcRescan = -1\n+\t\tgp.gcAssistBytes = 0\n+\t}\n+\tunlock(&allglock)\n+\n+\t// Clear rescan list.\n+\twork.rescan.list = work.rescan.list[:0]\n+\n+\twork.bytesMarked = 0\n+\twork.initialHeapLive = memstats.heap_live\n+\twork.markrootDone = false\n+}\n+\n+// Hooks for other packages\n+\n+var poolcleanup func()\n+\n+//go:linkname sync_runtime_registerPoolCleanup sync.runtime_registerPoolCleanup\n+func sync_runtime_registerPoolCleanup(f func()) {\n+\tpoolcleanup = f\n+}\n+\n+func clearpools() {\n+\t// clear sync.Pools\n+\tif poolcleanup != nil {\n+\t\tpoolcleanup()\n+\t}\n+\n+\t// Clear central sudog cache.\n+\t// Leave per-P caches alone, they have strictly bounded size.\n+\t// Disconnect cached list before dropping it on the floor,\n+\t// so that a dangling ref to one entry does not pin all of them.\n+\tlock(&sched.sudoglock)\n+\tvar sg, sgnext *sudog\n+\tfor sg = sched.sudogcache; sg != nil; sg = sgnext {\n+\t\tsgnext = sg.next\n+\t\tsg.next = nil\n+\t}\n+\tsched.sudogcache = nil\n+\tunlock(&sched.sudoglock)\n+\n+\t// Clear central defer pools.\n+\t// Leave per-P pools alone, they have strictly bounded size.\n+\tlock(&sched.deferlock)\n+\t// disconnect cached list before dropping it on the floor,\n+\t// so that a dangling ref to one entry does not pin all of them.\n+\tvar d, dlink *_defer\n+\tfor d = sched.deferpool; d != nil; d = dlink {\n+\t\tdlink = d.link\n+\t\td.link = nil\n+\t}\n+\tsched.deferpool = nil\n+\tunlock(&sched.deferlock)\n+}\n+\n+// Timing\n+\n+//go:nowritebarrier\n+func gchelper() {\n+\t_g_ := getg()\n+\t_g_.m.traceback = 2\n+\tgchelperstart()\n+\n+\tif trace.enabled {\n+\t\ttraceGCScanStart()\n+\t}\n+\n+\t// Parallel mark over GC roots and heap\n+\tif gcphase == _GCmarktermination {\n+\t\tgcw := &_g_.m.p.ptr().gcw\n+\t\tif work.helperDrainBlock {\n+\t\t\tgcDrain(gcw, gcDrainBlock) // blocks in getfull\n+\t\t} else {\n+\t\t\tgcDrain(gcw, gcDrainNoBlock)\n+\t\t}\n+\t\tgcw.dispose()\n+\t}\n+\n+\tif trace.enabled {\n+\t\ttraceGCScanDone()\n+\t}\n+\n+\tnproc := work.nproc // work.nproc can change right after we increment work.ndone\n+\tif atomic.Xadd(&work.ndone, +1) == nproc-1 {\n+\t\tnotewakeup(&work.alldone)\n+\t}\n+\t_g_.m.traceback = 0\n+}\n+\n+func gchelperstart() {\n+\t_g_ := getg()\n+\n+\tif _g_.m.helpgc < 0 || _g_.m.helpgc >= _MaxGcproc {\n+\t\tthrow(\"gchelperstart: bad m->helpgc\")\n+\t}\n+\t// For gccgo we run gchelper on the normal g stack.\n+\t// if _g_ != _g_.m.g0 {\n+\t// \tthrow(\"gchelper not running on g0 stack\")\n+\t// }\n+}\n+\n+// itoaDiv formats val/(10**dec) into buf.\n+func itoaDiv(buf []byte, val uint64, dec int) []byte {\n+\ti := len(buf) - 1\n+\tidec := i - dec\n+\tfor val >= 10 || i >= idec {\n+\t\tbuf[i] = byte(val%10 + '0')\n+\t\ti--\n+\t\tif i == idec {\n+\t\t\tbuf[i] = '.'\n+\t\t\ti--\n+\t\t}\n+\t\tval /= 10\n+\t}\n+\tbuf[i] = byte(val + '0')\n+\treturn buf[i:]\n+}\n+\n+// fmtNSAsMS nicely formats ns nanoseconds as milliseconds.\n+func fmtNSAsMS(buf []byte, ns uint64) []byte {\n+\tif ns >= 10e6 {\n+\t\t// Format as whole milliseconds.\n+\t\treturn itoaDiv(buf, ns/1e6, 0)\n+\t}\n+\t// Format two digits of precision, with at most three decimal places.\n+\tx := ns / 1e3\n+\tif x == 0 {\n+\t\tbuf[0] = '0'\n+\t\treturn buf[:1]\n+\t}\n+\tdec := 3\n+\tfor x >= 100 {\n+\t\tx /= 10\n+\t\tdec--\n+\t}\n+\treturn itoaDiv(buf, x, dec)\n+}"}, {"sha": "c1fa1547adc76a09766a0fabea124dbe04055301", "filename": "libgo/go/runtime/mgc_gccgo.go", "status": "added", "additions": 87, "deletions": 0, "changes": 87, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmgc_gccgo.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmgc_gccgo.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmgc_gccgo.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,87 @@\n+// Copyright 2016 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// gccgo-specific support for GC.\n+\n+package runtime\n+\n+import \"unsafe\"\n+\n+// gcRoot is a single GC root: a variable plus a ptrmask.\n+type gcRoot struct {\n+\tdecl    unsafe.Pointer // Pointer to variable.\n+\tsize    uintptr        // Size of variable.\n+\tptrdata uintptr        // Length of gcdata.\n+\tgcdata  *uint8         // Pointer mask.\n+}\n+\n+// gcRootList is the set of GC roots for a package.\n+// The next field is used to put this all into a linked list.\n+// count gives the real length of the array.\n+type gcRootList struct {\n+\tnext  *gcRootList\n+\tcount int\n+\troots [1 << 26]gcRoot\n+}\n+\n+// roots is the list of GC roots for the program.\n+// The compiler keeps this variable itself off the list.\n+var gcRoots *gcRootList\n+\n+// registerGCRoots is called by compiler-generated code.\n+//go:linkname registerGCRoots runtime.registerGCRoots\n+\n+// registerGCRoots is called by init functions to register the GC\n+// roots for a package.  The init functions are run sequentially at\n+// the start of the program, so no locking is needed.\n+func registerGCRoots(r *gcRootList) {\n+\tr.next = gcRoots\n+\tgcRoots = r\n+}\n+\n+// checkPreempt is called when the preempt field in the running G is true.\n+// It preempts the goroutine if it is safe to do so.\n+// If preemptscan is true, this scans the stack for the garbage collector\n+// and carries on.\n+func checkPreempt() {\n+\tgp := getg()\n+\tif !gp.preempt || gp != gp.m.curg || gp.m.locks != 0 || gp.m.mallocing != 0 || gp.m.preemptoff != \"\" {\n+\t\treturn\n+\t}\n+\n+\t// Synchronize with scang.\n+\tgp.scanningself = true\n+\tcasgstatus(gp, _Grunning, _Gwaiting)\n+\tif gp.preemptscan {\n+\t\tfor !castogscanstatus(gp, _Gwaiting, _Gscanwaiting) {\n+\t\t\t// Likely to be racing with the GC as\n+\t\t\t// it sees a _Gwaiting and does the\n+\t\t\t// stack scan. If so, gcworkdone will\n+\t\t\t// be set and gcphasework will simply\n+\t\t\t// return.\n+\t\t}\n+\t\tif !gp.gcscandone {\n+\t\t\tmp := acquirem()\n+\t\t\tgcw := &gp.m.p.ptr().gcw\n+\t\t\tscanstack(gp, gcw)\n+\t\t\tif gcBlackenPromptly {\n+\t\t\t\tgcw.dispose()\n+\t\t\t}\n+\t\t\treleasem(mp)\n+\t\t\tgp.gcscandone = true\n+\t\t}\n+\t\tgp.preemptscan = false\n+\t\tgp.preempt = false\n+\t\tcasfrom_Gscanstatus(gp, _Gscanwaiting, _Gwaiting)\n+\t\t// This clears gcscanvalid.\n+\t\tcasgstatus(gp, _Gwaiting, _Grunning)\n+\t\tgp.scanningself = false\n+\t\treturn\n+\t}\n+\n+\t// Act like goroutine called runtime.Gosched.\n+\tcasgstatus(gp, _Gwaiting, _Grunning)\n+\tgp.scanningself = false\n+\tmcall(gopreempt_m)\n+}"}, {"sha": "93252ba8a9e6600a2d50724efeac1c8799ee3469", "filename": "libgo/go/runtime/mgcmark.go", "status": "added", "additions": 1374, "deletions": 0, "changes": 1374, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmgcmark.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmgcmark.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmgcmark.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,1374 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Garbage collector: marking and scanning\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/atomic\"\n+\t\"runtime/internal/sys\"\n+\t\"unsafe\"\n+)\n+\n+const (\n+\tfixedRootFinalizers = iota\n+\tfixedRootFreeGStacks\n+\tfixedRootCount\n+\n+\t// rootBlockBytes is the number of bytes to scan per data or\n+\t// BSS root.\n+\trootBlockBytes = 256 << 10\n+\n+\t// rootBlockSpans is the number of spans to scan per span\n+\t// root.\n+\trootBlockSpans = 8 * 1024 // 64MB worth of spans\n+\n+\t// maxObletBytes is the maximum bytes of an object to scan at\n+\t// once. Larger objects will be split up into \"oblets\" of at\n+\t// most this size. Since we can scan 1\u20132 MB/ms, 128 KB bounds\n+\t// scan preemption at ~100 \u00b5s.\n+\t//\n+\t// This must be > _MaxSmallSize so that the object base is the\n+\t// span base.\n+\tmaxObletBytes = 128 << 10\n+\n+\t// idleCheckThreshold specifies how many units of work to do\n+\t// between run queue checks in an idle worker. Assuming a scan\n+\t// rate of 1 MB/ms, this is ~100 \u00b5s. Lower values have higher\n+\t// overhead in the scan loop (the scheduler check may perform\n+\t// a syscall, so its overhead is nontrivial). Higher values\n+\t// make the system less responsive to incoming work.\n+\tidleCheckThreshold = 100000\n+)\n+\n+// gcMarkRootPrepare queues root scanning jobs (stacks, globals, and\n+// some miscellany) and initializes scanning-related state.\n+//\n+// The caller must have call gcCopySpans().\n+//\n+// The world must be stopped.\n+//\n+//go:nowritebarrier\n+func gcMarkRootPrepare() {\n+\tif gcphase == _GCmarktermination {\n+\t\twork.nFlushCacheRoots = int(gomaxprocs)\n+\t} else {\n+\t\twork.nFlushCacheRoots = 0\n+\t}\n+\n+\twork.nDataRoots = 0\n+\n+\t// Only scan globals once per cycle; preferably concurrently.\n+\tif !work.markrootDone {\n+\t\troots := gcRoots\n+\t\tfor roots != nil {\n+\t\t\twork.nDataRoots++\n+\t\t\troots = roots.next\n+\t\t}\n+\t}\n+\n+\tif !work.markrootDone {\n+\t\t// On the first markroot, we need to scan span roots.\n+\t\t// In concurrent GC, this happens during concurrent\n+\t\t// mark and we depend on addfinalizer to ensure the\n+\t\t// above invariants for objects that get finalizers\n+\t\t// after concurrent mark. In STW GC, this will happen\n+\t\t// during mark termination.\n+\t\t//\n+\t\t// We're only interested in scanning the in-use spans,\n+\t\t// which will all be swept at this point. More spans\n+\t\t// may be added to this list during concurrent GC, but\n+\t\t// we only care about spans that were allocated before\n+\t\t// this mark phase.\n+\t\twork.nSpanRoots = mheap_.sweepSpans[mheap_.sweepgen/2%2].numBlocks()\n+\n+\t\t// On the first markroot, we need to scan all Gs. Gs\n+\t\t// may be created after this point, but it's okay that\n+\t\t// we ignore them because they begin life without any\n+\t\t// roots, so there's nothing to scan, and any roots\n+\t\t// they create during the concurrent phase will be\n+\t\t// scanned during mark termination. During mark\n+\t\t// termination, allglen isn't changing, so we'll scan\n+\t\t// all Gs.\n+\t\twork.nStackRoots = int(atomic.Loaduintptr(&allglen))\n+\t\twork.nRescanRoots = 0\n+\t} else {\n+\t\t// We've already scanned span roots and kept the scan\n+\t\t// up-to-date during concurrent mark.\n+\t\twork.nSpanRoots = 0\n+\n+\t\t// On the second pass of markroot, we're just scanning\n+\t\t// dirty stacks. It's safe to access rescan since the\n+\t\t// world is stopped.\n+\t\twork.nStackRoots = 0\n+\t\twork.nRescanRoots = len(work.rescan.list)\n+\t}\n+\n+\twork.markrootNext = 0\n+\twork.markrootJobs = uint32(fixedRootCount + work.nFlushCacheRoots + work.nDataRoots + work.nSpanRoots + work.nStackRoots + work.nRescanRoots)\n+}\n+\n+// gcMarkRootCheck checks that all roots have been scanned. It is\n+// purely for debugging.\n+func gcMarkRootCheck() {\n+\tif work.markrootNext < work.markrootJobs {\n+\t\tprint(work.markrootNext, \" of \", work.markrootJobs, \" markroot jobs done\\n\")\n+\t\tthrow(\"left over markroot jobs\")\n+\t}\n+\n+\tlock(&allglock)\n+\t// Check that stacks have been scanned.\n+\tvar gp *g\n+\tif gcphase == _GCmarktermination && debug.gcrescanstacks > 0 {\n+\t\tfor i := 0; i < len(allgs); i++ {\n+\t\t\tgp = allgs[i]\n+\t\t\tif !(gp.gcscandone && gp.gcscanvalid) && readgstatus(gp) != _Gdead {\n+\t\t\t\tgoto fail\n+\t\t\t}\n+\t\t}\n+\t} else {\n+\t\tfor i := 0; i < work.nStackRoots; i++ {\n+\t\t\tgp = allgs[i]\n+\t\t\tif !gp.gcscandone {\n+\t\t\t\tgoto fail\n+\t\t\t}\n+\t\t}\n+\t}\n+\tunlock(&allglock)\n+\treturn\n+\n+fail:\n+\tprintln(\"gp\", gp, \"goid\", gp.goid,\n+\t\t\"status\", readgstatus(gp),\n+\t\t\"gcscandone\", gp.gcscandone,\n+\t\t\"gcscanvalid\", gp.gcscanvalid)\n+\tunlock(&allglock) // Avoid self-deadlock with traceback.\n+\tthrow(\"scan missed a g\")\n+}\n+\n+// ptrmask for an allocation containing a single pointer.\n+var oneptrmask = [...]uint8{1}\n+\n+// markroot scans the i'th root.\n+//\n+// Preemption must be disabled (because this uses a gcWork).\n+//\n+// nowritebarrier is only advisory here.\n+//\n+//go:nowritebarrier\n+func markroot(gcw *gcWork, i uint32) {\n+\t// TODO(austin): This is a bit ridiculous. Compute and store\n+\t// the bases in gcMarkRootPrepare instead of the counts.\n+\tbaseFlushCache := uint32(fixedRootCount)\n+\tbaseData := baseFlushCache + uint32(work.nFlushCacheRoots)\n+\tbaseSpans := baseData + uint32(work.nDataRoots)\n+\tbaseStacks := baseSpans + uint32(work.nSpanRoots)\n+\tbaseRescan := baseStacks + uint32(work.nStackRoots)\n+\tend := baseRescan + uint32(work.nRescanRoots)\n+\n+\t// Note: if you add a case here, please also update heapdump.go:dumproots.\n+\tswitch {\n+\tcase baseFlushCache <= i && i < baseData:\n+\t\tflushmcache(int(i - baseFlushCache))\n+\n+\tcase baseData <= i && i < baseSpans:\n+\t\troots := gcRoots\n+\t\tc := baseData\n+\t\tfor roots != nil {\n+\t\t\tif i == c {\n+\t\t\t\tmarkrootBlock(roots, gcw)\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\troots = roots.next\n+\t\t\tc++\n+\t\t}\n+\n+\tcase i == fixedRootFinalizers:\n+\t\tfor fb := allfin; fb != nil; fb = fb.alllink {\n+\t\t\tcnt := uintptr(atomic.Load(&fb.cnt))\n+\t\t\tscanblock(uintptr(unsafe.Pointer(&fb.fin[0])), cnt*unsafe.Sizeof(fb.fin[0]), &finptrmask[0], gcw)\n+\t\t}\n+\n+\tcase i == fixedRootFreeGStacks:\n+\t\t// FIXME: We don't do this for gccgo.\n+\n+\tcase baseSpans <= i && i < baseStacks:\n+\t\t// mark MSpan.specials\n+\t\tmarkrootSpans(gcw, int(i-baseSpans))\n+\n+\tdefault:\n+\t\t// the rest is scanning goroutine stacks\n+\t\tvar gp *g\n+\t\tif baseStacks <= i && i < baseRescan {\n+\t\t\tgp = allgs[i-baseStacks]\n+\t\t} else if baseRescan <= i && i < end {\n+\t\t\tgp = work.rescan.list[i-baseRescan].ptr()\n+\t\t\tif gp.gcRescan != int32(i-baseRescan) {\n+\t\t\t\t// Looking for issue #17099.\n+\t\t\t\tprintln(\"runtime: gp\", gp, \"found at rescan index\", i-baseRescan, \"but should be at\", gp.gcRescan)\n+\t\t\t\tthrow(\"bad g rescan index\")\n+\t\t\t}\n+\t\t} else {\n+\t\t\tthrow(\"markroot: bad index\")\n+\t\t}\n+\n+\t\t// remember when we've first observed the G blocked\n+\t\t// needed only to output in traceback\n+\t\tstatus := readgstatus(gp) // We are not in a scan state\n+\t\tif (status == _Gwaiting || status == _Gsyscall) && gp.waitsince == 0 {\n+\t\t\tgp.waitsince = work.tstart\n+\t\t}\n+\n+\t\t// scang must be done on the system stack in case\n+\t\t// we're trying to scan our own stack.\n+\t\tsystemstack(func() {\n+\t\t\t// If this is a self-scan, put the user G in\n+\t\t\t// _Gwaiting to prevent self-deadlock. It may\n+\t\t\t// already be in _Gwaiting if this is a mark\n+\t\t\t// worker or we're in mark termination.\n+\t\t\tuserG := getg().m.curg\n+\t\t\tselfScan := gp == userG && readgstatus(userG) == _Grunning\n+\t\t\tif selfScan {\n+\t\t\t\tcasgstatus(userG, _Grunning, _Gwaiting)\n+\t\t\t\tuserG.waitreason = \"garbage collection scan\"\n+\t\t\t}\n+\n+\t\t\t// TODO: scang blocks until gp's stack has\n+\t\t\t// been scanned, which may take a while for\n+\t\t\t// running goroutines. Consider doing this in\n+\t\t\t// two phases where the first is non-blocking:\n+\t\t\t// we scan the stacks we can and ask running\n+\t\t\t// goroutines to scan themselves; and the\n+\t\t\t// second blocks.\n+\t\t\tscang(gp, gcw)\n+\n+\t\t\tif selfScan {\n+\t\t\t\tcasgstatus(userG, _Gwaiting, _Grunning)\n+\t\t\t}\n+\t\t})\n+\t}\n+}\n+\n+// markrootBlock scans one element of the list of GC roots.\n+//\n+//go:nowritebarrier\n+func markrootBlock(roots *gcRootList, gcw *gcWork) {\n+\tfor i := 0; i < roots.count; i++ {\n+\t\tr := &roots.roots[i]\n+\t\tscanblock(uintptr(r.decl), r.ptrdata, r.gcdata, gcw)\n+\t}\n+}\n+\n+// markrootSpans marks roots for one shard of work.spans.\n+//\n+//go:nowritebarrier\n+func markrootSpans(gcw *gcWork, shard int) {\n+\t// Objects with finalizers have two GC-related invariants:\n+\t//\n+\t// 1) Everything reachable from the object must be marked.\n+\t// This ensures that when we pass the object to its finalizer,\n+\t// everything the finalizer can reach will be retained.\n+\t//\n+\t// 2) Finalizer specials (which are not in the garbage\n+\t// collected heap) are roots. In practice, this means the fn\n+\t// field must be scanned.\n+\t//\n+\t// TODO(austin): There are several ideas for making this more\n+\t// efficient in issue #11485.\n+\n+\tif work.markrootDone {\n+\t\tthrow(\"markrootSpans during second markroot\")\n+\t}\n+\n+\tsg := mheap_.sweepgen\n+\tspans := mheap_.sweepSpans[mheap_.sweepgen/2%2].block(shard)\n+\t// Note that work.spans may not include spans that were\n+\t// allocated between entering the scan phase and now. This is\n+\t// okay because any objects with finalizers in those spans\n+\t// must have been allocated and given finalizers after we\n+\t// entered the scan phase, so addfinalizer will have ensured\n+\t// the above invariants for them.\n+\tfor _, s := range spans {\n+\t\tif s.state != mSpanInUse {\n+\t\t\tcontinue\n+\t\t}\n+\t\tif !useCheckmark && s.sweepgen != sg {\n+\t\t\t// sweepgen was updated (+2) during non-checkmark GC pass\n+\t\t\tprint(\"sweep \", s.sweepgen, \" \", sg, \"\\n\")\n+\t\t\tthrow(\"gc: unswept span\")\n+\t\t}\n+\n+\t\t// Speculatively check if there are any specials\n+\t\t// without acquiring the span lock. This may race with\n+\t\t// adding the first special to a span, but in that\n+\t\t// case addfinalizer will observe that the GC is\n+\t\t// active (which is globally synchronized) and ensure\n+\t\t// the above invariants. We may also ensure the\n+\t\t// invariants, but it's okay to scan an object twice.\n+\t\tif s.specials == nil {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\t// Lock the specials to prevent a special from being\n+\t\t// removed from the list while we're traversing it.\n+\t\tlock(&s.speciallock)\n+\n+\t\tfor sp := s.specials; sp != nil; sp = sp.next {\n+\t\t\tif sp.kind != _KindSpecialFinalizer {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\t// don't mark finalized object, but scan it so we\n+\t\t\t// retain everything it points to.\n+\t\t\tspf := (*specialfinalizer)(unsafe.Pointer(sp))\n+\t\t\t// A finalizer can be set for an inner byte of an object, find object beginning.\n+\t\t\tp := s.base() + uintptr(spf.special.offset)/s.elemsize*s.elemsize\n+\n+\t\t\t// Mark everything that can be reached from\n+\t\t\t// the object (but *not* the object itself or\n+\t\t\t// we'll never collect it).\n+\t\t\tscanobject(p, gcw)\n+\n+\t\t\t// The special itself is a root.\n+\t\t\tscanblock(uintptr(unsafe.Pointer(&spf.fn)), sys.PtrSize, &oneptrmask[0], gcw)\n+\t\t}\n+\n+\t\tunlock(&s.speciallock)\n+\t}\n+}\n+\n+// gcAssistAlloc performs GC work to make gp's assist debt positive.\n+// gp must be the calling user gorountine.\n+//\n+// This must be called with preemption enabled.\n+func gcAssistAlloc(gp *g) {\n+\t// Don't assist in non-preemptible contexts. These are\n+\t// generally fragile and won't allow the assist to block.\n+\tif getg() == gp.m.g0 {\n+\t\treturn\n+\t}\n+\tif mp := getg().m; mp.locks > 0 || mp.preemptoff != \"\" {\n+\t\treturn\n+\t}\n+\n+retry:\n+\t// Compute the amount of scan work we need to do to make the\n+\t// balance positive. When the required amount of work is low,\n+\t// we over-assist to build up credit for future allocations\n+\t// and amortize the cost of assisting.\n+\tdebtBytes := -gp.gcAssistBytes\n+\tscanWork := int64(gcController.assistWorkPerByte * float64(debtBytes))\n+\tif scanWork < gcOverAssistWork {\n+\t\tscanWork = gcOverAssistWork\n+\t\tdebtBytes = int64(gcController.assistBytesPerWork * float64(scanWork))\n+\t}\n+\n+\t// Steal as much credit as we can from the background GC's\n+\t// scan credit. This is racy and may drop the background\n+\t// credit below 0 if two mutators steal at the same time. This\n+\t// will just cause steals to fail until credit is accumulated\n+\t// again, so in the long run it doesn't really matter, but we\n+\t// do have to handle the negative credit case.\n+\tbgScanCredit := atomic.Loadint64(&gcController.bgScanCredit)\n+\tstolen := int64(0)\n+\tif bgScanCredit > 0 {\n+\t\tif bgScanCredit < scanWork {\n+\t\t\tstolen = bgScanCredit\n+\t\t\tgp.gcAssistBytes += 1 + int64(gcController.assistBytesPerWork*float64(stolen))\n+\t\t} else {\n+\t\t\tstolen = scanWork\n+\t\t\tgp.gcAssistBytes += debtBytes\n+\t\t}\n+\t\tatomic.Xaddint64(&gcController.bgScanCredit, -stolen)\n+\n+\t\tscanWork -= stolen\n+\n+\t\tif scanWork == 0 {\n+\t\t\t// We were able to steal all of the credit we\n+\t\t\t// needed.\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\t// Perform assist work\n+\tsystemstack(func() {\n+\t\tgcAssistAlloc1(gp, scanWork)\n+\t\t// The user stack may have moved, so this can't touch\n+\t\t// anything on it until it returns from systemstack.\n+\t})\n+\n+\tcompleted := gp.param != nil\n+\tgp.param = nil\n+\tif completed {\n+\t\tgcMarkDone()\n+\t}\n+\n+\tif gp.gcAssistBytes < 0 {\n+\t\t// We were unable steal enough credit or perform\n+\t\t// enough work to pay off the assist debt. We need to\n+\t\t// do one of these before letting the mutator allocate\n+\t\t// more to prevent over-allocation.\n+\t\t//\n+\t\t// If this is because we were preempted, reschedule\n+\t\t// and try some more.\n+\t\tif gp.preempt {\n+\t\t\tGosched()\n+\t\t\tgoto retry\n+\t\t}\n+\n+\t\t// Add this G to an assist queue and park. When the GC\n+\t\t// has more background credit, it will satisfy queued\n+\t\t// assists before flushing to the global credit pool.\n+\t\t//\n+\t\t// Note that this does *not* get woken up when more\n+\t\t// work is added to the work list. The theory is that\n+\t\t// there wasn't enough work to do anyway, so we might\n+\t\t// as well let background marking take care of the\n+\t\t// work that is available.\n+\t\tif !gcParkAssist() {\n+\t\t\tgoto retry\n+\t\t}\n+\n+\t\t// At this point either background GC has satisfied\n+\t\t// this G's assist debt, or the GC cycle is over.\n+\t}\n+}\n+\n+// gcAssistAlloc1 is the part of gcAssistAlloc that runs on the system\n+// stack. This is a separate function to make it easier to see that\n+// we're not capturing anything from the user stack, since the user\n+// stack may move while we're in this function.\n+//\n+// gcAssistAlloc1 indicates whether this assist completed the mark\n+// phase by setting gp.param to non-nil. This can't be communicated on\n+// the stack since it may move.\n+//\n+//go:systemstack\n+func gcAssistAlloc1(gp *g, scanWork int64) {\n+\t// Clear the flag indicating that this assist completed the\n+\t// mark phase.\n+\tgp.param = nil\n+\n+\tif atomic.Load(&gcBlackenEnabled) == 0 {\n+\t\t// The gcBlackenEnabled check in malloc races with the\n+\t\t// store that clears it but an atomic check in every malloc\n+\t\t// would be a performance hit.\n+\t\t// Instead we recheck it here on the non-preemptable system\n+\t\t// stack to determine if we should preform an assist.\n+\n+\t\t// GC is done, so ignore any remaining debt.\n+\t\tgp.gcAssistBytes = 0\n+\t\treturn\n+\t}\n+\t// Track time spent in this assist. Since we're on the\n+\t// system stack, this is non-preemptible, so we can\n+\t// just measure start and end time.\n+\tstartTime := nanotime()\n+\n+\tdecnwait := atomic.Xadd(&work.nwait, -1)\n+\tif decnwait == work.nproc {\n+\t\tprintln(\"runtime: work.nwait =\", decnwait, \"work.nproc=\", work.nproc)\n+\t\tthrow(\"nwait > work.nprocs\")\n+\t}\n+\n+\t// gcDrainN requires the caller to be preemptible.\n+\tcasgstatus(gp, _Grunning, _Gwaiting)\n+\tgp.waitreason = \"GC assist marking\"\n+\n+\t// drain own cached work first in the hopes that it\n+\t// will be more cache friendly.\n+\tgcw := &getg().m.p.ptr().gcw\n+\tworkDone := gcDrainN(gcw, scanWork)\n+\t// If we are near the end of the mark phase\n+\t// dispose of the gcw.\n+\tif gcBlackenPromptly {\n+\t\tgcw.dispose()\n+\t}\n+\n+\tcasgstatus(gp, _Gwaiting, _Grunning)\n+\n+\t// Record that we did this much scan work.\n+\t//\n+\t// Back out the number of bytes of assist credit that\n+\t// this scan work counts for. The \"1+\" is a poor man's\n+\t// round-up, to ensure this adds credit even if\n+\t// assistBytesPerWork is very low.\n+\tgp.gcAssistBytes += 1 + int64(gcController.assistBytesPerWork*float64(workDone))\n+\n+\t// If this is the last worker and we ran out of work,\n+\t// signal a completion point.\n+\tincnwait := atomic.Xadd(&work.nwait, +1)\n+\tif incnwait > work.nproc {\n+\t\tprintln(\"runtime: work.nwait=\", incnwait,\n+\t\t\t\"work.nproc=\", work.nproc,\n+\t\t\t\"gcBlackenPromptly=\", gcBlackenPromptly)\n+\t\tthrow(\"work.nwait > work.nproc\")\n+\t}\n+\n+\tif incnwait == work.nproc && !gcMarkWorkAvailable(nil) {\n+\t\t// This has reached a background completion point. Set\n+\t\t// gp.param to a non-nil value to indicate this. It\n+\t\t// doesn't matter what we set it to (it just has to be\n+\t\t// a valid pointer).\n+\t\tgp.param = unsafe.Pointer(gp)\n+\t}\n+\tduration := nanotime() - startTime\n+\t_p_ := gp.m.p.ptr()\n+\t_p_.gcAssistTime += duration\n+\tif _p_.gcAssistTime > gcAssistTimeSlack {\n+\t\tatomic.Xaddint64(&gcController.assistTime, _p_.gcAssistTime)\n+\t\t_p_.gcAssistTime = 0\n+\t}\n+}\n+\n+// gcWakeAllAssists wakes all currently blocked assists. This is used\n+// at the end of a GC cycle. gcBlackenEnabled must be false to prevent\n+// new assists from going to sleep after this point.\n+func gcWakeAllAssists() {\n+\tlock(&work.assistQueue.lock)\n+\tinjectglist(work.assistQueue.head.ptr())\n+\twork.assistQueue.head.set(nil)\n+\twork.assistQueue.tail.set(nil)\n+\tunlock(&work.assistQueue.lock)\n+}\n+\n+// gcParkAssist puts the current goroutine on the assist queue and parks.\n+//\n+// gcParkAssist returns whether the assist is now satisfied. If it\n+// returns false, the caller must retry the assist.\n+//\n+//go:nowritebarrier\n+func gcParkAssist() bool {\n+\tlock(&work.assistQueue.lock)\n+\t// If the GC cycle finished while we were getting the lock,\n+\t// exit the assist. The cycle can't finish while we hold the\n+\t// lock.\n+\tif atomic.Load(&gcBlackenEnabled) == 0 {\n+\t\tunlock(&work.assistQueue.lock)\n+\t\treturn true\n+\t}\n+\n+\tgp := getg()\n+\toldHead, oldTail := work.assistQueue.head, work.assistQueue.tail\n+\tif oldHead == 0 {\n+\t\twork.assistQueue.head.set(gp)\n+\t} else {\n+\t\toldTail.ptr().schedlink.set(gp)\n+\t}\n+\twork.assistQueue.tail.set(gp)\n+\tgp.schedlink.set(nil)\n+\n+\t// Recheck for background credit now that this G is in\n+\t// the queue, but can still back out. This avoids a\n+\t// race in case background marking has flushed more\n+\t// credit since we checked above.\n+\tif atomic.Loadint64(&gcController.bgScanCredit) > 0 {\n+\t\twork.assistQueue.head = oldHead\n+\t\twork.assistQueue.tail = oldTail\n+\t\tif oldTail != 0 {\n+\t\t\toldTail.ptr().schedlink.set(nil)\n+\t\t}\n+\t\tunlock(&work.assistQueue.lock)\n+\t\treturn false\n+\t}\n+\t// Park.\n+\tgoparkunlock(&work.assistQueue.lock, \"GC assist wait\", traceEvGoBlockGC, 2)\n+\treturn true\n+}\n+\n+// gcFlushBgCredit flushes scanWork units of background scan work\n+// credit. This first satisfies blocked assists on the\n+// work.assistQueue and then flushes any remaining credit to\n+// gcController.bgScanCredit.\n+//\n+// Write barriers are disallowed because this is used by gcDrain after\n+// it has ensured that all work is drained and this must preserve that\n+// condition.\n+//\n+//go:nowritebarrierrec\n+func gcFlushBgCredit(scanWork int64) {\n+\tif work.assistQueue.head == 0 {\n+\t\t// Fast path; there are no blocked assists. There's a\n+\t\t// small window here where an assist may add itself to\n+\t\t// the blocked queue and park. If that happens, we'll\n+\t\t// just get it on the next flush.\n+\t\tatomic.Xaddint64(&gcController.bgScanCredit, scanWork)\n+\t\treturn\n+\t}\n+\n+\tscanBytes := int64(float64(scanWork) * gcController.assistBytesPerWork)\n+\n+\tlock(&work.assistQueue.lock)\n+\tgp := work.assistQueue.head.ptr()\n+\tfor gp != nil && scanBytes > 0 {\n+\t\t// Note that gp.gcAssistBytes is negative because gp\n+\t\t// is in debt. Think carefully about the signs below.\n+\t\tif scanBytes+gp.gcAssistBytes >= 0 {\n+\t\t\t// Satisfy this entire assist debt.\n+\t\t\tscanBytes += gp.gcAssistBytes\n+\t\t\tgp.gcAssistBytes = 0\n+\t\t\txgp := gp\n+\t\t\tgp = gp.schedlink.ptr()\n+\t\t\t// It's important that we *not* put xgp in\n+\t\t\t// runnext. Otherwise, it's possible for user\n+\t\t\t// code to exploit the GC worker's high\n+\t\t\t// scheduler priority to get itself always run\n+\t\t\t// before other goroutines and always in the\n+\t\t\t// fresh quantum started by GC.\n+\t\t\tready(xgp, 0, false)\n+\t\t} else {\n+\t\t\t// Partially satisfy this assist.\n+\t\t\tgp.gcAssistBytes += scanBytes\n+\t\t\tscanBytes = 0\n+\t\t\t// As a heuristic, we move this assist to the\n+\t\t\t// back of the queue so that large assists\n+\t\t\t// can't clog up the assist queue and\n+\t\t\t// substantially delay small assists.\n+\t\t\txgp := gp\n+\t\t\tgp = gp.schedlink.ptr()\n+\t\t\tif gp == nil {\n+\t\t\t\t// gp is the only assist in the queue.\n+\t\t\t\tgp = xgp\n+\t\t\t} else {\n+\t\t\t\txgp.schedlink = 0\n+\t\t\t\twork.assistQueue.tail.ptr().schedlink.set(xgp)\n+\t\t\t\twork.assistQueue.tail.set(xgp)\n+\t\t\t}\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\twork.assistQueue.head.set(gp)\n+\tif gp == nil {\n+\t\twork.assistQueue.tail.set(nil)\n+\t}\n+\n+\tif scanBytes > 0 {\n+\t\t// Convert from scan bytes back to work.\n+\t\tscanWork = int64(float64(scanBytes) * gcController.assistWorkPerByte)\n+\t\tatomic.Xaddint64(&gcController.bgScanCredit, scanWork)\n+\t}\n+\tunlock(&work.assistQueue.lock)\n+}\n+\n+// We use a C function to find the stack.\n+func doscanstack(*g, *gcWork)\n+\n+// scanstack scans gp's stack, greying all pointers found on the stack.\n+//\n+// During mark phase, it also installs stack barriers while traversing\n+// gp's stack. During mark termination, it stops scanning when it\n+// reaches an unhit stack barrier.\n+//\n+// scanstack is marked go:systemstack because it must not be preempted\n+// while using a workbuf.\n+//\n+//go:nowritebarrier\n+//go:systemstack\n+func scanstack(gp *g, gcw *gcWork) {\n+\tif gp.gcscanvalid {\n+\t\treturn\n+\t}\n+\n+\tif readgstatus(gp)&_Gscan == 0 {\n+\t\tprint(\"runtime:scanstack: gp=\", gp, \", goid=\", gp.goid, \", gp->atomicstatus=\", hex(readgstatus(gp)), \"\\n\")\n+\t\tthrow(\"scanstack - bad status\")\n+\t}\n+\n+\tswitch readgstatus(gp) &^ _Gscan {\n+\tdefault:\n+\t\tprint(\"runtime: gp=\", gp, \", goid=\", gp.goid, \", gp->atomicstatus=\", readgstatus(gp), \"\\n\")\n+\t\tthrow(\"mark - bad status\")\n+\tcase _Gdead:\n+\t\treturn\n+\tcase _Grunning:\n+\t\t// ok for gccgo, though not for gc.\n+\tcase _Grunnable, _Gsyscall, _Gwaiting:\n+\t\t// ok\n+\t}\n+\n+\tmp := gp.m\n+\tif mp != nil && mp.helpgc != 0 {\n+\t\tthrow(\"can't scan gchelper stack\")\n+\t}\n+\n+\t// Scan the stack.\n+\tdoscanstack(gp, gcw)\n+\n+\t// Conservatively scan the saved register values.\n+\tscanstackblock(uintptr(unsafe.Pointer(&gp.gcregs)), unsafe.Sizeof(gp.gcregs), gcw)\n+\tscanstackblock(uintptr(unsafe.Pointer(&gp.context)), unsafe.Sizeof(gp.context), gcw)\n+\n+\tif gcphase == _GCmark {\n+\t\t// gp may have added itself to the rescan list between\n+\t\t// when GC started and now. It's clean now, so remove\n+\t\t// it. This isn't safe during mark termination because\n+\t\t// mark termination is consuming this list, but it's\n+\t\t// also not necessary.\n+\t\tdequeueRescan(gp)\n+\t}\n+\tgp.gcscanvalid = true\n+}\n+\n+// queueRescan adds gp to the stack rescan list and clears\n+// gp.gcscanvalid. The caller must own gp and ensure that gp isn't\n+// already on the rescan list.\n+func queueRescan(gp *g) {\n+\tif debug.gcrescanstacks == 0 {\n+\t\t// Clear gcscanvalid to keep assertions happy.\n+\t\t//\n+\t\t// TODO: Remove gcscanvalid entirely when we remove\n+\t\t// stack rescanning.\n+\t\tgp.gcscanvalid = false\n+\t\treturn\n+\t}\n+\n+\tif gcphase == _GCoff {\n+\t\tgp.gcscanvalid = false\n+\t\treturn\n+\t}\n+\tif gp.gcRescan != -1 {\n+\t\tthrow(\"g already on rescan list\")\n+\t}\n+\n+\tlock(&work.rescan.lock)\n+\tgp.gcscanvalid = false\n+\n+\t// Recheck gcphase under the lock in case there was a phase change.\n+\tif gcphase == _GCoff {\n+\t\tunlock(&work.rescan.lock)\n+\t\treturn\n+\t}\n+\tif len(work.rescan.list) == cap(work.rescan.list) {\n+\t\tthrow(\"rescan list overflow\")\n+\t}\n+\tn := len(work.rescan.list)\n+\tgp.gcRescan = int32(n)\n+\twork.rescan.list = work.rescan.list[:n+1]\n+\twork.rescan.list[n].set(gp)\n+\tunlock(&work.rescan.lock)\n+}\n+\n+// dequeueRescan removes gp from the stack rescan list, if gp is on\n+// the rescan list. The caller must own gp.\n+func dequeueRescan(gp *g) {\n+\tif debug.gcrescanstacks == 0 {\n+\t\treturn\n+\t}\n+\n+\tif gp.gcRescan == -1 {\n+\t\treturn\n+\t}\n+\tif gcphase == _GCoff {\n+\t\tgp.gcRescan = -1\n+\t\treturn\n+\t}\n+\n+\tlock(&work.rescan.lock)\n+\tif work.rescan.list[gp.gcRescan].ptr() != gp {\n+\t\tthrow(\"bad dequeueRescan\")\n+\t}\n+\t// Careful: gp may itself be the last G on the list.\n+\tlast := work.rescan.list[len(work.rescan.list)-1]\n+\twork.rescan.list[gp.gcRescan] = last\n+\tlast.ptr().gcRescan = gp.gcRescan\n+\tgp.gcRescan = -1\n+\twork.rescan.list = work.rescan.list[:len(work.rescan.list)-1]\n+\tunlock(&work.rescan.lock)\n+}\n+\n+type gcDrainFlags int\n+\n+const (\n+\tgcDrainUntilPreempt gcDrainFlags = 1 << iota\n+\tgcDrainNoBlock\n+\tgcDrainFlushBgCredit\n+\tgcDrainIdle\n+\n+\t// gcDrainBlock means neither gcDrainUntilPreempt or\n+\t// gcDrainNoBlock. It is the default, but callers should use\n+\t// the constant for documentation purposes.\n+\tgcDrainBlock gcDrainFlags = 0\n+)\n+\n+// gcDrain scans roots and objects in work buffers, blackening grey\n+// objects until all roots and work buffers have been drained.\n+//\n+// If flags&gcDrainUntilPreempt != 0, gcDrain returns when g.preempt\n+// is set. This implies gcDrainNoBlock.\n+//\n+// If flags&gcDrainIdle != 0, gcDrain returns when there is other work\n+// to do. This implies gcDrainNoBlock.\n+//\n+// If flags&gcDrainNoBlock != 0, gcDrain returns as soon as it is\n+// unable to get more work. Otherwise, it will block until all\n+// blocking calls are blocked in gcDrain.\n+//\n+// If flags&gcDrainFlushBgCredit != 0, gcDrain flushes scan work\n+// credit to gcController.bgScanCredit every gcCreditSlack units of\n+// scan work.\n+//\n+//go:nowritebarrier\n+func gcDrain(gcw *gcWork, flags gcDrainFlags) {\n+\tif !writeBarrier.needed {\n+\t\tthrow(\"gcDrain phase incorrect\")\n+\t}\n+\n+\tgp := getg().m.curg\n+\tpreemptible := flags&gcDrainUntilPreempt != 0\n+\tblocking := flags&(gcDrainUntilPreempt|gcDrainIdle|gcDrainNoBlock) == 0\n+\tflushBgCredit := flags&gcDrainFlushBgCredit != 0\n+\tidle := flags&gcDrainIdle != 0\n+\n+\tinitScanWork := gcw.scanWork\n+\t// idleCheck is the scan work at which to perform the next\n+\t// idle check with the scheduler.\n+\tidleCheck := initScanWork + idleCheckThreshold\n+\n+\t// Drain root marking jobs.\n+\tif work.markrootNext < work.markrootJobs {\n+\t\tfor !(preemptible && gp.preempt) {\n+\t\t\tjob := atomic.Xadd(&work.markrootNext, +1) - 1\n+\t\t\tif job >= work.markrootJobs {\n+\t\t\t\tbreak\n+\t\t\t}\n+\t\t\tmarkroot(gcw, job)\n+\t\t\tif idle && pollWork() {\n+\t\t\t\tgoto done\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Drain heap marking jobs.\n+\tfor !(preemptible && gp.preempt) {\n+\t\t// Try to keep work available on the global queue. We used to\n+\t\t// check if there were waiting workers, but it's better to\n+\t\t// just keep work available than to make workers wait. In the\n+\t\t// worst case, we'll do O(log(_WorkbufSize)) unnecessary\n+\t\t// balances.\n+\t\tif work.full == 0 {\n+\t\t\tgcw.balance()\n+\t\t}\n+\n+\t\tvar b uintptr\n+\t\tif blocking {\n+\t\t\tb = gcw.get()\n+\t\t} else {\n+\t\t\tb = gcw.tryGetFast()\n+\t\t\tif b == 0 {\n+\t\t\t\tb = gcw.tryGet()\n+\t\t\t}\n+\t\t}\n+\t\tif b == 0 {\n+\t\t\t// work barrier reached or tryGet failed.\n+\t\t\tbreak\n+\t\t}\n+\t\tscanobject(b, gcw)\n+\n+\t\t// Flush background scan work credit to the global\n+\t\t// account if we've accumulated enough locally so\n+\t\t// mutator assists can draw on it.\n+\t\tif gcw.scanWork >= gcCreditSlack {\n+\t\t\tatomic.Xaddint64(&gcController.scanWork, gcw.scanWork)\n+\t\t\tif flushBgCredit {\n+\t\t\t\tgcFlushBgCredit(gcw.scanWork - initScanWork)\n+\t\t\t\tinitScanWork = 0\n+\t\t\t}\n+\t\t\tidleCheck -= gcw.scanWork\n+\t\t\tgcw.scanWork = 0\n+\n+\t\t\tif idle && idleCheck <= 0 {\n+\t\t\t\tidleCheck += idleCheckThreshold\n+\t\t\t\tif pollWork() {\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// In blocking mode, write barriers are not allowed after this\n+\t// point because we must preserve the condition that the work\n+\t// buffers are empty.\n+\n+done:\n+\t// Flush remaining scan work credit.\n+\tif gcw.scanWork > 0 {\n+\t\tatomic.Xaddint64(&gcController.scanWork, gcw.scanWork)\n+\t\tif flushBgCredit {\n+\t\t\tgcFlushBgCredit(gcw.scanWork - initScanWork)\n+\t\t}\n+\t\tgcw.scanWork = 0\n+\t}\n+}\n+\n+// gcDrainN blackens grey objects until it has performed roughly\n+// scanWork units of scan work or the G is preempted. This is\n+// best-effort, so it may perform less work if it fails to get a work\n+// buffer. Otherwise, it will perform at least n units of work, but\n+// may perform more because scanning is always done in whole object\n+// increments. It returns the amount of scan work performed.\n+//\n+// The caller goroutine must be in a preemptible state (e.g.,\n+// _Gwaiting) to prevent deadlocks during stack scanning. As a\n+// consequence, this must be called on the system stack.\n+//\n+//go:nowritebarrier\n+//go:systemstack\n+func gcDrainN(gcw *gcWork, scanWork int64) int64 {\n+\tif !writeBarrier.needed {\n+\t\tthrow(\"gcDrainN phase incorrect\")\n+\t}\n+\n+\t// There may already be scan work on the gcw, which we don't\n+\t// want to claim was done by this call.\n+\tworkFlushed := -gcw.scanWork\n+\n+\tgp := getg().m.curg\n+\tfor !gp.preempt && workFlushed+gcw.scanWork < scanWork {\n+\t\t// See gcDrain comment.\n+\t\tif work.full == 0 {\n+\t\t\tgcw.balance()\n+\t\t}\n+\n+\t\t// This might be a good place to add prefetch code...\n+\t\t// if(wbuf.nobj > 4) {\n+\t\t//         PREFETCH(wbuf->obj[wbuf.nobj - 3];\n+\t\t//  }\n+\t\t//\n+\t\tb := gcw.tryGetFast()\n+\t\tif b == 0 {\n+\t\t\tb = gcw.tryGet()\n+\t\t}\n+\n+\t\tif b == 0 {\n+\t\t\t// Try to do a root job.\n+\t\t\t//\n+\t\t\t// TODO: Assists should get credit for this\n+\t\t\t// work.\n+\t\t\tif work.markrootNext < work.markrootJobs {\n+\t\t\t\tjob := atomic.Xadd(&work.markrootNext, +1) - 1\n+\t\t\t\tif job < work.markrootJobs {\n+\t\t\t\t\tmarkroot(gcw, job)\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\t// No heap or root jobs.\n+\t\t\tbreak\n+\t\t}\n+\t\tscanobject(b, gcw)\n+\n+\t\t// Flush background scan work credit.\n+\t\tif gcw.scanWork >= gcCreditSlack {\n+\t\t\tatomic.Xaddint64(&gcController.scanWork, gcw.scanWork)\n+\t\t\tworkFlushed += gcw.scanWork\n+\t\t\tgcw.scanWork = 0\n+\t\t}\n+\t}\n+\n+\t// Unlike gcDrain, there's no need to flush remaining work\n+\t// here because this never flushes to bgScanCredit and\n+\t// gcw.dispose will flush any remaining work to scanWork.\n+\n+\treturn workFlushed + gcw.scanWork\n+}\n+\n+// scanblock scans b as scanobject would, but using an explicit\n+// pointer bitmap instead of the heap bitmap.\n+//\n+// This is used to scan non-heap roots, so it does not update\n+// gcw.bytesMarked or gcw.scanWork.\n+//\n+//go:nowritebarrier\n+func scanblock(b0, n0 uintptr, ptrmask *uint8, gcw *gcWork) {\n+\t// Use local copies of original parameters, so that a stack trace\n+\t// due to one of the throws below shows the original block\n+\t// base and extent.\n+\tb := b0\n+\tn := n0\n+\n+\tarena_start := mheap_.arena_start\n+\tarena_used := mheap_.arena_used\n+\n+\tfor i := uintptr(0); i < n; {\n+\t\t// Find bits for the next word.\n+\t\tbits := uint32(*addb(ptrmask, i/(sys.PtrSize*8)))\n+\t\tif bits == 0 {\n+\t\t\ti += sys.PtrSize * 8\n+\t\t\tcontinue\n+\t\t}\n+\t\tfor j := 0; j < 8 && i < n; j++ {\n+\t\t\tif bits&1 != 0 {\n+\t\t\t\t// Same work as in scanobject; see comments there.\n+\t\t\t\tobj := *(*uintptr)(unsafe.Pointer(b + i))\n+\t\t\t\tif obj != 0 && arena_start <= obj && obj < arena_used {\n+\t\t\t\t\tif obj, hbits, span, objIndex := heapBitsForObject(obj, b, i, false); obj != 0 {\n+\t\t\t\t\t\tgreyobject(obj, b, i, hbits, span, gcw, objIndex, false)\n+\t\t\t\t\t}\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tbits >>= 1\n+\t\t\ti += sys.PtrSize\n+\t\t}\n+\t}\n+}\n+\n+// scanobject scans the object starting at b, adding pointers to gcw.\n+// b must point to the beginning of a heap object or an oblet.\n+// scanobject consults the GC bitmap for the pointer mask and the\n+// spans for the size of the object.\n+//\n+//go:nowritebarrier\n+func scanobject(b uintptr, gcw *gcWork) {\n+\t// Note that arena_used may change concurrently during\n+\t// scanobject and hence scanobject may encounter a pointer to\n+\t// a newly allocated heap object that is *not* in\n+\t// [start,used). It will not mark this object; however, we\n+\t// know that it was just installed by a mutator, which means\n+\t// that mutator will execute a write barrier and take care of\n+\t// marking it. This is even more pronounced on relaxed memory\n+\t// architectures since we access arena_used without barriers\n+\t// or synchronization, but the same logic applies.\n+\tarena_start := mheap_.arena_start\n+\tarena_used := mheap_.arena_used\n+\n+\t// Find the bits for b and the size of the object at b.\n+\t//\n+\t// b is either the beginning of an object, in which case this\n+\t// is the size of the object to scan, or it points to an\n+\t// oblet, in which case we compute the size to scan below.\n+\thbits := heapBitsForAddr(b)\n+\ts := spanOfUnchecked(b)\n+\tn := s.elemsize\n+\tif n == 0 {\n+\t\tthrow(\"scanobject n == 0\")\n+\t}\n+\n+\tif n > maxObletBytes {\n+\t\t// Large object. Break into oblets for better\n+\t\t// parallelism and lower latency.\n+\t\tif b == s.base() {\n+\t\t\t// It's possible this is a noscan object (not\n+\t\t\t// from greyobject, but from other code\n+\t\t\t// paths), in which case we must *not* enqueue\n+\t\t\t// oblets since their bitmaps will be\n+\t\t\t// uninitialized.\n+\t\t\tif !hbits.hasPointers(n) {\n+\t\t\t\t// Bypass the whole scan.\n+\t\t\t\tgcw.bytesMarked += uint64(n)\n+\t\t\t\treturn\n+\t\t\t}\n+\n+\t\t\t// Enqueue the other oblets to scan later.\n+\t\t\t// Some oblets may be in b's scalar tail, but\n+\t\t\t// these will be marked as \"no more pointers\",\n+\t\t\t// so we'll drop out immediately when we go to\n+\t\t\t// scan those.\n+\t\t\tfor oblet := b + maxObletBytes; oblet < s.base()+s.elemsize; oblet += maxObletBytes {\n+\t\t\t\tif !gcw.putFast(oblet) {\n+\t\t\t\t\tgcw.put(oblet)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Compute the size of the oblet. Since this object\n+\t\t// must be a large object, s.base() is the beginning\n+\t\t// of the object.\n+\t\tn = s.base() + s.elemsize - b\n+\t\tif n > maxObletBytes {\n+\t\t\tn = maxObletBytes\n+\t\t}\n+\t}\n+\n+\tvar i uintptr\n+\tfor i = 0; i < n; i += sys.PtrSize {\n+\t\t// Find bits for this word.\n+\t\tif i != 0 {\n+\t\t\t// Avoid needless hbits.next() on last iteration.\n+\t\t\thbits = hbits.next()\n+\t\t}\n+\t\t// Load bits once. See CL 22712 and issue 16973 for discussion.\n+\t\tbits := hbits.bits()\n+\t\t// During checkmarking, 1-word objects store the checkmark\n+\t\t// in the type bit for the one word. The only one-word objects\n+\t\t// are pointers, or else they'd be merged with other non-pointer\n+\t\t// data into larger allocations.\n+\t\tif i != 1*sys.PtrSize && bits&bitScan == 0 {\n+\t\t\tbreak // no more pointers in this object\n+\t\t}\n+\t\tif bits&bitPointer == 0 {\n+\t\t\tcontinue // not a pointer\n+\t\t}\n+\n+\t\t// Work here is duplicated in scanblock and above.\n+\t\t// If you make changes here, make changes there too.\n+\t\tobj := *(*uintptr)(unsafe.Pointer(b + i))\n+\n+\t\t// At this point we have extracted the next potential pointer.\n+\t\t// Check if it points into heap and not back at the current object.\n+\t\tif obj != 0 && arena_start <= obj && obj < arena_used && obj-b >= n {\n+\t\t\t// Mark the object.\n+\t\t\tif obj, hbits, span, objIndex := heapBitsForObject(obj, b, i, false); obj != 0 {\n+\t\t\t\tgreyobject(obj, b, i, hbits, span, gcw, objIndex, false)\n+\t\t\t}\n+\t\t}\n+\t}\n+\tgcw.bytesMarked += uint64(n)\n+\tgcw.scanWork += int64(i)\n+}\n+\n+//go:linkname scanstackblock runtime.scanstackblock\n+\n+// scanstackblock is called by the stack scanning code in C to\n+// actually find and mark pointers in the stack block. This is like\n+// scanblock, but we scan the stack conservatively, so there is no\n+// bitmask of pointers.\n+func scanstackblock(b, n uintptr, gcw *gcWork) {\n+\tarena_start := mheap_.arena_start\n+\tarena_used := mheap_.arena_used\n+\n+\tfor i := uintptr(0); i < n; i += sys.PtrSize {\n+\t\t// Same work as in scanobject; see comments there.\n+\t\tobj := *(*uintptr)(unsafe.Pointer(b + i))\n+\t\tif obj != 0 && arena_start <= obj && obj < arena_used {\n+\t\t\tif obj, hbits, span, objIndex := heapBitsForObject(obj, b, i, true); obj != 0 {\n+\t\t\t\tgreyobject(obj, b, i, hbits, span, gcw, objIndex, true)\n+\t\t\t}\n+\t\t}\n+\t}\n+}\n+\n+// Shade the object if it isn't already.\n+// The object is not nil and known to be in the heap.\n+// Preemption must be disabled.\n+//go:nowritebarrier\n+func shade(b uintptr) {\n+\t// shade can be called to shade a pointer found on the stack,\n+\t// so pass forStack as true to heapBitsForObject and greyobject.\n+\tif obj, hbits, span, objIndex := heapBitsForObject(b, 0, 0, true); obj != 0 {\n+\t\tgcw := &getg().m.p.ptr().gcw\n+\t\tgreyobject(obj, 0, 0, hbits, span, gcw, objIndex, true)\n+\t\tif gcphase == _GCmarktermination || gcBlackenPromptly {\n+\t\t\t// Ps aren't allowed to cache work during mark\n+\t\t\t// termination.\n+\t\t\tgcw.dispose()\n+\t\t}\n+\t}\n+}\n+\n+// obj is the start of an object with mark mbits.\n+// If it isn't already marked, mark it and enqueue into gcw.\n+// base and off are for debugging only and could be removed.\n+//go:nowritebarrierrec\n+func greyobject(obj, base, off uintptr, hbits heapBits, span *mspan, gcw *gcWork, objIndex uintptr, forStack bool) {\n+\t// obj should be start of allocation, and so must be at least pointer-aligned.\n+\tif obj&(sys.PtrSize-1) != 0 {\n+\t\tthrow(\"greyobject: obj not pointer-aligned\")\n+\t}\n+\tmbits := span.markBitsForIndex(objIndex)\n+\n+\tif useCheckmark {\n+\t\tif !mbits.isMarked() {\n+\t\t\t// Stack scanning is conservative, so we can\n+\t\t\t// see a reference to an object not previously\n+\t\t\t// found. Assume the object was correctly not\n+\t\t\t// marked and ignore the pointer.\n+\t\t\tif forStack {\n+\t\t\t\treturn\n+\t\t\t}\n+\t\t\tprintlock()\n+\t\t\tprint(\"runtime:greyobject: checkmarks finds unexpected unmarked object obj=\", hex(obj), \"\\n\")\n+\t\t\tprint(\"runtime: found obj at *(\", hex(base), \"+\", hex(off), \")\\n\")\n+\n+\t\t\t// Dump the source (base) object\n+\t\t\tgcDumpObject(\"base\", base, off)\n+\n+\t\t\t// Dump the object\n+\t\t\tgcDumpObject(\"obj\", obj, ^uintptr(0))\n+\n+\t\t\tthrow(\"checkmark found unmarked object\")\n+\t\t}\n+\t\tif hbits.isCheckmarked(span.elemsize) {\n+\t\t\treturn\n+\t\t}\n+\t\thbits.setCheckmarked(span.elemsize)\n+\t\tif !hbits.isCheckmarked(span.elemsize) {\n+\t\t\tthrow(\"setCheckmarked and isCheckmarked disagree\")\n+\t\t}\n+\t} else {\n+\t\t// Stack scanning is conservative, so we can see a\n+\t\t// pointer to a free object. Assume the object was\n+\t\t// correctly freed and we must ignore the pointer.\n+\t\tif forStack && span.isFree(objIndex) {\n+\t\t\treturn\n+\t\t}\n+\n+\t\tif debug.gccheckmark > 0 && span.isFree(objIndex) {\n+\t\t\tprint(\"runtime: marking free object \", hex(obj), \" found at *(\", hex(base), \"+\", hex(off), \")\\n\")\n+\t\t\tgcDumpObject(\"base\", base, off)\n+\t\t\tgcDumpObject(\"obj\", obj, ^uintptr(0))\n+\t\t\tthrow(\"marking free object\")\n+\t\t}\n+\n+\t\t// If marked we have nothing to do.\n+\t\tif mbits.isMarked() {\n+\t\t\treturn\n+\t\t}\n+\t\t// mbits.setMarked() // Avoid extra call overhead with manual inlining.\n+\t\tatomic.Or8(mbits.bytep, mbits.mask)\n+\t\t// If this is a noscan object, fast-track it to black\n+\t\t// instead of greying it.\n+\t\tif !hbits.hasPointers(span.elemsize) {\n+\t\t\tgcw.bytesMarked += uint64(span.elemsize)\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\t// Queue the obj for scanning. The PREFETCH(obj) logic has been removed but\n+\t// seems like a nice optimization that can be added back in.\n+\t// There needs to be time between the PREFETCH and the use.\n+\t// Previously we put the obj in an 8 element buffer that is drained at a rate\n+\t// to give the PREFETCH time to do its work.\n+\t// Use of PREFETCHNTA might be more appropriate than PREFETCH\n+\tif !gcw.putFast(obj) {\n+\t\tgcw.put(obj)\n+\t}\n+}\n+\n+// gcDumpObject dumps the contents of obj for debugging and marks the\n+// field at byte offset off in obj.\n+func gcDumpObject(label string, obj, off uintptr) {\n+\tif obj < mheap_.arena_start || obj >= mheap_.arena_used {\n+\t\tprint(label, \"=\", hex(obj), \" is not in the Go heap\\n\")\n+\t\treturn\n+\t}\n+\tk := obj >> _PageShift\n+\tx := k\n+\tx -= mheap_.arena_start >> _PageShift\n+\ts := mheap_.spans[x]\n+\tprint(label, \"=\", hex(obj), \" k=\", hex(k))\n+\tif s == nil {\n+\t\tprint(\" s=nil\\n\")\n+\t\treturn\n+\t}\n+\tprint(\" s.base()=\", hex(s.base()), \" s.limit=\", hex(s.limit), \" s.sizeclass=\", s.sizeclass, \" s.elemsize=\", s.elemsize, \" s.state=\")\n+\tif 0 <= s.state && int(s.state) < len(mSpanStateNames) {\n+\t\tprint(mSpanStateNames[s.state], \"\\n\")\n+\t} else {\n+\t\tprint(\"unknown(\", s.state, \")\\n\")\n+\t}\n+\n+\tskipped := false\n+\tsize := s.elemsize\n+\tif s.state == _MSpanStack && size == 0 {\n+\t\t// We're printing something from a stack frame. We\n+\t\t// don't know how big it is, so just show up to an\n+\t\t// including off.\n+\t\tsize = off + sys.PtrSize\n+\t}\n+\tfor i := uintptr(0); i < size; i += sys.PtrSize {\n+\t\t// For big objects, just print the beginning (because\n+\t\t// that usually hints at the object's type) and the\n+\t\t// fields around off.\n+\t\tif !(i < 128*sys.PtrSize || off-16*sys.PtrSize < i && i < off+16*sys.PtrSize) {\n+\t\t\tskipped = true\n+\t\t\tcontinue\n+\t\t}\n+\t\tif skipped {\n+\t\t\tprint(\" ...\\n\")\n+\t\t\tskipped = false\n+\t\t}\n+\t\tprint(\" *(\", label, \"+\", i, \") = \", hex(*(*uintptr)(unsafe.Pointer(obj + i))))\n+\t\tif i == off {\n+\t\t\tprint(\" <==\")\n+\t\t}\n+\t\tprint(\"\\n\")\n+\t}\n+\tif skipped {\n+\t\tprint(\" ...\\n\")\n+\t}\n+}\n+\n+// gcmarknewobject marks a newly allocated object black. obj must\n+// not contain any non-nil pointers.\n+//\n+// This is nosplit so it can manipulate a gcWork without preemption.\n+//\n+//go:nowritebarrier\n+//go:nosplit\n+func gcmarknewobject(obj, size, scanSize uintptr) {\n+\tif useCheckmark && !gcBlackenPromptly { // The world should be stopped so this should not happen.\n+\t\tthrow(\"gcmarknewobject called while doing checkmark\")\n+\t}\n+\tmarkBitsForAddr(obj).setMarked()\n+\tgcw := &getg().m.p.ptr().gcw\n+\tgcw.bytesMarked += uint64(size)\n+\tgcw.scanWork += int64(scanSize)\n+\tif gcBlackenPromptly {\n+\t\t// There shouldn't be anything in the work queue, but\n+\t\t// we still need to flush stats.\n+\t\tgcw.dispose()\n+\t}\n+}\n+\n+// gcMarkTinyAllocs greys all active tiny alloc blocks.\n+//\n+// The world must be stopped.\n+func gcMarkTinyAllocs() {\n+\tfor _, p := range &allp {\n+\t\tif p == nil || p.status == _Pdead {\n+\t\t\tbreak\n+\t\t}\n+\t\tc := p.mcache\n+\t\tif c == nil || c.tiny == 0 {\n+\t\t\tcontinue\n+\t\t}\n+\t\t_, hbits, span, objIndex := heapBitsForObject(c.tiny, 0, 0, false)\n+\t\tgcw := &p.gcw\n+\t\tgreyobject(c.tiny, 0, 0, hbits, span, gcw, objIndex, false)\n+\t\tif gcBlackenPromptly {\n+\t\t\tgcw.dispose()\n+\t\t}\n+\t}\n+}\n+\n+// Checkmarking\n+\n+// To help debug the concurrent GC we remark with the world\n+// stopped ensuring that any object encountered has their normal\n+// mark bit set. To do this we use an orthogonal bit\n+// pattern to indicate the object is marked. The following pattern\n+// uses the upper two bits in the object's boundary nibble.\n+// 01: scalar  not marked\n+// 10: pointer not marked\n+// 11: pointer     marked\n+// 00: scalar      marked\n+// Xoring with 01 will flip the pattern from marked to unmarked and vica versa.\n+// The higher bit is 1 for pointers and 0 for scalars, whether the object\n+// is marked or not.\n+// The first nibble no longer holds the typeDead pattern indicating that the\n+// there are no more pointers in the object. This information is held\n+// in the second nibble.\n+\n+// If useCheckmark is true, marking of an object uses the\n+// checkmark bits (encoding above) instead of the standard\n+// mark bits.\n+var useCheckmark = false\n+\n+//go:nowritebarrier\n+func initCheckmarks() {\n+\tuseCheckmark = true\n+\tfor _, s := range mheap_.allspans {\n+\t\tif s.state == _MSpanInUse {\n+\t\t\theapBitsForSpan(s.base()).initCheckmarkSpan(s.layout())\n+\t\t}\n+\t}\n+}\n+\n+func clearCheckmarks() {\n+\tuseCheckmark = false\n+\tfor _, s := range mheap_.allspans {\n+\t\tif s.state == _MSpanInUse {\n+\t\t\theapBitsForSpan(s.base()).clearCheckmarkSpan(s.layout())\n+\t\t}\n+\t}\n+}"}, {"sha": "9f24fb1b628340fb6842c09f72caa91860d127ff", "filename": "libgo/go/runtime/mgcsweep.go", "status": "added", "additions": 428, "deletions": 0, "changes": 428, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmgcsweep.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmgcsweep.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmgcsweep.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,428 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Garbage collector: sweeping\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/atomic\"\n+\t\"unsafe\"\n+)\n+\n+var sweep sweepdata\n+\n+// State of background sweep.\n+type sweepdata struct {\n+\tlock    mutex\n+\tg       *g\n+\tparked  bool\n+\tstarted bool\n+\n+\tnbgsweep    uint32\n+\tnpausesweep uint32\n+\n+\t// pacertracegen is the sweepgen at which the last pacer trace\n+\t// \"sweep finished\" message was printed.\n+\tpacertracegen uint32\n+}\n+\n+// finishsweep_m ensures that all spans are swept.\n+//\n+// The world must be stopped. This ensures there are no sweeps in\n+// progress.\n+//\n+//go:nowritebarrier\n+func finishsweep_m() {\n+\t// Sweeping must be complete before marking commences, so\n+\t// sweep any unswept spans. If this is a concurrent GC, there\n+\t// shouldn't be any spans left to sweep, so this should finish\n+\t// instantly. If GC was forced before the concurrent sweep\n+\t// finished, there may be spans to sweep.\n+\tfor sweepone() != ^uintptr(0) {\n+\t\tsweep.npausesweep++\n+\t}\n+\n+\tnextMarkBitArenaEpoch()\n+}\n+\n+func bgsweep(c chan int) {\n+\tsweep.g = getg()\n+\n+\tlock(&sweep.lock)\n+\tsweep.parked = true\n+\tc <- 1\n+\tgoparkunlock(&sweep.lock, \"GC sweep wait\", traceEvGoBlock, 1)\n+\n+\tfor {\n+\t\tfor gosweepone() != ^uintptr(0) {\n+\t\t\tsweep.nbgsweep++\n+\t\t\tGosched()\n+\t\t}\n+\t\tlock(&sweep.lock)\n+\t\tif !gosweepdone() {\n+\t\t\t// This can happen if a GC runs between\n+\t\t\t// gosweepone returning ^0 above\n+\t\t\t// and the lock being acquired.\n+\t\t\tunlock(&sweep.lock)\n+\t\t\tcontinue\n+\t\t}\n+\t\tsweep.parked = true\n+\t\tgoparkunlock(&sweep.lock, \"GC sweep wait\", traceEvGoBlock, 1)\n+\t}\n+}\n+\n+// sweeps one span\n+// returns number of pages returned to heap, or ^uintptr(0) if there is nothing to sweep\n+//go:nowritebarrier\n+func sweepone() uintptr {\n+\t_g_ := getg()\n+\n+\t// increment locks to ensure that the goroutine is not preempted\n+\t// in the middle of sweep thus leaving the span in an inconsistent state for next GC\n+\t_g_.m.locks++\n+\tsg := mheap_.sweepgen\n+\tfor {\n+\t\ts := mheap_.sweepSpans[1-sg/2%2].pop()\n+\t\tif s == nil {\n+\t\t\tmheap_.sweepdone = 1\n+\t\t\t_g_.m.locks--\n+\t\t\tif debug.gcpacertrace > 0 && atomic.Cas(&sweep.pacertracegen, sg-2, sg) {\n+\t\t\t\tprint(\"pacer: sweep done at heap size \", memstats.heap_live>>20, \"MB; allocated \", mheap_.spanBytesAlloc>>20, \"MB of spans; swept \", mheap_.pagesSwept, \" pages at \", mheap_.sweepPagesPerByte, \" pages/byte\\n\")\n+\t\t\t}\n+\t\t\treturn ^uintptr(0)\n+\t\t}\n+\t\tif s.state != mSpanInUse {\n+\t\t\t// This can happen if direct sweeping already\n+\t\t\t// swept this span, but in that case the sweep\n+\t\t\t// generation should always be up-to-date.\n+\t\t\tif s.sweepgen != sg {\n+\t\t\t\tprint(\"runtime: bad span s.state=\", s.state, \" s.sweepgen=\", s.sweepgen, \" sweepgen=\", sg, \"\\n\")\n+\t\t\t\tthrow(\"non in-use span in unswept list\")\n+\t\t\t}\n+\t\t\tcontinue\n+\t\t}\n+\t\tif s.sweepgen != sg-2 || !atomic.Cas(&s.sweepgen, sg-2, sg-1) {\n+\t\t\tcontinue\n+\t\t}\n+\t\tnpages := s.npages\n+\t\tif !s.sweep(false) {\n+\t\t\t// Span is still in-use, so this returned no\n+\t\t\t// pages to the heap and the span needs to\n+\t\t\t// move to the swept in-use list.\n+\t\t\tnpages = 0\n+\t\t}\n+\t\t_g_.m.locks--\n+\t\treturn npages\n+\t}\n+}\n+\n+//go:nowritebarrier\n+func gosweepone() uintptr {\n+\tvar ret uintptr\n+\tsystemstack(func() {\n+\t\tret = sweepone()\n+\t})\n+\treturn ret\n+}\n+\n+//go:nowritebarrier\n+func gosweepdone() bool {\n+\treturn mheap_.sweepdone != 0\n+}\n+\n+// Returns only when span s has been swept.\n+//go:nowritebarrier\n+func (s *mspan) ensureSwept() {\n+\t// Caller must disable preemption.\n+\t// Otherwise when this function returns the span can become unswept again\n+\t// (if GC is triggered on another goroutine).\n+\t_g_ := getg()\n+\tif _g_.m.locks == 0 && _g_.m.mallocing == 0 && _g_ != _g_.m.g0 {\n+\t\tthrow(\"MSpan_EnsureSwept: m is not locked\")\n+\t}\n+\n+\tsg := mheap_.sweepgen\n+\tif atomic.Load(&s.sweepgen) == sg {\n+\t\treturn\n+\t}\n+\t// The caller must be sure that the span is a MSpanInUse span.\n+\tif atomic.Cas(&s.sweepgen, sg-2, sg-1) {\n+\t\ts.sweep(false)\n+\t\treturn\n+\t}\n+\t// unfortunate condition, and we don't have efficient means to wait\n+\tfor atomic.Load(&s.sweepgen) != sg {\n+\t\tosyield()\n+\t}\n+}\n+\n+// Sweep frees or collects finalizers for blocks not marked in the mark phase.\n+// It clears the mark bits in preparation for the next GC round.\n+// Returns true if the span was returned to heap.\n+// If preserve=true, don't return it to heap nor relink in MCentral lists;\n+// caller takes care of it.\n+//TODO go:nowritebarrier\n+func (s *mspan) sweep(preserve bool) bool {\n+\t// It's critical that we enter this function with preemption disabled,\n+\t// GC must not start while we are in the middle of this function.\n+\t_g_ := getg()\n+\tif _g_.m.locks == 0 && _g_.m.mallocing == 0 && _g_ != _g_.m.g0 {\n+\t\tthrow(\"MSpan_Sweep: m is not locked\")\n+\t}\n+\tsweepgen := mheap_.sweepgen\n+\tif s.state != mSpanInUse || s.sweepgen != sweepgen-1 {\n+\t\tprint(\"MSpan_Sweep: state=\", s.state, \" sweepgen=\", s.sweepgen, \" mheap.sweepgen=\", sweepgen, \"\\n\")\n+\t\tthrow(\"MSpan_Sweep: bad span state\")\n+\t}\n+\n+\tif trace.enabled {\n+\t\ttraceGCSweepStart()\n+\t}\n+\n+\tatomic.Xadd64(&mheap_.pagesSwept, int64(s.npages))\n+\n+\tcl := s.sizeclass\n+\tsize := s.elemsize\n+\tres := false\n+\tnfree := 0\n+\n+\tc := _g_.m.mcache\n+\tfreeToHeap := false\n+\n+\t// The allocBits indicate which unmarked objects don't need to be\n+\t// processed since they were free at the end of the last GC cycle\n+\t// and were not allocated since then.\n+\t// If the allocBits index is >= s.freeindex and the bit\n+\t// is not marked then the object remains unallocated\n+\t// since the last GC.\n+\t// This situation is analogous to being on a freelist.\n+\n+\t// Unlink & free special records for any objects we're about to free.\n+\t// Two complications here:\n+\t// 1. An object can have both finalizer and profile special records.\n+\t//    In such case we need to queue finalizer for execution,\n+\t//    mark the object as live and preserve the profile special.\n+\t// 2. A tiny object can have several finalizers setup for different offsets.\n+\t//    If such object is not marked, we need to queue all finalizers at once.\n+\t// Both 1 and 2 are possible at the same time.\n+\tspecialp := &s.specials\n+\tspecial := *specialp\n+\tfor special != nil {\n+\t\t// A finalizer can be set for an inner byte of an object, find object beginning.\n+\t\tobjIndex := uintptr(special.offset) / size\n+\t\tp := s.base() + objIndex*size\n+\t\tmbits := s.markBitsForIndex(objIndex)\n+\t\tif !mbits.isMarked() {\n+\t\t\t// This object is not marked and has at least one special record.\n+\t\t\t// Pass 1: see if it has at least one finalizer.\n+\t\t\thasFin := false\n+\t\t\tendOffset := p - s.base() + size\n+\t\t\tfor tmp := special; tmp != nil && uintptr(tmp.offset) < endOffset; tmp = tmp.next {\n+\t\t\t\tif tmp.kind == _KindSpecialFinalizer {\n+\t\t\t\t\t// Stop freeing of object if it has a finalizer.\n+\t\t\t\t\tmbits.setMarkedNonAtomic()\n+\t\t\t\t\thasFin = true\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\t// Pass 2: queue all finalizers _or_ handle profile record.\n+\t\t\tfor special != nil && uintptr(special.offset) < endOffset {\n+\t\t\t\t// Find the exact byte for which the special was setup\n+\t\t\t\t// (as opposed to object beginning).\n+\t\t\t\tp := s.base() + uintptr(special.offset)\n+\t\t\t\tif special.kind == _KindSpecialFinalizer || !hasFin {\n+\t\t\t\t\t// Splice out special record.\n+\t\t\t\t\ty := special\n+\t\t\t\t\tspecial = special.next\n+\t\t\t\t\t*specialp = special\n+\t\t\t\t\tfreespecial(y, unsafe.Pointer(p), size)\n+\t\t\t\t} else {\n+\t\t\t\t\t// This is profile record, but the object has finalizers (so kept alive).\n+\t\t\t\t\t// Keep special record.\n+\t\t\t\t\tspecialp = &special.next\n+\t\t\t\t\tspecial = *specialp\n+\t\t\t\t}\n+\t\t\t}\n+\t\t} else {\n+\t\t\t// object is still live: keep special record\n+\t\t\tspecialp = &special.next\n+\t\t\tspecial = *specialp\n+\t\t}\n+\t}\n+\n+\tif debug.allocfreetrace != 0 || raceenabled || msanenabled {\n+\t\t// Find all newly freed objects. This doesn't have to\n+\t\t// efficient; allocfreetrace has massive overhead.\n+\t\tmbits := s.markBitsForBase()\n+\t\tabits := s.allocBitsForIndex(0)\n+\t\tfor i := uintptr(0); i < s.nelems; i++ {\n+\t\t\tif !mbits.isMarked() && (abits.index < s.freeindex || abits.isMarked()) {\n+\t\t\t\tx := s.base() + i*s.elemsize\n+\t\t\t\tif debug.allocfreetrace != 0 {\n+\t\t\t\t\ttracefree(unsafe.Pointer(x), size)\n+\t\t\t\t}\n+\t\t\t\tif raceenabled {\n+\t\t\t\t\tracefree(unsafe.Pointer(x), size)\n+\t\t\t\t}\n+\t\t\t\tif msanenabled {\n+\t\t\t\t\tmsanfree(unsafe.Pointer(x), size)\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tmbits.advance()\n+\t\t\tabits.advance()\n+\t\t}\n+\t}\n+\n+\t// Count the number of free objects in this span.\n+\tnfree = s.countFree()\n+\tif cl == 0 && nfree != 0 {\n+\t\ts.needzero = 1\n+\t\tfreeToHeap = true\n+\t}\n+\tnalloc := uint16(s.nelems) - uint16(nfree)\n+\tnfreed := s.allocCount - nalloc\n+\n+\t// This test is not reliable with gccgo, because of\n+\t// conservative stack scanning. The test boils down to\n+\t// checking that no new bits have been set in gcmarkBits since\n+\t// the span was added to the sweep count. New bits are set by\n+\t// greyobject. Seeing a new bit means that a live pointer has\n+\t// appeared that was not found during the mark phase. That can\n+\t// not happen when pointers are followed strictly. However,\n+\t// with conservative checking, it is possible for a pointer\n+\t// that will never be used to appear live and to cause a mark\n+\t// to be added. That is unfortunate in that it causes this\n+\t// check to be inaccurate, and it will keep an object live\n+\t// unnecessarily, but provided the pointer is not really live\n+\t// it is not otherwise a problem. So we disable the test for gccgo.\n+\tif false && nalloc > s.allocCount {\n+\t\tprint(\"runtime: nelems=\", s.nelems, \" nfree=\", nfree, \" nalloc=\", nalloc, \" previous allocCount=\", s.allocCount, \" nfreed=\", nfreed, \"\\n\")\n+\t\tthrow(\"sweep increased allocation count\")\n+\t}\n+\n+\ts.allocCount = nalloc\n+\twasempty := s.nextFreeIndex() == s.nelems\n+\ts.freeindex = 0 // reset allocation index to start of span.\n+\n+\t// gcmarkBits becomes the allocBits.\n+\t// get a fresh cleared gcmarkBits in preparation for next GC\n+\ts.allocBits = s.gcmarkBits\n+\ts.gcmarkBits = newMarkBits(s.nelems)\n+\n+\t// Initialize alloc bits cache.\n+\ts.refillAllocCache(0)\n+\n+\t// We need to set s.sweepgen = h.sweepgen only when all blocks are swept,\n+\t// because of the potential for a concurrent free/SetFinalizer.\n+\t// But we need to set it before we make the span available for allocation\n+\t// (return it to heap or mcentral), because allocation code assumes that a\n+\t// span is already swept if available for allocation.\n+\tif freeToHeap || nfreed == 0 {\n+\t\t// The span must be in our exclusive ownership until we update sweepgen,\n+\t\t// check for potential races.\n+\t\tif s.state != mSpanInUse || s.sweepgen != sweepgen-1 {\n+\t\t\tprint(\"MSpan_Sweep: state=\", s.state, \" sweepgen=\", s.sweepgen, \" mheap.sweepgen=\", sweepgen, \"\\n\")\n+\t\t\tthrow(\"MSpan_Sweep: bad span state after sweep\")\n+\t\t}\n+\t\t// Serialization point.\n+\t\t// At this point the mark bits are cleared and allocation ready\n+\t\t// to go so release the span.\n+\t\tatomic.Store(&s.sweepgen, sweepgen)\n+\t}\n+\n+\tif nfreed > 0 && cl != 0 {\n+\t\tc.local_nsmallfree[cl] += uintptr(nfreed)\n+\t\tres = mheap_.central[cl].mcentral.freeSpan(s, preserve, wasempty)\n+\t\t// MCentral_FreeSpan updates sweepgen\n+\t} else if freeToHeap {\n+\t\t// Free large span to heap\n+\n+\t\t// NOTE(rsc,dvyukov): The original implementation of efence\n+\t\t// in CL 22060046 used SysFree instead of SysFault, so that\n+\t\t// the operating system would eventually give the memory\n+\t\t// back to us again, so that an efence program could run\n+\t\t// longer without running out of memory. Unfortunately,\n+\t\t// calling SysFree here without any kind of adjustment of the\n+\t\t// heap data structures means that when the memory does\n+\t\t// come back to us, we have the wrong metadata for it, either in\n+\t\t// the MSpan structures or in the garbage collection bitmap.\n+\t\t// Using SysFault here means that the program will run out of\n+\t\t// memory fairly quickly in efence mode, but at least it won't\n+\t\t// have mysterious crashes due to confused memory reuse.\n+\t\t// It should be possible to switch back to SysFree if we also\n+\t\t// implement and then call some kind of MHeap_DeleteSpan.\n+\t\tif debug.efence > 0 {\n+\t\t\ts.limit = 0 // prevent mlookup from finding this span\n+\t\t\tsysFault(unsafe.Pointer(s.base()), size)\n+\t\t} else {\n+\t\t\tmheap_.freeSpan(s, 1)\n+\t\t}\n+\t\tc.local_nlargefree++\n+\t\tc.local_largefree += size\n+\t\tres = true\n+\t}\n+\tif !res {\n+\t\t// The span has been swept and is still in-use, so put\n+\t\t// it on the swept in-use list.\n+\t\tmheap_.sweepSpans[sweepgen/2%2].push(s)\n+\t}\n+\tif trace.enabled {\n+\t\ttraceGCSweepDone()\n+\t}\n+\treturn res\n+}\n+\n+// deductSweepCredit deducts sweep credit for allocating a span of\n+// size spanBytes. This must be performed *before* the span is\n+// allocated to ensure the system has enough credit. If necessary, it\n+// performs sweeping to prevent going in to debt. If the caller will\n+// also sweep pages (e.g., for a large allocation), it can pass a\n+// non-zero callerSweepPages to leave that many pages unswept.\n+//\n+// deductSweepCredit makes a worst-case assumption that all spanBytes\n+// bytes of the ultimately allocated span will be available for object\n+// allocation. The caller should call reimburseSweepCredit if that\n+// turns out not to be the case once the span is allocated.\n+//\n+// deductSweepCredit is the core of the \"proportional sweep\" system.\n+// It uses statistics gathered by the garbage collector to perform\n+// enough sweeping so that all pages are swept during the concurrent\n+// sweep phase between GC cycles.\n+//\n+// mheap_ must NOT be locked.\n+func deductSweepCredit(spanBytes uintptr, callerSweepPages uintptr) {\n+\tif mheap_.sweepPagesPerByte == 0 {\n+\t\t// Proportional sweep is done or disabled.\n+\t\treturn\n+\t}\n+\n+\t// Account for this span allocation.\n+\tspanBytesAlloc := atomic.Xadd64(&mheap_.spanBytesAlloc, int64(spanBytes))\n+\n+\t// Fix debt if necessary.\n+\tpagesOwed := int64(mheap_.sweepPagesPerByte * float64(spanBytesAlloc))\n+\tfor pagesOwed-int64(atomic.Load64(&mheap_.pagesSwept)) > int64(callerSweepPages) {\n+\t\tif gosweepone() == ^uintptr(0) {\n+\t\t\tmheap_.sweepPagesPerByte = 0\n+\t\t\tbreak\n+\t\t}\n+\t}\n+}\n+\n+// reimburseSweepCredit records that unusableBytes bytes of a\n+// just-allocated span are not available for object allocation. This\n+// offsets the worst-case charge performed by deductSweepCredit.\n+func reimburseSweepCredit(unusableBytes uintptr) {\n+\tif mheap_.sweepPagesPerByte == 0 {\n+\t\t// Nobody cares about the credit. Avoid the atomic.\n+\t\treturn\n+\t}\n+\tnval := atomic.Xadd64(&mheap_.spanBytesAlloc, -int64(unusableBytes))\n+\tif int64(nval) < 0 {\n+\t\t// Debugging for #18043.\n+\t\tprint(\"runtime: bad spanBytesAlloc=\", nval, \" (was \", nval+uint64(unusableBytes), \") unusableBytes=\", unusableBytes, \" sweepPagesPerByte=\", mheap_.sweepPagesPerByte, \"\\n\")\n+\t\tthrow(\"spanBytesAlloc underflow\")\n+\t}\n+}"}, {"sha": "6c1118e3857ccfe4b44585590fba7ad81b6bfcc6", "filename": "libgo/go/runtime/mgcsweepbuf.go", "status": "added", "additions": 178, "deletions": 0, "changes": 178, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmgcsweepbuf.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmgcsweepbuf.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmgcsweepbuf.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,178 @@\n+// Copyright 2016 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/atomic\"\n+\t\"runtime/internal/sys\"\n+\t\"unsafe\"\n+)\n+\n+// A gcSweepBuf is a set of *mspans.\n+//\n+// gcSweepBuf is safe for concurrent push operations *or* concurrent\n+// pop operations, but not both simultaneously.\n+type gcSweepBuf struct {\n+\t// A gcSweepBuf is a two-level data structure consisting of a\n+\t// growable spine that points to fixed-sized blocks. The spine\n+\t// can be accessed without locks, but adding a block or\n+\t// growing it requires taking the spine lock.\n+\t//\n+\t// Because each mspan covers at least 8K of heap and takes at\n+\t// most 8 bytes in the gcSweepBuf, the growth of the spine is\n+\t// quite limited.\n+\t//\n+\t// The spine and all blocks are allocated off-heap, which\n+\t// allows this to be used in the memory manager and avoids the\n+\t// need for write barriers on all of these. We never release\n+\t// this memory because there could be concurrent lock-free\n+\t// access and we're likely to reuse it anyway. (In principle,\n+\t// we could do this during STW.)\n+\n+\tspineLock mutex\n+\tspine     unsafe.Pointer // *[N]*gcSweepBlock, accessed atomically\n+\tspineLen  uintptr        // Spine array length, accessed atomically\n+\tspineCap  uintptr        // Spine array cap, accessed under lock\n+\n+\t// index is the first unused slot in the logical concatenation\n+\t// of all blocks. It is accessed atomically.\n+\tindex uint32\n+}\n+\n+const (\n+\tgcSweepBlockEntries    = 512 // 4KB on 64-bit\n+\tgcSweepBufInitSpineCap = 256 // Enough for 1GB heap on 64-bit\n+)\n+\n+type gcSweepBlock struct {\n+\tspans [gcSweepBlockEntries]*mspan\n+}\n+\n+// push adds span s to buffer b. push is safe to call concurrently\n+// with other push operations, but NOT to call concurrently with pop.\n+func (b *gcSweepBuf) push(s *mspan) {\n+\t// Obtain our slot.\n+\tcursor := uintptr(atomic.Xadd(&b.index, +1) - 1)\n+\ttop, bottom := cursor/gcSweepBlockEntries, cursor%gcSweepBlockEntries\n+\n+\t// Do we need to add a block?\n+\tspineLen := atomic.Loaduintptr(&b.spineLen)\n+\tvar block *gcSweepBlock\n+retry:\n+\tif top < spineLen {\n+\t\tspine := atomic.Loadp(unsafe.Pointer(&b.spine))\n+\t\tblockp := add(spine, sys.PtrSize*top)\n+\t\tblock = (*gcSweepBlock)(atomic.Loadp(blockp))\n+\t} else {\n+\t\t// Add a new block to the spine, potentially growing\n+\t\t// the spine.\n+\t\tlock(&b.spineLock)\n+\t\t// spineLen cannot change until we release the lock,\n+\t\t// but may have changed while we were waiting.\n+\t\tspineLen = atomic.Loaduintptr(&b.spineLen)\n+\t\tif top < spineLen {\n+\t\t\tunlock(&b.spineLock)\n+\t\t\tgoto retry\n+\t\t}\n+\n+\t\tif spineLen == b.spineCap {\n+\t\t\t// Grow the spine.\n+\t\t\tnewCap := b.spineCap * 2\n+\t\t\tif newCap == 0 {\n+\t\t\t\tnewCap = gcSweepBufInitSpineCap\n+\t\t\t}\n+\t\t\tnewSpine := persistentalloc(newCap*sys.PtrSize, sys.CacheLineSize, &memstats.gc_sys)\n+\t\t\tif b.spineCap != 0 {\n+\t\t\t\t// Blocks are allocated off-heap, so\n+\t\t\t\t// no write barriers.\n+\t\t\t\tmemmove(newSpine, b.spine, b.spineCap*sys.PtrSize)\n+\t\t\t}\n+\t\t\t// Spine is allocated off-heap, so no write barrier.\n+\t\t\tatomic.StorepNoWB(unsafe.Pointer(&b.spine), newSpine)\n+\t\t\tb.spineCap = newCap\n+\t\t\t// We can't immediately free the old spine\n+\t\t\t// since a concurrent push with a lower index\n+\t\t\t// could still be reading from it. We let it\n+\t\t\t// leak because even a 1TB heap would waste\n+\t\t\t// less than 2MB of memory on old spines. If\n+\t\t\t// this is a problem, we could free old spines\n+\t\t\t// during STW.\n+\t\t}\n+\n+\t\t// Allocate a new block and add it to the spine.\n+\t\tblock = (*gcSweepBlock)(persistentalloc(unsafe.Sizeof(gcSweepBlock{}), sys.CacheLineSize, &memstats.gc_sys))\n+\t\tblockp := add(b.spine, sys.PtrSize*top)\n+\t\t// Blocks are allocated off-heap, so no write barrier.\n+\t\tatomic.StorepNoWB(blockp, unsafe.Pointer(block))\n+\t\tatomic.Storeuintptr(&b.spineLen, spineLen+1)\n+\t\tunlock(&b.spineLock)\n+\t}\n+\n+\t// We have a block. Insert the span.\n+\tblock.spans[bottom] = s\n+}\n+\n+// pop removes and returns a span from buffer b, or nil if b is empty.\n+// pop is safe to call concurrently with other pop operations, but NOT\n+// to call concurrently with push.\n+func (b *gcSweepBuf) pop() *mspan {\n+\tcursor := atomic.Xadd(&b.index, -1)\n+\tif int32(cursor) < 0 {\n+\t\tatomic.Xadd(&b.index, +1)\n+\t\treturn nil\n+\t}\n+\n+\t// There are no concurrent spine or block modifications during\n+\t// pop, so we can omit the atomics.\n+\ttop, bottom := cursor/gcSweepBlockEntries, cursor%gcSweepBlockEntries\n+\tblockp := (**gcSweepBlock)(add(b.spine, sys.PtrSize*uintptr(top)))\n+\tblock := *blockp\n+\ts := block.spans[bottom]\n+\t// Clear the pointer for block(i).\n+\tblock.spans[bottom] = nil\n+\treturn s\n+}\n+\n+// numBlocks returns the number of blocks in buffer b. numBlocks is\n+// safe to call concurrently with any other operation. Spans that have\n+// been pushed prior to the call to numBlocks are guaranteed to appear\n+// in some block in the range [0, numBlocks()), assuming there are no\n+// intervening pops. Spans that are pushed after the call may also\n+// appear in these blocks.\n+func (b *gcSweepBuf) numBlocks() int {\n+\treturn int((atomic.Load(&b.index) + gcSweepBlockEntries - 1) / gcSweepBlockEntries)\n+}\n+\n+// block returns the spans in the i'th block of buffer b. block is\n+// safe to call concurrently with push.\n+func (b *gcSweepBuf) block(i int) []*mspan {\n+\t// Perform bounds check before loading spine address since\n+\t// push ensures the allocated length is at least spineLen.\n+\tif i < 0 || uintptr(i) >= atomic.Loaduintptr(&b.spineLen) {\n+\t\tthrow(\"block index out of range\")\n+\t}\n+\n+\t// Get block i.\n+\tspine := atomic.Loadp(unsafe.Pointer(&b.spine))\n+\tblockp := add(spine, sys.PtrSize*uintptr(i))\n+\tblock := (*gcSweepBlock)(atomic.Loadp(blockp))\n+\n+\t// Slice the block if necessary.\n+\tcursor := uintptr(atomic.Load(&b.index))\n+\ttop, bottom := cursor/gcSweepBlockEntries, cursor%gcSweepBlockEntries\n+\tvar spans []*mspan\n+\tif uintptr(i) < top {\n+\t\tspans = block.spans[:]\n+\t} else {\n+\t\tspans = block.spans[:bottom]\n+\t}\n+\n+\t// push may have reserved a slot but not filled it yet, so\n+\t// trim away unused entries.\n+\tfor len(spans) > 0 && spans[len(spans)-1] == nil {\n+\t\tspans = spans[:len(spans)-1]\n+\t}\n+\treturn spans\n+}"}, {"sha": "5eb05a767c055302960465094cd03ac20b539334", "filename": "libgo/go/runtime/mgcwork.go", "status": "added", "additions": 444, "deletions": 0, "changes": 444, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmgcwork.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmgcwork.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmgcwork.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,444 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/atomic\"\n+\t\"runtime/internal/sys\"\n+\t\"unsafe\"\n+)\n+\n+const (\n+\t_WorkbufSize = 2048 // in bytes; larger values result in less contention\n+)\n+\n+// Garbage collector work pool abstraction.\n+//\n+// This implements a producer/consumer model for pointers to grey\n+// objects. A grey object is one that is marked and on a work\n+// queue. A black object is marked and not on a work queue.\n+//\n+// Write barriers, root discovery, stack scanning, and object scanning\n+// produce pointers to grey objects. Scanning consumes pointers to\n+// grey objects, thus blackening them, and then scans them,\n+// potentially producing new pointers to grey objects.\n+\n+// A wbufptr holds a workbuf*, but protects it from write barriers.\n+// workbufs never live on the heap, so write barriers are unnecessary.\n+// Write barriers on workbuf pointers may also be dangerous in the GC.\n+//\n+// TODO: Since workbuf is now go:notinheap, this isn't necessary.\n+type wbufptr uintptr\n+\n+func wbufptrOf(w *workbuf) wbufptr {\n+\treturn wbufptr(unsafe.Pointer(w))\n+}\n+\n+func (wp wbufptr) ptr() *workbuf {\n+\treturn (*workbuf)(unsafe.Pointer(wp))\n+}\n+\n+// A gcWork provides the interface to produce and consume work for the\n+// garbage collector.\n+//\n+// A gcWork can be used on the stack as follows:\n+//\n+//     (preemption must be disabled)\n+//     gcw := &getg().m.p.ptr().gcw\n+//     .. call gcw.put() to produce and gcw.get() to consume ..\n+//     if gcBlackenPromptly {\n+//         gcw.dispose()\n+//     }\n+//\n+// It's important that any use of gcWork during the mark phase prevent\n+// the garbage collector from transitioning to mark termination since\n+// gcWork may locally hold GC work buffers. This can be done by\n+// disabling preemption (systemstack or acquirem).\n+type gcWork struct {\n+\t// wbuf1 and wbuf2 are the primary and secondary work buffers.\n+\t//\n+\t// This can be thought of as a stack of both work buffers'\n+\t// pointers concatenated. When we pop the last pointer, we\n+\t// shift the stack up by one work buffer by bringing in a new\n+\t// full buffer and discarding an empty one. When we fill both\n+\t// buffers, we shift the stack down by one work buffer by\n+\t// bringing in a new empty buffer and discarding a full one.\n+\t// This way we have one buffer's worth of hysteresis, which\n+\t// amortizes the cost of getting or putting a work buffer over\n+\t// at least one buffer of work and reduces contention on the\n+\t// global work lists.\n+\t//\n+\t// wbuf1 is always the buffer we're currently pushing to and\n+\t// popping from and wbuf2 is the buffer that will be discarded\n+\t// next.\n+\t//\n+\t// Invariant: Both wbuf1 and wbuf2 are nil or neither are.\n+\twbuf1, wbuf2 wbufptr\n+\n+\t// Bytes marked (blackened) on this gcWork. This is aggregated\n+\t// into work.bytesMarked by dispose.\n+\tbytesMarked uint64\n+\n+\t// Scan work performed on this gcWork. This is aggregated into\n+\t// gcController by dispose and may also be flushed by callers.\n+\tscanWork int64\n+}\n+\n+func (w *gcWork) init() {\n+\tw.wbuf1 = wbufptrOf(getempty())\n+\twbuf2 := trygetfull()\n+\tif wbuf2 == nil {\n+\t\twbuf2 = getempty()\n+\t}\n+\tw.wbuf2 = wbufptrOf(wbuf2)\n+}\n+\n+// put enqueues a pointer for the garbage collector to trace.\n+// obj must point to the beginning of a heap object or an oblet.\n+//go:nowritebarrier\n+func (w *gcWork) put(obj uintptr) {\n+\tflushed := false\n+\twbuf := w.wbuf1.ptr()\n+\tif wbuf == nil {\n+\t\tw.init()\n+\t\twbuf = w.wbuf1.ptr()\n+\t\t// wbuf is empty at this point.\n+\t} else if wbuf.nobj == len(wbuf.obj) {\n+\t\tw.wbuf1, w.wbuf2 = w.wbuf2, w.wbuf1\n+\t\twbuf = w.wbuf1.ptr()\n+\t\tif wbuf.nobj == len(wbuf.obj) {\n+\t\t\tputfull(wbuf)\n+\t\t\twbuf = getempty()\n+\t\t\tw.wbuf1 = wbufptrOf(wbuf)\n+\t\t\tflushed = true\n+\t\t}\n+\t}\n+\n+\twbuf.obj[wbuf.nobj] = obj\n+\twbuf.nobj++\n+\n+\t// If we put a buffer on full, let the GC controller know so\n+\t// it can encourage more workers to run. We delay this until\n+\t// the end of put so that w is in a consistent state, since\n+\t// enlistWorker may itself manipulate w.\n+\tif flushed && gcphase == _GCmark {\n+\t\tgcController.enlistWorker()\n+\t}\n+}\n+\n+// putFast does a put and returns true if it can be done quickly\n+// otherwise it returns false and the caller needs to call put.\n+//go:nowritebarrier\n+func (w *gcWork) putFast(obj uintptr) bool {\n+\twbuf := w.wbuf1.ptr()\n+\tif wbuf == nil {\n+\t\treturn false\n+\t} else if wbuf.nobj == len(wbuf.obj) {\n+\t\treturn false\n+\t}\n+\n+\twbuf.obj[wbuf.nobj] = obj\n+\twbuf.nobj++\n+\treturn true\n+}\n+\n+// tryGet dequeues a pointer for the garbage collector to trace.\n+//\n+// If there are no pointers remaining in this gcWork or in the global\n+// queue, tryGet returns 0.  Note that there may still be pointers in\n+// other gcWork instances or other caches.\n+//go:nowritebarrier\n+func (w *gcWork) tryGet() uintptr {\n+\twbuf := w.wbuf1.ptr()\n+\tif wbuf == nil {\n+\t\tw.init()\n+\t\twbuf = w.wbuf1.ptr()\n+\t\t// wbuf is empty at this point.\n+\t}\n+\tif wbuf.nobj == 0 {\n+\t\tw.wbuf1, w.wbuf2 = w.wbuf2, w.wbuf1\n+\t\twbuf = w.wbuf1.ptr()\n+\t\tif wbuf.nobj == 0 {\n+\t\t\towbuf := wbuf\n+\t\t\twbuf = trygetfull()\n+\t\t\tif wbuf == nil {\n+\t\t\t\treturn 0\n+\t\t\t}\n+\t\t\tputempty(owbuf)\n+\t\t\tw.wbuf1 = wbufptrOf(wbuf)\n+\t\t}\n+\t}\n+\n+\twbuf.nobj--\n+\treturn wbuf.obj[wbuf.nobj]\n+}\n+\n+// tryGetFast dequeues a pointer for the garbage collector to trace\n+// if one is readily available. Otherwise it returns 0 and\n+// the caller is expected to call tryGet().\n+//go:nowritebarrier\n+func (w *gcWork) tryGetFast() uintptr {\n+\twbuf := w.wbuf1.ptr()\n+\tif wbuf == nil {\n+\t\treturn 0\n+\t}\n+\tif wbuf.nobj == 0 {\n+\t\treturn 0\n+\t}\n+\n+\twbuf.nobj--\n+\treturn wbuf.obj[wbuf.nobj]\n+}\n+\n+// get dequeues a pointer for the garbage collector to trace, blocking\n+// if necessary to ensure all pointers from all queues and caches have\n+// been retrieved.  get returns 0 if there are no pointers remaining.\n+//go:nowritebarrier\n+func (w *gcWork) get() uintptr {\n+\twbuf := w.wbuf1.ptr()\n+\tif wbuf == nil {\n+\t\tw.init()\n+\t\twbuf = w.wbuf1.ptr()\n+\t\t// wbuf is empty at this point.\n+\t}\n+\tif wbuf.nobj == 0 {\n+\t\tw.wbuf1, w.wbuf2 = w.wbuf2, w.wbuf1\n+\t\twbuf = w.wbuf1.ptr()\n+\t\tif wbuf.nobj == 0 {\n+\t\t\towbuf := wbuf\n+\t\t\twbuf = getfull()\n+\t\t\tif wbuf == nil {\n+\t\t\t\treturn 0\n+\t\t\t}\n+\t\t\tputempty(owbuf)\n+\t\t\tw.wbuf1 = wbufptrOf(wbuf)\n+\t\t}\n+\t}\n+\n+\t// TODO: This might be a good place to add prefetch code\n+\n+\twbuf.nobj--\n+\treturn wbuf.obj[wbuf.nobj]\n+}\n+\n+// dispose returns any cached pointers to the global queue.\n+// The buffers are being put on the full queue so that the\n+// write barriers will not simply reacquire them before the\n+// GC can inspect them. This helps reduce the mutator's\n+// ability to hide pointers during the concurrent mark phase.\n+//\n+//go:nowritebarrier\n+func (w *gcWork) dispose() {\n+\tif wbuf := w.wbuf1.ptr(); wbuf != nil {\n+\t\tif wbuf.nobj == 0 {\n+\t\t\tputempty(wbuf)\n+\t\t} else {\n+\t\t\tputfull(wbuf)\n+\t\t}\n+\t\tw.wbuf1 = 0\n+\n+\t\twbuf = w.wbuf2.ptr()\n+\t\tif wbuf.nobj == 0 {\n+\t\t\tputempty(wbuf)\n+\t\t} else {\n+\t\t\tputfull(wbuf)\n+\t\t}\n+\t\tw.wbuf2 = 0\n+\t}\n+\tif w.bytesMarked != 0 {\n+\t\t// dispose happens relatively infrequently. If this\n+\t\t// atomic becomes a problem, we should first try to\n+\t\t// dispose less and if necessary aggregate in a per-P\n+\t\t// counter.\n+\t\tatomic.Xadd64(&work.bytesMarked, int64(w.bytesMarked))\n+\t\tw.bytesMarked = 0\n+\t}\n+\tif w.scanWork != 0 {\n+\t\tatomic.Xaddint64(&gcController.scanWork, w.scanWork)\n+\t\tw.scanWork = 0\n+\t}\n+}\n+\n+// balance moves some work that's cached in this gcWork back on the\n+// global queue.\n+//go:nowritebarrier\n+func (w *gcWork) balance() {\n+\tif w.wbuf1 == 0 {\n+\t\treturn\n+\t}\n+\tif wbuf := w.wbuf2.ptr(); wbuf.nobj != 0 {\n+\t\tputfull(wbuf)\n+\t\tw.wbuf2 = wbufptrOf(getempty())\n+\t} else if wbuf := w.wbuf1.ptr(); wbuf.nobj > 4 {\n+\t\tw.wbuf1 = wbufptrOf(handoff(wbuf))\n+\t} else {\n+\t\treturn\n+\t}\n+\t// We flushed a buffer to the full list, so wake a worker.\n+\tif gcphase == _GCmark {\n+\t\tgcController.enlistWorker()\n+\t}\n+}\n+\n+// empty returns true if w has no mark work available.\n+//go:nowritebarrier\n+func (w *gcWork) empty() bool {\n+\treturn w.wbuf1 == 0 || (w.wbuf1.ptr().nobj == 0 && w.wbuf2.ptr().nobj == 0)\n+}\n+\n+// Internally, the GC work pool is kept in arrays in work buffers.\n+// The gcWork interface caches a work buffer until full (or empty) to\n+// avoid contending on the global work buffer lists.\n+\n+type workbufhdr struct {\n+\tnode lfnode // must be first\n+\tnobj int\n+}\n+\n+//go:notinheap\n+type workbuf struct {\n+\tworkbufhdr\n+\t// account for the above fields\n+\tobj [(_WorkbufSize - unsafe.Sizeof(workbufhdr{})) / sys.PtrSize]uintptr\n+}\n+\n+// workbuf factory routines. These funcs are used to manage the\n+// workbufs.\n+// If the GC asks for some work these are the only routines that\n+// make wbufs available to the GC.\n+\n+func (b *workbuf) checknonempty() {\n+\tif b.nobj == 0 {\n+\t\tthrow(\"workbuf is empty\")\n+\t}\n+}\n+\n+func (b *workbuf) checkempty() {\n+\tif b.nobj != 0 {\n+\t\tthrow(\"workbuf is not empty\")\n+\t}\n+}\n+\n+// getempty pops an empty work buffer off the work.empty list,\n+// allocating new buffers if none are available.\n+//go:nowritebarrier\n+func getempty() *workbuf {\n+\tvar b *workbuf\n+\tif work.empty != 0 {\n+\t\tb = (*workbuf)(lfstackpop(&work.empty))\n+\t\tif b != nil {\n+\t\t\tb.checkempty()\n+\t\t}\n+\t}\n+\tif b == nil {\n+\t\tb = (*workbuf)(persistentalloc(unsafe.Sizeof(*b), sys.CacheLineSize, &memstats.gc_sys))\n+\t}\n+\treturn b\n+}\n+\n+// putempty puts a workbuf onto the work.empty list.\n+// Upon entry this go routine owns b. The lfstackpush relinquishes ownership.\n+//go:nowritebarrier\n+func putempty(b *workbuf) {\n+\tb.checkempty()\n+\tlfstackpush(&work.empty, &b.node)\n+}\n+\n+// putfull puts the workbuf on the work.full list for the GC.\n+// putfull accepts partially full buffers so the GC can avoid competing\n+// with the mutators for ownership of partially full buffers.\n+//go:nowritebarrier\n+func putfull(b *workbuf) {\n+\tb.checknonempty()\n+\tlfstackpush(&work.full, &b.node)\n+}\n+\n+// trygetfull tries to get a full or partially empty workbuffer.\n+// If one is not immediately available return nil\n+//go:nowritebarrier\n+func trygetfull() *workbuf {\n+\tb := (*workbuf)(lfstackpop(&work.full))\n+\tif b != nil {\n+\t\tb.checknonempty()\n+\t\treturn b\n+\t}\n+\treturn b\n+}\n+\n+// Get a full work buffer off the work.full list.\n+// If nothing is available wait until all the other gc helpers have\n+// finished and then return nil.\n+// getfull acts as a barrier for work.nproc helpers. As long as one\n+// gchelper is actively marking objects it\n+// may create a workbuffer that the other helpers can work on.\n+// The for loop either exits when a work buffer is found\n+// or when _all_ of the work.nproc GC helpers are in the loop\n+// looking for work and thus not capable of creating new work.\n+// This is in fact the termination condition for the STW mark\n+// phase.\n+//go:nowritebarrier\n+func getfull() *workbuf {\n+\tb := (*workbuf)(lfstackpop(&work.full))\n+\tif b != nil {\n+\t\tb.checknonempty()\n+\t\treturn b\n+\t}\n+\n+\tincnwait := atomic.Xadd(&work.nwait, +1)\n+\tif incnwait > work.nproc {\n+\t\tprintln(\"runtime: work.nwait=\", incnwait, \"work.nproc=\", work.nproc)\n+\t\tthrow(\"work.nwait > work.nproc\")\n+\t}\n+\tfor i := 0; ; i++ {\n+\t\tif work.full != 0 {\n+\t\t\tdecnwait := atomic.Xadd(&work.nwait, -1)\n+\t\t\tif decnwait == work.nproc {\n+\t\t\t\tprintln(\"runtime: work.nwait=\", decnwait, \"work.nproc=\", work.nproc)\n+\t\t\t\tthrow(\"work.nwait > work.nproc\")\n+\t\t\t}\n+\t\t\tb = (*workbuf)(lfstackpop(&work.full))\n+\t\t\tif b != nil {\n+\t\t\t\tb.checknonempty()\n+\t\t\t\treturn b\n+\t\t\t}\n+\t\t\tincnwait := atomic.Xadd(&work.nwait, +1)\n+\t\t\tif incnwait > work.nproc {\n+\t\t\t\tprintln(\"runtime: work.nwait=\", incnwait, \"work.nproc=\", work.nproc)\n+\t\t\t\tthrow(\"work.nwait > work.nproc\")\n+\t\t\t}\n+\t\t}\n+\t\tif work.nwait == work.nproc && work.markrootNext >= work.markrootJobs {\n+\t\t\treturn nil\n+\t\t}\n+\t\t_g_ := getg()\n+\t\tif i < 10 {\n+\t\t\t_g_.m.gcstats.nprocyield++\n+\t\t\tprocyield(20)\n+\t\t} else if i < 20 {\n+\t\t\t_g_.m.gcstats.nosyield++\n+\t\t\tosyield()\n+\t\t} else {\n+\t\t\t_g_.m.gcstats.nsleep++\n+\t\t\tusleep(100)\n+\t\t}\n+\t}\n+}\n+\n+//go:nowritebarrier\n+func handoff(b *workbuf) *workbuf {\n+\t// Make new buffer with half of b's pointers.\n+\tb1 := getempty()\n+\tn := b.nobj / 2\n+\tb.nobj -= n\n+\tb1.nobj = n\n+\tmemmove(unsafe.Pointer(&b1.obj[0]), unsafe.Pointer(&b.obj[b.nobj]), uintptr(n)*unsafe.Sizeof(b1.obj[0]))\n+\t_g_ := getg()\n+\t_g_.m.gcstats.nhandoff++\n+\t_g_.m.gcstats.nhandoffcnt += uint64(n)\n+\n+\t// Put b on full list - let first half of b get stolen.\n+\tputfull(b)\n+\treturn b1\n+}"}, {"sha": "72627485b87b626dee9640ee38df11289c8ac4a4", "filename": "libgo/go/runtime/mheap.go", "status": "added", "additions": 1427, "deletions": 0, "changes": 1427, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmheap.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmheap.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmheap.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,1427 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Page heap.\n+//\n+// See malloc.go for overview.\n+\n+package runtime\n+\n+import (\n+\t\"runtime/internal/atomic\"\n+\t\"runtime/internal/sys\"\n+\t\"unsafe\"\n+)\n+\n+// minPhysPageSize is a lower-bound on the physical page size. The\n+// true physical page size may be larger than this. In contrast,\n+// sys.PhysPageSize is an upper-bound on the physical page size.\n+const minPhysPageSize = 4096\n+\n+// Main malloc heap.\n+// The heap itself is the \"free[]\" and \"large\" arrays,\n+// but all the other global data is here too.\n+//\n+// mheap must not be heap-allocated because it contains mSpanLists,\n+// which must not be heap-allocated.\n+//\n+//go:notinheap\n+type mheap struct {\n+\tlock      mutex\n+\tfree      [_MaxMHeapList]mSpanList // free lists of given length\n+\tfreelarge mSpanList                // free lists length >= _MaxMHeapList\n+\tbusy      [_MaxMHeapList]mSpanList // busy lists of large objects of given length\n+\tbusylarge mSpanList                // busy lists of large objects length >= _MaxMHeapList\n+\tsweepgen  uint32                   // sweep generation, see comment in mspan\n+\tsweepdone uint32                   // all spans are swept\n+\n+\t// allspans is a slice of all mspans ever created. Each mspan\n+\t// appears exactly once.\n+\t//\n+\t// The memory for allspans is manually managed and can be\n+\t// reallocated and move as the heap grows.\n+\t//\n+\t// In general, allspans is protected by mheap_.lock, which\n+\t// prevents concurrent access as well as freeing the backing\n+\t// store. Accesses during STW might not hold the lock, but\n+\t// must ensure that allocation cannot happen around the\n+\t// access (since that may free the backing store).\n+\tallspans []*mspan // all spans out there\n+\n+\t// spans is a lookup table to map virtual address page IDs to *mspan.\n+\t// For allocated spans, their pages map to the span itself.\n+\t// For free spans, only the lowest and highest pages map to the span itself.\n+\t// Internal pages map to an arbitrary span.\n+\t// For pages that have never been allocated, spans entries are nil.\n+\t//\n+\t// This is backed by a reserved region of the address space so\n+\t// it can grow without moving. The memory up to len(spans) is\n+\t// mapped. cap(spans) indicates the total reserved memory.\n+\tspans []*mspan\n+\n+\t// sweepSpans contains two mspan stacks: one of swept in-use\n+\t// spans, and one of unswept in-use spans. These two trade\n+\t// roles on each GC cycle. Since the sweepgen increases by 2\n+\t// on each cycle, this means the swept spans are in\n+\t// sweepSpans[sweepgen/2%2] and the unswept spans are in\n+\t// sweepSpans[1-sweepgen/2%2]. Sweeping pops spans from the\n+\t// unswept stack and pushes spans that are still in-use on the\n+\t// swept stack. Likewise, allocating an in-use span pushes it\n+\t// on the swept stack.\n+\tsweepSpans [2]gcSweepBuf\n+\n+\t_ uint32 // align uint64 fields on 32-bit for atomics\n+\n+\t// Proportional sweep\n+\tpagesInUse        uint64  // pages of spans in stats _MSpanInUse; R/W with mheap.lock\n+\tspanBytesAlloc    uint64  // bytes of spans allocated this cycle; updated atomically\n+\tpagesSwept        uint64  // pages swept this cycle; updated atomically\n+\tsweepPagesPerByte float64 // proportional sweep ratio; written with lock, read without\n+\t// TODO(austin): pagesInUse should be a uintptr, but the 386\n+\t// compiler can't 8-byte align fields.\n+\n+\t// Malloc stats.\n+\tlargefree  uint64                  // bytes freed for large objects (>maxsmallsize)\n+\tnlargefree uint64                  // number of frees for large objects (>maxsmallsize)\n+\tnsmallfree [_NumSizeClasses]uint64 // number of frees for small objects (<=maxsmallsize)\n+\n+\t// range of addresses we might see in the heap\n+\tbitmap         uintptr // Points to one byte past the end of the bitmap\n+\tbitmap_mapped  uintptr\n+\tarena_start    uintptr\n+\tarena_used     uintptr // always mHeap_Map{Bits,Spans} before updating\n+\tarena_end      uintptr\n+\tarena_reserved bool\n+\n+\t// central free lists for small size classes.\n+\t// the padding makes sure that the MCentrals are\n+\t// spaced CacheLineSize bytes apart, so that each MCentral.lock\n+\t// gets its own cache line.\n+\tcentral [_NumSizeClasses]struct {\n+\t\tmcentral mcentral\n+\t\tpad      [sys.CacheLineSize]byte\n+\t}\n+\n+\tspanalloc             fixalloc // allocator for span*\n+\tcachealloc            fixalloc // allocator for mcache*\n+\tspecialfinalizeralloc fixalloc // allocator for specialfinalizer*\n+\tspecialprofilealloc   fixalloc // allocator for specialprofile*\n+\tspeciallock           mutex    // lock for special record allocators.\n+}\n+\n+var mheap_ mheap\n+\n+// An MSpan is a run of pages.\n+//\n+// When a MSpan is in the heap free list, state == MSpanFree\n+// and heapmap(s->start) == span, heapmap(s->start+s->npages-1) == span.\n+//\n+// When a MSpan is allocated, state == MSpanInUse or MSpanStack\n+// and heapmap(i) == span for all s->start <= i < s->start+s->npages.\n+\n+// Every MSpan is in one doubly-linked list,\n+// either one of the MHeap's free lists or one of the\n+// MCentral's span lists.\n+\n+// An MSpan representing actual memory has state _MSpanInUse,\n+// _MSpanStack, or _MSpanFree. Transitions between these states are\n+// constrained as follows:\n+//\n+// * A span may transition from free to in-use or stack during any GC\n+//   phase.\n+//\n+// * During sweeping (gcphase == _GCoff), a span may transition from\n+//   in-use to free (as a result of sweeping) or stack to free (as a\n+//   result of stacks being freed).\n+//\n+// * During GC (gcphase != _GCoff), a span *must not* transition from\n+//   stack or in-use to free. Because concurrent GC may read a pointer\n+//   and then look up its span, the span state must be monotonic.\n+type mSpanState uint8\n+\n+const (\n+\t_MSpanDead  mSpanState = iota\n+\t_MSpanInUse            // allocated for garbage collected heap\n+\t_MSpanStack            // allocated for use by stack allocator\n+\t_MSpanFree\n+)\n+\n+// mSpanStateNames are the names of the span states, indexed by\n+// mSpanState.\n+var mSpanStateNames = []string{\n+\t\"_MSpanDead\",\n+\t\"_MSpanInUse\",\n+\t\"_MSpanStack\",\n+\t\"_MSpanFree\",\n+}\n+\n+// mSpanList heads a linked list of spans.\n+//\n+//go:notinheap\n+type mSpanList struct {\n+\tfirst *mspan // first span in list, or nil if none\n+\tlast  *mspan // last span in list, or nil if none\n+}\n+\n+//go:notinheap\n+type mspan struct {\n+\tnext *mspan     // next span in list, or nil if none\n+\tprev *mspan     // previous span in list, or nil if none\n+\tlist *mSpanList // For debugging. TODO: Remove.\n+\n+\tstartAddr     uintptr   // address of first byte of span aka s.base()\n+\tnpages        uintptr   // number of pages in span\n+\tstackfreelist gclinkptr // list of free stacks, avoids overloading freelist\n+\n+\t// freeindex is the slot index between 0 and nelems at which to begin scanning\n+\t// for the next free object in this span.\n+\t// Each allocation scans allocBits starting at freeindex until it encounters a 0\n+\t// indicating a free object. freeindex is then adjusted so that subsequent scans begin\n+\t// just past the the newly discovered free object.\n+\t//\n+\t// If freeindex == nelem, this span has no free objects.\n+\t//\n+\t// allocBits is a bitmap of objects in this span.\n+\t// If n >= freeindex and allocBits[n/8] & (1<<(n%8)) is 0\n+\t// then object n is free;\n+\t// otherwise, object n is allocated. Bits starting at nelem are\n+\t// undefined and should never be referenced.\n+\t//\n+\t// Object n starts at address n*elemsize + (start << pageShift).\n+\tfreeindex uintptr\n+\t// TODO: Look up nelems from sizeclass and remove this field if it\n+\t// helps performance.\n+\tnelems uintptr // number of object in the span.\n+\n+\t// Cache of the allocBits at freeindex. allocCache is shifted\n+\t// such that the lowest bit corresponds to the bit freeindex.\n+\t// allocCache holds the complement of allocBits, thus allowing\n+\t// ctz (count trailing zero) to use it directly.\n+\t// allocCache may contain bits beyond s.nelems; the caller must ignore\n+\t// these.\n+\tallocCache uint64\n+\n+\t// allocBits and gcmarkBits hold pointers to a span's mark and\n+\t// allocation bits. The pointers are 8 byte aligned.\n+\t// There are three arenas where this data is held.\n+\t// free: Dirty arenas that are no longer accessed\n+\t//       and can be reused.\n+\t// next: Holds information to be used in the next GC cycle.\n+\t// current: Information being used during this GC cycle.\n+\t// previous: Information being used during the last GC cycle.\n+\t// A new GC cycle starts with the call to finishsweep_m.\n+\t// finishsweep_m moves the previous arena to the free arena,\n+\t// the current arena to the previous arena, and\n+\t// the next arena to the current arena.\n+\t// The next arena is populated as the spans request\n+\t// memory to hold gcmarkBits for the next GC cycle as well\n+\t// as allocBits for newly allocated spans.\n+\t//\n+\t// The pointer arithmetic is done \"by hand\" instead of using\n+\t// arrays to avoid bounds checks along critical performance\n+\t// paths.\n+\t// The sweep will free the old allocBits and set allocBits to the\n+\t// gcmarkBits. The gcmarkBits are replaced with a fresh zeroed\n+\t// out memory.\n+\tallocBits  *uint8\n+\tgcmarkBits *uint8\n+\n+\t// sweep generation:\n+\t// if sweepgen == h->sweepgen - 2, the span needs sweeping\n+\t// if sweepgen == h->sweepgen - 1, the span is currently being swept\n+\t// if sweepgen == h->sweepgen, the span is swept and ready to use\n+\t// h->sweepgen is incremented by 2 after every GC\n+\n+\tsweepgen    uint32\n+\tdivMul      uint16     // for divide by elemsize - divMagic.mul\n+\tbaseMask    uint16     // if non-0, elemsize is a power of 2, & this will get object allocation base\n+\tallocCount  uint16     // capacity - number of objects in freelist\n+\tsizeclass   uint8      // size class\n+\tincache     bool       // being used by an mcache\n+\tstate       mSpanState // mspaninuse etc\n+\tneedzero    uint8      // needs to be zeroed before allocation\n+\tdivShift    uint8      // for divide by elemsize - divMagic.shift\n+\tdivShift2   uint8      // for divide by elemsize - divMagic.shift2\n+\telemsize    uintptr    // computed from sizeclass or from npages\n+\tunusedsince int64      // first time spotted by gc in mspanfree state\n+\tnpreleased  uintptr    // number of pages released to the os\n+\tlimit       uintptr    // end of data in span\n+\tspeciallock mutex      // guards specials list\n+\tspecials    *special   // linked list of special records sorted by offset.\n+}\n+\n+func (s *mspan) base() uintptr {\n+\treturn s.startAddr\n+}\n+\n+func (s *mspan) layout() (size, n, total uintptr) {\n+\ttotal = s.npages << _PageShift\n+\tsize = s.elemsize\n+\tif size > 0 {\n+\t\tn = total / size\n+\t}\n+\treturn\n+}\n+\n+func recordspan(vh unsafe.Pointer, p unsafe.Pointer) {\n+\th := (*mheap)(vh)\n+\ts := (*mspan)(p)\n+\tif len(h.allspans) >= cap(h.allspans) {\n+\t\tn := 64 * 1024 / sys.PtrSize\n+\t\tif n < cap(h.allspans)*3/2 {\n+\t\t\tn = cap(h.allspans) * 3 / 2\n+\t\t}\n+\t\tvar new []*mspan\n+\t\tsp := (*slice)(unsafe.Pointer(&new))\n+\t\tsp.array = sysAlloc(uintptr(n)*sys.PtrSize, &memstats.other_sys)\n+\t\tif sp.array == nil {\n+\t\t\tthrow(\"runtime: cannot allocate memory\")\n+\t\t}\n+\t\tsp.len = len(h.allspans)\n+\t\tsp.cap = n\n+\t\tif len(h.allspans) > 0 {\n+\t\t\tcopy(new, h.allspans)\n+\t\t}\n+\t\toldAllspans := h.allspans\n+\t\th.allspans = new\n+\t\tif len(oldAllspans) != 0 {\n+\t\t\tsysFree(unsafe.Pointer(&oldAllspans[0]), uintptr(cap(oldAllspans))*unsafe.Sizeof(oldAllspans[0]), &memstats.other_sys)\n+\t\t}\n+\t}\n+\th.allspans = append(h.allspans, s)\n+}\n+\n+// inheap reports whether b is a pointer into a (potentially dead) heap object.\n+// It returns false for pointers into stack spans.\n+// Non-preemptible because it is used by write barriers.\n+//go:nowritebarrier\n+//go:nosplit\n+func inheap(b uintptr) bool {\n+\tif b == 0 || b < mheap_.arena_start || b >= mheap_.arena_used {\n+\t\treturn false\n+\t}\n+\t// Not a beginning of a block, consult span table to find the block beginning.\n+\ts := mheap_.spans[(b-mheap_.arena_start)>>_PageShift]\n+\tif s == nil || b < s.base() || b >= s.limit || s.state != mSpanInUse {\n+\t\treturn false\n+\t}\n+\treturn true\n+}\n+\n+// inHeapOrStack is a variant of inheap that returns true for pointers into stack spans.\n+//go:nowritebarrier\n+//go:nosplit\n+func inHeapOrStack(b uintptr) bool {\n+\tif b == 0 || b < mheap_.arena_start || b >= mheap_.arena_used {\n+\t\treturn false\n+\t}\n+\t// Not a beginning of a block, consult span table to find the block beginning.\n+\ts := mheap_.spans[(b-mheap_.arena_start)>>_PageShift]\n+\tif s == nil || b < s.base() {\n+\t\treturn false\n+\t}\n+\tswitch s.state {\n+\tcase mSpanInUse:\n+\t\treturn b < s.limit\n+\tcase _MSpanStack:\n+\t\treturn b < s.base()+s.npages<<_PageShift\n+\tdefault:\n+\t\treturn false\n+\t}\n+}\n+\n+// TODO: spanOf and spanOfUnchecked are open-coded in a lot of places.\n+// Use the functions instead.\n+\n+// spanOf returns the span of p. If p does not point into the heap or\n+// no span contains p, spanOf returns nil.\n+func spanOf(p uintptr) *mspan {\n+\tif p == 0 || p < mheap_.arena_start || p >= mheap_.arena_used {\n+\t\treturn nil\n+\t}\n+\treturn spanOfUnchecked(p)\n+}\n+\n+// spanOfUnchecked is equivalent to spanOf, but the caller must ensure\n+// that p points into the heap (that is, mheap_.arena_start <= p <\n+// mheap_.arena_used).\n+func spanOfUnchecked(p uintptr) *mspan {\n+\treturn mheap_.spans[(p-mheap_.arena_start)>>_PageShift]\n+}\n+\n+func mlookup(v uintptr, base *uintptr, size *uintptr, sp **mspan) int32 {\n+\t_g_ := getg()\n+\n+\t_g_.m.mcache.local_nlookup++\n+\tif sys.PtrSize == 4 && _g_.m.mcache.local_nlookup >= 1<<30 {\n+\t\t// purge cache stats to prevent overflow\n+\t\tlock(&mheap_.lock)\n+\t\tpurgecachedstats(_g_.m.mcache)\n+\t\tunlock(&mheap_.lock)\n+\t}\n+\n+\ts := mheap_.lookupMaybe(unsafe.Pointer(v))\n+\tif sp != nil {\n+\t\t*sp = s\n+\t}\n+\tif s == nil {\n+\t\tif base != nil {\n+\t\t\t*base = 0\n+\t\t}\n+\t\tif size != nil {\n+\t\t\t*size = 0\n+\t\t}\n+\t\treturn 0\n+\t}\n+\n+\tp := s.base()\n+\tif s.sizeclass == 0 {\n+\t\t// Large object.\n+\t\tif base != nil {\n+\t\t\t*base = p\n+\t\t}\n+\t\tif size != nil {\n+\t\t\t*size = s.npages << _PageShift\n+\t\t}\n+\t\treturn 1\n+\t}\n+\n+\tn := s.elemsize\n+\tif base != nil {\n+\t\ti := (v - p) / n\n+\t\t*base = p + i*n\n+\t}\n+\tif size != nil {\n+\t\t*size = n\n+\t}\n+\n+\treturn 1\n+}\n+\n+// Initialize the heap.\n+func (h *mheap) init(spansStart, spansBytes uintptr) {\n+\th.spanalloc.init(unsafe.Sizeof(mspan{}), recordspan, unsafe.Pointer(h), &memstats.mspan_sys)\n+\th.cachealloc.init(unsafe.Sizeof(mcache{}), nil, nil, &memstats.mcache_sys)\n+\th.specialfinalizeralloc.init(unsafe.Sizeof(specialfinalizer{}), nil, nil, &memstats.other_sys)\n+\th.specialprofilealloc.init(unsafe.Sizeof(specialprofile{}), nil, nil, &memstats.other_sys)\n+\n+\t// Don't zero mspan allocations. Background sweeping can\n+\t// inspect a span concurrently with allocating it, so it's\n+\t// important that the span's sweepgen survive across freeing\n+\t// and re-allocating a span to prevent background sweeping\n+\t// from improperly cas'ing it from 0.\n+\t//\n+\t// This is safe because mspan contains no heap pointers.\n+\th.spanalloc.zero = false\n+\n+\t// h->mapcache needs no init\n+\tfor i := range h.free {\n+\t\th.free[i].init()\n+\t\th.busy[i].init()\n+\t}\n+\n+\th.freelarge.init()\n+\th.busylarge.init()\n+\tfor i := range h.central {\n+\t\th.central[i].mcentral.init(int32(i))\n+\t}\n+\n+\tsp := (*slice)(unsafe.Pointer(&h.spans))\n+\tsp.array = unsafe.Pointer(spansStart)\n+\tsp.len = 0\n+\tsp.cap = int(spansBytes / sys.PtrSize)\n+}\n+\n+// mHeap_MapSpans makes sure that the spans are mapped\n+// up to the new value of arena_used.\n+//\n+// It must be called with the expected new value of arena_used,\n+// *before* h.arena_used has been updated.\n+// Waiting to update arena_used until after the memory has been mapped\n+// avoids faults when other threads try access the bitmap immediately\n+// after observing the change to arena_used.\n+func (h *mheap) mapSpans(arena_used uintptr) {\n+\t// Map spans array, PageSize at a time.\n+\tn := arena_used\n+\tn -= h.arena_start\n+\tn = n / _PageSize * sys.PtrSize\n+\tn = round(n, physPageSize)\n+\tneed := n / unsafe.Sizeof(h.spans[0])\n+\thave := uintptr(len(h.spans))\n+\tif have >= need {\n+\t\treturn\n+\t}\n+\th.spans = h.spans[:need]\n+\tsysMap(unsafe.Pointer(&h.spans[have]), (need-have)*unsafe.Sizeof(h.spans[0]), h.arena_reserved, &memstats.other_sys)\n+}\n+\n+// Sweeps spans in list until reclaims at least npages into heap.\n+// Returns the actual number of pages reclaimed.\n+func (h *mheap) reclaimList(list *mSpanList, npages uintptr) uintptr {\n+\tn := uintptr(0)\n+\tsg := mheap_.sweepgen\n+retry:\n+\tfor s := list.first; s != nil; s = s.next {\n+\t\tif s.sweepgen == sg-2 && atomic.Cas(&s.sweepgen, sg-2, sg-1) {\n+\t\t\tlist.remove(s)\n+\t\t\t// swept spans are at the end of the list\n+\t\t\tlist.insertBack(s)\n+\t\t\tunlock(&h.lock)\n+\t\t\tsnpages := s.npages\n+\t\t\tif s.sweep(false) {\n+\t\t\t\tn += snpages\n+\t\t\t}\n+\t\t\tlock(&h.lock)\n+\t\t\tif n >= npages {\n+\t\t\t\treturn n\n+\t\t\t}\n+\t\t\t// the span could have been moved elsewhere\n+\t\t\tgoto retry\n+\t\t}\n+\t\tif s.sweepgen == sg-1 {\n+\t\t\t// the span is being sweept by background sweeper, skip\n+\t\t\tcontinue\n+\t\t}\n+\t\t// already swept empty span,\n+\t\t// all subsequent ones must also be either swept or in process of sweeping\n+\t\tbreak\n+\t}\n+\treturn n\n+}\n+\n+// Sweeps and reclaims at least npage pages into heap.\n+// Called before allocating npage pages.\n+func (h *mheap) reclaim(npage uintptr) {\n+\t// First try to sweep busy spans with large objects of size >= npage,\n+\t// this has good chances of reclaiming the necessary space.\n+\tfor i := int(npage); i < len(h.busy); i++ {\n+\t\tif h.reclaimList(&h.busy[i], npage) != 0 {\n+\t\t\treturn // Bingo!\n+\t\t}\n+\t}\n+\n+\t// Then -- even larger objects.\n+\tif h.reclaimList(&h.busylarge, npage) != 0 {\n+\t\treturn // Bingo!\n+\t}\n+\n+\t// Now try smaller objects.\n+\t// One such object is not enough, so we need to reclaim several of them.\n+\treclaimed := uintptr(0)\n+\tfor i := 0; i < int(npage) && i < len(h.busy); i++ {\n+\t\treclaimed += h.reclaimList(&h.busy[i], npage-reclaimed)\n+\t\tif reclaimed >= npage {\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n+\t// Now sweep everything that is not yet swept.\n+\tunlock(&h.lock)\n+\tfor {\n+\t\tn := sweepone()\n+\t\tif n == ^uintptr(0) { // all spans are swept\n+\t\t\tbreak\n+\t\t}\n+\t\treclaimed += n\n+\t\tif reclaimed >= npage {\n+\t\t\tbreak\n+\t\t}\n+\t}\n+\tlock(&h.lock)\n+}\n+\n+// Allocate a new span of npage pages from the heap for GC'd memory\n+// and record its size class in the HeapMap and HeapMapCache.\n+func (h *mheap) alloc_m(npage uintptr, sizeclass int32, large bool) *mspan {\n+\t_g_ := getg()\n+\tlock(&h.lock)\n+\n+\t// To prevent excessive heap growth, before allocating n pages\n+\t// we need to sweep and reclaim at least n pages.\n+\tif h.sweepdone == 0 {\n+\t\t// TODO(austin): This tends to sweep a large number of\n+\t\t// spans in order to find a few completely free spans\n+\t\t// (for example, in the garbage benchmark, this sweeps\n+\t\t// ~30x the number of pages its trying to allocate).\n+\t\t// If GC kept a bit for whether there were any marks\n+\t\t// in a span, we could release these free spans\n+\t\t// at the end of GC and eliminate this entirely.\n+\t\th.reclaim(npage)\n+\t}\n+\n+\t// transfer stats from cache to global\n+\tmemstats.heap_scan += uint64(_g_.m.mcache.local_scan)\n+\t_g_.m.mcache.local_scan = 0\n+\tmemstats.tinyallocs += uint64(_g_.m.mcache.local_tinyallocs)\n+\t_g_.m.mcache.local_tinyallocs = 0\n+\n+\ts := h.allocSpanLocked(npage)\n+\tif s != nil {\n+\t\t// Record span info, because gc needs to be\n+\t\t// able to map interior pointer to containing span.\n+\t\tatomic.Store(&s.sweepgen, h.sweepgen)\n+\t\th.sweepSpans[h.sweepgen/2%2].push(s) // Add to swept in-use list.\n+\t\ts.state = _MSpanInUse\n+\t\ts.allocCount = 0\n+\t\ts.sizeclass = uint8(sizeclass)\n+\t\tif sizeclass == 0 {\n+\t\t\ts.elemsize = s.npages << _PageShift\n+\t\t\ts.divShift = 0\n+\t\t\ts.divMul = 0\n+\t\t\ts.divShift2 = 0\n+\t\t\ts.baseMask = 0\n+\t\t} else {\n+\t\t\ts.elemsize = uintptr(class_to_size[sizeclass])\n+\t\t\tm := &class_to_divmagic[sizeclass]\n+\t\t\ts.divShift = m.shift\n+\t\t\ts.divMul = m.mul\n+\t\t\ts.divShift2 = m.shift2\n+\t\t\ts.baseMask = m.baseMask\n+\t\t}\n+\n+\t\t// update stats, sweep lists\n+\t\th.pagesInUse += uint64(npage)\n+\t\tif large {\n+\t\t\tmemstats.heap_objects++\n+\t\t\tatomic.Xadd64(&memstats.heap_live, int64(npage<<_PageShift))\n+\t\t\t// Swept spans are at the end of lists.\n+\t\t\tif s.npages < uintptr(len(h.free)) {\n+\t\t\t\th.busy[s.npages].insertBack(s)\n+\t\t\t} else {\n+\t\t\t\th.busylarge.insertBack(s)\n+\t\t\t}\n+\t\t}\n+\t}\n+\t// heap_scan and heap_live were updated.\n+\tif gcBlackenEnabled != 0 {\n+\t\tgcController.revise()\n+\t}\n+\n+\tif trace.enabled {\n+\t\ttraceHeapAlloc()\n+\t}\n+\n+\t// h.spans is accessed concurrently without synchronization\n+\t// from other threads. Hence, there must be a store/store\n+\t// barrier here to ensure the writes to h.spans above happen\n+\t// before the caller can publish a pointer p to an object\n+\t// allocated from s. As soon as this happens, the garbage\n+\t// collector running on another processor could read p and\n+\t// look up s in h.spans. The unlock acts as the barrier to\n+\t// order these writes. On the read side, the data dependency\n+\t// between p and the index in h.spans orders the reads.\n+\tunlock(&h.lock)\n+\treturn s\n+}\n+\n+func (h *mheap) alloc(npage uintptr, sizeclass int32, large bool, needzero bool) *mspan {\n+\t// Don't do any operations that lock the heap on the G stack.\n+\t// It might trigger stack growth, and the stack growth code needs\n+\t// to be able to allocate heap.\n+\tvar s *mspan\n+\tsystemstack(func() {\n+\t\ts = h.alloc_m(npage, sizeclass, large)\n+\t})\n+\n+\tif s != nil {\n+\t\tif needzero && s.needzero != 0 {\n+\t\t\tmemclrNoHeapPointers(unsafe.Pointer(s.base()), s.npages<<_PageShift)\n+\t\t}\n+\t\ts.needzero = 0\n+\t}\n+\treturn s\n+}\n+\n+func (h *mheap) allocStack(npage uintptr) *mspan {\n+\t_g_ := getg()\n+\tif _g_ != _g_.m.g0 {\n+\t\tthrow(\"mheap_allocstack not on g0 stack\")\n+\t}\n+\tlock(&h.lock)\n+\ts := h.allocSpanLocked(npage)\n+\tif s != nil {\n+\t\ts.state = _MSpanStack\n+\t\ts.stackfreelist = 0\n+\t\ts.allocCount = 0\n+\t\tmemstats.stacks_inuse += uint64(s.npages << _PageShift)\n+\t}\n+\n+\t// This unlock acts as a release barrier. See mHeap_Alloc_m.\n+\tunlock(&h.lock)\n+\treturn s\n+}\n+\n+// Allocates a span of the given size.  h must be locked.\n+// The returned span has been removed from the\n+// free list, but its state is still MSpanFree.\n+func (h *mheap) allocSpanLocked(npage uintptr) *mspan {\n+\tvar list *mSpanList\n+\tvar s *mspan\n+\n+\t// Try in fixed-size lists up to max.\n+\tfor i := int(npage); i < len(h.free); i++ {\n+\t\tlist = &h.free[i]\n+\t\tif !list.isEmpty() {\n+\t\t\ts = list.first\n+\t\t\tgoto HaveSpan\n+\t\t}\n+\t}\n+\n+\t// Best fit in list of large spans.\n+\tlist = &h.freelarge\n+\ts = h.allocLarge(npage)\n+\tif s == nil {\n+\t\tif !h.grow(npage) {\n+\t\t\treturn nil\n+\t\t}\n+\t\ts = h.allocLarge(npage)\n+\t\tif s == nil {\n+\t\t\treturn nil\n+\t\t}\n+\t}\n+\n+HaveSpan:\n+\t// Mark span in use.\n+\tif s.state != _MSpanFree {\n+\t\tthrow(\"MHeap_AllocLocked - MSpan not free\")\n+\t}\n+\tif s.npages < npage {\n+\t\tthrow(\"MHeap_AllocLocked - bad npages\")\n+\t}\n+\tlist.remove(s)\n+\tif s.inList() {\n+\t\tthrow(\"still in list\")\n+\t}\n+\tif s.npreleased > 0 {\n+\t\tsysUsed(unsafe.Pointer(s.base()), s.npages<<_PageShift)\n+\t\tmemstats.heap_released -= uint64(s.npreleased << _PageShift)\n+\t\ts.npreleased = 0\n+\t}\n+\n+\tif s.npages > npage {\n+\t\t// Trim extra and put it back in the heap.\n+\t\tt := (*mspan)(h.spanalloc.alloc())\n+\t\tt.init(s.base()+npage<<_PageShift, s.npages-npage)\n+\t\ts.npages = npage\n+\t\tp := (t.base() - h.arena_start) >> _PageShift\n+\t\tif p > 0 {\n+\t\t\th.spans[p-1] = s\n+\t\t}\n+\t\th.spans[p] = t\n+\t\th.spans[p+t.npages-1] = t\n+\t\tt.needzero = s.needzero\n+\t\ts.state = _MSpanStack // prevent coalescing with s\n+\t\tt.state = _MSpanStack\n+\t\th.freeSpanLocked(t, false, false, s.unusedsince)\n+\t\ts.state = _MSpanFree\n+\t}\n+\ts.unusedsince = 0\n+\n+\tp := (s.base() - h.arena_start) >> _PageShift\n+\tfor n := uintptr(0); n < npage; n++ {\n+\t\th.spans[p+n] = s\n+\t}\n+\n+\tmemstats.heap_inuse += uint64(npage << _PageShift)\n+\tmemstats.heap_idle -= uint64(npage << _PageShift)\n+\n+\t//println(\"spanalloc\", hex(s.start<<_PageShift))\n+\tif s.inList() {\n+\t\tthrow(\"still in list\")\n+\t}\n+\treturn s\n+}\n+\n+// Allocate a span of exactly npage pages from the list of large spans.\n+func (h *mheap) allocLarge(npage uintptr) *mspan {\n+\treturn bestFit(&h.freelarge, npage, nil)\n+}\n+\n+// Search list for smallest span with >= npage pages.\n+// If there are multiple smallest spans, take the one\n+// with the earliest starting address.\n+func bestFit(list *mSpanList, npage uintptr, best *mspan) *mspan {\n+\tfor s := list.first; s != nil; s = s.next {\n+\t\tif s.npages < npage {\n+\t\t\tcontinue\n+\t\t}\n+\t\tif best == nil || s.npages < best.npages || (s.npages == best.npages && s.base() < best.base()) {\n+\t\t\tbest = s\n+\t\t}\n+\t}\n+\treturn best\n+}\n+\n+// Try to add at least npage pages of memory to the heap,\n+// returning whether it worked.\n+//\n+// h must be locked.\n+func (h *mheap) grow(npage uintptr) bool {\n+\t// Ask for a big chunk, to reduce the number of mappings\n+\t// the operating system needs to track; also amortizes\n+\t// the overhead of an operating system mapping.\n+\t// Allocate a multiple of 64kB.\n+\tnpage = round(npage, (64<<10)/_PageSize)\n+\task := npage << _PageShift\n+\tif ask < _HeapAllocChunk {\n+\t\task = _HeapAllocChunk\n+\t}\n+\n+\tv := h.sysAlloc(ask)\n+\tif v == nil {\n+\t\tif ask > npage<<_PageShift {\n+\t\t\task = npage << _PageShift\n+\t\t\tv = h.sysAlloc(ask)\n+\t\t}\n+\t\tif v == nil {\n+\t\t\tprint(\"runtime: out of memory: cannot allocate \", ask, \"-byte block (\", memstats.heap_sys, \" in use)\\n\")\n+\t\t\treturn false\n+\t\t}\n+\t}\n+\n+\t// Create a fake \"in use\" span and free it, so that the\n+\t// right coalescing happens.\n+\ts := (*mspan)(h.spanalloc.alloc())\n+\ts.init(uintptr(v), ask>>_PageShift)\n+\tp := (s.base() - h.arena_start) >> _PageShift\n+\tfor i := p; i < p+s.npages; i++ {\n+\t\th.spans[i] = s\n+\t}\n+\tatomic.Store(&s.sweepgen, h.sweepgen)\n+\ts.state = _MSpanInUse\n+\th.pagesInUse += uint64(s.npages)\n+\th.freeSpanLocked(s, false, true, 0)\n+\treturn true\n+}\n+\n+// Look up the span at the given address.\n+// Address is guaranteed to be in map\n+// and is guaranteed to be start or end of span.\n+func (h *mheap) lookup(v unsafe.Pointer) *mspan {\n+\tp := uintptr(v)\n+\tp -= h.arena_start\n+\treturn h.spans[p>>_PageShift]\n+}\n+\n+// Look up the span at the given address.\n+// Address is *not* guaranteed to be in map\n+// and may be anywhere in the span.\n+// Map entries for the middle of a span are only\n+// valid for allocated spans. Free spans may have\n+// other garbage in their middles, so we have to\n+// check for that.\n+func (h *mheap) lookupMaybe(v unsafe.Pointer) *mspan {\n+\tif uintptr(v) < h.arena_start || uintptr(v) >= h.arena_used {\n+\t\treturn nil\n+\t}\n+\ts := h.spans[(uintptr(v)-h.arena_start)>>_PageShift]\n+\tif s == nil || uintptr(v) < s.base() || uintptr(v) >= uintptr(unsafe.Pointer(s.limit)) || s.state != _MSpanInUse {\n+\t\treturn nil\n+\t}\n+\treturn s\n+}\n+\n+// Free the span back into the heap.\n+func (h *mheap) freeSpan(s *mspan, acct int32) {\n+\tsystemstack(func() {\n+\t\tmp := getg().m\n+\t\tlock(&h.lock)\n+\t\tmemstats.heap_scan += uint64(mp.mcache.local_scan)\n+\t\tmp.mcache.local_scan = 0\n+\t\tmemstats.tinyallocs += uint64(mp.mcache.local_tinyallocs)\n+\t\tmp.mcache.local_tinyallocs = 0\n+\t\tif msanenabled {\n+\t\t\t// Tell msan that this entire span is no longer in use.\n+\t\t\tbase := unsafe.Pointer(s.base())\n+\t\t\tbytes := s.npages << _PageShift\n+\t\t\tmsanfree(base, bytes)\n+\t\t}\n+\t\tif acct != 0 {\n+\t\t\tmemstats.heap_objects--\n+\t\t}\n+\t\tif gcBlackenEnabled != 0 {\n+\t\t\t// heap_scan changed.\n+\t\t\tgcController.revise()\n+\t\t}\n+\t\th.freeSpanLocked(s, true, true, 0)\n+\t\tunlock(&h.lock)\n+\t})\n+}\n+\n+func (h *mheap) freeStack(s *mspan) {\n+\t_g_ := getg()\n+\tif _g_ != _g_.m.g0 {\n+\t\tthrow(\"mheap_freestack not on g0 stack\")\n+\t}\n+\ts.needzero = 1\n+\tlock(&h.lock)\n+\tmemstats.stacks_inuse -= uint64(s.npages << _PageShift)\n+\th.freeSpanLocked(s, true, true, 0)\n+\tunlock(&h.lock)\n+}\n+\n+// s must be on a busy list (h.busy or h.busylarge) or unlinked.\n+func (h *mheap) freeSpanLocked(s *mspan, acctinuse, acctidle bool, unusedsince int64) {\n+\tswitch s.state {\n+\tcase _MSpanStack:\n+\t\tif s.allocCount != 0 {\n+\t\t\tthrow(\"MHeap_FreeSpanLocked - invalid stack free\")\n+\t\t}\n+\tcase _MSpanInUse:\n+\t\tif s.allocCount != 0 || s.sweepgen != h.sweepgen {\n+\t\t\tprint(\"MHeap_FreeSpanLocked - span \", s, \" ptr \", hex(s.base()), \" allocCount \", s.allocCount, \" sweepgen \", s.sweepgen, \"/\", h.sweepgen, \"\\n\")\n+\t\t\tthrow(\"MHeap_FreeSpanLocked - invalid free\")\n+\t\t}\n+\t\th.pagesInUse -= uint64(s.npages)\n+\tdefault:\n+\t\tthrow(\"MHeap_FreeSpanLocked - invalid span state\")\n+\t}\n+\n+\tif acctinuse {\n+\t\tmemstats.heap_inuse -= uint64(s.npages << _PageShift)\n+\t}\n+\tif acctidle {\n+\t\tmemstats.heap_idle += uint64(s.npages << _PageShift)\n+\t}\n+\ts.state = _MSpanFree\n+\tif s.inList() {\n+\t\th.busyList(s.npages).remove(s)\n+\t}\n+\n+\t// Stamp newly unused spans. The scavenger will use that\n+\t// info to potentially give back some pages to the OS.\n+\ts.unusedsince = unusedsince\n+\tif unusedsince == 0 {\n+\t\ts.unusedsince = nanotime()\n+\t}\n+\ts.npreleased = 0\n+\n+\t// Coalesce with earlier, later spans.\n+\tp := (s.base() - h.arena_start) >> _PageShift\n+\tif p > 0 {\n+\t\tt := h.spans[p-1]\n+\t\tif t != nil && t.state == _MSpanFree {\n+\t\t\ts.startAddr = t.startAddr\n+\t\t\ts.npages += t.npages\n+\t\t\ts.npreleased = t.npreleased // absorb released pages\n+\t\t\ts.needzero |= t.needzero\n+\t\t\tp -= t.npages\n+\t\t\th.spans[p] = s\n+\t\t\th.freeList(t.npages).remove(t)\n+\t\t\tt.state = _MSpanDead\n+\t\t\th.spanalloc.free(unsafe.Pointer(t))\n+\t\t}\n+\t}\n+\tif (p + s.npages) < uintptr(len(h.spans)) {\n+\t\tt := h.spans[p+s.npages]\n+\t\tif t != nil && t.state == _MSpanFree {\n+\t\t\ts.npages += t.npages\n+\t\t\ts.npreleased += t.npreleased\n+\t\t\ts.needzero |= t.needzero\n+\t\t\th.spans[p+s.npages-1] = s\n+\t\t\th.freeList(t.npages).remove(t)\n+\t\t\tt.state = _MSpanDead\n+\t\t\th.spanalloc.free(unsafe.Pointer(t))\n+\t\t}\n+\t}\n+\n+\t// Insert s into appropriate list.\n+\th.freeList(s.npages).insert(s)\n+}\n+\n+func (h *mheap) freeList(npages uintptr) *mSpanList {\n+\tif npages < uintptr(len(h.free)) {\n+\t\treturn &h.free[npages]\n+\t}\n+\treturn &h.freelarge\n+}\n+\n+func (h *mheap) busyList(npages uintptr) *mSpanList {\n+\tif npages < uintptr(len(h.free)) {\n+\t\treturn &h.busy[npages]\n+\t}\n+\treturn &h.busylarge\n+}\n+\n+func scavengelist(list *mSpanList, now, limit uint64) uintptr {\n+\tif list.isEmpty() {\n+\t\treturn 0\n+\t}\n+\n+\tvar sumreleased uintptr\n+\tfor s := list.first; s != nil; s = s.next {\n+\t\tif (now-uint64(s.unusedsince)) > limit && s.npreleased != s.npages {\n+\t\t\tstart := s.base()\n+\t\t\tend := start + s.npages<<_PageShift\n+\t\t\tif physPageSize > _PageSize {\n+\t\t\t\t// We can only release pages in\n+\t\t\t\t// physPageSize blocks, so round start\n+\t\t\t\t// and end in. (Otherwise, madvise\n+\t\t\t\t// will round them *out* and release\n+\t\t\t\t// more memory than we want.)\n+\t\t\t\tstart = (start + physPageSize - 1) &^ (physPageSize - 1)\n+\t\t\t\tend &^= physPageSize - 1\n+\t\t\t\tif end <= start {\n+\t\t\t\t\t// start and end don't span a\n+\t\t\t\t\t// whole physical page.\n+\t\t\t\t\tcontinue\n+\t\t\t\t}\n+\t\t\t}\n+\t\t\tlen := end - start\n+\n+\t\t\treleased := len - (s.npreleased << _PageShift)\n+\t\t\tif physPageSize > _PageSize && released == 0 {\n+\t\t\t\tcontinue\n+\t\t\t}\n+\t\t\tmemstats.heap_released += uint64(released)\n+\t\t\tsumreleased += released\n+\t\t\ts.npreleased = len >> _PageShift\n+\t\t\tsysUnused(unsafe.Pointer(start), len)\n+\t\t}\n+\t}\n+\treturn sumreleased\n+}\n+\n+func (h *mheap) scavenge(k int32, now, limit uint64) {\n+\tlock(&h.lock)\n+\tvar sumreleased uintptr\n+\tfor i := 0; i < len(h.free); i++ {\n+\t\tsumreleased += scavengelist(&h.free[i], now, limit)\n+\t}\n+\tsumreleased += scavengelist(&h.freelarge, now, limit)\n+\tunlock(&h.lock)\n+\n+\tif debug.gctrace > 0 {\n+\t\tif sumreleased > 0 {\n+\t\t\tprint(\"scvg\", k, \": \", sumreleased>>20, \" MB released\\n\")\n+\t\t}\n+\t\t// TODO(dvyukov): these stats are incorrect as we don't subtract stack usage from heap.\n+\t\t// But we can't call ReadMemStats on g0 holding locks.\n+\t\tprint(\"scvg\", k, \": inuse: \", memstats.heap_inuse>>20, \", idle: \", memstats.heap_idle>>20, \", sys: \", memstats.heap_sys>>20, \", released: \", memstats.heap_released>>20, \", consumed: \", (memstats.heap_sys-memstats.heap_released)>>20, \" (MB)\\n\")\n+\t}\n+}\n+\n+//go:linkname runtime_debug_freeOSMemory runtime_debug.freeOSMemory\n+func runtime_debug_freeOSMemory() {\n+\tgcStart(gcForceBlockMode, false)\n+\tsystemstack(func() { mheap_.scavenge(-1, ^uint64(0), 0) })\n+}\n+\n+// Initialize a new span with the given start and npages.\n+func (span *mspan) init(base uintptr, npages uintptr) {\n+\t// span is *not* zeroed.\n+\tspan.next = nil\n+\tspan.prev = nil\n+\tspan.list = nil\n+\tspan.startAddr = base\n+\tspan.npages = npages\n+\tspan.allocCount = 0\n+\tspan.sizeclass = 0\n+\tspan.incache = false\n+\tspan.elemsize = 0\n+\tspan.state = _MSpanDead\n+\tspan.unusedsince = 0\n+\tspan.npreleased = 0\n+\tspan.speciallock.key = 0\n+\tspan.specials = nil\n+\tspan.needzero = 0\n+\tspan.freeindex = 0\n+\tspan.allocBits = nil\n+\tspan.gcmarkBits = nil\n+}\n+\n+func (span *mspan) inList() bool {\n+\treturn span.list != nil\n+}\n+\n+// Initialize an empty doubly-linked list.\n+func (list *mSpanList) init() {\n+\tlist.first = nil\n+\tlist.last = nil\n+}\n+\n+func (list *mSpanList) remove(span *mspan) {\n+\tif span.list != list {\n+\t\tprintln(\"runtime: failed MSpanList_Remove\", span, span.prev, span.list, list)\n+\t\tthrow(\"MSpanList_Remove\")\n+\t}\n+\tif list.first == span {\n+\t\tlist.first = span.next\n+\t} else {\n+\t\tspan.prev.next = span.next\n+\t}\n+\tif list.last == span {\n+\t\tlist.last = span.prev\n+\t} else {\n+\t\tspan.next.prev = span.prev\n+\t}\n+\tspan.next = nil\n+\tspan.prev = nil\n+\tspan.list = nil\n+}\n+\n+func (list *mSpanList) isEmpty() bool {\n+\treturn list.first == nil\n+}\n+\n+func (list *mSpanList) insert(span *mspan) {\n+\tif span.next != nil || span.prev != nil || span.list != nil {\n+\t\tprintln(\"runtime: failed MSpanList_Insert\", span, span.next, span.prev, span.list)\n+\t\tthrow(\"MSpanList_Insert\")\n+\t}\n+\tspan.next = list.first\n+\tif list.first != nil {\n+\t\t// The list contains at least one span; link it in.\n+\t\t// The last span in the list doesn't change.\n+\t\tlist.first.prev = span\n+\t} else {\n+\t\t// The list contains no spans, so this is also the last span.\n+\t\tlist.last = span\n+\t}\n+\tlist.first = span\n+\tspan.list = list\n+}\n+\n+func (list *mSpanList) insertBack(span *mspan) {\n+\tif span.next != nil || span.prev != nil || span.list != nil {\n+\t\tprintln(\"failed MSpanList_InsertBack\", span, span.next, span.prev, span.list)\n+\t\tthrow(\"MSpanList_InsertBack\")\n+\t}\n+\tspan.prev = list.last\n+\tif list.last != nil {\n+\t\t// The list contains at least one span.\n+\t\tlist.last.next = span\n+\t} else {\n+\t\t// The list contains no spans, so this is also the first span.\n+\t\tlist.first = span\n+\t}\n+\tlist.last = span\n+\tspan.list = list\n+}\n+\n+const (\n+\t_KindSpecialFinalizer = 1\n+\t_KindSpecialProfile   = 2\n+\t// Note: The finalizer special must be first because if we're freeing\n+\t// an object, a finalizer special will cause the freeing operation\n+\t// to abort, and we want to keep the other special records around\n+\t// if that happens.\n+)\n+\n+//go:notinheap\n+type special struct {\n+\tnext   *special // linked list in span\n+\toffset uint16   // span offset of object\n+\tkind   byte     // kind of special\n+}\n+\n+// Adds the special record s to the list of special records for\n+// the object p. All fields of s should be filled in except for\n+// offset & next, which this routine will fill in.\n+// Returns true if the special was successfully added, false otherwise.\n+// (The add will fail only if a record with the same p and s->kind\n+//  already exists.)\n+func addspecial(p unsafe.Pointer, s *special) bool {\n+\tspan := mheap_.lookupMaybe(p)\n+\tif span == nil {\n+\t\tthrow(\"addspecial on invalid pointer\")\n+\t}\n+\n+\t// Ensure that the span is swept.\n+\t// Sweeping accesses the specials list w/o locks, so we have\n+\t// to synchronize with it. And it's just much safer.\n+\tmp := acquirem()\n+\tspan.ensureSwept()\n+\n+\toffset := uintptr(p) - span.base()\n+\tkind := s.kind\n+\n+\tlock(&span.speciallock)\n+\n+\t// Find splice point, check for existing record.\n+\tt := &span.specials\n+\tfor {\n+\t\tx := *t\n+\t\tif x == nil {\n+\t\t\tbreak\n+\t\t}\n+\t\tif offset == uintptr(x.offset) && kind == x.kind {\n+\t\t\tunlock(&span.speciallock)\n+\t\t\treleasem(mp)\n+\t\t\treturn false // already exists\n+\t\t}\n+\t\tif offset < uintptr(x.offset) || (offset == uintptr(x.offset) && kind < x.kind) {\n+\t\t\tbreak\n+\t\t}\n+\t\tt = &x.next\n+\t}\n+\n+\t// Splice in record, fill in offset.\n+\ts.offset = uint16(offset)\n+\ts.next = *t\n+\t*t = s\n+\tunlock(&span.speciallock)\n+\treleasem(mp)\n+\n+\treturn true\n+}\n+\n+// Removes the Special record of the given kind for the object p.\n+// Returns the record if the record existed, nil otherwise.\n+// The caller must FixAlloc_Free the result.\n+func removespecial(p unsafe.Pointer, kind uint8) *special {\n+\tspan := mheap_.lookupMaybe(p)\n+\tif span == nil {\n+\t\tthrow(\"removespecial on invalid pointer\")\n+\t}\n+\n+\t// Ensure that the span is swept.\n+\t// Sweeping accesses the specials list w/o locks, so we have\n+\t// to synchronize with it. And it's just much safer.\n+\tmp := acquirem()\n+\tspan.ensureSwept()\n+\n+\toffset := uintptr(p) - span.base()\n+\n+\tlock(&span.speciallock)\n+\tt := &span.specials\n+\tfor {\n+\t\ts := *t\n+\t\tif s == nil {\n+\t\t\tbreak\n+\t\t}\n+\t\t// This function is used for finalizers only, so we don't check for\n+\t\t// \"interior\" specials (p must be exactly equal to s->offset).\n+\t\tif offset == uintptr(s.offset) && kind == s.kind {\n+\t\t\t*t = s.next\n+\t\t\tunlock(&span.speciallock)\n+\t\t\treleasem(mp)\n+\t\t\treturn s\n+\t\t}\n+\t\tt = &s.next\n+\t}\n+\tunlock(&span.speciallock)\n+\treleasem(mp)\n+\treturn nil\n+}\n+\n+// The described object has a finalizer set for it.\n+//\n+// specialfinalizer is allocated from non-GC'd memory, so any heap\n+// pointers must be specially handled.\n+//\n+//go:notinheap\n+type specialfinalizer struct {\n+\tspecial special\n+\tfn      *funcval  // May be a heap pointer.\n+\tft      *functype // May be a heap pointer, but always live.\n+\tot      *ptrtype  // May be a heap pointer, but always live.\n+}\n+\n+// Adds a finalizer to the object p. Returns true if it succeeded.\n+func addfinalizer(p unsafe.Pointer, f *funcval, ft *functype, ot *ptrtype) bool {\n+\tlock(&mheap_.speciallock)\n+\ts := (*specialfinalizer)(mheap_.specialfinalizeralloc.alloc())\n+\tunlock(&mheap_.speciallock)\n+\ts.special.kind = _KindSpecialFinalizer\n+\ts.fn = f\n+\ts.ft = ft\n+\ts.ot = ot\n+\tif addspecial(p, &s.special) {\n+\t\t// This is responsible for maintaining the same\n+\t\t// GC-related invariants as markrootSpans in any\n+\t\t// situation where it's possible that markrootSpans\n+\t\t// has already run but mark termination hasn't yet.\n+\t\tif gcphase != _GCoff {\n+\t\t\t_, base, _ := findObject(p)\n+\t\t\tmp := acquirem()\n+\t\t\tgcw := &mp.p.ptr().gcw\n+\t\t\t// Mark everything reachable from the object\n+\t\t\t// so it's retained for the finalizer.\n+\t\t\tscanobject(uintptr(base), gcw)\n+\t\t\t// Mark the finalizer itself, since the\n+\t\t\t// special isn't part of the GC'd heap.\n+\t\t\tscanblock(uintptr(unsafe.Pointer(&s.fn)), sys.PtrSize, &oneptrmask[0], gcw)\n+\t\t\tif gcBlackenPromptly {\n+\t\t\t\tgcw.dispose()\n+\t\t\t}\n+\t\t\treleasem(mp)\n+\t\t}\n+\t\treturn true\n+\t}\n+\n+\t// There was an old finalizer\n+\tlock(&mheap_.speciallock)\n+\tmheap_.specialfinalizeralloc.free(unsafe.Pointer(s))\n+\tunlock(&mheap_.speciallock)\n+\treturn false\n+}\n+\n+// Removes the finalizer (if any) from the object p.\n+func removefinalizer(p unsafe.Pointer) {\n+\ts := (*specialfinalizer)(unsafe.Pointer(removespecial(p, _KindSpecialFinalizer)))\n+\tif s == nil {\n+\t\treturn // there wasn't a finalizer to remove\n+\t}\n+\tlock(&mheap_.speciallock)\n+\tmheap_.specialfinalizeralloc.free(unsafe.Pointer(s))\n+\tunlock(&mheap_.speciallock)\n+}\n+\n+// The described object is being heap profiled.\n+//\n+//go:notinheap\n+type specialprofile struct {\n+\tspecial special\n+\tb       *bucket\n+}\n+\n+// Set the heap profile bucket associated with addr to b.\n+func setprofilebucket(p unsafe.Pointer, b *bucket) {\n+\tlock(&mheap_.speciallock)\n+\ts := (*specialprofile)(mheap_.specialprofilealloc.alloc())\n+\tunlock(&mheap_.speciallock)\n+\ts.special.kind = _KindSpecialProfile\n+\ts.b = b\n+\tif !addspecial(p, &s.special) {\n+\t\tthrow(\"setprofilebucket: profile already set\")\n+\t}\n+}\n+\n+// Do whatever cleanup needs to be done to deallocate s. It has\n+// already been unlinked from the MSpan specials list.\n+func freespecial(s *special, p unsafe.Pointer, size uintptr) {\n+\tswitch s.kind {\n+\tcase _KindSpecialFinalizer:\n+\t\tsf := (*specialfinalizer)(unsafe.Pointer(s))\n+\t\tqueuefinalizer(p, sf.fn, sf.ft, sf.ot)\n+\t\tlock(&mheap_.speciallock)\n+\t\tmheap_.specialfinalizeralloc.free(unsafe.Pointer(sf))\n+\t\tunlock(&mheap_.speciallock)\n+\tcase _KindSpecialProfile:\n+\t\tsp := (*specialprofile)(unsafe.Pointer(s))\n+\t\tmProf_Free(sp.b, size)\n+\t\tlock(&mheap_.speciallock)\n+\t\tmheap_.specialprofilealloc.free(unsafe.Pointer(sp))\n+\t\tunlock(&mheap_.speciallock)\n+\tdefault:\n+\t\tthrow(\"bad special kind\")\n+\t\tpanic(\"not reached\")\n+\t}\n+}\n+\n+const gcBitsChunkBytes = uintptr(64 << 10)\n+const gcBitsHeaderBytes = unsafe.Sizeof(gcBitsHeader{})\n+\n+type gcBitsHeader struct {\n+\tfree uintptr // free is the index into bits of the next free byte.\n+\tnext uintptr // *gcBits triggers recursive type bug. (issue 14620)\n+}\n+\n+//go:notinheap\n+type gcBits struct {\n+\t// gcBitsHeader // side step recursive type bug (issue 14620) by including fields by hand.\n+\tfree uintptr // free is the index into bits of the next free byte.\n+\tnext *gcBits\n+\tbits [gcBitsChunkBytes - gcBitsHeaderBytes]uint8\n+}\n+\n+var gcBitsArenas struct {\n+\tlock     mutex\n+\tfree     *gcBits\n+\tnext     *gcBits\n+\tcurrent  *gcBits\n+\tprevious *gcBits\n+}\n+\n+// newMarkBits returns a pointer to 8 byte aligned bytes\n+// to be used for a span's mark bits.\n+func newMarkBits(nelems uintptr) *uint8 {\n+\tlock(&gcBitsArenas.lock)\n+\tblocksNeeded := uintptr((nelems + 63) / 64)\n+\tbytesNeeded := blocksNeeded * 8\n+\tif gcBitsArenas.next == nil ||\n+\t\tgcBitsArenas.next.free+bytesNeeded > uintptr(len(gcBits{}.bits)) {\n+\t\t// Allocate a new arena.\n+\t\tfresh := newArena()\n+\t\tfresh.next = gcBitsArenas.next\n+\t\tgcBitsArenas.next = fresh\n+\t}\n+\tif gcBitsArenas.next.free >= gcBitsChunkBytes {\n+\t\tprintln(\"runtime: gcBitsArenas.next.free=\", gcBitsArenas.next.free, gcBitsChunkBytes)\n+\t\tthrow(\"markBits overflow\")\n+\t}\n+\tresult := &gcBitsArenas.next.bits[gcBitsArenas.next.free]\n+\tgcBitsArenas.next.free += bytesNeeded\n+\tunlock(&gcBitsArenas.lock)\n+\treturn result\n+}\n+\n+// newAllocBits returns a pointer to 8 byte aligned bytes\n+// to be used for this span's alloc bits.\n+// newAllocBits is used to provide newly initialized spans\n+// allocation bits. For spans not being initialized the\n+// the mark bits are repurposed as allocation bits when\n+// the span is swept.\n+func newAllocBits(nelems uintptr) *uint8 {\n+\treturn newMarkBits(nelems)\n+}\n+\n+// nextMarkBitArenaEpoch establishes a new epoch for the arenas\n+// holding the mark bits. The arenas are named relative to the\n+// current GC cycle which is demarcated by the call to finishweep_m.\n+//\n+// All current spans have been swept.\n+// During that sweep each span allocated room for its gcmarkBits in\n+// gcBitsArenas.next block. gcBitsArenas.next becomes the gcBitsArenas.current\n+// where the GC will mark objects and after each span is swept these bits\n+// will be used to allocate objects.\n+// gcBitsArenas.current becomes gcBitsArenas.previous where the span's\n+// gcAllocBits live until all the spans have been swept during this GC cycle.\n+// The span's sweep extinguishes all the references to gcBitsArenas.previous\n+// by pointing gcAllocBits into the gcBitsArenas.current.\n+// The gcBitsArenas.previous is released to the gcBitsArenas.free list.\n+func nextMarkBitArenaEpoch() {\n+\tlock(&gcBitsArenas.lock)\n+\tif gcBitsArenas.previous != nil {\n+\t\tif gcBitsArenas.free == nil {\n+\t\t\tgcBitsArenas.free = gcBitsArenas.previous\n+\t\t} else {\n+\t\t\t// Find end of previous arenas.\n+\t\t\tlast := gcBitsArenas.previous\n+\t\t\tfor last = gcBitsArenas.previous; last.next != nil; last = last.next {\n+\t\t\t}\n+\t\t\tlast.next = gcBitsArenas.free\n+\t\t\tgcBitsArenas.free = gcBitsArenas.previous\n+\t\t}\n+\t}\n+\tgcBitsArenas.previous = gcBitsArenas.current\n+\tgcBitsArenas.current = gcBitsArenas.next\n+\tgcBitsArenas.next = nil // newMarkBits calls newArena when needed\n+\tunlock(&gcBitsArenas.lock)\n+}\n+\n+// newArena allocates and zeroes a gcBits arena.\n+func newArena() *gcBits {\n+\tvar result *gcBits\n+\tif gcBitsArenas.free == nil {\n+\t\tresult = (*gcBits)(sysAlloc(gcBitsChunkBytes, &memstats.gc_sys))\n+\t\tif result == nil {\n+\t\t\tthrow(\"runtime: cannot allocate memory\")\n+\t\t}\n+\t} else {\n+\t\tresult = gcBitsArenas.free\n+\t\tgcBitsArenas.free = gcBitsArenas.free.next\n+\t\tmemclrNoHeapPointers(unsafe.Pointer(result), gcBitsChunkBytes)\n+\t}\n+\tresult.next = nil\n+\t// If result.bits is not 8 byte aligned adjust index so\n+\t// that &result.bits[result.free] is 8 byte aligned.\n+\tif uintptr(unsafe.Offsetof(gcBits{}.bits))&7 == 0 {\n+\t\tresult.free = 0\n+\t} else {\n+\t\tresult.free = 8 - (uintptr(unsafe.Pointer(&result.bits[0])) & 7)\n+\t}\n+\treturn result\n+}"}, {"sha": "87f84a72acdbbf50ff463bfbfc04c1cee3e775c3", "filename": "libgo/go/runtime/mprof.go", "status": "modified", "additions": 0, "deletions": 9, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmprof.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmprof.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmprof.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -12,15 +12,6 @@ import (\n \t\"unsafe\"\n )\n \n-// Export temporarily for gccgo's C code to call:\n-//go:linkname mProf_Malloc runtime.mProf_Malloc\n-//go:linkname mProf_Free runtime.mProf_Free\n-//go:linkname mProf_GC runtime.mProf_GC\n-//go:linkname tracealloc runtime.tracealloc\n-//go:linkname tracefree runtime.tracefree\n-//go:linkname tracegc runtime.tracegc\n-//go:linkname iterate_memprof runtime.iterate_memprof\n-\n // NOTE(rsc): Everything here could use cas if contention became an issue.\n var proflock mutex\n "}, {"sha": "438c9875135f4f80976ade0e9c2004af9cde958e", "filename": "libgo/go/runtime/msize.go", "status": "added", "additions": 47, "deletions": 0, "changes": 47, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmsize.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmsize.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmsize.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -0,0 +1,47 @@\n+// Copyright 2009 The Go Authors. All rights reserved.\n+// Use of this source code is governed by a BSD-style\n+// license that can be found in the LICENSE file.\n+\n+// Malloc small size classes.\n+//\n+// See malloc.go for overview.\n+// See also mksizeclasses.go for how we decide what size classes to use.\n+\n+package runtime\n+\n+// sizeToClass(0 <= n <= MaxSmallSize) returns the size class,\n+//\t1 <= sizeclass < NumSizeClasses, for n.\n+//\tSize class 0 is reserved to mean \"not small\".\n+//\n+// The sizeToClass lookup is implemented using two arrays,\n+// one mapping sizes <= 1024 to their class and one mapping\n+// sizes >= 1024 and <= MaxSmallSize to their class.\n+// All objects are 8-aligned, so the first array is indexed by\n+// the size divided by 8 (rounded up).  Objects >= 1024 bytes\n+// are 128-aligned, so the second array is indexed by the\n+// size divided by 128 (rounded up).  The arrays are constants\n+// in sizeclass.go generated by mksizeclass.go.\n+func sizeToClass(size uint32) uint32 {\n+\tif size > _MaxSmallSize {\n+\t\tthrow(\"invalid size\")\n+\t}\n+\tif size > smallSizeMax-8 {\n+\t\treturn uint32(size_to_class128[(size-smallSizeMax+largeSizeDiv-1)/largeSizeDiv])\n+\t}\n+\treturn uint32(size_to_class8[(size+smallSizeDiv-1)/smallSizeDiv])\n+}\n+\n+// Returns size of the memory block that mallocgc will allocate if you ask for the size.\n+func roundupsize(size uintptr) uintptr {\n+\tif size < _MaxSmallSize {\n+\t\tif size <= smallSizeMax-8 {\n+\t\t\treturn uintptr(class_to_size[size_to_class8[(size+smallSizeDiv-1)/smallSizeDiv]])\n+\t\t} else {\n+\t\t\treturn uintptr(class_to_size[size_to_class128[(size-smallSizeMax+largeSizeDiv-1)/largeSizeDiv]])\n+\t\t}\n+\t}\n+\tif size+_PageSize < size {\n+\t\treturn size\n+\t}\n+\treturn round(size, _PageSize)\n+}"}, {"sha": "aa3cfef0e1069c565f1b63f778266b5d822fc899", "filename": "libgo/go/runtime/mstats.go", "status": "modified", "additions": 1, "deletions": 7, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmstats.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fmstats.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fmstats.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -467,10 +467,7 @@ func readmemstats_m(stats *MemStats) {\n // For gccgo this is in runtime/mgc0.c.\n func updatememstats(stats *gcstats)\n \n-/*\n-For gccgo these are still in runtime/mgc0.c.\n-\n-//go:linkname readGCStats runtime/debug.readGCStats\n+//go:linkname readGCStats runtime_debug.readGCStats\n func readGCStats(pauses *[]uint64) {\n \tsystemstack(func() {\n \t\treadGCStats_m(pauses)\n@@ -618,7 +615,6 @@ func flushmcache(i int) {\n \t\treturn\n \t}\n \tc.releaseAll()\n-\tstackcache_clear(c)\n }\n \n // flushallmcaches flushes the mcaches of all Ps.\n@@ -652,8 +648,6 @@ func purgecachedstats(c *mcache) {\n \t}\n }\n \n-*/\n-\n // Atomically increases a given *system* memory stat. We are counting on this\n // stat never overflowing a uintptr, so this function must only be used for\n // system memory stats."}, {"sha": "8932455a19a3f464e91ee8cae3223c99496102cb", "filename": "libgo/go/runtime/netpoll.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fnetpoll.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fnetpoll.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fnetpoll.go?ref=f163907e0afadfa09f23320c51b90927c771c106", "patch": "@@ -2,7 +2,7 @@\n // Use of this source code is governed by a BSD-style\n // license that can be found in the LICENSE file.\n \n-// +build darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n+// +build aix darwin dragonfly freebsd linux nacl netbsd openbsd solaris windows\n \n package runtime\n "}, {"sha": "e40dfb635263771b80426705a6b4283cacb46d2f", "filename": "libgo/go/runtime/netpoll_aix.go", "status": "added", "additions": 0, "deletions": 0, "changes": 0, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fnetpoll_aix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fnetpoll_aix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fnetpoll_aix.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "246b9c3c944af16d44dffb0094296f0c7a5016f9", "filename": "libgo/go/runtime/os_aix.go", "status": "added", "additions": 98, "deletions": 0, "changes": 98, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fos_aix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fos_aix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fos_aix.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "358a38bd3a3cdb6c1d5cac0da8a675748ba553bd", "filename": "libgo/go/runtime/os_gccgo.go", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fos_gccgo.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fos_gccgo.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fos_gccgo.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "e1a6a308cf2d059ea39743c03bedd4414f7d27d9", "filename": "libgo/go/runtime/os_linux.go", "status": "modified", "additions": 0, "deletions": 3, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fos_linux.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fos_linux.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fos_linux.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "aa196aee7b564e52d93fab76e94a56bc68ba5616", "filename": "libgo/go/runtime/panic.go", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fpanic.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fpanic.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fpanic.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "5ebd46b198fd555909107ee3cecac174ca296f40", "filename": "libgo/go/runtime/pprof/mprof_test.go", "status": "modified", "additions": 4, "deletions": 2, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fpprof%2Fmprof_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fpprof%2Fmprof_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fpprof%2Fmprof_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "b28e26bec6dc4b139954478cdf5bb12210fde0bc", "filename": "libgo/go/runtime/proc.go", "status": "modified", "additions": 1247, "deletions": 38, "changes": 1285, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fproc.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fproc.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fproc.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "58710de406c3e8f6b6acdd5d81551f014f882cf8", "filename": "libgo/go/runtime/runtime.go", "status": "modified", "additions": 7, "deletions": 10, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fruntime.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fruntime.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fruntime.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "dd3f7b277a63fe05c9c7bdf293ded1437cd65543", "filename": "libgo/go/runtime/runtime1.go", "status": "modified", "additions": 16, "deletions": 13, "changes": 29, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fruntime1.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fruntime1.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fruntime1.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "22847eaf59850f44c34e1ba323f47075fe580619", "filename": "libgo/go/runtime/runtime2.go", "status": "modified", "additions": 10, "deletions": 14, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fruntime2.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fruntime2.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fruntime2.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "b0cbbbe3e60d409155e08059019754fc9a81fcb4", "filename": "libgo/go/runtime/runtime_unix_test.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fruntime_unix_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fruntime_unix_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fruntime_unix_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "056be36a729716c739a1a3415b37e8938852a405", "filename": "libgo/go/runtime/signal_gccgo.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fsignal_gccgo.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fsignal_gccgo.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fsignal_gccgo.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "b71b21e1d5c35b9724d3f22d531772ee444ddfbf", "filename": "libgo/go/runtime/signal_sighandler.go", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fsignal_sighandler.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fsignal_sighandler.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fsignal_sighandler.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "c8713b6770ac0f5632075ebfe50f1e55051f523d", "filename": "libgo/go/runtime/signal_unix.go", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fsignal_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fsignal_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fsignal_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "f61f85e0fcb032f4ccf2058e4752d363e855a69c", "filename": "libgo/go/runtime/slice.go", "status": "modified", "additions": 4, "deletions": 14, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fslice.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fslice.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fslice.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "a3d091811d5d0c99aaa95ff78c7d05b44990493e", "filename": "libgo/go/runtime/stubs.go", "status": "modified", "additions": 57, "deletions": 239, "changes": 296, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fstubs.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fstubs.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fstubs.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "490405d51fdb6ee5a0b477e4314b3d88a200285c", "filename": "libgo/go/runtime/stubs2.go", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fstubs2.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fstubs2.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fstubs2.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "bad03471c46b07f6c60037b52f550cd1015b392a", "filename": "libgo/go/runtime/symtab.go", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fsymtab.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Fsymtab.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fsymtab.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "d060e09162751a442810fe1fcf718ca3fd42c1bc", "filename": "libgo/go/runtime/traceback_gccgo.go", "status": "modified", "additions": 46, "deletions": 3, "changes": 49, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Ftraceback_gccgo.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Ftraceback_gccgo.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Ftraceback_gccgo.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "6788f2425e1323066746b0c496c50fb579f499f4", "filename": "libgo/go/runtime/type.go", "status": "modified", "additions": 4, "deletions": 3, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Ftype.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fruntime%2Ftype.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Ftype.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "eb93e2e68708c7708c8faf5d3791dcd8d3de01dc", "filename": "libgo/go/syscall/env_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fenv_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fenv_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Fenv_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "80991ec5c5d52ae0bbd79804ee935c13b7c3738f", "filename": "libgo/go/syscall/exec_bsd.go", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fexec_bsd.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fexec_bsd.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Fexec_bsd.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "f2bc74144082bcf059c8719540ef6c6fa4711542", "filename": "libgo/go/syscall/exec_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fexec_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fexec_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Fexec_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "58708da19ddc7e50f218596912f2ceadea5a7b06", "filename": "libgo/go/syscall/exec_unix_test.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fexec_unix_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fexec_unix_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Fexec_unix_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "120500c3c883e6993e457c086d774333ddb7dfdb", "filename": "libgo/go/syscall/export_unix_test.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fexport_unix_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fexport_unix_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Fexport_unix_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "992eeb4d46f4d01139b9dd72de8772f641d69dd8", "filename": "libgo/go/syscall/libcall_aix.go", "status": "added", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Flibcall_aix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Flibcall_aix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Flibcall_aix.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "9b137355e011c45f5c795f63f1bdc7986385f7a5", "filename": "libgo/go/syscall/libcall_posix_largefile.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Flibcall_posix_largefile.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Flibcall_posix_largefile.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Flibcall_posix_largefile.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "5b8f75a746b66fc391dcd420176a688904e12412", "filename": "libgo/go/syscall/libcall_posix_regfile.go", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Flibcall_posix_regfile.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Flibcall_posix_regfile.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Flibcall_posix_regfile.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "372b0d75eb260ff971d4f1f01b9de1c679a856e7", "filename": "libgo/go/syscall/libcall_posix_utimesnano.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Flibcall_posix_utimesnano.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Flibcall_posix_utimesnano.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Flibcall_posix_utimesnano.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "00b6874d1ad7f9351e2d93626662f049e2fcda6e", "filename": "libgo/go/syscall/libcall_wait4.go", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Flibcall_wait4.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Flibcall_wait4.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Flibcall_wait4.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "9c25d04d8c274064d581bacb8d7c04ebb0ba7fbb", "filename": "libgo/go/syscall/libcall_wait4_aix.go", "status": "added", "additions": 26, "deletions": 0, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Flibcall_wait4_aix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Flibcall_wait4_aix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Flibcall_wait4_aix.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "d0b3644b59cb25992b432f3f3de02d37419b4d35", "filename": "libgo/go/syscall/mmap_unix_test.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fmmap_unix_test.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fmmap_unix_test.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Fmmap_unix_test.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "dca92a909a2547f26413df2fea3a2b8611fb3b2a", "filename": "libgo/go/syscall/signame.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fsigname.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fsigname.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Fsigname.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "c01602f718ac997891ad43c457b4e80cb7c17268", "filename": "libgo/go/syscall/sockcmsg_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fsockcmsg_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fsockcmsg_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Fsockcmsg_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "ecdab06070178f653b05fe09f3db0e39593e36ce", "filename": "libgo/go/syscall/socket_bsd.go", "status": "modified", "additions": 3, "deletions": 1, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fsocket_bsd.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fsocket_bsd.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Fsocket_bsd.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "61aa1c435cde4c9eccae1955e44f1031343bf974", "filename": "libgo/go/syscall/syscall_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fsyscall_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fsyscall_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Fsyscall_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "6ece3389834383672e94f7d821c05ba54f5576b3", "filename": "libgo/go/syscall/timestruct.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Ftimestruct.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Ftimestruct.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Ftimestruct.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "a50f7d6f5c2f227312f627ab83435ac013bc3310", "filename": "libgo/go/syscall/wait.c", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fwait.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Fsyscall%2Fwait.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fsyscall%2Fwait.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "4c68bbdc6d1a529cbbb2f81085bbd497573d4cb8", "filename": "libgo/go/time/sys_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Ftime%2Fsys_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Ftime%2Fsys_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Ftime%2Fsys_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "a876e27c54e8e9968eefb4b7236ee71072d0b2ef", "filename": "libgo/go/time/zoneinfo_unix.go", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Ftime%2Fzoneinfo_unix.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fgo%2Ftime%2Fzoneinfo_unix.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Ftime%2Fzoneinfo_unix.go?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "7897ad591c772f1c23b67ee70c331964d486c15c", "filename": "libgo/libgo.imp", "status": "added", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Flibgo.imp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Flibgo.imp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Flibgo.imp?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "dc50cc4dc7ea14aa4f966e9a12b9159b5a4ae331", "filename": "libgo/match.sh", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fmatch.sh", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fmatch.sh", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fmatch.sh?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "a86e9143bf4622c94989e7a085171ce5f8432dca", "filename": "libgo/mkrsysinfo.sh", "status": "modified", "additions": 44, "deletions": 0, "changes": 44, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fmkrsysinfo.sh", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fmkrsysinfo.sh", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fmkrsysinfo.sh?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "22547ffadc3a2e9b700aec368b12395985e9e689", "filename": "libgo/mksigtab.sh", "status": "modified", "additions": 65, "deletions": 49, "changes": 114, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fmksigtab.sh", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fmksigtab.sh", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fmksigtab.sh?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "1220b3ac617c271e00f0beebb7530754da0c9a4e", "filename": "libgo/mksysinfo.sh", "status": "modified", "additions": 25, "deletions": 0, "changes": 25, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fmksysinfo.sh", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fmksysinfo.sh", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fmksysinfo.sh?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "8cffa73d0a6a2a41ab0f6d7ee49345d8be1f9e8b", "filename": "libgo/runtime/env_posix.c", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fenv_posix.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fenv_posix.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fenv_posix.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "064eed8570cdf3533b4f1c8adf236709d90d63a6", "filename": "libgo/runtime/getncpu-aix.c", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgetncpu-aix.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgetncpu-aix.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgetncpu-aix.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "e403a2a967a2702206595589f889c4fee34ca852", "filename": "libgo/runtime/go-breakpoint.c", "status": "removed", "additions": 0, "deletions": 17, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fgo-breakpoint.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fgo-breakpoint.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-breakpoint.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "09def91b1e80bbd7d31006faa4907e1a2247fe35", "filename": "libgo/runtime/go-callers.c", "status": "modified", "additions": 21, "deletions": 4, "changes": 25, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-callers.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-callers.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-callers.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "7be259e45054112ac274ba53ae5c40b140338b6c", "filename": "libgo/runtime/go-fieldtrack.c", "status": "modified", "additions": 9, "deletions": 1, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-fieldtrack.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-fieldtrack.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-fieldtrack.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "06f6bd32f3528f6f904d21353eac735cdad60b36", "filename": "libgo/runtime/go-libmain.c", "status": "modified", "additions": 12, "deletions": 5, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-libmain.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-libmain.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-libmain.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "26354ce69b0f094156b5194f2fb9cca82f8852dd", "filename": "libgo/runtime/go-main.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-main.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-main.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-main.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "da44074a5d5428f4ccf13602114e91992198f437", "filename": "libgo/runtime/go-new.c", "status": "removed", "additions": 0, "deletions": 18, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fgo-new.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fgo-new.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-new.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "154f6b1e8be467802b86d376aa99907ec9b7213d", "filename": "libgo/runtime/go-nosys.c", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-nosys.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-nosys.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-nosys.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "ad29662d0fda46ceb2be2ca05130efb888bf9eb7", "filename": "libgo/runtime/go-signal.c", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-signal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-signal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-signal.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "0e897812be624f656a91c37f43d2fc970f595f37", "filename": "libgo/runtime/go-strslice.c", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-strslice.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-strslice.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-strslice.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "03806f6febe8622a48fb3180db636aff0d9e778b", "filename": "libgo/runtime/go-type.h", "status": "modified", "additions": 12, "deletions": 8, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-type.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-type.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-type.h?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "07f274f99a6d1e5c5a7c2013d04c12328b874ed5", "filename": "libgo/runtime/go-unsafe-new.c", "status": "removed", "additions": 0, "deletions": 24, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fgo-unsafe-new.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fgo-unsafe-new.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-unsafe-new.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "409ddd95dc345015a0ede6ffd25d07fb1737630c", "filename": "libgo/runtime/go-unsafe-newarray.c", "status": "removed", "additions": 0, "deletions": 24, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fgo-unsafe-newarray.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fgo-unsafe-newarray.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-unsafe-newarray.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "9df7f3aee081a6e7a896cc99b0819ca4bea78d10", "filename": "libgo/runtime/go-unsafe-pointer.c", "status": "modified", "additions": 18, "deletions": 17, "changes": 35, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-unsafe-pointer.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fgo-unsafe-pointer.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgo-unsafe-pointer.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "68281c36bca4ad453400c6ac092fb60c581a5898", "filename": "libgo/runtime/goc2c.c", "status": "removed", "additions": 0, "deletions": 689, "changes": 689, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fgoc2c.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fgoc2c.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fgoc2c.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "4c673f418252219602ac9d6f067a272ffad10928", "filename": "libgo/runtime/heapdump.c", "status": "removed", "additions": 0, "deletions": 776, "changes": 776, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fheapdump.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fheapdump.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fheapdump.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "232210fc4eb148ab1e49dcbef23450e918d6949e", "filename": "libgo/runtime/malloc.goc", "status": "removed", "additions": 0, "deletions": 882, "changes": 882, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmalloc.goc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmalloc.goc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmalloc.goc?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "00e4166d812b0c41f28e442deabc9c27d7d58838", "filename": "libgo/runtime/malloc.h", "status": "removed", "additions": 0, "deletions": 544, "changes": 544, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmalloc.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmalloc.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmalloc.h?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "46684bc82409323d474bf37c8c58b9403146068d", "filename": "libgo/runtime/mcache.c", "status": "removed", "additions": 0, "deletions": 131, "changes": 131, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmcache.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmcache.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmcache.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "491cac5330f10375314211a8f6458e799e150619", "filename": "libgo/runtime/mcentral.c", "status": "removed", "additions": 0, "deletions": 307, "changes": 307, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmcentral.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmcentral.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmcentral.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "6312480b69d074e0627dde06f6767590783aa764", "filename": "libgo/runtime/mem.c", "status": "removed", "additions": 0, "deletions": 230, "changes": 230, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmem.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmem.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmem.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "853b5c7ae835fb997e3b333f589e8d69a9ffb0e5", "filename": "libgo/runtime/mem_posix_memalign.c", "status": "removed", "additions": 0, "deletions": 48, "changes": 48, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmem_posix_memalign.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmem_posix_memalign.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmem_posix_memalign.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "9d0b3bbda7e8f9c7adfe43681464c513d42d2fdb", "filename": "libgo/runtime/mfixalloc.c", "status": "removed", "additions": 0, "deletions": 64, "changes": 64, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmfixalloc.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmfixalloc.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmfixalloc.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "fc5424149e2f3e0e7a8687dff92e41a7cd4e4760", "filename": "libgo/runtime/mgc0.c", "status": "removed", "additions": 0, "deletions": 2732, "changes": 2732, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmgc0.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmgc0.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmgc0.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "16000d1ae2ea8b7ce613a55455e376d460bff270", "filename": "libgo/runtime/mgc0.h", "status": "removed", "additions": 0, "deletions": 87, "changes": 87, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmgc0.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmgc0.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmgc0.h?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "c167bdc81959ee4144400180209a42f48e9ef639", "filename": "libgo/runtime/mheap.c", "status": "removed", "additions": 0, "deletions": 957, "changes": 957, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmheap.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmheap.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmheap.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "b82c70929798766f595eeff1c446c626b388db1f", "filename": "libgo/runtime/msize.c", "status": "removed", "additions": 0, "deletions": 177, "changes": 177, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmsize.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fmsize.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fmsize.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "8cb6bfffafd3d755ef933f6d81449aa14a7f0155", "filename": "libgo/runtime/panic.c", "status": "modified", "additions": 15, "deletions": 11, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fpanic.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fpanic.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fpanic.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "d64d74ccd3600e7732a18ff7cb470012f83750e1", "filename": "libgo/runtime/parfor.c", "status": "removed", "additions": 0, "deletions": 202, "changes": 202, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fparfor.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0f3744176fe1c1f739c8c53c1f6627ee8791eb03/libgo%2Fruntime%2Fparfor.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fparfor.c?ref=0f3744176fe1c1f739c8c53c1f6627ee8791eb03"}, {"sha": "cc56642bff54ba4249c289b3c41d6218ec745acb", "filename": "libgo/runtime/proc.c", "status": "modified", "additions": 118, "deletions": 940, "changes": 1058, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fproc.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fproc.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fproc.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "9f84f523f5ee54a3716ce264604b5d6c39d1a2f5", "filename": "libgo/runtime/runtime.h", "status": "modified", "additions": 34, "deletions": 99, "changes": 133, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fruntime.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fruntime.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime.h?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "336b2611324f50cd0a46f2ede930ee22e7fc3452", "filename": "libgo/runtime/runtime_c.c", "status": "modified", "additions": 27, "deletions": 37, "changes": 64, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fruntime_c.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fruntime_c.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fruntime_c.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "98f98fe0a82f0b03ad3b20fdb6b72b2cf175d915", "filename": "libgo/runtime/stack.c", "status": "added", "additions": 103, "deletions": 0, "changes": 103, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fstack.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fstack.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fstack.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "f6d8be90caa4f8da044d3b68ed9692680f08de9b", "filename": "libgo/runtime/thread-linux.c", "status": "modified", "additions": 2, "deletions": 6, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fthread-linux.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fthread-linux.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fthread-linux.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "77d53c416843a2c5a2361694fbff27393a1c116a", "filename": "libgo/runtime/thread-sema.c", "status": "modified", "additions": 3, "deletions": 6, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fthread-sema.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Fruntime%2Fthread-sema.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fruntime%2Fthread-sema.c?ref=f163907e0afadfa09f23320c51b90927c771c106"}, {"sha": "a961cbf187f7857ce15199e9f1ca1442cfdf8e31", "filename": "libgo/testsuite/gotest", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Ftestsuite%2Fgotest", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f163907e0afadfa09f23320c51b90927c771c106/libgo%2Ftestsuite%2Fgotest", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Ftestsuite%2Fgotest?ref=f163907e0afadfa09f23320c51b90927c771c106"}]}