{"sha": "8505bf12ff693b71ae7f5cfeaf4cb55bf72a0c8a", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ODUwNWJmMTJmZjY5M2I3MWFlN2Y1Y2ZlYWY0Y2I1NWJmNzJhMGM4YQ==", "commit": {"author": {"name": "Paul A. Clarke", "email": "pc@us.ibm.com", "date": "2018-10-29T19:44:57Z"}, "committer": {"name": "Paul Clarke", "email": "pc@gcc.gnu.org", "date": "2018-10-29T19:44:57Z"}, "message": "[rs6000] Consistently use '__vector' instead of 'vector'\n\nRevision r265535 committed changes that used 'vector' instead of the\npreferred '__vector'.  There is a reason that '__vector' is preferred,\nbecause it ensures no conflicts with C++ namespace.  Indeed,\ngcc/config/rs6000/xmmintrin.h undefines it, leading to errors:\n\n  gcc/include/xmmintrin.h:999:20: error: 'vector' undeclared (first use in this function); did you mean 'vec_or'?\n  gcc/include/xmmintrin.h:999:20: note: each undeclared identifier is reported only once for each function it appears in\n  gcc/include/xmmintrin.h:999:26: error: expected ')' before 'long'\n  gcc/include/xmmintrin.h:999:37: error: expected ')' before 'result'\n\nAlso fixed a few whitespace issues.\n\n[gcc]\n\n2018-10-29  Paul A. Clarke  <pc@us.ibm.com>\n\n\t* gcc/config/rs6000/mmintrin.h (_mm_packs_pi16, _mm_packs_pi32,\n\t_mm_packs_pu16, _mm_unpackhi_pi8, _mm_unpacklo_pi8, _mm_add_pi8,\n\t_mm_add_pi16, _mm_add_pi32, _mm_sub_pi8, _mm_sub_pi16, _mm_sub_pi32,\n\t_mm_cmpgt_pi8, _mm_cmpeq_pi16, _mm_cmpgt_pi16, _mm_cmpeq_pi32,\n\t_mm_cmpgt_pi32, _mm_adds_pi8, _mm_adds_pi16, _mm_adds_pu8,\n\t_mm_adds_pu16, _mm_subs_pi8, _mm_subs_pi16, _mm_subs_pu8,\n\t_mm_subs_pu16, _mm_madd_pi16, _mm_mulhi_pi16, _mm_mullo_pi16,\n\t_mm_sll_pi16, _mm_sra_pi16, _mm_srl_pi16, _mm_set1_pi16, _mm_set1_pi8):\n\tChange 'vector' to '__vector'.\n\t* gcc/config/rs6000/xmmintrin.h (_mm_cvtps_pi32, _mm_cvttps_pi32,\n\t_mm_cvtps_pi16, _mm_cvtps_pi8, _mm_max_pi16, _mm_max_pu8, _mm_min_pi16,\n\t_mm_min_pu8, _mm_mulhi_pu16, _mm_shuffle_pi16, _mm_avg_pu8,\n\t_mm_avg_pu16): Likewise.  And, whitespace corrections.\n\nFrom-SVN: r265601", "tree": {"sha": "26a177e1602c19f7b9779123abe8bc3794c4d7cc", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/26a177e1602c19f7b9779123abe8bc3794c4d7cc"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/8505bf12ff693b71ae7f5cfeaf4cb55bf72a0c8a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8505bf12ff693b71ae7f5cfeaf4cb55bf72a0c8a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8505bf12ff693b71ae7f5cfeaf4cb55bf72a0c8a", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8505bf12ff693b71ae7f5cfeaf4cb55bf72a0c8a/comments", "author": {"login": "ThinkOpenly", "id": 12301795, "node_id": "MDQ6VXNlcjEyMzAxNzk1", "avatar_url": "https://avatars.githubusercontent.com/u/12301795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ThinkOpenly", "html_url": "https://github.com/ThinkOpenly", "followers_url": "https://api.github.com/users/ThinkOpenly/followers", "following_url": "https://api.github.com/users/ThinkOpenly/following{/other_user}", "gists_url": "https://api.github.com/users/ThinkOpenly/gists{/gist_id}", "starred_url": "https://api.github.com/users/ThinkOpenly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ThinkOpenly/subscriptions", "organizations_url": "https://api.github.com/users/ThinkOpenly/orgs", "repos_url": "https://api.github.com/users/ThinkOpenly/repos", "events_url": "https://api.github.com/users/ThinkOpenly/events{/privacy}", "received_events_url": "https://api.github.com/users/ThinkOpenly/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "a385474c79d28d89861d25720715ed361e912b1a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a385474c79d28d89861d25720715ed361e912b1a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a385474c79d28d89861d25720715ed361e912b1a"}], "stats": {"total": 110, "additions": 63, "deletions": 47}, "files": [{"sha": "37fe9013b13e8a4261ce1ef741228b3c7fd22df0", "filename": "gcc/ChangeLog", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8505bf12ff693b71ae7f5cfeaf4cb55bf72a0c8a/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8505bf12ff693b71ae7f5cfeaf4cb55bf72a0c8a/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=8505bf12ff693b71ae7f5cfeaf4cb55bf72a0c8a", "patch": "@@ -1,3 +1,19 @@\n+2018-10-29  Paul A. Clarke  <pc@us.ibm.com>\n+\n+\t* gcc/config/rs6000/mmintrin.h (_mm_packs_pi16, _mm_packs_pi32,\n+\t_mm_packs_pu16, _mm_unpackhi_pi8, _mm_unpacklo_pi8, _mm_add_pi8,\n+\t_mm_add_pi16, _mm_add_pi32, _mm_sub_pi8, _mm_sub_pi16, _mm_sub_pi32,\n+\t_mm_cmpgt_pi8, _mm_cmpeq_pi16, _mm_cmpgt_pi16, _mm_cmpeq_pi32,\n+\t_mm_cmpgt_pi32, _mm_adds_pi8, _mm_adds_pi16, _mm_adds_pu8,\n+\t_mm_adds_pu16, _mm_subs_pi8, _mm_subs_pi16, _mm_subs_pu8,\n+\t_mm_subs_pu16, _mm_madd_pi16, _mm_mulhi_pi16, _mm_mullo_pi16,\n+\t_mm_sll_pi16, _mm_sra_pi16, _mm_srl_pi16, _mm_set1_pi16, _mm_set1_pi8):\n+\tChange 'vector' to '__vector'.\n+\t* gcc/config/rs6000/xmmintrin.h (_mm_cvtps_pi32, _mm_cvttps_pi32,\n+\t_mm_cvtps_pi16, _mm_cvtps_pi8, _mm_max_pi16, _mm_max_pu8, _mm_min_pi16,\n+\t_mm_min_pu8, _mm_mulhi_pu16, _mm_shuffle_pi16, _mm_avg_pu8,\n+\t_mm_avg_pu16): Likewise.  And, whitespace corrections.\n+\n 2018-10-29  Richard Biener  <rguenther@suse.de>\n \n \tPR tree-optimization/87785"}, {"sha": "7456c5bdec36b3c333fc15afbd7e2f629d99faf5", "filename": "gcc/config/rs6000/mmintrin.h", "status": "modified", "additions": 32, "deletions": 32, "changes": 64, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8505bf12ff693b71ae7f5cfeaf4cb55bf72a0c8a/gcc%2Fconfig%2Frs6000%2Fmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8505bf12ff693b71ae7f5cfeaf4cb55bf72a0c8a/gcc%2Fconfig%2Frs6000%2Fmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Fmmintrin.h?ref=8505bf12ff693b71ae7f5cfeaf4cb55bf72a0c8a", "patch": "@@ -174,7 +174,7 @@ _mm_packs_pi16 (__m64 __m1, __m64 __m2)\n \n   vm1 = (__vector signed short) (__vector unsigned long long) { __m2, __m1 };\n   vresult = vec_vpkshss (vm1, vm1);\n-  return (__m64) ((vector long long) vresult)[0];\n+  return (__m64) ((__vector long long) vresult)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -194,7 +194,7 @@ _mm_packs_pi32 (__m64 __m1, __m64 __m2)\n \n   vm1 = (__vector signed int) (__vector unsigned long long) { __m2, __m1 };\n   vresult = vec_vpkswss (vm1, vm1);\n-  return (__m64) ((vector long long) vresult)[0];\n+  return (__m64) ((__vector long long) vresult)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -214,7 +214,7 @@ _mm_packs_pu16 (__m64 __m1, __m64 __m2)\n \n   vm1 = (__vector signed short) (__vector unsigned long long) { __m2, __m1 };\n   vresult = vec_vpkshus (vm1, vm1);\n-  return (__m64) ((vector long long) vresult)[0];\n+  return (__m64) ((__vector long long) vresult)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -235,7 +235,7 @@ _mm_unpackhi_pi8 (__m64 __m1, __m64 __m2)\n   a = (__vector unsigned char)vec_splats (__m1);\n   b = (__vector unsigned char)vec_splats (__m2);\n   c = vec_mergel (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -316,7 +316,7 @@ _mm_unpacklo_pi8 (__m64 __m1, __m64 __m2)\n   a = (__vector unsigned char)vec_splats (__m1);\n   b = (__vector unsigned char)vec_splats (__m2);\n   c = vec_mergel (a, b);\n-  return (__m64) ((vector long long) c)[1];\n+  return (__m64) ((__vector long long) c)[1];\n #else\n   __m64_union m1, m2, res;\n \n@@ -397,7 +397,7 @@ _mm_add_pi8 (__m64 __m1, __m64 __m2)\n   a = (__vector signed char)vec_splats (__m1);\n   b = (__vector signed char)vec_splats (__m2);\n   c = vec_add (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -433,7 +433,7 @@ _mm_add_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = vec_add (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -465,7 +465,7 @@ _mm_add_pi32 (__m64 __m1, __m64 __m2)\n   a = (__vector signed int)vec_splats (__m1);\n   b = (__vector signed int)vec_splats (__m2);\n   c = vec_add (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -495,7 +495,7 @@ _mm_sub_pi8 (__m64 __m1, __m64 __m2)\n   a = (__vector signed char)vec_splats (__m1);\n   b = (__vector signed char)vec_splats (__m2);\n   c = vec_sub (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -531,7 +531,7 @@ _mm_sub_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = vec_sub (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -563,7 +563,7 @@ _mm_sub_pi32 (__m64 __m1, __m64 __m2)\n   a = (__vector signed int)vec_splats (__m1);\n   b = (__vector signed int)vec_splats (__m2);\n   c = vec_sub (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -753,7 +753,7 @@ _mm_cmpgt_pi8 (__m64 __m1, __m64 __m2)\n   a = (__vector signed char)vec_splats (__m1);\n   b = (__vector signed char)vec_splats (__m2);\n   c = (__vector signed char)vec_cmpgt (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -790,7 +790,7 @@ _mm_cmpeq_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = (__vector signed short)vec_cmpeq (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -821,7 +821,7 @@ _mm_cmpgt_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = (__vector signed short)vec_cmpgt (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -854,7 +854,7 @@ _mm_cmpeq_pi32 (__m64 __m1, __m64 __m2)\n   a = (__vector signed int)vec_splats (__m1);\n   b = (__vector signed int)vec_splats (__m2);\n   c = (__vector signed int)vec_cmpeq (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -883,7 +883,7 @@ _mm_cmpgt_pi32 (__m64 __m1, __m64 __m2)\n   a = (__vector signed int)vec_splats (__m1);\n   b = (__vector signed int)vec_splats (__m2);\n   c = (__vector signed int)vec_cmpgt (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -914,7 +914,7 @@ _mm_adds_pi8 (__m64 __m1, __m64 __m2)\n   a = (__vector signed char)vec_splats (__m1);\n   b = (__vector signed char)vec_splats (__m2);\n   c = vec_adds (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -932,7 +932,7 @@ _mm_adds_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = vec_adds (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -950,7 +950,7 @@ _mm_adds_pu8 (__m64 __m1, __m64 __m2)\n   a = (__vector unsigned char)vec_splats (__m1);\n   b = (__vector unsigned char)vec_splats (__m2);\n   c = vec_adds (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -969,7 +969,7 @@ _mm_adds_pu16 (__m64 __m1, __m64 __m2)\n   a = (__vector unsigned short)vec_splats (__m1);\n   b = (__vector unsigned short)vec_splats (__m2);\n   c = vec_adds (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -988,7 +988,7 @@ _mm_subs_pi8 (__m64 __m1, __m64 __m2)\n   a = (__vector signed char)vec_splats (__m1);\n   b = (__vector signed char)vec_splats (__m2);\n   c = vec_subs (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1007,7 +1007,7 @@ _mm_subs_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = vec_subs (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1026,7 +1026,7 @@ _mm_subs_pu8 (__m64 __m1, __m64 __m2)\n   a = (__vector unsigned char)vec_splats (__m1);\n   b = (__vector unsigned char)vec_splats (__m2);\n   c = vec_subs (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1045,7 +1045,7 @@ _mm_subs_pu16 (__m64 __m1, __m64 __m2)\n   a = (__vector unsigned short)vec_splats (__m1);\n   b = (__vector unsigned short)vec_splats (__m2);\n   c = vec_subs (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1067,7 +1067,7 @@ _mm_madd_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = vec_vmsumshm (a, b, zero);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1095,7 +1095,7 @@ _mm_mulhi_pi16 (__m64 __m1, __m64 __m2)\n   w1 = vec_vmulosh (a, b);\n   c = (__vector signed short)vec_perm (w0, w1, xform1);\n \n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1114,7 +1114,7 @@ _mm_mullo_pi16 (__m64 __m1, __m64 __m2)\n   a = (__vector signed short)vec_splats (__m1);\n   b = (__vector signed short)vec_splats (__m2);\n   c = a * b;\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1135,7 +1135,7 @@ _mm_sll_pi16 (__m64 __m, __m64 __count)\n       m = (__vector signed short)vec_splats (__m);\n       c = (__vector unsigned short)vec_splats ((unsigned short)__count);\n       r = vec_sl (m, (__vector unsigned short)c);\n-      return (__m64) ((vector long long) r)[0];\n+      return (__m64) ((__vector long long) r)[0];\n     }\n   else\n   return (0);\n@@ -1204,7 +1204,7 @@ _mm_sra_pi16 (__m64 __m, __m64 __count)\n \tm = (__vector signed short)vec_splats (__m);\n \tc = (__vector unsigned short)vec_splats ((unsigned short)__count);\n \tr = vec_sra (m, (__vector unsigned short)c);\n-        return (__m64) ((vector long long) r)[0];\n+        return (__m64) ((__vector long long) r)[0];\n     }\n   else\n   return (0);\n@@ -1273,7 +1273,7 @@ _mm_srl_pi16 (__m64 __m, __m64 __count)\n \tm = (__vector unsigned short)vec_splats (__m);\n \tc = (__vector unsigned short)vec_splats ((unsigned short)__count);\n \tr = vec_sr (m, (__vector unsigned short)c);\n-        return (__m64) ((vector long long) r)[0];\n+        return (__m64) ((__vector long long) r)[0];\n     }\n   else\n     return (0);\n@@ -1416,7 +1416,7 @@ _mm_set1_pi16 (short __w)\n   __vector signed short w;\n \n   w = (__vector signed short)vec_splats (__w);\n-  return (__m64) ((vector long long) w)[0];\n+  return (__m64) ((__vector long long) w)[0];\n #else\n   __m64_union res;\n \n@@ -1436,7 +1436,7 @@ _mm_set1_pi8 (signed char __b)\n   __vector signed char b;\n \n   b = (__vector signed char)vec_splats (__b);\n-  return (__m64) ((vector long long) b)[0];\n+  return (__m64) ((__vector long long) b)[0];\n #else\n   __m64_union res;\n "}, {"sha": "d8e3291b2df2ea1472f805192ca4daf9a614eedb", "filename": "gcc/config/rs6000/xmmintrin.h", "status": "modified", "additions": 15, "deletions": 15, "changes": 30, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8505bf12ff693b71ae7f5cfeaf4cb55bf72a0c8a/gcc%2Fconfig%2Frs6000%2Fxmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8505bf12ff693b71ae7f5cfeaf4cb55bf72a0c8a/gcc%2Fconfig%2Frs6000%2Fxmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Fxmmintrin.h?ref=8505bf12ff693b71ae7f5cfeaf4cb55bf72a0c8a", "patch": "@@ -996,7 +996,7 @@ _mm_cvtps_pi32 (__m128 __A)\n   rounded = vec_rint(temp);\n   result = (__vector unsigned long long) vec_cts (rounded, 0);\n \n-  return (__m64) ((vector long long) result)[0];\n+  return (__m64) ((__vector long long) result)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1053,7 +1053,7 @@ _mm_cvttps_pi32 (__m128 __A)\n   temp = (__v4sf) vec_splat ((__vector long long)__A, 0);\n   result = (__vector unsigned long long) vec_cts (temp, 0);\n \n-  return (__m64) ((vector long long) result)[0];\n+  return (__m64) ((__vector long long) result)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1190,7 +1190,7 @@ _mm_cvtpu8_ps (__m64  __A)\n \n /* Convert the four signed 32-bit values in A and B to SPFP form.  */\n extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n-_mm_cvtpi32x2_ps(__m64 __A, __m64 __B)\n+_mm_cvtpi32x2_ps (__m64 __A, __m64 __B)\n {\n   __vector signed int vi4;\n   __vector float vf4;\n@@ -1202,7 +1202,7 @@ _mm_cvtpi32x2_ps(__m64 __A, __m64 __B)\n \n /* Convert the four SPFP values in A to four signed 16-bit integers.  */\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n-_mm_cvtps_pi16(__m128 __A)\n+_mm_cvtps_pi16 (__m128 __A)\n {\n   __v4sf rounded;\n   __vector signed int temp;\n@@ -1212,12 +1212,12 @@ _mm_cvtps_pi16(__m128 __A)\n   temp = vec_cts (rounded, 0);\n   result = (__vector unsigned long long) vec_pack (temp, temp);\n \n-  return (__m64) ((vector long long) result)[0];\n+  return (__m64) ((__vector long long) result)[0];\n }\n \n /* Convert the four SPFP values in A to four signed 8-bit integers.  */\n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n-_mm_cvtps_pi8(__m128 __A)\n+_mm_cvtps_pi8 (__m128 __A)\n {\n   __v4sf rounded;\n   __vector signed int tmp_i;\n@@ -1229,7 +1229,7 @@ _mm_cvtps_pi8(__m128 __A)\n   tmp_i = vec_cts (rounded, 0);\n   tmp_s = vec_pack (tmp_i, zero);\n   res_v = vec_pack (tmp_s, tmp_s);\n-  return (__m64) ((vector long long) res_v)[0];\n+  return (__m64) ((__vector long long) res_v)[0];\n }\n \n /* Selects four specific SPFP values from A and B based on MASK.  */\n@@ -1429,7 +1429,7 @@ _mm_max_pi16 (__m64 __A, __m64 __B)\n   b = (__vector signed short)vec_splats (__B);\n   c = (__vector __bool short)vec_cmpgt (a, b);\n   r = vec_sel (b, a, c);\n-  return (__m64) ((vector long long) r)[0];\n+  return (__m64) ((__vector long long) r)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -1467,7 +1467,7 @@ _mm_max_pu8 (__m64 __A, __m64 __B)\n   b = (__vector unsigned char)vec_splats (__B);\n   c = (__vector __bool char)vec_cmpgt (a, b);\n   r = vec_sel (b, a, c);\n-  return (__m64) ((vector long long) r)[0];\n+  return (__m64) ((__vector long long) r)[0];\n #else\n   __m64_union m1, m2, res;\n   long i;\n@@ -1503,7 +1503,7 @@ _mm_min_pi16 (__m64 __A, __m64 __B)\n   b = (__vector signed short)vec_splats (__B);\n   c = (__vector __bool short)vec_cmplt (a, b);\n   r = vec_sel (b, a, c);\n-  return (__m64) ((vector long long) r)[0];\n+  return (__m64) ((__vector long long) r)[0];\n #else\n   __m64_union m1, m2, res;\n \n@@ -1541,7 +1541,7 @@ _mm_min_pu8 (__m64 __A, __m64 __B)\n   b = (__vector unsigned char)vec_splats (__B);\n   c = (__vector __bool char)vec_cmplt (a, b);\n   r = vec_sel (b, a, c);\n-  return (__m64) ((vector long long) r)[0];\n+  return (__m64) ((__vector long long) r)[0];\n #else\n   __m64_union m1, m2, res;\n   long i;\n@@ -1600,7 +1600,7 @@ _mm_mulhi_pu16 (__m64 __A, __m64 __B)\n   w1 = vec_vmulouh (a, b);\n   c = (__vector unsigned short)vec_perm (w0, w1, xform1);\n \n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1643,7 +1643,7 @@ _mm_shuffle_pi16 (__m64 __A, int const __N)\n   p = vec_splats (t.as_m64);\n   a = vec_splats (__A);\n   r = vec_perm (a, a, (__vector unsigned char)p);\n-  return (__m64) ((vector long long) r)[0];\n+  return (__m64) ((__vector long long) r)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1683,7 +1683,7 @@ _mm_avg_pu8 (__m64 __A, __m64 __B)\n   a = (__vector unsigned char)vec_splats (__A);\n   b = (__vector unsigned char)vec_splats (__B);\n   c = vec_avg (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n@@ -1701,7 +1701,7 @@ _mm_avg_pu16 (__m64 __A, __m64 __B)\n   a = (__vector unsigned short)vec_splats (__A);\n   b = (__vector unsigned short)vec_splats (__B);\n   c = vec_avg (a, b);\n-  return (__m64) ((vector long long) c)[0];\n+  return (__m64) ((__vector long long) c)[0];\n }\n \n extern __inline __m64 __attribute__((__gnu_inline__, __always_inline__, __artificial__))"}]}