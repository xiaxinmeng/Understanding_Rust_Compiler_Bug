{"sha": "5f6a6c91d7c592cb49f7c519f289777eac09bb74", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NWY2YTZjOTFkN2M1OTJjYjQ5ZjdjNTE5ZjI4OTc3N2VhYzA5YmI3NA==", "commit": {"author": {"name": "Richard Earnshaw", "email": "rearnsha@arm.com", "date": "2021-09-03T16:06:15Z"}, "committer": {"name": "Richard Earnshaw", "email": "rearnsha@arm.com", "date": "2021-09-13T10:26:48Z"}, "message": "gimple: allow more folding of memcpy [PR102125]\n\nThe current restriction on folding memcpy to a single element of size\nMOVE_MAX is excessively cautious on most machines and limits some\nsignificant further optimizations.  So relax the restriction provided\nthe copy size does not exceed MOVE_MAX * MOVE_RATIO and that a SET\ninsn exists for moving the value into machine registers.\n\nNote that there were already checks in place for having misaligned\nmove operations when one or more of the operands were unaligned.\n\nOn Arm this now permits optimizing\n\nuint64_t bar64(const uint8_t *rData1)\n{\n    uint64_t buffer;\n    memcpy(&buffer, rData1, sizeof(buffer));\n    return buffer;\n}\n\nfrom\n        ldr     r2, [r0]        @ unaligned\n        sub     sp, sp, #8\n        ldr     r3, [r0, #4]    @ unaligned\n        strd    r2, [sp]\n        ldrd    r0, [sp]\n        add     sp, sp, #8\n\nto\n        mov     r3, r0\n        ldr     r0, [r0]        @ unaligned\n        ldr     r1, [r3, #4]    @ unaligned\n\nPR target/102125 - (ARM Cortex-M3 and newer) missed optimization. memcpy not needed operations\n\ngcc/ChangeLog:\n\n\tPR target/102125\n\t* gimple-fold.c (gimple_fold_builtin_memory_op): Allow folding\n\tmemcpy if the size is not more than MOVE_MAX * MOVE_RATIO.", "tree": {"sha": "63b2e89f201b605fd70aa9f92c00c1e693a30599", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/63b2e89f201b605fd70aa9f92c00c1e693a30599"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/5f6a6c91d7c592cb49f7c519f289777eac09bb74", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5f6a6c91d7c592cb49f7c519f289777eac09bb74", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5f6a6c91d7c592cb49f7c519f289777eac09bb74", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5f6a6c91d7c592cb49f7c519f289777eac09bb74/comments", "author": null, "committer": null, "parents": [{"sha": "f0cfd070b68772eaaa19a3b711fbd9e85b244240", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f0cfd070b68772eaaa19a3b711fbd9e85b244240", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f0cfd070b68772eaaa19a3b711fbd9e85b244240"}], "stats": {"total": 16, "additions": 11, "deletions": 5}, "files": [{"sha": "6fea8a6f9fdb51d5dd717a28f3d2f49efbf59d75", "filename": "gcc/gimple-fold.c", "status": "modified", "additions": 11, "deletions": 5, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5f6a6c91d7c592cb49f7c519f289777eac09bb74/gcc%2Fgimple-fold.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5f6a6c91d7c592cb49f7c519f289777eac09bb74/gcc%2Fgimple-fold.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgimple-fold.c?ref=5f6a6c91d7c592cb49f7c519f289777eac09bb74", "patch": "@@ -67,6 +67,8 @@ along with GCC; see the file COPYING3.  If not see\n #include \"tree-vector-builder.h\"\n #include \"tree-ssa-strlen.h\"\n #include \"varasm.h\"\n+#include \"memmodel.h\"\n+#include \"optabs.h\"\n \n enum strlen_range_kind {\n   /* Compute the exact constant string length.  */\n@@ -957,14 +959,17 @@ gimple_fold_builtin_memory_op (gimple_stmt_iterator *gsi,\n \t= build_int_cst (build_pointer_type_for_mode (char_type_node,\n \t\t\t\t\t\t      ptr_mode, true), 0);\n \n-      /* If we can perform the copy efficiently with first doing all loads\n-         and then all stores inline it that way.  Currently efficiently\n-\t means that we can load all the memory into a single integer\n-\t register which is what MOVE_MAX gives us.  */\n+      /* If we can perform the copy efficiently with first doing all loads and\n+\t then all stores inline it that way.  Currently efficiently means that\n+\t we can load all the memory with a single set operation and that the\n+\t total size is less than MOVE_MAX * MOVE_RATIO.  */\n       src_align = get_pointer_alignment (src);\n       dest_align = get_pointer_alignment (dest);\n       if (tree_fits_uhwi_p (len)\n-\t  && compare_tree_int (len, MOVE_MAX) <= 0\n+\t  && (compare_tree_int\n+\t      (len, (MOVE_MAX\n+\t\t     * MOVE_RATIO (optimize_function_for_size_p (cfun))))\n+\t      <= 0)\n \t  /* FIXME: Don't transform copies from strings with known length.\n \t     Until GCC 9 this prevented a case in gcc.dg/strlenopt-8.c\n \t     from being handled, and the case was XFAILed for that reason.\n@@ -1000,6 +1005,7 @@ gimple_fold_builtin_memory_op (gimple_stmt_iterator *gsi,\n \t      if (type\n \t\t  && is_a <scalar_int_mode> (TYPE_MODE (type), &mode)\n \t\t  && GET_MODE_SIZE (mode) * BITS_PER_UNIT == ilen * 8\n+\t\t  && have_insn_for (SET, mode)\n \t\t  /* If the destination pointer is not aligned we must be able\n \t\t     to emit an unaligned store.  */\n \t\t  && (dest_align >= GET_MODE_ALIGNMENT (mode)"}]}