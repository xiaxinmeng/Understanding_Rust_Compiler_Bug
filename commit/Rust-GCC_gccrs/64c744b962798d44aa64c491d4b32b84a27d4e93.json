{"sha": "64c744b962798d44aa64c491d4b32b84a27d4e93", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NjRjNzQ0Yjk2Mjc5OGQ0NGFhNjRjNDkxZDRiMzJiODRhMjdkNGU5Mw==", "commit": {"author": {"name": "Dominik Vogt", "email": "vogt@linux.vnet.ibm.com", "date": "2016-09-23T09:53:29Z"}, "committer": {"name": "Andreas Krebbel", "email": "krebbel@gcc.gnu.org", "date": "2016-09-23T09:53:29Z"}, "message": "S/390: Improved risbg usage.\n\ngcc/ChangeLog:\n\n2016-09-23  Dominik Vogt  <vogt@linux.vnet.ibm.com>\n\n\t* config/s390/s390.md (\"*extzv<mode>_zEC12\", \"*extzv<mode>_z10\")\n\t(\"*extzv<mode><clobbercc_or_nocc>\"):\n\tCorrect a typo in a comment.\n\tMerged patterns.\n\t(\"*insv<mode>_zEC12\", \"*insv<mode>_z10\")\n\t(\"*insv<mode><clobbercc_or_nocc>\"): Ditto.\n\t(\"*insv<mode>_zEC12_appendbitsleft\")\n\t(\"*insv<mode><clobbercc_or_nocc>_appendbitsleft\")\n\t(\"*insv<mode>_z10_appendbitsleft\"): Ditto.\n\t(\"*insv<mode>_zEC12_noshift\", \"*insv<mode>_z10_noshift\")\n\t(\"*insv<mode><clobbercc_or_nocc>_noshift\"): Ditto.\n\tProvide pattern with operands switched.\n\t(\"*pre_z10_extv<mode>\"):\n\tUse new subst patterns.\n\t(\"*extzvdi<clobbercc_or_nocc>_lshiftrt\", \"*<risbg_n>_ior_and_sr_ze\")\n\t(\"*extvsidi<clobbercc_or_nocc>\", \"*<risbg_n>_and_subregdi_rotr\")\n\t(\"*<risbg_n>_and_subregdi_rotl\", \"*<risbg_n>_di_and_rot\")\n\t(\"*insv_z10_noshift_cc\", \"*insv_z10_noshift_cconly\")\n\t(\"*<risbg_n>_<mode>_ior_and_lshiftrt\")\n\t(\"*<risbg_n>_sidi_ior_and_lshiftrt\")\n\t(\"*trunc_sidi_and_subreg_lshrt<clobbercc_or_nocc>\"):\n\tNew patterns.\n\t(\"*extzv_<mode>_sll\", \"*extzv_<mode>_srl\")\n\t(\"*extzv_<mode>_srl<clobbercc_or_nocc>\")\n\t(\"*extzv_<mode>_sll<clobbercc_or_nocc>\"): Renamed patterns, use risbgn\n\ton zEC12.\n\t(\"SINT\"): New mode_iterator with SI, HI, QI.\n\t* config/s390/subst.md (\"clobbercc_or_nocc_subst\", \"z10_or_zEC12_cond\")\n\t(\"clobbercc_or_nocc\", \"risbg_n\"): New constructs for risbg pattern\n\tduplication.\n\t\ngcc/testsuite/ChangeLog:\n\n2016-09-23  Dominik Vogt  <vogt@linux.vnet.ibm.com>\n\n\t* gcc.target/s390/risbg-ll-1.c: Ported risbg tests from llvm.\n\t* gcc.target/s390/risbg-ll-2.c: Ditto.\n\t* gcc.target/s390/risbg-ll-3.c: Ditto.\n\nFrom-SVN: r240414", "tree": {"sha": "190cfabca5a4b4fc05eca1139784f3808247d6a4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/190cfabca5a4b4fc05eca1139784f3808247d6a4"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/64c744b962798d44aa64c491d4b32b84a27d4e93", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/64c744b962798d44aa64c491d4b32b84a27d4e93", "html_url": "https://github.com/Rust-GCC/gccrs/commit/64c744b962798d44aa64c491d4b32b84a27d4e93", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/64c744b962798d44aa64c491d4b32b84a27d4e93/comments", "author": {"login": "vogtd", "id": 9690100, "node_id": "MDQ6VXNlcjk2OTAxMDA=", "avatar_url": "https://avatars.githubusercontent.com/u/9690100?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vogtd", "html_url": "https://github.com/vogtd", "followers_url": "https://api.github.com/users/vogtd/followers", "following_url": "https://api.github.com/users/vogtd/following{/other_user}", "gists_url": "https://api.github.com/users/vogtd/gists{/gist_id}", "starred_url": "https://api.github.com/users/vogtd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vogtd/subscriptions", "organizations_url": "https://api.github.com/users/vogtd/orgs", "repos_url": "https://api.github.com/users/vogtd/repos", "events_url": "https://api.github.com/users/vogtd/events{/privacy}", "received_events_url": "https://api.github.com/users/vogtd/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "c2586c82cd3c03d20d5f00f5e4e8fded7652d4bc", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c2586c82cd3c03d20d5f00f5e4e8fded7652d4bc", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c2586c82cd3c03d20d5f00f5e4e8fded7652d4bc"}], "stats": {"total": 976, "additions": 910, "deletions": 66}, "files": [{"sha": "19dc8f224817b20239b165279a4c7bd50037a7ae", "filename": "gcc/ChangeLog", "status": "modified", "additions": 33, "deletions": 0, "changes": 33, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64c744b962798d44aa64c491d4b32b84a27d4e93/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64c744b962798d44aa64c491d4b32b84a27d4e93/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=64c744b962798d44aa64c491d4b32b84a27d4e93", "patch": "@@ -1,3 +1,36 @@\n+2016-09-23  Dominik Vogt  <vogt@linux.vnet.ibm.com>\n+\n+\t* config/s390/s390.md (\"*extzv<mode>_zEC12\", \"*extzv<mode>_z10\")\n+\t(\"*extzv<mode><clobbercc_or_nocc>\"):\n+\tCorrect a typo in a comment.\n+\tMerged patterns.\n+\t(\"*insv<mode>_zEC12\", \"*insv<mode>_z10\")\n+\t(\"*insv<mode><clobbercc_or_nocc>\"): Ditto.\n+\t(\"*insv<mode>_zEC12_appendbitsleft\")\n+\t(\"*insv<mode><clobbercc_or_nocc>_appendbitsleft\")\n+\t(\"*insv<mode>_z10_appendbitsleft\"): Ditto.\n+\t(\"*insv<mode>_zEC12_noshift\", \"*insv<mode>_z10_noshift\")\n+\t(\"*insv<mode><clobbercc_or_nocc>_noshift\"): Ditto.\n+\tProvide pattern with operands switched.\n+\t(\"*pre_z10_extv<mode>\"):\n+\tUse new subst patterns.\n+\t(\"*extzvdi<clobbercc_or_nocc>_lshiftrt\", \"*<risbg_n>_ior_and_sr_ze\")\n+\t(\"*extvsidi<clobbercc_or_nocc>\", \"*<risbg_n>_and_subregdi_rotr\")\n+\t(\"*<risbg_n>_and_subregdi_rotl\", \"*<risbg_n>_di_and_rot\")\n+\t(\"*insv_z10_noshift_cc\", \"*insv_z10_noshift_cconly\")\n+\t(\"*<risbg_n>_<mode>_ior_and_lshiftrt\")\n+\t(\"*<risbg_n>_sidi_ior_and_lshiftrt\")\n+\t(\"*trunc_sidi_and_subreg_lshrt<clobbercc_or_nocc>\"):\n+\tNew patterns.\n+\t(\"*extzv_<mode>_sll\", \"*extzv_<mode>_srl\")\n+\t(\"*extzv_<mode>_srl<clobbercc_or_nocc>\")\n+\t(\"*extzv_<mode>_sll<clobbercc_or_nocc>\"): Renamed patterns, use risbgn\n+\ton zEC12.\n+\t(\"SINT\"): New mode_iterator with SI, HI, QI.\n+\t* config/s390/subst.md (\"clobbercc_or_nocc_subst\", \"z10_or_zEC12_cond\")\n+\t(\"clobbercc_or_nocc\", \"risbg_n\"): New constructs for risbg pattern\n+\tduplication.\n+\n 2016-09-23  Dominik Vogt  <vogt@linux.vnet.ibm.com>\n \n \t* config/s390/predicates.md (\"contiguous_bitmask_operand\"): Adapt to new"}, {"sha": "9de442f085a6c4a79cf7c08f9c623710a82b983b", "filename": "gcc/config/s390/s390.md", "status": "modified", "additions": 185, "deletions": 66, "changes": 251, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64c744b962798d44aa64c491d4b32b84a27d4e93/gcc%2Fconfig%2Fs390%2Fs390.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64c744b962798d44aa64c491d4b32b84a27d4e93/gcc%2Fconfig%2Fs390%2Fs390.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fs390%2Fs390.md?ref=64c744b962798d44aa64c491d4b32b84a27d4e93", "patch": "@@ -606,6 +606,7 @@\n ;; same template.\n (define_mode_iterator INT [(DI \"TARGET_ZARCH\") SI HI QI])\n (define_mode_iterator DINT [(TI \"TARGET_ZARCH\") DI SI HI QI])\n+(define_mode_iterator SINT [SI HI QI])\n \n ;; This iterator allows some 'ashift' and 'lshiftrt' pattern to be defined from\n ;; the same template.\n@@ -3750,25 +3751,93 @@\n     }\n })\n \n-(define_insn \"*extzv<mode>_zEC12\"\n+(define_insn \"*extzv<mode><clobbercc_or_nocc>\"\n   [(set (match_operand:GPR 0 \"register_operand\" \"=d\")\n       (zero_extract:GPR\n         (match_operand:GPR 1 \"register_operand\" \"d\")\n         (match_operand 2 \"const_int_operand\" \"\")   ; size\n-        (match_operand 3 \"const_int_operand\" \"\")))] ; start]\n-  \"TARGET_ZEC12\"\n-  \"risbgn\\t%0,%1,64-%2,128+63,<bitoff_plus>%3+%2\" ; dst, src, start, end, shift\n-  [(set_attr \"op_type\" \"RIE\")])\n+        (match_operand 3 \"const_int_operand\" \"\"))) ; start\n+  ]\n+  \"<z10_or_zEC12_cond>\"\n+  \"<risbg_n>\\t%0,%1,64-%2,128+63,<bitoff_plus>%3+%2\" ; dst, src, start, end, shift\n+  [(set_attr \"op_type\" \"RIE\")\n+   (set_attr \"z10prop\" \"z10_super_E1\")])\n \n-(define_insn \"*extzv<mode>_z10\"\n-  [(set (match_operand:GPR 0 \"register_operand\" \"=d\")\n-      (zero_extract:GPR\n-       (match_operand:GPR 1 \"register_operand\" \"d\")\n-       (match_operand 2 \"const_int_operand\" \"\")   ; size\n-       (match_operand 3 \"const_int_operand\" \"\"))) ; start\n-   (clobber (reg:CC CC_REGNUM))]\n-  \"TARGET_Z10\"\n-  \"risbg\\t%0,%1,64-%2,128+63,<bitsize>+%3+%2\" ; dst, src, start, end, shift\n+; 64 bit: (a & -16) | ((b >> 8) & 15)\n+(define_insn \"*extzvdi<clobbercc_or_nocc>_lshiftrt\"\n+  [(set (zero_extract:DI (match_operand:DI 0 \"register_operand\" \"+d\")\n+\t\t\t (match_operand 1 \"const_int_operand\" \"\")  ; size\n+\t\t\t (match_operand 2 \"const_int_operand\" \"\")) ; start\n+\t(lshiftrt:DI (match_operand:DI 3 \"register_operand\" \"d\")\n+\t\t     (match_operand:DI 4 \"nonzero_shift_count_operand\" \"\")))]\n+  \"<z10_or_zEC12_cond>\n+   && 64 - UINTVAL (operands[4]) >= UINTVAL (operands[1])\"\n+  \"<risbg_n>\\t%0,%3,%2,%2+%1-1,128-%2-%1-%4\"\n+  [(set_attr \"op_type\" \"RIE\")\n+   (set_attr \"z10prop\" \"z10_super_E1\")])\n+\n+; 32 bit: (a & -16) | ((b >> 8) & 15)\n+(define_insn \"*<risbg_n>_ior_and_sr_ze\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=d\")\n+\t(ior:SI (and:SI\n+\t\t (match_operand:SI 1 \"register_operand\" \"0\")\n+\t\t (match_operand:SI 2 \"const_int_operand\" \"\"))\n+\t\t(subreg:SI\n+\t\t (zero_extract:DI\n+\t\t  (match_operand:DI 3 \"register_operand\" \"d\")\n+\t\t  (match_operand 4 \"const_int_operand\" \"\") ; size\n+\t\t  (match_operand 5 \"const_int_operand\" \"\")) ; start\n+\t\t 4)))]\n+  \"<z10_or_zEC12_cond>\n+   && UINTVAL (operands[2]) == (~(0ULL) << UINTVAL (operands[4]))\"\n+  \"<risbg_n>\\t%0,%3,64-%4,63,%4+%5\"\n+  [(set_attr \"op_type\" \"RIE\")\n+   (set_attr \"z10prop\" \"z10_super_E1\")])\n+\n+; ((int)foo >> 10) & 1;\n+(define_insn \"*extract1bitdi<clobbercc_or_nocc>\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=d\")\n+\t(ne:DI (zero_extract:DI\n+\t\t(match_operand:DI 1 \"register_operand\" \"d\")\n+\t\t(const_int 1)  ; size\n+\t\t(match_operand 2 \"const_int_operand\" \"\")) ; start\n+\t       (const_int 0)))]\n+  \"<z10_or_zEC12_cond>\"\n+  \"<risbg_n>\\t%0,%1,64-1,128+63,%2+1\" ; dst, src, start, end, shift\n+  [(set_attr \"op_type\" \"RIE\")\n+   (set_attr \"z10prop\" \"z10_super_E1\")])\n+\n+(define_insn \"*<risbg_n>_and_subregdi_rotr\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=d\")\n+\t(and:DI (subreg:DI\n+\t\t (rotate:SINT (match_operand:SINT 1 \"register_operand\" \"d\")\n+\t\t\t     (match_operand:SINT 2 \"const_int_operand\" \"\")) 0)\n+\t\t(match_operand:DI 3 \"contiguous_bitmask_operand\" \"\")))]\n+  \"<z10_or_zEC12_cond>\n+   && UINTVAL (operands[3]) < (1ULL << (UINTVAL (operands[2]) & 0x3f))\"\n+  \"<risbg_n>\\t%0,%1,%s3,128+%e3,<bitoff_plus>%2\" ; dst, src, start, end, shift\n+  [(set_attr \"op_type\" \"RIE\")\n+   (set_attr \"z10prop\" \"z10_super_E1\")])\n+\n+(define_insn \"*<risbg_n>_and_subregdi_rotl\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=d\")\n+\t(and:DI (subreg:DI\n+\t\t (rotate:SINT (match_operand:SINT 1 \"register_operand\" \"d\")\n+\t\t\t     (match_operand:SINT 2 \"const_int_operand\" \"\")) 0)\n+\t\t(match_operand:DI 3 \"contiguous_bitmask_operand\" \"\")))]\n+  \"<z10_or_zEC12_cond>\n+   && !(UINTVAL (operands[3]) & ((1ULL << (UINTVAL (operands[2]) & 0x3f)) - 1))\"\n+  \"<risbg_n>\\t%0,%1,%s3,128+%e3,%2\" ; dst, src, start, end, shift\n+  [(set_attr \"op_type\" \"RIE\")\n+   (set_attr \"z10prop\" \"z10_super_E1\")])\n+\n+(define_insn \"*<risbg_n>_di_and_rot\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=d\")\n+\t(and:DI (rotate:DI (match_operand:DI 1 \"register_operand\" \"d\")\n+\t\t\t    (match_operand:DI 2 \"const_int_operand\" \"\"))\n+\t\t(match_operand:DI 3 \"contiguous_bitmask_operand\" \"\")))]\n+  \"<z10_or_zEC12_cond>\"\n+  \"<risbg_n>\\t%0,%1,%s3,128+%e3,%2\" ; dst, src, start, end, shift\n   [(set_attr \"op_type\" \"RIE\")\n    (set_attr \"z10prop\" \"z10_super_E1\")])\n \n@@ -3842,74 +3911,126 @@\n ; The normal RTL expansion will never generate a zero_extract where\n ; the location operand isn't word mode.  However, we do this in the\n ; back-end when generating atomic operations. See s390_two_part_insv.\n-(define_insn \"*insv<mode>_zEC12\"\n+(define_insn \"*insv<mode><clobbercc_or_nocc>\"\n   [(set (zero_extract:GPR (match_operand:GPR 0 \"nonimmediate_operand\" \"+d\")\n \t\t\t  (match_operand 1 \"const_int_operand\"    \"I\")  ; size\n \t\t\t  (match_operand 2 \"const_int_operand\"    \"I\")) ; pos\n \t(match_operand:GPR 3 \"nonimmediate_operand\" \"d\"))]\n-  \"TARGET_ZEC12\n-   && (INTVAL (operands[1]) + INTVAL (operands[2])) <= <bitsize>\"\n-  \"risbgn\\t%0,%3,<bitoff_plus>%2,<bitoff_plus>%2+%1-1,<bitsize>-%2-%1\"\n-  [(set_attr \"op_type\" \"RIE\")])\n-\n-(define_insn \"*insv<mode>_z10\"\n-  [(set (zero_extract:GPR (match_operand:GPR 0 \"nonimmediate_operand\" \"+d\")\n-\t\t\t  (match_operand 1 \"const_int_operand\"    \"I\")  ; size\n-\t\t\t  (match_operand 2 \"const_int_operand\"    \"I\")) ; pos\n-\t(match_operand:GPR 3 \"nonimmediate_operand\" \"d\"))\n-   (clobber (reg:CC CC_REGNUM))]\n-  \"TARGET_Z10\n+  \"<z10_or_zEC12_cond>\n    && (INTVAL (operands[1]) + INTVAL (operands[2])) <= <bitsize>\"\n-  \"risbg\\t%0,%3,<bitoff_plus>%2,<bitoff_plus>%2+%1-1,<bitsize>-%2-%1\"\n+  \"<risbg_n>\\t%0,%3,<bitoff_plus>%2,<bitoff_plus>%2+%1-1,<bitsize>-%2-%1\"\n   [(set_attr \"op_type\" \"RIE\")\n    (set_attr \"z10prop\" \"z10_super_E1\")])\n \n ; and op1 with a mask being 1 for the selected bits and 0 for the rest\n ; and op3=op0 with a mask being 0 for the selected bits and 1 for the rest\n-(define_insn \"*insv<mode>_zEC12_noshift\"\n-  [(set (match_operand:GPR 0 \"nonimmediate_operand\" \"=d\")\n-\t(ior:GPR (and:GPR (match_operand:GPR 1 \"nonimmediate_operand\" \"d\")\n+(define_insn \"*insv<mode><clobbercc_or_nocc>_noshift\"\n+  [(set (match_operand:GPR 0 \"nonimmediate_operand\" \"=d,d\")\n+\t(ior:GPR (and:GPR (match_operand:GPR 1 \"nonimmediate_operand\" \"d,0\")\n \t\t\t  (match_operand:GPR 2 \"contiguous_bitmask_operand\" \"\"))\n-\t\t (and:GPR (match_operand:GPR 3 \"nonimmediate_operand\" \"0\")\n+\t\t (and:GPR (match_operand:GPR 3 \"nonimmediate_operand\" \"0,d\")\n \t\t\t  (match_operand:GPR 4 \"const_int_operand\" \"\"))))]\n-  \"TARGET_ZEC12 && INTVAL (operands[2]) == ~INTVAL (operands[4])\"\n-  \"risbgn\\t%0,%1,%<bfstart>2,%<bfend>2,0\"\n-  [(set_attr \"op_type\" \"RIE\")])\n+  \"<z10_or_zEC12_cond> && INTVAL (operands[2]) == ~INTVAL (operands[4])\"\n+  \"@\n+   <risbg_n>\\t%0,%1,%<bfstart>2,%<bfend>2,0\n+   <risbg_n>\\t%0,%3,%<bfstart>4,%<bfend>4,0\"\n+  [(set_attr \"op_type\" \"RIE\")\n+   (set_attr \"z10prop\" \"z10_super_E1\")])\n \n-(define_insn \"*insv<mode>_z10_noshift\"\n-  [(set (match_operand:GPR 0 \"nonimmediate_operand\" \"=d\")\n-\t(ior:GPR (and:GPR (match_operand:GPR 1 \"nonimmediate_operand\" \"d\")\n-\t\t\t  (match_operand:GPR 2 \"contiguous_bitmask_operand\" \"\"))\n-\t\t (and:GPR (match_operand:GPR 3 \"nonimmediate_operand\" \"0\")\n-\t\t\t  (match_operand:GPR 4 \"const_int_operand\" \"\"))))\n-   (clobber (reg:CC CC_REGNUM))]\n-  \"TARGET_Z10 && INTVAL (operands[2]) == ~INTVAL (operands[4])\"\n-  \"risbg\\t%0,%1,%<bfstart>2,%<bfend>2,0\"\n+(define_insn \"*insv_z10_noshift_cc\"\n+  [(set (reg CC_REGNUM)\n+       (compare\n+\t(ior:DI\n+\t (and:DI (match_operand:DI 1 \"nonimmediate_operand\" \"d,0\")\n+\t\t  (match_operand:DI 2 \"contiguous_bitmask_operand\" \"\"))\n+\t (and:DI (match_operand:DI 3 \"nonimmediate_operand\" \"0,d\")\n+\t\t  (match_operand:DI 4 \"const_int_operand\" \"\")))\n+\t(const_int 0)))\n+   (set (match_operand:DI 0 \"nonimmediate_operand\" \"=d,d\")\n+\t(ior:DI (and:DI (match_dup 1) (match_dup 2))\n+\t\t (and:DI (match_dup 3) (match_dup 4))))]\n+  \"TARGET_Z10 && s390_match_ccmode (insn, CCSmode)\n+   && INTVAL (operands[2]) == ~INTVAL (operands[4])\"\n+  \"@\n+   risbg\\t%0,%1,%s2,%e2,0\n+   risbg\\t%0,%3,%s4,%e4,0\"\n+  [(set_attr \"op_type\" \"RIE\")\n+   (set_attr \"z10prop\" \"z10_super_E1\")])\n+\n+(define_insn \"*insv_z10_noshift_cconly\"\n+  [(set\n+    (reg CC_REGNUM)\n+    (compare\n+     (ior:DI\n+      (and:DI (match_operand:DI 1 \"nonimmediate_operand\" \"d,0\")\n+\t       (match_operand:DI 2 \"contiguous_bitmask_operand\" \"\"))\n+      (and:DI (match_operand:DI 3 \"nonimmediate_operand\" \"0,d\")\n+\t       (match_operand:DI 4 \"const_int_operand\" \"\")))\n+     (const_int 0)))\n+  (clobber (match_scratch:DI 0 \"=d,d\"))]\n+  \"TARGET_Z10 && s390_match_ccmode (insn, CCSmode)\n+   && INTVAL (operands[2]) == ~INTVAL (operands[4])\"\n+  \"@\n+   risbg\\t%0,%1,%s2,%e2,0\n+   risbg\\t%0,%3,%s4,%e4,0\"\n   [(set_attr \"op_type\" \"RIE\")\n    (set_attr \"z10prop\" \"z10_super_E1\")])\n \n ; Implement appending Y on the left of S bits of X\n ; x = (y << s) | (x & ((1 << s) - 1))\n-(define_insn \"*insv<mode>_zEC12_appendbitsleft\"\n+(define_insn \"*insv<mode><clobbercc_or_nocc>_appendbitsleft\"\n   [(set (match_operand:GPR 0 \"nonimmediate_operand\" \"=d\")\n \t(ior:GPR (and:GPR (match_operand:GPR 1 \"nonimmediate_operand\" \"0\")\n \t\t\t  (match_operand:GPR 2 \"immediate_operand\" \"\"))\n \t\t (ashift:GPR (match_operand:GPR 3 \"nonimmediate_operand\" \"d\")\n \t\t\t     (match_operand:GPR 4 \"nonzero_shift_count_operand\" \"\"))))]\n-  \"TARGET_ZEC12 && UINTVAL (operands[2]) == (1UL << UINTVAL (operands[4])) - 1\"\n-  \"risbgn\\t%0,%3,<bitoff>,64-%4-1,%4\"\n+  \"<z10_or_zEC12_cond>\n+   && UINTVAL (operands[2]) == (1UL << UINTVAL (operands[4])) - 1\"\n+  \"<risbg_n>\\t%0,%3,<bitoff>,64-%4-1,%4\"\n   [(set_attr \"op_type\" \"RIE\")\n    (set_attr \"z10prop\" \"z10_super_E1\")])\n \n-(define_insn \"*insv<mode>_z10_appendbitsleft\"\n-  [(set (match_operand:GPR 0 \"nonimmediate_operand\" \"=d\")\n-\t(ior:GPR (and:GPR (match_operand:GPR 1 \"nonimmediate_operand\" \"0\")\n-\t\t\t  (match_operand:GPR 2 \"immediate_operand\" \"\"))\n-\t\t (ashift:GPR (match_operand:GPR 3 \"nonimmediate_operand\" \"d\")\n-\t\t\t     (match_operand:GPR 4 \"nonzero_shift_count_operand\" \"\"))))\n-   (clobber (reg:CC CC_REGNUM))]\n-  \"TARGET_Z10 && !TARGET_ZEC12 && UINTVAL (operands[2]) == (1UL << UINTVAL (operands[4])) - 1\"\n-  \"risbg\\t%0,%3,<bitoff>,64-%4-1,%4\"\n+; a = ((i32)a & -16777216) | (((ui32)b) >> 8)\n+(define_insn \"*<risbg_n>_<mode>_ior_and_lshiftrt\"\n+  [(set (match_operand:GPR 0 \"register_operand\" \"=d\")\n+\t(ior:GPR (and:GPR\n+\t\t  (match_operand:GPR 1 \"register_operand\" \"0\")\n+\t\t  (match_operand:GPR 2 \"const_int_operand\" \"\"))\n+\t\t (lshiftrt:GPR\n+\t\t  (match_operand:GPR 3 \"register_operand\" \"d\")\n+\t\t  (match_operand:GPR 4 \"nonzero_shift_count_operand\" \"\"))))]\n+  \"<z10_or_zEC12_cond> && UINTVAL (operands[2])\n+   == (~(0ULL) << (GET_MODE_BITSIZE (<MODE>mode) - UINTVAL (operands[4])))\"\n+  \"<risbg_n>\\t%0,%3,<bitoff_plus>%4,63,64-%4\"\n+  [(set_attr \"op_type\" \"RIE\")\n+   (set_attr \"z10prop\" \"z10_super_E1\")])\n+\n+; (ui32)(((ui64)x) >> 48) | ((i32)y & -65536);\n+(define_insn \"*<risbg_n>_sidi_ior_and_lshiftrt\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=d\")\n+\t(ior:SI (and:SI\n+\t\t (match_operand:SI 1 \"register_operand\" \"0\")\n+\t\t (match_operand:SI 2 \"const_int_operand\" \"\"))\n+\t\t(subreg:SI\n+\t\t (lshiftrt:DI\n+\t\t  (match_operand:DI 3 \"register_operand\" \"d\")\n+\t\t  (match_operand:DI 4 \"nonzero_shift_count_operand\" \"\")) 4)))]\n+  \"<z10_or_zEC12_cond>\n+   && UINTVAL (operands[2]) == ~(~(0ULL) >> UINTVAL (operands[4]))\"\n+  \"<risbg_n>\\t%0,%3,%4,63,64-%4\"\n+  [(set_attr \"op_type\" \"RIE\")\n+   (set_attr \"z10prop\" \"z10_super_E1\")])\n+\n+; (ui32)(((ui64)x) >> 12) & -4\n+(define_insn \"*trunc_sidi_and_subreg_lshrt<clobbercc_or_nocc>\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=d\")\n+\t(and:SI\n+\t (subreg:SI (lshiftrt:DI\n+\t\t     (match_operand:DI 1 \"register_operand\" \"d\")\n+\t\t     (match_operand:DI 2 \"nonzero_shift_count_operand\" \"\")) 4)\n+\t (match_operand:SI 3 \"contiguous_bitmask_nowrap_operand\" \"\")))]\n+  \"<z10_or_zEC12_cond>\"\n+  \"<risbg_n>\\t%0,%1,%t3,128+%f3,64-%2\"\n   [(set_attr \"op_type\" \"RIE\")\n    (set_attr \"z10prop\" \"z10_super_E1\")])\n \n@@ -7049,32 +7170,30 @@\n   \"s390_narrow_logical_operator (AND, &operands[0], &operands[1]);\")\n \n ;; These two are what combine generates for (ashift (zero_extract)).\n-(define_insn \"*extzv_<mode>_srl\"\n+(define_insn \"*extzv_<mode>_srl<clobbercc_or_nocc>\"\n   [(set (match_operand:GPR 0 \"register_operand\" \"=d\")\n \t(and:GPR (lshiftrt:GPR\n \t\t   (match_operand:GPR 1 \"register_operand\" \"d\")\n \t\t   (match_operand:GPR 2 \"nonzero_shift_count_operand\" \"\"))\n-\t\t(match_operand:GPR 3 \"contiguous_bitmask_operand\" \"\")))\n-   (clobber (reg:CC CC_REGNUM))]\n-  \"TARGET_Z10\n+\t\t(match_operand:GPR 3 \"contiguous_bitmask_operand\" \"\")))]\n+  \"<z10_or_zEC12_cond>\n    /* Note that even for the SImode pattern, the rotate is always DImode.  */\n    && s390_extzv_shift_ok (<bitsize>, -INTVAL (operands[2]),\n \t\t\t   INTVAL (operands[3]))\"\n-  \"risbg\\t%0,%1,%<bfstart>3,128+%<bfend>3,64-%2\"\n+  \"<risbg_n>\\t%0,%1,%<bfstart>3,128+%<bfend>3,64-%2\"\n   [(set_attr \"op_type\" \"RIE\")\n    (set_attr \"z10prop\" \"z10_super_E1\")])\n \n-(define_insn \"*extzv_<mode>_sll\"\n+(define_insn \"*extzv_<mode>_sll<clobbercc_or_nocc>\"\n   [(set (match_operand:GPR 0 \"register_operand\" \"=d\")\n \t(and:GPR (ashift:GPR\n \t\t  (match_operand:GPR 1 \"register_operand\" \"d\")\n \t\t  (match_operand:GPR 2 \"nonzero_shift_count_operand\" \"\"))\n-\t\t(match_operand:GPR 3 \"contiguous_bitmask_operand\" \"\")))\n-   (clobber (reg:CC CC_REGNUM))]\n-  \"TARGET_Z10\n+\t\t(match_operand:GPR 3 \"contiguous_bitmask_operand\" \"\")))]\n+  \"<z10_or_zEC12_cond>\n    && s390_extzv_shift_ok (<bitsize>, INTVAL (operands[2]),\n \t\t\t   INTVAL (operands[3]))\"\n-  \"risbg\\t%0,%1,%<bfstart>3,128+%<bfend>3,%2\"\n+  \"<risbg_n>\\t%0,%1,%<bfstart>3,128+%<bfend>3,%2\"\n   [(set_attr \"op_type\" \"RIE\")\n    (set_attr \"z10prop\" \"z10_super_E1\")])\n "}, {"sha": "ad456442de3b195390043d1f5a4acd539386c5ae", "filename": "gcc/config/s390/subst.md", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64c744b962798d44aa64c491d4b32b84a27d4e93/gcc%2Fconfig%2Fs390%2Fsubst.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64c744b962798d44aa64c491d4b32b84a27d4e93/gcc%2Fconfig%2Fs390%2Fsubst.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fs390%2Fsubst.md?ref=64c744b962798d44aa64c491d4b32b84a27d4e93", "patch": "@@ -120,3 +120,24 @@\n    (clobber (match_scratch:DSI 0 \"=d,d\"))])\n \n (define_subst_attr \"cconly\" \"cconly_subst\" \"\" \"_cconly\")\n+\n+\n+; Does transformations to switch between patterns unsing risbg +\n+; clobber CC (z10) and risbgn without clobber (zEC12).\n+(define_subst \"clobbercc_or_nocc_subst\"\n+  [(set (match_operand 0 \"\" \"\") (match_operand 1 \"\" \"\"))]\n+  \"\"\n+  [(set (match_dup 0) (match_dup 1))\n+   (clobber (reg:CC CC_REGNUM))])\n+\n+; Use this in the insn name to add the target suffix.\n+(define_subst_attr \"clobbercc_or_nocc\" \"clobbercc_or_nocc_subst\"\n+  \"_nocc\" \"_clobbercc\")\n+\n+; Use this in the condition.\n+(define_subst_attr \"z10_or_zEC12_cond\" \"clobbercc_or_nocc_subst\"\n+  \"TARGET_ZEC12\" \"TARGET_Z10 && ! TARGET_ZEC12\")\n+\n+; Use this instead of the risbg instruction.\n+(define_subst_attr \"risbg_n\" \"clobbercc_or_nocc_subst\"\n+  \"risbgn\" \"risbg\")"}, {"sha": "19ae0fa92bb8564ba7c693425c0e0e05c62ea1ca", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64c744b962798d44aa64c491d4b32b84a27d4e93/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64c744b962798d44aa64c491d4b32b84a27d4e93/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=64c744b962798d44aa64c491d4b32b84a27d4e93", "patch": "@@ -1,3 +1,9 @@\n+2016-09-23  Dominik Vogt  <vogt@linux.vnet.ibm.com>\n+\n+\t* gcc.target/s390/risbg-ll-1.c: Ported risbg tests from llvm.\n+\t* gcc.target/s390/risbg-ll-2.c: Ditto.\n+\t* gcc.target/s390/risbg-ll-3.c: Ditto.\n+\n 2016-09-23  Matthew Wahab  <matthew.wahab@arm.com>\n \n \t* gcc.target/arm/armv8_2-fp16-arith-1.c: New."}, {"sha": "30350d04c453a83b654e48141332b8bf12310282", "filename": "gcc/testsuite/gcc.target/s390/risbg-ll-1.c", "status": "added", "additions": 498, "deletions": 0, "changes": 498, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64c744b962798d44aa64c491d4b32b84a27d4e93/gcc%2Ftestsuite%2Fgcc.target%2Fs390%2Frisbg-ll-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64c744b962798d44aa64c491d4b32b84a27d4e93/gcc%2Ftestsuite%2Fgcc.target%2Fs390%2Frisbg-ll-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fs390%2Frisbg-ll-1.c?ref=64c744b962798d44aa64c491d4b32b84a27d4e93", "patch": "@@ -0,0 +1,498 @@\n+// Test sequences that can use RISBG with a zeroed first operand.\n+// The tests here assume that RISBLG isn't available.\n+\n+/* Tests ported from the Llvm testsuite. */\n+\n+/* { dg-do compile { target s390x-*-* } } */\n+/* { dg-options \"-O3 -march=z10 -mzarch -fno-asynchronous-unwind-tables\" } */\n+\n+#define i64 signed long long\n+#define ui64 unsigned long long\n+#define i32 signed int\n+#define ui32 unsigned int\n+#define i8 signed char\n+#define ui8 unsigned char\n+\n+// Test an extraction of bit 0 from a right-shifted value.\n+i32 f1 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f1:\\n\\trisbg\\t%r2,%r2,64-1,128\\\\\\+63,53\\\\\\+1\" } } */\n+  i32 v_shr = ((ui32)v_foo) >> 10;\n+  i32 v_and = v_shr & 1;\n+  return v_and;\n+}\n+\n+// ...and again with i64.\n+i64 f2 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f2:\\n\\trisbg\\t%r2,%r2,64-1,128\\\\\\+63,53\\\\\\+1\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f2:\\n\\trisbg\\t%r3,%r3,64-1,128\\\\\\+63,53\\\\\\+1\\n\\tlhi\\t%r2,0\" { target { ! lp64 } } } } */\n+  i64 v_shr = ((ui64)v_foo) >> 10;\n+  i64 v_and = v_shr & 1;\n+  return v_and;\n+}\n+\n+// Test an extraction of other bits from a right-shifted value.\n+i32 f3 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f3:\\n\\trisbg\\t%r2,%r2,60,128\\\\\\+61,64-22\" } } */\n+  i32 v_shr = ((ui32)v_foo) >> 22;\n+  i32 v_and = v_shr & 12;\n+  return v_and;\n+}\n+\n+// ...and again with i64.\n+i64 f4 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f4:\\n\\trisbg\\t%r2,%r2,60,128\\\\\\+61,64-22\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f4:\\n\\trisbg\\t%r3,%r3,60,128\\\\\\+61,64-22\\n\\tlhi\\t%r2,0\" { target { ! lp64 } } } } */\n+  i64 v_shr = ((ui64)v_foo) >> 22;\n+  i64 v_and = v_shr & 12;\n+  return v_and;\n+}\n+\n+// Test an extraction of most bits from a right-shifted value.\n+// The range should be reduced to exclude the zeroed high bits.\n+i32 f5 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f5:\\n\\trisbg\\t%r2,%r2,34,128\\\\\\+60,64-2\" } } */\n+  i32 v_shr = ((ui32)v_foo) >> 2;\n+  i32 v_and = v_shr & -8;\n+  return v_and;\n+}\n+\n+// ...and again with i64.\n+i64 f6 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f6:\\n\\trisbg\\t%r2,%r2,2,128\\\\\\+60,64-2\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f6:\\n\\trisbg\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\trisbg\\t%r2,%r3,2,128\\\\\\+60,64-2\" { target { ! lp64 } } } } */\n+  i64 v_shr = ((ui64)v_foo) >> 2;\n+  i64 v_and = v_shr & -8;\n+  return v_and;\n+}\n+\n+// Try the next value up (mask ....1111001).  This needs a separate shift\n+// and mask.\n+i32 f7 (i32 v_foo)\n+{\n+  /* Should be\n+     { dg-final { scan-assembler \"f7:\\n\\tsrl\\t%r2,2\\n\\tnill\\t%r2,65529\" { xfail { lp64 } } } }\n+     but because a zeroextend is merged into the pattern it is actually\n+     { dg-final { scan-assembler \"f7:\\n\\tsrl\\t%r2,2\\n\\tlgfi\\t%r1,1073741817\\n\\tngr\\t%r2,%r1\" { target { lp64 } } } }\n+     { dg-final { scan-assembler \"f7:\\n\\tsrl\\t%r2,2\\n\\tnill\\t%r2,65529\" { target { ! lp64 } } } } */\n+  i32 v_shr = ((ui32)v_foo) >> 2;\n+  i32 v_and = v_shr & -7;\n+  return v_and;\n+}\n+\n+// ...and again with i64.\n+i64 f8 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f8:\\n\\tsrlg\\t%r2,%r2,2\\n\\tnill\\t%r2,65529\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f8:\\n\\trisbg\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\tsrlg\\t%r2,%r3,2\\n\\tnill\\t%r2,65529\" { target { ! lp64 } } } } */\n+  i64 v_shr = ((ui64)v_foo) >> 2;\n+  i64 v_and = v_shr & -7;\n+  return v_and;\n+}\n+\n+// Test an extraction of bits from a left-shifted value.  The range should\n+// be reduced to exclude the zeroed low bits.\n+i32 f9 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f9:\\n\\trisbg\\t%r2,%r2,56,128\\\\\\+61,2\" } } */\n+  i32 v_shr = v_foo << 2;\n+  i32 v_and = v_shr & 255;\n+  return v_and;\n+}\n+\n+// ...and again with i64.\n+i64 f10 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f10:\\n\\trisbg\\t%r2,%r2,56,128\\\\\\+61,2\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f10:\\n\\trisbg\\t%r3,%r3,56,128\\\\\\+61,2\\n\\tlhi\\t%r2,0\" { target { ! lp64 } } } } */\n+  i64 v_shr = v_foo << 2;\n+  i64 v_and = v_shr & 255;\n+  return v_and;\n+}\n+\n+// Try a wrap-around mask (mask ....111100001111).  This needs a separate shift\n+// and mask.\n+i32 f11 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f11:\\n\\tsll\\t%r2,2\\n\\tnill\\t%r2,65295\" } } */\n+  i32 v_shr = v_foo << 2;\n+  i32 v_and = v_shr & -241;\n+  return v_and;\n+}\n+\n+// ...and again with i64.\n+i64 f12 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f12:\\n\\tsllg\\t%r2,%r2,2\\n\\tnill\\t%r2,65295\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f12:\\n\\trisbg\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\tsllg\\t%r2,%r3,2\\n\\tnill\\t%r2,65295\" { target { ! lp64 } } } } */\n+  i64 v_shr = v_foo << 2;\n+  i64 v_and = v_shr & -241;\n+  return v_and;\n+}\n+\n+// Test an extraction from a rotated value, no mask wraparound.\n+// This is equivalent to the lshr case, because the bits from the\n+// shl are not used.\n+i32 f13 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f13:\\n\\trisbg\\t%r2,%r2,56,128\\\\\\+60,32\\\\\\+14\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f13:\\n\\trll\\t%r2,%r2,14\\n\\tnilf\\t%r2,248\" { target { ! lp64 } } } } */\n+  i32 v_parta = v_foo << 14;\n+  i32 v_partb = ((ui32)v_foo) >> 18;\n+  i32 v_rotl = v_parta | v_partb;\n+  i32 v_and = v_rotl & 248;\n+  return v_and;\n+}\n+\n+// ...and again with i64.\n+i64 f14 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f14:\\n\\trisbg\\t%r2,%r2,56,128\\\\\\+60,14\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f14:\\n\\trisbg\\t%r3,%r2,56,128\\\\\\+60,46\\n\\tlhi\\t%r2,0\" { target { ! lp64 } } } } */\n+  i64 v_parta = v_foo << 14;\n+  i64 v_partb = ((ui64)v_foo) >> 50;\n+  i64 v_rotl = v_parta | v_partb;\n+  i64 v_and = v_rotl & 248;\n+  return v_and;\n+}\n+\n+// Try a case in which only the bits from the shl are used.\n+i32 f15 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f15:\\n\\trisbg\\t%r2,%r2,47,128\\\\\\+49,14\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f15:\\n\\trll\\t%r2,%r2,14\\n\\tnilf\\t%r2,114688\" { target { ! lp64 } } } } */\n+  i32 v_parta = v_foo << 14;\n+  i32 v_partb = ((ui32)v_foo) >> 18;\n+  i32 v_rotl = v_parta | v_partb;\n+  i32 v_and = v_rotl & 114688;\n+  return v_and;\n+}\n+\n+// ...and again with i64.\n+i64 f16 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f16:\\n\\trisbg\\t%r2,%r2,47,128\\\\\\+49,14\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f16:\\n\\trisbg\\t%r3,%r3,47,128\\\\\\+49,14\\n\\tlhi\\t%r2,0\" { target { ! lp64 } } } } */\n+  i64 v_parta = v_foo << 14;\n+  i64 v_partb = ((ui64)v_foo) >> 50;\n+  i64 v_rotl = v_parta | v_partb;\n+  i64 v_and = v_rotl & 114688;\n+  return v_and;\n+}\n+\n+// Test a 32-bit rotate in which both parts of the OR are needed.\n+// This needs a separate shift and mask.\n+i32 f17 (i32 v_foo)\n+{\n+  /* Should be\n+     { dg-final { scan-assembler \"f17:\\n\\trll\\t%r2,%r2,4\\n\\tnilf\\t%r2,126\" { xfail { lp64 } } } }\n+     but because a zeroextend is merged into the pattern it is actually\n+     { dg-final { scan-assembler \"f17:\\n\\trll\\t%r2,%r2,4\\n\\trisbg\\t%r2,%r2,57,128\\\\\\+62,0\" { target { lp64 } } } }\n+     { dg-final { scan-assembler \"f17:\\n\\trll\\t%r2,%r2,4\\n\\tnilf\\t%r2,126\" { target { ! lp64 } } } } */\n+  i32 v_parta = v_foo << 4;\n+  i32 v_partb = ((ui32)v_foo) >> 28;\n+  i32 v_rotl = v_parta | v_partb;\n+  i32 v_and = v_rotl & 126;\n+  return v_and;\n+}\n+\n+// ...and for i64, where RISBG should do the rotate too.\n+i64 f18 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f18:\\n\\trisbg\\t%r2,%r2,57,128\\\\\\+62,4\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f18:\\n\\trisbg\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\tlhi\\t%r2,0\\n\\trisbg\\t%r3,%r3,57,128\\\\\\+62,4\" { target { ! lp64 } } } } */\n+  i64 v_parta = v_foo << 4;\n+  i64 v_partb = ((ui64)v_foo) >> 60;\n+  i64 v_rotl = v_parta | v_partb;\n+  i64 v_and = v_rotl & 126;\n+  return v_and;\n+}\n+\n+// Test an arithmetic shift right in which some of the sign bits are kept.\n+// This needs a separate shift and mask.\n+i32 f19 (i32 v_foo)\n+{\n+  /* Should be\n+     { dg-final { scan-assembler \"f19:\\n\\tsra\\t%r2,28\\n\\tnilf\\t%r2,30\" { xfail { lp64 } } } }\n+     but because a zeroextend is merged into the pattern it is actually\n+     { dg-final { scan-assembler \"f19:\\n\\tsra\\t%r2,28\\n\\trisbg\\t%r2,%r2,59,128\\\\\\+62,0\" { target { lp64 } } } }\n+     { dg-final { scan-assembler \"f19:\\n\\tsra\\t%r2,28\\n\\tnilf\\t%r2,30\" { target { ! lp64 } } } } */\n+  i32 v_shr = v_foo >> 28;\n+  i32 v_and = v_shr & 30;\n+  return v_and;\n+}\n+\n+// ...and again with i64.  In this case RISBG is the best way of doing the AND.\n+i64 f20 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f20:\\n\\tsrag\\t%r2,%r2,60\\n\\trisbg\\t%r2,%r2,59,128\\\\\\+62,0\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f20:\\n\\trisbg\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\tlhi\\t%r2,0\\n\\tsrag\\t%r3,%r3,60\\n\\tnilf\\t%r3,30\" { target { ! lp64 } } } } */\n+  i64 v_shr = v_foo >> 60;\n+  i64 v_and = v_shr & 30;\n+  return v_and;\n+}\n+\n+// Now try an arithmetic right shift in which the sign bits aren't needed.\n+// Note: Unlike Llvm, Gcc replaces the ashrt with a lshrt in any case, using\n+// a risbg pattern without ashrt.\n+i32 f21 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f21:\\n\\trisbg\\t%r2,%r2,60,128\\\\\\+62,64-28\" } } */\n+  i32 v_shr = v_foo >> 28;\n+  i32 v_and = v_shr & 14;\n+  return v_and;\n+}\n+\n+// ...and again with i64.\n+i64 f22 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f22:\\n\\trisbg\\t%r2,%r2,60,128\\\\\\+62,64-60\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f22:\\n\\trisbg\\t%r3,%r2,60,128\\\\\\+62,64-28\\n\\tlhi\\t%r2,0\" { target { ! lp64 } } } } */\n+  i64 v_shr = v_foo >> 60;\n+  i64 v_and = v_shr & 14;\n+  return v_and;\n+}\n+\n+// Check that we use RISBG for shifted values even if the AND is a\n+// natural zero extension.\n+i64 f23 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f23:\\n\\trisbg\\t%r2,%r2,64-8,128\\\\\\+63,54\\\\\\+8\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f23:\\n\\trisbg\\t%r3,%r3,64-8,128\\\\\\+63,54\\\\\\+8\\n\\tlhi\\t%r2,0\" { target { ! lp64 } } } } */\n+  i64 v_shr = ((ui64)v_foo) >> 2;\n+  i64 v_and = v_shr & 255;\n+  return v_and;\n+}\n+\n+// Test a case where the AND comes before a rotate.  This needs a separate\n+// mask and rotate.\n+i32 f24 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f24:\\n\\tnilf\\t%r2,254\\n\\trll\\t%r2,%r2,29\" } } */\n+  i32 v_and = v_foo & 254;\n+  i32 v_parta = ((ui32)v_and) >> 3;\n+  i32 v_partb = v_and << 29;\n+  i32 v_rotl = v_parta | v_partb;\n+  return v_rotl;\n+}\n+\n+// ...and again with i64, where a single RISBG is enough.\n+i64 f25 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f25:\\n\\trisbg\\t%r2,%r2,57,128\\\\\\+59,3\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f25:\\n\\trisbg\\t%r3,%r3,57,128\\\\\\+59,3\\n\\tlhi\\t%r2,0\" { target { ! lp64 } } } } */\n+  i64 v_and = v_foo & 14;\n+  i64 v_parta = v_and << 3;\n+  i64 v_partb = ((ui64)v_and) >> 61;\n+  i64 v_rotl = v_parta | v_partb;\n+  return v_rotl;\n+}\n+\n+// Test a wrap-around case in which the AND comes before a rotate.\n+// This again needs a separate mask and rotate.\n+i32 f26 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f26:\\n\\tnill\\t%r2,65487\\n\\trll\\t%r2,%r2,5\" } } */\n+  i32 v_and = v_foo & -49;\n+  i32 v_parta = v_and << 5;\n+  i32 v_partb = ((ui32)v_and) >> 27;\n+  i32 v_rotl = v_parta | v_partb;\n+  return v_rotl;\n+}\n+\n+// ...and again with i64, where a single RISBG is OK.\n+i64 f27 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f27:\\n\\trisbg\\t%r2,%r2,55,128\\\\\\+52,5\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f27:\\n\\trisbg\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\trisbg\\t%r2,%r3,55,128\\\\\\+52,5\" { target { ! lp64 } } } } */\n+  i64 v_and = v_foo & -49;\n+  i64 v_parta = v_and << 5;\n+  i64 v_partb = ((ui64)v_and) >> 59;\n+  i64 v_rotl = v_parta | v_partb;\n+  return v_rotl;\n+}\n+\n+// Test a case where the AND comes before a shift left.\n+i32 f28 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f28:\\n\\trisbg\\t%r2,%r2,32,128\\\\\\+45,17\" } } */\n+  i32 v_and = v_foo & 32766;\n+  i32 v_shl = v_and << 17;\n+  return v_shl;\n+}\n+\n+// ...and again with i64.\n+i64 f29 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f29:\\n\\trisbg\\t%r2,%r2,0,128\\\\\\+13,49\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f29:\\n\\trisbg\\t%r\\[23\\],%r3,0,128\\\\\\+13,49\\n\\tlr\\t%r\\[23\\],%r\\[32\\]\\n\\tsrlg\\t%r2,%r2\" { target { ! lp64 } } } } */\n+  i64 v_and = v_foo & 32766;\n+  i64 v_shl = v_and << 49;\n+  return v_shl;\n+}\n+\n+// Test the next shift up from f28, in which the mask should get shortened.\n+i32 f30 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f30:\\n\\trisbg\\t%r2,%r2,32,128\\\\\\+44,18\" } } */\n+  i32 v_and = v_foo & 32766;\n+  i32 v_shl = v_and << 18;\n+  return v_shl;\n+}\n+\n+// ...and again with i64.\n+i64 f31 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f31:\\n\\trisbg\\t%r2,%r2,0,128\\\\\\+12,50\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f31:\\n\\trisbg\\t%r\\[23\\],%r3,0,128\\\\\\+12,50\\n\\tlr\\t%r\\[23\\],%r\\[32\\]\\n\\tsrlg\\t%r2,%r2\" { target { ! lp64 } } } } */\n+  i64 v_and = v_foo & 32766;\n+  i64 v_shl = v_and << 50;\n+  return v_shl;\n+}\n+\n+// Test a wrap-around case in which the shift left comes after the AND.\n+// We can't use RISBG for the shift in that case.\n+i32 f32 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f32:\\n\\tsll\\t%r2,10\\n\\tnill\\t%r2,58368\" } } */\n+  i32 v_and = v_foo & -7;\n+  i32 v_shl = v_and << 10;\n+  return v_shl;\n+}\n+\n+// ...and again with i64.\n+i64 f33 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f33:\\n\\tsllg\\t%r2,%r2,10\\n\\tnill\\t%r2,58368\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f33:\\n\\trisbg\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\tsllg\\t%r2,%r3,10\\n\\tnill\\t%r2,58368\" { target { ! lp64 } } } } */\n+  i64 v_and = v_foo & -7;\n+  i64 v_shl = v_and << 10;\n+  return v_shl;\n+}\n+\n+// Test a case where the AND comes before a shift right.\n+i32 f34 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f34:\\n\\trisbg\\t%r2,%r2,64-7,128\\\\\\+63,48\\\\\\+7\" } } */\n+  i32 v_and = v_foo & 65535;\n+  i32 v_shl = ((ui32)v_and) >> 9;\n+  return v_shl;\n+}\n+\n+// ...and again with i64.\n+i64 f35 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f35:\\n\\trisbg\\t%r2,%r2,64-7,128\\\\\\+63,48\\\\\\+7\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f35:\\n\\trisbg\\t%r3,%r3,64-7,128\\\\\\+63,48\\\\\\+7\\n\\tlhi\\t%r2,0\" { target { ! lp64 } } } } */\n+  i64 v_and = v_foo & 65535;\n+  i64 v_shl = ((ui64)v_and) >> 9;\n+  return v_shl;\n+}\n+\n+// Test a wrap-around case where the AND comes before a shift right.\n+// We can't use RISBG for the shift in that case.\n+i32 f36 (i32 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f36:\\n\\tsrl\\t%r2,1\\n\\tlgfi\\t%r1,2147483635\\n\\tngr\\t%r2,%r1\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f36:\\n\\tsrl\\t%r2,1\\n\\tnilf\\t%r2,2147483635\" { target { ! lp64 } } } } */\n+  i32 v_and = v_foo & -25;\n+  i32 v_shl = ((ui32)v_and) >> 1;\n+  return v_shl;\n+}\n+\n+// ...and again with i64.\n+i64 f37 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f37:\\n\\(\\t.*\\n\\)*\\tsrlg\\t%r2,%r2,1\\n\\tng\\t%r2,\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f37:\\n\\(\\t.*\\n\\)*\\trisbg\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\tsrlg\\t%r2,%r3,1\\n\\tng\\t%r2,\" { target { ! lp64 } } } } */\n+  i64 v_and = v_foo & -25;\n+  i64 v_shl = ((ui64)v_and) >> 1;\n+  return v_shl;\n+}\n+\n+// Test a combination involving a large ASHR and a shift left.  We can't\n+// use RISBG there.\n+i64 f38 (i64 v_foo)\n+{\n+  /* { dg-final { scan-assembler \"f38:\\n\\tsrag\\t%r2,%r2,32\\n\\tsllg\\t%r2,%r2,5\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f38:\\n\\trisbg\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\tsrag\\t%r2,%r3,32\\n\\tsllg\\t%r2,%r2,5\" { target { ! lp64 } } } } */\n+  i64 v_ashr = v_foo >> 32;\n+  i64 v_shl = v_ashr << 5;\n+  return v_shl;\n+}\n+\n+// Try a similar thing in which no shifted sign bits are kept.\n+i64 f39 (i64 v_foo, i64 *v_dest)\n+{\n+  /* { dg-final { scan-assembler \"f39:\\n\\tsrag\\t%r2,%r2,35\\n\\(\\t.*\\n\\)*\\trisbg\\t%r2,%r2,33,128\\\\\\+61,2\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f39:\\n\\trisbg\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\tlhi\\t%r2,0\\n\\tsrag\\t%r3,%r3,35\\n\\(\\t.*\\n\\)*\\trisbg\\t%r3,%r3,33,128\\\\\\+61,2\" { target { ! lp64 } } } } */\n+  i64 v_ashr = v_foo >> 35;\n+  *v_dest = v_ashr;\n+  i64 v_shl = v_ashr << 2;\n+  i64 v_and = v_shl & 2147483647;\n+  return v_and;\n+}\n+\n+// ...and again with the next highest shift value, where one sign bit is kept.\n+i64 f40 (i64 v_foo, i64 *v_dest)\n+{\n+  /* { dg-final { scan-assembler \"f40:\\n\\tsrag\\t%r2,%r2,36\\n\\(\\t.*\\n\\)*\\trisbg\\t%r2,%r2,33,128\\\\\\+61,2\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f40:\\n\\trisbg\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\tlhi\\t%r2,0\\n\\tsrag\\t%r3,%r3,36\\n\\(\\t.*\\n\\)*\\trisbg\\t%r3,%r3,33,128\\\\\\+61,2\" { target { ! lp64 } } } } */\n+  i64 v_ashr = v_foo >> 36;\n+  *v_dest = v_ashr;\n+  i64 v_shl = v_ashr << 2;\n+  i64 v_and = v_shl & 2147483647;\n+  return v_and;\n+}\n+\n+// Check a case where the result is zero-extended.\n+i64 f41 (i32 v_a)\n+{\n+  /* { dg-final { scan-assembler \"f41:\\n\\trisbg\\t%r2,%r2,64-28,128\\\\\\+63,34\\\\\\+28\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f41:\\n\\trisbg\\t%r3,%r2,64-28,128\\\\\\+63,34\\\\\\+28\\n\\tlhi\\t%r2,0\" { target { ! lp64 } } } } */\n+  i32 v_shl = v_a << 2;\n+  i32 v_shr = ((ui32)v_shl) >> 4;\n+  i64 v_ext = (ui64)v_shr;\n+  return v_ext;\n+}\n+\n+// In this case the sign extension is converted to a pair of 32-bit shifts,\n+// which is then extended to 64 bits.  We previously used the wrong bit size\n+// when testing whether the shifted-in bits of the shift right were significant.\n+typedef struct { ui64 pad : 63; ui8 a : 1; } t42;\n+i64 f42 (t42 v_x)\n+{\n+  /* { dg-final { scan-assembler \"f42:\\n\\tsllg\\t%r2,%r2,63\\n\\tsrag\\t%r2,%r2,63\\n\\tllgcr\\t%r2,%r2\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f42:\\n\\tsllg\\t%r3,%r3,63\\n\\tlhi\\t%r2,0\\n\\tsrag\\t%r3,%r3,63\\n\\tllcr\\t%r3,%r3\" { target { ! lp64 } } } } */\n+  ui8 a = v_x.a << 7;\n+  i8 ext = ((i8)a) >> 7;\n+  i64 ext2 = (ui64)(ui8)ext;\n+  return ext2;\n+}\n+\n+// Check that we get the case where a 64-bit shift is used by a 32-bit and.\n+i32 f43 (i64 v_x)\n+{\n+  /* { dg-final { scan-assembler \"f43:\\n\\trisbg\\t%r2,%r2,32,128\\\\\\+61,64-12\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f43:\\n\\trisbg\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\trisbg\\t%r2,%r3,32,128\\\\\\+61,64-12\" { target { ! lp64 } } } } */\n+  i64 v_shr3 = ((ui64)v_x) >> 12;\n+  i32 v_shr3_tr = (ui32)v_shr3;\n+  i32 v_conv = v_shr3_tr & -4;\n+  return v_conv;\n+}\n+\n+// Check that we don't get the case where the 32-bit and mask is not contiguous\n+i32 f44 (i64 v_x)\n+{\n+  /* { dg-final { scan-assembler \"f44:\\n\\tsrlg\\t%r2,%r2,12\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f44:\\n\\tsrlg\\t%r2,%r3,12\\n\\tnilf\\t%r2,10\" { target { ! lp64 } } } } */\n+  i64 v_shr4 = ((ui64)v_x) >> 12;\n+  i32 v_conv = (ui32)v_shr4;\n+  i32 v_and = v_conv & 10;\n+  return v_and;\n+}"}, {"sha": "6588dc7ae96eac2d43c621f57cd5a987f3f6fc30", "filename": "gcc/testsuite/gcc.target/s390/risbg-ll-2.c", "status": "added", "additions": 123, "deletions": 0, "changes": 123, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64c744b962798d44aa64c491d4b32b84a27d4e93/gcc%2Ftestsuite%2Fgcc.target%2Fs390%2Frisbg-ll-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64c744b962798d44aa64c491d4b32b84a27d4e93/gcc%2Ftestsuite%2Fgcc.target%2Fs390%2Frisbg-ll-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fs390%2Frisbg-ll-2.c?ref=64c744b962798d44aa64c491d4b32b84a27d4e93", "patch": "@@ -0,0 +1,123 @@\n+// Test sequences that can use RISBG with a normal first operand.\n+\n+/* Tests ported from the Llvm testsuite. */\n+\n+/* { dg-do compile { target s390x-*-* } } */\n+/* { dg-options \"-O3 -march=z10 -mzarch -fno-asynchronous-unwind-tables\" }  */\n+\n+#define i64 signed long long\n+#define ui64 unsigned long long\n+#define i32 signed int\n+#define ui32 unsigned int\n+\n+// Test a case with two ANDs.\n+i32 f1 (i32 v_a, i32 v_b)\n+{\n+  /* { dg-final { scan-assembler \"f1:\\n\\trisbg\\t%r2,%r3,60,62,0\" } } */\n+  i32 v_anda = v_a & -15;\n+  i32 v_andb = v_b & 14;\n+  i32 v_or = v_anda | v_andb;\n+  return v_or;\n+}\n+\n+// ...and again with i64.\n+i64 f2 (i64 v_a, i64 v_b)\n+{\n+  /* { dg-final { scan-assembler \"f2:\\n\\trisbg\\t%r2,%r3,60,62,0\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f2:\\n\\trisbg\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\(\\t.*\\n\\)*\\trisbg\\t%r\\[23\\],%r5,60,62,0\" { target { ! lp64 } } } } */\n+  i64 v_anda = v_a & -15;\n+  i64 v_andb = v_b & 14;\n+  i64 v_or = v_anda | v_andb;\n+  return v_or;\n+}\n+\n+// Test a case with two ANDs and a shift.\n+i32 f3 (i32 v_a, i32 v_b)\n+{\n+  /* { dg-final { scan-assembler \"f3:\\n\\trisbg\\t%r2,%r3,64-4,63,4\\\\\\+52\" } } */\n+  i32 v_anda = v_a & -16;\n+  i32 v_shr = ((ui32)v_b) >> 8;\n+  i32 v_andb = v_shr & 15;\n+  i32 v_or = v_anda | v_andb;\n+  return v_or;\n+}\n+\n+// ...and again with i64.\n+i64 f4 (i64 v_a, i64 v_b)\n+{\n+  /* { dg-final { scan-assembler \"f4:\\n\\trisbg\\t%r2,%r3,60,60\\\\\\+4-1,128-60-4-8\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f4:\\n\\(\\t.*\\n\\)*\\trisbg\\t%r5,%r5,64-4,128\\\\\\+63,52\\\\\\+4\" { target { ! lp64 } } } } */\n+  i64 v_anda = v_a & -16;\n+  i64 v_shr = ((ui64)v_b) >> 8;\n+  i64 v_andb = v_shr & 15;\n+  i64 v_or = v_anda | v_andb;\n+  return v_or;\n+}\n+\n+// Test a case with a single AND and a left shift.\n+i32 f5 (i32 v_a, i32 v_b)\n+{\n+  /* { dg-final { scan-assembler \"f5:\\n\\trisbg\\t%r2,%r3,32,64-10-1,10\" } } */\n+  i32 v_anda = v_a & 1023;\n+  i32 v_shlb = v_b << 10;\n+  i32 v_or = v_anda | v_shlb;\n+  return v_or;\n+}\n+\n+// ...and again with i64.\n+i64 f6 (i64 v_a, i64 v_b)\n+{\n+  /* { dg-final { scan-assembler \"f6:\\n\\trisbg\\t%r2,%r3,0,64-10-1,10\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f6:\\n\\trisbg\\t%r5,%r4,0,0\\\\\\+32-1,64-0-32\\n\\(\\t.*\\n\\)*\\trisbg\\t%r\\[23\\],%r5,0,64-10-1,10\" { target { ! lp64 } } } } */\n+  i64 v_anda = v_a & 1023;\n+  i64 v_shlb = v_b << 10;\n+  i64 v_or = v_anda | v_shlb;\n+  return v_or;\n+}\n+\n+// Test a case with a single AND and a right shift.\n+i32 f7 (i32 v_a, i32 v_b)\n+{\n+  /* { dg-final { scan-assembler \"f7:\\n\\trisbg\\t%r2,%r3,32\\\\\\+8,63,64-8\" } } */\n+  i32 v_anda = v_a & -16777216;\n+  i32 v_shrb = ((ui32)v_b) >> 8;\n+  i32 v_or = v_anda | v_shrb;\n+  return v_or;\n+}\n+\n+// ...and again with i64.\n+i64 f8 (i64 v_a, i64 v_b)\n+{\n+  /* { dg-final { scan-assembler \"f8:\\n\\trisbg\\t%r2,%r3,8,63,64-8\" { target { lp64 } } } } */\n+  /* With -m31 risbg is not really useful here, so do not test for it.  */\n+  i64 v_anda = v_a & -72057594037927936;\n+  i64 v_shrb = ((ui64)v_b) >> 8;\n+  i64 v_or = v_anda | v_shrb;\n+  return v_or;\n+}\n+\n+// Check that we can get the case where a 64-bit shift feeds a 32-bit or of\n+// ands with complement masks.\n+i32 f9 (i64 v_x, i32 v_y)\n+{\n+  /* { dg-final { scan-assembler \"f9:\\n\\trisbg\\t%r3,%r2,48,63,64-48\" { target { lp64 } }} } */\n+  /* { dg-final { scan-assembler \"f9:\\n\\trisbg\\t%r4,%r2,32\\\\+16,63,64-16\" { target { ! lp64 } }} } */\n+  i64 v_shr6 = ((ui64)v_x) >> 48;\n+  i32 v_conv = (ui32)v_shr6;\n+  i32 v_and1 = v_y & -65536;\n+  i32 v_or = v_conv | v_and1;\n+  return v_or;\n+}\n+\n+// Check that we don't get the case where a 64-bit shift feeds a 32-bit or of\n+// ands with incompatible masks.\n+i32 f10 (i64 v_x, i32 v_y)\n+{\n+  /* { dg-final { scan-assembler \"f10:\\n\\tsrlg\\t%r2,%r2,48\\n\\trosbg\\t%r2,%r3,32,39,0\" { target { lp64 } } } } */\n+  /* { dg-final { scan-assembler \"f10:\\n\\tnilf\\t%r4,4278190080\\n\\trosbg\\t%r4,%r2,32\\\\\\+16,63,64-16\" { target { ! lp64 } } } } */\n+  i64 v_shr6 = ((ui64)v_x) >> 48;\n+  i32 v_conv = (ui32)v_shr6;\n+  i32 v_and1 = v_y & -16777216;\n+  i32 v_or = v_conv | v_and1;\n+  return v_or;\n+}"}, {"sha": "838f1ffbd91092d9bc8c1e236398b448f1d42c38", "filename": "gcc/testsuite/gcc.target/s390/risbg-ll-3.c", "status": "added", "additions": 44, "deletions": 0, "changes": 44, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/64c744b962798d44aa64c491d4b32b84a27d4e93/gcc%2Ftestsuite%2Fgcc.target%2Fs390%2Frisbg-ll-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/64c744b962798d44aa64c491d4b32b84a27d4e93/gcc%2Ftestsuite%2Fgcc.target%2Fs390%2Frisbg-ll-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fs390%2Frisbg-ll-3.c?ref=64c744b962798d44aa64c491d4b32b84a27d4e93", "patch": "@@ -0,0 +1,44 @@\n+// Test use of RISBG vs RISBGN on zEC12.\n+\n+/* Tests ported from the Llvm testsuite. */\n+\n+/* { dg-do compile { target s390x-*-* } } */\n+/* { dg-options \"-O3 -march=zEC12 -mzarch -fno-asynchronous-unwind-tables\" } */\n+\n+#define i64 signed long long\n+#define ui64 unsigned long long\n+\n+// On zEC12, we generally prefer RISBGN.\n+i64 f1 (i64 v_a, i64 v_b)\n+{\n+/* { dg-final { scan-assembler \"f1:\\n\\trisbgn\\t%r2,%r3,60,60\\\\\\+3-1,128-60-3-1\" { target { lp64 } } } } */\n+/* { dg-final { scan-assembler \"f1:\\n\\trisbgn\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\trisbgn\\t%r3,%r5,60,62,0\\n\" { target { ! lp64 } } } } */\n+  i64 v_anda = v_a & -15;\n+  i64 v_andb = v_b & 14;\n+  i64 v_or = v_anda | v_andb;\n+  return v_or;\n+}\n+\n+// But we may fall back to RISBG if we can use the condition code.\n+extern i64 f2_foo();\n+i64 f2 (i64 v_a, i64 v_b)\n+{\n+/* { dg-final { scan-assembler \"f2:\\n\\trisbg\\t%r2,%r3,60,62,0\\n\\tje\\t\" { target { lp64 } } } } */\n+/* { dg-final { scan-assembler \"f2:\\n\\trisbgn\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\trisbg\\t%r3,%r5,60,62,0\" { target { ! lp64 } } } } */\n+  i64 v_anda = v_a & -15;\n+  i64 v_andb = v_b & 14;\n+  i64 v_or = v_anda | v_andb;\n+  if (! v_or)\n+    return f2_foo();\n+  else\n+    return v_or;\n+}\n+\n+void f2_bar ();\n+void f2_cconly (i64 v_a, i64 v_b)\n+{\n+/* { dg-final { scan-assembler \"f2_cconly:\\n\\trisbg\\t%r3,%r2,63,59,0\\n\\tjne\\t\"  { target { lp64 } } } } */\n+/* { dg-final { scan-assembler \"f2_cconly:\\n\\trisbgn\\t%r3,%r2,0,0\\\\\\+32-1,64-0-32\\n\\trisbg\\t%r3,%r5,60,62,0\\n\\tjne\\t\" { target { ! lp64 } } } } */\n+  if ((v_a & -15) | (v_b & 14))\n+    f2_bar();\n+}"}]}