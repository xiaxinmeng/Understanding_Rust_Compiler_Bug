{"sha": "a7f24614b3c58d03f40d55fe195056dd8423f8f5", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YTdmMjQ2MTRiM2M1OGQwM2Y0MGQ1NWZlMTk1MDU2ZGQ4NDIzZjhmNQ==", "commit": {"author": {"name": "Richard Biener", "email": "rguenther@suse.de", "date": "2014-11-14T09:30:08Z"}, "committer": {"name": "Richard Biener", "email": "rguenth@gcc.gnu.org", "date": "2014-11-14T09:30:08Z"}, "message": "match.pd: Implement more binary patterns exercised by fold_stmt.\n\n2014-11-14  Richard Biener  <rguenther@suse.de>\n\n\t* match.pd: Implement more binary patterns exercised by\n\tfold_stmt.\n\t* fold-const.c (sing_bit_p): Export.\n\t(exact_inverse): Likewise.\n\t(fold_binary_loc): Remove patterns here.\n\t(tree_unary_nonnegative_warnv_p): Use CASE_CONVERT.\n\t* fold-const.h (sing_bit_p): Declare.\n\t(exact_inverse): Likewise.\n\n\t* gcc.c-torture/execute/shiftopt-1.c: XFAIL invalid parts.\n\nFrom-SVN: r217545", "tree": {"sha": "9177e5ba8b8545c43beebde22b0469869aeb6402", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/9177e5ba8b8545c43beebde22b0469869aeb6402"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/a7f24614b3c58d03f40d55fe195056dd8423f8f5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a7f24614b3c58d03f40d55fe195056dd8423f8f5", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a7f24614b3c58d03f40d55fe195056dd8423f8f5", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a7f24614b3c58d03f40d55fe195056dd8423f8f5/comments", "author": {"login": "rguenth", "id": 2046526, "node_id": "MDQ6VXNlcjIwNDY1MjY=", "avatar_url": "https://avatars.githubusercontent.com/u/2046526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rguenth", "html_url": "https://github.com/rguenth", "followers_url": "https://api.github.com/users/rguenth/followers", "following_url": "https://api.github.com/users/rguenth/following{/other_user}", "gists_url": "https://api.github.com/users/rguenth/gists{/gist_id}", "starred_url": "https://api.github.com/users/rguenth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rguenth/subscriptions", "organizations_url": "https://api.github.com/users/rguenth/orgs", "repos_url": "https://api.github.com/users/rguenth/repos", "events_url": "https://api.github.com/users/rguenth/events{/privacy}", "received_events_url": "https://api.github.com/users/rguenth/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "5b98e88f0ca10ffc89eed4c8fb69ad1c74ef5b44", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5b98e88f0ca10ffc89eed4c8fb69ad1c74ef5b44", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5b98e88f0ca10ffc89eed4c8fb69ad1c74ef5b44"}], "stats": {"total": 459, "additions": 243, "deletions": 216}, "files": [{"sha": "49e1b7b1d49c34d601575be6f5e074f91ffe16aa", "filename": "gcc/ChangeLog", "status": "modified", "additions": 11, "deletions": 0, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a7f24614b3c58d03f40d55fe195056dd8423f8f5/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a7f24614b3c58d03f40d55fe195056dd8423f8f5/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=a7f24614b3c58d03f40d55fe195056dd8423f8f5", "patch": "@@ -1,3 +1,14 @@\n+2014-11-14  Richard Biener  <rguenther@suse.de>\n+\n+\t* match.pd: Implement more binary patterns exercised by\n+\tfold_stmt.\n+\t* fold-const.c (sing_bit_p): Export.\n+\t(exact_inverse): Likewise.\n+\t(fold_binary_loc): Remove patterns here.\n+\t(tree_unary_nonnegative_warnv_p): Use CASE_CONVERT.\n+\t* fold-const.h (sing_bit_p): Declare.\n+\t(exact_inverse): Likewise.\n+\n 2014-11-14  Marek Polacek  <polacek@redhat.com>\n \n \t* tree.c (build_common_builtin_nodes): Remove doubled ECF_LEAF."}, {"sha": "0170b88daefb69ec7955de5e5560cd4626731ee6", "filename": "gcc/fold-const.c", "status": "modified", "additions": 3, "deletions": 207, "changes": 210, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a7f24614b3c58d03f40d55fe195056dd8423f8f5/gcc%2Ffold-const.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a7f24614b3c58d03f40d55fe195056dd8423f8f5/gcc%2Ffold-const.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ffold-const.c?ref=a7f24614b3c58d03f40d55fe195056dd8423f8f5", "patch": "@@ -130,7 +130,6 @@ static tree decode_field_reference (location_t, tree, HOST_WIDE_INT *,\n \t\t\t\t    HOST_WIDE_INT *,\n \t\t\t\t    machine_mode *, int *, int *,\n \t\t\t\t    tree *, tree *);\n-static tree sign_bit_p (tree, const_tree);\n static int simple_operand_p (const_tree);\n static bool simple_operand_p_2 (tree);\n static tree range_binop (enum tree_code, tree, tree, int, tree, int);\n@@ -3651,7 +3650,7 @@ all_ones_mask_p (const_tree mask, unsigned int size)\n    The return value is the (sub)expression whose sign bit is VAL,\n    or NULL_TREE otherwise.  */\n \n-static tree\n+tree\n sign_bit_p (tree exp, const_tree val)\n {\n   int width;\n@@ -9474,7 +9473,7 @@ fold_addr_of_array_ref_difference (location_t loc, tree type,\n /* If the real or vector real constant CST of type TYPE has an exact\n    inverse, return it, else return NULL.  */\n \n-static tree\n+tree\n exact_inverse (tree type, tree cst)\n {\n   REAL_VALUE_TYPE r;\n@@ -9963,25 +9962,6 @@ fold_binary_loc (location_t loc,\n \t}\n       else\n \t{\n-\t  /* See if ARG1 is zero and X + ARG1 reduces to X.  */\n-\t  if (fold_real_zero_addition_p (TREE_TYPE (arg0), arg1, 0))\n-\t    return non_lvalue_loc (loc, fold_convert_loc (loc, type, arg0));\n-\n-\t  /* Likewise if the operands are reversed.  */\n-\t  if (fold_real_zero_addition_p (TREE_TYPE (arg1), arg0, 0))\n-\t    return non_lvalue_loc (loc, fold_convert_loc (loc, type, arg1));\n-\n-\t  /* Convert X + -C into X - C.  */\n-\t  if (TREE_CODE (arg1) == REAL_CST\n-\t      && REAL_VALUE_NEGATIVE (TREE_REAL_CST (arg1)))\n-\t    {\n-\t      tem = fold_negate_const (arg1, type);\n-\t      if (!TREE_OVERFLOW (arg1) || !flag_trapping_math)\n-\t\treturn fold_build2_loc (loc, MINUS_EXPR, type,\n-\t\t\t\t    fold_convert_loc (loc, type, arg0),\n-\t\t\t\t    fold_convert_loc (loc, type, tem));\n-\t    }\n-\n \t  /* Fold __complex__ ( x, 0 ) + __complex__ ( 0, y )\n \t     to __complex__ ( x, y ).  This is not the same for SNaNs or\n \t     if signed zeros are involved.  */\n@@ -10023,12 +10003,6 @@ fold_binary_loc (location_t loc,\n \t      && (tem = distribute_real_division (loc, code, type, arg0, arg1)))\n \t    return tem;\n \n-\t  /* Convert x+x into x*2.0.  */\n-\t  if (operand_equal_p (arg0, arg1, 0)\n-\t      && SCALAR_FLOAT_TYPE_P (type))\n-\t    return fold_build2_loc (loc, MULT_EXPR, type, arg0,\n-\t\t\t\tbuild_real (type, dconst2));\n-\n           /* Convert a + (b*c + d*e) into (a + b*c) + d*e.\n              We associate floats only if the user has specified\n              -fassociative-math.  */\n@@ -10381,9 +10355,6 @@ fold_binary_loc (location_t loc,\n \n       if (! FLOAT_TYPE_P (type))\n \t{\n-\t  if (integer_zerop (arg0))\n-\t    return negate_expr (fold_convert_loc (loc, type, arg1));\n-\n \t  /* Fold A - (A & B) into ~B & A.  */\n \t  if (!TREE_SIDE_EFFECTS (arg0)\n \t      && TREE_CODE (arg1) == BIT_AND_EXPR)\n@@ -10428,16 +10399,6 @@ fold_binary_loc (location_t loc,\n \t    }\n \t}\n \n-      /* See if ARG1 is zero and X - ARG1 reduces to X.  */\n-      else if (fold_real_zero_addition_p (TREE_TYPE (arg0), arg1, 1))\n-\treturn non_lvalue_loc (loc, fold_convert_loc (loc, type, arg0));\n-\n-      /* (ARG0 - ARG1) is the same as (-ARG1 + ARG0).  So check whether\n-\t ARG0 is zero and X + ARG0 reduces to X, since that would mean\n-\t (-ARG1 + ARG0) reduces to -ARG1.  */\n-      else if (fold_real_zero_addition_p (TREE_TYPE (arg1), arg0, 0))\n-\treturn negate_expr (fold_convert_loc (loc, type, arg1));\n-\n       /* Fold __complex__ ( x, 0 ) - __complex__ ( 0, y ) to\n \t __complex__ ( x, -y ).  This is not the same for SNaNs or if\n \t signed zeros are involved.  */\n@@ -10553,11 +10514,6 @@ fold_binary_loc (location_t loc,\n \n       if (! FLOAT_TYPE_P (type))\n \t{\n-\t  /* Transform x * -1 into -x.  Make sure to do the negation\n-\t     on the original operand with conversions not stripped\n-\t     because we can only strip non-sign-changing conversions.  */\n-\t  if (integer_minus_onep (arg1))\n-\t    return fold_convert_loc (loc, type, negate_expr (op0));\n \t  /* Transform x * -C into -x * C if x is easily negatable.  */\n \t  if (TREE_CODE (arg1) == INTEGER_CST\n \t      && tree_int_cst_sgn (arg1) == -1\n@@ -10621,29 +10577,6 @@ fold_binary_loc (location_t loc,\n \t}\n       else\n \t{\n-\t  /* Maybe fold x * 0 to 0.  The expressions aren't the same\n-\t     when x is NaN, since x * 0 is also NaN.  Nor are they the\n-\t     same in modes with signed zeros, since multiplying a\n-\t     negative value by 0 gives -0, not +0.  */\n-\t  if (!HONOR_NANS (TYPE_MODE (TREE_TYPE (arg0)))\n-\t      && !HONOR_SIGNED_ZEROS (TYPE_MODE (TREE_TYPE (arg0)))\n-\t      && real_zerop (arg1))\n-\t    return omit_one_operand_loc (loc, type, arg1, arg0);\n-\t  /* In IEEE floating point, x*1 is not equivalent to x for snans.\n-\t     Likewise for complex arithmetic with signed zeros.  */\n-\t  if (!HONOR_SNANS (TYPE_MODE (TREE_TYPE (arg0)))\n-\t      && (!HONOR_SIGNED_ZEROS (TYPE_MODE (TREE_TYPE (arg0)))\n-\t\t  || !COMPLEX_FLOAT_TYPE_P (TREE_TYPE (arg0)))\n-\t      && real_onep (arg1))\n-\t    return non_lvalue_loc (loc, fold_convert_loc (loc, type, arg0));\n-\n-\t  /* Transform x * -1.0 into -x.  */\n-\t  if (!HONOR_SNANS (TYPE_MODE (TREE_TYPE (arg0)))\n-\t      && (!HONOR_SIGNED_ZEROS (TYPE_MODE (TREE_TYPE (arg0)))\n-\t\t  || !COMPLEX_FLOAT_TYPE_P (TREE_TYPE (arg0)))\n-\t      && real_minus_onep (arg1))\n-\t    return fold_convert_loc (loc, type, negate_expr (arg0));\n-\n \t  /* Convert (C1/X)*C2 into (C1*C2)/X.  This transformation may change\n              the result for floating point types due to rounding so it is applied\n              only if -fassociative-math was specify.  */\n@@ -11522,33 +11455,6 @@ fold_binary_loc (location_t loc,\n \t  && real_zerop (arg1))\n \treturn NULL_TREE;\n \n-      /* Optimize A / A to 1.0 if we don't care about\n-\t NaNs or Infinities.  Skip the transformation\n-\t for non-real operands.  */\n-      if (SCALAR_FLOAT_TYPE_P (TREE_TYPE (arg0))\n-\t  && ! HONOR_NANS (TYPE_MODE (TREE_TYPE (arg0)))\n-\t  && ! HONOR_INFINITIES (TYPE_MODE (TREE_TYPE (arg0)))\n-\t  && operand_equal_p (arg0, arg1, 0))\n-\t{\n-\t  tree r = build_real (TREE_TYPE (arg0), dconst1);\n-\n-\t  return omit_two_operands_loc (loc, type, r, arg0, arg1);\n-\t}\n-\n-      /* The complex version of the above A / A optimization.  */\n-      if (COMPLEX_FLOAT_TYPE_P (TREE_TYPE (arg0))\n-\t  && operand_equal_p (arg0, arg1, 0))\n-\t{\n-\t  tree elem_type = TREE_TYPE (TREE_TYPE (arg0));\n-\t  if (! HONOR_NANS (TYPE_MODE (elem_type))\n-\t      && ! HONOR_INFINITIES (TYPE_MODE (elem_type)))\n-\t    {\n-\t      tree r = build_real (elem_type, dconst1);\n-\t      /* omit_two_operands will call fold_convert for us.  */\n-\t      return omit_two_operands_loc (loc, type, r, arg0, arg1);\n-\t    }\n-\t}\n-\n       /* (-A) / (-B) -> A / B  */\n       if (TREE_CODE (arg0) == NEGATE_EXPR && negate_expr_p (arg1))\n \treturn fold_build2_loc (loc, RDIV_EXPR, type,\n@@ -11559,42 +11465,6 @@ fold_binary_loc (location_t loc,\n \t\t\t    negate_expr (arg0),\n \t\t\t    TREE_OPERAND (arg1, 0));\n \n-      /* In IEEE floating point, x/1 is not equivalent to x for snans.  */\n-      if (!HONOR_SNANS (TYPE_MODE (TREE_TYPE (arg0)))\n-\t  && real_onep (arg1))\n-\treturn non_lvalue_loc (loc, fold_convert_loc (loc, type, arg0));\n-\n-      /* In IEEE floating point, x/-1 is not equivalent to -x for snans.  */\n-      if (!HONOR_SNANS (TYPE_MODE (TREE_TYPE (arg0)))\n-\t  && real_minus_onep (arg1))\n-\treturn non_lvalue_loc (loc, fold_convert_loc (loc, type,\n-\t\t\t\t\t\t  negate_expr (arg0)));\n-\n-      /* If ARG1 is a constant, we can convert this to a multiply by the\n-\t reciprocal.  This does not have the same rounding properties,\n-\t so only do this if -freciprocal-math.  We can actually\n-\t always safely do it if ARG1 is a power of two, but it's hard to\n-\t tell if it is or not in a portable manner.  */\n-      if (optimize\n-\t  && (TREE_CODE (arg1) == REAL_CST\n-\t      || (TREE_CODE (arg1) == COMPLEX_CST\n-\t\t  && COMPLEX_FLOAT_TYPE_P (TREE_TYPE (arg1)))\n-\t      || (TREE_CODE (arg1) == VECTOR_CST\n-\t\t  && VECTOR_FLOAT_TYPE_P (TREE_TYPE (arg1)))))\n-\t{\n-\t  if (flag_reciprocal_math\n-\t      && 0 != (tem = const_binop (code, build_one_cst (type), arg1)))\n-\t    return fold_build2_loc (loc, MULT_EXPR, type, arg0, tem);\n-\t  /* Find the reciprocal if optimizing and the result is exact.\n-\t     TODO: Complex reciprocal not implemented.  */\n-\t  if (TREE_CODE (arg1) != COMPLEX_CST)\n-\t    {\n-\t      tree inverse = exact_inverse (TREE_TYPE (arg0), arg1);\n-\n-\t      if (inverse)\n-\t\treturn fold_build2_loc (loc, MULT_EXPR, type, arg0, inverse);\n-\t    }\n-\t}\n       /* Convert A/B/C to A/(B*C).  */\n       if (flag_reciprocal_math\n \t  && TREE_CODE (arg0) == RDIV_EXPR)\n@@ -11817,25 +11687,13 @@ fold_binary_loc (location_t loc,\n \t    }\n \t}\n \n-      /* For unsigned integral types, FLOOR_DIV_EXPR is the same as\n-\t TRUNC_DIV_EXPR.  Rewrite into the latter in this case.  */\n-      if (INTEGRAL_TYPE_P (type)\n-\t  && TYPE_UNSIGNED (type)\n-\t  && code == FLOOR_DIV_EXPR)\n-\treturn fold_build2_loc (loc, TRUNC_DIV_EXPR, type, op0, op1);\n-\n       /* Fall through */\n \n     case ROUND_DIV_EXPR:\n     case CEIL_DIV_EXPR:\n     case EXACT_DIV_EXPR:\n       if (integer_zerop (arg1))\n \treturn NULL_TREE;\n-      /* X / -1 is -X.  */\n-      if (!TYPE_UNSIGNED (type)\n-\t  && TREE_CODE (arg1) == INTEGER_CST\n-\t  && wi::eq_p (arg1, -1))\n-\treturn fold_convert_loc (loc, type, negate_expr (arg0));\n \n       /* Convert -A / -B to A / B when the type is signed and overflow is\n \t undefined.  */\n@@ -11898,26 +11756,6 @@ fold_binary_loc (location_t loc,\n     case FLOOR_MOD_EXPR:\n     case ROUND_MOD_EXPR:\n     case TRUNC_MOD_EXPR:\n-      /* X % -1 is zero.  */\n-      if (!TYPE_UNSIGNED (type)\n-\t  && TREE_CODE (arg1) == INTEGER_CST\n-\t  && wi::eq_p (arg1, -1))\n-\treturn omit_one_operand_loc (loc, type, integer_zero_node, arg0);\n-\n-      /* X % -C is the same as X % C.  */\n-      if (code == TRUNC_MOD_EXPR\n-\t  && TYPE_SIGN (type) == SIGNED\n-\t  && TREE_CODE (arg1) == INTEGER_CST\n-\t  && !TREE_OVERFLOW (arg1)\n-\t  && wi::neg_p (arg1)\n-\t  && !TYPE_OVERFLOW_TRAPS (type)\n-\t  /* Avoid this transformation if C is INT_MIN, i.e. C == -C.  */\n-\t  && !sign_bit_p (arg1, arg1))\n-\treturn fold_build2_loc (loc, code, type,\n-\t\t\t    fold_convert_loc (loc, type, arg0),\n-\t\t\t    fold_convert_loc (loc, type,\n-\t\t\t\t\t      negate_expr (arg1)));\n-\n       /* X % -Y is the same as X % Y.  */\n       if (code == TRUNC_MOD_EXPR\n \t  && !TYPE_UNSIGNED (type)\n@@ -11971,30 +11809,8 @@ fold_binary_loc (location_t loc,\n \n     case LROTATE_EXPR:\n     case RROTATE_EXPR:\n-      if (integer_all_onesp (arg0))\n-\treturn omit_one_operand_loc (loc, type, arg0, arg1);\n-      goto shift;\n-\n     case RSHIFT_EXPR:\n-      /* Optimize -1 >> x for arithmetic right shifts.  */\n-      if (integer_all_onesp (arg0) && !TYPE_UNSIGNED (type)\n-\t  && tree_expr_nonnegative_p (arg1))\n-\treturn omit_one_operand_loc (loc, type, arg0, arg1);\n-      /* ... fall through ...  */\n-\n     case LSHIFT_EXPR:\n-    shift:\n-      if (integer_zerop (arg1))\n-\treturn non_lvalue_loc (loc, fold_convert_loc (loc, type, arg0));\n-      if (integer_zerop (arg0))\n-\treturn omit_one_operand_loc (loc, type, arg0, arg1);\n-\n-      /* Prefer vector1 << scalar to vector1 << vector2\n-\t if vector2 is uniform.  */\n-      if (VECTOR_TYPE_P (TREE_TYPE (arg1))\n-\t  && (tem = uniform_vector_p (arg1)) != NULL_TREE)\n-\treturn fold_build2_loc (loc, code, type, op0, tem);\n-\n       /* Since negative shift count is not well-defined,\n \t don't try to compute it in the compiler.  */\n       if (TREE_CODE (arg1) == INTEGER_CST && tree_int_cst_sgn (arg1) < 0)\n@@ -12054,15 +11870,6 @@ fold_binary_loc (location_t loc,\n \t    }\n \t}\n \n-      /* Rewrite an LROTATE_EXPR by a constant into an\n-\t RROTATE_EXPR by a new constant.  */\n-      if (code == LROTATE_EXPR && TREE_CODE (arg1) == INTEGER_CST)\n-\t{\n-\t  tree tem = build_int_cst (TREE_TYPE (arg1), prec);\n-\t  tem = const_binop (MINUS_EXPR, tem, arg1);\n-\t  return fold_build2_loc (loc, RROTATE_EXPR, type, op0, tem);\n-\t}\n-\n       /* If we have a rotate of a bit operation with the rotate count and\n \t the second operand of the bit operation both constant,\n \t permute the two operations.  */\n@@ -12110,23 +11917,12 @@ fold_binary_loc (location_t loc,\n       return NULL_TREE;\n \n     case MIN_EXPR:\n-      if (operand_equal_p (arg0, arg1, 0))\n-\treturn omit_one_operand_loc (loc, type, arg0, arg1);\n-      if (INTEGRAL_TYPE_P (type)\n-\t  && operand_equal_p (arg1, TYPE_MIN_VALUE (type), OEP_ONLY_CONST))\n-\treturn omit_one_operand_loc (loc, type, arg1, arg0);\n       tem = fold_minmax (loc, MIN_EXPR, type, arg0, arg1);\n       if (tem)\n \treturn tem;\n       goto associate;\n \n     case MAX_EXPR:\n-      if (operand_equal_p (arg0, arg1, 0))\n-\treturn omit_one_operand_loc (loc, type, arg0, arg1);\n-      if (INTEGRAL_TYPE_P (type)\n-\t  && TYPE_MAX_VALUE (type)\n-\t  && operand_equal_p (arg1, TYPE_MAX_VALUE (type), OEP_ONLY_CONST))\n-\treturn omit_one_operand_loc (loc, type, arg1, arg0);\n       tem = fold_minmax (loc, MAX_EXPR, type, arg0, arg1);\n       if (tem)\n \treturn tem;\n@@ -14799,7 +14595,7 @@ tree_unary_nonnegative_warnv_p (enum tree_code code, tree type, tree op0,\n       return tree_expr_nonnegative_warnv_p (op0,\n \t\t\t\t\t    strict_overflow_p);\n \n-    case NOP_EXPR:\n+    CASE_CONVERT:\n       {\n \ttree inner_type = TREE_TYPE (op0);\n \ttree outer_type = type;"}, {"sha": "09ece6735c7a28c3069cac7a4f0f83c5b50c1a89", "filename": "gcc/fold-const.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a7f24614b3c58d03f40d55fe195056dd8423f8f5/gcc%2Ffold-const.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a7f24614b3c58d03f40d55fe195056dd8423f8f5/gcc%2Ffold-const.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ffold-const.h?ref=a7f24614b3c58d03f40d55fe195056dd8423f8f5", "patch": "@@ -167,5 +167,7 @@ extern tree make_range_step (location_t, enum tree_code, tree, tree, tree,\n extern tree build_range_check (location_t, tree, tree, int, tree, tree);\n extern bool merge_ranges (int *, tree *, tree *, int, tree, tree, int,\n \t\t\t  tree, tree);\n+extern tree sign_bit_p (tree, const_tree);\n+extern tree exact_inverse (tree, tree);\n \n #endif // GCC_FOLD_CONST_H"}, {"sha": "127c7d9b5e4e1cada2c05376daeac1e2d8b36283", "filename": "gcc/match.pd", "status": "modified", "additions": 218, "deletions": 9, "changes": 227, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a7f24614b3c58d03f40d55fe195056dd8423f8f5/gcc%2Fmatch.pd", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a7f24614b3c58d03f40d55fe195056dd8423f8f5/gcc%2Fmatch.pd", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fmatch.pd?ref=a7f24614b3c58d03f40d55fe195056dd8423f8f5", "patch": "@@ -53,19 +53,59 @@ along with GCC; see the file COPYING3.  If not see\n  (pointer_plus integer_zerop @1)\n  (non_lvalue (convert @1)))\n \n+/* See if ARG1 is zero and X + ARG1 reduces to X.\n+   Likewise if the operands are reversed.  */\n+(simplify\n+ (plus:c @0 real_zerop@1)\n+ (if (fold_real_zero_addition_p (type, @1, 0))\n+  (non_lvalue @0)))\n+\n+/* See if ARG1 is zero and X - ARG1 reduces to X.  */\n+(simplify\n+ (minus @0 real_zerop@1)\n+ (if (fold_real_zero_addition_p (type, @1, 1))\n+  (non_lvalue @0)))\n+\n /* Simplify x - x.\n    This is unsafe for certain floats even in non-IEEE formats.\n    In IEEE, it is unsafe because it does wrong for NaNs.\n    Also note that operand_equal_p is always false if an operand\n    is volatile.  */\n (simplify\n-  (minus @0 @0)\n-  (if (!FLOAT_TYPE_P (type) || !HONOR_NANS (TYPE_MODE (type)))\n-   { build_zero_cst (type); }))\n+ (minus @0 @0)\n+ (if (!FLOAT_TYPE_P (type) || !HONOR_NANS (TYPE_MODE (type)))\n+  { build_zero_cst (type); }))\n \n (simplify\n-  (mult @0 integer_zerop@1)\n-  @1)\n+ (mult @0 integer_zerop@1)\n+ @1)\n+\n+/* Maybe fold x * 0 to 0.  The expressions aren't the same\n+   when x is NaN, since x * 0 is also NaN.  Nor are they the\n+   same in modes with signed zeros, since multiplying a\n+   negative value by 0 gives -0, not +0.  */\n+(simplify\n+ (mult @0 real_zerop@1)\n+ (if (!HONOR_NANS (TYPE_MODE (type))\n+      && !HONOR_SIGNED_ZEROS (TYPE_MODE (type)))\n+  @1))\n+\n+/* In IEEE floating point, x*1 is not equivalent to x for snans.\n+   Likewise for complex arithmetic with signed zeros.  */\n+(simplify\n+ (mult @0 real_onep)\n+ (if (!HONOR_SNANS (TYPE_MODE (type))\n+      && (!HONOR_SIGNED_ZEROS (TYPE_MODE (type))\n+          || !COMPLEX_FLOAT_TYPE_P (type)))\n+  (non_lvalue @0)))\n+\n+/* Transform x * -1.0 into -x.  */\n+(simplify\n+ (mult @0 real_minus_onep)\n+  (if (!HONOR_SNANS (TYPE_MODE (type))\n+       && (!HONOR_SIGNED_ZEROS (TYPE_MODE (type))\n+           || !COMPLEX_FLOAT_TYPE_P (type)))\n+   (negate @0)))\n \n /* Make sure to preserve divisions by zero.  This is the reason why\n    we don't simplify x / x to 1 or 0 / x to 0.  */\n@@ -74,19 +114,98 @@ along with GCC; see the file COPYING3.  If not see\n     (op @0 integer_onep)\n     (non_lvalue @0)))\n \n+/* X / -1 is -X.  */\n+(for div (trunc_div ceil_div floor_div round_div exact_div)\n+ (simplify\n+   (div @0 INTEGER_CST@1)\n+   (if (!TYPE_UNSIGNED (type)\n+        && wi::eq_p (@1, -1))\n+    (negate @0))))\n+\n+/* For unsigned integral types, FLOOR_DIV_EXPR is the same as\n+   TRUNC_DIV_EXPR.  Rewrite into the latter in this case.  */\n+(simplify\n+ (floor_div @0 @1)\n+ (if (INTEGRAL_TYPE_P (type) && TYPE_UNSIGNED (type))\n+  (trunc_div @0 @1)))\n+\n+/* Optimize A / A to 1.0 if we don't care about\n+   NaNs or Infinities.  Skip the transformation\n+   for non-real operands.  */\n+(simplify\n+ (rdiv @0 @0)\n+ (if (SCALAR_FLOAT_TYPE_P (type)\n+      && ! HONOR_NANS (TYPE_MODE (type))\n+      && ! HONOR_INFINITIES (TYPE_MODE (type)))\n+  { build_real (type, dconst1); })\n+ /* The complex version of the above A / A optimization.  */\n+ (if (COMPLEX_FLOAT_TYPE_P (type)\n+      && ! HONOR_NANS (TYPE_MODE (TREE_TYPE (type)))\n+      && ! HONOR_INFINITIES (TYPE_MODE (TREE_TYPE (type))))\n+  { build_complex (type, build_real (TREE_TYPE (type), dconst1),\n+\t\t   build_real (TREE_TYPE (type), dconst0)); }))\n+\n+/* In IEEE floating point, x/1 is not equivalent to x for snans.  */\n+(simplify\n+ (rdiv @0 real_onep)\n+ (if (!HONOR_SNANS (TYPE_MODE (type)))\n+  (non_lvalue @0)))\n+\n+/* In IEEE floating point, x/-1 is not equivalent to -x for snans.  */\n+(simplify\n+ (rdiv @0 real_minus_onep)\n+ (if (!HONOR_SNANS (TYPE_MODE (type)))\n+  (negate @0)))\n+\n+/* If ARG1 is a constant, we can convert this to a multiply by the\n+   reciprocal.  This does not have the same rounding properties,\n+   so only do this if -freciprocal-math.  We can actually\n+   always safely do it if ARG1 is a power of two, but it's hard to\n+   tell if it is or not in a portable manner.  */\n+(for cst (REAL_CST COMPLEX_CST VECTOR_CST)\n+ (simplify\n+  (rdiv @0 cst@1)\n+  (if (optimize)\n+   (if (flag_reciprocal_math)\n+    (with\n+     { tree tem = fold_binary (RDIV_EXPR, type, build_one_cst (type), @1); }\n+     (if (tem)\n+      (mult @0 { tem; } ))))\n+   (if (cst != COMPLEX_CST)\n+    (with { tree inverse = exact_inverse (type, @1); }\n+     (if (inverse)\n+      (mult @0 { inverse; } )))))))\n+\n /* Same applies to modulo operations, but fold is inconsistent here\n    and simplifies 0 % x to 0, only preserving literal 0 % 0.  */\n-(for op (ceil_mod floor_mod round_mod trunc_mod)\n+(for mod (ceil_mod floor_mod round_mod trunc_mod)\n  /* 0 % X is always zero.  */\n  (simplify\n-  (op integer_zerop@0 @1)\n+  (mod integer_zerop@0 @1)\n   /* But not for 0 % 0 so that we can get the proper warnings and errors.  */\n   (if (!integer_zerop (@1))\n    @0))\n  /* X % 1 is always zero.  */\n  (simplify\n-  (op @0 integer_onep)\n-  { build_zero_cst (type); }))\n+  (mod @0 integer_onep)\n+  { build_zero_cst (type); })\n+ /* X % -1 is zero.  */\n+ (simplify\n+  (mod @0 INTEGER_CST@1)\n+  (if (!TYPE_UNSIGNED (type)\n+       && wi::eq_p (@1, -1))\n+   { build_zero_cst (type); })))\n+\n+/* X % -C is the same as X % C.  */\n+(simplify\n+ (trunc_mod @0 INTEGER_CST@1)\n+  (if (TYPE_SIGN (type) == SIGNED\n+       && !TREE_OVERFLOW (@1)\n+       && wi::neg_p (@1)\n+       && !TYPE_OVERFLOW_TRAPS (type)\n+       /* Avoid this transformation if C is INT_MIN, i.e. C == -C.  */\n+       && !sign_bit_p (@1, @1))\n+   (trunc_mod @0 (negate @1))))\n \n /* x | ~0 -> ~0  */\n (simplify\n@@ -393,6 +512,64 @@ along with GCC; see the file COPYING3.  If not see\n      (convert @1))))))\n \n \n+/* Simplifications of MIN_EXPR and MAX_EXPR.  */\n+\n+(for minmax (min max)\n+ (simplify\n+  (minmax @0 @0)\n+  @0))\n+(simplify\n+ (min @0 @1)\n+ (if (INTEGRAL_TYPE_P (type)\n+      && TYPE_MIN_VALUE (type)\n+      && operand_equal_p (@1, TYPE_MIN_VALUE (type), OEP_ONLY_CONST))\n+  @1))\n+(simplify\n+ (max @0 @1)\n+ (if (INTEGRAL_TYPE_P (type)\n+      && TYPE_MAX_VALUE (type)\n+      && operand_equal_p (@1, TYPE_MAX_VALUE (type), OEP_ONLY_CONST))\n+  @1))\n+\n+\n+/* Simplifications of shift and rotates.  */\n+\n+(for rotate (lrotate rrotate)\n+ (simplify\n+  (rotate integer_all_onesp@0 @1)\n+  @0))\n+\n+/* Optimize -1 >> x for arithmetic right shifts.  */\n+(simplify\n+ (rshift integer_all_onesp@0 @1)\n+ (if (!TYPE_UNSIGNED (type)\n+      && tree_expr_nonnegative_p (@1))\n+  @0))\n+\n+(for shiftrotate (lrotate rrotate lshift rshift)\n+ (simplify\n+  (shiftrotate @0 integer_zerop)\n+  (non_lvalue @0))\n+ (simplify\n+  (shiftrotate integer_zerop@0 @1)\n+  @0)\n+ /* Prefer vector1 << scalar to vector1 << vector2\n+    if vector2 is uniform.  */\n+ (for vec (VECTOR_CST CONSTRUCTOR)\n+  (simplify\n+   (shiftrotate @0 vec@1)\n+   (with { tree tem = uniform_vector_p (@1); }\n+    (if (tem)\n+     (shiftrotate @0 { tem; }))))))\n+\n+/* Rewrite an LROTATE_EXPR by a constant into an\n+   RROTATE_EXPR by a new constant.  */\n+(simplify\n+ (lrotate @0 INTEGER_CST@1)\n+ (rrotate @0 { fold_binary (MINUS_EXPR, TREE_TYPE (@1),\n+\t\t\t    build_int_cst (TREE_TYPE (@1),\n+\t\t\t\t\t   element_precision (type)), @1); }))\n+\n \n /* Simplifications of conversions.  */\n \n@@ -568,6 +745,38 @@ along with GCC; see the file COPYING3.  If not see\n   (if (TYPE_PRECISION (TREE_TYPE (@0)) == TYPE_PRECISION (type))\n    (convert @0)))\n \n+/* Canonicalization of binary operations.  */\n+\n+/* Convert X + -C into X - C.  */\n+(simplify\n+ (plus @0 REAL_CST@1)\n+ (if (REAL_VALUE_NEGATIVE (TREE_REAL_CST (@1)))\n+  (with { tree tem = fold_unary (NEGATE_EXPR, type, @1); }\n+   (if (!TREE_OVERFLOW (tem) || !flag_trapping_math)\n+    (minus @0 { tem; })))))\n+\n+/* Convert x+x into x*2.0.  */\n+(simplify\n+ (plus @0 @0)\n+ (if (SCALAR_FLOAT_TYPE_P (type))\n+  (mult @0 { build_real (type, dconst2); })))\n+\n+(simplify\n+ (minus integer_zerop @1)\n+ (negate @1))\n+\n+/* (ARG0 - ARG1) is the same as (-ARG1 + ARG0).  So check whether\n+   ARG0 is zero and X + ARG0 reduces to X, since that would mean\n+   (-ARG1 + ARG0) reduces to -ARG1.  */\n+(simplify\n+ (minus real_zerop@0 @1)\n+ (if (fold_real_zero_addition_p (type, @0, 0))\n+  (negate @1)))\n+\n+/* Transform x * -1 into -x.  */\n+(simplify\n+ (mult @0 integer_minus_onep)\n+ (negate @0))\n \n /* COMPLEX_EXPR and REALPART/IMAGPART_EXPR cancellations.  */\n (simplify"}, {"sha": "5ffb5d445b4069aeb0ffaa1d7f55b992d1a42221", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a7f24614b3c58d03f40d55fe195056dd8423f8f5/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a7f24614b3c58d03f40d55fe195056dd8423f8f5/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=a7f24614b3c58d03f40d55fe195056dd8423f8f5", "patch": "@@ -1,3 +1,7 @@\n+2014-11-14  Richard Biener  <rguenther@suse.de>\n+\n+\t* gcc.c-torture/execute/shiftopt-1.c: XFAIL invalid parts.\n+\n 2014-11-13  Teresa Johnson  <tejohnson@google.com>\n \n \tPR tree-optimization/63841"}, {"sha": "3ff714d3b475fddfd82c6059610ad74596ca0d6a", "filename": "gcc/testsuite/gcc.c-torture/execute/shiftopt-1.c", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a7f24614b3c58d03f40d55fe195056dd8423f8f5/gcc%2Ftestsuite%2Fgcc.c-torture%2Fexecute%2Fshiftopt-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a7f24614b3c58d03f40d55fe195056dd8423f8f5/gcc%2Ftestsuite%2Fgcc.c-torture%2Fexecute%2Fshiftopt-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.c-torture%2Fexecute%2Fshiftopt-1.c?ref=a7f24614b3c58d03f40d55fe195056dd8423f8f5", "patch": "@@ -22,11 +22,16 @@ utest (unsigned int x)\n   if (0 >> x != 0)\n     link_error ();\n \n+  /* XFAIL: the C frontend converts the shift amount to 'int'\n+     thus we get -1 >> (int)x which means the shift amount may\n+     be negative.  See PR63862.  */\n+#if 0\n   if (-1 >> x != -1)\n     link_error ();\n \n   if (~0 >> x != ~0)\n     link_error ();\n+#endif\n }\n \n void"}]}