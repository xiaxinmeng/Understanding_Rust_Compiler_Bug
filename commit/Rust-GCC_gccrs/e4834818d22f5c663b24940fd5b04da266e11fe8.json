{"sha": "e4834818d22f5c663b24940fd5b04da266e11fe8", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZTQ4MzQ4MThkMjJmNWM2NjNiMjQ5NDBmZDViMDRkYTI2NmUxMWZlOA==", "commit": {"author": {"name": "Nathan Sidwell", "email": "nathan@codesourcery.com", "date": "2015-10-28T03:00:50Z"}, "committer": {"name": "Nathan Sidwell", "email": "nathan@gcc.gnu.org", "date": "2015-10-28T03:00:50Z"}, "message": "omp-low.c (struct omp_context): Remove gwv_below, gwv_this fields.\n\n\t* omp-low.c (struct omp_context): Remove gwv_below, gwv_this\n\tfields.\n\t(is_oacc_parallel, is_oacc_kernels): New.\n\t(enclosing_target_ctx): May return NULL.\n\t(ctx_in_oacc_kernels_region): New.\n\t(check_oacc_kernel_gwv): New.\n\t(oacc_loop_or_target_p): Delete.\n\t(scan_omp_for): Don't calculate gwv mask.  Check parallel clause\n\toperands.  Strip reductions fro kernels.\n\t(scan_omp_target): Don't calculate gwv mask.\n\t(lower_oacc_head_mark, lower_oacc_loop_marker,\n\tlower_oacc_head_tail): New.\n\t(struct oacc_collapse): New.\n\t(expand_oacc_collapse_init, expand_oacc_collapse_vars): New.\n\t(expand_omp_for_static_nochunk, expand_omp_for_static_chunk):\n\tRemove OpenACC handling.\n\t(expand_oacc_for): New.\n\t(expand_omp_for): Call expand_oacc_for.\n\t(lower_omp_for): Call lower_oacc_head_tail.\n\nFrom-SVN: r229472", "tree": {"sha": "cf0f5b4ce7a31924dfd29c43d294f7b3153f4496", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/cf0f5b4ce7a31924dfd29c43d294f7b3153f4496"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/e4834818d22f5c663b24940fd5b04da266e11fe8", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e4834818d22f5c663b24940fd5b04da266e11fe8", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e4834818d22f5c663b24940fd5b04da266e11fe8", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e4834818d22f5c663b24940fd5b04da266e11fe8/comments", "author": null, "committer": null, "parents": [{"sha": "a1c1908bbd8b298e601f85d76ffa19a487c54ea2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a1c1908bbd8b298e601f85d76ffa19a487c54ea2", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a1c1908bbd8b298e601f85d76ffa19a487c54ea2"}], "stats": {"total": 1015, "additions": 909, "deletions": 106}, "files": [{"sha": "463e6012d0015738e3d3c02229ed23a8f1f42345", "filename": "gcc/ChangeLog", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e4834818d22f5c663b24940fd5b04da266e11fe8/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e4834818d22f5c663b24940fd5b04da266e11fe8/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=e4834818d22f5c663b24940fd5b04da266e11fe8", "patch": "@@ -1,3 +1,25 @@\n+2015-10-27  Nathan Sidwell  <nathan@codesourcery.com>\n+\n+\t* omp-low.c (struct omp_context): Remove gwv_below, gwv_this\n+\tfields.\n+\t(is_oacc_parallel, is_oacc_kernels): New.\n+\t(enclosing_target_ctx): May return NULL.\n+\t(ctx_in_oacc_kernels_region): New.\n+\t(check_oacc_kernel_gwv): New.\n+\t(oacc_loop_or_target_p): Delete.\n+\t(scan_omp_for): Don't calculate gwv mask.  Check parallel clause\n+\toperands.  Strip reductions fro kernels.\n+\t(scan_omp_target): Don't calculate gwv mask.\n+\t(lower_oacc_head_mark, lower_oacc_loop_marker,\n+\tlower_oacc_head_tail): New.\n+\t(struct oacc_collapse): New.\n+\t(expand_oacc_collapse_init, expand_oacc_collapse_vars): New.\n+\t(expand_omp_for_static_nochunk, expand_omp_for_static_chunk):\n+\tRemove OpenACC handling.\n+\t(expand_oacc_for): New.\n+\t(expand_omp_for): Call expand_oacc_for.\n+\t(lower_omp_for): Call lower_oacc_head_tail.\n+\n 2015-10-27  Mikhail Maltsev  <maltsevm@gmail.com>\n \n \t* attribs.c (check_attribute_tables): New function, broken out from..."}, {"sha": "c4411669c4686f998ac2ef9d56d56e7a2a8c2e29", "filename": "gcc/omp-low.c", "status": "modified", "additions": 887, "deletions": 106, "changes": 993, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/e4834818d22f5c663b24940fd5b04da266e11fe8/gcc%2Fomp-low.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/e4834818d22f5c663b24940fd5b04da266e11fe8/gcc%2Fomp-low.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fomp-low.c?ref=e4834818d22f5c663b24940fd5b04da266e11fe8", "patch": "@@ -200,14 +200,6 @@ struct omp_context\n \n   /* True if this construct can be cancelled.  */\n   bool cancellable;\n-\n-  /* For OpenACC loops, a mask of gang, worker and vector used at\n-     levels below this one.  */\n-  int gwv_below;\n-  /* For OpenACC loops, a mask of gang, worker and vector used at\n-     this level and above.  For parallel and kernels clauses, a mask\n-     indicating which of num_gangs/num_workers/num_vectors was used.  */\n-  int gwv_this;\n };\n \n /* A structure holding the elements of:\n@@ -299,6 +291,28 @@ static gphi *find_phi_with_arg_on_edge (tree, edge);\n       *handled_ops_p = false; \\\n       break;\n \n+/* Return true if CTX corresponds to an oacc parallel region.  */\n+\n+static bool\n+is_oacc_parallel (omp_context *ctx)\n+{\n+  enum gimple_code outer_type = gimple_code (ctx->stmt);\n+  return ((outer_type == GIMPLE_OMP_TARGET)\n+\t  && (gimple_omp_target_kind (ctx->stmt)\n+\t      == GF_OMP_TARGET_KIND_OACC_PARALLEL));\n+}\n+\n+/* Return true if CTX corresponds to an oacc kernels region.  */\n+\n+static bool\n+is_oacc_kernels (omp_context *ctx)\n+{\n+  enum gimple_code outer_type = gimple_code (ctx->stmt);\n+  return ((outer_type == GIMPLE_OMP_TARGET)\n+\t  && (gimple_omp_target_kind (ctx->stmt)\n+\t      == GF_OMP_TARGET_KIND_OACC_KERNELS));\n+}\n+\n /* Helper function to get the name of the array containing the partial\n    reductions for OpenACC reductions.  */\n static const char *\n@@ -2933,81 +2947,158 @@ finish_taskreg_scan (omp_context *ctx)\n     }\n }\n \n+/* Find the enclosing offload context.  */\n \n static omp_context *\n enclosing_target_ctx (omp_context *ctx)\n {\n-  while (ctx != NULL\n-\t && gimple_code (ctx->stmt) != GIMPLE_OMP_TARGET)\n-    ctx = ctx->outer;\n-  gcc_assert (ctx != NULL);\n+  for (; ctx; ctx = ctx->outer)\n+    if (gimple_code (ctx->stmt) == GIMPLE_OMP_TARGET)\n+      break;\n+\n   return ctx;\n }\n \n+/* Return true if ctx is part of an oacc kernels region.  */\n+\n static bool\n-oacc_loop_or_target_p (gimple *stmt)\n+ctx_in_oacc_kernels_region (omp_context *ctx)\n {\n-  enum gimple_code outer_type = gimple_code (stmt);\n-  return ((outer_type == GIMPLE_OMP_TARGET\n-\t   && ((gimple_omp_target_kind (stmt)\n-\t\t== GF_OMP_TARGET_KIND_OACC_PARALLEL)\n-\t       || (gimple_omp_target_kind (stmt)\n-\t\t   == GF_OMP_TARGET_KIND_OACC_KERNELS)))\n-\t  || (outer_type == GIMPLE_OMP_FOR\n-\t      && gimple_omp_for_kind (stmt) == GF_OMP_FOR_KIND_OACC_LOOP));\n+  for (;ctx != NULL; ctx = ctx->outer)\n+    {\n+      gimple *stmt = ctx->stmt;\n+      if (gimple_code (stmt) == GIMPLE_OMP_TARGET\n+\t  && gimple_omp_target_kind (stmt) == GF_OMP_TARGET_KIND_OACC_KERNELS)\n+\treturn true;\n+    }\n+\n+  return false;\n+}\n+\n+/* Check the parallelism clauses inside a kernels regions.\n+   Until kernels handling moves to use the same loop indirection\n+   scheme as parallel, we need to do this checking early.  */\n+\n+static unsigned\n+check_oacc_kernel_gwv (gomp_for *stmt, omp_context *ctx)\n+{\n+  bool checking = true;\n+  unsigned outer_mask = 0;\n+  unsigned this_mask = 0;\n+  bool has_seq = false, has_auto = false;\n+\n+  if (ctx->outer)\n+    outer_mask = check_oacc_kernel_gwv (NULL,  ctx->outer);\n+  if (!stmt)\n+    {\n+      checking = false;\n+      if (gimple_code (ctx->stmt) != GIMPLE_OMP_FOR)\n+\treturn outer_mask;\n+      stmt = as_a <gomp_for *> (ctx->stmt);\n+    }\n+\n+  for (tree c = gimple_omp_for_clauses (stmt); c; c = OMP_CLAUSE_CHAIN (c))\n+    {\n+      switch (OMP_CLAUSE_CODE (c))\n+\t{\n+\tcase OMP_CLAUSE_GANG:\n+\t  this_mask |= GOMP_DIM_MASK (GOMP_DIM_GANG);\n+\t  break;\n+\tcase OMP_CLAUSE_WORKER:\n+\t  this_mask |= GOMP_DIM_MASK (GOMP_DIM_WORKER);\n+\t  break;\n+\tcase OMP_CLAUSE_VECTOR:\n+\t  this_mask |= GOMP_DIM_MASK (GOMP_DIM_VECTOR);\n+\t  break;\n+\tcase OMP_CLAUSE_SEQ:\n+\t  has_seq = true;\n+\t  break;\n+\tcase OMP_CLAUSE_AUTO:\n+\t  has_auto = true;\n+\t  break;\n+\tdefault:\n+\t  break;\n+\t}\n+    }\n+\n+  if (checking)\n+    {\n+      if (has_seq && (this_mask || has_auto))\n+\terror_at (gimple_location (stmt), \"%<seq%> overrides other\"\n+\t\t  \" OpenACC loop specifiers\");\n+      else if (has_auto && this_mask)\n+\terror_at (gimple_location (stmt), \"%<auto%> conflicts with other\"\n+\t\t  \" OpenACC loop specifiers\");\n+\n+      if (this_mask & outer_mask)\n+\terror_at (gimple_location (stmt), \"inner loop uses same\"\n+\t\t  \" OpenACC parallelism as containing loop\");\n+    }\n+\n+  return outer_mask | this_mask;\n }\n \n /* Scan a GIMPLE_OMP_FOR.  */\n \n static void\n scan_omp_for (gomp_for *stmt, omp_context *outer_ctx)\n {\n-  enum gimple_code outer_type = GIMPLE_ERROR_MARK;\n   omp_context *ctx;\n   size_t i;\n   tree clauses = gimple_omp_for_clauses (stmt);\n \n-  if (outer_ctx)\n-    outer_type = gimple_code (outer_ctx->stmt);\n-\n   ctx = new_omp_context (stmt, outer_ctx);\n \n   if (is_gimple_omp_oacc (stmt))\n     {\n-      if (outer_ctx && outer_type == GIMPLE_OMP_FOR)\n-\tctx->gwv_this = outer_ctx->gwv_this;\n-      for (tree c = clauses; c; c = OMP_CLAUSE_CHAIN (c))\n+      omp_context *tgt = enclosing_target_ctx (outer_ctx);\n+\n+      if (!tgt || is_oacc_parallel (tgt))\n+\tfor (tree c = clauses; c; c = OMP_CLAUSE_CHAIN (c))\n+\t  {\n+\t    char const *check = NULL;\n+\n+\t    switch (OMP_CLAUSE_CODE (c))\n+\t      {\n+\t      case OMP_CLAUSE_GANG:\n+\t\tcheck = \"gang\";\n+\t\tbreak;\n+\n+\t      case OMP_CLAUSE_WORKER:\n+\t\tcheck = \"worker\";\n+\t\tbreak;\n+\n+\t      case OMP_CLAUSE_VECTOR:\n+\t\tcheck = \"vector\";\n+\t\tbreak;\n+\n+\t      default:\n+\t\tbreak;\n+\t      }\n+\n+\t    if (check && OMP_CLAUSE_OPERAND (c, 0))\n+\t      error_at (gimple_location (stmt),\n+\t\t\t\"argument not permitted on %qs clause in\"\n+\t\t\t\" OpenACC %<parallel%>\", check);\n+\t  }\n+\n+      if (tgt && is_oacc_kernels (tgt))\n \t{\n-\t  int val;\n-\t  if (OMP_CLAUSE_CODE (c) == OMP_CLAUSE_GANG)\n-\t    val = MASK_GANG;\n-\t  else if (OMP_CLAUSE_CODE (c) == OMP_CLAUSE_WORKER)\n-\t    val = MASK_WORKER;\n-\t  else if (OMP_CLAUSE_CODE (c) == OMP_CLAUSE_VECTOR)\n-\t    val = MASK_VECTOR;\n-\t  else\n-\t    continue;\n-\t  ctx->gwv_this |= val;\n-\t  if (!outer_ctx)\n-\t    {\n-\t      /* Skip; not nested inside a region.  */\n-\t      continue;\n-\t    }\n-\t  if (!oacc_loop_or_target_p (outer_ctx->stmt))\n-\t    {\n-\t      /* Skip; not nested inside an OpenACC region.  */\n-\t      continue;\n-\t    }\n-\t  if (outer_type == GIMPLE_OMP_FOR)\n-\t    outer_ctx->gwv_below |= val;\n-\t  if (OMP_CLAUSE_OPERAND (c, 0) != NULL_TREE)\n+\t  /* Strip out reductions, as they are not  handled yet.  */\n+\t  tree *prev_ptr = &clauses;\n+\n+\t  while (tree probe = *prev_ptr)\n \t    {\n-\t      omp_context *enclosing = enclosing_target_ctx (outer_ctx);\n-\t      if (gimple_omp_target_kind (enclosing->stmt)\n-\t\t  == GF_OMP_TARGET_KIND_OACC_PARALLEL)\n-\t\terror_at (gimple_location (stmt),\n-\t\t\t  \"no arguments allowed to gang, worker and vector clauses inside parallel\");\n+\t      tree *next_ptr = &OMP_CLAUSE_CHAIN (probe);\n+\t      \n+\t      if (OMP_CLAUSE_CODE (probe) == OMP_CLAUSE_REDUCTION)\n+\t\t*prev_ptr = *next_ptr;\n+\t      else\n+\t\tprev_ptr = next_ptr;\n \t    }\n+\n+\t  gimple_omp_for_set_clauses (stmt, clauses);\n+\t  check_oacc_kernel_gwv (stmt, ctx);\n \t}\n     }\n \n@@ -3022,19 +3113,6 @@ scan_omp_for (gomp_for *stmt, omp_context *outer_ctx)\n       scan_omp_op (gimple_omp_for_incr_ptr (stmt, i), ctx);\n     }\n   scan_omp (gimple_omp_body_ptr (stmt), ctx);\n-\n-  if (is_gimple_omp_oacc (stmt))\n-    {\n-      if (ctx->gwv_this & ctx->gwv_below)\n-\terror_at (gimple_location (stmt),\n-\t\t  \"gang, worker and vector may occur only once in a loop nest\");\n-      else if (ctx->gwv_below != 0\n-\t       && ctx->gwv_this > ctx->gwv_below)\n-\terror_at (gimple_location (stmt),\n-\t\t  \"gang, worker and vector must occur in this order in a loop nest\");\n-      if (outer_ctx && outer_type == GIMPLE_OMP_FOR)\n-\touter_ctx->gwv_below |= ctx->gwv_below;\n-    }\n }\n \n /* Scan an OpenMP sections directive.  */\n@@ -3105,19 +3183,6 @@ scan_omp_target (gomp_target *stmt, omp_context *outer_ctx)\n       gimple_omp_target_set_child_fn (stmt, ctx->cb.dst_fn);\n     }\n \n-  if (is_gimple_omp_oacc (stmt))\n-    {\n-      for (tree c = clauses; c; c = OMP_CLAUSE_CHAIN (c))\n-\t{\n-\t  if (OMP_CLAUSE_CODE (c) == OMP_CLAUSE_NUM_GANGS)\n-\t    ctx->gwv_this |= MASK_GANG;\n-\t  else if (OMP_CLAUSE_CODE (c) == OMP_CLAUSE_NUM_WORKERS)\n-\t    ctx->gwv_this |= MASK_WORKER;\n-\t  else if (OMP_CLAUSE_CODE (c) == OMP_CLAUSE_VECTOR_LENGTH)\n-\t    ctx->gwv_this |= MASK_VECTOR;\n-\t}\n-    }\n-\n   scan_sharing_clauses (clauses, ctx);\n   scan_omp (gimple_omp_body_ptr (stmt), ctx);\n \n@@ -5850,6 +5915,176 @@ lower_send_shared_vars (gimple_seq *ilist, gimple_seq *olist, omp_context *ctx)\n     }\n }\n \n+/* Emit an OpenACC head marker call, encapulating the partitioning and\n+   other information that must be processed by the target compiler.\n+   Return the maximum number of dimensions the associated loop might\n+   be partitioned over.  */\n+\n+static unsigned\n+lower_oacc_head_mark (location_t loc, tree ddvar, tree clauses,\n+\t\t      gimple_seq *seq, omp_context *ctx)\n+{\n+  unsigned levels = 0;\n+  unsigned tag = 0;\n+  tree gang_static = NULL_TREE;\n+  auto_vec<tree, 5> args;\n+\n+  args.quick_push (build_int_cst\n+\t\t   (integer_type_node, IFN_UNIQUE_OACC_HEAD_MARK));\n+  args.quick_push (ddvar);\n+  for (tree c = clauses; c; c = OMP_CLAUSE_CHAIN (c))\n+    {\n+      switch (OMP_CLAUSE_CODE (c))\n+\t{\n+\tcase OMP_CLAUSE_GANG:\n+\t  tag |= OLF_DIM_GANG;\n+\t  gang_static = OMP_CLAUSE_GANG_STATIC_EXPR (c);\n+\t  /* static:* is represented by -1, and we can ignore it, as\n+\t     scheduling is always static.  */\n+\t  if (gang_static && integer_minus_onep (gang_static))\n+\t    gang_static = NULL_TREE;\n+\t  levels++;\n+\t  break;\n+\n+\tcase OMP_CLAUSE_WORKER:\n+\t  tag |= OLF_DIM_WORKER;\n+\t  levels++;\n+\t  break;\n+\n+\tcase OMP_CLAUSE_VECTOR:\n+\t  tag |= OLF_DIM_VECTOR;\n+\t  levels++;\n+\t  break;\n+\n+\tcase OMP_CLAUSE_SEQ:\n+\t  tag |= OLF_SEQ;\n+\t  break;\n+\n+\tcase OMP_CLAUSE_AUTO:\n+\t  tag |= OLF_AUTO;\n+\t  break;\n+\n+\tcase OMP_CLAUSE_INDEPENDENT:\n+\t  tag |= OLF_INDEPENDENT;\n+\t  break;\n+\n+\tdefault:\n+\t  continue;\n+\t}\n+    }\n+\n+  if (gang_static)\n+    {\n+      if (DECL_P (gang_static))\n+\tgang_static = build_outer_var_ref (gang_static, ctx);\n+      tag |= OLF_GANG_STATIC;\n+    }\n+\n+  /* In a parallel region, loops are implicitly INDEPENDENT.  */\n+  omp_context *tgt = enclosing_target_ctx (ctx);\n+  if (!tgt || is_oacc_parallel (tgt))\n+    tag |= OLF_INDEPENDENT;\n+\n+  /* A loop lacking SEQ, GANG, WORKER and/or VECTOR is implicitly AUTO.  */\n+  if (!(tag & (((GOMP_DIM_MASK (GOMP_DIM_MAX) - 1) << OLF_DIM_BASE)\n+\t       | OLF_SEQ)))\n+      tag |= OLF_AUTO;\n+\n+  /* Ensure at least one level.  */\n+  if (!levels)\n+    levels++;\n+\n+  args.quick_push (build_int_cst (integer_type_node, levels));\n+  args.quick_push (build_int_cst (integer_type_node, tag));\n+  if (gang_static)\n+    args.quick_push (gang_static);\n+\n+  gcall *call = gimple_build_call_internal_vec (IFN_UNIQUE, args);\n+  gimple_set_location (call, loc);\n+  gimple_set_lhs (call, ddvar);\n+  gimple_seq_add_stmt (seq, call);\n+\n+  return levels;\n+}\n+\n+/* Emit an OpenACC lopp head or tail marker to SEQ.  LEVEL is the\n+   partitioning level of the enclosed region.  */ \n+\n+static void\n+lower_oacc_loop_marker (location_t loc, tree ddvar, bool head,\n+\t\t\ttree tofollow, gimple_seq *seq)\n+{\n+  int marker_kind = (head ? IFN_UNIQUE_OACC_HEAD_MARK\n+\t\t     : IFN_UNIQUE_OACC_TAIL_MARK);\n+  tree marker = build_int_cst (integer_type_node, marker_kind);\n+  int nargs = 2 + (tofollow != NULL_TREE);\n+  gcall *call = gimple_build_call_internal (IFN_UNIQUE, nargs,\n+\t\t\t\t\t    marker, ddvar, tofollow);\n+  gimple_set_location (call, loc);\n+  gimple_set_lhs (call, ddvar);\n+  gimple_seq_add_stmt (seq, call);\n+}\n+\n+/* Generate the before and after OpenACC loop sequences.  CLAUSES are\n+   the loop clauses, from which we extract reductions.  Initialize\n+   HEAD and TAIL.  */\n+\n+static void\n+lower_oacc_head_tail (location_t loc, tree clauses,\n+\t\t      gimple_seq *head, gimple_seq *tail, omp_context *ctx)\n+{\n+  bool inner = false;\n+  tree ddvar = create_tmp_var (integer_type_node, \".data_dep\");\n+  gimple_seq_add_stmt (head, gimple_build_assign (ddvar, integer_zero_node));\n+\n+  unsigned count = lower_oacc_head_mark (loc, ddvar, clauses, head, ctx);\n+  if (!count)\n+    lower_oacc_loop_marker (loc, ddvar, false, integer_zero_node, tail);\n+  \n+  tree fork_kind = build_int_cst (unsigned_type_node, IFN_UNIQUE_OACC_FORK);\n+  tree join_kind = build_int_cst (unsigned_type_node, IFN_UNIQUE_OACC_JOIN);\n+\n+  for (unsigned done = 1; count; count--, done++)\n+    {\n+      gimple_seq fork_seq = NULL;\n+      gimple_seq join_seq = NULL;\n+\n+      tree place = build_int_cst (integer_type_node, -1);\n+      gcall *fork = gimple_build_call_internal (IFN_UNIQUE, 3,\n+\t\t\t\t\t\tfork_kind, ddvar, place);\n+      gimple_set_location (fork, loc);\n+      gimple_set_lhs (fork, ddvar);\n+\n+      gcall *join = gimple_build_call_internal (IFN_UNIQUE, 3,\n+\t\t\t\t\t\tjoin_kind, ddvar, place);\n+      gimple_set_location (join, loc);\n+      gimple_set_lhs (join, ddvar);\n+\n+      /* Mark the beginning of this level sequence.  */\n+      if (inner)\n+\tlower_oacc_loop_marker (loc, ddvar, true,\n+\t\t\t\tbuild_int_cst (integer_type_node, count),\n+\t\t\t\t&fork_seq);\n+      lower_oacc_loop_marker (loc, ddvar, false,\n+\t\t\t      build_int_cst (integer_type_node, done),\n+\t\t\t      &join_seq);\n+\n+      gimple_seq_add_stmt (&fork_seq, fork);\n+      gimple_seq_add_stmt (&join_seq, join);\n+\n+      /* Append this level to head. */\n+      gimple_seq_add_seq (head, fork_seq);\n+      /* Prepend it to tail.  */\n+      gimple_seq_add_seq (&join_seq, *tail);\n+      *tail = join_seq;\n+\n+      inner = true;\n+    }\n+\n+  /* Mark the end of the sequence.  */\n+  lower_oacc_loop_marker (loc, ddvar, true, NULL_TREE, head);\n+  lower_oacc_loop_marker (loc, ddvar, false, NULL_TREE, tail);\n+}\n \n /* A convenience function to build an empty GIMPLE_COND with just the\n    condition.  */\n@@ -6760,6 +6995,149 @@ expand_omp_taskreg (struct omp_region *region)\n     update_ssa (TODO_update_ssa_only_virtuals);\n }\n \n+/* Information about members of an OpenACC collapsed loop nest.  */\n+\n+struct oacc_collapse\n+{\n+  tree base;  /* Base value. */\n+  tree iters; /* Number of steps.  */\n+  tree step;  /* step size.  */\n+};\n+\n+/* Helper for expand_oacc_for.  Determine collapsed loop information.\n+   Fill in COUNTS array.  Emit any initialization code before GSI.\n+   Return the calculated outer loop bound of BOUND_TYPE.  */\n+\n+static tree\n+expand_oacc_collapse_init (const struct omp_for_data *fd,\n+\t\t\t   gimple_stmt_iterator *gsi,\n+\t\t\t   oacc_collapse *counts, tree bound_type)\n+{\n+  tree total = build_int_cst (bound_type, 1);\n+  int ix;\n+  \n+  gcc_assert (integer_onep (fd->loop.step));\n+  gcc_assert (integer_zerop (fd->loop.n1));\n+\n+  for (ix = 0; ix != fd->collapse; ix++)\n+    {\n+      const omp_for_data_loop *loop = &fd->loops[ix];\n+\n+      tree iter_type = TREE_TYPE (loop->v);\n+      tree diff_type = iter_type;\n+      tree plus_type = iter_type;\n+\n+      gcc_assert (loop->cond_code == fd->loop.cond_code);\n+      \n+      if (POINTER_TYPE_P (iter_type))\n+\tplus_type = sizetype;\n+      if (POINTER_TYPE_P (diff_type) || TYPE_UNSIGNED (diff_type))\n+\tdiff_type = signed_type_for (diff_type);\n+\n+      tree b = loop->n1;\n+      tree e = loop->n2;\n+      tree s = loop->step;\n+      bool up = loop->cond_code == LT_EXPR;\n+      tree dir = build_int_cst (diff_type, up ? +1 : -1);\n+      bool negating;\n+      tree expr;\n+\n+      b = force_gimple_operand_gsi (gsi, b, true, NULL_TREE,\n+\t\t\t\t    true, GSI_SAME_STMT);\n+      e = force_gimple_operand_gsi (gsi, e, true, NULL_TREE,\n+\t\t\t\t    true, GSI_SAME_STMT);\n+\n+      /* Convert the step, avoiding possible unsigned->signed overflow. */\n+      negating = !up && TYPE_UNSIGNED (TREE_TYPE (s));\n+      if (negating)\n+\ts = fold_build1 (NEGATE_EXPR, TREE_TYPE (s), s);\n+      s = fold_convert (diff_type, s);\n+      if (negating)\n+\ts = fold_build1 (NEGATE_EXPR, diff_type, s);\n+      s = force_gimple_operand_gsi (gsi, s, true, NULL_TREE,\n+\t\t\t\t    true, GSI_SAME_STMT);\n+\n+      /* Determine the range, avoiding possible unsigned->signed overflow. */\n+      negating = !up && TYPE_UNSIGNED (iter_type);\n+      expr = fold_build2 (MINUS_EXPR, plus_type,\n+\t\t\t  fold_convert (plus_type, negating ? b : e),\n+\t\t\t  fold_convert (plus_type, negating ? e : b));\n+      expr = fold_convert (diff_type, expr);\n+      if (negating)\n+\texpr = fold_build1 (NEGATE_EXPR, diff_type, expr);\n+      tree range = force_gimple_operand_gsi\n+\t(gsi, expr, true, NULL_TREE, true, GSI_SAME_STMT);\n+\n+      /* Determine number of iterations.  */\n+      expr = fold_build2 (MINUS_EXPR, diff_type, range, dir);\n+      expr = fold_build2 (PLUS_EXPR, diff_type, expr, s);\n+      expr = fold_build2 (TRUNC_DIV_EXPR, diff_type, expr, s);\n+\n+      tree iters = force_gimple_operand_gsi (gsi, expr, true, NULL_TREE,\n+\t\t\t\t\t     true, GSI_SAME_STMT);\n+\n+      counts[ix].base = b;\n+      counts[ix].iters = iters;\n+      counts[ix].step = s;\n+\n+      total = fold_build2 (MULT_EXPR, bound_type, total,\n+\t\t\t   fold_convert (bound_type, iters));\n+    }\n+\n+  return total;\n+}\n+\n+/* Emit initializers for collapsed loop members.  IVAR is the outer\n+   loop iteration variable, from which collapsed loop iteration values\n+   are  calculated.  COUNTS array has been initialized by\n+   expand_oacc_collapse_inits.  */\n+\n+static void\n+expand_oacc_collapse_vars (const struct omp_for_data *fd,\n+\t\t\t   gimple_stmt_iterator *gsi,\n+\t\t\t   const oacc_collapse *counts, tree ivar)\n+{\n+  tree ivar_type = TREE_TYPE (ivar);\n+\n+  /*  The most rapidly changing iteration variable is the innermost\n+      one.  */\n+  for (int ix = fd->collapse; ix--;)\n+    {\n+      const omp_for_data_loop *loop = &fd->loops[ix];\n+      const oacc_collapse *collapse = &counts[ix];\n+      tree iter_type = TREE_TYPE (loop->v);\n+      tree diff_type = TREE_TYPE (collapse->step);\n+      tree plus_type = iter_type;\n+      enum tree_code plus_code = PLUS_EXPR;\n+      tree expr;\n+\n+      if (POINTER_TYPE_P (iter_type))\n+\t{\n+\t  plus_code = POINTER_PLUS_EXPR;\n+\t  plus_type = sizetype;\n+\t}\n+\n+      expr = fold_build2 (TRUNC_MOD_EXPR, ivar_type, ivar,\n+\t\t\t  fold_convert (ivar_type, collapse->iters));\n+      expr = fold_build2 (MULT_EXPR, diff_type, fold_convert (diff_type, expr),\n+\t\t\t  collapse->step);\n+      expr = fold_build2 (plus_code, iter_type, collapse->base,\n+\t\t\t  fold_convert (plus_type, expr));\n+      expr = force_gimple_operand_gsi (gsi, expr, false, NULL_TREE,\n+\t\t\t\t       true, GSI_SAME_STMT);\n+      gassign *ass = gimple_build_assign (loop->v, expr);\n+      gsi_insert_before (gsi, ass, GSI_SAME_STMT);\n+\n+      if (ix)\n+\t{\n+\t  expr = fold_build2 (TRUNC_DIV_EXPR, ivar_type, ivar,\n+\t\t\t      fold_convert (ivar_type, collapse->iters));\n+\t  ivar = force_gimple_operand_gsi (gsi, expr, true, NULL_TREE,\n+\t\t\t\t\t   true, GSI_SAME_STMT);\n+\t}\n+    }\n+}\n+\n \n /* Helper function for expand_omp_{for_*,simd}.  If this is the outermost\n    of the combined collapse > 1 loop constructs, generate code like:\n@@ -8406,10 +8784,6 @@ expand_omp_for_static_nochunk (struct omp_region *region,\n   tree *counts = NULL;\n   tree n1, n2, step;\n \n-  gcc_checking_assert ((gimple_omp_for_kind (fd->for_stmt)\n-\t\t\t!= GF_OMP_FOR_KIND_OACC_LOOP)\n-\t\t       || !inner_stmt);\n-\n   itype = type = TREE_TYPE (fd->loop.v);\n   if (POINTER_TYPE_P (type))\n     itype = signed_type_for (type);\n@@ -8502,10 +8876,6 @@ expand_omp_for_static_nochunk (struct omp_region *region,\n       nthreads = builtin_decl_explicit (BUILT_IN_OMP_GET_NUM_TEAMS);\n       threadid = builtin_decl_explicit (BUILT_IN_OMP_GET_TEAM_NUM);\n       break;\n-    case GF_OMP_FOR_KIND_OACC_LOOP:\n-      nthreads = builtin_decl_explicit (BUILT_IN_GOACC_GET_NUM_THREADS);\n-      threadid = builtin_decl_explicit (BUILT_IN_GOACC_GET_THREAD_NUM);\n-      break;\n     default:\n       gcc_unreachable ();\n     }\n@@ -8732,10 +9102,7 @@ expand_omp_for_static_nochunk (struct omp_region *region,\n   if (!gimple_omp_return_nowait_p (gsi_stmt (gsi)))\n     {\n       t = gimple_omp_return_lhs (gsi_stmt (gsi));\n-      if (gimple_omp_for_kind (fd->for_stmt) == GF_OMP_FOR_KIND_OACC_LOOP)\n-\tgcc_checking_assert (t == NULL_TREE);\n-      else\n-\tgsi_insert_after (&gsi, build_omp_barrier (t), GSI_SAME_STMT);\n+      gsi_insert_after (&gsi, build_omp_barrier (t), GSI_SAME_STMT);\n     }\n   gsi_remove (&gsi, true);\n \n@@ -8873,10 +9240,6 @@ expand_omp_for_static_chunk (struct omp_region *region,\n   tree *counts = NULL;\n   tree n1, n2, step;\n \n-  gcc_checking_assert ((gimple_omp_for_kind (fd->for_stmt)\n-\t\t\t!= GF_OMP_FOR_KIND_OACC_LOOP)\n-\t\t       || !inner_stmt);\n-\n   itype = type = TREE_TYPE (fd->loop.v);\n   if (POINTER_TYPE_P (type))\n     itype = signed_type_for (type);\n@@ -8973,10 +9336,6 @@ expand_omp_for_static_chunk (struct omp_region *region,\n       nthreads = builtin_decl_explicit (BUILT_IN_OMP_GET_NUM_TEAMS);\n       threadid = builtin_decl_explicit (BUILT_IN_OMP_GET_TEAM_NUM);\n       break;\n-    case GF_OMP_FOR_KIND_OACC_LOOP:\n-      nthreads = builtin_decl_explicit (BUILT_IN_GOACC_GET_NUM_THREADS);\n-      threadid = builtin_decl_explicit (BUILT_IN_GOACC_GET_THREAD_NUM);\n-      break;\n     default:\n       gcc_unreachable ();\n     }\n@@ -9236,10 +9595,7 @@ expand_omp_for_static_chunk (struct omp_region *region,\n   if (!gimple_omp_return_nowait_p (gsi_stmt (gsi)))\n     {\n       t = gimple_omp_return_lhs (gsi_stmt (gsi));\n-      if (gimple_omp_for_kind (fd->for_stmt) == GF_OMP_FOR_KIND_OACC_LOOP)\n-\tgcc_checking_assert (t == NULL_TREE);\n-      else\n-\tgsi_insert_after (&gsi, build_omp_barrier (t), GSI_SAME_STMT);\n+      gsi_insert_after (&gsi, build_omp_barrier (t), GSI_SAME_STMT);\n     }\n   gsi_remove (&gsi, true);\n \n@@ -10289,6 +10645,410 @@ expand_omp_taskloop_for_inner (struct omp_region *region,\n     }\n }\n \n+/* A subroutine of expand_omp_for.  Generate code for an OpenACC\n+   partitioned loop.  The lowering here is abstracted, in that the\n+   loop parameters are passed through internal functions, which are\n+   further lowered by oacc_device_lower, once we get to the target\n+   compiler.  The loop is of the form:\n+\n+   for (V = B; V LTGT E; V += S) {BODY}\n+\n+   where LTGT is < or >.  We may have a specified chunking size, CHUNKING\n+   (constant 0 for no chunking) and we will have a GWV partitioning\n+   mask, specifying dimensions over which the loop is to be\n+   partitioned (see note below).  We generate code that looks like:\n+\n+   <entry_bb> [incoming FALL->body, BRANCH->exit]\n+     typedef signedintify (typeof (V)) T;  // underlying signed integral type\n+     T range = E - B;\n+     T chunk_no = 0;\n+     T DIR = LTGT == '<' ? +1 : -1;\n+     T chunk_max = GOACC_LOOP_CHUNK (dir, range, S, CHUNK_SIZE, GWV);\n+     T step = GOACC_LOOP_STEP (dir, range, S, CHUNK_SIZE, GWV);\n+\n+   <head_bb> [created by splitting end of entry_bb]\n+     T offset = GOACC_LOOP_OFFSET (dir, range, S, CHUNK_SIZE, GWV, chunk_no);\n+     T bound = GOACC_LOOP_BOUND (dir, range, S, CHUNK_SIZE, GWV, offset);\n+     if (!(offset LTGT bound)) goto bottom_bb;\n+\n+   <body_bb> [incoming]\n+     V = B + offset;\n+     {BODY}\n+\n+   <cont_bb> [incoming, may == body_bb FALL->exit_bb, BRANCH->body_bb]\n+     offset += step;\n+     if (offset LTGT bound) goto body_bb; [*]\n+\n+   <bottom_bb> [created by splitting start of exit_bb] insert BRANCH->head_bb\n+     chunk_no++;\n+     if (chunk < chunk_max) goto head_bb;\n+\n+   <exit_bb> [incoming]\n+     V = B + ((range -/+ 1) / S +/- 1) * S [*]\n+\n+   [*] Needed if V live at end of loop\n+\n+   Note: CHUNKING & GWV mask are specified explicitly here.  This is a\n+   transition, and will be specified by a more general mechanism shortly.\n+ */\n+\n+static void\n+expand_oacc_for (struct omp_region *region, struct omp_for_data *fd)\n+{\n+  tree v = fd->loop.v;\n+  enum tree_code cond_code = fd->loop.cond_code;\n+  enum tree_code plus_code = PLUS_EXPR;\n+\n+  tree chunk_size = integer_minus_one_node;\n+  tree gwv = integer_zero_node;\n+  tree iter_type = TREE_TYPE (v);\n+  tree diff_type = iter_type;\n+  tree plus_type = iter_type;\n+  struct oacc_collapse *counts = NULL;\n+\n+  gcc_checking_assert (gimple_omp_for_kind (fd->for_stmt)\n+\t\t       == GF_OMP_FOR_KIND_OACC_LOOP);\n+  gcc_assert (!gimple_omp_for_combined_into_p (fd->for_stmt));\n+  gcc_assert (cond_code == LT_EXPR || cond_code == GT_EXPR);\n+\n+  if (POINTER_TYPE_P (iter_type))\n+    {\n+      plus_code = POINTER_PLUS_EXPR;\n+      plus_type = sizetype;\n+    }\n+  if (POINTER_TYPE_P (diff_type) || TYPE_UNSIGNED (diff_type))\n+    diff_type = signed_type_for (diff_type);\n+\n+  basic_block entry_bb = region->entry; /* BB ending in OMP_FOR */\n+  basic_block exit_bb = region->exit; /* BB ending in OMP_RETURN */\n+  basic_block cont_bb = region->cont; /* BB ending in OMP_CONTINUE  */\n+  basic_block bottom_bb = NULL;\n+\n+  /* entry_bb has two sucessors; the branch edge is to the exit\n+     block,  fallthrough edge to body.  */\n+  gcc_assert (EDGE_COUNT (entry_bb->succs) == 2\n+\t      && BRANCH_EDGE (entry_bb)->dest == exit_bb);\n+\n+  /* If cont_bb non-NULL, it has 2 successors.  The branch successor is\n+     body_bb, or to a block whose only successor is the body_bb.  Its\n+     fallthrough successor is the final block (same as the branch\n+     successor of the entry_bb).  */\n+  if (cont_bb)\n+    {\n+      basic_block body_bb = FALLTHRU_EDGE (entry_bb)->dest;\n+      basic_block bed = BRANCH_EDGE (cont_bb)->dest;\n+\n+      gcc_assert (FALLTHRU_EDGE (cont_bb)->dest == exit_bb);\n+      gcc_assert (bed == body_bb || single_succ_edge (bed)->dest == body_bb);\n+    }\n+  else\n+    gcc_assert (!gimple_in_ssa_p (cfun));\n+\n+  /* The exit block only has entry_bb and cont_bb as predecessors.  */\n+  gcc_assert (EDGE_COUNT (exit_bb->preds) == 1 + (cont_bb != NULL));\n+\n+  tree chunk_no;\n+  tree chunk_max = NULL_TREE;\n+  tree bound, offset;\n+  tree step = create_tmp_var (diff_type, \".step\");\n+  bool up = cond_code == LT_EXPR;\n+  tree dir = build_int_cst (diff_type, up ? +1 : -1);\n+  bool chunking = !gimple_in_ssa_p (cfun);;\n+  bool negating;\n+\n+  /* SSA instances.  */\n+  tree offset_incr = NULL_TREE;\n+  tree offset_init = NULL_TREE;\n+\n+  gimple_stmt_iterator gsi;\n+  gassign *ass;\n+  gcall *call;\n+  gimple *stmt;\n+  tree expr;\n+  location_t loc;\n+  edge split, be, fte;\n+\n+  /* Split the end of entry_bb to create head_bb.  */\n+  split = split_block (entry_bb, last_stmt (entry_bb));\n+  basic_block head_bb = split->dest;\n+  entry_bb = split->src;\n+\n+  /* Chunk setup goes at end of entry_bb, replacing the omp_for.  */\n+  gsi = gsi_last_bb (entry_bb);\n+  gomp_for *for_stmt = as_a <gomp_for *> (gsi_stmt (gsi));\n+  loc = gimple_location (for_stmt);\n+\n+  if (gimple_in_ssa_p (cfun))\n+    {\n+      offset_init = gimple_omp_for_index (for_stmt, 0);\n+      gcc_assert (integer_zerop (fd->loop.n1));\n+      /* The SSA parallelizer does gang parallelism.  */\n+      gwv = build_int_cst (integer_type_node, GOMP_DIM_MASK (GOMP_DIM_GANG));\n+    }\n+\n+  if (fd->collapse > 1)\n+    {\n+      counts = XALLOCAVEC (struct oacc_collapse, fd->collapse);\n+      tree total = expand_oacc_collapse_init (fd, &gsi, counts,\n+\t\t\t\t\t      TREE_TYPE (fd->loop.n2));\n+\n+      if (SSA_VAR_P (fd->loop.n2))\n+\t{\n+\t  total = force_gimple_operand_gsi (&gsi, total, false, NULL_TREE,\n+\t\t\t\t\t    true, GSI_SAME_STMT);\n+\t  ass = gimple_build_assign (fd->loop.n2, total);\n+\t  gsi_insert_before (&gsi, ass, GSI_SAME_STMT);\n+\t}\n+      \n+    }\n+\n+  tree b = fd->loop.n1;\n+  tree e = fd->loop.n2;\n+  tree s = fd->loop.step;\n+\n+  b = force_gimple_operand_gsi (&gsi, b, true, NULL_TREE, true, GSI_SAME_STMT);\n+  e = force_gimple_operand_gsi (&gsi, e, true, NULL_TREE, true, GSI_SAME_STMT);\n+\n+  /* Convert the step, avoiding possible unsigned->signed overflow. */\n+  negating = !up && TYPE_UNSIGNED (TREE_TYPE (s));\n+  if (negating)\n+    s = fold_build1 (NEGATE_EXPR, TREE_TYPE (s), s);\n+  s = fold_convert (diff_type, s);\n+  if (negating)\n+    s = fold_build1 (NEGATE_EXPR, diff_type, s);\n+  s = force_gimple_operand_gsi (&gsi, s, true, NULL_TREE, true, GSI_SAME_STMT);\n+\n+  if (!chunking)\n+    chunk_size = integer_zero_node;\n+  expr = fold_convert (diff_type, chunk_size);\n+  chunk_size = force_gimple_operand_gsi (&gsi, expr, true,\n+\t\t\t\t\t NULL_TREE, true, GSI_SAME_STMT);\n+  /* Determine the range, avoiding possible unsigned->signed overflow. */\n+  negating = !up && TYPE_UNSIGNED (iter_type);\n+  expr = fold_build2 (MINUS_EXPR, plus_type,\n+\t\t      fold_convert (plus_type, negating ? b : e),\n+\t\t      fold_convert (plus_type, negating ? e : b));\n+  expr = fold_convert (diff_type, expr);\n+  if (negating)\n+    expr = fold_build1 (NEGATE_EXPR, diff_type, expr);\n+  tree range = force_gimple_operand_gsi (&gsi, expr, true,\n+\t\t\t\t\t NULL_TREE, true, GSI_SAME_STMT);\n+\n+  chunk_no = build_int_cst (diff_type, 0);\n+  if (chunking)\n+    {\n+      gcc_assert (!gimple_in_ssa_p (cfun));\n+\n+      expr = chunk_no;\n+      chunk_max = create_tmp_var (diff_type, \".chunk_max\");\n+      chunk_no = create_tmp_var (diff_type, \".chunk_no\");\n+\n+      ass = gimple_build_assign (chunk_no, expr);\n+      gsi_insert_before (&gsi, ass, GSI_SAME_STMT);\n+\n+      call = gimple_build_call_internal (IFN_GOACC_LOOP, 6,\n+\t\t\t\t\t build_int_cst (integer_type_node,\n+\t\t\t\t\t\t\tIFN_GOACC_LOOP_CHUNKS),\n+\t\t\t\t\t dir, range, s, chunk_size, gwv);\n+      gimple_call_set_lhs (call, chunk_max);\n+      gimple_set_location (call, loc);\n+      gsi_insert_before (&gsi, call, GSI_SAME_STMT);\n+    }\n+  else\n+    chunk_size = chunk_no;\n+\n+  call = gimple_build_call_internal (IFN_GOACC_LOOP, 6,\n+\t\t\t\t     build_int_cst (integer_type_node,\n+\t\t\t\t\t\t    IFN_GOACC_LOOP_STEP),\n+\t\t\t\t     dir, range, s, chunk_size, gwv);\n+  gimple_call_set_lhs (call, step);\n+  gimple_set_location (call, loc);\n+  gsi_insert_before (&gsi, call, GSI_SAME_STMT);\n+\n+  /* Remove the GIMPLE_OMP_FOR.  */\n+  gsi_remove (&gsi, true);\n+\n+  /* Fixup edges from head_bb */\n+  be = BRANCH_EDGE (head_bb);\n+  fte = FALLTHRU_EDGE (head_bb);\n+  be->flags |= EDGE_FALSE_VALUE;\n+  fte->flags ^= EDGE_FALLTHRU | EDGE_TRUE_VALUE;\n+\n+  basic_block body_bb = fte->dest;\n+\n+  if (gimple_in_ssa_p (cfun))\n+    {\n+      gsi = gsi_last_bb (cont_bb);\n+      gomp_continue *cont_stmt = as_a <gomp_continue *> (gsi_stmt (gsi));\n+\n+      offset = gimple_omp_continue_control_use (cont_stmt);\n+      offset_incr = gimple_omp_continue_control_def (cont_stmt);\n+    }\n+  else\n+    {\n+      offset = create_tmp_var (diff_type, \".offset\");\n+      offset_init = offset_incr = offset;\n+    }\n+  bound = create_tmp_var (TREE_TYPE (offset), \".bound\");\n+\n+  /* Loop offset & bound go into head_bb.  */\n+  gsi = gsi_start_bb (head_bb);\n+\n+  call = gimple_build_call_internal (IFN_GOACC_LOOP, 7,\n+\t\t\t\t     build_int_cst (integer_type_node,\n+\t\t\t\t\t\t    IFN_GOACC_LOOP_OFFSET),\n+\t\t\t\t     dir, range, s,\n+\t\t\t\t     chunk_size, gwv, chunk_no);\n+  gimple_call_set_lhs (call, offset_init);\n+  gimple_set_location (call, loc);\n+  gsi_insert_after (&gsi, call, GSI_CONTINUE_LINKING);\n+\n+  call = gimple_build_call_internal (IFN_GOACC_LOOP, 7,\n+\t\t\t\t     build_int_cst (integer_type_node,\n+\t\t\t\t\t\t    IFN_GOACC_LOOP_BOUND),\n+\t\t\t\t     dir, range, s,\n+\t\t\t\t     chunk_size, gwv, offset_init);\n+  gimple_call_set_lhs (call, bound);\n+  gimple_set_location (call, loc);\n+  gsi_insert_after (&gsi, call, GSI_CONTINUE_LINKING);\n+\n+  expr = build2 (cond_code, boolean_type_node, offset_init, bound);\n+  gsi_insert_after (&gsi, gimple_build_cond_empty (expr),\n+\t\t    GSI_CONTINUE_LINKING);\n+\n+  /* V assignment goes into body_bb.  */\n+  if (!gimple_in_ssa_p (cfun))\n+    {\n+      gsi = gsi_start_bb (body_bb);\n+\n+      expr = build2 (plus_code, iter_type, b,\n+\t\t     fold_convert (plus_type, offset));\n+      expr = force_gimple_operand_gsi (&gsi, expr, false, NULL_TREE,\n+\t\t\t\t       true, GSI_SAME_STMT);\n+      ass = gimple_build_assign (v, expr);\n+      gsi_insert_before (&gsi, ass, GSI_SAME_STMT);\n+      if (fd->collapse > 1)\n+\texpand_oacc_collapse_vars (fd, &gsi, counts, v);\n+    }\n+\n+  /* Loop increment goes into cont_bb.  If this is not a loop, we\n+     will have spawned threads as if it was, and each one will\n+     execute one iteration.  The specification is not explicit about\n+     whether such constructs are ill-formed or not, and they can\n+     occur, especially when noreturn routines are involved.  */\n+  if (cont_bb)\n+    {\n+      gsi = gsi_last_bb (cont_bb);\n+      gomp_continue *cont_stmt = as_a <gomp_continue *> (gsi_stmt (gsi));\n+      loc = gimple_location (cont_stmt);\n+\n+      /* Increment offset.  */\n+      if (gimple_in_ssa_p (cfun))\n+\texpr= build2 (plus_code, iter_type, offset,\n+\t\t      fold_convert (plus_type, step));\n+      else\n+\texpr = build2 (PLUS_EXPR, diff_type, offset, step);\n+      expr = force_gimple_operand_gsi (&gsi, expr, false, NULL_TREE,\n+\t\t\t\t       true, GSI_SAME_STMT);\n+      ass = gimple_build_assign (offset_incr, expr);\n+      gsi_insert_before (&gsi, ass, GSI_SAME_STMT);\n+      expr = build2 (cond_code, boolean_type_node, offset_incr, bound);\n+      gsi_insert_before (&gsi, gimple_build_cond_empty (expr), GSI_SAME_STMT);\n+\n+      /*  Remove the GIMPLE_OMP_CONTINUE.  */\n+      gsi_remove (&gsi, true);\n+\n+      /* Fixup edges from cont_bb */\n+      be = BRANCH_EDGE (cont_bb);\n+      fte = FALLTHRU_EDGE (cont_bb);\n+      be->flags |= EDGE_TRUE_VALUE;\n+      fte->flags ^= EDGE_FALLTHRU | EDGE_FALSE_VALUE;\n+\n+      if (chunking)\n+\t{\n+\t  /* Split the beginning of exit_bb to make bottom_bb.  We\n+\t     need to insert a nop at the start, because splitting is\n+  \t     after a stmt, not before.  */\n+\t  gsi = gsi_start_bb (exit_bb);\n+\t  stmt = gimple_build_nop ();\n+\t  gsi_insert_before (&gsi, stmt, GSI_SAME_STMT);\n+\t  split = split_block (exit_bb, stmt);\n+\t  bottom_bb = split->src;\n+\t  exit_bb = split->dest;\n+\t  gsi = gsi_last_bb (bottom_bb);\n+\n+\t  /* Chunk increment and test goes into bottom_bb.  */\n+\t  expr = build2 (PLUS_EXPR, diff_type, chunk_no,\n+\t\t\t build_int_cst (diff_type, 1));\n+\t  ass = gimple_build_assign (chunk_no, expr);\n+\t  gsi_insert_after (&gsi, ass, GSI_CONTINUE_LINKING);\n+\n+\t  /* Chunk test at end of bottom_bb.  */\n+\t  expr = build2 (LT_EXPR, boolean_type_node, chunk_no, chunk_max);\n+\t  gsi_insert_after (&gsi, gimple_build_cond_empty (expr),\n+\t\t\t    GSI_CONTINUE_LINKING);\n+\n+\t  /* Fixup edges from bottom_bb. */\n+\t  split->flags ^= EDGE_FALLTHRU | EDGE_FALSE_VALUE;\n+\t  make_edge (bottom_bb, head_bb, EDGE_TRUE_VALUE);\n+\t}\n+    }\n+\n+  gsi = gsi_last_bb (exit_bb);\n+  gcc_assert (gimple_code (gsi_stmt (gsi)) == GIMPLE_OMP_RETURN);\n+  loc = gimple_location (gsi_stmt (gsi));\n+\n+  if (!gimple_in_ssa_p (cfun))\n+    {\n+      /* Insert the final value of V, in case it is live.  This is the\n+\t value for the only thread that survives past the join.  */\n+      expr = fold_build2 (MINUS_EXPR, diff_type, range, dir);\n+      expr = fold_build2 (PLUS_EXPR, diff_type, expr, s);\n+      expr = fold_build2 (TRUNC_DIV_EXPR, diff_type, expr, s);\n+      expr = fold_build2 (MULT_EXPR, diff_type, expr, s);\n+      expr = build2 (plus_code, iter_type, b, fold_convert (plus_type, expr));\n+      expr = force_gimple_operand_gsi (&gsi, expr, false, NULL_TREE,\n+\t\t\t\t       true, GSI_SAME_STMT);\n+      ass = gimple_build_assign (v, expr);\n+      gsi_insert_before (&gsi, ass, GSI_SAME_STMT);\n+    }\n+\n+  /* Remove the OMP_RETURN. */\n+  gsi_remove (&gsi, true);\n+\n+  if (cont_bb)\n+    {\n+      /* We now have one or two nested loops.  Update the loop\n+\t structures.  */\n+      struct loop *parent = entry_bb->loop_father;\n+      struct loop *body = body_bb->loop_father;\n+      \n+      if (chunking)\n+\t{\n+\t  struct loop *chunk_loop = alloc_loop ();\n+\t  chunk_loop->header = head_bb;\n+\t  chunk_loop->latch = bottom_bb;\n+\t  add_loop (chunk_loop, parent);\n+\t  parent = chunk_loop;\n+\t}\n+      else if (parent != body)\n+\t{\n+\t  gcc_assert (body->header == body_bb);\n+\t  gcc_assert (body->latch == cont_bb\n+\t\t      || single_pred (body->latch) == cont_bb);\n+\t  parent = NULL;\n+\t}\n+\n+      if (parent)\n+\t{\n+\t  struct loop *body_loop = alloc_loop ();\n+\t  body_loop->header = body_bb;\n+\t  body_loop->latch = cont_bb;\n+\t  add_loop (body_loop, parent);\n+\t}\n+    }\n+}\n+\n /* Expand the OMP loop defined by REGION.  */\n \n static void\n@@ -10324,6 +11084,11 @@ expand_omp_for (struct omp_region *region, gimple *inner_stmt)\n     expand_omp_simd (region, &fd);\n   else if (gimple_omp_for_kind (fd.for_stmt) == GF_OMP_FOR_KIND_CILKFOR)\n     expand_cilk_for (region, &fd);\n+  else if (gimple_omp_for_kind (fd.for_stmt) == GF_OMP_FOR_KIND_OACC_LOOP)\n+    {\n+      gcc_assert (!inner_stmt);\n+      expand_oacc_for (region, &fd);\n+    }\n   else if (gimple_omp_for_kind (fd.for_stmt) == GF_OMP_FOR_KIND_TASKLOOP)\n     {\n       if (gimple_omp_for_combined_into_p (fd.for_stmt))\n@@ -13521,6 +14286,7 @@ lower_omp_for (gimple_stmt_iterator *gsi_p, omp_context *ctx)\n   gomp_for *stmt = as_a <gomp_for *> (gsi_stmt (*gsi_p));\n   gbind *new_stmt;\n   gimple_seq omp_for_body, body, dlist;\n+  gimple_seq oacc_head = NULL, oacc_tail = NULL;\n   size_t i;\n \n   push_gimplify_context ();\n@@ -13629,6 +14395,16 @@ lower_omp_for (gimple_stmt_iterator *gsi_p, omp_context *ctx)\n   /* Once lowered, extract the bounds and clauses.  */\n   extract_omp_for_data (stmt, &fd, NULL);\n \n+  if (is_gimple_omp_oacc (ctx->stmt)\n+      && !ctx_in_oacc_kernels_region (ctx))\n+    lower_oacc_head_tail (gimple_location (stmt),\n+\t\t\t  gimple_omp_for_clauses (stmt),\n+\t\t\t  &oacc_head, &oacc_tail, ctx);\n+\n+  /* Add OpenACC partitioning markers just before the loop  */\n+  if (oacc_head)\n+    gimple_seq_add_seq (&body, oacc_head);\n+  \n   lower_omp_for_lastprivate (&fd, &body, &dlist, ctx);\n \n   if (gimple_omp_for_kind (stmt) == GF_OMP_FOR_KIND_FOR)\n@@ -13662,6 +14438,11 @@ lower_omp_for (gimple_stmt_iterator *gsi_p, omp_context *ctx)\n   /* Region exit marker goes at the end of the loop body.  */\n   gimple_seq_add_stmt (&body, gimple_build_omp_return (fd.have_nowait));\n   maybe_add_implicit_barrier_cancel (ctx, &body);\n+\n+  /* Add OpenACC joining and reduction markers just after the loop.  */\n+  if (oacc_tail)\n+    gimple_seq_add_seq (&body, oacc_tail);\n+\n   pop_gimplify_context (new_stmt);\n \n   gimple_bind_append_vars (new_stmt, ctx->block_vars);"}]}