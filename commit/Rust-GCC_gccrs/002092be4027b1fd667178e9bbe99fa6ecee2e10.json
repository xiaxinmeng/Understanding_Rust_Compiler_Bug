{"sha": "002092be4027b1fd667178e9bbe99fa6ecee2e10", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MDAyMDkyYmU0MDI3YjFmZDY2NzE3OGU5YmJlOTlmYTZlY2VlMmUxMA==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@linaro.org", "date": "2018-02-01T11:04:28Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2018-02-01T11:04:28Z"}, "message": "[AArch64] Handle SVE subregs that are effectively REVs\n\nSubreg reads should be equivalent to storing the inner register to\nmemory and loading the appropriate memory bytes back, with subreg\nwrites doing the reverse.  For the reasons explained in the comments,\nthis isn't what happens for big-endian SVE if we simply reinterpret\none vector register as having a different element size, so the\nconceptual store and load is needed in the general case.\n\nHowever, that obviously produces poor code if we do it too often.\nThe patch therefore adds a pattern for handling the operation in\nregisters.  This copes with the important case of a VIEW_CONVERT\ncreated by tree-vect-slp.c:duplicate_and_interleave.\n\nIt might make sense to tighten the predicates in aarch64-sve.md so\nthat such subregs are not allowed as operands to most instructions,\nbut that's future work.\n\nThis fixes the sve/slp_*.c tests on aarch64_be.\n\n2018-02-01  Richard Sandiford  <richard.sandiford@linaro.org>\n\ngcc/\n\t* config/aarch64/aarch64-protos.h (aarch64_split_sve_subreg_move)\n\t(aarch64_maybe_expand_sve_subreg_move): Declare.\n\t* config/aarch64/aarch64.md (UNSPEC_REV_SUBREG): New unspec.\n\t* config/aarch64/predicates.md (aarch64_any_register_operand): New\n\tpredicate.\n\t* config/aarch64/aarch64-sve.md (mov<mode>): Optimize subreg moves\n\tthat are semantically a reverse operation.\n\t(*aarch64_sve_mov<mode>_subreg_be): New pattern.\n\t* config/aarch64/aarch64.c (aarch64_maybe_expand_sve_subreg_move):\n\t(aarch64_replace_reg_mode, aarch64_split_sve_subreg_move): New\n\tfunctions.\n\t(aarch64_can_change_mode_class): For big-endian, forbid changes\n\tbetween two SVE modes if they have different element sizes.\n\nReviewed-by: James Greenhalgh <james.greenhalgh@arm.com>\n\nFrom-SVN: r257289", "tree": {"sha": "e5e9f2d690b39be0b02462b747c0a8c2b3cf6740", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/e5e9f2d690b39be0b02462b747c0a8c2b3cf6740"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/002092be4027b1fd667178e9bbe99fa6ecee2e10", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/002092be4027b1fd667178e9bbe99fa6ecee2e10", "html_url": "https://github.com/Rust-GCC/gccrs/commit/002092be4027b1fd667178e9bbe99fa6ecee2e10", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/002092be4027b1fd667178e9bbe99fa6ecee2e10/comments", "author": null, "committer": null, "parents": [{"sha": "8179efe00e04285184112de7dbb977a75852197c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8179efe00e04285184112de7dbb977a75852197c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8179efe00e04285184112de7dbb977a75852197c"}], "stats": {"total": 188, "additions": 184, "deletions": 4}, "files": [{"sha": "b655476aaac247dfe50fb8b002d545b34b907692", "filename": "gcc/ChangeLog", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/002092be4027b1fd667178e9bbe99fa6ecee2e10/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/002092be4027b1fd667178e9bbe99fa6ecee2e10/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=002092be4027b1fd667178e9bbe99fa6ecee2e10", "patch": "@@ -1,3 +1,19 @@\n+2018-02-01  Richard Sandiford  <richard.sandiford@linaro.org>\n+\n+\t* config/aarch64/aarch64-protos.h (aarch64_split_sve_subreg_move)\n+\t(aarch64_maybe_expand_sve_subreg_move): Declare.\n+\t* config/aarch64/aarch64.md (UNSPEC_REV_SUBREG): New unspec.\n+\t* config/aarch64/predicates.md (aarch64_any_register_operand): New\n+\tpredicate.\n+\t* config/aarch64/aarch64-sve.md (mov<mode>): Optimize subreg moves\n+\tthat are semantically a reverse operation.\n+\t(*aarch64_sve_mov<mode>_subreg_be): New pattern.\n+\t* config/aarch64/aarch64.c (aarch64_maybe_expand_sve_subreg_move):\n+\t(aarch64_replace_reg_mode, aarch64_split_sve_subreg_move): New\n+\tfunctions.\n+\t(aarch64_can_change_mode_class): For big-endian, forbid changes\n+\tbetween two SVE modes if they have different element sizes.\n+\n 2018-02-01  Richard Sandiford  <richard.sandiford@linaro.org>\n \n \t* config/aarch64/aarch64.c (aarch64_expand_sve_const_vector): Prefer"}, {"sha": "cda2895d28e7496f8fd6c1b365c4bb497b54c323", "filename": "gcc/config/aarch64/aarch64-protos.h", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/002092be4027b1fd667178e9bbe99fa6ecee2e10/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/002092be4027b1fd667178e9bbe99fa6ecee2e10/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h?ref=002092be4027b1fd667178e9bbe99fa6ecee2e10", "patch": "@@ -447,6 +447,8 @@ void aarch64_expand_epilogue (bool);\n void aarch64_expand_mov_immediate (rtx, rtx, rtx (*) (rtx, rtx) = 0);\n void aarch64_emit_sve_pred_move (rtx, rtx, rtx);\n void aarch64_expand_sve_mem_move (rtx, rtx, machine_mode);\n+bool aarch64_maybe_expand_sve_subreg_move (rtx, rtx);\n+void aarch64_split_sve_subreg_move (rtx, rtx, rtx);\n void aarch64_expand_prologue (void);\n void aarch64_expand_vector_init (rtx, rtx);\n void aarch64_init_cumulative_args (CUMULATIVE_ARGS *, const_tree, rtx,"}, {"sha": "9140862d7473f88ac2fa6cd3de57e66ee3c7d3f9", "filename": "gcc/config/aarch64/aarch64-sve.md", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/002092be4027b1fd667178e9bbe99fa6ecee2e10/gcc%2Fconfig%2Faarch64%2Faarch64-sve.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/002092be4027b1fd667178e9bbe99fa6ecee2e10/gcc%2Fconfig%2Faarch64%2Faarch64-sve.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-sve.md?ref=002092be4027b1fd667178e9bbe99fa6ecee2e10", "patch": "@@ -84,6 +84,32 @@\n \t\t\t\t      gen_vec_duplicate<mode>);\n \tDONE;\n       }\n+\n+    /* Optimize subregs on big-endian targets: we can use REV[BHW]\n+       instead of going through memory.  */\n+    if (BYTES_BIG_ENDIAN\n+        && aarch64_maybe_expand_sve_subreg_move (operands[0], operands[1]))\n+      DONE;\n+  }\n+)\n+\n+;; A pattern for optimizing SUBREGs that have a reinterpreting effect\n+;; on big-endian targets; see aarch64_maybe_expand_sve_subreg_move\n+;; for details.  We use a special predicate for operand 2 to reduce\n+;; the number of patterns.\n+(define_insn_and_split \"*aarch64_sve_mov<mode>_subreg_be\"\n+  [(set (match_operand:SVE_ALL 0 \"aarch64_sve_nonimmediate_operand\" \"=w\")\n+\t(unspec:SVE_ALL\n+          [(match_operand:VNx16BI 1 \"register_operand\" \"Upl\")\n+\t   (match_operand 2 \"aarch64_any_register_operand\" \"w\")]\n+\t  UNSPEC_REV_SUBREG))]\n+  \"TARGET_SVE && BYTES_BIG_ENDIAN\"\n+  \"#\"\n+  \"&& reload_completed\"\n+  [(const_int 0)]\n+  {\n+    aarch64_split_sve_subreg_move (operands[0], operands[1], operands[2]);\n+    DONE;\n   }\n )\n "}, {"sha": "7b34bdf5cf46443696318e148574bb58881d9dc3", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 135, "deletions": 4, "changes": 139, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/002092be4027b1fd667178e9bbe99fa6ecee2e10/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/002092be4027b1fd667178e9bbe99fa6ecee2e10/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=002092be4027b1fd667178e9bbe99fa6ecee2e10", "patch": "@@ -3074,6 +3074,120 @@ aarch64_expand_sve_mem_move (rtx dest, rtx src, machine_mode pred_mode)\n   aarch64_emit_sve_pred_move (dest, ptrue, src);\n }\n \n+/* Called only on big-endian targets.  See whether an SVE vector move\n+   from SRC to DEST is effectively a REV[BHW] instruction, because at\n+   least one operand is a subreg of an SVE vector that has wider or\n+   narrower elements.  Return true and emit the instruction if so.\n+\n+   For example:\n+\n+     (set (reg:VNx8HI R1) (subreg:VNx8HI (reg:VNx16QI R2) 0))\n+\n+   represents a VIEW_CONVERT between the following vectors, viewed\n+   in memory order:\n+\n+     R2: { [0].high, [0].low,  [1].high, [1].low, ... }\n+     R1: { [0],      [1],      [2],      [3],     ... }\n+\n+   The high part of lane X in R2 should therefore correspond to lane X*2\n+   of R1, but the register representations are:\n+\n+         msb                                      lsb\n+     R2: ...... [1].high  [1].low   [0].high  [0].low\n+     R1: ...... [3]       [2]       [1]       [0]\n+\n+   where the low part of lane X in R2 corresponds to lane X*2 in R1.\n+   We therefore need a reverse operation to swap the high and low values\n+   around.\n+\n+   This is purely an optimization.  Without it we would spill the\n+   subreg operand to the stack in one mode and reload it in the\n+   other mode, which has the same effect as the REV.  */\n+\n+bool\n+aarch64_maybe_expand_sve_subreg_move (rtx dest, rtx src)\n+{\n+  gcc_assert (BYTES_BIG_ENDIAN);\n+  if (GET_CODE (dest) == SUBREG)\n+    dest = SUBREG_REG (dest);\n+  if (GET_CODE (src) == SUBREG)\n+    src = SUBREG_REG (src);\n+\n+  /* The optimization handles two single SVE REGs with different element\n+     sizes.  */\n+  if (!REG_P (dest)\n+      || !REG_P (src)\n+      || aarch64_classify_vector_mode (GET_MODE (dest)) != VEC_SVE_DATA\n+      || aarch64_classify_vector_mode (GET_MODE (src)) != VEC_SVE_DATA\n+      || (GET_MODE_UNIT_SIZE (GET_MODE (dest))\n+\t  == GET_MODE_UNIT_SIZE (GET_MODE (src))))\n+    return false;\n+\n+  /* Generate *aarch64_sve_mov<mode>_subreg_be.  */\n+  rtx ptrue = force_reg (VNx16BImode, CONSTM1_RTX (VNx16BImode));\n+  rtx unspec = gen_rtx_UNSPEC (GET_MODE (dest), gen_rtvec (2, ptrue, src),\n+\t\t\t       UNSPEC_REV_SUBREG);\n+  emit_insn (gen_rtx_SET (dest, unspec));\n+  return true;\n+}\n+\n+/* Return a copy of X with mode MODE, without changing its other\n+   attributes.  Unlike gen_lowpart, this doesn't care whether the\n+   mode change is valid.  */\n+\n+static rtx\n+aarch64_replace_reg_mode (rtx x, machine_mode mode)\n+{\n+  if (GET_MODE (x) == mode)\n+    return x;\n+\n+  x = shallow_copy_rtx (x);\n+  set_mode_and_regno (x, mode, REGNO (x));\n+  return x;\n+}\n+\n+/* Split a *aarch64_sve_mov<mode>_subreg_be pattern with the given\n+   operands.  */\n+\n+void\n+aarch64_split_sve_subreg_move (rtx dest, rtx ptrue, rtx src)\n+{\n+  /* Decide which REV operation we need.  The mode with narrower elements\n+     determines the mode of the operands and the mode with the wider\n+     elements determines the reverse width.  */\n+  machine_mode mode_with_wider_elts = GET_MODE (dest);\n+  machine_mode mode_with_narrower_elts = GET_MODE (src);\n+  if (GET_MODE_UNIT_SIZE (mode_with_wider_elts)\n+      < GET_MODE_UNIT_SIZE (mode_with_narrower_elts))\n+    std::swap (mode_with_wider_elts, mode_with_narrower_elts);\n+\n+  unsigned int wider_bytes = GET_MODE_UNIT_SIZE (mode_with_wider_elts);\n+  unsigned int unspec;\n+  if (wider_bytes == 8)\n+    unspec = UNSPEC_REV64;\n+  else if (wider_bytes == 4)\n+    unspec = UNSPEC_REV32;\n+  else if (wider_bytes == 2)\n+    unspec = UNSPEC_REV16;\n+  else\n+    gcc_unreachable ();\n+  machine_mode pred_mode = aarch64_sve_pred_mode (wider_bytes).require ();\n+\n+  /* Emit:\n+\n+       (set DEST (unspec [PTRUE (unspec [SRC] UNSPEC_REV<nn>)]\n+\t\t\t UNSPEC_MERGE_PTRUE))\n+\n+     with the appropriate modes.  */\n+  ptrue = gen_lowpart (pred_mode, ptrue);\n+  dest = aarch64_replace_reg_mode (dest, mode_with_narrower_elts);\n+  src = aarch64_replace_reg_mode (src, mode_with_narrower_elts);\n+  src = gen_rtx_UNSPEC (mode_with_narrower_elts, gen_rtvec (1, src), unspec);\n+  src = gen_rtx_UNSPEC (mode_with_narrower_elts, gen_rtvec (2, ptrue, src),\n+\t\t\tUNSPEC_MERGE_PTRUE);\n+  emit_insn (gen_rtx_SET (dest, src));\n+}\n+\n static bool\n aarch64_function_ok_for_sibcall (tree decl ATTRIBUTE_UNUSED,\n \t\t\t\t tree exp ATTRIBUTE_UNUSED)\n@@ -17197,10 +17311,27 @@ static bool\n aarch64_can_change_mode_class (machine_mode from,\n \t\t\t       machine_mode to, reg_class_t)\n {\n-  /* See the comment at the head of aarch64-sve.md for details.  */\n-  if (BYTES_BIG_ENDIAN\n-      && (aarch64_sve_data_mode_p (from) != aarch64_sve_data_mode_p (to)))\n-    return false;\n+  if (BYTES_BIG_ENDIAN)\n+    {\n+      bool from_sve_p = aarch64_sve_data_mode_p (from);\n+      bool to_sve_p = aarch64_sve_data_mode_p (to);\n+\n+      /* Don't allow changes between SVE data modes and non-SVE modes.\n+\t See the comment at the head of aarch64-sve.md for details.  */\n+      if (from_sve_p != to_sve_p)\n+\treturn false;\n+\n+      /* Don't allow changes in element size: lane 0 of the new vector\n+\t would not then be lane 0 of the old vector.  See the comment\n+\t above aarch64_maybe_expand_sve_subreg_move for a more detailed\n+\t description.\n+\n+\t In the worst case, this forces a register to be spilled in\n+\t one mode and reloaded in the other, which handles the\n+\t endianness correctly.  */\n+      if (from_sve_p && GET_MODE_UNIT_SIZE (from) != GET_MODE_UNIT_SIZE (to))\n+\treturn false;\n+    }\n   return true;\n }\n "}, {"sha": "5a2a9309a3bbbfad6fcb6db07422d774909f0ba1", "filename": "gcc/config/aarch64/aarch64.md", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/002092be4027b1fd667178e9bbe99fa6ecee2e10/gcc%2Fconfig%2Faarch64%2Faarch64.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/002092be4027b1fd667178e9bbe99fa6ecee2e10/gcc%2Fconfig%2Faarch64%2Faarch64.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.md?ref=002092be4027b1fd667178e9bbe99fa6ecee2e10", "patch": "@@ -168,6 +168,7 @@\n     UNSPEC_INSR\n     UNSPEC_CLASTB\n     UNSPEC_FADDA\n+    UNSPEC_REV_SUBREG\n ])\n \n (define_c_enum \"unspecv\" ["}, {"sha": "804be16a13a78e989b15023259872f2c81e2e037", "filename": "gcc/config/aarch64/predicates.md", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/002092be4027b1fd667178e9bbe99fa6ecee2e10/gcc%2Fconfig%2Faarch64%2Fpredicates.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/002092be4027b1fd667178e9bbe99fa6ecee2e10/gcc%2Fconfig%2Faarch64%2Fpredicates.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fpredicates.md?ref=002092be4027b1fd667178e9bbe99fa6ecee2e10", "patch": "@@ -617,3 +617,7 @@\n (define_predicate \"aarch64_gather_scale_operand_d\"\n   (and (match_code \"const_int\")\n        (match_test \"INTVAL (op) == 1 || INTVAL (op) == 8\")))\n+\n+;; A special predicate that doesn't match a particular mode.\n+(define_special_predicate \"aarch64_any_register_operand\"\n+  (match_code \"reg\"))"}]}