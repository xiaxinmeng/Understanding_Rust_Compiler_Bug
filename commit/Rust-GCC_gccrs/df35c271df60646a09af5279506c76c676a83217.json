{"sha": "df35c271df60646a09af5279506c76c676a83217", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZGYzNWMyNzFkZjYwNjQ2YTA5YWY1Mjc5NTA2Yzc2YzY3NmE4MzIxNw==", "commit": {"author": {"name": "Steven Bosscher", "email": "steven@gcc.gnu.org", "date": "2009-04-30T10:51:08Z"}, "committer": {"name": "Steven Bosscher", "email": "steven@gcc.gnu.org", "date": "2009-04-30T10:51:08Z"}, "message": "gcse.c (ae_gen): Remove.\n\n\t* gcse.c (ae_gen): Remove.\n\t(can_assign_to_reg_p): Rename to can_assign_to_reg_without_clobbers_p\n\tand make non-static function to make it available in store-motion.c.\n\tUpdate call sites with search-and-replace.\n\t(enumerate_ldsts, reg_set_info, reg_clear_last_set, store_ops_ok,\n\textract_mentioned_regs, extract_mentioned_regs_helper,\n\tfind_moveable_store, compute_store_table, load_kills_store, find_loads,\n\tstore_killed_in_insn, store_killed_after, store_killed_before,\n\tbuild_store_vectors, insert_insn_start_basic_block, insert-store,\n\tremove_reachable_equiv_notes, replace_store_insn, delete_store,\n\tfree_store_memory, one_store_motion_pass, gate_rtl_store_motion,\n\texecute_rtl_store_motion, pass_rtl_store_motion): Move to...\n\t* store-motion.c: ...new file.  Also copy data structures from gcse.c\n\tand clean up to remove parts not used by store motion.\n\t* rtl.h (can_assign_to_reg_without_clobbers_p): Add prototype.\n\t* Makefile.in (store-motion.o): New rule. Add to OBJS-common.\n\nFrom-SVN: r147001", "tree": {"sha": "5267d15c4c34801fce266c5a6afb6cea477323ea", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/5267d15c4c34801fce266c5a6afb6cea477323ea"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/df35c271df60646a09af5279506c76c676a83217", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/df35c271df60646a09af5279506c76c676a83217", "html_url": "https://github.com/Rust-GCC/gccrs/commit/df35c271df60646a09af5279506c76c676a83217", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/df35c271df60646a09af5279506c76c676a83217/comments", "author": null, "committer": null, "parents": [{"sha": "f711a87a64fb1c46e076f065d3b94b528432242a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f711a87a64fb1c46e076f065d3b94b528432242a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f711a87a64fb1c46e076f065d3b94b528432242a"}], "stats": {"total": 2945, "additions": 1611, "deletions": 1334}, "files": [{"sha": "8cee45b87b2d31ab11ff9e0059e8c54cda47aa88", "filename": "gcc/ChangeLog", "status": "modified", "additions": 19, "deletions": 0, "changes": 19, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/df35c271df60646a09af5279506c76c676a83217/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/df35c271df60646a09af5279506c76c676a83217/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=df35c271df60646a09af5279506c76c676a83217", "patch": "@@ -1,3 +1,22 @@\n+2009-04-30  Steven Bosscher  <steven@gcc.gnu.org>\n+\n+\t* gcse.c (ae_gen): Remove.\n+\t(can_assign_to_reg_p): Rename to can_assign_to_reg_without_clobbers_p\n+\tand make non-static function to make it available in store-motion.c.\n+\tUpdate call sites with search-and-replace.\n+\t(enumerate_ldsts, reg_set_info, reg_clear_last_set, store_ops_ok,\n+\textract_mentioned_regs, extract_mentioned_regs_helper,\n+\tfind_moveable_store, compute_store_table, load_kills_store, find_loads,\n+\tstore_killed_in_insn, store_killed_after, store_killed_before,\n+\tbuild_store_vectors, insert_insn_start_basic_block, insert-store,\n+\tremove_reachable_equiv_notes, replace_store_insn, delete_store,\n+\tfree_store_memory, one_store_motion_pass, gate_rtl_store_motion,\n+\texecute_rtl_store_motion, pass_rtl_store_motion): Move to...\n+\t* store-motion.c: ...new file.  Also copy data structures from gcse.c\n+\tand clean up to remove parts not used by store motion.\n+\t* rtl.h (can_assign_to_reg_without_clobbers_p): Add prototype.\n+\t* Makefile.in (store-motion.o): New rule. Add to OBJS-common.\n+\n 2009-04-30  Ramana Radhakrishnan  <ramana.radhakrishnan@arm.com>\n \n \tPR target/38571"}, {"sha": "66d4590289db675d1e8cd9f1133f1fcf0cb2862d", "filename": "gcc/Makefile.in", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/df35c271df60646a09af5279506c76c676a83217/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/df35c271df60646a09af5279506c76c676a83217/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=df35c271df60646a09af5279506c76c676a83217", "patch": "@@ -1199,6 +1199,7 @@ OBJS-common = \\\n \tstatistics.o \\\n \tstmt.o \\\n \tstor-layout.o \\\n+\tstore-motion.o \\\n \tstringpool.o \\\n \ttarghooks.o \\\n \ttimevar.o \\\n@@ -2698,6 +2699,11 @@ see.o : see.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \\\n    $(DF_H) $(OBSTACK_H) $(TIMEVAR_H) $(TREE_PASS_H) $(RECOG_H) $(EXPR_H) \\\n    $(SPLAY_TREE_H) $(HASHTAB_H) $(REGS_H) dce.h\n gcse.o : gcse.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \\\n+   $(REGS_H) hard-reg-set.h $(FLAGS_H) $(REAL_H) insn-config.h $(GGC_H) \\\n+   $(RECOG_H) $(EXPR_H) $(BASIC_BLOCK_H) $(FUNCTION_H) output.h $(TOPLEV_H) \\\n+   $(TM_P_H) $(PARAMS_H) $(EXCEPT_H) $(TREE_H) $(TIMEVAR_H) \\\n+   intl.h $(OBSTACK_H) $(TREE_PASS_H) $(DF_H) $(DBGCNT_H)\n+store-motion.o : store-motion.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \\\n    $(REGS_H) hard-reg-set.h $(FLAGS_H) $(REAL_H) insn-config.h $(GGC_H) \\\n    $(RECOG_H) $(EXPR_H) $(BASIC_BLOCK_H) $(FUNCTION_H) output.h $(TOPLEV_H) \\\n    $(TM_P_H) $(PARAMS_H) $(EXCEPT_H) gt-gcse.h $(TREE_H) cselib.h $(TIMEVAR_H) \\"}, {"sha": "b3fa362aff3c349c562340e6c436ea8339a4cfca", "filename": "gcc/gcse.c", "status": "modified", "additions": 180, "deletions": 1334, "changes": 1514, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/df35c271df60646a09af5279506c76c676a83217/gcc%2Fgcse.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/df35c271df60646a09af5279506c76c676a83217/gcc%2Fgcse.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgcse.c?ref=df35c271df60646a09af5279506c76c676a83217", "patch": "@@ -437,7 +437,7 @@ static int global_const_prop_count;\n static int global_copy_prop_count;\n \f\n /* For available exprs */\n-static sbitmap *ae_kill, *ae_gen;\n+static sbitmap *ae_kill;\n \f\n static void compute_can_copy (void);\n static void *gmalloc (size_t) ATTRIBUTE_MALLOC;\n@@ -450,7 +450,6 @@ static void hash_scan_set (rtx, rtx, struct hash_table *);\n static void hash_scan_clobber (rtx, rtx, struct hash_table *);\n static void hash_scan_call (rtx, rtx, struct hash_table *);\n static int want_to_gcse_p (rtx);\n-static bool can_assign_to_reg_p (rtx);\n static bool gcse_constant_p (const_rtx);\n static int oprs_unchanged_p (const_rtx, const_rtx, int);\n static int oprs_anticipatable_p (const_rtx, const_rtx);\n@@ -527,34 +526,13 @@ static void free_ldst_entry (struct ls_expr *);\n static void free_ldst_mems (void);\n static void print_ldst_list (FILE *);\n static struct ls_expr * find_rtx_in_ldst (rtx);\n-static int enumerate_ldsts (void);\n static inline struct ls_expr * first_ls_expr (void);\n static inline struct ls_expr * next_ls_expr (struct ls_expr *);\n static int simple_mem (const_rtx);\n static void invalidate_any_buried_refs (rtx);\n static void compute_ld_motion_mems (void);\n static void trim_ld_motion_mems (void);\n static void update_ld_motion_stores (struct expr *);\n-static void reg_set_info (rtx, const_rtx, void *);\n-static void reg_clear_last_set (rtx, const_rtx, void *);\n-static bool store_ops_ok (const_rtx, int *);\n-static rtx extract_mentioned_regs (rtx);\n-static rtx extract_mentioned_regs_helper (rtx, rtx);\n-static void find_moveable_store (rtx, int *, int *);\n-static int compute_store_table (void);\n-static bool load_kills_store (const_rtx, const_rtx, int);\n-static bool find_loads (const_rtx, const_rtx, int);\n-static bool store_killed_in_insn (const_rtx, const_rtx, const_rtx, int);\n-static bool store_killed_after (const_rtx, const_rtx, const_rtx, const_basic_block, int *, rtx *);\n-static bool store_killed_before (const_rtx, const_rtx, const_rtx, const_basic_block, int *);\n-static void build_store_vectors (void);\n-static void insert_insn_start_basic_block (rtx, basic_block);\n-static int insert_store (struct ls_expr *, edge);\n-static void remove_reachable_equiv_notes (basic_block, struct ls_expr *);\n-static void replace_store_insn (rtx, rtx, basic_block, struct ls_expr *);\n-static void delete_store (struct ls_expr *, basic_block);\n-static void free_store_memory (void);\n-static int one_store_motion_pass (void);\n static void free_insn_expr_list_list (rtx *);\n static void clear_modify_mem_tables (void);\n static void free_modify_mem_tables (void);\n@@ -816,18 +794,23 @@ want_to_gcse_p (rtx x)\n       return 0;\n \n     default:\n-      return can_assign_to_reg_p (x);\n+      return can_assign_to_reg_without_clobbers_p (x);\n     }\n }\n \n-/* Used internally by can_assign_to_reg_p.  */\n+/* Used internally by can_assign_to_reg_without_clobbers_p.  */\n \n static GTY(()) rtx test_insn;\n \n-/* Return true if we can assign X to a pseudo register.  */\n+/* Return true if we can assign X to a pseudo register such that the\n+   resulting insn does not result in clobbering a hard register as a\n+   side-effect.\n+   This function is typically used by code motion passes, to verify\n+   that it is safe to insert an insn without worrying about clobbering\n+   maybe live hard regs.  */\n \n-static bool\n-can_assign_to_reg_p (rtx x)\n+bool\n+can_assign_to_reg_without_clobbers_p (rtx x)\n {\n   int num_clobbers = 0;\n   int icode;\n@@ -4642,20 +4625,6 @@ find_rtx_in_ldst (rtx x)\n   return (struct ls_expr *) *slot;\n }\n \n-/* Assign each element of the list of mems a monotonically increasing value.  */\n-\n-static int\n-enumerate_ldsts (void)\n-{\n-  struct ls_expr * ptr;\n-  int n = 0;\n-\n-  for (ptr = pre_ldst_mems; ptr != NULL; ptr = ptr->next)\n-    ptr->index = n++;\n-\n-  return n;\n-}\n-\n /* Return first item in the list.  */\n \n static inline struct ls_expr *\n@@ -4801,7 +4770,7 @@ compute_ld_motion_mems (void)\n \t\t\t  && GET_CODE (src) != ASM_OPERANDS\n \t\t\t  /* Check for REG manually since want_to_gcse_p\n \t\t\t     returns 0 for all REGs.  */\n-\t\t\t  && can_assign_to_reg_p (src))\n+\t\t\t  && can_assign_to_reg_without_clobbers_p (src))\n \t\t\tptr->stores = alloc_INSN_LIST (insn, ptr->stores);\n \t\t      else\n \t\t\tptr->invalid = 1;\n@@ -4920,1318 +4889,216 @@ update_ld_motion_stores (struct expr * expr)\n     }\n }\n \f\n-/* Store motion code.  */\n-\n-#define ANTIC_STORE_LIST(x)\t\t((x)->loads)\n-#define AVAIL_STORE_LIST(x)\t\t((x)->stores)\n-#define LAST_AVAIL_CHECK_FAILURE(x)\t((x)->reaching_reg)\n-\n-/* This is used to communicate the target bitvector we want to use in the\n-   reg_set_info routine when called via the note_stores mechanism.  */\n-static int * regvec;\n+/* Return true if the graph is too expensive to optimize. PASS is the\n+   optimization about to be performed.  */\n \n-/* And current insn, for the same routine.  */\n-static rtx compute_store_table_current_insn;\n+static bool\n+is_too_expensive (const char *pass)\n+{\n+  /* Trying to perform global optimizations on flow graphs which have\n+     a high connectivity will take a long time and is unlikely to be\n+     particularly useful.\n \n-/* Used in computing the reverse edge graph bit vectors.  */\n-static sbitmap * st_antloc;\n+     In normal circumstances a cfg should have about twice as many\n+     edges as blocks.  But we do not want to punish small functions\n+     which have a couple switch statements.  Rather than simply\n+     threshold the number of blocks, uses something with a more\n+     graceful degradation.  */\n+  if (n_edges > 20000 + n_basic_blocks * 4)\n+    {\n+      warning (OPT_Wdisabled_optimization,\n+\t       \"%s: %d basic blocks and %d edges/basic block\",\n+\t       pass, n_basic_blocks, n_edges / n_basic_blocks);\n \n-/* Global holding the number of store expressions we are dealing with.  */\n-static int num_stores;\n+      return true;\n+    }\n \n-/* Checks to set if we need to mark a register set.  Called from\n-   note_stores.  */\n+  /* If allocating memory for the cprop bitmap would take up too much\n+     storage it's better just to disable the optimization.  */\n+  if ((n_basic_blocks\n+       * SBITMAP_SET_SIZE (max_reg_num ())\n+       * sizeof (SBITMAP_ELT_TYPE)) > MAX_GCSE_MEMORY)\n+    {\n+      warning (OPT_Wdisabled_optimization,\n+\t       \"%s: %d basic blocks and %d registers\",\n+\t       pass, n_basic_blocks, max_reg_num ());\n \n-static void\n-reg_set_info (rtx dest, const_rtx setter ATTRIBUTE_UNUSED,\n-\t      void *data ATTRIBUTE_UNUSED)\n-{\n-  if (GET_CODE (dest) == SUBREG)\n-    dest = SUBREG_REG (dest);\n+      return true;\n+    }\n \n-  if (REG_P (dest))\n-    regvec[REGNO (dest)] = INSN_UID (compute_store_table_current_insn);\n+  return false;\n }\n \n-/* Clear any mark that says that this insn sets dest.  Called from\n-   note_stores.  */\n+\f\n+/* Main function for the CPROP pass.  */\n \n-static void\n-reg_clear_last_set (rtx dest, const_rtx setter ATTRIBUTE_UNUSED,\n-\t      void *data)\n+static int\n+one_cprop_pass (void)\n {\n-  int *dead_vec = (int *) data;\n+  int changed = 0;\n \n-  if (GET_CODE (dest) == SUBREG)\n-    dest = SUBREG_REG (dest);\n+  /* Return if there's nothing to do, or it is too expensive.  */\n+  if (n_basic_blocks <= NUM_FIXED_BLOCKS + 1\n+      || is_too_expensive (_ (\"const/copy propagation disabled\")))\n+    return 0;\n \n-  if (REG_P (dest) &&\n-      dead_vec[REGNO (dest)] == INSN_UID (compute_store_table_current_insn))\n-    dead_vec[REGNO (dest)] = 0;\n-}\n+  global_const_prop_count = local_const_prop_count = 0;\n+  global_copy_prop_count = local_copy_prop_count = 0;\n \n-/* Return zero if some of the registers in list X are killed\n-   due to set of registers in bitmap REGS_SET.  */\n+  bytes_used = 0;\n+  gcc_obstack_init (&gcse_obstack);\n+  alloc_gcse_mem ();\n \n-static bool\n-store_ops_ok (const_rtx x, int *regs_set)\n-{\n-  const_rtx reg;\n+  /* Do a local const/copy propagation pass first.  The global pass\n+     only handles global opportunities.\n+     If the local pass changes something, remove any unreachable blocks\n+     because the CPROP global dataflow analysis may get into infinite\n+     loops for CFGs with unreachable blocks.\n \n-  for (; x; x = XEXP (x, 1))\n+     FIXME: This local pass should not be necessary after CSE (but for\n+\t    some reason it still is).  It is also (proven) not necessary\n+\t    to run the local pass right after FWPWOP.\n+\t    \n+     FIXME: The global analysis would not get into infinite loops if it\n+\t    would use the DF solver (via df_simple_dataflow) instead of\n+\t    the solver implemented in this file.  */\n+  if (local_cprop_pass ())\n     {\n-      reg = XEXP (x, 0);\n-      if (regs_set[REGNO(reg)])\n-\treturn false;\n+      delete_unreachable_blocks ();\n+      df_analyze ();\n     }\n \n-  return true;\n-}\n-\n-/* Returns a list of registers mentioned in X.  */\n-static rtx\n-extract_mentioned_regs (rtx x)\n-{\n-  return extract_mentioned_regs_helper (x, NULL_RTX);\n-}\n-\n-/* Helper for extract_mentioned_regs; ACCUM is used to accumulate used\n-   registers.  */\n-static rtx\n-extract_mentioned_regs_helper (rtx x, rtx accum)\n-{\n-  int i;\n-  enum rtx_code code;\n-  const char * fmt;\n+  /* Determine implicit sets.  */\n+  implicit_sets = XCNEWVEC (rtx, last_basic_block);\n+  find_implicit_sets ();\n \n-  /* Repeat is used to turn tail-recursion into iteration.  */\n- repeat:\n+  alloc_hash_table (get_max_uid (), &set_hash_table, 1);\n+  compute_hash_table (&set_hash_table);\n \n-  if (x == 0)\n-    return accum;\n+  /* Free implicit_sets before peak usage.  */\n+  free (implicit_sets);\n+  implicit_sets = NULL;\n \n-  code = GET_CODE (x);\n-  switch (code)\n+  if (dump_file)\n+    dump_hash_table (dump_file, \"SET\", &set_hash_table);\n+  if (set_hash_table.n_elems > 0)\n     {\n-    case REG:\n-      return alloc_EXPR_LIST (0, x, accum);\n-\n-    case MEM:\n-      x = XEXP (x, 0);\n-      goto repeat;\n-\n-    case PRE_DEC:\n-    case PRE_INC:\n-    case PRE_MODIFY:\n-    case POST_DEC:\n-    case POST_INC:\n-    case POST_MODIFY:\n-      /* We do not run this function with arguments having side effects.  */\n-      gcc_unreachable ();\n-\n-    case PC:\n-    case CC0: /*FIXME*/\n-    case CONST:\n-    case CONST_INT:\n-    case CONST_DOUBLE:\n-    case CONST_FIXED:\n-    case CONST_VECTOR:\n-    case SYMBOL_REF:\n-    case LABEL_REF:\n-    case ADDR_VEC:\n-    case ADDR_DIFF_VEC:\n-      return accum;\n-\n-    default:\n-      break;\n-    }\n+      basic_block bb;\n+      rtx insn;\n \n-  i = GET_RTX_LENGTH (code) - 1;\n-  fmt = GET_RTX_FORMAT (code);\n+      alloc_cprop_mem (last_basic_block, set_hash_table.n_elems);\n+      compute_cprop_data ();\n \n-  for (; i >= 0; i--)\n-    {\n-      if (fmt[i] == 'e')\n+      FOR_BB_BETWEEN (bb, ENTRY_BLOCK_PTR->next_bb->next_bb, EXIT_BLOCK_PTR, next_bb)\n \t{\n-\t  rtx tem = XEXP (x, i);\n+\t  /* Reset tables used to keep track of what's still valid [since\n+\t     the start of the block].  */\n+\t  reset_opr_set_tables ();\n \n-\t  /* If we are about to do the last recursive call\n-\t     needed at this level, change it into iteration.  */\n-\t  if (i == 0)\n-\t    {\n-\t      x = tem;\n-\t      goto repeat;\n-\t    }\n+\t  FOR_BB_INSNS (bb, insn)\n+\t    if (INSN_P (insn))\n+\t      {\n+\t\tchanged |= cprop_insn (insn);\n \n-\t  accum = extract_mentioned_regs_helper (tem, accum);\n+\t\t/* Keep track of everything modified by this insn.  */\n+\t\t/* ??? Need to be careful w.r.t. mods done to INSN.\n+\t\t       Don't call mark_oprs_set if we turned the\n+\t\t       insn into a NOTE.  */\n+\t\tif (! NOTE_P (insn))\n+\t\t  mark_oprs_set (insn);\n+\t      }\n \t}\n-      else if (fmt[i] == 'E')\n-\t{\n-\t  int j;\n \n-\t  for (j = 0; j < XVECLEN (x, i); j++)\n-\t    accum = extract_mentioned_regs_helper (XVECEXP (x, i, j), accum);\n-\t}\n+      changed |= bypass_conditional_jumps ();\n+      free_cprop_mem ();\n     }\n \n-  return accum;\n-}\n+  free_hash_table (&set_hash_table);\n+  free_gcse_mem ();\n+  obstack_free (&gcse_obstack, NULL);\n \n-/* Determine whether INSN is MEM store pattern that we will consider moving.\n-   REGS_SET_BEFORE is bitmap of registers set before (and including) the\n-   current insn, REGS_SET_AFTER is bitmap of registers set after (and\n-   including) the insn in this basic block.  We must be passing through BB from\n-   head to end, as we are using this fact to speed things up.\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"CPROP of %s, %d basic blocks, %d bytes needed, \",\n+\t       current_function_name (), n_basic_blocks, bytes_used);\n+      fprintf (dump_file, \"%d local const props, %d local copy props, \",\n+\t       local_const_prop_count, local_copy_prop_count);\n+      fprintf (dump_file, \"%d global const props, %d global copy props\\n\\n\",\n+\t       global_const_prop_count, global_copy_prop_count);\n+    }\n \n-   The results are stored this way:\n+  return changed;\n+}\n \n-   -- the first anticipatable expression is added into ANTIC_STORE_LIST\n-   -- if the processed expression is not anticipatable, NULL_RTX is added\n-      there instead, so that we can use it as indicator that no further\n-      expression of this type may be anticipatable\n-   -- if the expression is available, it is added as head of AVAIL_STORE_LIST;\n-      consequently, all of them but this head are dead and may be deleted.\n-   -- if the expression is not available, the insn due to that it fails to be\n-      available is stored in reaching_reg.\n+\f\n+/* All the passes implemented in this file.  Each pass has its\n+   own gate and execute function, and at the end of the file a\n+   pass definition for passes.c.\n \n-   The things are complicated a bit by fact that there already may be stores\n-   to the same MEM from other blocks; also caller must take care of the\n-   necessary cleanup of the temporary markers after end of the basic block.\n-   */\n+   We do not construct an accurate cfg in functions which call\n+   setjmp, so none of these passes runs if the function calls\n+   setjmp.\n+   FIXME: Should just handle setjmp via REG_SETJMP notes.  */\n \n-static void\n-find_moveable_store (rtx insn, int *regs_set_before, int *regs_set_after)\n+static bool\n+gate_rtl_cprop (void)\n {\n-  struct ls_expr * ptr;\n-  rtx dest, set, tmp;\n-  int check_anticipatable, check_available;\n-  basic_block bb = BLOCK_FOR_INSN (insn);\n+  return optimize > 0 && flag_gcse\n+    && !cfun->calls_setjmp\n+    && dbg_cnt (cprop);\n+}\n \n-  set = single_set (insn);\n-  if (!set)\n-    return;\n+static unsigned int\n+execute_rtl_cprop (void)\n+{\n+  delete_unreachable_blocks ();\n+  df_note_add_problem ();\n+  df_set_flags (DF_LR_RUN_DCE);\n+  df_analyze ();\n+  flag_rerun_cse_after_global_opts |= one_cprop_pass ();\n+  return 0;\n+}\n \n-  dest = SET_DEST (set);\n+static bool\n+gate_rtl_pre (void)\n+{\n+  return optimize > 0 && flag_gcse\n+    && !cfun->calls_setjmp\n+    && optimize_function_for_speed_p (cfun)\n+    && dbg_cnt (pre);\n+}\n \n-  if (! MEM_P (dest) || MEM_VOLATILE_P (dest)\n-      || GET_MODE (dest) == BLKmode)\n-    return;\n+static unsigned int\n+execute_rtl_pre (void)\n+{\n+  delete_unreachable_blocks ();\n+  df_note_add_problem ();\n+  df_analyze ();\n+  flag_rerun_cse_after_global_opts |= one_pre_gcse_pass ();\n+  return 0;\n+}\n \n-  if (side_effects_p (dest))\n-    return;\n+static bool\n+gate_rtl_hoist (void)\n+{\n+  return optimize > 0 && flag_gcse\n+    && !cfun->calls_setjmp\n+    /* It does not make sense to run code hoisting unless we are optimizing\n+       for code size -- it rarely makes programs faster, and can make then\n+       bigger if we did PRE (when optimizing for space, we don't run PRE).  */\n+    && optimize_function_for_size_p (cfun)\n+    && dbg_cnt (hoist);\n+}\n \n-  /* If we are handling exceptions, we must be careful with memory references\n-     that may trap. If we are not, the behavior is undefined, so we may just\n-     continue.  */\n-  if (flag_non_call_exceptions && may_trap_p (dest))\n-    return;\n-\n-  /* Even if the destination cannot trap, the source may.  In this case we'd\n-     need to handle updating the REG_EH_REGION note.  */\n-  if (find_reg_note (insn, REG_EH_REGION, NULL_RTX))\n-    return;\n-\n-  /* Make sure that the SET_SRC of this store insns can be assigned to\n-     a register, or we will fail later on in replace_store_insn, which\n-     assumes that we can do this.  But sometimes the target machine has\n-     oddities like MEM read-modify-write instruction.  See for example\n-     PR24257.  */\n-  if (!can_assign_to_reg_p (SET_SRC (set)))\n-    return;\n-\n-  ptr = ldst_entry (dest);\n-  if (!ptr->pattern_regs)\n-    ptr->pattern_regs = extract_mentioned_regs (dest);\n-\n-  /* Do not check for anticipatability if we either found one anticipatable\n-     store already, or tested for one and found out that it was killed.  */\n-  check_anticipatable = 0;\n-  if (!ANTIC_STORE_LIST (ptr))\n-    check_anticipatable = 1;\n-  else\n-    {\n-      tmp = XEXP (ANTIC_STORE_LIST (ptr), 0);\n-      if (tmp != NULL_RTX\n-\t  && BLOCK_FOR_INSN (tmp) != bb)\n-\tcheck_anticipatable = 1;\n-    }\n-  if (check_anticipatable)\n-    {\n-      if (store_killed_before (dest, ptr->pattern_regs, insn, bb, regs_set_before))\n-\ttmp = NULL_RTX;\n-      else\n-\ttmp = insn;\n-      ANTIC_STORE_LIST (ptr) = alloc_INSN_LIST (tmp,\n-\t\t\t\t\t\tANTIC_STORE_LIST (ptr));\n-    }\n-\n-  /* It is not necessary to check whether store is available if we did\n-     it successfully before; if we failed before, do not bother to check\n-     until we reach the insn that caused us to fail.  */\n-  check_available = 0;\n-  if (!AVAIL_STORE_LIST (ptr))\n-    check_available = 1;\n-  else\n-    {\n-      tmp = XEXP (AVAIL_STORE_LIST (ptr), 0);\n-      if (BLOCK_FOR_INSN (tmp) != bb)\n-\tcheck_available = 1;\n-    }\n-  if (check_available)\n-    {\n-      /* Check that we have already reached the insn at that the check\n-\t failed last time.  */\n-      if (LAST_AVAIL_CHECK_FAILURE (ptr))\n-\t{\n-\t  for (tmp = BB_END (bb);\n-\t       tmp != insn && tmp != LAST_AVAIL_CHECK_FAILURE (ptr);\n-\t       tmp = PREV_INSN (tmp))\n-\t    continue;\n-\t  if (tmp == insn)\n-\t    check_available = 0;\n-\t}\n-      else\n-\tcheck_available = store_killed_after (dest, ptr->pattern_regs, insn,\n-\t\t\t\t\t      bb, regs_set_after,\n-\t\t\t\t\t      &LAST_AVAIL_CHECK_FAILURE (ptr));\n-    }\n-  if (!check_available)\n-    AVAIL_STORE_LIST (ptr) = alloc_INSN_LIST (insn, AVAIL_STORE_LIST (ptr));\n-}\n-\n-/* Find available and anticipatable stores.  */\n-\n-static int\n-compute_store_table (void)\n-{\n-  int ret;\n-  basic_block bb;\n-  unsigned regno;\n-  rtx insn, pat, tmp;\n-  int *last_set_in, *already_set;\n-  struct ls_expr * ptr, **prev_next_ptr_ptr;\n-  unsigned int max_gcse_regno = max_reg_num ();\n-\n-  pre_ldst_mems = 0;\n-  pre_ldst_table = htab_create (13, pre_ldst_expr_hash,\n-\t\t\t\tpre_ldst_expr_eq, NULL);\n-  last_set_in = XCNEWVEC (int, max_gcse_regno);\n-  already_set = XNEWVEC (int, max_gcse_regno);\n-\n-  /* Find all the stores we care about.  */\n-  FOR_EACH_BB (bb)\n-    {\n-      /* First compute the registers set in this block.  */\n-      regvec = last_set_in;\n-\n-      FOR_BB_INSNS (bb, insn)\n-\t{\n-\t  if (! INSN_P (insn))\n-\t    continue;\n-\n-\t  if (CALL_P (insn))\n-\t    {\n-\t      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n-\t\tif (TEST_HARD_REG_BIT (regs_invalidated_by_call, regno))\n-\t\t  last_set_in[regno] = INSN_UID (insn);\n-\t    }\n-\n-\t  pat = PATTERN (insn);\n-\t  compute_store_table_current_insn = insn;\n-\t  note_stores (pat, reg_set_info, NULL);\n-\t}\n-\n-      /* Now find the stores.  */\n-      memset (already_set, 0, sizeof (int) * max_gcse_regno);\n-      regvec = already_set;\n-      FOR_BB_INSNS (bb, insn)\n-\t{\n-\t  if (! INSN_P (insn))\n-\t    continue;\n-\n-\t  if (CALL_P (insn))\n-\t    {\n-\t      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n-\t\tif (TEST_HARD_REG_BIT (regs_invalidated_by_call, regno))\n-\t\t  already_set[regno] = 1;\n-\t    }\n-\n-\t  pat = PATTERN (insn);\n-\t  note_stores (pat, reg_set_info, NULL);\n-\n-\t  /* Now that we've marked regs, look for stores.  */\n-\t  find_moveable_store (insn, already_set, last_set_in);\n-\n-\t  /* Unmark regs that are no longer set.  */\n-\t  compute_store_table_current_insn = insn;\n-\t  note_stores (pat, reg_clear_last_set, last_set_in);\n-\t  if (CALL_P (insn))\n-\t    {\n-\t      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n-\t\tif (TEST_HARD_REG_BIT (regs_invalidated_by_call, regno)\n-\t\t    && last_set_in[regno] == INSN_UID (insn))\n-\t\t  last_set_in[regno] = 0;\n-\t    }\n-\t}\n-\n-#ifdef ENABLE_CHECKING\n-      /* last_set_in should now be all-zero.  */\n-      for (regno = 0; regno < max_gcse_regno; regno++)\n-\tgcc_assert (!last_set_in[regno]);\n-#endif\n-\n-      /* Clear temporary marks.  */\n-      for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n-\t{\n-\t  LAST_AVAIL_CHECK_FAILURE(ptr) = NULL_RTX;\n-\t  if (ANTIC_STORE_LIST (ptr)\n-\t      && (tmp = XEXP (ANTIC_STORE_LIST (ptr), 0)) == NULL_RTX)\n-\t    ANTIC_STORE_LIST (ptr) = XEXP (ANTIC_STORE_LIST (ptr), 1);\n-\t}\n-    }\n-\n-  /* Remove the stores that are not available anywhere, as there will\n-     be no opportunity to optimize them.  */\n-  for (ptr = pre_ldst_mems, prev_next_ptr_ptr = &pre_ldst_mems;\n-       ptr != NULL;\n-       ptr = *prev_next_ptr_ptr)\n-    {\n-      if (!AVAIL_STORE_LIST (ptr))\n-\t{\n-\t  *prev_next_ptr_ptr = ptr->next;\n-\t  htab_remove_elt_with_hash (pre_ldst_table, ptr, ptr->hash_index);\n-\t  free_ldst_entry (ptr);\n-\t}\n-      else\n-\tprev_next_ptr_ptr = &ptr->next;\n-    }\n-\n-  ret = enumerate_ldsts ();\n-\n-  if (dump_file)\n-    {\n-      fprintf (dump_file, \"ST_avail and ST_antic (shown under loads..)\\n\");\n-      print_ldst_list (dump_file);\n-    }\n-\n-  free (last_set_in);\n-  free (already_set);\n-  return ret;\n-}\n-\n-/* Check to see if the load X is aliased with STORE_PATTERN.\n-   AFTER is true if we are checking the case when STORE_PATTERN occurs\n-   after the X.  */\n-\n-static bool\n-load_kills_store (const_rtx x, const_rtx store_pattern, int after)\n-{\n-  if (after)\n-    return anti_dependence (x, store_pattern);\n-  else\n-    return true_dependence (store_pattern, GET_MODE (store_pattern), x,\n-\t\t\t    rtx_addr_varies_p);\n-}\n-\n-/* Go through the entire insn X, looking for any loads which might alias\n-   STORE_PATTERN.  Return true if found.\n-   AFTER is true if we are checking the case when STORE_PATTERN occurs\n-   after the insn X.  */\n-\n-static bool\n-find_loads (const_rtx x, const_rtx store_pattern, int after)\n-{\n-  const char * fmt;\n-  int i, j;\n-  int ret = false;\n-\n-  if (!x)\n-    return false;\n-\n-  if (GET_CODE (x) == SET)\n-    x = SET_SRC (x);\n-\n-  if (MEM_P (x))\n-    {\n-      if (load_kills_store (x, store_pattern, after))\n-\treturn true;\n-    }\n-\n-  /* Recursively process the insn.  */\n-  fmt = GET_RTX_FORMAT (GET_CODE (x));\n-\n-  for (i = GET_RTX_LENGTH (GET_CODE (x)) - 1; i >= 0 && !ret; i--)\n-    {\n-      if (fmt[i] == 'e')\n-\tret |= find_loads (XEXP (x, i), store_pattern, after);\n-      else if (fmt[i] == 'E')\n-\tfor (j = XVECLEN (x, i) - 1; j >= 0; j--)\n-\t  ret |= find_loads (XVECEXP (x, i, j), store_pattern, after);\n-    }\n-  return ret;\n-}\n-\n-static inline bool\n-store_killed_in_pat (const_rtx x, const_rtx pat, int after)\n-{\n-  if (GET_CODE (pat) == SET)\n-    {\n-      rtx dest = SET_DEST (pat);\n-\n-      if (GET_CODE (dest) == ZERO_EXTRACT)\n-\tdest = XEXP (dest, 0);\n-\n-      /* Check for memory stores to aliased objects.  */\n-      if (MEM_P (dest)\n-\t  && !expr_equiv_p (dest, x))\n-\t{\n-\t  if (after)\n-\t    {\n-\t      if (output_dependence (dest, x))\n-\t\treturn true;\n-\t    }\n-\t  else\n-\t    {\n-\t      if (output_dependence (x, dest))\n-\t\treturn true;\n-\t    }\n-\t}\n-    }\n-\n-  if (find_loads (pat, x, after))\n-    return true;\n-\n-  return false;\n-}\n-\n-/* Check if INSN kills the store pattern X (is aliased with it).\n-   AFTER is true if we are checking the case when store X occurs\n-   after the insn.  Return true if it does.  */\n-\n-static bool\n-store_killed_in_insn (const_rtx x, const_rtx x_regs, const_rtx insn, int after)\n-{\n-  const_rtx reg, base, note, pat;\n-\n-  if (!INSN_P (insn))\n-    return false;\n-\n-  if (CALL_P (insn))\n-    {\n-      /* A normal or pure call might read from pattern,\n-\t but a const call will not.  */\n-      if (!RTL_CONST_CALL_P (insn))\n-\treturn true;\n-\n-      /* But even a const call reads its parameters.  Check whether the\n-\t base of some of registers used in mem is stack pointer.  */\n-      for (reg = x_regs; reg; reg = XEXP (reg, 1))\n-\t{\n-\t  base = find_base_term (XEXP (reg, 0));\n-\t  if (!base\n-\t      || (GET_CODE (base) == ADDRESS\n-\t\t  && GET_MODE (base) == Pmode\n-\t\t  && XEXP (base, 0) == stack_pointer_rtx))\n-\t    return true;\n-\t}\n-\n-      return false;\n-    }\n-\n-  pat = PATTERN (insn);\n-  if (GET_CODE (pat) == SET)\n-    {\n-      if (store_killed_in_pat (x, pat, after))\n-\treturn true;\n-    }\n-  else if (GET_CODE (pat) == PARALLEL)\n-    {\n-      int i;\n-\n-      for (i = 0; i < XVECLEN (pat, 0); i++)\n-\tif (store_killed_in_pat (x, XVECEXP (pat, 0, i), after))\n-\t  return true;\n-    }\n-  else if (find_loads (PATTERN (insn), x, after))\n-    return true;\n-\n-  /* If this insn has a REG_EQUAL or REG_EQUIV note referencing a memory\n-     location aliased with X, then this insn kills X.  */\n-  note = find_reg_equal_equiv_note (insn);\n-  if (! note)\n-    return false;\n-  note = XEXP (note, 0);\n-\n-  /* However, if the note represents a must alias rather than a may\n-     alias relationship, then it does not kill X.  */\n-  if (expr_equiv_p (note, x))\n-    return false;\n-\n-  /* See if there are any aliased loads in the note.  */\n-  return find_loads (note, x, after);\n-}\n-\n-/* Returns true if the expression X is loaded or clobbered on or after INSN\n-   within basic block BB.  REGS_SET_AFTER is bitmap of registers set in\n-   or after the insn.  X_REGS is list of registers mentioned in X. If the store\n-   is killed, return the last insn in that it occurs in FAIL_INSN.  */\n-\n-static bool\n-store_killed_after (const_rtx x, const_rtx x_regs, const_rtx insn, const_basic_block bb,\n-\t\t    int *regs_set_after, rtx *fail_insn)\n-{\n-  rtx last = BB_END (bb), act;\n-\n-  if (!store_ops_ok (x_regs, regs_set_after))\n-    {\n-      /* We do not know where it will happen.  */\n-      if (fail_insn)\n-\t*fail_insn = NULL_RTX;\n-      return true;\n-    }\n-\n-  /* Scan from the end, so that fail_insn is determined correctly.  */\n-  for (act = last; act != PREV_INSN (insn); act = PREV_INSN (act))\n-    if (store_killed_in_insn (x, x_regs, act, false))\n-      {\n-\tif (fail_insn)\n-\t  *fail_insn = act;\n-\treturn true;\n-      }\n-\n-  return false;\n-}\n-\n-/* Returns true if the expression X is loaded or clobbered on or before INSN\n-   within basic block BB. X_REGS is list of registers mentioned in X.\n-   REGS_SET_BEFORE is bitmap of registers set before or in this insn.  */\n-static bool\n-store_killed_before (const_rtx x, const_rtx x_regs, const_rtx insn, const_basic_block bb,\n-\t\t     int *regs_set_before)\n-{\n-  rtx first = BB_HEAD (bb);\n-\n-  if (!store_ops_ok (x_regs, regs_set_before))\n-    return true;\n-\n-  for ( ; insn != PREV_INSN (first); insn = PREV_INSN (insn))\n-    if (store_killed_in_insn (x, x_regs, insn, true))\n-      return true;\n-\n-  return false;\n-}\n-\n-/* Fill in available, anticipatable, transparent and kill vectors in\n-   STORE_DATA, based on lists of available and anticipatable stores.  */\n-static void\n-build_store_vectors (void)\n-{\n-  basic_block bb;\n-  int *regs_set_in_block;\n-  rtx insn, st;\n-  struct ls_expr * ptr;\n-  unsigned int max_gcse_regno = max_reg_num ();\n-\n-  /* Build the gen_vector. This is any store in the table which is not killed\n-     by aliasing later in its block.  */\n-  ae_gen = sbitmap_vector_alloc (last_basic_block, num_stores);\n-  sbitmap_vector_zero (ae_gen, last_basic_block);\n-\n-  st_antloc = sbitmap_vector_alloc (last_basic_block, num_stores);\n-  sbitmap_vector_zero (st_antloc, last_basic_block);\n-\n-  for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n-    {\n-      for (st = AVAIL_STORE_LIST (ptr); st != NULL; st = XEXP (st, 1))\n-\t{\n-\t  insn = XEXP (st, 0);\n-\t  bb = BLOCK_FOR_INSN (insn);\n-\n-\t  /* If we've already seen an available expression in this block,\n-\t     we can delete this one (It occurs earlier in the block). We'll\n-\t     copy the SRC expression to an unused register in case there\n-\t     are any side effects.  */\n-\t  if (TEST_BIT (ae_gen[bb->index], ptr->index))\n-\t    {\n-\t      rtx r = gen_reg_rtx_and_attrs (ptr->pattern);\n-\t      if (dump_file)\n-\t\tfprintf (dump_file, \"Removing redundant store:\\n\");\n-\t      replace_store_insn (r, XEXP (st, 0), bb, ptr);\n-\t      continue;\n-\t    }\n-\t  SET_BIT (ae_gen[bb->index], ptr->index);\n-\t}\n-\n-      for (st = ANTIC_STORE_LIST (ptr); st != NULL; st = XEXP (st, 1))\n-\t{\n-\t  insn = XEXP (st, 0);\n-\t  bb = BLOCK_FOR_INSN (insn);\n-\t  SET_BIT (st_antloc[bb->index], ptr->index);\n-\t}\n-    }\n-\n-  ae_kill = sbitmap_vector_alloc (last_basic_block, num_stores);\n-  sbitmap_vector_zero (ae_kill, last_basic_block);\n-\n-  transp = sbitmap_vector_alloc (last_basic_block, num_stores);\n-  sbitmap_vector_zero (transp, last_basic_block);\n-  regs_set_in_block = XNEWVEC (int, max_gcse_regno);\n-\n-  FOR_EACH_BB (bb)\n-    {\n-      FOR_BB_INSNS (bb, insn)\n-\tif (INSN_P (insn))\n-\t  {\n-\t    df_ref *def_rec;\n-\t    for (def_rec = DF_INSN_DEFS (insn); *def_rec; def_rec++)\n-\t      {\n-\t\tunsigned int ref_regno = DF_REF_REGNO (*def_rec);\n-\t\tif (ref_regno < max_gcse_regno)\n-\t\t  regs_set_in_block[DF_REF_REGNO (*def_rec)] = 1;\n-\t      }\n-\t  }\n-\n-      for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n-\t{\n-\t  if (store_killed_after (ptr->pattern, ptr->pattern_regs, BB_HEAD (bb),\n-\t\t\t\t  bb, regs_set_in_block, NULL))\n-\t    {\n-\t      /* It should not be necessary to consider the expression\n-\t\t killed if it is both anticipatable and available.  */\n-\t      if (!TEST_BIT (st_antloc[bb->index], ptr->index)\n-\t\t  || !TEST_BIT (ae_gen[bb->index], ptr->index))\n-\t\tSET_BIT (ae_kill[bb->index], ptr->index);\n-\t    }\n-\t  else\n-\t    SET_BIT (transp[bb->index], ptr->index);\n-\t}\n-    }\n-\n-  free (regs_set_in_block);\n-\n-  if (dump_file)\n-    {\n-      dump_sbitmap_vector (dump_file, \"st_antloc\", \"\", st_antloc, last_basic_block);\n-      dump_sbitmap_vector (dump_file, \"st_kill\", \"\", ae_kill, last_basic_block);\n-      dump_sbitmap_vector (dump_file, \"Transpt\", \"\", transp, last_basic_block);\n-      dump_sbitmap_vector (dump_file, \"st_avloc\", \"\", ae_gen, last_basic_block);\n-    }\n-}\n-\n-/* Insert an instruction at the beginning of a basic block, and update\n-   the BB_HEAD if needed.  */\n-\n-static void\n-insert_insn_start_basic_block (rtx insn, basic_block bb)\n-{\n-  /* Insert at start of successor block.  */\n-  rtx prev = PREV_INSN (BB_HEAD (bb));\n-  rtx before = BB_HEAD (bb);\n-  while (before != 0)\n-    {\n-      if (! LABEL_P (before)\n-\t  && !NOTE_INSN_BASIC_BLOCK_P (before))\n-\tbreak;\n-      prev = before;\n-      if (prev == BB_END (bb))\n-\tbreak;\n-      before = NEXT_INSN (before);\n-    }\n-\n-  insn = emit_insn_after_noloc (insn, prev, bb);\n-\n-  if (dump_file)\n-    {\n-      fprintf (dump_file, \"STORE_MOTION  insert store at start of BB %d:\\n\",\n-\t       bb->index);\n-      print_inline_rtx (dump_file, insn, 6);\n-      fprintf (dump_file, \"\\n\");\n-    }\n-}\n-\n-/* This routine will insert a store on an edge. EXPR is the ldst entry for\n-   the memory reference, and E is the edge to insert it on.  Returns nonzero\n-   if an edge insertion was performed.  */\n-\n-static int\n-insert_store (struct ls_expr * expr, edge e)\n-{\n-  rtx reg, insn;\n-  basic_block bb;\n-  edge tmp;\n-  edge_iterator ei;\n-\n-  /* We did all the deleted before this insert, so if we didn't delete a\n-     store, then we haven't set the reaching reg yet either.  */\n-  if (expr->reaching_reg == NULL_RTX)\n-    return 0;\n-\n-  if (e->flags & EDGE_FAKE)\n-    return 0;\n-\n-  reg = expr->reaching_reg;\n-  insn = gen_move_insn (copy_rtx (expr->pattern), reg);\n-\n-  /* If we are inserting this expression on ALL predecessor edges of a BB,\n-     insert it at the start of the BB, and reset the insert bits on the other\n-     edges so we don't try to insert it on the other edges.  */\n-  bb = e->dest;\n-  FOR_EACH_EDGE (tmp, ei, e->dest->preds)\n-    if (!(tmp->flags & EDGE_FAKE))\n-      {\n-\tint index = EDGE_INDEX (edge_list, tmp->src, tmp->dest);\n-\t\n-\tgcc_assert (index != EDGE_INDEX_NO_EDGE);\n-\tif (! TEST_BIT (pre_insert_map[index], expr->index))\n-\t  break;\n-      }\n-\n-  /* If tmp is NULL, we found an insertion on every edge, blank the\n-     insertion vector for these edges, and insert at the start of the BB.  */\n-  if (!tmp && bb != EXIT_BLOCK_PTR)\n-    {\n-      FOR_EACH_EDGE (tmp, ei, e->dest->preds)\n-\t{\n-\t  int index = EDGE_INDEX (edge_list, tmp->src, tmp->dest);\n-\t  RESET_BIT (pre_insert_map[index], expr->index);\n-\t}\n-      insert_insn_start_basic_block (insn, bb);\n-      return 0;\n-    }\n-\n-  /* We can't put stores in the front of blocks pointed to by abnormal\n-     edges since that may put a store where one didn't used to be.  */\n-  gcc_assert (!(e->flags & EDGE_ABNORMAL));\n-\n-  insert_insn_on_edge (insn, e);\n-\n-  if (dump_file)\n-    {\n-      fprintf (dump_file, \"STORE_MOTION  insert insn on edge (%d, %d):\\n\",\n-\t       e->src->index, e->dest->index);\n-      print_inline_rtx (dump_file, insn, 6);\n-      fprintf (dump_file, \"\\n\");\n-    }\n-\n-  return 1;\n-}\n-\n-/* Remove any REG_EQUAL or REG_EQUIV notes containing a reference to the\n-   memory location in SMEXPR set in basic block BB.\n-\n-   This could be rather expensive.  */\n-\n-static void\n-remove_reachable_equiv_notes (basic_block bb, struct ls_expr *smexpr)\n-{\n-  edge_iterator *stack, ei;\n-  int sp;\n-  edge act;\n-  sbitmap visited = sbitmap_alloc (last_basic_block);\n-  rtx last, insn, note;\n-  rtx mem = smexpr->pattern;\n-\n-  stack = XNEWVEC (edge_iterator, n_basic_blocks);\n-  sp = 0;\n-  ei = ei_start (bb->succs);\n-\n-  sbitmap_zero (visited);\n-\n-  act = (EDGE_COUNT (ei_container (ei)) > 0 ? EDGE_I (ei_container (ei), 0) : NULL);\n-  while (1)\n-    {\n-      if (!act)\n-\t{\n-\t  if (!sp)\n-\t    {\n-\t      free (stack);\n-\t      sbitmap_free (visited);\n-\t      return;\n-\t    }\n-\t  act = ei_edge (stack[--sp]);\n-\t}\n-      bb = act->dest;\n-\n-      if (bb == EXIT_BLOCK_PTR\n-\t  || TEST_BIT (visited, bb->index))\n-\t{\n-\t  if (!ei_end_p (ei))\n-\t      ei_next (&ei);\n-\t  act = (! ei_end_p (ei)) ? ei_edge (ei) : NULL;\n-\t  continue;\n-\t}\n-      SET_BIT (visited, bb->index);\n-\n-      if (TEST_BIT (st_antloc[bb->index], smexpr->index))\n-\t{\n-\t  for (last = ANTIC_STORE_LIST (smexpr);\n-\t       BLOCK_FOR_INSN (XEXP (last, 0)) != bb;\n-\t       last = XEXP (last, 1))\n-\t    continue;\n-\t  last = XEXP (last, 0);\n-\t}\n-      else\n-\tlast = NEXT_INSN (BB_END (bb));\n-\n-      for (insn = BB_HEAD (bb); insn != last; insn = NEXT_INSN (insn))\n-\tif (INSN_P (insn))\n-\t  {\n-\t    note = find_reg_equal_equiv_note (insn);\n-\t    if (!note || !expr_equiv_p (XEXP (note, 0), mem))\n-\t      continue;\n-\n-\t    if (dump_file)\n-\t      fprintf (dump_file, \"STORE_MOTION  drop REG_EQUAL note at insn %d:\\n\",\n-\t\t       INSN_UID (insn));\n-\t    remove_note (insn, note);\n-\t  }\n-\n-      if (!ei_end_p (ei))\n-\tei_next (&ei);\n-      act = (! ei_end_p (ei)) ? ei_edge (ei) : NULL;\n-\n-      if (EDGE_COUNT (bb->succs) > 0)\n-\t{\n-\t  if (act)\n-\t    stack[sp++] = ei;\n-\t  ei = ei_start (bb->succs);\n-\t  act = (EDGE_COUNT (ei_container (ei)) > 0 ? EDGE_I (ei_container (ei), 0) : NULL);\n-\t}\n-    }\n-}\n-\n-/* This routine will replace a store with a SET to a specified register.  */\n-\n-static void\n-replace_store_insn (rtx reg, rtx del, basic_block bb, struct ls_expr *smexpr)\n-{\n-  rtx insn, mem, note, set, ptr;\n-\n-  mem = smexpr->pattern;\n-  insn = gen_move_insn (reg, SET_SRC (single_set (del)));\n-\n-  for (ptr = ANTIC_STORE_LIST (smexpr); ptr; ptr = XEXP (ptr, 1))\n-    if (XEXP (ptr, 0) == del)\n-      {\n-\tXEXP (ptr, 0) = insn;\n-\tbreak;\n-      }\n-\n-  /* Move the notes from the deleted insn to its replacement.  */\n-  REG_NOTES (insn) = REG_NOTES (del);\n-\n-  /* Emit the insn AFTER all the notes are transferred.\n-     This is cheaper since we avoid df rescanning for the note change.  */\n-  insn = emit_insn_after (insn, del);\n-\n-  if (dump_file)\n-    {\n-      fprintf (dump_file,\n-\t       \"STORE_MOTION  delete insn in BB %d:\\n      \", bb->index);\n-      print_inline_rtx (dump_file, del, 6);\n-      fprintf (dump_file, \"\\nSTORE_MOTION  replaced with insn:\\n      \");\n-      print_inline_rtx (dump_file, insn, 6);\n-      fprintf (dump_file, \"\\n\");\n-    }\n-\n-  delete_insn (del);\n-\n-  /* Now we must handle REG_EQUAL notes whose contents is equal to the mem;\n-     they are no longer accurate provided that they are reached by this\n-     definition, so drop them.  */\n-  for (; insn != NEXT_INSN (BB_END (bb)); insn = NEXT_INSN (insn))\n-    if (INSN_P (insn))\n-      {\n-\tset = single_set (insn);\n-\tif (!set)\n-\t  continue;\n-\tif (expr_equiv_p (SET_DEST (set), mem))\n-\t  return;\n-\tnote = find_reg_equal_equiv_note (insn);\n-\tif (!note || !expr_equiv_p (XEXP (note, 0), mem))\n-\t  continue;\n-\n-\tif (dump_file)\n-\t  fprintf (dump_file, \"STORE_MOTION  drop REG_EQUAL note at insn %d:\\n\",\n-\t\t   INSN_UID (insn));\n-\tremove_note (insn, note);\n-      }\n-  remove_reachable_equiv_notes (bb, smexpr);\n-}\n-\n-\n-/* Delete a store, but copy the value that would have been stored into\n-   the reaching_reg for later storing.  */\n-\n-static void\n-delete_store (struct ls_expr * expr, basic_block bb)\n-{\n-  rtx reg, i, del;\n-\n-  if (expr->reaching_reg == NULL_RTX)\n-    expr->reaching_reg = gen_reg_rtx_and_attrs (expr->pattern);\n-\n-  reg = expr->reaching_reg;\n-\n-  for (i = AVAIL_STORE_LIST (expr); i; i = XEXP (i, 1))\n-    {\n-      del = XEXP (i, 0);\n-      if (BLOCK_FOR_INSN (del) == bb)\n-\t{\n-\t  /* We know there is only one since we deleted redundant\n-\t     ones during the available computation.  */\n-\t  replace_store_insn (reg, del, bb, expr);\n-\t  break;\n-\t}\n-    }\n-}\n-\n-/* Free memory used by store motion.  */\n-\n-static void\n-free_store_memory (void)\n-{\n-  free_ldst_mems ();\n-\n-  if (ae_gen)\n-    sbitmap_vector_free (ae_gen);\n-  if (ae_kill)\n-    sbitmap_vector_free (ae_kill);\n-  if (transp)\n-    sbitmap_vector_free (transp);\n-  if (st_antloc)\n-    sbitmap_vector_free (st_antloc);\n-  if (pre_insert_map)\n-    sbitmap_vector_free (pre_insert_map);\n-  if (pre_delete_map)\n-    sbitmap_vector_free (pre_delete_map);\n-\n-  ae_gen = ae_kill = transp = st_antloc = NULL;\n-  pre_insert_map = pre_delete_map = NULL;\n-}\n-\n-/* Perform store motion. Much like gcse, except we move expressions the\n-   other way by looking at the flowgraph in reverse.\n-   Return non-zero if transformations are performed by the pass.  */\n-\n-static int\n-one_store_motion_pass (void)\n-{\n-  basic_block bb;\n-  int x;\n-  struct ls_expr * ptr;\n-  int update_flow = 0;\n-\n-  gcse_subst_count = 0;\n-  gcse_create_count = 0;\n-\n-  init_alias_analysis ();\n-\n-  /* Find all the available and anticipatable stores.  */\n-  num_stores = compute_store_table ();\n-  if (num_stores == 0)\n-    {\n-      htab_delete (pre_ldst_table);\n-      pre_ldst_table = NULL;\n-      end_alias_analysis ();\n-      return 0;\n-    }\n-\n-  /* Now compute kill & transp vectors.  */\n-  build_store_vectors ();\n-  add_noreturn_fake_exit_edges ();\n-  connect_infinite_loops_to_exit ();\n-\n-  edge_list = pre_edge_rev_lcm (num_stores, transp, ae_gen,\n-\t\t\t\tst_antloc, ae_kill, &pre_insert_map,\n-\t\t\t\t&pre_delete_map);\n-\n-  /* Now we want to insert the new stores which are going to be needed.  */\n-  for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n-    {\n-      /* If any of the edges we have above are abnormal, we can't move this\n-\t store.  */\n-      for (x = NUM_EDGES (edge_list) - 1; x >= 0; x--)\n-\tif (TEST_BIT (pre_insert_map[x], ptr->index)\n-\t    && (INDEX_EDGE (edge_list, x)->flags & EDGE_ABNORMAL))\n-\t  break;\n-\n-      if (x >= 0)\n-\t{\n-\t  if (dump_file != NULL)\n-\t    fprintf (dump_file,\n-\t\t     \"Can't replace store %d: abnormal edge from %d to %d\\n\",\n-\t\t     ptr->index, INDEX_EDGE (edge_list, x)->src->index,\n-\t\t     INDEX_EDGE (edge_list, x)->dest->index);\n-\t  continue;\n-\t}\n-\t\t      \n-      /* Now we want to insert the new stores which are going to be needed.  */\n-\n-      FOR_EACH_BB (bb)\n-\tif (TEST_BIT (pre_delete_map[bb->index], ptr->index))\n-\t  {\n-\t    delete_store (ptr, bb);\n-\t    gcse_subst_count++;\n-\t  }\n-\n-      for (x = 0; x < NUM_EDGES (edge_list); x++)\n-\tif (TEST_BIT (pre_insert_map[x], ptr->index))\n-\t  {\n-\t    update_flow |= insert_store (ptr, INDEX_EDGE (edge_list, x));\n-\t    gcse_create_count++;\n-\t  }\n-    }\n-\n-  if (update_flow)\n-    commit_edge_insertions ();\n-\n-  free_store_memory ();\n-  free_edge_list (edge_list);\n-  remove_fake_exit_edges ();\n-  end_alias_analysis ();\n-\n-  if (dump_file)\n-    {\n-      fprintf (dump_file, \"STORE_MOTION of %s, %d basic blocks, \",\n-\t       current_function_name (), n_basic_blocks);\n-      fprintf (dump_file, \"%d substs, %d insns created\\n\",\n-\t       gcse_subst_count, gcse_create_count);\n-    }\n-\n-  return (gcse_subst_count > 0 || gcse_create_count > 0);\n-}\n-\n-\f\n-/* Return true if the graph is too expensive to optimize. PASS is the\n-   optimization about to be performed.  */\n-\n-static bool\n-is_too_expensive (const char *pass)\n-{\n-  /* Trying to perform global optimizations on flow graphs which have\n-     a high connectivity will take a long time and is unlikely to be\n-     particularly useful.\n-\n-     In normal circumstances a cfg should have about twice as many\n-     edges as blocks.  But we do not want to punish small functions\n-     which have a couple switch statements.  Rather than simply\n-     threshold the number of blocks, uses something with a more\n-     graceful degradation.  */\n-  if (n_edges > 20000 + n_basic_blocks * 4)\n-    {\n-      warning (OPT_Wdisabled_optimization,\n-\t       \"%s: %d basic blocks and %d edges/basic block\",\n-\t       pass, n_basic_blocks, n_edges / n_basic_blocks);\n-\n-      return true;\n-    }\n-\n-  /* If allocating memory for the cprop bitmap would take up too much\n-     storage it's better just to disable the optimization.  */\n-  if ((n_basic_blocks\n-       * SBITMAP_SET_SIZE (max_reg_num ())\n-       * sizeof (SBITMAP_ELT_TYPE)) > MAX_GCSE_MEMORY)\n-    {\n-      warning (OPT_Wdisabled_optimization,\n-\t       \"%s: %d basic blocks and %d registers\",\n-\t       pass, n_basic_blocks, max_reg_num ());\n-\n-      return true;\n-    }\n-\n-  return false;\n-}\n-\n-\f\n-/* Main function for the CPROP pass.  */\n-\n-static int\n-one_cprop_pass (void)\n-{\n-  int changed = 0;\n-\n-  /* Return if there's nothing to do, or it is too expensive.  */\n-  if (n_basic_blocks <= NUM_FIXED_BLOCKS + 1\n-      || is_too_expensive (_ (\"const/copy propagation disabled\")))\n-    return 0;\n-\n-  global_const_prop_count = local_const_prop_count = 0;\n-  global_copy_prop_count = local_copy_prop_count = 0;\n-\n-  bytes_used = 0;\n-  gcc_obstack_init (&gcse_obstack);\n-  alloc_gcse_mem ();\n-\n-  /* Do a local const/copy propagation pass first.  The global pass\n-     only handles global opportunities.\n-     If the local pass changes something, remove any unreachable blocks\n-     because the CPROP global dataflow analysis may get into infinite\n-     loops for CFGs with unreachable blocks.\n-\n-     FIXME: This local pass should not be necessary after CSE (but for\n-\t    some reason it still is).  It is also (proven) not necessary\n-\t    to run the local pass right after FWPWOP.\n-\t    \n-     FIXME: The global analysis would not get into infinite loops if it\n-\t    would use the DF solver (via df_simple_dataflow) instead of\n-\t    the solver implemented in this file.  */\n-  if (local_cprop_pass ())\n-    {\n-      delete_unreachable_blocks ();\n-      df_analyze ();\n-    }\n-\n-  /* Determine implicit sets.  */\n-  implicit_sets = XCNEWVEC (rtx, last_basic_block);\n-  find_implicit_sets ();\n-\n-  alloc_hash_table (get_max_uid (), &set_hash_table, 1);\n-  compute_hash_table (&set_hash_table);\n-\n-  /* Free implicit_sets before peak usage.  */\n-  free (implicit_sets);\n-  implicit_sets = NULL;\n-\n-  if (dump_file)\n-    dump_hash_table (dump_file, \"SET\", &set_hash_table);\n-  if (set_hash_table.n_elems > 0)\n-    {\n-      basic_block bb;\n-      rtx insn;\n-\n-      alloc_cprop_mem (last_basic_block, set_hash_table.n_elems);\n-      compute_cprop_data ();\n-\n-      FOR_BB_BETWEEN (bb, ENTRY_BLOCK_PTR->next_bb->next_bb, EXIT_BLOCK_PTR, next_bb)\n-\t{\n-\t  /* Reset tables used to keep track of what's still valid [since\n-\t     the start of the block].  */\n-\t  reset_opr_set_tables ();\n-\n-\t  FOR_BB_INSNS (bb, insn)\n-\t    if (INSN_P (insn))\n-\t      {\n-\t\tchanged |= cprop_insn (insn);\n-\n-\t\t/* Keep track of everything modified by this insn.  */\n-\t\t/* ??? Need to be careful w.r.t. mods done to INSN.\n-\t\t       Don't call mark_oprs_set if we turned the\n-\t\t       insn into a NOTE.  */\n-\t\tif (! NOTE_P (insn))\n-\t\t  mark_oprs_set (insn);\n-\t      }\n-\t}\n-\n-      changed |= bypass_conditional_jumps ();\n-      free_cprop_mem ();\n-    }\n-\n-  free_hash_table (&set_hash_table);\n-  free_gcse_mem ();\n-  obstack_free (&gcse_obstack, NULL);\n-\n-  if (dump_file)\n-    {\n-      fprintf (dump_file, \"CPROP of %s, %d basic blocks, %d bytes needed, \",\n-\t       current_function_name (), n_basic_blocks, bytes_used);\n-      fprintf (dump_file, \"%d local const props, %d local copy props, \",\n-\t       local_const_prop_count, local_copy_prop_count);\n-      fprintf (dump_file, \"%d global const props, %d global copy props\\n\\n\",\n-\t       global_const_prop_count, global_copy_prop_count);\n-    }\n-\n-  return changed;\n-}\n-\n-\f\n-/* All the passes implemented in this file.  Each pass has its\n-   own gate and execute function, and at the end of the file a\n-   pass definition for passes.c.\n-\n-   We do not construct an accurate cfg in functions which call\n-   setjmp, so none of these passes runs if the function calls\n-   setjmp.\n-   FIXME: Should just handle setjmp via REG_SETJMP notes.  */\n-\n-static bool\n-gate_rtl_cprop (void)\n-{\n-  return optimize > 0 && flag_gcse\n-    && !cfun->calls_setjmp\n-    && dbg_cnt (cprop);\n-}\n-\n-static unsigned int\n-execute_rtl_cprop (void)\n-{\n-  delete_unreachable_blocks ();\n-  df_note_add_problem ();\n-  df_set_flags (DF_LR_RUN_DCE);\n-  df_analyze ();\n-  flag_rerun_cse_after_global_opts |= one_cprop_pass ();\n-  return 0;\n-}\n-\n-static bool\n-gate_rtl_pre (void)\n-{\n-  return optimize > 0 && flag_gcse\n-    && !cfun->calls_setjmp\n-    && optimize_function_for_speed_p (cfun)\n-    && dbg_cnt (pre);\n-}\n-\n-static unsigned int\n-execute_rtl_pre (void)\n-{\n-  delete_unreachable_blocks ();\n-  df_note_add_problem ();\n-  df_analyze ();\n-  flag_rerun_cse_after_global_opts |= one_pre_gcse_pass ();\n-  return 0;\n-}\n-\n-static bool\n-gate_rtl_hoist (void)\n-{\n-  return optimize > 0 && flag_gcse\n-    && !cfun->calls_setjmp\n-    /* It does not make sense to run code hoisting unless we are optimizing\n-       for code size -- it rarely makes programs faster, and can make then\n-       bigger if we did PRE (when optimizing for space, we don't run PRE).  */\n-    && optimize_function_for_size_p (cfun)\n-    && dbg_cnt (hoist);\n-}\n-\n-static unsigned int\n-execute_rtl_hoist (void)\n-{\n-  delete_unreachable_blocks ();\n-  df_note_add_problem ();\n-  df_analyze ();\n-  flag_rerun_cse_after_global_opts |= one_code_hoisting_pass ();\n-  return 0;\n-}\n-\n-static bool\n-gate_rtl_store_motion (void)\n-{\n-  return optimize > 0 && flag_gcse_sm\n-    && !cfun->calls_setjmp\n-    && optimize_function_for_speed_p (cfun)\n-    && dbg_cnt (store_motion);\n-}\n-\n-static unsigned int\n-execute_rtl_store_motion (void)\n-{\n-  delete_unreachable_blocks ();\n-  df_note_add_problem ();\n-  df_analyze ();\n-  flag_rerun_cse_after_global_opts |= one_store_motion_pass ();\n-  return 0;\n-}\n+static unsigned int\n+execute_rtl_hoist (void)\n+{\n+  delete_unreachable_blocks ();\n+  df_note_add_problem ();\n+  df_analyze ();\n+  flag_rerun_cse_after_global_opts |= one_code_hoisting_pass ();\n+  return 0;\n+}\n \n struct rtl_opt_pass pass_rtl_cprop =\n {\n@@ -6296,25 +5163,4 @@ struct rtl_opt_pass pass_rtl_hoist =\n  }\n };\n \n-struct rtl_opt_pass pass_rtl_store_motion =\n-{\n- {\n-  RTL_PASS,\n-  \"store_motion\",                       /* name */\n-  gate_rtl_store_motion,                /* gate */   \n-  execute_rtl_store_motion,\t\t/* execute */       \n-  NULL,                                 /* sub */\n-  NULL,                                 /* next */\n-  0,                                    /* static_pass_number */\n-  TV_LSM,                               /* tv_id */\n-  PROP_cfglayout,                       /* properties_required */\n-  0,                                    /* properties_provided */\n-  0,                                    /* properties_destroyed */\n-  0,                                    /* todo_flags_start */\n-  TODO_df_finish | TODO_verify_rtl_sharing |\n-  TODO_dump_func |\n-  TODO_verify_flow | TODO_ggc_collect   /* todo_flags_finish */\n- }\n-};\n-\n #include \"gt-gcse.h\""}, {"sha": "1282b909cf7a00cbbdb574ab9ff66ac5dab18f45", "filename": "gcc/rtl.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/df35c271df60646a09af5279506c76c676a83217/gcc%2Frtl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/df35c271df60646a09af5279506c76c676a83217/gcc%2Frtl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtl.h?ref=df35c271df60646a09af5279506c76c676a83217", "patch": "@@ -2222,6 +2222,7 @@ extern void expand_dec (rtx, rtx);\n \n /* In gcse.c */\n extern bool can_copy_p (enum machine_mode);\n+extern bool can_assign_to_reg_without_clobbers_p (rtx);\n extern rtx fis_get_condition (rtx);\n \n /* In ira.c */"}, {"sha": "da614cc944a6850134d7f6d49212cca899e94123", "filename": "gcc/store-motion.c", "status": "added", "additions": 1405, "deletions": 0, "changes": 1405, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/df35c271df60646a09af5279506c76c676a83217/gcc%2Fstore-motion.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/df35c271df60646a09af5279506c76c676a83217/gcc%2Fstore-motion.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fstore-motion.c?ref=df35c271df60646a09af5279506c76c676a83217", "patch": "@@ -0,0 +1,1405 @@\n+/* Store motion via Lazy Code Motion on the reverse CFG.\n+   Copyright (C) 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005,\n+   2006, 2007, 2008, 2009 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"toplev.h\"\n+\n+#include \"rtl.h\"\n+#include \"tree.h\"\n+#include \"tm_p.h\"\n+#include \"regs.h\"\n+#include \"hard-reg-set.h\"\n+#include \"flags.h\"\n+#include \"real.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"basic-block.h\"\n+#include \"output.h\"\n+#include \"function.h\"\n+#include \"expr.h\"\n+#include \"except.h\"\n+#include \"ggc.h\"\n+#include \"params.h\"\n+#include \"intl.h\"\n+#include \"timevar.h\"\n+#include \"tree-pass.h\"\n+#include \"hashtab.h\"\n+#include \"df.h\"\n+#include \"dbgcnt.h\"\n+\n+\f\n+/* This is a list of expressions which are MEMs and will be used by load\n+   or store motion.\n+   Load motion tracks MEMs which aren't killed by\n+   anything except itself. (i.e., loads and stores to a single location).\n+   We can then allow movement of these MEM refs with a little special\n+   allowance. (all stores copy the same value to the reaching reg used\n+   for the loads).  This means all values used to store into memory must have\n+   no side effects so we can re-issue the setter value.\n+   Store Motion uses this structure as an expression table to track stores\n+   which look interesting, and might be moveable towards the exit block.  */\n+\n+struct ls_expr\n+{\n+  rtx pattern;\t\t\t/* Pattern of this mem.  */\n+  rtx pattern_regs;\t\t/* List of registers mentioned by the mem.  */\n+  rtx loads;\t\t\t/* INSN list of loads seen.  */\n+  rtx stores;\t\t\t/* INSN list of stores seen.  */\n+  struct ls_expr * next;\t/* Next in the list.  */\n+  int invalid;\t\t\t/* Invalid for some reason.  */\n+  int index;\t\t\t/* If it maps to a bitmap index.  */\n+  unsigned int hash_index;\t/* Index when in a hash table.  */\n+  rtx reaching_reg;\t\t/* Register to use when re-writing.  */\n+};\n+\n+/* Head of the list of load/store memory refs.  */\n+static struct ls_expr * pre_ldst_mems = NULL;\n+\n+/* Hashtable for the load/store memory refs.  */\n+static htab_t pre_ldst_table = NULL;\n+\n+/* Various variables for statistics gathering.  */\n+\n+/* GCSE substitutions made.  */\n+static int gcse_subst_count;\n+/* Number of copy instructions created.  */\n+static int gcse_create_count;\n+/* For available exprs */\n+static sbitmap *ae_kill, *ae_gen;\n+\f\n+/* Nonzero for expressions that are transparent in the block.  */\n+static sbitmap *transp;\n+\n+/* Nonzero for expressions which should be inserted on a specific edge.  */\n+static sbitmap *pre_insert_map;\n+\n+/* Nonzero for expressions which should be deleted in a specific block.  */\n+static sbitmap *pre_delete_map;\n+\n+/* Contains the edge_list returned by pre_edge_lcm.  */\n+static struct edge_list *edge_list;\n+\n+/*  Here we provide the things required to do store motion towards\n+    the exit. In order for this to be effective, PRE load motion also needed\n+    to be taught how to move a load when it is kill only by a store to itself.\n+\n+\t    int i;\n+\t    float a[10];\n+\n+\t    void foo(float scale)\n+\t    {\n+\t      for (i=0; i<10; i++)\n+\t\ta[i] *= scale;\n+\t    }\n+\n+    'i' is both loaded and stored to in the loop. Normally, gcse cannot move\n+    the load out since its live around the loop, and stored at the bottom\n+    of the loop.\n+\n+      The 'Load Motion' referred to and implemented in this file is\n+    an enhancement to gcse which when using edge based lcm, recognizes\n+    this situation and allows gcse to move the load out of the loop.\n+\n+      Once gcse has hoisted the load, store motion can then push this\n+    load towards the exit, and we end up with no loads or stores of 'i'\n+    in the loop.  */\n+\n+static hashval_t\n+pre_ldst_expr_hash (const void *p)\n+{\n+  int do_not_record_p = 0;\n+  const struct ls_expr *const x = (const struct ls_expr *) p;\n+  return hash_rtx (x->pattern, GET_MODE (x->pattern), &do_not_record_p, NULL, false);\n+}\n+\n+static int\n+pre_ldst_expr_eq (const void *p1, const void *p2)\n+{\n+  const struct ls_expr *const ptr1 = (const struct ls_expr *) p1,\n+    *const ptr2 = (const struct ls_expr *) p2;\n+  return exp_equiv_p (ptr1->pattern, ptr2->pattern, 0, true);\n+}\n+\n+/* This will search the ldst list for a matching expression. If it\n+   doesn't find one, we create one and initialize it.  */\n+\n+static struct ls_expr *\n+ldst_entry (rtx x)\n+{\n+  int do_not_record_p = 0;\n+  struct ls_expr * ptr;\n+  unsigned int hash;\n+  void **slot;\n+  struct ls_expr e;\n+\n+  hash = hash_rtx (x, GET_MODE (x), &do_not_record_p,\n+\t\t   NULL,  /*have_reg_qty=*/false);\n+\n+  e.pattern = x;\n+  slot = htab_find_slot_with_hash (pre_ldst_table, &e, hash, INSERT);\n+  if (*slot)\n+    return (struct ls_expr *)*slot;\n+\n+  ptr = XNEW (struct ls_expr);\n+\n+  ptr->next         = pre_ldst_mems;\n+  ptr->pattern      = x;\n+  ptr->pattern_regs = NULL_RTX;\n+  ptr->loads        = NULL_RTX;\n+  ptr->stores       = NULL_RTX;\n+  ptr->reaching_reg = NULL_RTX;\n+  ptr->invalid      = 0;\n+  ptr->index        = 0;\n+  ptr->hash_index   = hash;\n+  pre_ldst_mems     = ptr;\n+  *slot = ptr;\n+\n+  return ptr;\n+}\n+\n+/* Free up an individual ldst entry.  */\n+\n+static void\n+free_ldst_entry (struct ls_expr * ptr)\n+{\n+  free_INSN_LIST_list (& ptr->loads);\n+  free_INSN_LIST_list (& ptr->stores);\n+\n+  free (ptr);\n+}\n+\n+/* Free up all memory associated with the ldst list.  */\n+\n+static void\n+free_ldst_mems (void)\n+{\n+  if (pre_ldst_table)\n+    htab_delete (pre_ldst_table);\n+  pre_ldst_table = NULL;\n+\n+  while (pre_ldst_mems)\n+    {\n+      struct ls_expr * tmp = pre_ldst_mems;\n+\n+      pre_ldst_mems = pre_ldst_mems->next;\n+\n+      free_ldst_entry (tmp);\n+    }\n+\n+  pre_ldst_mems = NULL;\n+}\n+\n+/* Assign each element of the list of mems a monotonically increasing value.  */\n+\n+static int\n+enumerate_ldsts (void)\n+{\n+  struct ls_expr * ptr;\n+  int n = 0;\n+\n+  for (ptr = pre_ldst_mems; ptr != NULL; ptr = ptr->next)\n+    ptr->index = n++;\n+\n+  return n;\n+}\n+\n+/* Return first item in the list.  */\n+\n+static inline struct ls_expr *\n+first_ls_expr (void)\n+{\n+  return pre_ldst_mems;\n+}\n+\n+/* Return the next item in the list after the specified one.  */\n+\n+static inline struct ls_expr *\n+next_ls_expr (struct ls_expr * ptr)\n+{\n+  return ptr->next;\n+}\n+\n+/* Dump debugging info about the ldst list.  */\n+\n+static void\n+print_ldst_list (FILE * file)\n+{\n+  struct ls_expr * ptr;\n+\n+  fprintf (file, \"LDST list: \\n\");\n+\n+  for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n+    {\n+      fprintf (file, \"  Pattern (%3d): \", ptr->index);\n+\n+      print_rtl (file, ptr->pattern);\n+\n+      fprintf (file, \"\\n\t Loads : \");\n+\n+      if (ptr->loads)\n+\tprint_rtl (file, ptr->loads);\n+      else\n+\tfprintf (file, \"(nil)\");\n+\n+      fprintf (file, \"\\n\tStores : \");\n+\n+      if (ptr->stores)\n+\tprint_rtl (file, ptr->stores);\n+      else\n+\tfprintf (file, \"(nil)\");\n+\n+      fprintf (file, \"\\n\\n\");\n+    }\n+\n+  fprintf (file, \"\\n\");\n+}\n+\f\n+/* Store motion code.  */\n+\n+#define ANTIC_STORE_LIST(x)\t\t((x)->loads)\n+#define AVAIL_STORE_LIST(x)\t\t((x)->stores)\n+#define LAST_AVAIL_CHECK_FAILURE(x)\t((x)->reaching_reg)\n+\n+/* This is used to communicate the target bitvector we want to use in the\n+   reg_set_info routine when called via the note_stores mechanism.  */\n+static int * regvec;\n+\n+/* And current insn, for the same routine.  */\n+static rtx compute_store_table_current_insn;\n+\n+/* Used in computing the reverse edge graph bit vectors.  */\n+static sbitmap * st_antloc;\n+\n+/* Global holding the number of store expressions we are dealing with.  */\n+static int num_stores;\n+\n+/* Checks to set if we need to mark a register set.  Called from\n+   note_stores.  */\n+\n+static void\n+reg_set_info (rtx dest, const_rtx setter ATTRIBUTE_UNUSED,\n+\t      void *data ATTRIBUTE_UNUSED)\n+{\n+  if (GET_CODE (dest) == SUBREG)\n+    dest = SUBREG_REG (dest);\n+\n+  if (REG_P (dest))\n+    regvec[REGNO (dest)] = INSN_UID (compute_store_table_current_insn);\n+}\n+\n+/* Clear any mark that says that this insn sets dest.  Called from\n+   note_stores.  */\n+\n+static void\n+reg_clear_last_set (rtx dest, const_rtx setter ATTRIBUTE_UNUSED,\n+\t      void *data)\n+{\n+  int *dead_vec = (int *) data;\n+\n+  if (GET_CODE (dest) == SUBREG)\n+    dest = SUBREG_REG (dest);\n+\n+  if (REG_P (dest) &&\n+      dead_vec[REGNO (dest)] == INSN_UID (compute_store_table_current_insn))\n+    dead_vec[REGNO (dest)] = 0;\n+}\n+\n+/* Return zero if some of the registers in list X are killed\n+   due to set of registers in bitmap REGS_SET.  */\n+\n+static bool\n+store_ops_ok (const_rtx x, int *regs_set)\n+{\n+  const_rtx reg;\n+\n+  for (; x; x = XEXP (x, 1))\n+    {\n+      reg = XEXP (x, 0);\n+      if (regs_set[REGNO(reg)])\n+\treturn false;\n+    }\n+\n+  return true;\n+}\n+\n+/* Helper for extract_mentioned_regs; ACCUM is used to accumulate used\n+   registers.  */\n+static rtx\n+extract_mentioned_regs_helper (rtx x, rtx accum)\n+{\n+  int i;\n+  enum rtx_code code;\n+  const char * fmt;\n+\n+  /* Repeat is used to turn tail-recursion into iteration.  */\n+ repeat:\n+\n+  if (x == 0)\n+    return accum;\n+\n+  code = GET_CODE (x);\n+  switch (code)\n+    {\n+    case REG:\n+      return alloc_EXPR_LIST (0, x, accum);\n+\n+    case MEM:\n+      x = XEXP (x, 0);\n+      goto repeat;\n+\n+    case PRE_DEC:\n+    case PRE_INC:\n+    case PRE_MODIFY:\n+    case POST_DEC:\n+    case POST_INC:\n+    case POST_MODIFY:\n+      /* We do not run this function with arguments having side effects.  */\n+      gcc_unreachable ();\n+\n+    case PC:\n+    case CC0: /*FIXME*/\n+    case CONST:\n+    case CONST_INT:\n+    case CONST_DOUBLE:\n+    case CONST_FIXED:\n+    case CONST_VECTOR:\n+    case SYMBOL_REF:\n+    case LABEL_REF:\n+    case ADDR_VEC:\n+    case ADDR_DIFF_VEC:\n+      return accum;\n+\n+    default:\n+      break;\n+    }\n+\n+  i = GET_RTX_LENGTH (code) - 1;\n+  fmt = GET_RTX_FORMAT (code);\n+\n+  for (; i >= 0; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\t{\n+\t  rtx tem = XEXP (x, i);\n+\n+\t  /* If we are about to do the last recursive call\n+\t     needed at this level, change it into iteration.  */\n+\t  if (i == 0)\n+\t    {\n+\t      x = tem;\n+\t      goto repeat;\n+\t    }\n+\n+\t  accum = extract_mentioned_regs_helper (tem, accum);\n+\t}\n+      else if (fmt[i] == 'E')\n+\t{\n+\t  int j;\n+\n+\t  for (j = 0; j < XVECLEN (x, i); j++)\n+\t    accum = extract_mentioned_regs_helper (XVECEXP (x, i, j), accum);\n+\t}\n+    }\n+\n+  return accum;\n+}\n+\n+/* Returns a list of registers mentioned in X.  */\n+/* ??? Reimplement with for_each_rtx?  */\n+static rtx\n+extract_mentioned_regs (rtx x)\n+{\n+  return extract_mentioned_regs_helper (x, NULL_RTX);\n+}\n+\n+/* Check to see if the load X is aliased with STORE_PATTERN.\n+   AFTER is true if we are checking the case when STORE_PATTERN occurs\n+   after the X.  */\n+\n+static bool\n+load_kills_store (const_rtx x, const_rtx store_pattern, int after)\n+{\n+  if (after)\n+    return anti_dependence (x, store_pattern);\n+  else\n+    return true_dependence (store_pattern, GET_MODE (store_pattern), x,\n+\t\t\t    rtx_addr_varies_p);\n+}\n+\n+/* Go through the entire insn X, looking for any loads which might alias\n+   STORE_PATTERN.  Return true if found.\n+   AFTER is true if we are checking the case when STORE_PATTERN occurs\n+   after the insn X.  */\n+\n+static bool\n+find_loads (const_rtx x, const_rtx store_pattern, int after)\n+{\n+  const char * fmt;\n+  int i, j;\n+  int ret = false;\n+\n+  if (!x)\n+    return false;\n+\n+  if (GET_CODE (x) == SET)\n+    x = SET_SRC (x);\n+\n+  if (MEM_P (x))\n+    {\n+      if (load_kills_store (x, store_pattern, after))\n+\treturn true;\n+    }\n+\n+  /* Recursively process the insn.  */\n+  fmt = GET_RTX_FORMAT (GET_CODE (x));\n+\n+  for (i = GET_RTX_LENGTH (GET_CODE (x)) - 1; i >= 0 && !ret; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\tret |= find_loads (XEXP (x, i), store_pattern, after);\n+      else if (fmt[i] == 'E')\n+\tfor (j = XVECLEN (x, i) - 1; j >= 0; j--)\n+\t  ret |= find_loads (XVECEXP (x, i, j), store_pattern, after);\n+    }\n+  return ret;\n+}\n+\n+/* Go through pattern PAT looking for any loads which might kill the\n+   store in X.  Return true if found.\n+   AFTER is true if we are checking the case when loads kill X occurs\n+   after the insn for PAT.  */\n+\n+static inline bool\n+store_killed_in_pat (const_rtx x, const_rtx pat, int after)\n+{\n+  if (GET_CODE (pat) == SET)\n+    {\n+      rtx dest = SET_DEST (pat);\n+\n+      if (GET_CODE (dest) == ZERO_EXTRACT)\n+\tdest = XEXP (dest, 0);\n+\n+      /* Check for memory stores to aliased objects.  */\n+      if (MEM_P (dest)\n+\t  && !exp_equiv_p (dest, x, 0, true))\n+\t{\n+\t  if (after)\n+\t    {\n+\t      if (output_dependence (dest, x))\n+\t\treturn true;\n+\t    }\n+\t  else\n+\t    {\n+\t      if (output_dependence (x, dest))\n+\t\treturn true;\n+\t    }\n+\t}\n+    }\n+\n+  if (find_loads (pat, x, after))\n+    return true;\n+\n+  return false;\n+}\n+\n+/* Check if INSN kills the store pattern X (is aliased with it).\n+   AFTER is true if we are checking the case when store X occurs\n+   after the insn.  Return true if it does.  */\n+\n+static bool\n+store_killed_in_insn (const_rtx x, const_rtx x_regs, const_rtx insn, int after)\n+{\n+  const_rtx reg, base, note, pat;\n+\n+  if (!INSN_P (insn))\n+    return false;\n+\n+  if (CALL_P (insn))\n+    {\n+      /* A normal or pure call might read from pattern,\n+\t but a const call will not.  */\n+      if (!RTL_CONST_CALL_P (insn))\n+\treturn true;\n+\n+      /* But even a const call reads its parameters.  Check whether the\n+\t base of some of registers used in mem is stack pointer.  */\n+      for (reg = x_regs; reg; reg = XEXP (reg, 1))\n+\t{\n+\t  base = find_base_term (XEXP (reg, 0));\n+\t  if (!base\n+\t      || (GET_CODE (base) == ADDRESS\n+\t\t  && GET_MODE (base) == Pmode\n+\t\t  && XEXP (base, 0) == stack_pointer_rtx))\n+\t    return true;\n+\t}\n+\n+      return false;\n+    }\n+\n+  pat = PATTERN (insn);\n+  if (GET_CODE (pat) == SET)\n+    {\n+      if (store_killed_in_pat (x, pat, after))\n+\treturn true;\n+    }\n+  else if (GET_CODE (pat) == PARALLEL)\n+    {\n+      int i;\n+\n+      for (i = 0; i < XVECLEN (pat, 0); i++)\n+\tif (store_killed_in_pat (x, XVECEXP (pat, 0, i), after))\n+\t  return true;\n+    }\n+  else if (find_loads (PATTERN (insn), x, after))\n+    return true;\n+\n+  /* If this insn has a REG_EQUAL or REG_EQUIV note referencing a memory\n+     location aliased with X, then this insn kills X.  */\n+  note = find_reg_equal_equiv_note (insn);\n+  if (! note)\n+    return false;\n+  note = XEXP (note, 0);\n+\n+  /* However, if the note represents a must alias rather than a may\n+     alias relationship, then it does not kill X.  */\n+  if (exp_equiv_p (note, x, 0, true))\n+    return false;\n+\n+  /* See if there are any aliased loads in the note.  */\n+  return find_loads (note, x, after);\n+}\n+\n+/* Returns true if the expression X is loaded or clobbered on or after INSN\n+   within basic block BB.  REGS_SET_AFTER is bitmap of registers set in\n+   or after the insn.  X_REGS is list of registers mentioned in X. If the store\n+   is killed, return the last insn in that it occurs in FAIL_INSN.  */\n+\n+static bool\n+store_killed_after (const_rtx x, const_rtx x_regs, const_rtx insn, const_basic_block bb,\n+\t\t    int *regs_set_after, rtx *fail_insn)\n+{\n+  rtx last = BB_END (bb), act;\n+\n+  if (!store_ops_ok (x_regs, regs_set_after))\n+    {\n+      /* We do not know where it will happen.  */\n+      if (fail_insn)\n+\t*fail_insn = NULL_RTX;\n+      return true;\n+    }\n+\n+  /* Scan from the end, so that fail_insn is determined correctly.  */\n+  for (act = last; act != PREV_INSN (insn); act = PREV_INSN (act))\n+    if (store_killed_in_insn (x, x_regs, act, false))\n+      {\n+\tif (fail_insn)\n+\t  *fail_insn = act;\n+\treturn true;\n+      }\n+\n+  return false;\n+}\n+\n+/* Returns true if the expression X is loaded or clobbered on or before INSN\n+   within basic block BB. X_REGS is list of registers mentioned in X.\n+   REGS_SET_BEFORE is bitmap of registers set before or in this insn.  */\n+static bool\n+store_killed_before (const_rtx x, const_rtx x_regs, const_rtx insn, const_basic_block bb,\n+\t\t     int *regs_set_before)\n+{\n+  rtx first = BB_HEAD (bb);\n+\n+  if (!store_ops_ok (x_regs, regs_set_before))\n+    return true;\n+\n+  for ( ; insn != PREV_INSN (first); insn = PREV_INSN (insn))\n+    if (store_killed_in_insn (x, x_regs, insn, true))\n+      return true;\n+\n+  return false;\n+}\n+\n+/* Determine whether INSN is MEM store pattern that we will consider moving.\n+   REGS_SET_BEFORE is bitmap of registers set before (and including) the\n+   current insn, REGS_SET_AFTER is bitmap of registers set after (and\n+   including) the insn in this basic block.  We must be passing through BB from\n+   head to end, as we are using this fact to speed things up.\n+\n+   The results are stored this way:\n+\n+   -- the first anticipatable expression is added into ANTIC_STORE_LIST\n+   -- if the processed expression is not anticipatable, NULL_RTX is added\n+      there instead, so that we can use it as indicator that no further\n+      expression of this type may be anticipatable\n+   -- if the expression is available, it is added as head of AVAIL_STORE_LIST;\n+      consequently, all of them but this head are dead and may be deleted.\n+   -- if the expression is not available, the insn due to that it fails to be\n+      available is stored in reaching_reg.\n+\n+   The things are complicated a bit by fact that there already may be stores\n+   to the same MEM from other blocks; also caller must take care of the\n+   necessary cleanup of the temporary markers after end of the basic block.\n+   */\n+\n+static void\n+find_moveable_store (rtx insn, int *regs_set_before, int *regs_set_after)\n+{\n+  struct ls_expr * ptr;\n+  rtx dest, set, tmp;\n+  int check_anticipatable, check_available;\n+  basic_block bb = BLOCK_FOR_INSN (insn);\n+\n+  set = single_set (insn);\n+  if (!set)\n+    return;\n+\n+  dest = SET_DEST (set);\n+\n+  if (! MEM_P (dest) || MEM_VOLATILE_P (dest)\n+      || GET_MODE (dest) == BLKmode)\n+    return;\n+\n+  if (side_effects_p (dest))\n+    return;\n+\n+  /* If we are handling exceptions, we must be careful with memory references\n+     that may trap. If we are not, the behavior is undefined, so we may just\n+     continue.  */\n+  if (flag_non_call_exceptions && may_trap_p (dest))\n+    return;\n+\n+  /* Even if the destination cannot trap, the source may.  In this case we'd\n+     need to handle updating the REG_EH_REGION note.  */\n+  if (find_reg_note (insn, REG_EH_REGION, NULL_RTX))\n+    return;\n+\n+  /* Make sure that the SET_SRC of this store insns can be assigned to\n+     a register, or we will fail later on in replace_store_insn, which\n+     assumes that we can do this.  But sometimes the target machine has\n+     oddities like MEM read-modify-write instruction.  See for example\n+     PR24257.  */\n+  if (!can_assign_to_reg_without_clobbers_p (SET_SRC (set)))\n+    return;\n+\n+  ptr = ldst_entry (dest);\n+  if (!ptr->pattern_regs)\n+    ptr->pattern_regs = extract_mentioned_regs (dest);\n+\n+  /* Do not check for anticipatability if we either found one anticipatable\n+     store already, or tested for one and found out that it was killed.  */\n+  check_anticipatable = 0;\n+  if (!ANTIC_STORE_LIST (ptr))\n+    check_anticipatable = 1;\n+  else\n+    {\n+      tmp = XEXP (ANTIC_STORE_LIST (ptr), 0);\n+      if (tmp != NULL_RTX\n+\t  && BLOCK_FOR_INSN (tmp) != bb)\n+\tcheck_anticipatable = 1;\n+    }\n+  if (check_anticipatable)\n+    {\n+      if (store_killed_before (dest, ptr->pattern_regs, insn, bb, regs_set_before))\n+\ttmp = NULL_RTX;\n+      else\n+\ttmp = insn;\n+      ANTIC_STORE_LIST (ptr) = alloc_INSN_LIST (tmp,\n+\t\t\t\t\t\tANTIC_STORE_LIST (ptr));\n+    }\n+\n+  /* It is not necessary to check whether store is available if we did\n+     it successfully before; if we failed before, do not bother to check\n+     until we reach the insn that caused us to fail.  */\n+  check_available = 0;\n+  if (!AVAIL_STORE_LIST (ptr))\n+    check_available = 1;\n+  else\n+    {\n+      tmp = XEXP (AVAIL_STORE_LIST (ptr), 0);\n+      if (BLOCK_FOR_INSN (tmp) != bb)\n+\tcheck_available = 1;\n+    }\n+  if (check_available)\n+    {\n+      /* Check that we have already reached the insn at that the check\n+\t failed last time.  */\n+      if (LAST_AVAIL_CHECK_FAILURE (ptr))\n+\t{\n+\t  for (tmp = BB_END (bb);\n+\t       tmp != insn && tmp != LAST_AVAIL_CHECK_FAILURE (ptr);\n+\t       tmp = PREV_INSN (tmp))\n+\t    continue;\n+\t  if (tmp == insn)\n+\t    check_available = 0;\n+\t}\n+      else\n+\tcheck_available = store_killed_after (dest, ptr->pattern_regs, insn,\n+\t\t\t\t\t      bb, regs_set_after,\n+\t\t\t\t\t      &LAST_AVAIL_CHECK_FAILURE (ptr));\n+    }\n+  if (!check_available)\n+    AVAIL_STORE_LIST (ptr) = alloc_INSN_LIST (insn, AVAIL_STORE_LIST (ptr));\n+}\n+\n+/* Find available and anticipatable stores.  */\n+\n+static int\n+compute_store_table (void)\n+{\n+  int ret;\n+  basic_block bb;\n+  unsigned regno;\n+  rtx insn, pat, tmp;\n+  int *last_set_in, *already_set;\n+  struct ls_expr * ptr, **prev_next_ptr_ptr;\n+  unsigned int max_gcse_regno = max_reg_num ();\n+\n+  pre_ldst_mems = 0;\n+  pre_ldst_table = htab_create (13, pre_ldst_expr_hash,\n+\t\t\t\tpre_ldst_expr_eq, NULL);\n+  last_set_in = XCNEWVEC (int, max_gcse_regno);\n+  already_set = XNEWVEC (int, max_gcse_regno);\n+\n+  /* Find all the stores we care about.  */\n+  FOR_EACH_BB (bb)\n+    {\n+      /* First compute the registers set in this block.  */\n+      regvec = last_set_in;\n+\n+      FOR_BB_INSNS (bb, insn)\n+\t{\n+\t  if (! INSN_P (insn))\n+\t    continue;\n+\n+\t  if (CALL_P (insn))\n+\t    {\n+\t      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n+\t\tif (TEST_HARD_REG_BIT (regs_invalidated_by_call, regno))\n+\t\t  last_set_in[regno] = INSN_UID (insn);\n+\t    }\n+\n+\t  pat = PATTERN (insn);\n+\t  compute_store_table_current_insn = insn;\n+\t  note_stores (pat, reg_set_info, NULL);\n+\t}\n+\n+      /* Now find the stores.  */\n+      memset (already_set, 0, sizeof (int) * max_gcse_regno);\n+      regvec = already_set;\n+      FOR_BB_INSNS (bb, insn)\n+\t{\n+\t  if (! INSN_P (insn))\n+\t    continue;\n+\n+\t  if (CALL_P (insn))\n+\t    {\n+\t      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n+\t\tif (TEST_HARD_REG_BIT (regs_invalidated_by_call, regno))\n+\t\t  already_set[regno] = 1;\n+\t    }\n+\n+\t  pat = PATTERN (insn);\n+\t  note_stores (pat, reg_set_info, NULL);\n+\n+\t  /* Now that we've marked regs, look for stores.  */\n+\t  find_moveable_store (insn, already_set, last_set_in);\n+\n+\t  /* Unmark regs that are no longer set.  */\n+\t  compute_store_table_current_insn = insn;\n+\t  note_stores (pat, reg_clear_last_set, last_set_in);\n+\t  if (CALL_P (insn))\n+\t    {\n+\t      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n+\t\tif (TEST_HARD_REG_BIT (regs_invalidated_by_call, regno)\n+\t\t    && last_set_in[regno] == INSN_UID (insn))\n+\t\t  last_set_in[regno] = 0;\n+\t    }\n+\t}\n+\n+#ifdef ENABLE_CHECKING\n+      /* last_set_in should now be all-zero.  */\n+      for (regno = 0; regno < max_gcse_regno; regno++)\n+\tgcc_assert (!last_set_in[regno]);\n+#endif\n+\n+      /* Clear temporary marks.  */\n+      for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n+\t{\n+\t  LAST_AVAIL_CHECK_FAILURE(ptr) = NULL_RTX;\n+\t  if (ANTIC_STORE_LIST (ptr)\n+\t      && (tmp = XEXP (ANTIC_STORE_LIST (ptr), 0)) == NULL_RTX)\n+\t    ANTIC_STORE_LIST (ptr) = XEXP (ANTIC_STORE_LIST (ptr), 1);\n+\t}\n+    }\n+\n+  /* Remove the stores that are not available anywhere, as there will\n+     be no opportunity to optimize them.  */\n+  for (ptr = pre_ldst_mems, prev_next_ptr_ptr = &pre_ldst_mems;\n+       ptr != NULL;\n+       ptr = *prev_next_ptr_ptr)\n+    {\n+      if (!AVAIL_STORE_LIST (ptr))\n+\t{\n+\t  *prev_next_ptr_ptr = ptr->next;\n+\t  htab_remove_elt_with_hash (pre_ldst_table, ptr, ptr->hash_index);\n+\t  free_ldst_entry (ptr);\n+\t}\n+      else\n+\tprev_next_ptr_ptr = &ptr->next;\n+    }\n+\n+  ret = enumerate_ldsts ();\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"ST_avail and ST_antic (shown under loads..)\\n\");\n+      print_ldst_list (dump_file);\n+    }\n+\n+  free (last_set_in);\n+  free (already_set);\n+  return ret;\n+}\n+\n+/* Insert an instruction at the beginning of a basic block, and update\n+   the BB_HEAD if needed.  */\n+\n+static void\n+insert_insn_start_basic_block (rtx insn, basic_block bb)\n+{\n+  /* Insert at start of successor block.  */\n+  rtx prev = PREV_INSN (BB_HEAD (bb));\n+  rtx before = BB_HEAD (bb);\n+  while (before != 0)\n+    {\n+      if (! LABEL_P (before)\n+\t  && !NOTE_INSN_BASIC_BLOCK_P (before))\n+\tbreak;\n+      prev = before;\n+      if (prev == BB_END (bb))\n+\tbreak;\n+      before = NEXT_INSN (before);\n+    }\n+\n+  insn = emit_insn_after_noloc (insn, prev, bb);\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"STORE_MOTION  insert store at start of BB %d:\\n\",\n+\t       bb->index);\n+      print_inline_rtx (dump_file, insn, 6);\n+      fprintf (dump_file, \"\\n\");\n+    }\n+}\n+\n+/* This routine will insert a store on an edge. EXPR is the ldst entry for\n+   the memory reference, and E is the edge to insert it on.  Returns nonzero\n+   if an edge insertion was performed.  */\n+\n+static int\n+insert_store (struct ls_expr * expr, edge e)\n+{\n+  rtx reg, insn;\n+  basic_block bb;\n+  edge tmp;\n+  edge_iterator ei;\n+\n+  /* We did all the deleted before this insert, so if we didn't delete a\n+     store, then we haven't set the reaching reg yet either.  */\n+  if (expr->reaching_reg == NULL_RTX)\n+    return 0;\n+\n+  if (e->flags & EDGE_FAKE)\n+    return 0;\n+\n+  reg = expr->reaching_reg;\n+  insn = gen_move_insn (copy_rtx (expr->pattern), reg);\n+\n+  /* If we are inserting this expression on ALL predecessor edges of a BB,\n+     insert it at the start of the BB, and reset the insert bits on the other\n+     edges so we don't try to insert it on the other edges.  */\n+  bb = e->dest;\n+  FOR_EACH_EDGE (tmp, ei, e->dest->preds)\n+    if (!(tmp->flags & EDGE_FAKE))\n+      {\n+\tint index = EDGE_INDEX (edge_list, tmp->src, tmp->dest);\n+\t\n+\tgcc_assert (index != EDGE_INDEX_NO_EDGE);\n+\tif (! TEST_BIT (pre_insert_map[index], expr->index))\n+\t  break;\n+      }\n+\n+  /* If tmp is NULL, we found an insertion on every edge, blank the\n+     insertion vector for these edges, and insert at the start of the BB.  */\n+  if (!tmp && bb != EXIT_BLOCK_PTR)\n+    {\n+      FOR_EACH_EDGE (tmp, ei, e->dest->preds)\n+\t{\n+\t  int index = EDGE_INDEX (edge_list, tmp->src, tmp->dest);\n+\t  RESET_BIT (pre_insert_map[index], expr->index);\n+\t}\n+      insert_insn_start_basic_block (insn, bb);\n+      return 0;\n+    }\n+\n+  /* We can't put stores in the front of blocks pointed to by abnormal\n+     edges since that may put a store where one didn't used to be.  */\n+  gcc_assert (!(e->flags & EDGE_ABNORMAL));\n+\n+  insert_insn_on_edge (insn, e);\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"STORE_MOTION  insert insn on edge (%d, %d):\\n\",\n+\t       e->src->index, e->dest->index);\n+      print_inline_rtx (dump_file, insn, 6);\n+      fprintf (dump_file, \"\\n\");\n+    }\n+\n+  return 1;\n+}\n+\n+/* Remove any REG_EQUAL or REG_EQUIV notes containing a reference to the\n+   memory location in SMEXPR set in basic block BB.\n+\n+   This could be rather expensive.  */\n+\n+static void\n+remove_reachable_equiv_notes (basic_block bb, struct ls_expr *smexpr)\n+{\n+  edge_iterator *stack, ei;\n+  int sp;\n+  edge act;\n+  sbitmap visited = sbitmap_alloc (last_basic_block);\n+  rtx last, insn, note;\n+  rtx mem = smexpr->pattern;\n+\n+  stack = XNEWVEC (edge_iterator, n_basic_blocks);\n+  sp = 0;\n+  ei = ei_start (bb->succs);\n+\n+  sbitmap_zero (visited);\n+\n+  act = (EDGE_COUNT (ei_container (ei)) > 0 ? EDGE_I (ei_container (ei), 0) : NULL);\n+  while (1)\n+    {\n+      if (!act)\n+\t{\n+\t  if (!sp)\n+\t    {\n+\t      free (stack);\n+\t      sbitmap_free (visited);\n+\t      return;\n+\t    }\n+\t  act = ei_edge (stack[--sp]);\n+\t}\n+      bb = act->dest;\n+\n+      if (bb == EXIT_BLOCK_PTR\n+\t  || TEST_BIT (visited, bb->index))\n+\t{\n+\t  if (!ei_end_p (ei))\n+\t      ei_next (&ei);\n+\t  act = (! ei_end_p (ei)) ? ei_edge (ei) : NULL;\n+\t  continue;\n+\t}\n+      SET_BIT (visited, bb->index);\n+\n+      if (TEST_BIT (st_antloc[bb->index], smexpr->index))\n+\t{\n+\t  for (last = ANTIC_STORE_LIST (smexpr);\n+\t       BLOCK_FOR_INSN (XEXP (last, 0)) != bb;\n+\t       last = XEXP (last, 1))\n+\t    continue;\n+\t  last = XEXP (last, 0);\n+\t}\n+      else\n+\tlast = NEXT_INSN (BB_END (bb));\n+\n+      for (insn = BB_HEAD (bb); insn != last; insn = NEXT_INSN (insn))\n+\tif (INSN_P (insn))\n+\t  {\n+\t    note = find_reg_equal_equiv_note (insn);\n+\t    if (!note || !exp_equiv_p (XEXP (note, 0), mem, 0, true))\n+\t      continue;\n+\n+\t    if (dump_file)\n+\t      fprintf (dump_file, \"STORE_MOTION  drop REG_EQUAL note at insn %d:\\n\",\n+\t\t       INSN_UID (insn));\n+\t    remove_note (insn, note);\n+\t  }\n+\n+      if (!ei_end_p (ei))\n+\tei_next (&ei);\n+      act = (! ei_end_p (ei)) ? ei_edge (ei) : NULL;\n+\n+      if (EDGE_COUNT (bb->succs) > 0)\n+\t{\n+\t  if (act)\n+\t    stack[sp++] = ei;\n+\t  ei = ei_start (bb->succs);\n+\t  act = (EDGE_COUNT (ei_container (ei)) > 0 ? EDGE_I (ei_container (ei), 0) : NULL);\n+\t}\n+    }\n+}\n+\n+/* This routine will replace a store with a SET to a specified register.  */\n+\n+static void\n+replace_store_insn (rtx reg, rtx del, basic_block bb, struct ls_expr *smexpr)\n+{\n+  rtx insn, mem, note, set, ptr;\n+\n+  mem = smexpr->pattern;\n+  insn = gen_move_insn (reg, SET_SRC (single_set (del)));\n+\n+  for (ptr = ANTIC_STORE_LIST (smexpr); ptr; ptr = XEXP (ptr, 1))\n+    if (XEXP (ptr, 0) == del)\n+      {\n+\tXEXP (ptr, 0) = insn;\n+\tbreak;\n+      }\n+\n+  /* Move the notes from the deleted insn to its replacement.  */\n+  REG_NOTES (insn) = REG_NOTES (del);\n+\n+  /* Emit the insn AFTER all the notes are transferred.\n+     This is cheaper since we avoid df rescanning for the note change.  */\n+  insn = emit_insn_after (insn, del);\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file,\n+\t       \"STORE_MOTION  delete insn in BB %d:\\n      \", bb->index);\n+      print_inline_rtx (dump_file, del, 6);\n+      fprintf (dump_file, \"\\nSTORE_MOTION  replaced with insn:\\n      \");\n+      print_inline_rtx (dump_file, insn, 6);\n+      fprintf (dump_file, \"\\n\");\n+    }\n+\n+  delete_insn (del);\n+\n+  /* Now we must handle REG_EQUAL notes whose contents is equal to the mem;\n+     they are no longer accurate provided that they are reached by this\n+     definition, so drop them.  */\n+  for (; insn != NEXT_INSN (BB_END (bb)); insn = NEXT_INSN (insn))\n+    if (INSN_P (insn))\n+      {\n+\tset = single_set (insn);\n+\tif (!set)\n+\t  continue;\n+\tif (exp_equiv_p (SET_DEST (set), mem, 0, true))\n+\t  return;\n+\tnote = find_reg_equal_equiv_note (insn);\n+\tif (!note || !exp_equiv_p (XEXP (note, 0), mem, 0, true))\n+\t  continue;\n+\n+\tif (dump_file)\n+\t  fprintf (dump_file, \"STORE_MOTION  drop REG_EQUAL note at insn %d:\\n\",\n+\t\t   INSN_UID (insn));\n+\tremove_note (insn, note);\n+      }\n+  remove_reachable_equiv_notes (bb, smexpr);\n+}\n+\n+\n+/* Delete a store, but copy the value that would have been stored into\n+   the reaching_reg for later storing.  */\n+\n+static void\n+delete_store (struct ls_expr * expr, basic_block bb)\n+{\n+  rtx reg, i, del;\n+\n+  if (expr->reaching_reg == NULL_RTX)\n+    expr->reaching_reg = gen_reg_rtx_and_attrs (expr->pattern);\n+\n+  reg = expr->reaching_reg;\n+\n+  for (i = AVAIL_STORE_LIST (expr); i; i = XEXP (i, 1))\n+    {\n+      del = XEXP (i, 0);\n+      if (BLOCK_FOR_INSN (del) == bb)\n+\t{\n+\t  /* We know there is only one since we deleted redundant\n+\t     ones during the available computation.  */\n+\t  replace_store_insn (reg, del, bb, expr);\n+\t  break;\n+\t}\n+    }\n+}\n+\n+/* Fill in available, anticipatable, transparent and kill vectors in\n+   STORE_DATA, based on lists of available and anticipatable stores.  */\n+static void\n+build_store_vectors (void)\n+{\n+  basic_block bb;\n+  int *regs_set_in_block;\n+  rtx insn, st;\n+  struct ls_expr * ptr;\n+  unsigned int max_gcse_regno = max_reg_num ();\n+\n+  /* Build the gen_vector. This is any store in the table which is not killed\n+     by aliasing later in its block.  */\n+  ae_gen = sbitmap_vector_alloc (last_basic_block, num_stores);\n+  sbitmap_vector_zero (ae_gen, last_basic_block);\n+\n+  st_antloc = sbitmap_vector_alloc (last_basic_block, num_stores);\n+  sbitmap_vector_zero (st_antloc, last_basic_block);\n+\n+  for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n+    {\n+      for (st = AVAIL_STORE_LIST (ptr); st != NULL; st = XEXP (st, 1))\n+\t{\n+\t  insn = XEXP (st, 0);\n+\t  bb = BLOCK_FOR_INSN (insn);\n+\n+\t  /* If we've already seen an available expression in this block,\n+\t     we can delete this one (It occurs earlier in the block). We'll\n+\t     copy the SRC expression to an unused register in case there\n+\t     are any side effects.  */\n+\t  if (TEST_BIT (ae_gen[bb->index], ptr->index))\n+\t    {\n+\t      rtx r = gen_reg_rtx_and_attrs (ptr->pattern);\n+\t      if (dump_file)\n+\t\tfprintf (dump_file, \"Removing redundant store:\\n\");\n+\t      replace_store_insn (r, XEXP (st, 0), bb, ptr);\n+\t      continue;\n+\t    }\n+\t  SET_BIT (ae_gen[bb->index], ptr->index);\n+\t}\n+\n+      for (st = ANTIC_STORE_LIST (ptr); st != NULL; st = XEXP (st, 1))\n+\t{\n+\t  insn = XEXP (st, 0);\n+\t  bb = BLOCK_FOR_INSN (insn);\n+\t  SET_BIT (st_antloc[bb->index], ptr->index);\n+\t}\n+    }\n+\n+  ae_kill = sbitmap_vector_alloc (last_basic_block, num_stores);\n+  sbitmap_vector_zero (ae_kill, last_basic_block);\n+\n+  transp = sbitmap_vector_alloc (last_basic_block, num_stores);\n+  sbitmap_vector_zero (transp, last_basic_block);\n+  regs_set_in_block = XNEWVEC (int, max_gcse_regno);\n+\n+  FOR_EACH_BB (bb)\n+    {\n+      FOR_BB_INSNS (bb, insn)\n+\tif (INSN_P (insn))\n+\t  {\n+\t    df_ref *def_rec;\n+\t    for (def_rec = DF_INSN_DEFS (insn); *def_rec; def_rec++)\n+\t      {\n+\t\tunsigned int ref_regno = DF_REF_REGNO (*def_rec);\n+\t\tif (ref_regno < max_gcse_regno)\n+\t\t  regs_set_in_block[DF_REF_REGNO (*def_rec)] = 1;\n+\t      }\n+\t  }\n+\n+      for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n+\t{\n+\t  if (store_killed_after (ptr->pattern, ptr->pattern_regs, BB_HEAD (bb),\n+\t\t\t\t  bb, regs_set_in_block, NULL))\n+\t    {\n+\t      /* It should not be necessary to consider the expression\n+\t\t killed if it is both anticipatable and available.  */\n+\t      if (!TEST_BIT (st_antloc[bb->index], ptr->index)\n+\t\t  || !TEST_BIT (ae_gen[bb->index], ptr->index))\n+\t\tSET_BIT (ae_kill[bb->index], ptr->index);\n+\t    }\n+\t  else\n+\t    SET_BIT (transp[bb->index], ptr->index);\n+\t}\n+    }\n+\n+  free (regs_set_in_block);\n+\n+  if (dump_file)\n+    {\n+      dump_sbitmap_vector (dump_file, \"st_antloc\", \"\", st_antloc, last_basic_block);\n+      dump_sbitmap_vector (dump_file, \"st_kill\", \"\", ae_kill, last_basic_block);\n+      dump_sbitmap_vector (dump_file, \"Transpt\", \"\", transp, last_basic_block);\n+      dump_sbitmap_vector (dump_file, \"st_avloc\", \"\", ae_gen, last_basic_block);\n+    }\n+}\n+\n+/* Free memory used by store motion.  */\n+\n+static void\n+free_store_memory (void)\n+{\n+  free_ldst_mems ();\n+\n+  if (ae_gen)\n+    sbitmap_vector_free (ae_gen);\n+  if (ae_kill)\n+    sbitmap_vector_free (ae_kill);\n+  if (transp)\n+    sbitmap_vector_free (transp);\n+  if (st_antloc)\n+    sbitmap_vector_free (st_antloc);\n+  if (pre_insert_map)\n+    sbitmap_vector_free (pre_insert_map);\n+  if (pre_delete_map)\n+    sbitmap_vector_free (pre_delete_map);\n+\n+  ae_gen = ae_kill = transp = st_antloc = NULL;\n+  pre_insert_map = pre_delete_map = NULL;\n+}\n+\n+/* Perform store motion. Much like gcse, except we move expressions the\n+   other way by looking at the flowgraph in reverse.\n+   Return non-zero if transformations are performed by the pass.  */\n+\n+static int\n+one_store_motion_pass (void)\n+{\n+  basic_block bb;\n+  int x;\n+  struct ls_expr * ptr;\n+  int update_flow = 0;\n+\n+  gcse_subst_count = 0;\n+  gcse_create_count = 0;\n+\n+  init_alias_analysis ();\n+\n+  /* Find all the available and anticipatable stores.  */\n+  num_stores = compute_store_table ();\n+  if (num_stores == 0)\n+    {\n+      htab_delete (pre_ldst_table);\n+      pre_ldst_table = NULL;\n+      end_alias_analysis ();\n+      return 0;\n+    }\n+\n+  /* Now compute kill & transp vectors.  */\n+  build_store_vectors ();\n+  add_noreturn_fake_exit_edges ();\n+  connect_infinite_loops_to_exit ();\n+\n+  edge_list = pre_edge_rev_lcm (num_stores, transp, ae_gen,\n+\t\t\t\tst_antloc, ae_kill, &pre_insert_map,\n+\t\t\t\t&pre_delete_map);\n+\n+  /* Now we want to insert the new stores which are going to be needed.  */\n+  for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n+    {\n+      /* If any of the edges we have above are abnormal, we can't move this\n+\t store.  */\n+      for (x = NUM_EDGES (edge_list) - 1; x >= 0; x--)\n+\tif (TEST_BIT (pre_insert_map[x], ptr->index)\n+\t    && (INDEX_EDGE (edge_list, x)->flags & EDGE_ABNORMAL))\n+\t  break;\n+\n+      if (x >= 0)\n+\t{\n+\t  if (dump_file != NULL)\n+\t    fprintf (dump_file,\n+\t\t     \"Can't replace store %d: abnormal edge from %d to %d\\n\",\n+\t\t     ptr->index, INDEX_EDGE (edge_list, x)->src->index,\n+\t\t     INDEX_EDGE (edge_list, x)->dest->index);\n+\t  continue;\n+\t}\n+\t\t      \n+      /* Now we want to insert the new stores which are going to be needed.  */\n+\n+      FOR_EACH_BB (bb)\n+\tif (TEST_BIT (pre_delete_map[bb->index], ptr->index))\n+\t  {\n+\t    delete_store (ptr, bb);\n+\t    gcse_subst_count++;\n+\t  }\n+\n+      for (x = 0; x < NUM_EDGES (edge_list); x++)\n+\tif (TEST_BIT (pre_insert_map[x], ptr->index))\n+\t  {\n+\t    update_flow |= insert_store (ptr, INDEX_EDGE (edge_list, x));\n+\t    gcse_create_count++;\n+\t  }\n+    }\n+\n+  if (update_flow)\n+    commit_edge_insertions ();\n+\n+  free_store_memory ();\n+  free_edge_list (edge_list);\n+  remove_fake_exit_edges ();\n+  end_alias_analysis ();\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"STORE_MOTION of %s, %d basic blocks, \",\n+\t       current_function_name (), n_basic_blocks);\n+      fprintf (dump_file, \"%d substs, %d insns created\\n\",\n+\t       gcse_subst_count, gcse_create_count);\n+    }\n+\n+  return (gcse_subst_count > 0 || gcse_create_count > 0);\n+}\n+\n+\f\n+static bool\n+gate_rtl_store_motion (void)\n+{\n+  return optimize > 0 && flag_gcse_sm\n+    && !cfun->calls_setjmp\n+    && optimize_function_for_speed_p (cfun)\n+    && dbg_cnt (store_motion);\n+}\n+\n+static unsigned int\n+execute_rtl_store_motion (void)\n+{\n+  delete_unreachable_blocks ();\n+  df_note_add_problem ();\n+  df_analyze ();\n+  flag_rerun_cse_after_global_opts |= one_store_motion_pass ();\n+  return 0;\n+}\n+\n+struct rtl_opt_pass pass_rtl_store_motion =\n+{\n+ {\n+  RTL_PASS,\n+  \"store_motion\",                       /* name */\n+  gate_rtl_store_motion,                /* gate */   \n+  execute_rtl_store_motion,\t\t/* execute */       \n+  NULL,                                 /* sub */\n+  NULL,                                 /* next */\n+  0,                                    /* static_pass_number */\n+  TV_LSM,                               /* tv_id */\n+  PROP_cfglayout,                       /* properties_required */\n+  0,                                    /* properties_provided */\n+  0,                                    /* properties_destroyed */\n+  0,                                    /* todo_flags_start */\n+  TODO_df_finish | TODO_verify_rtl_sharing |\n+  TODO_dump_func |\n+  TODO_verify_flow | TODO_ggc_collect   /* todo_flags_finish */\n+ }\n+};\n+"}]}