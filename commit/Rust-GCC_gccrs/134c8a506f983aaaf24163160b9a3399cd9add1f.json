{"sha": "134c8a506f983aaaf24163160b9a3399cd9add1f", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MTM0YzhhNTA2Zjk4M2FhYWYyNDE2MzE2MGI5YTMzOTljZDlhZGQxZg==", "commit": {"author": {"name": "Bob Wilson", "email": "bob.wilson@acm.org", "date": "2006-01-09T23:41:11Z"}, "committer": {"name": "Bob Wilson", "email": "bwilson@gcc.gnu.org", "date": "2006-01-09T23:41:11Z"}, "message": "xtensa-config.h (XCHAL_HAVE_MUL32_HIGH): Define.\n\ninclude:\n\t* xtensa-config.h (XCHAL_HAVE_MUL32_HIGH): Define.\ngcc:\n\t* config/xtensa/ieee754-df.S: New file.\n\t* config/xtensa/ieee754-sf.S: New file.\n\t* config/xtensa/t-xtensa (LIB2FUNCS_EXTRA): Remove fp-bit.c & dp-bit.c.\n\t(LIB1ASMFUNCS): Add SFmode and DFmode floating-point functions.\n\t* config/xtensa/lib1funcs.asm: Include ieee754-df.S and ieee754-sf.S.\n\nFrom-SVN: r109518", "tree": {"sha": "76a450e08f54ee9462708d61f67f55f1f9ee77fd", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/76a450e08f54ee9462708d61f67f55f1f9ee77fd"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/134c8a506f983aaaf24163160b9a3399cd9add1f", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/134c8a506f983aaaf24163160b9a3399cd9add1f", "html_url": "https://github.com/Rust-GCC/gccrs/commit/134c8a506f983aaaf24163160b9a3399cd9add1f", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/134c8a506f983aaaf24163160b9a3399cd9add1f/comments", "author": null, "committer": null, "parents": [{"sha": "42801b989af277b4c094cce7e2d0136730bea815", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/42801b989af277b4c094cce7e2d0136730bea815", "html_url": "https://github.com/Rust-GCC/gccrs/commit/42801b989af277b4c094cce7e2d0136730bea815"}], "stats": {"total": 4140, "additions": 4127, "deletions": 13}, "files": [{"sha": "19176ec476c36ed8f026fb162dee4f2a9367278f", "filename": "gcc/ChangeLog", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/134c8a506f983aaaf24163160b9a3399cd9add1f/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/134c8a506f983aaaf24163160b9a3399cd9add1f/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=134c8a506f983aaaf24163160b9a3399cd9add1f", "patch": "@@ -1,3 +1,11 @@\n+2006-01-09  Bob Wilson  <bob.wilson@acm.org>\n+\n+\t* config/xtensa/ieee754-df.S: New file.\n+\t* config/xtensa/ieee754-sf.S: New file.\n+\t* config/xtensa/t-xtensa (LIB2FUNCS_EXTRA): Remove fp-bit.c & dp-bit.c.\n+\t(LIB1ASMFUNCS): Add SFmode and DFmode floating-point functions.\n+\t* config/xtensa/lib1funcs.asm: Include ieee754-df.S and ieee754-sf.S.\n+\n 2006-01-09  Kazu Hirata  <kazu@codesourcery.com>\n \n \t* config/sh/predicates.md (binary_float_operator,"}, {"sha": "66c4ea0d0184a6a4971bbeec10e4816e1bf1df3d", "filename": "gcc/config/xtensa/ieee754-df.S", "status": "added", "additions": 2365, "deletions": 0, "changes": 2365, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/134c8a506f983aaaf24163160b9a3399cd9add1f/gcc%2Fconfig%2Fxtensa%2Fieee754-df.S", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/134c8a506f983aaaf24163160b9a3399cd9add1f/gcc%2Fconfig%2Fxtensa%2Fieee754-df.S", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fxtensa%2Fieee754-df.S?ref=134c8a506f983aaaf24163160b9a3399cd9add1f", "patch": "@@ -0,0 +1,2365 @@\n+/* IEEE-754 double-precision functions for Xtensa\n+   Copyright (C) 2006 Free Software Foundation, Inc.\n+   Contributed by Bob Wilson (bwilson@tensilica.com) at Tensilica.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 2, or (at your option)\n+   any later version.\n+\n+   In addition to the permissions in the GNU General Public License,\n+   the Free Software Foundation gives you unlimited permission to link\n+   the compiled version of this file into combinations with other\n+   programs, and to distribute those combinations without any\n+   restriction coming from the use of this file.  (The General Public\n+   License restrictions do apply in other respects; for example, they\n+   cover modification of the file, and distribution when not linked\n+   into a combine executable.)\n+\n+   GCC is distributed in the hope that it will be useful, but WITHOUT\n+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY\n+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public\n+   License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING.  If not, write to the Free\n+   Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n+   02111-1307, USA.  */\n+\n+#ifdef __XTENSA_EB__\n+#define xh a2\n+#define xl a3\n+#define yh a4\n+#define yl a5\n+#else\n+#define xh a3\n+#define xl a2\n+#define yh a5\n+#define yl a4\n+#endif\n+\n+/*  Warning!  The branch displacements for some Xtensa branch instructions\n+    are quite small, and this code has been carefully laid out to keep\n+    branch targets in range.  If you change anything, be sure to check that\n+    the assembler is not relaxing anything to branch over a jump.  */\n+\n+#ifdef L_negdf2\n+\n+\t.align\t4\n+\t.global\t__negdf2\n+\t.type\t__negdf2, @function\n+__negdf2:\n+\tabi_entry sp, 32\n+\tmovi\ta4, 0x80000000\n+\txor\txh, xh, a4\n+\tabi_return\n+\n+#endif /* L_negdf2 */\n+\n+#ifdef L_addsubdf3\n+\n+\t/* Addition */\n+__adddf3_aux:\n+\t\n+\t/* Handle NaNs and Infinities.  (This code is placed before the\n+\t   start of the function just to keep it in range of the limited\n+\t   branch displacements.)  */\n+\n+.Ladd_xnan_or_inf:\n+\t/* If y is neither Infinity nor NaN, return x.  */\n+\tbnall\tyh, a6, 1f\n+\t/* If x is a NaN, return it.  Otherwise, return y.  */\n+\tslli\ta7, xh, 12\n+\tor\ta7, a7, xl\n+\tbeqz\ta7, .Ladd_ynan_or_inf\n+1:\tabi_return\n+\n+.Ladd_ynan_or_inf:\n+\t/* Return y.  */\n+\tmov\txh, yh\n+\tmov\txl, yl\n+\tabi_return\n+\n+.Ladd_opposite_signs:\n+\t/* Operand signs differ.  Do a subtraction.  */\n+\tslli\ta7, a6, 11\n+\txor\tyh, yh, a7\n+\tj\t.Lsub_same_sign\n+\n+\t.align\t4\n+\t.global\t__adddf3\n+\t.type\t__adddf3, @function\n+__adddf3:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7ff00000\n+\n+\t/* Check if the two operands have the same sign.  */\n+\txor\ta7, xh, yh\n+\tbltz\ta7, .Ladd_opposite_signs\n+\n+.Ladd_same_sign:\t\n+\t/* Check if either exponent == 0x7ff (i.e., NaN or Infinity).  */\n+\tball\txh, a6, .Ladd_xnan_or_inf\n+\tball\tyh, a6, .Ladd_ynan_or_inf\n+\n+\t/* Compare the exponents.  The smaller operand will be shifted\n+\t   right by the exponent difference and added to the larger\n+\t   one.  */\n+\textui\ta7, xh, 20, 12\n+\textui\ta8, yh, 20, 12\n+\tbltu\ta7, a8, .Ladd_shiftx\n+\n+.Ladd_shifty:\n+\t/* Check if the smaller (or equal) exponent is zero.  */\n+\tbnone\tyh, a6, .Ladd_yexpzero\n+\n+\t/* Replace yh sign/exponent with 0x001.  */\n+\tor\tyh, yh, a6\n+\tslli\tyh, yh, 11\n+\tsrli\tyh, yh, 11\n+\n+.Ladd_yexpdiff:\n+\t/* Compute the exponent difference.  Optimize for difference < 32.  */\n+\tsub\ta10, a7, a8\n+\tbgeui\ta10, 32, .Ladd_bigshifty\n+\t\n+\t/* Shift yh/yl right by the exponent difference.  Any bits that are\n+\t   shifted out of yl are saved in a9 for rounding the result.  */\n+\tssr\ta10\n+\tmovi\ta9, 0\n+\tsrc\ta9, yl, a9\n+\tsrc\tyl, yh, yl\n+\tsrl\tyh, yh\n+\n+.Ladd_addy:\n+\t/* Do the 64-bit addition.  */\n+\tadd\txl, xl, yl\n+\tadd\txh, xh, yh\n+\tbgeu\txl, yl, 1f\n+\taddi\txh, xh, 1\n+1:\n+\t/* Check if the add overflowed into the exponent.  */\n+\textui\ta10, xh, 20, 12\n+\tbeq\ta10, a7, .Ladd_round\n+\tmov\ta8, a7\n+\tj\t.Ladd_carry\n+\n+.Ladd_yexpzero:\n+\t/* y is a subnormal value.  Replace its sign/exponent with zero,\n+\t   i.e., no implicit \"1.0\", and increment the apparent exponent\n+\t   because subnormals behave as if they had the minimum (nonzero)\n+\t   exponent.  Test for the case when both exponents are zero.  */\n+\tslli\tyh, yh, 12\n+\tsrli\tyh, yh, 12\n+\tbnone\txh, a6, .Ladd_bothexpzero\n+\taddi\ta8, a8, 1\n+\tj\t.Ladd_yexpdiff\n+\n+.Ladd_bothexpzero:\n+\t/* Both exponents are zero.  Handle this as a special case.  There\n+\t   is no need to shift or round, and the normal code for handling\n+\t   a carry into the exponent field will not work because it\n+\t   assumes there is an implicit \"1.0\" that needs to be added.  */\n+\tadd\txl, xl, yl\n+\tadd\txh, xh, yh\n+\tbgeu\txl, yl, 1f\n+\taddi\txh, xh, 1\n+1:\tabi_return\n+\n+.Ladd_bigshifty:\n+\t/* Exponent difference > 64 -- just return the bigger value.  */\n+\tbgeui\ta10, 64, 1b\n+\n+\t/* Shift yh/yl right by the exponent difference.  Any bits that are\n+\t   shifted out are saved in a9 for rounding the result.  */\n+\tssr\ta10\n+\tsll\ta11, yl\t\t/* lost bits shifted out of yl */\n+\tsrc\ta9, yh, yl\n+\tsrl\tyl, yh\n+\tmovi\tyh, 0\n+\tbeqz\ta11, .Ladd_addy\n+\tor\ta9, a9, a10\t/* any positive, nonzero value will work */\n+\tj\t.Ladd_addy\n+\n+.Ladd_xexpzero:\n+\t/* Same as \"yexpzero\" except skip handling the case when both\n+\t   exponents are zero.  */\n+\tslli\txh, xh, 12\n+\tsrli\txh, xh, 12\n+\taddi\ta7, a7, 1\n+\tj\t.Ladd_xexpdiff\n+\n+.Ladd_shiftx:\n+\t/* Same thing as the \"shifty\" code, but with x and y swapped.  Also,\n+\t   because the exponent difference is always nonzero in this version,\n+\t   the shift sequence can use SLL and skip loading a constant zero.  */\n+\tbnone\txh, a6, .Ladd_xexpzero\n+\n+\tor\txh, xh, a6\n+\tslli\txh, xh, 11\n+\tsrli\txh, xh, 11\n+\n+.Ladd_xexpdiff:\n+\tsub\ta10, a8, a7\n+\tbgeui\ta10, 32, .Ladd_bigshiftx\n+\t\n+\tssr\ta10\n+\tsll\ta9, xl\n+\tsrc\txl, xh, xl\n+\tsrl\txh, xh\n+\n+.Ladd_addx:\n+\tadd\txl, xl, yl\n+\tadd\txh, xh, yh\n+\tbgeu\txl, yl, 1f\n+\taddi\txh, xh, 1\n+1:\n+\t/* Check if the add overflowed into the exponent.  */\n+\textui\ta10, xh, 20, 12\n+\tbne\ta10, a8, .Ladd_carry\n+\n+.Ladd_round:\n+\t/* Round up if the leftover fraction is >= 1/2.  */\n+\tbgez\ta9, 1f\n+\taddi\txl, xl, 1\n+\tbeqz\txl, .Ladd_roundcarry\n+\n+\t/* Check if the leftover fraction is exactly 1/2.  */\n+\tslli\ta9, a9, 1\n+\tbeqz\ta9, .Ladd_exactlyhalf\n+1:\tabi_return\n+\n+.Ladd_bigshiftx:\n+\t/* Mostly the same thing as \"bigshifty\"....  */\n+\tbgeui\ta10, 64, .Ladd_returny\n+\n+\tssr\ta10\n+\tsll\ta11, xl\n+\tsrc\ta9, xh, xl\n+\tsrl\txl, xh\n+\tmovi\txh, 0\n+\tbeqz\ta11, .Ladd_addx\n+\tor\ta9, a9, a10\n+\tj\t.Ladd_addx\n+\n+.Ladd_returny:\n+\tmov\txh, yh\n+\tmov\txl, yl\n+\tabi_return\n+\n+.Ladd_carry:\t\n+\t/* The addition has overflowed into the exponent field, so the\n+\t   value needs to be renormalized.  The mantissa of the result\n+\t   can be recovered by subtracting the original exponent and\n+\t   adding 0x100000 (which is the explicit \"1.0\" for the\n+\t   mantissa of the non-shifted operand -- the \"1.0\" for the\n+\t   shifted operand was already added).  The mantissa can then\n+\t   be shifted right by one bit.  The explicit \"1.0\" of the\n+\t   shifted mantissa then needs to be replaced by the exponent,\n+\t   incremented by one to account for the normalizing shift.\n+\t   It is faster to combine these operations: do the shift first\n+\t   and combine the additions and subtractions.  If x is the\n+\t   original exponent, the result is:\n+\t       shifted mantissa - (x << 19) + (1 << 19) + (x << 20)\n+\t   or:\n+\t       shifted mantissa + ((x + 1) << 19)\n+\t   Note that the exponent is incremented here by leaving the\n+\t   explicit \"1.0\" of the mantissa in the exponent field.  */\n+\n+\t/* Shift xh/xl right by one bit.  Save the lsb of xl.  */\n+\tmov\ta10, xl\n+\tssai\t1\n+\tsrc\txl, xh, xl\n+\tsrl\txh, xh\n+\n+\t/* See explanation above.  The original exponent is in a8.  */\n+\taddi\ta8, a8, 1\n+\tslli\ta8, a8, 19\n+\tadd\txh, xh, a8\n+\n+\t/* Return an Infinity if the exponent overflowed.  */\n+\tball\txh, a6, .Ladd_infinity\n+\t\n+\t/* Same thing as the \"round\" code except the msb of the leftover\n+\t   fraction is bit 0 of a10, with the rest of the fraction in a9.  */\n+\tbbci.l\ta10, 0, 1f\n+\taddi\txl, xl, 1\n+\tbeqz\txl, .Ladd_roundcarry\n+\tbeqz\ta9, .Ladd_exactlyhalf\n+1:\tabi_return\n+\n+.Ladd_infinity:\n+\t/* Clear the mantissa.  */\n+\tmovi\txl, 0\n+\tsrli\txh, xh, 20\n+\tslli\txh, xh, 20\n+\n+\t/* The sign bit may have been lost in a carry-out.  Put it back.  */\n+\tslli\ta8, a8, 1\n+\tor\txh, xh, a8\n+\tabi_return\n+\n+.Ladd_exactlyhalf:\n+\t/* Round down to the nearest even value.  */\n+\tsrli\txl, xl, 1\n+\tslli\txl, xl, 1\n+\tabi_return\n+\n+.Ladd_roundcarry:\n+\t/* xl is always zero when the rounding increment overflows, so\n+\t   there's no need to round it to an even value.  */\n+\taddi\txh, xh, 1\n+\t/* Overflow to the exponent is OK.  */\n+\tabi_return\n+\n+\n+\t/* Subtraction */\n+__subdf3_aux:\n+\t\n+\t/* Handle NaNs and Infinities.  (This code is placed before the\n+\t   start of the function just to keep it in range of the limited\n+\t   branch displacements.)  */\n+\n+.Lsub_xnan_or_inf:\n+\t/* If y is neither Infinity nor NaN, return x.  */\n+\tbnall\tyh, a6, 1f\n+\t/* Both x and y are either NaN or Inf, so the result is NaN.  */\n+\tmovi\ta4, 0x80000\t/* make it a quiet NaN */\n+\tor\txh, xh, a4\n+1:\tabi_return\n+\n+.Lsub_ynan_or_inf:\n+\t/* Negate y and return it.  */\n+\tslli\ta7, a6, 11\n+\txor\txh, yh, a7\n+\tmov\txl, yl\n+\tabi_return\n+\n+.Lsub_opposite_signs:\n+\t/* Operand signs differ.  Do an addition.  */\n+\tslli\ta7, a6, 11\n+\txor\tyh, yh, a7\n+\tj\t.Ladd_same_sign\n+\n+\t.align\t4\n+\t.global\t__subdf3\n+\t.type\t__subdf3, @function\n+__subdf3:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7ff00000\n+\n+\t/* Check if the two operands have the same sign.  */\n+\txor\ta7, xh, yh\n+\tbltz\ta7, .Lsub_opposite_signs\n+\n+.Lsub_same_sign:\t\n+\t/* Check if either exponent == 0x7ff (i.e., NaN or Infinity).  */\n+\tball\txh, a6, .Lsub_xnan_or_inf\n+\tball\tyh, a6, .Lsub_ynan_or_inf\n+\n+\t/* Compare the operands.  In contrast to addition, the entire\n+\t   value matters here.  */\n+\textui\ta7, xh, 20, 11\n+\textui\ta8, yh, 20, 11\n+\tbltu\txh, yh, .Lsub_xsmaller\n+\tbeq\txh, yh, .Lsub_compare_low\n+\n+.Lsub_ysmaller:\n+\t/* Check if the smaller (or equal) exponent is zero.  */\n+\tbnone\tyh, a6, .Lsub_yexpzero\n+\n+\t/* Replace yh sign/exponent with 0x001.  */\n+\tor\tyh, yh, a6\n+\tslli\tyh, yh, 11\n+\tsrli\tyh, yh, 11\n+\n+.Lsub_yexpdiff:\n+\t/* Compute the exponent difference.  Optimize for difference < 32.  */\n+\tsub\ta10, a7, a8\n+\tbgeui\ta10, 32, .Lsub_bigshifty\n+\t\n+\t/* Shift yh/yl right by the exponent difference.  Any bits that are\n+\t   shifted out of yl are saved in a9 for rounding the result.  */\n+\tssr\ta10\n+\tmovi\ta9, 0\n+\tsrc\ta9, yl, a9\n+\tsrc\tyl, yh, yl\n+\tsrl\tyh, yh\n+\n+.Lsub_suby:\n+\t/* Do the 64-bit subtraction.  */\n+\tsub\txh, xh, yh\n+\tbgeu\txl, yl, 1f\n+\taddi\txh, xh, -1\n+1:\tsub\txl, xl, yl\n+\n+\t/* Subtract the leftover bits in a9 from zero and propagate any\n+\t   borrow from xh/xl.  */\n+\tneg\ta9, a9\n+\tbeqz\ta9, 1f\n+\taddi\ta5, xh, -1\n+\tmoveqz\txh, a5, xl\n+\taddi\txl, xl, -1\n+1:\n+\t/* Check if the subtract underflowed into the exponent.  */\n+\textui\ta10, xh, 20, 11\n+\tbeq\ta10, a7, .Lsub_round\n+\tj\t.Lsub_borrow\n+\n+.Lsub_compare_low:\n+\t/* The high words are equal.  Compare the low words.  */\n+\tbltu\txl, yl, .Lsub_xsmaller\n+\tbltu\tyl, xl, .Lsub_ysmaller\n+\t/* The operands are equal.  Return 0.0.  */\n+\tmovi\txh, 0\n+\tmovi\txl, 0\n+1:\tabi_return\n+\n+.Lsub_yexpzero:\n+\t/* y is a subnormal value.  Replace its sign/exponent with zero,\n+\t   i.e., no implicit \"1.0\".  Unless x is also a subnormal, increment\n+\t   y's apparent exponent because subnormals behave as if they had\n+\t   the minimum (nonzero) exponent.  */\n+\tslli\tyh, yh, 12\n+\tsrli\tyh, yh, 12\n+\tbnone\txh, a6, .Lsub_yexpdiff\n+\taddi\ta8, a8, 1\n+\tj\t.Lsub_yexpdiff\n+\n+.Lsub_bigshifty:\n+\t/* Exponent difference > 64 -- just return the bigger value.  */\n+\tbgeui\ta10, 64, 1b\n+\n+\t/* Shift yh/yl right by the exponent difference.  Any bits that are\n+\t   shifted out are saved in a9 for rounding the result.  */\n+\tssr\ta10\n+\tsll\ta11, yl\t\t/* lost bits shifted out of yl */\n+\tsrc\ta9, yh, yl\n+\tsrl\tyl, yh\n+\tmovi\tyh, 0\n+\tbeqz\ta11, .Lsub_suby\n+\tor\ta9, a9, a10\t/* any positive, nonzero value will work */\n+\tj\t.Lsub_suby\n+\n+.Lsub_xsmaller:\n+\t/* Same thing as the \"ysmaller\" code, but with x and y swapped and\n+\t   with y negated.  */\n+\tbnone\txh, a6, .Lsub_xexpzero\n+\n+\tor\txh, xh, a6\n+\tslli\txh, xh, 11\n+\tsrli\txh, xh, 11\n+\n+.Lsub_xexpdiff:\n+\tsub\ta10, a8, a7\n+\tbgeui\ta10, 32, .Lsub_bigshiftx\n+\t\n+\tssr\ta10\n+\tmovi\ta9, 0\n+\tsrc\ta9, xl, a9\n+\tsrc\txl, xh, xl\n+\tsrl\txh, xh\n+\n+\t/* Negate y.  */\n+\tslli\ta11, a6, 11\n+\txor\tyh, yh, a11\n+\n+.Lsub_subx:\n+\tsub\txl, yl, xl\n+\tsub\txh, yh, xh\n+\tbgeu\tyl, xl, 1f\n+\taddi\txh, xh, -1\n+1:\n+\t/* Subtract the leftover bits in a9 from zero and propagate any\n+\t   borrow from xh/xl.  */\n+\tneg\ta9, a9\n+\tbeqz\ta9, 1f\n+\taddi\ta5, xh, -1\n+\tmoveqz\txh, a5, xl\n+\taddi\txl, xl, -1\n+1:\n+\t/* Check if the subtract underflowed into the exponent.  */\n+\textui\ta10, xh, 20, 11\n+\tbne\ta10, a8, .Lsub_borrow\n+\n+.Lsub_round:\n+\t/* Round up if the leftover fraction is >= 1/2.  */\n+\tbgez\ta9, 1f\n+\taddi\txl, xl, 1\n+\tbeqz\txl, .Lsub_roundcarry\n+\n+\t/* Check if the leftover fraction is exactly 1/2.  */\n+\tslli\ta9, a9, 1\n+\tbeqz\ta9, .Lsub_exactlyhalf\n+1:\tabi_return\n+\n+.Lsub_xexpzero:\n+\t/* Same as \"yexpzero\".  */\n+\tslli\txh, xh, 12\n+\tsrli\txh, xh, 12\n+\tbnone\tyh, a6, .Lsub_xexpdiff\n+\taddi\ta7, a7, 1\n+\tj\t.Lsub_xexpdiff\n+\n+.Lsub_bigshiftx:\n+\t/* Mostly the same thing as \"bigshifty\", but with the sign bit of the\n+\t   shifted value set so that the subsequent subtraction flips the\n+\t   sign of y.  */\n+\tbgeui\ta10, 64, .Lsub_returny\n+\n+\tssr\ta10\n+\tsll\ta11, xl\n+\tsrc\ta9, xh, xl\n+\tsrl\txl, xh\n+\tslli\txh, a6, 11\t/* set sign bit of xh */\n+\tbeqz\ta11, .Lsub_subx\n+\tor\ta9, a9, a10\n+\tj\t.Lsub_subx\n+\n+.Lsub_returny:\n+\t/* Negate and return y.  */\n+\tslli\ta7, a6, 11\n+\txor\txh, yh, a7\n+\tmov\txl, yl\n+\tabi_return\n+\n+.Lsub_borrow:\t\n+\t/* The subtraction has underflowed into the exponent field, so the\n+\t   value needs to be renormalized.  Shift the mantissa left as\n+\t   needed to remove any leading zeros and adjust the exponent\n+\t   accordingly.  If the exponent is not large enough to remove\n+\t   all the leading zeros, the result will be a subnormal value.  */\n+\n+\tslli\ta8, xh, 12\n+\tbeqz\ta8, .Lsub_xhzero\n+\tdo_nsau\ta6, a8, a7, a11\n+\tsrli\ta8, a8, 12\n+\tbge\ta6, a10, .Lsub_subnormal\n+\taddi\ta6, a6, 1\n+\n+.Lsub_shift_lt32:\n+\t/* Shift the mantissa (a8/xl/a9) left by a6.  */\n+\tssl\ta6\n+\tsrc\ta8, a8, xl\n+\tsrc\txl, xl, a9\n+\tsll\ta9, a9\n+\n+\t/* Combine the shifted mantissa with the sign and exponent,\n+\t   decrementing the exponent by a6.  (The exponent has already\n+\t   been decremented by one due to the borrow from the subtraction,\n+\t   but adding the mantissa will increment the exponent by one.)  */\n+\tsrli\txh, xh, 20\n+\tsub\txh, xh, a6\n+\tslli\txh, xh, 20\n+\tadd\txh, xh, a8\n+\tj\t.Lsub_round\n+\n+.Lsub_exactlyhalf:\n+\t/* Round down to the nearest even value.  */\n+\tsrli\txl, xl, 1\n+\tslli\txl, xl, 1\n+\tabi_return\n+\n+.Lsub_roundcarry:\n+\t/* xl is always zero when the rounding increment overflows, so\n+\t   there's no need to round it to an even value.  */\n+\taddi\txh, xh, 1\n+\t/* Overflow to the exponent is OK.  */\n+\tabi_return\n+\n+.Lsub_xhzero:\n+\t/* When normalizing the result, all the mantissa bits in the high\n+\t   word are zero.  Shift by \"20 + (leading zero count of xl) + 1\".  */\n+\tdo_nsau\ta6, xl, a7, a11\n+\taddi\ta6, a6, 21\n+\tblt\ta10, a6, .Lsub_subnormal\n+\n+.Lsub_normalize_shift:\n+\tbltui\ta6, 32, .Lsub_shift_lt32\n+\n+\tssl\ta6\n+\tsrc\ta8, xl, a9\n+\tsll\txl, a9\n+\tmovi\ta9, 0\n+\n+\tsrli\txh, xh, 20\n+\tsub\txh, xh, a6\n+\tslli\txh, xh, 20\n+\tadd\txh, xh, a8\n+\tj\t.Lsub_round\n+\n+.Lsub_subnormal:\n+\t/* The exponent is too small to shift away all the leading zeros.\n+\t   Set a6 to the current exponent (which has already been\n+\t   decremented by the borrow) so that the exponent of the result\n+\t   will be zero.  Do not add 1 to a6 in this case, because: (1)\n+\t   adding the mantissa will not increment the exponent, so there is\n+\t   no need to subtract anything extra from the exponent to\n+\t   compensate, and (2) the effective exponent of a subnormal is 1\n+\t   not 0 so the shift amount must be 1 smaller than normal. */\n+\tmov\ta6, a10\n+\tj\t.Lsub_normalize_shift\n+\n+#endif /* L_addsubdf3 */\n+\n+#ifdef L_muldf3\n+\n+\t/* Multiplication */\n+__muldf3_aux:\n+\n+\t/* Handle unusual cases (zeros, subnormals, NaNs and Infinities).\n+\t   (This code is placed before the start of the function just to\n+\t   keep it in range of the limited branch displacements.)  */\n+\n+.Lmul_xexpzero:\n+\t/* Clear the sign bit of x.  */\n+\tslli\txh, xh, 1\n+\tsrli\txh, xh, 1\n+\n+\t/* If x is zero, return zero.  */\n+\tor\ta10, xh, xl\n+\tbeqz\ta10, .Lmul_return_zero\n+\n+\t/* Normalize x.  Adjust the exponent in a8.  */\n+\tbeqz\txh, .Lmul_xh_zero\n+\tdo_nsau\ta10, xh, a11, a12\n+\taddi\ta10, a10, -11\n+\tssl\ta10\n+\tsrc\txh, xh, xl\n+\tsll\txl, xl\n+\tmovi\ta8, 1\n+\tsub\ta8, a8, a10\n+\tj\t.Lmul_xnormalized\t\n+.Lmul_xh_zero:\n+\tdo_nsau\ta10, xl, a11, a12\n+\taddi\ta10, a10, -11\n+\tmovi\ta8, -31\n+\tsub\ta8, a8, a10\n+\tssl\ta10\n+\tbltz\ta10, .Lmul_xl_srl\n+\tsll\txh, xl\n+\tmovi\txl, 0\n+\tj\t.Lmul_xnormalized\n+.Lmul_xl_srl:\n+\tsrl\txh, xl\n+\tsll\txl, xl\n+\tj\t.Lmul_xnormalized\n+\t\n+.Lmul_yexpzero:\n+\t/* Clear the sign bit of y.  */\n+\tslli\tyh, yh, 1\n+\tsrli\tyh, yh, 1\n+\n+\t/* If y is zero, return zero.  */\n+\tor\ta10, yh, yl\n+\tbeqz\ta10, .Lmul_return_zero\n+\n+\t/* Normalize y.  Adjust the exponent in a9.  */\n+\tbeqz\tyh, .Lmul_yh_zero\n+\tdo_nsau\ta10, yh, a11, a12\n+\taddi\ta10, a10, -11\n+\tssl\ta10\n+\tsrc\tyh, yh, yl\n+\tsll\tyl, yl\n+\tmovi\ta9, 1\n+\tsub\ta9, a9, a10\n+\tj\t.Lmul_ynormalized\t\n+.Lmul_yh_zero:\n+\tdo_nsau\ta10, yl, a11, a12\n+\taddi\ta10, a10, -11\n+\tmovi\ta9, -31\n+\tsub\ta9, a9, a10\n+\tssl\ta10\n+\tbltz\ta10, .Lmul_yl_srl\n+\tsll\tyh, yl\n+\tmovi\tyl, 0\n+\tj\t.Lmul_ynormalized\n+.Lmul_yl_srl:\n+\tsrl\tyh, yl\n+\tsll\tyl, yl\n+\tj\t.Lmul_ynormalized\t\n+\n+.Lmul_return_zero:\n+\t/* Return zero with the appropriate sign bit.  */\n+\tsrli\txh, a7, 31\n+\tslli\txh, xh, 31\n+\tmovi\txl, 0\n+\tj\t.Lmul_done\n+\n+.Lmul_xnan_or_inf:\n+\t/* If y is zero, return NaN.  */\n+\tbnez\tyl, 1f\n+\tslli\ta8, yh, 1\n+\tbnez\ta8, 1f\n+\tmovi\ta4, 0x80000\t/* make it a quiet NaN */\n+\tor\txh, xh, a4\n+\tj\t.Lmul_done\n+1:\n+\t/* If y is NaN, return y.  */\n+\tbnall\tyh, a6, .Lmul_returnx\n+\tslli\ta8, yh, 12\n+\tor\ta8, a8, yl\n+\tbeqz\ta8, .Lmul_returnx\n+\n+.Lmul_returny:\n+\tmov\txh, yh\n+\tmov\txl, yl\n+\n+.Lmul_returnx:\n+\t/* Set the sign bit and return.  */\n+\textui\ta7, a7, 31, 1\n+\tslli\txh, xh, 1\n+\tssai\t1\n+\tsrc\txh, a7, xh\n+\tj\t.Lmul_done\n+\n+.Lmul_ynan_or_inf:\n+\t/* If x is zero, return NaN.  */\n+\tbnez\txl, .Lmul_returny\n+\tslli\ta8, xh, 1\n+\tbnez\ta8, .Lmul_returny\n+\tmovi\ta7, 0x80000\t/* make it a quiet NaN */\n+\tor\txh, yh, a7\n+\tj\t.Lmul_done\n+\n+\t.align\t4\n+\t.global\t__muldf3\n+\t.type\t__muldf3, @function\n+__muldf3:\n+\tabi_entry sp, 48\n+#if __XTENSA_CALL0_ABI__\n+\taddi\tsp, sp, -32\n+\ts32i\ta12, sp, 16\n+\ts32i\ta13, sp, 20\n+\ts32i\ta14, sp, 24\n+\ts32i\ta15, sp, 28\n+#endif\n+\tmovi\ta6, 0x7ff00000\n+\n+\t/* Get the sign of the result.  */\n+\txor\ta7, xh, yh\n+\n+\t/* Check for NaN and infinity.  */\n+\tball\txh, a6, .Lmul_xnan_or_inf\n+\tball\tyh, a6, .Lmul_ynan_or_inf\n+\n+\t/* Extract the exponents.  */\n+\textui\ta8, xh, 20, 11\n+\textui\ta9, yh, 20, 11\n+\n+\tbeqz\ta8, .Lmul_xexpzero\n+.Lmul_xnormalized:\t\n+\tbeqz\ta9, .Lmul_yexpzero\n+.Lmul_ynormalized:\t\n+\n+\t/* Add the exponents.  */\n+\tadd\ta8, a8, a9\n+\n+\t/* Replace sign/exponent fields with explicit \"1.0\".  */\n+\tmovi\ta10, 0x1fffff\n+\tor\txh, xh, a6\n+\tand\txh, xh, a10\n+\tor\tyh, yh, a6\n+\tand\tyh, yh, a10\n+\n+\t/* Multiply 64x64 to 128 bits.  The result ends up in xh/xl/a6.\n+\t   The least-significant word of the result is thrown away except\n+\t   that if it is nonzero, the lsb of a6 is set to 1.  */\n+#if XCHAL_HAVE_MUL32_HIGH\n+\n+\t/* Compute a6 with any carry-outs in a10.  */\n+\tmovi\ta10, 0\n+\tmull\ta6, xl, yh\n+\tmull\ta11, xh, yl\n+\tadd\ta6, a6, a11\n+\tbgeu\ta6, a11, 1f\n+\taddi\ta10, a10, 1\n+1:\n+\tmuluh\ta11, xl, yl\n+\tadd\ta6, a6, a11\n+\tbgeu\ta6, a11, 1f\n+\taddi\ta10, a10, 1\n+1:\t\n+\t/* If the low word of the result is nonzero, set the lsb of a6.  */\n+\tmull\ta11, xl, yl\n+\tbeqz\ta11, 1f\n+\tmovi\ta9, 1\n+\tor\ta6, a6, a9\n+1:\n+\t/* Compute xl with any carry-outs in a9.  */\n+\tmovi\ta9, 0\n+\tmull\ta11, xh, yh\n+\tadd\ta10, a10, a11\n+\tbgeu\ta10, a11, 1f\n+\taddi\ta9, a9, 1\n+1:\t\n+\tmuluh\ta11, xh, yl\n+\tadd\ta10, a10, a11\n+\tbgeu\ta10, a11, 1f\n+\taddi\ta9, a9, 1\n+1:\t\n+\tmuluh\txl, xl, yh\n+\tadd\txl, xl, a10\n+\tbgeu\txl, a10, 1f\n+\taddi\ta9, a9, 1\n+1:\n+\t/* Compute xh.  */\n+\tmuluh\txh, xh, yh\n+\tadd\txh, xh, a9\n+\n+#else\n+\n+\t/* Break the inputs into 16-bit chunks and compute 16 32-bit partial\n+\t   products.  These partial products are:\n+\n+\t\t0 xll * yll\n+\n+\t\t1 xll * ylh\n+\t\t2 xlh * yll\n+\n+\t\t3 xll * yhl\n+\t\t4 xlh * ylh\n+\t\t5 xhl * yll\n+\n+\t\t6 xll * yhh\n+\t\t7 xlh * yhl\n+\t\t8 xhl * ylh\n+\t\t9 xhh * yll\n+\n+\t\t10 xlh * yhh\n+\t\t11 xhl * yhl\n+\t\t12 xhh * ylh\n+\n+\t\t13 xhl * yhh\n+\t\t14 xhh * yhl\n+\n+\t\t15 xhh * yhh\n+\n+\t   where the input chunks are (hh, hl, lh, ll).  If using the Mul16\n+\t   or Mul32 multiplier options, these input chunks must be stored in\n+\t   separate registers.  For Mac16, the UMUL.AA.* opcodes can specify\n+\t   that the inputs come from either half of the registers, so there\n+\t   is no need to shift them out ahead of time.  If there is no\n+\t   multiply hardware, the 16-bit chunks can be extracted when setting\n+\t   up the arguments to the separate multiply function.  */\n+\n+\t/* Save a7 since it is needed to hold a temporary value.  */\n+\ts32i\ta7, sp, 4\n+#if !XCHAL_HAVE_MUL16 && !XCHAL_HAVE_MUL32 && !XCHAL_HAVE_MAC16\n+\t/* Calling a separate multiply function will clobber a0 and requires\n+\t   use of a8 as a temporary, so save those values now.  (The function\n+\t   uses a custom ABI so nothing else needs to be saved.)  */\n+\ts32i\ta0, sp, 0\n+\ts32i\ta8, sp, 8\n+#endif\n+\n+#if XCHAL_HAVE_MUL16 || XCHAL_HAVE_MUL32\n+\n+#define xlh a12\n+#define ylh a13\n+#define xhh a14\n+#define yhh a15\n+\n+\t/* Get the high halves of the inputs into registers.  */\n+\tsrli\txlh, xl, 16\n+\tsrli\tylh, yl, 16\n+\tsrli\txhh, xh, 16\n+\tsrli\tyhh, yh, 16\n+\n+#define xll xl\n+#define yll yl\n+#define xhl xh\n+#define yhl yh\n+\n+#if XCHAL_HAVE_MUL32 && !XCHAL_HAVE_MUL16\n+\t/* Clear the high halves of the inputs.  This does not matter\n+\t   for MUL16 because the high bits are ignored.  */\n+\textui\txl, xl, 0, 16\n+\textui\txh, xh, 0, 16\n+\textui\tyl, yl, 0, 16\n+\textui\tyh, yh, 0, 16\n+#endif\n+#endif /* MUL16 || MUL32 */\n+\n+\n+#if XCHAL_HAVE_MUL16\n+\n+#define do_mul(dst, xreg, xhalf, yreg, yhalf) \\\n+\tmul16u\tdst, xreg ## xhalf, yreg ## yhalf\n+\n+#elif XCHAL_HAVE_MUL32\n+\n+#define do_mul(dst, xreg, xhalf, yreg, yhalf) \\\n+\tmull\tdst, xreg ## xhalf, yreg ## yhalf\n+\n+#elif XCHAL_HAVE_MAC16\n+\n+/* The preprocessor insists on inserting a space when concatenating after\n+   a period in the definition of do_mul below.  These macros are a workaround\n+   using underscores instead of periods when doing the concatenation.  */\n+#define umul_aa_ll umul.aa.ll\n+#define umul_aa_lh umul.aa.lh\n+#define umul_aa_hl umul.aa.hl\n+#define umul_aa_hh umul.aa.hh\n+\n+#define do_mul(dst, xreg, xhalf, yreg, yhalf) \\\n+\tumul_aa_ ## xhalf ## yhalf\txreg, yreg; \\\n+\trsr\tdst, ACCLO\n+\n+#else /* no multiply hardware */\n+\t\n+#define set_arg_l(dst, src) \\\n+\textui\tdst, src, 0, 16\n+#define set_arg_h(dst, src) \\\n+\tsrli\tdst, src, 16\n+\n+#define do_mul(dst, xreg, xhalf, yreg, yhalf) \\\n+\tset_arg_ ## xhalf (a13, xreg); \\\n+\tset_arg_ ## yhalf (a14, yreg); \\\n+\tcall0\t.Lmul_mulsi3; \\\n+\tmov\tdst, a12\n+#endif\n+\n+\t/* Add pp1 and pp2 into a10 with carry-out in a9.  */\n+\tdo_mul(a10, xl, l, yl, h)\t/* pp 1 */\n+\tdo_mul(a11, xl, h, yl, l)\t/* pp 2 */\n+\tmovi\ta9, 0\n+\tadd\ta10, a10, a11\n+\tbgeu\ta10, a11, 1f\n+\taddi\ta9, a9, 1\n+1:\n+\t/* Initialize a6 with a9/a10 shifted into position.  Note that\n+\t   this value can be safely incremented without any carry-outs.  */\n+\tssai\t16\n+\tsrc\ta6, a9, a10\n+\n+\t/* Compute the low word into a10.  */\n+\tdo_mul(a11, xl, l, yl, l)\t/* pp 0 */\n+\tsll\ta10, a10\n+\tadd\ta10, a10, a11\n+\tbgeu\ta10, a11, 1f\n+\taddi\ta6, a6, 1\n+1:\n+\t/* Compute the contributions of pp0-5 to a6, with carry-outs in a9.\n+\t   This is good enough to determine the low half of a6, so that any\n+\t   nonzero bits from the low word of the result can be collapsed\n+\t   into a6, freeing up a register.  */\n+\tmovi\ta9, 0\n+\tdo_mul(a11, xl, l, yh, l)\t/* pp 3 */\n+\tadd\ta6, a6, a11\n+\tbgeu\ta6, a11, 1f\n+\taddi\ta9, a9, 1\n+1:\n+\tdo_mul(a11, xl, h, yl, h)\t/* pp 4 */\n+\tadd\ta6, a6, a11\n+\tbgeu\ta6, a11, 1f\n+\taddi\ta9, a9, 1\n+1:\n+\tdo_mul(a11, xh, l, yl, l)\t/* pp 5 */\n+\tadd\ta6, a6, a11\n+\tbgeu\ta6, a11, 1f\n+\taddi\ta9, a9, 1\n+1:\n+\t/* Collapse any nonzero bits from the low word into a6.  */\n+\tbeqz\ta10, 1f\n+\tmovi\ta11, 1\n+\tor\ta6, a6, a11\n+1:\n+\t/* Add pp6-9 into a11 with carry-outs in a10.  */\n+\tdo_mul(a7, xl, l, yh, h)\t/* pp 6 */\n+\tdo_mul(a11, xh, h, yl, l)\t/* pp 9 */\n+\tmovi\ta10, 0\n+\tadd\ta11, a11, a7\n+\tbgeu\ta11, a7, 1f\n+\taddi\ta10, a10, 1\n+1:\t\n+\tdo_mul(a7, xl, h, yh, l)\t/* pp 7 */\n+\tadd\ta11, a11, a7\n+\tbgeu\ta11, a7, 1f\n+\taddi\ta10, a10, 1\n+1:\t\n+\tdo_mul(a7, xh, l, yl, h)\t/* pp 8 */\n+\tadd\ta11, a11, a7\n+\tbgeu\ta11, a7, 1f\n+\taddi\ta10, a10, 1\n+1:\t\n+\t/* Shift a10/a11 into position, and add low half of a11 to a6.  */\n+\tsrc\ta10, a10, a11\n+\tadd\ta10, a10, a9\n+\tsll\ta11, a11\n+\tadd\ta6, a6, a11\n+\tbgeu\ta6, a11, 1f\n+\taddi\ta10, a10, 1\n+1:\n+\t/* Add pp10-12 into xl with carry-outs in a9.  */\n+\tmovi\ta9, 0\n+\tdo_mul(xl, xl, h, yh, h)\t/* pp 10 */\n+\tadd\txl, xl, a10\n+\tbgeu\txl, a10, 1f\n+\taddi\ta9, a9, 1\n+1:\n+\tdo_mul(a10, xh, l, yh, l)\t/* pp 11 */\n+\tadd\txl, xl, a10\n+\tbgeu\txl, a10, 1f\n+\taddi\ta9, a9, 1\n+1:\n+\tdo_mul(a10, xh, h, yl, h)\t/* pp 12 */\n+\tadd\txl, xl, a10\n+\tbgeu\txl, a10, 1f\n+\taddi\ta9, a9, 1\n+1:\n+\t/* Add pp13-14 into a11 with carry-outs in a10.  */\n+\tdo_mul(a11, xh, l, yh, h)\t/* pp 13 */\n+\tdo_mul(a7, xh, h, yh, l)\t/* pp 14 */\n+\tmovi\ta10, 0\n+\tadd\ta11, a11, a7\n+\tbgeu\ta11, a7, 1f\n+\taddi\ta10, a10, 1\n+1:\n+\t/* Shift a10/a11 into position, and add low half of a11 to a6.  */\n+\tsrc\ta10, a10, a11\n+\tadd\ta10, a10, a9\n+\tsll\ta11, a11\n+\tadd\txl, xl, a11\n+\tbgeu\txl, a11, 1f\n+\taddi\ta10, a10, 1\n+1:\n+\t/* Compute xh.  */\n+\tdo_mul(xh, xh, h, yh, h)\t/* pp 15 */\n+\tadd\txh, xh, a10\n+\n+\t/* Restore values saved on the stack during the multiplication.  */\n+\tl32i\ta7, sp, 4\n+#if !XCHAL_HAVE_MUL16 && !XCHAL_HAVE_MUL32 && !XCHAL_HAVE_MAC16\n+\tl32i\ta0, sp, 0\n+\tl32i\ta8, sp, 8\n+#endif\n+#endif\n+\n+\t/* Shift left by 12 bits, unless there was a carry-out from the\n+\t   multiply, in which case, shift by 11 bits and increment the\n+\t   exponent.  Note: It is convenient to use the constant 0x3ff\n+\t   instead of 0x400 when removing the extra exponent bias (so that\n+\t   it is easy to construct 0x7fe for the overflow check).  Reverse\n+\t   the logic here to decrement the exponent sum by one unless there\n+\t   was a carry-out.  */\n+\tmovi\ta4, 11\n+\tsrli\ta5, xh, 21 - 12\n+\tbnez\ta5, 1f\n+\taddi\ta4, a4, 1\n+\taddi\ta8, a8, -1\n+1:\tssl\ta4\n+\tsrc\txh, xh, xl\n+\tsrc\txl, xl, a6\n+\tsll\ta6, a6\n+\n+\t/* Subtract the extra bias from the exponent sum (plus one to account\n+\t   for the explicit \"1.0\" of the mantissa that will be added to the\n+\t   exponent in the final result).  */\n+\tmovi\ta4, 0x3ff\n+\tsub\ta8, a8, a4\n+\t\n+\t/* Check for over/underflow.  The value in a8 is one less than the\n+\t   final exponent, so values in the range 0..7fd are OK here.  */\n+\tslli\ta4, a4, 1\t/* 0x7fe */\n+\tbgeu\ta8, a4, .Lmul_overflow\n+\t\n+.Lmul_round:\n+\t/* Round.  */\n+\tbgez\ta6, .Lmul_rounded\n+\taddi\txl, xl, 1\n+\tbeqz\txl, .Lmul_roundcarry\n+\tslli\ta6, a6, 1\n+\tbeqz\ta6, .Lmul_exactlyhalf\n+\n+.Lmul_rounded:\n+\t/* Add the exponent to the mantissa.  */\n+\tslli\ta8, a8, 20\n+\tadd\txh, xh, a8\n+\n+.Lmul_addsign:\n+\t/* Add the sign bit.  */\n+\tsrli\ta7, a7, 31\n+\tslli\ta7, a7, 31\n+\tor\txh, xh, a7\n+\n+.Lmul_done:\n+#if __XTENSA_CALL0_ABI__\n+\tl32i\ta12, sp, 16\n+\tl32i\ta13, sp, 20\n+\tl32i\ta14, sp, 24\n+\tl32i\ta15, sp, 28\n+\taddi\tsp, sp, 32\n+#endif\n+\tabi_return\n+\n+.Lmul_exactlyhalf:\n+\t/* Round down to the nearest even value.  */\n+\tsrli\txl, xl, 1\n+\tslli\txl, xl, 1\n+\tj\t.Lmul_rounded\n+\n+.Lmul_roundcarry:\n+\t/* xl is always zero when the rounding increment overflows, so\n+\t   there's no need to round it to an even value.  */\n+\taddi\txh, xh, 1\n+\t/* Overflow is OK -- it will be added to the exponent.  */\n+\tj\t.Lmul_rounded\n+\n+.Lmul_overflow:\n+\tbltz\ta8, .Lmul_underflow\n+\t/* Return +/- Infinity.  */\n+\taddi\ta8, a4, 1\t/* 0x7ff */\n+\tslli\txh, a8, 20\n+\tmovi\txl, 0\n+\tj\t.Lmul_addsign\n+\n+.Lmul_underflow:\n+\t/* Create a subnormal value, where the exponent field contains zero,\n+\t   but the effective exponent is 1.  The value of a8 is one less than\n+\t   the actual exponent, so just negate it to get the shift amount.  */\n+\tneg\ta8, a8\n+\tmov\ta9, a6\n+\tssr\ta8\n+\tbgeui\ta8, 32, .Lmul_bigshift\n+\t\n+\t/* Shift xh/xl right.  Any bits that are shifted out of xl are saved\n+\t   in a6 (combined with the shifted-out bits currently in a6) for\n+\t   rounding the result.  */\n+\tsll\ta6, xl\n+\tsrc\txl, xh, xl\n+\tsrl\txh, xh\n+\tj\t1f\n+\n+.Lmul_bigshift:\n+\tbgeui\ta8, 64, .Lmul_flush_to_zero\n+\tsll\ta10, xl\t\t/* lost bits shifted out of xl */\n+\tsrc\ta6, xh, xl\n+\tsrl\txl, xh\n+\tmovi\txh, 0\n+\tor\ta9, a9, a10\n+\n+\t/* Set the exponent to zero.  */\n+1:\tmovi\ta8, 0\n+\n+\t/* Pack any nonzero bits shifted out into a6.  */\n+\tbeqz\ta9, .Lmul_round\n+\tmovi\ta9, 1\n+\tor\ta6, a6, a9\n+\tj\t.Lmul_round\n+\t\n+.Lmul_flush_to_zero:\n+\t/* Return zero with the appropriate sign bit.  */\n+\tsrli\txh, a7, 31\n+\tslli\txh, xh, 31\n+\tmovi\txl, 0\n+\tj\t.Lmul_done\n+\n+#if !XCHAL_HAVE_MUL16 && !XCHAL_HAVE_MUL32 && !XCHAL_HAVE_MAC16\n+\t\n+\t/* For Xtensa processors with no multiply hardware, this simplified\n+\t   version of _mulsi3 is used for multiplying 16-bit chunks of\n+\t   the floating-point mantissas.  It uses a custom ABI:\tthe inputs\n+\t   are passed in a13 and a14, the result is returned in a12, and\n+\t   a8 and a15 are clobbered.  */\n+\t.align\t4\n+.Lmul_mulsi3:\n+\tmovi\ta12, 0\n+.Lmul_mult_loop:\n+\tadd\ta15, a14, a12\n+\textui\ta8, a13, 0, 1\n+\tmovnez\ta12, a15, a8\n+\n+\tdo_addx2 a15, a14, a12, a15\n+\textui\ta8, a13, 1, 1\n+\tmovnez\ta12, a15, a8\n+\n+\tdo_addx4 a15, a14, a12, a15\n+\textui\ta8, a13, 2, 1\n+\tmovnez\ta12, a15, a8\n+\n+\tdo_addx8 a15, a14, a12, a15\n+\textui\ta8, a13, 3, 1\n+\tmovnez\ta12, a15, a8\n+\n+\tsrli\ta13, a13, 4\n+\tslli\ta14, a14, 4\n+\tbnez\ta13, .Lmul_mult_loop\n+\tret\n+#endif /* !MUL16 && !MUL32 && !MAC16 */\n+#endif /* L_muldf3 */\n+\n+#ifdef L_divdf3\n+\n+\t/* Division */\n+__divdf3_aux:\n+\n+\t/* Handle unusual cases (zeros, subnormals, NaNs and Infinities).\n+\t   (This code is placed before the start of the function just to\n+\t   keep it in range of the limited branch displacements.)  */\n+\n+.Ldiv_yexpzero:\n+\t/* Clear the sign bit of y.  */\n+\tslli\tyh, yh, 1\n+\tsrli\tyh, yh, 1\n+\n+\t/* Check for division by zero.  */\n+\tor\ta10, yh, yl\n+\tbeqz\ta10, .Ldiv_yzero\n+\n+\t/* Normalize y.  Adjust the exponent in a9.  */\n+\tbeqz\tyh, .Ldiv_yh_zero\n+\tdo_nsau\ta10, yh, a11, a9\n+\taddi\ta10, a10, -11\n+\tssl\ta10\n+\tsrc\tyh, yh, yl\n+\tsll\tyl, yl\n+\tmovi\ta9, 1\n+\tsub\ta9, a9, a10\n+\tj\t.Ldiv_ynormalized\t\n+.Ldiv_yh_zero:\n+\tdo_nsau\ta10, yl, a11, a9\n+\taddi\ta10, a10, -11\n+\tmovi\ta9, -31\n+\tsub\ta9, a9, a10\n+\tssl\ta10\n+\tbltz\ta10, .Ldiv_yl_srl\n+\tsll\tyh, yl\n+\tmovi\tyl, 0\n+\tj\t.Ldiv_ynormalized\n+.Ldiv_yl_srl:\n+\tsrl\tyh, yl\n+\tsll\tyl, yl\n+\tj\t.Ldiv_ynormalized\t\n+\n+.Ldiv_yzero:\n+\t/* y is zero.  Return NaN if x is also zero; otherwise, infinity.  */\n+\tslli\txh, xh, 1\n+\tsrli\txh, xh, 1\n+\tor\txl, xl, xh\n+\tsrli\txh, a7, 31\n+\tslli\txh, xh, 31\n+\tor\txh, xh, a6\n+\tbnez\txl, 1f\n+\tmovi\ta4, 0x80000\t/* make it a quiet NaN */\n+\tor\txh, xh, a4\n+1:\tmovi\txl, 0\n+\tabi_return\n+\n+.Ldiv_xexpzero:\n+\t/* Clear the sign bit of x.  */\n+\tslli\txh, xh, 1\n+\tsrli\txh, xh, 1\n+\n+\t/* If x is zero, return zero.  */\n+\tor\ta10, xh, xl\n+\tbeqz\ta10, .Ldiv_return_zero\n+\n+\t/* Normalize x.  Adjust the exponent in a8.  */\n+\tbeqz\txh, .Ldiv_xh_zero\n+\tdo_nsau\ta10, xh, a11, a8\n+\taddi\ta10, a10, -11\n+\tssl\ta10\n+\tsrc\txh, xh, xl\n+\tsll\txl, xl\n+\tmovi\ta8, 1\n+\tsub\ta8, a8, a10\n+\tj\t.Ldiv_xnormalized\t\n+.Ldiv_xh_zero:\n+\tdo_nsau\ta10, xl, a11, a8\n+\taddi\ta10, a10, -11\n+\tmovi\ta8, -31\n+\tsub\ta8, a8, a10\n+\tssl\ta10\n+\tbltz\ta10, .Ldiv_xl_srl\n+\tsll\txh, xl\n+\tmovi\txl, 0\n+\tj\t.Ldiv_xnormalized\n+.Ldiv_xl_srl:\n+\tsrl\txh, xl\n+\tsll\txl, xl\n+\tj\t.Ldiv_xnormalized\n+\t\n+.Ldiv_return_zero:\n+\t/* Return zero with the appropriate sign bit.  */\n+\tsrli\txh, a7, 31\n+\tslli\txh, xh, 31\n+\tmovi\txl, 0\n+\tabi_return\n+\n+.Ldiv_xnan_or_inf:\n+\t/* Set the sign bit of the result.  */\n+\tsrli\ta7, yh, 31\n+\tslli\ta7, a7, 31\n+\txor\txh, xh, a7\n+\t/* If y is NaN or Inf, return NaN.  */\n+\tbnall\tyh, a6, 1f\n+\tmovi\ta4, 0x80000\t/* make it a quiet NaN */\n+\tor\txh, xh, a4\n+1:\tabi_return\n+\n+.Ldiv_ynan_or_inf:\n+\t/* If y is Infinity, return zero.  */\n+\tslli\ta8, yh, 12\n+\tor\ta8, a8, yl\n+\tbeqz\ta8, .Ldiv_return_zero\n+\t/* y is NaN; return it.  */\n+\tmov\txh, yh\n+\tmov\txl, yl\n+\tabi_return\n+\n+.Ldiv_highequal1:\n+\tbltu\txl, yl, 2f\n+\tj\t3f\n+\n+\t.align\t4\n+\t.global\t__divdf3\n+\t.type\t__divdf3, @function\n+__divdf3:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7ff00000\n+\n+\t/* Get the sign of the result.  */\n+\txor\ta7, xh, yh\n+\n+\t/* Check for NaN and infinity.  */\n+\tball\txh, a6, .Ldiv_xnan_or_inf\n+\tball\tyh, a6, .Ldiv_ynan_or_inf\n+\n+\t/* Extract the exponents.  */\n+\textui\ta8, xh, 20, 11\n+\textui\ta9, yh, 20, 11\n+\n+\tbeqz\ta9, .Ldiv_yexpzero\n+.Ldiv_ynormalized:\t\n+\tbeqz\ta8, .Ldiv_xexpzero\n+.Ldiv_xnormalized:\t\n+\n+\t/* Subtract the exponents.  */\n+\tsub\ta8, a8, a9\n+\n+\t/* Replace sign/exponent fields with explicit \"1.0\".  */\n+\tmovi\ta10, 0x1fffff\n+\tor\txh, xh, a6\n+\tand\txh, xh, a10\n+\tor\tyh, yh, a6\n+\tand\tyh, yh, a10\n+\n+\t/* Set SAR for left shift by one.  */\n+\tssai\t(32 - 1)\n+\n+\t/* The first digit of the mantissa division must be a one.\n+\t   Shift x (and adjust the exponent) as needed to make this true.  */\n+\tbltu\tyh, xh, 3f\n+\tbeq\tyh, xh, .Ldiv_highequal1\n+2:\tsrc\txh, xh, xl\n+\tsll\txl, xl\n+\taddi\ta8, a8, -1\n+3:\n+\t/* Do the first subtraction and shift.  */\n+\tsub\txh, xh, yh\n+\tbgeu\txl, yl, 1f\n+\taddi\txh, xh, -1\n+1:\tsub\txl, xl, yl\n+\tsrc\txh, xh, xl\n+\tsll\txl, xl\n+\n+\t/* Put the quotient into a10/a11.  */\n+\tmovi\ta10, 0\n+\tmovi\ta11, 1\n+\n+\t/* Divide one bit at a time for 52 bits.  */\n+\tmovi\ta9, 52\n+#if XCHAL_HAVE_LOOPS\n+\tloop\ta9, .Ldiv_loopend\n+#endif\n+.Ldiv_loop:\n+\t/* Shift the quotient << 1.  */\n+\tsrc\ta10, a10, a11\n+\tsll\ta11, a11\n+\n+\t/* Is this digit a 0 or 1?  */\n+\tbltu\txh, yh, 3f\n+\tbeq\txh, yh, .Ldiv_highequal2\n+\n+\t/* Output a 1 and subtract.  */\n+2:\taddi\ta11, a11, 1\n+\tsub\txh, xh, yh\n+\tbgeu\txl, yl, 1f\n+\taddi\txh, xh, -1\n+1:\tsub\txl, xl, yl\n+\n+\t/* Shift the dividend << 1.  */\n+3:\tsrc\txh, xh, xl\n+\tsll\txl, xl\n+\n+#if !XCHAL_HAVE_LOOPS\n+\taddi\ta9, a9, -1\n+\tbnez\ta9, .Ldiv_loop\n+#endif\n+.Ldiv_loopend:\n+\n+\t/* Add the exponent bias (less one to account for the explicit \"1.0\"\n+\t   of the mantissa that will be added to the exponent in the final\n+\t   result).  */\n+\tmovi\ta9, 0x3fe\n+\tadd\ta8, a8, a9\n+\t\n+\t/* Check for over/underflow.  The value in a8 is one less than the\n+\t   final exponent, so values in the range 0..7fd are OK here.  */\n+\taddmi\ta9, a9, 0x400\t/* 0x7fe */\n+\tbgeu\ta8, a9, .Ldiv_overflow\n+\n+.Ldiv_round:\n+\t/* Round.  The remainder (<< 1) is in xh/xl.  */\n+\tbltu\txh, yh, .Ldiv_rounded\n+\tbeq\txh, yh, .Ldiv_highequal3\n+.Ldiv_roundup:\n+\taddi\ta11, a11, 1\n+\tbeqz\ta11, .Ldiv_roundcarry\n+\n+.Ldiv_rounded:\n+\tmov\txl, a11\n+\t/* Add the exponent to the mantissa.  */\n+\tslli\ta8, a8, 20\n+\tadd\txh, a10, a8\n+\n+.Ldiv_addsign:\n+\t/* Add the sign bit.  */\n+\tsrli\ta7, a7, 31\n+\tslli\ta7, a7, 31\n+\tor\txh, xh, a7\n+\tabi_return\n+\n+.Ldiv_highequal2:\n+\tbgeu\txl, yl, 2b\n+\tj\t3b\n+\n+.Ldiv_highequal3:\n+\tbltu\txl, yl, .Ldiv_rounded\n+\tbne\txl, yl, .Ldiv_roundup\n+\n+\t/* Remainder is exactly half the divisor.  Round even.  */\n+\taddi\ta11, a11, 1\n+\tbeqz\ta11, .Ldiv_roundcarry\n+\tsrli\ta11, a11, 1\n+\tslli\ta11, a11, 1\n+\tj\t.Ldiv_rounded\n+\n+.Ldiv_overflow:\n+\tbltz\ta8, .Ldiv_underflow\n+\t/* Return +/- Infinity.  */\n+\taddi\ta8, a9, 1\t/* 0x7ff */\n+\tslli\txh, a8, 20\n+\tmovi\txl, 0\n+\tj\t.Ldiv_addsign\n+\n+.Ldiv_underflow:\n+\t/* Create a subnormal value, where the exponent field contains zero,\n+\t   but the effective exponent is 1.  The value of a8 is one less than\n+\t   the actual exponent, so just negate it to get the shift amount.  */\n+\tneg\ta8, a8\n+\tssr\ta8\n+\tbgeui\ta8, 32, .Ldiv_bigshift\n+\t\n+\t/* Shift a10/a11 right.  Any bits that are shifted out of a11 are\n+\t   saved in a6 for rounding the result.  */\n+\tsll\ta6, a11\n+\tsrc\ta11, a10, a11\n+\tsrl\ta10, a10\n+\tj\t1f\n+\n+.Ldiv_bigshift:\n+\tbgeui\ta8, 64, .Ldiv_flush_to_zero\n+\tsll\ta9, a11\t\t/* lost bits shifted out of a11 */\n+\tsrc\ta6, a10, a11\n+\tsrl\ta11, a10\n+\tmovi\ta10, 0\n+\tor\txl, xl, a9\n+\n+\t/* Set the exponent to zero.  */\n+1:\tmovi\ta8, 0\n+\n+\t/* Pack any nonzero remainder (in xh/xl) into a6.  */\n+\tor\txh, xh, xl\n+\tbeqz\txh, 1f\n+\tmovi\ta9, 1\n+\tor\ta6, a6, a9\n+\t\n+\t/* Round a10/a11 based on the bits shifted out into a6.  */\n+1:\tbgez\ta6, .Ldiv_rounded\n+\taddi\ta11, a11, 1\n+\tbeqz\ta11, .Ldiv_roundcarry\n+\tslli\ta6, a6, 1\n+\tbnez\ta6, .Ldiv_rounded\n+\tsrli\ta11, a11, 1\n+\tslli\ta11, a11, 1\n+\tj\t.Ldiv_rounded\n+\n+.Ldiv_roundcarry:\n+\t/* a11 is always zero when the rounding increment overflows, so\n+\t   there's no need to round it to an even value.  */\n+\taddi\ta10, a10, 1\n+\t/* Overflow to the exponent field is OK.  */\n+\tj\t.Ldiv_rounded\n+\n+.Ldiv_flush_to_zero:\n+\t/* Return zero with the appropriate sign bit.  */\n+\tsrli\txh, a7, 31\n+\tslli\txh, xh, 31\n+\tmovi\txl, 0\n+\tabi_return\n+\n+#endif /* L_divdf3 */\n+\n+#ifdef L_cmpdf2\n+\n+\t/* Equal and Not Equal */\n+\n+\t.align\t4\n+\t.global\t__eqdf2\n+\t.global\t__nedf2\n+\t.set\t__nedf2, __eqdf2\n+\t.type\t__eqdf2, @function\n+__eqdf2:\n+\tabi_entry sp, 32\n+\tbne\txl, yl, 2f\n+\tbne\txh, yh, 4f\n+\n+\t/* The values are equal but NaN != NaN.  Check the exponent.  */\n+\tmovi\ta6, 0x7ff00000\n+\tball\txh, a6, 3f\n+\n+\t/* Equal.  */\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+\t/* Not equal.  */\n+2:\tmovi\ta2, 1\n+\tabi_return\n+\n+\t/* Check if the mantissas are nonzero.  */\n+3:\tslli\ta7, xh, 12\n+\tor\ta7, a7, xl\n+\tj\t5f\n+\n+\t/* Check if x and y are zero with different signs.  */\n+4:\tor\ta7, xh, yh\n+\tslli\ta7, a7, 1\n+\tor\ta7, a7, xl\t/* xl == yl here */\n+\n+\t/* Equal if a7 == 0, where a7 is either abs(x | y) or the mantissa\n+\t   or x when exponent(x) = 0x7ff and x == y.  */\n+5:\tmovi\ta2, 0\n+\tmovi\ta3, 1\n+\tmovnez\ta2, a3, a7\t\n+\tabi_return\n+\n+\n+\t/* Greater Than */\n+\n+\t.align\t4\n+\t.global\t__gtdf2\n+\t.type\t__gtdf2, @function\n+__gtdf2:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7ff00000\n+\tball\txh, a6, 2f\n+1:\tbnall\tyh, a6, .Lle_cmp\n+\n+\t/* Check if y is a NaN.  */\n+\tslli\ta7, yh, 12\n+\tor\ta7, a7, yl\n+\tbeqz\ta7, .Lle_cmp\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+\t/* Check if x is a NaN.  */\n+2:\tslli\ta7, xh, 12\n+\tor\ta7, a7, xl\n+\tbeqz\ta7, 1b\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+\n+\t/* Less Than or Equal */\n+\n+\t.align\t4\n+\t.global\t__ledf2\n+\t.type\t__ledf2, @function\n+__ledf2:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7ff00000\n+\tball\txh, a6, 2f\n+1:\tbnall\tyh, a6, .Lle_cmp\n+\n+\t/* Check if y is a NaN.  */\n+\tslli\ta7, yh, 12\n+\tor\ta7, a7, yl\n+\tbeqz\ta7, .Lle_cmp\n+\tmovi\ta2, 1\n+\tabi_return\n+\n+\t/* Check if x is a NaN.  */\n+2:\tslli\ta7, xh, 12\n+\tor\ta7, a7, xl\n+\tbeqz\ta7, 1b\n+\tmovi\ta2, 1\n+\tabi_return\n+\n+.Lle_cmp:\n+\t/* Check if x and y have different signs.  */\n+\txor\ta7, xh, yh\n+\tbltz\ta7, .Lle_diff_signs\n+\n+\t/* Check if x is negative.  */\n+\tbltz\txh, .Lle_xneg\n+\n+\t/* Check if x <= y.  */\n+\tbltu\txh, yh, 4f\n+\tbne\txh, yh, 5f\n+\tbltu\tyl, xl, 5f\n+4:\tmovi\ta2, 0\n+\tabi_return\n+\n+.Lle_xneg:\n+\t/* Check if y <= x.  */\n+\tbltu\tyh, xh, 4b\n+\tbne\tyh, xh, 5f\n+\tbgeu\txl, yl, 4b\n+5:\tmovi\ta2, 1\n+\tabi_return\n+\n+.Lle_diff_signs:\n+\tbltz\txh, 4b\n+\n+\t/* Check if both x and y are zero.  */\n+\tor\ta7, xh, yh\n+\tslli\ta7, a7, 1\n+\tor\ta7, a7, xl\n+\tor\ta7, a7, yl\n+\tmovi\ta2, 1\n+\tmovi\ta3, 0\n+\tmoveqz\ta2, a3, a7\n+\tabi_return\n+\n+\n+\t/* Greater Than or Equal */\n+\n+\t.align\t4\n+\t.global\t__gedf2\n+\t.type\t__gedf2, @function\n+__gedf2:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7ff00000\n+\tball\txh, a6, 2f\n+1:\tbnall\tyh, a6, .Llt_cmp\n+\n+\t/* Check if y is a NaN.  */\n+\tslli\ta7, yh, 12\n+\tor\ta7, a7, yl\n+\tbeqz\ta7, .Llt_cmp\n+\tmovi\ta2, -1\n+\tabi_return\n+\n+\t/* Check if x is a NaN.  */\n+2:\tslli\ta7, xh, 12\n+\tor\ta7, a7, xl\n+\tbeqz\ta7, 1b\n+\tmovi\ta2, -1\n+\tabi_return\n+\n+\n+\t/* Less Than */\n+\n+\t.align\t4\n+\t.global\t__ltdf2\n+\t.type\t__ltdf2, @function\n+__ltdf2:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7ff00000\n+\tball\txh, a6, 2f\n+1:\tbnall\tyh, a6, .Llt_cmp\n+\n+\t/* Check if y is a NaN.  */\n+\tslli\ta7, yh, 12\n+\tor\ta7, a7, yl\n+\tbeqz\ta7, .Llt_cmp\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+\t/* Check if x is a NaN.  */\n+2:\tslli\ta7, xh, 12\n+\tor\ta7, a7, xl\n+\tbeqz\ta7, 1b\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+.Llt_cmp:\n+\t/* Check if x and y have different signs.  */\n+\txor\ta7, xh, yh\n+\tbltz\ta7, .Llt_diff_signs\n+\n+\t/* Check if x is negative.  */\n+\tbltz\txh, .Llt_xneg\n+\n+\t/* Check if x < y.  */\n+\tbltu\txh, yh, 4f\n+\tbne\txh, yh, 5f\n+\tbgeu\txl, yl, 5f\n+4:\tmovi\ta2, -1\n+\tabi_return\n+\n+.Llt_xneg:\n+\t/* Check if y < x.  */\n+\tbltu\tyh, xh, 4b\n+\tbne\tyh, xh, 5f\n+\tbltu\tyl, xl, 4b\n+5:\tmovi\ta2, 0\n+\tabi_return\n+\n+.Llt_diff_signs:\n+\tbgez\txh, 5b\n+\n+\t/* Check if both x and y are nonzero.  */\n+\tor\ta7, xh, yh\n+\tslli\ta7, a7, 1\n+\tor\ta7, a7, xl\n+\tor\ta7, a7, yl\n+\tmovi\ta2, 0\n+\tmovi\ta3, -1\n+\tmovnez\ta2, a3, a7\n+\tabi_return\n+\n+\n+\t/* Unordered */\n+\n+\t.align\t4\n+\t.global\t__unorddf2\n+\t.type\t__unorddf2, @function\n+__unorddf2:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7ff00000\n+\tball\txh, a6, 3f\n+1:\tball\tyh, a6, 4f\n+2:\tmovi\ta2, 0\n+\tabi_return\n+\n+3:\tslli\ta7, xh, 12\n+\tor\ta7, a7, xl\n+\tbeqz\ta7, 1b\n+\tmovi\ta2, 1\n+\tabi_return\n+\n+4:\tslli\ta7, yh, 12\n+\tor\ta7, a7, yl\n+\tbeqz\ta7, 2b\n+\tmovi\ta2, 1\n+\tabi_return\n+\n+#endif /* L_cmpdf2 */\n+\n+#ifdef L_fixdfsi\n+\n+\t.align\t4\n+\t.global\t__fixdfsi\n+\t.type\t__fixdfsi, @function\n+__fixdfsi:\n+\tabi_entry sp, 32\n+\n+\t/* Check for NaN and Infinity.  */\n+\tmovi\ta6, 0x7ff00000\n+\tball\txh, a6, .Lfixdfsi_nan_or_inf\n+\n+\t/* Extract the exponent and check if 0 < (exp - 0x3fe) < 32.  */\n+\textui\ta4, xh, 20, 11\n+\textui\ta5, a6, 19, 10\t/* 0x3fe */\n+\tsub\ta4, a4, a5\n+\tbgei\ta4, 32, .Lfixdfsi_maxint\n+\tblti\ta4, 1, .Lfixdfsi_zero\n+\n+\t/* Add explicit \"1.0\" and shift << 11.  */\n+\tor\ta7, xh, a6\n+\tssai\t(32 - 11)\n+\tsrc\ta5, a7, xl\n+\n+\t/* Shift back to the right, based on the exponent.  */\n+\tssl\ta4\t\t/* shift by 32 - a4 */\n+\tsrl\ta5, a5\n+\n+\t/* Negate the result if sign != 0.  */\n+\tneg\ta2, a5\n+\tmovgez\ta2, a5, a7\n+\tabi_return\n+\n+.Lfixdfsi_nan_or_inf:\n+\t/* Handle Infinity and NaN.  */\n+\tslli\ta4, xh, 12\n+\tor\ta4, a4, xl\n+\tbeqz\ta4, .Lfixdfsi_maxint\n+\n+\t/* Translate NaN to +maxint.  */\n+\tmovi\txh, 0\n+\n+.Lfixdfsi_maxint:\n+\tslli\ta4, a6, 11\t/* 0x80000000 */\n+\taddi\ta5, a4, -1\t/* 0x7fffffff */\n+\tmovgez\ta4, a5, xh\n+\tmov\ta2, a4\n+\tabi_return\n+\n+.Lfixdfsi_zero:\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+#endif /* L_fixdfsi */\n+\n+#ifdef L_fixdfdi\n+\n+\t.align\t4\n+\t.global\t__fixdfdi\n+\t.type\t__fixdfdi, @function\n+__fixdfdi:\n+\tabi_entry sp, 32\n+\n+\t/* Check for NaN and Infinity.  */\n+\tmovi\ta6, 0x7ff00000\n+\tball\txh, a6, .Lfixdfdi_nan_or_inf\n+\n+\t/* Extract the exponent and check if 0 < (exp - 0x3fe) < 64.  */\n+\textui\ta4, xh, 20, 11\n+\textui\ta5, a6, 19, 10\t/* 0x3fe */\n+\tsub\ta4, a4, a5\n+\tbgei\ta4, 64, .Lfixdfdi_maxint\n+\tblti\ta4, 1, .Lfixdfdi_zero\n+\n+\t/* Add explicit \"1.0\" and shift << 11.  */\n+\tor\ta7, xh, a6\n+\tssai\t(32 - 11)\n+\tsrc\txh, a7, xl\n+\tsll\txl, xl\n+\n+\t/* Shift back to the right, based on the exponent.  */\n+\tssl\ta4\t\t/* shift by 64 - a4 */\n+\tbgei\ta4, 32, .Lfixdfdi_smallshift\n+\tsrl\txl, xh\n+\tmovi\txh, 0\n+\n+.Lfixdfdi_shifted:\t\n+\t/* Negate the result if sign != 0.  */\n+\tbgez\ta7, 1f\n+\tneg\txl, xl\n+\tneg\txh, xh\n+\tbeqz\txl, 1f\n+\taddi\txh, xh, -1\n+1:\tabi_return\n+\n+.Lfixdfdi_smallshift:\n+\tsrc\txl, xh, xl\n+\tsrl\txh, xh\n+\tj\t.Lfixdfdi_shifted\n+\n+.Lfixdfdi_nan_or_inf:\n+\t/* Handle Infinity and NaN.  */\n+\tslli\ta4, xh, 12\n+\tor\ta4, a4, xl\n+\tbeqz\ta4, .Lfixdfdi_maxint\n+\n+\t/* Translate NaN to +maxint.  */\n+\tmovi\txh, 0\n+\n+.Lfixdfdi_maxint:\n+\tslli\ta7, a6, 11\t/* 0x80000000 */\n+\tbgez\txh, 1f\n+\tmov\txh, a7\n+\tmovi\txl, 0\n+\tabi_return\n+\n+1:\taddi\txh, a7, -1\t/* 0x7fffffff */\n+\tmovi\txl, -1\n+\tabi_return\n+\n+.Lfixdfdi_zero:\n+\tmovi\txh, 0\n+\tmovi\txl, 0\n+\tabi_return\n+\n+#endif /* L_fixdfdi */\n+\n+#ifdef L_fixunsdfsi\n+\n+\t.align\t4\n+\t.global\t__fixunsdfsi\n+\t.type\t__fixunsdfsi, @function\n+__fixunsdfsi:\n+\tabi_entry sp, 32\n+\n+\t/* Check for NaN and Infinity.  */\n+\tmovi\ta6, 0x7ff00000\n+\tball\txh, a6, .Lfixunsdfsi_nan_or_inf\n+\n+\t/* Extract the exponent and check if 0 <= (exp - 0x3ff) < 32.  */\n+\textui\ta4, xh, 20, 11\n+\textui\ta5, a6, 20, 10\t/* 0x3ff */\n+\tsub\ta4, a4, a5\n+\tbgei\ta4, 32, .Lfixunsdfsi_maxint\n+\tbltz\ta4, .Lfixunsdfsi_zero\n+\n+\t/* Add explicit \"1.0\" and shift << 11.  */\n+\tor\ta7, xh, a6\n+\tssai\t(32 - 11)\n+\tsrc\ta5, a7, xl\n+\n+\t/* Shift back to the right, based on the exponent.  */\n+\taddi\ta4, a4, 1\n+\tbeqi\ta4, 32, .Lfixunsdfsi_bigexp\n+\tssl\ta4\t\t/* shift by 32 - a4 */\n+\tsrl\ta5, a5\n+\n+\t/* Negate the result if sign != 0.  */\n+\tneg\ta2, a5\n+\tmovgez\ta2, a5, a7\n+\tabi_return\n+\n+.Lfixunsdfsi_nan_or_inf:\n+\t/* Handle Infinity and NaN.  */\n+\tslli\ta4, xh, 12\n+\tor\ta4, a4, xl\n+\tbeqz\ta4, .Lfixunsdfsi_maxint\n+\n+\t/* Translate NaN to 0xffffffff.  */\n+\tmovi\ta2, -1\n+\tabi_return\n+\n+.Lfixunsdfsi_maxint:\n+\tslli\ta4, a6, 11\t/* 0x80000000 */\n+\tmovi\ta5, -1\t\t/* 0xffffffff */\n+\tmovgez\ta4, a5, xh\n+\tmov\ta2, a4\n+\tabi_return\n+\n+.Lfixunsdfsi_zero:\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+.Lfixunsdfsi_bigexp:\n+\t/* Handle unsigned maximum exponent case.  */\n+\tbltz\txh, 1f\n+\tmov\ta2, a5\t\t/* no shift needed */\n+\tabi_return\n+\n+\t/* Return 0x80000000 if negative.  */\n+1:\tslli\ta2, a6, 11\n+\tabi_return\n+\n+#endif /* L_fixunsdfsi */\n+\n+#ifdef L_fixunsdfdi\n+\n+\t.align\t4\n+\t.global\t__fixunsdfdi\n+\t.type\t__fixunsdfdi, @function\n+__fixunsdfdi:\n+\tabi_entry sp, 32\n+\n+\t/* Check for NaN and Infinity.  */\n+\tmovi\ta6, 0x7ff00000\n+\tball\txh, a6, .Lfixunsdfdi_nan_or_inf\n+\n+\t/* Extract the exponent and check if 0 <= (exp - 0x3ff) < 64.  */\n+\textui\ta4, xh, 20, 11\n+\textui\ta5, a6, 20, 10\t/* 0x3ff */\n+\tsub\ta4, a4, a5\n+\tbgei\ta4, 64, .Lfixunsdfdi_maxint\n+\tbltz\ta4, .Lfixunsdfdi_zero\n+\n+\t/* Add explicit \"1.0\" and shift << 11.  */\n+\tor\ta7, xh, a6\n+\tssai\t(32 - 11)\n+\tsrc\txh, a7, xl\n+\tsll\txl, xl\n+\n+\t/* Shift back to the right, based on the exponent.  */\n+\taddi\ta4, a4, 1\n+\tbeqi\ta4, 64, .Lfixunsdfdi_bigexp\n+\tssl\ta4\t\t/* shift by 64 - a4 */\n+\tbgei\ta4, 32, .Lfixunsdfdi_smallshift\n+\tsrl\txl, xh\n+\tmovi\txh, 0\n+\n+.Lfixunsdfdi_shifted:\n+\t/* Negate the result if sign != 0.  */\n+\tbgez\ta7, 1f\n+\tneg\txl, xl\n+\tneg\txh, xh\n+\tbeqz\txl, 1f\n+\taddi\txh, xh, -1\n+1:\tabi_return\n+\n+.Lfixunsdfdi_smallshift:\n+\tsrc\txl, xh, xl\n+\tsrl\txh, xh\n+\tj\t.Lfixunsdfdi_shifted\n+\n+.Lfixunsdfdi_nan_or_inf:\n+\t/* Handle Infinity and NaN.  */\n+\tslli\ta4, xh, 12\n+\tor\ta4, a4, xl\n+\tbeqz\ta4, .Lfixunsdfdi_maxint\n+\n+\t/* Translate NaN to 0xffffffff.... */\n+1:\tmovi\txh, -1\n+\tmovi\txl, -1\n+\tabi_return\n+\n+.Lfixunsdfdi_maxint:\n+\tbgez\txh, 1b\n+2:\tslli\txh, a6, 11\t/* 0x80000000 */\n+\tmovi\txl, 0\n+\tabi_return\n+\n+.Lfixunsdfdi_zero:\n+\tmovi\txh, 0\n+\tmovi\txl, 0\n+\tabi_return\n+\n+.Lfixunsdfdi_bigexp:\n+\t/* Handle unsigned maximum exponent case.  */\n+\tbltz\ta7, 2b\n+\tabi_return\t\t/* no shift needed */\n+\n+#endif /* L_fixunsdfdi */\n+\n+#ifdef L_floatsidf\n+\n+\t.align\t4\n+\t.global\t__floatunsidf\n+\t.type\t__floatunsidf, @function\n+__floatunsidf:\n+\tabi_entry sp, 32\n+\tbeqz\ta2, .Lfloatsidf_return_zero\n+\n+\t/* Set the sign to zero and jump to the floatsidf code.  */\n+\tmovi\ta7, 0\n+\tj\t.Lfloatsidf_normalize\n+\n+\t.align\t4\n+\t.global\t__floatsidf\n+\t.type\t__floatsidf, @function\n+__floatsidf:\n+\tabi_entry sp, 32\n+\n+\t/* Check for zero.  */\n+\tbeqz\ta2, .Lfloatsidf_return_zero\n+\n+\t/* Save the sign.  */\n+\textui\ta7, a2, 31, 1\n+\n+\t/* Get the absolute value.  */\n+#if XCHAL_HAVE_ABS\n+\tabs\ta2, a2\n+#else\n+\tneg\ta4, a2\n+\tmovltz\ta2, a4, a2\n+#endif\n+\n+.Lfloatsidf_normalize:\n+\t/* Normalize with the first 1 bit in the msb.  */\n+\tdo_nsau\ta4, a2, a5, a6\n+\tssl\ta4\n+\tsll\ta5, a2\n+\n+\t/* Shift the mantissa into position.  */\n+\tsrli\txh, a5, 11\n+\tslli\txl, a5, (32 - 11)\n+\n+\t/* Set the exponent.  */\n+\tmovi\ta5, 0x41d\t/* 0x3fe + 31 */\n+\tsub\ta5, a5, a4\n+\tslli\ta5, a5, 20\n+\tadd\txh, xh, a5\n+\n+\t/* Add the sign and return. */\n+\tslli\ta7, a7, 31\n+\tor\txh, xh, a7\n+\tabi_return\n+\n+.Lfloatsidf_return_zero:\n+\tmovi\ta3, 0\n+\tabi_return\n+\n+#endif /* L_floatsidf */\n+\n+#ifdef L_floatdidf\n+\n+\t.align\t4\n+\t.global\t__floatundidf\n+\t.type\t__floatundidf, @function\n+__floatundidf:\n+\tabi_entry sp, 32\n+\n+\t/* Check for zero.  */\n+\tor\ta4, xh, xl\n+\tbeqz\ta4, 2f\n+\n+\t/* Set the sign to zero and jump to the floatdidf code.  */\n+\tmovi\ta7, 0\n+\tj\t.Lfloatdidf_normalize\n+\n+\t.align\t4\n+\t.global\t__floatdidf\n+\t.type\t__floatdidf, @function\n+__floatdidf:\n+\tabi_entry sp, 32\n+\n+\t/* Check for zero.  */\n+\tor\ta4, xh, xl\n+\tbeqz\ta4, 2f\n+\n+\t/* Save the sign.  */\n+\textui\ta7, xh, 31, 1\n+\n+\t/* Get the absolute value.  */\n+\tbgez\txh, .Lfloatdidf_normalize\n+\tneg\txl, xl\n+\tneg\txh, xh\n+\tbeqz\txl, .Lfloatdidf_normalize\n+\taddi\txh, xh, -1\n+\n+.Lfloatdidf_normalize:\n+\t/* Normalize with the first 1 bit in the msb of xh.  */\n+\tbeqz\txh, .Lfloatdidf_bigshift\n+\tdo_nsau\ta4, xh, a5, a6\n+\tssl\ta4\n+\tsrc\txh, xh, xl\n+\tsll\txl, xl\n+\n+.Lfloatdidf_shifted:\n+\t/* Shift the mantissa into position, with rounding bits in a6.  */\n+\tssai\t11\n+\tsll\ta6, xl\n+\tsrc\txl, xh, xl\n+\tsrl\txh, xh\n+\n+\t/* Set the exponent.  */\n+\tmovi\ta5, 0x43d\t/* 0x3fe + 63 */\n+\tsub\ta5, a5, a4\n+\tslli\ta5, a5, 20\n+\tadd\txh, xh, a5\n+\n+\t/* Add the sign.  */\n+\tslli\ta7, a7, 31\n+\tor\txh, xh, a7\n+\n+\t/* Round up if the leftover fraction is >= 1/2.  */\n+\tbgez\ta6, 2f\n+\taddi\txl, xl, 1\n+\tbeqz\txl, .Lfloatdidf_roundcarry\n+\n+\t/* Check if the leftover fraction is exactly 1/2.  */\n+\tslli\ta6, a6, 1\n+\tbeqz\ta6, .Lfloatdidf_exactlyhalf\n+2:\tabi_return\n+\n+.Lfloatdidf_bigshift:\n+\t/* xh is zero.  Normalize with first 1 bit of xl in the msb of xh.  */\n+\tdo_nsau\ta4, xl, a5, a6\n+\tssl\ta4\n+\tsll\txh, xl\n+\tmovi\txl, 0\n+\taddi\ta4, a4, 32\n+\tj\t.Lfloatdidf_shifted\n+\n+.Lfloatdidf_exactlyhalf:\n+\t/* Round down to the nearest even value.  */\n+\tsrli\txl, xl, 1\n+\tslli\txl, xl, 1\n+\tabi_return\n+\n+.Lfloatdidf_roundcarry:\n+\t/* xl is always zero when the rounding increment overflows, so\n+\t   there's no need to round it to an even value.  */\n+\taddi\txh, xh, 1\n+\t/* Overflow to the exponent is OK.  */\n+\tabi_return\n+\n+#endif /* L_floatdidf */\n+\n+#ifdef L_truncdfsf2\n+\n+\t.align\t4\n+\t.global\t__truncdfsf2\n+\t.type\t__truncdfsf2, @function\n+__truncdfsf2:\n+\tabi_entry sp, 32\n+\n+\t/* Adjust the exponent bias.  */\n+\tmovi\ta4, (0x3ff - 0x7f) << 20\n+\tsub\ta5, xh, a4\n+\n+\t/* Check for underflow.  */\n+\txor\ta6, xh, a5\n+\tbltz\ta6, .Ltrunc_underflow\n+\textui\ta6, a5, 20, 11\n+\tbeqz\ta6, .Ltrunc_underflow\n+\n+\t/* Check for overflow.  */\n+\tmovi\ta4, 255\n+\tbge\ta6, a4, .Ltrunc_overflow\n+\n+\t/* Shift a5/xl << 3 into a5/a4.  */\n+\tssai\t(32 - 3)\n+\tsrc\ta5, a5, xl\n+\tsll\ta4, xl\n+\n+.Ltrunc_addsign:\n+\t/* Add the sign bit.  */\n+\textui\ta6, xh, 31, 1\n+\tslli\ta6, a6, 31\n+\tor\ta2, a6, a5\n+\n+\t/* Round up if the leftover fraction is >= 1/2.  */\n+\tbgez\ta4, 1f\n+\taddi\ta2, a2, 1\n+\t/* Overflow to the exponent is OK.  The answer will be correct.  */\n+\n+\t/* Check if the leftover fraction is exactly 1/2.  */\n+\tslli\ta4, a4, 1\n+\tbeqz\ta4, .Ltrunc_exactlyhalf\n+1:\tabi_return\n+\n+.Ltrunc_exactlyhalf:\n+\t/* Round down to the nearest even value.  */\n+\tsrli\ta2, a2, 1\n+\tslli\ta2, a2, 1\n+\tabi_return\n+\n+.Ltrunc_overflow:\n+\t/* Check if exponent == 0x7ff.  */\n+\tmovi\ta4, 0x7ff00000\n+\tbnall\txh, a4, 1f\n+\n+\t/* Check if mantissa is nonzero.  */\n+\tslli\ta5, xh, 12\n+\tor\ta5, a5, xl\n+\tbeqz\ta5, 1f\n+\n+\t/* Shift a4 to set a bit in the mantissa, making a quiet NaN.  */\n+\tsrli\ta4, a4, 1\n+\n+1:\tslli\ta4, a4, 4\t/* 0xff000000 or 0xff800000 */\n+\t/* Add the sign bit.  */\n+\textui\ta6, xh, 31, 1\n+\tssai\t1\n+\tsrc\ta2, a6, a4\n+\tabi_return\n+\n+.Ltrunc_underflow:\n+\t/* Find shift count for a subnormal.  Flush to zero if >= 32.  */\n+\textui\ta6, xh, 20, 11\n+\tmovi\ta5, 0x3ff - 0x7f\n+\tsub\ta6, a5, a6\n+\taddi\ta6, a6, 1\n+\tbgeui\ta6, 32, 1f\n+\n+\t/* Replace the exponent with an explicit \"1.0\".  */\n+\tslli\ta5, a5, 13\t/* 0x700000 */\n+\tor\ta5, a5, xh\n+\tslli\ta5, a5, 11\n+\tsrli\ta5, a5, 11\n+\n+\t/* Shift the mantissa left by 3 bits (into a5/a4).  */\n+\tssai\t(32 - 3)\n+\tsrc\ta5, a5, xl\n+\tsll\ta4, xl\n+\n+\t/* Shift right by a6.  */\n+\tssr\ta6\n+\tsll\ta7, a4\n+\tsrc\ta4, a5, a4\n+\tsrl\ta5, a5\n+\tbeqz\ta7, .Ltrunc_addsign\n+\tor\ta4, a4, a6\t/* any positive, nonzero value will work */\n+\tj\t.Ltrunc_addsign\n+\n+\t/* Return +/- zero.  */\n+1:\textui\ta2, xh, 31, 1\n+\tslli\ta2, a2, 31\n+\tabi_return\n+\n+#endif /* L_truncdfsf2 */\n+\n+#ifdef L_extendsfdf2\n+\n+\t.align\t4\n+\t.global\t__extendsfdf2\n+\t.type\t__extendsfdf2, @function\n+__extendsfdf2:\n+\tabi_entry sp, 32\n+\n+\t/* Save the sign bit and then shift it off.  */\n+\textui\ta5, a2, 31, 1\n+\tslli\ta5, a5, 31\n+\tslli\ta4, a2, 1\n+\n+\t/* Extract and check the exponent.  */\n+\textui\ta6, a2, 23, 8\n+\tbeqz\ta6, .Lextend_expzero\n+\taddi\ta6, a6, 1\n+\tbeqi\ta6, 256, .Lextend_nan_or_inf\n+\n+\t/* Shift >> 3 into a4/xl.  */\n+\tsrli\ta4, a4, 4\n+\tslli\txl, a2, (32 - 3)\n+\n+\t/* Adjust the exponent bias.  */\n+\tmovi\ta6, (0x3ff - 0x7f) << 20\n+\tadd\ta4, a4, a6\n+\n+\t/* Add the sign bit.  */\n+\tor\txh, a4, a5\n+\tabi_return\n+\n+.Lextend_nan_or_inf:\n+\tmovi\ta4, 0x7ff00000\n+\n+\t/* Check for NaN.  */\n+\tslli\ta7, a2, 9\n+\tbeqz\ta7, 1f\n+\n+\tslli\ta6, a6, 11\t/* 0x80000 */\n+\tor\ta4, a4, a6\n+\n+\t/* Add the sign and return.  */\n+1:\tor\txh, a4, a5\n+\tmovi\txl, 0\n+\tabi_return\n+\n+.Lextend_expzero:\n+\tbeqz\ta4, 1b\n+\n+\t/* Normalize it to have 8 zero bits before the first 1 bit.  */\n+\tdo_nsau\ta7, a4, a2, a3\n+\taddi\ta7, a7, -8\n+\tssl\ta7\n+\tsll\ta4, a4\n+\t\n+\t/* Shift >> 3 into a4/xl.  */\n+\tslli\txl, a4, (32 - 3)\n+\tsrli\ta4, a4, 3\n+\n+\t/* Set the exponent.  */\n+\tmovi\ta6, 0x3fe - 0x7f\n+\tsub\ta6, a6, a7\n+\tslli\ta6, a6, 20\n+\tadd\ta4, a4, a6\n+\n+\t/* Add the sign and return.  */\n+\tor\txh, a4, a5\n+\tabi_return\n+\n+#endif /* L_extendsfdf2 */\n+\n+"}, {"sha": "498b20bb11a904c62cfc8930e0a3326a8d01b056", "filename": "gcc/config/xtensa/ieee754-sf.S", "status": "added", "additions": 1734, "deletions": 0, "changes": 1734, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/134c8a506f983aaaf24163160b9a3399cd9add1f/gcc%2Fconfig%2Fxtensa%2Fieee754-sf.S", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/134c8a506f983aaaf24163160b9a3399cd9add1f/gcc%2Fconfig%2Fxtensa%2Fieee754-sf.S", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fxtensa%2Fieee754-sf.S?ref=134c8a506f983aaaf24163160b9a3399cd9add1f", "patch": "@@ -0,0 +1,1734 @@\n+/* IEEE-754 single-precision functions for Xtensa\n+   Copyright (C) 2006 Free Software Foundation, Inc.\n+   Contributed by Bob Wilson (bwilson@tensilica.com) at Tensilica.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 2, or (at your option)\n+   any later version.\n+\n+   In addition to the permissions in the GNU General Public License,\n+   the Free Software Foundation gives you unlimited permission to link\n+   the compiled version of this file into combinations with other\n+   programs, and to distribute those combinations without any\n+   restriction coming from the use of this file.  (The General Public\n+   License restrictions do apply in other respects; for example, they\n+   cover modification of the file, and distribution when not linked\n+   into a combine executable.)\n+\n+   GCC is distributed in the hope that it will be useful, but WITHOUT\n+   ANY WARRANTY; without even the implied warranty of MERCHANTABILITY\n+   or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public\n+   License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING.  If not, write to the Free\n+   Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n+   02111-1307, USA.  */\n+\n+#ifdef __XTENSA_EB__\n+#define xh a2\n+#define xl a3\n+#define yh a4\n+#define yl a5\n+#else\n+#define xh a3\n+#define xl a2\n+#define yh a5\n+#define yl a4\n+#endif\n+\n+/*  Warning!  The branch displacements for some Xtensa branch instructions\n+    are quite small, and this code has been carefully laid out to keep\n+    branch targets in range.  If you change anything, be sure to check that\n+    the assembler is not relaxing anything to branch over a jump.  */\n+\n+#ifdef L_negsf2\n+\n+\t.align\t4\n+\t.global\t__negsf2\n+\t.type\t__negsf2, @function\n+__negsf2:\n+\tabi_entry sp, 32\n+\tmovi\ta4, 0x80000000\n+\txor\ta2, a2, a4\n+\tabi_return\n+\n+#endif /* L_negsf2 */\n+\n+#ifdef L_addsubsf3\n+\n+\t/* Addition */\n+__addsf3_aux:\n+\n+\t/* Handle NaNs and Infinities.  (This code is placed before the\n+\t   start of the function just to keep it in range of the limited\n+\t   branch displacements.)  */\n+\n+.Ladd_xnan_or_inf:\n+\t/* If y is neither Infinity nor NaN, return x.  */\n+\tbnall\ta3, a6, 1f\n+\t/* If x is a NaN, return it.  Otherwise, return y.  */\n+\tslli\ta7, a2, 9\n+\tbeqz\ta7, .Ladd_ynan_or_inf\n+1:\tabi_return\n+\n+.Ladd_ynan_or_inf:\n+\t/* Return y.  */\n+\tmov\ta2, a3\n+\tabi_return\n+\n+.Ladd_opposite_signs:\n+\t/* Operand signs differ.  Do a subtraction.  */\n+\tslli\ta7, a6, 8\n+\txor\ta3, a3, a7\n+\tj\t.Lsub_same_sign\n+\n+\t.align\t4\n+\t.global\t__addsf3\n+\t.type\t__addsf3, @function\n+__addsf3:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7f800000\n+\n+\t/* Check if the two operands have the same sign.  */\n+\txor\ta7, a2, a3\n+\tbltz\ta7, .Ladd_opposite_signs\n+\n+.Ladd_same_sign:\t\n+\t/* Check if either exponent == 0x7f8 (i.e., NaN or Infinity).  */\n+\tball\ta2, a6, .Ladd_xnan_or_inf\n+\tball\ta3, a6, .Ladd_ynan_or_inf\n+\n+\t/* Compare the exponents.  The smaller operand will be shifted\n+\t   right by the exponent difference and added to the larger\n+\t   one.  */\n+\textui\ta7, a2, 23, 9\n+\textui\ta8, a3, 23, 9\n+\tbltu\ta7, a8, .Ladd_shiftx\n+\n+.Ladd_shifty:\n+\t/* Check if the smaller (or equal) exponent is zero.  */\n+\tbnone\ta3, a6, .Ladd_yexpzero\n+\n+\t/* Replace y sign/exponent with 0x008.  */\n+\tor\ta3, a3, a6\n+\tslli\ta3, a3, 8\n+\tsrli\ta3, a3, 8\n+\n+.Ladd_yexpdiff:\n+\t/* Compute the exponent difference.  */\n+\tsub\ta10, a7, a8\n+\n+\t/* Exponent difference > 32 -- just return the bigger value.  */\n+\tbgeui\ta10, 32, 1f\n+\t\n+\t/* Shift y right by the exponent difference.  Any bits that are\n+\t   shifted out of y are saved in a9 for rounding the result.  */\n+\tssr\ta10\n+\tmovi\ta9, 0\n+\tsrc\ta9, a3, a9\n+\tsrl\ta3, a3\n+\n+\t/* Do the addition.  */\n+\tadd\ta2, a2, a3\n+\n+\t/* Check if the add overflowed into the exponent.  */\n+\textui\ta10, a2, 23, 9\n+\tbeq\ta10, a7, .Ladd_round\n+\tmov\ta8, a7\n+\tj\t.Ladd_carry\n+\n+.Ladd_yexpzero:\n+\t/* y is a subnormal value.  Replace its sign/exponent with zero,\n+\t   i.e., no implicit \"1.0\", and increment the apparent exponent\n+\t   because subnormals behave as if they had the minimum (nonzero)\n+\t   exponent.  Test for the case when both exponents are zero.  */\n+\tslli\ta3, a3, 9\n+\tsrli\ta3, a3, 9\n+\tbnone\ta2, a6, .Ladd_bothexpzero\n+\taddi\ta8, a8, 1\n+\tj\t.Ladd_yexpdiff\n+\n+.Ladd_bothexpzero:\n+\t/* Both exponents are zero.  Handle this as a special case.  There\n+\t   is no need to shift or round, and the normal code for handling\n+\t   a carry into the exponent field will not work because it\n+\t   assumes there is an implicit \"1.0\" that needs to be added.  */\n+\tadd\ta2, a2, a3\n+1:\tabi_return\n+\n+.Ladd_xexpzero:\n+\t/* Same as \"yexpzero\" except skip handling the case when both\n+\t   exponents are zero.  */\n+\tslli\ta2, a2, 9\n+\tsrli\ta2, a2, 9\n+\taddi\ta7, a7, 1\n+\tj\t.Ladd_xexpdiff\n+\n+.Ladd_shiftx:\n+\t/* Same thing as the \"shifty\" code, but with x and y swapped.  Also,\n+\t   because the exponent difference is always nonzero in this version,\n+\t   the shift sequence can use SLL and skip loading a constant zero.  */\n+\tbnone\ta2, a6, .Ladd_xexpzero\n+\n+\tor\ta2, a2, a6\n+\tslli\ta2, a2, 8\n+\tsrli\ta2, a2, 8\n+\n+.Ladd_xexpdiff:\n+\tsub\ta10, a8, a7\n+\tbgeui\ta10, 32, .Ladd_returny\n+\t\n+\tssr\ta10\n+\tsll\ta9, a2\n+\tsrl\ta2, a2\n+\n+\tadd\ta2, a2, a3\n+\n+\t/* Check if the add overflowed into the exponent.  */\n+\textui\ta10, a2, 23, 9\n+\tbne\ta10, a8, .Ladd_carry\n+\n+.Ladd_round:\n+\t/* Round up if the leftover fraction is >= 1/2.  */\n+\tbgez\ta9, 1f\n+\taddi\ta2, a2, 1\n+\n+\t/* Check if the leftover fraction is exactly 1/2.  */\n+\tslli\ta9, a9, 1\n+\tbeqz\ta9, .Ladd_exactlyhalf\n+1:\tabi_return\n+\n+.Ladd_returny:\n+\tmov\ta2, a3\n+\tabi_return\n+\n+.Ladd_carry:\t\n+\t/* The addition has overflowed into the exponent field, so the\n+\t   value needs to be renormalized.  The mantissa of the result\n+\t   can be recovered by subtracting the original exponent and\n+\t   adding 0x800000 (which is the explicit \"1.0\" for the\n+\t   mantissa of the non-shifted operand -- the \"1.0\" for the\n+\t   shifted operand was already added).  The mantissa can then\n+\t   be shifted right by one bit.  The explicit \"1.0\" of the\n+\t   shifted mantissa then needs to be replaced by the exponent,\n+\t   incremented by one to account for the normalizing shift.\n+\t   It is faster to combine these operations: do the shift first\n+\t   and combine the additions and subtractions.  If x is the\n+\t   original exponent, the result is:\n+\t       shifted mantissa - (x << 22) + (1 << 22) + (x << 23)\n+\t   or:\n+\t       shifted mantissa + ((x + 1) << 22)\n+\t   Note that the exponent is incremented here by leaving the\n+\t   explicit \"1.0\" of the mantissa in the exponent field.  */\n+\n+\t/* Shift x right by one bit.  Save the lsb.  */\n+\tmov\ta10, a2\n+\tsrli\ta2, a2, 1\n+\n+\t/* See explanation above.  The original exponent is in a8.  */\n+\taddi\ta8, a8, 1\n+\tslli\ta8, a8, 22\n+\tadd\ta2, a2, a8\n+\n+\t/* Return an Infinity if the exponent overflowed.  */\n+\tball\ta2, a6, .Ladd_infinity\n+\t\n+\t/* Same thing as the \"round\" code except the msb of the leftover\n+\t   fraction is bit 0 of a10, with the rest of the fraction in a9.  */\n+\tbbci.l\ta10, 0, 1f\n+\taddi\ta2, a2, 1\n+\tbeqz\ta9, .Ladd_exactlyhalf\n+1:\tabi_return\n+\n+.Ladd_infinity:\n+\t/* Clear the mantissa.  */\n+\tsrli\ta2, a2, 23\n+\tslli\ta2, a2, 23\n+\n+\t/* The sign bit may have been lost in a carry-out.  Put it back.  */\n+\tslli\ta8, a8, 1\n+\tor\ta2, a2, a8\n+\tabi_return\n+\n+.Ladd_exactlyhalf:\n+\t/* Round down to the nearest even value.  */\n+\tsrli\ta2, a2, 1\n+\tslli\ta2, a2, 1\n+\tabi_return\n+\n+\n+\t/* Subtraction */\n+__subsf3_aux:\n+\t\n+\t/* Handle NaNs and Infinities.  (This code is placed before the\n+\t   start of the function just to keep it in range of the limited\n+\t   branch displacements.)  */\n+\n+.Lsub_xnan_or_inf:\n+\t/* If y is neither Infinity nor NaN, return x.  */\n+\tbnall\ta3, a6, 1f\n+\t/* Both x and y are either NaN or Inf, so the result is NaN.  */\n+\tmovi\ta4, 0x400000\t/* make it a quiet NaN */\n+\tor\ta2, a2, a4\n+1:\tabi_return\n+\n+.Lsub_ynan_or_inf:\n+\t/* Negate y and return it.  */\n+\tslli\ta7, a6, 8\n+\txor\ta2, a3, a7\n+\tabi_return\n+\n+.Lsub_opposite_signs:\n+\t/* Operand signs differ.  Do an addition.  */\n+\tslli\ta7, a6, 8\n+\txor\ta3, a3, a7\n+\tj\t.Ladd_same_sign\n+\n+\t.align\t4\n+\t.global\t__subsf3\n+\t.type\t__subsf3, @function\n+__subsf3:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7f800000\n+\n+\t/* Check if the two operands have the same sign.  */\n+\txor\ta7, a2, a3\n+\tbltz\ta7, .Lsub_opposite_signs\n+\n+.Lsub_same_sign:\t\n+\t/* Check if either exponent == 0x7f8 (i.e., NaN or Infinity).  */\n+\tball\ta2, a6, .Lsub_xnan_or_inf\n+\tball\ta3, a6, .Lsub_ynan_or_inf\n+\n+\t/* Compare the operands.  In contrast to addition, the entire\n+\t   value matters here.  */\n+\textui\ta7, a2, 23, 8\n+\textui\ta8, a3, 23, 8\n+\tbltu\ta2, a3, .Lsub_xsmaller\n+\n+.Lsub_ysmaller:\n+\t/* Check if the smaller (or equal) exponent is zero.  */\n+\tbnone\ta3, a6, .Lsub_yexpzero\n+\n+\t/* Replace y sign/exponent with 0x008.  */\n+\tor\ta3, a3, a6\n+\tslli\ta3, a3, 8\n+\tsrli\ta3, a3, 8\n+\n+.Lsub_yexpdiff:\n+\t/* Compute the exponent difference.  */\n+\tsub\ta10, a7, a8\n+\n+\t/* Exponent difference > 32 -- just return the bigger value.  */\n+\tbgeui\ta10, 32, 1f\n+\t\n+\t/* Shift y right by the exponent difference.  Any bits that are\n+\t   shifted out of y are saved in a9 for rounding the result.  */\n+\tssr\ta10\n+\tmovi\ta9, 0\n+\tsrc\ta9, a3, a9\n+\tsrl\ta3, a3\n+\n+\tsub\ta2, a2, a3\n+\n+\t/* Subtract the leftover bits in a9 from zero and propagate any\n+\t   borrow from a2.  */\n+\tneg\ta9, a9\n+\taddi\ta10, a2, -1\n+\tmovnez\ta2, a10, a9\n+\n+\t/* Check if the subtract underflowed into the exponent.  */\n+\textui\ta10, a2, 23, 8\n+\tbeq\ta10, a7, .Lsub_round\n+\tj\t.Lsub_borrow\n+\n+.Lsub_yexpzero:\n+\t/* Return zero if the inputs are equal.  (For the non-subnormal\n+\t   case, subtracting the \"1.0\" will cause a borrow from the exponent\n+\t   and this case can be detected when handling the borrow.)  */\n+\tbeq\ta2, a3, .Lsub_return_zero\n+\n+\t/* y is a subnormal value.  Replace its sign/exponent with zero,\n+\t   i.e., no implicit \"1.0\".  Unless x is also a subnormal, increment\n+\t   y's apparent exponent because subnormals behave as if they had\n+\t   the minimum (nonzero) exponent.  */\n+\tslli\ta3, a3, 9\n+\tsrli\ta3, a3, 9\n+\tbnone\ta2, a6, .Lsub_yexpdiff\n+\taddi\ta8, a8, 1\n+\tj\t.Lsub_yexpdiff\n+\n+.Lsub_returny:\n+\t/* Negate and return y.  */\n+\tslli\ta7, a6, 8\n+\txor\ta2, a3, a7\n+1:\tabi_return\n+\n+.Lsub_xsmaller:\n+\t/* Same thing as the \"ysmaller\" code, but with x and y swapped and\n+\t   with y negated.  */\n+\tbnone\ta2, a6, .Lsub_xexpzero\n+\n+\tor\ta2, a2, a6\n+\tslli\ta2, a2, 8\n+\tsrli\ta2, a2, 8\n+\n+.Lsub_xexpdiff:\n+\tsub\ta10, a8, a7\n+\tbgeui\ta10, 32, .Lsub_returny\n+\t\n+\tssr\ta10\n+\tmovi\ta9, 0\n+\tsrc\ta9, a2, a9\n+\tsrl\ta2, a2\n+\n+\t/* Negate y.  */\n+\tslli\ta11, a6, 8\n+\txor\ta3, a3, a11\n+\n+\tsub\ta2, a3, a2\n+\n+\tneg\ta9, a9\n+\taddi\ta10, a2, -1\n+\tmovnez\ta2, a10, a9\n+\n+\t/* Check if the subtract underflowed into the exponent.  */\n+\textui\ta10, a2, 23, 8\n+\tbne\ta10, a8, .Lsub_borrow\n+\n+.Lsub_round:\n+\t/* Round up if the leftover fraction is >= 1/2.  */\n+\tbgez\ta9, 1f\n+\taddi\ta2, a2, 1\n+\n+\t/* Check if the leftover fraction is exactly 1/2.  */\n+\tslli\ta9, a9, 1\n+\tbeqz\ta9, .Lsub_exactlyhalf\n+1:\tabi_return\n+\n+.Lsub_xexpzero:\n+\t/* Same as \"yexpzero\".  */\n+\tbeq\ta2, a3, .Lsub_return_zero\n+\tslli\ta2, a2, 9\n+\tsrli\ta2, a2, 9\n+\tbnone\ta3, a6, .Lsub_xexpdiff\n+\taddi\ta7, a7, 1\n+\tj\t.Lsub_xexpdiff\n+\n+.Lsub_return_zero:\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+.Lsub_borrow:\t\n+\t/* The subtraction has underflowed into the exponent field, so the\n+\t   value needs to be renormalized.  Shift the mantissa left as\n+\t   needed to remove any leading zeros and adjust the exponent\n+\t   accordingly.  If the exponent is not large enough to remove\n+\t   all the leading zeros, the result will be a subnormal value.  */\n+\n+\tslli\ta8, a2, 9\n+\tbeqz\ta8, .Lsub_xzero\n+\tdo_nsau\ta6, a8, a7, a11\n+\tsrli\ta8, a8, 9\n+\tbge\ta6, a10, .Lsub_subnormal\n+\taddi\ta6, a6, 1\n+\n+.Lsub_normalize_shift:\n+\t/* Shift the mantissa (a8/a9) left by a6.  */\n+\tssl\ta6\n+\tsrc\ta8, a8, a9\n+\tsll\ta9, a9\n+\n+\t/* Combine the shifted mantissa with the sign and exponent,\n+\t   decrementing the exponent by a6.  (The exponent has already\n+\t   been decremented by one due to the borrow from the subtraction,\n+\t   but adding the mantissa will increment the exponent by one.)  */\n+\tsrli\ta2, a2, 23\n+\tsub\ta2, a2, a6\n+\tslli\ta2, a2, 23\n+\tadd\ta2, a2, a8\n+\tj\t.Lsub_round\n+\n+.Lsub_exactlyhalf:\n+\t/* Round down to the nearest even value.  */\n+\tsrli\ta2, a2, 1\n+\tslli\ta2, a2, 1\n+\tabi_return\n+\n+.Lsub_xzero:\n+\t/* If there was a borrow from the exponent, and the mantissa and\n+\t   guard digits are all zero, then the inputs were equal and the\n+\t   result should be zero.  */\n+\tbeqz\ta9, .Lsub_return_zero\n+\n+\t/* Only the guard digit is nonzero.  Shift by min(24, a10).  */\n+\taddi\ta11, a10, -24\n+\tmovi\ta6, 24\n+\tmovltz\ta6, a10, a11\n+\tj\t.Lsub_normalize_shift\n+\n+.Lsub_subnormal:\n+\t/* The exponent is too small to shift away all the leading zeros.\n+\t   Set a6 to the current exponent (which has already been\n+\t   decremented by the borrow) so that the exponent of the result\n+\t   will be zero.  Do not add 1 to a6 in this case, because: (1)\n+\t   adding the mantissa will not increment the exponent, so there is\n+\t   no need to subtract anything extra from the exponent to\n+\t   compensate, and (2) the effective exponent of a subnormal is 1\n+\t   not 0 so the shift amount must be 1 smaller than normal. */\n+\tmov\ta6, a10\n+\tj\t.Lsub_normalize_shift\n+\n+#endif /* L_addsubsf3 */\n+\n+#ifdef L_mulsf3\n+\n+\t/* Multiplication */\n+__mulsf3_aux:\n+\n+\t/* Handle unusual cases (zeros, subnormals, NaNs and Infinities).\n+\t   (This code is placed before the start of the function just to\n+\t   keep it in range of the limited branch displacements.)  */\n+\n+.Lmul_xexpzero:\n+\t/* Clear the sign bit of x.  */\n+\tslli\ta2, a2, 1\n+\tsrli\ta2, a2, 1\n+\n+\t/* If x is zero, return zero.  */\n+\tbeqz\ta2, .Lmul_return_zero\n+\n+\t/* Normalize x.  Adjust the exponent in a8.  */\n+\tdo_nsau\ta10, a2, a11, a12\n+\taddi\ta10, a10, -8\n+\tssl\ta10\n+\tsll\ta2, a2 \n+\tmovi\ta8, 1\n+\tsub\ta8, a8, a10\n+\tj\t.Lmul_xnormalized\t\n+\t\n+.Lmul_yexpzero:\n+\t/* Clear the sign bit of y.  */\n+\tslli\ta3, a3, 1\n+\tsrli\ta3, a3, 1\n+\n+\t/* If y is zero, return zero.  */\n+\tbeqz\ta3, .Lmul_return_zero\n+\n+\t/* Normalize y.  Adjust the exponent in a9.  */\n+\tdo_nsau\ta10, a3, a11, a12\n+\taddi\ta10, a10, -8\n+\tssl\ta10\n+\tsll\ta3, a3\n+\tmovi\ta9, 1\n+\tsub\ta9, a9, a10\n+\tj\t.Lmul_ynormalized\t\n+\n+.Lmul_return_zero:\n+\t/* Return zero with the appropriate sign bit.  */\n+\tsrli\ta2, a7, 31\n+\tslli\ta2, a2, 31\n+\tj\t.Lmul_done\n+\n+.Lmul_xnan_or_inf:\n+\t/* If y is zero, return NaN.  */\n+\tslli\ta8, a3, 1\n+\tbnez\ta8, 1f\n+\tmovi\ta4, 0x400000\t/* make it a quiet NaN */\n+\tor\ta2, a2, a4\n+\tj\t.Lmul_done\n+1:\n+\t/* If y is NaN, return y.  */\n+\tbnall\ta3, a6, .Lmul_returnx\n+\tslli\ta8, a3, 9\n+\tbeqz\ta8, .Lmul_returnx\n+\n+.Lmul_returny:\n+\tmov\ta2, a3\n+\n+.Lmul_returnx:\n+\t/* Set the sign bit and return.  */\n+\textui\ta7, a7, 31, 1\n+\tslli\ta2, a2, 1\n+\tssai\t1\n+\tsrc\ta2, a7, a2\n+\tj\t.Lmul_done\n+\n+.Lmul_ynan_or_inf:\n+\t/* If x is zero, return NaN.  */\n+\tslli\ta8, a2, 1\n+\tbnez\ta8, .Lmul_returny\n+\tmovi\ta7, 0x400000\t/* make it a quiet NaN */\n+\tor\ta2, a3, a7\n+\tj\t.Lmul_done\n+\n+\t.align\t4\n+\t.global\t__mulsf3\n+\t.type\t__mulsf3, @function\n+__mulsf3:\n+\tabi_entry sp, 48\n+#if __XTENSA_CALL0_ABI__\n+\taddi\tsp, sp, -32\n+\ts32i\ta12, sp, 16\n+\ts32i\ta13, sp, 20\n+\ts32i\ta14, sp, 24\n+\ts32i\ta15, sp, 28\n+#endif\n+\tmovi\ta6, 0x7f800000\n+\n+\t/* Get the sign of the result.  */\n+\txor\ta7, a2, a3\n+\n+\t/* Check for NaN and infinity.  */\n+\tball\ta2, a6, .Lmul_xnan_or_inf\n+\tball\ta3, a6, .Lmul_ynan_or_inf\n+\n+\t/* Extract the exponents.  */\n+\textui\ta8, a2, 23, 8\n+\textui\ta9, a3, 23, 8\n+\n+\tbeqz\ta8, .Lmul_xexpzero\n+.Lmul_xnormalized:\t\n+\tbeqz\ta9, .Lmul_yexpzero\n+.Lmul_ynormalized:\t\n+\n+\t/* Add the exponents.  */\n+\tadd\ta8, a8, a9\n+\n+\t/* Replace sign/exponent fields with explicit \"1.0\".  */\n+\tmovi\ta10, 0xffffff\n+\tor\ta2, a2, a6\n+\tand\ta2, a2, a10\n+\tor\ta3, a3, a6\n+\tand\ta3, a3, a10\n+\n+\t/* Multiply 32x32 to 64 bits.  The result ends up in a2/a6.  */\n+\n+#if XCHAL_HAVE_MUL32_HIGH\n+\n+\tmull\ta6, a2, a3\n+\tmuluh\ta2, a2, a3\n+\n+#else\n+\n+\t/* Break the inputs into 16-bit chunks and compute 4 32-bit partial\n+\t   products.  These partial products are:\n+\n+\t\t0 xl * yl\n+\n+\t\t1 xl * yh\n+\t\t2 xh * yl\n+\n+\t\t3 xh * yh\n+\n+\t   If using the Mul16 or Mul32 multiplier options, these input\n+\t   chunks must be stored in separate registers.  For Mac16, the\n+\t   UMUL.AA.* opcodes can specify that the inputs come from either\n+\t   half of the registers, so there is no need to shift them out\n+\t   ahead of time.  If there is no multiply hardware, the 16-bit\n+\t   chunks can be extracted when setting up the arguments to the\n+\t   separate multiply function.  */\n+\n+#if !XCHAL_HAVE_MUL16 && !XCHAL_HAVE_MUL32 && !XCHAL_HAVE_MAC16\n+\t/* Calling a separate multiply function will clobber a0 and requires\n+\t   use of a8 as a temporary, so save those values now.  (The function\n+\t   uses a custom ABI so nothing else needs to be saved.)  */\n+\ts32i\ta0, sp, 0\n+\ts32i\ta8, sp, 4\n+#endif\n+\n+#if XCHAL_HAVE_MUL16 || XCHAL_HAVE_MUL32\n+\n+#define a2h a4\n+#define a3h a5\n+\n+\t/* Get the high halves of the inputs into registers.  */\n+\tsrli\ta2h, a2, 16\n+\tsrli\ta3h, a3, 16\n+\n+#define a2l a2\n+#define a3l a3\n+\n+#if XCHAL_HAVE_MUL32 && !XCHAL_HAVE_MUL16\n+\t/* Clear the high halves of the inputs.  This does not matter\n+\t   for MUL16 because the high bits are ignored.  */\n+\textui\ta2, a2, 0, 16\n+\textui\ta3, a3, 0, 16\n+#endif\n+#endif /* MUL16 || MUL32 */\n+\n+\n+#if XCHAL_HAVE_MUL16\n+\n+#define do_mul(dst, xreg, xhalf, yreg, yhalf) \\\n+\tmul16u\tdst, xreg ## xhalf, yreg ## yhalf\n+\n+#elif XCHAL_HAVE_MUL32\n+\n+#define do_mul(dst, xreg, xhalf, yreg, yhalf) \\\n+\tmull\tdst, xreg ## xhalf, yreg ## yhalf\n+\n+#elif XCHAL_HAVE_MAC16\n+\n+/* The preprocessor insists on inserting a space when concatenating after\n+   a period in the definition of do_mul below.  These macros are a workaround\n+   using underscores instead of periods when doing the concatenation.  */\n+#define umul_aa_ll umul.aa.ll\n+#define umul_aa_lh umul.aa.lh\n+#define umul_aa_hl umul.aa.hl\n+#define umul_aa_hh umul.aa.hh\n+\n+#define do_mul(dst, xreg, xhalf, yreg, yhalf) \\\n+\tumul_aa_ ## xhalf ## yhalf\txreg, yreg; \\\n+\trsr\tdst, ACCLO\n+\n+#else /* no multiply hardware */\n+\t\n+#define set_arg_l(dst, src) \\\n+\textui\tdst, src, 0, 16\n+#define set_arg_h(dst, src) \\\n+\tsrli\tdst, src, 16\n+\n+#define do_mul(dst, xreg, xhalf, yreg, yhalf) \\\n+\tset_arg_ ## xhalf (a13, xreg); \\\n+\tset_arg_ ## yhalf (a14, yreg); \\\n+\tcall0\t.Lmul_mulsi3; \\\n+\tmov\tdst, a12\n+#endif\n+\n+\t/* Add pp1 and pp2 into a6 with carry-out in a9.  */\n+\tdo_mul(a6, a2, l, a3, h)\t/* pp 1 */\n+\tdo_mul(a11, a2, h, a3, l)\t/* pp 2 */\n+\tmovi\ta9, 0\n+\tadd\ta6, a6, a11\n+\tbgeu\ta6, a11, 1f\n+\taddi\ta9, a9, 1\n+1:\n+\t/* Shift the high half of a9/a6 into position in a9.  Note that\n+\t   this value can be safely incremented without any carry-outs.  */\n+\tssai\t16\n+\tsrc\ta9, a9, a6\n+\n+\t/* Compute the low word into a6.  */\n+\tdo_mul(a11, a2, l, a3, l)\t/* pp 0 */\n+\tsll\ta6, a6\n+\tadd\ta6, a6, a11\n+\tbgeu\ta6, a11, 1f\n+\taddi\ta9, a9, 1\n+1:\n+\t/* Compute the high word into a2.  */\n+\tdo_mul(a2, a2, h, a3, h)\t/* pp 3 */\n+\tadd\ta2, a2, a9\n+\t\n+#if !XCHAL_HAVE_MUL16 && !XCHAL_HAVE_MUL32 && !XCHAL_HAVE_MAC16\n+\t/* Restore values saved on the stack during the multiplication.  */\n+\tl32i\ta0, sp, 0\n+\tl32i\ta8, sp, 4\n+#endif\n+#endif\n+\n+\t/* Shift left by 9 bits, unless there was a carry-out from the\n+\t   multiply, in which case, shift by 8 bits and increment the\n+\t   exponent.  */\n+\tmovi\ta4, 9\n+\tsrli\ta5, a2, 24 - 9\n+\tbeqz\ta5, 1f\n+\taddi\ta4, a4, -1\n+\taddi\ta8, a8, 1\n+1:\tssl\ta4\n+\tsrc\ta2, a2, a6\n+\tsll\ta6, a6\n+\n+\t/* Subtract the extra bias from the exponent sum (plus one to account\n+\t   for the explicit \"1.0\" of the mantissa that will be added to the\n+\t   exponent in the final result).  */\n+\tmovi\ta4, 0x80\n+\tsub\ta8, a8, a4\n+\t\n+\t/* Check for over/underflow.  The value in a8 is one less than the\n+\t   final exponent, so values in the range 0..fd are OK here.  */\n+\tmovi\ta4, 0xfe\n+\tbgeu\ta8, a4, .Lmul_overflow\n+\t\n+.Lmul_round:\n+\t/* Round.  */\n+\tbgez\ta6, .Lmul_rounded\n+\taddi\ta2, a2, 1\n+\tslli\ta6, a6, 1\n+\tbeqz\ta6, .Lmul_exactlyhalf\n+\n+.Lmul_rounded:\n+\t/* Add the exponent to the mantissa.  */\n+\tslli\ta8, a8, 23\n+\tadd\ta2, a2, a8\n+\n+.Lmul_addsign:\n+\t/* Add the sign bit.  */\n+\tsrli\ta7, a7, 31\n+\tslli\ta7, a7, 31\n+\tor\ta2, a2, a7\n+\n+.Lmul_done:\n+#if __XTENSA_CALL0_ABI__\n+\tl32i\ta12, sp, 16\n+\tl32i\ta13, sp, 20\n+\tl32i\ta14, sp, 24\n+\tl32i\ta15, sp, 28\n+\taddi\tsp, sp, 32\n+#endif\n+\tabi_return\n+\n+.Lmul_exactlyhalf:\n+\t/* Round down to the nearest even value.  */\n+\tsrli\ta2, a2, 1\n+\tslli\ta2, a2, 1\n+\tj\t.Lmul_rounded\n+\n+.Lmul_overflow:\n+\tbltz\ta8, .Lmul_underflow\n+\t/* Return +/- Infinity.  */\n+\tmovi\ta8, 0xff\n+\tslli\ta2, a8, 23\n+\tj\t.Lmul_addsign\n+\n+.Lmul_underflow:\n+\t/* Create a subnormal value, where the exponent field contains zero,\n+\t   but the effective exponent is 1.  The value of a8 is one less than\n+\t   the actual exponent, so just negate it to get the shift amount.  */\n+\tneg\ta8, a8\n+\tmov\ta9, a6\n+\tssr\ta8\n+\tbgeui\ta8, 32, .Lmul_flush_to_zero\n+\t\n+\t/* Shift a2 right.  Any bits that are shifted out of a2 are saved\n+\t   in a6 (combined with the shifted-out bits currently in a6) for\n+\t   rounding the result.  */\n+\tsll\ta6, a2\n+\tsrl\ta2, a2\n+\n+\t/* Set the exponent to zero.  */\n+\tmovi\ta8, 0\n+\n+\t/* Pack any nonzero bits shifted out into a6.  */\n+\tbeqz\ta9, .Lmul_round\n+\tmovi\ta9, 1\n+\tor\ta6, a6, a9\n+\tj\t.Lmul_round\n+\t\n+.Lmul_flush_to_zero:\n+\t/* Return zero with the appropriate sign bit.  */\n+\tsrli\ta2, a7, 31\n+\tslli\ta2, a2, 31\n+\tj\t.Lmul_done\n+\n+#if !XCHAL_HAVE_MUL16 && !XCHAL_HAVE_MUL32 && !XCHAL_HAVE_MAC16\n+\t\n+\t/* For Xtensa processors with no multiply hardware, this simplified\n+\t   version of _mulsi3 is used for multiplying 16-bit chunks of\n+\t   the floating-point mantissas.  It uses a custom ABI:\tthe inputs\n+\t   are passed in a13 and a14, the result is returned in a12, and\n+\t   a8 and a15 are clobbered.  */\n+\t.align\t4\n+.Lmul_mulsi3:\n+\tmovi\ta12, 0\n+.Lmul_mult_loop:\n+\tadd\ta15, a14, a12\n+\textui\ta8, a13, 0, 1\n+\tmovnez\ta12, a15, a8\n+\n+\tdo_addx2 a15, a14, a12, a15\n+\textui\ta8, a13, 1, 1\n+\tmovnez\ta12, a15, a8\n+\n+\tdo_addx4 a15, a14, a12, a15\n+\textui\ta8, a13, 2, 1\n+\tmovnez\ta12, a15, a8\n+\n+\tdo_addx8 a15, a14, a12, a15\n+\textui\ta8, a13, 3, 1\n+\tmovnez\ta12, a15, a8\n+\n+\tsrli\ta13, a13, 4\n+\tslli\ta14, a14, 4\n+\tbnez\ta13, .Lmul_mult_loop\n+\tret\n+#endif /* !MUL16 && !MUL32 && !MAC16 */\n+#endif /* L_mulsf3 */\n+\n+#ifdef L_divsf3\n+\n+\t/* Division */\n+__divsf3_aux:\n+\n+\t/* Handle unusual cases (zeros, subnormals, NaNs and Infinities).\n+\t   (This code is placed before the start of the function just to\n+\t   keep it in range of the limited branch displacements.)  */\n+\n+.Ldiv_yexpzero:\n+\t/* Clear the sign bit of y.  */\n+\tslli\ta3, a3, 1\n+\tsrli\ta3, a3, 1\n+\n+\t/* Check for division by zero.  */\n+\tbeqz\ta3, .Ldiv_yzero\n+\n+\t/* Normalize y.  Adjust the exponent in a9.  */\n+\tdo_nsau\ta10, a3, a4, a5\n+\taddi\ta10, a10, -8\n+\tssl\ta10\n+\tsll\ta3, a3\n+\tmovi\ta9, 1\n+\tsub\ta9, a9, a10\n+\tj\t.Ldiv_ynormalized\t\n+\n+.Ldiv_yzero:\n+\t/* y is zero.  Return NaN if x is also zero; otherwise, infinity.  */\n+\tslli\ta4, a2, 1\n+\tsrli\ta4, a4, 1\n+\tsrli\ta2, a7, 31\n+\tslli\ta2, a2, 31\n+\tor\ta2, a2, a6\n+\tbnez\ta4, 1f\n+\tmovi\ta4, 0x400000\t/* make it a quiet NaN */\n+\tor\ta2, a2, a4\n+1:\tabi_return\n+\n+.Ldiv_xexpzero:\n+\t/* Clear the sign bit of x.  */\n+\tslli\ta2, a2, 1\n+\tsrli\ta2, a2, 1\n+\n+\t/* If x is zero, return zero.  */\n+\tbeqz\ta2, .Ldiv_return_zero\n+\n+\t/* Normalize x.  Adjust the exponent in a8.  */\n+\tdo_nsau\ta10, a2, a4, a5\n+\taddi\ta10, a10, -8\n+\tssl\ta10\n+\tsll\ta2, a2\n+\tmovi\ta8, 1\n+\tsub\ta8, a8, a10\n+\tj\t.Ldiv_xnormalized\t\n+\t\n+.Ldiv_return_zero:\n+\t/* Return zero with the appropriate sign bit.  */\n+\tsrli\ta2, a7, 31\n+\tslli\ta2, a2, 31\n+\tabi_return\n+\n+.Ldiv_xnan_or_inf:\n+\t/* Set the sign bit of the result.  */\n+\tsrli\ta7, a3, 31\n+\tslli\ta7, a7, 31\n+\txor\ta2, a2, a7\n+\t/* If y is NaN or Inf, return NaN.  */\n+\tbnall\ta3, a6, 1f\n+\tmovi\ta4, 0x400000\t/* make it a quiet NaN */\n+\tor\ta2, a2, a4\n+1:\tabi_return\n+\n+.Ldiv_ynan_or_inf:\n+\t/* If y is Infinity, return zero.  */\n+\tslli\ta8, a3, 9\n+\tbeqz\ta8, .Ldiv_return_zero\n+\t/* y is NaN; return it.  */\n+\tmov\ta2, a3\n+\tabi_return\n+\n+\t.align\t4\n+\t.global\t__divsf3\n+\t.type\t__divsf3, @function\n+__divsf3:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7f800000\n+\n+\t/* Get the sign of the result.  */\n+\txor\ta7, a2, a3\n+\n+\t/* Check for NaN and infinity.  */\n+\tball\ta2, a6, .Ldiv_xnan_or_inf\n+\tball\ta3, a6, .Ldiv_ynan_or_inf\n+\n+\t/* Extract the exponents.  */\n+\textui\ta8, a2, 23, 8\n+\textui\ta9, a3, 23, 8\n+\n+\tbeqz\ta9, .Ldiv_yexpzero\n+.Ldiv_ynormalized:\t\n+\tbeqz\ta8, .Ldiv_xexpzero\n+.Ldiv_xnormalized:\t\n+\n+\t/* Subtract the exponents.  */\n+\tsub\ta8, a8, a9\n+\n+\t/* Replace sign/exponent fields with explicit \"1.0\".  */\n+\tmovi\ta10, 0xffffff\n+\tor\ta2, a2, a6\n+\tand\ta2, a2, a10\n+\tor\ta3, a3, a6\n+\tand\ta3, a3, a10\n+\n+\t/* The first digit of the mantissa division must be a one.\n+\t   Shift x (and adjust the exponent) as needed to make this true.  */\n+\tbltu\ta3, a2, 1f\n+\tslli\ta2, a2, 1\n+\taddi\ta8, a8, -1\n+1:\n+\t/* Do the first subtraction and shift.  */\n+\tsub\ta2, a2, a3\n+\tslli\ta2, a2, 1\n+\n+\t/* Put the quotient into a10.  */\n+\tmovi\ta10, 1\n+\n+\t/* Divide one bit at a time for 23 bits.  */\n+\tmovi\ta9, 23\n+#if XCHAL_HAVE_LOOPS\n+\tloop\ta9, .Ldiv_loopend\n+#endif\n+.Ldiv_loop:\n+\t/* Shift the quotient << 1.  */\n+\tslli\ta10, a10, 1\n+\n+\t/* Is this digit a 0 or 1?  */\n+\tbltu\ta2, a3, 1f\n+\n+\t/* Output a 1 and subtract.  */\n+\taddi\ta10, a10, 1\n+\tsub\ta2, a2, a3\n+\n+\t/* Shift the dividend << 1.  */\n+1:\tslli\ta2, a2, 1\n+\n+#if !XCHAL_HAVE_LOOPS\n+\taddi\ta9, a9, -1\n+\tbnez\ta9, .Ldiv_loop\n+#endif\n+.Ldiv_loopend:\n+\n+\t/* Add the exponent bias (less one to account for the explicit \"1.0\"\n+\t   of the mantissa that will be added to the exponent in the final\n+\t   result).  */\n+\taddi\ta8, a8, 0x7e\n+\t\n+\t/* Check for over/underflow.  The value in a8 is one less than the\n+\t   final exponent, so values in the range 0..fd are OK here.  */\n+\tmovi\ta4, 0xfe\n+\tbgeu\ta8, a4, .Ldiv_overflow\n+\t\n+.Ldiv_round:\n+\t/* Round.  The remainder (<< 1) is in a2.  */\n+\tbltu\ta2, a3, .Ldiv_rounded\n+\taddi\ta10, a10, 1\n+\tbeq\ta2, a3, .Ldiv_exactlyhalf\n+\n+.Ldiv_rounded:\n+\t/* Add the exponent to the mantissa.  */\n+\tslli\ta8, a8, 23\n+\tadd\ta2, a10, a8\n+\n+.Ldiv_addsign:\n+\t/* Add the sign bit.  */\n+\tsrli\ta7, a7, 31\n+\tslli\ta7, a7, 31\n+\tor\ta2, a2, a7\n+\tabi_return\n+\n+.Ldiv_overflow:\n+\tbltz\ta8, .Ldiv_underflow\n+\t/* Return +/- Infinity.  */\n+\taddi\ta8, a4, 1\t/* 0xff */\n+\tslli\ta2, a8, 23\n+\tj\t.Ldiv_addsign\n+\n+.Ldiv_exactlyhalf:\n+\t/* Remainder is exactly half the divisor.  Round even.  */\n+\tsrli\ta10, a10, 1\n+\tslli\ta10, a10, 1\n+\tj\t.Ldiv_rounded\n+\n+.Ldiv_underflow:\n+\t/* Create a subnormal value, where the exponent field contains zero,\n+\t   but the effective exponent is 1.  The value of a8 is one less than\n+\t   the actual exponent, so just negate it to get the shift amount.  */\n+\tneg\ta8, a8\n+\tssr\ta8\n+\tbgeui\ta8, 32, .Ldiv_flush_to_zero\n+\t\n+\t/* Shift a10 right.  Any bits that are shifted out of a10 are\n+\t   saved in a6 for rounding the result.  */\n+\tsll\ta6, a10\n+\tsrl\ta10, a10\n+\n+\t/* Set the exponent to zero.  */\n+\tmovi\ta8, 0\n+\n+\t/* Pack any nonzero remainder (in a2) into a6.  */\n+\tbeqz\ta2, 1f\n+\tmovi\ta9, 1\n+\tor\ta6, a6, a9\n+\t\n+\t/* Round a10 based on the bits shifted out into a6.  */\n+1:\tbgez\ta6, .Ldiv_rounded\n+\taddi\ta10, a10, 1\n+\tslli\ta6, a6, 1\n+\tbnez\ta6, .Ldiv_rounded\n+\tsrli\ta10, a10, 1\n+\tslli\ta10, a10, 1\n+\tj\t.Ldiv_rounded\n+\n+.Ldiv_flush_to_zero:\n+\t/* Return zero with the appropriate sign bit.  */\n+\tsrli\ta2, a7, 31\n+\tslli\ta2, a2, 31\n+\tabi_return\n+\n+#endif /* L_divsf3 */\n+\n+#ifdef L_cmpsf2\n+\n+\t/* Equal and Not Equal */\n+\n+\t.align\t4\n+\t.global\t__eqsf2\n+\t.global\t__nesf2\n+\t.set\t__nesf2, __eqsf2\n+\t.type\t__eqsf2, @function\n+__eqsf2:\n+\tabi_entry sp, 32\n+\tbne\ta2, a3, 4f\n+\n+\t/* The values are equal but NaN != NaN.  Check the exponent.  */\n+\tmovi\ta6, 0x7f800000\n+\tball\ta2, a6, 3f\n+\n+\t/* Equal.  */\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+\t/* Not equal.  */\n+2:\tmovi\ta2, 1\n+\tabi_return\n+\n+\t/* Check if the mantissas are nonzero.  */\n+3:\tslli\ta7, a2, 9\n+\tj\t5f\n+\n+\t/* Check if x and y are zero with different signs.  */\n+4:\tor\ta7, a2, a3\n+\tslli\ta7, a7, 1\n+\n+\t/* Equal if a7 == 0, where a7 is either abs(x | y) or the mantissa\n+\t   or x when exponent(x) = 0x7f8 and x == y.  */\n+5:\tmovi\ta2, 0\n+\tmovi\ta3, 1\n+\tmovnez\ta2, a3, a7\t\n+\tabi_return\n+\n+\n+\t/* Greater Than */\n+\n+\t.align\t4\n+\t.global\t__gtsf2\n+\t.type\t__gtsf2, @function\n+__gtsf2:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7f800000\n+\tball\ta2, a6, 2f\n+1:\tbnall\ta3, a6, .Lle_cmp\n+\n+\t/* Check if y is a NaN.  */\n+\tslli\ta7, a3, 9\n+\tbeqz\ta7, .Lle_cmp\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+\t/* Check if x is a NaN.  */\n+2:\tslli\ta7, a2, 9\n+\tbeqz\ta7, 1b\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+\n+\t/* Less Than or Equal */\n+\n+\t.align\t4\n+\t.global\t__lesf2\n+\t.type\t__lesf2, @function\n+__lesf2:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7f800000\n+\tball\ta2, a6, 2f\n+1:\tbnall\ta3, a6, .Lle_cmp\n+\n+\t/* Check if y is a NaN.  */\n+\tslli\ta7, a3, 9\n+\tbeqz\ta7, .Lle_cmp\n+\tmovi\ta2, 1\n+\tabi_return\n+\n+\t/* Check if x is a NaN.  */\n+2:\tslli\ta7, a2, 9\n+\tbeqz\ta7, 1b\n+\tmovi\ta2, 1\n+\tabi_return\n+\n+.Lle_cmp:\n+\t/* Check if x and y have different signs.  */\n+\txor\ta7, a2, a3\n+\tbltz\ta7, .Lle_diff_signs\n+\n+\t/* Check if x is negative.  */\n+\tbltz\ta2, .Lle_xneg\n+\n+\t/* Check if x <= y.  */\n+\tbltu\ta3, a2, 5f\n+4:\tmovi\ta2, 0\n+\tabi_return\n+\n+.Lle_xneg:\n+\t/* Check if y <= x.  */\n+\tbgeu\ta2, a3, 4b\n+5:\tmovi\ta2, 1\n+\tabi_return\n+\n+.Lle_diff_signs:\n+\tbltz\ta2, 4b\n+\n+\t/* Check if both x and y are zero.  */\n+\tor\ta7, a2, a3\n+\tslli\ta7, a7, 1\n+\tmovi\ta2, 1\n+\tmovi\ta3, 0\n+\tmoveqz\ta2, a3, a7\n+\tabi_return\n+\n+\n+\t/* Greater Than or Equal */\n+\n+\t.align\t4\n+\t.global\t__gesf2\n+\t.type\t__gesf2, @function\n+__gesf2:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7f800000\n+\tball\ta2, a6, 2f\n+1:\tbnall\ta3, a6, .Llt_cmp\n+\n+\t/* Check if y is a NaN.  */\n+\tslli\ta7, a3, 9\n+\tbeqz\ta7, .Llt_cmp\n+\tmovi\ta2, -1\n+\tabi_return\n+\n+\t/* Check if x is a NaN.  */\n+2:\tslli\ta7, a2, 9\n+\tbeqz\ta7, 1b\n+\tmovi\ta2, -1\n+\tabi_return\n+\n+\n+\t/* Less Than */\n+\n+\t.align\t4\n+\t.global\t__ltsf2\n+\t.type\t__ltsf2, @function\n+__ltsf2:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7f800000\n+\tball\ta2, a6, 2f\n+1:\tbnall\ta3, a6, .Llt_cmp\n+\n+\t/* Check if y is a NaN.  */\n+\tslli\ta7, a3, 9\n+\tbeqz\ta7, .Llt_cmp\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+\t/* Check if x is a NaN.  */\n+2:\tslli\ta7, a2, 9\n+\tbeqz\ta7, 1b\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+.Llt_cmp:\n+\t/* Check if x and y have different signs.  */\n+\txor\ta7, a2, a3\n+\tbltz\ta7, .Llt_diff_signs\n+\n+\t/* Check if x is negative.  */\n+\tbltz\ta2, .Llt_xneg\n+\n+\t/* Check if x < y.  */\n+\tbgeu\ta2, a3, 5f\n+4:\tmovi\ta2, -1\n+\tabi_return\n+\n+.Llt_xneg:\n+\t/* Check if y < x.  */\n+\tbltu\ta3, a2, 4b\n+5:\tmovi\ta2, 0\n+\tabi_return\n+\n+.Llt_diff_signs:\n+\tbgez\ta2, 5b\n+\n+\t/* Check if both x and y are nonzero.  */\n+\tor\ta7, a2, a3\n+\tslli\ta7, a7, 1\n+\tmovi\ta2, 0\n+\tmovi\ta3, -1\n+\tmovnez\ta2, a3, a7\n+\tabi_return\n+\n+\n+\t/* Unordered */\n+\n+\t.align\t4\n+\t.global\t__unordsf2\n+\t.type\t__unordsf2, @function\n+__unordsf2:\n+\tabi_entry sp, 32\n+\tmovi\ta6, 0x7f800000\n+\tball\ta2, a6, 3f\n+1:\tball\ta3, a6, 4f\n+2:\tmovi\ta2, 0\n+\tabi_return\n+\n+3:\tslli\ta7, a2, 9\n+\tbeqz\ta7, 1b\n+\tmovi\ta2, 1\n+\tabi_return\n+\n+4:\tslli\ta7, a3, 9\n+\tbeqz\ta7, 2b\n+\tmovi\ta2, 1\n+\tabi_return\n+\n+#endif /* L_cmpsf2 */\n+\n+#ifdef L_fixsfsi\n+\n+\t.align\t4\n+\t.global\t__fixsfsi\n+\t.type\t__fixsfsi, @function\n+__fixsfsi:\n+\tabi_entry sp, 32\n+\n+\t/* Check for NaN and Infinity.  */\n+\tmovi\ta6, 0x7f800000\n+\tball\ta2, a6, .Lfixsfsi_nan_or_inf\n+\n+\t/* Extract the exponent and check if 0 < (exp - 0x7e) < 32.  */\n+\textui\ta4, a2, 23, 8\n+\taddi\ta4, a4, -0x7e\n+\tbgei\ta4, 32, .Lfixsfsi_maxint\n+\tblti\ta4, 1, .Lfixsfsi_zero\n+\n+\t/* Add explicit \"1.0\" and shift << 8.  */\n+\tor\ta7, a2, a6\n+\tslli\ta5, a7, 8\n+\n+\t/* Shift back to the right, based on the exponent.  */\n+\tssl\ta4\t\t/* shift by 32 - a4 */\n+\tsrl\ta5, a5\n+\n+\t/* Negate the result if sign != 0.  */\n+\tneg\ta2, a5\n+\tmovgez\ta2, a5, a7\n+\tabi_return\n+\n+.Lfixsfsi_nan_or_inf:\n+\t/* Handle Infinity and NaN.  */\n+\tslli\ta4, a2, 9\n+\tbeqz\ta4, .Lfixsfsi_maxint\n+\n+\t/* Translate NaN to +maxint.  */\n+\tmovi\ta2, 0\n+\n+.Lfixsfsi_maxint:\n+\tslli\ta4, a6, 8\t/* 0x80000000 */\n+\taddi\ta5, a4, -1\t/* 0x7fffffff */\n+\tmovgez\ta4, a5, a2\n+\tmov\ta2, a4\n+\tabi_return\n+\n+.Lfixsfsi_zero:\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+#endif /* L_fixsfsi */\n+\n+#ifdef L_fixsfdi\n+\n+\t.align\t4\n+\t.global\t__fixsfdi\n+\t.type\t__fixsfdi, @function\n+__fixsfdi:\n+\tabi_entry sp, 32\n+\n+\t/* Check for NaN and Infinity.  */\n+\tmovi\ta6, 0x7f800000\n+\tball\ta2, a6, .Lfixsfdi_nan_or_inf\n+\n+\t/* Extract the exponent and check if 0 < (exp - 0x7e) < 64.  */\n+\textui\ta4, a2, 23, 8\n+\taddi\ta4, a4, -0x7e\n+\tbgei\ta4, 64, .Lfixsfdi_maxint\n+\tblti\ta4, 1, .Lfixsfdi_zero\n+\n+\t/* Add explicit \"1.0\" and shift << 8.  */\n+\tor\ta7, a2, a6\n+\tslli\txh, a7, 8\n+\n+\t/* Shift back to the right, based on the exponent.  */\n+\tssl\ta4\t\t/* shift by 64 - a4 */\n+\tbgei\ta4, 32, .Lfixsfdi_smallshift\n+\tsrl\txl, xh\n+\tmovi\txh, 0\n+\n+.Lfixsfdi_shifted:\t\n+\t/* Negate the result if sign != 0.  */\n+\tbgez\ta7, 1f\n+\tneg\txl, xl\n+\tneg\txh, xh\n+\tbeqz\txl, 1f\n+\taddi\txh, xh, -1\n+1:\tabi_return\n+\n+.Lfixsfdi_smallshift:\n+\tmovi\txl, 0\n+\tsll\txl, xh\n+\tsrl\txh, xh\n+\tj\t.Lfixsfdi_shifted\n+\n+.Lfixsfdi_nan_or_inf:\n+\t/* Handle Infinity and NaN.  */\n+\tslli\ta4, a2, 9\n+\tbeqz\ta4, .Lfixsfdi_maxint\n+\n+\t/* Translate NaN to +maxint.  */\n+\tmovi\ta2, 0\n+\n+.Lfixsfdi_maxint:\n+\tslli\ta7, a6, 8\t/* 0x80000000 */\n+\tbgez\ta2, 1f\n+\tmov\txh, a7\n+\tmovi\txl, 0\n+\tabi_return\n+\n+1:\taddi\txh, a7, -1\t/* 0x7fffffff */\n+\tmovi\txl, -1\n+\tabi_return\n+\n+.Lfixsfdi_zero:\n+\tmovi\txh, 0\n+\tmovi\txl, 0\n+\tabi_return\n+\n+#endif /* L_fixsfdi */\n+\n+#ifdef L_fixunssfsi\n+\n+\t.align\t4\n+\t.global\t__fixunssfsi\n+\t.type\t__fixunssfsi, @function\n+__fixunssfsi:\n+\tabi_entry sp, 32\n+\n+\t/* Check for NaN and Infinity.  */\n+\tmovi\ta6, 0x7f800000\n+\tball\ta2, a6, .Lfixunssfsi_nan_or_inf\n+\n+\t/* Extract the exponent and check if 0 <= (exp - 0x7f) < 32.  */\n+\textui\ta4, a2, 23, 8\n+\taddi\ta4, a4, -0x7f\n+\tbgei\ta4, 32, .Lfixunssfsi_maxint\n+\tbltz\ta4, .Lfixunssfsi_zero\n+\n+\t/* Add explicit \"1.0\" and shift << 8.  */\n+\tor\ta7, a2, a6\n+\tslli\ta5, a7, 8\n+\n+\t/* Shift back to the right, based on the exponent.  */\n+\taddi\ta4, a4, 1\n+\tbeqi\ta4, 32, .Lfixunssfsi_bigexp\n+\tssl\ta4\t\t/* shift by 32 - a4 */\n+\tsrl\ta5, a5\n+\n+\t/* Negate the result if sign != 0.  */\n+\tneg\ta2, a5\n+\tmovgez\ta2, a5, a7\n+\tabi_return\n+\n+.Lfixunssfsi_nan_or_inf:\n+\t/* Handle Infinity and NaN.  */\n+\tslli\ta4, a2, 9\n+\tbeqz\ta4, .Lfixunssfsi_maxint\n+\n+\t/* Translate NaN to 0xffffffff.  */\n+\tmovi\ta2, -1\n+\tabi_return\n+\n+.Lfixunssfsi_maxint:\n+\tslli\ta4, a6, 8\t/* 0x80000000 */\n+\tmovi\ta5, -1\t\t/* 0xffffffff */\n+\tmovgez\ta4, a5, a2\n+\tmov\ta2, a4\n+\tabi_return\n+\n+.Lfixunssfsi_zero:\n+\tmovi\ta2, 0\n+\tabi_return\n+\n+.Lfixunssfsi_bigexp:\n+\t/* Handle unsigned maximum exponent case.  */\n+\tbltz\ta2, 1f\n+\tmov\ta2, a5\t\t/* no shift needed */\n+\tabi_return\n+\n+\t/* Return 0x80000000 if negative.  */\n+1:\tslli\ta2, a6, 8\n+\tabi_return\n+\n+#endif /* L_fixunssfsi */\n+\n+#ifdef L_fixunssfdi\n+\n+\t.align\t4\n+\t.global\t__fixunssfdi\n+\t.type\t__fixunssfdi, @function\n+__fixunssfdi:\n+\tabi_entry sp, 32\n+\n+\t/* Check for NaN and Infinity.  */\n+\tmovi\ta6, 0x7f800000\n+\tball\ta2, a6, .Lfixunssfdi_nan_or_inf\n+\n+\t/* Extract the exponent and check if 0 <= (exp - 0x7f) < 64.  */\n+\textui\ta4, a2, 23, 8\n+\taddi\ta4, a4, -0x7f\n+\tbgei\ta4, 64, .Lfixunssfdi_maxint\n+\tbltz\ta4, .Lfixunssfdi_zero\n+\n+\t/* Add explicit \"1.0\" and shift << 8.  */\n+\tor\ta7, a2, a6\n+\tslli\txh, a7, 8\n+\n+\t/* Shift back to the right, based on the exponent.  */\n+\taddi\ta4, a4, 1\n+\tbeqi\ta4, 64, .Lfixunssfdi_bigexp\n+\tssl\ta4\t\t/* shift by 64 - a4 */\n+\tbgei\ta4, 32, .Lfixunssfdi_smallshift\n+\tsrl\txl, xh\n+\tmovi\txh, 0\n+\n+.Lfixunssfdi_shifted:\n+\t/* Negate the result if sign != 0.  */\n+\tbgez\ta7, 1f\n+\tneg\txl, xl\n+\tneg\txh, xh\n+\tbeqz\txl, 1f\n+\taddi\txh, xh, -1\n+1:\tabi_return\n+\n+.Lfixunssfdi_smallshift:\n+\tmovi\txl, 0\n+\tsrc\txl, xh, xl\n+\tsrl\txh, xh\n+\tj\t.Lfixunssfdi_shifted\n+\n+.Lfixunssfdi_nan_or_inf:\n+\t/* Handle Infinity and NaN.  */\n+\tslli\ta4, a2, 9\n+\tbeqz\ta4, .Lfixunssfdi_maxint\n+\n+\t/* Translate NaN to 0xffffffff.... */\n+1:\tmovi\txh, -1\n+\tmovi\txl, -1\n+\tabi_return\n+\n+.Lfixunssfdi_maxint:\n+\tbgez\ta2, 1b\n+2:\tslli\txh, a6, 8\t/* 0x80000000 */\n+\tmovi\txl, 0\n+\tabi_return\n+\n+.Lfixunssfdi_zero:\n+\tmovi\txh, 0\n+\tmovi\txl, 0\n+\tabi_return\n+\n+.Lfixunssfdi_bigexp:\n+\t/* Handle unsigned maximum exponent case.  */\n+\tbltz\ta7, 2b\n+\tmovi\txl, 0\n+\tabi_return\t\t/* no shift needed */\n+\n+#endif /* L_fixunssfdi */\n+\n+#ifdef L_floatsisf\n+\n+\t.align\t4\n+\t.global\t__floatunsisf\n+\t.type\t__floatunsisf, @function\n+__floatunsisf:\n+\tabi_entry sp, 32\n+\tbeqz\ta2, .Lfloatsisf_return\n+\n+\t/* Set the sign to zero and jump to the floatsisf code.  */\n+\tmovi\ta7, 0\n+\tj\t.Lfloatsisf_normalize\n+\n+\t.align\t4\n+\t.global\t__floatsisf\n+\t.type\t__floatsisf, @function\n+__floatsisf:\n+\tabi_entry sp, 32\n+\n+\t/* Check for zero.  */\n+\tbeqz\ta2, .Lfloatsisf_return\n+\n+\t/* Save the sign.  */\n+\textui\ta7, a2, 31, 1\n+\n+\t/* Get the absolute value.  */\n+#if XCHAL_HAVE_ABS\n+\tabs\ta2, a2\n+#else\n+\tneg\ta4, a2\n+\tmovltz\ta2, a4, a2\n+#endif\n+\n+.Lfloatsisf_normalize:\n+\t/* Normalize with the first 1 bit in the msb.  */\n+\tdo_nsau\ta4, a2, a5, a6\n+\tssl\ta4\n+\tsll\ta5, a2\n+\n+\t/* Shift the mantissa into position, with rounding bits in a6.  */\n+\tsrli\ta2, a5, 8\n+\tslli\ta6, a5, (32 - 8)\n+\n+\t/* Set the exponent.  */\n+\tmovi\ta5, 0x9d\t/* 0x7e + 31 */\n+\tsub\ta5, a5, a4\n+\tslli\ta5, a5, 23\n+\tadd\ta2, a2, a5\n+\n+\t/* Add the sign.  */\n+\tslli\ta7, a7, 31\n+\tor\ta2, a2, a7\n+\n+\t/* Round up if the leftover fraction is >= 1/2.  */\n+\tbgez\ta6, .Lfloatsisf_return\n+\taddi\ta2, a2, 1\t/* Overflow to the exponent is OK.  */\n+\n+\t/* Check if the leftover fraction is exactly 1/2.  */\n+\tslli\ta6, a6, 1\n+\tbeqz\ta6, .Lfloatsisf_exactlyhalf\n+\n+.Lfloatsisf_return:\n+\tabi_return\n+\n+.Lfloatsisf_exactlyhalf:\n+\t/* Round down to the nearest even value.  */\n+\tsrli\ta2, a2, 1\n+\tslli\ta2, a2, 1\n+\tabi_return\n+\n+#endif /* L_floatsisf */\n+\n+#ifdef L_floatdisf\n+\n+\t.align\t4\n+\t.global\t__floatundisf\n+\t.type\t__floatundisf, @function\n+__floatundisf:\n+\tabi_entry sp, 32\n+\n+\t/* Check for zero.  */\n+\tor\ta4, xh, xl\n+\tbeqz\ta4, 2f\n+\n+\t/* Set the sign to zero and jump to the floatdisf code.  */\n+\tmovi\ta7, 0\n+\tj\t.Lfloatdisf_normalize\n+\n+\t.align\t4\n+\t.global\t__floatdisf\n+\t.type\t__floatdisf, @function\n+__floatdisf:\n+\tabi_entry sp, 32\n+\n+\t/* Check for zero.  */\n+\tor\ta4, xh, xl\n+\tbeqz\ta4, 2f\n+\n+\t/* Save the sign.  */\n+\textui\ta7, xh, 31, 1\n+\n+\t/* Get the absolute value.  */\n+\tbgez\txh, .Lfloatdisf_normalize\n+\tneg\txl, xl\n+\tneg\txh, xh\n+\tbeqz\txl, .Lfloatdisf_normalize\n+\taddi\txh, xh, -1\n+\n+.Lfloatdisf_normalize:\n+\t/* Normalize with the first 1 bit in the msb of xh.  */\n+\tbeqz\txh, .Lfloatdisf_bigshift\n+\tdo_nsau\ta4, xh, a5, a6\n+\tssl\ta4\n+\tsrc\txh, xh, xl\n+\tsll\txl, xl\n+\n+.Lfloatdisf_shifted:\n+\t/* Shift the mantissa into position, with rounding bits in a6.  */\n+\tssai\t8\n+\tsll\ta5, xl\n+\tsrc\ta6, xh, xl\n+\tsrl\txh, xh\n+\tbeqz\ta5, 1f\n+\tmovi\ta5, 1\n+\tor\ta6, a6, a5\n+1:\n+\t/* Set the exponent.  */\n+\tmovi\ta5, 0xbd\t/* 0x7e + 63 */\n+\tsub\ta5, a5, a4\n+\tslli\ta5, a5, 23\n+\tadd\ta2, xh, a5\n+\n+\t/* Add the sign.  */\n+\tslli\ta7, a7, 31\n+\tor\ta2, a2, a7\n+\n+\t/* Round up if the leftover fraction is >= 1/2.  */\n+\tbgez\ta6, 2f\n+\taddi\ta2, a2, 1\t/* Overflow to the exponent is OK.  */\n+\n+\t/* Check if the leftover fraction is exactly 1/2.  */\n+\tslli\ta6, a6, 1\n+\tbeqz\ta6, .Lfloatdisf_exactlyhalf\n+2:\tabi_return\n+\n+.Lfloatdisf_bigshift:\n+\t/* xh is zero.  Normalize with first 1 bit of xl in the msb of xh.  */\n+\tdo_nsau\ta4, xl, a5, a6\n+\tssl\ta4\n+\tsll\txh, xl\n+\tmovi\txl, 0\n+\taddi\ta4, a4, 32\n+\tj\t.Lfloatdisf_shifted\n+\n+.Lfloatdisf_exactlyhalf:\n+\t/* Round down to the nearest even value.  */\n+\tsrli\ta2, a2, 1\n+\tslli\ta2, a2, 1\n+\tabi_return\n+\n+#endif /* L_floatdisf */"}, {"sha": "ebfd54ddfcd561697e2ba1a0cbcc93af24a2621c", "filename": "gcc/config/xtensa/lib1funcs.asm", "status": "modified", "additions": 4, "deletions": 1, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/134c8a506f983aaaf24163160b9a3399cd9add1f/gcc%2Fconfig%2Fxtensa%2Flib1funcs.asm", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/134c8a506f983aaaf24163160b9a3399cd9add1f/gcc%2Fconfig%2Fxtensa%2Flib1funcs.asm", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fxtensa%2Flib1funcs.asm?ref=134c8a506f983aaaf24163160b9a3399cd9add1f", "patch": "@@ -1,5 +1,5 @@\n /* Assembly functions for the Xtensa version of libgcc1.\n-   Copyright (C) 2001,2002,2003, 2005 Free Software Foundation, Inc.\n+   Copyright (C) 2001, 2002, 2003, 2005, 2006 Free Software Foundation, Inc.\n    Contributed by Bob Wilson (bwilson@tensilica.com) at Tensilica.\n \n This file is part of GCC.\n@@ -484,3 +484,6 @@ __modsi3:\n \t.size\t__modsi3,.-__modsi3\n \n #endif /* L_modsi3 */\n+\n+#include \"ieee754-df.S\"\n+#include \"ieee754-sf.S\""}, {"sha": "6bcaa616c5e2c3a7c08eb7db88a3fa720bbd91e8", "filename": "gcc/config/xtensa/t-xtensa", "status": "modified", "additions": 9, "deletions": 12, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/134c8a506f983aaaf24163160b9a3399cd9add1f/gcc%2Fconfig%2Fxtensa%2Ft-xtensa", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/134c8a506f983aaaf24163160b9a3399cd9add1f/gcc%2Fconfig%2Fxtensa%2Ft-xtensa", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fxtensa%2Ft-xtensa?ref=134c8a506f983aaaf24163160b9a3399cd9add1f", "patch": "@@ -1,17 +1,14 @@\n-# Use GCC's floating-point emulation code\n-LIB2FUNCS_EXTRA = fp-bit.c dp-bit.c\n-\n-dp-bit.c: $(srcdir)/config/fp-bit.c\n-\tcat $(srcdir)/config/fp-bit.c > dp-bit.c\n-\n-fp-bit.c: $(srcdir)/config/fp-bit.c\n-\techo '#define FLOAT' > fp-bit.c\n-\tcat $(srcdir)/config/fp-bit.c >> fp-bit.c\n-\n LIB1ASMSRC = xtensa/lib1funcs.asm\n-LIB1ASMFUNCS = _mulsi3 _nsau _divsi3 _modsi3 _udivsi3 _umodsi3\n+LIB1ASMFUNCS = _mulsi3 _nsau _divsi3 _modsi3 _udivsi3 _umodsi3 \\\n+\t_negsf2 _addsubsf3 _mulsf3 _divsf3 _cmpsf2 _fixsfsi _fixsfdi \\\n+\t_fixunssfsi _fixunssfdi _floatsisf _floatunsisf \\\n+\t_floatdisf _floatundisf \\\n+\t_negdf2 _addsubdf3 _muldf3 _divdf3 _cmpdf2 _fixdfsi _fixdfdi \\\n+\t_fixunsdfsi _fixunsdfdi _floatsidf _floatunsidf \\\n+\t_floatdidf _floatundidf \\\n+\t_truncdfsf2 _extendsfdf2\n \n-LIB2FUNCS_EXTRA += $(srcdir)/config/xtensa/lib2funcs.S\n+LIB2FUNCS_EXTRA = $(srcdir)/config/xtensa/lib2funcs.S\n \n $(T)crti.o: $(srcdir)/config/xtensa/crti.asm $(GCC_PASSES)\n \t$(GCC_FOR_TARGET) $(GCC_CFLAGS) $(MULTILIB_CFLAGS) $(INCLUDES) \\"}, {"sha": "dc9e265b4ef231bd5565bb9bd629c8d34c0dd8c0", "filename": "include/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/134c8a506f983aaaf24163160b9a3399cd9add1f/include%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/134c8a506f983aaaf24163160b9a3399cd9add1f/include%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/include%2FChangeLog?ref=134c8a506f983aaaf24163160b9a3399cd9add1f", "patch": "@@ -1,3 +1,7 @@\n+2006-01-09  Bob Wilson  <bob.wilson@acm.org>\n+\n+\t* xtensa-config.h (XCHAL_HAVE_MUL32_HIGH): Define.\n+\n 2005-12-30  Bob Wilson  <bob.wilson@acm.org>\n \n \t* xtensa-config.h (XCHAL_HAVE_WIDE_BRANCHES): New."}, {"sha": "5c0315d6198dcfca8716219cf24d4d7484f9604b", "filename": "include/xtensa-config.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/134c8a506f983aaaf24163160b9a3399cd9add1f/include%2Fxtensa-config.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/134c8a506f983aaaf24163160b9a3399cd9add1f/include%2Fxtensa-config.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/include%2Fxtensa-config.h?ref=134c8a506f983aaaf24163160b9a3399cd9add1f", "patch": "@@ -54,6 +54,9 @@\n #undef XCHAL_HAVE_MUL32\n #define XCHAL_HAVE_MUL32\t\t0\n \n+#undef XCHAL_HAVE_MUL32_HIGH\n+#define XCHAL_HAVE_MUL32_HIGH\t\t0\n+\n #undef XCHAL_HAVE_DIV32\n #define XCHAL_HAVE_DIV32\t\t0\n "}]}