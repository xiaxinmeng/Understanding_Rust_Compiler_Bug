{"sha": "7ff57009bcc728044ba2de339ecd16721d48aba3", "node_id": "C_kwDOANBUbNoAKDdmZjU3MDA5YmNjNzI4MDQ0YmEyZGUzMzllY2QxNjcyMWQ0OGFiYTM", "commit": {"author": {"name": "Ju-Zhe Zhong", "email": "juzhe.zhong@rivai.ai", "date": "2023-03-10T03:02:05Z"}, "committer": {"name": "Kito Cheng", "email": "kito.cheng@sifive.com", "date": "2023-03-13T16:25:10Z"}, "message": "RISC-V: Fine tune RA constraint for narrow instructions\n\nAccording to RVV ISA, for narrow instructions:\n\nThe destination EEW is smaller than the source EEW and the overlap is\nin the lowest-numbered part of the source register group.\n(e.g., when LMUL=1, vnsrl.wi v0, v0, 3 is legal, but a destination of v1 is not).\n\nWe should allow narrow instructions partially overlap base on the rule of RVV ISA above\nso that we could exploit the useage of vector registers.\n\nConsider these cases:\nhttps://godbolt.org/z/o6sc4eqGj\n\nsome cases in LLVM have redundant move instructions,\nsome cases in LLVM have redundant register spillings.\n\nNow after this patch, GCC can have perfect RA && codegen for different pressure RA cases.\n\ngcc/ChangeLog:\n\n\t* config/riscv/vector.md: Fine tune RA constraints.\n\ngcc/testsuite/ChangeLog:\n\n\t* gcc.target/riscv/rvv/base/narrow_constraint-1.c: New test.\n\t* gcc.target/riscv/rvv/base/narrow_constraint-10.c: New test.\n\t* gcc.target/riscv/rvv/base/narrow_constraint-11.c: New test.\n\t* gcc.target/riscv/rvv/base/narrow_constraint-2.c: New test.\n\t* gcc.target/riscv/rvv/base/narrow_constraint-3.c: New test.\n\t* gcc.target/riscv/rvv/base/narrow_constraint-4.c: New test.\n\t* gcc.target/riscv/rvv/base/narrow_constraint-5.c: New test.\n\t* gcc.target/riscv/rvv/base/narrow_constraint-6.c: New test.\n\t* gcc.target/riscv/rvv/base/narrow_constraint-7.c: New test.\n\t* gcc.target/riscv/rvv/base/narrow_constraint-8.c: New test.\n\t* gcc.target/riscv/rvv/base/narrow_constraint-9.c: New test.", "tree": {"sha": "80440de803912b430a8353eb69e2a9950bd055dc", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/80440de803912b430a8353eb69e2a9950bd055dc"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/7ff57009bcc728044ba2de339ecd16721d48aba3", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7ff57009bcc728044ba2de339ecd16721d48aba3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/7ff57009bcc728044ba2de339ecd16721d48aba3", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7ff57009bcc728044ba2de339ecd16721d48aba3/comments", "author": {"login": "zhongjuzhe", "id": 66454988, "node_id": "MDQ6VXNlcjY2NDU0OTg4", "avatar_url": "https://avatars.githubusercontent.com/u/66454988?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhongjuzhe", "html_url": "https://github.com/zhongjuzhe", "followers_url": "https://api.github.com/users/zhongjuzhe/followers", "following_url": "https://api.github.com/users/zhongjuzhe/following{/other_user}", "gists_url": "https://api.github.com/users/zhongjuzhe/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhongjuzhe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhongjuzhe/subscriptions", "organizations_url": "https://api.github.com/users/zhongjuzhe/orgs", "repos_url": "https://api.github.com/users/zhongjuzhe/repos", "events_url": "https://api.github.com/users/zhongjuzhe/events{/privacy}", "received_events_url": "https://api.github.com/users/zhongjuzhe/received_events", "type": "User", "site_admin": false}, "committer": {"login": "kito-cheng", "id": 2723185, "node_id": "MDQ6VXNlcjI3MjMxODU=", "avatar_url": "https://avatars.githubusercontent.com/u/2723185?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kito-cheng", "html_url": "https://github.com/kito-cheng", "followers_url": "https://api.github.com/users/kito-cheng/followers", "following_url": "https://api.github.com/users/kito-cheng/following{/other_user}", "gists_url": "https://api.github.com/users/kito-cheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/kito-cheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kito-cheng/subscriptions", "organizations_url": "https://api.github.com/users/kito-cheng/orgs", "repos_url": "https://api.github.com/users/kito-cheng/repos", "events_url": "https://api.github.com/users/kito-cheng/events{/privacy}", "received_events_url": "https://api.github.com/users/kito-cheng/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "ced122b849b8961b854053f0d1ac96983c5802e5", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ced122b849b8961b854053f0d1ac96983c5802e5", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ced122b849b8961b854053f0d1ac96983c5802e5"}], "stats": {"total": 3872, "additions": 3788, "deletions": 84}, "files": [{"sha": "178d2950493b7135be6cbab819202f00ba378acc", "filename": "gcc/config/riscv/vector.md", "status": "modified", "additions": 84, "deletions": 84, "changes": 168, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Fconfig%2Friscv%2Fvector.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Fconfig%2Friscv%2Fvector.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Fvector.md?ref=7ff57009bcc728044ba2de339ecd16721d48aba3", "patch": "@@ -3058,62 +3058,62 @@\n ;; lowest-numbered part of the source register group\n ;; e.g, when LMUL = 1, vnsrl.wi v0,v0,3 is legal but a destination of v1 is not.\n (define_insn \"@pred_narrow_<optab><mode>\"\n-  [(set (match_operand:<V_DOUBLE_TRUNC> 0 \"register_operand\"           \"=vd, vr,  &vr, vd, vr,  &vr\")\n+  [(set (match_operand:<V_DOUBLE_TRUNC> 0 \"register_operand\"           \"=vd,vd, vr, vr,vd, vr,  &vr,  &vr, vd, vr,  &vr,  &vr\")\n \t(if_then_else:<V_DOUBLE_TRUNC>\n \t  (unspec:<VM>\n-\t    [(match_operand:<VM> 1 \"vector_mask_operand\"               \" vm,Wc1,vmWc1, vm,Wc1,vmWc1\")\n-\t     (match_operand 5 \"vector_length_operand\"                  \" rK, rK,   rK, rK, rK,   rK\")\n-\t     (match_operand 6 \"const_int_operand\"                      \"  i,  i,    i,  i,  i,    i\")\n-\t     (match_operand 7 \"const_int_operand\"                      \"  i,  i,    i,  i,  i,    i\")\n-\t     (match_operand 8 \"const_int_operand\"                      \"  i,  i,    i,  i,  i,    i\")\n+\t    [(match_operand:<VM> 1 \"vector_mask_operand\"               \" vm,vm,Wc1,Wc1,vm,Wc1,vmWc1,vmWc1, vm,Wc1,vmWc1,vmWc1\")\n+\t     (match_operand 5 \"vector_length_operand\"                  \" rK,rK, rK, rK,rK, rK,   rK,   rK, rK, rK,   rK,   rK\")\n+\t     (match_operand 6 \"const_int_operand\"                      \"  i, i,  i,  i, i,  i,    i,    i,  i,  i,    i,    i\")\n+\t     (match_operand 7 \"const_int_operand\"                      \"  i, i,  i,  i, i,  i,    i,    i,  i,  i,    i,    i\")\n+\t     (match_operand 8 \"const_int_operand\"                      \"  i, i,  i,  i, i,  i,    i,    i,  i,  i,    i,    i\")\n \t     (reg:SI VL_REGNUM)\n \t     (reg:SI VTYPE_REGNUM)] UNSPEC_VPREDICATE)\n \t  (truncate:<V_DOUBLE_TRUNC>\n \t    (any_shiftrt:VWEXTI\n-\t     (match_operand:VWEXTI 3 \"register_operand\"                \"  0,  0,   vr,  0,  0,   vr\")\n-\t     (match_operand:<V_DOUBLE_TRUNC> 4 \"vector_shift_operand\"  \" vr, vr,   vr, vk, vk,   vk\")))\n-\t  (match_operand:<V_DOUBLE_TRUNC> 2 \"vector_merge_operand\"     \"0vu,0vu,  0vu,0vu,0vu,  0vu\")))]\n+\t     (match_operand:VWEXTI 3 \"register_operand\"                \" vr,vr, vr, vr, 0,  0,   vr,   vr,  0,  0,   vr,   vr\")\n+\t     (match_operand:<V_DOUBLE_TRUNC> 4 \"vector_shift_operand\"  \"  0, 0,  0,  0,vr, vr,   vr,   vr, vk, vk,   vk,   vk\")))\n+\t  (match_operand:<V_DOUBLE_TRUNC> 2 \"vector_merge_operand\"     \"  0,vu,  0, vu,vu, vu,   vu,    0, vu, vu,   vu,    0\")))]\n   \"TARGET_VECTOR\"\n   \"vn<insn>.w%o4\\t%0,%3,%v4%p1\"\n   [(set_attr \"type\" \"vnshift\")\n    (set_attr \"mode\" \"<V_DOUBLE_TRUNC>\")])\n \n (define_insn \"@pred_narrow_<optab><mode>_scalar\"\n-  [(set (match_operand:<V_DOUBLE_TRUNC> 0 \"register_operand\"           \"=vd, vr,  &vr, vd, vr,  &vr\")\n+  [(set (match_operand:<V_DOUBLE_TRUNC> 0 \"register_operand\"           \"=vd, vd, vr, vr,  &vr,  &vr\")\n \t(if_then_else:<V_DOUBLE_TRUNC>\n \t  (unspec:<VM>\n-\t    [(match_operand:<VM> 1 \"vector_mask_operand\"               \" vm,Wc1,vmWc1, vm,Wc1,vmWc1\")\n-\t     (match_operand 5 \"vector_length_operand\"                  \" rK, rK,   rK, rK, rK,   rK\")\n-\t     (match_operand 6 \"const_int_operand\"                      \"  i,  i,    i,  i,  i,    i\")\n-\t     (match_operand 7 \"const_int_operand\"                      \"  i,  i,    i,  i,  i,    i\")\n-\t     (match_operand 8 \"const_int_operand\"                      \"  i,  i,    i,  i,  i,    i\")\n+\t    [(match_operand:<VM> 1 \"vector_mask_operand\"               \" vm, vm,Wc1,Wc1,vmWc1,vmWc1\")\n+\t     (match_operand 5 \"vector_length_operand\"                  \" rK, rK, rK, rK,   rK,   rK\")\n+\t     (match_operand 6 \"const_int_operand\"                      \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 7 \"const_int_operand\"                      \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 8 \"const_int_operand\"                      \"  i,  i,  i,  i,    i,    i\")\n \t     (reg:SI VL_REGNUM)\n \t     (reg:SI VTYPE_REGNUM)] UNSPEC_VPREDICATE)\n \t  (truncate:<V_DOUBLE_TRUNC>\n \t    (any_shiftrt:VWEXTI\n-\t     (match_operand:VWEXTI 3 \"register_operand\"                \"  0,  0,   vr,  0,  0,   vr\")\n-\t     (match_operand 4 \"pmode_reg_or_uimm5_operand\"             \"  r,  r,    r,  K,  K,    K\")))\n-\t  (match_operand:<V_DOUBLE_TRUNC> 2 \"vector_merge_operand\"     \"0vu,0vu,  0vu,0vu,0vu,  0vu\")))]\n+\t     (match_operand:VWEXTI 3 \"register_operand\"                \"  0,  0,  0,  0,   vr,   vr\")\n+\t     (match_operand 4 \"pmode_reg_or_uimm5_operand\"             \" rK, rK, rK, rK,   rK,   rK\")))\n+\t  (match_operand:<V_DOUBLE_TRUNC> 2 \"vector_merge_operand\"     \" vu,  0, vu,  0,   vu,    0\")))]\n   \"TARGET_VECTOR\"\n   \"vn<insn>.w%o4\\t%0,%3,%4%p1\"\n   [(set_attr \"type\" \"vnshift\")\n    (set_attr \"mode\" \"<V_DOUBLE_TRUNC>\")])\n \n ;; vncvt.x.x.w\n (define_insn \"@pred_trunc<mode>\"\n-  [(set (match_operand:<V_DOUBLE_TRUNC> 0 \"register_operand\"           \"=vd, vr,  &vr\")\n+  [(set (match_operand:<V_DOUBLE_TRUNC> 0 \"register_operand\"           \"=vd, vd, vr, vr,  &vr,  &vr\")\n \t(if_then_else:<V_DOUBLE_TRUNC>\n \t  (unspec:<VM>\n-\t    [(match_operand:<VM> 1 \"vector_mask_operand\"               \" vm,Wc1,vmWc1\")\n-\t     (match_operand 4 \"vector_length_operand\"                  \" rK, rK,   rK\")\n-\t     (match_operand 5 \"const_int_operand\"                      \"  i,  i,    i\")\n-\t     (match_operand 6 \"const_int_operand\"                      \"  i,  i,    i\")\n-\t     (match_operand 7 \"const_int_operand\"                      \"  i,  i,    i\")\n+\t    [(match_operand:<VM> 1 \"vector_mask_operand\"               \" vm, vm,Wc1,Wc1,vmWc1,vmWc1\")\n+\t     (match_operand 4 \"vector_length_operand\"                  \" rK, rK, rK, rK,   rK,   rK\")\n+\t     (match_operand 5 \"const_int_operand\"                      \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 6 \"const_int_operand\"                      \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 7 \"const_int_operand\"                      \"  i,  i,  i,  i,    i,    i\")\n \t     (reg:SI VL_REGNUM)\n \t     (reg:SI VTYPE_REGNUM)] UNSPEC_VPREDICATE)\n \t  (truncate:<V_DOUBLE_TRUNC>\n-\t    (match_operand:VWEXTI 3 \"register_operand\"                 \"  0,  0,   vr\"))\n-\t  (match_operand:<V_DOUBLE_TRUNC> 2 \"vector_merge_operand\"     \"0vu,0vu,  0vu\")))]\n+\t    (match_operand:VWEXTI 3 \"register_operand\"                 \"  0,  0,  0,  0,   vr,   vr\"))\n+\t  (match_operand:<V_DOUBLE_TRUNC> 2 \"vector_merge_operand\"     \" vu,  0, vu,  0,   vu,    0\")))]\n   \"TARGET_VECTOR\"\n   \"vncvt.x.x.w\\t%0,%3%p1\"\n   [(set_attr \"type\" \"vnshift\")\n@@ -3496,40 +3496,40 @@\n \n ;; CLIP\n (define_insn \"@pred_narrow_clip<v_su><mode>\"\n-  [(set (match_operand:<V_DOUBLE_TRUNC> 0 \"register_operand\"           \"=vd, vr,  &vr, vd, vr,  &vr\")\n+  [(set (match_operand:<V_DOUBLE_TRUNC> 0 \"register_operand\"           \"=vd,vd, vr, vr,vd, vr,  &vr,  &vr, vd, vr,  &vr,  &vr\")\n \t(if_then_else:<V_DOUBLE_TRUNC>\n \t  (unspec:<VM>\n-\t    [(match_operand:<VM> 1 \"vector_mask_operand\"               \" vm,Wc1,vmWc1, vm,Wc1,vmWc1\")\n-\t     (match_operand 5 \"vector_length_operand\"                  \" rK, rK,   rK, rK, rK,   rK\")\n-\t     (match_operand 6 \"const_int_operand\"                      \"  i,  i,    i,  i,  i,    i\")\n-\t     (match_operand 7 \"const_int_operand\"                      \"  i,  i,    i,  i,  i,    i\")\n-\t     (match_operand 8 \"const_int_operand\"                      \"  i,  i,    i,  i,  i,    i\")\n+\t    [(match_operand:<VM> 1 \"vector_mask_operand\"               \" vm,vm,Wc1,Wc1,vm,Wc1,vmWc1,vmWc1, vm,Wc1,vmWc1,vmWc1\")\n+\t     (match_operand 5 \"vector_length_operand\"                  \" rK,rK, rK, rK,rK, rK,   rK,   rK, rK, rK,   rK,   rK\")\n+\t     (match_operand 6 \"const_int_operand\"                      \"  i, i,  i,  i, i,  i,    i,    i,  i,  i,    i,    i\")\n+\t     (match_operand 7 \"const_int_operand\"                      \"  i, i,  i,  i, i,  i,    i,    i,  i,  i,    i,    i\")\n+\t     (match_operand 8 \"const_int_operand\"                      \"  i, i,  i,  i, i,  i,    i,    i,  i,  i,    i,    i\")\n \t     (reg:SI VL_REGNUM)\n \t     (reg:SI VTYPE_REGNUM)] UNSPEC_VPREDICATE)\n \t  (unspec:<V_DOUBLE_TRUNC>\n-\t    [(match_operand:VWEXTI 3 \"register_operand\"                 \"  0,  0,   vr,  0,  0,   vr\")\n-\t     (match_operand:<V_DOUBLE_TRUNC> 4 \"vector_shift_operand\"   \" vr, vr,   vr, vk, vk,   vk\")] VNCLIP)\n-\t  (match_operand:<V_DOUBLE_TRUNC> 2 \"vector_merge_operand\"      \"0vu,0vu,  0vu,0vu,0vu,  0vu\")))]\n+\t    [(match_operand:VWEXTI 3 \"register_operand\"                \" vr,vr, vr, vr, 0,  0,   vr,   vr,  0,  0,   vr,   vr\")\n+\t     (match_operand:<V_DOUBLE_TRUNC> 4 \"vector_shift_operand\"  \"  0, 0,  0,  0,vr, vr,   vr,   vr, vk, vk,   vk,   vk\")] VNCLIP)\n+\t  (match_operand:<V_DOUBLE_TRUNC> 2 \"vector_merge_operand\"     \"  0,vu,  0, vu,vu, vu,   vu,    0, vu, vu,   vu,    0\")))]\n   \"TARGET_VECTOR\"\n   \"vnclip<v_su>.w%o4\\t%0,%3,%v4%p1\"\n   [(set_attr \"type\" \"vnclip\")\n    (set_attr \"mode\" \"<V_DOUBLE_TRUNC>\")])\n \n (define_insn \"@pred_narrow_clip<v_su><mode>_scalar\"\n-  [(set (match_operand:<V_DOUBLE_TRUNC> 0 \"register_operand\"           \"=vd, vr,  &vr, vd, vr,  &vr\")\n+  [(set (match_operand:<V_DOUBLE_TRUNC> 0 \"register_operand\"           \"=vd, vd, vr, vr,  &vr,  &vr\")\n \t(if_then_else:<V_DOUBLE_TRUNC>\n \t  (unspec:<VM>\n-\t    [(match_operand:<VM> 1 \"vector_mask_operand\"               \" vm,Wc1,vmWc1, vm,Wc1,vmWc1\")\n-\t     (match_operand 5 \"vector_length_operand\"                  \" rK, rK,   rK, rK, rK,   rK\")\n-\t     (match_operand 6 \"const_int_operand\"                      \"  i,  i,    i,  i,  i,    i\")\n-\t     (match_operand 7 \"const_int_operand\"                      \"  i,  i,    i,  i,  i,    i\")\n-\t     (match_operand 8 \"const_int_operand\"                      \"  i,  i,    i,  i,  i,    i\")\n+\t    [(match_operand:<VM> 1 \"vector_mask_operand\"               \" vm, vm,Wc1,Wc1,vmWc1,vmWc1\")\n+\t     (match_operand 5 \"vector_length_operand\"                  \" rK, rK, rK, rK,   rK,   rK\")\n+\t     (match_operand 6 \"const_int_operand\"                      \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 7 \"const_int_operand\"                      \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 8 \"const_int_operand\"                      \"  i,  i,  i,  i,    i,    i\")\n \t     (reg:SI VL_REGNUM)\n \t     (reg:SI VTYPE_REGNUM)] UNSPEC_VPREDICATE)\n \t  (unspec:<V_DOUBLE_TRUNC>\n-\t    [(match_operand:VWEXTI 3 \"register_operand\"                \"  0,  0,   vr,  0,  0,   vr\")\n-\t     (match_operand 4 \"pmode_reg_or_uimm5_operand\"             \"  r,  r,    r,  K,  K,    K\")] VNCLIP)\n-\t  (match_operand:<V_DOUBLE_TRUNC> 2 \"vector_merge_operand\"     \"0vu,0vu,  0vu,0vu,0vu,  0vu\")))]\n+\t    [(match_operand:VWEXTI 3 \"register_operand\"                \"  0,  0,  0,  0,   vr,   vr\")\n+\t     (match_operand 4 \"pmode_reg_or_uimm5_operand\"             \" rK, rK, rK, rK,   rK,   rK\")] VNCLIP)\n+\t  (match_operand:<V_DOUBLE_TRUNC> 2 \"vector_merge_operand\"     \" vu,  0, vu,  0,   vu,    0\")))]\n   \"TARGET_VECTOR\"\n   \"vnclip<v_su>.w%o4\\t%0,%3,%4%p1\"\n   [(set_attr \"type\" \"vnclip\")\n@@ -6342,96 +6342,96 @@\n ;; -------------------------------------------------------------------------------\n \n (define_insn \"@pred_narrow_fcvt_x<v_su>_f<mode>\"\n-  [(set (match_operand:<VNCONVERT> 0 \"register_operand\"        \"=vd, vr, ?&vr\")\n+  [(set (match_operand:<VNCONVERT> 0 \"register_operand\"        \"=vd, vd, vr, vr,  &vr,  &vr\")\n \t(if_then_else:<VNCONVERT>\n \t  (unspec:<VM>\n-\t    [(match_operand:<VM> 1 \"vector_mask_operand\"       \" vm,Wc1,vmWc1\")\n-\t     (match_operand 4 \"vector_length_operand\"          \" rK, rK,   rK\")\n-\t     (match_operand 5 \"const_int_operand\"              \"  i,  i,    i\")\n-\t     (match_operand 6 \"const_int_operand\"              \"  i,  i,    i\")\n-\t     (match_operand 7 \"const_int_operand\"              \"  i,  i,    i\")\n+\t    [(match_operand:<VM> 1 \"vector_mask_operand\"       \" vm, vm,Wc1,Wc1,vmWc1,vmWc1\")\n+\t     (match_operand 4 \"vector_length_operand\"          \" rK, rK, rK, rK,   rK,   rK\")\n+\t     (match_operand 5 \"const_int_operand\"              \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 6 \"const_int_operand\"              \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 7 \"const_int_operand\"              \"  i,  i,  i,  i,    i,    i\")\n \t     (reg:SI VL_REGNUM)\n \t     (reg:SI VTYPE_REGNUM)] UNSPEC_VPREDICATE)\n \t  (unspec:<VNCONVERT>\n-\t     [(match_operand:VF 3 \"register_operand\"           \"  0,  0,   vr\")] VFCVTS)\n-\t  (match_operand:<VNCONVERT> 2 \"vector_merge_operand\"  \"0vu,0vu,  0vu\")))]\n+\t     [(match_operand:VF 3 \"register_operand\"           \"  0,  0,  0,  0,   vr,   vr\")] VFCVTS)\n+\t  (match_operand:<VNCONVERT> 2 \"vector_merge_operand\"  \" vu,  0, vu,  0,   vu,    0\")))]\n   \"TARGET_VECTOR\"\n   \"vfncvt.x<v_su>.f.w\\t%0,%3%p1\"\n   [(set_attr \"type\" \"vfncvtftoi\")\n    (set_attr \"mode\" \"<VNCONVERT>\")])\n \n (define_insn \"@pred_narrow_<fix_cvt><mode>\"\n-  [(set (match_operand:<VNCONVERT> 0 \"register_operand\"        \"=vd, vr, ?&vr\")\n+  [(set (match_operand:<VNCONVERT> 0 \"register_operand\"        \"=vd, vd, vr, vr,  &vr,  &vr\")\n \t(if_then_else:<VNCONVERT>\n \t  (unspec:<VM>\n-\t    [(match_operand:<VM> 1 \"vector_mask_operand\"      \"  vm,Wc1,vmWc1\")\n-\t     (match_operand 4 \"vector_length_operand\"         \"  rK, rK,   rK\")\n-\t     (match_operand 5 \"const_int_operand\"             \"   i,  i,    i\")\n-\t     (match_operand 6 \"const_int_operand\"             \"   i,  i,    i\")\n-\t     (match_operand 7 \"const_int_operand\"             \"   i,  i,    i\")\n+\t    [(match_operand:<VM> 1 \"vector_mask_operand\"      \" vm, vm,Wc1,Wc1,vmWc1,vmWc1\")\n+\t     (match_operand 4 \"vector_length_operand\"         \" rK, rK, rK, rK,   rK,   rK\")\n+\t     (match_operand 5 \"const_int_operand\"             \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 6 \"const_int_operand\"             \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 7 \"const_int_operand\"             \"  i,  i,  i,  i,    i,    i\")\n \t     (reg:SI VL_REGNUM)\n \t     (reg:SI VTYPE_REGNUM)] UNSPEC_VPREDICATE)\n \t  (any_fix:<VNCONVERT>\n-\t     (match_operand:VF 3 \"register_operand\"           \"   0,  0,   vr\"))\n-\t  (match_operand:<VNCONVERT> 2 \"vector_merge_operand\" \" 0vu,0vu,  0vu\")))]\n+\t     (match_operand:VF 3 \"register_operand\"           \"  0,  0,  0,  0,   vr,   vr\"))\n+\t  (match_operand:<VNCONVERT> 2 \"vector_merge_operand\" \" vu,  0, vu,  0,   vu,    0\")))]\n   \"TARGET_VECTOR\"\n   \"vfncvt.rtz.x<u>.f.w\\t%0,%3%p1\"\n   [(set_attr \"type\" \"vfncvtftoi\")\n    (set_attr \"mode\" \"<VNCONVERT>\")])\n \n (define_insn \"@pred_narrow_<float_cvt><mode>\"\n-  [(set (match_operand:<VNCONVERT> 0 \"register_operand\"       \"=vd, vr, ?&vr\")\n+  [(set (match_operand:<VNCONVERT> 0 \"register_operand\"       \"=vd, vd, vr, vr,  &vr,  &vr\")\n \t(if_then_else:<VNCONVERT>\n \t  (unspec:<VM>\n-\t    [(match_operand:<VM> 1 \"vector_mask_operand\"      \" vm,Wc1,vmWc1\")\n-\t     (match_operand 4 \"vector_length_operand\"         \" rK, rK,   rK\")\n-\t     (match_operand 5 \"const_int_operand\"             \"  i,  i,    i\")\n-\t     (match_operand 6 \"const_int_operand\"             \"  i,  i,    i\")\n-\t     (match_operand 7 \"const_int_operand\"             \"  i,  i,    i\")\n+\t    [(match_operand:<VM> 1 \"vector_mask_operand\"      \" vm, vm,Wc1,Wc1,vmWc1,vmWc1\")\n+\t     (match_operand 4 \"vector_length_operand\"         \" rK, rK, rK, rK,   rK,   rK\")\n+\t     (match_operand 5 \"const_int_operand\"             \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 6 \"const_int_operand\"             \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 7 \"const_int_operand\"             \"  i,  i,  i,  i,    i,    i\")\n \t     (reg:SI VL_REGNUM)\n \t     (reg:SI VTYPE_REGNUM)] UNSPEC_VPREDICATE)\n \t  (any_float:<VNCONVERT>\n-\t     (match_operand:VWCONVERTI 3 \"register_operand\"   \"  0,  0,   vr\"))\n-\t  (match_operand:<VNCONVERT> 2 \"vector_merge_operand\" \"0vu,0vu,  0vu\")))]\n+\t     (match_operand:VWCONVERTI 3 \"register_operand\"   \"  0,  0,  0,  0,   vr,   vr\"))\n+\t  (match_operand:<VNCONVERT> 2 \"vector_merge_operand\" \" vu,  0, vu,  0,   vu,    0\")))]\n   \"TARGET_VECTOR\"\n   \"vfncvt.f.x<u>.w\\t%0,%3%p1\"\n   [(set_attr \"type\" \"vfncvtitof\")\n    (set_attr \"mode\" \"<VNCONVERT>\")])\n \n (define_insn \"@pred_trunc<mode>\"\n-  [(set (match_operand:<V_DOUBLE_TRUNC> 0 \"register_operand\"       \"=vd, vr, ?&vr\")\n+  [(set (match_operand:<V_DOUBLE_TRUNC> 0 \"register_operand\"       \"=vd, vd, vr, vr,  &vr,  &vr\")\n \t(if_then_else:<V_DOUBLE_TRUNC>\n \t  (unspec:<VM>\n-\t    [(match_operand:<VM> 1 \"vector_mask_operand\"           \" vm,Wc1,vmWc1\")\n-\t     (match_operand 4 \"vector_length_operand\"              \" rK, rK,   rK\")\n-\t     (match_operand 5 \"const_int_operand\"                  \"  i,  i,    i\")\n-\t     (match_operand 6 \"const_int_operand\"                  \"  i,  i,    i\")\n-\t     (match_operand 7 \"const_int_operand\"                  \"  i,  i,    i\")\n+\t    [(match_operand:<VM> 1 \"vector_mask_operand\"           \" vm, vm,Wc1,Wc1,vmWc1,vmWc1\")\n+\t     (match_operand 4 \"vector_length_operand\"              \" rK, rK, rK, rK,   rK,   rK\")\n+\t     (match_operand 5 \"const_int_operand\"                  \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 6 \"const_int_operand\"                  \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 7 \"const_int_operand\"                  \"  i,  i,  i,  i,    i,    i\")\n \t     (reg:SI VL_REGNUM)\n \t     (reg:SI VTYPE_REGNUM)] UNSPEC_VPREDICATE)\n \t  (float_truncate:<V_DOUBLE_TRUNC>\n-\t     (match_operand:VWEXTF 3 \"register_operand\"            \"  0,  0,   vr\"))\n-\t  (match_operand:<V_DOUBLE_TRUNC> 2 \"vector_merge_operand\" \"0vu,0vu,  0vu\")))]\n+\t     (match_operand:VWEXTF 3 \"register_operand\"            \"  0,  0,  0,  0,   vr,   vr\"))\n+\t  (match_operand:<V_DOUBLE_TRUNC> 2 \"vector_merge_operand\" \" vu,  0, vu,  0,   vu,    0\")))]\n   \"TARGET_VECTOR\"\n   \"vfncvt.f.f.w\\t%0,%3%p1\"\n   [(set_attr \"type\" \"vfncvtftof\")\n    (set_attr \"mode\" \"<V_DOUBLE_TRUNC>\")])\n \n (define_insn \"@pred_rod_trunc<mode>\"\n-  [(set (match_operand:<V_DOUBLE_TRUNC> 0 \"register_operand\"       \"=vd, vr, ?&vr\")\n+  [(set (match_operand:<V_DOUBLE_TRUNC> 0 \"register_operand\"       \"=vd, vd, vr, vr,  &vr,  &vr\")\n \t(if_then_else:<V_DOUBLE_TRUNC>\n \t  (unspec:<VM>\n-\t    [(match_operand:<VM> 1 \"vector_mask_operand\"           \" vm,Wc1,vmWc1\")\n-\t     (match_operand 4 \"vector_length_operand\"              \" rK, rK,   rK\")\n-\t     (match_operand 5 \"const_int_operand\"                  \"  i,  i,    i\")\n-\t     (match_operand 6 \"const_int_operand\"                  \"  i,  i,    i\")\n-\t     (match_operand 7 \"const_int_operand\"                  \"  i,  i,    i\")\n+\t    [(match_operand:<VM> 1 \"vector_mask_operand\"           \" vm, vm,Wc1,Wc1,vmWc1,vmWc1\")\n+\t     (match_operand 4 \"vector_length_operand\"              \" rK, rK, rK, rK,   rK,   rK\")\n+\t     (match_operand 5 \"const_int_operand\"                  \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 6 \"const_int_operand\"                  \"  i,  i,  i,  i,    i,    i\")\n+\t     (match_operand 7 \"const_int_operand\"                  \"  i,  i,  i,  i,    i,    i\")\n \t     (reg:SI VL_REGNUM)\n \t     (reg:SI VTYPE_REGNUM)] UNSPEC_VPREDICATE)\n \t  (unspec:<V_DOUBLE_TRUNC>\n \t    [(float_truncate:<V_DOUBLE_TRUNC>\n-\t       (match_operand:VWEXTF 3 \"register_operand\"            \"  0,  0,   vr\"))] UNSPEC_ROD)\n-\t  (match_operand:<V_DOUBLE_TRUNC> 2 \"vector_merge_operand\" \"0vu,0vu,  0vu\")))]\n+\t       (match_operand:VWEXTF 3 \"register_operand\"          \"  0,  0,  0,  0,   vr,   vr\"))] UNSPEC_ROD)\n+\t  (match_operand:<V_DOUBLE_TRUNC> 2 \"vector_merge_operand\" \" vu,  0, vu,  0,   vu,    0\")))]\n   \"TARGET_VECTOR\"\n   \"vfncvt.rod.f.f.w\\t%0,%3%p1\"\n   [(set_attr \"type\" \"vfncvtftof\")"}, {"sha": "0cdf60cde0678bd305e82cba4d3e5316bdcb94e3", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/narrow_constraint-1.c", "status": "added", "additions": 319, "deletions": 0, "changes": 319, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-1.c?ref=7ff57009bcc728044ba2de339ecd16721d48aba3", "patch": "@@ -0,0 +1,319 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+void f0 (int16_t *base,int8_t *out,size_t vl)\n+{\n+    vint16mf4_t src = __riscv_vle16_v_i16mf4 (base, vl);\n+    vint8mf8_t v = __riscv_vncvt_x_x_w_i8mf8(src,vl);\n+    __riscv_vse8_v_i8mf8 (out,v,vl);\n+}\n+\n+void f1 (int16_t *base,int8_t *out,size_t vl)\n+{\n+    vint16mf4_t src = __riscv_vle16_v_i16mf4 (base, vl);\n+    vint8mf8_t src2 = __riscv_vle8_v_i8mf8 ((int8_t *)(base + 100), vl);\n+    vint8mf8_t v = __riscv_vncvt_x_x_w_i8mf8_tu(src2,src,vl);\n+    __riscv_vse8_v_i8mf8 (out,v,vl);\n+}\n+\n+void f2 (int16_t *base,int8_t *out,size_t vl)\n+{\n+    vint16mf4_t src = __riscv_vle16_v_i16mf4 (base, vl);\n+    vint8mf8_t v = __riscv_vncvt_x_x_w_i8mf8(src,vl);\n+    vint16mf4_t v2 = __riscv_vadd_vv_i16mf4 (src, src,vl);\n+    __riscv_vse8_v_i8mf8 (out,v,vl);\n+    __riscv_vse16_v_i16mf4 ((int16_t *)out,v2,vl);\n+}\n+\n+void f3 (int16_t *base,int8_t *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vint16mf4_t src = __riscv_vle16_v_i16mf4 (base + 100*i, vl);\n+      vint8mf8_t v = __riscv_vncvt_x_x_w_i8mf8(src,vl);\n+      vint16mf4_t v2 = __riscv_vadd_vv_i16mf4 (src, src,vl);\n+      __riscv_vse8_v_i8mf8 (out + 100*i,v,vl);\n+      __riscv_vse16_v_i16mf4 ((int16_t *)(out + 200*i),v2,vl);\n+    }\n+}\n+\n+void f4 (int16_t *base,int8_t *out,size_t vl)\n+{\n+    vint16mf4_t src = __riscv_vle16_v_i16mf4 (base, vl);\n+    vint8mf8_t v = __riscv_vncvt_x_x_w_i8mf8(src,vl);\n+    v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+    v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+    vint16mf4_t v2 = __riscv_vadd_vv_i16mf4 (src, src,vl);\n+    __riscv_vse8_v_i8mf8 (out,v,vl);\n+    __riscv_vse16_v_i16mf4 ((int16_t *)out,v2,vl);\n+}\n+\n+void f5 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vint16mf4_t src = __riscv_vle16_v_i16mf4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vint8mf8_t v = __riscv_vncvt_x_x_w_i8mf8_m(m,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+      v = __riscv_vle8_v_i8mf8_tu (v, base2, vl);\n+      __riscv_vse8_v_i8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f6 (int16_t *base,int8_t *out,size_t vl)\n+{\n+    vint16m2_t src = __riscv_vle16_v_i16m2 (base, vl);\n+    vint8m1_t v = __riscv_vncvt_x_x_w_i8m1(src,vl);\n+    __riscv_vse8_v_i8m1 (out,v,vl);\n+}\n+\n+void f7 (int16_t *base,int8_t *out,size_t vl)\n+{\n+    vint16m2_t src = __riscv_vle16_v_i16m2 (base, vl);\n+    vint8m1_t src2 = __riscv_vle8_v_i8m1 ((int8_t *)(base + 100), vl);\n+    vint8m1_t v = __riscv_vncvt_x_x_w_i8m1_tu(src2,src,vl);\n+    __riscv_vse8_v_i8m1 (out,v,vl);\n+}\n+\n+void f8 (int16_t *base,int8_t *out,size_t vl)\n+{\n+    vint16m2_t src = __riscv_vle16_v_i16m2 (base, vl);\n+    vint8m1_t v = __riscv_vncvt_x_x_w_i8m1(src,vl);\n+    vint16m2_t v2 = __riscv_vadd_vv_i16m2 (src, src,vl);\n+    __riscv_vse8_v_i8m1 (out,v,vl);\n+    __riscv_vse16_v_i16m2 ((int16_t *)out,v2,vl);\n+}\n+\n+void f9 (int16_t *base,int8_t *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vint16m2_t src = __riscv_vle16_v_i16m2 (base + 100*i, vl);\n+      vint8m1_t v = __riscv_vncvt_x_x_w_i8m1(src,vl);\n+      vint16m2_t v2 = __riscv_vadd_vv_i16m2 (src, src,vl);\n+      __riscv_vse8_v_i8m1 (out + 100*i,v,vl);\n+      __riscv_vse16_v_i16m2 ((int16_t *)(out + 200*i),v2,vl);\n+    }\n+}\n+\n+void f10 (int16_t *base,int8_t *out,size_t vl)\n+{\n+    vint16m2_t src = __riscv_vle16_v_i16m2 (base, vl);\n+    vint8m1_t v = __riscv_vncvt_x_x_w_i8m1(src,vl);\n+    v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+    v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+    vint16m2_t v2 = __riscv_vadd_vv_i16m2 (src, src,vl);\n+    __riscv_vse8_v_i8m1 (out,v,vl);\n+    __riscv_vse16_v_i16m2 ((int16_t *)out,v2,vl);\n+}\n+\n+void f11 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vint16m2_t src = __riscv_vle16_v_i16m2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vint8m1_t v = __riscv_vncvt_x_x_w_i8m1_m(m,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+      v = __riscv_vle8_v_i8m1_tu (v, base2, vl);\n+      __riscv_vse8_v_i8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f12 (int16_t *base,int8_t *out,size_t vl, int n)\n+{\n+    vint8mf8_t v = __riscv_vle8_v_i8mf8 ((int8_t *)(base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vint16mf4_t src = __riscv_vle16_v_i16mf4 (base + 100*i, vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+      __riscv_vse8_v_i8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f13 (int16_t *base,int8_t *out,size_t vl, int n)\n+{\n+    vint8m1_t v = __riscv_vle8_v_i8m1 ((int8_t *)(base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vint16m2_t src = __riscv_vle16_v_i16m2 (base + 100*i, vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+      __riscv_vse8_v_i8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f14 (int16_t *base,int8_t *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vint8mf8_t v = __riscv_vle8_v_i8mf8 ((int8_t *)(base + 1000 * i), vl);\n+      vint16mf4_t src = __riscv_vle16_v_i16mf4 (base + 100*i, vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src,vl);\n+      __riscv_vse8_v_i8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f15 (int16_t *base,int8_t *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vint8m1_t v = __riscv_vle8_v_i8m1 ((int8_t *)(base + 1000 * i), vl);\n+      vint16m2_t src = __riscv_vle16_v_i16m2 (base + 100*i, vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src,vl);\n+      __riscv_vse8_v_i8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f16 (int16_t *base,int8_t *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vint8mf8_t v = __riscv_vle8_v_i8mf8 ((int8_t *)(base + 1000 * i), vl);\n+      vint16mf4_t src1 = __riscv_vle16_v_i16mf4 (base + 100*i, vl);\n+      vint16mf4_t src2 = __riscv_vle16_v_i16mf4 (base + 200*i, vl);\n+      vint16mf4_t src3 = __riscv_vle16_v_i16mf4 (base + 300*i, vl);\n+      vint16mf4_t src4 = __riscv_vle16_v_i16mf4 (base + 400*i, vl);\n+      vint16mf4_t src5 = __riscv_vle16_v_i16mf4 (base + 500*i, vl);\n+      vint16mf4_t src6 = __riscv_vle16_v_i16mf4 (base + 600*i, vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src1,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src2,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src3,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src4,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src5,vl);\n+      v = __riscv_vncvt_x_x_w_i8mf8_tu(v,src6,vl);\n+      __riscv_vse8_v_i8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f17 (int16_t *base,int8_t *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vint8m1_t v = __riscv_vle8_v_i8m1 ((int8_t *)(base + 1000 * i), vl);\n+      vint16m2_t src1 = __riscv_vle16_v_i16m2 (base + 100*i, vl);\n+      vint16m2_t src2 = __riscv_vle16_v_i16m2 (base + 200*i, vl);\n+      vint16m2_t src3 = __riscv_vle16_v_i16m2 (base + 300*i, vl);\n+      vint16m2_t src4 = __riscv_vle16_v_i16m2 (base + 400*i, vl);\n+      vint16m2_t src5 = __riscv_vle16_v_i16m2 (base + 500*i, vl);\n+      vint16m2_t src6 = __riscv_vle16_v_i16m2 (base + 600*i, vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src1,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src2,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src3,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src4,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src5,vl);\n+      v = __riscv_vncvt_x_x_w_i8m1_tu(v,src6,vl);\n+      __riscv_vse8_v_i8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f18 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vint32mf2_t src = __riscv_vle32_v_i32mf2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vint16mf4_t v = __riscv_vncvt_x_x_w_i16mf4_m(m,src,vl);\n+      vint16mf4_t v2 = __riscv_vle16_v_i16mf4_tu (v, base2 + i, vl);\n+      vint8mf8_t v3 = __riscv_vncvt_x_x_w_i8mf8_m(m,v2,vl);\n+      __riscv_vse8_v_i8mf8 (out + 100*i,v3,vl);\n+    }\n+}\n+\n+void f19 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vint32m4_t src = __riscv_vle32_v_i32m4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vint16m2_t v = __riscv_vncvt_x_x_w_i16m2_m(m,src,vl);\n+      vint16m2_t v2 = __riscv_vle16_v_i16m2_tu (v, base2 + i, vl);\n+      vint8m1_t v3 = __riscv_vncvt_x_x_w_i8m1_m(m,v2,vl);\n+      vint8m1_t v4 = __riscv_vncvt_x_x_w_i8m1_tumu(m,v3,v2,vl);\n+      __riscv_vse8_v_i8m1 (out + 100*i,v3,vl);\n+      __riscv_vse8_v_i8m1 (out + 222*i,v4,vl);\n+    }\n+}\n+\n+void f20 (int16_t *base,int8_t *out,size_t vl)\n+{\n+    vint16m2_t src = __riscv_vle16_v_i16m2 (base, vl);\n+    /* Only allow load v30,v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+\n+    vint8m1_t v = __riscv_vncvt_x_x_w_i8m1(src,vl);\n+    /* Only allow vncvt SRC == DEST v30.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v31\");\n+\n+    __riscv_vse8_v_i8m1 (out,v,vl);\n+}\n+\n+void f21 (int16_t *base,int8_t *out,size_t vl)\n+{\n+    vint16m1_t src = __riscv_vle16_v_i16m1 (base, vl);\n+    /* Only allow load v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+\n+    vint8mf2_t v = __riscv_vncvt_x_x_w_i8mf2(src,vl);\n+    /* Only allow vncvt SRC == DEST v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+\n+    __riscv_vse8_v_i8mf2 (out,v,vl);\n+}\n+\n+void f22 (int16_t *base,int8_t *out,size_t vl)\n+{\n+    vint16m2_t src = __riscv_vle16_v_i16m2 (base, vl);\n+    /* Only allow load v30,v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+\n+    vint8m1_t v = __riscv_vncvt_x_x_w_i8m1(src,vl);\n+    /* Only allow v29.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+    v = __riscv_vadd_vv_i8m1 (v,v,vl);\n+    /* Only allow v29.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+\n+    __riscv_vse8_v_i8m1 (out,v,vl);\n+}\n+\n+/* { dg-final { scan-assembler-not {vmv} } } */\n+/* { dg-final { scan-assembler-not {csrr} } } */"}, {"sha": "5b371482d9bf421355f621cf357e439b17766721", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/narrow_constraint-10.c", "status": "added", "additions": 293, "deletions": 0, "changes": 293, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-10.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-10.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-10.c?ref=7ff57009bcc728044ba2de339ecd16721d48aba3", "patch": "@@ -0,0 +1,293 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+void f0 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m1_t src = __riscv_vle64_v_f64m1 (base, vl);\n+    vfloat32mf2_t v = __riscv_vfncvt_f_f_w_f32mf2(src,vl);\n+    __riscv_vse32_v_f32mf2 (out,v,vl);\n+}\n+\n+void f1 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m1_t src = __riscv_vle64_v_f64m1 (base, vl);\n+    vfloat32mf2_t src2 = __riscv_vle32_v_f32mf2 ((void *)(base + 100), vl);\n+    vfloat32mf2_t v = __riscv_vfncvt_f_f_w_f32mf2_tu(src2,src,vl);\n+    __riscv_vse32_v_f32mf2 (out,v,vl);\n+}\n+\n+void f2 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m1_t src = __riscv_vle64_v_f64m1 (base, vl);\n+    vfloat32mf2_t v = __riscv_vfncvt_f_f_w_f32mf2(src,vl);\n+    vfloat64m1_t v2 = __riscv_vfadd_vv_f64m1 (src, src,vl);\n+    __riscv_vse32_v_f32mf2 (out,v,vl);\n+    __riscv_vse64_v_f64m1 ((void *)out,v2,vl);\n+}\n+\n+void f3 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vfloat64m1_t src = __riscv_vle64_v_f64m1 (base + 100*i, vl);\n+      vfloat32mf2_t v = __riscv_vfncvt_f_f_w_f32mf2(src,vl);\n+      vfloat64m1_t v2 = __riscv_vfadd_vv_f64m1 (src, src,vl);\n+      __riscv_vse32_v_f32mf2 (out + 100*i,v,vl);\n+      __riscv_vse64_v_f64m1 ((void *)(out + 200*i),v2,vl);\n+    }\n+}\n+\n+void f4 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m1_t src = __riscv_vle64_v_f64m1 (base, vl);\n+    vfloat32mf2_t v = __riscv_vfncvt_f_f_w_f32mf2(src,vl);\n+    v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+    v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+    vfloat64m1_t v2 = __riscv_vfadd_vv_f64m1 (src, src,vl);\n+    __riscv_vse32_v_f32mf2 (out,v,vl);\n+    __riscv_vse64_v_f64m1 ((void *)out,v2,vl);\n+}\n+\n+void f5 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vfloat64m1_t src = __riscv_vle64_v_f64m1 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vfloat32mf2_t v = __riscv_vfncvt_f_f_w_f32mf2_m(m,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vle32_v_f32mf2_tu (v, base2, vl);\n+      __riscv_vse32_v_f32mf2 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f6 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m2_t src = __riscv_vle64_v_f64m2 (base, vl);\n+    vfloat32m1_t v = __riscv_vfncvt_f_f_w_f32m1(src,vl);\n+    __riscv_vse32_v_f32m1 (out,v,vl);\n+}\n+\n+void f7 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m2_t src = __riscv_vle64_v_f64m2 (base, vl);\n+    vfloat32m1_t src2 = __riscv_vle32_v_f32m1 ((void *)(base + 100), vl);\n+    vfloat32m1_t v = __riscv_vfncvt_f_f_w_f32m1_tu(src2,src,vl);\n+    __riscv_vse32_v_f32m1 (out,v,vl);\n+}\n+\n+void f8 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m2_t src = __riscv_vle64_v_f64m2 (base, vl);\n+    vfloat32m1_t v = __riscv_vfncvt_f_f_w_f32m1(src,vl);\n+    vfloat64m2_t v2 = __riscv_vfadd_vv_f64m2 (src, src,vl);\n+    __riscv_vse32_v_f32m1 (out,v,vl);\n+    __riscv_vse64_v_f64m2 ((void *)out,v2,vl);\n+}\n+\n+void f9 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vfloat64m2_t src = __riscv_vle64_v_f64m2 (base + 100*i, vl);\n+      vfloat32m1_t v = __riscv_vfncvt_f_f_w_f32m1(src,vl);\n+      vfloat64m2_t v2 = __riscv_vfadd_vv_f64m2 (src, src,vl);\n+      __riscv_vse32_v_f32m1 (out + 100*i,v,vl);\n+      __riscv_vse64_v_f64m2 ((void *)(out + 200*i),v2,vl);\n+    }\n+}\n+\n+void f10 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m2_t src = __riscv_vle64_v_f64m2 (base, vl);\n+    vfloat32m1_t v = __riscv_vfncvt_f_f_w_f32m1(src,vl);\n+    v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+    v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+    vfloat64m2_t v2 = __riscv_vfadd_vv_f64m2 (src, src,vl);\n+    __riscv_vse32_v_f32m1 (out,v,vl);\n+    __riscv_vse64_v_f64m2 ((void *)out,v2,vl);\n+}\n+\n+void f11 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vfloat64m2_t src = __riscv_vle64_v_f64m2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool32_t m = __riscv_vlm_v_b32 (base + i, vl);\n+      vfloat32m1_t v = __riscv_vfncvt_f_f_w_f32m1_m(m,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vle32_v_f32m1_tu (v, base2, vl);\n+      __riscv_vse32_v_f32m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f12 (void *base,void *out,size_t vl, int n)\n+{\n+    vfloat32mf2_t v = __riscv_vle32_v_f32mf2 ((void *)(base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vfloat64m1_t src = __riscv_vle64_v_f64m1 (base + 100*i, vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      __riscv_vse32_v_f32mf2 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f13 (void *base,void *out,size_t vl, int n)\n+{\n+    vfloat32m1_t v = __riscv_vle32_v_f32m1 ((void *)(base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vfloat64m2_t src = __riscv_vle64_v_f64m2 (base + 100*i, vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+      __riscv_vse32_v_f32m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f14 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vfloat32mf2_t v = __riscv_vle32_v_f32mf2 ((void *)(base + 1000 * i), vl);\n+      vfloat64m1_t src = __riscv_vle64_v_f64m1 (base + 100*i, vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      __riscv_vse32_v_f32mf2 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f15 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vfloat32m1_t v = __riscv_vle32_v_f32m1 ((void *)(base + 1000 * i), vl);\n+      vfloat64m2_t src = __riscv_vle64_v_f64m2 (base + 100*i, vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src,vl);\n+      __riscv_vse32_v_f32m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f16 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vfloat32mf2_t v = __riscv_vle32_v_f32mf2 ((void *)(base + 1000 * i), vl);\n+      vfloat64m1_t src1 = __riscv_vle64_v_f64m1 (base + 100*i, vl);\n+      vfloat64m1_t src2 = __riscv_vle64_v_f64m1 (base + 200*i, vl);\n+      vfloat64m1_t src3 = __riscv_vle64_v_f64m1 (base + 300*i, vl);\n+      vfloat64m1_t src4 = __riscv_vle64_v_f64m1 (base + 400*i, vl);\n+      vfloat64m1_t src5 = __riscv_vle64_v_f64m1 (base + 500*i, vl);\n+      vfloat64m1_t src6 = __riscv_vle64_v_f64m1 (base + 600*i, vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src1,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src2,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src3,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src4,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src5,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src6,vl);\n+      __riscv_vse32_v_f32mf2 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f17 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vfloat32m1_t v = __riscv_vle32_v_f32m1 ((void *)(base + 1000 * i), vl);\n+      vfloat64m2_t src1 = __riscv_vle64_v_f64m2 (base + 100*i, vl);\n+      vfloat64m2_t src2 = __riscv_vle64_v_f64m2 (base + 200*i, vl);\n+      vfloat64m2_t src3 = __riscv_vle64_v_f64m2 (base + 300*i, vl);\n+      vfloat64m2_t src4 = __riscv_vle64_v_f64m2 (base + 400*i, vl);\n+      vfloat64m2_t src5 = __riscv_vle64_v_f64m2 (base + 500*i, vl);\n+      vfloat64m2_t src6 = __riscv_vle64_v_f64m2 (base + 600*i, vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src1,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src2,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src3,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src4,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src5,vl);\n+      v = __riscv_vfncvt_f_f_w_f32m1_tu(v,src6,vl);\n+      __riscv_vse32_v_f32m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f18 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m2_t src = __riscv_vle64_v_f64m2 (base, vl);\n+    /* Only allow load v30,v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+\n+    vfloat32m1_t v = __riscv_vfncvt_f_f_w_f32m1(src,vl);\n+    /* Only allow vncvt SRC == DEST v30.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v31\");\n+\n+    __riscv_vse32_v_f32m1 (out,v,vl);\n+}\n+\n+void f19 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m1_t src = __riscv_vle64_v_f64m1 (base, vl);\n+    /* Only allow load v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+\n+    vfloat32mf2_t v = __riscv_vfncvt_f_f_w_f32mf2(src,vl);\n+    /* Only allow vncvt SRC == DEST v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+\n+    __riscv_vse32_v_f32mf2 (out,v,vl);\n+}\n+\n+void f20 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m2_t src = __riscv_vle64_v_f64m2 (base, vl);\n+    /* Only allow load v30,v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+\n+    vfloat32m1_t v = __riscv_vfncvt_f_f_w_f32m1(src,vl);\n+    /* Only allow v29.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+    v = __riscv_vfadd_vv_f32m1 (v,v,vl);\n+    /* Only allow v29.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+\n+    __riscv_vse32_v_f32m1 (out,v,vl);\n+}\n+\n+/* { dg-final { scan-assembler-not {vmv} } } */\n+/* { dg-final { scan-assembler-not {csrr} } } */"}, {"sha": "4d4e4f30e9f4ae2a23cf39afad6b0edeb711e5f8", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/narrow_constraint-11.c", "status": "added", "additions": 293, "deletions": 0, "changes": 293, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-11.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-11.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-11.c?ref=7ff57009bcc728044ba2de339ecd16721d48aba3", "patch": "@@ -0,0 +1,293 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+void f0 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m1_t src = __riscv_vle64_v_f64m1 (base, vl);\n+    vfloat32mf2_t v = __riscv_vfncvt_f_f_w_f32mf2(src,vl);\n+    __riscv_vse32_v_f32mf2 (out,v,vl);\n+}\n+\n+void f1 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m1_t src = __riscv_vle64_v_f64m1 (base, vl);\n+    vfloat32mf2_t src2 = __riscv_vle32_v_f32mf2 ((void *)(base + 100), vl);\n+    vfloat32mf2_t v = __riscv_vfncvt_f_f_w_f32mf2_tu(src2,src,vl);\n+    __riscv_vse32_v_f32mf2 (out,v,vl);\n+}\n+\n+void f2 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m1_t src = __riscv_vle64_v_f64m1 (base, vl);\n+    vfloat32mf2_t v = __riscv_vfncvt_f_f_w_f32mf2(src,vl);\n+    vfloat64m1_t v2 = __riscv_vfadd_vv_f64m1 (src, src,vl);\n+    __riscv_vse32_v_f32mf2 (out,v,vl);\n+    __riscv_vse64_v_f64m1 ((void *)out,v2,vl);\n+}\n+\n+void f3 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vfloat64m1_t src = __riscv_vle64_v_f64m1 (base + 100*i, vl);\n+      vfloat32mf2_t v = __riscv_vfncvt_f_f_w_f32mf2(src,vl);\n+      vfloat64m1_t v2 = __riscv_vfadd_vv_f64m1 (src, src,vl);\n+      __riscv_vse32_v_f32mf2 (out + 100*i,v,vl);\n+      __riscv_vse64_v_f64m1 ((void *)(out + 200*i),v2,vl);\n+    }\n+}\n+\n+void f4 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m1_t src = __riscv_vle64_v_f64m1 (base, vl);\n+    vfloat32mf2_t v = __riscv_vfncvt_f_f_w_f32mf2(src,vl);\n+    v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+    v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+    vfloat64m1_t v2 = __riscv_vfadd_vv_f64m1 (src, src,vl);\n+    __riscv_vse32_v_f32mf2 (out,v,vl);\n+    __riscv_vse64_v_f64m1 ((void *)out,v2,vl);\n+}\n+\n+void f5 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vfloat64m1_t src = __riscv_vle64_v_f64m1 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vfloat32mf2_t v = __riscv_vfncvt_f_f_w_f32mf2_m(m,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vle32_v_f32mf2_tu (v, base2, vl);\n+      __riscv_vse32_v_f32mf2 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f6 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m2_t src = __riscv_vle64_v_f64m2 (base, vl);\n+    vfloat32m1_t v = __riscv_vfncvt_rod_f_f_w_f32m1(src,vl);\n+    __riscv_vse32_v_f32m1 (out,v,vl);\n+}\n+\n+void f7 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m2_t src = __riscv_vle64_v_f64m2 (base, vl);\n+    vfloat32m1_t src2 = __riscv_vle32_v_f32m1 ((void *)(base + 100), vl);\n+    vfloat32m1_t v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(src2,src,vl);\n+    __riscv_vse32_v_f32m1 (out,v,vl);\n+}\n+\n+void f8 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m2_t src = __riscv_vle64_v_f64m2 (base, vl);\n+    vfloat32m1_t v = __riscv_vfncvt_rod_f_f_w_f32m1(src,vl);\n+    vfloat64m2_t v2 = __riscv_vfadd_vv_f64m2 (src, src,vl);\n+    __riscv_vse32_v_f32m1 (out,v,vl);\n+    __riscv_vse64_v_f64m2 ((void *)out,v2,vl);\n+}\n+\n+void f9 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vfloat64m2_t src = __riscv_vle64_v_f64m2 (base + 100*i, vl);\n+      vfloat32m1_t v = __riscv_vfncvt_rod_f_f_w_f32m1(src,vl);\n+      vfloat64m2_t v2 = __riscv_vfadd_vv_f64m2 (src, src,vl);\n+      __riscv_vse32_v_f32m1 (out + 100*i,v,vl);\n+      __riscv_vse64_v_f64m2 ((void *)(out + 200*i),v2,vl);\n+    }\n+}\n+\n+void f10 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m2_t src = __riscv_vle64_v_f64m2 (base, vl);\n+    vfloat32m1_t v = __riscv_vfncvt_rod_f_f_w_f32m1(src,vl);\n+    v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+    v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+    vfloat64m2_t v2 = __riscv_vfadd_vv_f64m2 (src, src,vl);\n+    __riscv_vse32_v_f32m1 (out,v,vl);\n+    __riscv_vse64_v_f64m2 ((void *)out,v2,vl);\n+}\n+\n+void f11 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vfloat64m2_t src = __riscv_vle64_v_f64m2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool32_t m = __riscv_vlm_v_b32 (base + i, vl);\n+      vfloat32m1_t v = __riscv_vfncvt_rod_f_f_w_f32m1_m(m,src,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vle32_v_f32m1_tu (v, base2, vl);\n+      __riscv_vse32_v_f32m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f12 (void *base,void *out,size_t vl, int n)\n+{\n+    vfloat32mf2_t v = __riscv_vle32_v_f32mf2 ((void *)(base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vfloat64m1_t src = __riscv_vle64_v_f64m1 (base + 100*i, vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      __riscv_vse32_v_f32mf2 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f13 (void *base,void *out,size_t vl, int n)\n+{\n+    vfloat32m1_t v = __riscv_vle32_v_f32m1 ((void *)(base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vfloat64m2_t src = __riscv_vle64_v_f64m2 (base + 100*i, vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+      __riscv_vse32_v_f32m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f14 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vfloat32mf2_t v = __riscv_vle32_v_f32mf2 ((void *)(base + 1000 * i), vl);\n+      vfloat64m1_t src = __riscv_vle64_v_f64m1 (base + 100*i, vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src,vl);\n+      __riscv_vse32_v_f32mf2 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f15 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vfloat32m1_t v = __riscv_vle32_v_f32m1 ((void *)(base + 1000 * i), vl);\n+      vfloat64m2_t src = __riscv_vle64_v_f64m2 (base + 100*i, vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src,vl);\n+      __riscv_vse32_v_f32m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f16 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vfloat32mf2_t v = __riscv_vle32_v_f32mf2 ((void *)(base + 1000 * i), vl);\n+      vfloat64m1_t src1 = __riscv_vle64_v_f64m1 (base + 100*i, vl);\n+      vfloat64m1_t src2 = __riscv_vle64_v_f64m1 (base + 200*i, vl);\n+      vfloat64m1_t src3 = __riscv_vle64_v_f64m1 (base + 300*i, vl);\n+      vfloat64m1_t src4 = __riscv_vle64_v_f64m1 (base + 400*i, vl);\n+      vfloat64m1_t src5 = __riscv_vle64_v_f64m1 (base + 500*i, vl);\n+      vfloat64m1_t src6 = __riscv_vle64_v_f64m1 (base + 600*i, vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src1,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src2,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src3,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src4,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src5,vl);\n+      v = __riscv_vfncvt_f_f_w_f32mf2_tu(v,src6,vl);\n+      __riscv_vse32_v_f32mf2 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f17 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vfloat32m1_t v = __riscv_vle32_v_f32m1 ((void *)(base + 1000 * i), vl);\n+      vfloat64m2_t src1 = __riscv_vle64_v_f64m2 (base + 100*i, vl);\n+      vfloat64m2_t src2 = __riscv_vle64_v_f64m2 (base + 200*i, vl);\n+      vfloat64m2_t src3 = __riscv_vle64_v_f64m2 (base + 300*i, vl);\n+      vfloat64m2_t src4 = __riscv_vle64_v_f64m2 (base + 400*i, vl);\n+      vfloat64m2_t src5 = __riscv_vle64_v_f64m2 (base + 500*i, vl);\n+      vfloat64m2_t src6 = __riscv_vle64_v_f64m2 (base + 600*i, vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src1,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src2,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src3,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src4,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src5,vl);\n+      v = __riscv_vfncvt_rod_f_f_w_f32m1_tu(v,src6,vl);\n+      __riscv_vse32_v_f32m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f18 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m2_t src = __riscv_vle64_v_f64m2 (base, vl);\n+    /* Only allow load v30,v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+\n+    vfloat32m1_t v = __riscv_vfncvt_rod_f_f_w_f32m1(src,vl);\n+    /* Only allow vncvt SRC == DEST v30.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v31\");\n+\n+    __riscv_vse32_v_f32m1 (out,v,vl);\n+}\n+\n+void f19 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m1_t src = __riscv_vle64_v_f64m1 (base, vl);\n+    /* Only allow load v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+\n+    vfloat32mf2_t v = __riscv_vfncvt_f_f_w_f32mf2(src,vl);\n+    /* Only allow vncvt SRC == DEST v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+\n+    __riscv_vse32_v_f32mf2 (out,v,vl);\n+}\n+\n+void f20 (void *base,void *out,size_t vl)\n+{\n+    vfloat64m2_t src = __riscv_vle64_v_f64m2 (base, vl);\n+    /* Only allow load v30,v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+\n+    vfloat32m1_t v = __riscv_vfncvt_rod_f_f_w_f32m1(src,vl);\n+    /* Only allow v29.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+    v = __riscv_vfadd_vv_f32m1 (v,v,vl);\n+    /* Only allow v29.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+\n+    __riscv_vse32_v_f32m1 (out,v,vl);\n+}\n+\n+/* { dg-final { scan-assembler-not {vmv} } } */\n+/* { dg-final { scan-assembler-not {csrr} } } */"}, {"sha": "28ea6217ce7442011944978d0333f2386665289e", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/narrow_constraint-2.c", "status": "added", "additions": 370, "deletions": 0, "changes": 370, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-2.c?ref=7ff57009bcc728044ba2de339ecd16721d48aba3", "patch": "@@ -0,0 +1,370 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+void f0 (uint16_t *base,uint8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+}\n+\n+void f1 (uint16_t *base,uint8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+}\n+\n+void f2 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+100,src,vl);\n+}\n+\n+void f3 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+100,src,vl);\n+}\n+\n+void f4 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vbool64_t m = __riscv_vlm_v_b64 (base + 500, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnsrl_wv_u8mf8_tumu(m,v,src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+100,src,vl);\n+}\n+\n+void f5 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vbool64_t m = __riscv_vlm_v_b64 (base + 500, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnsrl_wv_u8mf8_m(m,src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+100,src,vl);\n+}\n+\n+void f6 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vbool64_t m = __riscv_vlm_v_b64 (base + 500, vl);\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v2 = __riscv_vnsrl_wv_u8mf8_m(m,src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    __riscv_vse8_v_u8mf8 (out+100,v,vl);\n+}\n+\n+void f7 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v2 = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    v2 = __riscv_vnsrl_wv_u8mf8 (src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    __riscv_vse8_v_u8mf8 (out+100,v,vl);\n+}\n+\n+void f8 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v2 = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    v2 = __riscv_vnsrl_wv_u8mf8 (src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    __riscv_vse8_v_u8mf8 (out+100,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+200,src,vl);\n+}\n+\n+void f9 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v2 = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    v2 = __riscv_vnsrl_wv_u8mf8_tu (v2,src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    __riscv_vse8_v_u8mf8 (out+100,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+200,src,vl);\n+}\n+\n+void f10 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+    v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+    v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+    v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+100,src,vl);\n+}\n+\n+void f11 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v2 = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    v2 = __riscv_vnsrl_wv_u8mf8_tu (v2,src,v,vl);\n+    v2 = __riscv_vnsrl_wv_u8mf8_tu (v2,src,v,vl);\n+    v2 = __riscv_vnsrl_wv_u8mf8_tu (v2,src,v,vl);\n+    v2 = __riscv_vnsrl_wv_u8mf8_tu (v2,src,v,vl);\n+    v2 = __riscv_vnsrl_wv_u8mf8_tu (v2,src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    __riscv_vse8_v_u8mf8 (out+100,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+200,src,vl);\n+}\n+\n+void f12 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v2 = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    v2 = __riscv_vnsrl_wv_u8mf8(src,v2,vl);\n+    v2 = __riscv_vnsrl_wv_u8mf8(src,v2,vl);\n+    v2 = __riscv_vnsrl_wv_u8mf8(src,v2,vl);\n+    v2 = __riscv_vnsrl_wv_u8mf8 (src,v2,vl);\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    __riscv_vse8_v_u8mf8 (out+100,v,vl);\n+}\n+\n+void f13 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8_m(m,src,vl,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vle8_v_u8mf8_tu (v, base2, vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f14 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+      vuint8mf8_t v2 = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+      v = __riscv_vle8_v_u8mf8_tu (v, base2, vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v2,vl);\n+    }\n+}\n+\n+void f15 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+      vuint8mf8_t v2 = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+      v = __riscv_vle8_v_u8mf8_tu (v, base2, vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v2,vl);\n+    }\n+}\n+\n+void f16 (uint16_t *base,uint8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vncvt_x_x_w_u8mf8(src,vl);\n+    vuint8mf8_t v3 = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse8_v_u8mf8 (out + 100,v3,vl);\n+}\n+\n+void f17 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      vuint8mf8_t src2 = __riscv_vle8_v_u8mf8 (base + 200*i, vl);\n+      vuint8mf8_t v = __riscv_vnsrl_wv_u8mf8(src,src2,vl);\n+      vuint16mf4_t v2 = __riscv_vadd_vv_u16mf4 (src, src,vl);\n+      asm volatile (\"\":::\"memory\");\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16mf4 ((out + 200*i),src,vl);\n+      __riscv_vse8_v_u8mf8 ((out + 300*i),src2,vl);\n+    }\n+}\n+\n+void f18 (void *base,void *out,size_t vl, int n)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f19 (void *base,void *out,size_t vl, int n)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+      vuint8mf8_t v2 = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+      v2 = __riscv_vnsrl_wv_u8mf8(src,v2,vl);\n+      v2 = __riscv_vnsrl_wv_u8mf8(src,v2,vl);\n+      v2 = __riscv_vnsrl_wv_u8mf8(src,v2,vl);\n+      v2 = __riscv_vnsrl_wv_u8mf8(src,v2,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8mf8 (out + 200*i,v2,vl);\n+    }\n+}\n+\n+void f20 (void *base,void *out,size_t vl, int n)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+      vuint8mf8_t v2 = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+      v2 = __riscv_vnsrl_wv_u8mf8(src,v2,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8mf8 (out + 200*i,v2,vl);\n+    }\n+}\n+\n+void f21 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((base + 1000 * i), vl);\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src,v,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16mf4 (out + 200*i,src,vl);\n+    }\n+}\n+\n+void f22 (uint16_t *base,uint8_t *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((uint8_t *)(base + 1000 * i), vl);\n+      vuint16mf4_t src1 = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      vuint16mf4_t src2 = __riscv_vle16_v_u16mf4 (base + 200*i, vl);\n+      vuint16mf4_t src3 = __riscv_vle16_v_u16mf4 (base + 300*i, vl);\n+      vuint16mf4_t src4 = __riscv_vle16_v_u16mf4 (base + 400*i, vl);\n+      vuint16mf4_t src5 = __riscv_vle16_v_u16mf4 (base + 500*i, vl);\n+      vuint16mf4_t src6 = __riscv_vle16_v_u16mf4 (base + 600*i, vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src1,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src2,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src3,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src4,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src5,v,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src6,v,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f23 (uint16_t *base,uint8_t *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((uint8_t *)(base + 1000 * i), vl);\n+      vuint8mf8_t v2 = __riscv_vle8_v_u8mf8 ((uint8_t *)(base + 2000 * i), vl);\n+      vuint16mf4_t src1 = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      vuint16mf4_t src2 = __riscv_vle16_v_u16mf4 (base + 200*i, vl);\n+      vuint16mf4_t src3 = __riscv_vle16_v_u16mf4 (base + 300*i, vl);\n+      vuint16mf4_t src4 = __riscv_vle16_v_u16mf4 (base + 400*i, vl);\n+      vuint16mf4_t src5 = __riscv_vle16_v_u16mf4 (base + 500*i, vl);\n+      vuint16mf4_t src6 = __riscv_vle16_v_u16mf4 (base + 600*i, vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src1,v2,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src2,v2,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src3,v2,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src4,v2,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src5,v2,vl);\n+      v = __riscv_vnsrl_wv_u8mf8_tu(v,src6,v2,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f24 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint32mf2_t src = __riscv_vle32_v_u32mf2 (base + 100, vl);\n+    vuint16mf4_t src2 = __riscv_vle16_v_u16mf4 (base + 200, vl);\n+    vuint8mf8_t src3 = __riscv_vle8_v_u8mf8 (base + 300, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint16mf4_t v = __riscv_vnsrl_wv_u16mf4_m(m,src,src2,vl);\n+      vuint16mf4_t v2 = __riscv_vle16_v_u16mf4_tu (v, base2 + i, vl);\n+      vuint8mf8_t v3 = __riscv_vnsrl_wv_u8mf8_m(m,v2,src3,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v3,vl);\n+    }\n+}\n+\n+void f25 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 100, vl);\n+    vuint8mf8_t v2 = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+}\n+\n+void f26 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 100, vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v2 = __riscv_vnsrl_wv_u8mf8(src,v,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+}\n+\n+/* { dg-final { scan-assembler-not {vmv} } } */\n+/* { dg-final { scan-assembler-not {csrr} } } */\n+"}, {"sha": "72bff02c518e07b9395dac490fc7da2d2adf834c", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/narrow_constraint-3.c", "status": "added", "additions": 392, "deletions": 0, "changes": 392, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-3.c?ref=7ff57009bcc728044ba2de339ecd16721d48aba3", "patch": "@@ -0,0 +1,392 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+void f0 (uint16_t *base,uint8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f1 (uint16_t *base,uint8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f2 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 (out+100,src,vl);\n+}\n+\n+void f3 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 (out+100,src,vl);\n+}\n+\n+void f4 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vbool8_t m = __riscv_vlm_v_b8 (base + 500, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnsrl_wv_u8m1_tumu(m,v,src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 (out+100,src,vl);\n+}\n+\n+void f5 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vbool8_t m = __riscv_vlm_v_b8 (base + 500, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnsrl_wv_u8m1_m(m,src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 (out+100,src,vl);\n+}\n+\n+void f6 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vbool8_t m = __riscv_vlm_v_b8 (base + 500, vl);\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v2 = __riscv_vnsrl_wv_u8m1_m(m,src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    __riscv_vse8_v_u8m1 (out+100,v,vl);\n+}\n+\n+void f7 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v2 = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    v2 = __riscv_vnsrl_wv_u8m1 (src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    __riscv_vse8_v_u8m1 (out+100,v,vl);\n+}\n+\n+void f8 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v2 = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    v2 = __riscv_vnsrl_wv_u8m1 (src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    __riscv_vse8_v_u8m1 (out+100,v,vl);\n+    __riscv_vse16_v_u16m2 (out+200,src,vl);\n+}\n+\n+void f9 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v2 = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    v2 = __riscv_vnsrl_wv_u8m1_tu (v2,src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    __riscv_vse8_v_u8m1 (out+100,v,vl);\n+    __riscv_vse16_v_u16m2 (out+200,src,vl);\n+}\n+\n+void f10 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+    v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+    v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+    v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 (out+100,src,vl);\n+}\n+\n+void f11 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v2 = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    v2 = __riscv_vnsrl_wv_u8m1_tu (v2,src,v,vl);\n+    v2 = __riscv_vnsrl_wv_u8m1_tu (v2,src,v,vl);\n+    v2 = __riscv_vnsrl_wv_u8m1_tu (v2,src,v,vl);\n+    v2 = __riscv_vnsrl_wv_u8m1_tu (v2,src,v,vl);\n+    v2 = __riscv_vnsrl_wv_u8m1_tu (v2,src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    __riscv_vse8_v_u8m1 (out+100,v,vl);\n+    __riscv_vse16_v_u16m2 (out+200,src,vl);\n+}\n+\n+void f12 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v2 = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    v2 = __riscv_vnsrl_wv_u8m1(src,v2,vl);\n+    v2 = __riscv_vnsrl_wv_u8m1(src,v2,vl);\n+    v2 = __riscv_vnsrl_wv_u8m1(src,v2,vl);\n+    v2 = __riscv_vnsrl_wv_u8m1 (src,v2,vl);\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    __riscv_vse8_v_u8m1 (out+100,v,vl);\n+}\n+\n+void f13 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint8m1_t v = __riscv_vnsrl_wx_u8m1_m(m,src,vl,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vle8_v_u8m1_tu (v, base2, vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f14 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+      vuint8m1_t v2 = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+      v = __riscv_vle8_v_u8m1_tu (v, base2, vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v2,vl);\n+    }\n+}\n+\n+void f15 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+      vuint8m1_t v2 = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+      v = __riscv_vle8_v_u8m1_tu (v, base2, vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v2,vl);\n+    }\n+}\n+\n+void f16 (uint16_t *base,uint8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vncvt_x_x_w_u8m1(src,vl);\n+    vuint8m1_t v3 = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse8_v_u8m1 (out + 100,v3,vl);\n+}\n+\n+void f17 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      vuint8m1_t src2 = __riscv_vle8_v_u8m1 (base + 200*i, vl);\n+      vuint8m1_t v = __riscv_vnsrl_wv_u8m1(src,src2,vl);\n+      vuint16m2_t v2 = __riscv_vadd_vv_u16m2 (src, src,vl);\n+      asm volatile (\"\":::\"memory\");\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16m2 ((out + 200*i),src,vl);\n+      __riscv_vse8_v_u8m1 ((out + 300*i),src2,vl);\n+    }\n+}\n+\n+void f18 (void *base,void *out,size_t vl, int n)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 ((base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f19 (void *base,void *out,size_t vl, int n)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 ((base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+      vuint8m1_t v2 = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+      v2 = __riscv_vnsrl_wv_u8m1(src,v2,vl);\n+      v2 = __riscv_vnsrl_wv_u8m1(src,v2,vl);\n+      v2 = __riscv_vnsrl_wv_u8m1(src,v2,vl);\n+      v2 = __riscv_vnsrl_wv_u8m1(src,v2,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8m1 (out + 200*i,v2,vl);\n+    }\n+}\n+\n+void f20 (void *base,void *out,size_t vl, int n)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 ((base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+      vuint8m1_t v2 = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+      v2 = __riscv_vnsrl_wv_u8m1(src,v2,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8m1 (out + 200*i,v2,vl);\n+    }\n+}\n+\n+void f21 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 ((base + 1000 * i), vl);\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src,v,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16m2 (out + 200*i,src,vl);\n+    }\n+}\n+\n+void f22 (uint16_t *base,uint8_t *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 ((uint8_t *)(base + 1000 * i), vl);\n+      vuint16m2_t src1 = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      vuint16m2_t src2 = __riscv_vle16_v_u16m2 (base + 200*i, vl);\n+      vuint16m2_t src3 = __riscv_vle16_v_u16m2 (base + 300*i, vl);\n+      vuint16m2_t src4 = __riscv_vle16_v_u16m2 (base + 400*i, vl);\n+      vuint16m2_t src5 = __riscv_vle16_v_u16m2 (base + 500*i, vl);\n+      vuint16m2_t src6 = __riscv_vle16_v_u16m2 (base + 600*i, vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src1,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src2,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src3,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src4,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src5,v,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src6,v,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f23 (uint16_t *base,uint8_t *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 ((uint8_t *)(base + 1000 * i), vl);\n+      vuint8m1_t v2 = __riscv_vle8_v_u8m1 ((uint8_t *)(base + 2000 * i), vl);\n+      vuint16m2_t src1 = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      vuint16m2_t src2 = __riscv_vle16_v_u16m2 (base + 200*i, vl);\n+      vuint16m2_t src3 = __riscv_vle16_v_u16m2 (base + 300*i, vl);\n+      vuint16m2_t src4 = __riscv_vle16_v_u16m2 (base + 400*i, vl);\n+      vuint16m2_t src5 = __riscv_vle16_v_u16m2 (base + 500*i, vl);\n+      vuint16m2_t src6 = __riscv_vle16_v_u16m2 (base + 600*i, vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src1,v2,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src2,v2,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src3,v2,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src4,v2,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src5,v2,vl);\n+      v = __riscv_vnsrl_wv_u8m1_tu(v,src6,v2,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f24 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint32m4_t src = __riscv_vle32_v_u32m4 (base + 100, vl);\n+    vuint16m2_t src2 = __riscv_vle16_v_u16m2 (base + 200, vl);\n+    vuint8m1_t src3 = __riscv_vle8_v_u8m1 (base + 300, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint16m2_t v = __riscv_vnsrl_wv_u16m2_m(m,src,src2,vl);\n+      vuint16m2_t v2 = __riscv_vle16_v_u16m2_tu (v, base2 + i, vl);\n+      vuint8m1_t v3 = __riscv_vnsrl_wv_u8m1_m(m,v2,src3,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v3,vl);\n+    }\n+}\n+\n+void f25 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 100, vl);\n+    vuint8m1_t v2 = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v31\");\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v31\");\n+}\n+\n+void f26 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 100, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\");\n+    vuint8m1_t v2 = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+}\n+\n+void f27 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 100, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\");\n+    vuint8m1_t v2 = __riscv_vnsrl_wv_u8m1(src,v,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v29\", \"v28\", \"v30\", \"v31\");\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v29\", \"v28\", \"v30\", \"v31\");\n+}\n+\n+/* { dg-final { scan-assembler-not {vmv} } } */\n+/* { dg-final { scan-assembler-not {csrr} } } */"}, {"sha": "28971a0aad8cf3e84b2498f952cac1f3058367f2", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/narrow_constraint-4.c", "status": "added", "additions": 319, "deletions": 0, "changes": 319, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-4.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-4.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-4.c?ref=7ff57009bcc728044ba2de339ecd16721d48aba3", "patch": "@@ -0,0 +1,319 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+void f0 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+}\n+\n+void f1 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t src2 = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 100), vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8_tu(src2,src,shift,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+}\n+\n+void f2 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    vuint16mf4_t v2 = __riscv_vadd_vv_u16mf4 (src, src,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 ((int16_t *)out,v2,vl);\n+}\n+\n+void f3 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+      vuint16mf4_t v2 = __riscv_vadd_vv_u16mf4 (src, src,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16mf4 ((int16_t *)(out + 200*i),v2,vl);\n+    }\n+}\n+\n+void f4 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+    v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+    vuint16mf4_t v2 = __riscv_vadd_vv_u16mf4 (src, src,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 ((int16_t *)out,v2,vl);\n+}\n+\n+void f5 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8_m(m,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vle8_v_u8mf8_tu (v, base2, vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f6 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f7 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t src2 = __riscv_vle8_v_u8m1 ((int8_t *)(base + 100), vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1_tu(src2,src,shift,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f8 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    vuint16m2_t v2 = __riscv_vadd_vv_u16m2 (src, src,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 ((int16_t *)out,v2,vl);\n+}\n+\n+void f9 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+      vuint16m2_t v2 = __riscv_vadd_vv_u16m2 (src, src,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16m2 ((int16_t *)(out + 200*i),v2,vl);\n+    }\n+}\n+\n+void f10 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+    v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+    vuint16m2_t v2 = __riscv_vadd_vv_u16m2 (src, src,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 ((int16_t *)out,v2,vl);\n+}\n+\n+void f11 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint8m1_t v = __riscv_vnsrl_wx_u8m1_m(m,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vle8_v_u8m1_tu (v, base2, vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f12 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f13 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 ((int8_t *)(base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f14 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,shift,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f15 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,shift,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f16 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16mf4_t src1 = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      vuint16mf4_t src2 = __riscv_vle16_v_u16mf4 (base + 200*i, vl);\n+      vuint16mf4_t src3 = __riscv_vle16_v_u16mf4 (base + 300*i, vl);\n+      vuint16mf4_t src4 = __riscv_vle16_v_u16mf4 (base + 400*i, vl);\n+      vuint16mf4_t src5 = __riscv_vle16_v_u16mf4 (base + 500*i, vl);\n+      vuint16mf4_t src6 = __riscv_vle16_v_u16mf4 (base + 600*i, vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src1,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src2,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src3,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src4,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src5,shift,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src6,shift,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f17 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16m2_t src1 = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      vuint16m2_t src2 = __riscv_vle16_v_u16m2 (base + 200*i, vl);\n+      vuint16m2_t src3 = __riscv_vle16_v_u16m2 (base + 300*i, vl);\n+      vuint16m2_t src4 = __riscv_vle16_v_u16m2 (base + 400*i, vl);\n+      vuint16m2_t src5 = __riscv_vle16_v_u16m2 (base + 500*i, vl);\n+      vuint16m2_t src6 = __riscv_vle16_v_u16m2 (base + 600*i, vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src1,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src2,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src3,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src4,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src5,shift,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src6,shift,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f18 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint32mf2_t src = __riscv_vle32_v_u32mf2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint16mf4_t v = __riscv_vnsrl_wx_u16mf4_m(m,src,shift,vl);\n+      vuint16mf4_t v2 = __riscv_vle16_v_u16mf4_tu (v, base2 + i, vl);\n+      vuint8mf8_t v3 = __riscv_vnsrl_wx_u8mf8_m(m,v2,shift,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v3,vl);\n+    }\n+}\n+\n+void f19 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint32m4_t src = __riscv_vle32_v_u32m4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint16m2_t v = __riscv_vnsrl_wx_u16m2_m(m,src,shift,vl);\n+      vuint16m2_t v2 = __riscv_vle16_v_u16m2_tu (v, base2 + i, vl);\n+      vuint8m1_t v3 = __riscv_vnsrl_wx_u8m1_m(m,v2,shift,vl);\n+      vuint8m1_t v4 = __riscv_vnsrl_wx_u8m1_tumu(m,v3,v2,shift,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v3,vl);\n+      __riscv_vse8_v_u8m1 (out + 222*i,v4,vl);\n+    }\n+}\n+\n+void f20 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    /* Only allow load v30,v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    /* Only allow vncvt SRC == DEST v30.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v31\");\n+\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f21 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m1_t src = __riscv_vle16_v_u16m1 (base, vl);\n+    /* Only allow load v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+\n+    vuint8mf2_t v = __riscv_vnsrl_wx_u8mf2(src,shift,vl);\n+    /* Only allow vncvt SRC == DEST v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+\n+    __riscv_vse8_v_u8mf2 (out,v,vl);\n+}\n+\n+void f22 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    /* Only allow load v30,v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,shift,vl);\n+    /* Only allow v29.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+    v = __riscv_vadd_vv_u8m1 (v,v,vl);\n+    /* Only allow v29.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+/* { dg-final { scan-assembler-not {vmv} } } */\n+/* { dg-final { scan-assembler-not {csrr} } } */"}, {"sha": "26675bcc87c79977dd9b303b4051f21ab4af4796", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/narrow_constraint-5.c", "status": "added", "additions": 319, "deletions": 0, "changes": 319, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-5.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-5.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-5.c?ref=7ff57009bcc728044ba2de339ecd16721d48aba3", "patch": "@@ -0,0 +1,319 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+void f0 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,31,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+}\n+\n+void f1 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t src2 = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 100), vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8_tu(src2,src,31,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+}\n+\n+void f2 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,31,vl);\n+    vuint16mf4_t v2 = __riscv_vadd_vv_u16mf4 (src, src,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 ((int16_t *)out,v2,vl);\n+}\n+\n+void f3 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,31,vl);\n+      vuint16mf4_t v2 = __riscv_vadd_vv_u16mf4 (src, src,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16mf4 ((int16_t *)(out + 200*i),v2,vl);\n+    }\n+}\n+\n+void f4 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8(src,31,vl);\n+    v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+    v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+    vuint16mf4_t v2 = __riscv_vadd_vv_u16mf4 (src, src,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 ((int16_t *)out,v2,vl);\n+}\n+\n+void f5 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint8mf8_t v = __riscv_vnsrl_wx_u8mf8_m(m,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vle8_v_u8mf8_tu (v, base2, vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f6 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,31,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f7 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t src2 = __riscv_vle8_v_u8m1 ((int8_t *)(base + 100), vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1_tu(src2,src,31,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f8 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,31,vl);\n+    vuint16m2_t v2 = __riscv_vadd_vv_u16m2 (src, src,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 ((int16_t *)out,v2,vl);\n+}\n+\n+void f9 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,31,vl);\n+      vuint16m2_t v2 = __riscv_vadd_vv_u16m2 (src, src,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16m2 ((int16_t *)(out + 200*i),v2,vl);\n+    }\n+}\n+\n+void f10 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,31,vl);\n+    v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+    v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+    vuint16m2_t v2 = __riscv_vadd_vv_u16m2 (src, src,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 ((int16_t *)out,v2,vl);\n+}\n+\n+void f11 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint8m1_t v = __riscv_vnsrl_wx_u8m1_m(m,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vle8_v_u8m1_tu (v, base2, vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f12 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f13 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 ((int8_t *)(base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f14 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src,31,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f15 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src,31,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f16 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16mf4_t src1 = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      vuint16mf4_t src2 = __riscv_vle16_v_u16mf4 (base + 200*i, vl);\n+      vuint16mf4_t src3 = __riscv_vle16_v_u16mf4 (base + 300*i, vl);\n+      vuint16mf4_t src4 = __riscv_vle16_v_u16mf4 (base + 400*i, vl);\n+      vuint16mf4_t src5 = __riscv_vle16_v_u16mf4 (base + 500*i, vl);\n+      vuint16mf4_t src6 = __riscv_vle16_v_u16mf4 (base + 600*i, vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src1,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src2,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src3,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src4,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src5,31,vl);\n+      v = __riscv_vnsrl_wx_u8mf8_tu(v,src6,31,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f17 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16m2_t src1 = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      vuint16m2_t src2 = __riscv_vle16_v_u16m2 (base + 200*i, vl);\n+      vuint16m2_t src3 = __riscv_vle16_v_u16m2 (base + 300*i, vl);\n+      vuint16m2_t src4 = __riscv_vle16_v_u16m2 (base + 400*i, vl);\n+      vuint16m2_t src5 = __riscv_vle16_v_u16m2 (base + 500*i, vl);\n+      vuint16m2_t src6 = __riscv_vle16_v_u16m2 (base + 600*i, vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src1,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src2,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src3,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src4,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src5,31,vl);\n+      v = __riscv_vnsrl_wx_u8m1_tu(v,src6,31,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f18 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint32mf2_t src = __riscv_vle32_v_u32mf2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint16mf4_t v = __riscv_vnsrl_wx_u16mf4_m(m,src,31,vl);\n+      vuint16mf4_t v2 = __riscv_vle16_v_u16mf4_tu (v, base2 + i, vl);\n+      vuint8mf8_t v3 = __riscv_vnsrl_wx_u8mf8_m(m,v2,31,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v3,vl);\n+    }\n+}\n+\n+void f19 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint32m4_t src = __riscv_vle32_v_u32m4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint16m2_t v = __riscv_vnsrl_wx_u16m2_m(m,src,31,vl);\n+      vuint16m2_t v2 = __riscv_vle16_v_u16m2_tu (v, base2 + i, vl);\n+      vuint8m1_t v3 = __riscv_vnsrl_wx_u8m1_m(m,v2,31,vl);\n+      vuint8m1_t v4 = __riscv_vnsrl_wx_u8m1_tumu(m,v3,v2,31,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v3,vl);\n+      __riscv_vse8_v_u8m1 (out + 222*i,v4,vl);\n+    }\n+}\n+\n+void f20 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    /* Only allow load v30,v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,31,vl);\n+    /* Only allow vncvt SRC == DEST v30.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v31\");\n+\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f21 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m1_t src = __riscv_vle16_v_u16m1 (base, vl);\n+    /* Only allow load v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+\n+    vuint8mf2_t v = __riscv_vnsrl_wx_u8mf2(src,31,vl);\n+    /* Only allow vncvt SRC == DEST v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+\n+    __riscv_vse8_v_u8mf2 (out,v,vl);\n+}\n+\n+void f22 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    /* Only allow load v30,v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+\n+    vuint8m1_t v = __riscv_vnsrl_wx_u8m1(src,31,vl);\n+    /* Only allow v29.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+    v = __riscv_vadd_vv_u8m1 (v,v,vl);\n+    /* Only allow v29.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+/* { dg-final { scan-assembler-not {vmv} } } */\n+/* { dg-final { scan-assembler-not {csrr} } } */"}, {"sha": "fd7ffd3c97ba5468a6070b00189d5547499c48ef", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/narrow_constraint-6.c", "status": "added", "additions": 369, "deletions": 0, "changes": 369, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-6.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-6.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-6.c?ref=7ff57009bcc728044ba2de339ecd16721d48aba3", "patch": "@@ -0,0 +1,369 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+void f0 (uint16_t *base,uint8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+}\n+\n+void f1 (uint16_t *base,uint8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+}\n+\n+void f2 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+100,src,vl);\n+}\n+\n+void f3 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+100,src,vl);\n+}\n+\n+void f4 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vbool64_t m = __riscv_vlm_v_b64 (base + 500, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnclipu_wv_u8mf8_tumu(m,v,src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+100,src,vl);\n+}\n+\n+void f5 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vbool64_t m = __riscv_vlm_v_b64 (base + 500, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnclipu_wv_u8mf8_m(m,src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+100,src,vl);\n+}\n+\n+void f6 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vbool64_t m = __riscv_vlm_v_b64 (base + 500, vl);\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v2 = __riscv_vnclipu_wv_u8mf8_m(m,src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    __riscv_vse8_v_u8mf8 (out+100,v,vl);\n+}\n+\n+void f7 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v2 = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    v2 = __riscv_vnclipu_wv_u8mf8 (src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    __riscv_vse8_v_u8mf8 (out+100,v,vl);\n+}\n+\n+void f8 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v2 = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    v2 = __riscv_vnclipu_wv_u8mf8 (src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    __riscv_vse8_v_u8mf8 (out+100,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+200,src,vl);\n+}\n+\n+void f9 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v2 = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    v2 = __riscv_vnclipu_wv_u8mf8_tu (v2,src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    __riscv_vse8_v_u8mf8 (out+100,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+200,src,vl);\n+}\n+\n+void f10 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+    v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+    v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+    v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+100,src,vl);\n+}\n+\n+void f11 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v2 = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    v2 = __riscv_vnclipu_wv_u8mf8_tu (v2,src,v,vl);\n+    v2 = __riscv_vnclipu_wv_u8mf8_tu (v2,src,v,vl);\n+    v2 = __riscv_vnclipu_wv_u8mf8_tu (v2,src,v,vl);\n+    v2 = __riscv_vnclipu_wv_u8mf8_tu (v2,src,v,vl);\n+    v2 = __riscv_vnclipu_wv_u8mf8_tu (v2,src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    __riscv_vse8_v_u8mf8 (out+100,v,vl);\n+    __riscv_vse16_v_u16mf4 (out+200,src,vl);\n+}\n+\n+void f12 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v2 = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    v2 = __riscv_vnclipu_wv_u8mf8(src,v2,vl);\n+    v2 = __riscv_vnclipu_wv_u8mf8(src,v2,vl);\n+    v2 = __riscv_vnclipu_wv_u8mf8(src,v2,vl);\n+    v2 = __riscv_vnclipu_wv_u8mf8 (src,v2,vl);\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    __riscv_vse8_v_u8mf8 (out+100,v,vl);\n+}\n+\n+void f13 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8_m(m,src,vl,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vle8_v_u8mf8_tu (v, base2, vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f14 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+      vuint8mf8_t v2 = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+      v = __riscv_vle8_v_u8mf8_tu (v, base2, vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v2,vl);\n+    }\n+}\n+\n+void f15 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 600, vl);\n+      vuint8mf8_t v2 = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+      v = __riscv_vle8_v_u8mf8_tu (v, base2, vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v2,vl);\n+    }\n+}\n+\n+void f16 (uint16_t *base,uint8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vncvt_x_x_w_u8mf8(src,vl);\n+    vuint8mf8_t v3 = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse8_v_u8mf8 (out + 100,v3,vl);\n+}\n+\n+void f17 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      vuint8mf8_t src2 = __riscv_vle8_v_u8mf8 (base + 200*i, vl);\n+      vuint8mf8_t v = __riscv_vnclipu_wv_u8mf8(src,src2,vl);\n+      vuint16mf4_t v2 = __riscv_vadd_vv_u16mf4 (src, src,vl);\n+      asm volatile (\"\":::\"memory\");\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16mf4 ((out + 200*i),src,vl);\n+      __riscv_vse8_v_u8mf8 ((out + 300*i),src2,vl);\n+    }\n+}\n+\n+void f18 (void *base,void *out,size_t vl, int n)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f19 (void *base,void *out,size_t vl, int n)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+      vuint8mf8_t v2 = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+      v2 = __riscv_vnclipu_wv_u8mf8(src,v2,vl);\n+      v2 = __riscv_vnclipu_wv_u8mf8(src,v2,vl);\n+      v2 = __riscv_vnclipu_wv_u8mf8(src,v2,vl);\n+      v2 = __riscv_vnclipu_wv_u8mf8(src,v2,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8mf8 (out + 200*i,v2,vl);\n+    }\n+}\n+\n+void f20 (void *base,void *out,size_t vl, int n)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+      vuint8mf8_t v2 = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+      v2 = __riscv_vnclipu_wv_u8mf8(src,v2,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8mf8 (out + 200*i,v2,vl);\n+    }\n+}\n+\n+void f21 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((base + 1000 * i), vl);\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src,v,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16mf4 (out + 200*i,src,vl);\n+    }\n+}\n+\n+void f22 (uint16_t *base,uint8_t *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((uint8_t *)(base + 1000 * i), vl);\n+      vuint16mf4_t src1 = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      vuint16mf4_t src2 = __riscv_vle16_v_u16mf4 (base + 200*i, vl);\n+      vuint16mf4_t src3 = __riscv_vle16_v_u16mf4 (base + 300*i, vl);\n+      vuint16mf4_t src4 = __riscv_vle16_v_u16mf4 (base + 400*i, vl);\n+      vuint16mf4_t src5 = __riscv_vle16_v_u16mf4 (base + 500*i, vl);\n+      vuint16mf4_t src6 = __riscv_vle16_v_u16mf4 (base + 600*i, vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src1,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src2,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src3,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src4,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src5,v,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src6,v,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f23 (uint16_t *base,uint8_t *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((uint8_t *)(base + 1000 * i), vl);\n+      vuint8mf8_t v2 = __riscv_vle8_v_u8mf8 ((uint8_t *)(base + 2000 * i), vl);\n+      vuint16mf4_t src1 = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      vuint16mf4_t src2 = __riscv_vle16_v_u16mf4 (base + 200*i, vl);\n+      vuint16mf4_t src3 = __riscv_vle16_v_u16mf4 (base + 300*i, vl);\n+      vuint16mf4_t src4 = __riscv_vle16_v_u16mf4 (base + 400*i, vl);\n+      vuint16mf4_t src5 = __riscv_vle16_v_u16mf4 (base + 500*i, vl);\n+      vuint16mf4_t src6 = __riscv_vle16_v_u16mf4 (base + 600*i, vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src1,v2,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src2,v2,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src3,v2,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src4,v2,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src5,v2,vl);\n+      v = __riscv_vnclipu_wv_u8mf8_tu(v,src6,v2,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f24 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint32mf2_t src = __riscv_vle32_v_u32mf2 (base + 100, vl);\n+    vuint16mf4_t src2 = __riscv_vle16_v_u16mf4 (base + 200, vl);\n+    vuint8mf8_t src3 = __riscv_vle8_v_u8mf8 (base + 300, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint16mf4_t v = __riscv_vnclipu_wv_u16mf4_m(m,src,src2,vl);\n+      vuint16mf4_t v2 = __riscv_vle16_v_u16mf4_tu (v, base2 + i, vl);\n+      vuint8mf8_t v3 = __riscv_vnclipu_wv_u8mf8_m(m,v2,src3,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v3,vl);\n+    }\n+}\n+\n+void f25 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 100, vl);\n+    vuint8mf8_t v2 = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+}\n+\n+void f26 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 (base + 100, vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v2 = __riscv_vnclipu_wv_u8mf8(src,v,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+    __riscv_vse8_v_u8mf8 (out,v2,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+}\n+\n+/* { dg-final { scan-assembler-not {vmv} } } */\n+/* { dg-final { scan-assembler-not {csrr} } } */"}, {"sha": "70ba7d7459e5619777024bff22086a64ec53d4c8", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/narrow_constraint-7.c", "status": "added", "additions": 392, "deletions": 0, "changes": 392, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-7.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-7.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-7.c?ref=7ff57009bcc728044ba2de339ecd16721d48aba3", "patch": "@@ -0,0 +1,392 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3 -fno-schedule-insns -fno-schedule-insns2\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+void f0 (uint16_t *base,uint8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f1 (uint16_t *base,uint8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f2 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 (out+100,src,vl);\n+}\n+\n+void f3 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 (out+100,src,vl);\n+}\n+\n+void f4 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vbool8_t m = __riscv_vlm_v_b8 (base + 500, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnclipu_wv_u8m1_tumu(m,v,src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 (out+100,src,vl);\n+}\n+\n+void f5 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vbool8_t m = __riscv_vlm_v_b8 (base + 500, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnclipu_wv_u8m1_m(m,src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 (out+100,src,vl);\n+}\n+\n+void f6 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vbool8_t m = __riscv_vlm_v_b8 (base + 500, vl);\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v2 = __riscv_vnclipu_wv_u8m1_m(m,src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    __riscv_vse8_v_u8m1 (out+100,v,vl);\n+}\n+\n+void f7 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v2 = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    v2 = __riscv_vnclipu_wv_u8m1 (src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    __riscv_vse8_v_u8m1 (out+100,v,vl);\n+}\n+\n+void f8 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v2 = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    v2 = __riscv_vnclipu_wv_u8m1 (src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    __riscv_vse8_v_u8m1 (out+100,v,vl);\n+    __riscv_vse16_v_u16m2 (out+200,src,vl);\n+}\n+\n+void f9 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v2 = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    v2 = __riscv_vnclipu_wv_u8m1_tu (v2,src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    __riscv_vse8_v_u8m1 (out+100,v,vl);\n+    __riscv_vse16_v_u16m2 (out+200,src,vl);\n+}\n+\n+void f10 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+    v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+    v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+    v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 (out+100,src,vl);\n+}\n+\n+void f11 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v2 = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    v2 = __riscv_vnclipu_wv_u8m1_tu (v2,src,v,vl);\n+    v2 = __riscv_vnclipu_wv_u8m1_tu (v2,src,v,vl);\n+    v2 = __riscv_vnclipu_wv_u8m1_tu (v2,src,v,vl);\n+    v2 = __riscv_vnclipu_wv_u8m1_tu (v2,src,v,vl);\n+    v2 = __riscv_vnclipu_wv_u8m1_tu (v2,src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    __riscv_vse8_v_u8m1 (out+100,v,vl);\n+    __riscv_vse16_v_u16m2 (out+200,src,vl);\n+}\n+\n+void f12 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v2 = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    v2 = __riscv_vnclipu_wv_u8m1(src,v2,vl);\n+    v2 = __riscv_vnclipu_wv_u8m1(src,v2,vl);\n+    v2 = __riscv_vnclipu_wv_u8m1(src,v2,vl);\n+    v2 = __riscv_vnclipu_wv_u8m1 (src,v2,vl);\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    __riscv_vse8_v_u8m1 (out+100,v,vl);\n+}\n+\n+void f13 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint8m1_t v = __riscv_vnclipu_wx_u8m1_m(m,src,vl,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vle8_v_u8m1_tu (v, base2, vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f14 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+      vuint8m1_t v2 = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+      v = __riscv_vle8_v_u8m1_tu (v, base2, vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v2,vl);\n+    }\n+}\n+\n+void f15 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 600, vl);\n+      vuint8m1_t v2 = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+      v = __riscv_vle8_v_u8m1_tu (v, base2, vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v2,vl);\n+    }\n+}\n+\n+void f16 (uint16_t *base,uint8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vncvt_x_x_w_u8m1(src,vl);\n+    vuint8m1_t v3 = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse8_v_u8m1 (out + 100,v3,vl);\n+}\n+\n+void f17 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      vuint8m1_t src2 = __riscv_vle8_v_u8m1 (base + 200*i, vl);\n+      vuint8m1_t v = __riscv_vnclipu_wv_u8m1(src,src2,vl);\n+      vuint16m2_t v2 = __riscv_vadd_vv_u16m2 (src, src,vl);\n+      asm volatile (\"\":::\"memory\");\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16m2 ((out + 200*i),src,vl);\n+      __riscv_vse8_v_u8m1 ((out + 300*i),src2,vl);\n+    }\n+}\n+\n+void f18 (void *base,void *out,size_t vl, int n)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 ((base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f19 (void *base,void *out,size_t vl, int n)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 ((base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+      vuint8m1_t v2 = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+      v2 = __riscv_vnclipu_wv_u8m1(src,v2,vl);\n+      v2 = __riscv_vnclipu_wv_u8m1(src,v2,vl);\n+      v2 = __riscv_vnclipu_wv_u8m1(src,v2,vl);\n+      v2 = __riscv_vnclipu_wv_u8m1(src,v2,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8m1 (out + 200*i,v2,vl);\n+    }\n+}\n+\n+void f20 (void *base,void *out,size_t vl, int n)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 ((base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+      vuint8m1_t v2 = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+      v2 = __riscv_vnclipu_wv_u8m1(src,v2,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse8_v_u8m1 (out + 200*i,v2,vl);\n+    }\n+}\n+\n+void f21 (void *base,void *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 ((base + 1000 * i), vl);\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src,v,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16m2 (out + 200*i,src,vl);\n+    }\n+}\n+\n+void f22 (uint16_t *base,uint8_t *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 ((uint8_t *)(base + 1000 * i), vl);\n+      vuint16m2_t src1 = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      vuint16m2_t src2 = __riscv_vle16_v_u16m2 (base + 200*i, vl);\n+      vuint16m2_t src3 = __riscv_vle16_v_u16m2 (base + 300*i, vl);\n+      vuint16m2_t src4 = __riscv_vle16_v_u16m2 (base + 400*i, vl);\n+      vuint16m2_t src5 = __riscv_vle16_v_u16m2 (base + 500*i, vl);\n+      vuint16m2_t src6 = __riscv_vle16_v_u16m2 (base + 600*i, vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src1,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src2,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src3,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src4,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src5,v,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src6,v,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f23 (uint16_t *base,uint8_t *out,size_t vl, int n)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 ((uint8_t *)(base + 1000 * i), vl);\n+      vuint8m1_t v2 = __riscv_vle8_v_u8m1 ((uint8_t *)(base + 2000 * i), vl);\n+      vuint16m2_t src1 = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      vuint16m2_t src2 = __riscv_vle16_v_u16m2 (base + 200*i, vl);\n+      vuint16m2_t src3 = __riscv_vle16_v_u16m2 (base + 300*i, vl);\n+      vuint16m2_t src4 = __riscv_vle16_v_u16m2 (base + 400*i, vl);\n+      vuint16m2_t src5 = __riscv_vle16_v_u16m2 (base + 500*i, vl);\n+      vuint16m2_t src6 = __riscv_vle16_v_u16m2 (base + 600*i, vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src1,v2,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src2,v2,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src3,v2,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src4,v2,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src5,v2,vl);\n+      v = __riscv_vnclipu_wv_u8m1_tu(v,src6,v2,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f24 (void *base,void *base2,void *out,size_t vl, int n)\n+{\n+    vuint32m4_t src = __riscv_vle32_v_u32m4 (base + 100, vl);\n+    vuint16m2_t src2 = __riscv_vle16_v_u16m2 (base + 200, vl);\n+    vuint8m1_t src3 = __riscv_vle8_v_u8m1 (base + 300, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint16m2_t v = __riscv_vnclipu_wv_u16m2_m(m,src,src2,vl);\n+      vuint16m2_t v2 = __riscv_vle16_v_u16m2_tu (v, base2 + i, vl);\n+      vuint8m1_t v3 = __riscv_vnclipu_wv_u8m1_m(m,v2,src3,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v3,vl);\n+    }\n+}\n+\n+void f25 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 100, vl);\n+    vuint8m1_t v2 = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v31\");\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v31\");\n+}\n+\n+void f26 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 100, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\");\n+    vuint8m1_t v2 = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+}\n+\n+void f27 (void *base,void *out,size_t vl, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 (base + 100, vl);\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\");\n+    vuint8m1_t v2 = __riscv_vnclipu_wv_u8m1(src,v,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v29\", \"v28\", \"v30\", \"v31\");\n+    __riscv_vse8_v_u8m1 (out,v2,vl);\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v29\", \"v28\", \"v30\", \"v31\");\n+}\n+\n+/* { dg-final { scan-assembler-not {vmv} } } */\n+/* { dg-final { scan-assembler-not {csrr} } } */"}, {"sha": "ec8a5565bd915416464e1a2576c2305f892c2d5e", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/narrow_constraint-8.c", "status": "added", "additions": 319, "deletions": 0, "changes": 319, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-8.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-8.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-8.c?ref=7ff57009bcc728044ba2de339ecd16721d48aba3", "patch": "@@ -0,0 +1,319 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+void f0 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+}\n+\n+void f1 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t src2 = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 100), vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8_tu(src2,src,shift,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+}\n+\n+void f2 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    vuint16mf4_t v2 = __riscv_vadd_vv_u16mf4 (src, src,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 ((int16_t *)out,v2,vl);\n+}\n+\n+void f3 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+      vuint16mf4_t v2 = __riscv_vadd_vv_u16mf4 (src, src,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16mf4 ((int16_t *)(out + 200*i),v2,vl);\n+    }\n+}\n+\n+void f4 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,shift,vl);\n+    v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+    v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+    vuint16mf4_t v2 = __riscv_vadd_vv_u16mf4 (src, src,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 ((int16_t *)out,v2,vl);\n+}\n+\n+void f5 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8_m(m,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vle8_v_u8mf8_tu (v, base2, vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f6 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f7 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t src2 = __riscv_vle8_v_u8m1 ((int8_t *)(base + 100), vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1_tu(src2,src,shift,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f8 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    vuint16m2_t v2 = __riscv_vadd_vv_u16m2 (src, src,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 ((int16_t *)out,v2,vl);\n+}\n+\n+void f9 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+      vuint16m2_t v2 = __riscv_vadd_vv_u16m2 (src, src,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16m2 ((int16_t *)(out + 200*i),v2,vl);\n+    }\n+}\n+\n+void f10 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+    v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+    vuint16m2_t v2 = __riscv_vadd_vv_u16m2 (src, src,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 ((int16_t *)out,v2,vl);\n+}\n+\n+void f11 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint8m1_t v = __riscv_vnclipu_wx_u8m1_m(m,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vle8_v_u8m1_tu (v, base2, vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f12 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f13 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 ((int8_t *)(base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f14 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,shift,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f15 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,shift,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f16 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16mf4_t src1 = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      vuint16mf4_t src2 = __riscv_vle16_v_u16mf4 (base + 200*i, vl);\n+      vuint16mf4_t src3 = __riscv_vle16_v_u16mf4 (base + 300*i, vl);\n+      vuint16mf4_t src4 = __riscv_vle16_v_u16mf4 (base + 400*i, vl);\n+      vuint16mf4_t src5 = __riscv_vle16_v_u16mf4 (base + 500*i, vl);\n+      vuint16mf4_t src6 = __riscv_vle16_v_u16mf4 (base + 600*i, vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src1,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src2,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src3,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src4,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src5,shift,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src6,shift,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f17 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16m2_t src1 = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      vuint16m2_t src2 = __riscv_vle16_v_u16m2 (base + 200*i, vl);\n+      vuint16m2_t src3 = __riscv_vle16_v_u16m2 (base + 300*i, vl);\n+      vuint16m2_t src4 = __riscv_vle16_v_u16m2 (base + 400*i, vl);\n+      vuint16m2_t src5 = __riscv_vle16_v_u16m2 (base + 500*i, vl);\n+      vuint16m2_t src6 = __riscv_vle16_v_u16m2 (base + 600*i, vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src1,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src2,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src3,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src4,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src5,shift,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src6,shift,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f18 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint32mf2_t src = __riscv_vle32_v_u32mf2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint16mf4_t v = __riscv_vnclipu_wx_u16mf4_m(m,src,shift,vl);\n+      vuint16mf4_t v2 = __riscv_vle16_v_u16mf4_tu (v, base2 + i, vl);\n+      vuint8mf8_t v3 = __riscv_vnclipu_wx_u8mf8_m(m,v2,shift,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v3,vl);\n+    }\n+}\n+\n+void f19 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint32m4_t src = __riscv_vle32_v_u32m4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint16m2_t v = __riscv_vnclipu_wx_u16m2_m(m,src,shift,vl);\n+      vuint16m2_t v2 = __riscv_vle16_v_u16m2_tu (v, base2 + i, vl);\n+      vuint8m1_t v3 = __riscv_vnclipu_wx_u8m1_m(m,v2,shift,vl);\n+      vuint8m1_t v4 = __riscv_vnclipu_wx_u8m1_tumu(m,v3,v2,shift,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v3,vl);\n+      __riscv_vse8_v_u8m1 (out + 222*i,v4,vl);\n+    }\n+}\n+\n+void f20 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    /* Only allow load v30,v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    /* Only allow vncvt SRC == DEST v30.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v31\");\n+\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f21 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m1_t src = __riscv_vle16_v_u16m1 (base, vl);\n+    /* Only allow load v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+\n+    vuint8mf2_t v = __riscv_vnclipu_wx_u8mf2(src,shift,vl);\n+    /* Only allow vncvt SRC == DEST v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+\n+    __riscv_vse8_v_u8mf2 (out,v,vl);\n+}\n+\n+void f22 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    /* Only allow load v30,v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,shift,vl);\n+    /* Only allow v29.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+    v = __riscv_vadd_vv_u8m1 (v,v,vl);\n+    /* Only allow v29.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+/* { dg-final { scan-assembler-not {vmv} } } */\n+/* { dg-final { scan-assembler-not {csrr} } } */"}, {"sha": "ff34749bdb9ed47b493fd5c36e8dbd91baa9e4c5", "filename": "gcc/testsuite/gcc.target/riscv/rvv/base/narrow_constraint-9.c", "status": "added", "additions": 319, "deletions": 0, "changes": 319, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-9.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7ff57009bcc728044ba2de339ecd16721d48aba3/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-9.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Friscv%2Frvv%2Fbase%2Fnarrow_constraint-9.c?ref=7ff57009bcc728044ba2de339ecd16721d48aba3", "patch": "@@ -0,0 +1,319 @@\n+/* { dg-do compile } */\n+/* { dg-options \"-march=rv32gcv -mabi=ilp32d -O3\" } */\n+\n+#include \"riscv_vector.h\"\n+\n+void f0 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,31,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+}\n+\n+void f1 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t src2 = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 100), vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8_tu(src2,src,31,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+}\n+\n+void f2 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,31,vl);\n+    vuint16mf4_t v2 = __riscv_vadd_vv_u16mf4 (src, src,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 ((int16_t *)out,v2,vl);\n+}\n+\n+void f3 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,31,vl);\n+      vuint16mf4_t v2 = __riscv_vadd_vv_u16mf4 (src, src,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16mf4 ((int16_t *)(out + 200*i),v2,vl);\n+    }\n+}\n+\n+void f4 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base, vl);\n+    vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8(src,31,vl);\n+    v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+    v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+    vuint16mf4_t v2 = __riscv_vadd_vv_u16mf4 (src, src,vl);\n+    __riscv_vse8_v_u8mf8 (out,v,vl);\n+    __riscv_vse16_v_u16mf4 ((int16_t *)out,v2,vl);\n+}\n+\n+void f5 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint8mf8_t v = __riscv_vnclipu_wx_u8mf8_m(m,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vle8_v_u8mf8_tu (v, base2, vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f6 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,31,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f7 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t src2 = __riscv_vle8_v_u8m1 ((int8_t *)(base + 100), vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1_tu(src2,src,31,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f8 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,31,vl);\n+    vuint16m2_t v2 = __riscv_vadd_vv_u16m2 (src, src,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 ((int16_t *)out,v2,vl);\n+}\n+\n+void f9 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,31,vl);\n+      vuint16m2_t v2 = __riscv_vadd_vv_u16m2 (src, src,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+      __riscv_vse16_v_u16m2 ((int16_t *)(out + 200*i),v2,vl);\n+    }\n+}\n+\n+void f10 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,31,vl);\n+    v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+    v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+    vuint16m2_t v2 = __riscv_vadd_vv_u16m2 (src, src,vl);\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+    __riscv_vse16_v_u16m2 ((int16_t *)out,v2,vl);\n+}\n+\n+void f11 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint8m1_t v = __riscv_vnclipu_wx_u8m1_m(m,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vle8_v_u8m1_tu (v, base2, vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f12 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f13 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    vuint8m1_t v = __riscv_vle8_v_u8m1 ((int8_t *)(base + 1000), vl);\n+    for (int i = 0; i < n; i++){\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f14 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16mf4_t src = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src,31,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f15 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16m2_t src = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src,31,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f16 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8mf8_t v = __riscv_vle8_v_u8mf8 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16mf4_t src1 = __riscv_vle16_v_u16mf4 (base + 100*i, vl);\n+      vuint16mf4_t src2 = __riscv_vle16_v_u16mf4 (base + 200*i, vl);\n+      vuint16mf4_t src3 = __riscv_vle16_v_u16mf4 (base + 300*i, vl);\n+      vuint16mf4_t src4 = __riscv_vle16_v_u16mf4 (base + 400*i, vl);\n+      vuint16mf4_t src5 = __riscv_vle16_v_u16mf4 (base + 500*i, vl);\n+      vuint16mf4_t src6 = __riscv_vle16_v_u16mf4 (base + 600*i, vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src1,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src2,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src3,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src4,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src5,31,vl);\n+      v = __riscv_vnclipu_wx_u8mf8_tu(v,src6,31,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f17 (int16_t *base,int8_t *out,size_t vl, int n, size_t shift)\n+{\n+    for (int i = 0; i < n; i++){\n+      vuint8m1_t v = __riscv_vle8_v_u8m1 ((int8_t *)(base + 1000 * i), vl);\n+      vuint16m2_t src1 = __riscv_vle16_v_u16m2 (base + 100*i, vl);\n+      vuint16m2_t src2 = __riscv_vle16_v_u16m2 (base + 200*i, vl);\n+      vuint16m2_t src3 = __riscv_vle16_v_u16m2 (base + 300*i, vl);\n+      vuint16m2_t src4 = __riscv_vle16_v_u16m2 (base + 400*i, vl);\n+      vuint16m2_t src5 = __riscv_vle16_v_u16m2 (base + 500*i, vl);\n+      vuint16m2_t src6 = __riscv_vle16_v_u16m2 (base + 600*i, vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src1,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src2,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src3,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src4,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src5,31,vl);\n+      v = __riscv_vnclipu_wx_u8m1_tu(v,src6,31,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v,vl);\n+    }\n+}\n+\n+void f18 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint32mf2_t src = __riscv_vle32_v_u32mf2 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool64_t m = __riscv_vlm_v_b64 (base + i, vl);\n+      vuint16mf4_t v = __riscv_vnclipu_wx_u16mf4_m(m,src,31,vl);\n+      vuint16mf4_t v2 = __riscv_vle16_v_u16mf4_tu (v, base2 + i, vl);\n+      vuint8mf8_t v3 = __riscv_vnclipu_wx_u8mf8_m(m,v2,31,vl);\n+      __riscv_vse8_v_u8mf8 (out + 100*i,v3,vl);\n+    }\n+}\n+\n+void f19 (void *base,void *base2,void *out,size_t vl, int n, size_t shift)\n+{\n+    vuint32m4_t src = __riscv_vle32_v_u32m4 (base + 100, vl);\n+    for (int i = 0; i < n; i++){\n+      vbool8_t m = __riscv_vlm_v_b8 (base + i, vl);\n+      vuint16m2_t v = __riscv_vnclipu_wx_u16m2_m(m,src,31,vl);\n+      vuint16m2_t v2 = __riscv_vle16_v_u16m2_tu (v, base2 + i, vl);\n+      vuint8m1_t v3 = __riscv_vnclipu_wx_u8m1_m(m,v2,31,vl);\n+      vuint8m1_t v4 = __riscv_vnclipu_wx_u8m1_tumu(m,v3,v2,31,vl);\n+      __riscv_vse8_v_u8m1 (out + 100*i,v3,vl);\n+      __riscv_vse8_v_u8m1 (out + 222*i,v4,vl);\n+    }\n+}\n+\n+void f20 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    /* Only allow load v30,v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,31,vl);\n+    /* Only allow vncvt SRC == DEST v30.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v31\");\n+\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+void f21 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m1_t src = __riscv_vle16_v_u16m1 (base, vl);\n+    /* Only allow load v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+\n+    vuint8mf2_t v = __riscv_vnclipu_wx_u8mf2(src,31,vl);\n+    /* Only allow vncvt SRC == DEST v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\", \"v30\");\n+\n+    __riscv_vse8_v_u8mf2 (out,v,vl);\n+}\n+\n+void f22 (int16_t *base,int8_t *out,size_t vl, size_t shift)\n+{\n+    vuint16m2_t src = __riscv_vle16_v_u16m2 (base, vl);\n+    /* Only allow load v30,v31.  */\n+    asm volatile(\"#\" ::\n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\",\n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\", \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",  \n+\t\t   \"v26\", \"v27\", \"v28\", \"v29\");\n+\n+    vuint8m1_t v = __riscv_vnclipu_wx_u8m1(src,31,vl);\n+    /* Only allow v29.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+    v = __riscv_vadd_vv_u8m1 (v,v,vl);\n+    /* Only allow v29.  */\n+    asm volatile(\"#\" ::                                                        \n+\t\t : \"v0\", \"v1\", \"v2\", \"v3\", \"v4\", \"v5\", \"v6\", \"v7\", \"v8\", \"v9\", \n+\t\t   \"v10\", \"v11\", \"v12\", \"v13\", \"v14\", \"v15\", \"v16\", \"v17\",     \n+\t\t   \"v18\", \"v19\", \"v20\", \"v21\", \"v22\", \"v23\", \"v24\", \"v25\",     \n+\t\t   \"v26\", \"v27\", \"v28\", \"v30\", \"v31\");\n+\n+    __riscv_vse8_v_u8m1 (out,v,vl);\n+}\n+\n+/* { dg-final { scan-assembler-not {vmv} } } */\n+/* { dg-final { scan-assembler-not {csrr} } } */"}]}