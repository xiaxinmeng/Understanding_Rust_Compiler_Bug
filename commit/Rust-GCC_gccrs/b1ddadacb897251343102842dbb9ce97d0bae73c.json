{"sha": "b1ddadacb897251343102842dbb9ce97d0bae73c", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YjFkZGFkYWNiODk3MjUxMzQzMTAyODQyZGJiOWNlOTdkMGJhZTczYw==", "commit": {"author": {"name": "Paul A. Clarke", "email": "pc@us.ibm.com", "date": "2018-10-26T18:38:25Z"}, "committer": {"name": "Paul Clarke", "email": "pc@gcc.gnu.org", "date": "2018-10-26T18:38:25Z"}, "message": "[rs6000] Add compatible implementations of x86 SSSE3 intrinsics\n\nThis is a follow-on to earlier commits for adding compatibility\nimplementations of x86 intrinsics for PPC64LE.  This is the first of\ntwo patches.  This patch adds the 32 x86 intrinsics from\n<tmmintrin.h> (\"SSSE3\").  (Patch 2/2 adds tests for these intrinsics,\nand briefly describes the tests performed.)\n\ngcc/ChangeLog:\n\n2018-10-26  Paul A. Clarke  <pc@us.ibm.com>\n\n\t* config/rs6000/tmmintrin.h: New file.\n\t* config.gcc (powerpc*-*-*): Add tmmintrin.h to extra_headers.\n\nFrom-SVN: r265542", "tree": {"sha": "70c7c92e47d59b9c49b521d5face6a7e65574958", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/70c7c92e47d59b9c49b521d5face6a7e65574958"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/b1ddadacb897251343102842dbb9ce97d0bae73c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b1ddadacb897251343102842dbb9ce97d0bae73c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b1ddadacb897251343102842dbb9ce97d0bae73c", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b1ddadacb897251343102842dbb9ce97d0bae73c/comments", "author": {"login": "ThinkOpenly", "id": 12301795, "node_id": "MDQ6VXNlcjEyMzAxNzk1", "avatar_url": "https://avatars.githubusercontent.com/u/12301795?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ThinkOpenly", "html_url": "https://github.com/ThinkOpenly", "followers_url": "https://api.github.com/users/ThinkOpenly/followers", "following_url": "https://api.github.com/users/ThinkOpenly/following{/other_user}", "gists_url": "https://api.github.com/users/ThinkOpenly/gists{/gist_id}", "starred_url": "https://api.github.com/users/ThinkOpenly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ThinkOpenly/subscriptions", "organizations_url": "https://api.github.com/users/ThinkOpenly/orgs", "repos_url": "https://api.github.com/users/ThinkOpenly/repos", "events_url": "https://api.github.com/users/ThinkOpenly/events{/privacy}", "received_events_url": "https://api.github.com/users/ThinkOpenly/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "827651b0740936feea4f31d718a2175be128cc42", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/827651b0740936feea4f31d718a2175be128cc42", "html_url": "https://github.com/Rust-GCC/gccrs/commit/827651b0740936feea4f31d718a2175be128cc42"}], "stats": {"total": 511, "additions": 509, "deletions": 2}, "files": [{"sha": "9e9058c4b62d6d312c4e143ac34cd50217cf0bda", "filename": "gcc/ChangeLog", "status": "modified", "additions": 6, "deletions": 1, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b1ddadacb897251343102842dbb9ce97d0bae73c/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b1ddadacb897251343102842dbb9ce97d0bae73c/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=b1ddadacb897251343102842dbb9ce97d0bae73c", "patch": "@@ -1,4 +1,9 @@\n-2018-10-25  Paul A. Clarke  <pc@us.ibm.com>\n+2018-10-26  Paul A. Clarke  <pc@us.ibm.com>\n+\n+\t* config/rs6000/tmmintrin.h: New file.\n+\t* config.gcc (powerpc*-*-*): Add tmmintrin.h to extra_headers.\n+\n+2018-10-26  Paul A. Clarke  <pc@us.ibm.com>\n \n \t* config/rs6000/mmintrin.h: Enable 32bit compilation.\n \t* config/rs6000/xmmintrin.h: Likewise."}, {"sha": "71f62a2aba2333664226ae8a72ba47bbe2c00c27", "filename": "gcc/config.gcc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b1ddadacb897251343102842dbb9ce97d0bae73c/gcc%2Fconfig.gcc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b1ddadacb897251343102842dbb9ce97d0bae73c/gcc%2Fconfig.gcc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig.gcc?ref=b1ddadacb897251343102842dbb9ce97d0bae73c", "patch": "@@ -485,7 +485,7 @@ powerpc*-*-*)\n \textra_headers=\"${extra_headers} bmi2intrin.h bmiintrin.h\"\n \textra_headers=\"${extra_headers} xmmintrin.h mm_malloc.h emmintrin.h\"\n \textra_headers=\"${extra_headers} mmintrin.h x86intrin.h\"\n-\textra_headers=\"${extra_headers} pmmintrin.h\"\n+\textra_headers=\"${extra_headers} pmmintrin.h tmmintrin.h\"\n \textra_headers=\"${extra_headers} ppu_intrinsics.h spu2vmx.h vec_types.h si2vmx.h\"\n \textra_headers=\"${extra_headers} amo.h\"\n \tcase x$with_cpu in"}, {"sha": "90af3b3de8671aaa137aa1360a5792476f8c8c90", "filename": "gcc/config/rs6000/tmmintrin.h", "status": "added", "additions": 502, "deletions": 0, "changes": 502, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b1ddadacb897251343102842dbb9ce97d0bae73c/gcc%2Fconfig%2Frs6000%2Ftmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b1ddadacb897251343102842dbb9ce97d0bae73c/gcc%2Fconfig%2Frs6000%2Ftmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Ftmmintrin.h?ref=b1ddadacb897251343102842dbb9ce97d0bae73c", "patch": "@@ -0,0 +1,502 @@\n+/* Copyright (C) 2003-2018 Free Software Foundation, Inc.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify\n+   it under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 3, or (at your option)\n+   any later version.\n+\n+   GCC is distributed in the hope that it will be useful,\n+   but WITHOUT ANY WARRANTY; without even the implied warranty of\n+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+   GNU General Public License for more details.\n+\n+   Under Section 7 of GPL version 3, you are granted additional\n+   permissions described in the GCC Runtime Library Exception, version\n+   3.1, as published by the Free Software Foundation.\n+\n+   You should have received a copy of the GNU General Public License and\n+   a copy of the GCC Runtime Library Exception along with this program;\n+   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+/* Implemented from the specification included in the Intel C++ Compiler\n+   User Guide and Reference, version 9.0.  */\n+\n+#ifndef NO_WARN_X86_INTRINSICS\n+/* This header is distributed to simplify porting x86_64 code that\n+   makes explicit use of Intel intrinsics to powerpc64le.\n+   It is the user's responsibility to determine if the results are\n+   acceptable and make additional changes as necessary.\n+   Note that much code that uses Intel intrinsics can be rewritten in\n+   standard C or GNU C extensions, which are more portable and better\n+   optimized across multiple targets.  */\n+#endif\n+\n+#ifndef TMMINTRIN_H_\n+#define TMMINTRIN_H_\n+\n+#include <altivec.h>\n+#include <assert.h>\n+\n+/* We need definitions from the SSE header files.  */\n+#include <pmmintrin.h>\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_abs_epi16 (__m128i __A)\n+{\n+  return (__m128i) vec_abs ((__v8hi) __A);\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_abs_epi32 (__m128i __A)\n+{\n+  return (__m128i) vec_abs ((__v4si) __A);\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_abs_epi8 (__m128i __A)\n+{\n+  return (__m128i) vec_abs ((__v16qi) __A);\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_abs_pi16 (__m64 __A)\n+{\n+  __v8hi __B = (__v8hi) (__v2du) { __A, __A };\n+  return (__m64) ((__v2du) vec_abs (__B))[0];\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_abs_pi32 (__m64 __A)\n+{\n+  __v4si __B = (__v4si) (__v2du) { __A, __A };\n+  return (__m64) ((__v2du) vec_abs (__B))[0];\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_abs_pi8 (__m64 __A)\n+{\n+  __v16qi __B = (__v16qi) (__v2du) { __A, __A };\n+  return (__m64) ((__v2du) vec_abs (__B))[0];\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_alignr_epi8 (__m128i __A, __m128i __B, const unsigned int __count)\n+{\n+  if (__builtin_constant_p (__count) && __count < 16)\n+    {\n+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n+      __A = (__m128i) vec_reve ((__v16qu) __A);\n+      __B = (__m128i) vec_reve ((__v16qu) __B);\n+#endif\n+      __A = (__m128i) vec_sld ((__v16qu) __B, (__v16qu) __A, __count);\n+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n+      __A = (__m128i) vec_reve ((__v16qu) __A);\n+#endif\n+      return __A;\n+    }\n+\n+  if (__count == 0)\n+    return __B;\n+\n+  if (__count >= 16)\n+    {\n+      if (__count >= 32)\n+\t{\n+\t  const __v16qu zero = { 0 };\n+\t  return (__m128i) zero;\n+\t}\n+      else\n+\t{\n+\t  const __v16qu __shift =\n+\t    vec_splats ((unsigned char) ((__count - 16) * 8));\n+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n+\t  return (__m128i) vec_sro ((__v16qu) __A, __shift);\n+#else\n+\t  return (__m128i) vec_slo ((__v16qu) __A, __shift);\n+#endif\n+\t}\n+    }\n+  else\n+    {\n+      const __v16qu __shiftA =\n+\tvec_splats ((unsigned char) ((16 - __count) * 8));\n+      const __v16qu __shiftB = vec_splats ((unsigned char) (__count * 8));\n+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n+      __A = (__m128i) vec_slo ((__v16qu) __A, __shiftA);\n+      __B = (__m128i) vec_sro ((__v16qu) __B, __shiftB);\n+#else\n+      __A = (__m128i) vec_sro ((__v16qu) __A, __shiftA);\n+      __B = (__m128i) vec_slo ((__v16qu) __B, __shiftB);\n+#endif\n+      return (__m128i) vec_or ((__v16qu) __A, (__v16qu) __B);\n+    }\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_alignr_pi8 (__m64 __A, __m64 __B, unsigned int __count)\n+{\n+  if (__count < 16)\n+    {\n+      __v2du __C = { __B, __A };\n+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n+      const __v4su __shift = { __count << 3, 0, 0, 0 };\n+      __C = (__v2du) vec_sro ((__v16qu) __C, (__v16qu) __shift);\n+#else\n+      const __v4su __shift = { 0, 0, 0, __count << 3 };\n+      __C = (__v2du) vec_slo ((__v16qu) __C, (__v16qu) __shift);\n+#endif\n+      return (__m64) __C[0];\n+    }\n+  else\n+    {\n+      const __m64 __zero = { 0 };\n+      return __zero;\n+    }\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_hadd_epi16 (__m128i __A, __m128i __B)\n+{\n+  const __v16qu __P =\n+    {  0,  1,  4,  5,  8,  9, 12, 13, 16, 17, 20, 21, 24, 25, 28, 29 };\n+  const __v16qu __Q =\n+    {  2,  3,  6,  7, 10, 11, 14, 15, 18, 19, 22, 23, 26, 27, 30, 31 };\n+  __v8hi __C = vec_perm ((__v8hi) __A, (__v8hi) __B, __P);\n+  __v8hi __D = vec_perm ((__v8hi) __A, (__v8hi) __B, __Q);\n+  return (__m128i) vec_add (__C, __D);\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_hadd_epi32 (__m128i __A, __m128i __B)\n+{\n+  const __v16qu __P =\n+    {  0,  1,  2,  3,  8,  9, 10, 11, 16, 17, 18, 19, 24, 25, 26, 27 };\n+  const __v16qu __Q =\n+    {  4,  5,  6,  7, 12, 13, 14, 15, 20, 21, 22, 23, 28, 29, 30, 31 };\n+  __v4si __C = vec_perm ((__v4si) __A, (__v4si) __B, __P);\n+  __v4si __D = vec_perm ((__v4si) __A, (__v4si) __B, __Q);\n+  return (__m128i) vec_add (__C, __D);\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_hadd_pi16 (__m64 __A, __m64 __B)\n+{\n+  __v8hi __C = (__v8hi) (__v2du) { __A, __B };\n+  const __v16qu __P =\n+    {  0,  1,  4,  5,  8,  9, 12, 13,  0,  1,  4,  5,  8,  9, 12, 13 };\n+  const __v16qu __Q =\n+    {  2,  3,  6,  7, 10, 11, 14, 15,  2,  3,  6,  7, 10, 11, 14, 15 };\n+  __v8hi __D = vec_perm (__C, __C, __Q);\n+  __C = vec_perm (__C, __C, __P);\n+  __C = vec_add (__C, __D);\n+  return (__m64) ((__v2du) __C)[1];\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_hadd_pi32 (__m64 __A, __m64 __B)\n+{\n+  __v4si __C = (__v4si) (__v2du) { __A, __B };\n+  const __v16qu __P =\n+    {  0,  1,  2,  3,  8,  9, 10, 11,  0,  1,  2,  3,  8,  9, 10, 11 };\n+  const __v16qu __Q =\n+    {  4,  5,  6,  7, 12, 13, 14, 15,  4,  5,  6,  7, 12, 13, 14, 15 };\n+  __v4si __D = vec_perm (__C, __C, __Q);\n+  __C = vec_perm (__C, __C, __P);\n+  __C = vec_add (__C, __D);\n+  return (__m64) ((__v2du) __C)[1];\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_hadds_epi16 (__m128i __A, __m128i __B)\n+{\n+  __v4si __C = { 0 }, __D = { 0 };\n+  __C = vec_sum4s ((__v8hi) __A, __C);\n+  __D = vec_sum4s ((__v8hi) __B, __D);\n+  __C = (__v4si) vec_packs (__D, __C);\n+  return (__m128i) __C;\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_hadds_pi16 (__m64 __A, __m64 __B)\n+{\n+  const __v4si __zero = { 0 };\n+  __v8hi __C = (__v8hi) (__v2du) { __A, __B };\n+  __v4si __D = vec_sum4s (__C, __zero);\n+  __C = vec_packs (__D, __D);\n+  return (__m64) ((__v2du) __C)[1];\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_hsub_epi16 (__m128i __A, __m128i __B)\n+{\n+  const __v16qu __P =\n+    {  0,  1,  4,  5,  8,  9, 12, 13, 16, 17, 20, 21, 24, 25, 28, 29 };\n+  const __v16qu __Q =\n+    {  2,  3,  6,  7, 10, 11, 14, 15, 18, 19, 22, 23, 26, 27, 30, 31 };\n+  __v8hi __C = vec_perm ((__v8hi) __A, (__v8hi) __B, __P);\n+  __v8hi __D = vec_perm ((__v8hi) __A, (__v8hi) __B, __Q);\n+  return (__m128i) vec_sub (__C, __D);\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_hsub_epi32 (__m128i __A, __m128i __B)\n+{\n+  const __v16qu __P =\n+    {  0,  1,  2,  3,  8,  9, 10, 11, 16, 17, 18, 19, 24, 25, 26, 27 };\n+  const __v16qu __Q =\n+    {  4,  5,  6,  7, 12, 13, 14, 15, 20, 21, 22, 23, 28, 29, 30, 31 };\n+  __v4si __C = vec_perm ((__v4si) __B, (__v4si) __A, __P);\n+  __v4si __D = vec_perm ((__v4si) __B, (__v4si) __A, __Q);\n+  return (__m128i) vec_sub (__C, __D);\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_hsub_pi16 (__m64 __A, __m64 __B)\n+{\n+  const __v16qu __P =\n+    {  0,  1,  4,  5,  8,  9, 12, 13,  0,  1,  4,  5,  8,  9, 12, 13 };\n+  const __v16qu __Q =\n+    {  2,  3,  6,  7, 10, 11, 14, 15,  2,  3,  6,  7, 10, 11, 14, 15 };\n+  __v8hi __C = (__v8hi) (__v2du) { __A, __B };\n+  __v8hi __D = vec_perm (__C, __C, __Q);\n+  __C = vec_perm (__C, __C, __P);\n+  __C = vec_sub (__C, __D);\n+  return (__m64) ((__v2du) __C)[1];\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_hsub_pi32 (__m64 __A, __m64 __B)\n+{\n+  const __v16qu __P =\n+    {  0,  1,  2,  3,  8,  9, 10, 11,  0,  1,  2,  3,  8,  9, 10, 11 };\n+  const __v16qu __Q =\n+    {  4,  5,  6,  7, 12, 13, 14, 15,  4,  5,  6,  7, 12, 13, 14, 15 };\n+  __v4si __C = (__v4si) (__v2du) { __A, __B };\n+  __v4si __D = vec_perm (__C, __C, __Q);\n+  __C = vec_perm (__C, __C, __P);\n+  __C = vec_sub (__C, __D);\n+  return (__m64) ((__v2du) __C)[1];\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_hsubs_epi16 (__m128i __A, __m128i __B)\n+{\n+  const __v16qu __P =\n+    {  0,  1,  4,  5,  8,  9, 12, 13, 16, 17, 20, 21, 24, 25, 28, 29 };\n+  const __v16qu __Q =\n+    {  2,  3,  6,  7, 10, 11, 14, 15, 18, 19, 22, 23, 26, 27, 30, 31 };\n+  __v8hi __C = vec_perm ((__v8hi) __A, (__v8hi) __B, __P);\n+  __v8hi __D = vec_perm ((__v8hi) __A, (__v8hi) __B, __Q);\n+  return (__m128i) vec_subs (__C, __D);\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_hsubs_pi16 (__m64 __A, __m64 __B)\n+{\n+  const __v16qu __P =\n+    {  0,  1,  4,  5,  8,  9, 12, 13,  0,  1,  4,  5,  8,  9, 12, 13 };\n+  const __v16qu __Q =\n+    {  2,  3,  6,  7, 10, 11, 14, 15,  2,  3,  6,  7, 10, 11, 14, 15 };\n+  __v8hi __C = (__v8hi) (__v2du) { __A, __B };\n+  __v8hi __D = vec_perm (__C, __C, __P);\n+  __v8hi __E = vec_perm (__C, __C, __Q);\n+  __C = vec_subs (__D, __E);\n+  return (__m64) ((__v2du) __C)[1];\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_shuffle_epi8 (__m128i __A, __m128i __B)\n+{\n+  const __v16qi __zero = { 0 };\n+  __vector __bool char __select = vec_cmplt ((__v16qi) __A, __zero);\n+  __v16qi __C = vec_perm ((__v16qi) __A, (__v16qi) __A, (__v16qu) __B);\n+  return (__m128i) vec_sel (__C, __zero, __select);\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_shuffle_pi8 (__m64 __A, __m64 __B)\n+{\n+  const __v16qi __zero = { 0 };\n+  __v16qi __C = (__v16qi) (__v2du) { __A, __A };\n+  __v16qi __D = (__v16qi) (__v2du) { __B, __B };\n+  __vector __bool char __select = vec_cmplt ((__v16qi) __C, __zero);\n+  __C = vec_perm ((__v16qi) __C, (__v16qi) __C, (__v16qu) __D);\n+  __C = vec_sel (__C, __zero, __select);\n+  return (__m64) ((__v2du) (__C))[0];\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_sign_epi8 (__m128i __A, __m128i __B)\n+{\n+  const __v16qi __zero = { 0 };\n+  __v16qi __selectneg = (__v16qi) vec_cmplt ((__v16qi) __B, __zero);\n+  __v16qi __selectpos =\n+    (__v16qi) vec_neg ((__v16qi) vec_cmpgt ((__v16qi) __B, __zero));\n+  __v16qi __conv = vec_add (__selectneg, __selectpos);\n+  return (__m128i) vec_mul ((__v16qi) __A, (__v16qi) __conv);\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_sign_epi16 (__m128i __A, __m128i __B)\n+{\n+  const __v8hi __zero = { 0 };\n+  __v8hi __selectneg = (__v8hi) vec_cmplt ((__v8hi) __B, __zero);\n+  __v8hi __selectpos =\n+    (__v8hi) vec_neg ((__v8hi) vec_cmpgt ((__v8hi) __B, __zero));\n+  __v8hi __conv = vec_add (__selectneg, __selectpos);\n+  return (__m128i) vec_mul ((__v8hi) __A, (__v8hi) __conv);\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_sign_epi32 (__m128i __A, __m128i __B)\n+{\n+  const __v4si __zero = { 0 };\n+  __v4si __selectneg = (__v4si) vec_cmplt ((__v4si) __B, __zero);\n+  __v4si __selectpos =\n+    (__v4si) vec_neg ((__v4si) vec_cmpgt ((__v4si) __B, __zero));\n+  __v4si __conv = vec_add (__selectneg, __selectpos);\n+  return (__m128i) vec_mul ((__v4si) __A, (__v4si) __conv);\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_sign_pi8 (__m64 __A, __m64 __B)\n+{\n+  const __v16qi __zero = { 0 };\n+  __v16qi __C = (__v16qi) (__v2du) { __A, __A };\n+  __v16qi __D = (__v16qi) (__v2du) { __B, __B };\n+  __C = (__v16qi) _mm_sign_epi8 ((__m128i) __C, (__m128i) __D);\n+  return (__m64) ((__v2du) (__C))[0];\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_sign_pi16 (__m64 __A, __m64 __B)\n+{\n+  const __v8hi __zero = { 0 };\n+  __v8hi __C = (__v8hi) (__v2du) { __A, __A };\n+  __v8hi __D = (__v8hi) (__v2du) { __B, __B };\n+  __C = (__v8hi) _mm_sign_epi16 ((__m128i) __C, (__m128i) __D);\n+  return (__m64) ((__v2du) (__C))[0];\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_sign_pi32 (__m64 __A, __m64 __B)\n+{\n+  const __v4si __zero = { 0 };\n+  __v4si __C = (__v4si) (__v2du) { __A, __A };\n+  __v4si __D = (__v4si) (__v2du) { __B, __B };\n+  __C = (__v4si) _mm_sign_epi32 ((__m128i) __C, (__m128i) __D);\n+  return (__m64) ((__v2du) (__C))[0];\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_maddubs_epi16 (__m128i __A, __m128i __B)\n+{\n+  __v8hi __C = vec_unpackh ((__v16qi) __A);\n+  __v8hi __D = vec_unpackl ((__v16qi) __A);\n+  __v8hi __unsigned = vec_splats ((signed short) 0x00ff);\n+  __v8hi __E = vec_and (vec_unpackh ((__v16qi) __B), __unsigned);\n+  __v8hi __F = vec_and (vec_unpackl ((__v16qi) __B), __unsigned);\n+  __C = vec_mul (__C, __E);\n+  __D = vec_mul (__D, __F);\n+  const __v16qu __odds  =\n+    {  0,  1,  4,  5,  8,  9, 12, 13, 16, 17, 20, 21, 24, 25, 28, 29 };\n+  const __v16qu __evens =\n+    {  2,  3,  6,  7, 10, 11, 14, 15, 18, 19, 22, 23, 26, 27, 30, 31 };\n+  __E = vec_perm (__C, __D, __odds);\n+  __F = vec_perm (__C, __D, __evens);\n+  return (__m128i) vec_adds (__E, __F);\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_maddubs_pi16 (__m64 __A, __m64 __B)\n+{\n+  __v8hi __C = (__v8hi) (__v2du) { __A, __A };\n+  __C = vec_unpackl ((__v16qi) __C);\n+  __v8hi __D = (__v8hi) (__v2du) { __B, __B };\n+  __D = vec_unpackl ((__v16qi) __D);\n+  const __v8hi __unsigned = vec_splats ((signed short) 0x00ff);\n+  __D = vec_and (__D, __unsigned);\n+  __D = vec_mul (__C, __D);\n+  const __v16qu __odds  =\n+    {  0,  1,  4,  5,  8,  9, 12, 13, 16, 17, 20, 21, 24, 25, 28, 29 };\n+  const __v16qu __evens =\n+    {  2,  3,  6,  7, 10, 11, 14, 15, 18, 19, 22, 23, 26, 27, 30, 31 };\n+  __C = vec_perm (__D, __D, __odds);\n+  __D = vec_perm (__D, __D, __evens);\n+  __C = vec_adds (__C, __D);\n+  return (__m64) ((__v2du) (__C))[0];\n+}\n+\n+extern __inline __m128i\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_mulhrs_epi16 (__m128i __A, __m128i __B)\n+{\n+  __v4si __C = vec_unpackh ((__v8hi) __A);\n+  __v4si __D = vec_unpackh ((__v8hi) __B);\n+  __C = vec_mul (__C, __D);\n+  __D = vec_unpackl ((__v8hi) __A);\n+  __v4si __E = vec_unpackl ((__v8hi) __B);\n+  __D = vec_mul (__D, __E);\n+  const __v4su __shift = vec_splats ((unsigned int) 14);\n+  __C = vec_sr (__C, __shift);\n+  __D = vec_sr (__D, __shift);\n+  const __v4si __ones = vec_splats ((signed int) 1);\n+  __C = vec_add (__C, __ones);\n+  __C = vec_sr (__C, (__v4su) __ones);\n+  __D = vec_add (__D, __ones);\n+  __D = vec_sr (__D, (__v4su) __ones);\n+  return (__m128i) vec_pack (__C, __D);\n+}\n+\n+extern __inline __m64\n+__attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_mulhrs_pi16 (__m64 __A, __m64 __B)\n+{\n+  __v4si __C = (__v4si) (__v2du) { __A, __A };\n+  __C = vec_unpackh ((__v8hi) __C);\n+  __v4si __D = (__v4si) (__v2du) { __B, __B };\n+  __D = vec_unpackh ((__v8hi) __D);\n+  __C = vec_mul (__C, __D);\n+  const __v4su __shift = vec_splats ((unsigned int) 14);\n+  __C = vec_sr (__C, __shift);\n+  const __v4si __ones = vec_splats ((signed int) 1);\n+  __C = vec_add (__C, __ones);\n+  __C = vec_sr (__C, (__v4su) __ones);\n+  __v8hi __E = vec_pack (__C, __D);\n+  return (__m64) ((__v2du) (__E))[0];\n+}\n+\n+#endif"}]}