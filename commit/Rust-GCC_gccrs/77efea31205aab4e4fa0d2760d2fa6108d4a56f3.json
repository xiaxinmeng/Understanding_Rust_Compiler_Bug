{"sha": "77efea31205aab4e4fa0d2760d2fa6108d4a56f3", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NzdlZmVhMzEyMDVhYWI0ZTRmYTBkMjc2MGQyZmE2MTA4ZDRhNTZmMw==", "commit": {"author": {"name": "Felix Yang", "email": "felix.yang@huawei.com", "date": "2014-10-24T10:53:08Z"}, "committer": {"name": "Fei Yang", "email": "fyang@gcc.gnu.org", "date": "2014-10-24T10:53:08Z"}, "message": "re PR target/63173 (performance problem with simd intrinsics vld2_dup_* on aarch64-none-elf)\n\n        PR target/63173\n        * config/aarch64/arm_neon.h (__LD2R_FUNC): Remove macro.\n        (__LD3R_FUNC): Ditto.\n        (__LD4R_FUNC): Ditto.\n        (vld2_dup_s8, vld2_dup_s16, vld2_dup_s32, vld2_dup_f32, vld2_dup_f64,\n         vld2_dup_u8, vld2_dup_u16, vld2_dup_u32, vld2_dup_p8, vld2_dup_p16\n         vld2_dup_s64, vld2_dup_u64, vld2q_dup_s8, vld2q_dup_p8, \n         vld2q_dup_s16, vld2q_dup_p16, vld2q_dup_s32, vld2q_dup_s64, \n         vld2q_dup_u8, vld2q_dup_u16, vld2q_dup_u32, vld2q_dup_u64 \n         vld2q_dup_f32, vld2q_dup_f64): Rewrite using builtin functions.\n        (vld3_dup_s64, vld3_dup_u64, vld3_dup_f64, vld3_dup_s8 \n         vld3_dup_p8, vld3_dup_s16, vld3_dup_p16, vld3_dup_s32 \n         vld3_dup_u8, vld3_dup_u16, vld3_dup_u32, vld3_dup_f32\n         vld3q_dup_s8, vld3q_dup_p8, vld3q_dup_s16, vld3q_dup_p16 \n         vld3q_dup_s32, vld3q_dup_s64, vld3q_dup_u8, vld3q_dup_u16 \n         vld3q_dup_u32, vld3q_dup_u64, vld3q_dup_f32, vld3q_dup_f64): Likewise.\n        (vld4_dup_s64, vld4_dup_u64, vld4_dup_f64, vld4_dup_s8 \n         vld4_dup_p8, vld4_dup_s16, vld4_dup_p16, vld4_dup_s32 \n         vld4_dup_u8, vld4_dup_u16, vld4_dup_u32, vld4_dup_f32 \n         vld4q_dup_s8, vld4q_dup_p8, vld4q_dup_s16, vld4q_dup_p16 \n         vld4q_dup_s32, vld4q_dup_s64, vld4q_dup_u8, vld4q_dup_u16 \n         vld4q_dup_u32, vld4q_dup_u64, vld4q_dup_f32, vld4q_dup_f64): Likewise.\n        * config/aarch64/aarch64.md (define_c_enum \"unspec\"): Add\n        UNSPEC_LD2_DUP, UNSPEC_LD3_DUP, UNSPEC_LD4_DUP.\n        * config/aarch64/aarch64-simd-builtins.def (ld2r, ld3r, ld4r): New\n        builtins.\n        * config/aarch64/aarch64-simd.md (aarch64_simd_ld2r<mode>): New pattern.\n        (aarch64_simd_ld3r<mode>): Likewise.\n        (aarch64_simd_ld4r<mode>): Likewise.\n        (aarch64_ld2r<mode>): New expand.\n        (aarch64_ld3r<mode>): Likewise.\n        (aarch64_ld4r<mode>): Likewise.\n\nCo-Authored-By: Jiji Jiang <jiangjiji@huawei.com>\n\nFrom-SVN: r216630", "tree": {"sha": "5eb609950b2a805788039bc39a7f1679236df936", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/5eb609950b2a805788039bc39a7f1679236df936"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/77efea31205aab4e4fa0d2760d2fa6108d4a56f3", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/77efea31205aab4e4fa0d2760d2fa6108d4a56f3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/77efea31205aab4e4fa0d2760d2fa6108d4a56f3", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/77efea31205aab4e4fa0d2760d2fa6108d4a56f3/comments", "author": null, "committer": null, "parents": [{"sha": "e7d8c7020c4e0e392c5d64af86d1135ab02d5902", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e7d8c7020c4e0e392c5d64af86d1135ab02d5902", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e7d8c7020c4e0e392c5d64af86d1135ab02d5902"}], "stats": {"total": 1098, "additions": 978, "deletions": 120}, "files": [{"sha": "738848856878f651f7439f0b46e5101fcbbf69c9", "filename": "gcc/ChangeLog", "status": "modified", "additions": 36, "deletions": 0, "changes": 36, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/77efea31205aab4e4fa0d2760d2fa6108d4a56f3/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/77efea31205aab4e4fa0d2760d2fa6108d4a56f3/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=77efea31205aab4e4fa0d2760d2fa6108d4a56f3", "patch": "@@ -1,3 +1,39 @@\n+2014-10-24  Felix Yang  <felix.yang@huawei.com>\n+\tJiji Jiang  <jiangjiji@huawei.com>\n+\n+\tPR target/63173\n+\t* config/aarch64/arm_neon.h (__LD2R_FUNC): Remove macro.\n+\t(__LD3R_FUNC): Ditto.\n+\t(__LD4R_FUNC): Ditto.\n+\t(vld2_dup_s8, vld2_dup_s16, vld2_dup_s32, vld2_dup_f32, vld2_dup_f64,\n+\t vld2_dup_u8, vld2_dup_u16, vld2_dup_u32, vld2_dup_p8, vld2_dup_p16\n+\t vld2_dup_s64, vld2_dup_u64, vld2q_dup_s8, vld2q_dup_p8, \n+\t vld2q_dup_s16, vld2q_dup_p16, vld2q_dup_s32, vld2q_dup_s64, \n+\t vld2q_dup_u8, vld2q_dup_u16, vld2q_dup_u32, vld2q_dup_u64 \n+\t vld2q_dup_f32, vld2q_dup_f64): Rewrite using builtin functions.\n+\t(vld3_dup_s64, vld3_dup_u64, vld3_dup_f64, vld3_dup_s8 \n+\t vld3_dup_p8, vld3_dup_s16, vld3_dup_p16, vld3_dup_s32 \n+\t vld3_dup_u8, vld3_dup_u16, vld3_dup_u32, vld3_dup_f32\n+\t vld3q_dup_s8, vld3q_dup_p8, vld3q_dup_s16, vld3q_dup_p16 \n+\t vld3q_dup_s32, vld3q_dup_s64, vld3q_dup_u8, vld3q_dup_u16 \n+\t vld3q_dup_u32, vld3q_dup_u64, vld3q_dup_f32, vld3q_dup_f64): Likewise.\n+\t(vld4_dup_s64, vld4_dup_u64, vld4_dup_f64, vld4_dup_s8 \n+\t vld4_dup_p8, vld4_dup_s16, vld4_dup_p16, vld4_dup_s32 \n+\t vld4_dup_u8, vld4_dup_u16, vld4_dup_u32, vld4_dup_f32 \n+\t vld4q_dup_s8, vld4q_dup_p8, vld4q_dup_s16, vld4q_dup_p16 \n+\t vld4q_dup_s32, vld4q_dup_s64, vld4q_dup_u8, vld4q_dup_u16 \n+\t vld4q_dup_u32, vld4q_dup_u64, vld4q_dup_f32, vld4q_dup_f64): Likewise.\n+\t* config/aarch64/aarch64.md (define_c_enum \"unspec\"): Add\n+\tUNSPEC_LD2_DUP, UNSPEC_LD3_DUP, UNSPEC_LD4_DUP.\n+\t* config/aarch64/aarch64-simd-builtins.def (ld2r, ld3r, ld4r): New\n+\tbuiltins.\n+\t* config/aarch64/aarch64-simd.md (aarch64_simd_ld2r<mode>): New pattern.\n+\t(aarch64_simd_ld3r<mode>): Likewise.\n+\t(aarch64_simd_ld4r<mode>): Likewise.\n+\t(aarch64_ld2r<mode>): New expand.\n+\t(aarch64_ld3r<mode>): Likewise.\n+\t(aarch64_ld4r<mode>): Likewise.\n+\n 2014-10-24  Maxim Kuvyrkov  <maxim.kuvyrkov@gmail.com>\n \n         * rtlanal.c (get_base_term): Handle SCRATCH."}, {"sha": "ace5ebe0b7b181735ef508495eed9f92fcf12b66", "filename": "gcc/config/aarch64/aarch64-simd-builtins.def", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/77efea31205aab4e4fa0d2760d2fa6108d4a56f3/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/77efea31205aab4e4fa0d2760d2fa6108d4a56f3/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def?ref=77efea31205aab4e4fa0d2760d2fa6108d4a56f3", "patch": "@@ -83,6 +83,10 @@\n   BUILTIN_VQ (LOADSTRUCT, ld2, 0)\n   BUILTIN_VQ (LOADSTRUCT, ld3, 0)\n   BUILTIN_VQ (LOADSTRUCT, ld4, 0)\n+  /* Implemented by aarch64_ld<VSTRUCT:nregs>r<VALLDIF:mode>.  */\n+  BUILTIN_VALLDIF (LOADSTRUCT, ld2r, 0)\n+  BUILTIN_VALLDIF (LOADSTRUCT, ld3r, 0)\n+  BUILTIN_VALLDIF (LOADSTRUCT, ld4r, 0)\n   /* Implemented by aarch64_st<VSTRUCT:nregs><VDC:mode>.  */\n   BUILTIN_VDC (STORESTRUCT, st2, 0)\n   BUILTIN_VDC (STORESTRUCT, st3, 0)"}, {"sha": "da576a57154e7f6a31a269ec36236941b1a6aec4", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 69, "deletions": 0, "changes": 69, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/77efea31205aab4e4fa0d2760d2fa6108d4a56f3/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/77efea31205aab4e4fa0d2760d2fa6108d4a56f3/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=77efea31205aab4e4fa0d2760d2fa6108d4a56f3", "patch": "@@ -3991,6 +3991,16 @@\n   [(set_attr \"type\" \"neon_load2_2reg<q>\")]\n )\n \n+(define_insn \"aarch64_simd_ld2r<mode>\"\n+  [(set (match_operand:OI 0 \"register_operand\" \"=w\")\n+       (unspec:OI [(match_operand:<V_TWO_ELEM> 1 \"aarch64_simd_struct_operand\" \"Utv\")\n+                   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY) ]\n+                  UNSPEC_LD2_DUP))]\n+  \"TARGET_SIMD\"\n+  \"ld2r\\\\t{%S0.<Vtype> - %T0.<Vtype>}, %1\"\n+  [(set_attr \"type\" \"neon_load2_all_lanes<q>\")]\n+)\n+\n (define_insn \"vec_store_lanesoi<mode>\"\n   [(set (match_operand:OI 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n \t(unspec:OI [(match_operand:OI 1 \"register_operand\" \"w\")\n@@ -4022,6 +4032,16 @@\n   [(set_attr \"type\" \"neon_load3_3reg<q>\")]\n )\n \n+(define_insn \"aarch64_simd_ld3r<mode>\"\n+  [(set (match_operand:CI 0 \"register_operand\" \"=w\")\n+       (unspec:CI [(match_operand:<V_THREE_ELEM> 1 \"aarch64_simd_struct_operand\" \"Utv\")\n+                   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY) ]\n+                  UNSPEC_LD3_DUP))]\n+  \"TARGET_SIMD\"\n+  \"ld3r\\\\t{%S0.<Vtype> - %U0.<Vtype>}, %1\"\n+  [(set_attr \"type\" \"neon_load3_all_lanes<q>\")]\n+)\n+\n (define_insn \"vec_store_lanesci<mode>\"\n   [(set (match_operand:CI 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n \t(unspec:CI [(match_operand:CI 1 \"register_operand\" \"w\")\n@@ -4053,6 +4073,16 @@\n   [(set_attr \"type\" \"neon_load4_4reg<q>\")]\n )\n \n+(define_insn \"aarch64_simd_ld4r<mode>\"\n+  [(set (match_operand:XI 0 \"register_operand\" \"=w\")\n+       (unspec:XI [(match_operand:<V_FOUR_ELEM> 1 \"aarch64_simd_struct_operand\" \"Utv\")\n+                   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY) ]\n+                  UNSPEC_LD4_DUP))]\n+  \"TARGET_SIMD\"\n+  \"ld4r\\\\t{%S0.<Vtype> - %V0.<Vtype>}, %1\"\n+  [(set_attr \"type\" \"neon_load4_all_lanes<q>\")]\n+)\n+\n (define_insn \"vec_store_lanesxi<mode>\"\n   [(set (match_operand:XI 0 \"aarch64_simd_struct_operand\" \"=Utv\")\n \t(unspec:XI [(match_operand:XI 1 \"register_operand\" \"w\")\n@@ -4193,6 +4223,45 @@\n   aarch64_simd_disambiguate_copy (operands, dest, src, 4);\n })\n \n+(define_expand \"aarch64_ld2r<mode>\"\n+  [(match_operand:OI 0 \"register_operand\" \"=w\")\n+   (match_operand:DI 1 \"register_operand\" \"w\")\n+   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+  \"TARGET_SIMD\"\n+{\n+  enum machine_mode mode = <V_TWO_ELEM>mode;\n+  rtx mem = gen_rtx_MEM (mode, operands[1]);\n+\n+  emit_insn (gen_aarch64_simd_ld2r<mode> (operands[0], mem));\n+  DONE;\n+})\n+\n+(define_expand \"aarch64_ld3r<mode>\"\n+  [(match_operand:CI 0 \"register_operand\" \"=w\")\n+   (match_operand:DI 1 \"register_operand\" \"w\")\n+   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+  \"TARGET_SIMD\"\n+{\n+  enum machine_mode mode = <V_THREE_ELEM>mode;\n+  rtx mem = gen_rtx_MEM (mode, operands[1]);\n+\n+  emit_insn (gen_aarch64_simd_ld3r<mode> (operands[0], mem));\n+  DONE;\n+})\n+\n+(define_expand \"aarch64_ld4r<mode>\"\n+  [(match_operand:XI 0 \"register_operand\" \"=w\")\n+   (match_operand:DI 1 \"register_operand\" \"w\")\n+   (unspec:VALLDIF [(const_int 0)] UNSPEC_VSTRUCTDUMMY)]\n+  \"TARGET_SIMD\"\n+{\n+  enum machine_mode mode = <V_FOUR_ELEM>mode;\n+  rtx mem = gen_rtx_MEM (mode, operands[1]);\n+\n+  emit_insn (gen_aarch64_simd_ld4r<mode> (operands[0],mem));\n+  DONE;\n+})\n+\n (define_insn \"aarch64_ld2<mode>_dreg\"\n   [(set (match_operand:OI 0 \"register_operand\" \"=w\")\n \t(subreg:OI"}, {"sha": "cda69791cdb53463e6ae286de22b7f81cc5b49c3", "filename": "gcc/config/aarch64/aarch64.md", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/77efea31205aab4e4fa0d2760d2fa6108d4a56f3/gcc%2Fconfig%2Faarch64%2Faarch64.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/77efea31205aab4e4fa0d2760d2fa6108d4a56f3/gcc%2Fconfig%2Faarch64%2Faarch64.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.md?ref=77efea31205aab4e4fa0d2760d2fa6108d4a56f3", "patch": "@@ -90,8 +90,11 @@\n     UNSPEC_GOTTINYPIC\n     UNSPEC_LD1\n     UNSPEC_LD2\n+    UNSPEC_LD2_DUP\n     UNSPEC_LD3\n+    UNSPEC_LD3_DUP\n     UNSPEC_LD4\n+    UNSPEC_LD4_DUP\n     UNSPEC_MB\n     UNSPEC_NOP\n     UNSPEC_PRLG_STK"}, {"sha": "24af53bd021a83cc7513ea41913acea4c73de26b", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 866, "deletions": 120, "changes": 986, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/77efea31205aab4e4fa0d2760d2fa6108d4a56f3/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/77efea31205aab4e4fa0d2760d2fa6108d4a56f3/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=77efea31205aab4e4fa0d2760d2fa6108d4a56f3", "patch": "@@ -11765,46 +11765,6 @@ __STRUCTN (poly, 8, 4)\n __STRUCTN (float, 64, 4)\n #undef __STRUCTN\n \n-#define __LD2R_FUNC(rettype, structtype, ptrtype,\t\t\t\\\n-\t\t    regsuffix, funcsuffix, Q)\t\t\t\t\\\n-  __extension__ static __inline rettype\t\t\t\t\t\\\n-  __attribute__ ((__always_inline__)) \t\t\t\t\t\\\n-  vld2 ## Q ## _dup_ ## funcsuffix (const ptrtype *ptr)\t\t\t\\\n-  {\t\t\t\t\t\t\t\t\t\\\n-    rettype result;\t\t\t\t\t\t\t\\\n-    __asm__ (\"ld2r {v16.\" #regsuffix \", v17.\" #regsuffix \"}, %1\\n\\t\"\t\\\n-\t     \"st1 {v16.\" #regsuffix \", v17.\" #regsuffix \"}, %0\\n\\t\"\t\\\n-\t     : \"=Q\"(result)\t\t\t\t\t\t\\\n-\t     : \"Q\"(*(const structtype *)ptr)\t\t\t\t\\\n-\t     : \"memory\", \"v16\", \"v17\");\t\t\t\t\t\\\n-    return result;\t\t\t\t\t\t\t\\\n-  }\n-\n-__LD2R_FUNC (float32x2x2_t, float32x2_t, float32_t, 2s, f32,)\n-__LD2R_FUNC (float64x1x2_t, float64x2_t, float64_t, 1d, f64,)\n-__LD2R_FUNC (poly8x8x2_t, poly8x2_t, poly8_t, 8b, p8,)\n-__LD2R_FUNC (poly16x4x2_t, poly16x2_t, poly16_t, 4h, p16,)\n-__LD2R_FUNC (int8x8x2_t, int8x2_t, int8_t, 8b, s8,)\n-__LD2R_FUNC (int16x4x2_t, int16x2_t, int16_t, 4h, s16,)\n-__LD2R_FUNC (int32x2x2_t, int32x2_t, int32_t, 2s, s32,)\n-__LD2R_FUNC (int64x1x2_t, int64x2_t, int64_t, 1d, s64,)\n-__LD2R_FUNC (uint8x8x2_t, uint8x2_t, uint8_t, 8b, u8,)\n-__LD2R_FUNC (uint16x4x2_t, uint16x2_t, uint16_t, 4h, u16,)\n-__LD2R_FUNC (uint32x2x2_t, uint32x2_t, uint32_t, 2s, u32,)\n-__LD2R_FUNC (uint64x1x2_t, uint64x2_t, uint64_t, 1d, u64,)\n-__LD2R_FUNC (float32x4x2_t, float32x2_t, float32_t, 4s, f32, q)\n-__LD2R_FUNC (float64x2x2_t, float64x2_t, float64_t, 2d, f64, q)\n-__LD2R_FUNC (poly8x16x2_t, poly8x2_t, poly8_t, 16b, p8, q)\n-__LD2R_FUNC (poly16x8x2_t, poly16x2_t, poly16_t, 8h, p16, q)\n-__LD2R_FUNC (int8x16x2_t, int8x2_t, int8_t, 16b, s8, q)\n-__LD2R_FUNC (int16x8x2_t, int16x2_t, int16_t, 8h, s16, q)\n-__LD2R_FUNC (int32x4x2_t, int32x2_t, int32_t, 4s, s32, q)\n-__LD2R_FUNC (int64x2x2_t, int64x2_t, int64_t, 2d, s64, q)\n-__LD2R_FUNC (uint8x16x2_t, uint8x2_t, uint8_t, 16b, u8, q)\n-__LD2R_FUNC (uint16x8x2_t, uint16x2_t, uint16_t, 8h, u16, q)\n-__LD2R_FUNC (uint32x4x2_t, uint32x2_t, uint32_t, 4s, u32, q)\n-__LD2R_FUNC (uint64x2x2_t, uint64x2_t, uint64_t, 2d, u64, q)\n-\n #define __LD2_LANE_FUNC(rettype, ptrtype, regsuffix,\t\t\t\\\n \t\t\tlnsuffix, funcsuffix, Q)\t\t\t\\\n   __extension__ static __inline rettype\t\t\t\t\t\\\n@@ -11847,46 +11807,6 @@ __LD2_LANE_FUNC (uint16x8x2_t, uint16_t, 8h, h, u16, q)\n __LD2_LANE_FUNC (uint32x4x2_t, uint32_t, 4s, s, u32, q)\n __LD2_LANE_FUNC (uint64x2x2_t, uint64_t, 2d, d, u64, q)\n \n-#define __LD3R_FUNC(rettype, structtype, ptrtype,\t\t\t\\\n-\t\t    regsuffix, funcsuffix, Q)\t\t\t\t\\\n-  __extension__ static __inline rettype\t\t\t\t\t\\\n-  __attribute__ ((__always_inline__))\t\t\t\t\t\\\n-  vld3 ## Q ## _dup_ ## funcsuffix (const ptrtype *ptr)\t\t\t\\\n-  {\t\t\t\t\t\t\t\t\t\\\n-    rettype result;\t\t\t\t\t\t\t\\\n-    __asm__ (\"ld3r {v16.\" #regsuffix \" - v18.\" #regsuffix \"}, %1\\n\\t\"\t\\\n-\t     \"st1 {v16.\" #regsuffix \" - v18.\" #regsuffix \"}, %0\\n\\t\"\t\\\n-\t     : \"=Q\"(result)\t\t\t\t\t\t\\\n-\t     : \"Q\"(*(const structtype *)ptr)\t\t\t\t\\\n-\t     : \"memory\", \"v16\", \"v17\", \"v18\");\t\t\t\t\\\n-    return result;\t\t\t\t\t\t\t\\\n-  }\n-\n-__LD3R_FUNC (float32x2x3_t, float32x3_t, float32_t, 2s, f32,)\n-__LD3R_FUNC (float64x1x3_t, float64x3_t, float64_t, 1d, f64,)\n-__LD3R_FUNC (poly8x8x3_t, poly8x3_t, poly8_t, 8b, p8,)\n-__LD3R_FUNC (poly16x4x3_t, poly16x3_t, poly16_t, 4h, p16,)\n-__LD3R_FUNC (int8x8x3_t, int8x3_t, int8_t, 8b, s8,)\n-__LD3R_FUNC (int16x4x3_t, int16x3_t, int16_t, 4h, s16,)\n-__LD3R_FUNC (int32x2x3_t, int32x3_t, int32_t, 2s, s32,)\n-__LD3R_FUNC (int64x1x3_t, int64x3_t, int64_t, 1d, s64,)\n-__LD3R_FUNC (uint8x8x3_t, uint8x3_t, uint8_t, 8b, u8,)\n-__LD3R_FUNC (uint16x4x3_t, uint16x3_t, uint16_t, 4h, u16,)\n-__LD3R_FUNC (uint32x2x3_t, uint32x3_t, uint32_t, 2s, u32,)\n-__LD3R_FUNC (uint64x1x3_t, uint64x3_t, uint64_t, 1d, u64,)\n-__LD3R_FUNC (float32x4x3_t, float32x3_t, float32_t, 4s, f32, q)\n-__LD3R_FUNC (float64x2x3_t, float64x3_t, float64_t, 2d, f64, q)\n-__LD3R_FUNC (poly8x16x3_t, poly8x3_t, poly8_t, 16b, p8, q)\n-__LD3R_FUNC (poly16x8x3_t, poly16x3_t, poly16_t, 8h, p16, q)\n-__LD3R_FUNC (int8x16x3_t, int8x3_t, int8_t, 16b, s8, q)\n-__LD3R_FUNC (int16x8x3_t, int16x3_t, int16_t, 8h, s16, q)\n-__LD3R_FUNC (int32x4x3_t, int32x3_t, int32_t, 4s, s32, q)\n-__LD3R_FUNC (int64x2x3_t, int64x3_t, int64_t, 2d, s64, q)\n-__LD3R_FUNC (uint8x16x3_t, uint8x3_t, uint8_t, 16b, u8, q)\n-__LD3R_FUNC (uint16x8x3_t, uint16x3_t, uint16_t, 8h, u16, q)\n-__LD3R_FUNC (uint32x4x3_t, uint32x3_t, uint32_t, 4s, u32, q)\n-__LD3R_FUNC (uint64x2x3_t, uint64x3_t, uint64_t, 2d, u64, q)\n-\n #define __LD3_LANE_FUNC(rettype, ptrtype, regsuffix,\t\t\t\\\n \t\t\tlnsuffix, funcsuffix, Q)\t\t\t\\\n   __extension__ static __inline rettype\t\t\t\t\t\\\n@@ -11929,46 +11849,6 @@ __LD3_LANE_FUNC (uint16x8x3_t, uint16_t, 8h, h, u16, q)\n __LD3_LANE_FUNC (uint32x4x3_t, uint32_t, 4s, s, u32, q)\n __LD3_LANE_FUNC (uint64x2x3_t, uint64_t, 2d, d, u64, q)\n \n-#define __LD4R_FUNC(rettype, structtype, ptrtype,\t\t\t\\\n-\t\t    regsuffix, funcsuffix, Q)\t\t\t\t\\\n-  __extension__ static __inline rettype\t\t\t\t\t\\\n-  __attribute__ ((__always_inline__))\t\t\t\t\t\\\n-  vld4 ## Q ## _dup_ ## funcsuffix (const ptrtype *ptr)\t\t\t\\\n-  {\t\t\t\t\t\t\t\t\t\\\n-    rettype result;\t\t\t\t\t\t\t\\\n-    __asm__ (\"ld4r {v16.\" #regsuffix \" - v19.\" #regsuffix \"}, %1\\n\\t\"\t\\\n-\t     \"st1 {v16.\" #regsuffix \" - v19.\" #regsuffix \"}, %0\\n\\t\"\t\\\n-\t     : \"=Q\"(result)\t\t\t\t\t\t\\\n-\t     : \"Q\"(*(const structtype *)ptr)\t\t\t\t\\\n-\t     : \"memory\", \"v16\", \"v17\", \"v18\", \"v19\");\t\t\t\\\n-    return result;\t\t\t\t\t\t\t\\\n-  }\n-\n-__LD4R_FUNC (float32x2x4_t, float32x4_t, float32_t, 2s, f32,)\n-__LD4R_FUNC (float64x1x4_t, float64x4_t, float64_t, 1d, f64,)\n-__LD4R_FUNC (poly8x8x4_t, poly8x4_t, poly8_t, 8b, p8,)\n-__LD4R_FUNC (poly16x4x4_t, poly16x4_t, poly16_t, 4h, p16,)\n-__LD4R_FUNC (int8x8x4_t, int8x4_t, int8_t, 8b, s8,)\n-__LD4R_FUNC (int16x4x4_t, int16x4_t, int16_t, 4h, s16,)\n-__LD4R_FUNC (int32x2x4_t, int32x4_t, int32_t, 2s, s32,)\n-__LD4R_FUNC (int64x1x4_t, int64x4_t, int64_t, 1d, s64,)\n-__LD4R_FUNC (uint8x8x4_t, uint8x4_t, uint8_t, 8b, u8,)\n-__LD4R_FUNC (uint16x4x4_t, uint16x4_t, uint16_t, 4h, u16,)\n-__LD4R_FUNC (uint32x2x4_t, uint32x4_t, uint32_t, 2s, u32,)\n-__LD4R_FUNC (uint64x1x4_t, uint64x4_t, uint64_t, 1d, u64,)\n-__LD4R_FUNC (float32x4x4_t, float32x4_t, float32_t, 4s, f32, q)\n-__LD4R_FUNC (float64x2x4_t, float64x4_t, float64_t, 2d, f64, q)\n-__LD4R_FUNC (poly8x16x4_t, poly8x4_t, poly8_t, 16b, p8, q)\n-__LD4R_FUNC (poly16x8x4_t, poly16x4_t, poly16_t, 8h, p16, q)\n-__LD4R_FUNC (int8x16x4_t, int8x4_t, int8_t, 16b, s8, q)\n-__LD4R_FUNC (int16x8x4_t, int16x4_t, int16_t, 8h, s16, q)\n-__LD4R_FUNC (int32x4x4_t, int32x4_t, int32_t, 4s, s32, q)\n-__LD4R_FUNC (int64x2x4_t, int64x4_t, int64_t, 2d, s64, q)\n-__LD4R_FUNC (uint8x16x4_t, uint8x4_t, uint8_t, 16b, u8, q)\n-__LD4R_FUNC (uint16x8x4_t, uint16x4_t, uint16_t, 8h, u16, q)\n-__LD4R_FUNC (uint32x4x4_t, uint32x4_t, uint32_t, 4s, u32, q)\n-__LD4R_FUNC (uint64x2x4_t, uint64x4_t, uint64_t, 2d, u64, q)\n-\n #define __LD4_LANE_FUNC(rettype, ptrtype, regsuffix,\t\t\t\\\n \t\t\tlnsuffix, funcsuffix, Q)\t\t\t\\\n   __extension__ static __inline rettype\t\t\t\t\t\\\n@@ -17583,6 +17463,872 @@ vld4q_f64 (const float64_t * __a)\n   return ret;\n }\n \n+/* vldn_dup */\n+\n+__extension__ static __inline int8x8x2_t __attribute__ ((__always_inline__))\n+vld2_dup_s8 (const int8_t * __a)\n+{\n+  int8x8x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv8qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (int8x8_t) __builtin_aarch64_get_dregoiv8qi (__o, 0);\n+  ret.val[1] = (int8x8_t) __builtin_aarch64_get_dregoiv8qi (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline int16x4x2_t __attribute__ ((__always_inline__))\n+vld2_dup_s16 (const int16_t * __a)\n+{\n+  int16x4x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv4hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (int16x4_t) __builtin_aarch64_get_dregoiv4hi (__o, 0);\n+  ret.val[1] = (int16x4_t) __builtin_aarch64_get_dregoiv4hi (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline int32x2x2_t __attribute__ ((__always_inline__))\n+vld2_dup_s32 (const int32_t * __a)\n+{\n+  int32x2x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv2si ((const __builtin_aarch64_simd_si *) __a);\n+  ret.val[0] = (int32x2_t) __builtin_aarch64_get_dregoiv2si (__o, 0);\n+  ret.val[1] = (int32x2_t) __builtin_aarch64_get_dregoiv2si (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline float32x2x2_t __attribute__ ((__always_inline__))\n+vld2_dup_f32 (const float32_t * __a)\n+{\n+  float32x2x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv2sf ((const __builtin_aarch64_simd_sf *) __a);\n+  ret.val[0] = (float32x2_t) __builtin_aarch64_get_dregoiv2sf (__o, 0);\n+  ret.val[1] = (float32x2_t) __builtin_aarch64_get_dregoiv2sf (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline float64x1x2_t __attribute__ ((__always_inline__))\n+vld2_dup_f64 (const float64_t * __a)\n+{\n+  float64x1x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rdf ((const __builtin_aarch64_simd_df *) __a);\n+  ret.val[0] = (float64x1_t) {__builtin_aarch64_get_dregoidf (__o, 0)};\n+  ret.val[1] = (float64x1_t) {__builtin_aarch64_get_dregoidf (__o, 1)};\n+  return ret;\n+}\n+\n+__extension__ static __inline uint8x8x2_t __attribute__ ((__always_inline__))\n+vld2_dup_u8 (const uint8_t * __a)\n+{\n+  uint8x8x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv8qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (uint8x8_t) __builtin_aarch64_get_dregoiv8qi (__o, 0);\n+  ret.val[1] = (uint8x8_t) __builtin_aarch64_get_dregoiv8qi (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint16x4x2_t __attribute__ ((__always_inline__))\n+vld2_dup_u16 (const uint16_t * __a)\n+{\n+  uint16x4x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv4hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (uint16x4_t) __builtin_aarch64_get_dregoiv4hi (__o, 0);\n+  ret.val[1] = (uint16x4_t) __builtin_aarch64_get_dregoiv4hi (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint32x2x2_t __attribute__ ((__always_inline__))\n+vld2_dup_u32 (const uint32_t * __a)\n+{\n+  uint32x2x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv2si ((const __builtin_aarch64_simd_si *) __a);\n+  ret.val[0] = (uint32x2_t) __builtin_aarch64_get_dregoiv2si (__o, 0);\n+  ret.val[1] = (uint32x2_t) __builtin_aarch64_get_dregoiv2si (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline poly8x8x2_t __attribute__ ((__always_inline__))\n+vld2_dup_p8 (const poly8_t * __a)\n+{\n+  poly8x8x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv8qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (poly8x8_t) __builtin_aarch64_get_dregoiv8qi (__o, 0);\n+  ret.val[1] = (poly8x8_t) __builtin_aarch64_get_dregoiv8qi (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline poly16x4x2_t __attribute__ ((__always_inline__))\n+vld2_dup_p16 (const poly16_t * __a)\n+{\n+  poly16x4x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv4hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (poly16x4_t) __builtin_aarch64_get_dregoiv4hi (__o, 0);\n+  ret.val[1] = (poly16x4_t) __builtin_aarch64_get_dregoiv4hi (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline int64x1x2_t __attribute__ ((__always_inline__))\n+vld2_dup_s64 (const int64_t * __a)\n+{\n+  int64x1x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rdi ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (int64x1_t) __builtin_aarch64_get_dregoidi (__o, 0);\n+  ret.val[1] = (int64x1_t) __builtin_aarch64_get_dregoidi (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint64x1x2_t __attribute__ ((__always_inline__))\n+vld2_dup_u64 (const uint64_t * __a)\n+{\n+  uint64x1x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rdi ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (uint64x1_t) __builtin_aarch64_get_dregoidi (__o, 0);\n+  ret.val[1] = (uint64x1_t) __builtin_aarch64_get_dregoidi (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline int8x16x2_t __attribute__ ((__always_inline__))\n+vld2q_dup_s8 (const int8_t * __a)\n+{\n+  int8x16x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv16qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (int8x16_t) __builtin_aarch64_get_qregoiv16qi (__o, 0);\n+  ret.val[1] = (int8x16_t) __builtin_aarch64_get_qregoiv16qi (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline poly8x16x2_t __attribute__ ((__always_inline__))\n+vld2q_dup_p8 (const poly8_t * __a)\n+{\n+  poly8x16x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv16qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (poly8x16_t) __builtin_aarch64_get_qregoiv16qi (__o, 0);\n+  ret.val[1] = (poly8x16_t) __builtin_aarch64_get_qregoiv16qi (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline int16x8x2_t __attribute__ ((__always_inline__))\n+vld2q_dup_s16 (const int16_t * __a)\n+{\n+  int16x8x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv8hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (int16x8_t) __builtin_aarch64_get_qregoiv8hi (__o, 0);\n+  ret.val[1] = (int16x8_t) __builtin_aarch64_get_qregoiv8hi (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline poly16x8x2_t __attribute__ ((__always_inline__))\n+vld2q_dup_p16 (const poly16_t * __a)\n+{\n+  poly16x8x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv8hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (poly16x8_t) __builtin_aarch64_get_qregoiv8hi (__o, 0);\n+  ret.val[1] = (poly16x8_t) __builtin_aarch64_get_qregoiv8hi (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline int32x4x2_t __attribute__ ((__always_inline__))\n+vld2q_dup_s32 (const int32_t * __a)\n+{\n+  int32x4x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv4si ((const __builtin_aarch64_simd_si *) __a);\n+  ret.val[0] = (int32x4_t) __builtin_aarch64_get_qregoiv4si (__o, 0);\n+  ret.val[1] = (int32x4_t) __builtin_aarch64_get_qregoiv4si (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline int64x2x2_t __attribute__ ((__always_inline__))\n+vld2q_dup_s64 (const int64_t * __a)\n+{\n+  int64x2x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (int64x2_t) __builtin_aarch64_get_qregoiv2di (__o, 0);\n+  ret.val[1] = (int64x2_t) __builtin_aarch64_get_qregoiv2di (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint8x16x2_t __attribute__ ((__always_inline__))\n+vld2q_dup_u8 (const uint8_t * __a)\n+{\n+  uint8x16x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv16qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (uint8x16_t) __builtin_aarch64_get_qregoiv16qi (__o, 0);\n+  ret.val[1] = (uint8x16_t) __builtin_aarch64_get_qregoiv16qi (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint16x8x2_t __attribute__ ((__always_inline__))\n+vld2q_dup_u16 (const uint16_t * __a)\n+{\n+  uint16x8x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv8hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (uint16x8_t) __builtin_aarch64_get_qregoiv8hi (__o, 0);\n+  ret.val[1] = (uint16x8_t) __builtin_aarch64_get_qregoiv8hi (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint32x4x2_t __attribute__ ((__always_inline__))\n+vld2q_dup_u32 (const uint32_t * __a)\n+{\n+  uint32x4x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv4si ((const __builtin_aarch64_simd_si *) __a);\n+  ret.val[0] = (uint32x4_t) __builtin_aarch64_get_qregoiv4si (__o, 0);\n+  ret.val[1] = (uint32x4_t) __builtin_aarch64_get_qregoiv4si (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint64x2x2_t __attribute__ ((__always_inline__))\n+vld2q_dup_u64 (const uint64_t * __a)\n+{\n+  uint64x2x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (uint64x2_t) __builtin_aarch64_get_qregoiv2di (__o, 0);\n+  ret.val[1] = (uint64x2_t) __builtin_aarch64_get_qregoiv2di (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline float32x4x2_t __attribute__ ((__always_inline__))\n+vld2q_dup_f32 (const float32_t * __a)\n+{\n+  float32x4x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv4sf ((const __builtin_aarch64_simd_sf *) __a);\n+  ret.val[0] = (float32x4_t) __builtin_aarch64_get_qregoiv4sf (__o, 0);\n+  ret.val[1] = (float32x4_t) __builtin_aarch64_get_qregoiv4sf (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline float64x2x2_t __attribute__ ((__always_inline__))\n+vld2q_dup_f64 (const float64_t * __a)\n+{\n+  float64x2x2_t ret;\n+  __builtin_aarch64_simd_oi __o;\n+  __o = __builtin_aarch64_ld2rv2df ((const __builtin_aarch64_simd_df *) __a);\n+  ret.val[0] = (float64x2_t) __builtin_aarch64_get_qregoiv2df (__o, 0);\n+  ret.val[1] = (float64x2_t) __builtin_aarch64_get_qregoiv2df (__o, 1);\n+  return ret;\n+}\n+\n+__extension__ static __inline int64x1x3_t __attribute__ ((__always_inline__))\n+vld3_dup_s64 (const int64_t * __a)\n+{\n+  int64x1x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rdi ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (int64x1_t) __builtin_aarch64_get_dregcidi (__o, 0);\n+  ret.val[1] = (int64x1_t) __builtin_aarch64_get_dregcidi (__o, 1);\n+  ret.val[2] = (int64x1_t) __builtin_aarch64_get_dregcidi (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint64x1x3_t __attribute__ ((__always_inline__))\n+vld3_dup_u64 (const uint64_t * __a)\n+{\n+  uint64x1x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rdi ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (uint64x1_t) __builtin_aarch64_get_dregcidi (__o, 0);\n+  ret.val[1] = (uint64x1_t) __builtin_aarch64_get_dregcidi (__o, 1);\n+  ret.val[2] = (uint64x1_t) __builtin_aarch64_get_dregcidi (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline float64x1x3_t __attribute__ ((__always_inline__))\n+vld3_dup_f64 (const float64_t * __a)\n+{\n+  float64x1x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rdf ((const __builtin_aarch64_simd_df *) __a);\n+  ret.val[0] = (float64x1_t) {__builtin_aarch64_get_dregcidf (__o, 0)};\n+  ret.val[1] = (float64x1_t) {__builtin_aarch64_get_dregcidf (__o, 1)};\n+  ret.val[2] = (float64x1_t) {__builtin_aarch64_get_dregcidf (__o, 2)};\n+  return ret;\n+}\n+\n+__extension__ static __inline int8x8x3_t __attribute__ ((__always_inline__))\n+vld3_dup_s8 (const int8_t * __a)\n+{\n+  int8x8x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv8qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (int8x8_t) __builtin_aarch64_get_dregciv8qi (__o, 0);\n+  ret.val[1] = (int8x8_t) __builtin_aarch64_get_dregciv8qi (__o, 1);\n+  ret.val[2] = (int8x8_t) __builtin_aarch64_get_dregciv8qi (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline poly8x8x3_t __attribute__ ((__always_inline__))\n+vld3_dup_p8 (const poly8_t * __a)\n+{\n+  poly8x8x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv8qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (poly8x8_t) __builtin_aarch64_get_dregciv8qi (__o, 0);\n+  ret.val[1] = (poly8x8_t) __builtin_aarch64_get_dregciv8qi (__o, 1);\n+  ret.val[2] = (poly8x8_t) __builtin_aarch64_get_dregciv8qi (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline int16x4x3_t __attribute__ ((__always_inline__))\n+vld3_dup_s16 (const int16_t * __a)\n+{\n+  int16x4x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv4hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (int16x4_t) __builtin_aarch64_get_dregciv4hi (__o, 0);\n+  ret.val[1] = (int16x4_t) __builtin_aarch64_get_dregciv4hi (__o, 1);\n+  ret.val[2] = (int16x4_t) __builtin_aarch64_get_dregciv4hi (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline poly16x4x3_t __attribute__ ((__always_inline__))\n+vld3_dup_p16 (const poly16_t * __a)\n+{\n+  poly16x4x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv4hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (poly16x4_t) __builtin_aarch64_get_dregciv4hi (__o, 0);\n+  ret.val[1] = (poly16x4_t) __builtin_aarch64_get_dregciv4hi (__o, 1);\n+  ret.val[2] = (poly16x4_t) __builtin_aarch64_get_dregciv4hi (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline int32x2x3_t __attribute__ ((__always_inline__))\n+vld3_dup_s32 (const int32_t * __a)\n+{\n+  int32x2x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv2si ((const __builtin_aarch64_simd_si *) __a);\n+  ret.val[0] = (int32x2_t) __builtin_aarch64_get_dregciv2si (__o, 0);\n+  ret.val[1] = (int32x2_t) __builtin_aarch64_get_dregciv2si (__o, 1);\n+  ret.val[2] = (int32x2_t) __builtin_aarch64_get_dregciv2si (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint8x8x3_t __attribute__ ((__always_inline__))\n+vld3_dup_u8 (const uint8_t * __a)\n+{\n+  uint8x8x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv8qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (uint8x8_t) __builtin_aarch64_get_dregciv8qi (__o, 0);\n+  ret.val[1] = (uint8x8_t) __builtin_aarch64_get_dregciv8qi (__o, 1);\n+  ret.val[2] = (uint8x8_t) __builtin_aarch64_get_dregciv8qi (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint16x4x3_t __attribute__ ((__always_inline__))\n+vld3_dup_u16 (const uint16_t * __a)\n+{\n+  uint16x4x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv4hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (uint16x4_t) __builtin_aarch64_get_dregciv4hi (__o, 0);\n+  ret.val[1] = (uint16x4_t) __builtin_aarch64_get_dregciv4hi (__o, 1);\n+  ret.val[2] = (uint16x4_t) __builtin_aarch64_get_dregciv4hi (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint32x2x3_t __attribute__ ((__always_inline__))\n+vld3_dup_u32 (const uint32_t * __a)\n+{\n+  uint32x2x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv2si ((const __builtin_aarch64_simd_si *) __a);\n+  ret.val[0] = (uint32x2_t) __builtin_aarch64_get_dregciv2si (__o, 0);\n+  ret.val[1] = (uint32x2_t) __builtin_aarch64_get_dregciv2si (__o, 1);\n+  ret.val[2] = (uint32x2_t) __builtin_aarch64_get_dregciv2si (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline float32x2x3_t __attribute__ ((__always_inline__))\n+vld3_dup_f32 (const float32_t * __a)\n+{\n+  float32x2x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv2sf ((const __builtin_aarch64_simd_sf *) __a);\n+  ret.val[0] = (float32x2_t) __builtin_aarch64_get_dregciv2sf (__o, 0);\n+  ret.val[1] = (float32x2_t) __builtin_aarch64_get_dregciv2sf (__o, 1);\n+  ret.val[2] = (float32x2_t) __builtin_aarch64_get_dregciv2sf (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline int8x16x3_t __attribute__ ((__always_inline__))\n+vld3q_dup_s8 (const int8_t * __a)\n+{\n+  int8x16x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv16qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (int8x16_t) __builtin_aarch64_get_qregciv16qi (__o, 0);\n+  ret.val[1] = (int8x16_t) __builtin_aarch64_get_qregciv16qi (__o, 1);\n+  ret.val[2] = (int8x16_t) __builtin_aarch64_get_qregciv16qi (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline poly8x16x3_t __attribute__ ((__always_inline__))\n+vld3q_dup_p8 (const poly8_t * __a)\n+{\n+  poly8x16x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv16qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (poly8x16_t) __builtin_aarch64_get_qregciv16qi (__o, 0);\n+  ret.val[1] = (poly8x16_t) __builtin_aarch64_get_qregciv16qi (__o, 1);\n+  ret.val[2] = (poly8x16_t) __builtin_aarch64_get_qregciv16qi (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline int16x8x3_t __attribute__ ((__always_inline__))\n+vld3q_dup_s16 (const int16_t * __a)\n+{\n+  int16x8x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv8hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (int16x8_t) __builtin_aarch64_get_qregciv8hi (__o, 0);\n+  ret.val[1] = (int16x8_t) __builtin_aarch64_get_qregciv8hi (__o, 1);\n+  ret.val[2] = (int16x8_t) __builtin_aarch64_get_qregciv8hi (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline poly16x8x3_t __attribute__ ((__always_inline__))\n+vld3q_dup_p16 (const poly16_t * __a)\n+{\n+  poly16x8x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv8hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (poly16x8_t) __builtin_aarch64_get_qregciv8hi (__o, 0);\n+  ret.val[1] = (poly16x8_t) __builtin_aarch64_get_qregciv8hi (__o, 1);\n+  ret.val[2] = (poly16x8_t) __builtin_aarch64_get_qregciv8hi (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline int32x4x3_t __attribute__ ((__always_inline__))\n+vld3q_dup_s32 (const int32_t * __a)\n+{\n+  int32x4x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv4si ((const __builtin_aarch64_simd_si *) __a);\n+  ret.val[0] = (int32x4_t) __builtin_aarch64_get_qregciv4si (__o, 0);\n+  ret.val[1] = (int32x4_t) __builtin_aarch64_get_qregciv4si (__o, 1);\n+  ret.val[2] = (int32x4_t) __builtin_aarch64_get_qregciv4si (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline int64x2x3_t __attribute__ ((__always_inline__))\n+vld3q_dup_s64 (const int64_t * __a)\n+{\n+  int64x2x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (int64x2_t) __builtin_aarch64_get_qregciv2di (__o, 0);\n+  ret.val[1] = (int64x2_t) __builtin_aarch64_get_qregciv2di (__o, 1);\n+  ret.val[2] = (int64x2_t) __builtin_aarch64_get_qregciv2di (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint8x16x3_t __attribute__ ((__always_inline__))\n+vld3q_dup_u8 (const uint8_t * __a)\n+{\n+  uint8x16x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv16qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (uint8x16_t) __builtin_aarch64_get_qregciv16qi (__o, 0);\n+  ret.val[1] = (uint8x16_t) __builtin_aarch64_get_qregciv16qi (__o, 1);\n+  ret.val[2] = (uint8x16_t) __builtin_aarch64_get_qregciv16qi (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint16x8x3_t __attribute__ ((__always_inline__))\n+vld3q_dup_u16 (const uint16_t * __a)\n+{\n+  uint16x8x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv8hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (uint16x8_t) __builtin_aarch64_get_qregciv8hi (__o, 0);\n+  ret.val[1] = (uint16x8_t) __builtin_aarch64_get_qregciv8hi (__o, 1);\n+  ret.val[2] = (uint16x8_t) __builtin_aarch64_get_qregciv8hi (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint32x4x3_t __attribute__ ((__always_inline__))\n+vld3q_dup_u32 (const uint32_t * __a)\n+{\n+  uint32x4x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv4si ((const __builtin_aarch64_simd_si *) __a);\n+  ret.val[0] = (uint32x4_t) __builtin_aarch64_get_qregciv4si (__o, 0);\n+  ret.val[1] = (uint32x4_t) __builtin_aarch64_get_qregciv4si (__o, 1);\n+  ret.val[2] = (uint32x4_t) __builtin_aarch64_get_qregciv4si (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint64x2x3_t __attribute__ ((__always_inline__))\n+vld3q_dup_u64 (const uint64_t * __a)\n+{\n+  uint64x2x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (uint64x2_t) __builtin_aarch64_get_qregciv2di (__o, 0);\n+  ret.val[1] = (uint64x2_t) __builtin_aarch64_get_qregciv2di (__o, 1);\n+  ret.val[2] = (uint64x2_t) __builtin_aarch64_get_qregciv2di (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline float32x4x3_t __attribute__ ((__always_inline__))\n+vld3q_dup_f32 (const float32_t * __a)\n+{\n+  float32x4x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv4sf ((const __builtin_aarch64_simd_sf *) __a);\n+  ret.val[0] = (float32x4_t) __builtin_aarch64_get_qregciv4sf (__o, 0);\n+  ret.val[1] = (float32x4_t) __builtin_aarch64_get_qregciv4sf (__o, 1);\n+  ret.val[2] = (float32x4_t) __builtin_aarch64_get_qregciv4sf (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline float64x2x3_t __attribute__ ((__always_inline__))\n+vld3q_dup_f64 (const float64_t * __a)\n+{\n+  float64x2x3_t ret;\n+  __builtin_aarch64_simd_ci __o;\n+  __o = __builtin_aarch64_ld3rv2df ((const __builtin_aarch64_simd_df *) __a);\n+  ret.val[0] = (float64x2_t) __builtin_aarch64_get_qregciv2df (__o, 0);\n+  ret.val[1] = (float64x2_t) __builtin_aarch64_get_qregciv2df (__o, 1);\n+  ret.val[2] = (float64x2_t) __builtin_aarch64_get_qregciv2df (__o, 2);\n+  return ret;\n+}\n+\n+__extension__ static __inline int64x1x4_t __attribute__ ((__always_inline__))\n+vld4_dup_s64 (const int64_t * __a)\n+{\n+  int64x1x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rdi ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (int64x1_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  ret.val[1] = (int64x1_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  ret.val[2] = (int64x1_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  ret.val[3] = (int64x1_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint64x1x4_t __attribute__ ((__always_inline__))\n+vld4_dup_u64 (const uint64_t * __a)\n+{\n+  uint64x1x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rdi ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (uint64x1_t) __builtin_aarch64_get_dregxidi (__o, 0);\n+  ret.val[1] = (uint64x1_t) __builtin_aarch64_get_dregxidi (__o, 1);\n+  ret.val[2] = (uint64x1_t) __builtin_aarch64_get_dregxidi (__o, 2);\n+  ret.val[3] = (uint64x1_t) __builtin_aarch64_get_dregxidi (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline float64x1x4_t __attribute__ ((__always_inline__))\n+vld4_dup_f64 (const float64_t * __a)\n+{\n+  float64x1x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rdf ((const __builtin_aarch64_simd_df *) __a);\n+  ret.val[0] = (float64x1_t) {__builtin_aarch64_get_dregxidf (__o, 0)};\n+  ret.val[1] = (float64x1_t) {__builtin_aarch64_get_dregxidf (__o, 1)};\n+  ret.val[2] = (float64x1_t) {__builtin_aarch64_get_dregxidf (__o, 2)};\n+  ret.val[3] = (float64x1_t) {__builtin_aarch64_get_dregxidf (__o, 3)};\n+  return ret;\n+}\n+\n+__extension__ static __inline int8x8x4_t __attribute__ ((__always_inline__))\n+vld4_dup_s8 (const int8_t * __a)\n+{\n+  int8x8x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv8qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (int8x8_t) __builtin_aarch64_get_dregxiv8qi (__o, 0);\n+  ret.val[1] = (int8x8_t) __builtin_aarch64_get_dregxiv8qi (__o, 1);\n+  ret.val[2] = (int8x8_t) __builtin_aarch64_get_dregxiv8qi (__o, 2);\n+  ret.val[3] = (int8x8_t) __builtin_aarch64_get_dregxiv8qi (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline poly8x8x4_t __attribute__ ((__always_inline__))\n+vld4_dup_p8 (const poly8_t * __a)\n+{\n+  poly8x8x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv8qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (poly8x8_t) __builtin_aarch64_get_dregxiv8qi (__o, 0);\n+  ret.val[1] = (poly8x8_t) __builtin_aarch64_get_dregxiv8qi (__o, 1);\n+  ret.val[2] = (poly8x8_t) __builtin_aarch64_get_dregxiv8qi (__o, 2);\n+  ret.val[3] = (poly8x8_t) __builtin_aarch64_get_dregxiv8qi (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline int16x4x4_t __attribute__ ((__always_inline__))\n+vld4_dup_s16 (const int16_t * __a)\n+{\n+  int16x4x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv4hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (int16x4_t) __builtin_aarch64_get_dregxiv4hi (__o, 0);\n+  ret.val[1] = (int16x4_t) __builtin_aarch64_get_dregxiv4hi (__o, 1);\n+  ret.val[2] = (int16x4_t) __builtin_aarch64_get_dregxiv4hi (__o, 2);\n+  ret.val[3] = (int16x4_t) __builtin_aarch64_get_dregxiv4hi (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline poly16x4x4_t __attribute__ ((__always_inline__))\n+vld4_dup_p16 (const poly16_t * __a)\n+{\n+  poly16x4x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv4hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (poly16x4_t) __builtin_aarch64_get_dregxiv4hi (__o, 0);\n+  ret.val[1] = (poly16x4_t) __builtin_aarch64_get_dregxiv4hi (__o, 1);\n+  ret.val[2] = (poly16x4_t) __builtin_aarch64_get_dregxiv4hi (__o, 2);\n+  ret.val[3] = (poly16x4_t) __builtin_aarch64_get_dregxiv4hi (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline int32x2x4_t __attribute__ ((__always_inline__))\n+vld4_dup_s32 (const int32_t * __a)\n+{\n+  int32x2x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv2si ((const __builtin_aarch64_simd_si *) __a);\n+  ret.val[0] = (int32x2_t) __builtin_aarch64_get_dregxiv2si (__o, 0);\n+  ret.val[1] = (int32x2_t) __builtin_aarch64_get_dregxiv2si (__o, 1);\n+  ret.val[2] = (int32x2_t) __builtin_aarch64_get_dregxiv2si (__o, 2);\n+  ret.val[3] = (int32x2_t) __builtin_aarch64_get_dregxiv2si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint8x8x4_t __attribute__ ((__always_inline__))\n+vld4_dup_u8 (const uint8_t * __a)\n+{\n+  uint8x8x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv8qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (uint8x8_t) __builtin_aarch64_get_dregxiv8qi (__o, 0);\n+  ret.val[1] = (uint8x8_t) __builtin_aarch64_get_dregxiv8qi (__o, 1);\n+  ret.val[2] = (uint8x8_t) __builtin_aarch64_get_dregxiv8qi (__o, 2);\n+  ret.val[3] = (uint8x8_t) __builtin_aarch64_get_dregxiv8qi (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint16x4x4_t __attribute__ ((__always_inline__))\n+vld4_dup_u16 (const uint16_t * __a)\n+{\n+  uint16x4x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv4hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (uint16x4_t) __builtin_aarch64_get_dregxiv4hi (__o, 0);\n+  ret.val[1] = (uint16x4_t) __builtin_aarch64_get_dregxiv4hi (__o, 1);\n+  ret.val[2] = (uint16x4_t) __builtin_aarch64_get_dregxiv4hi (__o, 2);\n+  ret.val[3] = (uint16x4_t) __builtin_aarch64_get_dregxiv4hi (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint32x2x4_t __attribute__ ((__always_inline__))\n+vld4_dup_u32 (const uint32_t * __a)\n+{\n+  uint32x2x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv2si ((const __builtin_aarch64_simd_si *) __a);\n+  ret.val[0] = (uint32x2_t) __builtin_aarch64_get_dregxiv2si (__o, 0);\n+  ret.val[1] = (uint32x2_t) __builtin_aarch64_get_dregxiv2si (__o, 1);\n+  ret.val[2] = (uint32x2_t) __builtin_aarch64_get_dregxiv2si (__o, 2);\n+  ret.val[3] = (uint32x2_t) __builtin_aarch64_get_dregxiv2si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline float32x2x4_t __attribute__ ((__always_inline__))\n+vld4_dup_f32 (const float32_t * __a)\n+{\n+  float32x2x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv2sf ((const __builtin_aarch64_simd_sf *) __a);\n+  ret.val[0] = (float32x2_t) __builtin_aarch64_get_dregxiv2sf (__o, 0);\n+  ret.val[1] = (float32x2_t) __builtin_aarch64_get_dregxiv2sf (__o, 1);\n+  ret.val[2] = (float32x2_t) __builtin_aarch64_get_dregxiv2sf (__o, 2);\n+  ret.val[3] = (float32x2_t) __builtin_aarch64_get_dregxiv2sf (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline int8x16x4_t __attribute__ ((__always_inline__))\n+vld4q_dup_s8 (const int8_t * __a)\n+{\n+  int8x16x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv16qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (int8x16_t) __builtin_aarch64_get_qregxiv16qi (__o, 0);\n+  ret.val[1] = (int8x16_t) __builtin_aarch64_get_qregxiv16qi (__o, 1);\n+  ret.val[2] = (int8x16_t) __builtin_aarch64_get_qregxiv16qi (__o, 2);\n+  ret.val[3] = (int8x16_t) __builtin_aarch64_get_qregxiv16qi (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline poly8x16x4_t __attribute__ ((__always_inline__))\n+vld4q_dup_p8 (const poly8_t * __a)\n+{\n+  poly8x16x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv16qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (poly8x16_t) __builtin_aarch64_get_qregxiv16qi (__o, 0);\n+  ret.val[1] = (poly8x16_t) __builtin_aarch64_get_qregxiv16qi (__o, 1);\n+  ret.val[2] = (poly8x16_t) __builtin_aarch64_get_qregxiv16qi (__o, 2);\n+  ret.val[3] = (poly8x16_t) __builtin_aarch64_get_qregxiv16qi (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline int16x8x4_t __attribute__ ((__always_inline__))\n+vld4q_dup_s16 (const int16_t * __a)\n+{\n+  int16x8x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv8hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (int16x8_t) __builtin_aarch64_get_qregxiv8hi (__o, 0);\n+  ret.val[1] = (int16x8_t) __builtin_aarch64_get_qregxiv8hi (__o, 1);\n+  ret.val[2] = (int16x8_t) __builtin_aarch64_get_qregxiv8hi (__o, 2);\n+  ret.val[3] = (int16x8_t) __builtin_aarch64_get_qregxiv8hi (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline poly16x8x4_t __attribute__ ((__always_inline__))\n+vld4q_dup_p16 (const poly16_t * __a)\n+{\n+  poly16x8x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv8hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (poly16x8_t) __builtin_aarch64_get_qregxiv8hi (__o, 0);\n+  ret.val[1] = (poly16x8_t) __builtin_aarch64_get_qregxiv8hi (__o, 1);\n+  ret.val[2] = (poly16x8_t) __builtin_aarch64_get_qregxiv8hi (__o, 2);\n+  ret.val[3] = (poly16x8_t) __builtin_aarch64_get_qregxiv8hi (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline int32x4x4_t __attribute__ ((__always_inline__))\n+vld4q_dup_s32 (const int32_t * __a)\n+{\n+  int32x4x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv4si ((const __builtin_aarch64_simd_si *) __a);\n+  ret.val[0] = (int32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (int32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (int32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (int32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline int64x2x4_t __attribute__ ((__always_inline__))\n+vld4q_dup_s64 (const int64_t * __a)\n+{\n+  int64x2x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (int64x2_t) __builtin_aarch64_get_qregxiv2di (__o, 0);\n+  ret.val[1] = (int64x2_t) __builtin_aarch64_get_qregxiv2di (__o, 1);\n+  ret.val[2] = (int64x2_t) __builtin_aarch64_get_qregxiv2di (__o, 2);\n+  ret.val[3] = (int64x2_t) __builtin_aarch64_get_qregxiv2di (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint8x16x4_t __attribute__ ((__always_inline__))\n+vld4q_dup_u8 (const uint8_t * __a)\n+{\n+  uint8x16x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv16qi ((const __builtin_aarch64_simd_qi *) __a);\n+  ret.val[0] = (uint8x16_t) __builtin_aarch64_get_qregxiv16qi (__o, 0);\n+  ret.val[1] = (uint8x16_t) __builtin_aarch64_get_qregxiv16qi (__o, 1);\n+  ret.val[2] = (uint8x16_t) __builtin_aarch64_get_qregxiv16qi (__o, 2);\n+  ret.val[3] = (uint8x16_t) __builtin_aarch64_get_qregxiv16qi (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint16x8x4_t __attribute__ ((__always_inline__))\n+vld4q_dup_u16 (const uint16_t * __a)\n+{\n+  uint16x8x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv8hi ((const __builtin_aarch64_simd_hi *) __a);\n+  ret.val[0] = (uint16x8_t) __builtin_aarch64_get_qregxiv8hi (__o, 0);\n+  ret.val[1] = (uint16x8_t) __builtin_aarch64_get_qregxiv8hi (__o, 1);\n+  ret.val[2] = (uint16x8_t) __builtin_aarch64_get_qregxiv8hi (__o, 2);\n+  ret.val[3] = (uint16x8_t) __builtin_aarch64_get_qregxiv8hi (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint32x4x4_t __attribute__ ((__always_inline__))\n+vld4q_dup_u32 (const uint32_t * __a)\n+{\n+  uint32x4x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv4si ((const __builtin_aarch64_simd_si *) __a);\n+  ret.val[0] = (uint32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 0);\n+  ret.val[1] = (uint32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 1);\n+  ret.val[2] = (uint32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 2);\n+  ret.val[3] = (uint32x4_t) __builtin_aarch64_get_qregxiv4si (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline uint64x2x4_t __attribute__ ((__always_inline__))\n+vld4q_dup_u64 (const uint64_t * __a)\n+{\n+  uint64x2x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv2di ((const __builtin_aarch64_simd_di *) __a);\n+  ret.val[0] = (uint64x2_t) __builtin_aarch64_get_qregxiv2di (__o, 0);\n+  ret.val[1] = (uint64x2_t) __builtin_aarch64_get_qregxiv2di (__o, 1);\n+  ret.val[2] = (uint64x2_t) __builtin_aarch64_get_qregxiv2di (__o, 2);\n+  ret.val[3] = (uint64x2_t) __builtin_aarch64_get_qregxiv2di (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline float32x4x4_t __attribute__ ((__always_inline__))\n+vld4q_dup_f32 (const float32_t * __a)\n+{\n+  float32x4x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv4sf ((const __builtin_aarch64_simd_sf *) __a);\n+  ret.val[0] = (float32x4_t) __builtin_aarch64_get_qregxiv4sf (__o, 0);\n+  ret.val[1] = (float32x4_t) __builtin_aarch64_get_qregxiv4sf (__o, 1);\n+  ret.val[2] = (float32x4_t) __builtin_aarch64_get_qregxiv4sf (__o, 2);\n+  ret.val[3] = (float32x4_t) __builtin_aarch64_get_qregxiv4sf (__o, 3);\n+  return ret;\n+}\n+\n+__extension__ static __inline float64x2x4_t __attribute__ ((__always_inline__))\n+vld4q_dup_f64 (const float64_t * __a)\n+{\n+  float64x2x4_t ret;\n+  __builtin_aarch64_simd_xi __o;\n+  __o = __builtin_aarch64_ld4rv2df ((const __builtin_aarch64_simd_df *) __a);\n+  ret.val[0] = (float64x2_t) __builtin_aarch64_get_qregxiv2df (__o, 0);\n+  ret.val[1] = (float64x2_t) __builtin_aarch64_get_qregxiv2df (__o, 1);\n+  ret.val[2] = (float64x2_t) __builtin_aarch64_get_qregxiv2df (__o, 2);\n+  ret.val[3] = (float64x2_t) __builtin_aarch64_get_qregxiv2df (__o, 3);\n+  return ret;\n+}\n+\n /* vmax */\n \n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))"}]}