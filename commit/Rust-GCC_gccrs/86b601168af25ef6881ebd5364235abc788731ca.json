{"sha": "86b601168af25ef6881ebd5364235abc788731ca", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ODZiNjAxMTY4YWYyNWVmNjg4MWViZDUzNjQyMzVhYmM3ODg3MzFjYQ==", "commit": {"author": {"name": "Julian Brown", "email": "julian@codesourcery.com", "date": "2011-09-14T13:47:42Z"}, "committer": {"name": "Julian Brown", "email": "jules@gcc.gnu.org", "date": "2011-09-14T13:47:42Z"}, "message": "arm.c (arm_override_options): Add unaligned_access support.\n\n\tgcc/\n\t* config/arm/arm.c (arm_override_options): Add unaligned_access\n\tsupport.\n\t(arm_file_start): Emit attribute for unaligned access as\n\tappropriate.\n\t* config/arm/arm.md (UNSPEC_UNALIGNED_LOAD)\n\t(UNSPEC_UNALIGNED_STORE): Add constants for unspecs.\n\t(insv, extzv): Add unaligned-access support.\n\t(extv): Change to expander. Likewise.\n\t(extzv_t1, extv_regsi): Add helpers.\n\t(unaligned_loadsi, unaligned_loadhis, unaligned_loadhiu)\n\t(unaligned_storesi, unaligned_storehi): New.\n\t(*extv_reg): New (previous extv implementation).\n\t* config/arm/arm.opt (munaligned_access): Add option.\n\t* config/arm/constraints.md (Uw): New constraint.\n\t* expmed.c (store_bit_field_1): Adjust bitfield numbering according\n\tto size of access, not size of unit, when BITS_BIG_ENDIAN !=\n\tBYTES_BIG_ENDIAN. Don't use bitfield accesses for\n\tvolatile accesses when -fstrict-volatile-bitfields is in effect.\n\t(extract_bit_field_1): Likewise.\n\nFrom-SVN: r178852", "tree": {"sha": "1e6ac52a6b4abbbaab31d0119d7a31c0a62ec1e4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/1e6ac52a6b4abbbaab31d0119d7a31c0a62ec1e4"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/86b601168af25ef6881ebd5364235abc788731ca", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/86b601168af25ef6881ebd5364235abc788731ca", "html_url": "https://github.com/Rust-GCC/gccrs/commit/86b601168af25ef6881ebd5364235abc788731ca", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/86b601168af25ef6881ebd5364235abc788731ca/comments", "author": {"login": "jtb20", "id": 6094880, "node_id": "MDQ6VXNlcjYwOTQ4ODA=", "avatar_url": "https://avatars.githubusercontent.com/u/6094880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jtb20", "html_url": "https://github.com/jtb20", "followers_url": "https://api.github.com/users/jtb20/followers", "following_url": "https://api.github.com/users/jtb20/following{/other_user}", "gists_url": "https://api.github.com/users/jtb20/gists{/gist_id}", "starred_url": "https://api.github.com/users/jtb20/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jtb20/subscriptions", "organizations_url": "https://api.github.com/users/jtb20/orgs", "repos_url": "https://api.github.com/users/jtb20/repos", "events_url": "https://api.github.com/users/jtb20/events{/privacy}", "received_events_url": "https://api.github.com/users/jtb20/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "fafaf06fba1bd3a03b1518666160d9765965a0db", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fafaf06fba1bd3a03b1518666160d9765965a0db", "html_url": "https://github.com/Rust-GCC/gccrs/commit/fafaf06fba1bd3a03b1518666160d9765965a0db"}], "stats": {"total": 400, "additions": 352, "deletions": 48}, "files": [{"sha": "1770b738dfc59be8f679bd049420c5f0e5d8db52", "filename": "gcc/ChangeLog", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/86b601168af25ef6881ebd5364235abc788731ca/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/86b601168af25ef6881ebd5364235abc788731ca/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=86b601168af25ef6881ebd5364235abc788731ca", "patch": "@@ -1,3 +1,25 @@\n+2011-09-14  Julian Brown  <julian@codesourcery.com>\n+\n+\t* config/arm/arm.c (arm_override_options): Add unaligned_access\n+\tsupport.\n+\t(arm_file_start): Emit attribute for unaligned access as\n+\tappropriate.\n+\t* config/arm/arm.md (UNSPEC_UNALIGNED_LOAD)\n+\t(UNSPEC_UNALIGNED_STORE): Add constants for unspecs.\n+\t(insv, extzv): Add unaligned-access support.\n+\t(extv): Change to expander. Likewise.\n+\t(extzv_t1, extv_regsi): Add helpers.\n+\t(unaligned_loadsi, unaligned_loadhis, unaligned_loadhiu)\n+\t(unaligned_storesi, unaligned_storehi): New.\n+\t(*extv_reg): New (previous extv implementation).\n+\t* config/arm/arm.opt (munaligned_access): Add option.\n+\t* config/arm/constraints.md (Uw): New constraint.\n+\t* expmed.c (store_bit_field_1): Adjust bitfield numbering according\n+\tto size of access, not size of unit, when BITS_BIG_ENDIAN !=\n+\tBYTES_BIG_ENDIAN. Don't use bitfield accesses for\n+\tvolatile accesses when -fstrict-volatile-bitfields is in effect.\n+\t(extract_bit_field_1): Likewise.\n+\n 2011-09-14  Richard Sandiford  <richard.sandiford@linaro.org>\n \n \t* simplify-rtx.c (simplify_subreg): Check that the inner mode is"}, {"sha": "b45f9392e2fea4f1bb3df715e1ea773a7511728d", "filename": "gcc/config/arm/arm.c", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/86b601168af25ef6881ebd5364235abc788731ca/gcc%2Fconfig%2Farm%2Farm.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/86b601168af25ef6881ebd5364235abc788731ca/gcc%2Fconfig%2Farm%2Farm.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm.c?ref=86b601168af25ef6881ebd5364235abc788731ca", "patch": "@@ -1915,6 +1915,28 @@ arm_option_override (void)\n \tfix_cm3_ldrd = 0;\n     }\n \n+  /* Enable -munaligned-access by default for\n+     - all ARMv6 architecture-based processors\n+     - ARMv7-A, ARMv7-R, and ARMv7-M architecture-based processors.\n+\n+     Disable -munaligned-access by default for\n+     - all pre-ARMv6 architecture-based processors\n+     - ARMv6-M architecture-based processors.  */\n+\n+  if (unaligned_access == 2)\n+    {\n+      if (arm_arch6 && (arm_arch_notm || arm_arch7))\n+\tunaligned_access = 1;\n+      else\n+\tunaligned_access = 0;\n+    }\n+  else if (unaligned_access == 1\n+\t   && !(arm_arch6 && (arm_arch_notm || arm_arch7)))\n+    {\n+      warning (0, \"target CPU does not support unaligned accesses\");\n+      unaligned_access = 0;\n+    }\n+\n   if (TARGET_THUMB1 && flag_schedule_insns)\n     {\n       /* Don't warn since it's on by default in -O2.  */\n@@ -22274,6 +22296,10 @@ arm_file_start (void)\n \tval = 6;\n       asm_fprintf (asm_out_file, \"\\t.eabi_attribute 30, %d\\n\", val);\n \n+      /* Tag_CPU_unaligned_access.  */\n+      asm_fprintf (asm_out_file, \"\\t.eabi_attribute 34, %d\\n\",\n+\t\t   unaligned_access);\n+\n       /* Tag_ABI_FP_16bit_format.  */\n       if (arm_fp16_format)\n \tasm_fprintf (asm_out_file, \"\\t.eabi_attribute 38, %d\\n\","}, {"sha": "bb9e3283fc369287acb11acfa98116eba928761f", "filename": "gcc/config/arm/arm.md", "status": "modified", "additions": 260, "deletions": 34, "changes": 294, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/86b601168af25ef6881ebd5364235abc788731ca/gcc%2Fconfig%2Farm%2Farm.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/86b601168af25ef6881ebd5364235abc788731ca/gcc%2Fconfig%2Farm%2Farm.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm.md?ref=86b601168af25ef6881ebd5364235abc788731ca", "patch": "@@ -112,6 +112,10 @@\n   UNSPEC_SYMBOL_OFFSET  ; The offset of the start of the symbol from\n                         ; another symbolic address.\n   UNSPEC_MEMORY_BARRIER ; Represent a memory barrier.\n+  UNSPEC_UNALIGNED_LOAD\t; Used to represent ldr/ldrh instructions that access\n+\t\t\t; unaligned locations, on architectures which support\n+\t\t\t; that.\n+  UNSPEC_UNALIGNED_STORE ; Same for str/strh.\n ])\n \n ;; UNSPEC_VOLATILE Usage:\n@@ -2481,10 +2485,10 @@\n ;;; this insv pattern, so this pattern needs to be reevalutated.\n \n (define_expand \"insv\"\n-  [(set (zero_extract:SI (match_operand:SI 0 \"s_register_operand\" \"\")\n-                         (match_operand:SI 1 \"general_operand\" \"\")\n-                         (match_operand:SI 2 \"general_operand\" \"\"))\n-        (match_operand:SI 3 \"reg_or_int_operand\" \"\"))]\n+  [(set (zero_extract (match_operand 0 \"nonimmediate_operand\" \"\")\n+                      (match_operand 1 \"general_operand\" \"\")\n+                      (match_operand 2 \"general_operand\" \"\"))\n+        (match_operand 3 \"reg_or_int_operand\" \"\"))]\n   \"TARGET_ARM || arm_arch_thumb2\"\n   \"\n   {\n@@ -2495,35 +2499,70 @@\n \n     if (arm_arch_thumb2)\n       {\n-\tbool use_bfi = TRUE;\n-\n-\tif (GET_CODE (operands[3]) == CONST_INT)\n+        if (unaligned_access && MEM_P (operands[0])\n+\t    && s_register_operand (operands[3], GET_MODE (operands[3]))\n+\t    && (width == 16 || width == 32) && (start_bit % BITS_PER_UNIT) == 0)\n \t  {\n-\t    HOST_WIDE_INT val = INTVAL (operands[3]) & mask;\n+\t    rtx base_addr;\n+\n+\t    if (BYTES_BIG_ENDIAN)\n+\t      start_bit = GET_MODE_BITSIZE (GET_MODE (operands[3])) - width\n+\t\t\t  - start_bit;\n \n-\t    if (val == 0)\n+\t    if (width == 32)\n \t      {\n-\t\temit_insn (gen_insv_zero (operands[0], operands[1],\n-\t\t\t\t\t  operands[2]));\n-\t\tDONE;\n+\t        base_addr = adjust_address (operands[0], SImode,\n+\t\t\t\t\t    start_bit / BITS_PER_UNIT);\n+\t\temit_insn (gen_unaligned_storesi (base_addr, operands[3]));\n \t      }\n+\t    else\n+\t      {\n+\t        rtx tmp = gen_reg_rtx (HImode);\n \n-\t    /* See if the set can be done with a single orr instruction.  */\n-\t    if (val == mask && const_ok_for_arm (val << start_bit))\n-\t      use_bfi = FALSE;\n+\t        base_addr = adjust_address (operands[0], HImode,\n+\t\t\t\t\t    start_bit / BITS_PER_UNIT);\n+\t\temit_move_insn (tmp, gen_lowpart (HImode, operands[3]));\n+\t\temit_insn (gen_unaligned_storehi (base_addr, tmp));\n+\t      }\n+\t    DONE;\n \t  }\n-\t  \n-\tif (use_bfi)\n+\telse if (s_register_operand (operands[0], GET_MODE (operands[0])))\n \t  {\n-\t    if (GET_CODE (operands[3]) != REG)\n-\t      operands[3] = force_reg (SImode, operands[3]);\n+\t    bool use_bfi = TRUE;\n \n-\t    emit_insn (gen_insv_t2 (operands[0], operands[1], operands[2],\n-\t\t\t\t    operands[3]));\n-\t    DONE;\n+\t    if (GET_CODE (operands[3]) == CONST_INT)\n+\t      {\n+\t\tHOST_WIDE_INT val = INTVAL (operands[3]) & mask;\n+\n+\t\tif (val == 0)\n+\t\t  {\n+\t\t    emit_insn (gen_insv_zero (operands[0], operands[1],\n+\t\t\t\t\t      operands[2]));\n+\t\t    DONE;\n+\t\t  }\n+\n+\t\t/* See if the set can be done with a single orr instruction.  */\n+\t\tif (val == mask && const_ok_for_arm (val << start_bit))\n+\t\t  use_bfi = FALSE;\n+\t      }\n+\n+\t    if (use_bfi)\n+\t      {\n+\t\tif (GET_CODE (operands[3]) != REG)\n+\t\t  operands[3] = force_reg (SImode, operands[3]);\n+\n+\t\temit_insn (gen_insv_t2 (operands[0], operands[1], operands[2],\n+\t\t\t\t\toperands[3]));\n+\t\tDONE;\n+\t      }\n \t  }\n+\telse\n+\t  FAIL;\n       }\n \n+    if (!s_register_operand (operands[0], GET_MODE (operands[0])))\n+      FAIL;\n+\n     target = copy_rtx (operands[0]);\n     /* Avoid using a subreg as a subtarget, and avoid writing a paradoxical \n        subreg as the final target.  */\n@@ -3715,12 +3754,10 @@\n ;; to reduce register pressure later on.\n \n (define_expand \"extzv\"\n-  [(set (match_dup 4)\n-\t(ashift:SI (match_operand:SI   1 \"register_operand\" \"\")\n-\t\t   (match_operand:SI   2 \"const_int_operand\" \"\")))\n-   (set (match_operand:SI              0 \"register_operand\" \"\")\n-\t(lshiftrt:SI (match_dup 4)\n-\t\t     (match_operand:SI 3 \"const_int_operand\" \"\")))]\n+  [(set (match_operand 0 \"s_register_operand\" \"\")\n+\t(zero_extract (match_operand 1 \"nonimmediate_operand\" \"\")\n+\t\t      (match_operand 2 \"const_int_operand\" \"\")\n+\t\t      (match_operand 3 \"const_int_operand\" \"\")))]\n   \"TARGET_THUMB1 || arm_arch_thumb2\"\n   \"\n   {\n@@ -3729,10 +3766,57 @@\n     \n     if (arm_arch_thumb2)\n       {\n-\temit_insn (gen_extzv_t2 (operands[0], operands[1], operands[2],\n-\t\t\t\t operands[3]));\n-\tDONE;\n+\tHOST_WIDE_INT width = INTVAL (operands[2]);\n+\tHOST_WIDE_INT bitpos = INTVAL (operands[3]);\n+\n+\tif (unaligned_access && MEM_P (operands[1])\n+\t    && (width == 16 || width == 32) && (bitpos % BITS_PER_UNIT) == 0)\n+\t  {\n+\t    rtx base_addr;\n+\n+\t    if (BYTES_BIG_ENDIAN)\n+\t      bitpos = GET_MODE_BITSIZE (GET_MODE (operands[0])) - width\n+\t\t       - bitpos;\n+\n+\t    if (width == 32)\n+              {\n+\t\tbase_addr = adjust_address (operands[1], SImode,\n+\t\t\t\t\t    bitpos / BITS_PER_UNIT);\n+\t\temit_insn (gen_unaligned_loadsi (operands[0], base_addr));\n+              }\n+\t    else\n+              {\n+\t\trtx dest = operands[0];\n+\t\trtx tmp = gen_reg_rtx (SImode);\n+\n+\t\t/* We may get a paradoxical subreg here.  Strip it off.  */\n+\t\tif (GET_CODE (dest) == SUBREG\n+\t\t    && GET_MODE (dest) == SImode\n+\t\t    && GET_MODE (SUBREG_REG (dest)) == HImode)\n+\t\t  dest = SUBREG_REG (dest);\n+\n+\t\tif (GET_MODE_BITSIZE (GET_MODE (dest)) != width)\n+\t\t  FAIL;\n+\n+\t\tbase_addr = adjust_address (operands[1], HImode,\n+\t\t\t\t\t    bitpos / BITS_PER_UNIT);\n+\t\temit_insn (gen_unaligned_loadhiu (tmp, base_addr));\n+\t\temit_move_insn (gen_lowpart (SImode, dest), tmp);\n+\t      }\n+\t    DONE;\n+\t  }\n+\telse if (s_register_operand (operands[1], GET_MODE (operands[1])))\n+\t  {\n+\t    emit_insn (gen_extzv_t2 (operands[0], operands[1], operands[2],\n+\t\t\t\t     operands[3]));\n+\t    DONE;\n+\t  }\n+\telse\n+\t  FAIL;\n       }\n+    \n+    if (!s_register_operand (operands[1], GET_MODE (operands[1])))\n+      FAIL;\n \n     operands[3] = GEN_INT (rshift);\n     \n@@ -3742,12 +3826,154 @@\n         DONE;\n       }\n       \n-    operands[2] = GEN_INT (lshift);\n-    operands[4] = gen_reg_rtx (SImode);\n+    emit_insn (gen_extzv_t1 (operands[0], operands[1], GEN_INT (lshift),\n+\t\t\t     operands[3], gen_reg_rtx (SImode)));\n+    DONE;\n   }\"\n )\n \n-(define_insn \"extv\"\n+;; Helper for extzv, for the Thumb-1 register-shifts case.\n+\n+(define_expand \"extzv_t1\"\n+  [(set (match_operand:SI 4 \"s_register_operand\" \"\")\n+\t(ashift:SI (match_operand:SI 1 \"nonimmediate_operand\" \"\")\n+\t\t   (match_operand:SI 2 \"const_int_operand\" \"\")))\n+   (set (match_operand:SI 0 \"s_register_operand\" \"\")\n+\t(lshiftrt:SI (match_dup 4)\n+\t\t     (match_operand:SI 3 \"const_int_operand\" \"\")))]\n+  \"TARGET_THUMB1\"\n+  \"\")\n+\n+(define_expand \"extv\"\n+  [(set (match_operand 0 \"s_register_operand\" \"\")\n+\t(sign_extract (match_operand 1 \"nonimmediate_operand\" \"\")\n+\t\t      (match_operand 2 \"const_int_operand\" \"\")\n+\t\t      (match_operand 3 \"const_int_operand\" \"\")))]\n+  \"arm_arch_thumb2\"\n+{\n+  HOST_WIDE_INT width = INTVAL (operands[2]);\n+  HOST_WIDE_INT bitpos = INTVAL (operands[3]);\n+\n+  if (unaligned_access && MEM_P (operands[1]) && (width == 16 || width == 32)\n+      && (bitpos % BITS_PER_UNIT)  == 0)\n+    {\n+      rtx base_addr;\n+      \n+      if (BYTES_BIG_ENDIAN)\n+\tbitpos = GET_MODE_BITSIZE (GET_MODE (operands[0])) - width - bitpos;\n+      \n+      if (width == 32)\n+        {\n+\t  base_addr = adjust_address (operands[1], SImode,\n+\t\t\t\t      bitpos / BITS_PER_UNIT);\n+\t  emit_insn (gen_unaligned_loadsi (operands[0], base_addr));\n+        }\n+      else\n+        {\n+\t  rtx dest = operands[0];\n+\t  rtx tmp = gen_reg_rtx (SImode);\n+\t  \n+\t  /* We may get a paradoxical subreg here.  Strip it off.  */\n+\t  if (GET_CODE (dest) == SUBREG\n+\t      && GET_MODE (dest) == SImode\n+\t      && GET_MODE (SUBREG_REG (dest)) == HImode)\n+\t    dest = SUBREG_REG (dest);\n+\t  \n+\t  if (GET_MODE_BITSIZE (GET_MODE (dest)) != width)\n+\t    FAIL;\n+\t  \n+\t  base_addr = adjust_address (operands[1], HImode,\n+\t\t\t\t      bitpos / BITS_PER_UNIT);\n+\t  emit_insn (gen_unaligned_loadhis (tmp, base_addr));\n+\t  emit_move_insn (gen_lowpart (SImode, dest), tmp);\n+\t}\n+\n+      DONE;\n+    }\n+  else if (!s_register_operand (operands[1], GET_MODE (operands[1])))\n+    FAIL;\n+  else if (GET_MODE (operands[0]) == SImode\n+\t   && GET_MODE (operands[1]) == SImode)\n+    {\n+      emit_insn (gen_extv_regsi (operands[0], operands[1], operands[2],\n+\t\t\t\t operands[3]));\n+      DONE;\n+    }\n+\n+  FAIL;\n+})\n+\n+; Helper to expand register forms of extv with the proper modes.\n+\n+(define_expand \"extv_regsi\"\n+  [(set (match_operand:SI 0 \"s_register_operand\" \"\")\n+\t(sign_extract:SI (match_operand:SI 1 \"s_register_operand\" \"\")\n+\t\t\t (match_operand 2 \"const_int_operand\" \"\")\n+\t\t\t (match_operand 3 \"const_int_operand\" \"\")))]\n+  \"\"\n+{\n+})\n+\n+; ARMv6+ unaligned load/store instructions (used for packed structure accesses).\n+\n+(define_insn \"unaligned_loadsi\"\n+  [(set (match_operand:SI 0 \"s_register_operand\" \"=l,r\")\n+\t(unspec:SI [(match_operand:SI 1 \"memory_operand\" \"Uw,m\")]\n+\t\t   UNSPEC_UNALIGNED_LOAD))]\n+  \"unaligned_access && TARGET_32BIT\"\n+  \"ldr%?\\t%0, %1\\t@ unaligned\"\n+  [(set_attr \"arch\" \"t2,any\")\n+   (set_attr \"length\" \"2,4\")\n+   (set_attr \"predicable\" \"yes\")\n+   (set_attr \"type\" \"load1\")])\n+\n+(define_insn \"unaligned_loadhis\"\n+  [(set (match_operand:SI 0 \"s_register_operand\" \"=l,r\")\n+\t(sign_extend:SI\n+\t  (unspec:HI [(match_operand:HI 1 \"memory_operand\" \"Uw,m\")]\n+\t\t     UNSPEC_UNALIGNED_LOAD)))]\n+  \"unaligned_access && TARGET_32BIT\"\n+  \"ldr%(sh%)\\t%0, %1\\t@ unaligned\"\n+  [(set_attr \"arch\" \"t2,any\")\n+   (set_attr \"length\" \"2,4\")\n+   (set_attr \"predicable\" \"yes\")\n+   (set_attr \"type\" \"load_byte\")])\n+\n+(define_insn \"unaligned_loadhiu\"\n+  [(set (match_operand:SI 0 \"s_register_operand\" \"=l,r\")\n+\t(zero_extend:SI\n+\t  (unspec:HI [(match_operand:HI 1 \"memory_operand\" \"Uw,m\")]\n+\t\t     UNSPEC_UNALIGNED_LOAD)))]\n+  \"unaligned_access && TARGET_32BIT\"\n+  \"ldr%(h%)\\t%0, %1\\t@ unaligned\"\n+  [(set_attr \"arch\" \"t2,any\")\n+   (set_attr \"length\" \"2,4\")\n+   (set_attr \"predicable\" \"yes\")\n+   (set_attr \"type\" \"load_byte\")])\n+\n+(define_insn \"unaligned_storesi\"\n+  [(set (match_operand:SI 0 \"memory_operand\" \"=Uw,m\")\n+\t(unspec:SI [(match_operand:SI 1 \"s_register_operand\" \"l,r\")]\n+\t\t   UNSPEC_UNALIGNED_STORE))]\n+  \"unaligned_access && TARGET_32BIT\"\n+  \"str%?\\t%1, %0\\t@ unaligned\"\n+  [(set_attr \"arch\" \"t2,any\")\n+   (set_attr \"length\" \"2,4\")\n+   (set_attr \"predicable\" \"yes\")\n+   (set_attr \"type\" \"store1\")])\n+\n+(define_insn \"unaligned_storehi\"\n+  [(set (match_operand:HI 0 \"memory_operand\" \"=Uw,m\")\n+\t(unspec:HI [(match_operand:HI 1 \"s_register_operand\" \"l,r\")]\n+\t\t   UNSPEC_UNALIGNED_STORE))]\n+  \"unaligned_access && TARGET_32BIT\"\n+  \"str%(h%)\\t%1, %0\\t@ unaligned\"\n+  [(set_attr \"arch\" \"t2,any\")\n+   (set_attr \"length\" \"2,4\")\n+   (set_attr \"predicable\" \"yes\")\n+   (set_attr \"type\" \"store1\")])\n+\n+(define_insn \"*extv_reg\"\n   [(set (match_operand:SI 0 \"s_register_operand\" \"=r\")\n \t(sign_extract:SI (match_operand:SI 1 \"s_register_operand\" \"r\")\n                          (match_operand:SI 2 \"const_int_operand\" \"M\")"}, {"sha": "e33b460520f830a3fc06a33cc875cc8af8967d6d", "filename": "gcc/config/arm/arm.opt", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/86b601168af25ef6881ebd5364235abc788731ca/gcc%2Fconfig%2Farm%2Farm.opt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/86b601168af25ef6881ebd5364235abc788731ca/gcc%2Fconfig%2Farm%2Farm.opt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Farm.opt?ref=86b601168af25ef6881ebd5364235abc788731ca", "patch": "@@ -253,3 +253,7 @@ mfix-cortex-m3-ldrd\n Target Report Var(fix_cm3_ldrd) Init(2)\n Avoid overlapping destination and address registers on LDRD instructions\n that may trigger Cortex-M3 errata.\n+\n+munaligned-access\n+Target Report Var(unaligned_access) Init(2)\n+Enable unaligned word and halfword accesses to packed data."}, {"sha": "d8ce9826b179b33337e50b60d4e423fd2b7f8884", "filename": "gcc/config/arm/constraints.md", "status": "modified", "additions": 14, "deletions": 1, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/86b601168af25ef6881ebd5364235abc788731ca/gcc%2Fconfig%2Farm%2Fconstraints.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/86b601168af25ef6881ebd5364235abc788731ca/gcc%2Fconfig%2Farm%2Fconstraints.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Farm%2Fconstraints.md?ref=86b601168af25ef6881ebd5364235abc788731ca", "patch": "@@ -36,7 +36,7 @@\n ;; The following memory constraints have been used:\n ;; in ARM/Thumb-2 state: Q, Ut, Uv, Uy, Un, Um, Us\n ;; in ARM state: Uq\n-;; in Thumb state: Uu\n+;; in Thumb state: Uu, Uw\n \n \n (define_register_constraint \"f\" \"TARGET_ARM ? FPA_REGS : NO_REGS\"\n@@ -353,6 +353,19 @@\n \t\t   && thumb1_legitimate_address_p (GET_MODE (op), XEXP (op, 0),\n \t\t\t\t\t\t   0)\")))\n \n+; The 16-bit post-increment LDR/STR accepted by thumb1_legitimate_address_p\n+; are actually LDM/STM instructions, so cannot be used to access unaligned\n+; data.\n+(define_memory_constraint \"Uw\"\n+ \"@internal\n+  In Thumb state an address that is valid in 16bit encoding, and that can be\n+  used for unaligned accesses.\"\n+ (and (match_code \"mem\")\n+      (match_test \"TARGET_THUMB\n+\t\t   && thumb1_legitimate_address_p (GET_MODE (op), XEXP (op, 0),\n+\t\t\t\t\t\t   0)\n+\t\t   && GET_CODE (XEXP (op, 0)) != POST_INC\")))\n+\n ;; We used to have constraint letters for S and R in ARM state, but\n ;; all uses of these now appear to have been removed.\n "}, {"sha": "1528fbb11127d5d00a7f52540e337456dbad237c", "filename": "gcc/expmed.c", "status": "modified", "additions": 26, "deletions": 13, "changes": 39, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/86b601168af25ef6881ebd5364235abc788731ca/gcc%2Fexpmed.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/86b601168af25ef6881ebd5364235abc788731ca/gcc%2Fexpmed.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpmed.c?ref=86b601168af25ef6881ebd5364235abc788731ca", "patch": "@@ -620,6 +620,10 @@ store_bit_field_1 (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,\n       && GET_MODE (value) != BLKmode\n       && bitsize > 0\n       && GET_MODE_BITSIZE (op_mode) >= bitsize\n+      /* Do not use insv for volatile bitfields when\n+         -fstrict-volatile-bitfields is in effect.  */\n+      && !(MEM_P (op0) && MEM_VOLATILE_P (op0)\n+\t   && flag_strict_volatile_bitfields > 0)\n       && ! ((REG_P (op0) || GET_CODE (op0) == SUBREG)\n \t    && (bitsize + bitpos > GET_MODE_BITSIZE (op_mode))))\n     {\n@@ -659,19 +663,21 @@ store_bit_field_1 (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,\n \t  copy_back = true;\n \t}\n \n-      /* On big-endian machines, we count bits from the most significant.\n-\t If the bit field insn does not, we must invert.  */\n-\n-      if (BITS_BIG_ENDIAN != BYTES_BIG_ENDIAN)\n-\txbitpos = unit - bitsize - xbitpos;\n-\n       /* We have been counting XBITPOS within UNIT.\n \t Count instead within the size of the register.  */\n-      if (BITS_BIG_ENDIAN && !MEM_P (xop0))\n+      if (BYTES_BIG_ENDIAN && !MEM_P (xop0))\n \txbitpos += GET_MODE_BITSIZE (op_mode) - unit;\n \n       unit = GET_MODE_BITSIZE (op_mode);\n \n+      /* If BITS_BIG_ENDIAN is zero on a BYTES_BIG_ENDIAN machine, we count\n+         \"backwards\" from the size of the unit we are inserting into.\n+\t Otherwise, we count bits from the most significant on a\n+\t BYTES/BITS_BIG_ENDIAN machine.  */\n+\n+      if (BITS_BIG_ENDIAN != BYTES_BIG_ENDIAN)\n+\txbitpos = unit - bitsize - xbitpos;\n+\n       /* Convert VALUE to op_mode (which insv insn wants) in VALUE1.  */\n       value1 = value;\n       if (GET_MODE (value) != op_mode)\n@@ -1507,6 +1513,10 @@ extract_bit_field_1 (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,\n   if (ext_mode != MAX_MACHINE_MODE\n       && bitsize > 0\n       && GET_MODE_BITSIZE (ext_mode) >= bitsize\n+      /* Do not use extv/extzv for volatile bitfields when\n+         -fstrict-volatile-bitfields is in effect.  */\n+      && !(MEM_P (op0) && MEM_VOLATILE_P (op0)\n+\t   && flag_strict_volatile_bitfields > 0)\n       /* If op0 is a register, we need it in EXT_MODE to make it\n \t acceptable to the format of ext(z)v.  */\n       && !(GET_CODE (op0) == SUBREG && GET_MODE (op0) != ext_mode)\n@@ -1528,17 +1538,20 @@ extract_bit_field_1 (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,\n \t/* Get ref to first byte containing part of the field.  */\n \txop0 = adjust_address (xop0, byte_mode, xoffset);\n \n-      /* On big-endian machines, we count bits from the most significant.\n-\t If the bit field insn does not, we must invert.  */\n-      if (BITS_BIG_ENDIAN != BYTES_BIG_ENDIAN)\n-\txbitpos = unit - bitsize - xbitpos;\n-\n       /* Now convert from counting within UNIT to counting in EXT_MODE.  */\n-      if (BITS_BIG_ENDIAN && !MEM_P (xop0))\n+      if (BYTES_BIG_ENDIAN && !MEM_P (xop0))\n \txbitpos += GET_MODE_BITSIZE (ext_mode) - unit;\n \n       unit = GET_MODE_BITSIZE (ext_mode);\n \n+      /* If BITS_BIG_ENDIAN is zero on a BYTES_BIG_ENDIAN machine, we count\n+         \"backwards\" from the size of the unit we are extracting from.\n+\t Otherwise, we count bits from the most significant on a\n+\t BYTES/BITS_BIG_ENDIAN machine.  */\n+\n+      if (BITS_BIG_ENDIAN != BYTES_BIG_ENDIAN)\n+\txbitpos = unit - bitsize - xbitpos;\n+\n       if (xtarget == 0)\n \txtarget = xspec_target = gen_reg_rtx (tmode);\n "}]}