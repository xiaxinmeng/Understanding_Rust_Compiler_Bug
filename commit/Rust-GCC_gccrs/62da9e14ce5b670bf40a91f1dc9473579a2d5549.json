{"sha": "62da9e14ce5b670bf40a91f1dc9473579a2d5549", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NjJkYTllMTRjZTViNjcwYmY0MGE5MWYxZGM5NDczNTc5YTJkNTU0OQ==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2016-07-06T08:16:53Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2016-07-06T08:16:53Z"}, "message": "[7/7] Add negative and zero strides to vect_memory_access_type\n\nThis patch uses the vect_memory_access_type from patch 6 to represent\nthe effect of a negative contiguous stride or a zero stride.  The latter\nis valid only for loads.\n\nTested on aarch64-linux-gnu and x86_64-linux-gnu.\n\ngcc/\n\t* tree-vectorizer.h (vect_memory_access_type): Add\n\tVMAT_INVARIANT, VMAT_CONTIGUOUS_DOWN and VMAT_CONTIGUOUS_REVERSED.\n\t* tree-vect-stmts.c (compare_step_with_zero): New function.\n\t(perm_mask_for_reverse): Move further up file.\n\t(get_group_load_store_type): Stick to VMAT_ELEMENTWISE if the\n\tstep is negative.\n\t(get_negative_load_store_type): New function.\n\t(get_load_store_type): Call it.  Add an ncopies argument.\n\t(vectorizable_mask_load_store): Update call accordingly and\n\tremove tests for negative steps.\n\t(vectorizable_store, vectorizable_load): Likewise.  Handle new\n\tmemory_access_types.\n\nFrom-SVN: r238039", "tree": {"sha": "f64f6affdc9e9aa406115967f1fe3db21b1b124c", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/f64f6affdc9e9aa406115967f1fe3db21b1b124c"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/62da9e14ce5b670bf40a91f1dc9473579a2d5549", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/62da9e14ce5b670bf40a91f1dc9473579a2d5549", "html_url": "https://github.com/Rust-GCC/gccrs/commit/62da9e14ce5b670bf40a91f1dc9473579a2d5549", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/62da9e14ce5b670bf40a91f1dc9473579a2d5549/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "2de001eed013bb666f832694bc75b8f055ffdc76", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2de001eed013bb666f832694bc75b8f055ffdc76", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2de001eed013bb666f832694bc75b8f055ffdc76"}], "stats": {"total": 261, "additions": 140, "deletions": 121}, "files": [{"sha": "c853885a2b36ff58f388f7b64b88c241618ca316", "filename": "gcc/ChangeLog", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/62da9e14ce5b670bf40a91f1dc9473579a2d5549/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/62da9e14ce5b670bf40a91f1dc9473579a2d5549/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=62da9e14ce5b670bf40a91f1dc9473579a2d5549", "patch": "@@ -1,3 +1,18 @@\n+2016-07-06  Richard Sandiford  <richard.sandiford@arm.com>\n+\n+\t* tree-vectorizer.h (vect_memory_access_type): Add\n+\tVMAT_INVARIANT, VMAT_CONTIGUOUS_DOWN and VMAT_CONTIGUOUS_REVERSED.\n+\t* tree-vect-stmts.c (compare_step_with_zero): New function.\n+\t(perm_mask_for_reverse): Move further up file.\n+\t(get_group_load_store_type): Stick to VMAT_ELEMENTWISE if the\n+\tstep is negative.\n+\t(get_negative_load_store_type): New function.\n+\t(get_load_store_type): Call it.  Add an ncopies argument.\n+\t(vectorizable_mask_load_store): Update call accordingly and\n+\tremove tests for negative steps.\n+\t(vectorizable_store, vectorizable_load): Likewise.  Handle new\n+\tmemory_access_types.\n+\n 2016-07-06  Richard Sandiford  <richard.sandiford@arm.com>\n \n \t* tree-vectorizer.h (vect_memory_access_type): New enum."}, {"sha": "ffa5e9881d683e3a647c4e7b40aa9cd37f659459", "filename": "gcc/tree-vect-stmts.c", "status": "modified", "additions": 113, "deletions": 121, "changes": 234, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/62da9e14ce5b670bf40a91f1dc9473579a2d5549/gcc%2Ftree-vect-stmts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/62da9e14ce5b670bf40a91f1dc9473579a2d5549/gcc%2Ftree-vect-stmts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-stmts.c?ref=62da9e14ce5b670bf40a91f1dc9473579a2d5549", "patch": "@@ -1672,6 +1672,42 @@ vectorizable_internal_function (combined_fn cfn, tree fndecl,\n static tree permute_vec_elements (tree, tree, tree, gimple *,\n \t\t\t\t  gimple_stmt_iterator *);\n \n+/* STMT is a non-strided load or store, meaning that it accesses\n+   elements with a known constant step.  Return -1 if that step\n+   is negative, 0 if it is zero, and 1 if it is greater than zero.  */\n+\n+static int\n+compare_step_with_zero (gimple *stmt)\n+{\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n+  tree step;\n+  if (loop_vinfo && nested_in_vect_loop_p (LOOP_VINFO_LOOP (loop_vinfo), stmt))\n+    step = STMT_VINFO_DR_STEP (stmt_info);\n+  else\n+    step = DR_STEP (STMT_VINFO_DATA_REF (stmt_info));\n+  return tree_int_cst_compare (step, size_zero_node);\n+}\n+\n+/* If the target supports a permute mask that reverses the elements in\n+   a vector of type VECTYPE, return that mask, otherwise return null.  */\n+\n+static tree\n+perm_mask_for_reverse (tree vectype)\n+{\n+  int i, nunits;\n+  unsigned char *sel;\n+\n+  nunits = TYPE_VECTOR_SUBPARTS (vectype);\n+  sel = XALLOCAVEC (unsigned char, nunits);\n+\n+  for (i = 0; i < nunits; ++i)\n+    sel[i] = nunits - 1 - i;\n+\n+  if (!can_vec_perm_p (TYPE_MODE (vectype), false, sel))\n+    return NULL_TREE;\n+  return vect_gen_perm_mask_checked (vectype, sel);\n+}\n \n /* A subroutine of get_load_store_type, with a subset of the same\n    arguments.  Handle the case where STMT is part of a grouped load\n@@ -1755,7 +1791,8 @@ get_group_load_store_type (gimple *stmt, tree vectype, bool slp,\n \t would access excess elements in the last iteration.  */\n       bool would_overrun_p = (gap != 0);\n       if (!STMT_VINFO_STRIDED_P (stmt_info)\n-\t  && (can_overrun_p || !would_overrun_p))\n+\t  && (can_overrun_p || !would_overrun_p)\n+\t  && compare_step_with_zero (stmt) > 0)\n \t{\n \t  /* First try using LOAD/STORE_LANES.  */\n \t  if (vls_type == VLS_LOAD\n@@ -1814,17 +1851,69 @@ get_group_load_store_type (gimple *stmt, tree vectype, bool slp,\n   return true;\n }\n \n+/* A subroutine of get_load_store_type, with a subset of the same\n+   arguments.  Handle the case where STMT is a load or store that\n+   accesses consecutive elements with a negative step.  */\n+\n+static vect_memory_access_type\n+get_negative_load_store_type (gimple *stmt, tree vectype,\n+\t\t\t      vec_load_store_type vls_type,\n+\t\t\t      unsigned int ncopies)\n+{\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n+  dr_alignment_support alignment_support_scheme;\n+\n+  if (ncopies > 1)\n+    {\n+      if (dump_enabled_p ())\n+\tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n+\t\t\t \"multiple types with negative step.\\n\");\n+      return VMAT_ELEMENTWISE;\n+    }\n+\n+  alignment_support_scheme = vect_supportable_dr_alignment (dr, false);\n+  if (alignment_support_scheme != dr_aligned\n+      && alignment_support_scheme != dr_unaligned_supported)\n+    {\n+      if (dump_enabled_p ())\n+\tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n+\t\t\t \"negative step but alignment required.\\n\");\n+      return VMAT_ELEMENTWISE;\n+    }\n+\n+  if (vls_type == VLS_STORE_INVARIANT)\n+    {\n+      if (dump_enabled_p ())\n+\tdump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t \"negative step with invariant source;\"\n+\t\t\t \" no permute needed.\\n\");\n+      return VMAT_CONTIGUOUS_DOWN;\n+    }\n+\n+  if (!perm_mask_for_reverse (vectype))\n+    {\n+      if (dump_enabled_p ())\n+\tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n+\t\t\t \"negative step and reversing not supported.\\n\");\n+      return VMAT_ELEMENTWISE;\n+    }\n+\n+  return VMAT_CONTIGUOUS_REVERSE;\n+}\n+\n /* Analyze load or store statement STMT of type VLS_TYPE.  Return true\n    if there is a memory access type that the vectorized form can use,\n    storing it in *MEMORY_ACCESS_TYPE if so.  If we decide to use gathers\n    or scatters, fill in GS_INFO accordingly.\n \n    SLP says whether we're performing SLP rather than loop vectorization.\n-   VECTYPE is the vector type that the vectorized statements will use.  */\n+   VECTYPE is the vector type that the vectorized statements will use.\n+   NCOPIES is the number of vector statements that will be needed.  */\n \n static bool\n get_load_store_type (gimple *stmt, tree vectype, bool slp,\n-\t\t     vec_load_store_type vls_type,\n+\t\t     vec_load_store_type vls_type, unsigned int ncopies,\n \t\t     vect_memory_access_type *memory_access_type,\n \t\t     gather_scatter_info *gs_info)\n {\n@@ -1860,7 +1949,19 @@ get_load_store_type (gimple *stmt, tree vectype, bool slp,\n       *memory_access_type = VMAT_ELEMENTWISE;\n     }\n   else\n-    *memory_access_type = VMAT_CONTIGUOUS;\n+    {\n+      int cmp = compare_step_with_zero (stmt);\n+      if (cmp < 0)\n+\t*memory_access_type = get_negative_load_store_type\n+\t  (stmt, vectype, vls_type, ncopies);\n+      else if (cmp == 0)\n+\t{\n+\t  gcc_assert (vls_type == VLS_LOAD);\n+\t  *memory_access_type = VMAT_INVARIANT;\n+\t}\n+      else\n+\t*memory_access_type = VMAT_CONTIGUOUS;\n+    }\n \n   /* FIXME: At the moment the cost model seems to underestimate the\n      cost of using elementwise accesses.  This check preserves the\n@@ -1971,7 +2072,7 @@ vectorizable_mask_load_store (gimple *stmt, gimple_stmt_iterator *gsi,\n     vls_type = VLS_LOAD;\n \n   vect_memory_access_type memory_access_type;\n-  if (!get_load_store_type (stmt, vectype, false, vls_type,\n+  if (!get_load_store_type (stmt, vectype, false, vls_type, ncopies,\n \t\t\t    &memory_access_type, &gs_info))\n     return false;\n \n@@ -1996,10 +2097,6 @@ vectorizable_mask_load_store (gimple *stmt, gimple_stmt_iterator *gsi,\n \t\t\t vls_type == VLS_LOAD ? \"load\" : \"store\");\n       return false;\n     }\n-  else if (tree_int_cst_compare (nested_in_vect_loop\n-\t\t\t\t ? STMT_VINFO_DR_STEP (stmt_info)\n-\t\t\t\t : DR_STEP (dr), size_zero_node) <= 0)\n-    return false;\n   else if (!VECTOR_MODE_P (TYPE_MODE (vectype))\n \t   || !can_vec_mask_load_store_p (TYPE_MODE (vectype),\n \t\t\t\t\t  TYPE_MODE (mask_vectype),\n@@ -5340,27 +5437,6 @@ ensure_base_align (stmt_vec_info stmt_info, struct data_reference *dr)\n }\n \n \n-/* Given a vector type VECTYPE returns the VECTOR_CST mask that implements\n-   reversal of the vector elements.  If that is impossible to do,\n-   returns NULL.  */\n-\n-static tree\n-perm_mask_for_reverse (tree vectype)\n-{\n-  int i, nunits;\n-  unsigned char *sel;\n-\n-  nunits = TYPE_VECTOR_SUBPARTS (vectype);\n-  sel = XALLOCAVEC (unsigned char, nunits);\n-\n-  for (i = 0; i < nunits; ++i)\n-    sel[i] = nunits - 1 - i;\n-\n-  if (!can_vec_perm_p (TYPE_MODE (vectype), false, sel))\n-    return NULL_TREE;\n-  return vect_gen_perm_mask_checked (vectype, sel);\n-}\n-\n /* Function vectorizable_store.\n \n    Check if STMT defines a non scalar data-ref (array/pointer/structure) that\n@@ -5400,7 +5476,6 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n   vec<tree> oprnds = vNULL;\n   vec<tree> result_chain = vNULL;\n   bool inv_p;\n-  bool negative = false;\n   tree offset = NULL_TREE;\n   vec<tree> vec_oprnds = vNULL;\n   bool slp = (slp_node != NULL);\n@@ -5504,44 +5579,8 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n   if (!STMT_VINFO_DATA_REF (stmt_info))\n     return false;\n \n-  if (!STMT_VINFO_STRIDED_P (stmt_info))\n-    {\n-      negative = \n-\t  tree_int_cst_compare (loop && nested_in_vect_loop_p (loop, stmt)\n-\t\t\t\t? STMT_VINFO_DR_STEP (stmt_info) : DR_STEP (dr),\n-\t\t\t\tsize_zero_node) < 0;\n-      if (negative && ncopies > 1)\n-\t{\n-\t  if (dump_enabled_p ())\n-\t    dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-\t\t\t     \"multiple types with negative step.\\n\");\n-\t  return false;\n-\t}\n-      if (negative)\n-\t{\n-\t  alignment_support_scheme = vect_supportable_dr_alignment (dr, false);\n-\t  if (alignment_support_scheme != dr_aligned\n-\t      && alignment_support_scheme != dr_unaligned_supported)\n-\t    {\n-\t      if (dump_enabled_p ())\n-\t\tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-\t\t\t\t \"negative step but alignment required.\\n\");\n-\t      return false;\n-\t    }\n-\t  if (dt != vect_constant_def \n-\t      && dt != vect_external_def\n-\t      && !perm_mask_for_reverse (vectype))\n-\t    {\n-\t      if (dump_enabled_p ())\n-\t\tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-\t\t\t\t \"negative step and reversing not supported.\\n\");\n-\t      return false;\n-\t    }\n-\t}\n-    }\n-\n   vect_memory_access_type memory_access_type;\n-  if (!get_load_store_type (stmt, vectype, slp, vls_type,\n+  if (!get_load_store_type (stmt, vectype, slp, vls_type, ncopies,\n \t\t\t    &memory_access_type, &gs_info))\n     return false;\n \n@@ -5947,7 +5986,8 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t      || alignment_support_scheme == dr_aligned\n \t      || alignment_support_scheme == dr_unaligned_supported);\n \n-  if (negative)\n+  if (memory_access_type == VMAT_CONTIGUOUS_DOWN\n+      || memory_access_type == VMAT_CONTIGUOUS_REVERSE)\n     offset = size_int (-TYPE_VECTOR_SUBPARTS (vectype) + 1);\n \n   if (memory_access_type == VMAT_LOAD_STORE_LANES)\n@@ -6169,9 +6209,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\tset_ptr_info_alignment (get_ptr_info (dataref_ptr), align,\n \t\t\t\t\tmisalign);\n \n-\t      if (negative\n-\t\t  && dt != vect_constant_def \n-\t\t  && dt != vect_external_def)\n+\t      if (memory_access_type == VMAT_CONTIGUOUS_REVERSE)\n \t\t{\n \t\t  tree perm_mask = perm_mask_for_reverse (vectype);\n \t\t  tree perm_dest \n@@ -6375,7 +6413,6 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n   gimple *first_stmt;\n   gimple *first_stmt_for_drptr = NULL;\n   bool inv_p;\n-  bool negative = false;\n   bool compute_in_loop = false;\n   struct loop *at_loop;\n   int vec_num;\n@@ -6531,55 +6568,10 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n     }\n \n   vect_memory_access_type memory_access_type;\n-  if (!get_load_store_type (stmt, vectype, slp, VLS_LOAD,\n+  if (!get_load_store_type (stmt, vectype, slp, VLS_LOAD, ncopies,\n \t\t\t    &memory_access_type, &gs_info))\n     return false;\n \n-  if (!STMT_VINFO_GATHER_SCATTER_P (stmt_info)\n-      && !STMT_VINFO_STRIDED_P (stmt_info))\n-    {\n-      negative = tree_int_cst_compare (nested_in_vect_loop\n-\t\t\t\t       ? STMT_VINFO_DR_STEP (stmt_info)\n-\t\t\t\t       : DR_STEP (dr),\n-\t\t\t\t       size_zero_node) < 0;\n-      if (negative && ncopies > 1)\n-\t{\n-\t  if (dump_enabled_p ())\n-\t    dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-                             \"multiple types with negative step.\\n\");\n-\t  return false;\n-\t}\n-\n-      if (negative)\n-\t{\n-\t  if (grouped_load)\n-\t    {\n-\t      if (dump_enabled_p ())\n-\t\tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-\t\t\t\t \"negative step for group load not supported\"\n-                                 \"\\n\");\n-\t      return false;\n-\t    }\n-\t  alignment_support_scheme = vect_supportable_dr_alignment (dr, false);\n-\t  if (alignment_support_scheme != dr_aligned\n-\t      && alignment_support_scheme != dr_unaligned_supported)\n-\t    {\n-              if (dump_enabled_p ())\n-                dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-                                 \"negative step but alignment required.\\n\");\n-\t      return false;\n-\t    }\n-\t  if (!perm_mask_for_reverse (vectype))\n-\t    {\n-              if (dump_enabled_p ())\n-                dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-                                 \"negative step and reversing not supported.\"\n-                                 \"\\n\");\n-\t      return false;\n-\t    }\n-\t}\n-    }\n-\n   if (!vec_stmt) /* transformation not required.  */\n     {\n       if (!slp)\n@@ -7120,7 +7112,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n   else\n     at_loop = loop;\n \n-  if (negative)\n+  if (memory_access_type == VMAT_CONTIGUOUS_REVERSE)\n     offset = size_int (-TYPE_VECTOR_SUBPARTS (vectype) + 1);\n \n   if (memory_access_type == VMAT_LOAD_STORE_LANES)\n@@ -7409,7 +7401,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t    }\n \t\t}\n \n-\t      if (negative)\n+\t      if (memory_access_type == VMAT_CONTIGUOUS_REVERSE)\n \t\t{\n \t\t  tree perm_mask = perm_mask_for_reverse (vectype);\n \t\t  new_temp = permute_vec_elements (new_temp, new_temp,"}, {"sha": "2cfb72a6fa3ac17100fdf7086f96e96e4145147a", "filename": "gcc/tree-vectorizer.h", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/62da9e14ce5b670bf40a91f1dc9473579a2d5549/gcc%2Ftree-vectorizer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/62da9e14ce5b670bf40a91f1dc9473579a2d5549/gcc%2Ftree-vectorizer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.h?ref=62da9e14ce5b670bf40a91f1dc9473579a2d5549", "patch": "@@ -484,14 +484,26 @@ enum slp_vect_type {\n /* Describes how we're going to vectorize an individual load or store,\n    or a group of loads or stores.  */\n enum vect_memory_access_type {\n+  /* An access to an invariant address.  This is used only for loads.  */\n+  VMAT_INVARIANT,\n+\n   /* A simple contiguous access.  */\n   VMAT_CONTIGUOUS,\n \n+  /* A contiguous access that goes down in memory rather than up,\n+     with no additional permutation.  This is used only for stores\n+     of invariants.  */\n+  VMAT_CONTIGUOUS_DOWN,\n+\n   /* A simple contiguous access in which the elements need to be permuted\n      after loading or before storing.  Only used for loop vectorization;\n      SLP uses separate permutes.  */\n   VMAT_CONTIGUOUS_PERMUTE,\n \n+  /* A simple contiguous access in which the elements need to be reversed\n+     after loading or before storing.  */\n+  VMAT_CONTIGUOUS_REVERSE,\n+\n   /* An access that uses IFN_LOAD_LANES or IFN_STORE_LANES.  */\n   VMAT_LOAD_STORE_LANES,\n "}]}