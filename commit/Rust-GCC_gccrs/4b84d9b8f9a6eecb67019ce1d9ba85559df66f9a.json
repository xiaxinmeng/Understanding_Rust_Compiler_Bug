{"sha": "4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NGI4NGQ5YjhmOWE2ZWVjYjY3MDE5Y2UxZDliYTg1NTU5ZGY2NmY5YQ==", "commit": {"author": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2017-11-20T10:10:23Z"}, "committer": {"name": "Jakub Jelinek", "email": "jakub@gcc.gnu.org", "date": "2017-11-20T10:10:23Z"}, "message": "re PR tree-optimization/78821 (GCC7: Copying whole 32 bits structure field by field not optimised into copying whole 32 bits at once)\n\n\tPR tree-optimization/78821\n\t* gimple-ssa-store-merging.c (find_bswap_or_nop_load): Give up\n\tif base is TARGET_MEM_REF.  If base is not MEM_REF, set base_addr\n\tto the address of the base rather than the base itself.\n\t(find_bswap_or_nop_1): Just use pointer comparison for vuse check.\n\t(find_bswap_or_nop_finalize): New function.\n\t(find_bswap_or_nop): Use it.\n\t(bswap_replace): Return a tree rather than bool, change first\n\targument from gimple * to gimple_stmt_iterator, allow inserting\n\tinto an empty sequence, allow ins_stmt to be NULL - then emit\n\tall stmts into gsi.  Fix up MEM_REF address gimplification.\n\t(pass_optimize_bswap::execute): Adjust bswap_replace caller.\n\t(struct store_immediate_info): Add N and INS_STMT non-static\n\tdata members.\n\t(store_immediate_info::store_immediate_info): Initialize them\n\tfrom newly added ctor args.\n\t(merged_store_group::apply_stores): Formatting fixes.  Sort by\n\tbitpos at the end.\n\t(stmts_may_clobber_ref_p): For stores call also\n\trefs_anti_dependent_p.\n\t(gather_bswap_load_refs): New function.\n\t(imm_store_chain_info::try_coalesce_bswap): New method.\n\t(imm_store_chain_info::coalesce_immediate_stores): Use it.\n\t(split_group): Handle LROTATE_EXPR and NOP_EXPR rhs_code specially.\n\t(imm_store_chain_info::output_merged_store): Fail if number of\n\tnew estimated stmts is bigger or equal than old.  Handle LROTATE_EXPR\n\tand NOP_EXPR rhs_code.\n\t(pass_store_merging::process_store): Compute n and ins_stmt, if\n\tins_stmt is non-NULL and the store rhs is otherwise invalid, use\n\tLROTATE_EXPR rhs_code.  Pass n and ins_stmt to store_immediate_info\n\tctor.\n\t(pass_store_merging::execute): Calculate dominators.\n\n\t* gcc.dg/store_merging_16.c: New test.\n\nFrom-SVN: r254948", "tree": {"sha": "9ac76119fdabe84fcc32f344e5c0e6ad6c8181ed", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/9ac76119fdabe84fcc32f344e5c0e6ad6c8181ed"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a/comments", "author": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "dffec8ebdb449be77bf02fe0cf59237362be991a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/dffec8ebdb449be77bf02fe0cf59237362be991a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/dffec8ebdb449be77bf02fe0cf59237362be991a"}], "stats": {"total": 931, "additions": 813, "deletions": 118}, "files": [{"sha": "48f87abfd97784be3ecf55cab96d4676c6cdf0f7", "filename": "gcc/ChangeLog", "status": "modified", "additions": 33, "deletions": 0, "changes": 33, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a", "patch": "@@ -1,5 +1,38 @@\n 2017-11-20  Jakub Jelinek  <jakub@redhat.com>\n \n+\tPR tree-optimization/78821\n+\t* gimple-ssa-store-merging.c (find_bswap_or_nop_load): Give up\n+\tif base is TARGET_MEM_REF.  If base is not MEM_REF, set base_addr\n+\tto the address of the base rather than the base itself.\n+\t(find_bswap_or_nop_1): Just use pointer comparison for vuse check.\n+\t(find_bswap_or_nop_finalize): New function.\n+\t(find_bswap_or_nop): Use it.\n+\t(bswap_replace): Return a tree rather than bool, change first\n+\targument from gimple * to gimple_stmt_iterator, allow inserting\n+\tinto an empty sequence, allow ins_stmt to be NULL - then emit\n+\tall stmts into gsi.  Fix up MEM_REF address gimplification.\n+\t(pass_optimize_bswap::execute): Adjust bswap_replace caller.\n+\t(struct store_immediate_info): Add N and INS_STMT non-static\n+\tdata members.\n+\t(store_immediate_info::store_immediate_info): Initialize them\n+\tfrom newly added ctor args.\n+\t(merged_store_group::apply_stores): Formatting fixes.  Sort by\n+\tbitpos at the end.\n+\t(stmts_may_clobber_ref_p): For stores call also\n+\trefs_anti_dependent_p.\n+\t(gather_bswap_load_refs): New function.\n+\t(imm_store_chain_info::try_coalesce_bswap): New method.\n+\t(imm_store_chain_info::coalesce_immediate_stores): Use it.\n+\t(split_group): Handle LROTATE_EXPR and NOP_EXPR rhs_code specially.\n+\t(imm_store_chain_info::output_merged_store): Fail if number of\n+\tnew estimated stmts is bigger or equal than old.  Handle LROTATE_EXPR\n+\tand NOP_EXPR rhs_code.\n+\t(pass_store_merging::process_store): Compute n and ins_stmt, if\n+\tins_stmt is non-NULL and the store rhs is otherwise invalid, use\n+\tLROTATE_EXPR rhs_code.  Pass n and ins_stmt to store_immediate_info\n+\tctor.\n+\t(pass_store_merging::execute): Calculate dominators.\n+\n \t* tree-ssa-math-opts.c (nop_stats, bswap_stats, struct symbolic_number,\n \tBITS_PER_MARKER, MARKER_MASK, MARKER_BYTE_UNKNOWN, HEAD_MARKER, CMPNOP,\n \tCMPXCHG, do_shift_rotate, verify_symbolic_number_p,"}, {"sha": "ac8d9eec2b60d8c8f46aa2bf4f7421578e35d154", "filename": "gcc/gimple-ssa-store-merging.c", "status": "modified", "additions": 618, "deletions": 118, "changes": 736, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a/gcc%2Fgimple-ssa-store-merging.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a/gcc%2Fgimple-ssa-store-merging.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgimple-ssa-store-merging.c?ref=4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a", "patch": "@@ -369,7 +369,10 @@ find_bswap_or_nop_load (gimple *stmt, tree ref, struct symbolic_number *n)\n   base_addr = get_inner_reference (ref, &bitsize, &bitpos, &offset, &mode,\n \t\t\t\t   &unsignedp, &reversep, &volatilep);\n \n-  if (TREE_CODE (base_addr) == MEM_REF)\n+  if (TREE_CODE (base_addr) == TARGET_MEM_REF)\n+    /* Do not rewrite TARGET_MEM_REF.  */\n+    return false;\n+  else if (TREE_CODE (base_addr) == MEM_REF)\n     {\n       offset_int bit_offset = 0;\n       tree off = TREE_OPERAND (base_addr, 1);\n@@ -401,6 +404,8 @@ find_bswap_or_nop_load (gimple *stmt, tree ref, struct symbolic_number *n)\n \n       bitpos += bit_offset.to_shwi ();\n     }\n+  else\n+    base_addr = build_fold_addr_expr (base_addr);\n \n   if (bitpos % BITS_PER_UNIT)\n     return false;\n@@ -743,8 +748,7 @@ find_bswap_or_nop_1 (gimple *stmt, struct symbolic_number *n, int limit)\n \t  if (TYPE_PRECISION (n1.type) != TYPE_PRECISION (n2.type))\n \t    return NULL;\n \n-\t  if (!n1.vuse != !n2.vuse\n-\t      || (n1.vuse && !operand_equal_p (n1.vuse, n2.vuse, 0)))\n+\t  if (n1.vuse != n2.vuse)\n \t    return NULL;\n \n \t  source_stmt\n@@ -765,39 +769,21 @@ find_bswap_or_nop_1 (gimple *stmt, struct symbolic_number *n, int limit)\n   return NULL;\n }\n \n-/* Check if STMT completes a bswap implementation or a read in a given\n-   endianness consisting of ORs, SHIFTs and ANDs and sets *BSWAP\n-   accordingly.  It also sets N to represent the kind of operations\n-   performed: size of the resulting expression and whether it works on\n-   a memory source, and if so alias-set and vuse.  At last, the\n-   function returns a stmt whose rhs's first tree is the source\n-   expression.  */\n+/* Helper for find_bswap_or_nop and try_coalesce_bswap to compute\n+   *CMPXCHG, *CMPNOP and adjust *N.  */\n \n-gimple *\n-find_bswap_or_nop (gimple *stmt, struct symbolic_number *n, bool *bswap)\n+void\n+find_bswap_or_nop_finalize (struct symbolic_number *n, uint64_t *cmpxchg,\n+\t\t\t    uint64_t *cmpnop)\n {\n   unsigned rsize;\n   uint64_t tmpn, mask;\n-/* The number which the find_bswap_or_nop_1 result should match in order\n-   to have a full byte swap.  The number is shifted to the right\n-   according to the size of the symbolic number before using it.  */\n-  uint64_t cmpxchg = CMPXCHG;\n-  uint64_t cmpnop = CMPNOP;\n \n-  gimple *ins_stmt;\n-  int limit;\n-\n-  /* The last parameter determines the depth search limit.  It usually\n-     correlates directly to the number n of bytes to be touched.  We\n-     increase that number by log2(n) + 1 here in order to also\n-     cover signed -> unsigned conversions of the src operand as can be seen\n-     in libgcc, and for initial shift/and operation of the src operand.  */\n-  limit = TREE_INT_CST_LOW (TYPE_SIZE_UNIT (gimple_expr_type (stmt)));\n-  limit += 1 + (int) ceil_log2 ((unsigned HOST_WIDE_INT) limit);\n-  ins_stmt = find_bswap_or_nop_1 (stmt, n, limit);\n-\n-  if (!ins_stmt)\n-    return NULL;\n+  /* The number which the find_bswap_or_nop_1 result should match in order\n+     to have a full byte swap.  The number is shifted to the right\n+     according to the size of the symbolic number before using it.  */\n+  *cmpxchg = CMPXCHG;\n+  *cmpnop = CMPNOP;\n \n   /* Find real size of result (highest non-zero byte).  */\n   if (n->base_addr)\n@@ -810,8 +796,8 @@ find_bswap_or_nop (gimple *stmt, struct symbolic_number *n, bool *bswap)\n   if (n->range < (int) sizeof (int64_t))\n     {\n       mask = ((uint64_t) 1 << (n->range * BITS_PER_MARKER)) - 1;\n-      cmpxchg >>= (64 / BITS_PER_MARKER - n->range) * BITS_PER_MARKER;\n-      cmpnop &= mask;\n+      *cmpxchg >>= (64 / BITS_PER_MARKER - n->range) * BITS_PER_MARKER;\n+      *cmpnop &= mask;\n     }\n \n   /* Zero out the bits corresponding to unused bytes in the result of the\n@@ -821,18 +807,47 @@ find_bswap_or_nop (gimple *stmt, struct symbolic_number *n, bool *bswap)\n       if (BYTES_BIG_ENDIAN)\n \t{\n \t  mask = ((uint64_t) 1 << (rsize * BITS_PER_MARKER)) - 1;\n-\t  cmpxchg &= mask;\n-\t  cmpnop >>= (n->range - rsize) * BITS_PER_MARKER;\n+\t  *cmpxchg &= mask;\n+\t  *cmpnop >>= (n->range - rsize) * BITS_PER_MARKER;\n \t}\n       else\n \t{\n \t  mask = ((uint64_t) 1 << (rsize * BITS_PER_MARKER)) - 1;\n-\t  cmpxchg >>= (n->range - rsize) * BITS_PER_MARKER;\n-\t  cmpnop &= mask;\n+\t  *cmpxchg >>= (n->range - rsize) * BITS_PER_MARKER;\n+\t  *cmpnop &= mask;\n \t}\n       n->range = rsize;\n     }\n \n+  n->range *= BITS_PER_UNIT;\n+}\n+\n+/* Check if STMT completes a bswap implementation or a read in a given\n+   endianness consisting of ORs, SHIFTs and ANDs and sets *BSWAP\n+   accordingly.  It also sets N to represent the kind of operations\n+   performed: size of the resulting expression and whether it works on\n+   a memory source, and if so alias-set and vuse.  At last, the\n+   function returns a stmt whose rhs's first tree is the source\n+   expression.  */\n+\n+gimple *\n+find_bswap_or_nop (gimple *stmt, struct symbolic_number *n, bool *bswap)\n+{\n+  /* The last parameter determines the depth search limit.  It usually\n+     correlates directly to the number n of bytes to be touched.  We\n+     increase that number by log2(n) + 1 here in order to also\n+     cover signed -> unsigned conversions of the src operand as can be seen\n+     in libgcc, and for initial shift/and operation of the src operand.  */\n+  int limit = TREE_INT_CST_LOW (TYPE_SIZE_UNIT (gimple_expr_type (stmt)));\n+  limit += 1 + (int) ceil_log2 ((unsigned HOST_WIDE_INT) limit);\n+  gimple *ins_stmt = find_bswap_or_nop_1 (stmt, n, limit);\n+\n+  if (!ins_stmt)\n+    return NULL;\n+\n+  uint64_t cmpxchg, cmpnop;\n+  find_bswap_or_nop_finalize (n, &cmpxchg, &cmpnop);\n+\n   /* A complete byte swap should make the symbolic number to start with\n      the largest digit in the highest order byte. Unchanged symbolic\n      number indicates a read with same endianness as target architecture.  */\n@@ -847,7 +862,6 @@ find_bswap_or_nop (gimple *stmt, struct symbolic_number *n, bool *bswap)\n   if (!n->base_addr && n->n == cmpnop && n->n_ops == 1)\n     return NULL;\n \n-  n->range *= BITS_PER_UNIT;\n   return ins_stmt;\n }\n \n@@ -882,68 +896,89 @@ class pass_optimize_bswap : public gimple_opt_pass\n }; // class pass_optimize_bswap\n \n /* Perform the bswap optimization: replace the expression computed in the rhs\n-   of CUR_STMT by an equivalent bswap, load or load + bswap expression.\n+   of gsi_stmt (GSI) (or if NULL add instead of replace) by an equivalent\n+   bswap, load or load + bswap expression.\n    Which of these alternatives replace the rhs is given by N->base_addr (non\n    null if a load is needed) and BSWAP.  The type, VUSE and set-alias of the\n    load to perform are also given in N while the builtin bswap invoke is given\n-   in FNDEL.  Finally, if a load is involved, SRC_STMT refers to one of the\n-   load statements involved to construct the rhs in CUR_STMT and N->range gives\n-   the size of the rhs expression for maintaining some statistics.\n+   in FNDEL.  Finally, if a load is involved, INS_STMT refers to one of the\n+   load statements involved to construct the rhs in gsi_stmt (GSI) and\n+   N->range gives the size of the rhs expression for maintaining some\n+   statistics.\n \n-   Note that if the replacement involve a load, CUR_STMT is moved just after\n-   SRC_STMT to do the load with the same VUSE which can lead to CUR_STMT\n-   changing of basic block.  */\n+   Note that if the replacement involve a load and if gsi_stmt (GSI) is\n+   non-NULL, that stmt is moved just after INS_STMT to do the load with the\n+   same VUSE which can lead to gsi_stmt (GSI) changing of basic block.  */\n \n-bool\n-bswap_replace (gimple *cur_stmt, gimple *ins_stmt, tree fndecl,\n+tree\n+bswap_replace (gimple_stmt_iterator gsi, gimple *ins_stmt, tree fndecl,\n \t       tree bswap_type, tree load_type, struct symbolic_number *n,\n \t       bool bswap)\n {\n-  gimple_stmt_iterator gsi;\n-  tree src, tmp, tgt;\n+  tree src, tmp, tgt = NULL_TREE;\n   gimple *bswap_stmt;\n \n-  gsi = gsi_for_stmt (cur_stmt);\n+  gimple *cur_stmt = gsi_stmt (gsi);\n   src = n->src;\n-  tgt = gimple_assign_lhs (cur_stmt);\n+  if (cur_stmt)\n+    tgt = gimple_assign_lhs (cur_stmt);\n \n   /* Need to load the value from memory first.  */\n   if (n->base_addr)\n     {\n-      gimple_stmt_iterator gsi_ins = gsi_for_stmt (ins_stmt);\n+      gimple_stmt_iterator gsi_ins = gsi;\n+      if (ins_stmt)\n+\tgsi_ins = gsi_for_stmt (ins_stmt);\n       tree addr_expr, addr_tmp, val_expr, val_tmp;\n       tree load_offset_ptr, aligned_load_type;\n-      gimple *addr_stmt, *load_stmt;\n-      unsigned align;\n+      gimple *load_stmt;\n+      unsigned align = get_object_alignment (src);\n       HOST_WIDE_INT load_offset = 0;\n-      basic_block ins_bb, cur_bb;\n-\n-      ins_bb = gimple_bb (ins_stmt);\n-      cur_bb = gimple_bb (cur_stmt);\n-      if (!dominated_by_p (CDI_DOMINATORS, cur_bb, ins_bb))\n-\treturn false;\n \n-      align = get_object_alignment (src);\n-\n-      /* Move cur_stmt just before  one of the load of the original\n-\t to ensure it has the same VUSE.  See PR61517 for what could\n-\t go wrong.  */\n-      if (gimple_bb (cur_stmt) != gimple_bb (ins_stmt))\n-\treset_flow_sensitive_info (gimple_assign_lhs (cur_stmt));\n-      gsi_move_before (&gsi, &gsi_ins);\n-      gsi = gsi_for_stmt (cur_stmt);\n+      if (cur_stmt)\n+\t{\n+\t  basic_block ins_bb = gimple_bb (ins_stmt);\n+\t  basic_block cur_bb = gimple_bb (cur_stmt);\n+\t  if (!dominated_by_p (CDI_DOMINATORS, cur_bb, ins_bb))\n+\t    return NULL_TREE;\n+\n+\t  /* Move cur_stmt just before one of the load of the original\n+\t     to ensure it has the same VUSE.  See PR61517 for what could\n+\t     go wrong.  */\n+\t  if (gimple_bb (cur_stmt) != gimple_bb (ins_stmt))\n+\t    reset_flow_sensitive_info (gimple_assign_lhs (cur_stmt));\n+\t  gsi_move_before (&gsi, &gsi_ins);\n+\t  gsi = gsi_for_stmt (cur_stmt);\n+\t}\n+      else\n+\tgsi = gsi_ins;\n \n       /* Compute address to load from and cast according to the size\n \t of the load.  */\n-      addr_expr = build_fold_addr_expr (unshare_expr (src));\n+      addr_expr = build_fold_addr_expr (src);\n       if (is_gimple_mem_ref_addr (addr_expr))\n-\taddr_tmp = addr_expr;\n+\taddr_tmp = unshare_expr (addr_expr);\n       else\n \t{\n-\t  addr_tmp = make_temp_ssa_name (TREE_TYPE (addr_expr), NULL,\n-\t\t\t\t\t \"load_src\");\n-\t  addr_stmt = gimple_build_assign (addr_tmp, addr_expr);\n-\t  gsi_insert_before (&gsi, addr_stmt, GSI_SAME_STMT);\n+\t  addr_tmp = unshare_expr (n->base_addr);\n+\t  if (!is_gimple_mem_ref_addr (addr_tmp))\n+\t    addr_tmp = force_gimple_operand_gsi_1 (&gsi, addr_tmp,\n+\t\t\t\t\t\t   is_gimple_mem_ref_addr,\n+\t\t\t\t\t\t   NULL_TREE, true,\n+\t\t\t\t\t\t   GSI_SAME_STMT);\n+\t  load_offset = n->bytepos;\n+\t  if (n->offset)\n+\t    {\n+\t      tree off\n+\t\t= force_gimple_operand_gsi (&gsi, unshare_expr (n->offset),\n+\t\t\t\t\t    true, NULL_TREE, true,\n+\t\t\t\t\t    GSI_SAME_STMT);\n+\t      gimple *stmt\n+\t\t= gimple_build_assign (make_ssa_name (TREE_TYPE (addr_tmp)),\n+\t\t\t\t       POINTER_PLUS_EXPR, addr_tmp, off);\n+\t      gsi_insert_before (&gsi, stmt, GSI_SAME_STMT);\n+\t      addr_tmp = gimple_assign_lhs (stmt);\n+\t    }\n \t}\n \n       /* Perform the load.  */\n@@ -967,21 +1002,29 @@ bswap_replace (gimple *cur_stmt, gimple *ins_stmt, tree fndecl,\n \t    }\n \n \t  /* Convert the result of load if necessary.  */\n-\t  if (!useless_type_conversion_p (TREE_TYPE (tgt), load_type))\n+\t  if (tgt && !useless_type_conversion_p (TREE_TYPE (tgt), load_type))\n \t    {\n \t      val_tmp = make_temp_ssa_name (aligned_load_type, NULL,\n \t\t\t\t\t    \"load_dst\");\n \t      load_stmt = gimple_build_assign (val_tmp, val_expr);\n \t      gimple_set_vuse (load_stmt, n->vuse);\n \t      gsi_insert_before (&gsi, load_stmt, GSI_SAME_STMT);\n \t      gimple_assign_set_rhs_with_ops (&gsi, NOP_EXPR, val_tmp);\n+\t      update_stmt (cur_stmt);\n \t    }\n-\t  else\n+\t  else if (cur_stmt)\n \t    {\n \t      gimple_assign_set_rhs_with_ops (&gsi, MEM_REF, val_expr);\n \t      gimple_set_vuse (cur_stmt, n->vuse);\n+\t      update_stmt (cur_stmt);\n+\t    }\n+\t  else\n+\t    {\n+\t      tgt = make_ssa_name (load_type);\n+\t      cur_stmt = gimple_build_assign (tgt, MEM_REF, val_expr);\n+\t      gimple_set_vuse (cur_stmt, n->vuse);\n+\t      gsi_insert_before (&gsi, cur_stmt, GSI_SAME_STMT);\n \t    }\n-\t  update_stmt (cur_stmt);\n \n \t  if (dump_file)\n \t    {\n@@ -990,7 +1033,7 @@ bswap_replace (gimple *cur_stmt, gimple *ins_stmt, tree fndecl,\n \t\t       (int) n->range);\n \t      print_gimple_stmt (dump_file, cur_stmt, 0);\n \t    }\n-\t  return true;\n+\t  return tgt;\n \t}\n       else\n \t{\n@@ -1003,15 +1046,17 @@ bswap_replace (gimple *cur_stmt, gimple *ins_stmt, tree fndecl,\n     }\n   else if (!bswap)\n     {\n-      gimple *g;\n-      if (!useless_type_conversion_p (TREE_TYPE (tgt), TREE_TYPE (src)))\n+      gimple *g = NULL;\n+      if (tgt && !useless_type_conversion_p (TREE_TYPE (tgt), TREE_TYPE (src)))\n \t{\n \t  if (!is_gimple_val (src))\n-\t    return false;\n+\t    return NULL_TREE;\n \t  g = gimple_build_assign (tgt, NOP_EXPR, src);\n \t}\n-      else\n+      else if (cur_stmt)\n \tg = gimple_build_assign (tgt, src);\n+      else\n+\ttgt = src;\n       if (n->range == 16)\n \tnop_stats.found_16bit++;\n       else if (n->range == 32)\n@@ -1026,10 +1071,17 @@ bswap_replace (gimple *cur_stmt, gimple *ins_stmt, tree fndecl,\n \t  fprintf (dump_file,\n \t\t   \"%d bit reshuffle in target endianness found at: \",\n \t\t   (int) n->range);\n-\t  print_gimple_stmt (dump_file, cur_stmt, 0);\n+\t  if (cur_stmt)\n+\t    print_gimple_stmt (dump_file, cur_stmt, 0);\n+\t  else\n+\t    {\n+\t      print_generic_expr (dump_file, tgt, 0);\n+\t      fprintf (dump_file, \"\\n\");\n+\t    }\n \t}\n-      gsi_replace (&gsi, g, true);\n-      return true;\n+      if (cur_stmt)\n+\tgsi_replace (&gsi, g, true);\n+      return tgt;\n     }\n   else if (TREE_CODE (src) == BIT_FIELD_REF)\n     src = TREE_OPERAND (src, 0);\n@@ -1069,6 +1121,8 @@ bswap_replace (gimple *cur_stmt, gimple *ins_stmt, tree fndecl,\n   else\n     bswap_stmt = gimple_build_call (fndecl, 1, tmp);\n \n+  if (tgt == NULL_TREE)\n+    tgt = make_ssa_name (bswap_type);\n   tmp = tgt;\n \n   /* Convert the result if necessary.  */\n@@ -1087,12 +1141,23 @@ bswap_replace (gimple *cur_stmt, gimple *ins_stmt, tree fndecl,\n     {\n       fprintf (dump_file, \"%d bit bswap implementation found at: \",\n \t       (int) n->range);\n-      print_gimple_stmt (dump_file, cur_stmt, 0);\n+      if (cur_stmt)\n+\tprint_gimple_stmt (dump_file, cur_stmt, 0);\n+      else\n+\t{\n+\t  print_generic_expr (dump_file, tgt, 0);\n+\t  fprintf (dump_file, \"\\n\");\n+\t}\n     }\n \n-  gsi_insert_after (&gsi, bswap_stmt, GSI_SAME_STMT);\n-  gsi_remove (&gsi, true);\n-  return true;\n+  if (cur_stmt)\n+    {\n+      gsi_insert_after (&gsi, bswap_stmt, GSI_SAME_STMT);\n+      gsi_remove (&gsi, true);\n+    }\n+  else\n+    gsi_insert_before (&gsi, bswap_stmt, GSI_SAME_STMT);\n+  return tgt;\n }\n \n /* Find manual byte swap implementations as well as load in a given\n@@ -1211,8 +1276,8 @@ pass_optimize_bswap::execute (function *fun)\n \t  if (bswap && !fndecl && n.range != 16)\n \t    continue;\n \n-\t  if (bswap_replace (cur_stmt, ins_stmt, fndecl, bswap_type, load_type,\n-\t\t\t     &n, bswap))\n+\t  if (bswap_replace (gsi_for_stmt (cur_stmt), ins_stmt, fndecl,\n+\t\t\t     bswap_type, load_type, &n, bswap))\n \t    changed = true;\n \t}\n     }\n@@ -1281,8 +1346,15 @@ struct store_immediate_info\n   gimple *stmt;\n   unsigned int order;\n   /* INTEGER_CST for constant stores, MEM_REF for memory copy or\n-     BIT_*_EXPR for logical bitwise operation.  */\n+     BIT_*_EXPR for logical bitwise operation.\n+     LROTATE_EXPR if it can be only bswap optimized and\n+     ops are not really meaningful.\n+     NOP_EXPR if bswap optimization detected identity, ops\n+     are not meaningful.  */\n   enum tree_code rhs_code;\n+  /* Two fields for bswap optimization purposes.  */\n+  struct symbolic_number n;\n+  gimple *ins_stmt;\n   /* True if BIT_{AND,IOR,XOR}_EXPR result is inverted before storing.  */\n   bool bit_not_p;\n   /* True if ops have been swapped and thus ops[1] represents\n@@ -1293,7 +1365,8 @@ struct store_immediate_info\n   store_operand_info ops[2];\n   store_immediate_info (unsigned HOST_WIDE_INT, unsigned HOST_WIDE_INT,\n \t\t\tunsigned HOST_WIDE_INT, unsigned HOST_WIDE_INT,\n-\t\t\tgimple *, unsigned int, enum tree_code, bool,\n+\t\t\tgimple *, unsigned int, enum tree_code,\n+\t\t\tstruct symbolic_number &, gimple *, bool,\n \t\t\tconst store_operand_info &,\n \t\t\tconst store_operand_info &);\n };\n@@ -1305,12 +1378,14 @@ store_immediate_info::store_immediate_info (unsigned HOST_WIDE_INT bs,\n \t\t\t\t\t    gimple *st,\n \t\t\t\t\t    unsigned int ord,\n \t\t\t\t\t    enum tree_code rhscode,\n+\t\t\t\t\t    struct symbolic_number &nr,\n+\t\t\t\t\t    gimple *ins_stmtp,\n \t\t\t\t\t    bool bitnotp,\n \t\t\t\t\t    const store_operand_info &op0r,\n \t\t\t\t\t    const store_operand_info &op1r)\n   : bitsize (bs), bitpos (bp), bitregion_start (brs), bitregion_end (bre),\n-    stmt (st), order (ord), rhs_code (rhscode), bit_not_p (bitnotp),\n-    ops_swapped_p (false)\n+    stmt (st), order (ord), rhs_code (rhscode), n (nr),\n+    ins_stmt (ins_stmtp), bit_not_p (bitnotp), ops_swapped_p (false)\n #if __cplusplus >= 201103L\n     , ops { op0r, op1r }\n {\n@@ -1884,13 +1959,13 @@ merged_store_group::apply_stores ()\n \t      fprintf (dump_file, \"After writing \");\n \t      print_generic_expr (dump_file, cst, 0);\n \t      fprintf (dump_file, \" of size \" HOST_WIDE_INT_PRINT_DEC\n-\t\t\t\" at position %d the merged region contains:\\n\",\n-\t\t\tinfo->bitsize, pos_in_buffer);\n+\t\t       \" at position %d the merged region contains:\\n\",\n+\t\t       info->bitsize, pos_in_buffer);\n \t      dump_char_array (dump_file, val, buf_size);\n \t    }\n \t  else\n \t    fprintf (dump_file, \"Failed to merge stores\\n\");\n-        }\n+\t}\n       if (!ret)\n \treturn false;\n       unsigned char *m = mask + (pos_in_buffer / BITS_PER_UNIT);\n@@ -1901,6 +1976,7 @@ merged_store_group::apply_stores ()\n       else\n \tclear_bit_region (m, pos_in_buffer % BITS_PER_UNIT, info->bitsize);\n     }\n+  stores.qsort (sort_by_bitpos);\n   return true;\n }\n \n@@ -1937,6 +2013,7 @@ struct imm_store_chain_info\n       }\n   }\n   bool terminate_and_process_chain ();\n+  bool try_coalesce_bswap (merged_store_group *, unsigned int, unsigned int);\n   bool coalesce_immediate_stores ();\n   bool output_merged_store (merged_store_group *);\n   bool output_merged_stores ();\n@@ -2075,7 +2152,8 @@ pass_store_merging::terminate_and_release_chain (imm_store_chain_info *chain_inf\n \n /* Return true if stmts in between FIRST (inclusive) and LAST (exclusive)\n    may clobber REF.  FIRST and LAST must be in the same basic block and\n-   have non-NULL vdef.  */\n+   have non-NULL vdef.  We want to be able to sink load of REF across\n+   stores between FIRST and LAST, up to right before LAST.  */\n \n bool\n stmts_may_clobber_ref_p (gimple *first, gimple *last, tree ref)\n@@ -2092,6 +2170,9 @@ stmts_may_clobber_ref_p (gimple *first, gimple *last, tree ref)\n       stmt = SSA_NAME_DEF_STMT (vop);\n       if (stmt_may_clobber_ref_p_1 (stmt, &r))\n \treturn true;\n+      if (gimple_store_p (stmt)\n+\t  && refs_anti_dependent_p (ref, gimple_get_lhs (stmt)))\n+\treturn true;\n       /* Avoid quadratic compile time by bounding the number of checks\n \t we perform.  */\n       if (++count > MAX_STORE_ALIAS_CHECKS)\n@@ -2192,6 +2273,252 @@ compatible_load_p (merged_store_group *merged_store,\n   return true;\n }\n \n+/* Add all refs loaded to compute VAL to REFS vector.  */\n+\n+void\n+gather_bswap_load_refs (vec<tree> *refs, tree val)\n+{\n+  if (TREE_CODE (val) != SSA_NAME)\n+    return;\n+\n+  gimple *stmt = SSA_NAME_DEF_STMT (val);\n+  if (!is_gimple_assign (stmt))\n+    return;\n+\n+  if (gimple_assign_load_p (stmt))\n+    {\n+      refs->safe_push (gimple_assign_rhs1 (stmt));\n+      return;\n+    }\n+\n+  switch (gimple_assign_rhs_class (stmt))\n+    {\n+    case GIMPLE_BINARY_RHS:\n+      gather_bswap_load_refs (refs, gimple_assign_rhs2 (stmt));\n+      /* FALLTHRU */\n+    case GIMPLE_UNARY_RHS:\n+      gather_bswap_load_refs (refs, gimple_assign_rhs1 (stmt));\n+      break;\n+    default:\n+      gcc_unreachable ();\n+    }\n+}\n+\n+/* Return true if m_store_info[first] and at least one following store\n+   form a group which store try_size bitsize value which is byte swapped\n+   from a memory load or some value, or identity from some value.\n+   This uses the bswap pass APIs.  */\n+\n+bool\n+imm_store_chain_info::try_coalesce_bswap (merged_store_group *merged_store,\n+\t\t\t\t\t  unsigned int first,\n+\t\t\t\t\t  unsigned int try_size)\n+{\n+  unsigned int len = m_store_info.length (), last = first;\n+  unsigned HOST_WIDE_INT width = m_store_info[first]->bitsize;\n+  if (width >= try_size)\n+    return false;\n+  for (unsigned int i = first + 1; i < len; ++i)\n+    {\n+      if (m_store_info[i]->bitpos != m_store_info[first]->bitpos + width\n+\t  || m_store_info[i]->ins_stmt == NULL)\n+\treturn false;\n+      width += m_store_info[i]->bitsize;\n+      if (width >= try_size)\n+\t{\n+\t  last = i;\n+\t  break;\n+\t}\n+    }\n+  if (width != try_size)\n+    return false;\n+\n+  bool allow_unaligned\n+    = !STRICT_ALIGNMENT && PARAM_VALUE (PARAM_STORE_MERGING_ALLOW_UNALIGNED);\n+  /* Punt if the combined store would not be aligned and we need alignment.  */\n+  if (!allow_unaligned)\n+    {\n+      unsigned int align = merged_store->align;\n+      unsigned HOST_WIDE_INT align_base = merged_store->align_base;\n+      for (unsigned int i = first + 1; i <= last; ++i)\n+\t{\n+\t  unsigned int this_align;\n+\t  unsigned HOST_WIDE_INT align_bitpos = 0;\n+\t  get_object_alignment_1 (gimple_assign_lhs (m_store_info[i]->stmt),\n+\t\t\t\t  &this_align, &align_bitpos);\n+\t  if (this_align > align)\n+\t    {\n+\t      align = this_align;\n+\t      align_base = m_store_info[i]->bitpos - align_bitpos;\n+\t    }\n+\t}\n+      unsigned HOST_WIDE_INT align_bitpos\n+\t= (m_store_info[first]->bitpos - align_base) & (align - 1);\n+      if (align_bitpos)\n+\talign = least_bit_hwi (align_bitpos);\n+      if (align < try_size)\n+\treturn false;\n+    }\n+\n+  tree type;\n+  switch (try_size)\n+    {\n+    case 16: type = uint16_type_node; break;\n+    case 32: type = uint32_type_node; break;\n+    case 64: type = uint64_type_node; break;\n+    default: gcc_unreachable ();\n+    }\n+  struct symbolic_number n;\n+  gimple *ins_stmt = NULL;\n+  int vuse_store = -1;\n+  unsigned int first_order = merged_store->first_order;\n+  unsigned int last_order = merged_store->last_order;\n+  gimple *first_stmt = merged_store->first_stmt;\n+  gimple *last_stmt = merged_store->last_stmt;\n+  store_immediate_info *infof = m_store_info[first];\n+\n+  for (unsigned int i = first; i <= last; ++i)\n+    {\n+      store_immediate_info *info = m_store_info[i];\n+      struct symbolic_number this_n = info->n;\n+      this_n.type = type;\n+      if (!this_n.base_addr)\n+\tthis_n.range = try_size / BITS_PER_UNIT;\n+      unsigned int bitpos = info->bitpos - infof->bitpos;\n+      if (!do_shift_rotate (LSHIFT_EXPR, &this_n,\n+\t\t\t    BYTES_BIG_ENDIAN\n+\t\t\t    ? try_size - info->bitsize - bitpos\n+\t\t\t    : bitpos))\n+\treturn false;\n+      if (n.base_addr && vuse_store)\n+\t{\n+\t  unsigned int j;\n+\t  for (j = first; j <= last; ++j)\n+\t    if (this_n.vuse == gimple_vuse (m_store_info[j]->stmt))\n+\t      break;\n+\t  if (j > last)\n+\t    {\n+\t      if (vuse_store == 1)\n+\t\treturn false;\n+\t      vuse_store = 0;\n+\t    }\n+\t}\n+      if (i == first)\n+\t{\n+\t  n = this_n;\n+\t  ins_stmt = info->ins_stmt;\n+\t}\n+      else\n+\t{\n+\t  if (n.base_addr)\n+\t    {\n+\t      if (n.vuse != this_n.vuse)\n+\t\t{\n+\t\t  if (vuse_store == 0)\n+\t\t    return false;\n+\t\t  vuse_store = 1;\n+\t\t}\n+\t      if (info->order > last_order)\n+\t\t{\n+\t\t  last_order = info->order;\n+\t\t  last_stmt = info->stmt;\n+\t\t}\n+\t      else if (info->order < first_order)\n+\t\t{\n+\t\t  first_order = info->order;\n+\t\t  first_stmt = info->stmt;\n+\t\t}\n+\t    }\n+\n+\t  ins_stmt = perform_symbolic_merge (ins_stmt, &n, info->ins_stmt,\n+\t\t\t\t\t     &this_n, &n);\n+\t  if (ins_stmt == NULL)\n+\t    return false;\n+\t}\n+    }\n+\n+  uint64_t cmpxchg, cmpnop;\n+  find_bswap_or_nop_finalize (&n, &cmpxchg, &cmpnop);\n+\n+  /* A complete byte swap should make the symbolic number to start with\n+     the largest digit in the highest order byte.  Unchanged symbolic\n+     number indicates a read with same endianness as target architecture.  */\n+  if (n.n != cmpnop && n.n != cmpxchg)\n+    return false;\n+\n+  if (n.base_addr == NULL_TREE && !is_gimple_val (n.src))\n+    return false;\n+\n+  /* Don't handle memory copy this way if normal non-bswap processing\n+     would handle it too.  */\n+  if (n.n == cmpnop && (unsigned) n.n_ops == last - first + 1)\n+    {\n+      unsigned int i;\n+      for (i = first; i <= last; ++i)\n+\tif (m_store_info[i]->rhs_code != MEM_REF)\n+\t  break;\n+      if (i == last + 1)\n+\treturn false;\n+    }\n+\n+  if (n.n == cmpxchg)\n+    switch (try_size)\n+      {\n+      case 16:\n+\t/* Will emit LROTATE_EXPR.  */\n+\tbreak;\n+      case 32:\n+\tif (builtin_decl_explicit_p (BUILT_IN_BSWAP32)\n+\t    && optab_handler (bswap_optab, SImode) != CODE_FOR_nothing)\n+\t  break;\n+\treturn false;\n+      case 64:\n+\tif (builtin_decl_explicit_p (BUILT_IN_BSWAP64)\n+\t    && optab_handler (bswap_optab, DImode) != CODE_FOR_nothing)\n+\t  break;\n+\treturn false;\n+      default:\n+\tgcc_unreachable ();\n+      }\n+\n+  if (!allow_unaligned && n.base_addr)\n+    {\n+      unsigned int align = get_object_alignment (n.src);\n+      if (align < try_size)\n+\treturn false;\n+    }\n+\n+  /* If each load has vuse of the corresponding store, need to verify\n+     the loads can be sunk right before the last store.  */\n+  if (vuse_store == 1)\n+    {\n+      auto_vec<tree, 64> refs;\n+      for (unsigned int i = first; i <= last; ++i)\n+\tgather_bswap_load_refs (&refs,\n+\t\t\t\tgimple_assign_rhs1 (m_store_info[i]->stmt));\n+\n+      unsigned int i;\n+      tree ref;\n+      FOR_EACH_VEC_ELT (refs, i, ref)\n+\tif (stmts_may_clobber_ref_p (first_stmt, last_stmt, ref))\n+\t  return false;\n+      n.vuse = NULL_TREE;\n+    }\n+\n+  infof->n = n;\n+  infof->ins_stmt = ins_stmt;\n+  for (unsigned int i = first; i <= last; ++i)\n+    {\n+      m_store_info[i]->rhs_code = n.n == cmpxchg ? LROTATE_EXPR : NOP_EXPR;\n+      m_store_info[i]->ops[0].base_addr = NULL_TREE;\n+      m_store_info[i]->ops[1].base_addr = NULL_TREE;\n+      if (i != first)\n+\tmerged_store->merge_into (m_store_info[i]);\n+    }\n+\n+  return true;\n+}\n+\n /* Go through the candidate stores recorded in m_store_info and merge them\n    into merged_store_group objects recorded into m_merged_store_groups\n    representing the widened stores.  Return true if coalescing was successful\n@@ -2210,7 +2537,7 @@ imm_store_chain_info::coalesce_immediate_stores ()\n \t     m_store_info.length ());\n \n   store_immediate_info *info;\n-  unsigned int i;\n+  unsigned int i, ignore = 0;\n \n   /* Order the stores by the bitposition they write to.  */\n   m_store_info.qsort (sort_by_bitpos);\n@@ -2229,14 +2556,41 @@ imm_store_chain_info::coalesce_immediate_stores ()\n \t  fprintf (dump_file, \"\\n------------\\n\");\n \t}\n \n-      if (i == 0)\n+      if (i <= ignore)\n \tcontinue;\n \n+      /* First try to handle group of stores like:\n+\t p[0] = data >> 24;\n+\t p[1] = data >> 16;\n+\t p[2] = data >> 8;\n+\t p[3] = data;\n+\t using the bswap framework.  */\n+      if (info->bitpos == merged_store->start + merged_store->width\n+\t  && merged_store->stores.length () == 1\n+\t  && merged_store->stores[0]->ins_stmt != NULL\n+\t  && info->ins_stmt != NULL)\n+\t{\n+\t  unsigned int try_size;\n+\t  for (try_size = 64; try_size >= 16; try_size >>= 1)\n+\t    if (try_coalesce_bswap (merged_store, i - 1, try_size))\n+\t      break;\n+\n+\t  if (try_size >= 16)\n+\t    {\n+\t      ignore = i + merged_store->stores.length () - 1;\n+\t      m_merged_store_groups.safe_push (merged_store);\n+\t      if (ignore < m_store_info.length ())\n+\t\tmerged_store = new merged_store_group (m_store_info[ignore]);\n+\t      else\n+\t\tmerged_store = NULL;\n+\t      continue;\n+\t    }\n+\t}\n+\n       /* |---store 1---|\n \t       |---store 2---|\n-       Overlapping stores.  */\n-      unsigned HOST_WIDE_INT start = info->bitpos;\n-      if (IN_RANGE (start, merged_store->start,\n+\t Overlapping stores.  */\n+      if (IN_RANGE (info->bitpos, merged_store->start,\n \t\t    merged_store->start + merged_store->width - 1))\n \t{\n \t  /* Only allow overlapping stores of constants.  */\n@@ -2251,7 +2605,8 @@ imm_store_chain_info::coalesce_immediate_stores ()\n \t This store is consecutive to the previous one.\n \t Merge it into the current store group.  There can be gaps in between\n \t the stores, but there can't be gaps in between bitregions.  */\n-      else if (info->bitregion_start <= merged_store->bitregion_end\n+      else if (info->rhs_code != LROTATE_EXPR\n+\t       && info->bitregion_start <= merged_store->bitregion_end\n \t       && info->rhs_code == merged_store->stores[0]->rhs_code)\n \t{\n \t  store_immediate_info *infof = merged_store->stores[0];\n@@ -2297,10 +2652,13 @@ imm_store_chain_info::coalesce_immediate_stores ()\n     }\n \n   /* Record or discard the last store group.  */\n-  if (!merged_store->apply_stores ())\n-    delete merged_store;\n-  else\n-    m_merged_store_groups.safe_push (merged_store);\n+  if (merged_store)\n+    {\n+      if (!merged_store->apply_stores ())\n+\tdelete merged_store;\n+      else\n+\tm_merged_store_groups.safe_push (merged_store);\n+    }\n \n   gcc_assert (m_merged_store_groups.length () <= m_store_info.length ());\n   bool success\n@@ -2560,9 +2918,41 @@ split_group (merged_store_group *group, bool allow_unaligned_store,\n \n   gcc_assert ((size % BITS_PER_UNIT == 0) && (pos % BITS_PER_UNIT == 0));\n \n+  if (group->stores[0]->rhs_code == LROTATE_EXPR\n+      || group->stores[0]->rhs_code == NOP_EXPR)\n+    {\n+      /* For bswap framework using sets of stores, all the checking\n+\t has been done earlier in try_coalesce_bswap and needs to be\n+\t emitted as a single store.  */\n+      if (total_orig)\n+\t{\n+\t  /* Avoid the old/new stmt count heuristics.  It should be\n+\t     always beneficial.  */\n+\t  total_new[0] = 1;\n+\t  total_orig[0] = 2;\n+\t}\n+\n+      if (split_stores)\n+\t{\n+\t  unsigned HOST_WIDE_INT align_bitpos\n+\t    = (group->start - align_base) & (group_align - 1);\n+\t  unsigned HOST_WIDE_INT align = group_align;\n+\t  if (align_bitpos)\n+\t    align = least_bit_hwi (align_bitpos);\n+\t  bytepos = group->start / BITS_PER_UNIT;\n+\t  struct split_store *store\n+\t    = new split_store (bytepos, group->width, align);\n+\t  unsigned int first = 0;\n+\t  find_constituent_stores (group, &store->orig_stores,\n+\t\t\t\t   &first, group->start, group->width);\n+\t  split_stores->safe_push (store);\n+\t}\n+\n+      return 1;\n+    }\n+\n   unsigned int ret = 0, first = 0;\n   unsigned HOST_WIDE_INT try_pos = bytepos;\n-  group->stores.qsort (sort_by_bitpos);\n \n   if (total_orig)\n     {\n@@ -2904,20 +3294,71 @@ imm_store_chain_info::output_merged_store (merged_store_group *group)\n \t\t\t    \" not larger than estimated number of new\"\n \t\t\t    \" stmts (%u).\\n\",\n \t\t total_orig, total_new);\n+      return false;\n     }\n \n   gimple_stmt_iterator last_gsi = gsi_for_stmt (group->last_stmt);\n   gimple_seq seq = NULL;\n   tree last_vdef, new_vuse;\n   last_vdef = gimple_vdef (group->last_stmt);\n   new_vuse = gimple_vuse (group->last_stmt);\n+  tree bswap_res = NULL_TREE;\n+\n+  if (group->stores[0]->rhs_code == LROTATE_EXPR\n+      || group->stores[0]->rhs_code == NOP_EXPR)\n+    {\n+      tree fndecl = NULL_TREE, bswap_type = NULL_TREE, load_type;\n+      gimple *ins_stmt = group->stores[0]->ins_stmt;\n+      struct symbolic_number *n = &group->stores[0]->n;\n+      bool bswap = group->stores[0]->rhs_code == LROTATE_EXPR;\n+\n+      switch (n->range)\n+\t{\n+\tcase 16:\n+\t  load_type = bswap_type = uint16_type_node;\n+\t  break;\n+\tcase 32:\n+\t  load_type = uint32_type_node;\n+\t  if (bswap)\n+\t    {\n+\t      fndecl = builtin_decl_explicit (BUILT_IN_BSWAP32);\n+\t      bswap_type = TREE_VALUE (TYPE_ARG_TYPES (TREE_TYPE (fndecl)));\n+\t    }\n+\t  break;\n+\tcase 64:\n+\t  load_type = uint64_type_node;\n+\t  if (bswap)\n+\t    {\n+\t      fndecl = builtin_decl_explicit (BUILT_IN_BSWAP64);\n+\t      bswap_type = TREE_VALUE (TYPE_ARG_TYPES (TREE_TYPE (fndecl)));\n+\t    }\n+\t  break;\n+\tdefault:\n+\t  gcc_unreachable ();\n+\t}\n+\n+      /* If the loads have each vuse of the corresponding store,\n+\t we've checked the aliasing already in try_coalesce_bswap and\n+\t we want to sink the need load into seq.  So need to use new_vuse\n+\t on the load.  */\n+      if (n->base_addr && n->vuse == NULL)\n+\t{\n+\t  n->vuse = new_vuse;\n+\t  ins_stmt = NULL;\n+\t}\n+      bswap_res = bswap_replace (gsi_start (seq), ins_stmt, fndecl,\n+\t\t\t\t bswap_type, load_type, n, bswap);\n+      gcc_assert (bswap_res);\n+    }\n \n   gimple *stmt = NULL;\n   split_store *split_store;\n   unsigned int i;\n   auto_vec<gimple *, 32> orig_stmts;\n-  tree addr = force_gimple_operand_1 (unshare_expr (base_addr), &seq,\n+  gimple_seq this_seq;\n+  tree addr = force_gimple_operand_1 (unshare_expr (base_addr), &this_seq,\n \t\t\t\t      is_gimple_mem_ref_addr, NULL_TREE);\n+  gimple_seq_add_seq_without_update (&seq, this_seq);\n \n   tree load_addr[2] = { NULL_TREE, NULL_TREE };\n   gimple_seq load_seq[2] = { NULL, NULL };\n@@ -2941,7 +3382,6 @@ imm_store_chain_info::output_merged_store (merged_store_group *group)\n \tload_addr[j] = addr;\n       else\n \t{\n-\t  gimple_seq this_seq;\n \t  load_addr[j]\n \t    = force_gimple_operand_1 (unshare_expr (op.base_addr),\n \t\t\t\t      &this_seq, is_gimple_mem_ref_addr,\n@@ -2988,18 +3428,22 @@ imm_store_chain_info::output_merged_store (merged_store_group *group)\n \t      MR_DEPENDENCE_BASE (dest) = base;\n \t    }\n \n-\t  tree mask\n-\t    = native_interpret_expr (int_type,\n-\t\t\t\t     group->mask + try_pos - start_byte_pos,\n-\t\t\t\t     group->buf_size);\n+\t  tree mask = integer_zero_node;\n+\t  if (!bswap_res)\n+\t    mask = native_interpret_expr (int_type,\n+\t\t\t\t\t  group->mask + try_pos\n+\t\t\t\t\t  - start_byte_pos,\n+\t\t\t\t\t  group->buf_size);\n \n \t  tree ops[2];\n \t  for (int j = 0;\n \t       j < 1 + (split_store->orig_stores[0]->ops[1].val != NULL_TREE);\n \t       ++j)\n \t    {\n \t      store_operand_info &op = split_store->orig_stores[0]->ops[j];\n-\t      if (op.base_addr)\n+\t      if (bswap_res)\n+\t\tops[j] = bswap_res;\n+\t      else if (op.base_addr)\n \t\t{\n \t\t  FOR_EACH_VEC_ELT (split_store->orig_stores, k, info)\n \t\t    orig_stmts.safe_push (info->ops[j].stmt);\n@@ -3124,6 +3568,24 @@ imm_store_chain_info::output_merged_store (merged_store_group *group)\n \t\t  src = gimple_assign_lhs (stmt);\n \t\t}\n \t      break;\n+\t    case LROTATE_EXPR:\n+\t    case NOP_EXPR:\n+\t      src = ops[0];\n+\t      if (!is_gimple_val (src))\n+\t\t{\n+\t\t  stmt = gimple_build_assign (make_ssa_name (TREE_TYPE (src)),\n+\t\t\t\t\t      src);\n+\t\t  gimple_seq_add_stmt_without_update (&seq, stmt);\n+\t\t  src = gimple_assign_lhs (stmt);\n+\t\t}\n+\t      if (!useless_type_conversion_p (int_type, TREE_TYPE (src)))\n+\t\t{\n+\t\t  stmt = gimple_build_assign (make_ssa_name (int_type),\n+\t\t\t\t\t      NOP_EXPR, src);\n+\t\t  gimple_seq_add_stmt_without_update (&seq, stmt);\n+\t\t  src = gimple_assign_lhs (stmt);\n+\t\t}\n+\t      break;\n \t    default:\n \t      src = ops[0];\n \t      break;\n@@ -3494,6 +3956,8 @@ pass_store_merging::process_store (gimple *stmt)\n \t\t       && (TREE_CODE (rhs) != INTEGER_CST)));\n   enum tree_code rhs_code = ERROR_MARK;\n   bool bit_not_p = false;\n+  struct symbolic_number n;\n+  gimple *ins_stmt = NULL;\n   store_operand_info ops[2];\n   if (invalid)\n     ;\n@@ -3558,6 +4022,35 @@ pass_store_merging::process_store (gimple *stmt)\n \t    invalid = true;\n \t    break;\n \t  }\n+      if ((bitsize % BITS_PER_UNIT) == 0\n+\t  && (bitpos % BITS_PER_UNIT) == 0\n+\t  && bitsize <= 64\n+\t  && BYTES_BIG_ENDIAN == WORDS_BIG_ENDIAN)\n+\t{\n+\t  ins_stmt = find_bswap_or_nop_1 (def_stmt, &n, 12);\n+\t  if (ins_stmt)\n+\t    {\n+\t      uint64_t nn = n.n;\n+\t      for (unsigned HOST_WIDE_INT i = 0;\n+\t\t   i < bitsize; i += BITS_PER_UNIT, nn >>= BITS_PER_MARKER)\n+\t\tif ((nn & MARKER_MASK) == 0\n+\t\t    || (nn & MARKER_MASK) == MARKER_BYTE_UNKNOWN)\n+\t\t  {\n+\t\t    ins_stmt = NULL;\n+\t\t    break;\n+\t\t  }\n+\t      if (ins_stmt)\n+\t\t{\n+\t\t  if (invalid)\n+\t\t    {\n+\t\t      rhs_code = LROTATE_EXPR;\n+\t\t      ops[0].base_addr = NULL_TREE;\n+\t\t      ops[1].base_addr = NULL_TREE;\n+\t\t    }\n+\t\t  invalid = false;\n+\t\t}\n+\t    }\n+\t}\n     }\n \n   if (invalid)\n@@ -3566,6 +4059,9 @@ pass_store_merging::process_store (gimple *stmt)\n       return;\n     }\n \n+  if (!ins_stmt)\n+    memset (&n, 0, sizeof (n));\n+\n   struct imm_store_chain_info **chain_info = NULL;\n   if (base_addr)\n     chain_info = m_stores.get (base_addr);\n@@ -3576,6 +4072,7 @@ pass_store_merging::process_store (gimple *stmt)\n       unsigned int ord = (*chain_info)->m_store_info.length ();\n       info = new store_immediate_info (bitsize, bitpos, bitregion_start,\n \t\t\t\t       bitregion_end, stmt, ord, rhs_code,\n+\t\t\t\t       n, ins_stmt,\n \t\t\t\t       bit_not_p, ops[0], ops[1]);\n       if (dump_file && (dump_flags & TDF_DETAILS))\n \t{\n@@ -3604,6 +4101,7 @@ pass_store_merging::process_store (gimple *stmt)\n     = new imm_store_chain_info (m_stores_head, base_addr);\n   info = new store_immediate_info (bitsize, bitpos, bitregion_start,\n \t\t\t\t   bitregion_end, stmt, 0, rhs_code,\n+\t\t\t\t   n, ins_stmt,\n \t\t\t\t   bit_not_p, ops[0], ops[1]);\n   new_chain->m_store_info.safe_push (info);\n   m_stores.put (base_addr, new_chain);\n@@ -3628,6 +4126,8 @@ pass_store_merging::execute (function *fun)\n   basic_block bb;\n   hash_set<gimple *> orig_stmts;\n \n+  calculate_dominance_info (CDI_DOMINATORS);\n+\n   FOR_EACH_BB_FN (bb, fun)\n     {\n       gimple_stmt_iterator gsi;"}, {"sha": "4cd2f15d4f134d335a915e2310e3cfe461fb41f1", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a", "patch": "@@ -1,3 +1,8 @@\n+2017-11-20  Jakub Jelinek  <jakub@redhat.com>\n+\n+\tPR tree-optimization/78821\n+\t* gcc.dg/store_merging_16.c: New test.\n+\n 2017-11-19  Jan Hubicka  <hubicka@ucw.cz>\n \n \tPR target/82281"}, {"sha": "e2101bc78b82f9866e154cd49eda3b16dda89e2a", "filename": "gcc/testsuite/gcc.dg/store_merging_16.c", "status": "added", "additions": 157, "deletions": 0, "changes": 157, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_16.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_16.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Fstore_merging_16.c?ref=4b84d9b8f9a6eecb67019ce1d9ba85559df66f9a", "patch": "@@ -0,0 +1,157 @@\n+/* Only test on some 64-bit targets which do have bswap{si,di}2 patterns and\n+   are either big or little endian (not pdp endian).  */\n+/* { dg-do compile { target { lp64 && { i?86-*-* x86_64-*-* powerpc*-*-* aarch64*-*-* } } } } */\n+/* { dg-require-effective-target store_merge } */\n+/* { dg-options \"-O2 -fdump-tree-store-merging\" } */\n+\n+__attribute__((noipa)) void\n+f1 (unsigned char *p, unsigned long long q)\n+{\n+  p[0] = q;\n+  p[1] = q >> 8;\n+  p[2] = q >> 16;\n+  p[3] = q >> 24;\n+  p[4] = q >> 32;\n+  p[5] = q >> 40;\n+  p[6] = q >> 48;\n+  p[7] = q >> 56;\n+}\n+\n+__attribute__((noipa)) void\n+f2 (unsigned char *p, unsigned long long q)\n+{\n+  p[0] = q >> 56;\n+  p[1] = q >> 48;\n+  p[2] = q >> 40;\n+  p[3] = q >> 32;\n+  p[4] = q >> 24;\n+  p[5] = q >> 16;\n+  p[6] = q >> 8;\n+  p[7] = q;\n+}\n+\n+__attribute__((noipa)) void\n+f3 (unsigned char *__restrict p, unsigned char *__restrict q)\n+{\n+  unsigned char q3 = q[3];\n+  unsigned char q2 = q[2];\n+  unsigned char q1 = q[1];\n+  unsigned char q0 = q[0];\n+  p[0] = q3;\n+  p[1] = q2;\n+  p[2] = q1;\n+  p[3] = q0;\n+}\n+\n+__attribute__((noipa)) void\n+f4 (unsigned char *__restrict p, unsigned char *__restrict q)\n+{\n+  p[0] = q[3];\n+  p[1] = q[2];\n+  p[2] = q[1];\n+  p[3] = q[0];\n+}\n+\n+struct S { unsigned char a, b; unsigned short c; };\n+\n+__attribute__((noipa)) void\n+f5 (struct S *__restrict p, struct S *__restrict q)\n+{\n+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n+  unsigned char pa = q->c >> 8;\n+  unsigned char pb = q->c;\n+  unsigned short pc = (q->a << 8) | q->b;\n+#elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n+  unsigned char pa = q->c;\n+  unsigned char pb = q->c >> 8;\n+  unsigned short pc = q->a | (q->b << 8);\n+#endif\n+  p->a = pa;\n+  p->b = pb;\n+  p->c = pc;\n+}\n+\n+__attribute__((noipa)) void\n+f6 (struct S *__restrict p, struct S *__restrict q)\n+{\n+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n+  p->a = q->c >> 8;\n+  p->b = q->c;\n+  p->c = (q->a << 8) | q->b;\n+#elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n+  p->a = q->c;\n+  p->b = q->c >> 8;\n+  p->c = q->a | (q->b << 8);\n+#endif\n+}\n+\n+struct T { unsigned long long a : 8, b : 8, c : 8, d : 8, e : 8, f : 8, g : 8, h : 8; };\n+\n+__attribute__((noipa)) void\n+f7 (struct T *__restrict p, struct T *__restrict q)\n+{\n+  p->a = q->h;\n+  p->b = q->g;\n+  p->c = q->f;\n+  p->d = q->e;\n+  p->e = q->d;\n+  p->f = q->c;\n+  p->g = q->b;\n+  p->h = q->a;\n+}\n+\n+struct S b = { 0x11, 0x12,\n+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n+\t       0x1413\n+#elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n+\t       0x1314\n+#endif\n+\t     };\n+struct T e = { 0x21, 0x22, 0x23, 0x24, 0x25, 0x26, 0x27, 0x28 };\n+\n+int\n+main ()\n+{\n+  unsigned char a[8];\n+  int i;\n+  struct S b, c, d;\n+  f1 (a, 0x0102030405060708ULL);\n+  for (i = 0; i < 8; ++i)\n+    if (a[i] != 8 - i)\n+      __builtin_abort ();\n+  f2 (a, 0x0102030405060708ULL);\n+  for (i = 0; i < 8; ++i)\n+    if (a[i] != 1 + i)\n+      __builtin_abort ();\n+  f3 (a, a + 4);\n+  for (i = 0; i < 8; ++i)\n+    if (a[i] != (i < 4 ? 8 - i : 1 + i))\n+      __builtin_abort ();\n+  f2 (a, 0x090a0b0c0d0e0f10ULL);\n+  f4 (a + 4, a);\n+  for (i = 0; i < 8; ++i)\n+    if (a[i] != (i < 4 ? 9 + i : 16 - i))\n+      __builtin_abort ();\n+  f5 (&c, &b);\n+  if (c.a != 0x14 || c.b != 0x13\n+#if __BYTE_ORDER__ == __ORDER_LITTLE_ENDIAN__\n+      || c.c != 0x1112\n+#elif __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n+      || c.c != 0x1211\n+#endif\n+      )\n+    __builtin_abort ();\n+  f6 (&d, &c);\n+  if (d.a != 0x11 || d.b != 0x12 || d.c != b.c)\n+    __builtin_abort ();\n+  struct T f;\n+  f7 (&f, &e);\n+  if (f.a != 0x28 || f.b != 0x27 || f.c != 0x26 || f.d != 0x25\n+      || f.e != 0x24 || f.f != 0x23 || f.g != 0x22 || f.h != 0x21)\n+    __builtin_abort ();\n+  return 0;\n+}\n+\n+/* { dg-final { scan-tree-dump-times \"Merging successful\" 7 \"store-merging\" } } */\n+/* { dg-final { scan-tree-dump-times \"__builtin_bswap64\" 2 \"store-merging\" } } */\n+/* { dg-final { scan-tree-dump-times \"__builtin_bswap32\" 4 \"store-merging\" } } */"}]}