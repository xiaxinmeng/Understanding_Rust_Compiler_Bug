{"sha": "3b924b0d7c0218956dbc2ce0ca2740e8923c2c4a", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6M2I5MjRiMGQ3YzAyMTg5NTZkYmMyY2UwY2EyNzQwZTg5MjNjMmM0YQ==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2021-03-26T16:08:35Z"}, "committer": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2021-03-26T16:08:34Z"}, "message": "aarch64: Try to detect when Advanced SIMD code would be completely unrolled\n\nGCC usually costs the SVE and Advanced SIMD versions of a loop\nand picks the one with the lowest cost.  By default it will choose\nSVE over Advanced SIMD in the event of tie.\n\nThis is normally the correct behaviour, not least because SVE can\nhandle every scalar iteration count whereas Advanced SIMD can only\nhandle full vectors.  However, there is one important exception\nthat GCC failed to consider: we can completely unroll Advanced SIMD\ncode at compile time, but we can't do the same for SVE.\n\nThis patch therefore adds an opt-in heuristic to guess whether\nthe Advanced SIMD version of a loop is likely to be unrolled.\nThis will only be suitable for some CPUs, so it is not enabled\nby default and is controlled separately from use_new_vector_costs.\n\nLike with previous patches, this one only becomes active if a\nCPU selects both of the new tuning parameters.  It should therefore\nhave a very low impact on other CPUs.\n\ngcc/\n\t* config/aarch64/aarch64-tuning-flags.def (matched_vector_throughput):\n\tNew tuning parameter.\n\t* config/aarch64/aarch64.c (neoversev1_tunings): Use it.\n\t(aarch64_estimated_sve_vq): New function.\n\t(aarch64_vector_costs::analyzed_vinfo): New member variable.\n\t(aarch64_vector_costs::is_loop): Likewise.\n\t(aarch64_vector_costs::unrolled_advsimd_niters): Likewise.\n\t(aarch64_vector_costs::unrolled_advsimd_stmts): Likewise.\n\t(aarch64_record_potential_advsimd_unrolling): New function.\n\t(aarch64_analyze_loop_vinfo, aarch64_analyze_bb_vinfo): Likewise.\n\t(aarch64_add_stmt_cost): Call aarch64_analyze_loop_vinfo or\n\taarch64_analyze_bb_vinfo on the first use of a costs structure.\n\tDetect whether we're vectorizing a loop for SVE that might be\n\tcompletely unrolled if it used Advanced SIMD instead.\n\t(aarch64_adjust_body_cost_for_latency): New function.\n\t(aarch64_finish_cost): Call it.", "tree": {"sha": "9c4cb4243d0e29cc5640034a5e97904ceca5e6a8", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/9c4cb4243d0e29cc5640034a5e97904ceca5e6a8"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/3b924b0d7c0218956dbc2ce0ca2740e8923c2c4a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3b924b0d7c0218956dbc2ce0ca2740e8923c2c4a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/3b924b0d7c0218956dbc2ce0ca2740e8923c2c4a", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3b924b0d7c0218956dbc2ce0ca2740e8923c2c4a/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "50a525b50c912999073a78220c6d62d87946b579", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/50a525b50c912999073a78220c6d62d87946b579", "html_url": "https://github.com/Rust-GCC/gccrs/commit/50a525b50c912999073a78220c6d62d87946b579"}], "stats": {"total": 217, "additions": 210, "deletions": 7}, "files": [{"sha": "65b4c37d652268a674a265ef5f577a7618459487", "filename": "gcc/config/aarch64/aarch64-tuning-flags.def", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3b924b0d7c0218956dbc2ce0ca2740e8923c2c4a/gcc%2Fconfig%2Faarch64%2Faarch64-tuning-flags.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3b924b0d7c0218956dbc2ce0ca2740e8923c2c4a/gcc%2Fconfig%2Faarch64%2Faarch64-tuning-flags.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-tuning-flags.def?ref=3b924b0d7c0218956dbc2ce0ca2740e8923c2c4a", "patch": "@@ -50,4 +50,6 @@ AARCH64_EXTRA_TUNING_OPTION (\"cse_sve_vl_constants\", CSE_SVE_VL_CONSTANTS)\n \n AARCH64_EXTRA_TUNING_OPTION (\"use_new_vector_costs\", USE_NEW_VECTOR_COSTS)\n \n+AARCH64_EXTRA_TUNING_OPTION (\"matched_vector_throughput\", MATCHED_VECTOR_THROUGHPUT)\n+\n #undef AARCH64_EXTRA_TUNING_OPTION"}, {"sha": "63750e3886242983bb8fc10741d3958d18cd44a1", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 208, "deletions": 7, "changes": 215, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3b924b0d7c0218956dbc2ce0ca2740e8923c2c4a/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3b924b0d7c0218956dbc2ce0ca2740e8923c2c4a/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=3b924b0d7c0218956dbc2ce0ca2740e8923c2c4a", "patch": "@@ -1732,7 +1732,8 @@ static const struct tune_params neoversev1_tunings =\n   0,\t/* max_case_values.  */\n   tune_params::AUTOPREFETCHER_WEAK,\t/* autoprefetcher_model.  */\n   (AARCH64_EXTRA_TUNE_CSE_SVE_VL_CONSTANTS\n-   | AARCH64_EXTRA_TUNE_USE_NEW_VECTOR_COSTS),\t/* tune_flags.  */\n+   | AARCH64_EXTRA_TUNE_USE_NEW_VECTOR_COSTS\n+   | AARCH64_EXTRA_TUNE_MATCHED_VECTOR_THROUGHPUT),\t/* tune_flags.  */\n   &generic_prefetch_tune\n };\n \n@@ -2539,6 +2540,14 @@ aarch64_bit_representation (rtx x)\n   return x;\n }\n \n+/* Return an estimate for the number of quadwords in an SVE vector.  This is\n+   equivalent to the number of Advanced SIMD vectors in an SVE vector.  */\n+static unsigned int\n+aarch64_estimated_sve_vq ()\n+{\n+  return estimated_poly_value (BITS_PER_SVE_VECTOR) / 128;\n+}\n+\n /* Return true if MODE is any of the Advanced SIMD structure modes.  */\n static bool\n aarch64_advsimd_struct_mode_p (machine_mode mode)\n@@ -14117,6 +14126,39 @@ struct aarch64_vector_costs\n   /* The normal latency-based costs for each region (prologue, body and\n      epilogue), indexed by vect_cost_model_location.  */\n   unsigned int region[3] = {};\n+\n+  /* True if we have performed one-time initialization based on the vec_info.\n+\n+     This variable exists because the vec_info is not passed to the\n+     init_cost hook.  We therefore have to defer initialization based on\n+     it till later.  */\n+  bool analyzed_vinfo = false;\n+\n+  /* True if we're costing a vector loop, false if we're costing block-level\n+     vectorization.  */\n+  bool is_loop = false;\n+\n+  /* - If VEC_FLAGS is zero then we're costing the original scalar code.\n+     - If VEC_FLAGS & VEC_ADVSIMD is nonzero then we're costing Advanced\n+       SIMD code.\n+     - If VEC_FLAGS & VEC_ANY_SVE is nonzero then we're costing SVE code.  */\n+  unsigned int vec_flags = 0;\n+\n+  /* On some CPUs, SVE and Advanced SIMD provide the same theoretical vector\n+     throughput, such as 4x128 Advanced SIMD vs. 2x256 SVE.  In those\n+     situations, we try to predict whether an Advanced SIMD implementation\n+     of the loop could be completely unrolled and become straight-line code.\n+     If so, it is generally better to use the Advanced SIMD version rather\n+     than length-agnostic SVE, since the SVE loop would execute an unknown\n+     number of times and so could not be completely unrolled in the same way.\n+\n+     If we're applying this heuristic, UNROLLED_ADVSIMD_NITERS is the\n+     number of Advanced SIMD loop iterations that would be unrolled and\n+     UNROLLED_ADVSIMD_STMTS estimates the total number of statements\n+     in the unrolled loop.  Both values are zero if we're not applying\n+     the heuristic.  */\n+  unsigned HOST_WIDE_INT unrolled_advsimd_niters = 0;\n+  unsigned HOST_WIDE_INT unrolled_advsimd_stmts = 0;\n };\n \n /* Implement TARGET_VECTORIZE_INIT_COST.  */\n@@ -14148,6 +14190,94 @@ aarch64_simd_vec_costs (tree vectype)\n   return costs->advsimd;\n }\n \n+/* Decide whether to use the unrolling heuristic described above\n+   aarch64_vector_costs::unrolled_advsimd_niters, updating that\n+   field if so.  LOOP_VINFO describes the loop that we're vectorizing\n+   and COSTS are the costs that we're calculating for it.  */\n+static void\n+aarch64_record_potential_advsimd_unrolling (loop_vec_info loop_vinfo,\n+\t\t\t\t\t    aarch64_vector_costs *costs)\n+{\n+  /* The heuristic only makes sense on targets that have the same\n+     vector throughput for SVE and Advanced SIMD.  */\n+  if (!(aarch64_tune_params.extra_tuning_flags\n+\t& AARCH64_EXTRA_TUNE_MATCHED_VECTOR_THROUGHPUT))\n+    return;\n+\n+  /* We only want to apply the heuristic if LOOP_VINFO is being\n+     vectorized for SVE.  */\n+  if (!(costs->vec_flags & VEC_ANY_SVE))\n+    return;\n+\n+  /* Check whether it is possible in principle to use Advanced SIMD\n+     instead.  */\n+  if (aarch64_autovec_preference == 2)\n+    return;\n+\n+  /* We don't want to apply the heuristic to outer loops, since it's\n+     harder to track two levels of unrolling.  */\n+  if (LOOP_VINFO_LOOP (loop_vinfo)->inner)\n+    return;\n+\n+  /* Only handle cases in which the number of Advanced SIMD iterations\n+     would be known at compile time but the number of SVE iterations\n+     would not.  */\n+  if (!LOOP_VINFO_NITERS_KNOWN_P (loop_vinfo)\n+      || aarch64_sve_vg.is_constant ())\n+    return;\n+\n+  /* Guess how many times the Advanced SIMD loop would iterate and make\n+     sure that it is within the complete unrolling limit.  Even if the\n+     number of iterations is small enough, the number of statements might\n+     not be, which is why we need to estimate the number of statements too.  */\n+  unsigned int estimated_vq = aarch64_estimated_sve_vq ();\n+  unsigned int advsimd_vf = CEIL (vect_vf_for_cost (loop_vinfo), estimated_vq);\n+  unsigned HOST_WIDE_INT unrolled_advsimd_niters\n+    = LOOP_VINFO_INT_NITERS (loop_vinfo) / advsimd_vf;\n+  if (unrolled_advsimd_niters > (unsigned int) param_max_completely_peel_times)\n+    return;\n+\n+  /* Record that we're applying the heuristic and should try to estimate\n+     the number of statements in the Advanced SIMD loop.  */\n+  costs->unrolled_advsimd_niters = unrolled_advsimd_niters;\n+}\n+\n+/* Do one-time initialization of COSTS given that we're costing the loop\n+   vectorization described by LOOP_VINFO.  */\n+static void\n+aarch64_analyze_loop_vinfo (loop_vec_info loop_vinfo,\n+\t\t\t    aarch64_vector_costs *costs)\n+{\n+  costs->is_loop = true;\n+\n+  /* Detect whether we're costing the scalar code or the vector code.\n+     This is a bit hacky: it would be better if the vectorizer told\n+     us directly.\n+\n+     If we're costing the vector code, record whether we're vectorizing\n+     for Advanced SIMD or SVE.  */\n+  if (costs == LOOP_VINFO_TARGET_COST_DATA (loop_vinfo))\n+    costs->vec_flags = aarch64_classify_vector_mode (loop_vinfo->vector_mode);\n+  else\n+    costs->vec_flags = 0;\n+\n+  /* Detect whether we're vectorizing for SVE and should\n+     apply the unrolling heuristic described above\n+     aarch64_vector_costs::unrolled_advsimd_niters.  */\n+  aarch64_record_potential_advsimd_unrolling (loop_vinfo, costs);\n+}\n+\n+/* Do one-time initialization of COSTS given that we're costing the block\n+   vectorization described by BB_VINFO.  */\n+static void\n+aarch64_analyze_bb_vinfo (bb_vec_info bb_vinfo, aarch64_vector_costs *costs)\n+{\n+  /* Unfortunately, there's no easy way of telling whether we're costing\n+     the vector code or the scalar code, so just assume that we're costing\n+     the vector code.  */\n+  costs->vec_flags = aarch64_classify_vector_mode (bb_vinfo->vector_mode);\n+}\n+\n /* Implement targetm.vectorize.builtin_vectorization_cost.  */\n static int\n aarch64_builtin_vectorization_cost (enum vect_cost_for_stmt type_of_cost,\n@@ -14555,8 +14685,20 @@ aarch64_add_stmt_cost (class vec_info *vinfo, void *data, int count,\n \n   if (flag_vect_cost_model)\n     {\n-      int stmt_cost =\n-\t    aarch64_builtin_vectorization_cost (kind, vectype, misalign);\n+      int stmt_cost\n+\t= aarch64_builtin_vectorization_cost (kind, vectype, misalign);\n+\n+      /* Do one-time initialization based on the vinfo.  */\n+      loop_vec_info loop_vinfo = dyn_cast<loop_vec_info> (vinfo);\n+      bb_vec_info bb_vinfo = dyn_cast<bb_vec_info> (vinfo);\n+      if (!costs->analyzed_vinfo && aarch64_use_new_vector_costs_p ())\n+\t{\n+\t  if (loop_vinfo)\n+\t    aarch64_analyze_loop_vinfo (loop_vinfo, costs);\n+\t  else\n+\t    aarch64_analyze_bb_vinfo (bb_vinfo, costs);\n+\t  costs->analyzed_vinfo = true;\n+\t}\n \n       /* Try to get a more accurate cost by looking at STMT_INFO instead\n \t of just looking at KIND.  */\n@@ -14571,10 +14713,21 @@ aarch64_add_stmt_cost (class vec_info *vinfo, void *data, int count,\n \t\t\t\t\t\t  vectype, stmt_cost);\n \n       if (stmt_info && aarch64_use_new_vector_costs_p ())\n-\t/* Account for any extra \"embedded\" costs that apply additively\n-\t   to the base cost calculated above.  */\n-\tstmt_cost = aarch64_adjust_stmt_cost (kind, stmt_info, vectype,\n-\t\t\t\t\t      stmt_cost);\n+\t{\n+\t  /* Account for any extra \"embedded\" costs that apply additively\n+\t     to the base cost calculated above.  */\n+\t  stmt_cost = aarch64_adjust_stmt_cost (kind, stmt_info, vectype,\n+\t\t\t\t\t\tstmt_cost);\n+\n+\t  /* If we're applying the SVE vs. Advanced SIMD unrolling heuristic,\n+\t     estimate the number of statements in the unrolled Advanced SIMD\n+\t     loop.  For simplicitly, we assume that one iteration of the\n+\t     Advanced SIMD loop would need the same number of statements\n+\t     as one iteration of the SVE loop.  */\n+\t  if (where == vect_body && costs->unrolled_advsimd_niters)\n+\t    costs->unrolled_advsimd_stmts\n+\t      += count * costs->unrolled_advsimd_niters;\n+\t}\n \n       /* Statements in an inner loop relative to the loop being\n \t vectorized are weighted more heavily.  The value here is\n@@ -14590,6 +14743,49 @@ aarch64_add_stmt_cost (class vec_info *vinfo, void *data, int count,\n   return retval;\n }\n \n+/* BODY_COST is the cost of a vector loop body recorded in COSTS.\n+   Adjust the cost as necessary and return the new cost.  */\n+static unsigned int\n+aarch64_adjust_body_cost (aarch64_vector_costs *costs, unsigned int body_cost)\n+{\n+  unsigned int orig_body_cost = body_cost;\n+\n+  if (costs->unrolled_advsimd_stmts)\n+    {\n+      if (dump_enabled_p ())\n+\tdump_printf_loc (MSG_NOTE, vect_location, \"Number of insns in\"\n+\t\t\t \" unrolled Advanced SIMD loop = %d\\n\",\n+\t\t\t costs->unrolled_advsimd_stmts);\n+\n+      /* Apply the Advanced SIMD vs. SVE unrolling heuristic described above\n+\t aarch64_vector_costs::unrolled_advsimd_niters.\n+\n+\t The balance here is tricky.  On the one hand, we can't be sure whether\n+\t the code is vectorizable with Advanced SIMD or not.  However, even if\n+\t it isn't vectorizable with Advanced SIMD, there's a possibility that\n+\t the scalar code could also be unrolled.  Some of the code might then\n+\t benefit from SLP, or from using LDP and STP.  We therefore apply\n+\t the heuristic regardless of can_use_advsimd_p.  */\n+      if (costs->unrolled_advsimd_stmts\n+\t  && (costs->unrolled_advsimd_stmts\n+\t      <= (unsigned int) param_max_completely_peeled_insns))\n+\t{\n+\t  unsigned int estimated_vq = aarch64_estimated_sve_vq ();\n+\t  unsigned int min_cost = (orig_body_cost * estimated_vq) + 1;\n+\t  if (body_cost < min_cost)\n+\t    {\n+\t      if (dump_enabled_p ())\n+\t\tdump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t\t \"Increasing body cost to %d to account for\"\n+\t\t\t\t \" unrolling\\n\", min_cost);\n+\t      body_cost = min_cost;\n+\t    }\n+\t}\n+    }\n+\n+  return body_cost;\n+}\n+\n /* Implement TARGET_VECTORIZE_FINISH_COST.  */\n static void\n aarch64_finish_cost (void *data, unsigned *prologue_cost,\n@@ -14599,6 +14795,11 @@ aarch64_finish_cost (void *data, unsigned *prologue_cost,\n   *prologue_cost = costs->region[vect_prologue];\n   *body_cost     = costs->region[vect_body];\n   *epilogue_cost = costs->region[vect_epilogue];\n+\n+  if (costs->is_loop\n+      && costs->vec_flags\n+      && aarch64_use_new_vector_costs_p ())\n+    *body_cost = aarch64_adjust_body_cost (costs, *body_cost);\n }\n \n /* Implement TARGET_VECTORIZE_DESTROY_COST_DATA.  */"}]}