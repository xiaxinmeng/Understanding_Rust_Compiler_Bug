{"sha": "cbdd87d44471ec0b9099dd8d6e32ae09039a1b48", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6Y2JkZDg3ZDQ0NDcxZWMwYjkwOTlkZDhkNmUzMmFlMDkwMzlhMWI0OA==", "commit": {"author": {"name": "Richard Guenther", "email": "rguenther@suse.de", "date": "2010-04-15T12:58:05Z"}, "committer": {"name": "Richard Biener", "email": "rguenth@gcc.gnu.org", "date": "2010-04-15T12:58:05Z"}, "message": "Makefile.in (OBJS-common): Add gimple-fold.o.\n\n2010-04-15  Richard Guenther  <rguenther@suse.de>\n\n\t* Makefile.in (OBJS-common): Add gimple-fold.o.\n\t(gimple-fold.o): New rule.\n\t* tree.h (maybe_fold_offset_to_reference,\n\tmaybe_fold_offset_to_address, maybe_fold_stmt_addition): Move\n\tprototypes ...\n\t* gimple.h: ... here.\n\t* tree-flow.h (fold_stmt, fold_stmt_inplace, get_symbol_constant_value,\n\tmay_propagate_address_into_dereference): Move prototypes ...\n\t* gimple.h: ... here.\n\t* tree-ssa-ccp.c (get_symbol_constant_value,\n\tmay_propagate_address_into_dereference, maybe_fold_offset_to_array_ref,\n\tmaybe_fold_offset_to_component_ref, maybe_fold_offset_to_reference,\n\tmaybe_fold_offset_to_address, maybe_fold_stmt_indirect,\n\tmaybe_fold_stmt_addition, maybe_fold_reference, get_maxval_strlen,\n\tccp_fold_builtin, fold_gimple_assign, fold_gimple_cond,\n\tfold_gimple_call, fold_stmt_1, fold_stmt, fold_stmt_inplace,\n\tgimplify_and_update_call_from_tree): Move ...\n\t* gimple-fold.c: ... here.  New file.\n\t(ccp_fold_builtin): Rename to ...\n\t(gimple_fold_builtin): ... this.\n\t* tree-ssa-ccp.c (execute_fold_all_builtins): Adjust.\n\nFrom-SVN: r158373", "tree": {"sha": "07442da6494114f13c624a8b816bc1146c0fe1fd", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/07442da6494114f13c624a8b816bc1146c0fe1fd"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48", "html_url": "https://github.com/Rust-GCC/gccrs/commit/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/comments", "author": {"login": "rguenth", "id": 2046526, "node_id": "MDQ6VXNlcjIwNDY1MjY=", "avatar_url": "https://avatars.githubusercontent.com/u/2046526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rguenth", "html_url": "https://github.com/rguenth", "followers_url": "https://api.github.com/users/rguenth/followers", "following_url": "https://api.github.com/users/rguenth/following{/other_user}", "gists_url": "https://api.github.com/users/rguenth/gists{/gist_id}", "starred_url": "https://api.github.com/users/rguenth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rguenth/subscriptions", "organizations_url": "https://api.github.com/users/rguenth/orgs", "repos_url": "https://api.github.com/users/rguenth/repos", "events_url": "https://api.github.com/users/rguenth/events{/privacy}", "received_events_url": "https://api.github.com/users/rguenth/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "330db1e301250f2b81a1dc79a8c07b2d9e7dfc29", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/330db1e301250f2b81a1dc79a8c07b2d9e7dfc29", "html_url": "https://github.com/Rust-GCC/gccrs/commit/330db1e301250f2b81a1dc79a8c07b2d9e7dfc29"}], "stats": {"total": 3202, "additions": 1639, "deletions": 1563}, "files": [{"sha": "d6bbad99cc8001ae85e7ee8aded30650d78f1a5c", "filename": "gcc/ChangeLog", "status": "modified", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=cbdd87d44471ec0b9099dd8d6e32ae09039a1b48", "patch": "@@ -1,3 +1,27 @@\n+2010-04-15  Richard Guenther  <rguenther@suse.de>\n+\n+\t* Makefile.in (OBJS-common): Add gimple-fold.o.\n+\t(gimple-fold.o): New rule.\n+\t* tree.h (maybe_fold_offset_to_reference,\n+\tmaybe_fold_offset_to_address, maybe_fold_stmt_addition): Move\n+\tprototypes ...\n+\t* gimple.h: ... here.\n+\t* tree-flow.h (fold_stmt, fold_stmt_inplace, get_symbol_constant_value,\n+\tmay_propagate_address_into_dereference): Move prototypes ...\n+\t* gimple.h: ... here.\n+\t* tree-ssa-ccp.c (get_symbol_constant_value,\n+\tmay_propagate_address_into_dereference, maybe_fold_offset_to_array_ref,\n+\tmaybe_fold_offset_to_component_ref, maybe_fold_offset_to_reference,\n+\tmaybe_fold_offset_to_address, maybe_fold_stmt_indirect,\n+\tmaybe_fold_stmt_addition, maybe_fold_reference, get_maxval_strlen,\n+\tccp_fold_builtin, fold_gimple_assign, fold_gimple_cond,\n+\tfold_gimple_call, fold_stmt_1, fold_stmt, fold_stmt_inplace,\n+\tgimplify_and_update_call_from_tree): Move ...\n+\t* gimple-fold.c: ... here.  New file.\n+\t(ccp_fold_builtin): Rename to ...\n+\t(gimple_fold_builtin): ... this.\n+\t* tree-ssa-ccp.c (execute_fold_all_builtins): Adjust.\n+\n 2010-04-15  Richard Guenther  <rguenther@suse.de>\n \n \t* fold-const.c (LOWPART, HIGHPART, BASE, encode, decode,"}, {"sha": "e212225fdeb5dc8b0e95a90a0d59e143877ceffc", "filename": "gcc/Makefile.in", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=cbdd87d44471ec0b9099dd8d6e32ae09039a1b48", "patch": "@@ -1207,6 +1207,7 @@ OBJS-common = \\\n \tggc-common.o \\\n \tgimple.o \\\n \tgimple-iterator.o \\\n+\tgimple-fold.o \\\n \tgimple-low.o \\\n \tgimple-pretty-print.o \\\n \tgimplify.o \\\n@@ -2534,6 +2535,11 @@ gimplify.o : gimplify.c $(CONFIG_H) $(SYSTEM_H) $(TREE_H) $(GIMPLE_H) \\\n    $(REAL_H) $(SPLAY_TREE_H) vec.h tree-iterator.h tree-pass.h\n gimple-iterator.o : gimple-iterator.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n    $(TREE_H) $(GIMPLE_H) $(TREE_FLOW_H) value-prof.h\n+gimple-fold.o : gimple-fold.c $(TREE_FLOW_H) $(CONFIG_H) \\\n+   $(SYSTEM_H) $(RTL_H) $(TREE_H) $(TM_P_H) $(EXPR_H) $(GGC_H) output.h \\\n+   $(DIAGNOSTIC_H) $(FUNCTION_H) $(TIMEVAR_H) $(TM_H) coretypes.h \\\n+   $(TREE_DUMP_H) $(BASIC_BLOCK_H) $(TREE_PASS_H) langhooks.h \\\n+   tree-ssa-propagate.h value-prof.h $(FLAGS_H) $(TARGET_H)\n gimple-low.o : gimple-low.c $(CONFIG_H) $(SYSTEM_H) $(TREE_H) \\\n    $(DIAGNOSTIC_H) $(GIMPLE_H) $(TREE_INLINE_H) $(VARRAY_H) langhooks.h \\\n    $(LANGHOOKS_DEF_H) $(TREE_FLOW_H) $(TIMEVAR_H) $(TM_H) coretypes.h \\"}, {"sha": "270475c0c6cdc484b6b21ba0e2e5d484d7f5b084", "filename": "gcc/gimple-fold.c", "status": "added", "additions": 1596, "deletions": 0, "changes": 1596, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/gcc%2Fgimple-fold.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/gcc%2Fgimple-fold.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgimple-fold.c?ref=cbdd87d44471ec0b9099dd8d6e32ae09039a1b48", "patch": "@@ -0,0 +1,1596 @@\n+/* Statement simplification on GIMPLE.\n+   Copyright (C) 2010 Free Software Foundation, Inc.\n+   Split out from tree-ssa-ccp.c.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it\n+under the terms of the GNU General Public License as published by the\n+Free Software Foundation; either version 3, or (at your option) any\n+later version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT\n+ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"tree.h\"\n+#include \"flags.h\"\n+#include \"rtl.h\"\n+#include \"tm_p.h\"\n+#include \"ggc.h\"\n+#include \"basic-block.h\"\n+#include \"output.h\"\n+#include \"expr.h\"\n+#include \"function.h\"\n+#include \"diagnostic.h\"\n+#include \"timevar.h\"\n+#include \"tree-dump.h\"\n+#include \"tree-flow.h\"\n+#include \"tree-pass.h\"\n+#include \"tree-ssa-propagate.h\"\n+#include \"value-prof.h\"\n+#include \"langhooks.h\"\n+#include \"target.h\"\n+\n+\n+/* If SYM is a constant variable with known value, return the value.\n+   NULL_TREE is returned otherwise.  */\n+\n+tree\n+get_symbol_constant_value (tree sym)\n+{\n+  if (TREE_STATIC (sym)\n+      && (TREE_READONLY (sym)\n+\t  || TREE_CODE (sym) == CONST_DECL))\n+    {\n+      tree val = DECL_INITIAL (sym);\n+      if (val)\n+\t{\n+\t  STRIP_NOPS (val);\n+\t  if (is_gimple_min_invariant (val))\n+\t    {\n+\t      if (TREE_CODE (val) == ADDR_EXPR)\n+\t\t{\n+\t\t  tree base = get_base_address (TREE_OPERAND (val, 0));\n+\t\t  if (base && TREE_CODE (base) == VAR_DECL)\n+\t\t    {\n+\t\t      TREE_ADDRESSABLE (base) = 1;\n+\t\t      if (gimple_referenced_vars (cfun))\n+\t\t\tadd_referenced_var (base);\n+\t\t    }\n+\t\t}\n+\t      return val;\n+\t    }\n+\t}\n+      /* Variables declared 'const' without an initializer\n+\t have zero as the initializer if they may not be\n+\t overridden at link or run time.  */\n+      if (!val\n+\t  && !DECL_EXTERNAL (sym)\n+\t  && targetm.binds_local_p (sym)\n+          && (INTEGRAL_TYPE_P (TREE_TYPE (sym))\n+\t       || SCALAR_FLOAT_TYPE_P (TREE_TYPE (sym))))\n+\treturn fold_convert (TREE_TYPE (sym), integer_zero_node);\n+    }\n+\n+  return NULL_TREE;\n+}\n+\n+\n+/* Return true if we may propagate the address expression ADDR into the\n+   dereference DEREF and cancel them.  */\n+\n+bool\n+may_propagate_address_into_dereference (tree addr, tree deref)\n+{\n+  gcc_assert (INDIRECT_REF_P (deref)\n+\t      && TREE_CODE (addr) == ADDR_EXPR);\n+\n+  /* Don't propagate if ADDR's operand has incomplete type.  */\n+  if (!COMPLETE_TYPE_P (TREE_TYPE (TREE_OPERAND (addr, 0))))\n+    return false;\n+\n+  /* If the address is invariant then we do not need to preserve restrict\n+     qualifications.  But we do need to preserve volatile qualifiers until\n+     we can annotate the folded dereference itself properly.  */\n+  if (is_gimple_min_invariant (addr)\n+      && (!TREE_THIS_VOLATILE (deref)\n+\t  || TYPE_VOLATILE (TREE_TYPE (addr))))\n+    return useless_type_conversion_p (TREE_TYPE (deref),\n+\t\t\t\t      TREE_TYPE (TREE_OPERAND (addr, 0)));\n+\n+  /* Else both the address substitution and the folding must result in\n+     a valid useless type conversion sequence.  */\n+  return (useless_type_conversion_p (TREE_TYPE (TREE_OPERAND (deref, 0)),\n+\t\t\t\t     TREE_TYPE (addr))\n+\t  && useless_type_conversion_p (TREE_TYPE (deref),\n+\t\t\t\t\tTREE_TYPE (TREE_OPERAND (addr, 0))));\n+}\n+\n+\n+/* A subroutine of fold_stmt.  Attempts to fold *(A+O) to A[X].\n+   BASE is an array type.  OFFSET is a byte displacement.  ORIG_TYPE\n+   is the desired result type.\n+\n+   LOC is the location of the original expression.  */\n+\n+static tree\n+maybe_fold_offset_to_array_ref (location_t loc, tree base, tree offset,\n+\t\t\t\ttree orig_type,\n+\t\t\t\tbool allow_negative_idx)\n+{\n+  tree min_idx, idx, idx_type, elt_offset = integer_zero_node;\n+  tree array_type, elt_type, elt_size;\n+  tree domain_type;\n+\n+  /* If BASE is an ARRAY_REF, we can pick up another offset (this time\n+     measured in units of the size of elements type) from that ARRAY_REF).\n+     We can't do anything if either is variable.\n+\n+     The case we handle here is *(&A[N]+O).  */\n+  if (TREE_CODE (base) == ARRAY_REF)\n+    {\n+      tree low_bound = array_ref_low_bound (base);\n+\n+      elt_offset = TREE_OPERAND (base, 1);\n+      if (TREE_CODE (low_bound) != INTEGER_CST\n+\t  || TREE_CODE (elt_offset) != INTEGER_CST)\n+\treturn NULL_TREE;\n+\n+      elt_offset = int_const_binop (MINUS_EXPR, elt_offset, low_bound, 0);\n+      base = TREE_OPERAND (base, 0);\n+    }\n+\n+  /* Ignore stupid user tricks of indexing non-array variables.  */\n+  array_type = TREE_TYPE (base);\n+  if (TREE_CODE (array_type) != ARRAY_TYPE)\n+    return NULL_TREE;\n+  elt_type = TREE_TYPE (array_type);\n+  if (!useless_type_conversion_p (orig_type, elt_type))\n+    return NULL_TREE;\n+\n+  /* Use signed size type for intermediate computation on the index.  */\n+  idx_type = signed_type_for (size_type_node);\n+\n+  /* If OFFSET and ELT_OFFSET are zero, we don't care about the size of the\n+     element type (so we can use the alignment if it's not constant).\n+     Otherwise, compute the offset as an index by using a division.  If the\n+     division isn't exact, then don't do anything.  */\n+  elt_size = TYPE_SIZE_UNIT (elt_type);\n+  if (!elt_size)\n+    return NULL;\n+  if (integer_zerop (offset))\n+    {\n+      if (TREE_CODE (elt_size) != INTEGER_CST)\n+\telt_size = size_int (TYPE_ALIGN (elt_type));\n+\n+      idx = build_int_cst (idx_type, 0);\n+    }\n+  else\n+    {\n+      unsigned HOST_WIDE_INT lquo, lrem;\n+      HOST_WIDE_INT hquo, hrem;\n+      double_int soffset;\n+\n+      /* The final array offset should be signed, so we need\n+\t to sign-extend the (possibly pointer) offset here\n+\t and use signed division.  */\n+      soffset = double_int_sext (tree_to_double_int (offset),\n+\t\t\t\t TYPE_PRECISION (TREE_TYPE (offset)));\n+      if (TREE_CODE (elt_size) != INTEGER_CST\n+\t  || div_and_round_double (TRUNC_DIV_EXPR, 0,\n+\t\t\t\t   soffset.low, soffset.high,\n+\t\t\t\t   TREE_INT_CST_LOW (elt_size),\n+\t\t\t\t   TREE_INT_CST_HIGH (elt_size),\n+\t\t\t\t   &lquo, &hquo, &lrem, &hrem)\n+\t  || lrem || hrem)\n+\treturn NULL_TREE;\n+\n+      idx = build_int_cst_wide (idx_type, lquo, hquo);\n+    }\n+\n+  /* Assume the low bound is zero.  If there is a domain type, get the\n+     low bound, if any, convert the index into that type, and add the\n+     low bound.  */\n+  min_idx = build_int_cst (idx_type, 0);\n+  domain_type = TYPE_DOMAIN (array_type);\n+  if (domain_type)\n+    {\n+      idx_type = domain_type;\n+      if (TYPE_MIN_VALUE (idx_type))\n+\tmin_idx = TYPE_MIN_VALUE (idx_type);\n+      else\n+\tmin_idx = fold_convert (idx_type, min_idx);\n+\n+      if (TREE_CODE (min_idx) != INTEGER_CST)\n+\treturn NULL_TREE;\n+\n+      elt_offset = fold_convert (idx_type, elt_offset);\n+    }\n+\n+  if (!integer_zerop (min_idx))\n+    idx = int_const_binop (PLUS_EXPR, idx, min_idx, 0);\n+  if (!integer_zerop (elt_offset))\n+    idx = int_const_binop (PLUS_EXPR, idx, elt_offset, 0);\n+\n+  /* Make sure to possibly truncate late after offsetting.  */\n+  idx = fold_convert (idx_type, idx);\n+\n+  /* We don't want to construct access past array bounds. For example\n+       char *(c[4]);\n+       c[3][2];\n+     should not be simplified into (*c)[14] or tree-vrp will\n+     give false warnings.  The same is true for\n+       struct A { long x; char d[0]; } *a;\n+       (char *)a - 4;\n+     which should be not folded to &a->d[-8].  */\n+  if (domain_type\n+      && TYPE_MAX_VALUE (domain_type)\n+      && TREE_CODE (TYPE_MAX_VALUE (domain_type)) == INTEGER_CST)\n+    {\n+      tree up_bound = TYPE_MAX_VALUE (domain_type);\n+\n+      if (tree_int_cst_lt (up_bound, idx)\n+\t  /* Accesses after the end of arrays of size 0 (gcc\n+\t     extension) and 1 are likely intentional (\"struct\n+\t     hack\").  */\n+\t  && compare_tree_int (up_bound, 1) > 0)\n+\treturn NULL_TREE;\n+    }\n+  if (domain_type\n+      && TYPE_MIN_VALUE (domain_type))\n+    {\n+      if (!allow_negative_idx\n+\t  && TREE_CODE (TYPE_MIN_VALUE (domain_type)) == INTEGER_CST\n+\t  && tree_int_cst_lt (idx, TYPE_MIN_VALUE (domain_type)))\n+\treturn NULL_TREE;\n+    }\n+  else if (!allow_negative_idx\n+\t   && compare_tree_int (idx, 0) < 0)\n+    return NULL_TREE;\n+\n+  {\n+    tree t = build4 (ARRAY_REF, elt_type, base, idx, NULL_TREE, NULL_TREE);\n+    SET_EXPR_LOCATION (t, loc);\n+    return t;\n+  }\n+}\n+\n+\n+/* Attempt to fold *(S+O) to S.X.\n+   BASE is a record type.  OFFSET is a byte displacement.  ORIG_TYPE\n+   is the desired result type.\n+\n+   LOC is the location of the original expression.  */\n+\n+static tree\n+maybe_fold_offset_to_component_ref (location_t loc, tree record_type,\n+\t\t\t\t    tree base, tree offset, tree orig_type)\n+{\n+  tree f, t, field_type, tail_array_field, field_offset;\n+  tree ret;\n+  tree new_base;\n+\n+  if (TREE_CODE (record_type) != RECORD_TYPE\n+      && TREE_CODE (record_type) != UNION_TYPE\n+      && TREE_CODE (record_type) != QUAL_UNION_TYPE)\n+    return NULL_TREE;\n+\n+  /* Short-circuit silly cases.  */\n+  if (useless_type_conversion_p (record_type, orig_type))\n+    return NULL_TREE;\n+\n+  tail_array_field = NULL_TREE;\n+  for (f = TYPE_FIELDS (record_type); f ; f = TREE_CHAIN (f))\n+    {\n+      int cmp;\n+\n+      if (TREE_CODE (f) != FIELD_DECL)\n+\tcontinue;\n+      if (DECL_BIT_FIELD (f))\n+\tcontinue;\n+\n+      if (!DECL_FIELD_OFFSET (f))\n+\tcontinue;\n+      field_offset = byte_position (f);\n+      if (TREE_CODE (field_offset) != INTEGER_CST)\n+\tcontinue;\n+\n+      /* ??? Java creates \"interesting\" fields for representing base classes.\n+\t They have no name, and have no context.  With no context, we get into\n+\t trouble with nonoverlapping_component_refs_p.  Skip them.  */\n+      if (!DECL_FIELD_CONTEXT (f))\n+\tcontinue;\n+\n+      /* The previous array field isn't at the end.  */\n+      tail_array_field = NULL_TREE;\n+\n+      /* Check to see if this offset overlaps with the field.  */\n+      cmp = tree_int_cst_compare (field_offset, offset);\n+      if (cmp > 0)\n+\tcontinue;\n+\n+      field_type = TREE_TYPE (f);\n+\n+      /* Here we exactly match the offset being checked.  If the types match,\n+\t then we can return that field.  */\n+      if (cmp == 0\n+\t  && useless_type_conversion_p (orig_type, field_type))\n+\t{\n+\t  t = fold_build3 (COMPONENT_REF, field_type, base, f, NULL_TREE);\n+\t  return t;\n+\t}\n+\n+      /* Don't care about offsets into the middle of scalars.  */\n+      if (!AGGREGATE_TYPE_P (field_type))\n+\tcontinue;\n+\n+      /* Check for array at the end of the struct.  This is often\n+\t used as for flexible array members.  We should be able to\n+\t turn this into an array access anyway.  */\n+      if (TREE_CODE (field_type) == ARRAY_TYPE)\n+\ttail_array_field = f;\n+\n+      /* Check the end of the field against the offset.  */\n+      if (!DECL_SIZE_UNIT (f)\n+\t  || TREE_CODE (DECL_SIZE_UNIT (f)) != INTEGER_CST)\n+\tcontinue;\n+      t = int_const_binop (MINUS_EXPR, offset, field_offset, 1);\n+      if (!tree_int_cst_lt (t, DECL_SIZE_UNIT (f)))\n+\tcontinue;\n+\n+      /* If we matched, then set offset to the displacement into\n+\t this field.  */\n+      new_base = fold_build3 (COMPONENT_REF, field_type, base, f, NULL_TREE);\n+      SET_EXPR_LOCATION (new_base, loc);\n+\n+      /* Recurse to possibly find the match.  */\n+      ret = maybe_fold_offset_to_array_ref (loc, new_base, t, orig_type,\n+\t\t\t\t\t    f == TYPE_FIELDS (record_type));\n+      if (ret)\n+\treturn ret;\n+      ret = maybe_fold_offset_to_component_ref (loc, field_type, new_base, t,\n+\t\t\t\t\t\torig_type);\n+      if (ret)\n+\treturn ret;\n+    }\n+\n+  if (!tail_array_field)\n+    return NULL_TREE;\n+\n+  f = tail_array_field;\n+  field_type = TREE_TYPE (f);\n+  offset = int_const_binop (MINUS_EXPR, offset, byte_position (f), 1);\n+\n+  /* If we get here, we've got an aggregate field, and a possibly\n+     nonzero offset into them.  Recurse and hope for a valid match.  */\n+  base = fold_build3 (COMPONENT_REF, field_type, base, f, NULL_TREE);\n+  SET_EXPR_LOCATION (base, loc);\n+\n+  t = maybe_fold_offset_to_array_ref (loc, base, offset, orig_type,\n+\t\t\t\t      f == TYPE_FIELDS (record_type));\n+  if (t)\n+    return t;\n+  return maybe_fold_offset_to_component_ref (loc, field_type, base, offset,\n+\t\t\t\t\t     orig_type);\n+}\n+\n+/* Attempt to express (ORIG_TYPE)BASE+OFFSET as BASE->field_of_orig_type\n+   or BASE[index] or by combination of those.\n+\n+   LOC is the location of original expression.\n+\n+   Before attempting the conversion strip off existing ADDR_EXPRs and\n+   handled component refs.  */\n+\n+tree\n+maybe_fold_offset_to_reference (location_t loc, tree base, tree offset,\n+\t\t\t\ttree orig_type)\n+{\n+  tree ret;\n+  tree type;\n+\n+  STRIP_NOPS (base);\n+  if (TREE_CODE (base) != ADDR_EXPR)\n+    return NULL_TREE;\n+\n+  base = TREE_OPERAND (base, 0);\n+\n+  /* Handle case where existing COMPONENT_REF pick e.g. wrong field of union,\n+     so it needs to be removed and new COMPONENT_REF constructed.\n+     The wrong COMPONENT_REF are often constructed by folding the\n+     (type *)&object within the expression (type *)&object+offset  */\n+  if (handled_component_p (base))\n+    {\n+      HOST_WIDE_INT sub_offset, size, maxsize;\n+      tree newbase;\n+      newbase = get_ref_base_and_extent (base, &sub_offset,\n+\t\t\t\t\t &size, &maxsize);\n+      gcc_assert (newbase);\n+      if (size == maxsize\n+\t  && size != -1\n+\t  && !(sub_offset & (BITS_PER_UNIT - 1)))\n+\t{\n+\t  base = newbase;\n+\t  if (sub_offset)\n+\t    offset = int_const_binop (PLUS_EXPR, offset,\n+\t\t\t\t      build_int_cst (TREE_TYPE (offset),\n+\t\t\t\t\t\t     sub_offset / BITS_PER_UNIT), 1);\n+\t}\n+    }\n+  if (useless_type_conversion_p (orig_type, TREE_TYPE (base))\n+      && integer_zerop (offset))\n+    return base;\n+  type = TREE_TYPE (base);\n+\n+  ret = maybe_fold_offset_to_component_ref (loc, type, base, offset, orig_type);\n+  if (!ret)\n+    ret = maybe_fold_offset_to_array_ref (loc, base, offset, orig_type, true);\n+\n+  return ret;\n+}\n+\n+/* Attempt to express (ORIG_TYPE)&BASE+OFFSET as &BASE->field_of_orig_type\n+   or &BASE[index] or by combination of those.\n+\n+   LOC is the location of the original expression.\n+\n+   Before attempting the conversion strip off existing component refs.  */\n+\n+tree\n+maybe_fold_offset_to_address (location_t loc, tree addr, tree offset,\n+\t\t\t      tree orig_type)\n+{\n+  tree t;\n+\n+  gcc_assert (POINTER_TYPE_P (TREE_TYPE (addr))\n+\t      && POINTER_TYPE_P (orig_type));\n+\n+  t = maybe_fold_offset_to_reference (loc, addr, offset,\n+\t\t\t\t      TREE_TYPE (orig_type));\n+  if (t != NULL_TREE)\n+    {\n+      tree orig = addr;\n+      tree ptr_type;\n+\n+      /* For __builtin_object_size to function correctly we need to\n+         make sure not to fold address arithmetic so that we change\n+\t reference from one array to another.  This would happen for\n+\t example for\n+\n+\t   struct X { char s1[10]; char s2[10] } s;\n+\t   char *foo (void) { return &s.s2[-4]; }\n+\n+\t where we need to avoid generating &s.s1[6].  As the C and\n+\t C++ frontends create different initial trees\n+\t (char *) &s.s1 + -4  vs.  &s.s1[-4]  we have to do some\n+\t sophisticated comparisons here.  Note that checking for the\n+\t condition after the fact is easier than trying to avoid doing\n+\t the folding.  */\n+      STRIP_NOPS (orig);\n+      if (TREE_CODE (orig) == ADDR_EXPR)\n+\torig = TREE_OPERAND (orig, 0);\n+      if ((TREE_CODE (orig) == ARRAY_REF\n+\t   || (TREE_CODE (orig) == COMPONENT_REF\n+\t       && TREE_CODE (TREE_TYPE (TREE_OPERAND (orig, 1))) == ARRAY_TYPE))\n+\t  && (TREE_CODE (t) == ARRAY_REF\n+\t      || TREE_CODE (t) == COMPONENT_REF)\n+\t  && !operand_equal_p (TREE_CODE (orig) == ARRAY_REF\n+\t\t\t       ? TREE_OPERAND (orig, 0) : orig,\n+\t\t\t       TREE_CODE (t) == ARRAY_REF\n+\t\t\t       ? TREE_OPERAND (t, 0) : t, 0))\n+\treturn NULL_TREE;\n+\n+      ptr_type = build_pointer_type (TREE_TYPE (t));\n+      if (!useless_type_conversion_p (orig_type, ptr_type))\n+\treturn NULL_TREE;\n+      return build_fold_addr_expr_with_type_loc (loc, t, ptr_type);\n+    }\n+\n+  return NULL_TREE;\n+}\n+\n+/* A subroutine of fold_stmt.  Attempt to simplify *(BASE+OFFSET).\n+   Return the simplified expression, or NULL if nothing could be done.  */\n+\n+static tree\n+maybe_fold_stmt_indirect (tree expr, tree base, tree offset)\n+{\n+  tree t;\n+  bool volatile_p = TREE_THIS_VOLATILE (expr);\n+  location_t loc = EXPR_LOCATION (expr);\n+\n+  /* We may well have constructed a double-nested PLUS_EXPR via multiple\n+     substitutions.  Fold that down to one.  Remove NON_LVALUE_EXPRs that\n+     are sometimes added.  */\n+  base = fold (base);\n+  STRIP_TYPE_NOPS (base);\n+  TREE_OPERAND (expr, 0) = base;\n+\n+  /* One possibility is that the address reduces to a string constant.  */\n+  t = fold_read_from_constant_string (expr);\n+  if (t)\n+    return t;\n+\n+  /* Add in any offset from a POINTER_PLUS_EXPR.  */\n+  if (TREE_CODE (base) == POINTER_PLUS_EXPR)\n+    {\n+      tree offset2;\n+\n+      offset2 = TREE_OPERAND (base, 1);\n+      if (TREE_CODE (offset2) != INTEGER_CST)\n+\treturn NULL_TREE;\n+      base = TREE_OPERAND (base, 0);\n+\n+      offset = fold_convert (sizetype,\n+\t\t\t     int_const_binop (PLUS_EXPR, offset, offset2, 1));\n+    }\n+\n+  if (TREE_CODE (base) == ADDR_EXPR)\n+    {\n+      tree base_addr = base;\n+\n+      /* Strip the ADDR_EXPR.  */\n+      base = TREE_OPERAND (base, 0);\n+\n+      /* Fold away CONST_DECL to its value, if the type is scalar.  */\n+      if (TREE_CODE (base) == CONST_DECL\n+\t  && is_gimple_min_invariant (DECL_INITIAL (base)))\n+\treturn DECL_INITIAL (base);\n+\n+      /* If there is no offset involved simply return the folded base.  */\n+      if (integer_zerop (offset))\n+\treturn base;\n+\n+      /* Try folding *(&B+O) to B.X.  */\n+      t = maybe_fold_offset_to_reference (loc, base_addr, offset,\n+\t\t\t\t\t  TREE_TYPE (expr));\n+      if (t)\n+\t{\n+\t  /* Preserve volatileness of the original expression.\n+\t     We can end up with a plain decl here which is shared\n+\t     and we shouldn't mess with its flags.  */\n+\t  if (!SSA_VAR_P (t))\n+\t    TREE_THIS_VOLATILE (t) = volatile_p;\n+\t  return t;\n+\t}\n+    }\n+  else\n+    {\n+      /* We can get here for out-of-range string constant accesses,\n+\t such as \"_\"[3].  Bail out of the entire substitution search\n+\t and arrange for the entire statement to be replaced by a\n+\t call to __builtin_trap.  In all likelihood this will all be\n+\t constant-folded away, but in the meantime we can't leave with\n+\t something that get_expr_operands can't understand.  */\n+\n+      t = base;\n+      STRIP_NOPS (t);\n+      if (TREE_CODE (t) == ADDR_EXPR\n+\t  && TREE_CODE (TREE_OPERAND (t, 0)) == STRING_CST)\n+\t{\n+\t  /* FIXME: Except that this causes problems elsewhere with dead\n+\t     code not being deleted, and we die in the rtl expanders\n+\t     because we failed to remove some ssa_name.  In the meantime,\n+\t     just return zero.  */\n+\t  /* FIXME2: This condition should be signaled by\n+\t     fold_read_from_constant_string directly, rather than\n+\t     re-checking for it here.  */\n+\t  return integer_zero_node;\n+\t}\n+\n+      /* Try folding *(B+O) to B->X.  Still an improvement.  */\n+      if (POINTER_TYPE_P (TREE_TYPE (base)))\n+\t{\n+          t = maybe_fold_offset_to_reference (loc, base, offset,\n+\t\t\t\t              TREE_TYPE (expr));\n+\t  if (t)\n+\t    return t;\n+\t}\n+    }\n+\n+  /* Otherwise we had an offset that we could not simplify.  */\n+  return NULL_TREE;\n+}\n+\n+\n+/* A quaint feature extant in our address arithmetic is that there\n+   can be hidden type changes here.  The type of the result need\n+   not be the same as the type of the input pointer.\n+\n+   What we're after here is an expression of the form\n+\t(T *)(&array + const)\n+   where array is OP0, const is OP1, RES_TYPE is T and\n+   the cast doesn't actually exist, but is implicit in the\n+   type of the POINTER_PLUS_EXPR.  We'd like to turn this into\n+\t&array[x]\n+   which may be able to propagate further.  */\n+\n+tree\n+maybe_fold_stmt_addition (location_t loc, tree res_type, tree op0, tree op1)\n+{\n+  tree ptd_type;\n+  tree t;\n+\n+  /* The first operand should be an ADDR_EXPR.  */\n+  if (TREE_CODE (op0) != ADDR_EXPR)\n+    return NULL_TREE;\n+  op0 = TREE_OPERAND (op0, 0);\n+\n+  /* It had better be a constant.  */\n+  if (TREE_CODE (op1) != INTEGER_CST)\n+    {\n+      /* Or op0 should now be A[0] and the non-constant offset defined\n+\t via a multiplication by the array element size.  */\n+      if (TREE_CODE (op0) == ARRAY_REF\n+\t  && integer_zerop (TREE_OPERAND (op0, 1))\n+\t  && TREE_CODE (op1) == SSA_NAME\n+\t  && host_integerp (TYPE_SIZE_UNIT (TREE_TYPE (op0)), 1))\n+\t{\n+\t  gimple offset_def = SSA_NAME_DEF_STMT (op1);\n+\t  if (!is_gimple_assign (offset_def))\n+\t    return NULL_TREE;\n+\n+\t  if (gimple_assign_rhs_code (offset_def) == MULT_EXPR\n+\t      && TREE_CODE (gimple_assign_rhs2 (offset_def)) == INTEGER_CST\n+\t      && tree_int_cst_equal (gimple_assign_rhs2 (offset_def),\n+\t\t\t\t     TYPE_SIZE_UNIT (TREE_TYPE (op0))))\n+\t    return build_fold_addr_expr\n+\t\t\t  (build4 (ARRAY_REF, TREE_TYPE (op0),\n+\t\t\t\t   TREE_OPERAND (op0, 0),\n+\t\t\t\t   gimple_assign_rhs1 (offset_def),\n+\t\t\t\t   TREE_OPERAND (op0, 2),\n+\t\t\t\t   TREE_OPERAND (op0, 3)));\n+\t  else if (integer_onep (TYPE_SIZE_UNIT (TREE_TYPE (op0)))\n+\t\t   && gimple_assign_rhs_code (offset_def) != MULT_EXPR)\n+\t    return build_fold_addr_expr\n+\t\t\t  (build4 (ARRAY_REF, TREE_TYPE (op0),\n+\t\t\t\t   TREE_OPERAND (op0, 0),\n+\t\t\t\t   op1,\n+\t\t\t\t   TREE_OPERAND (op0, 2),\n+\t\t\t\t   TREE_OPERAND (op0, 3)));\n+\t}\n+      return NULL_TREE;\n+    }\n+\n+  /* If the first operand is an ARRAY_REF, expand it so that we can fold\n+     the offset into it.  */\n+  while (TREE_CODE (op0) == ARRAY_REF)\n+    {\n+      tree array_obj = TREE_OPERAND (op0, 0);\n+      tree array_idx = TREE_OPERAND (op0, 1);\n+      tree elt_type = TREE_TYPE (op0);\n+      tree elt_size = TYPE_SIZE_UNIT (elt_type);\n+      tree min_idx;\n+\n+      if (TREE_CODE (array_idx) != INTEGER_CST)\n+\tbreak;\n+      if (TREE_CODE (elt_size) != INTEGER_CST)\n+\tbreak;\n+\n+      /* Un-bias the index by the min index of the array type.  */\n+      min_idx = TYPE_DOMAIN (TREE_TYPE (array_obj));\n+      if (min_idx)\n+\t{\n+\t  min_idx = TYPE_MIN_VALUE (min_idx);\n+\t  if (min_idx)\n+\t    {\n+\t      if (TREE_CODE (min_idx) != INTEGER_CST)\n+\t\tbreak;\n+\n+\t      array_idx = fold_convert (TREE_TYPE (min_idx), array_idx);\n+\t      if (!integer_zerop (min_idx))\n+\t\tarray_idx = int_const_binop (MINUS_EXPR, array_idx,\n+\t\t\t\t\t     min_idx, 0);\n+\t    }\n+\t}\n+\n+      /* Convert the index to a byte offset.  */\n+      array_idx = fold_convert (sizetype, array_idx);\n+      array_idx = int_const_binop (MULT_EXPR, array_idx, elt_size, 0);\n+\n+      /* Update the operands for the next round, or for folding.  */\n+      op1 = int_const_binop (PLUS_EXPR,\n+\t\t\t     array_idx, op1, 0);\n+      op0 = array_obj;\n+    }\n+\n+  ptd_type = TREE_TYPE (res_type);\n+  /* If we want a pointer to void, reconstruct the reference from the\n+     array element type.  A pointer to that can be trivially converted\n+     to void *.  This happens as we fold (void *)(ptr p+ off).  */\n+  if (VOID_TYPE_P (ptd_type)\n+      && TREE_CODE (TREE_TYPE (op0)) == ARRAY_TYPE)\n+    ptd_type = TREE_TYPE (TREE_TYPE (op0));\n+\n+  /* At which point we can try some of the same things as for indirects.  */\n+  t = maybe_fold_offset_to_array_ref (loc, op0, op1, ptd_type, true);\n+  if (!t)\n+    t = maybe_fold_offset_to_component_ref (loc, TREE_TYPE (op0), op0, op1,\n+\t\t\t\t\t    ptd_type);\n+  if (t)\n+    {\n+      t = build1 (ADDR_EXPR, res_type, t);\n+      SET_EXPR_LOCATION (t, loc);\n+    }\n+\n+  return t;\n+}\n+\n+/* Subroutine of fold_stmt.  We perform several simplifications of the\n+   memory reference tree EXPR and make sure to re-gimplify them properly\n+   after propagation of constant addresses.  IS_LHS is true if the\n+   reference is supposed to be an lvalue.  */\n+\n+static tree\n+maybe_fold_reference (tree expr, bool is_lhs)\n+{\n+  tree *t = &expr;\n+\n+  if (TREE_CODE (expr) == ARRAY_REF\n+      && !is_lhs)\n+    {\n+      tree tem = fold_read_from_constant_string (expr);\n+      if (tem)\n+\treturn tem;\n+    }\n+\n+  /* ???  We might want to open-code the relevant remaining cases\n+     to avoid using the generic fold.  */\n+  if (handled_component_p (*t)\n+      && CONSTANT_CLASS_P (TREE_OPERAND (*t, 0)))\n+    {\n+      tree tem = fold (*t);\n+      if (tem != *t)\n+\treturn tem;\n+    }\n+\n+  while (handled_component_p (*t))\n+    t = &TREE_OPERAND (*t, 0);\n+\n+  if (TREE_CODE (*t) == INDIRECT_REF)\n+    {\n+      tree tem = maybe_fold_stmt_indirect (*t, TREE_OPERAND (*t, 0),\n+\t\t\t\t\t   integer_zero_node);\n+      /* Avoid folding *\"abc\" = 5 into 'a' = 5.  */\n+      if (is_lhs && tem && CONSTANT_CLASS_P (tem))\n+\ttem = NULL_TREE;\n+      if (!tem\n+\t  && TREE_CODE (TREE_OPERAND (*t, 0)) == ADDR_EXPR)\n+\t/* If we had a good reason for propagating the address here,\n+\t   make sure we end up with valid gimple.  See PR34989.  */\n+\ttem = TREE_OPERAND (TREE_OPERAND (*t, 0), 0);\n+\n+      if (tem)\n+\t{\n+\t  *t = tem;\n+\t  tem = maybe_fold_reference (expr, is_lhs);\n+\t  if (tem)\n+\t    return tem;\n+\t  return expr;\n+\t}\n+    }\n+  else if (!is_lhs\n+\t   && DECL_P (*t))\n+    {\n+      tree tem = get_symbol_constant_value (*t);\n+      if (tem\n+\t  && useless_type_conversion_p (TREE_TYPE (*t), TREE_TYPE (tem)))\n+\t{\n+\t  *t = unshare_expr (tem);\n+\t  tem = maybe_fold_reference (expr, is_lhs);\n+\t  if (tem)\n+\t    return tem;\n+\t  return expr;\n+\t}\n+    }\n+\n+  return NULL_TREE;\n+}\n+\n+\n+/* Attempt to fold an assignment statement pointed-to by SI.  Returns a\n+   replacement rhs for the statement or NULL_TREE if no simplification\n+   could be made.  It is assumed that the operands have been previously\n+   folded.  */\n+\n+static tree\n+fold_gimple_assign (gimple_stmt_iterator *si)\n+{\n+  gimple stmt = gsi_stmt (*si);\n+  enum tree_code subcode = gimple_assign_rhs_code (stmt);\n+  location_t loc = gimple_location (stmt);\n+\n+  tree result = NULL_TREE;\n+\n+  switch (get_gimple_rhs_class (subcode))\n+    {\n+    case GIMPLE_SINGLE_RHS:\n+      {\n+        tree rhs = gimple_assign_rhs1 (stmt);\n+\n+        /* Try to fold a conditional expression.  */\n+        if (TREE_CODE (rhs) == COND_EXPR)\n+          {\n+\t    tree op0 = COND_EXPR_COND (rhs);\n+\t    tree tem;\n+\t    bool set = false;\n+\t    location_t cond_loc = EXPR_LOCATION (rhs);\n+\n+\t    if (COMPARISON_CLASS_P (op0))\n+\t      {\n+\t\tfold_defer_overflow_warnings ();\n+\t\ttem = fold_binary_loc (cond_loc,\n+\t\t\t\t   TREE_CODE (op0), TREE_TYPE (op0),\n+\t\t\t\t   TREE_OPERAND (op0, 0),\n+\t\t\t\t   TREE_OPERAND (op0, 1));\n+\t\t/* This is actually a conditional expression, not a GIMPLE\n+\t\t   conditional statement, however, the valid_gimple_rhs_p\n+\t\t   test still applies.  */\n+\t\tset = (tem && is_gimple_condexpr (tem)\n+\t\t       && valid_gimple_rhs_p (tem));\n+\t\tfold_undefer_overflow_warnings (set, stmt, 0);\n+\t      }\n+\t    else if (is_gimple_min_invariant (op0))\n+\t      {\n+\t\ttem = op0;\n+\t\tset = true;\n+\t      }\n+\t    else\n+\t      return NULL_TREE;\n+\n+\t    if (set)\n+\t      result = fold_build3_loc (cond_loc, COND_EXPR, TREE_TYPE (rhs), tem,\n+\t\t\t\t    COND_EXPR_THEN (rhs), COND_EXPR_ELSE (rhs));\n+          }\n+\n+\telse if (TREE_CODE (rhs) == TARGET_MEM_REF)\n+\t  return maybe_fold_tmr (rhs);\n+\n+\telse if (REFERENCE_CLASS_P (rhs))\n+\t  return maybe_fold_reference (rhs, false);\n+\n+\telse if (TREE_CODE (rhs) == ADDR_EXPR)\n+\t  {\n+\t    tree tem = maybe_fold_reference (TREE_OPERAND (rhs, 0), true);\n+\t    if (tem)\n+\t      result = fold_convert (TREE_TYPE (rhs),\n+\t\t\t\t     build_fold_addr_expr_loc (loc, tem));\n+\t  }\n+\n+\telse if (TREE_CODE (rhs) == CONSTRUCTOR\n+\t\t && TREE_CODE (TREE_TYPE (rhs)) == VECTOR_TYPE\n+\t\t && (CONSTRUCTOR_NELTS (rhs)\n+\t\t     == TYPE_VECTOR_SUBPARTS (TREE_TYPE (rhs))))\n+\t  {\n+\t    /* Fold a constant vector CONSTRUCTOR to VECTOR_CST.  */\n+\t    unsigned i;\n+\t    tree val;\n+\n+\t    FOR_EACH_CONSTRUCTOR_VALUE (CONSTRUCTOR_ELTS (rhs), i, val)\n+\t      if (TREE_CODE (val) != INTEGER_CST\n+\t\t  && TREE_CODE (val) != REAL_CST\n+\t\t  && TREE_CODE (val) != FIXED_CST)\n+\t\treturn NULL_TREE;\n+\n+\t    return build_vector_from_ctor (TREE_TYPE (rhs),\n+\t\t\t\t\t   CONSTRUCTOR_ELTS (rhs));\n+\t  }\n+\n+\telse if (DECL_P (rhs))\n+\t  return unshare_expr (get_symbol_constant_value (rhs));\n+\n+        /* If we couldn't fold the RHS, hand over to the generic\n+           fold routines.  */\n+        if (result == NULL_TREE)\n+          result = fold (rhs);\n+\n+        /* Strip away useless type conversions.  Both the NON_LVALUE_EXPR\n+           that may have been added by fold, and \"useless\" type\n+           conversions that might now be apparent due to propagation.  */\n+        STRIP_USELESS_TYPE_CONVERSION (result);\n+\n+        if (result != rhs && valid_gimple_rhs_p (result))\n+\t  return result;\n+\n+\treturn NULL_TREE;\n+      }\n+      break;\n+\n+    case GIMPLE_UNARY_RHS:\n+      {\n+\ttree rhs = gimple_assign_rhs1 (stmt);\n+\n+\tresult = fold_unary_loc (loc, subcode, gimple_expr_type (stmt), rhs);\n+\tif (result)\n+\t  {\n+\t    /* If the operation was a conversion do _not_ mark a\n+\t       resulting constant with TREE_OVERFLOW if the original\n+\t       constant was not.  These conversions have implementation\n+\t       defined behavior and retaining the TREE_OVERFLOW flag\n+\t       here would confuse later passes such as VRP.  */\n+\t    if (CONVERT_EXPR_CODE_P (subcode)\n+\t\t&& TREE_CODE (result) == INTEGER_CST\n+\t\t&& TREE_CODE (rhs) == INTEGER_CST)\n+\t      TREE_OVERFLOW (result) = TREE_OVERFLOW (rhs);\n+\n+\t    STRIP_USELESS_TYPE_CONVERSION (result);\n+\t    if (valid_gimple_rhs_p (result))\n+\t      return result;\n+\t  }\n+\telse if (CONVERT_EXPR_CODE_P (subcode)\n+\t\t && POINTER_TYPE_P (gimple_expr_type (stmt))\n+\t\t && POINTER_TYPE_P (TREE_TYPE (gimple_assign_rhs1 (stmt))))\n+\t  {\n+\t    tree type = gimple_expr_type (stmt);\n+\t    tree t = maybe_fold_offset_to_address (loc,\n+\t\t\t\t\t\t   gimple_assign_rhs1 (stmt),\n+\t\t\t\t\t\t   integer_zero_node, type);\n+\t    if (t)\n+\t      return t;\n+\t  }\n+      }\n+      break;\n+\n+    case GIMPLE_BINARY_RHS:\n+      /* Try to fold pointer addition.  */\n+      if (gimple_assign_rhs_code (stmt) == POINTER_PLUS_EXPR)\n+\t{\n+\t  tree type = TREE_TYPE (gimple_assign_rhs1 (stmt));\n+\t  if (TREE_CODE (TREE_TYPE (type)) == ARRAY_TYPE)\n+\t    {\n+\t      type = build_pointer_type (TREE_TYPE (TREE_TYPE (type)));\n+\t      if (!useless_type_conversion_p\n+\t\t    (TREE_TYPE (gimple_assign_lhs (stmt)), type))\n+\t\ttype = TREE_TYPE (gimple_assign_rhs1 (stmt));\n+\t    }\n+\t  result = maybe_fold_stmt_addition (gimple_location (stmt),\n+\t\t\t\t\t     type,\n+\t\t\t\t\t     gimple_assign_rhs1 (stmt),\n+\t\t\t\t\t     gimple_assign_rhs2 (stmt));\n+\t}\n+\n+      if (!result)\n+        result = fold_binary_loc (loc, subcode,\n+                              TREE_TYPE (gimple_assign_lhs (stmt)),\n+                              gimple_assign_rhs1 (stmt),\n+                              gimple_assign_rhs2 (stmt));\n+\n+      if (result)\n+        {\n+          STRIP_USELESS_TYPE_CONVERSION (result);\n+          if (valid_gimple_rhs_p (result))\n+\t    return result;\n+\n+\t  /* Fold might have produced non-GIMPLE, so if we trust it blindly\n+\t     we lose canonicalization opportunities.  Do not go again\n+\t     through fold here though, or the same non-GIMPLE will be\n+\t     produced.  */\n+          if (commutative_tree_code (subcode)\n+              && tree_swap_operands_p (gimple_assign_rhs1 (stmt),\n+                                       gimple_assign_rhs2 (stmt), false))\n+            return build2 (subcode, TREE_TYPE (gimple_assign_lhs (stmt)),\n+                           gimple_assign_rhs2 (stmt),\n+                           gimple_assign_rhs1 (stmt));\n+        }\n+      break;\n+\n+    case GIMPLE_INVALID_RHS:\n+      gcc_unreachable ();\n+    }\n+\n+  return NULL_TREE;\n+}\n+\n+/* Attempt to fold a conditional statement. Return true if any changes were\n+   made. We only attempt to fold the condition expression, and do not perform\n+   any transformation that would require alteration of the cfg.  It is\n+   assumed that the operands have been previously folded.  */\n+\n+static bool\n+fold_gimple_cond (gimple stmt)\n+{\n+  tree result = fold_binary_loc (gimple_location (stmt),\n+\t\t\t     gimple_cond_code (stmt),\n+                             boolean_type_node,\n+                             gimple_cond_lhs (stmt),\n+                             gimple_cond_rhs (stmt));\n+\n+  if (result)\n+    {\n+      STRIP_USELESS_TYPE_CONVERSION (result);\n+      if (is_gimple_condexpr (result) && valid_gimple_rhs_p (result))\n+        {\n+          gimple_cond_set_condition_from_tree (stmt, result);\n+          return true;\n+        }\n+    }\n+\n+  return false;\n+}\n+\n+/* Convert EXPR into a GIMPLE value suitable for substitution on the\n+   RHS of an assignment.  Insert the necessary statements before\n+   iterator *SI_P.  The statement at *SI_P, which must be a GIMPLE_CALL\n+   is replaced.  If the call is expected to produces a result, then it\n+   is replaced by an assignment of the new RHS to the result variable.\n+   If the result is to be ignored, then the call is replaced by a\n+   GIMPLE_NOP.  */\n+\n+void\n+gimplify_and_update_call_from_tree (gimple_stmt_iterator *si_p, tree expr)\n+{\n+  tree lhs;\n+  tree tmp = NULL_TREE;  /* Silence warning.  */\n+  gimple stmt, new_stmt;\n+  gimple_stmt_iterator i;\n+  gimple_seq stmts = gimple_seq_alloc();\n+  struct gimplify_ctx gctx;\n+  gimple last = NULL;\n+\n+  stmt = gsi_stmt (*si_p);\n+\n+  gcc_assert (is_gimple_call (stmt));\n+\n+  lhs = gimple_call_lhs (stmt);\n+\n+  push_gimplify_context (&gctx);\n+\n+  if (lhs == NULL_TREE)\n+    gimplify_and_add (expr, &stmts);\n+  else\n+    tmp = get_initialized_tmp_var (expr, &stmts, NULL);\n+\n+  pop_gimplify_context (NULL);\n+\n+  if (gimple_has_location (stmt))\n+    annotate_all_with_location (stmts, gimple_location (stmt));\n+\n+  /* The replacement can expose previously unreferenced variables.  */\n+  for (i = gsi_start (stmts); !gsi_end_p (i); gsi_next (&i))\n+    {\n+      if (last)\n+\t{\n+\t  gsi_insert_before (si_p, last, GSI_NEW_STMT);\n+\t  gsi_next (si_p);\n+\t}\n+      new_stmt = gsi_stmt (i);\n+      find_new_referenced_vars (new_stmt);\n+      mark_symbols_for_renaming (new_stmt);\n+      last = new_stmt;\n+    }\n+\n+  if (lhs == NULL_TREE)\n+    {\n+      unlink_stmt_vdef (stmt);\n+      release_defs (stmt);\n+      new_stmt = last;\n+    }\n+  else\n+    {\n+      if (last)\n+\t{\n+\t  gsi_insert_before (si_p, last, GSI_NEW_STMT);\n+\t  gsi_next (si_p);\n+\t}\n+      new_stmt = gimple_build_assign (lhs, tmp);\n+      gimple_set_vuse (new_stmt, gimple_vuse (stmt));\n+      gimple_set_vdef (new_stmt, gimple_vdef (stmt));\n+      move_ssa_defining_stmt_for_defs (new_stmt, stmt);\n+    }\n+\n+  gimple_set_location (new_stmt, gimple_location (stmt));\n+  gsi_replace (si_p, new_stmt, false);\n+}\n+\n+/* Return the string length, maximum string length or maximum value of\n+   ARG in LENGTH.\n+   If ARG is an SSA name variable, follow its use-def chains.  If LENGTH\n+   is not NULL and, for TYPE == 0, its value is not equal to the length\n+   we determine or if we are unable to determine the length or value,\n+   return false.  VISITED is a bitmap of visited variables.\n+   TYPE is 0 if string length should be returned, 1 for maximum string\n+   length and 2 for maximum value ARG can have.  */\n+\n+static bool\n+get_maxval_strlen (tree arg, tree *length, bitmap visited, int type)\n+{\n+  tree var, val;\n+  gimple def_stmt;\n+\n+  if (TREE_CODE (arg) != SSA_NAME)\n+    {\n+      if (TREE_CODE (arg) == COND_EXPR)\n+        return get_maxval_strlen (COND_EXPR_THEN (arg), length, visited, type)\n+               && get_maxval_strlen (COND_EXPR_ELSE (arg), length, visited, type);\n+      /* We can end up with &(*iftmp_1)[0] here as well, so handle it.  */\n+      else if (TREE_CODE (arg) == ADDR_EXPR\n+\t       && TREE_CODE (TREE_OPERAND (arg, 0)) == ARRAY_REF\n+\t       && integer_zerop (TREE_OPERAND (TREE_OPERAND (arg, 0), 1)))\n+\t{\n+\t  tree aop0 = TREE_OPERAND (TREE_OPERAND (arg, 0), 0);\n+\t  if (TREE_CODE (aop0) == INDIRECT_REF\n+\t      && TREE_CODE (TREE_OPERAND (aop0, 0)) == SSA_NAME)\n+\t    return get_maxval_strlen (TREE_OPERAND (aop0, 0),\n+\t\t\t\t      length, visited, type);\n+\t}\n+\n+      if (type == 2)\n+\t{\n+\t  val = arg;\n+\t  if (TREE_CODE (val) != INTEGER_CST\n+\t      || tree_int_cst_sgn (val) < 0)\n+\t    return false;\n+\t}\n+      else\n+\tval = c_strlen (arg, 1);\n+      if (!val)\n+\treturn false;\n+\n+      if (*length)\n+\t{\n+\t  if (type > 0)\n+\t    {\n+\t      if (TREE_CODE (*length) != INTEGER_CST\n+\t\t  || TREE_CODE (val) != INTEGER_CST)\n+\t\treturn false;\n+\n+\t      if (tree_int_cst_lt (*length, val))\n+\t\t*length = val;\n+\t      return true;\n+\t    }\n+\t  else if (simple_cst_equal (val, *length) != 1)\n+\t    return false;\n+\t}\n+\n+      *length = val;\n+      return true;\n+    }\n+\n+  /* If we were already here, break the infinite cycle.  */\n+  if (bitmap_bit_p (visited, SSA_NAME_VERSION (arg)))\n+    return true;\n+  bitmap_set_bit (visited, SSA_NAME_VERSION (arg));\n+\n+  var = arg;\n+  def_stmt = SSA_NAME_DEF_STMT (var);\n+\n+  switch (gimple_code (def_stmt))\n+    {\n+      case GIMPLE_ASSIGN:\n+        /* The RHS of the statement defining VAR must either have a\n+           constant length or come from another SSA_NAME with a constant\n+           length.  */\n+        if (gimple_assign_single_p (def_stmt)\n+            || gimple_assign_unary_nop_p (def_stmt))\n+          {\n+            tree rhs = gimple_assign_rhs1 (def_stmt);\n+            return get_maxval_strlen (rhs, length, visited, type);\n+          }\n+        return false;\n+\n+      case GIMPLE_PHI:\n+\t{\n+\t  /* All the arguments of the PHI node must have the same constant\n+\t     length.  */\n+\t  unsigned i;\n+\n+\t  for (i = 0; i < gimple_phi_num_args (def_stmt); i++)\n+          {\n+            tree arg = gimple_phi_arg (def_stmt, i)->def;\n+\n+            /* If this PHI has itself as an argument, we cannot\n+               determine the string length of this argument.  However,\n+               if we can find a constant string length for the other\n+               PHI args then we can still be sure that this is a\n+               constant string length.  So be optimistic and just\n+               continue with the next argument.  */\n+            if (arg == gimple_phi_result (def_stmt))\n+              continue;\n+\n+            if (!get_maxval_strlen (arg, length, visited, type))\n+              return false;\n+          }\n+        }\n+        return true;\n+\n+      default:\n+        return false;\n+    }\n+}\n+\n+\n+/* Fold builtin call in statement STMT.  Returns a simplified tree.\n+   We may return a non-constant expression, including another call\n+   to a different function and with different arguments, e.g.,\n+   substituting memcpy for strcpy when the string length is known.\n+   Note that some builtins expand into inline code that may not\n+   be valid in GIMPLE.  Callers must take care.  */\n+\n+tree\n+gimple_fold_builtin (gimple stmt)\n+{\n+  tree result, val[3];\n+  tree callee, a;\n+  int arg_idx, type;\n+  bitmap visited;\n+  bool ignore;\n+  int nargs;\n+  location_t loc = gimple_location (stmt);\n+\n+  gcc_assert (is_gimple_call (stmt));\n+\n+  ignore = (gimple_call_lhs (stmt) == NULL);\n+\n+  /* First try the generic builtin folder.  If that succeeds, return the\n+     result directly.  */\n+  result = fold_call_stmt (stmt, ignore);\n+  if (result)\n+    {\n+      if (ignore)\n+\tSTRIP_NOPS (result);\n+      return result;\n+    }\n+\n+  /* Ignore MD builtins.  */\n+  callee = gimple_call_fndecl (stmt);\n+  if (DECL_BUILT_IN_CLASS (callee) == BUILT_IN_MD)\n+    return NULL_TREE;\n+\n+  /* If the builtin could not be folded, and it has no argument list,\n+     we're done.  */\n+  nargs = gimple_call_num_args (stmt);\n+  if (nargs == 0)\n+    return NULL_TREE;\n+\n+  /* Limit the work only for builtins we know how to simplify.  */\n+  switch (DECL_FUNCTION_CODE (callee))\n+    {\n+    case BUILT_IN_STRLEN:\n+    case BUILT_IN_FPUTS:\n+    case BUILT_IN_FPUTS_UNLOCKED:\n+      arg_idx = 0;\n+      type = 0;\n+      break;\n+    case BUILT_IN_STRCPY:\n+    case BUILT_IN_STRNCPY:\n+      arg_idx = 1;\n+      type = 0;\n+      break;\n+    case BUILT_IN_MEMCPY_CHK:\n+    case BUILT_IN_MEMPCPY_CHK:\n+    case BUILT_IN_MEMMOVE_CHK:\n+    case BUILT_IN_MEMSET_CHK:\n+    case BUILT_IN_STRNCPY_CHK:\n+      arg_idx = 2;\n+      type = 2;\n+      break;\n+    case BUILT_IN_STRCPY_CHK:\n+    case BUILT_IN_STPCPY_CHK:\n+      arg_idx = 1;\n+      type = 1;\n+      break;\n+    case BUILT_IN_SNPRINTF_CHK:\n+    case BUILT_IN_VSNPRINTF_CHK:\n+      arg_idx = 1;\n+      type = 2;\n+      break;\n+    default:\n+      return NULL_TREE;\n+    }\n+\n+  if (arg_idx >= nargs)\n+    return NULL_TREE;\n+\n+  /* Try to use the dataflow information gathered by the CCP process.  */\n+  visited = BITMAP_ALLOC (NULL);\n+  bitmap_clear (visited);\n+\n+  memset (val, 0, sizeof (val));\n+  a = gimple_call_arg (stmt, arg_idx);\n+  if (!get_maxval_strlen (a, &val[arg_idx], visited, type))\n+    val[arg_idx] = NULL_TREE;\n+\n+  BITMAP_FREE (visited);\n+\n+  result = NULL_TREE;\n+  switch (DECL_FUNCTION_CODE (callee))\n+    {\n+    case BUILT_IN_STRLEN:\n+      if (val[0] && nargs == 1)\n+\t{\n+\t  tree new_val =\n+              fold_convert (TREE_TYPE (gimple_call_lhs (stmt)), val[0]);\n+\n+\t  /* If the result is not a valid gimple value, or not a cast\n+\t     of a valid gimple value, then we can not use the result.  */\n+\t  if (is_gimple_val (new_val)\n+\t      || (is_gimple_cast (new_val)\n+\t\t  && is_gimple_val (TREE_OPERAND (new_val, 0))))\n+\t    return new_val;\n+\t}\n+      break;\n+\n+    case BUILT_IN_STRCPY:\n+      if (val[1] && is_gimple_val (val[1]) && nargs == 2)\n+\tresult = fold_builtin_strcpy (loc, callee,\n+                                      gimple_call_arg (stmt, 0),\n+                                      gimple_call_arg (stmt, 1),\n+\t\t\t\t      val[1]);\n+      break;\n+\n+    case BUILT_IN_STRNCPY:\n+      if (val[1] && is_gimple_val (val[1]) && nargs == 3)\n+\tresult = fold_builtin_strncpy (loc, callee,\n+                                       gimple_call_arg (stmt, 0),\n+                                       gimple_call_arg (stmt, 1),\n+                                       gimple_call_arg (stmt, 2),\n+\t\t\t\t       val[1]);\n+      break;\n+\n+    case BUILT_IN_FPUTS:\n+      if (nargs == 2)\n+\tresult = fold_builtin_fputs (loc, gimple_call_arg (stmt, 0),\n+\t\t\t\t     gimple_call_arg (stmt, 1),\n+\t\t\t\t     ignore, false, val[0]);\n+      break;\n+\n+    case BUILT_IN_FPUTS_UNLOCKED:\n+      if (nargs == 2)\n+\tresult = fold_builtin_fputs (loc, gimple_call_arg (stmt, 0),\n+\t\t\t\t     gimple_call_arg (stmt, 1),\n+\t\t\t\t     ignore, true, val[0]);\n+      break;\n+\n+    case BUILT_IN_MEMCPY_CHK:\n+    case BUILT_IN_MEMPCPY_CHK:\n+    case BUILT_IN_MEMMOVE_CHK:\n+    case BUILT_IN_MEMSET_CHK:\n+      if (val[2] && is_gimple_val (val[2]) && nargs == 4)\n+\tresult = fold_builtin_memory_chk (loc, callee,\n+                                          gimple_call_arg (stmt, 0),\n+                                          gimple_call_arg (stmt, 1),\n+                                          gimple_call_arg (stmt, 2),\n+                                          gimple_call_arg (stmt, 3),\n+\t\t\t\t\t  val[2], ignore,\n+\t\t\t\t\t  DECL_FUNCTION_CODE (callee));\n+      break;\n+\n+    case BUILT_IN_STRCPY_CHK:\n+    case BUILT_IN_STPCPY_CHK:\n+      if (val[1] && is_gimple_val (val[1]) && nargs == 3)\n+\tresult = fold_builtin_stxcpy_chk (loc, callee,\n+                                          gimple_call_arg (stmt, 0),\n+                                          gimple_call_arg (stmt, 1),\n+                                          gimple_call_arg (stmt, 2),\n+\t\t\t\t\t  val[1], ignore,\n+\t\t\t\t\t  DECL_FUNCTION_CODE (callee));\n+      break;\n+\n+    case BUILT_IN_STRNCPY_CHK:\n+      if (val[2] && is_gimple_val (val[2]) && nargs == 4)\n+\tresult = fold_builtin_strncpy_chk (loc, gimple_call_arg (stmt, 0),\n+                                           gimple_call_arg (stmt, 1),\n+                                           gimple_call_arg (stmt, 2),\n+                                           gimple_call_arg (stmt, 3),\n+\t\t\t\t\t   val[2]);\n+      break;\n+\n+    case BUILT_IN_SNPRINTF_CHK:\n+    case BUILT_IN_VSNPRINTF_CHK:\n+      if (val[1] && is_gimple_val (val[1]))\n+\tresult = gimple_fold_builtin_snprintf_chk (stmt, val[1],\n+                                                   DECL_FUNCTION_CODE (callee));\n+      break;\n+\n+    default:\n+      gcc_unreachable ();\n+    }\n+\n+  if (result && ignore)\n+    result = fold_ignored_result (result);\n+  return result;\n+}\n+\n+/* Attempt to fold a call statement referenced by the statement iterator GSI.\n+   The statement may be replaced by another statement, e.g., if the call\n+   simplifies to a constant value. Return true if any changes were made.\n+   It is assumed that the operands have been previously folded.  */\n+\n+static bool\n+fold_gimple_call (gimple_stmt_iterator *gsi)\n+{\n+  gimple stmt = gsi_stmt (*gsi);\n+\n+  tree callee = gimple_call_fndecl (stmt);\n+\n+  /* Check for builtins that CCP can handle using information not\n+     available in the generic fold routines.  */\n+  if (callee && DECL_BUILT_IN (callee))\n+    {\n+      tree result = gimple_fold_builtin (stmt);\n+\n+      if (result)\n+\t{\n+          if (!update_call_from_tree (gsi, result))\n+\t    gimplify_and_update_call_from_tree (gsi, result);\n+\t  return true;\n+\t}\n+    }\n+  else\n+    {\n+      /* Check for resolvable OBJ_TYPE_REF.  The only sorts we can resolve\n+         here are when we've propagated the address of a decl into the\n+         object slot.  */\n+      /* ??? Should perhaps do this in fold proper.  However, doing it\n+         there requires that we create a new CALL_EXPR, and that requires\n+         copying EH region info to the new node.  Easier to just do it\n+         here where we can just smash the call operand.  */\n+      /* ??? Is there a good reason not to do this in fold_stmt_inplace?  */\n+      callee = gimple_call_fn (stmt);\n+      if (TREE_CODE (callee) == OBJ_TYPE_REF\n+          && lang_hooks.fold_obj_type_ref\n+          && TREE_CODE (OBJ_TYPE_REF_OBJECT (callee)) == ADDR_EXPR\n+          && DECL_P (TREE_OPERAND\n+                     (OBJ_TYPE_REF_OBJECT (callee), 0)))\n+        {\n+          tree t;\n+\n+          /* ??? Caution: Broken ADDR_EXPR semantics means that\n+             looking at the type of the operand of the addr_expr\n+             can yield an array type.  See silly exception in\n+             check_pointer_types_r.  */\n+          t = TREE_TYPE (TREE_TYPE (OBJ_TYPE_REF_OBJECT (callee)));\n+          t = lang_hooks.fold_obj_type_ref (callee, t);\n+          if (t)\n+            {\n+              gimple_call_set_fn (stmt, t);\n+              return true;\n+            }\n+        }\n+    }\n+\n+  return false;\n+}\n+\n+/* Worker for both fold_stmt and fold_stmt_inplace.  The INPLACE argument\n+   distinguishes both cases.  */\n+\n+static bool\n+fold_stmt_1 (gimple_stmt_iterator *gsi, bool inplace)\n+{\n+  bool changed = false;\n+  gimple stmt = gsi_stmt (*gsi);\n+  unsigned i;\n+\n+  /* Fold the main computation performed by the statement.  */\n+  switch (gimple_code (stmt))\n+    {\n+    case GIMPLE_ASSIGN:\n+      {\n+\tunsigned old_num_ops = gimple_num_ops (stmt);\n+\ttree new_rhs = fold_gimple_assign (gsi);\n+\ttree lhs = gimple_assign_lhs (stmt);\n+\tif (new_rhs\n+\t    && !useless_type_conversion_p (TREE_TYPE (lhs),\n+\t\t\t\t\t   TREE_TYPE (new_rhs)))\n+\t  new_rhs = fold_convert (TREE_TYPE (lhs), new_rhs);\n+\tif (new_rhs\n+\t    && (!inplace\n+\t\t|| get_gimple_rhs_num_ops (TREE_CODE (new_rhs)) < old_num_ops))\n+\t  {\n+\t    gimple_assign_set_rhs_from_tree (gsi, new_rhs);\n+\t    changed = true;\n+\t  }\n+\tbreak;\n+      }\n+\n+    case GIMPLE_COND:\n+      changed |= fold_gimple_cond (stmt);\n+      break;\n+\n+    case GIMPLE_CALL:\n+      /* Fold *& in call arguments.  */\n+      for (i = 0; i < gimple_call_num_args (stmt); ++i)\n+\tif (REFERENCE_CLASS_P (gimple_call_arg (stmt, i)))\n+\t  {\n+\t    tree tmp = maybe_fold_reference (gimple_call_arg (stmt, i), false);\n+\t    if (tmp)\n+\t      {\n+\t\tgimple_call_set_arg (stmt, i, tmp);\n+\t\tchanged = true;\n+\t      }\n+\t  }\n+      /* The entire statement may be replaced in this case.  */\n+      if (!inplace)\n+\tchanged |= fold_gimple_call (gsi);\n+      break;\n+\n+    case GIMPLE_ASM:\n+      /* Fold *& in asm operands.  */\n+      for (i = 0; i < gimple_asm_noutputs (stmt); ++i)\n+\t{\n+\t  tree link = gimple_asm_output_op (stmt, i);\n+\t  tree op = TREE_VALUE (link);\n+\t  if (REFERENCE_CLASS_P (op)\n+\t      && (op = maybe_fold_reference (op, true)) != NULL_TREE)\n+\t    {\n+\t      TREE_VALUE (link) = op;\n+\t      changed = true;\n+\t    }\n+\t}\n+      for (i = 0; i < gimple_asm_ninputs (stmt); ++i)\n+\t{\n+\t  tree link = gimple_asm_input_op (stmt, i);\n+\t  tree op = TREE_VALUE (link);\n+\t  if (REFERENCE_CLASS_P (op)\n+\t      && (op = maybe_fold_reference (op, false)) != NULL_TREE)\n+\t    {\n+\t      TREE_VALUE (link) = op;\n+\t      changed = true;\n+\t    }\n+\t}\n+      break;\n+\n+    default:;\n+    }\n+\n+  stmt = gsi_stmt (*gsi);\n+\n+  /* Fold *& on the lhs.  */\n+  if (gimple_has_lhs (stmt))\n+    {\n+      tree lhs = gimple_get_lhs (stmt);\n+      if (lhs && REFERENCE_CLASS_P (lhs))\n+\t{\n+\t  tree new_lhs = maybe_fold_reference (lhs, true);\n+\t  if (new_lhs)\n+\t    {\n+\t      gimple_set_lhs (stmt, new_lhs);\n+\t      changed = true;\n+\t    }\n+\t}\n+    }\n+\n+  return changed;\n+}\n+\n+/* Fold the statement pointed to by GSI.  In some cases, this function may\n+   replace the whole statement with a new one.  Returns true iff folding\n+   makes any changes.\n+   The statement pointed to by GSI should be in valid gimple form but may\n+   be in unfolded state as resulting from for example constant propagation\n+   which can produce *&x = 0.  */\n+\n+bool\n+fold_stmt (gimple_stmt_iterator *gsi)\n+{\n+  return fold_stmt_1 (gsi, false);\n+}\n+\n+/* Perform the minimal folding on statement STMT.  Only operations like\n+   *&x created by constant propagation are handled.  The statement cannot\n+   be replaced with a new one.  Return true if the statement was\n+   changed, false otherwise.\n+   The statement STMT should be in valid gimple form but may\n+   be in unfolded state as resulting from for example constant propagation\n+   which can produce *&x = 0.  */\n+\n+bool\n+fold_stmt_inplace (gimple stmt)\n+{\n+  gimple_stmt_iterator gsi = gsi_for_stmt (stmt);\n+  bool changed = fold_stmt_1 (&gsi, true);\n+  gcc_assert (gsi_stmt (gsi) == stmt);\n+  return changed;\n+}\n+"}, {"sha": "18ebbb1e5605badd24e27ab99b5d97e991e48567", "filename": "gcc/gimple.h", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/gcc%2Fgimple.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/gcc%2Fgimple.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgimple.h?ref=cbdd87d44471ec0b9099dd8d6e32ae09039a1b48", "patch": "@@ -4787,4 +4787,16 @@ gimple_alloc_kind (enum gimple_code code)\n \n extern void dump_gimple_statistics (void);\n \n+/* In gimple-fold.c.  */\n+void gimplify_and_update_call_from_tree (gimple_stmt_iterator *, tree);\n+tree gimple_fold_builtin (gimple);\n+bool fold_stmt (gimple_stmt_iterator *);\n+bool fold_stmt_inplace (gimple);\n+tree maybe_fold_offset_to_reference (location_t, tree, tree, tree);\n+tree maybe_fold_offset_to_address (location_t, tree, tree, tree);\n+tree maybe_fold_stmt_addition (location_t, tree, tree, tree);\n+tree get_symbol_constant_value (tree);\n+bool may_propagate_address_into_dereference (tree, tree);\n+\n+\n #endif  /* GCC_GIMPLE_H */"}, {"sha": "2f9cf3df10acf814a869cb0784678f560e943964", "filename": "gcc/tree-flow.h", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/gcc%2Ftree-flow.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/gcc%2Ftree-flow.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-flow.h?ref=cbdd87d44471ec0b9099dd8d6e32ae09039a1b48", "patch": "@@ -614,12 +614,7 @@ extern void ssanames_print_statistics (void);\n #endif\n \n /* In tree-ssa-ccp.c  */\n-bool fold_stmt (gimple_stmt_iterator *);\n-bool fold_stmt_inplace (gimple);\n-tree get_symbol_constant_value (tree);\n tree fold_const_aggregate_ref (tree);\n-bool may_propagate_address_into_dereference (tree, tree);\n-\n \n /* In tree-ssa-dom.c  */\n extern void dump_dominator_optimization_stats (FILE *);"}, {"sha": "f688d10ccbc9424684e02751d7fae76387e3856b", "filename": "gcc/tree-ssa-ccp.c", "status": "modified", "additions": 1, "deletions": 1553, "changes": 1554, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/gcc%2Ftree-ssa-ccp.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/gcc%2Ftree-ssa-ccp.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-ccp.c?ref=cbdd87d44471ec0b9099dd8d6e32ae09039a1b48", "patch": "@@ -269,50 +269,6 @@ debug_lattice_value (prop_value_t val)\n }\n \n \n-\n-/* If SYM is a constant variable with known value, return the value.\n-   NULL_TREE is returned otherwise.  */\n-\n-tree\n-get_symbol_constant_value (tree sym)\n-{\n-  if (TREE_STATIC (sym)\n-      && (TREE_READONLY (sym)\n-\t  || TREE_CODE (sym) == CONST_DECL))\n-    {\n-      tree val = DECL_INITIAL (sym);\n-      if (val)\n-\t{\n-\t  STRIP_NOPS (val);\n-\t  if (is_gimple_min_invariant (val))\n-\t    {\n-\t      if (TREE_CODE (val) == ADDR_EXPR)\n-\t\t{\n-\t\t  tree base = get_base_address (TREE_OPERAND (val, 0));\n-\t\t  if (base && TREE_CODE (base) == VAR_DECL)\n-\t\t    {\n-\t\t      TREE_ADDRESSABLE (base) = 1;\n-\t\t      if (gimple_referenced_vars (cfun))\n-\t\t\tadd_referenced_var (base);\n-\t\t    }\n-\t\t}\n-\t      return val;\n-\t    }\n-\t}\n-      /* Variables declared 'const' without an initializer\n-\t have zero as the initializer if they may not be\n-\t overridden at link or run time.  */\n-      if (!val\n-\t  && !DECL_EXTERNAL (sym)\n-\t  && targetm.binds_local_p (sym)\n-          && (INTEGRAL_TYPE_P (TREE_TYPE (sym))\n-\t       || SCALAR_FLOAT_TYPE_P (TREE_TYPE (sym))))\n-\treturn fold_convert (TREE_TYPE (sym), integer_zero_node);\n-    }\n-\n-  return NULL_TREE;\n-}\n-\n /* Compute a default value for variable VAR and store it in the\n    CONST_VAL array.  The following rules are used to get default\n    values:\n@@ -885,36 +841,6 @@ ccp_visit_phi_node (gimple phi)\n     return SSA_PROP_NOT_INTERESTING;\n }\n \n-/* Return true if we may propagate the address expression ADDR into the\n-   dereference DEREF and cancel them.  */\n-\n-bool\n-may_propagate_address_into_dereference (tree addr, tree deref)\n-{\n-  gcc_assert (INDIRECT_REF_P (deref)\n-\t      && TREE_CODE (addr) == ADDR_EXPR);\n-\n-  /* Don't propagate if ADDR's operand has incomplete type.  */\n-  if (!COMPLETE_TYPE_P (TREE_TYPE (TREE_OPERAND (addr, 0))))\n-    return false;\n-\n-  /* If the address is invariant then we do not need to preserve restrict\n-     qualifications.  But we do need to preserve volatile qualifiers until\n-     we can annotate the folded dereference itself properly.  */\n-  if (is_gimple_min_invariant (addr)\n-      && (!TREE_THIS_VOLATILE (deref)\n-\t  || TYPE_VOLATILE (TREE_TYPE (addr))))\n-    return useless_type_conversion_p (TREE_TYPE (deref),\n-\t\t\t\t      TREE_TYPE (TREE_OPERAND (addr, 0)));\n-\n-  /* Else both the address substitution and the folding must result in\n-     a valid useless type conversion sequence.  */\n-  return (useless_type_conversion_p (TREE_TYPE (TREE_OPERAND (deref, 0)),\n-\t\t\t\t     TREE_TYPE (addr))\n-\t  && useless_type_conversion_p (TREE_TYPE (deref),\n-\t\t\t\t\tTREE_TYPE (TREE_OPERAND (addr, 0))));\n-}\n-\n /* CCP specific front-end to the non-destructive constant folding\n    routines.\n \n@@ -1771,1410 +1697,6 @@ struct gimple_opt_pass pass_ccp =\n };\n \n \n-/* A subroutine of fold_stmt.  Attempts to fold *(A+O) to A[X].\n-   BASE is an array type.  OFFSET is a byte displacement.  ORIG_TYPE\n-   is the desired result type.\n-\n-   LOC is the location of the original expression.  */\n-\n-static tree\n-maybe_fold_offset_to_array_ref (location_t loc, tree base, tree offset,\n-\t\t\t\ttree orig_type,\n-\t\t\t\tbool allow_negative_idx)\n-{\n-  tree min_idx, idx, idx_type, elt_offset = integer_zero_node;\n-  tree array_type, elt_type, elt_size;\n-  tree domain_type;\n-\n-  /* If BASE is an ARRAY_REF, we can pick up another offset (this time\n-     measured in units of the size of elements type) from that ARRAY_REF).\n-     We can't do anything if either is variable.\n-\n-     The case we handle here is *(&A[N]+O).  */\n-  if (TREE_CODE (base) == ARRAY_REF)\n-    {\n-      tree low_bound = array_ref_low_bound (base);\n-\n-      elt_offset = TREE_OPERAND (base, 1);\n-      if (TREE_CODE (low_bound) != INTEGER_CST\n-\t  || TREE_CODE (elt_offset) != INTEGER_CST)\n-\treturn NULL_TREE;\n-\n-      elt_offset = int_const_binop (MINUS_EXPR, elt_offset, low_bound, 0);\n-      base = TREE_OPERAND (base, 0);\n-    }\n-\n-  /* Ignore stupid user tricks of indexing non-array variables.  */\n-  array_type = TREE_TYPE (base);\n-  if (TREE_CODE (array_type) != ARRAY_TYPE)\n-    return NULL_TREE;\n-  elt_type = TREE_TYPE (array_type);\n-  if (!useless_type_conversion_p (orig_type, elt_type))\n-    return NULL_TREE;\n-\n-  /* Use signed size type for intermediate computation on the index.  */\n-  idx_type = signed_type_for (size_type_node);\n-\n-  /* If OFFSET and ELT_OFFSET are zero, we don't care about the size of the\n-     element type (so we can use the alignment if it's not constant).\n-     Otherwise, compute the offset as an index by using a division.  If the\n-     division isn't exact, then don't do anything.  */\n-  elt_size = TYPE_SIZE_UNIT (elt_type);\n-  if (!elt_size)\n-    return NULL;\n-  if (integer_zerop (offset))\n-    {\n-      if (TREE_CODE (elt_size) != INTEGER_CST)\n-\telt_size = size_int (TYPE_ALIGN (elt_type));\n-\n-      idx = build_int_cst (idx_type, 0);\n-    }\n-  else\n-    {\n-      unsigned HOST_WIDE_INT lquo, lrem;\n-      HOST_WIDE_INT hquo, hrem;\n-      double_int soffset;\n-\n-      /* The final array offset should be signed, so we need\n-\t to sign-extend the (possibly pointer) offset here\n-\t and use signed division.  */\n-      soffset = double_int_sext (tree_to_double_int (offset),\n-\t\t\t\t TYPE_PRECISION (TREE_TYPE (offset)));\n-      if (TREE_CODE (elt_size) != INTEGER_CST\n-\t  || div_and_round_double (TRUNC_DIV_EXPR, 0,\n-\t\t\t\t   soffset.low, soffset.high,\n-\t\t\t\t   TREE_INT_CST_LOW (elt_size),\n-\t\t\t\t   TREE_INT_CST_HIGH (elt_size),\n-\t\t\t\t   &lquo, &hquo, &lrem, &hrem)\n-\t  || lrem || hrem)\n-\treturn NULL_TREE;\n-\n-      idx = build_int_cst_wide (idx_type, lquo, hquo);\n-    }\n-\n-  /* Assume the low bound is zero.  If there is a domain type, get the\n-     low bound, if any, convert the index into that type, and add the\n-     low bound.  */\n-  min_idx = build_int_cst (idx_type, 0);\n-  domain_type = TYPE_DOMAIN (array_type);\n-  if (domain_type)\n-    {\n-      idx_type = domain_type;\n-      if (TYPE_MIN_VALUE (idx_type))\n-\tmin_idx = TYPE_MIN_VALUE (idx_type);\n-      else\n-\tmin_idx = fold_convert (idx_type, min_idx);\n-\n-      if (TREE_CODE (min_idx) != INTEGER_CST)\n-\treturn NULL_TREE;\n-\n-      elt_offset = fold_convert (idx_type, elt_offset);\n-    }\n-\n-  if (!integer_zerop (min_idx))\n-    idx = int_const_binop (PLUS_EXPR, idx, min_idx, 0);\n-  if (!integer_zerop (elt_offset))\n-    idx = int_const_binop (PLUS_EXPR, idx, elt_offset, 0);\n-\n-  /* Make sure to possibly truncate late after offsetting.  */\n-  idx = fold_convert (idx_type, idx);\n-\n-  /* We don't want to construct access past array bounds. For example\n-       char *(c[4]);\n-       c[3][2];\n-     should not be simplified into (*c)[14] or tree-vrp will\n-     give false warnings.  The same is true for\n-       struct A { long x; char d[0]; } *a;\n-       (char *)a - 4;\n-     which should be not folded to &a->d[-8].  */\n-  if (domain_type\n-      && TYPE_MAX_VALUE (domain_type)\n-      && TREE_CODE (TYPE_MAX_VALUE (domain_type)) == INTEGER_CST)\n-    {\n-      tree up_bound = TYPE_MAX_VALUE (domain_type);\n-\n-      if (tree_int_cst_lt (up_bound, idx)\n-\t  /* Accesses after the end of arrays of size 0 (gcc\n-\t     extension) and 1 are likely intentional (\"struct\n-\t     hack\").  */\n-\t  && compare_tree_int (up_bound, 1) > 0)\n-\treturn NULL_TREE;\n-    }\n-  if (domain_type\n-      && TYPE_MIN_VALUE (domain_type))\n-    {\n-      if (!allow_negative_idx\n-\t  && TREE_CODE (TYPE_MIN_VALUE (domain_type)) == INTEGER_CST\n-\t  && tree_int_cst_lt (idx, TYPE_MIN_VALUE (domain_type)))\n-\treturn NULL_TREE;\n-    }\n-  else if (!allow_negative_idx\n-\t   && compare_tree_int (idx, 0) < 0)\n-    return NULL_TREE;\n-\n-  {\n-    tree t = build4 (ARRAY_REF, elt_type, base, idx, NULL_TREE, NULL_TREE);\n-    SET_EXPR_LOCATION (t, loc);\n-    return t;\n-  }\n-}\n-\n-\n-/* Attempt to fold *(S+O) to S.X.\n-   BASE is a record type.  OFFSET is a byte displacement.  ORIG_TYPE\n-   is the desired result type.\n-\n-   LOC is the location of the original expression.  */\n-\n-static tree\n-maybe_fold_offset_to_component_ref (location_t loc, tree record_type,\n-\t\t\t\t    tree base, tree offset, tree orig_type)\n-{\n-  tree f, t, field_type, tail_array_field, field_offset;\n-  tree ret;\n-  tree new_base;\n-\n-  if (TREE_CODE (record_type) != RECORD_TYPE\n-      && TREE_CODE (record_type) != UNION_TYPE\n-      && TREE_CODE (record_type) != QUAL_UNION_TYPE)\n-    return NULL_TREE;\n-\n-  /* Short-circuit silly cases.  */\n-  if (useless_type_conversion_p (record_type, orig_type))\n-    return NULL_TREE;\n-\n-  tail_array_field = NULL_TREE;\n-  for (f = TYPE_FIELDS (record_type); f ; f = TREE_CHAIN (f))\n-    {\n-      int cmp;\n-\n-      if (TREE_CODE (f) != FIELD_DECL)\n-\tcontinue;\n-      if (DECL_BIT_FIELD (f))\n-\tcontinue;\n-\n-      if (!DECL_FIELD_OFFSET (f))\n-\tcontinue;\n-      field_offset = byte_position (f);\n-      if (TREE_CODE (field_offset) != INTEGER_CST)\n-\tcontinue;\n-\n-      /* ??? Java creates \"interesting\" fields for representing base classes.\n-\t They have no name, and have no context.  With no context, we get into\n-\t trouble with nonoverlapping_component_refs_p.  Skip them.  */\n-      if (!DECL_FIELD_CONTEXT (f))\n-\tcontinue;\n-\n-      /* The previous array field isn't at the end.  */\n-      tail_array_field = NULL_TREE;\n-\n-      /* Check to see if this offset overlaps with the field.  */\n-      cmp = tree_int_cst_compare (field_offset, offset);\n-      if (cmp > 0)\n-\tcontinue;\n-\n-      field_type = TREE_TYPE (f);\n-\n-      /* Here we exactly match the offset being checked.  If the types match,\n-\t then we can return that field.  */\n-      if (cmp == 0\n-\t  && useless_type_conversion_p (orig_type, field_type))\n-\t{\n-\t  t = fold_build3 (COMPONENT_REF, field_type, base, f, NULL_TREE);\n-\t  return t;\n-\t}\n-\n-      /* Don't care about offsets into the middle of scalars.  */\n-      if (!AGGREGATE_TYPE_P (field_type))\n-\tcontinue;\n-\n-      /* Check for array at the end of the struct.  This is often\n-\t used as for flexible array members.  We should be able to\n-\t turn this into an array access anyway.  */\n-      if (TREE_CODE (field_type) == ARRAY_TYPE)\n-\ttail_array_field = f;\n-\n-      /* Check the end of the field against the offset.  */\n-      if (!DECL_SIZE_UNIT (f)\n-\t  || TREE_CODE (DECL_SIZE_UNIT (f)) != INTEGER_CST)\n-\tcontinue;\n-      t = int_const_binop (MINUS_EXPR, offset, field_offset, 1);\n-      if (!tree_int_cst_lt (t, DECL_SIZE_UNIT (f)))\n-\tcontinue;\n-\n-      /* If we matched, then set offset to the displacement into\n-\t this field.  */\n-      new_base = fold_build3 (COMPONENT_REF, field_type, base, f, NULL_TREE);\n-      SET_EXPR_LOCATION (new_base, loc);\n-\n-      /* Recurse to possibly find the match.  */\n-      ret = maybe_fold_offset_to_array_ref (loc, new_base, t, orig_type,\n-\t\t\t\t\t    f == TYPE_FIELDS (record_type));\n-      if (ret)\n-\treturn ret;\n-      ret = maybe_fold_offset_to_component_ref (loc, field_type, new_base, t,\n-\t\t\t\t\t\torig_type);\n-      if (ret)\n-\treturn ret;\n-    }\n-\n-  if (!tail_array_field)\n-    return NULL_TREE;\n-\n-  f = tail_array_field;\n-  field_type = TREE_TYPE (f);\n-  offset = int_const_binop (MINUS_EXPR, offset, byte_position (f), 1);\n-\n-  /* If we get here, we've got an aggregate field, and a possibly\n-     nonzero offset into them.  Recurse and hope for a valid match.  */\n-  base = fold_build3 (COMPONENT_REF, field_type, base, f, NULL_TREE);\n-  SET_EXPR_LOCATION (base, loc);\n-\n-  t = maybe_fold_offset_to_array_ref (loc, base, offset, orig_type,\n-\t\t\t\t      f == TYPE_FIELDS (record_type));\n-  if (t)\n-    return t;\n-  return maybe_fold_offset_to_component_ref (loc, field_type, base, offset,\n-\t\t\t\t\t     orig_type);\n-}\n-\n-/* Attempt to express (ORIG_TYPE)BASE+OFFSET as BASE->field_of_orig_type\n-   or BASE[index] or by combination of those.\n-\n-   LOC is the location of original expression.\n-\n-   Before attempting the conversion strip off existing ADDR_EXPRs and\n-   handled component refs.  */\n-\n-tree\n-maybe_fold_offset_to_reference (location_t loc, tree base, tree offset,\n-\t\t\t\ttree orig_type)\n-{\n-  tree ret;\n-  tree type;\n-\n-  STRIP_NOPS (base);\n-  if (TREE_CODE (base) != ADDR_EXPR)\n-    return NULL_TREE;\n-\n-  base = TREE_OPERAND (base, 0);\n-\n-  /* Handle case where existing COMPONENT_REF pick e.g. wrong field of union,\n-     so it needs to be removed and new COMPONENT_REF constructed.\n-     The wrong COMPONENT_REF are often constructed by folding the\n-     (type *)&object within the expression (type *)&object+offset  */\n-  if (handled_component_p (base))\n-    {\n-      HOST_WIDE_INT sub_offset, size, maxsize;\n-      tree newbase;\n-      newbase = get_ref_base_and_extent (base, &sub_offset,\n-\t\t\t\t\t &size, &maxsize);\n-      gcc_assert (newbase);\n-      if (size == maxsize\n-\t  && size != -1\n-\t  && !(sub_offset & (BITS_PER_UNIT - 1)))\n-\t{\n-\t  base = newbase;\n-\t  if (sub_offset)\n-\t    offset = int_const_binop (PLUS_EXPR, offset,\n-\t\t\t\t      build_int_cst (TREE_TYPE (offset),\n-\t\t\t\t\t\t     sub_offset / BITS_PER_UNIT), 1);\n-\t}\n-    }\n-  if (useless_type_conversion_p (orig_type, TREE_TYPE (base))\n-      && integer_zerop (offset))\n-    return base;\n-  type = TREE_TYPE (base);\n-\n-  ret = maybe_fold_offset_to_component_ref (loc, type, base, offset, orig_type);\n-  if (!ret)\n-    ret = maybe_fold_offset_to_array_ref (loc, base, offset, orig_type, true);\n-\n-  return ret;\n-}\n-\n-/* Attempt to express (ORIG_TYPE)&BASE+OFFSET as &BASE->field_of_orig_type\n-   or &BASE[index] or by combination of those.\n-\n-   LOC is the location of the original expression.\n-\n-   Before attempting the conversion strip off existing component refs.  */\n-\n-tree\n-maybe_fold_offset_to_address (location_t loc, tree addr, tree offset,\n-\t\t\t      tree orig_type)\n-{\n-  tree t;\n-\n-  gcc_assert (POINTER_TYPE_P (TREE_TYPE (addr))\n-\t      && POINTER_TYPE_P (orig_type));\n-\n-  t = maybe_fold_offset_to_reference (loc, addr, offset,\n-\t\t\t\t      TREE_TYPE (orig_type));\n-  if (t != NULL_TREE)\n-    {\n-      tree orig = addr;\n-      tree ptr_type;\n-\n-      /* For __builtin_object_size to function correctly we need to\n-         make sure not to fold address arithmetic so that we change\n-\t reference from one array to another.  This would happen for\n-\t example for\n-\n-\t   struct X { char s1[10]; char s2[10] } s;\n-\t   char *foo (void) { return &s.s2[-4]; }\n-\n-\t where we need to avoid generating &s.s1[6].  As the C and\n-\t C++ frontends create different initial trees\n-\t (char *) &s.s1 + -4  vs.  &s.s1[-4]  we have to do some\n-\t sophisticated comparisons here.  Note that checking for the\n-\t condition after the fact is easier than trying to avoid doing\n-\t the folding.  */\n-      STRIP_NOPS (orig);\n-      if (TREE_CODE (orig) == ADDR_EXPR)\n-\torig = TREE_OPERAND (orig, 0);\n-      if ((TREE_CODE (orig) == ARRAY_REF\n-\t   || (TREE_CODE (orig) == COMPONENT_REF\n-\t       && TREE_CODE (TREE_TYPE (TREE_OPERAND (orig, 1))) == ARRAY_TYPE))\n-\t  && (TREE_CODE (t) == ARRAY_REF\n-\t      || TREE_CODE (t) == COMPONENT_REF)\n-\t  && !operand_equal_p (TREE_CODE (orig) == ARRAY_REF\n-\t\t\t       ? TREE_OPERAND (orig, 0) : orig,\n-\t\t\t       TREE_CODE (t) == ARRAY_REF\n-\t\t\t       ? TREE_OPERAND (t, 0) : t, 0))\n-\treturn NULL_TREE;\n-\n-      ptr_type = build_pointer_type (TREE_TYPE (t));\n-      if (!useless_type_conversion_p (orig_type, ptr_type))\n-\treturn NULL_TREE;\n-      return build_fold_addr_expr_with_type_loc (loc, t, ptr_type);\n-    }\n-\n-  return NULL_TREE;\n-}\n-\n-/* A subroutine of fold_stmt.  Attempt to simplify *(BASE+OFFSET).\n-   Return the simplified expression, or NULL if nothing could be done.  */\n-\n-static tree\n-maybe_fold_stmt_indirect (tree expr, tree base, tree offset)\n-{\n-  tree t;\n-  bool volatile_p = TREE_THIS_VOLATILE (expr);\n-  location_t loc = EXPR_LOCATION (expr);\n-\n-  /* We may well have constructed a double-nested PLUS_EXPR via multiple\n-     substitutions.  Fold that down to one.  Remove NON_LVALUE_EXPRs that\n-     are sometimes added.  */\n-  base = fold (base);\n-  STRIP_TYPE_NOPS (base);\n-  TREE_OPERAND (expr, 0) = base;\n-\n-  /* One possibility is that the address reduces to a string constant.  */\n-  t = fold_read_from_constant_string (expr);\n-  if (t)\n-    return t;\n-\n-  /* Add in any offset from a POINTER_PLUS_EXPR.  */\n-  if (TREE_CODE (base) == POINTER_PLUS_EXPR)\n-    {\n-      tree offset2;\n-\n-      offset2 = TREE_OPERAND (base, 1);\n-      if (TREE_CODE (offset2) != INTEGER_CST)\n-\treturn NULL_TREE;\n-      base = TREE_OPERAND (base, 0);\n-\n-      offset = fold_convert (sizetype,\n-\t\t\t     int_const_binop (PLUS_EXPR, offset, offset2, 1));\n-    }\n-\n-  if (TREE_CODE (base) == ADDR_EXPR)\n-    {\n-      tree base_addr = base;\n-\n-      /* Strip the ADDR_EXPR.  */\n-      base = TREE_OPERAND (base, 0);\n-\n-      /* Fold away CONST_DECL to its value, if the type is scalar.  */\n-      if (TREE_CODE (base) == CONST_DECL\n-\t  && is_gimple_min_invariant (DECL_INITIAL (base)))\n-\treturn DECL_INITIAL (base);\n-\n-      /* If there is no offset involved simply return the folded base.  */\n-      if (integer_zerop (offset))\n-\treturn base;\n-\n-      /* Try folding *(&B+O) to B.X.  */\n-      t = maybe_fold_offset_to_reference (loc, base_addr, offset,\n-\t\t\t\t\t  TREE_TYPE (expr));\n-      if (t)\n-\t{\n-\t  /* Preserve volatileness of the original expression.\n-\t     We can end up with a plain decl here which is shared\n-\t     and we shouldn't mess with its flags.  */\n-\t  if (!SSA_VAR_P (t))\n-\t    TREE_THIS_VOLATILE (t) = volatile_p;\n-\t  return t;\n-\t}\n-    }\n-  else\n-    {\n-      /* We can get here for out-of-range string constant accesses,\n-\t such as \"_\"[3].  Bail out of the entire substitution search\n-\t and arrange for the entire statement to be replaced by a\n-\t call to __builtin_trap.  In all likelihood this will all be\n-\t constant-folded away, but in the meantime we can't leave with\n-\t something that get_expr_operands can't understand.  */\n-\n-      t = base;\n-      STRIP_NOPS (t);\n-      if (TREE_CODE (t) == ADDR_EXPR\n-\t  && TREE_CODE (TREE_OPERAND (t, 0)) == STRING_CST)\n-\t{\n-\t  /* FIXME: Except that this causes problems elsewhere with dead\n-\t     code not being deleted, and we die in the rtl expanders\n-\t     because we failed to remove some ssa_name.  In the meantime,\n-\t     just return zero.  */\n-\t  /* FIXME2: This condition should be signaled by\n-\t     fold_read_from_constant_string directly, rather than\n-\t     re-checking for it here.  */\n-\t  return integer_zero_node;\n-\t}\n-\n-      /* Try folding *(B+O) to B->X.  Still an improvement.  */\n-      if (POINTER_TYPE_P (TREE_TYPE (base)))\n-\t{\n-          t = maybe_fold_offset_to_reference (loc, base, offset,\n-\t\t\t\t              TREE_TYPE (expr));\n-\t  if (t)\n-\t    return t;\n-\t}\n-    }\n-\n-  /* Otherwise we had an offset that we could not simplify.  */\n-  return NULL_TREE;\n-}\n-\n-\n-/* A quaint feature extant in our address arithmetic is that there\n-   can be hidden type changes here.  The type of the result need\n-   not be the same as the type of the input pointer.\n-\n-   What we're after here is an expression of the form\n-\t(T *)(&array + const)\n-   where array is OP0, const is OP1, RES_TYPE is T and\n-   the cast doesn't actually exist, but is implicit in the\n-   type of the POINTER_PLUS_EXPR.  We'd like to turn this into\n-\t&array[x]\n-   which may be able to propagate further.  */\n-\n-tree\n-maybe_fold_stmt_addition (location_t loc, tree res_type, tree op0, tree op1)\n-{\n-  tree ptd_type;\n-  tree t;\n-\n-  /* The first operand should be an ADDR_EXPR.  */\n-  if (TREE_CODE (op0) != ADDR_EXPR)\n-    return NULL_TREE;\n-  op0 = TREE_OPERAND (op0, 0);\n-\n-  /* It had better be a constant.  */\n-  if (TREE_CODE (op1) != INTEGER_CST)\n-    {\n-      /* Or op0 should now be A[0] and the non-constant offset defined\n-\t via a multiplication by the array element size.  */\n-      if (TREE_CODE (op0) == ARRAY_REF\n-\t  && integer_zerop (TREE_OPERAND (op0, 1))\n-\t  && TREE_CODE (op1) == SSA_NAME\n-\t  && host_integerp (TYPE_SIZE_UNIT (TREE_TYPE (op0)), 1))\n-\t{\n-\t  gimple offset_def = SSA_NAME_DEF_STMT (op1);\n-\t  if (!is_gimple_assign (offset_def))\n-\t    return NULL_TREE;\n-\n-\t  if (gimple_assign_rhs_code (offset_def) == MULT_EXPR\n-\t      && TREE_CODE (gimple_assign_rhs2 (offset_def)) == INTEGER_CST\n-\t      && tree_int_cst_equal (gimple_assign_rhs2 (offset_def),\n-\t\t\t\t     TYPE_SIZE_UNIT (TREE_TYPE (op0))))\n-\t    return build_fold_addr_expr\n-\t\t\t  (build4 (ARRAY_REF, TREE_TYPE (op0),\n-\t\t\t\t   TREE_OPERAND (op0, 0),\n-\t\t\t\t   gimple_assign_rhs1 (offset_def),\n-\t\t\t\t   TREE_OPERAND (op0, 2),\n-\t\t\t\t   TREE_OPERAND (op0, 3)));\n-\t  else if (integer_onep (TYPE_SIZE_UNIT (TREE_TYPE (op0)))\n-\t\t   && gimple_assign_rhs_code (offset_def) != MULT_EXPR)\n-\t    return build_fold_addr_expr\n-\t\t\t  (build4 (ARRAY_REF, TREE_TYPE (op0),\n-\t\t\t\t   TREE_OPERAND (op0, 0),\n-\t\t\t\t   op1,\n-\t\t\t\t   TREE_OPERAND (op0, 2),\n-\t\t\t\t   TREE_OPERAND (op0, 3)));\n-\t}\n-      return NULL_TREE;\n-    }\n-\n-  /* If the first operand is an ARRAY_REF, expand it so that we can fold\n-     the offset into it.  */\n-  while (TREE_CODE (op0) == ARRAY_REF)\n-    {\n-      tree array_obj = TREE_OPERAND (op0, 0);\n-      tree array_idx = TREE_OPERAND (op0, 1);\n-      tree elt_type = TREE_TYPE (op0);\n-      tree elt_size = TYPE_SIZE_UNIT (elt_type);\n-      tree min_idx;\n-\n-      if (TREE_CODE (array_idx) != INTEGER_CST)\n-\tbreak;\n-      if (TREE_CODE (elt_size) != INTEGER_CST)\n-\tbreak;\n-\n-      /* Un-bias the index by the min index of the array type.  */\n-      min_idx = TYPE_DOMAIN (TREE_TYPE (array_obj));\n-      if (min_idx)\n-\t{\n-\t  min_idx = TYPE_MIN_VALUE (min_idx);\n-\t  if (min_idx)\n-\t    {\n-\t      if (TREE_CODE (min_idx) != INTEGER_CST)\n-\t\tbreak;\n-\n-\t      array_idx = fold_convert (TREE_TYPE (min_idx), array_idx);\n-\t      if (!integer_zerop (min_idx))\n-\t\tarray_idx = int_const_binop (MINUS_EXPR, array_idx,\n-\t\t\t\t\t     min_idx, 0);\n-\t    }\n-\t}\n-\n-      /* Convert the index to a byte offset.  */\n-      array_idx = fold_convert (sizetype, array_idx);\n-      array_idx = int_const_binop (MULT_EXPR, array_idx, elt_size, 0);\n-\n-      /* Update the operands for the next round, or for folding.  */\n-      op1 = int_const_binop (PLUS_EXPR,\n-\t\t\t     array_idx, op1, 0);\n-      op0 = array_obj;\n-    }\n-\n-  ptd_type = TREE_TYPE (res_type);\n-  /* If we want a pointer to void, reconstruct the reference from the\n-     array element type.  A pointer to that can be trivially converted\n-     to void *.  This happens as we fold (void *)(ptr p+ off).  */\n-  if (VOID_TYPE_P (ptd_type)\n-      && TREE_CODE (TREE_TYPE (op0)) == ARRAY_TYPE)\n-    ptd_type = TREE_TYPE (TREE_TYPE (op0));\n-\n-  /* At which point we can try some of the same things as for indirects.  */\n-  t = maybe_fold_offset_to_array_ref (loc, op0, op1, ptd_type, true);\n-  if (!t)\n-    t = maybe_fold_offset_to_component_ref (loc, TREE_TYPE (op0), op0, op1,\n-\t\t\t\t\t    ptd_type);\n-  if (t)\n-    {\n-      t = build1 (ADDR_EXPR, res_type, t);\n-      SET_EXPR_LOCATION (t, loc);\n-    }\n-\n-  return t;\n-}\n-\n-/* Subroutine of fold_stmt.  We perform several simplifications of the\n-   memory reference tree EXPR and make sure to re-gimplify them properly\n-   after propagation of constant addresses.  IS_LHS is true if the\n-   reference is supposed to be an lvalue.  */\n-\n-static tree\n-maybe_fold_reference (tree expr, bool is_lhs)\n-{\n-  tree *t = &expr;\n-\n-  if (TREE_CODE (expr) == ARRAY_REF\n-      && !is_lhs)\n-    {\n-      tree tem = fold_read_from_constant_string (expr);\n-      if (tem)\n-\treturn tem;\n-    }\n-\n-  /* ???  We might want to open-code the relevant remaining cases\n-     to avoid using the generic fold.  */\n-  if (handled_component_p (*t)\n-      && CONSTANT_CLASS_P (TREE_OPERAND (*t, 0)))\n-    {\n-      tree tem = fold (*t);\n-      if (tem != *t)\n-\treturn tem;\n-    }\n-\n-  while (handled_component_p (*t))\n-    t = &TREE_OPERAND (*t, 0);\n-\n-  if (TREE_CODE (*t) == INDIRECT_REF)\n-    {\n-      tree tem = maybe_fold_stmt_indirect (*t, TREE_OPERAND (*t, 0),\n-\t\t\t\t\t   integer_zero_node);\n-      /* Avoid folding *\"abc\" = 5 into 'a' = 5.  */\n-      if (is_lhs && tem && CONSTANT_CLASS_P (tem))\n-\ttem = NULL_TREE;\n-      if (!tem\n-\t  && TREE_CODE (TREE_OPERAND (*t, 0)) == ADDR_EXPR)\n-\t/* If we had a good reason for propagating the address here,\n-\t   make sure we end up with valid gimple.  See PR34989.  */\n-\ttem = TREE_OPERAND (TREE_OPERAND (*t, 0), 0);\n-\n-      if (tem)\n-\t{\n-\t  *t = tem;\n-\t  tem = maybe_fold_reference (expr, is_lhs);\n-\t  if (tem)\n-\t    return tem;\n-\t  return expr;\n-\t}\n-    }\n-  else if (!is_lhs\n-\t   && DECL_P (*t))\n-    {\n-      tree tem = get_symbol_constant_value (*t);\n-      if (tem\n-\t  && useless_type_conversion_p (TREE_TYPE (*t), TREE_TYPE (tem)))\n-\t{\n-\t  *t = unshare_expr (tem);\n-\t  tem = maybe_fold_reference (expr, is_lhs);\n-\t  if (tem)\n-\t    return tem;\n-\t  return expr;\n-\t}\n-    }\n-\n-  return NULL_TREE;\n-}\n-\n-\n-/* Return the string length, maximum string length or maximum value of\n-   ARG in LENGTH.\n-   If ARG is an SSA name variable, follow its use-def chains.  If LENGTH\n-   is not NULL and, for TYPE == 0, its value is not equal to the length\n-   we determine or if we are unable to determine the length or value,\n-   return false.  VISITED is a bitmap of visited variables.\n-   TYPE is 0 if string length should be returned, 1 for maximum string\n-   length and 2 for maximum value ARG can have.  */\n-\n-static bool\n-get_maxval_strlen (tree arg, tree *length, bitmap visited, int type)\n-{\n-  tree var, val;\n-  gimple def_stmt;\n-\n-  if (TREE_CODE (arg) != SSA_NAME)\n-    {\n-      if (TREE_CODE (arg) == COND_EXPR)\n-        return get_maxval_strlen (COND_EXPR_THEN (arg), length, visited, type)\n-               && get_maxval_strlen (COND_EXPR_ELSE (arg), length, visited, type);\n-      /* We can end up with &(*iftmp_1)[0] here as well, so handle it.  */\n-      else if (TREE_CODE (arg) == ADDR_EXPR\n-\t       && TREE_CODE (TREE_OPERAND (arg, 0)) == ARRAY_REF\n-\t       && integer_zerop (TREE_OPERAND (TREE_OPERAND (arg, 0), 1)))\n-\t{\n-\t  tree aop0 = TREE_OPERAND (TREE_OPERAND (arg, 0), 0);\n-\t  if (TREE_CODE (aop0) == INDIRECT_REF\n-\t      && TREE_CODE (TREE_OPERAND (aop0, 0)) == SSA_NAME)\n-\t    return get_maxval_strlen (TREE_OPERAND (aop0, 0),\n-\t\t\t\t      length, visited, type);\n-\t}\n-\n-      if (type == 2)\n-\t{\n-\t  val = arg;\n-\t  if (TREE_CODE (val) != INTEGER_CST\n-\t      || tree_int_cst_sgn (val) < 0)\n-\t    return false;\n-\t}\n-      else\n-\tval = c_strlen (arg, 1);\n-      if (!val)\n-\treturn false;\n-\n-      if (*length)\n-\t{\n-\t  if (type > 0)\n-\t    {\n-\t      if (TREE_CODE (*length) != INTEGER_CST\n-\t\t  || TREE_CODE (val) != INTEGER_CST)\n-\t\treturn false;\n-\n-\t      if (tree_int_cst_lt (*length, val))\n-\t\t*length = val;\n-\t      return true;\n-\t    }\n-\t  else if (simple_cst_equal (val, *length) != 1)\n-\t    return false;\n-\t}\n-\n-      *length = val;\n-      return true;\n-    }\n-\n-  /* If we were already here, break the infinite cycle.  */\n-  if (bitmap_bit_p (visited, SSA_NAME_VERSION (arg)))\n-    return true;\n-  bitmap_set_bit (visited, SSA_NAME_VERSION (arg));\n-\n-  var = arg;\n-  def_stmt = SSA_NAME_DEF_STMT (var);\n-\n-  switch (gimple_code (def_stmt))\n-    {\n-      case GIMPLE_ASSIGN:\n-        /* The RHS of the statement defining VAR must either have a\n-           constant length or come from another SSA_NAME with a constant\n-           length.  */\n-        if (gimple_assign_single_p (def_stmt)\n-            || gimple_assign_unary_nop_p (def_stmt))\n-          {\n-            tree rhs = gimple_assign_rhs1 (def_stmt);\n-            return get_maxval_strlen (rhs, length, visited, type);\n-          }\n-        return false;\n-\n-      case GIMPLE_PHI:\n-\t{\n-\t  /* All the arguments of the PHI node must have the same constant\n-\t     length.  */\n-\t  unsigned i;\n-\n-\t  for (i = 0; i < gimple_phi_num_args (def_stmt); i++)\n-          {\n-            tree arg = gimple_phi_arg (def_stmt, i)->def;\n-\n-            /* If this PHI has itself as an argument, we cannot\n-               determine the string length of this argument.  However,\n-               if we can find a constant string length for the other\n-               PHI args then we can still be sure that this is a\n-               constant string length.  So be optimistic and just\n-               continue with the next argument.  */\n-            if (arg == gimple_phi_result (def_stmt))\n-              continue;\n-\n-            if (!get_maxval_strlen (arg, length, visited, type))\n-              return false;\n-          }\n-        }\n-        return true;\n-\n-      default:\n-        return false;\n-    }\n-}\n-\n-\n-/* Fold builtin call in statement STMT.  Returns a simplified tree.\n-   We may return a non-constant expression, including another call\n-   to a different function and with different arguments, e.g.,\n-   substituting memcpy for strcpy when the string length is known.\n-   Note that some builtins expand into inline code that may not\n-   be valid in GIMPLE.  Callers must take care.  */\n-\n-static tree\n-ccp_fold_builtin (gimple stmt)\n-{\n-  tree result, val[3];\n-  tree callee, a;\n-  int arg_idx, type;\n-  bitmap visited;\n-  bool ignore;\n-  int nargs;\n-  location_t loc = gimple_location (stmt);\n-\n-  gcc_assert (is_gimple_call (stmt));\n-\n-  ignore = (gimple_call_lhs (stmt) == NULL);\n-\n-  /* First try the generic builtin folder.  If that succeeds, return the\n-     result directly.  */\n-  result = fold_call_stmt (stmt, ignore);\n-  if (result)\n-    {\n-      if (ignore)\n-\tSTRIP_NOPS (result);\n-      return result;\n-    }\n-\n-  /* Ignore MD builtins.  */\n-  callee = gimple_call_fndecl (stmt);\n-  if (DECL_BUILT_IN_CLASS (callee) == BUILT_IN_MD)\n-    return NULL_TREE;\n-\n-  /* If the builtin could not be folded, and it has no argument list,\n-     we're done.  */\n-  nargs = gimple_call_num_args (stmt);\n-  if (nargs == 0)\n-    return NULL_TREE;\n-\n-  /* Limit the work only for builtins we know how to simplify.  */\n-  switch (DECL_FUNCTION_CODE (callee))\n-    {\n-    case BUILT_IN_STRLEN:\n-    case BUILT_IN_FPUTS:\n-    case BUILT_IN_FPUTS_UNLOCKED:\n-      arg_idx = 0;\n-      type = 0;\n-      break;\n-    case BUILT_IN_STRCPY:\n-    case BUILT_IN_STRNCPY:\n-      arg_idx = 1;\n-      type = 0;\n-      break;\n-    case BUILT_IN_MEMCPY_CHK:\n-    case BUILT_IN_MEMPCPY_CHK:\n-    case BUILT_IN_MEMMOVE_CHK:\n-    case BUILT_IN_MEMSET_CHK:\n-    case BUILT_IN_STRNCPY_CHK:\n-      arg_idx = 2;\n-      type = 2;\n-      break;\n-    case BUILT_IN_STRCPY_CHK:\n-    case BUILT_IN_STPCPY_CHK:\n-      arg_idx = 1;\n-      type = 1;\n-      break;\n-    case BUILT_IN_SNPRINTF_CHK:\n-    case BUILT_IN_VSNPRINTF_CHK:\n-      arg_idx = 1;\n-      type = 2;\n-      break;\n-    default:\n-      return NULL_TREE;\n-    }\n-\n-  if (arg_idx >= nargs)\n-    return NULL_TREE;\n-\n-  /* Try to use the dataflow information gathered by the CCP process.  */\n-  visited = BITMAP_ALLOC (NULL);\n-  bitmap_clear (visited);\n-\n-  memset (val, 0, sizeof (val));\n-  a = gimple_call_arg (stmt, arg_idx);\n-  if (!get_maxval_strlen (a, &val[arg_idx], visited, type))\n-    val[arg_idx] = NULL_TREE;\n-\n-  BITMAP_FREE (visited);\n-\n-  result = NULL_TREE;\n-  switch (DECL_FUNCTION_CODE (callee))\n-    {\n-    case BUILT_IN_STRLEN:\n-      if (val[0] && nargs == 1)\n-\t{\n-\t  tree new_val =\n-              fold_convert (TREE_TYPE (gimple_call_lhs (stmt)), val[0]);\n-\n-\t  /* If the result is not a valid gimple value, or not a cast\n-\t     of a valid gimple value, then we can not use the result.  */\n-\t  if (is_gimple_val (new_val)\n-\t      || (is_gimple_cast (new_val)\n-\t\t  && is_gimple_val (TREE_OPERAND (new_val, 0))))\n-\t    return new_val;\n-\t}\n-      break;\n-\n-    case BUILT_IN_STRCPY:\n-      if (val[1] && is_gimple_val (val[1]) && nargs == 2)\n-\tresult = fold_builtin_strcpy (loc, callee,\n-                                      gimple_call_arg (stmt, 0),\n-                                      gimple_call_arg (stmt, 1),\n-\t\t\t\t      val[1]);\n-      break;\n-\n-    case BUILT_IN_STRNCPY:\n-      if (val[1] && is_gimple_val (val[1]) && nargs == 3)\n-\tresult = fold_builtin_strncpy (loc, callee,\n-                                       gimple_call_arg (stmt, 0),\n-                                       gimple_call_arg (stmt, 1),\n-                                       gimple_call_arg (stmt, 2),\n-\t\t\t\t       val[1]);\n-      break;\n-\n-    case BUILT_IN_FPUTS:\n-      if (nargs == 2)\n-\tresult = fold_builtin_fputs (loc, gimple_call_arg (stmt, 0),\n-\t\t\t\t     gimple_call_arg (stmt, 1),\n-\t\t\t\t     ignore, false, val[0]);\n-      break;\n-\n-    case BUILT_IN_FPUTS_UNLOCKED:\n-      if (nargs == 2)\n-\tresult = fold_builtin_fputs (loc, gimple_call_arg (stmt, 0),\n-\t\t\t\t     gimple_call_arg (stmt, 1),\n-\t\t\t\t     ignore, true, val[0]);\n-      break;\n-\n-    case BUILT_IN_MEMCPY_CHK:\n-    case BUILT_IN_MEMPCPY_CHK:\n-    case BUILT_IN_MEMMOVE_CHK:\n-    case BUILT_IN_MEMSET_CHK:\n-      if (val[2] && is_gimple_val (val[2]) && nargs == 4)\n-\tresult = fold_builtin_memory_chk (loc, callee,\n-                                          gimple_call_arg (stmt, 0),\n-                                          gimple_call_arg (stmt, 1),\n-                                          gimple_call_arg (stmt, 2),\n-                                          gimple_call_arg (stmt, 3),\n-\t\t\t\t\t  val[2], ignore,\n-\t\t\t\t\t  DECL_FUNCTION_CODE (callee));\n-      break;\n-\n-    case BUILT_IN_STRCPY_CHK:\n-    case BUILT_IN_STPCPY_CHK:\n-      if (val[1] && is_gimple_val (val[1]) && nargs == 3)\n-\tresult = fold_builtin_stxcpy_chk (loc, callee,\n-                                          gimple_call_arg (stmt, 0),\n-                                          gimple_call_arg (stmt, 1),\n-                                          gimple_call_arg (stmt, 2),\n-\t\t\t\t\t  val[1], ignore,\n-\t\t\t\t\t  DECL_FUNCTION_CODE (callee));\n-      break;\n-\n-    case BUILT_IN_STRNCPY_CHK:\n-      if (val[2] && is_gimple_val (val[2]) && nargs == 4)\n-\tresult = fold_builtin_strncpy_chk (loc, gimple_call_arg (stmt, 0),\n-                                           gimple_call_arg (stmt, 1),\n-                                           gimple_call_arg (stmt, 2),\n-                                           gimple_call_arg (stmt, 3),\n-\t\t\t\t\t   val[2]);\n-      break;\n-\n-    case BUILT_IN_SNPRINTF_CHK:\n-    case BUILT_IN_VSNPRINTF_CHK:\n-      if (val[1] && is_gimple_val (val[1]))\n-\tresult = gimple_fold_builtin_snprintf_chk (stmt, val[1],\n-                                                   DECL_FUNCTION_CODE (callee));\n-      break;\n-\n-    default:\n-      gcc_unreachable ();\n-    }\n-\n-  if (result && ignore)\n-    result = fold_ignored_result (result);\n-  return result;\n-}\n-\n-/* Attempt to fold an assignment statement pointed-to by SI.  Returns a\n-   replacement rhs for the statement or NULL_TREE if no simplification\n-   could be made.  It is assumed that the operands have been previously\n-   folded.  */\n-\n-static tree\n-fold_gimple_assign (gimple_stmt_iterator *si)\n-{\n-  gimple stmt = gsi_stmt (*si);\n-  enum tree_code subcode = gimple_assign_rhs_code (stmt);\n-  location_t loc = gimple_location (stmt);\n-\n-  tree result = NULL_TREE;\n-\n-  switch (get_gimple_rhs_class (subcode))\n-    {\n-    case GIMPLE_SINGLE_RHS:\n-      {\n-        tree rhs = gimple_assign_rhs1 (stmt);\n-\n-        /* Try to fold a conditional expression.  */\n-        if (TREE_CODE (rhs) == COND_EXPR)\n-          {\n-\t    tree op0 = COND_EXPR_COND (rhs);\n-\t    tree tem;\n-\t    bool set = false;\n-\t    location_t cond_loc = EXPR_LOCATION (rhs);\n-\n-\t    if (COMPARISON_CLASS_P (op0))\n-\t      {\n-\t\tfold_defer_overflow_warnings ();\n-\t\ttem = fold_binary_loc (cond_loc,\n-\t\t\t\t   TREE_CODE (op0), TREE_TYPE (op0),\n-\t\t\t\t   TREE_OPERAND (op0, 0),\n-\t\t\t\t   TREE_OPERAND (op0, 1));\n-\t\t/* This is actually a conditional expression, not a GIMPLE\n-\t\t   conditional statement, however, the valid_gimple_rhs_p\n-\t\t   test still applies.  */\n-\t\tset = (tem && is_gimple_condexpr (tem)\n-\t\t       && valid_gimple_rhs_p (tem));\n-\t\tfold_undefer_overflow_warnings (set, stmt, 0);\n-\t      }\n-\t    else if (is_gimple_min_invariant (op0))\n-\t      {\n-\t\ttem = op0;\n-\t\tset = true;\n-\t      }\n-\t    else\n-\t      return NULL_TREE;\n-\n-\t    if (set)\n-\t      result = fold_build3_loc (cond_loc, COND_EXPR, TREE_TYPE (rhs), tem,\n-\t\t\t\t    COND_EXPR_THEN (rhs), COND_EXPR_ELSE (rhs));\n-          }\n-\n-\telse if (TREE_CODE (rhs) == TARGET_MEM_REF)\n-\t  return maybe_fold_tmr (rhs);\n-\n-\telse if (REFERENCE_CLASS_P (rhs))\n-\t  return maybe_fold_reference (rhs, false);\n-\n-\telse if (TREE_CODE (rhs) == ADDR_EXPR)\n-\t  {\n-\t    tree tem = maybe_fold_reference (TREE_OPERAND (rhs, 0), true);\n-\t    if (tem)\n-\t      result = fold_convert (TREE_TYPE (rhs),\n-\t\t\t\t     build_fold_addr_expr_loc (loc, tem));\n-\t  }\n-\n-\telse if (TREE_CODE (rhs) == CONSTRUCTOR\n-\t\t && TREE_CODE (TREE_TYPE (rhs)) == VECTOR_TYPE\n-\t\t && (CONSTRUCTOR_NELTS (rhs)\n-\t\t     == TYPE_VECTOR_SUBPARTS (TREE_TYPE (rhs))))\n-\t  {\n-\t    /* Fold a constant vector CONSTRUCTOR to VECTOR_CST.  */\n-\t    unsigned i;\n-\t    tree val;\n-\n-\t    FOR_EACH_CONSTRUCTOR_VALUE (CONSTRUCTOR_ELTS (rhs), i, val)\n-\t      if (TREE_CODE (val) != INTEGER_CST\n-\t\t  && TREE_CODE (val) != REAL_CST\n-\t\t  && TREE_CODE (val) != FIXED_CST)\n-\t\treturn NULL_TREE;\n-\n-\t    return build_vector_from_ctor (TREE_TYPE (rhs),\n-\t\t\t\t\t   CONSTRUCTOR_ELTS (rhs));\n-\t  }\n-\n-\telse if (DECL_P (rhs))\n-\t  return unshare_expr (get_symbol_constant_value (rhs));\n-\n-        /* If we couldn't fold the RHS, hand over to the generic\n-           fold routines.  */\n-        if (result == NULL_TREE)\n-          result = fold (rhs);\n-\n-        /* Strip away useless type conversions.  Both the NON_LVALUE_EXPR\n-           that may have been added by fold, and \"useless\" type\n-           conversions that might now be apparent due to propagation.  */\n-        STRIP_USELESS_TYPE_CONVERSION (result);\n-\n-        if (result != rhs && valid_gimple_rhs_p (result))\n-\t  return result;\n-\n-\treturn NULL_TREE;\n-      }\n-      break;\n-\n-    case GIMPLE_UNARY_RHS:\n-      {\n-\ttree rhs = gimple_assign_rhs1 (stmt);\n-\n-\tresult = fold_unary_loc (loc, subcode, gimple_expr_type (stmt), rhs);\n-\tif (result)\n-\t  {\n-\t    /* If the operation was a conversion do _not_ mark a\n-\t       resulting constant with TREE_OVERFLOW if the original\n-\t       constant was not.  These conversions have implementation\n-\t       defined behavior and retaining the TREE_OVERFLOW flag\n-\t       here would confuse later passes such as VRP.  */\n-\t    if (CONVERT_EXPR_CODE_P (subcode)\n-\t\t&& TREE_CODE (result) == INTEGER_CST\n-\t\t&& TREE_CODE (rhs) == INTEGER_CST)\n-\t      TREE_OVERFLOW (result) = TREE_OVERFLOW (rhs);\n-\n-\t    STRIP_USELESS_TYPE_CONVERSION (result);\n-\t    if (valid_gimple_rhs_p (result))\n-\t      return result;\n-\t  }\n-\telse if (CONVERT_EXPR_CODE_P (subcode)\n-\t\t && POINTER_TYPE_P (gimple_expr_type (stmt))\n-\t\t && POINTER_TYPE_P (TREE_TYPE (gimple_assign_rhs1 (stmt))))\n-\t  {\n-\t    tree type = gimple_expr_type (stmt);\n-\t    tree t = maybe_fold_offset_to_address (loc,\n-\t\t\t\t\t\t   gimple_assign_rhs1 (stmt),\n-\t\t\t\t\t\t   integer_zero_node, type);\n-\t    if (t)\n-\t      return t;\n-\t  }\n-      }\n-      break;\n-\n-    case GIMPLE_BINARY_RHS:\n-      /* Try to fold pointer addition.  */\n-      if (gimple_assign_rhs_code (stmt) == POINTER_PLUS_EXPR)\n-\t{\n-\t  tree type = TREE_TYPE (gimple_assign_rhs1 (stmt));\n-\t  if (TREE_CODE (TREE_TYPE (type)) == ARRAY_TYPE)\n-\t    {\n-\t      type = build_pointer_type (TREE_TYPE (TREE_TYPE (type)));\n-\t      if (!useless_type_conversion_p\n-\t\t    (TREE_TYPE (gimple_assign_lhs (stmt)), type))\n-\t\ttype = TREE_TYPE (gimple_assign_rhs1 (stmt));\n-\t    }\n-\t  result = maybe_fold_stmt_addition (gimple_location (stmt),\n-\t\t\t\t\t     type,\n-\t\t\t\t\t     gimple_assign_rhs1 (stmt),\n-\t\t\t\t\t     gimple_assign_rhs2 (stmt));\n-\t}\n-\n-      if (!result)\n-        result = fold_binary_loc (loc, subcode,\n-                              TREE_TYPE (gimple_assign_lhs (stmt)),\n-                              gimple_assign_rhs1 (stmt),\n-                              gimple_assign_rhs2 (stmt));\n-\n-      if (result)\n-        {\n-          STRIP_USELESS_TYPE_CONVERSION (result);\n-          if (valid_gimple_rhs_p (result))\n-\t    return result;\n-\n-\t  /* Fold might have produced non-GIMPLE, so if we trust it blindly\n-\t     we lose canonicalization opportunities.  Do not go again\n-\t     through fold here though, or the same non-GIMPLE will be\n-\t     produced.  */\n-          if (commutative_tree_code (subcode)\n-              && tree_swap_operands_p (gimple_assign_rhs1 (stmt),\n-                                       gimple_assign_rhs2 (stmt), false))\n-            return build2 (subcode, TREE_TYPE (gimple_assign_lhs (stmt)),\n-                           gimple_assign_rhs2 (stmt),\n-                           gimple_assign_rhs1 (stmt));\n-        }\n-      break;\n-\n-    case GIMPLE_INVALID_RHS:\n-      gcc_unreachable ();\n-    }\n-\n-  return NULL_TREE;\n-}\n-\n-/* Attempt to fold a conditional statement. Return true if any changes were\n-   made. We only attempt to fold the condition expression, and do not perform\n-   any transformation that would require alteration of the cfg.  It is\n-   assumed that the operands have been previously folded.  */\n-\n-static bool\n-fold_gimple_cond (gimple stmt)\n-{\n-  tree result = fold_binary_loc (gimple_location (stmt),\n-\t\t\t     gimple_cond_code (stmt),\n-                             boolean_type_node,\n-                             gimple_cond_lhs (stmt),\n-                             gimple_cond_rhs (stmt));\n-\n-  if (result)\n-    {\n-      STRIP_USELESS_TYPE_CONVERSION (result);\n-      if (is_gimple_condexpr (result) && valid_gimple_rhs_p (result))\n-        {\n-          gimple_cond_set_condition_from_tree (stmt, result);\n-          return true;\n-        }\n-    }\n-\n-  return false;\n-}\n-\n-static void gimplify_and_update_call_from_tree (gimple_stmt_iterator *, tree);\n-\n-/* Attempt to fold a call statement referenced by the statement iterator GSI.\n-   The statement may be replaced by another statement, e.g., if the call\n-   simplifies to a constant value. Return true if any changes were made.\n-   It is assumed that the operands have been previously folded.  */\n-\n-static bool\n-fold_gimple_call (gimple_stmt_iterator *gsi)\n-{\n-  gimple stmt = gsi_stmt (*gsi);\n-\n-  tree callee = gimple_call_fndecl (stmt);\n-\n-  /* Check for builtins that CCP can handle using information not\n-     available in the generic fold routines.  */\n-  if (callee && DECL_BUILT_IN (callee))\n-    {\n-      tree result = ccp_fold_builtin (stmt);\n-\n-      if (result)\n-\t{\n-          if (!update_call_from_tree (gsi, result))\n-\t    gimplify_and_update_call_from_tree (gsi, result);\n-\t  return true;\n-\t}\n-    }\n-  else\n-    {\n-      /* Check for resolvable OBJ_TYPE_REF.  The only sorts we can resolve\n-         here are when we've propagated the address of a decl into the\n-         object slot.  */\n-      /* ??? Should perhaps do this in fold proper.  However, doing it\n-         there requires that we create a new CALL_EXPR, and that requires\n-         copying EH region info to the new node.  Easier to just do it\n-         here where we can just smash the call operand.  */\n-      /* ??? Is there a good reason not to do this in fold_stmt_inplace?  */\n-      callee = gimple_call_fn (stmt);\n-      if (TREE_CODE (callee) == OBJ_TYPE_REF\n-          && lang_hooks.fold_obj_type_ref\n-          && TREE_CODE (OBJ_TYPE_REF_OBJECT (callee)) == ADDR_EXPR\n-          && DECL_P (TREE_OPERAND\n-                     (OBJ_TYPE_REF_OBJECT (callee), 0)))\n-        {\n-          tree t;\n-\n-          /* ??? Caution: Broken ADDR_EXPR semantics means that\n-             looking at the type of the operand of the addr_expr\n-             can yield an array type.  See silly exception in\n-             check_pointer_types_r.  */\n-          t = TREE_TYPE (TREE_TYPE (OBJ_TYPE_REF_OBJECT (callee)));\n-          t = lang_hooks.fold_obj_type_ref (callee, t);\n-          if (t)\n-            {\n-              gimple_call_set_fn (stmt, t);\n-              return true;\n-            }\n-        }\n-    }\n-\n-  return false;\n-}\n-\n-/* Worker for both fold_stmt and fold_stmt_inplace.  The INPLACE argument\n-   distinguishes both cases.  */\n-\n-static bool\n-fold_stmt_1 (gimple_stmt_iterator *gsi, bool inplace)\n-{\n-  bool changed = false;\n-  gimple stmt = gsi_stmt (*gsi);\n-  unsigned i;\n-\n-  /* Fold the main computation performed by the statement.  */\n-  switch (gimple_code (stmt))\n-    {\n-    case GIMPLE_ASSIGN:\n-      {\n-\tunsigned old_num_ops = gimple_num_ops (stmt);\n-\ttree new_rhs = fold_gimple_assign (gsi);\n-\ttree lhs = gimple_assign_lhs (stmt);\n-\tif (new_rhs\n-\t    && !useless_type_conversion_p (TREE_TYPE (lhs),\n-\t\t\t\t\t   TREE_TYPE (new_rhs)))\n-\t  new_rhs = fold_convert (TREE_TYPE (lhs), new_rhs);\n-\tif (new_rhs\n-\t    && (!inplace\n-\t\t|| get_gimple_rhs_num_ops (TREE_CODE (new_rhs)) < old_num_ops))\n-\t  {\n-\t    gimple_assign_set_rhs_from_tree (gsi, new_rhs);\n-\t    changed = true;\n-\t  }\n-\tbreak;\n-      }\n-\n-    case GIMPLE_COND:\n-      changed |= fold_gimple_cond (stmt);\n-      break;\n-\n-    case GIMPLE_CALL:\n-      /* Fold *& in call arguments.  */\n-      for (i = 0; i < gimple_call_num_args (stmt); ++i)\n-\tif (REFERENCE_CLASS_P (gimple_call_arg (stmt, i)))\n-\t  {\n-\t    tree tmp = maybe_fold_reference (gimple_call_arg (stmt, i), false);\n-\t    if (tmp)\n-\t      {\n-\t\tgimple_call_set_arg (stmt, i, tmp);\n-\t\tchanged = true;\n-\t      }\n-\t  }\n-      /* The entire statement may be replaced in this case.  */\n-      if (!inplace)\n-\tchanged |= fold_gimple_call (gsi);\n-      break;\n-\n-    case GIMPLE_ASM:\n-      /* Fold *& in asm operands.  */\n-      for (i = 0; i < gimple_asm_noutputs (stmt); ++i)\n-\t{\n-\t  tree link = gimple_asm_output_op (stmt, i);\n-\t  tree op = TREE_VALUE (link);\n-\t  if (REFERENCE_CLASS_P (op)\n-\t      && (op = maybe_fold_reference (op, true)) != NULL_TREE)\n-\t    {\n-\t      TREE_VALUE (link) = op;\n-\t      changed = true;\n-\t    }\n-\t}\n-      for (i = 0; i < gimple_asm_ninputs (stmt); ++i)\n-\t{\n-\t  tree link = gimple_asm_input_op (stmt, i);\n-\t  tree op = TREE_VALUE (link);\n-\t  if (REFERENCE_CLASS_P (op)\n-\t      && (op = maybe_fold_reference (op, false)) != NULL_TREE)\n-\t    {\n-\t      TREE_VALUE (link) = op;\n-\t      changed = true;\n-\t    }\n-\t}\n-      break;\n-\n-    default:;\n-    }\n-\n-  stmt = gsi_stmt (*gsi);\n-\n-  /* Fold *& on the lhs.  */\n-  if (gimple_has_lhs (stmt))\n-    {\n-      tree lhs = gimple_get_lhs (stmt);\n-      if (lhs && REFERENCE_CLASS_P (lhs))\n-\t{\n-\t  tree new_lhs = maybe_fold_reference (lhs, true);\n-\t  if (new_lhs)\n-\t    {\n-\t      gimple_set_lhs (stmt, new_lhs);\n-\t      changed = true;\n-\t    }\n-\t}\n-    }\n-\n-  return changed;\n-}\n-\n-/* Fold the statement pointed to by GSI.  In some cases, this function may\n-   replace the whole statement with a new one.  Returns true iff folding\n-   makes any changes.\n-   The statement pointed to by GSI should be in valid gimple form but may\n-   be in unfolded state as resulting from for example constant propagation\n-   which can produce *&x = 0.  */\n-\n-bool\n-fold_stmt (gimple_stmt_iterator *gsi)\n-{\n-  return fold_stmt_1 (gsi, false);\n-}\n-\n-/* Perform the minimal folding on statement STMT.  Only operations like\n-   *&x created by constant propagation are handled.  The statement cannot\n-   be replaced with a new one.  Return true if the statement was\n-   changed, false otherwise.\n-   The statement STMT should be in valid gimple form but may\n-   be in unfolded state as resulting from for example constant propagation\n-   which can produce *&x = 0.  */\n-\n-bool\n-fold_stmt_inplace (gimple stmt)\n-{\n-  gimple_stmt_iterator gsi = gsi_for_stmt (stmt);\n-  bool changed = fold_stmt_1 (&gsi, true);\n-  gcc_assert (gsi_stmt (gsi) == stmt);\n-  return changed;\n-}\n \n /* Try to optimize out __builtin_stack_restore.  Optimize it out\n    if there is another __builtin_stack_restore in the same basic\n@@ -3337,80 +1859,6 @@ optimize_stdarg_builtin (gimple call)\n     }\n }\n \n-/* Convert EXPR into a GIMPLE value suitable for substitution on the\n-   RHS of an assignment.  Insert the necessary statements before\n-   iterator *SI_P.  The statement at *SI_P, which must be a GIMPLE_CALL\n-   is replaced.  If the call is expected to produces a result, then it\n-   is replaced by an assignment of the new RHS to the result variable.\n-   If the result is to be ignored, then the call is replaced by a\n-   GIMPLE_NOP.  */\n-\n-static void\n-gimplify_and_update_call_from_tree (gimple_stmt_iterator *si_p, tree expr)\n-{\n-  tree lhs;\n-  tree tmp = NULL_TREE;  /* Silence warning.  */\n-  gimple stmt, new_stmt;\n-  gimple_stmt_iterator i;\n-  gimple_seq stmts = gimple_seq_alloc();\n-  struct gimplify_ctx gctx;\n-  gimple last = NULL;\n-\n-  stmt = gsi_stmt (*si_p);\n-\n-  gcc_assert (is_gimple_call (stmt));\n-\n-  lhs = gimple_call_lhs (stmt);\n-\n-  push_gimplify_context (&gctx);\n-\n-  if (lhs == NULL_TREE)\n-    gimplify_and_add (expr, &stmts);\n-  else\n-    tmp = get_initialized_tmp_var (expr, &stmts, NULL);\n-\n-  pop_gimplify_context (NULL);\n-\n-  if (gimple_has_location (stmt))\n-    annotate_all_with_location (stmts, gimple_location (stmt));\n-\n-  /* The replacement can expose previously unreferenced variables.  */\n-  for (i = gsi_start (stmts); !gsi_end_p (i); gsi_next (&i))\n-    {\n-      if (last)\n-\t{\n-\t  gsi_insert_before (si_p, last, GSI_NEW_STMT);\n-\t  gsi_next (si_p);\n-\t}\n-      new_stmt = gsi_stmt (i);\n-      find_new_referenced_vars (new_stmt);\n-      mark_symbols_for_renaming (new_stmt);\n-      last = new_stmt;\n-    }\n-\n-  if (lhs == NULL_TREE)\n-    {\n-      unlink_stmt_vdef (stmt);\n-      release_defs (stmt);\n-      new_stmt = last;\n-    }\n-  else\n-    {\n-      if (last)\n-\t{\n-\t  gsi_insert_before (si_p, last, GSI_NEW_STMT);\n-\t  gsi_next (si_p);\n-\t}\n-      new_stmt = gimple_build_assign (lhs, tmp);\n-      gimple_set_vuse (new_stmt, gimple_vuse (stmt));\n-      gimple_set_vdef (new_stmt, gimple_vdef (stmt));\n-      move_ssa_defining_stmt_for_defs (new_stmt, stmt);\n-    }\n-\n-  gimple_set_location (new_stmt, gimple_location (stmt));\n-  gsi_replace (si_p, new_stmt, false);\n-}\n-\n /* A simple pass that attempts to fold all builtin functions.  This pass\n    is run after we've propagated as many constants as we can.  */\n \n@@ -3445,7 +1893,7 @@ execute_fold_all_builtins (void)\n \t    }\n \t  fcode = DECL_FUNCTION_CODE (callee);\n \n-\t  result = ccp_fold_builtin (stmt);\n+\t  result = gimple_fold_builtin (stmt);\n \n \t  if (result)\n \t    gimple_remove_stmt_histograms (cfun, stmt);"}, {"sha": "8c979c066be3fc95f679b0e5f940b999d2daa793", "filename": "gcc/tree.h", "status": "modified", "additions": 0, "deletions": 5, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/gcc%2Ftree.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cbdd87d44471ec0b9099dd8d6e32ae09039a1b48/gcc%2Ftree.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree.h?ref=cbdd87d44471ec0b9099dd8d6e32ae09039a1b48", "patch": "@@ -5282,11 +5282,6 @@ struct GTY(()) tree_priority_map {\n \n tree target_for_debug_bind (tree);\n \n-/* In tree-ssa-ccp.c */\n-extern tree maybe_fold_offset_to_reference (location_t, tree, tree, tree);\n-extern tree maybe_fold_offset_to_address (location_t, tree, tree, tree);\n-extern tree maybe_fold_stmt_addition (location_t, tree, tree, tree);\n-\n /* In tree-ssa-address.c.  */\n extern tree tree_mem_ref_addr (tree, tree);\n extern void copy_mem_ref_info (tree, tree);"}]}