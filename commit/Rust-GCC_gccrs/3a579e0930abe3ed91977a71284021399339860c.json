{"sha": "3a579e0930abe3ed91977a71284021399339860c", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6M2E1NzllMDkzMGFiZTNlZDkxOTc3YTcxMjg0MDIxMzk5MzM5ODYwYw==", "commit": {"author": {"name": "Vladimir Yakovlev", "email": "vladimir.b.yakovlev@intel.com", "date": "2013-01-15T10:07:08Z"}, "committer": {"name": "Kirill Yukhin", "email": "kyukhin@gcc.gnu.org", "date": "2013-01-15T10:07:08Z"}, "message": "i386-c.c (ix86_target_macros_internal): New case.\n\n        * config/i386/i386-c.c (ix86_target_macros_internal): New case.\n        (ix86_target_macros_internal): Likewise.\n\n        * config/i386/i386.c (m_CORE2I7): Removed.\n        (m_CORE_HASWELL): New macro.\n        (m_CORE_ALL): Likewise.\n        (initial_ix86_tune_features): m_CORE2I7 is replaced by m_CORE_ALL.\n        (initial_ix86_arch_features): Likewise.\n        (processor_target_table): Initializations for Core avx2.\n        (cpu_names): New names \"core-avx2\".\n        (ix86_option_override_internal): Changed PROCESSOR_COREI7 by\n        PROCESSOR_CORE_HASWELL.\n        (ix86_issue_rate): New case.\n        (ia32_multipass_dfa_lookahead): Likewise.\n        (ix86_sched_init_global): Likewise.\n\n        * config/i386/i386.h (TARGET_HASWELL): New macro.\n        (target_cpu_default): New TARGET_CPU_DEFAULT_haswell.\n        (processor_type): New PROCESSOR_HASWELL.\n\nFrom-SVN: r195191", "tree": {"sha": "dea8b0b77418353b43e78daf53c88a7a0440ffb9", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/dea8b0b77418353b43e78daf53c88a7a0440ffb9"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/3a579e0930abe3ed91977a71284021399339860c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3a579e0930abe3ed91977a71284021399339860c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/3a579e0930abe3ed91977a71284021399339860c", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3a579e0930abe3ed91977a71284021399339860c/comments", "author": null, "committer": null, "parents": [{"sha": "ff7848297eb1516dca6f023766866c8e6771d57f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ff7848297eb1516dca6f023766866c8e6771d57f", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ff7848297eb1516dca6f023766866c8e6771d57f"}], "stats": {"total": 95, "additions": 67, "deletions": 28}, "files": [{"sha": "efe553bd04b71418909b52c9a950d13746b63e88", "filename": "gcc/ChangeLog", "status": "modified", "additions": 22, "deletions": 0, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3a579e0930abe3ed91977a71284021399339860c/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3a579e0930abe3ed91977a71284021399339860c/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=3a579e0930abe3ed91977a71284021399339860c", "patch": "@@ -1,3 +1,25 @@\n+2013-01-15  Vladimir Yakovlev  <vladimir.b.yakovlev@intel.com>\n+\n+\t* config/i386/i386-c.c (ix86_target_macros_internal): New case.\n+\t(ix86_target_macros_internal): Likewise.\n+\n+\t* config/i386/i386.c (m_CORE2I7): Removed.\n+\t(m_CORE_HASWELL): New macro.\n+\t(m_CORE_ALL): Likewise.\n+\t(initial_ix86_tune_features): m_CORE2I7 is replaced by m_CORE_ALL.\n+\t(initial_ix86_arch_features): Likewise.\n+\t(processor_target_table): Initializations for Core avx2.\n+\t(cpu_names): New names \"core-avx2\".\n+\t(ix86_option_override_internal): Changed PROCESSOR_COREI7 by\n+\tPROCESSOR_CORE_HASWELL.\n+\t(ix86_issue_rate): New case.\n+\t(ia32_multipass_dfa_lookahead): Likewise.\n+\t(ix86_sched_init_global): Likewise.\n+\n+\t* config/i386/i386.h (TARGET_HASWELL): New macro.\n+\t(target_cpu_default): New TARGET_CPU_DEFAULT_haswell.\n+\t(processor_type): New PROCESSOR_HASWELL.\n+\n 2013-01-15  Jakub Jelinek  <jakub@redhat.com>\n \n \tPR tree-optimization/55955"}, {"sha": "51fec844bdf09eaddade31ee49f9c593fdeb13ca", "filename": "gcc/config/i386/i386-c.c", "status": "modified", "additions": 7, "deletions": 0, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3a579e0930abe3ed91977a71284021399339860c/gcc%2Fconfig%2Fi386%2Fi386-c.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3a579e0930abe3ed91977a71284021399339860c/gcc%2Fconfig%2Fi386%2Fi386-c.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-c.c?ref=3a579e0930abe3ed91977a71284021399339860c", "patch": "@@ -141,6 +141,10 @@ ix86_target_macros_internal (HOST_WIDE_INT isa_flag,\n       def_or_undef (parse_in, \"__corei7\");\n       def_or_undef (parse_in, \"__corei7__\");\n       break;\n+    case PROCESSOR_HASWELL:\n+      def_or_undef (parse_in, \"__core_avx2\");\n+      def_or_undef (parse_in, \"__core_avx2__\");\n+      break;\n     case PROCESSOR_ATOM:\n       def_or_undef (parse_in, \"__atom\");\n       def_or_undef (parse_in, \"__atom__\");\n@@ -231,6 +235,9 @@ ix86_target_macros_internal (HOST_WIDE_INT isa_flag,\n     case PROCESSOR_COREI7:\n       def_or_undef (parse_in, \"__tune_corei7__\");\n       break;\n+    case PROCESSOR_HASWELL:\n+      def_or_undef (parse_in, \"__tune_core_avx2__\");\n+      break;\n     case PROCESSOR_ATOM:\n       def_or_undef (parse_in, \"__tune_atom__\");\n       break;"}, {"sha": "0e98a1b56756375c85bea1e5e5546d79ee822249", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 35, "deletions": 28, "changes": 63, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3a579e0930abe3ed91977a71284021399339860c/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3a579e0930abe3ed91977a71284021399339860c/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=3a579e0930abe3ed91977a71284021399339860c", "patch": "@@ -1730,7 +1730,8 @@ const struct processor_costs *ix86_cost = &pentium_cost;\n #define m_P4_NOCONA (m_PENT4 | m_NOCONA)\n #define m_CORE2 (1<<PROCESSOR_CORE2)\n #define m_COREI7 (1<<PROCESSOR_COREI7)\n-#define m_CORE2I7 (m_CORE2 | m_COREI7)\n+#define m_HASWELL (1<<PROCESSOR_HASWELL)\n+#define m_CORE_ALL (m_CORE2 | m_COREI7  | m_HASWELL)\n #define m_ATOM (1<<PROCESSOR_ATOM)\n \n #define m_GEODE (1<<PROCESSOR_GEODE)\n@@ -1766,16 +1767,16 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n      negatively, so enabling for Generic64 seems like good code size\n      tradeoff.  We can't enable it for 32bit generic because it does not\n      work well with PPro base chips.  */\n-  m_386 | m_CORE2I7 | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC64,\n+  m_386 | m_CORE_ALL | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC64,\n \n   /* X86_TUNE_PUSH_MEMORY */\n-  m_386 | m_P4_NOCONA | m_CORE2I7 | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC,\n+  m_386 | m_P4_NOCONA | m_CORE_ALL | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC,\n \n   /* X86_TUNE_ZERO_EXTEND_WITH_AND */\n   m_486 | m_PENT,\n \n   /* X86_TUNE_UNROLL_STRLEN */\n-  m_486 | m_PENT | m_PPRO | m_ATOM | m_CORE2I7 | m_K6 | m_AMD_MULTIPLE | m_GENERIC,\n+  m_486 | m_PENT | m_PPRO | m_ATOM | m_CORE_ALL | m_K6 | m_AMD_MULTIPLE | m_GENERIC,\n \n   /* X86_TUNE_BRANCH_PREDICTION_HINTS: Branch hints were put in P4 based\n      on simulation result. But after P4 was made, no performance benefit\n@@ -1787,11 +1788,11 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n   ~m_386,\n \n   /* X86_TUNE_USE_SAHF */\n-  m_PPRO | m_P4_NOCONA | m_CORE2I7 | m_ATOM | m_K6_GEODE | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC,\n+  m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_K6_GEODE | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC,\n \n   /* X86_TUNE_MOVX: Enable to zero extend integer registers to avoid\n      partial dependencies.  */\n-  m_PPRO | m_P4_NOCONA | m_CORE2I7 | m_ATOM | m_GEODE | m_AMD_MULTIPLE  | m_GENERIC,\n+  m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_GEODE | m_AMD_MULTIPLE  | m_GENERIC,\n \n   /* X86_TUNE_PARTIAL_REG_STALL: We probably ought to watch for partial\n      register stalls on Generic32 compilation setting as well.  However\n@@ -1804,17 +1805,17 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n   m_PPRO,\n \n   /* X86_TUNE_PARTIAL_FLAG_REG_STALL */\n-  m_CORE2I7 | m_GENERIC,\n+  m_CORE_ALL | m_GENERIC,\n \n   /* X86_TUNE_LCP_STALL: Avoid an expensive length-changing prefix stall\n    * on 16-bit immediate moves into memory on Core2 and Corei7.  */\n-  m_CORE2I7 | m_GENERIC,\n+  m_CORE_ALL | m_GENERIC,\n \n   /* X86_TUNE_USE_HIMODE_FIOP */\n   m_386 | m_486 | m_K6_GEODE,\n \n   /* X86_TUNE_USE_SIMODE_FIOP */\n-  ~(m_PENT | m_PPRO | m_CORE2I7 | m_ATOM | m_AMD_MULTIPLE | m_GENERIC),\n+  ~(m_PENT | m_PPRO | m_CORE_ALL | m_ATOM | m_AMD_MULTIPLE | m_GENERIC),\n \n   /* X86_TUNE_USE_MOV0 */\n   m_K6,\n@@ -1835,7 +1836,7 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n   ~(m_PENT | m_PPRO),\n \n   /* X86_TUNE_PROMOTE_QIMODE */\n-  m_386 | m_486 | m_PENT | m_CORE2I7 | m_ATOM | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC,\n+  m_386 | m_486 | m_PENT | m_CORE_ALL | m_ATOM | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC,\n \n   /* X86_TUNE_FAST_PREFIX */\n   ~(m_386 | m_486 | m_PENT),\n@@ -1876,10 +1877,10 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n \n   /* X86_TUNE_INTEGER_DFMODE_MOVES: Enable if integer moves are preferred\n      for DFmode copies */\n-  ~(m_PPRO | m_P4_NOCONA | m_CORE2I7 | m_ATOM | m_GEODE | m_AMD_MULTIPLE | m_ATOM | m_GENERIC),\n+  ~(m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_GEODE | m_AMD_MULTIPLE | m_ATOM | m_GENERIC),\n \n   /* X86_TUNE_PARTIAL_REG_DEPENDENCY */\n-  m_P4_NOCONA | m_CORE2I7 | m_ATOM | m_AMD_MULTIPLE | m_GENERIC,\n+  m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_AMD_MULTIPLE | m_GENERIC,\n \n   /* X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY: In the Generic model we have a\n      conflict here in between PPro/Pentium4 based chips that thread 128bit\n@@ -1890,7 +1891,7 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n      shows that disabling this option on P4 brings over 20% SPECfp regression,\n      while enabling it on K8 brings roughly 2.4% regression that can be partly\n      masked by careful scheduling of moves.  */\n-  m_PPRO | m_P4_NOCONA | m_CORE2I7 | m_ATOM  | m_AMDFAM10 | m_BDVER | m_GENERIC,\n+  m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM  | m_AMDFAM10 | m_BDVER | m_GENERIC,\n \n   /* X86_TUNE_SSE_UNALIGNED_LOAD_OPTIMAL */\n   m_COREI7 | m_AMDFAM10 | m_BDVER | m_BTVER,\n@@ -1914,7 +1915,7 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n   m_PPRO | m_P4_NOCONA,\n \n   /* X86_TUNE_MEMORY_MISMATCH_STALL */\n-  m_P4_NOCONA | m_CORE2I7 | m_ATOM | m_AMD_MULTIPLE | m_GENERIC,\n+  m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_AMD_MULTIPLE | m_GENERIC,\n \n   /* X86_TUNE_PROLOGUE_USING_MOVE */\n   m_PPRO | m_ATHLON_K8,\n@@ -1936,40 +1937,40 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n \n   /* X86_TUNE_FOUR_JUMP_LIMIT: Some CPU cores are not able to predict more\n      than 4 branch instructions in the 16 byte window.  */\n-  m_PPRO | m_P4_NOCONA | m_CORE2I7 | m_ATOM | m_AMD_MULTIPLE | m_GENERIC,\n+  m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_AMD_MULTIPLE | m_GENERIC,\n \n   /* X86_TUNE_SCHEDULE */\n-  m_PENT | m_PPRO | m_CORE2I7 | m_ATOM | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC,\n+  m_PENT | m_PPRO | m_CORE_ALL | m_ATOM | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC,\n \n   /* X86_TUNE_USE_BT */\n-  m_CORE2I7 | m_ATOM | m_AMD_MULTIPLE | m_GENERIC,\n+  m_CORE_ALL | m_ATOM | m_AMD_MULTIPLE | m_GENERIC,\n \n   /* X86_TUNE_USE_INCDEC */\n-  ~(m_P4_NOCONA | m_CORE2I7 | m_ATOM | m_GENERIC),\n+  ~(m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_GENERIC),\n \n   /* X86_TUNE_PAD_RETURNS */\n-  m_CORE2I7 | m_AMD_MULTIPLE | m_GENERIC,\n+  m_CORE_ALL | m_AMD_MULTIPLE | m_GENERIC,\n \n   /* X86_TUNE_PAD_SHORT_FUNCTION: Pad short funtion.  */\n   m_ATOM,\n \n   /* X86_TUNE_EXT_80387_CONSTANTS */\n-  m_PPRO | m_P4_NOCONA | m_CORE2I7 | m_ATOM | m_K6_GEODE | m_ATHLON_K8 | m_GENERIC,\n+  m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_K6_GEODE | m_ATHLON_K8 | m_GENERIC,\n \n   /* X86_TUNE_AVOID_VECTOR_DECODE */\n-  m_CORE2I7 | m_K8 | m_GENERIC64,\n+  m_CORE_ALL | m_K8 | m_GENERIC64,\n \n   /* X86_TUNE_PROMOTE_HIMODE_IMUL: Modern CPUs have same latency for HImode\n      and SImode multiply, but 386 and 486 do HImode multiply faster.  */\n   ~(m_386 | m_486),\n \n   /* X86_TUNE_SLOW_IMUL_IMM32_MEM: Imul of 32-bit constant and memory is\n      vector path on AMD machines.  */\n-  m_CORE2I7 | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC64,\n+  m_CORE_ALL | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC64,\n \n   /* X86_TUNE_SLOW_IMUL_IMM8: Imul of 8-bit constant is vector path on AMD\n      machines.  */\n-  m_CORE2I7 | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC64,\n+  m_CORE_ALL | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC64,\n \n   /* X86_TUNE_MOVE_M1_VIA_OR: On pentiums, it is faster to load -1 via OR\n      than a MOV.  */\n@@ -1986,7 +1987,7 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n \n   /* X86_TUNE_USE_VECTOR_FP_CONVERTS: Prefer vector packed SSE conversion\n      from FP to FP. */\n-  m_CORE2I7 | m_AMDFAM10 | m_GENERIC,\n+  m_CORE_ALL | m_AMDFAM10 | m_GENERIC,\n \n   /* X86_TUNE_USE_VECTOR_CONVERTS: Prefer vector packed SSE conversion\n      from integer to FP. */\n@@ -2024,7 +2025,7 @@ static unsigned int initial_ix86_tune_features[X86_TUNE_LAST] = {\n \n   /* X86_TUNE_GENERAL_REGS_SSE_SPILL: Try to spill general regs to SSE\n      regs instead of memory.  */\n-  m_COREI7 | m_CORE2I7,\n+  m_CORE_ALL,\n \n   /* X86_TUNE_AVOID_MEM_OPND_FOR_CMOVE: Try to avoid memory operands for\n      a conditional move.  */\n@@ -2054,10 +2055,10 @@ static unsigned int initial_ix86_arch_features[X86_ARCH_LAST] = {\n };\n \n static const unsigned int x86_accumulate_outgoing_args\n-  = m_PPRO | m_P4_NOCONA | m_ATOM | m_CORE2I7 | m_AMD_MULTIPLE | m_GENERIC;\n+  = m_PPRO | m_P4_NOCONA | m_ATOM | m_CORE_ALL | m_AMD_MULTIPLE | m_GENERIC;\n \n static const unsigned int x86_arch_always_fancy_math_387\n-  = m_PENT | m_PPRO | m_P4_NOCONA | m_CORE2I7 | m_ATOM | m_AMD_MULTIPLE | m_GENERIC;\n+  = m_PENT | m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_AMD_MULTIPLE | m_GENERIC;\n \n static const unsigned int x86_avx256_split_unaligned_load\n   = m_COREI7 | m_GENERIC;\n@@ -2432,6 +2433,8 @@ static const struct ptt processor_target_table[PROCESSOR_max] =\n   {&core_cost, 16, 10, 16, 10, 16},\n   /* Core i7  */\n   {&core_cost, 16, 10, 16, 10, 16},\n+  /* Core avx2  */\n+  {&core_cost, 16, 10, 16, 10, 16},\n   {&generic32_cost, 16, 7, 16, 7, 16},\n   {&generic64_cost, 16, 10, 16, 10, 16},\n   {&amdfam10_cost, 32, 24, 32, 7, 32},\n@@ -2459,6 +2462,7 @@ static const char *const cpu_names[TARGET_CPU_DEFAULT_max] =\n   \"nocona\",\n   \"core2\",\n   \"corei7\",\n+  \"core-avx2\",\n   \"atom\",\n   \"geode\",\n   \"k6\",\n@@ -2910,7 +2914,7 @@ ix86_option_override_internal (bool main_args_p)\n \t| PTA_SSSE3 | PTA_SSE4_1 | PTA_SSE4_2 | PTA_AVX\n \t| PTA_CX16 | PTA_POPCNT | PTA_AES | PTA_PCLMUL | PTA_FSGSBASE\n \t| PTA_RDRND | PTA_F16C | PTA_FXSR | PTA_XSAVE | PTA_XSAVEOPT},\n-      {\"core-avx2\", PROCESSOR_COREI7, CPU_COREI7,\n+      {\"core-avx2\", PROCESSOR_HASWELL, CPU_COREI7,\n \tPTA_64BIT | PTA_MMX | PTA_SSE | PTA_SSE2 | PTA_SSE3\n \t| PTA_SSSE3 | PTA_SSE4_1 | PTA_SSE4_2 | PTA_AVX | PTA_AVX2\n \t| PTA_CX16 | PTA_POPCNT | PTA_AES | PTA_PCLMUL | PTA_FSGSBASE\n@@ -24048,6 +24052,7 @@ ix86_issue_rate (void)\n     case PROCESSOR_PENTIUM4:\n     case PROCESSOR_CORE2:\n     case PROCESSOR_COREI7:\n+    case PROCESSOR_HASWELL:\n     case PROCESSOR_ATHLON:\n     case PROCESSOR_K8:\n     case PROCESSOR_AMDFAM10:\n@@ -24304,6 +24309,7 @@ ia32_multipass_dfa_lookahead (void)\n \n     case PROCESSOR_CORE2:\n     case PROCESSOR_COREI7:\n+    case PROCESSOR_HASWELL:\n     case PROCESSOR_ATOM:\n       /* Generally, we want haifa-sched:max_issue() to look ahead as far\n \t as many instructions can be executed on a cycle, i.e.,\n@@ -24848,6 +24854,7 @@ ix86_sched_init_global (FILE *dump ATTRIBUTE_UNUSED,\n     {\n     case PROCESSOR_CORE2:\n     case PROCESSOR_COREI7:\n+    case PROCESSOR_HASWELL:\n       /* Do not perform multipass scheduling for pre-reload schedule\n          to save compile time.  */\n       if (reload_completed)"}, {"sha": "af293b428b36f123599fd66b7248a396f26b50e0", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3a579e0930abe3ed91977a71284021399339860c/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3a579e0930abe3ed91977a71284021399339860c/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=3a579e0930abe3ed91977a71284021399339860c", "patch": "@@ -246,6 +246,7 @@ extern const struct processor_costs ix86_size_cost;\n #define TARGET_NOCONA (ix86_tune == PROCESSOR_NOCONA)\n #define TARGET_CORE2 (ix86_tune == PROCESSOR_CORE2)\n #define TARGET_COREI7 (ix86_tune == PROCESSOR_COREI7)\n+#define TARGET_HASWELL (ix86_tune == PROCESSOR_HASWELL)\n #define TARGET_GENERIC32 (ix86_tune == PROCESSOR_GENERIC32)\n #define TARGET_GENERIC64 (ix86_tune == PROCESSOR_GENERIC64)\n #define TARGET_GENERIC (TARGET_GENERIC32 || TARGET_GENERIC64)\n@@ -604,6 +605,7 @@ enum target_cpu_default\n   TARGET_CPU_DEFAULT_nocona,\n   TARGET_CPU_DEFAULT_core2,\n   TARGET_CPU_DEFAULT_corei7,\n+  TARGET_CPU_DEFAULT_haswell,\n   TARGET_CPU_DEFAULT_atom,\n \n   TARGET_CPU_DEFAULT_geode,\n@@ -2096,6 +2098,7 @@ enum processor_type\n   PROCESSOR_NOCONA,\n   PROCESSOR_CORE2,\n   PROCESSOR_COREI7,\n+  PROCESSOR_HASWELL,\n   PROCESSOR_GENERIC32,\n   PROCESSOR_GENERIC64,\n   PROCESSOR_AMDFAM10,"}]}