{"sha": "cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6Y2M2OGY3YzJkYWJiZjlkOTBlYmUwMjVhMTFmNmQ2ZWQ1ZDQ5YTExMw==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2019-11-16T11:02:09Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2019-11-16T11:02:09Z"}, "message": "[AArch64] Add autovec support for partial SVE vectors\n\nThis patch adds the bare minimum needed to support autovectorisation of\npartial SVE vectors, namely moves and integer addition.  Later patches\nadd more interesting cases.\n\n2019-11-16  Richard Sandiford  <richard.sandiford@arm.com>\n\ngcc/\n\t* config/aarch64/aarch64-modes.def: Define partial SVE vector\n\tfloat modes.\n\t* config/aarch64/aarch64-protos.h (aarch64_sve_pred_mode): New\n\tfunction.\n\t* config/aarch64/aarch64.c (aarch64_classify_vector_mode): Handle the\n\tnew vector float modes.\n\t(aarch64_sve_container_bits): New function.\n\t(aarch64_sve_pred_mode): Likewise.\n\t(aarch64_get_mask_mode): Use it.\n\t(aarch64_sve_element_int_mode): Handle structure modes and partial\n\tmodes.\n\t(aarch64_sve_container_int_mode): New function.\n\t(aarch64_vectorize_related_mode): Return SVE modes when given\n\tSVE modes.  Handle partial modes, taking the preferred number\n\tof units from the size of the given mode.\n\t(aarch64_hard_regno_mode_ok): Allow partial modes to be stored\n\tin registers.\n\t(aarch64_expand_sve_ld1rq): Use the mode form of aarch64_sve_pred_mode.\n\t(aarch64_expand_sve_const_vector): Handle partial SVE vectors.\n\t(aarch64_split_sve_subreg_move): Use the mode form of\n\taarch64_sve_pred_mode.\n\t(aarch64_secondary_reload): Handle partial modes in the same way\n\tas full big-endian vectors.\n\t(aarch64_vector_mode_supported_p): Allow partial SVE vectors.\n\t(aarch64_autovectorize_vector_modes): Try unpacked SVE vectors,\n\tmerging with the Advanced SIMD modes.  If two modes have the\n\tsame size, try the Advanced SIMD mode first.\n\t(aarch64_simd_valid_immediate): Use the container rather than\n\tthe element mode for INDEX constants.\n\t(aarch64_simd_vector_alignment): Make the alignment of partial\n\tSVE vector modes the same as their minimum size.\n\t(aarch64_evpc_sel): Use the mode form of aarch64_sve_pred_mode.\n\t* config/aarch64/aarch64-sve.md (mov<SVE_FULL:mode>): Extend to...\n\t(mov<SVE_ALL:mode>): ...this.\n\t(movmisalign<SVE_FULL:mode>): Extend to...\n\t(movmisalign<SVE_ALL:mode>): ...this.\n\t(*aarch64_sve_mov<mode>_le): Rename to...\n\t(*aarch64_sve_mov<mode>_ldr_str): ...this.\n\t(*aarch64_sve_mov<SVE_FULL:mode>_be): Rename and extend to...\n\t(*aarch64_sve_mov<SVE_ALL:mode>_no_ldr_str): ...this.  Handle\n\tpartial modes regardless of endianness.\n\t(aarch64_sve_reload_be): Rename to...\n\t(aarch64_sve_reload_mem): ...this and enable for little-endian.\n\tUse aarch64_sve_pred_mode to get the appropriate predicate mode.\n\t(@aarch64_pred_mov<SVE_FULL:mode>): Extend to...\n\t(@aarch64_pred_mov<SVE_ALL:mode>): ...this.\n\t(*aarch64_sve_mov<SVE_FULL:mode>_subreg_be): Extend to...\n\t(*aarch64_sve_mov<SVE_ALL:mode>_subreg_be): ...this.\n\t(@aarch64_sve_reinterpret<SVE_FULL:mode>): Extend to...\n\t(@aarch64_sve_reinterpret<SVE_ALL:mode>): ...this.\n\t(*aarch64_sve_reinterpret<SVE_FULL:mode>): Extend to...\n\t(*aarch64_sve_reinterpret<SVE_ALL:mode>): ...this.\n\t(maskload<SVE_FULL:mode><vpred>): Extend to...\n\t(maskload<SVE_ALL:mode><vpred>): ...this.\n\t(maskstore<SVE_FULL:mode><vpred>): Extend to...\n\t(maskstore<SVE_ALL:mode><vpred>): ...this.\n\t(vec_duplicate<SVE_FULL:mode>): Extend to...\n\t(vec_duplicate<SVE_ALL:mode>): ...this.\n\t(*vec_duplicate<SVE_FULL:mode>_reg): Extend to...\n\t(*vec_duplicate<SVE_ALL:mode>_reg): ...this.\n\t(sve_ld1r<SVE_FULL:mode>): Extend to...\n\t(sve_ld1r<SVE_ALL:mode>): ...this.\n\t(vec_series<SVE_FULL_I:mode>): Extend to...\n\t(vec_series<SVE_I:mode>): ...this.\n\t(*vec_series<SVE_FULL_I:mode>_plus): Extend to...\n\t(*vec_series<SVE_I:mode>_plus): ...this.\n\t(@aarch64_pred_sxt<SVE_FULL_HSDI:mode><SVE_PARTIAL_I:mode>): Avoid\n\tnew VPRED ambiguity.\n\t(@aarch64_cond_sxt<SVE_FULL_HSDI:mode><SVE_PARTIAL_I:mode>): Likewise.\n\t(add<SVE_FULL_I:mode>3): Extend to...\n\t(add<SVE_I:mode>3): ...this.\n\t* config/aarch64/iterators.md (SVE_ALL, SVE_I): New mode iterators.\n\t(Vetype, Vesize, VEL, Vel, vwcore): Handle partial SVE vector modes.\n\t(VPRED, vpred): Likewise.\n\t(Vctype): New iterator.\n\t(vw): Remove SVE modes.\n\ngcc/testsuite/\n\t* gcc.target/aarch64/sve/mixed_size_1.c: New test.\n\t* gcc.target/aarch64/sve/mixed_size_2.c: Likewise.\n\t* gcc.target/aarch64/sve/mixed_size_3.c: Likewise.\n\t* gcc.target/aarch64/sve/mixed_size_4.c: Likewise.\n\t* gcc.target/aarch64/sve/mixed_size_5.c: Likewise.\n\nFrom-SVN: r278341", "tree": {"sha": "e1153ba5bef0a2876d3723ec1fc5c332230bbb15", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/e1153ba5bef0a2876d3723ec1fc5c332230bbb15"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "html_url": "https://github.com/Rust-GCC/gccrs/commit/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "7f333599848c282904c62418abc109ede65751da", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7f333599848c282904c62418abc109ede65751da", "html_url": "https://github.com/Rust-GCC/gccrs/commit/7f333599848c282904c62418abc109ede65751da"}], "stats": {"total": 841, "additions": 674, "deletions": 167}, "files": [{"sha": "afb995fc1a70733a4a671c783f517739ed946f40", "filename": "gcc/ChangeLog", "status": "modified", "additions": 79, "deletions": 0, "changes": 79, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "patch": "@@ -1,3 +1,82 @@\n+2019-11-16  Richard Sandiford  <richard.sandiford@arm.com>\n+\n+\t* config/aarch64/aarch64-modes.def: Define partial SVE vector\n+\tfloat modes.\n+\t* config/aarch64/aarch64-protos.h (aarch64_sve_pred_mode): New\n+\tfunction.\n+\t* config/aarch64/aarch64.c (aarch64_classify_vector_mode): Handle the\n+\tnew vector float modes.\n+\t(aarch64_sve_container_bits): New function.\n+\t(aarch64_sve_pred_mode): Likewise.\n+\t(aarch64_get_mask_mode): Use it.\n+\t(aarch64_sve_element_int_mode): Handle structure modes and partial\n+\tmodes.\n+\t(aarch64_sve_container_int_mode): New function.\n+\t(aarch64_vectorize_related_mode): Return SVE modes when given\n+\tSVE modes.  Handle partial modes, taking the preferred number\n+\tof units from the size of the given mode.\n+\t(aarch64_hard_regno_mode_ok): Allow partial modes to be stored\n+\tin registers.\n+\t(aarch64_expand_sve_ld1rq): Use the mode form of aarch64_sve_pred_mode.\n+\t(aarch64_expand_sve_const_vector): Handle partial SVE vectors.\n+\t(aarch64_split_sve_subreg_move): Use the mode form of\n+\taarch64_sve_pred_mode.\n+\t(aarch64_secondary_reload): Handle partial modes in the same way\n+\tas full big-endian vectors.\n+\t(aarch64_vector_mode_supported_p): Allow partial SVE vectors.\n+\t(aarch64_autovectorize_vector_modes): Try unpacked SVE vectors,\n+\tmerging with the Advanced SIMD modes.  If two modes have the\n+\tsame size, try the Advanced SIMD mode first.\n+\t(aarch64_simd_valid_immediate): Use the container rather than\n+\tthe element mode for INDEX constants.\n+\t(aarch64_simd_vector_alignment): Make the alignment of partial\n+\tSVE vector modes the same as their minimum size.\n+\t(aarch64_evpc_sel): Use the mode form of aarch64_sve_pred_mode.\n+\t* config/aarch64/aarch64-sve.md (mov<SVE_FULL:mode>): Extend to...\n+\t(mov<SVE_ALL:mode>): ...this.\n+\t(movmisalign<SVE_FULL:mode>): Extend to...\n+\t(movmisalign<SVE_ALL:mode>): ...this.\n+\t(*aarch64_sve_mov<mode>_le): Rename to...\n+\t(*aarch64_sve_mov<mode>_ldr_str): ...this.\n+\t(*aarch64_sve_mov<SVE_FULL:mode>_be): Rename and extend to...\n+\t(*aarch64_sve_mov<SVE_ALL:mode>_no_ldr_str): ...this.  Handle\n+\tpartial modes regardless of endianness.\n+\t(aarch64_sve_reload_be): Rename to...\n+\t(aarch64_sve_reload_mem): ...this and enable for little-endian.\n+\tUse aarch64_sve_pred_mode to get the appropriate predicate mode.\n+\t(@aarch64_pred_mov<SVE_FULL:mode>): Extend to...\n+\t(@aarch64_pred_mov<SVE_ALL:mode>): ...this.\n+\t(*aarch64_sve_mov<SVE_FULL:mode>_subreg_be): Extend to...\n+\t(*aarch64_sve_mov<SVE_ALL:mode>_subreg_be): ...this.\n+\t(@aarch64_sve_reinterpret<SVE_FULL:mode>): Extend to...\n+\t(@aarch64_sve_reinterpret<SVE_ALL:mode>): ...this.\n+\t(*aarch64_sve_reinterpret<SVE_FULL:mode>): Extend to...\n+\t(*aarch64_sve_reinterpret<SVE_ALL:mode>): ...this.\n+\t(maskload<SVE_FULL:mode><vpred>): Extend to...\n+\t(maskload<SVE_ALL:mode><vpred>): ...this.\n+\t(maskstore<SVE_FULL:mode><vpred>): Extend to...\n+\t(maskstore<SVE_ALL:mode><vpred>): ...this.\n+\t(vec_duplicate<SVE_FULL:mode>): Extend to...\n+\t(vec_duplicate<SVE_ALL:mode>): ...this.\n+\t(*vec_duplicate<SVE_FULL:mode>_reg): Extend to...\n+\t(*vec_duplicate<SVE_ALL:mode>_reg): ...this.\n+\t(sve_ld1r<SVE_FULL:mode>): Extend to...\n+\t(sve_ld1r<SVE_ALL:mode>): ...this.\n+\t(vec_series<SVE_FULL_I:mode>): Extend to...\n+\t(vec_series<SVE_I:mode>): ...this.\n+\t(*vec_series<SVE_FULL_I:mode>_plus): Extend to...\n+\t(*vec_series<SVE_I:mode>_plus): ...this.\n+\t(@aarch64_pred_sxt<SVE_FULL_HSDI:mode><SVE_PARTIAL_I:mode>): Avoid\n+\tnew VPRED ambiguity.\n+\t(@aarch64_cond_sxt<SVE_FULL_HSDI:mode><SVE_PARTIAL_I:mode>): Likewise.\n+\t(add<SVE_FULL_I:mode>3): Extend to...\n+\t(add<SVE_I:mode>3): ...this.\n+\t* config/aarch64/iterators.md (SVE_ALL, SVE_I): New mode iterators.\n+\t(Vetype, Vesize, VEL, Vel, vwcore): Handle partial SVE vector modes.\n+\t(VPRED, vpred): Likewise.\n+\t(Vctype): New iterator.\n+\t(vw): Remove SVE modes.\n+\n 2019-11-16  Richard Sandiford  <richard.sandiford@arm.com>\n \n \t* config/aarch64/iterators.md (SVE_PARTIAL): Rename to..."}, {"sha": "3c698b620cd9ddf317f20da8b8a63aee64b39b32", "filename": "gcc/config/aarch64/aarch64-modes.def", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-modes.def?ref=cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "patch": "@@ -123,13 +123,18 @@ SVE_MODES (4, VNx64, VNx32, VNx16, VNx8)\n VECTOR_MODES_WITH_PREFIX (VNx, INT, 2, 1);\n VECTOR_MODES_WITH_PREFIX (VNx, INT, 4, 1);\n VECTOR_MODES_WITH_PREFIX (VNx, INT, 8, 1);\n+VECTOR_MODES_WITH_PREFIX (VNx, FLOAT, 4, 1);\n+VECTOR_MODES_WITH_PREFIX (VNx, FLOAT, 8, 1);\n \n ADJUST_NUNITS (VNx2QI, aarch64_sve_vg);\n ADJUST_NUNITS (VNx2HI, aarch64_sve_vg);\n ADJUST_NUNITS (VNx2SI, aarch64_sve_vg);\n+ADJUST_NUNITS (VNx2HF, aarch64_sve_vg);\n+ADJUST_NUNITS (VNx2SF, aarch64_sve_vg);\n \n ADJUST_NUNITS (VNx4QI, aarch64_sve_vg * 2);\n ADJUST_NUNITS (VNx4HI, aarch64_sve_vg * 2);\n+ADJUST_NUNITS (VNx4HF, aarch64_sve_vg * 2);\n \n ADJUST_NUNITS (VNx8QI, aarch64_sve_vg * 4);\n \n@@ -139,8 +144,11 @@ ADJUST_ALIGNMENT (VNx8QI, 1);\n \n ADJUST_ALIGNMENT (VNx2HI, 2);\n ADJUST_ALIGNMENT (VNx4HI, 2);\n+ADJUST_ALIGNMENT (VNx2HF, 2);\n+ADJUST_ALIGNMENT (VNx4HF, 2);\n \n ADJUST_ALIGNMENT (VNx2SI, 4);\n+ADJUST_ALIGNMENT (VNx2SF, 4);\n \n /* Quad float: 128-bit floating mode for long doubles.  */\n FLOAT_MODE (TF, 16, ieee_quad_format);"}, {"sha": "bcb3fd498c5830099b6214c17aba28e386333438", "filename": "gcc/config/aarch64/aarch64-protos.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h?ref=cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "patch": "@@ -512,6 +512,7 @@ bool aarch64_zero_extend_const_eq (machine_mode, rtx, machine_mode, rtx);\n bool aarch64_move_imm (HOST_WIDE_INT, machine_mode);\n machine_mode aarch64_sve_int_mode (machine_mode);\n opt_machine_mode aarch64_sve_pred_mode (unsigned int);\n+machine_mode aarch64_sve_pred_mode (machine_mode);\n opt_machine_mode aarch64_sve_data_mode (scalar_mode, poly_uint64);\n bool aarch64_sve_mode_p (machine_mode);\n HOST_WIDE_INT aarch64_fold_sve_cnt_pat (aarch64_svpattern, unsigned int);"}, {"sha": "b43d4fbe85747d2365a6cf74d3d4ef95d09194c0", "filename": "gcc/config/aarch64/aarch64-sve.md", "status": "modified", "additions": 68, "deletions": 65, "changes": 133, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Fconfig%2Faarch64%2Faarch64-sve.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Fconfig%2Faarch64%2Faarch64-sve.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-sve.md?ref=cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "patch": "@@ -546,8 +546,8 @@\n ;; -------------------------------------------------------------------------\n \n (define_expand \"mov<mode>\"\n-  [(set (match_operand:SVE_FULL 0 \"nonimmediate_operand\")\n-\t(match_operand:SVE_FULL 1 \"general_operand\"))]\n+  [(set (match_operand:SVE_ALL 0 \"nonimmediate_operand\")\n+\t(match_operand:SVE_ALL 1 \"general_operand\"))]\n   \"TARGET_SVE\"\n   {\n     /* Use the predicated load and store patterns where possible.\n@@ -576,8 +576,8 @@\n )\n \n (define_expand \"movmisalign<mode>\"\n-  [(set (match_operand:SVE_FULL 0 \"nonimmediate_operand\")\n-\t(match_operand:SVE_FULL 1 \"general_operand\"))]\n+  [(set (match_operand:SVE_ALL 0 \"nonimmediate_operand\")\n+\t(match_operand:SVE_ALL 1 \"general_operand\"))]\n   \"TARGET_SVE\"\n   {\n     /* Equivalent to a normal move for our purpooses.  */\n@@ -586,10 +586,11 @@\n   }\n )\n \n-;; Unpredicated moves (bytes or little-endian).  Only allow memory operations\n-;; during and after RA; before RA we want the predicated load and store\n-;; patterns to be used instead.\n-(define_insn \"*aarch64_sve_mov<mode>_le\"\n+;; Unpredicated moves that can use LDR and STR, i.e. full vectors for which\n+;; little-endian ordering is acceptable.  Only allow memory operations during\n+;; and after RA; before RA we want the predicated load and store patterns to\n+;; be used instead.\n+(define_insn \"*aarch64_sve_mov<mode>_ldr_str\"\n   [(set (match_operand:SVE_FULL 0 \"aarch64_sve_nonimmediate_operand\" \"=w, Utr, w, w\")\n \t(match_operand:SVE_FULL 1 \"aarch64_sve_general_operand\" \"Utr, w, w, Dn\"))]\n   \"TARGET_SVE\n@@ -604,35 +605,37 @@\n    * return aarch64_output_sve_mov_immediate (operands[1]);\"\n )\n \n-;; Unpredicated moves (non-byte big-endian).  Memory accesses require secondary\n-;; reloads.\n-(define_insn \"*aarch64_sve_mov<mode>_be\"\n-  [(set (match_operand:SVE_FULL 0 \"register_operand\" \"=w, w\")\n-\t(match_operand:SVE_FULL 1 \"aarch64_nonmemory_operand\" \"w, Dn\"))]\n-  \"TARGET_SVE && BYTES_BIG_ENDIAN && <MODE>mode != VNx16QImode\"\n+;; Unpredicated moves that cannot use LDR and STR, i.e. partial vectors\n+;; or vectors for which little-endian ordering isn't acceptable.  Memory\n+;; accesses require secondary reloads.\n+(define_insn \"*aarch64_sve_mov<mode>_no_ldr_str\"\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w, w\")\n+\t(match_operand:SVE_ALL 1 \"aarch64_nonmemory_operand\" \"w, Dn\"))]\n+  \"TARGET_SVE\n+   && <MODE>mode != VNx16QImode\n+   && (BYTES_BIG_ENDIAN\n+       || maybe_ne (BYTES_PER_SVE_VECTOR, GET_MODE_SIZE (<MODE>mode)))\"\n   \"@\n    mov\\t%0.d, %1.d\n    * return aarch64_output_sve_mov_immediate (operands[1]);\"\n )\n \n-;; Handle big-endian memory reloads.  We use byte PTRUE for all modes\n-;; to try to encourage reuse.\n-;; This pattern needs constraints due to TARGET_SECONDARY_RELOAD hook.\n-(define_expand \"aarch64_sve_reload_be\"\n+;; Handle memory reloads for modes that can't use LDR and STR.  We use\n+;; byte PTRUE for all modes to try to encourage reuse.  This pattern\n+;; needs constraints because it is returned by TARGET_SECONDARY_RELOAD.\n+(define_expand \"aarch64_sve_reload_mem\"\n   [(parallel\n      [(set (match_operand 0)\n \t   (match_operand 1))\n       (clobber (match_operand:VNx16BI 2 \"register_operand\" \"=Upl\"))])]\n-  \"TARGET_SVE && BYTES_BIG_ENDIAN\"\n+  \"TARGET_SVE\"\n   {\n     /* Create a PTRUE.  */\n     emit_move_insn (operands[2], CONSTM1_RTX (VNx16BImode));\n \n     /* Refer to the PTRUE in the appropriate mode for this move.  */\n     machine_mode mode = GET_MODE (operands[0]);\n-    machine_mode pred_mode\n-      = aarch64_sve_pred_mode (GET_MODE_UNIT_SIZE (mode)).require ();\n-    rtx pred = gen_lowpart (pred_mode, operands[2]);\n+    rtx pred = gen_lowpart (aarch64_sve_pred_mode (mode), operands[2]);\n \n     /* Emit a predicated load or store.  */\n     aarch64_emit_sve_pred_move (operands[0], pred, operands[1]);\n@@ -644,18 +647,18 @@\n ;; Note that this pattern is generated directly by aarch64_emit_sve_pred_move,\n ;; so changes to this pattern will need changes there as well.\n (define_insn_and_split \"@aarch64_pred_mov<mode>\"\n-  [(set (match_operand:SVE_FULL 0 \"nonimmediate_operand\" \"=w, w, m\")\n-\t(unspec:SVE_FULL\n+  [(set (match_operand:SVE_ALL 0 \"nonimmediate_operand\" \"=w, w, m\")\n+\t(unspec:SVE_ALL\n \t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl, Upl\")\n-\t   (match_operand:SVE_FULL 2 \"nonimmediate_operand\" \"w, m, w\")]\n+\t   (match_operand:SVE_ALL 2 \"nonimmediate_operand\" \"w, m, w\")]\n \t  UNSPEC_PRED_X))]\n   \"TARGET_SVE\n    && (register_operand (operands[0], <MODE>mode)\n        || register_operand (operands[2], <MODE>mode))\"\n   \"@\n    #\n-   ld1<Vesize>\\t%0.<Vetype>, %1/z, %2\n-   st1<Vesize>\\t%2.<Vetype>, %1, %0\"\n+   ld1<Vesize>\\t%0.<Vctype>, %1/z, %2\n+   st1<Vesize>\\t%2.<Vctype>, %1, %0\"\n   \"&& register_operand (operands[0], <MODE>mode)\n    && register_operand (operands[2], <MODE>mode)\"\n   [(set (match_dup 0) (match_dup 2))]\n@@ -666,8 +669,8 @@\n ;; for details.  We use a special predicate for operand 2 to reduce\n ;; the number of patterns.\n (define_insn_and_split \"*aarch64_sve_mov<mode>_subreg_be\"\n-  [(set (match_operand:SVE_FULL 0 \"aarch64_sve_nonimmediate_operand\" \"=w\")\n-\t(unspec:SVE_FULL\n+  [(set (match_operand:SVE_ALL 0 \"aarch64_sve_nonimmediate_operand\" \"=w\")\n+\t(unspec:SVE_ALL\n \t  [(match_operand:VNx16BI 1 \"register_operand\" \"Upl\")\n \t   (match_operand 2 \"aarch64_any_register_operand\" \"w\")]\n \t  UNSPEC_REV_SUBREG))]\n@@ -685,8 +688,8 @@\n ;; This is equivalent to a subreg on little-endian targets but not for\n ;; big-endian; see the comment at the head of the file for details.\n (define_expand \"@aarch64_sve_reinterpret<mode>\"\n-  [(set (match_operand:SVE_FULL 0 \"register_operand\")\n-\t(unspec:SVE_FULL\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\")\n+\t(unspec:SVE_ALL\n \t  [(match_operand 1 \"aarch64_any_register_operand\")]\n \t  UNSPEC_REINTERPRET))]\n   \"TARGET_SVE\"\n@@ -702,8 +705,8 @@\n ;; A pattern for handling type punning on big-endian targets.  We use a\n ;; special predicate for operand 1 to reduce the number of patterns.\n (define_insn_and_split \"*aarch64_sve_reinterpret<mode>\"\n-  [(set (match_operand:SVE_FULL 0 \"register_operand\" \"=w\")\n-\t(unspec:SVE_FULL\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_ALL\n \t  [(match_operand 1 \"aarch64_any_register_operand\" \"w\")]\n \t  UNSPEC_REINTERPRET))]\n   \"TARGET_SVE\"\n@@ -1141,13 +1144,13 @@\n \n ;; Predicated LD1.\n (define_insn \"maskload<mode><vpred>\"\n-  [(set (match_operand:SVE_FULL 0 \"register_operand\" \"=w\")\n-\t(unspec:SVE_FULL\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_ALL\n \t  [(match_operand:<VPRED> 2 \"register_operand\" \"Upl\")\n-\t   (match_operand:SVE_FULL 1 \"memory_operand\" \"m\")]\n+\t   (match_operand:SVE_ALL 1 \"memory_operand\" \"m\")]\n \t  UNSPEC_LD1_SVE))]\n   \"TARGET_SVE\"\n-  \"ld1<Vesize>\\t%0.<Vetype>, %2/z, %1\"\n+  \"ld1<Vesize>\\t%0.<Vctype>, %2/z, %1\"\n )\n \n ;; Unpredicated LD[234].\n@@ -1940,14 +1943,14 @@\n \n ;; Predicated ST1.\n (define_insn \"maskstore<mode><vpred>\"\n-  [(set (match_operand:SVE_FULL 0 \"memory_operand\" \"+m\")\n-\t(unspec:SVE_FULL\n+  [(set (match_operand:SVE_ALL 0 \"memory_operand\" \"+m\")\n+\t(unspec:SVE_ALL\n \t  [(match_operand:<VPRED> 2 \"register_operand\" \"Upl\")\n-\t   (match_operand:SVE_FULL 1 \"register_operand\" \"w\")\n+\t   (match_operand:SVE_ALL 1 \"register_operand\" \"w\")\n \t   (match_dup 0)]\n \t  UNSPEC_ST1_SVE))]\n   \"TARGET_SVE\"\n-  \"st1<Vesize>\\t%1.<Vetype>, %2, %0\"\n+  \"st1<Vesize>\\t%1.<Vctype>, %2, %0\"\n )\n \n ;; Unpredicated ST[234].  This is always a full update, so the dependence\n@@ -2283,8 +2286,8 @@\n \n (define_expand \"vec_duplicate<mode>\"\n   [(parallel\n-    [(set (match_operand:SVE_FULL 0 \"register_operand\")\n-\t  (vec_duplicate:SVE_FULL\n+    [(set (match_operand:SVE_ALL 0 \"register_operand\")\n+\t  (vec_duplicate:SVE_ALL\n \t    (match_operand:<VEL> 1 \"aarch64_sve_dup_operand\")))\n      (clobber (scratch:VNx16BI))])]\n   \"TARGET_SVE\"\n@@ -2304,8 +2307,8 @@\n ;; the load at the first opportunity in order to allow the PTRUE to be\n ;; optimized with surrounding code.\n (define_insn_and_split \"*vec_duplicate<mode>_reg\"\n-  [(set (match_operand:SVE_FULL 0 \"register_operand\" \"=w, w, w\")\n-\t(vec_duplicate:SVE_FULL\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w, w, w\")\n+\t(vec_duplicate:SVE_ALL\n \t  (match_operand:<VEL> 1 \"aarch64_sve_dup_operand\" \"r, w, Uty\")))\n    (clobber (match_scratch:VNx16BI 2 \"=X, X, Upl\"))]\n   \"TARGET_SVE\"\n@@ -2364,12 +2367,12 @@\n ;; be used by combine to optimize selects of a a vec_duplicate<mode>\n ;; with zero.\n (define_insn \"sve_ld1r<mode>\"\n-  [(set (match_operand:SVE_FULL 0 \"register_operand\" \"=w\")\n-\t(unspec:SVE_FULL\n+  [(set (match_operand:SVE_ALL 0 \"register_operand\" \"=w\")\n+\t(unspec:SVE_ALL\n \t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n-\t   (vec_duplicate:SVE_FULL\n+\t   (vec_duplicate:SVE_ALL\n \t     (match_operand:<VEL> 2 \"aarch64_sve_ld1r_operand\" \"Uty\"))\n-\t   (match_operand:SVE_FULL 3 \"aarch64_simd_imm_zero\")]\n+\t   (match_operand:SVE_ALL 3 \"aarch64_simd_imm_zero\")]\n \t  UNSPEC_SEL))]\n   \"TARGET_SVE\"\n   \"ld1r<Vesize>\\t%0.<Vetype>, %1/z, %2\"\n@@ -2431,29 +2434,29 @@\n ;; -------------------------------------------------------------------------\n \n (define_insn \"vec_series<mode>\"\n-  [(set (match_operand:SVE_FULL_I 0 \"register_operand\" \"=w, w, w\")\n-\t(vec_series:SVE_FULL_I\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w, w, w\")\n+\t(vec_series:SVE_I\n \t  (match_operand:<VEL> 1 \"aarch64_sve_index_operand\" \"Usi, r, r\")\n \t  (match_operand:<VEL> 2 \"aarch64_sve_index_operand\" \"r, Usi, r\")))]\n   \"TARGET_SVE\"\n   \"@\n-   index\\t%0.<Vetype>, #%1, %<vw>2\n-   index\\t%0.<Vetype>, %<vw>1, #%2\n-   index\\t%0.<Vetype>, %<vw>1, %<vw>2\"\n+   index\\t%0.<Vctype>, #%1, %<vwcore>2\n+   index\\t%0.<Vctype>, %<vwcore>1, #%2\n+   index\\t%0.<Vctype>, %<vwcore>1, %<vwcore>2\"\n )\n \n ;; Optimize {x, x, x, x, ...} + {0, n, 2*n, 3*n, ...} if n is in range\n ;; of an INDEX instruction.\n (define_insn \"*vec_series<mode>_plus\"\n-  [(set (match_operand:SVE_FULL_I 0 \"register_operand\" \"=w\")\n-\t(plus:SVE_FULL_I\n-\t  (vec_duplicate:SVE_FULL_I\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w\")\n+\t(plus:SVE_I\n+\t  (vec_duplicate:SVE_I\n \t    (match_operand:<VEL> 1 \"register_operand\" \"r\"))\n-\t  (match_operand:SVE_FULL_I 2 \"immediate_operand\")))]\n+\t  (match_operand:SVE_I 2 \"immediate_operand\")))]\n   \"TARGET_SVE && aarch64_check_zero_based_sve_index_immediate (operands[2])\"\n   {\n     operands[2] = aarch64_check_zero_based_sve_index_immediate (operands[2]);\n-    return \"index\\t%0.<Vetype>, %<vw>1, #%2\";\n+    return \"index\\t%0.<Vctype>, %<vwcore>1, #%2\";\n   }\n )\n \n@@ -2821,7 +2824,7 @@\n (define_insn \"@aarch64_pred_sxt<SVE_FULL_HSDI:mode><SVE_PARTIAL_I:mode>\"\n   [(set (match_operand:SVE_FULL_HSDI 0 \"register_operand\" \"=w\")\n \t(unspec:SVE_FULL_HSDI\n-\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl\")\n+\t  [(match_operand:<SVE_FULL_HSDI:VPRED> 1 \"register_operand\" \"Upl\")\n \t   (sign_extend:SVE_FULL_HSDI\n \t     (truncate:SVE_PARTIAL_I\n \t       (match_operand:SVE_FULL_HSDI 2 \"register_operand\" \"w\")))]\n@@ -2834,7 +2837,7 @@\n (define_insn \"@aarch64_cond_sxt<SVE_FULL_HSDI:mode><SVE_PARTIAL_I:mode>\"\n   [(set (match_operand:SVE_FULL_HSDI 0 \"register_operand\" \"=w, ?&w, ?&w\")\n \t(unspec:SVE_FULL_HSDI\n-\t  [(match_operand:<VPRED> 1 \"register_operand\" \"Upl, Upl, Upl\")\n+\t  [(match_operand:<SVE_FULL_HSDI:VPRED> 1 \"register_operand\" \"Upl, Upl, Upl\")\n \t   (sign_extend:SVE_FULL_HSDI\n \t     (truncate:SVE_PARTIAL_I\n \t       (match_operand:SVE_FULL_HSDI 2 \"register_operand\" \"w, w, w\")))\n@@ -3386,10 +3389,10 @@\n ;; -------------------------------------------------------------------------\n \n (define_insn \"add<mode>3\"\n-  [(set (match_operand:SVE_FULL_I 0 \"register_operand\" \"=w, w, w, ?w, ?w, w\")\n-\t(plus:SVE_FULL_I\n-\t  (match_operand:SVE_FULL_I 1 \"register_operand\" \"%0, 0, 0, w, w, w\")\n-\t  (match_operand:SVE_FULL_I 2 \"aarch64_sve_add_operand\" \"vsa, vsn, vsi, vsa, vsn, w\")))]\n+  [(set (match_operand:SVE_I 0 \"register_operand\" \"=w, w, w, ?w, ?w, w\")\n+\t(plus:SVE_I\n+\t  (match_operand:SVE_I 1 \"register_operand\" \"%0, 0, 0, w, w, w\")\n+\t  (match_operand:SVE_I 2 \"aarch64_sve_add_operand\" \"vsa, vsn, vsi, vsa, vsn, w\")))]\n   \"TARGET_SVE\"\n   \"@\n    add\\t%0.<Vetype>, %0.<Vetype>, #%D2"}, {"sha": "d175e1f5e01bc4d4e4bfe4b5be3ad9d7abcc2845", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 185, "deletions": 45, "changes": 230, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "patch": "@@ -1625,6 +1625,11 @@ aarch64_classify_vector_mode (machine_mode mode)\n     case E_VNx4HImode:\n     /* Partial SVE SI vector.  */\n     case E_VNx2SImode:\n+    /* Partial SVE HF vectors.  */\n+    case E_VNx2HFmode:\n+    case E_VNx4HFmode:\n+    /* Partial SVE SF vector.  */\n+    case E_VNx2SFmode:\n       return TARGET_SVE ? VEC_SVE_DATA | VEC_PARTIAL : 0;\n \n     case E_VNx16QImode:\n@@ -1753,6 +1758,22 @@ aarch64_array_mode_supported_p (machine_mode mode,\n   return false;\n }\n \n+/* MODE is some form of SVE vector mode.  For data modes, return the number\n+   of vector register bits that each element of MODE occupies, such as 64\n+   for both VNx2DImode and VNx2SImode (where each 32-bit value is stored\n+   in a 64-bit container).  For predicate modes, return the number of\n+   data bits controlled by each significant predicate bit.  */\n+\n+static unsigned int\n+aarch64_sve_container_bits (machine_mode mode)\n+{\n+  unsigned int vec_flags = aarch64_classify_vector_mode (mode);\n+  poly_uint64 vector_bits = (vec_flags & (VEC_PARTIAL | VEC_SVE_PRED)\n+\t\t\t     ? BITS_PER_SVE_VECTOR\n+\t\t\t     : GET_MODE_BITSIZE (mode));\n+  return vector_element_size (vector_bits, GET_MODE_NUNITS (mode));\n+}\n+\n /* Return the SVE predicate mode to use for elements that have\n    ELEM_NBYTES bytes, if such a mode exists.  */\n \n@@ -1773,14 +1794,24 @@ aarch64_sve_pred_mode (unsigned int elem_nbytes)\n   return opt_machine_mode ();\n }\n \n+/* Return the SVE predicate mode that should be used to control\n+   SVE mode MODE.  */\n+\n+machine_mode\n+aarch64_sve_pred_mode (machine_mode mode)\n+{\n+  unsigned int bits = aarch64_sve_container_bits (mode);\n+  return aarch64_sve_pred_mode (bits / BITS_PER_UNIT).require ();\n+}\n+\n /* Implement TARGET_VECTORIZE_GET_MASK_MODE.  */\n \n static opt_machine_mode\n aarch64_get_mask_mode (machine_mode mode)\n {\n   unsigned int vec_flags = aarch64_classify_vector_mode (mode);\n   if (vec_flags & VEC_SVE_DATA)\n-    return aarch64_sve_pred_mode (GET_MODE_UNIT_SIZE (mode));\n+    return aarch64_sve_pred_mode (mode);\n \n   return default_get_mask_mode (mode);\n }\n@@ -1806,11 +1837,25 @@ aarch64_sve_data_mode (scalar_mode inner_mode, poly_uint64 nunits)\n static scalar_int_mode\n aarch64_sve_element_int_mode (machine_mode mode)\n {\n-  unsigned int elt_bits = vector_element_size (BITS_PER_SVE_VECTOR,\n+  poly_uint64 vector_bits = (GET_MODE_CLASS (mode) == MODE_VECTOR_BOOL\n+\t\t\t     ? BITS_PER_SVE_VECTOR\n+\t\t\t     : GET_MODE_BITSIZE (mode));\n+  unsigned int elt_bits = vector_element_size (vector_bits,\n \t\t\t\t\t       GET_MODE_NUNITS (mode));\n   return int_mode_for_size (elt_bits, 0).require ();\n }\n \n+/* Return an integer element mode that contains exactly\n+   aarch64_sve_container_bits (MODE) bits.  This is wider than\n+   aarch64_sve_element_int_mode if MODE is a partial vector,\n+   otherwise it's the same.  */\n+\n+static scalar_int_mode\n+aarch64_sve_container_int_mode (machine_mode mode)\n+{\n+  return int_mode_for_size (aarch64_sve_container_bits (mode), 0).require ();\n+}\n+\n /* Return the integer vector mode associated with SVE mode MODE.\n    Unlike related_int_vector_mode, this can handle the case in which\n    MODE is a predicate (and thus has a different total size).  */\n@@ -1831,6 +1876,37 @@ aarch64_vectorize_related_mode (machine_mode vector_mode,\n {\n   unsigned int vec_flags = aarch64_classify_vector_mode (vector_mode);\n \n+  /* If we're operating on SVE vectors, try to return an SVE mode.  */\n+  poly_uint64 sve_nunits;\n+  if ((vec_flags & VEC_SVE_DATA)\n+      && multiple_p (BYTES_PER_SVE_VECTOR,\n+\t\t     GET_MODE_SIZE (element_mode), &sve_nunits))\n+    {\n+      machine_mode sve_mode;\n+      if (maybe_ne (nunits, 0U))\n+\t{\n+\t  /* Try to find a full or partial SVE mode with exactly\n+\t     NUNITS units.  */\n+\t  if (multiple_p (sve_nunits, nunits)\n+\t      && aarch64_sve_data_mode (element_mode,\n+\t\t\t\t\tnunits).exists (&sve_mode))\n+\t    return sve_mode;\n+\t}\n+      else\n+\t{\n+\t  /* Take the preferred number of units from the number of bytes\n+\t     that fit in VECTOR_MODE.  We always start by \"autodetecting\"\n+\t     a full vector mode with preferred_simd_mode, so vectors\n+\t     chosen here will also be full vector modes.  Then\n+\t     autovectorize_vector_modes tries smaller starting modes\n+\t     and thus smaller preferred numbers of units.  */\n+\t  sve_nunits = ordered_min (sve_nunits, GET_MODE_SIZE (vector_mode));\n+\t  if (aarch64_sve_data_mode (element_mode,\n+\t\t\t\t     sve_nunits).exists (&sve_mode))\n+\t    return sve_mode;\n+\t}\n+    }\n+\n   /* Prefer to use 1 128-bit vector instead of 2 64-bit vectors.  */\n   if ((vec_flags & VEC_ADVSIMD)\n       && known_eq (nunits, 0U)\n@@ -1907,11 +1983,6 @@ aarch64_hard_regno_mode_ok (unsigned regno, machine_mode mode)\n     return mode == DImode;\n \n   unsigned int vec_flags = aarch64_classify_vector_mode (mode);\n-  /* At the moment, partial vector modes are only useful for memory\n-     references, but that could change in future.  */\n-  if (vec_flags & VEC_PARTIAL)\n-    return false;\n-\n   if (vec_flags & VEC_SVE_PRED)\n     return pr_or_ffr_regnum_p (regno);\n \n@@ -4015,8 +4086,7 @@ aarch64_expand_sve_ld1rq (rtx dest, rtx src)\n     }\n \n   machine_mode mode = GET_MODE (dest);\n-  unsigned int elem_bytes = GET_MODE_UNIT_SIZE (mode);\n-  machine_mode pred_mode = aarch64_sve_pred_mode (elem_bytes).require ();\n+  machine_mode pred_mode = aarch64_sve_pred_mode (mode);\n   rtx ptrue = aarch64_ptrue_reg (pred_mode);\n   emit_insn (gen_aarch64_sve_ld1rq (mode, dest, src, ptrue));\n   return true;\n@@ -4037,7 +4107,26 @@ aarch64_expand_sve_const_vector (rtx target, rtx src)\n   unsigned int nelts_per_pattern = CONST_VECTOR_NELTS_PER_PATTERN (src);\n   scalar_mode elt_mode = GET_MODE_INNER (mode);\n   unsigned int elt_bits = GET_MODE_BITSIZE (elt_mode);\n-  unsigned int encoded_bits = npatterns * nelts_per_pattern * elt_bits;\n+  unsigned int container_bits = aarch64_sve_container_bits (mode);\n+  unsigned int encoded_bits = npatterns * nelts_per_pattern * container_bits;\n+\n+  if (nelts_per_pattern == 1\n+      && encoded_bits <= 128\n+      && container_bits != elt_bits)\n+    {\n+      /* We have a partial vector mode and a constant whose full-vector\n+\t equivalent would occupy a repeating 128-bit sequence.  Build that\n+\t full-vector equivalent instead, so that we have the option of\n+\t using LD1RQ and Advanced SIMD operations.  */\n+      unsigned int repeat = container_bits / elt_bits;\n+      machine_mode full_mode = aarch64_full_sve_mode (elt_mode).require ();\n+      rtx_vector_builder builder (full_mode, npatterns * repeat, 1);\n+      for (unsigned int i = 0; i < npatterns; ++i)\n+\tfor (unsigned int j = 0; j < repeat; ++j)\n+\t  builder.quick_push (CONST_VECTOR_ENCODED_ELT (src, i));\n+      target = aarch64_target_reg (target, full_mode);\n+      return aarch64_expand_sve_const_vector (target, builder.build ());\n+    }\n \n   if (nelts_per_pattern == 1 && encoded_bits == 128)\n     {\n@@ -4730,8 +4819,7 @@ aarch64_split_sve_subreg_move (rtx dest, rtx ptrue, rtx src)\n     std::swap (mode_with_wider_elts, mode_with_narrower_elts);\n \n   unsigned int unspec = aarch64_sve_rev_unspec (mode_with_narrower_elts);\n-  unsigned int wider_bytes = GET_MODE_UNIT_SIZE (mode_with_wider_elts);\n-  machine_mode pred_mode = aarch64_sve_pred_mode (wider_bytes).require ();\n+  machine_mode pred_mode = aarch64_sve_pred_mode (mode_with_wider_elts);\n \n   /* Get the operands in the appropriate modes and emit the instruction.  */\n   ptrue = gen_lowpart (pred_mode, ptrue);\n@@ -9971,19 +10059,21 @@ aarch64_secondary_reload (bool in_p ATTRIBUTE_UNUSED, rtx x,\n \t\t\t  machine_mode mode,\n \t\t\t  secondary_reload_info *sri)\n {\n-  /* Use aarch64_sve_reload_be for SVE reloads that cannot be handled\n-     directly by the *aarch64_sve_mov<mode>_[lb]e move patterns.  See the\n-     comment at the head of aarch64-sve.md for more details about the\n-     big-endian handling.  */\n-  if (BYTES_BIG_ENDIAN\n-      && reg_class_subset_p (rclass, FP_REGS)\n+  /* Use aarch64_sve_reload_mem for SVE memory reloads that cannot use\n+     LDR and STR.  See the comment at the head of aarch64-sve.md for\n+     more details about the big-endian handling.  */\n+  if (reg_class_subset_p (rclass, FP_REGS)\n       && !((REG_P (x) && HARD_REGISTER_P (x))\n \t   || aarch64_simd_valid_immediate (x, NULL))\n-      && mode != VNx16QImode\n-      && aarch64_sve_data_mode_p (mode))\n+      && mode != VNx16QImode)\n     {\n-      sri->icode = CODE_FOR_aarch64_sve_reload_be;\n-      return NO_REGS;\n+      unsigned int vec_flags = aarch64_classify_vector_mode (mode);\n+      if ((vec_flags & VEC_SVE_DATA)\n+\t  && ((vec_flags & VEC_PARTIAL) || BYTES_BIG_ENDIAN))\n+\t{\n+\t  sri->icode = CODE_FOR_aarch64_sve_reload_mem;\n+\t  return NO_REGS;\n+\t}\n     }\n \n   /* If we have to disable direct literal pool loads and stores because the\n@@ -15837,7 +15927,7 @@ static bool\n aarch64_vector_mode_supported_p (machine_mode mode)\n {\n   unsigned int vec_flags = aarch64_classify_vector_mode (mode);\n-  return vec_flags != 0 && (vec_flags & (VEC_STRUCT | VEC_PARTIAL)) == 0;\n+  return vec_flags != 0 && (vec_flags & VEC_STRUCT) == 0;\n }\n \n /* Return the full-width SVE vector mode for element mode MODE, if one\n@@ -15938,29 +16028,72 @@ aarch64_preferred_simd_mode (scalar_mode mode)\n static unsigned int\n aarch64_autovectorize_vector_modes (vector_modes *modes, bool)\n {\n-  if (TARGET_SVE)\n-    modes->safe_push (VNx16QImode);\n+  static const machine_mode sve_modes[] = {\n+    /* Try using full vectors for all element types.  */\n+    VNx16QImode,\n+\n+    /* Try using 16-bit containers for 8-bit elements and full vectors\n+       for wider elements.  */\n+    VNx8QImode,\n+\n+    /* Try using 32-bit containers for 8-bit and 16-bit elements and\n+       full vectors for wider elements.  */\n+    VNx4QImode,\n \n-  /* Try using 128-bit vectors for all element types.  */\n-  modes->safe_push (V16QImode);\n+    /* Try using 64-bit containers for all element types.  */\n+    VNx2QImode\n+  };\n+\n+  static const machine_mode advsimd_modes[] = {\n+    /* Try using 128-bit vectors for all element types.  */\n+    V16QImode,\n+\n+    /* Try using 64-bit vectors for 8-bit elements and 128-bit vectors\n+       for wider elements.  */\n+    V8QImode,\n+\n+    /* Try using 64-bit vectors for 16-bit elements and 128-bit vectors\n+       for wider elements.\n+\n+       TODO: We could support a limited form of V4QImode too, so that\n+       we use 32-bit vectors for 8-bit elements.  */\n+    V4HImode,\n+\n+    /* Try using 64-bit vectors for 32-bit elements and 128-bit vectors\n+       for 64-bit elements.\n \n-  /* Try using 64-bit vectors for 8-bit elements and 128-bit vectors\n-     for wider elements.  */\n-  modes->safe_push (V8QImode);\n+       TODO: We could similarly support limited forms of V2QImode and V2HImode\n+       for this case.  */\n+    V2SImode\n+  };\n \n-  /* Try using 64-bit vectors for 16-bit elements and 128-bit vectors\n-     for wider elements.\n+  /* Try using N-byte SVE modes only after trying N-byte Advanced SIMD mode.\n+     This is because:\n \n-     TODO: We could support a limited form of V4QImode too, so that\n-     we use 32-bit vectors for 8-bit elements.  */\n-  modes->safe_push (V4HImode);\n+     - If we can't use N-byte Advanced SIMD vectors then the placement\n+       doesn't matter; we'll just continue as though the Advanced SIMD\n+       entry didn't exist.\n \n-  /* Try using 64-bit vectors for 32-bit elements and 128-bit vectors\n-     for 64-bit elements.\n+     - If an SVE main loop with N bytes ends up being cheaper than an\n+       Advanced SIMD main loop with N bytes then by default we'll replace\n+       the Advanced SIMD version with the SVE one.\n \n-     TODO: We could similarly support limited forms of V2QImode and V2HImode\n-     for this case.  */\n-  modes->safe_push (V2SImode);\n+     - If an Advanced SIMD main loop with N bytes ends up being cheaper\n+       than an SVE main loop with N bytes then by default we'll try to\n+       use the SVE loop to vectorize the epilogue instead.  */\n+  unsigned int sve_i = TARGET_SVE ? 0 : ARRAY_SIZE (sve_modes);\n+  unsigned int advsimd_i = 0;\n+  while (advsimd_i < ARRAY_SIZE (advsimd_modes))\n+    {\n+      if (sve_i < ARRAY_SIZE (sve_modes)\n+\t  && maybe_gt (GET_MODE_NUNITS (sve_modes[sve_i]),\n+\t\t       GET_MODE_NUNITS (advsimd_modes[advsimd_i])))\n+\tmodes->safe_push (sve_modes[sve_i++]);\n+      else\n+\tmodes->safe_push (advsimd_modes[advsimd_i++]);\n+    }\n+  while (sve_i < ARRAY_SIZE (sve_modes))\n+    modes->safe_push (sve_modes[sve_i++]);\n \n   unsigned int flags = 0;\n   /* Consider enabling VECT_COMPARE_COSTS for SVE, both so that we\n@@ -16507,7 +16640,14 @@ aarch64_simd_valid_immediate (rtx op, simd_immediate_info *info,\n \treturn false;\n \n       if (info)\n-\t*info = simd_immediate_info (elt_mode, base, step);\n+\t{\n+\t  /* Get the corresponding container mode.  E.g. an INDEX on V2SI\n+\t     should yield two integer values per 128-bit block, meaning\n+\t     that we need to treat it in the same way as V2DI and then\n+\t     ignore the upper 32 bits of each element.  */\n+\t  elt_mode = aarch64_sve_container_int_mode (mode);\n+\t  *info = simd_immediate_info (elt_mode, base, step);\n+\t}\n       return true;\n     }\n   else if (GET_CODE (op) == CONST_VECTOR\n@@ -16976,9 +17116,9 @@ aarch64_simd_vector_alignment (const_tree type)\n      direct way we have of identifying real SVE predicate types.  */\n   if (GET_MODE_CLASS (TYPE_MODE (type)) == MODE_VECTOR_BOOL)\n     return 16;\n-  if (TREE_CODE (TYPE_SIZE (type)) != INTEGER_CST)\n-    return 128;\n-  return wi::umin (wi::to_wide (TYPE_SIZE (type)), 128).to_uhwi ();\n+  widest_int min_size\n+    = constant_lower_bound (wi::to_poly_widest (TYPE_SIZE (type)));\n+  return wi::umin (min_size, 128).to_uhwi ();\n }\n \n /* Implement target hook TARGET_VECTORIZE_PREFERRED_VECTOR_ALIGNMENT.  */\n@@ -19154,7 +19294,7 @@ aarch64_evpc_sel (struct expand_vec_perm_d *d)\n   if (d->testing_p)\n     return true;\n \n-  machine_mode pred_mode = aarch64_sve_pred_mode (unit_size).require ();\n+  machine_mode pred_mode = aarch64_sve_pred_mode (vmode);\n \n   rtx_vector_builder builder (pred_mode, n_patterns, 2);\n   for (int i = 0; i < n_patterns * 2; i++)"}, {"sha": "4c9035f88d6c16aeb0eafe4cbe9d9515989f4a47", "filename": "gcc/config/aarch64/iterators.md", "status": "modified", "additions": 119, "deletions": 57, "changes": 176, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Fconfig%2Faarch64%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Fconfig%2Faarch64%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fiterators.md?ref=cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "patch": "@@ -344,6 +344,21 @@\n \t\t\t\t     VNx4HI VNx2HI\n \t\t\t\t     VNx2SI])\n \n+;; All SVE vector modes.\n+(define_mode_iterator SVE_ALL [VNx16QI VNx8QI VNx4QI VNx2QI\n+\t\t\t       VNx8HI VNx4HI VNx2HI\n+\t\t\t       VNx8HF VNx4HF VNx2HF\n+\t\t\t       VNx4SI VNx2SI\n+\t\t\t       VNx4SF VNx2SF\n+\t\t\t       VNx2DI\n+\t\t\t       VNx2DF])\n+\n+;; All SVE integer vector modes.\n+(define_mode_iterator SVE_I [VNx16QI VNx8QI VNx4QI VNx2QI\n+\t\t\t     VNx8HI VNx4HI VNx2HI\n+\t\t\t     VNx4SI VNx2SI\n+\t\t\t     VNx2DI])\n+\n ;; Modes involved in extending or truncating SVE data, for 8 elements per\n ;; 128-bit block.\n (define_mode_iterator VNx8_NARROW [VNx8QI])\n@@ -776,28 +791,37 @@\n \t\t\t   (HI   \"\")])\n \n ;; Mode-to-individual element type mapping.\n-(define_mode_attr Vetype [(V8QI \"b\") (V16QI \"b\") (VNx16QI \"b\") (VNx16BI \"b\")\n-\t\t\t  (V4HI \"h\") (V8HI  \"h\") (VNx8HI  \"h\") (VNx8BI  \"h\")\n-\t\t\t  (V2SI \"s\") (V4SI  \"s\") (VNx4SI  \"s\") (VNx4BI  \"s\")\n-\t\t\t  (V2DI \"d\")             (VNx2DI  \"d\") (VNx2BI  \"d\")\n-\t\t\t  (V4HF \"h\") (V8HF  \"h\") (VNx8HF  \"h\")\n-\t\t\t  (V2SF \"s\") (V4SF  \"s\") (VNx4SF  \"s\")\n-\t\t\t  (V2DF \"d\")             (VNx2DF  \"d\")\n-\t\t\t  (HF   \"h\")\n-\t\t\t  (SF   \"s\") (DF  \"d\")\n-\t\t\t  (QI \"b\")   (HI \"h\")\n-\t\t\t  (SI \"s\")   (DI \"d\")])\n+(define_mode_attr Vetype [(V8QI \"b\") (V16QI \"b\")\n+\t\t\t  (V4HI \"h\") (V8HI  \"h\")\n+\t\t\t  (V2SI \"s\") (V4SI  \"s\")\n+\t\t\t  (V2DI \"d\")\n+\t\t\t  (V4HF \"h\") (V8HF  \"h\")\n+\t\t\t  (V2SF \"s\") (V4SF  \"s\")\n+\t\t\t  (V2DF \"d\")\n+\t\t\t  (VNx16BI \"b\") (VNx8BI \"h\") (VNx4BI \"s\") (VNx2BI \"d\")\n+\t\t\t  (VNx16QI \"b\") (VNx8QI \"b\") (VNx4QI \"b\") (VNx2QI \"b\")\n+\t\t\t  (VNx8HI \"h\") (VNx4HI \"h\") (VNx2HI \"h\")\n+\t\t\t  (VNx8HF \"h\") (VNx4HF \"h\") (VNx2HF \"h\")\n+\t\t\t  (VNx4SI \"s\") (VNx2SI \"s\")\n+\t\t\t  (VNx4SF \"s\") (VNx2SF \"s\")\n+\t\t\t  (VNx2DI \"d\")\n+\t\t\t  (VNx2DF \"d\")\n+\t\t\t  (HF \"h\")\n+\t\t\t  (SF \"s\") (DF \"d\")\n+\t\t\t  (QI \"b\") (HI \"h\")\n+\t\t\t  (SI \"s\") (DI \"d\")])\n \n ;; Like Vetype, but map to types that are a quarter of the element size.\n (define_mode_attr Vetype_fourth [(VNx4SI \"b\") (VNx2DI \"h\")])\n \n ;; Equivalent of \"size\" for a vector element.\n-(define_mode_attr Vesize [(VNx16QI \"b\") (VNx8QI  \"b\")\n-\t\t\t  (VNx4QI  \"b\") (VNx2QI  \"b\")\n-\t\t\t  (VNx8HI  \"h\") (VNx4HI  \"h\")\n-\t\t\t  (VNx2HI  \"h\") (VNx8HF  \"h\")\n-\t\t\t  (VNx4SI  \"w\") (VNx2SI  \"w\") (VNx4SF  \"w\")\n-\t\t\t  (VNx2DI  \"d\") (VNx2DF  \"d\")\n+(define_mode_attr Vesize [(VNx16QI \"b\") (VNx8QI \"b\") (VNx4QI \"b\") (VNx2QI \"b\")\n+\t\t\t  (VNx8HI \"h\") (VNx4HI \"h\") (VNx2HI \"h\")\n+\t\t\t  (VNx8HF \"h\") (VNx4HF \"h\") (VNx2HF \"h\")\n+\t\t\t  (VNx4SI \"w\") (VNx2SI \"w\")\n+\t\t\t  (VNx4SF \"w\") (VNx2SF \"w\")\n+\t\t\t  (VNx2DI \"d\")\n+\t\t\t  (VNx2DF \"d\")\n \t\t\t  (VNx32QI \"b\") (VNx48QI \"b\") (VNx64QI \"b\")\n \t\t\t  (VNx16HI \"h\") (VNx24HI \"h\") (VNx32HI \"h\")\n \t\t\t  (VNx16HF \"h\") (VNx24HF \"h\") (VNx32HF \"h\")\n@@ -806,6 +830,16 @@\n \t\t\t  (VNx4DI  \"d\") (VNx6DI  \"d\") (VNx8DI  \"d\")\n \t\t\t  (VNx4DF  \"d\") (VNx6DF  \"d\") (VNx8DF  \"d\")])\n \n+;; The Z register suffix for an SVE mode's element container, i.e. the\n+;; Vetype of full SVE modes that have the same number of elements.\n+(define_mode_attr Vctype [(VNx16QI \"b\") (VNx8QI \"h\") (VNx4QI \"s\") (VNx2QI \"d\")\n+\t\t\t  (VNx8HI \"h\") (VNx4HI \"s\") (VNx2HI \"d\")\n+\t\t\t  (VNx8HF \"h\") (VNx4HF \"s\") (VNx2HF \"d\")\n+\t\t\t  (VNx4SI \"s\") (VNx2SI \"d\")\n+\t\t\t  (VNx4SF \"s\") (VNx2SF \"d\")\n+\t\t\t  (VNx2DI \"d\")\n+\t\t\t  (VNx2DF \"d\")])\n+\n ;; Vetype is used everywhere in scheduling type and assembly output,\n ;; sometimes they are not the same, for example HF modes on some\n ;; instructions.  stype is defined to represent scheduling type\n@@ -827,26 +861,40 @@\n \t\t\t  (SI   \"8b\")  (SF    \"8b\")])\n \n ;; Define element mode for each vector mode.\n-(define_mode_attr VEL [(V8QI  \"QI\") (V16QI \"QI\") (VNx16QI \"QI\")\n-\t\t\t(V4HI \"HI\") (V8HI  \"HI\") (VNx8HI  \"HI\")\n-\t\t\t(V2SI \"SI\") (V4SI  \"SI\") (VNx4SI  \"SI\")\n-\t\t\t(DI   \"DI\") (V2DI  \"DI\") (VNx2DI  \"DI\")\n-\t\t\t(V4HF \"HF\") (V8HF  \"HF\") (VNx8HF  \"HF\")\n-\t\t\t(V2SF \"SF\") (V4SF  \"SF\") (VNx4SF  \"SF\")\n-\t\t\t(DF   \"DF\") (V2DF  \"DF\") (VNx2DF  \"DF\")\n-\t\t\t(SI   \"SI\") (HI    \"HI\")\n-\t\t\t(QI   \"QI\")])\n+(define_mode_attr VEL [(V8QI  \"QI\") (V16QI \"QI\")\n+\t\t       (V4HI \"HI\") (V8HI  \"HI\")\n+\t\t       (V2SI \"SI\") (V4SI  \"SI\")\n+\t\t       (DI   \"DI\") (V2DI  \"DI\")\n+\t\t       (V4HF \"HF\") (V8HF  \"HF\")\n+\t\t       (V2SF \"SF\") (V4SF  \"SF\")\n+\t\t       (DF   \"DF\") (V2DF  \"DF\")\n+\t\t       (SI   \"SI\") (HI    \"HI\")\n+\t\t       (QI   \"QI\")\n+\t\t       (VNx16QI \"QI\") (VNx8QI \"QI\") (VNx4QI \"QI\") (VNx2QI \"QI\")\n+\t\t       (VNx8HI \"HI\") (VNx4HI \"HI\") (VNx2HI \"HI\")\n+\t\t       (VNx8HF \"HF\") (VNx4HF \"HF\") (VNx2HF \"HF\")\n+\t\t       (VNx4SI \"SI\") (VNx2SI \"SI\")\n+\t\t       (VNx4SF \"SF\") (VNx2SF \"SF\")\n+\t\t       (VNx2DI \"DI\")\n+\t\t       (VNx2DF \"DF\")])\n \n ;; Define element mode for each vector mode (lower case).\n-(define_mode_attr Vel [(V8QI \"qi\") (V16QI \"qi\") (VNx16QI \"qi\")\n-\t\t\t(V4HI \"hi\") (V8HI \"hi\") (VNx8HI  \"hi\")\n-\t\t\t(V2SI \"si\") (V4SI \"si\") (VNx4SI  \"si\")\n-\t\t\t(DI \"di\")   (V2DI \"di\") (VNx2DI  \"di\")\n-\t\t\t(V4HF \"hf\") (V8HF \"hf\") (VNx8HF  \"hf\")\n-\t\t\t(V2SF \"sf\") (V4SF \"sf\") (VNx4SF  \"sf\")\n-\t\t\t(V2DF \"df\") (DF \"df\")   (VNx2DF  \"df\")\n-\t\t\t(SI   \"si\") (HI   \"hi\")\n-\t\t\t(QI   \"qi\")])\n+(define_mode_attr Vel [(V8QI \"qi\") (V16QI \"qi\")\n+\t\t       (V4HI \"hi\") (V8HI \"hi\")\n+\t\t       (V2SI \"si\") (V4SI \"si\")\n+\t\t       (DI   \"di\") (V2DI \"di\")\n+\t\t       (V4HF \"hf\") (V8HF \"hf\")\n+\t\t       (V2SF \"sf\") (V4SF \"sf\")\n+\t\t       (V2DF \"df\") (DF   \"df\")\n+\t\t       (SI   \"si\") (HI   \"hi\")\n+\t\t       (QI   \"qi\")\n+\t\t       (VNx16QI \"qi\") (VNx8QI \"qi\") (VNx4QI \"qi\") (VNx2QI \"qi\")\n+\t\t       (VNx8HI \"hi\") (VNx4HI \"hi\") (VNx2HI \"hi\")\n+\t\t       (VNx8HF \"hf\") (VNx4HF \"hf\") (VNx2HF \"hf\")\n+\t\t       (VNx4SI \"si\") (VNx2SI \"si\")\n+\t\t       (VNx4SF \"sf\") (VNx2SF \"sf\")\n+\t\t       (VNx2DI \"di\")\n+\t\t       (VNx2DF \"df\")])\n \n ;; Element mode with floating-point values replaced by like-sized integers.\n (define_mode_attr VEL_INT [(VNx16QI \"QI\")\n@@ -994,23 +1042,29 @@\n \t\t\t     (V4SF \"2s\")])\n \n ;; Define corresponding core/FP element mode for each vector mode.\n-(define_mode_attr vw [(V8QI \"w\") (V16QI \"w\") (VNx16QI \"w\")\n-\t\t      (V4HI \"w\") (V8HI \"w\") (VNx8HI \"w\")\n-\t\t      (V2SI \"w\") (V4SI \"w\") (VNx4SI \"w\")\n-\t\t      (DI   \"x\") (V2DI \"x\") (VNx2DI \"x\")\n-\t\t      (VNx8HF \"h\")\n-\t\t      (V2SF \"s\") (V4SF \"s\") (VNx4SF \"s\")\n-\t\t      (V2DF \"d\") (VNx2DF \"d\")])\n+(define_mode_attr vw [(V8QI \"w\") (V16QI \"w\")\n+\t\t      (V4HI \"w\") (V8HI \"w\")\n+\t\t      (V2SI \"w\") (V4SI \"w\")\n+\t\t      (DI   \"x\") (V2DI \"x\")\n+\t\t      (V2SF \"s\") (V4SF \"s\")\n+\t\t      (V2DF \"d\")])\n \n ;; Corresponding core element mode for each vector mode.  This is a\n ;; variation on <vw> mapping FP modes to GP regs.\n-(define_mode_attr vwcore [(V8QI \"w\") (V16QI \"w\") (VNx16QI \"w\")\n-\t\t\t  (V4HI \"w\") (V8HI \"w\") (VNx8HI \"w\")\n-\t\t\t  (V2SI \"w\") (V4SI \"w\") (VNx4SI \"w\")\n-\t\t\t  (DI   \"x\") (V2DI \"x\") (VNx2DI \"x\")\n-\t\t\t  (V4HF \"w\") (V8HF \"w\") (VNx8HF \"w\")\n-\t\t\t  (V2SF \"w\") (V4SF \"w\") (VNx4SF \"w\")\n-\t\t\t  (V2DF \"x\") (VNx2DF \"x\")])\n+(define_mode_attr vwcore [(V8QI \"w\") (V16QI \"w\")\n+\t\t\t  (V4HI \"w\") (V8HI \"w\")\n+\t\t\t  (V2SI \"w\") (V4SI \"w\")\n+\t\t\t  (DI   \"x\") (V2DI \"x\")\n+\t\t\t  (V4HF \"w\") (V8HF \"w\")\n+\t\t\t  (V2SF \"w\") (V4SF \"w\")\n+\t\t\t  (V2DF \"x\")\n+\t\t\t  (VNx16QI \"w\") (VNx8QI \"w\") (VNx4QI \"w\") (VNx2QI \"w\")\n+\t\t\t  (VNx8HI \"w\") (VNx4HI \"w\") (VNx2HI \"w\")\n+\t\t\t  (VNx8HF \"w\") (VNx4HF \"w\") (VNx2HF \"w\")\n+\t\t\t  (VNx4SI \"w\") (VNx2SI \"w\")\n+\t\t\t  (VNx4SF \"w\") (VNx2SF \"w\")\n+\t\t\t  (VNx2DI \"x\")\n+\t\t\t  (VNx2DF \"x\")])\n \n ;; Double vector types for ALLX.\n (define_mode_attr Vallxd [(QI \"8b\") (HI \"4h\") (SI \"2s\")])\n@@ -1248,10 +1302,14 @@\n \n ;; The predicate mode associated with an SVE data mode.  For structure modes\n ;; this is equivalent to the <VPRED> of the subvector mode.\n-(define_mode_attr VPRED [(VNx16QI \"VNx16BI\")\n-\t\t\t (VNx8HI \"VNx8BI\") (VNx8HF \"VNx8BI\")\n-\t\t\t (VNx4SI \"VNx4BI\") (VNx4SF \"VNx4BI\")\n-\t\t\t (VNx2DI \"VNx2BI\") (VNx2DF \"VNx2BI\")\n+(define_mode_attr VPRED [(VNx16QI \"VNx16BI\") (VNx8QI \"VNx8BI\")\n+\t\t\t (VNx4QI \"VNx4BI\") (VNx2QI \"VNx2BI\")\n+\t\t\t (VNx8HI \"VNx8BI\") (VNx4HI \"VNx4BI\") (VNx2HI \"VNx2BI\")\n+\t\t\t (VNx8HF \"VNx8BI\") (VNx4HF \"VNx4BI\") (VNx2HF \"VNx2BI\")\n+\t\t\t (VNx4SI \"VNx4BI\") (VNx2SI \"VNx2BI\")\n+\t\t\t (VNx4SF \"VNx4BI\") (VNx2SF \"VNx2BI\")\n+\t\t\t (VNx2DI \"VNx2BI\")\n+\t\t\t (VNx2DF \"VNx2BI\")\n \t\t\t (VNx32QI \"VNx16BI\")\n \t\t\t (VNx16HI \"VNx8BI\") (VNx16HF \"VNx8BI\")\n \t\t\t (VNx8SI \"VNx4BI\") (VNx8SF \"VNx4BI\")\n@@ -1266,10 +1324,14 @@\n \t\t\t (VNx8DI \"VNx2BI\") (VNx8DF \"VNx2BI\")])\n \n ;; ...and again in lower case.\n-(define_mode_attr vpred [(VNx16QI \"vnx16bi\")\n-\t\t\t (VNx8HI \"vnx8bi\") (VNx8HF \"vnx8bi\")\n-\t\t\t (VNx4SI \"vnx4bi\") (VNx4SF \"vnx4bi\")\n-\t\t\t (VNx2DI \"vnx2bi\") (VNx2DF \"vnx2bi\")\n+(define_mode_attr vpred [(VNx16QI \"vnx16bi\") (VNx8QI \"vnx8bi\")\n+\t\t\t (VNx4QI \"vnx4bi\") (VNx2QI \"vnx2bi\")\n+\t\t\t (VNx8HI \"vnx8bi\") (VNx4HI \"vnx4bi\") (VNx2HI \"vnx2bi\")\n+\t\t\t (VNx8HF \"vnx8bi\") (VNx4HF \"vnx4bi\") (VNx2HF \"vnx2bi\")\n+\t\t\t (VNx4SI \"vnx4bi\") (VNx2SI \"vnx2bi\")\n+\t\t\t (VNx4SF \"vnx4bi\") (VNx2SF \"vnx2bi\")\n+\t\t\t (VNx2DI \"vnx2bi\")\n+\t\t\t (VNx2DF \"vnx2bi\")\n \t\t\t (VNx32QI \"vnx16bi\")\n \t\t\t (VNx16HI \"vnx8bi\") (VNx16HF \"vnx8bi\")\n \t\t\t (VNx8SI \"vnx4bi\") (VNx8SF \"vnx4bi\")"}, {"sha": "57505e9e77c5092d706b8bfdc252ff41f296248b", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "patch": "@@ -1,3 +1,11 @@\n+2019-11-16  Richard Sandiford  <richard.sandiford@arm.com>\n+\n+\t* gcc.target/aarch64/sve/mixed_size_1.c: New test.\n+\t* gcc.target/aarch64/sve/mixed_size_2.c: Likewise.\n+\t* gcc.target/aarch64/sve/mixed_size_3.c: Likewise.\n+\t* gcc.target/aarch64/sve/mixed_size_4.c: Likewise.\n+\t* gcc.target/aarch64/sve/mixed_size_5.c: Likewise.\n+\n 2019-11-16  Richard Sandiford  <richard.sandiford@arm.com>\n \n \t* gcc.target/aarch64/sve/clastb_8.c: Use assembly tests to"}, {"sha": "a5659b65bd117737cb7a0a5bd10c21a48a417099", "filename": "gcc/testsuite/gcc.target/aarch64/sve/mixed_size_1.c", "status": "added", "additions": 39, "deletions": 0, "changes": 39, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_1.c?ref=cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "patch": "@@ -0,0 +1,39 @@\n+/* { dg-options \"-O2 -ftree-vectorize -fno-tree-loop-distribute-patterns\" } */\n+\n+#include <stdint.h>\n+\n+#define TEST_LOOP(TYPE1, TYPE2)\t\t\t\t\t\t\\\n+  void\t\t\t\t\t\t\t\t\t\\\n+  f_##TYPE1##_##TYPE2 (TYPE1 *restrict dst1, TYPE1 *restrict src1,\t\\\n+\t\t       TYPE2 *restrict dst2, TYPE2 *restrict src2,\t\\\n+\t\t       int n)\t\t\t\t\t\t\\\n+  {\t\t\t\t\t\t\t\t\t\\\n+    for (int i = 0; i < n; ++i)\t\t\t\t\t\t\\\n+      {\t\t\t\t\t\t\t\t\t\\\n+\tdst1[i] += src1[i];\t\t\t\t\t\t\\\n+\tdst2[i] = src2[i];\t\t\t\t\t\t\\\n+      }\t\t\t\t\t\t\t\t\t\\\n+  }\n+\n+#define TEST_ALL(T) \\\n+  T (uint16_t, uint8_t) \\\n+  T (uint32_t, uint16_t) \\\n+  T (uint32_t, _Float16) \\\n+  T (uint64_t, uint32_t) \\\n+  T (uint64_t, float)\n+\n+TEST_ALL (TEST_LOOP)\n+\n+/* { dg-final { scan-assembler-times {\\tld1b\\tz[0-9]+\\.h,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tst1b\\tz[0-9]+\\.h,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tld1h\\tz[0-9]+\\.s,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tst1h\\tz[0-9]+\\.s,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tld1w\\tz[0-9]+\\.d,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tst1w\\tz[0-9]+\\.d,} 2 } } */\n+\n+/* { dg-final { scan-assembler-times {\\tld1h\\tz[0-9]+\\.h,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tst1h\\tz[0-9]+\\.h,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tld1w\\tz[0-9]+\\.s,} 4 } } */\n+/* { dg-final { scan-assembler-times {\\tst1w\\tz[0-9]+\\.s,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tld1d\\tz[0-9]+\\.d,} 4 } } */\n+/* { dg-final { scan-assembler-times {\\tst1d\\tz[0-9]+\\.d,} 2 } } */"}, {"sha": "34b58e350a3748dab8669537e5a960f0e36fb564", "filename": "gcc/testsuite/gcc.target/aarch64/sve/mixed_size_2.c", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_2.c?ref=cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "patch": "@@ -0,0 +1,41 @@\n+/* { dg-options \"-O2 -ftree-vectorize -fno-tree-loop-distribute-patterns\" } */\n+\n+#include <stdint.h>\n+\n+#define TEST_LOOP(TYPE1, TYPE2)\t\t\t\t\t\t\\\n+  void\t\t\t\t\t\t\t\t\t\\\n+  f_##TYPE1##_##TYPE2 (TYPE1 *restrict dst1, TYPE1 *restrict src1,\t\\\n+\t\t       TYPE2 *restrict dst2, int n)\t\t\t\\\n+  {\t\t\t\t\t\t\t\t\t\\\n+    for (int i = 0; i < n; ++i)\t\t\t\t\t\t\\\n+      {\t\t\t\t\t\t\t\t\t\\\n+\tdst1[i] += src1[i];\t\t\t\t\t\t\\\n+\tdst2[i] = 1;\t\t\t\t\t\t\t\\\n+      }\t\t\t\t\t\t\t\t\t\\\n+  }\n+\n+#define TEST_ALL(T) \\\n+  T (uint16_t, uint8_t) \\\n+  T (uint32_t, uint16_t) \\\n+  T (uint32_t, _Float16) \\\n+  T (uint64_t, uint32_t) \\\n+  T (uint64_t, float)\n+\n+TEST_ALL (TEST_LOOP)\n+\n+/* { dg-final { scan-assembler-times {\\tmov\\tz[0-9]+\\.b, #1\\n} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tmov\\tz[0-9]+\\.h, #1\\n} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tmov\\tz[0-9]+\\.s, #1\\n} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tfmov\\tz[0-9]+\\.h, #1\\.0} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tfmov\\tz[0-9]+\\.s, #1\\.0} 1 } } */\n+\n+/* { dg-final { scan-assembler-times {\\tst1b\\tz[0-9]+\\.h,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tst1h\\tz[0-9]+\\.s,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tst1w\\tz[0-9]+\\.d,} 2 } } */\n+\n+/* { dg-final { scan-assembler-times {\\tld1h\\tz[0-9]+\\.h,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tst1h\\tz[0-9]+\\.h,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tld1w\\tz[0-9]+\\.s,} 4 } } */\n+/* { dg-final { scan-assembler-times {\\tst1w\\tz[0-9]+\\.s,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tld1d\\tz[0-9]+\\.d,} 4 } } */\n+/* { dg-final { scan-assembler-times {\\tst1d\\tz[0-9]+\\.d,} 2 } } */"}, {"sha": "9ae3e7b2c796acfd89c29a3a918fdd8ae8f676e1", "filename": "gcc/testsuite/gcc.target/aarch64/sve/mixed_size_3.c", "status": "added", "additions": 41, "deletions": 0, "changes": 41, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_3.c?ref=cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "patch": "@@ -0,0 +1,41 @@\n+/* { dg-options \"-O2 -ftree-vectorize -fno-tree-loop-distribute-patterns\" } */\n+\n+#include <stdint.h>\n+\n+#define TEST_LOOP(TYPE1, TYPE2)\t\t\t\t\t\t\\\n+  void\t\t\t\t\t\t\t\t\t\\\n+  f_##TYPE1##_##TYPE2 (TYPE1 *restrict dst1, TYPE1 *restrict src1,\t\\\n+\t\t       TYPE2 *restrict dst2, TYPE2 src2, int n)\t\t\\\n+  {\t\t\t\t\t\t\t\t\t\\\n+    for (int i = 0; i < n; ++i)\t\t\t\t\t\t\\\n+      {\t\t\t\t\t\t\t\t\t\\\n+\tdst1[i] += src1[i];\t\t\t\t\t\t\\\n+\tdst2[i] = src2;\t\t\t\t\t\t\t\\\n+      }\t\t\t\t\t\t\t\t\t\\\n+  }\n+\n+#define TEST_ALL(T) \\\n+  T (uint16_t, uint8_t) \\\n+  T (uint32_t, uint16_t) \\\n+  T (uint32_t, _Float16) \\\n+  T (uint64_t, uint32_t) \\\n+  T (uint64_t, float)\n+\n+TEST_ALL (TEST_LOOP)\n+\n+/* { dg-final { scan-assembler-times {\\tmov\\tz[0-9]+\\.b, w3\\n} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tmov\\tz[0-9]+\\.h, w3\\n} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tmov\\tz[0-9]+\\.s, w3\\n} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tmov\\tz[0-9]+\\.h, h0\\n} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tmov\\tz[0-9]+\\.s, s0\\n} 1 } } */\n+\n+/* { dg-final { scan-assembler-times {\\tst1b\\tz[0-9]+\\.h,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tst1h\\tz[0-9]+\\.s,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tst1w\\tz[0-9]+\\.d,} 2 } } */\n+\n+/* { dg-final { scan-assembler-times {\\tld1h\\tz[0-9]+\\.h,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tst1h\\tz[0-9]+\\.h,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tld1w\\tz[0-9]+\\.s,} 4 } } */\n+/* { dg-final { scan-assembler-times {\\tst1w\\tz[0-9]+\\.s,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tld1d\\tz[0-9]+\\.d,} 4 } } */\n+/* { dg-final { scan-assembler-times {\\tst1d\\tz[0-9]+\\.d,} 2 } } */"}, {"sha": "4c475fb9c68f6b66ead3ef2ae999af6efdc1417e", "filename": "gcc/testsuite/gcc.target/aarch64/sve/mixed_size_4.c", "status": "added", "additions": 43, "deletions": 0, "changes": 43, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_4.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_4.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_4.c?ref=cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "patch": "@@ -0,0 +1,43 @@\n+/* { dg-options \"-O2 -ftree-vectorize -fno-tree-loop-distribute-patterns\" } */\n+\n+#include <stdint.h>\n+\n+#define TEST_LOOP(TYPE1, TYPE2)\t\t\t\t\t\t\\\n+  void\t\t\t\t\t\t\t\t\t\\\n+  f_##TYPE1##_##TYPE2 (TYPE1 *restrict dst1, TYPE1 *restrict src1,\t\\\n+\t\t       TYPE2 *restrict dst2, TYPE2 n)\t\t\t\\\n+  {\t\t\t\t\t\t\t\t\t\\\n+    for (TYPE2 i = 0; i < n; ++i)\t\t\t\t\t\\\n+      {\t\t\t\t\t\t\t\t\t\\\n+\tdst1[i] += src1[i];\t\t\t\t\t\t\\\n+\tdst2[i] = i;\t\t\t\t\t\t\t\\\n+      }\t\t\t\t\t\t\t\t\t\\\n+  }\n+\n+#define TEST_ALL(T) \\\n+  T (uint16_t, uint8_t) \\\n+  T (uint32_t, uint16_t) \\\n+  T (uint64_t, uint32_t)\n+\n+TEST_ALL (TEST_LOOP)\n+\n+/* { dg-final { scan-assembler-not {\\tindex\\tz[0-9]+\\.b,} } } */\n+/* { dg-final { scan-assembler-times {\\tindex\\tz[0-9]+\\.h, #0, #1\\n} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tindex\\tz[0-9]+\\.s, #0, #1\\n} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tindex\\tz[0-9]+\\.d, #0, #1\\n} 1 } } */\n+\n+/* { dg-final { scan-assembler-not {\\tcntb\\t} } } */\n+/* { dg-final { scan-assembler-times {\\tcnth\\t} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tcntw\\t} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tcntd\\t} 1 } } */\n+\n+/* { dg-final { scan-assembler-times {\\tst1b\\tz[0-9]+\\.h,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tst1h\\tz[0-9]+\\.s,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tst1w\\tz[0-9]+\\.d,} 1 } } */\n+\n+/* { dg-final { scan-assembler-times {\\tld1h\\tz[0-9]+\\.h,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tst1h\\tz[0-9]+\\.h,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tld1w\\tz[0-9]+\\.s,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tst1w\\tz[0-9]+\\.s,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tld1d\\tz[0-9]+\\.d,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tst1d\\tz[0-9]+\\.d,} 1 } } */"}, {"sha": "83be00f47e4ff11647bc738e8e60f1e74e258b93", "filename": "gcc/testsuite/gcc.target/aarch64/sve/mixed_size_5.c", "status": "added", "additions": 42, "deletions": 0, "changes": 42, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_5.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_5.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsve%2Fmixed_size_5.c?ref=cc68f7c2dabbf9d90ebe025a11f6d6ed5d49a113", "patch": "@@ -0,0 +1,42 @@\n+/* { dg-options \"-O2 -ftree-vectorize -fno-tree-loop-distribute-patterns -msve-vector-bits=512\" } */\n+\n+#include <stdint.h>\n+\n+#define TEST_LOOP(TYPE1, TYPE2)\t\t\t\t\t\t\\\n+  void\t\t\t\t\t\t\t\t\t\\\n+  f_##TYPE1##_##TYPE2 (TYPE1 *restrict dst1, TYPE1 *restrict src1,\t\\\n+\t\t       TYPE2 *restrict dst2, TYPE2 *restrict src2,\t\\\n+\t\t       int n)\t\t\t\t\t\t\\\n+  {\t\t\t\t\t\t\t\t\t\\\n+    for (int i = 0; i < n; ++i)\t\t\t\t\t\t\\\n+      {\t\t\t\t\t\t\t\t\t\\\n+\tdst1[i * 2] = src1[i * 2] + 1;\t\t\t\t\t\\\n+\tdst1[i * 2 + 1] = src1[i * 2 + 1] + 1;\t\t\t\t\\\n+\tdst2[i * 2] = 2;\t\t\t\t\t\t\\\n+\tdst2[i * 2 + 1] = 3;\t\t\t\t\t\t\\\n+      }\t\t\t\t\t\t\t\t\t\\\n+  }\n+\n+#define TEST_ALL(T) \\\n+  T (uint16_t, uint8_t) \\\n+  T (uint32_t, uint16_t) \\\n+  T (uint32_t, _Float16) \\\n+  T (uint64_t, uint32_t) \\\n+  T (uint64_t, float)\n+\n+TEST_ALL (TEST_LOOP)\n+\n+/* { dg-final { scan-assembler-times {\\tld1rw\\tz[0-9]+\\.s,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tld1rd\\tz[0-9]+\\.d,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tld1rqw\\tz[0-9]+\\.s,} 2 } } */\n+\n+/* { dg-final { scan-assembler-times {\\tst1b\\tz[0-9]+\\.h,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tst1h\\tz[0-9]+\\.s,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tst1w\\tz[0-9]+\\.d,} 2 } } */\n+\n+/* { dg-final { scan-assembler-times {\\tld1h\\tz[0-9]+\\.h,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tst1h\\tz[0-9]+\\.h,} 1 } } */\n+/* { dg-final { scan-assembler-times {\\tld1w\\tz[0-9]+\\.s,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tst1w\\tz[0-9]+\\.s,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tld1d\\tz[0-9]+\\.d,} 2 } } */\n+/* { dg-final { scan-assembler-times {\\tst1d\\tz[0-9]+\\.d,} 2 } } */"}]}