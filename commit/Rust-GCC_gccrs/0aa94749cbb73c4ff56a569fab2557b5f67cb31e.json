{"sha": "0aa94749cbb73c4ff56a569fab2557b5f67cb31e", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MGFhOTQ3NDljYmI3M2M0ZmY1NmE1NjlmYWIyNTU3YjVmNjdjYjMxZQ==", "commit": {"author": {"name": "Iain Buclaw", "email": "ibuclaw@gdcproject.org", "date": "2019-04-20T17:14:50Z"}, "committer": {"name": "Iain Buclaw", "email": "ibuclaw@gcc.gnu.org", "date": "2019-04-20T17:14:50Z"}, "message": "libphobos: core.atomic should have fallback when there's no libatomic.\n\nlibphobos/ChangeLog:\n\n2019-04-20  Iain Buclaw  <ibuclaw@gdcproject.org>\n\n\tPR d/89293\n\t* libdruntime/core/atomic.d (casImpl): Remove static assert for\n\tGNU_Have_Atomics, add static path to handle missing atomic support.\n\t(atomicLoad): Likewise.\n\t(atomicStore): Likewise.\n\t(atomicFence):  Likewise.\n\t(atomicMutexHandle, AtomicMutex): Declare types.\n\t(_getAtomicMutex): New function.\n\t(getAtomicMutex): Declare.\n\nFrom-SVN: r270470", "tree": {"sha": "fedb67de507eeb9933d91d1bb0e2a401dba4c752", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/fedb67de507eeb9933d91d1bb0e2a401dba4c752"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/0aa94749cbb73c4ff56a569fab2557b5f67cb31e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0aa94749cbb73c4ff56a569fab2557b5f67cb31e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0aa94749cbb73c4ff56a569fab2557b5f67cb31e", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0aa94749cbb73c4ff56a569fab2557b5f67cb31e/comments", "author": {"login": "ibuclaw", "id": 397929, "node_id": "MDQ6VXNlcjM5NzkyOQ==", "avatar_url": "https://avatars.githubusercontent.com/u/397929?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ibuclaw", "html_url": "https://github.com/ibuclaw", "followers_url": "https://api.github.com/users/ibuclaw/followers", "following_url": "https://api.github.com/users/ibuclaw/following{/other_user}", "gists_url": "https://api.github.com/users/ibuclaw/gists{/gist_id}", "starred_url": "https://api.github.com/users/ibuclaw/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ibuclaw/subscriptions", "organizations_url": "https://api.github.com/users/ibuclaw/orgs", "repos_url": "https://api.github.com/users/ibuclaw/repos", "events_url": "https://api.github.com/users/ibuclaw/events{/privacy}", "received_events_url": "https://api.github.com/users/ibuclaw/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "1474a2e567653b9a1856b4cb28a64a8eeb3c7c9d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1474a2e567653b9a1856b4cb28a64a8eeb3c7c9d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/1474a2e567653b9a1856b4cb28a64a8eeb3c7c9d"}], "stats": {"total": 280, "additions": 214, "deletions": 66}, "files": [{"sha": "4f158776e233a37600ee96065a77c7a93417b9b8", "filename": "libphobos/ChangeLog", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0aa94749cbb73c4ff56a569fab2557b5f67cb31e/libphobos%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0aa94749cbb73c4ff56a569fab2557b5f67cb31e/libphobos%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libphobos%2FChangeLog?ref=0aa94749cbb73c4ff56a569fab2557b5f67cb31e", "patch": "@@ -1,3 +1,15 @@\n+2019-04-20  Iain Buclaw  <ibuclaw@gdcproject.org>\n+\n+\tPR d/89293\n+\t* libdruntime/core/atomic.d (casImpl): Remove static assert for\n+\tGNU_Have_Atomics, add static path to handle missing atomic support.\n+\t(atomicLoad): Likewise.\n+\t(atomicStore): Likewise.\n+\t(atomicFence):  Likewise.\n+\t(atomicMutexHandle, AtomicMutex): Declare types.\n+\t(_getAtomicMutex): New function.\n+\t(getAtomicMutex): Declare.\n+\n 2019-04-16  Iain Buclaw  <ibuclaw@gdcproject.org>\n \n \t* config.h.in: Regenerate."}, {"sha": "1d0a2ea8b489bafb289d07914215e88a29be8f9e", "filename": "libphobos/libdruntime/core/atomic.d", "status": "modified", "additions": 202, "deletions": 66, "changes": 268, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0aa94749cbb73c4ff56a569fab2557b5f67cb31e/libphobos%2Flibdruntime%2Fcore%2Fatomic.d", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0aa94749cbb73c4ff56a569fab2557b5f67cb31e/libphobos%2Flibdruntime%2Fcore%2Fatomic.d", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libphobos%2Flibdruntime%2Fcore%2Fatomic.d?ref=0aa94749cbb73c4ff56a569fab2557b5f67cb31e", "patch": "@@ -1353,36 +1353,62 @@ else version (GNU)\n \n     private bool casImpl(T,V1,V2)( shared(T)* here, V1 ifThis, V2 writeThis ) pure nothrow @nogc @trusted\n     {\n-        static assert(GNU_Have_Atomics, \"cas() not supported on this architecture\");\n         bool res = void;\n \n-        static if (T.sizeof == byte.sizeof)\n+        static if (GNU_Have_Atomics || GNU_Have_LibAtomic)\n         {\n-            res = __atomic_compare_exchange_1(here, cast(void*) &ifThis, *cast(ubyte*) &writeThis,\n-                                              false, MemoryOrder.seq, MemoryOrder.seq);\n-        }\n-        else static if (T.sizeof == short.sizeof)\n-        {\n-            res = __atomic_compare_exchange_2(here, cast(void*) &ifThis, *cast(ushort*) &writeThis,\n-                                              false, MemoryOrder.seq, MemoryOrder.seq);\n-        }\n-        else static if (T.sizeof == int.sizeof)\n-        {\n-            res = __atomic_compare_exchange_4(here, cast(void*) &ifThis, *cast(uint*) &writeThis,\n-                                              false, MemoryOrder.seq, MemoryOrder.seq);\n-        }\n-        else static if (T.sizeof == long.sizeof && GNU_Have_64Bit_Atomics)\n-        {\n-            res = __atomic_compare_exchange_8(here, cast(void*) &ifThis, *cast(ulong*) &writeThis,\n-                                              false, MemoryOrder.seq, MemoryOrder.seq);\n+            static if (T.sizeof == byte.sizeof)\n+            {\n+                res = __atomic_compare_exchange_1(here, cast(void*) &ifThis, *cast(ubyte*) &writeThis,\n+                                                  false, MemoryOrder.seq, MemoryOrder.seq);\n+            }\n+            else static if (T.sizeof == short.sizeof)\n+            {\n+                res = __atomic_compare_exchange_2(here, cast(void*) &ifThis, *cast(ushort*) &writeThis,\n+                                                  false, MemoryOrder.seq, MemoryOrder.seq);\n+            }\n+            else static if (T.sizeof == int.sizeof)\n+            {\n+                res = __atomic_compare_exchange_4(here, cast(void*) &ifThis, *cast(uint*) &writeThis,\n+                                                  false, MemoryOrder.seq, MemoryOrder.seq);\n+            }\n+            else static if (T.sizeof == long.sizeof && GNU_Have_64Bit_Atomics)\n+            {\n+                res = __atomic_compare_exchange_8(here, cast(void*) &ifThis, *cast(ulong*) &writeThis,\n+                                                  false, MemoryOrder.seq, MemoryOrder.seq);\n+            }\n+            else static if (GNU_Have_LibAtomic)\n+            {\n+                res = __atomic_compare_exchange(T.sizeof, here, cast(void*) &ifThis, cast(void*) &writeThis,\n+                                                MemoryOrder.seq, MemoryOrder.seq);\n+            }\n+            else\n+                static assert(0, \"Invalid template type specified.\");\n         }\n-        else static if (GNU_Have_LibAtomic)\n+        else\n         {\n-            res = __atomic_compare_exchange(T.sizeof, here, cast(void*) &ifThis, cast(void*) &writeThis,\n-                                            MemoryOrder.seq, MemoryOrder.seq);\n+            static if (T.sizeof == byte.sizeof)\n+                alias U = byte;\n+            else static if (T.sizeof == short.sizeof)\n+                alias U = short;\n+            else static if (T.sizeof == int.sizeof)\n+                alias U = int;\n+            else static if (T.sizeof == long.sizeof)\n+                alias U = long;\n+            else\n+                static assert(0, \"Invalid template type specified.\");\n+\n+            getAtomicMutex.lock();\n+            scope(exit) getAtomicMutex.unlock();\n+\n+            if (*cast(U*)here == *cast(U*)&ifThis)\n+            {\n+                *here = writeThis;\n+                res = true;\n+            }\n+            else\n+                res = false;\n         }\n-        else\n-            static assert(0, \"Invalid template type specified.\");\n \n         return res;\n     }\n@@ -1406,36 +1432,44 @@ else version (GNU)\n     {\n         static assert(ms != MemoryOrder.rel, \"Invalid MemoryOrder for atomicLoad\");\n         static assert(__traits(isPOD, T), \"argument to atomicLoad() must be POD\");\n-        static assert(GNU_Have_Atomics, \"atomicLoad() not supported on this architecture\");\n \n-        static if (T.sizeof == ubyte.sizeof)\n+        static if (GNU_Have_Atomics || GNU_Have_LibAtomic)\n         {\n-            ubyte value = __atomic_load_1(&val, ms);\n-            return *cast(HeadUnshared!T*) &value;\n-        }\n-        else static if (T.sizeof == ushort.sizeof)\n-        {\n-            ushort value = __atomic_load_2(&val, ms);\n-            return *cast(HeadUnshared!T*) &value;\n-        }\n-        else static if (T.sizeof == uint.sizeof)\n-        {\n-            uint value = __atomic_load_4(&val, ms);\n-            return *cast(HeadUnshared!T*) &value;\n-        }\n-        else static if (T.sizeof == ulong.sizeof && GNU_Have_64Bit_Atomics)\n-        {\n-            ulong value = __atomic_load_8(&val, ms);\n-            return *cast(HeadUnshared!T*) &value;\n+            static if (T.sizeof == ubyte.sizeof)\n+            {\n+                ubyte value = __atomic_load_1(&val, ms);\n+                return *cast(HeadUnshared!T*) &value;\n+            }\n+            else static if (T.sizeof == ushort.sizeof)\n+            {\n+                ushort value = __atomic_load_2(&val, ms);\n+                return *cast(HeadUnshared!T*) &value;\n+            }\n+            else static if (T.sizeof == uint.sizeof)\n+            {\n+                uint value = __atomic_load_4(&val, ms);\n+                return *cast(HeadUnshared!T*) &value;\n+            }\n+            else static if (T.sizeof == ulong.sizeof && GNU_Have_64Bit_Atomics)\n+            {\n+                ulong value = __atomic_load_8(&val, ms);\n+                return *cast(HeadUnshared!T*) &value;\n+            }\n+            else static if (GNU_Have_LibAtomic)\n+            {\n+                T value;\n+                __atomic_load(T.sizeof, &val, cast(void*)&value, ms);\n+                return *cast(HeadUnshared!T*) &value;\n+            }\n+            else\n+                static assert(0, \"Invalid template type specified.\");\n         }\n-        else static if (GNU_Have_LibAtomic)\n+        else\n         {\n-            T value;\n-            __atomic_load(T.sizeof, &val, cast(void*)&value, ms);\n-            return *cast(HeadUnshared!T*) &value;\n+            getAtomicMutex.lock();\n+            scope(exit) getAtomicMutex.unlock();\n+            return *cast(HeadUnshared!T*)&val;\n         }\n-        else\n-            static assert(0, \"Invalid template type specified.\");\n     }\n \n \n@@ -1444,36 +1478,138 @@ else version (GNU)\n     {\n         static assert(ms != MemoryOrder.acq, \"Invalid MemoryOrder for atomicStore\");\n         static assert(__traits(isPOD, T), \"argument to atomicLoad() must be POD\");\n-        static assert(GNU_Have_Atomics, \"atomicStore() not supported on this architecture\");\n \n-        static if (T.sizeof == ubyte.sizeof)\n+        static if (GNU_Have_Atomics || GNU_Have_LibAtomic)\n         {\n-            __atomic_store_1(&val, *cast(ubyte*) &newval, ms);\n+            static if (T.sizeof == ubyte.sizeof)\n+            {\n+                __atomic_store_1(&val, *cast(ubyte*) &newval, ms);\n+            }\n+            else static if (T.sizeof == ushort.sizeof)\n+            {\n+                __atomic_store_2(&val, *cast(ushort*) &newval, ms);\n+            }\n+            else static if (T.sizeof == uint.sizeof)\n+            {\n+                __atomic_store_4(&val, *cast(uint*) &newval, ms);\n+            }\n+            else static if (T.sizeof == ulong.sizeof && GNU_Have_64Bit_Atomics)\n+            {\n+                __atomic_store_8(&val, *cast(ulong*) &newval, ms);\n+            }\n+            else static if (GNU_Have_LibAtomic)\n+            {\n+                __atomic_store(T.sizeof, &val, cast(void*)&newval, ms);\n+            }\n+            else\n+                static assert(0, \"Invalid template type specified.\");\n         }\n-        else static if (T.sizeof == ushort.sizeof)\n+        else\n         {\n-            __atomic_store_2(&val, *cast(ushort*) &newval, ms);\n+            getAtomicMutex.lock();\n+            val = newval;\n+            getAtomicMutex.unlock();\n         }\n-        else static if (T.sizeof == uint.sizeof)\n+    }\n+\n+\n+    void atomicFence() nothrow @nogc\n+    {\n+        static if (GNU_Have_Atomics || GNU_Have_LibAtomic)\n+            __atomic_thread_fence(MemoryOrder.seq);\n+        else\n         {\n-            __atomic_store_4(&val, *cast(uint*) &newval, ms);\n+            getAtomicMutex.lock();\n+            getAtomicMutex.unlock();\n         }\n-        else static if (T.sizeof == ulong.sizeof && GNU_Have_64Bit_Atomics)\n+    }\n+\n+    static if (!GNU_Have_Atomics && !GNU_Have_LibAtomic)\n+    {\n+        // Use system mutex for atomics, faking the purity of the functions so\n+        // that they can be used in pure/nothrow/@safe code.\n+        extern (C) private pure @trusted @nogc nothrow\n         {\n-            __atomic_store_8(&val, *cast(ulong*) &newval, ms);\n+            static if (GNU_Thread_Model == ThreadModel.Posix)\n+            {\n+                import core.sys.posix.pthread;\n+                alias atomicMutexHandle = pthread_mutex_t;\n+\n+                pragma(mangle, \"pthread_mutex_init\") int fakePureMutexInit(pthread_mutex_t*, pthread_mutexattr_t*);\n+                pragma(mangle, \"pthread_mutex_lock\") int fakePureMutexLock(pthread_mutex_t*);\n+                pragma(mangle, \"pthread_mutex_unlock\") int fakePureMutexUnlock(pthread_mutex_t*);\n+            }\n+            else static if (GNU_Thread_Model == ThreadModel.Win32)\n+            {\n+                import core.sys.windows.winbase;\n+                alias atomicMutexHandle = CRITICAL_SECTION;\n+\n+                pragma(mangle, \"InitializeCriticalSection\") int fakePureMutexInit(CRITICAL_SECTION*);\n+                pragma(mangle, \"EnterCriticalSection\") void fakePureMutexLock(CRITICAL_SECTION*);\n+                pragma(mangle, \"LeaveCriticalSection\") int fakePureMutexUnlock(CRITICAL_SECTION*);\n+            }\n+            else\n+            {\n+                alias atomicMutexHandle = int;\n+            }\n         }\n-        else static if (GNU_Have_LibAtomic)\n+\n+        // Implements lock/unlock operations.\n+        private struct AtomicMutex\n         {\n-            __atomic_store(T.sizeof, &val, cast(void*)&newval, ms);\n+            int lock() pure @trusted @nogc nothrow\n+            {\n+                static if (GNU_Thread_Model == ThreadModel.Posix)\n+                {\n+                    if (!_inited)\n+                    {\n+                        fakePureMutexInit(&_handle, null);\n+                        _inited = true;\n+                    }\n+                    return fakePureMutexLock(&_handle);\n+                }\n+                else\n+                {\n+                    static if (GNU_Thread_Model == ThreadModel.Win32)\n+                    {\n+                        if (!_inited)\n+                        {\n+                            fakePureMutexInit(&_handle);\n+                            _inited = true;\n+                        }\n+                        fakePureMutexLock(&_handle);\n+                    }\n+                    return 0;\n+                }\n+            }\n+\n+            int unlock() pure @trusted @nogc nothrow\n+            {\n+                static if (GNU_Thread_Model == ThreadModel.Posix)\n+                    return fakePureMutexUnlock(&_handle);\n+                else\n+                {\n+                    static if (GNU_Thread_Model == ThreadModel.Win32)\n+                        fakePureMutexUnlock(&_handle);\n+                    return 0;\n+                }\n+            }\n+\n+        private:\n+            atomicMutexHandle _handle;\n+            bool _inited;\n         }\n-        else\n-            static assert(0, \"Invalid template type specified.\");\n-    }\n \n+        // Internal static mutex reference.\n+        private AtomicMutex* _getAtomicMutex() @trusted @nogc nothrow\n+        {\n+            __gshared static AtomicMutex mutex;\n+            return &mutex;\n+        }\n \n-    void atomicFence() nothrow @nogc\n-    {\n-        __atomic_thread_fence(MemoryOrder.seq);\n+        // Pure alias for _getAtomicMutex.\n+        pragma(mangle, _getAtomicMutex.mangleof)\n+        private AtomicMutex* getAtomicMutex() pure @trusted @nogc nothrow @property;\n     }\n }\n "}]}