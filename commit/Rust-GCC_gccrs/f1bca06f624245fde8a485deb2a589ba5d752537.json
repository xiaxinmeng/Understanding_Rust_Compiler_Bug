{"sha": "f1bca06f624245fde8a485deb2a589ba5d752537", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZjFiY2EwNmY2MjQyNDVmZGU4YTQ4NWRlYjJhNTg5YmE1ZDc1MjUzNw==", "commit": {"author": {"name": "Georg-Johann Lay", "email": "gjl@gcc.gnu.org", "date": "2016-12-02T15:08:27Z"}, "committer": {"name": "Georg-Johann Lay", "email": "gjl@gcc.gnu.org", "date": "2016-12-02T15:08:27Z"}, "message": "avr.c: Fix coding rule glitches.\n\n\t* config/avr/avr.c: Fix coding rule glitches.\n\nFrom-SVN: r243186", "tree": {"sha": "9e3c37a4b9b518e08cb60a437be0a8b0aa26826f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/9e3c37a4b9b518e08cb60a437be0a8b0aa26826f"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/f1bca06f624245fde8a485deb2a589ba5d752537", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f1bca06f624245fde8a485deb2a589ba5d752537", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f1bca06f624245fde8a485deb2a589ba5d752537", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f1bca06f624245fde8a485deb2a589ba5d752537/comments", "author": {"login": "sprintersb", "id": 8905355, "node_id": "MDQ6VXNlcjg5MDUzNTU=", "avatar_url": "https://avatars.githubusercontent.com/u/8905355?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sprintersb", "html_url": "https://github.com/sprintersb", "followers_url": "https://api.github.com/users/sprintersb/followers", "following_url": "https://api.github.com/users/sprintersb/following{/other_user}", "gists_url": "https://api.github.com/users/sprintersb/gists{/gist_id}", "starred_url": "https://api.github.com/users/sprintersb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sprintersb/subscriptions", "organizations_url": "https://api.github.com/users/sprintersb/orgs", "repos_url": "https://api.github.com/users/sprintersb/repos", "events_url": "https://api.github.com/users/sprintersb/events{/privacy}", "received_events_url": "https://api.github.com/users/sprintersb/received_events", "type": "User", "site_admin": false}, "committer": {"login": "sprintersb", "id": 8905355, "node_id": "MDQ6VXNlcjg5MDUzNTU=", "avatar_url": "https://avatars.githubusercontent.com/u/8905355?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sprintersb", "html_url": "https://github.com/sprintersb", "followers_url": "https://api.github.com/users/sprintersb/followers", "following_url": "https://api.github.com/users/sprintersb/following{/other_user}", "gists_url": "https://api.github.com/users/sprintersb/gists{/gist_id}", "starred_url": "https://api.github.com/users/sprintersb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sprintersb/subscriptions", "organizations_url": "https://api.github.com/users/sprintersb/orgs", "repos_url": "https://api.github.com/users/sprintersb/repos", "events_url": "https://api.github.com/users/sprintersb/events{/privacy}", "received_events_url": "https://api.github.com/users/sprintersb/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "c5af52eb8cc208890ccb3bdce75b35bf5bbaa8bf", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c5af52eb8cc208890ccb3bdce75b35bf5bbaa8bf", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c5af52eb8cc208890ccb3bdce75b35bf5bbaa8bf"}], "stats": {"total": 367, "additions": 184, "deletions": 183}, "files": [{"sha": "5849b0f1e5f5415510785a80b6e963d338e1df21", "filename": "gcc/ChangeLog", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f1bca06f624245fde8a485deb2a589ba5d752537/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f1bca06f624245fde8a485deb2a589ba5d752537/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=f1bca06f624245fde8a485deb2a589ba5d752537", "patch": "@@ -1,4 +1,8 @@\n-2016-12-09  Martin Jambor  <mjambor@suse.cz>\n+2016-12-02  Georg-Johann Lay  <avr@gjlay.de>\n+\n+\t* config/avr/avr.c: Fix coding rule glitches.\n+\n+2016-12-02  Martin Jambor  <mjambor@suse.cz>\n \n \t* hsa.c (hsa_callable_function_p): Return false for artificial\n \tfunctions."}, {"sha": "8ceeff466f2cb65f0fecbba39072f8102ebbd246", "filename": "gcc/config/avr/avr.c", "status": "modified", "additions": 179, "deletions": 182, "changes": 361, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f1bca06f624245fde8a485deb2a589ba5d752537/gcc%2Fconfig%2Favr%2Favr.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f1bca06f624245fde8a485deb2a589ba5d752537/gcc%2Fconfig%2Favr%2Favr.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Favr%2Favr.c?ref=f1bca06f624245fde8a485deb2a589ba5d752537", "patch": "@@ -747,8 +747,8 @@ avr_option_override (void)\n      introduces additional code in LIM and increases reg pressure.  */\n \n   maybe_set_param_value (PARAM_ALLOW_STORE_DATA_RACES, 1,\n-      global_options.x_param_values,\n-      global_options_set.x_param_values);\n+                         global_options.x_param_values,\n+                         global_options_set.x_param_values);\n \n   /* Unwind tables currently require a frame pointer for correctness,\n      see toplev.c:process_options().  */\n@@ -1034,7 +1034,7 @@ avr_set_current_function (tree decl)\n   if (cfun->machine->is_OS_task + cfun->machine->is_OS_main\n       + (cfun->machine->is_signal || cfun->machine->is_interrupt) > 1)\n     error_at (loc, \"function attributes %qs, %qs and %qs are mutually\"\n-               \" exclusive\", \"OS_task\", \"OS_main\", isr);\n+              \" exclusive\", \"OS_task\", \"OS_main\", isr);\n \n   /* 'naked' will hide effects of 'OS_task' and 'OS_main'.  */\n \n@@ -1299,7 +1299,7 @@ avr_return_addr_rtx (int count, rtx tem)\n \n   /* Can only return this function's return address. Others not supported.  */\n   if (count)\n-     return NULL;\n+    return NULL;\n \n   if (AVR_3_BYTE_PC)\n     {\n@@ -1313,7 +1313,7 @@ avr_return_addr_rtx (int count, rtx tem)\n   r = gen_rtx_PLUS (Pmode, tem, r);\n   r = gen_frame_mem (Pmode, memory_address (Pmode, r));\n   r = gen_rtx_ROTATE (HImode, r, GEN_INT (8));\n-  return  r;\n+  return r;\n }\n \n /* Return 1 if the function epilogue is just a single \"ret\".  */\n@@ -2093,7 +2093,6 @@ avr_asm_function_begin_epilogue (FILE *file)\n static bool\n avr_cannot_modify_jumps_p (void)\n {\n-\n   /* Naked Functions must not have any instructions after\n      their epilogue, see PR42240 */\n \n@@ -2698,7 +2697,7 @@ avr_print_operand (FILE *file, rtx x, int code)\n       else if (code == 'b')\n         {\n           if (GET_CODE (addr) != PLUS)\n-               fatal_insn (\"bad address, not (reg+disp):\", addr);\n+            fatal_insn (\"bad address, not (reg+disp):\", addr);\n \n           avr_print_operand_address (file, VOIDmode, XEXP (addr, 0));\n         }\n@@ -2708,7 +2707,7 @@ avr_print_operand (FILE *file, rtx x, int code)\n             fatal_insn (\"bad address, not post_inc or pre_dec:\", addr);\n \n           if (code == 'p')\n-\t    /* X, Y, Z */\n+            /* X, Y, Z */\n             avr_print_operand_address (file, VOIDmode, XEXP (addr, 0));\n           else\n             avr_print_operand (file, XEXP (addr, 0), 0);  /* r26, r28, r30 */\n@@ -3723,8 +3722,6 @@ output_movhi (rtx_insn *insn, rtx xop[], int *plen)\n       return avr_out_lpm (insn, xop, plen);\n     }\n \n-  gcc_assert (2 == GET_MODE_SIZE (GET_MODE (dest)));\n-\n   if (REG_P (dest))\n     {\n       if (REG_P (src)) /* mov r,r */\n@@ -3825,8 +3822,8 @@ out_movqi_r_mr (rtx_insn *insn, rtx op[], int *plen)\n     }\n \n   if (GET_CODE (x) == PLUS\n-           && REG_P (XEXP (x, 0))\n-           && CONST_INT_P (XEXP (x, 1)))\n+      && REG_P (XEXP (x, 0))\n+      && CONST_INT_P (XEXP (x, 1)))\n     {\n       /* memory access by reg+disp */\n \n@@ -4016,7 +4013,7 @@ out_movhi_r_mr (rtx_insn *insn, rtx op[], int *plen)\n                            \"ldd %B0,Y+63\"    CR_TAB\n                            \"sbiw r28,%o1-62\", op, plen, -4)\n \n-              : avr_asm_len (\"subi r28,lo8(-%o1)\" CR_TAB\n+            : avr_asm_len (\"subi r28,lo8(-%o1)\" CR_TAB\n                            \"sbci r29,hi8(-%o1)\" CR_TAB\n                            \"ld %A0,Y\"           CR_TAB\n                            \"ldd %B0,Y+1\"        CR_TAB\n@@ -4385,7 +4382,7 @@ avr_out_movsi_mr_r_reg_no_disp_tiny (rtx_insn *insn, rtx op[], int *l)\n \n   if (reg_base == reg_src)\n     {\n-\t  /* \"ld r26,-X\" is undefined */\n+      /* \"ld r26,-X\" is undefined */\n       if (reg_unused_after (insn, base))\n         {\n           return *l = 7, (\"mov __tmp_reg__, %B1\"  CR_TAB\n@@ -4672,6 +4669,7 @@ output_movsisf (rtx_insn *insn, rtx operands[], int *l)\n     l = &dummy;\n \n   gcc_assert (4 == GET_MODE_SIZE (GET_MODE (dest)));\n+\n   if (REG_P (dest))\n     {\n       if (REG_P (src)) /* mov r,r */\n@@ -4717,7 +4715,7 @@ output_movsisf (rtx_insn *insn, rtx operands[], int *l)\n       const char *templ;\n \n       if (src == CONST0_RTX (GET_MODE (dest)))\n-\t  operands[1] = zero_reg_rtx;\n+        operands[1] = zero_reg_rtx;\n \n       templ = out_movsi_mr_r (insn, operands, real_l);\n \n@@ -4785,7 +4783,7 @@ avr_out_load_psi_reg_disp_tiny (rtx_insn *insn, rtx *op, int *plen)\n                           TINY_SBIW (%I1, %J1, 1)     CR_TAB\n                           \"ld %A0,%b1\"                CR_TAB\n                           \"mov %B0,__tmp_reg__\", op, plen, -8);\n-   }\n+    }\n   else\n     {\n       avr_asm_len (TINY_ADIW (%I1, %J1, %o1)   CR_TAB\n@@ -4914,9 +4912,9 @@ avr_out_load_psi (rtx_insn *insn, rtx *op, int *plen)\n                             \"ldd %A0,%A1\" CR_TAB\n                             \"mov %B0,__tmp_reg__\", op, plen, -4);\n \n-        return avr_asm_len (\"ldd %A0,%A1\" CR_TAB\n-                            \"ldd %B0,%B1\" CR_TAB\n-                            \"ldd %C0,%C1\", op, plen, -3);\n+      return avr_asm_len (\"ldd %A0,%A1\" CR_TAB\n+                          \"ldd %B0,%B1\" CR_TAB\n+                          \"ldd %C0,%C1\", op, plen, -3);\n     }\n   else if (GET_CODE (base) == PRE_DEC) /* (--R) */\n     return avr_asm_len (\"ld %C0,%1\" CR_TAB\n@@ -5191,14 +5189,14 @@ avr_out_movqi_mr_r_reg_disp_tiny (rtx_insn *insn, rtx op[], int *plen)\n                    TINY_ADIW (%I0, %J0, %o0) CR_TAB\n                    \"st %b0,__tmp_reg__\", op, plen, -4);\n     }\n-    else\n+  else\n     {\n       avr_asm_len (TINY_ADIW (%I0, %J0, %o0) CR_TAB\n                    \"st %b0,%1\", op, plen, -3);\n     }\n \n   if (!reg_unused_after (insn, XEXP (x, 0)))\n-      avr_asm_len (TINY_SBIW (%I0, %J0, %o0), op, plen, 2);\n+    avr_asm_len (TINY_SBIW (%I0, %J0, %o0), op, plen, 2);\n \n   return \"\";\n }\n@@ -5410,11 +5408,11 @@ avr_out_movhi_mr_r_reg_no_disp_tiny (rtx_insn *insn, rtx op[], int *plen)\n     }\n \n   return !mem_volatile_p && reg_unused_after (insn, base)\n-      ? avr_asm_len (\"st %0+,%A1\" CR_TAB\n-                     \"st %0,%B1\", op, plen, -2)\n-      : avr_asm_len (TINY_ADIW (%E0, %F0, 1) CR_TAB\n-                     \"st %0,%B1\"             CR_TAB\n-                     \"st -%0,%A1\", op, plen, -4);\n+    ? avr_asm_len (\"st %0+,%A1\" CR_TAB\n+                   \"st %0,%B1\", op, plen, -2)\n+    : avr_asm_len (TINY_ADIW (%E0, %F0, 1) CR_TAB\n+                   \"st %0,%B1\"             CR_TAB\n+                   \"st -%0,%A1\", op, plen, -4);\n }\n \n static const char*\n@@ -5797,8 +5795,8 @@ avr_out_compare (rtx_insn *insn, rtx *xop, int *plen)\n               && reg_unused_after (insn, xreg))\n             {\n               return AVR_TINY\n-                  ? avr_asm_len (TINY_ADIW (%A0, %B0, %n1), xop, plen, 2)\n-                  : avr_asm_len (\"adiw %0,%n1\", xop, plen, 1);\n+                ? avr_asm_len (TINY_ADIW (%A0, %B0, %n1), xop, plen, 2)\n+                : avr_asm_len (\"adiw %0,%n1\", xop, plen, 1);\n             }\n         }\n \n@@ -5973,7 +5971,7 @@ out_shift_with_cnt (const char *templ, rtx_insn *insn, rtx operands[],\n       int max_len = 10;  /* If larger than this, always use a loop.  */\n \n       if (count <= 0)\n-          return;\n+        return;\n \n       if (count < 8 && !scratch)\n         use_zero_reg = true;\n@@ -6044,7 +6042,7 @@ out_shift_with_cnt (const char *templ, rtx_insn *insn, rtx operands[],\n     fatal_insn (\"bad shift insn:\", insn);\n \n   if (second_label)\n-      avr_asm_len (\"rjmp 2f\", op, plen, 1);\n+    avr_asm_len (\"rjmp 2f\", op, plen, 1);\n \n   avr_asm_len (\"1:\", op, plen, 0);\n   avr_asm_len (templ, op, plen, t_len);\n@@ -8774,9 +8772,9 @@ avr_out_fract (rtx_insn *insn, rtx operands[], bool intsigned, int *plen)\n \t      xop[3] = all_regs_rtx[dest.regno_msb];\n \t      avr_asm_len (\"ldi %3,127\", xop, plen, 1);\n \t      avr_asm_len ((have_carry && lsb_in_tmp_reg ? \"adc __tmp_reg__,%3\"\n-\t\t\t   : have_carry ? \"adc %2,%3\"\n-\t\t\t   : lsb_in_tmp_reg ? \"add __tmp_reg__,%3\"\n-\t\t\t   : \"add %2,%3\"),\n+\t\t\t    : have_carry ? \"adc %2,%3\"\n+\t\t\t    : lsb_in_tmp_reg ? \"add __tmp_reg__,%3\"\n+\t\t\t    : \"add %2,%3\"),\n \t\t\t   xop, plen, 1);\n \t    }\n \t  else\n@@ -8860,7 +8858,7 @@ avr_out_fract (rtx_insn *insn, rtx operands[], bool intsigned, int *plen)\n                      \"lsl __tmp_reg__\", &all_regs_rtx[s0], plen, 2);\n \n       sign_in_carry = true;\n-  }\n+    }\n \n   gcc_assert (sign_in_carry + msb_in_carry + lsb_in_carry <= 1);\n \n@@ -8979,150 +8977,150 @@ avr_out_round (rtx_insn *insn ATTRIBUTE_UNUSED, rtx *xop, int *plen)\n \n \n /* Create RTL split patterns for byte sized rotate expressions.  This\n-  produces a series of move instructions and considers overlap situations.\n-  Overlapping non-HImode operands need a scratch register.  */\n+   produces a series of move instructions and considers overlap situations.\n+   Overlapping non-HImode operands need a scratch register.  */\n \n bool\n avr_rotate_bytes (rtx operands[])\n {\n-    machine_mode mode = GET_MODE (operands[0]);\n-    bool overlapped = reg_overlap_mentioned_p (operands[0], operands[1]);\n-    bool same_reg = rtx_equal_p (operands[0], operands[1]);\n-    int num = INTVAL (operands[2]);\n-    rtx scratch = operands[3];\n-    /* Work out if byte or word move is needed.  Odd byte rotates need QImode.\n-       Word move if no scratch is needed, otherwise use size of scratch.  */\n-    machine_mode move_mode = QImode;\n-    int move_size, offset, size;\n-\n-    if (num & 0xf)\n-      move_mode = QImode;\n-    else if ((mode == SImode && !same_reg) || !overlapped)\n-      move_mode = HImode;\n-    else\n-      move_mode = GET_MODE (scratch);\n-\n-    /* Force DI rotate to use QI moves since other DI moves are currently split\n-       into QI moves so forward propagation works better.  */\n-    if (mode == DImode)\n-      move_mode = QImode;\n-    /* Make scratch smaller if needed.  */\n-    if (SCRATCH != GET_CODE (scratch)\n-        && HImode == GET_MODE (scratch)\n-        && QImode == move_mode)\n-      scratch = simplify_gen_subreg (move_mode, scratch, HImode, 0);\n-\n-    move_size = GET_MODE_SIZE (move_mode);\n-    /* Number of bytes/words to rotate.  */\n-    offset = (num  >> 3) / move_size;\n-    /* Number of moves needed.  */\n-    size = GET_MODE_SIZE (mode) / move_size;\n-    /* Himode byte swap is special case to avoid a scratch register.  */\n-    if (mode == HImode && same_reg)\n-      {\n-\t/* HImode byte swap, using xor.  This is as quick as using scratch.  */\n-\trtx src, dst;\n-\tsrc = simplify_gen_subreg (move_mode, operands[1], mode, 0);\n-\tdst = simplify_gen_subreg (move_mode, operands[0], mode, 1);\n-\tif (!rtx_equal_p (dst, src))\n-\t  {\n-\t     emit_move_insn (dst, gen_rtx_XOR (QImode, dst, src));\n-\t     emit_move_insn (src, gen_rtx_XOR (QImode, src, dst));\n-\t     emit_move_insn (dst, gen_rtx_XOR (QImode, dst, src));\n-\t  }\n-      }\n-    else\n-      {\n+  machine_mode mode = GET_MODE (operands[0]);\n+  bool overlapped = reg_overlap_mentioned_p (operands[0], operands[1]);\n+  bool same_reg = rtx_equal_p (operands[0], operands[1]);\n+  int num = INTVAL (operands[2]);\n+  rtx scratch = operands[3];\n+  /* Work out if byte or word move is needed.  Odd byte rotates need QImode.\n+     Word move if no scratch is needed, otherwise use size of scratch.  */\n+  machine_mode move_mode = QImode;\n+  int move_size, offset, size;\n+\n+  if (num & 0xf)\n+    move_mode = QImode;\n+  else if ((mode == SImode && !same_reg) || !overlapped)\n+    move_mode = HImode;\n+  else\n+    move_mode = GET_MODE (scratch);\n+\n+  /* Force DI rotate to use QI moves since other DI moves are currently split\n+     into QI moves so forward propagation works better.  */\n+  if (mode == DImode)\n+    move_mode = QImode;\n+  /* Make scratch smaller if needed.  */\n+  if (SCRATCH != GET_CODE (scratch)\n+      && HImode == GET_MODE (scratch)\n+      && QImode == move_mode)\n+    scratch = simplify_gen_subreg (move_mode, scratch, HImode, 0);\n+\n+  move_size = GET_MODE_SIZE (move_mode);\n+  /* Number of bytes/words to rotate.  */\n+  offset = (num  >> 3) / move_size;\n+  /* Number of moves needed.  */\n+  size = GET_MODE_SIZE (mode) / move_size;\n+  /* Himode byte swap is special case to avoid a scratch register.  */\n+  if (mode == HImode && same_reg)\n+    {\n+      /* HImode byte swap, using xor.  This is as quick as using scratch.  */\n+      rtx src, dst;\n+      src = simplify_gen_subreg (move_mode, operands[1], mode, 0);\n+      dst = simplify_gen_subreg (move_mode, operands[0], mode, 1);\n+      if (!rtx_equal_p (dst, src))\n+        {\n+          emit_move_insn (dst, gen_rtx_XOR (QImode, dst, src));\n+          emit_move_insn (src, gen_rtx_XOR (QImode, src, dst));\n+          emit_move_insn (dst, gen_rtx_XOR (QImode, dst, src));\n+        }\n+    }\n+  else\n+    {\n #define MAX_SIZE 8 /* GET_MODE_SIZE (DImode) / GET_MODE_SIZE (QImode)  */\n-\t/* Create linked list of moves to determine move order.  */\n-\tstruct {\n-\t  rtx src, dst;\n-\t  int links;\n-\t} move[MAX_SIZE + 8];\n-\tint blocked, moves;\n-\n-\tgcc_assert (size <= MAX_SIZE);\n-\t/* Generate list of subreg moves.  */\n-\tfor (int i = 0; i < size; i++)\n-          {\n-\t    int from = i;\n-\t    int to = (from + offset) % size;\n-\t    move[i].src = simplify_gen_subreg (move_mode, operands[1],\n-                                               mode, from * move_size);\n-\t    move[i].dst = simplify_gen_subreg (move_mode, operands[0],\n-                                               mode, to * move_size);\n-            move[i].links = -1;\n-          }\n-\t/* Mark dependence where a dst of one move is the src of another move.\n-\t   The first move is a conflict as it must wait until second is\n-\t   performed.  We ignore moves to self - we catch this later.  */\n-\tif (overlapped)\n-\t  for (int i = 0; i < size; i++)\n-\t    if (reg_overlap_mentioned_p (move[i].dst, operands[1]))\n-\t      for (int j = 0; j < size; j++)\n-\t\tif (j != i && rtx_equal_p (move[j].src, move[i].dst))\n-\t\t  {\n-\t\t    /* The dst of move i is the src of move j.  */\n-\t\t    move[i].links = j;\n-\t\t    break;\n-\t\t  }\n-\n-\tblocked = -1;\n-\tmoves = 0;\n-\t/* Go through move list and perform non-conflicting moves.  As each\n-\t   non-overlapping move is made, it may remove other conflicts\n-\t   so the process is repeated until no conflicts remain.  */\n-\tdo\n-\t  {\n-\t    blocked = -1;\n-\t    moves = 0;\n-\t    /* Emit move where dst is not also a src or we have used that\n-\t       src already.  */\n-\t    for (int i = 0; i < size; i++)\n-\t      if (move[i].src != NULL_RTX)\n-\t\t{\n-\t\t  if (move[i].links == -1\n-\t\t      || move[move[i].links].src == NULL_RTX)\n-\t\t    {\n-\t\t      moves++;\n-\t\t      /* Ignore NOP moves to self.  */\n-\t\t      if (!rtx_equal_p (move[i].dst, move[i].src))\n-\t\t\temit_move_insn (move[i].dst, move[i].src);\n-\n-\t\t      /* Remove  conflict from list.  */\n-\t\t      move[i].src = NULL_RTX;\n-\t\t    }\n-\t\t  else\n-\t\t    blocked = i;\n-\t\t}\n+      /* Create linked list of moves to determine move order.  */\n+      struct {\n+        rtx src, dst;\n+        int links;\n+      } move[MAX_SIZE + 8];\n+      int blocked, moves;\n+\n+      gcc_assert (size <= MAX_SIZE);\n+      /* Generate list of subreg moves.  */\n+      for (int i = 0; i < size; i++)\n+        {\n+          int from = i;\n+          int to = (from + offset) % size;\n+          move[i].src = simplify_gen_subreg (move_mode, operands[1],\n+                                             mode, from * move_size);\n+          move[i].dst = simplify_gen_subreg (move_mode, operands[0],\n+                                             mode, to * move_size);\n+          move[i].links = -1;\n+        }\n+      /* Mark dependence where a dst of one move is the src of another move.\n+         The first move is a conflict as it must wait until second is\n+         performed.  We ignore moves to self - we catch this later.  */\n+      if (overlapped)\n+        for (int i = 0; i < size; i++)\n+          if (reg_overlap_mentioned_p (move[i].dst, operands[1]))\n+            for (int j = 0; j < size; j++)\n+              if (j != i && rtx_equal_p (move[j].src, move[i].dst))\n+                {\n+                  /* The dst of move i is the src of move j.  */\n+                  move[i].links = j;\n+                  break;\n+                }\n \n-\t    /* Check for deadlock. This is when no moves occurred and we have\n-\t       at least one blocked move.  */\n-\t    if (moves == 0 && blocked != -1)\n-\t      {\n-\t\t/* Need to use scratch register to break deadlock.\n-\t\t   Add move to put dst of blocked move into scratch.\n-\t\t   When this move occurs, it will break chain deadlock.\n-\t\t   The scratch register is substituted for real move.  */\n-\n-\t\tgcc_assert (SCRATCH != GET_CODE (scratch));\n-\n-\t\tmove[size].src = move[blocked].dst;\n-\t\tmove[size].dst =  scratch;\n-\t\t/* Scratch move is never blocked.  */\n-\t\tmove[size].links = -1;\n-\t\t/* Make sure we have valid link.  */\n-\t\tgcc_assert (move[blocked].links != -1);\n-\t\t/* Replace src of  blocking move with scratch reg.  */\n-\t\tmove[move[blocked].links].src = scratch;\n-\t\t/* Make dependent on scratch move occurring.  */\n-\t\tmove[blocked].links = size;\n-\t\tsize=size+1;\n-\t      }\n-\t  }\n-\twhile (blocked != -1);\n-      }\n-    return true;\n+      blocked = -1;\n+      moves = 0;\n+      /* Go through move list and perform non-conflicting moves.  As each\n+         non-overlapping move is made, it may remove other conflicts\n+         so the process is repeated until no conflicts remain.  */\n+      do\n+        {\n+          blocked = -1;\n+          moves = 0;\n+          /* Emit move where dst is not also a src or we have used that\n+             src already.  */\n+          for (int i = 0; i < size; i++)\n+            if (move[i].src != NULL_RTX)\n+              {\n+                if (move[i].links == -1\n+                    || move[move[i].links].src == NULL_RTX)\n+                  {\n+                    moves++;\n+                    /* Ignore NOP moves to self.  */\n+                    if (!rtx_equal_p (move[i].dst, move[i].src))\n+                      emit_move_insn (move[i].dst, move[i].src);\n+\n+                    /* Remove  conflict from list.  */\n+                    move[i].src = NULL_RTX;\n+                  }\n+                else\n+                  blocked = i;\n+              }\n+\n+          /* Check for deadlock. This is when no moves occurred and we have\n+             at least one blocked move.  */\n+          if (moves == 0 && blocked != -1)\n+            {\n+              /* Need to use scratch register to break deadlock.\n+                 Add move to put dst of blocked move into scratch.\n+                 When this move occurs, it will break chain deadlock.\n+                 The scratch register is substituted for real move.  */\n+\n+              gcc_assert (SCRATCH != GET_CODE (scratch));\n+\n+              move[size].src = move[blocked].dst;\n+              move[size].dst =  scratch;\n+              /* Scratch move is never blocked.  */\n+              move[size].links = -1;\n+              /* Make sure we have valid link.  */\n+              gcc_assert (move[blocked].links != -1);\n+              /* Replace src of  blocking move with scratch reg.  */\n+              move[move[blocked].links].src = scratch;\n+              /* Make dependent on scratch move occurring.  */\n+              move[blocked].links = size;\n+              size=size+1;\n+            }\n+        }\n+      while (blocked != -1);\n+    }\n+  return true;\n }\n \n \n@@ -9900,7 +9898,6 @@ avr_asm_output_aligned_decl_common (FILE * stream,\n       && SYMBOL_REF_P ((symbol = XEXP (mem, 0)))\n       && (SYMBOL_REF_FLAGS (symbol) & (SYMBOL_FLAG_IO | SYMBOL_FLAG_ADDRESS)))\n     {\n-\n       if (!local_p)\n \t{\n \t  fprintf (stream, \"\\t.globl\\t\");\n@@ -10139,7 +10136,7 @@ avr_encode_section_info (tree decl, rtx rtl, int new_decl_p)\n       && TREE_CODE (decl) != FUNCTION_DECL\n       && MEM_P (rtl)\n       && SYMBOL_REF_P (XEXP (rtl, 0)))\n-   {\n+    {\n       rtx sym = XEXP (rtl, 0);\n       tree type = TREE_TYPE (decl);\n       tree attr = DECL_ATTRIBUTES (decl);\n@@ -10345,7 +10342,7 @@ avr_adjust_reg_alloc_order (void)\n       17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2,\n       0, 1,\n       32, 33, 34, 35\n-  };\n+    };\n   static const int tiny_order_0[] = {\n     20, 21,\n     22, 23,\n@@ -10366,7 +10363,7 @@ avr_adjust_reg_alloc_order (void)\n       17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2,\n       0, 1,\n       32, 33, 34, 35\n-  };\n+    };\n   static const int tiny_order_1[] = {\n     22, 23,\n     24, 25,\n@@ -10386,7 +10383,7 @@ avr_adjust_reg_alloc_order (void)\n       17, 16, 15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2,\n       1, 0,\n       32, 33, 34, 35\n-  };\n+    };\n \n   /* Select specific register allocation order.\n      Tiny Core (ATtiny4/5/9/10/20/40) devices have only 16 registers,\n@@ -10397,7 +10394,7 @@ avr_adjust_reg_alloc_order (void)\n                       : (AVR_TINY ? tiny_order_0 : order_0));\n \n   for (size_t i = 0; i < ARRAY_SIZE (order_0); ++i)\n-      reg_alloc_order[i] = order[i];\n+    reg_alloc_order[i] = order[i];\n }\n \n \n@@ -10767,10 +10764,10 @@ avr_rtx_costs_1 (rtx x, machine_mode mode, int outer_code ATTRIBUTE_UNUSED,\n                 *total = COSTS_N_INSNS (AVR_HAVE_JMP_CALL ? 5 : 4);\n             }\n \n-\t   if (mode == DImode)\n-\t     *total *= 2;\n+\t  if (mode == DImode)\n+\t    *total *= 2;\n \n-\t   return true;\n+\t  return true;\n \n \tdefault:\n \t  return false;\n@@ -13187,7 +13184,7 @@ avr_expand_delay_cycles (rtx operands0)\n       emit_insn (gen_delay_cycles_1 (gen_int_mode (loop_count, QImode),\n                                      avr_mem_clobber()));\n       cycles -= cycles_used;\n-      }\n+    }\n \n   while (cycles >= 2)\n     {"}]}