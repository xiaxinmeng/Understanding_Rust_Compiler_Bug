{"sha": "bdc05efbc844f9878e36b0e5427af4806502709c", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YmRjMDVlZmJjODQ0Zjk4NzhlMzZiMGU1NDI3YWY0ODA2NTAyNzA5Yw==", "commit": {"author": {"name": "Paolo Carlini", "email": "paolo.carlini@oracle.com", "date": "2011-08-04T19:57:48Z"}, "committer": {"name": "Paolo Carlini", "email": "paolo@gcc.gnu.org", "date": "2011-08-04T19:57:48Z"}, "message": "atomic.cc: Use noexcept.\n\n2011-08-04  Paolo Carlini  <paolo.carlini@oracle.com>\n\n\t* src/atomic.cc: Use noexcept.\n\t* include/std/atomic: Likewise.\n\t* include/bits/atomic_0.h: Likewise.\n\t* include/bits/atomic_2.h: Likewise.\n\t* include/bits/atomic_base.h: Likewise.\n\nFrom-SVN: r177413", "tree": {"sha": "d2c10764f8edcd3a97242d67280f13a5330498b4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/d2c10764f8edcd3a97242d67280f13a5330498b4"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/bdc05efbc844f9878e36b0e5427af4806502709c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/bdc05efbc844f9878e36b0e5427af4806502709c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/bdc05efbc844f9878e36b0e5427af4806502709c", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/bdc05efbc844f9878e36b0e5427af4806502709c/comments", "author": null, "committer": null, "parents": [{"sha": "c1ea7f07d93e5d06b31585c49e001892a56dff45", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c1ea7f07d93e5d06b31585c49e001892a56dff45", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c1ea7f07d93e5d06b31585c49e001892a56dff45"}], "stats": {"total": 813, "additions": 435, "deletions": 378}, "files": [{"sha": "97a1d83aac742271e1e89f07a433df73b9cfcf08", "filename": "libstdc++-v3/ChangeLog", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bdc05efbc844f9878e36b0e5427af4806502709c/libstdc%2B%2B-v3%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bdc05efbc844f9878e36b0e5427af4806502709c/libstdc%2B%2B-v3%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2FChangeLog?ref=bdc05efbc844f9878e36b0e5427af4806502709c", "patch": "@@ -1,3 +1,11 @@\n+2011-08-04  Paolo Carlini  <paolo.carlini@oracle.com>\n+\n+\t* src/atomic.cc: Use noexcept.\n+\t* include/std/atomic: Likewise.\n+\t* include/bits/atomic_0.h: Likewise.\n+\t* include/bits/atomic_2.h: Likewise.\n+\t* include/bits/atomic_base.h: Likewise.\n+\n 2011-08-03  Benjamin Kosnik  <bkoz@redhat.com>\n \n \t* testsuite/performance/27_io/filebuf_sputn_unbuf.cc: Include"}, {"sha": "4f8b0929f16184b3264aaf53099fe35fa4301982", "filename": "libstdc++-v3/include/bits/atomic_0.h", "status": "modified", "additions": 108, "deletions": 91, "changes": 199, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bdc05efbc844f9878e36b0e5427af4806502709c/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_0.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bdc05efbc844f9878e36b0e5427af4806502709c/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_0.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_0.h?ref=bdc05efbc844f9878e36b0e5427af4806502709c", "patch": "@@ -111,26 +111,26 @@ namespace __atomic0\n   /// atomic_flag\n   struct atomic_flag : public __atomic_flag_base\n   {\n-    atomic_flag() = default;\n-    ~atomic_flag() = default;\n+    atomic_flag() noexcept = default;\n+    ~atomic_flag() noexcept = default;\n     atomic_flag(const atomic_flag&) = delete;\n     atomic_flag& operator=(const atomic_flag&) = delete;\n     atomic_flag& operator=(const atomic_flag&) volatile = delete;\n \n     // Conversion to ATOMIC_FLAG_INIT.\n-    atomic_flag(bool __i): __atomic_flag_base({ __i }) { }\n+    atomic_flag(bool __i) noexcept : __atomic_flag_base({ __i }) { }\n \n     bool\n-    test_and_set(memory_order __m = memory_order_seq_cst);\n+    test_and_set(memory_order __m = memory_order_seq_cst) noexcept;\n \n     bool\n-    test_and_set(memory_order __m = memory_order_seq_cst) volatile;\n+    test_and_set(memory_order __m = memory_order_seq_cst) volatile noexcept;\n \n     void\n-    clear(memory_order __m = memory_order_seq_cst);\n+    clear(memory_order __m = memory_order_seq_cst) noexcept;\n \n     void\n-    clear(memory_order __m = memory_order_seq_cst) volatile;\n+    clear(memory_order __m = memory_order_seq_cst) volatile noexcept;\n   };\n \n \n@@ -166,117 +166,117 @@ namespace __atomic0\n       __int_type \t_M_i;\n \n     public:\n-      __atomic_base() = default;\n-      ~__atomic_base() = default;\n+      __atomic_base() noexcept = default;\n+      ~__atomic_base() noexcept = default;\n       __atomic_base(const __atomic_base&) = delete;\n       __atomic_base& operator=(const __atomic_base&) = delete;\n       __atomic_base& operator=(const __atomic_base&) volatile = delete;\n \n       // Requires __int_type convertible to _M_base._M_i.\n-      constexpr __atomic_base(__int_type __i): _M_i (__i) { }\n+      constexpr __atomic_base(__int_type __i) noexcept : _M_i (__i) { }\n \n-      operator __int_type() const\n+      operator __int_type() const noexcept\n       { return load(); }\n \n-      operator __int_type() const volatile\n+      operator __int_type() const volatile noexcept\n       { return load(); }\n \n       __int_type\n-      operator=(__int_type __i)\n+      operator=(__int_type __i) noexcept\n       {\n \tstore(__i);\n \treturn __i;\n       }\n \n       __int_type\n-      operator=(__int_type __i) volatile\n+      operator=(__int_type __i) volatile noexcept\n       {\n \tstore(__i);\n \treturn __i;\n       }\n \n       __int_type\n-      operator++(int)\n+      operator++(int) noexcept\n       { return fetch_add(1); }\n \n       __int_type\n-      operator++(int) volatile\n+      operator++(int) volatile noexcept\n       { return fetch_add(1); }\n \n       __int_type\n-      operator--(int)\n+      operator--(int) noexcept\n       { return fetch_sub(1); }\n \n       __int_type\n-      operator--(int) volatile\n+      operator--(int) volatile noexcept\n       { return fetch_sub(1); }\n \n       __int_type\n-      operator++()\n+      operator++() noexcept\n       { return fetch_add(1) + 1; }\n \n       __int_type\n-      operator++() volatile\n+      operator++() volatile noexcept\n       { return fetch_add(1) + 1; }\n \n       __int_type\n-      operator--()\n+      operator--() noexcept\n       { return fetch_sub(1) - 1; }\n \n       __int_type\n-      operator--() volatile\n+      operator--() volatile noexcept\n       { return fetch_sub(1) - 1; }\n \n       __int_type\n-      operator+=(__int_type __i)\n+      operator+=(__int_type __i) noexcept\n       { return fetch_add(__i) + __i; }\n \n       __int_type\n-      operator+=(__int_type __i) volatile\n+      operator+=(__int_type __i) volatile noexcept\n       { return fetch_add(__i) + __i; }\n \n       __int_type\n-      operator-=(__int_type __i)\n+      operator-=(__int_type __i) noexcept\n       { return fetch_sub(__i) - __i; }\n \n       __int_type\n-      operator-=(__int_type __i) volatile\n+      operator-=(__int_type __i) volatile noexcept\n       { return fetch_sub(__i) - __i; }\n \n       __int_type\n-      operator&=(__int_type __i)\n+      operator&=(__int_type __i) noexcept\n       { return fetch_and(__i) & __i; }\n \n       __int_type\n-      operator&=(__int_type __i) volatile\n+      operator&=(__int_type __i) volatile noexcept\n       { return fetch_and(__i) & __i; }\n \n       __int_type\n-      operator|=(__int_type __i)\n+      operator|=(__int_type __i) noexcept\n       { return fetch_or(__i) | __i; }\n \n       __int_type\n-      operator|=(__int_type __i) volatile\n+      operator|=(__int_type __i) volatile noexcept\n       { return fetch_or(__i) | __i; }\n \n       __int_type\n-      operator^=(__int_type __i)\n+      operator^=(__int_type __i) noexcept\n       { return fetch_xor(__i) ^ __i; }\n \n       __int_type\n-      operator^=(__int_type __i) volatile\n+      operator^=(__int_type __i) volatile noexcept\n       { return fetch_xor(__i) ^ __i; }\n \n       bool\n-      is_lock_free() const\n+      is_lock_free() const noexcept\n       { return false; }\n \n       bool\n-      is_lock_free() const volatile\n+      is_lock_free() const volatile noexcept\n       { return false; }\n \n       void\n-      store(__int_type __i, memory_order __m = memory_order_seq_cst)\n+      store(__int_type __i, memory_order __m = memory_order_seq_cst) noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_acquire);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n@@ -285,7 +285,8 @@ namespace __atomic0\n       }\n \n       void\n-      store(__int_type __i, memory_order __m = memory_order_seq_cst) volatile\n+      store(__int_type __i,\n+\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_acquire);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n@@ -294,32 +295,34 @@ namespace __atomic0\n       }\n \n       __int_type\n-      load(memory_order __m = memory_order_seq_cst) const\n+      load(memory_order __m = memory_order_seq_cst) const noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_release);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n \treturn _ATOMIC_LOAD_(this, __m);\n       }\n \n       __int_type\n-      load(memory_order __m = memory_order_seq_cst) const volatile\n+      load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_release);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n \treturn _ATOMIC_LOAD_(this, __m);\n       }\n \n       __int_type\n-      exchange(__int_type __i, memory_order __m = memory_order_seq_cst)\n+      exchange(__int_type __i,\n+\t       memory_order __m = memory_order_seq_cst) noexcept\n       { return _ATOMIC_MODIFY_(this, =, __i, __m); }\n \n       __int_type\n-      exchange(__int_type __i, memory_order __m = memory_order_seq_cst) volatile\n+      exchange(__int_type __i,\n+\t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _ATOMIC_MODIFY_(this, =, __i, __m); }\n \n       bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n-\t\t\t    memory_order __m1, memory_order __m2)\n+\t\t\t    memory_order __m1, memory_order __m2) noexcept\n       {\n \t__glibcxx_assert(__m2 != memory_order_release);\n \t__glibcxx_assert(__m2 != memory_order_acq_rel);\n@@ -329,7 +332,8 @@ namespace __atomic0\n \n       bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n-\t\t\t    memory_order __m1, memory_order __m2) volatile\n+\t\t\t    memory_order __m1,\n+\t\t\t    memory_order __m2) volatile noexcept\n       {\n \t__glibcxx_assert(__m2 != memory_order_release);\n \t__glibcxx_assert(__m2 != memory_order_acq_rel);\n@@ -339,23 +343,23 @@ namespace __atomic0\n \n       bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n-\t\t\t    memory_order __m = memory_order_seq_cst)\n+\t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn compare_exchange_weak(__i1, __i2, __m,\n \t\t\t\t     __calculate_memory_order(__m));\n       }\n \n       bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n-\t\t\t    memory_order __m = memory_order_seq_cst) volatile\n+\t\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn compare_exchange_weak(__i1, __i2, __m,\n \t\t\t\t     __calculate_memory_order(__m));\n       }\n \n       bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n-\t\t\t      memory_order __m1, memory_order __m2)\n+\t\t\t      memory_order __m1, memory_order __m2) noexcept\n       {\n \t__glibcxx_assert(__m2 != memory_order_release);\n \t__glibcxx_assert(__m2 != memory_order_acq_rel);\n@@ -365,7 +369,8 @@ namespace __atomic0\n \n       bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n-\t\t\t      memory_order __m1, memory_order __m2) volatile\n+\t\t\t      memory_order __m1,\n+\t\t\t      memory_order __m2) volatile noexcept\n       {\n \t__glibcxx_assert(__m2 != memory_order_release);\n \t__glibcxx_assert(__m2 != memory_order_acq_rel);\n@@ -375,62 +380,68 @@ namespace __atomic0\n \n       bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n-\t\t\t      memory_order __m = memory_order_seq_cst)\n+\t\t\t      memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn compare_exchange_strong(__i1, __i2, __m,\n \t\t\t\t       __calculate_memory_order(__m));\n       }\n \n       bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n-\t\t\t      memory_order __m = memory_order_seq_cst) volatile\n+\t\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn compare_exchange_strong(__i1, __i2, __m,\n \t\t\t\t       __calculate_memory_order(__m));\n       }\n \n       __int_type\n-      fetch_add(__int_type __i, memory_order __m = memory_order_seq_cst)\n+      fetch_add(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return _ATOMIC_MODIFY_(this, +=, __i, __m); }\n \n       __int_type\n       fetch_add(__int_type __i,\n-\t\tmemory_order __m = memory_order_seq_cst) volatile\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _ATOMIC_MODIFY_(this, +=, __i, __m); }\n \n       __int_type\n-      fetch_sub(__int_type __i, memory_order __m = memory_order_seq_cst)\n+      fetch_sub(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return _ATOMIC_MODIFY_(this, -=, __i, __m); }\n \n       __int_type\n       fetch_sub(__int_type __i,\n-\t\tmemory_order __m = memory_order_seq_cst) volatile\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _ATOMIC_MODIFY_(this, -=, __i, __m); }\n \n       __int_type\n-      fetch_and(__int_type __i, memory_order __m = memory_order_seq_cst)\n+      fetch_and(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return _ATOMIC_MODIFY_(this, &=, __i, __m); }\n \n       __int_type\n       fetch_and(__int_type __i,\n-\t\tmemory_order __m = memory_order_seq_cst) volatile\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _ATOMIC_MODIFY_(this, &=, __i, __m); }\n \n       __int_type\n-      fetch_or(__int_type __i, memory_order __m = memory_order_seq_cst)\n+      fetch_or(__int_type __i,\n+\t       memory_order __m = memory_order_seq_cst) noexcept\n       { return _ATOMIC_MODIFY_(this, |=, __i, __m); }\n \n       __int_type\n-      fetch_or(__int_type __i, memory_order __m = memory_order_seq_cst) volatile\n+      fetch_or(__int_type __i,\n+\t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _ATOMIC_MODIFY_(this, |=, __i, __m); }\n \n       __int_type\n-      fetch_xor(__int_type __i, memory_order __m = memory_order_seq_cst)\n+      fetch_xor(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return _ATOMIC_MODIFY_(this, ^=, __i, __m); }\n \n       __int_type\n       fetch_xor(__int_type __i,\n-\t\tmemory_order __m = memory_order_seq_cst) volatile\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _ATOMIC_MODIFY_(this, ^=, __i, __m); }\n     };\n \n@@ -445,93 +456,95 @@ namespace __atomic0\n       __pointer_type \t_M_i;\n \n     public:\n-      __atomic_base() = default;\n-      ~__atomic_base() = default;\n+      __atomic_base() noexcept = default;\n+      ~__atomic_base() noexcept = default;\n       __atomic_base(const __atomic_base&) = delete;\n       __atomic_base& operator=(const __atomic_base&) = delete;\n       __atomic_base& operator=(const __atomic_base&) volatile = delete;\n \n       // Requires __pointer_type convertible to _M_i.\n-      constexpr __atomic_base(__return_pointer_type __p): _M_i (__p) { }\n+      constexpr __atomic_base(__return_pointer_type __p) noexcept\n+      : _M_i (__p) { }\n \n-      operator __return_pointer_type() const\n+      operator __return_pointer_type() const noexcept\n       { return reinterpret_cast<__return_pointer_type>(load()); }\n \n-      operator __return_pointer_type() const volatile\n+      operator __return_pointer_type() const volatile noexcept\n       { return reinterpret_cast<__return_pointer_type>(load()); }\n \n       __return_pointer_type\n-      operator=(__pointer_type __p)\n+      operator=(__pointer_type __p) noexcept\n       {\n \tstore(__p);\n \treturn reinterpret_cast<__return_pointer_type>(__p);\n       }\n \n       __return_pointer_type\n-      operator=(__pointer_type __p) volatile\n+      operator=(__pointer_type __p) volatile noexcept\n       {\n \tstore(__p);\n \treturn reinterpret_cast<__return_pointer_type>(__p);\n       }\n \n       __return_pointer_type\n-      operator++(int)\n+      operator++(int) noexcept\n       { return reinterpret_cast<__return_pointer_type>(fetch_add(1)); }\n \n       __return_pointer_type\n-      operator++(int) volatile\n+      operator++(int) volatile noexcept\n       { return reinterpret_cast<__return_pointer_type>(fetch_add(1)); }\n \n       __return_pointer_type\n-      operator--(int)\n+      operator--(int) noexcept\n       { return reinterpret_cast<__return_pointer_type>(fetch_sub(1)); }\n \n       __return_pointer_type\n-      operator--(int) volatile\n+      operator--(int) volatile noexcept\n       { return reinterpret_cast<__return_pointer_type>(fetch_sub(1)); }\n \n       __return_pointer_type\n-      operator++()\n+      operator++() noexcept\n       { return reinterpret_cast<__return_pointer_type>(fetch_add(1) + 1); }\n \n       __return_pointer_type\n-      operator++() volatile\n+      operator++() volatile noexcept\n       { return reinterpret_cast<__return_pointer_type>(fetch_add(1) + 1); }\n \n       __return_pointer_type\n-      operator--()\n+      operator--() noexcept\n       { return reinterpret_cast<__return_pointer_type>(fetch_sub(1) - 1); }\n \n       __return_pointer_type\n-      operator--() volatile\n+      operator--() volatile noexcept\n       { return reinterpret_cast<__return_pointer_type>(fetch_sub(1) - 1); }\n \n       __return_pointer_type\n-      operator+=(ptrdiff_t __d)\n+      operator+=(ptrdiff_t __d) noexcept\n       { return reinterpret_cast<__return_pointer_type>(fetch_add(__d) + __d); }\n \n       __return_pointer_type\n-      operator+=(ptrdiff_t __d) volatile\n+      operator+=(ptrdiff_t __d) volatile noexcept\n       { return reinterpret_cast<__return_pointer_type>(fetch_add(__d) + __d); }\n \n       __return_pointer_type\n-      operator-=(ptrdiff_t __d)\n+      operator-=(ptrdiff_t __d) noexcept\n       { return reinterpret_cast<__return_pointer_type>(fetch_sub(__d) - __d); }\n \n       __return_pointer_type\n-      operator-=(ptrdiff_t __d) volatile\n+      operator-=(ptrdiff_t __d) volatile noexcept\n       { return reinterpret_cast<__return_pointer_type>(fetch_sub(__d) - __d); }\n \n       bool\n-      is_lock_free() const\n+      is_lock_free() const noexcept\n       { return true; }\n \n       bool\n-      is_lock_free() const volatile\n+      is_lock_free() const volatile noexcept\n       { return true; }\n \n       void\n-      store(__pointer_type __p, memory_order __m = memory_order_seq_cst)\n+      store(__pointer_type __p,\n+\t    memory_order __m = memory_order_seq_cst) noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_acquire);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n@@ -541,7 +554,7 @@ namespace __atomic0\n \n       void\n       store(__pointer_type __p,\n-\t    memory_order __m = memory_order_seq_cst) volatile\n+\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_acquire);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n@@ -556,7 +569,7 @@ namespace __atomic0\n       }\n \n       __return_pointer_type\n-      load(memory_order __m = memory_order_seq_cst) const\n+      load(memory_order __m = memory_order_seq_cst) const noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_release);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n@@ -565,7 +578,7 @@ namespace __atomic0\n       }\n \n       __return_pointer_type\n-      load(memory_order __m = memory_order_seq_cst) const volatile\n+      load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_release);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n@@ -574,15 +587,16 @@ namespace __atomic0\n       }\n \n       __return_pointer_type\n-      exchange(__pointer_type __p, memory_order __m = memory_order_seq_cst)\n+      exchange(__pointer_type __p,\n+\t       memory_order __m = memory_order_seq_cst) noexcept\n       {\n \tvoid* __v = _ATOMIC_MODIFY_(this, =, __p, __m);\n \treturn reinterpret_cast<__return_pointer_type>(__v);\n       }\n \n       __return_pointer_type\n       exchange(__pointer_type __p,\n-\t       memory_order __m = memory_order_seq_cst) volatile\n+\t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \tvolatile __pointer_type* __p2 = &_M_i;\n \t__typeof__(__p) __w = (__p);\n@@ -597,7 +611,7 @@ namespace __atomic0\n \n       bool\n       compare_exchange_strong(__return_pointer_type& __rp1, __pointer_type __p2,\n-\t\t\t      memory_order __m1, memory_order __m2)\n+\t\t\t      memory_order __m1, memory_order __m2) noexcept\n       {\n \t__glibcxx_assert(__m2 != memory_order_release);\n \t__glibcxx_assert(__m2 != memory_order_acq_rel);\n@@ -608,7 +622,8 @@ namespace __atomic0\n \n       bool\n       compare_exchange_strong(__return_pointer_type& __rp1, __pointer_type __p2,\n-\t\t\t      memory_order __m1, memory_order __m2) volatile\n+\t\t\t      memory_order __m1,\n+\t\t\t      memory_order __m2) volatile noexcept\n       {\n \t__glibcxx_assert(__m2 != memory_order_release);\n \t__glibcxx_assert(__m2 != memory_order_acq_rel);\n@@ -618,30 +633,32 @@ namespace __atomic0\n       }\n \n       __return_pointer_type\n-      fetch_add(ptrdiff_t __d, memory_order __m = memory_order_seq_cst)\n+      fetch_add(ptrdiff_t __d,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n       {\n \tvoid* __v = _ATOMIC_MODIFY_(this, +=, __d, __m);\n \treturn reinterpret_cast<__return_pointer_type>(__v);\n       }\n \n       __return_pointer_type\n       fetch_add(ptrdiff_t __d,\n-\t\tmemory_order __m = memory_order_seq_cst) volatile\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \tvoid* __v = _ATOMIC_MODIFY_(this, +=, __d, __m);\n \treturn reinterpret_cast<__return_pointer_type>(__v);\n       }\n \n       __return_pointer_type\n-      fetch_sub(ptrdiff_t __d, memory_order __m = memory_order_seq_cst)\n+      fetch_sub(ptrdiff_t __d,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n       {\n \tvoid* __v = _ATOMIC_MODIFY_(this, -=, __d, __m);\n \treturn reinterpret_cast<__return_pointer_type>(__v);\n       }\n \n       __return_pointer_type\n       fetch_sub(ptrdiff_t __d,\n-\t\tmemory_order __m = memory_order_seq_cst) volatile\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \tvoid* __v = _ATOMIC_MODIFY_(this, -=, __d, __m);\n \treturn reinterpret_cast<__return_pointer_type>(__v);"}, {"sha": "072e82a0a9f6977aa587e65048dcdcb676401e07", "filename": "libstdc++-v3/include/bits/atomic_2.h", "status": "modified", "additions": 107, "deletions": 91, "changes": 198, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bdc05efbc844f9878e36b0e5427af4806502709c/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_2.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bdc05efbc844f9878e36b0e5427af4806502709c/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_2.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_2.h?ref=bdc05efbc844f9878e36b0e5427af4806502709c", "patch": "@@ -48,17 +48,17 @@ namespace __atomic2\n   /// atomic_flag\n   struct atomic_flag : public __atomic_flag_base\n   {\n-    atomic_flag() = default;\n-    ~atomic_flag() = default;\n+    atomic_flag() noexcept = default;\n+    ~atomic_flag() noexcept = default;\n     atomic_flag(const atomic_flag&) = delete;\n     atomic_flag& operator=(const atomic_flag&) = delete;\n     atomic_flag& operator=(const atomic_flag&) volatile = delete;\n \n     // Conversion to ATOMIC_FLAG_INIT.\n-    atomic_flag(bool __i): __atomic_flag_base({ __i }) { }\n+    atomic_flag(bool __i) noexcept : __atomic_flag_base({ __i }) { }\n \n     bool\n-    test_and_set(memory_order __m = memory_order_seq_cst)\n+    test_and_set(memory_order __m = memory_order_seq_cst) noexcept\n     {\n       // Redundant synchronize if built-in for lock is a full barrier.\n       if (__m != memory_order_acquire && __m != memory_order_acq_rel)\n@@ -67,7 +67,7 @@ namespace __atomic2\n     }\n \n     bool\n-    test_and_set(memory_order __m = memory_order_seq_cst) volatile\n+    test_and_set(memory_order __m = memory_order_seq_cst) volatile noexcept\n     {\n       // Redundant synchronize if built-in for lock is a full barrier.\n       if (__m != memory_order_acquire && __m != memory_order_acq_rel)\n@@ -76,7 +76,7 @@ namespace __atomic2\n     }\n \n     void\n-    clear(memory_order __m = memory_order_seq_cst)\n+    clear(memory_order __m = memory_order_seq_cst) noexcept\n     {\n       __glibcxx_assert(__m != memory_order_consume);\n       __glibcxx_assert(__m != memory_order_acquire);\n@@ -88,7 +88,7 @@ namespace __atomic2\n     }\n \n     void\n-    clear(memory_order __m = memory_order_seq_cst) volatile\n+    clear(memory_order __m = memory_order_seq_cst) volatile noexcept\n     {\n       __glibcxx_assert(__m != memory_order_consume);\n       __glibcxx_assert(__m != memory_order_acquire);\n@@ -133,117 +133,117 @@ namespace __atomic2\n       __int_type \t_M_i;\n \n     public:\n-      __atomic_base() = default;\n-      ~__atomic_base() = default;\n+      __atomic_base() noexcept = default;\n+      ~__atomic_base() noexcept = default;\n       __atomic_base(const __atomic_base&) = delete;\n       __atomic_base& operator=(const __atomic_base&) = delete;\n       __atomic_base& operator=(const __atomic_base&) volatile = delete;\n \n       // Requires __int_type convertible to _M_i.\n-      constexpr __atomic_base(__int_type __i): _M_i (__i) { }\n+      constexpr __atomic_base(__int_type __i) noexcept : _M_i (__i) { }\n \n-      operator __int_type() const\n+      operator __int_type() const noexcept\n       { return load(); }\n \n-      operator __int_type() const volatile\n+      operator __int_type() const volatile noexcept\n       { return load(); }\n \n       __int_type\n-      operator=(__int_type __i)\n+      operator=(__int_type __i) noexcept\n       {\n \tstore(__i);\n \treturn __i;\n       }\n \n       __int_type\n-      operator=(__int_type __i) volatile\n+      operator=(__int_type __i) volatile noexcept\n       {\n \tstore(__i);\n \treturn __i;\n       }\n \n       __int_type\n-      operator++(int)\n+      operator++(int) noexcept\n       { return fetch_add(1); }\n \n       __int_type\n-      operator++(int) volatile\n+      operator++(int) volatile noexcept\n       { return fetch_add(1); }\n \n       __int_type\n-      operator--(int)\n+      operator--(int) noexcept\n       { return fetch_sub(1); }\n \n       __int_type\n-      operator--(int) volatile\n+      operator--(int) volatile noexcept\n       { return fetch_sub(1); }\n \n       __int_type\n-      operator++()\n+      operator++() noexcept\n       { return __sync_add_and_fetch(&_M_i, 1); }\n \n       __int_type\n-      operator++() volatile\n+      operator++() volatile noexcept\n       { return __sync_add_and_fetch(&_M_i, 1); }\n \n       __int_type\n-      operator--()\n+      operator--() noexcept\n       { return __sync_sub_and_fetch(&_M_i, 1); }\n \n       __int_type\n-      operator--() volatile\n+      operator--() volatile noexcept\n       { return __sync_sub_and_fetch(&_M_i, 1); }\n \n       __int_type\n-      operator+=(__int_type __i)\n+      operator+=(__int_type __i) noexcept\n       { return __sync_add_and_fetch(&_M_i, __i); }\n \n       __int_type\n-      operator+=(__int_type __i) volatile\n+      operator+=(__int_type __i) volatile noexcept\n       { return __sync_add_and_fetch(&_M_i, __i); }\n \n       __int_type\n-      operator-=(__int_type __i)\n+      operator-=(__int_type __i) noexcept\n       { return __sync_sub_and_fetch(&_M_i, __i); }\n \n       __int_type\n-      operator-=(__int_type __i) volatile\n+      operator-=(__int_type __i) volatile noexcept\n       { return __sync_sub_and_fetch(&_M_i, __i); }\n \n       __int_type\n-      operator&=(__int_type __i)\n+      operator&=(__int_type __i) noexcept\n       { return __sync_and_and_fetch(&_M_i, __i); }\n \n       __int_type\n-      operator&=(__int_type __i) volatile\n+      operator&=(__int_type __i) volatile noexcept\n       { return __sync_and_and_fetch(&_M_i, __i); }\n \n       __int_type\n-      operator|=(__int_type __i)\n+      operator|=(__int_type __i) noexcept\n       { return __sync_or_and_fetch(&_M_i, __i); }\n \n       __int_type\n-      operator|=(__int_type __i) volatile\n+      operator|=(__int_type __i) volatile noexcept\n       { return __sync_or_and_fetch(&_M_i, __i); }\n \n       __int_type\n-      operator^=(__int_type __i)\n+      operator^=(__int_type __i) noexcept\n       { return __sync_xor_and_fetch(&_M_i, __i); }\n \n       __int_type\n-      operator^=(__int_type __i) volatile\n+      operator^=(__int_type __i) volatile noexcept\n       { return __sync_xor_and_fetch(&_M_i, __i); }\n \n       bool\n-      is_lock_free() const\n+      is_lock_free() const noexcept\n       { return true; }\n \n       bool\n-      is_lock_free() const volatile\n+      is_lock_free() const volatile noexcept\n       { return true; }\n \n       void\n-      store(__int_type __i, memory_order __m = memory_order_seq_cst)\n+      store(__int_type __i, memory_order __m = memory_order_seq_cst) noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_acquire);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n@@ -261,7 +261,8 @@ namespace __atomic2\n       }\n \n       void\n-      store(__int_type __i, memory_order __m = memory_order_seq_cst) volatile\n+      store(__int_type __i,\n+\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_acquire);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n@@ -279,7 +280,7 @@ namespace __atomic2\n       }\n \n       __int_type\n-      load(memory_order __m = memory_order_seq_cst) const\n+      load(memory_order __m = memory_order_seq_cst) const noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_release);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n@@ -291,7 +292,7 @@ namespace __atomic2\n       }\n \n       __int_type\n-      load(memory_order __m = memory_order_seq_cst) const volatile\n+      load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_release);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n@@ -303,49 +304,52 @@ namespace __atomic2\n       }\n \n       __int_type\n-      exchange(__int_type __i, memory_order __m = memory_order_seq_cst)\n+      exchange(__int_type __i,\n+\t       memory_order __m = memory_order_seq_cst) noexcept\n       {\n \t// XXX built-in assumes memory_order_acquire.\n \treturn __sync_lock_test_and_set(&_M_i, __i);\n       }\n \n \n       __int_type\n-      exchange(__int_type __i, memory_order __m = memory_order_seq_cst) volatile\n+      exchange(__int_type __i,\n+\t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \t// XXX built-in assumes memory_order_acquire.\n \treturn __sync_lock_test_and_set(&_M_i, __i);\n       }\n \n       bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n-\t\t\t    memory_order __m1, memory_order __m2)\n+\t\t\t    memory_order __m1, memory_order __m2) noexcept\n       { return compare_exchange_strong(__i1, __i2, __m1, __m2); }\n \n       bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n-\t\t\t    memory_order __m1, memory_order __m2) volatile\n+\t\t\t    memory_order __m1,\n+\t\t\t    memory_order __m2) volatile noexcept\n       { return compare_exchange_strong(__i1, __i2, __m1, __m2); }\n \n       bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n-\t\t\t    memory_order __m = memory_order_seq_cst)\n+\t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn compare_exchange_weak(__i1, __i2, __m,\n \t\t\t\t     __calculate_memory_order(__m));\n       }\n \n       bool\n       compare_exchange_weak(__int_type& __i1, __int_type __i2,\n-\t\t\t    memory_order __m = memory_order_seq_cst) volatile\n+\t\t   memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn compare_exchange_weak(__i1, __i2, __m,\n \t\t\t\t     __calculate_memory_order(__m));\n       }\n \n       bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n-\t\t\t      memory_order __m1, memory_order __m2)\n+\t\t\t      memory_order __m1, memory_order __m2) noexcept\n       {\n \t__glibcxx_assert(__m2 != memory_order_release);\n \t__glibcxx_assert(__m2 != memory_order_acq_rel);\n@@ -361,7 +365,8 @@ namespace __atomic2\n \n       bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n-\t\t\t      memory_order __m1, memory_order __m2) volatile\n+\t\t\t      memory_order __m1,\n+\t\t\t      memory_order __m2) volatile noexcept\n       {\n \t__glibcxx_assert(__m2 != memory_order_release);\n \t__glibcxx_assert(__m2 != memory_order_acq_rel);\n@@ -377,63 +382,68 @@ namespace __atomic2\n \n       bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n-\t\t\t      memory_order __m = memory_order_seq_cst)\n+\t\t\t      memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn compare_exchange_strong(__i1, __i2, __m,\n \t\t\t\t       __calculate_memory_order(__m));\n       }\n \n       bool\n       compare_exchange_strong(__int_type& __i1, __int_type __i2,\n-\t\t\t      memory_order __m = memory_order_seq_cst) volatile\n+\t\t memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn compare_exchange_strong(__i1, __i2, __m,\n \t\t\t\t       __calculate_memory_order(__m));\n       }\n \n       __int_type\n-      fetch_add(__int_type __i, memory_order __m = memory_order_seq_cst)\n+      fetch_add(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __sync_fetch_and_add(&_M_i, __i); }\n \n       __int_type\n       fetch_add(__int_type __i,\n-\t\tmemory_order __m = memory_order_seq_cst) volatile\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __sync_fetch_and_add(&_M_i, __i); }\n \n       __int_type\n-      fetch_sub(__int_type __i, memory_order __m = memory_order_seq_cst)\n+      fetch_sub(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __sync_fetch_and_sub(&_M_i, __i); }\n \n       __int_type\n       fetch_sub(__int_type __i,\n-\t\tmemory_order __m = memory_order_seq_cst) volatile\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __sync_fetch_and_sub(&_M_i, __i); }\n \n       __int_type\n-      fetch_and(__int_type __i, memory_order __m = memory_order_seq_cst)\n+      fetch_and(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __sync_fetch_and_and(&_M_i, __i); }\n \n       __int_type\n       fetch_and(__int_type __i,\n-\t\tmemory_order __m = memory_order_seq_cst) volatile\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __sync_fetch_and_and(&_M_i, __i); }\n \n       __int_type\n-      fetch_or(__int_type __i, memory_order __m = memory_order_seq_cst)\n+      fetch_or(__int_type __i,\n+\t       memory_order __m = memory_order_seq_cst) noexcept\n       { return __sync_fetch_and_or(&_M_i, __i); }\n \n       __int_type\n       fetch_or(__int_type __i,\n-\t       memory_order __m = memory_order_seq_cst) volatile\n+\t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __sync_fetch_and_or(&_M_i, __i); }\n \n       __int_type\n-      fetch_xor(__int_type __i, memory_order __m = memory_order_seq_cst)\n+      fetch_xor(__int_type __i,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __sync_fetch_and_xor(&_M_i, __i); }\n \n       __int_type\n       fetch_xor(__int_type __i,\n-\t\tmemory_order __m = memory_order_seq_cst) volatile\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __sync_fetch_and_xor(&_M_i, __i); }\n     };\n \n@@ -448,93 +458,94 @@ namespace __atomic2\n       __pointer_type \t_M_p;\n \n     public:\n-      __atomic_base() = default;\n-      ~__atomic_base() = default;\n+      __atomic_base() noexcept = default;\n+      ~__atomic_base() noexcept = default;\n       __atomic_base(const __atomic_base&) = delete;\n       __atomic_base& operator=(const __atomic_base&) = delete;\n       __atomic_base& operator=(const __atomic_base&) volatile = delete;\n \n       // Requires __pointer_type convertible to _M_p.\n-      constexpr __atomic_base(__pointer_type __p): _M_p (__p) { }\n+      constexpr __atomic_base(__pointer_type __p) noexcept : _M_p (__p) { }\n \n-      operator __pointer_type() const\n+      operator __pointer_type() const noexcept\n       { return load(); }\n \n-      operator __pointer_type() const volatile\n+      operator __pointer_type() const volatile noexcept\n       { return load(); }\n \n       __pointer_type\n-      operator=(__pointer_type __p)\n+      operator=(__pointer_type __p) noexcept\n       {\n \tstore(__p);\n \treturn __p;\n       }\n \n       __pointer_type\n-      operator=(__pointer_type __p) volatile\n+      operator=(__pointer_type __p) volatile noexcept\n       {\n \tstore(__p);\n \treturn __p;\n       }\n \n       __pointer_type\n-      operator++(int)\n+      operator++(int) noexcept\n       { return fetch_add(1); }\n \n       __pointer_type\n-      operator++(int) volatile\n+      operator++(int) volatile noexcept\n       { return fetch_add(1); }\n \n       __pointer_type\n-      operator--(int)\n+      operator--(int) noexcept\n       { return fetch_sub(1); }\n \n       __pointer_type\n-      operator--(int) volatile\n+      operator--(int) volatile noexcept\n       { return fetch_sub(1); }\n \n       __pointer_type\n-      operator++()\n+      operator++() noexcept\n       { return fetch_add(1) + 1; }\n \n       __pointer_type\n-      operator++() volatile\n+      operator++() volatile noexcept\n       { return fetch_add(1) + 1; }\n \n       __pointer_type\n-      operator--()\n+      operator--() noexcept\n       { return fetch_sub(1) -1; }\n \n       __pointer_type\n-      operator--() volatile\n+      operator--() volatile noexcept\n       { return fetch_sub(1) -1; }\n \n       __pointer_type\n-      operator+=(ptrdiff_t __d)\n+      operator+=(ptrdiff_t __d) noexcept\n       { return fetch_add(__d) + __d; }\n \n       __pointer_type\n-      operator+=(ptrdiff_t __d) volatile\n+      operator+=(ptrdiff_t __d) volatile noexcept\n       { return fetch_add(__d) + __d; }\n \n       __pointer_type\n-      operator-=(ptrdiff_t __d)\n+      operator-=(ptrdiff_t __d) noexcept\n       { return fetch_sub(__d) - __d; }\n \n       __pointer_type\n-      operator-=(ptrdiff_t __d) volatile\n+      operator-=(ptrdiff_t __d) volatile noexcept\n       { return fetch_sub(__d) - __d; }\n \n       bool\n-      is_lock_free() const\n+      is_lock_free() const noexcept\n       { return true; }\n \n       bool\n-      is_lock_free() const volatile\n+      is_lock_free() const volatile noexcept\n       { return true; }\n \n       void\n-      store(__pointer_type __p, memory_order __m = memory_order_seq_cst)\n+      store(__pointer_type __p,\n+\t    memory_order __m = memory_order_seq_cst) noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_acquire);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n@@ -553,7 +564,7 @@ namespace __atomic2\n \n       void\n       store(__pointer_type __p,\n-\t    memory_order __m = memory_order_seq_cst) volatile\n+\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_acquire);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n@@ -571,7 +582,7 @@ namespace __atomic2\n       }\n \n       __pointer_type\n-      load(memory_order __m = memory_order_seq_cst) const\n+      load(memory_order __m = memory_order_seq_cst) const noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_release);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n@@ -583,7 +594,7 @@ namespace __atomic2\n       }\n \n       __pointer_type\n-      load(memory_order __m = memory_order_seq_cst) const volatile\n+      load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n       {\n \t__glibcxx_assert(__m != memory_order_release);\n \t__glibcxx_assert(__m != memory_order_acq_rel);\n@@ -595,7 +606,8 @@ namespace __atomic2\n       }\n \n       __pointer_type\n-      exchange(__pointer_type __p, memory_order __m = memory_order_seq_cst)\n+      exchange(__pointer_type __p,\n+\t       memory_order __m = memory_order_seq_cst) noexcept\n       {\n \t// XXX built-in assumes memory_order_acquire.\n \treturn __sync_lock_test_and_set(&_M_p, __p);\n@@ -604,15 +616,16 @@ namespace __atomic2\n \n       __pointer_type\n       exchange(__pointer_type __p,\n-\t       memory_order __m = memory_order_seq_cst) volatile\n+\t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \t// XXX built-in assumes memory_order_acquire.\n \treturn __sync_lock_test_and_set(&_M_p, __p);\n       }\n \n       bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n-\t\t\t      memory_order __m1, memory_order __m2)\n+\t\t\t      memory_order __m1,\n+\t\t\t      memory_order __m2) noexcept\n       {\n \t__glibcxx_assert(__m2 != memory_order_release);\n \t__glibcxx_assert(__m2 != memory_order_acq_rel);\n@@ -628,7 +641,8 @@ namespace __atomic2\n \n       bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n-\t\t\t      memory_order __m1, memory_order __m2) volatile\n+\t\t\t      memory_order __m1,\n+\t\t\t      memory_order __m2) volatile noexcept\n       {\n \t__glibcxx_assert(__m2 != memory_order_release);\n \t__glibcxx_assert(__m2 != memory_order_acq_rel);\n@@ -643,21 +657,23 @@ namespace __atomic2\n       }\n \n       __pointer_type\n-      fetch_add(ptrdiff_t __d, memory_order __m = memory_order_seq_cst)\n+      fetch_add(ptrdiff_t __d,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __sync_fetch_and_add(&_M_p, __d); }\n \n       __pointer_type\n       fetch_add(ptrdiff_t __d,\n-\t\tmemory_order __m = memory_order_seq_cst) volatile\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __sync_fetch_and_add(&_M_p, __d); }\n \n       __pointer_type\n-      fetch_sub(ptrdiff_t __d, memory_order __m = memory_order_seq_cst)\n+      fetch_sub(ptrdiff_t __d,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return __sync_fetch_and_sub(&_M_p, __d); }\n \n       __pointer_type\n       fetch_sub(ptrdiff_t __d,\n-\t\tmemory_order __m = memory_order_seq_cst) volatile\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return __sync_fetch_and_sub(&_M_p, __d); }\n     };\n "}, {"sha": "ebb7d58ac7241e09cd913bc4d253a60c089fc2b8", "filename": "libstdc++-v3/include/bits/atomic_base.h", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bdc05efbc844f9878e36b0e5427af4806502709c/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_base.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bdc05efbc844f9878e36b0e5427af4806502709c/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_base.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_base.h?ref=bdc05efbc844f9878e36b0e5427af4806502709c", "patch": "@@ -59,7 +59,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n     } memory_order;\n \n   inline memory_order\n-  __calculate_memory_order(memory_order __m)\n+  __calculate_memory_order(memory_order __m) noexcept\n   {\n     const bool __cond1 = __m == memory_order_release;\n     const bool __cond2 = __m == memory_order_acq_rel;\n@@ -69,15 +69,15 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n   }\n \n   void\n-  atomic_thread_fence(memory_order);\n+  atomic_thread_fence(memory_order) noexcept;\n \n   void\n-  atomic_signal_fence(memory_order);\n+  atomic_signal_fence(memory_order) noexcept;\n \n   /// kill_dependency\n   template<typename _Tp>\n     inline _Tp\n-    kill_dependency(_Tp __y)\n+    kill_dependency(_Tp __y) noexcept\n     {\n       _Tp __ret(__y);\n       return __ret;"}, {"sha": "b3fa7d8120b8f329588ba6ace679e67e723960a6", "filename": "libstdc++-v3/include/std/atomic", "status": "modified", "additions": 205, "deletions": 189, "changes": 394, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bdc05efbc844f9878e36b0e5427af4806502709c/libstdc%2B%2B-v3%2Finclude%2Fstd%2Fatomic", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bdc05efbc844f9878e36b0e5427af4806502709c/libstdc%2B%2B-v3%2Finclude%2Fstd%2Fatomic", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fstd%2Fatomic?ref=bdc05efbc844f9878e36b0e5427af4806502709c", "patch": "@@ -59,92 +59,93 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n     __atomic_base<bool>\t_M_base;\n \n   public:\n-    atomic_bool() = default;\n-    ~atomic_bool() = default;\n+    atomic_bool() noexcept = default;\n+    ~atomic_bool() noexcept = default;\n     atomic_bool(const atomic_bool&) = delete;\n     atomic_bool& operator=(const atomic_bool&) = delete;\n     atomic_bool& operator=(const atomic_bool&) volatile = delete;\n \n-    constexpr atomic_bool(bool __i) : _M_base(__i) { }\n+    constexpr atomic_bool(bool __i) noexcept : _M_base(__i) { }\n \n     bool\n-    operator=(bool __i)\n+    operator=(bool __i) noexcept\n     { return _M_base.operator=(__i); }\n \n-    operator bool() const\n+    operator bool() const noexcept\n     { return _M_base.load(); }\n \n-    operator bool() const volatile\n+    operator bool() const volatile noexcept\n     { return _M_base.load(); }\n \n     bool\n-    is_lock_free() const { return _M_base.is_lock_free(); }\n+    is_lock_free() const noexcept { return _M_base.is_lock_free(); }\n \n     bool\n-    is_lock_free() const volatile { return _M_base.is_lock_free(); }\n+    is_lock_free() const volatile noexcept { return _M_base.is_lock_free(); }\n \n     void\n-    store(bool __i, memory_order __m = memory_order_seq_cst)\n+    store(bool __i, memory_order __m = memory_order_seq_cst) noexcept\n     { _M_base.store(__i, __m); }\n \n     void\n-    store(bool __i, memory_order __m = memory_order_seq_cst) volatile\n+    store(bool __i, memory_order __m = memory_order_seq_cst) volatile noexcept\n     { _M_base.store(__i, __m); }\n \n     bool\n-    load(memory_order __m = memory_order_seq_cst) const\n+    load(memory_order __m = memory_order_seq_cst) const noexcept\n     { return _M_base.load(__m); }\n \n     bool\n-    load(memory_order __m = memory_order_seq_cst) const volatile\n+    load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n     { return _M_base.load(__m); }\n \n     bool\n-    exchange(bool __i, memory_order __m = memory_order_seq_cst)\n+    exchange(bool __i, memory_order __m = memory_order_seq_cst) noexcept\n     { return _M_base.exchange(__i, __m); }\n \n     bool\n-    exchange(bool __i, memory_order __m = memory_order_seq_cst) volatile\n+    exchange(bool __i,\n+\t     memory_order __m = memory_order_seq_cst) volatile noexcept\n     { return _M_base.exchange(__i, __m); }\n \n     bool\n     compare_exchange_weak(bool& __i1, bool __i2, memory_order __m1,\n-\t\t\t  memory_order __m2)\n+\t\t\t  memory_order __m2) noexcept\n     { return _M_base.compare_exchange_weak(__i1, __i2, __m1, __m2); }\n \n     bool\n     compare_exchange_weak(bool& __i1, bool __i2, memory_order __m1,\n-\t\t\t  memory_order __m2) volatile\n+\t\t\t  memory_order __m2) volatile noexcept\n     { return _M_base.compare_exchange_weak(__i1, __i2, __m1, __m2); }\n \n     bool\n     compare_exchange_weak(bool& __i1, bool __i2,\n-\t\t\t  memory_order __m = memory_order_seq_cst)\n+\t\t\t  memory_order __m = memory_order_seq_cst) noexcept\n     { return _M_base.compare_exchange_weak(__i1, __i2, __m); }\n \n     bool\n     compare_exchange_weak(bool& __i1, bool __i2,\n-\t\t\t  memory_order __m = memory_order_seq_cst) volatile\n+\t\t     memory_order __m = memory_order_seq_cst) volatile noexcept\n     { return _M_base.compare_exchange_weak(__i1, __i2, __m); }\n \n     bool\n     compare_exchange_strong(bool& __i1, bool __i2, memory_order __m1,\n-\t\t\t    memory_order __m2)\n+\t\t\t    memory_order __m2) noexcept\n     { return _M_base.compare_exchange_strong(__i1, __i2, __m1, __m2); }\n \n     bool\n     compare_exchange_strong(bool& __i1, bool __i2, memory_order __m1,\n-\t\t\t    memory_order __m2) volatile\n+\t\t\t    memory_order __m2) volatile noexcept\n     { return _M_base.compare_exchange_strong(__i1, __i2, __m1, __m2); }\n \n     bool\n     compare_exchange_strong(bool& __i1, bool __i2,\n-\t\t\t    memory_order __m = memory_order_seq_cst)\n+\t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n     { return _M_base.compare_exchange_strong(__i1, __i2, __m); }\n \n     bool\n     compare_exchange_strong(bool& __i1, bool __i2,\n-\t\t\t    memory_order __m = memory_order_seq_cst) volatile\n+\t\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n     { return _M_base.compare_exchange_strong(__i1, __i2, __m); }\n   };\n \n@@ -158,73 +159,77 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       _Tp _M_i;\n \n     public:\n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(_Tp __i) : _M_i(__i) { }\n+      constexpr atomic(_Tp __i) noexcept : _M_i(__i) { }\n \n-      operator _Tp() const;\n+      operator _Tp() const noexcept;\n \n-      operator _Tp() const volatile;\n+      operator _Tp() const volatile noexcept;\n \n       _Tp\n-      operator=(_Tp __i) { store(__i); return __i; }\n+      operator=(_Tp __i) noexcept { store(__i); return __i; }\n \n       _Tp\n-      operator=(_Tp __i) volatile { store(__i); return __i; }\n+      operator=(_Tp __i) volatile noexcept { store(__i); return __i; }\n \n       bool\n-      is_lock_free() const;\n+      is_lock_free() const noexcept;\n \n       bool\n-      is_lock_free() const volatile;\n+      is_lock_free() const volatile noexcept;\n \n       void\n-      store(_Tp, memory_order = memory_order_seq_cst);\n+      store(_Tp, memory_order = memory_order_seq_cst) noexcept;\n \n       void\n-      store(_Tp, memory_order = memory_order_seq_cst) volatile;\n+      store(_Tp, memory_order = memory_order_seq_cst) volatile noexcept;\n \n       _Tp\n-      load(memory_order = memory_order_seq_cst) const;\n+      load(memory_order = memory_order_seq_cst) const noexcept;\n \n       _Tp\n-      load(memory_order = memory_order_seq_cst) const volatile;\n+      load(memory_order = memory_order_seq_cst) const volatile noexcept;\n \n       _Tp\n-      exchange(_Tp __i, memory_order = memory_order_seq_cst);\n+      exchange(_Tp __i, memory_order = memory_order_seq_cst) noexcept;\n \n       _Tp\n-      exchange(_Tp __i, memory_order = memory_order_seq_cst) volatile;\n+      exchange(_Tp __i, memory_order = memory_order_seq_cst) volatile noexcept;\n \n       bool\n-      compare_exchange_weak(_Tp&, _Tp, memory_order, memory_order);\n+      compare_exchange_weak(_Tp&, _Tp, memory_order, memory_order) noexcept;\n \n       bool\n-      compare_exchange_weak(_Tp&, _Tp, memory_order, memory_order) volatile;\n+      compare_exchange_weak(_Tp&, _Tp, memory_order,\n+\t\t\t    memory_order) volatile noexcept;\n \n       bool\n-      compare_exchange_weak(_Tp&, _Tp, memory_order = memory_order_seq_cst);\n+      compare_exchange_weak(_Tp&, _Tp,\n+\t\t\t    memory_order = memory_order_seq_cst) noexcept;\n \n       bool\n       compare_exchange_weak(_Tp&, _Tp,\n-\t\t\t    memory_order = memory_order_seq_cst) volatile;\n+\t\t       memory_order = memory_order_seq_cst) volatile noexcept;\n \n       bool\n-      compare_exchange_strong(_Tp&, _Tp, memory_order, memory_order);\n+      compare_exchange_strong(_Tp&, _Tp, memory_order, memory_order) noexcept;\n \n       bool\n-      compare_exchange_strong(_Tp&, _Tp, memory_order, memory_order) volatile;\n+      compare_exchange_strong(_Tp&, _Tp, memory_order,\n+\t\t\t      memory_order) volatile noexcept;\n \n       bool\n-      compare_exchange_strong(_Tp&, _Tp, memory_order = memory_order_seq_cst);\n+      compare_exchange_strong(_Tp&, _Tp,\n+\t\t\t      memory_order = memory_order_seq_cst) noexcept;\n \n       bool\n       compare_exchange_strong(_Tp&, _Tp,\n-\t\t\t      memory_order = memory_order_seq_cst) volatile;\n+\t\t       memory_order = memory_order_seq_cst) volatile noexcept;\n     };\n \n \n@@ -236,178 +241,184 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef __atomic_base<_Tp*>\t__base_type;\n       __base_type\t\t\t_M_b;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__pointer_type __p) : _M_b(__p) { }\n+      constexpr atomic(__pointer_type __p) noexcept : _M_b(__p) { }\n \n-      operator __pointer_type() const\n+      operator __pointer_type() const noexcept\n       { return __pointer_type(_M_b); }\n \n-      operator __pointer_type() const volatile\n+      operator __pointer_type() const volatile noexcept\n       { return __pointer_type(_M_b); }\n \n       __pointer_type\n-      operator=(__pointer_type __p)\n+      operator=(__pointer_type __p) noexcept\n       { return _M_b.operator=(__p); }\n \n       __pointer_type\n-      operator=(__pointer_type __p) volatile\n+      operator=(__pointer_type __p) volatile noexcept\n       { return _M_b.operator=(__p); }\n \n       __pointer_type\n-      operator++(int)\n+      operator++(int) noexcept\n       { return _M_b++; }\n \n       __pointer_type\n-      operator++(int) volatile\n+      operator++(int) volatile noexcept\n       { return _M_b++; }\n \n       __pointer_type\n-      operator--(int)\n+      operator--(int) noexcept\n       { return _M_b--; }\n \n       __pointer_type\n-      operator--(int) volatile\n+      operator--(int) volatile noexcept\n       { return _M_b--; }\n \n       __pointer_type\n-      operator++()\n+      operator++() noexcept\n       { return ++_M_b; }\n \n       __pointer_type\n-      operator++() volatile\n+      operator++() volatile noexcept\n       { return ++_M_b; }\n \n       __pointer_type\n-      operator--()\n+      operator--() noexcept\n       { return --_M_b; }\n \n       __pointer_type\n-      operator--() volatile\n+      operator--() volatile noexcept\n       { return --_M_b; }\n \n       __pointer_type\n-      operator+=(ptrdiff_t __d)\n+      operator+=(ptrdiff_t __d) noexcept\n       { return _M_b.operator+=(__d); }\n \n       __pointer_type\n-      operator+=(ptrdiff_t __d) volatile\n+      operator+=(ptrdiff_t __d) volatile noexcept\n       { return _M_b.operator+=(__d); }\n \n       __pointer_type\n-      operator-=(ptrdiff_t __d)\n+      operator-=(ptrdiff_t __d) noexcept\n       { return _M_b.operator-=(__d); }\n \n       __pointer_type\n-      operator-=(ptrdiff_t __d) volatile\n+      operator-=(ptrdiff_t __d) volatile noexcept\n       { return _M_b.operator-=(__d); }\n \n       bool\n-      is_lock_free() const\n+      is_lock_free() const noexcept\n       { return _M_b.is_lock_free(); }\n \n       bool\n-      is_lock_free() const volatile\n+      is_lock_free() const volatile noexcept\n       { return _M_b.is_lock_free(); }\n \n       void\n-      store(__pointer_type __p, memory_order __m = memory_order_seq_cst)\n+      store(__pointer_type __p,\n+\t    memory_order __m = memory_order_seq_cst) noexcept\n       { return _M_b.store(__p, __m); }\n \n       void\n       store(__pointer_type __p,\n-\t    memory_order __m = memory_order_seq_cst) volatile\n+\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _M_b.store(__p, __m); }\n \n       __pointer_type\n-      load(memory_order __m = memory_order_seq_cst) const\n+      load(memory_order __m = memory_order_seq_cst) const noexcept\n       { return _M_b.load(__m); }\n \n       __pointer_type\n-      load(memory_order __m = memory_order_seq_cst) const volatile\n+      load(memory_order __m = memory_order_seq_cst) const volatile noexcept\n       { return _M_b.load(__m); }\n \n       __pointer_type\n-      exchange(__pointer_type __p, memory_order __m = memory_order_seq_cst)\n+      exchange(__pointer_type __p,\n+\t       memory_order __m = memory_order_seq_cst) noexcept\n       { return _M_b.exchange(__p, __m); }\n \n       __pointer_type\n       exchange(__pointer_type __p,\n-\t       memory_order __m = memory_order_seq_cst) volatile\n+\t       memory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _M_b.exchange(__p, __m); }\n \n       bool\n       compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n-\t\t\t    memory_order __m1, memory_order __m2)\n+\t\t\t    memory_order __m1, memory_order __m2) noexcept\n       { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n \n       bool\n       compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n-\t\t\t    memory_order __m1, memory_order __m2) volatile\n+\t\t\t    memory_order __m1,\n+\t\t\t    memory_order __m2) volatile noexcept\n       { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n \n       bool\n       compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n-\t\t\t    memory_order __m = memory_order_seq_cst)\n+\t\t\t    memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn compare_exchange_weak(__p1, __p2, __m,\n \t\t\t\t     __calculate_memory_order(__m));\n       }\n \n       bool\n       compare_exchange_weak(__pointer_type& __p1, __pointer_type __p2,\n-\t\t\t    memory_order __m = memory_order_seq_cst) volatile\n+\t\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn compare_exchange_weak(__p1, __p2, __m,\n \t\t\t\t     __calculate_memory_order(__m));\n       }\n \n       bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n-\t\t\t      memory_order __m1, memory_order __m2)\n+\t\t\t      memory_order __m1, memory_order __m2) noexcept\n       { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n \n       bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n-\t\t\t      memory_order __m1, memory_order __m2) volatile\n+\t\t\t      memory_order __m1,\n+\t\t\t      memory_order __m2) volatile noexcept\n       { return _M_b.compare_exchange_strong(__p1, __p2, __m1, __m2); }\n \n       bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n-\t\t\t      memory_order __m = memory_order_seq_cst)\n+\t\t\t      memory_order __m = memory_order_seq_cst) noexcept\n       {\n \treturn _M_b.compare_exchange_strong(__p1, __p2, __m,\n \t\t\t\t\t    __calculate_memory_order(__m));\n       }\n \n       bool\n       compare_exchange_strong(__pointer_type& __p1, __pointer_type __p2,\n-\t\t\t      memory_order __m = memory_order_seq_cst) volatile\n+\t\t    memory_order __m = memory_order_seq_cst) volatile noexcept\n       {\n \treturn _M_b.compare_exchange_strong(__p1, __p2, __m,\n \t\t\t\t\t    __calculate_memory_order(__m));\n       }\n \n       __pointer_type\n-      fetch_add(ptrdiff_t __d, memory_order __m = memory_order_seq_cst)\n+      fetch_add(ptrdiff_t __d,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return _M_b.fetch_add(__d, __m); }\n \n       __pointer_type\n       fetch_add(ptrdiff_t __d,\n-\t\tmemory_order __m = memory_order_seq_cst) volatile\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _M_b.fetch_add(__d, __m); }\n \n       __pointer_type\n-      fetch_sub(ptrdiff_t __d, memory_order __m = memory_order_seq_cst)\n+      fetch_sub(ptrdiff_t __d,\n+\t\tmemory_order __m = memory_order_seq_cst) noexcept\n       { return _M_b.fetch_sub(__d, __m); }\n \n       __pointer_type\n       fetch_sub(ptrdiff_t __d,\n-\t\tmemory_order __m = memory_order_seq_cst) volatile\n+\t\tmemory_order __m = memory_order_seq_cst) volatile noexcept\n       { return _M_b.fetch_sub(__d, __m); }\n     };\n \n@@ -419,13 +430,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef bool \t\t\t__integral_type;\n       typedef atomic_bool \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -438,13 +449,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef char \t\t\t__integral_type;\n       typedef atomic_char \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -457,13 +468,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef signed char \t\t__integral_type;\n       typedef atomic_schar \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept= default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -476,13 +487,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef unsigned char \t\t__integral_type;\n       typedef atomic_uchar \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept= default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -495,13 +506,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef short \t\t\t__integral_type;\n       typedef atomic_short \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -514,13 +525,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef unsigned short \t      \t__integral_type;\n       typedef atomic_ushort \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -533,13 +544,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef int \t\t\t__integral_type;\n       typedef atomic_int \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -552,13 +563,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef unsigned int\t\t__integral_type;\n       typedef atomic_uint \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -571,13 +582,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef long \t\t\t__integral_type;\n       typedef atomic_long \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -590,13 +601,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef unsigned long \t\t__integral_type;\n       typedef atomic_ulong \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -609,13 +620,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef long long \t\t__integral_type;\n       typedef atomic_llong \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -628,13 +639,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef unsigned long long       \t__integral_type;\n       typedef atomic_ullong \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -647,13 +658,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef wchar_t \t\t\t__integral_type;\n       typedef atomic_wchar_t \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -666,13 +677,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef char16_t \t\t\t__integral_type;\n       typedef atomic_char16_t \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -685,13 +696,13 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       typedef char32_t \t\t\t__integral_type;\n       typedef atomic_char32_t \t\t__base_type;\n \n-      atomic() = default;\n-      ~atomic() = default;\n+      atomic() noexcept = default;\n+      ~atomic() noexcept = default;\n       atomic(const atomic&) = delete;\n       atomic& operator=(const atomic&) = delete;\n       atomic& operator=(const atomic&) volatile = delete;\n \n-      constexpr atomic(__integral_type __i) : __base_type(__i) { }\n+      constexpr atomic(__integral_type __i) noexcept : __base_type(__i) { }\n \n       using __base_type::operator __integral_type;\n       using __base_type::operator=;\n@@ -700,157 +711,162 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n \n   // Function definitions, atomic_flag operations.\n   inline bool\n-  atomic_flag_test_and_set_explicit(atomic_flag* __a, memory_order __m)\n+  atomic_flag_test_and_set_explicit(atomic_flag* __a,\n+\t\t\t\t    memory_order __m) noexcept\n   { return __a->test_and_set(__m); }\n \n   inline bool\n   atomic_flag_test_and_set_explicit(volatile atomic_flag* __a,\n-\t\t\t\t    memory_order __m)\n+\t\t\t\t    memory_order __m) noexcept\n   { return __a->test_and_set(__m); }\n \n   inline void\n-  atomic_flag_clear_explicit(atomic_flag* __a, memory_order __m)\n+  atomic_flag_clear_explicit(atomic_flag* __a, memory_order __m) noexcept\n   { __a->clear(__m); }\n \n   inline void\n-  atomic_flag_clear_explicit(volatile atomic_flag* __a, memory_order __m)\n+  atomic_flag_clear_explicit(volatile atomic_flag* __a,\n+\t\t\t     memory_order __m) noexcept\n   { __a->clear(__m); }\n \n   inline bool\n-  atomic_flag_test_and_set(atomic_flag* __a)\n+  atomic_flag_test_and_set(atomic_flag* __a) noexcept\n   { return atomic_flag_test_and_set_explicit(__a, memory_order_seq_cst); }\n \n   inline bool\n-  atomic_flag_test_and_set(volatile atomic_flag* __a)\n+  atomic_flag_test_and_set(volatile atomic_flag* __a) noexcept\n   { return atomic_flag_test_and_set_explicit(__a, memory_order_seq_cst); }\n \n   inline void\n-  atomic_flag_clear(atomic_flag* __a)\n+  atomic_flag_clear(atomic_flag* __a) noexcept\n   { atomic_flag_clear_explicit(__a, memory_order_seq_cst); }\n \n   inline void\n-  atomic_flag_clear(volatile atomic_flag* __a)\n+  atomic_flag_clear(volatile atomic_flag* __a) noexcept\n   { atomic_flag_clear_explicit(__a, memory_order_seq_cst); }\n \n \n   // Function templates generally applicable to atomic types.\n   template<typename _ITp>\n     inline bool\n-    atomic_is_lock_free(const atomic<_ITp>* __a)\n+    atomic_is_lock_free(const atomic<_ITp>* __a) noexcept\n     { return __a->is_lock_free(); }\n \n   template<typename _ITp>\n     inline bool\n-    atomic_is_lock_free(const volatile atomic<_ITp>* __a)\n+    atomic_is_lock_free(const volatile atomic<_ITp>* __a) noexcept\n     { return __a->is_lock_free(); }\n \n   template<typename _ITp>\n     inline void\n-    atomic_init(atomic<_ITp>* __a, _ITp __i);\n+    atomic_init(atomic<_ITp>* __a, _ITp __i) noexcept;\n \n   template<typename _ITp>\n     inline void\n-    atomic_init(volatile atomic<_ITp>* __a, _ITp __i);\n+    atomic_init(volatile atomic<_ITp>* __a, _ITp __i) noexcept;\n \n   template<typename _ITp>\n     inline void\n-    atomic_store_explicit(atomic<_ITp>* __a, _ITp __i, memory_order __m)\n+    atomic_store_explicit(atomic<_ITp>* __a, _ITp __i,\n+\t\t\t  memory_order __m) noexcept\n     { __a->store(__i, __m); }\n \n   template<typename _ITp>\n     inline void\n     atomic_store_explicit(volatile atomic<_ITp>* __a, _ITp __i,\n-\t\t\t  memory_order __m)\n+\t\t\t  memory_order __m) noexcept\n     { __a->store(__i, __m); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_load_explicit(const atomic<_ITp>* __a, memory_order __m)\n+    atomic_load_explicit(const atomic<_ITp>* __a, memory_order __m) noexcept\n     { return __a->load(__m); }\n \n   template<typename _ITp>\n     inline _ITp\n     atomic_load_explicit(const volatile atomic<_ITp>* __a,\n-\t\t\t memory_order __m)\n+\t\t\t memory_order __m) noexcept\n     { return __a->load(__m); }\n \n   template<typename _ITp>\n     inline _ITp\n     atomic_exchange_explicit(atomic<_ITp>* __a, _ITp __i,\n-\t\t\t     memory_order __m)\n+\t\t\t     memory_order __m) noexcept\n     { return __a->exchange(__i, __m); }\n \n   template<typename _ITp>\n     inline _ITp\n     atomic_exchange_explicit(volatile atomic<_ITp>* __a, _ITp __i,\n-\t\t\t     memory_order __m)\n+\t\t\t     memory_order __m) noexcept\n     { return __a->exchange(__i, __m); }\n \n   template<typename _ITp>\n     inline bool\n     atomic_compare_exchange_weak_explicit(atomic<_ITp>* __a,\n \t\t\t\t\t  _ITp* __i1, _ITp __i2,\n-\t\t\t\t\t  memory_order __m1, memory_order __m2)\n+\t\t\t\t\t  memory_order __m1,\n+\t\t\t\t\t  memory_order __m2) noexcept\n     { return __a->compare_exchange_weak(*__i1, __i2, __m1, __m2); }\n \n   template<typename _ITp>\n     inline bool\n     atomic_compare_exchange_weak_explicit(volatile atomic<_ITp>* __a,\n \t\t\t\t\t  _ITp* __i1, _ITp __i2,\n-\t\t\t\t\t  memory_order __m1, memory_order __m2)\n+\t\t\t\t\t  memory_order __m1,\n+\t\t\t\t\t  memory_order __m2) noexcept\n     { return __a->compare_exchange_weak(*__i1, __i2, __m1, __m2); }\n \n   template<typename _ITp>\n     inline bool\n     atomic_compare_exchange_strong_explicit(atomic<_ITp>* __a,\n \t\t\t\t\t    _ITp* __i1, _ITp __i2,\n \t\t\t\t\t    memory_order __m1,\n-\t\t\t\t\t    memory_order __m2)\n+\t\t\t\t\t    memory_order __m2) noexcept\n     { return __a->compare_exchange_strong(*__i1, __i2, __m1, __m2); }\n \n   template<typename _ITp>\n     inline bool\n     atomic_compare_exchange_strong_explicit(volatile atomic<_ITp>* __a,\n \t\t\t\t\t    _ITp* __i1, _ITp __i2,\n \t\t\t\t\t    memory_order __m1,\n-\t\t\t\t\t    memory_order __m2)\n+\t\t\t\t\t    memory_order __m2) noexcept\n     { return __a->compare_exchange_strong(*__i1, __i2, __m1, __m2); }\n \n \n   template<typename _ITp>\n     inline void\n-    atomic_store(atomic<_ITp>* __a, _ITp __i)\n+    atomic_store(atomic<_ITp>* __a, _ITp __i) noexcept\n     { atomic_store_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline void\n-    atomic_store(volatile atomic<_ITp>* __a, _ITp __i)\n+    atomic_store(volatile atomic<_ITp>* __a, _ITp __i) noexcept\n     { atomic_store_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_load(const atomic<_ITp>* __a)\n+    atomic_load(const atomic<_ITp>* __a) noexcept\n     { return atomic_load_explicit(__a, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_load(const volatile atomic<_ITp>* __a)\n+    atomic_load(const volatile atomic<_ITp>* __a) noexcept\n     { return atomic_load_explicit(__a, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_exchange(atomic<_ITp>* __a, _ITp __i)\n+    atomic_exchange(atomic<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_exchange_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_exchange(volatile atomic<_ITp>* __a, _ITp __i)\n+    atomic_exchange(volatile atomic<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_exchange_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline bool\n     atomic_compare_exchange_weak(atomic<_ITp>* __a,\n-\t\t\t\t _ITp* __i1, _ITp __i2)\n+\t\t\t\t _ITp* __i1, _ITp __i2) noexcept\n     {\n       return atomic_compare_exchange_weak_explicit(__a, __i1, __i2,\n \t\t\t\t\t\t   memory_order_seq_cst,\n@@ -860,7 +876,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n   template<typename _ITp>\n     inline bool\n     atomic_compare_exchange_weak(volatile atomic<_ITp>* __a,\n-\t\t\t\t _ITp* __i1, _ITp __i2)\n+\t\t\t\t _ITp* __i1, _ITp __i2) noexcept\n     {\n       return atomic_compare_exchange_weak_explicit(__a, __i1, __i2,\n \t\t\t\t\t\t   memory_order_seq_cst,\n@@ -870,7 +886,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n   template<typename _ITp>\n     inline bool\n     atomic_compare_exchange_strong(atomic<_ITp>* __a,\n-\t\t\t\t   _ITp* __i1, _ITp __i2)\n+\t\t\t\t   _ITp* __i1, _ITp __i2) noexcept\n     {\n       return atomic_compare_exchange_strong_explicit(__a, __i1, __i2,\n \t\t\t\t\t\t     memory_order_seq_cst,\n@@ -880,7 +896,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n   template<typename _ITp>\n     inline bool\n     atomic_compare_exchange_strong(volatile atomic<_ITp>* __a,\n-\t\t\t\t   _ITp* __i1, _ITp __i2)\n+\t\t\t\t   _ITp* __i1, _ITp __i2) noexcept\n     {\n       return atomic_compare_exchange_strong_explicit(__a, __i1, __i2,\n \t\t\t\t\t\t     memory_order_seq_cst,\n@@ -894,157 +910,157 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n   template<typename _ITp>\n     inline _ITp\n     atomic_fetch_add_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n-\t\t\t      memory_order __m)\n+\t\t\t      memory_order __m) noexcept\n     { return __a->fetch_add(__i, __m); }\n \n   template<typename _ITp>\n     inline _ITp\n     atomic_fetch_add_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n-\t\t\t      memory_order __m)\n+\t\t\t      memory_order __m) noexcept\n     { return __a->fetch_add(__i, __m); }\n \n   template<typename _ITp>\n     inline _ITp\n     atomic_fetch_sub_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n-\t\t\t      memory_order __m)\n+\t\t\t      memory_order __m) noexcept\n     { return __a->fetch_sub(__i, __m); }\n \n   template<typename _ITp>\n     inline _ITp\n     atomic_fetch_sub_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n-\t\t\t      memory_order __m)\n+\t\t\t      memory_order __m) noexcept\n     { return __a->fetch_sub(__i, __m); }\n \n   template<typename _ITp>\n     inline _ITp\n     atomic_fetch_and_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n-\t\t\t      memory_order __m)\n+\t\t\t      memory_order __m) noexcept\n     { return __a->fetch_and(__i, __m); }\n \n   template<typename _ITp>\n     inline _ITp\n     atomic_fetch_and_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n-\t\t\t      memory_order __m)\n+\t\t\t      memory_order __m) noexcept\n     { return __a->fetch_and(__i, __m); }\n \n   template<typename _ITp>\n     inline _ITp\n     atomic_fetch_or_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n-\t\t\t     memory_order __m)\n+\t\t\t     memory_order __m) noexcept\n     { return __a->fetch_or(__i, __m); }\n \n   template<typename _ITp>\n     inline _ITp\n     atomic_fetch_or_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n-\t\t\t     memory_order __m)\n+\t\t\t     memory_order __m) noexcept\n     { return __a->fetch_or(__i, __m); }\n \n   template<typename _ITp>\n     inline _ITp\n     atomic_fetch_xor_explicit(__atomic_base<_ITp>* __a, _ITp __i,\n-\t\t\t      memory_order __m)\n+\t\t\t      memory_order __m) noexcept\n     { return __a->fetch_xor(__i, __m); }\n \n   template<typename _ITp>\n     inline _ITp\n     atomic_fetch_xor_explicit(volatile __atomic_base<_ITp>* __a, _ITp __i,\n-\t\t\t      memory_order __m)\n+\t\t\t      memory_order __m) noexcept\n     { return __a->fetch_xor(__i, __m); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_fetch_add(__atomic_base<_ITp>* __a, _ITp __i)\n+    atomic_fetch_add(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_add_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_fetch_add(volatile __atomic_base<_ITp>* __a, _ITp __i)\n+    atomic_fetch_add(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_add_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_fetch_sub(__atomic_base<_ITp>* __a, _ITp __i)\n+    atomic_fetch_sub(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_sub_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_fetch_sub(volatile __atomic_base<_ITp>* __a, _ITp __i)\n+    atomic_fetch_sub(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_sub_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_fetch_and(__atomic_base<_ITp>* __a, _ITp __i)\n+    atomic_fetch_and(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_and_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_fetch_and(volatile __atomic_base<_ITp>* __a, _ITp __i)\n+    atomic_fetch_and(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_and_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_fetch_or(__atomic_base<_ITp>* __a, _ITp __i)\n+    atomic_fetch_or(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_or_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_fetch_or(volatile __atomic_base<_ITp>* __a, _ITp __i)\n+    atomic_fetch_or(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_or_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_fetch_xor(__atomic_base<_ITp>* __a, _ITp __i)\n+    atomic_fetch_xor(__atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_xor_explicit(__a, __i, memory_order_seq_cst); }\n \n   template<typename _ITp>\n     inline _ITp\n-    atomic_fetch_xor(volatile __atomic_base<_ITp>* __a, _ITp __i)\n+    atomic_fetch_xor(volatile __atomic_base<_ITp>* __a, _ITp __i) noexcept\n     { return atomic_fetch_xor_explicit(__a, __i, memory_order_seq_cst); }\n \n \n   // Partial specializations for pointers.\n   template<typename _ITp>\n     inline _ITp*\n     atomic_fetch_add_explicit(atomic<_ITp*>* __a, ptrdiff_t __d,\n-\t\t\t      memory_order __m)\n+\t\t\t      memory_order __m) noexcept\n     { return __a->fetch_add(__d, __m); }\n \n   template<typename _ITp>\n     inline _ITp*\n     atomic_fetch_add_explicit(volatile atomic<_ITp*>* __a, ptrdiff_t __d,\n-\t\t\t      memory_order __m)\n+\t\t\t      memory_order __m) noexcept\n     { return __a->fetch_add(__d, __m); }\n \n   template<typename _ITp>\n     inline _ITp*\n-    atomic_fetch_add(volatile atomic<_ITp*>* __a, ptrdiff_t __d)\n+    atomic_fetch_add(volatile atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n     { return __a->fetch_add(__d); }\n \n   template<typename _ITp>\n     inline _ITp*\n-    atomic_fetch_add(atomic<_ITp*>* __a, ptrdiff_t __d)\n+    atomic_fetch_add(atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n     { return __a->fetch_add(__d); }\n \n   template<typename _ITp>\n     inline _ITp*\n     atomic_fetch_sub_explicit(volatile atomic<_ITp*>* __a,\n-\t\t\t      ptrdiff_t __d, memory_order __m)\n+\t\t\t      ptrdiff_t __d, memory_order __m) noexcept\n     { return __a->fetch_sub(__d, __m); }\n \n   template<typename _ITp>\n     inline _ITp*\n     atomic_fetch_sub_explicit(atomic<_ITp*>* __a, ptrdiff_t __d,\n-\t\t\t      memory_order __m)\n+\t\t\t      memory_order __m) noexcept\n     { return __a->fetch_sub(__d, __m); }\n \n   template<typename _ITp>\n     inline _ITp*\n-    atomic_fetch_sub(volatile atomic<_ITp*>* __a, ptrdiff_t __d)\n+    atomic_fetch_sub(volatile atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n     { return __a->fetch_sub(__d); }\n \n   template<typename _ITp>\n     inline _ITp*\n-    atomic_fetch_sub(atomic<_ITp*>* __a, ptrdiff_t __d)\n+    atomic_fetch_sub(atomic<_ITp*>* __a, ptrdiff_t __d) noexcept\n     { return __a->fetch_sub(__d); }\n   // @} group atomics\n "}, {"sha": "5752d39feedcc8902bf40809c8ac8f8ea7d20d2c", "filename": "libstdc++-v3/src/atomic.cc", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bdc05efbc844f9878e36b0e5427af4806502709c/libstdc%2B%2B-v3%2Fsrc%2Fatomic.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bdc05efbc844f9878e36b0e5427af4806502709c/libstdc%2B%2B-v3%2Fsrc%2Fatomic.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Fsrc%2Fatomic.cc?ref=bdc05efbc844f9878e36b0e5427af4806502709c", "patch": "@@ -1,6 +1,6 @@\n // Support for atomic operations -*- C++ -*-\n \n-// Copyright (C) 2008, 2009, 2010\n+// Copyright (C) 2008, 2009, 2010, 2011\n // Free Software Foundation, Inc.\n //\n // This file is part of the GNU ISO C++ Library.  This library is free\n@@ -56,7 +56,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n   namespace __atomic0\n   {\n     bool\n-    atomic_flag::test_and_set(memory_order)\n+    atomic_flag::test_and_set(memory_order) noexcept\n     {\n #if defined(_GLIBCXX_HAS_GTHREADS) && defined(_GLIBCXX_USE_C99_STDINT_TR1)\n       lock_guard<mutex> __lock(get_atomic_mutex());\n@@ -67,7 +67,7 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n     }\n \n     void\n-    atomic_flag::clear(memory_order)\n+    atomic_flag::clear(memory_order) noexcept\n     {\n #if defined(_GLIBCXX_HAS_GTHREADS) && defined(_GLIBCXX_USE_C99_STDINT_TR1)\n       lock_guard<mutex> __lock(get_atomic_mutex());"}]}