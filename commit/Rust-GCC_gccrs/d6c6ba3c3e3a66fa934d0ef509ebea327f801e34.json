{"sha": "d6c6ba3c3e3a66fa934d0ef509ebea327f801e34", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZDZjNmJhM2MzZTNhNjZmYTkzNGQwZWY1MDllYmVhMzI3ZjgwMWUzNA==", "commit": {"author": {"name": "Jan Hubicka", "email": "jh@suse.cz", "date": "2013-10-21T11:02:08Z"}, "committer": {"name": "Jan Hubicka", "email": "hubicka@gcc.gnu.org", "date": "2013-10-21T11:02:08Z"}, "message": "* config/i386/i386-tune.def: Add comment; organize into categories\n\nFrom-SVN: r203888", "tree": {"sha": "0781f1ba2464799469eafdf480c7f22b0e875bd9", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/0781f1ba2464799469eafdf480c7f22b0e875bd9"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/d6c6ba3c3e3a66fa934d0ef509ebea327f801e34", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d6c6ba3c3e3a66fa934d0ef509ebea327f801e34", "html_url": "https://github.com/Rust-GCC/gccrs/commit/d6c6ba3c3e3a66fa934d0ef509ebea327f801e34", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d6c6ba3c3e3a66fa934d0ef509ebea327f801e34/comments", "author": null, "committer": null, "parents": [{"sha": "d606b917e1c986eee196ced3db2d3ab318b50430", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d606b917e1c986eee196ced3db2d3ab318b50430", "html_url": "https://github.com/Rust-GCC/gccrs/commit/d606b917e1c986eee196ced3db2d3ab318b50430"}], "stats": {"total": 613, "additions": 333, "deletions": 280}, "files": [{"sha": "bf54da96451ffe18996b81f63f31098ca59e6ec9", "filename": "gcc/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d6c6ba3c3e3a66fa934d0ef509ebea327f801e34/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d6c6ba3c3e3a66fa934d0ef509ebea327f801e34/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=d6c6ba3c3e3a66fa934d0ef509ebea327f801e34", "patch": "@@ -1,3 +1,7 @@\n+2013-10-20  Jan Hubicka  <jh@suse.cz>\n+\n+\t* config/i386/i386-tune.def: Add comment; organize into categories\n+\n 2013-10-21  Michael Zolotukhin  <michael.v.zolotukhin@gmail.com>\n \n \t* config/i386/i386.c (expand_set_or_movmem_via_loop): Add issetmem"}, {"sha": "a6e70f57231db53687f89df5481ac256f084e5fd", "filename": "gcc/config/i386/x86-tune.def", "status": "modified", "additions": 329, "deletions": 280, "changes": 609, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d6c6ba3c3e3a66fa934d0ef509ebea327f801e34/gcc%2Fconfig%2Fi386%2Fx86-tune.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d6c6ba3c3e3a66fa934d0ef509ebea327f801e34/gcc%2Fconfig%2Fi386%2Fx86-tune.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fx86-tune.def?ref=d6c6ba3c3e3a66fa934d0ef509ebea327f801e34", "patch": "@@ -18,61 +18,54 @@ a copy of the GCC Runtime Library Exception along with this program;\n see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n <http://www.gnu.org/licenses/>.  */\n \n-/* X86_TUNE_USE_LEAVE: Use \"leave\" instruction in epilogues where it fits.  */\n-DEF_TUNE (X86_TUNE_USE_LEAVE, \"use_leave\", \n-\t  m_386 | m_CORE_ALL | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC)\n-\n-/* X86_TUNE_PUSH_MEMORY: Enable generation of \"push mem\" instructions.\n-   Some chips, like 486 and Pentium works faster with separate load\n-   and push instructions.  */\n-DEF_TUNE (X86_TUNE_PUSH_MEMORY, \"push_memory\", \n-          m_386 | m_P4_NOCONA | m_CORE_ALL | m_K6_GEODE | m_AMD_MULTIPLE \n-          | m_GENERIC)\n-\n-/* X86_TUNE_ZERO_EXTEND_WITH_AND: Use AND instruction instead\n-   of mozbl/movwl.  */\n-DEF_TUNE (X86_TUNE_ZERO_EXTEND_WITH_AND, \"zero_extend_with_and\",  m_486 | m_PENT)\n-\n-/* X86_TUNE_UNROLL_STRLEN: Produce (quite lame) unrolled sequence for\n-   inline strlen.  This affects only -minline-all-stringops mode. By\n-   default we always dispatch to a library since our internal strlen\n-   is bad.  */\n-DEF_TUNE (X86_TUNE_UNROLL_STRLEN, \"unroll_strlen\",\n-\t m_486 | m_PENT | m_PPRO | m_ATOM | m_SLM | m_CORE_ALL | m_K6\n-\t | m_AMD_MULTIPLE | m_GENERIC)\n+/* Tuning for a given CPU XXXX consists of:\n+    - adding new CPU into:\n+\t- adding PROCESSOR_XXX to processor_type (in i386.h)\n+\t- possibly adding XXX into CPU attribute in i386.md\n+\t- adding XXX to processor_alias_table (in i386.c)\n+    - introducing ix86_XXX_cost in i386.c\n+\t- Stringop generation table can be build based on test_stringop\n+\t- script (once rest of tuning is complete)\n+    - designing a scheduler model in\n+\t- XXXX.md file\n+\t- Updating ix86_issue_rate and ix86_adjust_cost in i386.md\n+\t- possibly updating ia32_multipass_dfa_lookahead, ix86_sched_reorder\n+\t  and ix86_sched_init_global if those tricks are needed.\n+    - Tunning the flags bellow. Those are split into sections and each\n+      section is very roughly ordered by importance.  */\n+\n+/*****************************************************************************/\n+/* Scheduling flags. \t\t\t\t\t                     */\n+/*****************************************************************************/\n \n-/* X86_TUNE_BRANCH_PREDICTION_HINTS: Branch hints were put in P4 based\n-   on simulation result. But after P4 was made, no performance benefit\n-   was observed with branch hints.  It also increases the code size.\n-   As a result, icc never generates branch hints.  */\n-DEF_TUNE (X86_TUNE_BRANCH_PREDICTION_HINTS, \"branch_prediction_hints\", 0)\n-\n-/* X86_TUNE_DOUBLE_WITH_ADD: Use add instead of sal to double value in\n-   an integer register.  */\n-DEF_TUNE (X86_TUNE_DOUBLE_WITH_ADD, \"double_with_add\", ~m_386)\n-\n-/* X86_TUNE_USE_SAHF: Controls use of SAHF.  */\n-DEF_TUNE (X86_TUNE_USE_SAHF, \"use_sahf\",\n-          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_K6_GEODE\n-          | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC)\n+/* X86_TUNE_SCHEDULE: Enable scheduling.  */\n+DEF_TUNE (X86_TUNE_SCHEDULE, \"schedule\",\n+          m_PENT | m_PPRO | m_CORE_ALL | m_ATOM | m_SLM | m_K6_GEODE \n+          | m_AMD_MULTIPLE | m_GENERIC)\n \n-/* X86_TUNE_MOVX: Enable to zero extend integer registers to avoid\n-   partial dependencies.  */\n-DEF_TUNE (X86_TUNE_MOVX, \"movx\",\n-          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_GEODE \n-          | m_AMD_MULTIPLE  | m_GENERIC)\n+/* X86_TUNE_PARTIAL_REG_DEPENDENCY: Enable more register renaming\n+   on modern chips.  Preffer stores affecting whole integer register\n+   over partial stores.  For example preffer MOVZBL or MOVQ to load 8bit\n+   value over movb.  */\n+DEF_TUNE (X86_TUNE_PARTIAL_REG_DEPENDENCY, \"partial_reg_dependency\",\n+          m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_AMD_MULTIPLE \n+          | m_GENERIC)\n \n-/* X86_TUNE_PARTIAL_REG_STALL: Pentium pro, unlike later chips, handled\n-   use of partial registers by renaming.  This improved performance of 16bit\n-   code where upper halves of registers are not used.  It also leads to\n-   an penalty whenever a 16bit store is followed by 32bit use.  This flag\n-   disables production of such sequences in common cases.\n-   See also X86_TUNE_HIMODE_MATH.\n+/* X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY: This knob promotes all store\n+   destinations to be 128bit to allow register renaming on 128bit SSE units,\n+   but usually results in one extra microop on 64bit SSE units.\n+   Experimental results shows that disabling this option on P4 brings over 20%\n+   SPECfp regression, while enabling it on K8 brings roughly 2.4% regression\n+   that can be partly masked by careful scheduling of moves.  */\n+DEF_TUNE (X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY, \"sse_partial_reg_dependency\",\n+          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_AMDFAM10 \n+          | m_BDVER | m_GENERIC)\n \n-   In current implementation the partial register stalls are not eliminated\n-   very well - they can be introduced via subregs synthesized by combine\n-   and can happen in caller/callee saving sequences.  */\n-DEF_TUNE (X86_TUNE_PARTIAL_REG_STALL, \"partial_reg_stall\", m_PPRO)\n+/* X86_TUNE_SSE_SPLIT_REGS: Set for machines where the type and dependencies\n+   are resolved on SSE register parts instead of whole registers, so we may\n+   maintain just lower part of scalar values in proper format leaving the\n+   upper part undefined.  */\n+DEF_TUNE (X86_TUNE_SSE_SPLIT_REGS, \"sse_split_regs\", m_ATHLON_K8)\n \n /* X86_TUNE_PARTIAL_FLAG_REG_STALL: this flag disables use of of flags\n    set by instructions affecting just some flags (in particular shifts).\n@@ -88,74 +81,81 @@ DEF_TUNE (X86_TUNE_PARTIAL_REG_STALL, \"partial_reg_stall\", m_PPRO)\n DEF_TUNE (X86_TUNE_PARTIAL_FLAG_REG_STALL, \"partial_flag_reg_stall\",\n           m_CORE2 | m_GENERIC)\n \n-/* X86_TUNE_LCP_STALL: Avoid an expensive length-changing prefix stall\n-   on 16-bit immediate moves into memory on Core2 and Corei7.  */\n-DEF_TUNE (X86_TUNE_LCP_STALL, \"lcp_stall\", m_CORE_ALL | m_GENERIC)\n-\n-/* X86_TUNE_USE_HIMODE_FIOP: Enables use of x87 instructions with 16bit\n-   integer operand.\n-   FIXME: Why this is disabled for modern chips?  */\n-DEF_TUNE (X86_TUNE_USE_HIMODE_FIOP, \"use_himode_fiop\", \n-          m_386 | m_486 | m_K6_GEODE)\n-\n-/* X86_TUNE_USE_SIMODE_FIOP: Enables use of x87 instructions with 32bit\n-   integer operand.  */\n-DEF_TUNE (X86_TUNE_USE_SIMODE_FIOP, \"use_simode_fiop\",\n-          ~(m_PENT | m_PPRO | m_CORE_ALL | m_ATOM \n-            | m_SLM | m_AMD_MULTIPLE | m_GENERIC))\n+/* X86_TUNE_MOVX: Enable to zero extend integer registers to avoid\n+   partial dependencies.  */\n+DEF_TUNE (X86_TUNE_MOVX, \"movx\",\n+          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_GEODE \n+          | m_AMD_MULTIPLE  | m_GENERIC)\n \n-/* X86_TUNE_USE_MOV0: Use \"mov $0, reg\" instead of \"xor reg, reg\" to clear\n-   integer register.  */\n-DEF_TUNE (X86_TUNE_USE_MOV0, \"use_mov0\", m_K6)\n+/* X86_TUNE_MEMORY_MISMATCH_STALL: Avoid partial stores that are followed by\n+   full sized loads.  */\n+DEF_TUNE (X86_TUNE_MEMORY_MISMATCH_STALL, \"memory_mismatch_stall\",\n+          m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_AMD_MULTIPLE | m_GENERIC)\n \n-/* X86_TUNE_USE_CLTD: Controls use of CLTD and CTQO instructions.  */\n-DEF_TUNE (X86_TUNE_USE_CLTD, \"use_cltd\", ~(m_PENT | m_ATOM | m_SLM | m_K6))\n+/* X86_TUNE_FUSE_CMP_AND_BRANCH: Fuse a compare or test instruction\n+   with a subsequent conditional jump instruction into a single\n+   compare-and-branch uop.\n+   FIXME: revisit for generic.  */\n+DEF_TUNE (X86_TUNE_FUSE_CMP_AND_BRANCH, \"fuse_cmp_and_branch\", m_BDVER | m_CORE_ALL)\n \n-/* X86_TUNE_USE_XCHGB: Use xchgb %rh,%rl instead of rolw/rorw $8,rx.  */\n-DEF_TUNE (X86_TUNE_USE_XCHGB, \"use_xchgb\", m_PENT4)\n+/* X86_TUNE_REASSOC_INT_TO_PARALLEL: Try to produce parallel computations\n+   during reassociation of integer computation.  */\n+DEF_TUNE (X86_TUNE_REASSOC_INT_TO_PARALLEL, \"reassoc_int_to_parallel\",\n+          m_ATOM)\n \n-/* X86_TUNE_SPLIT_LONG_MOVES: Avoid instructions moving immediates\n-   directly to memory.  */\n-DEF_TUNE (X86_TUNE_SPLIT_LONG_MOVES, \"split_long_moves\", m_PPRO)\n+/* X86_TUNE_REASSOC_FP_TO_PARALLEL: Try to produce parallel computations\n+   during reassociation of fp computation.  */\n+DEF_TUNE (X86_TUNE_REASSOC_FP_TO_PARALLEL, \"reassoc_fp_to_parallel\",\n+          m_ATOM | m_SLM | m_HASWELL | m_BDVER1 | m_BDVER2 | m_GENERIC)\n \n-/* X86_TUNE_READ_MODIFY_WRITE: Enable use of read modify write instructions\n-   such as \"add $1, mem\".  */\n-DEF_TUNE (X86_TUNE_READ_MODIFY_WRITE, \"read_modify_write\", ~m_PENT)\n+/*****************************************************************************/\n+/* Function prologue, epilogue and function calling sequences.               */\n+/*****************************************************************************/\n \n-/* X86_TUNE_READ_MODIFY: Enable use of read-modify instructions such\n-   as \"add mem, reg\".  */\n-DEF_TUNE (X86_TUNE_READ_MODIFY, \"read_modify\", ~(m_PENT | m_PPRO))\n+/* X86_TUNE_ACCUMULATE_OUTGOING_ARGS: Allocate stack space for outgoing\n+   arguments in prologue/epilogue instead of separately for each call\n+   by push/pop instructions.\n+   This increase code size by about 5% in 32bit mode, less so in 64bit mode\n+   because parameters are passed in registers.  It is considerable\n+   win for targets without stack engine that prevents multple push operations\n+   to happen in parallel.\n \n-/* X86_TUNE_PROMOTE_QIMODE: When it is cheap, turn 8bit arithmetic to\n-   corresponding 32bit arithmetic.  */\n-DEF_TUNE (X86_TUNE_PROMOTE_QIMODE, \"promote_qimode\",\n-          m_386 | m_486 | m_PENT | m_CORE_ALL | m_ATOM | m_SLM \n-          | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC)\n+   FIXME: the flags is incorrectly enabled for amdfam10, Bulldozer,\n+   Bobcat and Generic.  This is because disabling it causes large\n+   regression on mgrid due to IRA limitation leading to unecessary\n+   use of the frame pointer in 32bit mode.  */\n+DEF_TUNE (X86_TUNE_ACCUMULATE_OUTGOING_ARGS, \"accumulate_outgoing_args\", \n+\t  m_PPRO | m_P4_NOCONA | m_ATOM | m_SLM | m_AMD_MULTIPLE | m_GENERIC)\n \n-/* X86_TUNE_FAST_PREFIX: Enable demoting some 32bit or 64bit arithmetic\n-   into 16bit/8bit when resulting sequence is shorter.  For example\n-   for \"and $-65536, reg\" to 16bit store of 0.  */\n-DEF_TUNE (X86_TUNE_FAST_PREFIX, \"fast_prefix\", ~(m_386 | m_486 | m_PENT))\n+/* X86_TUNE_PROLOGUE_USING_MOVE: Do not use push/pop in prologues that are\n+   considered on critical path.  */\n+DEF_TUNE (X86_TUNE_PROLOGUE_USING_MOVE, \"prologue_using_move\", \n+          m_PPRO | m_ATHLON_K8)\n \n-/* X86_TUNE_SINGLE_STRINGOP: Enable use of single string operations, such\n-   as MOVS and STOS (without a REP prefix) to move/set sequences of bytes.  */\n-DEF_TUNE (X86_TUNE_SINGLE_STRINGOP, \"single_stringop\", m_386 | m_P4_NOCONA)\n+/* X86_TUNE_PROLOGUE_USING_MOVE: Do not use push/pop in epilogues that are\n+   considered on critical path.  */\n+DEF_TUNE (X86_TUNE_EPILOGUE_USING_MOVE, \"epilogue_using_move\",\n+          m_PPRO | m_ATHLON_K8)\t\n \n-/* X86_TUNE_QIMODE_MATH: Enable use of 8bit arithmetic.  */\n-DEF_TUNE (X86_TUNE_QIMODE_MATH, \"qimode_math\", ~0)\n+/* X86_TUNE_USE_LEAVE: Use \"leave\" instruction in epilogues where it fits.  */\n+DEF_TUNE (X86_TUNE_USE_LEAVE, \"use_leave\", \n+\t  m_386 | m_CORE_ALL | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC)\n \n-/* X86_TUNE_HIMODE_MATH: Enable use of 16bit arithmetic.\n-   On PPro this flag is meant to avoid partial register stalls.  */\n-DEF_TUNE (X86_TUNE_HIMODE_MATH, \"himode_math\", ~m_PPRO)\n+/* X86_TUNE_PUSH_MEMORY: Enable generation of \"push mem\" instructions.\n+   Some chips, like 486 and Pentium works faster with separate load\n+   and push instructions.  */\n+DEF_TUNE (X86_TUNE_PUSH_MEMORY, \"push_memory\", \n+          m_386 | m_P4_NOCONA | m_CORE_ALL | m_K6_GEODE | m_AMD_MULTIPLE \n+          | m_GENERIC)\n \n-/* X86_TUNE_PROMOTE_QI_REGS: This enables generic code that promotes all 8bit\n-   arithmetic to 32bit via PROMOTE_MODE macro.  This code generation scheme\n-   is usually used for RISC targets.  */\n-DEF_TUNE (X86_TUNE_PROMOTE_QI_REGS, \"promote_qi_regs\", 0)\n+/* X86_TUNE_SINGLE_PUSH: Enable if single push insn is preferred\n+   over esp subtraction.  */\n+DEF_TUNE (X86_TUNE_SINGLE_PUSH, \"single_push\", m_386 | m_486 | m_PENT \n+          | m_K6_GEODE)\n \n-/* X86_TUNE_PROMOTE_HI_REGS: Same, but for 16bit artihmetic.  Again we avoid\n-   partial register stalls on PentiumPro targets. */\n-DEF_TUNE (X86_TUNE_PROMOTE_HI_REGS, \"promote_hi_regs\", m_PPRO)\n+/* X86_TUNE_DOUBLE_PUSH. Enable if double push insn is preferred\n+   over esp subtraction.  */\n+DEF_TUNE (X86_TUNE_DOUBLE_PUSH, \"double_push\", m_PENT | m_K6_GEODE)\n \n /* X86_TUNE_SINGLE_POP: Enable if single pop insn is preferred\n    over esp addition.  */\n@@ -165,38 +165,128 @@ DEF_TUNE (X86_TUNE_SINGLE_POP, \"single_pop\", m_386 | m_486 | m_PENT | m_PPRO)\n    over esp addition.  */\n DEF_TUNE (X86_TUNE_DOUBLE_POP, \"double_pop\", m_PENT)\n \n-/* X86_TUNE_SINGLE_PUSH: Enable if single push insn is preferred\n-   over esp subtraction.  */\n-DEF_TUNE (X86_TUNE_SINGLE_PUSH, \"single_push\", m_386 | m_486 | m_PENT \n-          | m_K6_GEODE)\n+/*****************************************************************************/\n+/* Branch predictor tuning  \t\t                                     */\n+/*****************************************************************************/\n \n-/* X86_TUNE_DOUBLE_PUSH. Enable if double push insn is preferred\n-   over esp subtraction.  */\n-DEF_TUNE (X86_TUNE_DOUBLE_PUSH, \"double_push\", m_PENT | m_K6_GEODE)\n+/* X86_TUNE_PAD_SHORT_FUNCTION: Make every function to be at least 4\n+   instructions long.  */\n+DEF_TUNE (X86_TUNE_PAD_SHORT_FUNCTION, \"pad_short_function\", m_ATOM)\n+\n+/* X86_TUNE_PAD_RETURNS: Place NOP before every RET that is a destination\n+   of conditional jump or directly preceded by other jump instruction.\n+   This is important for AND K8-AMDFAM10 because the branch prediction\n+   architecture expect at most one jump per 2 byte window.  Failing to\n+   pad returns leads to misaligned return stack.  */\n+DEF_TUNE (X86_TUNE_PAD_RETURNS, \"pad_returns\",\n+          m_ATHLON_K8 | m_AMDFAM10 | m_GENERIC)\n+\n+/* X86_TUNE_FOUR_JUMP_LIMIT: Some CPU cores are not able to predict more\n+   than 4 branch instructions in the 16 byte window.  */\n+DEF_TUNE (X86_TUNE_FOUR_JUMP_LIMIT, \"four_jump_limit\",\n+          m_PPRO | m_P4_NOCONA | m_ATOM | m_SLM | m_ATHLON_K8 | m_AMDFAM10)\n+\n+/*****************************************************************************/\n+/* Integer instruction selection tuning                                      */\n+/*****************************************************************************/\n+\n+/* X86_TUNE_SOFTWARE_PREFETCHING_BENEFICIAL: Enable software prefetching\n+   at -O3.  For the moment, the prefetching seems badly tuned for Intel\n+   chips.  */\n+DEF_TUNE (X86_TUNE_SOFTWARE_PREFETCHING_BENEFICIAL, \"software_prefetching_beneficial\",\n+          m_K6_GEODE | m_AMD_MULTIPLE)\n+\n+/* X86_TUNE_LCP_STALL: Avoid an expensive length-changing prefix stall\n+   on 16-bit immediate moves into memory on Core2 and Corei7.  */\n+DEF_TUNE (X86_TUNE_LCP_STALL, \"lcp_stall\", m_CORE_ALL | m_GENERIC)\n+\n+/* X86_TUNE_READ_MODIFY: Enable use of read-modify instructions such\n+   as \"add mem, reg\".  */\n+DEF_TUNE (X86_TUNE_READ_MODIFY, \"read_modify\", ~(m_PENT | m_PPRO))\n+\n+/* X86_TUNE_USE_INCDEC: Enable use of inc/dec instructions.   */\n+DEF_TUNE (X86_TUNE_USE_INCDEC, \"use_incdec\",\n+          ~(m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_GENERIC))\n \n /* X86_TUNE_INTEGER_DFMODE_MOVES: Enable if integer moves are preferred\n    for DFmode copies */\n DEF_TUNE (X86_TUNE_INTEGER_DFMODE_MOVES, \"integer_dfmode_moves\",\n           ~(m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM \n           | m_GEODE | m_AMD_MULTIPLE | m_GENERIC))\n \n-/* X86_TUNE_PARTIAL_REG_DEPENDENCY: Enable more register renaming\n-   on modern chips.  Preffer stores affecting whole integer register\n-   over partial stores.  For example preffer MOVZBL or MOVQ to load 8bit\n-   value over movb.  */\n-DEF_TUNE (X86_TUNE_PARTIAL_REG_DEPENDENCY, \"partial_reg_dependency\",\n-          m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_AMD_MULTIPLE \n-          | m_GENERIC)\n+/* X86_TUNE_OPT_AGU: Optimize for Address Generation Unit. This flag\n+   will impact LEA instruction selection. */\n+DEF_TUNE (X86_TUNE_OPT_AGU, \"opt_agu\", m_ATOM | m_SLM)\n \n-/* X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY: This knob promotes all store\n-   destinations to be 128bit to allow register renaming on 128bit SSE units,\n-   but usually results in one extra microop on 64bit SSE units.\n-   Experimental results shows that disabling this option on P4 brings over 20%\n-   SPECfp regression, while enabling it on K8 brings roughly 2.4% regression\n-   that can be partly masked by careful scheduling of moves.  */\n-DEF_TUNE (X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY, \"sse_partial_reg_dependency\",\n-          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_AMDFAM10 \n-          | m_BDVER | m_GENERIC)\n+/* X86_TUNE_SLOW_IMUL_IMM32_MEM: Imul of 32-bit constant and memory is\n+   vector path on AMD machines. \n+   FIXME: Do we need to enable this for core? */\n+DEF_TUNE (X86_TUNE_SLOW_IMUL_IMM32_MEM, \"slow_imul_imm32_mem\",\n+          m_K8 | m_AMDFAM10)\n+\n+/* X86_TUNE_SLOW_IMUL_IMM8: Imul of 8-bit constant is vector path on AMD\n+   machines. \n+   FIXME: Do we need to enable this for core? */\n+DEF_TUNE (X86_TUNE_SLOW_IMUL_IMM8, \"slow_imul_imm8\",\n+          m_K8 | m_AMDFAM10)\n+\n+/* X86_TUNE_AVOID_MEM_OPND_FOR_CMOVE: Try to avoid memory operands for\n+   a conditional move.  */\n+DEF_TUNE (X86_TUNE_AVOID_MEM_OPND_FOR_CMOVE, \"avoid_mem_opnd_for_cmove\",\n+\t  m_ATOM | m_SLM)\n+\n+/* X86_TUNE_SINGLE_STRINGOP: Enable use of single string operations, such\n+   as MOVS and STOS (without a REP prefix) to move/set sequences of bytes.  */\n+DEF_TUNE (X86_TUNE_SINGLE_STRINGOP, \"single_stringop\", m_386 | m_P4_NOCONA)\n+\n+/* X86_TUNE_USE_SAHF: Controls use of SAHF.  */\n+DEF_TUNE (X86_TUNE_USE_SAHF, \"use_sahf\",\n+          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_K6_GEODE\n+          | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC)\n+\n+/* X86_TUNE_USE_CLTD: Controls use of CLTD and CTQO instructions.  */\n+DEF_TUNE (X86_TUNE_USE_CLTD, \"use_cltd\", ~(m_PENT | m_ATOM | m_SLM | m_K6))\n+\n+/* X86_TUNE_USE_BT: Enable use of BT (bit test) instructions.  */\n+DEF_TUNE (X86_TUNE_USE_BT, \"use_bt\",\n+          m_CORE_ALL | m_ATOM | m_SLM | m_AMD_MULTIPLE | m_GENERIC)\n+\n+/*****************************************************************************/\n+/* 387 instruction selection tuning                                          */\n+/*****************************************************************************/\n+\n+/* X86_TUNE_USE_HIMODE_FIOP: Enables use of x87 instructions with 16bit\n+   integer operand.\n+   FIXME: Why this is disabled for modern chips?  */\n+DEF_TUNE (X86_TUNE_USE_HIMODE_FIOP, \"use_himode_fiop\", \n+          m_386 | m_486 | m_K6_GEODE)\n+\n+/* X86_TUNE_USE_SIMODE_FIOP: Enables use of x87 instructions with 32bit\n+   integer operand.  */\n+DEF_TUNE (X86_TUNE_USE_SIMODE_FIOP, \"use_simode_fiop\",\n+          ~(m_PENT | m_PPRO | m_CORE_ALL | m_ATOM \n+            | m_SLM | m_AMD_MULTIPLE | m_GENERIC))\n+\n+/* X86_TUNE_USE_FFREEP: Use freep instruction instead of fstp.  */\n+DEF_TUNE (X86_TUNE_USE_FFREEP, \"use_ffreep\", m_AMD_MULTIPLE)\n+\n+/* X86_TUNE_EXT_80387_CONSTANTS: Use fancy 80387 constants, such as PI.  */\n+DEF_TUNE (X86_TUNE_EXT_80387_CONSTANTS, \"ext_80387_constants\",\n+          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_K6_GEODE\n+          | m_ATHLON_K8 | m_GENERIC)\n+\n+/*****************************************************************************/\n+/* SSE instruction selection tuning                                          */\n+/*****************************************************************************/\n+\n+/* X86_TUNE_VECTORIZE_DOUBLE: Enable double precision vector\n+   instructions.  */\n+DEF_TUNE (X86_TUNE_VECTORIZE_DOUBLE, \"vectorize_double\", ~m_ATOM)\n+\n+/* X86_TUNE_GENERAL_REGS_SSE_SPILL: Try to spill general regs to SSE\n+   regs instead of memory.  */\n+DEF_TUNE (X86_TUNE_GENERAL_REGS_SSE_SPILL, \"general_regs_sse_spill\",\n+          m_CORE_ALL)\n \n /* X86_TUNE_SSE_UNALIGNED_LOAD_OPTIMAL: Use movups for misaligned loads instead\n    of a sequence loading registers by parts.  */\n@@ -208,27 +298,11 @@ DEF_TUNE (X86_TUNE_SSE_UNALIGNED_LOAD_OPTIMAL, \"sse_unaligned_load_optimal\",\n DEF_TUNE (X86_TUNE_SSE_UNALIGNED_STORE_OPTIMAL, \"sse_unaligned_store_optimal\",\n           m_COREI7 | m_BDVER | m_SLM | m_GENERIC)\n \n-/* X86_TUNE_AVX256_UNALIGNED_LOAD_OPTIMAL: if true, unaligned loads are\n-   split.  */\n-DEF_TUNE (X86_TUNE_AVX256_UNALIGNED_LOAD_OPTIMAL, \"256_unaligned_load_optimal\", \n-          ~(m_COREI7 | m_GENERIC))\n-\n-/* X86_TUNE_AVX256_UNALIGNED_STORE_OPTIMAL: if true, unaligned loads are\n-   split.  */\n-DEF_TUNE (X86_TUNE_AVX256_UNALIGNED_STORE_OPTIMAL, \"256_unaligned_load_optimal\", \n-          ~(m_COREI7 | m_BDVER | m_GENERIC))\n-\n /* Use packed single precision instructions where posisble.  I.e. movups instead\n    of movupd.  */\n DEF_TUNE (X86_TUNE_SSE_PACKED_SINGLE_INSN_OPTIMAL, \"sse_packed_single_insn_optimal\",\n           m_BDVER)\n \n-/* X86_TUNE_SSE_SPLIT_REGS: Set for machines where the type and dependencies\n-   are resolved on SSE register parts instead of whole registers, so we may\n-   maintain just lower part of scalar values in proper format leaving the\n-   upper part undefined.  */\n-DEF_TUNE (X86_TUNE_SSE_SPLIT_REGS, \"sse_split_regs\", m_ATHLON_K8)\n-\n /* X86_TUNE_SSE_TYPELESS_STORES: Always movaps/movups for 128bit stores.   */\n DEF_TUNE (X86_TUNE_SSE_TYPELESS_STORES, \"sse_typeless_stores\",\n \t  m_AMD_MULTIPLE | m_CORE_ALL | m_GENERIC)\n@@ -238,28 +312,6 @@ DEF_TUNE (X86_TUNE_SSE_TYPELESS_STORES, \"sse_typeless_stores\",\n DEF_TUNE (X86_TUNE_SSE_LOAD0_BY_PXOR, \"sse_load0_by_pxor\",\n \t  m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_BDVER | m_BTVER | m_GENERIC)\n \n-/* X86_TUNE_MEMORY_MISMATCH_STALL: Avoid partial stores that are followed by\n-   full sized loads.  */\n-DEF_TUNE (X86_TUNE_MEMORY_MISMATCH_STALL, \"memory_mismatch_stall\",\n-          m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_AMD_MULTIPLE | m_GENERIC)\n-\n-/* X86_TUNE_PROLOGUE_USING_MOVE: Do not use push/pop in prologues that are\n-   considered on critical path.  */\n-DEF_TUNE (X86_TUNE_PROLOGUE_USING_MOVE, \"prologue_using_move\", \n-          m_PPRO | m_ATHLON_K8)\n-\n-/* X86_TUNE_PROLOGUE_USING_MOVE: Do not use push/pop in epilogues that are\n-   considered on critical path.  */\n-DEF_TUNE (X86_TUNE_EPILOGUE_USING_MOVE, \"epilogue_using_move\",\n-          m_PPRO | m_ATHLON_K8)\t\n-\n-/* X86_TUNE_SHIFT1: Enables use of short encoding of \"sal reg\" instead of\n-   longer \"sal $1, reg\".  */\n-DEF_TUNE (X86_TUNE_SHIFT1, \"shift1\", ~m_486)\n-\n-/* X86_TUNE_USE_FFREEP: Use freep instruction instead of fstp.  */\n-DEF_TUNE (X86_TUNE_USE_FFREEP, \"use_ffreep\", m_AMD_MULTIPLE)\n-\n /* X86_TUNE_INTER_UNIT_MOVES_TO_VEC: Enable moves in from integer\n    to SSE registers.  If disabled, the moves will be done by storing\n    the value to memory and reloading.  */\n@@ -278,62 +330,80 @@ DEF_TUNE (X86_TUNE_INTER_UNIT_MOVES_FROM_VEC, \"inter_unit_moves_from_vec\",\n DEF_TUNE (X86_TUNE_INTER_UNIT_CONVERSIONS, \"inter_unit_conversions\",\n           ~(m_AMDFAM10 | m_BDVER))\n \n-/* X86_TUNE_FOUR_JUMP_LIMIT: Some CPU cores are not able to predict more\n-   than 4 branch instructions in the 16 byte window.  */\n-DEF_TUNE (X86_TUNE_FOUR_JUMP_LIMIT, \"four_jump_limit\",\n-          m_PPRO | m_P4_NOCONA | m_ATOM | m_SLM | m_ATHLON_K8 | m_AMDFAM10)\n+/* X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS: Try to split memory operand for\n+   fp converts to destination register.  */\n+DEF_TUNE (X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS, \"split_mem_opnd_for_fp_converts\",\n+          m_SLM)\n \n-/* X86_TUNE_SCHEDULE: Enable scheduling.  */\n-DEF_TUNE (X86_TUNE_SCHEDULE, \"schedule\",\n-          m_PENT | m_PPRO | m_CORE_ALL | m_ATOM | m_SLM | m_K6_GEODE \n-          | m_AMD_MULTIPLE | m_GENERIC)\n+/* X86_TUNE_USE_VECTOR_FP_CONVERTS: Prefer vector packed SSE conversion\n+   from FP to FP.  This form of instructions avoids partial write to the\n+   destination.  */\n+DEF_TUNE (X86_TUNE_USE_VECTOR_FP_CONVERTS, \"use_vector_fp_converts\",\n+          m_AMDFAM10)\n \n-/* X86_TUNE_USE_BT: Enable use of BT (bit test) instructions.  */\n-DEF_TUNE (X86_TUNE_USE_BT, \"use_bt\",\n-          m_CORE_ALL | m_ATOM | m_SLM | m_AMD_MULTIPLE | m_GENERIC)\n+/* X86_TUNE_USE_VECTOR_CONVERTS: Prefer vector packed SSE conversion\n+   from integer to FP. */\n+DEF_TUNE (X86_TUNE_USE_VECTOR_CONVERTS, \"use_vector_converts\", m_AMDFAM10)\n \n-/* X86_TUNE_USE_INCDEC: Enable use of inc/dec instructions.   */\n-DEF_TUNE (X86_TUNE_USE_INCDEC, \"use_incdec\",\n-          ~(m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_GENERIC))\n+/*****************************************************************************/\n+/* AVX instruction selection tuning (some of SSE flags affects AVX, too)     */\n+/*****************************************************************************/\n \n-/* X86_TUNE_PAD_RETURNS: Place NOP before every RET that is a destination\n-   of conditional jump or directly preceded by other jump instruction.\n-   This is important for AND K8-AMDFAM10 because the branch prediction\n-   architecture expect at most one jump per 2 byte window.  Failing to\n-   pad returns leads to misaligned return stack.  */\n-DEF_TUNE (X86_TUNE_PAD_RETURNS, \"pad_returns\",\n-          m_ATHLON_K8 | m_AMDFAM10 | m_GENERIC)\n+/* X86_TUNE_AVX256_UNALIGNED_LOAD_OPTIMAL: if true, unaligned loads are\n+   split.  */\n+DEF_TUNE (X86_TUNE_AVX256_UNALIGNED_LOAD_OPTIMAL, \"256_unaligned_load_optimal\", \n+          ~(m_COREI7 | m_GENERIC))\n \n-/* X86_TUNE_PAD_SHORT_FUNCTION: Make every function to be at least 4\n-   instructions long.  */\n-DEF_TUNE (X86_TUNE_PAD_SHORT_FUNCTION, \"pad_short_function\", m_ATOM)\n+/* X86_TUNE_AVX256_UNALIGNED_STORE_OPTIMAL: if true, unaligned loads are\n+   split.  */\n+DEF_TUNE (X86_TUNE_AVX256_UNALIGNED_STORE_OPTIMAL, \"256_unaligned_load_optimal\", \n+          ~(m_COREI7 | m_BDVER | m_GENERIC))\n \n-/* X86_TUNE_EXT_80387_CONSTANTS: Use fancy 80387 constants, such as PI.  */\n-DEF_TUNE (X86_TUNE_EXT_80387_CONSTANTS, \"ext_80387_constants\",\n-          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_ATOM | m_SLM | m_K6_GEODE\n-          | m_ATHLON_K8 | m_GENERIC)\n+/* X86_TUNE_AVX128_OPTIMAL: Enable 128-bit AVX instruction generation for\n+   the auto-vectorizer.  */\n+DEF_TUNE (X86_TUNE_AVX128_OPTIMAL, \"avx128_optimal\", m_BDVER | m_BTVER2)\n \n-/* X86_TUNE_AVOID_VECTOR_DECODE: Enable splitters that avoid vector decoded\n-   forms of instructions on K8 targets.  */\n-DEF_TUNE (X86_TUNE_AVOID_VECTOR_DECODE, \"avoid_vector_decode\",\n-          m_K8)\n+/*****************************************************************************/\n+/* Historical relics: tuning flags that helps a specific old CPU designs     */\n+/*****************************************************************************/\n+\n+/* X86_TUNE_DOUBLE_WITH_ADD: Use add instead of sal to double value in\n+   an integer register.  */\n+DEF_TUNE (X86_TUNE_DOUBLE_WITH_ADD, \"double_with_add\", ~m_386)\n+\n+/* X86_TUNE_ALWAYS_FANCY_MATH_387: controls use of fancy 387 operations,\n+   such as fsqrt, fprem, fsin, fcos, fsincos etc.\n+   Should be enabled for all targets that always has coprocesor.  */\n+DEF_TUNE (X86_TUNE_ALWAYS_FANCY_MATH_387, \"always_fancy_math_387\", \n+          ~(m_386 | m_486))\n+\n+/* X86_TUNE_UNROLL_STRLEN: Produce (quite lame) unrolled sequence for\n+   inline strlen.  This affects only -minline-all-stringops mode. By\n+   default we always dispatch to a library since our internal strlen\n+   is bad.  */\n+DEF_TUNE (X86_TUNE_UNROLL_STRLEN, \"unroll_strlen\", ~m_386)\n+\n+/* X86_TUNE_SHIFT1: Enables use of short encoding of \"sal reg\" instead of\n+   longer \"sal $1, reg\".  */\n+DEF_TUNE (X86_TUNE_SHIFT1, \"shift1\", ~m_486)\n+\n+/* X86_TUNE_ZERO_EXTEND_WITH_AND: Use AND instruction instead\n+   of mozbl/movwl.  */\n+DEF_TUNE (X86_TUNE_ZERO_EXTEND_WITH_AND, \"zero_extend_with_and\",  m_486 | m_PENT)\n \n /* X86_TUNE_PROMOTE_HIMODE_IMUL: Modern CPUs have same latency for HImode\n    and SImode multiply, but 386 and 486 do HImode multiply faster.  */\n DEF_TUNE (X86_TUNE_PROMOTE_HIMODE_IMUL, \"promote_himode_imul\",\n           ~(m_386 | m_486))\n \n-/* X86_TUNE_SLOW_IMUL_IMM32_MEM: Imul of 32-bit constant and memory is\n-   vector path on AMD machines. \n-   FIXME: Do we need to enable this for core? */\n-DEF_TUNE (X86_TUNE_SLOW_IMUL_IMM32_MEM, \"slow_imul_imm32_mem\",\n-          m_K8 | m_AMDFAM10)\n+/* X86_TUNE_FAST_PREFIX: Enable demoting some 32bit or 64bit arithmetic\n+   into 16bit/8bit when resulting sequence is shorter.  For example\n+   for \"and $-65536, reg\" to 16bit store of 0.  */\n+DEF_TUNE (X86_TUNE_FAST_PREFIX, \"fast_prefix\", ~(m_386 | m_486 | m_PENT))\n \n-/* X86_TUNE_SLOW_IMUL_IMM8: Imul of 8-bit constant is vector path on AMD\n-   machines. \n-   FIXME: Do we need to enable this for core? */\n-DEF_TUNE (X86_TUNE_SLOW_IMUL_IMM8, \"slow_imul_imm8\",\n-          m_K8 | m_AMDFAM10)\n+/* X86_TUNE_READ_MODIFY_WRITE: Enable use of read modify write instructions\n+   such as \"add $1, mem\".  */\n+DEF_TUNE (X86_TUNE_READ_MODIFY_WRITE, \"read_modify_write\", ~m_PENT)\n \n /* X86_TUNE_MOVE_M1_VIA_OR: On pentiums, it is faster to load -1 via OR\n    than a MOV.  */\n@@ -343,87 +413,66 @@ DEF_TUNE (X86_TUNE_MOVE_M1_VIA_OR, \"move_m1_via_or\", m_PENT)\n    but one byte longer.  */\n DEF_TUNE (X86_TUNE_NOT_UNPAIRABLE, \"not_unpairable\", m_PENT)\n \n-/* X86_TUNE_NOT_VECTORMODE: On AMD K6, NOT is vector decoded with memory\n-   operand that cannot be represented using a modRM byte.  The XOR\n-   replacement is long decoded, so this split helps here as well.  */\n-DEF_TUNE (X86_TUNE_NOT_VECTORMODE, \"not_vectormode\", m_K6)\n-\n-/* X86_TUNE_USE_VECTOR_FP_CONVERTS: Prefer vector packed SSE conversion\n-   from FP to FP.  This form of instructions avoids partial write to the\n-   destination.  */\n-DEF_TUNE (X86_TUNE_USE_VECTOR_FP_CONVERTS, \"use_vector_fp_converts\",\n-          m_AMDFAM10)\n-\n-/* X86_TUNE_USE_VECTOR_CONVERTS: Prefer vector packed SSE conversion\n-   from integer to FP. */\n-DEF_TUNE (X86_TUNE_USE_VECTOR_CONVERTS, \"use_vector_converts\", m_AMDFAM10)\n+/* X86_TUNE_PARTIAL_REG_STALL: Pentium pro, unlike later chips, handled\n+   use of partial registers by renaming.  This improved performance of 16bit\n+   code where upper halves of registers are not used.  It also leads to\n+   an penalty whenever a 16bit store is followed by 32bit use.  This flag\n+   disables production of such sequences in common cases.\n+   See also X86_TUNE_HIMODE_MATH.\n \n-/* X86_TUNE_FUSE_CMP_AND_BRANCH: Fuse a compare or test instruction\n-   with a subsequent conditional jump instruction into a single\n-   compare-and-branch uop.\n-   FIXME: revisit for generic.  */\n-DEF_TUNE (X86_TUNE_FUSE_CMP_AND_BRANCH, \"fuse_cmp_and_branch\", m_BDVER | m_CORE_ALL)\n+   In current implementation the partial register stalls are not eliminated\n+   very well - they can be introduced via subregs synthesized by combine\n+   and can happen in caller/callee saving sequences.  */\n+DEF_TUNE (X86_TUNE_PARTIAL_REG_STALL, \"partial_reg_stall\", m_PPRO)\n \n-/* X86_TUNE_OPT_AGU: Optimize for Address Generation Unit. This flag\n-   will impact LEA instruction selection. */\n-DEF_TUNE (X86_TUNE_OPT_AGU, \"opt_agu\", m_ATOM | m_SLM)\n+/* X86_TUNE_PROMOTE_QIMODE: When it is cheap, turn 8bit arithmetic to\n+   corresponding 32bit arithmetic.  */\n+DEF_TUNE (X86_TUNE_PROMOTE_QIMODE, \"promote_qimode\",\n+\t  ~m_PPRO)\n \n-/* X86_TUNE_VECTORIZE_DOUBLE: Enable double precision vector\n-   instructions.  */\n-DEF_TUNE (X86_TUNE_VECTORIZE_DOUBLE, \"vectorize_double\", ~m_ATOM)\n+/* X86_TUNE_PROMOTE_HI_REGS: Same, but for 16bit artihmetic.  Again we avoid\n+   partial register stalls on PentiumPro targets. */\n+DEF_TUNE (X86_TUNE_PROMOTE_HI_REGS, \"promote_hi_regs\", m_PPRO)\n \n-/* X86_TUNE_SOFTWARE_PREFETCHING_BENEFICIAL: Enable software prefetching\n-   at -O3.  For the moment, the prefetching seems badly tuned for Intel\n-   chips.  */\n-DEF_TUNE (X86_TUNE_SOFTWARE_PREFETCHING_BENEFICIAL, \"software_prefetching_beneficial\",\n-          m_K6_GEODE | m_AMD_MULTIPLE)\n+/* X86_TUNE_HIMODE_MATH: Enable use of 16bit arithmetic.\n+   On PPro this flag is meant to avoid partial register stalls.  */\n+DEF_TUNE (X86_TUNE_HIMODE_MATH, \"himode_math\", ~m_PPRO)\n \n-/* X86_TUNE_AVX128_OPTIMAL: Enable 128-bit AVX instruction generation for\n-   the auto-vectorizer.  */\n-DEF_TUNE (X86_TUNE_AVX128_OPTIMAL, \"avx128_optimal\", m_BDVER | m_BTVER2)\n+/* X86_TUNE_SPLIT_LONG_MOVES: Avoid instructions moving immediates\n+   directly to memory.  */\n+DEF_TUNE (X86_TUNE_SPLIT_LONG_MOVES, \"split_long_moves\", m_PPRO)\n \n-/* X86_TUNE_REASSOC_INT_TO_PARALLEL: Try to produce parallel computations\n-   during reassociation of integer computation.  */\n-DEF_TUNE (X86_TUNE_REASSOC_INT_TO_PARALLEL, \"reassoc_int_to_parallel\",\n-          m_ATOM)\n+/* X86_TUNE_USE_XCHGB: Use xchgb %rh,%rl instead of rolw/rorw $8,rx.  */\n+DEF_TUNE (X86_TUNE_USE_XCHGB, \"use_xchgb\", m_PENT4)\n \n-/* X86_TUNE_REASSOC_FP_TO_PARALLEL: Try to produce parallel computations\n-   during reassociation of fp computation.  */\n-DEF_TUNE (X86_TUNE_REASSOC_FP_TO_PARALLEL, \"reassoc_fp_to_parallel\",\n-          m_ATOM | m_SLM | m_HASWELL | m_BDVER1 | m_BDVER2 | m_GENERIC)\n+/* X86_TUNE_USE_MOV0: Use \"mov $0, reg\" instead of \"xor reg, reg\" to clear\n+   integer register.  */\n+DEF_TUNE (X86_TUNE_USE_MOV0, \"use_mov0\", m_K6)\n \n-/* X86_TUNE_GENERAL_REGS_SSE_SPILL: Try to spill general regs to SSE\n-   regs instead of memory.  */\n-DEF_TUNE (X86_TUNE_GENERAL_REGS_SSE_SPILL, \"general_regs_sse_spill\",\n-          m_CORE_ALL)\n+/* X86_TUNE_NOT_VECTORMODE: On AMD K6, NOT is vector decoded with memory\n+   operand that cannot be represented using a modRM byte.  The XOR\n+   replacement is long decoded, so this split helps here as well.  */\n+DEF_TUNE (X86_TUNE_NOT_VECTORMODE, \"not_vectormode\", m_K6)\n \n-/* X86_TUNE_AVOID_MEM_OPND_FOR_CMOVE: Try to avoid memory operands for\n-   a conditional move.  */\n-DEF_TUNE (X86_TUNE_AVOID_MEM_OPND_FOR_CMOVE, \"avoid_mem_opnd_for_cmove\",\n-\t  m_ATOM | m_SLM)\n+/* X86_TUNE_AVOID_VECTOR_DECODE: Enable splitters that avoid vector decoded\n+   forms of instructions on K8 targets.  */\n+DEF_TUNE (X86_TUNE_AVOID_VECTOR_DECODE, \"avoid_vector_decode\",\n+          m_K8)\n \n-/* X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS: Try to split memory operand for\n-   fp converts to destination register.  */\n-DEF_TUNE (X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS, \"split_mem_opnd_for_fp_converts\",\n-          m_SLM)\n+/*****************************************************************************/\n+/* This never worked well before.                                            */\n+/*****************************************************************************/\n \n-/* X86_TUNE_ACCUMULATE_OUTGOING_ARGS: Allocate stack space for outgoing\n-   arguments in prologue/epilogue instead of separately for each call\n-   by push/pop instructions.\n-   This increase code size by about 5% in 32bit mode, less so in 64bit mode\n-   because parameters are passed in registers.  It is considerable\n-   win for targets without stack engine that prevents multple push operations\n-   to happen in parallel.\n+/* X86_TUNE_BRANCH_PREDICTION_HINTS: Branch hints were put in P4 based\n+   on simulation result. But after P4 was made, no performance benefit\n+   was observed with branch hints.  It also increases the code size.\n+   As a result, icc never generates branch hints.  */\n+DEF_TUNE (X86_TUNE_BRANCH_PREDICTION_HINTS, \"branch_prediction_hints\", 0)\n \n-   FIXME: the flags is incorrectly enabled for amdfam10, Bulldozer,\n-   Bobcat and Generic.  This is because disabling it causes large\n-   regression on mgrid due to IRA limitation leading to unecessary\n-   use of the frame pointer in 32bit mode.  */\n-DEF_TUNE (X86_TUNE_ACCUMULATE_OUTGOING_ARGS, \"accumulate_outgoing_args\", \n-\t  m_PPRO | m_P4_NOCONA | m_ATOM | m_SLM | m_AMD_MULTIPLE | m_GENERIC)\n+/* X86_TUNE_QIMODE_MATH: Enable use of 8bit arithmetic.  */\n+DEF_TUNE (X86_TUNE_QIMODE_MATH, \"qimode_math\", ~0)\n \n-/* X86_TUNE_ALWAYS_FANCY_MATH_387: controls use of fancy 387 operations,\n-   such as fsqrt, fprem, fsin, fcos, fsincos etc.\n-   Should be enabled for all targets that always has coprocesor.  */\n-DEF_TUNE (X86_TUNE_ALWAYS_FANCY_MATH_387, \"always_fancy_math_387\", \n-          ~(m_386 | m_486))\n+/* X86_TUNE_PROMOTE_QI_REGS: This enables generic code that promotes all 8bit\n+   arithmetic to 32bit via PROMOTE_MODE macro.  This code generation scheme\n+   is usually used for RISC targets.  */\n+DEF_TUNE (X86_TUNE_PROMOTE_QI_REGS, \"promote_qi_regs\", 0)"}]}