{"sha": "5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1", "node_id": "C_kwDOANBUbNoAKDVlZDc3ZmIzZWQxZWUwMjg5YTBlYzk0OTllZjUyYjk5YjM5NDIxZjE", "commit": {"author": {"name": "Tom de Vries", "email": "tdevries@suse.de", "date": "2021-04-20T06:47:03Z"}, "committer": {"name": "Tom de Vries", "email": "tdevries@suse.de", "date": "2022-02-22T14:48:03Z"}, "message": "[libgomp, nvptx] Fix hang in gomp_team_barrier_wait_end\n\nConsider the following omp fragment.\n...\n  #pragma omp target\n  #pragma omp parallel num_threads (2)\n  #pragma omp task\n    ;\n...\n\nThis hangs at -O0 for nvptx.\n\nInvestigating the behaviour gives us the following trace of events:\n- both threads execute GOMP_task, where they:\n  - deposit a task, and\n  - execute gomp_team_barrier_wake\n- thread 1 executes gomp_team_barrier_wait_end and, not being the last thread,\n  proceeds to wait at the team barrier\n- thread 0 executes gomp_team_barrier_wait_end and, being the last thread, it\n  calls gomp_barrier_handle_tasks, where it:\n  - executes both tasks and marks the team barrier done\n  - executes a gomp_team_barrier_wake which wakes up thread 1\n- thread 1 exits the team barrier\n- thread 0 returns from gomp_barrier_handle_tasks and goes to wait at\n  the team barrier.\n- thread 0 hangs.\n\nTo understand why there is a hang here, it's good to understand how things\nare setup for nvptx.  The libgomp/config/nvptx/bar.c implementation is\na copy of the libgomp/config/linux/bar.c implementation, with uses of both\nfutex_wake and do_wait replaced with uses of ptx insn bar.sync:\n...\n  if (bar->total > 1)\n    asm (\"bar.sync 1, %0;\" : : \"r\" (32 * bar->total));\n...\n\nThe point where thread 0 goes to wait at the team barrier, corresponds in\nthe linux implementation with a do_wait.  In the linux case, the call to\ndo_wait doesn't hang, because it's waiting for bar->generation to become\na certain value, and if bar->generation already has that value, it just\nproceeds, without any need for coordination with other threads.\n\nIn the nvtpx case, the bar.sync waits until thread 1 joins it in the same\nlogical barrier, which never happens: thread 1 is lingering in the\nthread pool at the thread pool barrier (using a different logical barrier),\nwaiting to join a new team.\n\nThe easiest way to fix this is to revert to the posix implementation for\nbar.{c,h}.  That however falls back on a busy-waiting approach, and\ndoes not take advantage of the ptx bar.sync insn.\n\nInstead, we revert to the linux implementation for bar.c,\nand implement bar.c local functions futex_wait and futex_wake using the\nbar.sync insn.\n\nThe bar.sync insn takes an argument specifying how many threads are\nparticipating, and that doesn't play well with the futex syntax where it's\nnot clear in advance how many threads will be woken up.\n\nThis is solved by waking up all waiting threads each time a futex_wait or\nfutex_wake happens, and possibly going back to sleep with an updated thread\ncount.\n\nTested libgomp on x86_64 with nvptx accelerator.\n\nlibgomp/ChangeLog:\n\n2021-04-20  Tom de Vries  <tdevries@suse.de>\n\n\tPR target/99555\n\t* config/nvptx/bar.c (generation_to_barrier): New function, copied\n\tfrom config/rtems/bar.c.\n\t(futex_wait, futex_wake): New function.\n\t(do_spin, do_wait): New function, copied from config/linux/wait.h.\n\t(gomp_barrier_wait_end, gomp_barrier_wait_last)\n\t(gomp_team_barrier_wake, gomp_team_barrier_wait_end):\n\t(gomp_team_barrier_wait_cancel_end, gomp_team_barrier_cancel): Remove\n\tand replace with include of config/linux/bar.c.\n\t* config/nvptx/bar.h (gomp_barrier_t): Add fields waiters and lock.\n\t(gomp_barrier_init): Init new fields.\n\t* testsuite/libgomp.c-c++-common/task-detach-6.c: Remove nvptx-specific\n\tworkarounds.\n\t* testsuite/libgomp.c/pr99555-1.c: Same.\n\t* testsuite/libgomp.fortran/task-detach-6.f90: Same.", "tree": {"sha": "d66851899afed264a750c122e64520b130f3220d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/d66851899afed264a750c122e64520b130f3220d"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1/comments", "author": {"login": "vries", "id": 4057235, "node_id": "MDQ6VXNlcjQwNTcyMzU=", "avatar_url": "https://avatars.githubusercontent.com/u/4057235?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vries", "html_url": "https://github.com/vries", "followers_url": "https://api.github.com/users/vries/followers", "following_url": "https://api.github.com/users/vries/following{/other_user}", "gists_url": "https://api.github.com/users/vries/gists{/gist_id}", "starred_url": "https://api.github.com/users/vries/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vries/subscriptions", "organizations_url": "https://api.github.com/users/vries/orgs", "repos_url": "https://api.github.com/users/vries/repos", "events_url": "https://api.github.com/users/vries/events{/privacy}", "received_events_url": "https://api.github.com/users/vries/received_events", "type": "User", "site_admin": false}, "committer": {"login": "vries", "id": 4057235, "node_id": "MDQ6VXNlcjQwNTcyMzU=", "avatar_url": "https://avatars.githubusercontent.com/u/4057235?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vries", "html_url": "https://github.com/vries", "followers_url": "https://api.github.com/users/vries/followers", "following_url": "https://api.github.com/users/vries/following{/other_user}", "gists_url": "https://api.github.com/users/vries/gists{/gist_id}", "starred_url": "https://api.github.com/users/vries/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vries/subscriptions", "organizations_url": "https://api.github.com/users/vries/orgs", "repos_url": "https://api.github.com/users/vries/repos", "events_url": "https://api.github.com/users/vries/events{/privacy}", "received_events_url": "https://api.github.com/users/vries/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "bd73d8dd312c759ee505b401d6b4fd7be07a3f1a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/bd73d8dd312c759ee505b401d6b4fd7be07a3f1a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/bd73d8dd312c759ee505b401d6b4fd7be07a3f1a"}], "stats": {"total": 280, "additions": 105, "deletions": 175}, "files": [{"sha": "eee21071f473be8ed28e8be203a08d4ece755f8d", "filename": "libgomp/config/nvptx/bar.c", "status": "modified", "additions": 101, "deletions": 147, "changes": 248, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1/libgomp%2Fconfig%2Fnvptx%2Fbar.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1/libgomp%2Fconfig%2Fnvptx%2Fbar.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgomp%2Fconfig%2Fnvptx%2Fbar.c?ref=5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1", "patch": "@@ -30,183 +30,137 @@\n #include <limits.h>\n #include \"libgomp.h\"\n \n+/* For cpu_relax.  */\n+#include \"doacross.h\"\n \n-void\n-gomp_barrier_wait_end (gomp_barrier_t *bar, gomp_barrier_state_t state)\n-{\n-  if (__builtin_expect (state & BAR_WAS_LAST, 0))\n-    {\n-      /* Next time we'll be awaiting TOTAL threads again.  */\n-      bar->awaited = bar->total;\n-      __atomic_store_n (&bar->generation, bar->generation + BAR_INCR,\n-\t\t\tMEMMODEL_RELEASE);\n-    }\n-  if (bar->total > 1)\n-    asm (\"bar.sync 1, %0;\" : : \"r\" (32 * bar->total));\n-}\n+/* Assuming ADDR is &bar->generation, return bar.  Copied from\n+   rtems/bar.c.  */\n \n-void\n-gomp_barrier_wait (gomp_barrier_t *bar)\n+static gomp_barrier_t *\n+generation_to_barrier (int *addr)\n {\n-  gomp_barrier_wait_end (bar, gomp_barrier_wait_start (bar));\n+  char *bar\n+    = (char *) addr - __builtin_offsetof (gomp_barrier_t, generation);\n+  return (gomp_barrier_t *)bar;\n }\n \n-/* Like gomp_barrier_wait, except that if the encountering thread\n-   is not the last one to hit the barrier, it returns immediately.\n-   The intended usage is that a thread which intends to gomp_barrier_destroy\n-   this barrier calls gomp_barrier_wait, while all other threads\n-   call gomp_barrier_wait_last.  When gomp_barrier_wait returns,\n-   the barrier can be safely destroyed.  */\n+/* Implement futex_wait-like behaviour to plug into the linux/bar.c\n+   implementation.  Assumes ADDR is &bar->generation.   */\n \n-void\n-gomp_barrier_wait_last (gomp_barrier_t *bar)\n+static inline void\n+futex_wait (int *addr, int val)\n {\n-  /* Deferring to gomp_barrier_wait does not use the optimization opportunity\n-     allowed by the interface contract for all-but-last participants.  The\n-     original implementation in config/linux/bar.c handles this better.  */\n-  gomp_barrier_wait (bar);\n-}\n+  gomp_barrier_t *bar = generation_to_barrier (addr);\n \n-void\n-gomp_team_barrier_wake (gomp_barrier_t *bar, int count)\n-{\n-  if (bar->total > 1)\n-    asm (\"bar.sync 1, %0;\" : : \"r\" (32 * bar->total));\n-}\n+  if (bar->total < 2)\n+    /* A barrier with less than two threads, nop.  */\n+    return;\n \n-void\n-gomp_team_barrier_wait_end (gomp_barrier_t *bar, gomp_barrier_state_t state)\n-{\n-  unsigned int generation, gen;\n+  gomp_mutex_lock (&bar->lock);\n \n-  if (__builtin_expect (state & BAR_WAS_LAST, 0))\n+  /* Futex semantics: only go to sleep if *addr == val.  */\n+  if (__builtin_expect (__atomic_load_n (addr, MEMMODEL_ACQUIRE) != val, 0))\n     {\n-      /* Next time we'll be awaiting TOTAL threads again.  */\n-      struct gomp_thread *thr = gomp_thread ();\n-      struct gomp_team *team = thr->ts.team;\n-\n-      bar->awaited = bar->total;\n-      team->work_share_cancelled = 0;\n-      if (__builtin_expect (team->task_count, 0))\n-\t{\n-\t  gomp_barrier_handle_tasks (state);\n-\t  state &= ~BAR_WAS_LAST;\n-\t}\n-      else\n-\t{\n-\t  state &= ~BAR_CANCELLED;\n-\t  state += BAR_INCR - BAR_WAS_LAST;\n-\t  __atomic_store_n (&bar->generation, state, MEMMODEL_RELEASE);\n-\t  if (bar->total > 1)\n-\t    asm (\"bar.sync 1, %0;\" : : \"r\" (32 * bar->total));\n-\t  return;\n-\t}\n+      gomp_mutex_unlock (&bar->lock);\n+      return;\n     }\n \n-  generation = state;\n-  state &= ~BAR_CANCELLED;\n-  do\n+  /* Register as waiter.  */\n+  unsigned int waiters\n+    = __atomic_add_fetch (&bar->waiters, 1, MEMMODEL_ACQ_REL);\n+  if (waiters == 0)\n+    __builtin_abort ();\n+  unsigned int waiter_id = waiters;\n+\n+  if (waiters > 1)\n     {\n-      if (bar->total > 1)\n-\tasm (\"bar.sync 1, %0;\" : : \"r\" (32 * bar->total));\n-      gen = __atomic_load_n (&bar->generation, MEMMODEL_ACQUIRE);\n-      if (__builtin_expect (gen & BAR_TASK_PENDING, 0))\n-\t{\n-\t  gomp_barrier_handle_tasks (state);\n-\t  gen = __atomic_load_n (&bar->generation, MEMMODEL_ACQUIRE);\n-\t}\n-      generation |= gen & BAR_WAITING_FOR_TASK;\n+      /* Wake other threads in bar.sync.  */\n+      asm volatile (\"bar.sync 1, %0;\" : : \"r\" (32 * waiters));\n+\n+      /* Ensure that they have updated waiters.  */\n+      asm volatile (\"bar.sync 1, %0;\" : : \"r\" (32 * waiters));\n     }\n-  while (gen != state + BAR_INCR);\n-}\n \n-void\n-gomp_team_barrier_wait (gomp_barrier_t *bar)\n-{\n-  gomp_team_barrier_wait_end (bar, gomp_barrier_wait_start (bar));\n-}\n+  gomp_mutex_unlock (&bar->lock);\n \n-void\n-gomp_team_barrier_wait_final (gomp_barrier_t *bar)\n-{\n-  gomp_barrier_state_t state = gomp_barrier_wait_final_start (bar);\n-  if (__builtin_expect (state & BAR_WAS_LAST, 0))\n-    bar->awaited_final = bar->total;\n-  gomp_team_barrier_wait_end (bar, state);\n+  while (1)\n+    {\n+      /* Wait for next thread in barrier.  */\n+      asm volatile (\"bar.sync 1, %0;\" : : \"r\" (32 * (waiters + 1)));\n+\n+      /* Get updated waiters.  */\n+      unsigned int updated_waiters\n+\t= __atomic_load_n (&bar->waiters, MEMMODEL_ACQUIRE);\n+\n+      /* Notify that we have updated waiters.  */\n+      asm volatile (\"bar.sync 1, %0;\" : : \"r\" (32 * (waiters + 1)));\n+\n+      waiters = updated_waiters;\n+\n+      if (waiter_id > waiters)\n+\t/* A wake happened, and we're in the group of woken threads.  */\n+\tbreak;\n+\n+      /* Continue waiting.  */\n+    }\n }\n \n-bool\n-gomp_team_barrier_wait_cancel_end (gomp_barrier_t *bar,\n-\t\t\t\t   gomp_barrier_state_t state)\n+/* Implement futex_wake-like behaviour to plug into the linux/bar.c\n+   implementation.  Assumes ADDR is &bar->generation.  */\n+\n+static inline void\n+futex_wake (int *addr, int count)\n {\n-  unsigned int generation, gen;\n+  gomp_barrier_t *bar = generation_to_barrier (addr);\n+\n+  if (bar->total < 2)\n+    /* A barrier with less than two threads, nop.  */\n+    return;\n \n-  if (__builtin_expect (state & BAR_WAS_LAST, 0))\n+  gomp_mutex_lock (&bar->lock);\n+  unsigned int waiters = __atomic_load_n (&bar->waiters, MEMMODEL_ACQUIRE);\n+  if (waiters == 0)\n     {\n-      /* Next time we'll be awaiting TOTAL threads again.  */\n-      /* BAR_CANCELLED should never be set in state here, because\n-\t cancellation means that at least one of the threads has been\n-\t cancelled, thus on a cancellable barrier we should never see\n-\t all threads to arrive.  */\n-      struct gomp_thread *thr = gomp_thread ();\n-      struct gomp_team *team = thr->ts.team;\n-\n-      bar->awaited = bar->total;\n-      team->work_share_cancelled = 0;\n-      if (__builtin_expect (team->task_count, 0))\n-\t{\n-\t  gomp_barrier_handle_tasks (state);\n-\t  state &= ~BAR_WAS_LAST;\n-\t}\n-      else\n-\t{\n-\t  state += BAR_INCR - BAR_WAS_LAST;\n-\t  __atomic_store_n (&bar->generation, state, MEMMODEL_RELEASE);\n-\t  if (bar->total > 1)\n-\t    asm (\"bar.sync 1, %0;\" : : \"r\" (32 * bar->total));\n-\t  return false;\n-\t}\n+      /* No threads to wake.  */\n+      gomp_mutex_unlock (&bar->lock);\n+      return;\n     }\n \n-  if (__builtin_expect (state & BAR_CANCELLED, 0))\n-    return true;\n+  if (count == INT_MAX)\n+    /* Release all threads.  */\n+    __atomic_store_n (&bar->waiters, 0, MEMMODEL_RELEASE);\n+  else if (count < bar->total)\n+    /* Release count threads.  */\n+    __atomic_add_fetch (&bar->waiters, -count, MEMMODEL_ACQ_REL);\n+  else\n+    /* Count has an illegal value.  */\n+    __builtin_abort ();\n \n-  generation = state;\n-  do\n-    {\n-      if (bar->total > 1)\n-\tasm (\"bar.sync 1, %0;\" : : \"r\" (32 * bar->total));\n-      gen = __atomic_load_n (&bar->generation, MEMMODEL_ACQUIRE);\n-      if (__builtin_expect (gen & BAR_CANCELLED, 0))\n-\treturn true;\n-      if (__builtin_expect (gen & BAR_TASK_PENDING, 0))\n-\t{\n-\t  gomp_barrier_handle_tasks (state);\n-\t  gen = __atomic_load_n (&bar->generation, MEMMODEL_ACQUIRE);\n-\t}\n-      generation |= gen & BAR_WAITING_FOR_TASK;\n-    }\n-  while (gen != state + BAR_INCR);\n+  /* Wake other threads in bar.sync.  */\n+  asm volatile (\"bar.sync 1, %0;\" : : \"r\" (32 * (waiters + 1)));\n+\n+  /* Let them get the updated waiters.  */\n+  asm volatile (\"bar.sync 1, %0;\" : : \"r\" (32 * (waiters + 1)));\n \n-  return false;\n+  gomp_mutex_unlock (&bar->lock);\n }\n \n-bool\n-gomp_team_barrier_wait_cancel (gomp_barrier_t *bar)\n+/* Copied from linux/wait.h.  */\n+\n+static inline int do_spin (int *addr, int val)\n {\n-  return gomp_team_barrier_wait_cancel_end (bar, gomp_barrier_wait_start (bar));\n+  /* The current implementation doesn't spin.  */\n+  return 1;\n }\n \n-void\n-gomp_team_barrier_cancel (struct gomp_team *team)\n+/* Copied from linux/wait.h.  */\n+\n+static inline void do_wait (int *addr, int val)\n {\n-  gomp_mutex_lock (&team->task_lock);\n-  if (team->barrier.generation & BAR_CANCELLED)\n-    {\n-      gomp_mutex_unlock (&team->task_lock);\n-      return;\n-    }\n-  team->barrier.generation |= BAR_CANCELLED;\n-  gomp_mutex_unlock (&team->task_lock);\n-  gomp_team_barrier_wake (&team->barrier, INT_MAX);\n+  if (do_spin (addr, val))\n+    futex_wait (addr, val);\n }\n+\n+/* Reuse the linux implementation.  */\n+#define GOMP_WAIT_H 1\n+#include \"../linux/bar.c\""}, {"sha": "28bf7f4d31305279b80a91b4a9663f8cf011092c", "filename": "libgomp/config/nvptx/bar.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1/libgomp%2Fconfig%2Fnvptx%2Fbar.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1/libgomp%2Fconfig%2Fnvptx%2Fbar.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgomp%2Fconfig%2Fnvptx%2Fbar.h?ref=5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1", "patch": "@@ -38,6 +38,8 @@ typedef struct\n   unsigned generation;\n   unsigned awaited;\n   unsigned awaited_final;\n+  unsigned waiters;\n+  gomp_mutex_t lock;\n } gomp_barrier_t;\n \n typedef unsigned int gomp_barrier_state_t;\n@@ -57,6 +59,8 @@ static inline void gomp_barrier_init (gomp_barrier_t *bar, unsigned count)\n   bar->awaited = count;\n   bar->awaited_final = count;\n   bar->generation = 0;\n+  bar->waiters = 0;\n+  gomp_mutex_init (&bar->lock);\n }\n \n static inline void gomp_barrier_reinit (gomp_barrier_t *bar, unsigned count)"}, {"sha": "e5c2291e6ff0c0070d874cd593143ef65076e9e6", "filename": "libgomp/testsuite/libgomp.c-c++-common/task-detach-6.c", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1/libgomp%2Ftestsuite%2Flibgomp.c-c%2B%2B-common%2Ftask-detach-6.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1/libgomp%2Ftestsuite%2Flibgomp.c-c%2B%2B-common%2Ftask-detach-6.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgomp%2Ftestsuite%2Flibgomp.c-c%2B%2B-common%2Ftask-detach-6.c?ref=5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1", "patch": "@@ -2,21 +2,13 @@\n \n #include <omp.h>\n #include <assert.h>\n-#include <unistd.h> // For 'alarm'.\n-\n-#include \"on_device_arch.h\"\n \n /* Test tasks with detach clause on an offload device.  Each device\n    thread spawns off a chain of tasks, that can then be executed by\n    any available thread.  */\n \n int main (void)\n {\n-  //TODO See '../libgomp.c/pr99555-1.c'.\n-  if (on_device_arch_nvptx ())\n-    alarm (4); /*TODO Until resolved, make sure that we exit quickly, with error status.\n-\t\t { dg-xfail-run-if \"PR99555\" { offload_device_nvptx } } */\n-\n   int x = 0, y = 0, z = 0;\n   int thread_count;\n   omp_event_handle_t detach_event1, detach_event2;"}, {"sha": "7386e016fd202730b00aa6288a40219be453fe0e", "filename": "libgomp/testsuite/libgomp.c/pr99555-1.c", "status": "modified", "additions": 0, "deletions": 8, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1/libgomp%2Ftestsuite%2Flibgomp.c%2Fpr99555-1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1/libgomp%2Ftestsuite%2Flibgomp.c%2Fpr99555-1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgomp%2Ftestsuite%2Flibgomp.c%2Fpr99555-1.c?ref=5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1", "patch": "@@ -2,16 +2,8 @@\n \n // { dg-additional-options \"-O0\" }\n \n-#include <unistd.h> // For 'alarm'.\n-\n-#include \"../libgomp.c-c++-common/on_device_arch.h\"\n-\n int main (void)\n {\n-  if (on_device_arch_nvptx ())\n-    alarm (4); /*TODO Until resolved, make sure that we exit quickly, with error status.\n-\t\t { dg-xfail-run-if \"PR99555\" { offload_device_nvptx } } */\n-\n #pragma omp target\n #pragma omp parallel // num_threads(1)\n #pragma omp task"}, {"sha": "03a3b61540dd6e6bbd44b3a5e1299b67e85ab7de", "filename": "libgomp/testsuite/libgomp.fortran/task-detach-6.f90", "status": "modified", "additions": 0, "deletions": 12, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1/libgomp%2Ftestsuite%2Flibgomp.fortran%2Ftask-detach-6.f90", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1/libgomp%2Ftestsuite%2Flibgomp.fortran%2Ftask-detach-6.f90", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgomp%2Ftestsuite%2Flibgomp.fortran%2Ftask-detach-6.f90?ref=5ed77fb3ed1ee0289a0ec9499ef52b99b39421f1", "patch": "@@ -1,6 +1,5 @@\n ! { dg-do run }\n \n-! { dg-additional-sources on_device_arch.c }\n   ! { dg-prune-output \"command-line option '-fintrinsic-modules-path=.*' is valid for Fortran but not for C\" }\n \n ! Test tasks with detach clause on an offload device.  Each device\n@@ -14,17 +13,6 @@ program task_detach_6\n   integer :: x = 0, y = 0, z = 0\n   integer :: thread_count\n \n-  interface\n-    integer function on_device_arch_nvptx() bind(C)\n-    end function on_device_arch_nvptx\n-  end interface\n-\n-  !TODO See '../libgomp.c/pr99555-1.c'.\n-  if (on_device_arch_nvptx () /= 0) then\n-     call alarm (4, 0); !TODO Until resolved, make sure that we exit quickly, with error status.\n-     ! { dg-xfail-run-if \"PR99555\" { offload_device_nvptx } }\n-  end if\n-\n   !$omp target map (tofrom: x, y, z) map (from: thread_count)\n     !$omp parallel private (detach_event1, detach_event2)\n       !$omp single"}]}