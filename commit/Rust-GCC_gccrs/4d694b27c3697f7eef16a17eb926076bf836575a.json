{"sha": "4d694b27c3697f7eef16a17eb926076bf836575a", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NGQ2OTRiMjdjMzY5N2Y3ZWVmMTZhMTdlYjkyNjA3NmJmODM2NTc1YQ==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@linaro.org", "date": "2018-01-03T07:16:06Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2018-01-03T07:16:06Z"}, "message": "poly_int: vectorizable_load/store\n\nThis patch makes vectorizable_load and vectorizable_store cope with\nvariable-length vectors.  The reverse and permute cases will be\nexcluded by the code that checks the permutation mask (although a\npatch after the main SVE submission adds support for the reversed\ncase).  Here we also need to exclude VMAT_ELEMENTWISE and\nVMAT_STRIDED_SLP, which split the operation up into a constant\nnumber of constant-sized operations.  We also don't try to extend\nthe current widening gather/scatter support to variable-length\nvectors, since SVE uses a different approach.\n\n2018-01-03  Richard Sandiford  <richard.sandiford@linaro.org>\n\t    Alan Hayward  <alan.hayward@arm.com>\n\t    David Sherwood  <david.sherwood@arm.com>\n\ngcc/\n\t* tree-vect-stmts.c (get_load_store_type): Treat the number of\n\tunits as polynomial.  Reject VMAT_ELEMENTWISE and VMAT_STRIDED_SLP\n\tfor variable-length vectors.\n\t(vectorizable_mask_load_store): Treat the number of units as\n\tpolynomial, asserting that it is constant if the condition has\n\talready been enforced.\n\t(vectorizable_store, vectorizable_load): Likewise.\n\nCo-Authored-By: Alan Hayward <alan.hayward@arm.com>\nCo-Authored-By: David Sherwood <david.sherwood@arm.com>\n\nFrom-SVN: r256136", "tree": {"sha": "ba4aa662cc4e78460fb98744bdcbf8b908c970b1", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/ba4aa662cc4e78460fb98744bdcbf8b908c970b1"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/4d694b27c3697f7eef16a17eb926076bf836575a", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4d694b27c3697f7eef16a17eb926076bf836575a", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4d694b27c3697f7eef16a17eb926076bf836575a", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4d694b27c3697f7eef16a17eb926076bf836575a/comments", "author": null, "committer": null, "parents": [{"sha": "fa780794692994d63febf4fb187567e245cdd4ee", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fa780794692994d63febf4fb187567e245cdd4ee", "html_url": "https://github.com/Rust-GCC/gccrs/commit/fa780794692994d63febf4fb187567e245cdd4ee"}], "stats": {"total": 186, "additions": 117, "deletions": 69}, "files": [{"sha": "1e8431c92ec70203cc6f9d254e0f097715bbfbeb", "filename": "gcc/ChangeLog", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4d694b27c3697f7eef16a17eb926076bf836575a/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4d694b27c3697f7eef16a17eb926076bf836575a/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=4d694b27c3697f7eef16a17eb926076bf836575a", "patch": "@@ -1,3 +1,15 @@\n+2018-01-03  Richard Sandiford  <richard.sandiford@linaro.org>\n+\t    Alan Hayward  <alan.hayward@arm.com>\n+\t    David Sherwood  <david.sherwood@arm.com>\n+\n+\t* tree-vect-stmts.c (get_load_store_type): Treat the number of\n+\tunits as polynomial.  Reject VMAT_ELEMENTWISE and VMAT_STRIDED_SLP\n+\tfor variable-length vectors.\n+\t(vectorizable_mask_load_store): Treat the number of units as\n+\tpolynomial, asserting that it is constant if the condition has\n+\talready been enforced.\n+\t(vectorizable_store, vectorizable_load): Likewise.\n+\n 2018-01-03  Richard Sandiford  <richard.sandiford@linaro.org>\n \t    Alan Hayward  <alan.hayward@arm.com>\n \t    David Sherwood  <david.sherwood@arm.com>"}, {"sha": "932c3095f9ddd2758a9ef7f48dc153fb29aea56b", "filename": "gcc/tree-vect-stmts.c", "status": "modified", "additions": 105, "deletions": 69, "changes": 174, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4d694b27c3697f7eef16a17eb926076bf836575a/gcc%2Ftree-vect-stmts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4d694b27c3697f7eef16a17eb926076bf836575a/gcc%2Ftree-vect-stmts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-stmts.c?ref=4d694b27c3697f7eef16a17eb926076bf836575a", "patch": "@@ -1965,6 +1965,7 @@ get_load_store_type (gimple *stmt, tree vectype, bool slp,\n   stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n   vec_info *vinfo = stmt_info->vinfo;\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n+  poly_uint64 nunits = TYPE_VECTOR_SUBPARTS (vectype);\n   if (STMT_VINFO_GATHER_SCATTER_P (stmt_info))\n     {\n       *memory_access_type = VMAT_GATHER_SCATTER;\n@@ -2008,6 +2009,17 @@ get_load_store_type (gimple *stmt, tree vectype, bool slp,\n \t*memory_access_type = VMAT_CONTIGUOUS;\n     }\n \n+  if ((*memory_access_type == VMAT_ELEMENTWISE\n+       || *memory_access_type == VMAT_STRIDED_SLP)\n+      && !nunits.is_constant ())\n+    {\n+      if (dump_enabled_p ())\n+\tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n+\t\t\t \"Not using elementwise accesses due to variable \"\n+\t\t\t \"vectorization factor.\\n\");\n+      return false;\n+    }\n+\n   /* FIXME: At the moment the cost model seems to underestimate the\n      cost of using elementwise accesses.  This check preserves the\n      traditional behavior until that can be fixed.  */\n@@ -2048,7 +2060,7 @@ vectorizable_mask_load_store (gimple *stmt, gimple_stmt_iterator *gsi,\n   tree dummy;\n   tree dataref_ptr = NULL_TREE;\n   gimple *ptr_incr;\n-  int nunits = TYPE_VECTOR_SUBPARTS (vectype);\n+  poly_uint64 nunits = TYPE_VECTOR_SUBPARTS (vectype);\n   int ncopies;\n   int i, j;\n   bool inv_p;\n@@ -2178,7 +2190,8 @@ vectorizable_mask_load_store (gimple *stmt, gimple_stmt_iterator *gsi,\n       gimple_seq seq;\n       basic_block new_bb;\n       enum { NARROW, NONE, WIDEN } modifier;\n-      int gather_off_nunits = TYPE_VECTOR_SUBPARTS (gs_info.offset_vectype);\n+      poly_uint64 gather_off_nunits\n+\t= TYPE_VECTOR_SUBPARTS (gs_info.offset_vectype);\n \n       rettype = TREE_TYPE (TREE_TYPE (gs_info.decl));\n       srctype = TREE_VALUE (arglist); arglist = TREE_CHAIN (arglist);\n@@ -2189,37 +2202,41 @@ vectorizable_mask_load_store (gimple *stmt, gimple_stmt_iterator *gsi,\n       gcc_checking_assert (types_compatible_p (srctype, rettype)\n \t\t\t   && types_compatible_p (srctype, masktype));\n \n-      if (nunits == gather_off_nunits)\n+      if (known_eq (nunits, gather_off_nunits))\n \tmodifier = NONE;\n-      else if (nunits == gather_off_nunits / 2)\n+      else if (known_eq (nunits * 2, gather_off_nunits))\n \t{\n \t  modifier = WIDEN;\n \n-\t  vec_perm_builder sel (gather_off_nunits, gather_off_nunits, 1);\n-\t  for (i = 0; i < gather_off_nunits; ++i)\n-\t    sel.quick_push (i | nunits);\n+\t  /* Currently widening gathers and scatters are only supported for\n+\t     fixed-length vectors.  */\n+\t  int count = gather_off_nunits.to_constant ();\n+\t  vec_perm_builder sel (count, count, 1);\n+\t  for (i = 0; i < count; ++i)\n+\t    sel.quick_push (i | (count / 2));\n \n-\t  vec_perm_indices indices (sel, 1, gather_off_nunits);\n+\t  vec_perm_indices indices (sel, 1, count);\n \t  perm_mask = vect_gen_perm_mask_checked (gs_info.offset_vectype,\n \t\t\t\t\t\t  indices);\n \t}\n-      else if (nunits == gather_off_nunits * 2)\n+      else if (known_eq (nunits, gather_off_nunits * 2))\n \t{\n \t  modifier = NARROW;\n \n-\t  vec_perm_builder sel (nunits, nunits, 1);\n-\t  sel.quick_grow (nunits);\n-\t  for (i = 0; i < nunits; ++i)\n-\t    sel[i] = i < gather_off_nunits\n-\t\t     ? i : i + nunits - gather_off_nunits;\n-\t  vec_perm_indices indices (sel, 2, nunits);\n+\t  /* Currently narrowing gathers and scatters are only supported for\n+\t     fixed-length vectors.  */\n+\t  int count = nunits.to_constant ();\n+\t  vec_perm_builder sel (count, count, 1);\n+\t  sel.quick_grow (count);\n+\t  for (i = 0; i < count; ++i)\n+\t    sel[i] = i < count / 2 ? i : i + count / 2;\n+\t  vec_perm_indices indices (sel, 2, count);\n \t  perm_mask = vect_gen_perm_mask_checked (vectype, indices);\n \n \t  ncopies *= 2;\n-\n-\t  for (i = 0; i < nunits; ++i)\n-\t    sel[i] = i | gather_off_nunits;\n-\t  indices.new_vector (sel, 2, gather_off_nunits);\n+\t  for (i = 0; i < count; ++i)\n+\t    sel[i] = i | (count / 2);\n+\t  indices.new_vector (sel, 2, count);\n \t  mask_perm_mask = vect_gen_perm_mask_checked (masktype, indices);\n \t}\n       else\n@@ -5746,7 +5763,7 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n   gcc_assert (gimple_assign_single_p (stmt));\n \n   tree vectype = STMT_VINFO_VECTYPE (stmt_info), rhs_vectype = NULL_TREE;\n-  unsigned int nunits = TYPE_VECTOR_SUBPARTS (vectype);\n+  poly_uint64 nunits = TYPE_VECTOR_SUBPARTS (vectype);\n \n   if (loop_vinfo)\n     {\n@@ -5840,32 +5857,39 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n       gimple_seq seq;\n       basic_block new_bb;\n       enum { NARROW, NONE, WIDEN } modifier;\n-      int scatter_off_nunits = TYPE_VECTOR_SUBPARTS (gs_info.offset_vectype);\n+      poly_uint64 scatter_off_nunits\n+\t= TYPE_VECTOR_SUBPARTS (gs_info.offset_vectype);\n \n-      if (nunits == (unsigned int) scatter_off_nunits)\n+      if (known_eq (nunits, scatter_off_nunits))\n \tmodifier = NONE;\n-      else if (nunits == (unsigned int) scatter_off_nunits / 2)\n+      else if (known_eq (nunits * 2, scatter_off_nunits))\n \t{\n \t  modifier = WIDEN;\n \n-\t  vec_perm_builder sel (scatter_off_nunits, scatter_off_nunits, 1);\n-\t  for (i = 0; i < (unsigned int) scatter_off_nunits; ++i)\n-\t    sel.quick_push (i | nunits);\n+\t  /* Currently gathers and scatters are only supported for\n+\t     fixed-length vectors.  */\n+\t  unsigned int count = scatter_off_nunits.to_constant ();\n+\t  vec_perm_builder sel (count, count, 1);\n+\t  for (i = 0; i < (unsigned int) count; ++i)\n+\t    sel.quick_push (i | (count / 2));\n \n-\t  vec_perm_indices indices (sel, 1, scatter_off_nunits);\n+\t  vec_perm_indices indices (sel, 1, count);\n \t  perm_mask = vect_gen_perm_mask_checked (gs_info.offset_vectype,\n \t\t\t\t\t\t  indices);\n \t  gcc_assert (perm_mask != NULL_TREE);\n \t}\n-      else if (nunits == (unsigned int) scatter_off_nunits * 2)\n+      else if (known_eq (nunits, scatter_off_nunits * 2))\n \t{\n \t  modifier = NARROW;\n \n-\t  vec_perm_builder sel (nunits, nunits, 1);\n-\t  for (i = 0; i < (unsigned int) nunits; ++i)\n-\t    sel.quick_push (i | scatter_off_nunits);\n+\t  /* Currently gathers and scatters are only supported for\n+\t     fixed-length vectors.  */\n+\t  unsigned int count = nunits.to_constant ();\n+\t  vec_perm_builder sel (count, count, 1);\n+\t  for (i = 0; i < (unsigned int) count; ++i)\n+\t    sel.quick_push (i | (count / 2));\n \n-\t  vec_perm_indices indices (sel, 2, nunits);\n+\t  vec_perm_indices indices (sel, 2, count);\n \t  perm_mask = vect_gen_perm_mask_checked (vectype, indices);\n \t  gcc_assert (perm_mask != NULL_TREE);\n \t  ncopies *= 2;\n@@ -6038,6 +6062,8 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n       tree stride_base, stride_step, alias_off;\n       tree vec_oprnd;\n       unsigned int g;\n+      /* Checked by get_load_store_type.  */\n+      unsigned int const_nunits = nunits.to_constant ();\n \n       gcc_assert (!nested_in_vect_loop_p (loop, stmt));\n \n@@ -6067,16 +6093,16 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t     ...\n          */\n \n-      unsigned nstores = nunits;\n+      unsigned nstores = const_nunits;\n       unsigned lnel = 1;\n       tree ltype = elem_type;\n       tree lvectype = vectype;\n       if (slp)\n \t{\n-\t  if (group_size < nunits\n-\t      && nunits % group_size == 0)\n+\t  if (group_size < const_nunits\n+\t      && const_nunits % group_size == 0)\n \t    {\n-\t      nstores = nunits / group_size;\n+\t      nstores = const_nunits / group_size;\n \t      lnel = group_size;\n \t      ltype = build_vector_type (elem_type, group_size);\n \t      lvectype = vectype;\n@@ -6099,17 +6125,17 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t  unsigned lsize\n \t\t    = group_size * GET_MODE_BITSIZE (elmode);\n \t\t  elmode = int_mode_for_size (lsize, 0).require ();\n+\t\t  unsigned int lnunits = const_nunits / group_size;\n \t\t  /* If we can't construct such a vector fall back to\n \t\t     element extracts from the original vector type and\n \t\t     element size stores.  */\n-\t\t  if (mode_for_vector (elmode,\n-\t\t\t\t       nunits / group_size).exists (&vmode)\n+\t\t  if (mode_for_vector (elmode, lnunits).exists (&vmode)\n \t\t      && VECTOR_MODE_P (vmode)\n \t\t      && (convert_optab_handler (vec_extract_optab,\n \t\t\t\t\t\t vmode, elmode)\n \t\t\t  != CODE_FOR_nothing))\n \t\t    {\n-\t\t      nstores = nunits / group_size;\n+\t\t      nstores = lnunits;\n \t\t      lnel = group_size;\n \t\t      ltype = build_nonstandard_integer_type (lsize, 1);\n \t\t      lvectype = build_vector_type (ltype, nstores);\n@@ -6121,11 +6147,11 @@ vectorizable_store (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t     issue exists here for reasonable archs.  */\n \t\t}\n \t    }\n-\t  else if (group_size >= nunits\n-\t\t   && group_size % nunits == 0)\n+\t  else if (group_size >= const_nunits\n+\t\t   && group_size % const_nunits == 0)\n \t    {\n \t      nstores = 1;\n-\t      lnel = nunits;\n+\t      lnel = const_nunits;\n \t      ltype = vectype;\n \t      lvectype = vectype;\n \t    }\n@@ -6680,8 +6706,9 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n   tree dataref_offset = NULL_TREE;\n   gimple *ptr_incr = NULL;\n   int ncopies;\n-  int i, j, group_size;\n-  poly_int64 group_gap_adj;\n+  int i, j;\n+  unsigned int group_size;\n+  poly_uint64 group_gap_adj;\n   tree msq = NULL_TREE, lsq;\n   tree offset = NULL_TREE;\n   tree byte_offset = NULL_TREE;\n@@ -6735,7 +6762,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n     return false;\n \n   tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n-  int nunits = TYPE_VECTOR_SUBPARTS (vectype);\n+  poly_uint64 nunits = TYPE_VECTOR_SUBPARTS (vectype);\n \n   if (loop_vinfo)\n     {\n@@ -6874,32 +6901,38 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n       gimple_seq seq;\n       basic_block new_bb;\n       enum { NARROW, NONE, WIDEN } modifier;\n-      int gather_off_nunits = TYPE_VECTOR_SUBPARTS (gs_info.offset_vectype);\n+      poly_uint64 gather_off_nunits\n+\t= TYPE_VECTOR_SUBPARTS (gs_info.offset_vectype);\n \n-      if (nunits == gather_off_nunits)\n+      if (known_eq (nunits, gather_off_nunits))\n \tmodifier = NONE;\n-      else if (nunits == gather_off_nunits / 2)\n+      else if (known_eq (nunits * 2, gather_off_nunits))\n \t{\n \t  modifier = WIDEN;\n \n-\t  vec_perm_builder sel (gather_off_nunits, gather_off_nunits, 1);\n-\t  for (i = 0; i < gather_off_nunits; ++i)\n-\t    sel.quick_push (i | nunits);\n+\t  /* Currently widening gathers are only supported for\n+\t     fixed-length vectors.  */\n+\t  int count = gather_off_nunits.to_constant ();\n+\t  vec_perm_builder sel (count, count, 1);\n+\t  for (i = 0; i < count; ++i)\n+\t    sel.quick_push (i | (count / 2));\n \n-\t  vec_perm_indices indices (sel, 1, gather_off_nunits);\n+\t  vec_perm_indices indices (sel, 1, count);\n \t  perm_mask = vect_gen_perm_mask_checked (gs_info.offset_vectype,\n \t\t\t\t\t\t  indices);\n \t}\n-      else if (nunits == gather_off_nunits * 2)\n+      else if (known_eq (nunits, gather_off_nunits * 2))\n \t{\n \t  modifier = NARROW;\n \n-\t  vec_perm_builder sel (nunits, nunits, 1);\n-\t  for (i = 0; i < nunits; ++i)\n-\t    sel.quick_push (i < gather_off_nunits\n-\t\t\t    ? i : i + nunits - gather_off_nunits);\n+\t  /* Currently narrowing gathers are only supported for\n+\t     fixed-length vectors.  */\n+\t  int count = nunits.to_constant ();\n+\t  vec_perm_builder sel (count, count, 1);\n+\t  for (i = 0; i < count; ++i)\n+\t    sel.quick_push (i < count / 2 ? i : i + count / 2);\n \n-\t  vec_perm_indices indices (sel, 2, nunits);\n+\t  vec_perm_indices indices (sel, 2, count);\n \t  perm_mask = vect_gen_perm_mask_checked (vectype, indices);\n \t  ncopies *= 2;\n \t}\n@@ -7047,6 +7080,8 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n       vec<constructor_elt, va_gc> *v = NULL;\n       gimple_seq stmts = NULL;\n       tree stride_base, stride_step, alias_off;\n+      /* Checked by get_load_store_type.  */\n+      unsigned int const_nunits = nunits.to_constant ();\n \n       gcc_assert (!nested_in_vect_loop);\n \n@@ -7108,14 +7143,14 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n       prev_stmt_info = NULL;\n       running_off = offvar;\n       alias_off = build_int_cst (ref_type, 0);\n-      int nloads = nunits;\n+      int nloads = const_nunits;\n       int lnel = 1;\n       tree ltype = TREE_TYPE (vectype);\n       tree lvectype = vectype;\n       auto_vec<tree> dr_chain;\n       if (memory_access_type == VMAT_STRIDED_SLP)\n \t{\n-\t  if (group_size < nunits)\n+\t  if (group_size < const_nunits)\n \t    {\n \t      /* First check if vec_init optab supports construction from\n \t\t vector elts directly.  */\n@@ -7127,7 +7162,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t\t\t\t     TYPE_MODE (vectype), vmode)\n \t\t      != CODE_FOR_nothing))\n \t\t{\n-\t\t  nloads = nunits / group_size;\n+\t\t  nloads = const_nunits / group_size;\n \t\t  lnel = group_size;\n \t\t  ltype = build_vector_type (TREE_TYPE (vectype), group_size);\n \t\t}\n@@ -7143,15 +7178,15 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t\t  unsigned lsize\n \t\t    = group_size * TYPE_PRECISION (TREE_TYPE (vectype));\n \t\t  elmode = int_mode_for_size (lsize, 0).require ();\n+\t\t  unsigned int lnunits = const_nunits / group_size;\n \t\t  /* If we can't construct such a vector fall back to\n \t\t     element loads of the original vector type.  */\n-\t\t  if (mode_for_vector (elmode,\n-\t\t\t\t       nunits / group_size).exists (&vmode)\n+\t\t  if (mode_for_vector (elmode, lnunits).exists (&vmode)\n \t\t      && VECTOR_MODE_P (vmode)\n \t\t      && (convert_optab_handler (vec_init_optab, vmode, elmode)\n \t\t\t  != CODE_FOR_nothing))\n \t\t    {\n-\t\t      nloads = nunits / group_size;\n+\t\t      nloads = lnunits;\n \t\t      lnel = group_size;\n \t\t      ltype = build_nonstandard_integer_type (lsize, 1);\n \t\t      lvectype = build_vector_type (ltype, nloads);\n@@ -7161,7 +7196,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t  else\n \t    {\n \t      nloads = 1;\n-\t      lnel = nunits;\n+\t      lnel = const_nunits;\n \t      ltype = vectype;\n \t    }\n \t  ltype = build_aligned_type (ltype, TYPE_ALIGN (TREE_TYPE (vectype)));\n@@ -7176,13 +7211,13 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t      /* We don't yet generate SLP_TREE_LOAD_PERMUTATIONs for\n \t\t variable VF.  */\n \t      unsigned int const_vf = vf.to_constant ();\n-\t      ncopies = (group_size * const_vf + nunits - 1) / nunits;\n+\t      ncopies = CEIL (group_size * const_vf, const_nunits);\n \t      dr_chain.create (ncopies);\n \t    }\n \t  else\n \t    ncopies = SLP_TREE_NUMBER_OF_VEC_STMTS (slp_node);\n \t}\n-      int group_el = 0;\n+      unsigned int group_el = 0;\n       unsigned HOST_WIDE_INT\n \telsz = tree_to_uhwi (TYPE_SIZE_UNIT (TREE_TYPE (vectype)));\n       for (j = 0; j < ncopies; j++)\n@@ -7297,7 +7332,8 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n \t      /* We don't yet generate SLP_TREE_LOAD_PERMUTATIONs for\n \t\t variable VF.  */\n \t      unsigned int const_vf = vf.to_constant ();\n-\t      vec_num = (group_size * const_vf + nunits - 1) / nunits;\n+\t      unsigned int const_nunits = nunits.to_constant ();\n+\t      vec_num = CEIL (group_size * const_vf, const_nunits);\n \t      group_gap_adj = vf * group_size - nunits * vec_num;\n \t    }\n \t  else\n@@ -7465,7 +7501,7 @@ vectorizable_load (gimple *stmt, gimple_stmt_iterator *gsi, gimple **vec_stmt,\n     aggr_type = vectype;\n \n   prev_stmt_info = NULL;\n-  int group_elt = 0;\n+  poly_uint64 group_elt = 0;\n   for (j = 0; j < ncopies; j++)\n     {\n       /* 1. Create the vector or array pointer update chain.  */"}]}