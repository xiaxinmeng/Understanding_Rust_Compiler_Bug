{"sha": "fb768529d28e74ceca93efdd2e0a6ada3bb141fe", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZmI3Njg1MjlkMjhlNzRjZWNhOTNlZmRkMmUwYTZhZGEzYmIxNDFmZQ==", "commit": {"author": {"name": "Richard Biener", "email": "rguenther@suse.de", "date": "2020-01-08T14:30:44Z"}, "committer": {"name": "Richard Biener", "email": "rguenth@gcc.gnu.org", "date": "2020-01-08T14:30:44Z"}, "message": "re PR tree-optimization/93199 (Compile time hog in sink_clobbers)\n\n2020-01-08  Richard Biener  <rguenther@suse.de>\n\n\tPR middle-end/93199\n\t* tree-eh.c (sink_clobbers): Update virtual operands for\n\tthe first and last stmt only.  Add a dry-run capability.\n\t(pass_lower_eh_dispatch::execute): Perform clobber sinking\n\tafter CFG manipulations and in RPO order to catch all\n\tsecondary opportunities reliably.\n\nFrom-SVN: r280006", "tree": {"sha": "f694066dffc2103638c47c3d9e476cf5f2e69fd3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/f694066dffc2103638c47c3d9e476cf5f2e69fd3"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/fb768529d28e74ceca93efdd2e0a6ada3bb141fe", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fb768529d28e74ceca93efdd2e0a6ada3bb141fe", "html_url": "https://github.com/Rust-GCC/gccrs/commit/fb768529d28e74ceca93efdd2e0a6ada3bb141fe", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fb768529d28e74ceca93efdd2e0a6ada3bb141fe/comments", "author": {"login": "rguenth", "id": 2046526, "node_id": "MDQ6VXNlcjIwNDY1MjY=", "avatar_url": "https://avatars.githubusercontent.com/u/2046526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rguenth", "html_url": "https://github.com/rguenth", "followers_url": "https://api.github.com/users/rguenth/followers", "following_url": "https://api.github.com/users/rguenth/following{/other_user}", "gists_url": "https://api.github.com/users/rguenth/gists{/gist_id}", "starred_url": "https://api.github.com/users/rguenth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rguenth/subscriptions", "organizations_url": "https://api.github.com/users/rguenth/orgs", "repos_url": "https://api.github.com/users/rguenth/repos", "events_url": "https://api.github.com/users/rguenth/events{/privacy}", "received_events_url": "https://api.github.com/users/rguenth/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "949f00625424f0b95dea8be73f00e6bb5fe90294", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/949f00625424f0b95dea8be73f00e6bb5fe90294", "html_url": "https://github.com/Rust-GCC/gccrs/commit/949f00625424f0b95dea8be73f00e6bb5fe90294"}], "stats": {"total": 94, "additions": 66, "deletions": 28}, "files": [{"sha": "b733b56059e932fd6c68528a9315b204e1659543", "filename": "gcc/ChangeLog", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fb768529d28e74ceca93efdd2e0a6ada3bb141fe/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fb768529d28e74ceca93efdd2e0a6ada3bb141fe/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=fb768529d28e74ceca93efdd2e0a6ada3bb141fe", "patch": "@@ -1,3 +1,12 @@\n+2020-01-08  Richard Biener  <rguenther@suse.de>\n+\n+\tPR middle-end/93199\n+\t* tree-eh.c (sink_clobbers): Update virtual operands for\n+\tthe first and last stmt only.  Add a dry-run capability.\n+\t(pass_lower_eh_dispatch::execute): Perform clobber sinking\n+\tafter CFG manipulations and in RPO order to catch all\n+\tsecondary opportunities reliably.\n+\n 2020-01-08  Georg-Johann Lay  <avr@gjlay.de>\n \n \tPR target/93182"}, {"sha": "f25d2de8d9f34376e979e0f37448d831c14b181f", "filename": "gcc/tree-eh.c", "status": "modified", "additions": 57, "deletions": 28, "changes": 85, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/fb768529d28e74ceca93efdd2e0a6ada3bb141fe/gcc%2Ftree-eh.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/fb768529d28e74ceca93efdd2e0a6ada3bb141fe/gcc%2Ftree-eh.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-eh.c?ref=fb768529d28e74ceca93efdd2e0a6ada3bb141fe", "patch": "@@ -3550,10 +3550,11 @@ optimize_clobbers (basic_block bb)\n }\n \n /* Try to sink var = {v} {CLOBBER} stmts followed just by\n-   internal throw to successor BB.  */\n+   internal throw to successor BB.  If FOUND_OPPORTUNITY is not NULL\n+   then do not perform the optimization but set *FOUND_OPPORTUNITY to true.  */\n \n static int\n-sink_clobbers (basic_block bb)\n+sink_clobbers (basic_block bb, bool *found_opportunity = NULL)\n {\n   edge e;\n   edge_iterator ei;\n@@ -3591,25 +3592,32 @@ sink_clobbers (basic_block bb)\n   if (!any_clobbers)\n     return 0;\n \n+  /* If this was a dry run, tell it we found clobbers to sink.  */\n+  if (found_opportunity)\n+    {\n+      *found_opportunity = true;\n+      return 0;\n+    }\n+\n   edge succe = single_succ_edge (bb);\n   succbb = succe->dest;\n \n   /* See if there is a virtual PHI node to take an updated virtual\n      operand from.  */\n   gphi *vphi = NULL;\n-  tree vuse = NULL_TREE;\n   for (gphi_iterator gpi = gsi_start_phis (succbb);\n        !gsi_end_p (gpi); gsi_next (&gpi))\n     {\n       tree res = gimple_phi_result (gpi.phi ());\n       if (virtual_operand_p (res))\n \t{\n \t  vphi = gpi.phi ();\n-\t  vuse = res;\n \t  break;\n \t}\n     }\n \n+  gimple *first_sunk = NULL;\n+  gimple *last_sunk = NULL;\n   dgsi = gsi_after_labels (succbb);\n   gsi = gsi_last_bb (bb);\n   for (gsi_prev (&gsi); !gsi_end_p (gsi); gsi_prev (&gsi))\n@@ -3641,36 +3649,37 @@ sink_clobbers (basic_block bb)\n          forwarder edge we can keep virtual operands in place.  */\n       gsi_remove (&gsi, false);\n       gsi_insert_before (&dgsi, stmt, GSI_NEW_STMT);\n-\n-      /* But adjust virtual operands if we sunk across a PHI node.  */\n-      if (vuse)\n+      if (!first_sunk)\n+\tfirst_sunk = stmt;\n+      last_sunk = stmt;\n+    }\n+  if (first_sunk)\n+    {\n+      /* Adjust virtual operands if we sunk across a virtual PHI.  */\n+      if (vphi)\n \t{\n-\t  gimple *use_stmt;\n \t  imm_use_iterator iter;\n \t  use_operand_p use_p;\n-\t  FOR_EACH_IMM_USE_STMT (use_stmt, iter, vuse)\n+\t  gimple *use_stmt;\n+\t  tree phi_def = gimple_phi_result (vphi);\n+\t  FOR_EACH_IMM_USE_STMT (use_stmt, iter, phi_def)\n \t    FOR_EACH_IMM_USE_ON_STMT (use_p, iter)\n-\t      SET_USE (use_p, gimple_vdef (stmt));\n-\t  if (SSA_NAME_OCCURS_IN_ABNORMAL_PHI (vuse))\n+              SET_USE (use_p, gimple_vdef (first_sunk));\n+\t  if (SSA_NAME_OCCURS_IN_ABNORMAL_PHI (phi_def))\n \t    {\n-\t      SSA_NAME_OCCURS_IN_ABNORMAL_PHI (gimple_vdef (stmt)) = 1;\n-\t      SSA_NAME_OCCURS_IN_ABNORMAL_PHI (vuse) = 0;\n+\t      SSA_NAME_OCCURS_IN_ABNORMAL_PHI (gimple_vdef (first_sunk)) = 1;\n+\t      SSA_NAME_OCCURS_IN_ABNORMAL_PHI (phi_def) = 0;\n \t    }\n-\t  /* Adjust the incoming virtual operand.  */\n-\t  SET_USE (PHI_ARG_DEF_PTR_FROM_EDGE (vphi, succe), gimple_vuse (stmt));\n-\t  SET_USE (gimple_vuse_op (stmt), vuse);\n+\t  SET_USE (PHI_ARG_DEF_PTR_FROM_EDGE (vphi, succe),\n+\t\t   gimple_vuse (last_sunk));\n+\t  SET_USE (gimple_vuse_op (last_sunk), phi_def);\n \t}\n       /* If there isn't a single predecessor but no virtual PHI node\n          arrange for virtual operands to be renamed.  */\n-      else if (gimple_vuse_op (stmt) != NULL_USE_OPERAND_P\n-\t       && !single_pred_p (succbb))\n+      else if (!single_pred_p (succbb)\n+\t       && TREE_CODE (gimple_vuse (last_sunk)) == SSA_NAME)\n \t{\n-\t  /* In this case there will be no use of the VDEF of this stmt. \n-\t     ???  Unless this is a secondary opportunity and we have not\n-\t     removed unreachable blocks yet, so we cannot assert this.  \n-\t     Which also means we will end up renaming too many times.  */\n-\t  SET_USE (gimple_vuse_op (stmt), gimple_vop (cfun));\n-\t  mark_virtual_operands_for_renaming (cfun);\n+\t  mark_virtual_operand_for_renaming (gimple_vuse (last_sunk));\n \t  todo |= TODO_update_ssa_only_virtuals;\n \t}\n     }\n@@ -3863,6 +3872,7 @@ pass_lower_eh_dispatch::execute (function *fun)\n   basic_block bb;\n   int flags = 0;\n   bool redirected = false;\n+  bool any_resx_to_process = false;\n \n   assign_filter_values ();\n \n@@ -3879,18 +3889,37 @@ pass_lower_eh_dispatch::execute (function *fun)\n \t}\n       else if (gimple_code (last) == GIMPLE_RESX)\n \t{\n-\t  if (stmt_can_throw_external (cfun, last))\n+\t  if (stmt_can_throw_external (fun, last))\n \t    optimize_clobbers (bb);\n-\t  else\n-\t    flags |= sink_clobbers (bb);\n+\t  else if (!any_resx_to_process)\n+\t    sink_clobbers (bb, &any_resx_to_process);\n \t}\n     }\n-\n   if (redirected)\n     {\n       free_dominance_info (CDI_DOMINATORS);\n       delete_unreachable_blocks ();\n     }\n+\n+  if (any_resx_to_process)\n+    {\n+      /* Make sure to catch all secondary sinking opportunities by processing\n+\t blocks in RPO order and after all CFG modifications from lowering\n+\t and unreachable block removal.  */\n+      int *rpo = XNEWVEC  (int, n_basic_blocks_for_fn (fun));\n+      int rpo_n = pre_and_rev_post_order_compute_fn (fun, NULL, rpo, false);\n+      for (int i = 0; i < rpo_n; ++i)\n+\t{\n+\t  bb = BASIC_BLOCK_FOR_FN (fun, rpo[i]);\n+\t  gimple *last = last_stmt (bb);\n+\t  if (last\n+\t      && gimple_code (last) == GIMPLE_RESX\n+\t      && !stmt_can_throw_external (fun, last))\n+\t    flags |= sink_clobbers (bb);\n+\t}\n+      free (rpo);\n+    }\n+\n   return flags;\n }\n "}]}