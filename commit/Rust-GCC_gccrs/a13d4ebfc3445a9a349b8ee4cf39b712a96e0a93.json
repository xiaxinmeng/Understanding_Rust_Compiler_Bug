{"sha": "a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YTEzZDRlYmZjMzQ0NWE5YTM0OWI4ZWU0Y2YzOWI3MTJhOTZlMGE5Mw==", "commit": {"author": {"name": "Andrew MacLeod", "email": "amacleod@redhat.com", "date": "2001-04-09T14:27:05Z"}, "committer": {"name": "Andrew Macleod", "email": "amacleod@gcc.gnu.org", "date": "2001-04-09T14:27:05Z"}, "message": "alias.c (get_addr): Externalize.\n\n2001-04-09  Andrew MacLeod  <amacleod@redhat.com>\n\t    Jeff Law  <law@cygnus.com>\n\n\t* alias.c (get_addr): Externalize.\n\t(canon_true_dependence): New function. Behaves like true_dependance\n\texcept it already assumes a MEM has been canonicalized.\n\t* flags.h (flag_gcse_lm, flag_gcse_sm): New optimization flags.\n\t* gcse.c (struct ls_expr): Add load/store expressions structure.\n\t(modify_mem_list, canon_modify_mem_list): New variable.\n\t(gcse_main): Initialize & finalize alias analysis. Use enhanced\n\tload motion and store motion if requested.\n\t(alloc_gcse_mem): Allocate space for modify_mem_list array.\n\t(free_gcse_mem): Free the modify_mem_list array.\n\t(oprs_unchanged_p): Use load_killed_in_block_p.\n\t(gcse_mems_conflict_p, gcse_mem_operand): New variables.\n\t(mems_conflict_for_gcse_p): New function.  Don't kill loads\n\twith stores to themselves if its in the load/store expression list.\n\t(load_killed_in_block_p): New function.\n\t(canon_list_insert): New Function.\n\t(record_last_mem_set_info): Keep a list of all instructions which\n\tcan modify memory for each basic block.\n\t(compute_hash_table, reset_opr_set_tables): Clear modify_mem_list.\n\t(oprs_not_set_p): Use load_killed_in_block_p.\n\t(mark_call, mark_set, mark_clobber): Use record_last_mem_set_info.\n\t(expr_killed_p): Use load_killed_in_block_p.\n\t(compute_transp): Do not pessimize memory references.\n\t(pre_edge_insert): Update stores for a load motion expression.\n\t(one_pre_gcse_pass): Check loads/stores for extra load motion.\n\t(ldst_entry): Find or create a ldst_expr structure.\n\t(free_ldst_entry): Free memory for an individual item.\n\t(free_ldst_mems): Free entire load/store expression list.\n\t(print_ldst_list): Print debug info.\n\t(find_rtx_in_ldst): Try to find an rtx expression in the ldst list.\n\t(enumerate_ldsts): Assign integer values to each entry in list.\n\t(first_ls_expr): First expression in the list.\n\t(next_ls_expr): Next expression in the list.\n\t(simple_mem): Check if expression qualifies for ld/st expression list.\n\t(invalidate_any_buried_refs): Remove from expression list if its\n\tused in some other way we dont understand.\n\t(compute_ld_motion_mems): Find all potential enhanced load motion\n\texpression.\n\t(trim_ld_motion_mems): Remove any expressions which are invalid.\n\t(update_ld_motion_stores): Copy store values to registers for loads\n\twhich have been moved.\n\t(regvec, st_antloc, num_store): New global statics.\n\t(reg_set_info): Marks registers as set.\n\t(store_ops_ok): Verfies registers expressions are valid in a block.\n\t(find_moveable_store): Look for moveable stores in a pattern.\n\t(compute_store_table): Find stores in a function worth moving, maybe.\n\t(load_kills_store): Check dependance of a load and store.\n\t(find_loads): Find any loads in a pattern.\n\t(store_killed_in_insn): Check if a store is killed in an insn.\n\t(store_killed_after): Check is store killed after an insn in a block.\n\t(store_killed_before): Check is store killed before an insn in a block.\n\t(build_store_vectors): Generate the antic and avail vectors.\n\t(insert_insn_start_bb): Insert at the start of a BB, update BLOCK_HEAD.\n\t(insert_store): Add a store to an edge.\n\t(replace_store_insn): Replace a store with a SET insn.\n\t(delete_store): Delete a store insn.\n\t(free_store_memory): Free memory.\n\t(store_motion): Perform store motion.\n\t* invoke.texi: Add documentation for -fcse-lm and -fgcse-sm.\n\t* rtl.h (get_addr, canon_true_dependence): Add prototypes.\n\t* toplev.c (flag_gcse_lm, flag_gcse_sm): New Variables.\n\t(f_options): Add gcse-lm and gcse-sm.\n\nCo-Authored-By: Jeff Law <law@redhat.com>\n\nFrom-SVN: r41207", "tree": {"sha": "cf7c04fbaaa5ba499552edce36a0bf5ddf9db722", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/cf7c04fbaaa5ba499552edce36a0bf5ddf9db722"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/comments", "author": null, "committer": null, "parents": [{"sha": "92d0fb09386debda7597d86c80c4d8044c5362e2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/92d0fb09386debda7597d86c80c4d8044c5362e2", "html_url": "https://github.com/Rust-GCC/gccrs/commit/92d0fb09386debda7597d86c80c4d8044c5362e2"}], "stats": {"total": 1672, "additions": 1669, "deletions": 3}, "files": [{"sha": "88d17f154f1635eb6c6886145e74153be13135fd", "filename": "gcc/ChangeLog", "status": "modified", "additions": 66, "deletions": 0, "changes": 66, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93", "patch": "@@ -1,3 +1,69 @@\n+2001-04-09  Andrew MacLeod  <amacleod@redhat.com>\n+\t    Jeff Law  <law@redhat.com>\n+\n+\t* alias.c (get_addr): Externalize.\n+\t(canon_true_dependence): New function. Behaves like true_dependance\n+\texcept it already assumes a MEM has been canonicalized.\n+\t* flags.h (flag_gcse_lm, flag_gcse_sm): New optimization flags.\n+\t* gcse.c (struct ls_expr): Add load/store expressions structure.\n+\t(modify_mem_list, canon_modify_mem_list): New variable.\n+\t(gcse_main): Initialize & finalize alias analysis. Use enhanced \n+\tload motion and store motion if requested.\n+\t(alloc_gcse_mem): Allocate space for modify_mem_list array.\n+\t(free_gcse_mem): Free the modify_mem_list array.\n+\t(oprs_unchanged_p): Use load_killed_in_block_p.\n+\t(gcse_mems_conflict_p, gcse_mem_operand): New variables.\n+\t(mems_conflict_for_gcse_p): New function.  Don't kill loads \n+\twith stores to themselves if its in the load/store expression list.\n+\t(load_killed_in_block_p): New function.\n+\t(canon_list_insert): New Function.\n+\t(record_last_mem_set_info): Keep a list of all instructions which\n+\tcan modify memory for each basic block.\n+\t(compute_hash_table, reset_opr_set_tables): Clear modify_mem_list.\n+\t(oprs_not_set_p): Use load_killed_in_block_p.\n+\t(mark_call, mark_set, mark_clobber): Use record_last_mem_set_info.\n+\t(expr_killed_p): Use load_killed_in_block_p.\n+\t(compute_transp): Do not pessimize memory references.\n+\t(pre_edge_insert): Update stores for a load motion expression.\n+\t(one_pre_gcse_pass): Check loads/stores for extra load motion.\n+\t(ldst_entry): Find or create a ldst_expr structure.\n+\t(free_ldst_entry): Free memory for an individual item.\n+\t(free_ldst_mems): Free entire load/store expression list.\n+\t(print_ldst_list): Print debug info.\n+\t(find_rtx_in_ldst): Try to find an rtx expression in the ldst list.\n+\t(enumerate_ldsts): Assign integer values to each entry in list.\n+\t(first_ls_expr): First expression in the list.\n+\t(next_ls_expr): Next expression in the list.\n+\t(simple_mem): Check if expression qualifies for ld/st expression list.\n+\t(invalidate_any_buried_refs): Remove from expression list if its\n+\tused in some other way we dont understand.\n+\t(compute_ld_motion_mems): Find all potential enhanced load motion\n+\texpression.\n+\t(trim_ld_motion_mems): Remove any expressions which are invalid.\n+\t(update_ld_motion_stores): Copy store values to registers for loads\n+\twhich have been moved.\n+\t(regvec, st_antloc, num_store): New global statics.\n+\t(reg_set_info): Marks registers as set.\n+\t(store_ops_ok): Verfies registers expressions are valid in a block.\n+\t(find_moveable_store): Look for moveable stores in a pattern.\n+\t(compute_store_table): Find stores in a function worth moving, maybe.\n+\t(load_kills_store): Check dependance of a load and store.\n+\t(find_loads): Find any loads in a pattern.\n+\t(store_killed_in_insn): Check if a store is killed in an insn.\n+\t(store_killed_after): Check is store killed after an insn in a block.\n+\t(store_killed_before): Check is store killed before an insn in a block.\n+\t(build_store_vectors): Generate the antic and avail vectors.\n+\t(insert_insn_start_bb): Insert at the start of a BB, update BLOCK_HEAD.\n+\t(insert_store): Add a store to an edge.\n+\t(replace_store_insn): Replace a store with a SET insn.\n+\t(delete_store): Delete a store insn.\n+\t(free_store_memory): Free memory.\n+\t(store_motion): Perform store motion.\n+\t* invoke.texi: Add documentation for -fcse-lm and -fgcse-sm.\n+\t* rtl.h (get_addr, canon_true_dependence): Add prototypes.\n+\t* toplev.c (flag_gcse_lm, flag_gcse_sm): New Variables.\n+\t(f_options): Add gcse-lm and gcse-sm.\n+\n Mon Apr  9 16:18:03 CEST 2001  Jan Hubicka  <jh@suse.cz>\n \n \t* i386.c (expand_fp_movcc): Fix condition reversal code."}, {"sha": "8792da690e299ce9dcc945151c8c511ce4561fba", "filename": "gcc/alias.c", "status": "modified", "additions": 59, "deletions": 2, "changes": 61, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/gcc%2Falias.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/gcc%2Falias.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Falias.c?ref=a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93", "patch": "@@ -87,7 +87,7 @@ typedef struct alias_set_entry\n \n static int rtx_equal_for_memref_p\tPARAMS ((rtx, rtx));\n static rtx find_symbolic_term\t\tPARAMS ((rtx));\n-static rtx get_addr\t\t\tPARAMS ((rtx));\n+rtx get_addr\t\t\t\tPARAMS ((rtx));\n static int memrefs_conflict_p\t\tPARAMS ((int, rtx, int, rtx,\n \t\t\t\t\t\t HOST_WIDE_INT));\n static void record_set\t\t\tPARAMS ((rtx, rtx, void *));\n@@ -1340,7 +1340,7 @@ base_alias_check (x, y, x_mode, y_mode)\n    it unchanged unless it is a value; in the latter case we call cselib to get\n    a more useful rtx.  */\n \n-static rtx\n+rtx\n get_addr (x)\n      rtx x;\n {\n@@ -1765,6 +1765,63 @@ true_dependence (mem, mem_mode, x, varies)\n \t\t\t\t\t      varies);\n }\n \n+/* Canonical true dependence: X is read after store in MEM takes place.\n+   Variant of true_dependece which assumes MEM has already been \n+   canonicalized (hence we no longer do that here).  \n+   The mem_addr argument has been added, since true_dependence computed \n+   this value prior to canonicalizing.  */\n+\n+int\n+canon_true_dependence (mem, mem_mode, mem_addr, x, varies)\n+     rtx mem, mem_addr, x;\n+     enum machine_mode mem_mode;\n+     int (*varies) PARAMS ((rtx, int));\n+{\n+  register rtx x_addr;\n+\n+  if (MEM_VOLATILE_P (x) && MEM_VOLATILE_P (mem))\n+    return 1;\n+\n+  if (DIFFERENT_ALIAS_SETS_P (x, mem))\n+    return 0;\n+\n+  /* If X is an unchanging read, then it can't possibly conflict with any\n+     non-unchanging store.  It may conflict with an unchanging write though,\n+     because there may be a single store to this address to initialize it.\n+     Just fall through to the code below to resolve the case where we have\n+     both an unchanging read and an unchanging write.  This won't handle all\n+     cases optimally, but the possible performance loss should be\n+     negligible.  */\n+  if (RTX_UNCHANGING_P (x) && ! RTX_UNCHANGING_P (mem))\n+    return 0;\n+\n+  x_addr = get_addr (XEXP (x, 0));\n+\n+  if (! base_alias_check (x_addr, mem_addr, GET_MODE (x), mem_mode))\n+    return 0;\n+\n+  x_addr = canon_rtx (x_addr);\n+  if (! memrefs_conflict_p (GET_MODE_SIZE (mem_mode), mem_addr,\n+\t\t\t    SIZE_FOR_MODE (x), x_addr, 0))\n+    return 0;\n+\n+  if (aliases_everything_p (x))\n+    return 1;\n+\n+  /* We cannot use aliases_everyting_p to test MEM, since we must look\n+     at MEM_MODE, rather than GET_MODE (MEM).  */\n+  if (mem_mode == QImode || GET_CODE (mem_addr) == AND)\n+    return 1;\n+\n+  /* In true_dependence we also allow BLKmode to alias anything.  Why\n+     don't we do this in anti_dependence and output_dependence?  */\n+  if (mem_mode == BLKmode || GET_MODE (x) == BLKmode)\n+    return 1;\n+\n+  return ! fixed_scalar_and_varying_struct_p (mem, x, mem_addr, x_addr,\n+\t\t\t\t\t      varies);\n+}\n+\n /* Returns non-zero if a write to X might alias a previous read from\n    (or, if WRITEP is non-zero, a write to) MEM.  */\n "}, {"sha": "d637826b918266efcbb7bce0b93fafcccf04942b", "filename": "gcc/flags.h", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/gcc%2Fflags.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/gcc%2Fflags.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fflags.h?ref=a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93", "patch": "@@ -611,6 +611,15 @@ extern enum graph_dump_types graph_dump_format;\n \n extern int flag_no_ident;\n \n+/* Nonzero if we want to perform enhanced load motion during gcse.  */\n+\n+extern int flag_gcse_lm;\n+\n+/* Nonzero if we want to perform store motion after gcse.  */\n+\n+extern int flag_gcse_sm;\n+\n+\n /* Nonzero means we should do dwarf2 duplicate elimination.  */\n \n extern int flag_eliminate_dwarf2_dups;"}, {"sha": "f8dc557693010b7614bdc282055dc270d2ee510d", "filename": "gcc/gcse.c", "status": "modified", "additions": 1503, "deletions": 0, "changes": 1503, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/gcc%2Fgcse.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/gcc%2Fgcse.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgcse.c?ref=a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93", "patch": "@@ -454,6 +454,33 @@ static int reg_set_table_size;\n /* Amount to grow `reg_set_table' by when it's full.  */\n #define REG_SET_TABLE_SLOP 100\n \n+/* This is a list of expressions which are MEMs and will be used by load\n+   or store motion. \n+   Load motion tracks MEMs which aren't killed by\n+   anything except itself. (ie, loads and stores to a single location).\n+   We can then allow movement of these MEM refs with a little special \n+   allowance. (all stores copy the same value to the reaching reg used\n+   for the loads).  This means all values used to store into memory must have\n+   no side effects so we can re-issue the setter value.  \n+   Store Motion uses this structure as an expression table to track stores\n+   which look interesting, and might be moveable towards the exit block.  */\n+\n+struct ls_expr\n+{\n+  struct expr * expr;\t\t/* Gcse expression reference for LM.  */\n+  rtx pattern;\t\t\t/* Pattern of this mem.  */\n+  rtx loads;\t\t\t/* INSN list of loads seen.  */\n+  rtx stores;\t\t\t/* INSN list of stores seen.  */\n+  struct ls_expr * next;\t/* Next in the list.  */\n+  int invalid;\t\t\t/* Invalid for some reason.  */\n+  int index;\t\t\t/* If it maps to a bitmap index.  */\n+  int hash_index;\t\t/* Index when in a hash table.  */\n+  rtx reaching_reg;\t\t/* Register to use when re-writing.  */\n+};\n+\n+/* Head of the list of load/store memory refs.  */\n+static struct ls_expr * pre_ldst_mems = NULL;\n+\n /* Bitmap containing one bit for each register in the program.\n    Used when performing GCSE to track which registers have been set since\n    the start of the basic block.  */\n@@ -466,6 +493,13 @@ static sbitmap reg_set_bitmap;\n    gcse) and it's currently not easy to realloc sbitmap vectors.  */\n static sbitmap *reg_set_in_block;\n \n+/* Array, indexed by basic block number for a list of insns which modify\n+   memory within that block.  */\n+static rtx * modify_mem_list;\n+\n+/* This array parallels modify_mem_list, but is kept canonicalized.  */\n+static rtx * canon_modify_mem_list;\n+\n /* For each block, non-zero if memory is set in that block.\n    This is computed during hash table computation and is used by\n    expr_killed_p and compute_transp.\n@@ -588,6 +622,9 @@ static int cprop_jump\t\tPARAMS ((rtx, rtx, rtx));\n #ifdef HAVE_cc0\n static int cprop_cc0_jump\tPARAMS ((rtx, struct reg_use *, rtx));\n #endif\n+static void mems_conflict_for_gcse_p PARAMS ((rtx, rtx, void *));\n+static int load_killed_in_block_p    PARAMS ((int, int, rtx, int));\n+static void canon_list_insert        PARAMS ((rtx, rtx, void *));\n static int cprop_insn\t\tPARAMS ((rtx, int));\n static int cprop\t\tPARAMS ((int));\n static int one_cprop_pass\tPARAMS ((int, int));\n@@ -637,6 +674,35 @@ static int expr_reaches_here_p_work PARAMS ((struct occr *, struct expr *,\n \t\t\t\t\t     int, int, char *));\n static int pre_expr_reaches_here_p_work\tPARAMS ((int, struct expr *,\n \t\t\t\t\t\t int, char *));\n+static struct ls_expr * ldst_entry \tPARAMS ((rtx));\n+static void free_ldst_entry \t\tPARAMS ((struct ls_expr *));\n+static void free_ldst_mems\t\tPARAMS ((void));\n+static void print_ldst_list \t\tPARAMS ((FILE *));\n+static struct ls_expr * find_rtx_in_ldst PARAMS ((rtx));\n+static int enumerate_ldsts\t\tPARAMS ((void));\n+static inline struct ls_expr * first_ls_expr PARAMS ((void));\n+static inline struct ls_expr * next_ls_expr  PARAMS ((struct ls_expr *));\n+static int simple_mem\t\t\tPARAMS ((rtx));\n+static void invalidate_any_buried_refs\tPARAMS ((rtx));\n+static void compute_ld_motion_mems\tPARAMS ((void)); \n+static void trim_ld_motion_mems\t\tPARAMS ((void));\n+static void update_ld_motion_stores\tPARAMS ((struct expr *));\n+static void reg_set_info\t\tPARAMS ((rtx, rtx, void *));\n+static int store_ops_ok\t\t\tPARAMS ((rtx, int));\n+static void find_moveable_store\t\tPARAMS ((rtx));\n+static int compute_store_table\t\tPARAMS ((void));\n+static int load_kills_store\t\tPARAMS ((rtx, rtx));\n+static int find_loads\t\t\tPARAMS ((rtx, rtx));\n+static int store_killed_in_insn\t\tPARAMS ((rtx, rtx));\n+static int store_killed_after\t\tPARAMS ((rtx, rtx, int));\n+static int store_killed_before\t\tPARAMS ((rtx, rtx, int));\n+static void build_store_vectors\t\tPARAMS ((void));\n+static void insert_insn_start_bb\tPARAMS ((rtx, int));\n+static int insert_store\t\t\tPARAMS ((struct ls_expr *, edge));\n+static void replace_store_insn\t\tPARAMS ((rtx, rtx, int));\n+static void delete_store\t\tPARAMS ((struct ls_expr *, int));\n+static void free_store_memory\t\tPARAMS ((void));\n+static void store_motion\t\tPARAMS ((void));\n \f\n /* Entry point for global common subexpression elimination.\n    F is the first instruction in the function.  */\n@@ -654,6 +720,10 @@ gcse_main (f, file)\n   /* Point to release obstack data from for each pass.  */\n   char *gcse_obstack_bottom;\n \n+  /* Insertion of instructions on edges can create new basic blocks; we\n+     need the original basic block count so that we can properly deallocate\n+     arrays sized on the number of basic blocks originally in the cfg.  */\n+  int orig_bb_count;\n   /* We do not construct an accurate cfg in functions which call\n      setjmp, so just punt to be safe.  */\n   if (current_function_calls_setjmp)\n@@ -673,6 +743,7 @@ gcse_main (f, file)\n   if (file)\n     dump_flow_info (file);\n \n+  orig_bb_count = n_basic_blocks;\n   /* Return if there's nothing to do.  */\n   if (n_basic_blocks <= 1)\n     return 0;\n@@ -703,6 +774,8 @@ gcse_main (f, file)\n   gcc_obstack_init (&gcse_obstack);\n   bytes_used = 0;\n \n+  /* We need alias.  */\n+  init_alias_analysis ();\n   /* Record where pseudo-registers are set.  This data is kept accurate\n      during each pass.  ??? We could also record hard-reg information here\n      [since it's unchanging], however it is currently done during hash table\n@@ -744,6 +817,28 @@ gcse_main (f, file)\n       else\n         {\n \t  changed |= one_pre_gcse_pass (pass + 1);\n+\t  /* We may have just created new basic blocks.  Release and\n+\t     recompute various things which are sized on the number of\n+\t     basic blocks.  */\n+\t  if (changed)\n+\t    {\n+\t      int i;\n+\n+\t      for (i = 0; i < orig_bb_count; i++)\n+\t        {\n+\t\t  if (modify_mem_list[i])\n+\t\t    free_INSN_LIST_list (modify_mem_list + i);\n+\t\t  if (canon_modify_mem_list[i])\n+\t\t    free_INSN_LIST_list (canon_modify_mem_list + i); \n+\t\t}\n+\t      modify_mem_list\n+\t\t= (rtx *) gmalloc (n_basic_blocks * sizeof (rtx *));\n+\t      canon_modify_mem_list\n+\t\t= (rtx *) gmalloc (n_basic_blocks * sizeof (rtx *));\n+\t      memset ((char *) modify_mem_list, 0, n_basic_blocks * sizeof (rtx *));\n+\t      memset ((char *) canon_modify_mem_list, 0, n_basic_blocks * sizeof (rtx *));\n+\t      orig_bb_count = n_basic_blocks;\n+\t    }\n \t  free_reg_set_mem ();\n \t  alloc_reg_set_mem (max_reg_num ());\n \t  compute_sets (f);\n@@ -804,6 +899,13 @@ gcse_main (f, file)\n \n   obstack_free (&gcse_obstack, NULL_PTR);\n   free_reg_set_mem ();\n+  /* We are finished with alias.  */\n+  end_alias_analysis ();\n+  allocate_reg_info (max_reg_num (), FALSE, FALSE);\n+\n+  if (!optimize_size && flag_gcse_sm)\n+    store_motion ();\n+  /* Record where pseudo-registers are set.  */\n   return run_jump_opt_after_gcse;\n }\n \f\n@@ -917,6 +1019,12 @@ alloc_gcse_mem (f)\n   reg_set_in_block = (sbitmap *) sbitmap_vector_alloc (n_basic_blocks,\n \t\t\t\t\t\t       max_gcse_regno);\n   mem_set_in_block = (char *) gmalloc (n_basic_blocks);\n+  /* Allocate array to keep a list of insns which modify memory in each\n+     basic block.  */\n+  modify_mem_list = (rtx *) gmalloc (n_basic_blocks * sizeof (rtx *));\n+  canon_modify_mem_list = (rtx *) gmalloc (n_basic_blocks * sizeof (rtx *));\n+  memset ((char *) modify_mem_list, 0, n_basic_blocks * sizeof (rtx *));\n+  memset ((char *) canon_modify_mem_list, 0, n_basic_blocks * sizeof (rtx *));\n }\n \n /* Free memory allocated by alloc_gcse_mem.  */\n@@ -931,6 +1039,23 @@ free_gcse_mem ()\n \n   free (reg_set_in_block);\n   free (mem_set_in_block);\n+  /* re-Cache any INSN_LIST nodes we have allocated.  */\n+  {\n+    int i;\n+\n+    for (i = 0; i < n_basic_blocks; i++)\n+      {\n+        if (modify_mem_list[i])\n+          free_INSN_LIST_list (modify_mem_list + i);\n+        if (canon_modify_mem_list[i])\n+          free_INSN_LIST_list (canon_modify_mem_list + i);\n+      }\n+\n+    free (modify_mem_list);\n+    free (canon_modify_mem_list);\n+    modify_mem_list = 0;\n+    canon_modify_mem_list = 0;\n+  }\n }\n \n /* Many of the global optimization algorithms work by solving dataflow\n@@ -1267,6 +1392,9 @@ oprs_unchanged_p (x, insn, avail_p)\n \t\t|| reg_first_set[REGNO (x)] >= INSN_CUID (insn));\n \n     case MEM:\n+      if (load_killed_in_block_p (BLOCK_NUM (insn), INSN_CUID (insn),\n+\t\t\t\t  x, avail_p))\n+\treturn 0;\n       if (avail_p && mem_last_set != NEVER_SET\n \t  && mem_last_set >= INSN_CUID (insn))\n \treturn 0;\n@@ -1321,6 +1449,105 @@ oprs_unchanged_p (x, insn, avail_p)\n   return 1;\n }\n \n+/* Used for communication between mems_conflict_for_gcse_p and\n+   load_killed_in_block_p.  Nonzero if mems_conflict_for_gcse_p finds a\n+   conflict between two memory references.  */\n+static int gcse_mems_conflict_p;\n+\n+/* Used for communication between mems_conflict_for_gcse_p and\n+   load_killed_in_block_p.  A memory reference for a load instruction,\n+   mems_conflict_for_gcse_p will see if a memory store conflicts with\n+   this memory load.  */\n+static rtx gcse_mem_operand;\n+\n+/* DEST is the output of an instruction.  If it is a memory reference, and\n+   possibly conflicts with the load found in gcse_mem_operand, then set\n+   gcse_mems_conflict_p to a nonzero value.  */\n+\n+static void\n+mems_conflict_for_gcse_p (dest, setter, data)\n+     rtx dest, setter ATTRIBUTE_UNUSED;\n+     void *data ATTRIBUTE_UNUSED;\n+{\n+  while (GET_CODE (dest) == SUBREG\n+\t || GET_CODE (dest) == ZERO_EXTRACT\n+\t || GET_CODE (dest) == SIGN_EXTRACT\n+\t || GET_CODE (dest) == STRICT_LOW_PART)\n+    dest = XEXP (dest, 0);\n+\n+  /* If DEST is not a MEM, then it will not conflict with the load.  Note\n+     that function calls are assumed to clobber memory, but are handled\n+     elsewhere.  */\n+  if (GET_CODE (dest) != MEM)\n+    return;\n+\n+  /* If we are setting a MEM in our list of specially recognized MEMs,\n+     don't mark as killed this time.  */ \n+  \n+  if (dest == gcse_mem_operand && pre_ldst_mems != NULL)\n+    {\n+      if (!find_rtx_in_ldst (dest))\n+\tgcse_mems_conflict_p = 1;\n+      return;\n+    }\n+\n+  if (true_dependence (dest, GET_MODE (dest), gcse_mem_operand,\n+\t\t       rtx_addr_varies_p))\n+    gcse_mems_conflict_p = 1;\n+}\n+\n+/* Return nonzero if the expression in X (a memory reference) is killed\n+   in block BB before or after the insn with the CUID in UID_LIMIT.\n+   AVAIL_P is nonzero for kills after UID_LIMIT, and zero for kills\n+   before UID_LIMIT.\n+\n+   To check the entire block, set UID_LIMIT to max_uid + 1 and\n+   AVAIL_P to 0.  */\n+\n+static int\n+load_killed_in_block_p (bb, uid_limit, x, avail_p)\n+     int bb;\n+     int uid_limit;\n+     rtx x;\n+     int avail_p;\n+{\n+  rtx list_entry = modify_mem_list[bb];\n+  while (list_entry)\n+    {\n+      rtx setter;\n+      /* Ignore entries in the list that do not apply.  */\n+      if ((avail_p\n+\t   && INSN_CUID (XEXP (list_entry, 0)) < uid_limit)\n+\t  || (! avail_p\n+\t      && INSN_CUID (XEXP (list_entry, 0)) > uid_limit))\n+\t{\n+\t  list_entry = XEXP (list_entry, 1);\n+\t  continue;\n+\t}\n+\n+      setter = XEXP (list_entry, 0);\n+\n+      /* If SETTER is a call everything is clobbered.  Note that calls\n+\t to pure functions are never put on the list, so we need not\n+\t worry about them.  */\n+      if (GET_CODE (setter) == CALL_INSN)\n+\treturn 1;\n+\n+      /* SETTER must be an INSN of some kind that sets memory.  Call\n+\t note_stores to examine each hunk of memory that is modified. \n+\n+\t The note_stores interface is pretty limited, so we have to\n+\t communicate via global variables.  Yuk.  */\n+      gcse_mem_operand = x;\n+      gcse_mems_conflict_p = 0;\n+      note_stores (PATTERN (setter), mems_conflict_for_gcse_p, NULL);\n+      if (gcse_mems_conflict_p)\n+\treturn 1;\n+      list_entry = XEXP (list_entry, 1);\n+    }\n+  return 0;\n+}\n+\n /* Return non-zero if the operands of expression X are unchanged from\n    the start of INSN's basic block up to but not including INSN.  */\n \n@@ -2126,7 +2353,46 @@ record_last_reg_set_info (insn, regno)\n   SET_BIT (reg_set_in_block[BLOCK_NUM (insn)], regno);\n }\n \n+\n+/* Record all of the canonicalized MEMs of record_last_mem_set_info's insn.\n+   Note we store a pair of elements in the list, so they have to be\n+   taken off pairwise.  */\n+\n+static void \n+canon_list_insert (dest, unused1, v_insn)\n+     rtx    dest ATTRIBUTE_UNUSED;\n+     rtx    unused1 ATTRIBUTE_UNUSED;\n+     void * v_insn;\n+{\n+  rtx dest_addr, insn;\n+\n+  while (GET_CODE (dest) == SUBREG\n+      || GET_CODE (dest) == ZERO_EXTRACT\n+      || GET_CODE (dest) == SIGN_EXTRACT\n+      || GET_CODE (dest) == STRICT_LOW_PART)\n+    dest = XEXP (dest, 0);\n+\n+  /* If DEST is not a MEM, then it will not conflict with a load.  Note\n+     that function calls are assumed to clobber memory, but are handled\n+     elsewhere.  */\n+\n+  if (GET_CODE (dest) != MEM)\n+    return;\n+\n+  dest_addr = get_addr (XEXP (dest, 0));\n+  dest_addr = canon_rtx (dest_addr);\n+  insn = (rtx) v_insn;  \n+\n+  canon_modify_mem_list[BLOCK_NUM (insn)] = \n+    alloc_INSN_LIST (dest_addr, canon_modify_mem_list[BLOCK_NUM (insn)]);\n+  canon_modify_mem_list[BLOCK_NUM (insn)] = \n+    alloc_INSN_LIST (dest, canon_modify_mem_list[BLOCK_NUM (insn)]);\n+}\n+\n /* Record memory first/last/block set information for INSN.  */\n+/* Record memory modification information for INSN.  We do not actually care\n+   about the memory location(s) that are set, or even how they are set (consider\n+   a CALL_INSN).  We merely need to record which insns modify memory.  */\n \n static void\n record_last_mem_set_info (insn)\n@@ -2137,6 +2403,19 @@ record_last_mem_set_info (insn)\n \n   mem_last_set = INSN_CUID (insn);\n   mem_set_in_block[BLOCK_NUM (insn)] = 1;\n+  modify_mem_list[BLOCK_NUM (insn)] = \n+    alloc_INSN_LIST (insn, modify_mem_list[BLOCK_NUM (insn)]);\n+\n+  if (GET_CODE (insn) == CALL_INSN)\n+    {\n+      /* Note that traversals of this loop (other than for free-ing)\n+\t will break after encountering a CALL_INSN.  So, there's no\n+\t need to insert a pair of items, as canon_list_insert does. */\n+      canon_modify_mem_list[BLOCK_NUM (insn)] = \n+        alloc_INSN_LIST (insn, canon_modify_mem_list[BLOCK_NUM (insn)]);\n+    }\n+  else\n+    note_stores (PATTERN (insn), canon_list_insert, (void*)insn );\n }\n \n /* Called from compute_hash_table via note_stores to handle one\n@@ -2193,6 +2472,17 @@ compute_hash_table (set_p)\n   sbitmap_vector_zero (reg_set_in_block, n_basic_blocks);\n   memset ((char *) mem_set_in_block, 0, n_basic_blocks);\n \n+  /* re-Cache any INSN_LIST nodes we have allocated.  */\n+  {\n+    int i;\n+    for (i = 0; i < n_basic_blocks; i++)\n+      {\n+        if (modify_mem_list[i])\n+\t  free_INSN_LIST_list (modify_mem_list + i);\n+        if (canon_modify_mem_list[i])\n+\t  free_INSN_LIST_list (canon_modify_mem_list + i);\n+      }\n+  }\n   /* Some working arrays used to track first and last set in each block.  */\n   /* ??? One could use alloca here, but at some size a threshold is crossed\n      beyond which one should use malloc.  Are we at that threshold here?  */\n@@ -2450,6 +2740,18 @@ reset_opr_set_tables ()\n      For now this is very trivial, we only record whether any memory\n      location has been modified.  */\n   mem_last_set = 0;\n+  {\n+    int i;\n+\n+    /* re-Cache any INSN_LIST nodes we have allocated.  */\n+    for (i = 0; i < n_basic_blocks; i++)\n+      {\n+        if (modify_mem_list[i]) \n+\t  free_INSN_LIST_list (modify_mem_list + i);\n+        if (canon_modify_mem_list[i]) \n+\t  free_INSN_LIST_list (canon_modify_mem_list + i);\n+      }\n+  }\n }\n \n /* Return non-zero if the operands of X are not set before INSN in\n@@ -2481,6 +2783,8 @@ oprs_not_set_p (x, insn)\n       return 1;\n \n     case MEM:\n+      if (load_killed_in_block_p (BLOCK_NUM (insn), INSN_CUID (insn), x, 0))\n+\treturn 0;\n       if (mem_last_set != 0)\n \treturn 0;\n       else\n@@ -2522,6 +2826,8 @@ mark_call (insn)\n      rtx insn;\n {\n   mem_last_set = INSN_CUID (insn);\n+  if (! CONST_CALL_P (insn))\n+    record_last_mem_set_info (insn);\n }\n \n /* Mark things set by a SET.  */\n@@ -2538,6 +2844,11 @@ mark_set (pat, insn)\n \t || GET_CODE (dest) == STRICT_LOW_PART)\n     dest = XEXP (dest, 0);\n \n+  if (GET_CODE (dest) == REG)\n+    SET_BIT (reg_set_bitmap, REGNO (dest));\n+  else if (GET_CODE (dest) == MEM)\n+    record_last_mem_set_info (insn);\n+\n   if (GET_CODE (dest) == REG)\n     SET_BIT (reg_set_bitmap, REGNO (dest));\n   else if (GET_CODE (dest) == MEM)\n@@ -2562,6 +2873,10 @@ mark_clobber (pat, insn)\n     SET_BIT (reg_set_bitmap, REGNO (clob));\n   else\n     mem_last_set = INSN_CUID (insn);\n+  if (GET_CODE (clob) == REG)\n+    SET_BIT (reg_set_bitmap, REGNO (clob));\n+  else\n+    record_last_mem_set_info (insn);\n }\n \n /* Record things set by INSN.\n@@ -2809,6 +3124,8 @@ expr_killed_p (x, bb)\n       return TEST_BIT (reg_set_in_block[bb], REGNO (x));\n \n     case MEM:\n+      if (load_killed_in_block_p (bb, get_max_uid () + 1, x, 0))\n+\treturn 1;\n       if (mem_set_in_block[bb])\n \treturn 1;\n       else\n@@ -3476,6 +3793,41 @@ compute_transp (x, indx, bmap, set_p)\n       return;\n \n     case MEM:\n+      for (bb = 0; bb < n_basic_blocks; bb++)\n+\t{\n+\t  rtx list_entry = canon_modify_mem_list[bb];\n+\n+\t  while (list_entry)\n+\t    {\n+\t      rtx dest, dest_addr;\n+\n+\t      if (GET_CODE (XEXP (list_entry, 0)) == CALL_INSN)\n+\t\t{\n+\t\t  if (set_p)\n+\t\t    SET_BIT (bmap[bb], indx);\n+\t\t  else\n+\t\t    RESET_BIT (bmap[bb], indx);\n+\t\t  break;\n+\t\t}\n+\t      /* LIST_ENTRY must be an INSN of some kind that sets memory.\n+\t\t Examine each hunk of memory that is modified.  */\n+\n+\t      dest = XEXP (list_entry, 0);\n+\t      list_entry = XEXP (list_entry, 1);\n+\t      dest_addr = XEXP (list_entry, 0);\n+\t      \n+\t      if (canon_true_dependence (dest, GET_MODE (dest), dest_addr,\n+\t\t\t\t\t x, rtx_addr_varies_p))\n+\t\t{\n+\t\t  if (set_p)\n+\t\t    SET_BIT (bmap[bb], indx);\n+\t\t  else\n+\t\t    RESET_BIT (bmap[bb], indx);\n+\t\t  break;\n+\t\t}\n+\t      list_entry = XEXP (list_entry, 1);\n+\t    }\n+\t}\n       if (set_p)\n \t{\n \t  for (bb = 0; bb < n_basic_blocks; bb++)\n@@ -4555,6 +4907,7 @@ pre_edge_insert (edge_list, index_map)\n \t\t\t\t     expr->bitmap_index);\n \t\t\t  }\n \n+\t\t\tupdate_ld_motion_stores (expr);\n \t\t\tSET_BIT (inserted[e], j);\n \t\t\tdid_insert = 1;\n \t\t\tgcse_create_count++;\n@@ -4815,7 +5168,11 @@ one_pre_gcse_pass (pass)\n \n   alloc_expr_hash_table (max_cuid);\n   add_noreturn_fake_exit_edges ();\n+  if (flag_gcse_lm)\n+    compute_ld_motion_mems ();\n+\n   compute_expr_hash_table ();\n+  trim_ld_motion_mems ();\n   if (gcse_file)\n     dump_hash_table (gcse_file, \"Expression\", expr_hash_table,\n \t\t     expr_hash_table_size, n_exprs);\n@@ -4829,6 +5186,7 @@ one_pre_gcse_pass (pass)\n       free_pre_mem ();\n     }\n \n+  free_ldst_mems ();\n   remove_fake_edges ();\n   free_expr_hash_table ();\n \n@@ -5603,3 +5961,1148 @@ one_code_hoisting_pass ()\n \n   return changed;\n }\n+\f\n+/*  Here we provide the things required to do store motion towards\n+    the exit. In order for this to be effective, gcse also needed to\n+    be taught how to move a load when it is kill only by a store to itself.\n+\n+\t    int i;\n+\t    float a[10];\n+\n+\t    void foo(float scale)\n+\t    {\n+\t      for (i=0; i<10; i++)\n+\t\ta[i] *= scale;\n+\t    }\n+\n+    'i' is both loaded and stored to in the loop. Normally, gcse cannot move\n+    the load out since its live around the loop, and stored at the bottom \n+    of the loop. \n+\n+      The 'Load Motion' referred to and implemented in this file is \n+    an enhancement to gcse which when using edge based lcm, recognizes\n+    this situation and allows gcse to move the load out of the loop.\n+\n+      Once gcse has hoisted the load, store motion can then push this\n+    load towards the exit, and we end up with no loads or stores of 'i'\n+    in the loop.  */\n+\n+/* This will search the ldst list for a matching expresion. If it\n+   doesn't find one, we create one and initialize it.  */\n+\n+static struct ls_expr *\n+ldst_entry (x)\n+     rtx x;\n+{\n+  struct ls_expr * ptr;\n+\n+  for (ptr = first_ls_expr(); ptr != NULL; ptr = next_ls_expr (ptr))\n+    if (expr_equiv_p (ptr->pattern, x))\n+      break;\n+\n+  if (!ptr)\n+    {\n+      ptr = (struct ls_expr *) xmalloc (sizeof (struct ls_expr));\n+\n+      ptr->next         = pre_ldst_mems;\n+      ptr->expr         = NULL;\n+      ptr->pattern      = x;\n+      ptr->loads        = NULL_RTX;\n+      ptr->stores       = NULL_RTX;\n+      ptr->reaching_reg = NULL_RTX;\n+      ptr->invalid      = 0;\n+      ptr->index        = 0;\n+      ptr->hash_index   = 0;\n+      pre_ldst_mems     = ptr;\n+    }\n+  \n+  return ptr;\n+}\n+\n+/* Free up an individual ldst entry.  */\n+\n+static void \n+free_ldst_entry (ptr)\n+     struct ls_expr * ptr;\n+{\n+  free_INSN_LIST_list (& ptr->loads);\n+  free_INSN_LIST_list (& ptr->stores);\n+\n+  free (ptr);\n+}\n+\n+/* Free up all memory associated with the ldst list.  */\n+\n+static void\n+free_ldst_mems ()\n+{\n+  while (pre_ldst_mems) \n+    {\n+      struct ls_expr * tmp = pre_ldst_mems;\n+\n+      pre_ldst_mems = pre_ldst_mems->next;\n+\n+      free_ldst_entry (tmp);\n+    }\n+\n+  pre_ldst_mems = NULL;\n+}\n+\n+/* Dump debugging info about the ldst list.  */\n+\n+static void\n+print_ldst_list (file)\n+     FILE * file;\n+{\n+  struct ls_expr * ptr;\n+\n+  fprintf (file, \"LDST list: \\n\");\n+\n+  for (ptr = first_ls_expr(); ptr != NULL; ptr = next_ls_expr (ptr))\n+    {\n+      fprintf (file, \"  Pattern (%3d): \", ptr->index);\n+\n+      print_rtl (file, ptr->pattern);\n+\n+      fprintf (file, \"\\n\t Loads : \");\n+\n+      if (ptr->loads)\n+\tprint_rtl (file, ptr->loads);\n+      else\n+\tfprintf (file, \"(nil)\");\n+\n+      fprintf (file, \"\\n\tStores : \");\n+\n+      if (ptr->stores)\n+\tprint_rtl (file, ptr->stores);\n+      else\n+\tfprintf (file, \"(nil)\");\n+\n+      fprintf (file, \"\\n\\n\");\n+    }\n+\n+  fprintf (file, \"\\n\");\n+}\n+\n+/* Returns 1 if X is in the list of ldst only expressions.  */\n+\n+static struct ls_expr *\n+find_rtx_in_ldst (x)\n+     rtx x;\n+{\n+  struct ls_expr * ptr;\n+  \n+  for (ptr = pre_ldst_mems; ptr != NULL; ptr = ptr->next)\n+    if (expr_equiv_p (ptr->pattern, x) && ! ptr->invalid)\n+      return ptr;\n+\n+  return NULL;\n+}\n+\n+/* Assign each element of the list of mems a monotonically increasing value.  */\n+\n+static int\n+enumerate_ldsts ()\n+{\n+  struct ls_expr * ptr;\n+  int n = 0;\n+\n+  for (ptr = pre_ldst_mems; ptr != NULL; ptr = ptr->next)\n+    ptr->index = n++;\n+\n+  return n;\n+}\n+\n+/* Return first item in the list.  */\n+\n+static inline struct ls_expr *\n+first_ls_expr ()\n+{\n+  return pre_ldst_mems;\n+}\n+\n+/* Return the next item in ther list after the specified one.  */\n+\n+static inline struct ls_expr *\n+next_ls_expr (ptr)\n+     struct ls_expr * ptr;\n+{\n+  return ptr->next;\n+}\n+\f\n+/* Load Motion for loads which only kill themselves.  */\n+\n+/* Return true if x is a simple MEM operation, with no registers or\n+   side effects. These are the types of loads we consider for the\n+   ld_motion list, otherwise we let the usual aliasing take care of it.  */\n+\n+static int \n+simple_mem (x)\n+     rtx x;\n+{\n+  if (GET_CODE (x) != MEM)\n+    return 0;\n+  \n+  if (MEM_VOLATILE_P (x))\n+    return 0;\n+  \n+  if (GET_MODE (x) == BLKmode)\n+    return 0;\n+\n+  if (!rtx_varies_p (XEXP (x, 0), 0))\n+    return 1;\n+  \n+  return 0;\n+}\n+\n+/* Make sure there isn't a buried reference in this pattern anywhere.  \n+   If there is, invalidate the entry for it since we're not capable \n+   of fixing it up just yet.. We have to be sure we know about ALL \n+   loads since the aliasing code will allow all entries in the\n+   ld_motion list to not-alias itself.  If we miss a load, we will get\n+   the wrong value since gcse might common it and we won't know to \n+   fix it up.  */\n+\n+static void\n+invalidate_any_buried_refs (x)\n+     rtx x;\n+{\n+  const char * fmt;\n+  int i,j;\n+  struct ls_expr * ptr;\n+\n+  /* Invalidate it in the list.  */\n+  if (GET_CODE (x) == MEM && simple_mem (x))\n+    {\n+      ptr = ldst_entry (x);\n+      ptr->invalid = 1;\n+    }\n+\n+  /* Recursively process the insn.  */\n+  fmt = GET_RTX_FORMAT (GET_CODE (x));\n+  \n+  for (i = GET_RTX_LENGTH (GET_CODE (x)) - 1; i >= 0; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\tinvalidate_any_buried_refs (XEXP (x, i));\n+      else if (fmt[i] == 'E')\n+\tfor (j = XVECLEN (x, i) - 1; j >= 0; j--)\n+\t  invalidate_any_buried_refs (XVECEXP (x, i, j));\n+    }\n+}\n+\n+/* Find all the 'simple' MEMs which are used in LOADs and STORES. Simple\n+   being defined as MEM loads and stores to symbols, with no\n+   side effects and no registers in the expression. If there are any \n+   uses/defs which dont match this criteria, it is invalidated and\n+   trimmed out later.  */\n+\n+static void \n+compute_ld_motion_mems ()\n+{\n+  struct ls_expr * ptr;\n+  int bb;\n+  rtx insn;\n+  \n+  pre_ldst_mems = NULL;\n+\n+  for (bb = 0; bb < n_basic_blocks; bb++)\n+    {\n+      for (insn = BLOCK_HEAD (bb);\n+\t   insn && insn != NEXT_INSN (BLOCK_END (bb));\n+\t   insn = NEXT_INSN (insn))\n+\t{\n+\t  if (GET_RTX_CLASS (GET_CODE (insn)) == 'i')\n+\t    {\n+\t      if (GET_CODE (PATTERN (insn)) == SET)\n+\t\t{\n+\t\t  rtx src = SET_SRC (PATTERN (insn));\n+\t\t  rtx dest = SET_DEST (PATTERN (insn));\n+\n+\t\t  /* Check for a simple LOAD...  */\n+\t\t  if (GET_CODE (src) == MEM && simple_mem (src))\n+\t\t    {\n+\t\t      ptr = ldst_entry (src);\n+\t\t      if (GET_CODE (dest) == REG)\n+\t\t\tptr->loads = alloc_INSN_LIST (insn, ptr->loads);\n+\t\t      else\n+\t\t\tptr->invalid = 1;\n+\t\t    }\n+\t\t  else\n+\t\t    {\n+\t\t      /* Make sure there isn't a buried load somewhere.  */\n+\t\t      invalidate_any_buried_refs (src);\n+\t\t    }\n+\t\t  \n+\t\t  /* Check for stores. Don't worry about aliased ones, they\n+\t\t     will block any movement we might do later. We only care\n+\t\t     about this exact pattern since those are the only\n+\t\t     circumstance that we will ignore the aliasing info.  */\n+\t\t  if (GET_CODE (dest) == MEM && simple_mem (dest))\n+\t\t    {\n+\t\t      ptr = ldst_entry (dest);\n+\t\t      \n+\t\t      if (GET_CODE (src) != MEM)\n+\t\t\tptr->stores = alloc_INSN_LIST (insn, ptr->stores);\n+\t\t      else\n+\t\t\tptr->invalid = 1;\n+\t\t    }\n+\t\t}\n+\t      else\n+\t\tinvalidate_any_buried_refs (PATTERN (insn));\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* Remove any references that have been either invalidated or are not in the \n+   expression list for pre gcse.  */\n+\n+static void\n+trim_ld_motion_mems ()\n+{\n+  struct ls_expr * last = NULL;\n+  struct ls_expr * ptr = first_ls_expr ();\n+\n+  while (ptr != NULL)\n+    {\n+      int del = ptr->invalid;\n+      struct expr * expr = NULL;\n+      \n+      /* Delete if entry has been made invalid.  */\n+      if (!del) \n+\t{\n+\t  unsigned int i;\n+\t  \n+\t  del = 1;\n+\t  /* Delete if we cannot find this mem in the expression list.  */\n+\t  for (i = 0; i < expr_hash_table_size && del; i++)\n+\t    {\n+\t      for (expr = expr_hash_table[i]; \n+\t\t   expr != NULL; \n+\t\t   expr = expr->next_same_hash)\n+\t\tif (expr_equiv_p (expr->expr, ptr->pattern))\n+\t\t  {\n+\t\t    del = 0;\n+\t\t    break;\n+\t\t  }\n+\t    }\n+\t}\n+      \n+      if (del)\n+\t{\n+\t  if (last != NULL)\n+\t    {\n+\t      last->next = ptr->next;\n+\t      free_ldst_entry (ptr);\n+\t      ptr = last->next;\n+\t    }\n+\t  else\n+\t    {\n+\t      pre_ldst_mems = pre_ldst_mems->next;\n+\t      free_ldst_entry (ptr);\n+\t      ptr = pre_ldst_mems;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  /* Set the expression field if we are keeping it.  */\n+\t  last = ptr;\n+\t  ptr->expr = expr;\n+\t  ptr = ptr->next;\n+\t}\n+    }\n+\n+  /* Show the world what we've found.  */\n+  if (gcse_file && pre_ldst_mems != NULL)\n+    print_ldst_list (gcse_file);\n+}\n+\n+/* This routine will take an expression which we are replacing with\n+   a reaching register, and update any stores that are needed if\n+   that expression is in the ld_motion list.  Stores are updated by\n+   copying their SRC to the reaching register, and then storeing\n+   the reaching register into the store location. These keeps the\n+   correct value in the reaching register for the loads.  */\n+\n+static void\n+update_ld_motion_stores (expr)\n+     struct expr * expr;\n+{\n+  struct ls_expr * mem_ptr;\n+\n+  if ((mem_ptr = find_rtx_in_ldst (expr->expr)))\n+    {\n+      /* We can try to find just the REACHED stores, but is shouldn't \n+\t matter to set the reaching reg everywhere...  some might be \n+\t dead and should be eliminated later.  */\n+\n+      /* We replace  SET mem = expr   with\n+\t   SET reg = expr\n+\t   SET mem = reg , where reg is the \n+\t   reaching reg used in the load.  */\n+      rtx list = mem_ptr->stores;\n+      \n+      for ( ; list != NULL_RTX; list = XEXP (list, 1))\n+\t{\n+\t  rtx insn = XEXP (list, 0);\n+\t  rtx pat = PATTERN (insn);\n+\t  rtx src = SET_SRC (pat);\n+\t  rtx reg = expr->reaching_reg;\n+\t  rtx copy, i;\n+\n+\t  /* If we've already copied it, continue.  */\n+\t  if (expr->reaching_reg == src)\n+\t    continue;\n+\t  \n+\t  if (gcse_file)\n+\t    {\n+\t      fprintf (gcse_file, \"PRE:  store updated with reaching reg \");\n+\t      print_rtl (gcse_file, expr->reaching_reg);\n+\t      fprintf (gcse_file, \":\\n\t\");\n+\t      print_inline_rtx (gcse_file, insn, 8);\n+\t      fprintf (gcse_file, \"\\n\");\n+\t    }\n+\t  \n+\t  copy = gen_move_insn ( reg, SET_SRC (pat));\n+\t  i = emit_insn_before (copy, insn);\n+\t  record_one_set (REGNO (reg), i);\n+\t  set_block_num (i, BLOCK_NUM (insn));\n+\t  SET_SRC (pat) = reg;\n+\n+\t  /* un-recognize this pattern since it's probably different now.  */\n+\t  INSN_CODE (insn) = -1;\n+\t  gcse_create_count++;\n+\t}\n+    }\n+}\n+\f\n+/* Store motion code.  */\n+\n+/* This is used to communicate the target bitvector we want to use in the \n+   reg_set_info routine when called via the note_stores mechanism.  */\n+static sbitmap * regvec;\n+\n+/* Used in computing the reverse edge graph bit vectors.  */\n+static sbitmap * st_antloc;\n+\n+/* Global holding the number of store expressions we are dealing with.  */\n+static int num_stores;\n+\n+/* Checks to set if we need to mark a register set. Called from note_stores.  */\n+\n+static void\n+reg_set_info (dest, setter, data)\n+     rtx dest, setter ATTRIBUTE_UNUSED;\n+     void * data ATTRIBUTE_UNUSED;\n+{\n+  if (GET_CODE (dest) == SUBREG)\n+    dest = SUBREG_REG (dest);\n+\n+  if (GET_CODE (dest) == REG)\n+    SET_BIT (*regvec, REGNO (dest));\n+}\n+\n+/* Return non-zero if the register operands of expression X are killed \n+   anywhere in basic block BB.  */\n+\n+static int\n+store_ops_ok (x, bb)\n+     rtx x;\n+     int bb;\n+{\n+  int i;\n+  enum rtx_code code;\n+  const char * fmt;\n+\n+  /* Repeat is used to turn tail-recursion into iteration.  */\n+ repeat:\n+\n+  if (x == 0)\n+    return 1;\n+\n+  code = GET_CODE (x);\n+  switch (code)\n+    {\n+    case REG:\n+\t/* If a reg has changed after us in this\n+\t   block, the operand has been killed.  */\n+\treturn TEST_BIT (reg_set_in_block[bb], REGNO (x));\n+\n+    case MEM:\n+      x = XEXP (x, 0);\n+      goto repeat;\n+\n+    case PRE_DEC:\n+    case PRE_INC:\n+    case POST_DEC:\n+    case POST_INC:\n+      return 0;\n+\n+    case PC:\n+    case CC0: /*FIXME*/\n+    case CONST:\n+    case CONST_INT:\n+    case CONST_DOUBLE:\n+    case SYMBOL_REF:\n+    case LABEL_REF:\n+    case ADDR_VEC:\n+    case ADDR_DIFF_VEC:\n+      return 1;\n+\n+    default:\n+      break;\n+    }\n+\n+  i = GET_RTX_LENGTH (code) - 1;\n+  fmt = GET_RTX_FORMAT (code);\n+  \n+  for (; i >= 0; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\t{\n+\t  rtx tem = XEXP (x, i);\n+\n+\t  /* If we are about to do the last recursive call\n+\t     needed at this level, change it into iteration.\n+\t     This function is called enough to be worth it.  */\n+\t  if (i == 0)\n+\t    {\n+\t      x = tem;\n+\t      goto repeat;\n+\t    }\n+\t  \n+\t  if (! store_ops_ok (tem, bb))\n+\t    return 0;\n+\t}\n+      else if (fmt[i] == 'E')\n+\t{\n+\t  int j;\n+\t  \n+\t  for (j = 0; j < XVECLEN (x, i); j++)\n+\t    {\n+\t      if (! store_ops_ok (XVECEXP (x, i, j), bb))\n+\t\treturn 0;\n+\t    }\n+\t}\n+    }\n+\n+  return 1;\n+}\n+\n+/* Determine whether insn is MEM store pattern that we will consider moving.  */\n+\n+static void\n+find_moveable_store (insn)\n+     rtx insn;\n+{\n+  struct ls_expr * ptr;\n+  rtx dest = PATTERN (insn);\n+\n+  if (GET_CODE (dest) != SET)\n+    return;\n+\n+  dest = SET_DEST (dest);\n+  \n+  if (GET_CODE (dest) != MEM || MEM_VOLATILE_P (dest)\n+      || GET_MODE (dest) == BLKmode)\n+    return;\n+\n+  if (GET_CODE (XEXP (dest, 0)) != SYMBOL_REF)\n+      return;\n+\n+  if (rtx_varies_p (XEXP (dest, 0), 0))\n+    return;\n+\n+  ptr = ldst_entry (dest);\n+  ptr->stores = alloc_INSN_LIST (insn, ptr->stores);\n+}\n+\n+/* Perform store motion. Much like gcse, except we move expressions the\n+   other way by looking at the flowgraph in reverse.  */\n+\n+static int\n+compute_store_table ()\n+{\n+  int bb, ret;\n+  unsigned regno;\n+  rtx insn, pat;\n+\n+  max_gcse_regno = max_reg_num ();\n+\n+  reg_set_in_block = (sbitmap *) sbitmap_vector_alloc (n_basic_blocks,\n+\t\t\t\t\t\t       max_gcse_regno);\n+  sbitmap_vector_zero (reg_set_in_block, n_basic_blocks);\n+  pre_ldst_mems = 0;\n+\n+  /* Find all the stores we care about.  */\n+  for (bb = 0; bb < n_basic_blocks; bb++)\n+    {\n+      regvec = & (reg_set_in_block[bb]);\n+      for (insn = BLOCK_END (bb);\n+\t   insn && insn != PREV_INSN (BLOCK_HEAD (bb));\n+\t   insn = PREV_INSN (insn))\n+\t{\n+#ifdef NON_SAVING_SETJMP \n+\t  if (NON_SAVING_SETJMP && GET_CODE (insn) == NOTE\n+\t      && NOTE_LINE_NUMBER (insn) == NOTE_INSN_SETJMP)\n+\t    {\n+\t      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n+\t\tSET_BIT (reg_set_in_block[bb], regno);\n+\t      continue;\n+\t    }\n+#endif\n+\t/* Ignore anything that is not a normal insn.  */\n+\tif (GET_RTX_CLASS (GET_CODE (insn)) != 'i')\n+\t    continue;\n+\n+\t  if (GET_CODE (insn) == CALL_INSN)\n+\t    {\n+\t      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n+\t\tif ((call_used_regs[regno]\n+\t\t     && regno != STACK_POINTER_REGNUM\n+#if HARD_FRAME_POINTER_REGNUM != FRAME_POINTER_REGNUM\n+\t\t     && regno != HARD_FRAME_POINTER_REGNUM\n+#endif\n+#if ARG_POINTER_REGNUM != FRAME_POINTER_REGNUM\n+\t\t     && ! (regno == ARG_POINTER_REGNUM && fixed_regs[regno])\n+#endif\n+#if defined (PIC_OFFSET_TABLE_REGNUM) && !defined (PIC_OFFSET_TABLE_REG_CALL_CLOBBERED)\n+\t\t     && ! (regno == PIC_OFFSET_TABLE_REGNUM && flag_pic)\n+#endif\n+\n+\t\t     && regno != FRAME_POINTER_REGNUM)\n+\t\t    || global_regs[regno])\n+\t\tSET_BIT (reg_set_in_block[bb], regno);\n+\t    }\n+\t  \n+\t  pat = PATTERN (insn);\n+\t  note_stores (pat, reg_set_info, NULL);\n+\t  \n+\t  /* Now that we've marked regs, look for stores.  */\n+\t  if (GET_CODE (pat) == SET)\n+\t    find_moveable_store (insn);\n+\t}\n+    }\n+\n+  ret = enumerate_ldsts ();\n+  \n+  if (gcse_file)\n+    {\n+      fprintf (gcse_file, \"Store Motion Expressions.\\n\");\n+      print_ldst_list (gcse_file);\n+    }\n+  \n+  return ret;\n+}\n+\n+/* Check to see if the load X is aliased with STORE_PATTERN.  */\n+\n+static int\n+load_kills_store (x, store_pattern)\n+     rtx x, store_pattern;\n+{\n+  if (true_dependence (x, GET_MODE (x), store_pattern, rtx_addr_varies_p))\n+    return 1;\n+  return 0;\n+}\n+\n+/* Go through the entire insn X, looking for any loads which might alias \n+   STORE_PATTERN.  Return 1 if found.  */\n+\n+static int\n+find_loads (x, store_pattern)\n+     rtx x, store_pattern;\n+{\n+  const char * fmt;\n+  int i,j;\n+  int ret = 0;\n+\n+  if (GET_CODE (x) == SET) \n+    x = SET_SRC (x);\n+\n+  if (GET_CODE (x) == MEM)\n+    {\n+      if (load_kills_store (x, store_pattern))\n+\treturn 1;\n+    }\n+\n+  /* Recursively process the insn.  */\n+  fmt = GET_RTX_FORMAT (GET_CODE (x));\n+  \n+  for (i = GET_RTX_LENGTH (GET_CODE (x)) - 1; i >= 0 && !ret; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\tret |= find_loads (XEXP (x, i), store_pattern);\n+      else if (fmt[i] == 'E')\n+\tfor (j = XVECLEN (x, i) - 1; j >= 0; j--)\n+\t  ret |= find_loads (XVECEXP (x, i, j), store_pattern);\n+    }\n+  return ret;\n+}\n+\n+/* Check if INSN kills the store pattern X (is aliased with it).  \n+   Return 1 if it it does.  */\n+\n+static int \n+store_killed_in_insn (x, insn)\n+     rtx x, insn;\n+{\n+  if (GET_RTX_CLASS (GET_CODE (insn)) != 'i')\n+    return 0;\n+  \n+  if (GET_CODE (insn) == CALL_INSN)\n+    {\n+      if (CONST_CALL_P (insn))\n+\treturn 0;\n+      else\n+\treturn 1;\n+    }\n+  \n+  if (GET_CODE (PATTERN (insn)) == SET)\n+    {\n+      rtx pat = PATTERN (insn);\n+      /* Check for memory stores to aliased objects.  */\n+      if (GET_CODE (SET_DEST (pat)) == MEM && !expr_equiv_p (SET_DEST (pat), x))\n+\t/* pretend its a load and check for aliasing.  */\n+\tif (find_loads (SET_DEST (pat), x))\n+\t  return 1;\n+      return find_loads (SET_SRC (pat), x);\n+    }\n+  else\n+    return find_loads (PATTERN (insn), x);\n+}\n+\n+/* Returns 1 if the expression X is loaded or clobbered on or after INSN\n+   within basic block BB.  */\n+\n+static int \n+store_killed_after (x, insn, bb)\n+     rtx x, insn;\n+     int bb;\n+{\n+   rtx last = BLOCK_END (bb);\n+   \n+   if (insn == last)\n+     return 0;\n+\n+  /* Check if the register operands of the store are OK in this block.\n+     Note that if registers are changed ANYWHERE in the block, we'll \n+     decide we can't move it, regardless of whether it changed above \n+     or below the store. This could be improved by checking the register\n+     operands while lookinng for aliasing in each insn.  */\n+  if (!store_ops_ok (XEXP (x, 0), bb))\n+    return 1;\n+\n+   for ( ; insn && insn != NEXT_INSN (last); insn = NEXT_INSN (insn))\n+     if (store_killed_in_insn (x, insn))\n+       return 1;\n+   \n+  return 0;\n+}\n+\n+/* Returns 1 if the expression X is loaded or clobbered on or before INSN\n+   within basic block BB.  */\n+static int \n+store_killed_before (x, insn, bb)\n+     rtx x, insn;\n+     int bb;\n+{\n+   rtx first = BLOCK_HEAD (bb);\n+\n+   if (insn == first)\n+     return store_killed_in_insn (x, insn);\n+   \n+  /* Check if the register operands of the store are OK in this block.\n+     Note that if registers are changed ANYWHERE in the block, we'll \n+     decide we can't move it, regardless of whether it changed above \n+     or below the store. This could be improved by checking the register\n+     operands while lookinng for aliasing in each insn.  */\n+  if (!store_ops_ok (XEXP (x, 0), bb))\n+    return 1;\n+\n+   for ( ; insn && insn != PREV_INSN (first); insn = PREV_INSN (insn))\n+     if (store_killed_in_insn (x, insn))\n+       return 1;\n+   \n+   return 0;\n+}\n+\n+#define ANTIC_STORE_LIST(x)\t((x)->loads)\n+#define AVAIL_STORE_LIST(x)\t((x)->stores)\n+\n+/* Given the table of available store insns at the end of blocks,\n+   determine which ones are not killed by aliasing, and generate\n+   the appropriate vectors for gen and killed.  */\n+static void\n+build_store_vectors () \n+{\n+  int bb;\n+  rtx insn, st;\n+  struct ls_expr * ptr;\n+\n+  /* Build the gen_vector. This is any store in the table which is not killed\n+     by aliasing later in its block.  */\n+  ae_gen = (sbitmap *) sbitmap_vector_alloc (n_basic_blocks, num_stores);\n+  sbitmap_vector_zero (ae_gen, n_basic_blocks);\n+\n+  st_antloc = (sbitmap *) sbitmap_vector_alloc (n_basic_blocks, num_stores);\n+  sbitmap_vector_zero (st_antloc, n_basic_blocks);\n+\n+  for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n+    { \n+      /* Put all the stores into either the antic list, or the avail list,\n+\t or both.  */\n+      rtx store_list = ptr->stores;\n+      ptr->stores = NULL_RTX;\n+\n+      for (st = store_list; st != NULL; st = XEXP (st, 1))\n+\t{\n+\t  insn = XEXP (st, 0);\n+\t  bb = BLOCK_NUM (insn);\n+\t  \n+\t  if (!store_killed_after (ptr->pattern, insn, bb))\n+\t    {\n+\t      /* If we've already seen an availale expression in this block,\n+\t\t we can delete the one we saw already (It occurs earlier in\n+\t\t the block), and replace it with this one). We'll copy the\n+\t\t old SRC expression to an unused register in case there\n+\t\t are any side effects.  */\n+\t      if (TEST_BIT (ae_gen[bb], ptr->index))\n+\t\t{\n+\t\t  /* Find previous store.  */\n+\t\t  rtx st;\n+\t\t  for (st = AVAIL_STORE_LIST (ptr); st ; st = XEXP (st, 1))\n+\t\t    if (BLOCK_NUM (XEXP (st, 0)) == bb)\n+\t\t      break;\n+\t\t  if (st)\n+\t\t    {\n+\t\t      rtx r = gen_reg_rtx (GET_MODE (ptr->pattern));\n+\t\t      if (gcse_file)\n+\t\t\tfprintf(gcse_file, \"Removing redundant store:\\n\");\n+\t\t      replace_store_insn (r, XEXP (st, 0), bb);\n+\t\t      XEXP (st, 0) = insn;\n+\t\t      continue;\n+\t\t    }\n+\t\t}\n+\t      SET_BIT (ae_gen[bb], ptr->index);\n+\t      AVAIL_STORE_LIST (ptr) = alloc_INSN_LIST (insn,\n+\t\t\t\t\t\t\tAVAIL_STORE_LIST (ptr));\n+\t    }\n+\t  \n+\t  if (!store_killed_before (ptr->pattern, insn, bb))\n+\t    {\n+\t      SET_BIT (st_antloc[BLOCK_NUM (insn)], ptr->index);\n+\t      ANTIC_STORE_LIST (ptr) = alloc_INSN_LIST (insn,\n+\t\t\t\t\t\t\tANTIC_STORE_LIST (ptr));\n+\t    }\n+\t}\n+      \n+      /* Free the original list of store insns.  */\n+      free_INSN_LIST_list (&store_list);\n+    }\n+\t  \n+  ae_kill = (sbitmap *) sbitmap_vector_alloc (n_basic_blocks, num_stores);\n+  sbitmap_vector_zero (ae_kill, n_basic_blocks);\n+\n+  transp = (sbitmap *) sbitmap_vector_alloc (n_basic_blocks, num_stores);\n+  sbitmap_vector_zero (transp, n_basic_blocks);\n+\n+  for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n+    for (bb = 0; bb < n_basic_blocks; bb++)\n+      {\n+\tif (store_killed_after (ptr->pattern, BLOCK_HEAD (bb), bb))\n+\t  {\n+\t    /* The anticipatable expression is not killed if it's gen'd. */\n+\t    /*\n+\t      We leave this check out for now. If we have a code sequence \n+\t      in a block which looks like:\n+\t\t\tST MEMa = x\n+\t\t\tL     y = MEMa\n+\t\t\tST MEMa = z\n+\t      We should flag this as having an ANTIC expression, NOT\n+\t      transparent, NOT killed, and AVAIL.\n+\t      Unfortunately, since we haven't re-written all loads to\n+\t      use the reaching reg, we'll end up doing an incorrect \n+\t      Load in the middle here if we push the store down. It happens in\n+\t\t    gcc.c-torture/execute/960311-1.c with -O3\n+\t      If we always kill it in this case, we'll sometimes do\n+\t      uneccessary work, but it shouldn't actually hurt anything.\n+\t    if (!TEST_BIT (ae_gen[bb], ptr->index)).  */\n+\t    SET_BIT (ae_kill[bb], ptr->index);\n+\t  }\n+\telse\n+\t  SET_BIT (transp[bb], ptr->index);\n+      }\n+\n+  /* Any block with no exits calls some non-returning function, so\n+     we better mark the store killed here, or we might not store to\n+     it at all.  If we knew it was abort, we wouldn't have to store,\n+     but we don't know that for sure.  */\n+  if (gcse_file) \n+    {\n+      fprintf (gcse_file, \"ST_avail and ST_antic (shown under loads..)\\n\");\n+      print_ldst_list (gcse_file);\n+      dump_sbitmap_vector (gcse_file, \"st_antloc\", \"\", st_antloc, n_basic_blocks);\n+      dump_sbitmap_vector (gcse_file, \"st_kill\", \"\", ae_kill, n_basic_blocks);\n+      dump_sbitmap_vector (gcse_file, \"Transpt\", \"\", transp, n_basic_blocks);\n+      dump_sbitmap_vector (gcse_file, \"st_avloc\", \"\", ae_gen, n_basic_blocks);\n+    }\n+}\n+\n+/* Insert an instruction at the begining of a basic block, and update \n+   the BLOCK_HEAD if needed.  */\n+\n+static void \n+insert_insn_start_bb (insn, bb)\n+     rtx insn;\n+     int bb;\n+{\n+  /* Insert at start of successor block.  */\n+  rtx prev = PREV_INSN (BLOCK_HEAD (bb));\n+  rtx before = BLOCK_HEAD (bb);\n+  while (before != 0)\n+    {\n+      if (GET_CODE (before) != CODE_LABEL\n+\t  && (GET_CODE (before) != NOTE\n+\t      || NOTE_LINE_NUMBER (before) != NOTE_INSN_BASIC_BLOCK))\n+\tbreak;\n+      prev = before;\n+      if (prev == BLOCK_END (bb))\n+\tbreak;\n+      before = NEXT_INSN (before);\n+    }\n+\n+  insn = emit_insn_after (insn, prev);\n+\n+  if (prev == BLOCK_END (bb))\n+    BLOCK_END (bb) = insn;\n+  while (insn != prev)\n+    {\n+      set_block_num (insn, bb);\n+      insn = PREV_INSN (insn);\n+    }\n+\n+  if (gcse_file)\n+    {\n+      fprintf (gcse_file, \"STORE_MOTION  insert store at start of BB %d:\\n\",\n+\t       bb);\n+      print_inline_rtx (gcse_file, insn, 6);\n+      fprintf (gcse_file, \"\\n\");\n+    }\n+}\n+\n+/* This routine will insert a store on an edge. EXPR is the ldst entry for\n+   the memory reference, and E is the edge to insert it on.  Returns non-zero\n+   if an edge insertion was performed.  */\n+\n+static int\n+insert_store (expr, e)\n+     struct ls_expr * expr;\n+     edge e;\n+{\n+  rtx reg, insn;\n+  int bb;\n+  edge tmp;\n+\n+  /* We did all the deleted before this insert, so if we didn't delete a\n+     store, then we haven't set the reaching reg yet either.  */\n+  if (expr->reaching_reg == NULL_RTX)\n+    return 0;\n+\n+  reg = expr->reaching_reg;\n+  insn = gen_move_insn (expr->pattern, reg);\n+  \n+  /* If we are inserting this expression on ALL predecessor edges of a BB,\n+     insert it at the start of the BB, and reset the insert bits on the other\n+     edges so we don;t try to insert it on the other edges.  */\n+  bb = e->dest->index;\n+  for (tmp = e->dest->pred; tmp ; tmp = tmp->pred_next)\n+    {\n+      int index = EDGE_INDEX (edge_list, tmp->src, tmp->dest);\n+      if (index == EDGE_INDEX_NO_EDGE)\n+\tabort ();\n+      if (! TEST_BIT (pre_insert_map[index], expr->index))\n+\tbreak;\n+    }\n+\n+  /* If tmp is NULL, we found an insertion on every edge, blank the\n+     insertion vector for these edges, and insert at the start of the BB.  */\n+  if (!tmp && bb != EXIT_BLOCK)\n+    {\n+      for (tmp = e->dest->pred; tmp ; tmp = tmp->pred_next)\n+\t{\n+\t  int index = EDGE_INDEX (edge_list, tmp->src, tmp->dest);\n+\t  RESET_BIT (pre_insert_map[index], expr->index);\n+\t}\n+      insert_insn_start_bb (insn, bb);\n+      return 0;\n+    }\n+  \n+  /* We can't insert on this edge, so we'll insert at the head of the\n+     successors block.  See Morgan, sec 10.5.  */\n+  if ((e->flags & EDGE_ABNORMAL) == EDGE_ABNORMAL)\n+    {\n+      insert_insn_start_bb (insn, bb);\n+      return 0;\n+    }\n+\n+  insert_insn_on_edge (insn, e);\n+  \n+  if (gcse_file)\n+    {\n+      fprintf (gcse_file, \"STORE_MOTION  insert insn on edge (%d, %d):\\n\",\n+\t       e->src->index, e->dest->index);\n+      print_inline_rtx (gcse_file, insn, 6);\n+      fprintf (gcse_file, \"\\n\");\n+    }\n+  \n+  return 1;\n+}\n+\n+/* This routine will replace a store with a SET to a specified register.  */\n+\n+static void\n+replace_store_insn (reg, del, bb)\n+     rtx reg, del;\n+     int bb;\n+{\n+  rtx insn;\n+  \n+  insn = gen_move_insn (reg, SET_SRC (PATTERN (del)));\n+  insn = emit_insn_after (insn, del);\n+  set_block_num (insn, bb);\n+  \n+  if (gcse_file)\n+    {\n+      fprintf (gcse_file, \n+\t       \"STORE_MOTION  delete insn in BB %d:\\n      \", bb);\n+      print_inline_rtx (gcse_file, del, 6);\n+      fprintf(gcse_file, \"\\nSTORE MOTION  replaced with insn:\\n      \");\n+      print_inline_rtx (gcse_file, insn, 6);\n+      fprintf(gcse_file, \"\\n\");\n+    }\n+  \n+  if (BLOCK_END (bb) == del)\n+    BLOCK_END (bb) = insn;\n+  \n+  if (BLOCK_HEAD (bb) == del)\n+    BLOCK_HEAD (bb) = insn;\n+  \n+  delete_insn (del);\n+}\n+\n+\n+/* Delete a store, but copy the value that would have been stored into\n+   the reaching_reg for later storing.  */\n+\n+static void\n+delete_store (expr, bb)\n+     struct ls_expr * expr;\n+     int bb;\n+{\n+  rtx reg, i, del;\n+\n+  if (expr->reaching_reg == NULL_RTX)\n+    expr->reaching_reg = gen_reg_rtx (GET_MODE (expr->pattern));\n+  \n+\n+  /* If there is more than 1 store, the earlier ones will be dead, \n+     but it doesn't hurt to replace them here.  */  \n+  reg = expr->reaching_reg;\n+  \n+  for (i = AVAIL_STORE_LIST (expr); i; i = XEXP (i, 1))\n+    {\n+      del = XEXP (i, 0);\n+      if (BLOCK_NUM (del) == bb)\n+\t{\n+\t  /* We know there is only one since we deleted redundant \n+\t     ones during the available computation.  */\n+\t  replace_store_insn (reg, del, bb);\n+\t  break;\n+\t}\n+    }\n+}\n+\n+/* Free memory used by store motion.  */\n+\n+static void \n+free_store_memory ()\n+{\n+  free_ldst_mems ();\n+  \n+  if (ae_gen)\n+    free (ae_gen);\n+  if (ae_kill)\n+    free (ae_kill);\n+  if (transp)\n+    free (transp);\n+  if (st_antloc)\n+    free (st_antloc);\n+  if (pre_insert_map)\n+    free (pre_insert_map);\n+  if (pre_delete_map)\n+    free (pre_delete_map);\n+  if (reg_set_in_block)\n+    free (reg_set_in_block);\n+  \n+  ae_gen = ae_kill = transp = st_antloc = NULL;\n+  pre_insert_map = pre_delete_map = reg_set_in_block = NULL;\n+}\n+\n+/* Perform store motion. Much like gcse, except we move expressions the\n+   other way by looking at the flowgraph in reverse.  */\n+\n+static void\n+store_motion ()\n+{\n+  int x;\n+  struct ls_expr * ptr;\n+  int update_flow = 0;\n+\n+  if (gcse_file)\n+    {\n+      fprintf (gcse_file, \"before store motion\\n\");\n+      print_rtl (gcse_file, get_insns ());\n+    }\n+\n+\n+  init_alias_analysis ();\n+\n+  /* Find all the stores that are live to the end of their block.  */\n+  num_stores = compute_store_table ();\n+  if (num_stores == 0)\n+    {\n+      free (reg_set_in_block);\n+      end_alias_analysis ();\n+      return;\n+    }\n+\n+  /* Now compute whats actually available to move.  */\n+  add_noreturn_fake_exit_edges ();\n+  build_store_vectors ();\n+\n+  edge_list = pre_edge_rev_lcm (gcse_file, num_stores, transp, ae_gen, \n+\t\t\t\tst_antloc, ae_kill, &pre_insert_map, \n+\t\t\t\t&pre_delete_map);\n+\n+  /* Now we want to insert the new stores which are going to be needed.  */\n+  for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n+    {\n+      for (x = 0; x < n_basic_blocks; x++)\n+\tif (TEST_BIT (pre_delete_map[x], ptr->index))\n+\t  delete_store (ptr, x);\n+\n+      for (x = 0; x < NUM_EDGES (edge_list); x++)\n+\tif (TEST_BIT (pre_insert_map[x], ptr->index))\n+\t  update_flow |= insert_store (ptr, INDEX_EDGE (edge_list, x));\n+    }\n+\n+  if (update_flow)\n+    commit_edge_insertions ();\n+\n+  free_store_memory ();\n+  free_edge_list (edge_list);\n+  remove_fake_edges ();\n+  end_alias_analysis ();\n+}"}, {"sha": "d3c179be07216e7af0d6a8d9261dcb28ee38d6cf", "filename": "gcc/invoke.texi", "status": "modified", "additions": 13, "deletions": 1, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/gcc%2Finvoke.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/gcc%2Finvoke.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Finvoke.texi?ref=a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93", "patch": "@@ -241,7 +241,7 @@ in the following sections.\n -fcse-follow-jumps  -fcse-skip-blocks  -fdata-sections  -fdce @gol\n -fdelayed-branch  -fdelete-null-pointer-checks @gol\n -fexpensive-optimizations  -ffast-math  -ffloat-store @gol\n--fforce-addr  -fforce-mem  -ffunction-sections  -fgcse  @gol\n+-fforce-addr  -fforce-mem  -ffunction-sections  -fgcse  -fgcse-lm -fgcse-sm @gol\n -finline-functions  -finline-limit=@var{n}  -fkeep-inline-functions @gol\n -fkeep-static-consts  -fmove-all-movables @gol\n -fno-default-inline  -fno-defer-pop @gol\n@@ -3034,6 +3034,18 @@ Run the loop optimizer twice.\n Perform a global common subexpression elimination pass.\n This pass also performs global constant and copy propagation.\n \n+@item -fgcse-lm\n+When -fgcse-lm is enabled, global common subexpression elimination will\n+attempt to move loads which are only killed by stores into themselves. This\n+allows a loop containing a load/store sequence to be changed to a load outside\n+the loop, and a copy/store within the loop. \n+\n+@item -fgcse-sm\n+When -fgcse-sm is enabled, A store motion pass is run after global common \n+subexpression elimination. This pass will attempt to move stores out of loops.\n+When used in conjunction with -fgcse-lm, loops containing a load/store sequence\n+can be changed to a load before the loop and a store after the loop.\n+\n @item -fdelete-null-pointer-checks\n Use global dataflow analysis to identify and eliminate useless null\n pointer checks.  Programs which rely on NULL pointer dereferences @emph{not}"}, {"sha": "11da9c5dc48a6836653efd21d5623ba8d8add1fd", "filename": "gcc/rtl.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/gcc%2Frtl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/gcc%2Frtl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtl.h?ref=a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93", "patch": "@@ -2031,6 +2031,9 @@ extern void fancy_abort PARAMS ((const char *, int, const char *))\n extern rtx canon_rtx                    PARAMS ((rtx));\n extern int true_dependence\t\tPARAMS ((rtx, enum machine_mode, rtx,\n \t\t\t\t\t\tint (*)(rtx, int)));\n+extern rtx get_addr                     PARAMS ((rtx));\n+extern int canon_true_dependence        PARAMS ((rtx, enum machine_mode, rtx,\n+                                                rtx, int (*)(rtx, int)));\n extern int read_dependence\t\tPARAMS ((rtx, rtx));\n extern int anti_dependence\t\tPARAMS ((rtx, rtx));\n extern int output_dependence\t\tPARAMS ((rtx, rtx));"}, {"sha": "043c0b633575baff745917ffed478e7d71b61382", "filename": "gcc/toplev.c", "status": "modified", "additions": 16, "deletions": 0, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/gcc%2Ftoplev.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93/gcc%2Ftoplev.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftoplev.c?ref=a13d4ebfc3445a9a349b8ee4cf39b712a96e0a93", "patch": "@@ -668,6 +668,18 @@ static int flag_gcse;\n \n static int flag_delete_null_pointer_checks;\n \n+/* Nonzero means to do the enhanced load motion during gcse, which trys\n+   to hoist loads by not killing them when a store to the same location\n+   is seen.  */\n+\n+int flag_gcse_lm = 1;\n+\n+/* Nonzero means to perform store motion after gcse, which will try to\n+   move stores closer to the exit block.  Its not very effective without\n+   flag_gcse_lm.  */\n+\n+int flag_gcse_sm = 1;\n+\n /* Nonzero means to rerun cse after loop optimization.  This increases\n    compilation time about 20% and picks up a few more common expressions.  */\n \n@@ -1047,6 +1059,10 @@ lang_independent_options f_options[] =\n    \"Attempt to fill delay slots of branch instructions\" },\n   {\"gcse\", &flag_gcse, 1,\n    \"Perform the global common subexpression elimination\" },\n+  {\"gcse-lm\", &flag_gcse_lm, 1,\n+   \"Perform enhanced load motion during global subexpression elimination\" },\n+  {\"gcse-sm\", &flag_gcse_sm, 1,\n+   \"Perform store motion after global subexpression elimination\" },\n   {\"rerun-cse-after-loop\", &flag_rerun_cse_after_loop, 1,\n    \"Run CSE pass after loop optimisations\"},\n   {\"rerun-loop-opt\", &flag_rerun_loop_opt, 1,"}]}