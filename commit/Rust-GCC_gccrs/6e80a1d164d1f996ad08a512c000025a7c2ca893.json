{"sha": "6e80a1d164d1f996ad08a512c000025a7c2ca893", "node_id": "C_kwDOANBUbNoAKDZlODBhMWQxNjRkMWY5OTZhZDA4YTUxMmMwMDAwMjVhN2MyY2E4OTM", "commit": {"author": {"name": "Thomas Neumann", "email": "tneumann@users.sourceforge.net", "date": "2022-03-01T20:57:35Z"}, "committer": {"name": "Thomas Neumann", "email": "tneumann@users.sourceforge.net", "date": "2022-09-16T22:58:14Z"}, "message": "eliminate mutex in fast path of __register_frame\n\nThe __register_frame/__deregister_frame functions are used to register\nunwinding frames from JITed code in a sorted list. That list itself\nis protected by object_mutex, which leads to terrible performance\nin multi-threaded code and is somewhat expensive even if single-threaded.\nThere was already a fast-path that avoided taking the mutex if no\nframe was registered at all.\n\nThis commit eliminates both the mutex and the sorted list from\nthe atomic fast path, and replaces it with a btree that uses\noptimistic lock coupling during lookup. This allows for fully parallel\nunwinding and is essential to scale exception handling to large\ncore counts.\n\nlibgcc/ChangeLog:\n\n\t* unwind-dw2-fde.c (release_registered_frames): Cleanup at shutdown.\n\t(__register_frame_info_table_bases): Use btree in atomic fast path.\n\t(__deregister_frame_info_bases): Likewise.\n\t(_Unwind_Find_FDE): Likewise.\n\t(base_from_object): Make parameter const.\n\t(classify_object_over_fdes): Add query-only mode.\n\t(get_pc_range): Compute PC range for lookup.\n\t* unwind-dw2-fde.h (last_fde): Make parameter const.\n\t* unwind-dw2-btree.h: New file.", "tree": {"sha": "650091c6cd09cfdc3daaa84492caa1213b18f348", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/650091c6cd09cfdc3daaa84492caa1213b18f348"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/6e80a1d164d1f996ad08a512c000025a7c2ca893", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6e80a1d164d1f996ad08a512c000025a7c2ca893", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6e80a1d164d1f996ad08a512c000025a7c2ca893", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6e80a1d164d1f996ad08a512c000025a7c2ca893/comments", "author": {"login": "neumannt", "id": 25097991, "node_id": "MDQ6VXNlcjI1MDk3OTkx", "avatar_url": "https://avatars.githubusercontent.com/u/25097991?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neumannt", "html_url": "https://github.com/neumannt", "followers_url": "https://api.github.com/users/neumannt/followers", "following_url": "https://api.github.com/users/neumannt/following{/other_user}", "gists_url": "https://api.github.com/users/neumannt/gists{/gist_id}", "starred_url": "https://api.github.com/users/neumannt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neumannt/subscriptions", "organizations_url": "https://api.github.com/users/neumannt/orgs", "repos_url": "https://api.github.com/users/neumannt/repos", "events_url": "https://api.github.com/users/neumannt/events{/privacy}", "received_events_url": "https://api.github.com/users/neumannt/received_events", "type": "User", "site_admin": false}, "committer": {"login": "neumannt", "id": 25097991, "node_id": "MDQ6VXNlcjI1MDk3OTkx", "avatar_url": "https://avatars.githubusercontent.com/u/25097991?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neumannt", "html_url": "https://github.com/neumannt", "followers_url": "https://api.github.com/users/neumannt/followers", "following_url": "https://api.github.com/users/neumannt/following{/other_user}", "gists_url": "https://api.github.com/users/neumannt/gists{/gist_id}", "starred_url": "https://api.github.com/users/neumannt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neumannt/subscriptions", "organizations_url": "https://api.github.com/users/neumannt/orgs", "repos_url": "https://api.github.com/users/neumannt/repos", "events_url": "https://api.github.com/users/neumannt/events{/privacy}", "received_events_url": "https://api.github.com/users/neumannt/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "cf0fded5d837bad590eb091d8a3dc4898872560f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/cf0fded5d837bad590eb091d8a3dc4898872560f", "html_url": "https://github.com/Rust-GCC/gccrs/commit/cf0fded5d837bad590eb091d8a3dc4898872560f"}], "stats": {"total": 1149, "additions": 1098, "deletions": 51}, "files": [{"sha": "8853f0eab486b847393c14df224373c9881c3fcd", "filename": "libgcc/unwind-dw2-btree.h", "status": "added", "additions": 953, "deletions": 0, "changes": 953, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6e80a1d164d1f996ad08a512c000025a7c2ca893/libgcc%2Funwind-dw2-btree.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6e80a1d164d1f996ad08a512c000025a7c2ca893/libgcc%2Funwind-dw2-btree.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgcc%2Funwind-dw2-btree.h?ref=6e80a1d164d1f996ad08a512c000025a7c2ca893", "patch": "@@ -0,0 +1,953 @@\n+/* Lock-free btree for manually registered unwind frames.  */\n+/* Copyright (C) 2022 Free Software Foundation, Inc.\n+   Contributed by Thomas Neumann\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+Under Section 7 of GPL version 3, you are granted additional\n+permissions described in the GCC Runtime Library Exception, version\n+3.1, as published by the Free Software Foundation.\n+\n+You should have received a copy of the GNU General Public License and\n+a copy of the GCC Runtime Library Exception along with this program;\n+see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#ifndef GCC_UNWIND_DW2_BTREE_H\n+#define GCC_UNWIND_DW2_BTREE_H\n+\n+#include <stdbool.h>\n+\n+// Common logic for version locks.\n+struct version_lock\n+{\n+  // The lock itself. The lowest bit indicates an exclusive lock,\n+  // the second bit indicates waiting threads. All other bits are\n+  // used as counter to recognize changes.\n+  // Overflows are okay here, we must only prevent overflow to the\n+  // same value within one lock_optimistic/validate\n+  // range. Even on 32 bit platforms that would require 1 billion\n+  // frame registrations within the time span of a few assembler\n+  // instructions.\n+  uintptr_t version_lock;\n+};\n+\n+#ifdef __GTHREAD_HAS_COND\n+// We should never get contention within the tree as it rarely changes.\n+// But if we ever do get contention we use these for waiting.\n+static __gthread_mutex_t version_lock_mutex = __GTHREAD_MUTEX_INIT;\n+static __gthread_cond_t version_lock_cond = __GTHREAD_COND_INIT;\n+#endif\n+\n+// Initialize in locked state.\n+static inline void\n+version_lock_initialize_locked_exclusive (struct version_lock *vl)\n+{\n+  vl->version_lock = 1;\n+}\n+\n+// Try to lock the node exclusive.\n+static inline bool\n+version_lock_try_lock_exclusive (struct version_lock *vl)\n+{\n+  uintptr_t state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n+  if (state & 1)\n+    return false;\n+  return __atomic_compare_exchange_n (&(vl->version_lock), &state, state | 1,\n+\t\t\t\t      false, __ATOMIC_SEQ_CST,\n+\t\t\t\t      __ATOMIC_SEQ_CST);\n+}\n+\n+// Lock the node exclusive, blocking as needed.\n+static void\n+version_lock_lock_exclusive (struct version_lock *vl)\n+{\n+#ifndef __GTHREAD_HAS_COND\n+restart:\n+#endif\n+\n+  // We should virtually never get contention here, as frame\n+  // changes are rare.\n+  uintptr_t state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n+  if (!(state & 1))\n+    {\n+      if (__atomic_compare_exchange_n (&(vl->version_lock), &state, state | 1,\n+\t\t\t\t       false, __ATOMIC_SEQ_CST,\n+\t\t\t\t       __ATOMIC_SEQ_CST))\n+\treturn;\n+    }\n+\n+    // We did get contention, wait properly.\n+#ifdef __GTHREAD_HAS_COND\n+  __gthread_mutex_lock (&version_lock_mutex);\n+  state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n+  while (true)\n+    {\n+      // Check if the lock is still held.\n+      if (!(state & 1))\n+\t{\n+\t  if (__atomic_compare_exchange_n (&(vl->version_lock), &state,\n+\t\t\t\t\t   state | 1, false, __ATOMIC_SEQ_CST,\n+\t\t\t\t\t   __ATOMIC_SEQ_CST))\n+\t    {\n+\t      __gthread_mutex_unlock (&version_lock_mutex);\n+\t      return;\n+\t    }\n+\t  else\n+\t    {\n+\t      continue;\n+\t    }\n+\t}\n+\n+      // Register waiting thread.\n+      if (!(state & 2))\n+\t{\n+\t  if (!__atomic_compare_exchange_n (&(vl->version_lock), &state,\n+\t\t\t\t\t    state | 2, false, __ATOMIC_SEQ_CST,\n+\t\t\t\t\t    __ATOMIC_SEQ_CST))\n+\t    continue;\n+\t}\n+\n+      // And sleep.\n+      __gthread_cond_wait (&version_lock_cond, &version_lock_mutex);\n+      state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n+    }\n+#else\n+  // Spin if we do not have condition variables available.\n+  // We expect no contention here, spinning should be okay.\n+  goto restart;\n+#endif\n+}\n+\n+// Release a locked node and increase the version lock.\n+static void\n+version_lock_unlock_exclusive (struct version_lock *vl)\n+{\n+  // increase version, reset exclusive lock bits\n+  uintptr_t state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n+  uintptr_t ns = (state + 4) & (~((uintptr_t) 3));\n+  state = __atomic_exchange_n (&(vl->version_lock), ns, __ATOMIC_SEQ_CST);\n+\n+#ifdef __GTHREAD_HAS_COND\n+  if (state & 2)\n+    {\n+      // Wake up waiting threads. This should be extremely rare.\n+      __gthread_mutex_lock (&version_lock_mutex);\n+      __gthread_cond_broadcast (&version_lock_cond);\n+      __gthread_mutex_unlock (&version_lock_mutex);\n+    }\n+#endif\n+}\n+\n+// Acquire an optimistic \"lock\". Note that this does not lock at all, it\n+// only allows for validation later.\n+static inline bool\n+version_lock_lock_optimistic (const struct version_lock *vl, uintptr_t *lock)\n+{\n+  uintptr_t state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n+  *lock = state;\n+\n+  // Acquiring the lock fails when there is currently an exclusive lock.\n+  return !(state & 1);\n+}\n+\n+// Validate a previously acquired \"lock\".\n+static inline bool\n+version_lock_validate (const struct version_lock *vl, uintptr_t lock)\n+{\n+  // Prevent the reordering of non-atomic loads behind the atomic load.\n+  // Hans Boehm, Can Seqlocks Get Along with Programming Language Memory\n+  // Models?, Section 4.\n+  __atomic_thread_fence (__ATOMIC_ACQUIRE);\n+\n+  // Check that the node is still in the same state.\n+  uintptr_t state = __atomic_load_n (&(vl->version_lock), __ATOMIC_SEQ_CST);\n+  return (state == lock);\n+}\n+\n+// The largest possible separator value.\n+static const uintptr_t max_separator = ~((uintptr_t) (0));\n+\n+struct btree_node;\n+\n+// Inner entry. The child tree contains all entries <= separator.\n+struct inner_entry\n+{\n+  uintptr_t separator;\n+  struct btree_node *child;\n+};\n+\n+// Leaf entry. Stores an object entry.\n+struct leaf_entry\n+{\n+  uintptr_t base, size;\n+  struct object *ob;\n+};\n+\n+// Node types.\n+enum node_type\n+{\n+  btree_node_inner,\n+  btree_node_leaf,\n+  btree_node_free\n+};\n+\n+// Node sizes. Chosen such that the result size is roughly 256 bytes.\n+#define max_fanout_inner 15\n+#define max_fanout_leaf 10\n+\n+// A btree node.\n+struct btree_node\n+{\n+  // The version lock used for optimistic lock coupling.\n+  struct version_lock version_lock;\n+  // The number of entries.\n+  unsigned entry_count;\n+  // The type.\n+  enum node_type type;\n+  // The payload.\n+  union\n+  {\n+    // The inner nodes have fence keys, i.e., the right-most entry includes a\n+    // separator.\n+    struct inner_entry children[max_fanout_inner];\n+    struct leaf_entry entries[max_fanout_leaf];\n+  } content;\n+};\n+\n+// Is an inner node?\n+static inline bool\n+btree_node_is_inner (const struct btree_node *n)\n+{\n+  return n->type == btree_node_inner;\n+}\n+\n+// Is a leaf node?\n+static inline bool\n+btree_node_is_leaf (const struct btree_node *n)\n+{\n+  return n->type == btree_node_leaf;\n+}\n+\n+// Should the node be merged?\n+static inline bool\n+btree_node_needs_merge (const struct btree_node *n)\n+{\n+  return n->entry_count < (btree_node_is_inner (n) ? (max_fanout_inner / 2)\n+\t\t\t\t\t\t   : (max_fanout_leaf / 2));\n+}\n+\n+// Get the fence key for inner nodes.\n+static inline uintptr_t\n+btree_node_get_fence_key (const struct btree_node *n)\n+{\n+  // For inner nodes we just return our right-most entry.\n+  return n->content.children[n->entry_count - 1].separator;\n+}\n+\n+// Find the position for a slot in an inner node.\n+static unsigned\n+btree_node_find_inner_slot (const struct btree_node *n, uintptr_t value)\n+{\n+  for (unsigned index = 0, ec = n->entry_count; index != ec; ++index)\n+    if (n->content.children[index].separator >= value)\n+      return index;\n+  return n->entry_count;\n+}\n+\n+// Find the position for a slot in a leaf node.\n+static unsigned\n+btree_node_find_leaf_slot (const struct btree_node *n, uintptr_t value)\n+{\n+  for (unsigned index = 0, ec = n->entry_count; index != ec; ++index)\n+    if (n->content.entries[index].base + n->content.entries[index].size > value)\n+      return index;\n+  return n->entry_count;\n+}\n+\n+// Try to lock the node exclusive.\n+static inline bool\n+btree_node_try_lock_exclusive (struct btree_node *n)\n+{\n+  return version_lock_try_lock_exclusive (&(n->version_lock));\n+}\n+\n+// Lock the node exclusive, blocking as needed.\n+static inline void\n+btree_node_lock_exclusive (struct btree_node *n)\n+{\n+  version_lock_lock_exclusive (&(n->version_lock));\n+}\n+\n+// Release a locked node and increase the version lock.\n+static inline void\n+btree_node_unlock_exclusive (struct btree_node *n)\n+{\n+  version_lock_unlock_exclusive (&(n->version_lock));\n+}\n+\n+// Acquire an optimistic \"lock\". Note that this does not lock at all, it\n+// only allows for validation later.\n+static inline bool\n+btree_node_lock_optimistic (const struct btree_node *n, uintptr_t *lock)\n+{\n+  return version_lock_lock_optimistic (&(n->version_lock), lock);\n+}\n+\n+// Validate a previously acquire lock.\n+static inline bool\n+btree_node_validate (const struct btree_node *n, uintptr_t lock)\n+{\n+  return version_lock_validate (&(n->version_lock), lock);\n+}\n+\n+// Insert a new separator after splitting.\n+static void\n+btree_node_update_separator_after_split (struct btree_node *n,\n+\t\t\t\t\t uintptr_t old_separator,\n+\t\t\t\t\t uintptr_t new_separator,\n+\t\t\t\t\t struct btree_node *new_right)\n+{\n+  unsigned slot = btree_node_find_inner_slot (n, old_separator);\n+  for (unsigned index = n->entry_count; index > slot; --index)\n+    n->content.children[index] = n->content.children[index - 1];\n+  n->content.children[slot].separator = new_separator;\n+  n->content.children[slot + 1].child = new_right;\n+  n->entry_count++;\n+}\n+\n+// A btree. Suitable for static initialization, all members are zero at the\n+// beginning.\n+struct btree\n+{\n+  // The root of the btree.\n+  struct btree_node *root;\n+  // The free list of released node.\n+  struct btree_node *free_list;\n+  // The version lock used to protect the root.\n+  struct version_lock root_lock;\n+};\n+\n+// Initialize a btree. Not actually used, just for exposition.\n+static inline void\n+btree_init (struct btree *t)\n+{\n+  t->root = NULL;\n+  t->free_list = NULL;\n+  t->root_lock.version_lock = 0;\n+};\n+\n+static void\n+btree_release_tree_recursively (struct btree *t, struct btree_node *n);\n+\n+// Destroy a tree and release all nodes.\n+static void\n+btree_destroy (struct btree *t)\n+{\n+  // Disable the mechanism before cleaning up.\n+  struct btree_node *old_root\n+    = __atomic_exchange_n (&(t->root), NULL, __ATOMIC_SEQ_CST);\n+  if (old_root)\n+    btree_release_tree_recursively (t, old_root);\n+\n+  // Release all free nodes.\n+  while (t->free_list)\n+    {\n+      struct btree_node *next = t->free_list->content.children[0].child;\n+      free (t->free_list);\n+      t->free_list = next;\n+    }\n+}\n+\n+// Allocate a node. This node will be returned in locked exclusive state.\n+static struct btree_node *\n+btree_allocate_node (struct btree *t, bool inner)\n+{\n+  while (true)\n+    {\n+      // Try the free list first.\n+      struct btree_node *next_free\n+\t= __atomic_load_n (&(t->free_list), __ATOMIC_SEQ_CST);\n+      if (next_free)\n+\t{\n+\t  if (!btree_node_try_lock_exclusive (next_free))\n+\t    continue;\n+\t  // The node might no longer be free, check that again after acquiring\n+\t  // the exclusive lock.\n+\t  if (next_free->type == btree_node_free)\n+\t    {\n+\t      struct btree_node *ex = next_free;\n+\t      if (__atomic_compare_exchange_n (\n+\t\t    &(t->free_list), &ex, next_free->content.children[0].child,\n+\t\t    false, __ATOMIC_SEQ_CST, __ATOMIC_SEQ_CST))\n+\t\t{\n+\t\t  next_free->entry_count = 0;\n+\t\t  next_free->type = inner ? btree_node_inner : btree_node_leaf;\n+\t\t  return next_free;\n+\t\t}\n+\t    }\n+\t  btree_node_unlock_exclusive (next_free);\n+\t  continue;\n+\t}\n+\n+      // No free node available, allocate a new one.\n+      struct btree_node *new_node\n+\t= (struct btree_node *) (malloc (sizeof (struct btree_node)));\n+      version_lock_initialize_locked_exclusive (\n+\t&(new_node->version_lock)); // initialize the node in locked state.\n+      new_node->entry_count = 0;\n+      new_node->type = inner ? btree_node_inner : btree_node_leaf;\n+      return new_node;\n+    }\n+}\n+\n+// Release a node. This node must be currently locked exclusively and will\n+// be placed in the free list.\n+static void\n+btree_release_node (struct btree *t, struct btree_node *node)\n+{\n+  // We cannot release the memory immediately because there might still be\n+  // concurrent readers on that node. Put it in the free list instead.\n+  node->type = btree_node_free;\n+  struct btree_node *next_free\n+    = __atomic_load_n (&(t->free_list), __ATOMIC_SEQ_CST);\n+  do\n+    {\n+      node->content.children[0].child = next_free;\n+  } while (!__atomic_compare_exchange_n (&(t->free_list), &next_free, node,\n+\t\t\t\t\t false, __ATOMIC_SEQ_CST,\n+\t\t\t\t\t __ATOMIC_SEQ_CST));\n+  btree_node_unlock_exclusive (node);\n+}\n+\n+// Recursively release a tree. The btree is by design very shallow, thus\n+// we can risk recursion here.\n+static void\n+btree_release_tree_recursively (struct btree *t, struct btree_node *node)\n+{\n+  btree_node_lock_exclusive (node);\n+  if (btree_node_is_inner (node))\n+    {\n+      for (unsigned index = 0; index < node->entry_count; ++index)\n+\tbtree_release_tree_recursively (t, node->content.children[index].child);\n+    }\n+  btree_release_node (t, node);\n+}\n+\n+// Check if we are splitting the root.\n+static void\n+btree_handle_root_split (struct btree *t, struct btree_node **node,\n+\t\t\t struct btree_node **parent)\n+{\n+  // We want to keep the root pointer stable to allow for contention\n+  // free reads. Thus, we split the root by first moving the content\n+  // of the root node to a new node, and then split that new node.\n+  if (!*parent)\n+    {\n+      // Allocate a new node, this guarantees us that we will have a parent\n+      // afterwards.\n+      struct btree_node *new_node\n+\t= btree_allocate_node (t, btree_node_is_inner (*node));\n+      struct btree_node *old_node = *node;\n+      new_node->entry_count = old_node->entry_count;\n+      new_node->content = old_node->content;\n+      old_node->content.children[0].separator = max_separator;\n+      old_node->content.children[0].child = new_node;\n+      old_node->entry_count = 1;\n+      old_node->type = btree_node_inner;\n+\n+      *parent = old_node;\n+      *node = new_node;\n+    }\n+}\n+\n+// Split an inner node.\n+static void\n+btree_split_inner (struct btree *t, struct btree_node **inner,\n+\t\t   struct btree_node **parent, uintptr_t target)\n+{\n+  // Check for the root.\n+  btree_handle_root_split (t, inner, parent);\n+\n+  // Create two inner node.\n+  uintptr_t right_fence = btree_node_get_fence_key (*inner);\n+  struct btree_node *left_inner = *inner;\n+  struct btree_node *right_inner = btree_allocate_node (t, true);\n+  unsigned split = left_inner->entry_count / 2;\n+  right_inner->entry_count = left_inner->entry_count - split;\n+  for (unsigned index = 0; index < right_inner->entry_count; ++index)\n+    right_inner->content.children[index]\n+      = left_inner->content.children[split + index];\n+  left_inner->entry_count = split;\n+  uintptr_t left_fence = btree_node_get_fence_key (left_inner);\n+  btree_node_update_separator_after_split (*parent, right_fence, left_fence,\n+\t\t\t\t\t   right_inner);\n+  if (target <= left_fence)\n+    {\n+      *inner = left_inner;\n+      btree_node_unlock_exclusive (right_inner);\n+    }\n+  else\n+    {\n+      *inner = right_inner;\n+      btree_node_unlock_exclusive (left_inner);\n+    }\n+}\n+\n+// Split a leaf node.\n+static void\n+btree_split_leaf (struct btree *t, struct btree_node **leaf,\n+\t\t  struct btree_node **parent, uintptr_t fence, uintptr_t target)\n+{\n+  // Check for the root.\n+  btree_handle_root_split (t, leaf, parent);\n+\n+  // Create two leaf nodes.\n+  uintptr_t right_fence = fence;\n+  struct btree_node *left_leaf = *leaf;\n+  struct btree_node *right_leaf = btree_allocate_node (t, false);\n+  unsigned split = left_leaf->entry_count / 2;\n+  right_leaf->entry_count = left_leaf->entry_count - split;\n+  for (unsigned index = 0; index != right_leaf->entry_count; ++index)\n+    right_leaf->content.entries[index]\n+      = left_leaf->content.entries[split + index];\n+  left_leaf->entry_count = split;\n+  uintptr_t left_fence = right_leaf->content.entries[0].base - 1;\n+  btree_node_update_separator_after_split (*parent, right_fence, left_fence,\n+\t\t\t\t\t   right_leaf);\n+  if (target <= left_fence)\n+    {\n+      *leaf = left_leaf;\n+      btree_node_unlock_exclusive (right_leaf);\n+    }\n+  else\n+    {\n+      *leaf = right_leaf;\n+      btree_node_unlock_exclusive (left_leaf);\n+    }\n+}\n+\n+// Merge (or balance) child nodes.\n+static struct btree_node *\n+btree_merge_node (struct btree *t, unsigned child_slot,\n+\t\t  struct btree_node *parent, uintptr_t target)\n+{\n+  // Choose the emptiest neighbor and lock both. The target child is already\n+  // locked.\n+  unsigned left_slot;\n+  struct btree_node *left_node, *right_node;\n+  if ((child_slot == 0)\n+      || (((child_slot + 1) < parent->entry_count)\n+\t  && (parent->content.children[child_slot + 1].child->entry_count\n+\t      < parent->content.children[child_slot - 1].child->entry_count)))\n+    {\n+      left_slot = child_slot;\n+      left_node = parent->content.children[left_slot].child;\n+      right_node = parent->content.children[left_slot + 1].child;\n+      btree_node_lock_exclusive (right_node);\n+    }\n+  else\n+    {\n+      left_slot = child_slot - 1;\n+      left_node = parent->content.children[left_slot].child;\n+      right_node = parent->content.children[left_slot + 1].child;\n+      btree_node_lock_exclusive (left_node);\n+    }\n+\n+  // Can we merge both nodes into one node?\n+  unsigned total_count = left_node->entry_count + right_node->entry_count;\n+  unsigned max_count\n+    = btree_node_is_inner (left_node) ? max_fanout_inner : max_fanout_leaf;\n+  if (total_count <= max_count)\n+    {\n+      // Merge into the parent?\n+      if (parent->entry_count == 2)\n+\t{\n+\t  // Merge children into parent. This can only happen at the root.\n+\t  if (btree_node_is_inner (left_node))\n+\t    {\n+\t      for (unsigned index = 0; index != left_node->entry_count; ++index)\n+\t\tparent->content.children[index]\n+\t\t  = left_node->content.children[index];\n+\t      for (unsigned index = 0; index != right_node->entry_count;\n+\t\t   ++index)\n+\t\tparent->content.children[index + left_node->entry_count]\n+\t\t  = right_node->content.children[index];\n+\t    }\n+\t  else\n+\t    {\n+\t      parent->type = btree_node_leaf;\n+\t      for (unsigned index = 0; index != left_node->entry_count; ++index)\n+\t\tparent->content.entries[index]\n+\t\t  = left_node->content.entries[index];\n+\t      for (unsigned index = 0; index != right_node->entry_count;\n+\t\t   ++index)\n+\t\tparent->content.entries[index + left_node->entry_count]\n+\t\t  = right_node->content.entries[index];\n+\t    }\n+\t  parent->entry_count = total_count;\n+\t  btree_release_node (t, left_node);\n+\t  btree_release_node (t, right_node);\n+\t  return parent;\n+\t}\n+      else\n+\t{\n+\t  // Regular merge.\n+\t  if (btree_node_is_inner (left_node))\n+\t    {\n+\t      for (unsigned index = 0; index != right_node->entry_count;\n+\t\t   ++index)\n+\t\tleft_node->content.children[left_node->entry_count++]\n+\t\t  = right_node->content.children[index];\n+\t    }\n+\t  else\n+\t    {\n+\t      for (unsigned index = 0; index != right_node->entry_count;\n+\t\t   ++index)\n+\t\tleft_node->content.entries[left_node->entry_count++]\n+\t\t  = right_node->content.entries[index];\n+\t    }\n+\t  parent->content.children[left_slot].separator\n+\t    = parent->content.children[left_slot + 1].separator;\n+\t  for (unsigned index = left_slot + 1; index + 1 < parent->entry_count;\n+\t       ++index)\n+\t    parent->content.children[index]\n+\t      = parent->content.children[index + 1];\n+\t  parent->entry_count--;\n+\t  btree_release_node (t, right_node);\n+\t  btree_node_unlock_exclusive (parent);\n+\t  return left_node;\n+\t}\n+    }\n+\n+  // No merge possible, rebalance instead.\n+  if (left_node->entry_count > right_node->entry_count)\n+    {\n+      // Shift from left to right.\n+      unsigned to_shift\n+\t= (left_node->entry_count - right_node->entry_count) / 2;\n+      if (btree_node_is_inner (left_node))\n+\t{\n+\t  for (unsigned index = 0; index != right_node->entry_count; ++index)\n+\t    {\n+\t      unsigned pos = right_node->entry_count - 1 - index;\n+\t      right_node->content.children[pos + to_shift]\n+\t\t= right_node->content.children[pos];\n+\t    }\n+\t  for (unsigned index = 0; index != to_shift; ++index)\n+\t    right_node->content.children[index]\n+\t      = left_node->content\n+\t\t  .children[left_node->entry_count - to_shift + index];\n+\t}\n+      else\n+\t{\n+\t  for (unsigned index = 0; index != right_node->entry_count; ++index)\n+\t    {\n+\t      unsigned pos = right_node->entry_count - 1 - index;\n+\t      right_node->content.entries[pos + to_shift]\n+\t\t= right_node->content.entries[pos];\n+\t    }\n+\t  for (unsigned index = 0; index != to_shift; ++index)\n+\t    right_node->content.entries[index]\n+\t      = left_node->content\n+\t\t  .entries[left_node->entry_count - to_shift + index];\n+\t}\n+      left_node->entry_count -= to_shift;\n+      right_node->entry_count += to_shift;\n+    }\n+  else\n+    {\n+      // Shift from right to left.\n+      unsigned to_shift\n+\t= (right_node->entry_count - left_node->entry_count) / 2;\n+      if (btree_node_is_inner (left_node))\n+\t{\n+\t  for (unsigned index = 0; index != to_shift; ++index)\n+\t    left_node->content.children[left_node->entry_count + index]\n+\t      = right_node->content.children[index];\n+\t  for (unsigned index = 0; index != right_node->entry_count - to_shift;\n+\t       ++index)\n+\t    right_node->content.children[index]\n+\t      = right_node->content.children[index + to_shift];\n+\t}\n+      else\n+\t{\n+\t  for (unsigned index = 0; index != to_shift; ++index)\n+\t    left_node->content.entries[left_node->entry_count + index]\n+\t      = right_node->content.entries[index];\n+\t  for (unsigned index = 0; index != right_node->entry_count - to_shift;\n+\t       ++index)\n+\t    right_node->content.entries[index]\n+\t      = right_node->content.entries[index + to_shift];\n+\t}\n+      left_node->entry_count += to_shift;\n+      right_node->entry_count -= to_shift;\n+    }\n+  uintptr_t left_fence;\n+  if (btree_node_is_leaf (left_node))\n+    {\n+      left_fence = right_node->content.entries[0].base - 1;\n+    }\n+  else\n+    {\n+      left_fence = btree_node_get_fence_key (left_node);\n+    }\n+  parent->content.children[left_slot].separator = left_fence;\n+  btree_node_unlock_exclusive (parent);\n+  if (target <= left_fence)\n+    {\n+      btree_node_unlock_exclusive (right_node);\n+      return left_node;\n+    }\n+  else\n+    {\n+      btree_node_unlock_exclusive (left_node);\n+      return right_node;\n+    }\n+}\n+\n+// Insert an entry.\n+static bool\n+btree_insert (struct btree *t, uintptr_t base, uintptr_t size,\n+\t      struct object *ob)\n+{\n+  // Sanity check.\n+  if (!size)\n+    return false;\n+\n+  // Access the root.\n+  struct btree_node *iter, *parent = NULL;\n+  {\n+    version_lock_lock_exclusive (&(t->root_lock));\n+    iter = t->root;\n+    if (iter)\n+      {\n+\tbtree_node_lock_exclusive (iter);\n+      }\n+    else\n+      {\n+\tt->root = iter = btree_allocate_node (t, false);\n+      }\n+    version_lock_unlock_exclusive (&(t->root_lock));\n+  }\n+\n+  // Walk down the btree with classic lock coupling and eager splits.\n+  // Strictly speaking this is not performance optimal, we could use\n+  // optimistic lock coupling until we hit a node that has to be modified.\n+  // But that is more difficult to implement and frame registration is\n+  // rare anyway, we use simple locking for now.\n+\n+  uintptr_t fence = max_separator;\n+  while (btree_node_is_inner (iter))\n+    {\n+      // Use eager splits to avoid lock coupling up.\n+      if (iter->entry_count == max_fanout_inner)\n+\tbtree_split_inner (t, &iter, &parent, base);\n+\n+      unsigned slot = btree_node_find_inner_slot (iter, base);\n+      if (parent)\n+\tbtree_node_unlock_exclusive (parent);\n+      parent = iter;\n+      fence = iter->content.children[slot].separator;\n+      iter = iter->content.children[slot].child;\n+      btree_node_lock_exclusive (iter);\n+    }\n+\n+  // Make sure we have space.\n+  if (iter->entry_count == max_fanout_leaf)\n+    btree_split_leaf (t, &iter, &parent, fence, base);\n+  if (parent)\n+    btree_node_unlock_exclusive (parent);\n+\n+  // Insert in node.\n+  unsigned slot = btree_node_find_leaf_slot (iter, base);\n+  if ((slot < iter->entry_count) && (iter->content.entries[slot].base == base))\n+    {\n+      // Duplicate entry, this should never happen.\n+      btree_node_unlock_exclusive (iter);\n+      return false;\n+    }\n+  for (unsigned index = iter->entry_count; index > slot; --index)\n+    iter->content.entries[index] = iter->content.entries[index - 1];\n+  struct leaf_entry *e = &(iter->content.entries[slot]);\n+  e->base = base;\n+  e->size = size;\n+  e->ob = ob;\n+  iter->entry_count++;\n+  btree_node_unlock_exclusive (iter);\n+  return true;\n+}\n+\n+// Remove an entry.\n+static struct object *\n+btree_remove (struct btree *t, uintptr_t base)\n+{\n+  // Access the root.\n+  version_lock_lock_exclusive (&(t->root_lock));\n+  struct btree_node *iter = t->root;\n+  if (iter)\n+    btree_node_lock_exclusive (iter);\n+  version_lock_unlock_exclusive (&(t->root_lock));\n+  if (!iter)\n+    return NULL;\n+\n+  // Same strategy as with insert, walk down with lock coupling and\n+  // merge eagerly.\n+  while (btree_node_is_inner (iter))\n+    {\n+      unsigned slot = btree_node_find_inner_slot (iter, base);\n+      struct btree_node *next = iter->content.children[slot].child;\n+      btree_node_lock_exclusive (next);\n+      if (btree_node_needs_merge (next))\n+\t{\n+\t  // Use eager merges to avoid lock coupling up.\n+\t  iter = btree_merge_node (t, slot, iter, base);\n+\t}\n+      else\n+\t{\n+\t  btree_node_unlock_exclusive (iter);\n+\t  iter = next;\n+\t}\n+    }\n+\n+  // Remove existing entry.\n+  unsigned slot = btree_node_find_leaf_slot (iter, base);\n+  if ((slot >= iter->entry_count) || (iter->content.entries[slot].base != base))\n+    {\n+      // Not found, this should never happen.\n+      btree_node_unlock_exclusive (iter);\n+      return NULL;\n+    }\n+  struct object *ob = iter->content.entries[slot].ob;\n+  for (unsigned index = slot; index + 1 < iter->entry_count; ++index)\n+    iter->content.entries[index] = iter->content.entries[index + 1];\n+  iter->entry_count--;\n+  btree_node_unlock_exclusive (iter);\n+  return ob;\n+}\n+\n+// Find the corresponding entry for the given address.\n+static struct object *\n+btree_lookup (const struct btree *t, uintptr_t target_addr)\n+{\n+  // Within this function many loads are relaxed atomic loads.\n+  // Use a macro to keep the code reasonable.\n+#define RLOAD(x) __atomic_load_n (&(x), __ATOMIC_RELAXED)\n+\n+  // For targets where unwind info is usually not registered through these\n+  // APIs anymore, avoid any sequential consistent atomics.\n+  // Use relaxed MO here, it is up to the app to ensure that the library\n+  // loading/initialization happens-before using that library in other\n+  // threads (in particular unwinding with that library's functions\n+  // appearing in the backtraces).  Calling that library's functions\n+  // without waiting for the library to initialize would be racy.\n+  if (__builtin_expect (!RLOAD (t->root), 1))\n+    return NULL;\n+\n+  // The unwinding tables are mostly static, they only change when\n+  // frames are added or removed. This makes it extremely unlikely that they\n+  // change during a given unwinding sequence. Thus, we optimize for the\n+  // contention free case and use optimistic lock coupling. This does not\n+  // require any writes to shared state, instead we validate every read. It is\n+  // important that we do not trust any value that we have read until we call\n+  // validate again. Data can change at arbitrary points in time, thus we always\n+  // copy something into a local variable and validate again before acting on\n+  // the read. In the unlikely event that we encounter a concurrent change we\n+  // simply restart and try again.\n+\n+restart:\n+  struct btree_node *iter;\n+  uintptr_t lock;\n+  {\n+    // Accessing the root node requires defending against concurrent pointer\n+    // changes Thus we couple rootLock -> lock on root node -> validate rootLock\n+    if (!version_lock_lock_optimistic (&(t->root_lock), &lock))\n+      goto restart;\n+    iter = RLOAD (t->root);\n+    if (!version_lock_validate (&(t->root_lock), lock))\n+      goto restart;\n+    if (!iter)\n+      return NULL;\n+    uintptr_t child_lock;\n+    if ((!btree_node_lock_optimistic (iter, &child_lock))\n+\t|| (!version_lock_validate (&(t->root_lock), lock)))\n+      goto restart;\n+    lock = child_lock;\n+  }\n+\n+  // Now we can walk down towards the right leaf node.\n+  while (true)\n+    {\n+      enum node_type type = RLOAD (iter->type);\n+      unsigned entry_count = RLOAD (iter->entry_count);\n+      if (!btree_node_validate (iter, lock))\n+\tgoto restart;\n+      if (!entry_count)\n+\treturn NULL;\n+\n+      if (type == btree_node_inner)\n+\t{\n+\t  // We cannot call find_inner_slot here because we need (relaxed)\n+\t  // atomic reads here.\n+\t  unsigned slot = 0;\n+\t  while (\n+\t    ((slot + 1) < entry_count)\n+\t    && (RLOAD (iter->content.children[slot].separator) < target_addr))\n+\t    ++slot;\n+\t  struct btree_node *child = RLOAD (iter->content.children[slot].child);\n+\t  if (!btree_node_validate (iter, lock))\n+\t    goto restart;\n+\n+\t  // The node content can change at any point in time, thus we must\n+\t  // interleave parent and child checks.\n+\t  uintptr_t child_lock;\n+\t  if (!btree_node_lock_optimistic (child, &child_lock))\n+\t    goto restart;\n+\t  if (!btree_node_validate (iter, lock))\n+\t    goto restart; // make sure we still point to the correct node after\n+\t\t\t  // acquiring the optimistic lock.\n+\n+\t  // Go down\n+\t  iter = child;\n+\t  lock = child_lock;\n+\t}\n+      else\n+\t{\n+\t  // We cannot call find_leaf_slot here because we need (relaxed)\n+\t  // atomic reads here.\n+\t  unsigned slot = 0;\n+\t  while (((slot + 1) < entry_count)\n+\t\t && (RLOAD (iter->content.entries[slot].base)\n+\t\t       + RLOAD (iter->content.entries[slot].size)\n+\t\t     <= target_addr))\n+\t    ++slot;\n+\t  struct leaf_entry entry;\n+\t  entry.base = RLOAD (iter->content.entries[slot].base);\n+\t  entry.size = RLOAD (iter->content.entries[slot].size);\n+\t  entry.ob = RLOAD (iter->content.entries[slot].ob);\n+\t  if (!btree_node_validate (iter, lock))\n+\t    goto restart;\n+\n+\t  // Check if we have a hit.\n+\t  if ((entry.base <= target_addr)\n+\t      && (target_addr < entry.base + entry.size))\n+\t    {\n+\t      return entry.ob;\n+\t    }\n+\t  return NULL;\n+\t}\n+    }\n+#undef RLOAD\n+}\n+\n+#endif /* unwind-dw2-btree.h */"}, {"sha": "000ee69b18a00b0cd3d9af9ef44b1d994fda058d", "filename": "libgcc/unwind-dw2-fde.c", "status": "modified", "additions": 144, "deletions": 50, "changes": 194, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6e80a1d164d1f996ad08a512c000025a7c2ca893/libgcc%2Funwind-dw2-fde.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6e80a1d164d1f996ad08a512c000025a7c2ca893/libgcc%2Funwind-dw2-fde.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgcc%2Funwind-dw2-fde.c?ref=6e80a1d164d1f996ad08a512c000025a7c2ca893", "patch": "@@ -42,15 +42,34 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n #endif\n #endif\n \n+#ifdef ATOMIC_FDE_FAST_PATH\n+#include \"unwind-dw2-btree.h\"\n+\n+static struct btree registered_frames;\n+\n+static void\n+release_registered_frames (void) __attribute__ ((destructor (110)));\n+static void\n+release_registered_frames (void)\n+{\n+  /* Release the b-tree and all frames. Frame releases that happen later are\n+   * silently ignored */\n+  btree_destroy (&registered_frames);\n+}\n+\n+static void\n+get_pc_range (const struct object *ob, uintptr_t *range);\n+static void\n+init_object (struct object *ob);\n+\n+#else\n+\n /* The unseen_objects list contains objects that have been registered\n    but not yet categorized in any way.  The seen_objects list has had\n    its pc_begin and count fields initialized at minimum, and is sorted\n    by decreasing value of pc_begin.  */\n static struct object *unseen_objects;\n static struct object *seen_objects;\n-#ifdef ATOMIC_FDE_FAST_PATH\n-static int any_objects_registered;\n-#endif\n \n #ifdef __GTHREAD_MUTEX_INIT\n static __gthread_mutex_t object_mutex = __GTHREAD_MUTEX_INIT;\n@@ -78,6 +97,7 @@ init_object_mutex_once (void)\n static __gthread_mutex_t object_mutex;\n #endif\n #endif\n+#endif\n \n /* Called from crtbegin.o to register the unwind info for an object.  */\n \n@@ -99,23 +119,23 @@ __register_frame_info_bases (const void *begin, struct object *ob,\n   ob->fde_end = NULL;\n #endif\n \n+#ifdef ATOMIC_FDE_FAST_PATH\n+  // Initialize eagerly to avoid locking later\n+  init_object (ob);\n+\n+  // And register the frame\n+  uintptr_t range[2];\n+  get_pc_range (ob, range);\n+  btree_insert (&registered_frames, range[0], range[1] - range[0], ob);\n+#else\n   init_object_mutex_once ();\n   __gthread_mutex_lock (&object_mutex);\n \n   ob->next = unseen_objects;\n   unseen_objects = ob;\n-#ifdef ATOMIC_FDE_FAST_PATH\n-  /* Set flag that at least one library has registered FDEs.\n-     Use relaxed MO here, it is up to the app to ensure that the library\n-     loading/initialization happens-before using that library in other\n-     threads (in particular unwinding with that library's functions\n-     appearing in the backtraces).  Calling that library's functions\n-     without waiting for the library to initialize would be racy.  */\n-  if (!any_objects_registered)\n-    __atomic_store_n (&any_objects_registered, 1, __ATOMIC_RELAXED);\n-#endif\n \n   __gthread_mutex_unlock (&object_mutex);\n+#endif\n }\n \n void\n@@ -153,23 +173,23 @@ __register_frame_info_table_bases (void *begin, struct object *ob,\n   ob->s.b.from_array = 1;\n   ob->s.b.encoding = DW_EH_PE_omit;\n \n+#ifdef ATOMIC_FDE_FAST_PATH\n+  // Initialize eagerly to avoid locking later\n+  init_object (ob);\n+\n+  // And register the frame\n+  uintptr_t range[2];\n+  get_pc_range (ob, range);\n+  btree_insert (&registered_frames, range[0], range[1] - range[0], ob);\n+#else\n   init_object_mutex_once ();\n   __gthread_mutex_lock (&object_mutex);\n \n   ob->next = unseen_objects;\n   unseen_objects = ob;\n-#ifdef ATOMIC_FDE_FAST_PATH\n-  /* Set flag that at least one library has registered FDEs.\n-     Use relaxed MO here, it is up to the app to ensure that the library\n-     loading/initialization happens-before using that library in other\n-     threads (in particular unwinding with that library's functions\n-     appearing in the backtraces).  Calling that library's functions\n-     without waiting for the library to initialize would be racy.  */\n-  if (!any_objects_registered)\n-    __atomic_store_n (&any_objects_registered, 1, __ATOMIC_RELAXED);\n-#endif\n \n   __gthread_mutex_unlock (&object_mutex);\n+#endif\n }\n \n void\n@@ -200,16 +220,33 @@ __register_frame_table (void *begin)\n void *\n __deregister_frame_info_bases (const void *begin)\n {\n-  struct object **p;\n   struct object *ob = 0;\n \n   /* If .eh_frame is empty, we haven't registered.  */\n   if ((const uword *) begin == 0 || *(const uword *) begin == 0)\n     return ob;\n \n+#ifdef ATOMIC_FDE_FAST_PATH\n+  // Find the corresponding PC range\n+  struct object lookupob;\n+  lookupob.tbase = 0;\n+  lookupob.dbase = 0;\n+  lookupob.u.single = begin;\n+  lookupob.s.i = 0;\n+  lookupob.s.b.encoding = DW_EH_PE_omit;\n+#ifdef DWARF2_OBJECT_END_PTR_EXTENSION\n+  lookupob.fde_end = NULL;\n+#endif\n+  uintptr_t range[2];\n+  get_pc_range (&lookupob, range);\n+\n+  // And remove\n+  ob = btree_remove (&registered_frames, range[0]);\n+#else\n   init_object_mutex_once ();\n   __gthread_mutex_lock (&object_mutex);\n \n+  struct object **p;\n   for (p = &unseen_objects; *p ; p = &(*p)->next)\n     if ((*p)->u.single == begin)\n       {\n@@ -241,6 +278,8 @@ __deregister_frame_info_bases (const void *begin)\n \n  out:\n   __gthread_mutex_unlock (&object_mutex);\n+#endif\n+\n   gcc_assert (ob);\n   return (void *) ob;\n }\n@@ -264,7 +303,7 @@ __deregister_frame (void *begin)\n    instead of an _Unwind_Context.  */\n \n static _Unwind_Ptr\n-base_from_object (unsigned char encoding, struct object *ob)\n+base_from_object (unsigned char encoding, const struct object *ob)\n {\n   if (encoding == DW_EH_PE_omit)\n     return 0;\n@@ -628,13 +667,17 @@ end_fde_sort (struct object *ob, struct fde_accumulator *accu, size_t count)\n     }\n }\n \n-\f\n-/* Update encoding, mixed_encoding, and pc_begin for OB for the\n-   fde array beginning at THIS_FDE.  Return the number of fdes\n-   encountered along the way.  */\n+/* Inspect the fde array beginning at this_fde. This\n+   function can be used either in query mode (RANGE is\n+   not null, OB is const), or in update mode (RANGE is\n+   null, OB is modified). In query mode the function computes\n+   the range of PC values and stores it in RANGE. In\n+   update mode it updates encoding, mixed_encoding, and pc_begin\n+   for OB. Return the number of fdes encountered along the way. */\n \n static size_t\n-classify_object_over_fdes (struct object *ob, const fde *this_fde)\n+classify_object_over_fdes (struct object *ob, const fde *this_fde,\n+\t\t\t   uintptr_t *range)\n {\n   const struct dwarf_cie *last_cie = 0;\n   size_t count = 0;\n@@ -660,14 +703,18 @@ classify_object_over_fdes (struct object *ob, const fde *this_fde)\n \t  if (encoding == DW_EH_PE_omit)\n \t    return -1;\n \t  base = base_from_object (encoding, ob);\n-\t  if (ob->s.b.encoding == DW_EH_PE_omit)\n-\t    ob->s.b.encoding = encoding;\n-\t  else if (ob->s.b.encoding != encoding)\n-\t    ob->s.b.mixed_encoding = 1;\n+\t  if (!range)\n+\t    {\n+\t      if (ob->s.b.encoding == DW_EH_PE_omit)\n+\t\tob->s.b.encoding = encoding;\n+\t      else if (ob->s.b.encoding != encoding)\n+\t\tob->s.b.mixed_encoding = 1;\n+\t    }\n \t}\n \n-      read_encoded_value_with_base (encoding, base, this_fde->pc_begin,\n-\t\t\t\t    &pc_begin);\n+      const unsigned char *p;\n+      p = read_encoded_value_with_base (encoding, base, this_fde->pc_begin,\n+\t\t\t\t\t&pc_begin);\n \n       /* Take care to ignore link-once functions that were removed.\n \t In these cases, the function address will be NULL, but if\n@@ -683,8 +730,29 @@ classify_object_over_fdes (struct object *ob, const fde *this_fde)\n \tcontinue;\n \n       count += 1;\n-      if ((void *) pc_begin < ob->pc_begin)\n-\tob->pc_begin = (void *) pc_begin;\n+      if (range)\n+\t{\n+\t  _Unwind_Ptr pc_range, pc_end;\n+\t  read_encoded_value_with_base (encoding & 0x0F, 0, p, &pc_range);\n+\t  pc_end = pc_begin + pc_range;\n+\t  if ((!range[0]) && (!range[1]))\n+\t    {\n+\t      range[0] = pc_begin;\n+\t      range[1] = pc_end;\n+\t    }\n+\t  else\n+\t    {\n+\t      if (pc_begin < range[0])\n+\t\trange[0] = pc_begin;\n+\t      if (pc_end > range[1])\n+\t\trange[1] = pc_end;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  if ((void *) pc_begin < ob->pc_begin)\n+\t    ob->pc_begin = (void *) pc_begin;\n+\t}\n     }\n \n   return count;\n@@ -769,15 +837,15 @@ init_object (struct object* ob)\n \t  fde **p = ob->u.array;\n \t  for (count = 0; *p; ++p)\n \t    {\n-\t      size_t cur_count = classify_object_over_fdes (ob, *p);\n+\t      size_t cur_count = classify_object_over_fdes (ob, *p, NULL);\n \t      if (cur_count == (size_t) -1)\n \t\tgoto unhandled_fdes;\n \t      count += cur_count;\n \t    }\n \t}\n       else\n \t{\n-\t  count = classify_object_over_fdes (ob, ob->u.single);\n+\t  count = classify_object_over_fdes (ob, ob->u.single, NULL);\n \t  if (count == (size_t) -1)\n \t    {\n \t      static const fde terminator;\n@@ -821,6 +889,32 @@ init_object (struct object* ob)\n   ob->s.b.sorted = 1;\n }\n \n+#ifdef ATOMIC_FDE_FAST_PATH\n+/* Get the PC range for lookup */\n+static void\n+get_pc_range (const struct object *ob, uintptr_t *range)\n+{\n+  // It is safe to cast to non-const object* here as\n+  // classify_object_over_fdes does not modify ob in query mode.\n+  struct object *ncob = (struct object *) (uintptr_t) ob;\n+  range[0] = range[1] = 0;\n+  if (ob->s.b.sorted)\n+    {\n+      classify_object_over_fdes (ncob, ob->u.sort->orig_data, range);\n+    }\n+  else if (ob->s.b.from_array)\n+    {\n+      fde **p = ob->u.array;\n+      for (; *p; ++p)\n+\tclassify_object_over_fdes (ncob, *p, range);\n+    }\n+  else\n+    {\n+      classify_object_over_fdes (ncob, ob->u.single, range);\n+    }\n+}\n+#endif\n+\n /* A linear search through a set of FDEs for the given PC.  This is\n    used when there was insufficient memory to allocate and sort an\n    array.  */\n@@ -985,6 +1079,9 @@ binary_search_mixed_encoding_fdes (struct object *ob, void *pc)\n static const fde *\n search_object (struct object* ob, void *pc)\n {\n+  /* The fast path initializes objects eagerly to avoid locking.\n+   * On the slow path we initialize them now */\n+#ifndef ATOMIC_FDE_FAST_PATH\n   /* If the data hasn't been sorted, try to do this now.  We may have\n      more memory available than last time we tried.  */\n   if (! ob->s.b.sorted)\n@@ -997,6 +1094,7 @@ search_object (struct object* ob, void *pc)\n       if (pc < ob->pc_begin)\n \treturn NULL;\n     }\n+#endif\n \n   if (ob->s.b.sorted)\n     {\n@@ -1033,17 +1131,12 @@ _Unwind_Find_FDE (void *pc, struct dwarf_eh_bases *bases)\n   const fde *f = NULL;\n \n #ifdef ATOMIC_FDE_FAST_PATH\n-  /* For targets where unwind info is usually not registered through these\n-     APIs anymore, avoid taking a global lock.\n-     Use relaxed MO here, it is up to the app to ensure that the library\n-     loading/initialization happens-before using that library in other\n-     threads (in particular unwinding with that library's functions\n-     appearing in the backtraces).  Calling that library's functions\n-     without waiting for the library to initialize would be racy.  */\n-  if (__builtin_expect (!__atomic_load_n (&any_objects_registered,\n-\t\t\t\t\t  __ATOMIC_RELAXED), 1))\n+  ob = btree_lookup (&registered_frames, (uintptr_t) pc);\n+  if (!ob)\n     return NULL;\n-#endif\n+\n+  f = search_object (ob, pc);\n+#else\n \n   init_object_mutex_once ();\n   __gthread_mutex_lock (&object_mutex);\n@@ -1081,6 +1174,7 @@ _Unwind_Find_FDE (void *pc, struct dwarf_eh_bases *bases)\n \n  fini:\n   __gthread_mutex_unlock (&object_mutex);\n+#endif\n \n   if (f)\n     {"}, {"sha": "77c2caa4f5a18e24728b4031773a014fd310f02b", "filename": "libgcc/unwind-dw2-fde.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6e80a1d164d1f996ad08a512c000025a7c2ca893/libgcc%2Funwind-dw2-fde.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6e80a1d164d1f996ad08a512c000025a7c2ca893/libgcc%2Funwind-dw2-fde.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgcc%2Funwind-dw2-fde.h?ref=6e80a1d164d1f996ad08a512c000025a7c2ca893", "patch": "@@ -166,7 +166,7 @@ next_fde (const fde *f)\n extern const fde * _Unwind_Find_FDE (void *, struct dwarf_eh_bases *);\n \n static inline int\n-last_fde (struct object *obj __attribute__ ((__unused__)), const fde *f)\n+last_fde (const struct object *obj __attribute__ ((__unused__)), const fde *f)\n {\n #ifdef DWARF2_OBJECT_END_PTR_EXTENSION\n   return f == (const fde *) obj->fde_end || f->length == 0;"}]}