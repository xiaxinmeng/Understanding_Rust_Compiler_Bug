{"sha": "d3c2fee09607e7d70cc7e69822638fab2bda6c7b", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZDNjMmZlZTA5NjA3ZTdkNzBjYzdlNjk4MjI2MzhmYWIyYmRhNmM3Yg==", "commit": {"author": {"name": "Alexander Ivchenko", "email": "alexander.ivchenko@intel.com", "date": "2013-12-31T11:09:42Z"}, "committer": {"name": "Kirill Yukhin", "email": "kyukhin@gcc.gnu.org", "date": "2013-12-31T11:09:42Z"}, "message": "i386.c (MAX_CLASSES): Increase number of classes.\n\ngcc/\n\t* config/i386/i386.c (MAX_CLASSES): Increase number of classes.\n\t(classify_argument): Extend for 512 bit vectors.\n\t(construct_container): Ditto.\n\t(function_arg_advance_32): Ditto.\n\t(function_arg_advance_64): Ditto.\n\t(function_arg_32): Ditto.\n\t(function_arg_64): Ditto.\n\t(function_value_32): Ditto.\n\t(return_in_memory_32): Ditto.\n\t(ix86_gimplify_va_arg): Ditto.\n\t(standard_sse_constant_p): Ditto.\n\t(standard_sse_constant_opcode): Ditto.\n\t(ix86_expand_vector_convert_uns_vsivsf): Ditto.\n\t(ix86_build_const_vector): Ditto.\n\t(ix86_build_signbit_mask): Ditto.\n\t(ix86_expand_sse_cmp): Extend for AVX512.\n\t(ix86_expand_sse_movcc): Ditto.\n\t(ix86_expand_int_vcond): Ditto.\n\t(ix86_expand_vec_perm): Ditto.\n\t(ix86_expand_sse_unpack): Ditto.\n\t(ix86_constant_alignment): Ditto.\n\t(ix86_builtin_vectorized_function): Ditto.\n\t(ix86_vectorize_builtin_gather): Ditto.\n\t(avx_vpermilp_parallel): Ditto.\n\t(ix86_rtx_costs): Ditto.\n\t(ix86_expand_vector_init_duplicate): Ditto.\n\t(ix86_expand_vector_init_concat): Ditto.\n\t(ix86_expand_vector_init_general): Ditto.\n\t(ix86_expand_vector_extract): Ditto.\n\t(emit_reduc_half): Ditto.\n\t(ix86_vector_mode_supported_p): Ditto.\n\t(ix86_emit_swdivsf): Ditto.\n\t(ix86_emit_swsqrtsf): Ditto.\n\t(expand_vec_perm_1): Ditto.\n\t(ix86_vectorize_vec_perm_const_ok): Ditto.\n\t(ix86_expand_mul_widen_evenodd): Ditto.\n\t(ix86_expand_sse2_mulvxdi3): Ditto.\n\t(ix86_preferred_simd_mode): Ditto.\n\t(ix86_autovectorize_vector_sizes): Ditto.\n\t(ix86_expand_vec_perm_vpermi2): New.\n\t(ix86_vector_duplicate_value): Ditto.\n\t(IX86_BUILTIN_SQRTPD512, IX86_BUILTIN_EXP2PS, IX86_BUILTIN_SQRTPS_NR512,\n\tIX86_BUILTIN_GATHER3ALTDIV16SF, IX86_BUILTIN_GATHER3ALTDIV16SI,\n\tIX86_BUILTIN_GATHER3ALTSIV8DF, IX86_BUILTIN_GATHER3ALTSIV8DI,\n\tIX86_BUILTIN_GATHER3DIV16SF, IX86_BUILTIN_GATHER3DIV16SI,\n\tIX86_BUILTIN_GATHER3DIV8DF, IX86_BUILTIN_GATHER3DIV8DI,\n\tIX86_BUILTIN_GATHER3SIV16SF, IX86_BUILTIN_GATHER3SIV16SI,\n\tIX86_BUILTIN_GATHER3SIV8DF, IX86_BUILTIN_CEILPD_VEC_PACK_SFIX512,\n\tIX86_BUILTIN_CPYSGNPS512, IX86_BUILTIN_CPYSGNPD512,\n\tIX86_BUILTIN_FLOORPD_VEC_PACK_SFIX512,\n\tIX86_BUILTIN_ROUNDPD_AZ_VEC_PACK_SFIX512): Ditto.\n\t* config/i386/sse.md (*mov<mode>_internal): Disable SSE typeless\n\tstores vectors > 128bit (AVX*).\n\t(<sse>_storeu<ssemodesuffix><avxsizesuffix>): Ditto.\n\t(<sse2_avx_avx512f>_storedqu<mode>): Extend for AVX-512, disable\n\tSSE typeless stores vectors > 128bit (AVX*).\n\t(fixuns_trunc<mode><sseintvecmodelower>2): Extend for AVX-512.\n\t(vec_pack_ufix_trunc_<mode>): Ditto.\n\t(vec_unpacku_float_hi_v16si): New.\n\t* tree-vect-stmts.c (vectorizable_load): Support AVX512's gathers.\n\t* tree-vectorizer.h (MAX_VECTORIZATION_FACTOR): Extend for 512 bit\n\tvectors.\n\ntestsuite/\n\t* gcc.target/i386/pr49002-2.c: allow vmovapd generation.\n\n\nCo-Authored-By: Andrey Turetskiy <andrey.turetskiy@intel.com>\nCo-Authored-By: Anna Tikhonova <anna.tikhonova@intel.com>\nCo-Authored-By: Ilya Tocar <ilya.tocar@intel.com>\nCo-Authored-By: Ilya Verbin <ilya.verbin@intel.com>\nCo-Authored-By: Kirill Yukhin <kirill.yukhin@intel.com>\nCo-Authored-By: Maxim Kuznetsov <maxim.kuznetsov@intel.com>\nCo-Authored-By: Michael Zolotukhin <michael.v.zolotukhin@intel.com>\nCo-Authored-By: Sergey Lega <sergey.s.lega@intel.com>\n\nFrom-SVN: r206260", "tree": {"sha": "c68ae3cee65d5606c9b4d515b06f89e789e4d701", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/c68ae3cee65d5606c9b4d515b06f89e789e4d701"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/d3c2fee09607e7d70cc7e69822638fab2bda6c7b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d3c2fee09607e7d70cc7e69822638fab2bda6c7b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/d3c2fee09607e7d70cc7e69822638fab2bda6c7b", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/comments", "author": null, "committer": null, "parents": [{"sha": "41a828454e0f161d970298be6d7c29ac8f11b66b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/41a828454e0f161d970298be6d7c29ac8f11b66b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/41a828454e0f161d970298be6d7c29ac8f11b66b"}], "stats": {"total": 913, "additions": 802, "deletions": 111}, "files": [{"sha": "1bb86a88fc9caa61dd0ea9696be12bc7bbbbb5d6", "filename": "gcc/ChangeLog", "status": "modified", "additions": 73, "deletions": 0, "changes": 73, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=d3c2fee09607e7d70cc7e69822638fab2bda6c7b", "patch": "@@ -1,3 +1,76 @@\n+2013-12-31  Alexander Ivchenko  <alexander.ivchenko@intel.com>\n+\t    Maxim Kuznetsov  <maxim.kuznetsov@intel.com>\n+\t    Sergey Lega  <sergey.s.lega@intel.com>\n+\t    Anna Tikhonova  <anna.tikhonova@intel.com>\n+\t    Ilya Tocar  <ilya.tocar@intel.com>\n+\t    Andrey Turetskiy  <andrey.turetskiy@intel.com>\n+\t    Ilya Verbin  <ilya.verbin@intel.com>\n+\t    Kirill Yukhin  <kirill.yukhin@intel.com>\n+\t    Michael Zolotukhin  <michael.v.zolotukhin@intel.com>\n+\n+\t* config/i386/i386.c (MAX_CLASSES): Increase number of classes.\n+\t(classify_argument): Extend for 512 bit vectors.\n+\t(construct_container): Ditto.\n+\t(function_arg_advance_32): Ditto.\n+\t(function_arg_advance_64): Ditto.\n+\t(function_arg_32): Ditto.\n+\t(function_arg_64): Ditto.\n+\t(function_value_32): Ditto.\n+\t(return_in_memory_32): Ditto.\n+\t(ix86_gimplify_va_arg): Ditto.\n+\t(standard_sse_constant_p): Ditto.\n+\t(standard_sse_constant_opcode): Ditto.\n+\t(ix86_expand_vector_convert_uns_vsivsf): Ditto.\n+\t(ix86_build_const_vector): Ditto.\n+\t(ix86_build_signbit_mask): Ditto.\n+\t(ix86_expand_sse_cmp): Extend for AVX512.\n+\t(ix86_expand_sse_movcc): Ditto.\n+\t(ix86_expand_int_vcond): Ditto.\n+\t(ix86_expand_vec_perm): Ditto.\n+\t(ix86_expand_sse_unpack): Ditto.\n+\t(ix86_constant_alignment): Ditto.\n+\t(ix86_builtin_vectorized_function): Ditto.\n+\t(ix86_vectorize_builtin_gather): Ditto.\n+\t(avx_vpermilp_parallel): Ditto.\n+\t(ix86_rtx_costs): Ditto.\n+\t(ix86_expand_vector_init_duplicate): Ditto.\n+\t(ix86_expand_vector_init_concat): Ditto.\n+\t(ix86_expand_vector_init_general): Ditto.\n+\t(ix86_expand_vector_extract): Ditto.\n+\t(emit_reduc_half): Ditto.\n+\t(ix86_vector_mode_supported_p): Ditto.\n+\t(ix86_emit_swdivsf): Ditto.\n+\t(ix86_emit_swsqrtsf): Ditto.\n+\t(expand_vec_perm_1): Ditto.\n+\t(ix86_vectorize_vec_perm_const_ok): Ditto.\n+\t(ix86_expand_mul_widen_evenodd): Ditto.\n+\t(ix86_expand_sse2_mulvxdi3): Ditto.\n+\t(ix86_preferred_simd_mode): Ditto.\n+\t(ix86_autovectorize_vector_sizes): Ditto.\n+\t(ix86_expand_vec_perm_vpermi2): New.\n+\t(ix86_vector_duplicate_value): Ditto.\n+\t(IX86_BUILTIN_SQRTPD512, IX86_BUILTIN_EXP2PS, IX86_BUILTIN_SQRTPS_NR512,\n+\tIX86_BUILTIN_GATHER3ALTDIV16SF, IX86_BUILTIN_GATHER3ALTDIV16SI,\n+\tIX86_BUILTIN_GATHER3ALTSIV8DF, IX86_BUILTIN_GATHER3ALTSIV8DI,\n+\tIX86_BUILTIN_GATHER3DIV16SF, IX86_BUILTIN_GATHER3DIV16SI,\n+\tIX86_BUILTIN_GATHER3DIV8DF, IX86_BUILTIN_GATHER3DIV8DI,\n+\tIX86_BUILTIN_GATHER3SIV16SF, IX86_BUILTIN_GATHER3SIV16SI,\n+\tIX86_BUILTIN_GATHER3SIV8DF, IX86_BUILTIN_CEILPD_VEC_PACK_SFIX512,\n+\tIX86_BUILTIN_CPYSGNPS512, IX86_BUILTIN_CPYSGNPD512,\n+\tIX86_BUILTIN_FLOORPD_VEC_PACK_SFIX512,\n+\tIX86_BUILTIN_ROUNDPD_AZ_VEC_PACK_SFIX512): Ditto.\n+\t* config/i386/sse.md (*mov<mode>_internal): Disable SSE typeless\n+\tstores vectors > 128bit (AVX*).\n+\t(<sse>_storeu<ssemodesuffix><avxsizesuffix>): Ditto.\n+\t(<sse2_avx_avx512f>_storedqu<mode>): Extend for AVX-512, disable\n+\tSSE typeless stores vectors > 128bit (AVX*).\n+\t(fixuns_trunc<mode><sseintvecmodelower>2): Extend for AVX-512.\n+\t(vec_pack_ufix_trunc_<mode>): Ditto.\n+\t(vec_unpacku_float_hi_v16si): New.\n+\t* tree-vect-stmts.c (vectorizable_load): Support AVX512's gathers.\n+\t* tree-vectorizer.h (MAX_VECTORIZATION_FACTOR): Extend for 512 bit\n+\tvectors.\n+\n 2013-12-31  Chung-Lin Tang  <cltang@codesourcery.com>\n \t    Sandra Loosemore  <sandra@codesourcery.com>\n \t    Based on patches from Altera Corporation"}, {"sha": "dd48cc51656659530b3504d1b0ec99e4ea2b1986", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 602, "deletions": 71, "changes": 673, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=d3c2fee09607e7d70cc7e69822638fab2bda6c7b", "patch": "@@ -2308,7 +2308,7 @@ enum x86_64_reg_class\n     X86_64_MEMORY_CLASS\n   };\n \n-#define MAX_CLASSES 4\n+#define MAX_CLASSES 8\n \n /* Table of constants used by fldpi, fldln2, etc....  */\n static REAL_VALUE_TYPE ext_80387_constants_table [5];\n@@ -6242,7 +6242,7 @@ merge_classes (enum x86_64_reg_class class1, enum x86_64_reg_class class2)\n    sized containers, classes[0] will be NO_CLASS and 1 is returned.\n \n    BIT_OFFSET is used internally for handling records and specifies offset\n-   of the offset in bits modulo 256 to avoid overflow cases.\n+   of the offset in bits modulo 512 to avoid overflow cases.\n \n    See the x86-64 PS ABI for details.\n */\n@@ -6342,7 +6342,7 @@ classify_argument (enum machine_mode mode, const_tree type,\n \t\t      num = classify_argument (TYPE_MODE (type), type,\n \t\t\t\t\t       subclasses,\n \t\t\t\t\t       (int_bit_position (field)\n-\t\t\t\t\t\t+ bit_offset) % 256);\n+\t\t\t\t\t\t+ bit_offset) % 512);\n \t\t      if (!num)\n \t\t\treturn 0;\n \t\t      pos = (int_bit_position (field)\n@@ -6592,6 +6592,21 @@ classify_argument (enum machine_mode mode, const_tree type,\n       classes[2] = X86_64_SSEUP_CLASS;\n       classes[3] = X86_64_SSEUP_CLASS;\n       return 4;\n+    case V8DFmode:\n+    case V16SFmode:\n+    case V8DImode:\n+    case V16SImode:\n+    case V32HImode:\n+    case V64QImode:\n+      classes[0] = X86_64_SSE_CLASS;\n+      classes[1] = X86_64_SSEUP_CLASS;\n+      classes[2] = X86_64_SSEUP_CLASS;\n+      classes[3] = X86_64_SSEUP_CLASS;\n+      classes[4] = X86_64_SSEUP_CLASS;\n+      classes[5] = X86_64_SSEUP_CLASS;\n+      classes[6] = X86_64_SSEUP_CLASS;\n+      classes[7] = X86_64_SSEUP_CLASS;\n+      return 8;\n     case V4SFmode:\n     case V4SImode:\n     case V16QImode:\n@@ -6777,6 +6792,18 @@ construct_container (enum machine_mode mode, enum machine_mode orig_mode,\n       && mode != BLKmode)\n     return gen_reg_or_parallel (mode, orig_mode,\n \t\t\t\tSSE_REGNO (sse_regno));\n+  if (n == 8\n+      && regclass[0] == X86_64_SSE_CLASS\n+      && regclass[1] == X86_64_SSEUP_CLASS\n+      && regclass[2] == X86_64_SSEUP_CLASS\n+      && regclass[3] == X86_64_SSEUP_CLASS\n+      && regclass[4] == X86_64_SSEUP_CLASS\n+      && regclass[5] == X86_64_SSEUP_CLASS\n+      && regclass[6] == X86_64_SSEUP_CLASS\n+      && regclass[7] == X86_64_SSEUP_CLASS\n+      && mode != BLKmode)\n+    return gen_reg_or_parallel (mode, orig_mode,\n+\t\t\t\tSSE_REGNO (sse_regno));\n   if (n == 2\n       && regclass[0] == X86_64_X87_CLASS\n       && regclass[1] == X86_64_X87UP_CLASS)\n@@ -6858,6 +6885,18 @@ construct_container (enum machine_mode mode, enum machine_mode orig_mode,\n \t\ttmpmode = OImode;\n \t\ti += 3;\n \t\tbreak;\n+\t      case 8:\n+\t\tgcc_assert (i == 0\n+\t\t\t    && regclass[1] == X86_64_SSEUP_CLASS\n+\t\t\t    && regclass[2] == X86_64_SSEUP_CLASS\n+\t\t\t    && regclass[3] == X86_64_SSEUP_CLASS\n+\t\t\t    && regclass[4] == X86_64_SSEUP_CLASS\n+\t\t\t    && regclass[5] == X86_64_SSEUP_CLASS\n+\t\t\t    && regclass[6] == X86_64_SSEUP_CLASS\n+\t\t\t    && regclass[7] == X86_64_SSEUP_CLASS);\n+\t\ttmpmode = XImode;\n+\t\ti += 7;\n+\t\tbreak;\n \t      default:\n \t\tgcc_unreachable ();\n \t      }\n@@ -6931,6 +6970,12 @@ function_arg_advance_32 (CUMULATIVE_ARGS *cum, enum machine_mode mode,\n \n     case V8SFmode:\n     case V8SImode:\n+    case V64QImode:\n+    case V32HImode:\n+    case V16SImode:\n+    case V8DImode:\n+    case V16SFmode:\n+    case V8DFmode:\n     case V32QImode:\n     case V16HImode:\n     case V4DFmode:\n@@ -6982,8 +7027,9 @@ function_arg_advance_64 (CUMULATIVE_ARGS *cum, enum machine_mode mode,\n {\n   int int_nregs, sse_nregs;\n \n-  /* Unnamed 256bit vector mode parameters are passed on stack.  */\n-  if (!named && VALID_AVX256_REG_MODE (mode))\n+  /* Unnamed 512 and 256bit vector mode parameters are passed on stack.  */\n+  if (!named && (VALID_AVX512F_REG_MODE (mode)\n+\t\t || VALID_AVX256_REG_MODE (mode)))\n     return;\n \n   if (examine_argument (mode, type, 0, &int_nregs, &sse_nregs)\n@@ -7134,9 +7180,16 @@ function_arg_32 (const CUMULATIVE_ARGS *cum, enum machine_mode mode,\n       break;\n \n     case OImode:\n-      /* OImode shouldn't be used directly.  */\n+    case XImode:\n+      /* OImode and XImode shouldn't be used directly.  */\n       gcc_unreachable ();\n \n+    case V64QImode:\n+    case V32HImode:\n+    case V16SImode:\n+    case V8DImode:\n+    case V16SFmode:\n+    case V8DFmode:\n     case V8SFmode:\n     case V8SImode:\n     case V32QImode:\n@@ -7199,7 +7252,13 @@ function_arg_64 (const CUMULATIVE_ARGS *cum, enum machine_mode mode,\n     case V16HImode:\n     case V4DFmode:\n     case V4DImode:\n-      /* Unnamed 256bit vector mode parameters are passed on stack.  */\n+    case V16SFmode:\n+    case V16SImode:\n+    case V64QImode:\n+    case V32HImode:\n+    case V8DFmode:\n+    case V8DImode:\n+      /* Unnamed 256 and 512bit vector mode parameters are passed on stack.  */\n       if (!named)\n \treturn NULL;\n       break;\n@@ -7602,6 +7661,10 @@ function_value_32 (enum machine_mode orig_mode, enum machine_mode mode,\n   else if (VECTOR_MODE_P (mode) && GET_MODE_SIZE (mode) == 32)\n     regno = FIRST_SSE_REG;\n \n+  /* 64-byte vector modes in %zmm0.   */\n+  else if (VECTOR_MODE_P (mode) && GET_MODE_SIZE (mode) == 64)\n+    regno = FIRST_SSE_REG;\n+\n   /* Floating point return values in %st(0) (unless -mno-fp-ret-in-387).  */\n   else if (X87_FLOAT_MODE_P (mode) && TARGET_FLOAT_RETURNS_IN_80387)\n     regno = FIRST_FLOAT_REG;\n@@ -7809,6 +7872,10 @@ return_in_memory_32 (const_tree type, enum machine_mode mode)\n       /* AVX values are returned in YMM0, except when it doesn't exist.  */\n       if (size == 32)\n \treturn !TARGET_AVX;\n+\n+      /* AVX512F values are returned in ZMM0, except when it doesn't exist.  */\n+      if (size == 64)\n+\treturn !TARGET_AVX512F;\n     }\n \n   if (mode == XFmode)\n@@ -8345,7 +8412,13 @@ ix86_gimplify_va_arg (tree valist, tree type, gimple_seq *pre_p,\n     case V16HImode:\n     case V4DFmode:\n     case V4DImode:\n-      /* Unnamed 256bit vector mode parameters are passed on stack.  */\n+    case V16SFmode:\n+    case V16SImode:\n+    case V64QImode:\n+    case V32HImode:\n+    case V8DFmode:\n+    case V8DImode:\n+      /* Unnamed 256 and 512bit vector mode parameters are passed on stack.  */\n       if (!TARGET_64BIT_MS_ABI)\n \t{\n \t  container = NULL;\n@@ -8760,6 +8833,12 @@ standard_sse_constant_p (rtx x)\n       case V4DImode:\n \tif (TARGET_AVX2)\n \t  return 2;\n+      case V64QImode:\n+      case V32HImode:\n+      case V16SImode:\n+      case V8DImode:\n+\tif (TARGET_AVX512F)\n+\t  return 2;\n       default:\n \tbreak;\n       }\n@@ -8778,6 +8857,11 @@ standard_sse_constant_opcode (rtx insn, rtx x)\n     case 1:\n       switch (get_attr_mode (insn))\n \t{\n+\tcase MODE_XI:\n+\tcase MODE_V16SF:\n+\t  return \"vpxord\\t%g0, %g0, %g0\";\n+\tcase MODE_V8DF:\n+\t  return \"vpxorq\\t%g0, %g0, %g0\";\n \tcase MODE_TI:\n \t  return \"%vpxor\\t%0, %d0\";\n \tcase MODE_V2DF:\n@@ -18693,17 +18777,23 @@ ix86_build_const_vector (enum machine_mode mode, bool vect, rtx value)\n \n   switch (mode)\n     {\n+    case V64QImode:\n     case V32QImode:\n     case V16QImode:\n+    case V32HImode:\n     case V16HImode:\n     case V8HImode:\n+    case V16SImode:\n     case V8SImode:\n     case V4SImode:\n+    case V8DImode:\n     case V4DImode:\n     case V2DImode:\n       gcc_assert (vect);\n+    case V16SFmode:\n     case V8SFmode:\n     case V4SFmode:\n+    case V8DFmode:\n     case V4DFmode:\n     case V2DFmode:\n       n_elt = GET_MODE_NUNITS (mode);\n@@ -18740,6 +18830,8 @@ ix86_build_signbit_mask (enum machine_mode mode, bool vect, bool invert)\n   /* Find the sign bit, sign extended to 2*HWI.  */\n   switch (mode)\n     {\n+    case V16SImode:\n+    case V16SFmode:\n     case V8SImode:\n     case V4SImode:\n     case V8SFmode:\n@@ -18750,8 +18842,10 @@ ix86_build_signbit_mask (enum machine_mode mode, bool vect, bool invert)\n       lo = 0x80000000, hi = lo < 0;\n       break;\n \n+    case V8DImode:\n     case V4DImode:\n     case V2DImode:\n+    case V8DFmode:\n     case V4DFmode:\n     case V2DFmode:\n       vec_mode = mode;\n@@ -20608,22 +20702,63 @@ ix86_expand_sse_cmp (rtx dest, enum rtx_code code, rtx cmp_op0, rtx cmp_op1,\n \t\t     rtx op_true, rtx op_false)\n {\n   enum machine_mode mode = GET_MODE (dest);\n-  enum machine_mode cmp_mode = GET_MODE (cmp_op0);\n+  enum machine_mode cmp_ops_mode = GET_MODE (cmp_op0);\n+\n+  /* In general case result of comparison can differ from operands' type.  */\n+  enum machine_mode cmp_mode;\n+\n+  /* In AVX512F the result of comparison is an integer mask.  */\n+  bool maskcmp = false;\n   rtx x;\n \n-  cmp_op0 = force_reg (cmp_mode, cmp_op0);\n-  if (!nonimmediate_operand (cmp_op1, cmp_mode))\n-    cmp_op1 = force_reg (cmp_mode, cmp_op1);\n+  if (GET_MODE_SIZE (cmp_ops_mode) == 64)\n+    {\n+      cmp_mode = mode_for_size (GET_MODE_NUNITS (cmp_ops_mode), MODE_INT, 0);\n+      gcc_assert (cmp_mode != BLKmode);\n+\n+      maskcmp = true;\n+    }\n+  else\n+    cmp_mode = cmp_ops_mode;\n+\n+\n+  cmp_op0 = force_reg (cmp_ops_mode, cmp_op0);\n+  if (!nonimmediate_operand (cmp_op1, cmp_ops_mode))\n+    cmp_op1 = force_reg (cmp_ops_mode, cmp_op1);\n \n   if (optimize\n       || reg_overlap_mentioned_p (dest, op_true)\n       || reg_overlap_mentioned_p (dest, op_false))\n-    dest = gen_reg_rtx (mode);\n+    dest = gen_reg_rtx (maskcmp ? cmp_mode : mode);\n+\n+  /* Compare patterns for int modes are unspec in AVX512F only.  */\n+  if (maskcmp && (code == GT || code == EQ))\n+    {\n+      rtx (*gen)(rtx, rtx, rtx);\n \n+      switch (cmp_ops_mode)\n+\t{\n+\tcase V16SImode:\n+\t  gen = code == GT ? gen_avx512f_gtv16si3 : gen_avx512f_eqv16si3_1;\n+\t  break;\n+\tcase V8DImode:\n+\t  gen = code == GT ? gen_avx512f_gtv8di3 : gen_avx512f_eqv8di3_1;\n+\t  break;\n+\tdefault:\n+\t  gen = NULL;\n+\t}\n+\n+      if (gen)\n+\t{\n+\t  emit_insn (gen (dest, cmp_op0, cmp_op1));\n+\t  return dest;\n+\t}\n+    }\n   x = gen_rtx_fmt_ee (code, cmp_mode, cmp_op0, cmp_op1);\n-  if (cmp_mode != mode)\n+\n+  if (cmp_mode != mode && !maskcmp)\n     {\n-      x = force_reg (cmp_mode, x);\n+      x = force_reg (cmp_ops_mode, x);\n       convert_move (dest, x, false);\n     }\n   else\n@@ -20639,33 +20774,43 @@ static void\n ix86_expand_sse_movcc (rtx dest, rtx cmp, rtx op_true, rtx op_false)\n {\n   enum machine_mode mode = GET_MODE (dest);\n+  enum machine_mode cmpmode = GET_MODE (cmp);\n+\n+  /* In AVX512F the result of comparison is an integer mask.  */\n+  bool maskcmp = (mode != cmpmode && TARGET_AVX512F);\n+\n   rtx t2, t3, x;\n \n   if (vector_all_ones_operand (op_true, mode)\n-      && rtx_equal_p (op_false, CONST0_RTX (mode)))\n+      && rtx_equal_p (op_false, CONST0_RTX (mode))\n+      && !maskcmp)\n     {\n       emit_insn (gen_rtx_SET (VOIDmode, dest, cmp));\n     }\n-  else if (op_false == CONST0_RTX (mode))\n+  else if (op_false == CONST0_RTX (mode)\n+      && !maskcmp)\n     {\n       op_true = force_reg (mode, op_true);\n       x = gen_rtx_AND (mode, cmp, op_true);\n       emit_insn (gen_rtx_SET (VOIDmode, dest, x));\n     }\n-  else if (op_true == CONST0_RTX (mode))\n+  else if (op_true == CONST0_RTX (mode)\n+      && !maskcmp)\n     {\n       op_false = force_reg (mode, op_false);\n       x = gen_rtx_NOT (mode, cmp);\n       x = gen_rtx_AND (mode, x, op_false);\n       emit_insn (gen_rtx_SET (VOIDmode, dest, x));\n     }\n-  else if (INTEGRAL_MODE_P (mode) && op_true == CONSTM1_RTX (mode))\n+  else if (INTEGRAL_MODE_P (mode) && op_true == CONSTM1_RTX (mode)\n+      && !maskcmp)\n     {\n       op_false = force_reg (mode, op_false);\n       x = gen_rtx_IOR (mode, cmp, op_false);\n       emit_insn (gen_rtx_SET (VOIDmode, dest, x));\n     }\n-  else if (TARGET_XOP)\n+  else if (TARGET_XOP\n+      && !maskcmp)\n     {\n       op_true = force_reg (mode, op_true);\n \n@@ -20733,6 +20878,20 @@ ix86_expand_sse_movcc (rtx dest, rtx cmp, rtx op_true, rtx op_false)\n \t      cmp = gen_lowpart (V32QImode, cmp);\n \t    }\n \t  break;\n+\n+\tcase V16SImode:\n+\t  gen = gen_avx512f_blendmv16si;\n+\t  break;\n+\tcase V8DImode:\n+\t  gen = gen_avx512f_blendmv8di;\n+\t  break;\n+\tcase V8DFmode:\n+\t  gen = gen_avx512f_blendmv8df;\n+\t  break;\n+\tcase V16SFmode:\n+\t  gen = gen_avx512f_blendmv16sf;\n+\t  break;\n+\n \tdefault:\n \t  break;\n \t}\n@@ -21000,6 +21159,8 @@ ix86_expand_int_vcond (rtx operands[])\n \n \t  switch (mode)\n \t    {\n+\t    case V16SImode:\n+\t    case V8DImode:\n \t    case V8SImode:\n \t    case V4DImode:\n \t    case V4SImode:\n@@ -21010,6 +21171,8 @@ ix86_expand_int_vcond (rtx operands[])\n \n \t\t  switch (mode)\n \t\t    {\n+\t\t    case V16SImode: gen_sub3 = gen_subv16si3; break;\n+\t\t    case V8DImode: gen_sub3 = gen_subv8di3; break;\n \t\t    case V8SImode: gen_sub3 = gen_subv8si3; break;\n \t\t    case V4DImode: gen_sub3 = gen_subv4di3; break;\n \t\t    case V4SImode: gen_sub3 = gen_subv4si3; break;\n@@ -21065,14 +21228,44 @@ ix86_expand_int_vcond (rtx operands[])\n       gcc_assert (GET_MODE_SIZE (data_mode) == GET_MODE_SIZE (mode));\n       x = ix86_expand_sse_cmp (gen_reg_rtx (mode), code, cop0, cop1,\n \t\t\t       operands[1+negate], operands[2-negate]);\n-      x = gen_lowpart (data_mode, x);\n+      if (GET_MODE (x) == mode)\n+\tx = gen_lowpart (data_mode, x);\n     }\n \n   ix86_expand_sse_movcc (operands[0], x, operands[1+negate],\n \t\t\t operands[2-negate]);\n   return true;\n }\n \n+static bool\n+ix86_expand_vec_perm_vpermi2 (rtx target, rtx op0, rtx mask, rtx op1)\n+{\n+  enum machine_mode mode = GET_MODE (op0);\n+  switch (mode)\n+    {\n+    case V16SImode:\n+      emit_insn (gen_avx512f_vpermi2varv16si3 (target, op0,\n+\t\t\t\t\t      force_reg (V16SImode, mask),\n+\t\t\t\t\t      op1));\n+      return true;\n+    case V16SFmode:\n+      emit_insn (gen_avx512f_vpermi2varv16sf3 (target, op0,\n+\t\t\t\t\t      force_reg (V16SImode, mask),\n+\t\t\t\t\t      op1));\n+      return true;\n+    case V8DImode:\n+      emit_insn (gen_avx512f_vpermi2varv8di3 (target, op0,\n+\t\t\t\t\t     force_reg (V8DImode, mask), op1));\n+      return true;\n+    case V8DFmode:\n+      emit_insn (gen_avx512f_vpermi2varv8df3 (target, op0,\n+\t\t\t\t\t     force_reg (V8DImode, mask), op1));\n+      return true;\n+    default:\n+      return false;\n+    }\n+}\n+\n /* Expand a variable vector permutation.  */\n \n void\n@@ -21091,7 +21284,10 @@ ix86_expand_vec_perm (rtx operands[])\n   /* Number of elements in the vector.  */\n   w = GET_MODE_NUNITS (mode);\n   e = GET_MODE_UNIT_SIZE (mode);\n-  gcc_assert (w <= 32);\n+  gcc_assert (w <= 64);\n+\n+  if (ix86_expand_vec_perm_vpermi2 (target, op0, mask, op1))\n+    return;\n \n   if (TARGET_AVX2)\n     {\n@@ -21471,6 +21667,15 @@ ix86_expand_sse_unpack (rtx dest, rtx src, bool unsigned_p, bool high_p)\n \t  extract\n \t    = high_p ? gen_vec_extract_hi_v32qi : gen_vec_extract_lo_v32qi;\n \t  break;\n+\tcase V32HImode:\n+\t  if (unsigned_p)\n+\t    unpack = gen_avx512f_zero_extendv16hiv16si2;\n+\t  else\n+\t    unpack = gen_avx512f_sign_extendv16hiv16si2;\n+\t  halfmode = V16HImode;\n+\t  extract\n+\t    = high_p ? gen_vec_extract_hi_v32hi : gen_vec_extract_lo_v32hi;\n+\t  break;\n \tcase V16HImode:\n \t  if (unsigned_p)\n \t    unpack = gen_avx2_zero_extendv8hiv8si2;\n@@ -21480,6 +21685,15 @@ ix86_expand_sse_unpack (rtx dest, rtx src, bool unsigned_p, bool high_p)\n \t  extract\n \t    = high_p ? gen_vec_extract_hi_v16hi : gen_vec_extract_lo_v16hi;\n \t  break;\n+\tcase V16SImode:\n+\t  if (unsigned_p)\n+\t    unpack = gen_avx512f_zero_extendv8siv8di2;\n+\t  else\n+\t    unpack = gen_avx512f_sign_extendv8siv8di2;\n+\t  halfmode = V8SImode;\n+\t  extract\n+\t    = high_p ? gen_vec_extract_hi_v16si : gen_vec_extract_lo_v16si;\n+\t  break;\n \tcase V8SImode:\n \t  if (unsigned_p)\n \t    unpack = gen_avx2_zero_extendv4siv4di2;\n@@ -21511,7 +21725,7 @@ ix86_expand_sse_unpack (rtx dest, rtx src, bool unsigned_p, bool high_p)\n \t  gcc_unreachable ();\n \t}\n \n-      if (GET_MODE_SIZE (imode) == 32)\n+      if (GET_MODE_SIZE (imode) >= 32)\n \t{\n \t  tmp = gen_reg_rtx (halfmode);\n \t  emit_insn (extract (tmp, src));\n@@ -26245,7 +26459,8 @@ ix86_constant_alignment (tree exp, int align)\n int\n ix86_data_alignment (tree type, int align, bool opt)\n {\n-  int max_align = optimize_size ? BITS_PER_WORD : MIN (256, MAX_OFILE_ALIGNMENT);\n+  int max_align = optimize_size ? BITS_PER_WORD\n+\t\t\t\t: MIN (512, MAX_OFILE_ALIGNMENT);\n \n   if (opt\n       && AGGREGATE_TYPE_P (type)\n@@ -27707,12 +27922,27 @@ enum ix86_builtins\n   IX86_BUILTIN_GATHERDIV4SI,\n   IX86_BUILTIN_GATHERDIV8SI,\n \n+  IX86_BUILTIN_SQRTPD512,\n+  IX86_BUILTIN_EXP2PS,\n+  IX86_BUILTIN_SQRTPS_NR512,\n+\n   /* Alternate 4 element gather for the vectorizer where\n      all operands are 32-byte wide.  */\n   IX86_BUILTIN_GATHERALTSIV4DF,\n   IX86_BUILTIN_GATHERALTDIV8SF,\n   IX86_BUILTIN_GATHERALTSIV4DI,\n   IX86_BUILTIN_GATHERALTDIV8SI,\n+  IX86_BUILTIN_GATHER3ALTDIV16SF,\n+  IX86_BUILTIN_GATHER3ALTDIV16SI,\n+  IX86_BUILTIN_GATHER3ALTSIV8DF,\n+  IX86_BUILTIN_GATHER3ALTSIV8DI,\n+  IX86_BUILTIN_GATHER3DIV16SF,\n+  IX86_BUILTIN_GATHER3DIV16SI,\n+  IX86_BUILTIN_GATHER3DIV8DF,\n+  IX86_BUILTIN_GATHER3DIV8DI,\n+  IX86_BUILTIN_GATHER3SIV16SF,\n+  IX86_BUILTIN_GATHER3SIV16SI,\n+  IX86_BUILTIN_GATHER3SIV8DF,\n \n   /* TFmode support builtins.  */\n   IX86_BUILTIN_INFQ,\n@@ -27721,10 +27951,16 @@ enum ix86_builtins\n   IX86_BUILTIN_COPYSIGNQ,\n \n   /* Vectorizer support builtins.  */\n+  IX86_BUILTIN_CEILPD_VEC_PACK_SFIX512,\n   IX86_BUILTIN_CPYSGNPS,\n   IX86_BUILTIN_CPYSGNPD,\n   IX86_BUILTIN_CPYSGNPS256,\n+  IX86_BUILTIN_CPYSGNPS512,\n   IX86_BUILTIN_CPYSGNPD256,\n+  IX86_BUILTIN_CPYSGNPD512,\n+  IX86_BUILTIN_FLOORPD_VEC_PACK_SFIX512,\n+  IX86_BUILTIN_ROUNDPD_AZ_VEC_PACK_SFIX512,\n+\n \n   /* FMA4 instructions.  */\n   IX86_BUILTIN_VFMADDSS,\n@@ -33902,6 +34138,16 @@ ix86_builtin_vectorized_function (tree fndecl, tree type_out,\n \t    return ix86_get_builtin (IX86_BUILTIN_SQRTPD);\n \t  else if (out_n == 4 && in_n == 4)\n \t    return ix86_get_builtin (IX86_BUILTIN_SQRTPD256);\n+\t  else if (out_n == 8 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_SQRTPD512);\n+\t}\n+      break;\n+\n+    case BUILT_IN_EXP2F:\n+      if (out_mode == SFmode && in_mode == SFmode)\n+\t{\n+\t  if (out_n == 16 && in_n == 16)\n+\t    return ix86_get_builtin (IX86_BUILTIN_EXP2PS);\n \t}\n       break;\n \n@@ -33912,6 +34158,8 @@ ix86_builtin_vectorized_function (tree fndecl, tree type_out,\n \t    return ix86_get_builtin (IX86_BUILTIN_SQRTPS_NR);\n \t  else if (out_n == 8 && in_n == 8)\n \t    return ix86_get_builtin (IX86_BUILTIN_SQRTPS_NR256);\n+\t  else if (out_n == 16 && in_n == 16)\n+\t    return ix86_get_builtin (IX86_BUILTIN_SQRTPS_NR512);\n \t}\n       break;\n \n@@ -33928,6 +34176,8 @@ ix86_builtin_vectorized_function (tree fndecl, tree type_out,\n \t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD_VEC_PACK_SFIX);\n \t  else if (out_n == 8 && in_n == 4)\n \t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD_VEC_PACK_SFIX256);\n+\t  else if (out_n == 16 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_FLOORPD_VEC_PACK_SFIX512);\n \t}\n       break;\n \n@@ -33960,6 +34210,8 @@ ix86_builtin_vectorized_function (tree fndecl, tree type_out,\n \t    return ix86_get_builtin (IX86_BUILTIN_CEILPD_VEC_PACK_SFIX);\n \t  else if (out_n == 8 && in_n == 4)\n \t    return ix86_get_builtin (IX86_BUILTIN_CEILPD_VEC_PACK_SFIX256);\n+\t  else if (out_n == 16 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CEILPD_VEC_PACK_SFIX512);\n \t}\n       break;\n \n@@ -34016,6 +34268,8 @@ ix86_builtin_vectorized_function (tree fndecl, tree type_out,\n \t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPD_AZ_VEC_PACK_SFIX);\n \t  else if (out_n == 8 && in_n == 4)\n \t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPD_AZ_VEC_PACK_SFIX256);\n+\t  else if (out_n == 16 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_ROUNDPD_AZ_VEC_PACK_SFIX512);\n \t}\n       break;\n \n@@ -34042,6 +34296,8 @@ ix86_builtin_vectorized_function (tree fndecl, tree type_out,\n \t    return ix86_get_builtin (IX86_BUILTIN_CPYSGNPD);\n \t  else if (out_n == 4 && in_n == 4)\n \t    return ix86_get_builtin (IX86_BUILTIN_CPYSGNPD256);\n+\t  else if (out_n == 8 && in_n == 8)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CPYSGNPD512);\n \t}\n       break;\n \n@@ -34052,6 +34308,8 @@ ix86_builtin_vectorized_function (tree fndecl, tree type_out,\n \t    return ix86_get_builtin (IX86_BUILTIN_CPYSGNPS);\n \t  else if (out_n == 8 && in_n == 8)\n \t    return ix86_get_builtin (IX86_BUILTIN_CPYSGNPS256);\n+\t  else if (out_n == 16 && in_n == 16)\n+\t    return ix86_get_builtin (IX86_BUILTIN_CPYSGNPS512);\n \t}\n       break;\n \n@@ -34487,6 +34745,34 @@ ix86_vectorize_builtin_gather (const_tree mem_vectype,\n     case V8SImode:\n       code = si ? IX86_BUILTIN_GATHERSIV8SI : IX86_BUILTIN_GATHERALTDIV8SI;\n       break;\n+#if 0\n+    /*  FIXME: Commented until vectorizer can work with (mask_type != src_type)\n+\tPR59617.   */\n+    case V8DFmode:\n+      if (TARGET_AVX512F)\n+\tcode = si ? IX86_BUILTIN_GATHER3ALTSIV8DF : IX86_BUILTIN_GATHER3DIV8DF;\n+      else\n+\treturn NULL_TREE;\n+      break;\n+    case V8DImode:\n+      if (TARGET_AVX512F)\n+\tcode = si ? IX86_BUILTIN_GATHER3ALTSIV8DI : IX86_BUILTIN_GATHER3DIV8DI;\n+      else\n+\treturn NULL_TREE;\n+      break;\n+    case V16SFmode:\n+      if (TARGET_AVX512F)\n+\tcode = si ? IX86_BUILTIN_GATHER3SIV16SF : IX86_BUILTIN_GATHER3ALTDIV16SF;\n+      else\n+\treturn NULL_TREE;\n+      break;\n+    case V16SImode:\n+      if (TARGET_AVX512F)\n+\tcode = si ? IX86_BUILTIN_GATHER3SIV16SI : IX86_BUILTIN_GATHER3ALTDIV16SI;\n+      else\n+\treturn NULL_TREE;\n+      break;\n+#endif\n     default:\n       return NULL_TREE;\n     }\n@@ -34542,7 +34828,7 @@ avx_vpermilp_parallel (rtx par, enum machine_mode mode)\n {\n   unsigned i, nelt = GET_MODE_NUNITS (mode);\n   unsigned mask = 0;\n-  unsigned char ipar[8] = {};  /* Silence -Wuninitialized warning.  */\n+  unsigned char ipar[16] = {};  /* Silence -Wuninitialized warning.  */\n \n   if (XVECLEN (par, 0) != (int) nelt)\n     return 0;\n@@ -34565,6 +34851,24 @@ avx_vpermilp_parallel (rtx par, enum machine_mode mode)\n \n   switch (mode)\n     {\n+    case V8DFmode:\n+      /* In the 512-bit DFmode case, we can only move elements within\n+         a 128-bit lane.  First fill the second part of the mask,\n+\t then fallthru.  */\n+      for (i = 4; i < 6; ++i)\n+\t{\n+\t  if (ipar[i] < 4 || ipar[i] >= 6)\n+\t    return 0;\n+\t  mask |= (ipar[i] - 4) << i;\n+\t}\n+      for (i = 6; i < 8; ++i)\n+\t{\n+\t  if (ipar[i] < 6)\n+\t    return 0;\n+\t  mask |= (ipar[i] - 6) << i;\n+\t}\n+      /* FALLTHRU */\n+\n     case V4DFmode:\n       /* In the 256-bit DFmode case, we can only move elements within\n          a 128-bit lane.  */\n@@ -34582,10 +34886,18 @@ avx_vpermilp_parallel (rtx par, enum machine_mode mode)\n \t}\n       break;\n \n+    case V16SFmode:\n+      /* In 512 bit SFmode case, permutation in the upper 256 bits\n+\t must mirror the permutation in the lower 256-bits.  */\n+      for (i = 0; i < 8; ++i)\n+\tif (ipar[i] + 8 != ipar[i + 8])\n+\t  return 0;\n+      /* FALLTHRU */\n+\n     case V8SFmode:\n-      /* In the 256-bit SFmode case, we have full freedom of movement\n-\t within the low 128-bit lane, but the high 128-bit lane must\n-\t mirror the exact same pattern.  */\n+      /* In 256 bit SFmode case, we have full freedom of\n+         movement within the low 128-bit lane, but the high 128-bit\n+         lane must mirror the exact same pattern.  */\n       for (i = 0; i < 4; ++i)\n \tif (ipar[i] + 4 != ipar[i + 4])\n \t  return 0;\n@@ -35536,6 +35848,7 @@ static bool\n ix86_rtx_costs (rtx x, int code_i, int outer_code_i, int opno, int *total,\n \t\tbool speed)\n {\n+  rtx mask;\n   enum rtx_code code = (enum rtx_code) code_i;\n   enum rtx_code outer_code = (enum rtx_code) outer_code_i;\n   enum machine_mode mode = GET_MODE (x);\n@@ -36012,13 +36325,21 @@ ix86_rtx_costs (rtx x, int code_i, int outer_code_i, int opno, int *total,\n \n     case VEC_SELECT:\n     case VEC_CONCAT:\n-    case VEC_MERGE:\n     case VEC_DUPLICATE:\n       /* ??? Assume all of these vector manipulation patterns are\n \t recognizable.  In which case they all pretty much have the\n \t same cost.  */\n      *total = cost->fabs;\n      return true;\n+    case VEC_MERGE:\n+      mask = XEXP (x, 2);\n+      /* This is masked instruction, assume the same cost,\n+\t as nonmasked variant.  */\n+      if (TARGET_AVX512F && register_operand (mask, GET_MODE (mask)))\n+\t*total = rtx_cost (XEXP (x, 0), outer_code, opno, speed);\n+      else\n+\t*total = cost->fabs;\n+      return true;\n \n     default:\n       return false;\n@@ -37184,6 +37505,36 @@ get_mode_wider_vector (enum machine_mode o)\n   return n;\n }\n \n+/* A subroutine of ix86_expand_vector_init_duplicate.  Tries to\n+   fill target with val via vec_duplicate.  */\n+\n+static bool\n+ix86_vector_duplicate_value (enum machine_mode mode, rtx target, rtx val)\n+{\n+  bool ok;\n+  rtx insn, dup;\n+\n+  /* First attempt to recognize VAL as-is.  */\n+  dup = gen_rtx_VEC_DUPLICATE (mode, val);\n+  insn = emit_insn (gen_rtx_SET (VOIDmode, target, dup));\n+  if (recog_memoized (insn) < 0)\n+    {\n+      rtx seq;\n+      /* If that fails, force VAL into a register.  */\n+\n+      start_sequence ();\n+      XEXP (dup, 0) = force_reg (GET_MODE_INNER (mode), val);\n+      seq = get_insns ();\n+      end_sequence ();\n+      if (seq)\n+\temit_insn_before (seq, insn);\n+\n+      ok = recog_memoized (insn) >= 0;\n+      gcc_assert (ok);\n+    }\n+  return true;\n+}\n+\n /* A subroutine of ix86_expand_vector_init.  Store into TARGET a vector\n    with all elements equal to VAR.  Return true if successful.  */\n \n@@ -37209,29 +37560,11 @@ ix86_expand_vector_init_duplicate (bool mmx_ok, enum machine_mode mode,\n     case V2DImode:\n     case V4SFmode:\n     case V4SImode:\n-      {\n-\trtx insn, dup;\n-\n-\t/* First attempt to recognize VAL as-is.  */\n-\tdup = gen_rtx_VEC_DUPLICATE (mode, val);\n-\tinsn = emit_insn (gen_rtx_SET (VOIDmode, target, dup));\n-\tif (recog_memoized (insn) < 0)\n-\t  {\n-\t    rtx seq;\n-\t    /* If that fails, force VAL into a register.  */\n-\n-\t    start_sequence ();\n-\t    XEXP (dup, 0) = force_reg (GET_MODE_INNER (mode), val);\n-\t    seq = get_insns ();\n-\t    end_sequence ();\n-\t    if (seq)\n-\t      emit_insn_before (seq, insn);\n-\n-\t    ok = recog_memoized (insn) >= 0;\n-\t    gcc_assert (ok);\n-\t  }\n-      }\n-      return true;\n+    case V16SImode:\n+    case V8DImode:\n+    case V16SFmode:\n+    case V8DFmode:\n+      return ix86_vector_duplicate_value (mode, target, val);\n \n     case V4HImode:\n       if (!mmx_ok)\n@@ -37581,8 +37914,8 @@ static void\n ix86_expand_vector_init_concat (enum machine_mode mode,\n \t\t\t\trtx target, rtx *ops, int n)\n {\n-  enum machine_mode cmode, hmode = VOIDmode;\n-  rtx first[8], second[4];\n+  enum machine_mode cmode, hmode = VOIDmode, gmode = VOIDmode;\n+  rtx first[16], second[8], third[4];\n   rtvec v;\n   int i, j;\n \n@@ -37591,6 +37924,18 @@ ix86_expand_vector_init_concat (enum machine_mode mode,\n     case 2:\n       switch (mode)\n \t{\n+\tcase V16SImode:\n+\t  cmode = V8SImode;\n+\t  break;\n+\tcase V16SFmode:\n+\t  cmode = V8SFmode;\n+\t  break;\n+\tcase V8DImode:\n+\t  cmode = V4DImode;\n+\t  break;\n+\tcase V8DFmode:\n+\t  cmode = V4DFmode;\n+\t  break;\n \tcase V8SImode:\n \t  cmode = V4SImode;\n \t  break;\n@@ -37657,6 +38002,14 @@ ix86_expand_vector_init_concat (enum machine_mode mode,\n     case 8:\n       switch (mode)\n \t{\n+\tcase V8DImode:\n+\t  cmode = V2DImode;\n+\t  hmode = V4DImode;\n+\t  break;\n+\tcase V8DFmode:\n+\t  cmode = V2DFmode;\n+\t  hmode = V4DFmode;\n+\t  break;\n \tcase V8SImode:\n \t  cmode = V2SImode;\n \t  hmode = V4SImode;\n@@ -37670,6 +38023,24 @@ ix86_expand_vector_init_concat (enum machine_mode mode,\n \t}\n       goto half;\n \n+    case 16:\n+      switch (mode)\n+\t{\n+\tcase V16SImode:\n+\t  cmode = V2SImode;\n+\t  hmode = V4SImode;\n+\t  gmode = V8SImode;\n+\t  break;\n+\tcase V16SFmode:\n+\t  cmode = V2SFmode;\n+\t  hmode = V4SFmode;\n+\t  gmode = V8SFmode;\n+\t  break;\n+\tdefault:\n+\t  gcc_unreachable ();\n+\t}\n+      goto half;\n+\n half:\n       /* FIXME: We process inputs backward to help RA.  PR 36222.  */\n       i = n - 1;\n@@ -37683,7 +38054,27 @@ ix86_expand_vector_init_concat (enum machine_mode mode,\n \t}\n \n       n >>= 1;\n-      if (n > 2)\n+      if (n > 4)\n+\t{\n+\t  gcc_assert (hmode != VOIDmode);\n+\t  gcc_assert (gmode != VOIDmode);\n+\t  for (i = j = 0; i < n; i += 2, j++)\n+\t    {\n+\t      second[j] = gen_reg_rtx (hmode);\n+\t      ix86_expand_vector_init_concat (hmode, second [j],\n+\t\t\t\t\t      &first [i], 2);\n+\t    }\n+\t  n >>= 1;\n+\t  for (i = j = 0; i < n; i += 2, j++)\n+\t    {\n+\t      third[j] = gen_reg_rtx (gmode);\n+\t      ix86_expand_vector_init_concat (gmode, third[j],\n+\t\t\t\t\t      &second[i], 2);\n+\t    }\n+\t  n >>= 1;\n+\t  ix86_expand_vector_init_concat (mode, target, third, n);\n+\t}\n+      else if (n > 2)\n \t{\n \t  gcc_assert (hmode != VOIDmode);\n \t  for (i = j = 0; i < n; i += 2, j++)\n@@ -37826,7 +38217,7 @@ static void\n ix86_expand_vector_init_general (bool mmx_ok, enum machine_mode mode,\n \t\t\t\t rtx target, rtx vals)\n {\n-  rtx ops[32], op0, op1;\n+  rtx ops[64], op0, op1;\n   enum machine_mode half_mode = VOIDmode;\n   int n, i;\n \n@@ -37838,6 +38229,10 @@ ix86_expand_vector_init_general (bool mmx_ok, enum machine_mode mode,\n \tbreak;\n       /* FALLTHRU */\n \n+    case V16SImode:\n+    case V16SFmode:\n+    case V8DFmode:\n+    case V8DImode:\n     case V8SFmode:\n     case V8SImode:\n     case V4DFmode:\n@@ -38463,6 +38858,42 @@ ix86_expand_vector_extract (bool mmx_ok, rtx target, rtx vec, int elt)\n \t}\n       break;\n \n+    case V16SFmode:\n+      tmp = gen_reg_rtx (V8SFmode);\n+      if (elt < 8)\n+\temit_insn (gen_vec_extract_lo_v16sf (tmp, vec));\n+      else\n+\temit_insn (gen_vec_extract_hi_v16sf (tmp, vec));\n+      ix86_expand_vector_extract (false, target, tmp, elt & 7);\n+      return;\n+\n+    case V8DFmode:\n+      tmp = gen_reg_rtx (V4DFmode);\n+      if (elt < 4)\n+\temit_insn (gen_vec_extract_lo_v8df (tmp, vec));\n+      else\n+\temit_insn (gen_vec_extract_hi_v8df (tmp, vec));\n+      ix86_expand_vector_extract (false, target, tmp, elt & 3);\n+      return;\n+\n+    case V16SImode:\n+      tmp = gen_reg_rtx (V8SImode);\n+      if (elt < 8)\n+\temit_insn (gen_vec_extract_lo_v16si (tmp, vec));\n+      else\n+\temit_insn (gen_vec_extract_hi_v16si (tmp, vec));\n+      ix86_expand_vector_extract (false, target, tmp, elt & 7);\n+      return;\n+\n+    case V8DImode:\n+      tmp = gen_reg_rtx (V4DImode);\n+      if (elt < 4)\n+\temit_insn (gen_vec_extract_lo_v8di (tmp, vec));\n+      else\n+\temit_insn (gen_vec_extract_hi_v8di (tmp, vec));\n+      ix86_expand_vector_extract (false, target, tmp, elt & 3);\n+      return;\n+\n     case V8QImode:\n       /* ??? Could extract the appropriate HImode element and shift.  */\n     default:\n@@ -38555,6 +38986,44 @@ emit_reduc_half (rtx dest, rtx src, int i)\n \t\t\t\t    GEN_INT (i / 2));\n \t}\n       break;\n+    case V16SImode:\n+    case V16SFmode:\n+    case V8DImode:\n+    case V8DFmode:\n+      if (i > 128)\n+\ttem = gen_avx512f_shuf_i32x4_1 (gen_lowpart (V16SImode, dest),\n+\t\t\t\t      gen_lowpart (V16SImode, src),\n+\t\t\t\t      gen_lowpart (V16SImode, src),\n+\t\t\t\t      GEN_INT (0x4 + (i == 512 ? 4 : 0)),\n+\t\t\t\t      GEN_INT (0x5 + (i == 512 ? 4 : 0)),\n+\t\t\t\t      GEN_INT (0x6 + (i == 512 ? 4 : 0)),\n+\t\t\t\t      GEN_INT (0x7 + (i == 512 ? 4 : 0)),\n+\t\t\t\t      GEN_INT (0xC), GEN_INT (0xD),\n+\t\t\t\t      GEN_INT (0xE), GEN_INT (0xF),\n+\t\t\t\t      GEN_INT (0x10), GEN_INT (0x11),\n+\t\t\t\t      GEN_INT (0x12), GEN_INT (0x13),\n+\t\t\t\t      GEN_INT (0x14), GEN_INT (0x15),\n+\t\t\t\t      GEN_INT (0x16), GEN_INT (0x17));\n+      else\n+\ttem = gen_avx512f_pshufd_1 (gen_lowpart (V16SImode, dest),\n+\t\t\t\t   gen_lowpart (V16SImode, src),\n+\t\t\t\t   GEN_INT (i == 128 ? 0x2 : 0x1),\n+\t\t\t\t   GEN_INT (0x3),\n+\t\t\t\t   GEN_INT (0x3),\n+\t\t\t\t   GEN_INT (0x3),\n+\t\t\t\t   GEN_INT (i == 128 ? 0x6 : 0x5),\n+\t\t\t\t   GEN_INT (0x7),\n+\t\t\t\t   GEN_INT (0x7),\n+\t\t\t\t   GEN_INT (0x7),\n+\t\t\t\t   GEN_INT (i == 128 ? 0xA : 0x9),\n+\t\t\t\t   GEN_INT (0xB),\n+\t\t\t\t   GEN_INT (0xB),\n+\t\t\t\t   GEN_INT (0xB),\n+\t\t\t\t   GEN_INT (i == 128 ? 0xE : 0xD),\n+\t\t\t\t   GEN_INT (0xF),\n+\t\t\t\t   GEN_INT (0xF),\n+\t\t\t\t   GEN_INT (0xF));\n+      break;\n     default:\n       gcc_unreachable ();\n     }\n@@ -38619,6 +39088,8 @@ ix86_vector_mode_supported_p (enum machine_mode mode)\n     return true;\n   if (TARGET_AVX && VALID_AVX256_REG_MODE (mode))\n     return true;\n+  if (TARGET_AVX512F && VALID_AVX512F_REG_MODE (mode))\n+    return true;\n   if (TARGET_MMX && VALID_MMX_REG_MODE (mode))\n     return true;\n   if (TARGET_3DNOW && VALID_MMX_REG_MODE_3DNOW (mode))\n@@ -38932,9 +39403,15 @@ void ix86_emit_swdivsf (rtx res, rtx a, rtx b, enum machine_mode mode)\n   b = force_reg (mode, b);\n \n   /* x0 = rcp(b) estimate */\n-  emit_insn (gen_rtx_SET (VOIDmode, x0,\n-\t\t\t  gen_rtx_UNSPEC (mode, gen_rtvec (1, b),\n-\t\t\t\t\t  UNSPEC_RCP)));\n+  if (mode == V16SFmode || mode == V8DFmode)\n+    emit_insn (gen_rtx_SET (VOIDmode, x0,\n+\t\t\t    gen_rtx_UNSPEC (mode, gen_rtvec (1, b),\n+\t\t\t\t\t    UNSPEC_RCP14)));\n+  else\n+    emit_insn (gen_rtx_SET (VOIDmode, x0,\n+\t\t\t    gen_rtx_UNSPEC (mode, gen_rtvec (1, b),\n+\t\t\t\t\t    UNSPEC_RCP)));\n+\n   /* e0 = x0 * b */\n   emit_insn (gen_rtx_SET (VOIDmode, e0,\n \t\t\t  gen_rtx_MULT (mode, x0, b)));\n@@ -38964,6 +39441,7 @@ void ix86_emit_swsqrtsf (rtx res, rtx a, enum machine_mode mode,\n {\n   rtx x0, e0, e1, e2, e3, mthree, mhalf;\n   REAL_VALUE_TYPE r;\n+  int unspec;\n \n   x0 = gen_reg_rtx (mode);\n   e0 = gen_reg_rtx (mode);\n@@ -38976,11 +39454,15 @@ void ix86_emit_swsqrtsf (rtx res, rtx a, enum machine_mode mode,\n \n   real_arithmetic (&r, NEGATE_EXPR, &dconsthalf, NULL);\n   mhalf = CONST_DOUBLE_FROM_REAL_VALUE (r, SFmode);\n+  unspec = UNSPEC_RSQRT;\n \n   if (VECTOR_MODE_P (mode))\n     {\n       mthree = ix86_build_const_vector (mode, true, mthree);\n       mhalf = ix86_build_const_vector (mode, true, mhalf);\n+      /* There is no 512-bit rsqrt.  There is however rsqrt14.  */\n+      if (GET_MODE_SIZE (mode) == 64)\n+\tunspec = UNSPEC_RSQRT14;\n     }\n \n   /* sqrt(a)  = -0.5 * a * rsqrtss(a) * (a * rsqrtss(a) * rsqrtss(a) - 3.0)\n@@ -38991,7 +39473,7 @@ void ix86_emit_swsqrtsf (rtx res, rtx a, enum machine_mode mode,\n   /* x0 = rsqrt(a) estimate */\n   emit_insn (gen_rtx_SET (VOIDmode, x0,\n \t\t\t  gen_rtx_UNSPEC (mode, gen_rtvec (1, a),\n-\t\t\t\t\t  UNSPEC_RSQRT)));\n+\t\t\t\t\t  unspec)));\n \n   /* If (a == 0.0) Filter out infinity to prevent NaN for sqrt(0.0).  */\n   if (!recip)\n@@ -39002,11 +39484,23 @@ void ix86_emit_swsqrtsf (rtx res, rtx a, enum machine_mode mode,\n       mask = gen_reg_rtx (mode);\n \n       zero = force_reg (mode, CONST0_RTX(mode));\n-      emit_insn (gen_rtx_SET (VOIDmode, mask,\n-\t\t\t      gen_rtx_NE (mode, zero, a)));\n \n-      emit_insn (gen_rtx_SET (VOIDmode, x0,\n-\t\t\t      gen_rtx_AND (mode, x0, mask)));\n+      /* Handle masked compare.  */\n+      if (VECTOR_MODE_P (mode) && GET_MODE_SIZE (mode) == 64)\n+\t{\n+\t  mask = gen_reg_rtx (HImode);\n+\t  /* Imm value 0x4 corresponds to not-equal comparison.  */\n+\t  emit_insn (gen_avx512f_cmpv16sf3 (mask, zero, a, GEN_INT (0x4)));\n+\t  emit_insn (gen_avx512f_blendmv16sf (x0, zero, x0, mask));\n+\t}\n+      else\n+\t{\n+\t  emit_insn (gen_rtx_SET (VOIDmode, mask,\n+\t\t\t\t  gen_rtx_NE (mode, zero, a)));\n+\n+\t  emit_insn (gen_rtx_SET (VOIDmode, x0,\n+\t\t\t\t  gen_rtx_AND (mode, x0, mask)));\n+\t}\n     }\n \n   /* e0 = x0 * a */\n@@ -40528,6 +41022,19 @@ expand_vec_perm_1 (struct expand_vec_perm_d *d)\n   if (expand_vec_perm_pshufb (d))\n     return true;\n \n+  /* Try the AVX512F vpermi2 instructions.  */\n+  rtx vec[64];\n+  enum machine_mode mode = d->vmode;\n+  if (mode == V8DFmode)\n+    mode = V8DImode;\n+  else if (mode == V16SFmode)\n+    mode = V16SImode;\n+  for (i = 0; i < nelt; ++i)\n+    vec[i] = GEN_INT (d->perm[i]);\n+  rtx mask = gen_rtx_CONST_VECTOR (mode, gen_rtvec_v (nelt, vec));\n+  if (ix86_expand_vec_perm_vpermi2 (d->target, d->op0, mask, d->op1))\n+    return true;\n+\n   return false;\n }\n \n@@ -42135,6 +42642,10 @@ ix86_vectorize_vec_perm_const_ok (enum machine_mode vmode,\n \n   /* Given sufficient ISA support we can just return true here\n      for selected vector modes.  */\n+  if (d.vmode == V16SImode || d.vmode == V16SFmode\n+      || d.vmode == V8DFmode || d.vmode == V8DImode)\n+    /* All implementable with a single vpermi2 insn.  */\n+    return true;\n   if (GET_MODE_SIZE (d.vmode) == 16)\n     {\n       /* All implementable with a single vpperm insn.  */\n@@ -42377,7 +42888,7 @@ ix86_expand_mul_widen_evenodd (rtx dest, rtx op1, rtx op2,\n     op2 = force_reg (mode, op2);\n \n   /* We only play even/odd games with vectors of SImode.  */\n-  gcc_assert (mode == V4SImode || mode == V8SImode);\n+  gcc_assert (mode == V4SImode || mode == V8SImode || mode == V16SImode);\n \n   /* If we're looking for the odd results, shift those members down to\n      the even slots.  For some cpus this is faster than a PSHUFD.  */\n@@ -42403,7 +42914,14 @@ ix86_expand_mul_widen_evenodd (rtx dest, rtx op1, rtx op2,\n       op2 = gen_lowpart (mode, op2);\n     }\n \n-  if (mode == V8SImode)\n+  if (mode == V16SImode)\n+    {\n+      if (uns_p)\n+\tx = gen_vec_widen_umult_even_v16si (dest, op1, op2);\n+      else\n+\tx = gen_vec_widen_smult_even_v16si (dest, op1, op2);\n+    }\n+  else if (mode == V8SImode)\n     {\n       if (uns_p)\n \tx = gen_vec_widen_umult_even_v8si (dest, op1, op2);\n@@ -42623,6 +43141,11 @@ ix86_expand_sse2_mulvxdi3 (rtx op0, rtx op1, rtx op2)\n \t  umul = gen_vec_widen_umult_even_v8si;\n \t  nmode = V8SImode;\n \t}\n+      else if (mode == V8DImode)\n+\t{\n+\t  umul = gen_vec_widen_umult_even_v16si;\n+\t  nmode = V16SImode;\n+\t}\n       else\n \tgcc_unreachable ();\n \n@@ -43769,19 +44292,25 @@ ix86_preferred_simd_mode (enum machine_mode mode)\n     case HImode:\n       return (TARGET_AVX && !TARGET_PREFER_AVX128) ? V16HImode : V8HImode;\n     case SImode:\n-      return (TARGET_AVX && !TARGET_PREFER_AVX128) ? V8SImode : V4SImode;\n+      return TARGET_AVX512F ? V16SImode :\n+\t(TARGET_AVX && !TARGET_PREFER_AVX128) ? V8SImode : V4SImode;\n     case DImode:\n-      return (TARGET_AVX && !TARGET_PREFER_AVX128) ? V4DImode : V2DImode;\n+      return TARGET_AVX512F ? V8DImode :\n+\t(TARGET_AVX && !TARGET_PREFER_AVX128) ? V4DImode : V2DImode;\n \n     case SFmode:\n-      if (TARGET_AVX && !TARGET_PREFER_AVX128)\n+      if (TARGET_AVX512F)\n+\treturn V16SFmode;\n+      else if (TARGET_AVX && !TARGET_PREFER_AVX128)\n \treturn V8SFmode;\n       else\n \treturn V4SFmode;\n \n     case DFmode:\n       if (!TARGET_VECTORIZE_DOUBLE)\n \treturn word_mode;\n+      else if (TARGET_AVX512F)\n+\treturn V8DFmode;\n       else if (TARGET_AVX && !TARGET_PREFER_AVX128)\n \treturn V4DFmode;\n       else if (TARGET_SSE2)\n@@ -43794,12 +44323,14 @@ ix86_preferred_simd_mode (enum machine_mode mode)\n }\n \n /* If AVX is enabled then try vectorizing with both 256bit and 128bit\n-   vectors.  */\n+   vectors.  If AVX512F is enabled then try vectorizing with 512bit,\n+   256bit and 128bit vectors.  */\n \n static unsigned int\n ix86_autovectorize_vector_sizes (void)\n {\n-  return (TARGET_AVX && !TARGET_PREFER_AVX128) ? 32 | 16 : 0;\n+  return TARGET_AVX512F ? 64 | 32 | 16 :\n+    (TARGET_AVX && !TARGET_PREFER_AVX128) ? 32 | 16 : 0;\n }\n \n \f"}, {"sha": "a3c0e0c23985eec40ab77b577f4d8489e57ad3be", "filename": "gcc/config/i386/sse.md", "status": "modified", "additions": 85, "deletions": 30, "changes": 115, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/gcc%2Fconfig%2Fi386%2Fsse.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/gcc%2Fconfig%2Fi386%2Fsse.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fsse.md?ref=d3c2fee09607e7d70cc7e69822638fab2bda6c7b", "patch": "@@ -748,8 +748,9 @@\n    (set (attr \"mode\")\n \t(cond [(match_test \"TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\")\n \t\t (const_string \"<ssePSmode>\")\n-\t       (and (eq_attr \"alternative\" \"2\")\n-\t\t    (match_test \"TARGET_SSE_TYPELESS_STORES\"))\n+\t       (and (match_test \"GET_MODE_SIZE (<MODE>mode) == 16\")\n+\t\t    (and (eq_attr \"alternative\" \"2\")\n+\t\t\t (match_test \"TARGET_SSE_TYPELESS_STORES\")))\n \t\t (const_string \"<ssePSmode>\")\n \t       (match_test \"TARGET_AVX\")\n \t\t (const_string \"<sseinsnmode>\")\n@@ -986,8 +987,9 @@\n    (set_attr \"ssememalign\" \"8\")\n    (set_attr \"prefix\" \"maybe_vex\")\n    (set (attr \"mode\")\n-\t(cond [(ior (match_test \"TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\")\n-\t\t    (match_test \"TARGET_SSE_TYPELESS_STORES\"))\n+        (cond [(and (match_test \"GET_MODE_SIZE (<MODE>mode) == 16\")\n+                    (ior (match_test \"TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\")\n+                         (match_test \"TARGET_SSE_TYPELESS_STORES\")))\n \t\t (const_string \"<ssePSmode>\")\n \t       (match_test \"TARGET_AVX\")\n \t\t (const_string \"<MODE>\")\n@@ -1091,6 +1093,7 @@\n {\n   switch (get_attr_mode (insn))\n     {\n+    case MODE_V16SF:\n     case MODE_V8SF:\n     case MODE_V4SF:\n       return \"%vmovups\\t{%1, %0|%0, %1}\";\n@@ -1113,8 +1116,9 @@\n      (const_string \"1\")))\n    (set_attr \"prefix\" \"maybe_vex\")\n    (set (attr \"mode\")\n-\t(cond [(ior (match_test \"TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\")\n-\t\t    (match_test \"TARGET_SSE_TYPELESS_STORES\"))\n+\t(cond [(and (match_test \"GET_MODE_SIZE (<MODE>mode) == 16\")\n+\t\t    (ior (match_test \"TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\")\n+\t\t\t (match_test \"TARGET_SSE_TYPELESS_STORES\")))\n \t\t (const_string \"<ssePSmode>\")\n \t       (match_test \"TARGET_AVX\")\n \t\t (const_string \"<sseinsnmode>\")\n@@ -3492,7 +3496,11 @@\n    (match_operand:<sseintvecmode> 1 \"register_operand\")]\n   \"TARGET_SSE2 && (<MODE>mode == V4SFmode || TARGET_AVX2)\"\n {\n-  ix86_expand_vector_convert_uns_vsivsf (operands[0], operands[1]);\n+  if (<MODE>mode == V16SFmode)\n+    emit_insn (gen_ufloatv16siv16sf2 (operands[0], operands[1]));\n+  else\n+    ix86_expand_vector_convert_uns_vsivsf (operands[0], operands[1]);\n+\n   DONE;\n })\n \n@@ -3583,11 +3591,17 @@\n    (match_operand:VF1 1 \"register_operand\")]\n   \"TARGET_SSE2\"\n {\n-  rtx tmp[3];\n-  tmp[0] = ix86_expand_adjust_ufix_to_sfix_si (operands[1], &tmp[2]);\n-  tmp[1] = gen_reg_rtx (<sseintvecmode>mode);\n-  emit_insn (gen_fix_trunc<mode><sseintvecmodelower>2 (tmp[1], tmp[0]));\n-  emit_insn (gen_xor<sseintvecmodelower>3 (operands[0], tmp[1], tmp[2]));\n+  if (<MODE>mode == V16SFmode)\n+    emit_insn (gen_ufix_truncv16sfv16si2 (operands[0],\n+\t\t\t\t\t  operands[1]));\n+  else\n+    {\n+      rtx tmp[3];\n+      tmp[0] = ix86_expand_adjust_ufix_to_sfix_si (operands[1], &tmp[2]);\n+      tmp[1] = gen_reg_rtx (<sseintvecmode>mode);\n+      emit_insn (gen_fix_trunc<mode><sseintvecmodelower>2 (tmp[1], tmp[0]));\n+      emit_insn (gen_xor<sseintvecmodelower>3 (operands[0], tmp[1], tmp[2]));\n+    }\n   DONE;\n })\n \n@@ -4514,6 +4528,32 @@\n   DONE;\n })\n \n+(define_expand \"vec_unpacku_float_hi_v16si\"\n+  [(match_operand:V8DF 0 \"register_operand\")\n+   (match_operand:V16SI 1 \"register_operand\")]\n+  \"TARGET_AVX512F\"\n+{\n+  REAL_VALUE_TYPE TWO32r;\n+  rtx k, x, tmp[4];\n+\n+  real_ldexp (&TWO32r, &dconst1, 32);\n+  x = const_double_from_real_value (TWO32r, DFmode);\n+\n+  tmp[0] = force_reg (V8DFmode, CONST0_RTX (V8DFmode));\n+  tmp[1] = force_reg (V8DFmode, ix86_build_const_vector (V8DFmode, 1, x));\n+  tmp[2] = gen_reg_rtx (V8DFmode);\n+  tmp[3] = gen_reg_rtx (V8SImode);\n+  k = gen_reg_rtx (QImode);\n+\n+  emit_insn (gen_vec_extract_hi_v16si (tmp[3], operands[1]));\n+  emit_insn (gen_floatv8siv8df2 (tmp[2], tmp[3]));\n+  emit_insn (gen_rtx_SET (VOIDmode, k,\n+\t\t\t  gen_rtx_LT (QImode, tmp[2], tmp[0])));\n+  emit_insn (gen_addv8df3_mask (tmp[2], tmp[2], tmp[1], tmp[2], k));\n+  emit_move_insn (operands[0], tmp[2]);\n+  DONE;\n+})\n+\n (define_expand \"vec_unpacku_float_lo_v8si\"\n   [(match_operand:V4DF 0 \"register_operand\")\n    (match_operand:V8SI 1 \"nonimmediate_operand\")]\n@@ -4679,31 +4719,46 @@\n \n (define_expand \"vec_pack_ufix_trunc_<mode>\"\n   [(match_operand:<ssepackfltmode> 0 \"register_operand\")\n-   (match_operand:VF2_128_256 1 \"register_operand\")\n-   (match_operand:VF2_128_256 2 \"register_operand\")]\n+   (match_operand:VF2 1 \"register_operand\")\n+   (match_operand:VF2 2 \"register_operand\")]\n   \"TARGET_SSE2\"\n {\n-  rtx tmp[7];\n-  tmp[0] = ix86_expand_adjust_ufix_to_sfix_si (operands[1], &tmp[2]);\n-  tmp[1] = ix86_expand_adjust_ufix_to_sfix_si (operands[2], &tmp[3]);\n-  tmp[4] = gen_reg_rtx (<ssepackfltmode>mode);\n-  emit_insn (gen_vec_pack_sfix_trunc_<mode> (tmp[4], tmp[0], tmp[1]));\n-  if (<ssepackfltmode>mode == V4SImode || TARGET_AVX2)\n+  if (<MODE>mode == V8DFmode)\n     {\n-      tmp[5] = gen_reg_rtx (<ssepackfltmode>mode);\n-      ix86_expand_vec_extract_even_odd (tmp[5], tmp[2], tmp[3], 0);\n+      rtx r1, r2;\n+\n+      r1 = gen_reg_rtx (V8SImode);\n+      r2 = gen_reg_rtx (V8SImode);\n+\n+      emit_insn (gen_ufix_truncv8dfv8si2 (r1, operands[1]));\n+      emit_insn (gen_ufix_truncv8dfv8si2 (r2, operands[2]));\n+      emit_insn (gen_avx_vec_concatv16si (operands[0], r1, r2));\n     }\n   else\n     {\n-      tmp[5] = gen_reg_rtx (V8SFmode);\n-      ix86_expand_vec_extract_even_odd (tmp[5], gen_lowpart (V8SFmode, tmp[2]),\n-\t\t\t\t\tgen_lowpart (V8SFmode, tmp[3]), 0);\n-      tmp[5] = gen_lowpart (V8SImode, tmp[5]);\n+      rtx tmp[7];\n+      tmp[0] = ix86_expand_adjust_ufix_to_sfix_si (operands[1], &tmp[2]);\n+      tmp[1] = ix86_expand_adjust_ufix_to_sfix_si (operands[2], &tmp[3]);\n+      tmp[4] = gen_reg_rtx (<ssepackfltmode>mode);\n+      emit_insn (gen_vec_pack_sfix_trunc_<mode> (tmp[4], tmp[0], tmp[1]));\n+      if (<ssepackfltmode>mode == V4SImode || TARGET_AVX2)\n+\t{\n+\t  tmp[5] = gen_reg_rtx (<ssepackfltmode>mode);\n+\t  ix86_expand_vec_extract_even_odd (tmp[5], tmp[2], tmp[3], 0);\n+\t}\n+      else\n+\t{\n+\t  tmp[5] = gen_reg_rtx (V8SFmode);\n+\t  ix86_expand_vec_extract_even_odd (tmp[5], gen_lowpart (V8SFmode, tmp[2]),\n+\t\t\t\t\t    gen_lowpart (V8SFmode, tmp[3]), 0);\n+\t  tmp[5] = gen_lowpart (V8SImode, tmp[5]);\n+\t}\n+      tmp[6] = expand_simple_binop (<ssepackfltmode>mode, XOR, tmp[4], tmp[5],\n+\t\t\t\t    operands[0], 0, OPTAB_DIRECT);\n+      if (tmp[6] != operands[0])\n+\temit_move_insn (operands[0], tmp[6]);\n     }\n-  tmp[6] = expand_simple_binop (<ssepackfltmode>mode, XOR, tmp[4], tmp[5],\n-\t\t\t\toperands[0], 0, OPTAB_DIRECT);\n-  if (tmp[6] != operands[0])\n-    emit_move_insn (operands[0], tmp[6]);\n+\n   DONE;\n })\n "}, {"sha": "ed9467b04bb88231b065246c94e28190719ab940", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=d3c2fee09607e7d70cc7e69822638fab2bda6c7b", "patch": "@@ -1,3 +1,15 @@\n+2013-12-31  Alexander Ivchenko  <alexander.ivchenko@intel.com>\n+\tMaxim Kuznetsov  <maxim.kuznetsov@intel.com>\n+\tSergey Lega  <sergey.s.lega@intel.com>\n+\tAnna Tikhonova  <anna.tikhonova@intel.com>\n+\tIlya Tocar  <ilya.tocar@intel.com>\n+\tAndrey Turetskiy  <andrey.turetskiy@intel.com>\n+\tIlya Verbin  <ilya.verbin@intel.com>\n+\tKirill Yukhin  <kirill.yukhin@intel.com>\n+\tMichael Zolotukhin  <michael.v.zolotukhin@intel.com>\n+\n+\t* gcc.target/i386/pr49002-2.c: allow vmovapd generation.\n+\n 2013-12-31  Sandra Loosemore  <sandra@codesourcery.com>\n \t    Chung-Lin Tang  <cltang@codesourcery.com>\n \t    Based on patches from Altera Corporation"}, {"sha": "dfb83b4a75dbd5a42063ad6755517d026fb06d9c", "filename": "gcc/testsuite/gcc.target/i386/pr49002-2.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fpr49002-2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fpr49002-2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fi386%2Fpr49002-2.c?ref=d3c2fee09607e7d70cc7e69822638fab2bda6c7b", "patch": "@@ -12,4 +12,4 @@ void foo(const __m128d from, __m256d *to)\n /* Ensure we store ymm, not xmm.  */\n /* { dg-final { scan-assembler-not \"vmovapd\\[\\t \\]*%xmm\\[0-9\\]\\+,\\[^,\\]*\" } } */\n /* { dg-final { scan-assembler-not \"vmovaps\\[\\t \\]*%xmm\\[0-9\\]\\+,\\[^,\\]*\" } } */\n-/* { dg-final { scan-assembler \"vmovaps\\[\\t \\]*%ymm\\[0-9\\]\\+,\\[^,\\]*\" } } */\n+/* { dg-final { scan-assembler \"vmovap\\[sd\\]\\[\\t \\]*%ymm\\[0-9\\]\\+,\\[^,\\]*\" } } */"}, {"sha": "e4f04c44760dda08ecb236bb2f98ca8d995b3f59", "filename": "gcc/tree-vect-stmts.c", "status": "modified", "additions": 27, "deletions": 7, "changes": 34, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/gcc%2Ftree-vect-stmts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/gcc%2Ftree-vect-stmts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-stmts.c?ref=d3c2fee09607e7d70cc7e69822638fab2bda6c7b", "patch": "@@ -5699,7 +5699,7 @@ vectorizable_load (gimple stmt, gimple_stmt_iterator *gsi, gimple *vec_stmt,\n       tree vec_oprnd0 = NULL_TREE, op;\n       tree arglist = TYPE_ARG_TYPES (TREE_TYPE (gather_decl));\n       tree rettype, srctype, ptrtype, idxtype, masktype, scaletype;\n-      tree ptr, mask, var, scale, perm_mask = NULL_TREE, prev_res = NULL_TREE;\n+      tree ptr, mask, var, scale, merge, perm_mask = NULL_TREE, prev_res = NULL_TREE;\n       edge pe = loop_preheader_edge (loop);\n       gimple_seq seq;\n       basic_block new_bb;\n@@ -5741,8 +5741,7 @@ vectorizable_load (gimple stmt, gimple_stmt_iterator *gsi, gimple *vec_stmt,\n       idxtype = TREE_VALUE (arglist); arglist = TREE_CHAIN (arglist);\n       masktype = TREE_VALUE (arglist); arglist = TREE_CHAIN (arglist);\n       scaletype = TREE_VALUE (arglist);\n-      gcc_checking_assert (types_compatible_p (srctype, rettype)\n-\t\t\t   && types_compatible_p (srctype, masktype));\n+      gcc_checking_assert (types_compatible_p (srctype, rettype));\n \n       vec_dest = vect_create_destination_var (scalar_dest, vectype);\n \n@@ -5756,8 +5755,13 @@ vectorizable_load (gimple stmt, gimple_stmt_iterator *gsi, gimple *vec_stmt,\n \n       /* Currently we support only unconditional gather loads,\n \t so mask should be all ones.  */\n-      if (TREE_CODE (TREE_TYPE (masktype)) == INTEGER_TYPE)\n-\tmask = build_int_cst (TREE_TYPE (masktype), -1);\n+      if (TREE_CODE (masktype) == INTEGER_TYPE)\n+\tmask = build_int_cst (masktype, -1);\n+      else if (TREE_CODE (TREE_TYPE (masktype)) == INTEGER_TYPE)\n+\t{\n+\t  mask = build_int_cst (TREE_TYPE (masktype), -1);\n+\t  mask = build_vector_from_val (masktype, mask);\n+\t}\n       else if (SCALAR_FLOAT_TYPE_P (TREE_TYPE (masktype)))\n \t{\n \t  REAL_VALUE_TYPE r;\n@@ -5766,14 +5770,30 @@ vectorizable_load (gimple stmt, gimple_stmt_iterator *gsi, gimple *vec_stmt,\n \t    tmp[j] = -1;\n \t  real_from_target (&r, tmp, TYPE_MODE (TREE_TYPE (masktype)));\n \t  mask = build_real (TREE_TYPE (masktype), r);\n+\t  mask = build_vector_from_val (masktype, mask);\n \t}\n       else\n \tgcc_unreachable ();\n-      mask = build_vector_from_val (masktype, mask);\n       mask = vect_init_vector (stmt, mask, masktype, NULL);\n \n       scale = build_int_cst (scaletype, gather_scale);\n \n+      if (TREE_CODE (TREE_TYPE (rettype)) == INTEGER_TYPE)\n+\tmerge = build_int_cst (TREE_TYPE (rettype), 0);\n+      else if (SCALAR_FLOAT_TYPE_P (TREE_TYPE (rettype)))\n+\t{\n+\t  REAL_VALUE_TYPE r;\n+\t  long tmp[6];\n+\t  for (j = 0; j < 6; ++j)\n+\t    tmp[j] = 0;\n+\t  real_from_target (&r, tmp, TYPE_MODE (TREE_TYPE (rettype)));\n+\t  merge = build_real (TREE_TYPE (rettype), r);\n+\t}\n+      else\n+\tgcc_unreachable ();\n+      merge = build_vector_from_val (rettype, merge);\n+      merge = vect_init_vector (stmt, merge, rettype, NULL);\n+\n       prev_stmt_info = NULL;\n       for (j = 0; j < ncopies; ++j)\n \t{\n@@ -5802,7 +5822,7 @@ vectorizable_load (gimple stmt, gimple_stmt_iterator *gsi, gimple *vec_stmt,\n \t    }\n \n \t  new_stmt\n-\t    = gimple_build_call (gather_decl, 5, mask, ptr, op, mask, scale);\n+\t    = gimple_build_call (gather_decl, 5, merge, ptr, op, mask, scale);\n \n \t  if (!useless_type_conversion_p (vectype, rettype))\n \t    {"}, {"sha": "00e56dcb388015d2d5933dedd09fac25ae3a6035", "filename": "gcc/tree-vectorizer.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/gcc%2Ftree-vectorizer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d3c2fee09607e7d70cc7e69822638fab2bda6c7b/gcc%2Ftree-vectorizer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.h?ref=d3c2fee09607e7d70cc7e69822638fab2bda6c7b", "patch": "@@ -683,8 +683,8 @@ struct dataref_aux {\n    conversion.  */\n #define MAX_INTERM_CVT_STEPS         3\n \n-/* The maximum vectorization factor supported by any target (V32QI).  */\n-#define MAX_VECTORIZATION_FACTOR 32\n+/* The maximum vectorization factor supported by any target (V64QI).  */\n+#define MAX_VECTORIZATION_FACTOR 64\n \n /* Avoid GTY(()) on stmt_vec_info.  */\n typedef void *vec_void_p;"}]}