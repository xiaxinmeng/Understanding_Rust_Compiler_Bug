{"sha": "bd94906f987eb065dd6d4e9cadef82f7513d05cd", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YmQ5NDkwNmY5ODdlYjA2NWRkNmQ0ZTljYWRlZjgyZjc1MTNkMDVjZA==", "commit": {"author": {"name": "Andrew MacLeod", "email": "amacleod@redhat.com", "date": "2015-11-11T14:28:46Z"}, "committer": {"name": "Andrew Macleod", "email": "amacleod@gcc.gnu.org", "date": "2015-11-11T14:28:46Z"}, "message": "count-headers: Initial file.\n\n\n2015-11-11  Andrew MacLeod  <amacleod@redhat.com>\n\n\t* count-headers: Initial file.\n\t* gcc-order-headers: Initial file.\n\t* graph-header-logs: Initial file.\n\t* graph-include-web: Initial file.\n\t* headerutils.py: Initial file.\n\t* included-by: Initial file.\n\t* README: Initial file.\n\t* reduce-headers: Initial file.\n\t* replace-header: Initial file.\n\t* show-headers: Initial file.\n\nFrom-SVN: r230171", "tree": {"sha": "8925516ac8012caf8509124279d6185b20dcfd4a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/8925516ac8012caf8509124279d6185b20dcfd4a"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/bd94906f987eb065dd6d4e9cadef82f7513d05cd", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/bd94906f987eb065dd6d4e9cadef82f7513d05cd", "html_url": "https://github.com/Rust-GCC/gccrs/commit/bd94906f987eb065dd6d4e9cadef82f7513d05cd", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/bd94906f987eb065dd6d4e9cadef82f7513d05cd/comments", "author": null, "committer": null, "parents": [{"sha": "7337ccc4b2d75d2db24dcb1f2e944f50dbe1e2ab", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7337ccc4b2d75d2db24dcb1f2e944f50dbe1e2ab", "html_url": "https://github.com/Rust-GCC/gccrs/commit/7337ccc4b2d75d2db24dcb1f2e944f50dbe1e2ab"}], "stats": {"total": 2570, "additions": 2570, "deletions": 0}, "files": [{"sha": "543e9520e02417b40a804149ddbf130941566857", "filename": "contrib/ChangeLog", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2FChangeLog?ref=bd94906f987eb065dd6d4e9cadef82f7513d05cd", "patch": "@@ -1,3 +1,8 @@\n+2015-11-11  Andrew MacLeod  <amacleod@redhat.com>\n+\n+\t* header-tools: New.  Directory containing a set of tools for\n+\tmanipulating header files.\n+\n 2015-10-30  Nathan Sidwell  <nathan@acm.org>\n \n \t* config-list.mk (nvptx-none): Add it."}, {"sha": "9baeaa6548d87065bb2345bb2658087156528b85", "filename": "contrib/header-tools/ChangeLog", "status": "added", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2Fheader-tools%2FChangeLog?ref=bd94906f987eb065dd6d4e9cadef82f7513d05cd", "patch": "@@ -0,0 +1,12 @@\n+2015-11-11  Andrew MacLeod  <amacleod@redhat.com>\n+\n+\t* count-headers: Initial file.\n+\t* gcc-order-headers: Initial file.\n+\t* graph-header-logs: Initial file.\n+\t* graph-include-web: Initial file.\n+\t* headerutils.py: Initial file.\n+\t* included-by: Initial file.\n+\t* README: Initial file.\n+\t* reduce-headers: Initial file.\n+\t* replace-header: Initial file.\n+\t* show-headers: Initial file."}, {"sha": "05d3b97f62a10dd2f5321c8032d94f9bbb65c87a", "filename": "contrib/header-tools/README", "status": "added", "additions": 283, "deletions": 0, "changes": 283, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2FREADME", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2FREADME", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2Fheader-tools%2FREADME?ref=bd94906f987eb065dd6d4e9cadef82f7513d05cd", "patch": "@@ -0,0 +1,283 @@\n+Quick start documentation for the header file utilities.  \n+\n+This isn't a full breakdown of the tools, just they typical use scenarios.\n+\n+- Each tool accepts -h to show it's usage.  Usually no parameters will also\n+trigger the help message.  Help may specify additional functionality to what is\n+listed here.\n+\n+- For all tools, option format for specifying filenames must have no spaces\n+between the option and filename.\n+ie.:     tool -lfilename.h  target.h\n+\n+- Many of the tools are required to be run from the core gcc source directory\n+containing coretypes.h.  Typically that is in gcc/gcc from a source checkout.\n+For these tools to work on files not in this directory, their path needs to be\n+specified on the command line.\n+ie.:     tool c/c-decl.c  lto/lto.c\n+\n+- options can be intermixed with filenames anywhere on the command line\n+ie.   tool ssa.h rtl.h -a   is equivalent to \n+      tool ssa.h -a rtl.h\n+\n+\n+\n+\n+\n+gcc-order-headers\n+-----------------\n+  This will reorder any primary backend headers files known to the tool into a\n+  canonical order which will resolve any hidden dependencies they may have.\n+  Any unknown headers will simply be placed after the recognized files, and\n+  retain the same relative ordering they had.\n+ \n+  This tool must be run in the core gcc source directory.\n+\n+  Simply execute the command listing any files you wish to process on the\n+  command line.\n+\n+  Any files which are changed are output, and the original is saved with a\n+  .bak extention.\n+\n+  ex.:     gcc-order-headers tree-ssa.c c/c-decl.c\n+\n+  -s will list all of the known headers in their canonical order. It does not\n+  show which of those headers include other headers, just the final canonical\n+  ordering.\n+\n+  if any header files are included within a conditional code block, the tool\n+  will issue a message and not change the file.  When this happens, you can\n+  manually inspect the file to determine if reordering it is actually OK.  Then\n+  rerun the command with the -i option.  This will ignore the conditional error\n+  condition and perform the re-ordering anyway.\n+  \n+  If any #include line has the beginning of a multi-line comment, it will also\n+  refuse to process the file until that is resolved by terminating the comment\n+  on the same line, or removing it.\n+\n+\n+show-headers\n+------------\n+  This will show the include structure for any given file. Each level of nesting\n+  is indented, and when any duplicate headers are seen, they have their\n+  duplicate number shown\n+\n+  -i may be used to specify additional search directories for headers to parse.\n+  -s specifies headers to look for and emphasize in the output.\n+\n+  This tool must be run in the core gcc source directory.\n+\n+  ex.: show-headers -sansidecl.h tree-ssa.c\n+\ttree-ssa.c\n+\t  config.h\n+\t    auto-host.h\n+\t    ansidecl.h  (1)               <<-------\n+\t  system.h\n+\t    safe-ctype.h\n+\t    filenames.h\n+\t      hashtab.h  (1)\n+\t\tansidecl.h  (2)                <<-------\n+\t    libiberty.h\n+\t      ansidecl.h  (3)                <<-------\n+\t    hwint.h\n+\t  coretypes.h\n+\t    machmode.h  (1)\n+\t      insn-modes.h  (1)\n+\t    signop.h\n+\t  <...>\n+\n+\n+\n+\n+count-headers\n+-------------\n+  simply count all the headers found in the specified files. A summary is \n+  printed showing occurrences from high to low.\n+\n+  ex.:    count-headers  tree*.c\n+\t    86 : coretypes.h\n+\t    86 : config.h\n+\t    86 : system.h\n+\t    86 : tree.h\n+\t    82 : backend.h\n+\t    80 : gimple.h\n+\t    72 : gimple-iterator.h\n+\t    70 : ssa.h\n+\t    68 : fold-const.h\n+            <...>\n+\n+\n+\n+included-by\n+-----------\n+  This tool will search all the .c,.cc and .h files and output a list of files\n+  which include the specified header(s).\n+\n+  A 4 level deep 'find' of all source files is performed from the current\n+  directory and each of those is inspected for a #include of the specified\n+  headers.  So expect a little bit of slowness.\n+\n+  -i limits the search to only other header files.\n+  -c limits the search to .c and .cc files.\n+  -a shows only source files which include all specified headers.\n+  -f allows you to specify a file which contains a list of source files to\n+     check rather than performing the much slower find command.\n+\n+  ex: included-by tree-vectorizer.h\n+\tconfig/aarch64/aarch64.c\n+\tconfig/i386/i386.c\n+\tconfig/rs6000/rs6000.c\n+\ttree-loop-distribution.c\n+\ttree-parloops.c\n+\ttree-ssa-loop-ivopts.c\n+\ttree-ssa-loop.c\n+\n+\n+\n+\n+replace-header\n+--------------\n+  This tool simply replaces a single header file with one or more other headers.\n+  -r specifies the include to replace, and one or more -f options specify the\n+  replacement headers, in the order they occur.\n+  \n+  This is commonly used in conjunction with 'included-by' to change all \n+  occurrences of a header file to something else, or to insert new headers \n+  before or after.  \n+\n+  ex:  to insert #include \"before.h\" before every occurence of tree.h in all\n+  .c and .cc source files:\n+\n+  replace-header -rtree.h -fbefore.h -ftree.h `included-by -c tree.h`\n+\n+\n+\n+\n+reduce-headers\n+--------------\n+\n+  This tool removes any header files which are not needed from a source file.\n+\n+  This tool must be run for the core gcc source directory, and requires either\n+  a native build and sometimes target builds, depending on what you are trying\n+  to reduce.\n+\n+  it is good practice to run 'gcc-order-headers' on a source file before trying\n+  to reduce it.  This removes duplicates and performs some simplifications \n+  which reduce the chances of the reduction tool missing things.\n+  \n+  start with a completely bootstrapped native compiler.\n+\n+  Any desired target builds should be built in one directory using a modified\n+  config-list.mk file which does not delete the build directory when it is done.\n+  any target directories which do not successfully complete a 'make all-gcc'\n+  may cause the tool to not reduce anything.\n+  (todo - provide a config-list.mk that leaves successful target builds, but\n+          deletes ones which do not compile)\n+\n+  The tool will examine all the target builds to determine which targets build\n+  the file, and include those targets in the testing.\n+  \n+\n+\n+  The tool will analyze a source file and attempt to remove each non-conditional\n+  header from last to first in the file.:\n+    It will first attempt to build the native all-gcc target.\n+    If that succeeds, it will attempt to build any target build .o files\n+    If that succeeds, it will check to see if there are any conditional\n+       compilation dependencies between this header file and the source file or\n+       any header which have already been determined as non-removable.\n+    If all these tests are passed, the header file is determined to be removable\n+       and is removed from the source file.\n+    This continues until all headers have been checked.\n+  At this point, a bootstrap is attempted in the native build, and if that\n+     passes the file is considered reduced.\n+\n+  Any files from the config subdirectory require target builds to be present\n+  in order to proceed.\n+\n+  A small subset of targets has been determined to provide excellent coverage,\n+  at least as of Aug 31/15 .  They were found by reducing all the files\n+  contained in libbackend.a oer a full set of targets(207).  All conditions\n+  which disallowed removal of a header file were triggered by one or more of\n+  these targets.  They are also known to the tool.  When building targets it\n+  will check those targets before the rest.  \n+  This coverage can be achieved by building config-list.mk with :\n+  LIST=\"aarch64-linux-gnu arm-netbsdelf avr-rtems c6x-elf epiphany-elf hppa2.0-hpux10.1 i686-mingw32crt i686-pc-msdosdjgpp mipsel-elf powerpc-eabisimaltivec rs6000-ibm-aix5.1.0 sh-superh-elf sparc64-elf spu-elf\"\n+\n+  -b specifies the native bootstrapped build root directory\n+  -t specifies a target build root directory that config-list.mk was run from\n+  -f is used to limit the headers for consideration.\n+\n+  example:\n+\n+  mkdir gcc          // checkout gcc in subdir gcc\n+  mdsir build        // boostrap gcc in subdir build\n+  mkdir target       // create target directory and run config-list.mk\n+  cd gcc/gcc\n+\n+  reduce-headers -b../../build -t../../targets -falias.h -fexpr.h tree*.c  (1)\n+       #  This will attempt to remove only alias.h and expr.h from tree*.c\n+\n+  reduce-headers -b../../build -t../../targets tree-ssa-live.c\n+       #  This will attempt to remove all header files from tree-ssa-live.c\n+  \n+\n+  the tool will generate a number of log files:\n+\n+    reduce-headers.log : All compilation failures from attempted reductions.\n+    reduce-headers.sum : One line summary of what happened to each source file.\n+\n+  (All the remaining logs are appended to, so if the tool is run multiple times\n+  these files are just added to. You must physically remove them yourself in\n+  order to reset the logs.)\n+\n+    reduce-headers-kept.log: List of all the successful compiles that were\n+                             ignored because of conditional macro dependencies\n+\t\t\t     and why it thinks that is the case\n+    $src.c.log  : for each failed header removal, the compilation\n+\t\t  messages as to why it failed.\n+    $header.h.log: The same log is put into the relevant header log as well.\n+\n+\n+a sample output from ira.c.log:\n+\n+Compilation failed:\n+ for shrink-wrap.h:\n+\n+ ============================================\n+ /gcc/2015-09-09/gcc/gcc/ira.c: In function \u2018bool split_live_ranges_for_shrink_wrap()\u2019:\n+ /gcc/2015-09-09/gcc/gcc/ira.c:4839:8: error: \u2018SHRINK_WRAPPING_ENABLED\u2019 was not declared in this scope\n+    if (!SHRINK_WRAPPING_ENABLED)\n+            ^\n+\t    make: *** [ira.o] Error 1\n+\n+\n+the same message would be put into shrink-wrap.h.log.\n+\n+\n+\n+graph-header-logs\n+-----------------\n+  This tool will parse all the messages from the .C files, looking for failures\n+  that show up in other headers...  meaning there is a compilation dependency\n+  between the 2 header files. \n+\n+  The tool will aggregate all these and generate a graph of the dependencies\n+  exposed during compilation.  Red lines indicate dependencies that are\n+  present because a header file physically includes another file. Black lines\n+  represent data dependencies causing compilation failures if the header is\n+  not present.\n+\n+  ex.: graph-header-logs *.c.log\n+\n+\n+\n+graph-include-web\n+-----------------\n+  This tool can be used to visualize the include structure in files.  It is\n+  rapidly turned useless if you specify too many things, but it can be \n+  useful for finding cycles and redundancies, or simply to see what a single\n+  file looks like.\n+\n+  ex.: graph-include-web tree.c"}, {"sha": "7a92596a6021f7c50cc3621d8dcd7b8147669381", "filename": "contrib/header-tools/count-headers", "status": "added", "additions": 58, "deletions": 0, "changes": 58, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Fcount-headers", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Fcount-headers", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2Fheader-tools%2Fcount-headers?ref=bd94906f987eb065dd6d4e9cadef82f7513d05cd", "patch": "@@ -0,0 +1,58 @@\n+#! /usr/bin/python2\n+import os.path\n+import sys\n+import shlex\n+import re\n+\n+from headerutils import *\n+\n+usage = False\n+src = list ()\n+flist = { }\n+process_h = True\n+process_c = True\n+verbose = False\n+all_inc = True\n+level = 0\n+\n+only_use_list = list ()\n+\n+for x in sys.argv[1:]:\n+  if x[0:2] == \"-h\":\n+    usage = True\n+  else:\n+    src.append (x)\n+\n+\n+if not usage and len (src) > 0:\n+  incl = { }\n+  for fn in src:\n+    src = readwholefile (fn)\n+    dup = { }\n+    for line in src:\n+      d = find_pound_include (line, True, True)\n+      if d != \"\" and d[-2:] ==\".h\":\n+        if dup.get (d) == None:\n+          if incl.get (d) == None:\n+            incl[d] = 1\n+          else:\n+            incl[d] = incl[d]+ 1\n+          dup[d] = 1\n+\n+  l = list ()\n+  for i in incl:\n+    l.append ((incl[i], i))\n+  l.sort (key=lambda tup:tup[0], reverse=True)\n+\n+  for f in l:\n+    print str (f[0]) + \" : \" + f[1]\n+\n+else:\n+  print \"count-headers file1 [filen]\"\n+  print \"Count the number of occurrences of all includes across all listed files\"\n+\n+ \n+\n+\n+\n+"}, {"sha": "ee76cba4b18e1372ec15bda6353ba96c49e4e091", "filename": "contrib/header-tools/gcc-order-headers", "status": "added", "additions": 397, "deletions": 0, "changes": 397, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Fgcc-order-headers", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Fgcc-order-headers", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2Fheader-tools%2Fgcc-order-headers?ref=bd94906f987eb065dd6d4e9cadef82f7513d05cd", "patch": "@@ -0,0 +1,397 @@\n+#! /usr/bin/python2\n+import os\n+import sys\n+import shlex\n+import re\n+\n+from headerutils import *\n+import Queue\n+\n+file_list = list ()\n+usage = False\n+\n+ignore_conditional = False\n+\n+order = [\n+  \"system.h\",\n+  \"coretypes.h\",\n+  \"backend.h\",\n+  \"target.h\",\n+  \"rtl.h\",\n+  \"c-family/c-target.h\",\n+  \"c-family/c-target-def.h\",\n+  \"tree.h\",\n+  \"cp/cp-tree.h\",\n+  \"c-family/c-common.h\",  # these must come before diagnostic.h\n+  \"c/c-tree.h\",\n+  \"fortran/gfortran.h\",\n+  \"gimple.h\",\n+  \"cfghooks.h\",\n+  \"df.h\",\n+  \"tm_p.h\",\n+  \"gimple-iterators.h\",\n+  \"ssa.h\",\n+  \"expmed.h\",\n+  \"optabs.h\",\n+  \"regs.h\",\n+  \"ira.h\",\n+  \"ira-int.h\",\n+  \"gimple-streamer.h\"\n+\n+]\n+\n+exclude_special = [  \"bversion.h\", \"obstack.h\", \"insn-codes.h\", \"hooks.h\" ]\n+\n+# includes is a dictionary indexed by a header files basename.\n+# it consists of a 2 element tuple:\n+# [0] - Name of header file which included this header.\n+# [1] - vector of header file names included by this file.\n+\n+includes = { }\n+\n+# when a header is included multiple times, indexing this dictionary will\n+# return a vector of all the headers which included it.\n+dups = { }\n+\n+# When creating the master list, do not descend into these files for what \n+# they include. Simply put the file itself in the list.  This is primarily\n+# required because the front end files inlcude orders tend to be at odds with\n+# the order of middle end files, and its impossible to synchronize them.\\\n+# They are ordered such that everything resolves properly.\n+exclude_processing = [ \"tree-vectorizer.h\" , \"c-target.h\", \"c-target-def.h\", \"cp-tree.h\", \"c-common.h\", \"c-tree.h\", \"gfortran.h\" ]\n+\n+master_list = list ()\n+# where include file comes from in src\n+h_from = { }\n+\n+# create the master ordering list... this is the desired order of headers\n+def create_master_list (fn, verbose):\n+  if fn not in exclude_processing:\n+    for x in includes[fn][1]:\n+      create_master_list (x, verbose)\n+  if not fn in master_list:\n+    # Don't put diagnostic*.h into the ordering list. It is special since\n+    # various front ends have to set GCC_DIAG_STYLE before including it.\n+    # for each file, we'll tailor where it belongs by looking at the include\n+    # list and determine its position appropriately.\n+    if fn != \"diagnostic.h\" and fn != \"diagnostic-core.h\":\n+      master_list.append (fn)\n+      if (verbose):\n+        print fn + \"      included by: \" + includes[fn][0]\n+\n+\n+\n+def print_dups ():\n+  if dups:\n+    print \"\\nduplicated includes\"\n+  for i in dups:\n+    string =  \"dup : \" + i + \" : \"\n+    string += includes[i][0] \n+    for i2 in dups[i]:\n+      string += \", \"+i2\n+    print string\n+\n+\n+def process_known_dups ():\n+  # rtl.h gets tagged as a duplicate includer for all of coretypes.h, but that\n+  # is really for only generator files\n+  rtl_remove = includes[\"coretypes.h\"][1] + [\"statistics.h\", \"vec.h\"]\n+  if dups:\n+    for i in rtl_remove:\n+      if dups[i] and \"rtl.h\" in dups[i]:\n+        dups[i].remove(\"rtl.h\")\n+      if not dups[i]:\n+        dups.pop (i, None)\n+\n+  # make sure diagnostic.h is the owner of diagnostic-core.h\n+  if includes[\"diagnostic-core.h\"][0] != \"diagnostic.h\":\n+    dups[\"diagnostic-core.h\"].append (includes[\"diagnostic-core.h\"][0])\n+    includes[\"diagnostic-core.h\"] = (\"diagnostic.h\", includes[\"diagnostic-core.h\"][1])\n+\n+# This function scans back thorugh the list of headers which included other\n+# headers to determine what file in HEADER_LIST brought 'HEADER' in.\n+def indirectly_included (header, header_list):\n+  nm = os.path.basename (header)\n+  while nm and includes.get(nm):\n+    if includes[nm][0] in header_list:\n+      return includes[nm][0]\n+    nm = includes[nm][0]\n+\n+  # diagnostic.h and diagnostic-core.h may not show up because we removed them\n+  # from the header list to manually position in an appropriate place. They have\n+  # specific requirements that they need to occur after certain FE files which\n+  # may overide the definition of GCC_DIAG_STYLE.\n+  # Check the dup list for whete they may have been included from and return\n+  # that header.\n+  if header == \"diagnostic-core.h\":\n+    if dups.get(\"diagnostic-core.h\"):\n+      for f in dups[\"diagnostic-core.h\"]:\n+        if f in header_list:\n+          return f\n+    else:\n+      if header in header_list:\n+        return header\n+    # Now check if diagnostics is included indirectly anywhere\n+    header = \"diagnostic.h\"\n+\n+  if header == \"diagnostic.h\":\n+    if dups.get(\"diagnostic.h\"):\n+      for f in dups[\"diagnostic.h\"]:\n+        if f in header_list:\n+          return f\n+    else:\n+      if header in header_list:\n+        return header \n+\n+  return \"\"\n+\n+\n+# This function will take a list of headers from a source file and return \n+# the desired new new order of the canonical headers in DESIRED_ORDER. \n+def get_new_order (src_h, desired_order):\n+  new_order = list ()\n+  for h in desired_order:\n+    if h in master_list:\n+      # Create the list of nested headers which included this file.\n+      iclist = list ()\n+      ib = includes[h][0]\n+      while ib:\n+        iclist.insert(0, ib)\n+        ib = includes[ib][0]\n+      if iclist:\n+        for x in iclist:\n+          # If header is in the source code, and we are allowed to look inside\n+          if x in src_h and x not in exclude_processing:\n+            if x not in new_order and x[:10] != \"diagnostic\" and h not in exclude_special:\n+              new_order.append (x)\n+              break;\n+      else:\n+        if h not in new_order:\n+          new_order.append (h)\n+\n+  f = \"\"\n+  if \"diagnostic.h\" in src_h:\n+    f = \"diagnostic.h\"\n+  elif \"diagnostic-core.h\" in src_h:\n+    f = \"diagnostic-core.h\"\n+\n+ \n+  # If either diagnostic header was directly included in the main file, check to\n+  # see if its already included indirectly, or whether we need to add it to the\n+  # end of the canonically orders headers.\n+  if f:\n+    ii = indirectly_included (f, src_h)\n+    if not ii or ii == f:\n+      new_order.append (f)\n+\n+  return new_order\n+        \n+    \n+\n+# stack of files to process\n+process_stack = list ()\n+\n+def process_one (info):\n+  i = info[0]\n+  owner = info[1]\n+  name = os.path.basename(i)\n+  if os.path.exists (i):\n+    if includes.get(name) == None:\n+      l = find_unique_include_list (i)\n+      # create a list which has just basenames in it\n+      new_list = list ()\n+      for x in l:\n+        new_list.append (os.path.basename (x))\n+        process_stack.append((x, name))\n+      includes[name] = (owner, new_list)\n+    elif owner:\n+      if dups.get(name) == None:\n+        dups[name] = [ owner ]\n+      else:\n+        dups[name].append (owner)\n+  else:\n+    # seed tm.h with options.h since it is a build file and won't be seen. \n+    if not includes.get(name):\n+      if name == \"tm.h\":\n+        includes[name] = (owner, [ \"options.h\" ])\n+        includes[\"options.h\"] = (\"tm.h\", list ())\n+      else:\n+        includes[name] = (owner, list ())\n+\n+\n+show_master = False\n+\n+for arg in sys.argv[1:]:\n+  if arg[0:1] == \"-\":\n+    if arg[0:2] == \"-h\":\n+      usage = True\n+    elif arg[0:2] == \"-i\":\n+      ignore_conditional = True\n+    elif arg[0:2] == \"-v\":\n+      show_master = True\n+    else:\n+      print \"Error: unrecognized option \" + arg\n+  elif os.path.exists(arg):\n+    file_list.append (arg)\n+  else:\n+    print \"Error: file \" + arg + \" Does not exist.\"\n+    usage = True\n+\n+if not file_list and not show_master:\n+  usage = True\n+\n+if not usage and not os.path.exists (\"coretypes.h\"):\n+  usage = True\n+  print \"Error: Must run command in main gcc source directory containing coretypes.h\\n\"\n+\n+# process diagnostic.h first.. it's special since GCC_DIAG_STYLE can be\n+# overridden by languages, but must be done so by a file included BEFORE it.\n+# so make sure it isn't seen as included by one of those files by making it \n+# appear to be included by the src file.\n+process_stack.insert (0, (\"diagnostic.h\", \"\"))\n+\n+# Add the list of files in reverse order since it is processed as a stack later\n+for i in order:\n+  process_stack.insert (0, (i, \"\") )\n+\n+# build up the library of what header files include what other files.\n+while process_stack:\n+  info = process_stack.pop ()\n+  process_one (info)\n+\n+# Now create the master ordering list\n+for i in order:\n+  create_master_list (os.path.basename (i), show_master)\n+\n+# handle warts in the duplicate list\n+process_known_dups ()\n+desired_order = master_list\n+\n+if show_master:\n+  print \" Canonical order of gcc include files: \"\n+  for x in master_list:\n+    print x\n+  print \" \"\n+\n+if usage:\n+  print \"gcc-order-headers [-i] [-v] file1 [filen]\"\n+  print \"    Ensures gcc's headers files are included in a normalized form with\"\n+  print \"    redundant headers removed.  The original files are saved in filename.bak\"\n+  print \"    Outputs a list of files which changed.\"\n+  print \" -i ignore conditional compilation.\"\n+  print \"    Use after examining the file to be sure includes within #ifs are safe\"\n+  print \"    Any headers within conditional sections will be ignored.\"\n+  print \" -v Show the canonical order of known headers\"\n+  sys.exit(0)\n+\n+\n+didnt_do = list ()\n+\n+for fn in file_list:\n+  nest = 0\n+  src_h = list ()\n+  src_line = { }\n+\n+  master_list = list ()\n+\n+  includes = { }\n+  dups = { }\n+\n+  iinfo = process_ii_src (fn)\n+  src = ii_src (iinfo)\n+  include_list = ii_include_list (iinfo)\n+\n+  if ii_include_list_cond (iinfo):\n+    if not ignore_conditional:\n+      print fn + \": Cannot process due to conditional compilation of includes\"\n+      didnt_do.append (fn)\n+      src = list ()\n+\n+  if not src:\n+    continue\n+\n+  process_stack = list ()\n+  # prime the stack with headers in the main ordering list so we get them in\n+  # this order.\n+  for d in order:\n+    if d in include_list:\n+      process_stack.insert (0, (d, \"\"))\n+\n+  for d in include_list:\n+      nm = os.path.basename(d)\n+      src_h.append (nm)\n+      iname = d\n+      iname2 = os.path.dirname (fn) + \"/\" + d\n+      if not os.path.exists (d) and os.path.exists (iname2):\n+        iname = iname2\n+      if iname not in process_stack:\n+        process_stack.insert (0, (iname, \"\"))\n+      src_line[nm] = ii_src_line(iinfo)[d]\n+      if src_line[nm].find(\"/*\") != -1 and src_line[nm].find(\"*/\") == -1:\n+        # this means we have a multi line comment, abort!'\n+        print fn + \": Cannot process due to a multi-line comment :\"\n+        print \"        \" + src_line[nm]\n+        if fn not in didnt_do:\n+          didnt_do.append (fn)\n+        src = list ()\n+\n+  if not src:\n+    continue\n+\n+  # Now create the list of includes as seen by the source file.\n+  while process_stack:\n+    info = process_stack.pop ()\n+    process_one (info)\n+ \n+  for i in include_list:\n+    create_master_list (os.path.basename (i), False)\n+\n+  new_src = list ()\n+  header_added = list ()\n+  new_order = list ()\n+  for line in src:\n+    d = find_pound_include (line, True, True)\n+    if not d or d[-2:] != \".h\":\n+      new_src.append (line)\n+    else:\n+      if d == order[0] and not new_order:\n+        new_order = get_new_order (src_h, desired_order)\n+        for i in new_order:\n+          new_src.append (src_line[i])\n+          # if not seen, add it.\n+          if i not in header_added:\n+            header_added.append (i)\n+      else:\n+        nm = os.path.basename(d)\n+        if nm not in header_added:\n+          iby = indirectly_included (nm, src_h)\n+          if not iby:\n+            new_src.append (line)\n+            header_added.append (nm)\n+\n+  if src != new_src:\n+    os.rename (fn, fn + \".bak\")\n+    fl = open(fn,\"w\")\n+    for line in new_src:\n+      fl.write (line)\n+    fl.close ()\n+    print fn \n+\n+ \n+if didnt_do:\n+  print \"\\n\\n Did not process the following files due to conditional dependencies:\"\n+  str = \"\"\n+  for x in didnt_do:\n+    str += x + \" \"\n+  print str\n+  print \"\\n\"\n+  print \"Please examine to see if they are safe to process, and re-try with -i. \"\n+  print \"Safeness is determined by checking whether any of the reordered headers are\"\n+  print \"within a conditional and could be hauled out of the conditional, thus changing\"\n+  print \"what the compiler will see.\"\n+  print \"Multi-line comments after a #include can also cause failuer, they must be turned\"\n+  print \"into single line comments or removed.\"\n+\n+\n+\n+"}, {"sha": "d4febd7f5710360eb9c6ddc95b0e2db2671407e8", "filename": "contrib/header-tools/graph-header-logs", "status": "added", "additions": 227, "deletions": 0, "changes": 227, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Fgraph-header-logs", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Fgraph-header-logs", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2Fheader-tools%2Fgraph-header-logs?ref=bd94906f987eb065dd6d4e9cadef82f7513d05cd", "patch": "@@ -0,0 +1,227 @@\n+#! /usr/bin/python2\n+import os.path\n+import sys\n+import shlex\n+import re\n+\n+from headerutils import *\n+\n+header_roots = { }\n+extra_edges = list()\n+verbose = False\n+verbosity = 0\n+nodes = list()\n+\n+def unpretty (name):\n+  if name[-2:] == \"_h\":\n+    name = name[:-2] + \".h\"\n+  return name.replace(\"_\", \"-\")\n+\n+def pretty_name (name):\n+  name = os.path.basename (name)\n+  return name.replace(\".\",\"_\").replace(\"-\",\"_\").replace(\"/\",\"_\").replace(\"+\",\"_\");\n+\n+depstring = (\"In file included from\", \"                 from\")\n+\n+# indentation indicates nesting levels of included files\n+ignore = [ \"coretypes_h\",\n+             \"machmode_h\",\n+             \"signop_h\",\n+             \"wide_int_h\",\n+             \"double_int_h\",\n+             \"real_h\",\n+             \"fixed_value_h\",\n+             \"hash_table_h\",\n+               \"statistics_h\",\n+               \"ggc_h\",\n+               \"vec_h\",\n+               \"hashtab_h\",\n+               \"inchash_h\",\n+               \"mem_stats_traits_h\",\n+               \"hash_map_traits_h\",\n+               \"mem_stats_h\",\n+               \"hash_map_h\",\n+             \"hash_set_h\",\n+             \"input_h\",\n+               \"line_map_h\",\n+             \"is_a_h\",\n+           \"system_h\",\n+           \"config_h\" ]\n+\n+def process_log_file (header, logfile):\n+  if header_roots.get (header) != None:\n+    print \"Error: already processed log file: \" + header + \".log\"\n+    return\n+  hname = pretty_name (header)\n+  header_roots[hname] = { }\n+  \n+  sline = list();\n+  incfrom = list()\n+  newinc = True\n+  for line in logfile:\n+    if len (line) > 21 and line[:21] in depstring:\n+      if newinc:\n+        incfrom = list()\n+        newinc = False\n+      fn = re.findall(ur\".*/(.*?):\", line)\n+      if len(fn) != 1:\n+        continue\n+      if fn[0][-2:] != \".h\":\n+        continue\n+      n = pretty_name (fn[0])\n+      if n not in ignore:\n+        incfrom.append (n)\n+      continue\n+    newinc = True\n+    note = re.findall (ur\"^.*note: (.*)\", line)\n+    if len(note) > 0:\n+      sline.append ((\"note\", note[0]))\n+    else:\n+      err_msg = re.findall (ur\"^.*: error: (.*)\", line)\n+      if len(err_msg) == 1:\n+        msg = err_msg[0]\n+        if (len (re.findall(\"error: forward declaration\", line))) != 0:\n+          continue\n+        path = re.findall (ur\"^(.*?):.*error: \", line)\n+        if len(path) != 1:\n+          continue\n+        if path[0][-2:] != \".h\":\n+          continue\n+        fname = pretty_name (path[0])\n+        if fname in ignore or fname[0:3] == \"gt_\":\n+          continue\n+        sline.append ((\"error\", msg, fname, incfrom))\n+\n+  print str(len(sline)) + \" lines to process\"\n+  lastline = \"note\"\n+  for line in sline:\n+    if line[0] != \"note\" and lastline[0] == \"error\":\n+      fname = lastline[2]\n+      msg = lastline[1]\n+      incfrom = lastline[3]\n+      string = \"\"\n+      ofname = fname\n+      if len(incfrom) != 0:\n+        for t in incfrom:\n+          string = string + t + \" : \"\n+          ee = (fname, t)\n+          if ee not in extra_edges:\n+            extra_edges.append (ee)\n+          fname = t\n+          print string\n+\n+      if hname not in nodes:\n+        nodes.append(hname)\n+      if fname not in nodes:\n+        nodes.append (ofname)\n+      for y in incfrom:\n+        if y not in nodes:\n+          nodes.append (y)\n+\n+\n+      if header_roots[hname].get(fname) == None:\n+        header_roots[hname][fname] = list()\n+      if msg not in header_roots[hname][fname]:\n+        print string + ofname + \" : \" +msg\n+        header_roots[hname][fname].append (msg)\n+    lastline = line;\n+\n+\n+dotname = \"graph.dot\"\n+graphname = \"graph.png\"\n+\n+\n+def build_dot_file (file_list):\n+  output = open(dotname, \"w\")\n+  output.write (\"digraph incweb {\\n\");\n+  for x in file_list:\n+    if os.path.exists (x) and x[-4:] == \".log\":\n+      header =  x[:-4]\n+      logfile = open(x).read().splitlines()\n+      process_log_file (header, logfile)\n+    elif os.path.exists (x + \".log\"):\n+      logfile = open(x + \".log\").read().splitlines()\n+      process_log_file (x, logfile)\n+\n+  for n in nodes:\n+    fn = unpretty(n)\n+    label = n + \" [ label = \\\"\" + fn  + \"\\\" ];\"\n+    output.write (label + \"\\n\")\n+    if os.path.exists (fn):\n+      h = open(fn).read().splitlines()\n+      for l in h:\n+        t = find_pound_include (l, True, False)\n+        if t != \"\":\n+          t = pretty_name (t)\n+          if t in ignore or t[-2:] != \"_h\":\n+            continue\n+          if t not in nodes:\n+            nodes.append (t)\n+          ee = (t, n)\n+          if ee not in extra_edges:\n+            extra_edges.append (ee)\n+\n+  depcount = list()\n+  for h in header_roots:\n+    for dep in header_roots[h]:\n+      label = \" [ label = \"+ str(len(header_roots[h][dep])) + \" ];\"\n+      string = h + \" -> \" + dep + label\n+      output.write (string + \"\\n\");\n+      if verbose:\n+        depcount.append ((h, dep, len(header_roots[h][dep])))\n+\n+  for ee in extra_edges:\n+    string = ee[0] + \" -> \" + ee[1] + \"[ color=red ];\"\n+    output.write (string + \"\\n\");\n+\n+  \n+  if verbose:\n+    depcount.sort(key=lambda tup:tup[2])\n+    for x in depcount:\n+      print \" (\"+str(x[2])+ \") : \" + x[0] + \" -> \" + x[1]\n+      if (x[2] <= verbosity):\n+        for l in header_roots[x[0]][x[1]]:\n+          print \"            \" + l\n+\n+  output.write (\"}\\n\");\n+\n+\n+files = list()\n+dohelp = False\n+edge_thresh = 0\n+for arg in sys.argv[1:]:\n+  if arg[0:2] == \"-o\":\n+    dotname = arg[2:]+\".dot\"\n+    graphname = arg[2:]+\".png\"\n+  elif arg[0:2] == \"-h\":\n+    dohelp = True\n+  elif arg[0:2] == \"-v\":\n+    verbose = True\n+    if len(arg) > 2:\n+      verbosity = int (arg[2:])\n+      if (verbosity == 9):\n+        verbosity = 9999\n+  elif arg[0:1] == \"-\":\n+    print \"Unrecognized option \" + arg\n+    dohelp = True\n+  else:\n+    files.append (arg)\n+    \n+if len(sys.argv) == 1:\n+  dohelp = True\n+\n+if dohelp:\n+  print \"Parses the log files from the reduce-headers tool to generate\"\n+  print \"dependency graphs for the include web for specified files.\"\n+  print \"Usage:  [-nnum] [-h] [-v[n]] [-ooutput] file1 [[file2] ... [filen]]\"\n+  print \"       -ooutput : Specifies output to output.dot and output.png\"\n+  print \"                  Defaults to 'graph.dot and graph.png\"\n+  print \"       -vn : verbose mode, shows the number of connections, and if n\"\n+  print \"             is specified, show the messages if # < n. 9 is infinity\"\n+  print \"       -h : help\"\n+else:\n+  print files\n+  build_dot_file (files)\n+  os.system (\"dot -Tpng \" + dotname + \" -o\" + graphname)\n+\n+"}, {"sha": "47576a177dea960514eb425b7ac3c00f30611711", "filename": "contrib/header-tools/graph-include-web", "status": "added", "additions": 122, "deletions": 0, "changes": 122, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Fgraph-include-web", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Fgraph-include-web", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2Fheader-tools%2Fgraph-include-web?ref=bd94906f987eb065dd6d4e9cadef82f7513d05cd", "patch": "@@ -0,0 +1,122 @@\n+#! /usr/bin/python2\n+import os.path\n+import sys\n+import shlex\n+import re\n+\n+from headerutils import *\n+\n+def pretty_name (name):\n+  return name.replace(\".\",\"_\").replace(\"-\",\"_\").replace(\"/\",\"_\").replace(\"+\",\"_\");\n+\n+\n+include_files = list()\n+edges = 0\n+one_c = False\n+clink = list()\n+noterm = False\n+\n+def build_inclist (output, filen):\n+  global edges\n+  global one_c\n+  global clink\n+  global noterm\n+  inc = build_include_list (filen)\n+  if one_c and filen[-2:] == \".c\":\n+    pn = \"all_c\"\n+  else:\n+    pn = pretty_name(filen)\n+  for nm in inc:\n+    if pn == \"all_c\":\n+      if nm not in clink:\n+        if len(build_include_list(nm)) != 0 or not noterm:\n+          output.write (pretty_name(nm) + \" -> \" + pn + \";\\n\")\n+          edges = edges + 1\n+          if nm not in include_files:\n+            include_files.append(nm)\n+        clink.append (nm)\n+    else:\n+      output.write (pretty_name(nm) + \" -> \" + pn + \";\\n\")\n+      edges = edges + 1\n+      if nm not in include_files:\n+        include_files.append(nm)\n+  return len(inc) == 0\n+\n+dotname = \"graph.dot\"\n+graphname = \"graph.png\"\n+\n+def build_dot_file (file_list):\n+  global one_c\n+  output = open(dotname, \"w\")\n+  output.write (\"digraph incweb {\\n\");\n+  if one_c:\n+    output.write (\"all_c [shape=box];\\n\");\n+  for x in file_list:\n+    if x[-2:] == \".h\":\n+      include_files.append (x)\n+    elif os.path.exists (x):\n+      build_inclist (output, x)\n+      if not one_c:\n+        output.write (pretty_name (x) + \"[shape=box];\\n\")\n+\n+  for x in include_files:\n+    term = build_inclist (output, x)\n+    if term:\n+      output.write (pretty_name(x) + \" [style=filled];\\n\")\n+\n+  output.write (\"}\\n\");\n+\n+\n+files = list()\n+dohelp = False\n+edge_thresh = 0\n+for arg in sys.argv[1:]:\n+  if arg[0:2] == \"-o\":\n+    dotname = arg[2:]+\".dot\"\n+    graphname = arg[2:]+\".png\"\n+  elif arg[0:2] == \"-h\":\n+    dohelp = True\n+  elif arg[0:2] == \"-a\":\n+    one_c = True\n+    if arg[0:3] == \"-at\":\n+      noterm = True\n+  elif arg[0:2] == \"-f\":\n+    if not os.path.exists (arg[2:]):\n+      print \"Option \" + arg +\" doesn't specify a proper file\"\n+      dohelp = True\n+    else:\n+      sfile = open (arg[2:], \"r\")\n+      srcdata = sfile.readlines()\n+      sfile.close()\n+      for x in srcdata:\n+        files.append(x.rstrip())\n+  elif arg[0:2] == \"-n\":\n+    edge_thresh = int (arg[2:])\n+  elif arg[0:1] == \"-\":\n+    print \"Unrecognized option \" + arg\n+    dohelp = True\n+  else:\n+    files.append (arg)\n+    \n+if len(sys.argv) == 1:\n+  dohelp = True\n+\n+if dohelp:\n+  print \"Generates a graph of the include web for specified files.\"\n+  print \"Usage:  [-finput_file] [-h] [-ooutput] [file1 ... [filen]]\"\n+  print \"  -finput_file : Input file containing a list of files to process.\"\n+  print \"  -ooutput : Specifies output to output.dot and output.png.\"\n+  print \"             defaults to graph.dot and graph.png.\"\n+  print \"  -nnum : Specifies the # of edges beyond which sfdp is invoked. def=0.\"\n+  print \"  -a : Aggregate all .c files to 1 file.  Shows only include web.\"\n+  print \"  -at : Aggregate, but don't include terminal.h to .c links.\"\n+  print \"  -h : Print this help.\"\n+else:\n+  print files\n+  build_dot_file (files)\n+  if edges > edge_thresh:\n+    os.system (\"sfdp -Tpng \" + dotname + \" -o\" + graphname)\n+  else:\n+    os.system (\"dot -Tpng \" + dotname + \" -o\" + graphname)\n+\n+"}, {"sha": "95c47fb4b6956d23aea46a91c251fd68e8af27b9", "filename": "contrib/header-tools/headerutils.py", "status": "added", "additions": 554, "deletions": 0, "changes": 554, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Fheaderutils.py", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Fheaderutils.py", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2Fheader-tools%2Fheaderutils.py?ref=bd94906f987eb065dd6d4e9cadef82f7513d05cd", "patch": "@@ -0,0 +1,554 @@\n+#! /usr/bin/python2\n+import os.path\n+import sys\n+import shlex\n+import re\n+import subprocess\n+import shutil\n+import pickle\n+\n+import multiprocessing \n+\n+def find_pound_include (line, use_outside, use_slash):\n+  inc = re.findall (ur\"^\\s*#\\s*include\\s*\\\"(.+?)\\\"\", line)\n+  if len(inc) == 1:\n+    nm = inc[0]\n+    if use_outside or os.path.exists (nm):\n+      if use_slash or '/' not in nm:\n+        return nm\n+  return \"\"\n+\n+def find_system_include (line):\n+  inc = re.findall (ur\"^\\s*#\\s*include\\s*<(.+?)>\", line)\n+  if len(inc) == 1:\n+    return inc[0]\n+  return \"\"\n+  \n+def find_pound_define (line):\n+  inc = re.findall (ur\"^\\s*#\\s*define ([A-Za-z0-9_]+)\", line)\n+  if len(inc) != 0:\n+    if len(inc) > 1:\n+      print \"What? more than 1 match in #define??\"\n+      print inc\n+      sys.exit(5)\n+    return inc[0];\n+  return \"\"\n+\n+def is_pound_if (line):\n+  inc = re.findall (\"^\\s*#\\s*if\\s\", line)\n+  if not inc:\n+    inc = re.findall (\"^\\s*#\\s*if[n]?def\\s\", line)\n+  if inc:\n+    return True\n+  return False\n+\n+def is_pound_endif (line):\n+  inc = re.findall (\"^\\s*#\\s*endif\", line)\n+  if inc:\n+    return True\n+  return False\n+\n+def find_pound_if (line):\n+  inc = re.findall (ur\"^\\s*#\\s*if\\s+(.*)\", line)\n+  if len(inc) == 0:\n+    inc = re.findall (ur\"^\\s*#\\s*elif\\s+(.*)\", line)\n+  if len(inc) > 0:\n+    inc2 = re.findall (ur\"defined\\s*\\((.+?)\\)\", inc[0])\n+    inc3 = re.findall (ur\"defined\\s+([a-zA-Z0-9_]+)\", inc[0])\n+    for yy in inc3:\n+      inc2.append (yy)\n+    return inc2\n+  else:\n+    inc = re.findall (ur\"^\\s*#\\s*ifdef\\s(.*)\", line)\n+    if len(inc) == 0:\n+      inc = re.findall (ur\"^\\s*#\\s*ifndef\\s(.*)\", line)\n+    if len(inc) > 0:\n+      inc2 = re.findall (\"[A-Za-z_][A-Za-z_0-9]*\", inc[0])\n+      return inc2\n+  if len(inc) == 0:\n+    return list ()\n+  print \"WTF. more than one line returned for find_pound_if\"\n+  print inc\n+  sys.exit(5)\n+\n+\n+# IINFO - this is a vector of include information. It consists of 7 elements.\n+# [0] - base name of the file\n+# [1] - path leading to this file.\n+# [2] - orderd list of all headers directly included by this file.\n+# [3] - Ordered list of any headers included within condionally compiled code.\n+#       headers files are expected to have all includes one level deep due to\n+#       the omnipresent guards at the top of the file.  \n+# [4] - List of all macros which are consumed (used) within this file.\n+# [5] - list of all macros which may be defined in this file.\n+# [6] - The source code for this file, if cached.\n+# [7] - line number info for any headers in the source file.  Indexed by base\n+#       name, returning the line the include is on.\n+\n+empty_iinfo =  (\"\", \"\", list(), list(), list(), list(), list())\n+\n+# This function will process a file and extract interesting information.\n+# DO_MACROS indicates whether macros defined and used should be recorded.\n+# KEEP_SRC indicates the source for the file should be cached.\n+def process_include_info (filen, do_macros, keep_src):\n+  header = False\n+  if not os.path.exists (filen):\n+    return empty_iinfo\n+\n+  sfile = open (filen, \"r\");\n+  data = sfile.readlines()\n+  sfile.close()\n+\n+  # Ignore the initial #ifdef HEADER_H in header files\n+  if filen[-2:] == \".h\":\n+    nest = -1\n+    header = True\n+  else:\n+    nest = 0\n+\n+  macout = list ()\n+  macin = list()\n+  incl = list()\n+  cond_incl = list()\n+  src_line = { }\n+  guard = \"\"\n+\n+  for line in (data):\n+    if is_pound_if (line):\n+      nest += 1\n+    elif is_pound_endif (line):\n+      nest -= 1\n+\n+    nm = find_pound_include (line, True, True)\n+    if nm != \"\" and nm not in incl and nm[-2:] == \".h\":\n+      incl.append (nm)\n+      if nest > 0:\n+        cond_incl.append (nm)\n+      if keep_src:\n+        src_line[nm] = line\n+      continue\n+\n+    if do_macros:\n+      d = find_pound_define (line)\n+      if d:\n+        if d not in macout:\n+          macout.append (d);\n+          continue\n+\n+      d = find_pound_if (line)\n+      if d:\n+        # The first #if in a header file should be the guard\n+        if header and len (d) == 1 and guard == \"\":\n+          if d[0][-2:] == \"_H\":\n+            guard = d\n+          else:\n+            guard = \"Guess there was no guard...\"\n+        else:\n+          for mac in d:\n+            if mac != \"defined\" and mac not in macin:\n+              macin.append (mac);\n+\n+  if not keep_src:\n+    data = list()\n+\n+  return (os.path.basename (filen), os.path.dirname (filen), incl, cond_incl,\n+          macin, macout, data, src_line)\n+\n+# Extract header info, but no macros or source code.\n+def process_ii (filen):\n+  return process_include_info (filen, False, False)\n+\n+# Extract header information, and collect macro information.\n+def process_ii_macro (filen):\n+  return process_include_info (filen, True, False)\n+\n+# Extract header information, cache the source lines.\n+def process_ii_src (filen):\n+  return process_include_info (filen, False, True)\n+\n+# Extract header information, coolewc macro info and cache the source lines.\n+def process_ii_macro_src (filen):\n+  return process_include_info (filen, True, True)\n+\n+\n+def ii_base (iinfo):\n+  return iinfo[0]\n+\n+def ii_path (iinfo):\n+  return iinfo[1]\n+\n+def ii_include_list (iinfo):\n+  return iinfo[2]\n+\n+def ii_include_list_cond (iinfo):\n+  return iinfo[3]\n+\n+def ii_include_list_non_cond (iinfo):\n+  l = ii_include_list (iinfo)\n+  for n in ii_include_list_cond (iinfo):\n+    l.remove (n)\n+  return l\n+\n+def ii_macro_consume (iinfo):\n+  return iinfo[4]\n+  \n+def ii_macro_define (iinfo):\n+  return iinfo[5]\n+\n+def ii_src (iinfo):\n+  return iinfo[6]\n+\n+def ii_src_line (iinfo):\n+  return iinfo[7]\n+\n+def ii_read (fname):\n+  f = open (fname, 'rb')\n+  incl = pickle.load (f)\n+  consumes = pickle.load (f)\n+  defines = pickle.load (f)\n+  obj = (fname,fname,incl,list(), list(), consumes, defines, list(), list())\n+  return obj\n+\n+def ii_write (fname, obj):\n+  f = open (fname, 'wb')\n+  pickle.dump (obj[2], f)\n+  pickle.dump (obj[4], f)\n+  pickle.dump (obj[5], f)\n+  f.close ()\n+\n+# execute a system command which returns file names\n+def execute_command (command):\n+  files = list()\n+  f = os.popen (command)\n+  for x in f:\n+    if x[0:2] == \"./\":\n+      fn = x.rstrip()[2:]\n+    else:\n+      fn = x.rstrip()\n+    files.append(fn)\n+  return files\n+\n+# Try to locate a build directory from PATH\n+def find_gcc_bld_dir (path):\n+  blddir = \"\"\n+  # Look for blddir/gcc/tm.h\n+  command = \"find \" + path + \" -mindepth 2 -maxdepth 3 -name tm.h\"\n+  files = execute_command (command)\n+  for y in files:\n+    p = os.path.dirname (y)\n+    if os.path.basename (p) == \"gcc\":\n+      blddir = p\n+      break\n+  # If not found, try looking a bit deeper\n+  # Dont look this deep initially because a lot of cross target builds may show\n+  # up in the list before a native build... but those are better than nothing.\n+  if not blddir:\n+    command = \"find \" + path + \" -mindepth 3 -maxdepth 5 -name tm.h\"\n+    files = execute_command (command)\n+    for y in files:\n+      p = os.path.dirname (y)\n+      if os.path.basename (p) == \"gcc\":\n+\tblddir = p\n+\tbreak\n+\n+  return blddir\n+\n+\n+# Find files matching pattern NAME, return in a list.\n+# CURRENT is True if you want to include the current directory\n+# DEEPER is True if you want to search 3 levels below the current directory\n+# any files with testsuite diurectories are ignored\n+\n+def find_gcc_files (name, current, deeper):\n+  files = list()\n+  command = \"\"\n+  if current:\n+    if not deeper:\n+      command = \"find -maxdepth 1 -name \" + name + \" -not -path \\\"./testsuite/*\\\"\"\n+    else:\n+      command = \"find -maxdepth 4 -name \" + name + \" -not -path \\\"./testsuite/*\\\"\"\n+  else:\n+    if deeper:\n+      command = \"find -maxdepth 4 -mindepth 2 -name \" + name + \" -not -path \\\"./testsuite/*\\\"\"\n+\n+  if command != \"\":\n+    files = execute_command (command)\n+\n+  return files\n+\n+# find the list of unique include names found in a file.\n+def find_unique_include_list_src (data):\n+  found = list ()\n+  for line in data:\n+    d = find_pound_include (line, True, True)\n+    if d and d not in found and d[-2:] == \".h\":\n+      found.append (d)\n+  return found\n+\n+# find the list of unique include names found in a file.\n+def find_unique_include_list (filen):\n+  data = open (filen).read().splitlines()\n+  return find_unique_include_list_src (data)\n+\n+\n+# Create the macin, macout, and incl vectors for a file FILEN.\n+# macin are the macros that are used in #if* conditional expressions\n+# macout are the macros which are #defined\n+# incl is the list of incluide files encountered\n+# returned as a tuple of the filename followed by the triplet of lists\n+# (filen, macin, macout, incl)\n+\n+def create_macro_in_out (filen):\n+  sfile = open (filen, \"r\");\n+  data = sfile.readlines()\n+  sfile.close()\n+\n+  macout = list ()\n+  macin = list()\n+  incl = list()\n+\n+  for line in (data):\n+    d = find_pound_define (line)\n+    if d != \"\":\n+      if d not in macout:\n+        macout.append (d);\n+      continue\n+\n+    d = find_pound_if (line)\n+    if len(d) != 0:\n+      for mac in d:\n+        if mac != \"defined\" and mac not in macin:\n+          macin.append (mac);\n+      continue\n+\n+    nm = find_pound_include (line, True, True)\n+    if nm != \"\" and nm not in incl:\n+      incl.append (nm)\n+\n+  return (filen, macin, macout, incl)\n+\n+# create the macro information for filen, and create .macin, .macout, and .incl\n+# files.  Return the created macro tuple.\n+def create_include_data_files (filen):\n+\n+  macros = create_macro_in_out (filen)\n+  depends = macros[1]\n+  defines = macros[2]\n+  incls = macros[3]\n+  \n+  disp_message = filen\n+  if len (defines) > 0:\n+    disp_message = disp_message + \" \" + str(len (defines)) + \" #defines\"\n+  dfile = open (filen + \".macout\", \"w\")\n+  for x in defines:\n+    dfile.write (x + \"\\n\")\n+  dfile.close ()\n+\n+  if len (depends) > 0:\n+    disp_message = disp_message + \" \" + str(len (depends)) + \" #if dependencies\"\n+  dfile = open (filen + \".macin\", \"w\")\n+  for x in depends:\n+    dfile.write (x + \"\\n\")\n+  dfile.close ()\n+\n+  if len (incls) > 0:\n+    disp_message = disp_message + \" \" + str(len (incls)) + \" #includes\"\n+  dfile = open (filen + \".incl\", \"w\")\n+  for x in incls:\n+    dfile.write (x + \"\\n\")\n+  dfile.close ()\n+\n+  return macros\n+\n+\n+\n+# extract data for include file name_h and enter it into the dictionary.\n+# this does not change once read in.  use_requires is True if you want to \n+# prime the values with already created .requires and .provides files.\n+def get_include_data (name_h, use_requires):\n+  macin = list()\n+  macout = list()\n+  incl = list ()\n+  if use_requires and os.path.exists (name_h + \".requires\"):\n+    macin = open (name_h + \".requires\").read().splitlines()\n+  elif os.path.exists (name_h + \".macin\"):\n+    macin = open (name_h + \".macin\").read().splitlines()\n+\n+  if use_requires and os.path.exists (name_h + \".provides\"):\n+    macout  = open (name_h + \".provides\").read().splitlines()\n+  elif os.path.exists (name_h + \".macout\"):\n+    macout  = open (name_h + \".macout\").read().splitlines()\n+\n+  if os.path.exists (name_h + \".incl\"):\n+    incl = open (name_h + \".incl\").read().splitlines()\n+\n+  if len(macin) == 0 and len(macout) == 0 and len(incl) == 0:\n+    return ()\n+  data = ( name_h, macin, macout, incl )\n+  return data\n+  \n+# find FIND in src, and replace it with the list of headers in REPLACE.\n+# Remove any duplicates of FIND in REPLACE, and if some of the REPLACE\n+# headers occur earlier in the include chain, leave them.\n+# Return the new SRC only if anything changed.\n+def find_replace_include (find, replace, src):\n+  res = list()\n+  seen = { }\n+  anything = False\n+  for line in src:\n+    inc = find_pound_include (line, True, True)\n+    if inc == find:\n+      for y in replace:\n+        if seen.get(y) == None:\n+          res.append(\"#include \\\"\"+y+\"\\\"\\n\")\n+          seen[y] = True\n+          if y != find:\n+            anything = True\n+# if find isnt in the replacement list, then we are deleting FIND, so changes.\n+      if find not in replace:\n+        anything = True\n+    else:\n+      if inc in replace:\n+        if seen.get(inc) == None:\n+          res.append (line)\n+          seen[inc] = True\n+      else:\n+        res.append (line)\n+\n+  if (anything):\n+    return res\n+  else:\n+    return list()\n+      \n+\n+# pass in a require and provide dictionary to be read in.\n+def read_require_provides (require, provide):\n+  if not os.path.exists (\"require-provide.master\"):\n+    print \"require-provide.master file is not available. please run data collection.\"\n+    sys.exit(1)\n+  incl_list = open(\"require-provide.master\").read().splitlines()\n+  for f in incl_list:\n+    if os.path.exists (f+\".requires\"):\n+      require[os.path.basename (f)] = open (f + \".requires\").read().splitlines()\n+    else:\n+      require[os.path.basename (f)] = list ()\n+    if os.path.exists (f+\".provides\"):\n+      provide[os.path.basename (f)] = open (f + \".provides\").read().splitlines()\n+    else:\n+      provide [os.path.basename (f)] = list ()\n+\n+   \n+def build_include_list (filen):\n+  include_files = list()\n+  sfile = open (filen, \"r\")\n+  data = sfile.readlines()\n+  sfile.close()\n+  for line in data:\n+    nm = find_pound_include (line, False, False)\n+    if nm != \"\" and nm[-2:] == \".h\":\n+      if nm not in include_files:\n+        include_files.append(nm)\n+  return include_files\n+ \n+def build_reverse_include_list (filen):\n+  include_files = list()\n+  sfile = open (filen, \"r\")\n+  data = sfile.readlines()\n+  sfile.close()\n+  for line in reversed(data):\n+    nm = find_pound_include (line, False, False)\n+    if nm != \"\":\n+      if nm not in include_files:\n+        include_files.append(nm)\n+  return include_files\n+     \n+# Get compilation return code, and compensate for a warning that we want to \n+# consider an error when it comes to inlined templates.\n+def get_make_rc (rc, output):\n+  rc = rc % 1280\n+  if rc == 0:\n+    # This is not considered an error during compilation of an individual file,\n+    # but it will cause an error during link if it isn't defined.  If this\n+    # warning is seen during compiling a file, make it a build error so we \n+    # don't remove the header.\n+    h = re.findall (\"warning: inline function.*used but never defined\", output)\n+    if len(h) != 0:\n+      rc = 1\n+  return rc;\n+\n+def get_make_output (build_dir, make_opt):\n+  devnull = open('/dev/null', 'w')\n+  at_a_time = multiprocessing.cpu_count() * 2\n+  make = \"make -j\"+str(at_a_time)+ \" \"\n+  if build_dir != \"\":\n+    command = \"cd \" + build_dir +\"; \" + make + make_opt\n+  else:\n+    command = make + make_opt\n+  process = subprocess.Popen(command, stdout=devnull, stderr=subprocess.PIPE, shell=True)\n+  output = process.communicate();\n+  rc = get_make_rc (process.returncode, output[1])\n+  return (rc , output[1])\n+\n+def spawn_makes (command_list):\n+  devnull = open('/dev/null', 'w')\n+  rc = (0,\"\", \"\")\n+  proc_res = list()\n+  text = \"  Trying target builds : \"\n+  for command_pair in command_list:\n+    tname = command_pair[0]\n+    command = command_pair[1]\n+    text += tname + \", \"\n+    c = subprocess.Popen(command, bufsize=-1, stdout=devnull, stderr=subprocess.PIPE, shell=True)\n+    proc_res.append ((c, tname))\n+\n+  print text[:-2]\n+\n+  for p in proc_res:\n+    output = p[0].communicate()\n+    ret = (get_make_rc (p[0].returncode, output[1]), output[1], p[1])\n+    if (ret[0] != 0):\n+      # Just record the first one.\n+      if rc[0] == 0:\n+        rc = ret;\n+  return rc\n+\n+def get_make_output_parallel (targ_list, make_opt, at_a_time):\n+  command = list()\n+  targname = list()\n+  if at_a_time == 0:\n+    at_a_time = multiprocessing.cpu_count() * 2\n+  proc_res = [0] * at_a_time\n+  for x in targ_list:\n+    if make_opt[-2:] == \".o\":\n+      s = \"cd \" + x[1] + \"/gcc/; make \" + make_opt\n+    else:\n+      s = \"cd \" + x[1] +\"; make \" + make_opt\n+    command.append ((x[0],s))\n+\n+  num = len(command) \n+  rc = (0,\"\", \"\")\n+  loops = num // at_a_time\n+  \n+  if (loops > 0):\n+    for idx in range (loops):\n+      ret = spawn_makes (command[idx*at_a_time:(idx+1)*at_a_time])\n+      if ret[0] != 0:\n+        rc = ret\n+        break\n+\n+  if (rc[0] == 0):\n+    leftover = num % at_a_time\n+    if (leftover > 0):\n+      ret = spawn_makes (command[-leftover:])\n+      if ret[0] != 0:\n+        rc = ret\n+\n+  return rc\n+\n+\n+def readwholefile (src_file):\n+  sfile = open (src_file, \"r\")\n+  src_data = sfile.readlines()\n+  sfile.close()\n+  return src_data\n+"}, {"sha": "9947fee6b2b924a4aed2ef1a20f1ad85c27c980a", "filename": "contrib/header-tools/included-by", "status": "added", "additions": 112, "deletions": 0, "changes": 112, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Fincluded-by", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Fincluded-by", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2Fheader-tools%2Fincluded-by?ref=bd94906f987eb065dd6d4e9cadef82f7513d05cd", "patch": "@@ -0,0 +1,112 @@\n+#! /usr/bin/python2\n+import os.path\n+import sys\n+import shlex\n+import re\n+\n+from headerutils import *\n+\n+\n+\n+usage = False\n+src = list()\n+flist = { }\n+process_h = False\n+process_c = False\n+verbose = False\n+level = 0\n+match_all = False\n+num_match = 1\n+\n+file_list = list()\n+current = True\n+deeper = True\n+scanfiles = True\n+for x in sys.argv[1:]:\n+  if x[0:2] == \"-h\":\n+    usage = True\n+  elif x[0:2] == \"-i\":\n+    process_h = True\n+  elif x[0:2] == \"-s\" or x[0:2] == \"-c\":\n+    process_c = True\n+  elif x[0:2] == \"-v\":\n+    verbose = True\n+  elif x[0:2] == \"-a\":\n+    match_all = True\n+  elif x[0:2] == \"-n\":\n+    num_match = int(x[2:])\n+  elif x[0:2] == \"-1\":\n+    deeper = False\n+  elif x[0:2] == \"-2\":\n+    current = False\n+  elif x[0:2] == \"-f\":\n+    file_list = open (x[2:]).read().splitlines()\n+    scanfiles = False\n+  elif x[0] == \"-\":\n+    print \"Error: Unknown option \" + x\n+    usage = True\n+  else:\n+    src.append (x)\n+\n+if match_all:\n+  num_match = len (src)\n+\n+if not process_h and not process_c:\n+  process_h = True\n+  process_c = True\n+\n+if len(src) == 0:\n+  usage = True\n+\n+if not usage:\n+  if scanfiles:\n+    if process_h:\n+      file_list = find_gcc_files (\"\\*.h\", current, deeper)\n+    if process_c:\n+      file_list = file_list + find_gcc_files (\"\\*.c\", current, deeper)\n+      file_list = file_list + find_gcc_files (\"\\*.cc\", current, deeper)\n+  else:\n+    newlist = list()\n+    for x in file_list:\n+      if process_h and x[-2:] == \".h\":\n+        newlist.append (x)\n+      elif process_c and (x[-2:] == \".c\" or x[-3:] == \".cc\"):\n+        newlist.append (x)\n+    file_list = newlist;\n+     \n+  file_list.sort()\n+  for fn in file_list:\n+    found = find_unique_include_list (fn)\n+    careabout = list()\n+    output = \"\"\n+    for inc in found:\n+      if inc in src:\n+        careabout.append (inc)\n+        if output == \"\":\n+          output = fn\n+        if verbose:\n+          output = output + \" [\" + inc +\"]\"\n+    if len (careabout) < num_match:\n+        output = \"\"\n+    if output != \"\":\n+      print output\n+else:\n+  print \"included-by [-h] [-i] [-c] [-v] [-a] [-nx] file1 [file2] ... [filen]\"\n+  print \"find the list of all files in subdirectories that include any of \"\n+  print \"the listed files. processed to a depth of 3 subdirs\"\n+  print \" -h  : Show this message\"\n+  print \" -i  : process only header files (*.h) for #include\"\n+  print \" -c  : process only source files (*.c *.cc) for #include\"\n+  print \"       If nothing is specified, defaults to -i -c\"\n+  print \" -s  : Same as -c.\"\n+  print \" -v  : Show which include(s) were found\"\n+  print \" -nx : Only list files which have at least x different matches. Default = 1\"\n+  print \" -a  : Show only files which all listed files are included\"\n+  print \"       This is equivilent to -nT where T == # of items in list\"\n+  print \" -flistfile  : Show only files contained in the list of files\"\n+\n+ \n+\n+\n+\n+"}, {"sha": "e4f4d7b123dccaf9669e555870a6af89ddf85707", "filename": "contrib/header-tools/reduce-headers", "status": "added", "additions": 596, "deletions": 0, "changes": 596, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Freduce-headers", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Freduce-headers", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2Fheader-tools%2Freduce-headers?ref=bd94906f987eb065dd6d4e9cadef82f7513d05cd", "patch": "@@ -0,0 +1,596 @@\n+#! /usr/bin/python2\n+import os.path\n+import sys\n+import shlex\n+import re\n+import tempfile\n+import copy\n+\n+from headerutils import *\n+\n+requires = { }\n+provides = { }\n+\n+no_remove = [ \"system.h\", \"coretypes.h\", \"config.h\" , \"bconfig.h\", \"backend.h\" ]\n+\n+# These targets are the ones which provide \"coverage\".  Typically, if any\n+# target is going to fail compilation, it's one of these.  This was determined\n+# during the initial runs of reduce-headers... On a full set of target builds,\n+# every failure which occured was triggered by one of these.  \n+# This list is used during target-list construction simply to put any of these\n+# *first* in the candidate list, increasing the probability that a failure is \n+# found quickly.\n+target_priority = [\n+    \"aarch64-linux-gnu\",\n+    \"arm-netbsdelf\",\n+    \"avr-rtems\",\n+    \"c6x-elf\",\n+    \"epiphany-elf\",\n+    \"hppa2.0-hpux10.1\",\n+    \"i686-mingw32crt\",\n+    \"i686-pc-msdosdjgpp\",\n+    \"mipsel-elf\",\n+    \"powerpc-eabisimaltivec\",\n+    \"rs6000-ibm-aix5.1.0\",\n+    \"sh-superh-elf\",\n+    \"sparc64-elf\",\n+    \"spu-elf\"\n+]\n+\n+\n+target_dir = \"\"\n+build_dir = \"\"\n+ignore_list = list()\n+target_builds = list()\n+\n+target_dict = { }\n+header_dict = { }\n+search_path = [ \".\", \"../include\", \"../libcpp/include\" ]\n+\n+remove_count = { }\n+\n+\n+# Given a header name, normalize it.  ie.  cp/cp-tree.h could be in gcc, while\n+# the same header could be referenced from within the cp subdirectory as\n+# just cp-tree.h\n+# for now, just assume basenames are unique\n+\n+def normalize_header (header):\n+  return os.path.basename (header)\n+\n+\n+# Adds a header file and its sub-includes to the global dictionary if they\n+# aren't already there.  Specify s_path since different build directories may\n+# append themselves on demand to the global list.\n+# return entry for the specified header, knowing all sub entries are completed\n+\n+def get_header_info (header, s_path):\n+  global header_dict\n+  global empty_iinfo\n+  process_list = list ()\n+  location = \"\"\n+  bname = \"\"\n+  bname_iinfo = empty_iinfo\n+  for path in s_path:\n+    if os.path.exists (path + \"/\" + header):\n+      location = path + \"/\" + header\n+      break\n+\n+  if location:\n+    bname = normalize_header (location)\n+    if header_dict.get (bname):\n+      bname_iinfo = header_dict[bname]\n+      loc2 = ii_path (bname_iinfo)+ \"/\" + bname\n+      if loc2[:2] == \"./\":\n+        loc2 = loc2[2:]\n+      if location[:2] == \"./\":\n+        location = location[2:]\n+      if loc2 != location:\n+        # Don't use the cache if it isnt the right one.\n+        bname_iinfo = process_ii_macro (location)\n+      return bname_iinfo\n+\n+    bname_iinfo = process_ii_macro (location)\n+    header_dict[bname] = bname_iinfo\n+    # now decend into the include tree\n+    for i in ii_include_list (bname_iinfo):\n+      get_header_info (i, s_path)\n+  else:\n+    # if the file isnt in the source directories, look in the build and target\n+    # directories. If it is here, then aggregate all the versions.\n+    location = build_dir + \"/gcc/\" + header\n+    build_inc = target_inc = False\n+    if os.path.exists (location):\n+      build_inc = True\n+    for x in target_dict:\n+      location = target_dict[x] + \"/gcc/\" + header\n+      if os.path.exists (location):\n+        target_inc = True\n+        break\n+\n+    if (build_inc or target_inc):\n+      bname = normalize_header(header)\n+      defines = set()\n+      consumes = set()\n+      incl = set()\n+      if build_inc:\n+        iinfo = process_ii_macro (build_dir + \"/gcc/\" + header)\n+        defines = set (ii_macro_define (iinfo))\n+        consumes = set (ii_macro_consume (iinfo))\n+        incl = set (ii_include_list (iinfo))\n+\n+      if (target_inc):\n+        for x in target_dict:\n+          location = target_dict[x] + \"/gcc/\" + header\n+          if os.path.exists (location):\n+            iinfo = process_ii_macro (location)\n+            defines.update (ii_macro_define (iinfo))\n+            consumes.update (ii_macro_consume (iinfo))\n+            incl.update (ii_include_list (iinfo))\n+\n+      bname_iinfo = (header, \"build\", list(incl), list(), list(consumes), list(defines), list(), list())\n+\n+      header_dict[bname] = bname_iinfo\n+      for i in incl:\n+        get_header_info (i, s_path)\n+\n+  return bname_iinfo\n+\n+\n+# return a list of all headers brought in by this header\n+def all_headers (fname):\n+  global header_dict\n+  headers_stack = list()\n+  headers_list = list()\n+  if header_dict.get (fname) == None:\n+    return list ()\n+  for y in ii_include_list (header_dict[fname]):\n+    headers_stack.append (y)\n+\n+  while headers_stack:\n+    h = headers_stack.pop ()\n+    hn = normalize_header (h)\n+    if hn not in headers_list:\n+      headers_list.append (hn)\n+      if header_dict.get(hn):\n+        for y in ii_include_list (header_dict[hn]):\n+          if normalize_header (y) not in headers_list:\n+            headers_stack.append (y)\n+\n+  return headers_list\n+\n+\n+\n+\n+# Search bld_dir for all target tuples, confirm that they have a build path with\n+# bld_dir/target-tuple/gcc, and build a dictionary of build paths indexed by\n+# target tuple..\n+\n+def build_target_dict (bld_dir, just_these):\n+  global target_dict\n+  target_doct = { }\n+  error = False\n+  if os.path.exists (bld_dir):\n+    if just_these:\n+      ls = just_these\n+    else:\n+      ls = os.listdir(bld_dir)\n+    for t in ls:\n+      if t.find(\"-\") != -1:\n+        target = t.strip()\n+        tpath = bld_dir + \"/\" + target\n+        if not os.path.exists (tpath + \"/gcc\"):\n+          print \"Error: gcc build directory for target \" + t + \" Does not exist: \" + tpath + \"/gcc\"\n+          error = True\n+        else:\n+          target_dict[target] = tpath\n+\n+  if error:\n+    target_dict = { }\n+\n+def get_obj_name (src_file):\n+  if src_file[-2:] == \".c\":\n+    return src_file.replace (\".c\", \".o\")\n+  elif src_file[-3:] == \".cc\":\n+    return src_file.replace (\".cc\", \".o\")\n+  return \"\"\n+\n+def target_obj_exists (target, obj_name):\n+  global target_dict\n+  # look in a subdir if src has a subdir, then check gcc base directory.\n+  if target_dict.get(target):\n+    obj = target_dict[target] + \"/gcc/\" + obj_name\n+    if not os.path.exists (obj):\n+      obj = target_dict[target] + \"/gcc/\" + os.path.basename(obj_name)\n+    if os.path.exists (obj):\n+      return True\n+  return False\n+ \n+# Given a src file, return a list of targets which may build this file.\n+def find_targets (src_file):\n+  global target_dict\n+  targ_list = list()\n+  obj_name = get_obj_name (src_file)\n+  if not obj_name:\n+    print \"Error: \" + src_file + \" - Cannot determine object name.\"\n+    return list()\n+\n+  # Put the high priority targets which tend to trigger failures first\n+  for target in target_priority:\n+    if target_obj_exists (target, obj_name):\n+      targ_list.append ((target, target_dict[target]))\n+\n+  for target in target_dict:\n+    if target not in target_priority and target_obj_exists (target, obj_name):\n+      targ_list.append ((target, target_dict[target]))\n+        \n+  return targ_list\n+\n+\n+def try_to_remove (src_file, h_list, verbose):\n+  global target_dict\n+  global header_dict\n+  global build_dir\n+\n+  # build from scratch each time\n+  header_dict = { }\n+  summary = \"\"\n+  rmcount = 0\n+\n+  because = { }\n+  src_info = process_ii_macro_src (src_file)\n+  src_data = ii_src (src_info)\n+  if src_data:\n+    inclist = ii_include_list_non_cond (src_info)\n+    # work is done if there are no includes to check\n+    if not inclist:\n+      return src_file + \": No include files to attempt to remove\"\n+\n+    # work on the include list in reverse.\n+    inclist.reverse()\n+\n+    # Get the target list \n+    targ_list = list()\n+    targ_list = find_targets (src_file)\n+\n+    spath = search_path\n+    if os.path.dirname (src_file):\n+      spath.append (os.path.dirname (src_file))\n+\n+    hostbuild = True\n+    if src_file.find(\"config/\") != -1:\n+      # config files dont usually build on the host\n+      hostbuild = False\n+      obn = get_obj_name (os.path.basename (src_file))\n+      if obn and os.path.exists (build_dir + \"/gcc/\" + obn):\n+        hostbuild = True\n+      if not target_dict:\n+        summary = src_file + \": Target builds are required for config files.  None found.\"\n+        print summary\n+        return summary\n+      if not targ_list:\n+        summary =src_file + \": Cannot find any targets which build this file.\"\n+        print summary\n+        return summary\n+\n+    if hostbuild:\n+      # confirm it actually builds before we do anything\n+      print \"Confirming source file builds\"\n+      res = get_make_output (build_dir + \"/gcc\", \"all\")\n+      if res[0] != 0:\n+        message = \"Error: \" + src_file + \" does not build currently.\"\n+        summary = src_file + \" does not build on host.\"\n+        print message\n+        print res[1]\n+        if verbose:\n+          verbose.write (message + \"\\n\")\n+          verbose.write (res[1]+ \"\\n\")\n+        return summary\n+\n+    src_requires = set (ii_macro_consume (src_info))\n+    for macro in src_requires:\n+      because[macro] = src_file\n+    header_seen = list ()\n+\n+    os.rename (src_file, src_file + \".bak\")\n+    src_orig = copy.deepcopy (src_data)\n+    src_tmp = copy.deepcopy (src_data)\n+\n+    try:\n+      # process the includes from bottom to top.  This is because we know that\n+      # later includes have are known to be needed, so any dependency from this \n+      # header is a true dependency\n+      for inc_file in inclist:\n+        inc_file_norm = normalize_header (inc_file)\n+        \n+        if inc_file in no_remove:\n+          continue\n+        if len (h_list) != 0 and inc_file_norm not in h_list:\n+          continue\n+        if inc_file_norm[0:3] == \"gt-\":\n+          continue\n+        if inc_file_norm[0:6] == \"gtype-\":\n+          continue\n+        if inc_file_norm.replace(\".h\",\".c\") == os.path.basename(src_file):\n+          continue\n+             \n+        lookfor = ii_src_line(src_info)[inc_file]\n+        src_tmp.remove (lookfor)\n+        message = \"Trying \" + src_file + \" without \" + inc_file\n+        print message\n+        if verbose:\n+          verbose.write (message + \"\\n\")\n+        out = open(src_file, \"w\")\n+        for line in src_tmp:\n+          out.write (line)\n+        out.close()\n+          \n+        keep = False\n+        if hostbuild:\n+          res = get_make_output (build_dir + \"/gcc\", \"all\")\n+        else:\n+          res = (0, \"\")\n+\n+        rc = res[0]\n+        message = \"Passed Host build\"\n+        if (rc != 0):\n+          # host build failed\n+          message  = \"Compilation failed:\\n\";\n+          keep = True\n+        else:\n+          if targ_list:\n+            objfile = get_obj_name (src_file)\n+            t1 = targ_list[0]\n+            if objfile and os.path.exists(t1[1] +\"/gcc/\"+objfile):\n+              res = get_make_output_parallel (targ_list, objfile, 0)\n+            else:\n+              res = get_make_output_parallel (targ_list, \"all-gcc\", 0)\n+            rc = res[0]\n+            if rc != 0:\n+              message = \"Compilation failed on TARGET : \" + res[2]\n+              keep = True\n+            else:\n+              message = \"Passed host and target builds\"\n+\n+        if keep:\n+          print message + \"\\n\"\n+\n+        if (rc != 0):\n+          if verbose:\n+            verbose.write (message + \"\\n\");\n+            verbose.write (res[1])\n+            verbose.write (\"\\n\");\n+            if os.path.exists (inc_file):\n+              ilog = open(inc_file+\".log\",\"a\")\n+              ilog.write (message + \" for \" + src_file + \":\\n\\n\");\n+              ilog.write (\"============================================\\n\");\n+              ilog.write (res[1])\n+              ilog.write (\"\\n\");\n+              ilog.close()\n+            if os.path.exists (src_file):\n+              ilog = open(src_file+\".log\",\"a\")\n+              ilog.write (message + \" for \" +inc_file + \":\\n\\n\");\n+              ilog.write (\"============================================\\n\");\n+              ilog.write (res[1])\n+              ilog.write (\"\\n\");\n+              ilog.close()\n+\n+        # Given a sequence where :\n+        # #include \"tm.h\"\n+        # #include \"target.h\"  // includes tm.h\n+\n+        # target.h was required, and when attempting to remove tm.h we'd see that\n+        # all the macro defintions are \"required\" since they all look like:\n+        # #ifndef HAVE_blah\n+        # #define HAVE_blah\n+        # endif\n+\n+        # when target.h was found to be required, tm.h will be tagged as included.\n+        # so when we get this far, we know we dont have to check the macros for\n+        # tm.h since we know it is already been included.\n+\n+        if inc_file_norm not in header_seen:\n+          iinfo = get_header_info (inc_file, spath)\n+          newlist = all_headers (inc_file_norm)\n+          if ii_path(iinfo) == \"build\" and not target_dict:\n+            keep = True\n+            text = message + \" : Will not remove a build file without some targets.\"\n+            print text\n+            ilog = open(src_file+\".log\",\"a\")\n+            ilog.write (text +\"\\n\")\n+            ilog.write (\"============================================\\n\");\n+            ilog.close()\n+            ilog = open(\"reduce-headers-kept.log\",\"a\")\n+            ilog.write (src_file + \" \" + text +\"\\n\")\n+            ilog.close()\n+        else:\n+          newlist = list()\n+        if not keep and inc_file_norm not in header_seen:\n+          # now look for any macro requirements.\n+          for h in newlist:\n+            if not h in header_seen:\n+              if header_dict.get(h):\n+                defined = ii_macro_define (header_dict[h])\n+                for dep in defined:\n+                  if dep in src_requires and dep not in ignore_list:\n+                    keep = True;\n+                    text = message + \", but must keep \" + inc_file + \" because it provides \" + dep \n+                    if because.get(dep) != None:\n+                      text = text + \" Possibly required by \" + because[dep]\n+                    print text\n+                    ilog = open(inc_file+\".log\",\"a\")\n+                    ilog.write (because[dep]+\": Requires [dep] in \"+src_file+\"\\n\")\n+                    ilog.write (\"============================================\\n\");\n+                    ilog.close()\n+                    ilog = open(src_file+\".log\",\"a\")\n+                    ilog.write (text +\"\\n\")\n+                    ilog.write (\"============================================\\n\");\n+                    ilog.close()\n+                    ilog = open(\"reduce-headers-kept.log\",\"a\")\n+                    ilog.write (src_file + \" \" + text +\"\\n\")\n+                    ilog.close()\n+                    if verbose:\n+                      verbose.write (text + \"\\n\")\n+\n+        if keep:\n+          # add all headers 'consumes' to src_requires list, and mark as seen\n+          for h in newlist:\n+            if not h in header_seen:\n+              header_seen.append (h)\n+              if header_dict.get(h):\n+                consume = ii_macro_consume (header_dict[h])\n+                for dep in consume:\n+                  if dep not in src_requires:\n+                    src_requires.add (dep)\n+                    if because.get(dep) == None:\n+                      because[dep] = inc_file\n+\n+          src_tmp = copy.deepcopy (src_data)\n+        else:\n+          print message + \"  --> removing \" + inc_file + \"\\n\"\n+          rmcount += 1\n+          if verbose:\n+            verbose.write (message + \"  --> removing \" + inc_file + \"\\n\")\n+          if remove_count.get(inc_file) == None:\n+            remove_count[inc_file] = 1\n+          else:\n+            remove_count[inc_file] += 1\n+          src_data = copy.deepcopy (src_tmp)\n+    except:\n+      print \"Interuption: restoring original file\"\n+      out = open(src_file, \"w\")\n+      for line in src_orig:\n+        out.write (line)\n+      out.close()\n+      raise\n+\n+    # copy current version, since it is the \"right\" one now.\n+    out = open(src_file, \"w\")\n+    for line in src_data:\n+      out.write (line)\n+    out.close()\n+    \n+    # Try a final host bootstrap build to make sure everything is kosher.\n+    if hostbuild:\n+      res = get_make_output (build_dir, \"all\")\n+      rc = res[0]\n+      if (rc != 0):\n+        # host build failed! return to original version\n+        print \"Error: \" + src_file + \" Failed to bootstrap at end!!! restoring.\"\n+        print \"        Bad version at \" + src_file + \".bad\"\n+        os.rename (src_file, src_file + \".bad\")\n+        out = open(src_file, \"w\")\n+        for line in src_orig:\n+          out.write (line)\n+        out.close()\n+        return src_file + \": failed to build after reduction.  Restored original\"\n+\n+    if src_data == src_orig:\n+      summary = src_file + \": No change.\"\n+    else:\n+      summary = src_file + \": Reduction performed, \"+str(rmcount)+\" includes removed.\"\n+  print summary\n+  return summary\n+\n+only_h = list ()\n+ignore_cond = False\n+\n+usage = False\n+src = list()\n+only_targs = list ()\n+for x in sys.argv[1:]:\n+  if x[0:2] == \"-b\":\n+    build_dir = x[2:]\n+  elif x[0:2] == \"-f\":\n+    fn = normalize_header (x[2:])\n+    if fn not in only_h:\n+      only_h.append (fn)\n+  elif x[0:2] == \"-h\":\n+    usage = True\n+  elif x[0:2] == \"-d\":\n+    ignore_cond = True\n+  elif x[0:2] == \"-D\":\n+    ignore_list.append(x[2:])\n+  elif x[0:2] == \"-T\":\n+    only_targs.append(x[2:])\n+  elif x[0:2] == \"-t\":\n+    target_dir = x[2:]\n+  elif x[0] == \"-\":\n+    print \"Error:  Unrecognized option \" + x\n+    usgae = True\n+  else:\n+    if not os.path.exists (x):\n+      print \"Error: specified file \" + x + \" does not exist.\"\n+      usage = True\n+    else:\n+      src.append (x)\n+\n+if target_dir:\n+  build_target_dict (target_dir, only_targs)\n+\n+if build_dir == \"\" and target_dir == \"\":\n+  print \"Error: Must specify a build directory, and/or a target directory.\"\n+  usage = True\n+\n+if build_dir and not os.path.exists (build_dir):\n+    print \"Error: specified build directory does not exist : \" + build_dir\n+    usage = True\n+\n+if target_dir and not os.path.exists (target_dir):\n+    print \"Error: specified target directory does not exist : \" + target_dir\n+    usage = True\n+\n+if usage:\n+  print \"Attempts to remove extraneous include files from source files.\"\n+  print \" \"\n+  print \"Should be run from the main gcc source directory, and works on a target\"\n+  print \"directory, as we attempt to make the 'all' target.\"\n+  print \" \"\n+  print \"By default, gcc-reorder-includes is run on each file before attempting\"\n+  print \"to remove includes. this removes duplicates and puts some headers in a\"\n+  print \"canonical ordering\"\n+  print \" \"\n+  print \"The build directory should be ready to compile via make. Time is saved\"\n+  print \"if the build is already complete, so that only changes need to be built.\"\n+  print \" \"\n+  print \"Usage: [options] file1.c [file2.c] ... [filen.c]\"\n+  print \"      -bdir    : the root build directory to attempt buiding .o files.\"\n+  print \"      -tdir    : the target build directory\"\n+  print \"      -d       : Ignore conditional macro dependencies.\"\n+  print \" \"\n+  print \"      -Dmacro  : Ignore a specific macro for dependencies\"\n+  print \"      -Ttarget : Only consider target in target directory.\"\n+  print \"      -fheader : Specifies a specific .h file to be considered.\"\n+  print \" \"\n+  print \"      -D, -T, and -f can be specified mulitple times and are aggregated.\"\n+  print \" \"\n+  print \"  The original file will be in filen.bak\"\n+  print \" \"\n+  sys.exit (0)\n+ \n+if only_h:\n+  print \"Attempting to remove only these files:\"\n+  for x in only_h:\n+    print x\n+  print \" \"\n+\n+logfile = open(\"reduce-headers.log\",\"w\")\n+\n+for x in src:\n+  msg = try_to_remove (x, only_h, logfile)\n+  ilog = open(\"reduce-headers.sum\",\"a\")\n+  ilog.write (msg + \"\\n\")\n+  ilog.close()\n+\n+ilog = open(\"reduce-headers.sum\",\"a\")\n+ilog.write (\"===============================================================\\n\")\n+for x in remove_count:\n+  msg = x + \": Removed \" + str(remove_count[x]) + \" times.\"\n+  print msg\n+  logfile.write (msg + \"\\n\")\n+  ilog.write (msg + \"\\n\")\n+\n+\n+\n+\n+"}, {"sha": "ce20096a453d755c705dfe97088e02db14c4ee89", "filename": "contrib/header-tools/replace-header", "status": "added", "additions": 53, "deletions": 0, "changes": 53, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Freplace-header", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Freplace-header", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2Fheader-tools%2Freplace-header?ref=bd94906f987eb065dd6d4e9cadef82f7513d05cd", "patch": "@@ -0,0 +1,53 @@\n+#! /usr/bin/python2\n+import os.path\n+import sys\n+import shlex\n+import re\n+\n+from headerutils import *\n+\n+\n+files = list()\n+replace = list()\n+find = \"\"\n+usage = False\n+\n+for x in sys.argv[1:]:\n+  if x[0:2] == \"-h\":\n+    usage = True\n+  elif x[0:2] == \"-f\" and find == \"\":\n+    find = x[2:]\n+  elif x[0:2] == \"-r\":\n+    replace.append (x[2:])\n+  elif x[0:1] == \"-\":\n+    print \"Error: unrecognized option \" + x\n+    usage = True\n+  else:\n+    files.append (x)\n+\n+if find == \"\":\n+  usage = True\n+\n+if usage:\n+  print \"replace-header -fheader -rheader [-rheader] file1 [filen.]\"\n+  sys.exit(0)\n+\n+string = \"\"\n+for x in replace:\n+  string = string + \" '\"+x+\"'\"\n+print \"Replacing '\"+find+\"'  with\"+string\n+\n+for x in files:\n+  src = readwholefile (x)\n+  src = find_replace_include (find, replace, src)\n+  if (len(src) > 0):\n+    print x + \": Changed\"\n+    out = open(x, \"w\")\n+    for line in src:\n+      out.write (line);\n+    out.close ()\n+  else:\n+    print x\n+\n+\n+"}, {"sha": "cb949ec1f4431b5416041288cf168f3b20d32a6e", "filename": "contrib/header-tools/show-headers", "status": "added", "additions": 151, "deletions": 0, "changes": 151, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Fshow-headers", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/bd94906f987eb065dd6d4e9cadef82f7513d05cd/contrib%2Fheader-tools%2Fshow-headers", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2Fheader-tools%2Fshow-headers?ref=bd94906f987eb065dd6d4e9cadef82f7513d05cd", "patch": "@@ -0,0 +1,151 @@\n+#! /usr/bin/python2\n+import os.path\n+import sys\n+import shlex\n+import re\n+\n+from headerutils import *\n+\n+\n+tabstop = 2\n+padding = \"                                                                  \"\n+seen = { }\n+output = list()\n+summary = list()\n+sawcore = False\n+\n+# list of headers to emphasize\n+highlight = list ()\n+\n+bld_dir = \"\"\n+# search path for headers\n+incl_dirs = [\"../include\", \"../libcpp/include\", \"common\", \"c-family\", \"c\", \"cp\", \"config\" ]\n+# extra search paths to look in *after* the directory the source file is in. \n+\n+# append (1) to the end of the first line which includes INC in list INC.\n+def append_1 (output, inc):\n+  for n,t in enumerate (output):\n+    idx = t.find(inc)\n+    if idx != -1:\n+      eos = idx + len (inc)\n+      t = t[:eos] + \"  (1)\" + t[eos+1:]\n+      output[n] = t\n+      return\n+\n+# These headers show up as duplicates in rtl.h due to conditional code arund the includes\n+rtl_core = [ \"machmode.h\" , \"signop.h\" , \"wide-int.h\" , \"double-int.h\" , \"real.h\" , \"fixed-value.h\" , \"statistics.h\" , \"vec.h\" , \"hash-table.h\" , \"hash-set.h\" , \"input.h\" , \"is-a.h\" ]\n+\n+def find_include_data (inc):\n+  global sawcore\n+  for x in incl_dirs:\n+    nm = x+\"/\"+inc\n+    if os.path.exists (nm):\n+      info = find_unique_include_list (nm)\n+      # rtl.h mimics coretypes for GENERATOR FILES, remove if coretypes.h seen.\n+      if inc == \"coretypes.h\":\n+        sawcore = True\n+      elif inc  == \"rtl.h\" and sawcore:\n+        for i in rtl_core:\n+          if i in info:\n+            info.remove (i)\n+      return info\n+  return list()\n+\n+def process_include (inc, indent):\n+  if inc[-2:] != \".h\":\n+    return\n+  bname  = os.path.basename (inc)\n+  if bname in highlight:\n+    arrow = \"                <<-------\"\n+    if bname not in summary:\n+      summary.append (bname)\n+  else:\n+    arrow = \"\"\n+  if seen.get(inc) == None:\n+    seen[inc] = 1\n+    output.append (padding[:indent*tabstop] + bname + arrow)\n+    info = find_include_data (inc)\n+    for y in info:\n+      process_include (y, indent+1)\n+  else:\n+    seen[inc] += 1\n+    if (seen[inc] == 2):\n+      append_1(output, inc)\n+    output.append (padding[:indent*tabstop] + bname + \"  (\"+str(seen[inc])+\")\" + arrow)\n+\n+    \n+\n+extradir = list()\n+usage = False\n+src = list()\n+\n+for x in sys.argv[1:]:\n+  if x[0:2] == \"-i\":\n+    bld = x[2:]\n+    extradir.append (bld)\n+  elif x[0:2] == \"-s\":\n+    highlight.append (os.path.basename (x[2:]))\n+  elif x[0:2] == \"-h\":\n+    usage = True\n+  else:\n+    src.append (x)\n+\n+if len(src) != 1:\n+  usage = True\n+elif not os.path.exists (src[0]):\n+  print src[0] + \": Requested source file does not exist.\\n\"\n+  usage = True\n+\n+if usage:\n+  print \"show-headers [-idir] [-sfilen] file1 \"\n+  print \" \"\n+  print \" Show a hierarchical visual format how many times each header file\"\n+  print \" is included in a source file.  Should be run from the source directory\"\n+  print \" files from find-include-depends\"\n+  print \"      -s : search for a header, and point it out.\"\n+  print \"      -i : Specifies additonal directories to search for includes.\"\n+  sys.exit(0)\n+\n+\n+\n+if extradir:\n+  incl_dirs = extradir + incl_dirs;\n+\n+blddir = find_gcc_bld_dir (\"../..\")\n+\n+if blddir:\n+  print \"Using build directory: \" + blddir\n+  incl_dirs.insert (0, blddir)\n+else:\n+  print \"Could not find a build directory, better results if you specify one with -i\"\n+\n+# search path is now \".\", blddir, extradirs_from_-i, built_in_incl_dirs\n+incl_dirs.insert (0, \".\")\n+\n+# if source is in a subdirectory, prepend the subdirectory to the search list\n+x = src[0]\n+srcpath = os.path.dirname(x)\n+if srcpath:\n+  incl_dirs.insert (0, srcpath)\n+\n+output = list()\n+sawcore = False\n+\n+data = open (x).read().splitlines()\n+for line in data:\n+  d = find_pound_include (line, True, True)\n+  if d and d[-2:] == \".h\":\n+    process_include (d, 1)\n+\n+print \"\\n\" + x\n+for line in output:\n+  print line\n+\n+if highlight:\n+  print \" \"\n+  for h in summary:\n+    print h + \" is included by source file.\"\n+  for h in highlight:\n+    if h not in summary:\n+      print h + \" is not included by source file.\"\n+"}]}