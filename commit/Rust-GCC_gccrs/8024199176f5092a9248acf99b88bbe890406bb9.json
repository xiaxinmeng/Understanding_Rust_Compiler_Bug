{"sha": "8024199176f5092a9248acf99b88bbe890406bb9", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ODAyNDE5OTE3NmY1MDkyYTkyNDhhY2Y5OWI4OGJiZTg5MDQwNmJiOQ==", "commit": {"author": {"name": "Jonathan Wakely", "email": "jwakely.gcc@gmail.com", "date": "2012-10-09T08:16:13Z"}, "committer": {"name": "Jonathan Wakely", "email": "redi@gcc.gnu.org", "date": "2012-10-09T08:16:13Z"}, "message": "re PR libstdc++/54754 ([parallel mode] 'make check-parallel' only works on x86-64)\n\n\tPR libstdc++/54754\n\t* include/parallel/compatibility.h: Use atomic built-ins when they are\n\tlock-free.\n\nFrom-SVN: r192240", "tree": {"sha": "87cbb09e7fd23d73d39e84854a1fad151118a777", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/87cbb09e7fd23d73d39e84854a1fad151118a777"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/8024199176f5092a9248acf99b88bbe890406bb9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8024199176f5092a9248acf99b88bbe890406bb9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8024199176f5092a9248acf99b88bbe890406bb9", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8024199176f5092a9248acf99b88bbe890406bb9/comments", "author": null, "committer": {"login": "jwakely", "id": 1254480, "node_id": "MDQ6VXNlcjEyNTQ0ODA=", "avatar_url": "https://avatars.githubusercontent.com/u/1254480?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwakely", "html_url": "https://github.com/jwakely", "followers_url": "https://api.github.com/users/jwakely/followers", "following_url": "https://api.github.com/users/jwakely/following{/other_user}", "gists_url": "https://api.github.com/users/jwakely/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwakely/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwakely/subscriptions", "organizations_url": "https://api.github.com/users/jwakely/orgs", "repos_url": "https://api.github.com/users/jwakely/repos", "events_url": "https://api.github.com/users/jwakely/events{/privacy}", "received_events_url": "https://api.github.com/users/jwakely/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "eef26c05fb7c3a758b48b8bf873390623fa3d747", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/eef26c05fb7c3a758b48b8bf873390623fa3d747", "html_url": "https://github.com/Rust-GCC/gccrs/commit/eef26c05fb7c3a758b48b8bf873390623fa3d747"}], "stats": {"total": 176, "additions": 49, "deletions": 127}, "files": [{"sha": "b9f3533798cc8f802e6003b2ac60ac0ff4140508", "filename": "libstdc++-v3/ChangeLog", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8024199176f5092a9248acf99b88bbe890406bb9/libstdc%2B%2B-v3%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8024199176f5092a9248acf99b88bbe890406bb9/libstdc%2B%2B-v3%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2FChangeLog?ref=8024199176f5092a9248acf99b88bbe890406bb9", "patch": "@@ -1,3 +1,9 @@\n+2012-10-09  Jonathan Wakely  <jwakely.gcc@gmail.com>\n+\n+\tPR libstdc++/54754\n+\t* include/parallel/compatibility.h: Use atomic built-ins when they are\n+\tlock-free.\n+\n 2012-10-09  Uros Bizjak  <ubizjak@gmail.com>\n \n \t* testsuite/util/testsuite_abi.cc (check_version): Add CXXABI_1.3.7."}, {"sha": "a58e65fe60ce95f10728ab2ff03951a8ef913bf7", "filename": "libstdc++-v3/include/parallel/compatibility.h", "status": "modified", "additions": 43, "deletions": 127, "changes": 170, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8024199176f5092a9248acf99b88bbe890406bb9/libstdc%2B%2B-v3%2Finclude%2Fparallel%2Fcompatibility.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8024199176f5092a9248acf99b88bbe890406bb9/libstdc%2B%2B-v3%2Finclude%2Fparallel%2Fcompatibility.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fparallel%2Fcompatibility.h?ref=8024199176f5092a9248acf99b88bbe890406bb9", "patch": "@@ -51,154 +51,70 @@ __attribute((dllimport)) void __attribute__((stdcall)) Sleep (unsigned long);\n \n namespace __gnu_parallel\n {\n-  // These atomic functions only work on integers\n-\n-  /** @brief Add a value to a variable, atomically.\n-   *\n-   *  Implementation is heavily platform-dependent.\n-   *  @param __ptr Pointer to a 32-bit signed integer.\n-   *  @param __addend Value to add.\n-   */\n-  inline int32_t\n-  __fetch_and_add_32(volatile int32_t* __ptr, int32_t __addend)\n-  {\n-    return __atomic_fetch_add(__ptr, __addend, __ATOMIC_ACQ_REL);\n-  }\n-\n-  /** @brief Add a value to a variable, atomically.\n-   *\n-   *  Implementation is heavily platform-dependent.\n-   *  @param __ptr Pointer to a 64-bit signed integer.\n-   *  @param __addend Value to add.\n-   */\n-  inline int64_t\n-  __fetch_and_add_64(volatile int64_t* __ptr, int64_t __addend)\n-  {\n-#if defined(__x86_64)\n-    return __atomic_fetch_add(__ptr, __addend, __ATOMIC_ACQ_REL);\n-#elif defined(__i386) &&                   \\\n-  (defined(__i686) || defined(__pentium4) || defined(__athlon)  \\\n-   || defined(__k8) || defined(__core2))\n-    return __atomic_fetch_add(__ptr, __addend, __ATOMIC_ACQ_REL);\n-#else   //fallback, slow\n-#if defined(__i386)\n-    // XXX doesn'__t work with -march=native\n-    //#warning \"please compile with -march=i686 or better\"\n-#endif\n-#pragma message(\"slow __fetch_and_add_64\")\n-    int64_t __res;\n-#pragma omp critical\n+  template<typename _Tp>\n+    inline _Tp\n+    __add_omp(volatile _Tp* __ptr, _Tp __addend)\n     {\n-      __res = *__ptr;\n-      *(__ptr) += __addend;\n+      int64_t __res;\n+#pragma omp critical\n+      {\n+\t__res = *__ptr;\n+\t*(__ptr) += __addend;\n+      }\n+      return __res;\n     }\n-    return __res;\n-#endif\n-  }\n \n   /** @brief Add a value to a variable, atomically.\n    *\n-   *  Implementation is heavily platform-dependent.\n    *  @param __ptr Pointer to a signed integer.\n    *  @param __addend Value to add.\n    */\n   template<typename _Tp>\n-  inline _Tp\n-  __fetch_and_add(volatile _Tp* __ptr, _Tp __addend)\n-  {\n-    if (sizeof(_Tp) == sizeof(int32_t))\n-      return\n-        (_Tp)__fetch_and_add_32((volatile int32_t*) __ptr, (int32_t)__addend);\n-    else if (sizeof(_Tp) == sizeof(int64_t))\n-      return\n-        (_Tp)__fetch_and_add_64((volatile int64_t*) __ptr, (int64_t)__addend);\n-    else\n-      _GLIBCXX_PARALLEL_ASSERT(false);\n-  }\n-\n-  /** @brief Compare @c *__ptr and @c __comparand. If equal, let @c\n-   * *__ptr=__replacement and return @c true, return @c false otherwise.\n-   *\n-   *  Implementation is heavily platform-dependent.\n-   *  @param __ptr Pointer to 32-bit signed integer.\n-   *  @param __comparand Compare value.\n-   *  @param __replacement Replacement value.\n-   */\n-  inline bool\n-  __compare_and_swap_32(volatile int32_t* __ptr, int32_t __comparand,\n-                        int32_t __replacement)\n-  {\n-    return __atomic_compare_exchange_n(__ptr, &__comparand, __replacement,\n-\t\t\t\t       false, __ATOMIC_ACQ_REL,\n-\t\t\t\t       __ATOMIC_RELAXED);\n-  }\n+    inline _Tp\n+    __fetch_and_add(volatile _Tp* __ptr, _Tp __addend)\n+    {\n+      if (__atomic_always_lock_free(sizeof(_Tp), __ptr))\n+\treturn __atomic_fetch_add(__ptr, __addend, __ATOMIC_ACQ_REL);\n+      return __add_omp(__ptr, __addend);\n+    }\n \n-  /** @brief Compare @c *__ptr and @c __comparand. If equal, let @c\n-   * *__ptr=__replacement and return @c true, return @c false otherwise.\n-   *\n-   *  Implementation is heavily platform-dependent.\n-   *  @param __ptr Pointer to 64-bit signed integer.\n-   *  @param __comparand Compare value.\n-   *  @param __replacement Replacement value.\n-   */\n-  inline bool\n-  __compare_and_swap_64(volatile int64_t* __ptr, int64_t __comparand,\n-                        int64_t __replacement)\n-  {\n-#if defined(__x86_64)\n-    return __atomic_compare_exchange_n(__ptr, &__comparand, __replacement,\n-\t\t\t\t       false, __ATOMIC_ACQ_REL,\n-\t\t\t\t       __ATOMIC_RELAXED);\n-#elif defined(__i386) &&                   \\\n-  (defined(__i686) || defined(__pentium4) || defined(__athlon)  \\\n-   || defined(__k8) || defined(__core2))\n-    return __atomic_compare_exchange_n(__ptr, &__comparand, __replacement,\n-\t\t\t\t       false, __ATOMIC_ACQ_REL,\n-\t\t\t\t       __ATOMIC_RELAXED);\n-#else\n-#if defined(__i386)\n-    // XXX -march=native\n-    //#warning \"please compile with -march=i686 or better\"\n-#endif\n-#pragma message(\"slow __compare_and_swap_64\")\n-    bool __res = false;\n-#pragma omp critical\n+  template<typename _Tp>\n+    inline bool\n+    __cas_omp(volatile _Tp* __ptr, _Tp __comparand, _Tp __replacement)\n     {\n-      if (*__ptr == __comparand)\n-        {\n-          *__ptr = __replacement;\n-          __res = true;\n-        }\n+      bool __res = false;\n+#pragma omp critical\n+      {\n+\tif (*__ptr == __comparand)\n+\t  {\n+\t    *__ptr = __replacement;\n+\t    __res = true;\n+\t  }\n+      }\n+      return __res;\n     }\n-    return __res;\n-#endif\n-  }\n \n-  /** @brief Compare @c *__ptr and @c __comparand. If equal, let @c\n+  /** @brief Compare-and-swap\n+   *\n+   * Compare @c *__ptr and @c __comparand. If equal, let @c\n    * *__ptr=__replacement and return @c true, return @c false otherwise.\n    *\n-   *  Implementation is heavily platform-dependent.\n    *  @param __ptr Pointer to signed integer.\n    *  @param __comparand Compare value.\n    *  @param __replacement Replacement value.\n    */\n   template<typename _Tp>\n-  inline bool\n-  __compare_and_swap(volatile _Tp* __ptr, _Tp __comparand, _Tp __replacement)\n-  {\n-    if (sizeof(_Tp) == sizeof(int32_t))\n-      return __compare_and_swap_32((volatile int32_t*) __ptr,\n-                                   (int32_t)__comparand,\n-                                   (int32_t)__replacement);\n-    else if (sizeof(_Tp) == sizeof(int64_t))\n-      return __compare_and_swap_64((volatile int64_t*) __ptr,\n-                                   (int64_t)__comparand,\n-                                   (int64_t)__replacement);\n-    else\n-      _GLIBCXX_PARALLEL_ASSERT(false);\n-  }\n+    inline bool\n+    __compare_and_swap(volatile _Tp* __ptr, _Tp __comparand, _Tp __replacement)\n+    {\n+      if (__atomic_always_lock_free(sizeof(_Tp), __ptr))\n+\treturn __atomic_compare_exchange_n(__ptr, &__comparand, __replacement,\n+\t\t\t\t\t   false, __ATOMIC_ACQ_REL,\n+\t\t\t\t\t   __ATOMIC_RELAXED);\n+      return __cas_omp(__ptr, __comparand, __replacement);\n+    }\n \n-  /** @brief Yield the control to another thread, without waiting for\n+  /** @brief Yield control to another thread, without waiting for\n    *  the end of the time slice.\n    */\n   inline void"}]}