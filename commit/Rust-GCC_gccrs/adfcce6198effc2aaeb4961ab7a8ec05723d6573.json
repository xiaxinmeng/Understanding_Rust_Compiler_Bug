{"sha": "adfcce6198effc2aaeb4961ab7a8ec05723d6573", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YWRmY2NlNjE5OGVmZmMyYWFlYjQ5NjFhYjdhOGVjMDU3MjNkNjU3Mw==", "commit": {"author": {"name": "Daniel Berlin", "email": "dan@cgsoftware.com", "date": "2001-08-03T16:52:01Z"}, "committer": {"name": "Daniel Berlin", "email": "dberlin@gcc.gnu.org", "date": "2001-08-03T16:52:01Z"}, "message": "gcse.c: Include df.h for use as a dataflow analyzer.\n\n2001-07-16  Daniel Berlin  <dan@cgsoftware.com>\n\n        * gcse.c: Include df.h for use as a dataflow analyzer.\n        Remove regvec.\n        Declaration of reg_set_info: gone.\n        New df_analyzer variable used by store motion.\n        (reg_set_info): Deleted.\n        (mark_mem_regs): New function, analyze regs used by a mem.\n        (store_ops_ok): Use dataflow analyzer results to determine if\n        necessary regs are changed in the block.\n        (find_moveable_store): Remove check for symbol ref, we can handle\n        much more complex expressions now.\n        (compute_store_table): Remove most of the code, it's unnecessary\n        now that the dataflow analyzer records the info for us.\n        (store_killed_after): Add parameter to say whether to do the\n        store_ops_okay test, used to speed up testing when we already know\n        the answer, and just want to know if the store itself was killed.\n        (build_store_vector): Largely rewritten to calculate the various\n        vectors properly, and somewhat optimized.\n        (store_motion): Init the df_analyzer, get REG_DEF chains.\n        Also handle trapping expressions (since mems almost always trap)\n        (simple_mem): Redefine what a simple mem is.\n\nFrom-SVN: r44603", "tree": {"sha": "a59239bd456400b3c2358c1d1d7db15e2eba19d4", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/a59239bd456400b3c2358c1d1d7db15e2eba19d4"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/adfcce6198effc2aaeb4961ab7a8ec05723d6573", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/adfcce6198effc2aaeb4961ab7a8ec05723d6573", "html_url": "https://github.com/Rust-GCC/gccrs/commit/adfcce6198effc2aaeb4961ab7a8ec05723d6573", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/adfcce6198effc2aaeb4961ab7a8ec05723d6573/comments", "author": null, "committer": null, "parents": [{"sha": "667ccf73ebde1d2340f3e1ff41e4471724bb7126", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/667ccf73ebde1d2340f3e1ff41e4471724bb7126", "html_url": "https://github.com/Rust-GCC/gccrs/commit/667ccf73ebde1d2340f3e1ff41e4471724bb7126"}], "stats": {"total": 476, "additions": 335, "deletions": 141}, "files": [{"sha": "58e465821d36af233edec466311ab4aa70e66d55", "filename": "gcc/ChangeLog", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/adfcce6198effc2aaeb4961ab7a8ec05723d6573/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/adfcce6198effc2aaeb4961ab7a8ec05723d6573/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=adfcce6198effc2aaeb4961ab7a8ec05723d6573", "patch": "@@ -1,3 +1,26 @@\n+2001-07-16  Daniel Berlin  <dan@cgsoftware.com>\n+\n+        * gcse.c: Include df.h for use as a dataflow analyzer.\n+        Remove regvec.\n+        Declaration of reg_set_info: gone.\n+        New df_analyzer variable used by store motion.\n+        (reg_set_info): Deleted.\n+        (mark_mem_regs): New function, analyze regs used by a mem.\n+        (store_ops_ok): Use dataflow analyzer results to determine if\n+        necessary regs are changed in the block.\n+        (find_moveable_store): Remove check for symbol ref, we can handle\n+        much more complex expressions now.\n+        (compute_store_table): Remove most of the code, it's unnecessary\n+        now that the dataflow analyzer records the info for us.\n+        (store_killed_after): Add parameter to say whether to do the\n+        store_ops_okay test, used to speed up testing when we already know\n+        the answer, and just want to know if the store itself was killed.\n+        (build_store_vector): Largely rewritten to calculate the various\n+        vectors properly, and somewhat optimized.\n+        (store_motion): Init the df_analyzer, get REG_DEF chains. \n+        Also handle trapping expressions (since mems almost always trap)\n+        (simple_mem): Redefine what a simple mem is.\n+\n 2001-08-03  DJ Delorie  <dj@redhat.com>\n \n \t* ifcvt.c (noce_get_alt_condition): Don't make an auxiliary"}, {"sha": "349eae660cfdeedb5805ae015b2431a717528fea", "filename": "gcc/gcse.c", "status": "modified", "additions": 312, "deletions": 141, "changes": 453, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/adfcce6198effc2aaeb4961ab7a8ec05723d6573/gcc%2Fgcse.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/adfcce6198effc2aaeb4961ab7a8ec05723d6573/gcc%2Fgcse.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgcse.c?ref=adfcce6198effc2aaeb4961ab7a8ec05723d6573", "patch": "@@ -160,8 +160,8 @@ Boston, MA 02111-1307, USA.  */\n #include \"expr.h\" \n #include \"ggc.h\"\n #include \"params.h\"\n-\n #include \"obstack.h\"\n+#include \"df.h\"\n #define obstack_chunk_alloc gmalloc\n #define obstack_chunk_free free\n \n@@ -305,6 +305,10 @@ static char can_copy_p[(int) NUM_MACHINE_MODES];\n /* Non-zero if can_copy_p has been initialized.  */\n static int can_copy_init_p;\n \n+/* Dataflow analyzer  */\n+struct df *df_analyzer;\n+\n+\n struct reg_use {rtx reg_rtx; };\n \n /* Hash table of expressions.  */\n@@ -466,8 +470,8 @@ struct ls_expr\n {\n   struct expr * expr;\t\t/* Gcse expression reference for LM.  */\n   rtx pattern;\t\t\t/* Pattern of this mem.  */\n-  rtx loads;\t\t\t/* INSN list of loads seen.  */\n-  rtx stores;\t\t\t/* INSN list of stores seen.  */\n+  rtx loads;\t\t\t/* INSN list for where load appears */\n+  rtx stores;\t\t        /* INSN list for where store appears */\n   struct ls_expr * next;\t/* Next in the list.  */\n   int invalid;\t\t\t/* Invalid for some reason.  */\n   int index;\t\t\t/* If it maps to a bitmap index.  */\n@@ -676,14 +680,13 @@ static void invalidate_any_buried_refs\tPARAMS ((rtx));\n static void compute_ld_motion_mems\tPARAMS ((void)); \n static void trim_ld_motion_mems\t\tPARAMS ((void));\n static void update_ld_motion_stores\tPARAMS ((struct expr *));\n-static void reg_set_info\t\tPARAMS ((rtx, rtx, void *));\n-static int store_ops_ok\t\t\tPARAMS ((rtx, basic_block));\n+static int store_ops_ok\t\t\tPARAMS ((rtx, basic_block, rtx, int));\n static void find_moveable_store\t\tPARAMS ((rtx));\n static int compute_store_table\t\tPARAMS ((void));\n static int load_kills_store\t\tPARAMS ((rtx, rtx));\n static int find_loads\t\t\tPARAMS ((rtx, rtx));\n static int store_killed_in_insn\t\tPARAMS ((rtx, rtx));\n-static int store_killed_after\t\tPARAMS ((rtx, rtx, basic_block));\n+static int store_killed_after\t\tPARAMS ((rtx, rtx, basic_block, int));\n static int store_killed_before\t\tPARAMS ((rtx, rtx, basic_block));\n static void build_store_vectors\t\tPARAMS ((void));\n static void insert_insn_start_bb\tPARAMS ((rtx, basic_block));\n@@ -1466,7 +1469,6 @@ mems_conflict_for_gcse_p (dest, setter, data)\n      elsewhere.  */\n   if (GET_CODE (dest) != MEM)\n     return;\n-\n   /* If we are setting a MEM in our list of specially recognized MEMs,\n      don't mark as killed this time.  */ \n   \n@@ -1476,7 +1478,6 @@ mems_conflict_for_gcse_p (dest, setter, data)\n \tgcse_mems_conflict_p = 1;\n       return;\n     }\n-\n   if (true_dependence (dest, GET_MODE (dest), gcse_mem_operand,\n \t\t       rtx_addr_varies_p))\n     gcse_mems_conflict_p = 1;\n@@ -1754,6 +1755,7 @@ hash_expr_1 (x, mode, do_not_record_p)\n \thash += hash_string_1 (XSTR (x, i));\n       else if (fmt[i] == 'i')\n \thash += (unsigned int) XINT (x, i);\n+      else if (fmt[i] == 't');\n       else\n \tabort ();\n     }\n@@ -1912,8 +1914,9 @@ expr_equiv_p (x, y)\n \tbreak;\n \n \tcase '0':\n+\tcase 't':\n \t  break;\n-\n+\t\n \tdefault:\n \t  abort ();\n \t}\n@@ -5896,9 +5899,9 @@ static void\n free_ldst_entry (ptr)\n      struct ls_expr * ptr;\n {\n-  free_INSN_LIST_list (& ptr->loads);\n-  free_INSN_LIST_list (& ptr->stores);\n \n+  free_INSN_LIST_list (&ptr->stores);\n+  free_INSN_LIST_list (&ptr->loads);\n   free (ptr);\n }\n \n@@ -6019,10 +6022,11 @@ simple_mem (x)\n   \n   if (GET_MODE (x) == BLKmode)\n     return 0;\n-\n-  if (!rtx_varies_p (XEXP (x, 0), 0))\n+#if 0\n+  /* See comment in find_moveable_store */\n+  if (!rtx_addr_varies_p (XEXP (x, 0), 0))\n     return 1;\n-  \n+#endif\n   return 0;\n }\n \n@@ -6104,7 +6108,6 @@ compute_ld_motion_mems ()\n \t\t      /* Make sure there isn't a buried load somewhere.  */\n \t\t      invalidate_any_buried_refs (src);\n \t\t    }\n-\t\t  \n \t\t  /* Check for stores. Don't worry about aliased ones, they\n \t\t     will block any movement we might do later. We only care\n \t\t     about this exact pattern since those are the only\n@@ -6251,37 +6254,59 @@ update_ld_motion_stores (expr)\n \f\n /* Store motion code.  */\n \n-/* This is used to communicate the target bitvector we want to use in the \n-   reg_set_info routine when called via the note_stores mechanism.  */\n-static sbitmap * regvec;\n-\n /* Used in computing the reverse edge graph bit vectors.  */\n static sbitmap * st_antloc;\n \n /* Global holding the number of store expressions we are dealing with.  */\n static int num_stores;\n \n-/* Checks to set if we need to mark a register set. Called from note_stores.  */\n \n-static void\n-reg_set_info (dest, setter, data)\n-     rtx dest, setter ATTRIBUTE_UNUSED;\n-     void * data ATTRIBUTE_UNUSED;\n+/* Mark which registers are used by the mem, in the sbitmap used. */\n+static int\n+mark_mem_regs (x, used)\n+     rtx x;\n+     sbitmap used;\n {\n-  if (GET_CODE (dest) == SUBREG)\n-    dest = SUBREG_REG (dest);\n+  register const char *fmt;\n+  int i, j;\n \n-  if (GET_CODE (dest) == REG)\n-    SET_BIT (*regvec, REGNO (dest));\n+  if (GET_CODE (x) == REG)\n+    {\n+      if (!TEST_BIT (used, REGNO (x)))\n+\t{\n+\t  SET_BIT (used, REGNO (x));\n+\t  return 1;\n+}\n+      return 0;\n+    }\n+\n+  fmt = GET_RTX_FORMAT (GET_CODE (x));\n+  for (i = GET_RTX_LENGTH (GET_CODE (x)) - 1; i >= 0; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\t{\n+\t  if (mark_mem_regs (XEXP (x, i),used))\n+\t    return 1;\n+\t}\n+      else if (fmt[i] == 'E')\n+\tfor (j = XVECLEN (x, i) - 1; j >= 0; j--)\n+\t  if (mark_mem_regs (XVECEXP (x, i, j),used))\n+\t    return 1;\n+    }\n+\n+  return 0;\n }\n \n+\n /* Return non-zero if the register operands of expression X are killed \n-   anywhere in basic block BB.  */\n+   before/after insn in basic block BB.  */\n \n static int\n-store_ops_ok (x, bb)\n+store_ops_ok (x, bb,insn, before)\n      rtx x;\n      basic_block bb;\n+     rtx insn;\n+     int before;\n {\n   int i;\n   enum rtx_code code;\n@@ -6297,10 +6322,46 @@ store_ops_ok (x, bb)\n   switch (code)\n     {\n     case REG:\n-\t/* If a reg has changed after us in this\n-\t   block, the operand has been killed.  */\n-\treturn TEST_BIT (reg_set_in_block[bb->index], REGNO (x));\n+\t{\n+\t  /* Okay, since the reg def chains are ordered by bb/insn\n+\t     (since that's how it calculates them, and even if it didn't,\n+\t     we could just sort them), we just walk until we find a def\n+\t     in our BB, then walk until we find a def after/before our\n+\t     insn, and if we find a reg def after/before our insn, in the\n+\t     same bb, we return the approriate value.  If there is no\n+\t     such def, to prevent walking *every* reg def, we stop once\n+\t     we are out of our BB again. */\n+\t  struct df_link *currref;\n+\t  bool thereyet=FALSE;\n+\t  for (currref = df_analyzer->regs[REGNO(x)].defs;\n+\t       currref;\n+\t       currref = currref->next)\n+\t    {\n+\t      if (! (DF_REF_BB (currref->ref)  == bb))\n+\t\t{\n+\t\t  if (!thereyet)\n+\t\t    continue;\n+\t\t  else \n+\t\t    return 1;\n+\t\t}\n+\t      if (before)\n+\t\t{\n+\t\t  if (INSN_UID (DF_REF_INSN (currref->ref)) >= INSN_UID (insn))\n+\t\t    continue;\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  if (INSN_UID (DF_REF_INSN (currref->ref)) < INSN_UID (insn))\n+\t\t    continue;\n+\t\t}\n+\t      thereyet = TRUE;\n+\t      if (DF_REF_TYPE (currref->ref) == DF_REF_REG_DEF)\n+\t\treturn 0;\n+\t    }\n+\t  return 1;\n+\t}\n \n+\t\n     case MEM:\n       x = XEXP (x, 0);\n       goto repeat;\n@@ -6344,7 +6405,7 @@ store_ops_ok (x, bb)\n \t      goto repeat;\n \t    }\n \t  \n-\t  if (! store_ops_ok (tem, bb))\n+\t  if (! store_ops_ok (tem, bb, insn, before))\n \t    return 0;\n \t}\n       else if (fmt[i] == 'E')\n@@ -6353,7 +6414,7 @@ store_ops_ok (x, bb)\n \t  \n \t  for (j = 0; j < XVECLEN (x, i); j++)\n \t    {\n-\t      if (! store_ops_ok (XVECEXP (x, i, j), bb))\n+\t      if (! store_ops_ok (XVECEXP (x, i, j), bb, insn, before))\n \t\treturn 0;\n \t    }\n \t}\n@@ -6362,7 +6423,9 @@ store_ops_ok (x, bb)\n   return 1;\n }\n \n-/* Determine whether insn is MEM store pattern that we will consider moving.  */\n+/* Determine whether insn is MEM store pattern that we will consider\n+   moving.  We'll consider moving pretty much anything that we can\n+   move safely. */\n \n static void\n find_moveable_store (insn)\n@@ -6371,6 +6434,9 @@ find_moveable_store (insn)\n   struct ls_expr * ptr;\n   rtx dest = PATTERN (insn);\n \n+  /* It's it's not a set, it's not a mem store we want to consider.\n+     Also, if it's an ASM, we certainly don't want to try to touch\n+     it. */\n   if (GET_CODE (dest) != SET\n       || GET_CODE (SET_SRC (dest)) == ASM_OPERANDS)\n     return;\n@@ -6379,66 +6445,43 @@ find_moveable_store (insn)\n   \n   if (GET_CODE (dest) != MEM || MEM_VOLATILE_P (dest)\n       || GET_MODE (dest) == BLKmode)\n-    return;\n-\n-  if (GET_CODE (XEXP (dest, 0)) != SYMBOL_REF)\n       return;\n-\n-  if (rtx_varies_p (XEXP (dest, 0), 0))\n+#if 0\n+  /* ??? Is this conservative, or just correct? We get more\n+     *candidates* without it, but i don't think we ever remove any\n+     stores where the address did vary. */\n+  if (rtx_addr_varies_p (XEXP (dest, 0), 0))\n     return;\n-\n+#endif\n   ptr = ldst_entry (dest);\n   ptr->stores = alloc_INSN_LIST (insn, ptr->stores);\n }\n \n-/* Perform store motion. Much like gcse, except we move expressions the\n-   other way by looking at the flowgraph in reverse.  */\n+/* Perform store motion. \n+   Store motion is modeled as a lazy code motion problem, like PRE is\n+   above. The main diffence is that we want to move stores down as far\n+   as possible, so we have LCM work on the reverse flowgraph. */\n \n static int\n compute_store_table ()\n {\n   int bb, ret;\n-  unsigned regno;\n   rtx insn, pat;\n-\n   max_gcse_regno = max_reg_num ();\n \n-  reg_set_in_block = (sbitmap *) sbitmap_vector_alloc (n_basic_blocks,\n-\t\t\t\t\t\t       max_gcse_regno);\n-  sbitmap_vector_zero (reg_set_in_block, n_basic_blocks);\n   pre_ldst_mems = 0;\n-\n   /* Find all the stores we care about.  */\n   for (bb = 0; bb < n_basic_blocks; bb++)\n     {\n-      regvec = & (reg_set_in_block[bb]);\n       for (insn = BLOCK_END (bb);\n \t   insn && insn != PREV_INSN (BLOCK_HEAD (bb));\n \t   insn = PREV_INSN (insn))\n \t{\n-#ifdef NON_SAVING_SETJMP \n-\t  if (NON_SAVING_SETJMP && GET_CODE (insn) == NOTE\n-\t      && NOTE_LINE_NUMBER (insn) == NOTE_INSN_SETJMP)\n-\t    {\n-\t      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n-\t\tSET_BIT (reg_set_in_block[bb], regno);\n-\t      continue;\n-\t    }\n-#endif\n-\t/* Ignore anything that is not a normal insn.  */\n-\tif (GET_RTX_CLASS (GET_CODE (insn)) != 'i')\n+\t  /* Ignore anything that is not a normal insn.  */\n+\t  if (!INSN_P (insn))\n \t    continue;\n \n-\t  if (GET_CODE (insn) == CALL_INSN)\n-\t    {\n-\t      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n-\t\tif (TEST_HARD_REG_BIT (regs_invalidated_by_call, regno))\n-\t\t  SET_BIT (reg_set_in_block[bb], regno);\n-\t    }\n-\t  \n \t  pat = PATTERN (insn);\n-\t  note_stores (pat, reg_set_info, NULL);\n-\t  \n \t  /* Now that we've marked regs, look for stores.  */\n \t  if (GET_CODE (pat) == SET)\n \t    find_moveable_store (insn);\n@@ -6456,7 +6499,8 @@ compute_store_table ()\n   return ret;\n }\n \n-/* Check to see if the load X is aliased with STORE_PATTERN.  */\n+/* Check to see if the load X is aliased with STORE_PATTERN. \n+   If it is, it means that load kills the store.*/\n \n static int\n load_kills_store (x, store_pattern)\n@@ -6467,8 +6511,8 @@ load_kills_store (x, store_pattern)\n   return 0;\n }\n \n-/* Go through the entire insn X, looking for any loads which might alias \n-   STORE_PATTERN.  Return 1 if found.  */\n+/* Go through the entire insn X, looking for any loads which might\n+   alias, and therefore, kill, STORE_PATTERN.  Return 1 if found.  */\n \n static int\n find_loads (x, store_pattern)\n@@ -6524,9 +6568,10 @@ store_killed_in_insn (x, insn)\n       rtx pat = PATTERN (insn);\n       /* Check for memory stores to aliased objects.  */\n       if (GET_CODE (SET_DEST (pat)) == MEM && !expr_equiv_p (SET_DEST (pat), x))\n-\t/* pretend its a load and check for aliasing.  */\n+\t{\n \tif (find_loads (SET_DEST (pat), x))\n \t  return 1;\n+\t}\n       return find_loads (SET_SRC (pat), x);\n     }\n   else\n@@ -6537,31 +6582,31 @@ store_killed_in_insn (x, insn)\n    within basic block BB.  */\n \n static int \n-store_killed_after (x, insn, bb)\n+store_killed_after (x, insn, bb, testops)\n      rtx x, insn;\n      basic_block bb;\n+     int testops;\n {\n    rtx last = bb->end;\n    \n    if (insn == last)\n      return 0;\n-\n-  /* Check if the register operands of the store are OK in this block.\n-     Note that if registers are changed ANYWHERE in the block, we'll \n-     decide we can't move it, regardless of whether it changed above \n-     or below the store. This could be improved by checking the register\n-     operands while lookinng for aliasing in each insn.  */\n-  if (!store_ops_ok (XEXP (x, 0), bb))\n+   \n+   if (testops)\n+     /* Check if the register operands of the store are OK in this block.*/\n+     if (!store_ops_ok (XEXP (x, 0), bb, insn, 0))\n     return 1;\n \n-   for ( ; insn && insn != NEXT_INSN (last); insn = NEXT_INSN (insn))\n+   for ( ; \n+\t insn && insn != NEXT_INSN (last); \n+\t insn = NEXT_INSN (insn))\n      if (store_killed_in_insn (x, insn))\n        return 1;\n    \n   return 0;\n }\n \n-/* Returns 1 if the expression X is loaded or clobbered on or before INSN\n+/* Returns 1 if the expression X is loaded or clobbered before INSN\n    within basic block BB.  */\n static int \n store_killed_before (x, insn, bb)\n@@ -6572,16 +6617,14 @@ store_killed_before (x, insn, bb)\n \n    if (insn == first)\n      return store_killed_in_insn (x, insn);\n-   \n-  /* Check if the register operands of the store are OK in this block.\n-     Note that if registers are changed ANYWHERE in the block, we'll \n-     decide we can't move it, regardless of whether it changed above \n-     or below the store. This could be improved by checking the register\n-     operands while lookinng for aliasing in each insn.  */\n-  if (!store_ops_ok (XEXP (x, 0), bb))\n+   /* Check if the register operands of the store are OK in this block.*/\n+   if (!store_ops_ok (XEXP (x, 0), bb, insn, 1))\n     return 1;\n \n-   for ( ; insn && insn != PREV_INSN (first); insn = PREV_INSN (insn))\n+   for (insn = PREV_INSN (insn) ; \n+\tinsn && insn != PREV_INSN (first); \n+\tinsn = PREV_INSN (insn))\n+     \n      if (store_killed_in_insn (x, insn))\n        return 1;\n    \n@@ -6598,9 +6641,11 @@ static void\n build_store_vectors () \n {\n   basic_block bb;\n-  int b;\n+  int b,i,j;\n   rtx insn, st;\n   struct ls_expr * ptr;\n+  sbitmap tested, *result;\n+  sbitmap used;\n \n   /* Build the gen_vector. This is any store in the table which is not killed\n      by aliasing later in its block.  */\n@@ -6609,7 +6654,16 @@ build_store_vectors ()\n \n   st_antloc = (sbitmap *) sbitmap_vector_alloc (n_basic_blocks, num_stores);\n   sbitmap_vector_zero (st_antloc, n_basic_blocks);\n-\n+  \n+  /* Note: In case someone needs something to optimize about store\n+     motion, here's the next place to look.  We currently test one more\n+     basic block per store than necessary (at least).  Since we know, at\n+     the end of this for loop, whether a store was killed in one of the\n+     basic blocks (We know both whether it's killed before, and killed\n+     after, the insn in the bb it resides in. So unless the insn\n+     consists of multiple store/loads, we know whether it was killed\n+     in the entire bb), we could avoid testing it for kill and transp in\n+     the next for loop. */\n   for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n     { \n       /* Put all the stores into either the antic list, or the avail list,\n@@ -6621,8 +6675,7 @@ build_store_vectors ()\n \t{\n \t  insn = XEXP (st, 0);\n \t  bb = BLOCK_FOR_INSN (insn);\n-\t  \n-\t  if (!store_killed_after (ptr->pattern, insn, bb))\n+\t  if (!store_killed_after (ptr->pattern, insn, bb, 1))\n \t    {\n \t      /* If we've already seen an availale expression in this block,\n \t\t we can delete the one we saw already (It occurs earlier in\n@@ -6666,50 +6719,142 @@ build_store_vectors ()\n   ae_kill = (sbitmap *) sbitmap_vector_alloc (n_basic_blocks, num_stores);\n   sbitmap_vector_zero (ae_kill, n_basic_blocks);\n \n+\n   transp = (sbitmap *) sbitmap_vector_alloc (n_basic_blocks, num_stores);\n-  sbitmap_vector_zero (transp, n_basic_blocks);\n+  sbitmap_vector_ones (transp, n_basic_blocks);\n+\n+  tested = sbitmap_alloc (max_gcse_regno);\n+  sbitmap_zero (tested);\n+  result = sbitmap_vector_alloc (n_basic_blocks, max_gcse_regno);\n+  sbitmap_vector_zero (result, n_basic_blocks);\n+  used = sbitmap_alloc (max_gcse_regno);\n+  sbitmap_zero (used);\n+\n+  /* This whole big nasty thing computes kill and transparent.\n+     It's done in this nasty way because profiling showed store motion\n+     taking twice as long as GCSE, with the cause being 1 million calls\n+     to store_ops_ok taking 30% of the entire runtime of the\n+     compiler. \n+     Since store most expressions use the same registers, there's no\n+     point in checking them 8 million times for the same basic blocks. If\n+     they weren't okay in a BB the last time we checked, they won't be\n+     okay now. Since we check all the bb's on each iteration, we don't\n+     need a vector for which registers we've tested, just the results.\n+     We then proceed to use the results of what store_ops_ok was for a\n+     given reg and bb, and if the results were a kill, we don't even need\n+     to check if the store was killed in the basic block, it'll be\n+     in the kill set because it's regs changed between here and there.\n+\n+     \n+     If the whole store had no registers, we just skip store_ops_okay\n+     anyway (since it's checking reg operands), and proceed to see if\n+     it's okay in each bb, setting the approriate bits.\n+\n+     With this in place, we now take almost no time at all to perform\n+     store motion. (It's not on the first page of the profile, it\n+     takes less than a second).\n+     \n+  */\n \n   for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr (ptr))\n-    for (b = 0; b < n_basic_blocks; b++)\n       {\n-\tif (store_killed_after (ptr->pattern, BLOCK_HEAD (b), BASIC_BLOCK (b)))\n+      /* Make sure we don't have a load-only expr, which we never seem\n+\t to, but i don't think there's actually a guarantee */\n+      if (ptr->stores != NULL)\n \t  {\n-\t    /* The anticipatable expression is not killed if it's gen'd. */\n-\t    /*\n-\t      We leave this check out for now. If we have a code sequence \n-\t      in a block which looks like:\n-\t\t\tST MEMa = x\n-\t\t\tL     y = MEMa\n-\t\t\tST MEMa = z\n-\t      We should flag this as having an ANTIC expression, NOT\n-\t      transparent, NOT killed, and AVAIL.\n-\t      Unfortunately, since we haven't re-written all loads to\n-\t      use the reaching reg, we'll end up doing an incorrect \n-\t      Load in the middle here if we push the store down. It happens in\n-\t\t    gcc.c-torture/execute/960311-1.c with -O3\n-\t      If we always kill it in this case, we'll sometimes do\n-\t      uneccessary work, but it shouldn't actually hurt anything.\n-\t    if (!TEST_BIT (ae_gen[b], ptr->index)).  */\n-\t    SET_BIT (ae_kill[b], ptr->index);\n-\t  }\n-\telse\n-\t  SET_BIT (transp[b], ptr->index);\n-      }\n-\n-  /* Any block with no exits calls some non-returning function, so\n-     we better mark the store killed here, or we might not store to\n-     it at all.  If we knew it was abort, we wouldn't have to store,\n-     but we don't know that for sure.  */\n-  if (gcse_file) \n-    {\n-      fprintf (gcse_file, \"ST_avail and ST_antic (shown under loads..)\\n\");\n-      print_ldst_list (gcse_file);\n-      dump_sbitmap_vector (gcse_file, \"st_antloc\", \"\", st_antloc, n_basic_blocks);\n-      dump_sbitmap_vector (gcse_file, \"st_kill\", \"\", ae_kill, n_basic_blocks);\n-      dump_sbitmap_vector (gcse_file, \"Transpt\", \"\", transp, n_basic_blocks);\n-      dump_sbitmap_vector (gcse_file, \"st_avloc\", \"\", ae_gen, n_basic_blocks);\n+\t  /* First mark the regs used by the mem */\n+\t  mark_mem_regs (ptr->pattern, used);\n+\t  /* Now see if it had any regs */\n+\t  if (!(sbitmap_first_set_bit (used) == -1))\n+\t    {\n+\t      /* For each register, see if we've tested it */\n+\t      EXECUTE_IF_SET_IN_SBITMAP (used, 0, i, \n+\t      {\n+\t\tif (TEST_BIT (tested, i))\n+\t\t  {\n+\t\t    /* Already tested the register, so check the\n+\t\t       result, and if we had an okay result, check the\n+\t\t       store itself. */\n+\t\t    for (j = 0; j < n_basic_blocks; j++)\n+\t\t      {\n+\t\t\tif (!TEST_BIT (result[j], i) \n+\t\t\t    || store_killed_after (ptr->pattern, BLOCK_HEAD (j), \n+\t\t\t\t\t\t   BASIC_BLOCK (j), FALSE))\n+\t\t\t  {\n+\t\t\t    SET_BIT (ae_kill[j], ptr->index);\n+\t\t\t    if (!TEST_BIT (ae_gen[j], ptr->index)\n+\t\t\t\t|| !TEST_BIT (st_antloc[j], ptr->index))\n+\t\t\t      RESET_BIT (transp[j], ptr->index);\n+\t\t\t  }\n+\t\t      }\n+\t\t  }\n+\t\telse\n+\t\t  {\n+\t\t    /* We haven't tested it yet, so mark it tested,\n+\t\t       and perform the tests */\n+\t\t    SET_BIT (tested, i);\n+\t\t    /* Check if it's okay in each BB */\n+\t\t    for (j = 0; j < n_basic_blocks; j++)\n+\t\t      {\n+\t\t\tif (store_ops_ok (XEXP (ptr->pattern, 0), \n+\t\t\t\t\t  BASIC_BLOCK (j), BLOCK_HEAD (j), 0))\n+\t\t\t  {\n+\t\t\t    SET_BIT (result[j], ptr->index);\n+\t\t\t  }\n+\t\t\telse\n+\t\t\t  {\n+\t\t\t    /* It's not okay, so it's killed and maybe\n+\t\t\t       not transparent */\n+\t\t\t    SET_BIT (ae_kill[j], ptr->index);\n+\t\t\t    if (!TEST_BIT (ae_gen[j], ptr->index)\n+\t\t\t\t|| !TEST_BIT (st_antloc[j], ptr->index))\n+\t\t\t      {\n+\t\t\t\tRESET_BIT (transp[j], ptr->index);\n+\t\t\t      }\n+\t\t\t    continue;\n+\t\t\t  }\n+\t\t\t/* The ops were okay, so check the store\n+\t\t\t   itself */\n+\t\t\tif (store_killed_after (ptr->pattern, BLOCK_HEAD (j), \n+\t\t\t\t\t\tBASIC_BLOCK (j), FALSE))\n+\t\t\t  {\n+\t\t\t    SET_BIT (ae_kill[j], ptr->index);\n+\t\t\t    if (!TEST_BIT (ae_gen[j], ptr->index)\n+\t\t\t\t|| !TEST_BIT (st_antloc[j], ptr->index))\n+\t\t\t      {\n+\t\t\t\tRESET_BIT (transp[j], ptr->index);\n+\t\t\t      }\n+\t\t\t  }\n+\t\t      }\n+\t\t  }\n+\t      });\n+\t      /* Reset the used list */\n+\t      sbitmap_zero (used);\n+\t    }\n+\t  /* If it had no registers, we come here, and do the\n+\t     approriate testing */\n+\t  else\n+\t    {\n+\t      for (j = 0; j < n_basic_blocks; j++)\n+\t\t{\n+\t\t  if (store_killed_after (ptr->pattern, BLOCK_HEAD (j), \n+\t\t\t\t\t  BASIC_BLOCK (j), FALSE))\n+\t\t    {\n+\t\t      SET_BIT (ae_kill[j], ptr->index);\n+\t\t      if (!TEST_BIT (ae_gen[j], ptr->index)\n+\t\t\t  || !TEST_BIT (st_antloc[j], ptr->index))\n+\t\t\t{\n+\t\t\t  RESET_BIT (transp[j], ptr->index);\n+\t\t\t}\n+\t\t    }\n+\t\t}\n+\t    }  \n     }\n }\n+  sbitmap_free (tested);\n+  sbitmap_free (used);\n+  sbitmap_vector_free (result);\n+}\n \n /* Insert an instruction at the begining of a basic block, and update \n    the BLOCK_HEAD if needed.  */\n@@ -6888,7 +7033,6 @@ static void\n free_store_memory ()\n {\n   free_ldst_mems ();\n-  \n   if (ae_gen)\n     sbitmap_vector_free (ae_gen);\n   if (ae_kill)\n@@ -6901,8 +7045,6 @@ free_store_memory ()\n     sbitmap_vector_free (pre_insert_map);\n   if (pre_delete_map)\n     sbitmap_vector_free (pre_delete_map);\n-  if (reg_set_in_block)\n-    sbitmap_vector_free (reg_set_in_block);\n   \n   ae_gen = ae_kill = transp = st_antloc = NULL;\n   pre_insert_map = pre_delete_map = reg_set_in_block = NULL;\n@@ -6916,8 +7058,10 @@ store_motion ()\n {\n   int x;\n   struct ls_expr * ptr;\n-  int update_flow = 0;\n+  sbitmap trapping_expr;\n+  int i;\n \n+  int update_flow = 0;\n   if (gcse_file)\n     {\n       fprintf (gcse_file, \"before store motion\\n\");\n@@ -6926,12 +7070,13 @@ store_motion ()\n \n \n   init_alias_analysis ();\n-\n+  df_analyzer = df_init();\n+  df_analyse (df_analyzer, 0,   DF_RD_CHAIN | DF_HARD_REGS);\n   /* Find all the stores that are live to the end of their block.  */\n   num_stores = compute_store_table ();\n   if (num_stores == 0)\n     {\n-      sbitmap_vector_free (reg_set_in_block);\n+      df_finish (df_analyzer);\n       end_alias_analysis ();\n       return;\n     }\n@@ -6940,6 +7085,31 @@ store_motion ()\n   add_noreturn_fake_exit_edges ();\n   build_store_vectors ();\n \n+  /* Collect expressions which might trap.  */\n+  trapping_expr = sbitmap_alloc (num_stores);\n+  sbitmap_zero (trapping_expr);\n+  for (ptr = first_ls_expr (); ptr != NULL; ptr = next_ls_expr(ptr))\n+    {\n+\t    if (may_trap_p (ptr->pattern))\n+\t\t    SET_BIT (trapping_expr, ptr->index);\n+    }\n+  for (i = 0; i < n_basic_blocks; i++)\n+    {\n+      edge e;\n+\n+      /* If the current block is the destination of an abnormal edge, we\n+\t kill all trapping expressions because we won't be able to properly\n+\t place the instruction on the edge.  So make them neither\n+\t anticipatable nor transparent.  This is fairly conservative.  */\n+      for (e = BASIC_BLOCK (i)->pred; e ; e = e->pred_next)\n+\tif (e->flags & EDGE_ABNORMAL)\n+\t  {\n+\t    sbitmap_difference (st_antloc[i], st_antloc[i], trapping_expr);\n+\t    sbitmap_difference (transp[i], transp[i], trapping_expr);\n+\t    break;\n+\t  }\n+    }\n+\n   edge_list = pre_edge_rev_lcm (gcse_file, num_stores, transp, ae_gen, \n \t\t\t\tst_antloc, ae_kill, &pre_insert_map, \n \t\t\t\t&pre_delete_map);\n@@ -6958,9 +7128,10 @@ store_motion ()\n \n   if (update_flow)\n     commit_edge_insertions ();\n-\n+  sbitmap_free (trapping_expr);\n   free_store_memory ();\n   free_edge_list (edge_list);\n   remove_fake_edges ();\n   end_alias_analysis ();\n+  df_finish (df_analyzer);\n }"}]}