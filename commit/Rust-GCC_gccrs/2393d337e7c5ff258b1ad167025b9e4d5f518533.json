{"sha": "2393d337e7c5ff258b1ad167025b9e4d5f518533", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MjM5M2QzMzdlN2M1ZmYyNThiMWFkMTY3MDI1YjllNGQ1ZjUxODUzMw==", "commit": {"author": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2017-01-17T09:44:17Z"}, "committer": {"name": "Jakub Jelinek", "email": "jakub@gcc.gnu.org", "date": "2017-01-17T09:44:17Z"}, "message": "configfrag.ac: For --without-cuda-driver don't initialize CUDA_DRIVER_INCLUDE nor CUDA_DRIVER_LIB.\n\n\t* plugin/configfrag.ac: For --without-cuda-driver don't initialize\n\tCUDA_DRIVER_INCLUDE nor CUDA_DRIVER_LIB.  If both\n\tCUDA_DRIVER_INCLUDE and CUDA_DRIVER_LIB are empty and linking small\n\tcuda program fails, define PLUGIN_NVPTX_DYNAMIC to 1 and use\n\tplugin/include/cuda as include dir and -ldl instead of -lcuda as\n\tlibrary to link ptx plugin against.\n\t* plugin/plugin-nvptx.c: Include dlfcn.h if PLUGIN_NVPTX_DYNAMIC.\n\t(CUDA_CALLS): Define.\n\t(cuda_lib, cuda_lib_inited): New variables.\n\t(init_cuda_lib): New function.\n\t(CUDA_CALL_PREFIX): Define.\n\t(CUDA_CALL_ERET, CUDA_CALL_ASSERT): Use CUDA_CALL_PREFIX.\n\t(CUDA_CALL): Use FN instead of (FN).\n\t(CUDA_CALL_NOCHECK): Define.\n\t(cuda_error, fini_streams_for_device, select_stream_for_async,\n\tnvptx_attach_host_thread_to_device, nvptx_open_device, link_ptx,\n\tevent_gc, nvptx_exec, nvptx_async_test, nvptx_async_test_all,\n\tnvptx_wait_all, nvptx_set_clocktick, GOMP_OFFLOAD_unload_image,\n\tnvptx_stacks_alloc, nvptx_stacks_free, GOMP_OFFLOAD_run): Use\n\tCUDA_CALL_NOCHECK.\n\t(nvptx_init): Call init_cuda_lib, if it fails, return false.  Use\n\tCUDA_CALL_NOCHECK.\n\t(nvptx_get_num_devices): Call init_cuda_lib, if it fails, return 0.\n\tUse CUDA_CALL_NOCHECK.\n\t* plugin/cuda/cuda.h: New file.\n\t* config.h.in: Regenerated.\n\t* configure: Regenerated.\n\nFrom-SVN: r244522", "tree": {"sha": "eaf913e6c721a84f592e444c942b624bbb14fe06", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/eaf913e6c721a84f592e444c942b624bbb14fe06"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/2393d337e7c5ff258b1ad167025b9e4d5f518533", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2393d337e7c5ff258b1ad167025b9e4d5f518533", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2393d337e7c5ff258b1ad167025b9e4d5f518533", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2393d337e7c5ff258b1ad167025b9e4d5f518533/comments", "author": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "3c36aa6ba2be894d4092a6ce8129d39ef846c964", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3c36aa6ba2be894d4092a6ce8129d39ef846c964", "html_url": "https://github.com/Rust-GCC/gccrs/commit/3c36aa6ba2be894d4092a6ce8129d39ef846c964"}], "stats": {"total": 487, "additions": 418, "deletions": 69}, "files": [{"sha": "0e04c7416d147c97764ed5d8009b15fc3d5f2318", "filename": "libgomp/ChangeLog", "status": "modified", "additions": 28, "deletions": 0, "changes": 28, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2393d337e7c5ff258b1ad167025b9e4d5f518533/libgomp%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2393d337e7c5ff258b1ad167025b9e4d5f518533/libgomp%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgomp%2FChangeLog?ref=2393d337e7c5ff258b1ad167025b9e4d5f518533", "patch": "@@ -1,5 +1,33 @@\n 2017-01-17  Jakub Jelinek  <jakub@redhat.com>\n \n+\t* plugin/configfrag.ac: For --without-cuda-driver don't initialize\n+\tCUDA_DRIVER_INCLUDE nor CUDA_DRIVER_LIB.  If both\n+\tCUDA_DRIVER_INCLUDE and CUDA_DRIVER_LIB are empty and linking small\n+\tcuda program fails, define PLUGIN_NVPTX_DYNAMIC to 1 and use\n+\tplugin/include/cuda as include dir and -ldl instead of -lcuda as\n+\tlibrary to link ptx plugin against.\n+\t* plugin/plugin-nvptx.c: Include dlfcn.h if PLUGIN_NVPTX_DYNAMIC.\n+\t(CUDA_CALLS): Define.\n+\t(cuda_lib, cuda_lib_inited): New variables.\n+\t(init_cuda_lib): New function.\n+\t(CUDA_CALL_PREFIX): Define.\n+\t(CUDA_CALL_ERET, CUDA_CALL_ASSERT): Use CUDA_CALL_PREFIX.\n+\t(CUDA_CALL): Use FN instead of (FN).\n+\t(CUDA_CALL_NOCHECK): Define.\n+\t(cuda_error, fini_streams_for_device, select_stream_for_async,\n+\tnvptx_attach_host_thread_to_device, nvptx_open_device, link_ptx,\n+\tevent_gc, nvptx_exec, nvptx_async_test, nvptx_async_test_all,\n+\tnvptx_wait_all, nvptx_set_clocktick, GOMP_OFFLOAD_unload_image,\n+\tnvptx_stacks_alloc, nvptx_stacks_free, GOMP_OFFLOAD_run): Use\n+\tCUDA_CALL_NOCHECK.\n+\t(nvptx_init): Call init_cuda_lib, if it fails, return false.  Use\n+\tCUDA_CALL_NOCHECK.\n+\t(nvptx_get_num_devices): Call init_cuda_lib, if it fails, return 0.\n+\tUse CUDA_CALL_NOCHECK.\n+\t* plugin/cuda/cuda.h: New file.\n+\t* config.h.in: Regenerated.\n+\t* configure: Regenerated.\n+\n \tPR other/79046\n \t* configure.ac: Add GCC_BASE_VER.\n \t* Makefile.am (gcc_version): Use @get_gcc_base_ver@ instead of cat to"}, {"sha": "e7bc4d9737441e344eba04dcc118dd3e18eb44a3", "filename": "libgomp/config.h.in", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2393d337e7c5ff258b1ad167025b9e4d5f518533/libgomp%2Fconfig.h.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2393d337e7c5ff258b1ad167025b9e4d5f518533/libgomp%2Fconfig.h.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgomp%2Fconfig.h.in?ref=2393d337e7c5ff258b1ad167025b9e4d5f518533", "patch": "@@ -155,6 +155,10 @@\n /* Define to 1 if the NVIDIA plugin is built, 0 if not. */\n #undef PLUGIN_NVPTX\n \n+/* Define to 1 if the NVIDIA plugin should dlopen libcuda.so.1, 0 if it should\n+   be linked against it. */\n+#undef PLUGIN_NVPTX_DYNAMIC\n+\n /* Define if all infrastructure, needed for plugins, is supported. */\n #undef PLUGIN_SUPPORT\n "}, {"sha": "b7e9f40b85292526e136f272c42897eea8086015", "filename": "libgomp/configure", "status": "modified", "additions": 23, "deletions": 7, "changes": 30, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2393d337e7c5ff258b1ad167025b9e4d5f518533/libgomp%2Fconfigure", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2393d337e7c5ff258b1ad167025b9e4d5f518533/libgomp%2Fconfigure", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgomp%2Fconfigure?ref=2393d337e7c5ff258b1ad167025b9e4d5f518533", "patch": "@@ -15299,10 +15299,12 @@ if test \"${with_cuda_driver_lib+set}\" = set; then :\n   withval=$with_cuda_driver_lib;\n fi\n \n-if test \"x$with_cuda_driver\" != x; then\n-  CUDA_DRIVER_INCLUDE=$with_cuda_driver/include\n-  CUDA_DRIVER_LIB=$with_cuda_driver/lib\n-fi\n+case \"x$with_cuda_driver\" in\n+  x | xno) ;;\n+  *) CUDA_DRIVER_INCLUDE=$with_cuda_driver/include\n+     CUDA_DRIVER_LIB=$with_cuda_driver/lib\n+     ;;\n+esac\n if test \"x$with_cuda_driver_include\" != x; then\n   CUDA_DRIVER_INCLUDE=$with_cuda_driver_include\n fi\n@@ -15320,6 +15322,7 @@ PLUGIN_NVPTX=0\n PLUGIN_NVPTX_CPPFLAGS=\n PLUGIN_NVPTX_LDFLAGS=\n PLUGIN_NVPTX_LIBS=\n+PLUGIN_NVPTX_DYNAMIC=0\n \n \n \n@@ -15426,9 +15429,17 @@ rm -f core conftest.err conftest.$ac_objext \\\n \tLIBS=$PLUGIN_NVPTX_save_LIBS\n \tcase $PLUGIN_NVPTX in\n \t  nvptx*)\n-\t    PLUGIN_NVPTX=0\n-\t    as_fn_error \"CUDA driver package required for nvptx support\" \"$LINENO\" 5\n-\t    ;;\n+\t    if test \"x$CUDA_DRIVER_INCLUDE\" = x \\\n+\t       && test \"x$CUDA_DRIVER_LIB\" = x; then\n+\t      PLUGIN_NVPTX=1\n+\t      PLUGIN_NVPTX_CPPFLAGS='-I$(srcdir)/plugin/cuda'\n+\t      PLUGIN_NVPTX_LIBS='-ldl'\n+\t      PLUGIN_NVPTX_DYNAMIC=1\n+\t    else\n+\t      PLUGIN_NVPTX=0\n+\t      as_fn_error \"CUDA driver package required for nvptx support\" \"$LINENO\" 5\n+\t    fi\n+\t  ;;\n \tesac\n \t;;\n       hsa*)\n@@ -15513,6 +15524,11 @@ cat >>confdefs.h <<_ACEOF\n #define PLUGIN_NVPTX $PLUGIN_NVPTX\n _ACEOF\n \n+\n+cat >>confdefs.h <<_ACEOF\n+#define PLUGIN_NVPTX_DYNAMIC $PLUGIN_NVPTX_DYNAMIC\n+_ACEOF\n+\n  if test $PLUGIN_HSA = 1; then\n   PLUGIN_HSA_TRUE=\n   PLUGIN_HSA_FALSE='#'"}, {"sha": "c4a92795926fbe6662c61aa73e26ec30b6d57506", "filename": "libgomp/plugin/configfrag.ac", "status": "modified", "additions": 20, "deletions": 7, "changes": 27, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2393d337e7c5ff258b1ad167025b9e4d5f518533/libgomp%2Fplugin%2Fconfigfrag.ac", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2393d337e7c5ff258b1ad167025b9e4d5f518533/libgomp%2Fplugin%2Fconfigfrag.ac", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgomp%2Fplugin%2Fconfigfrag.ac?ref=2393d337e7c5ff258b1ad167025b9e4d5f518533", "patch": "@@ -58,10 +58,12 @@ AC_ARG_WITH(cuda-driver-include,\n AC_ARG_WITH(cuda-driver-lib,\n \t[AS_HELP_STRING([--with-cuda-driver-lib=PATH],\n \t\t[specify directory for the installed CUDA driver library])])\n-if test \"x$with_cuda_driver\" != x; then\n-  CUDA_DRIVER_INCLUDE=$with_cuda_driver/include\n-  CUDA_DRIVER_LIB=$with_cuda_driver/lib\n-fi\n+case \"x$with_cuda_driver\" in\n+  x | xno) ;;\n+  *) CUDA_DRIVER_INCLUDE=$with_cuda_driver/include\n+     CUDA_DRIVER_LIB=$with_cuda_driver/lib\n+     ;;\n+esac\n if test \"x$with_cuda_driver_include\" != x; then\n   CUDA_DRIVER_INCLUDE=$with_cuda_driver_include\n fi\n@@ -79,6 +81,7 @@ PLUGIN_NVPTX=0\n PLUGIN_NVPTX_CPPFLAGS=\n PLUGIN_NVPTX_LDFLAGS=\n PLUGIN_NVPTX_LIBS=\n+PLUGIN_NVPTX_DYNAMIC=0\n AC_SUBST(PLUGIN_NVPTX)\n AC_SUBST(PLUGIN_NVPTX_CPPFLAGS)\n AC_SUBST(PLUGIN_NVPTX_LDFLAGS)\n@@ -167,9 +170,17 @@ if test x\"$enable_offload_targets\" != x; then\n \tLIBS=$PLUGIN_NVPTX_save_LIBS\n \tcase $PLUGIN_NVPTX in\n \t  nvptx*)\n-\t    PLUGIN_NVPTX=0\n-\t    AC_MSG_ERROR([CUDA driver package required for nvptx support])\n-\t    ;;\n+\t    if test \"x$CUDA_DRIVER_INCLUDE\" = x \\\n+\t       && test \"x$CUDA_DRIVER_LIB\" = x; then\n+\t      PLUGIN_NVPTX=1\n+\t      PLUGIN_NVPTX_CPPFLAGS='-I$(srcdir)/plugin/cuda'\n+\t      PLUGIN_NVPTX_LIBS='-ldl'\n+\t      PLUGIN_NVPTX_DYNAMIC=1\n+\t    else\n+\t      PLUGIN_NVPTX=0\n+\t      AC_MSG_ERROR([CUDA driver package required for nvptx support])\n+\t    fi\n+\t  ;;\n \tesac\n \t;;\n       hsa*)\n@@ -241,6 +252,8 @@ AC_DEFINE_UNQUOTED(OFFLOAD_TARGETS, \"$offload_targets\",\n AM_CONDITIONAL([PLUGIN_NVPTX], [test $PLUGIN_NVPTX = 1])\n AC_DEFINE_UNQUOTED([PLUGIN_NVPTX], [$PLUGIN_NVPTX],\n   [Define to 1 if the NVIDIA plugin is built, 0 if not.])\n+AC_DEFINE_UNQUOTED([PLUGIN_NVPTX_DYNAMIC], [$PLUGIN_NVPTX_DYNAMIC],\n+  [Define to 1 if the NVIDIA plugin should dlopen libcuda.so.1, 0 if it should be linked against it.])\n AM_CONDITIONAL([PLUGIN_HSA], [test $PLUGIN_HSA = 1])\n AC_DEFINE_UNQUOTED([PLUGIN_HSA], [$PLUGIN_HSA],\n   [Define to 1 if the HSA plugin is built, 0 if not.])"}, {"sha": "eb92b18f74525a130021e376161197ee03bcdd43", "filename": "libgomp/plugin/cuda/cuda.h", "status": "added", "additions": 179, "deletions": 0, "changes": 179, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2393d337e7c5ff258b1ad167025b9e4d5f518533/libgomp%2Fplugin%2Fcuda%2Fcuda.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2393d337e7c5ff258b1ad167025b9e4d5f518533/libgomp%2Fplugin%2Fcuda%2Fcuda.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgomp%2Fplugin%2Fcuda%2Fcuda.h?ref=2393d337e7c5ff258b1ad167025b9e4d5f518533", "patch": "@@ -0,0 +1,179 @@\n+/* CUDA API description.\n+   Copyright (C) 2017 Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 3, or (at your option)\n+any later version.\n+\n+GCC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+Under Section 7 of GPL version 3, you are granted additional\n+permissions described in the GCC Runtime Library Exception, version\n+3.1, as published by the Free Software Foundation.\n+\n+You should have received a copy of the GNU General Public License and\n+a copy of the GCC Runtime Library Exception along with this program;\n+see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+<http://www.gnu.org/licenses/>.\n+\n+This header provides the minimum amount of typedefs, enums and function\n+declarations to be able to compile plugin-nvptx.c if cuda.h and\n+libcuda.so.1 are not available.  */\n+\n+#ifndef GCC_CUDA_H\n+#define GCC_CUDA_H\n+\n+#include <stdlib.h>\n+\n+#define CUDA_VERSION 8000\n+\n+typedef void *CUcontext;\n+typedef int CUdevice;\n+#ifdef __LP64__\n+typedef unsigned long long CUdeviceptr;\n+#else\n+typedef unsigned CUdeviceptr;\n+#endif\n+typedef void *CUevent;\n+typedef void *CUfunction;\n+typedef void *CUlinkState;\n+typedef void *CUmodule;\n+typedef void *CUstream;\n+\n+typedef enum {\n+  CUDA_SUCCESS = 0,\n+  CUDA_ERROR_INVALID_VALUE = 1,\n+  CUDA_ERROR_OUT_OF_MEMORY = 2,\n+  CUDA_ERROR_INVALID_CONTEXT = 201,\n+  CUDA_ERROR_NOT_FOUND = 500,\n+  CUDA_ERROR_NOT_READY = 600,\n+  CUDA_ERROR_LAUNCH_FAILED = 719\n+} CUresult;\n+\n+typedef enum {\n+  CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK = 1,\n+  CU_DEVICE_ATTRIBUTE_WARP_SIZE = 10,\n+  CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_BLOCK = 12,\n+  CU_DEVICE_ATTRIBUTE_CLOCK_RATE = 13,\n+  CU_DEVICE_ATTRIBUTE_GPU_OVERLAP = 15,\n+  CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT = 16,\n+  CU_DEVICE_ATTRIBUTE_INTEGRATED = 18,\n+  CU_DEVICE_ATTRIBUTE_CAN_MAP_HOST_MEMORY = 19,\n+  CU_DEVICE_ATTRIBUTE_COMPUTE_MODE = 20,\n+  CU_DEVICE_ATTRIBUTE_CONCURRENT_KERNELS = 31,\n+  CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR = 39,\n+  CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT = 40,\n+  CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_MULTIPROCESSOR = 82\n+} CUdevice_attribute;\n+\n+enum {\n+  CU_EVENT_DEFAULT = 0,\n+  CU_EVENT_DISABLE_TIMING = 2\n+};\n+\n+typedef enum {\n+  CU_FUNC_ATTRIBUTE_MAX_THREADS_PER_BLOCK = 0,\n+  CU_FUNC_ATTRIBUTE_NUM_REGS = 4\n+} CUfunction_attribute;\n+\n+typedef enum {\n+  CU_JIT_WALL_TIME = 2,\n+  CU_JIT_INFO_LOG_BUFFER = 3,\n+  CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES = 4,\n+  CU_JIT_ERROR_LOG_BUFFER = 5,\n+  CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES = 6,\n+  CU_JIT_LOG_VERBOSE = 12\n+} CUjit_option;\n+\n+typedef enum {\n+  CU_JIT_INPUT_PTX = 1\n+} CUjitInputType;\n+\n+enum {\n+  CU_CTX_SCHED_AUTO = 0\n+};\n+\n+#define CU_LAUNCH_PARAM_END ((void *) 0)\n+#define CU_LAUNCH_PARAM_BUFFER_POINTER ((void *) 1)\n+#define CU_LAUNCH_PARAM_BUFFER_SIZE ((void *) 2)\n+\n+enum {\n+  CU_STREAM_DEFAULT = 0,\n+  CU_STREAM_NON_BLOCKING = 1\n+};\n+\n+#define cuCtxCreate cuCtxCreate_v2\n+CUresult cuCtxCreate (CUcontext *, unsigned, CUdevice);\n+#define cuCtxDestroy cuCtxDestroy_v2\n+CUresult cuCtxDestroy (CUcontext);\n+CUresult cuCtxGetCurrent (CUcontext *);\n+CUresult cuCtxGetDevice (CUdevice *);\n+#define cuCtxPopCurrent cuCtxPopCurrent_v2\n+CUresult cuCtxPopCurrent (CUcontext *);\n+#define cuCtxPushCurrent cuCtxPushCurrent_v2\n+CUresult cuCtxPushCurrent (CUcontext);\n+CUresult cuCtxSynchronize (void);\n+CUresult cuDeviceGet (CUdevice *, int);\n+CUresult cuDeviceGetAttribute (int *, CUdevice_attribute, CUdevice);\n+CUresult cuDeviceGetCount (int *);\n+CUresult cuEventCreate (CUevent *, unsigned);\n+#define cuEventDestroy cuEventDestroy_v2\n+CUresult cuEventDestroy (CUevent);\n+CUresult cuEventElapsedTime (float *, CUevent, CUevent);\n+CUresult cuEventQuery (CUevent);\n+CUresult cuEventRecord (CUevent, CUstream);\n+CUresult cuEventSynchronize (CUevent);\n+CUresult cuFuncGetAttribute (int *, CUfunction_attribute, CUfunction);\n+CUresult cuGetErrorString (CUresult, const char **);\n+CUresult cuInit (unsigned);\n+CUresult cuLaunchKernel (CUfunction, unsigned, unsigned, unsigned, unsigned,\n+\t\t\t unsigned, unsigned, unsigned, CUstream, void **, void **);\n+#define cuLinkAddData cuLinkAddData_v2\n+CUresult cuLinkAddData (CUlinkState, CUjitInputType, void *, size_t, const char *,\n+\t\t\tunsigned, CUjit_option *, void **);\n+CUresult cuLinkComplete (CUlinkState, void **, size_t *);\n+#define cuLinkCreate cuLinkCreate_v2\n+CUresult cuLinkCreate (unsigned, CUjit_option *, void **, CUlinkState *);\n+CUresult cuLinkDestroy (CUlinkState);\n+#define cuMemAlloc cuMemAlloc_v2\n+CUresult cuMemAlloc (CUdeviceptr *, size_t);\n+#define cuMemAllocHost cuMemAllocHost_v2\n+CUresult cuMemAllocHost (void **, size_t);\n+CUresult cuMemcpy (CUdeviceptr, CUdeviceptr, size_t);\n+#define cuMemcpyDtoDAsync cuMemcpyDtoDAsync_v2\n+CUresult cuMemcpyDtoDAsync (CUdeviceptr, CUdeviceptr, size_t, CUstream);\n+#define cuMemcpyDtoH cuMemcpyDtoH_v2\n+CUresult cuMemcpyDtoH (void *, CUdeviceptr, size_t);\n+#define cuMemcpyDtoHAsync cuMemcpyDtoHAsync_v2\n+CUresult cuMemcpyDtoHAsync (void *, CUdeviceptr, size_t, CUstream);\n+#define cuMemcpyHtoD cuMemcpyHtoD_v2\n+CUresult cuMemcpyHtoD (CUdeviceptr, const void *, size_t);\n+#define cuMemcpyHtoDAsync cuMemcpyHtoDAsync_v2\n+CUresult cuMemcpyHtoDAsync (CUdeviceptr, const void *, size_t, CUstream);\n+#define cuMemFree cuMemFree_v2\n+CUresult cuMemFree (CUdeviceptr);\n+CUresult cuMemFreeHost (void *);\n+#define cuMemGetAddressRange cuMemGetAddressRange_v2\n+CUresult cuMemGetAddressRange (CUdeviceptr *, size_t *, CUdeviceptr);\n+#define cuMemHostGetDevicePointer cuMemHostGetDevicePointer_v2\n+CUresult cuMemHostGetDevicePointer (CUdeviceptr *, void *, unsigned);\n+CUresult cuModuleGetFunction (CUfunction *, CUmodule, const char *);\n+#define cuModuleGetGlobal cuModuleGetGlobal_v2\n+CUresult cuModuleGetGlobal (CUdeviceptr *, size_t *, CUmodule, const char *);\n+CUresult cuModuleLoad (CUmodule *, const char *);\n+CUresult cuModuleLoadData (CUmodule *, const void *);\n+CUresult cuModuleUnload (CUmodule);\n+CUresult cuStreamCreate (CUstream *, unsigned);\n+#define cuStreamDestroy cuStreamDestroy_v2\n+CUresult cuStreamDestroy (CUstream);\n+CUresult cuStreamQuery (CUstream);\n+CUresult cuStreamSynchronize (CUstream);\n+CUresult cuStreamWaitEvent (CUstream, CUevent, unsigned);\n+\n+#endif /* GCC_CUDA_H */"}, {"sha": "4144218ae8f7b7a39922706af54b97f1c05c9b53", "filename": "libgomp/plugin/plugin-nvptx.c", "status": "modified", "additions": 164, "deletions": 55, "changes": 219, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/2393d337e7c5ff258b1ad167025b9e4d5f518533/libgomp%2Fplugin%2Fplugin-nvptx.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/2393d337e7c5ff258b1ad167025b9e4d5f518533/libgomp%2Fplugin%2Fplugin-nvptx.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgomp%2Fplugin%2Fplugin-nvptx.c?ref=2393d337e7c5ff258b1ad167025b9e4d5f518533", "patch": "@@ -48,30 +48,104 @@\n #include <assert.h>\n #include <errno.h>\n \n-static const char *\n-cuda_error (CUresult r)\n-{\n-#if CUDA_VERSION < 7000\n-  /* Specified in documentation and present in library from at least\n-     5.5.  Not declared in header file prior to 7.0.  */\n-  extern CUresult cuGetErrorString (CUresult, const char **);\n-#endif\n-  const char *desc;\n-\n-  r = cuGetErrorString (r, &desc);\n-  if (r != CUDA_SUCCESS)\n-    desc = \"unknown cuda error\";\n-\n-  return desc;\n+#if PLUGIN_NVPTX_DYNAMIC\n+# include <dlfcn.h>\n+\n+# define CUDA_CALLS \\\n+CUDA_ONE_CALL (cuCtxCreate)\t\t\\\n+CUDA_ONE_CALL (cuCtxDestroy)\t\t\\\n+CUDA_ONE_CALL (cuCtxGetCurrent)\t\t\\\n+CUDA_ONE_CALL (cuCtxGetDevice)\t\t\\\n+CUDA_ONE_CALL (cuCtxPopCurrent)\t\t\\\n+CUDA_ONE_CALL (cuCtxPushCurrent)\t\\\n+CUDA_ONE_CALL (cuCtxSynchronize)\t\\\n+CUDA_ONE_CALL (cuDeviceGet)\t\t\\\n+CUDA_ONE_CALL (cuDeviceGetAttribute)\t\\\n+CUDA_ONE_CALL (cuDeviceGetCount)\t\\\n+CUDA_ONE_CALL (cuEventCreate)\t\t\\\n+CUDA_ONE_CALL (cuEventDestroy)\t\t\\\n+CUDA_ONE_CALL (cuEventElapsedTime)\t\\\n+CUDA_ONE_CALL (cuEventQuery)\t\t\\\n+CUDA_ONE_CALL (cuEventRecord)\t\t\\\n+CUDA_ONE_CALL (cuEventSynchronize)\t\\\n+CUDA_ONE_CALL (cuFuncGetAttribute)\t\\\n+CUDA_ONE_CALL (cuGetErrorString)\t\\\n+CUDA_ONE_CALL (cuInit)\t\t\t\\\n+CUDA_ONE_CALL (cuLaunchKernel)\t\t\\\n+CUDA_ONE_CALL (cuLinkAddData)\t\t\\\n+CUDA_ONE_CALL (cuLinkComplete)\t\t\\\n+CUDA_ONE_CALL (cuLinkCreate)\t\t\\\n+CUDA_ONE_CALL (cuLinkDestroy)\t\t\\\n+CUDA_ONE_CALL (cuMemAlloc)\t\t\\\n+CUDA_ONE_CALL (cuMemAllocHost)\t\t\\\n+CUDA_ONE_CALL (cuMemcpy)\t\t\\\n+CUDA_ONE_CALL (cuMemcpyDtoDAsync)\t\\\n+CUDA_ONE_CALL (cuMemcpyDtoH)\t\t\\\n+CUDA_ONE_CALL (cuMemcpyDtoHAsync)\t\\\n+CUDA_ONE_CALL (cuMemcpyHtoD)\t\t\\\n+CUDA_ONE_CALL (cuMemcpyHtoDAsync)\t\\\n+CUDA_ONE_CALL (cuMemFree)\t\t\\\n+CUDA_ONE_CALL (cuMemFreeHost)\t\t\\\n+CUDA_ONE_CALL (cuMemGetAddressRange)\t\\\n+CUDA_ONE_CALL (cuMemHostGetDevicePointer)\\\n+CUDA_ONE_CALL (cuModuleGetFunction)\t\\\n+CUDA_ONE_CALL (cuModuleGetGlobal)\t\\\n+CUDA_ONE_CALL (cuModuleLoad)\t\t\\\n+CUDA_ONE_CALL (cuModuleLoadData)\t\\\n+CUDA_ONE_CALL (cuModuleUnload)\t\t\\\n+CUDA_ONE_CALL (cuStreamCreate)\t\t\\\n+CUDA_ONE_CALL (cuStreamDestroy)\t\t\\\n+CUDA_ONE_CALL (cuStreamQuery)\t\t\\\n+CUDA_ONE_CALL (cuStreamSynchronize)\t\\\n+CUDA_ONE_CALL (cuStreamWaitEvent)\n+# define CUDA_ONE_CALL(call) \\\n+  __typeof (call) *call;\n+struct cuda_lib_s {\n+  CUDA_CALLS\n+} cuda_lib;\n+\n+/* -1 if init_cuda_lib has not been called yet, false\n+   if it has been and failed, true if it has been and succeeded.  */\n+static char cuda_lib_inited = -1;\n+\n+/* Dynamically load the CUDA runtime library and initialize function\n+   pointers, return false if unsuccessful, true if successful.  */\n+static bool\n+init_cuda_lib (void)\n+{\n+  if (cuda_lib_inited != -1)\n+    return cuda_lib_inited;\n+  const char *cuda_runtime_lib = \"libcuda.so.1\";\n+  void *h = dlopen (cuda_runtime_lib, RTLD_LAZY);\n+  cuda_lib_inited = false;\n+  if (h == NULL)\n+    return false;\n+# undef CUDA_ONE_CALL\n+# define CUDA_ONE_CALL(call) CUDA_ONE_CALL_1 (call)\n+# define CUDA_ONE_CALL_1(call) \\\n+  cuda_lib.call = dlsym (h, #call);\t\\\n+  if (cuda_lib.call == NULL)\t\t\\\n+    return false;\n+  CUDA_CALLS\n+  cuda_lib_inited = true;\n+  return true;\n }\n+# undef CUDA_ONE_CALL\n+# undef CUDA_ONE_CALL_1\n+# define CUDA_CALL_PREFIX cuda_lib.\n+#else\n+# define CUDA_CALL_PREFIX\n+# define init_cuda_lib() true\n+#endif\n \n /* Convenience macros for the frequently used CUDA library call and\n-   error handling sequence.  This does not capture all the cases we\n-   use in this file, but is common enough.  */\n+   error handling sequence as well as CUDA library calls that\n+   do the error checking themselves or don't do it at all.  */\n \n #define CUDA_CALL_ERET(ERET, FN, ...)\t\t\\\n   do {\t\t\t\t\t\t\\\n-    unsigned __r = FN (__VA_ARGS__);\t\t\\\n+    unsigned __r\t\t\t\t\\\n+      = CUDA_CALL_PREFIX FN (__VA_ARGS__);\t\\\n     if (__r != CUDA_SUCCESS)\t\t\t\\\n       {\t\t\t\t\t\t\\\n \tGOMP_PLUGIN_error (#FN \" error: %s\",\t\\\n@@ -81,18 +155,39 @@ cuda_error (CUresult r)\n   } while (0)\n \n #define CUDA_CALL(FN, ...)\t\t\t\\\n-  CUDA_CALL_ERET (false, (FN), __VA_ARGS__)\n+  CUDA_CALL_ERET (false, FN, __VA_ARGS__)\n \n #define CUDA_CALL_ASSERT(FN, ...)\t\t\\\n   do {\t\t\t\t\t\t\\\n-    unsigned __r = FN (__VA_ARGS__);\t\t\\\n+    unsigned __r\t\t\t\t\\\n+      = CUDA_CALL_PREFIX FN (__VA_ARGS__);\t\\\n     if (__r != CUDA_SUCCESS)\t\t\t\\\n       {\t\t\t\t\t\t\\\n \tGOMP_PLUGIN_fatal (#FN \" error: %s\",\t\\\n \t\t\t   cuda_error (__r));\t\\\n       }\t\t\t\t\t\t\\\n   } while (0)\n \n+#define CUDA_CALL_NOCHECK(FN, ...)\t\t\\\n+  CUDA_CALL_PREFIX FN (__VA_ARGS__)\n+\n+static const char *\n+cuda_error (CUresult r)\n+{\n+#if CUDA_VERSION < 7000\n+  /* Specified in documentation and present in library from at least\n+     5.5.  Not declared in header file prior to 7.0.  */\n+  extern CUresult cuGetErrorString (CUresult, const char **);\n+#endif\n+  const char *desc;\n+\n+  r = CUDA_CALL_NOCHECK (cuGetErrorString, r, &desc);\n+  if (r != CUDA_SUCCESS)\n+    desc = \"unknown cuda error\";\n+\n+  return desc;\n+}\n+\n static unsigned int instantiated_devices = 0;\n static pthread_mutex_t ptx_dev_lock = PTHREAD_MUTEX_INITIALIZER;\n \n@@ -401,7 +496,7 @@ fini_streams_for_device (struct ptx_device *ptx_dev)\n \n       ret &= map_fini (s);\n \n-      CUresult r = cuStreamDestroy (s->stream);\n+      CUresult r = CUDA_CALL_NOCHECK (cuStreamDestroy, s->stream);\n       if (r != CUDA_SUCCESS)\n \t{\n \t  GOMP_PLUGIN_error (\"cuStreamDestroy error: %s\", cuda_error (r));\n@@ -484,7 +579,8 @@ select_stream_for_async (int async, pthread_t thread, bool create,\n \t    s->stream = existing;\n \t  else\n \t    {\n-\t      r = cuStreamCreate (&s->stream, CU_STREAM_DEFAULT);\n+\t      r = CUDA_CALL_NOCHECK (cuStreamCreate, &s->stream,\n+\t\t\t\t     CU_STREAM_DEFAULT);\n \t      if (r != CUDA_SUCCESS)\n \t\t{\n \t\t  pthread_mutex_unlock (&ptx_dev->stream_lock);\n@@ -554,10 +650,14 @@ nvptx_init (void)\n   if (instantiated_devices != 0)\n     return true;\n \n-  CUDA_CALL (cuInit, 0);\n   ptx_events = NULL;\n   pthread_mutex_init (&ptx_event_lock, NULL);\n \n+  if (!init_cuda_lib ())\n+    return false;\n+\n+  CUDA_CALL (cuInit, 0);\n+\n   CUDA_CALL (cuDeviceGetCount, &ndevs);\n   ptx_devices = GOMP_PLUGIN_malloc_cleared (sizeof (struct ptx_device *)\n \t\t\t\t\t    * ndevs);\n@@ -575,7 +675,7 @@ nvptx_attach_host_thread_to_device (int n)\n   struct ptx_device *ptx_dev;\n   CUcontext thd_ctx;\n \n-  r = cuCtxGetDevice (&dev);\n+  r = CUDA_CALL_NOCHECK (cuCtxGetDevice, &dev);\n   if (r != CUDA_SUCCESS && r != CUDA_ERROR_INVALID_CONTEXT)\n     {\n       GOMP_PLUGIN_error (\"cuCtxGetDevice error: %s\", cuda_error (r));\n@@ -623,7 +723,7 @@ nvptx_open_device (int n)\n   ptx_dev->dev = dev;\n   ptx_dev->ctx_shared = false;\n \n-  r = cuCtxGetDevice (&ctx_dev);\n+  r = CUDA_CALL_NOCHECK (cuCtxGetDevice, &ctx_dev);\n   if (r != CUDA_SUCCESS && r != CUDA_ERROR_INVALID_CONTEXT)\n     {\n       GOMP_PLUGIN_error (\"cuCtxGetDevice error: %s\", cuda_error (r));\n@@ -669,7 +769,7 @@ nvptx_open_device (int n)\n \t\t  &pi, CU_DEVICE_ATTRIBUTE_CLOCK_RATE, dev);\n   ptx_dev->clock_khz = pi;\n \n-  CUDA_CALL_ERET (NULL,  cuDeviceGetAttribute,\n+  CUDA_CALL_ERET (NULL, cuDeviceGetAttribute,\n \t\t  &pi, CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT, dev);\n   ptx_dev->num_sms = pi;\n \n@@ -679,7 +779,7 @@ nvptx_open_device (int n)\n \n   /* CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_MULTIPROCESSOR = 82 is defined only\n      in CUDA 6.0 and newer.  */\n-  r = cuDeviceGetAttribute (&pi, 82, dev);\n+  r = CUDA_CALL_NOCHECK (cuDeviceGetAttribute, &pi, 82, dev);\n   /* Fallback: use limit of registers per block, which is usually equal.  */\n   if (r == CUDA_ERROR_INVALID_VALUE)\n     pi = ptx_dev->regs_per_block;\n@@ -698,8 +798,8 @@ nvptx_open_device (int n)\n       return NULL;\n     }\n \n-  r = cuDeviceGetAttribute (&async_engines,\n-\t\t\t    CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT, dev);\n+  r = CUDA_CALL_NOCHECK (cuDeviceGetAttribute, &async_engines,\n+\t\t\t CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT, dev);\n   if (r != CUDA_SUCCESS)\n     async_engines = 1;\n \n@@ -746,7 +846,9 @@ nvptx_get_num_devices (void)\n      further initialization).  */\n   if (instantiated_devices == 0)\n     {\n-      CUresult r = cuInit (0);\n+      if (!init_cuda_lib ())\n+\treturn 0;\n+      CUresult r = CUDA_CALL_NOCHECK (cuInit, 0);\n       /* This is not an error: e.g. we may have CUDA libraries installed but\n          no devices available.  */\n       if (r != CUDA_SUCCESS)\n@@ -797,8 +899,9 @@ link_ptx (CUmodule *module, const struct targ_ptx_obj *ptx_objs,\n       /* cuLinkAddData's 'data' argument erroneously omits the const\n \t qualifier.  */\n       GOMP_PLUGIN_debug (0, \"Loading:\\n---\\n%s\\n---\\n\", ptx_objs->code);\n-      r = cuLinkAddData (linkstate, CU_JIT_INPUT_PTX, (char*)ptx_objs->code,\n-\t\t\t ptx_objs->size, 0, 0, 0, 0);\n+      r = CUDA_CALL_NOCHECK (cuLinkAddData, linkstate, CU_JIT_INPUT_PTX,\n+\t\t\t     (char *) ptx_objs->code, ptx_objs->size,\n+\t\t\t     0, 0, 0, 0);\n       if (r != CUDA_SUCCESS)\n \t{\n \t  GOMP_PLUGIN_error (\"Link error log %s\\n\", &elog[0]);\n@@ -809,7 +912,7 @@ link_ptx (CUmodule *module, const struct targ_ptx_obj *ptx_objs,\n     }\n \n   GOMP_PLUGIN_debug (0, \"Linking\\n\");\n-  r = cuLinkComplete (linkstate, &linkout, &linkoutsize);\n+  r = CUDA_CALL_NOCHECK (cuLinkComplete, linkstate, &linkout, &linkoutsize);\n \n   GOMP_PLUGIN_debug (0, \"Link complete: %fms\\n\", elapsed);\n   GOMP_PLUGIN_debug (0, \"Link log %s\\n\", &ilog[0]);\n@@ -844,7 +947,7 @@ event_gc (bool memmap_lockable)\n       if (e->ord != nvthd->ptx_dev->ord)\n \tcontinue;\n \n-      r = cuEventQuery (*e->evt);\n+      r = CUDA_CALL_NOCHECK (cuEventQuery, *e->evt);\n       if (r == CUDA_SUCCESS)\n \t{\n \t  bool append_async = false;\n@@ -877,7 +980,7 @@ event_gc (bool memmap_lockable)\n \t      break;\n \t    }\n \n-\t  cuEventDestroy (*te);\n+\t  CUDA_CALL_NOCHECK (cuEventDestroy, *te);\n \t  free ((void *)te);\n \n \t  /* Unlink 'e' from ptx_events list.  */\n@@ -1015,10 +1118,14 @@ nvptx_exec (void (*fn), size_t mapnum, void **hostaddrs, void **devaddrs,\n \t  cu_mpc = CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT;\n \t  cu_tpm  = CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR;\n \n-\t  if (cuDeviceGetAttribute (&block_size, cu_tpb, dev) == CUDA_SUCCESS\n-\t      && cuDeviceGetAttribute (&warp_size, cu_ws, dev) == CUDA_SUCCESS\n-\t      && cuDeviceGetAttribute (&dev_size, cu_mpc, dev) == CUDA_SUCCESS\n-\t      && cuDeviceGetAttribute (&cpu_size, cu_tpm, dev)  == CUDA_SUCCESS)\n+\t  if (CUDA_CALL_NOCHECK (cuDeviceGetAttribute, &block_size, cu_tpb,\n+\t\t\t\t dev) == CUDA_SUCCESS\n+\t      && CUDA_CALL_NOCHECK (cuDeviceGetAttribute, &warp_size, cu_ws,\n+\t\t\t\t    dev) == CUDA_SUCCESS\n+\t      && CUDA_CALL_NOCHECK (cuDeviceGetAttribute, &dev_size, cu_mpc,\n+\t\t\t\t    dev) == CUDA_SUCCESS\n+\t      && CUDA_CALL_NOCHECK (cuDeviceGetAttribute, &cpu_size, cu_tpm,\n+\t\t\t\t    dev) == CUDA_SUCCESS)\n \t    {\n \t      GOMP_PLUGIN_debug (0, \" warp_size=%d, block_size=%d,\"\n \t\t\t\t \" dev_size=%d, cpu_size=%d\\n\",\n@@ -1090,7 +1197,7 @@ nvptx_exec (void (*fn), size_t mapnum, void **hostaddrs, void **devaddrs,\n #ifndef DISABLE_ASYNC\n   if (async < acc_async_noval)\n     {\n-      r = cuStreamSynchronize (dev_str->stream);\n+      r = CUDA_CALL_NOCHECK (cuStreamSynchronize, dev_str->stream);\n       if (r == CUDA_ERROR_LAUNCH_FAILED)\n \tGOMP_PLUGIN_fatal (\"cuStreamSynchronize error: %s %s\\n\", cuda_error (r),\n \t\t\t   maybe_abort_msg);\n@@ -1103,7 +1210,7 @@ nvptx_exec (void (*fn), size_t mapnum, void **hostaddrs, void **devaddrs,\n \n       e = (CUevent *)GOMP_PLUGIN_malloc (sizeof (CUevent));\n \n-      r = cuEventCreate (e, CU_EVENT_DISABLE_TIMING);\n+      r = CUDA_CALL_NOCHECK (cuEventCreate, e, CU_EVENT_DISABLE_TIMING);\n       if (r == CUDA_ERROR_LAUNCH_FAILED)\n \tGOMP_PLUGIN_fatal (\"cuEventCreate error: %s %s\\n\", cuda_error (r),\n \t\t\t   maybe_abort_msg);\n@@ -1117,7 +1224,7 @@ nvptx_exec (void (*fn), size_t mapnum, void **hostaddrs, void **devaddrs,\n       event_add (PTX_EVT_KNL, e, (void *)dev_str, 0);\n     }\n #else\n-  r = cuCtxSynchronize ();\n+  r = CUDA_CALL_NOCHECK (cuCtxSynchronize, );\n   if (r == CUDA_ERROR_LAUNCH_FAILED)\n     GOMP_PLUGIN_fatal (\"cuCtxSynchronize error: %s %s\\n\", cuda_error (r),\n \t\t       maybe_abort_msg);\n@@ -1294,7 +1401,7 @@ nvptx_async_test (int async)\n   if (!s)\n     GOMP_PLUGIN_fatal (\"unknown async %d\", async);\n \n-  r = cuStreamQuery (s->stream);\n+  r = CUDA_CALL_NOCHECK (cuStreamQuery, s->stream);\n   if (r == CUDA_SUCCESS)\n     {\n       /* The oacc-parallel.c:goacc_wait function calls this hook to determine\n@@ -1325,7 +1432,8 @@ nvptx_async_test_all (void)\n   for (s = nvthd->ptx_dev->active_streams; s != NULL; s = s->next)\n     {\n       if ((s->multithreaded || pthread_equal (s->host_thread, self))\n-\t  && cuStreamQuery (s->stream) == CUDA_ERROR_NOT_READY)\n+\t  && CUDA_CALL_NOCHECK (cuStreamQuery,\n+\t\t\t\ts->stream) == CUDA_ERROR_NOT_READY)\n \t{\n \t  pthread_mutex_unlock (&nvthd->ptx_dev->stream_lock);\n \t  return 0;\n@@ -1400,7 +1508,7 @@ nvptx_wait_all (void)\n     {\n       if (s->multithreaded || pthread_equal (s->host_thread, self))\n \t{\n-\t  r = cuStreamQuery (s->stream);\n+\t  r = CUDA_CALL_NOCHECK (cuStreamQuery, s->stream);\n \t  if (r == CUDA_SUCCESS)\n \t    continue;\n \t  else if (r != CUDA_ERROR_NOT_READY)\n@@ -1632,13 +1740,15 @@ static void\n nvptx_set_clocktick (CUmodule module, struct ptx_device *dev)\n {\n   CUdeviceptr dptr;\n-  CUresult r = cuModuleGetGlobal (&dptr, NULL, module, \"__nvptx_clocktick\");\n+  CUresult r = CUDA_CALL_NOCHECK (cuModuleGetGlobal, &dptr, NULL,\n+\t\t\t\t  module, \"__nvptx_clocktick\");\n   if (r == CUDA_ERROR_NOT_FOUND)\n     return;\n   if (r != CUDA_SUCCESS)\n     GOMP_PLUGIN_fatal (\"cuModuleGetGlobal error: %s\", cuda_error (r));\n   double __nvptx_clocktick = 1e-3 / dev->clock_khz;\n-  r = cuMemcpyHtoD (dptr, &__nvptx_clocktick, sizeof (__nvptx_clocktick));\n+  r = CUDA_CALL_NOCHECK (cuMemcpyHtoD, dptr, &__nvptx_clocktick,\n+\t\t\t sizeof (__nvptx_clocktick));\n   if (r != CUDA_SUCCESS)\n     GOMP_PLUGIN_fatal (\"cuMemcpyHtoD error: %s\", cuda_error (r));\n }\n@@ -1761,7 +1871,7 @@ GOMP_OFFLOAD_unload_image (int ord, unsigned version, const void *target_data)\n     if (image->target_data == target_data)\n       {\n \t*prev_p = image->next;\n-\tif (cuModuleUnload (image->module) != CUDA_SUCCESS)\n+\tif (CUDA_CALL_NOCHECK (cuModuleUnload, image->module) != CUDA_SUCCESS)\n \t  ret = false;\n \tfree (image->fns);\n \tfree (image);\n@@ -1974,7 +2084,7 @@ static void *\n nvptx_stacks_alloc (size_t size, int num)\n {\n   CUdeviceptr stacks;\n-  CUresult r = cuMemAlloc (&stacks, size * num);\n+  CUresult r = CUDA_CALL_NOCHECK (cuMemAlloc, &stacks, size * num);\n   if (r != CUDA_SUCCESS)\n     GOMP_PLUGIN_fatal (\"cuMemAlloc error: %s\", cuda_error (r));\n   return (void *) stacks;\n@@ -1985,7 +2095,7 @@ nvptx_stacks_alloc (size_t size, int num)\n static void\n nvptx_stacks_free (void *p, int num)\n {\n-  CUresult r = cuMemFree ((CUdeviceptr) p);\n+  CUresult r = CUDA_CALL_NOCHECK (cuMemFree, (CUdeviceptr) p);\n   if (r != CUDA_SUCCESS)\n     GOMP_PLUGIN_fatal (\"cuMemFree error: %s\", cuda_error (r));\n }\n@@ -2028,14 +2138,13 @@ GOMP_OFFLOAD_run (int ord, void *tgt_fn, void *tgt_vars, void **args)\n     CU_LAUNCH_PARAM_BUFFER_SIZE, &fn_args_size,\n     CU_LAUNCH_PARAM_END\n   };\n-  r = cuLaunchKernel (function,\n-\t\t      teams, 1, 1,\n-\t\t      32, threads, 1,\n-\t\t      0, ptx_dev->null_stream->stream, NULL, config);\n+  r = CUDA_CALL_NOCHECK (cuLaunchKernel, function, teams, 1, 1,\n+\t\t\t 32, threads, 1, 0, ptx_dev->null_stream->stream,\n+\t\t\t NULL, config);\n   if (r != CUDA_SUCCESS)\n     GOMP_PLUGIN_fatal (\"cuLaunchKernel error: %s\", cuda_error (r));\n \n-  r = cuCtxSynchronize ();\n+  r = CUDA_CALL_NOCHECK (cuCtxSynchronize, );\n   if (r == CUDA_ERROR_LAUNCH_FAILED)\n     GOMP_PLUGIN_fatal (\"cuCtxSynchronize error: %s %s\\n\", cuda_error (r),\n \t\t       maybe_abort_msg);"}]}