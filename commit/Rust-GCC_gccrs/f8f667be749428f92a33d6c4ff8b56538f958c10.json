{"sha": "f8f667be749428f92a33d6c4ff8b56538f958c10", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZjhmNjY3YmU3NDk0MjhmOTJhMzNkNmM0ZmY4YjU2NTM4Zjk1OGMxMA==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@linaro.org", "date": "2017-12-21T07:01:30Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2017-12-21T07:01:30Z"}, "message": "poly_int: emit_group_load/store\n\nThis patch changes the sizes passed to emit_group_load and\nemit_group_store from int to poly_int64.\n\n2017-12-21  Richard Sandiford  <richard.sandiford@linaro.org>\n\t    Alan Hayward  <alan.hayward@arm.com>\n\t    David Sherwood  <david.sherwood@arm.com>\n\ngcc/\n\t* expr.h (emit_group_load, emit_group_load_into_temps)\n\t(emit_group_store): Take the size as a poly_int64 rather than an int.\n\t* expr.c (emit_group_load_1, emit_group_load): Likewise.\n\t(emit_group_load_into_temp, emit_group_store): Likewise.\n\nCo-Authored-By: Alan Hayward <alan.hayward@arm.com>\nCo-Authored-By: David Sherwood <david.sherwood@arm.com>\n\nFrom-SVN: r255925", "tree": {"sha": "fa602dfb49ef094128300da58ed242681ee11f93", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/fa602dfb49ef094128300da58ed242681ee11f93"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/f8f667be749428f92a33d6c4ff8b56538f958c10", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f8f667be749428f92a33d6c4ff8b56538f958c10", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f8f667be749428f92a33d6c4ff8b56538f958c10", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f8f667be749428f92a33d6c4ff8b56538f958c10/comments", "author": null, "committer": null, "parents": [{"sha": "80ce7eb44961b50d92b5481b26ce20a50567baae", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/80ce7eb44961b50d92b5481b26ce20a50567baae", "html_url": "https://github.com/Rust-GCC/gccrs/commit/80ce7eb44961b50d92b5481b26ce20a50567baae"}], "stats": {"total": 105, "additions": 62, "deletions": 43}, "files": [{"sha": "9c985b247975c39787e7dea423bed7429c8312bd", "filename": "gcc/ChangeLog", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f8f667be749428f92a33d6c4ff8b56538f958c10/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f8f667be749428f92a33d6c4ff8b56538f958c10/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=f8f667be749428f92a33d6c4ff8b56538f958c10", "patch": "@@ -1,3 +1,12 @@\n+2017-12-21  Richard Sandiford  <richard.sandiford@linaro.org>\n+\t    Alan Hayward  <alan.hayward@arm.com>\n+\t    David Sherwood  <david.sherwood@arm.com>\n+\n+\t* expr.h (emit_group_load, emit_group_load_into_temps)\n+\t(emit_group_store): Take the size as a poly_int64 rather than an int.\n+\t* expr.c (emit_group_load_1, emit_group_load): Likewise.\n+\t(emit_group_load_into_temp, emit_group_store): Likewise.\n+\n 2017-12-21  Richard Sandiford  <richard.sandiford@linaro.org>\n \t    Alan Hayward  <alan.hayward@arm.com>\n \t    David Sherwood  <david.sherwood@arm.com>"}, {"sha": "8a1227908faf9887d2f39cd0b448a8c4ebc0d9cb", "filename": "gcc/expr.c", "status": "modified", "additions": 50, "deletions": 40, "changes": 90, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f8f667be749428f92a33d6c4ff8b56538f958c10/gcc%2Fexpr.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f8f667be749428f92a33d6c4ff8b56538f958c10/gcc%2Fexpr.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpr.c?ref=f8f667be749428f92a33d6c4ff8b56538f958c10", "patch": "@@ -2095,7 +2095,8 @@ gen_group_rtx (rtx orig)\n    into corresponding XEXP (XVECEXP (DST, 0, i), 0) element.  */\n \n static void\n-emit_group_load_1 (rtx *tmps, rtx dst, rtx orig_src, tree type, int ssize)\n+emit_group_load_1 (rtx *tmps, rtx dst, rtx orig_src, tree type,\n+\t\t   poly_int64 ssize)\n {\n   rtx src;\n   int start, i;\n@@ -2134,12 +2135,16 @@ emit_group_load_1 (rtx *tmps, rtx dst, rtx orig_src, tree type, int ssize)\n   for (i = start; i < XVECLEN (dst, 0); i++)\n     {\n       machine_mode mode = GET_MODE (XEXP (XVECEXP (dst, 0, i), 0));\n-      HOST_WIDE_INT bytepos = INTVAL (XEXP (XVECEXP (dst, 0, i), 1));\n-      unsigned int bytelen = GET_MODE_SIZE (mode);\n-      int shift = 0;\n+      poly_int64 bytepos = INTVAL (XEXP (XVECEXP (dst, 0, i), 1));\n+      poly_int64 bytelen = GET_MODE_SIZE (mode);\n+      poly_int64 shift = 0;\n \n-      /* Handle trailing fragments that run over the size of the struct.  */\n-      if (ssize >= 0 && bytepos + (HOST_WIDE_INT) bytelen > ssize)\n+      /* Handle trailing fragments that run over the size of the struct.\n+\t It's the target's responsibility to make sure that the fragment\n+\t cannot be strictly smaller in some cases and strictly larger\n+\t in others.  */\n+      gcc_checking_assert (ordered_p (bytepos + bytelen, ssize));\n+      if (known_size_p (ssize) && maybe_gt (bytepos + bytelen, ssize))\n \t{\n \t  /* Arrange to shift the fragment to where it belongs.\n \t     extract_bit_field loads to the lsb of the reg.  */\n@@ -2153,7 +2158,7 @@ emit_group_load_1 (rtx *tmps, rtx dst, rtx orig_src, tree type, int ssize)\n \t      )\n \t    shift = (bytelen - (ssize - bytepos)) * BITS_PER_UNIT;\n \t  bytelen = ssize - bytepos;\n-\t  gcc_assert (bytelen > 0);\n+\t  gcc_assert (maybe_gt (bytelen, 0));\n \t}\n \n       /* If we won't be loading directly from memory, protect the real source\n@@ -2177,33 +2182,34 @@ emit_group_load_1 (rtx *tmps, rtx dst, rtx orig_src, tree type, int ssize)\n       if (MEM_P (src)\n \t  && (! targetm.slow_unaligned_access (mode, MEM_ALIGN (src))\n \t      || MEM_ALIGN (src) >= GET_MODE_ALIGNMENT (mode))\n-\t  && bytepos * BITS_PER_UNIT % GET_MODE_ALIGNMENT (mode) == 0\n-\t  && bytelen == GET_MODE_SIZE (mode))\n+\t  && multiple_p (bytepos * BITS_PER_UNIT, GET_MODE_ALIGNMENT (mode))\n+\t  && known_eq (bytelen, GET_MODE_SIZE (mode)))\n \t{\n \t  tmps[i] = gen_reg_rtx (mode);\n \t  emit_move_insn (tmps[i], adjust_address (src, mode, bytepos));\n \t}\n       else if (COMPLEX_MODE_P (mode)\n \t       && GET_MODE (src) == mode\n-\t       && bytelen == GET_MODE_SIZE (mode))\n+\t       && known_eq (bytelen, GET_MODE_SIZE (mode)))\n \t/* Let emit_move_complex do the bulk of the work.  */\n \ttmps[i] = src;\n       else if (GET_CODE (src) == CONCAT)\n \t{\n-\t  unsigned int slen = GET_MODE_SIZE (GET_MODE (src));\n-\t  unsigned int slen0 = GET_MODE_SIZE (GET_MODE (XEXP (src, 0)));\n-\t  unsigned int elt = bytepos / slen0;\n-\t  unsigned int subpos = bytepos % slen0;\n+\t  poly_int64 slen = GET_MODE_SIZE (GET_MODE (src));\n+\t  poly_int64 slen0 = GET_MODE_SIZE (GET_MODE (XEXP (src, 0)));\n+\t  unsigned int elt;\n+\t  poly_int64 subpos;\n \n-\t  if (subpos + bytelen <= slen0)\n+\t  if (can_div_trunc_p (bytepos, slen0, &elt, &subpos)\n+\t      && known_le (subpos + bytelen, slen0))\n \t    {\n \t      /* The following assumes that the concatenated objects all\n \t\t have the same size.  In this case, a simple calculation\n \t\t can be used to determine the object and the bit field\n \t\t to be extracted.  */\n \t      tmps[i] = XEXP (src, elt);\n-\t      if (subpos != 0\n-\t\t  || subpos + bytelen != slen0\n+\t      if (maybe_ne (subpos, 0)\n+\t\t  || maybe_ne (subpos + bytelen, slen0)\n \t\t  || (!CONSTANT_P (tmps[i])\n \t\t      && (!REG_P (tmps[i]) || GET_MODE (tmps[i]) != mode)))\n \t\ttmps[i] = extract_bit_field (tmps[i], bytelen * BITS_PER_UNIT,\n@@ -2215,7 +2221,7 @@ emit_group_load_1 (rtx *tmps, rtx dst, rtx orig_src, tree type, int ssize)\n \t    {\n \t      rtx mem;\n \n-\t      gcc_assert (!bytepos);\n+\t      gcc_assert (known_eq (bytepos, 0));\n \t      mem = assign_stack_temp (GET_MODE (src), slen);\n \t      emit_move_insn (mem, src);\n \t      tmps[i] = extract_bit_field (mem, bytelen * BITS_PER_UNIT,\n@@ -2234,23 +2240,21 @@ emit_group_load_1 (rtx *tmps, rtx dst, rtx orig_src, tree type, int ssize)\n \n \t  mem = assign_stack_temp (GET_MODE (src), slen);\n \t  emit_move_insn (mem, src);\n-\t  tmps[i] = adjust_address (mem, mode, (int) bytepos);\n+\t  tmps[i] = adjust_address (mem, mode, bytepos);\n \t}\n       else if (CONSTANT_P (src) && GET_MODE (dst) != BLKmode\n                && XVECLEN (dst, 0) > 1)\n         tmps[i] = simplify_gen_subreg (mode, src, GET_MODE (dst), bytepos);\n       else if (CONSTANT_P (src))\n \t{\n-\t  HOST_WIDE_INT len = (HOST_WIDE_INT) bytelen;\n-\n-\t  if (len == ssize)\n+\t  if (known_eq (bytelen, ssize))\n \t    tmps[i] = src;\n \t  else\n \t    {\n \t      rtx first, second;\n \n \t      /* TODO: const_wide_int can have sizes other than this...  */\n-\t      gcc_assert (2 * len == ssize);\n+\t      gcc_assert (known_eq (2 * bytelen, ssize));\n \t      split_double (src, &first, &second);\n \t      if (i)\n \t\ttmps[i] = second;\n@@ -2265,7 +2269,7 @@ emit_group_load_1 (rtx *tmps, rtx dst, rtx orig_src, tree type, int ssize)\n \t\t\t\t     bytepos * BITS_PER_UNIT, 1, NULL_RTX,\n \t\t\t\t     mode, mode, false, NULL);\n \n-      if (shift)\n+      if (maybe_ne (shift, 0))\n \ttmps[i] = expand_shift (LSHIFT_EXPR, mode, tmps[i],\n \t\t\t\tshift, tmps[i], 0);\n     }\n@@ -2277,7 +2281,7 @@ emit_group_load_1 (rtx *tmps, rtx dst, rtx orig_src, tree type, int ssize)\n    if not known.  */\n \n void\n-emit_group_load (rtx dst, rtx src, tree type, int ssize)\n+emit_group_load (rtx dst, rtx src, tree type, poly_int64 ssize)\n {\n   rtx *tmps;\n   int i;\n@@ -2300,7 +2304,7 @@ emit_group_load (rtx dst, rtx src, tree type, int ssize)\n    in the right place.  */\n \n rtx\n-emit_group_load_into_temps (rtx parallel, rtx src, tree type, int ssize)\n+emit_group_load_into_temps (rtx parallel, rtx src, tree type, poly_int64 ssize)\n {\n   rtvec vec;\n   int i;\n@@ -2371,7 +2375,8 @@ emit_group_move_into_temps (rtx src)\n    known.  */\n \n void\n-emit_group_store (rtx orig_dst, rtx src, tree type ATTRIBUTE_UNUSED, int ssize)\n+emit_group_store (rtx orig_dst, rtx src, tree type ATTRIBUTE_UNUSED,\n+\t\t  poly_int64 ssize)\n {\n   rtx *tmps, dst;\n   int start, finish, i;\n@@ -2502,24 +2507,28 @@ emit_group_store (rtx orig_dst, rtx src, tree type ATTRIBUTE_UNUSED, int ssize)\n   /* Process the pieces.  */\n   for (i = start; i < finish; i++)\n     {\n-      HOST_WIDE_INT bytepos = INTVAL (XEXP (XVECEXP (src, 0, i), 1));\n+      poly_int64 bytepos = INTVAL (XEXP (XVECEXP (src, 0, i), 1));\n       machine_mode mode = GET_MODE (tmps[i]);\n-      unsigned int bytelen = GET_MODE_SIZE (mode);\n-      unsigned int adj_bytelen;\n+      poly_int64 bytelen = GET_MODE_SIZE (mode);\n+      poly_uint64 adj_bytelen;\n       rtx dest = dst;\n \n-      /* Handle trailing fragments that run over the size of the struct.  */\n-      if (ssize >= 0 && bytepos + (HOST_WIDE_INT) bytelen > ssize)\n+      /* Handle trailing fragments that run over the size of the struct.\n+\t It's the target's responsibility to make sure that the fragment\n+\t cannot be strictly smaller in some cases and strictly larger\n+\t in others.  */\n+      gcc_checking_assert (ordered_p (bytepos + bytelen, ssize));\n+      if (known_size_p (ssize) && maybe_gt (bytepos + bytelen, ssize))\n \tadj_bytelen = ssize - bytepos;\n       else\n \tadj_bytelen = bytelen;\n \n       if (GET_CODE (dst) == CONCAT)\n \t{\n-\t  if (bytepos + adj_bytelen\n-\t      <= GET_MODE_SIZE (GET_MODE (XEXP (dst, 0))))\n+\t  if (known_le (bytepos + adj_bytelen,\n+\t\t\tGET_MODE_SIZE (GET_MODE (XEXP (dst, 0)))))\n \t    dest = XEXP (dst, 0);\n-\t  else if (bytepos >= GET_MODE_SIZE (GET_MODE (XEXP (dst, 0))))\n+\t  else if (known_ge (bytepos, GET_MODE_SIZE (GET_MODE (XEXP (dst, 0)))))\n \t    {\n \t      bytepos -= GET_MODE_SIZE (GET_MODE (XEXP (dst, 0)));\n \t      dest = XEXP (dst, 1);\n@@ -2529,7 +2538,7 @@ emit_group_store (rtx orig_dst, rtx src, tree type ATTRIBUTE_UNUSED, int ssize)\n \t      machine_mode dest_mode = GET_MODE (dest);\n \t      machine_mode tmp_mode = GET_MODE (tmps[i]);\n \n-\t      gcc_assert (bytepos == 0 && XVECLEN (src, 0));\n+\t      gcc_assert (known_eq (bytepos, 0) && XVECLEN (src, 0));\n \n \t      if (GET_MODE_ALIGNMENT (dest_mode)\n \t\t  >= GET_MODE_ALIGNMENT (tmp_mode))\n@@ -2554,7 +2563,7 @@ emit_group_store (rtx orig_dst, rtx src, tree type ATTRIBUTE_UNUSED, int ssize)\n \t}\n \n       /* Handle trailing fragments that run over the size of the struct.  */\n-      if (ssize >= 0 && bytepos + (HOST_WIDE_INT) bytelen > ssize)\n+      if (known_size_p (ssize) && maybe_gt (bytepos + bytelen, ssize))\n \t{\n \t  /* store_bit_field always takes its value from the lsb.\n \t     Move the fragment to the lsb if it's not already there.  */\n@@ -2567,7 +2576,7 @@ emit_group_store (rtx orig_dst, rtx src, tree type ATTRIBUTE_UNUSED, int ssize)\n #endif\n \t      )\n \t    {\n-\t      int shift = (bytelen - (ssize - bytepos)) * BITS_PER_UNIT;\n+\t      poly_int64 shift = (bytelen - (ssize - bytepos)) * BITS_PER_UNIT;\n \t      tmps[i] = expand_shift (RSHIFT_EXPR, mode, tmps[i],\n \t\t\t\t      shift, tmps[i], 0);\n \t    }\n@@ -2583,8 +2592,9 @@ emit_group_store (rtx orig_dst, rtx src, tree type ATTRIBUTE_UNUSED, int ssize)\n       else if (MEM_P (dest)\n \t       && (!targetm.slow_unaligned_access (mode, MEM_ALIGN (dest))\n \t\t   || MEM_ALIGN (dest) >= GET_MODE_ALIGNMENT (mode))\n-\t       && bytepos * BITS_PER_UNIT % GET_MODE_ALIGNMENT (mode) == 0\n-\t       && bytelen == GET_MODE_SIZE (mode))\n+\t       && multiple_p (bytepos * BITS_PER_UNIT,\n+\t\t\t      GET_MODE_ALIGNMENT (mode))\n+\t       && known_eq (bytelen, GET_MODE_SIZE (mode)))\n \temit_move_insn (adjust_address (dest, mode, bytepos), tmps[i]);\n \n       else"}, {"sha": "105c30e655191fcb5fc14e8f2a59b93d24397a3f", "filename": "gcc/expr.h", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/f8f667be749428f92a33d6c4ff8b56538f958c10/gcc%2Fexpr.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/f8f667be749428f92a33d6c4ff8b56538f958c10/gcc%2Fexpr.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpr.h?ref=f8f667be749428f92a33d6c4ff8b56538f958c10", "patch": "@@ -128,10 +128,10 @@ extern rtx gen_group_rtx (rtx);\n \n /* Load a BLKmode value into non-consecutive registers represented by a\n    PARALLEL.  */\n-extern void emit_group_load (rtx, rtx, tree, int);\n+extern void emit_group_load (rtx, rtx, tree, poly_int64);\n \n /* Similarly, but load into new temporaries.  */\n-extern rtx emit_group_load_into_temps (rtx, rtx, tree, int);\n+extern rtx emit_group_load_into_temps (rtx, rtx, tree, poly_int64);\n \n /* Move a non-consecutive group of registers represented by a PARALLEL into\n    a non-consecutive group of registers represented by a PARALLEL.  */\n@@ -142,7 +142,7 @@ extern rtx emit_group_move_into_temps (rtx);\n \n /* Store a BLKmode value from non-consecutive registers represented by a\n    PARALLEL.  */\n-extern void emit_group_store (rtx, rtx, tree, int);\n+extern void emit_group_store (rtx, rtx, tree, poly_int64);\n \n extern rtx maybe_emit_group_store (rtx, tree);\n "}]}