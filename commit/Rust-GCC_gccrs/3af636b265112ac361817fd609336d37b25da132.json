{"sha": "3af636b265112ac361817fd609336d37b25da132", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6M2FmNjM2YjI2NTExMmFjMzYxODE3ZmQ2MDkzMzZkMzdiMjVkYTEzMg==", "commit": {"author": {"name": "Janis Johnson", "email": "janis@gcc.gnu.org", "date": "2001-09-10T21:37:44Z"}, "committer": {"name": "Janis Johnson", "email": "janis@gcc.gnu.org", "date": "2001-09-10T21:37:44Z"}, "message": "Support procedures for testing profile-directed optimizations.\n\nFrom-SVN: r45525", "tree": {"sha": "8c0bd01cf84d867966207c3fc4d1fd49be38b3ca", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/8c0bd01cf84d867966207c3fc4d1fd49be38b3ca"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/3af636b265112ac361817fd609336d37b25da132", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3af636b265112ac361817fd609336d37b25da132", "html_url": "https://github.com/Rust-GCC/gccrs/commit/3af636b265112ac361817fd609336d37b25da132", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/3af636b265112ac361817fd609336d37b25da132/comments", "author": null, "committer": null, "parents": [{"sha": "1aa084e6077cd6acae783275cc70367b48b16d34", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1aa084e6077cd6acae783275cc70367b48b16d34", "html_url": "https://github.com/Rust-GCC/gccrs/commit/1aa084e6077cd6acae783275cc70367b48b16d34"}], "stats": {"total": 284, "additions": 284, "deletions": 0}, "files": [{"sha": "ca62c67836c14e2ac49eb3bdec0939bb58dee423", "filename": "gcc/testsuite/lib/profopt.exp", "status": "added", "additions": 284, "deletions": 0, "changes": 284, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/3af636b265112ac361817fd609336d37b25da132/gcc%2Ftestsuite%2Flib%2Fprofopt.exp", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/3af636b265112ac361817fd609336d37b25da132/gcc%2Ftestsuite%2Flib%2Fprofopt.exp", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Flib%2Fprofopt.exp?ref=3af636b265112ac361817fd609336d37b25da132", "patch": "@@ -0,0 +1,284 @@\n+#   Copyright (C) 2001 Free Software Foundation, Inc.\n+\n+# This program is free software; you can redistribute it and/or modify\n+# it under the terms of the GNU General Public License as published by\n+# the Free Software Foundation; either version 2 of the License, or\n+# (at your option) any later version.\n+# \n+# This program is distributed in the hope that it will be useful,\n+# but WITHOUT ANY WARRANTY; without even the implied warranty of\n+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+# GNU General Public License for more details.\n+# \n+# You should have received a copy of the GNU General Public License\n+# along with this program; if not, write to the Free Software\n+# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  \n+#\n+# This script was submitted by Janis Johnson <janis187@us.ibm.com>.\n+\n+# Test the functionality and optionally, performance improvement, of\n+# programs compiled with profile-directed optimizations.  Compile and\n+# run a test with profile options, compile it with options using the\n+# profile feedback, and then run the test again.  Optionally compile\n+# and run a third time without the profile-directed optimization and\n+# compare timing results of the program with normal optimization and\n+# with the profile-directed optimization. Each test is run using\n+# multiple sets of optimization and/or code generation options in\n+# addition to the profiling and feedback options.\n+\n+# If perf_ext is defined and the performance value for the\n+# profile-directed test run is non-zero then the performance check will\n+# be done.\n+\n+global PROFOPT_OPTIONS perf_delta\n+\n+# The including .exp file must define these.\n+global tool profile_option feedback_option prof_ext\n+if ![info exists tool] {\n+    error \"Tools is not specified.\"\n+}\n+if ![info exists profile_option] {\n+    error \"No profile option specified for first compile.\"\n+}\n+if ![info exists feedback_option] {\n+    error \"No feedback option specified for second compile.\"\n+}\n+if ![info exists prof_ext] {\n+    error \"No profile data file extension specified.\"\n+}\n+\n+# The maximum perforance degradation can be defined in the including file.\n+if ![info exists perf_delta] {\n+    set perf_delta 4\n+}\n+\n+# The default option list can be overridden by\n+# PROFOPT_OPTIONS=\"{ { list1 } ... { list2 } }\"\n+\n+if ![info exists PROFOPT_OPTIONS] {\n+    set PROFOPT_OPTIONS [list \\\n+\t{ -g } \\\n+\t{ -O0 } \\\n+\t{ -O1 } \\\n+\t{ -O2 } \\\n+\t{ -O3 } \\\n+\t{ -O3 -g } \\\n+\t{ -Os } ]\n+}\n+\n+set option_list $PROFOPT_OPTIONS\n+\n+#\n+# profopt-cleanup -- remove profiling or performance results files.\n+#\n+# EXT is the extension of files to remove\n+#\n+proc profopt-cleanup { ext } {\n+    set files [glob -nocomplain *.$ext]\n+    if { $files != \"\" } {\n+\teval \"remote_file build delete $files\"\n+    }\n+}\n+\n+#\n+# profopt-perf-value -- get performance value for a test\n+#\n+# TESTCASE is the name of the test\n+# PERF_EXT is the extension of the performance result file\n+# OPTSTR is the string of compiler options\n+#\n+proc profopt-perf-value { testcase perf_ext optstr } {\n+    set basename [file tail $testcase]\n+    set base [file rootname $basename]\n+    set files [glob -nocomplain $base.$perf_ext]\n+    # The file doesn't exist; let the caller decide if that's a problem.\n+    if { $files == \"\" } {\n+        return -2 \n+    }\n+    remote_upload host $base.$perf_ext $base.$perf_ext\n+    set fd [open $base.$perf_ext r]\n+    gets $fd line\n+    set val -2\n+    if [regexp \"TIME\" $line] {\n+        if [regexp \"TIME -1\" $line] {\n+\t    fail \"$testcase perf check: no consistent time available, $optstr\"\n+\t    set val -1\n+\t} elseif ![regexp \"(\\[0-9\\]+)\" \"$line\" val] {\n+\t    set val -2\n+\t}\n+    }\n+    # Report problems with an existing file.\n+    if { $val == -2 } {\n+\tfail \"$testcase perf check: file $base.$perf_ext has wrong format, $optstr\"\n+    }\n+    close $fd\n+    profopt-cleanup $perf_ext\n+    return $val\n+}\n+\n+#\n+# c-prof-execute -- compile for profiling and then feedback, then normal\n+#\n+# SRC is the full pathname of the testcase.\n+#\n+proc profopt-execute { src } {\n+    global srcdir tmpdir\n+    global option_list\n+    global tool profile_option feedback_option prof_ext perf_ext perf_delta\n+    global verbose\n+\n+    regsub \"^$srcdir/?\" $src \"\" testcase\n+    # If we couldn't rip $srcdir out of `src' then just do the best we can.\n+    # The point is to reduce the unnecessary noise in the logs.  Don't strip\n+    # out too much because different testcases with the same name can confuse\n+    # `test-tool'.\n+    if [string match \"/*\" $testcase] {\n+\tset testcase \"[file tail [file dirname $src]]/[file tail $src]\"\n+    }\n+\n+    set executable $tmpdir/[file tail [file rootname $src].x]\n+\n+    set count 0\n+    foreach option $option_list {\n+\tset execname1 \"${executable}${count}1\"\n+\tset execname2 \"${executable}${count}2\"\n+\tset execname3 \"${executable}${count}3\"\n+\tincr count\n+\n+\tremote_file build delete $execname1\n+\tremote_file build delete $execname2\n+\tremote_file build delete $execname3\n+\tverbose \"Testing $testcase, $option\" 1\n+\n+\t# Compile for profiling.\n+\n+\tset options \"\"\n+\tlappend options \"additional_flags=$option $profile_option\"\n+\tset optstr \"$option $profile_option\"\n+\tset comp_output [${tool}_target_compile \"$src\" \"$execname1\" executable $options];\n+\tif ![${tool}_check_compile \"$testcase compilation\" $optstr $execname1 $comp_output] {\n+ \t    unresolved \"$testcase execution,   $optstr\"\n+ \t    unresolved \"$testcase compilation, $option $feedback_option\"\n+ \t    unresolved \"$testcase execution,   $option $feedback_option\"\n+\t    continue\n+\t}\n+\n+\t# Run the profiled test.\n+\n+\tset result [${tool}_load $execname1 \"\" \"\"]\n+\tset status [lindex $result 0]\n+\t# Make sure the profile data was generated, and fail if not.\n+\tif { $status == \"pass\" } {\n+\t    set basename [file tail $testcase]\n+\t    set base [file rootname $basename]\n+\t    set files [glob -nocomplain $base.$prof_ext]\n+\t    if { $files == \"\" } {\n+\t\tset status \"fail\"\n+\t\tfail \"$testcase execution: file $base.$prof_ext does not exist, $option $profile_option\"\n+\t    } else {\n+\t        $status \"$testcase execution,   $optstr\"\n+\t    }\n+\t} else {\n+\t    $status \"$testcase execution,   $optstr\"\n+\t}\n+\t# Quit for this round if it failed\n+\tif { $status != \"pass\" } {\n+ \t    unresolved \"$testcase compilation, $option $feedback_option\"\n+ \t    unresolved \"$testcase execution,   $option $feedback_option\"\n+\t    continue\n+\t}\n+\tremote_file build delete $execname1\n+\n+\t# Compile with feedback-directed optimizations.\n+\n+\tset options \"\"\n+\tlappend options \"additional_flags=$option $feedback_option\"\n+\tset optstr \"$option $feedback_option\"\n+\tset comp_output [${tool}_target_compile \"$src\" \"$execname2\" \"executable\" $options];\n+\tif ![${tool}_check_compile \"$testcase compilation\" $optstr $execname2 $comp_output] {\n+ \t    unresolved \"$testcase execution,   $optstr\"\n+\t    continue\n+\t}\n+\n+\t# Run the profile-directed optimized test.\n+\n+\tset result [${tool}_load \"$execname2\" \"\" \"\"]\n+\tset status [lindex $result 0]\n+\t$status \"$testcase execution,   $optstr\"\n+\tif { $status != \"pass\" } {\n+\t    continue\n+\t}\n+\n+\t# Remove the profiling data files.\n+\tprofopt-cleanup $prof_ext\n+\n+\t# If the test is not expected to produce performance data then\n+\t# we're done now.\n+\tif ![info exists perf_ext] {\n+\t    remote_file build delete $execname2\n+\t    continue\n+\t}\n+\n+\t# Get the performance data from the test built with\n+\t# profile-directed optimization.  If the file doesn't exist or if\n+\t# the value is zero, skip the performance comparison.\n+\tset val2 [profopt-perf-value $testcase $perf_ext $optstr]\n+\tif { $val2 <= 0 } {\n+\t    remote_file build delete $execname2\n+\t    continue\n+\t}\n+\n+\t# Compile with normal optimizations.\n+\n+\tset options \"\"\n+\tlappend options \"additional_flags=$option\"\n+\tset optstr \"$option\"\n+\tset comp_output [${tool}_target_compile \"$src\" \"$execname3\" \"executable\" $options];\n+\tif ![${tool}_check_compile \"$testcase compilation\" $optstr $execname3 $comp_output] {\n+ \t    unresolved \"$testcase execution,   $optstr\"\n+\t    unresolved \"$testcase perf check,  $optstr\"\n+\t    continue\n+\t}\n+\n+\t# Run the test with normal optimizations.\n+\n+\tset result [${tool}_load \"$execname3\" \"\" \"\"]\n+\tset status [lindex $result 0]\n+\t$status \"$testcase execution,   $optstr\"\n+\tif { $status != \"pass\" } {\n+\t    unresolved \"$testcase perf check, $optstr\"\n+\t    continue\n+\t}\n+\n+\t# Get the performance data from the test built with normal\n+\t# optimization.\n+\tset val1 [profopt-perf-value $testcase $perf_ext $optstr]\n+\tif { $val1 < 0 } {\n+\t    if { $val1 == -2 } {\n+\t\t# The data file existed with the profile-directed\n+\t\t# optimization so this one should, too.\n+\t\tfail \"$testcase perf check: file $base.$perf_ext does not exist, $optstr\"\n+\t    }\n+\t    continue\n+\t}\n+\n+\t# Compare results of the two runs and fail if the time with the\n+\t# profile-directed optimization is significantly more than the time\n+\t# without it.\n+\tset status \"pass\"\n+\tif { $val2 > $val1 } {\n+\t    # Check for a performance degration outside of allowable limits.\n+\t    if { [expr $val2 - $val1] > [expr [expr $val1 * $perf_delta] / 100] } {\n+\t\tset status \"fail\"\n+\t    }\n+\t}\n+\tif { $status == \"fail\" } {\n+\t    fail \"$testcase perf check: orig: $val1  new: $val2, $optstr\"\n+\t} else {\n+\t    $status \"$testcase perf check,  $optstr\"\n+\t    verbose \"$testcase orig: $val1  new: $val2, $optstr\" 2\n+\t    remote_file build delete $execname2\n+\t    remote_file build delete $execname3\n+\t}\n+    }\n+}"}]}