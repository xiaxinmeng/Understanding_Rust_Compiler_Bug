{"sha": "d5ecead9406647c58e178bccc1f9d85259b087e5", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZDVlY2VhZDk0MDY2NDdjNThlMTc4YmNjYzFmOWQ4NTI1OWIwODdlNQ==", "commit": {"author": {"name": "James Greenhalgh", "email": "james.greenhalgh@arm.com", "date": "2014-01-23T14:59:20Z"}, "committer": {"name": "Kyrylo Tkachov", "email": "ktkachov@gcc.gnu.org", "date": "2014-01-23T14:59:20Z"}, "message": "[AArch64_BE 4/4] Big-Endian lane numbering fix\n\n2014-01-23  James Greenhalgh  <james.greenhalgh@arm.com>\n\t    Alex Velenko  <Alex.Velenko@arm.com>\n\n\t* config/aarch64/arm_neon.h (vaddv_s8): __LANE0 cleanup.\n\t(vaddv_s16): Likewise.\n\t(vaddv_s32): Likewise.\n\t(vaddv_u8): Likewise.\n\t(vaddv_u16): Likewise.\n\t(vaddv_u32): Likewise.\n\t(vaddvq_s8): Likewise.\n\t(vaddvq_s16): Likewise.\n\t(vaddvq_s32): Likewise.\n\t(vaddvq_s64): Likewise.\n\t(vaddvq_u8): Likewise.\n\t(vaddvq_u16): Likewise.\n\t(vaddvq_u32): Likewise.\n\t(vaddvq_u64): Likewise.\n\t(vaddv_f32): Likewise.\n\t(vaddvq_f32): Likewise.\n\t(vaddvq_f64): Likewise.\n\t(vmaxv_f32): Likewise.\n\t(vmaxv_s8): Likewise.\n\t(vmaxv_s16): Likewise.\n\t(vmaxv_s32): Likewise.\n\t(vmaxv_u8): Likewise.\n\t(vmaxv_u16): Likewise.\n\t(vmaxv_u32): Likewise.\n\t(vmaxvq_f32): Likewise.\n\t(vmaxvq_f64): Likewise.\n\t(vmaxvq_s8): Likewise.\n\t(vmaxvq_s16): Likewise.\n\t(vmaxvq_s32): Likewise.\n\t(vmaxvq_u8): Likewise.\n\t(vmaxvq_u16): Likewise.\n\t(vmaxvq_u32): Likewise.\n\t(vmaxnmv_f32): Likewise.\n\t(vmaxnmvq_f32): Likewise.\n\t(vmaxnmvq_f64): Likewise.\n\t(vminv_f32): Likewise.\n\t(vminv_s8): Likewise.\n\t(vminv_s16): Likewise.\n\t(vminv_s32): Likewise.\n\t(vminv_u8): Likewise.\n\t(vminv_u16): Likewise.\n\t(vminv_u32): Likewise.\n\t(vminvq_f32): Likewise.\n\t(vminvq_f64): Likewise.\n\t(vminvq_s8): Likewise.\n\t(vminvq_s16): Likewise.\n\t(vminvq_s32): Likewise.\n\t(vminvq_u8): Likewise.\n\t(vminvq_u16): Likewise.\n\t(vminvq_u32): Likewise.\n\t(vminnmv_f32): Likewise.\n\t(vminnmvq_f32): Likewise.\n\t(vminnmvq_f64): Likewise.\n\nCo-Authored-By: Alex Velenko <Alex.Velenko@arm.com>\n\nFrom-SVN: r206973", "tree": {"sha": "27c24a6f1be9d77c1481994659933258ed752445", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/27c24a6f1be9d77c1481994659933258ed752445"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/d5ecead9406647c58e178bccc1f9d85259b087e5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d5ecead9406647c58e178bccc1f9d85259b087e5", "html_url": "https://github.com/Rust-GCC/gccrs/commit/d5ecead9406647c58e178bccc1f9d85259b087e5", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d5ecead9406647c58e178bccc1f9d85259b087e5/comments", "author": {"login": "jgreenhalgh-arm", "id": 6104025, "node_id": "MDQ6VXNlcjYxMDQwMjU=", "avatar_url": "https://avatars.githubusercontent.com/u/6104025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jgreenhalgh-arm", "html_url": "https://github.com/jgreenhalgh-arm", "followers_url": "https://api.github.com/users/jgreenhalgh-arm/followers", "following_url": "https://api.github.com/users/jgreenhalgh-arm/following{/other_user}", "gists_url": "https://api.github.com/users/jgreenhalgh-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jgreenhalgh-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jgreenhalgh-arm/subscriptions", "organizations_url": "https://api.github.com/users/jgreenhalgh-arm/orgs", "repos_url": "https://api.github.com/users/jgreenhalgh-arm/repos", "events_url": "https://api.github.com/users/jgreenhalgh-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/jgreenhalgh-arm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "1dd055a27d0d94a90d488bf262fda3f6bda10c2d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1dd055a27d0d94a90d488bf262fda3f6bda10c2d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/1dd055a27d0d94a90d488bf262fda3f6bda10c2d"}], "stats": {"total": 171, "additions": 110, "deletions": 61}, "files": [{"sha": "753df289ad1b72779ea47d2a389b2a3b4e4834f6", "filename": "gcc/ChangeLog", "status": "modified", "additions": 57, "deletions": 0, "changes": 57, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d5ecead9406647c58e178bccc1f9d85259b087e5/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d5ecead9406647c58e178bccc1f9d85259b087e5/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=d5ecead9406647c58e178bccc1f9d85259b087e5", "patch": "@@ -1,3 +1,60 @@\n+2014-01-23  James Greenhalgh  <james.greenhalgh@arm.com>\n+\t    Alex Velenko  <Alex.Velenko@arm.com>\n+\n+\t* config/aarch64/arm_neon.h (vaddv_s8): __LANE0 cleanup.\n+\t(vaddv_s16): Likewise.\n+\t(vaddv_s32): Likewise.\n+\t(vaddv_u8): Likewise.\n+\t(vaddv_u16): Likewise.\n+\t(vaddv_u32): Likewise.\n+\t(vaddvq_s8): Likewise.\n+\t(vaddvq_s16): Likewise.\n+\t(vaddvq_s32): Likewise.\n+\t(vaddvq_s64): Likewise.\n+\t(vaddvq_u8): Likewise.\n+\t(vaddvq_u16): Likewise.\n+\t(vaddvq_u32): Likewise.\n+\t(vaddvq_u64): Likewise.\n+\t(vaddv_f32): Likewise.\n+\t(vaddvq_f32): Likewise.\n+\t(vaddvq_f64): Likewise.\n+\t(vmaxv_f32): Likewise.\n+\t(vmaxv_s8): Likewise.\n+\t(vmaxv_s16): Likewise.\n+\t(vmaxv_s32): Likewise.\n+\t(vmaxv_u8): Likewise.\n+\t(vmaxv_u16): Likewise.\n+\t(vmaxv_u32): Likewise.\n+\t(vmaxvq_f32): Likewise.\n+\t(vmaxvq_f64): Likewise.\n+\t(vmaxvq_s8): Likewise.\n+\t(vmaxvq_s16): Likewise.\n+\t(vmaxvq_s32): Likewise.\n+\t(vmaxvq_u8): Likewise.\n+\t(vmaxvq_u16): Likewise.\n+\t(vmaxvq_u32): Likewise.\n+\t(vmaxnmv_f32): Likewise.\n+\t(vmaxnmvq_f32): Likewise.\n+\t(vmaxnmvq_f64): Likewise.\n+\t(vminv_f32): Likewise.\n+\t(vminv_s8): Likewise.\n+\t(vminv_s16): Likewise.\n+\t(vminv_s32): Likewise.\n+\t(vminv_u8): Likewise.\n+\t(vminv_u16): Likewise.\n+\t(vminv_u32): Likewise.\n+\t(vminvq_f32): Likewise.\n+\t(vminvq_f64): Likewise.\n+\t(vminvq_s8): Likewise.\n+\t(vminvq_s16): Likewise.\n+\t(vminvq_s32): Likewise.\n+\t(vminvq_u8): Likewise.\n+\t(vminvq_u16): Likewise.\n+\t(vminvq_u32): Likewise.\n+\t(vminnmv_f32): Likewise.\n+\t(vminnmvq_f32): Likewise.\n+\t(vminnmvq_f64): Likewise.\n+\n 2014-01-23  James Greenhalgh  <james.greenhalgh@arm.com>\n \n \t* config/aarch64/aarch64-simd.md"}, {"sha": "b6aee7a7426f8fe427701770b0e9f26189e08d06", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 53, "deletions": 61, "changes": 114, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d5ecead9406647c58e178bccc1f9d85259b087e5/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d5ecead9406647c58e178bccc1f9d85259b087e5/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=d5ecead9406647c58e178bccc1f9d85259b087e5", "patch": "@@ -15311,132 +15311,126 @@ vaddd_u64 (uint64x1_t __a, uint64x1_t __b)\n   return __a + __b;\n }\n \n-#if __AARCH64EB__\n-#define __LANE0(__t) ((__t) - 1)\n-#else\n-#define __LANE0(__t) 0\n-#endif\n-\n /* vaddv */\n \n __extension__ static __inline int8_t __attribute__ ((__always_inline__))\n vaddv_s8 (int8x8_t __a)\n {\n-  return vget_lane_s8 (__builtin_aarch64_reduc_splus_v8qi (__a), __LANE0 (8));\n+  return vget_lane_s8 (__builtin_aarch64_reduc_splus_v8qi (__a), 0);\n }\n \n __extension__ static __inline int16_t __attribute__ ((__always_inline__))\n vaddv_s16 (int16x4_t __a)\n {\n-  return vget_lane_s16 (__builtin_aarch64_reduc_splus_v4hi (__a), __LANE0 (4));\n+  return vget_lane_s16 (__builtin_aarch64_reduc_splus_v4hi (__a), 0);\n }\n \n __extension__ static __inline int32_t __attribute__ ((__always_inline__))\n vaddv_s32 (int32x2_t __a)\n {\n-  return vget_lane_s32 (__builtin_aarch64_reduc_splus_v2si (__a), __LANE0 (2));\n+  return vget_lane_s32 (__builtin_aarch64_reduc_splus_v2si (__a), 0);\n }\n \n __extension__ static __inline uint8_t __attribute__ ((__always_inline__))\n vaddv_u8 (uint8x8_t __a)\n {\n   return vget_lane_u8 ((uint8x8_t)\n \t\t__builtin_aarch64_reduc_uplus_v8qi ((int8x8_t) __a),\n-\t\t__LANE0 (8));\n+\t\t0);\n }\n \n __extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n vaddv_u16 (uint16x4_t __a)\n {\n   return vget_lane_u16 ((uint16x4_t)\n \t\t__builtin_aarch64_reduc_uplus_v4hi ((int16x4_t) __a),\n-\t\t__LANE0 (4));\n+\t\t0);\n }\n \n __extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n vaddv_u32 (uint32x2_t __a)\n {\n   return vget_lane_u32 ((uint32x2_t)\n \t\t__builtin_aarch64_reduc_uplus_v2si ((int32x2_t) __a),\n-\t\t__LANE0 (2));\n+\t\t0);\n }\n \n __extension__ static __inline int8_t __attribute__ ((__always_inline__))\n vaddvq_s8 (int8x16_t __a)\n {\n   return vgetq_lane_s8 (__builtin_aarch64_reduc_splus_v16qi (__a),\n-\t\t\t__LANE0 (16));\n+\t\t\t0);\n }\n \n __extension__ static __inline int16_t __attribute__ ((__always_inline__))\n vaddvq_s16 (int16x8_t __a)\n {\n-  return vgetq_lane_s16 (__builtin_aarch64_reduc_splus_v8hi (__a), __LANE0 (8));\n+  return vgetq_lane_s16 (__builtin_aarch64_reduc_splus_v8hi (__a), 0);\n }\n \n __extension__ static __inline int32_t __attribute__ ((__always_inline__))\n vaddvq_s32 (int32x4_t __a)\n {\n-  return vgetq_lane_s32 (__builtin_aarch64_reduc_splus_v4si (__a), __LANE0 (4));\n+  return vgetq_lane_s32 (__builtin_aarch64_reduc_splus_v4si (__a), 0);\n }\n \n __extension__ static __inline int64_t __attribute__ ((__always_inline__))\n vaddvq_s64 (int64x2_t __a)\n {\n-  return vgetq_lane_s64 (__builtin_aarch64_reduc_splus_v2di (__a), __LANE0 (2));\n+  return vgetq_lane_s64 (__builtin_aarch64_reduc_splus_v2di (__a), 0);\n }\n \n __extension__ static __inline uint8_t __attribute__ ((__always_inline__))\n vaddvq_u8 (uint8x16_t __a)\n {\n   return vgetq_lane_u8 ((uint8x16_t)\n \t\t__builtin_aarch64_reduc_uplus_v16qi ((int8x16_t) __a),\n-\t\t__LANE0 (16));\n+\t\t0);\n }\n \n __extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n vaddvq_u16 (uint16x8_t __a)\n {\n   return vgetq_lane_u16 ((uint16x8_t)\n \t\t__builtin_aarch64_reduc_uplus_v8hi ((int16x8_t) __a),\n-\t\t__LANE0 (8));\n+\t\t0);\n }\n \n __extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n vaddvq_u32 (uint32x4_t __a)\n {\n   return vgetq_lane_u32 ((uint32x4_t)\n \t\t__builtin_aarch64_reduc_uplus_v4si ((int32x4_t) __a),\n-\t\t__LANE0 (4));\n+\t\t0);\n }\n \n __extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n vaddvq_u64 (uint64x2_t __a)\n {\n   return vgetq_lane_u64 ((uint64x2_t)\n \t\t__builtin_aarch64_reduc_uplus_v2di ((int64x2_t) __a),\n-\t\t__LANE0 (2));\n+\t\t0);\n }\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vaddv_f32 (float32x2_t __a)\n {\n   float32x2_t __t = __builtin_aarch64_reduc_splus_v2sf (__a);\n-  return vget_lane_f32 (__t, __LANE0 (2));\n+  return vget_lane_f32 (__t, 0);\n }\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vaddvq_f32 (float32x4_t __a)\n {\n   float32x4_t __t = __builtin_aarch64_reduc_splus_v4sf (__a);\n-  return vgetq_lane_f32 (__t, __LANE0 (4));\n+  return vgetq_lane_f32 (__t, 0);\n }\n \n __extension__ static __inline float64_t __attribute__ ((__always_inline__))\n vaddvq_f64 (float64x2_t __a)\n {\n   float64x2_t __t = __builtin_aarch64_reduc_splus_v2df (__a);\n-  return vgetq_lane_f64 (__t, __LANE0 (2));\n+  return vgetq_lane_f64 (__t, 0);\n }\n \n /* vbsl  */\n@@ -19848,105 +19842,105 @@ __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vmaxv_f32 (float32x2_t __a)\n {\n   return vget_lane_f32 (__builtin_aarch64_reduc_smax_nan_v2sf (__a),\n-\t\t\t__LANE0 (2));\n+\t\t\t0);\n }\n \n __extension__ static __inline int8_t __attribute__ ((__always_inline__))\n vmaxv_s8 (int8x8_t __a)\n {\n-  return vget_lane_s8 (__builtin_aarch64_reduc_smax_v8qi (__a), __LANE0 (8));\n+  return vget_lane_s8 (__builtin_aarch64_reduc_smax_v8qi (__a), 0);\n }\n \n __extension__ static __inline int16_t __attribute__ ((__always_inline__))\n vmaxv_s16 (int16x4_t __a)\n {\n-  return vget_lane_s16 (__builtin_aarch64_reduc_smax_v4hi (__a), __LANE0 (4));\n+  return vget_lane_s16 (__builtin_aarch64_reduc_smax_v4hi (__a), 0);\n }\n \n __extension__ static __inline int32_t __attribute__ ((__always_inline__))\n vmaxv_s32 (int32x2_t __a)\n {\n-  return vget_lane_s32 (__builtin_aarch64_reduc_smax_v2si (__a), __LANE0 (2));\n+  return vget_lane_s32 (__builtin_aarch64_reduc_smax_v2si (__a), 0);\n }\n \n __extension__ static __inline uint8_t __attribute__ ((__always_inline__))\n vmaxv_u8 (uint8x8_t __a)\n {\n   return vget_lane_u8 ((uint8x8_t)\n \t\t__builtin_aarch64_reduc_umax_v8qi ((int8x8_t) __a),\n-\t\t__LANE0 (8));\n+\t\t0);\n }\n \n __extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n vmaxv_u16 (uint16x4_t __a)\n {\n   return vget_lane_u16 ((uint16x4_t)\n \t\t__builtin_aarch64_reduc_umax_v4hi ((int16x4_t) __a),\n-\t\t__LANE0 (4));\n+\t\t0);\n }\n \n __extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n vmaxv_u32 (uint32x2_t __a)\n {\n   return vget_lane_u32 ((uint32x2_t)\n \t\t__builtin_aarch64_reduc_umax_v2si ((int32x2_t) __a),\n-\t\t__LANE0 (2));\n+\t\t0);\n }\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vmaxvq_f32 (float32x4_t __a)\n {\n   return vgetq_lane_f32 (__builtin_aarch64_reduc_smax_nan_v4sf (__a),\n-\t\t\t __LANE0 (4));\n+\t\t\t 0);\n }\n \n __extension__ static __inline float64_t __attribute__ ((__always_inline__))\n vmaxvq_f64 (float64x2_t __a)\n {\n   return vgetq_lane_f64 (__builtin_aarch64_reduc_smax_nan_v2df (__a),\n-\t\t\t __LANE0 (2));\n+\t\t\t 0);\n }\n \n __extension__ static __inline int8_t __attribute__ ((__always_inline__))\n vmaxvq_s8 (int8x16_t __a)\n {\n-  return vgetq_lane_s8 (__builtin_aarch64_reduc_smax_v16qi (__a), __LANE0 (16));\n+  return vgetq_lane_s8 (__builtin_aarch64_reduc_smax_v16qi (__a), 0);\n }\n \n __extension__ static __inline int16_t __attribute__ ((__always_inline__))\n vmaxvq_s16 (int16x8_t __a)\n {\n-  return vgetq_lane_s16 (__builtin_aarch64_reduc_smax_v8hi (__a), __LANE0 (8));\n+  return vgetq_lane_s16 (__builtin_aarch64_reduc_smax_v8hi (__a), 0);\n }\n \n __extension__ static __inline int32_t __attribute__ ((__always_inline__))\n vmaxvq_s32 (int32x4_t __a)\n {\n-  return vgetq_lane_s32 (__builtin_aarch64_reduc_smax_v4si (__a), __LANE0 (4));\n+  return vgetq_lane_s32 (__builtin_aarch64_reduc_smax_v4si (__a), 0);\n }\n \n __extension__ static __inline uint8_t __attribute__ ((__always_inline__))\n vmaxvq_u8 (uint8x16_t __a)\n {\n   return vgetq_lane_u8 ((uint8x16_t)\n \t\t__builtin_aarch64_reduc_umax_v16qi ((int8x16_t) __a),\n-\t\t__LANE0 (16));\n+\t\t0);\n }\n \n __extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n vmaxvq_u16 (uint16x8_t __a)\n {\n   return vgetq_lane_u16 ((uint16x8_t)\n \t\t__builtin_aarch64_reduc_umax_v8hi ((int16x8_t) __a),\n-\t\t__LANE0 (8));\n+\t\t0);\n }\n \n __extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n vmaxvq_u32 (uint32x4_t __a)\n {\n   return vgetq_lane_u32 ((uint32x4_t)\n \t\t__builtin_aarch64_reduc_umax_v4si ((int32x4_t) __a),\n-\t\t__LANE0 (4));\n+\t\t0);\n }\n \n /* vmaxnmv  */\n@@ -19955,19 +19949,19 @@ __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vmaxnmv_f32 (float32x2_t __a)\n {\n   return vget_lane_f32 (__builtin_aarch64_reduc_smax_v2sf (__a),\n-\t\t\t__LANE0 (2));\n+\t\t\t0);\n }\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vmaxnmvq_f32 (float32x4_t __a)\n {\n-  return vgetq_lane_f32 (__builtin_aarch64_reduc_smax_v4sf (__a), __LANE0 (4));\n+  return vgetq_lane_f32 (__builtin_aarch64_reduc_smax_v4sf (__a), 0);\n }\n \n __extension__ static __inline float64_t __attribute__ ((__always_inline__))\n vmaxnmvq_f64 (float64x2_t __a)\n {\n-  return vgetq_lane_f64 (__builtin_aarch64_reduc_smax_v2df (__a), __LANE0 (2));\n+  return vgetq_lane_f64 (__builtin_aarch64_reduc_smax_v2df (__a), 0);\n }\n \n /* vmin  */\n@@ -20094,126 +20088,126 @@ __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vminv_f32 (float32x2_t __a)\n {\n   return vget_lane_f32 (__builtin_aarch64_reduc_smin_nan_v2sf (__a),\n-\t\t\t__LANE0 (2));\n+\t\t\t0);\n }\n \n __extension__ static __inline int8_t __attribute__ ((__always_inline__))\n vminv_s8 (int8x8_t __a)\n {\n   return vget_lane_s8 (__builtin_aarch64_reduc_smin_v8qi (__a),\n-\t\t       __LANE0 (8));\n+\t\t       0);\n }\n \n __extension__ static __inline int16_t __attribute__ ((__always_inline__))\n vminv_s16 (int16x4_t __a)\n {\n-  return vget_lane_s16 (__builtin_aarch64_reduc_smin_v4hi (__a), __LANE0 (4));\n+  return vget_lane_s16 (__builtin_aarch64_reduc_smin_v4hi (__a), 0);\n }\n \n __extension__ static __inline int32_t __attribute__ ((__always_inline__))\n vminv_s32 (int32x2_t __a)\n {\n-  return vget_lane_s32 (__builtin_aarch64_reduc_smin_v2si (__a), __LANE0 (2));\n+  return vget_lane_s32 (__builtin_aarch64_reduc_smin_v2si (__a), 0);\n }\n \n __extension__ static __inline uint8_t __attribute__ ((__always_inline__))\n vminv_u8 (uint8x8_t __a)\n {\n   return vget_lane_u8 ((uint8x8_t)\n \t\t__builtin_aarch64_reduc_umin_v8qi ((int8x8_t) __a),\n-\t\t__LANE0 (8));\n+\t\t0);\n }\n \n __extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n vminv_u16 (uint16x4_t __a)\n {\n   return vget_lane_u16 ((uint16x4_t)\n \t\t__builtin_aarch64_reduc_umin_v4hi ((int16x4_t) __a),\n-\t\t__LANE0 (4));\n+\t\t0);\n }\n \n __extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n vminv_u32 (uint32x2_t __a)\n {\n   return vget_lane_u32 ((uint32x2_t)\n \t\t__builtin_aarch64_reduc_umin_v2si ((int32x2_t) __a),\n-\t\t__LANE0 (2));\n+\t\t0);\n }\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vminvq_f32 (float32x4_t __a)\n {\n   return vgetq_lane_f32 (__builtin_aarch64_reduc_smin_nan_v4sf (__a),\n-\t\t\t __LANE0 (4));\n+\t\t\t 0);\n }\n \n __extension__ static __inline float64_t __attribute__ ((__always_inline__))\n vminvq_f64 (float64x2_t __a)\n {\n   return vgetq_lane_f64 (__builtin_aarch64_reduc_smin_nan_v2df (__a),\n-\t\t\t __LANE0 (2));\n+\t\t\t 0);\n }\n \n __extension__ static __inline int8_t __attribute__ ((__always_inline__))\n vminvq_s8 (int8x16_t __a)\n {\n-  return vgetq_lane_s8 (__builtin_aarch64_reduc_smin_v16qi (__a), __LANE0 (16));\n+  return vgetq_lane_s8 (__builtin_aarch64_reduc_smin_v16qi (__a), 0);\n }\n \n __extension__ static __inline int16_t __attribute__ ((__always_inline__))\n vminvq_s16 (int16x8_t __a)\n {\n-  return vgetq_lane_s16 (__builtin_aarch64_reduc_smin_v8hi (__a), __LANE0 (8));\n+  return vgetq_lane_s16 (__builtin_aarch64_reduc_smin_v8hi (__a), 0);\n }\n \n __extension__ static __inline int32_t __attribute__ ((__always_inline__))\n vminvq_s32 (int32x4_t __a)\n {\n-  return vgetq_lane_s32 (__builtin_aarch64_reduc_smin_v4si (__a), __LANE0 (4));\n+  return vgetq_lane_s32 (__builtin_aarch64_reduc_smin_v4si (__a), 0);\n }\n \n __extension__ static __inline uint8_t __attribute__ ((__always_inline__))\n vminvq_u8 (uint8x16_t __a)\n {\n   return vgetq_lane_u8 ((uint8x16_t)\n \t\t__builtin_aarch64_reduc_umin_v16qi ((int8x16_t) __a),\n-\t\t__LANE0 (16));\n+\t\t0);\n }\n \n __extension__ static __inline uint16_t __attribute__ ((__always_inline__))\n vminvq_u16 (uint16x8_t __a)\n {\n   return vgetq_lane_u16 ((uint16x8_t)\n \t\t__builtin_aarch64_reduc_umin_v8hi ((int16x8_t) __a),\n-\t\t__LANE0 (8));\n+\t\t0);\n }\n \n __extension__ static __inline uint32_t __attribute__ ((__always_inline__))\n vminvq_u32 (uint32x4_t __a)\n {\n   return vgetq_lane_u32 ((uint32x4_t)\n \t\t__builtin_aarch64_reduc_umin_v4si ((int32x4_t) __a),\n-\t\t__LANE0 (4));\n+\t\t0);\n }\n \n /* vminnmv  */\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vminnmv_f32 (float32x2_t __a)\n {\n-  return vget_lane_f32 (__builtin_aarch64_reduc_smin_v2sf (__a), __LANE0 (2));\n+  return vget_lane_f32 (__builtin_aarch64_reduc_smin_v2sf (__a), 0);\n }\n \n __extension__ static __inline float32_t __attribute__ ((__always_inline__))\n vminnmvq_f32 (float32x4_t __a)\n {\n-  return vgetq_lane_f32 (__builtin_aarch64_reduc_smin_v4sf (__a), __LANE0 (4));\n+  return vgetq_lane_f32 (__builtin_aarch64_reduc_smin_v4sf (__a), 0);\n }\n \n __extension__ static __inline float64_t __attribute__ ((__always_inline__))\n vminnmvq_f64 (float64x2_t __a)\n {\n-  return vgetq_lane_f64 (__builtin_aarch64_reduc_smin_v2df (__a), __LANE0 (2));\n+  return vgetq_lane_f64 (__builtin_aarch64_reduc_smin_v2df (__a), 0);\n }\n \n /* vmla */\n@@ -25329,8 +25323,6 @@ __INTERLEAVE_LIST (zip)\n \n /* End of optimal implementations in approved order.  */\n \n-#undef __LANE0\n-\n #undef __aarch64_vget_lane_any\n #undef __aarch64_vget_lane_f32\n #undef __aarch64_vget_lane_f64"}]}