{"sha": "e1aeb9bc9e8704c99a6fb8a83983e42ea8967637", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZTFhZWI5YmM5ZTg3MDRjOTlhNmZiOGE4Mzk4M2U0MmVhODk2NzYzNw==", "commit": {"author": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2018-05-02T22:01:22Z"}, "committer": {"name": "Ian Lance Taylor", "email": "ian@gcc.gnu.org", "date": "2018-05-02T22:01:22Z"}, "message": "runtime: remove unused stack.go\n    \n    We're never going to use stack.go for gccgo.  Although a build tag\n    keeps it from being built, even having it around can be confusing.\n    Remove it.\n    \n    Reviewed-on: https://go-review.googlesource.com/40774\n\nFrom-SVN: r259865", "tree": {"sha": "c0a361fefa15568a6ec08a360c1182e367d790b5", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/c0a361fefa15568a6ec08a360c1182e367d790b5"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/e1aeb9bc9e8704c99a6fb8a83983e42ea8967637", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e1aeb9bc9e8704c99a6fb8a83983e42ea8967637", "html_url": "https://github.com/Rust-GCC/gccrs/commit/e1aeb9bc9e8704c99a6fb8a83983e42ea8967637", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/e1aeb9bc9e8704c99a6fb8a83983e42ea8967637/comments", "author": null, "committer": null, "parents": [{"sha": "cec9701b510ebcb5cfdf431a5b0dd9c143f1da69", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/cec9701b510ebcb5cfdf431a5b0dd9c143f1da69", "html_url": "https://github.com/Rust-GCC/gccrs/commit/cec9701b510ebcb5cfdf431a5b0dd9c143f1da69"}], "stats": {"total": 1229, "additions": 0, "deletions": 1229}, "files": [{"sha": "fd99e4d2f67e914c2c4e769b012e404138dfa493", "filename": "libgo/go/runtime/stack.go", "status": "removed", "additions": 0, "deletions": 1229, "changes": 1229, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/cec9701b510ebcb5cfdf431a5b0dd9c143f1da69/libgo%2Fgo%2Fruntime%2Fstack.go", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/cec9701b510ebcb5cfdf431a5b0dd9c143f1da69/libgo%2Fgo%2Fruntime%2Fstack.go", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libgo%2Fgo%2Fruntime%2Fstack.go?ref=cec9701b510ebcb5cfdf431a5b0dd9c143f1da69", "patch": "@@ -1,1229 +0,0 @@\n-// Copyright 2013 The Go Authors. All rights reserved.\n-// Use of this source code is governed by a BSD-style\n-// license that can be found in the LICENSE file.\n-\n-// +build ignore\n-\n-package runtime\n-\n-import (\n-\t\"runtime/internal/atomic\"\n-\t\"runtime/internal/sys\"\n-\t\"unsafe\"\n-)\n-\n-/*\n-Stack layout parameters.\n-Included both by runtime (compiled via 6c) and linkers (compiled via gcc).\n-\n-The per-goroutine g->stackguard is set to point StackGuard bytes\n-above the bottom of the stack.  Each function compares its stack\n-pointer against g->stackguard to check for overflow.  To cut one\n-instruction from the check sequence for functions with tiny frames,\n-the stack is allowed to protrude StackSmall bytes below the stack\n-guard.  Functions with large frames don't bother with the check and\n-always call morestack.  The sequences are (for amd64, others are\n-similar):\n-\n-\tguard = g->stackguard\n-\tframe = function's stack frame size\n-\targsize = size of function arguments (call + return)\n-\n-\tstack frame size <= StackSmall:\n-\t\tCMPQ guard, SP\n-\t\tJHI 3(PC)\n-\t\tMOVQ m->morearg, $(argsize << 32)\n-\t\tCALL morestack(SB)\n-\n-\tstack frame size > StackSmall but < StackBig\n-\t\tLEAQ (frame-StackSmall)(SP), R0\n-\t\tCMPQ guard, R0\n-\t\tJHI 3(PC)\n-\t\tMOVQ m->morearg, $(argsize << 32)\n-\t\tCALL morestack(SB)\n-\n-\tstack frame size >= StackBig:\n-\t\tMOVQ m->morearg, $((argsize << 32) | frame)\n-\t\tCALL morestack(SB)\n-\n-The bottom StackGuard - StackSmall bytes are important: there has\n-to be enough room to execute functions that refuse to check for\n-stack overflow, either because they need to be adjacent to the\n-actual caller's frame (deferproc) or because they handle the imminent\n-stack overflow (morestack).\n-\n-For example, deferproc might call malloc, which does one of the\n-above checks (without allocating a full frame), which might trigger\n-a call to morestack.  This sequence needs to fit in the bottom\n-section of the stack.  On amd64, morestack's frame is 40 bytes, and\n-deferproc's frame is 56 bytes.  That fits well within the\n-StackGuard - StackSmall bytes at the bottom.\n-The linkers explore all possible call traces involving non-splitting\n-functions to make sure that this limit cannot be violated.\n-*/\n-\n-const (\n-\t// StackSystem is a number of additional bytes to add\n-\t// to each stack below the usual guard area for OS-specific\n-\t// purposes like signal handling. Used on Windows, Plan 9,\n-\t// and Darwin/ARM because they do not use a separate stack.\n-\t_StackSystem = sys.GoosWindows*512*sys.PtrSize + sys.GoosPlan9*512 + sys.GoosDarwin*sys.GoarchArm*1024\n-\n-\t// The minimum size of stack used by Go code\n-\t_StackMin = 2048\n-\n-\t// The minimum stack size to allocate.\n-\t// The hackery here rounds FixedStack0 up to a power of 2.\n-\t_FixedStack0 = _StackMin + _StackSystem\n-\t_FixedStack1 = _FixedStack0 - 1\n-\t_FixedStack2 = _FixedStack1 | (_FixedStack1 >> 1)\n-\t_FixedStack3 = _FixedStack2 | (_FixedStack2 >> 2)\n-\t_FixedStack4 = _FixedStack3 | (_FixedStack3 >> 4)\n-\t_FixedStack5 = _FixedStack4 | (_FixedStack4 >> 8)\n-\t_FixedStack6 = _FixedStack5 | (_FixedStack5 >> 16)\n-\t_FixedStack  = _FixedStack6 + 1\n-\n-\t// Functions that need frames bigger than this use an extra\n-\t// instruction to do the stack split check, to avoid overflow\n-\t// in case SP - framesize wraps below zero.\n-\t// This value can be no bigger than the size of the unmapped\n-\t// space at zero.\n-\t_StackBig = 4096\n-\n-\t// The stack guard is a pointer this many bytes above the\n-\t// bottom of the stack.\n-\t_StackGuard = 880*sys.StackGuardMultiplier + _StackSystem\n-\n-\t// After a stack split check the SP is allowed to be this\n-\t// many bytes below the stack guard. This saves an instruction\n-\t// in the checking sequence for tiny frames.\n-\t_StackSmall = 128\n-\n-\t// The maximum number of bytes that a chain of NOSPLIT\n-\t// functions can use.\n-\t_StackLimit = _StackGuard - _StackSystem - _StackSmall\n-)\n-\n-// Goroutine preemption request.\n-// Stored into g->stackguard0 to cause split stack check failure.\n-// Must be greater than any real sp.\n-// 0xfffffade in hex.\n-const (\n-\t_StackPreempt = uintptrMask & -1314\n-\t_StackFork    = uintptrMask & -1234\n-)\n-\n-const (\n-\t// stackDebug == 0: no logging\n-\t//            == 1: logging of per-stack operations\n-\t//            == 2: logging of per-frame operations\n-\t//            == 3: logging of per-word updates\n-\t//            == 4: logging of per-word reads\n-\tstackDebug       = 0\n-\tstackFromSystem  = 0 // allocate stacks from system memory instead of the heap\n-\tstackFaultOnFree = 0 // old stacks are mapped noaccess to detect use after free\n-\tstackPoisonCopy  = 0 // fill stack that should not be accessed with garbage, to detect bad dereferences during copy\n-\n-\tstackCache = 1\n-\n-\t// check the BP links during traceback.\n-\tdebugCheckBP = false\n-)\n-\n-const (\n-\tuintptrMask = 1<<(8*sys.PtrSize) - 1\n-\n-\t// Goroutine preemption request.\n-\t// Stored into g->stackguard0 to cause split stack check failure.\n-\t// Must be greater than any real sp.\n-\t// 0xfffffade in hex.\n-\tstackPreempt = uintptrMask & -1314\n-\n-\t// Thread is forking.\n-\t// Stored into g->stackguard0 to cause split stack check failure.\n-\t// Must be greater than any real sp.\n-\tstackFork = uintptrMask & -1234\n-)\n-\n-// Global pool of spans that have free stacks.\n-// Stacks are assigned an order according to size.\n-//     order = log_2(size/FixedStack)\n-// There is a free list for each order.\n-// TODO: one lock per order?\n-var stackpool [_NumStackOrders]mSpanList\n-var stackpoolmu mutex\n-\n-// Global pool of large stack spans.\n-var stackLarge struct {\n-\tlock mutex\n-\tfree [_MHeapMap_Bits]mSpanList // free lists by log_2(s.npages)\n-}\n-\n-func stackinit() {\n-\tif _StackCacheSize&_PageMask != 0 {\n-\t\tthrow(\"cache size must be a multiple of page size\")\n-\t}\n-\tfor i := range stackpool {\n-\t\tstackpool[i].init()\n-\t}\n-\tfor i := range stackLarge.free {\n-\t\tstackLarge.free[i].init()\n-\t}\n-}\n-\n-// stacklog2 returns \u230alog_2(n)\u230b.\n-func stacklog2(n uintptr) int {\n-\tlog2 := 0\n-\tfor n > 1 {\n-\t\tn >>= 1\n-\t\tlog2++\n-\t}\n-\treturn log2\n-}\n-\n-// Allocates a stack from the free pool. Must be called with\n-// stackpoolmu held.\n-func stackpoolalloc(order uint8) gclinkptr {\n-\tlist := &stackpool[order]\n-\ts := list.first\n-\tif s == nil {\n-\t\t// no free stacks. Allocate another span worth.\n-\t\ts = mheap_.allocStack(_StackCacheSize >> _PageShift)\n-\t\tif s == nil {\n-\t\t\tthrow(\"out of memory\")\n-\t\t}\n-\t\tif s.allocCount != 0 {\n-\t\t\tthrow(\"bad allocCount\")\n-\t\t}\n-\t\tif s.stackfreelist.ptr() != nil {\n-\t\t\tthrow(\"bad stackfreelist\")\n-\t\t}\n-\t\tfor i := uintptr(0); i < _StackCacheSize; i += _FixedStack << order {\n-\t\t\tx := gclinkptr(s.base() + i)\n-\t\t\tx.ptr().next = s.stackfreelist\n-\t\t\ts.stackfreelist = x\n-\t\t}\n-\t\tlist.insert(s)\n-\t}\n-\tx := s.stackfreelist\n-\tif x.ptr() == nil {\n-\t\tthrow(\"span has no free stacks\")\n-\t}\n-\ts.stackfreelist = x.ptr().next\n-\ts.allocCount++\n-\tif s.stackfreelist.ptr() == nil {\n-\t\t// all stacks in s are allocated.\n-\t\tlist.remove(s)\n-\t}\n-\treturn x\n-}\n-\n-// Adds stack x to the free pool. Must be called with stackpoolmu held.\n-func stackpoolfree(x gclinkptr, order uint8) {\n-\ts := mheap_.lookup(unsafe.Pointer(x))\n-\tif s.state != _MSpanStack {\n-\t\tthrow(\"freeing stack not in a stack span\")\n-\t}\n-\tif s.stackfreelist.ptr() == nil {\n-\t\t// s will now have a free stack\n-\t\tstackpool[order].insert(s)\n-\t}\n-\tx.ptr().next = s.stackfreelist\n-\ts.stackfreelist = x\n-\ts.allocCount--\n-\tif gcphase == _GCoff && s.allocCount == 0 {\n-\t\t// Span is completely free. Return it to the heap\n-\t\t// immediately if we're sweeping.\n-\t\t//\n-\t\t// If GC is active, we delay the free until the end of\n-\t\t// GC to avoid the following type of situation:\n-\t\t//\n-\t\t// 1) GC starts, scans a SudoG but does not yet mark the SudoG.elem pointer\n-\t\t// 2) The stack that pointer points to is copied\n-\t\t// 3) The old stack is freed\n-\t\t// 4) The containing span is marked free\n-\t\t// 5) GC attempts to mark the SudoG.elem pointer. The\n-\t\t//    marking fails because the pointer looks like a\n-\t\t//    pointer into a free span.\n-\t\t//\n-\t\t// By not freeing, we prevent step #4 until GC is done.\n-\t\tstackpool[order].remove(s)\n-\t\ts.stackfreelist = 0\n-\t\tmheap_.freeStack(s)\n-\t}\n-}\n-\n-// stackcacherefill/stackcacherelease implement a global pool of stack segments.\n-// The pool is required to prevent unlimited growth of per-thread caches.\n-//\n-//go:systemstack\n-func stackcacherefill(c *mcache, order uint8) {\n-\tif stackDebug >= 1 {\n-\t\tprint(\"stackcacherefill order=\", order, \"\\n\")\n-\t}\n-\n-\t// Grab some stacks from the global cache.\n-\t// Grab half of the allowed capacity (to prevent thrashing).\n-\tvar list gclinkptr\n-\tvar size uintptr\n-\tlock(&stackpoolmu)\n-\tfor size < _StackCacheSize/2 {\n-\t\tx := stackpoolalloc(order)\n-\t\tx.ptr().next = list\n-\t\tlist = x\n-\t\tsize += _FixedStack << order\n-\t}\n-\tunlock(&stackpoolmu)\n-\tc.stackcache[order].list = list\n-\tc.stackcache[order].size = size\n-}\n-\n-//go:systemstack\n-func stackcacherelease(c *mcache, order uint8) {\n-\tif stackDebug >= 1 {\n-\t\tprint(\"stackcacherelease order=\", order, \"\\n\")\n-\t}\n-\tx := c.stackcache[order].list\n-\tsize := c.stackcache[order].size\n-\tlock(&stackpoolmu)\n-\tfor size > _StackCacheSize/2 {\n-\t\ty := x.ptr().next\n-\t\tstackpoolfree(x, order)\n-\t\tx = y\n-\t\tsize -= _FixedStack << order\n-\t}\n-\tunlock(&stackpoolmu)\n-\tc.stackcache[order].list = x\n-\tc.stackcache[order].size = size\n-}\n-\n-//go:systemstack\n-func stackcache_clear(c *mcache) {\n-\tif stackDebug >= 1 {\n-\t\tprint(\"stackcache clear\\n\")\n-\t}\n-\tlock(&stackpoolmu)\n-\tfor order := uint8(0); order < _NumStackOrders; order++ {\n-\t\tx := c.stackcache[order].list\n-\t\tfor x.ptr() != nil {\n-\t\t\ty := x.ptr().next\n-\t\t\tstackpoolfree(x, order)\n-\t\t\tx = y\n-\t\t}\n-\t\tc.stackcache[order].list = 0\n-\t\tc.stackcache[order].size = 0\n-\t}\n-\tunlock(&stackpoolmu)\n-}\n-\n-// stackalloc allocates an n byte stack.\n-//\n-// stackalloc must run on the system stack because it uses per-P\n-// resources and must not split the stack.\n-//\n-//go:systemstack\n-func stackalloc(n uint32) (stack, []stkbar) {\n-\t// Stackalloc must be called on scheduler stack, so that we\n-\t// never try to grow the stack during the code that stackalloc runs.\n-\t// Doing so would cause a deadlock (issue 1547).\n-\tthisg := getg()\n-\tif thisg != thisg.m.g0 {\n-\t\tthrow(\"stackalloc not on scheduler stack\")\n-\t}\n-\tif n&(n-1) != 0 {\n-\t\tthrow(\"stack size not a power of 2\")\n-\t}\n-\tif stackDebug >= 1 {\n-\t\tprint(\"stackalloc \", n, \"\\n\")\n-\t}\n-\n-\t// Compute the size of stack barrier array.\n-\tmaxstkbar := gcMaxStackBarriers(int(n))\n-\tnstkbar := unsafe.Sizeof(stkbar{}) * uintptr(maxstkbar)\n-\tvar stkbarSlice slice\n-\n-\tif debug.efence != 0 || stackFromSystem != 0 {\n-\t\tv := sysAlloc(round(uintptr(n), _PageSize), &memstats.stacks_sys)\n-\t\tif v == nil {\n-\t\t\tthrow(\"out of memory (stackalloc)\")\n-\t\t}\n-\t\ttop := uintptr(n) - nstkbar\n-\t\tif maxstkbar != 0 {\n-\t\t\tstkbarSlice = slice{add(v, top), 0, maxstkbar}\n-\t\t}\n-\t\treturn stack{uintptr(v), uintptr(v) + top}, *(*[]stkbar)(unsafe.Pointer(&stkbarSlice))\n-\t}\n-\n-\t// Small stacks are allocated with a fixed-size free-list allocator.\n-\t// If we need a stack of a bigger size, we fall back on allocating\n-\t// a dedicated span.\n-\tvar v unsafe.Pointer\n-\tif stackCache != 0 && n < _FixedStack<<_NumStackOrders && n < _StackCacheSize {\n-\t\torder := uint8(0)\n-\t\tn2 := n\n-\t\tfor n2 > _FixedStack {\n-\t\t\torder++\n-\t\t\tn2 >>= 1\n-\t\t}\n-\t\tvar x gclinkptr\n-\t\tc := thisg.m.mcache\n-\t\tif c == nil || thisg.m.preemptoff != \"\" || thisg.m.helpgc != 0 {\n-\t\t\t// c == nil can happen in the guts of exitsyscall or\n-\t\t\t// procresize. Just get a stack from the global pool.\n-\t\t\t// Also don't touch stackcache during gc\n-\t\t\t// as it's flushed concurrently.\n-\t\t\tlock(&stackpoolmu)\n-\t\t\tx = stackpoolalloc(order)\n-\t\t\tunlock(&stackpoolmu)\n-\t\t} else {\n-\t\t\tx = c.stackcache[order].list\n-\t\t\tif x.ptr() == nil {\n-\t\t\t\tstackcacherefill(c, order)\n-\t\t\t\tx = c.stackcache[order].list\n-\t\t\t}\n-\t\t\tc.stackcache[order].list = x.ptr().next\n-\t\t\tc.stackcache[order].size -= uintptr(n)\n-\t\t}\n-\t\tv = unsafe.Pointer(x)\n-\t} else {\n-\t\tvar s *mspan\n-\t\tnpage := uintptr(n) >> _PageShift\n-\t\tlog2npage := stacklog2(npage)\n-\n-\t\t// Try to get a stack from the large stack cache.\n-\t\tlock(&stackLarge.lock)\n-\t\tif !stackLarge.free[log2npage].isEmpty() {\n-\t\t\ts = stackLarge.free[log2npage].first\n-\t\t\tstackLarge.free[log2npage].remove(s)\n-\t\t}\n-\t\tunlock(&stackLarge.lock)\n-\n-\t\tif s == nil {\n-\t\t\t// Allocate a new stack from the heap.\n-\t\t\ts = mheap_.allocStack(npage)\n-\t\t\tif s == nil {\n-\t\t\t\tthrow(\"out of memory\")\n-\t\t\t}\n-\t\t}\n-\t\tv = unsafe.Pointer(s.base())\n-\t}\n-\n-\tif raceenabled {\n-\t\tracemalloc(v, uintptr(n))\n-\t}\n-\tif msanenabled {\n-\t\tmsanmalloc(v, uintptr(n))\n-\t}\n-\tif stackDebug >= 1 {\n-\t\tprint(\"  allocated \", v, \"\\n\")\n-\t}\n-\ttop := uintptr(n) - nstkbar\n-\tif maxstkbar != 0 {\n-\t\tstkbarSlice = slice{add(v, top), 0, maxstkbar}\n-\t}\n-\treturn stack{uintptr(v), uintptr(v) + top}, *(*[]stkbar)(unsafe.Pointer(&stkbarSlice))\n-}\n-\n-// stackfree frees an n byte stack allocation at stk.\n-//\n-// stackfree must run on the system stack because it uses per-P\n-// resources and must not split the stack.\n-//\n-//go:systemstack\n-func stackfree(stk stack, n uintptr) {\n-\tgp := getg()\n-\tv := unsafe.Pointer(stk.lo)\n-\tif n&(n-1) != 0 {\n-\t\tthrow(\"stack not a power of 2\")\n-\t}\n-\tif stk.lo+n < stk.hi {\n-\t\tthrow(\"bad stack size\")\n-\t}\n-\tif stackDebug >= 1 {\n-\t\tprintln(\"stackfree\", v, n)\n-\t\tmemclrNoHeapPointers(v, n) // for testing, clobber stack data\n-\t}\n-\tif debug.efence != 0 || stackFromSystem != 0 {\n-\t\tif debug.efence != 0 || stackFaultOnFree != 0 {\n-\t\t\tsysFault(v, n)\n-\t\t} else {\n-\t\t\tsysFree(v, n, &memstats.stacks_sys)\n-\t\t}\n-\t\treturn\n-\t}\n-\tif msanenabled {\n-\t\tmsanfree(v, n)\n-\t}\n-\tif stackCache != 0 && n < _FixedStack<<_NumStackOrders && n < _StackCacheSize {\n-\t\torder := uint8(0)\n-\t\tn2 := n\n-\t\tfor n2 > _FixedStack {\n-\t\t\torder++\n-\t\t\tn2 >>= 1\n-\t\t}\n-\t\tx := gclinkptr(v)\n-\t\tc := gp.m.mcache\n-\t\tif c == nil || gp.m.preemptoff != \"\" || gp.m.helpgc != 0 {\n-\t\t\tlock(&stackpoolmu)\n-\t\t\tstackpoolfree(x, order)\n-\t\t\tunlock(&stackpoolmu)\n-\t\t} else {\n-\t\t\tif c.stackcache[order].size >= _StackCacheSize {\n-\t\t\t\tstackcacherelease(c, order)\n-\t\t\t}\n-\t\t\tx.ptr().next = c.stackcache[order].list\n-\t\t\tc.stackcache[order].list = x\n-\t\t\tc.stackcache[order].size += n\n-\t\t}\n-\t} else {\n-\t\ts := mheap_.lookup(v)\n-\t\tif s.state != _MSpanStack {\n-\t\t\tprintln(hex(s.base()), v)\n-\t\t\tthrow(\"bad span state\")\n-\t\t}\n-\t\tif gcphase == _GCoff {\n-\t\t\t// Free the stack immediately if we're\n-\t\t\t// sweeping.\n-\t\t\tmheap_.freeStack(s)\n-\t\t} else {\n-\t\t\t// If the GC is running, we can't return a\n-\t\t\t// stack span to the heap because it could be\n-\t\t\t// reused as a heap span, and this state\n-\t\t\t// change would race with GC. Add it to the\n-\t\t\t// large stack cache instead.\n-\t\t\tlog2npage := stacklog2(s.npages)\n-\t\t\tlock(&stackLarge.lock)\n-\t\t\tstackLarge.free[log2npage].insert(s)\n-\t\t\tunlock(&stackLarge.lock)\n-\t\t}\n-\t}\n-}\n-\n-var maxstacksize uintptr = 1 << 20 // enough until runtime.main sets it for real\n-\n-var ptrnames = []string{\n-\t0: \"scalar\",\n-\t1: \"ptr\",\n-}\n-\n-// Stack frame layout\n-//\n-// (x86)\n-// +------------------+\n-// | args from caller |\n-// +------------------+ <- frame->argp\n-// |  return address  |\n-// +------------------+\n-// |  caller's BP (*) | (*) if framepointer_enabled && varp < sp\n-// +------------------+ <- frame->varp\n-// |     locals       |\n-// +------------------+\n-// |  args to callee  |\n-// +------------------+ <- frame->sp\n-//\n-// (arm)\n-// +------------------+\n-// | args from caller |\n-// +------------------+ <- frame->argp\n-// | caller's retaddr |\n-// +------------------+ <- frame->varp\n-// |     locals       |\n-// +------------------+\n-// |  args to callee  |\n-// +------------------+\n-// |  return address  |\n-// +------------------+ <- frame->sp\n-\n-type adjustinfo struct {\n-\told   stack\n-\tdelta uintptr // ptr distance from old to new stack (newbase - oldbase)\n-\tcache pcvalueCache\n-\n-\t// sghi is the highest sudog.elem on the stack.\n-\tsghi uintptr\n-}\n-\n-// Adjustpointer checks whether *vpp is in the old stack described by adjinfo.\n-// If so, it rewrites *vpp to point into the new stack.\n-func adjustpointer(adjinfo *adjustinfo, vpp unsafe.Pointer) {\n-\tpp := (*uintptr)(vpp)\n-\tp := *pp\n-\tif stackDebug >= 4 {\n-\t\tprint(\"        \", pp, \":\", hex(p), \"\\n\")\n-\t}\n-\tif adjinfo.old.lo <= p && p < adjinfo.old.hi {\n-\t\t*pp = p + adjinfo.delta\n-\t\tif stackDebug >= 3 {\n-\t\t\tprint(\"        adjust ptr \", pp, \":\", hex(p), \" -> \", hex(*pp), \"\\n\")\n-\t\t}\n-\t}\n-}\n-\n-// Information from the compiler about the layout of stack frames.\n-type bitvector struct {\n-\tn        int32 // # of bits\n-\tbytedata *uint8\n-}\n-\n-type gobitvector struct {\n-\tn        uintptr\n-\tbytedata []uint8\n-}\n-\n-func gobv(bv bitvector) gobitvector {\n-\treturn gobitvector{\n-\t\tuintptr(bv.n),\n-\t\t(*[1 << 30]byte)(unsafe.Pointer(bv.bytedata))[:(bv.n+7)/8],\n-\t}\n-}\n-\n-func ptrbit(bv *gobitvector, i uintptr) uint8 {\n-\treturn (bv.bytedata[i/8] >> (i % 8)) & 1\n-}\n-\n-// bv describes the memory starting at address scanp.\n-// Adjust any pointers contained therein.\n-func adjustpointers(scanp unsafe.Pointer, cbv *bitvector, adjinfo *adjustinfo, f *_func) {\n-\tbv := gobv(*cbv)\n-\tminp := adjinfo.old.lo\n-\tmaxp := adjinfo.old.hi\n-\tdelta := adjinfo.delta\n-\tnum := bv.n\n-\t// If this frame might contain channel receive slots, use CAS\n-\t// to adjust pointers. If the slot hasn't been received into\n-\t// yet, it may contain stack pointers and a concurrent send\n-\t// could race with adjusting those pointers. (The sent value\n-\t// itself can never contain stack pointers.)\n-\tuseCAS := uintptr(scanp) < adjinfo.sghi\n-\tfor i := uintptr(0); i < num; i++ {\n-\t\tif stackDebug >= 4 {\n-\t\t\tprint(\"        \", add(scanp, i*sys.PtrSize), \":\", ptrnames[ptrbit(&bv, i)], \":\", hex(*(*uintptr)(add(scanp, i*sys.PtrSize))), \" # \", i, \" \", bv.bytedata[i/8], \"\\n\")\n-\t\t}\n-\t\tif ptrbit(&bv, i) == 1 {\n-\t\t\tpp := (*uintptr)(add(scanp, i*sys.PtrSize))\n-\t\tretry:\n-\t\t\tp := *pp\n-\t\t\tif f != nil && 0 < p && p < minLegalPointer && debug.invalidptr != 0 {\n-\t\t\t\t// Looks like a junk value in a pointer slot.\n-\t\t\t\t// Live analysis wrong?\n-\t\t\t\tgetg().m.traceback = 2\n-\t\t\t\tprint(\"runtime: bad pointer in frame \", funcname(f), \" at \", pp, \": \", hex(p), \"\\n\")\n-\t\t\t\tthrow(\"invalid pointer found on stack\")\n-\t\t\t}\n-\t\t\tif minp <= p && p < maxp {\n-\t\t\t\tif stackDebug >= 3 {\n-\t\t\t\t\tprint(\"adjust ptr \", hex(p), \" \", funcname(f), \"\\n\")\n-\t\t\t\t}\n-\t\t\t\tif useCAS {\n-\t\t\t\t\tppu := (*unsafe.Pointer)(unsafe.Pointer(pp))\n-\t\t\t\t\tif !atomic.Casp1(ppu, unsafe.Pointer(p), unsafe.Pointer(p+delta)) {\n-\t\t\t\t\t\tgoto retry\n-\t\t\t\t\t}\n-\t\t\t\t} else {\n-\t\t\t\t\t*pp = p + delta\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-}\n-\n-// Note: the argument/return area is adjusted by the callee.\n-func adjustframe(frame *stkframe, arg unsafe.Pointer) bool {\n-\tadjinfo := (*adjustinfo)(arg)\n-\ttargetpc := frame.continpc\n-\tif targetpc == 0 {\n-\t\t// Frame is dead.\n-\t\treturn true\n-\t}\n-\tf := frame.fn\n-\tif stackDebug >= 2 {\n-\t\tprint(\"    adjusting \", funcname(f), \" frame=[\", hex(frame.sp), \",\", hex(frame.fp), \"] pc=\", hex(frame.pc), \" continpc=\", hex(frame.continpc), \"\\n\")\n-\t}\n-\tif f.entry == systemstack_switchPC {\n-\t\t// A special routine at the bottom of stack of a goroutine that does an systemstack call.\n-\t\t// We will allow it to be copied even though we don't\n-\t\t// have full GC info for it (because it is written in asm).\n-\t\treturn true\n-\t}\n-\tif targetpc != f.entry {\n-\t\ttargetpc--\n-\t}\n-\tpcdata := pcdatavalue(f, _PCDATA_StackMapIndex, targetpc, &adjinfo.cache)\n-\tif pcdata == -1 {\n-\t\tpcdata = 0 // in prologue\n-\t}\n-\n-\t// Adjust local variables if stack frame has been allocated.\n-\tsize := frame.varp - frame.sp\n-\tvar minsize uintptr\n-\tswitch sys.ArchFamily {\n-\tcase sys.ARM64:\n-\t\tminsize = sys.SpAlign\n-\tdefault:\n-\t\tminsize = sys.MinFrameSize\n-\t}\n-\tif size > minsize {\n-\t\tvar bv bitvector\n-\t\tstackmap := (*stackmap)(funcdata(f, _FUNCDATA_LocalsPointerMaps))\n-\t\tif stackmap == nil || stackmap.n <= 0 {\n-\t\t\tprint(\"runtime: frame \", funcname(f), \" untyped locals \", hex(frame.varp-size), \"+\", hex(size), \"\\n\")\n-\t\t\tthrow(\"missing stackmap\")\n-\t\t}\n-\t\t// Locals bitmap information, scan just the pointers in locals.\n-\t\tif pcdata < 0 || pcdata >= stackmap.n {\n-\t\t\t// don't know where we are\n-\t\t\tprint(\"runtime: pcdata is \", pcdata, \" and \", stackmap.n, \" locals stack map entries for \", funcname(f), \" (targetpc=\", targetpc, \")\\n\")\n-\t\t\tthrow(\"bad symbol table\")\n-\t\t}\n-\t\tbv = stackmapdata(stackmap, pcdata)\n-\t\tsize = uintptr(bv.n) * sys.PtrSize\n-\t\tif stackDebug >= 3 {\n-\t\t\tprint(\"      locals \", pcdata, \"/\", stackmap.n, \" \", size/sys.PtrSize, \" words \", bv.bytedata, \"\\n\")\n-\t\t}\n-\t\tadjustpointers(unsafe.Pointer(frame.varp-size), &bv, adjinfo, f)\n-\t}\n-\n-\t// Adjust saved base pointer if there is one.\n-\tif sys.ArchFamily == sys.AMD64 && frame.argp-frame.varp == 2*sys.RegSize {\n-\t\tif !framepointer_enabled {\n-\t\t\tprint(\"runtime: found space for saved base pointer, but no framepointer experiment\\n\")\n-\t\t\tprint(\"argp=\", hex(frame.argp), \" varp=\", hex(frame.varp), \"\\n\")\n-\t\t\tthrow(\"bad frame layout\")\n-\t\t}\n-\t\tif stackDebug >= 3 {\n-\t\t\tprint(\"      saved bp\\n\")\n-\t\t}\n-\t\tif debugCheckBP {\n-\t\t\t// Frame pointers should always point to the next higher frame on\n-\t\t\t// the Go stack (or be nil, for the top frame on the stack).\n-\t\t\tbp := *(*uintptr)(unsafe.Pointer(frame.varp))\n-\t\t\tif bp != 0 && (bp < adjinfo.old.lo || bp >= adjinfo.old.hi) {\n-\t\t\t\tprintln(\"runtime: found invalid frame pointer\")\n-\t\t\t\tprint(\"bp=\", hex(bp), \" min=\", hex(adjinfo.old.lo), \" max=\", hex(adjinfo.old.hi), \"\\n\")\n-\t\t\t\tthrow(\"bad frame pointer\")\n-\t\t\t}\n-\t\t}\n-\t\tadjustpointer(adjinfo, unsafe.Pointer(frame.varp))\n-\t}\n-\n-\t// Adjust arguments.\n-\tif frame.arglen > 0 {\n-\t\tvar bv bitvector\n-\t\tif frame.argmap != nil {\n-\t\t\tbv = *frame.argmap\n-\t\t} else {\n-\t\t\tstackmap := (*stackmap)(funcdata(f, _FUNCDATA_ArgsPointerMaps))\n-\t\t\tif stackmap == nil || stackmap.n <= 0 {\n-\t\t\t\tprint(\"runtime: frame \", funcname(f), \" untyped args \", frame.argp, \"+\", frame.arglen, \"\\n\")\n-\t\t\t\tthrow(\"missing stackmap\")\n-\t\t\t}\n-\t\t\tif pcdata < 0 || pcdata >= stackmap.n {\n-\t\t\t\t// don't know where we are\n-\t\t\t\tprint(\"runtime: pcdata is \", pcdata, \" and \", stackmap.n, \" args stack map entries for \", funcname(f), \" (targetpc=\", targetpc, \")\\n\")\n-\t\t\t\tthrow(\"bad symbol table\")\n-\t\t\t}\n-\t\t\tbv = stackmapdata(stackmap, pcdata)\n-\t\t}\n-\t\tif stackDebug >= 3 {\n-\t\t\tprint(\"      args\\n\")\n-\t\t}\n-\t\tadjustpointers(unsafe.Pointer(frame.argp), &bv, adjinfo, nil)\n-\t}\n-\treturn true\n-}\n-\n-func adjustctxt(gp *g, adjinfo *adjustinfo) {\n-\tadjustpointer(adjinfo, unsafe.Pointer(&gp.sched.ctxt))\n-\tif !framepointer_enabled {\n-\t\treturn\n-\t}\n-\tif debugCheckBP {\n-\t\tbp := gp.sched.bp\n-\t\tif bp != 0 && (bp < adjinfo.old.lo || bp >= adjinfo.old.hi) {\n-\t\t\tprintln(\"runtime: found invalid top frame pointer\")\n-\t\t\tprint(\"bp=\", hex(bp), \" min=\", hex(adjinfo.old.lo), \" max=\", hex(adjinfo.old.hi), \"\\n\")\n-\t\t\tthrow(\"bad top frame pointer\")\n-\t\t}\n-\t}\n-\tadjustpointer(adjinfo, unsafe.Pointer(&gp.sched.bp))\n-}\n-\n-func adjustdefers(gp *g, adjinfo *adjustinfo) {\n-\t// Adjust defer argument blocks the same way we adjust active stack frames.\n-\ttracebackdefers(gp, adjustframe, noescape(unsafe.Pointer(adjinfo)))\n-\n-\t// Adjust pointers in the Defer structs.\n-\t// Defer structs themselves are never on the stack.\n-\tfor d := gp._defer; d != nil; d = d.link {\n-\t\tadjustpointer(adjinfo, unsafe.Pointer(&d.fn))\n-\t\tadjustpointer(adjinfo, unsafe.Pointer(&d.sp))\n-\t\tadjustpointer(adjinfo, unsafe.Pointer(&d._panic))\n-\t}\n-}\n-\n-func adjustpanics(gp *g, adjinfo *adjustinfo) {\n-\t// Panics are on stack and already adjusted.\n-\t// Update pointer to head of list in G.\n-\tadjustpointer(adjinfo, unsafe.Pointer(&gp._panic))\n-}\n-\n-func adjustsudogs(gp *g, adjinfo *adjustinfo) {\n-\t// the data elements pointed to by a SudoG structure\n-\t// might be in the stack.\n-\tfor s := gp.waiting; s != nil; s = s.waitlink {\n-\t\tadjustpointer(adjinfo, unsafe.Pointer(&s.elem))\n-\t\tadjustpointer(adjinfo, unsafe.Pointer(&s.selectdone))\n-\t}\n-}\n-\n-func adjuststkbar(gp *g, adjinfo *adjustinfo) {\n-\tfor i := int(gp.stkbarPos); i < len(gp.stkbar); i++ {\n-\t\tadjustpointer(adjinfo, unsafe.Pointer(&gp.stkbar[i].savedLRPtr))\n-\t}\n-}\n-\n-func fillstack(stk stack, b byte) {\n-\tfor p := stk.lo; p < stk.hi; p++ {\n-\t\t*(*byte)(unsafe.Pointer(p)) = b\n-\t}\n-}\n-\n-func findsghi(gp *g, stk stack) uintptr {\n-\tvar sghi uintptr\n-\tfor sg := gp.waiting; sg != nil; sg = sg.waitlink {\n-\t\tp := uintptr(sg.elem) + uintptr(sg.c.elemsize)\n-\t\tif stk.lo <= p && p < stk.hi && p > sghi {\n-\t\t\tsghi = p\n-\t\t}\n-\t\tp = uintptr(unsafe.Pointer(sg.selectdone)) + unsafe.Sizeof(sg.selectdone)\n-\t\tif stk.lo <= p && p < stk.hi && p > sghi {\n-\t\t\tsghi = p\n-\t\t}\n-\t}\n-\treturn sghi\n-}\n-\n-// syncadjustsudogs adjusts gp's sudogs and copies the part of gp's\n-// stack they refer to while synchronizing with concurrent channel\n-// operations. It returns the number of bytes of stack copied.\n-func syncadjustsudogs(gp *g, used uintptr, adjinfo *adjustinfo) uintptr {\n-\tif gp.waiting == nil {\n-\t\treturn 0\n-\t}\n-\n-\t// Lock channels to prevent concurrent send/receive.\n-\t// It's important that we *only* do this for async\n-\t// copystack; otherwise, gp may be in the middle of\n-\t// putting itself on wait queues and this would\n-\t// self-deadlock.\n-\tvar lastc *hchan\n-\tfor sg := gp.waiting; sg != nil; sg = sg.waitlink {\n-\t\tif sg.c != lastc {\n-\t\t\tlock(&sg.c.lock)\n-\t\t}\n-\t\tlastc = sg.c\n-\t}\n-\n-\t// Adjust sudogs.\n-\tadjustsudogs(gp, adjinfo)\n-\n-\t// Copy the part of the stack the sudogs point in to\n-\t// while holding the lock to prevent races on\n-\t// send/receive slots.\n-\tvar sgsize uintptr\n-\tif adjinfo.sghi != 0 {\n-\t\toldBot := adjinfo.old.hi - used\n-\t\tnewBot := oldBot + adjinfo.delta\n-\t\tsgsize = adjinfo.sghi - oldBot\n-\t\tmemmove(unsafe.Pointer(newBot), unsafe.Pointer(oldBot), sgsize)\n-\t}\n-\n-\t// Unlock channels.\n-\tlastc = nil\n-\tfor sg := gp.waiting; sg != nil; sg = sg.waitlink {\n-\t\tif sg.c != lastc {\n-\t\t\tunlock(&sg.c.lock)\n-\t\t}\n-\t\tlastc = sg.c\n-\t}\n-\n-\treturn sgsize\n-}\n-\n-// Copies gp's stack to a new stack of a different size.\n-// Caller must have changed gp status to Gcopystack.\n-//\n-// If sync is true, this is a self-triggered stack growth and, in\n-// particular, no other G may be writing to gp's stack (e.g., via a\n-// channel operation). If sync is false, copystack protects against\n-// concurrent channel operations.\n-func copystack(gp *g, newsize uintptr, sync bool) {\n-\tif gp.syscallsp != 0 {\n-\t\tthrow(\"stack growth not allowed in system call\")\n-\t}\n-\told := gp.stack\n-\tif old.lo == 0 {\n-\t\tthrow(\"nil stackbase\")\n-\t}\n-\tused := old.hi - gp.sched.sp\n-\n-\t// allocate new stack\n-\tnew, newstkbar := stackalloc(uint32(newsize))\n-\tif stackPoisonCopy != 0 {\n-\t\tfillstack(new, 0xfd)\n-\t}\n-\tif stackDebug >= 1 {\n-\t\tprint(\"copystack gp=\", gp, \" [\", hex(old.lo), \" \", hex(old.hi-used), \" \", hex(old.hi), \"]/\", gp.stackAlloc, \" -> [\", hex(new.lo), \" \", hex(new.hi-used), \" \", hex(new.hi), \"]/\", newsize, \"\\n\")\n-\t}\n-\n-\t// Compute adjustment.\n-\tvar adjinfo adjustinfo\n-\tadjinfo.old = old\n-\tadjinfo.delta = new.hi - old.hi\n-\n-\t// Adjust sudogs, synchronizing with channel ops if necessary.\n-\tncopy := used\n-\tif sync {\n-\t\tadjustsudogs(gp, &adjinfo)\n-\t} else {\n-\t\t// sudogs can point in to the stack. During concurrent\n-\t\t// shrinking, these areas may be written to. Find the\n-\t\t// highest such pointer so we can handle everything\n-\t\t// there and below carefully. (This shouldn't be far\n-\t\t// from the bottom of the stack, so there's little\n-\t\t// cost in handling everything below it carefully.)\n-\t\tadjinfo.sghi = findsghi(gp, old)\n-\n-\t\t// Synchronize with channel ops and copy the part of\n-\t\t// the stack they may interact with.\n-\t\tncopy -= syncadjustsudogs(gp, used, &adjinfo)\n-\t}\n-\n-\t// Copy the stack (or the rest of it) to the new location\n-\tmemmove(unsafe.Pointer(new.hi-ncopy), unsafe.Pointer(old.hi-ncopy), ncopy)\n-\n-\t// Disallow sigprof scans of this stack and block if there's\n-\t// one in progress.\n-\tgcLockStackBarriers(gp)\n-\n-\t// Adjust remaining structures that have pointers into stacks.\n-\t// We have to do most of these before we traceback the new\n-\t// stack because gentraceback uses them.\n-\tadjustctxt(gp, &adjinfo)\n-\tadjustdefers(gp, &adjinfo)\n-\tadjustpanics(gp, &adjinfo)\n-\tadjuststkbar(gp, &adjinfo)\n-\tif adjinfo.sghi != 0 {\n-\t\tadjinfo.sghi += adjinfo.delta\n-\t}\n-\n-\t// copy old stack barriers to new stack barrier array\n-\tnewstkbar = newstkbar[:len(gp.stkbar)]\n-\tcopy(newstkbar, gp.stkbar)\n-\n-\t// Swap out old stack for new one\n-\tgp.stack = new\n-\tgp.stackguard0 = new.lo + _StackGuard // NOTE: might clobber a preempt request\n-\tgp.sched.sp = new.hi - used\n-\toldsize := gp.stackAlloc\n-\tgp.stackAlloc = newsize\n-\tgp.stkbar = newstkbar\n-\tgp.stktopsp += adjinfo.delta\n-\n-\t// Adjust pointers in the new stack.\n-\tgentraceback(^uintptr(0), ^uintptr(0), 0, gp, 0, nil, 0x7fffffff, adjustframe, noescape(unsafe.Pointer(&adjinfo)), 0)\n-\n-\tgcUnlockStackBarriers(gp)\n-\n-\t// free old stack\n-\tif stackPoisonCopy != 0 {\n-\t\tfillstack(old, 0xfc)\n-\t}\n-\tstackfree(old, oldsize)\n-}\n-\n-// round x up to a power of 2.\n-func round2(x int32) int32 {\n-\ts := uint(0)\n-\tfor 1<<s < x {\n-\t\ts++\n-\t}\n-\treturn 1 << s\n-}\n-\n-// Called from runtime\u00b7morestack when more stack is needed.\n-// Allocate larger stack and relocate to new stack.\n-// Stack growth is multiplicative, for constant amortized cost.\n-//\n-// g->atomicstatus will be Grunning or Gscanrunning upon entry.\n-// If the GC is trying to stop this g then it will set preemptscan to true.\n-//\n-// ctxt is the value of the context register on morestack. newstack\n-// will write it to g.sched.ctxt.\n-func newstack(ctxt unsafe.Pointer) {\n-\tthisg := getg()\n-\t// TODO: double check all gp. shouldn't be getg().\n-\tif thisg.m.morebuf.g.ptr().stackguard0 == stackFork {\n-\t\tthrow(\"stack growth after fork\")\n-\t}\n-\tif thisg.m.morebuf.g.ptr() != thisg.m.curg {\n-\t\tprint(\"runtime: newstack called from g=\", hex(thisg.m.morebuf.g), \"\\n\"+\"\\tm=\", thisg.m, \" m->curg=\", thisg.m.curg, \" m->g0=\", thisg.m.g0, \" m->gsignal=\", thisg.m.gsignal, \"\\n\")\n-\t\tmorebuf := thisg.m.morebuf\n-\t\ttraceback(morebuf.pc, morebuf.sp, morebuf.lr, morebuf.g.ptr())\n-\t\tthrow(\"runtime: wrong goroutine in newstack\")\n-\t}\n-\n-\tgp := thisg.m.curg\n-\t// Write ctxt to gp.sched. We do this here instead of in\n-\t// morestack so it has the necessary write barrier.\n-\tgp.sched.ctxt = ctxt\n-\n-\tif thisg.m.curg.throwsplit {\n-\t\t// Update syscallsp, syscallpc in case traceback uses them.\n-\t\tmorebuf := thisg.m.morebuf\n-\t\tgp.syscallsp = morebuf.sp\n-\t\tgp.syscallpc = morebuf.pc\n-\t\tprint(\"runtime: newstack sp=\", hex(gp.sched.sp), \" stack=[\", hex(gp.stack.lo), \", \", hex(gp.stack.hi), \"]\\n\",\n-\t\t\t\"\\tmorebuf={pc:\", hex(morebuf.pc), \" sp:\", hex(morebuf.sp), \" lr:\", hex(morebuf.lr), \"}\\n\",\n-\t\t\t\"\\tsched={pc:\", hex(gp.sched.pc), \" sp:\", hex(gp.sched.sp), \" lr:\", hex(gp.sched.lr), \" ctxt:\", gp.sched.ctxt, \"}\\n\")\n-\n-\t\ttraceback(morebuf.pc, morebuf.sp, morebuf.lr, gp)\n-\t\tthrow(\"runtime: stack split at bad time\")\n-\t}\n-\n-\tmorebuf := thisg.m.morebuf\n-\tthisg.m.morebuf.pc = 0\n-\tthisg.m.morebuf.lr = 0\n-\tthisg.m.morebuf.sp = 0\n-\tthisg.m.morebuf.g = 0\n-\n-\t// NOTE: stackguard0 may change underfoot, if another thread\n-\t// is about to try to preempt gp. Read it just once and use that same\n-\t// value now and below.\n-\tpreempt := atomic.Loaduintptr(&gp.stackguard0) == stackPreempt\n-\n-\t// Be conservative about where we preempt.\n-\t// We are interested in preempting user Go code, not runtime code.\n-\t// If we're holding locks, mallocing, or preemption is disabled, don't\n-\t// preempt.\n-\t// This check is very early in newstack so that even the status change\n-\t// from Grunning to Gwaiting and back doesn't happen in this case.\n-\t// That status change by itself can be viewed as a small preemption,\n-\t// because the GC might change Gwaiting to Gscanwaiting, and then\n-\t// this goroutine has to wait for the GC to finish before continuing.\n-\t// If the GC is in some way dependent on this goroutine (for example,\n-\t// it needs a lock held by the goroutine), that small preemption turns\n-\t// into a real deadlock.\n-\tif preempt {\n-\t\tif thisg.m.locks != 0 || thisg.m.mallocing != 0 || thisg.m.preemptoff != \"\" || thisg.m.p.ptr().status != _Prunning {\n-\t\t\t// Let the goroutine keep running for now.\n-\t\t\t// gp->preempt is set, so it will be preempted next time.\n-\t\t\tgp.stackguard0 = gp.stack.lo + _StackGuard\n-\t\t\tgogo(&gp.sched) // never return\n-\t\t}\n-\t}\n-\n-\tif gp.stack.lo == 0 {\n-\t\tthrow(\"missing stack in newstack\")\n-\t}\n-\tsp := gp.sched.sp\n-\tif sys.ArchFamily == sys.AMD64 || sys.ArchFamily == sys.I386 {\n-\t\t// The call to morestack cost a word.\n-\t\tsp -= sys.PtrSize\n-\t}\n-\tif stackDebug >= 1 || sp < gp.stack.lo {\n-\t\tprint(\"runtime: newstack sp=\", hex(sp), \" stack=[\", hex(gp.stack.lo), \", \", hex(gp.stack.hi), \"]\\n\",\n-\t\t\t\"\\tmorebuf={pc:\", hex(morebuf.pc), \" sp:\", hex(morebuf.sp), \" lr:\", hex(morebuf.lr), \"}\\n\",\n-\t\t\t\"\\tsched={pc:\", hex(gp.sched.pc), \" sp:\", hex(gp.sched.sp), \" lr:\", hex(gp.sched.lr), \" ctxt:\", gp.sched.ctxt, \"}\\n\")\n-\t}\n-\tif sp < gp.stack.lo {\n-\t\tprint(\"runtime: gp=\", gp, \", gp->status=\", hex(readgstatus(gp)), \"\\n \")\n-\t\tprint(\"runtime: split stack overflow: \", hex(sp), \" < \", hex(gp.stack.lo), \"\\n\")\n-\t\tthrow(\"runtime: split stack overflow\")\n-\t}\n-\n-\tif preempt {\n-\t\tif gp == thisg.m.g0 {\n-\t\t\tthrow(\"runtime: preempt g0\")\n-\t\t}\n-\t\tif thisg.m.p == 0 && thisg.m.locks == 0 {\n-\t\t\tthrow(\"runtime: g is running but p is not\")\n-\t\t}\n-\t\t// Synchronize with scang.\n-\t\tcasgstatus(gp, _Grunning, _Gwaiting)\n-\t\tif gp.preemptscan {\n-\t\t\tfor !castogscanstatus(gp, _Gwaiting, _Gscanwaiting) {\n-\t\t\t\t// Likely to be racing with the GC as\n-\t\t\t\t// it sees a _Gwaiting and does the\n-\t\t\t\t// stack scan. If so, gcworkdone will\n-\t\t\t\t// be set and gcphasework will simply\n-\t\t\t\t// return.\n-\t\t\t}\n-\t\t\tif !gp.gcscandone {\n-\t\t\t\t// gcw is safe because we're on the\n-\t\t\t\t// system stack.\n-\t\t\t\tgcw := &gp.m.p.ptr().gcw\n-\t\t\t\tscanstack(gp, gcw)\n-\t\t\t\tif gcBlackenPromptly {\n-\t\t\t\t\tgcw.dispose()\n-\t\t\t\t}\n-\t\t\t\tgp.gcscandone = true\n-\t\t\t}\n-\t\t\tgp.preemptscan = false\n-\t\t\tgp.preempt = false\n-\t\t\tcasfrom_Gscanstatus(gp, _Gscanwaiting, _Gwaiting)\n-\t\t\t// This clears gcscanvalid.\n-\t\t\tcasgstatus(gp, _Gwaiting, _Grunning)\n-\t\t\tgp.stackguard0 = gp.stack.lo + _StackGuard\n-\t\t\tgogo(&gp.sched) // never return\n-\t\t}\n-\n-\t\t// Act like goroutine called runtime.Gosched.\n-\t\tcasgstatus(gp, _Gwaiting, _Grunning)\n-\t\tgopreempt_m(gp) // never return\n-\t}\n-\n-\t// Allocate a bigger segment and move the stack.\n-\toldsize := int(gp.stackAlloc)\n-\tnewsize := oldsize * 2\n-\tif uintptr(newsize) > maxstacksize {\n-\t\tprint(\"runtime: goroutine stack exceeds \", maxstacksize, \"-byte limit\\n\")\n-\t\tthrow(\"stack overflow\")\n-\t}\n-\n-\t// The goroutine must be executing in order to call newstack,\n-\t// so it must be Grunning (or Gscanrunning).\n-\tcasgstatus(gp, _Grunning, _Gcopystack)\n-\n-\t// The concurrent GC will not scan the stack while we are doing the copy since\n-\t// the gp is in a Gcopystack status.\n-\tcopystack(gp, uintptr(newsize), true)\n-\tif stackDebug >= 1 {\n-\t\tprint(\"stack grow done\\n\")\n-\t}\n-\tcasgstatus(gp, _Gcopystack, _Grunning)\n-\tgogo(&gp.sched)\n-}\n-\n-//go:nosplit\n-func nilfunc() {\n-\t*(*uint8)(nil) = 0\n-}\n-\n-// adjust Gobuf as if it executed a call to fn\n-// and then did an immediate gosave.\n-func gostartcallfn(gobuf *gobuf, fv *funcval) {\n-\tvar fn unsafe.Pointer\n-\tif fv != nil {\n-\t\tfn = unsafe.Pointer(fv.fn)\n-\t} else {\n-\t\tfn = unsafe.Pointer(funcPC(nilfunc))\n-\t}\n-\tgostartcall(gobuf, fn, unsafe.Pointer(fv))\n-}\n-\n-// Maybe shrink the stack being used by gp.\n-// Called at garbage collection time.\n-// gp must be stopped, but the world need not be.\n-func shrinkstack(gp *g) {\n-\tgstatus := readgstatus(gp)\n-\tif gstatus&^_Gscan == _Gdead {\n-\t\tif gp.stack.lo != 0 {\n-\t\t\t// Free whole stack - it will get reallocated\n-\t\t\t// if G is used again.\n-\t\t\tstackfree(gp.stack, gp.stackAlloc)\n-\t\t\tgp.stack.lo = 0\n-\t\t\tgp.stack.hi = 0\n-\t\t\tgp.stkbar = nil\n-\t\t\tgp.stkbarPos = 0\n-\t\t}\n-\t\treturn\n-\t}\n-\tif gp.stack.lo == 0 {\n-\t\tthrow(\"missing stack in shrinkstack\")\n-\t}\n-\tif gstatus&_Gscan == 0 {\n-\t\tthrow(\"bad status in shrinkstack\")\n-\t}\n-\n-\tif debug.gcshrinkstackoff > 0 {\n-\t\treturn\n-\t}\n-\tif gp.startpc == gcBgMarkWorkerPC {\n-\t\t// We're not allowed to shrink the gcBgMarkWorker\n-\t\t// stack (see gcBgMarkWorker for explanation).\n-\t\treturn\n-\t}\n-\n-\toldsize := gp.stackAlloc\n-\tnewsize := oldsize / 2\n-\t// Don't shrink the allocation below the minimum-sized stack\n-\t// allocation.\n-\tif newsize < _FixedStack {\n-\t\treturn\n-\t}\n-\t// Compute how much of the stack is currently in use and only\n-\t// shrink the stack if gp is using less than a quarter of its\n-\t// current stack. The currently used stack includes everything\n-\t// down to the SP plus the stack guard space that ensures\n-\t// there's room for nosplit functions.\n-\tavail := gp.stack.hi - gp.stack.lo\n-\tif used := gp.stack.hi - gp.sched.sp + _StackLimit; used >= avail/4 {\n-\t\treturn\n-\t}\n-\n-\t// We can't copy the stack if we're in a syscall.\n-\t// The syscall might have pointers into the stack.\n-\tif gp.syscallsp != 0 {\n-\t\treturn\n-\t}\n-\tif sys.GoosWindows != 0 && gp.m != nil && gp.m.libcallsp != 0 {\n-\t\treturn\n-\t}\n-\n-\tif stackDebug > 0 {\n-\t\tprint(\"shrinking stack \", oldsize, \"->\", newsize, \"\\n\")\n-\t}\n-\n-\tcopystack(gp, newsize, false)\n-}\n-\n-// freeStackSpans frees unused stack spans at the end of GC.\n-func freeStackSpans() {\n-\tlock(&stackpoolmu)\n-\n-\t// Scan stack pools for empty stack spans.\n-\tfor order := range stackpool {\n-\t\tlist := &stackpool[order]\n-\t\tfor s := list.first; s != nil; {\n-\t\t\tnext := s.next\n-\t\t\tif s.allocCount == 0 {\n-\t\t\t\tlist.remove(s)\n-\t\t\t\ts.stackfreelist = 0\n-\t\t\t\tmheap_.freeStack(s)\n-\t\t\t}\n-\t\t\ts = next\n-\t\t}\n-\t}\n-\n-\tunlock(&stackpoolmu)\n-\n-\t// Free large stack spans.\n-\tlock(&stackLarge.lock)\n-\tfor i := range stackLarge.free {\n-\t\tfor s := stackLarge.free[i].first; s != nil; {\n-\t\t\tnext := s.next\n-\t\t\tstackLarge.free[i].remove(s)\n-\t\t\tmheap_.freeStack(s)\n-\t\t\ts = next\n-\t\t}\n-\t}\n-\tunlock(&stackLarge.lock)\n-}\n-\n-//go:nosplit\n-func morestackc() {\n-\tsystemstack(func() {\n-\t\tthrow(\"attempt to execute C code on Go stack\")\n-\t})\n-}"}]}