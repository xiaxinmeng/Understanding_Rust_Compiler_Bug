{"sha": "89fa689a9e898ccb81b966477b3ac4e254461b05", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ODlmYTY4OWE5ZTg5OGNjYjgxYjk2NjQ3N2IzYWM0ZTI1NDQ2MWIwNQ==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2018-07-31T14:26:02Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2018-07-31T14:26:02Z"}, "message": "[38/46] Use dr_vec_info to represent a data reference\n\nThis patch makes various routines (mostly in tree-vect-data-refs.c)\ntake dr_vec_infos rather than data_references.  The affected routines\nare really dealing with the way that an access is going to vectorised,\nrather than with the original scalar access described by the\ndata_reference.\n\n2018-07-31  Richard Sandiford  <richard.sandiford@arm.com>\n\ngcc/\n\t* tree-vectorizer.h (set_dr_misalignment, dr_misalignment)\n\t(DR_TARGET_ALIGNMENT, aligned_access_p, known_alignment_for_access_p)\n\t(vect_known_alignment_in_bytes, vect_dr_behavior)\n\t(vect_get_scalar_dr_size): Take references as dr_vec_infos\n\tinstead of data_references.  Update calls to other routines for\n\twhich the same change has been made.\n\t* tree-vect-data-refs.c (vect_preserves_scalar_order_p): Take\n\tdr_vec_infos instead of stmt_vec_infos.\n\t(vect_analyze_data_ref_dependence): Update call accordingly.\n\t(vect_slp_analyze_data_ref_dependence)\n\t(vect_record_base_alignments): Use DR_VECT_AUX.\n\t(vect_calculate_target_alignment, vect_compute_data_ref_alignment)\n\t(vect_update_misalignment_for_peel, verify_data_ref_alignment)\n\t(vector_alignment_reachable_p, vect_get_data_access_cost)\n\t(vect_peeling_supportable, vect_analyze_group_access_1)\n\t(vect_analyze_group_access, vect_analyze_data_ref_access)\n\t(vect_vfa_segment_size, vect_vfa_access_size, vect_vfa_align)\n\t(vect_compile_time_alias, vect_small_gap_p)\n\t(vectorizable_with_step_bound_p, vect_duplicate_ssa_name_ptr_info):\n\t(vect_supportable_dr_alignment): Take references as dr_vec_infos\n\tinstead of data_references.  Update calls to other routines for\n\twhich the same change has been made.\n\t(vect_verify_datarefs_alignment, vect_get_peeling_costs_all_drs)\n\t(vect_find_same_alignment_drs, vect_analyze_data_refs_alignment)\n\t(vect_slp_analyze_and_verify_node_alignment)\n\t(vect_analyze_data_ref_accesses, vect_prune_runtime_alias_test_list)\n\t(vect_create_addr_base_for_vector_ref, vect_create_data_ref_ptr)\n\t(vect_setup_realignment): Use dr_vec_infos.  Update calls after\n\tabove changes.\n\t(_vect_peel_info::dr): Replace with...\n\t(_vect_peel_info::dr_info): ...this new field.\n\t(vect_peeling_hash_get_most_frequent)\n\t(vect_peeling_hash_choose_best_peeling): Update accordingly.\n\t(vect_peeling_hash_get_lowest_cost):\n\t(vect_enhance_data_refs_alignment): Likewise.  Update calls to other\n\troutines for which the same change has been made.\n\t(vect_peeling_hash_insert): Likewise.  Take a dr_vec_info instead of a\n\tdata_reference.\n\t* tree-vect-loop-manip.c (get_misalign_in_elems)\n\t(vect_gen_prolog_loop_niters): Use dr_vec_infos.  Update calls after\n\tabove changes.\n\t* tree-vect-loop.c (vect_analyze_loop_2): Likewise.\n\t* tree-vect-stmts.c (vect_get_store_cost, vect_get_load_cost)\n\t(vect_truncate_gather_scatter_offset, compare_step_with_zero)\n\t(get_group_load_store_type, get_negative_load_store_type)\n\t(vect_get_data_ptr_increment, vectorizable_store)\n\t(vectorizable_load): Likewise.\n\t(ensure_base_align): Take a dr_vec_info instead of a data_reference.\n\tUpdate calls to other routines for which the same change has been made.\n\nFrom-SVN: r263153", "tree": {"sha": "72e941d118a57af1100d8c21ab1ed81d2fc9d581", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/72e941d118a57af1100d8c21ab1ed81d2fc9d581"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/89fa689a9e898ccb81b966477b3ac4e254461b05", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/89fa689a9e898ccb81b966477b3ac4e254461b05", "html_url": "https://github.com/Rust-GCC/gccrs/commit/89fa689a9e898ccb81b966477b3ac4e254461b05", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/89fa689a9e898ccb81b966477b3ac4e254461b05/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "f44fb7aa84e0c1f9c0721a69f7b0f157d6058686", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f44fb7aa84e0c1f9c0721a69f7b0f157d6058686", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f44fb7aa84e0c1f9c0721a69f7b0f157d6058686"}], "stats": {"total": 957, "additions": 533, "deletions": 424}, "files": [{"sha": "298e2e9cf29171c6d1e8b7d2d7a4318bbe914b4a", "filename": "gcc/ChangeLog", "status": "modified", "additions": 52, "deletions": 0, "changes": 52, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/89fa689a9e898ccb81b966477b3ac4e254461b05/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/89fa689a9e898ccb81b966477b3ac4e254461b05/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=89fa689a9e898ccb81b966477b3ac4e254461b05", "patch": "@@ -1,3 +1,55 @@\n+2018-07-31  Richard Sandiford  <richard.sandiford@arm.com>\n+\n+\t* tree-vectorizer.h (set_dr_misalignment, dr_misalignment)\n+\t(DR_TARGET_ALIGNMENT, aligned_access_p, known_alignment_for_access_p)\n+\t(vect_known_alignment_in_bytes, vect_dr_behavior)\n+\t(vect_get_scalar_dr_size): Take references as dr_vec_infos\n+\tinstead of data_references.  Update calls to other routines for\n+\twhich the same change has been made.\n+\t* tree-vect-data-refs.c (vect_preserves_scalar_order_p): Take\n+\tdr_vec_infos instead of stmt_vec_infos.\n+\t(vect_analyze_data_ref_dependence): Update call accordingly.\n+\t(vect_slp_analyze_data_ref_dependence)\n+\t(vect_record_base_alignments): Use DR_VECT_AUX.\n+\t(vect_calculate_target_alignment, vect_compute_data_ref_alignment)\n+\t(vect_update_misalignment_for_peel, verify_data_ref_alignment)\n+\t(vector_alignment_reachable_p, vect_get_data_access_cost)\n+\t(vect_peeling_supportable, vect_analyze_group_access_1)\n+\t(vect_analyze_group_access, vect_analyze_data_ref_access)\n+\t(vect_vfa_segment_size, vect_vfa_access_size, vect_vfa_align)\n+\t(vect_compile_time_alias, vect_small_gap_p)\n+\t(vectorizable_with_step_bound_p, vect_duplicate_ssa_name_ptr_info):\n+\t(vect_supportable_dr_alignment): Take references as dr_vec_infos\n+\tinstead of data_references.  Update calls to other routines for\n+\twhich the same change has been made.\n+\t(vect_verify_datarefs_alignment, vect_get_peeling_costs_all_drs)\n+\t(vect_find_same_alignment_drs, vect_analyze_data_refs_alignment)\n+\t(vect_slp_analyze_and_verify_node_alignment)\n+\t(vect_analyze_data_ref_accesses, vect_prune_runtime_alias_test_list)\n+\t(vect_create_addr_base_for_vector_ref, vect_create_data_ref_ptr)\n+\t(vect_setup_realignment): Use dr_vec_infos.  Update calls after\n+\tabove changes.\n+\t(_vect_peel_info::dr): Replace with...\n+\t(_vect_peel_info::dr_info): ...this new field.\n+\t(vect_peeling_hash_get_most_frequent)\n+\t(vect_peeling_hash_choose_best_peeling): Update accordingly.\n+\t(vect_peeling_hash_get_lowest_cost):\n+\t(vect_enhance_data_refs_alignment): Likewise.  Update calls to other\n+\troutines for which the same change has been made.\n+\t(vect_peeling_hash_insert): Likewise.  Take a dr_vec_info instead of a\n+\tdata_reference.\n+\t* tree-vect-loop-manip.c (get_misalign_in_elems)\n+\t(vect_gen_prolog_loop_niters): Use dr_vec_infos.  Update calls after\n+\tabove changes.\n+\t* tree-vect-loop.c (vect_analyze_loop_2): Likewise.\n+\t* tree-vect-stmts.c (vect_get_store_cost, vect_get_load_cost)\n+\t(vect_truncate_gather_scatter_offset, compare_step_with_zero)\n+\t(get_group_load_store_type, get_negative_load_store_type)\n+\t(vect_get_data_ptr_increment, vectorizable_store)\n+\t(vectorizable_load): Likewise.\n+\t(ensure_base_align): Take a dr_vec_info instead of a data_reference.\n+\tUpdate calls to other routines for which the same change has been made.\n+\n 2018-07-31  Richard Sandiford  <richard.sandiford@arm.com>\n \n \t* tree-vectorizer.h (vec_info::move_dr): New member function."}, {"sha": "193575dff54e86e661017f6b3d4725465b9cafaf", "filename": "gcc/tree-vect-data-refs.c", "status": "modified", "additions": 338, "deletions": 295, "changes": 633, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/89fa689a9e898ccb81b966477b3ac4e254461b05/gcc%2Ftree-vect-data-refs.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/89fa689a9e898ccb81b966477b3ac4e254461b05/gcc%2Ftree-vect-data-refs.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-data-refs.c?ref=89fa689a9e898ccb81b966477b3ac4e254461b05", "patch": "@@ -192,14 +192,16 @@ vect_check_nonzero_value (loop_vec_info loop_vinfo, tree value)\n   LOOP_VINFO_CHECK_NONZERO (loop_vinfo).safe_push (value);\n }\n \n-/* Return true if we know that the order of vectorized STMTINFO_A and\n-   vectorized STMTINFO_B will be the same as the order of STMTINFO_A and\n-   STMTINFO_B.  At least one of the statements is a write.  */\n+/* Return true if we know that the order of vectorized DR_INFO_A and\n+   vectorized DR_INFO_B will be the same as the order of DR_INFO_A and\n+   DR_INFO_B.  At least one of the accesses is a write.  */\n \n static bool\n-vect_preserves_scalar_order_p (stmt_vec_info stmtinfo_a,\n-\t\t\t       stmt_vec_info stmtinfo_b)\n+vect_preserves_scalar_order_p (dr_vec_info *dr_info_a, dr_vec_info *dr_info_b)\n {\n+  stmt_vec_info stmtinfo_a = dr_info_a->stmt;\n+  stmt_vec_info stmtinfo_b = dr_info_b->stmt;\n+\n   /* Single statements are always kept in their original order.  */\n   if (!STMT_VINFO_GROUPED_ACCESS (stmtinfo_a)\n       && !STMT_VINFO_GROUPED_ACCESS (stmtinfo_b))\n@@ -294,8 +296,10 @@ vect_analyze_data_ref_dependence (struct data_dependence_relation *ddr,\n   struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n   struct data_reference *dra = DDR_A (ddr);\n   struct data_reference *drb = DDR_B (ddr);\n-  stmt_vec_info stmtinfo_a = vect_dr_stmt (dra);\n-  stmt_vec_info stmtinfo_b = vect_dr_stmt (drb);\n+  dr_vec_info *dr_info_a = DR_VECT_AUX (dra);\n+  dr_vec_info *dr_info_b = DR_VECT_AUX (drb);\n+  stmt_vec_info stmtinfo_a = dr_info_a->stmt;\n+  stmt_vec_info stmtinfo_b = dr_info_b->stmt;\n   lambda_vector dist_v;\n   unsigned int loop_depth;\n \n@@ -471,7 +475,7 @@ vect_analyze_data_ref_dependence (struct data_dependence_relation *ddr,\n \t\t... = a[i];\n \t\ta[i+1] = ...;\n \t     where loads from the group interleave with the store.  */\n-\t  if (!vect_preserves_scalar_order_p (stmtinfo_a, stmtinfo_b))\n+\t  if (!vect_preserves_scalar_order_p (dr_info_a, dr_info_b))\n \t    {\n \t      if (dump_enabled_p ())\n \t\tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n@@ -609,6 +613,8 @@ vect_slp_analyze_data_ref_dependence (struct data_dependence_relation *ddr)\n {\n   struct data_reference *dra = DDR_A (ddr);\n   struct data_reference *drb = DDR_B (ddr);\n+  dr_vec_info *dr_info_a = DR_VECT_AUX (dra);\n+  dr_vec_info *dr_info_b = DR_VECT_AUX (drb);\n \n   /* We need to check dependences of statements marked as unvectorizable\n      as well, they still can prohibit vectorization.  */\n@@ -626,9 +632,9 @@ vect_slp_analyze_data_ref_dependence (struct data_dependence_relation *ddr)\n \n   /* If dra and drb are part of the same interleaving chain consider\n      them independent.  */\n-  if (STMT_VINFO_GROUPED_ACCESS (vect_dr_stmt (dra))\n-      && (DR_GROUP_FIRST_ELEMENT (vect_dr_stmt (dra))\n-\t  == DR_GROUP_FIRST_ELEMENT (vect_dr_stmt (drb))))\n+  if (STMT_VINFO_GROUPED_ACCESS (dr_info_a->stmt)\n+      && (DR_GROUP_FIRST_ELEMENT (dr_info_a->stmt)\n+\t  == DR_GROUP_FIRST_ELEMENT (dr_info_b->stmt)))\n     return false;\n \n   /* Unknown data dependence.  */\n@@ -842,7 +848,8 @@ vect_record_base_alignments (vec_info *vinfo)\n   unsigned int i;\n   FOR_EACH_VEC_ELT (vinfo->shared->datarefs, i, dr)\n     {\n-      stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+      dr_vec_info *dr_info = DR_VECT_AUX (dr);\n+      stmt_vec_info stmt_info = dr_info->stmt;\n       if (!DR_IS_CONDITIONAL_IN_STMT (dr)\n \t  && STMT_VINFO_VECTORIZABLE (stmt_info)\n \t  && !STMT_VINFO_GATHER_SCATTER_P (stmt_info))\n@@ -858,34 +865,33 @@ vect_record_base_alignments (vec_info *vinfo)\n     }\n }\n \n-/* Return the target alignment for the vectorized form of DR.  */\n+/* Return the target alignment for the vectorized form of DR_INFO.  */\n \n static unsigned int\n-vect_calculate_target_alignment (struct data_reference *dr)\n+vect_calculate_target_alignment (dr_vec_info *dr_info)\n {\n-  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n-  tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n+  tree vectype = STMT_VINFO_VECTYPE (dr_info->stmt);\n   return targetm.vectorize.preferred_vector_alignment (vectype);\n }\n \n /* Function vect_compute_data_ref_alignment\n \n-   Compute the misalignment of the data reference DR.\n+   Compute the misalignment of the data reference DR_INFO.\n \n    Output:\n-   1. DR_MISALIGNMENT (DR) is defined.\n+   1. DR_MISALIGNMENT (DR_INFO) is defined.\n \n    FOR NOW: No analysis is actually performed. Misalignment is calculated\n    only for trivial cases. TODO.  */\n \n static void\n-vect_compute_data_ref_alignment (struct data_reference *dr)\n+vect_compute_data_ref_alignment (dr_vec_info *dr_info)\n {\n-  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+  stmt_vec_info stmt_info = dr_info->stmt;\n   vec_base_alignments *base_alignments = &stmt_info->vinfo->base_alignments;\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   struct loop *loop = NULL;\n-  tree ref = DR_REF (dr);\n+  tree ref = DR_REF (dr_info->dr);\n   tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n \n   if (dump_enabled_p ())\n@@ -896,17 +902,17 @@ vect_compute_data_ref_alignment (struct data_reference *dr)\n     loop = LOOP_VINFO_LOOP (loop_vinfo);\n \n   /* Initialize misalignment to unknown.  */\n-  SET_DR_MISALIGNMENT (dr, DR_MISALIGNMENT_UNKNOWN);\n+  SET_DR_MISALIGNMENT (dr_info, DR_MISALIGNMENT_UNKNOWN);\n \n   if (STMT_VINFO_GATHER_SCATTER_P (stmt_info))\n     return;\n \n-  innermost_loop_behavior *drb = vect_dr_behavior (dr);\n+  innermost_loop_behavior *drb = vect_dr_behavior (dr_info);\n   bool step_preserves_misalignment_p;\n \n   unsigned HOST_WIDE_INT vector_alignment\n-    = vect_calculate_target_alignment (dr) / BITS_PER_UNIT;\n-  DR_TARGET_ALIGNMENT (dr) = vector_alignment;\n+    = vect_calculate_target_alignment (dr_info) / BITS_PER_UNIT;\n+  DR_TARGET_ALIGNMENT (dr_info) = vector_alignment;\n \n   /* No step for BB vectorization.  */\n   if (!loop)\n@@ -924,7 +930,7 @@ vect_compute_data_ref_alignment (struct data_reference *dr)\n   else if (nested_in_vect_loop_p (loop, stmt_info))\n     {\n       step_preserves_misalignment_p\n-\t= (DR_STEP_ALIGNMENT (dr) % vector_alignment) == 0;\n+\t= (DR_STEP_ALIGNMENT (dr_info->dr) % vector_alignment) == 0;\n \n       if (dump_enabled_p ())\n \t{\n@@ -946,7 +952,7 @@ vect_compute_data_ref_alignment (struct data_reference *dr)\n     {\n       poly_uint64 vf = LOOP_VINFO_VECT_FACTOR (loop_vinfo);\n       step_preserves_misalignment_p\n-\t= multiple_p (DR_STEP_ALIGNMENT (dr) * vf, vector_alignment);\n+\t= multiple_p (DR_STEP_ALIGNMENT (dr_info->dr) * vf, vector_alignment);\n \n       if (!step_preserves_misalignment_p && dump_enabled_p ())\n \tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n@@ -1009,8 +1015,8 @@ vect_compute_data_ref_alignment (struct data_reference *dr)\n           dump_printf (MSG_NOTE, \"\\n\");\n         }\n \n-      DR_VECT_AUX (dr)->base_decl = base;\n-      DR_VECT_AUX (dr)->base_misaligned = true;\n+      dr_info->base_decl = base;\n+      dr_info->base_misaligned = true;\n       base_misalignment = 0;\n     }\n   poly_int64 misalignment\n@@ -1038,12 +1044,13 @@ vect_compute_data_ref_alignment (struct data_reference *dr)\n       return;\n     }\n \n-  SET_DR_MISALIGNMENT (dr, const_misalignment);\n+  SET_DR_MISALIGNMENT (dr_info, const_misalignment);\n \n   if (dump_enabled_p ())\n     {\n       dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-                       \"misalign = %d bytes of ref \", DR_MISALIGNMENT (dr));\n+\t\t       \"misalign = %d bytes of ref \",\n+\t\t       DR_MISALIGNMENT (dr_info));\n       dump_generic_expr (MSG_MISSED_OPTIMIZATION, TDF_SLIM, ref);\n       dump_printf (MSG_MISSED_OPTIMIZATION, \"\\n\");\n     }\n@@ -1052,28 +1059,28 @@ vect_compute_data_ref_alignment (struct data_reference *dr)\n }\n \n /* Function vect_update_misalignment_for_peel.\n-   Sets DR's misalignment\n-   - to 0 if it has the same alignment as DR_PEEL,\n-   - to the misalignment computed using NPEEL if DR's salignment is known,\n+   Sets DR_INFO's misalignment\n+   - to 0 if it has the same alignment as DR_PEEL_INFO,\n+   - to the misalignment computed using NPEEL if DR_INFO's salignment is known,\n    - to -1 (unknown) otherwise.\n \n-   DR - the data reference whose misalignment is to be adjusted.\n-   DR_PEEL - the data reference whose misalignment is being made\n-             zero in the vector loop by the peel.\n+   DR_INFO - the data reference whose misalignment is to be adjusted.\n+   DR_PEEL_INFO - the data reference whose misalignment is being made\n+\t\t  zero in the vector loop by the peel.\n    NPEEL - the number of iterations in the peel loop if the misalignment\n-           of DR_PEEL is known at compile time.  */\n+           of DR_PEEL_INFO is known at compile time.  */\n \n static void\n-vect_update_misalignment_for_peel (struct data_reference *dr,\n-                                   struct data_reference *dr_peel, int npeel)\n+vect_update_misalignment_for_peel (dr_vec_info *dr_info,\n+\t\t\t\t   dr_vec_info *dr_peel_info, int npeel)\n {\n   unsigned int i;\n   vec<dr_p> same_aligned_drs;\n   struct data_reference *current_dr;\n-  int dr_size = vect_get_scalar_dr_size (dr);\n-  int dr_peel_size = vect_get_scalar_dr_size (dr_peel);\n-  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n-  stmt_vec_info peel_stmt_info = vect_dr_stmt (dr_peel);\n+  int dr_size = vect_get_scalar_dr_size (dr_info);\n+  int dr_peel_size = vect_get_scalar_dr_size (dr_peel_info);\n+  stmt_vec_info stmt_info = dr_info->stmt;\n+  stmt_vec_info peel_stmt_info = dr_peel_info->stmt;\n \n  /* For interleaved data accesses the step in the loop must be multiplied by\n      the size of the interleaving group.  */\n@@ -1084,51 +1091,52 @@ vect_update_misalignment_for_peel (struct data_reference *dr,\n \n   /* It can be assumed that the data refs with the same alignment as dr_peel\n      are aligned in the vector loop.  */\n-  same_aligned_drs = STMT_VINFO_SAME_ALIGN_REFS (vect_dr_stmt (dr_peel));\n+  same_aligned_drs = STMT_VINFO_SAME_ALIGN_REFS (peel_stmt_info);\n   FOR_EACH_VEC_ELT (same_aligned_drs, i, current_dr)\n     {\n-      if (current_dr != dr)\n+      if (current_dr != dr_info->dr)\n         continue;\n-      gcc_assert (!known_alignment_for_access_p (dr)\n-\t\t  || !known_alignment_for_access_p (dr_peel)\n-\t\t  || (DR_MISALIGNMENT (dr) / dr_size\n-\t\t      == DR_MISALIGNMENT (dr_peel) / dr_peel_size));\n-      SET_DR_MISALIGNMENT (dr, 0);\n+      gcc_assert (!known_alignment_for_access_p (dr_info)\n+\t\t  || !known_alignment_for_access_p (dr_peel_info)\n+\t\t  || (DR_MISALIGNMENT (dr_info) / dr_size\n+\t\t      == DR_MISALIGNMENT (dr_peel_info) / dr_peel_size));\n+      SET_DR_MISALIGNMENT (dr_info, 0);\n       return;\n     }\n \n-  if (known_alignment_for_access_p (dr)\n-      && known_alignment_for_access_p (dr_peel))\n+  if (known_alignment_for_access_p (dr_info)\n+      && known_alignment_for_access_p (dr_peel_info))\n     {\n-      bool negative = tree_int_cst_compare (DR_STEP (dr), size_zero_node) < 0;\n-      int misal = DR_MISALIGNMENT (dr);\n+      bool negative = tree_int_cst_compare (DR_STEP (dr_info->dr),\n+\t\t\t\t\t    size_zero_node) < 0;\n+      int misal = DR_MISALIGNMENT (dr_info);\n       misal += negative ? -npeel * dr_size : npeel * dr_size;\n-      misal &= DR_TARGET_ALIGNMENT (dr) - 1;\n-      SET_DR_MISALIGNMENT (dr, misal);\n+      misal &= DR_TARGET_ALIGNMENT (dr_info) - 1;\n+      SET_DR_MISALIGNMENT (dr_info, misal);\n       return;\n     }\n \n   if (dump_enabled_p ())\n     dump_printf_loc (MSG_NOTE, vect_location, \"Setting misalignment \" \\\n \t\t     \"to unknown (-1).\\n\");\n-  SET_DR_MISALIGNMENT (dr, DR_MISALIGNMENT_UNKNOWN);\n+  SET_DR_MISALIGNMENT (dr_info, DR_MISALIGNMENT_UNKNOWN);\n }\n \n \n /* Function verify_data_ref_alignment\n \n-   Return TRUE if DR can be handled with respect to alignment.  */\n+   Return TRUE if DR_INFO can be handled with respect to alignment.  */\n \n static bool\n-verify_data_ref_alignment (data_reference_p dr)\n+verify_data_ref_alignment (dr_vec_info *dr_info)\n {\n   enum dr_alignment_support supportable_dr_alignment\n-    = vect_supportable_dr_alignment (dr, false);\n+    = vect_supportable_dr_alignment (dr_info, false);\n   if (!supportable_dr_alignment)\n     {\n       if (dump_enabled_p ())\n \t{\n-\t  if (DR_IS_READ (dr))\n+\t  if (DR_IS_READ (dr_info->dr))\n \t    dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n \t\t\t     \"not vectorized: unsupported unaligned load.\");\n \t  else\n@@ -1137,7 +1145,7 @@ verify_data_ref_alignment (data_reference_p dr)\n \t\t\t     \"store.\");\n \n \t  dump_generic_expr (MSG_MISSED_OPTIMIZATION, TDF_SLIM,\n-\t\t\t     DR_REF (dr));\n+\t\t\t     DR_REF (dr_info->dr));\n \t  dump_printf (MSG_MISSED_OPTIMIZATION, \"\\n\");\n \t}\n       return false;\n@@ -1164,7 +1172,8 @@ vect_verify_datarefs_alignment (loop_vec_info vinfo)\n \n   FOR_EACH_VEC_ELT (datarefs, i, dr)\n     {\n-      stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+      dr_vec_info *dr_info = DR_VECT_AUX (dr);\n+      stmt_vec_info stmt_info = dr_info->stmt;\n \n       if (!STMT_VINFO_RELEVANT_P (stmt_info))\n \tcontinue;\n@@ -1180,7 +1189,7 @@ vect_verify_datarefs_alignment (loop_vec_info vinfo)\n \t  && !STMT_VINFO_GROUPED_ACCESS (stmt_info))\n \tcontinue;\n \n-      if (! verify_data_ref_alignment (dr))\n+      if (! verify_data_ref_alignment (dr_info))\n \treturn false;\n     }\n \n@@ -1202,13 +1211,13 @@ not_size_aligned (tree exp)\n \n /* Function vector_alignment_reachable_p\n \n-   Return true if vector alignment for DR is reachable by peeling\n+   Return true if vector alignment for DR_INFO is reachable by peeling\n    a few loop iterations.  Return false otherwise.  */\n \n static bool\n-vector_alignment_reachable_p (struct data_reference *dr)\n+vector_alignment_reachable_p (dr_vec_info *dr_info)\n {\n-  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+  stmt_vec_info stmt_info = dr_info->stmt;\n   tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n \n   if (STMT_VINFO_GROUPED_ACCESS (stmt_info))\n@@ -1219,21 +1228,21 @@ vector_alignment_reachable_p (struct data_reference *dr)\n       int elem_size, mis_in_elements;\n \n       /* FORNOW: handle only known alignment.  */\n-      if (!known_alignment_for_access_p (dr))\n+      if (!known_alignment_for_access_p (dr_info))\n \treturn false;\n \n       poly_uint64 nelements = TYPE_VECTOR_SUBPARTS (vectype);\n       poly_uint64 vector_size = GET_MODE_SIZE (TYPE_MODE (vectype));\n       elem_size = vector_element_size (vector_size, nelements);\n-      mis_in_elements = DR_MISALIGNMENT (dr) / elem_size;\n+      mis_in_elements = DR_MISALIGNMENT (dr_info) / elem_size;\n \n       if (!multiple_p (nelements - mis_in_elements, DR_GROUP_SIZE (stmt_info)))\n \treturn false;\n     }\n \n   /* If misalignment is known at the compile time then allow peeling\n      only if natural alignment is reachable through peeling.  */\n-  if (known_alignment_for_access_p (dr) && !aligned_access_p (dr))\n+  if (known_alignment_for_access_p (dr_info) && !aligned_access_p (dr_info))\n     {\n       HOST_WIDE_INT elmsize =\n \t\tint_cst_value (TYPE_SIZE_UNIT (TREE_TYPE (vectype)));\n@@ -1242,9 +1251,9 @@ vector_alignment_reachable_p (struct data_reference *dr)\n \t  dump_printf_loc (MSG_NOTE, vect_location,\n \t                   \"data size =\" HOST_WIDE_INT_PRINT_DEC, elmsize);\n \t  dump_printf (MSG_NOTE,\n-\t               \". misalignment = %d.\\n\", DR_MISALIGNMENT (dr));\n+\t               \". misalignment = %d.\\n\", DR_MISALIGNMENT (dr_info));\n \t}\n-      if (DR_MISALIGNMENT (dr) % elmsize)\n+      if (DR_MISALIGNMENT (dr_info) % elmsize)\n \t{\n \t  if (dump_enabled_p ())\n \t    dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n@@ -1253,10 +1262,10 @@ vector_alignment_reachable_p (struct data_reference *dr)\n \t}\n     }\n \n-  if (!known_alignment_for_access_p (dr))\n+  if (!known_alignment_for_access_p (dr_info))\n     {\n-      tree type = TREE_TYPE (DR_REF (dr));\n-      bool is_packed = not_size_aligned (DR_REF (dr));\n+      tree type = TREE_TYPE (DR_REF (dr_info->dr));\n+      bool is_packed = not_size_aligned (DR_REF (dr_info->dr));\n       if (dump_enabled_p ())\n \tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n \t                 \"Unknown misalignment, %snaturally aligned\\n\",\n@@ -1268,16 +1277,16 @@ vector_alignment_reachable_p (struct data_reference *dr)\n }\n \n \n-/* Calculate the cost of the memory access represented by DR.  */\n+/* Calculate the cost of the memory access represented by DR_INFO.  */\n \n static void\n-vect_get_data_access_cost (struct data_reference *dr,\n+vect_get_data_access_cost (dr_vec_info *dr_info,\n                            unsigned int *inside_cost,\n                            unsigned int *outside_cost,\n \t\t\t   stmt_vector_for_cost *body_cost_vec,\n \t\t\t   stmt_vector_for_cost *prologue_cost_vec)\n {\n-  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+  stmt_vec_info stmt_info = dr_info->stmt;\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   int ncopies;\n \n@@ -1286,7 +1295,7 @@ vect_get_data_access_cost (struct data_reference *dr,\n   else\n     ncopies = vect_get_num_copies (loop_vinfo, STMT_VINFO_VECTYPE (stmt_info));\n \n-  if (DR_IS_READ (dr))\n+  if (DR_IS_READ (dr_info->dr))\n     vect_get_load_cost (stmt_info, ncopies, true, inside_cost, outside_cost,\n \t\t\tprologue_cost_vec, body_cost_vec, false);\n   else\n@@ -1301,7 +1310,7 @@ vect_get_data_access_cost (struct data_reference *dr,\n \n typedef struct _vect_peel_info\n {\n-  struct data_reference *dr;\n+  dr_vec_info *dr_info;\n   int npeel;\n   unsigned int count;\n } *vect_peel_info;\n@@ -1335,16 +1344,17 @@ peel_info_hasher::equal (const _vect_peel_info *a, const _vect_peel_info *b)\n }\n \n \n-/* Insert DR into peeling hash table with NPEEL as key.  */\n+/* Insert DR_INFO into peeling hash table with NPEEL as key.  */\n \n static void\n vect_peeling_hash_insert (hash_table<peel_info_hasher> *peeling_htab,\n-\t\t\t  loop_vec_info loop_vinfo, struct data_reference *dr,\n+\t\t\t  loop_vec_info loop_vinfo, dr_vec_info *dr_info,\n                           int npeel)\n {\n   struct _vect_peel_info elem, *slot;\n   _vect_peel_info **new_slot;\n-  bool supportable_dr_alignment = vect_supportable_dr_alignment (dr, true);\n+  bool supportable_dr_alignment\n+    = vect_supportable_dr_alignment (dr_info, true);\n \n   elem.npeel = npeel;\n   slot = peeling_htab->find (&elem);\n@@ -1354,7 +1364,7 @@ vect_peeling_hash_insert (hash_table<peel_info_hasher> *peeling_htab,\n     {\n       slot = XNEW (struct _vect_peel_info);\n       slot->npeel = npeel;\n-      slot->dr = dr;\n+      slot->dr_info = dr_info;\n       slot->count = 1;\n       new_slot = peeling_htab->find_slot (slot, INSERT);\n       *new_slot = slot;\n@@ -1381,19 +1391,19 @@ vect_peeling_hash_get_most_frequent (_vect_peel_info **slot,\n     {\n       max->peel_info.npeel = elem->npeel;\n       max->peel_info.count = elem->count;\n-      max->peel_info.dr = elem->dr;\n+      max->peel_info.dr_info = elem->dr_info;\n     }\n \n   return 1;\n }\n \n /* Get the costs of peeling NPEEL iterations checking data access costs\n-   for all data refs.  If UNKNOWN_MISALIGNMENT is true, we assume DR0's\n+   for all data refs.  If UNKNOWN_MISALIGNMENT is true, we assume DR0_INFO's\n    misalignment will be zero after peeling.  */\n \n static void\n vect_get_peeling_costs_all_drs (vec<data_reference_p> datarefs,\n-\t\t\t\tstruct data_reference *dr0,\n+\t\t\t\tdr_vec_info *dr0_info,\n \t\t\t\tunsigned int *inside_cost,\n \t\t\t\tunsigned int *outside_cost,\n \t\t\t\tstmt_vector_for_cost *body_cost_vec,\n@@ -1406,7 +1416,8 @@ vect_get_peeling_costs_all_drs (vec<data_reference_p> datarefs,\n \n   FOR_EACH_VEC_ELT (datarefs, i, dr)\n     {\n-      stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+      dr_vec_info *dr_info = DR_VECT_AUX (dr);\n+      stmt_vec_info stmt_info = dr_info->stmt;\n       if (!STMT_VINFO_RELEVANT_P (stmt_info))\n \tcontinue;\n \n@@ -1423,16 +1434,16 @@ vect_get_peeling_costs_all_drs (vec<data_reference_p> datarefs,\n \tcontinue;\n \n       int save_misalignment;\n-      save_misalignment = DR_MISALIGNMENT (dr);\n+      save_misalignment = DR_MISALIGNMENT (dr_info);\n       if (npeel == 0)\n \t;\n-      else if (unknown_misalignment && dr == dr0)\n-\tSET_DR_MISALIGNMENT (dr, 0);\n+      else if (unknown_misalignment && dr_info == dr0_info)\n+\tSET_DR_MISALIGNMENT (dr_info, 0);\n       else\n-\tvect_update_misalignment_for_peel (dr, dr0, npeel);\n-      vect_get_data_access_cost (dr, inside_cost, outside_cost,\n+\tvect_update_misalignment_for_peel (dr_info, dr0_info, npeel);\n+      vect_get_data_access_cost (dr_info, inside_cost, outside_cost,\n \t\t\t\t body_cost_vec, prologue_cost_vec);\n-      SET_DR_MISALIGNMENT (dr, save_misalignment);\n+      SET_DR_MISALIGNMENT (dr_info, save_misalignment);\n     }\n }\n \n@@ -1446,7 +1457,7 @@ vect_peeling_hash_get_lowest_cost (_vect_peel_info **slot,\n   vect_peel_info elem = *slot;\n   int dummy;\n   unsigned int inside_cost = 0, outside_cost = 0;\n-  stmt_vec_info stmt_info = vect_dr_stmt (elem->dr);\n+  stmt_vec_info stmt_info = elem->dr_info->stmt;\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   stmt_vector_for_cost prologue_cost_vec, body_cost_vec,\n \t\t       epilogue_cost_vec;\n@@ -1456,7 +1467,7 @@ vect_peeling_hash_get_lowest_cost (_vect_peel_info **slot,\n   epilogue_cost_vec.create (2);\n \n   vect_get_peeling_costs_all_drs (LOOP_VINFO_DATAREFS (loop_vinfo),\n-\t\t\t\t  elem->dr, &inside_cost, &outside_cost,\n+\t\t\t\t  elem->dr_info, &inside_cost, &outside_cost,\n \t\t\t\t  &body_cost_vec, &prologue_cost_vec,\n \t\t\t\t  elem->npeel, false);\n \n@@ -1480,7 +1491,7 @@ vect_peeling_hash_get_lowest_cost (_vect_peel_info **slot,\n     {\n       min->inside_cost = inside_cost;\n       min->outside_cost = outside_cost;\n-      min->peel_info.dr = elem->dr;\n+      min->peel_info.dr_info = elem->dr_info;\n       min->peel_info.npeel = elem->npeel;\n       min->peel_info.count = elem->count;\n     }\n@@ -1499,7 +1510,7 @@ vect_peeling_hash_choose_best_peeling (hash_table<peel_info_hasher> *peeling_hta\n {\n    struct _vect_peel_extended_info res;\n \n-   res.peel_info.dr = NULL;\n+   res.peel_info.dr_info = NULL;\n \n    if (!unlimited_cost_model (LOOP_VINFO_LOOP (loop_vinfo)))\n      {\n@@ -1523,7 +1534,7 @@ vect_peeling_hash_choose_best_peeling (hash_table<peel_info_hasher> *peeling_hta\n /* Return true if the new peeling NPEEL is supported.  */\n \n static bool\n-vect_peeling_supportable (loop_vec_info loop_vinfo, struct data_reference *dr0,\n+vect_peeling_supportable (loop_vec_info loop_vinfo, dr_vec_info *dr0_info,\n \t\t\t  unsigned npeel)\n {\n   unsigned i;\n@@ -1536,10 +1547,11 @@ vect_peeling_supportable (loop_vec_info loop_vinfo, struct data_reference *dr0,\n     {\n       int save_misalignment;\n \n-      if (dr == dr0)\n+      if (dr == dr0_info->dr)\n \tcontinue;\n \n-      stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+      dr_vec_info *dr_info = DR_VECT_AUX (dr);\n+      stmt_vec_info stmt_info = dr_info->stmt;\n       /* For interleaving, only the alignment of the first access\n \t matters.  */\n       if (STMT_VINFO_GROUPED_ACCESS (stmt_info)\n@@ -1552,10 +1564,11 @@ vect_peeling_supportable (loop_vec_info loop_vinfo, struct data_reference *dr0,\n \t  && !STMT_VINFO_GROUPED_ACCESS (stmt_info))\n \tcontinue;\n \n-      save_misalignment = DR_MISALIGNMENT (dr);\n-      vect_update_misalignment_for_peel (dr, dr0, npeel);\n-      supportable_dr_alignment = vect_supportable_dr_alignment (dr, false);\n-      SET_DR_MISALIGNMENT (dr, save_misalignment);\n+      save_misalignment = DR_MISALIGNMENT (dr_info);\n+      vect_update_misalignment_for_peel (dr_info, dr0_info, npeel);\n+      supportable_dr_alignment\n+\t= vect_supportable_dr_alignment (dr_info, false);\n+      SET_DR_MISALIGNMENT (dr_info, save_misalignment);\n \n       if (!supportable_dr_alignment)\n \treturn false;\n@@ -1661,7 +1674,8 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n   vec<data_reference_p> datarefs = LOOP_VINFO_DATAREFS (loop_vinfo);\n   struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n   enum dr_alignment_support supportable_dr_alignment;\n-  struct data_reference *dr0 = NULL, *first_store = NULL;\n+  dr_vec_info *first_store = NULL;\n+  dr_vec_info *dr0_info = NULL;\n   struct data_reference *dr;\n   unsigned int i, j;\n   bool do_peeling = false;\n@@ -1671,7 +1685,7 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n   bool one_misalignment_known = false;\n   bool one_misalignment_unknown = false;\n   bool one_dr_unsupportable = false;\n-  struct data_reference *unsupportable_dr = NULL;\n+  dr_vec_info *unsupportable_dr_info = NULL;\n   poly_uint64 vf = LOOP_VINFO_VECT_FACTOR (loop_vinfo);\n   unsigned possible_npeel_number = 1;\n   tree vectype;\n@@ -1718,7 +1732,8 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \n   FOR_EACH_VEC_ELT (datarefs, i, dr)\n     {\n-      stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+      dr_vec_info *dr_info = DR_VECT_AUX (dr);\n+      stmt_vec_info stmt_info = dr_info->stmt;\n \n       if (!STMT_VINFO_RELEVANT_P (stmt_info))\n \tcontinue;\n@@ -1741,21 +1756,23 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \t  && !STMT_VINFO_GROUPED_ACCESS (stmt_info))\n \tcontinue;\n \n-      supportable_dr_alignment = vect_supportable_dr_alignment (dr, true);\n-      do_peeling = vector_alignment_reachable_p (dr);\n+      supportable_dr_alignment = vect_supportable_dr_alignment (dr_info, true);\n+      do_peeling = vector_alignment_reachable_p (dr_info);\n       if (do_peeling)\n         {\n-          if (known_alignment_for_access_p (dr))\n+          if (known_alignment_for_access_p (dr_info))\n             {\n \t      unsigned int npeel_tmp = 0;\n \t      bool negative = tree_int_cst_compare (DR_STEP (dr),\n \t\t\t\t\t\t    size_zero_node) < 0;\n \n \t      vectype = STMT_VINFO_VECTYPE (stmt_info);\n-\t      unsigned int target_align = DR_TARGET_ALIGNMENT (dr);\n-\t      unsigned int dr_size = vect_get_scalar_dr_size (dr);\n-\t      mis = (negative ? DR_MISALIGNMENT (dr) : -DR_MISALIGNMENT (dr));\n-\t      if (DR_MISALIGNMENT (dr) != 0)\n+\t      unsigned int target_align = DR_TARGET_ALIGNMENT (dr_info);\n+\t      unsigned int dr_size = vect_get_scalar_dr_size (dr_info);\n+\t      mis = (negative\n+\t\t     ? DR_MISALIGNMENT (dr_info)\n+\t\t     : -DR_MISALIGNMENT (dr_info));\n+\t      if (DR_MISALIGNMENT (dr_info) != 0)\n \t\tnpeel_tmp = (mis & (target_align - 1)) / dr_size;\n \n               /* For multiple types, it is possible that the bigger type access\n@@ -1780,7 +1797,7 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \n \t\t  /* NPEEL_TMP is 0 when there is no misalignment, but also\n \t\t     allow peeling NELEMENTS.  */\n-\t\t  if (DR_MISALIGNMENT (dr) == 0)\n+\t\t  if (DR_MISALIGNMENT (dr_info) == 0)\n \t\t    possible_npeel_number++;\n \t\t}\n \n@@ -1789,7 +1806,7 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n               for (j = 0; j < possible_npeel_number; j++)\n                 {\n                   vect_peeling_hash_insert (&peeling_htab, loop_vinfo,\n-\t\t\t\t\t    dr, npeel_tmp);\n+\t\t\t\t\t    dr_info, npeel_tmp);\n \t\t  npeel_tmp += target_align / dr_size;\n                 }\n \n@@ -1803,11 +1820,11 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n                  stores over load.  */\n \t      unsigned same_align_drs\n \t\t= STMT_VINFO_SAME_ALIGN_REFS (stmt_info).length ();\n-\t      if (!dr0\n+\t      if (!dr0_info\n \t\t  || same_align_drs_max < same_align_drs)\n \t\t{\n \t\t  same_align_drs_max = same_align_drs;\n-\t\t  dr0 = dr;\n+\t\t  dr0_info = dr_info;\n \t\t}\n \t      /* For data-refs with the same number of related\n \t\t accesses prefer the one where the misalign\n@@ -1816,13 +1833,13 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \t\t{\n \t\t  struct loop *ivloop0, *ivloop;\n \t\t  ivloop0 = outermost_invariant_loop_for_expr\n-\t\t    (loop, DR_BASE_ADDRESS (dr0));\n+\t\t    (loop, DR_BASE_ADDRESS (dr0_info->dr));\n \t\t  ivloop = outermost_invariant_loop_for_expr\n \t\t    (loop, DR_BASE_ADDRESS (dr));\n \t\t  if ((ivloop && !ivloop0)\n \t\t      || (ivloop && ivloop0\n \t\t\t  && flow_loop_nested_p (ivloop, ivloop0)))\n-\t\t    dr0 = dr;\n+\t\t    dr0_info = dr_info;\n \t\t}\n \n \t      one_misalignment_unknown = true;\n@@ -1832,16 +1849,16 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \t      if (!supportable_dr_alignment)\n \t      {\n \t\tone_dr_unsupportable = true;\n-\t\tunsupportable_dr = dr;\n+\t\tunsupportable_dr_info = dr_info;\n \t      }\n \n \t      if (!first_store && DR_IS_WRITE (dr))\n-\t\tfirst_store = dr;\n+\t\tfirst_store = dr_info;\n             }\n         }\n       else\n         {\n-          if (!aligned_access_p (dr))\n+          if (!aligned_access_p (dr_info))\n             {\n               if (dump_enabled_p ())\n                 dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n@@ -1879,7 +1896,7 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \n       stmt_vector_for_cost dummy;\n       dummy.create (2);\n-      vect_get_peeling_costs_all_drs (datarefs, dr0,\n+      vect_get_peeling_costs_all_drs (datarefs, dr0_info,\n \t\t\t\t      &load_inside_cost,\n \t\t\t\t      &load_outside_cost,\n \t\t\t\t      &dummy, &dummy, estimated_npeels, true);\n@@ -1905,7 +1922,7 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \t  || (load_inside_cost == store_inside_cost\n \t      && load_outside_cost > store_outside_cost))\n \t{\n-\t  dr0 = first_store;\n+\t  dr0_info = first_store;\n \t  peel_for_unknown_alignment.inside_cost = store_inside_cost;\n \t  peel_for_unknown_alignment.outside_cost = store_outside_cost;\n \t}\n@@ -1929,18 +1946,18 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n       epilogue_cost_vec.release ();\n \n       peel_for_unknown_alignment.peel_info.count = 1\n-\t+ STMT_VINFO_SAME_ALIGN_REFS (vect_dr_stmt (dr0)).length ();\n+\t+ STMT_VINFO_SAME_ALIGN_REFS (dr0_info->stmt).length ();\n     }\n \n   peel_for_unknown_alignment.peel_info.npeel = 0;\n-  peel_for_unknown_alignment.peel_info.dr = dr0;\n+  peel_for_unknown_alignment.peel_info.dr_info = dr0_info;\n \n   best_peel = peel_for_unknown_alignment;\n \n   peel_for_known_alignment.inside_cost = INT_MAX;\n   peel_for_known_alignment.outside_cost = INT_MAX;\n   peel_for_known_alignment.peel_info.count = 0;\n-  peel_for_known_alignment.peel_info.dr = NULL;\n+  peel_for_known_alignment.peel_info.dr_info = NULL;\n \n   if (do_peeling && one_misalignment_known)\n     {\n@@ -1952,7 +1969,7 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n     }\n \n   /* Compare costs of peeling for known and unknown alignment. */\n-  if (peel_for_known_alignment.peel_info.dr != NULL\n+  if (peel_for_known_alignment.peel_info.dr_info != NULL\n       && peel_for_unknown_alignment.inside_cost\n       >= peel_for_known_alignment.inside_cost)\n     {\n@@ -1969,7 +1986,7 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n      since we'd have to discard a chosen peeling except when it accidentally\n      aligned the unsupportable data ref.  */\n   if (one_dr_unsupportable)\n-    dr0 = unsupportable_dr;\n+    dr0_info = unsupportable_dr_info;\n   else if (do_peeling)\n     {\n       /* Calculate the penalty for no peeling, i.e. leaving everything as-is.\n@@ -2000,7 +2017,7 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n       epilogue_cost_vec.release ();\n \n       npeel = best_peel.peel_info.npeel;\n-      dr0 = best_peel.peel_info.dr;\n+      dr0_info = best_peel.peel_info.dr_info;\n \n       /* If no peeling is not more expensive than the best peeling we\n \t have so far, don't perform any peeling.  */\n@@ -2010,12 +2027,12 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \n   if (do_peeling)\n     {\n-      stmt_vec_info stmt_info = vect_dr_stmt (dr0);\n+      stmt_vec_info stmt_info = dr0_info->stmt;\n       vectype = STMT_VINFO_VECTYPE (stmt_info);\n \n-      if (known_alignment_for_access_p (dr0))\n+      if (known_alignment_for_access_p (dr0_info))\n         {\n-\t  bool negative = tree_int_cst_compare (DR_STEP (dr0),\n+\t  bool negative = tree_int_cst_compare (DR_STEP (dr0_info->dr),\n \t\t\t\t\t\tsize_zero_node) < 0;\n           if (!npeel)\n             {\n@@ -2024,16 +2041,17 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n                  updating DR_MISALIGNMENT values.  The peeling factor is the\n                  vectorization factor minus the misalignment as an element\n                  count.  */\n-\t      mis = negative ? DR_MISALIGNMENT (dr0) : -DR_MISALIGNMENT (dr0);\n-\t      unsigned int target_align = DR_TARGET_ALIGNMENT (dr0);\n+\t      mis = (negative\n+\t\t     ? DR_MISALIGNMENT (dr0_info)\n+\t\t     : -DR_MISALIGNMENT (dr0_info));\n+\t      unsigned int target_align = DR_TARGET_ALIGNMENT (dr0_info);\n \t      npeel = ((mis & (target_align - 1))\n-\t\t       / vect_get_scalar_dr_size (dr0));\n+\t\t       / vect_get_scalar_dr_size (dr0_info));\n             }\n \n \t  /* For interleaved data access every iteration accesses all the\n \t     members of the group, therefore we divide the number of iterations\n \t     by the group size.  */\n-\t  stmt_info = vect_dr_stmt (dr0);\n \t  if (STMT_VINFO_GROUPED_ACCESS (stmt_info))\n \t    npeel /= DR_GROUP_SIZE (stmt_info);\n \n@@ -2043,11 +2061,11 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n         }\n \n       /* Ensure that all datarefs can be vectorized after the peel.  */\n-      if (!vect_peeling_supportable (loop_vinfo, dr0, npeel))\n+      if (!vect_peeling_supportable (loop_vinfo, dr0_info, npeel))\n \tdo_peeling = false;\n \n       /* Check if all datarefs are supportable and log.  */\n-      if (do_peeling && known_alignment_for_access_p (dr0) && npeel == 0)\n+      if (do_peeling && known_alignment_for_access_p (dr0_info) && npeel == 0)\n         {\n           stat = vect_verify_datarefs_alignment (loop_vinfo);\n           if (!stat)\n@@ -2066,8 +2084,9 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n               unsigned max_peel = npeel;\n               if (max_peel == 0)\n                 {\n-\t\t  unsigned int target_align = DR_TARGET_ALIGNMENT (dr0);\n-\t\t  max_peel = target_align / vect_get_scalar_dr_size (dr0) - 1;\n+\t\t  unsigned int target_align = DR_TARGET_ALIGNMENT (dr0_info);\n+\t\t  max_peel = (target_align\n+\t\t\t      / vect_get_scalar_dr_size (dr0_info) - 1);\n                 }\n               if (max_peel > max_allowed_peel)\n                 {\n@@ -2103,25 +2122,26 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n              vectorization factor times the size).  Otherwise, the\n              misalignment of DR_i must be set to unknown.  */\n \t  FOR_EACH_VEC_ELT (datarefs, i, dr)\n-\t    if (dr != dr0)\n+\t    if (dr != dr0_info->dr)\n \t      {\n \t\t/* Strided accesses perform only component accesses, alignment\n \t\t   is irrelevant for them.  */\n-\t\tstmt_info = vect_dr_stmt (dr);\n+\t\tdr_vec_info *dr_info = DR_VECT_AUX (dr);\n+\t\tstmt_info = dr_info->stmt;\n \t\tif (STMT_VINFO_STRIDED_P (stmt_info)\n \t\t    && !STMT_VINFO_GROUPED_ACCESS (stmt_info))\n \t\t  continue;\n \n-\t\tvect_update_misalignment_for_peel (dr, dr0, npeel);\n+\t\tvect_update_misalignment_for_peel (dr_info, dr0_info, npeel);\n \t      }\n \n-          LOOP_VINFO_UNALIGNED_DR (loop_vinfo) = dr0;\n+          LOOP_VINFO_UNALIGNED_DR (loop_vinfo) = dr0_info->dr;\n           if (npeel)\n             LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo) = npeel;\n           else\n             LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo)\n-\t      = DR_MISALIGNMENT (dr0);\n-\t  SET_DR_MISALIGNMENT (dr0, 0);\n+\t      = DR_MISALIGNMENT (dr0_info);\n+\t  SET_DR_MISALIGNMENT (dr0_info, 0);\n \t  if (dump_enabled_p ())\n             {\n               dump_printf_loc (MSG_NOTE, vect_location,\n@@ -2156,11 +2176,12 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n     {\n       FOR_EACH_VEC_ELT (datarefs, i, dr)\n         {\n-\t  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+\t  dr_vec_info *dr_info = DR_VECT_AUX (dr);\n+\t  stmt_vec_info stmt_info = dr_info->stmt;\n \n \t  /* For interleaving, only the alignment of the first access\n \t     matters.  */\n-\t  if (aligned_access_p (dr)\n+\t  if (aligned_access_p (dr_info)\n \t      || (STMT_VINFO_GROUPED_ACCESS (stmt_info)\n \t\t  && DR_GROUP_FIRST_ELEMENT (stmt_info) != stmt_info))\n \t    continue;\n@@ -2175,22 +2196,22 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \t      break;\n \t    }\n \n-\t  supportable_dr_alignment = vect_supportable_dr_alignment (dr, false);\n+\t  supportable_dr_alignment\n+\t    = vect_supportable_dr_alignment (dr_info, false);\n \n           if (!supportable_dr_alignment)\n             {\n               int mask;\n               tree vectype;\n \n-              if (known_alignment_for_access_p (dr)\n+              if (known_alignment_for_access_p (dr_info)\n                   || LOOP_VINFO_MAY_MISALIGN_STMTS (loop_vinfo).length ()\n                      >= (unsigned) PARAM_VALUE (PARAM_VECT_MAX_VERSION_FOR_ALIGNMENT_CHECKS))\n                 {\n                   do_versioning = false;\n                   break;\n                 }\n \n-\t      stmt_info = vect_dr_stmt (dr);\n \t      vectype = STMT_VINFO_VECTYPE (stmt_info);\n \t      gcc_assert (vectype);\n \n@@ -2241,8 +2262,8 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n          of the loop being vectorized.  */\n       FOR_EACH_VEC_ELT (may_misalign_stmts, i, stmt_info)\n         {\n-          dr = STMT_VINFO_DATA_REF (stmt_info);\n-\t  SET_DR_MISALIGNMENT (dr, 0);\n+\t  dr_vec_info *dr_info = STMT_VINFO_DR_INFO (stmt_info);\n+\t  SET_DR_MISALIGNMENT (dr_info, 0);\n \t  if (dump_enabled_p ())\n             dump_printf_loc (MSG_NOTE, vect_location,\n                              \"Alignment of access forced using versioning.\\n\");\n@@ -2278,8 +2299,10 @@ vect_find_same_alignment_drs (struct data_dependence_relation *ddr)\n {\n   struct data_reference *dra = DDR_A (ddr);\n   struct data_reference *drb = DDR_B (ddr);\n-  stmt_vec_info stmtinfo_a = vect_dr_stmt (dra);\n-  stmt_vec_info stmtinfo_b = vect_dr_stmt (drb);\n+  dr_vec_info *dr_info_a = DR_VECT_AUX (dra);\n+  dr_vec_info *dr_info_b = DR_VECT_AUX (drb);\n+  stmt_vec_info stmtinfo_a = dr_info_a->stmt;\n+  stmt_vec_info stmtinfo_b = dr_info_b->stmt;\n \n   if (DDR_ARE_DEPENDENT (ddr) == chrec_known)\n     return;\n@@ -2302,9 +2325,9 @@ vect_find_same_alignment_drs (struct data_dependence_relation *ddr)\n   if (maybe_ne (diff, 0))\n     {\n       /* Get the wider of the two alignments.  */\n-      unsigned int align_a = (vect_calculate_target_alignment (dra)\n+      unsigned int align_a = (vect_calculate_target_alignment (dr_info_a)\n \t\t\t      / BITS_PER_UNIT);\n-      unsigned int align_b = (vect_calculate_target_alignment (drb)\n+      unsigned int align_b = (vect_calculate_target_alignment (dr_info_b)\n \t\t\t      / BITS_PER_UNIT);\n       unsigned int max_align = MAX (align_a, align_b);\n \n@@ -2352,9 +2375,9 @@ vect_analyze_data_refs_alignment (loop_vec_info vinfo)\n   vect_record_base_alignments (vinfo);\n   FOR_EACH_VEC_ELT (datarefs, i, dr)\n     {\n-      stmt_vec_info stmt_info = vect_dr_stmt (dr);\n-      if (STMT_VINFO_VECTORIZABLE (stmt_info))\n-\tvect_compute_data_ref_alignment (dr);\n+      dr_vec_info *dr_info = DR_VECT_AUX (dr);\n+      if (STMT_VINFO_VECTORIZABLE (dr_info->stmt))\n+\tvect_compute_data_ref_alignment (dr_info);\n     }\n \n   return true;\n@@ -2370,17 +2393,17 @@ vect_slp_analyze_and_verify_node_alignment (slp_tree node)\n      the node is permuted in which case we start from the first\n      element in the group.  */\n   stmt_vec_info first_stmt_info = SLP_TREE_SCALAR_STMTS (node)[0];\n-  data_reference_p first_dr = STMT_VINFO_DATA_REF (first_stmt_info);\n+  dr_vec_info *first_dr_info = STMT_VINFO_DR_INFO (first_stmt_info);\n   if (SLP_TREE_LOAD_PERMUTATION (node).exists ())\n     first_stmt_info = DR_GROUP_FIRST_ELEMENT (first_stmt_info);\n \n-  data_reference_p dr = STMT_VINFO_DATA_REF (first_stmt_info);\n-  vect_compute_data_ref_alignment (dr);\n+  dr_vec_info *dr_info = STMT_VINFO_DR_INFO (first_stmt_info);\n+  vect_compute_data_ref_alignment (dr_info);\n   /* For creating the data-ref pointer we need alignment of the\n      first element anyway.  */\n-  if (dr != first_dr)\n-    vect_compute_data_ref_alignment (first_dr);\n-  if (! verify_data_ref_alignment (dr))\n+  if (dr_info != first_dr_info)\n+    vect_compute_data_ref_alignment (first_dr_info);\n+  if (! verify_data_ref_alignment (dr_info))\n     {\n       if (dump_enabled_p ())\n \tdump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n@@ -2418,19 +2441,20 @@ vect_slp_analyze_and_verify_instance_alignment (slp_instance instance)\n }\n \n \n-/* Analyze groups of accesses: check that DR belongs to a group of\n+/* Analyze groups of accesses: check that DR_INFO belongs to a group of\n    accesses of legal size, step, etc.  Detect gaps, single element\n    interleaving, and other special cases. Set grouped access info.\n    Collect groups of strided stores for further use in SLP analysis.\n    Worker for vect_analyze_group_access.  */\n \n static bool\n-vect_analyze_group_access_1 (struct data_reference *dr)\n+vect_analyze_group_access_1 (dr_vec_info *dr_info)\n {\n+  data_reference *dr = dr_info->dr;\n   tree step = DR_STEP (dr);\n   tree scalar_type = TREE_TYPE (DR_REF (dr));\n   HOST_WIDE_INT type_size = TREE_INT_CST_LOW (TYPE_SIZE_UNIT (scalar_type));\n-  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+  stmt_vec_info stmt_info = dr_info->stmt;\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   bb_vec_info bb_vinfo = STMT_VINFO_BB_VINFO (stmt_info);\n   HOST_WIDE_INT dr_step = -1;\n@@ -2507,7 +2531,7 @@ vect_analyze_group_access_1 (struct data_reference *dr)\n       if (bb_vinfo)\n \t{\n \t  /* Mark the statement as unvectorizable.  */\n-\t  STMT_VINFO_VECTORIZABLE (vect_dr_stmt (dr)) = false;\n+\t  STMT_VINFO_VECTORIZABLE (stmt_info) = false;\n \t  return true;\n \t}\n \n@@ -2655,18 +2679,18 @@ vect_analyze_group_access_1 (struct data_reference *dr)\n   return true;\n }\n \n-/* Analyze groups of accesses: check that DR belongs to a group of\n+/* Analyze groups of accesses: check that DR_INFO belongs to a group of\n    accesses of legal size, step, etc.  Detect gaps, single element\n    interleaving, and other special cases. Set grouped access info.\n    Collect groups of strided stores for further use in SLP analysis.  */\n \n static bool\n-vect_analyze_group_access (struct data_reference *dr)\n+vect_analyze_group_access (dr_vec_info *dr_info)\n {\n-  if (!vect_analyze_group_access_1 (dr))\n+  if (!vect_analyze_group_access_1 (dr_info))\n     {\n       /* Dissolve the group if present.  */\n-      stmt_vec_info stmt_info = DR_GROUP_FIRST_ELEMENT (vect_dr_stmt (dr));\n+      stmt_vec_info stmt_info = DR_GROUP_FIRST_ELEMENT (dr_info->stmt);\n       while (stmt_info)\n \t{\n \t  stmt_vec_info next = DR_GROUP_NEXT_ELEMENT (stmt_info);\n@@ -2679,16 +2703,17 @@ vect_analyze_group_access (struct data_reference *dr)\n   return true;\n }\n \n-/* Analyze the access pattern of the data-reference DR.\n+/* Analyze the access pattern of the data-reference DR_INFO.\n    In case of non-consecutive accesses call vect_analyze_group_access() to\n    analyze groups of accesses.  */\n \n static bool\n-vect_analyze_data_ref_access (struct data_reference *dr)\n+vect_analyze_data_ref_access (dr_vec_info *dr_info)\n {\n+  data_reference *dr = dr_info->dr;\n   tree step = DR_STEP (dr);\n   tree scalar_type = TREE_TYPE (DR_REF (dr));\n-  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+  stmt_vec_info stmt_info = dr_info->stmt;\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   struct loop *loop = NULL;\n \n@@ -2768,10 +2793,10 @@ vect_analyze_data_ref_access (struct data_reference *dr)\n   if (TREE_CODE (step) != INTEGER_CST)\n     return (STMT_VINFO_STRIDED_P (stmt_info)\n \t    && (!STMT_VINFO_GROUPED_ACCESS (stmt_info)\n-\t\t|| vect_analyze_group_access (dr)));\n+\t\t|| vect_analyze_group_access (dr_info)));\n \n   /* Not consecutive access - check if it's a part of interleaving group.  */\n-  return vect_analyze_group_access (dr);\n+  return vect_analyze_group_access (dr_info);\n }\n \n /* Compare two data-references DRA and DRB to group them into chunks\n@@ -2916,7 +2941,8 @@ vect_analyze_data_ref_accesses (vec_info *vinfo)\n   for (i = 0; i < datarefs_copy.length () - 1;)\n     {\n       data_reference_p dra = datarefs_copy[i];\n-      stmt_vec_info stmtinfo_a = vect_dr_stmt (dra);\n+      dr_vec_info *dr_info_a = DR_VECT_AUX (dra);\n+      stmt_vec_info stmtinfo_a = dr_info_a->stmt;\n       stmt_vec_info lastinfo = NULL;\n       if (!STMT_VINFO_VECTORIZABLE (stmtinfo_a)\n \t  || STMT_VINFO_GATHER_SCATTER_P (stmtinfo_a))\n@@ -2927,7 +2953,8 @@ vect_analyze_data_ref_accesses (vec_info *vinfo)\n       for (i = i + 1; i < datarefs_copy.length (); ++i)\n \t{\n \t  data_reference_p drb = datarefs_copy[i];\n-\t  stmt_vec_info stmtinfo_b = vect_dr_stmt (drb);\n+\t  dr_vec_info *dr_info_b = DR_VECT_AUX (drb);\n+\t  stmt_vec_info stmtinfo_b = dr_info_b->stmt;\n \t  if (!STMT_VINFO_VECTORIZABLE (stmtinfo_b)\n \t      || STMT_VINFO_GATHER_SCATTER_P (stmtinfo_b))\n \t    break;\n@@ -3050,25 +3077,28 @@ vect_analyze_data_ref_accesses (vec_info *vinfo)\n     }\n \n   FOR_EACH_VEC_ELT (datarefs_copy, i, dr)\n-    if (STMT_VINFO_VECTORIZABLE (vect_dr_stmt (dr))\n-        && !vect_analyze_data_ref_access (dr))\n-      {\n-\tif (dump_enabled_p ())\n-\t  dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-\t                   \"not vectorized: complicated access pattern.\\n\");\n+    {\n+      dr_vec_info *dr_info = DR_VECT_AUX (dr);\n+      if (STMT_VINFO_VECTORIZABLE (dr_info->stmt)\n+\t  && !vect_analyze_data_ref_access (dr_info))\n+\t{\n+\t  if (dump_enabled_p ())\n+\t    dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n+\t\t\t     \"not vectorized: complicated access pattern.\\n\");\n \n-        if (is_a <bb_vec_info> (vinfo))\n-\t  {\n-\t    /* Mark the statement as not vectorizable.  */\n-\t    STMT_VINFO_VECTORIZABLE (vect_dr_stmt (dr)) = false;\n-\t    continue;\n-\t  }\n-        else\n-\t  {\n-\t    datarefs_copy.release ();\n-\t    return false;\n-\t  }\n-      }\n+\t  if (is_a <bb_vec_info> (vinfo))\n+\t    {\n+\t      /* Mark the statement as not vectorizable.  */\n+\t      STMT_VINFO_VECTORIZABLE (dr_info->stmt) = false;\n+\t      continue;\n+\t    }\n+\t  else\n+\t    {\n+\t      datarefs_copy.release ();\n+\t      return false;\n+\t    }\n+\t}\n+    }\n \n   datarefs_copy.release ();\n   return true;\n@@ -3077,7 +3107,7 @@ vect_analyze_data_ref_accesses (vec_info *vinfo)\n /* Function vect_vfa_segment_size.\n \n    Input:\n-     DR: The data reference.\n+     DR_INFO: The data reference.\n      LENGTH_FACTOR: segment length to consider.\n \n    Return a value suitable for the dr_with_seg_len::seg_len field.\n@@ -3086,32 +3116,32 @@ vect_analyze_data_ref_accesses (vec_info *vinfo)\n    the size of the access; in effect it only describes the first byte.  */\n \n static tree\n-vect_vfa_segment_size (struct data_reference *dr, tree length_factor)\n+vect_vfa_segment_size (dr_vec_info *dr_info, tree length_factor)\n {\n   length_factor = size_binop (MINUS_EXPR,\n \t\t\t      fold_convert (sizetype, length_factor),\n \t\t\t      size_one_node);\n-  return size_binop (MULT_EXPR, fold_convert (sizetype, DR_STEP (dr)),\n+  return size_binop (MULT_EXPR, fold_convert (sizetype, DR_STEP (dr_info->dr)),\n \t\t     length_factor);\n }\n \n-/* Return a value that, when added to abs (vect_vfa_segment_size (dr)),\n+/* Return a value that, when added to abs (vect_vfa_segment_size (DR_INFO)),\n    gives the worst-case number of bytes covered by the segment.  */\n \n static unsigned HOST_WIDE_INT\n-vect_vfa_access_size (data_reference *dr)\n+vect_vfa_access_size (dr_vec_info *dr_info)\n {\n-  stmt_vec_info stmt_vinfo = vect_dr_stmt (dr);\n-  tree ref_type = TREE_TYPE (DR_REF (dr));\n+  stmt_vec_info stmt_vinfo = dr_info->stmt;\n+  tree ref_type = TREE_TYPE (DR_REF (dr_info->dr));\n   unsigned HOST_WIDE_INT ref_size = tree_to_uhwi (TYPE_SIZE_UNIT (ref_type));\n   unsigned HOST_WIDE_INT access_size = ref_size;\n   if (DR_GROUP_FIRST_ELEMENT (stmt_vinfo))\n     {\n-      gcc_assert (DR_GROUP_FIRST_ELEMENT (stmt_vinfo) == vect_dr_stmt (dr));\n+      gcc_assert (DR_GROUP_FIRST_ELEMENT (stmt_vinfo) == stmt_vinfo);\n       access_size *= DR_GROUP_SIZE (stmt_vinfo) - DR_GROUP_GAP (stmt_vinfo);\n     }\n   if (STMT_VINFO_VEC_STMT (stmt_vinfo)\n-      && (vect_supportable_dr_alignment (dr, false)\n+      && (vect_supportable_dr_alignment (dr_info, false)\n \t  == dr_explicit_realign_optimized))\n     {\n       /* We might access a full vector's worth.  */\n@@ -3121,12 +3151,13 @@ vect_vfa_access_size (data_reference *dr)\n   return access_size;\n }\n \n-/* Get the minimum alignment for all the scalar accesses that DR describes.  */\n+/* Get the minimum alignment for all the scalar accesses that DR_INFO\n+   describes.  */\n \n static unsigned int\n-vect_vfa_align (const data_reference *dr)\n+vect_vfa_align (dr_vec_info *dr_info)\n {\n-  return TYPE_ALIGN_UNIT (TREE_TYPE (DR_REF (dr)));\n+  return TYPE_ALIGN_UNIT (TREE_TYPE (DR_REF (dr_info->dr)));\n }\n \n /* Function vect_no_alias_p.\n@@ -3139,27 +3170,27 @@ vect_vfa_align (const data_reference *dr)\n    of dr_with_seg_len::{seg_len,access_size} for A and B.  */\n \n static int\n-vect_compile_time_alias (struct data_reference *a, struct data_reference *b,\n+vect_compile_time_alias (dr_vec_info *a, dr_vec_info *b,\n \t\t\t tree segment_length_a, tree segment_length_b,\n \t\t\t unsigned HOST_WIDE_INT access_size_a,\n \t\t\t unsigned HOST_WIDE_INT access_size_b)\n {\n-  poly_offset_int offset_a = wi::to_poly_offset (DR_INIT (a));\n-  poly_offset_int offset_b = wi::to_poly_offset (DR_INIT (b));\n+  poly_offset_int offset_a = wi::to_poly_offset (DR_INIT (a->dr));\n+  poly_offset_int offset_b = wi::to_poly_offset (DR_INIT (b->dr));\n   poly_uint64 const_length_a;\n   poly_uint64 const_length_b;\n \n   /* For negative step, we need to adjust address range by TYPE_SIZE_UNIT\n      bytes, e.g., int a[3] -> a[1] range is [a+4, a+16) instead of\n      [a, a+12) */\n-  if (tree_int_cst_compare (DR_STEP (a), size_zero_node) < 0)\n+  if (tree_int_cst_compare (DR_STEP (a->dr), size_zero_node) < 0)\n     {\n       const_length_a = (-wi::to_poly_wide (segment_length_a)).force_uhwi ();\n       offset_a = (offset_a + access_size_a) - const_length_a;\n     }\n   else\n     const_length_a = tree_to_poly_uint64 (segment_length_a);\n-  if (tree_int_cst_compare (DR_STEP (b), size_zero_node) < 0)\n+  if (tree_int_cst_compare (DR_STEP (b->dr), size_zero_node) < 0)\n     {\n       const_length_b = (-wi::to_poly_wide (segment_length_b)).force_uhwi ();\n       offset_b = (offset_b + access_size_b) - const_length_b;\n@@ -3269,30 +3300,34 @@ vect_check_lower_bound (loop_vec_info loop_vinfo, tree expr, bool unsigned_p,\n   LOOP_VINFO_LOWER_BOUNDS (loop_vinfo).safe_push (lower_bound);\n }\n \n-/* Return true if it's unlikely that the step of the vectorized form of DR\n+/* Return true if it's unlikely that the step of the vectorized form of DR_INFO\n    will span fewer than GAP bytes.  */\n \n static bool\n-vect_small_gap_p (loop_vec_info loop_vinfo, data_reference *dr, poly_int64 gap)\n+vect_small_gap_p (loop_vec_info loop_vinfo, dr_vec_info *dr_info,\n+\t\t  poly_int64 gap)\n {\n-  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+  stmt_vec_info stmt_info = dr_info->stmt;\n   HOST_WIDE_INT count\n     = estimated_poly_value (LOOP_VINFO_VECT_FACTOR (loop_vinfo));\n   if (DR_GROUP_FIRST_ELEMENT (stmt_info))\n     count *= DR_GROUP_SIZE (DR_GROUP_FIRST_ELEMENT (stmt_info));\n-  return estimated_poly_value (gap) <= count * vect_get_scalar_dr_size (dr);\n+  return (estimated_poly_value (gap)\n+\t  <= count * vect_get_scalar_dr_size (dr_info));\n }\n \n-/* Return true if we know that there is no alias between DR_A and DR_B\n-   when abs (DR_STEP (DR_A)) >= N for some N.  When returning true, set\n-   *LOWER_BOUND_OUT to this N.  */\n+/* Return true if we know that there is no alias between DR_INFO_A and\n+   DR_INFO_B when abs (DR_STEP (DR_INFO_A->dr)) >= N for some N.\n+   When returning true, set *LOWER_BOUND_OUT to this N.  */\n \n static bool\n-vectorizable_with_step_bound_p (data_reference *dr_a, data_reference *dr_b,\n+vectorizable_with_step_bound_p (dr_vec_info *dr_info_a, dr_vec_info *dr_info_b,\n \t\t\t\tpoly_uint64 *lower_bound_out)\n {\n   /* Check that there is a constant gap of known sign between DR_A\n      and DR_B.  */\n+  data_reference *dr_a = dr_info_a->dr;\n+  data_reference *dr_b = dr_info_b->dr;\n   poly_int64 init_a, init_b;\n   if (!operand_equal_p (DR_BASE_ADDRESS (dr_a), DR_BASE_ADDRESS (dr_b), 0)\n       || !operand_equal_p (DR_OFFSET (dr_a), DR_OFFSET (dr_b), 0)\n@@ -3306,19 +3341,19 @@ vectorizable_with_step_bound_p (data_reference *dr_a, data_reference *dr_b,\n   if (maybe_lt (init_b, init_a))\n     {\n       std::swap (init_a, init_b);\n+      std::swap (dr_info_a, dr_info_b);\n       std::swap (dr_a, dr_b);\n     }\n \n   /* If the two accesses could be dependent within a scalar iteration,\n      make sure that we'd retain their order.  */\n-  if (maybe_gt (init_a + vect_get_scalar_dr_size (dr_a), init_b)\n-      && !vect_preserves_scalar_order_p (vect_dr_stmt (dr_a),\n-\t\t\t\t\t vect_dr_stmt (dr_b)))\n+  if (maybe_gt (init_a + vect_get_scalar_dr_size (dr_info_a), init_b)\n+      && !vect_preserves_scalar_order_p (dr_info_a, dr_info_b))\n     return false;\n \n   /* There is no alias if abs (DR_STEP) is greater than or equal to\n      the bytes spanned by the combination of the two accesses.  */\n-  *lower_bound_out = init_b + vect_get_scalar_dr_size (dr_b) - init_a;\n+  *lower_bound_out = init_b + vect_get_scalar_dr_size (dr_info_b) - init_a;\n   return true;\n }\n \n@@ -3376,7 +3411,6 @@ vect_prune_runtime_alias_test_list (loop_vec_info loop_vinfo)\n     {\n       int comp_res;\n       poly_uint64 lower_bound;\n-      struct data_reference *dr_a, *dr_b;\n       tree segment_length_a, segment_length_b;\n       unsigned HOST_WIDE_INT access_size_a, access_size_b;\n       unsigned int align_a, align_b;\n@@ -3404,25 +3438,26 @@ vect_prune_runtime_alias_test_list (loop_vec_info loop_vinfo)\n \t  continue;\n \t}\n \n-      dr_a = DDR_A (ddr);\n-      stmt_vec_info stmt_info_a = vect_dr_stmt (DDR_A (ddr));\n+      dr_vec_info *dr_info_a = DR_VECT_AUX (DDR_A (ddr));\n+      stmt_vec_info stmt_info_a = dr_info_a->stmt;\n \n-      dr_b = DDR_B (ddr);\n-      stmt_vec_info stmt_info_b = vect_dr_stmt (DDR_B (ddr));\n+      dr_vec_info *dr_info_b = DR_VECT_AUX (DDR_B (ddr));\n+      stmt_vec_info stmt_info_b = dr_info_b->stmt;\n \n       /* Skip the pair if inter-iteration dependencies are irrelevant\n \t and intra-iteration dependencies are guaranteed to be honored.  */\n       if (ignore_step_p\n-\t  && (vect_preserves_scalar_order_p (stmt_info_a, stmt_info_b)\n-\t      || vectorizable_with_step_bound_p (dr_a, dr_b, &lower_bound)))\n+\t  && (vect_preserves_scalar_order_p (dr_info_a, dr_info_b)\n+\t      || vectorizable_with_step_bound_p (dr_info_a, dr_info_b,\n+\t\t\t\t\t\t &lower_bound)))\n \t{\n \t  if (dump_enabled_p ())\n \t    {\n \t      dump_printf_loc (MSG_NOTE, vect_location,\n \t\t\t       \"no need for alias check between \");\n-\t      dump_generic_expr (MSG_NOTE, TDF_SLIM, DR_REF (dr_a));\n+\t      dump_generic_expr (MSG_NOTE, TDF_SLIM, DR_REF (dr_info_a->dr));\n \t      dump_printf (MSG_NOTE, \" and \");\n-\t      dump_generic_expr (MSG_NOTE, TDF_SLIM, DR_REF (dr_b));\n+\t      dump_generic_expr (MSG_NOTE, TDF_SLIM, DR_REF (dr_info_b->dr));\n \t      dump_printf (MSG_NOTE, \" when VF is 1\\n\");\n \t    }\n \t  continue;\n@@ -3433,20 +3468,21 @@ vect_prune_runtime_alias_test_list (loop_vec_info loop_vinfo)\n \t (It might not be, for example, if the minimum step is much larger\n \t than the number of bytes handled by one vector iteration.)  */\n       if (!ignore_step_p\n-\t  && TREE_CODE (DR_STEP (dr_a)) != INTEGER_CST\n-\t  && vectorizable_with_step_bound_p (dr_a, dr_b, &lower_bound)\n-\t  && (vect_small_gap_p (loop_vinfo, dr_a, lower_bound)\n-\t      || vect_small_gap_p (loop_vinfo, dr_b, lower_bound)))\n+\t  && TREE_CODE (DR_STEP (dr_info_a->dr)) != INTEGER_CST\n+\t  && vectorizable_with_step_bound_p (dr_info_a, dr_info_b,\n+\t\t\t\t\t     &lower_bound)\n+\t  && (vect_small_gap_p (loop_vinfo, dr_info_a, lower_bound)\n+\t      || vect_small_gap_p (loop_vinfo, dr_info_b, lower_bound)))\n \t{\n-\t  bool unsigned_p = dr_known_forward_stride_p (dr_a);\n+\t  bool unsigned_p = dr_known_forward_stride_p (dr_info_a->dr);\n \t  if (dump_enabled_p ())\n \t    {\n \t      dump_printf_loc (MSG_NOTE, vect_location, \"no alias between \");\n-\t      dump_generic_expr (MSG_NOTE, TDF_SLIM, DR_REF (dr_a));\n+\t      dump_generic_expr (MSG_NOTE, TDF_SLIM, DR_REF (dr_info_a->dr));\n \t      dump_printf (MSG_NOTE, \" and \");\n-\t      dump_generic_expr (MSG_NOTE, TDF_SLIM, DR_REF (dr_b));\n+\t      dump_generic_expr (MSG_NOTE, TDF_SLIM, DR_REF (dr_info_b->dr));\n \t      dump_printf (MSG_NOTE, \" when the step \");\n-\t      dump_generic_expr (MSG_NOTE, TDF_SLIM, DR_STEP (dr_a));\n+\t      dump_generic_expr (MSG_NOTE, TDF_SLIM, DR_STEP (dr_info_a->dr));\n \t      dump_printf (MSG_NOTE, \" is outside \");\n \t      if (unsigned_p)\n \t\tdump_printf (MSG_NOTE, \"[0\");\n@@ -3459,23 +3495,23 @@ vect_prune_runtime_alias_test_list (loop_vec_info loop_vinfo)\n \t      dump_dec (MSG_NOTE, lower_bound);\n \t      dump_printf (MSG_NOTE, \")\\n\");\n \t    }\n-\t  vect_check_lower_bound (loop_vinfo, DR_STEP (dr_a), unsigned_p,\n-\t\t\t\t  lower_bound);\n+\t  vect_check_lower_bound (loop_vinfo, DR_STEP (dr_info_a->dr),\n+\t\t\t\t  unsigned_p, lower_bound);\n \t  continue;\n \t}\n \n       stmt_vec_info dr_group_first_a = DR_GROUP_FIRST_ELEMENT (stmt_info_a);\n       if (dr_group_first_a)\n \t{\n \t  stmt_info_a = dr_group_first_a;\n-\t  dr_a = STMT_VINFO_DATA_REF (stmt_info_a);\n+\t  dr_info_a = STMT_VINFO_DR_INFO (stmt_info_a);\n \t}\n \n       stmt_vec_info dr_group_first_b = DR_GROUP_FIRST_ELEMENT (stmt_info_b);\n       if (dr_group_first_b)\n \t{\n \t  stmt_info_b = dr_group_first_b;\n-\t  dr_b = STMT_VINFO_DATA_REF (stmt_info_b);\n+\t  dr_info_b = STMT_VINFO_DR_INFO (stmt_info_b);\n \t}\n \n       if (ignore_step_p)\n@@ -3485,32 +3521,33 @@ vect_prune_runtime_alias_test_list (loop_vec_info loop_vinfo)\n \t}\n       else\n \t{\n-\t  if (!operand_equal_p (DR_STEP (dr_a), DR_STEP (dr_b), 0))\n+\t  if (!operand_equal_p (DR_STEP (dr_info_a->dr),\n+\t\t\t\tDR_STEP (dr_info_b->dr), 0))\n \t    length_factor = scalar_loop_iters;\n \t  else\n \t    length_factor = size_int (vect_factor);\n-\t  segment_length_a = vect_vfa_segment_size (dr_a, length_factor);\n-\t  segment_length_b = vect_vfa_segment_size (dr_b, length_factor);\n+\t  segment_length_a = vect_vfa_segment_size (dr_info_a, length_factor);\n+\t  segment_length_b = vect_vfa_segment_size (dr_info_b, length_factor);\n \t}\n-      access_size_a = vect_vfa_access_size (dr_a);\n-      access_size_b = vect_vfa_access_size (dr_b);\n-      align_a = vect_vfa_align (dr_a);\n-      align_b = vect_vfa_align (dr_b);\n+      access_size_a = vect_vfa_access_size (dr_info_a);\n+      access_size_b = vect_vfa_access_size (dr_info_b);\n+      align_a = vect_vfa_align (dr_info_a);\n+      align_b = vect_vfa_align (dr_info_b);\n \n-      comp_res = data_ref_compare_tree (DR_BASE_ADDRESS (dr_a),\n-\t\t\t\t\tDR_BASE_ADDRESS (dr_b));\n+      comp_res = data_ref_compare_tree (DR_BASE_ADDRESS (dr_info_a->dr),\n+\t\t\t\t\tDR_BASE_ADDRESS (dr_info_b->dr));\n       if (comp_res == 0)\n-\tcomp_res = data_ref_compare_tree (DR_OFFSET (dr_a),\n-\t\t\t\t\t  DR_OFFSET (dr_b));\n+\tcomp_res = data_ref_compare_tree (DR_OFFSET (dr_info_a->dr),\n+\t\t\t\t\t  DR_OFFSET (dr_info_b->dr));\n \n       /* See whether the alias is known at compilation time.  */\n       if (comp_res == 0\n-\t  && TREE_CODE (DR_STEP (dr_a)) == INTEGER_CST\n-\t  && TREE_CODE (DR_STEP (dr_b)) == INTEGER_CST\n+\t  && TREE_CODE (DR_STEP (dr_info_a->dr)) == INTEGER_CST\n+\t  && TREE_CODE (DR_STEP (dr_info_b->dr)) == INTEGER_CST\n \t  && poly_int_tree_p (segment_length_a)\n \t  && poly_int_tree_p (segment_length_b))\n \t{\n-\t  int res = vect_compile_time_alias (dr_a, dr_b,\n+\t  int res = vect_compile_time_alias (dr_info_a, dr_info_b,\n \t\t\t\t\t     segment_length_a,\n \t\t\t\t\t     segment_length_b,\n \t\t\t\t\t     access_size_a,\n@@ -3519,9 +3556,9 @@ vect_prune_runtime_alias_test_list (loop_vec_info loop_vinfo)\n \t    {\n \t      dump_printf_loc (MSG_NOTE, vect_location,\n \t\t\t       \"can tell at compile time that \");\n-\t      dump_generic_expr (MSG_NOTE, TDF_SLIM, DR_REF (dr_a));\n+\t      dump_generic_expr (MSG_NOTE, TDF_SLIM, DR_REF (dr_info_a->dr));\n \t      dump_printf (MSG_NOTE, \" and \");\n-\t      dump_generic_expr (MSG_NOTE, TDF_SLIM, DR_REF (dr_b));\n+\t      dump_generic_expr (MSG_NOTE, TDF_SLIM, DR_REF (dr_info_b->dr));\n \t      if (res == 0)\n \t\tdump_printf (MSG_NOTE, \" do not alias\\n\");\n \t      else\n@@ -3541,8 +3578,10 @@ vect_prune_runtime_alias_test_list (loop_vec_info loop_vinfo)\n \t}\n \n       dr_with_seg_len_pair_t dr_with_seg_len_pair\n-\t(dr_with_seg_len (dr_a, segment_length_a, access_size_a, align_a),\n-\t dr_with_seg_len (dr_b, segment_length_b, access_size_b, align_b));\n+\t(dr_with_seg_len (dr_info_a->dr, segment_length_a,\n+\t\t\t  access_size_a, align_a),\n+\t dr_with_seg_len (dr_info_b->dr, segment_length_b,\n+\t\t\t  access_size_b, align_b));\n \n       /* Canonicalize pairs by sorting the two DR members.  */\n       if (comp_res > 0)\n@@ -4451,18 +4490,18 @@ vect_get_new_ssa_name (tree type, enum vect_var_kind var_kind, const char *name)\n   return new_vect_var;\n }\n \n-/* Duplicate ptr info and set alignment/misaligment on NAME from DR.  */\n+/* Duplicate ptr info and set alignment/misaligment on NAME from DR_INFO.  */\n \n static void\n-vect_duplicate_ssa_name_ptr_info (tree name, data_reference *dr)\n+vect_duplicate_ssa_name_ptr_info (tree name, dr_vec_info *dr_info)\n {\n-  duplicate_ssa_name_ptr_info (name, DR_PTR_INFO (dr));\n-  int misalign = DR_MISALIGNMENT (dr);\n+  duplicate_ssa_name_ptr_info (name, DR_PTR_INFO (dr_info->dr));\n+  int misalign = DR_MISALIGNMENT (dr_info);\n   if (misalign == DR_MISALIGNMENT_UNKNOWN)\n     mark_ptr_info_alignment_unknown (SSA_NAME_PTR_INFO (name));\n   else\n     set_ptr_info_alignment (SSA_NAME_PTR_INFO (name),\n-\t\t\t    DR_TARGET_ALIGNMENT (dr), misalign);\n+\t\t\t    DR_TARGET_ALIGNMENT (dr_info), misalign);\n }\n \n /* Function vect_create_addr_base_for_vector_ref.\n@@ -4505,15 +4544,16 @@ vect_create_addr_base_for_vector_ref (stmt_vec_info stmt_info,\n \t\t\t\t      tree offset,\n \t\t\t\t      tree byte_offset)\n {\n-  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n+  dr_vec_info *dr_info = STMT_VINFO_DR_INFO (stmt_info);\n+  struct data_reference *dr = dr_info->dr;\n   const char *base_name;\n   tree addr_base;\n   tree dest;\n   gimple_seq seq = NULL;\n   tree vect_ptr_type;\n   tree step = TYPE_SIZE_UNIT (TREE_TYPE (DR_REF (dr)));\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n-  innermost_loop_behavior *drb = vect_dr_behavior (dr);\n+  innermost_loop_behavior *drb = vect_dr_behavior (dr_info);\n \n   tree data_ref_base = unshare_expr (drb->base_address);\n   tree base_offset = unshare_expr (drb->offset);\n@@ -4566,7 +4606,7 @@ vect_create_addr_base_for_vector_ref (stmt_vec_info stmt_info,\n       && TREE_CODE (addr_base) == SSA_NAME\n       && !SSA_NAME_PTR_INFO (addr_base))\n     {\n-      vect_duplicate_ssa_name_ptr_info (addr_base, dr);\n+      vect_duplicate_ssa_name_ptr_info (addr_base, dr_info);\n       if (offset || byte_offset)\n \tmark_ptr_info_alignment_unknown (SSA_NAME_PTR_INFO (addr_base));\n     }\n@@ -4658,7 +4698,8 @@ vect_create_data_ref_ptr (stmt_vec_info stmt_info, tree aggr_type,\n   edge pe = NULL;\n   basic_block new_bb;\n   tree aggr_ptr_init;\n-  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n+  dr_vec_info *dr_info = STMT_VINFO_DR_INFO (stmt_info);\n+  struct data_reference *dr = dr_info->dr;\n   tree aptr;\n   gimple_stmt_iterator incr_gsi;\n   bool insert_after;\n@@ -4687,7 +4728,7 @@ vect_create_data_ref_ptr (stmt_vec_info stmt_info, tree aggr_type,\n \n   /* Check the step (evolution) of the load in LOOP, and record\n      whether it's invariant.  */\n-  step = vect_dr_behavior (dr)->step;\n+  step = vect_dr_behavior (dr_info)->step;\n   if (integer_zerop (step))\n     *inv_p = true;\n   else\n@@ -4832,8 +4873,8 @@ vect_create_data_ref_ptr (stmt_vec_info stmt_info, tree aggr_type,\n       /* Copy the points-to information if it exists. */\n       if (DR_PTR_INFO (dr))\n \t{\n-\t  vect_duplicate_ssa_name_ptr_info (indx_before_incr, dr);\n-\t  vect_duplicate_ssa_name_ptr_info (indx_after_incr, dr);\n+\t  vect_duplicate_ssa_name_ptr_info (indx_before_incr, dr_info);\n+\t  vect_duplicate_ssa_name_ptr_info (indx_after_incr, dr_info);\n \t}\n       if (ptr_incr)\n \t*ptr_incr = incr;\n@@ -4862,8 +4903,8 @@ vect_create_data_ref_ptr (stmt_vec_info stmt_info, tree aggr_type,\n       /* Copy the points-to information if it exists. */\n       if (DR_PTR_INFO (dr))\n \t{\n-\t  vect_duplicate_ssa_name_ptr_info (indx_before_incr, dr);\n-\t  vect_duplicate_ssa_name_ptr_info (indx_after_incr, dr);\n+\t  vect_duplicate_ssa_name_ptr_info (indx_before_incr, dr_info);\n+\t  vect_duplicate_ssa_name_ptr_info (indx_after_incr, dr_info);\n \t}\n       if (ptr_incr)\n \t*ptr_incr = incr;\n@@ -5406,7 +5447,8 @@ vect_setup_realignment (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n {\n   tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n-  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n+  dr_vec_info *dr_info = STMT_VINFO_DR_INFO (stmt_info);\n+  struct data_reference *dr = dr_info->dr;\n   struct loop *loop = NULL;\n   edge pe = NULL;\n   tree scalar_dest = gimple_assign_lhs (stmt_info->stmt);\n@@ -5519,7 +5561,7 @@ vect_setup_realignment (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \tnew_temp = copy_ssa_name (ptr);\n       else\n \tnew_temp = make_ssa_name (TREE_TYPE (ptr));\n-      unsigned int align = DR_TARGET_ALIGNMENT (dr);\n+      unsigned int align = DR_TARGET_ALIGNMENT (dr_info);\n       new_stmt = gimple_build_assign\n \t\t   (new_temp, BIT_AND_EXPR, ptr,\n \t\t    build_int_cst (TREE_TYPE (ptr), -(HOST_WIDE_INT) align));\n@@ -6421,24 +6463,25 @@ vect_can_force_dr_alignment_p (const_tree decl, unsigned int alignment)\n }\n \n \n-/* Return whether the data reference DR is supported with respect to its\n+/* Return whether the data reference DR_INFO is supported with respect to its\n    alignment.\n    If CHECK_ALIGNED_ACCESSES is TRUE, check if the access is supported even\n    it is aligned, i.e., check if it is possible to vectorize it with different\n    alignment.  */\n \n enum dr_alignment_support\n-vect_supportable_dr_alignment (struct data_reference *dr,\n+vect_supportable_dr_alignment (dr_vec_info *dr_info,\n                                bool check_aligned_accesses)\n {\n-  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+  data_reference *dr = dr_info->dr;\n+  stmt_vec_info stmt_info = dr_info->stmt;\n   tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n   machine_mode mode = TYPE_MODE (vectype);\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   struct loop *vect_loop = NULL;\n   bool nested_in_vect_loop = false;\n \n-  if (aligned_access_p (dr) && !check_aligned_accesses)\n+  if (aligned_access_p (dr_info) && !check_aligned_accesses)\n     return dr_aligned;\n \n   /* For now assume all conditional loads/stores support unaligned\n@@ -6546,11 +6589,11 @@ vect_supportable_dr_alignment (struct data_reference *dr,\n \t  else\n \t    return dr_explicit_realign_optimized;\n \t}\n-      if (!known_alignment_for_access_p (dr))\n+      if (!known_alignment_for_access_p (dr_info))\n \tis_packed = not_size_aligned (DR_REF (dr));\n \n       if (targetm.vectorize.support_vector_misalignment\n-\t    (mode, type, DR_MISALIGNMENT (dr), is_packed))\n+\t    (mode, type, DR_MISALIGNMENT (dr_info), is_packed))\n \t/* Can't software pipeline the loads, but can at least do them.  */\n \treturn dr_unaligned_supported;\n     }\n@@ -6559,11 +6602,11 @@ vect_supportable_dr_alignment (struct data_reference *dr,\n       bool is_packed = false;\n       tree type = (TREE_TYPE (DR_REF (dr)));\n \n-      if (!known_alignment_for_access_p (dr))\n+      if (!known_alignment_for_access_p (dr_info))\n \tis_packed = not_size_aligned (DR_REF (dr));\n \n      if (targetm.vectorize.support_vector_misalignment\n-\t   (mode, type, DR_MISALIGNMENT (dr), is_packed))\n+\t   (mode, type, DR_MISALIGNMENT (dr_info), is_packed))\n        return dr_unaligned_supported;\n     }\n "}, {"sha": "58cfebf23bb576a58850bc699885c011d8f95780", "filename": "gcc/tree-vect-loop-manip.c", "status": "modified", "additions": 10, "deletions": 8, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/89fa689a9e898ccb81b966477b3ac4e254461b05/gcc%2Ftree-vect-loop-manip.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/89fa689a9e898ccb81b966477b3ac4e254461b05/gcc%2Ftree-vect-loop-manip.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-loop-manip.c?ref=89fa689a9e898ccb81b966477b3ac4e254461b05", "patch": "@@ -1560,14 +1560,15 @@ vect_update_ivs_after_vectorizer (loop_vec_info loop_vinfo,\n static tree\n get_misalign_in_elems (gimple **seq, loop_vec_info loop_vinfo)\n {\n-  struct data_reference *dr = LOOP_VINFO_UNALIGNED_DR (loop_vinfo);\n-  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+  dr_vec_info *dr_info = DR_VECT_AUX (LOOP_VINFO_UNALIGNED_DR (loop_vinfo));\n+  stmt_vec_info stmt_info = dr_info->stmt;\n   tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n \n-  unsigned int target_align = DR_TARGET_ALIGNMENT (dr);\n+  unsigned int target_align = DR_TARGET_ALIGNMENT (dr_info);\n   gcc_assert (target_align != 0);\n \n-  bool negative = tree_int_cst_compare (DR_STEP (dr), size_zero_node) < 0;\n+  bool negative = tree_int_cst_compare (DR_STEP (dr_info->dr),\n+\t\t\t\t\tsize_zero_node) < 0;\n   tree offset = (negative\n \t\t ? size_int (-TYPE_VECTOR_SUBPARTS (vectype) + 1)\n \t\t : size_zero_node);\n@@ -1626,14 +1627,14 @@ static tree\n vect_gen_prolog_loop_niters (loop_vec_info loop_vinfo,\n \t\t\t     basic_block bb, int *bound)\n {\n-  struct data_reference *dr = LOOP_VINFO_UNALIGNED_DR (loop_vinfo);\n+  dr_vec_info *dr_info = DR_VECT_AUX (LOOP_VINFO_UNALIGNED_DR (loop_vinfo));\n   tree var;\n   tree niters_type = TREE_TYPE (LOOP_VINFO_NITERS (loop_vinfo));\n   gimple_seq stmts = NULL, new_stmts = NULL;\n   tree iters, iters_name;\n-  stmt_vec_info stmt_info = vect_dr_stmt (dr);\n+  stmt_vec_info stmt_info = dr_info->stmt;\n   tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n-  unsigned int target_align = DR_TARGET_ALIGNMENT (dr);\n+  unsigned int target_align = DR_TARGET_ALIGNMENT (dr_info);\n \n   if (LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo) > 0)\n     {\n@@ -1658,7 +1659,8 @@ vect_gen_prolog_loop_niters (loop_vec_info loop_vinfo,\n \n       /* Create:  (niters_type) ((align_in_elems - misalign_in_elems)\n \t\t\t\t & (align_in_elems - 1)).  */\n-      bool negative = tree_int_cst_compare (DR_STEP (dr), size_zero_node) < 0;\n+      bool negative = tree_int_cst_compare (DR_STEP (dr_info->dr),\n+\t\t\t\t\t    size_zero_node) < 0;\n       if (negative)\n \titers = fold_build2 (MINUS_EXPR, type, misalign_in_elems,\n \t\t\t     align_in_elems_tree);"}, {"sha": "dc67dc5bebec0cd6816755f27b7cca48c7e467e3", "filename": "gcc/tree-vect-loop.c", "status": "modified", "additions": 3, "deletions": 2, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/89fa689a9e898ccb81b966477b3ac4e254461b05/gcc%2Ftree-vect-loop.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/89fa689a9e898ccb81b966477b3ac4e254461b05/gcc%2Ftree-vect-loop.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-loop.c?ref=89fa689a9e898ccb81b966477b3ac4e254461b05", "patch": "@@ -2142,8 +2142,9 @@ vect_analyze_loop_2 (loop_vec_info loop_vinfo, bool &fatal, unsigned *n_stmts)\n \t  /* Niters for peeled prolog loop.  */\n \t  if (LOOP_VINFO_PEELING_FOR_ALIGNMENT (loop_vinfo) < 0)\n \t    {\n-\t      struct data_reference *dr = LOOP_VINFO_UNALIGNED_DR (loop_vinfo);\n-\t      tree vectype = STMT_VINFO_VECTYPE (vect_dr_stmt (dr));\n+\t      dr_vec_info *dr_info\n+\t\t= DR_VECT_AUX (LOOP_VINFO_UNALIGNED_DR (loop_vinfo));\n+\t      tree vectype = STMT_VINFO_VECTYPE (dr_info->stmt);\n \t      niters_th += TYPE_VECTOR_SUBPARTS (vectype) - 1;\n \t    }\n \t  else"}, {"sha": "46ef3bb39ccb06362a56ea8aa1e739d78a8687ad", "filename": "gcc/tree-vect-stmts.c", "status": "modified", "additions": 100, "deletions": 88, "changes": 188, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/89fa689a9e898ccb81b966477b3ac4e254461b05/gcc%2Ftree-vect-stmts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/89fa689a9e898ccb81b966477b3ac4e254461b05/gcc%2Ftree-vect-stmts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-stmts.c?ref=89fa689a9e898ccb81b966477b3ac4e254461b05", "patch": "@@ -1057,8 +1057,9 @@ vect_get_store_cost (stmt_vec_info stmt_info, int ncopies,\n \t\t     unsigned int *inside_cost,\n \t\t     stmt_vector_for_cost *body_cost_vec)\n {\n-  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n-  int alignment_support_scheme = vect_supportable_dr_alignment (dr, false);\n+  dr_vec_info *dr_info = STMT_VINFO_DR_INFO (stmt_info);\n+  int alignment_support_scheme\n+    = vect_supportable_dr_alignment (dr_info, false);\n \n   switch (alignment_support_scheme)\n     {\n@@ -1079,7 +1080,8 @@ vect_get_store_cost (stmt_vec_info stmt_info, int ncopies,\n         /* Here, we assign an additional cost for the unaligned store.  */\n \t*inside_cost += record_stmt_cost (body_cost_vec, ncopies,\n \t\t\t\t\t  unaligned_store, stmt_info,\n-\t\t\t\t\t  DR_MISALIGNMENT (dr), vect_body);\n+\t\t\t\t\t  DR_MISALIGNMENT (dr_info),\n+\t\t\t\t\t  vect_body);\n         if (dump_enabled_p ())\n           dump_printf_loc (MSG_NOTE, vect_location,\n                            \"vect_model_store_cost: unaligned supported by \"\n@@ -1236,8 +1238,9 @@ vect_get_load_cost (stmt_vec_info stmt_info, int ncopies,\n \t\t    stmt_vector_for_cost *body_cost_vec,\n \t\t    bool record_prologue_costs)\n {\n-  data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n-  int alignment_support_scheme = vect_supportable_dr_alignment (dr, false);\n+  dr_vec_info *dr_info = STMT_VINFO_DR_INFO (stmt_info);\n+  int alignment_support_scheme\n+    = vect_supportable_dr_alignment (dr_info, false);\n \n   switch (alignment_support_scheme)\n     {\n@@ -1257,7 +1260,8 @@ vect_get_load_cost (stmt_vec_info stmt_info, int ncopies,\n         /* Here, we assign an additional cost for the unaligned load.  */\n \t*inside_cost += record_stmt_cost (body_cost_vec, ncopies,\n \t\t\t\t\t  unaligned_load, stmt_info,\n-\t\t\t\t\t  DR_MISALIGNMENT (dr), vect_body);\n+\t\t\t\t\t  DR_MISALIGNMENT (dr_info),\n+\t\t\t\t\t  vect_body);\n \n         if (dump_enabled_p ())\n           dump_printf_loc (MSG_NOTE, vect_location,\n@@ -1975,7 +1979,8 @@ vect_truncate_gather_scatter_offset (stmt_vec_info stmt_info,\n \t\t\t\t     loop_vec_info loop_vinfo, bool masked_p,\n \t\t\t\t     gather_scatter_info *gs_info)\n {\n-  data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n+  dr_vec_info *dr_info = STMT_VINFO_DR_INFO (stmt_info);\n+  data_reference *dr = dr_info->dr;\n   tree step = DR_STEP (dr);\n   if (TREE_CODE (step) != INTEGER_CST)\n     {\n@@ -2003,7 +2008,7 @@ vect_truncate_gather_scatter_offset (stmt_vec_info stmt_info,\n     count = max_iters.to_shwi ();\n \n   /* Try scales of 1 and the element size.  */\n-  int scales[] = { 1, vect_get_scalar_dr_size (dr) };\n+  int scales[] = { 1, vect_get_scalar_dr_size (dr_info) };\n   wi::overflow_type overflow = wi::OVF_NONE;\n   for (int i = 0; i < 2; ++i)\n     {\n@@ -2102,8 +2107,8 @@ vect_use_strided_gather_scatters_p (stmt_vec_info stmt_info,\n static int\n compare_step_with_zero (stmt_vec_info stmt_info)\n {\n-  data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n-  return tree_int_cst_compare (vect_dr_behavior (dr)->step,\n+  dr_vec_info *dr_info = STMT_VINFO_DR_INFO (stmt_info);\n+  return tree_int_cst_compare (vect_dr_behavior (dr_info)->step,\n \t\t\t       size_zero_node);\n }\n \n@@ -2166,7 +2171,7 @@ get_group_load_store_type (stmt_vec_info stmt_info, tree vectype, bool slp,\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   struct loop *loop = loop_vinfo ? LOOP_VINFO_LOOP (loop_vinfo) : NULL;\n   stmt_vec_info first_stmt_info = DR_GROUP_FIRST_ELEMENT (stmt_info);\n-  data_reference *first_dr = STMT_VINFO_DATA_REF (first_stmt_info);\n+  dr_vec_info *first_dr_info = STMT_VINFO_DR_INFO (first_stmt_info);\n   unsigned int group_size = DR_GROUP_SIZE (first_stmt_info);\n   bool single_element_p = (stmt_info == first_stmt_info\n \t\t\t   && !DR_GROUP_NEXT_ELEMENT (stmt_info));\n@@ -2218,8 +2223,8 @@ get_group_load_store_type (stmt_vec_info stmt_info, tree vectype, bool slp,\n \t     be a multiple of B and so we are guaranteed to access a\n \t     non-gap element in the same B-sized block.  */\n \t  if (overrun_p\n-\t      && gap < (vect_known_alignment_in_bytes (first_dr)\n-\t\t\t/ vect_get_scalar_dr_size (first_dr)))\n+\t      && gap < (vect_known_alignment_in_bytes (first_dr_info)\n+\t\t\t/ vect_get_scalar_dr_size (first_dr_info)))\n \t    overrun_p = false;\n \t  if (overrun_p && !can_overrun_p)\n \t    {\n@@ -2246,8 +2251,8 @@ get_group_load_store_type (stmt_vec_info stmt_info, tree vectype, bool slp,\n \t same B-sized block.  */\n       if (would_overrun_p\n \t  && !masked_p\n-\t  && gap < (vect_known_alignment_in_bytes (first_dr)\n-\t\t    / vect_get_scalar_dr_size (first_dr)))\n+\t  && gap < (vect_known_alignment_in_bytes (first_dr_info)\n+\t\t    / vect_get_scalar_dr_size (first_dr_info)))\n \twould_overrun_p = false;\n \n       if (!STMT_VINFO_STRIDED_P (stmt_info)\n@@ -2339,7 +2344,7 @@ get_negative_load_store_type (stmt_vec_info stmt_info, tree vectype,\n \t\t\t      vec_load_store_type vls_type,\n \t\t\t      unsigned int ncopies)\n {\n-  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info);\n+  dr_vec_info *dr_info = STMT_VINFO_DR_INFO (stmt_info);\n   dr_alignment_support alignment_support_scheme;\n \n   if (ncopies > 1)\n@@ -2350,7 +2355,7 @@ get_negative_load_store_type (stmt_vec_info stmt_info, tree vectype,\n       return VMAT_ELEMENTWISE;\n     }\n \n-  alignment_support_scheme = vect_supportable_dr_alignment (dr, false);\n+  alignment_support_scheme = vect_supportable_dr_alignment (dr_info, false);\n   if (alignment_support_scheme != dr_aligned\n       && alignment_support_scheme != dr_unaligned_supported)\n     {\n@@ -2923,19 +2928,19 @@ vect_get_strided_load_store_ops (stmt_vec_info stmt_info,\n }\n \n /* Return the amount that should be added to a vector pointer to move\n-   to the next or previous copy of AGGR_TYPE.  DR is the data reference\n+   to the next or previous copy of AGGR_TYPE.  DR_INFO is the data reference\n    being vectorized and MEMORY_ACCESS_TYPE describes the type of\n    vectorization.  */\n \n static tree\n-vect_get_data_ptr_increment (data_reference *dr, tree aggr_type,\n+vect_get_data_ptr_increment (dr_vec_info *dr_info, tree aggr_type,\n \t\t\t     vect_memory_access_type memory_access_type)\n {\n   if (memory_access_type == VMAT_INVARIANT)\n     return size_zero_node;\n \n   tree iv_step = TYPE_SIZE_UNIT (aggr_type);\n-  tree step = vect_dr_behavior (dr)->step;\n+  tree step = vect_dr_behavior (dr_info)->step;\n   if (tree_int_cst_sgn (step) == -1)\n     iv_step = fold_build1 (NEGATE_EXPR, TREE_TYPE (iv_step), iv_step);\n   return iv_step;\n@@ -6169,19 +6174,20 @@ vectorizable_operation (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n   return true;\n }\n \n-/* A helper function to ensure data reference DR's base alignment.  */\n+/* A helper function to ensure data reference DR_INFO's base alignment.  */\n \n static void\n-ensure_base_align (struct data_reference *dr)\n+ensure_base_align (dr_vec_info *dr_info)\n {\n-  if (DR_VECT_AUX (dr)->misalignment == DR_MISALIGNMENT_UNINITIALIZED)\n+  if (dr_info->misalignment == DR_MISALIGNMENT_UNINITIALIZED)\n     return;\n \n-  if (DR_VECT_AUX (dr)->base_misaligned)\n+  if (dr_info->base_misaligned)\n     {\n-      tree base_decl = DR_VECT_AUX (dr)->base_decl;\n+      tree base_decl = dr_info->base_decl;\n \n-      unsigned int align_base_to = DR_TARGET_ALIGNMENT (dr) * BITS_PER_UNIT;\n+      unsigned int align_base_to\n+\t= DR_TARGET_ALIGNMENT (dr_info) * BITS_PER_UNIT;\n \n       if (decl_in_symtab_p (base_decl))\n \tsymtab_node::get (base_decl)->increase_alignment (align_base_to);\n@@ -6190,7 +6196,7 @@ ensure_base_align (struct data_reference *dr)\n \t  SET_DECL_ALIGN (base_decl, align_base_to);\n           DECL_USER_ALIGN (base_decl) = 1;\n \t}\n-      DR_VECT_AUX (dr)->base_misaligned = false;\n+      dr_info->base_misaligned = false;\n     }\n }\n \n@@ -6239,7 +6245,6 @@ vectorizable_store (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n   tree data_ref;\n   tree op;\n   tree vec_oprnd = NULL_TREE;\n-  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info), *first_dr = NULL;\n   tree elem_type;\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   struct loop *loop = NULL;\n@@ -6401,19 +6406,20 @@ vectorizable_store (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \treturn false;\n     }\n \n+  dr_vec_info *dr_info = STMT_VINFO_DR_INFO (stmt_info), *first_dr_info = NULL;\n   grouped_store = (STMT_VINFO_GROUPED_ACCESS (stmt_info)\n \t\t   && memory_access_type != VMAT_GATHER_SCATTER\n \t\t   && (slp || memory_access_type != VMAT_CONTIGUOUS));\n   if (grouped_store)\n     {\n       first_stmt_info = DR_GROUP_FIRST_ELEMENT (stmt_info);\n-      first_dr = STMT_VINFO_DATA_REF (first_stmt_info);\n+      first_dr_info = STMT_VINFO_DR_INFO (first_stmt_info);\n       group_size = DR_GROUP_SIZE (first_stmt_info);\n     }\n   else\n     {\n       first_stmt_info = stmt_info;\n-      first_dr = dr;\n+      first_dr_info = dr_info;\n       group_size = vec_num = 1;\n     }\n \n@@ -6435,7 +6441,7 @@ vectorizable_store (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \n   /* Transform.  */\n \n-  ensure_base_align (dr);\n+  ensure_base_align (dr_info);\n \n   if (memory_access_type == VMAT_GATHER_SCATTER && gs_info.decl)\n     {\n@@ -6614,7 +6620,7 @@ vectorizable_store (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t  first_stmt_info = SLP_TREE_SCALAR_STMTS (slp_node)[0];\n \t  gcc_assert (DR_GROUP_FIRST_ELEMENT (first_stmt_info)\n \t\t      == first_stmt_info);\n-\t  first_dr = STMT_VINFO_DATA_REF (first_stmt_info);\n+\t  first_dr_info = STMT_VINFO_DR_INFO (first_stmt_info);\n \t  op = vect_get_store_rhs (first_stmt_info);\n         } \n       else\n@@ -6625,7 +6631,7 @@ vectorizable_store (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n       ref_type = get_group_alias_ptr_type (first_stmt_info);\n     }\n   else\n-    ref_type = reference_alias_ptr_type (DR_REF (first_dr));\n+    ref_type = reference_alias_ptr_type (DR_REF (first_dr_info->dr));\n \n   if (dump_enabled_p ())\n     dump_printf_loc (MSG_NOTE, vect_location,\n@@ -6651,11 +6657,11 @@ vectorizable_store (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \n       stride_base\n \t= fold_build_pointer_plus\n-\t    (DR_BASE_ADDRESS (first_dr),\n+\t    (DR_BASE_ADDRESS (first_dr_info->dr),\n \t     size_binop (PLUS_EXPR,\n-\t\t\t convert_to_ptrofftype (DR_OFFSET (first_dr)),\n-\t\t\t convert_to_ptrofftype (DR_INIT (first_dr))));\n-      stride_step = fold_convert (sizetype, DR_STEP (first_dr));\n+\t\t\t convert_to_ptrofftype (DR_OFFSET (first_dr_info->dr)),\n+\t\t\t convert_to_ptrofftype (DR_INIT (first_dr_info->dr))));\n+      stride_step = fold_convert (sizetype, DR_STEP (first_dr_info->dr));\n \n       /* For a store with loop-invariant (but other than power-of-2)\n          stride (i.e. not a grouped access) like so:\n@@ -6835,7 +6841,7 @@ vectorizable_store (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t\t\t\t\t\t group_el * elsz);\n \t\t  newref = build2 (MEM_REF, ltype,\n \t\t\t\t   running_off, this_off);\n-\t\t  vect_copy_ref_info (newref, DR_REF (first_dr));\n+\t\t  vect_copy_ref_info (newref, DR_REF (first_dr_info->dr));\n \n \t\t  /* And store it to *running_off.  */\n \t\t  assign = gimple_build_assign (newref, elem);\n@@ -6878,7 +6884,8 @@ vectorizable_store (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n   auto_vec<tree> dr_chain (group_size);\n   oprnds.create (group_size);\n \n-  alignment_support_scheme = vect_supportable_dr_alignment (first_dr, false);\n+  alignment_support_scheme\n+    = vect_supportable_dr_alignment (first_dr_info, false);\n   gcc_assert (alignment_support_scheme);\n   vec_loop_masks *loop_masks\n     = (loop_vinfo && LOOP_VINFO_FULLY_MASKED_P (loop_vinfo)\n@@ -6916,7 +6923,8 @@ vectorizable_store (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \taggr_type = build_array_type_nelts (elem_type, vec_num * nunits);\n       else\n \taggr_type = vectype;\n-      bump = vect_get_data_ptr_increment (dr, aggr_type, memory_access_type);\n+      bump = vect_get_data_ptr_increment (dr_info, aggr_type,\n+\t\t\t\t\t  memory_access_type);\n     }\n \n   if (mask)\n@@ -7011,14 +7019,14 @@ vectorizable_store (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t  bool simd_lane_access_p\n \t    = STMT_VINFO_SIMD_LANE_ACCESS_P (stmt_info);\n \t  if (simd_lane_access_p\n-\t      && TREE_CODE (DR_BASE_ADDRESS (first_dr)) == ADDR_EXPR\n-\t      && VAR_P (TREE_OPERAND (DR_BASE_ADDRESS (first_dr), 0))\n-\t      && integer_zerop (DR_OFFSET (first_dr))\n-\t      && integer_zerop (DR_INIT (first_dr))\n+\t      && TREE_CODE (DR_BASE_ADDRESS (first_dr_info->dr)) == ADDR_EXPR\n+\t      && VAR_P (TREE_OPERAND (DR_BASE_ADDRESS (first_dr_info->dr), 0))\n+\t      && integer_zerop (DR_OFFSET (first_dr_info->dr))\n+\t      && integer_zerop (DR_INIT (first_dr_info->dr))\n \t      && alias_sets_conflict_p (get_alias_set (aggr_type),\n \t\t\t\t\tget_alias_set (TREE_TYPE (ref_type))))\n \t    {\n-\t      dataref_ptr = unshare_expr (DR_BASE_ADDRESS (first_dr));\n+\t      dataref_ptr = unshare_expr (DR_BASE_ADDRESS (first_dr_info->dr));\n \t      dataref_offset = build_int_cst (ref_type, 0);\n \t      inv_p = false;\n \t    }\n@@ -7175,16 +7183,16 @@ vectorizable_store (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t\t   vect_permute_store_chain().  */\n \t\tvec_oprnd = result_chain[i];\n \n-\t      align = DR_TARGET_ALIGNMENT (first_dr);\n-\t      if (aligned_access_p (first_dr))\n+\t      align = DR_TARGET_ALIGNMENT (first_dr_info);\n+\t      if (aligned_access_p (first_dr_info))\n \t\tmisalign = 0;\n-\t      else if (DR_MISALIGNMENT (first_dr) == -1)\n+\t      else if (DR_MISALIGNMENT (first_dr_info) == -1)\n \t\t{\n-\t\t  align = dr_alignment (vect_dr_behavior (first_dr));\n+\t\t  align = dr_alignment (vect_dr_behavior (first_dr_info));\n \t\t  misalign = 0;\n \t\t}\n \t      else\n-\t\tmisalign = DR_MISALIGNMENT (first_dr);\n+\t\tmisalign = DR_MISALIGNMENT (first_dr_info);\n \t      if (dataref_offset == NULL_TREE\n \t\t  && TREE_CODE (dataref_ptr) == SSA_NAME)\n \t\tset_ptr_info_alignment (get_ptr_info (dataref_ptr), align,\n@@ -7227,17 +7235,17 @@ vectorizable_store (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t\t\t\t\t  dataref_offset\n \t\t\t\t\t  ? dataref_offset\n \t\t\t\t\t  : build_int_cst (ref_type, 0));\n-\t\t  if (aligned_access_p (first_dr))\n+\t\t  if (aligned_access_p (first_dr_info))\n \t\t    ;\n-\t\t  else if (DR_MISALIGNMENT (first_dr) == -1)\n+\t\t  else if (DR_MISALIGNMENT (first_dr_info) == -1)\n \t\t    TREE_TYPE (data_ref)\n \t\t      = build_aligned_type (TREE_TYPE (data_ref),\n \t\t\t\t\t    align * BITS_PER_UNIT);\n \t\t  else\n \t\t    TREE_TYPE (data_ref)\n \t\t      = build_aligned_type (TREE_TYPE (data_ref),\n \t\t\t\t\t    TYPE_ALIGN (elem_type));\n-\t\t  vect_copy_ref_info (data_ref, DR_REF (first_dr));\n+\t\t  vect_copy_ref_info (data_ref, DR_REF (first_dr_info->dr));\n \t\t  gassign *new_stmt\n \t\t    = gimple_build_assign (data_ref, vec_oprnd);\n \t\t  new_stmt_info\n@@ -7400,7 +7408,6 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n   struct loop *loop = NULL;\n   struct loop *containing_loop = gimple_bb (stmt_info->stmt)->loop_father;\n   bool nested_in_vect_loop = false;\n-  struct data_reference *dr = STMT_VINFO_DATA_REF (stmt_info), *first_dr = NULL;\n   tree elem_type;\n   tree new_temp;\n   machine_mode mode;\n@@ -7663,7 +7670,8 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \n   /* Transform.  */\n \n-  ensure_base_align (dr);\n+  dr_vec_info *dr_info = STMT_VINFO_DR_INFO (stmt_info), *first_dr_info = NULL;\n+  ensure_base_align (dr_info);\n \n   if (memory_access_type == VMAT_GATHER_SCATTER && gs_info.decl)\n     {\n@@ -7692,12 +7700,12 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n       if (grouped_load)\n \t{\n \t  first_stmt_info = DR_GROUP_FIRST_ELEMENT (stmt_info);\n-\t  first_dr = STMT_VINFO_DATA_REF (first_stmt_info);\n+\t  first_dr_info = STMT_VINFO_DR_INFO (first_stmt_info);\n \t}\n       else\n \t{\n \t  first_stmt_info = stmt_info;\n-\t  first_dr = dr;\n+\t  first_dr_info = dr_info;\n \t}\n       if (slp && grouped_load)\n \t{\n@@ -7712,16 +7720,16 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t\t * vect_get_place_in_interleaving_chain (stmt_info,\n \t\t\t\t\t\t\t first_stmt_info));\n \t  group_size = 1;\n-\t  ref_type = reference_alias_ptr_type (DR_REF (dr));\n+\t  ref_type = reference_alias_ptr_type (DR_REF (dr_info->dr));\n \t}\n \n       stride_base\n \t= fold_build_pointer_plus\n-\t    (DR_BASE_ADDRESS (first_dr),\n+\t    (DR_BASE_ADDRESS (first_dr_info->dr),\n \t     size_binop (PLUS_EXPR,\n-\t\t\t convert_to_ptrofftype (DR_OFFSET (first_dr)),\n-\t\t\t convert_to_ptrofftype (DR_INIT (first_dr))));\n-      stride_step = fold_convert (sizetype, DR_STEP (first_dr));\n+\t\t\t convert_to_ptrofftype (DR_OFFSET (first_dr_info->dr)),\n+\t\t\t convert_to_ptrofftype (DR_INIT (first_dr_info->dr))));\n+      stride_step = fold_convert (sizetype, DR_STEP (first_dr_info->dr));\n \n       /* For a load with loop-invariant (but other than power-of-2)\n          stride (i.e. not a grouped access) like so:\n@@ -7850,7 +7858,7 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t      tree this_off = build_int_cst (TREE_TYPE (alias_off),\n \t\t\t\t\t     group_el * elsz + cst_offset);\n \t      tree data_ref = build2 (MEM_REF, ltype, running_off, this_off);\n-\t      vect_copy_ref_info (data_ref, DR_REF (first_dr));\n+\t      vect_copy_ref_info (data_ref, DR_REF (first_dr_info->dr));\n \t      gassign *new_stmt\n \t\t= gimple_build_assign (make_ssa_name (ltype), data_ref);\n \t      new_stmt_info\n@@ -7946,7 +7954,7 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t  *vec_stmt = STMT_VINFO_VEC_STMT (stmt_info);\n \t  return true;\n \t}\n-      first_dr = STMT_VINFO_DATA_REF (first_stmt_info);\n+      first_dr_info = STMT_VINFO_DR_INFO (first_stmt_info);\n       group_gap_adj = 0;\n \n       /* VEC_NUM is the number of vect stmts to be created for this group.  */\n@@ -7980,13 +7988,14 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n   else\n     {\n       first_stmt_info = stmt_info;\n-      first_dr = dr;\n+      first_dr_info = dr_info;\n       group_size = vec_num = 1;\n       group_gap_adj = 0;\n-      ref_type = reference_alias_ptr_type (DR_REF (first_dr));\n+      ref_type = reference_alias_ptr_type (DR_REF (first_dr_info->dr));\n     }\n \n-  alignment_support_scheme = vect_supportable_dr_alignment (first_dr, false);\n+  alignment_support_scheme\n+    = vect_supportable_dr_alignment (first_dr_info, false);\n   gcc_assert (alignment_support_scheme);\n   vec_loop_masks *loop_masks\n     = (loop_vinfo && LOOP_VINFO_FULLY_MASKED_P (loop_vinfo)\n@@ -8105,7 +8114,7 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n      nested within an outer-loop that is being vectorized.  */\n \n   if (nested_in_vect_loop\n-      && !multiple_p (DR_STEP_ALIGNMENT (dr),\n+      && !multiple_p (DR_STEP_ALIGNMENT (dr_info->dr),\n \t\t      GET_MODE_SIZE (TYPE_MODE (vectype))))\n     {\n       gcc_assert (alignment_support_scheme != dr_explicit_realign_optimized);\n@@ -8151,7 +8160,8 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \taggr_type = build_array_type_nelts (elem_type, vec_num * nunits);\n       else\n \taggr_type = vectype;\n-      bump = vect_get_data_ptr_increment (dr, aggr_type, memory_access_type);\n+      bump = vect_get_data_ptr_increment (dr_info, aggr_type,\n+\t\t\t\t\t  memory_access_type);\n     }\n \n   tree vec_mask = NULL_TREE;\n@@ -8166,16 +8176,16 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t  bool simd_lane_access_p\n \t    = STMT_VINFO_SIMD_LANE_ACCESS_P (stmt_info);\n \t  if (simd_lane_access_p\n-\t      && TREE_CODE (DR_BASE_ADDRESS (first_dr)) == ADDR_EXPR\n-\t      && VAR_P (TREE_OPERAND (DR_BASE_ADDRESS (first_dr), 0))\n-\t      && integer_zerop (DR_OFFSET (first_dr))\n-\t      && integer_zerop (DR_INIT (first_dr))\n+\t      && TREE_CODE (DR_BASE_ADDRESS (first_dr_info->dr)) == ADDR_EXPR\n+\t      && VAR_P (TREE_OPERAND (DR_BASE_ADDRESS (first_dr_info->dr), 0))\n+\t      && integer_zerop (DR_OFFSET (first_dr_info->dr))\n+\t      && integer_zerop (DR_INIT (first_dr_info->dr))\n \t      && alias_sets_conflict_p (get_alias_set (aggr_type),\n \t\t\t\t\tget_alias_set (TREE_TYPE (ref_type)))\n \t      && (alignment_support_scheme == dr_aligned\n \t\t  || alignment_support_scheme == dr_unaligned_supported))\n \t    {\n-\t      dataref_ptr = unshare_expr (DR_BASE_ADDRESS (first_dr));\n+\t      dataref_ptr = unshare_expr (DR_BASE_ADDRESS (first_dr_info->dr));\n \t      dataref_offset = build_int_cst (ref_type, 0);\n \t      inv_p = false;\n \t    }\n@@ -8190,10 +8200,11 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t      /* Adjust the pointer by the difference to first_stmt.  */\n \t      data_reference_p ptrdr\n \t\t= STMT_VINFO_DATA_REF (first_stmt_info_for_drptr);\n-\t      tree diff = fold_convert (sizetype,\n-\t\t\t\t\tsize_binop (MINUS_EXPR,\n-\t\t\t\t\t\t    DR_INIT (first_dr),\n-\t\t\t\t\t\t    DR_INIT (ptrdr)));\n+\t      tree diff\n+\t\t= fold_convert (sizetype,\n+\t\t\t\tsize_binop (MINUS_EXPR,\n+\t\t\t\t\t    DR_INIT (first_dr_info->dr),\n+\t\t\t\t\t    DR_INIT (ptrdr)));\n \t      dataref_ptr = bump_vector_ptr (dataref_ptr, ptr_incr, gsi,\n \t\t\t\t\t     stmt_info, diff);\n \t    }\n@@ -8326,19 +8337,20 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t\t\tbreak;\n \t\t      }\n \n-\t\t    align = DR_TARGET_ALIGNMENT (dr);\n+\t\t    align = DR_TARGET_ALIGNMENT (dr_info);\n \t\t    if (alignment_support_scheme == dr_aligned)\n \t\t      {\n-\t\t\tgcc_assert (aligned_access_p (first_dr));\n+\t\t\tgcc_assert (aligned_access_p (first_dr_info));\n \t\t\tmisalign = 0;\n \t\t      }\n-\t\t    else if (DR_MISALIGNMENT (first_dr) == -1)\n+\t\t    else if (DR_MISALIGNMENT (first_dr_info) == -1)\n \t\t      {\n-\t\t\talign = dr_alignment (vect_dr_behavior (first_dr));\n+\t\t\talign = dr_alignment\n+\t\t\t  (vect_dr_behavior (first_dr_info));\n \t\t\tmisalign = 0;\n \t\t      }\n \t\t    else\n-\t\t      misalign = DR_MISALIGNMENT (first_dr);\n+\t\t      misalign = DR_MISALIGNMENT (first_dr_info);\n \t\t    if (dataref_offset == NULL_TREE\n \t\t\t&& TREE_CODE (dataref_ptr) == SSA_NAME)\n \t\t      set_ptr_info_alignment (get_ptr_info (dataref_ptr),\n@@ -8365,7 +8377,7 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t\t\t\t\t : build_int_cst (ref_type, 0));\n \t\t\tif (alignment_support_scheme == dr_aligned)\n \t\t\t  ;\n-\t\t\telse if (DR_MISALIGNMENT (first_dr) == -1)\n+\t\t\telse if (DR_MISALIGNMENT (first_dr_info) == -1)\n \t\t\t  TREE_TYPE (data_ref)\n \t\t\t    = build_aligned_type (TREE_TYPE (data_ref),\n \t\t\t\t\t\t  align * BITS_PER_UNIT);\n@@ -8392,7 +8404,7 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t\t      ptr = copy_ssa_name (dataref_ptr);\n \t\t    else\n \t\t      ptr = make_ssa_name (TREE_TYPE (dataref_ptr));\n-\t\t    unsigned int align = DR_TARGET_ALIGNMENT (first_dr);\n+\t\t    unsigned int align = DR_TARGET_ALIGNMENT (first_dr_info);\n \t\t    new_stmt = gimple_build_assign\n \t\t\t\t (ptr, BIT_AND_EXPR, dataref_ptr,\n \t\t\t\t  build_int_cst\n@@ -8402,7 +8414,7 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t\t    data_ref\n \t\t      = build2 (MEM_REF, vectype, ptr,\n \t\t\t\tbuild_int_cst (ref_type, 0));\n-\t\t    vect_copy_ref_info (data_ref, DR_REF (first_dr));\n+\t\t    vect_copy_ref_info (data_ref, DR_REF (first_dr_info->dr));\n \t\t    vec_dest = vect_create_destination_var (scalar_dest,\n \t\t\t\t\t\t\t    vectype);\n \t\t    new_stmt = gimple_build_assign (vec_dest, data_ref);\n@@ -8436,7 +8448,7 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t\t      new_temp = copy_ssa_name (dataref_ptr);\n \t\t    else\n \t\t      new_temp = make_ssa_name (TREE_TYPE (dataref_ptr));\n-\t\t    unsigned int align = DR_TARGET_ALIGNMENT (first_dr);\n+\t\t    unsigned int align = DR_TARGET_ALIGNMENT (first_dr_info);\n \t\t    new_stmt = gimple_build_assign\n \t\t      (new_temp, BIT_AND_EXPR, dataref_ptr,\n \t\t       build_int_cst (TREE_TYPE (dataref_ptr),\n@@ -8454,7 +8466,7 @@ vectorizable_load (stmt_vec_info stmt_info, gimple_stmt_iterator *gsi,\n \t      /* DATA_REF is null if we've already built the statement.  */\n \t      if (data_ref)\n \t\t{\n-\t\t  vect_copy_ref_info (data_ref, DR_REF (first_dr));\n+\t\t  vect_copy_ref_info (data_ref, DR_REF (first_dr_info->dr));\n \t\t  new_stmt = gimple_build_assign (vec_dest, data_ref);\n \t\t}\n \t      new_temp = make_ssa_name (vec_dest, new_stmt);"}, {"sha": "d45c573db38bf994bb1e0e0f1f20e4367cc507bf", "filename": "gcc/tree-vectorizer.h", "status": "modified", "additions": 30, "deletions": 31, "changes": 61, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/89fa689a9e898ccb81b966477b3ac4e254461b05/gcc%2Ftree-vectorizer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/89fa689a9e898ccb81b966477b3ac4e254461b05/gcc%2Ftree-vectorizer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.h?ref=89fa689a9e898ccb81b966477b3ac4e254461b05", "patch": "@@ -1294,15 +1294,15 @@ vect_dr_stmt (data_reference *dr)\n #define DR_MISALIGNMENT_UNINITIALIZED (-2)\n \n inline void\n-set_dr_misalignment (struct data_reference *dr, int val)\n+set_dr_misalignment (dr_vec_info *dr_info, int val)\n {\n-  DR_VECT_AUX (dr)->misalignment = val;\n+  dr_info->misalignment = val;\n }\n \n inline int\n-dr_misalignment (struct data_reference *dr)\n+dr_misalignment (dr_vec_info *dr_info)\n {\n-  int misalign = DR_VECT_AUX (dr)->misalignment;\n+  int misalign = dr_info->misalignment;\n   gcc_assert (misalign != DR_MISALIGNMENT_UNINITIALIZED);\n   return misalign;\n }\n@@ -1313,52 +1313,51 @@ dr_misalignment (struct data_reference *dr)\n #define SET_DR_MISALIGNMENT(DR, VAL) set_dr_misalignment (DR, VAL)\n \n /* Only defined once DR_MISALIGNMENT is defined.  */\n-#define DR_TARGET_ALIGNMENT(DR) DR_VECT_AUX (DR)->target_alignment\n+#define DR_TARGET_ALIGNMENT(DR) ((DR)->target_alignment)\n \n-/* Return true if data access DR is aligned to its target alignment\n+/* Return true if data access DR_INFO is aligned to its target alignment\n    (which may be less than a full vector).  */\n \n static inline bool\n-aligned_access_p (struct data_reference *data_ref_info)\n+aligned_access_p (dr_vec_info *dr_info)\n {\n-  return (DR_MISALIGNMENT (data_ref_info) == 0);\n+  return (DR_MISALIGNMENT (dr_info) == 0);\n }\n \n /* Return TRUE if the alignment of the data access is known, and FALSE\n    otherwise.  */\n \n static inline bool\n-known_alignment_for_access_p (struct data_reference *data_ref_info)\n+known_alignment_for_access_p (dr_vec_info *dr_info)\n {\n-  return (DR_MISALIGNMENT (data_ref_info) != DR_MISALIGNMENT_UNKNOWN);\n+  return (DR_MISALIGNMENT (dr_info) != DR_MISALIGNMENT_UNKNOWN);\n }\n \n /* Return the minimum alignment in bytes that the vectorized version\n-   of DR is guaranteed to have.  */\n+   of DR_INFO is guaranteed to have.  */\n \n static inline unsigned int\n-vect_known_alignment_in_bytes (struct data_reference *dr)\n+vect_known_alignment_in_bytes (dr_vec_info *dr_info)\n {\n-  if (DR_MISALIGNMENT (dr) == DR_MISALIGNMENT_UNKNOWN)\n-    return TYPE_ALIGN_UNIT (TREE_TYPE (DR_REF (dr)));\n-  if (DR_MISALIGNMENT (dr) == 0)\n-    return DR_TARGET_ALIGNMENT (dr);\n-  return DR_MISALIGNMENT (dr) & -DR_MISALIGNMENT (dr);\n+  if (DR_MISALIGNMENT (dr_info) == DR_MISALIGNMENT_UNKNOWN)\n+    return TYPE_ALIGN_UNIT (TREE_TYPE (DR_REF (dr_info->dr)));\n+  if (DR_MISALIGNMENT (dr_info) == 0)\n+    return DR_TARGET_ALIGNMENT (dr_info);\n+  return DR_MISALIGNMENT (dr_info) & -DR_MISALIGNMENT (dr_info);\n }\n \n-/* Return the behavior of DR with respect to the vectorization context\n+/* Return the behavior of DR_INFO with respect to the vectorization context\n    (which for outer loop vectorization might not be the behavior recorded\n-   in DR itself).  */\n+   in DR_INFO itself).  */\n \n static inline innermost_loop_behavior *\n-vect_dr_behavior (data_reference *dr)\n+vect_dr_behavior (dr_vec_info *dr_info)\n {\n-  gimple *stmt = DR_STMT (dr);\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  stmt_vec_info stmt_info = dr_info->stmt;\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   if (loop_vinfo == NULL\n       || !nested_in_vect_loop_p (LOOP_VINFO_LOOP (loop_vinfo), stmt_info))\n-    return &DR_INNERMOST (dr);\n+    return &DR_INNERMOST (dr_info->dr);\n   else\n     return &STMT_VINFO_DR_WRT_VEC_LOOP (stmt_info);\n }\n@@ -1451,17 +1450,17 @@ vect_max_vf (loop_vec_info loop_vinfo)\n   return MAX_VECTORIZATION_FACTOR;\n }\n \n-/* Return the size of the value accessed by unvectorized data reference DR.\n-   This is only valid once STMT_VINFO_VECTYPE has been calculated for the\n-   associated gimple statement, since that guarantees that DR accesses\n-   either a scalar or a scalar equivalent.  (\"Scalar equivalent\" here\n-   includes things like V1SI, which can be vectorized in the same way\n+/* Return the size of the value accessed by unvectorized data reference\n+   DR_INFO.  This is only valid once STMT_VINFO_VECTYPE has been calculated\n+   for the associated gimple statement, since that guarantees that DR_INFO\n+   accesses either a scalar or a scalar equivalent.  (\"Scalar equivalent\"\n+   here includes things like V1SI, which can be vectorized in the same way\n    as a plain SI.)  */\n \n inline unsigned int\n-vect_get_scalar_dr_size (struct data_reference *dr)\n+vect_get_scalar_dr_size (dr_vec_info *dr_info)\n {\n-  return tree_to_uhwi (TYPE_SIZE_UNIT (TREE_TYPE (DR_REF (dr))));\n+  return tree_to_uhwi (TYPE_SIZE_UNIT (TREE_TYPE (DR_REF (dr_info->dr))));\n }\n \n /* Source location + hotness information. */\n@@ -1561,7 +1560,7 @@ extern tree vect_get_mask_type_for_stmt (stmt_vec_info);\n /* In tree-vect-data-refs.c.  */\n extern bool vect_can_force_dr_alignment_p (const_tree, unsigned int);\n extern enum dr_alignment_support vect_supportable_dr_alignment\n-                                           (struct data_reference *, bool);\n+                                           (dr_vec_info *, bool);\n extern tree vect_get_smallest_scalar_type (stmt_vec_info, HOST_WIDE_INT *,\n                                            HOST_WIDE_INT *);\n extern bool vect_analyze_data_ref_dependences (loop_vec_info, unsigned int *);"}]}