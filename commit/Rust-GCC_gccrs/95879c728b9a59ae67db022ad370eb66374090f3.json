{"sha": "95879c728b9a59ae67db022ad370eb66374090f3", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OTU4NzljNzI4YjlhNTlhZTY3ZGIwMjJhZDM3MGViNjYzNzQwOTBmMw==", "commit": {"author": {"name": "H.J. Lu", "email": "hjl@gcc.gnu.org", "date": "2008-08-28T19:18:44Z"}, "committer": {"name": "H.J. Lu", "email": "hjl@gcc.gnu.org", "date": "2008-08-28T19:18:44Z"}, "message": "[multiple changes]\n\n2008-08-28  H.J. Lu  <hongjiu.lu@intel.com>\n\t    Joey Ye  <joey.ye@intel.com>\n\t    Xuepeng Guo  <xuepeng.guo@intel.com>\n\n\t* config.gcc (extra_headers): Add gmmintrin.h for x86 and x86-64.\n\n\t* config/i386/cpuid.h (bit_FMA): New.\n\t(bit_XSAVE): Likewise.\n\t(bit_OSXSAVE): Likewise.\n\t(bit_AVX): Likewise.\n\n\t* config/i386/gas.h (ASM_OUTPUT_OPCODE): Undefine before\n\tdefine.  Use ASM_OUTPUT_AVX_PREFIX.\n\n\t* config/i386/gmmintrin.h: New.\n\n\t* config/i386/i386.c (x86_64_reg_class): Add X86_64_AVX_CLASS.\n\t(OPTION_MASK_ISA_AVX_SET): New.\n\t(OPTION_MASK_ISA_FMA_SET): Likewise.\n\t(OPTION_MASK_ISA_AVX_UNSET): Likewise.\n\t(OPTION_MASK_ISA_FMA_SET): Likewise.\n\t(OPTION_MASK_ISA_SSE4_2_UNSET): Updated.\n\t(ix86_handle_option): Handle OPT_mavx and OPT_mfma.\n\t(pta_flags): Add PTA_AVX and PTA_FMA.\n\t(override_options): Handle PTA_AVX and PTA_FMA.\n\t(init_cumulative_args): Handle warn_avx.\n\t(classify_argument): Return 0 for COImode and OImode.  Return\n\t1 and X86_64_AVX_CLASS for 256bit vector types.\n\t(examine_argument): Handle X86_64_AVX_CLASS.\n\t(construct_container): Likewise.\n\t(function_arg_advance_32): Pass OImode and 256bit vector types\n\tin AVX register.\n\t(function_arg_advance_64): Take a new argument to indicate if a\n\tparameter is named.  Handle 256bit vector types.  Return\n\timmediately for unnamed 256bit vector mode parameters.\n\t(function_arg_advance): Updated.\n\t(function_arg_32): Add comments for TImode.  Handle OImode\n\tand 256bit vector types.\n\t(function_arg_64): Take a new argument to indicate if a\n\tparameter is named.  Handle 256bit vector types.  Return NULL\n\tfor unnamed 256bit vector mode parameters.\n\t(function_arg): Updated.\n\t(setup_incoming_varargs_64): Support\n\tAVX encoding for *sse_prologue_save_insn.\n\t(ix86_gimplify_va_arg): Handle 256bit vector mode parameters.\n\t(standard_sse_constant_p): Return -2 for all 1s if SSE2 isn't\n\tenabled.  For all 1s in 256bit vector modes, return 3 if AVX is\n\tenabled, otherwise return -3.\n\t(standard_sse_constant_opcode): Handle AVX and 256bit vector\n\tmodes.\n\t(print_reg): Support AVX registers.  Handle 'x' and 't'.\n\tHandle 'd' to duplicate the operand.\n\t(print_operand): Likewise.  Also support AVX vector compare\n\tinstructions.\n\t(output_387_binary_op): Support AVX.\n\t(output_fp_compare): Likewise.\n\t(ix86_expand_vector_move_misalign): Likewise.\n\t(ix86_attr_length_vex_default): New.\n\t(ix86_builtins): Add IX86_BUILTIN_ADDPD256,\n\tIX86_BUILTIN_ADDPS256, IX86_BUILTIN_ADDSUBPD256,\n\tIX86_BUILTIN_ADDSUBPS256, IX86_BUILTIN_ANDPD256,\n\tIX86_BUILTIN_ANDPS256, IX86_BUILTIN_ANDNPD256,\n\tIX86_BUILTIN_ANDNPS256, IX86_BUILTIN_BLENDPD256,\n\tIX86_BUILTIN_BLENDPS256, IX86_BUILTIN_BLENDVPD256,\n\tIX86_BUILTIN_BLENDVPS256, IX86_BUILTIN_DIVPD256,\n\tIX86_BUILTIN_DIVPS256, IX86_BUILTIN_DPPS256,\n\tIX86_BUILTIN_HADDPD256, IX86_BUILTIN_HADDPS256,\n\tIX86_BUILTIN_HSUBPD256, IX86_BUILTIN_HSUBPS256,\n\tIX86_BUILTIN_MAXPD256, IX86_BUILTIN_MAXPS256,\n\tIX86_BUILTIN_MINPD256, IX86_BUILTIN_MINPS256,\n\tIX86_BUILTIN_MULPD256, IX86_BUILTIN_MULPS256,\n\tIX86_BUILTIN_ORPD256, IX86_BUILTIN_ORPS256,\n\tIX86_BUILTIN_SHUFPD256, IX86_BUILTIN_SHUFPS256,\n\tIX86_BUILTIN_SUBPD256, IX86_BUILTIN_SUBPS256,\n\tIX86_BUILTIN_XORPD256, IX86_BUILTIN_XORPS256,\n\tIX86_BUILTIN_CMPSD, IX86_BUILTIN_CMPSS, IX86_BUILTIN_CMPPD,\n\tIX86_BUILTIN_CMPPS, IX86_BUILTIN_CMPPD256,\n\tIX86_BUILTIN_CMPPS256, IX86_BUILTIN_CVTDQ2PD256,\n\tIX86_BUILTIN_CVTDQ2PS256, IX86_BUILTIN_CVTPD2PS256,\n\tIX86_BUILTIN_CVTPS2DQ256, IX86_BUILTIN_CVTPS2PD256,\n\tIX86_BUILTIN_CVTTPD2DQ256, IX86_BUILTIN_CVTPD2DQ256,\n\tIX86_BUILTIN_CVTTPS2DQ256, IX86_BUILTIN_EXTRACTF128PD256,\n\tIX86_BUILTIN_EXTRACTF128PS256, IX86_BUILTIN_EXTRACTF128SI256,\n\tIX86_BUILTIN_VZEROALL, IX86_BUILTIN_VZEROUPPER,\n\tIX86_BUILTIN_VZEROUPPER_REX64, IX86_BUILTIN_VPERMILVARPD,\n\tIX86_BUILTIN_VPERMILVARPS, IX86_BUILTIN_VPERMILVARPD256,\n\tIX86_BUILTIN_VPERMILVARPS256, IX86_BUILTIN_VPERMILPD,\n\tIX86_BUILTIN_VPERMILPS, IX86_BUILTIN_VPERMILPD256,\n\tIX86_BUILTIN_VPERMILPS256, IX86_BUILTIN_VPERMIL2PD,\n\tIX86_BUILTIN_VPERMIL2PS, IX86_BUILTIN_VPERMIL2PD256,\n\tIX86_BUILTIN_VPERMIL2PS256, IX86_BUILTIN_VPERM2F128PD256,\n\tIX86_BUILTIN_VPERM2F128PS256, IX86_BUILTIN_VPERM2F128SI256,\n\tIX86_BUILTIN_VBROADCASTSS, IX86_BUILTIN_VBROADCASTSD256,\n\tIX86_BUILTIN_VBROADCASTSS256, IX86_BUILTIN_VBROADCASTPD256,\n\tIX86_BUILTIN_VBROADCASTPS256, IX86_BUILTIN_VINSERTF128PD256,\n\tIX86_BUILTIN_VINSERTF128PS256, IX86_BUILTIN_VINSERTF128SI256,\n\tIX86_BUILTIN_LOADUPD256, IX86_BUILTIN_LOADUPS256,\n\tIX86_BUILTIN_STOREUPD256, IX86_BUILTIN_STOREUPS256,\n\tIX86_BUILTIN_LDDQU256, IX86_BUILTIN_LOADDQU256,\n\tIX86_BUILTIN_STOREDQU256, IX86_BUILTIN_MASKLOADPD,\n\tIX86_BUILTIN_MASKLOADPS, IX86_BUILTIN_MASKSTOREPD,\n\tIX86_BUILTIN_MASKSTOREPS, IX86_BUILTIN_MASKLOADPD256,\n\tIX86_BUILTIN_MASKLOADPS256, IX86_BUILTIN_MASKSTOREPD256,\n\tIX86_BUILTIN_MASKSTOREPS256, IX86_BUILTIN_MOVSHDUP256,\n\tIX86_BUILTIN_MOVSLDUP256, IX86_BUILTIN_MOVDDUP256,\n\tIX86_BUILTIN_SQRTPD256, IX86_BUILTIN_SQRTPS256,\n\tIX86_BUILTIN_SQRTPS_NR256, IX86_BUILTIN_RSQRTPS256,\n\tIX86_BUILTIN_RSQRTPS_NR256, IX86_BUILTIN_RCPPS256,\n\tIX86_BUILTIN_ROUNDPD256, IX86_BUILTIN_ROUNDPS256,\n\tIX86_BUILTIN_UNPCKHPD256, IX86_BUILTIN_UNPCKLPD256,\n\tIX86_BUILTIN_UNPCKHPS256, IX86_BUILTIN_UNPCKLPS256,\n\tIX86_BUILTIN_SI256_SI, IX86_BUILTIN_PS256_PS,\n\tIX86_BUILTIN_PD256_PD, IX86_BUILTIN_SI_SI256,\n\tIX86_BUILTIN_PS_PS256, IX86_BUILTIN_PD_PD256,\n\tIX86_BUILTIN_VTESTZPD, IX86_BUILTIN_VTESTCPD,\n\tIX86_BUILTIN_VTESTNZCPD, IX86_BUILTIN_VTESTZPS,\n\tIX86_BUILTIN_VTESTCPS, IX86_BUILTIN_VTESTNZCPS,\n\tIX86_BUILTIN_VTESTZPD256, IX86_BUILTIN_VTESTCPD256,\n\tIX86_BUILTIN_VTESTNZCPD256, IX86_BUILTIN_VTESTZPS256,\n\tIX86_BUILTIN_VTESTCPS256, IX86_BUILTIN_VTESTNZCPS256,\n\tIX86_BUILTIN_PTESTZ256, IX86_BUILTIN_PTESTC256,\n\tIX86_BUILTIN_PTESTNZC256, IX86_BUILTIN_MOVMSKPD256\n\tand IX86_BUILTIN_MOVMSKPS256,\n\t(ix86_special_builtin_type): Add V32QI_FTYPE_PCCHAR,\n\tV8SF_FTYPE_PCV4SF, V8SF_FTYPE_PCFLOAT, V4DF_FTYPE_PCV2DF,\n\tV4DF_FTYPE_PCDOUBLE, V8SF_FTYPE_PCV8SF_V8SF,\n\tV4DF_FTYPE_PCV4DF_V4DF, V4SF_FTYPE_PCV4SF_V4SF,\n\tV2DF_FTYPE_PCV2DF_V2DF, VOID_FTYPE_PCHAR_V32QI,\n\tVOID_FTYPE_PFLOAT_V8SF, VOID_FTYPE_PDOUBLE_V4DF,\n\tVOID_FTYPE_PV8SF_V8SF_V8SF, VOID_FTYPE_PV4DF_V4DF_V4DF,\n\tVOID_FTYPE_PV4SF_V4SF_V4SF and VOID_FTYPE_PV2DF_V2DF_V2DF,\n\t(ix86_builtin_type): Add INT_FTYPE_V8SF_V8SF_PTEST,\n\tINT_FTYPE_V4DI_V4DI_PTEST, INT_FTYPE_V4DF_V4DF_PTEST,\n\tINT_FTYPE_V4SF_V4SF_PTEST, INT_FTYPE_V2DF_V2DF_PTEST,\n\tINT_FTYPE_V8SF, INT_FTYPE_V4DF, V8SI_FTYPE_V8SF, V8SI_FTYPE_V4SI,\n\tV8SF_FTYPE_V8SF, V8SF_FTYPE_V8SI, V8SF_FTYPE_V4SF,\n\tV4SI_FTYPE_V8SI, V4SI_FTYPE_V4DF, V4DF_FTYPE_V4DF,\n\tV4DF_FTYPE_V4SI, V4DF_FTYPE_V4SF, V4DF_FTYPE_V2DF,\n\tV4SF_FTYPE_V4DF, V4SF_FTYPE_V8SF, V2DF_FTYPE_V4DF,\n\tV8SF_FTYPE_V8SF_V8SF, V8SF_FTYPE_V8SF_V8SI,\n\tV4DF_FTYPE_V4DF_V4DF, V4DF_FTYPE_V4DF_V4DI,\n\tV4SF_FTYPE_V4SF_V4SI, V2DF_FTYPE_V2DF_V2DI,\n\tV8SF_FTYPE_V8SF_INT, V4SI_FTYPE_V8SI_INT, V4SF_FTYPE_V8SF_INT,\n\tV2DF_FTYPE_V4DF_INT, V4DF_FTYPE_V4DF_INT,\n\tV8SF_FTYPE_V8SF_V8SF_V8SF, V4DF_FTYPE_V4DF_V4DF_V4DF,\n\tV8SI_FTYPE_V8SI_V8SI_INT, V8SF_FTYPE_V8SF_V8SF_INT,\n\tV4DF_FTYPE_V4DF_V4DF_INT, V4DF_FTYPE_V4DF_V2DF_INT,\n\tV8SF_FTYPE_V8SF_V8SF_V8SI_INT, V4DF_FTYPE_V4DF_V4DF_V4DI_INT,\n\tV4SF_FTYPE_V4SF_V4SF_V4SI_INT and V2DF_FTYPE_V2DF_V2DF_V2DI_INT.\n\t(bdesc_special_args): Add IX86_BUILTIN_VZEROALL,\n\tIX86_BUILTIN_VZEROUPPER. IX86_BUILTIN_VZEROUPPER_REX64,\n\tIX86_BUILTIN_VBROADCASTSS, IX86_BUILTIN_VBROADCASTSD256,\n\tIX86_BUILTIN_VBROADCASTSS256, IX86_BUILTIN_VBROADCASTPD256,\n\tIX86_BUILTIN_VBROADCASTPS256, IX86_BUILTIN_LOADUPD256,\n\tIX86_BUILTIN_LOADUPS256, IX86_BUILTIN_STOREUPD256,\n\tIX86_BUILTIN_STOREUPS256, IX86_BUILTIN_LOADDQU256,\n\tIX86_BUILTIN_STOREDQU256, IX86_BUILTIN_LDDQU256,\n\tIX86_BUILTIN_MASKLOADPD, IX86_BUILTIN_MASKLOADPS,\n\tIX86_BUILTIN_MASKLOADPD256, IX86_BUILTIN_MASKLOADPS256,\n\tIX86_BUILTIN_MASKSTOREPD, IX86_BUILTIN_MASKSTOREPS,\n\tIX86_BUILTIN_MASKSTOREPD256 and IX86_BUILTIN_MASKSTOREPS256.\n\t(ix86_builtins): Add IX86_BUILTIN_ADDPD256,\n\tIX86_BUILTIN_ADDPS256, IX86_BUILTIN_ADDSUBPD256,\n\tIX86_BUILTIN_ADDSUBPS256, IX86_BUILTIN_ANDPD256,\n\tIX86_BUILTIN_ANDPS256, IX86_BUILTIN_ANDNPD256,\n\tIX86_BUILTIN_ANDNPS256, IX86_BUILTIN_DIVPD256,\n\tIX86_BUILTIN_DIVPS256, IX86_BUILTIN_HADDPD256,\n\tIX86_BUILTIN_HSUBPS256, IX86_BUILTIN_HSUBPD256,\n\tIX86_BUILTIN_HADDPS256, IX86_BUILTIN_MAXPD256,\n\tIX86_BUILTIN_MAXPS256, IX86_BUILTIN_MINPD256,\n\tIX86_BUILTIN_MINPS256, IX86_BUILTIN_MULPD256,\n\tIX86_BUILTIN_MULPS256, IX86_BUILTIN_ORPD256,\n\tIX86_BUILTIN_ORPS256, IX86_BUILTIN_SUBPD256,\n\tIX86_BUILTIN_SUBPS256, IX86_BUILTIN_XORPD256,\n\tIX86_BUILTIN_XORPS256, IX86_BUILTIN_VPERMILVARPD,\n\tIX86_BUILTIN_VPERMILVARPS, IX86_BUILTIN_VPERMILVARPD256,\n\tIX86_BUILTIN_VPERMILVARPS256, IX86_BUILTIN_BLENDPD256,\n\tIX86_BUILTIN_BLENDPS256, IX86_BUILTIN_BLENDVPD256,\n\tIX86_BUILTIN_BLENDVPS256, IX86_BUILTIN_DPPS256,\n\tIX86_BUILTIN_SHUFPD256, IX86_BUILTIN_SHUFPS256,\n\tIX86_BUILTIN_CMPSD, IX86_BUILTIN_CMPSS, IX86_BUILTIN_CMPPD,\n\tIX86_BUILTIN_CMPPS,\n\tIX86_BUILTIN_CMPPD256,IX86_BUILTIN_CMPPS256,\n\tIX86_BUILTIN_EXTRACTF128PD256, IX86_BUILTIN_EXTRACTF128PS256,\n\tIX86_BUILTIN_EXTRACTF128SI256, IX86_BUILTIN_CVTDQ2PD256,\n\tIX86_BUILTIN_CVTDQ2PS256, IX86_BUILTIN_CVTPD2PS256,\n\tIX86_BUILTIN_CVTPS2DQ256, IX86_BUILTIN_CVTPS2PD256,\n\tIX86_BUILTIN_CVTTPD2DQ256, IX86_BUILTIN_CVTPD2DQ256,\n\tIX86_BUILTIN_CVTTPS2DQ256, IX86_BUILTIN_VPERM2F128PD256,\n\tIX86_BUILTIN_VPERM2F128PS256, IX86_BUILTIN_VPERM2F128SI256,\n\tIX86_BUILTIN_VPERMILPD, IX86_BUILTIN_VPERMILPS,\n\tIX86_BUILTIN_VPERMILPD256, IX86_BUILTIN_VPERMILPS256,\n\tIX86_BUILTIN_VPERMIL2PD, IX86_BUILTIN_VPERMILPS,\n\tIX86_BUILTIN_VPERMILPD256, IX86_BUILTIN_VPERMILPS256,\n\tIX86_BUILTIN_VPERMIL2PD, IX86_BUILTIN_VPERMIL2PS,\n\tIX86_BUILTIN_VPERMIL2PD256, IX86_BUILTIN_VPERMIL2PS256,\n\tIX86_BUILTIN_VINSERTF128PD256, IX86_BUILTIN_VINSERTF128PS256,\n\tIX86_BUILTIN_VINSERTF128SI256, IX86_BUILTIN_MOVSHDUP256,\n\tIX86_BUILTIN_MOVSLDUP256, IX86_BUILTIN_MOVDDUP256,\n\tIX86_BUILTIN_SQRTPD256, IX86_BUILTIN_SQRTPS256,\n\tIX86_BUILTIN_SQRTPS_NR256, IX86_BUILTIN_RSQRTPS256,\n\tIX86_BUILTIN_RSQRTPS_NR256, IX86_BUILTIN_RCPPS256,\n\tIX86_BUILTIN_ROUNDPD256, IX86_BUILTIN_ROUNDPS256,\n\tIX86_BUILTIN_UNPCKHPD256, IX86_BUILTIN_UNPCKLPD256,\n\tIX86_BUILTIN_UNPCKHPS256, IX86_BUILTIN_UNPCKLPS256,\n\tIX86_BUILTIN_SI256_SI, IX86_BUILTIN_PS256_PS,\n\tIX86_BUILTIN_PD256_PD, IX86_BUILTIN_SI_SI256,\n\tIX86_BUILTIN_PS_PS256, IX86_BUILTIN_PD_PD256,\n\tIX86_BUILTIN_VTESTZPD, IX86_BUILTIN_VTESTCPD,\n\tIX86_BUILTIN_VTESTNZCPD, IX86_BUILTIN_VTESTZPS,\n\tIX86_BUILTIN_VTESTCPS, IX86_BUILTIN_VTESTNZCPS,\n\tIX86_BUILTIN_VTESTZPD256, IX86_BUILTIN_VTESTCPD256,\n\tIX86_BUILTIN_VTESTNZCPD256, IX86_BUILTIN_VTESTZPS256,\n\tIX86_BUILTIN_VTESTCPS256, IX86_BUILTIN_VTESTNZCPS256,\n\tIX86_BUILTIN_PTESTZ256, IX86_BUILTIN_PTESTC256,\n\tIX86_BUILTIN_PTESTNZC256, IX86_BUILTIN_MOVMSKPD256 and\n\tIX86_BUILTIN_MOVMSKPS256.\n\t(ix86_init_mmx_sse_builtins): Support AVX builtins.\n\t(ix86_expand_args_builtin): Likewise.\n\t(ix86_expand_special_args_builtin): Likewise.\n\t(ix86_hard_regno_mode_ok): Handle AVX modes.\n\t(ix86_expand_vector_init_duplicate): Likewise.\n\t(ix86_expand_vector_init_one_nonzero): Likewise.\n\t(ix86_expand_vector_init_one_var): Likewise.\n\t(ix86_expand_vector_init_concat): Likewise.\n\t(ix86_expand_vector_init_general): Likewise.\n\t(ix86_expand_vector_set): Likewise.\n\t(ix86_vector_mode_supported_p): Likewise.\n\t(x86_extended_reg_mentioned_p): Check INSN_P before using\n\tPATTERN.\n\n\t* config/i386/i386-c.c (ix86_target_macros_internal): Handle\n\tOPTION_MASK_ISA_AVX and OPTION_MASK_ISA_FMA.\n\n\t* config/i386/i386.h (TARGET_AVX): New.\n\t(TARGET_FMA): Likewise.\n\t(TARGET_CPU_CPP_BUILTINS): Handle TARGET_AVX and TARGET_FMA.\n\t(BIGGEST_ALIGNMENT): Set to 256 for TARGET_AVX.\n\t(VALID_AVX256_REG_MODE): New.\n\t(AVX256_VEC_FLOAT_MODE_P): Likewise.\n\t(AVX_FLOAT_MODE_P): Likewise.\n\t(AVX128_VEC_FLOAT_MODE_P): Likewise.\n\t(AVX256_VEC_FLOAT_MODE_P): Likewise.\n\t(AVX_VEC_FLOAT_MODE_P): Likewise.\n\t(ASM_OUTPUT_AVX_PREFIX): Likewise.\n\t(ASM_OUTPUT_OPCODE): Likewise.\n\t(UNITS_PER_SIMD_WORD): Add a FIXME for 32byte vectorizer\n\tsupport.\n\t(SSE_REG_MODE_P): Allow 256bit vector modes.\n\t(ix86_args): Add a warn_avx field.\n\n\t* config/i386/i386.md (UNSPEC_PCMP): New.\n\t(UNSPEC_VPERMIL): Likewise.\n\t(UNSPEC_VPERMIL2): Likewise.\n\t(UNSPEC_VPERMIL2F128): Likewise.\n\t(UNSPEC_MASKLOAD): Likewise.\n\t(UNSPEC_MASKSTORE): Likewise.\n\t(UNSPEC_CAST): Likewise.\n\t(UNSPEC_VTESTP): Likewise.\n\t(UNSPECV_VZEROALL): Likewise.\n\t(UNSPECV_VZEROUPPER): Likewise.\n\t(XMM0_REG): Likewise.\n\t(XMM1_REG): Likewise.\n\t(XMM2_REG): Likewise.\n\t(XMM3_REG): Likewise.\n\t(XMM4_REG): Likewise.\n\t(XMM5_REG): Likewise.\n\t(XMM6_REG): Likewise.\n\t(XMM8_REG): Likewise.\n\t(XMM9_REG): Likewise.\n\t(XMM10_REG): Likewise.\n\t(XMM11_REG): Likewise.\n\t(XMM12_REG): Likewise.\n\t(XMM13_REG): Likewise.\n\t(XMM14_REG): Likewise.\n\t(XMM15_REG): Likewise.\n\t(prefix): Likewise.\n\t(prefix_vex_imm8): Likewise.\n\t(prefix_vex_w): Likewise.\n\t(length_vex): Likewise.\n\t(maxmin): Likewise.\n\t(movoi): Likewise.\n\t(*avx_ashlti3): Likewise.\n\t(*avx_lshrti3): Likewise.\n\t(*avx_setcc<mode>): Likewise.\n\t(*fop_<mode>_comm_mixed_avx): Likewise.\n\t(*fop_<mode>_comm_avx): Likewise.\n\t(*fop_<mode>_1_mixed_avx): Likewise.\n\t(*fop_<mode>_1_avx): Likewise.\n\t(*avx_<code><mode>3): Likewise.\n\t(*avx_ieee_smin<mode>3): Likewise.\n\t(*avx_ieee_smax<mode>3): Likewise.\n\t(mode): Add OI, V8SF and V4DF.\n\t(length): Support VEX prefix.\n\t(*cmpfp_i_mixed): Set prefix attribute.\n\t(*cmpfp_i_sse): Likewise.\n\t(*cmpfp_iu_mixed): Likewise.\n\t(*cmpfp_iu_sse): Likewise.\n\t(*movsi_1): Support AVX.\n\t(*movdi_2): Likewise.\n\t(*movdi_1_rex64): Likewise.\n\t(*movti_internal): Likewise.\n\t(*movti_rex64): Likewise.\n\t(*movsf_1): Likewise.\n\t(*movdf_nointeger): Likewise.\n\t(*movdf_integer_rex64): Likewise.\n\t(*movtf_internal): Likewise.\n\t(zero_extendsidi2_32): Likewise.\n\t(zero_extendsidi2_rex64): Likewise.\n\t(*extendsfdf2_mixed): Likewise.\n\t(*extendsfdf2_sse): Likewise.\n\t(*truncdfsf_fast_mixed): Likewise.\n\t(*truncdfsf_fast_sse): Likewise.\n\t(*truncdfsf_mixed): Likewise.\n\t(fix_trunc<mode>di_sse): Likewise.\n\t(fix_trunc<mode>si_sse): Likewise.\n\t(*float<SSEMODEI24:mode><MODEF:mode>2_mixed_interunit): Likewise.\n\t(*float<SSEMODEI24:mode><MODEF:mode>2_mixed_nointerunit): Likewise.\n\t(*float<SSEMODEI24:mode><MODEF:mode>2_sse_interunit): Likewise.\n\t(*float<SSEMODEI24:mode><MODEF:mode>2_sse_nointerunit): Likewise.\n\t(*rcpsf2_sse): Likewise.\n\t(*rsqrtsf2_sse): Likewise.\n\t(*sqrt<mode>2_sse): Likewise.\n\t(sse4_1_round<mode>2): Likewise.\n\t(*sse_prologue_save_insn): Disallow REX prefix for AVX.\n\tSupport AVX.  Set length attribute properly for AVX.\n\n\t* config/i386/i386-modes.def (VECTOR_MODES (INT, 32)): New.\n\t(VECTOR_MODES (FLOAT, 32)): Likewise.\n\t(VECTOR_MODE (INT, DI, 8)): Likewise.\n\t(VECTOR_MODE (INT, HI, 32)): Likewise.\n\t(VECTOR_MODE (INT, QI, 64)): Likewise.\n\t(VECTOR_MODE (FLOAT, DF, 8)): Likewise.\n\t(VECTOR_MODE (FLOAT, SF, 16)): Likewise.\n\t(VECTOR_MODE (INT, DI, 4)): Removed.\n\t(VECTOR_MODE (INT, SI, 8)): Likewise.\n\t(VECTOR_MODE (INT, HI, 16)): Likewise.\n\t(VECTOR_MODE (INT, QI, 32)): Likewise.\n\t(VECTOR_MODE (FLOAT, SF, 8)): Likewise.\n\t(INT_MODE (OI, 32)): Likewise.\n\n\t* config/i386/i386.opt (mavx): New.\n\t(mfma): Likewise.\n\n\t* config/i386/i386-protos.h (ix86_attr_length_vex_default): New.\n\n\t* config/i386/mmx.md (*mov<mode>_internal_rex64): Support AVX.\n\t(*mov<mode>_internal_avx): New.\n\t(*movv2sf_internal_rex64_avx): Likewise.\n\t(*movv2sf_internal_avx): Likewise.\n\n\t* config/i386/predicates.md (const_4_to_5_operand): New.\n\t(const_6_to_7_operand): Likewise.\n\t(const_8_to_11_operand): Likewise.\n\t(const_12_to_15_operand): Likewise.\n\t(avx_comparison_float_operator): Likewise.\n\n\t* config/i386/sse.md (AVX256MODEI): New.\n\t(AVX256MODE): Likewise.\n\t(AVXMODEQI): Likewise.\n\t(AVXMODE): Likewise.\n\t(AVX256MODEF2P): Likewise.\n\t(AVX256MODE2P): Likewise.\n\t(AVX256MODE4P): Likewise.\n\t(AVX256MODE8P): Likewise.\n\t(AVXMODEF2P): Likewise.\n\t(AVXMODEF4P): Likewise.\n\t(AVXMODEDCVTDQ2PS): Likewise.\n\t(AVXMODEDCVTPS2DQ): Likewise.\n\t(avxvecmode): Likewise.\n\t(avxvecpsmode): Likewise.\n\t(avxhalfvecmode): Likewise.\n\t(avxscalarmode): Likewise.\n\t(avxcvtvecmode): Likewise.\n\t(avxpermvecmode): Likewise.\n\t(avxmodesuffixf2c): Likewise.\n\t(avxmodesuffixp): Likewise.\n\t(avxmodesuffixs): Likewise.\n\t(avxmodesuffix): Likewise.\n\t(vpermilbits): Likewise.\n\t(pinsrbits): Likewise.\n\t(mov<mode>): Likewise.\n\t(*mov<mode>_internal): Likewise.\n\t(push<mode>1): Likewise.\n\t(movmisalign<mode>): Likewise.\n\t(avx_movup<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n\t(avx_movdqu<avxmodesuffix>): Likewise.\n\t(avx_lddqu<avxmodesuffix>): Likewise.\n\t(<plusminus_insn><mode>3): Likewise.\n\t(*avx_<plusminus_insn><mode>3): Likewise.\n\t(*avx_vm<plusminus_insn><mode>3): Likewise.\n\t(mul<mode>3): Likewise.\n\t(*avx_mul<mode>3): Likewise.\n\t(*avx_vmmul<mode>3): Likewise.\n\t(divv8sf3): Likewise.\n\t(divv4df3): Likewise.\n\t(avx_div<mode>3): Likewise.\n\t(*avx_div<mode>3): Likewise.\n\t(*avx_vmdiv<mode>3): Likewise.\n\t(avx_rcpv8sf2): Likewise.\n\t(*avx_vmrcpv4sf2): Likewise.\n\t(sqrtv8sf2): Likewise.\n\t(avx_sqrtv8sf2): Likewise.\n\t(*avx_vmsqrt<mode>2): Likewise.\n\t(rsqrtv8sf2): Likewise.\n\t(avx_rsqrtv8sf2): Likewise.\n\t(*avx_vmrsqrtv4sf2): Likewise.\n\t(<code><mode>3): Likewise.\n\t(*avx_<code><mode>3_finite): Likewise.\n\t(*avx_<code><mode>3): Likewise.\n\t(*avx_vm<code><mode>3): Likewise.\n\t(*avx_ieee_smin<mode>3): Likewise.\n\t(*avx_ieee_smax<mode>3): Likewise.\n\t(avx_addsubv8sf3): Likewise.\n\t(avx_addsubv4df3): Likewise.\n\t(*avx_addsubv4sf3): Likewise.\n\t(*avx_addsubv2df3): Likewise.\n\t(avx_h<plusminus_insn>v4df3): Likewise.\n\t(avx_h<plusminus_insn>v8sf3): Likewise.\n\t(*avx_h<plusminus_insn>v4sf3): Likewise.\n\t(*avx_h<plusminus_insn>v2df3): Likewise.\n\t(avx_cmpp<avxmodesuffixf2c><mode>3): Likewise.\n\t(avx_cmps<ssemodesuffixf2c><mode>3): Likewise.\n\t(*avx_maskcmp<mode>3): Likewise.\n\t(avx_nand<mode>3): Likewise.\n\t(*avx_<code><mode>3): Likewise.\n\t(*avx_nand<mode>3): Likewise.\n\t(*avx_<code><mode>3): Likewise.\n\t(*avx_cvtsi2ss): Likewise.\n\t(*avx_cvtsi2ssq): Likewise.\n\t(*avx_cvtsi2sd): Likewise.\n\t(*avx_cvtsi2sdq): Likewise.\n\t(*avx_cvtsd2ss): Likewise.\n\t(avx_cvtss2sd): Likewise.\n\t(avx_cvtdq2ps<avxmodesuffix>): Likewise.\n\t(avx_cvtps2dq<avxmodesuffix>): Likewise.\n\t(avx_cvttps2dq<avxmodesuffix>): Likewise.\n\t(*avx_cvtsi2sd): Likewise.\n\t(*avx_cvtsi2sdq): Likewise.\n\t(avx_cvtdq2pd256): Likewise.\n\t(avx_cvtpd2dq256): Likewise.\n\t(avx_cvttpd2dq256): Likewise.\n\t(*avx_cvtsd2ss): Likewise.\n\t(*avx_cvtss2sd): Likewise.\n\t(avx_cvtpd2ps256): Likewise.\n\t(avx_cvtps2pd256): Likewise.\n\t(*avx_movhlps): Likewise.\n\t(*avx_movlhps): Likewise.\n\t(avx_unpckhps256): Likewise.\n\t(*avx_unpckhps): Likewise.\n\t(avx_unpcklps256): Likewise.\n\t(*avx_unpcklps): Likewise.\n\t(avx_movshdup256): Likewise.\n\t(avx_movsldup256): Likewise.\n\t(avx_shufps256): Likewise.\n\t(avx_shufps256_1): Likewise.\n\t(*avx_shufps_<mode>): Likewise.\n\t(*avx_loadhps): Likewise.\n\t(*avx_storelps): Likewise.\n\t(*avx_loadlps): Likewise.\n\t(*avx_movss): Likewise.\n\t(*vec_dupv4sf_avx): Likewise.\n\t(*vec_concatv2sf_avx): Likewise.\n\t(*vec_concatv4sf_avx): Likewise.\n\t(*vec_setv4sf_0_avx): Likewise.\n\t(*vec_setv4sf_avx): Likewise.\n\t(*avx_insertps): Likewise.\n\t(avx_vextractf128<mode>): Likewise.\n\t(vec_extract_lo_<mode>): Likewise.\n\t(vec_extract_hi_<mode>): Likewise.\n\t(vec_extract_lo_<mode>): Likewise.\n\t(vec_extract_hi_<mode>): Likewise.\n\t(vec_extract_lo_v16hi): Likewise.\n\t(vec_extract_hi_v16hi): Likewise.\n\t(vec_extract_lo_v32qi): Likewise.\n\t(vec_extract_hi_v32qi): Likewise.\n\t(avx_unpckhpd256): Likewise.\n\t(*avx_unpckhpd): Likewise.\n\t(avx_movddup256): Likewise.\n\t(*avx_movddup): Likewise.\n\t(avx_unpcklpd256): Likewise.\n\t(*avx_unpcklpd): Likewise.\n\t(avx_shufpd256): Likewise.\n\t(avx_shufpd256_1): Likewise.\n\t(*avx_punpckhqdq): Likewise.\n\t(*avx_punpcklqdq): Likewise.\n\t(*avx_shufpd_<mode>): Likewise.\n\t(*avx_storehpd): Likewise.\n\t(*avx_loadhpd): Likewise.\n\t(*avx_loadlpd): Likewise.\n\t(*avx_movsd): Likewise.\n\t(*vec_concatv2df_avx): Likewise.\n\t(*avx_<plusminus_insn><mode>3): Likewise.\n\t(*avx_<plusminus_insn><mode>3): Likewise.\n\t(*avx_mulv8hi3): Likewise.\n\t(*avxv8hi3_highpart): Likewise.\n\t(*avx_umulv8hi3_highpart): Likewise.\n\t(*avx_umulv2siv2di3): Likewise.\n\t(*avx_mulv2siv2di3): Likewise.\n\t(*avx_pmaddwd): Likewise.\n\t(*avx_mulv4si3): Likewise.\n\t(*avx_ashr<mode>3): Likewise.\n\t(*avx_lshr<mode>3): Likewise.\n\t(*avx_ashl<mode>3): Likewise.\n\t(*avx_<code><mode>3): Likewise.\n\t(*avx_eq<mode>3): Likewise.\n\t(*avx_gt<mode>3): Likewise.\n\t(*avx_nand<mode>3): Likewise.\n\t(*avx_nand<mode>3): Likewise.\n\t(*avx_<code><mode>3): Likewise.\n\t(*avx_<code><mode>3): Likewise.\n\t(*avx_packsswb): Likewise.\n\t(*avx_packssdw): Likewise.\n\t(*avx_packuswb): Likewise.\n\t(*avx_punpckhbw): Likewise.\n\t(*avx_punpcklbw): Likewise.\n\t(*avx_punpckhwd): Likewise.\n\t(*avx_punpcklwd): Likewise.\n\t(*avx_punpckhdq): Likewise.\n\t(*avx_punpckldq): Likewise.\n\t(*avx_pinsr<avxmodesuffixs>): Likewise.\n\t(*avx_pinsrq): Likewise.\n\t(*avx_loadld): Likewise.\n\t(*vec_extractv2di_1_rex64_avx): Likewise.\n\t(*vec_extractv2di_1_avx): Likewise.\n\t(*vec_dupv2di_avx): Likewise.\n\t(*vec_concatv2si_avx): Likewise.\n\t(*vec_concatv4si_1_avx): Likewise.\n\t(*vec_concatv2di_avx): Likewise.\n\t(*vec_concatv2di_rex64_avx): Likewise.\n\t(*avx_uavgv16qi3): Likewise.\n\t(*avx_uavgv8hi3): Likewise.\n\t(*avx_psadbw): Likewise.\n\t(avx_movmskp<avxmodesuffixf2c>256): Likewise.\n\t(*avx_phaddwv8hi3): Likewise.\n\t(*avx_phadddv4si3): Likewise.\n\t(*avx_phaddswv8hi3): Likewise.\n\t(*avx_phsubwv8hi3): Likewise.\n\t(*avx_phsubdv4si3): Likewise.\n\t(*avx_phsubswv8hi3): Likewise.\n\t(*avx_pmaddubsw128): Likewise.\n\t(*avx_pmulhrswv8hi3): Likewise.\n\t(*avx_pshufbv16qi3): Likewise.\n\t(*avx_psign<mode>3): Likewise.\n\t(*avx_palignrti): Likewise.\n\t(avx_blendp<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n\t(avx_blendvp<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n\t(avx_dpp<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n\t(*avx_mpsadbw): Likewise.\n\t(*avx_packusdw): Likewise.\n\t(*avx_pblendvb): Likewise.\n\t(*avx_pblendw): Likewise.\n\t(avx_vtestp<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n\t(avx_ptest256): Likewise.\n\t(avx_roundp<avxmodesuffixf2c>256): Likewise.\n\t(*avx_rounds<ssemodesuffixf2c>): Likewise.\n\t(*avx_aesenc): Likewise.\n\t(*avx_aesenclast): Likewise.\n\t(*avx_aesdec): Likewise.\n\t(*avx_aesdeclast): Likewise.\n\t(avx_vzeroupper): Likewise.\n\t(avx_vzeroupper_rex64): Likewise.\n\t(avx_vpermil<mode>): Likewise.\n\t(avx_vpermilvar<mode>3): Likewise.\n\t(avx_vpermil2<mode>3): Likewise.\n\t(avx_vperm2f128<mode>3): Likewise.\n\t(avx_vbroadcasts<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n\t(avx_vbroadcastss256): Likewise.\n\t(avx_vbroadcastf128_p<avxmodesuffixf2c>256): Likewise.\n\t(avx_vinsertf128<mode>): Likewise.\n\t(vec_set_lo_<mode>): Likewise.\n\t(vec_set_hi_<mode>): Likewise.\n\t(vec_set_lo_<mode>): Likewise.\n\t(vec_set_hi_<mode>): Likewise.\n\t(vec_set_lo_v16hi): Likewise.\n\t(vec_set_hi_v16hi): Likewise.\n\t(vec_set_lo_v32qi): Likewise.\n\t(vec_set_hi_v32qi): Likewise.\n\t(avx_maskloadp<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n\t(avx_maskstorep<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n\t(avx_<avxmodesuffixp><avxmodesuffix>_<avxmodesuffixp>): Likewise.\n\t(avx_<avxmodesuffixp>_<avxmodesuffixp><avxmodesuffix>): Likewise.\n\t(vec_init<mode>): Likewise.\n\t(*vec_concat<mode>_avx): Likewise.\n\t(blendbits): Support V8SF and V4DF.\n\t(sse2_movq128): Support AVX.\n\t(<sse>_movnt<mode>): Likewise.\n\t(sse2_movntv2di): Likewise.\n\t(sse_rcpv4sf2): Likewise.\n\t(sse_sqrtv4sf2): Likewise.\n\t(sse_rsqrtv4sf2): Likewise.\n\t(<sse>_comi): Likewise.\n\t(<sse>_ucomi): Likewise.\n\t(sse_cvtss2si): Likewise.\n\t(sse_cvtss2si_2): Likewise.\n\t(sse_cvtss2siq): Likewise.\n\t(sse_cvtss2siq_2): Likewise.\n\t(sse_cvttss2si): Likewise.\n\t(sse_cvttss2siq): Likewise.\n\t(sse2_cvtsd2si): Likewise.\n\t(sse2_cvtsd2si_2): Likewise.\n\t(sse2_cvtsd2siq): Likewise.\n\t(sse2_cvtsd2siq_2): Likewise.\n\t(sse2_cvttsd2si): Likewise.\n\t(sse2_cvttsd2siq): Likewise.\n\t(sse2_cvtdq2pd): Likewise.\n\t(*sse2_cvtpd2dq): Likewise.\n\t(*sse2_cvttpd2dq): Likewise.\n\t(*sse2_cvtpd2ps): Likewise.\n\t(sse2_cvtps2pd): Likewise.\n\t(sse3_movshdup): Likewise.\n\t(sse3_movsldup): Likewise.\n\t(sse_storehps): Likewise.\n\t(*sse4_1_extractps): Likewise.\n\t(sse2_storelpd): Likewise.\n\t(vec_dupv2df_sse3): Likewise.\n\t(*vec_concatv2df_sse3): Likewise.\n\t(*sse4_1_pextrb): Likewise.\n\t(*sse4_1_pextrb_memory): Likewise.\n\t(*sse2_pextrw): Likewise.\n\t(*sse4_1_pextrw_memory): Likewise.\n\t(*sse4_1_pextrd): Likewise.\n\t(*sse4_1_pextrq): Likewise.\n\t(sse2_pshufd_1): Likewise.\n\t(sse2_pshuflw_1): Likewise.\n\t(sse2_pshufhw_1): Likewise.\n\t(*sse2_storeq_rex64): Likewise.\n\t(*vec_dupv4si): Likewise.\n\t(<sse>_movmskp<ssemodesuffixf2c>): Likewise.\n\t(sse2_pmovmskb): Likewise.\n\t(*sse2_maskmovdqu): Likewise.\n\t(*sse2_maskmovdqu_rex64): Likewise.\n\t(sse_ldmxcsr): Likewise.\n\t(sse_stmxcsr): Likewise.\n\t(abs<mode>2): Likewise.\n\t(sse4_1_movntdqa): Likewise.\n\t(sse4_1_phminposuw): Likewise.\n\t(sse4_1_extendv8qiv8hi2): Likewise.\n\t(*sse4_1_extendv8qiv8hi2): Likewise.\n\t(sse4_1_extendv4qiv4si2): Likewise.\n\t(*sse4_1_extendv4qiv4si2): Likewise.\n\t(sse4_1_extendv2qiv2di2): Likewise.\n\t(*sse4_1_extendv2qiv2di2): Likewise.\n\t(sse4_1_extendv4hiv4si2): Likewise.\n\t(*sse4_1_extendv4hiv4si2): Likewise.\n\t(sse4_1_extendv2hiv2di2): Likewise.\n\t(*sse4_1_extendv2hiv2di2): Likewise.\n\t(sse4_1_extendv2siv2di2): Likewise.\n\t(*sse4_1_extendv2siv2di2): Likewise.\n\t(sse4_1_zero_extendv8qiv8hi2): Likewise.\n\t(*sse4_1_zero_extendv8qiv8hi2): Likewise.\n\t(sse4_1_zero_extendv4qiv4si2): Likewise.\n\t(*sse4_1_zero_extendv4qiv4si2): Likewise.\n\t(sse4_1_zero_extendv2qiv2di2): Likewise.\n\t(*sse4_1_zero_extendv2qiv2di2): Likewise.\n\t(sse4_1_zero_extendv4hiv4si2): Likewise.\n\t(*sse4_1_zero_extendv4hiv4si2): Likewise.\n\t(sse4_1_zero_extendv2hiv2di2): Likewise.\n\t(*sse4_1_zero_extendv2hiv2di2): Likewise.\n\t(sse4_1_zero_extendv2siv2di2): Likewise.\n\t(*sse4_1_zero_extendv2siv2di2): Likewise.\n\t(sse4_1_ptest): Likewise.\n\t(sse4_1_roundp<ssemodesuffixf2c>): Likewise.\n\t(sse4_2_pcmpestri): Likewise.\n\t(sse4_2_pcmpestrm): Likewise.\n\t(sse4_2_pcmpistri): Likewise.\n\t(sse4_2_pcmpistrm): Likewise.\n\t(aesimc): Likewise.\n\t(aeskeygenassist): Likewise.\n\n2008-08-28  Uros Bizjak  <ubizjak@gmail.com>\n\n\t* config/i386/predicates.md (vzeroall_operation): New.\n\n\t* config/i386/sse.md (avx_vzeroall): New.\n\t(*avx_vzeroall): Likewise.\n\nFrom-SVN: r139726", "tree": {"sha": "89f4cba7262e393b20f159a68224b7c1761827fe", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/89f4cba7262e393b20f159a68224b7c1761827fe"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/95879c728b9a59ae67db022ad370eb66374090f3", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/95879c728b9a59ae67db022ad370eb66374090f3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/95879c728b9a59ae67db022ad370eb66374090f3", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/95879c728b9a59ae67db022ad370eb66374090f3/comments", "author": null, "committer": null, "parents": [{"sha": "f67358da6b409b2f7109596bbf4be7af99963f05", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/f67358da6b409b2f7109596bbf4be7af99963f05", "html_url": "https://github.com/Rust-GCC/gccrs/commit/f67358da6b409b2f7109596bbf4be7af99963f05"}], "stats": {"total": 8113, "additions": 7808, "deletions": 305}, "files": [{"sha": "b326cc6fb78adae40866b36902b0fb88f3e2c19e", "filename": "gcc/ChangeLog", "status": "modified", "additions": 682, "deletions": 5, "changes": 687, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=95879c728b9a59ae67db022ad370eb66374090f3", "patch": "@@ -1,6 +1,683 @@\n+2008-08-28  H.J. Lu  <hongjiu.lu@intel.com>\n+\t    Joey Ye  <joey.ye@intel.com>\n+\t    Xuepeng Guo  <xuepeng.guo@intel.com>\n+\n+\t* config.gcc (extra_headers): Add gmmintrin.h for x86 and x86-64.\n+\n+\t* config/i386/cpuid.h (bit_FMA): New.\n+\t(bit_XSAVE): Likewise.\n+\t(bit_OSXSAVE): Likewise.\n+\t(bit_AVX): Likewise.\n+\n+\t* config/i386/gas.h (ASM_OUTPUT_OPCODE): Undefine before\n+\tdefine.  Use ASM_OUTPUT_AVX_PREFIX.\n+\n+\t* config/i386/gmmintrin.h: New.\n+\n+\t* config/i386/i386.c (x86_64_reg_class): Add X86_64_AVX_CLASS.\n+\t(OPTION_MASK_ISA_AVX_SET): New.\n+\t(OPTION_MASK_ISA_FMA_SET): Likewise.\n+\t(OPTION_MASK_ISA_AVX_UNSET): Likewise.\n+\t(OPTION_MASK_ISA_FMA_SET): Likewise.\n+\t(OPTION_MASK_ISA_SSE4_2_UNSET): Updated.\n+\t(ix86_handle_option): Handle OPT_mavx and OPT_mfma.\n+\t(pta_flags): Add PTA_AVX and PTA_FMA.\n+\t(override_options): Handle PTA_AVX and PTA_FMA.\n+\t(init_cumulative_args): Handle warn_avx.\n+\t(classify_argument): Return 0 for COImode and OImode.  Return\n+\t1 and X86_64_AVX_CLASS for 256bit vector types.\n+\t(examine_argument): Handle X86_64_AVX_CLASS.\n+\t(construct_container): Likewise.\n+\t(function_arg_advance_32): Pass OImode and 256bit vector types\n+\tin AVX register.\n+\t(function_arg_advance_64): Take a new argument to indicate if a\n+\tparameter is named.  Handle 256bit vector types.  Return\n+\timmediately for unnamed 256bit vector mode parameters.\n+\t(function_arg_advance): Updated.\n+\t(function_arg_32): Add comments for TImode.  Handle OImode\n+\tand 256bit vector types.\n+\t(function_arg_64): Take a new argument to indicate if a\n+\tparameter is named.  Handle 256bit vector types.  Return NULL\n+\tfor unnamed 256bit vector mode parameters.\n+\t(function_arg): Updated.\n+\t(setup_incoming_varargs_64): Support\n+\tAVX encoding for *sse_prologue_save_insn.\n+\t(ix86_gimplify_va_arg): Handle 256bit vector mode parameters.\n+\t(standard_sse_constant_p): Return -2 for all 1s if SSE2 isn't\n+\tenabled.  For all 1s in 256bit vector modes, return 3 if AVX is\n+\tenabled, otherwise return -3.\n+\t(standard_sse_constant_opcode): Handle AVX and 256bit vector\n+\tmodes.\n+\t(print_reg): Support AVX registers.  Handle 'x' and 't'.\n+\tHandle 'd' to duplicate the operand.\n+\t(print_operand): Likewise.  Also support AVX vector compare\n+\tinstructions.\n+\t(output_387_binary_op): Support AVX.\n+\t(output_fp_compare): Likewise.\n+\t(ix86_expand_vector_move_misalign): Likewise.\n+\t(ix86_attr_length_vex_default): New.\n+\t(ix86_builtins): Add IX86_BUILTIN_ADDPD256,\n+\tIX86_BUILTIN_ADDPS256, IX86_BUILTIN_ADDSUBPD256,\n+\tIX86_BUILTIN_ADDSUBPS256, IX86_BUILTIN_ANDPD256,\n+\tIX86_BUILTIN_ANDPS256, IX86_BUILTIN_ANDNPD256,\n+\tIX86_BUILTIN_ANDNPS256, IX86_BUILTIN_BLENDPD256,\n+\tIX86_BUILTIN_BLENDPS256, IX86_BUILTIN_BLENDVPD256,\n+\tIX86_BUILTIN_BLENDVPS256, IX86_BUILTIN_DIVPD256,\n+\tIX86_BUILTIN_DIVPS256, IX86_BUILTIN_DPPS256,\n+\tIX86_BUILTIN_HADDPD256, IX86_BUILTIN_HADDPS256,\n+\tIX86_BUILTIN_HSUBPD256, IX86_BUILTIN_HSUBPS256,\n+\tIX86_BUILTIN_MAXPD256, IX86_BUILTIN_MAXPS256,\n+\tIX86_BUILTIN_MINPD256, IX86_BUILTIN_MINPS256,\n+\tIX86_BUILTIN_MULPD256, IX86_BUILTIN_MULPS256,\n+\tIX86_BUILTIN_ORPD256, IX86_BUILTIN_ORPS256,\n+\tIX86_BUILTIN_SHUFPD256, IX86_BUILTIN_SHUFPS256,\n+\tIX86_BUILTIN_SUBPD256, IX86_BUILTIN_SUBPS256,\n+\tIX86_BUILTIN_XORPD256, IX86_BUILTIN_XORPS256,\n+\tIX86_BUILTIN_CMPSD, IX86_BUILTIN_CMPSS, IX86_BUILTIN_CMPPD,\n+\tIX86_BUILTIN_CMPPS, IX86_BUILTIN_CMPPD256,\n+\tIX86_BUILTIN_CMPPS256, IX86_BUILTIN_CVTDQ2PD256,\n+\tIX86_BUILTIN_CVTDQ2PS256, IX86_BUILTIN_CVTPD2PS256,\n+\tIX86_BUILTIN_CVTPS2DQ256, IX86_BUILTIN_CVTPS2PD256,\n+\tIX86_BUILTIN_CVTTPD2DQ256, IX86_BUILTIN_CVTPD2DQ256,\n+\tIX86_BUILTIN_CVTTPS2DQ256, IX86_BUILTIN_EXTRACTF128PD256,\n+\tIX86_BUILTIN_EXTRACTF128PS256, IX86_BUILTIN_EXTRACTF128SI256,\n+\tIX86_BUILTIN_VZEROALL, IX86_BUILTIN_VZEROUPPER,\n+\tIX86_BUILTIN_VZEROUPPER_REX64, IX86_BUILTIN_VPERMILVARPD,\n+\tIX86_BUILTIN_VPERMILVARPS, IX86_BUILTIN_VPERMILVARPD256,\n+\tIX86_BUILTIN_VPERMILVARPS256, IX86_BUILTIN_VPERMILPD,\n+\tIX86_BUILTIN_VPERMILPS, IX86_BUILTIN_VPERMILPD256,\n+\tIX86_BUILTIN_VPERMILPS256, IX86_BUILTIN_VPERMIL2PD,\n+\tIX86_BUILTIN_VPERMIL2PS, IX86_BUILTIN_VPERMIL2PD256,\n+\tIX86_BUILTIN_VPERMIL2PS256, IX86_BUILTIN_VPERM2F128PD256,\n+\tIX86_BUILTIN_VPERM2F128PS256, IX86_BUILTIN_VPERM2F128SI256,\n+\tIX86_BUILTIN_VBROADCASTSS, IX86_BUILTIN_VBROADCASTSD256,\n+\tIX86_BUILTIN_VBROADCASTSS256, IX86_BUILTIN_VBROADCASTPD256,\n+\tIX86_BUILTIN_VBROADCASTPS256, IX86_BUILTIN_VINSERTF128PD256,\n+\tIX86_BUILTIN_VINSERTF128PS256, IX86_BUILTIN_VINSERTF128SI256,\n+\tIX86_BUILTIN_LOADUPD256, IX86_BUILTIN_LOADUPS256,\n+\tIX86_BUILTIN_STOREUPD256, IX86_BUILTIN_STOREUPS256,\n+\tIX86_BUILTIN_LDDQU256, IX86_BUILTIN_LOADDQU256,\n+\tIX86_BUILTIN_STOREDQU256, IX86_BUILTIN_MASKLOADPD,\n+\tIX86_BUILTIN_MASKLOADPS, IX86_BUILTIN_MASKSTOREPD,\n+\tIX86_BUILTIN_MASKSTOREPS, IX86_BUILTIN_MASKLOADPD256,\n+\tIX86_BUILTIN_MASKLOADPS256, IX86_BUILTIN_MASKSTOREPD256,\n+\tIX86_BUILTIN_MASKSTOREPS256, IX86_BUILTIN_MOVSHDUP256,\n+\tIX86_BUILTIN_MOVSLDUP256, IX86_BUILTIN_MOVDDUP256,\n+\tIX86_BUILTIN_SQRTPD256, IX86_BUILTIN_SQRTPS256,\n+\tIX86_BUILTIN_SQRTPS_NR256, IX86_BUILTIN_RSQRTPS256,\n+\tIX86_BUILTIN_RSQRTPS_NR256, IX86_BUILTIN_RCPPS256,\n+\tIX86_BUILTIN_ROUNDPD256, IX86_BUILTIN_ROUNDPS256,\n+\tIX86_BUILTIN_UNPCKHPD256, IX86_BUILTIN_UNPCKLPD256,\n+\tIX86_BUILTIN_UNPCKHPS256, IX86_BUILTIN_UNPCKLPS256,\n+\tIX86_BUILTIN_SI256_SI, IX86_BUILTIN_PS256_PS,\n+\tIX86_BUILTIN_PD256_PD, IX86_BUILTIN_SI_SI256,\n+\tIX86_BUILTIN_PS_PS256, IX86_BUILTIN_PD_PD256,\n+\tIX86_BUILTIN_VTESTZPD, IX86_BUILTIN_VTESTCPD,\n+\tIX86_BUILTIN_VTESTNZCPD, IX86_BUILTIN_VTESTZPS,\n+\tIX86_BUILTIN_VTESTCPS, IX86_BUILTIN_VTESTNZCPS,\n+\tIX86_BUILTIN_VTESTZPD256, IX86_BUILTIN_VTESTCPD256,\n+\tIX86_BUILTIN_VTESTNZCPD256, IX86_BUILTIN_VTESTZPS256,\n+\tIX86_BUILTIN_VTESTCPS256, IX86_BUILTIN_VTESTNZCPS256,\n+\tIX86_BUILTIN_PTESTZ256, IX86_BUILTIN_PTESTC256,\n+\tIX86_BUILTIN_PTESTNZC256, IX86_BUILTIN_MOVMSKPD256\n+\tand IX86_BUILTIN_MOVMSKPS256,\n+\t(ix86_special_builtin_type): Add V32QI_FTYPE_PCCHAR,\n+\tV8SF_FTYPE_PCV4SF, V8SF_FTYPE_PCFLOAT, V4DF_FTYPE_PCV2DF,\n+\tV4DF_FTYPE_PCDOUBLE, V8SF_FTYPE_PCV8SF_V8SF,\n+\tV4DF_FTYPE_PCV4DF_V4DF, V4SF_FTYPE_PCV4SF_V4SF,\n+\tV2DF_FTYPE_PCV2DF_V2DF, VOID_FTYPE_PCHAR_V32QI,\n+\tVOID_FTYPE_PFLOAT_V8SF, VOID_FTYPE_PDOUBLE_V4DF,\n+\tVOID_FTYPE_PV8SF_V8SF_V8SF, VOID_FTYPE_PV4DF_V4DF_V4DF,\n+\tVOID_FTYPE_PV4SF_V4SF_V4SF and VOID_FTYPE_PV2DF_V2DF_V2DF,\n+\t(ix86_builtin_type): Add INT_FTYPE_V8SF_V8SF_PTEST,\n+\tINT_FTYPE_V4DI_V4DI_PTEST, INT_FTYPE_V4DF_V4DF_PTEST,\n+\tINT_FTYPE_V4SF_V4SF_PTEST, INT_FTYPE_V2DF_V2DF_PTEST,\n+\tINT_FTYPE_V8SF, INT_FTYPE_V4DF, V8SI_FTYPE_V8SF, V8SI_FTYPE_V4SI,\n+\tV8SF_FTYPE_V8SF, V8SF_FTYPE_V8SI, V8SF_FTYPE_V4SF,\n+\tV4SI_FTYPE_V8SI, V4SI_FTYPE_V4DF, V4DF_FTYPE_V4DF,\n+\tV4DF_FTYPE_V4SI, V4DF_FTYPE_V4SF, V4DF_FTYPE_V2DF,\n+\tV4SF_FTYPE_V4DF, V4SF_FTYPE_V8SF, V2DF_FTYPE_V4DF,\n+\tV8SF_FTYPE_V8SF_V8SF, V8SF_FTYPE_V8SF_V8SI,\n+\tV4DF_FTYPE_V4DF_V4DF, V4DF_FTYPE_V4DF_V4DI,\n+\tV4SF_FTYPE_V4SF_V4SI, V2DF_FTYPE_V2DF_V2DI,\n+\tV8SF_FTYPE_V8SF_INT, V4SI_FTYPE_V8SI_INT, V4SF_FTYPE_V8SF_INT,\n+\tV2DF_FTYPE_V4DF_INT, V4DF_FTYPE_V4DF_INT,\n+\tV8SF_FTYPE_V8SF_V8SF_V8SF, V4DF_FTYPE_V4DF_V4DF_V4DF,\n+\tV8SI_FTYPE_V8SI_V8SI_INT, V8SF_FTYPE_V8SF_V8SF_INT,\n+\tV4DF_FTYPE_V4DF_V4DF_INT, V4DF_FTYPE_V4DF_V2DF_INT,\n+\tV8SF_FTYPE_V8SF_V8SF_V8SI_INT, V4DF_FTYPE_V4DF_V4DF_V4DI_INT,\n+\tV4SF_FTYPE_V4SF_V4SF_V4SI_INT and V2DF_FTYPE_V2DF_V2DF_V2DI_INT.\n+\t(bdesc_special_args): Add IX86_BUILTIN_VZEROALL,\n+\tIX86_BUILTIN_VZEROUPPER. IX86_BUILTIN_VZEROUPPER_REX64,\n+\tIX86_BUILTIN_VBROADCASTSS, IX86_BUILTIN_VBROADCASTSD256,\n+\tIX86_BUILTIN_VBROADCASTSS256, IX86_BUILTIN_VBROADCASTPD256,\n+\tIX86_BUILTIN_VBROADCASTPS256, IX86_BUILTIN_LOADUPD256,\n+\tIX86_BUILTIN_LOADUPS256, IX86_BUILTIN_STOREUPD256,\n+\tIX86_BUILTIN_STOREUPS256, IX86_BUILTIN_LOADDQU256,\n+\tIX86_BUILTIN_STOREDQU256, IX86_BUILTIN_LDDQU256,\n+\tIX86_BUILTIN_MASKLOADPD, IX86_BUILTIN_MASKLOADPS,\n+\tIX86_BUILTIN_MASKLOADPD256, IX86_BUILTIN_MASKLOADPS256,\n+\tIX86_BUILTIN_MASKSTOREPD, IX86_BUILTIN_MASKSTOREPS,\n+\tIX86_BUILTIN_MASKSTOREPD256 and IX86_BUILTIN_MASKSTOREPS256.\n+\t(ix86_builtins): Add IX86_BUILTIN_ADDPD256,\n+\tIX86_BUILTIN_ADDPS256, IX86_BUILTIN_ADDSUBPD256,\n+\tIX86_BUILTIN_ADDSUBPS256, IX86_BUILTIN_ANDPD256,\n+\tIX86_BUILTIN_ANDPS256, IX86_BUILTIN_ANDNPD256,\n+\tIX86_BUILTIN_ANDNPS256, IX86_BUILTIN_DIVPD256,\n+\tIX86_BUILTIN_DIVPS256, IX86_BUILTIN_HADDPD256,\n+\tIX86_BUILTIN_HSUBPS256, IX86_BUILTIN_HSUBPD256,\n+\tIX86_BUILTIN_HADDPS256, IX86_BUILTIN_MAXPD256,\n+\tIX86_BUILTIN_MAXPS256, IX86_BUILTIN_MINPD256,\n+\tIX86_BUILTIN_MINPS256, IX86_BUILTIN_MULPD256,\n+\tIX86_BUILTIN_MULPS256, IX86_BUILTIN_ORPD256,\n+\tIX86_BUILTIN_ORPS256, IX86_BUILTIN_SUBPD256,\n+\tIX86_BUILTIN_SUBPS256, IX86_BUILTIN_XORPD256,\n+\tIX86_BUILTIN_XORPS256, IX86_BUILTIN_VPERMILVARPD,\n+\tIX86_BUILTIN_VPERMILVARPS, IX86_BUILTIN_VPERMILVARPD256,\n+\tIX86_BUILTIN_VPERMILVARPS256, IX86_BUILTIN_BLENDPD256,\n+\tIX86_BUILTIN_BLENDPS256, IX86_BUILTIN_BLENDVPD256,\n+\tIX86_BUILTIN_BLENDVPS256, IX86_BUILTIN_DPPS256,\n+\tIX86_BUILTIN_SHUFPD256, IX86_BUILTIN_SHUFPS256,\n+\tIX86_BUILTIN_CMPSD, IX86_BUILTIN_CMPSS, IX86_BUILTIN_CMPPD,\n+\tIX86_BUILTIN_CMPPS,\n+\tIX86_BUILTIN_CMPPD256,IX86_BUILTIN_CMPPS256,\n+\tIX86_BUILTIN_EXTRACTF128PD256, IX86_BUILTIN_EXTRACTF128PS256,\n+\tIX86_BUILTIN_EXTRACTF128SI256, IX86_BUILTIN_CVTDQ2PD256,\n+\tIX86_BUILTIN_CVTDQ2PS256, IX86_BUILTIN_CVTPD2PS256,\n+\tIX86_BUILTIN_CVTPS2DQ256, IX86_BUILTIN_CVTPS2PD256,\n+\tIX86_BUILTIN_CVTTPD2DQ256, IX86_BUILTIN_CVTPD2DQ256,\n+\tIX86_BUILTIN_CVTTPS2DQ256, IX86_BUILTIN_VPERM2F128PD256,\n+\tIX86_BUILTIN_VPERM2F128PS256, IX86_BUILTIN_VPERM2F128SI256,\n+\tIX86_BUILTIN_VPERMILPD, IX86_BUILTIN_VPERMILPS,\n+\tIX86_BUILTIN_VPERMILPD256, IX86_BUILTIN_VPERMILPS256,\n+\tIX86_BUILTIN_VPERMIL2PD, IX86_BUILTIN_VPERMILPS,\n+\tIX86_BUILTIN_VPERMILPD256, IX86_BUILTIN_VPERMILPS256,\n+\tIX86_BUILTIN_VPERMIL2PD, IX86_BUILTIN_VPERMIL2PS,\n+\tIX86_BUILTIN_VPERMIL2PD256, IX86_BUILTIN_VPERMIL2PS256,\n+\tIX86_BUILTIN_VINSERTF128PD256, IX86_BUILTIN_VINSERTF128PS256,\n+\tIX86_BUILTIN_VINSERTF128SI256, IX86_BUILTIN_MOVSHDUP256,\n+\tIX86_BUILTIN_MOVSLDUP256, IX86_BUILTIN_MOVDDUP256,\n+\tIX86_BUILTIN_SQRTPD256, IX86_BUILTIN_SQRTPS256,\n+\tIX86_BUILTIN_SQRTPS_NR256, IX86_BUILTIN_RSQRTPS256,\n+\tIX86_BUILTIN_RSQRTPS_NR256, IX86_BUILTIN_RCPPS256,\n+\tIX86_BUILTIN_ROUNDPD256, IX86_BUILTIN_ROUNDPS256,\n+\tIX86_BUILTIN_UNPCKHPD256, IX86_BUILTIN_UNPCKLPD256,\n+\tIX86_BUILTIN_UNPCKHPS256, IX86_BUILTIN_UNPCKLPS256,\n+\tIX86_BUILTIN_SI256_SI, IX86_BUILTIN_PS256_PS,\n+\tIX86_BUILTIN_PD256_PD, IX86_BUILTIN_SI_SI256,\n+\tIX86_BUILTIN_PS_PS256, IX86_BUILTIN_PD_PD256,\n+\tIX86_BUILTIN_VTESTZPD, IX86_BUILTIN_VTESTCPD,\n+\tIX86_BUILTIN_VTESTNZCPD, IX86_BUILTIN_VTESTZPS,\n+\tIX86_BUILTIN_VTESTCPS, IX86_BUILTIN_VTESTNZCPS,\n+\tIX86_BUILTIN_VTESTZPD256, IX86_BUILTIN_VTESTCPD256,\n+\tIX86_BUILTIN_VTESTNZCPD256, IX86_BUILTIN_VTESTZPS256,\n+\tIX86_BUILTIN_VTESTCPS256, IX86_BUILTIN_VTESTNZCPS256,\n+\tIX86_BUILTIN_PTESTZ256, IX86_BUILTIN_PTESTC256,\n+\tIX86_BUILTIN_PTESTNZC256, IX86_BUILTIN_MOVMSKPD256 and\n+\tIX86_BUILTIN_MOVMSKPS256.\n+\t(ix86_init_mmx_sse_builtins): Support AVX builtins.\n+\t(ix86_expand_args_builtin): Likewise.\n+\t(ix86_expand_special_args_builtin): Likewise.\n+\t(ix86_hard_regno_mode_ok): Handle AVX modes.\n+\t(ix86_expand_vector_init_duplicate): Likewise.\n+\t(ix86_expand_vector_init_one_nonzero): Likewise.\n+\t(ix86_expand_vector_init_one_var): Likewise.\n+\t(ix86_expand_vector_init_concat): Likewise.\n+\t(ix86_expand_vector_init_general): Likewise.\n+\t(ix86_expand_vector_set): Likewise.\n+\t(ix86_vector_mode_supported_p): Likewise.\n+\t(x86_extended_reg_mentioned_p): Check INSN_P before using\n+\tPATTERN.\n+\n+\t* config/i386/i386-c.c (ix86_target_macros_internal): Handle\n+\tOPTION_MASK_ISA_AVX and OPTION_MASK_ISA_FMA.\n+\n+\t* config/i386/i386.h (TARGET_AVX): New.\n+\t(TARGET_FMA): Likewise.\n+\t(TARGET_CPU_CPP_BUILTINS): Handle TARGET_AVX and TARGET_FMA.\n+\t(BIGGEST_ALIGNMENT): Set to 256 for TARGET_AVX.\n+\t(VALID_AVX256_REG_MODE): New.\n+\t(AVX256_VEC_FLOAT_MODE_P): Likewise.\n+\t(AVX_FLOAT_MODE_P): Likewise.\n+\t(AVX128_VEC_FLOAT_MODE_P): Likewise.\n+\t(AVX256_VEC_FLOAT_MODE_P): Likewise.\n+\t(AVX_VEC_FLOAT_MODE_P): Likewise.\n+\t(ASM_OUTPUT_AVX_PREFIX): Likewise.\n+\t(ASM_OUTPUT_OPCODE): Likewise.\n+\t(UNITS_PER_SIMD_WORD): Add a FIXME for 32byte vectorizer\n+\tsupport.\n+\t(SSE_REG_MODE_P): Allow 256bit vector modes.\n+\t(ix86_args): Add a warn_avx field.\n+\n+\t* config/i386/i386.md (UNSPEC_PCMP): New.\n+\t(UNSPEC_VPERMIL): Likewise.\n+\t(UNSPEC_VPERMIL2): Likewise.\n+\t(UNSPEC_VPERMIL2F128): Likewise.\n+\t(UNSPEC_MASKLOAD): Likewise.\n+\t(UNSPEC_MASKSTORE): Likewise.\n+\t(UNSPEC_CAST): Likewise.\n+\t(UNSPEC_VTESTP): Likewise.\n+\t(UNSPECV_VZEROALL): Likewise.\n+\t(UNSPECV_VZEROUPPER): Likewise.\n+\t(XMM0_REG): Likewise.\n+\t(XMM1_REG): Likewise.\n+\t(XMM2_REG): Likewise.\n+\t(XMM3_REG): Likewise.\n+\t(XMM4_REG): Likewise.\n+\t(XMM5_REG): Likewise.\n+\t(XMM6_REG): Likewise.\n+\t(XMM8_REG): Likewise.\n+\t(XMM9_REG): Likewise.\n+\t(XMM10_REG): Likewise.\n+\t(XMM11_REG): Likewise.\n+\t(XMM12_REG): Likewise.\n+\t(XMM13_REG): Likewise.\n+\t(XMM14_REG): Likewise.\n+\t(XMM15_REG): Likewise.\n+\t(prefix): Likewise.\n+\t(prefix_vex_imm8): Likewise.\n+\t(prefix_vex_w): Likewise.\n+\t(length_vex): Likewise.\n+\t(maxmin): Likewise.\n+\t(movoi): Likewise.\n+\t(*avx_ashlti3): Likewise.\n+\t(*avx_lshrti3): Likewise.\n+\t(*avx_setcc<mode>): Likewise.\n+\t(*fop_<mode>_comm_mixed_avx): Likewise.\n+\t(*fop_<mode>_comm_avx): Likewise.\n+\t(*fop_<mode>_1_mixed_avx): Likewise.\n+\t(*fop_<mode>_1_avx): Likewise.\n+\t(*avx_<code><mode>3): Likewise.\n+\t(*avx_ieee_smin<mode>3): Likewise.\n+\t(*avx_ieee_smax<mode>3): Likewise.\n+\t(mode): Add OI, V8SF and V4DF.\n+\t(length): Support VEX prefix.\n+\t(*cmpfp_i_mixed): Set prefix attribute.\n+\t(*cmpfp_i_sse): Likewise.\n+\t(*cmpfp_iu_mixed): Likewise.\n+\t(*cmpfp_iu_sse): Likewise.\n+\t(*movsi_1): Support AVX.\n+\t(*movdi_2): Likewise.\n+\t(*movdi_1_rex64): Likewise.\n+\t(*movti_internal): Likewise.\n+\t(*movti_rex64): Likewise.\n+\t(*movsf_1): Likewise.\n+\t(*movdf_nointeger): Likewise.\n+\t(*movdf_integer_rex64): Likewise.\n+\t(*movtf_internal): Likewise.\n+\t(zero_extendsidi2_32): Likewise.\n+\t(zero_extendsidi2_rex64): Likewise.\n+\t(*extendsfdf2_mixed): Likewise.\n+\t(*extendsfdf2_sse): Likewise.\n+\t(*truncdfsf_fast_mixed): Likewise.\n+\t(*truncdfsf_fast_sse): Likewise.\n+\t(*truncdfsf_mixed): Likewise.\n+\t(fix_trunc<mode>di_sse): Likewise.\n+\t(fix_trunc<mode>si_sse): Likewise.\n+\t(*float<SSEMODEI24:mode><MODEF:mode>2_mixed_interunit): Likewise.\n+\t(*float<SSEMODEI24:mode><MODEF:mode>2_mixed_nointerunit): Likewise.\n+\t(*float<SSEMODEI24:mode><MODEF:mode>2_sse_interunit): Likewise.\n+\t(*float<SSEMODEI24:mode><MODEF:mode>2_sse_nointerunit): Likewise.\n+\t(*rcpsf2_sse): Likewise.\n+\t(*rsqrtsf2_sse): Likewise.\n+\t(*sqrt<mode>2_sse): Likewise.\n+\t(sse4_1_round<mode>2): Likewise.\n+\t(*sse_prologue_save_insn): Disallow REX prefix for AVX.\n+\tSupport AVX.  Set length attribute properly for AVX.\n+\n+\t* config/i386/i386-modes.def (VECTOR_MODES (INT, 32)): New.\n+\t(VECTOR_MODES (FLOAT, 32)): Likewise.\n+\t(VECTOR_MODE (INT, DI, 8)): Likewise.\n+\t(VECTOR_MODE (INT, HI, 32)): Likewise.\n+\t(VECTOR_MODE (INT, QI, 64)): Likewise.\n+\t(VECTOR_MODE (FLOAT, DF, 8)): Likewise.\n+\t(VECTOR_MODE (FLOAT, SF, 16)): Likewise.\n+\t(VECTOR_MODE (INT, DI, 4)): Removed.\n+\t(VECTOR_MODE (INT, SI, 8)): Likewise.\n+\t(VECTOR_MODE (INT, HI, 16)): Likewise.\n+\t(VECTOR_MODE (INT, QI, 32)): Likewise.\n+\t(VECTOR_MODE (FLOAT, SF, 8)): Likewise.\n+\t(INT_MODE (OI, 32)): Likewise.\n+\n+\t* config/i386/i386.opt (mavx): New.\n+\t(mfma): Likewise.\n+\n+\t* config/i386/i386-protos.h (ix86_attr_length_vex_default): New.\n+\n+\t* config/i386/mmx.md (*mov<mode>_internal_rex64): Support AVX.\n+\t(*mov<mode>_internal_avx): New.\n+\t(*movv2sf_internal_rex64_avx): Likewise.\n+\t(*movv2sf_internal_avx): Likewise.\n+\n+\t* config/i386/predicates.md (const_4_to_5_operand): New.\n+\t(const_6_to_7_operand): Likewise.\n+\t(const_8_to_11_operand): Likewise.\n+\t(const_12_to_15_operand): Likewise.\n+\t(avx_comparison_float_operator): Likewise.\n+\n+\t* config/i386/sse.md (AVX256MODEI): New.\n+\t(AVX256MODE): Likewise.\n+\t(AVXMODEQI): Likewise.\n+\t(AVXMODE): Likewise.\n+\t(AVX256MODEF2P): Likewise.\n+\t(AVX256MODE2P): Likewise.\n+\t(AVX256MODE4P): Likewise.\n+\t(AVX256MODE8P): Likewise.\n+\t(AVXMODEF2P): Likewise.\n+\t(AVXMODEF4P): Likewise.\n+\t(AVXMODEDCVTDQ2PS): Likewise.\n+\t(AVXMODEDCVTPS2DQ): Likewise.\n+\t(avxvecmode): Likewise.\n+\t(avxvecpsmode): Likewise.\n+\t(avxhalfvecmode): Likewise.\n+\t(avxscalarmode): Likewise.\n+\t(avxcvtvecmode): Likewise.\n+\t(avxpermvecmode): Likewise.\n+\t(avxmodesuffixf2c): Likewise.\n+\t(avxmodesuffixp): Likewise.\n+\t(avxmodesuffixs): Likewise.\n+\t(avxmodesuffix): Likewise.\n+\t(vpermilbits): Likewise.\n+\t(pinsrbits): Likewise.\n+\t(mov<mode>): Likewise.\n+\t(*mov<mode>_internal): Likewise.\n+\t(push<mode>1): Likewise.\n+\t(movmisalign<mode>): Likewise.\n+\t(avx_movup<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n+\t(avx_movdqu<avxmodesuffix>): Likewise.\n+\t(avx_lddqu<avxmodesuffix>): Likewise.\n+\t(<plusminus_insn><mode>3): Likewise.\n+\t(*avx_<plusminus_insn><mode>3): Likewise.\n+\t(*avx_vm<plusminus_insn><mode>3): Likewise.\n+\t(mul<mode>3): Likewise.\n+\t(*avx_mul<mode>3): Likewise.\n+\t(*avx_vmmul<mode>3): Likewise.\n+\t(divv8sf3): Likewise.\n+\t(divv4df3): Likewise.\n+\t(avx_div<mode>3): Likewise.\n+\t(*avx_div<mode>3): Likewise.\n+\t(*avx_vmdiv<mode>3): Likewise.\n+\t(avx_rcpv8sf2): Likewise.\n+\t(*avx_vmrcpv4sf2): Likewise.\n+\t(sqrtv8sf2): Likewise.\n+\t(avx_sqrtv8sf2): Likewise.\n+\t(*avx_vmsqrt<mode>2): Likewise.\n+\t(rsqrtv8sf2): Likewise.\n+\t(avx_rsqrtv8sf2): Likewise.\n+\t(*avx_vmrsqrtv4sf2): Likewise.\n+\t(<code><mode>3): Likewise.\n+\t(*avx_<code><mode>3_finite): Likewise.\n+\t(*avx_<code><mode>3): Likewise.\n+\t(*avx_vm<code><mode>3): Likewise.\n+\t(*avx_ieee_smin<mode>3): Likewise.\n+\t(*avx_ieee_smax<mode>3): Likewise.\n+\t(avx_addsubv8sf3): Likewise.\n+\t(avx_addsubv4df3): Likewise.\n+\t(*avx_addsubv4sf3): Likewise.\n+\t(*avx_addsubv2df3): Likewise.\n+\t(avx_h<plusminus_insn>v4df3): Likewise.\n+\t(avx_h<plusminus_insn>v8sf3): Likewise.\n+\t(*avx_h<plusminus_insn>v4sf3): Likewise.\n+\t(*avx_h<plusminus_insn>v2df3): Likewise.\n+\t(avx_cmpp<avxmodesuffixf2c><mode>3): Likewise.\n+\t(avx_cmps<ssemodesuffixf2c><mode>3): Likewise.\n+\t(*avx_maskcmp<mode>3): Likewise.\n+\t(avx_nand<mode>3): Likewise.\n+\t(*avx_<code><mode>3): Likewise.\n+\t(*avx_nand<mode>3): Likewise.\n+\t(*avx_<code><mode>3): Likewise.\n+\t(*avx_cvtsi2ss): Likewise.\n+\t(*avx_cvtsi2ssq): Likewise.\n+\t(*avx_cvtsi2sd): Likewise.\n+\t(*avx_cvtsi2sdq): Likewise.\n+\t(*avx_cvtsd2ss): Likewise.\n+\t(avx_cvtss2sd): Likewise.\n+\t(avx_cvtdq2ps<avxmodesuffix>): Likewise.\n+\t(avx_cvtps2dq<avxmodesuffix>): Likewise.\n+\t(avx_cvttps2dq<avxmodesuffix>): Likewise.\n+\t(*avx_cvtsi2sd): Likewise.\n+\t(*avx_cvtsi2sdq): Likewise.\n+\t(avx_cvtdq2pd256): Likewise.\n+\t(avx_cvtpd2dq256): Likewise.\n+\t(avx_cvttpd2dq256): Likewise.\n+\t(*avx_cvtsd2ss): Likewise.\n+\t(*avx_cvtss2sd): Likewise.\n+\t(avx_cvtpd2ps256): Likewise.\n+\t(avx_cvtps2pd256): Likewise.\n+\t(*avx_movhlps): Likewise.\n+\t(*avx_movlhps): Likewise.\n+\t(avx_unpckhps256): Likewise.\n+\t(*avx_unpckhps): Likewise.\n+\t(avx_unpcklps256): Likewise.\n+\t(*avx_unpcklps): Likewise.\n+\t(avx_movshdup256): Likewise.\n+\t(avx_movsldup256): Likewise.\n+\t(avx_shufps256): Likewise.\n+\t(avx_shufps256_1): Likewise.\n+\t(*avx_shufps_<mode>): Likewise.\n+\t(*avx_loadhps): Likewise.\n+\t(*avx_storelps): Likewise.\n+\t(*avx_loadlps): Likewise.\n+\t(*avx_movss): Likewise.\n+\t(*vec_dupv4sf_avx): Likewise.\n+\t(*vec_concatv2sf_avx): Likewise.\n+\t(*vec_concatv4sf_avx): Likewise.\n+\t(*vec_setv4sf_0_avx): Likewise.\n+\t(*vec_setv4sf_avx): Likewise.\n+\t(*avx_insertps): Likewise.\n+\t(avx_vextractf128<mode>): Likewise.\n+\t(vec_extract_lo_<mode>): Likewise.\n+\t(vec_extract_hi_<mode>): Likewise.\n+\t(vec_extract_lo_<mode>): Likewise.\n+\t(vec_extract_hi_<mode>): Likewise.\n+\t(vec_extract_lo_v16hi): Likewise.\n+\t(vec_extract_hi_v16hi): Likewise.\n+\t(vec_extract_lo_v32qi): Likewise.\n+\t(vec_extract_hi_v32qi): Likewise.\n+\t(avx_unpckhpd256): Likewise.\n+\t(*avx_unpckhpd): Likewise.\n+\t(avx_movddup256): Likewise.\n+\t(*avx_movddup): Likewise.\n+\t(avx_unpcklpd256): Likewise.\n+\t(*avx_unpcklpd): Likewise.\n+\t(avx_shufpd256): Likewise.\n+\t(avx_shufpd256_1): Likewise.\n+\t(*avx_punpckhqdq): Likewise.\n+\t(*avx_punpcklqdq): Likewise.\n+\t(*avx_shufpd_<mode>): Likewise.\n+\t(*avx_storehpd): Likewise.\n+\t(*avx_loadhpd): Likewise.\n+\t(*avx_loadlpd): Likewise.\n+\t(*avx_movsd): Likewise.\n+\t(*vec_concatv2df_avx): Likewise.\n+\t(*avx_<plusminus_insn><mode>3): Likewise.\n+\t(*avx_<plusminus_insn><mode>3): Likewise.\n+\t(*avx_mulv8hi3): Likewise.\n+\t(*avxv8hi3_highpart): Likewise.\n+\t(*avx_umulv8hi3_highpart): Likewise.\n+\t(*avx_umulv2siv2di3): Likewise.\n+\t(*avx_mulv2siv2di3): Likewise.\n+\t(*avx_pmaddwd): Likewise.\n+\t(*avx_mulv4si3): Likewise.\n+\t(*avx_ashr<mode>3): Likewise.\n+\t(*avx_lshr<mode>3): Likewise.\n+\t(*avx_ashl<mode>3): Likewise.\n+\t(*avx_<code><mode>3): Likewise.\n+\t(*avx_eq<mode>3): Likewise.\n+\t(*avx_gt<mode>3): Likewise.\n+\t(*avx_nand<mode>3): Likewise.\n+\t(*avx_nand<mode>3): Likewise.\n+\t(*avx_<code><mode>3): Likewise.\n+\t(*avx_<code><mode>3): Likewise.\n+\t(*avx_packsswb): Likewise.\n+\t(*avx_packssdw): Likewise.\n+\t(*avx_packuswb): Likewise.\n+\t(*avx_punpckhbw): Likewise.\n+\t(*avx_punpcklbw): Likewise.\n+\t(*avx_punpckhwd): Likewise.\n+\t(*avx_punpcklwd): Likewise.\n+\t(*avx_punpckhdq): Likewise.\n+\t(*avx_punpckldq): Likewise.\n+\t(*avx_pinsr<avxmodesuffixs>): Likewise.\n+\t(*avx_pinsrq): Likewise.\n+\t(*avx_loadld): Likewise.\n+\t(*vec_extractv2di_1_rex64_avx): Likewise.\n+\t(*vec_extractv2di_1_avx): Likewise.\n+\t(*vec_dupv2di_avx): Likewise.\n+\t(*vec_concatv2si_avx): Likewise.\n+\t(*vec_concatv4si_1_avx): Likewise.\n+\t(*vec_concatv2di_avx): Likewise.\n+\t(*vec_concatv2di_rex64_avx): Likewise.\n+\t(*avx_uavgv16qi3): Likewise.\n+\t(*avx_uavgv8hi3): Likewise.\n+\t(*avx_psadbw): Likewise.\n+\t(avx_movmskp<avxmodesuffixf2c>256): Likewise.\n+\t(*avx_phaddwv8hi3): Likewise.\n+\t(*avx_phadddv4si3): Likewise.\n+\t(*avx_phaddswv8hi3): Likewise.\n+\t(*avx_phsubwv8hi3): Likewise.\n+\t(*avx_phsubdv4si3): Likewise.\n+\t(*avx_phsubswv8hi3): Likewise.\n+\t(*avx_pmaddubsw128): Likewise.\n+\t(*avx_pmulhrswv8hi3): Likewise.\n+\t(*avx_pshufbv16qi3): Likewise.\n+\t(*avx_psign<mode>3): Likewise.\n+\t(*avx_palignrti): Likewise.\n+\t(avx_blendp<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n+\t(avx_blendvp<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n+\t(avx_dpp<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n+\t(*avx_mpsadbw): Likewise.\n+\t(*avx_packusdw): Likewise.\n+\t(*avx_pblendvb): Likewise.\n+\t(*avx_pblendw): Likewise.\n+\t(avx_vtestp<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n+\t(avx_ptest256): Likewise.\n+\t(avx_roundp<avxmodesuffixf2c>256): Likewise.\n+\t(*avx_rounds<ssemodesuffixf2c>): Likewise.\n+\t(*avx_aesenc): Likewise.\n+\t(*avx_aesenclast): Likewise.\n+\t(*avx_aesdec): Likewise.\n+\t(*avx_aesdeclast): Likewise.\n+\t(avx_vzeroupper): Likewise.\n+\t(avx_vzeroupper_rex64): Likewise.\n+\t(avx_vpermil<mode>): Likewise.\n+\t(avx_vpermilvar<mode>3): Likewise.\n+\t(avx_vpermil2<mode>3): Likewise.\n+\t(avx_vperm2f128<mode>3): Likewise.\n+\t(avx_vbroadcasts<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n+\t(avx_vbroadcastss256): Likewise.\n+\t(avx_vbroadcastf128_p<avxmodesuffixf2c>256): Likewise.\n+\t(avx_vinsertf128<mode>): Likewise.\n+\t(vec_set_lo_<mode>): Likewise.\n+\t(vec_set_hi_<mode>): Likewise.\n+\t(vec_set_lo_<mode>): Likewise.\n+\t(vec_set_hi_<mode>): Likewise.\n+\t(vec_set_lo_v16hi): Likewise.\n+\t(vec_set_hi_v16hi): Likewise.\n+\t(vec_set_lo_v32qi): Likewise.\n+\t(vec_set_hi_v32qi): Likewise.\n+\t(avx_maskloadp<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n+\t(avx_maskstorep<avxmodesuffixf2c><avxmodesuffix>): Likewise.\n+\t(avx_<avxmodesuffixp><avxmodesuffix>_<avxmodesuffixp>): Likewise.\n+\t(avx_<avxmodesuffixp>_<avxmodesuffixp><avxmodesuffix>): Likewise.\n+\t(vec_init<mode>): Likewise.\n+\t(*vec_concat<mode>_avx): Likewise.\n+\t(blendbits): Support V8SF and V4DF.\n+\t(sse2_movq128): Support AVX.\n+\t(<sse>_movnt<mode>): Likewise.\n+\t(sse2_movntv2di): Likewise.\n+\t(sse_rcpv4sf2): Likewise.\n+\t(sse_sqrtv4sf2): Likewise.\n+\t(sse_rsqrtv4sf2): Likewise.\n+\t(<sse>_comi): Likewise.\n+\t(<sse>_ucomi): Likewise.\n+\t(sse_cvtss2si): Likewise.\n+\t(sse_cvtss2si_2): Likewise.\n+\t(sse_cvtss2siq): Likewise.\n+\t(sse_cvtss2siq_2): Likewise.\n+\t(sse_cvttss2si): Likewise.\n+\t(sse_cvttss2siq): Likewise.\n+\t(sse2_cvtsd2si): Likewise.\n+\t(sse2_cvtsd2si_2): Likewise.\n+\t(sse2_cvtsd2siq): Likewise.\n+\t(sse2_cvtsd2siq_2): Likewise.\n+\t(sse2_cvttsd2si): Likewise.\n+\t(sse2_cvttsd2siq): Likewise.\n+\t(sse2_cvtdq2pd): Likewise.\n+\t(*sse2_cvtpd2dq): Likewise.\n+\t(*sse2_cvttpd2dq): Likewise.\n+\t(*sse2_cvtpd2ps): Likewise.\n+\t(sse2_cvtps2pd): Likewise.\n+\t(sse3_movshdup): Likewise.\n+\t(sse3_movsldup): Likewise.\n+\t(sse_storehps): Likewise.\n+\t(*sse4_1_extractps): Likewise.\n+\t(sse2_storelpd): Likewise.\n+\t(vec_dupv2df_sse3): Likewise.\n+\t(*vec_concatv2df_sse3): Likewise.\n+\t(*sse4_1_pextrb): Likewise.\n+\t(*sse4_1_pextrb_memory): Likewise.\n+\t(*sse2_pextrw): Likewise.\n+\t(*sse4_1_pextrw_memory): Likewise.\n+\t(*sse4_1_pextrd): Likewise.\n+\t(*sse4_1_pextrq): Likewise.\n+\t(sse2_pshufd_1): Likewise.\n+\t(sse2_pshuflw_1): Likewise.\n+\t(sse2_pshufhw_1): Likewise.\n+\t(*sse2_storeq_rex64): Likewise.\n+\t(*vec_dupv4si): Likewise.\n+\t(<sse>_movmskp<ssemodesuffixf2c>): Likewise.\n+\t(sse2_pmovmskb): Likewise.\n+\t(*sse2_maskmovdqu): Likewise.\n+\t(*sse2_maskmovdqu_rex64): Likewise.\n+\t(sse_ldmxcsr): Likewise.\n+\t(sse_stmxcsr): Likewise.\n+\t(abs<mode>2): Likewise.\n+\t(sse4_1_movntdqa): Likewise.\n+\t(sse4_1_phminposuw): Likewise.\n+\t(sse4_1_extendv8qiv8hi2): Likewise.\n+\t(*sse4_1_extendv8qiv8hi2): Likewise.\n+\t(sse4_1_extendv4qiv4si2): Likewise.\n+\t(*sse4_1_extendv4qiv4si2): Likewise.\n+\t(sse4_1_extendv2qiv2di2): Likewise.\n+\t(*sse4_1_extendv2qiv2di2): Likewise.\n+\t(sse4_1_extendv4hiv4si2): Likewise.\n+\t(*sse4_1_extendv4hiv4si2): Likewise.\n+\t(sse4_1_extendv2hiv2di2): Likewise.\n+\t(*sse4_1_extendv2hiv2di2): Likewise.\n+\t(sse4_1_extendv2siv2di2): Likewise.\n+\t(*sse4_1_extendv2siv2di2): Likewise.\n+\t(sse4_1_zero_extendv8qiv8hi2): Likewise.\n+\t(*sse4_1_zero_extendv8qiv8hi2): Likewise.\n+\t(sse4_1_zero_extendv4qiv4si2): Likewise.\n+\t(*sse4_1_zero_extendv4qiv4si2): Likewise.\n+\t(sse4_1_zero_extendv2qiv2di2): Likewise.\n+\t(*sse4_1_zero_extendv2qiv2di2): Likewise.\n+\t(sse4_1_zero_extendv4hiv4si2): Likewise.\n+\t(*sse4_1_zero_extendv4hiv4si2): Likewise.\n+\t(sse4_1_zero_extendv2hiv2di2): Likewise.\n+\t(*sse4_1_zero_extendv2hiv2di2): Likewise.\n+\t(sse4_1_zero_extendv2siv2di2): Likewise.\n+\t(*sse4_1_zero_extendv2siv2di2): Likewise.\n+\t(sse4_1_ptest): Likewise.\n+\t(sse4_1_roundp<ssemodesuffixf2c>): Likewise.\n+\t(sse4_2_pcmpestri): Likewise.\n+\t(sse4_2_pcmpestrm): Likewise.\n+\t(sse4_2_pcmpistri): Likewise.\n+\t(sse4_2_pcmpistrm): Likewise.\n+\t(aesimc): Likewise.\n+\t(aeskeygenassist): Likewise.\n+\n+2008-08-28  Uros Bizjak  <ubizjak@gmail.com>\n+\n+\t* config/i386/predicates.md (vzeroall_operation): New.\n+\n+\t* config/i386/sse.md (avx_vzeroall): New.\n+\t(*avx_vzeroall): Likewise.\n+\n 2008-08-28  Paul Brook  <paul@codesourcery.com>\n-\tMark Shinwell  <shinwell@codesourcery.com>\n-\tRichard Earnshaw  <richard.earnshaw@arm.com>\n+\t    Mark Shinwell  <shinwell@codesourcery.com>\n+\t    Richard Earnshaw  <richard.earnshaw@arm.com>\n \n \t* config/arm/arm.c (TARGET_MAX_ANCHOR_OFFSET): New.\n \t(TARGET_MIN_ANCHOR_OFFSET): New.\n@@ -94,7 +771,7 @@\n \n 2008-08-28  Chris Fairles  <chris.fairles@gmail.com>\n \n-        * gthr-posix.h (__gthread_create,  __gthread_join, __gthread_detach,\n+\t* gthr-posix.h (__gthread_create,  __gthread_join, __gthread_detach,\n \t__gthread_mutex_timed_lock, __gthread_recursive_mutex_timed_lock,\n \t__gthread_cond_signal, __gthread_cond_timedwait,\n \t__gthread_cond_timedwait_recursive): New functions.\n@@ -522,8 +1199,8 @@\n \n 2008-08-24  Razya Ladelsky  <razya@il.ibm.com>\n \n-        PR tree-optimization/37185\n-        * matrix-reorg.c (transform_access_sites): Update changed stmt.\n+\tPR tree-optimization/37185\n+\t* matrix-reorg.c (transform_access_sites): Update changed stmt.\n \n 2008-08-23  Jan Hubicka  <jh@suse.cz>\n "}, {"sha": "877761bfe7853a98f21814a7f39f7c274ca85524", "filename": "gcc/config.gcc", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig.gcc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig.gcc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig.gcc?ref=95879c728b9a59ae67db022ad370eb66374090f3", "patch": "@@ -299,7 +299,7 @@ i[34567]86-*-*)\n \textra_headers=\"cpuid.h mmintrin.h mm3dnow.h xmmintrin.h emmintrin.h\n \t\t       pmmintrin.h tmmintrin.h ammintrin.h smmintrin.h\n \t\t       nmmintrin.h bmmintrin.h mmintrin-common.h\n-\t\t       wmmintrin.h cross-stdarg.h\"\n+\t\t       wmmintrin.h gmmintrin.h cross-stdarg.h\"\n \t;;\n x86_64-*-*)\n \tcpu_type=i386\n@@ -308,7 +308,7 @@ x86_64-*-*)\n \textra_headers=\"cpuid.h mmintrin.h mm3dnow.h xmmintrin.h emmintrin.h\n \t\t       pmmintrin.h tmmintrin.h ammintrin.h smmintrin.h\n \t\t       nmmintrin.h bmmintrin.h mmintrin-common.h\n-\t\t       wmmintrin.h cross-stdarg.h\"\n+\t\t       wmmintrin.h gmmintrin.h cross-stdarg.h\"\n \tneed_64bit_hwint=yes\n \t;;\n ia64-*-*)"}, {"sha": "90a28137afb86c24c28c9e302fe0427a5b26906c", "filename": "gcc/config/i386/cpuid.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fcpuid.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fcpuid.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fcpuid.h?ref=95879c728b9a59ae67db022ad370eb66374090f3", "patch": "@@ -35,11 +35,15 @@\n #define bit_SSE3\t(1 << 0)\n #define bit_PCLMUL\t(1 << 1)\n #define bit_SSSE3\t(1 << 9)\n+#define bit_FMA\t\t(1 << 12)\n #define bit_CMPXCHG16B\t(1 << 13)\n #define bit_SSE4_1\t(1 << 19)\n #define bit_SSE4_2\t(1 << 20)\n #define bit_POPCNT\t(1 << 23)\n #define bit_AES\t\t(1 << 25)\n+#define bit_XSAVE\t(1 << 26)\n+#define bit_OSXSAVE\t(1 << 27)\n+#define bit_AVX\t\t(1 << 28)\n \n /* %edx */\n #define bit_CMPXCHG8B\t(1 << 8)"}, {"sha": "bf8ac4833383bd09cb9fb40ec45420ec23b52310", "filename": "gcc/config/i386/gas.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fgas.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fgas.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fgas.h?ref=95879c728b9a59ae67db022ad370eb66374090f3", "patch": "@@ -86,6 +86,7 @@ along with GCC; see the file COPYING3.  If not see\n    GAS version 1.38.1 doesn't understand the `repz' opcode mnemonic.\n    So use `repe' instead.  */\n \n+#undef ASM_OUTPUT_OPCODE\n #define ASM_OUTPUT_OPCODE(STREAM, PTR)\t\\\n {\t\t\t\t\t\t\t\t\t\\\n   if ((PTR)[0] == 'r'\t\t\t\t\t\t\t\\\n@@ -103,6 +104,8 @@ along with GCC; see the file COPYING3.  If not see\n \t  (PTR) += 5;\t\t\t\t\t\t\t\\\n \t}\t\t\t\t\t\t\t\t\\\n     }\t\t\t\t\t\t\t\t\t\\\n+  else\t\t\t\t\t\t\t\t\t\\\n+    ASM_OUTPUT_AVX_PREFIX ((STREAM), (PTR));\t\t\t\t\\\n }\n \n /* Define macro used to output shift-double opcodes when the shift"}, {"sha": "1c6bb18be83fff41b7f3d07b3242687aa3088104", "filename": "gcc/config/i386/gmmintrin.h", "status": "added", "additions": 1482, "deletions": 0, "changes": 1482, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fgmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fgmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fgmmintrin.h?ref=95879c728b9a59ae67db022ad370eb66374090f3", "patch": "@@ -0,0 +1,1482 @@\n+/* Copyright (C) 2008 Free Software Foundation, Inc.\n+\n+   This file is part of GCC.\n+\n+   GCC is free software; you can redistribute it and/or modify\n+   it under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 2, or (at your option)\n+   any later version.\n+\n+   GCC is distributed in the hope that it will be useful,\n+   but WITHOUT ANY WARRANTY; without even the implied warranty of\n+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+   GNU General Public License for more details.\n+\n+   You should have received a copy of the GNU General Public License\n+   along with GCC; see the file COPYING.  If not, write to\n+   the Free Software Foundation, 59 Temple Place - Suite 330,\n+   Boston, MA 02111-1307, USA.  */\n+\n+/* As a special exception, if you include this header file into source\n+   files compiled by GCC, this header file does not by itself cause\n+   the resulting executable to be covered by the GNU General Public\n+   License.  This exception does not however invalidate any other\n+   reasons why the executable file might be covered by the GNU General\n+   Public License.  */\n+\n+/* Implemented from the specification included in the Intel C++ Compiler\n+   User Guide and Reference, version 11.0.  */\n+\n+#ifndef _GMMINTRIN_H_INCLUDED\n+#define _GMMINTRIN_H_INCLUDED\n+\n+#ifndef __AVX__\n+# error \"AVX instruction set not enabled\"\n+#else\n+\n+/* We need definitions from the SSE4, SSSE3, SSE3, SSE2 and SSE header\n+   files.  */\n+#include <smmintrin.h>\n+\n+/* Internal data types for implementing the intrinsics.  */\n+typedef double __v4df __attribute__ ((__vector_size__ (32)));\n+typedef float __v8sf __attribute__ ((__vector_size__ (32)));\n+typedef long long __v4di __attribute__ ((__vector_size__ (32)));\n+typedef int __v8si __attribute__ ((__vector_size__ (32)));\n+typedef short __v16hi __attribute__ ((__vector_size__ (32)));\n+typedef char __v32qi __attribute__ ((__vector_size__ (32)));\n+\n+/* The Intel API is flexible enough that we must allow aliasing with other\n+   vector types, and their scalar components.  */\n+typedef float __m256 __attribute__ ((__vector_size__ (32),\n+\t\t\t\t     __may_alias__));\n+typedef long long __m256i __attribute__ ((__vector_size__ (32),\n+\t\t\t\t\t  __may_alias__));\n+typedef double __m256d __attribute__ ((__vector_size__ (32),\n+\t\t\t\t       __may_alias__));\n+\n+/* Compare predicates for scalar and packed compare intrinsics.  */\n+\n+/* Equal (ordered, non-signaling)  */\n+#define _CMP_EQ_OQ\t0x00\n+/* Less-than (ordered, signaling)  */\n+#define _CMP_LT_OS\t0x01\n+/* Less-than-or-equal (ordered, signaling)  */\n+#define _CMP_LE_OS\t0x02\n+/* Unordered (non-signaling)  */\n+#define _CMP_UNORD_Q\t0x03\n+/* Not-equal (unordered, non-signaling)  */\n+#define _CMP_NEQ_UQ\t0x04\n+/* Not-less-than (unordered, signaling)  */\n+#define _CMP_NLT_US\t0x05\n+/* Not-less-than-or-equal (unordered, signaling)  */\n+#define _CMP_NLE_US\t0x06\n+/* Ordered (nonsignaling)   */\n+#define _CMP_ORD_Q\t0x07\n+/* Equal (unordered, non-signaling)  */\n+#define _CMP_EQ_UQ\t0x08\n+/* Not-greater-than-or-equal (unordered, signaling)  */\n+#define _CMP_NGE_US\t0x09\n+/* Not-greater-than (unordered, signaling)  */\n+#define _CMP_NGT_US\t0x0a\n+/* False (ordered, non-signaling)  */\n+#define _CMP_FALSE_OQ\t0x0b\n+/* Not-equal (ordered, non-signaling)  */\n+#define _CMP_NEQ_OQ\t0x0c\n+/* Greater-than-or-equal (ordered, signaling)  */\n+#define _CMP_GE_OS\t0x0d\n+/* Greater-than (ordered, signaling)  */\n+#define _CMP_GT_OS\t0x0e\n+/* True (unordered, non-signaling)  */\n+#define _CMP_TRUE_UQ\t0x0f\n+/* Equal (ordered, signaling)  */\n+#define _CMP_EQ_OS\t0x10\n+/* Less-than (ordered, non-signaling)  */\n+#define _CMP_LT_OQ\t0x11\n+/* Less-than-or-equal (ordered, non-signaling)  */\n+#define _CMP_LE_OQ\t0x12\n+/* Unordered (signaling)  */\n+#define _CMP_UNORD_S\t0x13\n+/* Not-equal (unordered, signaling)  */\n+#define _CMP_NEQ_US\t0x14\n+/* Not-less-than (unordered, non-signaling)  */\n+#define _CMP_NLT_UQ\t0x15\n+/* Not-less-than-or-equal (unordered, non-signaling)  */\n+#define _CMP_NLE_UQ\t0x16\n+/* Ordered (signaling)  */\n+#define _CMP_ORD_S\t0x17\n+/* Equal (unordered, signaling)  */\n+#define _CMP_EQ_US\t0x18\n+/* Not-greater-than-or-equal (unordered, non-signaling)  */\n+#define _CMP_NGE_UQ\t0x19\n+/* Not-greater-than (unordered, non-signaling)  */\n+#define _CMP_NGT_UQ\t0x1a\n+/* False (ordered, signaling)  */\n+#define _CMP_FALSE_OS\t0x1b\n+/* Not-equal (ordered, signaling)  */\n+#define _CMP_NEQ_OS\t0x1c\n+/* Greater-than-or-equal (ordered, non-signaling)  */\n+#define _CMP_GE_OQ\t0x1d\n+/* Greater-than (ordered, non-signaling)  */\n+#define _CMP_GT_OQ\t0x1e\n+/* True (unordered, signaling)  */\n+#define _CMP_TRUE_US\t0x1f\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_add_pd (__m256d __A, __m256d __B)\n+{\n+  return (__m256d) __builtin_ia32_addpd256 ((__v4df)__A, (__v4df)__B);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_add_ps (__m256 __A, __m256 __B)\n+{\n+  return (__m256) __builtin_ia32_addps256 ((__v8sf)__A, (__v8sf)__B);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_addsub_pd (__m256d __A, __m256d __B)\n+{\n+  return (__m256d) __builtin_ia32_addsubpd256 ((__v4df)__A, (__v4df)__B);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_addsub_ps (__m256 __A, __m256 __B)\n+{\n+  return (__m256) __builtin_ia32_addsubps256 ((__v8sf)__A, (__v8sf)__B);\n+}\n+\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_and_pd (__m256d __A, __m256d __B)\n+{\n+  return (__m256d) __builtin_ia32_andpd256 ((__v4df)__A, (__v4df)__B);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_and_ps (__m256 __A, __m256 __B)\n+{\n+  return (__m256) __builtin_ia32_andps256 ((__v8sf)__A, (__v8sf)__B);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_andnot_pd (__m256d __A, __m256d __B)\n+{\n+  return (__m256d) __builtin_ia32_andnpd256 ((__v4df)__A, (__v4df)__B);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_andnot_ps (__m256 __A, __m256 __B)\n+{\n+  return (__m256) __builtin_ia32_andnps256 ((__v8sf)__A, (__v8sf)__B);\n+}\n+\n+/* Double/single precision floating point blend instructions - select\n+   data from 2 sources using constant/variable mask.  */\n+\n+#ifdef __OPTIMIZE__\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_blend_pd (__m256d __X, __m256d __Y, const int __M)\n+{\n+  return (__m256d) __builtin_ia32_blendpd256 ((__v4df)__X,\n+\t\t\t\t\t      (__v4df)__Y,\n+\t\t\t\t\t      __M);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_blend_ps (__m256 __X, __m256 __Y, const int __M)\n+{\n+  return (__m256) __builtin_ia32_blendps256 ((__v8sf)__X,\n+\t\t\t\t\t     (__v8sf)__Y,\n+\t\t\t\t\t     __M);\n+}\n+#else\n+#define _mm256_blend_pd(X, Y, M)\t\t\t\t\t\\\n+  ((__m256d) __builtin_ia32_blendpd256 ((__v4df)(__m256d)(X),\t\t\\\n+\t\t\t\t\t(__v4df)(__m256d)(Y), (int)(M)))\n+\n+#define _mm256_blend_ps(X, Y, M)\t\t\t\t\t\\\n+  ((__m256) __builtin_ia32_blendps256 ((__v8sf)(__m256)(X),\t\t\\\n+\t\t\t\t       (__v8sf)(__m256)(Y), (int)(M)))\n+#endif\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_blendv_pd (__m256d __X, __m256d __Y, __m256d __M)\n+{\n+  return (__m256d) __builtin_ia32_blendvpd256 ((__v4df)__X,\n+\t\t\t\t\t       (__v4df)__Y,\n+\t\t\t\t\t       (__v4df)__M);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_blendv_ps (__m256 __X, __m256 __Y, __m256 __M)\n+{\n+  return (__m256) __builtin_ia32_blendvps256 ((__v8sf)__X,\n+\t\t\t\t\t      (__v8sf)__Y,\n+\t\t\t\t\t      (__v8sf)__M);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_div_pd (__m256d __A, __m256d __B)\n+{\n+  return (__m256d) __builtin_ia32_divpd256 ((__v4df)__A, (__v4df)__B);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_div_ps (__m256 __A, __m256 __B)\n+{\n+  return (__m256) __builtin_ia32_divps256 ((__v8sf)__A, (__v8sf)__B);\n+}\n+\n+/* Dot product instructions with mask-defined summing and zeroing parts\n+   of result.  */\n+\n+#ifdef __OPTIMIZE__\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_dp_ps (__m256 __X, __m256 __Y, const int __M)\n+{\n+  return (__m256) __builtin_ia32_dpps256 ((__v8sf)__X,\n+\t\t\t\t\t  (__v8sf)__Y,\n+\t\t\t\t\t  __M);\n+}\n+#else\n+#define _mm256_dp_ps(X, Y, M)\t\t\t\t\t\t\\\n+  ((__m256) __builtin_ia32_dpps256 ((__v8sf)(__m256)(X),\t\t\\\n+\t\t\t\t    (__v8sf)(__m256)(Y), (int)(M)))\n+#endif\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_hadd_pd (__m256d __X, __m256d __Y)\n+{\n+  return (__m256d) __builtin_ia32_haddpd256 ((__v4df)__X, (__v4df)__Y);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_hadd_ps (__m256 __X, __m256 __Y)\n+{\n+  return (__m256) __builtin_ia32_haddps256 ((__v8sf)__X, (__v8sf)__Y);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_hsub_pd (__m256d __X, __m256d __Y)\n+{\n+  return (__m256d) __builtin_ia32_hsubpd256 ((__v4df)__X, (__v4df)__Y);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_hsub_ps (__m256 __X, __m256 __Y)\n+{\n+  return (__m256) __builtin_ia32_hsubps256 ((__v8sf)__X, (__v8sf)__Y);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_max_pd (__m256d __A, __m256d __B)\n+{\n+  return (__m256d) __builtin_ia32_maxpd256 ((__v4df)__A, (__v4df)__B);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_max_ps (__m256 __A, __m256 __B)\n+{\n+  return (__m256) __builtin_ia32_maxps256 ((__v8sf)__A, (__v8sf)__B);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_min_pd (__m256d __A, __m256d __B)\n+{\n+  return (__m256d) __builtin_ia32_minpd256 ((__v4df)__A, (__v4df)__B);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_min_ps (__m256 __A, __m256 __B)\n+{\n+  return (__m256) __builtin_ia32_minps256 ((__v8sf)__A, (__v8sf)__B);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_mul_pd (__m256d __A, __m256d __B)\n+{\n+  return (__m256d) __builtin_ia32_mulpd256 ((__v4df)__A, (__v4df)__B);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_mul_ps (__m256 __A, __m256 __B)\n+{\n+  return (__m256) __builtin_ia32_mulps256 ((__v8sf)__A, (__v8sf)__B);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_or_pd (__m256d __A, __m256d __B)\n+{\n+  return (__m256d) __builtin_ia32_orpd256 ((__v4df)__A, (__v4df)__B);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_or_ps (__m256 __A, __m256 __B)\n+{\n+  return (__m256) __builtin_ia32_orps256 ((__v8sf)__A, (__v8sf)__B);\n+}\n+\n+#ifdef __OPTIMIZE__\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_shuffle_pd (__m256d __A, __m256d __B, const int __mask)\n+{\n+  return (__m256d) __builtin_ia32_shufpd256 ((__v4df)__A, (__v4df)__B,\n+\t\t\t\t\t     __mask);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_shuffle_ps (__m256 __A, __m256 __B, const int __mask)\n+{\n+  return (__m256) __builtin_ia32_shufps256 ((__v8sf)__A, (__v8sf)__B,\n+\t\t\t\t\t    __mask);\n+}\n+#else\n+#define _mm256_shuffle_pd(A, B, N)\t\t\t\t\t\\\n+  ((__m256d)__builtin_ia32_shufpd256 ((__v4df)(__m256d)(A),\t\t\\\n+\t\t\t\t      (__v4df)(__m256d)(B), (int)(N)))\n+\n+#define _mm256_shuffle_ps(A, B, N)\t\t\t\t\t\\\n+  ((__m256) __builtin_ia32_shufps256 ((__v8sf)(__m256)(A),\t\t\\\n+\t\t\t\t      (__v8sf)(__m256)(B), (int)(N)))\n+#endif\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_sub_pd (__m256d __A, __m256d __B)\n+{\n+  return (__m256d) __builtin_ia32_subpd256 ((__v4df)__A, (__v4df)__B);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_sub_ps (__m256 __A, __m256 __B)\n+{\n+  return (__m256) __builtin_ia32_subps256 ((__v8sf)__A, (__v8sf)__B);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_xor_pd (__m256d __A, __m256d __B)\n+{\n+  return (__m256d) __builtin_ia32_xorpd256 ((__v4df)__A, (__v4df)__B);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_xor_ps (__m256 __A, __m256 __B)\n+{\n+  return (__m256) __builtin_ia32_xorps256 ((__v8sf)__A, (__v8sf)__B);\n+}\n+\n+#ifdef __OPTIMIZE__\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_cmp_pd (__m128d __X, __m128d __Y, const int __P)\n+{\n+  return (__m128d) __builtin_ia32_cmppd ((__v2df)__X, (__v2df)__Y, __P);\n+}\n+\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_cmp_ps (__m128 __X, __m128 __Y, const int __P)\n+{\n+  return (__m128) __builtin_ia32_cmpps ((__v4sf)__X, (__v4sf)__Y, __P);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_cmp_pd (__m256d __X, __m256d __Y, const int __P)\n+{\n+  return (__m256d) __builtin_ia32_cmppd256 ((__v4df)__X, (__v4df)__Y,\n+\t\t\t\t\t    __P);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_cmp_ps (__m256 __X, __m256 __Y, const int __P)\n+{\n+  return (__m256) __builtin_ia32_cmpps256 ((__v8sf)__X, (__v8sf)__Y,\n+\t\t\t\t\t   __P);\n+}\n+\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_cmp_sd (__m128d __X, __m128d __Y, const int __P)\n+{\n+  return (__m128d) __builtin_ia32_cmpsd ((__v2df)__X, (__v2df)__Y, __P);\n+}\n+\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_cmp_ss (__m128 __X, __m128 __Y, const int __P)\n+{\n+  return (__m128) __builtin_ia32_cmpss ((__v4sf)__X, (__v4sf)__Y, __P);\n+}\n+#else\n+#define _mm_cmp_pd(X, Y, P)\t\t\t\t\t\t\\\n+  ((__m128d) __builtin_ia32_cmppd ((__v2df)(__m128d)(X),\t\t\\\n+\t\t\t\t   (__v2df)(__m128d)(Y), (int)(P)))\n+\n+#define _mm_cmp_ps(X, Y, P)\t\t\t\t\t\t\\\n+  ((__m128) __builtin_ia32_cmpps ((__v4sf)(__m128)(X),\t\t\t\\\n+\t\t\t\t  (__v4sf)(__m128)(Y), (int)(P)))\n+\n+#define _mm256_cmp_pd(X, Y, P)\t\t\t\t\t\t\\\n+  ((__m256d) __builtin_ia32_cmppd256 ((__v4df)(__m256d)(X),\t\t\\\n+\t\t\t\t      (__v4df)(__m256d)(Y), (int)(P)))\n+\n+#define _mm256_cmp_ps(X, Y, P)\t\t\t\t\t\t\\\n+  ((__m256) __builtin_ia32_cmpps256 ((__v8sf)(__m256)(X),\t\t\\\n+\t\t\t\t     (__v8sf)(__m256)(Y), (int)(P)))\n+\n+#define _mm_cmp_sd(X, Y, P)\t\t\t\t\t\t\\\n+  ((__m128d) __builtin_ia32_cmpsd ((__v2df)(__m128d)(X),\t\t\\\n+\t\t\t\t   (__v2df)(__m128d)(Y), (int)(P)))\n+\n+#define _mm_cmp_ss(X, Y, P)\t\t\t\t\t\t\\\n+  ((__m128) __builtin_ia32_cmpss ((__v4sf)(__m128)(X),\t\t\t\\\n+\t\t\t\t  (__v4sf)(__m128)(Y), (int)(P)))\n+#endif\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_cvtepi32_pd (__m128i __A)\n+{\n+  return (__m256d)__builtin_ia32_cvtdq2pd256 ((__v4si) __A);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_cvtepi32_ps (__m256i __A)\n+{\n+  return (__m256)__builtin_ia32_cvtdq2ps256 ((__v8si) __A);\n+}\n+\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_cvtpd_ps (__m256d __A)\n+{\n+  return (__m128)__builtin_ia32_cvtpd2ps256 ((__v4df) __A);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_cvtps_epi32 (__m256 __A)\n+{\n+  return (__m256i)__builtin_ia32_cvtps2dq256 ((__v8sf) __A);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_cvtps_pd (__m128 __A)\n+{\n+  return (__m256d)__builtin_ia32_cvtps2pd256 ((__v4sf) __A);\n+}\n+\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_cvttpd_epi32 (__m256d __A)\n+{\n+  return (__m128i)__builtin_ia32_cvttpd2dq256 ((__v4df) __A);\n+}\n+\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_cvtpd_epi32 (__m256d __A)\n+{\n+  return (__m128i)__builtin_ia32_cvtpd2dq256 ((__v4df) __A);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_cvttps_epi32 (__m256 __A)\n+{\n+  return (__m256i)__builtin_ia32_cvttps2dq256 ((__v8sf) __A);\n+}\n+\n+#ifdef __OPTIMIZE__\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_extractf128_pd (__m256d __X, const int __N)\n+{\n+  return (__m128d) __builtin_ia32_vextractf128_pd256 ((__v4df)__X, __N);\n+}\n+\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_extractf128_ps (__m256 __X, const int __N)\n+{\n+  return (__m128) __builtin_ia32_vextractf128_ps256 ((__v8sf)__X, __N);\n+}\n+\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_extractf128_si256 (__m256i __X, const int __N)\n+{\n+  return (__m128i) __builtin_ia32_vextractf128_si256 ((__v8si)__X, __N);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_extract_epi32 (__m256i __X, int const __N)\n+{\n+  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 2);\n+  return _mm_extract_epi32 (__Y, __N % 4);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_extract_epi16 (__m256i __X, int const __N)\n+{\n+  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 3);\n+  return _mm_extract_epi16 (__Y, __N % 8);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_extract_epi8 (__m256i __X, int const __N)\n+{\n+  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 4);\n+  return _mm_extract_epi8 (__Y, __N % 16);\n+}\n+\n+#ifdef __x86_64__\n+extern __inline long long  __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_extract_epi64 (__m256i __X, const int __N)\n+{\n+  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 1);\n+  return _mm_extract_epi64 (__Y, __N % 2);\n+}\n+#endif\n+#else\n+#define _mm256_extractf128_pd(X, N)\t\t\t\t\t\\\n+  ((__m128d) __builtin_ia32_vextractf128_pd256 ((__v4df)(__m256d)(X),\t\\\n+\t\t\t\t\t\t(int)(N)))\n+\n+#define _mm256_extractf128_ps(X, N)\t\t\t\t\t\\\n+  ((__m128) __builtin_ia32_vextractf128_ps256 ((__v8sf)(__m256)(X),\t\\\n+\t\t\t\t\t       (int)(N)))\n+\n+#define _mm256_extractf128_si256(X, N)\t\t\t\t\t\\\n+  ((__m128i) __builtin_ia32_vextractf128_si256 ((__v8si)(__m256i)(X),\t\\\n+\t\t\t\t\t\t(int)(N)))\n+\n+#define _mm256_extract_epi32(X, N)\t\t\t\t\t\\\n+  (__extension__\t\t\t\t\t\t\t\\\n+   ({\t\t\t\t\t\t\t\t\t\\\n+      __m128i __Y = _mm256_extractf128_si256 ((X), (N) >> 2);\t\t\\\n+      _mm_extract_epi32 (__Y, (N) % 4);\t\t\t\t\t\\\n+    }))\n+\n+#define _mm256_extract_epi16(X, N)\t\t\t\t\t\\\n+  (__extension__\t\t\t\t\t\t\t\\\n+   ({\t\t\t\t\t\t\t\t\t\\\n+      __m128i __Y = _mm256_extractf128_si256 ((X), (N) >> 3);\t\t\\\n+      _mm_extract_epi16 (__Y, (N) % 8);\t\t\t\t\t\\\n+    }))\n+\n+#define _mm256_extract_epi8(X, N)\t\t\t\t\t\\\n+  (__extension__\t\t\t\t\t\t\t\\\n+   ({\t\t\t\t\t\t\t\t\t\\\n+      __m128i __Y = _mm256_extractf128_si256 ((X), (N) >> 4);\t\t\\\n+      _mm_extract_epi8 (__Y, (N) % 16);\t\t\t\t\t\\\n+    }))\n+\n+#ifdef __x86_64__\n+#define _mm256_extract_epi64(X, N)\t\t\t\t\t\\\n+  (__extension__\t\t\t\t\t\t\t\\\n+   ({\t\t\t\t\t\t\t\t\t\\\n+      __m128i __Y = _mm256_extractf128_si256 ((X), (N) >> 1);\t\t\\\n+      _mm_extract_epi64 (__Y, (N) % 2);\t\t\t\t\t\\\n+    }))\n+#endif\n+#endif\n+\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_zeroall (void)\n+{\n+  __builtin_ia32_vzeroall ();\n+}\n+\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_zeroupper (void)\n+{\n+  __builtin_ia32_vzeroupper ();\n+}\n+\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_permutevar_pd (__m128d __A, __m128i __C)\n+{\n+  return (__m128d) __builtin_ia32_vpermilvarpd ((__v2df)__A,\n+\t\t\t\t\t\t(__v2di)__C);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_permutevar_pd (__m256d __A, __m256i __C)\n+{\n+  return (__m256d) __builtin_ia32_vpermilvarpd256 ((__v4df)__A,\n+\t\t\t\t\t\t   (__v4di)__C);\n+}\n+\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_permutevar_ps (__m128 __A, __m128i __C)\n+{\n+  return (__m128) __builtin_ia32_vpermilvarps ((__v4sf)__A,\n+\t\t\t\t\t       (__v4si)__C);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_permutevar_ps (__m256 __A, __m256i __C)\n+{\n+  return (__m256) __builtin_ia32_vpermilvarps256 ((__v8sf)__A,\n+\t\t\t\t\t\t  (__v8si)__C);\n+}\n+\n+#ifdef __OPTIMIZE__\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_permute_pd (__m128d __X, const int __C)\n+{\n+  return (__m128d) __builtin_ia32_vpermilpd ((__v2df)__X, __C);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_permute_pd (__m256d __X, const int __C)\n+{\n+  return (__m256d) __builtin_ia32_vpermilpd256 ((__v4df)__X, __C);\n+}\n+\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_permute_ps (__m128 __X, const int __C)\n+{\n+  return (__m128) __builtin_ia32_vpermilps ((__v4sf)__X, __C);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_permute_ps (__m256 __X, const int __C)\n+{\n+  return (__m256) __builtin_ia32_vpermilps256 ((__v8sf)__X, __C);\n+}\n+\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_permute2_pd (__m128d __X, __m128d __Y, __m128i __C, const int __I)\n+{\n+  return (__m128d) __builtin_ia32_vpermil2pd ((__v2df)__X,\n+\t\t\t\t\t      (__v2df)__Y,\n+\t\t\t\t\t      (__v2di)__C,\n+\t\t\t\t\t      __I);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_permute2_pd (__m256d __X, __m256d __Y, __m256i __C, const int __I)\n+{\n+  return (__m256d) __builtin_ia32_vpermil2pd256 ((__v4df)__X,\n+\t\t\t\t\t\t (__v4df)__Y,\n+\t\t\t\t\t\t (__v4di)__C,\n+\t\t\t\t\t\t __I);\n+}\n+\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_permute2_ps (__m128 __X, __m128 __Y, __m128i __C, const int __I)\n+{\n+  return (__m128) __builtin_ia32_vpermil2ps ((__v4sf)__X,\n+\t\t\t\t\t     (__v4sf)__Y,\n+\t\t\t\t\t     (__v4si)__C,\n+\t\t\t\t\t     __I);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_permute2_ps (__m256 __X, __m256 __Y, __m256i __C, const int __I)\n+{\n+  return (__m256) __builtin_ia32_vpermil2ps256 ((__v8sf)__X,\n+\t\t\t\t\t\t(__v8sf)__Y,\n+\t\t\t\t\t\t(__v8si)__C,\n+\t\t\t\t\t\t__I);\n+}\n+#else\n+#define _mm_permute_pd(X, C)\t\t\t\t\t\t\\\n+  ((__m128d) __builtin_ia32_vpermilpd ((__v2df)(__m128d)(X), (int)(C)))\n+\n+#define _mm256_permute_pd(X, C)\t\t\t\t\t\t\\\n+  ((__m256d) __builtin_ia32_vpermilpd256 ((__v4df)(__m256d)(X),\t(int)(C)))\n+\n+#define _mm_permute_ps(X, C)\t\t\t\t\t\t\\\n+  ((__m128) __builtin_ia32_vpermilps ((__v4sf)(__m128)(X), (int)(C)))\n+\n+#define _mm256_permute_ps(X, C)\t\t\t\t\t\t\\\n+  ((__m256) __builtin_ia32_vpermilps256 ((__v8sf)(__m256)(X), (int)(C)))\n+\n+#define _mm_permute2_pd(X, Y, C, I)\t\t\t\t\t\\\n+  ((__m128d) __builtin_ia32_vpermil2pd ((__v2df)(__m128d)(X),\t\t\\\n+\t\t\t\t\t(__v2df)(__m128d)(Y),\t\t\\\n+\t\t\t\t\t(__v2di)(__m128d)(C),\t\t\\\n+\t\t\t\t\t(int)(I)))\n+\n+#define _mm256_permute2_pd(X, Y, C, I)\t\t\t\t\t\\\n+  ((__m256d) __builtin_ia32_vpermil2pd256 ((__v4df)(__m256d)(X),\t\\\n+\t\t\t\t\t   (__v4df)(__m256d)(Y),\t\\\n+\t\t\t\t\t   (__v4di)(__m256d)(C),\t\\\n+\t\t\t\t\t   (int)(I)))\n+\n+#define _mm_permute2_ps(X, Y, C, I)\t\t\t\t\t\\\n+  ((__m128) __builtin_ia32_vpermil2ps ((__v4sf)(__m128)(X),\t\t\\\n+\t\t\t\t       (__v4sf)(__m128)(Y),\t\t\\\n+\t\t\t\t       (__v4si)(__m128)(C),\t\t\\\n+\t\t\t\t       (int)(I)))\n+\n+#define _mm256_permute2_ps(X, Y, C, I)\t\t\t\t\t\\\n+  ((__m256) __builtin_ia32_vpermil2ps256 ((__v8sf)(__m256)(X),\t\t\\\n+\t\t\t\t\t  (__v8sf)(__m256)(Y),  \t\\\n+\t\t\t\t\t  (__v8si)(__m256)(C),\t\t\\\n+\t\t\t\t\t  (int)(I)))\n+#endif\n+\n+#ifdef __OPTIMIZE__\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_permute2f128_pd (__m256d __X, __m256d __Y, const int __C)\n+{\n+  return (__m256d) __builtin_ia32_vperm2f128_pd256 ((__v4df)__X,\n+\t\t\t\t\t\t    (__v4df)__Y,\n+\t\t\t\t\t\t    __C);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_permute2f128_ps (__m256 __X, __m256 __Y, const int __C)\n+{\n+  return (__m256) __builtin_ia32_vperm2f128_ps256 ((__v8sf)__X,\n+\t\t\t\t\t\t   (__v8sf)__Y,\n+\t\t\t\t\t\t   __C);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_permute2f128_si256 (__m256i __X, __m256i __Y, const int __C)\n+{\n+  return (__m256i) __builtin_ia32_vperm2f128_si256 ((__v8si)__X,\n+\t\t\t\t\t\t    (__v8si)__Y,\n+\t\t\t\t\t\t    __C);\n+}\n+#else\n+#define _mm256_permute2f128_pd(X, Y, C)\t\t\t\t\t\\\n+  ((__m256d) __builtin_ia32_vperm2f128_pd256 ((__v4df)(__m256d)(X),\t\\\n+\t\t\t\t\t      (__v4df)(__m256d)(Y),\t\\\n+\t\t\t\t\t      (int)(C)))\n+\n+#define _mm256_permute2f128_ps(X, Y, C)\t\t\t\t\t\\\n+  ((__m256) __builtin_ia32_vperm2f128_ps256 ((__v8sf)(__m256)(X),\t\\\n+\t\t\t\t\t     (__v8sf)(__m256)(Y),\t\\\n+\t\t\t\t\t     (int)(C)))\n+\n+#define _mm256_permute2f128_si256(X, Y, C)\t\t\t\t\\\n+  ((__m256i) __builtin_ia32_vperm2f128_si256 ((__v8si)(__m256i)(X),\t\\\n+\t\t\t\t\t      (__v8si)(__m256i)(Y),\t\\\n+\t\t\t\t\t      (int)(C)))\n+#endif\n+\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_broadcast_ss (float const *__X)\n+{\n+  return (__m128) __builtin_ia32_vbroadcastss (__X);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_broadcast_sd (double const *__X)\n+{\n+  return (__m256d) __builtin_ia32_vbroadcastsd256 (__X);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_broadcast_ss (float const *__X)\n+{\n+  return (__m256) __builtin_ia32_vbroadcastss256 (__X);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_broadcast_pd (__m128d const *__X)\n+{\n+  return (__m256d) __builtin_ia32_vbroadcastf128_pd256 (__X);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_broadcast_ps (__m128 const *__X)\n+{\n+  return (__m256) __builtin_ia32_vbroadcastf128_ps256 (__X);\n+}\n+\n+#ifdef __OPTIMIZE__\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_insertf128_pd (__m256d __X, __m128d __Y, const int __O)\n+{\n+  return (__m256d) __builtin_ia32_vinsertf128_pd256 ((__v4df)__X,\n+\t\t\t\t\t\t     (__v2df)__Y,\n+\t\t\t\t\t\t     __O);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_insertf128_ps (__m256 __X, __m128 __Y, const int __O)\n+{\n+  return (__m256) __builtin_ia32_vinsertf128_ps256 ((__v8sf)__X,\n+\t\t\t\t\t\t    (__v4sf)__Y,\n+\t\t\t\t\t\t    __O);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_insertf128_si256 (__m256i __X, __m128i __Y, const int __O)\n+{\n+  return (__m256i) __builtin_ia32_vinsertf128_si256 ((__v8si)__X,\n+\t\t\t\t\t\t     (__v4si)__Y,\n+\t\t\t\t\t\t     __O);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_insert_epi32 (__m256i __X, int __D, int const __N)\n+{\n+  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 2);\n+  __Y = _mm_insert_epi16 (__Y, __D, __N % 4);\n+  return _mm256_insertf128_si256 (__X, __Y, __N >> 2);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_insert_epi16 (__m256i __X, int __D, int const __N)\n+{\n+  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 3);\n+  __Y = _mm_insert_epi16 (__Y, __D, __N % 8);\n+  return _mm256_insertf128_si256 (__X, __Y, __N >> 3);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_insert_epi8 (__m256i __X, int __D, int const __N)\n+{\n+  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 4);\n+  __Y = _mm_insert_epi8 (__Y, __D, __N % 16);\n+  return _mm256_insertf128_si256 (__X, __Y, __N >> 4);\n+}\n+\n+#ifdef __x86_64__\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_insert_epi64 (__m256i __X, int __D, int const __N)\n+{\n+  __m128i __Y = _mm256_extractf128_si256 (__X, __N >> 1);\n+  __Y = _mm_insert_epi16 (__Y, __D, __N % 2);\n+  return _mm256_insertf128_si256 (__X, __Y, __N >> 1);\n+}\n+#endif\n+#else\n+#define _mm256_insertf128_pd(X, Y, O)\t\t\t\t\t\\\n+  ((__m256d) __builtin_ia32_vinsertf128_pd256 ((__v4df)(__m256d)(X),\t\\\n+\t\t\t\t\t       (__v2df)(__m128d)(Y),\t\\\n+\t\t\t\t\t       (int)(O)))\n+\n+#define _mm256_insertf128_ps(X, Y, O)\t\t\t\t\t\\\n+  ((__m256) __builtin_ia32_vinsertf128_ps256 ((__v8sf)(__m256)(X),\t\\\n+\t\t\t\t\t      (__v4sf)(__m128)(Y),  \t\\\n+\t\t\t\t\t      (int)(O)))\n+\n+#define _mm256_insertf128_si256(X, Y, O)\t\t\t\t\\\n+  ((__m256i) __builtin_ia32_vinsertf128_si256 ((__v8si)(__m256i)(X),\t\\\n+\t\t\t\t\t       (__v4si)(__m128i)(Y),\t\\\n+\t\t\t\t\t       (int)(O)))\n+\n+#define _mm256_insert_epi32(X, D, N)\t\t\t\t\t\\\n+  (__extension__\t\t\t\t\t\t\t\\\n+   ({\t\t\t\t\t\t\t\t\t\\\n+      __m128i __Y = _mm256_extractf128_si256 ((X), (N) >> 2);\t\t\\\n+      __Y = _mm_insert_epi32 (__Y, (D), (N) % 4);\t\t\t\\\n+      _mm256_insertf128_si256 ((X), __Y, (N) >> 2);\t\t\t\\\n+    }))\n+\n+#define _mm256_insert_epi16(X, D, N)\t\t\t\t\t\\\n+  (__extension__\t\t\t\t\t\t\t\\\n+   ({\t\t\t\t\t\t\t\t\t\\\n+      __m128i __Y = _mm256_extractf128_si256 ((X), (N) >> 3);\t\t\\\n+      __Y = _mm_insert_epi16 (__Y, (D), (N) % 8);\t\t\t\\\n+      _mm256_insertf128_si256 ((X), __Y, (N) >> 3);\t\t\t\\\n+    }))\n+\n+#define _mm256_insert_epi8(X, D, N)\t\t\t\t\t\\\n+  (__extension__\t\t\t\t\t\t\t\\\n+   ({\t\t\t\t\t\t\t\t\t\\\n+      __m128i __Y = _mm256_extractf128_si256 ((X), (N) >> 4);\t\t\\\n+      __Y = _mm_insert_epi8 (__Y, (D), (N) % 16);\t\t\t\\\n+      _mm256_insertf128_si256 ((X), __Y, (N) >> 4);\t\t\t\\\n+    }))\n+\n+#ifdef __x86_64__\n+#define _mm256_insert_epi64(X, D, N)\t\t\t\t\t\\\n+  (__extension__\t\t\t\t\t\t\t\\\n+   ({\t\t\t\t\t\t\t\t\t\\\n+      __m128i __Y = _mm256_extractf128_si256 ((X), (N) >> 1);\t\t\\\n+      __Y = _mm_insert_epi64 (__Y, (D), (N) % 2);\t\t\t\\\n+      _mm256_insertf128_si256 ((X), __Y, (N) >> 1);\t\t\t\\\n+    }))\n+#endif\n+#endif\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_load_pd (double const *__P)\n+{\n+  return *(__m256d *)__P;\n+}\n+\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_store_pd (double *__P, __m256d __A)\n+{\n+  *(__m256d *)__P = __A;\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_load_ps (float const *__P)\n+{\n+  return *(__m256 *)__P;\n+}\n+\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_store_ps (float *__P, __m256 __A)\n+{\n+  *(__m256 *)__P = __A;\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_loadu_pd (double const *__P)\n+{\n+  return (__m256d) __builtin_ia32_loadupd256 (__P);\n+}\n+\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_storeu_pd (double *__P, __m256d __A)\n+{\n+  __builtin_ia32_storeupd256 (__P, (__v4df)__A);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_loadu_ps (float const *__P)\n+{\n+  return (__m256) __builtin_ia32_loadups256 (__P);\n+}\n+\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_storeu_ps (float *__P, __m256 __A)\n+{\n+  __builtin_ia32_storeups256 (__P, (__v8sf)__A);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_load_si256 (__m256i const *__P)\n+{\n+  return *__P;\n+}\n+\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_store_si256 (__m256i *__P, __m256i __A)\n+{\n+  *__P = __A;\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_loadu_si256 (__m256i const *__P)\n+{\n+  return (__m256i) __builtin_ia32_loaddqu256 ((char const *)__P);\n+}\n+\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_storeu_si256 (__m256i *__P, __m256i __A)\n+{\n+  __builtin_ia32_storedqu256 ((char *)__P, (__v32qi)__A);\n+}\n+\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_maskload_pd (double const *__P, __m128d __M)\n+{\n+  return (__m128d) __builtin_ia32_maskloadpd ((const __v2df *)__P,\n+\t\t\t\t\t      (__v2df)__M);\n+}\n+\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_maskstore_pd (double *__P, __m128d __M, __m128d __A)\n+{\n+  __builtin_ia32_maskstorepd ((__v2df *)__P, (__v2df)__M, (__v2df)__A);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_maskload_pd (double const *__P, __m256d __M)\n+{\n+  return (__m256d) __builtin_ia32_maskloadpd256 ((const __v4df *)__P,\n+\t\t\t\t\t\t (__v4df)__M);\n+}\n+\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_maskstore_pd (double *__P, __m256d __M, __m256d __A)\n+{\n+  __builtin_ia32_maskstorepd256 ((__v4df *)__P, (__v4df)__M, (__v4df)__A);\n+}\n+\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_maskload_ps (float const *__P, __m128 __M)\n+{\n+  return (__m128) __builtin_ia32_maskloadps ((const __v4sf *)__P,\n+\t\t\t\t\t     (__v4sf)__M);\n+}\n+\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_maskstore_ps (float *__P, __m128 __M, __m128 __A)\n+{\n+  __builtin_ia32_maskstoreps ((__v4sf *)__P, (__v4sf)__M, (__v4sf)__A);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_maskload_ps (float const *__P, __m256 __M)\n+{\n+  return (__m256) __builtin_ia32_maskloadps256 ((const __v8sf *)__P,\n+\t\t\t\t\t\t(__v8sf)__M);\n+}\n+\n+extern __inline void __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_maskstore_ps (float *__P, __m256 __M, __m256 __A)\n+{\n+  __builtin_ia32_maskstoreps256 ((__v8sf *)__P, (__v8sf)__M, (__v8sf)__A);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_movehdup_ps (__m256 __X)\n+{\n+  return (__m256) __builtin_ia32_movshdup256 ((__v8sf)__X);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_moveldup_ps (__m256 __X)\n+{\n+  return (__m256) __builtin_ia32_movsldup256 ((__v8sf)__X);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_movedup_pd (__m256d __X)\n+{\n+  return (__m256d) __builtin_ia32_movddup256 ((__v4df)__X);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_lddqu_si256 (__m256i const *__P)\n+{\n+  return (__m256i) __builtin_ia32_lddqu256 ((char const *)__P);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_rcp_ps (__m256 __A)\n+{\n+  return (__m256) __builtin_ia32_rcpps256 ((__v8sf)__A);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_rsqrt_ps (__m256 __A)\n+{\n+  return (__m256) __builtin_ia32_rsqrtps256 ((__v8sf)__A);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_sqrt_pd (__m256d __A)\n+{\n+  return (__m256d) __builtin_ia32_sqrtpd256 ((__v4df)__A);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_sqrt_ps (__m256 __A)\n+{\n+  return (__m256) __builtin_ia32_sqrtps256 ((__v8sf)__A);\n+}\n+\n+#ifdef __OPTIMIZE__\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_round_pd (__m256d __V, const int __M)\n+{\n+  return (__m256d) __builtin_ia32_roundpd256 ((__v4df)__V, __M);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_round_ps (__m256 __V, const int __M)\n+{\n+  return (__m256) __builtin_ia32_roundps256 ((__v8sf)__V, __M);\n+}\n+#else\n+#define _mm256_round_pd(V, M) \\\n+  ((__m256d) __builtin_ia32_roundpd256 ((__v4df)(__m256d)(V), (int)(M)))\n+\n+#define _mm256_round_ps(V, M) \\\n+  ((__m256) __builtin_ia32_roundps256 ((__v8sf)(__m256)(V), (int)(M)))\n+#endif\n+\n+#define _mm256_ceil_pd(V)\t_mm256_round_pd ((V), _MM_FROUND_CEIL)\n+#define _mm256_floor_pd(V)\t_mm256_round_pd ((V), _MM_FROUND_FLOOR)\n+#define _mm256_ceil_ps(V)\t_mm256_round_ps ((V), _MM_FROUND_CEIL)\n+#define _mm256_floor_ps(V)\t_mm256_round_ps ((V), _MM_FROUND_FLOOR)\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_unpackhi_pd (__m256d __A, __m256d __B)\n+{\n+  return (__m256d) __builtin_ia32_unpckhpd256 ((__v4df)__A, (__v4df)__B);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_unpacklo_pd (__m256d __A, __m256d __B)\n+{\n+  return (__m256d) __builtin_ia32_unpcklpd256 ((__v4df)__A, (__v4df)__B);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_unpackhi_ps (__m256 __A, __m256 __B)\n+{\n+  return (__m256) __builtin_ia32_unpckhps256 ((__v8sf)__A, (__v8sf)__B);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_unpacklo_ps (__m256 __A, __m256 __B)\n+{\n+  return (__m256) __builtin_ia32_unpcklps256 ((__v8sf)__A, (__v8sf)__B);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_testz_pd (__m128d __M, __m128d __V)\n+{\n+  return __builtin_ia32_vtestzpd ((__v2df)__M, (__v2df)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_testc_pd (__m128d __M, __m128d __V)\n+{\n+  return __builtin_ia32_vtestcpd ((__v2df)__M, (__v2df)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_testnzc_pd (__m128d __M, __m128d __V)\n+{\n+  return __builtin_ia32_vtestnzcpd ((__v2df)__M, (__v2df)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_testz_ps (__m128 __M, __m128 __V)\n+{\n+  return __builtin_ia32_vtestzps ((__v4sf)__M, (__v4sf)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_testc_ps (__m128 __M, __m128 __V)\n+{\n+  return __builtin_ia32_vtestcps ((__v4sf)__M, (__v4sf)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm_testnzc_ps (__m128 __M, __m128 __V)\n+{\n+  return __builtin_ia32_vtestnzcps ((__v4sf)__M, (__v4sf)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_testz_pd (__m256d __M, __m256d __V)\n+{\n+  return __builtin_ia32_vtestzpd256 ((__v4df)__M, (__v4df)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_testc_pd (__m256d __M, __m256d __V)\n+{\n+  return __builtin_ia32_vtestcpd256 ((__v4df)__M, (__v4df)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_testnzc_pd (__m256d __M, __m256d __V)\n+{\n+  return __builtin_ia32_vtestnzcpd256 ((__v4df)__M, (__v4df)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_testz_ps (__m256 __M, __m256 __V)\n+{\n+  return __builtin_ia32_vtestzps256 ((__v8sf)__M, (__v8sf)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_testc_ps (__m256 __M, __m256 __V)\n+{\n+  return __builtin_ia32_vtestcps256 ((__v8sf)__M, (__v8sf)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_testnzc_ps (__m256 __M, __m256 __V)\n+{\n+  return __builtin_ia32_vtestnzcps256 ((__v8sf)__M, (__v8sf)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_testz_si256 (__m256i __M, __m256i __V)\n+{\n+  return __builtin_ia32_ptestz256 ((__v4di)__M, (__v4di)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_testc_si256 (__m256i __M, __m256i __V)\n+{\n+  return __builtin_ia32_ptestc256 ((__v4di)__M, (__v4di)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_testnzc_si256 (__m256i __M, __m256i __V)\n+{\n+  return __builtin_ia32_ptestnzc256 ((__v4di)__M, (__v4di)__V);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_movemask_pd (__m256d __A)\n+{\n+  return __builtin_ia32_movmskpd256 ((__v4df)__A);\n+}\n+\n+extern __inline int __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_movemask_ps (__m256 __A)\n+{\n+  return __builtin_ia32_movmskps256 ((__v8sf)__A);\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_setzero_pd (void)\n+{\n+  return __extension__ (__m256d){ 0.0, 0.0, 0.0, 0.0 };\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_setzero_ps (void)\n+{\n+  return __extension__ (__m256){ 0.0, 0.0, 0.0, 0.0,\n+\t\t\t\t 0.0, 0.0, 0.0, 0.0 };\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_setzero_si256 (void)\n+{\n+  return __extension__ (__m256i)(__v4di){ 0, 0, 0, 0 };\n+}\n+\n+/* Create the vector [A B C D].  */\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_set_pd (double __A, double __B, double __C, double __D)\n+{\n+  return __extension__ (__m256d){ __D, __C, __B, __A };\n+}\n+\n+/* Create the vector [A B C D E F G H].  */\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_set_ps (float __A, float __B, float __C, float __D,\n+\t       float __E, float __F, float __G, float __H)\n+{\n+  return __extension__ (__m256){ __H, __G, __F, __E,\n+\t\t\t\t __D, __C, __B, __A };\n+}\n+\n+/* Create the vector [A B C D E F G H].  */\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_set_epi32 (int __A, int __B, int __C, int __D,\n+\t\t  int __E, int __F, int __G, int __H)\n+{\n+  return __extension__ (__m256i)(__v8si){ __H, __G, __F, __E,\n+\t\t\t\t\t  __D, __C, __B, __A };\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_set_epi16 (short __q15, short __q14, short __q13, short __q12,\n+\t\t  short __q11, short __q10, short __q09, short __q08,\n+\t\t  short __q07, short __q06, short __q05, short __q04,\n+\t\t  short __q03, short __q02, short __q01, short __q00)\n+{\n+  return __extension__ (__m256i)(__v16hi){\n+    __q00, __q01, __q02, __q03, __q04, __q05, __q06, __q07,\n+    __q08, __q09, __q10, __q11, __q12, __q13, __q14, __q15\n+  };\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_set_epi8  (char __q31, char __q30, char __q29, char __q28,\n+\t\t  char __q27, char __q26, char __q25, char __q24,\n+\t\t  char __q23, char __q22, char __q21, char __q20,\n+\t\t  char __q19, char __q18, char __q17, char __q16,\n+\t\t  char __q15, char __q14, char __q13, char __q12,\n+\t\t  char __q11, char __q10, char __q09, char __q08,\n+\t\t  char __q07, char __q06, char __q05, char __q04,\n+\t\t  char __q03, char __q02, char __q01, char __q00)\n+{\n+  return __extension__ (__m256i)(__v32qi){\n+    __q00, __q01, __q02, __q03, __q04, __q05, __q06, __q07,\n+    __q08, __q09, __q10, __q11, __q12, __q13, __q14, __q15,\n+    __q16, __q17, __q18, __q19, __q20, __q21, __q22, __q23,\n+    __q24, __q25, __q26, __q27, __q28, __q29, __q30, __q31\n+  };\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_set_epi64x (long long __A, long long __B, long long __C,\n+\t\t   long long __D)\n+{\n+  return __extension__ (__m256i)(__v4di){ __D, __C, __B, __A };\n+}\n+\n+/* Create a vector with all elements equal to A.  */\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_set1_pd (double __A)\n+{\n+  return __extension__ (__m256d){ __A, __A, __A, __A };\n+}\n+\n+/* Create a vector with all elements equal to A.  */\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_set1_ps (float __A)\n+{\n+  return __extension__ (__m256){ __A, __A, __A, __A,\n+\t\t\t\t __A, __A, __A, __A };\n+}\n+\n+/* Create a vector with all elements equal to A.  */\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_set1_epi32 (int __A)\n+{\n+  return __extension__ (__m256i)(__v8si){ __A, __A, __A, __A,\n+\t\t\t\t\t  __A, __A, __A, __A };\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_set1_epi16 (short __A)\n+{\n+  return _mm256_set_epi16 (__A, __A, __A, __A, __A, __A, __A, __A,\n+\t\t\t   __A, __A, __A, __A, __A, __A, __A, __A);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_set1_epi8 (char __A)\n+{\n+  return _mm256_set_epi8 (__A, __A, __A, __A, __A, __A, __A, __A,\n+\t\t\t  __A, __A, __A, __A, __A, __A, __A, __A,\n+\t\t\t  __A, __A, __A, __A, __A, __A, __A, __A,\n+\t\t\t  __A, __A, __A, __A, __A, __A, __A, __A);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_set1_epi64x (long long __A)\n+{\n+  return __extension__ (__m256i)(__v4di){ __A, __A, __A, __A };\n+}\n+\n+/* Create vectors of elements in the reversed order from the\n+   _mm256_set_XXX functions.  */\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_setr_pd (double __A, double __B, double __C, double __D)\n+{\n+  return _mm256_set_pd (__D, __C, __B, __A);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_setr_ps (float __A, float __B, float __C, float __D,\n+\t\tfloat __E, float __F, float __G, float __H)\n+{\n+  return _mm256_set_ps (__H, __G, __F, __E, __D, __C, __B, __A);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_setr_epi32 (int __A, int __B, int __C, int __D,\n+\t\t   int __E, int __F, int __G, int __H)\n+{\n+  return _mm256_set_epi32 (__H, __G, __F, __E, __D, __C, __B, __A);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_setr_epi16 (short __q15, short __q14, short __q13, short __q12,\n+\t\t   short __q11, short __q10, short __q09, short __q08,\n+\t\t   short __q07, short __q06, short __q05, short __q04,\n+\t\t   short __q03, short __q02, short __q01, short __q00)\n+{\n+  return _mm256_set_epi16 (__q00, __q01, __q02, __q03,\n+\t\t\t   __q04, __q05, __q06, __q07,\n+\t\t\t   __q08, __q09, __q10, __q11,\n+\t\t\t   __q12, __q13, __q14, __q15);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_setr_epi8  (char __q31, char __q30, char __q29, char __q28,\n+\t\t   char __q27, char __q26, char __q25, char __q24,\n+\t\t   char __q23, char __q22, char __q21, char __q20,\n+\t\t   char __q19, char __q18, char __q17, char __q16,\n+\t\t   char __q15, char __q14, char __q13, char __q12,\n+\t\t   char __q11, char __q10, char __q09, char __q08,\n+\t\t   char __q07, char __q06, char __q05, char __q04,\n+\t\t   char __q03, char __q02, char __q01, char __q00)\n+{\n+  return _mm256_set_epi8 (__q00, __q01, __q02, __q03,\n+\t\t\t  __q04, __q05, __q06, __q07,\n+\t\t\t  __q08, __q09, __q10, __q11,\n+\t\t\t  __q12, __q13, __q14, __q15,\n+\t\t\t  __q16, __q17, __q18, __q19,\n+\t\t\t  __q20, __q21, __q22, __q23,\n+\t\t\t  __q24, __q25, __q26, __q27,\n+\t\t\t  __q28, __q29, __q30, __q31);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_setr_epi64x (long long __A, long long __B, long long __C,\n+\t\t    long long __D)\n+{\n+  return _mm256_set_epi64x (__D, __C, __B, __A);\n+}\n+\n+/* Casts between various SP, DP, INT vector types.  Note that these do no\n+   conversion of values, they just change the type.  */\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_castpd_ps (__m256d __A)\n+{\n+  return (__m256) __A;\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_castpd_si256 (__m256d __A)\n+{\n+  return (__m256i) __A;\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_castps_pd (__m256 __A)\n+{\n+  return (__m256d) __A;\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_castps_si256(__m256 __A)\n+{\n+  return (__m256i) __A;\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_castsi256_ps (__m256i __A)\n+{\n+  return (__m256) __A;\n+}\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_castsi256_pd (__m256i __A)\n+{\n+  return (__m256d) __A;\n+}\n+\n+extern __inline __m128d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_castpd256_pd128 (__m256d __A)\n+{\n+  return (__m128d) __builtin_ia32_pd_pd256 ((__v4df)__A);\n+}\n+\n+extern __inline __m128 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_castps256_ps128 (__m256 __A)\n+{\n+  return (__m128) __builtin_ia32_ps_ps256 ((__v8sf)__A);\n+}\n+\n+extern __inline __m128i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_castsi256_si128 (__m256i __A)\n+{\n+  return (__m128i) __builtin_ia32_si_si256 ((__v8si)__A);\n+}\n+\n+/* When cast is done from a 128 to 256-bit type, the low 128 bits of\n+   the 256-bit result contain source parameter value and the upper 128\n+   bits of the result are undefined.  Those intrinsics shouldn't\n+   generate any extra moves.  */\n+\n+extern __inline __m256d __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_castpd128_pd256 (__m128d __A)\n+{\n+  return (__m256d) __builtin_ia32_pd256_pd ((__v2df)__A);\n+}\n+\n+extern __inline __m256 __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_castps128_ps256 (__m128 __A)\n+{\n+  return (__m256) __builtin_ia32_ps256_ps ((__v4sf)__A);\n+}\n+\n+extern __inline __m256i __attribute__((__gnu_inline__, __always_inline__, __artificial__))\n+_mm256_castsi128_si256 (__m128i __A)\n+{\n+  return (__m256i) __builtin_ia32_si256_si ((__v4si)__A);\n+}\n+\n+#endif /* __AVX__ */\n+\n+#endif /* _GMMINTRIN_H_INCLUDED */"}, {"sha": "411c28d4d5e1b35edc11bebf6e3ec0a30cc55e19", "filename": "gcc/config/i386/i386-c.c", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fi386-c.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fi386-c.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-c.c?ref=95879c728b9a59ae67db022ad370eb66374090f3", "patch": "@@ -217,6 +217,10 @@ ix86_target_macros_internal (int isa_flag,\n     def_or_undef (parse_in, \"__AES__\");\n   if (isa_flag & OPTION_MASK_ISA_PCLMUL)\n     def_or_undef (parse_in, \"__PCLMUL__\");\n+  if (isa_flag & OPTION_MASK_ISA_AVX)\n+    def_or_undef (parse_in, \"__AVX__\");\n+  if (isa_flag & OPTION_MASK_ISA_FMA)\n+    def_or_undef (parse_in, \"__FMA__\");\n   if (isa_flag & OPTION_MASK_ISA_SSE4A)\n     def_or_undef (parse_in, \"__SSE4A__\");\n   if (isa_flag & OPTION_MASK_ISA_SSE5)"}, {"sha": "f5fb906ecea5fa36f68d25b4dd896cf941e73755", "filename": "gcc/config/i386/i386-modes.def", "status": "modified", "additions": 9, "deletions": 6, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fi386-modes.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fi386-modes.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-modes.def?ref=95879c728b9a59ae67db022ad370eb66374090f3", "patch": "@@ -73,17 +73,20 @@ CC_MODE (CCFPU);\n VECTOR_MODES (INT, 4);        /*            V4QI V2HI */\n VECTOR_MODES (INT, 8);        /*       V8QI V4HI V2SI */\n VECTOR_MODES (INT, 16);       /* V16QI V8HI V4SI V2DI */\n+VECTOR_MODES (INT, 32);       /* V32QI V16HI V8SI V4DI */\n VECTOR_MODES (FLOAT, 8);      /*            V4HF V2SF */\n VECTOR_MODES (FLOAT, 16);     /*       V8HF V4SF V2DF */\n+VECTOR_MODES (FLOAT, 32);     /*      V16HF V8SF V4DF */\n VECTOR_MODE (INT, DI, 1);     /*                 V1DI */\n VECTOR_MODE (INT, SI, 1);     /*                 V1SI */\n VECTOR_MODE (INT, QI, 2);     /*                 V2QI */\n-VECTOR_MODE (INT, DI, 4);     /*                 V4DI */\n-VECTOR_MODE (INT, SI, 8);     /*                 V8SI */\n-VECTOR_MODE (INT, HI, 16);    /*                V16HI */\n-VECTOR_MODE (INT, QI, 32);    /*                V32QI */\n-VECTOR_MODE (FLOAT, DF, 4);   /*                 V4DF */\n-VECTOR_MODE (FLOAT, SF, 8);   /*                 V8SF */\n+VECTOR_MODE (INT, DI, 8);     /*                 V8DI */\n+VECTOR_MODE (INT, HI, 32);    /*                V32HI */\n+VECTOR_MODE (INT, QI, 64);    /*                V64QI */\n+VECTOR_MODE (FLOAT, DF, 8);   /*                 V8DF */\n+VECTOR_MODE (FLOAT, SF, 16);  /*                V16SF */\n+\n+INT_MODE (OI, 32);\n \n /* The symbol Pmode stands for one of the above machine modes (usually SImode).\n    The tm.h file specifies which one.  It is not a distinct mode.  */"}, {"sha": "5f055abcbbcf95700229e41a3f47caf5a006839c", "filename": "gcc/config/i386/i386-protos.h", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fi386-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fi386-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-protos.h?ref=95879c728b9a59ae67db022ad370eb66374090f3", "patch": "@@ -128,6 +128,7 @@ extern int ix86_check_movabs (rtx, int);\n extern rtx assign_386_stack_local (enum machine_mode, enum ix86_stack_slot);\n extern int ix86_attr_length_immediate_default (rtx, int);\n extern int ix86_attr_length_address_default (rtx);\n+extern int ix86_attr_length_vex_default (rtx, int, int);\n \n extern enum machine_mode ix86_fp_compare_mode (enum rtx_code);\n "}, {"sha": "4d45c8486a4104ea8fb8a4a2889fa0962f71d296", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 1542, "deletions": 85, "changes": 1627, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=95879c728b9a59ae67db022ad370eb66374090f3", "patch": "@@ -1764,6 +1764,7 @@ enum x86_64_reg_class\n     X86_64_NO_CLASS,\n     X86_64_INTEGER_CLASS,\n     X86_64_INTEGERSI_CLASS,\n+    X86_64_AVX_CLASS,\n     X86_64_SSE_CLASS,\n     X86_64_SSESF_CLASS,\n     X86_64_SSEDF_CLASS,\n@@ -1849,6 +1850,10 @@ static int ix86_isa_flags_explicit;\n   (OPTION_MASK_ISA_SSE4_1 | OPTION_MASK_ISA_SSSE3_SET)\n #define OPTION_MASK_ISA_SSE4_2_SET \\\n   (OPTION_MASK_ISA_SSE4_2 | OPTION_MASK_ISA_SSE4_1_SET)\n+#define OPTION_MASK_ISA_AVX_SET \\\n+  (OPTION_MASK_ISA_AVX | OPTION_MASK_ISA_SSE4_2_SET)\n+#define OPTION_MASK_ISA_FMA_SET \\\n+  (OPTION_MASK_ISA_FMA | OPTION_MASK_ISA_AVX_SET)\n \n /* SSE4 includes both SSE4.1 and SSE4.2. -msse4 should be the same\n    as -msse4.2.  */\n@@ -1892,7 +1897,11 @@ static int ix86_isa_flags_explicit;\n   (OPTION_MASK_ISA_SSSE3 | OPTION_MASK_ISA_SSE4_1_UNSET)\n #define OPTION_MASK_ISA_SSE4_1_UNSET \\\n   (OPTION_MASK_ISA_SSE4_1 | OPTION_MASK_ISA_SSE4_2_UNSET)\n-#define OPTION_MASK_ISA_SSE4_2_UNSET OPTION_MASK_ISA_SSE4_2\n+#define OPTION_MASK_ISA_SSE4_2_UNSET \\\n+  (OPTION_MASK_ISA_SSE4_2 | OPTION_MASK_ISA_AVX_UNSET )\n+#define OPTION_MASK_ISA_AVX_UNSET \\\n+  (OPTION_MASK_ISA_AVX | OPTION_MASK_ISA_FMA_UNSET)\n+#define OPTION_MASK_ISA_FMA_UNSET OPTION_MASK_ISA_FMA\n \n /* SSE4 includes both SSE4.1 and SSE4.2.  -mno-sse4 should the same\n    as -mno-sse4.1. */\n@@ -2081,6 +2090,32 @@ ix86_handle_option (size_t code, const char *arg ATTRIBUTE_UNUSED, int value)\n \t}\n       return true;\n \n+    case OPT_mavx:\n+      if (value)\n+\t{\n+\t  ix86_isa_flags |= OPTION_MASK_ISA_AVX_SET;\n+\t  ix86_isa_flags_explicit |= OPTION_MASK_ISA_AVX_SET;\n+\t}\n+      else\n+\t{\n+\t  ix86_isa_flags &= ~OPTION_MASK_ISA_AVX_UNSET;\n+\t  ix86_isa_flags_explicit |= OPTION_MASK_ISA_AVX_UNSET;\n+\t}\n+      return true;\n+\n+    case OPT_mfma:\n+      if (value)\n+\t{\n+\t  ix86_isa_flags |= OPTION_MASK_ISA_FMA_SET;\n+\t  ix86_isa_flags_explicit |= OPTION_MASK_ISA_FMA_SET;\n+\t}\n+      else\n+\t{\n+\t  ix86_isa_flags &= ~OPTION_MASK_ISA_FMA_UNSET;\n+\t  ix86_isa_flags_explicit |= OPTION_MASK_ISA_FMA_UNSET;\n+\t}\n+      return true;\n+\n     case OPT_msse4:\n       ix86_isa_flags |= OPTION_MASK_ISA_SSE4_SET;\n       ix86_isa_flags_explicit |= OPTION_MASK_ISA_SSE4_SET;\n@@ -2447,7 +2482,9 @@ override_options (bool main_args_p)\n       PTA_SSE4_2 = 1 << 15,\n       PTA_SSE5 = 1 << 16,\n       PTA_AES = 1 << 17,\n-      PTA_PCLMUL = 1 << 18\n+      PTA_PCLMUL = 1 << 18,\n+      PTA_AVX = 1 << 19,\n+      PTA_FMA = 1 << 20 \n     };\n \n   static struct pta\n@@ -2765,6 +2802,12 @@ override_options (bool main_args_p)\n \tif (processor_alias_table[i].flags & PTA_SSE4_2\n \t    && !(ix86_isa_flags_explicit & OPTION_MASK_ISA_SSE4_2))\n \t  ix86_isa_flags |= OPTION_MASK_ISA_SSE4_2;\n+\tif (processor_alias_table[i].flags & PTA_AVX\n+\t    && !(ix86_isa_flags_explicit & OPTION_MASK_ISA_AVX))\n+\t  ix86_isa_flags |= OPTION_MASK_ISA_AVX;\n+\tif (processor_alias_table[i].flags & PTA_FMA\n+\t    && !(ix86_isa_flags_explicit & OPTION_MASK_ISA_FMA))\n+\t  ix86_isa_flags |= OPTION_MASK_ISA_FMA;\n \tif (processor_alias_table[i].flags & PTA_SSE4A\n \t    && !(ix86_isa_flags_explicit & OPTION_MASK_ISA_SSE4A))\n \t  ix86_isa_flags |= OPTION_MASK_ISA_SSE4A;\n@@ -4587,6 +4630,7 @@ init_cumulative_args (CUMULATIVE_ARGS *cum,  /* Argument info to initialize */\n     }\n   if (TARGET_MMX)\n     cum->mmx_nregs = MMX_REGPARM_MAX;\n+  cum->warn_avx = true;\n   cum->warn_sse = true;\n   cum->warn_mmx = true;\n \n@@ -4611,6 +4655,7 @@ init_cumulative_args (CUMULATIVE_ARGS *cum,  /* Argument info to initialize */\n \t  cum->nregs = 0;\n \t  cum->sse_nregs = 0;\n \t  cum->mmx_nregs = 0;\n+\t  cum->warn_avx = 0;\n \t  cum->warn_sse = 0;\n \t  cum->warn_mmx = 0;\n \t  return;\n@@ -4963,6 +5008,8 @@ classify_argument (enum machine_mode mode, const_tree type,\n       classes[0] = classes[1] = X86_64_INTEGER_CLASS;\n       return 2;\n     case CTImode:\n+    case COImode:\n+    case OImode:\n       return 0;\n     case SFmode:\n       if (!(bit_offset % 64))\n@@ -4994,6 +5041,14 @@ classify_argument (enum machine_mode mode, const_tree type,\n     case TCmode:\n       /* This modes is larger than 16 bytes.  */\n       return 0;\n+    case V8SFmode:\n+    case V8SImode:\n+    case V32QImode:\n+    case V16HImode:\n+    case V4DFmode:\n+    case V4DImode:\n+      classes[0] = X86_64_AVX_CLASS;\n+      return 1;\n     case V4SFmode:\n     case V4SImode:\n     case V16QImode:\n@@ -5050,6 +5105,7 @@ examine_argument (enum machine_mode mode, const_tree type, int in_return,\n       case X86_64_INTEGERSI_CLASS:\n \t(*int_nregs)++;\n \tbreak;\n+      case X86_64_AVX_CLASS:\n       case X86_64_SSE_CLASS:\n       case X86_64_SSESF_CLASS:\n       case X86_64_SSEDF_CLASS:\n@@ -5148,6 +5204,7 @@ construct_container (enum machine_mode mode, enum machine_mode orig_mode,\n       case X86_64_INTEGER_CLASS:\n       case X86_64_INTEGERSI_CLASS:\n \treturn gen_rtx_REG (mode, intreg[0]);\n+      case X86_64_AVX_CLASS:\n       case X86_64_SSE_CLASS:\n       case X86_64_SSESF_CLASS:\n       case X86_64_SSEDF_CLASS:\n@@ -5281,6 +5338,13 @@ function_arg_advance_32 (CUMULATIVE_ARGS *cum, enum machine_mode mode,\n \tbreak;\n       /* FALLTHRU */\n \n+    case OImode:\n+    case V8SFmode:\n+    case V8SImode:\n+    case V32QImode:\n+    case V16HImode:\n+    case V4DFmode:\n+    case V4DImode:\n     case TImode:\n     case V16QImode:\n     case V8HImode:\n@@ -5323,10 +5387,14 @@ function_arg_advance_32 (CUMULATIVE_ARGS *cum, enum machine_mode mode,\n \n static void\n function_arg_advance_64 (CUMULATIVE_ARGS *cum, enum machine_mode mode,\n-\t\t\t tree type, HOST_WIDE_INT words)\n+\t\t\t tree type, HOST_WIDE_INT words, int named)\n {\n   int int_nregs, sse_nregs;\n \n+  /* Unnamed 256bit vector mode parameters are passed on stack.  */\n+  if (!named && VALID_AVX256_REG_MODE (mode))\n+    return;\n+\n   if (!examine_argument (mode, type, 0, &int_nregs, &sse_nregs))\n     cum->words += words;\n   else if (sse_nregs <= cum->sse_nregs && int_nregs <= cum->nregs)\n@@ -5357,7 +5425,7 @@ function_arg_advance_ms_64 (CUMULATIVE_ARGS *cum, HOST_WIDE_INT bytes,\n \n void\n function_arg_advance (CUMULATIVE_ARGS *cum, enum machine_mode mode,\n-\t\t      tree type, int named ATTRIBUTE_UNUSED)\n+\t\t      tree type, int named)\n {\n   HOST_WIDE_INT bytes, words;\n \n@@ -5373,7 +5441,7 @@ function_arg_advance (CUMULATIVE_ARGS *cum, enum machine_mode mode,\n   if (TARGET_64BIT && (cum ? cum->call_abi : DEFAULT_ABI) == MS_ABI)\n     function_arg_advance_ms_64 (cum, bytes, words);\n   else if (TARGET_64BIT)\n-    function_arg_advance_64 (cum, mode, type, words);\n+    function_arg_advance_64 (cum, mode, type, words, named);\n   else\n     function_arg_advance_32 (cum, mode, type, bytes, words);\n }\n@@ -5396,7 +5464,7 @@ function_arg_32 (CUMULATIVE_ARGS *cum, enum machine_mode mode,\n \t\t enum machine_mode orig_mode, tree type,\n \t\t HOST_WIDE_INT bytes, HOST_WIDE_INT words)\n {\n-  static bool warnedsse, warnedmmx;\n+  static bool warnedavx, warnedsse, warnedmmx;\n \n   /* Avoid the AL settings for the Unix64 ABI.  */\n   if (mode == VOIDmode)\n@@ -5445,6 +5513,7 @@ function_arg_32 (CUMULATIVE_ARGS *cum, enum machine_mode mode,\n \tbreak;\n       /* FALLTHRU */\n     case TImode:\n+      /* In 32bit, we pass TImode in xmm registers.  */\n     case V16QImode:\n     case V8HImode:\n     case V4SImode:\n@@ -5465,6 +5534,28 @@ function_arg_32 (CUMULATIVE_ARGS *cum, enum machine_mode mode,\n \t}\n       break;\n \n+    case OImode:\n+      /* In 32bit, we pass OImode in ymm registers.  */\n+    case V8SFmode:\n+    case V8SImode:\n+    case V32QImode:\n+    case V16HImode:\n+    case V4DFmode:\n+    case V4DImode:\n+      if (!type || !AGGREGATE_TYPE_P (type))\n+\t{\n+\t  if (!TARGET_AVX && !warnedavx && cum->warn_avx)\n+\t    {\n+\t      warnedavx = true;\n+\t      warning (0, \"AVX vector argument without AVX enabled \"\n+\t\t       \"changes the ABI\");\n+\t    }\n+\t  if (cum->sse_nregs)\n+\t    return gen_reg_or_parallel (mode, orig_mode,\n+\t\t\t\t        cum->sse_regno + FIRST_SSE_REG);\n+\t}\n+      break;\n+\n     case V8QImode:\n     case V4HImode:\n     case V2SImode:\n@@ -5490,8 +5581,10 @@ function_arg_32 (CUMULATIVE_ARGS *cum, enum machine_mode mode,\n \n static rtx\n function_arg_64 (CUMULATIVE_ARGS *cum, enum machine_mode mode,\n-\t\t enum machine_mode orig_mode, tree type)\n+\t\t enum machine_mode orig_mode, tree type, int named)\n {\n+  static bool warnedavx;\n+\n   /* Handle a hidden AL argument containing number of registers\n      for varargs x86-64 functions.  */\n   if (mode == VOIDmode)\n@@ -5504,6 +5597,35 @@ function_arg_64 (CUMULATIVE_ARGS *cum, enum machine_mode mode,\n  \t       : cum->sse_regno)\n \t\t    : -1);\n \n+  switch (mode)\n+    {\n+    default:\n+      break;\n+\n+    case V8SFmode:\n+    case V8SImode:\n+    case V32QImode:\n+    case V16HImode:\n+    case V4DFmode:\n+    case V4DImode:\n+      /* In 64bit, we pass TImode in interger registers and OImode on\n+\t stack.  */\n+      if (!type || !AGGREGATE_TYPE_P (type))\n+\t{\n+\t  if (!TARGET_AVX && !warnedavx && cum->warn_avx)\n+\t    {\n+\t      warnedavx = true;\n+\t      warning (0, \"AVX vector argument without AVX enabled \"\n+\t\t       \"changes the ABI\");\n+\t    }\n+\t}\n+\n+      /* Unnamed 256bit vector mode parameters are passed on stack.  */\n+      if (!named)\n+\treturn NULL;\n+      break;\n+    }\n+\n   return construct_container (mode, orig_mode, type, 0, cum->nregs,\n \t\t\t      cum->sse_nregs,\n \t\t\t      &x86_64_int_parameter_registers [cum->regno],\n@@ -5578,7 +5700,7 @@ function_arg (CUMULATIVE_ARGS *cum, enum machine_mode omode,\n   if (TARGET_64BIT && (cum ? cum->call_abi : DEFAULT_ABI) == MS_ABI)\n     return function_arg_ms_64 (cum, mode, omode, named, bytes);\n   else if (TARGET_64BIT)\n-    return function_arg_64 (cum, mode, omode, type);\n+    return function_arg_64 (cum, mode, omode, type, named);\n   else\n     return function_arg_32 (cum, mode, omode, type, bytes, words);\n }\n@@ -6202,27 +6324,37 @@ setup_incoming_varargs_64 (CUMULATIVE_ARGS *cum)\n       label_ref = gen_rtx_LABEL_REF (Pmode, label);\n \n       /* Compute address to jump to :\n-         label - eax*4 + nnamed_sse_arguments*4  */\n+         label - eax*4 + nnamed_sse_arguments*4 Or\n+         label - eax*5 + nnamed_sse_arguments*5 for AVX.  */\n       tmp_reg = gen_reg_rtx (Pmode);\n       nsse_reg = gen_reg_rtx (Pmode);\n       emit_insn (gen_zero_extendqidi2 (nsse_reg, gen_rtx_REG (QImode, AX_REG)));\n       emit_insn (gen_rtx_SET (VOIDmode, tmp_reg,\n \t\t\t      gen_rtx_MULT (Pmode, nsse_reg,\n \t\t\t\t\t    GEN_INT (4))));\n+\n+      /* vmovaps is one byte longer than movaps.  */\n+      if (TARGET_AVX)\n+\temit_insn (gen_rtx_SET (VOIDmode, tmp_reg,\n+\t\t\t\tgen_rtx_PLUS (Pmode, tmp_reg,\n+\t\t\t\t\t      nsse_reg)));\n+\n       if (cum->sse_regno)\n \temit_move_insn\n \t  (nsse_reg,\n \t   gen_rtx_CONST (DImode,\n \t\t\t  gen_rtx_PLUS (DImode,\n \t\t\t\t\tlabel_ref,\n-\t\t\t\t\tGEN_INT (cum->sse_regno * 4))));\n+\t\t\t\t\tGEN_INT (cum->sse_regno\n+\t\t\t\t\t\t * (TARGET_AVX ? 5 : 4)))));\n       else\n \temit_move_insn (nsse_reg, label_ref);\n       emit_insn (gen_subdi3 (nsse_reg, nsse_reg, tmp_reg));\n \n       /* Compute address of memory block we save into.  We always use pointer\n \t pointing 127 bytes after first byte to store - this is needed to keep\n-\t instruction size limited by 4 bytes.  */\n+\t instruction size limited by 4 bytes (5 bytes for AVX) with one\n+\t byte displacement.  */\n       tmp_reg = gen_reg_rtx (Pmode);\n       emit_insn (gen_rtx_SET (VOIDmode, tmp_reg,\n \t\t\t      plus_constant (save_area,\n@@ -6416,9 +6548,28 @@ ix86_gimplify_va_arg (tree valist, tree type, gimple_seq *pre_p,\n   rsize = (size + UNITS_PER_WORD - 1) / UNITS_PER_WORD;\n \n   nat_mode = type_natural_mode (type);\n-  container = construct_container (nat_mode, TYPE_MODE (type), type, 0,\n-\t\t\t\t   X86_64_REGPARM_MAX, X86_64_SSE_REGPARM_MAX,\n-\t\t\t\t   intreg, 0);\n+  switch (nat_mode)\n+    {\n+    case V8SFmode:\n+    case V8SImode:\n+    case V32QImode:\n+    case V16HImode:\n+    case V4DFmode:\n+    case V4DImode:\n+      /* Unnamed 256bit vector mode parameters are passed on stack.  */\n+      if (ix86_cfun_abi () == SYSV_ABI)\n+\t{\n+\t  container = NULL;\n+\t  break;\n+\t}\n+\n+    default:\n+      container = construct_container (nat_mode, TYPE_MODE (type),\n+\t\t\t\t       type, 0, X86_64_REGPARM_MAX,\n+\t\t\t\t       X86_64_SSE_REGPARM_MAX, intreg,\n+\t\t\t\t       0);\n+      break;\n+    }\n \n   /* Pull the value out of the saved registers.  */\n \n@@ -6793,18 +6944,24 @@ standard_sse_mode_p (enum machine_mode mode)\n     }\n }\n \n-/* Return 1 if X is FP constant we can load to SSE register w/o using memory.\n- */\n+/* Return 1 if X is all 0s.  For all 1s, return 2 if X is in 128bit\n+   SSE modes and SSE2 is enabled,  return 3 if X is in 256bit AVX\n+   modes and AVX is enabled.  */\n+\n int\n standard_sse_constant_p (rtx x)\n {\n   enum machine_mode mode = GET_MODE (x);\n \n   if (x == const0_rtx || x == CONST0_RTX (GET_MODE (x)))\n     return 1;\n-  if (vector_all_ones_operand (x, mode)\n-      && standard_sse_mode_p (mode))\n-    return TARGET_SSE2 ? 2 : -1;\n+  if (vector_all_ones_operand (x, mode))\n+    {\n+      if (standard_sse_mode_p (mode))\n+\treturn TARGET_SSE2 ? 2 : -2;\n+      else if (VALID_AVX256_REG_MODE (mode))\n+\treturn TARGET_AVX ? 3 : -3;\n+    }\n \n   return 0;\n }\n@@ -6818,14 +6975,37 @@ standard_sse_constant_opcode (rtx insn, rtx x)\n   switch (standard_sse_constant_p (x))\n     {\n     case 1:\n-      if (get_attr_mode (insn) == MODE_V4SF)\n-        return \"xorps\\t%0, %0\";\n-      else if (get_attr_mode (insn) == MODE_V2DF)\n-        return \"xorpd\\t%0, %0\";\n-      else\n-        return \"pxor\\t%0, %0\";\n+      switch (get_attr_mode (insn))\n+\t{\n+\tcase MODE_V4SF:\n+\t  return TARGET_AVX ? \"vxorps\\t%0, %0, %0\" : \"xorps\\t%0, %0\";\n+\tcase MODE_V2DF:\n+\t  return TARGET_AVX ? \"vxorpd\\t%0, %0, %0\" : \"xorpd\\t%0, %0\";\n+\tcase MODE_TI:\n+\t  return TARGET_AVX ? \"vpxor\\t%0, %0, %0\" : \"pxor\\t%0, %0\";\n+\tcase MODE_V8SF:\n+\t  return \"vxorps\\t%x0, %x0, %x0\";\n+\tcase MODE_V4DF:\n+\t  return \"vxorpd\\t%x0, %x0, %x0\";\n+\tcase MODE_OI:\n+\t  return \"vpxor\\t%x0, %x0, %x0\";\n+\tdefault:\n+\t  gcc_unreachable ();\n+\t}\n     case 2:\n-      return \"pcmpeqd\\t%0, %0\";\n+      if (TARGET_AVX)\n+\tswitch (get_attr_mode (insn))\n+\t  {\n+\t  case MODE_V4SF:\n+\t  case MODE_V2DF:\n+\t  case MODE_TI:\n+\t    return \"vpcmpeqd\\t%0, %0, %0\";\n+\t    break;\n+\t  default:\n+\t    gcc_unreachable ();\n+\t}\n+      else\n+\treturn \"pcmpeqd\\t%0, %0\";\n     }\n   gcc_unreachable ();\n }\n@@ -10035,12 +10215,19 @@ put_condition_code (enum rtx_code code, enum machine_mode mode, int reverse,\n    If CODE is 'b', pretend the mode is QImode.\n    If CODE is 'k', pretend the mode is SImode.\n    If CODE is 'q', pretend the mode is DImode.\n+   If CODE is 'x', pretend the mode is V4SFmode.\n+   If CODE is 't', pretend the mode is V8SFmode.\n    If CODE is 'h', pretend the reg is the 'high' byte register.\n-   If CODE is 'y', print \"st(0)\" instead of \"st\", if the reg is stack op.  */\n+   If CODE is 'y', print \"st(0)\" instead of \"st\", if the reg is stack op.\n+   If CODE is 'd', duplicate the operand for AVX instruction.\n+ */\n \n void\n print_reg (rtx x, int code, FILE *file)\n {\n+  const char *reg;\n+  bool duplicated = code == 'd' && TARGET_AVX;\n+\n   gcc_assert (x == pc_rtx\n \t      || (REGNO (x) != ARG_POINTER_REGNUM\n \t\t  && REGNO (x) != FRAME_POINTER_REGNUM\n@@ -10070,6 +10257,10 @@ print_reg (rtx x, int code, FILE *file)\n     code = 3;\n   else if (code == 'h')\n     code = 0;\n+  else if (code == 'x')\n+    code = 16;\n+  else if (code == 't')\n+    code = 32;\n   else\n     code = GET_MODE_SIZE (GET_MODE (x));\n \n@@ -10101,12 +10292,14 @@ print_reg (rtx x, int code, FILE *file)\n \t}\n       return;\n     }\n+\n+  reg = NULL;\n   switch (code)\n     {\n     case 3:\n       if (STACK_TOP_P (x))\n \t{\n-\t  fputs (\"st(0)\", file);\n+\t  reg = \"st(0)\";\n \t  break;\n \t}\n       /* FALLTHRU */\n@@ -10119,21 +10312,39 @@ print_reg (rtx x, int code, FILE *file)\n     case 16:\n     case 2:\n     normal:\n-      fputs (hi_reg_name[REGNO (x)], file);\n+      reg = hi_reg_name[REGNO (x)];\n       break;\n     case 1:\n       if (REGNO (x) >= ARRAY_SIZE (qi_reg_name))\n \tgoto normal;\n-      fputs (qi_reg_name[REGNO (x)], file);\n+      reg = qi_reg_name[REGNO (x)];\n       break;\n     case 0:\n       if (REGNO (x) >= ARRAY_SIZE (qi_high_reg_name))\n \tgoto normal;\n-      fputs (qi_high_reg_name[REGNO (x)], file);\n+      reg = qi_high_reg_name[REGNO (x)];\n+      break;\n+    case 32:\n+      if (SSE_REG_P (x))\n+\t{\n+\t  gcc_assert (!duplicated);\n+\t  putc ('y', file);\n+\t  fputs (hi_reg_name[REGNO (x)] + 1, file);\n+\t  return;\n+\t}\n       break;\n     default:\n       gcc_unreachable ();\n     }\n+\n+  fputs (reg, file);\n+  if (duplicated)\n+    {\n+      if (ASSEMBLER_DIALECT == ASM_ATT)\n+\tfprintf (file, \", %%%s\", reg);\n+      else\n+\tfprintf (file, \", %s\", reg);\n+    }\n }\n \n /* Locate some local-dynamic symbol still in use by this function\n@@ -10191,8 +10402,11 @@ get_some_local_dynamic_name (void)\n    w --  likewise, print the HImode name of the register.\n    k --  likewise, print the SImode name of the register.\n    q --  likewise, print the DImode name of the register.\n+   x --  likewise, print the V4SFmode name of the register.\n+   t --  likewise, print the V8SFmode name of the register.\n    h -- print the QImode name for a \"high\" register, either ah, bh, ch or dh.\n    y -- print \"st(0)\" instead of \"st\" as a register.\n+   d -- print duplicated register operand for AVX instruction.\n    D -- print condition for SSE cmp instruction.\n    P -- if PIC, print an @PLT suffix.\n    X -- don't print any sort of PIC '@' suffix for a symbol.\n@@ -10343,12 +10557,15 @@ print_operand (FILE *file, rtx x, int code)\n \t      gcc_unreachable ();\n \t    }\n \n+\tcase 'd':\n \tcase 'b':\n \tcase 'w':\n \tcase 'k':\n \tcase 'q':\n \tcase 'h':\n+\tcase 't':\n \tcase 'y':\n+\tcase 'x':\n \tcase 'X':\n \tcase 'P':\n \t  break;\n@@ -10365,40 +10582,93 @@ print_operand (FILE *file, rtx x, int code)\n \t  /* Little bit of braindamage here.  The SSE compare instructions\n \t     does use completely different names for the comparisons that the\n \t     fp conditional moves.  */\n-\t  switch (GET_CODE (x))\n+\t  if (TARGET_AVX)\n \t    {\n-\t    case EQ:\n-\t    case UNEQ:\n-\t      fputs (\"eq\", file);\n-\t      break;\n-\t    case LT:\n-\t    case UNLT:\n-\t      fputs (\"lt\", file);\n-\t      break;\n-\t    case LE:\n-\t    case UNLE:\n-\t      fputs (\"le\", file);\n-\t      break;\n-\t    case UNORDERED:\n-\t      fputs (\"unord\", file);\n-\t      break;\n-\t    case NE:\n-\t    case LTGT:\n-\t      fputs (\"neq\", file);\n-\t      break;\n-\t    case UNGE:\n-\t    case GE:\n-\t      fputs (\"nlt\", file);\n-\t      break;\n-\t    case UNGT:\n-\t    case GT:\n-\t      fputs (\"nle\", file);\n-\t      break;\n-\t    case ORDERED:\n-\t      fputs (\"ord\", file);\n-\t      break;\n-\t    default:\n-\t      gcc_unreachable ();\n+\t      switch (GET_CODE (x))\n+\t\t{\n+\t\tcase EQ:\n+\t\t  fputs (\"eq\", file);\n+\t\t  break;\n+\t\tcase UNEQ:\n+\t\t  fputs (\"eq_us\", file);\n+\t\t  break;\n+\t\tcase LT:\n+\t\t  fputs (\"lt\", file);\n+\t\t  break;\n+\t\tcase UNLT:\n+\t\t  fputs (\"nge\", file);\n+\t\t  break;\n+\t\tcase LE:\n+\t\t  fputs (\"le\", file);\n+\t\t  break;\n+\t\tcase UNLE:\n+\t\t  fputs (\"ngt\", file);\n+\t\t  break;\n+\t\tcase UNORDERED:\n+\t\t  fputs (\"unord\", file);\n+\t\t  break;\n+\t\tcase NE:\n+\t\t  fputs (\"neq\", file);\n+\t\t  break;\n+\t\tcase LTGT:\n+\t\t  fputs (\"neq_oq\", file);\n+\t\t  break;\n+\t\tcase GE:\n+\t\t  fputs (\"ge\", file);\n+\t\t  break;\n+\t\tcase UNGE:\n+\t\t  fputs (\"nlt\", file);\n+\t\t  break;\n+\t\tcase GT:\n+\t\t  fputs (\"gt\", file);\n+\t\t  break;\n+\t\tcase UNGT:\n+\t\t  fputs (\"nle\", file);\n+\t\t  break;\n+\t\tcase ORDERED:\n+\t\t  fputs (\"ord\", file);\n+\t\t  break;\n+\t\tdefault:\n+\t\t  gcc_unreachable ();\n+\t\t}\n+\t    }\n+\t  else\n+\t    {\n+\t      switch (GET_CODE (x))\n+\t\t{\n+\t\tcase EQ:\n+\t\tcase UNEQ:\n+\t\t  fputs (\"eq\", file);\n+\t\t  break;\n+\t\tcase LT:\n+\t\tcase UNLT:\n+\t\t  fputs (\"lt\", file);\n+\t\t  break;\n+\t\tcase LE:\n+\t\tcase UNLE:\n+\t\t  fputs (\"le\", file);\n+\t\t  break;\n+\t\tcase UNORDERED:\n+\t\t  fputs (\"unord\", file);\n+\t\t  break;\n+\t\tcase NE:\n+\t\tcase LTGT:\n+\t\t  fputs (\"neq\", file);\n+\t\t  break;\n+\t\tcase UNGE:\n+\t\tcase GE:\n+\t\t  fputs (\"nlt\", file);\n+\t\t  break;\n+\t\tcase UNGT:\n+\t\tcase GT:\n+\t\t  fputs (\"nle\", file);\n+\t\t  break;\n+\t\tcase ORDERED:\n+\t\t  fputs (\"ord\", file);\n+\t\t  break;\n+\t\tdefault:\n+\t\t  gcc_unreachable ();\n+\t\t}\n \t    }\n \t  return;\n \tcase 'O':\n@@ -10951,7 +11221,7 @@ split_ti (rtx operands[], int num, rtx lo_half[], rtx hi_half[])\n const char *\n output_387_binary_op (rtx insn, rtx *operands)\n {\n-  static char buf[30];\n+  static char buf[40];\n   const char *p;\n   const char *ssep;\n   int is_sse = SSE_REG_P (operands[0]) || SSE_REG_P (operands[1]) || SSE_REG_P (operands[2]);\n@@ -10980,7 +11250,7 @@ output_387_binary_op (rtx insn, rtx *operands)\n \tp = \"fiadd\";\n       else\n \tp = \"fadd\";\n-      ssep = \"add\";\n+      ssep = \"vadd\";\n       break;\n \n     case MINUS:\n@@ -10989,7 +11259,7 @@ output_387_binary_op (rtx insn, rtx *operands)\n \tp = \"fisub\";\n       else\n \tp = \"fsub\";\n-      ssep = \"sub\";\n+      ssep = \"vsub\";\n       break;\n \n     case MULT:\n@@ -10998,7 +11268,7 @@ output_387_binary_op (rtx insn, rtx *operands)\n \tp = \"fimul\";\n       else\n \tp = \"fmul\";\n-      ssep = \"mul\";\n+      ssep = \"vmul\";\n       break;\n \n     case DIV:\n@@ -11007,7 +11277,7 @@ output_387_binary_op (rtx insn, rtx *operands)\n \tp = \"fidiv\";\n       else\n \tp = \"fdiv\";\n-      ssep = \"div\";\n+      ssep = \"vdiv\";\n       break;\n \n     default:\n@@ -11016,11 +11286,22 @@ output_387_binary_op (rtx insn, rtx *operands)\n \n   if (is_sse)\n    {\n-      strcpy (buf, ssep);\n-      if (GET_MODE (operands[0]) == SFmode)\n-\tstrcat (buf, \"ss\\t{%2, %0|%0, %2}\");\n-      else\n-\tstrcat (buf, \"sd\\t{%2, %0|%0, %2}\");\n+     if (TARGET_AVX)\n+       {\n+\t strcpy (buf, ssep);\n+\t if (GET_MODE (operands[0]) == SFmode)\n+\t   strcat (buf, \"ss\\t{%2, %1, %0|%0, %1, %2}\");\n+\t else\n+\t   strcat (buf, \"sd\\t{%2, %1, %0|%0, %1, %2}\");\n+       }\n+     else\n+       {\n+\t strcpy (buf, ssep + 1);\n+\t if (GET_MODE (operands[0]) == SFmode)\n+\t   strcat (buf, \"ss\\t{%2, %0|%0, %2}\");\n+\t else\n+\t   strcat (buf, \"sd\\t{%2, %0|%0, %2}\");\n+       }\n       return buf;\n    }\n   strcpy (buf, p);\n@@ -11382,16 +11663,21 @@ output_fp_compare (rtx insn, rtx *operands, int eflags_p, int unordered_p)\n \n   if (is_sse)\n     {\n+      static const char ucomiss[] = \"vucomiss\\t{%1, %0|%0, %1}\";\n+      static const char ucomisd[] = \"vucomisd\\t{%1, %0|%0, %1}\";\n+      static const char comiss[] = \"vcomiss\\t{%1, %0|%0, %1}\";\n+      static const char comisd[] = \"vcomisd\\t{%1, %0|%0, %1}\";\n+\n       if (GET_MODE (operands[0]) == SFmode)\n \tif (unordered_p)\n-\t  return \"ucomiss\\t{%1, %0|%0, %1}\";\n+\t  return &ucomiss[TARGET_AVX ? 0 : 1];\n \telse\n-\t  return \"comiss\\t{%1, %0|%0, %1}\";\n+\t  return &comiss[TARGET_AVX ? 0 : 1];\n       else\n \tif (unordered_p)\n-\t  return \"ucomisd\\t{%1, %0|%0, %1}\";\n+\t  return &ucomisd[TARGET_AVX ? 0 : 1];\n \telse\n-\t  return \"comisd\\t{%1, %0|%0, %1}\";\n+\t  return &comisd[TARGET_AVX ? 0 : 1];\n     }\n \n   gcc_assert (STACK_TOP_P (cmp_op0));\n@@ -11805,6 +12091,58 @@ ix86_expand_vector_move_misalign (enum machine_mode mode, rtx operands[])\n   op0 = operands[0];\n   op1 = operands[1];\n \n+  if (TARGET_AVX)\n+    {\n+      switch (GET_MODE_CLASS (mode))\n+\t{\n+\tcase MODE_VECTOR_INT:\n+\tcase MODE_INT:\n+\t  switch (GET_MODE_SIZE (mode))\n+\t    {\n+\t    case 16:\n+\t      op0 = gen_lowpart (V16QImode, op0);\n+\t      op1 = gen_lowpart (V16QImode, op1);\n+\t      emit_insn (gen_avx_movdqu (op0, op1));\n+\t      break;\n+\t    case 32:\n+\t      op0 = gen_lowpart (V32QImode, op0);\n+\t      op1 = gen_lowpart (V32QImode, op1);\n+\t      emit_insn (gen_avx_movdqu256 (op0, op1));\n+\t      break;\n+\t    default:\n+\t      gcc_unreachable ();\n+\t    }\n+\t  break;\n+\tcase MODE_VECTOR_FLOAT:\n+\t  op0 = gen_lowpart (mode, op0);\n+\t  op1 = gen_lowpart (mode, op1);\n+\n+\t  switch (mode)\n+\t    { \n+\t    case V4SFmode:\n+\t      emit_insn (gen_avx_movups (op0, op1));\n+\t      break;\n+\t    case V8SFmode:\n+\t      emit_insn (gen_avx_movups256 (op0, op1));\n+\t      break;\n+\t    case V2DFmode:\n+\t      emit_insn (gen_avx_movupd (op0, op1));\n+\t      break;\n+\t    case V4DFmode:\n+\t      emit_insn (gen_avx_movupd256 (op0, op1));\n+\t      break;\n+\t    default:\n+\t      gcc_unreachable ();\n+\t    }\n+\t  break;\n+\n+\tdefault:\n+\t  gcc_unreachable ();\n+\t}\n+\n+      return;\n+    }\n+\n   if (MEM_P (op1))\n     {\n       /* If we're optimizing for size, movups is the smallest.  */\n@@ -17927,6 +18265,44 @@ ix86_attr_length_address_default (rtx insn)\n       }\n   return 0;\n }\n+\n+/* Compute default value for \"length_vex\" attribute. It includes\n+   2 or 3 byte VEX prefix and 1 opcode byte.  */\n+\n+int\n+ix86_attr_length_vex_default (rtx insn, int has_0f_opcode,\n+\t\t\t      int has_vex_w)\n+{\n+  int i;\n+\n+  /* Only 0f opcode can use 2 byte VEX prefix and  VEX W bit uses 3\n+     byte VEX prefix.  */\n+  if (!has_0f_opcode || has_vex_w)\n+    return 3 + 1;\n+\n+ /* We can always use 2 byte VEX prefix in 32bit.  */\n+  if (!TARGET_64BIT)\n+    return 2 + 1;\n+\n+  extract_insn_cached (insn);\n+\n+  for (i = recog_data.n_operands - 1; i >= 0; --i)\n+    if (REG_P (recog_data.operand[i]))\n+      {\n+\t/* REX.W bit uses 3 byte VEX prefix.  */\n+\tif (GET_MODE (recog_data.operand[i]) == DImode)\n+\t  return 3 + 1;\n+      }\n+    else\n+      {\n+\t/* REX.X or REX.B bits use 3 byte VEX prefix.  */\n+\tif (MEM_P (recog_data.operand[i])\n+\t    && x86_extended_reg_mentioned_p (recog_data.operand[i]))\n+\t  return 3 + 1;\n+      }\n+\n+  return 2 + 1;\n+}\n \f\n /* Return the maximum number of instructions a cpu can issue.  */\n \n@@ -18994,6 +19370,144 @@ enum ix86_builtins\n   /* PCLMUL instruction */\n   IX86_BUILTIN_PCLMULQDQ128,\n \n+  /* AVX */\n+  IX86_BUILTIN_ADDPD256,\n+  IX86_BUILTIN_ADDPS256,\n+  IX86_BUILTIN_ADDSUBPD256,\n+  IX86_BUILTIN_ADDSUBPS256,\n+  IX86_BUILTIN_ANDPD256,\n+  IX86_BUILTIN_ANDPS256,\n+  IX86_BUILTIN_ANDNPD256,\n+  IX86_BUILTIN_ANDNPS256,\n+  IX86_BUILTIN_BLENDPD256,\n+  IX86_BUILTIN_BLENDPS256,\n+  IX86_BUILTIN_BLENDVPD256,\n+  IX86_BUILTIN_BLENDVPS256,\n+  IX86_BUILTIN_DIVPD256,\n+  IX86_BUILTIN_DIVPS256,\n+  IX86_BUILTIN_DPPS256,\n+  IX86_BUILTIN_HADDPD256,\n+  IX86_BUILTIN_HADDPS256,\n+  IX86_BUILTIN_HSUBPD256,\n+  IX86_BUILTIN_HSUBPS256,\n+  IX86_BUILTIN_MAXPD256,\n+  IX86_BUILTIN_MAXPS256,\n+  IX86_BUILTIN_MINPD256,\n+  IX86_BUILTIN_MINPS256,\n+  IX86_BUILTIN_MULPD256,\n+  IX86_BUILTIN_MULPS256,\n+  IX86_BUILTIN_ORPD256,\n+  IX86_BUILTIN_ORPS256,\n+  IX86_BUILTIN_SHUFPD256,\n+  IX86_BUILTIN_SHUFPS256,\n+  IX86_BUILTIN_SUBPD256,\n+  IX86_BUILTIN_SUBPS256,\n+  IX86_BUILTIN_XORPD256,\n+  IX86_BUILTIN_XORPS256,\n+  IX86_BUILTIN_CMPSD,\n+  IX86_BUILTIN_CMPSS,\n+  IX86_BUILTIN_CMPPD,\n+  IX86_BUILTIN_CMPPS,\n+  IX86_BUILTIN_CMPPD256,\n+  IX86_BUILTIN_CMPPS256,\n+  IX86_BUILTIN_CVTDQ2PD256,\n+  IX86_BUILTIN_CVTDQ2PS256,\n+  IX86_BUILTIN_CVTPD2PS256,\n+  IX86_BUILTIN_CVTPS2DQ256,\n+  IX86_BUILTIN_CVTPS2PD256,\n+  IX86_BUILTIN_CVTTPD2DQ256,\n+  IX86_BUILTIN_CVTPD2DQ256,\n+  IX86_BUILTIN_CVTTPS2DQ256,\n+  IX86_BUILTIN_EXTRACTF128PD256,\n+  IX86_BUILTIN_EXTRACTF128PS256,\n+  IX86_BUILTIN_EXTRACTF128SI256,\n+  IX86_BUILTIN_VZEROALL,\n+  IX86_BUILTIN_VZEROUPPER,\n+  IX86_BUILTIN_VZEROUPPER_REX64,\n+  IX86_BUILTIN_VPERMILVARPD,\n+  IX86_BUILTIN_VPERMILVARPS,\n+  IX86_BUILTIN_VPERMILVARPD256,\n+  IX86_BUILTIN_VPERMILVARPS256,\n+  IX86_BUILTIN_VPERMILPD,\n+  IX86_BUILTIN_VPERMILPS,\n+  IX86_BUILTIN_VPERMILPD256,\n+  IX86_BUILTIN_VPERMILPS256,\n+  IX86_BUILTIN_VPERMIL2PD,\n+  IX86_BUILTIN_VPERMIL2PS,\n+  IX86_BUILTIN_VPERMIL2PD256,\n+  IX86_BUILTIN_VPERMIL2PS256,\n+  IX86_BUILTIN_VPERM2F128PD256,\n+  IX86_BUILTIN_VPERM2F128PS256,\n+  IX86_BUILTIN_VPERM2F128SI256,\n+  IX86_BUILTIN_VBROADCASTSS,\n+  IX86_BUILTIN_VBROADCASTSD256,\n+  IX86_BUILTIN_VBROADCASTSS256,\n+  IX86_BUILTIN_VBROADCASTPD256,\n+  IX86_BUILTIN_VBROADCASTPS256,\n+  IX86_BUILTIN_VINSERTF128PD256,\n+  IX86_BUILTIN_VINSERTF128PS256,\n+  IX86_BUILTIN_VINSERTF128SI256,\n+  IX86_BUILTIN_LOADUPD256,\n+  IX86_BUILTIN_LOADUPS256,\n+  IX86_BUILTIN_STOREUPD256,\n+  IX86_BUILTIN_STOREUPS256,\n+  IX86_BUILTIN_LDDQU256,\n+  IX86_BUILTIN_LOADDQU256,\n+  IX86_BUILTIN_STOREDQU256,\n+  IX86_BUILTIN_MASKLOADPD,\n+  IX86_BUILTIN_MASKLOADPS,\n+  IX86_BUILTIN_MASKSTOREPD,\n+  IX86_BUILTIN_MASKSTOREPS,\n+  IX86_BUILTIN_MASKLOADPD256,\n+  IX86_BUILTIN_MASKLOADPS256,\n+  IX86_BUILTIN_MASKSTOREPD256,\n+  IX86_BUILTIN_MASKSTOREPS256,\n+  IX86_BUILTIN_MOVSHDUP256,\n+  IX86_BUILTIN_MOVSLDUP256,\n+  IX86_BUILTIN_MOVDDUP256,\n+\n+  IX86_BUILTIN_SQRTPD256,\n+  IX86_BUILTIN_SQRTPS256,\n+  IX86_BUILTIN_SQRTPS_NR256,\n+  IX86_BUILTIN_RSQRTPS256,\n+  IX86_BUILTIN_RSQRTPS_NR256,\n+\n+  IX86_BUILTIN_RCPPS256,\n+\n+  IX86_BUILTIN_ROUNDPD256,\n+  IX86_BUILTIN_ROUNDPS256,\n+\n+  IX86_BUILTIN_UNPCKHPD256,\n+  IX86_BUILTIN_UNPCKLPD256,\n+  IX86_BUILTIN_UNPCKHPS256,\n+  IX86_BUILTIN_UNPCKLPS256,\n+\n+  IX86_BUILTIN_SI256_SI,\n+  IX86_BUILTIN_PS256_PS,\n+  IX86_BUILTIN_PD256_PD,\n+  IX86_BUILTIN_SI_SI256,\n+  IX86_BUILTIN_PS_PS256,\n+  IX86_BUILTIN_PD_PD256,\n+\n+  IX86_BUILTIN_VTESTZPD,\n+  IX86_BUILTIN_VTESTCPD,\n+  IX86_BUILTIN_VTESTNZCPD,\n+  IX86_BUILTIN_VTESTZPS,\n+  IX86_BUILTIN_VTESTCPS,\n+  IX86_BUILTIN_VTESTNZCPS,\n+  IX86_BUILTIN_VTESTZPD256,\n+  IX86_BUILTIN_VTESTCPD256,\n+  IX86_BUILTIN_VTESTNZCPD256,\n+  IX86_BUILTIN_VTESTZPS256,\n+  IX86_BUILTIN_VTESTCPS256,\n+  IX86_BUILTIN_VTESTNZCPS256,\n+  IX86_BUILTIN_PTESTZ256,\n+  IX86_BUILTIN_PTESTC256,\n+  IX86_BUILTIN_PTESTNZC256,\n+\n+  IX86_BUILTIN_MOVMSKPD256,\n+  IX86_BUILTIN_MOVMSKPS256,\n+\n   /* TFmode support builtins.  */\n   IX86_BUILTIN_INFQ,\n   IX86_BUILTIN_FABSQ,\n@@ -19328,19 +19842,35 @@ enum ix86_special_builtin_type\n {\n   SPECIAL_FTYPE_UNKNOWN,\n   VOID_FTYPE_VOID,\n+  V32QI_FTYPE_PCCHAR,\n   V16QI_FTYPE_PCCHAR,\n+  V8SF_FTYPE_PCV4SF,\n+  V8SF_FTYPE_PCFLOAT,\n+  V4DF_FTYPE_PCV2DF,\n+  V4DF_FTYPE_PCDOUBLE,\n   V4SF_FTYPE_PCFLOAT,\n   V2DF_FTYPE_PCDOUBLE,\n+  V8SF_FTYPE_PCV8SF_V8SF,\n+  V4DF_FTYPE_PCV4DF_V4DF,\n   V4SF_FTYPE_V4SF_PCV2SF,\n+  V4SF_FTYPE_PCV4SF_V4SF,\n   V2DF_FTYPE_V2DF_PCDOUBLE,\n+  V2DF_FTYPE_PCV2DF_V2DF,\n   V2DI_FTYPE_PV2DI,\n   VOID_FTYPE_PV2SF_V4SF,\n   VOID_FTYPE_PV2DI_V2DI,\n+  VOID_FTYPE_PCHAR_V32QI,\n   VOID_FTYPE_PCHAR_V16QI,\n+  VOID_FTYPE_PFLOAT_V8SF,\n   VOID_FTYPE_PFLOAT_V4SF,\n+  VOID_FTYPE_PDOUBLE_V4DF,\n   VOID_FTYPE_PDOUBLE_V2DF,\n   VOID_FTYPE_PDI_DI,\n-  VOID_FTYPE_PINT_INT\n+  VOID_FTYPE_PINT_INT,\n+  VOID_FTYPE_PV8SF_V8SF_V8SF,\n+  VOID_FTYPE_PV4DF_V4DF_V4DF,\n+  VOID_FTYPE_PV4SF_V4SF_V4SF,\n+  VOID_FTYPE_PV2DF_V2DF_V2DF\n };\n \n /* Builtin types */\n@@ -19350,25 +19880,45 @@ enum ix86_builtin_type\n   FLOAT128_FTYPE_FLOAT128,\n   FLOAT_FTYPE_FLOAT,\n   FLOAT128_FTYPE_FLOAT128_FLOAT128,\n+  INT_FTYPE_V8SF_V8SF_PTEST,\n+  INT_FTYPE_V4DI_V4DI_PTEST,\n+  INT_FTYPE_V4DF_V4DF_PTEST,\n+  INT_FTYPE_V4SF_V4SF_PTEST,\n   INT_FTYPE_V2DI_V2DI_PTEST,\n+  INT_FTYPE_V2DF_V2DF_PTEST,\n   INT64_FTYPE_V4SF,\n   INT64_FTYPE_V2DF,\n   INT_FTYPE_V16QI,\n   INT_FTYPE_V8QI,\n+  INT_FTYPE_V8SF,\n+  INT_FTYPE_V4DF,\n   INT_FTYPE_V4SF,\n   INT_FTYPE_V2DF,\n   V16QI_FTYPE_V16QI,\n+  V8SI_FTYPE_V8SF,\n+  V8SI_FTYPE_V4SI,\n   V8HI_FTYPE_V8HI,\n   V8HI_FTYPE_V16QI,\n   V8QI_FTYPE_V8QI,\n+  V8SF_FTYPE_V8SF,\n+  V8SF_FTYPE_V8SI,\n+  V8SF_FTYPE_V4SF,\n   V4SI_FTYPE_V4SI,\n   V4SI_FTYPE_V16QI,\n+  V4SI_FTYPE_V8SI,\n   V4SI_FTYPE_V8HI,\n+  V4SI_FTYPE_V4DF,\n   V4SI_FTYPE_V4SF,\n   V4SI_FTYPE_V2DF,\n   V4HI_FTYPE_V4HI,\n+  V4DF_FTYPE_V4DF,\n+  V4DF_FTYPE_V4SI,\n+  V4DF_FTYPE_V4SF,\n+  V4DF_FTYPE_V2DF,\n+  V4SF_FTYPE_V4DF,\n   V4SF_FTYPE_V4SF,\n   V4SF_FTYPE_V4SF_VEC_MERGE,\n+  V4SF_FTYPE_V8SF,\n   V4SF_FTYPE_V4SI,\n   V4SF_FTYPE_V2DF,\n   V2DI_FTYPE_V2DI,\n@@ -19378,6 +19928,7 @@ enum ix86_builtin_type\n   V2DF_FTYPE_V2DF,\n   V2DF_FTYPE_V2DF_VEC_MERGE,\n   V2DF_FTYPE_V4SI,\n+  V2DF_FTYPE_V4DF,\n   V2DF_FTYPE_V4SF,\n   V2DF_FTYPE_V2SI,\n   V2SI_FTYPE_V2SI,\n@@ -19395,6 +19946,8 @@ enum ix86_builtin_type\n   V8HI_FTYPE_V16QI_V16QI,\n   V8HI_FTYPE_V4SI_V4SI,\n   V8HI_FTYPE_V8HI_SI_COUNT,\n+  V8SF_FTYPE_V8SF_V8SF,\n+  V8SF_FTYPE_V8SF_V8SI,\n   V4SI_FTYPE_V4SI_V4SI,\n   V4SI_FTYPE_V4SI_V4SI_COUNT,\n   V4SI_FTYPE_V8HI_V8HI,\n@@ -19406,8 +19959,11 @@ enum ix86_builtin_type\n   V4HI_FTYPE_V8QI_V8QI,\n   V4HI_FTYPE_V2SI_V2SI,\n   V4HI_FTYPE_V4HI_SI_COUNT,\n+  V4DF_FTYPE_V4DF_V4DF,\n+  V4DF_FTYPE_V4DF_V4DI,\n   V4SF_FTYPE_V4SF_V4SF,\n   V4SF_FTYPE_V4SF_V4SF_SWAP,\n+  V4SF_FTYPE_V4SF_V4SI,\n   V4SF_FTYPE_V4SF_V2SI,\n   V4SF_FTYPE_V4SF_V2DF,\n   V4SF_FTYPE_V4SF_DI,\n@@ -19427,6 +19983,7 @@ enum ix86_builtin_type\n   V2DF_FTYPE_V2DF_V2DF,\n   V2DF_FTYPE_V2DF_V2DF_SWAP,\n   V2DF_FTYPE_V2DF_V4SF,\n+  V2DF_FTYPE_V2DF_V2DI,\n   V2DF_FTYPE_V2DF_DI,\n   V2DF_FTYPE_V2DF_SI,\n   V2SF_FTYPE_V2SF_V2SF,\n@@ -19442,21 +19999,38 @@ enum ix86_builtin_type\n   V8HI_FTYPE_V8HI_INT,\n   V4SI_FTYPE_V4SI_INT,\n   V4HI_FTYPE_V4HI_INT,\n+  V8SF_FTYPE_V8SF_INT,\n+  V4SI_FTYPE_V8SI_INT,\n+  V4SF_FTYPE_V8SF_INT,\n+  V2DF_FTYPE_V4DF_INT,\n+  V4DF_FTYPE_V4DF_INT,\n   V4SF_FTYPE_V4SF_INT,\n   V2DI_FTYPE_V2DI_INT,\n   V2DI2TI_FTYPE_V2DI_INT,\n   V2DF_FTYPE_V2DF_INT,\n   V16QI_FTYPE_V16QI_V16QI_V16QI,\n+  V8SF_FTYPE_V8SF_V8SF_V8SF,\n+  V4DF_FTYPE_V4DF_V4DF_V4DF,\n   V4SF_FTYPE_V4SF_V4SF_V4SF,\n   V2DF_FTYPE_V2DF_V2DF_V2DF,\n   V16QI_FTYPE_V16QI_V16QI_INT,\n+  V8SI_FTYPE_V8SI_V8SI_INT,\n+  V8SI_FTYPE_V8SI_V4SI_INT,\n   V8HI_FTYPE_V8HI_V8HI_INT,\n+  V8SF_FTYPE_V8SF_V8SF_INT,\n+  V8SF_FTYPE_V8SF_V4SF_INT,\n   V4SI_FTYPE_V4SI_V4SI_INT,\n+  V4DF_FTYPE_V4DF_V4DF_INT,\n+  V4DF_FTYPE_V4DF_V2DF_INT,\n   V4SF_FTYPE_V4SF_V4SF_INT,\n   V2DI_FTYPE_V2DI_V2DI_INT,\n   V2DI2TI_FTYPE_V2DI_V2DI_INT,\n   V1DI2DI_FTYPE_V1DI_V1DI_INT,\n   V2DF_FTYPE_V2DF_V2DF_INT,\n+  V8SF_FTYPE_V8SF_V8SF_V8SI_INT,\n+  V4DF_FTYPE_V4DF_V4DF_V4DI_INT,\n+  V4SF_FTYPE_V4SF_V4SF_V4SI_INT,\n+  V2DF_FTYPE_V2DF_V2DF_V2DI_INT,\n   V2DI_FTYPE_V2DI_UINT_UINT,\n   V2DI_FTYPE_V2DI_V2DI_UINT_UINT\n };\n@@ -19507,6 +20081,34 @@ static const struct builtin_description bdesc_special_args[] =\n   /* SSE4A */\n   { OPTION_MASK_ISA_SSE4A, CODE_FOR_sse4a_vmmovntv2df, \"__builtin_ia32_movntsd\", IX86_BUILTIN_MOVNTSD, UNKNOWN, (int) VOID_FTYPE_PDOUBLE_V2DF },\n   { OPTION_MASK_ISA_SSE4A, CODE_FOR_sse4a_vmmovntv4sf, \"__builtin_ia32_movntss\", IX86_BUILTIN_MOVNTSS, UNKNOWN, (int) VOID_FTYPE_PFLOAT_V4SF },\n+\n+  /* AVX */\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vzeroall, \"__builtin_ia32_vzeroall\", IX86_BUILTIN_VZEROALL, UNKNOWN, (int) VOID_FTYPE_VOID },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vzeroupper, 0, IX86_BUILTIN_VZEROUPPER, UNKNOWN, (int) VOID_FTYPE_VOID },\n+  { OPTION_MASK_ISA_AVX | OPTION_MASK_ISA_64BIT, CODE_FOR_avx_vzeroupper_rex64, 0, IX86_BUILTIN_VZEROUPPER_REX64, UNKNOWN, (int) VOID_FTYPE_VOID },\n+\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vbroadcastss, \"__builtin_ia32_vbroadcastss\", IX86_BUILTIN_VBROADCASTSS, UNKNOWN, (int) V4SF_FTYPE_PCFLOAT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vbroadcastsd256, \"__builtin_ia32_vbroadcastsd256\", IX86_BUILTIN_VBROADCASTSD256, UNKNOWN, (int) V4DF_FTYPE_PCDOUBLE },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vbroadcastss256, \"__builtin_ia32_vbroadcastss256\", IX86_BUILTIN_VBROADCASTSS256, UNKNOWN, (int) V8SF_FTYPE_PCFLOAT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vbroadcastf128_pd256, \"__builtin_ia32_vbroadcastf128_pd256\", IX86_BUILTIN_VBROADCASTPD256, UNKNOWN, (int) V4DF_FTYPE_PCV2DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vbroadcastf128_ps256, \"__builtin_ia32_vbroadcastf128_ps256\", IX86_BUILTIN_VBROADCASTPS256, UNKNOWN, (int) V8SF_FTYPE_PCV4SF },\n+\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_movupd256, \"__builtin_ia32_loadupd256\", IX86_BUILTIN_LOADUPD256, UNKNOWN, (int) V4DF_FTYPE_PCDOUBLE },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_movups256, \"__builtin_ia32_loadups256\", IX86_BUILTIN_LOADUPS256, UNKNOWN, (int) V8SF_FTYPE_PCFLOAT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_movupd256, \"__builtin_ia32_storeupd256\", IX86_BUILTIN_STOREUPD256, UNKNOWN, (int) VOID_FTYPE_PDOUBLE_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_movups256, \"__builtin_ia32_storeups256\", IX86_BUILTIN_STOREUPS256, UNKNOWN, (int) VOID_FTYPE_PFLOAT_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_movdqu256, \"__builtin_ia32_loaddqu256\", IX86_BUILTIN_LOADDQU256, UNKNOWN, (int) V32QI_FTYPE_PCCHAR },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_movdqu256, \"__builtin_ia32_storedqu256\", IX86_BUILTIN_STOREDQU256, UNKNOWN, (int) VOID_FTYPE_PCHAR_V32QI },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_lddqu256, \"__builtin_ia32_lddqu256\", IX86_BUILTIN_LDDQU256, UNKNOWN, (int) V32QI_FTYPE_PCCHAR },\n+\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_maskloadpd, \"__builtin_ia32_maskloadpd\", IX86_BUILTIN_MASKLOADPD, UNKNOWN, (int) V2DF_FTYPE_PCV2DF_V2DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_maskloadps, \"__builtin_ia32_maskloadps\", IX86_BUILTIN_MASKLOADPS, UNKNOWN, (int) V4SF_FTYPE_PCV4SF_V4SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_maskloadpd256, \"__builtin_ia32_maskloadpd256\", IX86_BUILTIN_MASKLOADPD256, UNKNOWN, (int) V4DF_FTYPE_PCV4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_maskloadps256, \"__builtin_ia32_maskloadps256\", IX86_BUILTIN_MASKLOADPS256, UNKNOWN, (int) V8SF_FTYPE_PCV8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_maskstorepd, \"__builtin_ia32_maskstorepd\", IX86_BUILTIN_MASKSTOREPD, UNKNOWN, (int) VOID_FTYPE_PV2DF_V2DF_V2DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_maskstoreps, \"__builtin_ia32_maskstoreps\", IX86_BUILTIN_MASKSTOREPS, UNKNOWN, (int) VOID_FTYPE_PV4SF_V4SF_V4SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_maskstorepd256, \"__builtin_ia32_maskstorepd256\", IX86_BUILTIN_MASKSTOREPD256, UNKNOWN, (int) VOID_FTYPE_PV4DF_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_maskstoreps256, \"__builtin_ia32_maskstoreps256\", IX86_BUILTIN_MASKSTOREPS256, UNKNOWN, (int) VOID_FTYPE_PV8SF_V8SF_V8SF },\n };\n \n /* Builtins with variable number of arguments.  */\n@@ -19991,6 +20593,124 @@ static const struct builtin_description bdesc_args[] =\n \n   /* PCLMUL */\n   { OPTION_MASK_ISA_SSE2, CODE_FOR_pclmulqdq, 0, IX86_BUILTIN_PCLMULQDQ128, UNKNOWN, (int) V2DI_FTYPE_V2DI_V2DI_INT },\n+\n+  /* AVX */\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_addv4df3, \"__builtin_ia32_addpd256\", IX86_BUILTIN_ADDPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_addv8sf3, \"__builtin_ia32_addps256\", IX86_BUILTIN_ADDPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_addsubv4df3, \"__builtin_ia32_addsubpd256\", IX86_BUILTIN_ADDSUBPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_addsubv8sf3, \"__builtin_ia32_addsubps256\", IX86_BUILTIN_ADDSUBPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_andv4df3, \"__builtin_ia32_andpd256\", IX86_BUILTIN_ANDPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_andv8sf3, \"__builtin_ia32_andps256\", IX86_BUILTIN_ANDPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_nandv4df3, \"__builtin_ia32_andnpd256\", IX86_BUILTIN_ANDNPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_nandv8sf3, \"__builtin_ia32_andnps256\", IX86_BUILTIN_ANDNPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_divv4df3, \"__builtin_ia32_divpd256\", IX86_BUILTIN_DIVPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_divv8sf3, \"__builtin_ia32_divps256\", IX86_BUILTIN_DIVPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_haddv4df3, \"__builtin_ia32_haddpd256\", IX86_BUILTIN_HADDPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_hsubv8sf3, \"__builtin_ia32_hsubps256\", IX86_BUILTIN_HSUBPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_hsubv4df3, \"__builtin_ia32_hsubpd256\", IX86_BUILTIN_HSUBPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_haddv8sf3, \"__builtin_ia32_haddps256\", IX86_BUILTIN_HADDPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_smaxv4df3, \"__builtin_ia32_maxpd256\", IX86_BUILTIN_MAXPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_smaxv8sf3, \"__builtin_ia32_maxps256\", IX86_BUILTIN_MAXPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_sminv4df3, \"__builtin_ia32_minpd256\", IX86_BUILTIN_MINPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_sminv8sf3, \"__builtin_ia32_minps256\", IX86_BUILTIN_MINPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_mulv4df3, \"__builtin_ia32_mulpd256\", IX86_BUILTIN_MULPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_mulv8sf3, \"__builtin_ia32_mulps256\", IX86_BUILTIN_MULPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_iorv4df3, \"__builtin_ia32_orpd256\", IX86_BUILTIN_ORPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_iorv8sf3, \"__builtin_ia32_orps256\", IX86_BUILTIN_ORPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_subv4df3, \"__builtin_ia32_subpd256\", IX86_BUILTIN_SUBPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_subv8sf3, \"__builtin_ia32_subps256\", IX86_BUILTIN_SUBPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_xorv4df3, \"__builtin_ia32_xorpd256\", IX86_BUILTIN_XORPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_xorv8sf3, \"__builtin_ia32_xorps256\", IX86_BUILTIN_XORPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vpermilvarv2df3, \"__builtin_ia32_vpermilvarpd\", IX86_BUILTIN_VPERMILVARPD, UNKNOWN, (int) V2DF_FTYPE_V2DF_V2DI },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vpermilvarv4sf3, \"__builtin_ia32_vpermilvarps\", IX86_BUILTIN_VPERMILVARPS, UNKNOWN, (int) V4SF_FTYPE_V4SF_V4SI },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vpermilvarv4df3, \"__builtin_ia32_vpermilvarpd256\", IX86_BUILTIN_VPERMILVARPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DI },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vpermilvarv8sf3, \"__builtin_ia32_vpermilvarps256\", IX86_BUILTIN_VPERMILVARPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SI },\n+\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_blendpd256, \"__builtin_ia32_blendpd256\", IX86_BUILTIN_BLENDPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_blendps256, \"__builtin_ia32_blendps256\", IX86_BUILTIN_BLENDPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_blendvpd256, \"__builtin_ia32_blendvpd256\", IX86_BUILTIN_BLENDVPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_blendvps256, \"__builtin_ia32_blendvps256\", IX86_BUILTIN_BLENDVPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_dpps256, \"__builtin_ia32_dpps256\", IX86_BUILTIN_DPPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_shufpd256, \"__builtin_ia32_shufpd256\", IX86_BUILTIN_SHUFPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_shufps256, \"__builtin_ia32_shufps256\", IX86_BUILTIN_SHUFPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_cmpsdv2df3, \"__builtin_ia32_cmpsd\", IX86_BUILTIN_CMPSD, UNKNOWN, (int) V2DF_FTYPE_V2DF_V2DF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_cmpssv4sf3, \"__builtin_ia32_cmpss\", IX86_BUILTIN_CMPSS, UNKNOWN, (int) V4SF_FTYPE_V4SF_V4SF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_cmppdv2df3, \"__builtin_ia32_cmppd\", IX86_BUILTIN_CMPPD, UNKNOWN, (int) V2DF_FTYPE_V2DF_V2DF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_cmppsv4sf3, \"__builtin_ia32_cmpps\", IX86_BUILTIN_CMPPS, UNKNOWN, (int) V4SF_FTYPE_V4SF_V4SF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_cmppdv4df3, \"__builtin_ia32_cmppd256\", IX86_BUILTIN_CMPPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_cmppsv8sf3, \"__builtin_ia32_cmpps256\", IX86_BUILTIN_CMPPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vextractf128v4df, \"__builtin_ia32_vextractf128_pd256\", IX86_BUILTIN_EXTRACTF128PD256, UNKNOWN, (int) V2DF_FTYPE_V4DF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vextractf128v8sf, \"__builtin_ia32_vextractf128_ps256\", IX86_BUILTIN_EXTRACTF128PS256, UNKNOWN, (int) V4SF_FTYPE_V8SF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vextractf128v8si, \"__builtin_ia32_vextractf128_si256\", IX86_BUILTIN_EXTRACTF128SI256, UNKNOWN, (int) V4SI_FTYPE_V8SI_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_cvtdq2pd256, \"__builtin_ia32_cvtdq2pd256\", IX86_BUILTIN_CVTDQ2PD256, UNKNOWN, (int) V4DF_FTYPE_V4SI },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_cvtdq2ps256, \"__builtin_ia32_cvtdq2ps256\", IX86_BUILTIN_CVTDQ2PS256, UNKNOWN, (int) V8SF_FTYPE_V8SI },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_cvtpd2ps256, \"__builtin_ia32_cvtpd2ps256\", IX86_BUILTIN_CVTPD2PS256, UNKNOWN, (int) V4SF_FTYPE_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_cvtps2dq256, \"__builtin_ia32_cvtps2dq256\", IX86_BUILTIN_CVTPS2DQ256, UNKNOWN, (int) V8SI_FTYPE_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_cvtps2pd256, \"__builtin_ia32_cvtps2pd256\", IX86_BUILTIN_CVTPS2PD256, UNKNOWN, (int) V4DF_FTYPE_V4SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_cvttpd2dq256, \"__builtin_ia32_cvttpd2dq256\", IX86_BUILTIN_CVTTPD2DQ256, UNKNOWN, (int) V4SI_FTYPE_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_cvtpd2dq256, \"__builtin_ia32_cvtpd2dq256\", IX86_BUILTIN_CVTPD2DQ256, UNKNOWN, (int) V4SI_FTYPE_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_cvttps2dq256, \"__builtin_ia32_cvttps2dq256\", IX86_BUILTIN_CVTTPS2DQ256, UNKNOWN, (int) V8SI_FTYPE_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vperm2f128v4df3, \"__builtin_ia32_vperm2f128_pd256\", IX86_BUILTIN_VPERM2F128PD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vperm2f128v8sf3, \"__builtin_ia32_vperm2f128_ps256\", IX86_BUILTIN_VPERM2F128PS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vperm2f128v8si3, \"__builtin_ia32_vperm2f128_si256\", IX86_BUILTIN_VPERM2F128SI256, UNKNOWN, (int) V8SI_FTYPE_V8SI_V8SI_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vpermilv2df, \"__builtin_ia32_vpermilpd\", IX86_BUILTIN_VPERMILPD, UNKNOWN, (int) V2DF_FTYPE_V2DF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vpermilv4sf, \"__builtin_ia32_vpermilps\", IX86_BUILTIN_VPERMILPS, UNKNOWN, (int) V4SF_FTYPE_V4SF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vpermilv4df, \"__builtin_ia32_vpermilpd256\", IX86_BUILTIN_VPERMILPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vpermilv8sf, \"__builtin_ia32_vpermilps256\", IX86_BUILTIN_VPERMILPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vpermil2v2df3,  \"__builtin_ia32_vpermil2pd\", IX86_BUILTIN_VPERMIL2PD, UNKNOWN, (int) V2DF_FTYPE_V2DF_V2DF_V2DI_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vpermil2v4sf3,  \"__builtin_ia32_vpermil2ps\", IX86_BUILTIN_VPERMIL2PS, UNKNOWN, (int) V4SF_FTYPE_V4SF_V4SF_V4SI_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vpermil2v4df3,  \"__builtin_ia32_vpermil2pd256\", IX86_BUILTIN_VPERMIL2PD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF_V4DI_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vpermil2v8sf3,  \"__builtin_ia32_vpermil2ps256\", IX86_BUILTIN_VPERMIL2PS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF_V8SI_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vinsertf128v4df, \"__builtin_ia32_vinsertf128_pd256\", IX86_BUILTIN_VINSERTF128PD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V2DF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vinsertf128v8sf, \"__builtin_ia32_vinsertf128_ps256\", IX86_BUILTIN_VINSERTF128PS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V4SF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vinsertf128v8si, \"__builtin_ia32_vinsertf128_si256\", IX86_BUILTIN_VINSERTF128SI256, UNKNOWN, (int) V8SI_FTYPE_V8SI_V4SI_INT },\n+\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_movshdup256, \"__builtin_ia32_movshdup256\", IX86_BUILTIN_MOVSHDUP256, UNKNOWN, (int) V8SF_FTYPE_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_movsldup256, \"__builtin_ia32_movsldup256\", IX86_BUILTIN_MOVSLDUP256, UNKNOWN, (int) V8SF_FTYPE_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_movddup256, \"__builtin_ia32_movddup256\", IX86_BUILTIN_MOVDDUP256, UNKNOWN, (int) V4DF_FTYPE_V4DF },\n+\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_sqrtv4df2, \"__builtin_ia32_sqrtpd256\", IX86_BUILTIN_SQRTPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_sqrtv8sf2, \"__builtin_ia32_sqrtps256\", IX86_BUILTIN_SQRTPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_sqrtv8sf2, \"__builtin_ia32_sqrtps_nr256\", IX86_BUILTIN_SQRTPS_NR256, UNKNOWN, (int) V8SF_FTYPE_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_rsqrtv8sf2, \"__builtin_ia32_rsqrtps256\", IX86_BUILTIN_RSQRTPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_rsqrtv8sf2, \"__builtin_ia32_rsqrtps_nr256\", IX86_BUILTIN_RSQRTPS_NR256, UNKNOWN, (int) V8SF_FTYPE_V8SF },\n+\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_rcpv8sf2, \"__builtin_ia32_rcpps256\", IX86_BUILTIN_RCPPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF },\n+\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_roundpd256, \"__builtin_ia32_roundpd256\", IX86_BUILTIN_ROUNDPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_INT },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_roundps256, \"__builtin_ia32_roundps256\", IX86_BUILTIN_ROUNDPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_INT },\n+\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_unpckhpd256,  \"__builtin_ia32_unpckhpd256\", IX86_BUILTIN_UNPCKHPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_unpcklpd256,  \"__builtin_ia32_unpcklpd256\", IX86_BUILTIN_UNPCKLPD256, UNKNOWN, (int) V4DF_FTYPE_V4DF_V4DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_unpckhps256,  \"__builtin_ia32_unpckhps256\", IX86_BUILTIN_UNPCKHPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_unpcklps256,  \"__builtin_ia32_unpcklps256\", IX86_BUILTIN_UNPCKLPS256, UNKNOWN, (int) V8SF_FTYPE_V8SF_V8SF },\n+\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_si256_si, \"__builtin_ia32_si256_si\", IX86_BUILTIN_SI256_SI, UNKNOWN, (int) V8SI_FTYPE_V4SI },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_ps256_ps, \"__builtin_ia32_ps256_ps\", IX86_BUILTIN_PS256_PS, UNKNOWN, (int) V8SF_FTYPE_V4SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_pd256_pd, \"__builtin_ia32_pd256_pd\", IX86_BUILTIN_PD256_PD, UNKNOWN, (int) V4DF_FTYPE_V2DF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_si_si256, \"__builtin_ia32_si_si256\", IX86_BUILTIN_SI_SI256, UNKNOWN, (int) V4SI_FTYPE_V8SI },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_ps_ps256, \"__builtin_ia32_ps_ps256\", IX86_BUILTIN_PS_PS256, UNKNOWN, (int) V4SF_FTYPE_V8SF },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_pd_pd256, \"__builtin_ia32_pd_pd256\", IX86_BUILTIN_PD_PD256, UNKNOWN, (int) V2DF_FTYPE_V4DF },\n+\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vtestpd, \"__builtin_ia32_vtestzpd\", IX86_BUILTIN_VTESTZPD, EQ, (int) INT_FTYPE_V2DF_V2DF_PTEST },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vtestpd, \"__builtin_ia32_vtestcpd\", IX86_BUILTIN_VTESTCPD, LTU, (int) INT_FTYPE_V2DF_V2DF_PTEST },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vtestpd, \"__builtin_ia32_vtestnzcpd\", IX86_BUILTIN_VTESTNZCPD, GTU, (int) INT_FTYPE_V2DF_V2DF_PTEST },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vtestps, \"__builtin_ia32_vtestzps\", IX86_BUILTIN_VTESTZPS, EQ, (int) INT_FTYPE_V4SF_V4SF_PTEST },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vtestps, \"__builtin_ia32_vtestcps\", IX86_BUILTIN_VTESTCPS, LTU, (int) INT_FTYPE_V4SF_V4SF_PTEST },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vtestps, \"__builtin_ia32_vtestnzcps\", IX86_BUILTIN_VTESTNZCPS, GTU, (int) INT_FTYPE_V4SF_V4SF_PTEST },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vtestpd256, \"__builtin_ia32_vtestzpd256\", IX86_BUILTIN_VTESTZPD256, EQ, (int) INT_FTYPE_V4DF_V4DF_PTEST },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vtestpd256, \"__builtin_ia32_vtestcpd256\", IX86_BUILTIN_VTESTCPD256, LTU, (int) INT_FTYPE_V4DF_V4DF_PTEST },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vtestpd256, \"__builtin_ia32_vtestnzcpd256\", IX86_BUILTIN_VTESTNZCPD256, GTU, (int) INT_FTYPE_V4DF_V4DF_PTEST },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vtestps256, \"__builtin_ia32_vtestzps256\", IX86_BUILTIN_VTESTZPS256, EQ, (int) INT_FTYPE_V8SF_V8SF_PTEST },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vtestps256, \"__builtin_ia32_vtestcps256\", IX86_BUILTIN_VTESTCPS256, LTU, (int) INT_FTYPE_V8SF_V8SF_PTEST },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_vtestps256, \"__builtin_ia32_vtestnzcps256\", IX86_BUILTIN_VTESTNZCPS256, GTU, (int) INT_FTYPE_V8SF_V8SF_PTEST },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_ptest256, \"__builtin_ia32_ptestz256\", IX86_BUILTIN_PTESTZ256, EQ, (int) INT_FTYPE_V4DI_V4DI_PTEST },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_ptest256, \"__builtin_ia32_ptestc256\", IX86_BUILTIN_PTESTC256, LTU, (int) INT_FTYPE_V4DI_V4DI_PTEST },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_ptest256, \"__builtin_ia32_ptestnzc256\", IX86_BUILTIN_PTESTNZC256, GTU, (int) INT_FTYPE_V4DI_V4DI_PTEST },\n+\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_movmskpd256, \"__builtin_ia32_movmskpd256\", IX86_BUILTIN_MOVMSKPD256, UNKNOWN, (int) INT_FTYPE_V4DF  },\n+  { OPTION_MASK_ISA_AVX, CODE_FOR_avx_movmskps256, \"__builtin_ia32_movmskps256\", IX86_BUILTIN_MOVMSKPS256, UNKNOWN, (int) INT_FTYPE_V8SF },\n };\n \n /* SSE5 */\n@@ -20787,6 +21507,276 @@ ix86_init_mmx_sse_builtins (void)\n \t\t\t\tfloat_type_node,\n \t\t\t\tNULL_TREE);\n \n+  /* AVX builtins  */\n+  tree V32QI_type_node = build_vector_type_for_mode (char_type_node,\n+\t\t\t\t\t\t     V32QImode);\n+  tree V8SI_type_node = build_vector_type_for_mode (intSI_type_node,\n+\t\t\t\t\t\t    V8SImode);\n+  tree V8SF_type_node = build_vector_type_for_mode (float_type_node,\n+\t\t\t\t\t\t    V8SFmode);\n+  tree V4DI_type_node = build_vector_type_for_mode (long_long_integer_type_node,\n+\t\t\t\t\t\t    V4DImode);\n+  tree V4DF_type_node = build_vector_type_for_mode (double_type_node,\n+\t\t\t\t\t\t    V4DFmode);\n+  tree v8sf_ftype_v8sf\n+    = build_function_type_list (V8SF_type_node,\n+\t\t\t\tV8SF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8si_ftype_v8sf\n+    = build_function_type_list (V8SI_type_node,\n+\t\t\t\tV8SF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8sf_ftype_v8si\n+    = build_function_type_list (V8SF_type_node,\n+\t\t\t\tV8SI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4si_ftype_v4df\n+    = build_function_type_list (V4SI_type_node,\n+\t\t\t\tV4DF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4df_ftype_v4df\n+    = build_function_type_list (V4DF_type_node,\n+\t\t\t\tV4DF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4df_ftype_v4si\n+    = build_function_type_list (V4DF_type_node,\n+\t\t\t\tV4SI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4df_ftype_v4sf\n+    = build_function_type_list (V4DF_type_node,\n+\t\t\t\tV4SF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4sf_ftype_v4df\n+    = build_function_type_list (V4SF_type_node,\n+\t\t\t\tV4DF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8sf_ftype_v8sf_v8sf\n+    = build_function_type_list (V8SF_type_node,\n+\t\t\t\tV8SF_type_node, V8SF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4df_ftype_v4df_v4df\n+    = build_function_type_list (V4DF_type_node,\n+\t\t\t\tV4DF_type_node, V4DF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8sf_ftype_v8sf_int\n+    = build_function_type_list (V8SF_type_node,\n+\t\t\t\tV8SF_type_node, integer_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4si_ftype_v8si_int\n+    = build_function_type_list (V4SI_type_node,\n+\t\t\t\tV8SI_type_node, integer_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4df_ftype_v4df_int\n+    = build_function_type_list (V4DF_type_node,\n+\t\t\t\tV4DF_type_node, integer_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4sf_ftype_v8sf_int\n+    = build_function_type_list (V4SF_type_node,\n+\t\t\t\tV8SF_type_node, integer_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v2df_ftype_v4df_int\n+    = build_function_type_list (V2DF_type_node,\n+\t\t\t\tV4DF_type_node, integer_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8sf_ftype_v8sf_v8sf_int\n+    = build_function_type_list (V8SF_type_node,\n+\t\t\t\tV8SF_type_node, V8SF_type_node,\n+\t\t\t\tinteger_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8sf_ftype_v8sf_v8sf_v8sf\n+    = build_function_type_list (V8SF_type_node,\n+\t\t\t\tV8SF_type_node, V8SF_type_node,\n+\t\t\t\tV8SF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4df_ftype_v4df_v4df_v4df\n+    = build_function_type_list (V4DF_type_node,\n+\t\t\t\tV4DF_type_node, V4DF_type_node,\n+\t\t\t\tV4DF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8si_ftype_v8si_v8si_int\n+    = build_function_type_list (V8SI_type_node,\n+\t\t\t\tV8SI_type_node, V8SI_type_node,\n+\t\t\t\tinteger_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4df_ftype_v4df_v4df_int\n+    = build_function_type_list (V4DF_type_node,\n+\t\t\t\tV4DF_type_node, V4DF_type_node,\n+\t\t\t\tinteger_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8sf_ftype_v8sf_v8sf_v8si_int\n+    = build_function_type_list (V8SF_type_node,\n+\t\t\t\tV8SF_type_node, V8SF_type_node,\n+\t\t\t\tV8SI_type_node, integer_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4df_ftype_v4df_v4df_v4di_int\n+    = build_function_type_list (V4DF_type_node,\n+\t\t\t\tV4DF_type_node, V4DF_type_node,\n+\t\t\t\tV4DI_type_node, integer_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4sf_ftype_v4sf_v4sf_v4si_int\n+    = build_function_type_list (V4SF_type_node,\n+\t\t\t\tV4SF_type_node, V4SF_type_node,\n+\t\t\t\tV4SI_type_node, integer_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v2df_ftype_v2df_v2df_v2di_int\n+    = build_function_type_list (V2DF_type_node,\n+\t\t\t\tV2DF_type_node, V2DF_type_node,\n+\t\t\t\tV2DI_type_node, integer_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8sf_ftype_pcfloat\n+    = build_function_type_list (V8SF_type_node,\n+\t\t\t\tpcfloat_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4df_ftype_pcdouble\n+    = build_function_type_list (V4DF_type_node,\n+\t\t\t\tpcdouble_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree pcv4sf_type_node\n+    = build_pointer_type (build_type_variant (V4SF_type_node, 1, 0));\n+  tree pcv2df_type_node\n+    = build_pointer_type (build_type_variant (V2DF_type_node, 1, 0));\n+  tree v8sf_ftype_pcv4sf\n+    = build_function_type_list (V8SF_type_node,\n+\t\t\t\tpcv4sf_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4df_ftype_pcv2df\n+    = build_function_type_list (V4DF_type_node,\n+\t\t\t\tpcv2df_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v32qi_ftype_pcchar\n+    = build_function_type_list (V32QI_type_node,\n+\t\t\t\tpcchar_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree void_ftype_pchar_v32qi\n+    = build_function_type_list (void_type_node,\n+\t\t\t        pchar_type_node, V32QI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8si_ftype_v8si_v4si_int\n+    = build_function_type_list (V8SI_type_node,\n+\t\t\t\tV8SI_type_node, V4SI_type_node,\n+\t\t\t\tinteger_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8sf_ftype_v8sf_v4sf_int\n+    = build_function_type_list (V8SF_type_node,\n+\t\t\t\tV8SF_type_node, V4SF_type_node,\n+\t\t\t\tinteger_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4df_ftype_v4df_v2df_int\n+    = build_function_type_list (V4DF_type_node,\n+\t\t\t\tV4DF_type_node, V2DF_type_node,\n+\t\t\t\tinteger_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree void_ftype_pfloat_v8sf\n+    = build_function_type_list (void_type_node,\n+\t\t\t        pfloat_type_node, V8SF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree void_ftype_pdouble_v4df\n+    = build_function_type_list (void_type_node,\n+\t\t\t        pdouble_type_node, V4DF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree pv8sf_type_node = build_pointer_type (V8SF_type_node);\n+  tree pv4sf_type_node = build_pointer_type (V4SF_type_node);\n+  tree pv4df_type_node = build_pointer_type (V4DF_type_node);\n+  tree pv2df_type_node = build_pointer_type (V2DF_type_node);\n+  tree pcv8sf_type_node\n+    = build_pointer_type (build_type_variant (V8SF_type_node, 1, 0));\n+  tree pcv4df_type_node\n+    = build_pointer_type (build_type_variant (V4DF_type_node, 1, 0));\n+  tree v8sf_ftype_pcv8sf_v8sf\n+    = build_function_type_list (V8SF_type_node,\n+\t\t\t\tpcv8sf_type_node, V8SF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4df_ftype_pcv4df_v4df\n+    = build_function_type_list (V4DF_type_node,\n+\t\t\t\tpcv4df_type_node, V4DF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4sf_ftype_pcv4sf_v4sf\n+    = build_function_type_list (V4SF_type_node,\n+\t\t\t\tpcv4sf_type_node, V4SF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v2df_ftype_pcv2df_v2df\n+    = build_function_type_list (V2DF_type_node,\n+\t\t\t\tpcv2df_type_node, V2DF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree void_ftype_pv8sf_v8sf_v8sf\n+    = build_function_type_list (void_type_node,\n+\t\t\t        pv8sf_type_node, V8SF_type_node,\n+\t\t\t\tV8SF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree void_ftype_pv4df_v4df_v4df\n+    = build_function_type_list (void_type_node,\n+\t\t\t        pv4df_type_node, V4DF_type_node,\n+\t\t\t\tV4DF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree void_ftype_pv4sf_v4sf_v4sf\n+    = build_function_type_list (void_type_node,\n+\t\t\t        pv4sf_type_node, V4SF_type_node,\n+\t\t\t\tV4SF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree void_ftype_pv2df_v2df_v2df\n+    = build_function_type_list (void_type_node,\n+\t\t\t        pv2df_type_node, V2DF_type_node,\n+\t\t\t\tV2DF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4df_ftype_v2df\n+    = build_function_type_list (V4DF_type_node,\n+\t\t\t\tV2DF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8sf_ftype_v4sf\n+    = build_function_type_list (V8SF_type_node,\n+\t\t\t\tV4SF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8si_ftype_v4si\n+    = build_function_type_list (V8SI_type_node,\n+\t\t\t\tV4SI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v2df_ftype_v4df\n+    = build_function_type_list (V2DF_type_node,\n+\t\t\t\tV4DF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4sf_ftype_v8sf\n+    = build_function_type_list (V4SF_type_node,\n+\t\t\t\tV8SF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4si_ftype_v8si\n+    = build_function_type_list (V4SI_type_node,\n+\t\t\t\tV8SI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree int_ftype_v4df\n+    = build_function_type_list (integer_type_node,\n+\t\t\t\tV4DF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree int_ftype_v8sf\n+    = build_function_type_list (integer_type_node,\n+\t\t\t\tV8SF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree int_ftype_v8sf_v8sf\n+    = build_function_type_list (integer_type_node,\n+\t\t\t\tV8SF_type_node, V8SF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree int_ftype_v4di_v4di\n+    = build_function_type_list (integer_type_node,\n+\t\t\t\tV4DI_type_node, V4DI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree int_ftype_v4df_v4df\n+    = build_function_type_list (integer_type_node,\n+\t\t\t\tV4DF_type_node, V4DF_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v8sf_ftype_v8sf_v8si\n+    = build_function_type_list (V8SF_type_node,\n+\t\t\t\tV8SF_type_node, V8SI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4df_ftype_v4df_v4di\n+    = build_function_type_list (V4DF_type_node,\n+\t\t\t\tV4DF_type_node, V4DI_type_node,\n+\t\t\t\tNULL_TREE);\n+  tree v4sf_ftype_v4sf_v4si\n+    = build_function_type_list (V4SF_type_node,\n+\t\t\t\tV4SF_type_node, V4SI_type_node, NULL_TREE);\n+  tree v2df_ftype_v2df_v2di\n+    = build_function_type_list (V2DF_type_node,\n+\t\t\t\tV2DF_type_node, V2DI_type_node, NULL_TREE);\n+\n   tree ftype;\n \n   /* Add all special builtins with variable number of operands.  */\n@@ -20804,9 +21794,24 @@ ix86_init_mmx_sse_builtins (void)\n \tcase VOID_FTYPE_VOID:\n \t  type = void_ftype_void;\n \t  break;\n+\tcase V32QI_FTYPE_PCCHAR:\n+\t  type = v32qi_ftype_pcchar;\n+\t  break;\n \tcase V16QI_FTYPE_PCCHAR:\n \t  type = v16qi_ftype_pcchar;\n \t  break;\n+\tcase V8SF_FTYPE_PCV4SF:\n+\t  type = v8sf_ftype_pcv4sf;\n+\t  break;\n+\tcase V8SF_FTYPE_PCFLOAT:\n+\t  type = v8sf_ftype_pcfloat;\n+\t  break;\n+\tcase V4DF_FTYPE_PCV2DF:\n+\t  type = v4df_ftype_pcv2df;\n+\t  break;\n+\tcase V4DF_FTYPE_PCDOUBLE:\n+\t  type = v4df_ftype_pcdouble;\n+\t  break;\n \tcase V4SF_FTYPE_PCFLOAT:\n \t  type = v4sf_ftype_pcfloat;\n \t  break;\n@@ -20816,24 +21821,45 @@ ix86_init_mmx_sse_builtins (void)\n \tcase V2DF_FTYPE_PCDOUBLE:\n \t  type = v2df_ftype_pcdouble;\n \t  break;\n+\tcase V8SF_FTYPE_PCV8SF_V8SF:\n+\t  type = v8sf_ftype_pcv8sf_v8sf;\n+\t  break;\n+\tcase V4DF_FTYPE_PCV4DF_V4DF:\n+\t  type = v4df_ftype_pcv4df_v4df;\n+\t  break;\n \tcase V4SF_FTYPE_V4SF_PCV2SF:\n \t  type = v4sf_ftype_v4sf_pcv2sf;\n \t  break;\n+\tcase V4SF_FTYPE_PCV4SF_V4SF:\n+\t  type = v4sf_ftype_pcv4sf_v4sf;\n+\t  break;\n \tcase V2DF_FTYPE_V2DF_PCDOUBLE:\n \t  type = v2df_ftype_v2df_pcdouble;\n \t  break;\n+\tcase V2DF_FTYPE_PCV2DF_V2DF:\n+\t  type = v2df_ftype_pcv2df_v2df;\n+\t  break;\n \tcase VOID_FTYPE_PV2SF_V4SF:\n \t  type = void_ftype_pv2sf_v4sf;\n \t  break;\n \tcase VOID_FTYPE_PV2DI_V2DI:\n \t  type = void_ftype_pv2di_v2di;\n \t  break;\n+\tcase VOID_FTYPE_PCHAR_V32QI:\n+\t  type = void_ftype_pchar_v32qi;\n+\t  break;\n \tcase VOID_FTYPE_PCHAR_V16QI:\n \t  type = void_ftype_pchar_v16qi;\n \t  break;\n+\tcase VOID_FTYPE_PFLOAT_V8SF:\n+\t  type = void_ftype_pfloat_v8sf;\n+\t  break;\n \tcase VOID_FTYPE_PFLOAT_V4SF:\n \t  type = void_ftype_pfloat_v4sf;\n \t  break;\n+\tcase VOID_FTYPE_PDOUBLE_V4DF:\n+\t  type = void_ftype_pdouble_v4df;\n+\t  break;\n \tcase VOID_FTYPE_PDOUBLE_V2DF:\n \t  type = void_ftype_pdouble_v2df;\n \t  break;\n@@ -20843,6 +21869,18 @@ ix86_init_mmx_sse_builtins (void)\n \tcase VOID_FTYPE_PINT_INT:\n \t  type = void_ftype_pint_int;\n \t  break;\n+\tcase VOID_FTYPE_PV8SF_V8SF_V8SF:\n+\t  type = void_ftype_pv8sf_v8sf_v8sf;\n+\t  break;\n+\tcase VOID_FTYPE_PV4DF_V4DF_V4DF:\n+\t  type = void_ftype_pv4df_v4df_v4df;\n+\t  break;\n+\tcase VOID_FTYPE_PV4SF_V4SF_V4SF:\n+\t  type = void_ftype_pv4sf_v4sf_v4sf;\n+\t  break;\n+\tcase VOID_FTYPE_PV2DF_V2DF_V2DF:\n+\t  type = void_ftype_pv2df_v2df_v2df;\n+\t  break;\n \tdefault:\n \t  gcc_unreachable ();\n \t}\n@@ -20865,9 +21903,24 @@ ix86_init_mmx_sse_builtins (void)\n \tcase FLOAT_FTYPE_FLOAT:\n \t  type = float_ftype_float;\n \t  break;\n+\tcase INT_FTYPE_V8SF_V8SF_PTEST:\n+\t  type = int_ftype_v8sf_v8sf;\n+\t  break;\n+\tcase INT_FTYPE_V4DI_V4DI_PTEST:\n+\t  type = int_ftype_v4di_v4di;\n+\t  break;\n+\tcase INT_FTYPE_V4DF_V4DF_PTEST:\n+\t  type = int_ftype_v4df_v4df;\n+\t  break;\n+\tcase INT_FTYPE_V4SF_V4SF_PTEST:\n+\t  type = int_ftype_v4sf_v4sf;\n+\t  break;\n \tcase INT_FTYPE_V2DI_V2DI_PTEST:\n \t  type = int_ftype_v2di_v2di;\n \t  break;\n+\tcase INT_FTYPE_V2DF_V2DF_PTEST:\n+\t  type = int_ftype_v2df_v2df;\n+\t  break;\n \tcase INT64_FTYPE_V4SF:\n \t  type = int64_ftype_v4sf;\n \t  break;\n@@ -20880,6 +21933,12 @@ ix86_init_mmx_sse_builtins (void)\n \tcase INT_FTYPE_V8QI:\n \t  type = int_ftype_v8qi;\n \t  break;\n+\tcase INT_FTYPE_V8SF:\n+\t  type = int_ftype_v8sf;\n+\t  break;\n+\tcase INT_FTYPE_V4DF:\n+\t  type = int_ftype_v4df;\n+\t  break;\n \tcase INT_FTYPE_V4SF:\n \t  type = int_ftype_v4sf;\n \t  break;\n@@ -20889,6 +21948,12 @@ ix86_init_mmx_sse_builtins (void)\n \tcase V16QI_FTYPE_V16QI:\n \t  type = v16qi_ftype_v16qi;\n \t  break;\n+\tcase V8SI_FTYPE_V8SF:\n+\t  type = v8si_ftype_v8sf;\n+\t  break;\n+\tcase V8SI_FTYPE_V4SI:\n+\t  type = v8si_ftype_v4si;\n+\t  break;\n \tcase V8HI_FTYPE_V8HI:\n \t  type = v8hi_ftype_v8hi;\n \t  break;\n@@ -20898,12 +21963,27 @@ ix86_init_mmx_sse_builtins (void)\n \tcase V8QI_FTYPE_V8QI:\n \t  type = v8qi_ftype_v8qi;\n \t  break;\n+\tcase V8SF_FTYPE_V8SF:\n+\t  type = v8sf_ftype_v8sf;\n+\t  break;\n+\tcase V8SF_FTYPE_V8SI:\n+\t  type = v8sf_ftype_v8si;\n+\t  break;\n+\tcase V8SF_FTYPE_V4SF:\n+\t  type = v8sf_ftype_v4sf;\n+\t  break;\n+\tcase V4SI_FTYPE_V4DF:\n+\t  type = v4si_ftype_v4df;\n+\t  break;\n \tcase V4SI_FTYPE_V4SI:\n \t  type = v4si_ftype_v4si;\n \t  break;\n \tcase V4SI_FTYPE_V16QI:\n \t  type = v4si_ftype_v16qi;\n \t  break;\n+\tcase V4SI_FTYPE_V8SI:\n+\t  type = v4si_ftype_v8si;\n+\t  break;\n \tcase V4SI_FTYPE_V8HI:\n \t  type = v4si_ftype_v8hi;\n \t  break;\n@@ -20916,13 +21996,31 @@ ix86_init_mmx_sse_builtins (void)\n \tcase V4HI_FTYPE_V4HI:\n \t  type = v4hi_ftype_v4hi;\n \t  break;\n+\tcase V4DF_FTYPE_V4DF:\n+\t  type = v4df_ftype_v4df;\n+\t  break;\n+\tcase V4DF_FTYPE_V4SI:\n+\t  type = v4df_ftype_v4si;\n+\t  break;\n+\tcase V4DF_FTYPE_V4SF:\n+\t  type = v4df_ftype_v4sf;\n+\t  break;\n+\tcase V4DF_FTYPE_V2DF:\n+\t  type = v4df_ftype_v2df;\n+\t  break;\n \tcase V4SF_FTYPE_V4SF:\n \tcase V4SF_FTYPE_V4SF_VEC_MERGE:\n \t  type = v4sf_ftype_v4sf;\n \t  break;\n+\tcase V4SF_FTYPE_V8SF:\n+\t  type = v4sf_ftype_v8sf;\n+\t  break;\n \tcase V4SF_FTYPE_V4SI:\n \t  type = v4sf_ftype_v4si;\n \t  break;\n+\tcase V4SF_FTYPE_V4DF:\n+\t  type = v4sf_ftype_v4df;\n+\t  break;\n \tcase V4SF_FTYPE_V2DF:\n \t  type = v4sf_ftype_v2df;\n \t  break;\n@@ -20950,6 +22048,9 @@ ix86_init_mmx_sse_builtins (void)\n \tcase V2SI_FTYPE_V2SF:\n \t  type = v2si_ftype_v2sf;\n \t  break;\n+\tcase V2DF_FTYPE_V4DF:\n+\t  type = v2df_ftype_v4df;\n+\t  break;\n \tcase V2DF_FTYPE_V4SF:\n \t  type = v2df_ftype_v4sf;\n \t  break;\n@@ -20994,6 +22095,12 @@ ix86_init_mmx_sse_builtins (void)\n \tcase V8HI_FTYPE_V8HI_SI_COUNT:\n \t  type = v8hi_ftype_v8hi_int;\n \t  break;\n+\tcase V8SF_FTYPE_V8SF_V8SF:\n+\t  type = v8sf_ftype_v8sf_v8sf;\n+\t  break;\n+\tcase V8SF_FTYPE_V8SF_V8SI:\n+\t  type = v8sf_ftype_v8sf_v8si;\n+\t  break;\n \tcase V4SI_FTYPE_V4SI_V4SI:\n \tcase V4SI_FTYPE_V4SI_V4SI_COUNT:\n \t  type = v4si_ftype_v4si_v4si;\n@@ -21023,10 +22130,19 @@ ix86_init_mmx_sse_builtins (void)\n \tcase V4HI_FTYPE_V4HI_SI_COUNT:\n \t  type = v4hi_ftype_v4hi_int;\n \t  break;\n+\tcase V4DF_FTYPE_V4DF_V4DF:\n+\t  type = v4df_ftype_v4df_v4df;\n+\t  break;\n+\tcase V4DF_FTYPE_V4DF_V4DI:\n+\t  type = v4df_ftype_v4df_v4di;\n+\t  break;\n \tcase V4SF_FTYPE_V4SF_V4SF:\n \tcase V4SF_FTYPE_V4SF_V4SF_SWAP:\n \t  type = v4sf_ftype_v4sf_v4sf;\n \t  break;\n+\tcase V4SF_FTYPE_V4SF_V4SI:\n+\t  type = v4sf_ftype_v4sf_v4si;\n+\t  break;\n \tcase V4SF_FTYPE_V4SF_V2SI:\n \t  type = v4sf_ftype_v4sf_v2si;\n \t  break;\n@@ -21078,6 +22194,9 @@ ix86_init_mmx_sse_builtins (void)\n \tcase V2DF_FTYPE_V2DF_V4SF:\n \t  type = v2df_ftype_v2df_v4sf;\n \t  break;\n+\tcase V2DF_FTYPE_V2DF_V2DI:\n+\t  type = v2df_ftype_v2df_v2di;\n+\t  break;\n \tcase V2DF_FTYPE_V2DF_DI:\n \t  type = v2df_ftype_v2df_int64;\n \t  break;\n@@ -21115,25 +22234,46 @@ ix86_init_mmx_sse_builtins (void)\n \tcase V8HI_FTYPE_V8HI_INT:\n \t  type = v8hi_ftype_v8hi_int;\n \t  break;\n+\tcase V8SF_FTYPE_V8SF_INT:\n+\t  type = v8sf_ftype_v8sf_int;\n+\t  break;\n \tcase V4SI_FTYPE_V4SI_INT:\n \t  type = v4si_ftype_v4si_int;\n \t  break;\n+\tcase V4SI_FTYPE_V8SI_INT:\n+\t  type = v4si_ftype_v8si_int;\n+\t  break;\n \tcase V4HI_FTYPE_V4HI_INT:\n \t  type = v4hi_ftype_v4hi_int;\n \t  break;\n+\tcase V4DF_FTYPE_V4DF_INT:\n+\t  type = v4df_ftype_v4df_int;\n+\t  break;\n \tcase V4SF_FTYPE_V4SF_INT:\n \t  type = v4sf_ftype_v4sf_int;\n \t  break;\n+\tcase V4SF_FTYPE_V8SF_INT:\n+\t  type = v4sf_ftype_v8sf_int;\n+\t  break;\n \tcase V2DI_FTYPE_V2DI_INT:\n \tcase V2DI2TI_FTYPE_V2DI_INT:\n \t  type = v2di_ftype_v2di_int;\n \t  break;\n \tcase V2DF_FTYPE_V2DF_INT:\n \t  type = v2df_ftype_v2df_int;\n \t  break;\n+\tcase V2DF_FTYPE_V4DF_INT:\n+\t  type = v2df_ftype_v4df_int;\n+\t  break;\n \tcase V16QI_FTYPE_V16QI_V16QI_V16QI:\n \t  type = v16qi_ftype_v16qi_v16qi_v16qi;\n \t  break;\n+\tcase V8SF_FTYPE_V8SF_V8SF_V8SF:\n+\t  type = v8sf_ftype_v8sf_v8sf_v8sf;\n+\t  break;\n+\tcase V4DF_FTYPE_V4DF_V4DF_V4DF:\n+\t  type = v4df_ftype_v4df_v4df_v4df;\n+\t  break;\n \tcase V4SF_FTYPE_V4SF_V4SF_V4SF:\n \t  type = v4sf_ftype_v4sf_v4sf_v4sf;\n \t  break;\n@@ -21143,12 +22283,30 @@ ix86_init_mmx_sse_builtins (void)\n \tcase V16QI_FTYPE_V16QI_V16QI_INT:\n \t  type = v16qi_ftype_v16qi_v16qi_int;\n \t  break;\n+\tcase V8SI_FTYPE_V8SI_V8SI_INT:\n+\t  type = v8si_ftype_v8si_v8si_int;\n+\t  break;\n+\tcase V8SI_FTYPE_V8SI_V4SI_INT:\n+\t  type = v8si_ftype_v8si_v4si_int;\n+\t  break;\n \tcase V8HI_FTYPE_V8HI_V8HI_INT:\n \t  type = v8hi_ftype_v8hi_v8hi_int;\n \t  break;\n+\tcase V8SF_FTYPE_V8SF_V8SF_INT:\n+\t  type = v8sf_ftype_v8sf_v8sf_int;\n+\t  break;\n+\tcase V8SF_FTYPE_V8SF_V4SF_INT:\n+\t  type = v8sf_ftype_v8sf_v4sf_int;\n+\t  break;\n \tcase V4SI_FTYPE_V4SI_V4SI_INT:\n \t  type = v4si_ftype_v4si_v4si_int;\n \t  break;\n+\tcase V4DF_FTYPE_V4DF_V4DF_INT:\n+\t  type = v4df_ftype_v4df_v4df_int;\n+\t  break;\n+\tcase V4DF_FTYPE_V4DF_V2DF_INT:\n+\t  type = v4df_ftype_v4df_v2df_int;\n+\t  break;\n \tcase V4SF_FTYPE_V4SF_V4SF_INT:\n \t  type = v4sf_ftype_v4sf_v4sf_int;\n \t  break;\n@@ -21168,6 +22326,18 @@ ix86_init_mmx_sse_builtins (void)\n \tcase V1DI2DI_FTYPE_V1DI_V1DI_INT:\n \t  type = v1di_ftype_v1di_v1di_int;\n \t  break;\n+\tcase V8SF_FTYPE_V8SF_V8SF_V8SI_INT:\n+\t  type = v8sf_ftype_v8sf_v8sf_v8si_int;\n+\t  break;\n+\tcase V4DF_FTYPE_V4DF_V4DF_V4DI_INT:\n+\t  type = v4df_ftype_v4df_v4df_v4di_int;\n+\t  break;\n+\tcase V4SF_FTYPE_V4SF_V4SF_V4SI_INT:\n+\t  type = v4sf_ftype_v4sf_v4sf_v4si_int;\n+\t  break;\n+\tcase V2DF_FTYPE_V2DF_V2DF_V2DI_INT:\n+\t  type = v2df_ftype_v2df_v2df_v2di_int;\n+\t  break;\n \tdefault:\n \t  gcc_unreachable ();\n \t}\n@@ -21234,6 +22404,10 @@ ix86_init_mmx_sse_builtins (void)\n   /* PCLMUL */\n   def_builtin_const (OPTION_MASK_ISA_PCLMUL, \"__builtin_ia32_pclmulqdq128\", v2di_ftype_v2di_v2di_int, IX86_BUILTIN_PCLMULQDQ128);\n \n+  /* AVX */\n+  def_builtin (OPTION_MASK_ISA_AVX, \"__builtin_ia32_vzeroupper\", void_ftype_void,\n+\t       TARGET_64BIT ? IX86_BUILTIN_VZEROUPPER_REX64 : IX86_BUILTIN_VZEROUPPER);\n+\n   /* Access to the vec_init patterns.  */\n   ftype = build_function_type_list (V2SI_type_node, integer_type_node,\n \t\t\t\t    integer_type_node, NULL_TREE);\n@@ -22134,35 +23308,56 @@ ix86_expand_args_builtin (const struct builtin_description *d,\n \n   switch ((enum ix86_builtin_type) d->flag)\n     {\n+    case INT_FTYPE_V8SF_V8SF_PTEST:\n+    case INT_FTYPE_V4DI_V4DI_PTEST:\n+    case INT_FTYPE_V4DF_V4DF_PTEST:\n+    case INT_FTYPE_V4SF_V4SF_PTEST:\n     case INT_FTYPE_V2DI_V2DI_PTEST:\n+    case INT_FTYPE_V2DF_V2DF_PTEST:\n       return ix86_expand_sse_ptest (d, exp, target);\n     case FLOAT128_FTYPE_FLOAT128:\n     case FLOAT_FTYPE_FLOAT:\n     case INT64_FTYPE_V4SF:\n     case INT64_FTYPE_V2DF:\n     case INT_FTYPE_V16QI:\n     case INT_FTYPE_V8QI:\n+    case INT_FTYPE_V8SF:\n+    case INT_FTYPE_V4DF:\n     case INT_FTYPE_V4SF:\n     case INT_FTYPE_V2DF:\n     case V16QI_FTYPE_V16QI:\n+    case V8SI_FTYPE_V8SF:\n+    case V8SI_FTYPE_V4SI:\n     case V8HI_FTYPE_V8HI:\n     case V8HI_FTYPE_V16QI:\n     case V8QI_FTYPE_V8QI:\n+    case V8SF_FTYPE_V8SF:\n+    case V8SF_FTYPE_V8SI:\n+    case V8SF_FTYPE_V4SF:\n     case V4SI_FTYPE_V4SI:\n     case V4SI_FTYPE_V16QI:\n     case V4SI_FTYPE_V4SF:\n+    case V4SI_FTYPE_V8SI:\n     case V4SI_FTYPE_V8HI:\n+    case V4SI_FTYPE_V4DF:\n     case V4SI_FTYPE_V2DF:\n     case V4HI_FTYPE_V4HI:\n+    case V4DF_FTYPE_V4DF:\n+    case V4DF_FTYPE_V4SI:\n+    case V4DF_FTYPE_V4SF:\n+    case V4DF_FTYPE_V2DF:\n     case V4SF_FTYPE_V4SF:\n     case V4SF_FTYPE_V4SI:\n+    case V4SF_FTYPE_V8SF:\n+    case V4SF_FTYPE_V4DF:\n     case V4SF_FTYPE_V2DF:\n     case V2DI_FTYPE_V2DI:\n     case V2DI_FTYPE_V16QI:\n     case V2DI_FTYPE_V8HI:\n     case V2DI_FTYPE_V4SI:\n     case V2DF_FTYPE_V2DF:\n     case V2DF_FTYPE_V4SI:\n+    case V2DF_FTYPE_V4DF:\n     case V2DF_FTYPE_V4SF:\n     case V2DF_FTYPE_V2SI:\n     case V2SI_FTYPE_V2SI:\n@@ -22184,14 +23379,19 @@ ix86_expand_args_builtin (const struct builtin_description *d,\n     case V8HI_FTYPE_V8HI_V8HI:\n     case V8HI_FTYPE_V16QI_V16QI:\n     case V8HI_FTYPE_V4SI_V4SI:\n+    case V8SF_FTYPE_V8SF_V8SF:\n+    case V8SF_FTYPE_V8SF_V8SI:\n     case V4SI_FTYPE_V4SI_V4SI:\n     case V4SI_FTYPE_V8HI_V8HI:\n     case V4SI_FTYPE_V4SF_V4SF:\n     case V4SI_FTYPE_V2DF_V2DF:\n     case V4HI_FTYPE_V4HI_V4HI:\n     case V4HI_FTYPE_V8QI_V8QI:\n     case V4HI_FTYPE_V2SI_V2SI:\n+    case V4DF_FTYPE_V4DF_V4DF:\n+    case V4DF_FTYPE_V4DF_V4DI:\n     case V4SF_FTYPE_V4SF_V4SF:\n+    case V4SF_FTYPE_V4SF_V4SI:\n     case V4SF_FTYPE_V4SF_V2SI:\n     case V4SF_FTYPE_V4SF_V2DF:\n     case V4SF_FTYPE_V4SF_DI:\n@@ -22206,6 +23406,7 @@ ix86_expand_args_builtin (const struct builtin_description *d,\n     case V2SI_FTYPE_V2SF_V2SF:\n     case V2DF_FTYPE_V2DF_V2DF:\n     case V2DF_FTYPE_V2DF_V4SF:\n+    case V2DF_FTYPE_V2DF_V2DI:\n     case V2DF_FTYPE_V2DF_DI:\n     case V2DF_FTYPE_V2DF_SI:\n     case V2SF_FTYPE_V2SF_V2SF:\n@@ -22249,22 +23450,35 @@ ix86_expand_args_builtin (const struct builtin_description *d,\n       nargs_constant = 1;\n       break;\n     case V8HI_FTYPE_V8HI_INT:\n+    case V8SF_FTYPE_V8SF_INT:\n     case V4SI_FTYPE_V4SI_INT:\n+    case V4SI_FTYPE_V8SI_INT:\n     case V4HI_FTYPE_V4HI_INT:\n+    case V4DF_FTYPE_V4DF_INT:\n     case V4SF_FTYPE_V4SF_INT:\n+    case V4SF_FTYPE_V8SF_INT:\n     case V2DI_FTYPE_V2DI_INT:\n     case V2DF_FTYPE_V2DF_INT:\n+    case V2DF_FTYPE_V4DF_INT:\n       nargs = 2;\n       nargs_constant = 1;\n       break;\n     case V16QI_FTYPE_V16QI_V16QI_V16QI:\n+    case V8SF_FTYPE_V8SF_V8SF_V8SF:\n+    case V4DF_FTYPE_V4DF_V4DF_V4DF:\n     case V4SF_FTYPE_V4SF_V4SF_V4SF:\n     case V2DF_FTYPE_V2DF_V2DF_V2DF:\n       nargs = 3;\n       break;\n     case V16QI_FTYPE_V16QI_V16QI_INT:\n     case V8HI_FTYPE_V8HI_V8HI_INT:\n+    case V8SI_FTYPE_V8SI_V8SI_INT:\n+    case V8SI_FTYPE_V8SI_V4SI_INT:\n+    case V8SF_FTYPE_V8SF_V8SF_INT: \n+    case V8SF_FTYPE_V8SF_V4SF_INT: \n     case V4SI_FTYPE_V4SI_V4SI_INT:\n+    case V4DF_FTYPE_V4DF_V4DF_INT:\n+    case V4DF_FTYPE_V4DF_V2DF_INT:\n     case V4SF_FTYPE_V4SF_V4SF_INT:\n     case V2DI_FTYPE_V2DI_V2DI_INT:\n     case V2DF_FTYPE_V2DF_V2DF_INT:\n@@ -22285,6 +23499,13 @@ ix86_expand_args_builtin (const struct builtin_description *d,\n       nargs = 3;\n       nargs_constant = 2;\n       break;\n+    case V8SF_FTYPE_V8SF_V8SF_V8SI_INT:\n+    case V4DF_FTYPE_V4DF_V4DF_V4DI_INT:\n+    case V4SF_FTYPE_V4SF_V4SF_V4SI_INT:\n+    case V2DF_FTYPE_V2DF_V2DF_V2DI_INT:\n+      nargs = 4;\n+      nargs_constant = 1;\n+      break;\n     case V2DI_FTYPE_V2DI_V2DI_UINT_UINT:\n       nargs = 4;\n       nargs_constant = 2;\n@@ -22345,13 +23566,40 @@ ix86_expand_args_builtin (const struct builtin_description *d,\n \t      case CODE_FOR_sse4_1_roundsd:\n \t      case CODE_FOR_sse4_1_roundss:\n \t      case CODE_FOR_sse4_1_blendps:\n+\t      case CODE_FOR_avx_blendpd256:\n+\t      case CODE_FOR_avx_vpermilv4df:\n+\t      case CODE_FOR_avx_roundpd256:\n+\t      case CODE_FOR_avx_roundps256:\n \t\terror (\"the last argument must be a 4-bit immediate\");\n \t\treturn const0_rtx;\n \n \t      case CODE_FOR_sse4_1_blendpd:\n+\t      case CODE_FOR_avx_vpermilv2df:\n+\t      case CODE_FOR_avx_vpermil2v2df3:\n+\t      case CODE_FOR_avx_vpermil2v4sf3:\n+\t      case CODE_FOR_avx_vpermil2v4df3:\n+\t      case CODE_FOR_avx_vpermil2v8sf3:\n \t\terror (\"the last argument must be a 2-bit immediate\");\n \t\treturn const0_rtx;\n \n+\t      case CODE_FOR_avx_vextractf128v4df:\n+\t      case CODE_FOR_avx_vextractf128v8sf:\n+\t      case CODE_FOR_avx_vextractf128v8si:\n+\t      case CODE_FOR_avx_vinsertf128v4df:\n+\t      case CODE_FOR_avx_vinsertf128v8sf:\n+\t      case CODE_FOR_avx_vinsertf128v8si:\n+\t\terror (\"the last argument must be a 1-bit immediate\");\n+\t\treturn const0_rtx;\n+\n+\t      case CODE_FOR_avx_cmpsdv2df3:\n+\t      case CODE_FOR_avx_cmpssv4sf3:\n+\t      case CODE_FOR_avx_cmppdv2df3:\n+\t      case CODE_FOR_avx_cmppsv4sf3:\n+\t      case CODE_FOR_avx_cmppdv4df3:\n+\t      case CODE_FOR_avx_cmppsv8sf3:\n+\t\terror (\"the last argument must be a 5-bit immediate\");\n+\t\treturn const0_rtx;\n+\n \t     default:\n \t\tswitch (nargs_constant)\n \t\t  {\n@@ -22450,17 +23698,25 @@ ix86_expand_special_args_builtin (const struct builtin_description *d,\n       emit_insn (GEN_FCN (icode) (target));\n       return 0;\n     case V2DI_FTYPE_PV2DI:\n+    case V32QI_FTYPE_PCCHAR:\n     case V16QI_FTYPE_PCCHAR:\n+    case V8SF_FTYPE_PCV4SF:\n+    case V8SF_FTYPE_PCFLOAT:\n     case V4SF_FTYPE_PCFLOAT:\n+    case V4DF_FTYPE_PCV2DF:\n+    case V4DF_FTYPE_PCDOUBLE:\n     case V2DF_FTYPE_PCDOUBLE:\n       nargs = 1;\n       klass = load;\n       memory = 0;\n       break;\n     case VOID_FTYPE_PV2SF_V4SF:\n     case VOID_FTYPE_PV2DI_V2DI:\n+    case VOID_FTYPE_PCHAR_V32QI:\n     case VOID_FTYPE_PCHAR_V16QI:\n+    case VOID_FTYPE_PFLOAT_V8SF:\n     case VOID_FTYPE_PFLOAT_V4SF:\n+    case VOID_FTYPE_PDOUBLE_V4DF:\n     case VOID_FTYPE_PDOUBLE_V2DF:\n     case VOID_FTYPE_PDI_DI:\n     case VOID_FTYPE_PINT_INT:\n@@ -22475,6 +23731,23 @@ ix86_expand_special_args_builtin (const struct builtin_description *d,\n       klass = load;\n       memory = 1;\n       break;\n+    case V8SF_FTYPE_PCV8SF_V8SF:\n+    case V4DF_FTYPE_PCV4DF_V4DF:\n+    case V4SF_FTYPE_PCV4SF_V4SF:\n+    case V2DF_FTYPE_PCV2DF_V2DF:\n+      nargs = 2;\n+      klass = load;\n+      memory = 0;\n+      break;\n+    case VOID_FTYPE_PV8SF_V8SF_V8SF:\n+    case VOID_FTYPE_PV4DF_V4DF_V4DF:\n+    case VOID_FTYPE_PV4SF_V4SF_V4SF:\n+    case VOID_FTYPE_PV2DF_V2DF_V2DF:\n+      nargs = 2;\n+      klass = store;\n+      /* Reserve memory operand for target.  */\n+      memory = ARRAY_SIZE (args);\n+      break;\n     default:\n       gcc_unreachable ();\n     }\n@@ -23761,8 +25034,11 @@ ix86_hard_regno_mode_ok (int regno, enum machine_mode mode)\n     {\n       /* We implement the move patterns for all vector modes into and\n \t out of SSE registers, even when no operation instructions\n-\t are available.  */\n-      return (VALID_SSE_REG_MODE (mode)\n+\t are available.  OImode move is available only when AVX is\n+\t enabled.  */\n+      return ((TARGET_AVX && mode == OImode)\n+\t      || VALID_AVX256_REG_MODE (mode)\n+\t      || VALID_SSE_REG_MODE (mode)\n \t      || VALID_SSE2_REG_MODE (mode)\n \t      || VALID_MMX_REG_MODE (mode)\n \t      || VALID_MMX_REG_MODE_3DNOW (mode));\n@@ -24911,7 +26187,8 @@ extended_reg_mentioned_1 (rtx *p, void *data ATTRIBUTE_UNUSED)\n bool\n x86_extended_reg_mentioned_p (rtx insn)\n {\n-  return for_each_rtx (&PATTERN (insn), extended_reg_mentioned_1, NULL);\n+  return for_each_rtx (INSN_P (insn) ? &PATTERN (insn) : &insn,\n+\t\t       extended_reg_mentioned_1, NULL);\n }\n \n /* Generate an unsigned DImode/SImode to FP conversion.  This is the same code\n@@ -24962,7 +26239,7 @@ static bool\n ix86_expand_vector_init_duplicate (bool mmx_ok, enum machine_mode mode,\n \t\t\t\t   rtx target, rtx val)\n {\n-  enum machine_mode smode, wsmode, wvmode;\n+  enum machine_mode hmode, smode, wsmode, wvmode;\n   rtx x;\n \n   switch (mode)\n@@ -25087,6 +26364,33 @@ ix86_expand_vector_init_duplicate (bool mmx_ok, enum machine_mode mode,\n       emit_move_insn (target, gen_lowpart (mode, x));\n       return true;\n \n+    case V4DFmode:\n+      hmode = V2DFmode;\n+      goto half;\n+    case V4DImode:\n+      hmode = V2DImode;\n+      goto half;\n+    case V8SFmode:\n+      hmode = V4SFmode;\n+      goto half;\n+    case V8SImode:\n+      hmode = V4SImode;\n+      goto half;\n+    case V16HImode:\n+      hmode = V8HImode;\n+      goto half;\n+    case V32QImode:\n+      hmode = V16QImode;\n+      goto half;\n+half:\n+      {\n+\trtx tmp = gen_reg_rtx (hmode);\n+\tix86_expand_vector_init_duplicate (mmx_ok, hmode, tmp, val);\n+\temit_insn (gen_rtx_SET (VOIDmode, target,\n+\t\t\t\tgen_rtx_VEC_CONCAT (mode, tmp, tmp)));\n+      }\n+      return true;\n+\n     default:\n       return false;\n     }\n@@ -25127,6 +26431,14 @@ ix86_expand_vector_init_one_nonzero (bool mmx_ok, enum machine_mode mode,\n     case V4HImode:\n       use_vector_set = TARGET_SSE || TARGET_3DNOW_A;\n       break;\n+    case V32QImode:\n+    case V16HImode:\n+    case V8SImode:\n+    case V8SFmode:\n+    case V4DImode:\n+    case V4DFmode:\n+      use_vector_set = TARGET_AVX;\n+      break;\n     default:\n       break;\n     }\n@@ -25265,6 +26577,12 @@ ix86_expand_vector_init_one_var (bool mmx_ok, enum machine_mode mode,\n \t the general case.  */\n       return false;\n \n+    case V4DFmode:\n+    case V4DImode:\n+    case V8SFmode:\n+    case V8SImode:\n+    case V16HImode:\n+    case V32QImode:\n     case V4SFmode:\n     case V4SImode:\n     case V8HImode:\n@@ -25325,7 +26643,7 @@ ix86_expand_vector_init_concat (enum machine_mode mode,\n \t\t\t\trtx target, rtx *ops, int n)\n {\n   enum machine_mode cmode, hmode = VOIDmode;\n-  rtx first[4], second[2];\n+  rtx first[8], second[4];\n   rtvec v;\n   int i, j;\n \n@@ -25334,6 +26652,18 @@ ix86_expand_vector_init_concat (enum machine_mode mode,\n     case 2:\n       switch (mode)\n \t{\n+\tcase V8SImode:\n+\t  cmode = V4SImode;\n+\t  break;\n+\tcase V8SFmode:\n+\t  cmode = V4SFmode;\n+\t  break;\n+\tcase V4DImode:\n+\t  cmode = V2DImode;\n+\t  break;\n+\tcase V4DFmode:\n+\t  cmode = V2DFmode;\n+\t  break;\n \tcase V4SImode:\n \t  cmode = V2SImode;\n \t  break;\n@@ -25368,6 +26698,12 @@ ix86_expand_vector_init_concat (enum machine_mode mode,\n     case 4:\n       switch (mode)\n \t{\n+\tcase V4DImode:\n+\t  cmode = V2DImode;\n+\t  break;\n+\tcase V4DFmode:\n+\t  cmode = V2DFmode;\n+\t  break;\n \tcase V4SImode:\n \t  cmode = V2SImode;\n \t  break;\n@@ -25379,6 +26715,22 @@ ix86_expand_vector_init_concat (enum machine_mode mode,\n \t}\n       goto half;\n \n+    case 8:\n+      switch (mode)\n+\t{\n+\tcase V8SImode:\n+\t  cmode = V2SImode;\n+\t  hmode = V4SImode;\n+\t  break;\n+\tcase V8SFmode:\n+\t  cmode = V2SFmode;\n+\t  hmode = V4SFmode;\n+\t  break;\n+\tdefault:\n+\t  gcc_unreachable ();\n+\t}\n+      goto half;\n+\n half:\n       /* FIXME: We process inputs backward to help RA.  PR 36222.  */\n       i = n - 1;\n@@ -25531,7 +26883,8 @@ static void\n ix86_expand_vector_init_general (bool mmx_ok, enum machine_mode mode,\n \t\t\t\t rtx target, rtx vals)\n {\n-  rtx ops[16];\n+  rtx ops[32], op0, op1;\n+  enum machine_mode half_mode = VOIDmode;\n   int n, i;\n \n   switch (mode)\n@@ -25542,6 +26895,10 @@ ix86_expand_vector_init_general (bool mmx_ok, enum machine_mode mode,\n \tbreak;\n       /* FALLTHRU */\n \n+    case V8SFmode:\n+    case V8SImode:\n+    case V4DFmode:\n+    case V4DImode:\n     case V4SFmode:\n     case V4SImode:\n     case V2DFmode:\n@@ -25552,6 +26909,28 @@ ix86_expand_vector_init_general (bool mmx_ok, enum machine_mode mode,\n       ix86_expand_vector_init_concat (mode, target, ops, n);\n       return;\n \n+    case V32QImode:\n+      half_mode = V16QImode;\n+      goto half;\n+\n+    case V16HImode:\n+      half_mode = V8HImode;\n+      goto half;\n+\n+half:\n+      n = GET_MODE_NUNITS (mode);\n+      for (i = 0; i < n; i++)\n+\tops[i] = XVECEXP (vals, 0, i);\n+      op0 = gen_reg_rtx (half_mode);\n+      op1 = gen_reg_rtx (half_mode);\n+      ix86_expand_vector_init_interleave (half_mode, op0, ops,\n+\t\t\t\t\t  n >> 2);\n+      ix86_expand_vector_init_interleave (half_mode, op1,\n+\t\t\t\t\t  &ops [n >> 1], n >> 2);\n+      emit_insn (gen_rtx_SET (VOIDmode, target,\n+\t\t\t      gen_rtx_VEC_CONCAT (mode, op0, op1)));\n+      return;\n+\n     case V16QImode:\n       if (!TARGET_SSE4_1)\n \tbreak;\n@@ -25694,8 +27073,28 @@ ix86_expand_vector_set (bool mmx_ok, rtx target, rtx val, int elt)\n {\n   enum machine_mode mode = GET_MODE (target);\n   enum machine_mode inner_mode = GET_MODE_INNER (mode);\n+  enum machine_mode half_mode;\n   bool use_vec_merge = false;\n   rtx tmp;\n+  static rtx (*gen_extract[6][2]) (rtx, rtx)\n+    = {\n+\t{ gen_vec_extract_lo_v32qi, gen_vec_extract_hi_v32qi },\n+\t{ gen_vec_extract_lo_v16hi, gen_vec_extract_hi_v16hi },\n+\t{ gen_vec_extract_lo_v8si, gen_vec_extract_hi_v8si },\n+\t{ gen_vec_extract_lo_v4di, gen_vec_extract_hi_v4di },\n+\t{ gen_vec_extract_lo_v8sf, gen_vec_extract_hi_v8sf },\n+\t{ gen_vec_extract_lo_v4df, gen_vec_extract_hi_v4df }\n+      };\n+  static rtx (*gen_insert[6][2]) (rtx, rtx, rtx)\n+    = {\n+\t{ gen_vec_set_lo_v32qi, gen_vec_set_hi_v32qi },\n+\t{ gen_vec_set_lo_v16hi, gen_vec_set_hi_v16hi },\n+\t{ gen_vec_set_lo_v8si, gen_vec_set_hi_v8si },\n+\t{ gen_vec_set_lo_v4di, gen_vec_set_hi_v4di },\n+\t{ gen_vec_set_lo_v8sf, gen_vec_set_hi_v8sf },\n+\t{ gen_vec_set_lo_v4df, gen_vec_set_hi_v4df }\n+      };\n+  int i, j, n;\n \n   switch (mode)\n     {\n@@ -25843,6 +27242,62 @@ ix86_expand_vector_set (bool mmx_ok, rtx target, rtx val, int elt)\n       break;\n \n     case V8QImode:\n+      break;\n+\n+    case V32QImode:\n+      half_mode = V16QImode;\n+      j = 0;\n+      n = 16;\n+      goto half;\n+\n+    case V16HImode:\n+      half_mode = V8HImode;\n+      j = 1;\n+      n = 8;\n+      goto half;\n+\n+    case V8SImode:\n+      half_mode = V4SImode;\n+      j = 2;\n+      n = 4;\n+      goto half;\n+\n+    case V4DImode:\n+      half_mode = V2DImode;\n+      j = 3;\n+      n = 2;\n+      goto half;\n+\n+    case V8SFmode:\n+      half_mode = V4SFmode;\n+      j = 4;\n+      n = 4;\n+      goto half;\n+\n+    case V4DFmode:\n+      half_mode = V2DFmode;\n+      j = 5;\n+      n = 2;\n+      goto half;\n+\n+half:\n+      /* Compute offset.  */\n+      i = elt / n;\n+      elt %= n;\n+\n+      gcc_assert (i <= 1);\n+\n+      /* Extract the half.  */\n+      tmp = gen_reg_rtx (half_mode);\n+      emit_insn ((*gen_extract[j][i]) (tmp, target));\n+\n+      /* Put val in tmp at elt.  */\n+      ix86_expand_vector_set (false, tmp, val, elt);\n+\n+      /* Put it back.  */\n+      emit_insn ((*gen_insert[j][i]) (target, target, tmp));\n+      return;\n+\n     default:\n       break;\n     }\n@@ -26044,6 +27499,8 @@ ix86_vector_mode_supported_p (enum machine_mode mode)\n     return true;\n   if (TARGET_SSE2 && VALID_SSE2_REG_MODE (mode))\n     return true;\n+  if (TARGET_AVX && VALID_AVX256_REG_MODE (mode))\n+    return true;\n   if (TARGET_MMX && VALID_MMX_REG_MODE (mode))\n     return true;\n   if (TARGET_3DNOW && VALID_MMX_REG_MODE_3DNOW (mode))"}, {"sha": "7ad706942423f2880527e0e042d644a60f869bf5", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 55, "deletions": 4, "changes": 59, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=95879c728b9a59ae67db022ad370eb66374090f3", "patch": "@@ -46,6 +46,8 @@ along with GCC; see the file COPYING3.  If not see\n #define TARGET_SSSE3\tOPTION_ISA_SSSE3\n #define TARGET_SSE4_1\tOPTION_ISA_SSE4_1\n #define TARGET_SSE4_2\tOPTION_ISA_SSE4_2\n+#define TARGET_AVX\tOPTION_ISA_AVX\n+#define TARGET_FMA\tOPTION_ISA_FMA\n #define TARGET_SSE4A\tOPTION_ISA_SSE4A\n #define TARGET_SSE5\tOPTION_ISA_SSE5\n #define TARGET_ROUND\tOPTION_ISA_ROUND\n@@ -702,7 +704,7 @@ enum target_cpu_default\n    Pentium+ prefers DFmode values to be aligned to 64 bit boundary\n    and Pentium Pro XFmode values at 128 bit boundaries.  */\n \n-#define BIGGEST_ALIGNMENT 128\n+#define BIGGEST_ALIGNMENT (TARGET_AVX ? 256: 128)\n \n /* Maximum stack alignment.  */\n #define MAX_STACK_ALIGNMENT MAX_OFILE_ALIGNMENT\n@@ -996,6 +998,10 @@ do {\t\t\t\t\t\t\t\t\t\\\n \n #define HARD_REGNO_NREGS_WITH_PADDING(REGNO, MODE) ((MODE) == XFmode ? 4 : 8)\n \n+#define VALID_AVX256_REG_MODE(MODE)\t\t\t\t\t\\\n+  ((MODE) == V32QImode || (MODE) == V16HImode || (MODE) == V8SImode\t\\\n+   || (MODE) == V4DImode || (MODE) == V8SFmode || (MODE) == V4DFmode)\n+\n #define VALID_SSE2_REG_MODE(MODE)\t\t\t\t\t\\\n   ((MODE) == V16QImode || (MODE) == V8HImode || (MODE) == V2DFmode\t\\\n    || (MODE) == V2DImode || (MODE) == DFmode)\n@@ -1013,8 +1019,14 @@ do {\t\t\t\t\t\t\t\t\t\\\n    || (MODE) == V4HImode || (MODE) == V8QImode)\n \n /* ??? No autovectorization into MMX or 3DNOW until we can reliably\n-   place emms and femms instructions.  */\n-#define UNITS_PER_SIMD_WORD(MODE) (TARGET_SSE ? 16 : UNITS_PER_WORD)\n+   place emms and femms instructions.\n+   FIXME: AVX has 32byte floating point vector operations and 16byte\n+   integer vector operations.  But vectorizer doesn't support\n+   different sizes for integer and floating point vectors.  We limit\n+   vector size to 16byte.  */\n+#define UNITS_PER_SIMD_WORD(MODE)\t\t\t\t\t\\\n+  (TARGET_AVX ? (((MODE) == DFmode || (MODE) == SFmode) ? 16 : 16)\t\\\n+   \t      : (TARGET_SSE ? 16 : UNITS_PER_WORD))\n \n #define VALID_DFP_MODE_P(MODE) \\\n   ((MODE) == SDmode || (MODE) == DDmode || (MODE) == TDmode)\n@@ -1035,7 +1047,9 @@ do {\t\t\t\t\t\t\t\t\t\\\n #define SSE_REG_MODE_P(MODE)\t\t\t\t\t\t\\\n   ((MODE) == TImode || (MODE) == V16QImode || (MODE) == TFmode\t\t\\\n    || (MODE) == V8HImode || (MODE) == V2DFmode || (MODE) == V2DImode\t\\\n-   || (MODE) == V4SFmode || (MODE) == V4SImode)\n+   || (MODE) == V4SFmode || (MODE) == V4SImode || (MODE) == V32QImode\t\\\n+   || (MODE) == V16HImode || (MODE) == V8SImode || (MODE) == V4DImode\t\\\n+   || (MODE) == V8SFmode || (MODE) == V4DFmode)\n \n /* Value is 1 if hard register REGNO can hold a value of machine-mode MODE.  */\n \n@@ -1339,6 +1353,19 @@ enum reg_class\n #define SSE_VEC_FLOAT_MODE_P(MODE) \\\n   ((TARGET_SSE && (MODE) == V4SFmode) || (TARGET_SSE2 && (MODE) == V2DFmode))\n \n+#define AVX_FLOAT_MODE_P(MODE) \\\n+  (TARGET_AVX && ((MODE) == SFmode || (MODE) == DFmode))\n+\n+#define AVX128_VEC_FLOAT_MODE_P(MODE) \\\n+  (TARGET_AVX && ((MODE) == V4SFmode || (MODE) == V2DFmode))\n+\n+#define AVX256_VEC_FLOAT_MODE_P(MODE) \\\n+  (TARGET_AVX && ((MODE) == V8SFmode || (MODE) == V4DFmode))\n+\n+#define AVX_VEC_FLOAT_MODE_P(MODE) \\\n+  (TARGET_AVX && ((MODE) == V4SFmode || (MODE) == V2DFmode \\\n+\t\t  || (MODE) == V8SFmode || (MODE) == V4DFmode))\n+\n #define MMX_REG_P(XOP) (REG_P (XOP) && MMX_REGNO_P (REGNO (XOP)))\n #define MMX_REGNO_P(N) IN_RANGE ((N), FIRST_MMX_REG, LAST_MMX_REG)\n \n@@ -1559,6 +1586,7 @@ typedef struct ix86_args {\n   int fastcall;\t\t\t/* fastcall calling convention is used */\n   int sse_words;\t\t/* # sse words passed so far */\n   int sse_nregs;\t\t/* # sse registers available for passing */\n+  int warn_avx;\t\t\t/* True when we want to warn about AVX ABI.  */\n   int warn_sse;\t\t\t/* True when we want to warn about SSE ABI.  */\n   int warn_mmx;\t\t\t/* True when we want to warn about MMX ABI.  */\n   int sse_regno;\t\t/* next available sse register number */\n@@ -2133,6 +2161,29 @@ do {\t\t\t\t\t\t\t\t\t\\\n #define ASM_OUTPUT_ADDR_DIFF_ELT(FILE, BODY, VALUE, REL) \\\n   ix86_output_addr_diff_elt ((FILE), (VALUE), (REL))\n \n+/* When we see %v, we will print the 'v' prefix if TARGET_AVX is\n+   true.  */\n+\n+#define ASM_OUTPUT_AVX_PREFIX(STREAM, PTR)\t\\\n+{\t\t\t\t\t\t\\\n+  if ((PTR)[0] == '%' && (PTR)[1] == 'v')\t\\\n+    {\t\t\t\t\t\t\\\n+      if (TARGET_AVX)\t\t\t\t\\\n+\t(PTR) += 1;\t\t\t\t\\\n+      else\t\t\t\t\t\\\n+\t(PTR) += 2;\t\t\t\t\\\n+    }\t\t\t\t\t\t\\\n+}\n+\n+/* A C statement or statements which output an assembler instruction\n+   opcode to the stdio stream STREAM.  The macro-operand PTR is a\n+   variable of type `char *' which points to the opcode name in\n+   its \"internal\" form--the form that is written in the machine\n+   description.  */\n+\n+#define ASM_OUTPUT_OPCODE(STREAM, PTR) \\\n+  ASM_OUTPUT_AVX_PREFIX ((STREAM), (PTR))\n+\n /* Under some conditions we need jump tables in the text section,\n    because the assembler cannot handle label differences between\n    sections.  This is the case for x86_64 on Mach-O for example.  */"}, {"sha": "0a13751779666f27d9aa00833e17be03ff97eeae", "filename": "gcc/config/i386/i386.md", "status": "modified", "additions": 430, "deletions": 80, "changes": 510, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fi386.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fi386.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.md?ref=95879c728b9a59ae67db022ad370eb66374090f3", "patch": "@@ -195,6 +195,16 @@\n \n    ; For PCLMUL support\n    (UNSPEC_PCLMUL\t\t165)\n+\n+   ; For AVX support\n+   (UNSPEC_PCMP\t\t\t166)\n+   (UNSPEC_VPERMIL\t\t167)\n+   (UNSPEC_VPERMIL2\t\t168)\n+   (UNSPEC_VPERMIL2F128\t\t169)\n+   (UNSPEC_MASKLOAD\t\t170)\n+   (UNSPEC_MASKSTORE\t\t171)\n+   (UNSPEC_CAST\t\t\t172)\n+   (UNSPEC_VTESTP\t\t173)\n   ])\n \n (define_constants\n@@ -214,6 +224,8 @@\n    (UNSPECV_LOCK\t\t13)\n    (UNSPECV_PROLOGUE_USE\t14)\n    (UNSPECV_CLD\t\t\t15)\n+   (UNSPECV_VZEROALL\t\t16)\n+   (UNSPECV_VZEROUPPER\t\t17)\n   ])\n \n ;; Constants to represent pcomtrue/pcomfalse variants\n@@ -253,9 +265,25 @@\n    (FLAGS_REG\t\t\t17)\n    (FPSR_REG\t\t\t18)\n    (FPCR_REG\t\t\t19)\n+   (XMM0_REG\t\t\t21)\n+   (XMM1_REG\t\t\t22)\n+   (XMM2_REG\t\t\t23)\n+   (XMM3_REG\t\t\t24)\n+   (XMM4_REG\t\t\t25)\n+   (XMM5_REG\t\t\t26)\n+   (XMM6_REG\t\t\t27)\n+   (XMM7_REG\t\t\t28)\n    (R10_REG\t\t\t39)\n    (R11_REG\t\t\t40)\n    (R13_REG\t\t\t42)\n+   (XMM8_REG\t\t\t45)\n+   (XMM9_REG\t\t\t46)\n+   (XMM10_REG\t\t\t47)\n+   (XMM11_REG\t\t\t48)\n+   (XMM12_REG\t\t\t49)\n+   (XMM13_REG\t\t\t50)\n+   (XMM14_REG\t\t\t51)\n+   (XMM15_REG\t\t\t52)\n   ])\n \n ;; Insns whose names begin with \"x86_\" are emitted by gen_FOO calls\n@@ -291,7 +319,7 @@\n \n ;; Main data type used by the insn\n (define_attr \"mode\"\n-  \"unknown,none,QI,HI,SI,DI,TI,SF,DF,XF,TF,V4SF,V2DF,V2SF,V1DF\"\n+  \"unknown,none,QI,HI,SI,DI,TI,OI,SF,DF,XF,TF,V8SF,V4DF,V4SF,V2DF,V2SF,V1DF\"\n   (const_string \"unknown\"))\n \n ;; The CPU unit operations uses.\n@@ -388,6 +416,28 @@\n ;; There are also additional prefixes in SSSE3.\n (define_attr \"prefix_extra\" \"\" (const_int 0))\n \n+;; Prefix used: original, VEX or maybe VEX.\n+(define_attr \"prefix\" \"orig,vex,maybe_vex\"\n+  (if_then_else (eq_attr \"mode\" \"OI,V8SF,V4DF\")\n+    (const_string \"vex\")\n+    (const_string \"orig\")))\n+\n+;; There is a 8bit immediate for VEX.\n+(define_attr \"prefix_vex_imm8\" \"\" (const_int 0))\n+\n+;; VEX W bit is used.\n+(define_attr \"prefix_vex_w\" \"\" (const_int 0))\n+\n+;; The length of VEX prefix\n+(define_attr \"length_vex\" \"\"\n+  (if_then_else (eq_attr \"prefix_0f\" \"1\")\n+    (if_then_else (eq_attr \"prefix_vex_w\" \"1\")\n+      (symbol_ref \"ix86_attr_length_vex_default (insn, 1, 1)\")\n+      (symbol_ref \"ix86_attr_length_vex_default (insn, 1, 0)\"))\n+    (if_then_else (eq_attr \"prefix_vex_w\" \"1\")\n+      (symbol_ref \"ix86_attr_length_vex_default (insn, 0, 1)\")\n+      (symbol_ref \"ix86_attr_length_vex_default (insn, 0, 0)\"))))\n+\n ;; Set when modrm byte is used.\n (define_attr \"modrm\" \"\"\n   (cond [(eq_attr \"type\" \"str,leave\")\n@@ -433,7 +483,14 @@\n \t (eq_attr \"unit\" \"i387\")\n \t   (plus (const_int 2)\n \t\t (plus (attr \"prefix_data16\")\n-\t\t       (attr \"length_address\")))]\n+\t\t       (attr \"length_address\")))\n+\t (ior (eq_attr \"prefix\" \"vex\")\n+\t      (and (eq_attr \"prefix\" \"maybe_vex\")\n+\t\t    (ne (symbol_ref \"TARGET_AVX\") (const_int 0))))\n+\t   (plus (attr \"length_vex\")\n+\t\t (plus (attr \"prefix_vex_imm8\")\n+\t\t       (plus (attr \"modrm\")\n+\t\t\t     (attr \"length_address\"))))]\n \t (plus (plus (attr \"modrm\")\n \t\t     (plus (attr \"prefix_0f\")\n \t\t\t   (plus (attr \"prefix_rex\")\n@@ -572,6 +629,9 @@\n ;; Mapping of unsigned max and min\n (define_code_iterator umaxmin [umax umin])\n \n+;; Mapping of signed/unsigned max and min\n+(define_code_iterator maxmin [smax smin umax umin])\n+\n ;; Base name for integer and FP insn mnemonic\n (define_code_attr maxminiprefix [(smax \"maxs\") (smin \"mins\")\n \t\t\t\t (umax \"maxu\") (umin \"minu\")])\n@@ -1254,6 +1314,7 @@\n    && GET_MODE (operands[0]) == GET_MODE (operands[1])\"\n   \"* return output_fp_compare (insn, operands, 1, 0);\"\n   [(set_attr \"type\" \"fcmp,ssecomi\")\n+   (set_attr \"prefix\" \"orig,maybe_vex\")\n    (set (attr \"mode\")\n      (if_then_else (match_operand:SF 1 \"\" \"\")\n         (const_string \"SF\")\n@@ -1270,6 +1331,7 @@\n    && GET_MODE (operands[0]) == GET_MODE (operands[1])\"\n   \"* return output_fp_compare (insn, operands, 1, 0);\"\n   [(set_attr \"type\" \"ssecomi\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set (attr \"mode\")\n      (if_then_else (match_operand:SF 1 \"\" \"\")\n         (const_string \"SF\")\n@@ -1306,6 +1368,7 @@\n    && GET_MODE (operands[0]) == GET_MODE (operands[1])\"\n   \"* return output_fp_compare (insn, operands, 1, 1);\"\n   [(set_attr \"type\" \"fcmp,ssecomi\")\n+   (set_attr \"prefix\" \"orig,maybe_vex\")\n    (set (attr \"mode\")\n      (if_then_else (match_operand:SF 1 \"\" \"\")\n         (const_string \"SF\")\n@@ -1322,6 +1385,7 @@\n    && GET_MODE (operands[0]) == GET_MODE (operands[1])\"\n   \"* return output_fp_compare (insn, operands, 1, 1);\"\n   [(set_attr \"type\" \"ssecomi\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set (attr \"mode\")\n      (if_then_else (match_operand:SF 1 \"\" \"\")\n         (const_string \"SF\")\n@@ -1450,20 +1514,20 @@\n     {\n     case TYPE_SSELOG1:\n       if (get_attr_mode (insn) == MODE_TI)\n-        return \"pxor\\t%0, %0\";\n-      return \"xorps\\t%0, %0\";\n+        return \"%vpxor\\t%0, %d0\";\n+      return \"%vxorps\\t%0, %d0\";\n \n     case TYPE_SSEMOV:\n       switch (get_attr_mode (insn))\n \t{\n \tcase MODE_TI:\n-\t  return \"movdqa\\t{%1, %0|%0, %1}\";\n+\t  return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n \tcase MODE_V4SF:\n-\t  return \"movaps\\t{%1, %0|%0, %1}\";\n+\t  return \"%vmovaps\\t{%1, %0|%0, %1}\";\n \tcase MODE_SI:\n-          return \"movd\\t{%1, %0|%0, %1}\";\n+          return \"%vmovd\\t{%1, %0|%0, %1}\";\n \tcase MODE_SF:\n-          return \"movss\\t{%1, %0|%0, %1}\";\n+          return \"%vmovss\\t{%1, %0|%0, %1}\";\n \tdefault:\n \t  gcc_unreachable ();\n \t}\n@@ -1497,6 +1561,10 @@\n \t      (const_string \"lea\")\n \t   ]\n \t   (const_string \"imov\")))\n+   (set (attr \"prefix\")\n+     (if_then_else (eq_attr \"alternative\" \"0,1,2,3,4,5\")\n+       (const_string \"orig\")\n+       (const_string \"maybe_vex\")))\n    (set (attr \"mode\")\n      (cond [(eq_attr \"alternative\" \"2,3\")\n \t      (const_string \"DI\")\n@@ -2225,15 +2293,19 @@\n    pxor\\t%0, %0\n    movq\\t{%1, %0|%0, %1}\n    movq\\t{%1, %0|%0, %1}\n-   pxor\\t%0, %0\n-   movq\\t{%1, %0|%0, %1}\n-   movdqa\\t{%1, %0|%0, %1}\n-   movq\\t{%1, %0|%0, %1}\n+   %vpxor\\t%0, %d0\n+   %vmovq\\t{%1, %0|%0, %1}\n+   %vmovdqa\\t{%1, %0|%0, %1}\n+   %vmovq\\t{%1, %0|%0, %1}\n    xorps\\t%0, %0\n    movlps\\t{%1, %0|%0, %1}\n    movaps\\t{%1, %0|%0, %1}\n    movlps\\t{%1, %0|%0, %1}\"\n   [(set_attr \"type\" \"*,*,mmx,mmxmov,mmxmov,sselog1,ssemov,ssemov,ssemov,sselog1,ssemov,ssemov,ssemov\")\n+   (set (attr \"prefix\")\n+     (if_then_else (eq_attr \"alternative\" \"5,6,7,8\")\n+       (const_string \"vex\")\n+       (const_string \"orig\")))\n    (set_attr \"mode\" \"DI,DI,DI,DI,DI,TI,DI,TI,DI,V4SF,V2SF,V4SF,V2SF\")])\n \n (define_split\n@@ -2270,6 +2342,14 @@\n \treturn \"movdq2q\\t{%1, %0|%0, %1}\";\n \n     case TYPE_SSEMOV:\n+      if (TARGET_AVX)\n+\t{\n+\t  if (get_attr_mode (insn) == MODE_TI)\n+\t    return \"vmovdqa\\t{%1, %0|%0, %1}\";\n+\t  else\n+\t    return \"vmovq\\t{%1, %0|%0, %1}\";\n+\t}\n+\n       if (get_attr_mode (insn) == MODE_TI)\n \treturn \"movdqa\\t{%1, %0|%0, %1}\";\n       /* FALLTHRU */\n@@ -2282,6 +2362,8 @@\n       return \"movq\\t{%1, %0|%0, %1}\";\n \n     case TYPE_SSELOG1:\n+      return \"%vpxor\\t%0, %d0\";\n+\n     case TYPE_MMXADD:\n       return \"pxor\\t%0, %0\";\n \n@@ -2320,6 +2402,10 @@\n \t   (const_string \"imov\")))\n    (set_attr \"modrm\" \"*,0,0,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*\")\n    (set_attr \"length_immediate\" \"*,4,8,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,*\")\n+   (set (attr \"prefix\")\n+     (if_then_else (eq_attr \"alternative\" \"11,12,13,14,15,16\")\n+       (const_string \"maybe_vex\")\n+       (const_string \"orig\")))\n    (set_attr \"mode\" \"SI,DI,DI,DI,SI,DI,DI,DI,DI,DI,DI,TI,TI,DI,DI,DI,DI,DI,DI\")])\n \n ;; Stores and loads of ax to arbitrary constant address.\n@@ -2402,6 +2488,37 @@\n    (set_attr \"athlon_decode\" \"vector\")\n    (set_attr \"amdfam10_decode\" \"double\")])\n \n+(define_expand \"movoi\"\n+  [(set (match_operand:OI 0 \"nonimmediate_operand\" \"\")\n+\t(match_operand:OI 1 \"general_operand\" \"\"))]\n+  \"TARGET_AVX\"\n+  \"ix86_expand_move (OImode, operands); DONE;\")\n+\n+(define_insn \"*movoi_internal\"\n+  [(set (match_operand:OI 0 \"nonimmediate_operand\" \"=x,x,m\")\n+\t(match_operand:OI 1 \"vector_move_operand\" \"C,xm,x\"))]\n+  \"TARGET_AVX\n+   && !(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n+{\n+  switch (which_alternative)\n+    {\n+    case 0:\n+      return \"vxorps\\t%0, %0, %0\";\n+    case 1:\n+    case 2:\n+      if (misaligned_operand (operands[0], OImode)\n+\t  || misaligned_operand (operands[1], OImode))\n+\treturn \"vmovdqu\\t{%1, %0|%0, %1}\";\n+      else\n+\treturn \"vmovdqa\\t{%1, %0|%0, %1}\";\n+    default:\n+      gcc_unreachable ();\n+    }\n+}\n+  [(set_attr \"type\" \"sselog1,ssemov,ssemov\")\n+   (set_attr \"prefix\" \"vex\")\n+   (set_attr \"mode\" \"OI\")])\n+\n (define_expand \"movti\"\n   [(set (match_operand:TI 0 \"nonimmediate_operand\" \"\")\n \t(match_operand:TI 1 \"nonimmediate_operand\" \"\"))]\n@@ -2426,9 +2543,9 @@\n     {\n     case 0:\n       if (get_attr_mode (insn) == MODE_V4SF)\n-\treturn \"xorps\\t%0, %0\";\n+\treturn \"%vxorps\\t%0, %d0\";\n       else\n-\treturn \"pxor\\t%0, %0\";\n+\treturn \"%vpxor\\t%0, %d0\";\n     case 1:\n     case 2:\n       /* TDmode values are passed as TImode on the stack.  Moving them\n@@ -2437,22 +2554,23 @@\n \t  || misaligned_operand (operands[1], TImode))\n \t{ \n \t  if (get_attr_mode (insn) == MODE_V4SF)\n-\t    return \"movups\\t{%1, %0|%0, %1}\";\n+\t    return \"%vmovups\\t{%1, %0|%0, %1}\";\n \t else\n-\t   return \"movdqu\\t{%1, %0|%0, %1}\";\n+\t   return \"%vmovdqu\\t{%1, %0|%0, %1}\";\n \t}\n       else\n \t{ \n \t  if (get_attr_mode (insn) == MODE_V4SF)\n-\t    return \"movaps\\t{%1, %0|%0, %1}\";\n+\t    return \"%vmovaps\\t{%1, %0|%0, %1}\";\n \t else\n-\t   return \"movdqa\\t{%1, %0|%0, %1}\";\n+\t   return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n \t}\n     default:\n       gcc_unreachable ();\n     }\n }\n   [(set_attr \"type\" \"sselog1,ssemov,ssemov\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set (attr \"mode\")\n \t(cond [(ior (eq (symbol_ref \"TARGET_SSE2\") (const_int 0))\n \t\t    (ne (symbol_ref \"optimize_size\") (const_int 0)))\n@@ -2476,9 +2594,9 @@\n       return \"#\";\n     case 2:\n       if (get_attr_mode (insn) == MODE_V4SF)\n-\treturn \"xorps\\t%0, %0\";\n+\treturn \"%vxorps\\t%0, %d0\";\n       else\n-\treturn \"pxor\\t%0, %0\";\n+\treturn \"%vpxor\\t%0, %d0\";\n     case 3:\n     case 4:\n       /* TDmode values are passed as TImode on the stack.  Moving them\n@@ -2487,22 +2605,23 @@\n \t  || misaligned_operand (operands[1], TImode))\n \t{ \n \t  if (get_attr_mode (insn) == MODE_V4SF)\n-\t    return \"movups\\t{%1, %0|%0, %1}\";\n+\t    return \"%vmovups\\t{%1, %0|%0, %1}\";\n \t else\n-\t   return \"movdqu\\t{%1, %0|%0, %1}\";\n+\t   return \"%vmovdqu\\t{%1, %0|%0, %1}\";\n \t}\n       else\n \t{ \n \t  if (get_attr_mode (insn) == MODE_V4SF)\n-\t    return \"movaps\\t{%1, %0|%0, %1}\";\n+\t    return \"%vmovaps\\t{%1, %0|%0, %1}\";\n \t else\n-\t   return \"movdqa\\t{%1, %0|%0, %1}\";\n+\t   return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n \t}\n     default:\n       gcc_unreachable ();\n     }\n }\n   [(set_attr \"type\" \"*,*,sselog1,ssemov,ssemov\")\n+   (set_attr \"prefix\" \"*,*,maybe_vex,maybe_vex,maybe_vex\")\n    (set (attr \"mode\")\n         (cond [(eq_attr \"alternative\" \"2,3\")\n \t\t (if_then_else\n@@ -2628,20 +2747,27 @@\n       return \"mov{l}\\t{%1, %0|%0, %1}\";\n     case 5:\n       if (get_attr_mode (insn) == MODE_TI)\n-\treturn \"pxor\\t%0, %0\";\n+\treturn \"%vpxor\\t%0, %d0\";\n       else\n-\treturn \"xorps\\t%0, %0\";\n+\treturn \"%vxorps\\t%0, %d0\";\n     case 6:\n       if (get_attr_mode (insn) == MODE_V4SF)\n-\treturn \"movaps\\t{%1, %0|%0, %1}\";\n+\treturn \"%vmovaps\\t{%1, %0|%0, %1}\";\n+      else\n+\treturn \"%vmovss\\t{%1, %d0|%d0, %1}\";\n+    case 7:\n+      if (TARGET_AVX)\n+\treturn REG_P (operands[1]) ? \"vmovss\\t{%1, %0, %0|%0, %0, %1}\"\n+\t\t\t\t   : \"vmovss\\t{%1, %0|%0, %1}\";\n       else\n \treturn \"movss\\t{%1, %0|%0, %1}\";\n-    case 7: case 8:\n-      return \"movss\\t{%1, %0|%0, %1}\";\n+    case 8:\n+      return \"%vmovss\\t{%1, %0|%0, %1}\";\n \n-    case 9: case 10:\n-    case 12: case 13: case 14: case 15:\n+    case 9: case 10: case 14: case 15:\n       return \"movd\\t{%1, %0|%0, %1}\";\n+    case 12: case 13:\n+      return \"%vmovd\\t{%1, %0|%0, %1}\";\n \n     case 11:\n       return \"movq\\t{%1, %0|%0, %1}\";\n@@ -2651,6 +2777,10 @@\n     }\n }\n   [(set_attr \"type\" \"fmov,fmov,fmov,imov,imov,sselog1,ssemov,ssemov,ssemov,mmxmov,mmxmov,mmxmov,ssemov,ssemov,mmxmov,mmxmov\")\n+   (set (attr \"prefix\")\n+     (if_then_else (eq_attr \"alternative\" \"5,6,7,8,12,13\")\n+       (const_string \"maybe_vex\")\n+       (const_string \"orig\")))\n    (set (attr \"mode\")\n         (cond [(eq_attr \"alternative\" \"3,4,9,10\")\n \t\t (const_string \"SI\")\n@@ -2790,11 +2920,11 @@\n       switch (get_attr_mode (insn))\n \t{\n \tcase MODE_V4SF:\n-\t  return \"xorps\\t%0, %0\";\n+\t  return \"%vxorps\\t%0, %d0\";\n \tcase MODE_V2DF:\n-\t  return \"xorpd\\t%0, %0\";\n+\t  return \"%vxorpd\\t%0, %d0\";\n \tcase MODE_TI:\n-\t  return \"pxor\\t%0, %0\";\n+\t  return \"%vpxor\\t%0, %d0\";\n \tdefault:\n \t  gcc_unreachable ();\n \t}\n@@ -2804,19 +2934,43 @@\n       switch (get_attr_mode (insn))\n \t{\n \tcase MODE_V4SF:\n-\t  return \"movaps\\t{%1, %0|%0, %1}\";\n+\t  return \"%vmovaps\\t{%1, %0|%0, %1}\";\n \tcase MODE_V2DF:\n-\t  return \"movapd\\t{%1, %0|%0, %1}\";\n+\t  return \"%vmovapd\\t{%1, %0|%0, %1}\";\n \tcase MODE_TI:\n-\t  return \"movdqa\\t{%1, %0|%0, %1}\";\n+\t  return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n \tcase MODE_DI:\n-\t  return \"movq\\t{%1, %0|%0, %1}\";\n+\t  return \"%vmovq\\t{%1, %0|%0, %1}\";\n \tcase MODE_DF:\n-\t  return \"movsd\\t{%1, %0|%0, %1}\";\n+\t  if (TARGET_AVX)\n+\t    {\n+\t      if (REG_P (operands[0]) && REG_P (operands[1]))\n+\t\treturn \"vmovsd\\t{%1, %0, %0|%0, %0, %1}\";\n+\t      else\n+\t\treturn \"vmovsd\\t{%1, %0|%0, %1}\";\n+\t    }\n+\t  else\n+\t    return \"movsd\\t{%1, %0|%0, %1}\";\n \tcase MODE_V1DF:\n-\t  return \"movlpd\\t{%1, %0|%0, %1}\";\n+\t  if (TARGET_AVX)\n+\t    {\n+\t      if (REG_P (operands[0]))\n+\t\treturn \"vmovlpd\\t{%1, %0, %0|%0, %0, %1}\";\n+\t      else\n+\t\treturn \"vmovlpd\\t{%1, %0|%0, %1}\";\n+\t    }\n+\t  else\n+\t    return \"movlpd\\t{%1, %0|%0, %1}\";\n \tcase MODE_V2SF:\n-\t  return \"movlps\\t{%1, %0|%0, %1}\";\n+\t  if (TARGET_AVX)\n+\t    {\n+\t      if (REG_P (operands[0]))\n+\t\treturn \"vmovlps\\t{%1, %0, %0|%0, %0, %1}\";\n+\t      else\n+\t\treturn \"vmovlps\\t{%1, %0|%0, %1}\";\n+\t    }\n+\t  else\n+\t    return \"movlps\\t{%1, %0|%0, %1}\";\n \tdefault:\n \t  gcc_unreachable ();\n \t}\n@@ -2826,6 +2980,10 @@\n     }\n }\n   [(set_attr \"type\" \"fmov,fmov,fmov,multi,multi,sselog1,ssemov,ssemov,ssemov\")\n+   (set (attr \"prefix\")\n+     (if_then_else (eq_attr \"alternative\" \"0,1,2,3,4\")\n+       (const_string \"orig\")\n+       (const_string \"maybe_vex\")))\n    (set (attr \"mode\")\n         (cond [(eq_attr \"alternative\" \"0,1,2\")\n \t\t (const_string \"DF\")\n@@ -2907,11 +3065,11 @@\n       switch (get_attr_mode (insn))\n \t{\n \tcase MODE_V4SF:\n-\t  return \"xorps\\t%0, %0\";\n+\t  return \"%vxorps\\t%0, %d0\";\n \tcase MODE_V2DF:\n-\t  return \"xorpd\\t%0, %0\";\n+\t  return \"%vxorpd\\t%0, %d0\";\n \tcase MODE_TI:\n-\t  return \"pxor\\t%0, %0\";\n+\t  return \"%vpxor\\t%0, %d0\";\n \tdefault:\n \t  gcc_unreachable ();\n \t}\n@@ -2921,32 +3079,44 @@\n       switch (get_attr_mode (insn))\n \t{\n \tcase MODE_V4SF:\n-\t  return \"movaps\\t{%1, %0|%0, %1}\";\n+\t  return \"%vmovaps\\t{%1, %0|%0, %1}\";\n \tcase MODE_V2DF:\n-\t  return \"movapd\\t{%1, %0|%0, %1}\";\n+\t  return \"%vmovapd\\t{%1, %0|%0, %1}\";\n \tcase MODE_TI:\n-\t  return \"movdqa\\t{%1, %0|%0, %1}\";\n+\t  return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n \tcase MODE_DI:\n-\t  return \"movq\\t{%1, %0|%0, %1}\";\n+\t  return \"%vmovq\\t{%1, %0|%0, %1}\";\n \tcase MODE_DF:\n-\t  return \"movsd\\t{%1, %0|%0, %1}\";\n+\t  if (TARGET_AVX)\n+\t    {\n+\t      if (REG_P (operands[0]) && REG_P (operands[1]))\n+\t\treturn \"vmovsd\\t{%1, %0, %0|%0, %0, %1}\";\n+\t      else\n+\t\treturn \"vmovsd\\t{%1, %0|%0, %1}\";\n+\t    }\n+\t  else\n+\t    return \"movsd\\t{%1, %0|%0, %1}\";\n \tcase MODE_V1DF:\n-\t  return \"movlpd\\t{%1, %0|%0, %1}\";\n+\t  return \"%vmovlpd\\t{%1, %d0|%d0, %1}\";\n \tcase MODE_V2SF:\n-\t  return \"movlps\\t{%1, %0|%0, %1}\";\n+\t  return \"%vmovlps\\t{%1, %d0|%d0, %1}\";\n \tdefault:\n \t  gcc_unreachable ();\n \t}\n \n     case 9:\n     case 10:\n-      return \"movd\\t{%1, %0|%0, %1}\";\n+    return \"%vmovd\\t{%1, %0|%0, %1}\";\n \n     default:\n       gcc_unreachable();\n     }\n }\n   [(set_attr \"type\" \"fmov,fmov,fmov,multi,multi,sselog1,ssemov,ssemov,ssemov,ssemov,ssemov\")\n+   (set (attr \"prefix\")\n+     (if_then_else (eq_attr \"alternative\" \"0,1,2,3,4\")\n+       (const_string \"orig\")\n+       (const_string \"maybe_vex\")))\n    (set (attr \"mode\")\n         (cond [(eq_attr \"alternative\" \"0,1,2\")\n \t\t (const_string \"DF\")\n@@ -3278,14 +3448,14 @@\n     case 0:\n     case 1:\n       if (get_attr_mode (insn) == MODE_V4SF)\n-\treturn \"movaps\\t{%1, %0|%0, %1}\";\n+\treturn \"%vmovaps\\t{%1, %0|%0, %1}\";\n       else\n-\treturn \"movdqa\\t{%1, %0|%0, %1}\";\n+\treturn \"%vmovdqa\\t{%1, %0|%0, %1}\";\n     case 2:\n       if (get_attr_mode (insn) == MODE_V4SF)\n-\treturn \"xorps\\t%0, %0\";\n+\treturn \"%vxorps\\t%0, %d0\";\n       else\n-\treturn \"pxor\\t%0, %0\";\n+\treturn \"%vpxor\\t%0, %d0\";\n     case 3:\n     case 4:\n \treturn \"#\";\n@@ -3294,6 +3464,7 @@\n     }\n }\n   [(set_attr \"type\" \"ssemov,ssemov,sselog1,*,*\")\n+   (set_attr \"prefix\" \"maybe_vex,maybe_vex,maybe_vex,*,*\")\n    (set (attr \"mode\")\n         (cond [(eq_attr \"alternative\" \"0,2\")\n \t\t (if_then_else\n@@ -3669,10 +3840,11 @@\n    #\n    movd\\t{%1, %0|%0, %1}\n    movd\\t{%1, %0|%0, %1}\n-   movd\\t{%1, %0|%0, %1}\n-   movd\\t{%1, %0|%0, %1}\"\n-  [(set_attr \"mode\" \"SI,SI,SI,DI,DI,TI,TI\")\n-   (set_attr \"type\" \"multi,multi,multi,mmxmov,mmxmov,ssemov,ssemov\")])\n+   %vmovd\\t{%1, %0|%0, %1}\n+   %vmovd\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"multi,multi,multi,mmxmov,mmxmov,ssemov,ssemov\")\n+   (set_attr \"prefix\" \"*,*,*,orig,orig,maybe_vex,maybe_vex\")\n+   (set_attr \"mode\" \"SI,SI,SI,DI,DI,TI,TI\")])\n \n (define_insn \"zero_extendsidi2_rex64\"\n   [(set (match_operand:DI 0 \"nonimmediate_operand\" \"=r,o,?*Ym,?*y,?*Yi,*Y2\")\n@@ -3684,9 +3856,10 @@\n    #\n    movd\\t{%1, %0|%0, %1}\n    movd\\t{%1, %0|%0, %1}\n-   movd\\t{%1, %0|%0, %1}\n-   movd\\t{%1, %0|%0, %1}\"\n+   %vmovd\\t{%1, %0|%0, %1}\n+   %vmovd\\t{%1, %0|%0, %1}\"\n   [(set_attr \"type\" \"imovx,imov,mmxmov,mmxmov,ssemov,ssemov\")\n+   (set_attr \"prefix\" \"orig,*,orig,orig,maybe_vex,maybe_vex\")\n    (set_attr \"mode\" \"SI,DI,DI,DI,TI,TI\")])\n \n (define_split\n@@ -4071,21 +4244,23 @@\n       return output_387_reg_move (insn, operands);\n \n     case 2:\n-      return \"cvtss2sd\\t{%1, %0|%0, %1}\";\n+      return \"%vcvtss2sd\\t{%1, %d0|%d0, %1}\";\n \n     default:\n       gcc_unreachable ();\n     }\n }\n   [(set_attr \"type\" \"fmov,fmov,ssecvt\")\n+   (set_attr \"prefix\" \"orig,orig,maybe_vex\")\n    (set_attr \"mode\" \"SF,XF,DF\")])\n \n (define_insn \"*extendsfdf2_sse\"\n   [(set (match_operand:DF 0 \"nonimmediate_operand\" \"=x\")\n         (float_extend:DF (match_operand:SF 1 \"nonimmediate_operand\" \"xm\")))]\n   \"TARGET_SSE2 && TARGET_SSE_MATH\"\n-  \"cvtss2sd\\t{%1, %0|%0, %1}\"\n+  \"%vcvtss2sd\\t{%1, %d0|%d0, %1}\"\n   [(set_attr \"type\" \"ssecvt\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set_attr \"mode\" \"DF\")])\n \n (define_insn \"*extendsfdf2_i387\"\n@@ -4214,12 +4389,13 @@\n     case 0:\n       return output_387_reg_move (insn, operands);\n     case 1:\n-      return \"cvtsd2ss\\t{%1, %0|%0, %1}\";\n+      return \"%vcvtsd2ss\\t{%1, %d0|%d0, %1}\";\n     default:\n       gcc_unreachable ();\n     }\n }\n   [(set_attr \"type\" \"fmov,ssecvt\")\n+   (set_attr \"prefix\" \"orig,maybe_vex\")\n    (set_attr \"mode\" \"SF\")])\n \n ;; Yes, this one doesn't depend on flag_unsafe_math_optimizations,\n@@ -4229,8 +4405,9 @@\n         (float_truncate:SF\n           (match_operand:DF 1 \"nonimmediate_operand\" \"xm\")))]\n   \"TARGET_SSE2 && TARGET_SSE_MATH\"\n-  \"cvtsd2ss\\t{%1, %0|%0, %1}\"\n+  \"%vcvtsd2ss\\t{%1, %d0|%d0, %1}\"\n   [(set_attr \"type\" \"ssecvt\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set_attr \"mode\" \"SF\")])\n \n (define_insn \"*truncdfsf_fast_i387\"\n@@ -4257,13 +4434,14 @@\n     case 1:\n       return \"#\";\n     case 2:\n-      return \"cvtsd2ss\\t{%1, %0|%0, %1}\";\n+      return \"%vcvtsd2ss\\t{%1, %d0|%d0, %1}\";\n     default:\n       gcc_unreachable ();\n     }\n }\n   [(set_attr \"type\" \"fmov,multi,ssecvt\")\n    (set_attr \"unit\" \"*,i387,*\")\n+   (set_attr \"prefix\" \"orig,orig,maybe_vex\")\n    (set_attr \"mode\" \"SF\")])\n \n (define_insn \"*truncdfsf_i387\"\n@@ -4550,8 +4728,9 @@\n \t(fix:DI (match_operand:MODEF 1 \"nonimmediate_operand\" \"x,m\")))]\n   \"TARGET_64BIT && SSE_FLOAT_MODE_P (<MODE>mode)\n    && (!TARGET_FISTTP || TARGET_SSE_MATH)\"\n-  \"cvtts<ssemodefsuffix>2si{q}\\t{%1, %0|%0, %1}\"\n+  \"%vcvtts<ssemodefsuffix>2si{q}\\t{%1, %0|%0, %1}\"\n   [(set_attr \"type\" \"sseicvt\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set_attr \"mode\" \"<MODE>\")\n    (set_attr \"athlon_decode\" \"double,vector\")\n    (set_attr \"amdfam10_decode\" \"double,double\")])\n@@ -4561,8 +4740,9 @@\n \t(fix:SI (match_operand:MODEF 1 \"nonimmediate_operand\" \"x,m\")))]\n   \"SSE_FLOAT_MODE_P (<MODE>mode)\n    && (!TARGET_FISTTP || TARGET_SSE_MATH)\"\n-  \"cvtts<ssemodefsuffix>2si\\t{%1, %0|%0, %1}\"\n+  \"%vcvtts<ssemodefsuffix>2si\\t{%1, %0|%0, %1}\"\n   [(set_attr \"type\" \"sseicvt\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set_attr \"mode\" \"<MODE>\")\n    (set_attr \"athlon_decode\" \"double,vector\")\n    (set_attr \"amdfam10_decode\" \"double,double\")])\n@@ -5042,9 +5222,10 @@\n    && (TARGET_INTER_UNIT_CONVERSIONS || optimize_size)\"\n   \"@\n    fild%z1\\t%1\n-   cvtsi2s<MODEF:ssemodefsuffix><SSEMODEI24:rex64suffix>\\t{%1, %0|%0, %1}\n-   cvtsi2s<MODEF:ssemodefsuffix><SSEMODEI24:rex64suffix>\\t{%1, %0|%0, %1}\"\n+   %vcvtsi2s<MODEF:ssemodefsuffix><SSEMODEI24:rex64suffix>\\t{%1, %d0|%d0, %1}\n+   %vcvtsi2s<MODEF:ssemodefsuffix><SSEMODEI24:rex64suffix>\\t{%1, %d0|%d0, %1}\"\n   [(set_attr \"type\" \"fmov,sseicvt,sseicvt\")\n+   (set_attr \"prefix\" \"orig,maybe_vex,maybe_vex\")\n    (set_attr \"mode\" \"<MODEF:MODE>\")\n    (set_attr \"unit\" \"i387,*,*\")\n    (set_attr \"athlon_decode\" \"*,double,direct\")\n@@ -5060,8 +5241,9 @@\n    && !(TARGET_INTER_UNIT_CONVERSIONS || optimize_size)\"\n   \"@\n    fild%z1\\t%1\n-   cvtsi2s<MODEF:ssemodefsuffix><SSEMODEI24:rex64suffix>\\t{%1, %0|%0, %1}\"\n+   %vcvtsi2s<MODEF:ssemodefsuffix><SSEMODEI24:rex64suffix>\\t{%1, %d0|%d0, %1}\"\n   [(set_attr \"type\" \"fmov,sseicvt\")\n+   (set_attr \"prefix\" \"orig,maybe_vex\")\n    (set_attr \"mode\" \"<MODEF:MODE>\")\n    (set_attr \"athlon_decode\" \"*,direct\")\n    (set_attr \"amdfam10_decode\" \"*,double\")\n@@ -5232,8 +5414,9 @@\n   \"(<SSEMODEI24:MODE>mode != DImode || TARGET_64BIT)\n    && SSE_FLOAT_MODE_P (<MODEF:MODE>mode) && TARGET_SSE_MATH\n    && (TARGET_INTER_UNIT_CONVERSIONS || optimize_size)\"\n-  \"cvtsi2s<MODEF:ssemodefsuffix><SSEMODEI24:rex64suffix>\\t{%1, %0|%0, %1}\"\n+  \"%vcvtsi2s<MODEF:ssemodefsuffix><SSEMODEI24:rex64suffix>\\t{%1, %d0|%d0, %1}\"\n   [(set_attr \"type\" \"sseicvt\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set_attr \"mode\" \"<MODEF:MODE>\")\n    (set_attr \"athlon_decode\" \"double,direct\")\n    (set_attr \"amdfam10_decode\" \"vector,double\")\n@@ -5260,8 +5443,9 @@\n   \"(<SSEMODEI24:MODE>mode != DImode || TARGET_64BIT)\n    && SSE_FLOAT_MODE_P (<MODEF:MODE>mode) && TARGET_SSE_MATH\n    && !(TARGET_INTER_UNIT_CONVERSIONS || optimize_size)\"\n-  \"cvtsi2s<MODEF:ssemodefsuffix><SSEMODEI24:rex64suffix>\\t{%1, %0|%0, %1}\"\n+  \"%vcvtsi2s<MODEF:ssemodefsuffix><SSEMODEI24:rex64suffix>\\t{%1, %d0|%d0, %1}\"\n   [(set_attr \"type\" \"sseicvt\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set_attr \"mode\" \"<MODEF:MODE>\")\n    (set_attr \"athlon_decode\" \"direct\")\n    (set_attr \"amdfam10_decode\" \"double\")\n@@ -10862,6 +11046,19 @@\n ;; This pattern must be defined before *ashlti3_1 to prevent\n ;; combine pass from converting sse2_ashlti3 to *ashlti3_1.\n \n+(define_insn \"*avx_ashlti3\"\n+  [(set (match_operand:TI 0 \"register_operand\" \"=x\")\n+\t(ashift:TI (match_operand:TI 1 \"register_operand\" \"x\")\n+\t\t   (match_operand:SI 2 \"const_0_to_255_mul_8_operand\" \"n\")))]\n+  \"TARGET_AVX\"\n+{\n+  operands[2] = GEN_INT (INTVAL (operands[2]) / 8);\n+  return \"vpslldq\\t{%2, %1, %0|%0, %1, %2}\";\n+}\n+  [(set_attr \"type\" \"sseishft\")\n+   (set_attr \"prefix\" \"vex\")\n+   (set_attr \"mode\" \"TI\")])\n+\n (define_insn \"sse2_ashlti3\"\n   [(set (match_operand:TI 0 \"register_operand\" \"=x\")\n \t(ashift:TI (match_operand:TI 1 \"register_operand\" \"0\")\n@@ -12561,6 +12758,19 @@\n ;; This pattern must be defined before *lshrti3_1 to prevent\n ;; combine pass from converting sse2_lshrti3 to *lshrti3_1.\n \n+(define_insn \"*avx_lshrti3\"\n+  [(set (match_operand:TI 0 \"register_operand\" \"=x\")\n+ \t(lshiftrt:TI (match_operand:TI 1 \"register_operand\" \"x\")\n+\t\t     (match_operand:SI 2 \"const_0_to_255_mul_8_operand\" \"n\")))]\n+  \"TARGET_AVX\"\n+{\n+  operands[2] = GEN_INT (INTVAL (operands[2]) / 8);\n+  return \"vpsrldq\\t{%2, %1, %0|%0, %1, %2}\";\n+}\n+  [(set_attr \"type\" \"sseishft\")\n+   (set_attr \"prefix\" \"vex\")\n+   (set_attr \"mode\" \"TI\")])\n+\n (define_insn \"sse2_lshrti3\"\n   [(set (match_operand:TI 0 \"register_operand\" \"=x\")\n  \t(lshiftrt:TI (match_operand:TI 1 \"register_operand\" \"0\")\n@@ -13935,6 +14145,17 @@\n ;; 0xffffffff is NaN, but not in normalized form, so we can't represent\n ;; it directly.\n \n+(define_insn \"*avx_setcc<mode>\"\n+  [(set (match_operand:MODEF 0 \"register_operand\" \"=x\")\n+\t(match_operator:MODEF 1 \"avx_comparison_float_operator\"\n+\t  [(match_operand:MODEF 2 \"register_operand\" \"x\")\n+\t   (match_operand:MODEF 3 \"nonimmediate_operand\" \"xm\")]))]\n+  \"TARGET_AVX\"\n+  \"vcmp%D1s<ssemodefsuffix>\\t{%3, %2, %0|%0, %2, %3}\"\n+  [(set_attr \"type\" \"ssecmp\")\n+   (set_attr \"prefix\" \"vex\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n (define_insn \"*sse_setcc<mode>\"\n   [(set (match_operand:MODEF 0 \"register_operand\" \"=x\")\n \t(match_operator:MODEF 1 \"sse_comparison_operator\"\n@@ -16013,6 +16234,26 @@\n ;; Gcc is slightly more smart about handling normal two address instructions\n ;; so use special patterns for add and mull.\n \n+(define_insn \"*fop_<mode>_comm_mixed_avx\"\n+  [(set (match_operand:MODEF 0 \"register_operand\" \"=f,x\")\n+\t(match_operator:MODEF 3 \"binary_fp_operator\"\n+\t  [(match_operand:MODEF 1 \"nonimmediate_operand\" \"%0,x\")\n+\t   (match_operand:MODEF 2 \"nonimmediate_operand\" \"fm,xm\")]))]\n+  \"AVX_FLOAT_MODE_P (<MODE>mode) && TARGET_MIX_SSE_I387\n+   && COMMUTATIVE_ARITH_P (operands[3])\n+   && !(MEM_P (operands[1]) && MEM_P (operands[2]))\"\n+  \"* return output_387_binary_op (insn, operands);\"\n+  [(set (attr \"type\")\n+\t(if_then_else (eq_attr \"alternative\" \"1\")\n+\t   (if_then_else (match_operand:MODEF 3 \"mult_operator\" \"\")\n+\t      (const_string \"ssemul\")\n+\t      (const_string \"sseadd\"))\n+\t   (if_then_else (match_operand:MODEF 3 \"mult_operator\" \"\")\n+\t      (const_string \"fmul\")\n+\t      (const_string \"fop\"))))\n+   (set_attr \"prefix\" \"orig,maybe_vex\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n (define_insn \"*fop_<mode>_comm_mixed\"\n   [(set (match_operand:MODEF 0 \"register_operand\" \"=f,x\")\n \t(match_operator:MODEF 3 \"binary_fp_operator\"\n@@ -16032,6 +16273,22 @@\n \t      (const_string \"fop\"))))\n    (set_attr \"mode\" \"<MODE>\")])\n \n+(define_insn \"*fop_<mode>_comm_avx\"\n+  [(set (match_operand:MODEF 0 \"register_operand\" \"=x\")\n+\t(match_operator:MODEF 3 \"binary_fp_operator\"\n+\t  [(match_operand:MODEF 1 \"nonimmediate_operand\" \"%x\")\n+\t   (match_operand:MODEF 2 \"nonimmediate_operand\" \"xm\")]))]\n+  \"AVX_FLOAT_MODE_P (<MODE>mode) && TARGET_SSE_MATH\n+   && COMMUTATIVE_ARITH_P (operands[3])\n+   && !(MEM_P (operands[1]) && MEM_P (operands[2]))\"\n+  \"* return output_387_binary_op (insn, operands);\"\n+  [(set (attr \"type\")\n+        (if_then_else (match_operand:MODEF 3 \"mult_operator\" \"\")\n+\t   (const_string \"ssemul\")\n+\t   (const_string \"sseadd\")))\n+   (set_attr \"prefix\" \"vex\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n (define_insn \"*fop_<mode>_comm_sse\"\n   [(set (match_operand:MODEF 0 \"register_operand\" \"=x\")\n \t(match_operator:MODEF 3 \"binary_fp_operator\"\n@@ -16062,6 +16319,33 @@\n \t   (const_string \"fop\")))\n    (set_attr \"mode\" \"<MODE>\")])\n \n+(define_insn \"*fop_<mode>_1_mixed_avx\"\n+  [(set (match_operand:MODEF 0 \"register_operand\" \"=f,f,x\")\n+\t(match_operator:MODEF 3 \"binary_fp_operator\"\n+\t  [(match_operand:MODEF 1 \"nonimmediate_operand\" \"0,fm,x\")\n+\t   (match_operand:MODEF 2 \"nonimmediate_operand\" \"fm,0,xm\")]))]\n+  \"AVX_FLOAT_MODE_P (<MODE>mode) && TARGET_MIX_SSE_I387\n+   && !COMMUTATIVE_ARITH_P (operands[3])\n+   && !(MEM_P (operands[1]) && MEM_P (operands[2]))\"\n+  \"* return output_387_binary_op (insn, operands);\"\n+  [(set (attr \"type\")\n+        (cond [(and (eq_attr \"alternative\" \"2\")\n+\t            (match_operand:MODEF 3 \"mult_operator\" \"\"))\n+                 (const_string \"ssemul\")\n+\t       (and (eq_attr \"alternative\" \"2\")\n+\t            (match_operand:MODEF 3 \"div_operator\" \"\"))\n+                 (const_string \"ssediv\")\n+\t       (eq_attr \"alternative\" \"2\")\n+                 (const_string \"sseadd\")\n+\t       (match_operand:MODEF 3 \"mult_operator\" \"\")\n+                 (const_string \"fmul\")\n+               (match_operand:MODEF 3 \"div_operator\" \"\")\n+                 (const_string \"fdiv\")\n+              ]\n+              (const_string \"fop\")))\n+   (set_attr \"prefix\" \"orig,orig,maybe_vex\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n (define_insn \"*fop_<mode>_1_mixed\"\n   [(set (match_operand:MODEF 0 \"register_operand\" \"=f,f,x\")\n \t(match_operator:MODEF 3 \"binary_fp_operator\"\n@@ -16093,10 +16377,29 @@\n \t(unspec:SF [(match_operand:SF 1 \"nonimmediate_operand\" \"xm\")]\n \t\t   UNSPEC_RCP))]\n   \"TARGET_SSE_MATH\"\n-  \"rcpss\\t{%1, %0|%0, %1}\"\n+  \"%vrcpss\\t{%1, %d0|%d0, %1}\"\n   [(set_attr \"type\" \"sse\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set_attr \"mode\" \"SF\")])\n \n+(define_insn \"*fop_<mode>_1_avx\"\n+  [(set (match_operand:MODEF 0 \"register_operand\" \"=x\")\n+\t(match_operator:MODEF 3 \"binary_fp_operator\"\n+\t  [(match_operand:MODEF 1 \"register_operand\" \"x\")\n+\t   (match_operand:MODEF 2 \"nonimmediate_operand\" \"xm\")]))]\n+  \"AVX_FLOAT_MODE_P (<MODE>mode) && TARGET_SSE_MATH\n+   && !COMMUTATIVE_ARITH_P (operands[3])\"\n+  \"* return output_387_binary_op (insn, operands);\"\n+  [(set (attr \"type\")\n+        (cond [(match_operand:MODEF 3 \"mult_operator\" \"\")\n+                 (const_string \"ssemul\")\n+\t       (match_operand:MODEF 3 \"div_operator\" \"\")\n+                 (const_string \"ssediv\")\n+              ]\n+              (const_string \"sseadd\")))\n+   (set_attr \"prefix\" \"vex\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n (define_insn \"*fop_<mode>_1_sse\"\n   [(set (match_operand:MODEF 0 \"register_operand\" \"=x\")\n \t(match_operator:MODEF 3 \"binary_fp_operator\"\n@@ -16425,8 +16728,9 @@\n \t(unspec:SF [(match_operand:SF 1 \"nonimmediate_operand\" \"xm\")]\n \t\t   UNSPEC_RSQRT))]\n   \"TARGET_SSE_MATH\"\n-  \"rsqrtss\\t{%1, %0|%0, %1}\"\n+  \"%vrsqrtss\\t{%1, %d0|%d0, %1}\"\n   [(set_attr \"type\" \"sse\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set_attr \"mode\" \"SF\")])\n \n (define_expand \"rsqrtsf2\"\n@@ -16444,8 +16748,9 @@\n \t(sqrt:MODEF\n \t  (match_operand:MODEF 1 \"nonimmediate_operand\" \"xm\")))]\n   \"SSE_FLOAT_MODE_P (<MODE>mode) && TARGET_SSE_MATH\"\n-  \"sqrts<ssemodefsuffix>\\t{%1, %0|%0, %1}\"\n+  \"%vsqrts<ssemodefsuffix>\\t{%1, %d0|%d0, %1}\"\n   [(set_attr \"type\" \"sse\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set_attr \"mode\" \"<MODE>\")\n    (set_attr \"athlon_decode\" \"*\")\n    (set_attr \"amdfam10_decode\" \"*\")])\n@@ -17588,9 +17893,10 @@\n \t\t       (match_operand:SI 2 \"const_0_to_15_operand\" \"n\")]\n \t\t      UNSPEC_ROUND))]\n   \"TARGET_ROUND\"\n-  \"rounds<ssemodefsuffix>\\t{%2, %1, %0|%0, %1, %2}\"\n+  \"%vrounds<ssemodefsuffix>\\t{%2, %1, %d0|%d0, %1, %2}\"\n   [(set_attr \"type\" \"ssecvt\")\n    (set_attr \"prefix_extra\" \"1\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set_attr \"mode\" \"<MODE>\")])\n \n (define_insn \"rintxf2\"\n@@ -19691,6 +19997,17 @@\n ;; Since both the tree-level MAX_EXPR and the rtl-level SMAX operator\n ;; are undefined in this condition, we're certain this is correct.\n \n+(define_insn \"*avx_<code><mode>3\"\n+  [(set (match_operand:MODEF 0 \"register_operand\" \"=x\")\n+\t(smaxmin:MODEF\n+\t  (match_operand:MODEF 1 \"nonimmediate_operand\" \"%x\")\n+\t  (match_operand:MODEF 2 \"nonimmediate_operand\" \"xm\")))]\n+  \"AVX_FLOAT_MODE_P (<MODE>mode) && TARGET_SSE_MATH\"\n+  \"v<maxminfprefix>s<ssemodefsuffix>\\t{%2, %1, %0|%0, %1, %2}\"\n+  [(set_attr \"type\" \"sseadd\")\n+   (set_attr \"prefix\" \"vex\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n (define_insn \"<code><mode>3\"\n   [(set (match_operand:MODEF 0 \"register_operand\" \"=x\")\n \t(smaxmin:MODEF\n@@ -19707,6 +20024,18 @@\n ;; Their operands are not commutative, and thus they may be used in the\n ;; presence of -0.0 and NaN.\n \n+(define_insn \"*avx_ieee_smin<mode>3\"\n+  [(set (match_operand:MODEF 0 \"register_operand\" \"=x\")\n+\t(unspec:MODEF\n+\t  [(match_operand:MODEF 1 \"register_operand\" \"x\")\n+\t   (match_operand:MODEF 2 \"nonimmediate_operand\" \"xm\")]\n+\t UNSPEC_IEEE_MIN))]\n+  \"AVX_FLOAT_MODE_P (<MODE>mode) && TARGET_SSE_MATH\"\n+  \"vmins<ssemodefsuffix>\\t{%2, %1, %0|%0, %1, %2}\"\n+  [(set_attr \"type\" \"sseadd\")\n+   (set_attr \"prefix\" \"vex\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n (define_insn \"*ieee_smin<mode>3\"\n   [(set (match_operand:MODEF 0 \"register_operand\" \"=x\")\n \t(unspec:MODEF\n@@ -19718,6 +20047,18 @@\n   [(set_attr \"type\" \"sseadd\")\n    (set_attr \"mode\" \"<MODE>\")])\n \n+(define_insn \"*avx_ieee_smax<mode>3\"\n+  [(set (match_operand:MODEF 0 \"register_operand\" \"=x\")\n+\t(unspec:MODEF\n+\t  [(match_operand:MODEF 1 \"register_operand\" \"0\")\n+\t   (match_operand:MODEF 2 \"nonimmediate_operand\" \"xm\")]\n+\t UNSPEC_IEEE_MAX))]\n+  \"AVX_FLOAT_MODE_P (<MODE>mode) && TARGET_SSE_MATH\"\n+  \"vmaxs<ssemodefsuffix>\\t{%2, %1, %0|%0, %1, %2}\"\n+  [(set_attr \"type\" \"sseadd\")\n+   (set_attr \"prefix\" \"vex\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n (define_insn \"*ieee_smax<mode>3\"\n   [(set (match_operand:MODEF 0 \"register_operand\" \"=x\")\n \t(unspec:MODEF\n@@ -21185,6 +21526,10 @@\n   int i;\n   operands[0] = gen_rtx_MEM (Pmode,\n \t\t\t     gen_rtx_PLUS (Pmode, operands[0], operands[4]));\n+  /* VEX instruction with a REX prefix will #UD.  */\n+  if (TARGET_AVX && GET_CODE (XEXP (operands[0], 0)) != PLUS)\n+    gcc_unreachable ();\n+\n   output_asm_insn (\"jmp\\t%A1\", operands);\n   for (i = X86_64_SSE_REGPARM_MAX - 1; i >= INTVAL (operands[2]); i--)\n     {\n@@ -21193,7 +21538,7 @@\n       PUT_MODE (operands[4], TImode);\n       if (GET_CODE (XEXP (operands[0], 0)) != PLUS)\n         output_asm_insn (\"rex\", operands);\n-      output_asm_insn (\"movaps\\t{%5, %4|%4, %5}\", operands);\n+      output_asm_insn (\"%vmovaps\\t{%5, %4|%4, %5}\", operands);\n     }\n   (*targetm.asm_out.internal_label) (asm_out_file, \"L\",\n \t\t\t\t     CODE_LABEL_NUMBER (operands[3]));\n@@ -21202,9 +21547,14 @@\n   [(set_attr \"type\" \"other\")\n    (set_attr \"length_immediate\" \"0\")\n    (set_attr \"length_address\" \"0\")\n-   (set_attr \"length\" \"34\")\n+   (set (attr \"length\")\n+     (if_then_else\n+       (eq (symbol_ref \"TARGET_AVX\") (const_int 0))\n+       (const_string \"34\")\n+       (const_string \"42\")))\n    (set_attr \"memory\" \"store\")\n    (set_attr \"modrm\" \"0\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set_attr \"mode\" \"DI\")])\n \n (define_expand \"prefetch\""}, {"sha": "f362ddeda261d12ad1031998f39d9977ae5ad5c4", "filename": "gcc/config/i386/i386.opt", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fi386.opt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fi386.opt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.opt?ref=95879c728b9a59ae67db022ad370eb66374090f3", "patch": "@@ -299,6 +299,14 @@ mno-sse4\n Target RejectNegative Report InverseMask(ISA_SSE4_1) MaskExists Var(ix86_isa_flags) VarExists Save\n Do not support SSE4.1 and SSE4.2 built-in functions and code generation\n \n+mavx\n+Target Report Mask(ISA_AVX) Var(ix86_isa_flags) VarExists\n+Support MMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2 and AVX built-in functions and code generation\n+\n+mfma\n+Target Report Mask(ISA_FMA) Var(ix86_isa_flags) VarExists\n+Support MMX, SSE, SSE2, SSE3, SSSE3, SSE4.1, SSE4.2, AVX and FMA built-in functions and code generation\n+\n msse4a\n Target Report Mask(ISA_SSE4A) Var(ix86_isa_flags) VarExists Save\n Support MMX, SSE, SSE2, SSE3 and SSE4A built-in functions and code generation"}, {"sha": "16aaf2c07937d8a33e921434822e0c7844a6a362", "filename": "gcc/config/i386/mmx.md", "status": "modified", "additions": 91, "deletions": 5, "changes": 96, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fmmx.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fmmx.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fmmx.md?ref=95879c728b9a59ae67db022ad370eb66374090f3", "patch": "@@ -78,15 +78,45 @@\n     movq\\t{%1, %0|%0, %1}\n     movdq2q\\t{%1, %0|%0, %1}\n     movq2dq\\t{%1, %0|%0, %1}\n-    pxor\\t%0, %0\n-    movq\\t{%1, %0|%0, %1}\n-    movq\\t{%1, %0|%0, %1}\n-    movd\\t{%1, %0|%0, %1}\n-    movd\\t{%1, %0|%0, %1}\"\n+    %vpxor\\t%0, %d0\n+    %vmovq\\t{%1, %0|%0, %1}\n+    %vmovq\\t{%1, %0|%0, %1}\n+    %vmovq\\t{%1, %0|%0, %1}\n+    %vmovq\\t{%1, %0|%0, %1}\"\n   [(set_attr \"type\" \"imov,imov,mmx,mmxmov,mmxmov,ssecvt,ssecvt,sselog1,ssemov,ssemov,ssemov,ssemov\")\n    (set_attr \"unit\" \"*,*,*,*,*,mmx,mmx,*,*,*,*,*\")\n+   (set (attr \"prefix\")\n+     (if_then_else (eq_attr \"alternative\" \"7,8,9,10,11\")\n+       (const_string \"maybe_vex\")\n+       (const_string \"orig\")))\n    (set_attr \"mode\" \"DI\")])\n \n+(define_insn \"*mov<mode>_internal_avx\"\n+  [(set (match_operand:MMXMODEI8 0 \"nonimmediate_operand\"\n+\t\t\t\"=!?y,!?y,m  ,!y ,*Y2,*Y2,*Y2 ,m  ,r  ,m\")\n+\t(match_operand:MMXMODEI8 1 \"vector_move_operand\"\n+\t\t\t\"C   ,!ym,!?y,*Y2,!y ,C  ,*Y2m,*Y2,irm,r\"))]\n+  \"TARGET_AVX\n+   && !(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n+  \"@\n+    pxor\\t%0, %0\n+    movq\\t{%1, %0|%0, %1}\n+    movq\\t{%1, %0|%0, %1}\n+    movdq2q\\t{%1, %0|%0, %1}\n+    movq2dq\\t{%1, %0|%0, %1}\n+    vpxor\\t%0, %0, %0\n+    vmovq\\t{%1, %0|%0, %1}\n+    vmovq\\t{%1, %0|%0, %1}\n+    #\n+    #\"\n+  [(set_attr \"type\" \"mmx,mmxmov,mmxmov,ssecvt,ssecvt,sselog1,ssemov,ssemov,*,*\")\n+   (set_attr \"unit\" \"*,*,*,mmx,mmx,*,*,*,*,*\")\n+   (set (attr \"prefix\")\n+     (if_then_else (eq_attr \"alternative\" \"5,6,7\")\n+       (const_string \"vex\")\n+       (const_string \"orig\")))\n+   (set_attr \"mode\" \"DI,DI,DI,DI,DI,TI,DI,DI,DI,DI\")])\n+\n (define_insn \"*mov<mode>_internal\"\n   [(set (match_operand:MMXMODEI8 0 \"nonimmediate_operand\"\n \t\t\t\"=!?y,!?y,m  ,!y ,*Y2,*Y2,*Y2 ,m  ,*x,*x,*x,m ,r  ,m\")\n@@ -122,6 +152,35 @@\n   DONE;\n })\n \n+(define_insn \"*movv2sf_internal_rex64_avx\"\n+  [(set (match_operand:V2SF 0 \"nonimmediate_operand\"\n+\t\t\t\t\"=rm,r ,!?y,!?y ,m ,!y,Y2,x,x,x,m,r,x\")\n+        (match_operand:V2SF 1 \"vector_move_operand\"\n+\t\t\t\t\"Cr ,m ,C  ,!?ym,!y,Y2,!y,C,x,m,x,x,r\"))]\n+  \"TARGET_64BIT && TARGET_AVX\n+   && !(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n+  \"@\n+    mov{q}\\t{%1, %0|%0, %1}\n+    mov{q}\\t{%1, %0|%0, %1}\n+    pxor\\t%0, %0\n+    movq\\t{%1, %0|%0, %1}\n+    movq\\t{%1, %0|%0, %1}\n+    movdq2q\\t{%1, %0|%0, %1}\n+    movq2dq\\t{%1, %0|%0, %1}\n+    vxorps\\t%0, %0, %0\n+    vmovaps\\t{%1, %0|%0, %1}\n+    vmovlps\\t{%1, %0, %0|%0, %0, %1}\n+    vmovlps\\t{%1, %0|%0, %1}\n+    vmovq\\t{%1, %0|%0, %1}\n+    vmovq\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"imov,imov,mmx,mmxmov,mmxmov,ssecvt,ssecvt,ssemov,sselog1,ssemov,ssemov,ssemov,ssemov\")\n+   (set_attr \"unit\" \"*,*,*,*,*,mmx,mmx,*,*,*,*,*,*\")\n+   (set (attr \"prefix\")\n+     (if_then_else (eq_attr \"alternative\" \"7,8,9,10,11,12\")\n+       (const_string \"vex\")\n+       (const_string \"orig\")))\n+   (set_attr \"mode\" \"DI,DI,DI,DI,DI,DI,DI,V4SF,V4SF,V2SF,V2SF,DI,DI\")])\n+\n (define_insn \"*movv2sf_internal_rex64\"\n   [(set (match_operand:V2SF 0 \"nonimmediate_operand\"\n \t\t\t\t\"=rm,r ,!?y,!?y ,m ,!y,*Y2,x,x,x,m,r,Yi\")\n@@ -147,6 +206,33 @@\n    (set_attr \"unit\" \"*,*,*,*,*,mmx,mmx,*,*,*,*,*,*\")\n    (set_attr \"mode\" \"DI,DI,DI,DI,DI,DI,DI,V4SF,V4SF,V2SF,V2SF,DI,DI\")])\n \n+(define_insn \"*movv2sf_internal_avx\"\n+  [(set (match_operand:V2SF 0 \"nonimmediate_operand\"\n+\t\t\t\"=!?y,!?y ,m  ,!y ,*Y2,*x,*x,*x,m ,r  ,m\")\n+        (match_operand:V2SF 1 \"vector_move_operand\"\n+\t\t\t\"C   ,!?ym,!?y,*Y2,!y ,C ,*x,m ,*x,irm,r\"))]\n+  \"TARGET_AVX\n+   && !(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n+  \"@\n+    pxor\\t%0, %0\n+    movq\\t{%1, %0|%0, %1}\n+    movq\\t{%1, %0|%0, %1}\n+    movdq2q\\t{%1, %0|%0, %1}\n+    movq2dq\\t{%1, %0|%0, %1}\n+    vxorps\\t%0, %0, %0\n+    vmovaps\\t{%1, %0|%0, %1}\n+    vmovlps\\t{%1, %0, %0|%0, %0, %1}\n+    vmovlps\\t{%1, %0|%0, %1}\n+    #\n+    #\"\n+  [(set_attr \"type\" \"mmx,mmxmov,mmxmov,ssecvt,ssecvt,sselog1,ssemov,ssemov,ssemov,*,*\")\n+   (set_attr \"unit\" \"*,*,*,mmx,mmx,*,*,*,*,*,*\")\n+   (set (attr \"prefix\")\n+     (if_then_else (eq_attr \"alternative\" \"5,6,7,8\")\n+       (const_string \"vex\")\n+       (const_string \"orig\")))\n+   (set_attr \"mode\" \"DI,DI,DI,DI,DI,V4SF,V4SF,V2SF,V2SF,DI,DI\")])\n+\n (define_insn \"*movv2sf_internal\"\n   [(set (match_operand:V2SF 0 \"nonimmediate_operand\"\n \t\t\t\"=!?y,!?y ,m  ,!y ,*Y2,*x,*x,*x,m ,r  ,m\")"}, {"sha": "36a1b3a7397f1b0c9aa607994edda339d7d6552c", "filename": "gcc/config/i386/predicates.md", "status": "modified", "additions": 37, "deletions": 0, "changes": 37, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fpredicates.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fpredicates.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fpredicates.md?ref=95879c728b9a59ae67db022ad370eb66374090f3", "patch": "@@ -640,11 +640,31 @@\n   (and (match_code \"const_int\")\n        (match_test \"IN_RANGE (INTVAL (op), 2, 3)\")))\n \n+;; Match 4 to 5.\n+(define_predicate \"const_4_to_5_operand\"\n+  (and (match_code \"const_int\")\n+       (match_test \"IN_RANGE (INTVAL (op), 4, 5)\")))\n+\n ;; Match 4 to 7.\n (define_predicate \"const_4_to_7_operand\"\n   (and (match_code \"const_int\")\n        (match_test \"IN_RANGE (INTVAL (op), 4, 7)\")))\n \n+;; Match 6 to 7.\n+(define_predicate \"const_6_to_7_operand\"\n+  (and (match_code \"const_int\")\n+       (match_test \"IN_RANGE (INTVAL (op), 6, 7)\")))\n+\n+;; Match 8 to 11.\n+(define_predicate \"const_8_to_11_operand\"\n+  (and (match_code \"const_int\")\n+       (match_test \"IN_RANGE (INTVAL (op), 8, 11)\")))\n+\n+;; Match 12 to 15.\n+(define_predicate \"const_12_to_15_operand\"\n+  (and (match_code \"const_int\")\n+       (match_test \"IN_RANGE (INTVAL (op), 12, 15)\")))\n+\n ;; Match exactly one bit in 2-bit mask.\n (define_predicate \"const_pow2_1_to_2_operand\"\n   (and (match_code \"const_int\")\n@@ -914,6 +934,11 @@\n (define_special_predicate \"sse_comparison_operator\"\n   (match_code \"eq,lt,le,unordered,ne,unge,ungt,ordered\"))\n \n+;; Return 1 if OP is a comparison operator that can be issued by\n+;; avx predicate generation instructions\n+(define_predicate \"avx_comparison_float_operator\"\n+  (match_code \"ne,eq,ge,gt,le,lt,unordered,ordered,uneq,unge,ungt,unle,unlt,ltgt\"))\n+\n ;; Return 1 if OP is a comparison operator that can be issued by sse predicate\n ;; generation instructions\n (define_predicate \"sse5_comparison_float_operator\"\n@@ -1057,3 +1082,15 @@\n (define_predicate \"misaligned_operand\"\n   (and (match_code \"mem\")\n        (match_test \"MEM_ALIGN (op) < GET_MODE_ALIGNMENT (mode)\")))\n+\n+;; Return 1 if OP is a vzeroall operation, known to be a PARALLEL.\n+(define_predicate \"vzeroall_operation\"\n+  (match_code \"parallel\")\n+{\n+  int nregs = TARGET_64BIT ? 16 : 8;\n+\n+  if (XVECLEN (op, 0) != nregs + 1)\n+    return 0;\n+\n+  return 1;\n+})"}, {"sha": "208a530d06deccd3996cb7926f8a88dc40351553", "filename": "gcc/config/i386/sse.md", "status": "modified", "additions": 3458, "deletions": 118, "changes": 3576, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fsse.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/95879c728b9a59ae67db022ad370eb66374090f3/gcc%2Fconfig%2Fi386%2Fsse.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fsse.md?ref=95879c728b9a59ae67db022ad370eb66374090f3"}]}