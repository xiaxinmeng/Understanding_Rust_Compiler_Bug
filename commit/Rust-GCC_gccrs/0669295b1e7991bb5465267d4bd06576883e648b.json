{"sha": "0669295b1e7991bb5465267d4bd06576883e648b", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MDY2OTI5NWIxZTc5OTFiYjU0NjUyNjdkNGJkMDY1NzY4ODNlNjQ4Yg==", "commit": {"author": {"name": "Andrew MacLeod", "email": "amacleod@redhat.com", "date": "2011-11-07T20:06:39Z"}, "committer": {"name": "Andrew Macleod", "email": "amacleod@gcc.gnu.org", "date": "2011-11-07T20:06:39Z"}, "message": "atomic_base.h (atomic_thread_fence): Call builtin.\n\n\n2011-11-07  Andrew MacLeod  <amacleod@redhat.com>\n\n\tlibstdc++-v3\n\t* include/bits/atomic_base.h (atomic_thread_fence): Call builtin.\n\t(atomic_signal_fence): Call builtin.\n\t(atomic_flag::test_and_set): Call __atomic_exchange when it is lockfree,\n\totherwise fall back to call __sync_lock_test_and_set.\n\t(atomic_flag::clear): Call __atomic_store when it is lockfree,\n\totherwise fall back to call __sync_lock_release.\n\n\tgcc\n\t* doc/extend.texi: Docuemnt behaviour change for __atomic_exchange and\n\t__atomic_store.\n\t* optabs.c (expand_atomic_exchange): Expand to __sync_lock_test_and_set\n\tonly when originated from that builtin.\n\t(expand_atomic_store): Expand to __sync_lock_release when originated\n\tfrom that builtin.\n\t* builtins.c (expand_builtin_sync_lock_test_and_set): Add flag that\n\texpand_atomic_exchange call originated from here.\n\t(expand_builtin_sync_lock_release): Add flag that expand_atomic_store\n\tcall originated from here.\n\t(expand_builtin_atomic_exchange): Add origination flag.\n\t(expand_builtin_atomic_store): Add origination flag.\n\t* expr.h (expand_atomic_exchange, expand_atomic_store): Add boolean \n\tparameters to indicate implementation fall back options.\n\nFrom-SVN: r181111", "tree": {"sha": "deab25b73551eb04852240c97020eb3e7a1060c6", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/deab25b73551eb04852240c97020eb3e7a1060c6"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/0669295b1e7991bb5465267d4bd06576883e648b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0669295b1e7991bb5465267d4bd06576883e648b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0669295b1e7991bb5465267d4bd06576883e648b", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0669295b1e7991bb5465267d4bd06576883e648b/comments", "author": null, "committer": null, "parents": [{"sha": "fd83db3d51f6379186a012f817d1f1ed003500b0", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fd83db3d51f6379186a012f817d1f1ed003500b0", "html_url": "https://github.com/Rust-GCC/gccrs/commit/fd83db3d51f6379186a012f817d1f1ed003500b0"}], "stats": {"total": 184, "additions": 141, "deletions": 43}, "files": [{"sha": "de26cb51dcea58492966e2306764075629346c54", "filename": "gcc/ChangeLog", "status": "modified", "additions": 17, "deletions": 0, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0669295b1e7991bb5465267d4bd06576883e648b/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0669295b1e7991bb5465267d4bd06576883e648b/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=0669295b1e7991bb5465267d4bd06576883e648b", "patch": "@@ -1,3 +1,20 @@\n+2011-11-07  Andrew MacLeod  <amacleod@redhat.com>\n+\n+\t* doc/extend.texi: Docuemnt behaviour change for __atomic_exchange and\n+\t__atomic_store.\n+\t* optabs.c (expand_atomic_exchange): Expand to __sync_lock_test_and_set\n+\tonly when originated from that builtin.\n+\t(expand_atomic_store): Expand to __sync_lock_release when originated\n+\tfrom that builtin.\n+\t* builtins.c (expand_builtin_sync_lock_test_and_set): Add flag that\n+\texpand_atomic_exchange call originated from here.\n+\t(expand_builtin_sync_lock_release): Add flag that expand_atomic_store\n+\tcall originated from here.\n+\t(expand_builtin_atomic_exchange): Add origination flag.\n+\t(expand_builtin_atomic_store): Add origination flag.\n+\t* expr.h (expand_atomic_exchange, expand_atomic_store): Add boolean \n+\tparameters to indicate implementation fall back options.\n+\n 2011-11-07  Georg-Johann Lay  <avr@gjlay.de>\n \n \t* config/avr/avr.c (output_reload_in_const): Can handle CONSTANT_P"}, {"sha": "205d586fc3328cd4d045cb3f6e2405286cbabeb6", "filename": "gcc/builtins.c", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0669295b1e7991bb5465267d4bd06576883e648b/gcc%2Fbuiltins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0669295b1e7991bb5465267d4bd06576883e648b/gcc%2Fbuiltins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fbuiltins.c?ref=0669295b1e7991bb5465267d4bd06576883e648b", "patch": "@@ -5221,7 +5221,7 @@ expand_builtin_sync_lock_test_and_set (enum machine_mode mode, tree exp,\n   mem = get_builtin_sync_mem (CALL_EXPR_ARG (exp, 0), mode);\n   val = expand_expr_force_mode (CALL_EXPR_ARG (exp, 1), mode);\n \n-  return expand_atomic_exchange (target, mem, val, MEMMODEL_ACQUIRE);\n+  return expand_atomic_exchange (target, mem, val, MEMMODEL_ACQUIRE, true);\n }\n \n /* Expand the __sync_lock_release intrinsic.  EXP is the CALL_EXPR.  */\n@@ -5234,7 +5234,7 @@ expand_builtin_sync_lock_release (enum machine_mode mode, tree exp)\n   /* Expand the operands.  */\n   mem = get_builtin_sync_mem (CALL_EXPR_ARG (exp, 0), mode);\n \n-  expand_atomic_store (mem, const0_rtx, MEMMODEL_RELEASE);\n+  expand_atomic_store (mem, const0_rtx, MEMMODEL_RELEASE, true);\n }\n \n /* Given an integer representing an ``enum memmodel'', verify its\n@@ -5285,7 +5285,7 @@ expand_builtin_atomic_exchange (enum machine_mode mode, tree exp, rtx target)\n   mem = get_builtin_sync_mem (CALL_EXPR_ARG (exp, 0), mode);\n   val = expand_expr_force_mode (CALL_EXPR_ARG (exp, 1), mode);\n \n-  return expand_atomic_exchange (target, mem, val, model);\n+  return expand_atomic_exchange (target, mem, val, model, false);\n }\n \n /* Expand the __atomic_compare_exchange intrinsic:\n@@ -5402,7 +5402,7 @@ expand_builtin_atomic_store (enum machine_mode mode, tree exp)\n   mem = get_builtin_sync_mem (CALL_EXPR_ARG (exp, 0), mode);\n   val = expand_expr_force_mode (CALL_EXPR_ARG (exp, 1), mode);\n \n-  return expand_atomic_store (mem, val, model);\n+  return expand_atomic_store (mem, val, model, false);\n }\n \n /* Expand the __atomic_fetch_XXX intrinsic:"}, {"sha": "c7e8ede9cb06e61281ae5d51c39055b129da1e79", "filename": "gcc/doc/extend.texi", "status": "modified", "additions": 1, "deletions": 7, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0669295b1e7991bb5465267d4bd06576883e648b/gcc%2Fdoc%2Fextend.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0669295b1e7991bb5465267d4bd06576883e648b/gcc%2Fdoc%2Fextend.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Fextend.texi?ref=0669295b1e7991bb5465267d4bd06576883e648b", "patch": "@@ -6910,9 +6910,7 @@ contents of @code{*@var{ptr}} in @code{*@var{ret}}.\n \n @deftypefn {Built-in Function} void __atomic_store_n (@var{type} *ptr, @var{type} val, int memmodel)\n This built-in function implements an atomic store operation.  It writes \n-@code{@var{val}} into @code{*@var{ptr}}.  On targets which are limited,\n-0 may be the only valid value. This mimics the behaviour of\n-@code{__sync_lock_release} on such hardware.\n+@code{@var{val}} into @code{*@var{ptr}}.  \n \n The valid memory model variants are\n @code{__ATOMIC_RELAXED}, @code{__ATOMIC_SEQ_CST}, and @code{__ATOMIC_RELEASE}.\n@@ -6930,10 +6928,6 @@ This built-in function implements an atomic exchange operation.  It writes\n @var{val} into @code{*@var{ptr}}, and returns the previous contents of\n @code{*@var{ptr}}.\n \n-On targets which are limited, a value of 1 may be the only valid value\n-written.  This mimics the behaviour of @code{__sync_lock_test_and_set} on\n-such hardware.\n-\n The valid memory model variants are\n @code{__ATOMIC_RELAXED}, @code{__ATOMIC_SEQ_CST}, @code{__ATOMIC_ACQUIRE},\n @code{__ATOMIC_RELEASE}, and @code{__ATOMIC_ACQ_REL}."}, {"sha": "2cc8152c740346ae84a183c5f67eca3e688a09c8", "filename": "gcc/expr.h", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0669295b1e7991bb5465267d4bd06576883e648b/gcc%2Fexpr.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0669295b1e7991bb5465267d4bd06576883e648b/gcc%2Fexpr.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpr.h?ref=0669295b1e7991bb5465267d4bd06576883e648b", "patch": "@@ -215,9 +215,9 @@ rtx emit_conditional_add (rtx, enum rtx_code, rtx, rtx, enum machine_mode,\n rtx expand_sync_operation (rtx, rtx, enum rtx_code);\n rtx expand_sync_fetch_operation (rtx, rtx, enum rtx_code, bool, rtx);\n \n-rtx expand_atomic_exchange (rtx, rtx, rtx, enum memmodel);\n+rtx expand_atomic_exchange (rtx, rtx, rtx, enum memmodel, bool);\n rtx expand_atomic_load (rtx, rtx, enum memmodel);\n-rtx expand_atomic_store (rtx, rtx, enum memmodel);\n+rtx expand_atomic_store (rtx, rtx, enum memmodel, bool);\n rtx expand_atomic_fetch_op (rtx, rtx, rtx, enum rtx_code, enum memmodel, \n \t\t\t      bool);\n void expand_atomic_thread_fence (enum memmodel);"}, {"sha": "7901b95f6321535ff9f0a25e80376f805ca84a1a", "filename": "gcc/optabs.c", "status": "modified", "additions": 54, "deletions": 24, "changes": 78, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0669295b1e7991bb5465267d4bd06576883e648b/gcc%2Foptabs.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0669295b1e7991bb5465267d4bd06576883e648b/gcc%2Foptabs.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Foptabs.c?ref=0669295b1e7991bb5465267d4bd06576883e648b", "patch": "@@ -7256,10 +7256,13 @@ expand_compare_and_swap_loop (rtx mem, rtx old_reg, rtx new_reg, rtx seq)\n    atomically store VAL in MEM and return the previous value in MEM.\n \n    MEMMODEL is the memory model variant to use.\n-   TARGET is an option place to stick the return value.  */\n+   TARGET is an optional place to stick the return value.  \n+   USE_TEST_AND_SET indicates whether __sync_lock_test_and_set should be used\n+   as a fall back if the atomic_exchange pattern does not exist.  */\n \n rtx\n-expand_atomic_exchange (rtx target, rtx mem, rtx val, enum memmodel model)\n+expand_atomic_exchange (rtx target, rtx mem, rtx val, enum memmodel model,\n+\t\t\tbool use_test_and_set)\t\t\t\n {\n   enum machine_mode mode = GET_MODE (mem);\n   enum insn_code icode;\n@@ -7284,31 +7287,39 @@ expand_atomic_exchange (rtx target, rtx mem, rtx val, enum memmodel model)\n      acquire barrier.  If the pattern exists, and the memory model is stronger\n      than acquire, add a release barrier before the instruction.\n      The barrier is not needed if sync_lock_test_and_set doesn't exist since\n-     it will expand into a compare-and-swap loop.  */\n+     it will expand into a compare-and-swap loop.\n \n-  icode = direct_optab_handler (sync_lock_test_and_set_optab, mode);\n-  last_insn = get_last_insn ();\n-  if ((icode != CODE_FOR_nothing) && (model == MEMMODEL_SEQ_CST || \n-\t\t\t\t      model == MEMMODEL_RELEASE ||\n-\t\t\t\t      model == MEMMODEL_ACQ_REL))\n-    expand_builtin_mem_thread_fence (model);\n+     Some targets have non-compliant test_and_sets, so it would be incorrect\n+     to emit a test_and_set in place of an __atomic_exchange.  The test_and_set\n+     builtin shares this expander since exchange can always replace the\n+     test_and_set.  */\n \n-  if (icode != CODE_FOR_nothing)\n+  if (use_test_and_set)\n     {\n-      struct expand_operand ops[3];\n+      icode = direct_optab_handler (sync_lock_test_and_set_optab, mode);\n+      last_insn = get_last_insn ();\n+      if ((icode != CODE_FOR_nothing) && (model == MEMMODEL_SEQ_CST || \n+\t\t\t\t\t  model == MEMMODEL_RELEASE ||\n+\t\t\t\t\t  model == MEMMODEL_ACQ_REL))\n+\texpand_builtin_mem_thread_fence (model);\n \n-      create_output_operand (&ops[0], target, mode);\n-      create_fixed_operand (&ops[1], mem);\n-      /* VAL may have been promoted to a wider mode.  Shrink it if so.  */\n-      create_convert_operand_to (&ops[2], val, mode, true);\n-      if (maybe_expand_insn (icode, 3, ops))\n-\treturn ops[0].value;\n-    }\n+      if (icode != CODE_FOR_nothing)\n+\t{\n+\t  struct expand_operand ops[3];\n+\n+\t  create_output_operand (&ops[0], target, mode);\n+\t  create_fixed_operand (&ops[1], mem);\n+\t  /* VAL may have been promoted to a wider mode.  Shrink it if so.  */\n+\t  create_convert_operand_to (&ops[2], val, mode, true);\n+\t  if (maybe_expand_insn (icode, 3, ops))\n+\t    return ops[0].value;\n+\t}\n \n-  /* Remove any fence we may have inserted since a compare and swap loop is a\n-     full memory barrier.  */\n-  if (last_insn != get_last_insn ())\n-    delete_insns_since (last_insn);\n+      /* Remove any fence that was inserted since a compare and swap loop is\n+\t already a full memory barrier.  */\n+      if (last_insn != get_last_insn ())\n+\tdelete_insns_since (last_insn);\n+    }\n \n   /* Otherwise, use a compare-and-swap loop for the exchange.  */\n   if (can_compare_and_swap_p (mode))\n@@ -7489,10 +7500,11 @@ expand_atomic_load (rtx target, rtx mem, enum memmodel model)\n /* This function expands the atomic store operation:\n    Atomically store VAL in MEM.\n    MEMMODEL is the memory model variant to use.\n+   USE_RELEASE is true if __sync_lock_release can be used as a fall back.\n    function returns const0_rtx if a pattern was emitted.  */\n \n rtx\n-expand_atomic_store (rtx mem, rtx val, enum memmodel model)\n+expand_atomic_store (rtx mem, rtx val, enum memmodel model, bool use_release)\n {\n   enum machine_mode mode = GET_MODE (mem);\n   enum insn_code icode;\n@@ -7509,12 +7521,30 @@ expand_atomic_store (rtx mem, rtx val, enum memmodel model)\n \treturn const0_rtx;\n     }\n \n+  /* If using __sync_lock_release is a viable alternative, try it.  */\n+  if (use_release)\n+    {\n+      icode = direct_optab_handler (sync_lock_release_optab, mode);\n+      if (icode != CODE_FOR_nothing)\n+\t{\n+\t  create_fixed_operand (&ops[0], mem);\n+\t  create_input_operand (&ops[1], const0_rtx, mode);\n+\t  if (maybe_expand_insn (icode, 2, ops))\n+\t    {\n+\t      /* lock_release is only a release barrier.  */\n+\t      if (model == MEMMODEL_SEQ_CST)\n+\t\texpand_builtin_mem_thread_fence (model);\n+\t      return const0_rtx;\n+\t    }\n+\t}\n+    }\n+\n   /* If the size of the object is greater than word size on this target,\n      a default store will not be atomic, Try a mem_exchange and throw away\n      the result.  If that doesn't work, don't do anything.  */\n   if (GET_MODE_PRECISION(mode) > BITS_PER_WORD)\n     {\n-      rtx target = expand_atomic_exchange (NULL_RTX, mem, val, model);\n+      rtx target = expand_atomic_exchange (NULL_RTX, mem, val, model, false);\n       if (target)\n         return const0_rtx;\n       else"}, {"sha": "f28bc5c4b42054ac783907c7987d5a4b34fdfb5b", "filename": "libstdc++-v3/ChangeLog", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0669295b1e7991bb5465267d4bd06576883e648b/libstdc%2B%2B-v3%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0669295b1e7991bb5465267d4bd06576883e648b/libstdc%2B%2B-v3%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2FChangeLog?ref=0669295b1e7991bb5465267d4bd06576883e648b", "patch": "@@ -1,3 +1,12 @@\n+2011-11-07  Andrew MacLeod  <amacleod@redhat.com>\n+\n+\t* include/bits/atomic_base.h (atomic_thread_fence): Call builtin.\n+\t(atomic_signal_fence): Call builtin.\n+\t(atomic_flag::test_and_set): Call __atomic_exchange when it is lockfree,\n+\totherwise fall back to call __sync_lock_test_and_set.\n+\t(atomic_flag::clear): Call __atomic_store when it is lockfree,\n+\totherwise fall back to call __sync_lock_release.\n+\n 2011-11-07  Rainer Orth  <ro@CeBiTec.Uni-Bielefeld.DE>\n \n \tPR bootstrap/50982"}, {"sha": "e297eb0e6ada9c3fb7a06a8da4e8e0c3701ea2ee", "filename": "libstdc++-v3/include/bits/atomic_base.h", "status": "modified", "additions": 54, "deletions": 6, "changes": 60, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0669295b1e7991bb5465267d4bd06576883e648b/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_base.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0669295b1e7991bb5465267d4bd06576883e648b/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_base.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fbits%2Fatomic_base.h?ref=0669295b1e7991bb5465267d4bd06576883e648b", "patch": "@@ -69,10 +69,16 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n   }\n \n   void\n-  atomic_thread_fence(memory_order) noexcept;\n+  atomic_thread_fence(memory_order __m) noexcept\n+  {\n+    __atomic_thread_fence (__m);\n+  }\n \n   void\n-  atomic_signal_fence(memory_order) noexcept;\n+  atomic_signal_fence(memory_order __m) noexcept\n+  {\n+    __atomic_signal_fence (__m);\n+  }\n \n   /// kill_dependency\n   template<typename _Tp>\n@@ -261,13 +267,35 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n     bool\n     test_and_set(memory_order __m = memory_order_seq_cst) noexcept\n     {\n-      return __atomic_exchange_n(&_M_i, 1, __m);\n+      /* The standard *requires* this to be lock free.  If exchange is not\n+\t always lock free, the resort to the old test_and_set.  */\n+      if (__atomic_always_lock_free (sizeof (_M_i), 0))\n+\treturn __atomic_exchange_n(&_M_i, 1, __m);\n+      else\n+        {\n+\t  /* Sync test and set is only guaranteed to be acquire.  */\n+\t  if (__m == memory_order_seq_cst || __m == memory_order_release\n+\t      || __m == memory_order_acq_rel)\n+\t    atomic_thread_fence (__m);\n+\t  return __sync_lock_test_and_set (&_M_i, 1);\n+\t}\n     }\n \n     bool\n     test_and_set(memory_order __m = memory_order_seq_cst) volatile noexcept\n     {\n-      return __atomic_exchange_n(&_M_i, 1, __m);\n+      /* The standard *requires* this to be lock free.  If exchange is not\n+\t always lock free, the resort to the old test_and_set.  */\n+      if (__atomic_always_lock_free (sizeof (_M_i), 0))\n+\treturn __atomic_exchange_n(&_M_i, 1, __m);\n+      else\n+        {\n+\t  /* Sync test and set is only guaranteed to be acquire.  */\n+\t  if (__m == memory_order_seq_cst || __m == memory_order_release\n+\t      || __m == memory_order_acq_rel)\n+\t    atomic_thread_fence (__m);\n+\t  return __sync_lock_test_and_set (&_M_i, 1);\n+\t}\n     }\n \n     void\n@@ -277,7 +305,17 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       __glibcxx_assert(__m != memory_order_acquire);\n       __glibcxx_assert(__m != memory_order_acq_rel);\n \n-      __atomic_store_n(&_M_i, 0, __m);\n+      /* The standard *requires* this to be lock free.  If store is not always\n+\t lock free, the resort to the old style __sync_lock_release.  */\n+      if (__atomic_always_lock_free (sizeof (_M_i), 0))\n+\t__atomic_store_n(&_M_i, 0, __m);\n+      else\n+        {\n+\t  __sync_lock_release (&_M_i, 0);\n+\t  /* __sync_lock_release is only guaranteed to be a release barrier.  */\n+\t  if (__m == memory_order_seq_cst)\n+\t    atomic_thread_fence (__m);\n+\t}\n     }\n \n     void\n@@ -287,7 +325,17 @@ _GLIBCXX_BEGIN_NAMESPACE_VERSION\n       __glibcxx_assert(__m != memory_order_acquire);\n       __glibcxx_assert(__m != memory_order_acq_rel);\n \n-      __atomic_store_n(&_M_i, 0, __m);\n+      /* The standard *requires* this to be lock free.  If store is not always\n+\t lock free, the resort to the old style __sync_lock_release.  */\n+      if (__atomic_always_lock_free (sizeof (_M_i), 0))\n+\t__atomic_store_n(&_M_i, 0, __m);\n+      else\n+        {\n+\t  __sync_lock_release (&_M_i, 0);\n+\t  /* __sync_lock_release is only guaranteed to be a release barrier.  */\n+\t  if (__m == memory_order_seq_cst)\n+\t    atomic_thread_fence (__m);\n+\t}\n     }\n   };\n "}]}