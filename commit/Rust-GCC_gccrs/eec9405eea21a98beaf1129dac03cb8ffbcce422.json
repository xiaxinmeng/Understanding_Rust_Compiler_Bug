{"sha": "eec9405eea21a98beaf1129dac03cb8ffbcce422", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZWVjOTQwNWVlYTIxYTk4YmVhZjExMjlkYWMwM2NiOGZmYmNjZTQyMg==", "commit": {"author": {"name": "Trevor Smigiel", "email": "Trevor_Smigiel@playstation.sony.com", "date": "2009-05-23T02:28:14Z"}, "committer": {"name": "Trevor Smigiel", "email": "tsmigiel@gcc.gnu.org", "date": "2009-05-23T02:28:14Z"}, "message": "Trevor Smigiel <Trevor_Smigiel@playstation.sony.com>\n\n\t* config/spu/spu-protos.h (aligned_mem_p, spu_valid_mov): Remove.\n\t(spu_split_load, spu_split_store): Change return type to int.\n\t(spu_split_convert): Declare.\n\t* config/spu/predicates.md (spu_mem_operand): Remove.\n\t(spu_mov_operand): Update.\n\t(spu_dest_operand, shiftrt_operator, extend_operator): Define.\n\t* config/spu/spu.c (regno_aligned_for_load): Remove.\n\t(reg_aligned_for_addr, spu_expand_load): Define.\n\t(spu_expand_extv): Reimplement and handle MEM.\n\t(spu_expand_insv): Handle MEM.\n\t(spu_sched_reorder): Handle insn's with length 0.\n\t(spu_legitimate_address_p): Reimplement.\n\t(store_with_one_insn_p): Return TRUE for any mode with size\n\tlarger than 16 bytes.\n\t(address_needs_split): Define.\n\t(spu_expand_mov): Call spu_split_load and spu_split_store for MEM\n\toperands.\n\t(spu_convert_move): Define.\n\t(spu_split_load): Use spu_expand_load and change all MEM's to\n\tTImode.\n\t(spu_split_store): Change all MEM's to TImode.\n\t(spu_init_expanders): Preallocate registers that correspond to\n\tLAST_VIRTUAL_REG+1 and LAST_VIRTUAL_REG+2 and set them with\n\tmark_reg_pointer.\n\t(spu_split_convert): Define.\n\t* config/spu/spu.md (QHSI, QHSDI): New mode iterators.\n\t(_move<mode>, _movdi, _movti): Update predicate and condition.\n\t(load, store): Change to define_split.\n\t(extendqiti2, extendhiti2, extendsiti2, extendditi2): Simplify to\n\textend<mode>ti2.\n\t(zero_extendqiti2, zero_extendhiti2, <v>lshr<mode>3_imm): Define.\n\t(lshr<mode>3, lshr<mode>3_imm, lshr<mode>3_re): Simplify to one\n\tdefine_insn_and_split of lshr<mode>3.\n\t(shrqbybi_<mode>, shrqby_<mode>): Simplify to define_expand.\n\t(<v>ashr<mode>3_imm): Define.\n\t(extv, extzv, insv): Allow MEM operands.\n\t(trunc_shr_ti<mode>, trunc_shr_tidi, shl_ext_<mode>ti,\n\tshl_ext_diti, sext_trunc_lshr_tiqisi, zext_trunc_lshr_tiqisi,\n\tsext_trunc_lshr_tihisi, zext_trunc_lshr_tihisi): Define for combine.\n\t(_spu_convert2): Change to define_insn_and_split and remove the\n\tcorresponding define_peephole2.\n\t(stack_protect_set, stack_protect_test, stack_protect_test_si):\n\tChange predicates to memory_operand.\n\nFrom-SVN: r147814", "tree": {"sha": "16412a85a6b2f3130e4c911f4056b8ff7e99fba3", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/16412a85a6b2f3130e4c911f4056b8ff7e99fba3"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/eec9405eea21a98beaf1129dac03cb8ffbcce422", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/eec9405eea21a98beaf1129dac03cb8ffbcce422", "html_url": "https://github.com/Rust-GCC/gccrs/commit/eec9405eea21a98beaf1129dac03cb8ffbcce422", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/eec9405eea21a98beaf1129dac03cb8ffbcce422/comments", "author": null, "committer": null, "parents": [{"sha": "6cfd7dcfaaffdf13aa2d92de9a77eb7c933f555d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6cfd7dcfaaffdf13aa2d92de9a77eb7c933f555d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6cfd7dcfaaffdf13aa2d92de9a77eb7c933f555d"}], "stats": {"total": 1108, "additions": 663, "deletions": 445}, "files": [{"sha": "7bb5cf33023e796bc275ebf54efa3a55c2541579", "filename": "gcc/ChangeLog", "status": "modified", "additions": 46, "deletions": 0, "changes": 46, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/eec9405eea21a98beaf1129dac03cb8ffbcce422/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/eec9405eea21a98beaf1129dac03cb8ffbcce422/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=eec9405eea21a98beaf1129dac03cb8ffbcce422", "patch": "@@ -1,3 +1,49 @@\n+2009-05-22  Trevor Smigiel <Trevor_Smigiel@playstation.sony.com>\n+\n+\t* config/spu/spu-protos.h (aligned_mem_p, spu_valid_mov): Remove.\n+\t(spu_split_load, spu_split_store): Change return type to int.\n+\t(spu_split_convert): Declare.\n+\t* config/spu/predicates.md (spu_mem_operand): Remove.\n+\t(spu_mov_operand): Update.\n+\t(spu_dest_operand, shiftrt_operator, extend_operator): Define.\n+\t* config/spu/spu.c (regno_aligned_for_load): Remove.\n+\t(reg_aligned_for_addr, spu_expand_load): Define.\n+\t(spu_expand_extv): Reimplement and handle MEM.\n+\t(spu_expand_insv): Handle MEM.\n+\t(spu_sched_reorder): Handle insn's with length 0.\n+\t(spu_legitimate_address_p): Reimplement.\n+\t(store_with_one_insn_p): Return TRUE for any mode with size\n+\tlarger than 16 bytes.\n+\t(address_needs_split): Define.\n+\t(spu_expand_mov): Call spu_split_load and spu_split_store for MEM\n+\toperands.\n+\t(spu_convert_move): Define.\n+\t(spu_split_load): Use spu_expand_load and change all MEM's to\n+\tTImode.\n+\t(spu_split_store): Change all MEM's to TImode.\n+\t(spu_init_expanders): Preallocate registers that correspond to\n+\tLAST_VIRTUAL_REG+1 and LAST_VIRTUAL_REG+2 and set them with\n+\tmark_reg_pointer.\n+\t(spu_split_convert): Define.\n+\t* config/spu/spu.md (QHSI, QHSDI): New mode iterators.\n+\t(_move<mode>, _movdi, _movti): Update predicate and condition.\n+\t(load, store): Change to define_split.\n+\t(extendqiti2, extendhiti2, extendsiti2, extendditi2): Simplify to\n+\textend<mode>ti2.\n+\t(zero_extendqiti2, zero_extendhiti2, <v>lshr<mode>3_imm): Define.\n+\t(lshr<mode>3, lshr<mode>3_imm, lshr<mode>3_re): Simplify to one\n+\tdefine_insn_and_split of lshr<mode>3.\n+\t(shrqbybi_<mode>, shrqby_<mode>): Simplify to define_expand.\n+\t(<v>ashr<mode>3_imm): Define.\n+\t(extv, extzv, insv): Allow MEM operands.\n+\t(trunc_shr_ti<mode>, trunc_shr_tidi, shl_ext_<mode>ti,\n+\tshl_ext_diti, sext_trunc_lshr_tiqisi, zext_trunc_lshr_tiqisi,\n+\tsext_trunc_lshr_tihisi, zext_trunc_lshr_tihisi): Define for combine.\n+\t(_spu_convert2): Change to define_insn_and_split and remove the\n+\tcorresponding define_peephole2.\n+\t(stack_protect_set, stack_protect_test, stack_protect_test_si):\n+\tChange predicates to memory_operand.\n+\n 2009-05-22  Mark Mitchell  <mark@codesourcery.com>\n \n \t* config/arm/thumb2.md: Add 16-bit multiply instructions."}, {"sha": "8c6798d806301d84b019c4b060a9da04a905fd92", "filename": "gcc/config/spu/predicates.md", "status": "modified", "additions": 11, "deletions": 5, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/eec9405eea21a98beaf1129dac03cb8ffbcce422/gcc%2Fconfig%2Fspu%2Fpredicates.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/eec9405eea21a98beaf1129dac03cb8ffbcce422/gcc%2Fconfig%2Fspu%2Fpredicates.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fpredicates.md?ref=eec9405eea21a98beaf1129dac03cb8ffbcce422", "patch": "@@ -39,14 +39,14 @@\n        (ior (not (match_code \"subreg\"))\n             (match_test \"valid_subreg (op)\"))))\n \n-(define_predicate \"spu_mem_operand\"\n-  (and (match_operand 0 \"memory_operand\")\n-       (match_test \"reload_in_progress || reload_completed || aligned_mem_p (op)\")))\n-\n (define_predicate \"spu_mov_operand\"\n-  (ior (match_operand 0 \"spu_mem_operand\")\n+  (ior (match_operand 0 \"memory_operand\")\n        (match_operand 0 \"spu_nonmem_operand\")))\n \n+(define_predicate \"spu_dest_operand\"\n+  (ior (match_operand 0 \"memory_operand\")\n+       (match_operand 0 \"spu_reg_operand\")))\n+\n (define_predicate \"call_operand\"\n   (and (match_code \"mem\")\n        (match_test \"(!TARGET_LARGE_MEM && satisfies_constraint_S (op))\n@@ -114,3 +114,9 @@\n        (and (match_operand 0 \"immediate_operand\")\n \t    (match_test \"exp2_immediate_p (op, mode, 0, 127)\"))))\n \n+(define_predicate \"shiftrt_operator\"\n+  (match_code \"lshiftrt,ashiftrt\"))\n+\n+(define_predicate \"extend_operator\"\n+  (match_code \"sign_extend,zero_extend\"))\n+"}, {"sha": "2dbc646075065061d5203b3caa6a5bd134aacffd", "filename": "gcc/config/spu/spu-protos.h", "status": "modified", "additions": 3, "deletions": 4, "changes": 7, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/eec9405eea21a98beaf1129dac03cb8ffbcce422/gcc%2Fconfig%2Fspu%2Fspu-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/eec9405eea21a98beaf1129dac03cb8ffbcce422/gcc%2Fconfig%2Fspu%2Fspu-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fspu-protos.h?ref=eec9405eea21a98beaf1129dac03cb8ffbcce422", "patch": "@@ -62,11 +62,9 @@ extern void spu_setup_incoming_varargs (int *cum, enum machine_mode mode,\n \t\t\t\t\ttree type, int *pretend_size,\n \t\t\t\t\tint no_rtl);\n extern void spu_conditional_register_usage (void);\n-extern int aligned_mem_p (rtx mem);\n extern int spu_expand_mov (rtx * ops, enum machine_mode mode);\n-extern void spu_split_load (rtx * ops);\n-extern void spu_split_store (rtx * ops);\n-extern int spu_valid_move (rtx * ops);\n+extern int spu_split_load (rtx * ops);\n+extern int spu_split_store (rtx * ops);\n extern int fsmbi_const_p (rtx x);\n extern int cpat_const_p (rtx x, enum machine_mode mode);\n extern rtx gen_cpat_const (rtx * ops);\n@@ -87,6 +85,7 @@ extern void spu_initialize_trampoline (rtx tramp, rtx fnaddr, rtx cxt);\n extern void spu_expand_sign_extend (rtx ops[]);\n extern void spu_expand_vector_init (rtx target, rtx vals);\n extern void spu_init_expanders (void);\n+extern void spu_split_convert (rtx *);\n \n /* spu-c.c */\n extern tree spu_resolve_overloaded_builtin (tree fndecl, void *fnargs);"}, {"sha": "5ee18e53a7e9d26d70efae87ccff4557ff18a3d4", "filename": "gcc/config/spu/spu.c", "status": "modified", "additions": 352, "deletions": 278, "changes": 630, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/eec9405eea21a98beaf1129dac03cb8ffbcce422/gcc%2Fconfig%2Fspu%2Fspu.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/eec9405eea21a98beaf1129dac03cb8ffbcce422/gcc%2Fconfig%2Fspu%2Fspu.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fspu.c?ref=eec9405eea21a98beaf1129dac03cb8ffbcce422", "patch": "@@ -189,9 +189,9 @@ static tree spu_build_builtin_va_list (void);\n static void spu_va_start (tree, rtx);\n static tree spu_gimplify_va_arg_expr (tree valist, tree type,\n \t\t\t\t      gimple_seq * pre_p, gimple_seq * post_p);\n-static int regno_aligned_for_load (int regno);\n static int store_with_one_insn_p (rtx mem);\n static int mem_is_padded_component_ref (rtx x);\n+static int reg_aligned_for_addr (rtx x);\n static bool spu_assemble_integer (rtx x, unsigned int size, int aligned_p);\n static void spu_asm_globalize_label (FILE * file, const char *name);\n static unsigned char spu_rtx_costs (rtx x, int code, int outer_code,\n@@ -211,6 +211,7 @@ static tree spu_builtin_vec_perm (tree, tree *);\n static int spu_sms_res_mii (struct ddg *g);\n static void asm_file_start (void);\n static unsigned int spu_section_type_flags (tree, const char *, int);\n+static rtx spu_expand_load (rtx, rtx, rtx, int);\n \n extern const char *reg_names[];\n \n@@ -582,66 +583,85 @@ adjust_operand (rtx op, HOST_WIDE_INT * start)\n void\n spu_expand_extv (rtx ops[], int unsignedp)\n {\n+  rtx dst = ops[0], src = ops[1];\n   HOST_WIDE_INT width = INTVAL (ops[2]);\n   HOST_WIDE_INT start = INTVAL (ops[3]);\n-  HOST_WIDE_INT src_size, dst_size;\n-  enum machine_mode src_mode, dst_mode;\n-  rtx dst = ops[0], src = ops[1];\n-  rtx s;\n+  HOST_WIDE_INT align_mask;\n+  rtx s0, s1, mask, r0;\n \n-  dst = adjust_operand (ops[0], 0);\n-  dst_mode = GET_MODE (dst);\n-  dst_size = GET_MODE_BITSIZE (GET_MODE (dst));\n-\n-  src = adjust_operand (src, &start);\n-  src_mode = GET_MODE (src);\n-  src_size = GET_MODE_BITSIZE (GET_MODE (src));\n+  gcc_assert (REG_P (dst) && GET_MODE (dst) == TImode);\n \n-  if (start > 0)\n+  if (MEM_P (src))\n     {\n-      s = gen_reg_rtx (src_mode);\n-      switch (src_mode)\n+      /* First, determine if we need 1 TImode load or 2.  We need only 1\n+         if the bits being extracted do not cross the alignment boundary\n+         as determined by the MEM and its address. */\n+\n+      align_mask = -MEM_ALIGN (src);\n+      if ((start & align_mask) == ((start + width - 1) & align_mask))\n \t{\n-\tcase SImode:\n-\t  emit_insn (gen_ashlsi3 (s, src, GEN_INT (start)));\n-\t  break;\n-\tcase DImode:\n-\t  emit_insn (gen_ashldi3 (s, src, GEN_INT (start)));\n-\t  break;\n-\tcase TImode:\n-\t  emit_insn (gen_ashlti3 (s, src, GEN_INT (start)));\n-\t  break;\n-\tdefault:\n-\t  abort ();\n+\t  /* Alignment is sufficient for 1 load. */\n+\t  s0 = gen_reg_rtx (TImode);\n+\t  r0 = spu_expand_load (s0, 0, src, start / 8);\n+\t  start &= 7;\n+\t  if (r0)\n+\t    emit_insn (gen_rotqby_ti (s0, s0, r0));\n \t}\n-      src = s;\n+      else\n+\t{\n+\t  /* Need 2 loads. */\n+\t  s0 = gen_reg_rtx (TImode);\n+\t  s1 = gen_reg_rtx (TImode);\n+\t  r0 = spu_expand_load (s0, s1, src, start / 8);\n+\t  start &= 7;\n+\n+\t  gcc_assert (start + width <= 128);\n+\t  if (r0)\n+\t    {\n+\t      rtx r1 = gen_reg_rtx (SImode);\n+\t      mask = gen_reg_rtx (TImode);\n+\t      emit_move_insn (mask, GEN_INT (-1));\n+\t      emit_insn (gen_rotqby_ti (s0, s0, r0));\n+\t      emit_insn (gen_rotqby_ti (s1, s1, r0));\n+\t      if (GET_CODE (r0) == CONST_INT)\n+\t\tr1 = GEN_INT (INTVAL (r0) & 15);\n+\t      else\n+\t\temit_insn (gen_andsi3 (r1, r0, GEN_INT (15)));\n+\t      emit_insn (gen_shlqby_ti (mask, mask, r1));\n+\t      emit_insn (gen_selb (s0, s1, s0, mask));\n+\t    }\n+\t}\n+\n+    }\n+  else if (GET_CODE (src) == SUBREG)\n+    {\n+      rtx r = SUBREG_REG (src);\n+      gcc_assert (REG_P (r) && SCALAR_INT_MODE_P (GET_MODE (r)));\n+      s0 = gen_reg_rtx (TImode);\n+      if (GET_MODE_SIZE (GET_MODE (r)) < GET_MODE_SIZE (TImode))\n+\temit_insn (gen_rtx_SET (VOIDmode, s0, gen_rtx_ZERO_EXTEND (TImode, r)));\n+      else\n+\temit_move_insn (s0, src);\n+    }\n+  else \n+    {\n+      gcc_assert (REG_P (src) && GET_MODE (src) == TImode);\n+      s0 = gen_reg_rtx (TImode);\n+      emit_move_insn (s0, src);\n     }\n \n-  if (width < src_size)\n+  /* Now s0 is TImode and contains the bits to extract at start. */\n+\n+  if (start)\n+    emit_insn (gen_rotlti3 (s0, s0, GEN_INT (start)));\n+\n+  if (128 - width)\n     {\n-      rtx pat;\n-      int icode;\n-      switch (src_mode)\n-\t{\n-\tcase SImode:\n-\t  icode = unsignedp ? CODE_FOR_lshrsi3 : CODE_FOR_ashrsi3;\n-\t  break;\n-\tcase DImode:\n-\t  icode = unsignedp ? CODE_FOR_lshrdi3 : CODE_FOR_ashrdi3;\n-\t  break;\n-\tcase TImode:\n-\t  icode = unsignedp ? CODE_FOR_lshrti3 : CODE_FOR_ashrti3;\n-\t  break;\n-\tdefault:\n-\t  abort ();\n-\t}\n-      s = gen_reg_rtx (src_mode);\n-      pat = GEN_FCN (icode) (s, src, GEN_INT (src_size - width));\n-      emit_insn (pat);\n-      src = s;\n+      tree c = build_int_cst (NULL_TREE, 128 - width);\n+      s0 = expand_shift (RSHIFT_EXPR, TImode, s0, c, s0, unsignedp);\n     }\n \n-  convert_move (dst, src, unsignedp);\n+  emit_move_insn (dst, s0);\n }\n \n void\n@@ -734,38 +754,41 @@ spu_expand_insv (rtx ops[])\n     }\n   if (GET_CODE (ops[0]) == MEM)\n     {\n-      rtx aligned = gen_reg_rtx (SImode);\n       rtx low = gen_reg_rtx (SImode);\n-      rtx addr = gen_reg_rtx (SImode);\n       rtx rotl = gen_reg_rtx (SImode);\n       rtx mask0 = gen_reg_rtx (TImode);\n+      rtx addr;\n+      rtx addr0;\n+      rtx addr1;\n       rtx mem;\n \n-      emit_move_insn (addr, XEXP (ops[0], 0));\n-      emit_insn (gen_andsi3 (aligned, addr, GEN_INT (-16)));\n+      addr = force_reg (Pmode, XEXP (ops[0], 0));\n+      addr0 = gen_rtx_AND (Pmode, addr, GEN_INT (-16));\n       emit_insn (gen_andsi3 (low, addr, GEN_INT (15)));\n       emit_insn (gen_negsi2 (rotl, low));\n       emit_insn (gen_rotqby_ti (shift_reg, shift_reg, rotl));\n       emit_insn (gen_rotqmby_ti (mask0, mask, rotl));\n-      mem = change_address (ops[0], TImode, aligned);\n+      mem = change_address (ops[0], TImode, addr0);\n       set_mem_alias_set (mem, 0);\n       emit_move_insn (dst, mem);\n       emit_insn (gen_selb (dst, dst, shift_reg, mask0));\n-      emit_move_insn (mem, dst);\n       if (start + width > MEM_ALIGN (ops[0]))\n \t{\n \t  rtx shl = gen_reg_rtx (SImode);\n \t  rtx mask1 = gen_reg_rtx (TImode);\n \t  rtx dst1 = gen_reg_rtx (TImode);\n \t  rtx mem1;\n+\t  addr1 = plus_constant (addr, 16);\n+\t  addr1 = gen_rtx_AND (Pmode, addr1, GEN_INT (-16));\n \t  emit_insn (gen_subsi3 (shl, GEN_INT (16), low));\n \t  emit_insn (gen_shlqby_ti (mask1, mask, shl));\n-\t  mem1 = adjust_address (mem, TImode, 16);\n+\t  mem1 = change_address (ops[0], TImode, addr1);\n \t  set_mem_alias_set (mem1, 0);\n \t  emit_move_insn (dst1, mem1);\n \t  emit_insn (gen_selb (dst1, dst1, shift_reg, mask1));\n \t  emit_move_insn (mem1, dst1);\n \t}\n+      emit_move_insn (mem, dst);\n     }\n   else\n     emit_insn (gen_selb (dst, copy_rtx (dst), shift_reg, mask));\n@@ -2998,7 +3021,7 @@ spu_sched_reorder (FILE *file ATTRIBUTE_UNUSED, int verbose ATTRIBUTE_UNUSED,\n       insn = ready[i];\n       if (INSN_CODE (insn) == -1\n \t  || INSN_CODE (insn) == CODE_FOR_blockage\n-\t  || INSN_CODE (insn) == CODE_FOR__spu_convert)\n+\t  || (INSN_P (insn) && get_attr_length (insn) == 0))\n \t{\n \t  ready[i] = ready[nready - 1];\n \t  ready[nready - 1] = insn;\n@@ -3129,8 +3152,8 @@ spu_sched_adjust_cost (rtx insn, rtx link, rtx dep_insn, int cost)\n       || INSN_CODE (dep_insn) == CODE_FOR_blockage)\n     return 0;\n \n-  if (INSN_CODE (insn) == CODE_FOR__spu_convert\n-      || INSN_CODE (dep_insn) == CODE_FOR__spu_convert)\n+  if ((INSN_P (insn) && get_attr_length (insn) == 0)\n+      || (INSN_P (dep_insn) && get_attr_length (dep_insn) == 0))\n     return 0;\n \n   /* Make sure hbrps are spread out. */\n@@ -3611,44 +3634,36 @@ spu_legitimate_constant_p (rtx x)\n /* Valid address are:\n    - symbol_ref, label_ref, const\n    - reg\n-   - reg + const, where either reg or const is 16 byte aligned\n+   - reg + const_int, where const_int is 16 byte aligned\n    - reg + reg, alignment doesn't matter\n   The alignment matters in the reg+const case because lqd and stqd\n-  ignore the 4 least significant bits of the const.  (TODO: It might be\n-  preferable to allow any alignment and fix it up when splitting.) */\n-bool\n-spu_legitimate_address_p (enum machine_mode mode ATTRIBUTE_UNUSED,\n+  ignore the 4 least significant bits of the const.  We only care about\n+  16 byte modes because the expand phase will change all smaller MEM\n+  references to TImode.  */\n+static bool\n+spu_legitimate_address_p (enum machine_mode mode,\n \t\t\t  rtx x, bool reg_ok_strict)\n {\n-  if (mode == TImode && GET_CODE (x) == AND\n+  int aligned = GET_MODE_SIZE (mode) >= 16;\n+  if (aligned\n+      && GET_CODE (x) == AND\n       && GET_CODE (XEXP (x, 1)) == CONST_INT\n-      && INTVAL (XEXP (x, 1)) == (HOST_WIDE_INT) -16)\n+      && INTVAL (XEXP (x, 1)) == (HOST_WIDE_INT) - 16)\n     x = XEXP (x, 0);\n   switch (GET_CODE (x))\n     {\n-    case SYMBOL_REF:\n     case LABEL_REF:\n-      return !TARGET_LARGE_MEM;\n-\n+    case SYMBOL_REF:\n     case CONST:\n-      if (!TARGET_LARGE_MEM && GET_CODE (XEXP (x, 0)) == PLUS)\n-\t{\n-\t  rtx sym = XEXP (XEXP (x, 0), 0);\n-\t  rtx cst = XEXP (XEXP (x, 0), 1);\n-\n-\t  /* Accept any symbol_ref + constant, assuming it does not\n-\t     wrap around the local store addressability limit.  */\n-\t  if (GET_CODE (sym) == SYMBOL_REF && GET_CODE (cst) == CONST_INT)\n-\t    return 1;\n-\t}\n-      return 0;\n+      return !TARGET_LARGE_MEM;\n \n     case CONST_INT:\n       return INTVAL (x) >= 0 && INTVAL (x) <= 0x3ffff;\n \n     case SUBREG:\n       x = XEXP (x, 0);\n-      gcc_assert (GET_CODE (x) == REG);\n+      if (REG_P (x))\n+\treturn 0;\n \n     case REG:\n       return INT_REG_OK_FOR_BASE_P (x, reg_ok_strict);\n@@ -3662,29 +3677,25 @@ spu_legitimate_address_p (enum machine_mode mode ATTRIBUTE_UNUSED,\n \t  op0 = XEXP (op0, 0);\n \tif (GET_CODE (op1) == SUBREG)\n \t  op1 = XEXP (op1, 0);\n-\t/* We can't just accept any aligned register because CSE can\n-\t   change it to a register that is not marked aligned and then\n-\t   recog will fail.   So we only accept frame registers because\n-\t   they will only be changed to other frame registers. */\n \tif (GET_CODE (op0) == REG\n \t    && INT_REG_OK_FOR_BASE_P (op0, reg_ok_strict)\n \t    && GET_CODE (op1) == CONST_INT\n \t    && INTVAL (op1) >= -0x2000\n \t    && INTVAL (op1) <= 0x1fff\n-\t    && (regno_aligned_for_load (REGNO (op0)) || (INTVAL (op1) & 15) == 0))\n-\t  return 1;\n+\t    && (!aligned || (INTVAL (op1) & 15) == 0))\n+\t  return TRUE;\n \tif (GET_CODE (op0) == REG\n \t    && INT_REG_OK_FOR_BASE_P (op0, reg_ok_strict)\n \t    && GET_CODE (op1) == REG\n \t    && INT_REG_OK_FOR_INDEX_P (op1, reg_ok_strict))\n-\t  return 1;\n+\t  return TRUE;\n       }\n       break;\n \n     default:\n       break;\n     }\n-  return 0;\n+  return FALSE;\n }\n \n /* When the address is reg + const_int, force the const_int into a\n@@ -4137,60 +4148,14 @@ spu_conditional_register_usage (void)\n     }\n }\n \n-/* This is called to decide when we can simplify a load instruction.  We\n-   must only return true for registers which we know will always be\n-   aligned.  Taking into account that CSE might replace this reg with\n-   another one that has not been marked aligned.  \n-   So this is really only true for frame, stack and virtual registers,\n-   which we know are always aligned and should not be adversely effected\n-   by CSE.  */\n+/* This is called any time we inspect the alignment of a register for\n+   addresses.  */\n static int\n-regno_aligned_for_load (int regno)\n+reg_aligned_for_addr (rtx x)\n {\n-  return regno == FRAME_POINTER_REGNUM\n-    || (frame_pointer_needed && regno == HARD_FRAME_POINTER_REGNUM)\n-    || regno == ARG_POINTER_REGNUM\n-    || regno == STACK_POINTER_REGNUM\n-    || (regno >= FIRST_VIRTUAL_REGISTER \n-\t&& regno <= LAST_VIRTUAL_REGISTER);\n-}\n-\n-/* Return TRUE when mem is known to be 16-byte aligned. */\n-int\n-aligned_mem_p (rtx mem)\n-{\n-  if (MEM_ALIGN (mem) >= 128)\n-    return 1;\n-  if (GET_MODE_SIZE (GET_MODE (mem)) >= 16)\n-    return 1;\n-  if (GET_CODE (XEXP (mem, 0)) == PLUS)\n-    {\n-      rtx p0 = XEXP (XEXP (mem, 0), 0);\n-      rtx p1 = XEXP (XEXP (mem, 0), 1);\n-      if (regno_aligned_for_load (REGNO (p0)))\n-\t{\n-\t  if (GET_CODE (p1) == REG && regno_aligned_for_load (REGNO (p1)))\n-\t    return 1;\n-\t  if (GET_CODE (p1) == CONST_INT && (INTVAL (p1) & 15) == 0)\n-\t    return 1;\n-\t}\n-    }\n-  else if (GET_CODE (XEXP (mem, 0)) == REG)\n-    {\n-      if (regno_aligned_for_load (REGNO (XEXP (mem, 0))))\n-\treturn 1;\n-    }\n-  else if (ALIGNED_SYMBOL_REF_P (XEXP (mem, 0)))\n-    return 1;\n-  else if (GET_CODE (XEXP (mem, 0)) == CONST)\n-    {\n-      rtx p0 = XEXP (XEXP (XEXP (mem, 0), 0), 0);\n-      rtx p1 = XEXP (XEXP (XEXP (mem, 0), 0), 1);\n-      if (GET_CODE (p0) == SYMBOL_REF\n-\t  && GET_CODE (p1) == CONST_INT && (INTVAL (p1) & 15) == 0)\n-\treturn 1;\n-    }\n-  return 0;\n+  int regno =\n+    REGNO (x) < FIRST_PSEUDO_REGISTER ? ORIGINAL_REGNO (x) : REGNO (x);\n+  return REGNO_POINTER_ALIGN (regno) >= 128;\n }\n \n /* Encode symbol attributes (local vs. global, tls model) of a SYMBOL_REF\n@@ -4219,9 +4184,12 @@ spu_encode_section_info (tree decl, rtx rtl, int first)\n static int\n store_with_one_insn_p (rtx mem)\n {\n+  enum machine_mode mode = GET_MODE (mem);\n   rtx addr = XEXP (mem, 0);\n-  if (GET_MODE (mem) == BLKmode)\n+  if (mode == BLKmode)\n     return 0;\n+  if (GET_MODE_SIZE (mode) >= 16)\n+    return 1;\n   /* Only static objects. */\n   if (GET_CODE (addr) == SYMBOL_REF)\n     {\n@@ -4245,6 +4213,22 @@ store_with_one_insn_p (rtx mem)\n   return 0;\n }\n \n+/* Return 1 when the address is not valid for a simple load and store as\n+   required by the '_mov*' patterns.   We could make this less strict\n+   for loads, but we prefer mem's to look the same so they are more\n+   likely to be merged.  */\n+static int\n+address_needs_split (rtx mem)\n+{\n+  if (GET_MODE_SIZE (GET_MODE (mem)) < 16\n+      && (GET_MODE_SIZE (GET_MODE (mem)) < 4\n+\t  || !(store_with_one_insn_p (mem)\n+\t       || mem_is_padded_component_ref (mem))))\n+    return 1;\n+\n+  return 0;\n+}\n+\n int\n spu_expand_mov (rtx * ops, enum machine_mode mode)\n {\n@@ -4289,54 +4273,63 @@ spu_expand_mov (rtx * ops, enum machine_mode mode)\n \treturn spu_split_immediate (ops);\n       return 0;\n     }\n-  else\n+\n+  /* Catch the SImode immediates greater than 0x7fffffff, and sign\n+     extend them. */\n+  if (GET_CODE (ops[1]) == CONST_INT)\n     {\n-      if (GET_CODE (ops[0]) == MEM)\n-\t{\n-\t  if (!spu_valid_move (ops))\n-\t    {\n-\t      emit_insn (gen_store (ops[0], ops[1], gen_reg_rtx (TImode),\n-\t\t\t\t    gen_reg_rtx (TImode)));\n-\t      return 1;\n-\t    }\n-\t}\n-      else if (GET_CODE (ops[1]) == MEM)\n+      HOST_WIDE_INT val = trunc_int_for_mode (INTVAL (ops[1]), mode);\n+      if (val != INTVAL (ops[1]))\n \t{\n-\t  if (!spu_valid_move (ops))\n-\t    {\n-\t      emit_insn (gen_load\n-\t\t\t (ops[0], ops[1], gen_reg_rtx (TImode),\n-\t\t\t  gen_reg_rtx (SImode)));\n-\t      return 1;\n-\t    }\n-\t}\n-      /* Catch the SImode immediates greater than 0x7fffffff, and sign\n-         extend them. */\n-      if (GET_CODE (ops[1]) == CONST_INT)\n-\t{\n-\t  HOST_WIDE_INT val = trunc_int_for_mode (INTVAL (ops[1]), mode);\n-\t  if (val != INTVAL (ops[1]))\n-\t    {\n-\t      emit_move_insn (ops[0], GEN_INT (val));\n-\t      return 1;\n-\t    }\n+\t  emit_move_insn (ops[0], GEN_INT (val));\n+\t  return 1;\n \t}\n     }\n+  if (MEM_P (ops[0]))\n+    return spu_split_store (ops);\n+  if (MEM_P (ops[1]))\n+    return spu_split_load (ops);\n+\n   return 0;\n }\n \n-void\n-spu_split_load (rtx * ops)\n+static void\n+spu_convert_move (rtx dst, rtx src)\n {\n-  enum machine_mode mode = GET_MODE (ops[0]);\n-  rtx addr, load, rot, mem, p0, p1;\n-  int rot_amt;\n+  enum machine_mode mode = GET_MODE (dst);\n+  enum machine_mode int_mode = mode_for_size (GET_MODE_BITSIZE (mode), MODE_INT, 0);\n+  rtx reg;\n+  gcc_assert (GET_MODE (src) == TImode);\n+  reg = int_mode != mode ? gen_reg_rtx (int_mode) : dst;\n+  emit_insn (gen_rtx_SET (VOIDmode, reg,\n+\t       gen_rtx_TRUNCATE (int_mode,\n+\t\t gen_rtx_LSHIFTRT (TImode, src,\n+\t\t   GEN_INT (int_mode == DImode ? 64 : 96)))));\n+  if (int_mode != mode)\n+    {\n+      reg = simplify_gen_subreg (mode, reg, int_mode, 0);\n+      emit_move_insn (dst, reg);\n+    }\n+}\n \n-  addr = XEXP (ops[1], 0);\n+/* Load TImode values into DST0 and DST1 (when it is non-NULL) using\n+   the address from SRC and SRC+16.  Return a REG or CONST_INT that \n+   specifies how many bytes to rotate the loaded registers, plus any\n+   extra from EXTRA_ROTQBY.  The address and rotate amounts are\n+   normalized to improve merging of loads and rotate computations. */\n+static rtx\n+spu_expand_load (rtx dst0, rtx dst1, rtx src, int extra_rotby)\n+{\n+  rtx addr = XEXP (src, 0);\n+  rtx p0, p1, rot, addr0, addr1;\n+  int rot_amt;\n \n   rot = 0;\n   rot_amt = 0;\n-  if (GET_CODE (addr) == PLUS)\n+\n+  if (MEM_ALIGN (src) >= 128)\n+    /* Address is already aligned; simply perform a TImode load.  */ ;\n+  else if (GET_CODE (addr) == PLUS)\n     {\n       /* 8 cases:\n          aligned reg   + aligned reg     => lqx\n@@ -4350,12 +4343,34 @@ spu_split_load (rtx * ops)\n        */\n       p0 = XEXP (addr, 0);\n       p1 = XEXP (addr, 1);\n-      if (REG_P (p0) && !regno_aligned_for_load (REGNO (p0)))\n+      if (!reg_aligned_for_addr (p0))\n \t{\n-\t  if (REG_P (p1) && !regno_aligned_for_load (REGNO (p1)))\n+\t  if (REG_P (p1) && !reg_aligned_for_addr (p1))\n \t    {\n-\t      emit_insn (gen_addsi3 (ops[3], p0, p1));\n-\t      rot = ops[3];\n+\t      rot = gen_reg_rtx (SImode);\n+\t      emit_insn (gen_addsi3 (rot, p0, p1));\n+\t    }\n+\t  else if (GET_CODE (p1) == CONST_INT && (INTVAL (p1) & 15))\n+\t    {\n+\t      if (INTVAL (p1) > 0\n+\t\t  && REG_POINTER (p0)\n+\t\t  && INTVAL (p1) * BITS_PER_UNIT\n+\t\t     < REGNO_POINTER_ALIGN (REGNO (p0)))\n+\t\t{\n+\t\t  rot = gen_reg_rtx (SImode);\n+\t\t  emit_insn (gen_addsi3 (rot, p0, p1));\n+\t\t  addr = p0;\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  rtx x = gen_reg_rtx (SImode);\n+\t\t  emit_move_insn (x, p1);\n+\t\t  if (!spu_arith_operand (p1, SImode))\n+\t\t    p1 = x;\n+\t\t  rot = gen_reg_rtx (SImode);\n+\t\t  emit_insn (gen_addsi3 (rot, p0, p1));\n+\t\t  addr = gen_rtx_PLUS (Pmode, p0, x);\n+\t\t}\n \t    }\n \t  else\n \t    rot = p0;\n@@ -4365,16 +4380,21 @@ spu_split_load (rtx * ops)\n \t  if (GET_CODE (p1) == CONST_INT && (INTVAL (p1) & 15))\n \t    {\n \t      rot_amt = INTVAL (p1) & 15;\n-\t      p1 = GEN_INT (INTVAL (p1) & -16);\n-\t      addr = gen_rtx_PLUS (SImode, p0, p1);\n+\t      if (INTVAL (p1) & -16)\n+\t\t{\n+\t\t  p1 = GEN_INT (INTVAL (p1) & -16);\n+\t\t  addr = gen_rtx_PLUS (SImode, p0, p1);\n+\t\t}\n+\t      else\n+\t\taddr = p0;\n \t    }\n-\t  else if (REG_P (p1) && !regno_aligned_for_load (REGNO (p1)))\n+\t  else if (REG_P (p1) && !reg_aligned_for_addr (p1))\n \t    rot = p1;\n \t}\n     }\n-  else if (GET_CODE (addr) == REG)\n+  else if (REG_P (addr))\n     {\n-      if (!regno_aligned_for_load (REGNO (addr)))\n+      if (!reg_aligned_for_addr (addr))\n \trot = addr;\n     }\n   else if (GET_CODE (addr) == CONST)\n@@ -4393,57 +4413,107 @@ spu_split_load (rtx * ops)\n \t    addr = XEXP (XEXP (addr, 0), 0);\n \t}\n       else\n-\trot = addr;\n+\t{\n+\t  rot = gen_reg_rtx (Pmode);\n+\t  emit_move_insn (rot, addr);\n+\t}\n     }\n   else if (GET_CODE (addr) == CONST_INT)\n     {\n       rot_amt = INTVAL (addr);\n       addr = GEN_INT (rot_amt & -16);\n     }\n   else if (!ALIGNED_SYMBOL_REF_P (addr))\n-    rot = addr;\n+    {\n+      rot = gen_reg_rtx (Pmode);\n+      emit_move_insn (rot, addr);\n+    }\n \n-  if (GET_MODE_SIZE (mode) < 4)\n-    rot_amt += GET_MODE_SIZE (mode) - 4;\n+  rot_amt += extra_rotby;\n \n   rot_amt &= 15;\n \n   if (rot && rot_amt)\n     {\n-      emit_insn (gen_addsi3 (ops[3], rot, GEN_INT (rot_amt)));\n-      rot = ops[3];\n+      rtx x = gen_reg_rtx (SImode);\n+      emit_insn (gen_addsi3 (x, rot, GEN_INT (rot_amt)));\n+      rot = x;\n       rot_amt = 0;\n     }\n+  if (!rot && rot_amt)\n+    rot = GEN_INT (rot_amt);\n+\n+  addr0 = copy_rtx (addr);\n+  addr0 = gen_rtx_AND (SImode, copy_rtx (addr), GEN_INT (-16));\n+  emit_insn (gen__movti (dst0, change_address (src, TImode, addr0)));\n+\n+  if (dst1)\n+    {\n+      addr1 = plus_constant (copy_rtx (addr), 16);\n+      addr1 = gen_rtx_AND (SImode, addr1, GEN_INT (-16));\n+      emit_insn (gen__movti (dst1, change_address (src, TImode, addr1)));\n+    }\n \n-  load = ops[2];\n+  return rot;\n+}\n+\n+int\n+spu_split_load (rtx * ops)\n+{\n+  enum machine_mode mode = GET_MODE (ops[0]);\n+  rtx addr, load, rot;\n+  int rot_amt;\n \n-  addr = gen_rtx_AND (SImode, copy_rtx (addr), GEN_INT (-16));\n-  mem = change_address (ops[1], TImode, addr);\n+  if (GET_MODE_SIZE (mode) >= 16)\n+    return 0;\n \n-  emit_insn (gen_movti (load, mem));\n+  addr = XEXP (ops[1], 0);\n+  gcc_assert (GET_CODE (addr) != AND);\n+\n+  if (!address_needs_split (ops[1]))\n+    {\n+      ops[1] = change_address (ops[1], TImode, addr);\n+      load = gen_reg_rtx (TImode);\n+      emit_insn (gen__movti (load, ops[1]));\n+      spu_convert_move (ops[0], load);\n+      return 1;\n+    }\n+\n+  rot_amt = GET_MODE_SIZE (mode) < 4 ? GET_MODE_SIZE (mode) - 4 : 0;\n+\n+  load = gen_reg_rtx (TImode);\n+  rot = spu_expand_load (load, 0, ops[1], rot_amt);\n \n   if (rot)\n     emit_insn (gen_rotqby_ti (load, load, rot));\n-  else if (rot_amt)\n-    emit_insn (gen_rotlti3 (load, load, GEN_INT (rot_amt * 8)));\n \n-  if (reload_completed)\n-    emit_move_insn (ops[0], gen_rtx_REG (GET_MODE (ops[0]), REGNO (load)));\n-  else\n-    emit_insn (gen_spu_convert (ops[0], load));\n+  spu_convert_move (ops[0], load);\n+  return 1;\n }\n \n-void\n+int\n spu_split_store (rtx * ops)\n {\n   enum machine_mode mode = GET_MODE (ops[0]);\n-  rtx pat = ops[2];\n-  rtx reg = ops[3];\n+  rtx reg;\n   rtx addr, p0, p1, p1_lo, smem;\n   int aform;\n   int scalar;\n \n+  if (GET_MODE_SIZE (mode) >= 16)\n+    return 0;\n+\n   addr = XEXP (ops[0], 0);\n+  gcc_assert (GET_CODE (addr) != AND);\n+\n+  if (!address_needs_split (ops[0]))\n+    {\n+      reg = gen_reg_rtx (TImode);\n+      emit_insn (gen_spu_convert (reg, ops[1]));\n+      ops[0] = change_address (ops[0], TImode, addr);\n+      emit_move_insn (ops[0], reg);\n+      return 1;\n+    }\n \n   if (GET_CODE (addr) == PLUS)\n     {\n@@ -4455,19 +4525,31 @@ spu_split_store (rtx * ops)\n          unaligned reg + aligned reg     => lqx, c?x, shuf, stqx\n          unaligned reg + unaligned reg   => lqx, c?x, shuf, stqx\n          unaligned reg + aligned const   => lqd, c?d, shuf, stqx\n-         unaligned reg + unaligned const -> not allowed by legitimate address\n+         unaligned reg + unaligned const -> lqx, c?d, shuf, stqx\n        */\n       aform = 0;\n       p0 = XEXP (addr, 0);\n       p1 = p1_lo = XEXP (addr, 1);\n-      if (GET_CODE (p0) == REG && GET_CODE (p1) == CONST_INT)\n+      if (REG_P (p0) && GET_CODE (p1) == CONST_INT)\n \t{\n \t  p1_lo = GEN_INT (INTVAL (p1) & 15);\n-\t  p1 = GEN_INT (INTVAL (p1) & -16);\n-\t  addr = gen_rtx_PLUS (SImode, p0, p1);\n+\t  if (reg_aligned_for_addr (p0))\n+\t    {\n+\t      p1 = GEN_INT (INTVAL (p1) & -16);\n+\t      if (p1 == const0_rtx)\n+\t\taddr = p0;\n+\t      else\n+\t\taddr = gen_rtx_PLUS (SImode, p0, p1);\n+\t    }\n+\t  else\n+\t    {\n+\t      rtx x = gen_reg_rtx (SImode);\n+\t      emit_move_insn (x, p1);\n+\t      addr = gen_rtx_PLUS (SImode, p0, x);\n+\t    }\n \t}\n     }\n-  else if (GET_CODE (addr) == REG)\n+  else if (REG_P (addr))\n     {\n       aform = 0;\n       p0 = addr;\n@@ -4481,31 +4563,34 @@ spu_split_store (rtx * ops)\n       p1_lo = addr;\n       if (ALIGNED_SYMBOL_REF_P (addr))\n \tp1_lo = const0_rtx;\n-      else if (GET_CODE (addr) == CONST)\n+      else if (GET_CODE (addr) == CONST\n+\t       && GET_CODE (XEXP (addr, 0)) == PLUS\n+\t       && ALIGNED_SYMBOL_REF_P (XEXP (XEXP (addr, 0), 0))\n+\t       && GET_CODE (XEXP (XEXP (addr, 0), 1)) == CONST_INT)\n \t{\n-\t  if (GET_CODE (XEXP (addr, 0)) == PLUS\n-\t      && ALIGNED_SYMBOL_REF_P (XEXP (XEXP (addr, 0), 0))\n-\t      && GET_CODE (XEXP (XEXP (addr, 0), 1)) == CONST_INT)\n-\t    {\n-\t      HOST_WIDE_INT v = INTVAL (XEXP (XEXP (addr, 0), 1));\n-\t      if ((v & -16) != 0)\n-\t\taddr = gen_rtx_CONST (Pmode,\n-\t\t\t\t      gen_rtx_PLUS (Pmode,\n-\t\t\t\t\t\t    XEXP (XEXP (addr, 0), 0),\n-\t\t\t\t\t\t    GEN_INT (v & -16)));\n-\t      else\n-\t\taddr = XEXP (XEXP (addr, 0), 0);\n-\t      p1_lo = GEN_INT (v & 15);\n-\t    }\n+\t  HOST_WIDE_INT v = INTVAL (XEXP (XEXP (addr, 0), 1));\n+\t  if ((v & -16) != 0)\n+\t    addr = gen_rtx_CONST (Pmode,\n+\t\t\t\t  gen_rtx_PLUS (Pmode,\n+\t\t\t\t\t\tXEXP (XEXP (addr, 0), 0),\n+\t\t\t\t\t\tGEN_INT (v & -16)));\n+\t  else\n+\t    addr = XEXP (XEXP (addr, 0), 0);\n+\t  p1_lo = GEN_INT (v & 15);\n \t}\n       else if (GET_CODE (addr) == CONST_INT)\n \t{\n \t  p1_lo = GEN_INT (INTVAL (addr) & 15);\n \t  addr = GEN_INT (INTVAL (addr) & -16);\n \t}\n+      else\n+\t{\n+\t  p1_lo = gen_reg_rtx (SImode);\n+\t  emit_move_insn (p1_lo, addr);\n+\t}\n     }\n \n-  addr = gen_rtx_AND (SImode, copy_rtx (addr), GEN_INT (-16));\n+  reg = gen_reg_rtx (TImode);\n \n   scalar = store_with_one_insn_p (ops[0]);\n   if (!scalar)\n@@ -4515,29 +4600,19 @@ spu_split_store (rtx * ops)\n          possible, and copying the flags will prevent that in certain\n          cases, e.g. consider the volatile flag. */\n \n+      rtx pat = gen_reg_rtx (TImode);\n       rtx lmem = change_address (ops[0], TImode, copy_rtx (addr));\n       set_mem_alias_set (lmem, 0);\n       emit_insn (gen_movti (reg, lmem));\n \n-      if (!p0 || regno_aligned_for_load (REGNO (p0)))\n+      if (!p0 || reg_aligned_for_addr (p0))\n \tp0 = stack_pointer_rtx;\n       if (!p1_lo)\n \tp1_lo = const0_rtx;\n \n       emit_insn (gen_cpat (pat, p0, p1_lo, GEN_INT (GET_MODE_SIZE (mode))));\n       emit_insn (gen_shufb (reg, ops[1], reg, pat));\n     }\n-  else if (reload_completed)\n-    {\n-      if (GET_CODE (ops[1]) == REG)\n-\temit_move_insn (reg, gen_rtx_REG (GET_MODE (reg), REGNO (ops[1])));\n-      else if (GET_CODE (ops[1]) == SUBREG)\n-\temit_move_insn (reg,\n-\t\t\tgen_rtx_REG (GET_MODE (reg),\n-\t\t\t\t     REGNO (SUBREG_REG (ops[1]))));\n-      else\n-\tabort ();\n-    }\n   else\n     {\n       if (GET_CODE (ops[1]) == REG)\n@@ -4549,15 +4624,16 @@ spu_split_store (rtx * ops)\n     }\n \n   if (GET_MODE_SIZE (mode) < 4 && scalar)\n-    emit_insn (gen_shlqby_ti\n-\t       (reg, reg, GEN_INT (4 - GET_MODE_SIZE (mode))));\n+    emit_insn (gen_ashlti3\n+\t       (reg, reg, GEN_INT (32 - GET_MODE_BITSIZE (mode))));\n \n-  smem = change_address (ops[0], TImode, addr);\n+  smem = change_address (ops[0], TImode, copy_rtx (addr));\n   /* We can't use the previous alias set because the memory has changed\n      size and can potentially overlap objects of other types.  */\n   set_mem_alias_set (smem, 0);\n \n   emit_insn (gen_movti (smem, reg));\n+  return 1;\n }\n \n /* Return TRUE if X is MEM which is a struct member reference\n@@ -4656,37 +4732,6 @@ fix_range (const char *const_str)\n     }\n }\n \n-int\n-spu_valid_move (rtx * ops)\n-{\n-  enum machine_mode mode = GET_MODE (ops[0]);\n-  if (!register_operand (ops[0], mode) && !register_operand (ops[1], mode))\n-    return 0;\n-\n-  /* init_expr_once tries to recog against load and store insns to set\n-     the direct_load[] and direct_store[] arrays.  We always want to\n-     consider those loads and stores valid.  init_expr_once is called in\n-     the context of a dummy function which does not have a decl. */\n-  if (cfun->decl == 0)\n-    return 1;\n-\n-  /* Don't allows loads/stores which would require more than 1 insn.\n-     During and after reload we assume loads and stores only take 1\n-     insn. */\n-  if (GET_MODE_SIZE (mode) < 16 && !reload_in_progress && !reload_completed)\n-    {\n-      if (GET_CODE (ops[0]) == MEM\n-\t  && (GET_MODE_SIZE (mode) < 4\n-\t      || !(store_with_one_insn_p (ops[0])\n-\t\t   || mem_is_padded_component_ref (ops[0]))))\n-\treturn 0;\n-      if (GET_CODE (ops[1]) == MEM\n-\t  && (GET_MODE_SIZE (mode) < 4 || !aligned_mem_p (ops[1])))\n-\treturn 0;\n-    }\n-  return 1;\n-}\n-\n /* Return TRUE if x is a CONST_INT, CONST_DOUBLE or CONST_VECTOR that\n    can be generated using the fsmbi instruction. */\n int\n@@ -6400,12 +6445,25 @@ spu_sms_res_mii (struct ddg *g)\n \n void\n spu_init_expanders (void)\n-{   \n-  /* HARD_FRAME_REGISTER is only 128 bit aligned when\n-   * frame_pointer_needed is true.  We don't know that until we're\n-   * expanding the prologue. */\n+{\n   if (cfun)\n-    REGNO_POINTER_ALIGN (HARD_FRAME_POINTER_REGNUM) = 8;\n+    {\n+      rtx r0, r1;\n+      /* HARD_FRAME_REGISTER is only 128 bit aligned when\n+         frame_pointer_needed is true.  We don't know that until we're\n+         expanding the prologue. */\n+      REGNO_POINTER_ALIGN (HARD_FRAME_POINTER_REGNUM) = 8;\n+\n+      /* A number of passes use LAST_VIRTUAL_REGISTER+1 and\n+\t LAST_VIRTUAL_REGISTER+2 to test the back-end.  We want them\n+\t to be treated as aligned, so generate them here. */\n+      r0 = gen_reg_rtx (SImode);\n+      r1 = gen_reg_rtx (SImode);\n+      mark_reg_pointer (r0, 128);\n+      mark_reg_pointer (r1, 128);\n+      gcc_assert (REGNO (r0) == LAST_VIRTUAL_REGISTER + 1\n+\t\t  && REGNO (r1) == LAST_VIRTUAL_REGISTER + 2);\n+    }\n }\n \n static enum machine_mode\n@@ -6480,4 +6538,20 @@ spu_gen_exp2 (enum machine_mode mode, rtx scale)\n     }\n }\n \n+/* After reload, just change the convert into a move instruction\n+   or a dead instruction. */\n+void\n+spu_split_convert (rtx ops[])\n+{\n+  if (REGNO (ops[0]) == REGNO (ops[1]))\n+    emit_note (NOTE_INSN_DELETED);\n+  else\n+    {\n+      /* Use TImode always as this might help hard reg copyprop.  */\n+      rtx op0 = gen_rtx_REG (TImode, REGNO (ops[0]));\n+      rtx op1 = gen_rtx_REG (TImode, REGNO (ops[1]));\n+      emit_insn (gen_move_insn (op0, op1));\n+    }\n+}\n+\n #include \"gt-spu.h\""}, {"sha": "181d0db991c50fbb619acdf9e7a9e8b6d325acc0", "filename": "gcc/config/spu/spu.md", "status": "modified", "additions": 251, "deletions": 158, "changes": 409, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/eec9405eea21a98beaf1129dac03cb8ffbcce422/gcc%2Fconfig%2Fspu%2Fspu.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/eec9405eea21a98beaf1129dac03cb8ffbcce422/gcc%2Fconfig%2Fspu%2Fspu.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fspu%2Fspu.md?ref=eec9405eea21a98beaf1129dac03cb8ffbcce422", "patch": "@@ -178,6 +178,8 @@\n                         SF V4SF\n                         DF V2DF])\n \n+(define_mode_iterator QHSI  [QI HI SI])\n+(define_mode_iterator QHSDI  [QI HI SI DI])\n (define_mode_iterator DTI  [DI TI])\n \n (define_mode_iterator VINT [QI V16QI\n@@ -316,9 +318,10 @@\n ;; move internal\n \n (define_insn \"_mov<mode>\"\n-  [(set (match_operand:MOV 0 \"spu_nonimm_operand\" \"=r,r,r,r,r,m\")\n+  [(set (match_operand:MOV 0 \"spu_dest_operand\" \"=r,r,r,r,r,m\")\n \t(match_operand:MOV 1 \"spu_mov_operand\" \"r,A,f,j,m,r\"))]\n-  \"spu_valid_move (operands)\"\n+  \"register_operand(operands[0], <MODE>mode)\n+   || register_operand(operands[1], <MODE>mode)\"\n   \"@\n    ori\\t%0,%1,0\n    il%s1\\t%0,%S1\n@@ -336,9 +339,10 @@\n   \"iohl\\t%0,%2@l\")\n \n (define_insn \"_movdi\"\n-  [(set (match_operand:DI 0 \"spu_nonimm_operand\" \"=r,r,r,r,r,m\")\n+  [(set (match_operand:DI 0 \"spu_dest_operand\" \"=r,r,r,r,r,m\")\n \t(match_operand:DI 1 \"spu_mov_operand\" \"r,a,f,k,m,r\"))]\n-  \"spu_valid_move (operands)\"\n+  \"register_operand(operands[0], DImode)\n+   || register_operand(operands[1], DImode)\"\n   \"@\n    ori\\t%0,%1,0\n    il%d1\\t%0,%D1\n@@ -349,9 +353,10 @@\n   [(set_attr \"type\" \"fx2,fx2,shuf,shuf,load,store\")])\n \n (define_insn \"_movti\"\n-  [(set (match_operand:TI 0 \"spu_nonimm_operand\" \"=r,r,r,r,r,m\")\n+  [(set (match_operand:TI 0 \"spu_dest_operand\" \"=r,r,r,r,r,m\")\n \t(match_operand:TI 1 \"spu_mov_operand\" \"r,U,f,l,m,r\"))]\n-  \"spu_valid_move (operands)\"\n+  \"register_operand(operands[0], TImode)\n+   || register_operand(operands[1], TImode)\"\n   \"@\n    ori\\t%0,%1,0\n    il%t1\\t%0,%T1\n@@ -361,30 +366,29 @@\n    stq%p0\\t%1,%0\"\n   [(set_attr \"type\" \"fx2,fx2,shuf,shuf,load,store\")])\n \n-(define_insn_and_split \"load\"\n-  [(set (match_operand 0 \"spu_reg_operand\" \"=r\")\n-\t(match_operand 1 \"memory_operand\" \"m\"))\n-   (clobber (match_operand:TI 2 \"spu_reg_operand\" \"=&r\"))\n-   (clobber (match_operand:SI 3 \"spu_reg_operand\" \"=&r\"))]\n-  \"GET_MODE(operands[0]) == GET_MODE(operands[1])\"\n-  \"#\"\n-  \"\"\n+(define_split\n+  [(set (match_operand 0 \"spu_reg_operand\")\n+\t(match_operand 1 \"memory_operand\"))]\n+  \"GET_MODE_SIZE (GET_MODE (operands[0])) < 16\n+   && GET_MODE(operands[0]) == GET_MODE(operands[1])\n+   && !reload_in_progress && !reload_completed\" \n   [(set (match_dup 0)\n \t(match_dup 1))]\n-  { spu_split_load(operands); DONE; })\n+  { if (spu_split_load(operands))\n+      DONE;\n+  })\n \n-(define_insn_and_split \"store\"\n-  [(set (match_operand 0 \"memory_operand\" \"=m\")\n-\t(match_operand 1 \"spu_reg_operand\" \"r\"))\n-   (clobber (match_operand:TI 2 \"spu_reg_operand\" \"=&r\"))\n-   (clobber (match_operand:TI 3 \"spu_reg_operand\" \"=&r\"))]\n-  \"GET_MODE(operands[0]) == GET_MODE(operands[1])\"\n-  \"#\"\n-  \"\"\n+(define_split\n+  [(set (match_operand 0 \"memory_operand\")\n+\t(match_operand 1 \"spu_reg_operand\"))]\n+  \"GET_MODE_SIZE (GET_MODE (operands[0])) < 16\n+   && GET_MODE(operands[0]) == GET_MODE(operands[1])\n+   && !reload_in_progress && !reload_completed\" \n   [(set (match_dup 0)\n \t(match_dup 1))]\n-  { spu_split_store(operands); DONE; })\n-\n+  { if (spu_split_store(operands))\n+      DONE;\n+  })\n ;; Operand 3 is the number of bytes. 1:b 2:h 4:w 8:d\n \n (define_expand \"cpat\"\n@@ -462,33 +466,20 @@\n   \"\"\n   \"xswd\\t%0,%1\");\n \n-(define_expand \"extendqiti2\"\n-  [(set (match_operand:TI 0 \"register_operand\" \"\")\n-\t(sign_extend:TI (match_operand:QI 1 \"register_operand\" \"\")))]\n-  \"\"\n-  \"spu_expand_sign_extend(operands);\n-   DONE;\")\n-\n-(define_expand \"extendhiti2\"\n+;; By splitting this late we don't allow much opportunity for sharing of\n+;; constants.  That's ok because this should really be optimized away.\n+(define_insn_and_split \"extend<mode>ti2\"\n   [(set (match_operand:TI 0 \"register_operand\" \"\")\n-\t(sign_extend:TI (match_operand:HI 1 \"register_operand\" \"\")))]\n+\t(sign_extend:TI (match_operand:QHSDI 1 \"register_operand\" \"\")))]\n   \"\"\n-  \"spu_expand_sign_extend(operands);\n-   DONE;\")\n-\n-(define_expand \"extendsiti2\"\n-  [(set (match_operand:TI 0 \"register_operand\" \"\")\n-\t(sign_extend:TI (match_operand:SI 1 \"register_operand\" \"\")))]\n-  \"\"\n-  \"spu_expand_sign_extend(operands);\n-   DONE;\")\n-\n-(define_expand \"extendditi2\"\n-  [(set (match_operand:TI 0 \"register_operand\" \"\")\n-\t(sign_extend:TI (match_operand:DI 1 \"register_operand\" \"\")))]\n+  \"#\"\n   \"\"\n-  \"spu_expand_sign_extend(operands);\n-   DONE;\")\n+  [(set (match_dup:TI 0)\n+\t(sign_extend:TI (match_dup:QHSDI 1)))]\n+  {\n+    spu_expand_sign_extend(operands);\n+    DONE;\n+  })\n \n \f\n ;; zero_extend\n@@ -525,6 +516,22 @@\n   \"rotqmbyi\\t%0,%1,-4\"\n   [(set_attr \"type\" \"shuf\")])\n \n+(define_insn \"zero_extendqiti2\"\n+  [(set (match_operand:TI 0 \"spu_reg_operand\" \"=r\")\n+\t(zero_extend:TI (match_operand:QI 1 \"spu_reg_operand\" \"r\")))]\n+  \"\"\n+  \"andi\\t%0,%1,0x00ff\\;rotqmbyi\\t%0,%0,-12\"\n+  [(set_attr \"type\" \"multi0\")\n+   (set_attr \"length\" \"8\")])\n+\n+(define_insn \"zero_extendhiti2\"\n+  [(set (match_operand:TI 0 \"spu_reg_operand\" \"=r\")\n+\t(zero_extend:TI (match_operand:HI 1 \"spu_reg_operand\" \"r\")))]\n+  \"\"\n+  \"shli\\t%0,%1,16\\;rotqmbyi\\t%0,%0,-14\"\n+  [(set_attr \"type\" \"multi1\")\n+   (set_attr \"length\" \"8\")])\n+\n (define_insn \"zero_extendsiti2\"\n   [(set (match_operand:TI 0 \"spu_reg_operand\" \"=r\")\n \t(zero_extend:TI (match_operand:SI 1 \"spu_reg_operand\" \"r\")))]\n@@ -2348,6 +2355,13 @@\n   \"\"\n   [(set_attr \"type\" \"*,fx3\")])\n   \n+(define_insn \"<v>lshr<mode>3_imm\"\n+  [(set (match_operand:VHSI 0 \"spu_reg_operand\" \"=r\")\n+\t(lshiftrt:VHSI (match_operand:VHSI 1 \"spu_reg_operand\" \"r\")\n+\t\t       (match_operand:VHSI 2 \"immediate_operand\" \"W\")))]\n+  \"\"\n+  \"rot<bh>mi\\t%0,%1,-%<umask>2\"\n+  [(set_attr \"type\" \"fx3\")])\n \n (define_insn \"rotm_<mode>\"\n   [(set (match_operand:VHSI 0 \"spu_reg_operand\" \"=r,r\")\n@@ -2359,89 +2373,59 @@\n    rot<bh>mi\\t%0,%1,-%<nmask>2\"\n   [(set_attr \"type\" \"fx3\")])\n  \n-(define_expand \"lshr<mode>3\"\n-  [(parallel [(set (match_operand:DTI 0 \"spu_reg_operand\" \"\")\n-\t\t   (lshiftrt:DTI (match_operand:DTI 1 \"spu_reg_operand\" \"\")\n-\t\t\t         (match_operand:SI 2 \"spu_nonmem_operand\" \"\")))\n-\t      (clobber (match_dup:DTI 3))\n-\t      (clobber (match_dup:SI 4))\n-\t      (clobber (match_dup:SI 5))])]\n-  \"\"\n-  \"if (GET_CODE (operands[2]) == CONST_INT)\n-    {\n-      emit_insn (gen_lshr<mode>3_imm(operands[0], operands[1], operands[2]));\n-      DONE;\n-    }\n-   operands[3] = gen_reg_rtx (<MODE>mode);\n-   operands[4] = gen_reg_rtx (SImode);\n-   operands[5] = gen_reg_rtx (SImode);\")\n-\n-(define_insn_and_split \"lshr<mode>3_imm\"\n-  [(set (match_operand:DTI 0 \"spu_reg_operand\" \"=r,r\")\n-\t(lshiftrt:DTI (match_operand:DTI 1 \"spu_reg_operand\" \"r,r\")\n-\t\t      (match_operand:SI 2 \"immediate_operand\" \"O,P\")))]\n+(define_insn_and_split \"lshr<mode>3\"\n+  [(set (match_operand:DTI 0 \"spu_reg_operand\" \"=r,r,r\")\n+\t(lshiftrt:DTI (match_operand:DTI 1 \"spu_reg_operand\" \"r,r,r\")\n+\t\t      (match_operand:SI 2 \"spu_nonmem_operand\" \"r,O,P\")))]\n   \"\"\n   \"@\n+   #\n    rotqmbyi\\t%0,%1,-%h2\n    rotqmbii\\t%0,%1,-%e2\"\n-  \"!satisfies_constraint_O (operands[2]) && !satisfies_constraint_P (operands[2])\"\n-  [(set (match_dup:DTI 0)\n+  \"REG_P (operands[2]) || (!satisfies_constraint_O (operands[2]) && !satisfies_constraint_P (operands[2]))\"\n+  [(set (match_dup:DTI 3)\n \t(lshiftrt:DTI (match_dup:DTI 1)\n \t\t      (match_dup:SI 4)))\n    (set (match_dup:DTI 0)\n-\t(lshiftrt:DTI (match_dup:DTI 0)\n+\t(lshiftrt:DTI (match_dup:DTI 3)\n \t\t      (match_dup:SI 5)))]\n   {\n-    HOST_WIDE_INT val = INTVAL(operands[2]);\n-    operands[4] = GEN_INT (val&7);\n-    operands[5] = GEN_INT (val&-8);\n+    operands[3] = gen_reg_rtx (<MODE>mode);\n+    if (GET_CODE (operands[2]) == CONST_INT)\n+      {\n+\tHOST_WIDE_INT val = INTVAL(operands[2]);\n+\toperands[4] = GEN_INT (val & 7);\n+\toperands[5] = GEN_INT (val & -8);\n+      }\n+    else\n+      {\n+        rtx t0 = gen_reg_rtx (SImode);\n+        rtx t1 = gen_reg_rtx (SImode);\n+\temit_insn (gen_subsi3(t0, GEN_INT(0), operands[2]));\n+\temit_insn (gen_subsi3(t1, GEN_INT(7), operands[2]));\n+        operands[4] = gen_rtx_AND (SImode, gen_rtx_NEG (SImode, t0), GEN_INT (7));\n+        operands[5] = gen_rtx_AND (SImode, gen_rtx_NEG (SImode, gen_rtx_AND (SImode, t1, GEN_INT (-8))), GEN_INT (-8));\n+      }\n   }\n-  [(set_attr \"type\" \"shuf,shuf\")])\n-\n-(define_insn_and_split \"lshr<mode>3_reg\"\n-  [(set (match_operand:DTI 0 \"spu_reg_operand\" \"=r\")\n-\t(lshiftrt:DTI (match_operand:DTI 1 \"spu_reg_operand\" \"r\")\n-\t\t      (match_operand:SI 2 \"spu_reg_operand\" \"r\")))\n-   (clobber (match_operand:DTI 3 \"spu_reg_operand\" \"=&r\"))\n-   (clobber (match_operand:SI 4 \"spu_reg_operand\" \"=&r\"))\n-   (clobber (match_operand:SI 5 \"spu_reg_operand\" \"=&r\"))]\n-  \"\"\n-  \"#\"\n-  \"\"\n-  [(set (match_dup:DTI 3)\n-\t(lshiftrt:DTI (match_dup:DTI 1)\n-\t\t     (and:SI (neg:SI (match_dup:SI 4))\n-\t\t\t     (const_int 7))))\n-   (set (match_dup:DTI 0)\n-\t(lshiftrt:DTI (match_dup:DTI 3)\n-\t\t     (and:SI (neg:SI (and:SI (match_dup:SI 5)\n-\t\t\t\t\t     (const_int -8)))\n-\t\t\t     (const_int -8))))]\n-  {\n-    emit_insn (gen_subsi3(operands[4], GEN_INT(0), operands[2]));\n-    emit_insn (gen_subsi3(operands[5], GEN_INT(7), operands[2]));\n-  })\n+  [(set_attr \"type\" \"*,shuf,shuf\")])\n \n-(define_insn_and_split \"shrqbybi_<mode>\"\n+(define_expand \"shrqbybi_<mode>\"\n   [(set (match_operand:DTI 0 \"spu_reg_operand\" \"=r,r\")\n \t(lshiftrt:DTI (match_operand:DTI 1 \"spu_reg_operand\" \"r,r\")\n-\t\t      (and:SI (match_operand:SI 2 \"spu_nonmem_operand\" \"r,I\")\n-\t\t\t      (const_int -8))))\n-   (clobber (match_scratch:SI 3 \"=&r,X\"))]\n-  \"\"\n-  \"#\"\n-  \"reload_completed\"\n-  [(set (match_dup:DTI 0)\n-\t(lshiftrt:DTI (match_dup:DTI 1)\n-\t\t      (and:SI (neg:SI (and:SI (match_dup:SI 3) (const_int -8)))\n+\t\t      (and:SI (neg:SI (and:SI (match_operand:SI 2 \"spu_nonmem_operand\" \"r,I\")\n+\t\t\t\t\t      (const_int -8)))\n \t\t\t      (const_int -8))))]\n+  \"\"\n   {\n     if (GET_CODE (operands[2]) == CONST_INT)\n-      operands[3] = GEN_INT (7 - INTVAL (operands[2]));\n+      operands[2] = GEN_INT (7 - INTVAL (operands[2]));\n     else\n-      emit_insn (gen_subsi3 (operands[3], GEN_INT (7), operands[2]));\n-  }\n-  [(set_attr \"type\" \"shuf\")])\n+      {\n+        rtx t0 = gen_reg_rtx (SImode);\n+\temit_insn (gen_subsi3 (t0, GEN_INT (7), operands[2]));\n+        operands[2] = t0;\n+      }\n+  })\n \n (define_insn \"rotqmbybi_<mode>\"\n   [(set (match_operand:DTI 0 \"spu_reg_operand\" \"=r,r\")\n@@ -2486,25 +2470,22 @@\n    rotqmbii\\t%0,%1,-%E2\"\n   [(set_attr \"type\" \"shuf\")])\n \n-(define_insn_and_split \"shrqby_<mode>\"\n+(define_expand \"shrqby_<mode>\"\n   [(set (match_operand:DTI 0 \"spu_reg_operand\" \"=r,r\")\n \t(lshiftrt:DTI (match_operand:DTI 1 \"spu_reg_operand\" \"r,r\")\n-\t\t      (mult:SI (match_operand:SI 2 \"spu_nonmem_operand\" \"r,I\")\n-\t\t\t       (const_int 8))))\n-   (clobber (match_scratch:SI 3 \"=&r,X\"))]\n+\t\t      (mult:SI (neg:SI (match_operand:SI 2 \"spu_nonmem_operand\" \"r,I\"))\n+\t\t\t       (const_int 8))))]\n   \"\"\n-  \"#\"\n-  \"reload_completed\"\n-  [(set (match_dup:DTI 0)\n-\t(lshiftrt:DTI (match_dup:DTI 1)\n-\t\t      (mult:SI (neg:SI (match_dup:SI 3)) (const_int 8))))]\n   {\n     if (GET_CODE (operands[2]) == CONST_INT)\n-      operands[3] = GEN_INT (-INTVAL (operands[2]));\n+      operands[2] = GEN_INT (-INTVAL (operands[2]));\n     else\n-      emit_insn (gen_subsi3 (operands[3], GEN_INT (0), operands[2]));\n-  }\n-  [(set_attr \"type\" \"shuf\")])\n+      {\n+        rtx t0 = gen_reg_rtx (SImode);\n+\temit_insn (gen_subsi3 (t0, GEN_INT (0), operands[2]));\n+        operands[2] = t0;\n+      }\n+  })\n \n (define_insn \"rotqmby_<mode>\"\n   [(set (match_operand:DTI 0 \"spu_reg_operand\" \"=r,r\")\n@@ -2538,6 +2519,14 @@\n   \"\"\n   [(set_attr \"type\" \"*,fx3\")])\n   \n+(define_insn \"<v>ashr<mode>3_imm\"\n+  [(set (match_operand:VHSI 0 \"spu_reg_operand\" \"=r\")\n+\t(ashiftrt:VHSI (match_operand:VHSI 1 \"spu_reg_operand\" \"r\")\n+\t\t       (match_operand:VHSI 2 \"immediate_operand\" \"W\")))]\n+  \"\"\n+  \"rotma<bh>i\\t%0,%1,-%<umask>2\"\n+  [(set_attr \"type\" \"fx3\")])\n+  \n \n (define_insn \"rotma_<mode>\"\n   [(set (match_operand:VHSI 0 \"spu_reg_operand\" \"=r,r\")\n@@ -2622,11 +2611,16 @@\n   })\n \n \n-(define_expand \"ashrti3\"\n-  [(set (match_operand:TI 0 \"spu_reg_operand\" \"\")\n-\t(ashiftrt:TI (match_operand:TI 1 \"spu_reg_operand\" \"\")\n-\t\t     (match_operand:SI 2 \"spu_nonmem_operand\" \"\")))]\n+(define_insn_and_split \"ashrti3\"\n+  [(set (match_operand:TI 0 \"spu_reg_operand\" \"=r,r\")\n+\t(ashiftrt:TI (match_operand:TI 1 \"spu_reg_operand\" \"r,r\")\n+\t\t     (match_operand:SI 2 \"spu_nonmem_operand\" \"r,i\")))]\n+  \"\"\n+  \"#\"\n   \"\"\n+  [(set (match_dup:TI 0)\n+\t(ashiftrt:TI (match_dup:TI 1)\n+\t\t     (match_dup:SI 2)))]\n   {\n     rtx sign_shift = gen_reg_rtx (SImode);\n     rtx sign_mask = gen_reg_rtx (TImode);\n@@ -2711,33 +2705,133 @@\n \n \f\n ;; struct extract/insert\n-;; We have to handle mem's because GCC will generate invalid SUBREG's\n-;; if it handles them.  We generate better code anyway.\n+;; We handle mem's because GCC will generate invalid SUBREG's\n+;; and inefficient code.\n \n (define_expand \"extv\"\n-  [(set (match_operand 0 \"register_operand\" \"\")\n-\t(sign_extract (match_operand 1 \"register_operand\" \"\")\n-\t\t      (match_operand:SI 2 \"const_int_operand\" \"\")\n-\t\t      (match_operand:SI 3 \"const_int_operand\" \"\")))]\n+  [(set (match_operand:TI 0 \"register_operand\" \"\")\n+\t(sign_extract:TI (match_operand 1 \"nonimmediate_operand\" \"\")\n+\t\t\t (match_operand:SI 2 \"const_int_operand\" \"\")\n+\t\t\t (match_operand:SI 3 \"const_int_operand\" \"\")))]\n   \"\"\n-  { spu_expand_extv(operands, 0); DONE; })\n+  {\n+    spu_expand_extv (operands, 0);\n+    DONE;\n+  })\n \n (define_expand \"extzv\"\n-  [(set (match_operand 0 \"register_operand\" \"\")\n-\t(zero_extract (match_operand 1 \"register_operand\" \"\")\n+  [(set (match_operand:TI 0 \"register_operand\" \"\")\n+\t(zero_extract:TI (match_operand 1 \"nonimmediate_operand\" \"\")\n \t\t\t (match_operand:SI 2 \"const_int_operand\" \"\")\n \t\t\t (match_operand:SI 3 \"const_int_operand\" \"\")))]\n   \"\"\n-  { spu_expand_extv(operands, 1); DONE; })\n+  {\n+    spu_expand_extv (operands, 1);\n+    DONE;\n+  })\n \n (define_expand \"insv\"\n-  [(set (zero_extract (match_operand 0 \"register_operand\" \"\")\n+  [(set (zero_extract (match_operand 0 \"nonimmediate_operand\" \"\")\n \t\t      (match_operand:SI 1 \"const_int_operand\" \"\")\n \t\t      (match_operand:SI 2 \"const_int_operand\" \"\"))\n \t(match_operand 3 \"nonmemory_operand\" \"\"))]\n   \"\"\n   { spu_expand_insv(operands); DONE; })\n \n+;; Simplify a number of patterns that get generated by extv, extzv,\n+;; insv, and loads.\n+(define_insn_and_split \"trunc_shr_ti<mode>\"\n+  [(set (match_operand:QHSI 0 \"spu_reg_operand\" \"=r\")\n+        (truncate:QHSI (match_operator:TI 2 \"shiftrt_operator\" [(match_operand:TI 1 \"spu_reg_operand\" \"0\")\n+\t\t\t\t\t\t\t\t(const_int 96)])))]\n+  \"\"\n+  \"#\"\n+  \"reload_completed\"\n+  [(const_int 0)]\n+  {\n+    spu_split_convert (operands);\n+    DONE;\n+  }\n+  [(set_attr \"type\" \"convert\")\n+   (set_attr \"length\" \"0\")])\n+\n+(define_insn_and_split \"trunc_shr_tidi\"\n+  [(set (match_operand:DI 0 \"spu_reg_operand\" \"=r\")\n+        (truncate:DI (match_operator:TI 2 \"shiftrt_operator\" [(match_operand:TI 1 \"spu_reg_operand\" \"0\")\n+\t\t\t\t\t\t\t      (const_int 64)])))]\n+  \"\"\n+  \"#\"\n+  \"reload_completed\"\n+  [(const_int 0)]\n+  {\n+    spu_split_convert (operands);\n+    DONE;\n+  }\n+  [(set_attr \"type\" \"convert\")\n+   (set_attr \"length\" \"0\")])\n+\n+(define_insn_and_split \"shl_ext_<mode>ti\"\n+  [(set (match_operand:TI 0 \"spu_reg_operand\" \"=r\")\n+        (ashift:TI (match_operator:TI 2 \"extend_operator\" [(match_operand:QHSI 1 \"spu_reg_operand\" \"0\")])\n+\t\t   (const_int 96)))]\n+  \"\"\n+  \"#\"\n+  \"reload_completed\"\n+  [(const_int 0)]\n+  {\n+    spu_split_convert (operands);\n+    DONE;\n+  }\n+  [(set_attr \"type\" \"convert\")\n+   (set_attr \"length\" \"0\")])\n+\n+(define_insn_and_split \"shl_ext_diti\"\n+  [(set (match_operand:TI 0 \"spu_reg_operand\" \"=r\")\n+        (ashift:TI (match_operator:TI 2 \"extend_operator\" [(match_operand:DI 1 \"spu_reg_operand\" \"0\")])\n+\t\t   (const_int 64)))]\n+  \"\"\n+  \"#\"\n+  \"reload_completed\"\n+  [(const_int 0)]\n+  {\n+    spu_split_convert (operands);\n+    DONE;\n+  }\n+  [(set_attr \"type\" \"convert\")\n+   (set_attr \"length\" \"0\")])\n+\n+(define_insn \"sext_trunc_lshr_tiqisi\"\n+  [(set (match_operand:SI 0 \"spu_reg_operand\" \"=r\")\n+        (sign_extend:SI (truncate:QI (match_operator:TI 2 \"shiftrt_operator\" [(match_operand:TI 1 \"spu_reg_operand\" \"r\")\n+\t\t\t\t\t\t\t\t\t      (const_int 120)]))))]\n+  \"\"\n+  \"rotmai\\t%0,%1,-24\"\n+  [(set_attr \"type\" \"fx3\")])\n+\n+(define_insn \"zext_trunc_lshr_tiqisi\"\n+  [(set (match_operand:SI 0 \"spu_reg_operand\" \"=r\")\n+        (zero_extend:SI (truncate:QI (match_operator:TI 2 \"shiftrt_operator\" [(match_operand:TI 1 \"spu_reg_operand\" \"r\")\n+\t\t\t\t\t\t\t\t\t      (const_int 120)]))))]\n+  \"\"\n+  \"rotmi\\t%0,%1,-24\"\n+  [(set_attr \"type\" \"fx3\")])\n+\n+(define_insn \"sext_trunc_lshr_tihisi\"\n+  [(set (match_operand:SI 0 \"spu_reg_operand\" \"=r\")\n+        (sign_extend:SI (truncate:HI (match_operator:TI 2 \"shiftrt_operator\" [(match_operand:TI 1 \"spu_reg_operand\" \"r\")\n+\t\t\t\t\t\t\t\t\t      (const_int 112)]))))]\n+  \"\"\n+  \"rotmai\\t%0,%1,-16\"\n+  [(set_attr \"type\" \"fx3\")])\n+\n+(define_insn \"zext_trunc_lshr_tihisi\"\n+  [(set (match_operand:SI 0 \"spu_reg_operand\" \"=r\")\n+        (zero_extend:SI (truncate:HI (match_operator:TI 2 \"shiftrt_operator\" [(match_operand:TI 1 \"spu_reg_operand\" \"r\")\n+\t\t\t\t\t\t\t\t\t      (const_int 112)]))))]\n+  \"\"\n+  \"rotmi\\t%0,%1,-16\"\n+  [(set_attr \"type\" \"fx3\")])\n+\n \f\n ;; String/block move insn.\n ;; Argument 0 is the destination\n@@ -4303,21 +4397,20 @@ selb\\t%0,%4,%0,%3\"\n     DONE;\n   })\n \n-(define_insn \"_spu_convert\"\n+(define_insn_and_split \"_spu_convert\"\n   [(set (match_operand 0 \"spu_reg_operand\" \"=r\")\n \t(unspec [(match_operand 1 \"spu_reg_operand\" \"0\")] UNSPEC_CONVERT))]\n-  \"operands\"\n   \"\"\n+  \"#\"\n+  \"reload_completed\"\n+  [(const_int 0)]\n+  {\n+    spu_split_convert (operands);\n+    DONE;\n+  }\n   [(set_attr \"type\" \"convert\")\n    (set_attr \"length\" \"0\")])\n \n-(define_peephole2\n-  [(set (match_operand 0 \"spu_reg_operand\")\n-\t(unspec [(match_operand 1 \"spu_reg_operand\")] UNSPEC_CONVERT))]\n-  \"\"\n-  [(use (const_int 0))]\n-  \"\")\n-\n \f\n ;;\n (include \"spu-builtins.md\")\n@@ -5186,8 +5279,8 @@ DONE;\n }\")\n \n (define_insn \"stack_protect_set\"\n-  [(set (match_operand:SI 0 \"spu_mem_operand\" \"=m\")\n-        (unspec:SI [(match_operand:SI 1 \"spu_mem_operand\" \"m\")] UNSPEC_SP_SET))\n+  [(set (match_operand:SI 0 \"memory_operand\" \"=m\")\n+        (unspec:SI [(match_operand:SI 1 \"memory_operand\" \"m\")] UNSPEC_SP_SET))\n    (set (match_scratch:SI 2 \"=&r\") (const_int 0))]\n   \"\"\n   \"lq%p1\\t%2,%1\\;stq%p0\\t%2,%0\\;xor\\t%2,%2,%2\"\n@@ -5196,8 +5289,8 @@ DONE;\n )\n \n (define_expand \"stack_protect_test\"\n-  [(match_operand 0 \"spu_mem_operand\" \"\")\n-   (match_operand 1 \"spu_mem_operand\" \"\")\n+  [(match_operand 0 \"memory_operand\" \"\")\n+   (match_operand 1 \"memory_operand\" \"\")\n    (match_operand 2 \"\" \"\")]\n   \"\"\n {\n@@ -5223,8 +5316,8 @@ DONE;\n \n (define_insn \"stack_protect_test_si\"\n   [(set (match_operand:SI 0 \"spu_reg_operand\" \"=&r\")\n-        (unspec:SI [(match_operand:SI 1 \"spu_mem_operand\" \"m\")\n-                    (match_operand:SI 2 \"spu_mem_operand\" \"m\")]\n+        (unspec:SI [(match_operand:SI 1 \"memory_operand\" \"m\")\n+                    (match_operand:SI 2 \"memory_operand\" \"m\")]\n                    UNSPEC_SP_TEST))\n    (set (match_scratch:SI 3 \"=&r\") (const_int 0))]\n   \"\""}]}