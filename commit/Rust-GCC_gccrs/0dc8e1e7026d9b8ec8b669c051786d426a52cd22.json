{"sha": "0dc8e1e7026d9b8ec8b669c051786d426a52cd22", "node_id": "C_kwDOANBUbNoAKDBkYzhlMWU3MDI2ZDliOGVjOGI2NjljMDUxNzg2ZDQyNmE1MmNkMjI", "commit": {"author": {"name": "Christophe Lyon", "email": "christophe.lyon@arm.com", "date": "2022-03-11T16:21:02Z"}, "committer": {"name": "Christophe Lyon", "email": "christophe.lyon@arm.com", "date": "2022-05-20T07:32:22Z"}, "message": "aarch64: Add backend support for DFP\n\nThis patch updates the aarch64 backend as needed to support DFP modes\n(SD, DD and TD).\n\nChanges\tv1->v2:\n\n* Drop\tsupport\tfor DFP\tmodes in\n  aarch64_gen_{load||store}[wb]_pair as these are only used in\n  prologue/epilogue where DFP modes are not used.  Drop\tthe\n  changes to the corresponding patterns in aarch64.md, and\n  useless GPF_PAIR iterator.\n\n* In aarch64_reinterpret_float_as_int, handle DDmode the same way\n  as DFmode (needed in case the representation of the\n  floating-point value can be loaded using mov/movk.\n\n* In aarch64_float_const_zero_rtx_p, reject constants with DFP\n  mode: when X is zero, the callers want to emit either '0' or\n  'zr' depending on the context, which is not the way 0.0 is\n  represented in DFP mode (in particular fmov d0, #0 is not right\n  for DFP).\n\n* In aarch64_legitimate_constant_p, accept DFP\n\n2022-03-31  Christophe Lyon  <christophe.lyon@arm.com>\n\n\tgcc/\n\t* config/aarch64/aarch64.cc\n\t(aarch64_split_128bit_move): Handle DFP modes.\n\t(aarch64_mode_valid_for_sched_fusion_p): Likewise.\n\t(aarch64_classify_address): Likewise.\n\t(aarch64_legitimize_address_displacement): Likewise.\n\t(aarch64_reinterpret_float_as_int): Likewise.\n\t(aarch64_float_const_zero_rtx_p): Likewise.\n\t(aarch64_can_const_movi_rtx_p): Likewise.\n\t(aarch64_anchor_offset): Likewise.\n\t(aarch64_secondary_reload): Likewise.\n\t(aarch64_rtx_costs): Likewise.\n\t(aarch64_legitimate_constant_p): Likewise.\n\t(aarch64_gimplify_va_arg_expr): Likewise.\n\t(aapcs_vfp_sub_candidate): Likewise.\n\t(aarch64_vfp_is_call_or_return_candidate): Likewise.\n\t(aarch64_output_scalar_simd_mov_immediate): Likewise.\n\t(aarch64_gen_adjusted_ldpstp): Likewise.\n\t(aarch64_scalar_mode_supported_p): Accept DFP modes if enabled.\n\t* config/aarch64/aarch64.md\n\t(movsf_aarch64): Use SFD iterator and rename into\n\tmov<mode>_aarch64.\n\t(movdf_aarch64): Use DFD iterator and rename into\n\tmov<mode>_aarch64.\n\t(movtf_aarch64): Use TFD iterator and rename into\n\tmov<mode>_aarch64.\n\t(split pattern for move TF mode): Use TFD iterator.\n\t* config/aarch64/iterators.md\n\t(GPF_TF_F16_MOV): Add DFP modes.\n\t(SFD, DFD, TFD): New iterators.\n\t(GPF_TF): Add DFP modes.\n\t(TX, DX, DX2): Likewise.", "tree": {"sha": "a988cc916ad210104d4c0f9a8fb8701ece30e809", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/a988cc916ad210104d4c0f9a8fb8701ece30e809"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/0dc8e1e7026d9b8ec8b669c051786d426a52cd22", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0dc8e1e7026d9b8ec8b669c051786d426a52cd22", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0dc8e1e7026d9b8ec8b669c051786d426a52cd22", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0dc8e1e7026d9b8ec8b669c051786d426a52cd22/comments", "author": null, "committer": null, "parents": [{"sha": "afd82c104b1038572ed4d473a0b5f6e2c778fa01", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/afd82c104b1038572ed4d473a0b5f6e2c778fa01", "html_url": "https://github.com/Rust-GCC/gccrs/commit/afd82c104b1038572ed4d473a0b5f6e2c778fa01"}], "stats": {"total": 140, "additions": 89, "deletions": 51}, "files": [{"sha": "4aad14cc8d380ee7b6c0bc28342a66c8d54a97aa", "filename": "gcc/config/aarch64/aarch64.cc", "status": "modified", "additions": 55, "deletions": 27, "changes": 82, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0dc8e1e7026d9b8ec8b669c051786d426a52cd22/gcc%2Fconfig%2Faarch64%2Faarch64.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0dc8e1e7026d9b8ec8b669c051786d426a52cd22/gcc%2Fconfig%2Faarch64%2Faarch64.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.cc?ref=0dc8e1e7026d9b8ec8b669c051786d426a52cd22", "patch": "@@ -4828,7 +4828,7 @@ aarch64_split_128bit_move (rtx dst, rtx src)\n \n   machine_mode mode = GET_MODE (dst);\n \n-  gcc_assert (mode == TImode || mode == TFmode);\n+  gcc_assert (mode == TImode || mode == TFmode || mode == TDmode);\n   gcc_assert (!(side_effects_p (src) || side_effects_p (dst)));\n   gcc_assert (mode == GET_MODE (src) || GET_MODE (src) == VOIDmode);\n \n@@ -10568,6 +10568,7 @@ aarch64_mode_valid_for_sched_fusion_p (machine_mode mode)\n {\n   return mode == SImode || mode == DImode\n \t || mode == SFmode || mode == DFmode\n+\t || mode == SDmode || mode == DDmode\n \t || (aarch64_vector_mode_supported_p (mode)\n \t     && (known_eq (GET_MODE_SIZE (mode), 8)\n \t\t || (known_eq (GET_MODE_SIZE (mode), 16)\n@@ -10610,12 +10611,13 @@ aarch64_classify_address (struct aarch64_address_info *info,\n   vec_flags &= ~VEC_PARTIAL;\n \n   /* On BE, we use load/store pair for all large int mode load/stores.\n-     TI/TFmode may also use a load/store pair.  */\n+     TI/TF/TDmode may also use a load/store pair.  */\n   bool advsimd_struct_p = (vec_flags == (VEC_ADVSIMD | VEC_STRUCT));\n   bool load_store_pair_p = (type == ADDR_QUERY_LDP_STP\n \t\t\t    || type == ADDR_QUERY_LDP_STP_N\n \t\t\t    || mode == TImode\n \t\t\t    || mode == TFmode\n+\t\t\t    || mode == TDmode\n \t\t\t    || (BYTES_BIG_ENDIAN && advsimd_struct_p));\n   /* If we are dealing with ADDR_QUERY_LDP_STP_N that means the incoming mode\n      corresponds to the actual size of the memory being loaded/stored and the\n@@ -10689,7 +10691,7 @@ aarch64_classify_address (struct aarch64_address_info *info,\n \t  info->offset = op1;\n \t  info->const_offset = offset;\n \n-\t  /* TImode and TFmode values are allowed in both pairs of X\n+\t  /* TImode, TFmode and TDmode values are allowed in both pairs of X\n \t     registers and individual Q registers.  The available\n \t     address modes are:\n \t     X,X: 7-bit signed scaled offset\n@@ -10698,7 +10700,7 @@ aarch64_classify_address (struct aarch64_address_info *info,\n \t     When performing the check for pairs of X registers i.e.  LDP/STP\n \t     pass down DImode since that is the natural size of the LDP/STP\n \t     instruction memory accesses.  */\n-\t  if (mode == TImode || mode == TFmode)\n+\t  if (mode == TImode || mode == TFmode || mode == TDmode)\n \t    return (aarch64_offset_7bit_signed_scaled_p (DImode, offset)\n \t\t    && (aarch64_offset_9bit_signed_unscaled_p (mode, offset)\n \t\t\t|| offset_12bit_unsigned_scaled_p (mode, offset)));\n@@ -10821,14 +10823,14 @@ aarch64_classify_address (struct aarch64_address_info *info,\n \t  info->offset = XEXP (XEXP (x, 1), 1);\n \t  info->const_offset = offset;\n \n-\t  /* TImode and TFmode values are allowed in both pairs of X\n+\t  /* TImode, TFmode and TDmode values are allowed in both pairs of X\n \t     registers and individual Q registers.  The available\n \t     address modes are:\n \t     X,X: 7-bit signed scaled offset\n \t     Q:   9-bit signed offset\n \t     We conservatively require an offset representable in either mode.\n \t   */\n-\t  if (mode == TImode || mode == TFmode)\n+\t  if (mode == TImode || mode == TFmode || mode == TDmode)\n \t    return (aarch64_offset_7bit_signed_scaled_p (mode, offset)\n \t\t    && aarch64_offset_9bit_signed_unscaled_p (mode, offset));\n \n@@ -10990,9 +10992,9 @@ aarch64_legitimize_address_displacement (rtx *offset1, rtx *offset2,\n \t offset.  Use 4KB range for 1- and 2-byte accesses and a 16KB\n \t range otherwise to increase opportunities for sharing the base\n \t address of different sizes.  Unaligned accesses use the signed\n-\t 9-bit range, TImode/TFmode use the intersection of signed\n+\t 9-bit range, TImode/TFmode/TDmode use the intersection of signed\n \t scaled 7-bit and signed 9-bit offset.  */\n-      if (mode == TImode || mode == TFmode)\n+      if (mode == TImode || mode == TFmode || mode == TDmode)\n \tsecond_offset = ((const_offset + 0x100) & 0x1f8) - 0x100;\n       else if ((const_offset & (size - 1)) != 0)\n \tsecond_offset = ((const_offset + 0x100) & 0x1ff) - 0x100;\n@@ -11073,7 +11075,7 @@ aarch64_reinterpret_float_as_int (rtx value, unsigned HOST_WIDE_INT *intval)\n \t\t  CONST_DOUBLE_REAL_VALUE (value),\n \t\t  REAL_MODE_FORMAT (mode));\n \n-  if (mode == DFmode)\n+  if (mode == DFmode || mode == DDmode)\n     {\n       int order = BYTES_BIG_ENDIAN ? 1 : 0;\n       ival = zext_hwi (res[order], 32);\n@@ -11114,11 +11116,15 @@ aarch64_float_const_rtx_p (rtx x)\n   return false;\n }\n \n-/* Return TRUE if rtx X is immediate constant 0.0 */\n+/* Return TRUE if rtx X is immediate constant 0.0 (but not in Decimal\n+   Floating Point).  */\n bool\n aarch64_float_const_zero_rtx_p (rtx x)\n {\n-  if (GET_MODE (x) == VOIDmode)\n+  /* 0.0 in Decimal Floating Point cannot be represented by #0 or\n+     zr as our callers expect, so no need to check the actual\n+     value if X is of Decimal Floating Point type.  */\n+  if (GET_MODE_CLASS (GET_MODE (x)) == MODE_DECIMAL_FLOAT)\n     return false;\n \n   if (REAL_VALUE_MINUS_ZERO (*CONST_DOUBLE_REAL_VALUE (x)))\n@@ -11156,7 +11162,7 @@ aarch64_can_const_movi_rtx_p (rtx x, machine_mode mode)\n   else\n     return false;\n \n-   /* use a 64 bit mode for everything except for DI/DF mode, where we use\n+   /* use a 64 bit mode for everything except for DI/DF/DD mode, where we use\n      a 128 bit vector mode.  */\n   int width = GET_MODE_BITSIZE (imode) == 64 ? 128 : 64;\n \n@@ -12356,7 +12362,7 @@ aarch64_anchor_offset (HOST_WIDE_INT offset, HOST_WIDE_INT size,\n   if (IN_RANGE (offset, -256, 0))\n     return 0;\n \n-  if (mode == TImode || mode == TFmode)\n+  if (mode == TImode || mode == TFmode || mode == TDmode)\n     return (offset + 0x100) & ~0x1ff;\n \n   /* Use 12-bit offset by access size.  */\n@@ -12465,22 +12471,26 @@ aarch64_secondary_reload (bool in_p ATTRIBUTE_UNUSED, rtx x,\n \n   /* Without the TARGET_SIMD instructions we cannot move a Q register\n      to a Q register directly.  We need a scratch.  */\n-  if (REG_P (x) && (mode == TFmode || mode == TImode) && mode == GET_MODE (x)\n+  if (REG_P (x)\n+      && (mode == TFmode || mode == TImode || mode == TDmode)\n+      && mode == GET_MODE (x)\n       && FP_REGNUM_P (REGNO (x)) && !TARGET_SIMD\n       && reg_class_subset_p (rclass, FP_REGS))\n     {\n       sri->icode = code_for_aarch64_reload_mov (mode);\n       return NO_REGS;\n     }\n \n-  /* A TFmode or TImode memory access should be handled via an FP_REGS\n+  /* A TFmode, TImode or TDmode memory access should be handled via an FP_REGS\n      because AArch64 has richer addressing modes for LDR/STR instructions\n      than LDP/STP instructions.  */\n   if (TARGET_FLOAT && rclass == GENERAL_REGS\n       && known_eq (GET_MODE_SIZE (mode), 16) && MEM_P (x))\n     return FP_REGS;\n \n-  if (rclass == FP_REGS && (mode == TImode || mode == TFmode) && CONSTANT_P(x))\n+  if (rclass == FP_REGS\n+      && (mode == TImode || mode == TFmode || mode == TDmode)\n+      && CONSTANT_P(x))\n       return GENERAL_REGS;\n \n   return NO_REGS;\n@@ -13611,9 +13621,9 @@ aarch64_rtx_costs (rtx x, machine_mode mode, int outer ATTRIBUTE_UNUSED,\n \t\t*cost += extra_cost->ldst.storev;\n \t      else if (GET_MODE_CLASS (mode) == MODE_INT)\n \t\t*cost += extra_cost->ldst.store;\n-\t      else if (mode == SFmode)\n+\t      else if (mode == SFmode || mode == SDmode)\n \t\t*cost += extra_cost->ldst.storef;\n-\t      else if (mode == DFmode)\n+\t      else if (mode == DFmode || mode == DDmode)\n \t\t*cost += extra_cost->ldst.stored;\n \n \t      *cost +=\n@@ -13737,11 +13747,11 @@ aarch64_rtx_costs (rtx x, machine_mode mode, int outer ATTRIBUTE_UNUSED,\n \t  /* mov[df,sf]_aarch64.  */\n \t  if (aarch64_float_const_representable_p (x))\n \t    /* FMOV (scalar immediate).  */\n-\t    *cost += extra_cost->fp[mode == DFmode].fpconst;\n+\t    *cost += extra_cost->fp[mode == DFmode || mode == DDmode].fpconst;\n \t  else if (!aarch64_float_const_zero_rtx_p (x))\n \t    {\n \t      /* This will be a load from memory.  */\n-\t      if (mode == DFmode)\n+\t      if (mode == DFmode || mode == DDmode)\n \t\t*cost += extra_cost->ldst.loadd;\n \t      else\n \t\t*cost += extra_cost->ldst.loadf;\n@@ -13767,9 +13777,9 @@ aarch64_rtx_costs (rtx x, machine_mode mode, int outer ATTRIBUTE_UNUSED,\n \t    *cost += extra_cost->ldst.loadv;\n \t  else if (GET_MODE_CLASS (mode) == MODE_INT)\n \t    *cost += extra_cost->ldst.load;\n-\t  else if (mode == SFmode)\n+\t  else if (mode == SFmode || mode == SDmode)\n \t    *cost += extra_cost->ldst.loadf;\n-\t  else if (mode == DFmode)\n+\t  else if (mode == DFmode || mode == DDmode)\n \t    *cost += extra_cost->ldst.loadd;\n \n \t  *cost +=\n@@ -19352,7 +19362,7 @@ aarch64_legitimate_constant_p (machine_mode mode, rtx x)\n {\n   /* Support CSE and rematerialization of common constants.  */\n   if (CONST_INT_P (x)\n-      || (CONST_DOUBLE_P (x) && GET_MODE_CLASS (mode) == MODE_FLOAT))\n+      || CONST_DOUBLE_P (x))\n     return true;\n \n   /* Only accept variable-length vector constants if they can be\n@@ -19793,6 +19803,18 @@ aarch64_gimplify_va_arg_expr (tree valist, tree type, gimple_seq *pre_p,\n \t  field_t = long_double_type_node;\n \t  field_ptr_t = long_double_ptr_type_node;\n \t  break;\n+\tcase SDmode:\n+\t  field_t = dfloat32_type_node;\n+\t  field_ptr_t = build_pointer_type (dfloat32_type_node);\n+\t  break;\n+\tcase DDmode:\n+\t  field_t = dfloat64_type_node;\n+\t  field_ptr_t = build_pointer_type (dfloat64_type_node);\n+\t  break;\n+\tcase TDmode:\n+\t  field_t = dfloat128_type_node;\n+\t  field_ptr_t = build_pointer_type (dfloat128_type_node);\n+\t  break;\n \tcase E_HFmode:\n \t  field_t = aarch64_fp16_type_node;\n \t  field_ptr_t = aarch64_fp16_ptr_type_node;\n@@ -20044,7 +20066,8 @@ aapcs_vfp_sub_candidate (const_tree type, machine_mode *modep,\n     case REAL_TYPE:\n       mode = TYPE_MODE (type);\n       if (mode != DFmode && mode != SFmode\n-\t  && mode != TFmode && mode != HFmode)\n+\t  && mode != TFmode && mode != HFmode\n+\t  && mode != SDmode && mode != DDmode && mode != TDmode)\n \treturn -1;\n \n       if (*modep == VOIDmode)\n@@ -20360,7 +20383,9 @@ aarch64_vfp_is_call_or_return_candidate (machine_mode mode,\n   machine_mode new_mode = VOIDmode;\n   bool composite_p = aarch64_composite_type_p (type, mode);\n \n-  if ((!composite_p && GET_MODE_CLASS (mode) == MODE_FLOAT)\n+  if ((!composite_p\n+       && (GET_MODE_CLASS (mode) == MODE_FLOAT\n+\t   || GET_MODE_CLASS (mode) == MODE_DECIMAL_FLOAT))\n       || aarch64_short_vector_p (type, mode))\n     {\n       *count = 1;\n@@ -23268,7 +23293,7 @@ aarch64_output_scalar_simd_mov_immediate (rtx immediate, scalar_int_mode mode)\n     }\n \n   machine_mode vmode;\n-  /* use a 64 bit mode for everything except for DI/DF mode, where we use\n+  /* use a 64 bit mode for everything except for DI/DF/DD mode, where we use\n      a 128 bit vector mode.  */\n   int width = GET_MODE_BITSIZE (mode) == 64 ? 128 : 64;\n \n@@ -26086,7 +26111,7 @@ aarch64_gen_adjusted_ldpstp (rtx *operands, bool load,\n     base_off = (off_val_1 + off_val_3) / 2;\n   else\n     /* However, due to issues with negative LDP/STP offset generation for\n-       larger modes, for DF, DI and vector modes. we must not use negative\n+       larger modes, for DF, DD, DI and vector modes. we must not use negative\n        addresses smaller than 9 signed unadjusted bits can store.  This\n        provides the most range in this case.  */\n     base_off = off_val_1;\n@@ -26364,6 +26389,9 @@ aarch64_libgcc_floating_mode_supported_p (scalar_float_mode mode)\n static bool\n aarch64_scalar_mode_supported_p (scalar_mode mode)\n {\n+  if (DECIMAL_FLOAT_MODE_P (mode))\n+    return default_decimal_float_supported_p ();\n+\n   return (mode == HFmode\n \t  ? true\n \t  : default_scalar_mode_supported_p (mode));"}, {"sha": "2ac8d564762ec0ae967da70e4fd225183bee7562", "filename": "gcc/config/aarch64/aarch64.md", "status": "modified", "additions": 17, "deletions": 17, "changes": 34, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0dc8e1e7026d9b8ec8b669c051786d426a52cd22/gcc%2Fconfig%2Faarch64%2Faarch64.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0dc8e1e7026d9b8ec8b669c051786d426a52cd22/gcc%2Fconfig%2Faarch64%2Faarch64.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.md?ref=0dc8e1e7026d9b8ec8b669c051786d426a52cd22", "patch": "@@ -1477,11 +1477,11 @@\n    (set_attr \"arch\" \"simd,fp16,simd,simd,simd,fp16,simd,*,*,*,*,*\")]\n )\n \n-(define_insn \"*movsf_aarch64\"\n-  [(set (match_operand:SF 0 \"nonimmediate_operand\" \"=w,w  ,?r,w,w  ,w  ,w,m,r,m ,r,r\")\n-\t(match_operand:SF 1 \"general_operand\"      \"Y ,?rY, w,w,Ufc,Uvi,m,w,m,rY,r,M\"))]\n-  \"TARGET_FLOAT && (register_operand (operands[0], SFmode)\n-    || aarch64_reg_or_fp_zero (operands[1], SFmode))\"\n+(define_insn \"*mov<mode>_aarch64\"\n+  [(set (match_operand:SFD 0 \"nonimmediate_operand\" \"=w,w  ,?r,w,w  ,w  ,w,m,r,m ,r,r\")\n+\t(match_operand:SFD 1 \"general_operand\"      \"Y ,?rY, w,w,Ufc,Uvi,m,w,m,rY,r,M\"))]\n+  \"TARGET_FLOAT && (register_operand (operands[0], <MODE>mode)\n+    || aarch64_reg_or_fp_zero (operands[1], <MODE>mode))\"\n   \"@\n    movi\\\\t%0.2s, #0\n    fmov\\\\t%s0, %w1\n@@ -1501,11 +1501,11 @@\n    (set_attr \"arch\" \"simd,*,*,*,*,simd,*,*,*,*,*,*\")]\n )\n \n-(define_insn \"*movdf_aarch64\"\n-  [(set (match_operand:DF 0 \"nonimmediate_operand\" \"=w, w  ,?r,w,w  ,w  ,w,m,r,m ,r,r\")\n-\t(match_operand:DF 1 \"general_operand\"      \"Y , ?rY, w,w,Ufc,Uvi,m,w,m,rY,r,N\"))]\n-  \"TARGET_FLOAT && (register_operand (operands[0], DFmode)\n-    || aarch64_reg_or_fp_zero (operands[1], DFmode))\"\n+(define_insn \"*mov<mode>_aarch64\"\n+  [(set (match_operand:DFD 0 \"nonimmediate_operand\" \"=w, w  ,?r,w,w  ,w  ,w,m,r,m ,r,r\")\n+\t(match_operand:DFD 1 \"general_operand\"      \"Y , ?rY, w,w,Ufc,Uvi,m,w,m,rY,r,N\"))]\n+  \"TARGET_FLOAT && (register_operand (operands[0], <MODE>mode)\n+    || aarch64_reg_or_fp_zero (operands[1], <MODE>mode))\"\n   \"@\n    movi\\\\t%d0, #0\n    fmov\\\\t%d0, %x1\n@@ -1545,13 +1545,13 @@\n   }\n )\n \n-(define_insn \"*movtf_aarch64\"\n-  [(set (match_operand:TF 0\n+(define_insn \"*mov<mode>_aarch64\"\n+  [(set (match_operand:TFD 0\n \t \"nonimmediate_operand\" \"=w,?r ,w ,?r,w,?w,w,m,?r,m ,m\")\n-\t(match_operand:TF 1\n+\t(match_operand:TFD 1\n \t \"general_operand\"      \" w,?rY,?r,w ,Y,Y ,m,w,m ,?r,Y\"))]\n-  \"TARGET_FLOAT && (register_operand (operands[0], TFmode)\n-    || aarch64_reg_or_fp_zero (operands[1], TFmode))\"\n+  \"TARGET_FLOAT && (register_operand (operands[0], <MODE>mode)\n+    || aarch64_reg_or_fp_zero (operands[1], <MODE>mode))\"\n   \"@\n    mov\\\\t%0.16b, %1.16b\n    #\n@@ -1571,8 +1571,8 @@\n )\n \n (define_split\n-   [(set (match_operand:TF 0 \"register_operand\" \"\")\n-\t (match_operand:TF 1 \"nonmemory_operand\" \"\"))]\n+   [(set (match_operand:TFD 0 \"register_operand\" \"\")\n+\t (match_operand:TFD 1 \"nonmemory_operand\" \"\"))]\n   \"reload_completed && aarch64_split_128bit_move_p (operands[0], operands[1])\"\n   [(const_int 0)]\n   {"}, {"sha": "1c1048368f06cf88e5e6be85850a40d9802db0b4", "filename": "gcc/config/aarch64/iterators.md", "status": "modified", "additions": 17, "deletions": 7, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0dc8e1e7026d9b8ec8b669c051786d426a52cd22/gcc%2Fconfig%2Faarch64%2Fiterators.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0dc8e1e7026d9b8ec8b669c051786d426a52cd22/gcc%2Fconfig%2Faarch64%2Fiterators.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Fiterators.md?ref=0dc8e1e7026d9b8ec8b669c051786d426a52cd22", "patch": "@@ -64,14 +64,24 @@\n (define_mode_iterator HFBF [HF BF])\n \n ;; Iterator for all scalar floating point modes suitable for moving, including\n-;; special BF type (HF, SF, DF, TF and BF)\n-(define_mode_iterator GPF_TF_F16_MOV [HF BF SF DF TF])\n+;; special BF type and decimal floating point types (HF, SF, DF, TF, BF,\n+;; SD, DD and TD)\n+(define_mode_iterator GPF_TF_F16_MOV [HF BF SF DF TF SD DD TD])\n+\n+;; Iterator for scalar 32bit fp modes (SF, SD)\n+(define_mode_iterator SFD [SD SF])\n+\n+;; Iterator for scalar 64bit fp modes (DF, DD)\n+(define_mode_iterator DFD [DD DF])\n+\n+;; Iterator for scalar 128bit fp modes (TF, TD)\n+(define_mode_iterator TFD [TD TF])\n \n ;; Double vector modes.\n (define_mode_iterator VDF [V2SF V4HF])\n \n-;; Iterator for all scalar floating point modes (SF, DF and TF)\n-(define_mode_iterator GPF_TF [SF DF TF])\n+;; Iterator for all scalar floating point modes (SF, DF, TF, SD, DD, and TD)\n+(define_mode_iterator GPF_TF [SF DF TF SD DD TD])\n \n ;; Integer Advanced SIMD modes.\n (define_mode_iterator VDQ_I [V8QI V16QI V4HI V8HI V2SI V4SI V2DI])\n@@ -298,7 +308,7 @@\n ;; 2 and 4 lane SI modes.\n (define_mode_iterator VS [V2SI V4SI])\n \n-(define_mode_iterator TX [TI TF])\n+(define_mode_iterator TX [TI TF TD])\n \n ;; Advanced SIMD opaque structure modes.\n (define_mode_iterator VSTRUCT [OI CI XI])\n@@ -400,10 +410,10 @@\n \t\t\t\t  V4x8HF V4x4SF V4x2DF V4x8BF])\n \n ;; Double scalar modes\n-(define_mode_iterator DX [DI DF])\n+(define_mode_iterator DX [DI DF DD])\n \n ;; Duplicate of the above\n-(define_mode_iterator DX2 [DI DF])\n+(define_mode_iterator DX2 [DI DF DD])\n \n ;; Single scalar modes\n (define_mode_iterator SX [SI SF])"}]}