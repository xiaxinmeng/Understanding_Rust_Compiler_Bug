{"sha": "8fe75e43623a88a049f0f33191fdffce583a69c3", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OGZlNzVlNDM2MjNhODhhMDQ5ZjBmMzMxOTFmZGZmY2U1ODNhNjljMw==", "commit": {"author": {"name": "Richard Henderson", "email": "rth@redhat.com", "date": "2004-08-13T04:29:06Z"}, "committer": {"name": "Richard Henderson", "email": "rth@gcc.gnu.org", "date": "2004-08-13T04:29:06Z"}, "message": "i386.c (internal_label_prefix): Export.\n\n\t* config/i386/i386.c (internal_label_prefix): Export.\n\t(internal_label_prefix_len, struct ix86_address,\n\tix86_decompose_address, maybe_get_pool_constant,\n\tix86_fp_compare_code_to_integer, ix86_fp_comparison_codes,\n\tmemory_address_length): Export.\n\t(any_fp_register_operand, fp_register_operand,\n\tregister_and_not_any_fp_reg_operand, register_and_not_fp_reg_operand,\n\tx86_64_general_operand, x86_64_szext_general_operand,\n\tx86_64_nonmemory_operand, x86_64_movabs_operand,\n\tx86_64_szext_nonmemory_operand, x86_64_immediate_operand,\n\tx86_64_zext_immediate_operand, const_int_1_31_operand,\n\tsymbolic_operand, pic_symbolic_operand, local_symbolic_operand,\n\ttls_symbolic_operand, global_dynamic_symbolic_operand,\n\tlocal_dynamic_symbolic_operand, initial_exec_symbolic_operand,\n\tlocal_exec_symbolic_operand, call_insn_operand, sibcall_insn_operand,\n\tconstant_call_address_operand, const0_operand, const1_operand,\n\tconst248_operand, const_0_to_3_operand, const_0_to_7_operand,\n\tconst_0_to_15_operand, const_0_to_255_operand, incdec_operand,\n\tshiftdi_operand, reg_no_sp_operand, mmx_reg_operand,\n\tgeneral_no_elim_operand, nonmemory_no_elim_operand,\n\tindex_register_operand, q_regs_operand, flags_reg_operand,\n\tnon_q_regs_operand, zero_extended_scalar_load_operand,\n\tvector_move_operand, no_seg_address_operand, sse_comparison_operator,\n\tix86_comparison_operator, ix86_carry_flag_operator,\n\tfcmov_comparison_operator, promotable_binary_operator,\n\tcmp_fp_expander_operand, ext_register_operand, binary_fp_operator,\n\tmult_operator, div_operator, arith_or_logical_operator,\n\tmemory_displacement_operand, cmpsi_operand, long_memory_operand,\n\taligned_operand): Move to predicates.md as define_predicates.\n\t(tls_symbolic_operand_1): Remove.\n\t(x86_64_sign_extended_value): Merge into x86_64_immediate_operand.\n\t(x86_64_zero_extended_value): Merge into x86_64_zext_immediate_operand.\n\t(legitimize_address): Merge tls_symbolic_operand contents.\n\t(ix86_expand_move): Likewise.\n\t* config/i386/i386-protos.h: Update for exports.\n\t* config/i386/i386.h (EXTRA_CONSTRAINT): Update for renames.\n\t(PREDICATE_CODES, SPECIAL_MODE_PREDICATES): Remove.\n\t* config/i386/i386.md: Include predicates.md.\n\t* config/i386/predicates.md: New file.\n\nFrom-SVN: r85930", "tree": {"sha": "3aaddd782a881e51d9007f1f4ae5265eb5649a34", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/3aaddd782a881e51d9007f1f4ae5265eb5649a34"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/8fe75e43623a88a049f0f33191fdffce583a69c3", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8fe75e43623a88a049f0f33191fdffce583a69c3", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8fe75e43623a88a049f0f33191fdffce583a69c3", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8fe75e43623a88a049f0f33191fdffce583a69c3/comments", "author": null, "committer": null, "parents": [{"sha": "1a6213c3deb341d45d9ad004a485deea000ff476", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/1a6213c3deb341d45d9ad004a485deea000ff476", "html_url": "https://github.com/Rust-GCC/gccrs/commit/1a6213c3deb341d45d9ad004a485deea000ff476"}], "stats": {"total": 2287, "additions": 986, "deletions": 1301}, "files": [{"sha": "68fcc19e67eac1bdd56fcdd48430102d6b9dae44", "filename": "gcc/ChangeLog", "status": "modified", "additions": 42, "deletions": 0, "changes": 42, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8fe75e43623a88a049f0f33191fdffce583a69c3/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8fe75e43623a88a049f0f33191fdffce583a69c3/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=8fe75e43623a88a049f0f33191fdffce583a69c3", "patch": "@@ -1,3 +1,45 @@\n+2004-08-12  Richard Henderson  <rth@redhat.com>\n+\n+\t* config/i386/i386.c (internal_label_prefix): Export.\n+\t(internal_label_prefix_len, struct ix86_address,\n+\tix86_decompose_address, maybe_get_pool_constant,\n+\tix86_fp_compare_code_to_integer, ix86_fp_comparison_codes,\n+\tmemory_address_length): Export.\n+\t(any_fp_register_operand, fp_register_operand, \n+\tregister_and_not_any_fp_reg_operand, register_and_not_fp_reg_operand,\n+\tx86_64_general_operand, x86_64_szext_general_operand, \n+\tx86_64_nonmemory_operand, x86_64_movabs_operand,\n+\tx86_64_szext_nonmemory_operand, x86_64_immediate_operand,\n+\tx86_64_zext_immediate_operand, const_int_1_31_operand,\n+\tsymbolic_operand, pic_symbolic_operand, local_symbolic_operand,\n+\ttls_symbolic_operand, global_dynamic_symbolic_operand,\n+\tlocal_dynamic_symbolic_operand, initial_exec_symbolic_operand,\n+\tlocal_exec_symbolic_operand, call_insn_operand, sibcall_insn_operand,\n+\tconstant_call_address_operand, const0_operand, const1_operand,\n+\tconst248_operand, const_0_to_3_operand, const_0_to_7_operand,\n+\tconst_0_to_15_operand, const_0_to_255_operand, incdec_operand,\n+\tshiftdi_operand, reg_no_sp_operand, mmx_reg_operand,\n+\tgeneral_no_elim_operand, nonmemory_no_elim_operand,\n+\tindex_register_operand, q_regs_operand, flags_reg_operand,\n+\tnon_q_regs_operand, zero_extended_scalar_load_operand,\n+\tvector_move_operand, no_seg_address_operand, sse_comparison_operator,\n+\tix86_comparison_operator, ix86_carry_flag_operator, \n+\tfcmov_comparison_operator, promotable_binary_operator,\n+\tcmp_fp_expander_operand, ext_register_operand, binary_fp_operator,\n+\tmult_operator, div_operator, arith_or_logical_operator, \n+\tmemory_displacement_operand, cmpsi_operand, long_memory_operand,\n+\taligned_operand): Move to predicates.md as define_predicates.\n+\t(tls_symbolic_operand_1): Remove.\n+\t(x86_64_sign_extended_value): Merge into x86_64_immediate_operand.\n+\t(x86_64_zero_extended_value): Merge into x86_64_zext_immediate_operand.\n+\t(legitimize_address): Merge tls_symbolic_operand contents.\n+\t(ix86_expand_move): Likewise.\n+\t* config/i386/i386-protos.h: Update for exports.\n+\t* config/i386/i386.h (EXTRA_CONSTRAINT): Update for renames.\n+\t(PREDICATE_CODES, SPECIAL_MODE_PREDICATES): Remove.\n+\t* config/i386/i386.md: Include predicates.md.\n+\t* config/i386/predicates.md: New file.\n+\n 2004-08-13  Mark Mitchell  <mark@codesourcery.com>\n \n \tPR c++/16924"}, {"sha": "57b6d054bafee9e8db3fba51b8f7b6d48d49559a", "filename": "gcc/config/i386/i386-protos.h", "status": "modified", "additions": 21, "deletions": 2, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8fe75e43623a88a049f0f33191fdffce583a69c3/gcc%2Fconfig%2Fi386%2Fi386-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8fe75e43623a88a049f0f33191fdffce583a69c3/gcc%2Fconfig%2Fi386%2Fi386-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-protos.h?ref=8fe75e43623a88a049f0f33191fdffce583a69c3", "patch": "@@ -156,8 +156,6 @@ extern int ix86_attr_length_address_default (rtx);\n \n extern enum machine_mode ix86_fp_compare_mode (enum rtx_code);\n \n-extern int x86_64_sign_extended_value (rtx);\n-extern int x86_64_zero_extended_value (rtx);\n extern rtx ix86_libcall_value (enum machine_mode);\n extern bool ix86_function_value_regno_p (int);\n extern bool ix86_function_arg_regno_p (int);\n@@ -227,3 +225,24 @@ extern void i386_pe_encode_section_info (tree, rtx, int);\n extern const char *i386_pe_strip_name_encoding (const char *);\n extern const char *i386_pe_strip_name_encoding_full (const char *);\n extern void i386_pe_output_labelref (FILE *, const char *);\n+extern rtx maybe_get_pool_constant (rtx);\n+\n+extern char internal_label_prefix[16];\n+extern int internal_label_prefix_len;\n+\n+enum ix86_address_seg { SEG_DEFAULT, SEG_FS, SEG_GS };\n+struct ix86_address\n+{\n+  rtx base, index, disp;\n+  HOST_WIDE_INT scale;\n+  enum ix86_address_seg seg;\n+};\n+\n+extern int ix86_decompose_address (rtx, struct ix86_address *);\n+extern int memory_address_length (rtx addr);\n+\n+#ifdef RTX_CODE\n+extern void ix86_fp_comparison_codes (enum rtx_code code, enum rtx_code *,\n+\t\t\t\t      enum rtx_code *, enum rtx_code *);\n+extern enum rtx_code ix86_fp_compare_code_to_integer (enum rtx_code);\n+#endif"}, {"sha": "703aa5af519600d70619d3a27a67a5f8369b8c46", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 91, "deletions": 1206, "changes": 1297, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8fe75e43623a88a049f0f33191fdffce583a69c3/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8fe75e43623a88a049f0f33191fdffce583a69c3/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=8fe75e43623a88a049f0f33191fdffce583a69c3", "patch": "@@ -829,17 +829,14 @@ const char *ix86_branch_cost_string;\n const char *ix86_align_funcs_string;\n \n /* Prefix built by ASM_GENERATE_INTERNAL_LABEL.  */\n-static char internal_label_prefix[16];\n-static int internal_label_prefix_len;\n+char internal_label_prefix[16];\n+int internal_label_prefix_len;\n \f\n-static int local_symbolic_operand (rtx, enum machine_mode);\n-static int tls_symbolic_operand_1 (rtx, enum tls_model);\n static void output_pic_addr_const (FILE *, rtx, int);\n static void put_condition_code (enum rtx_code, enum machine_mode,\n \t\t\t\tint, int, FILE *);\n static const char *get_some_local_dynamic_name (void);\n static int get_some_local_dynamic_name_1 (rtx *, void *);\n-static rtx maybe_get_pool_constant (rtx);\n static rtx ix86_expand_int_compare (enum rtx_code, rtx, rtx);\n static enum rtx_code ix86_prepare_fp_compare_args (enum rtx_code, rtx *,\n \t\t\t\t\t\t   rtx *);\n@@ -850,7 +847,6 @@ static rtx get_thread_pointer (int);\n static rtx legitimize_tls_address (rtx, enum tls_model, int);\n static void get_pc_thunk_name (char [32], unsigned int);\n static rtx gen_push (rtx);\n-static int memory_address_length (rtx addr);\n static int ix86_flags_dependant (rtx, rtx, enum attr_type);\n static int ix86_agi_dependant (rtx, rtx, enum attr_type);\n static struct machine_function * ix86_init_machine_status (void);\n@@ -880,14 +876,6 @@ static void ix86_setup_incoming_varargs (CUMULATIVE_ARGS *, enum machine_mode,\n \t\t\t\t\t tree, int *, int);\n static tree ix86_gimplify_va_arg (tree, tree, tree *, tree *);\n \n-struct ix86_address\n-{\n-  rtx base, index, disp;\n-  HOST_WIDE_INT scale;\n-  enum ix86_address_seg { SEG_DEFAULT, SEG_FS, SEG_GS } seg;\n-};\n-\n-static int ix86_decompose_address (rtx, struct ix86_address *);\n static int ix86_address_cost (rtx);\n static bool ix86_cannot_force_const_mem (rtx);\n static rtx ix86_delegitimize_address (rtx);\n@@ -902,9 +890,6 @@ static rtx ix86_expand_unop_builtin (enum insn_code, tree, rtx, int);\n static rtx ix86_expand_binop_builtin (enum insn_code, tree, rtx);\n static rtx ix86_expand_store_builtin (enum insn_code, tree);\n static rtx safe_vector_operand (rtx, enum machine_mode);\n-static enum rtx_code ix86_fp_compare_code_to_integer (enum rtx_code);\n-static void ix86_fp_comparison_codes (enum rtx_code code, enum rtx_code *,\n-\t\t\t\t      enum rtx_code *, enum rtx_code *);\n static rtx ix86_expand_fp_compare (enum rtx_code, rtx, rtx, rtx, rtx *, rtx *);\n static int ix86_fp_comparison_arithmetics_cost (enum rtx_code code);\n static int ix86_fp_comparison_fcomi_cost (enum rtx_code code);\n@@ -3447,1010 +3432,93 @@ ix86_gimplify_va_arg (tree valist, tree type, tree *pre_p, tree *post_p)\n \t\t\t\t       size_int (src_offset)));\n \t      src = build_fold_indirect_ref (src_addr);\n \n-\t      dest_addr = fold_convert (addr_type, addr);\n-\t      dest_addr = fold (build2 (PLUS_EXPR, addr_type, dest_addr,\n-\t\t\t\t\tsize_int (INTVAL (XEXP (slot, 1)))));\n-\t      dest = build_fold_indirect_ref (dest_addr);\n-\n-\t      t = build2 (MODIFY_EXPR, void_type_node, dest, src);\n-\t      gimplify_and_add (t, pre_p);\n-\t    }\n-\t}\n-\n-      if (needed_intregs)\n-\t{\n-\t  t = build2 (PLUS_EXPR, TREE_TYPE (gpr), gpr,\n-\t\t      build_int_2 (needed_intregs * 8, 0));\n-\t  t = build2 (MODIFY_EXPR, TREE_TYPE (gpr), gpr, t);\n-\t  gimplify_and_add (t, pre_p);\n-\t}\n-      if (needed_sseregs)\n-\t{\n-\t  t =\n-\t    build2 (PLUS_EXPR, TREE_TYPE (fpr), fpr,\n-\t\t   build_int_2 (needed_sseregs * 16, 0));\n-\t  t = build2 (MODIFY_EXPR, TREE_TYPE (fpr), fpr, t);\n-\t  gimplify_and_add (t, pre_p);\n-\t}\n-\n-      t = build1 (GOTO_EXPR, void_type_node, lab_over);\n-      gimplify_and_add (t, pre_p);\n-\n-      t = build1 (LABEL_EXPR, void_type_node, lab_false);\n-      append_to_statement_list (t, pre_p);\n-    }\n-\n-  /* ... otherwise out of the overflow area.  */\n-\n-  /* Care for on-stack alignment if needed.  */\n-  if (FUNCTION_ARG_BOUNDARY (VOIDmode, type) <= 64)\n-    t = ovf;\n-  else\n-    {\n-      HOST_WIDE_INT align = FUNCTION_ARG_BOUNDARY (VOIDmode, type) / 8;\n-      t = build (PLUS_EXPR, TREE_TYPE (ovf), ovf, build_int_2 (align - 1, 0));\n-      t = build (BIT_AND_EXPR, TREE_TYPE (t), t, build_int_2 (-align, -1));\n-    }\n-  gimplify_expr (&t, pre_p, NULL, is_gimple_val, fb_rvalue);\n-\n-  t2 = build2 (MODIFY_EXPR, void_type_node, addr, t);\n-  gimplify_and_add (t2, pre_p);\n-\n-  t = build2 (PLUS_EXPR, TREE_TYPE (t), t,\n-\t      build_int_2 (rsize * UNITS_PER_WORD, 0));\n-  t = build2 (MODIFY_EXPR, TREE_TYPE (ovf), ovf, t);\n-  gimplify_and_add (t, pre_p);\n-\n-  if (container)\n-    {\n-      t = build1 (LABEL_EXPR, void_type_node, lab_over);\n-      append_to_statement_list (t, pre_p);\n-    }\n-\n-  ptrtype = build_pointer_type (type);\n-  addr = fold_convert (ptrtype, addr);\n-\n-  if (indirect_p)\n-    addr = build_fold_indirect_ref (addr);\n-  return build_fold_indirect_ref (addr);\n-}\n-\f\n-/* Return nonzero if OP is either a i387 or SSE fp register.  */\n-int\n-any_fp_register_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return ANY_FP_REG_P (op);\n-}\n-\n-/* Return nonzero if OP is an i387 fp register.  */\n-int\n-fp_register_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return FP_REG_P (op);\n-}\n-\n-/* Return nonzero if OP is a non-fp register_operand.  */\n-int\n-register_and_not_any_fp_reg_operand (rtx op, enum machine_mode mode)\n-{\n-  return register_operand (op, mode) && !ANY_FP_REG_P (op);\n-}\n-\n-/* Return nonzero if OP is a register operand other than an\n-   i387 fp register.  */\n-int\n-register_and_not_fp_reg_operand (rtx op, enum machine_mode mode)\n-{\n-  return register_operand (op, mode) && !FP_REG_P (op);\n-}\n-\n-/* Return nonzero if OP is general operand representable on x86_64.  */\n-\n-int\n-x86_64_general_operand (rtx op, enum machine_mode mode)\n-{\n-  if (!TARGET_64BIT)\n-    return general_operand (op, mode);\n-  if (nonimmediate_operand (op, mode))\n-    return 1;\n-  return x86_64_sign_extended_value (op);\n-}\n-\n-/* Return nonzero if OP is general operand representable on x86_64\n-   as either sign extended or zero extended constant.  */\n-\n-int\n-x86_64_szext_general_operand (rtx op, enum machine_mode mode)\n-{\n-  if (!TARGET_64BIT)\n-    return general_operand (op, mode);\n-  if (nonimmediate_operand (op, mode))\n-    return 1;\n-  return x86_64_sign_extended_value (op) || x86_64_zero_extended_value (op);\n-}\n-\n-/* Return nonzero if OP is nonmemory operand representable on x86_64.  */\n-\n-int\n-x86_64_nonmemory_operand (rtx op, enum machine_mode mode)\n-{\n-  if (!TARGET_64BIT)\n-    return nonmemory_operand (op, mode);\n-  if (register_operand (op, mode))\n-    return 1;\n-  return x86_64_sign_extended_value (op);\n-}\n-\n-/* Return nonzero if OP is nonmemory operand acceptable by movabs patterns.  */\n-\n-int\n-x86_64_movabs_operand (rtx op, enum machine_mode mode)\n-{\n-  if (!TARGET_64BIT || !flag_pic)\n-    return nonmemory_operand (op, mode);\n-  if (register_operand (op, mode) || x86_64_sign_extended_value (op))\n-    return 1;\n-  if (CONSTANT_P (op) && !symbolic_reference_mentioned_p (op))\n-    return 1;\n-  return 0;\n-}\n-\n-/* Return nonzero if OPNUM's MEM should be matched\n-   in movabs* patterns.  */\n-\n-int\n-ix86_check_movabs (rtx insn, int opnum)\n-{\n-  rtx set, mem;\n-\n-  set = PATTERN (insn);\n-  if (GET_CODE (set) == PARALLEL)\n-    set = XVECEXP (set, 0, 0);\n-  if (GET_CODE (set) != SET)\n-    abort ();\n-  mem = XEXP (set, opnum);\n-  while (GET_CODE (mem) == SUBREG)\n-    mem = SUBREG_REG (mem);\n-  if (GET_CODE (mem) != MEM)\n-    abort ();\n-  return (volatile_ok || !MEM_VOLATILE_P (mem));\n-}\n-\n-/* Return nonzero if OP is nonmemory operand representable on x86_64.  */\n-\n-int\n-x86_64_szext_nonmemory_operand (rtx op, enum machine_mode mode)\n-{\n-  if (!TARGET_64BIT)\n-    return nonmemory_operand (op, mode);\n-  if (register_operand (op, mode))\n-    return 1;\n-  return x86_64_sign_extended_value (op) || x86_64_zero_extended_value (op);\n-}\n-\n-/* Return nonzero if OP is immediate operand representable on x86_64.  */\n-\n-int\n-x86_64_immediate_operand (rtx op, enum machine_mode mode)\n-{\n-  if (!TARGET_64BIT)\n-    return immediate_operand (op, mode);\n-  return x86_64_sign_extended_value (op);\n-}\n-\n-/* Return nonzero if OP is immediate operand representable on x86_64.  */\n-\n-int\n-x86_64_zext_immediate_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return x86_64_zero_extended_value (op);\n-}\n-\n-/* Return nonzero if OP is CONST_INT >= 1 and <= 31 (a valid operand\n-   for shift & compare patterns, as shifting by 0 does not change flags),\n-   else return zero.  */\n-\n-int\n-const_int_1_31_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return (GET_CODE (op) == CONST_INT && INTVAL (op) >= 1 && INTVAL (op) <= 31);\n-}\n-\n-/* Returns 1 if OP is either a symbol reference or a sum of a symbol\n-   reference and a constant.  */\n-\n-int\n-symbolic_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  switch (GET_CODE (op))\n-    {\n-    case SYMBOL_REF:\n-    case LABEL_REF:\n-      return 1;\n-\n-    case CONST:\n-      op = XEXP (op, 0);\n-      if (GET_CODE (op) == SYMBOL_REF\n-\t  || GET_CODE (op) == LABEL_REF\n-\t  || (GET_CODE (op) == UNSPEC\n-\t      && (XINT (op, 1) == UNSPEC_GOT\n-\t\t  || XINT (op, 1) == UNSPEC_GOTOFF\n-\t\t  || XINT (op, 1) == UNSPEC_GOTPCREL)))\n-\treturn 1;\n-      if (GET_CODE (op) != PLUS\n-\t  || GET_CODE (XEXP (op, 1)) != CONST_INT)\n-\treturn 0;\n-\n-      op = XEXP (op, 0);\n-      if (GET_CODE (op) == SYMBOL_REF\n-\t  || GET_CODE (op) == LABEL_REF)\n-\treturn 1;\n-      /* Only @GOTOFF gets offsets.  */\n-      if (GET_CODE (op) != UNSPEC\n-\t  || XINT (op, 1) != UNSPEC_GOTOFF)\n-\treturn 0;\n-\n-      op = XVECEXP (op, 0, 0);\n-      if (GET_CODE (op) == SYMBOL_REF\n-\t  || GET_CODE (op) == LABEL_REF)\n-\treturn 1;\n-      return 0;\n-\n-    default:\n-      return 0;\n-    }\n-}\n-\n-/* Return true if the operand contains a @GOT or @GOTOFF reference.  */\n-\n-int\n-pic_symbolic_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  if (GET_CODE (op) != CONST)\n-    return 0;\n-  op = XEXP (op, 0);\n-  if (TARGET_64BIT)\n-    {\n-      if (GET_CODE (op) == UNSPEC\n-\t  && XINT (op, 1) == UNSPEC_GOTPCREL)\n-\treturn 1;\n-      if (GET_CODE (op) == PLUS\n-\t  && GET_CODE (XEXP (op, 0)) == UNSPEC\n-\t  && XINT (XEXP (op, 0), 1) == UNSPEC_GOTPCREL)\n-\treturn 1;\n-    }\n-  else\n-    {\n-      if (GET_CODE (op) == UNSPEC)\n-\treturn 1;\n-      if (GET_CODE (op) != PLUS\n-\t  || GET_CODE (XEXP (op, 1)) != CONST_INT)\n-\treturn 0;\n-      op = XEXP (op, 0);\n-      if (GET_CODE (op) == UNSPEC)\n-\treturn 1;\n-    }\n-  return 0;\n-}\n-\n-/* Return true if OP is a symbolic operand that resolves locally.  */\n-\n-static int\n-local_symbolic_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  if (GET_CODE (op) == CONST\n-      && GET_CODE (XEXP (op, 0)) == PLUS\n-      && GET_CODE (XEXP (XEXP (op, 0), 1)) == CONST_INT)\n-    op = XEXP (XEXP (op, 0), 0);\n-\n-  if (GET_CODE (op) == LABEL_REF)\n-    return 1;\n-\n-  if (GET_CODE (op) != SYMBOL_REF)\n-    return 0;\n-\n-  if (SYMBOL_REF_LOCAL_P (op))\n-    return 1;\n-\n-  /* There is, however, a not insubstantial body of code in the rest of\n-     the compiler that assumes it can just stick the results of\n-     ASM_GENERATE_INTERNAL_LABEL in a symbol_ref and have done.  */\n-  /* ??? This is a hack.  Should update the body of the compiler to\n-     always create a DECL an invoke targetm.encode_section_info.  */\n-  if (strncmp (XSTR (op, 0), internal_label_prefix,\n-\t       internal_label_prefix_len) == 0)\n-    return 1;\n-\n-  return 0;\n-}\n-\n-/* Test for various thread-local symbols.  */\n-\n-int\n-tls_symbolic_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  if (GET_CODE (op) != SYMBOL_REF)\n-    return 0;\n-  return SYMBOL_REF_TLS_MODEL (op);\n-}\n-\n-static inline int\n-tls_symbolic_operand_1 (rtx op, enum tls_model kind)\n-{\n-  if (GET_CODE (op) != SYMBOL_REF)\n-    return 0;\n-  return SYMBOL_REF_TLS_MODEL (op) == kind;\n-}\n-\n-int\n-global_dynamic_symbolic_operand (rtx op,\n-\t\t\t\t enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return tls_symbolic_operand_1 (op, TLS_MODEL_GLOBAL_DYNAMIC);\n-}\n-\n-int\n-local_dynamic_symbolic_operand (rtx op,\n-\t\t\t\tenum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return tls_symbolic_operand_1 (op, TLS_MODEL_LOCAL_DYNAMIC);\n-}\n-\n-int\n-initial_exec_symbolic_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return tls_symbolic_operand_1 (op, TLS_MODEL_INITIAL_EXEC);\n-}\n-\n-int\n-local_exec_symbolic_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return tls_symbolic_operand_1 (op, TLS_MODEL_LOCAL_EXEC);\n-}\n-\n-/* Test for a valid operand for a call instruction.  Don't allow the\n-   arg pointer register or virtual regs since they may decay into\n-   reg + const, which the patterns can't handle.  */\n-\n-int\n-call_insn_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  /* Disallow indirect through a virtual register.  This leads to\n-     compiler aborts when trying to eliminate them.  */\n-  if (GET_CODE (op) == REG\n-      && (op == arg_pointer_rtx\n-\t  || op == frame_pointer_rtx\n-\t  || (REGNO (op) >= FIRST_PSEUDO_REGISTER\n-\t      && REGNO (op) <= LAST_VIRTUAL_REGISTER)))\n-    return 0;\n-\n-  /* Disallow `call 1234'.  Due to varying assembler lameness this\n-     gets either rejected or translated to `call .+1234'.  */\n-  if (GET_CODE (op) == CONST_INT)\n-    return 0;\n-\n-  /* Explicitly allow SYMBOL_REF even if pic.  */\n-  if (GET_CODE (op) == SYMBOL_REF)\n-    return 1;\n-\n-  /* Otherwise we can allow any general_operand in the address.  */\n-  return general_operand (op, Pmode);\n-}\n-\n-/* Test for a valid operand for a call instruction.  Don't allow the\n-   arg pointer register or virtual regs since they may decay into\n-   reg + const, which the patterns can't handle.  */\n-\n-int\n-sibcall_insn_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  /* Disallow indirect through a virtual register.  This leads to\n-     compiler aborts when trying to eliminate them.  */\n-  if (GET_CODE (op) == REG\n-      && (op == arg_pointer_rtx\n-\t  || op == frame_pointer_rtx\n-\t  || (REGNO (op) >= FIRST_PSEUDO_REGISTER\n-\t      && REGNO (op) <= LAST_VIRTUAL_REGISTER)))\n-    return 0;\n-\n-  /* Explicitly allow SYMBOL_REF even if pic.  */\n-  if (GET_CODE (op) == SYMBOL_REF)\n-    return 1;\n-\n-  /* Otherwise we can only allow register operands.  */\n-  return register_operand (op, Pmode);\n-}\n-\n-int\n-constant_call_address_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  if (GET_CODE (op) == CONST\n-      && GET_CODE (XEXP (op, 0)) == PLUS\n-      && GET_CODE (XEXP (XEXP (op, 0), 1)) == CONST_INT)\n-    op = XEXP (XEXP (op, 0), 0);\n-  return GET_CODE (op) == SYMBOL_REF;\n-}\n-\n-/* Match exactly zero and one.  */\n-\n-int\n-const0_operand (rtx op, enum machine_mode mode)\n-{\n-  return op == CONST0_RTX (mode);\n-}\n-\n-int\n-const1_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return op == const1_rtx;\n-}\n-\n-/* Match 2, 4, or 8.  Used for leal multiplicands.  */\n-\n-int\n-const248_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return (GET_CODE (op) == CONST_INT\n-\t  && (INTVAL (op) == 2 || INTVAL (op) == 4 || INTVAL (op) == 8));\n-}\n-\n-int\n-const_0_to_3_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return (GET_CODE (op) == CONST_INT && INTVAL (op) >= 0 && INTVAL (op) < 4);\n-}\n-\n-int\n-const_0_to_7_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return (GET_CODE (op) == CONST_INT && INTVAL (op) >= 0 && INTVAL (op) < 8);\n-}\n-\n-int\n-const_0_to_15_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return (GET_CODE (op) == CONST_INT && INTVAL (op) >= 0 && INTVAL (op) < 16);\n-}\n-\n-int\n-const_0_to_255_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return (GET_CODE (op) == CONST_INT && INTVAL (op) >= 0 && INTVAL (op) < 256);\n-}\n-\n-\n-/* True if this is a constant appropriate for an increment or decrement.  */\n-\n-int\n-incdec_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  /* On Pentium4, the inc and dec operations causes extra dependency on flag\n-     registers, since carry flag is not set.  */\n-  if ((TARGET_PENTIUM4 || TARGET_NOCONA) && !optimize_size)\n-    return 0;\n-  return op == const1_rtx || op == constm1_rtx;\n-}\n-\n-/* Return nonzero if OP is acceptable as operand of DImode shift\n-   expander.  */\n-\n-int\n-shiftdi_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  if (TARGET_64BIT)\n-    return nonimmediate_operand (op, mode);\n-  else\n-    return register_operand (op, mode);\n-}\n-\n-/* Return false if this is the stack pointer, or any other fake\n-   register eliminable to the stack pointer.  Otherwise, this is\n-   a register operand.\n-\n-   This is used to prevent esp from being used as an index reg.\n-   Which would only happen in pathological cases.  */\n-\n-int\n-reg_no_sp_operand (rtx op, enum machine_mode mode)\n-{\n-  rtx t = op;\n-  if (GET_CODE (t) == SUBREG)\n-    t = SUBREG_REG (t);\n-  if (t == stack_pointer_rtx || t == arg_pointer_rtx || t == frame_pointer_rtx)\n-    return 0;\n-\n-  return register_operand (op, mode);\n-}\n-\n-int\n-mmx_reg_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return MMX_REG_P (op);\n-}\n-\n-/* Return false if this is any eliminable register.  Otherwise\n-   general_operand.  */\n-\n-int\n-general_no_elim_operand (rtx op, enum machine_mode mode)\n-{\n-  rtx t = op;\n-  if (GET_CODE (t) == SUBREG)\n-    t = SUBREG_REG (t);\n-  if (t == arg_pointer_rtx || t == frame_pointer_rtx\n-      || t == virtual_incoming_args_rtx || t == virtual_stack_vars_rtx\n-      || t == virtual_stack_dynamic_rtx)\n-    return 0;\n-  if (REG_P (t)\n-      && REGNO (t) >= FIRST_VIRTUAL_REGISTER\n-      && REGNO (t) <= LAST_VIRTUAL_REGISTER)\n-    return 0;\n-\n-  return general_operand (op, mode);\n-}\n-\n-/* Return false if this is any eliminable register.  Otherwise\n-   register_operand or const_int.  */\n-\n-int\n-nonmemory_no_elim_operand (rtx op, enum machine_mode mode)\n-{\n-  rtx t = op;\n-  if (GET_CODE (t) == SUBREG)\n-    t = SUBREG_REG (t);\n-  if (t == arg_pointer_rtx || t == frame_pointer_rtx\n-      || t == virtual_incoming_args_rtx || t == virtual_stack_vars_rtx\n-      || t == virtual_stack_dynamic_rtx)\n-    return 0;\n-\n-  return GET_CODE (op) == CONST_INT || register_operand (op, mode);\n-}\n-\n-/* Return false if this is any eliminable register or stack register,\n-   otherwise work like register_operand.  */\n-\n-int\n-index_register_operand (rtx op, enum machine_mode mode)\n-{\n-  rtx t = op;\n-  if (GET_CODE (t) == SUBREG)\n-    t = SUBREG_REG (t);\n-  if (!REG_P (t))\n-    return 0;\n-  if (t == arg_pointer_rtx\n-      || t == frame_pointer_rtx\n-      || t == virtual_incoming_args_rtx\n-      || t == virtual_stack_vars_rtx\n-      || t == virtual_stack_dynamic_rtx\n-      || REGNO (t) == STACK_POINTER_REGNUM)\n-    return 0;\n-\n-  return general_operand (op, mode);\n-}\n-\n-/* Return true if op is a Q_REGS class register.  */\n-\n-int\n-q_regs_operand (rtx op, enum machine_mode mode)\n-{\n-  if (mode != VOIDmode && GET_MODE (op) != mode)\n-    return 0;\n-  if (GET_CODE (op) == SUBREG)\n-    op = SUBREG_REG (op);\n-  return ANY_QI_REG_P (op);\n-}\n-\n-/* Return true if op is an flags register.  */\n-\n-int\n-flags_reg_operand (rtx op, enum machine_mode mode)\n-{\n-  if (mode != VOIDmode && GET_MODE (op) != mode)\n-    return 0;\n-  return REG_P (op) && REGNO (op) == FLAGS_REG && GET_MODE (op) != VOIDmode;\n-}\n-\n-/* Return true if op is a NON_Q_REGS class register.  */\n-\n-int\n-non_q_regs_operand (rtx op, enum machine_mode mode)\n-{\n-  if (mode != VOIDmode && GET_MODE (op) != mode)\n-    return 0;\n-  if (GET_CODE (op) == SUBREG)\n-    op = SUBREG_REG (op);\n-  return NON_QI_REG_P (op);\n-}\n-\n-int\n-zero_extended_scalar_load_operand (rtx op,\n-\t\t\t\t   enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  unsigned n_elts;\n-  if (GET_CODE (op) != MEM)\n-    return 0;\n-  op = maybe_get_pool_constant (op);\n-  if (!op)\n-    return 0;\n-  if (GET_CODE (op) != CONST_VECTOR)\n-    return 0;\n-  n_elts =\n-    (GET_MODE_SIZE (GET_MODE (op)) /\n-     GET_MODE_SIZE (GET_MODE_INNER (GET_MODE (op))));\n-  for (n_elts--; n_elts > 0; n_elts--)\n-    {\n-      rtx elt = CONST_VECTOR_ELT (op, n_elts);\n-      if (elt != CONST0_RTX (GET_MODE_INNER (GET_MODE (op))))\n-\treturn 0;\n-    }\n-  return 1;\n-}\n-\n-/*  Return 1 when OP is operand acceptable for standard SSE move.  */\n-int\n-vector_move_operand (rtx op, enum machine_mode mode)\n-{\n-  if (nonimmediate_operand (op, mode))\n-    return 1;\n-  if (GET_MODE (op) != mode && mode != VOIDmode)\n-    return 0;\n-  return (op == CONST0_RTX (GET_MODE (op)));\n-}\n-\n-/* Return true if op if a valid address, and does not contain\n-   a segment override.  */\n-\n-int\n-no_seg_address_operand (rtx op, enum machine_mode mode)\n-{\n-  struct ix86_address parts;\n-\n-  if (! address_operand (op, mode))\n-    return 0;\n-\n-  if (! ix86_decompose_address (op, &parts))\n-    abort ();\n-\n-  return parts.seg == SEG_DEFAULT;\n-}\n-\n-/* Return 1 if OP is a comparison that can be used in the CMPSS/CMPPS\n-   insns.  */\n-int\n-sse_comparison_operator (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  enum rtx_code code = GET_CODE (op);\n-  switch (code)\n-    {\n-    /* Operations supported directly.  */\n-    case EQ:\n-    case LT:\n-    case LE:\n-    case UNORDERED:\n-    case NE:\n-    case UNGE:\n-    case UNGT:\n-    case ORDERED:\n-      return 1;\n-    /* These are equivalent to ones above in non-IEEE comparisons.  */\n-    case UNEQ:\n-    case UNLT:\n-    case UNLE:\n-    case LTGT:\n-    case GE:\n-    case GT:\n-      return !TARGET_IEEE_FP;\n-    default:\n-      return 0;\n-    }\n-}\n-/* Return 1 if OP is a valid comparison operator in valid mode.  */\n-int\n-ix86_comparison_operator (rtx op, enum machine_mode mode)\n-{\n-  enum machine_mode inmode;\n-  enum rtx_code code = GET_CODE (op);\n-  if (mode != VOIDmode && GET_MODE (op) != mode)\n-    return 0;\n-  if (!COMPARISON_P (op))\n-    return 0;\n-  inmode = GET_MODE (XEXP (op, 0));\n-\n-  if (inmode == CCFPmode || inmode == CCFPUmode)\n-    {\n-      enum rtx_code second_code, bypass_code;\n-      ix86_fp_comparison_codes (code, &bypass_code, &code, &second_code);\n-      return (bypass_code == NIL && second_code == NIL);\n-    }\n-  switch (code)\n-    {\n-    case EQ: case NE:\n-      return 1;\n-    case LT: case GE:\n-      if (inmode == CCmode || inmode == CCGCmode\n-\t  || inmode == CCGOCmode || inmode == CCNOmode)\n-\treturn 1;\n-      return 0;\n-    case LTU: case GTU: case LEU: case ORDERED: case UNORDERED: case GEU:\n-      if (inmode == CCmode)\n-\treturn 1;\n-      return 0;\n-    case GT: case LE:\n-      if (inmode == CCmode || inmode == CCGCmode || inmode == CCNOmode)\n-\treturn 1;\n-      return 0;\n-    default:\n-      return 0;\n-    }\n-}\n-\n-/* Return 1 if OP is a valid comparison operator testing carry flag\n-   to be set.  */\n-int\n-ix86_carry_flag_operator (rtx op, enum machine_mode mode)\n-{\n-  enum machine_mode inmode;\n-  enum rtx_code code = GET_CODE (op);\n-\n-  if (mode != VOIDmode && GET_MODE (op) != mode)\n-    return 0;\n-  if (!COMPARISON_P (op))\n-    return 0;\n-  inmode = GET_MODE (XEXP (op, 0));\n-  if (GET_CODE (XEXP (op, 0)) != REG\n-      || REGNO (XEXP (op, 0)) != 17\n-      || XEXP (op, 1) != const0_rtx)\n-    return 0;\n-\n-  if (inmode == CCFPmode || inmode == CCFPUmode)\n-    {\n-      enum rtx_code second_code, bypass_code;\n-\n-      ix86_fp_comparison_codes (code, &bypass_code, &code, &second_code);\n-      if (bypass_code != NIL || second_code != NIL)\n-\treturn 0;\n-      code = ix86_fp_compare_code_to_integer (code);\n-    }\n-  else if (inmode != CCmode)\n-    return 0;\n-  return code == LTU;\n-}\n+\t      dest_addr = fold_convert (addr_type, addr);\n+\t      dest_addr = fold (build2 (PLUS_EXPR, addr_type, dest_addr,\n+\t\t\t\t\tsize_int (INTVAL (XEXP (slot, 1)))));\n+\t      dest = build_fold_indirect_ref (dest_addr);\n \n-/* Return 1 if OP is a comparison operator that can be issued by fcmov.  */\n+\t      t = build2 (MODIFY_EXPR, void_type_node, dest, src);\n+\t      gimplify_and_add (t, pre_p);\n+\t    }\n+\t}\n \n-int\n-fcmov_comparison_operator (rtx op, enum machine_mode mode)\n-{\n-  enum machine_mode inmode;\n-  enum rtx_code code = GET_CODE (op);\n+      if (needed_intregs)\n+\t{\n+\t  t = build2 (PLUS_EXPR, TREE_TYPE (gpr), gpr,\n+\t\t      build_int_2 (needed_intregs * 8, 0));\n+\t  t = build2 (MODIFY_EXPR, TREE_TYPE (gpr), gpr, t);\n+\t  gimplify_and_add (t, pre_p);\n+\t}\n+      if (needed_sseregs)\n+\t{\n+\t  t =\n+\t    build2 (PLUS_EXPR, TREE_TYPE (fpr), fpr,\n+\t\t   build_int_2 (needed_sseregs * 16, 0));\n+\t  t = build2 (MODIFY_EXPR, TREE_TYPE (fpr), fpr, t);\n+\t  gimplify_and_add (t, pre_p);\n+\t}\n \n-  if (mode != VOIDmode && GET_MODE (op) != mode)\n-    return 0;\n-  if (!COMPARISON_P (op))\n-    return 0;\n-  inmode = GET_MODE (XEXP (op, 0));\n-  if (inmode == CCFPmode || inmode == CCFPUmode)\n-    {\n-      enum rtx_code second_code, bypass_code;\n+      t = build1 (GOTO_EXPR, void_type_node, lab_over);\n+      gimplify_and_add (t, pre_p);\n \n-      ix86_fp_comparison_codes (code, &bypass_code, &code, &second_code);\n-      if (bypass_code != NIL || second_code != NIL)\n-\treturn 0;\n-      code = ix86_fp_compare_code_to_integer (code);\n-    }\n-  /* i387 supports just limited amount of conditional codes.  */\n-  switch (code)\n-    {\n-    case LTU: case GTU: case LEU: case GEU:\n-      if (inmode == CCmode || inmode == CCFPmode || inmode == CCFPUmode)\n-\treturn 1;\n-      return 0;\n-    case ORDERED: case UNORDERED:\n-    case EQ: case NE:\n-      return 1;\n-    default:\n-      return 0;\n+      t = build1 (LABEL_EXPR, void_type_node, lab_false);\n+      append_to_statement_list (t, pre_p);\n     }\n-}\n \n-/* Return 1 if OP is a binary operator that can be promoted to wider mode.  */\n+  /* ... otherwise out of the overflow area.  */\n \n-int\n-promotable_binary_operator (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  switch (GET_CODE (op))\n+  /* Care for on-stack alignment if needed.  */\n+  if (FUNCTION_ARG_BOUNDARY (VOIDmode, type) <= 64)\n+    t = ovf;\n+  else\n     {\n-    case MULT:\n-      /* Modern CPUs have same latency for HImode and SImode multiply,\n-         but 386 and 486 do HImode multiply faster.  */\n-      return ix86_tune > PROCESSOR_I486;\n-    case PLUS:\n-    case AND:\n-    case IOR:\n-    case XOR:\n-    case ASHIFT:\n-      return 1;\n-    default:\n-      return 0;\n+      HOST_WIDE_INT align = FUNCTION_ARG_BOUNDARY (VOIDmode, type) / 8;\n+      t = build (PLUS_EXPR, TREE_TYPE (ovf), ovf, build_int_2 (align - 1, 0));\n+      t = build (BIT_AND_EXPR, TREE_TYPE (t), t, build_int_2 (-align, -1));\n     }\n-}\n-\n-/* Nearly general operand, but accept any const_double, since we wish\n-   to be able to drop them into memory rather than have them get pulled\n-   into registers.  */\n-\n-int\n-cmp_fp_expander_operand (rtx op, enum machine_mode mode)\n-{\n-  if (mode != VOIDmode && mode != GET_MODE (op))\n-    return 0;\n-  if (GET_CODE (op) == CONST_DOUBLE)\n-    return 1;\n-  return general_operand (op, mode);\n-}\n-\n-/* Match an SI or HImode register for a zero_extract.  */\n-\n-int\n-ext_register_operand (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  int regno;\n-  if ((!TARGET_64BIT || GET_MODE (op) != DImode)\n-      && GET_MODE (op) != SImode && GET_MODE (op) != HImode)\n-    return 0;\n-\n-  if (!register_operand (op, VOIDmode))\n-    return 0;\n-\n-  /* Be careful to accept only registers having upper parts.  */\n-  regno = REG_P (op) ? REGNO (op) : REGNO (SUBREG_REG (op));\n-  return (regno > LAST_VIRTUAL_REGISTER || regno < 4);\n-}\n+  gimplify_expr (&t, pre_p, NULL, is_gimple_val, fb_rvalue);\n \n-/* Return 1 if this is a valid binary floating-point operation.\n-   OP is the expression matched, and MODE is its mode.  */\n+  t2 = build2 (MODIFY_EXPR, void_type_node, addr, t);\n+  gimplify_and_add (t2, pre_p);\n \n-int\n-binary_fp_operator (rtx op, enum machine_mode mode)\n-{\n-  if (mode != VOIDmode && mode != GET_MODE (op))\n-    return 0;\n+  t = build2 (PLUS_EXPR, TREE_TYPE (t), t,\n+\t      build_int_2 (rsize * UNITS_PER_WORD, 0));\n+  t = build2 (MODIFY_EXPR, TREE_TYPE (ovf), ovf, t);\n+  gimplify_and_add (t, pre_p);\n \n-  switch (GET_CODE (op))\n+  if (container)\n     {\n-    case PLUS:\n-    case MINUS:\n-    case MULT:\n-    case DIV:\n-      return GET_MODE_CLASS (GET_MODE (op)) == MODE_FLOAT;\n-\n-    default:\n-      return 0;\n+      t = build1 (LABEL_EXPR, void_type_node, lab_over);\n+      append_to_statement_list (t, pre_p);\n     }\n-}\n \n-int\n-mult_operator (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return GET_CODE (op) == MULT;\n-}\n-\n-int\n-div_operator (rtx op, enum machine_mode mode ATTRIBUTE_UNUSED)\n-{\n-  return GET_CODE (op) == DIV;\n-}\n+  ptrtype = build_pointer_type (type);\n+  addr = fold_convert (ptrtype, addr);\n \n-int\n-arith_or_logical_operator (rtx op, enum machine_mode mode)\n-{\n-  return ((mode == VOIDmode || GET_MODE (op) == mode)\n-          && ARITHMETIC_P (op));\n+  if (indirect_p)\n+    addr = build_fold_indirect_ref (addr);\n+  return build_fold_indirect_ref (addr);\n }\n-\n-/* Returns 1 if OP is memory operand with a displacement.  */\n+\f\n+/* Return nonzero if OPNUM's MEM should be matched\n+   in movabs* patterns.  */\n \n int\n-memory_displacement_operand (rtx op, enum machine_mode mode)\n+ix86_check_movabs (rtx insn, int opnum)\n {\n-  struct ix86_address parts;\n-\n-  if (! memory_operand (op, mode))\n-    return 0;\n+  rtx set, mem;\n \n-  if (! ix86_decompose_address (XEXP (op, 0), &parts))\n+  set = PATTERN (insn);\n+  if (GET_CODE (set) == PARALLEL)\n+    set = XVECEXP (set, 0, 0);\n+  if (GET_CODE (set) != SET)\n     abort ();\n-\n-  return parts.disp != NULL_RTX;\n-}\n-\n-/* To avoid problems when jump re-emits comparisons like testqi_ext_ccno_0,\n-   re-recognize the operand to avoid a copy_to_mode_reg that will fail.\n-\n-   ??? It seems likely that this will only work because cmpsi is an\n-   expander, and no actual insns use this.  */\n-\n-int\n-cmpsi_operand (rtx op, enum machine_mode mode)\n-{\n-  if (nonimmediate_operand (op, mode))\n-    return 1;\n-\n-  if (GET_CODE (op) == AND\n-      && GET_MODE (op) == SImode\n-      && GET_CODE (XEXP (op, 0)) == ZERO_EXTRACT\n-      && GET_CODE (XEXP (XEXP (op, 0), 1)) == CONST_INT\n-      && GET_CODE (XEXP (XEXP (op, 0), 2)) == CONST_INT\n-      && INTVAL (XEXP (XEXP (op, 0), 1)) == 8\n-      && INTVAL (XEXP (XEXP (op, 0), 2)) == 8\n-      && GET_CODE (XEXP (op, 1)) == CONST_INT)\n-    return 1;\n-\n-  return 0;\n-}\n-\n-/* Returns 1 if OP is memory operand that can not be represented by the\n-   modRM array.  */\n-\n-int\n-long_memory_operand (rtx op, enum machine_mode mode)\n-{\n-  if (! memory_operand (op, mode))\n-    return 0;\n-\n-  return memory_address_length (op) != 0;\n-}\n-\n-/* Return nonzero if the rtx is known aligned.  */\n-\n-int\n-aligned_operand (rtx op, enum machine_mode mode)\n-{\n-  struct ix86_address parts;\n-\n-  if (!general_operand (op, mode))\n-    return 0;\n-\n-  /* Registers and immediate operands are always \"aligned\".  */\n-  if (GET_CODE (op) != MEM)\n-    return 1;\n-\n-  /* Don't even try to do any aligned optimizations with volatiles.  */\n-  if (MEM_VOLATILE_P (op))\n-    return 0;\n-\n-  op = XEXP (op, 0);\n-\n-  /* Pushes and pops are only valid on the stack pointer.  */\n-  if (GET_CODE (op) == PRE_DEC\n-      || GET_CODE (op) == POST_INC)\n-    return 1;\n-\n-  /* Decode the address.  */\n-  if (! ix86_decompose_address (op, &parts))\n+  mem = XEXP (set, opnum);\n+  while (GET_CODE (mem) == SUBREG)\n+    mem = SUBREG_REG (mem);\n+  if (GET_CODE (mem) != MEM)\n     abort ();\n-\n-  /* Look for some component that isn't known to be aligned.  */\n-  if (parts.index)\n-    {\n-      if (parts.scale < 4\n-\t  && REGNO_POINTER_ALIGN (REGNO (parts.index)) < 32)\n-\treturn 0;\n-    }\n-  if (parts.base)\n-    {\n-      if (REGNO_POINTER_ALIGN (REGNO (parts.base)) < 32)\n-\treturn 0;\n-    }\n-  if (parts.disp)\n-    {\n-      if (GET_CODE (parts.disp) != CONST_INT\n-\t  || (INTVAL (parts.disp) & 3) != 0)\n-\treturn 0;\n-    }\n-\n-  /* Didn't find one -- this must be an aligned address.  */\n-  return 1;\n+  return (volatile_ok || !MEM_VOLATILE_P (mem));\n }\n \f\n /* Initialize the table of extra 80387 mathematical constants.  */\n@@ -4644,196 +3712,6 @@ ix86_can_use_return_insn_p (void)\n   return frame.to_allocate == 0 && frame.nregs == 0;\n }\n \f\n-/* Return 1 if VALUE can be stored in the sign extended immediate field.  */\n-int\n-x86_64_sign_extended_value (rtx value)\n-{\n-  switch (GET_CODE (value))\n-    {\n-      /* CONST_DOUBLES never match, since HOST_BITS_PER_WIDE_INT is known\n-         to be at least 32 and this all acceptable constants are\n-\t represented as CONST_INT.  */\n-      case CONST_INT:\n-\tif (HOST_BITS_PER_WIDE_INT == 32)\n-\t  return 1;\n-\telse\n-\t  {\n-\t    HOST_WIDE_INT val = trunc_int_for_mode (INTVAL (value), DImode);\n-\t    return trunc_int_for_mode (val, SImode) == val;\n-\t  }\n-\tbreak;\n-\n-      /* For certain code models, the symbolic references are known to fit.\n-\t in CM_SMALL_PIC model we know it fits if it is local to the shared\n-\t library.  Don't count TLS SYMBOL_REFs here, since they should fit\n-\t only if inside of UNSPEC handled below.  */\n-      case SYMBOL_REF:\n-\t/* TLS symbols are not constant.  */\n-\tif (tls_symbolic_operand (value, Pmode))\n-\t  return false;\n-\treturn (ix86_cmodel == CM_SMALL || ix86_cmodel == CM_KERNEL);\n-\n-      /* For certain code models, the code is near as well.  */\n-      case LABEL_REF:\n-\treturn (ix86_cmodel == CM_SMALL || ix86_cmodel == CM_MEDIUM\n-\t\t|| ix86_cmodel == CM_KERNEL);\n-\n-      /* We also may accept the offsetted memory references in certain special\n-         cases.  */\n-      case CONST:\n-\tif (GET_CODE (XEXP (value, 0)) == UNSPEC)\n-\t  switch (XINT (XEXP (value, 0), 1))\n-\t    {\n-\t    case UNSPEC_GOTPCREL:\n-\t    case UNSPEC_DTPOFF:\n-\t    case UNSPEC_GOTNTPOFF:\n-\t    case UNSPEC_NTPOFF:\n-\t      return 1;\n-\t    default:\n-\t      break;\n-\t    }\n-\tif (GET_CODE (XEXP (value, 0)) == PLUS)\n-\t  {\n-\t    rtx op1 = XEXP (XEXP (value, 0), 0);\n-\t    rtx op2 = XEXP (XEXP (value, 0), 1);\n-\t    HOST_WIDE_INT offset;\n-\n-\t    if (ix86_cmodel == CM_LARGE)\n-\t      return 0;\n-\t    if (GET_CODE (op2) != CONST_INT)\n-\t      return 0;\n-\t    offset = trunc_int_for_mode (INTVAL (op2), DImode);\n-\t    switch (GET_CODE (op1))\n-\t      {\n-\t\tcase SYMBOL_REF:\n-\t\t  /* For CM_SMALL assume that latest object is 16MB before\n-\t\t     end of 31bits boundary.  We may also accept pretty\n-\t\t     large negative constants knowing that all objects are\n-\t\t     in the positive half of address space.  */\n-\t\t  if (ix86_cmodel == CM_SMALL\n-\t\t      && offset < 16*1024*1024\n-\t\t      && trunc_int_for_mode (offset, SImode) == offset)\n-\t\t    return 1;\n-\t\t  /* For CM_KERNEL we know that all object resist in the\n-\t\t     negative half of 32bits address space.  We may not\n-\t\t     accept negative offsets, since they may be just off\n-\t\t     and we may accept pretty large positive ones.  */\n-\t\t  if (ix86_cmodel == CM_KERNEL\n-\t\t      && offset > 0\n-\t\t      && trunc_int_for_mode (offset, SImode) == offset)\n-\t\t    return 1;\n-\t\t  break;\n-\t\tcase LABEL_REF:\n-\t\t  /* These conditions are similar to SYMBOL_REF ones, just the\n-\t\t     constraints for code models differ.  */\n-\t\t  if ((ix86_cmodel == CM_SMALL || ix86_cmodel == CM_MEDIUM)\n-\t\t      && offset < 16*1024*1024\n-\t\t      && trunc_int_for_mode (offset, SImode) == offset)\n-\t\t    return 1;\n-\t\t  if (ix86_cmodel == CM_KERNEL\n-\t\t      && offset > 0\n-\t\t      && trunc_int_for_mode (offset, SImode) == offset)\n-\t\t    return 1;\n-\t\t  break;\n-\t\tcase UNSPEC:\n-\t\t  switch (XINT (op1, 1))\n-\t\t    {\n-\t\t    case UNSPEC_DTPOFF:\n-\t\t    case UNSPEC_NTPOFF:\n-\t\t      if (offset > 0\n-\t\t\t  && trunc_int_for_mode (offset, SImode) == offset)\n-\t\t\treturn 1;\n-\t\t    }\n-\t\t  break;\n-\t\tdefault:\n-\t\t  return 0;\n-\t      }\n-\t  }\n-\treturn 0;\n-      default:\n-\treturn 0;\n-    }\n-}\n-\n-/* Return 1 if VALUE can be stored in the zero extended immediate field.  */\n-int\n-x86_64_zero_extended_value (rtx value)\n-{\n-  switch (GET_CODE (value))\n-    {\n-      case CONST_DOUBLE:\n-\tif (HOST_BITS_PER_WIDE_INT == 32)\n-\t  return  (GET_MODE (value) == VOIDmode\n-\t\t   && !CONST_DOUBLE_HIGH (value));\n-\telse\n-\t  return 0;\n-      case CONST_INT:\n-\tif (HOST_BITS_PER_WIDE_INT == 32)\n-\t  return INTVAL (value) >= 0;\n-\telse\n-\t  return !(INTVAL (value) & ~(HOST_WIDE_INT) 0xffffffff);\n-\tbreak;\n-\n-      /* For certain code models, the symbolic references are known to fit.  */\n-      case SYMBOL_REF:\n-\t/* TLS symbols are not constant.  */\n-\tif (tls_symbolic_operand (value, Pmode))\n-\t  return false;\n-\treturn ix86_cmodel == CM_SMALL;\n-\n-      /* For certain code models, the code is near as well.  */\n-      case LABEL_REF:\n-\treturn ix86_cmodel == CM_SMALL || ix86_cmodel == CM_MEDIUM;\n-\n-      /* We also may accept the offsetted memory references in certain special\n-         cases.  */\n-      case CONST:\n-\tif (GET_CODE (XEXP (value, 0)) == PLUS)\n-\t  {\n-\t    rtx op1 = XEXP (XEXP (value, 0), 0);\n-\t    rtx op2 = XEXP (XEXP (value, 0), 1);\n-\n-\t    if (ix86_cmodel == CM_LARGE)\n-\t      return 0;\n-\t    switch (GET_CODE (op1))\n-\t      {\n-\t\tcase SYMBOL_REF:\n-\t\t    return 0;\n-\t\t  /* For small code model we may accept pretty large positive\n-\t\t     offsets, since one bit is available for free.  Negative\n-\t\t     offsets are limited by the size of NULL pointer area\n-\t\t     specified by the ABI.  */\n-\t\t  if (ix86_cmodel == CM_SMALL\n-\t\t      && GET_CODE (op2) == CONST_INT\n-\t\t      && trunc_int_for_mode (INTVAL (op2), DImode) > -0x10000\n-\t\t      && (trunc_int_for_mode (INTVAL (op2), SImode)\n-\t\t\t  == INTVAL (op2)))\n-\t\t    return 1;\n-\t          /* ??? For the kernel, we may accept adjustment of\n-\t\t     -0x10000000, since we know that it will just convert\n-\t\t     negative address space to positive, but perhaps this\n-\t\t     is not worthwhile.  */\n-\t\t  break;\n-\t\tcase LABEL_REF:\n-\t\t  /* These conditions are similar to SYMBOL_REF ones, just the\n-\t\t     constraints for code models differ.  */\n-\t\t  if ((ix86_cmodel == CM_SMALL || ix86_cmodel == CM_MEDIUM)\n-\t\t      && GET_CODE (op2) == CONST_INT\n-\t\t      && trunc_int_for_mode (INTVAL (op2), DImode) > -0x10000\n-\t\t      && (trunc_int_for_mode (INTVAL (op2), SImode)\n-\t\t\t  == INTVAL (op2)))\n-\t\t    return 1;\n-\t\t  break;\n-\t\tdefault:\n-\t\t  return 0;\n-\t      }\n-\t  }\n-\treturn 0;\n-      default:\n-\treturn 0;\n-    }\n-}\n-\n /* Value should be nonzero if functions must have frame pointers.\n    Zero means the frame pointer need not be set up (and parms may\n    be accessed via the stack pointer) in functions that seem suitable.  */\n@@ -5664,7 +4542,7 @@ ix86_output_function_epilogue (FILE *file ATTRIBUTE_UNUSED,\n    grossly off.  Return -1 if the address contains ASHIFT, so it is not\n    strictly valid, but still used for computing length of lea instruction.  */\n \n-static int\n+int\n ix86_decompose_address (rtx addr, struct ix86_address *out)\n {\n   rtx base = NULL_RTX;\n@@ -6328,7 +5206,8 @@ legitimate_address_p (enum machine_mode mode, rtx addr, int strict)\n \t  reason = \"displacement is not constant\";\n \t  goto report_error;\n \t}\n-      else if (TARGET_64BIT && !x86_64_sign_extended_value (disp))\n+      else if (TARGET_64BIT\n+\t       && !x86_64_immediate_operand (disp, VOIDmode))\n \t{\n \t  reason = \"displacement is out of range\";\n \t  goto report_error;\n@@ -6705,12 +5584,13 @@ legitimize_address (rtx x, rtx oldx ATTRIBUTE_UNUSED, enum machine_mode mode)\n       debug_rtx (x);\n     }\n \n-  log = tls_symbolic_operand (x, mode);\n+  log = GET_CODE (x) == SYMBOL_REF ? SYMBOL_REF_TLS_MODEL (x) : 0;\n   if (log)\n     return legitimize_tls_address (x, log, false);\n   if (GET_CODE (x) == CONST\n       && GET_CODE (XEXP (x, 0)) == PLUS\n-      && (log = tls_symbolic_operand (XEXP (XEXP (x, 0), 0), Pmode)))\n+      && GET_CODE (XEXP (XEXP (x, 0), 0)) == SYMBOL_REF\n+      && (log = SYMBOL_REF_TLS_MODEL (XEXP (XEXP (x, 0), 0))))\n     {\n       rtx t = legitimize_tls_address (XEXP (XEXP (x, 0), 0), log, false);\n       return gen_rtx_PLUS (Pmode, t, XEXP (XEXP (x, 0), 1));\n@@ -8457,7 +7337,7 @@ ix86_expand_clear (rtx dest)\n /* X is an unchanging MEM.  If it is a constant pool reference, return\n    the constant pool rtx, else NULL.  */\n \n-static rtx\n+rtx\n maybe_get_pool_constant (rtx x)\n {\n   x = ix86_delegitimize_address (XEXP (x, 0));\n@@ -8478,7 +7358,7 @@ ix86_expand_move (enum machine_mode mode, rtx operands[])\n   op0 = operands[0];\n   op1 = operands[1];\n \n-  model = tls_symbolic_operand (op1, Pmode);\n+  model = GET_CODE (op1) == SYMBOL_REF ? SYMBOL_REF_TLS_MODEL (op1) : 0;\n   if (model)\n     {\n       op1 = legitimize_tls_address (op1, model, true);\n@@ -8527,7 +7407,7 @@ ix86_expand_move (enum machine_mode mode, rtx operands[])\n \t to get them CSEed.  */\n       if (TARGET_64BIT && mode == DImode\n \t  && immediate_operand (op1, mode)\n-\t  && !x86_64_zero_extended_value (op1)\n+\t  && !x86_64_zext_immediate_operand (op1, VOIDmode)\n \t  && !register_operand (op0, mode)\n \t  && optimize && !reload_completed && !reload_in_progress)\n \top1 = copy_to_mode_reg (mode, op1);\n@@ -9045,7 +7925,8 @@ ix86_prepare_fp_compare_args (enum rtx_code code, rtx *pop0, rtx *pop1)\n /* Convert comparison codes we use to represent FP comparison to integer\n    code that will result in proper branch.  Return UNKNOWN if no such code\n    is available.  */\n-static enum rtx_code\n+\n+enum rtx_code\n ix86_fp_compare_code_to_integer (enum rtx_code code)\n {\n   switch (code)\n@@ -9080,7 +7961,8 @@ ix86_fp_compare_code_to_integer (enum rtx_code code)\n    branch around FIRST_CODE and SECOND_CODE.  If some of branches\n    is not required, set value to NIL.\n    We never require more than two branches.  */\n-static void\n+\n+void\n ix86_fp_comparison_codes (enum rtx_code code, enum rtx_code *bypass_code,\n \t\t\t  enum rtx_code *first_code,\n \t\t\t  enum rtx_code *second_code)\n@@ -10142,7 +9024,8 @@ ix86_expand_int_movcc (rtx operands[])\n       if ((diff == 1 || diff == 2 || diff == 4 || diff == 8\n \t   || diff == 3 || diff == 5 || diff == 9)\n \t  && ((mode != QImode && mode != HImode) || !TARGET_PARTIAL_REG_STALL)\n-\t  && (mode != DImode || x86_64_sign_extended_value (GEN_INT (cf))))\n+\t  && (mode != DImode\n+\t      || x86_64_immediate_operand (GEN_INT (cf), VOIDmode)))\n \t{\n \t  /*\n \t   * xorl dest,dest\n@@ -11220,8 +10103,9 @@ ix86_expand_movmem (rtx dst, rtx src, rtx count_exp, rtx align_exp)\n   /* Figure out proper mode for counter.  For 32bits it is always SImode,\n      for 64bits use SImode when possible, otherwise DImode.\n      Set count to number of bytes copied when known at compile time.  */\n-  if (!TARGET_64BIT || GET_MODE (count_exp) == SImode\n-      || x86_64_zero_extended_value (count_exp))\n+  if (!TARGET_64BIT\n+      || GET_MODE (count_exp) == SImode\n+      || x86_64_zext_immediate_operand (count_exp, VOIDmode))\n     counter_mode = SImode;\n   else\n     counter_mode = DImode;\n@@ -11498,8 +10382,9 @@ ix86_expand_clrmem (rtx dst, rtx count_exp, rtx align_exp)\n   /* Figure out proper mode for counter.  For 32bits it is always SImode,\n      for 64bits use SImode when possible, otherwise DImode.\n      Set count to number of bytes copied when known at compile time.  */\n-  if (!TARGET_64BIT || GET_MODE (count_exp) == SImode\n-      || x86_64_zero_extended_value (count_exp))\n+  if (!TARGET_64BIT\n+      || GET_MODE (count_exp) == SImode\n+      || x86_64_zext_immediate_operand (count_exp, VOIDmode))\n     counter_mode = SImode;\n   else\n     counter_mode = DImode;\n@@ -12127,7 +11012,7 @@ ix86_tls_get_addr (void)\n /* Calculate the length of the memory address in the instruction\n    encoding.  Does not include the one-byte modrm, opcode, or prefix.  */\n \n-static int\n+int\n memory_address_length (rtx addr)\n {\n   struct ix86_address parts;\n@@ -12701,7 +11586,7 @@ x86_initialize_trampoline (rtx tramp, rtx fnaddr, rtx cxt)\n       /* Try to load address using shorter movl instead of movabs.\n          We may want to support movq for kernel mode, but kernel does not use\n          trampolines at the moment.  */\n-      if (x86_64_zero_extended_value (fnaddr))\n+      if (x86_64_zext_immediate_operand (fnaddr, VOIDmode))\n \t{\n \t  fnaddr = copy_to_mode_reg (DImode, fnaddr);\n \t  emit_move_insn (gen_rtx_MEM (HImode, plus_constant (tramp, offset)),\n@@ -14984,9 +13869,9 @@ ix86_rtx_costs (rtx x, int code, int outer_code, int *total)\n     case CONST:\n     case LABEL_REF:\n     case SYMBOL_REF:\n-      if (TARGET_64BIT && !x86_64_sign_extended_value (x))\n+      if (TARGET_64BIT && !x86_64_immediate_operand (x, VOIDmode))\n \t*total = 3;\n-      else if (TARGET_64BIT && !x86_64_zero_extended_value (x))\n+      else if (TARGET_64BIT && !x86_64_zext_immediate_operand (x, VOIDmode))\n \t*total = 2;\n       else if (flag_pic && SYMBOLIC_CONST (x)\n \t       && (!TARGET_64BIT"}, {"sha": "742575c978771958841b7a506b6d814fddb76300", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 4, "deletions": 92, "changes": 96, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8fe75e43623a88a049f0f33191fdffce583a69c3/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8fe75e43623a88a049f0f33191fdffce583a69c3/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=8fe75e43623a88a049f0f33191fdffce583a69c3", "patch": "@@ -1502,10 +1502,10 @@ enum reg_class\n    the constraint letter C.  If C is not defined as an extra\n    constraint, the value returned should be 0 regardless of VALUE.  */\n \n-#define EXTRA_CONSTRAINT(VALUE, D)\t\t\t\t\\\n-  ((D) == 'e' ? x86_64_sign_extended_value (VALUE)\t\t\\\n-   : (D) == 'Z' ? x86_64_zero_extended_value (VALUE)\t\t\\\n-   : (D) == 'C' ? standard_sse_constant_p (VALUE)\t\t\\\n+#define EXTRA_CONSTRAINT(VALUE, D)\t\t\t\t\t\\\n+  ((D) == 'e' ? x86_64_immediate_operand (VALUE, VOIDmode)\t\t\\\n+   : (D) == 'Z' ? x86_64_zext_immediate_operand (VALUE, VOIDmode)\t\\\n+   : (D) == 'C' ? standard_sse_constant_p (VALUE)\t\t\t\\\n    : 0)\n \n /* Place additional restrictions on the register class to use when it\n@@ -2830,94 +2830,6 @@ do {\t\t\t\t\t\t\\\n #define RET return \"\"\n #define AT_SP(MODE) (gen_rtx_MEM ((MODE), stack_pointer_rtx))\n \f\n-/* Define the codes that are matched by predicates in i386.c.  */\n-\n-#define PREDICATE_CODES\t\t\t\t\t\t\t\\\n-  {\"x86_64_immediate_operand\", {CONST_INT, SUBREG, REG,\t\t\t\\\n-\t\t\t\tSYMBOL_REF, LABEL_REF, CONST}},\t\t\\\n-  {\"x86_64_nonmemory_operand\", {CONST_INT, SUBREG, REG,\t\t\t\\\n-\t\t\t\tSYMBOL_REF, LABEL_REF, CONST}},\t\t\\\n-  {\"x86_64_movabs_operand\", {CONST_INT, SUBREG, REG,\t\t\t\\\n-\t\t\t\tSYMBOL_REF, LABEL_REF, CONST}},\t\t\\\n-  {\"x86_64_szext_nonmemory_operand\", {CONST_INT, SUBREG, REG,\t\t\\\n-\t\t\t\t     SYMBOL_REF, LABEL_REF, CONST}},\t\\\n-  {\"x86_64_general_operand\", {CONST_INT, SUBREG, REG, MEM,\t\t\\\n-\t\t\t      SYMBOL_REF, LABEL_REF, CONST}},\t\t\\\n-  {\"x86_64_szext_general_operand\", {CONST_INT, SUBREG, REG, MEM,\t\\\n-\t\t\t\t   SYMBOL_REF, LABEL_REF, CONST}},\t\\\n-  {\"x86_64_zext_immediate_operand\", {CONST_INT, CONST_DOUBLE, CONST,\t\\\n-\t\t\t\t       SYMBOL_REF, LABEL_REF}},\t\t\\\n-  {\"shiftdi_operand\", {SUBREG, REG, MEM}},\t\t\t\t\\\n-  {\"const_int_1_31_operand\", {CONST_INT}},\t\t\t\t\\\n-  {\"symbolic_operand\", {SYMBOL_REF, LABEL_REF, CONST}},\t\t\t\\\n-  {\"aligned_operand\", {CONST_INT, CONST_DOUBLE, CONST, SYMBOL_REF,\t\\\n-\t\t       LABEL_REF, SUBREG, REG, MEM}},\t\t\t\\\n-  {\"pic_symbolic_operand\", {CONST}},\t\t\t\t\t\\\n-  {\"call_insn_operand\", {REG, SUBREG, MEM, SYMBOL_REF}},\t\t\\\n-  {\"sibcall_insn_operand\", {REG, SUBREG, SYMBOL_REF}},\t\t\t\\\n-  {\"constant_call_address_operand\", {SYMBOL_REF, CONST}},\t\t\\\n-  {\"const0_operand\", {CONST_INT, CONST_DOUBLE}},\t\t\t\\\n-  {\"const1_operand\", {CONST_INT}},\t\t\t\t\t\\\n-  {\"const248_operand\", {CONST_INT}},\t\t\t\t\t\\\n-  {\"const_0_to_3_operand\", {CONST_INT}},\t\t\t\t\\\n-  {\"const_0_to_7_operand\", {CONST_INT}},\t\t\t\t\\\n-  {\"const_0_to_15_operand\", {CONST_INT}},\t\t\t\t\\\n-  {\"const_0_to_255_operand\", {CONST_INT}},\t\t\t\t\\\n-  {\"incdec_operand\", {CONST_INT}},\t\t\t\t\t\\\n-  {\"mmx_reg_operand\", {REG}},\t\t\t\t\t\t\\\n-  {\"reg_no_sp_operand\", {SUBREG, REG}},\t\t\t\t\t\\\n-  {\"general_no_elim_operand\", {CONST_INT, CONST_DOUBLE, CONST,\t\t\\\n-\t\t\tSYMBOL_REF, LABEL_REF, SUBREG, REG, MEM}},\t\\\n-  {\"nonmemory_no_elim_operand\", {CONST_INT, REG, SUBREG}},\t\t\\\n-  {\"index_register_operand\", {SUBREG, REG}},\t\t\t\t\\\n-  {\"flags_reg_operand\", {REG}},\t\t\t\t\t\t\\\n-  {\"q_regs_operand\", {SUBREG, REG}},\t\t\t\t\t\\\n-  {\"non_q_regs_operand\", {SUBREG, REG}},\t\t\t\t\\\n-  {\"fcmov_comparison_operator\", {EQ, NE, LTU, GTU, LEU, GEU, UNORDERED, \\\n-\t\t\t\t ORDERED, LT, UNLT, GT, UNGT, LE, UNLE,\t\\\n-\t\t\t\t GE, UNGE, LTGT, UNEQ}},\t\t\\\n-  {\"sse_comparison_operator\", {EQ, LT, LE, UNORDERED, NE, UNGE, UNGT,\t\\\n-\t\t\t       ORDERED, UNEQ, UNLT, UNLE, LTGT, GE, GT\t\\\n-\t\t\t       }},\t\t\t\t\t\\\n-  {\"ix86_comparison_operator\", {EQ, NE, LE, LT, GE, GT, LEU, LTU, GEU,\t\\\n-\t\t\t       GTU, UNORDERED, ORDERED, UNLE, UNLT,\t\\\n-\t\t\t       UNGE, UNGT, LTGT, UNEQ }},\t\t\\\n-  {\"ix86_carry_flag_operator\", {LTU, LT, UNLT, GT, UNGT, LE, UNLE,\t\\\n-\t\t\t\t GE, UNGE, LTGT, UNEQ}},\t\t\\\n-  {\"cmp_fp_expander_operand\", {CONST_DOUBLE, SUBREG, REG, MEM}},\t\\\n-  {\"ext_register_operand\", {SUBREG, REG}},\t\t\t\t\\\n-  {\"binary_fp_operator\", {PLUS, MINUS, MULT, DIV}},\t\t\t\\\n-  {\"mult_operator\", {MULT}},\t\t\t\t\t\t\\\n-  {\"div_operator\", {DIV}},\t\t\t\t\t\t\\\n-  {\"arith_or_logical_operator\", {PLUS, MULT, AND, IOR, XOR, SMIN, SMAX, \\\n-\t\t\t\t UMIN, UMAX, COMPARE, MINUS, DIV, MOD,\t\\\n-\t\t\t\t UDIV, UMOD, ASHIFT, ROTATE, ASHIFTRT,\t\\\n-\t\t\t\t LSHIFTRT, ROTATERT}},\t\t\t\\\n-  {\"promotable_binary_operator\", {PLUS, MULT, AND, IOR, XOR, ASHIFT}},\t\\\n-  {\"memory_displacement_operand\", {MEM}},\t\t\t\t\\\n-  {\"cmpsi_operand\", {CONST_INT, CONST_DOUBLE, CONST, SYMBOL_REF,\t\\\n-\t\t     LABEL_REF, SUBREG, REG, MEM, AND}},\t\t\\\n-  {\"long_memory_operand\", {MEM}},\t\t\t\t\t\\\n-  {\"tls_symbolic_operand\", {SYMBOL_REF}},\t\t\t\t\\\n-  {\"global_dynamic_symbolic_operand\", {SYMBOL_REF}},\t\t\t\\\n-  {\"local_dynamic_symbolic_operand\", {SYMBOL_REF}},\t\t\t\\\n-  {\"initial_exec_symbolic_operand\", {SYMBOL_REF}},\t\t\t\\\n-  {\"local_exec_symbolic_operand\", {SYMBOL_REF}},\t\t\t\\\n-  {\"any_fp_register_operand\", {REG}},\t\t\t\t\t\\\n-  {\"register_and_not_any_fp_reg_operand\", {REG}},\t\t\t\\\n-  {\"fp_register_operand\", {REG}},\t\t\t\t\t\\\n-  {\"register_and_not_fp_reg_operand\", {REG}},\t\t\t\t\\\n-  {\"zero_extended_scalar_load_operand\", {MEM}},\t\t\t\t\\\n-  {\"vector_move_operand\", {CONST_VECTOR, SUBREG, REG, MEM}},\t\t\\\n-  {\"no_seg_address_operand\", {CONST_INT, CONST_DOUBLE, CONST, SYMBOL_REF, \\\n-\t\t\t      LABEL_REF, SUBREG, REG, MEM, PLUS, MULT}},\n-\n-/* A list of predicates that do special things with modes, and so\n-   should not elicit warnings for VOIDmode match_operand.  */\n-\n-#define SPECIAL_MODE_PREDICATES \\\n-  \"ext_register_operand\",\n-\f\n /* Which processor to schedule for. The cpu attribute defines a list that\n    mirrors this list, so changes to i386.md must be made at the same time.  */\n "}, {"sha": "70ef6c9a12aaffc3195a8a28417624e0536d8a73", "filename": "gcc/config/i386/i386.md", "status": "modified", "additions": 8, "deletions": 1, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8fe75e43623a88a049f0f33191fdffce583a69c3/gcc%2Fconfig%2Fi386%2Fi386.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8fe75e43623a88a049f0f33191fdffce583a69c3/gcc%2Fconfig%2Fi386%2Fi386.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.md?ref=8fe75e43623a88a049f0f33191fdffce583a69c3", "patch": "@@ -1,4 +1,3 @@\n-\n ;; GCC machine description for IA-32 and x86-64.\n ;; Copyright (C) 1988, 1994, 1995, 1996, 1997, 1998, 1999, 2000,\n ;; 2001, 2002, 2003, 2004\n@@ -426,10 +425,18 @@\n   [(set_attr \"length\" \"128\")\n    (set_attr \"type\" \"multi\")])\n \f\n+;; Scheduling descriptions\n+\n (include \"pentium.md\")\n (include \"ppro.md\")\n (include \"k6.md\")\n (include \"athlon.md\")\n+\n+\f\n+;; Operand and operator predicates\n+\n+(include \"predicates.md\")\n+\n \f\n ;; Compare instructions.\n "}, {"sha": "27f2e4271aac54a17e1173b9a4ec46ba5f6a2d08", "filename": "gcc/config/i386/predicates.md", "status": "added", "additions": 820, "deletions": 0, "changes": 820, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/8fe75e43623a88a049f0f33191fdffce583a69c3/gcc%2Fconfig%2Fi386%2Fpredicates.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/8fe75e43623a88a049f0f33191fdffce583a69c3/gcc%2Fconfig%2Fi386%2Fpredicates.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fpredicates.md?ref=8fe75e43623a88a049f0f33191fdffce583a69c3", "patch": "@@ -0,0 +1,820 @@\n+;; Predicate definitions for IA-32 and x86-64.\n+;; Copyright (C) 2004 Free Software Foundation, Inc.\n+;;\n+;; This file is part of GCC.\n+;;\n+;; GCC is free software; you can redistribute it and/or modify\n+;; it under the terms of the GNU General Public License as published by\n+;; the Free Software Foundation; either version 2, or (at your option)\n+;; any later version.\n+;;\n+;; GCC is distributed in the hope that it will be useful,\n+;; but WITHOUT ANY WARRANTY; without even the implied warranty of\n+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+;; GNU General Public License for more details.\n+;;\n+;; You should have received a copy of the GNU General Public License\n+;; along with GCC; see the file COPYING.  If not, write to\n+;; the Free Software Foundation, 59 Temple Place - Suite 330,\n+;; Boston, MA 02111-1307, USA.\n+\n+;; Return nonzero if OP is either a i387 or SSE fp register.\n+(define_predicate \"any_fp_register_operand\"\n+  (and (match_code \"reg\")\n+       (match_test \"ANY_FP_REGNO_P (REGNO (op))\")))\n+\n+;; Return nonzero if OP is an i387 fp register.\n+(define_predicate \"fp_register_operand\"\n+  (and (match_code \"reg\")\n+       (match_test \"FP_REGNO_P (REGNO (op))\")))\n+\n+;; Return nonzero if OP is a non-fp register_operand.\n+(define_predicate \"register_and_not_any_fp_reg_operand\"\n+  (and (match_code \"reg\")\n+       (not (match_test \"ANY_FP_REGNO_P (REGNO (op))\"))))\n+\n+;; Return nonzero if OP is a register operand other than an i387 fp register.\n+(define_predicate \"register_and_not_fp_reg_operand\"\n+  (and (match_code \"reg\")\n+       (not (match_test \"FP_REGNO_P (REGNO (op))\"))))\n+\n+;; True if the operand is an MMX register.\n+(define_predicate \"mmx_reg_operand\"\n+  (and (match_code \"reg\")\n+       (match_test \"MMX_REGNO_P (REGNO (op))\")))\n+\n+;; True if the operand is a Q_REGS class register.\n+(define_predicate \"q_regs_operand\"\n+  (match_operand 0 \"register_operand\")\n+{\n+  if (GET_CODE (op) == SUBREG)\n+    op = SUBREG_REG (op);\n+  return ANY_QI_REG_P (op);\n+})\n+\n+;; Return true if op is a NON_Q_REGS class register.\n+(define_predicate \"non_q_regs_operand\"\n+  (match_operand 0 \"register_operand\")\n+{\n+  if (GET_CODE (op) == SUBREG)\n+    op = SUBREG_REG (op);\n+  return NON_QI_REG_P (op);\n+})\n+\n+;; Match an SI or HImode register for a zero_extract.\n+(define_special_predicate \"ext_register_operand\"\n+  (match_operand 0 \"register_operand\")\n+{\n+  if ((!TARGET_64BIT || GET_MODE (op) != DImode)\n+      && GET_MODE (op) != SImode && GET_MODE (op) != HImode)\n+    return 0;\n+  if (GET_CODE (op) == SUBREG)\n+    op = SUBREG_REG (op);\n+\n+  /* Be careful to accept only registers having upper parts.  */\n+  return REGNO (op) > LAST_VIRTUAL_REGISTER || REGNO (op) < 4;\n+})\n+\n+;; Return true if op is the flags register.\n+(define_predicate \"flags_reg_operand\"\n+  (and (match_code \"reg\")\n+       (match_test \"REGNO (op) == FLAGS_REG\")))\n+\n+;; Return 1 if VALUE can be stored in a sign extended immediate field.\n+(define_predicate \"x86_64_immediate_operand\"\n+  (match_code \"const_int,symbol_ref,label_ref,const\")\n+{\n+  if (!TARGET_64BIT)\n+    return immediate_operand (op, mode);\n+\n+  switch (GET_CODE (op))\n+    {\n+    case CONST_INT:\n+      /* CONST_DOUBLES never match, since HOST_BITS_PER_WIDE_INT is known\n+         to be at least 32 and this all acceptable constants are\n+\t represented as CONST_INT.  */\n+      if (HOST_BITS_PER_WIDE_INT == 32)\n+\treturn 1;\n+      else\n+\t{\n+\t  HOST_WIDE_INT val = trunc_int_for_mode (INTVAL (op), DImode);\n+\t  return trunc_int_for_mode (val, SImode) == val;\n+\t}\n+      break;\n+\n+    case SYMBOL_REF:\n+      /* For certain code models, the symbolic references are known to fit.\n+\t in CM_SMALL_PIC model we know it fits if it is local to the shared\n+\t library.  Don't count TLS SYMBOL_REFs here, since they should fit\n+\t only if inside of UNSPEC handled below.  */\n+      /* TLS symbols are not constant.  */\n+      if (tls_symbolic_operand (op, Pmode))\n+\treturn false;\n+      return (ix86_cmodel == CM_SMALL || ix86_cmodel == CM_KERNEL);\n+\n+    case LABEL_REF:\n+      /* For certain code models, the code is near as well.  */\n+      return (ix86_cmodel == CM_SMALL || ix86_cmodel == CM_MEDIUM\n+\t      || ix86_cmodel == CM_KERNEL);\n+\n+    case CONST:\n+      /* We also may accept the offsetted memory references in certain\n+\t special cases.  */\n+      if (GET_CODE (XEXP (op, 0)) == UNSPEC)\n+\tswitch (XINT (XEXP (op, 0), 1))\n+\t  {\n+\t  case UNSPEC_GOTPCREL:\n+\t  case UNSPEC_DTPOFF:\n+\t  case UNSPEC_GOTNTPOFF:\n+\t  case UNSPEC_NTPOFF:\n+\t    return 1;\n+\t  default:\n+\t    break;\n+\t  }\n+\n+      if (GET_CODE (XEXP (op, 0)) == PLUS)\n+\t{\n+\t  rtx op1 = XEXP (XEXP (op, 0), 0);\n+\t  rtx op2 = XEXP (XEXP (op, 0), 1);\n+\t  HOST_WIDE_INT offset;\n+\n+\t  if (ix86_cmodel == CM_LARGE)\n+\t    return 0;\n+\t  if (GET_CODE (op2) != CONST_INT)\n+\t    return 0;\n+\t  offset = trunc_int_for_mode (INTVAL (op2), DImode);\n+\t  switch (GET_CODE (op1))\n+\t    {\n+\t    case SYMBOL_REF:\n+\t      /* For CM_SMALL assume that latest object is 16MB before\n+\t\t end of 31bits boundary.  We may also accept pretty\n+\t\t large negative constants knowing that all objects are\n+\t\t in the positive half of address space.  */\n+\t      if (ix86_cmodel == CM_SMALL\n+\t\t  && offset < 16*1024*1024\n+\t\t  && trunc_int_for_mode (offset, SImode) == offset)\n+\t\treturn 1;\n+\t      /* For CM_KERNEL we know that all object resist in the\n+\t\t negative half of 32bits address space.  We may not\n+\t\t accept negative offsets, since they may be just off\n+\t\t and we may accept pretty large positive ones.  */\n+\t      if (ix86_cmodel == CM_KERNEL\n+\t\t  && offset > 0\n+\t\t  && trunc_int_for_mode (offset, SImode) == offset)\n+\t\treturn 1;\n+\t      break;\n+\n+\t    case LABEL_REF:\n+\t      /* These conditions are similar to SYMBOL_REF ones, just the\n+\t\t constraints for code models differ.  */\n+\t      if ((ix86_cmodel == CM_SMALL || ix86_cmodel == CM_MEDIUM)\n+\t\t  && offset < 16*1024*1024\n+\t\t  && trunc_int_for_mode (offset, SImode) == offset)\n+\t\treturn 1;\n+\t      if (ix86_cmodel == CM_KERNEL\n+\t\t  && offset > 0\n+\t\t  && trunc_int_for_mode (offset, SImode) == offset)\n+\t\treturn 1;\n+\t      break;\n+\n+\t    case UNSPEC:\n+\t      switch (XINT (op1, 1))\n+\t\t{\n+\t\tcase UNSPEC_DTPOFF:\n+\t\tcase UNSPEC_NTPOFF:\n+\t\t  if (offset > 0\n+\t\t      && trunc_int_for_mode (offset, SImode) == offset)\n+\t\t    return 1;\n+\t\t}\n+\t      break;\n+\n+\t    default:\n+\t      break;\n+\t    }\n+\t}\n+      break;\n+\n+      default:\n+\tabort ();\n+    }\n+\n+  return 0;\n+})\n+\n+;; Return 1 if VALUE can be stored in the zero extended immediate field.\n+(define_predicate \"x86_64_zext_immediate_operand\"\n+  (match_code \"const_double,const_int,symbol_ref,label_ref,const\")\n+{\n+  switch (GET_CODE (op))\n+    {\n+    case CONST_DOUBLE:\n+      if (HOST_BITS_PER_WIDE_INT == 32)\n+\treturn (GET_MODE (op) == VOIDmode && !CONST_DOUBLE_HIGH (op));\n+      else\n+\treturn 0;\n+\n+    case CONST_INT:\n+      if (HOST_BITS_PER_WIDE_INT == 32)\n+\treturn INTVAL (op) >= 0;\n+      else\n+\treturn !(INTVAL (op) & ~(HOST_WIDE_INT) 0xffffffff);\n+\n+    case SYMBOL_REF:\n+      /* For certain code models, the symbolic references are known to fit.  */\n+      /* TLS symbols are not constant.  */\n+      if (tls_symbolic_operand (op, Pmode))\n+\treturn false;\n+      return ix86_cmodel == CM_SMALL;\n+\n+    case LABEL_REF:\n+      /* For certain code models, the code is near as well.  */\n+      return ix86_cmodel == CM_SMALL || ix86_cmodel == CM_MEDIUM;\n+\n+    case CONST:\n+      /* We also may accept the offsetted memory references in certain\n+\t special cases.  */\n+      if (GET_CODE (XEXP (op, 0)) == PLUS)\n+\t{\n+\t  rtx op1 = XEXP (XEXP (op, 0), 0);\n+\t  rtx op2 = XEXP (XEXP (op, 0), 1);\n+\n+\t  if (ix86_cmodel == CM_LARGE)\n+\t    return 0;\n+\t  switch (GET_CODE (op1))\n+\t    {\n+\t    case SYMBOL_REF:\n+\t      /* For small code model we may accept pretty large positive\n+\t\t offsets, since one bit is available for free.  Negative\n+\t\t offsets are limited by the size of NULL pointer area\n+\t\t specified by the ABI.  */\n+\t      if (ix86_cmodel == CM_SMALL\n+\t\t  && GET_CODE (op2) == CONST_INT\n+\t\t  && trunc_int_for_mode (INTVAL (op2), DImode) > -0x10000\n+\t\t  && trunc_int_for_mode (INTVAL (op2), SImode) == INTVAL (op2))\n+\t\treturn 1;\n+\t      /* ??? For the kernel, we may accept adjustment of\n+\t\t -0x10000000, since we know that it will just convert\n+\t\t negative address space to positive, but perhaps this\n+\t\t is not worthwhile.  */\n+\t      break;\n+\n+\t    case LABEL_REF:\n+\t      /* These conditions are similar to SYMBOL_REF ones, just the\n+\t\t constraints for code models differ.  */\n+\t      if ((ix86_cmodel == CM_SMALL || ix86_cmodel == CM_MEDIUM)\n+\t\t  && GET_CODE (op2) == CONST_INT\n+\t\t  && trunc_int_for_mode (INTVAL (op2), DImode) > -0x10000\n+\t\t  && trunc_int_for_mode (INTVAL (op2), SImode) == INTVAL (op2))\n+\t\treturn 1;\n+\t      break;\n+\n+\t    default:\n+\t      return 0;\n+\t    }\n+\t}\n+      break;\n+\n+    default:\n+      abort ();\n+    }\n+  return 0;\n+})\n+\n+;; Return nonzero if OP is general operand representable on x86_64.\n+(define_predicate \"x86_64_general_operand\"\n+  (if_then_else (match_test \"TARGET_64BIT\")\n+    (ior (match_operand 0 \"nonimmediate_operand\")\n+\t (match_operand 0 \"x86_64_immediate_operand\"))\n+    (match_operand 0 \"general_operand\")))\n+\n+;; Return nonzero if OP is general operand representable on x86_64\n+;; as either sign extended or zero extended constant.\n+(define_predicate \"x86_64_szext_general_operand\"\n+  (if_then_else (match_test \"TARGET_64BIT\")\n+    (ior (match_operand 0 \"nonimmediate_operand\")\n+\t (ior (match_operand 0 \"x86_64_immediate_operand\")\n+\t      (match_operand 0 \"x86_64_zext_immediate_operand\")))\n+    (match_operand 0 \"general_operand\")))\n+\n+;; Return nonzero if OP is nonmemory operand representable on x86_64.\n+(define_predicate \"x86_64_nonmemory_operand\"\n+  (if_then_else (match_test \"TARGET_64BIT\")\n+    (ior (match_operand 0 \"register_operand\")\n+\t (match_operand 0 \"x86_64_immediate_operand\"))\n+    (match_operand 0 \"nonmemory_operand\")))\n+\n+;; Return nonzero if OP is nonmemory operand representable on x86_64.\n+(define_predicate \"x86_64_szext_nonmemory_operand\"\n+  (if_then_else (match_test \"TARGET_64BIT\")\n+    (ior (match_operand 0 \"register_operand\")\n+\t (ior (match_operand 0 \"x86_64_immediate_operand\")\n+\t      (match_operand 0 \"x86_64_zext_immediate_operand\")))\n+    (match_operand 0 \"nonmemory_operand\")))\n+\n+;; Return nonzero if OP is nonmemory operand acceptable by movabs patterns.\n+(define_predicate \"x86_64_movabs_operand\"\n+  (if_then_else (match_test \"!TARGET_64BIT || !flag_pic\")\n+    (match_operand 0 \"nonmemory_operand\")\n+    (ior (match_operand 0 \"register_operand\")\n+\t (and (match_operand 0 \"const_double_operand\")\n+\t      (match_test \"GET_MODE_SIZE (mode) <= 8\")))))\n+\n+;; Return nonzero if OP is CONST_INT >= 1 and <= 31 (a valid operand\n+;; for shift & compare patterns, as shifting by 0 does not change flags).\n+(define_predicate \"const_int_1_31_operand\"\n+  (and (match_code \"const_int\")\n+       (match_test \"INTVAL (op) >= 1 && INTVAL (op) <= 31\")))\n+\n+;; Returns nonzero if OP is either a symbol reference or a sum of a symbol\n+;; reference and a constant.\n+(define_predicate \"symbolic_operand\"\n+  (match_code \"symbol_ref,label_ref,const\")\n+{\n+  switch (GET_CODE (op))\n+    {\n+    case SYMBOL_REF:\n+    case LABEL_REF:\n+      return 1;\n+\n+    case CONST:\n+      op = XEXP (op, 0);\n+      if (GET_CODE (op) == SYMBOL_REF\n+\t  || GET_CODE (op) == LABEL_REF\n+\t  || (GET_CODE (op) == UNSPEC\n+\t      && (XINT (op, 1) == UNSPEC_GOT\n+\t\t  || XINT (op, 1) == UNSPEC_GOTOFF\n+\t\t  || XINT (op, 1) == UNSPEC_GOTPCREL)))\n+\treturn 1;\n+      if (GET_CODE (op) != PLUS\n+\t  || GET_CODE (XEXP (op, 1)) != CONST_INT)\n+\treturn 0;\n+\n+      op = XEXP (op, 0);\n+      if (GET_CODE (op) == SYMBOL_REF\n+\t  || GET_CODE (op) == LABEL_REF)\n+\treturn 1;\n+      /* Only @GOTOFF gets offsets.  */\n+      if (GET_CODE (op) != UNSPEC\n+\t  || XINT (op, 1) != UNSPEC_GOTOFF)\n+\treturn 0;\n+\n+      op = XVECEXP (op, 0, 0);\n+      if (GET_CODE (op) == SYMBOL_REF\n+\t  || GET_CODE (op) == LABEL_REF)\n+\treturn 1;\n+      return 0;\n+\n+    default:\n+      abort ();\n+    }\n+})\n+\n+;; Return true if the operand contains a @GOT or @GOTOFF reference.\n+(define_predicate \"pic_symbolic_operand\"\n+  (match_code \"const\")\n+{\n+  op = XEXP (op, 0);\n+  if (TARGET_64BIT)\n+    {\n+      if (GET_CODE (op) == UNSPEC\n+\t  && XINT (op, 1) == UNSPEC_GOTPCREL)\n+\treturn 1;\n+      if (GET_CODE (op) == PLUS\n+\t  && GET_CODE (XEXP (op, 0)) == UNSPEC\n+\t  && XINT (XEXP (op, 0), 1) == UNSPEC_GOTPCREL)\n+\treturn 1;\n+    }\n+  else\n+    {\n+      if (GET_CODE (op) == UNSPEC)\n+\treturn 1;\n+      if (GET_CODE (op) != PLUS\n+\t  || GET_CODE (XEXP (op, 1)) != CONST_INT)\n+\treturn 0;\n+      op = XEXP (op, 0);\n+      if (GET_CODE (op) == UNSPEC)\n+\treturn 1;\n+    }\n+  return 0;\n+})\n+\n+;; Return true if OP is a symbolic operand that resolves locally.\n+(define_predicate \"local_symbolic_operand\"\n+  (match_code \"const,label_ref,symbol_ref\")\n+{\n+  if (GET_CODE (op) == CONST\n+      && GET_CODE (XEXP (op, 0)) == PLUS\n+      && GET_CODE (XEXP (XEXP (op, 0), 1)) == CONST_INT)\n+    op = XEXP (XEXP (op, 0), 0);\n+\n+  if (GET_CODE (op) == LABEL_REF)\n+    return 1;\n+\n+  if (GET_CODE (op) != SYMBOL_REF)\n+    return 0;\n+\n+  if (SYMBOL_REF_LOCAL_P (op))\n+    return 1;\n+\n+  /* There is, however, a not insubstantial body of code in the rest of\n+     the compiler that assumes it can just stick the results of\n+     ASM_GENERATE_INTERNAL_LABEL in a symbol_ref and have done.  */\n+  /* ??? This is a hack.  Should update the body of the compiler to\n+     always create a DECL an invoke targetm.encode_section_info.  */\n+  if (strncmp (XSTR (op, 0), internal_label_prefix,\n+\t       internal_label_prefix_len) == 0)\n+    return 1;\n+\n+  return 0;\n+})\n+\n+;; Test for various thread-local symbols.\n+(define_predicate \"tls_symbolic_operand\"\n+  (and (match_code \"symbol_ref\")\n+       (match_test \"SYMBOL_REF_TLS_MODEL (op) != 0\")))\n+\n+(define_predicate \"global_dynamic_symbolic_operand\"\n+  (and (match_code \"symbol_ref\")\n+       (match_test \"SYMBOL_REF_TLS_MODEL (op) == TLS_MODEL_GLOBAL_DYNAMIC\")))\n+\n+(define_predicate \"local_dynamic_symbolic_operand\"\n+  (and (match_code \"symbol_ref\")\n+       (match_test \"SYMBOL_REF_TLS_MODEL (op) == TLS_MODEL_LOCAL_DYNAMIC\")))\n+\n+(define_predicate \"initial_exec_symbolic_operand\"\n+  (and (match_code \"symbol_ref\")\n+       (match_test \"SYMBOL_REF_TLS_MODEL (op) == TLS_MODEL_INITIAL_EXEC\")))\n+\n+(define_predicate \"local_exec_symbolic_operand\"\n+  (and (match_code \"symbol_ref\")\n+       (match_test \"SYMBOL_REF_TLS_MODEL (op) == TLS_MODEL_LOCAL_EXEC\")))\n+\n+;; Test for a pc-relative call operand\n+(define_predicate \"constant_call_address_operand\"\n+  (ior (match_code \"symbol_ref\")\n+       (match_operand 0 \"local_symbolic_operand\")))\n+\n+;; True for any non-virtual or eliminable register.  Used in places where\n+;; instantiation of such a register may cause the pattern to not be recognized.\n+(define_predicate \"register_no_elim_operand\"\n+  (match_operand 0 \"register_operand\")\n+{\n+  if (GET_CODE (op) == SUBREG)\n+    op = SUBREG_REG (op);\n+  return !(op == arg_pointer_rtx\n+\t   || op == frame_pointer_rtx\n+\t   || (REGNO (op) >= FIRST_PSEUDO_REGISTER\n+\t       && REGNO (op) <= LAST_VIRTUAL_REGISTER));\n+})\n+\n+;; Similarly, but include the stack pointer.  This is used to prevent esp\n+;; from being used as an index reg.\n+(define_predicate \"index_register_operand\"\n+  (match_operand 0 \"register_operand\")\n+{\n+  if (GET_CODE (op) == SUBREG)\n+    op = SUBREG_REG (op);\n+  return !(op == stack_pointer_rtx\n+\t   || op == arg_pointer_rtx\n+\t   || op == frame_pointer_rtx\n+\t   || (REGNO (op) >= FIRST_PSEUDO_REGISTER\n+\t       && REGNO (op) <= LAST_VIRTUAL_REGISTER));\n+})\n+\n+;; Return false if this is any eliminable register.  Otherwise general_operand.\n+(define_predicate \"general_no_elim_operand\"\n+  (if_then_else (match_code \"reg,subreg\")\n+    (match_operand 0 \"register_no_elim_operand\")\n+    (match_operand 0 \"general_operand\")))\n+\n+;; Return false if this is any eliminable register.  Otherwise\n+;; register_operand or a constant.\n+(define_predicate \"nonmemory_no_elim_operand\"\n+  (ior (match_operand 0 \"register_no_elim_operand\")\n+       (match_operand 0 \"immediate_operand\")))\n+\n+;; Test for a valid operand for a call instruction.\n+(define_predicate \"call_insn_operand\"\n+  (ior (match_operand 0 \"constant_call_address_operand\")\n+       (ior (match_operand 0 \"register_no_elim_operand\")\n+\t    (match_operand 0 \"memory_operand\"))))\n+\n+;; Simiarly, but for tail calls, in which we cannot allow memory references.\n+(define_predicate \"sibcall_insn_operand\"\n+  (ior (match_operand 0 \"constant_call_address_operand\")\n+       (match_operand 0 \"register_no_elim_operand\")))\n+\n+;; Match exactly zero.\n+(define_predicate \"const0_operand\"\n+  (and (match_code \"const_int,const_double,const_vector\")\n+       (match_test \"op == CONST0_RTX (mode)\")))\n+\n+;; Match exactly one.\n+(define_predicate \"const1_operand\"\n+  (and (match_code \"const_int\")\n+       (match_test \"op == const1_rtx\")))\n+\n+;; Match 2, 4, or 8.  Used for leal multiplicands.\n+(define_predicate \"const248_operand\"\n+  (match_code \"const_int\")\n+{\n+  HOST_WIDE_INT i = INTVAL (op);\n+  return i == 2 || i == 4 || i == 8;\n+})\n+\n+;; Match 0 to 3.\n+(define_predicate \"const_0_to_3_operand\"\n+  (and (match_code \"const_int\")\n+       (match_test \"INTVAL (op) >= 0 && INTVAL (op) <= 3\")))\n+\n+;; Match 0 to 7.\n+(define_predicate \"const_0_to_7_operand\"\n+  (and (match_code \"const_int\")\n+       (match_test \"INTVAL (op) >= 0 && INTVAL (op) <= 7\")))\n+\n+;; Match 0 to 15.\n+(define_predicate \"const_0_to_15_operand\"\n+  (and (match_code \"const_int\")\n+       (match_test \"INTVAL (op) >= 0 && INTVAL (op) <= 15\")))\n+\n+;; Match 0 to 255.\n+(define_predicate \"const_0_to_255_operand\"\n+  (and (match_code \"const_int\")\n+       (match_test \"INTVAL (op) >= 0 && INTVAL (op) <= 255\")))\n+\n+;; True if this is a constant appropriate for an increment or decrement.\n+(define_predicate \"incdec_operand\"\n+  (match_code \"const_int\")\n+{\n+  /* On Pentium4, the inc and dec operations causes extra dependency on flag\n+     registers, since carry flag is not set.  */\n+  if ((TARGET_PENTIUM4 || TARGET_NOCONA) && !optimize_size)\n+    return 0;\n+  return op == const1_rtx || op == constm1_rtx;\n+})\n+\n+;; True if OP is acceptable as operand of DImode shift expander.\n+(define_predicate \"shiftdi_operand\"\n+  (if_then_else (match_test \"TARGET_64BIT\")\n+    (match_operand 0 \"nonimmediate_operand\")\n+    (match_operand 0 \"register_operand\")))\n+\n+;; Return true if OP is a vector load from the constant pool with just\n+;; the first element non-zero.\n+(define_predicate \"zero_extended_scalar_load_operand\"\n+  (match_code \"mem\")\n+{\n+  unsigned n_elts;\n+  op = maybe_get_pool_constant (op);\n+  if (!op)\n+    return 0;\n+  if (GET_CODE (op) != CONST_VECTOR)\n+    return 0;\n+  n_elts =\n+    (GET_MODE_SIZE (GET_MODE (op)) /\n+     GET_MODE_SIZE (GET_MODE_INNER (GET_MODE (op))));\n+  for (n_elts--; n_elts > 0; n_elts--)\n+    {\n+      rtx elt = CONST_VECTOR_ELT (op, n_elts);\n+      if (elt != CONST0_RTX (GET_MODE_INNER (GET_MODE (op))))\n+\treturn 0;\n+    }\n+  return 1;\n+})\n+\n+;; Return 1 when OP is operand acceptable for standard SSE move.\n+(define_predicate \"vector_move_operand\"\n+  (ior (match_operand 0 \"nonimmediate_operand\")\n+       (match_operand 0 \"const0_operand\")))\n+\n+;; Return true if op if a valid address, and does not contain\n+;; a segment override.\n+(define_special_predicate \"no_seg_address_operand\"\n+  (match_operand 0 \"address_operand\")\n+{\n+  struct ix86_address parts;\n+  if (! ix86_decompose_address (op, &parts))\n+    abort ();\n+  return parts.seg == SEG_DEFAULT;\n+})\n+\n+;; Return nonzero if the rtx is known aligned.\n+(define_predicate \"aligned_operand\"\n+  (match_operand 0 \"general_operand\")\n+{\n+  struct ix86_address parts;\n+\n+  /* Registers and immediate operands are always \"aligned\".  */\n+  if (GET_CODE (op) != MEM)\n+    return 1;\n+\n+  /* Don't even try to do any aligned optimizations with volatiles.  */\n+  if (MEM_VOLATILE_P (op))\n+    return 0;\n+  op = XEXP (op, 0);\n+\n+  /* Pushes and pops are only valid on the stack pointer.  */\n+  if (GET_CODE (op) == PRE_DEC\n+      || GET_CODE (op) == POST_INC)\n+    return 1;\n+\n+  /* Decode the address.  */\n+  if (!ix86_decompose_address (op, &parts))\n+    abort ();\n+\n+  /* Look for some component that isn't known to be aligned.  */\n+  if (parts.index)\n+    {\n+      if (REGNO_POINTER_ALIGN (REGNO (parts.index)) * parts.scale < 32)\n+\treturn 0;\n+    }\n+  if (parts.base)\n+    {\n+      if (REGNO_POINTER_ALIGN (REGNO (parts.base)) < 32)\n+\treturn 0;\n+    }\n+  if (parts.disp)\n+    {\n+      if (GET_CODE (parts.disp) != CONST_INT\n+\t  || (INTVAL (parts.disp) & 3) != 0)\n+\treturn 0;\n+    }\n+\n+  /* Didn't find one -- this must be an aligned address.  */\n+  return 1;\n+})\n+\n+;; Returns 1 if OP is memory operand with a displacement.\n+(define_predicate \"memory_displacement_operand\"\n+  (match_operand 0 \"memory_operand\")\n+{\n+  struct ix86_address parts;\n+  if (!ix86_decompose_address (XEXP (op, 0), &parts))\n+    abort ();\n+  return parts.disp != NULL_RTX;\n+})\n+\n+;; Returns 1 if OP is memory operand that can not be represented\n+;; by the modRM array.\n+(define_predicate \"long_memory_operand\"\n+  (and (match_operand 0 \"memory_operand\")\n+       (match_test \"memory_address_length (op) != 0\")))\n+\n+;; Return 1 if OP is a comparison operator that can be issued by fcmov.\n+(define_predicate \"fcmov_comparison_operator\"\n+  (match_operand 0 \"comparison_operator\")\n+{\n+  enum machine_mode inmode = GET_MODE (XEXP (op, 0));\n+  enum rtx_code code = GET_CODE (op);\n+\n+  if (inmode == CCFPmode || inmode == CCFPUmode)\n+    {\n+      enum rtx_code second_code, bypass_code;\n+      ix86_fp_comparison_codes (code, &bypass_code, &code, &second_code);\n+      if (bypass_code != NIL || second_code != NIL)\n+\treturn 0;\n+      code = ix86_fp_compare_code_to_integer (code);\n+    }\n+  /* i387 supports just limited amount of conditional codes.  */\n+  switch (code)\n+    {\n+    case LTU: case GTU: case LEU: case GEU:\n+      if (inmode == CCmode || inmode == CCFPmode || inmode == CCFPUmode)\n+\treturn 1;\n+      return 0;\n+    case ORDERED: case UNORDERED:\n+    case EQ: case NE:\n+      return 1;\n+    default:\n+      return 0;\n+    }\n+})\n+\n+;; Return 1 if OP is a comparison that can be used in the CMPSS/CMPPS insns.\n+;; The first set are supported directly; the second set can't be done with\n+;; full IEEE support, i.e. NaNs.\n+;;\n+;; ??? It would seem that we have a lot of uses of this predicate that pass\n+;; it the wrong mode.  We got away with this because the old function didn't\n+;; check the mode at all.  Mirror that for now by calling this a special\n+;; predicate.\n+\n+(define_special_predicate \"sse_comparison_operator\"\n+  (ior (match_code \"eq,lt,le,unordered,ne,unge,ungt,ordered\")\n+       (and (match_code \"uneq,unlt,unle,ltgt,ge,gt\")\n+\t    (match_code \"!TARGET_IEEE_FP\"))))\n+\n+;; Return 1 if OP is a valid comparison operator in valid mode.\n+(define_predicate \"ix86_comparison_operator\"\n+  (match_operand 0 \"comparison_operator\")\n+{\n+  enum machine_mode inmode = GET_MODE (XEXP (op, 0));\n+  enum rtx_code code = GET_CODE (op);\n+\n+  if (inmode == CCFPmode || inmode == CCFPUmode)\n+    {\n+      enum rtx_code second_code, bypass_code;\n+      ix86_fp_comparison_codes (code, &bypass_code, &code, &second_code);\n+      return (bypass_code == NIL && second_code == NIL);\n+    }\n+  switch (code)\n+    {\n+    case EQ: case NE:\n+      return 1;\n+    case LT: case GE:\n+      if (inmode == CCmode || inmode == CCGCmode\n+\t  || inmode == CCGOCmode || inmode == CCNOmode)\n+\treturn 1;\n+      return 0;\n+    case LTU: case GTU: case LEU: case ORDERED: case UNORDERED: case GEU:\n+      if (inmode == CCmode)\n+\treturn 1;\n+      return 0;\n+    case GT: case LE:\n+      if (inmode == CCmode || inmode == CCGCmode || inmode == CCNOmode)\n+\treturn 1;\n+      return 0;\n+    default:\n+      return 0;\n+    }\n+})\n+\n+;; Return 1 if OP is a valid comparison operator testing carry flag to be set.\n+(define_predicate \"ix86_carry_flag_operator\"\n+  (match_code \"ltu,lt,unlt,gt,ungt,le,unle,ge,unge,ltgt,uneq\")\n+{\n+  enum machine_mode inmode = GET_MODE (XEXP (op, 0));\n+  enum rtx_code code = GET_CODE (op);\n+\n+  if (GET_CODE (XEXP (op, 0)) != REG\n+      || REGNO (XEXP (op, 0)) != FLAGS_REG\n+      || XEXP (op, 1) != const0_rtx)\n+    return 0;\n+\n+  if (inmode == CCFPmode || inmode == CCFPUmode)\n+    {\n+      enum rtx_code second_code, bypass_code;\n+      ix86_fp_comparison_codes (code, &bypass_code, &code, &second_code);\n+      if (bypass_code != NIL || second_code != NIL)\n+\treturn 0;\n+      code = ix86_fp_compare_code_to_integer (code);\n+    }\n+  else if (inmode != CCmode)\n+    return 0;\n+\n+  return code == LTU;\n+})\n+\n+;; Nearly general operand, but accept any const_double, since we wish\n+;; to be able to drop them into memory rather than have them get pulled\n+;; into registers.\n+(define_predicate \"cmp_fp_expander_operand\"\n+  (ior (match_code \"const_double\")\n+       (match_operand 0 \"general_operand\")))\n+\n+;; Return true if this is a valid binary floating-point operation.\n+(define_predicate \"binary_fp_operator\"\n+  (match_code \"plus,minus,mult,div\"))\n+\n+;; Return true if this is a multiply operation.\n+(define_predicate \"mult_operator\"\n+  (match_code \"mult\"))\n+\n+;; Return true if this is a division operation.\n+(define_predicate \"div_operator\"\n+  (match_code \"div\"))\n+\n+;; Return true for ARITHMETIC_P.\n+(define_predicate \"arith_or_logical_operator\"\n+  (match_code \"PLUS,MULT,AND,IOR,XOR,SMIN,SMAX,UMIN,UMAX,COMPARE,MINUS,DIV,\n+\t       MOD,UDIV,UMOD,ASHIFT,ROTATE,ASHIFTRT,LSHIFTRT,ROTATERT\"))\n+\n+;; Return 1 if OP is a binary operator that can be promoted to wider mode.\n+;; Modern CPUs have same latency for HImode and SImode multiply,\n+;; but 386 and 486 do HImode multiply faster.  */\n+(define_predicate \"promotable_binary_operator\"\n+  (ior (match_code \"plus,and,ior,xor,ashift\")\n+       (and (match_code \"mult\")\n+\t    (match_test \"ix86_tune > PROCESSOR_I486\"))))\n+\n+;; To avoid problems when jump re-emits comparisons like testqi_ext_ccno_0,\n+;; re-recognize the operand to avoid a copy_to_mode_reg that will fail.\n+;;\n+;; ??? It seems likely that this will only work because cmpsi is an\n+;; expander, and no actual insns use this.\n+\n+(define_predicate \"cmpsi_operand_1\"\n+  (match_code \"and\")\n+{\n+  return (GET_MODE (op) == SImode\n+\t  && GET_CODE (XEXP (op, 0)) == ZERO_EXTRACT\n+\t  && GET_CODE (XEXP (XEXP (op, 0), 1)) == CONST_INT\n+\t  && GET_CODE (XEXP (XEXP (op, 0), 2)) == CONST_INT\n+\t  && INTVAL (XEXP (XEXP (op, 0), 1)) == 8\n+\t  && INTVAL (XEXP (XEXP (op, 0), 2)) == 8\n+\t  && GET_CODE (XEXP (op, 1)) == CONST_INT);\n+})\n+\n+(define_predicate \"cmpsi_operand\"\n+  (ior (match_operand 0 \"nonimmediate_operand\")\n+       (match_operand 0 \"cmpsi_operand_1\")))"}]}