{"sha": "9243c3d1b63b9092a82178392145f9e9d62423d9", "node_id": "C_kwDOANBUbNoAKDkyNDNjM2QxYjYzYjkwOTJhODIxNzgzOTIxNDVmOWU5ZDYyNDIzZDk", "commit": {"author": {"name": "Ju-Zhe Zhong", "email": "juzhe.zhong@rivai.ai", "date": "2022-12-14T07:31:11Z"}, "committer": {"name": "Kito Cheng", "email": "kito.cheng@sifive.com", "date": "2022-12-19T14:21:37Z"}, "message": "RISC-V: Support VSETVL PASS for RVV support\n\nThis patch is to support VSETVL PASS for RVV support.\n1.The optimization and performance is guaranteed LCM (Lazy code motion).\n2.Base on RTL_SSA framework to gain better optimization chances.\n3.Also we do VL/VTYPE, demand information backward propagation across\n  blocks by RTL_SSA reverse order in CFG.\n4.It has been well and fully tested by about 200+ testcases for VLMAX\n  AVL situation (Only for VLMAX since we don't have an intrinsics to\n  test non-VLMAX).\n5.Will support AVL model in the next patch.\n\ngcc/ChangeLog:\n\n\t* config.gcc: Add riscv-vsetvl.o.\n\t* config/riscv/riscv-passes.def (INSERT_PASS_BEFORE): Add VSETVL PASS\n\tlocation.\n\t* config/riscv/riscv-protos.h (make_pass_vsetvl): New function.\n\t(enum avl_type): New enum.\n\t(get_ta): New function.\n\t(get_ma): Ditto.\n\t(get_avl_type): Ditto.\n\t(calculate_ratio): Ditto.\n\t(enum tail_policy): New enum.\n\t(enum mask_policy): Ditto.\n\t* config/riscv/riscv-v.cc (calculate_ratio): New function.\n\t(emit_pred_op): change the VLMAX mov codgen.\n\t(get_ta): New function.\n\t(get_ma): Ditto.\n\t(enum tail_policy): Change enum.\n\t(get_prefer_tail_policy): New function.\n\t(enum mask_policy): Change enum.\n\t(get_prefer_mask_policy): New function.\n\t* config/riscv/t-riscv: Add riscv-vsetvl.o\n\t* config/riscv/vector.md: Adjust attribute and pattern for VSETVL\n\tPASS.\n\t(@vlmax_avl<mode>): Ditto.\n\t(@vsetvl<mode>_no_side_effects): Delete.\n\t(vsetvl_vtype_change_only): New MD pattern.\n\t(@vsetvl_discard_result<mode>): Ditto.\n\t* config/riscv/riscv-vsetvl.cc: New file.\n\t* config/riscv/riscv-vsetvl.h: New file.", "tree": {"sha": "fccb9fd45b62ada34cd0ab9d9a48e51f718d9cd5", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/fccb9fd45b62ada34cd0ab9d9a48e51f718d9cd5"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/9243c3d1b63b9092a82178392145f9e9d62423d9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9243c3d1b63b9092a82178392145f9e9d62423d9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9243c3d1b63b9092a82178392145f9e9d62423d9", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9243c3d1b63b9092a82178392145f9e9d62423d9/comments", "author": {"login": "zhongjuzhe", "id": 66454988, "node_id": "MDQ6VXNlcjY2NDU0OTg4", "avatar_url": "https://avatars.githubusercontent.com/u/66454988?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhongjuzhe", "html_url": "https://github.com/zhongjuzhe", "followers_url": "https://api.github.com/users/zhongjuzhe/followers", "following_url": "https://api.github.com/users/zhongjuzhe/following{/other_user}", "gists_url": "https://api.github.com/users/zhongjuzhe/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhongjuzhe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhongjuzhe/subscriptions", "organizations_url": "https://api.github.com/users/zhongjuzhe/orgs", "repos_url": "https://api.github.com/users/zhongjuzhe/repos", "events_url": "https://api.github.com/users/zhongjuzhe/events{/privacy}", "received_events_url": "https://api.github.com/users/zhongjuzhe/received_events", "type": "User", "site_admin": false}, "committer": {"login": "kito-cheng", "id": 2723185, "node_id": "MDQ6VXNlcjI3MjMxODU=", "avatar_url": "https://avatars.githubusercontent.com/u/2723185?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kito-cheng", "html_url": "https://github.com/kito-cheng", "followers_url": "https://api.github.com/users/kito-cheng/followers", "following_url": "https://api.github.com/users/kito-cheng/following{/other_user}", "gists_url": "https://api.github.com/users/kito-cheng/gists{/gist_id}", "starred_url": "https://api.github.com/users/kito-cheng/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kito-cheng/subscriptions", "organizations_url": "https://api.github.com/users/kito-cheng/orgs", "repos_url": "https://api.github.com/users/kito-cheng/repos", "events_url": "https://api.github.com/users/kito-cheng/events{/privacy}", "received_events_url": "https://api.github.com/users/kito-cheng/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "22dc669e109de9a76c74535cdf30e7922e0ef5c1", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/22dc669e109de9a76c74535cdf30e7922e0ef5c1", "html_url": "https://github.com/Rust-GCC/gccrs/commit/22dc669e109de9a76c74535cdf30e7922e0ef5c1"}], "stats": {"total": 3112, "additions": 3076, "deletions": 36}, "files": [{"sha": "fa263f235d184053be2db886734ad0f17f5fd29a", "filename": "gcc/config.gcc", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig.gcc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig.gcc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig.gcc?ref=9243c3d1b63b9092a82178392145f9e9d62423d9", "patch": "@@ -529,7 +529,7 @@ pru-*-*)\n \t;;\n riscv*)\n \tcpu_type=riscv\n-\textra_objs=\"riscv-builtins.o riscv-c.o riscv-sr.o riscv-shorten-memrefs.o riscv-selftests.o riscv-v.o\"\n+\textra_objs=\"riscv-builtins.o riscv-c.o riscv-sr.o riscv-shorten-memrefs.o riscv-selftests.o riscv-v.o riscv-vsetvl.o\"\n \textra_objs=\"${extra_objs} riscv-vector-builtins.o riscv-vector-builtins-shapes.o riscv-vector-builtins-bases.o\"\n \td_target_objs=\"riscv-d.o\"\n \textra_headers=\"riscv_vector.h\""}, {"sha": "d2d48f231aa8f40c232f0ec262a4d793c6b06eb1", "filename": "gcc/config/riscv/riscv-passes.def", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig%2Friscv%2Friscv-passes.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig%2Friscv%2Friscv-passes.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-passes.def?ref=9243c3d1b63b9092a82178392145f9e9d62423d9", "patch": "@@ -18,3 +18,4 @@\n    <http://www.gnu.org/licenses/>.  */\n \n INSERT_PASS_AFTER (pass_rtl_store_motion, 1, pass_shorten_memrefs);\n+INSERT_PASS_BEFORE (pass_sched2, 1, pass_vsetvl);"}, {"sha": "cfd0f284f9163aad5285a46eb5b79a8d137e56e7", "filename": "gcc/config/riscv/riscv-protos.h", "status": "modified", "additions": 15, "deletions": 0, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig%2Friscv%2Friscv-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig%2Friscv%2Friscv-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-protos.h?ref=9243c3d1b63b9092a82178392145f9e9d62423d9", "patch": "@@ -96,6 +96,7 @@ extern void riscv_parse_arch_string (const char *, struct gcc_options *, locatio\n extern bool riscv_hard_regno_rename_ok (unsigned, unsigned);\n \n rtl_opt_pass * make_pass_shorten_memrefs (gcc::context *ctxt);\n+rtl_opt_pass * make_pass_vsetvl (gcc::context *ctxt);\n \n /* Information about one CPU we know about.  */\n struct riscv_cpu_info {\n@@ -131,6 +132,12 @@ enum vlmul_type\n   LMUL_F4 = 6,\n   LMUL_F2 = 7,\n };\n+\n+enum avl_type\n+{\n+  NONVLMAX,\n+  VLMAX,\n+};\n /* Routines implemented in riscv-vector-builtins.cc.  */\n extern void init_builtins (void);\n extern const char *mangle_builtin_type (const_tree);\n@@ -145,17 +152,25 @@ extern bool legitimize_move (rtx, rtx, machine_mode);\n extern void emit_pred_op (unsigned, rtx, rtx, machine_mode);\n extern enum vlmul_type get_vlmul (machine_mode);\n extern unsigned int get_ratio (machine_mode);\n+extern int get_ta (rtx);\n+extern int get_ma (rtx);\n+extern int get_avl_type (rtx);\n+extern unsigned int calculate_ratio (unsigned int, enum vlmul_type);\n enum tail_policy\n {\n   TAIL_UNDISTURBED = 0,\n   TAIL_AGNOSTIC = 1,\n+  TAIL_ANY = 2,\n };\n \n enum mask_policy\n {\n   MASK_UNDISTURBED = 0,\n   MASK_AGNOSTIC = 1,\n+  MASK_ANY = 2,\n };\n+enum tail_policy get_prefer_tail_policy ();\n+enum mask_policy get_prefer_mask_policy ();\n }\n \n /* We classify builtin types into two classes:"}, {"sha": "bae5e921dd0c33fe2a360ab15fde9a6264373ea3", "filename": "gcc/config/riscv/riscv-v.cc", "status": "modified", "additions": 94, "deletions": 8, "changes": 102, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig%2Friscv%2Friscv-v.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig%2Friscv%2Friscv-v.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-v.cc?ref=9243c3d1b63b9092a82178392145f9e9d62423d9", "patch": "@@ -72,11 +72,16 @@ template <int MAX_OPERANDS> class insn_expander\n   }\n   void add_policy_operand (enum tail_policy vta, enum mask_policy vma)\n   {\n-    rtx tail_policy_rtx = vta == TAIL_UNDISTURBED ? const0_rtx : const1_rtx;\n-    rtx mask_policy_rtx = vma == MASK_UNDISTURBED ? const0_rtx : const1_rtx;\n+    rtx tail_policy_rtx = gen_int_mode (vta, Pmode);\n+    rtx mask_policy_rtx = gen_int_mode (vma, Pmode);\n     add_input_operand (tail_policy_rtx, Pmode);\n     add_input_operand (mask_policy_rtx, Pmode);\n   }\n+  void add_avl_type_operand ()\n+  {\n+    rtx vlmax_rtx = gen_int_mode (avl_type::VLMAX, Pmode);\n+    add_input_operand (vlmax_rtx, Pmode);\n+  }\n \n   void expand (enum insn_code icode, bool temporary_volatile_p = false)\n   {\n@@ -112,19 +117,58 @@ emit_vlmax_vsetvl (machine_mode vmode)\n   unsigned int sew = GET_MODE_CLASS (vmode) == MODE_VECTOR_BOOL\n \t\t       ? 8\n \t\t       : GET_MODE_BITSIZE (GET_MODE_INNER (vmode));\n+  enum vlmul_type vlmul = get_vlmul (vmode);\n+  unsigned int ratio = calculate_ratio (sew, vlmul);\n+\n+  if (!optimize)\n+    emit_insn (gen_vsetvl (Pmode, vl, RVV_VLMAX, gen_int_mode (sew, Pmode),\n+\t\t\t   gen_int_mode (get_vlmul (vmode), Pmode), const0_rtx,\n+\t\t\t   const0_rtx));\n+  else\n+    emit_insn (gen_vlmax_avl (Pmode, vl, gen_int_mode (ratio, Pmode)));\n \n-  emit_insn (\n-    gen_vsetvl_no_side_effects (Pmode, vl, RVV_VLMAX, gen_int_mode (sew, Pmode),\n-\t\t\t\tgen_int_mode (get_vlmul (vmode), Pmode),\n-\t\t\t\tconst1_rtx, const1_rtx));\n   return vl;\n }\n \n+/* Calculate SEW/LMUL ratio.  */\n+unsigned int\n+calculate_ratio (unsigned int sew, enum vlmul_type vlmul)\n+{\n+  unsigned int ratio;\n+  switch (vlmul)\n+    {\n+    case LMUL_1:\n+      ratio = sew;\n+      break;\n+    case LMUL_2:\n+      ratio = sew / 2;\n+      break;\n+    case LMUL_4:\n+      ratio = sew / 4;\n+      break;\n+    case LMUL_8:\n+      ratio = sew / 8;\n+      break;\n+    case LMUL_F8:\n+      ratio = sew * 8;\n+      break;\n+    case LMUL_F4:\n+      ratio = sew * 4;\n+      break;\n+    case LMUL_F2:\n+      ratio = sew * 2;\n+      break;\n+    default:\n+      gcc_unreachable ();\n+    }\n+  return ratio;\n+}\n+\n /* Emit an RVV unmask && vl mov from SRC to DEST.  */\n void\n emit_pred_op (unsigned icode, rtx dest, rtx src, machine_mode mask_mode)\n {\n-  insn_expander<7> e;\n+  insn_expander<8> e;\n   machine_mode mode = GET_MODE (dest);\n \n   e.add_output_operand (dest, mode);\n@@ -137,7 +181,9 @@ emit_pred_op (unsigned icode, rtx dest, rtx src, machine_mode mask_mode)\n   e.add_input_operand (vlmax, Pmode);\n \n   if (GET_MODE_CLASS (mode) != MODE_VECTOR_BOOL)\n-    e.add_policy_operand (TAIL_AGNOSTIC, MASK_AGNOSTIC);\n+    e.add_policy_operand (get_prefer_tail_policy (), get_prefer_mask_policy ());\n+\n+  e.add_avl_type_operand ();\n \n   e.expand ((enum insn_code) icode, MEM_P (dest) || MEM_P (src));\n }\n@@ -256,4 +302,44 @@ get_ratio (machine_mode mode)\n     return mode_vtype_infos.ratio_for_min_vlen64[mode];\n }\n \n+/* Get ta according to operand[tail_op_idx].  */\n+int\n+get_ta (rtx ta)\n+{\n+  if (INTVAL (ta) == TAIL_ANY)\n+    return INVALID_ATTRIBUTE;\n+  return INTVAL (ta);\n+}\n+\n+/* Get ma according to operand[mask_op_idx].  */\n+int\n+get_ma (rtx ma)\n+{\n+  if (INTVAL (ma) == MASK_ANY)\n+    return INVALID_ATTRIBUTE;\n+  return INTVAL (ma);\n+}\n+\n+/* Get prefer tail policy.  */\n+enum tail_policy\n+get_prefer_tail_policy ()\n+{\n+  /* TODO: By default, we choose to use TAIL_ANY which allows\n+     compiler pick up either agnostic or undisturbed. Maybe we\n+     will have a compile option like -mprefer=agnostic to set\n+     this value???.  */\n+  return TAIL_ANY;\n+}\n+\n+/* Get prefer mask policy.  */\n+enum mask_policy\n+get_prefer_mask_policy ()\n+{\n+  /* TODO: By default, we choose to use MASK_ANY which allows\n+     compiler pick up either agnostic or undisturbed. Maybe we\n+     will have a compile option like -mprefer=agnostic to set\n+     this value???.  */\n+  return MASK_ANY;\n+}\n+\n } // namespace riscv_vector"}, {"sha": "3ca3fc15e5a9567f1cbf3b42894a70e176e4bcd4", "filename": "gcc/config/riscv/riscv-vsetvl.cc", "status": "added", "additions": 2509, "deletions": 0, "changes": 2509, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig%2Friscv%2Friscv-vsetvl.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig%2Friscv%2Friscv-vsetvl.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-vsetvl.cc?ref=9243c3d1b63b9092a82178392145f9e9d62423d9", "patch": "@@ -0,0 +1,2509 @@\n+/* VSETVL pass for RISC-V 'V' Extension for GNU compiler.\n+   Copyright(C) 2022-2022 Free Software Foundation, Inc.\n+   Contributed by Juzhe Zhong (juzhe.zhong@rivai.ai), RiVAI Technologies Ltd.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 3, or(at your option)\n+any later version.\n+\n+GCC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+/*  This pass is to Set VL/VTYPE global status for RVV instructions\n+    that depend on VL and VTYPE registers by Lazy code motion (LCM).\n+\n+    Strategy:\n+\n+    -  Backward demanded info fusion within block.\n+\n+    -  Lazy code motion (LCM) based demanded info backward propagation.\n+\n+    -  RTL_SSA framework for def-use, PHI analysis.\n+\n+    -  Lazy code motion (LCM) for global VL/VTYPE optimization.\n+\n+    Assumption:\n+\n+    -  Each avl operand is either an immediate (must be in range 0 ~ 31) or reg.\n+\n+    This pass consists of 5 phases:\n+\n+    -  Phase 1 - compute VL/VTYPE demanded information within each block\n+       by backward data-flow analysis.\n+\n+    -  Phase 2 - Emit vsetvl instructions within each basic block according to\n+       demand, compute and save ANTLOC && AVLOC of each block.\n+\n+    -  Phase 3 - Backward demanded info propagation and fusion across blocks.\n+\n+    -  Phase 4 - Lazy code motion including: compute local properties,\n+       pre_edge_lcm and vsetvl insertion && delete edges for LCM results.\n+\n+    -  Phase 5 - Cleanup AVL operand of RVV instruction since it will not be\n+       used any more and VL operand of VSETVL instruction if it is not used by\n+       any non-debug instructions.\n+\n+    Implementation:\n+\n+    -  The subroutine of optimize == 0 is simple_vsetvl.\n+       This function simplily vsetvl insertion for each RVV\n+       instruction. No optimization.\n+\n+    -  The subroutine of optimize > 0 is lazy_vsetvl.\n+       This function optimize vsetvl insertion process by\n+       lazy code motion (LCM) layering on RTL_SSA.  */\n+\n+#define IN_TARGET_CODE 1\n+#define INCLUDE_ALGORITHM\n+#define INCLUDE_FUNCTIONAL\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"backend.h\"\n+#include \"rtl.h\"\n+#include \"target.h\"\n+#include \"tree-pass.h\"\n+#include \"df.h\"\n+#include \"rtl-ssa.h\"\n+#include \"cfgcleanup.h\"\n+#include \"insn-config.h\"\n+#include \"insn-attr.h\"\n+#include \"insn-opinit.h\"\n+#include \"tm-constrs.h\"\n+#include \"cfgrtl.h\"\n+#include \"cfganal.h\"\n+#include \"lcm.h\"\n+#include \"predict.h\"\n+#include \"profile-count.h\"\n+#include \"riscv-vsetvl.h\"\n+\n+using namespace rtl_ssa;\n+using namespace riscv_vector;\n+\n+DEBUG_FUNCTION void\n+debug (const vector_insn_info *info)\n+{\n+  info->dump (stderr);\n+}\n+\n+DEBUG_FUNCTION void\n+debug (const vector_infos_manager *info)\n+{\n+  info->dump (stderr);\n+}\n+\n+static bool\n+vlmax_avl_p (rtx x)\n+{\n+  return x && rtx_equal_p (x, RVV_VLMAX);\n+}\n+\n+static bool\n+vlmax_avl_insn_p (rtx_insn *rinsn)\n+{\n+  return INSN_CODE (rinsn) == CODE_FOR_vlmax_avlsi\n+\t || INSN_CODE (rinsn) == CODE_FOR_vlmax_avldi;\n+}\n+\n+static bool\n+loop_basic_block_p (const basic_block cfg_bb)\n+{\n+  return JUMP_P (BB_END (cfg_bb)) && any_condjump_p (BB_END (cfg_bb));\n+}\n+\n+/* Return true if it is an RVV instruction depends on VTYPE global\n+   status register.  */\n+static bool\n+has_vtype_op (rtx_insn *rinsn)\n+{\n+  return recog_memoized (rinsn) >= 0 && get_attr_has_vtype_op (rinsn);\n+}\n+\n+/* Return true if it is an RVV instruction depends on VL global\n+   status register.  */\n+static bool\n+has_vl_op (rtx_insn *rinsn)\n+{\n+  return recog_memoized (rinsn) >= 0 && get_attr_has_vl_op (rinsn);\n+}\n+\n+/* Is this a SEW value that can be encoded into the VTYPE format.  */\n+static bool\n+valid_sew_p (size_t sew)\n+{\n+  return exact_log2 (sew) && sew >= 8 && sew <= 64;\n+}\n+\n+/* Return true if it is a vsetvl instruction.  */\n+static bool\n+vector_config_insn_p (rtx_insn *rinsn)\n+{\n+  return recog_memoized (rinsn) >= 0 && get_attr_type (rinsn) == TYPE_VSETVL;\n+}\n+\n+/* Return true if it is vsetvldi or vsetvlsi.  */\n+static bool\n+vsetvl_insn_p (rtx_insn *rinsn)\n+{\n+  return INSN_CODE (rinsn) == CODE_FOR_vsetvldi\n+\t || INSN_CODE (rinsn) == CODE_FOR_vsetvlsi;\n+}\n+\n+/* Return true if INSN1 comes befeore INSN2 in the same block.  */\n+static bool\n+same_bb_and_before_p (const insn_info *insn1, const insn_info *insn2)\n+{\n+  return (insn1->bb ()->index () == insn2->bb ()->index ())\n+\t && (*insn1 < *insn2);\n+}\n+\n+/* Return true if INSN1 comes after or equal INSN2 in the same block.  */\n+static bool\n+same_bb_and_after_or_equal_p (const insn_info *insn1, const insn_info *insn2)\n+{\n+  return (insn1->bb ()->index () == insn2->bb ()->index ())\n+\t && (*insn1 >= *insn2);\n+}\n+\n+/* An \"anticipatable occurrence\" is one that is the first occurrence in the\n+   basic block, the operands are not modified in the basic block prior\n+   to the occurrence and the output is not used between the start of\n+   the block and the occurrence.  */\n+static bool\n+anticipatable_occurrence_p (const insn_info *insn, const vector_insn_info dem)\n+{\n+  /* The only possible operand we care of VSETVL is AVL.  */\n+  if (dem.has_avl_reg ())\n+    {\n+      /* The operands shoule not be modified in the basic block prior\n+\t to the occurrence.  */\n+      if (!vlmax_avl_p (dem.get_avl ()))\n+\t{\n+\t  set_info *set\n+\t    = find_access (insn->uses (), REGNO (dem.get_avl ()))->def ();\n+\t  /* If it's undefined, it's not anticipatable conservatively.  */\n+\t  if (!set)\n+\t    return false;\n+\t  if (same_bb_and_before_p (set->insn (), insn))\n+\t    return false;\n+\t}\n+    }\n+\n+  /* The output should not be used between the start of the block\n+     and the occurrence.  */\n+  if (vsetvl_insn_p (insn->rtl ()))\n+    {\n+      rtx dest = SET_DEST (XVECEXP (PATTERN (insn->rtl ()), 0, 0));\n+      for (insn_info *i = insn->prev_nondebug_insn (); i != nullptr;\n+\t   i = i->prev_nondebug_insn ())\n+\tif (find_access (i->uses (), REGNO (dest)))\n+\t  return false;\n+    }\n+\n+  return true;\n+}\n+\n+/* An \"available occurrence\" is one that is the last occurrence in the\n+   basic block and the operands are not modified by following statements in\n+   the basic block [including this insn].  */\n+static bool\n+available_occurrence_p (const insn_info *insn, const vector_insn_info dem)\n+{\n+  /* The only possible operand we care of VSETVL is AVL.  */\n+  if (dem.has_avl_reg ())\n+    {\n+      /* The operands shoule not be modified in the basic block prior\n+\t to the occurrence.\n+\t e.g.\n+\t    bb:\n+\t      vsetvl hr3, hr2, ...\n+\t      ...\n+\t      vadd ... (vl=hr3)\n+      */\n+      if (!vlmax_avl_p (dem.get_avl ()))\n+\t{\n+\t  set_info *set\n+\t    = find_access (insn->uses (), REGNO (dem.get_avl ()))->def ();\n+\t  /* If it's undefined, it's not available conservatively.  */\n+\t  if (!set)\n+\t    return false;\n+\t  if (same_bb_and_after_or_equal_p (set->insn (), insn))\n+\t    return false;\n+\t}\n+    }\n+  return true;\n+}\n+\n+/* Return true if the branch probability is dominate.  */\n+static bool\n+dominate_probability_p (edge e)\n+{\n+  /* TODO: We simpily pick dominate probability >= 50%.\n+     However, it isn't always optimal. Consider cases\n+     like this:\n+       bb 0: 80% succs: bb 2, bb 3, bb 4, bb 5.\n+       bb 1: 20%\n+\n+     Assume bb 1, bb 2, bb 3, bb 4, bb 5 are different\n+     one another, and bb 2, bb 3, bb 4, bb 5 are incompatible.\n+\n+     ??? Maybe backward propagate bb 1 is better ???\n+     May need to support an accurate and reliable COST model\n+     in the future.  */\n+  return e->probability >= profile_probability::even ();\n+}\n+\n+/* Return true if the block is worthwhile backward propagation.  */\n+static bool\n+backward_propagate_worthwhile_p (const basic_block cfg_bb,\n+\t\t\t\t const vector_block_info block_info)\n+{\n+  if (loop_basic_block_p (cfg_bb))\n+    {\n+      if (block_info.local_dem.compatible_p (block_info.reaching_out))\n+\treturn true;\n+\n+      /* There is a obvious case that is not worthwhile and meaningless\n+\t to propagate the demand information:\n+\t\t\t  local_dem\n+\t\t\t     __________\n+\t\t\t ____|____     |\n+\t\t\t|        |     |\n+\t\t\t|________|     |\n+\t\t\t     |_________|\n+\t\t\t  reaching_out\n+\t  Header is incompatible with reaching_out and the block is loop itself,\n+\t  we don't backward propagete the local_dem since we can't avoid emit\n+\t  vsetvl for the local_dem.  */\n+      edge e;\n+      edge_iterator ei;\n+      FOR_EACH_EDGE (e, ei, cfg_bb->succs)\n+\tif (e->dest->index == cfg_bb->index)\n+\t  return false;\n+    }\n+\n+  return true;\n+}\n+\n+/* Helper function to get VL operand.  */\n+static rtx\n+get_vl (rtx_insn *rinsn)\n+{\n+  if (has_vl_op (rinsn))\n+    {\n+      /* We only call get_vl for VLMAX use VTYPE instruction.\n+\t It's used to get the VL operand to emit VLMAX VSETVL instruction:\n+\t vsetvl a5,zero,e32,m1,ta,ma.  */\n+      gcc_assert (get_attr_avl_type (rinsn) == VLMAX);\n+      extract_insn_cached (rinsn);\n+      return recog_data.operand[get_attr_vl_op_idx (rinsn)];\n+    }\n+  return SET_DEST (XVECEXP (PATTERN (rinsn), 0, 0));\n+}\n+\n+/* Helper function to get AVL operand.  */\n+static rtx\n+get_avl (rtx_insn *rinsn)\n+{\n+  if (vsetvl_insn_p (rinsn))\n+    return XVECEXP (SET_SRC (XVECEXP (PATTERN (rinsn), 0, 0)), 0, 0);\n+\n+  if (!has_vl_op (rinsn))\n+    return NULL_RTX;\n+  if (get_attr_avl_type (rinsn) == VLMAX)\n+    return RVV_VLMAX;\n+  extract_insn_cached (rinsn);\n+  return recog_data.operand[get_attr_vl_op_idx (rinsn)];\n+}\n+\n+static bool\n+can_backward_propagate_p (const function_info *ssa, const basic_block cfg_bb,\n+\t\t\t  const vector_insn_info prop)\n+{\n+  insn_info *insn = prop.get_insn ();\n+\n+  /* TODO: We don't backward propagate the explict VSETVL here\n+     since we will change vsetvl and vsetvlmax intrinsiscs into\n+     no side effects which can be optimized into optimzal location\n+     by GCC internal PASSes. We only need to support these backward\n+     propagation if vsetvl instrinsics have side effects.  */\n+  if (vsetvl_insn_p (insn->rtl ()))\n+    return false;\n+\n+  gcc_assert (has_vtype_op (insn->rtl ()));\n+  rtx reg = NULL_RTX;\n+\n+  /* Case 1: Don't need VL. Just let it backward propagate.  */\n+  if (!has_vl_op (insn->rtl ()))\n+    return true;\n+  else\n+    {\n+      /* Case 2: CONST_INT AVL, we don't need to check def.  */\n+      if (prop.has_avl_imm ())\n+\treturn true;\n+      else\n+\t{\n+\t  /* Case 3: REG AVL, we need to check the distance of def to make\n+\t     sure we won't backward propagate over the def.  */\n+\t  gcc_assert (prop.has_avl_reg ());\n+\t  if (vlmax_avl_p (prop.get_avl ()))\n+\t    /* Check VL operand for vsetvl vl,zero.  */\n+\t    reg = get_vl (insn->rtl ());\n+\t  else\n+\t    /* Check AVL operand for vsetvl zero,avl.  */\n+\t    reg = get_avl (insn->rtl ());\n+\t}\n+    }\n+\n+  def_info *def = find_access (insn->uses (), REGNO (reg))->def ();\n+\n+  /* If the definition is in the current block, we can't propagate it\n+     acrocss blocks.  */\n+  if (def->bb ()->cfg_bb ()->index == insn->bb ()->cfg_bb ()->index)\n+    {\n+      set_info *set = safe_dyn_cast<set_info *> (def);\n+\n+      /* True if it is a degenerate PHI that can be backward propagated.  */\n+      auto valid_degenerate_phi_p = [&] () {\n+\tif (!set)\n+\t  return false;\n+\n+\tphi_info *phi = safe_dyn_cast<phi_info *> (set);\n+\tif (!phi)\n+\t  return false;\n+\n+\tbasic_block iter_bb;\n+\tset_info *ultimate_def = look_through_degenerate_phi (set);\n+\tconst basic_block ultimate_bb = ultimate_def->bb ()->cfg_bb ();\n+\tFOR_BB_BETWEEN (iter_bb, ultimate_bb, def->bb ()->cfg_bb (), next_bb)\n+\t  if (iter_bb->index == cfg_bb->index)\n+\t    return true;\n+\n+\treturn false;\n+      };\n+\n+      if (valid_degenerate_phi_p ())\n+\treturn true;\n+\n+      /* TODO: Support non-degenerate PHI backward propagation later.  */\n+      return false;\n+    }\n+\n+  /* If the definition block is the current block that we iterate, we\n+     can backward propagate it since we will insert or change VL/VTYPE\n+     info at the end of the current block we iterate.  */\n+  if (def->bb ()->cfg_bb ()->index == cfg_bb->index)\n+    return true;\n+\n+  /* Make sure we don't backward propagete the VL/VTYPE info over the\n+     definition blocks.  */\n+  bool visited_p = false;\n+  for (const bb_info *bb : ssa->reverse_bbs ())\n+    {\n+      if (bb->cfg_bb ()->index == cfg_bb->index && visited_p)\n+\treturn false;\n+      if (bb->cfg_bb ()->index == def->bb ()->cfg_bb ()->index)\n+\tvisited_p = true;\n+    }\n+\n+  return true;\n+}\n+\n+/* Helper function to get SEW operand. We always have SEW value for\n+   all RVV instructions that have VTYPE OP.  */\n+static uint8_t\n+get_sew (rtx_insn *rinsn)\n+{\n+  return get_attr_sew (rinsn);\n+}\n+\n+/* Helper function to get VLMUL operand. We always have VLMUL value for\n+   all RVV instructions that have VTYPE OP. */\n+static enum vlmul_type\n+get_vlmul (rtx_insn *rinsn)\n+{\n+  return (enum vlmul_type) get_attr_vlmul (rinsn);\n+}\n+\n+/* Get default tail policy.  */\n+static bool\n+get_default_ta ()\n+{\n+  /* For the instruction that doesn't require TA, we still need a default value\n+     to emit vsetvl. We pick up the default value according to prefer policy. */\n+  return (bool) (get_prefer_tail_policy () & 0x1\n+\t\t || (get_prefer_tail_policy () >> 1 & 0x1));\n+}\n+\n+/* Get default mask policy.  */\n+static bool\n+get_default_ma ()\n+{\n+  /* For the instruction that doesn't require MA, we still need a default value\n+     to emit vsetvl. We pick up the default value according to prefer policy. */\n+  return (bool) (get_prefer_mask_policy () & 0x1\n+\t\t || (get_prefer_mask_policy () >> 1 & 0x1));\n+}\n+\n+/* Helper function to get TA operand.  */\n+static bool\n+tail_agnostic_p (rtx_insn *rinsn)\n+{\n+  /* If it doesn't have TA, we return agnostic by default.  */\n+  extract_insn_cached (rinsn);\n+  int ta = get_attr_ta (rinsn);\n+  return ta == INVALID_ATTRIBUTE ? get_default_ta () : IS_AGNOSTIC (ta);\n+}\n+\n+/* Helper function to get MA operand.  */\n+static bool\n+mask_agnostic_p (rtx_insn *rinsn)\n+{\n+  /* If it doesn't have MA, we return agnostic by default.  */\n+  extract_insn_cached (rinsn);\n+  int ma = get_attr_ma (rinsn);\n+  return ma == INVALID_ATTRIBUTE ? get_default_ma () : IS_AGNOSTIC (ma);\n+}\n+\n+/* Return true if FN has a vector instruction that use VL/VTYPE.  */\n+static bool\n+has_vector_insn (function *fn)\n+{\n+  basic_block cfg_bb;\n+  rtx_insn *rinsn;\n+  FOR_ALL_BB_FN (cfg_bb, fn)\n+    FOR_BB_INSNS (cfg_bb, rinsn)\n+      if (NONDEBUG_INSN_P (rinsn) && has_vtype_op (rinsn))\n+\treturn true;\n+  return false;\n+}\n+\n+/* Emit vsetvl instruction.  */\n+static rtx\n+gen_vsetvl_pat (enum vsetvl_type insn_type, vl_vtype_info info, rtx vl)\n+{\n+  rtx avl = info.get_avl ();\n+  rtx sew = gen_int_mode (info.get_sew (), Pmode);\n+  rtx vlmul = gen_int_mode (info.get_vlmul (), Pmode);\n+  rtx ta = gen_int_mode (info.get_ta (), Pmode);\n+  rtx ma = gen_int_mode (info.get_ma (), Pmode);\n+\n+  if (insn_type == VSETVL_NORMAL)\n+    {\n+      gcc_assert (vl != NULL_RTX);\n+      return gen_vsetvl (Pmode, vl, avl, sew, vlmul, ta, ma);\n+    }\n+  else if (insn_type == VSETVL_VTYPE_CHANGE_ONLY)\n+    return gen_vsetvl_vtype_change_only (sew, vlmul, ta, ma);\n+  else\n+    return gen_vsetvl_discard_result (Pmode, avl, sew, vlmul, ta, ma);\n+}\n+\n+static rtx\n+gen_vsetvl_pat (rtx_insn *rinsn, const vector_insn_info info)\n+{\n+  rtx new_pat;\n+  if (vsetvl_insn_p (rinsn) || vlmax_avl_p (info.get_avl ()))\n+    {\n+      rtx dest = get_vl (rinsn);\n+      new_pat = gen_vsetvl_pat (VSETVL_NORMAL, info, dest);\n+    }\n+  else if (INSN_CODE (rinsn) == CODE_FOR_vsetvl_vtype_change_only)\n+    new_pat = gen_vsetvl_pat (VSETVL_VTYPE_CHANGE_ONLY, info, NULL_RTX);\n+  else\n+    new_pat = gen_vsetvl_pat (VSETVL_DISCARD_RESULT, info, NULL_RTX);\n+  return new_pat;\n+}\n+\n+static void\n+emit_vsetvl_insn (enum vsetvl_type insn_type, enum emit_type emit_type,\n+\t\t  vl_vtype_info info, rtx vl, rtx_insn *rinsn)\n+{\n+  rtx pat = gen_vsetvl_pat (insn_type, info, vl);\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"\\nInsert vsetvl insn PATTERN:\\n\");\n+      print_rtl_single (dump_file, pat);\n+    }\n+\n+  if (emit_type == EMIT_DIRECT)\n+    emit_insn (pat);\n+  else if (emit_type == EMIT_BEFORE)\n+    emit_insn_before (pat, rinsn);\n+  else\n+    emit_insn_after (pat, rinsn);\n+}\n+\n+static void\n+eliminate_insn (rtx_insn *rinsn)\n+{\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"\\nEliminate insn %d:\\n\", INSN_UID (rinsn));\n+      print_rtl_single (dump_file, rinsn);\n+    }\n+  if (in_sequence_p ())\n+    remove_insn (rinsn);\n+  else\n+    delete_insn (rinsn);\n+}\n+\n+static void\n+insert_vsetvl (enum emit_type emit_type, rtx_insn *rinsn,\n+\t       const vector_insn_info &info, const vector_insn_info &prev_info)\n+{\n+  /* Use X0, X0 form if the AVL is the same and the SEW+LMUL gives the same\n+     VLMAX.  */\n+  if (prev_info.valid_or_dirty_p () && !prev_info.unknown_p ()\n+      && info.same_avl_p (prev_info) && info.same_vlmax_p (prev_info))\n+    {\n+      emit_vsetvl_insn (VSETVL_VTYPE_CHANGE_ONLY, emit_type, info, NULL_RTX,\n+\t\t\trinsn);\n+      return;\n+    }\n+\n+  if (info.has_avl_imm ())\n+    {\n+      emit_vsetvl_insn (VSETVL_DISCARD_RESULT, emit_type, info, NULL_RTX,\n+\t\t\trinsn);\n+      return;\n+    }\n+\n+  if (info.has_avl_no_reg ())\n+    {\n+      /* We can only use x0, x0 if there's no chance of the vtype change causing\n+\t the previous vl to become invalid.  */\n+      if (prev_info.valid_or_dirty_p () && !prev_info.unknown_p ()\n+\t  && info.same_vlmax_p (prev_info))\n+\t{\n+\t  emit_vsetvl_insn (VSETVL_VTYPE_CHANGE_ONLY, emit_type, info, NULL_RTX,\n+\t\t\t    rinsn);\n+\t  return;\n+\t}\n+      /* Otherwise use an AVL of 0 to avoid depending on previous vl.  */\n+      vl_vtype_info new_info = info;\n+      new_info.set_avl_info (avl_info (const0_rtx, nullptr));\n+      emit_vsetvl_insn (VSETVL_DISCARD_RESULT, emit_type, new_info, NULL_RTX,\n+\t\t\trinsn);\n+      return;\n+    }\n+\n+  /* Use X0 as the DestReg unless AVLReg is X0. We also need to change the\n+     opcode if the AVLReg is X0 as they have different register classes for\n+     the AVL operand.  */\n+  if (vlmax_avl_p (info.get_avl ()))\n+    {\n+      gcc_assert (has_vtype_op (rinsn) || vsetvl_insn_p (rinsn));\n+      rtx vl_op = get_vl (rinsn);\n+      gcc_assert (!vlmax_avl_p (vl_op));\n+      emit_vsetvl_insn (VSETVL_NORMAL, emit_type, info, vl_op, rinsn);\n+      return;\n+    }\n+\n+  emit_vsetvl_insn (VSETVL_DISCARD_RESULT, emit_type, info, NULL_RTX, rinsn);\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"Update VL/VTYPE info, previous info=\");\n+      prev_info.dump (dump_file);\n+    }\n+}\n+\n+/* If X contains any LABEL_REF's, add REG_LABEL_OPERAND notes for them\n+   to INSN.  If such notes are added to an insn which references a\n+   CODE_LABEL, the LABEL_NUSES count is incremented.  We have to add\n+   that note, because the following loop optimization pass requires\n+   them.  */\n+\n+/* ??? If there was a jump optimization pass after gcse and before loop,\n+   then we would not need to do this here, because jump would add the\n+   necessary REG_LABEL_OPERAND and REG_LABEL_TARGET notes.  */\n+\n+static void\n+add_label_notes (rtx x, rtx_insn *insn)\n+{\n+  enum rtx_code code = GET_CODE (x);\n+  int i, j;\n+  const char *fmt;\n+\n+  if (code == LABEL_REF && !LABEL_REF_NONLOCAL_P (x))\n+    {\n+      /* This code used to ignore labels that referred to dispatch tables to\n+\t avoid flow generating (slightly) worse code.\n+\n+\t We no longer ignore such label references (see LABEL_REF handling in\n+\t mark_jump_label for additional information).  */\n+\n+      /* There's no reason for current users to emit jump-insns with\n+\t such a LABEL_REF, so we don't have to handle REG_LABEL_TARGET\n+\t notes.  */\n+      gcc_assert (!JUMP_P (insn));\n+      add_reg_note (insn, REG_LABEL_OPERAND, label_ref_label (x));\n+\n+      if (LABEL_P (label_ref_label (x)))\n+\tLABEL_NUSES (label_ref_label (x))++;\n+\n+      return;\n+    }\n+\n+  for (i = GET_RTX_LENGTH (code) - 1, fmt = GET_RTX_FORMAT (code); i >= 0; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\tadd_label_notes (XEXP (x, i), insn);\n+      else if (fmt[i] == 'E')\n+\tfor (j = XVECLEN (x, i) - 1; j >= 0; j--)\n+\t  add_label_notes (XVECEXP (x, i, j), insn);\n+    }\n+}\n+\n+/* Add EXPR to the end of basic block BB.\n+\n+   This is used by both the PRE and code hoisting.  */\n+\n+static void\n+insert_insn_end_basic_block (rtx_insn *rinsn, basic_block cfg_bb)\n+{\n+  rtx_insn *end_rinsn = BB_END (cfg_bb);\n+  rtx_insn *new_insn;\n+  rtx_insn *pat, *pat_end;\n+\n+  pat = rinsn;\n+  gcc_assert (pat && INSN_P (pat));\n+\n+  pat_end = pat;\n+  while (NEXT_INSN (pat_end) != NULL_RTX)\n+    pat_end = NEXT_INSN (pat_end);\n+\n+  /* If the last end_rinsn is a jump, insert EXPR in front.  Similarly we need\n+     to take care of trapping instructions in presence of non-call exceptions.\n+   */\n+\n+  if (JUMP_P (end_rinsn)\n+      || (NONJUMP_INSN_P (end_rinsn)\n+\t  && (!single_succ_p (cfg_bb)\n+\t      || single_succ_edge (cfg_bb)->flags & EDGE_ABNORMAL)))\n+    {\n+      /* FIXME: What if something in jump uses value set in new end_rinsn?  */\n+      new_insn = emit_insn_before_noloc (pat, end_rinsn, cfg_bb);\n+    }\n+\n+  /* Likewise if the last end_rinsn is a call, as will happen in the presence\n+     of exception handling.  */\n+  else if (CALL_P (end_rinsn)\n+\t   && (!single_succ_p (cfg_bb)\n+\t       || single_succ_edge (cfg_bb)->flags & EDGE_ABNORMAL))\n+    {\n+      /* Keeping in mind targets with small register classes and parameters\n+\t in registers, we search backward and place the instructions before\n+\t the first parameter is loaded.  Do this for everyone for consistency\n+\t and a presumption that we'll get better code elsewhere as well.  */\n+\n+      /* Since different machines initialize their parameter registers\n+\t in different orders, assume nothing.  Collect the set of all\n+\t parameter registers.  */\n+      end_rinsn = find_first_parameter_load (end_rinsn, BB_HEAD (cfg_bb));\n+\n+      /* If we found all the parameter loads, then we want to insert\n+\t before the first parameter load.\n+\n+\t If we did not find all the parameter loads, then we might have\n+\t stopped on the head of the block, which could be a CODE_LABEL.\n+\t If we inserted before the CODE_LABEL, then we would be putting\n+\t the end_rinsn in the wrong basic block.  In that case, put the\n+\t end_rinsn after the CODE_LABEL.  Also, respect NOTE_INSN_BASIC_BLOCK.\n+       */\n+      while (LABEL_P (end_rinsn) || NOTE_INSN_BASIC_BLOCK_P (end_rinsn))\n+\tend_rinsn = NEXT_INSN (end_rinsn);\n+\n+      new_insn = emit_insn_before_noloc (pat, end_rinsn, cfg_bb);\n+    }\n+  else\n+    new_insn = emit_insn_after_noloc (pat, end_rinsn, cfg_bb);\n+\n+  while (1)\n+    {\n+      if (INSN_P (pat))\n+\tadd_label_notes (PATTERN (pat), new_insn);\n+      if (pat == pat_end)\n+\tbreak;\n+      pat = NEXT_INSN (pat);\n+    }\n+}\n+\n+/* Get VL/VTYPE information for INSN.  */\n+static vl_vtype_info\n+get_vl_vtype_info (const insn_info *insn)\n+{\n+  if (vector_config_insn_p (insn->rtl ()))\n+    gcc_assert (vsetvl_insn_p (insn->rtl ())\n+\t\t&& \"Can't handle X0, rs1 vsetvli yet\");\n+\n+  set_info *set = nullptr;\n+  rtx avl = ::get_avl (insn->rtl ());\n+  if (avl && REG_P (avl) && !vlmax_avl_p (avl))\n+    set = find_access (insn->uses (), REGNO (avl))->def ();\n+\n+  uint8_t sew = get_sew (insn->rtl ());\n+  enum vlmul_type vlmul = get_vlmul (insn->rtl ());\n+  uint8_t ratio = get_attr_ratio (insn->rtl ());\n+  /* when get_attr_ratio is invalid, this kind of instructions\n+     doesn't care about ratio. However, we still need this value\n+     in demand info backward analysis.  */\n+  if (ratio == INVALID_ATTRIBUTE)\n+    ratio = calculate_ratio (sew, vlmul);\n+  bool ta = tail_agnostic_p (insn->rtl ());\n+  bool ma = mask_agnostic_p (insn->rtl ());\n+\n+  /* If merge operand is undef value, we prefer agnostic.  */\n+  int merge_op_idx = get_attr_merge_op_idx (insn->rtl ());\n+  if (merge_op_idx != INVALID_ATTRIBUTE\n+      && satisfies_constraint_vu (recog_data.operand[merge_op_idx]))\n+    {\n+      ta = true;\n+      ma = true;\n+    }\n+\n+  vl_vtype_info info (avl_info (avl, set), sew, vlmul, ratio, ta, ma);\n+  return info;\n+}\n+\n+static void\n+change_insn (rtx_insn *rinsn, rtx new_pat)\n+{\n+  /* We don't apply change on RTL_SSA here since it's possible a\n+     new INSN we add in the PASS before which doesn't have RTL_SSA\n+     info yet.*/\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"\\nChange PATTERN of insn %d from:\\n\",\n+\t       INSN_UID (rinsn));\n+      print_rtl_single (dump_file, PATTERN (rinsn));\n+    }\n+\n+  validate_change (rinsn, &PATTERN (rinsn), new_pat, true);\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"\\nto:\\n\");\n+      print_rtl_single (dump_file, PATTERN (rinsn));\n+    }\n+}\n+\n+static bool\n+change_insn (function_info *ssa, insn_change change, insn_info *insn,\n+\t     rtx new_pat)\n+{\n+  rtx_insn *rinsn = insn->rtl ();\n+  auto attempt = ssa->new_change_attempt ();\n+  if (!restrict_movement (change))\n+    return false;\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"\\nChange PATTERN of insn %d from:\\n\",\n+\t       INSN_UID (rinsn));\n+      print_rtl_single (dump_file, PATTERN (rinsn));\n+      if (dump_flags & TDF_DETAILS)\n+\t{\n+\t  fprintf (dump_file, \"RTL_SSA info:\\n\");\n+\t  pretty_printer pp;\n+\t  pp.buffer->stream = dump_file;\n+\t  insn->print_full (&pp);\n+\t  pp_printf (&pp, \"\\n\");\n+\t  pp_flush (&pp);\n+\t}\n+    }\n+\n+  insn_change_watermark watermark;\n+  validate_change (rinsn, &PATTERN (rinsn), new_pat, true);\n+\n+  /* These routines report failures themselves.  */\n+  if (!recog (attempt, change) || !change_is_worthwhile (change, false))\n+    return false;\n+  confirm_change_group ();\n+  ssa->change_insn (change);\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"\\nto:\\n\");\n+      print_rtl_single (dump_file, PATTERN (rinsn));\n+      if (dump_flags & TDF_DETAILS)\n+\t{\n+\t  fprintf (dump_file, \"RTL_SSA info:\\n\");\n+\t  pretty_printer pp;\n+\t  pp.buffer->stream = dump_file;\n+\t  insn->print_full (&pp);\n+\t  pp_printf (&pp, \"\\n\");\n+\t  pp_flush (&pp);\n+\t}\n+    }\n+  return true;\n+}\n+\n+avl_info::avl_info (rtx value_in, set_info *source_in)\n+  : m_value (value_in), m_source (source_in)\n+{}\n+\n+avl_info &\n+avl_info::operator= (const avl_info &other)\n+{\n+  m_value = other.get_value ();\n+  m_source = other.get_source ();\n+  return *this;\n+}\n+\n+bool\n+avl_info::operator== (const avl_info &other) const\n+{\n+  if (!m_value)\n+    return !other.get_value ();\n+  if (!other.get_value ())\n+    return false;\n+\n+  /* It's safe to consider they are equal if their RTX value are\n+     strictly the same.  */\n+  if (m_value == other.get_value ())\n+    return true;\n+\n+  if (GET_CODE (m_value) != GET_CODE (other.get_value ()))\n+    return false;\n+\n+  /* Handle CONST_INT AVL.  */\n+  if (CONST_INT_P (m_value))\n+    return INTVAL (m_value) == INTVAL (other.get_value ());\n+\n+  /* Handle VLMAX AVL.  */\n+  if (vlmax_avl_p (m_value))\n+    return vlmax_avl_p (other.get_value ());\n+\n+  /* TODO: So far we only support VLMAX (AVL=zero) comparison,\n+     we will support non-VLMAX AVL in the future.  */\n+  return false;\n+}\n+\n+bool\n+avl_info::operator!= (const avl_info &other) const\n+{\n+  return !(*this == other);\n+}\n+\n+/* Initialize VL/VTYPE information.  */\n+vl_vtype_info::vl_vtype_info (avl_info avl_in, uint8_t sew_in,\n+\t\t\t      enum vlmul_type vlmul_in, uint8_t ratio_in,\n+\t\t\t      bool ta_in, bool ma_in)\n+  : m_avl (avl_in), m_sew (sew_in), m_vlmul (vlmul_in), m_ratio (ratio_in),\n+    m_ta (ta_in), m_ma (ma_in)\n+{\n+  gcc_assert (valid_sew_p (m_sew) && \"Unexpected SEW\");\n+}\n+\n+bool\n+vl_vtype_info::operator== (const vl_vtype_info &other) const\n+{\n+  return m_avl == other.get_avl_info () && m_sew == other.get_sew ()\n+\t && m_vlmul == other.get_vlmul () && m_ta == other.get_ta ()\n+\t && m_ma == other.get_ma () && m_ratio == other.get_ratio ();\n+}\n+\n+bool\n+vl_vtype_info::operator!= (const vl_vtype_info &other) const\n+{\n+  return !(*this == other);\n+}\n+\n+bool\n+vl_vtype_info::has_non_zero_avl () const\n+{\n+  if (has_avl_imm ())\n+    return INTVAL (get_avl ()) > 0;\n+  if (has_avl_reg ())\n+    return vlmax_avl_p (get_avl ());\n+  return false;\n+}\n+\n+bool\n+vl_vtype_info::same_avl_p (const vl_vtype_info &other) const\n+{\n+  return get_avl_info () == other.get_avl_info ();\n+}\n+\n+bool\n+vl_vtype_info::same_vtype_p (const vl_vtype_info &other) const\n+{\n+  return get_sew () == other.get_sew () && get_vlmul () == other.get_vlmul ()\n+\t && get_ta () == other.get_ta () && get_ma () == other.get_ma ();\n+}\n+\n+bool\n+vl_vtype_info::same_vlmax_p (const vl_vtype_info &other) const\n+{\n+  return get_ratio () == other.get_ratio ();\n+}\n+\n+/* Compare the compatibility between Dem1 and Dem2.\n+   If Dem1 > Dem2, Dem1 has bigger compatibility then Dem2\n+   meaning Dem1 is easier be compatible with others than Dem2\n+   or Dem2 is stricter than Dem1.\n+   For example, Dem1 (demand SEW + LMUL) > Dem2 (demand RATIO).  */\n+bool\n+vector_insn_info::operator> (const vector_insn_info &other) const\n+{\n+  if (other.compatible_p (static_cast<const vl_vtype_info &> (*this))\n+      && !this->compatible_p (static_cast<const vl_vtype_info &> (other)))\n+    return true;\n+  return false;\n+}\n+\n+bool\n+vector_insn_info::operator>= (const vector_insn_info &other) const\n+{\n+  if (*this > other)\n+    return true;\n+\n+  if (*this == other)\n+    return true;\n+\n+  if (!compatible_p (other))\n+    return false;\n+\n+  if (!demand_p (DEMAND_AVL) && other.demand_p (DEMAND_AVL))\n+    return false;\n+\n+  if (same_vlmax_p (other))\n+    {\n+      if (demand_p (DEMAND_RATIO) && !other.demand_p (DEMAND_RATIO)\n+\t  && (get_sew () != other.get_sew ()\n+\t      || get_vlmul () != other.get_vlmul ()))\n+\treturn false;\n+\n+      if (get_sew () == other.get_sew () && get_vlmul () == other.get_vlmul ())\n+\t{\n+\t  if (demand_p (DEMAND_RATIO) && !other.demand_p (DEMAND_RATIO))\n+\t    return false;\n+\t}\n+    }\n+\n+  if (demand_p (DEMAND_TAIL_POLICY) && !other.demand_p (DEMAND_TAIL_POLICY)\n+      && get_ta () != other.get_ta ())\n+    return false;\n+\n+  if (demand_p (DEMAND_MASK_POLICY) && !other.demand_p (DEMAND_MASK_POLICY)\n+      && get_ma () != other.get_ma ())\n+    return false;\n+\n+  return true;\n+}\n+\n+bool\n+vector_insn_info::operator== (const vector_insn_info &other) const\n+{\n+  gcc_assert (!uninit_p () && !other.uninit_p ()\n+\t      && \"Uninitialization should not happen\");\n+\n+  /* Empty is only equal to another Empty.  */\n+  if (empty_p ())\n+    return other.empty_p ();\n+  if (other.empty_p ())\n+    return empty_p ();\n+\n+  /* Unknown is only equal to another Unknown.  */\n+  if (unknown_p ())\n+    return other.unknown_p ();\n+  if (other.unknown_p ())\n+    return unknown_p ();\n+\n+  for (size_t i = 0; i < NUM_DEMAND; i++)\n+    if (m_demands[i] != other.demand_p ((enum demand_type) i))\n+      return false;\n+\n+  if (m_insn != other.get_insn ())\n+    return false;\n+  if (m_dirty_pat != other.get_dirty_pat ())\n+    return false;\n+\n+  if (!same_avl_p (other))\n+    return false;\n+\n+  /* If the full VTYPE is valid, check that it is the same.  */\n+  return same_vtype_p (other);\n+}\n+\n+void\n+vector_insn_info::parse_insn (rtx_insn *rinsn)\n+{\n+  *this = vector_insn_info ();\n+  if (!NONDEBUG_INSN_P (rinsn))\n+    return;\n+  if (!has_vtype_op (rinsn))\n+    return;\n+  m_state = VALID;\n+  extract_insn_cached (rinsn);\n+  const rtx avl = recog_data.operand[get_attr_vl_op_idx (rinsn)];\n+  m_avl = avl_info (avl, nullptr);\n+  m_sew = ::get_sew (rinsn);\n+  m_vlmul = ::get_vlmul (rinsn);\n+  m_ta = tail_agnostic_p (rinsn);\n+  m_ma = mask_agnostic_p (rinsn);\n+}\n+\n+void\n+vector_insn_info::parse_insn (insn_info *insn)\n+{\n+  *this = vector_insn_info ();\n+\n+  /* Return if it is debug insn for the consistency with optimize == 0.  */\n+  if (insn->is_debug_insn ())\n+    return;\n+\n+  /* We set it as unknown since we don't what will happen in CALL or ASM.  */\n+  if (insn->is_call () || insn->is_asm ())\n+    {\n+      set_unknown ();\n+      return;\n+    }\n+\n+  /* If this is something that updates VL/VTYPE that we don't know about, set\n+     the state to unknown.  */\n+  if (!vector_config_insn_p (insn->rtl ())\n+      && (find_access (insn->defs (), VL_REGNUM)\n+\t  || find_access (insn->defs (), VTYPE_REGNUM)))\n+    {\n+      set_unknown ();\n+      return;\n+    }\n+\n+  if (!vector_config_insn_p (insn->rtl ()) && !has_vtype_op (insn->rtl ()))\n+    return;\n+\n+  /* Warning: This function has to work on both the lowered (i.e. post\n+     emit_local_forward_vsetvls) and pre-lowering forms.  The main implication\n+     of this is that it can't use the value of a SEW, VL, or Policy operand as\n+     they might be stale after lowering.  */\n+  vl_vtype_info::operator= (get_vl_vtype_info (insn));\n+  m_insn = insn;\n+  m_state = VALID;\n+  if (vector_config_insn_p (insn->rtl ()))\n+    {\n+      m_demands[DEMAND_AVL] = true;\n+      m_demands[DEMAND_RATIO] = true;\n+      return;\n+    }\n+\n+  if (has_vl_op (insn->rtl ()))\n+    m_demands[DEMAND_AVL] = true;\n+\n+  if (get_attr_ratio (insn->rtl ()) != INVALID_ATTRIBUTE)\n+    m_demands[DEMAND_RATIO] = true;\n+  else\n+    {\n+      /* TODO: By default, if it doesn't demand RATIO, we set it\n+\t demand SEW && LMUL both. Some instructions may demand SEW\n+\t only and ignore LMUL, will fix it later.  */\n+      m_demands[DEMAND_SEW] = true;\n+      m_demands[DEMAND_LMUL] = true;\n+    }\n+\n+  if (get_attr_ta (insn->rtl ()) != INVALID_ATTRIBUTE)\n+    m_demands[DEMAND_TAIL_POLICY] = true;\n+  if (get_attr_ma (insn->rtl ()) != INVALID_ATTRIBUTE)\n+    m_demands[DEMAND_MASK_POLICY] = true;\n+}\n+\n+void\n+vector_insn_info::demand_vl_vtype ()\n+{\n+  m_state = VALID;\n+  m_demands[DEMAND_AVL] = true;\n+  m_demands[DEMAND_SEW] = true;\n+  m_demands[DEMAND_LMUL] = true;\n+  m_demands[DEMAND_TAIL_POLICY] = true;\n+  m_demands[DEMAND_MASK_POLICY] = true;\n+}\n+\n+bool\n+vector_insn_info::compatible_p (const vector_insn_info &other) const\n+{\n+  gcc_assert (valid_or_dirty_p () && other.valid_or_dirty_p ()\n+\t      && \"Can't compare invalid demanded infos\");\n+\n+  /* Check SEW.  */\n+  if (demand_p (DEMAND_SEW) && other.demand_p (DEMAND_SEW)\n+      && get_sew () != other.get_sew ())\n+    return false;\n+\n+  /* Check LMUL.  */\n+  if (demand_p (DEMAND_LMUL) && other.demand_p (DEMAND_LMUL)\n+      && get_vlmul () != other.get_vlmul ())\n+    return false;\n+\n+  /* Check RATIO.  */\n+  if (demand_p (DEMAND_RATIO) && other.demand_p (DEMAND_RATIO)\n+      && get_ratio () != other.get_ratio ())\n+    return false;\n+  if (demand_p (DEMAND_RATIO) && (other.get_sew () || other.get_vlmul ())\n+      && get_ratio () != other.get_ratio ())\n+    return false;\n+  if (other.demand_p (DEMAND_RATIO) && (get_sew () || get_vlmul ())\n+      && get_ratio () != other.get_ratio ())\n+    return false;\n+\n+  if (demand_p (DEMAND_TAIL_POLICY) && other.demand_p (DEMAND_TAIL_POLICY)\n+      && get_ta () != other.get_ta ())\n+    return false;\n+  if (demand_p (DEMAND_MASK_POLICY) && other.demand_p (DEMAND_MASK_POLICY)\n+      && get_ma () != other.get_ma ())\n+    return false;\n+\n+  if (demand_p (DEMAND_AVL) && other.demand_p (DEMAND_AVL))\n+    return m_avl == other.get_avl_info ();\n+\n+  return true;\n+}\n+\n+bool\n+vector_insn_info::compatible_avl_p (const vl_vtype_info &other) const\n+{\n+  gcc_assert (valid_or_dirty_p () && \"Can't compare invalid vl_vtype_info\");\n+  gcc_assert (!unknown_p () && \"Can't compare AVL in unknown state\");\n+  if (!demand_p (DEMAND_AVL))\n+    return true;\n+  return get_avl_info () == other.get_avl_info ();\n+}\n+\n+bool\n+vector_insn_info::compatible_vtype_p (const vl_vtype_info &other) const\n+{\n+  gcc_assert (valid_or_dirty_p () && \"Can't compare invalid vl_vtype_info\");\n+  gcc_assert (!unknown_p () && \"Can't compare VTYPE in unknown state\");\n+  if (demand_p (DEMAND_SEW) && m_sew != other.get_sew ())\n+    return false;\n+  if (demand_p (DEMAND_LMUL) && m_vlmul != other.get_vlmul ())\n+    return false;\n+  if (demand_p (DEMAND_RATIO) && m_ratio != other.get_ratio ())\n+    return false;\n+  if (demand_p (DEMAND_TAIL_POLICY) && m_ta != other.get_ta ())\n+    return false;\n+  if (demand_p (DEMAND_MASK_POLICY) && m_ma != other.get_ma ())\n+    return false;\n+  return true;\n+}\n+\n+/* Determine whether the vector instructions requirements represented by\n+   Require are compatible with the previous vsetvli instruction represented\n+   by this.  INSN is the instruction whose requirements we're considering.  */\n+bool\n+vector_insn_info::compatible_p (const vl_vtype_info &curr_info) const\n+{\n+  gcc_assert (!uninit_p () && \"Can't handle uninitialized info\");\n+  if (empty_p ())\n+    return false;\n+\n+  /* Nothing is compatible with Unknown.  */\n+  if (unknown_p ())\n+    return false;\n+\n+  /* If the instruction doesn't need an AVLReg and the SEW matches, consider\n+     it compatible.  */\n+  if (!demand_p (DEMAND_AVL))\n+    if (m_sew == curr_info.get_sew ())\n+      return true;\n+\n+  return compatible_avl_p (curr_info) && compatible_vtype_p (curr_info);\n+}\n+\n+vector_insn_info\n+vector_insn_info::merge (const vector_insn_info &merge_info,\n+\t\t\t bool across_bb_p = false) const\n+{\n+  gcc_assert (this->compatible_p (merge_info)\n+\t      && \"Can't merge incompatible demanded infos\");\n+\n+  vector_insn_info new_info;\n+  new_info.demand_vl_vtype ();\n+\n+  if (dirty_p ())\n+    {\n+      gcc_assert (across_bb_p);\n+      if (demand_p (DEMAND_AVL))\n+\tnew_info.set_insn (get_insn ());\n+      else\n+\tnew_info.set_insn (merge_info.get_insn ());\n+    }\n+  else\n+    {\n+      if (across_bb_p)\n+\tnew_info.set_insn (get_insn ());\n+      else\n+\tnew_info.set_insn (merge_info.get_insn ());\n+    }\n+\n+  new_info.set_dirty_pat (merge_info.get_dirty_pat ());\n+\n+  if (!demand_p (DEMAND_AVL) && !merge_info.demand_p (DEMAND_AVL))\n+    new_info.undemand (DEMAND_AVL);\n+  if (!demand_p (DEMAND_SEW) && !merge_info.demand_p (DEMAND_SEW))\n+    new_info.undemand (DEMAND_SEW);\n+  if (!demand_p (DEMAND_LMUL) && !merge_info.demand_p (DEMAND_LMUL))\n+    new_info.undemand (DEMAND_LMUL);\n+\n+  if (!demand_p (DEMAND_TAIL_POLICY)\n+      && !merge_info.demand_p (DEMAND_TAIL_POLICY))\n+    new_info.undemand (DEMAND_TAIL_POLICY);\n+  if (!demand_p (DEMAND_MASK_POLICY)\n+      && !merge_info.demand_p (DEMAND_MASK_POLICY))\n+    new_info.undemand (DEMAND_MASK_POLICY);\n+\n+  if (merge_info.demand_p (DEMAND_AVL))\n+    new_info.set_avl_info (merge_info.get_avl_info ());\n+  else if (demand_p (DEMAND_AVL))\n+    new_info.set_avl_info (get_avl_info ());\n+\n+  if (merge_info.demand_p (DEMAND_SEW))\n+    new_info.set_sew (merge_info.get_sew ());\n+  else if (demand_p (DEMAND_SEW))\n+    new_info.set_sew (get_sew ());\n+\n+  if (merge_info.demand_p (DEMAND_LMUL))\n+    new_info.set_vlmul (merge_info.get_vlmul ());\n+  else if (demand_p (DEMAND_LMUL))\n+    new_info.set_vlmul (get_vlmul ());\n+\n+  if (!new_info.demand_p (DEMAND_SEW) && !new_info.demand_p (DEMAND_LMUL))\n+    {\n+      if (demand_p (DEMAND_RATIO) || merge_info.demand_p (DEMAND_RATIO))\n+\tnew_info.demand (DEMAND_RATIO);\n+      /* Even though we don't demand_p SEW && VLMUL in this case, we still\n+       * need them.  */\n+      if (merge_info.demand_p (DEMAND_RATIO))\n+\t{\n+\t  new_info.set_sew (merge_info.get_sew ());\n+\t  new_info.set_vlmul (merge_info.get_vlmul ());\n+\t  new_info.set_ratio (merge_info.get_ratio ());\n+\t}\n+      else if (demand_p (DEMAND_RATIO))\n+\t{\n+\t  new_info.set_sew (get_sew ());\n+\t  new_info.set_vlmul (get_vlmul ());\n+\t  new_info.set_ratio (get_ratio ());\n+\t}\n+    }\n+  else\n+    {\n+      /* when get_attr_ratio is invalid, this kind of instructions\n+\t doesn't care about ratio. However, we still need this value\n+\t in demand_p info backward analysis.  */\n+      new_info.set_ratio (\n+\tcalculate_ratio (new_info.get_sew (), new_info.get_vlmul ()));\n+    }\n+\n+  if (merge_info.demand_p (DEMAND_TAIL_POLICY))\n+    new_info.set_ta (merge_info.get_ta ());\n+  else if (demand_p (DEMAND_TAIL_POLICY))\n+    new_info.set_ta (get_ta ());\n+  else\n+    new_info.set_ta (get_default_ta ());\n+\n+  if (merge_info.demand_p (DEMAND_MASK_POLICY))\n+    new_info.set_ma (merge_info.get_ma ());\n+  else if (demand_p (DEMAND_MASK_POLICY))\n+    new_info.set_ma (get_ma ());\n+  else\n+    new_info.set_ma (get_default_ma ());\n+\n+  return new_info;\n+}\n+\n+void\n+vector_insn_info::dump (FILE *file) const\n+{\n+  fprintf (file, \"[\");\n+  if (uninit_p ())\n+    fprintf (file, \"UNINITIALIZED,\");\n+  else if (valid_p ())\n+    fprintf (file, \"VALID,\");\n+  else if (unknown_p ())\n+    fprintf (file, \"UNKNOWN,\");\n+  else if (empty_p ())\n+    fprintf (file, \"EMPTY,\");\n+  else\n+    fprintf (file, \"DIRTY,\");\n+\n+  fprintf (file, \"Demand field={%d(VL),\", demand_p (DEMAND_AVL));\n+  fprintf (file, \"%d(SEW),\", demand_p (DEMAND_SEW));\n+  fprintf (file, \"%d(LMUL),\", demand_p (DEMAND_LMUL));\n+  fprintf (file, \"%d(RATIO),\", demand_p (DEMAND_RATIO));\n+  fprintf (file, \"%d(TAIL_POLICY),\", demand_p (DEMAND_TAIL_POLICY));\n+  fprintf (file, \"%d(MASK_POLICY)}\\n\", demand_p (DEMAND_MASK_POLICY));\n+\n+  fprintf (file, \"AVL=\");\n+  print_rtl_single (file, get_avl ());\n+  fprintf (file, \"SEW=%d,\", get_sew ());\n+  fprintf (file, \"VLMUL=%d,\", get_vlmul ());\n+  fprintf (file, \"RATIO=%d,\", get_ratio ());\n+  fprintf (file, \"TAIL_POLICY=%d,\", get_ta ());\n+  fprintf (file, \"MASK_POLICY=%d\", get_ma ());\n+  fprintf (file, \"]\\n\");\n+\n+  if (valid_p ())\n+    {\n+      if (get_insn ())\n+\t{\n+\t  fprintf (file, \"RTL_SSA insn_info=\");\n+\t  pretty_printer pp;\n+\t  pp.buffer->stream = file;\n+\t  get_insn ()->print_full (&pp);\n+\t  pp_printf (&pp, \"\\n\");\n+\t  pp_flush (&pp);\n+\t}\n+      if (get_dirty_pat ())\n+\t{\n+\t  fprintf (file, \"Dirty RTL Pattern=\");\n+\t  print_rtl_single (file, get_dirty_pat ());\n+\t}\n+    }\n+}\n+\n+vector_infos_manager::vector_infos_manager ()\n+{\n+  vector_edge_list = nullptr;\n+  vector_kill = nullptr;\n+  vector_del = nullptr;\n+  vector_insert = nullptr;\n+  vector_antic = nullptr;\n+  vector_transp = nullptr;\n+  vector_comp = nullptr;\n+  vector_avin = nullptr;\n+  vector_avout = nullptr;\n+  vector_insn_infos.safe_grow (get_max_uid ());\n+  vector_block_infos.safe_grow (last_basic_block_for_fn (cfun));\n+  if (!optimize)\n+    {\n+      basic_block cfg_bb;\n+      rtx_insn *rinsn;\n+      FOR_ALL_BB_FN (cfg_bb, cfun)\n+\t{\n+\t  vector_block_infos[cfg_bb->index].local_dem = vector_insn_info ();\n+\t  vector_block_infos[cfg_bb->index].reaching_out = vector_insn_info ();\n+\t  FOR_BB_INSNS (cfg_bb, rinsn)\n+\t    vector_insn_infos[INSN_UID (rinsn)].parse_insn (rinsn);\n+\t}\n+    }\n+  else\n+    {\n+      for (const bb_info *bb : crtl->ssa->bbs ())\n+\t{\n+\t  vector_block_infos[bb->index ()].local_dem = vector_insn_info ();\n+\t  vector_block_infos[bb->index ()].reaching_out = vector_insn_info ();\n+\t  for (insn_info *insn : bb->real_insns ())\n+\t    vector_insn_infos[insn->uid ()].parse_insn (insn);\n+\t}\n+    }\n+}\n+\n+void\n+vector_infos_manager::create_expr (vector_insn_info &info)\n+{\n+  for (size_t i = 0; i < vector_exprs.length (); i++)\n+    if (*vector_exprs[i] == info)\n+      return;\n+  vector_exprs.safe_push (&info);\n+}\n+\n+size_t\n+vector_infos_manager::get_expr_id (const vector_insn_info &info) const\n+{\n+  for (size_t i = 0; i < vector_exprs.length (); i++)\n+    if (*vector_exprs[i] == info)\n+      return i;\n+  gcc_unreachable ();\n+}\n+\n+auto_vec<size_t>\n+vector_infos_manager::get_all_available_exprs (\n+  const vector_insn_info &info) const\n+{\n+  auto_vec<size_t> available_list;\n+  for (size_t i = 0; i < vector_exprs.length (); i++)\n+    if (info >= *vector_exprs[i])\n+      available_list.safe_push (i);\n+  return available_list;\n+}\n+\n+bool\n+vector_infos_manager::all_same_ratio_p (sbitmap bitdata) const\n+{\n+  if (bitmap_empty_p (bitdata))\n+    return false;\n+\n+  int ratio = -1;\n+  unsigned int bb_index;\n+  sbitmap_iterator sbi;\n+\n+  EXECUTE_IF_SET_IN_BITMAP (bitdata, 0, bb_index, sbi)\n+  {\n+    if (ratio == -1)\n+      ratio = vector_exprs[bb_index]->get_ratio ();\n+    else if (vector_exprs[bb_index]->get_ratio () != ratio)\n+      return false;\n+  }\n+  return true;\n+}\n+\n+size_t\n+vector_infos_manager::expr_set_num (sbitmap bitdata) const\n+{\n+  size_t count = 0;\n+  for (size_t i = 0; i < vector_exprs.length (); i++)\n+    if (bitmap_bit_p (bitdata, i))\n+      count++;\n+  return count;\n+}\n+\n+void\n+vector_infos_manager::release (void)\n+{\n+  if (!vector_insn_infos.is_empty ())\n+    vector_insn_infos.release ();\n+  if (!vector_block_infos.is_empty ())\n+    vector_block_infos.release ();\n+  if (!vector_exprs.is_empty ())\n+    vector_exprs.release ();\n+\n+  if (optimize > 0)\n+    {\n+      /* Finished. Free up all the things we've allocated.  */\n+      free_edge_list (vector_edge_list);\n+      sbitmap_vector_free (vector_del);\n+      sbitmap_vector_free (vector_insert);\n+      sbitmap_vector_free (vector_kill);\n+      sbitmap_vector_free (vector_antic);\n+      sbitmap_vector_free (vector_transp);\n+      sbitmap_vector_free (vector_comp);\n+      sbitmap_vector_free (vector_avin);\n+      sbitmap_vector_free (vector_avout);\n+    }\n+}\n+\n+void\n+vector_infos_manager::dump (FILE *file) const\n+{\n+  basic_block cfg_bb;\n+  rtx_insn *rinsn;\n+\n+  fprintf (file, \"\\n\");\n+  FOR_ALL_BB_FN (cfg_bb, cfun)\n+    {\n+      fprintf (file, \"Local vector info of <bb %d>:\\n\", cfg_bb->index);\n+      fprintf (file, \"<HEADER>=\");\n+      vector_block_infos[cfg_bb->index].local_dem.dump (file);\n+      FOR_BB_INSNS (cfg_bb, rinsn)\n+\t{\n+\t  if (!NONDEBUG_INSN_P (rinsn) || !has_vtype_op (rinsn))\n+\t    continue;\n+\t  fprintf (file, \"<insn %d>=\", INSN_UID (rinsn));\n+\t  const auto &info = vector_insn_infos[INSN_UID (rinsn)];\n+\t  info.dump (file);\n+\t}\n+      fprintf (file, \"<FOOTER>=\");\n+      vector_block_infos[cfg_bb->index].reaching_out.dump (file);\n+      fprintf (file, \"\\n\\n\");\n+    }\n+\n+  fprintf (file, \"\\n\");\n+  FOR_ALL_BB_FN (cfg_bb, cfun)\n+    {\n+      fprintf (file, \"Local properties of <bb %d>:\\n\", cfg_bb->index);\n+\n+      fprintf (file, \"<ANTLOC>=\");\n+      if (vector_antic == nullptr)\n+\tfprintf (file, \"(nil)\\n\");\n+      else\n+\tdump_bitmap_file (file, vector_antic[cfg_bb->index]);\n+\n+      fprintf (file, \"<AVLOC>=\");\n+      if (vector_comp == nullptr)\n+\tfprintf (file, \"(nil)\\n\");\n+      else\n+\tdump_bitmap_file (file, vector_comp[cfg_bb->index]);\n+\n+      fprintf (file, \"<TRANSP>=\");\n+      if (vector_transp == nullptr)\n+\tfprintf (file, \"(nil)\\n\");\n+      else\n+\tdump_bitmap_file (file, vector_transp[cfg_bb->index]);\n+\n+      fprintf (file, \"<KILL>=\");\n+      if (vector_kill == nullptr)\n+\tfprintf (file, \"(nil)\\n\");\n+      else\n+\tdump_bitmap_file (file, vector_kill[cfg_bb->index]);\n+    }\n+\n+  fprintf (file, \"\\n\");\n+  FOR_ALL_BB_FN (cfg_bb, cfun)\n+    {\n+      fprintf (file, \"Global LCM (Lazy code motion) result of <bb %d>:\\n\",\n+\t       cfg_bb->index);\n+\n+      fprintf (file, \"<AVIN>=\");\n+      if (vector_avin == nullptr)\n+\tfprintf (file, \"(nil)\\n\");\n+      else\n+\tdump_bitmap_file (file, vector_avin[cfg_bb->index]);\n+\n+      fprintf (file, \"<AVOUT>=\");\n+      if (vector_avout == nullptr)\n+\tfprintf (file, \"(nil)\\n\");\n+      else\n+\tdump_bitmap_file (file, vector_avout[cfg_bb->index]);\n+\n+      fprintf (file, \"<DELETE>=\");\n+      if (vector_del == nullptr)\n+\tfprintf (file, \"(nil)\\n\");\n+      else\n+\tdump_bitmap_file (file, vector_del[cfg_bb->index]);\n+    }\n+\n+  fprintf (file, \"\\nGlobal LCM (Lazy code motion) INSERT info:\\n\");\n+  for (size_t i = 0; i < vector_exprs.length (); i++)\n+    {\n+      for (int ed = 0; ed < NUM_EDGES (vector_edge_list); ed++)\n+\t{\n+\t  edge eg = INDEX_EDGE (vector_edge_list, ed);\n+\t  if (bitmap_bit_p (vector_insert[ed], i))\n+\t    fprintf (dump_file,\n+\t\t     \"INSERT edge %d from bb %d to bb %d for VSETVL \"\n+\t\t     \"expr[%ld]\\n\",\n+\t\t     ed, eg->src->index, eg->dest->index, i);\n+\t}\n+    }\n+}\n+\n+const pass_data pass_data_vsetvl = {\n+  RTL_PASS,\t /* type */\n+  \"vsetvl\",\t /* name */\n+  OPTGROUP_NONE, /* optinfo_flags */\n+  TV_NONE,\t /* tv_id */\n+  0,\t\t /* properties_required */\n+  0,\t\t /* properties_provided */\n+  0,\t\t /* properties_destroyed */\n+  0,\t\t /* todo_flags_start */\n+  0,\t\t /* todo_flags_finish */\n+};\n+\n+class pass_vsetvl : public rtl_opt_pass\n+{\n+private:\n+  class vector_infos_manager *m_vector_manager;\n+\n+  void simple_vsetvl (void) const;\n+  void lazy_vsetvl (void);\n+\n+  /* Phase 1.  */\n+  void compute_local_backward_infos (const bb_info *);\n+\n+  /* Phase 2.  */\n+  bool need_vsetvl (const vector_insn_info &, const vector_insn_info &) const;\n+  void transfer_before (vector_insn_info &, insn_info *) const;\n+  void transfer_after (vector_insn_info &, insn_info *) const;\n+  void emit_local_forward_vsetvls (const bb_info *);\n+\n+  /* Phase 3.  */\n+  void merge_successors (const basic_block, const basic_block);\n+  void compute_global_backward_infos (void);\n+\n+  /* Phase 4.  */\n+  void prune_expressions (void);\n+  void compute_local_properties (void);\n+  bool can_refine_vsetvl_p (const basic_block, uint8_t) const;\n+  void refine_vsetvls (void) const;\n+  void cleanup_vsetvls (void);\n+  bool commit_vsetvls (void);\n+  void pre_vsetvl (void);\n+\n+  /* Phase 5.  */\n+  void cleanup_insns (void) const;\n+\n+  void init (void);\n+  void done (void);\n+\n+public:\n+  pass_vsetvl (gcc::context *ctxt) : rtl_opt_pass (pass_data_vsetvl, ctxt) {}\n+\n+  /* opt_pass methods: */\n+  virtual bool gate (function *) final override { return TARGET_VECTOR; }\n+  virtual unsigned int execute (function *) final override;\n+}; // class pass_vsetvl\n+\n+/* Simple m_vsetvl_insert vsetvl for optimize == 0.  */\n+void\n+pass_vsetvl::simple_vsetvl (void) const\n+{\n+  if (dump_file)\n+    fprintf (dump_file,\n+\t     \"\\nEntering Simple VSETVL PASS and Handling %d basic blocks for \"\n+\t     \"function:%s\\n\",\n+\t     n_basic_blocks_for_fn (cfun), function_name (cfun));\n+\n+  basic_block cfg_bb;\n+  rtx_insn *rinsn;\n+  FOR_ALL_BB_FN (cfg_bb, cfun)\n+    {\n+      FOR_BB_INSNS (cfg_bb, rinsn)\n+\t{\n+\t  if (!NONDEBUG_INSN_P (rinsn))\n+\t    continue;\n+\t  if (has_vtype_op (rinsn))\n+\t    {\n+\t      const auto info\n+\t\t= m_vector_manager->vector_insn_infos[INSN_UID (rinsn)];\n+\t      emit_vsetvl_insn (VSETVL_DISCARD_RESULT, EMIT_BEFORE, info,\n+\t\t\t\tNULL_RTX, rinsn);\n+\t    }\n+\t}\n+    }\n+}\n+\n+/* Compute demanded information by backward data-flow analysis.  */\n+void\n+pass_vsetvl::compute_local_backward_infos (const bb_info *bb)\n+{\n+  vector_insn_info change;\n+  change.set_empty ();\n+\n+  auto &block_info = m_vector_manager->vector_block_infos[bb->index ()];\n+  block_info.reaching_out = change;\n+\n+  for (insn_info *insn : bb->reverse_real_nondebug_insns ())\n+    {\n+      auto &info = m_vector_manager->vector_insn_infos[insn->uid ()];\n+\n+      if (info.uninit_p ())\n+\t/* If it is uninitialized, propagate it directly.  */\n+\tinfo = change;\n+      else if (info.unknown_p ())\n+\tchange = info;\n+      else\n+\t{\n+\t  gcc_assert (info.valid_p () && \"Unexpected Invalid demanded info\");\n+\t  if (change.valid_p () && change.compatible_p (info))\n+\t    info = change.merge (info);\n+\t  change = info;\n+\t}\n+    }\n+\n+  block_info.local_dem = change;\n+  if (block_info.local_dem.empty_p ())\n+    block_info.reaching_out = block_info.local_dem;\n+}\n+\n+/* Return true if a dem_info is required to transition from curr_info to\n+   require before INSN.  */\n+bool\n+pass_vsetvl::need_vsetvl (const vector_insn_info &require,\n+\t\t\t  const vector_insn_info &curr_info) const\n+{\n+  if (!curr_info.valid_p () || curr_info.unknown_p () || curr_info.uninit_p ())\n+    return true;\n+\n+  if (require.compatible_p (curr_info))\n+    return false;\n+\n+  return true;\n+}\n+\n+/* Given an incoming state reaching INSN, modifies that state so that it is\n+   minimally compatible with INSN.  The resulting state is guaranteed to be\n+   semantically legal for INSN, but may not be the state requested by INSN.  */\n+void\n+pass_vsetvl::transfer_before (vector_insn_info &info, insn_info *insn) const\n+{\n+  if (!has_vtype_op (insn->rtl ()))\n+    return;\n+\n+  const vector_insn_info require\n+    = m_vector_manager->vector_insn_infos[insn->uid ()];\n+  if (info.valid_p () && !need_vsetvl (require, info))\n+    return;\n+  info = require;\n+}\n+\n+/* Given a state with which we evaluated insn (see transfer_before above for why\n+   this might be different that the state insn requested), modify the state to\n+   reflect the changes insn might make.  */\n+void\n+pass_vsetvl::transfer_after (vector_insn_info &info, insn_info *insn) const\n+{\n+  if (vector_config_insn_p (insn->rtl ()))\n+    {\n+      info = m_vector_manager->vector_insn_infos[insn->uid ()];\n+      return;\n+    }\n+\n+  /* TODO: Support fault first load info update VL in the future.  */\n+\n+  /* If this is something that updates VL/VTYPE that we don't know about, set\n+     the state to unknown.  */\n+  if (insn->is_call () || insn->is_asm ()\n+      || find_access (insn->defs (), VL_REGNUM)\n+      || find_access (insn->defs (), VTYPE_REGNUM))\n+    info = vector_insn_info::get_unknown ();\n+}\n+\n+/* Emit vsetvl within each block by forward data-flow analysis.  */\n+void\n+pass_vsetvl::emit_local_forward_vsetvls (const bb_info *bb)\n+{\n+  auto &block_info = m_vector_manager->vector_block_infos[bb->index ()];\n+  if (block_info.local_dem.empty_p ())\n+    return;\n+\n+  vector_insn_info curr_info;\n+  for (insn_info *insn : bb->real_nondebug_insns ())\n+    {\n+      const vector_insn_info prev_info = curr_info;\n+      transfer_before (curr_info, insn);\n+\n+      if (has_vtype_op (insn->rtl ()))\n+\t{\n+\t  if (static_cast<const vl_vtype_info &> (prev_info)\n+\t      != static_cast<const vl_vtype_info &> (curr_info))\n+\t    {\n+\t      const auto require\n+\t\t= m_vector_manager->vector_insn_infos[insn->uid ()];\n+\t      if (!require.compatible_p (\n+\t\t    static_cast<const vl_vtype_info &> (prev_info)))\n+\t\tinsert_vsetvl (EMIT_BEFORE, insn->rtl (), require, prev_info);\n+\t    }\n+\t}\n+\n+      transfer_after (curr_info, insn);\n+    }\n+\n+  block_info.reaching_out = curr_info;\n+}\n+\n+/* Merge all successors of Father except child node.  */\n+void\n+pass_vsetvl::merge_successors (const basic_block father,\n+\t\t\t       const basic_block child)\n+{\n+  edge e;\n+  edge_iterator ei;\n+  auto &father_info = m_vector_manager->vector_block_infos[father->index];\n+  gcc_assert (father_info.local_dem.dirty_p ()\n+\t      || father_info.local_dem.empty_p ());\n+  gcc_assert (father_info.reaching_out.dirty_p ()\n+\t      || father_info.reaching_out.empty_p ());\n+\n+  FOR_EACH_EDGE (e, ei, father->succs)\n+    {\n+      const basic_block succ = e->dest;\n+      if (succ->index == child->index)\n+\tcontinue;\n+\n+      const auto succ_info\n+\t= m_vector_manager->vector_block_infos[succ->index].local_dem;\n+\n+      if (!succ_info.valid_p ())\n+\tcontinue;\n+\n+      vector_insn_info new_info;\n+      if (father_info.reaching_out.dirty_p ())\n+\t{\n+\t  if (!father_info.reaching_out.compatible_p (succ_info))\n+\t    continue;\n+\n+\t  new_info = succ_info.merge (father_info.reaching_out, true);\n+\t}\n+      else\n+\tnew_info = succ_info;\n+\n+      new_info.set_dirty ();\n+      rtx new_pat = gen_vsetvl_pat (new_info.get_insn ()->rtl (), new_info);\n+      new_info.set_dirty_pat (new_pat);\n+\n+      father_info.local_dem = new_info;\n+      father_info.reaching_out = new_info;\n+    }\n+}\n+\n+/* Compute global backward demanded info.  */\n+void\n+pass_vsetvl::compute_global_backward_infos (void)\n+{\n+  /* We compute global infos by backward propagation.\n+     We want to have better performance in these following cases:\n+\n+\t1. for (size_t i = 0; i < n; i++) {\n+\t     if (i != cond) {\n+\t       vint8mf8_t v = *(vint8mf8_t*)(in + i + 100);\n+\t       *(vint8mf8_t*)(out + i + 100) = v;\n+\t     } else {\n+\t       vbool1_t v = *(vbool1_t*)(in + i + 400);\n+\t       *(vbool1_t*)(out + i + 400) = v;\n+\t     }\n+\t   }\n+\n+\t   Since we don't have any RVV instruction in the BEFORE blocks,\n+\t   LCM fails to optimize such case. We want to backward propagate\n+\t   them into empty blocks so that we could have better performance\n+\t   in LCM.\n+\n+\t2. bb 0:\n+\t     vsetvl e8,mf8 (demand RATIO)\n+\t   bb 1:\n+\t     vsetvl e32,mf2 (demand SEW and LMUL)\n+\t   We backward propagate the first VSETVL into e32,mf2 so that we\n+\t   could be able to eliminate the second VSETVL in LCM.  */\n+\n+  for (const bb_info *bb : crtl->ssa->reverse_bbs ())\n+    {\n+      basic_block cfg_bb = bb->cfg_bb ();\n+      const auto &prop\n+\t= m_vector_manager->vector_block_infos[cfg_bb->index].local_dem;\n+\n+      /* If there is nothing to propagate, just skip it.  */\n+      if (!prop.valid_or_dirty_p ())\n+\tcontinue;\n+\n+      if (!backward_propagate_worthwhile_p (\n+\t    cfg_bb, m_vector_manager->vector_block_infos[cfg_bb->index]))\n+\tcontinue;\n+\n+      edge e;\n+      edge_iterator ei;\n+      /* Backward propagate to each predecessor.  */\n+      FOR_EACH_EDGE (e, ei, cfg_bb->preds)\n+\t{\n+\t  rtx new_pat;\n+\t  auto &block_info\n+\t    = m_vector_manager->vector_block_infos[e->src->index];\n+\n+\t  /* We don't propagate through critical edges.  */\n+\t  if (e->flags & EDGE_COMPLEX)\n+\t    continue;\n+\t  if (e->src->index == ENTRY_BLOCK_PTR_FOR_FN (cfun)->index)\n+\t    continue;\n+\n+\t  if (block_info.reaching_out.unknown_p ())\n+\t    continue;\n+\t  else if (block_info.reaching_out.empty_p ())\n+\t    {\n+\t      if (!can_backward_propagate_p (crtl->ssa, e->src, prop))\n+\t\tcontinue;\n+\n+\t      if (dominate_probability_p (e))\n+\t\t{\n+\t\t  rtx new_pat = gen_vsetvl_pat (prop.get_insn ()->rtl (), prop);\n+\n+\t\t  block_info.reaching_out = prop;\n+\t\t  block_info.reaching_out.set_dirty ();\n+\t\t  block_info.reaching_out.set_dirty_pat (new_pat);\n+\t\t  block_info.local_dem = block_info.reaching_out;\n+\t\t}\n+\n+\t      merge_successors (e->src, cfg_bb);\n+\t    }\n+\t  else if (block_info.reaching_out.dirty_p ())\n+\t    {\n+\t      /* DIRTY -> DIRTY or VALID -> DIRTY.  */\n+\t      vector_insn_info new_info;\n+\n+\t      if (block_info.reaching_out.compatible_p (prop))\n+\t\t{\n+\t\t  if (block_info.reaching_out >= prop)\n+\t\t    continue;\n+\t\t  new_info = block_info.reaching_out.merge (prop, true);\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  if (dominate_probability_p (e))\n+\t\t    new_info = prop;\n+\t\t  else\n+\t\t    continue;\n+\t\t}\n+\n+\t      rtx new_pat\n+\t\t= gen_vsetvl_pat (new_info.get_insn ()->rtl (), new_info);\n+\t      new_info.set_dirty ();\n+\t      new_info.set_dirty_pat (new_pat);\n+\t      block_info.local_dem = new_info;\n+\t      block_info.reaching_out = new_info;\n+\t    }\n+\t  else\n+\t    {\n+\t      /* We not only change the info during backward propagation,\n+\t\t but also change the VSETVL instruction.  */\n+\t      gcc_assert (block_info.reaching_out.valid_p ());\n+\t      if (!block_info.reaching_out.compatible_p (prop))\n+\t\tcontinue;\n+\t      if (block_info.reaching_out >= prop)\n+\t\tcontinue;\n+\n+\t      vector_insn_info be_merged = block_info.reaching_out;\n+\t      if (block_info.local_dem == block_info.reaching_out)\n+\t\tbe_merged = block_info.local_dem;\n+\t      vector_insn_info new_info = be_merged.merge (prop, true);\n+\n+\t      rtx_insn *rinsn;\n+\t      if (vector_config_insn_p (new_info.get_insn ()->rtl ()))\n+\t\t{\n+\t\t  rinsn = new_info.get_insn ()->rtl ();\n+\t\t  gcc_assert (vsetvl_insn_p (rinsn)\n+\t\t\t      && \"Can't handle X0, rs1 vsetvli yet\");\n+\t\t}\n+\t      else\n+\t\t{\n+\t\t  gcc_assert (has_vtype_op (new_info.get_insn ()->rtl ()));\n+\t\t  rinsn = PREV_INSN (new_info.get_insn ()->rtl ());\n+\t\t  gcc_assert (vector_config_insn_p (rinsn));\n+\t\t}\n+\t      new_pat = gen_vsetvl_pat (rinsn, new_info);\n+\t      change_insn (rinsn, new_pat);\n+\t      if (block_info.local_dem == block_info.reaching_out)\n+\t\tblock_info.local_dem = new_info;\n+\t      block_info.reaching_out = new_info;\n+\t    }\n+\t}\n+    }\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"\\n\\nDirty blocks list: \");\n+      for (size_t i = 0; i < m_vector_manager->vector_block_infos.length ();\n+\t   i++)\n+\t{\n+\t  if (m_vector_manager->vector_block_infos[i].reaching_out.dirty_p ())\n+\t    fprintf (dump_file, \"%ld \", i);\n+\t}\n+      fprintf (dump_file, \"\\n\\n\");\n+    }\n+}\n+\n+/* Assemble the candidates expressions for LCM.  */\n+void\n+pass_vsetvl::prune_expressions (void)\n+{\n+  for (size_t i = 0; i < m_vector_manager->vector_block_infos.length (); i++)\n+    {\n+      if (m_vector_manager->vector_block_infos[i].local_dem.valid_or_dirty_p ())\n+\tm_vector_manager->create_expr (\n+\t  m_vector_manager->vector_block_infos[i].local_dem);\n+      if (m_vector_manager->vector_block_infos[i]\n+\t    .reaching_out.valid_or_dirty_p ())\n+\tm_vector_manager->create_expr (\n+\t  m_vector_manager->vector_block_infos[i].reaching_out);\n+    }\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"\\nThe total VSETVL expression num = %d\\n\",\n+\t       m_vector_manager->vector_exprs.length ());\n+      fprintf (dump_file, \"Expression List:\\n\");\n+      for (size_t i = 0; i < m_vector_manager->vector_exprs.length (); i++)\n+\t{\n+\t  fprintf (dump_file, \"Expr[%ld]:\\n\", i);\n+\t  m_vector_manager->vector_exprs[i]->dump (dump_file);\n+\t  fprintf (dump_file, \"\\n\");\n+\t}\n+    }\n+}\n+\n+void\n+pass_vsetvl::compute_local_properties (void)\n+{\n+  /* -  If T is locally available at the end of a block, then T' must be\n+\tavailable at the end of the same block. Since some optimization has\n+\toccurred earlier, T' might not be locally available, however, it must\n+\thave been previously computed on all paths. As a formula, T at AVLOC(B)\n+\timplies that T' at AVOUT(B).\n+\tAn \"available occurrence\" is one that is the last occurrence in the\n+\tbasic block and the operands are not modified by following statements in\n+\tthe basic block [including this insn].\n+\n+     -  If T is locally anticipated at the beginning of a block, then either\n+\tT', is locally anticipated or it is already available from previous\n+\tblocks. As a formula, this means that T at ANTLOC(B) implies that T' at\n+\tANTLOC(B) at AVIN(B).\n+\tAn \"anticipatable occurrence\" is one that is the first occurrence in the\n+\tbasic block, the operands are not modified in the basic block prior\n+\tto the occurrence and the output is not used between the start of\n+\tthe block and the occurrence.  */\n+\n+  basic_block cfg_bb;\n+  FOR_EACH_BB_FN (cfg_bb, cfun)\n+    {\n+      int curr_bb_idx = cfg_bb->index;\n+      const auto local_dem\n+\t= m_vector_manager->vector_block_infos[curr_bb_idx].local_dem;\n+      const auto reaching_out\n+\t= m_vector_manager->vector_block_infos[curr_bb_idx].reaching_out;\n+\n+      if (!local_dem.empty_p ())\n+\t{\n+\t  for (size_t i = 0; i < m_vector_manager->vector_exprs.length (); i++)\n+\t    bitmap_clear_bit (m_vector_manager->vector_transp[curr_bb_idx], i);\n+\t}\n+\n+      if (local_dem.valid_or_dirty_p ())\n+\t{\n+\t  const insn_info *header_insn = local_dem.get_insn ();\n+\t  size_t header_index = m_vector_manager->get_expr_id (local_dem);\n+\t  if (anticipatable_occurrence_p (header_insn, local_dem))\n+\t    bitmap_set_bit (m_vector_manager->vector_antic[curr_bb_idx],\n+\t\t\t    header_index);\n+\t}\n+\n+      if (reaching_out.valid_or_dirty_p ())\n+\t{\n+\t  const insn_info *footer_insn = reaching_out.get_insn ();\n+\t  size_t footer_index = m_vector_manager->get_expr_id (reaching_out);\n+\t  if (available_occurrence_p (footer_insn, reaching_out))\n+\t    bitmap_set_bit (m_vector_manager->vector_comp[curr_bb_idx],\n+\t\t\t    footer_index);\n+\t  auto_vec<size_t> available_list\n+\t    = m_vector_manager->get_all_available_exprs (reaching_out);\n+\t  for (size_t i = 0; i < available_list.length (); i++)\n+\t    bitmap_set_bit (m_vector_manager->vector_comp[curr_bb_idx],\n+\t\t\t    available_list[i]);\n+\t}\n+    }\n+\n+  /* Compute kill for each basic block using:\n+\n+     ~(TRANSP | COMP)\n+  */\n+\n+  FOR_EACH_BB_FN (cfg_bb, cfun)\n+    {\n+      bitmap_ior (m_vector_manager->vector_kill[cfg_bb->index],\n+\t\t  m_vector_manager->vector_transp[cfg_bb->index],\n+\t\t  m_vector_manager->vector_comp[cfg_bb->index]);\n+      bitmap_not (m_vector_manager->vector_kill[cfg_bb->index],\n+\t\t  m_vector_manager->vector_kill[cfg_bb->index]);\n+    }\n+\n+  FOR_EACH_BB_FN (cfg_bb, cfun)\n+    {\n+      edge e;\n+      edge_iterator ei;\n+\n+      /* If the current block is the destination of an abnormal edge, we\n+\t kill all trapping (for PRE) and memory (for hoist) expressions\n+\t because we won't be able to properly place the instruction on\n+\t the edge.  So make them neither anticipatable nor transparent.\n+\t This is fairly conservative.\n+\n+\t ??? For hoisting it may be necessary to check for set-and-jump\n+\t instructions here, not just for abnormal edges.  The general problem\n+\t is that when an expression cannot not be placed right at the end of\n+\t a basic block we should account for any side-effects of a subsequent\n+\t jump instructions that could clobber the expression.  It would\n+\t be best to implement this check along the lines of\n+\t should_hoist_expr_to_dom where the target block is already known\n+\t and, hence, there's no need to conservatively prune expressions on\n+\t \"intermediate\" set-and-jump instructions.  */\n+      FOR_EACH_EDGE (e, ei, cfg_bb->preds)\n+\tif (e->flags & EDGE_COMPLEX)\n+\t  {\n+\t    bitmap_clear (m_vector_manager->vector_antic[cfg_bb->index]);\n+\t    bitmap_clear (m_vector_manager->vector_transp[cfg_bb->index]);\n+\t  }\n+    }\n+}\n+\n+/* Return true if VSETVL in the block can be refined as vsetvl zero,zero.  */\n+bool\n+pass_vsetvl::can_refine_vsetvl_p (const basic_block cfg_bb, uint8_t ratio) const\n+{\n+  if (!m_vector_manager->all_same_ratio_p (\n+\tm_vector_manager->vector_avin[cfg_bb->index]))\n+    return false;\n+\n+  size_t expr_id\n+    = bitmap_first_set_bit (m_vector_manager->vector_avin[cfg_bb->index]);\n+  if (m_vector_manager->vector_exprs[expr_id]->get_ratio () != ratio)\n+    return false;\n+\n+  edge e;\n+  edge_iterator ei;\n+  bool all_valid_p = true;\n+  FOR_EACH_EDGE (e, ei, cfg_bb->preds)\n+    {\n+      if (bitmap_empty_p (m_vector_manager->vector_avout[e->src->index]))\n+\t{\n+\t  all_valid_p = false;\n+\t  break;\n+\t}\n+    }\n+\n+  if (!all_valid_p)\n+    return false;\n+  return true;\n+}\n+\n+/* Optimize athe case like this:\n+\n+      bb 0:\n+\tvsetvl 0 a5,zero,e8,mf8\n+\tinsn 0 (demand SEW + LMUL)\n+      bb 1:\n+\tvsetvl 1 a5,zero,e16,mf4\n+\tinsn 1 (demand SEW + LMUL)\n+\n+   In this case, we should be able to refine\n+   vsetvl 1 into vsetvl zero, zero according AVIN.  */\n+void\n+pass_vsetvl::refine_vsetvls (void) const\n+{\n+  basic_block cfg_bb;\n+  FOR_EACH_BB_FN (cfg_bb, cfun)\n+    {\n+      auto info = m_vector_manager->vector_block_infos[cfg_bb->index].local_dem;\n+      insn_info *insn = info.get_insn ();\n+      if (!info.valid_p ())\n+\tcontinue;\n+\n+      rtx_insn *rinsn = insn->rtl ();\n+      if (!can_refine_vsetvl_p (cfg_bb, info.get_ratio ()))\n+\tcontinue;\n+\n+      if (!vector_config_insn_p (rinsn))\n+\trinsn = PREV_INSN (rinsn);\n+      rtx new_pat = gen_vsetvl_pat (VSETVL_VTYPE_CHANGE_ONLY, info, NULL_RTX);\n+      change_insn (rinsn, new_pat);\n+    }\n+}\n+\n+void\n+pass_vsetvl::cleanup_vsetvls ()\n+{\n+  basic_block cfg_bb;\n+  FOR_EACH_BB_FN (cfg_bb, cfun)\n+    {\n+      auto &info\n+\t= m_vector_manager->vector_block_infos[cfg_bb->index].reaching_out;\n+      gcc_assert (m_vector_manager->expr_set_num (\n+\t\t    m_vector_manager->vector_del[cfg_bb->index])\n+\t\t  <= 1);\n+      for (size_t i = 0; i < m_vector_manager->vector_exprs.length (); i++)\n+\t{\n+\t  if (bitmap_bit_p (m_vector_manager->vector_del[cfg_bb->index], i))\n+\t    {\n+\t      if (info.dirty_p ())\n+\t\tinfo.set_unknown ();\n+\t      else\n+\t\t{\n+\t\t  insn_info *insn\n+\t\t    = m_vector_manager->vector_exprs[i]->get_insn ();\n+\t\t  gcc_assert (insn && insn->rtl ());\n+\t\t  rtx_insn *rinsn;\n+\t\t  if (vector_config_insn_p (insn->rtl ()))\n+\t\t    rinsn = insn->rtl ();\n+\t\t  else\n+\t\t    {\n+\t\t      gcc_assert (has_vtype_op (insn->rtl ()));\n+\t\t      rinsn = PREV_INSN (insn->rtl ());\n+\t\t      gcc_assert (\n+\t\t\tvector_config_insn_p (PREV_INSN (insn->rtl ())));\n+\t\t    }\n+\t\t  eliminate_insn (rinsn);\n+\t\t}\n+\t    }\n+\t}\n+    }\n+}\n+\n+bool\n+pass_vsetvl::commit_vsetvls (void)\n+{\n+  bool need_commit = false;\n+\n+  for (int ed = 0; ed < NUM_EDGES (m_vector_manager->vector_edge_list); ed++)\n+    {\n+      for (size_t i = 0; i < m_vector_manager->vector_exprs.length (); i++)\n+\t{\n+\t  edge eg = INDEX_EDGE (m_vector_manager->vector_edge_list, ed);\n+\t  if (bitmap_bit_p (m_vector_manager->vector_insert[ed], i))\n+\t    {\n+\t      const vector_insn_info *require\n+\t\t= m_vector_manager->vector_exprs[i];\n+\t      gcc_assert (require->valid_or_dirty_p ());\n+\t      rtl_profile_for_edge (eg);\n+\t      start_sequence ();\n+\n+\t      insn_info *insn = require->get_insn ();\n+\t      vector_insn_info prev_info = vector_insn_info ();\n+\t      if (m_vector_manager->all_same_ratio_p (\n+\t\t    m_vector_manager->vector_avout[eg->src->index]))\n+\t\t{\n+\t\t  size_t first = bitmap_first_set_bit (\n+\t\t    m_vector_manager->vector_avout[eg->src->index]);\n+\t\t  prev_info = *m_vector_manager->vector_exprs[first];\n+\t\t}\n+\n+\t      insert_vsetvl (EMIT_DIRECT, insn->rtl (), *require, prev_info);\n+\t      rtx_insn *rinsn = get_insns ();\n+\t      end_sequence ();\n+\t      default_rtl_profile ();\n+\n+\t      /* We should not get an abnormal edge here.  */\n+\t      gcc_assert (!(eg->flags & EDGE_ABNORMAL));\n+\t      need_commit = true;\n+\t      insert_insn_on_edge (rinsn, eg);\n+\t    }\n+\t}\n+    }\n+\n+  basic_block cfg_bb;\n+  FOR_EACH_BB_FN (cfg_bb, cfun)\n+    {\n+      const auto reaching_out\n+\t= m_vector_manager->vector_block_infos[cfg_bb->index].reaching_out;\n+      if (!reaching_out.dirty_p ())\n+\tcontinue;\n+\n+      rtx new_pat = reaching_out.get_dirty_pat ();\n+      if (can_refine_vsetvl_p (cfg_bb, reaching_out.get_ratio ()))\n+\tnew_pat\n+\t  = gen_vsetvl_pat (VSETVL_VTYPE_CHANGE_ONLY, reaching_out, NULL_RTX);\n+\n+      start_sequence ();\n+      emit_insn (new_pat);\n+      rtx_insn *rinsn = get_insns ();\n+      end_sequence ();\n+      insert_insn_end_basic_block (rinsn, cfg_bb);\n+      if (dump_file)\n+\t{\n+\t  fprintf (dump_file,\n+\t\t   \"\\nInsert vsetvl insn %d at the end of <bb %d>:\\n\",\n+\t\t   INSN_UID (rinsn), cfg_bb->index);\n+\t  print_rtl_single (dump_file, rinsn);\n+\t}\n+    }\n+\n+  return need_commit;\n+}\n+\n+void\n+pass_vsetvl::pre_vsetvl (void)\n+{\n+  /* Compute entity list.  */\n+  prune_expressions ();\n+\n+  /* Create the bitmap vectors.  */\n+  m_vector_manager->vector_antic\n+    = sbitmap_vector_alloc (last_basic_block_for_fn (cfun),\n+\t\t\t    m_vector_manager->vector_exprs.length ());\n+  m_vector_manager->vector_transp\n+    = sbitmap_vector_alloc (last_basic_block_for_fn (cfun),\n+\t\t\t    m_vector_manager->vector_exprs.length ());\n+  m_vector_manager->vector_comp\n+    = sbitmap_vector_alloc (last_basic_block_for_fn (cfun),\n+\t\t\t    m_vector_manager->vector_exprs.length ());\n+  m_vector_manager->vector_avin\n+    = sbitmap_vector_alloc (last_basic_block_for_fn (cfun),\n+\t\t\t    m_vector_manager->vector_exprs.length ());\n+  m_vector_manager->vector_avout\n+    = sbitmap_vector_alloc (last_basic_block_for_fn (cfun),\n+\t\t\t    m_vector_manager->vector_exprs.length ());\n+  m_vector_manager->vector_kill\n+    = sbitmap_vector_alloc (last_basic_block_for_fn (cfun),\n+\t\t\t    m_vector_manager->vector_exprs.length ());\n+\n+  bitmap_vector_ones (m_vector_manager->vector_transp,\n+\t\t      last_basic_block_for_fn (cfun));\n+  bitmap_vector_clear (m_vector_manager->vector_antic,\n+\t\t       last_basic_block_for_fn (cfun));\n+  bitmap_vector_clear (m_vector_manager->vector_comp,\n+\t\t       last_basic_block_for_fn (cfun));\n+  compute_local_properties ();\n+  m_vector_manager->vector_edge_list = pre_edge_lcm_avs (\n+    m_vector_manager->vector_exprs.length (), m_vector_manager->vector_transp,\n+    m_vector_manager->vector_comp, m_vector_manager->vector_antic,\n+    m_vector_manager->vector_kill, m_vector_manager->vector_avin,\n+    m_vector_manager->vector_avout, &m_vector_manager->vector_insert,\n+    &m_vector_manager->vector_del);\n+\n+  /* We should dump the information before CFG is changed. Otherwise it will\n+     produce ICE (internal compiler error).  */\n+  if (dump_file)\n+    m_vector_manager->dump (dump_file);\n+\n+  refine_vsetvls ();\n+  cleanup_vsetvls ();\n+  bool need_commit = commit_vsetvls ();\n+  if (need_commit)\n+    commit_edge_insertions ();\n+}\n+\n+void\n+pass_vsetvl::cleanup_insns (void) const\n+{\n+  for (const bb_info *bb : crtl->ssa->bbs ())\n+    {\n+      for (insn_info *insn : bb->real_nondebug_insns ())\n+\t{\n+\t  rtx_insn *rinsn = insn->rtl ();\n+\n+\t  if (vlmax_avl_insn_p (rinsn))\n+\t    {\n+\t      eliminate_insn (rinsn);\n+\t      continue;\n+\t    }\n+\n+\t  /* Erase the AVL operand from the instruction.  */\n+\t  if (!has_vl_op (rinsn) || !REG_P (get_vl (rinsn)))\n+\t    continue;\n+\t  rtx avl = get_vl (rinsn);\n+\t  if (count_occurrences (PATTERN (rinsn), avl, true) == 1)\n+\t    {\n+\t      /* Get the list of uses for the new instruction.  */\n+\t      auto attempt = crtl->ssa->new_change_attempt ();\n+\t      insn_change change (insn);\n+\t      /* Remove the use of the substituted value.  */\n+\t      access_array_builder uses_builder (attempt);\n+\t      uses_builder.reserve (insn->num_uses () - 1);\n+\t      for (use_info *use : insn->uses ())\n+\t\tif (use != find_access (insn->uses (), REGNO (avl)))\n+\t\t  uses_builder.quick_push (use);\n+\t      use_array new_uses = use_array (uses_builder.finish ());\n+\t      change.new_uses = new_uses;\n+\t      change.move_range = insn->ebb ()->insn_range ();\n+\t      rtx pat = simplify_replace_rtx (PATTERN (rinsn), avl, const0_rtx);\n+\t      gcc_assert (change_insn (crtl->ssa, change, insn, pat));\n+\t    }\n+\t}\n+    }\n+}\n+\n+void\n+pass_vsetvl::init (void)\n+{\n+  if (optimize > 0)\n+    {\n+      /* Initialization of RTL_SSA.  */\n+      calculate_dominance_info (CDI_DOMINATORS);\n+      df_analyze ();\n+      crtl->ssa = new function_info (cfun);\n+    }\n+\n+  m_vector_manager = new vector_infos_manager ();\n+\n+  if (dump_file)\n+    {\n+      fprintf (dump_file, \"\\nPrologue: Initialize vector infos\\n\");\n+      m_vector_manager->dump (dump_file);\n+    }\n+}\n+\n+void\n+pass_vsetvl::done (void)\n+{\n+  if (optimize > 0)\n+    {\n+      /* Finalization of RTL_SSA.  */\n+      free_dominance_info (CDI_DOMINATORS);\n+      if (crtl->ssa->perform_pending_updates ())\n+\tcleanup_cfg (0);\n+      delete crtl->ssa;\n+      crtl->ssa = nullptr;\n+    }\n+  m_vector_manager->release ();\n+  delete m_vector_manager;\n+  m_vector_manager = nullptr;\n+}\n+\n+/* Lazy vsetvl insertion for optimize > 0. */\n+void\n+pass_vsetvl::lazy_vsetvl (void)\n+{\n+  if (dump_file)\n+    fprintf (dump_file,\n+\t     \"\\nEntering Lazy VSETVL PASS and Handling %d basic blocks for \"\n+\t     \"function:%s\\n\",\n+\t     n_basic_blocks_for_fn (cfun), function_name (cfun));\n+\n+  /* Phase 1 - Compute the local dems within each block.\n+     The data-flow analysis within each block is backward analysis.  */\n+  if (dump_file)\n+    fprintf (dump_file, \"\\nPhase 1: Compute local backward vector infos\\n\");\n+  for (const bb_info *bb : crtl->ssa->bbs ())\n+    compute_local_backward_infos (bb);\n+  if (dump_file)\n+    m_vector_manager->dump (dump_file);\n+\n+  /* Phase 2 - Emit vsetvl instructions within each basic block according to\n+     demand, compute and save ANTLOC && AVLOC of each block.  */\n+  if (dump_file)\n+    fprintf (dump_file,\n+\t     \"\\nPhase 2: Emit vsetvl instruction within each block\\n\");\n+  for (const bb_info *bb : crtl->ssa->bbs ())\n+    emit_local_forward_vsetvls (bb);\n+  if (dump_file)\n+    m_vector_manager->dump (dump_file);\n+\n+  /* Phase 3 - Propagate demanded info across blocks.  */\n+  if (dump_file)\n+    fprintf (dump_file, \"\\nPhase 3: Demands propagation across blocks\\n\");\n+  compute_global_backward_infos ();\n+  if (dump_file)\n+    m_vector_manager->dump (dump_file);\n+\n+  /* Phase 4 - Lazy code motion.  */\n+  if (dump_file)\n+    fprintf (dump_file, \"\\nPhase 4: PRE vsetvl by Lazy code motion (LCM)\\n\");\n+  pre_vsetvl ();\n+\n+  /* Phase 5 - Cleanup AVL && VL operand of RVV instruction.  */\n+  if (dump_file)\n+    fprintf (dump_file, \"\\nPhase 5: Cleanup AVL and VL operands\\n\");\n+  cleanup_insns ();\n+}\n+\n+/* Main entry point for this pass.  */\n+unsigned int\n+pass_vsetvl::execute (function *)\n+{\n+  if (n_basic_blocks_for_fn (cfun) <= 0)\n+    return 0;\n+\n+  /* The reason we have this since we didn't finish splitting yet\n+     when optimize == 0. In this case, we should conservatively\n+     split all instructions here to make sure we don't miss any\n+     RVV instruction.  */\n+  if (!optimize)\n+    split_all_insns ();\n+\n+  /* Early return for there is no vector instructions.  */\n+  if (!has_vector_insn (cfun))\n+    return 0;\n+\n+  init ();\n+\n+  if (!optimize)\n+    simple_vsetvl ();\n+  else\n+    lazy_vsetvl ();\n+\n+  done ();\n+  return 0;\n+}\n+\n+rtl_opt_pass *\n+make_pass_vsetvl (gcc::context *ctxt)\n+{\n+  return new pass_vsetvl (ctxt);\n+}"}, {"sha": "ad9bb27cebf71fee8d84d32dc01433a36d7131e1", "filename": "gcc/config/riscv/riscv-vsetvl.h", "status": "added", "additions": 344, "deletions": 0, "changes": 344, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig%2Friscv%2Friscv-vsetvl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig%2Friscv%2Friscv-vsetvl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Friscv-vsetvl.h?ref=9243c3d1b63b9092a82178392145f9e9d62423d9", "patch": "@@ -0,0 +1,344 @@\n+/* VSETVL pass header for RISC-V 'V' Extension for GNU compiler.\n+   Copyright(C) 2022-2022 Free Software Foundation, Inc.\n+   Contributed by Juzhe Zhong (juzhe.zhong@rivai.ai), RiVAI Technologies Ltd.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify\n+it under the terms of the GNU General Public License as published by\n+the Free Software Foundation; either version 3, or(at your option)\n+any later version.\n+\n+GCC is distributed in the hope that it will be useful,\n+but WITHOUT ANY WARRANTY; without even the implied warranty of\n+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+GNU General Public License for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#ifndef GCC_RISCV_VSETVL_H\n+#define GCC_RISCV_VSETVL_H\n+\n+#define IS_AGNOSTIC(VALUE) (bool) (VALUE & 0x1 || (VALUE >> 1 & 0x1))\n+\n+namespace riscv_vector {\n+\n+/* Classification of vsetvl instruction.  */\n+enum vsetvl_type\n+{\n+  VSETVL_NORMAL,\n+  VSETVL_VTYPE_CHANGE_ONLY,\n+  VSETVL_DISCARD_RESULT,\n+};\n+\n+enum emit_type\n+{\n+  /* emit_insn directly.  */\n+  EMIT_DIRECT,\n+  EMIT_BEFORE,\n+  EMIT_AFTER,\n+};\n+\n+enum demand_type\n+{\n+  DEMAND_AVL,\n+  DEMAND_SEW,\n+  DEMAND_LMUL,\n+  DEMAND_RATIO,\n+  DEMAND_TAIL_POLICY,\n+  DEMAND_MASK_POLICY,\n+  NUM_DEMAND\n+};\n+\n+/* AVL info for RVV instruction. Most RVV instructions have AVL operand in\n+   implicit dependency. The AVL comparison between 2 RVV instructions is\n+   very important since it affects our decision whether we should insert\n+   a vsetvl instruction in this situation. AVL operand of all RVV instructions\n+   can only be either a const_int value with < 32 or a reg value which can be\n+   define by either a real RTL instruction or a PHI instruction. So we need a\n+   standalone method to define AVL comparison and we can not simpily use\n+   operator \"==\" to compare 2 RTX value since it's to strict which will make\n+   use miss a lot of optimization opportunities. This method handle these\n+   following cases:\n+\n+     -  Background:\n+\t  Insert-vsetvl PASS is working after RA.\n+\n+     -  Terminology:\n+\t  - pr: Pseudo-register.\n+\t  - hr: Hardware-register.\n+\n+     -  Case 1:\n+\n+\tBefore RA:\n+\t  li pr138,13\n+\t  insn1 (implicit depend on pr138).\n+\t  li pr138,14\n+\t  insn2 (implicit depend on pr139).\n+\n+\tAfter RA:\n+\t  li hr5,13\n+\t  insn1 (implicit depend on hr5).\n+\t  li hr5,14\n+\t  insn2 (implicit depend on hr5).\n+\n+\tCorrect IR after vsetvl PASS:\n+\t  li hr5,13\n+\t  vsetvl1 zero,hr5....\n+\t  insn1 (implicit depend on hr5).\n+\t  li hr5,14\n+\t  vsetvl2 zero,hr5....\n+\t  insn2 (implicit depend on hr5).\n+\n+     In this case, both insn1 and insn2 are using hr5 as the same AVL.\n+     If we use \"rtx_equal_p\" or \"REGNO (AVL1) == REGNO (AVL)\", we will end\n+     up with missing the vsetvl2 instruction which creates wrong result.\n+\n+     Note: Using \"==\" operator to compare 2 AVL RTX strictly can fix this\n+     issue. However, it is a too strict comparison method since not all member\n+     variables in RTX data structure are not neccessary to be the same. It will\n+     make us miss a lot of optimization opportunities.\n+\n+     -  Case 2:\n+\n+\tAfter RA:\n+\tbb 0:\n+\t  li hr5,13\n+\tbb 1:\n+\t  li hr5,14\n+\tbb2:\n+\t  insn1 (implicit depend on hr5).\n+\t  insn2 (implicit depend on hr5).\n+\n+     In this case, we may end up with different AVL RTX and produce redundant\n+     vsetvl instruction.\n+\n+     VALUE is the implicit dependency in each RVV instruction.\n+     SOURCE is the source definition information of AVL operand.  */\n+class avl_info\n+{\n+private:\n+  rtx m_value;\n+  rtl_ssa::set_info *m_source;\n+\n+public:\n+  avl_info () : m_value (NULL_RTX), m_source (nullptr) {}\n+  avl_info (rtx, rtl_ssa::set_info *);\n+  rtx get_value () const { return m_value; }\n+  rtl_ssa::set_info *get_source () const { return m_source; }\n+  avl_info &operator= (const avl_info &);\n+  bool operator== (const avl_info &) const;\n+  bool operator!= (const avl_info &) const;\n+};\n+\n+/* Basic structure to save VL/VTYPE information.  */\n+struct vl_vtype_info\n+{\n+protected:\n+  /* AVL can be either register or const_int.  */\n+  avl_info m_avl;\n+  /* Fields from VTYPE. The VTYPE checking depend on the flag\n+     dem_* before.  */\n+  uint8_t m_sew;\n+  riscv_vector::vlmul_type m_vlmul;\n+  uint8_t m_ratio;\n+  bool m_ta;\n+  bool m_ma;\n+\n+public:\n+  void set_sew (uint8_t sew) { m_sew = sew; }\n+  void set_vlmul (riscv_vector::vlmul_type vlmul) { m_vlmul = vlmul; }\n+  void set_ratio (uint8_t ratio) { m_ratio = ratio; }\n+  void set_ta (bool ta) { m_ta = ta; }\n+  void set_ma (bool ma) { m_ma = ma; }\n+\n+  vl_vtype_info ()\n+    : m_avl (avl_info ()), m_sew (0), m_vlmul (riscv_vector::LMUL_RESERVED),\n+      m_ratio (0), m_ta (0), m_ma (0)\n+  {}\n+  vl_vtype_info (const vl_vtype_info &) = default;\n+  vl_vtype_info &operator= (const vl_vtype_info &) = default;\n+  vl_vtype_info (avl_info, uint8_t, riscv_vector::vlmul_type, uint8_t, bool,\n+\t\t bool);\n+\n+  bool operator== (const vl_vtype_info &) const;\n+  bool operator!= (const vl_vtype_info &) const;\n+\n+  bool has_avl_imm () const { return get_avl () && CONST_INT_P (get_avl ()); }\n+  bool has_avl_reg () const { return get_avl () && REG_P (get_avl ()); }\n+  bool has_avl_no_reg () const { return !get_avl (); }\n+  bool has_non_zero_avl () const;\n+\n+  rtx get_avl () const { return m_avl.get_value (); }\n+  avl_info get_avl_info () const { return m_avl; }\n+  void set_avl_info (const avl_info &avl) { m_avl = avl; }\n+  uint8_t get_sew () const { return m_sew; }\n+  riscv_vector::vlmul_type get_vlmul () const { return m_vlmul; }\n+  uint8_t get_ratio () const { return m_ratio; }\n+  bool get_ta () const { return m_ta; }\n+  bool get_ma () const { return m_ma; }\n+\n+  bool same_avl_p (const vl_vtype_info &) const;\n+  bool same_vtype_p (const vl_vtype_info &) const;\n+  bool same_vlmax_p (const vl_vtype_info &) const;\n+};\n+\n+class vector_insn_info : public vl_vtype_info\n+{\n+private:\n+  enum state_type\n+  {\n+    UNINITIALIZED,\n+    VALID,\n+    UNKNOWN,\n+    EMPTY,\n+\n+    /* The block is polluted as containing VSETVL instruction during dem\n+       backward propagation to gain better LCM optimization even though\n+       such VSETVL instruction is not really emit yet during this time.  */\n+    DIRTY\n+  };\n+\n+  enum state_type m_state;\n+\n+  bool m_demands[NUM_DEMAND];\n+\n+  /* TODO: Assume INSN1 = INSN holding of definition of AVL.\n+\t\t  INSN2 = INSN that is inserted a vsetvl insn before.\n+     We may need to add a new member to save INSN of holding AVL.\n+     m_insn is holding the INSN that is inserted a vsetvl insn before in\n+     Phase 2. Ideally, most of the time INSN1 == INSN2. However, considering\n+     such case:\n+\n+\tvmv.x.s (INSN2)\n+\tvle8.v (INSN1)\n+\n+     If these 2 instructions are compatible, we should only issue a vsetvl INSN\n+     (with AVL included) before vmv.x.s, but vmv.x.s is not the INSN holding the\n+     definition of AVL.  */\n+  rtl_ssa::insn_info *m_insn;\n+  /* Save instruction pattern for Dirty block.\n+     Since empty block may be polluted as a dirty block during dem backward\n+     propagation (phase 3) which is intending to cheat LCM there is a VSETVL\n+     instruction here to gain better LCM optimization. Such instruction is not\n+     emit yet, we save this here and then emit it in the 4th phase if it is\n+     necessary.  */\n+  rtx m_dirty_pat;\n+\n+  /* Parse the instruction to get VL/VTYPE information and demanding\n+   * information.  */\n+  /* This is only called by simple_vsetvl subroutine when optimize == 0.\n+     Since RTL_SSA can not be enabled when optimize == 0, we don't initialize\n+     the m_insn.  */\n+  void parse_insn (rtx_insn *);\n+  /* This is only called by lazy_vsetvl subroutine when optimize > 0.\n+     We use RTL_SSA framework to initialize the insn_info.  */\n+  void parse_insn (rtl_ssa::insn_info *);\n+\n+  friend class vector_infos_manager;\n+\n+public:\n+  vector_insn_info ()\n+    : vl_vtype_info (), m_state (UNINITIALIZED), m_demands{false},\n+      m_insn (nullptr), m_dirty_pat (NULL_RTX)\n+  {}\n+\n+  bool operator> (const vector_insn_info &) const;\n+  bool operator>= (const vector_insn_info &) const;\n+  bool operator== (const vector_insn_info &) const;\n+\n+  bool uninit_p () const { return m_state == UNINITIALIZED; }\n+  bool valid_p () const { return m_state == VALID; }\n+  bool unknown_p () const { return m_state == UNKNOWN; }\n+  bool empty_p () const { return m_state == EMPTY; }\n+  bool dirty_p () const { return m_state == DIRTY; }\n+  bool valid_or_dirty_p () const\n+  {\n+    return m_state == VALID || m_state == DIRTY;\n+  }\n+\n+  static vector_insn_info get_unknown ()\n+  {\n+    vector_insn_info info;\n+    info.set_unknown ();\n+    return info;\n+  }\n+\n+  void set_valid () { m_state = VALID; }\n+  void set_unknown () { m_state = UNKNOWN; }\n+  void set_empty () { m_state = EMPTY; }\n+  void set_dirty () { m_state = DIRTY; }\n+  void set_dirty_pat (rtx pat) { m_dirty_pat = pat; }\n+  void set_insn (rtl_ssa::insn_info *insn) { m_insn = insn; }\n+\n+  bool demand_p (enum demand_type type) const { return m_demands[type]; }\n+  void demand (enum demand_type type) { m_demands[type] = true; }\n+  void demand_vl_vtype ();\n+  void undemand (enum demand_type type) { m_demands[type] = false; }\n+\n+  bool compatible_p (const vector_insn_info &) const;\n+  bool compatible_avl_p (const vl_vtype_info &) const;\n+  bool compatible_vtype_p (const vl_vtype_info &) const;\n+  bool compatible_p (const vl_vtype_info &) const;\n+  vector_insn_info merge (const vector_insn_info &, bool) const;\n+\n+  rtl_ssa::insn_info *get_insn () const { return m_insn; }\n+  rtx get_dirty_pat () const { return m_dirty_pat; }\n+\n+  void dump (FILE *) const;\n+};\n+\n+struct vector_block_info\n+{\n+  /* The local_dem vector insn_info of the block.  */\n+  vector_insn_info local_dem;\n+\n+  /* The reaching_out vector insn_info of the block.  */\n+  vector_insn_info reaching_out;\n+\n+  vector_block_info () = default;\n+};\n+\n+class vector_infos_manager\n+{\n+public:\n+  auto_vec<vector_insn_info> vector_insn_infos;\n+  auto_vec<vector_block_info> vector_block_infos;\n+  auto_vec<vector_insn_info *> vector_exprs;\n+\n+  struct edge_list *vector_edge_list;\n+  sbitmap *vector_kill;\n+  sbitmap *vector_del;\n+  sbitmap *vector_insert;\n+  sbitmap *vector_antic;\n+  sbitmap *vector_transp;\n+  sbitmap *vector_comp;\n+  sbitmap *vector_avin;\n+  sbitmap *vector_avout;\n+\n+  vector_infos_manager ();\n+\n+  /* Create a new expr in expr list if it is not exist.  */\n+  void create_expr (vector_insn_info &);\n+\n+  /* Get the expr id of the pair of expr.  */\n+  size_t get_expr_id (const vector_insn_info &) const;\n+\n+  /* Return the number of expr that is set in the bitmap.  */\n+  size_t expr_set_num (sbitmap) const;\n+\n+  /* Get all relaxer expression id for corresponding vector info.  */\n+  auto_vec<size_t> get_all_available_exprs (const vector_insn_info &) const;\n+\n+  /* Return true if all expression set in bitmap are same ratio.  */\n+  bool all_same_ratio_p (sbitmap) const;\n+\n+  void release (void);\n+\n+  void dump (FILE *) const;\n+};\n+\n+} // namespace riscv_vector\n+#endif"}, {"sha": "7af9f5402ec713a4d505d89a25f0111bd4810e9d", "filename": "gcc/config/riscv/t-riscv", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig%2Friscv%2Ft-riscv", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig%2Friscv%2Ft-riscv", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Ft-riscv?ref=9243c3d1b63b9092a82178392145f9e9d62423d9", "patch": "@@ -51,6 +51,14 @@ riscv-c.o: $(srcdir)/config/riscv/riscv-c.cc $(CONFIG_H) $(SYSTEM_H) \\\n \t$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \\\n \t\t$(srcdir)/config/riscv/riscv-c.cc\n \n+riscv-vsetvl.o: $(srcdir)/config/riscv/riscv-vsetvl.cc \\\n+  $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) $(REGS_H) \\\n+  $(TARGET_H) tree-pass.h df.h rtl-ssa.h cfgcleanup.h insn-config.h \\\n+  insn-attr.h insn-opinit.h tm-constrs.h cfgrtl.h cfganal.h lcm.h \\\n+  predict.h profile-count.h $(srcdir)/config/riscv/riscv-vsetvl.h\n+\t$(COMPILER) -c $(ALL_COMPILERFLAGS) $(ALL_CPPFLAGS) $(INCLUDES) \\\n+\t\t$(srcdir)/config/riscv/riscv-vsetvl.cc\n+\n riscv-d.o: $(srcdir)/config/riscv/riscv-d.cc\n \t$(COMPILE) $<\n \t$(POSTCOMPILE)"}, {"sha": "985373838bb0ee3d2e9bdf55ab70d23ee93c2f03", "filename": "gcc/config/riscv/vector.md", "status": "modified", "additions": 104, "deletions": 27, "changes": 131, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig%2Friscv%2Fvector.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9243c3d1b63b9092a82178392145f9e9d62423d9/gcc%2Fconfig%2Friscv%2Fvector.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Friscv%2Fvector.md?ref=9243c3d1b63b9092a82178392145f9e9d62423d9", "patch": "@@ -32,6 +32,7 @@\n   UNSPEC_VSETVL\n   UNSPEC_VUNDEF\n   UNSPEC_VPREDICATE\n+  UNSPEC_VLMAX\n ])\n \n (define_constants [\n@@ -94,7 +95,13 @@\n \t (const_int 32)\n \t (eq_attr \"mode\" \"VNx1DI,VNx2DI,VNx4DI,VNx8DI,\\\n \t\t\t  VNx1DF,VNx2DF,VNx4DF,VNx8DF\")\n-\t (const_int 64)]\n+\t (const_int 64)\n+\n+\t (eq_attr \"type\" \"vsetvl\")\n+\t (if_then_else (eq_attr \"INSN_CODE (curr_insn) == CODE_FOR_vsetvldi\n+\t\t\t\t || INSN_CODE (curr_insn) == CODE_FOR_vsetvlsi\")\n+\t\t       (symbol_ref \"INTVAL (operands[2])\")\n+\t\t       (const_int INVALID_ATTRIBUTE))]\n \t(const_int INVALID_ATTRIBUTE)))\n \n ;; Ditto to LMUL.\n@@ -142,7 +149,12 @@\n \t (eq_attr \"mode\" \"VNx4DI,VNx4DF\")\n \t   (symbol_ref \"riscv_vector::get_vlmul(E_VNx4DImode)\")\n \t (eq_attr \"mode\" \"VNx8DI,VNx8DF\")\n-\t   (symbol_ref \"riscv_vector::get_vlmul(E_VNx8DImode)\")]\n+\t   (symbol_ref \"riscv_vector::get_vlmul(E_VNx8DImode)\")\n+\t (eq_attr \"type\" \"vsetvl\")\n+\t (if_then_else (eq_attr \"INSN_CODE (curr_insn) == CODE_FOR_vsetvldi\n+\t\t\t\t || INSN_CODE (curr_insn) == CODE_FOR_vsetvlsi\")\n+\t\t       (symbol_ref \"INTVAL (operands[3])\")\n+\t\t       (const_int INVALID_ATTRIBUTE))]\n \t(const_int INVALID_ATTRIBUTE)))\n \n ;; It is valid for instruction that require sew/lmul ratio.\n@@ -219,6 +231,34 @@\n \t (const_int 6)]\n \t(const_int INVALID_ATTRIBUTE)))\n \n+;; The index of operand[] to get the mask policy op.\n+(define_attr \"avl_type_op_idx\" \"\"\n+  (cond [(eq_attr \"type\" \"vlde,vlde,vste,vimov,vimov,vimov,vfmov,vlds,vlds\")\n+\t (const_int 7)\n+\t (eq_attr \"type\" \"vldm,vstm,vimov,vmalu,vmalu\")\n+\t (const_int 5)]\n+\t(const_int INVALID_ATTRIBUTE)))\n+\n+;; The tail policy op value.\n+(define_attr \"ta\" \"\"\n+  (cond [(eq_attr \"type\" \"vlde,vste,vimov,vfmov,vlds\")\n+\t   (symbol_ref \"riscv_vector::get_ta(operands[5])\")]\n+\t(const_int INVALID_ATTRIBUTE)))\n+\n+;; The mask policy op value.\n+(define_attr \"ma\" \"\"\n+  (cond [(eq_attr \"type\" \"vlde,vlds\")\n+\t   (symbol_ref \"riscv_vector::get_ma(operands[6])\")]\n+\t(const_int INVALID_ATTRIBUTE)))\n+\n+;; The avl type value.\n+(define_attr \"avl_type\" \"\"\n+  (cond [(eq_attr \"type\" \"vlde,vlde,vste,vimov,vimov,vimov,vfmov,vlds,vlds\")\n+\t   (symbol_ref \"INTVAL (operands[7])\")\n+\t (eq_attr \"type\" \"vldm,vstm,vimov,vmalu,vmalu\")\n+\t   (symbol_ref \"INTVAL (operands[5])\")]\n+\t(const_int INVALID_ATTRIBUTE)))\n+\n ;; -----------------------------------------------------------------\n ;; ---- Miscellaneous Operations\n ;; -----------------------------------------------------------------\n@@ -229,6 +269,37 @@\n   \"TARGET_VECTOR\"\n   \"\")\n \n+;; This pattern is used to hold the AVL operand for\n+;; RVV instructions that implicity use VLMAX AVL.\n+;; RVV instruction implicitly use GPR that is ultimately\n+;; defined by this pattern is safe for VSETVL pass emit\n+;; a vsetvl instruction modify this register after RA.\n+;; Case 1:\n+;;   vlmax_avl a5\n+;;   ... (across many blocks)\n+;;   vadd (implicit use a5)  ====> emit: vsetvl a5,zero\n+;; Case 2:\n+;;   vlmax_avl a5\n+;;   ... (across many blocks)\n+;;   mv a6,a5\n+;;   ... (across many blocks)\n+;;   vadd (implicit use a6)  ====> emit: vsetvl a6,zero\n+;; Case 3:\n+;;   vlmax_avl a5\n+;;   ... (across many blocks)\n+;;   store mem,a5 (spill)\n+;;   ... (across many blocks)\n+;;   load a7,mem (spill)\n+;;   ... (across many blocks)\n+;;   vadd (implicit use a7)  ====> emit: vsetvl a7,zero\n+;; Such cases are all safe for VSETVL PASS to emit a vsetvl\n+;; instruction that modifies the AVL operand.\n+(define_insn \"@vlmax_avl<mode>\"\n+  [(set (match_operand:P 0 \"register_operand\" \"=r\")\n+\t(unspec:P [(match_operand:P 1 \"const_int_operand\" \"i\")] UNSPEC_VLMAX))]\n+  \"TARGET_VECTOR\"\n+  \"\")\n+\n ;; -----------------------------------------------------------------\n ;; ---- Moves Operations\n ;; -----------------------------------------------------------------\n@@ -482,32 +553,35 @@\n   [(set_attr \"type\" \"vsetvl\")\n    (set_attr \"mode\" \"<MODE>\")])\n \n-;; We keep it as no side effects before reload_completed.\n-;; In this case, we can gain benefits from different GCC\n-;; internal PASS such as cprop, fwprop, combine,...etc.\n-\n-;; Then recover it for \"insert-vsetvl\" and \"sched2\" PASS\n-;; in order to get correct codegen.\n-(define_insn_and_split \"@vsetvl<mode>_no_side_effects\"\n-  [(set (match_operand:P 0 \"register_operand\" \"=r\")\n-\t(unspec:P [(match_operand:P 1 \"csr_operand\" \"rK\")\n-\t\t   (match_operand 2 \"const_int_operand\" \"i\")\n-\t\t   (match_operand 3 \"const_int_operand\" \"i\")\n-\t\t   (match_operand 4 \"const_int_operand\" \"i\")\n-\t\t   (match_operand 5 \"const_int_operand\" \"i\")] UNSPEC_VSETVL))]\n+;; vsetvl zero,zero,vtype instruction.\n+;; This pattern has no side effects and does not set X0 register.\n+(define_insn \"vsetvl_vtype_change_only\"\n+  [(set (reg:SI VTYPE_REGNUM)\n+\t(unspec:SI\n+\t  [(match_operand 0 \"const_int_operand\" \"i\")\n+\t   (match_operand 1 \"const_int_operand\" \"i\")\n+\t   (match_operand 2 \"const_int_operand\" \"i\")\n+\t   (match_operand 3 \"const_int_operand\" \"i\")] UNSPEC_VSETVL))]\n   \"TARGET_VECTOR\"\n-  \"#\"\n-  \"&& reload_completed\"\n-  [(parallel\n-    [(set (match_dup 0)\n-\t  (unspec:P [(match_dup 1) (match_dup 2) (match_dup 3)\n-\t\t     (match_dup 4) (match_dup 5)] UNSPEC_VSETVL))\n-     (set (reg:SI VL_REGNUM)\n-\t  (unspec:SI [(match_dup 1) (match_dup 2) (match_dup 3)] UNSPEC_VSETVL))\n-     (set (reg:SI VTYPE_REGNUM)\n-\t  (unspec:SI [(match_dup 2) (match_dup 3) (match_dup 4)\n-\t\t      (match_dup 5)] UNSPEC_VSETVL))])]\n-  \"\"\n+  \"vsetvli\\tzero,zero,e%0,%m1,t%p2,m%p3\"\n+  [(set_attr \"type\" \"vsetvl\")\n+   (set_attr \"mode\" \"SI\")])\n+\n+;; vsetvl zero,rs1,vtype instruction.\n+;; The reason we need this pattern since we should avoid setting X0 register\n+;; in vsetvl instruction pattern.\n+(define_insn \"@vsetvl_discard_result<mode>\"\n+  [(set (reg:SI VL_REGNUM)\n+\t(unspec:SI [(match_operand:P 0 \"csr_operand\" \"rK\")\n+\t\t    (match_operand 1 \"const_int_operand\" \"i\")\n+\t\t    (match_operand 2 \"const_int_operand\" \"i\")] UNSPEC_VSETVL))\n+   (set (reg:SI VTYPE_REGNUM)\n+\t(unspec:SI [(match_dup 1)\n+\t\t    (match_dup 2)\n+\t\t    (match_operand 3 \"const_int_operand\" \"i\")\n+\t\t    (match_operand 4 \"const_int_operand\" \"i\")] UNSPEC_VSETVL))]\n+  \"TARGET_VECTOR\"\n+  \"vsetvli\\tzero,%0,e%1,%m2,t%p3,m%p4\"\n   [(set_attr \"type\" \"vsetvl\")\n    (set_attr \"mode\" \"<MODE>\")])\n \n@@ -563,6 +637,7 @@\n \t     (match_operand 4 \"vector_length_operand\"    \" rK,  rK,    rK,    rK,    rK\")\n \t     (match_operand 5 \"const_int_operand\"        \"  i,   i,     i,     i,     i\")\n \t     (match_operand 6 \"const_int_operand\"        \"  i,   i,     i,     i,     i\")\n+\t     (match_operand 7 \"const_int_operand\"        \"  i,   i,     i,     i,     i\")\n \t     (reg:SI VL_REGNUM)\n \t     (reg:SI VTYPE_REGNUM)] UNSPEC_VPREDICATE)\n \t  (match_operand:V 3 \"vector_move_operand\"       \"  m,   m,    vr,    vr, viWc0\")\n@@ -593,6 +668,7 @@\n \t  (unspec:VB\n \t    [(match_operand:VB 1 \"vector_mask_operand\"   \"Wc1, Wc1, Wc1, Wc1, Wc1\")\n \t     (match_operand 4 \"vector_length_operand\"    \" rK,  rK,  rK,  rK,  rK\")\n+\t     (match_operand 5 \"const_int_operand\"        \"  i,   i,   i,   i,   i\")\n \t     (reg:SI VL_REGNUM)\n \t     (reg:SI VTYPE_REGNUM)] UNSPEC_VPREDICATE)\n \t  (match_operand:VB 3 \"vector_move_operand\"      \"  m,  vr,  vr, Wc0, Wc1\")\n@@ -628,6 +704,7 @@\n \t     (match_operand 4 \"vector_length_operand\"         \" rK,  rK,  rK,  rK\")\n \t     (match_operand 5 \"const_int_operand\"             \"  i,   i,   i,   i\")\n \t     (match_operand 6 \"const_int_operand\"             \"  i,   i,   i,   i\")\n+\t     (match_operand 7 \"const_int_operand\"             \"  i,   i,   i,   i\")\n \t     (reg:SI VL_REGNUM)\n \t     (reg:SI VTYPE_REGNUM)] UNSPEC_VPREDICATE)\n \t  (vec_duplicate:V"}]}