{"sha": "b9c257340bd20ec0e7debffc62ed3e3901c2908d", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YjljMjU3MzQwYmQyMGVjMGU3ZGViZmZjNjJlZDNlMzkwMWMyOTA4ZA==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@linaro.org", "date": "2017-12-20T12:52:58Z"}, "committer": {"name": "Richard Sandiford", "email": "rsandifo@gcc.gnu.org", "date": "2017-12-20T12:52:58Z"}, "message": "poly_int: ao_ref and vn_reference_op_t\n\nThis patch changes the offset, size and max_size fields\nof ao_ref from HOST_WIDE_INT to poly_int64 and propagates\nthe change through the code that references it.  This includes\nchanging the off field of vn_reference_op_struct in the same way.\n\n2017-12-20  Richard Sandiford  <richard.sandiford@linaro.org>\n\t    Alan Hayward  <alan.hayward@arm.com>\n\t    David Sherwood  <david.sherwood@arm.com>\n\ngcc/\n\t* inchash.h (inchash::hash::add_poly_int): New function.\n\t* tree-ssa-alias.h (ao_ref::offset, ao_ref::size, ao_ref::max_size):\n\tUse poly_int64 rather than HOST_WIDE_INT.\n\t(ao_ref::max_size_known_p): New function.\n\t* tree-ssa-sccvn.h (vn_reference_op_struct::off): Use poly_int64_pod\n\trather than HOST_WIDE_INT.\n\t* tree-ssa-alias.c (ao_ref_base): Apply get_ref_base_and_extent\n\tto temporaries until its interface is adjusted to match.\n\t(ao_ref_init_from_ptr_and_size): Handle polynomial offsets and sizes.\n\t(aliasing_component_refs_p, decl_refs_may_alias_p)\n\t(indirect_ref_may_alias_decl_p, indirect_refs_may_alias_p): Take\n\tthe offsets and max_sizes as poly_int64s instead of HOST_WIDE_INTs.\n\t(refs_may_alias_p_1, stmt_kills_ref_p): Adjust for changes to\n\tao_ref fields.\n\t* alias.c (ao_ref_from_mem): Likewise.\n\t* tree-ssa-dce.c (mark_aliased_reaching_defs_necessary_1): Likewise.\n\t* tree-ssa-dse.c (valid_ao_ref_for_dse, normalize_ref)\n\t(clear_bytes_written_by, setup_live_bytes_from_ref, compute_trims)\n\t(maybe_trim_complex_store, maybe_trim_constructor_store)\n\t(live_bytes_read, dse_classify_store): Likewise.\n\t* tree-ssa-sccvn.c (vn_reference_compute_hash, vn_reference_eq):\n\t(copy_reference_ops_from_ref, ao_ref_init_from_vn_reference)\n\t(fully_constant_vn_reference_p, valueize_refs_1): Likewise.\n\t(vn_reference_lookup_3): Likewise.\n\t* tree-ssa-uninit.c (warn_uninitialized_vars): Likewise.\n\nCo-Authored-By: Alan Hayward <alan.hayward@arm.com>\nCo-Authored-By: David Sherwood <david.sherwood@arm.com>\n\nFrom-SVN: r255872", "tree": {"sha": "dcbb7292f9ffa26eed52e3bf4b9f6f183f9df8cd", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/dcbb7292f9ffa26eed52e3bf4b9f6f183f9df8cd"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/b9c257340bd20ec0e7debffc62ed3e3901c2908d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b9c257340bd20ec0e7debffc62ed3e3901c2908d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b9c257340bd20ec0e7debffc62ed3e3901c2908d", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b9c257340bd20ec0e7debffc62ed3e3901c2908d/comments", "author": null, "committer": null, "parents": [{"sha": "5ffca72c5db83f53562a968a30d3955126f044f2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5ffca72c5db83f53562a968a30d3955126f044f2", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5ffca72c5db83f53562a968a30d3955126f044f2"}], "stats": {"total": 537, "additions": 292, "deletions": 245}, "files": [{"sha": "ccbe1b6d7e2ff9dbaad97a5d0dbf1280d0388b46", "filename": "gcc/ChangeLog", "status": "modified", "additions": 30, "deletions": 0, "changes": 30, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=b9c257340bd20ec0e7debffc62ed3e3901c2908d", "patch": "@@ -1,3 +1,33 @@\n+2017-12-20  Richard Sandiford  <richard.sandiford@linaro.org>\n+\t    Alan Hayward  <alan.hayward@arm.com>\n+\t    David Sherwood  <david.sherwood@arm.com>\n+\n+\t* inchash.h (inchash::hash::add_poly_int): New function.\n+\t* tree-ssa-alias.h (ao_ref::offset, ao_ref::size, ao_ref::max_size):\n+\tUse poly_int64 rather than HOST_WIDE_INT.\n+\t(ao_ref::max_size_known_p): New function.\n+\t* tree-ssa-sccvn.h (vn_reference_op_struct::off): Use poly_int64_pod\n+\trather than HOST_WIDE_INT.\n+\t* tree-ssa-alias.c (ao_ref_base): Apply get_ref_base_and_extent\n+\tto temporaries until its interface is adjusted to match.\n+\t(ao_ref_init_from_ptr_and_size): Handle polynomial offsets and sizes.\n+\t(aliasing_component_refs_p, decl_refs_may_alias_p)\n+\t(indirect_ref_may_alias_decl_p, indirect_refs_may_alias_p): Take\n+\tthe offsets and max_sizes as poly_int64s instead of HOST_WIDE_INTs.\n+\t(refs_may_alias_p_1, stmt_kills_ref_p): Adjust for changes to\n+\tao_ref fields.\n+\t* alias.c (ao_ref_from_mem): Likewise.\n+\t* tree-ssa-dce.c (mark_aliased_reaching_defs_necessary_1): Likewise.\n+\t* tree-ssa-dse.c (valid_ao_ref_for_dse, normalize_ref)\n+\t(clear_bytes_written_by, setup_live_bytes_from_ref, compute_trims)\n+\t(maybe_trim_complex_store, maybe_trim_constructor_store)\n+\t(live_bytes_read, dse_classify_store): Likewise.\n+\t* tree-ssa-sccvn.c (vn_reference_compute_hash, vn_reference_eq):\n+\t(copy_reference_ops_from_ref, ao_ref_init_from_vn_reference)\n+\t(fully_constant_vn_reference_p, valueize_refs_1): Likewise.\n+\t(vn_reference_lookup_3): Likewise.\n+\t* tree-ssa-uninit.c (warn_uninitialized_vars): Likewise.\n+\n 2017-12-20  Richard Sandiford  <richard.sandiford@linaro.org>\n \t    Alan Hayward  <alan.hayward@arm.com>\n \t    David Sherwood  <david.sherwood@arm.com>"}, {"sha": "b1ff8fdd887195b593e5efb98ff0973fa612222b", "filename": "gcc/alias.c", "status": "modified", "additions": 10, "deletions": 11, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Falias.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Falias.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Falias.c?ref=b9c257340bd20ec0e7debffc62ed3e3901c2908d", "patch": "@@ -331,9 +331,9 @@ ao_ref_from_mem (ao_ref *ref, const_rtx mem)\n   /* If MEM_OFFSET/MEM_SIZE get us outside of ref->offset/ref->max_size\n      drop ref->ref.  */\n   if (MEM_OFFSET (mem) < 0\n-      || (ref->max_size != -1\n-\t  && ((MEM_OFFSET (mem) + MEM_SIZE (mem)) * BITS_PER_UNIT\n-\t      > ref->max_size)))\n+      || (ref->max_size_known_p ()\n+\t  && maybe_gt ((MEM_OFFSET (mem) + MEM_SIZE (mem)) * BITS_PER_UNIT,\n+\t\t       ref->max_size)))\n     ref->ref = NULL_TREE;\n \n   /* Refine size and offset we got from analyzing MEM_EXPR by using\n@@ -344,19 +344,18 @@ ao_ref_from_mem (ao_ref *ref, const_rtx mem)\n \n   /* The MEM may extend into adjacent fields, so adjust max_size if\n      necessary.  */\n-  if (ref->max_size != -1\n-      && ref->size > ref->max_size)\n-    ref->max_size = ref->size;\n+  if (ref->max_size_known_p ())\n+    ref->max_size = upper_bound (ref->max_size, ref->size);\n \n-  /* If MEM_OFFSET and MEM_SIZE get us outside of the base object of\n+  /* If MEM_OFFSET and MEM_SIZE might get us outside of the base object of\n      the MEM_EXPR punt.  This happens for STRICT_ALIGNMENT targets a lot.  */\n   if (MEM_EXPR (mem) != get_spill_slot_decl (false)\n-      && (ref->offset < 0\n+      && (maybe_lt (ref->offset, 0)\n \t  || (DECL_P (ref->base)\n \t      && (DECL_SIZE (ref->base) == NULL_TREE\n-\t\t  || TREE_CODE (DECL_SIZE (ref->base)) != INTEGER_CST\n-\t\t  || wi::ltu_p (wi::to_offset (DECL_SIZE (ref->base)),\n-\t\t\t\tref->offset + ref->size)))))\n+\t\t  || !poly_int_tree_p (DECL_SIZE (ref->base))\n+\t\t  || maybe_lt (wi::to_poly_offset (DECL_SIZE (ref->base)),\n+\t\t\t       ref->offset + ref->size)))))\n     return false;\n \n   return true;"}, {"sha": "ba5a7de6c5bc93433e9ebcb186905f03af65083d", "filename": "gcc/inchash.h", "status": "modified", "additions": 8, "deletions": 0, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Finchash.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Finchash.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Finchash.h?ref=b9c257340bd20ec0e7debffc62ed3e3901c2908d", "patch": "@@ -57,6 +57,14 @@ class hash\n     val = iterative_hash_hashval_t (v, val);\n   }\n \n+  /* Add polynomial value V, treating each element as an unsigned int.  */\n+  template<unsigned int N, typename T>\n+  void add_poly_int (const poly_int_pod<N, T> &v)\n+  {\n+    for (unsigned int i = 0; i < N; ++i)\n+      add_int (v.coeffs[i]);\n+  }\n+\n   /* Add HOST_WIDE_INT value V.  */\n   void add_hwi (HOST_WIDE_INT v)\n   {"}, {"sha": "ce63cc1110938c8c143607dea14ded4b39cf1577", "filename": "gcc/tree-ssa-alias.c", "status": "modified", "additions": 52, "deletions": 53, "changes": 105, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Ftree-ssa-alias.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Ftree-ssa-alias.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-alias.c?ref=b9c257340bd20ec0e7debffc62ed3e3901c2908d", "patch": "@@ -635,11 +635,15 @@ tree\n ao_ref_base (ao_ref *ref)\n {\n   bool reverse;\n+  HOST_WIDE_INT offset, size, max_size;\n \n   if (ref->base)\n     return ref->base;\n-  ref->base = get_ref_base_and_extent (ref->ref, &ref->offset, &ref->size,\n-\t\t\t\t       &ref->max_size, &reverse);\n+  ref->base = get_ref_base_and_extent (ref->ref, &offset, &size,\n+\t\t\t\t       &max_size, &reverse);\n+  ref->offset = offset;\n+  ref->size = size;\n+  ref->max_size = max_size;\n   return ref->base;\n }\n \n@@ -679,7 +683,8 @@ ao_ref_alias_set (ao_ref *ref)\n void\n ao_ref_init_from_ptr_and_size (ao_ref *ref, tree ptr, tree size)\n {\n-  HOST_WIDE_INT t, size_hwi, extra_offset = 0;\n+  HOST_WIDE_INT t;\n+  poly_int64 size_hwi, extra_offset = 0;\n   ref->ref = NULL_TREE;\n   if (TREE_CODE (ptr) == SSA_NAME)\n     {\n@@ -689,11 +694,10 @@ ao_ref_init_from_ptr_and_size (ao_ref *ref, tree ptr, tree size)\n \tptr = gimple_assign_rhs1 (stmt);\n       else if (is_gimple_assign (stmt)\n \t       && gimple_assign_rhs_code (stmt) == POINTER_PLUS_EXPR\n-\t       && TREE_CODE (gimple_assign_rhs2 (stmt)) == INTEGER_CST)\n+\t       && ptrdiff_tree_p (gimple_assign_rhs2 (stmt), &extra_offset))\n \t{\n \t  ptr = gimple_assign_rhs1 (stmt);\n-\t  extra_offset = BITS_PER_UNIT\n-\t\t\t * int_cst_value (gimple_assign_rhs2 (stmt));\n+\t  extra_offset *= BITS_PER_UNIT;\n \t}\n     }\n \n@@ -717,8 +721,8 @@ ao_ref_init_from_ptr_and_size (ao_ref *ref, tree ptr, tree size)\n     }\n   ref->offset += extra_offset;\n   if (size\n-      && tree_fits_shwi_p (size)\n-      && (size_hwi = tree_to_shwi (size)) <= HOST_WIDE_INT_MAX / BITS_PER_UNIT)\n+      && poly_int_tree_p (size, &size_hwi)\n+      && coeffs_in_range_p (size_hwi, 0, HOST_WIDE_INT_MAX / BITS_PER_UNIT))\n     ref->max_size = ref->size = size_hwi * BITS_PER_UNIT;\n   else\n     ref->max_size = ref->size = -1;\n@@ -779,11 +783,11 @@ static bool\n aliasing_component_refs_p (tree ref1,\n \t\t\t   alias_set_type ref1_alias_set,\n \t\t\t   alias_set_type base1_alias_set,\n-\t\t\t   HOST_WIDE_INT offset1, HOST_WIDE_INT max_size1,\n+\t\t\t   poly_int64 offset1, poly_int64 max_size1,\n \t\t\t   tree ref2,\n \t\t\t   alias_set_type ref2_alias_set,\n \t\t\t   alias_set_type base2_alias_set,\n-\t\t\t   HOST_WIDE_INT offset2, HOST_WIDE_INT max_size2,\n+\t\t\t   poly_int64 offset2, poly_int64 max_size2,\n \t\t\t   bool ref2_is_decl)\n {\n   /* If one reference is a component references through pointers try to find a\n@@ -825,7 +829,7 @@ aliasing_component_refs_p (tree ref1,\n       offset2 -= offadj;\n       get_ref_base_and_extent (base1, &offadj, &sztmp, &msztmp, &reverse);\n       offset1 -= offadj;\n-      return ranges_overlap_p (offset1, max_size1, offset2, max_size2);\n+      return ranges_maybe_overlap_p (offset1, max_size1, offset2, max_size2);\n     }\n   /* If we didn't find a common base, try the other way around.  */\n   refp = &ref1;\n@@ -844,7 +848,7 @@ aliasing_component_refs_p (tree ref1,\n       offset1 -= offadj;\n       get_ref_base_and_extent (base2, &offadj, &sztmp, &msztmp, &reverse);\n       offset2 -= offadj;\n-      return ranges_overlap_p (offset1, max_size1, offset2, max_size2);\n+      return ranges_maybe_overlap_p (offset1, max_size1, offset2, max_size2);\n     }\n \n   /* If we have two type access paths B1.path1 and B2.path2 they may\n@@ -1090,9 +1094,9 @@ nonoverlapping_component_refs_p (const_tree x, const_tree y)\n \n static bool\n decl_refs_may_alias_p (tree ref1, tree base1,\n-\t\t       HOST_WIDE_INT offset1, HOST_WIDE_INT max_size1,\n+\t\t       poly_int64 offset1, poly_int64 max_size1,\n \t\t       tree ref2, tree base2,\n-\t\t       HOST_WIDE_INT offset2, HOST_WIDE_INT max_size2)\n+\t\t       poly_int64 offset2, poly_int64 max_size2)\n {\n   gcc_checking_assert (DECL_P (base1) && DECL_P (base2));\n \n@@ -1102,7 +1106,7 @@ decl_refs_may_alias_p (tree ref1, tree base1,\n \n   /* If both references are based on the same variable, they cannot alias if\n      the accesses do not overlap.  */\n-  if (!ranges_overlap_p (offset1, max_size1, offset2, max_size2))\n+  if (!ranges_maybe_overlap_p (offset1, max_size1, offset2, max_size2))\n     return false;\n \n   /* For components with variable position, the above test isn't sufficient,\n@@ -1124,12 +1128,11 @@ decl_refs_may_alias_p (tree ref1, tree base1,\n \n static bool\n indirect_ref_may_alias_decl_p (tree ref1 ATTRIBUTE_UNUSED, tree base1,\n-\t\t\t       HOST_WIDE_INT offset1,\n-\t\t\t       HOST_WIDE_INT max_size1 ATTRIBUTE_UNUSED,\n+\t\t\t       poly_int64 offset1, poly_int64 max_size1,\n \t\t\t       alias_set_type ref1_alias_set,\n \t\t\t       alias_set_type base1_alias_set,\n \t\t\t       tree ref2 ATTRIBUTE_UNUSED, tree base2,\n-\t\t\t       HOST_WIDE_INT offset2, HOST_WIDE_INT max_size2,\n+\t\t\t       poly_int64 offset2, poly_int64 max_size2,\n \t\t\t       alias_set_type ref2_alias_set,\n \t\t\t       alias_set_type base2_alias_set, bool tbaa_p)\n {\n@@ -1185,14 +1188,15 @@ indirect_ref_may_alias_decl_p (tree ref1 ATTRIBUTE_UNUSED, tree base1,\n      is bigger than the size of the decl we can't possibly access the\n      decl via that pointer.  */\n   if (DECL_SIZE (base2) && COMPLETE_TYPE_P (TREE_TYPE (ptrtype1))\n-      && TREE_CODE (DECL_SIZE (base2)) == INTEGER_CST\n-      && TREE_CODE (TYPE_SIZE (TREE_TYPE (ptrtype1))) == INTEGER_CST\n+      && poly_int_tree_p (DECL_SIZE (base2))\n+      && poly_int_tree_p (TYPE_SIZE (TREE_TYPE (ptrtype1)))\n       /* ???  This in turn may run afoul when a decl of type T which is\n \t a member of union type U is accessed through a pointer to\n \t type U and sizeof T is smaller than sizeof U.  */\n       && TREE_CODE (TREE_TYPE (ptrtype1)) != UNION_TYPE\n       && TREE_CODE (TREE_TYPE (ptrtype1)) != QUAL_UNION_TYPE\n-      && tree_int_cst_lt (DECL_SIZE (base2), TYPE_SIZE (TREE_TYPE (ptrtype1))))\n+      && known_lt (wi::to_poly_widest (DECL_SIZE (base2)),\n+\t\t   wi::to_poly_widest (TYPE_SIZE (TREE_TYPE (ptrtype1)))))\n     return false;\n \n   if (!ref2)\n@@ -1203,8 +1207,8 @@ indirect_ref_may_alias_decl_p (tree ref1 ATTRIBUTE_UNUSED, tree base1,\n   dbase2 = ref2;\n   while (handled_component_p (dbase2))\n     dbase2 = TREE_OPERAND (dbase2, 0);\n-  HOST_WIDE_INT doffset1 = offset1;\n-  offset_int doffset2 = offset2;\n+  poly_int64 doffset1 = offset1;\n+  poly_offset_int doffset2 = offset2;\n   if (TREE_CODE (dbase2) == MEM_REF\n       || TREE_CODE (dbase2) == TARGET_MEM_REF)\n     doffset2 -= mem_ref_offset (dbase2) << LOG2_BITS_PER_UNIT;\n@@ -1252,11 +1256,11 @@ indirect_ref_may_alias_decl_p (tree ref1 ATTRIBUTE_UNUSED, tree base1,\n \n static bool\n indirect_refs_may_alias_p (tree ref1 ATTRIBUTE_UNUSED, tree base1,\n-\t\t\t   HOST_WIDE_INT offset1, HOST_WIDE_INT max_size1,\n+\t\t\t   poly_int64 offset1, poly_int64 max_size1,\n \t\t\t   alias_set_type ref1_alias_set,\n \t\t\t   alias_set_type base1_alias_set,\n \t\t\t   tree ref2 ATTRIBUTE_UNUSED, tree base2,\n-\t\t\t   HOST_WIDE_INT offset2, HOST_WIDE_INT max_size2,\n+\t\t\t   poly_int64 offset2, poly_int64 max_size2,\n \t\t\t   alias_set_type ref2_alias_set,\n \t\t\t   alias_set_type base2_alias_set, bool tbaa_p)\n {\n@@ -1330,7 +1334,7 @@ indirect_refs_may_alias_p (tree ref1 ATTRIBUTE_UNUSED, tree base1,\n       /* But avoid treating arrays as \"objects\", instead assume they\n          can overlap by an exact multiple of their element size.  */\n       && TREE_CODE (TREE_TYPE (ptrtype1)) != ARRAY_TYPE)\n-    return ranges_overlap_p (offset1, max_size1, offset2, max_size2);\n+    return ranges_maybe_overlap_p (offset1, max_size1, offset2, max_size2);\n \n   /* Do type-based disambiguation.  */\n   if (base1_alias_set != base2_alias_set\n@@ -1365,8 +1369,8 @@ bool\n refs_may_alias_p_1 (ao_ref *ref1, ao_ref *ref2, bool tbaa_p)\n {\n   tree base1, base2;\n-  HOST_WIDE_INT offset1 = 0, offset2 = 0;\n-  HOST_WIDE_INT max_size1 = -1, max_size2 = -1;\n+  poly_int64 offset1 = 0, offset2 = 0;\n+  poly_int64 max_size1 = -1, max_size2 = -1;\n   bool var1_p, var2_p, ind1_p, ind2_p;\n \n   gcc_checking_assert ((!ref1->ref\n@@ -2442,14 +2446,17 @@ stmt_kills_ref_p (gimple *stmt, ao_ref *ref)\n          handling constant offset and size.  */\n       /* For a must-alias check we need to be able to constrain\n \t the access properly.  */\n-      if (ref->max_size == -1)\n+      if (!ref->max_size_known_p ())\n \treturn false;\n-      HOST_WIDE_INT size, offset, max_size, ref_offset = ref->offset;\n+      HOST_WIDE_INT size, max_size, const_offset;\n+      poly_int64 ref_offset = ref->offset;\n       bool reverse;\n       tree base\n-\t= get_ref_base_and_extent (lhs, &offset, &size, &max_size, &reverse);\n+\t= get_ref_base_and_extent (lhs, &const_offset, &size, &max_size,\n+\t\t\t\t   &reverse);\n       /* We can get MEM[symbol: sZ, index: D.8862_1] here,\n \t so base == ref->base does not always hold.  */\n+      poly_int64 offset = const_offset;\n       if (base != ref->base)\n \t{\n \t  /* Try using points-to info.  */\n@@ -2466,18 +2473,13 @@ stmt_kills_ref_p (gimple *stmt, ao_ref *ref)\n \t      if (!tree_int_cst_equal (TREE_OPERAND (base, 1),\n \t\t\t\t       TREE_OPERAND (ref->base, 1)))\n \t\t{\n-\t\t  offset_int off1 = mem_ref_offset (base);\n+\t\t  poly_offset_int off1 = mem_ref_offset (base);\n \t\t  off1 <<= LOG2_BITS_PER_UNIT;\n \t\t  off1 += offset;\n-\t\t  offset_int off2 = mem_ref_offset (ref->base);\n+\t\t  poly_offset_int off2 = mem_ref_offset (ref->base);\n \t\t  off2 <<= LOG2_BITS_PER_UNIT;\n \t\t  off2 += ref_offset;\n-\t\t  if (wi::fits_shwi_p (off1) && wi::fits_shwi_p (off2))\n-\t\t    {\n-\t\t      offset = off1.to_shwi ();\n-\t\t      ref_offset = off2.to_shwi ();\n-\t\t    }\n-\t\t  else\n+\t\t  if (!off1.to_shwi (&offset) || !off2.to_shwi (&ref_offset))\n \t\t    size = -1;\n \t\t}\n \t    }\n@@ -2486,12 +2488,9 @@ stmt_kills_ref_p (gimple *stmt, ao_ref *ref)\n \t}\n       /* For a must-alias check we need to be able to constrain\n \t the access properly.  */\n-      if (size != -1 && size == max_size)\n-\t{\n-\t  if (offset <= ref_offset\n-\t      && offset + size >= ref_offset + ref->max_size)\n-\t    return true;\n-\t}\n+      if (size == max_size\n+\t  && known_subrange_p (ref_offset, ref->max_size, offset, size))\n+\treturn true;\n     }\n \n   if (is_gimple_call (stmt))\n@@ -2524,19 +2523,19 @@ stmt_kills_ref_p (gimple *stmt, ao_ref *ref)\n \t    {\n \t      /* For a must-alias check we need to be able to constrain\n \t\t the access properly.  */\n-\t      if (ref->max_size == -1)\n+\t      if (!ref->max_size_known_p ())\n \t\treturn false;\n \t      tree dest = gimple_call_arg (stmt, 0);\n \t      tree len = gimple_call_arg (stmt, 2);\n-\t      if (!tree_fits_shwi_p (len))\n+\t      if (!poly_int_tree_p (len))\n \t\treturn false;\n \t      tree rbase = ref->base;\n-\t      offset_int roffset = ref->offset;\n+\t      poly_offset_int roffset = ref->offset;\n \t      ao_ref dref;\n \t      ao_ref_init_from_ptr_and_size (&dref, dest, len);\n \t      tree base = ao_ref_base (&dref);\n-\t      offset_int offset = dref.offset;\n-\t      if (!base || dref.size == -1)\n+\t      poly_offset_int offset = dref.offset;\n+\t      if (!base || !known_size_p (dref.size))\n \t\treturn false;\n \t      if (TREE_CODE (base) == MEM_REF)\n \t\t{\n@@ -2549,9 +2548,9 @@ stmt_kills_ref_p (gimple *stmt, ao_ref *ref)\n \t\t  rbase = TREE_OPERAND (rbase, 0);\n \t\t}\n \t      if (base == rbase\n-\t\t  && offset <= roffset\n-\t\t  && (roffset + ref->max_size\n-\t\t      <= offset + (wi::to_offset (len) << LOG2_BITS_PER_UNIT)))\n+\t\t  && known_subrange_p (roffset, ref->max_size, offset,\n+\t\t\t\t       wi::to_poly_offset (len)\n+\t\t\t\t       << LOG2_BITS_PER_UNIT))\n \t\treturn true;\n \t      break;\n \t    }"}, {"sha": "b6b23c91626d7d946e6001f9e4f91ca08223237e", "filename": "gcc/tree-ssa-alias.h", "status": "modified", "additions": 13, "deletions": 3, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Ftree-ssa-alias.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Ftree-ssa-alias.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-alias.h?ref=b9c257340bd20ec0e7debffc62ed3e3901c2908d", "patch": "@@ -80,11 +80,11 @@ struct ao_ref\n      the following fields are not yet computed.  */\n   tree base;\n   /* The offset relative to the base.  */\n-  HOST_WIDE_INT offset;\n+  poly_int64 offset;\n   /* The size of the access.  */\n-  HOST_WIDE_INT size;\n+  poly_int64 size;\n   /* The maximum possible extent of the access or -1 if unconstrained.  */\n-  HOST_WIDE_INT max_size;\n+  poly_int64 max_size;\n \n   /* The alias set of the access or -1 if not yet computed.  */\n   alias_set_type ref_alias_set;\n@@ -94,8 +94,18 @@ struct ao_ref\n \n   /* Whether the memory is considered a volatile access.  */\n   bool volatile_p;\n+\n+  bool max_size_known_p () const;\n };\n \n+/* Return true if the maximum size is known, rather than the special -1\n+   marker.  */\n+\n+inline bool\n+ao_ref::max_size_known_p () const\n+{\n+  return known_size_p (max_size);\n+}\n \n /* In tree-ssa-alias.c  */\n extern void ao_ref_init (ao_ref *, tree);"}, {"sha": "280356c5dc6dbdd1a6ff4b547c79a21f4f225082", "filename": "gcc/tree-ssa-dce.c", "status": "modified", "additions": 3, "deletions": 7, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Ftree-ssa-dce.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Ftree-ssa-dce.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-dce.c?ref=b9c257340bd20ec0e7debffc62ed3e3901c2908d", "patch": "@@ -488,13 +488,9 @@ mark_aliased_reaching_defs_necessary_1 (ao_ref *ref, tree vdef, void *data)\n \t{\n \t  /* For a must-alias check we need to be able to constrain\n \t     the accesses properly.  */\n-\t  if (size != -1 && size == max_size\n-\t      && ref->max_size != -1)\n-\t    {\n-\t      if (offset <= ref->offset\n-\t\t  && offset + size >= ref->offset + ref->max_size)\n-\t\treturn true;\n-\t    }\n+\t  if (size == max_size\n+\t      && known_subrange_p (ref->offset, ref->max_size, offset, size))\n+\t    return true;\n \t  /* Or they need to be exactly the same.  */\n \t  else if (ref->ref\n \t\t   /* Make sure there is no induction variable involved"}, {"sha": "392313b950b0587e4c9360aa74bcc951d7a22c30", "filename": "gcc/tree-ssa-dse.c", "status": "modified", "additions": 49, "deletions": 35, "changes": 84, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Ftree-ssa-dse.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Ftree-ssa-dse.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-dse.c?ref=b9c257340bd20ec0e7debffc62ed3e3901c2908d", "patch": "@@ -128,13 +128,12 @@ static bool\n valid_ao_ref_for_dse (ao_ref *ref)\n {\n   return (ao_ref_base (ref)\n-\t  && ref->max_size != -1\n-\t  && ref->size != 0\n-\t  && ref->max_size == ref->size\n-\t  && ref->offset >= 0\n-\t  && (ref->offset % BITS_PER_UNIT) == 0\n-\t  && (ref->size % BITS_PER_UNIT) == 0\n-\t  && (ref->size != -1));\n+\t  && known_size_p (ref->max_size)\n+\t  && maybe_ne (ref->size, 0)\n+\t  && known_eq (ref->max_size, ref->size)\n+\t  && known_ge (ref->offset, 0)\n+\t  && multiple_p (ref->offset, BITS_PER_UNIT)\n+\t  && multiple_p (ref->size, BITS_PER_UNIT));\n }\n \n /* Try to normalize COPY (an ao_ref) relative to REF.  Essentially when we are\n@@ -144,25 +143,31 @@ valid_ao_ref_for_dse (ao_ref *ref)\n static bool\n normalize_ref (ao_ref *copy, ao_ref *ref)\n {\n+  if (!ordered_p (copy->offset, ref->offset))\n+    return false;\n+\n   /* If COPY starts before REF, then reset the beginning of\n      COPY to match REF and decrease the size of COPY by the\n      number of bytes removed from COPY.  */\n-  if (copy->offset < ref->offset)\n+  if (maybe_lt (copy->offset, ref->offset))\n     {\n-      HOST_WIDE_INT diff = ref->offset - copy->offset;\n-      if (copy->size <= diff)\n+      poly_int64 diff = ref->offset - copy->offset;\n+      if (maybe_le (copy->size, diff))\n \treturn false;\n       copy->size -= diff;\n       copy->offset = ref->offset;\n     }\n \n-  HOST_WIDE_INT diff = copy->offset - ref->offset;\n-  if (ref->size <= diff)\n+  poly_int64 diff = copy->offset - ref->offset;\n+  if (maybe_le (ref->size, diff))\n     return false;\n \n   /* If COPY extends beyond REF, chop off its size appropriately.  */\n-  HOST_WIDE_INT limit = ref->size - diff;\n-  if (copy->size > limit)\n+  poly_int64 limit = ref->size - diff;\n+  if (!ordered_p (limit, copy->size))\n+    return false;\n+\n+  if (maybe_gt (copy->size, limit))\n     copy->size = limit;\n   return true;\n }\n@@ -183,15 +188,15 @@ clear_bytes_written_by (sbitmap live_bytes, gimple *stmt, ao_ref *ref)\n \n   /* Verify we have the same base memory address, the write\n      has a known size and overlaps with REF.  */\n+  HOST_WIDE_INT start, size;\n   if (valid_ao_ref_for_dse (&write)\n       && operand_equal_p (write.base, ref->base, OEP_ADDRESS_OF)\n-      && write.size == write.max_size\n-      && normalize_ref (&write, ref))\n-    {\n-      HOST_WIDE_INT start = write.offset - ref->offset;\n-      bitmap_clear_range (live_bytes, start / BITS_PER_UNIT,\n-\t\t\t  write.size / BITS_PER_UNIT);\n-    }\n+      && known_eq (write.size, write.max_size)\n+      && normalize_ref (&write, ref)\n+      && (write.offset - ref->offset).is_constant (&start)\n+      && write.size.is_constant (&size))\n+    bitmap_clear_range (live_bytes, start / BITS_PER_UNIT,\n+\t\t\tsize / BITS_PER_UNIT);\n }\n \n /* REF is a memory write.  Extract relevant information from it and\n@@ -201,12 +206,14 @@ clear_bytes_written_by (sbitmap live_bytes, gimple *stmt, ao_ref *ref)\n static bool\n setup_live_bytes_from_ref (ao_ref *ref, sbitmap live_bytes)\n {\n+  HOST_WIDE_INT const_size;\n   if (valid_ao_ref_for_dse (ref)\n-      && (ref->size / BITS_PER_UNIT\n+      && ref->size.is_constant (&const_size)\n+      && (const_size / BITS_PER_UNIT\n \t  <= PARAM_VALUE (PARAM_DSE_MAX_OBJECT_SIZE)))\n     {\n       bitmap_clear (live_bytes);\n-      bitmap_set_range (live_bytes, 0, ref->size / BITS_PER_UNIT);\n+      bitmap_set_range (live_bytes, 0, const_size / BITS_PER_UNIT);\n       return true;\n     }\n   return false;\n@@ -231,9 +238,15 @@ compute_trims (ao_ref *ref, sbitmap live, int *trim_head, int *trim_tail,\n      the REF to compute the trims.  */\n \n   /* Now identify how much, if any of the tail we can chop off.  */\n-  int last_orig = (ref->size / BITS_PER_UNIT) - 1;\n-  int last_live = bitmap_last_set_bit (live);\n-  *trim_tail = (last_orig - last_live) & ~0x1;\n+  HOST_WIDE_INT const_size;\n+  if (ref->size.is_constant (&const_size))\n+    {\n+      int last_orig = (const_size / BITS_PER_UNIT) - 1;\n+      int last_live = bitmap_last_set_bit (live);\n+      *trim_tail = (last_orig - last_live) & ~0x1;\n+    }\n+  else\n+    *trim_tail = 0;\n \n   /* Identify how much, if any of the head we can chop off.  */\n   int first_orig = 0;\n@@ -267,7 +280,7 @@ maybe_trim_complex_store (ao_ref *ref, sbitmap live, gimple *stmt)\n      least half the size of the object to ensure we're trimming\n      the entire real or imaginary half.  By writing things this\n      way we avoid more O(n) bitmap operations.  */\n-  if (trim_tail * 2 >= ref->size / BITS_PER_UNIT)\n+  if (known_ge (trim_tail * 2 * BITS_PER_UNIT, ref->size))\n     {\n       /* TREE_REALPART is live */\n       tree x = TREE_REALPART (gimple_assign_rhs1 (stmt));\n@@ -276,7 +289,7 @@ maybe_trim_complex_store (ao_ref *ref, sbitmap live, gimple *stmt)\n       gimple_assign_set_lhs (stmt, y);\n       gimple_assign_set_rhs1 (stmt, x);\n     }\n-  else if (trim_head * 2 >= ref->size / BITS_PER_UNIT)\n+  else if (known_ge (trim_head * 2 * BITS_PER_UNIT, ref->size))\n     {\n       /* TREE_IMAGPART is live */\n       tree x = TREE_IMAGPART (gimple_assign_rhs1 (stmt));\n@@ -326,7 +339,8 @@ maybe_trim_constructor_store (ao_ref *ref, sbitmap live, gimple *stmt)\n \treturn;\n \n       /* The number of bytes for the new constructor.  */\n-      int count = (ref->size / BITS_PER_UNIT) - head_trim - tail_trim;\n+      poly_int64 ref_bytes = exact_div (ref->size, BITS_PER_UNIT);\n+      poly_int64 count = ref_bytes - head_trim - tail_trim;\n \n       /* And the new type for the CONSTRUCTOR.  Essentially it's just\n \t a char array large enough to cover the non-trimmed parts of\n@@ -483,15 +497,15 @@ live_bytes_read (ao_ref use_ref, ao_ref *ref, sbitmap live)\n {\n   /* We have already verified that USE_REF and REF hit the same object.\n      Now verify that there's actually an overlap between USE_REF and REF.  */\n-  if (normalize_ref (&use_ref, ref))\n+  HOST_WIDE_INT start, size;\n+  if (normalize_ref (&use_ref, ref)\n+      && (use_ref.offset - ref->offset).is_constant (&start)\n+      && use_ref.size.is_constant (&size))\n     {\n-      HOST_WIDE_INT start = use_ref.offset - ref->offset;\n-      HOST_WIDE_INT size = use_ref.size;\n-\n       /* If USE_REF covers all of REF, then it will hit one or more\n \t live bytes.   This avoids useless iteration over the bitmap\n \t below.  */\n-      if (start == 0 && size == ref->size)\n+      if (start == 0 && known_eq (size, ref->size))\n \treturn true;\n \n       /* Now check if any of the remaining bits in use_ref are set in LIVE.  */\n@@ -593,7 +607,7 @@ dse_classify_store (ao_ref *ref, gimple *stmt, gimple **use_stmt,\n \t\t      ao_ref_init (&use_ref, gimple_assign_rhs1 (use_stmt));\n \t\t      if (valid_ao_ref_for_dse (&use_ref)\n \t\t\t  && use_ref.base == ref->base\n-\t\t\t  && use_ref.size == use_ref.max_size\n+\t\t\t  && known_eq (use_ref.size, use_ref.max_size)\n \t\t\t  && !live_bytes_read (use_ref, ref, live_bytes))\n \t\t\t{\n \t\t\t  /* If this statement has a VDEF, then it is the"}, {"sha": "e3dbebd56761e6b722b758fbf9c4538c87475e90", "filename": "gcc/tree-ssa-sccvn.c", "status": "modified", "additions": 119, "deletions": 128, "changes": 247, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Ftree-ssa-sccvn.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Ftree-ssa-sccvn.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-sccvn.c?ref=b9c257340bd20ec0e7debffc62ed3e3901c2908d", "patch": "@@ -560,7 +560,7 @@ vn_reference_compute_hash (const vn_reference_t vr1)\n   hashval_t result;\n   int i;\n   vn_reference_op_t vro;\n-  HOST_WIDE_INT off = -1;\n+  poly_int64 off = -1;\n   bool deref = false;\n \n   FOR_EACH_VEC_ELT (vr1->operands, i, vro)\n@@ -569,17 +569,17 @@ vn_reference_compute_hash (const vn_reference_t vr1)\n \tderef = true;\n       else if (vro->opcode != ADDR_EXPR)\n \tderef = false;\n-      if (vro->off != -1)\n+      if (maybe_ne (vro->off, -1))\n \t{\n-\t  if (off == -1)\n+\t  if (known_eq (off, -1))\n \t    off = 0;\n \t  off += vro->off;\n \t}\n       else\n \t{\n-\t  if (off != -1\n-\t      && off != 0)\n-\t    hstate.add_int (off);\n+\t  if (maybe_ne (off, -1)\n+\t      && maybe_ne (off, 0))\n+\t    hstate.add_poly_int (off);\n \t  off = -1;\n \t  if (deref\n \t      && vro->opcode == ADDR_EXPR)\n@@ -645,7 +645,7 @@ vn_reference_eq (const_vn_reference_t const vr1, const_vn_reference_t const vr2)\n   j = 0;\n   do\n     {\n-      HOST_WIDE_INT off1 = 0, off2 = 0;\n+      poly_int64 off1 = 0, off2 = 0;\n       vn_reference_op_t vro1, vro2;\n       vn_reference_op_s tem1, tem2;\n       bool deref1 = false, deref2 = false;\n@@ -656,7 +656,7 @@ vn_reference_eq (const_vn_reference_t const vr1, const_vn_reference_t const vr2)\n \t  /* Do not look through a storage order barrier.  */\n \t  else if (vro1->opcode == VIEW_CONVERT_EXPR && vro1->reverse)\n \t    return false;\n-\t  if (vro1->off == -1)\n+\t  if (known_eq (vro1->off, -1))\n \t    break;\n \t  off1 += vro1->off;\n \t}\n@@ -667,11 +667,11 @@ vn_reference_eq (const_vn_reference_t const vr1, const_vn_reference_t const vr2)\n \t  /* Do not look through a storage order barrier.  */\n \t  else if (vro2->opcode == VIEW_CONVERT_EXPR && vro2->reverse)\n \t    return false;\n-\t  if (vro2->off == -1)\n+\t  if (known_eq (vro2->off, -1))\n \t    break;\n \t  off2 += vro2->off;\n \t}\n-      if (off1 != off2)\n+      if (maybe_ne (off1, off2))\n \treturn false;\n       if (deref1 && vro1->opcode == ADDR_EXPR)\n \t{\n@@ -797,24 +797,23 @@ copy_reference_ops_from_ref (tree ref, vec<vn_reference_op_s> *result)\n \t  {\n \t    tree this_offset = component_ref_field_offset (ref);\n \t    if (this_offset\n-\t\t&& TREE_CODE (this_offset) == INTEGER_CST)\n+\t\t&& poly_int_tree_p (this_offset))\n \t      {\n \t\ttree bit_offset = DECL_FIELD_BIT_OFFSET (TREE_OPERAND (ref, 1));\n \t\tif (TREE_INT_CST_LOW (bit_offset) % BITS_PER_UNIT == 0)\n \t\t  {\n-\t\t    offset_int off\n-\t\t      = (wi::to_offset (this_offset)\n+\t\t    poly_offset_int off\n+\t\t      = (wi::to_poly_offset (this_offset)\n \t\t\t + (wi::to_offset (bit_offset) >> LOG2_BITS_PER_UNIT));\n-\t\t    if (wi::fits_shwi_p (off)\n-\t\t\t/* Probibit value-numbering zero offset components\n-\t\t\t   of addresses the same before the pass folding\n-\t\t\t   __builtin_object_size had a chance to run\n-\t\t\t   (checking cfun->after_inlining does the\n-\t\t\t   trick here).  */\n-\t\t\t&& (TREE_CODE (orig) != ADDR_EXPR\n-\t\t\t    || off != 0\n-\t\t\t    || cfun->after_inlining))\n-\t\t      temp.off = off.to_shwi ();\n+\t\t    /* Probibit value-numbering zero offset components\n+\t\t       of addresses the same before the pass folding\n+\t\t       __builtin_object_size had a chance to run\n+\t\t       (checking cfun->after_inlining does the\n+\t\t       trick here).  */\n+\t\t    if (TREE_CODE (orig) != ADDR_EXPR\n+\t\t\t|| maybe_ne (off, 0)\n+\t\t\t|| cfun->after_inlining)\n+\t\t      off.to_shwi (&temp.off);\n \t\t  }\n \t      }\n \t  }\n@@ -833,16 +832,15 @@ copy_reference_ops_from_ref (tree ref, vec<vn_reference_op_s> *result)\n \t    if (! temp.op2)\n \t      temp.op2 = size_binop (EXACT_DIV_EXPR, TYPE_SIZE_UNIT (eltype),\n \t\t\t\t     size_int (TYPE_ALIGN_UNIT (eltype)));\n-\t    if (TREE_CODE (temp.op0) == INTEGER_CST\n-\t\t&& TREE_CODE (temp.op1) == INTEGER_CST\n+\t    if (poly_int_tree_p (temp.op0)\n+\t\t&& poly_int_tree_p (temp.op1)\n \t\t&& TREE_CODE (temp.op2) == INTEGER_CST)\n \t      {\n-\t\toffset_int off = ((wi::to_offset (temp.op0)\n-\t\t\t\t   - wi::to_offset (temp.op1))\n-\t\t\t\t  * wi::to_offset (temp.op2)\n-\t\t\t\t  * vn_ref_op_align_unit (&temp));\n-\t\tif (wi::fits_shwi_p (off))\n-\t\t  temp.off = off.to_shwi();\n+\t\tpoly_offset_int off = ((wi::to_poly_offset (temp.op0)\n+\t\t\t\t\t- wi::to_poly_offset (temp.op1))\n+\t\t\t\t       * wi::to_offset (temp.op2)\n+\t\t\t\t       * vn_ref_op_align_unit (&temp));\n+\t\toff.to_shwi (&temp.off);\n \t      }\n \t  }\n \t  break;\n@@ -929,9 +927,9 @@ ao_ref_init_from_vn_reference (ao_ref *ref,\n   unsigned i;\n   tree base = NULL_TREE;\n   tree *op0_p = &base;\n-  offset_int offset = 0;\n-  offset_int max_size;\n-  offset_int size = -1;\n+  poly_offset_int offset = 0;\n+  poly_offset_int max_size;\n+  poly_offset_int size = -1;\n   tree size_tree = NULL_TREE;\n   alias_set_type base_alias_set = -1;\n \n@@ -947,11 +945,11 @@ ao_ref_init_from_vn_reference (ao_ref *ref,\n       if (mode == BLKmode)\n \tsize_tree = TYPE_SIZE (type);\n       else\n-\tsize = int (GET_MODE_BITSIZE (mode));\n+\tsize = GET_MODE_BITSIZE (mode);\n     }\n   if (size_tree != NULL_TREE\n-      && TREE_CODE (size_tree) == INTEGER_CST)\n-    size = wi::to_offset (size_tree);\n+      && poly_int_tree_p (size_tree))\n+    size = wi::to_poly_offset (size_tree);\n \n   /* Initially, maxsize is the same as the accessed element size.\n      In the following it will only grow (or become -1).  */\n@@ -974,7 +972,7 @@ ao_ref_init_from_vn_reference (ao_ref *ref,\n \t    {\n \t      vn_reference_op_t pop = &ops[i-1];\n \t      base = TREE_OPERAND (op->op0, 0);\n-\t      if (pop->off == -1)\n+\t      if (known_eq (pop->off, -1))\n \t\t{\n \t\t  max_size = -1;\n \t\t  offset = 0;\n@@ -1019,12 +1017,12 @@ ao_ref_init_from_vn_reference (ao_ref *ref,\n \t       parts manually.  */\n \t    tree this_offset = DECL_FIELD_OFFSET (field);\n \n-\t    if (op->op1 || TREE_CODE (this_offset) != INTEGER_CST)\n+\t    if (op->op1 || !poly_int_tree_p (this_offset))\n \t      max_size = -1;\n \t    else\n \t      {\n-\t\toffset_int woffset = (wi::to_offset (this_offset)\n-\t\t\t\t      << LOG2_BITS_PER_UNIT);\n+\t\tpoly_offset_int woffset = (wi::to_poly_offset (this_offset)\n+\t\t\t\t\t   << LOG2_BITS_PER_UNIT);\n \t\twoffset += wi::to_offset (DECL_FIELD_BIT_OFFSET (field));\n \t\toffset += woffset;\n \t      }\n@@ -1034,14 +1032,15 @@ ao_ref_init_from_vn_reference (ao_ref *ref,\n \tcase ARRAY_RANGE_REF:\n \tcase ARRAY_REF:\n \t  /* We recorded the lower bound and the element size.  */\n-\t  if (TREE_CODE (op->op0) != INTEGER_CST\n-\t      || TREE_CODE (op->op1) != INTEGER_CST\n+\t  if (!poly_int_tree_p (op->op0)\n+\t      || !poly_int_tree_p (op->op1)\n \t      || TREE_CODE (op->op2) != INTEGER_CST)\n \t    max_size = -1;\n \t  else\n \t    {\n-\t      offset_int woffset\n-\t\t= wi::sext (wi::to_offset (op->op0) - wi::to_offset (op->op1),\n+\t      poly_offset_int woffset\n+\t\t= wi::sext (wi::to_poly_offset (op->op0)\n+\t\t\t    - wi::to_poly_offset (op->op1),\n \t\t\t    TYPE_PRECISION (TREE_TYPE (op->op0)));\n \t      woffset *= wi::to_offset (op->op2) * vn_ref_op_align_unit (op);\n \t      woffset <<= LOG2_BITS_PER_UNIT;\n@@ -1086,29 +1085,23 @@ ao_ref_init_from_vn_reference (ao_ref *ref,\n   /* We discount volatiles from value-numbering elsewhere.  */\n   ref->volatile_p = false;\n \n-  if (!wi::fits_shwi_p (size) || wi::neg_p (size))\n+  if (!size.to_shwi (&ref->size) || maybe_lt (ref->size, 0))\n     {\n       ref->offset = 0;\n       ref->size = -1;\n       ref->max_size = -1;\n       return true;\n     }\n \n-  ref->size = size.to_shwi ();\n-\n-  if (!wi::fits_shwi_p (offset))\n+  if (!offset.to_shwi (&ref->offset))\n     {\n       ref->offset = 0;\n       ref->max_size = -1;\n       return true;\n     }\n \n-  ref->offset = offset.to_shwi ();\n-\n-  if (!wi::fits_shwi_p (max_size) || wi::neg_p (max_size))\n+  if (!max_size.to_shwi (&ref->max_size) || maybe_lt (ref->max_size, 0))\n     ref->max_size = -1;\n-  else\n-    ref->max_size = max_size.to_shwi ();\n \n   return true;\n }\n@@ -1353,7 +1346,7 @@ fully_constant_vn_reference_p (vn_reference_t ref)\n \t   && (!INTEGRAL_TYPE_P (ref->type)\n \t       || TYPE_PRECISION (ref->type) % BITS_PER_UNIT == 0))\n     {\n-      HOST_WIDE_INT off = 0;\n+      poly_int64 off = 0;\n       HOST_WIDE_INT size;\n       if (INTEGRAL_TYPE_P (ref->type))\n \tsize = TYPE_PRECISION (ref->type);\n@@ -1371,7 +1364,7 @@ fully_constant_vn_reference_p (vn_reference_t ref)\n \t      ++i;\n \t      break;\n \t    }\n-\t  if (operands[i].off == -1)\n+\t  if (known_eq (operands[i].off, -1))\n \t    return NULL_TREE;\n \t  off += operands[i].off;\n \t  if (operands[i].opcode == MEM_REF)\n@@ -1401,6 +1394,7 @@ fully_constant_vn_reference_p (vn_reference_t ref)\n \treturn build_zero_cst (ref->type);\n       else if (ctor != error_mark_node)\n \t{\n+\t  HOST_WIDE_INT const_off;\n \t  if (decl)\n \t    {\n \t      tree res = fold_ctor_reference (ref->type, ctor,\n@@ -1413,10 +1407,10 @@ fully_constant_vn_reference_p (vn_reference_t ref)\n \t\t    return res;\n \t\t}\n \t    }\n-\t  else\n+\t  else if (off.is_constant (&const_off))\n \t    {\n \t      unsigned char buf[MAX_BITSIZE_MODE_ANY_MODE / BITS_PER_UNIT];\n-\t      int len = native_encode_expr (ctor, buf, size, off);\n+\t      int len = native_encode_expr (ctor, buf, size, const_off);\n \t      if (len > 0)\n \t\treturn native_interpret_expr (ref->type, buf, len);\n \t    }\n@@ -1508,17 +1502,16 @@ valueize_refs_1 (vec<vn_reference_op_s> orig, bool *valueized_anything)\n       /* If it transforms a non-constant ARRAY_REF into a constant\n \t one, adjust the constant offset.  */\n       else if (vro->opcode == ARRAY_REF\n-\t       && vro->off == -1\n-\t       && TREE_CODE (vro->op0) == INTEGER_CST\n-\t       && TREE_CODE (vro->op1) == INTEGER_CST\n+\t       && known_eq (vro->off, -1)\n+\t       && poly_int_tree_p (vro->op0)\n+\t       && poly_int_tree_p (vro->op1)\n \t       && TREE_CODE (vro->op2) == INTEGER_CST)\n \t{\n-\t  offset_int off = ((wi::to_offset (vro->op0)\n-\t\t\t     - wi::to_offset (vro->op1))\n-\t\t\t    * wi::to_offset (vro->op2)\n-\t\t\t    * vn_ref_op_align_unit (vro));\n-\t  if (wi::fits_shwi_p (off))\n-\t    vro->off = off.to_shwi ();\n+\t  poly_offset_int off = ((wi::to_poly_offset (vro->op0)\n+\t\t\t\t  - wi::to_poly_offset (vro->op1))\n+\t\t\t\t * wi::to_offset (vro->op2)\n+\t\t\t\t * vn_ref_op_align_unit (vro));\n+\t  off.to_shwi (&vro->off);\n \t}\n     }\n \n@@ -1834,10 +1827,11 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n   vn_reference_t vr = (vn_reference_t)vr_;\n   gimple *def_stmt = SSA_NAME_DEF_STMT (vuse);\n   tree base = ao_ref_base (ref);\n-  HOST_WIDE_INT offset, maxsize;\n+  HOST_WIDE_INT offseti, maxsizei;\n   static vec<vn_reference_op_s> lhs_ops;\n   ao_ref lhs_ref;\n   bool lhs_ref_ok = false;\n+  poly_int64 copy_size;\n \n   /* If the reference is based on a parameter that was determined as\n      pointing to readonly memory it doesn't change.  */\n@@ -1949,14 +1943,14 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n   if (*disambiguate_only)\n     return (void *)-1;\n \n-  offset = ref->offset;\n-  maxsize = ref->max_size;\n-\n   /* If we cannot constrain the size of the reference we cannot\n      test if anything kills it.  */\n-  if (maxsize == -1)\n+  if (!ref->max_size_known_p ())\n     return (void *)-1;\n \n+  poly_int64 offset = ref->offset;\n+  poly_int64 maxsize = ref->max_size;\n+\n   /* We can't deduce anything useful from clobbers.  */\n   if (gimple_clobber_p (def_stmt))\n     return (void *)-1;\n@@ -1967,7 +1961,7 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n   if (is_gimple_reg_type (vr->type)\n       && gimple_call_builtin_p (def_stmt, BUILT_IN_MEMSET)\n       && integer_zerop (gimple_call_arg (def_stmt, 1))\n-      && tree_fits_uhwi_p (gimple_call_arg (def_stmt, 2))\n+      && poly_int_tree_p (gimple_call_arg (def_stmt, 2))\n       && TREE_CODE (gimple_call_arg (def_stmt, 0)) == ADDR_EXPR)\n     {\n       tree ref2 = TREE_OPERAND (gimple_call_arg (def_stmt, 0), 0);\n@@ -1976,13 +1970,11 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n       bool reverse;\n       base2 = get_ref_base_and_extent (ref2, &offset2, &size2, &maxsize2,\n \t\t\t\t       &reverse);\n-      size2 = tree_to_uhwi (gimple_call_arg (def_stmt, 2)) * 8;\n-      if ((unsigned HOST_WIDE_INT)size2 / 8\n-\t  == tree_to_uhwi (gimple_call_arg (def_stmt, 2))\n-\t  && maxsize2 != -1\n+      tree len = gimple_call_arg (def_stmt, 2);\n+      if (known_size_p (maxsize2)\n \t  && operand_equal_p (base, base2, 0)\n-\t  && offset2 <= offset\n-\t  && offset2 + size2 >= offset + maxsize)\n+\t  && known_subrange_p (offset, maxsize, offset2,\n+\t\t\t       wi::to_poly_offset (len) << LOG2_BITS_PER_UNIT))\n \t{\n \t  tree val = build_zero_cst (vr->type);\n \t  return vn_reference_lookup_or_insert_for_pieces\n@@ -2001,10 +1993,9 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n       bool reverse;\n       base2 = get_ref_base_and_extent (gimple_assign_lhs (def_stmt),\n \t\t\t\t       &offset2, &size2, &maxsize2, &reverse);\n-      if (maxsize2 != -1\n+      if (known_size_p (maxsize2)\n \t  && operand_equal_p (base, base2, 0)\n-\t  && offset2 <= offset\n-\t  && offset2 + size2 >= offset + maxsize)\n+\t  && known_subrange_p (offset, maxsize, offset2, size2))\n \t{\n \t  tree val = build_zero_cst (vr->type);\n \t  return vn_reference_lookup_or_insert_for_pieces\n@@ -2014,13 +2005,17 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n \n   /* 3) Assignment from a constant.  We can use folds native encode/interpret\n      routines to extract the assigned bits.  */\n-  else if (ref->size == maxsize\n+  else if (known_eq (ref->size, maxsize)\n \t   && is_gimple_reg_type (vr->type)\n \t   && !contains_storage_order_barrier_p (vr->operands)\n \t   && gimple_assign_single_p (def_stmt)\n \t   && CHAR_BIT == 8 && BITS_PER_UNIT == 8\n-\t   && maxsize % BITS_PER_UNIT == 0\n-\t   && offset % BITS_PER_UNIT == 0\n+\t   /* native_encode and native_decode operate on arrays of bytes\n+\t      and so fundamentally need a compile-time size and offset.  */\n+\t   && maxsize.is_constant (&maxsizei)\n+\t   && maxsizei % BITS_PER_UNIT == 0\n+\t   && offset.is_constant (&offseti)\n+\t   && offseti % BITS_PER_UNIT == 0\n \t   && (is_gimple_min_invariant (gimple_assign_rhs1 (def_stmt))\n \t       || (TREE_CODE (gimple_assign_rhs1 (def_stmt)) == SSA_NAME\n \t\t   && is_gimple_min_invariant (SSA_VAL (gimple_assign_rhs1 (def_stmt))))))\n@@ -2036,8 +2031,7 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n \t  && size2 % BITS_PER_UNIT == 0\n \t  && offset2 % BITS_PER_UNIT == 0\n \t  && operand_equal_p (base, base2, 0)\n-\t  && offset2 <= offset\n-\t  && offset2 + size2 >= offset + maxsize)\n+\t  && known_subrange_p (offseti, maxsizei, offset2, size2))\n \t{\n \t  /* We support up to 512-bit values (for V8DFmode).  */\n \t  unsigned char buffer[64];\n@@ -2054,14 +2048,14 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n \t      /* Make sure to interpret in a type that has a range\n \t         covering the whole access size.  */\n \t      if (INTEGRAL_TYPE_P (vr->type)\n-\t\t  && ref->size != TYPE_PRECISION (vr->type))\n-\t\ttype = build_nonstandard_integer_type (ref->size,\n+\t\t  && maxsizei != TYPE_PRECISION (vr->type))\n+\t\ttype = build_nonstandard_integer_type (maxsizei,\n \t\t\t\t\t\t       TYPE_UNSIGNED (type));\n \t      tree val = native_interpret_expr (type,\n \t\t\t\t\t\tbuffer\n-\t\t\t\t\t\t+ ((offset - offset2)\n+\t\t\t\t\t\t+ ((offseti - offset2)\n \t\t\t\t\t\t   / BITS_PER_UNIT),\n-\t\t\t\t\t\tref->size / BITS_PER_UNIT);\n+\t\t\t\t\t\tmaxsizei / BITS_PER_UNIT);\n \t      /* If we chop off bits because the types precision doesn't\n \t\t match the memory access size this is ok when optimizing\n \t\t reads but not when called from the DSE code during\n@@ -2084,7 +2078,7 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n \n   /* 4) Assignment from an SSA name which definition we may be able\n      to access pieces from.  */\n-  else if (ref->size == maxsize\n+  else if (known_eq (ref->size, maxsize)\n \t   && is_gimple_reg_type (vr->type)\n \t   && !contains_storage_order_barrier_p (vr->operands)\n \t   && gimple_assign_single_p (def_stmt)\n@@ -2100,15 +2094,14 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n \t  && maxsize2 != -1\n \t  && maxsize2 == size2\n \t  && operand_equal_p (base, base2, 0)\n-\t  && offset2 <= offset\n-\t  && offset2 + size2 >= offset + maxsize\n+\t  && known_subrange_p (offset, maxsize, offset2, size2)\n \t  /* ???  We can't handle bitfield precision extracts without\n \t     either using an alternate type for the BIT_FIELD_REF and\n \t     then doing a conversion or possibly adjusting the offset\n \t     according to endianness.  */\n \t  && (! INTEGRAL_TYPE_P (vr->type)\n-\t      || ref->size == TYPE_PRECISION (vr->type))\n-\t  && ref->size % BITS_PER_UNIT == 0)\n+\t      || known_eq (ref->size, TYPE_PRECISION (vr->type)))\n+\t  && multiple_p (ref->size, BITS_PER_UNIT))\n \t{\n \t  code_helper rcode = BIT_FIELD_REF;\n \t  tree ops[3];\n@@ -2136,7 +2129,6 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n \t       || handled_component_p (gimple_assign_rhs1 (def_stmt))))\n     {\n       tree base2;\n-      HOST_WIDE_INT maxsize2;\n       int i, j, k;\n       auto_vec<vn_reference_op_s> rhs;\n       vn_reference_op_t vro;\n@@ -2147,8 +2139,7 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n \n       /* See if the assignment kills REF.  */\n       base2 = ao_ref_base (&lhs_ref);\n-      maxsize2 = lhs_ref.max_size;\n-      if (maxsize2 == -1\n+      if (!lhs_ref.max_size_known_p ()\n \t  || (base != base2\n \t      && (TREE_CODE (base) != MEM_REF\n \t\t  || TREE_CODE (base2) != MEM_REF\n@@ -2175,15 +2166,15 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n \t may fail when comparing types for compatibility.  But we really\n \t don't care here - further lookups with the rewritten operands\n \t will simply fail if we messed up types too badly.  */\n-      HOST_WIDE_INT extra_off = 0;\n+      poly_int64 extra_off = 0;\n       if (j == 0 && i >= 0\n \t  && lhs_ops[0].opcode == MEM_REF\n-\t  && lhs_ops[0].off != -1)\n+\t  && maybe_ne (lhs_ops[0].off, -1))\n \t{\n-\t  if (lhs_ops[0].off == vr->operands[i].off)\n+\t  if (known_eq (lhs_ops[0].off, vr->operands[i].off))\n \t    i--, j--;\n \t  else if (vr->operands[i].opcode == MEM_REF\n-\t\t   && vr->operands[i].off != -1)\n+\t\t   && maybe_ne (vr->operands[i].off, -1))\n \t    {\n \t      extra_off = vr->operands[i].off - lhs_ops[0].off;\n \t      i--, j--;\n@@ -2209,11 +2200,11 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n       copy_reference_ops_from_ref (gimple_assign_rhs1 (def_stmt), &rhs);\n \n       /* Apply an extra offset to the inner MEM_REF of the RHS.  */\n-      if (extra_off != 0)\n+      if (maybe_ne (extra_off, 0))\n \t{\n \t  if (rhs.length () < 2\n \t      || rhs[0].opcode != MEM_REF\n-\t      || rhs[0].off == -1)\n+\t      || known_eq (rhs[0].off, -1))\n \t    return (void *)-1;\n \t  rhs[0].off += extra_off;\n \t  rhs[0].op0 = int_const_binop (PLUS_EXPR, rhs[0].op0,\n@@ -2244,7 +2235,7 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n       if (!ao_ref_init_from_vn_reference (&r, vr->set, vr->type, vr->operands))\n \treturn (void *)-1;\n       /* This can happen with bitfields.  */\n-      if (ref->size != r.size)\n+      if (maybe_ne (ref->size, r.size))\n \treturn (void *)-1;\n       *ref = r;\n \n@@ -2267,18 +2258,19 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n \t       || TREE_CODE (gimple_call_arg (def_stmt, 0)) == SSA_NAME)\n \t   && (TREE_CODE (gimple_call_arg (def_stmt, 1)) == ADDR_EXPR\n \t       || TREE_CODE (gimple_call_arg (def_stmt, 1)) == SSA_NAME)\n-\t   && tree_fits_uhwi_p (gimple_call_arg (def_stmt, 2)))\n+\t   && poly_int_tree_p (gimple_call_arg (def_stmt, 2), &copy_size))\n     {\n       tree lhs, rhs;\n       ao_ref r;\n-      HOST_WIDE_INT rhs_offset, copy_size, lhs_offset;\n+      poly_int64 rhs_offset, lhs_offset;\n       vn_reference_op_s op;\n-      HOST_WIDE_INT at;\n+      poly_uint64 mem_offset;\n+      poly_int64 at, byte_maxsize;\n \n       /* Only handle non-variable, addressable refs.  */\n-      if (ref->size != maxsize\n-\t  || offset % BITS_PER_UNIT != 0\n-\t  || ref->size % BITS_PER_UNIT != 0)\n+      if (maybe_ne (ref->size, maxsize)\n+\t  || !multiple_p (offset, BITS_PER_UNIT, &at)\n+\t  || !multiple_p (maxsize, BITS_PER_UNIT, &byte_maxsize))\n \treturn (void *)-1;\n \n       /* Extract a pointer base and an offset for the destination.  */\n@@ -2297,17 +2289,19 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n \t}\n       if (TREE_CODE (lhs) == ADDR_EXPR)\n \t{\n+\t  HOST_WIDE_INT tmp_lhs_offset;\n \t  tree tem = get_addr_base_and_unit_offset (TREE_OPERAND (lhs, 0),\n-\t\t\t\t\t\t    &lhs_offset);\n+\t\t\t\t\t\t    &tmp_lhs_offset);\n+\t  lhs_offset = tmp_lhs_offset;\n \t  if (!tem)\n \t    return (void *)-1;\n \t  if (TREE_CODE (tem) == MEM_REF\n-\t      && tree_fits_uhwi_p (TREE_OPERAND (tem, 1)))\n+\t      && poly_int_tree_p (TREE_OPERAND (tem, 1), &mem_offset))\n \t    {\n \t      lhs = TREE_OPERAND (tem, 0);\n \t      if (TREE_CODE (lhs) == SSA_NAME)\n \t\tlhs = SSA_VAL (lhs);\n-\t      lhs_offset += tree_to_uhwi (TREE_OPERAND (tem, 1));\n+\t      lhs_offset += mem_offset;\n \t    }\n \t  else if (DECL_P (tem))\n \t    lhs = build_fold_addr_expr (tem);\n@@ -2325,15 +2319,17 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n \trhs = SSA_VAL (rhs);\n       if (TREE_CODE (rhs) == ADDR_EXPR)\n \t{\n+\t  HOST_WIDE_INT tmp_rhs_offset;\n \t  tree tem = get_addr_base_and_unit_offset (TREE_OPERAND (rhs, 0),\n-\t\t\t\t\t\t    &rhs_offset);\n+\t\t\t\t\t\t    &tmp_rhs_offset);\n+\t  rhs_offset = tmp_rhs_offset;\n \t  if (!tem)\n \t    return (void *)-1;\n \t  if (TREE_CODE (tem) == MEM_REF\n-\t      && tree_fits_uhwi_p (TREE_OPERAND (tem, 1)))\n+\t      && poly_int_tree_p (TREE_OPERAND (tem, 1), &mem_offset))\n \t    {\n \t      rhs = TREE_OPERAND (tem, 0);\n-\t      rhs_offset += tree_to_uhwi (TREE_OPERAND (tem, 1));\n+\t      rhs_offset += mem_offset;\n \t    }\n \t  else if (DECL_P (tem)\n \t\t   || TREE_CODE (tem) == STRING_CST)\n@@ -2345,16 +2341,13 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n \t  && TREE_CODE (rhs) != ADDR_EXPR)\n \treturn (void *)-1;\n \n-      copy_size = tree_to_uhwi (gimple_call_arg (def_stmt, 2));\n-\n       /* The bases of the destination and the references have to agree.  */\n-      at = offset / BITS_PER_UNIT;\n       if (TREE_CODE (base) == MEM_REF)\n \t{\n \t  if (TREE_OPERAND (base, 0) != lhs\n-\t      || !tree_fits_uhwi_p (TREE_OPERAND (base, 1)))\n+\t      || !poly_int_tree_p (TREE_OPERAND (base, 1), &mem_offset))\n \t    return (void *) -1;\n-\t  at += tree_to_uhwi (TREE_OPERAND (base, 1));\n+\t  at += mem_offset;\n \t}\n       else if (!DECL_P (base)\n \t       || TREE_CODE (lhs) != ADDR_EXPR\n@@ -2363,12 +2356,10 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n \n       /* If the access is completely outside of the memcpy destination\n \t area there is no aliasing.  */\n-      if (lhs_offset >= at + maxsize / BITS_PER_UNIT\n-\t  || lhs_offset + copy_size <= at)\n+      if (!ranges_maybe_overlap_p (lhs_offset, copy_size, at, byte_maxsize))\n \treturn NULL;\n       /* And the access has to be contained within the memcpy destination.  */\n-      if (lhs_offset > at\n-\t  || lhs_offset + copy_size < at + maxsize / BITS_PER_UNIT)\n+      if (!known_subrange_p (at, byte_maxsize, lhs_offset, copy_size))\n \treturn (void *)-1;\n \n       /* Make room for 2 operands in the new reference.  */\n@@ -2406,7 +2397,7 @@ vn_reference_lookup_3 (ao_ref *ref, tree vuse, void *vr_,\n       if (!ao_ref_init_from_vn_reference (&r, vr->set, vr->type, vr->operands))\n \treturn (void *)-1;\n       /* This can happen with bitfields.  */\n-      if (ref->size != r.size)\n+      if (maybe_ne (ref->size, r.size))\n \treturn (void *)-1;\n       *ref = r;\n "}, {"sha": "830876849bf20899a0531ba003464d2e130464c7", "filename": "gcc/tree-ssa-sccvn.h", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Ftree-ssa-sccvn.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Ftree-ssa-sccvn.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-sccvn.h?ref=b9c257340bd20ec0e7debffc62ed3e3901c2908d", "patch": "@@ -93,7 +93,7 @@ typedef struct vn_reference_op_struct\n   /* For storing TYPE_ALIGN for array ref element size computation.  */\n   unsigned align : 6;\n   /* Constant offset this op adds or -1 if it is variable.  */\n-  HOST_WIDE_INT off;\n+  poly_int64_pod off;\n   tree type;\n   tree op0;\n   tree op1;"}, {"sha": "b92175115a7e0dd4a93c95e44a101d2432b55227", "filename": "gcc/tree-ssa-uninit.c", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Ftree-ssa-uninit.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/b9c257340bd20ec0e7debffc62ed3e3901c2908d/gcc%2Ftree-ssa-uninit.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-uninit.c?ref=b9c257340bd20ec0e7debffc62ed3e3901c2908d", "patch": "@@ -294,15 +294,15 @@ warn_uninitialized_vars (bool warn_possibly_uninitialized)\n \n \t      /* Do not warn if the access is fully outside of the\n \t         variable.  */\n+\t      poly_int64 decl_size;\n \t      if (DECL_P (base)\n-\t\t  && ref.size != -1\n-\t\t  && ((ref.max_size == ref.size\n-\t\t       && ref.offset + ref.size <= 0)\n-\t\t      || (ref.offset >= 0\n+\t\t  && known_size_p (ref.size)\n+\t\t  && ((known_eq (ref.max_size, ref.size)\n+\t\t       && known_le (ref.offset + ref.size, 0))\n+\t\t      || (known_ge (ref.offset, 0)\n \t\t\t  && DECL_SIZE (base)\n-\t\t\t  && TREE_CODE (DECL_SIZE (base)) == INTEGER_CST\n-\t\t\t  && compare_tree_int (DECL_SIZE (base),\n-\t\t\t\t\t       ref.offset) <= 0)))\n+\t\t\t  && poly_int_tree_p (DECL_SIZE (base), &decl_size)\n+\t\t\t  && known_le (decl_size, ref.offset))))\n \t\tcontinue;\n \n \t      /* Do not warn if the access is then used for a BIT_INSERT_EXPR. */"}]}