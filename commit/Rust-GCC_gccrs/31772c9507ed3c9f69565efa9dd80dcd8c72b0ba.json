{"sha": "31772c9507ed3c9f69565efa9dd80dcd8c72b0ba", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MzE3NzJjOTUwN2VkM2M5ZjY5NTY1ZWZhOWRkODBkY2Q4YzcyYjBiYQ==", "commit": {"author": {"name": "Torvald Riegel", "email": "triegel@redhat.com", "date": "2012-02-14T13:14:27Z"}, "committer": {"name": "Torvald Riegel", "email": "torvald@gcc.gnu.org", "date": "2012-02-14T13:14:27Z"}, "message": "libitm: Add multi-lock, write-through TM method.\n\n\tlibitm/\n\t* libitm_i.h (GTM::gtm_rwlog_entry): New.\n\t(GTM::gtm_thread): Add read and write logs.\n\t(GTM::dispatch_ml_wt): Declare.\n\t* retry.cc (parse_default_method): Support ml_wt.\n\t* method-ml.cc: New file.\n\t* Makefile.am: Add method-ml.cc.\n\t* Makefile.in: Regenerate.\n\nFrom-SVN: r184212", "tree": {"sha": "33ebc1d0c4dd79d49df01737fd6b19d67d044a13", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/33ebc1d0c4dd79d49df01737fd6b19d67d044a13"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba", "html_url": "https://github.com/Rust-GCC/gccrs/commit/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba/comments", "author": {"login": "triegelrh", "id": 62400967, "node_id": "MDQ6VXNlcjYyNDAwOTY3", "avatar_url": "https://avatars.githubusercontent.com/u/62400967?v=4", "gravatar_id": "", "url": "https://api.github.com/users/triegelrh", "html_url": "https://github.com/triegelrh", "followers_url": "https://api.github.com/users/triegelrh/followers", "following_url": "https://api.github.com/users/triegelrh/following{/other_user}", "gists_url": "https://api.github.com/users/triegelrh/gists{/gist_id}", "starred_url": "https://api.github.com/users/triegelrh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/triegelrh/subscriptions", "organizations_url": "https://api.github.com/users/triegelrh/orgs", "repos_url": "https://api.github.com/users/triegelrh/repos", "events_url": "https://api.github.com/users/triegelrh/events{/privacy}", "received_events_url": "https://api.github.com/users/triegelrh/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "5b9cf5d2bf66a36b0716e07044d492b91dd835a2", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5b9cf5d2bf66a36b0716e07044d492b91dd835a2", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5b9cf5d2bf66a36b0716e07044d492b91dd835a2"}], "stats": {"total": 645, "additions": 639, "deletions": 6}, "files": [{"sha": "4dbb1e85ecdb250e09078d47049a33d60c2910ae", "filename": "libitm/ChangeLog", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba/libitm%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba/libitm%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2FChangeLog?ref=31772c9507ed3c9f69565efa9dd80dcd8c72b0ba", "patch": "@@ -1,3 +1,13 @@\n+2012-02-14  Torvald Riegel  <triegel@redhat.com>\n+\n+\t* libitm_i.h (GTM::gtm_rwlog_entry): New.\n+\t(GTM::gtm_thread): Add read and write logs.\n+\t(GTM::dispatch_ml_wt): Declare.\n+\t* retry.cc (parse_default_method): Support ml_wt.\n+\t* method-ml.cc: New file.\n+\t* Makefile.am: Add method-ml.cc.\n+\t* Makefile.in: Regenerate.\n+\n 2012-02-14  Torvald Riegel  <triegel@redhat.com>\n \n \t* dispatch.h (GTM::abi_dispatch::supports): New."}, {"sha": "e754ccc770469442ac6cbecc34eaacbe9ec87482", "filename": "libitm/Makefile.am", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba/libitm%2FMakefile.am", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba/libitm%2FMakefile.am", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2FMakefile.am?ref=31772c9507ed3c9f69565efa9dd80dcd8c72b0ba", "patch": "@@ -60,7 +60,7 @@ libitm_la_SOURCES = \\\n \taatree.cc alloc.cc alloc_c.cc alloc_cpp.cc barrier.cc beginend.cc \\\n \tclone.cc eh_cpp.cc local.cc \\\n \tquery.cc retry.cc rwlock.cc useraction.cc util.cc \\\n-\tsjlj.S tls.cc method-serial.cc method-gl.cc\n+\tsjlj.S tls.cc method-serial.cc method-gl.cc method-ml.cc\n \n if ARCH_ARM\n libitm_la_SOURCES += hwcap.cc"}, {"sha": "66aacaba716beb330426e4014a80cb61ccb894d8", "filename": "libitm/Makefile.in", "status": "modified", "additions": 6, "deletions": 5, "changes": 11, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba/libitm%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba/libitm%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2FMakefile.in?ref=31772c9507ed3c9f69565efa9dd80dcd8c72b0ba", "patch": "@@ -100,15 +100,15 @@ libitm_la_LIBADD =\n am__libitm_la_SOURCES_DIST = aatree.cc alloc.cc alloc_c.cc \\\n \talloc_cpp.cc barrier.cc beginend.cc clone.cc eh_cpp.cc \\\n \tlocal.cc query.cc retry.cc rwlock.cc useraction.cc util.cc \\\n-\tsjlj.S tls.cc method-serial.cc method-gl.cc hwcap.cc \\\n-\tx86_sse.cc x86_avx.cc futex.cc\n+\tsjlj.S tls.cc method-serial.cc method-gl.cc method-ml.cc \\\n+\thwcap.cc x86_sse.cc x86_avx.cc futex.cc\n @ARCH_ARM_TRUE@am__objects_1 = hwcap.lo\n @ARCH_X86_TRUE@am__objects_2 = x86_sse.lo x86_avx.lo\n @ARCH_FUTEX_TRUE@am__objects_3 = futex.lo\n am_libitm_la_OBJECTS = aatree.lo alloc.lo alloc_c.lo alloc_cpp.lo \\\n \tbarrier.lo beginend.lo clone.lo eh_cpp.lo local.lo query.lo \\\n \tretry.lo rwlock.lo useraction.lo util.lo sjlj.lo tls.lo \\\n-\tmethod-serial.lo method-gl.lo $(am__objects_1) \\\n+\tmethod-serial.lo method-gl.lo method-ml.lo $(am__objects_1) \\\n \t$(am__objects_2) $(am__objects_3)\n libitm_la_OBJECTS = $(am_libitm_la_OBJECTS)\n DEFAULT_INCLUDES = -I.@am__isrc@\n@@ -379,8 +379,8 @@ libitm_la_LDFLAGS = $(libitm_version_info) $(libitm_version_script)\n libitm_la_SOURCES = aatree.cc alloc.cc alloc_c.cc alloc_cpp.cc \\\n \tbarrier.cc beginend.cc clone.cc eh_cpp.cc local.cc query.cc \\\n \tretry.cc rwlock.cc useraction.cc util.cc sjlj.S tls.cc \\\n-\tmethod-serial.cc method-gl.cc $(am__append_1) $(am__append_2) \\\n-\t$(am__append_3)\n+\tmethod-serial.cc method-gl.cc method-ml.cc $(am__append_1) \\\n+\t$(am__append_2) $(am__append_3)\n \n # Automake Documentation:\n # If your package has Texinfo files in many directories, you can use the\n@@ -512,6 +512,7 @@ distclean-compile:\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/hwcap.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/local.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/method-gl.Plo@am__quote@\n+@AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/method-ml.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/method-serial.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/query.Plo@am__quote@\n @AMDEP_TRUE@@am__include@ @am__quote@./$(DEPDIR)/retry.Plo@am__quote@"}, {"sha": "e826abd0656ab783a82fae4431aa9ab248e1dc98", "filename": "libitm/libitm_i.h", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba/libitm%2Flibitm_i.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba/libitm%2Flibitm_i.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Flibitm_i.h?ref=31772c9507ed3c9f69565efa9dd80dcd8c72b0ba", "patch": "@@ -145,6 +145,13 @@ struct gtm_undolog\n   void rollback (gtm_thread* tx, size_t until_size = 0);\n };\n \n+// An entry of a read or write log.  Used by multi-lock TM methods.\n+struct gtm_rwlog_entry\n+{\n+  atomic<gtm_word> *orec;\n+  gtm_word value;\n+};\n+\n // Contains all thread-specific data required by the entire library.\n // This includes all data relevant to a single transaction. Because most\n // thread-specific data is about the current transaction, we also refer to\n@@ -174,6 +181,10 @@ struct gtm_thread\n   // Data used by local.c for the undo log for both local and shared memory.\n   gtm_undolog undolog;\n \n+  // Read and write logs.  Used by multi-lock TM methods.\n+  vector<gtm_rwlog_entry> readlog;\n+  vector<gtm_rwlog_entry> writelog;\n+\n   // Data used by alloc.c for the malloc/free undo log.\n   aa_tree<uintptr_t, gtm_alloc_action> alloc_actions;\n \n@@ -320,6 +331,7 @@ extern abi_dispatch *dispatch_serial();\n extern abi_dispatch *dispatch_serialirr();\n extern abi_dispatch *dispatch_serialirr_onwrite();\n extern abi_dispatch *dispatch_gl_wt();\n+extern abi_dispatch *dispatch_ml_wt();\n \n extern gtm_cacheline_mask gtm_mask_stack(gtm_cacheline *, gtm_cacheline_mask);\n "}, {"sha": "88455e8f85e092cbaebb65657ac202a6eebec963", "filename": "libitm/method-ml.cc", "status": "added", "additions": 605, "deletions": 0, "changes": 605, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba/libitm%2Fmethod-ml.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba/libitm%2Fmethod-ml.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fmethod-ml.cc?ref=31772c9507ed3c9f69565efa9dd80dcd8c72b0ba", "patch": "@@ -0,0 +1,605 @@\n+/* Copyright (C) 2012 Free Software Foundation, Inc.\n+   Contributed by Torvald Riegel <triegel@redhat.com>.\n+\n+   This file is part of the GNU Transactional Memory Library (libitm).\n+\n+   Libitm is free software; you can redistribute it and/or modify it\n+   under the terms of the GNU General Public License as published by\n+   the Free Software Foundation; either version 3 of the License, or\n+   (at your option) any later version.\n+\n+   Libitm is distributed in the hope that it will be useful, but WITHOUT ANY\n+   WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS\n+   FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n+   more details.\n+\n+   Under Section 7 of GPL version 3, you are granted additional\n+   permissions described in the GCC Runtime Library Exception, version\n+   3.1, as published by the Free Software Foundation.\n+\n+   You should have received a copy of the GNU General Public License and\n+   a copy of the GCC Runtime Library Exception along with this program;\n+   see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n+   <http://www.gnu.org/licenses/>.  */\n+\n+#include \"libitm_i.h\"\n+\n+using namespace GTM;\n+\n+namespace {\n+\n+// This group consists of all TM methods that synchronize via multiple locks\n+// (or ownership records).\n+struct ml_mg : public method_group\n+{\n+  static const gtm_word LOCK_BIT = (~(gtm_word)0 >> 1) + 1;\n+  static const gtm_word INCARNATION_BITS = 3;\n+  static const gtm_word INCARNATION_MASK = 7;\n+  // Maximum time is all bits except the lock bit, the overflow reserve bit,\n+  // and the incarnation bits).\n+  static const gtm_word TIME_MAX = (~(gtm_word)0 >> (2 + INCARNATION_BITS));\n+  // The overflow reserve bit is the MSB of the timestamp part of an orec,\n+  // so we can have TIME_MAX+1 pending timestamp increases before we overflow.\n+  static const gtm_word OVERFLOW_RESERVE = TIME_MAX + 1;\n+\n+  static bool is_locked(gtm_word o) { return o & LOCK_BIT; }\n+  static gtm_word set_locked(gtm_thread *tx)\n+  {\n+    return ((uintptr_t)tx >> 1) | LOCK_BIT;\n+  }\n+  // Returns a time that includes the lock bit, which is required by both\n+  // validate() and is_more_recent_or_locked().\n+  static gtm_word get_time(gtm_word o) { return o >> INCARNATION_BITS; }\n+  static gtm_word set_time(gtm_word time) { return time << INCARNATION_BITS; }\n+  static bool is_more_recent_or_locked(gtm_word o, gtm_word than_time)\n+  {\n+    // LOCK_BIT is the MSB; thus, if O is locked, it is larger than TIME_MAX.\n+    return get_time(o) > than_time;\n+  }\n+  static bool has_incarnation_left(gtm_word o)\n+  {\n+    return (o & INCARNATION_MASK) < INCARNATION_MASK;\n+  }\n+  static gtm_word inc_incarnation(gtm_word o) { return o + 1; }\n+\n+  // The shared time base.\n+  atomic<gtm_word> time __attribute__((aligned(HW_CACHELINE_SIZE)));\n+\n+  // The array of ownership records.\n+  atomic<gtm_word>* orecs __attribute__((aligned(HW_CACHELINE_SIZE)));\n+  char tailpadding[HW_CACHELINE_SIZE - sizeof(atomic<gtm_word>*)];\n+\n+  // Location-to-orec mapping.  Stripes of 16B mapped to 2^19 orecs.\n+  static const gtm_word L2O_ORECS = 1 << 19;\n+  static const gtm_word L2O_SHIFT = 4;\n+  static size_t get_orec(const void* addr)\n+  {\n+    return ((uintptr_t)addr >> L2O_SHIFT) & (L2O_ORECS - 1);\n+  }\n+  static size_t get_next_orec(size_t orec)\n+  {\n+    return (orec + 1) & (L2O_ORECS - 1);\n+  }\n+  // Returns the next orec after the region.\n+  static size_t get_orec_end(const void* addr, size_t len)\n+  {\n+    return (((uintptr_t)addr + len + (1 << L2O_SHIFT) - 1) >> L2O_SHIFT)\n+        & (L2O_ORECS - 1);\n+  }\n+\n+  virtual void init()\n+  {\n+    // We assume that an atomic<gtm_word> is backed by just a gtm_word, so\n+    // starting with zeroed memory is fine.\n+    orecs = (atomic<gtm_word>*) xcalloc(\n+        sizeof(atomic<gtm_word>) * L2O_ORECS, true);\n+    // This store is only executed while holding the serial lock, so relaxed\n+    // memory order is sufficient here.\n+    time.store(0, memory_order_relaxed);\n+  }\n+\n+  virtual void fini()\n+  {\n+    free(orecs);\n+  }\n+\n+  // We only re-initialize when our time base overflows.  Thus, only reset\n+  // the time base and the orecs but do not re-allocate the orec array.\n+  virtual void reinit()\n+  {\n+    // This store is only executed while holding the serial lock, so relaxed\n+    // memory order is sufficient here.  Same holds for the memset.\n+    time.store(0, memory_order_relaxed);\n+    memset(orecs, 0, sizeof(atomic<gtm_word>) * L2O_ORECS);\n+  }\n+};\n+\n+static ml_mg o_ml_mg;\n+\n+\n+// The multiple lock, write-through TM method.\n+// Maps each memory location to one of the orecs in the orec array, and then\n+// acquires the associated orec eagerly before writing through.\n+// Writes require undo-logging because we are dealing with several locks/orecs\n+// and need to resolve deadlocks if necessary by aborting one of the\n+// transactions.\n+// Reads do time-based validation with snapshot time extensions.  Incarnation\n+// numbers are used to decrease contention on the time base (with those,\n+// aborted transactions do not need to acquire a new version number for the\n+// data that has been previously written in the transaction and needs to be\n+// rolled back).\n+// gtm_thread::shared_state is used to store a transaction's current\n+// snapshot time (or commit time). The serial lock uses ~0 for inactive\n+// transactions and 0 for active ones. Thus, we always have a meaningful\n+// timestamp in shared_state that can be used to implement quiescence-based\n+// privatization safety.\n+class ml_wt_dispatch : public abi_dispatch\n+{\n+protected:\n+  static void pre_write(gtm_thread *tx, const void *addr, size_t len)\n+  {\n+    gtm_word snapshot = tx->shared_state.load(memory_order_relaxed);\n+    gtm_word locked_by_tx = ml_mg::set_locked(tx);\n+\n+    // Lock all orecs that cover the region.\n+    size_t orec = ml_mg::get_orec(addr);\n+    size_t orec_end = ml_mg::get_orec_end(addr, len);\n+    do\n+      {\n+        // Load the orec.  Relaxed memory order is sufficient here because\n+        // either we have acquired the orec or we will try to acquire it with\n+        // a CAS with stronger memory order.\n+        gtm_word o = o_ml_mg.orecs[orec].load(memory_order_relaxed);\n+\n+        // Check whether we have acquired the orec already.\n+        if (likely (locked_by_tx != o))\n+          {\n+            // If not, acquire.  Make sure that our snapshot time is larger or\n+            // equal than the orec's version to avoid masking invalidations of\n+            // our snapshot with our own writes.\n+            if (unlikely (ml_mg::is_locked(o)))\n+              tx->restart(RESTART_LOCKED_WRITE);\n+\n+            if (unlikely (ml_mg::get_time(o) > snapshot))\n+              {\n+                // We only need to extend the snapshot if we have indeed read\n+                // from this orec before.  Given that we are an update\n+                // transaction, we will have to extend anyway during commit.\n+                // ??? Scan the read log instead, aborting if we have read\n+                // from data covered by this orec before?\n+                snapshot = extend(tx);\n+              }\n+\n+            // We need acquire memory order here to synchronize with other\n+            // (ownership) releases of the orec.  We do not need acq_rel order\n+            // because whenever another thread reads from this CAS'\n+            // modification, then it will abort anyway and does not rely on\n+            // any further happens-before relation to be established.\n+            if (unlikely (!o_ml_mg.orecs[orec].compare_exchange_strong(\n+                o, locked_by_tx, memory_order_acquire)))\n+              tx->restart(RESTART_LOCKED_WRITE);\n+\n+            // We use an explicit fence here to avoid having to use release\n+            // memory order for all subsequent data stores.  This fence will\n+            // synchronize with loads of the data with acquire memory order.\n+            // See post_load() for why this is necessary.\n+            // Adding require memory order to the prior CAS is not sufficient,\n+            // at least according to the Batty et al. formalization of the\n+            // memory model.\n+            atomic_thread_fence(memory_order_release);\n+\n+            // We log the previous value here to be able to use incarnation\n+            // numbers when we have to roll back.\n+            // ??? Reserve capacity early to avoid capacity checks here?\n+            gtm_rwlog_entry *e = tx->writelog.push();\n+            e->orec = o_ml_mg.orecs + orec;\n+            e->value = o;\n+          }\n+        orec = o_ml_mg.get_next_orec(orec);\n+      }\n+    while (orec != orec_end);\n+\n+    // Do undo logging.  We do not know which region prior writes logged\n+    // (even if orecs have been acquired), so just log everything.\n+    tx->undolog.log(addr, len);\n+  }\n+\n+  static void pre_write(const void *addr, size_t len)\n+  {\n+    gtm_thread *tx = gtm_thr();\n+    pre_write(tx, addr, len);\n+  }\n+\n+  // Returns true iff all the orecs in our read log still have the same time\n+  // or have been locked by the transaction itself.\n+  static bool validate(gtm_thread *tx)\n+  {\n+    gtm_word locked_by_tx = ml_mg::set_locked(tx);\n+    // ??? This might get called from pre_load() via extend().  In that case,\n+    // we don't really need to check the new entries that pre_load() is\n+    // adding.  Stop earlier?\n+    for (gtm_rwlog_entry *i = tx->readlog.begin(), *ie = tx->readlog.end();\n+        i != ie; i++)\n+      {\n+\t// Relaxed memory order is sufficient here because we do not need to\n+\t// establish any new synchronizes-with relationships.  We only need\n+\t// to read a value that is as least as current as enforced by the\n+\t// callers: extend() loads global time with acquire, and trycommit()\n+\t// increments global time with acquire.  Therefore, we will see the\n+\t// most recent orec updates before the global time that we load.\n+        gtm_word o = i->orec->load(memory_order_relaxed);\n+        // We compare only the time stamp and the lock bit here.  We know that\n+        // we have read only committed data before, so we can ignore\n+        // intermediate yet rolled-back updates presented by the incarnation\n+        // number bits.\n+        if (ml_mg::get_time(o) != ml_mg::get_time(i->value)\n+            && o != locked_by_tx)\n+          return false;\n+      }\n+    return true;\n+  }\n+\n+  // Tries to extend the snapshot to a more recent time.  Returns the new\n+  // snapshot time and updates TX->SHARED_STATE.  If the snapshot cannot be\n+  // extended to the current global time, TX is restarted.\n+  static gtm_word extend(gtm_thread *tx)\n+  {\n+    // We read global time here, even if this isn't strictly necessary\n+    // because we could just return the maximum of the timestamps that\n+    // validate sees.  However, the potential cache miss on global time is\n+    // probably a reasonable price to pay for avoiding unnecessary extensions\n+    // in the future.\n+    // We need acquire memory oder because we have to synchronize with the\n+    // increment of global time by update transactions, whose lock\n+    // acquisitions we have to observe (also see trycommit()).\n+    gtm_word snapshot = o_ml_mg.time.load(memory_order_acquire);\n+    if (!validate(tx))\n+      tx->restart(RESTART_VALIDATE_READ);\n+\n+    // Update our public snapshot time.  Probably useful to decrease waiting\n+    // due to quiescence-based privatization safety.\n+    // Use release memory order to establish synchronizes-with with the\n+    // privatizers; prior data loads should happen before the privatizers\n+    // potentially modify anything.\n+    tx->shared_state.store(snapshot, memory_order_release);\n+    return snapshot;\n+  }\n+\n+  // First pass over orecs.  Load and check all orecs that cover the region.\n+  // Write to read log, extend snapshot time if necessary.\n+  static gtm_rwlog_entry* pre_load(gtm_thread *tx, const void* addr,\n+      size_t len)\n+  {\n+    // Don't obtain an iterator yet because the log might get resized.\n+    size_t log_start = tx->readlog.size();\n+    gtm_word snapshot = tx->shared_state.load(memory_order_relaxed);\n+    gtm_word locked_by_tx = ml_mg::set_locked(tx);\n+\n+    size_t orec = ml_mg::get_orec(addr);\n+    size_t orec_end = ml_mg::get_orec_end(addr, len);\n+    do\n+      {\n+        // We need acquire memory order here so that this load will\n+        // synchronize with the store that releases the orec in trycommit().\n+        // In turn, this makes sure that subsequent data loads will read from\n+        // a visible sequence of side effects that starts with the most recent\n+        // store to the data right before the release of the orec.\n+        gtm_word o = o_ml_mg.orecs[orec].load(memory_order_acquire);\n+\n+        if (likely (!ml_mg::is_more_recent_or_locked(o, snapshot)))\n+          {\n+            success:\n+            gtm_rwlog_entry *e = tx->readlog.push();\n+            e->orec = o_ml_mg.orecs + orec;\n+            e->value = o;\n+          }\n+        else if (!ml_mg::is_locked(o))\n+          {\n+            // We cannot read this part of the region because it has been\n+            // updated more recently than our snapshot time.  If we can extend\n+            // our snapshot, then we can read.\n+            snapshot = extend(tx);\n+            goto success;\n+          }\n+        else\n+          {\n+            // If the orec is locked by us, just skip it because we can just\n+            // read from it.  Otherwise, restart the transaction.\n+            if (o != locked_by_tx)\n+              tx->restart(RESTART_LOCKED_READ);\n+          }\n+        orec = o_ml_mg.get_next_orec(orec);\n+      }\n+    while (orec != orec_end);\n+    return &tx->readlog[log_start];\n+  }\n+\n+  // Second pass over orecs, verifying that the we had a consistent read.\n+  // Restart the transaction if any of the orecs is locked by another\n+  // transaction.\n+  static void post_load(gtm_thread *tx, gtm_rwlog_entry* log)\n+  {\n+    for (gtm_rwlog_entry *end = tx->readlog.end(); log != end; log++)\n+      {\n+        // Check that the snapshot is consistent.  We expect the previous data\n+        // load to have acquire memory order, or be atomic and followed by an\n+        // acquire fence.\n+        // As a result, the data load will synchronize with the release fence\n+        // issued by the transactions whose data updates the data load has read\n+        // from.  This forces the orec load to read from a visible sequence of\n+        // side effects that starts with the other updating transaction's\n+        // store that acquired the orec and set it to locked.\n+        // We therefore either read a value with the locked bit set (and\n+        // restart) or read an orec value that was written after the data had\n+        // been written.  Either will allow us to detect inconsistent reads\n+        // because it will have a higher/different value.\n+\t// Also note that differently to validate(), we compare the raw value\n+\t// of the orec here, including incarnation numbers.  We must prevent\n+\t// returning uncommitted data from loads (whereas when validating, we\n+\t// already performed a consistent load).\n+        gtm_word o = log->orec->load(memory_order_relaxed);\n+        if (log->value != o)\n+          tx->restart(RESTART_VALIDATE_READ);\n+      }\n+  }\n+\n+  template <typename V> static V load(const V* addr, ls_modifier mod)\n+  {\n+    // Read-for-write should be unlikely, but we need to handle it or will\n+    // break later WaW optimizations.\n+    if (unlikely(mod == RfW))\n+      {\n+\tpre_write(addr, sizeof(V));\n+\treturn *addr;\n+      }\n+    if (unlikely(mod == RaW))\n+      return *addr;\n+    // ??? Optimize for RaR?\n+\n+    gtm_thread *tx = gtm_thr();\n+    gtm_rwlog_entry* log = pre_load(tx, addr, sizeof(V));\n+\n+    // Load the data.\n+    // This needs to have acquire memory order (see post_load()).\n+    // Alternatively, we can put an acquire fence after the data load but this\n+    // is probably less efficient.\n+    // FIXME We would need an atomic load with acquire memory order here but\n+    // we can't just forge an atomic load for nonatomic data because this\n+    // might not work on all implementations of atomics.  However, we need\n+    // the acquire memory order and we can only establish this if we link\n+    // it to the matching release using a reads-from relation between atomic\n+    // loads.  Also, the compiler is allowed to optimize nonatomic accesses\n+    // differently than atomic accesses (e.g., if the load would be moved to\n+    // after the fence, we potentially don't synchronize properly anymore).\n+    // Instead of the following, just use an ordinary load followed by an\n+    // acquire fence, and hope that this is good enough for now:\n+    // V v = atomic_load_explicit((atomic<V>*)addr, memory_order_acquire);\n+    V v = *addr;\n+    atomic_thread_fence(memory_order_acquire);\n+\n+    // ??? Retry the whole load if it wasn't consistent?\n+    post_load(tx, log);\n+\n+    return v;\n+  }\n+\n+  template <typename V> static void store(V* addr, const V value,\n+      ls_modifier mod)\n+  {\n+    if (likely(mod != WaW))\n+      pre_write(addr, sizeof(V));\n+    // FIXME We would need an atomic store here but we can't just forge an\n+    // atomic load for nonatomic data because this might not work on all\n+    // implementations of atomics.  However, we need this store to link the\n+    // release fence in pre_write() to the acquire operation in load, which\n+    // is only guaranteed if we have a reads-from relation between atomic\n+    // accesses.  Also, the compiler is allowed to optimize nonatomic accesses\n+    // differently than atomic accesses (e.g., if the store would be moved\n+    // to before the release fence in pre_write(), things could go wrong).\n+    // atomic_store_explicit((atomic<V>*)addr, value, memory_order_relaxed);\n+    *addr = value;\n+  }\n+\n+public:\n+  static void memtransfer_static(void *dst, const void* src, size_t size,\n+      bool may_overlap, ls_modifier dst_mod, ls_modifier src_mod)\n+  {\n+    gtm_rwlog_entry* log = 0;\n+    gtm_thread *tx = 0;\n+\n+    if (src_mod == RfW)\n+      {\n+        tx = gtm_thr();\n+        pre_write(tx, src, size);\n+      }\n+    else if (src_mod != RaW && src_mod != NONTXNAL)\n+      {\n+        tx = gtm_thr();\n+        log = pre_load(tx, src, size);\n+      }\n+    // ??? Optimize for RaR?\n+\n+    if (dst_mod != NONTXNAL && dst_mod != WaW)\n+      {\n+        if (src_mod != RfW && (src_mod == RaW || src_mod == NONTXNAL))\n+          tx = gtm_thr();\n+        pre_write(tx, dst, size);\n+      }\n+\n+    // FIXME We should use atomics here (see store()).  Let's just hope that\n+    // memcpy/memmove are good enough.\n+    if (!may_overlap)\n+      ::memcpy(dst, src, size);\n+    else\n+      ::memmove(dst, src, size);\n+\n+    // ??? Retry the whole memtransfer if it wasn't consistent?\n+    if (src_mod != RfW && src_mod != RaW && src_mod != NONTXNAL)\n+      {\n+\t// See load() for why we need the acquire fence here.\n+\tatomic_thread_fence(memory_order_acquire);\n+\tpost_load(tx, log);\n+      }\n+  }\n+\n+  static void memset_static(void *dst, int c, size_t size, ls_modifier mod)\n+  {\n+    if (mod != WaW)\n+      pre_write(dst, size);\n+    // FIXME We should use atomics here (see store()).  Let's just hope that\n+    // memset is good enough.\n+    ::memset(dst, c, size);\n+  }\n+\n+  virtual gtm_restart_reason begin_or_restart()\n+  {\n+    // We don't need to do anything for nested transactions.\n+    gtm_thread *tx = gtm_thr();\n+    if (tx->parent_txns.size() > 0)\n+      return NO_RESTART;\n+\n+    // Read the current time, which becomes our snapshot time.\n+    // Use acquire memory oder so that we see the lock acquisitions by update\n+    // transcations that incremented the global time (see trycommit()).\n+    gtm_word snapshot = o_ml_mg.time.load(memory_order_acquire);\n+    // Re-initialize method group on time overflow.\n+    if (snapshot >= o_ml_mg.TIME_MAX)\n+      return RESTART_INIT_METHOD_GROUP;\n+\n+    // We don't need to enforce any ordering for the following store. There\n+    // are no earlier data loads in this transaction, so the store cannot\n+    // become visible before those (which could lead to the violation of\n+    // privatization safety). The store can become visible after later loads\n+    // but this does not matter because the previous value will have been\n+    // smaller or equal (the serial lock will set shared_state to zero when\n+    // marking the transaction as active, and restarts enforce immediate\n+    // visibility of a smaller or equal value with a barrier (see\n+    // rollback()).\n+    tx->shared_state.store(snapshot, memory_order_relaxed);\n+    return NO_RESTART;\n+  }\n+\n+  virtual bool trycommit(gtm_word& priv_time)\n+  {\n+    gtm_thread* tx = gtm_thr();\n+\n+    // If we haven't updated anything, we can commit.\n+    if (!tx->writelog.size())\n+      {\n+        tx->readlog.clear();\n+        return true;\n+      }\n+\n+    // Get a commit time.\n+    // Overflow of o_ml_mg.time is prevented in begin_or_restart().\n+    // We need acq_rel here because (1) the acquire part is required for our\n+    // own subsequent call to validate(), and the release part is necessary to\n+    // make other threads' validate() work as explained there and in extend().\n+    gtm_word ct = o_ml_mg.time.fetch_add(1, memory_order_acq_rel) + 1;\n+\n+    // Extend our snapshot time to at least our commit time.\n+    // Note that we do not need to validate if our snapshot time is right\n+    // before the commit time because we are never sharing the same commit\n+    // time with other transactions.\n+    // No need to reset shared_state, which will be modified by the serial\n+    // lock right after our commit anyway.\n+    gtm_word snapshot = tx->shared_state.load(memory_order_relaxed);\n+    if (snapshot < ct - 1 && !validate(tx))\n+      return false;\n+\n+    // Release orecs.\n+    // See pre_load() / post_load() for why we need release memory order.\n+    // ??? Can we use a release fence and relaxed stores?\n+    gtm_word v = ml_mg::set_time(ct);\n+    for (gtm_rwlog_entry *i = tx->writelog.begin(), *ie = tx->writelog.end();\n+        i != ie; i++)\n+      i->orec->store(v, memory_order_release);\n+\n+    // We're done, clear the logs.\n+    tx->writelog.clear();\n+    tx->readlog.clear();\n+\n+    // Need to ensure privatization safety. Every other transaction must\n+    // have a snapshot time that is at least as high as our commit time\n+    // (i.e., our commit must be visible to them).\n+    priv_time = ct;\n+    return true;\n+  }\n+\n+  virtual void rollback(gtm_transaction_cp *cp)\n+  {\n+    // We don't do anything for rollbacks of nested transactions.\n+    // ??? We could release locks here if we snapshot writelog size.  readlog\n+    // is similar.  This is just a performance optimization though.  Nested\n+    // aborts should be rather infrequent, so the additional save/restore\n+    // overhead for the checkpoints could be higher.\n+    if (cp != 0)\n+      return;\n+\n+    gtm_thread *tx = gtm_thr();\n+    gtm_word overflow_value = 0;\n+\n+    // Release orecs.\n+    for (gtm_rwlog_entry *i = tx->writelog.begin(), *ie = tx->writelog.end();\n+        i != ie; i++)\n+      {\n+        // If possible, just increase the incarnation number.\n+        // See pre_load() / post_load() for why we need release memory order.\n+\t// ??? Can we use a release fence and relaxed stores?  (Same below.)\n+        if (ml_mg::has_incarnation_left(i->value))\n+          i->orec->store(ml_mg::inc_incarnation(i->value),\n+              memory_order_release);\n+        else\n+          {\n+            // We have an incarnation overflow.  Acquire a new timestamp, and\n+            // use it from now on as value for each orec whose incarnation\n+            // number cannot be increased.\n+            // Overflow of o_ml_mg.time is prevented in begin_or_restart().\n+            // See pre_load() / post_load() for why we need release memory\n+            // order.\n+            if (!overflow_value)\n+              // Release memory order is sufficient but required here.\n+              // In contrast to the increment in trycommit(), we need release\n+              // for the same reason but do not need the acquire because we\n+              // do not validate subsequently.\n+              overflow_value = ml_mg::set_time(\n+                  o_ml_mg.time.fetch_add(1, memory_order_release) + 1);\n+            i->orec->store(overflow_value, memory_order_release);\n+          }\n+      }\n+\n+    // We need this release fence to ensure that privatizers see the\n+    // rolled-back original state (not any uncommitted values) when they read\n+    // the new snapshot time that we write in begin_or_restart().\n+    atomic_thread_fence(memory_order_release);\n+\n+    // We're done, clear the logs.\n+    tx->writelog.clear();\n+    tx->readlog.clear();\n+  }\n+\n+  virtual bool supports(unsigned number_of_threads)\n+  {\n+    // Each txn can commit and fail and rollback once before checking for\n+    // overflow, so this bounds the number of threads that we can support.\n+    // In practice, this won't be a problem but we check it anyway so that\n+    // we never break in the occasional weird situation.\n+    return (number_of_threads * 2 <= ml_mg::OVERFLOW_RESERVE);\n+  }\n+\n+  CREATE_DISPATCH_METHODS(virtual, )\n+  CREATE_DISPATCH_METHODS_MEM()\n+\n+  ml_wt_dispatch() : abi_dispatch(false, true, false, false, &o_ml_mg)\n+  { }\n+};\n+\n+} // anon namespace\n+\n+static const ml_wt_dispatch o_ml_wt_dispatch;\n+\n+abi_dispatch *\n+GTM::dispatch_ml_wt ()\n+{\n+  return const_cast<ml_wt_dispatch *>(&o_ml_wt_dispatch);\n+}"}, {"sha": "d59c1834ef007a2a8c7811f2f52704c3493e282d", "filename": "libitm/retry.cc", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba/libitm%2Fretry.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/31772c9507ed3c9f69565efa9dd80dcd8c72b0ba/libitm%2Fretry.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libitm%2Fretry.cc?ref=31772c9507ed3c9f69565efa9dd80dcd8c72b0ba", "patch": "@@ -199,6 +199,11 @@ parse_default_method()\n       disp = GTM::dispatch_gl_wt();\n       env += 5;\n     }\n+  else if (strncmp(env, \"ml_wt\", 5) == 0)\n+    {\n+      disp = GTM::dispatch_ml_wt();\n+      env += 5;\n+    }\n   else\n     goto unknown;\n "}]}