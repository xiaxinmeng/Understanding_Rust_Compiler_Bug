{"sha": "ebfd146af752ce6aa298f866e36f9a7e66e32ec5", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZWJmZDE0NmFmNzUyY2U2YWEyOThmODY2ZTM2ZjlhN2U2NmUzMmVjNQ==", "commit": {"author": {"name": "Ira Rosen", "email": "irar@il.ibm.com", "date": "2009-03-30T07:22:04Z"}, "committer": {"name": "Ira Rosen", "email": "irar@gcc.gnu.org", "date": "2009-03-30T07:22:04Z"}, "message": "tree-vect-loop-manip.c: New file.\n\n\t* tree-vect-loop-manip.c: New file.\n\t* tree-vectorizer.c: Update documentation and included files.\n\t(vect_loop_location): Make extern.\n\t(rename_use_op): Move to tree-vect-loop-manip.c\n\t(rename_variables_in_bb, rename_variables_in_loop, \n\tslpeel_update_phis_for_duplicate_loop, \n\tslpeel_update_phi_nodes_for_guard1,\n\tslpeel_update_phi_nodes_for_guard2, slpeel_make_loop_iterate_ntimes,\n\tslpeel_tree_duplicate_loop_to_edge_cfg, slpeel_add_loop_guard,\n\tslpeel_can_duplicate_loop_p, slpeel_verify_cfg_after_peeling,\n\tset_prologue_iterations, slpeel_tree_peel_loop_to_edge, \n\tfind_loop_location): Likewise.\n\t(new_stmt_vec_info): Move to tree-vect-stmts.c.\n\t(init_stmt_vec_info_vec, free_stmt_vec_info_vec, free_stmt_vec_info,\n\tget_vectype_for_scalar_type, vect_is_simple_use,\n\tsupportable_widening_operation, supportable_narrowing_operation):\n\tLikewise.\n\t(bb_in_loop_p): Move to tree-vect-loop.c.\n\t(new_loop_vec_info, destroy_loop_vec_info, \n\treduction_code_for_scalar_code, report_vect_op, \n\tvect_is_simple_reduction, vect_is_simple_iv_evolution): Likewise.\n\t(vect_can_force_dr_alignment_p): Move to tree-vect-data-refs.c.\n\t(vect_supportable_dr_alignment): Likewise.\n\t* tree-vectorizer.h (tree-data-ref.h): Include.\n\t(vect_loop_location): Declare.\n\tReorganize function declarations according to the new file structure.\n\t* tree-vect-loop.c: New file.\n\t* tree-vect-analyze.c: Remove. Move functions to tree-vect-data-refs.c, \n\ttree-vect-stmts.c, tree-vect-slp.c, tree-vect-loop.c.\n\t* tree-vect-data-refs.c: New file.\n\t* tree-vect-patterns.c (timevar.h): Don't include.\n\t* tree-vect-stmts.c: New file.\n\t* tree-vect-transform.c: Remove. Move functions to tree-vect-stmts.c, \n\ttree-vect-slp.c, tree-vect-loop.c.\n\t* Makefile.in (OBJS-common): Remove tree-vect-analyze.o and \n\ttree-vect-transform.o. Add tree-vect-data-refs.o, tree-vect-stmts.o, \n\ttree-vect-loop.o, tree-vect-loop-manip.o, tree-vect-slp.o.\n\t(tree-vect-analyze.o): Remove.\n\t(tree-vect-transform.o): Likewise.\n\t(tree-vect-data-refs.o): Add rule.\n\t(tree-vect-stmts.o, tree-vect-loop.o, tree-vect-loop-manip.o, \n\ttree-vect-slp.o): Likewise.\n\t(tree-vect-patterns.o): Remove redundant dependencies.\n\t(tree-vectorizer.o): Likewise.\n\t* tree-vect-slp.c: New file.\n\nFrom-SVN: r145280", "tree": {"sha": "eb1de264e27b52e72961ef1403214ca950263b68", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/eb1de264e27b52e72961ef1403214ca950263b68"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/ebfd146af752ce6aa298f866e36f9a7e66e32ec5", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ebfd146af752ce6aa298f866e36f9a7e66e32ec5", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ebfd146af752ce6aa298f866e36f9a7e66e32ec5", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/comments", "author": {"login": "irar2", "id": 16818592, "node_id": "MDQ6VXNlcjE2ODE4NTky", "avatar_url": "https://avatars.githubusercontent.com/u/16818592?v=4", "gravatar_id": "", "url": "https://api.github.com/users/irar2", "html_url": "https://github.com/irar2", "followers_url": "https://api.github.com/users/irar2/followers", "following_url": "https://api.github.com/users/irar2/following{/other_user}", "gists_url": "https://api.github.com/users/irar2/gists{/gist_id}", "starred_url": "https://api.github.com/users/irar2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/irar2/subscriptions", "organizations_url": "https://api.github.com/users/irar2/orgs", "repos_url": "https://api.github.com/users/irar2/repos", "events_url": "https://api.github.com/users/irar2/events{/privacy}", "received_events_url": "https://api.github.com/users/irar2/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "40a1cfba7818100adbde7144be9f6515b9a6ed86", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/40a1cfba7818100adbde7144be9f6515b9a6ed86", "html_url": "https://github.com/Rust-GCC/gccrs/commit/40a1cfba7818100adbde7144be9f6515b9a6ed86"}], "stats": {"total": 32329, "additions": 16245, "deletions": 16084}, "files": [{"sha": "77ba2647edfebdc8de7fae3d05146885b9460f96", "filename": "gcc/ChangeLog", "status": "modified", "additions": 48, "deletions": 0, "changes": 48, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=ebfd146af752ce6aa298f866e36f9a7e66e32ec5", "patch": "@@ -1,3 +1,51 @@\n+2009-03-30  Ira Rosen  <irar@il.ibm.com>\n+\n+\t* tree-vect-loop-manip.c: New file.\n+\t* tree-vectorizer.c: Update documentation and included files.\n+\t(vect_loop_location): Make extern.\n+\t(rename_use_op): Move to tree-vect-loop-manip.c\n+\t(rename_variables_in_bb, rename_variables_in_loop, \n+\tslpeel_update_phis_for_duplicate_loop, \n+\tslpeel_update_phi_nodes_for_guard1,\n+\tslpeel_update_phi_nodes_for_guard2, slpeel_make_loop_iterate_ntimes,\n+\tslpeel_tree_duplicate_loop_to_edge_cfg, slpeel_add_loop_guard,\n+\tslpeel_can_duplicate_loop_p, slpeel_verify_cfg_after_peeling,\n+\tset_prologue_iterations, slpeel_tree_peel_loop_to_edge, \n+\tfind_loop_location): Likewise.\n+\t(new_stmt_vec_info): Move to tree-vect-stmts.c.\n+\t(init_stmt_vec_info_vec, free_stmt_vec_info_vec, free_stmt_vec_info,\n+\tget_vectype_for_scalar_type, vect_is_simple_use,\n+\tsupportable_widening_operation, supportable_narrowing_operation):\n+\tLikewise.\n+\t(bb_in_loop_p): Move to tree-vect-loop.c.\n+\t(new_loop_vec_info, destroy_loop_vec_info, \n+\treduction_code_for_scalar_code, report_vect_op, \n+\tvect_is_simple_reduction, vect_is_simple_iv_evolution): Likewise.\n+\t(vect_can_force_dr_alignment_p): Move to tree-vect-data-refs.c.\n+\t(vect_supportable_dr_alignment): Likewise.\n+\t* tree-vectorizer.h (tree-data-ref.h): Include.\n+\t(vect_loop_location): Declare.\n+\tReorganize function declarations according to the new file structure.\n+\t* tree-vect-loop.c: New file.\n+\t* tree-vect-analyze.c: Remove. Move functions to tree-vect-data-refs.c, \n+\ttree-vect-stmts.c, tree-vect-slp.c, tree-vect-loop.c.\n+\t* tree-vect-data-refs.c: New file.\n+\t* tree-vect-patterns.c (timevar.h): Don't include.\n+\t* tree-vect-stmts.c: New file.\n+\t* tree-vect-transform.c: Remove. Move functions to tree-vect-stmts.c, \n+\ttree-vect-slp.c, tree-vect-loop.c.\n+\t* Makefile.in (OBJS-common): Remove tree-vect-analyze.o and \n+\ttree-vect-transform.o. Add tree-vect-data-refs.o, tree-vect-stmts.o, \n+\ttree-vect-loop.o, tree-vect-loop-manip.o, tree-vect-slp.o.\n+\t(tree-vect-analyze.o): Remove.\n+\t(tree-vect-transform.o): Likewise.\n+\t(tree-vect-data-refs.o): Add rule.\n+\t(tree-vect-stmts.o, tree-vect-loop.o, tree-vect-loop-manip.o, \n+\ttree-vect-slp.o): Likewise.\n+\t(tree-vect-patterns.o): Remove redundant dependencies.\n+\t(tree-vectorizer.o): Likewise.\n+\t* tree-vect-slp.c: New file.\n+\n 2009-03-30  Ralf Wildenhues  <Ralf.Wildenhues@gmx.de>\n \n \t* optc-gen.awk: Warn if an option flag has multiple different"}, {"sha": "2651ca3aad264275d8ee5ab8d9b7b0e3ae466430", "filename": "gcc/Makefile.in", "status": "modified", "additions": 29, "deletions": 19, "changes": 48, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=ebfd146af752ce6aa298f866e36f9a7e66e32ec5", "patch": "@@ -1259,10 +1259,13 @@ OBJS-common = \\\n \ttree-ssanames.o \\\n \ttree-stdarg.o \\\n \ttree-tailcall.o \\\n-\ttree-vect-analyze.o \\\n \ttree-vect-generic.o \\\n \ttree-vect-patterns.o \\\n-\ttree-vect-transform.o \\\n+        tree-vect-data-refs.o \\\n+        tree-vect-stmts.o \\\n+        tree-vect-loop.o \\\n+        tree-vect-loop-manip.o \\\n+        tree-vect-slp.o \\\n \ttree-vectorizer.o \\\n \ttree-vrp.o \\\n \ttree.o \\\n@@ -2349,26 +2352,33 @@ graphite.o: graphite.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n    $(TREE_FLOW_H) $(TREE_DUMP_H) $(TIMEVAR_H) $(CFGLOOP_H) $(GIMPLE_H) domwalk.h \\\n    $(TREE_DATA_REF_H) $(SCEV_H) tree-pass.h tree-chrec.h graphite.h pointer-set.h \\\n    value-prof.h\n-tree-vect-analyze.o: tree-vect-analyze.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n-   $(TM_H) $(GGC_H) $(OPTABS_H) $(TREE_H) $(RECOG_H) $(BASIC_BLOCK_H) \\\n-   $(DIAGNOSTIC_H) $(TREE_FLOW_H) $(TREE_DUMP_H) $(TIMEVAR_H) $(CFGLOOP_H) \\\n-   tree-vectorizer.h $(TREE_DATA_REF_H) $(SCEV_H) $(EXPR_H) tree-chrec.h \\\n-   $(TOPLEV_H) $(RECOG_H)\n+tree-vect-loop.o: tree-vect-loop.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n+   $(TM_H) $(GGC_H) $(TREE_H) $(BASIC_BLOCK_H) $(DIAGNOSTIC_H) $(TREE_FLOW_H) \\\n+   $(TREE_DUMP_H) $(CFGLOOP_H) $(EXPR_H) $(RECOG_H) $(OPTABS_H) $(TOPLEV_H) \\\n+   tree-chrec.h $(SCEV_H) tree-vectorizer.h\n+tree-vect-loop-manip.o: tree-vect-loop-manip.c $(CONFIG_H) $(SYSTEM_H) \\\n+   coretypes.h $(TM_H) $(GGC_H) $(TREE_H) $(BASIC_BLOCK_H) $(DIAGNOSTIC_H) \\\n+   $(TREE_FLOW_H) $(TREE_DUMP_H) $(CFGLOOP_H) $(EXPR_H) $(TOPLEV_H) $(SCEV_H) \\\n+   tree-vectorizer.h langhooks.h\n tree-vect-patterns.o: tree-vect-patterns.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n    $(TM_H) $(GGC_H) $(TREE_H) $(TARGET_H) $(BASIC_BLOCK_H) $(DIAGNOSTIC_H) \\\n-   $(TREE_FLOW_H) $(TREE_DUMP_H) $(TIMEVAR_H) $(CFGLOOP_H) $(EXPR_H) \\\n-   $(OPTABS_H) $(PARAMS_H) $(TREE_DATA_REF_H) tree-vectorizer.h $(RECOG_H) $(TOPLEV_H)\n-tree-vect-transform.o: tree-vect-transform.c $(CONFIG_H) $(SYSTEM_H) \\\n-   coretypes.h $(TM_H) $(GGC_H) $(OPTABS_H) $(RECOG_H) $(TREE_H) $(RTL_H) \\\n-   $(BASIC_BLOCK_H) $(DIAGNOSTIC_H) $(TREE_FLOW_H) $(TREE_DUMP_H) \\\n-   $(TIMEVAR_H) $(CFGLOOP_H) $(TARGET_H) tree-pass.h $(EXPR_H) \\\n-   tree-vectorizer.h $(TREE_DATA_REF_H) $(SCEV_H) langhooks.h $(TOPLEV_H) \\\n-   tree-chrec.h\n+   $(TREE_FLOW_H) $(TREE_DUMP_H) $(CFGLOOP_H) $(EXPR_H) $(OPTABS_H) $(PARAMS_H) \\\n+   $(TREE_DATA_REF_H) tree-vectorizer.h $(RECOG_H) $(TOPLEV_H)\n+tree-vect-slp.o: tree-vect-slp.c $(CONFIG_H) $(SYSTEM_H) \\\n+   coretypes.h $(TM_H) $(GGC_H) $(TREE_H) $(TARGET_H) $(BASIC_BLOCK_H) \\\n+   $(DIAGNOSTIC_H) $(TREE_FLOW_H) $(TREE_DUMP_H) $(CFGLOOP_H) \\\n+   $(EXPR_H) $(RECOG_H) $(OPTABS_H) tree-vectorizer.h\n+tree-vect-stmts.o: tree-vect-stmts.c $(CONFIG_H) $(SYSTEM_H) \\\n+   coretypes.h $(TM_H) $(GGC_H) $(TREE_H) $(TARGET_H) $(BASIC_BLOCK_H) \\\n+   $(DIAGNOSTIC_H) $(TREE_FLOW_H) $(TREE_DUMP_H) $(CFGLOOP_H) \\\n+   $(EXPR_H) $(RECOG_H) $(OPTABS_H) tree-vectorizer.h langhooks.h\n+tree-vect-data-refs.o: tree-vect-data-refs.c $(CONFIG_H) $(SYSTEM_H) \\\n+   coretypes.h $(TM_H) $(GGC_H) $(TREE_H) $(TARGET_H) $(BASIC_BLOCK_H) \\\n+   $(DIAGNOSTIC_H) $(TREE_FLOW_H) $(TREE_DUMP_H) $(CFGLOOP_H) \\\n+   $(EXPR_H) $(OPTABS_H) tree-chrec.h $(SCEV_H) tree-vectorizer.h $(TOPLEV_H)\n tree-vectorizer.o: tree-vectorizer.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n-   $(TM_H) $(GGC_H) $(OPTABS_H) $(TREE_H) $(RTL_H) $(BASIC_BLOCK_H) \\\n-   $(DIAGNOSTIC_H) $(TREE_FLOW_H) $(TREE_DUMP_H) $(TIMEVAR_H) $(CFGLOOP_H) \\\n-   tree-pass.h $(EXPR_H) $(RECOG_H) tree-vectorizer.h $(TREE_DATA_REF_H) $(SCEV_H) \\\n-   $(INPUT_H) $(TARGET_H) $(CFGLAYOUT_H) $(TOPLEV_H) tree-chrec.h langhooks.h\n+   $(TM_H) $(GGC_H) $(TREE_H) $(DIAGNOSTIC_H) $(TREE_FLOW_H) $(TREE_DUMP_H) \\\n+   $(CFGLOOP_H) tree-pass.h tree-vectorizer.h $(TIMEVAR_H)\n tree-loop-linear.o: tree-loop-linear.c $(CONFIG_H) $(SYSTEM_H) coretypes.h \\\n    $(TM_H) $(GGC_H) $(OPTABS_H) $(TREE_H) $(RTL_H) $(BASIC_BLOCK_H) \\\n    $(DIAGNOSTIC_H) $(TREE_FLOW_H) $(TREE_DUMP_H) $(TIMEVAR_H) $(CFGLOOP_H) \\"}, {"sha": "eb5166bac92371d7ea626e573b45610c8ca4f28c", "filename": "gcc/tree-vect-analyze.c", "status": "removed", "additions": 0, "deletions": 4713, "changes": 4713, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/40a1cfba7818100adbde7144be9f6515b9a6ed86/gcc%2Ftree-vect-analyze.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/40a1cfba7818100adbde7144be9f6515b9a6ed86/gcc%2Ftree-vect-analyze.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-analyze.c?ref=40a1cfba7818100adbde7144be9f6515b9a6ed86"}, {"sha": "b4cabb6f0c1dd4b3c45c784075ee28b8f7a702d4", "filename": "gcc/tree-vect-data-refs.c", "status": "added", "additions": 3355, "deletions": 0, "changes": 3355, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vect-data-refs.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vect-data-refs.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-data-refs.c?ref=ebfd146af752ce6aa298f866e36f9a7e66e32ec5"}, {"sha": "22d515fe29f4892ad9847b7cf109c5c02a238383", "filename": "gcc/tree-vect-loop-manip.c", "status": "added", "additions": 2363, "deletions": 0, "changes": 2363, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vect-loop-manip.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vect-loop-manip.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-loop-manip.c?ref=ebfd146af752ce6aa298f866e36f9a7e66e32ec5", "patch": "@@ -0,0 +1,2363 @@\n+/* Vectorizer Specific Loop Manipulations \n+   Copyright (C) 2003, 2004, 2005, 2006, 2007, 2008, 2009 Free Software\n+   Foundation, Inc.\n+   Contributed by Dorit Naishlos <dorit@il.ibm.com> \n+   and Ira Rosen <irar@il.ibm.com>\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"ggc.h\"\n+#include \"tree.h\"\n+#include \"basic-block.h\"\n+#include \"diagnostic.h\"\n+#include \"tree-flow.h\"\n+#include \"tree-dump.h\"\n+#include \"cfgloop.h\"\n+#include \"cfglayout.h\"\n+#include \"expr.h\"\n+#include \"toplev.h\"\n+#include \"tree-scalar-evolution.h\"\n+#include \"tree-vectorizer.h\"\n+#include \"langhooks.h\"\n+\n+/*************************************************************************\n+  Simple Loop Peeling Utilities\n+\n+  Utilities to support loop peeling for vectorization purposes.\n+ *************************************************************************/\n+\n+\n+/* Renames the use *OP_P.  */\n+\n+static void\n+rename_use_op (use_operand_p op_p)\n+{\n+  tree new_name;\n+\n+  if (TREE_CODE (USE_FROM_PTR (op_p)) != SSA_NAME)\n+    return;\n+\n+  new_name = get_current_def (USE_FROM_PTR (op_p));\n+\n+  /* Something defined outside of the loop.  */\n+  if (!new_name)\n+    return;\n+\n+  /* An ordinary ssa name defined in the loop.  */\n+\n+  SET_USE (op_p, new_name);\n+}\n+\n+\n+/* Renames the variables in basic block BB.  */\n+\n+void\n+rename_variables_in_bb (basic_block bb)\n+{\n+  gimple_stmt_iterator gsi;\n+  gimple stmt;\n+  use_operand_p use_p;\n+  ssa_op_iter iter;\n+  edge e;\n+  edge_iterator ei;\n+  struct loop *loop = bb->loop_father;\n+\n+  for (gsi = gsi_start_bb (bb); !gsi_end_p (gsi); gsi_next (&gsi))\n+    {\n+      stmt = gsi_stmt (gsi);\n+      FOR_EACH_SSA_USE_OPERAND (use_p, stmt, iter, SSA_OP_ALL_USES)\n+\trename_use_op (use_p);\n+    }\n+\n+  FOR_EACH_EDGE (e, ei, bb->succs)\n+    {\n+      if (!flow_bb_inside_loop_p (loop, e->dest))\n+\tcontinue;\n+      for (gsi = gsi_start_phis (e->dest); !gsi_end_p (gsi); gsi_next (&gsi))\n+        rename_use_op (PHI_ARG_DEF_PTR_FROM_EDGE (gsi_stmt (gsi), e));\n+    }\n+}\n+\n+\n+/* Renames variables in new generated LOOP.  */\n+\n+void\n+rename_variables_in_loop (struct loop *loop)\n+{\n+  unsigned i;\n+  basic_block *bbs;\n+\n+  bbs = get_loop_body (loop);\n+\n+  for (i = 0; i < loop->num_nodes; i++)\n+    rename_variables_in_bb (bbs[i]);\n+\n+  free (bbs);\n+}\n+\n+\n+/* Update the PHI nodes of NEW_LOOP.\n+\n+   NEW_LOOP is a duplicate of ORIG_LOOP.\n+   AFTER indicates whether NEW_LOOP executes before or after ORIG_LOOP:\n+   AFTER is true if NEW_LOOP executes after ORIG_LOOP, and false if it\n+   executes before it.  */\n+\n+static void\n+slpeel_update_phis_for_duplicate_loop (struct loop *orig_loop,\n+\t\t\t\t       struct loop *new_loop, bool after)\n+{\n+  tree new_ssa_name;\n+  gimple phi_new, phi_orig;\n+  tree def;\n+  edge orig_loop_latch = loop_latch_edge (orig_loop);\n+  edge orig_entry_e = loop_preheader_edge (orig_loop);\n+  edge new_loop_exit_e = single_exit (new_loop);\n+  edge new_loop_entry_e = loop_preheader_edge (new_loop);\n+  edge entry_arg_e = (after ? orig_loop_latch : orig_entry_e);\n+  gimple_stmt_iterator gsi_new, gsi_orig;\n+\n+  /*\n+     step 1. For each loop-header-phi:\n+             Add the first phi argument for the phi in NEW_LOOP\n+            (the one associated with the entry of NEW_LOOP)\n+\n+     step 2. For each loop-header-phi:\n+             Add the second phi argument for the phi in NEW_LOOP\n+            (the one associated with the latch of NEW_LOOP)\n+\n+     step 3. Update the phis in the successor block of NEW_LOOP.\n+\n+        case 1: NEW_LOOP was placed before ORIG_LOOP:\n+                The successor block of NEW_LOOP is the header of ORIG_LOOP.\n+                Updating the phis in the successor block can therefore be done\n+                along with the scanning of the loop header phis, because the\n+                header blocks of ORIG_LOOP and NEW_LOOP have exactly the same\n+                phi nodes, organized in the same order.\n+\n+        case 2: NEW_LOOP was placed after ORIG_LOOP:\n+                The successor block of NEW_LOOP is the original exit block of \n+                ORIG_LOOP - the phis to be updated are the loop-closed-ssa phis.\n+                We postpone updating these phis to a later stage (when\n+                loop guards are added).\n+   */\n+\n+\n+  /* Scan the phis in the headers of the old and new loops\n+     (they are organized in exactly the same order).  */\n+\n+  for (gsi_new = gsi_start_phis (new_loop->header),\n+       gsi_orig = gsi_start_phis (orig_loop->header);\n+       !gsi_end_p (gsi_new) && !gsi_end_p (gsi_orig);\n+       gsi_next (&gsi_new), gsi_next (&gsi_orig))\n+    {\n+      phi_new = gsi_stmt (gsi_new);\n+      phi_orig = gsi_stmt (gsi_orig);\n+\n+      /* step 1.  */\n+      def = PHI_ARG_DEF_FROM_EDGE (phi_orig, entry_arg_e);\n+      add_phi_arg (phi_new, def, new_loop_entry_e);\n+\n+      /* step 2.  */\n+      def = PHI_ARG_DEF_FROM_EDGE (phi_orig, orig_loop_latch);\n+      if (TREE_CODE (def) != SSA_NAME)\n+        continue;\n+\n+      new_ssa_name = get_current_def (def);\n+      if (!new_ssa_name)\n+\t{\n+\t  /* This only happens if there are no definitions\n+\t     inside the loop. use the phi_result in this case.  */\n+\t  new_ssa_name = PHI_RESULT (phi_new);\n+\t}\n+\n+      /* An ordinary ssa name defined in the loop.  */\n+      add_phi_arg (phi_new, new_ssa_name, loop_latch_edge (new_loop));\n+\n+      /* step 3 (case 1).  */\n+      if (!after)\n+        {\n+          gcc_assert (new_loop_exit_e == orig_entry_e);\n+          SET_PHI_ARG_DEF (phi_orig,\n+                           new_loop_exit_e->dest_idx,\n+                           new_ssa_name);\n+        }\n+    }\n+}\n+\n+\n+/* Update PHI nodes for a guard of the LOOP.\n+\n+   Input:\n+   - LOOP, GUARD_EDGE: LOOP is a loop for which we added guard code that\n+        controls whether LOOP is to be executed.  GUARD_EDGE is the edge that\n+        originates from the guard-bb, skips LOOP and reaches the (unique) exit\n+        bb of LOOP.  This loop-exit-bb is an empty bb with one successor.\n+        We denote this bb NEW_MERGE_BB because before the guard code was added\n+        it had a single predecessor (the LOOP header), and now it became a merge\n+        point of two paths - the path that ends with the LOOP exit-edge, and\n+        the path that ends with GUARD_EDGE.\n+   - NEW_EXIT_BB: New basic block that is added by this function between LOOP\n+        and NEW_MERGE_BB. It is used to place loop-closed-ssa-form exit-phis.\n+\n+   ===> The CFG before the guard-code was added:\n+        LOOP_header_bb:\n+          loop_body\n+          if (exit_loop) goto update_bb\n+          else           goto LOOP_header_bb\n+        update_bb:\n+\n+   ==> The CFG after the guard-code was added:\n+        guard_bb:\n+          if (LOOP_guard_condition) goto new_merge_bb\n+          else                      goto LOOP_header_bb\n+        LOOP_header_bb:\n+          loop_body\n+          if (exit_loop_condition) goto new_merge_bb\n+          else                     goto LOOP_header_bb\n+        new_merge_bb:\n+          goto update_bb\n+        update_bb:\n+\n+   ==> The CFG after this function:\n+        guard_bb:\n+          if (LOOP_guard_condition) goto new_merge_bb\n+          else                      goto LOOP_header_bb\n+        LOOP_header_bb:\n+          loop_body\n+          if (exit_loop_condition) goto new_exit_bb\n+          else                     goto LOOP_header_bb\n+        new_exit_bb:\n+        new_merge_bb:\n+          goto update_bb\n+        update_bb:\n+\n+   This function:\n+   1. creates and updates the relevant phi nodes to account for the new\n+      incoming edge (GUARD_EDGE) into NEW_MERGE_BB. This involves:\n+      1.1. Create phi nodes at NEW_MERGE_BB.\n+      1.2. Update the phi nodes at the successor of NEW_MERGE_BB (denoted\n+           UPDATE_BB).  UPDATE_BB was the exit-bb of LOOP before NEW_MERGE_BB\n+   2. preserves loop-closed-ssa-form by creating the required phi nodes\n+      at the exit of LOOP (i.e, in NEW_EXIT_BB).\n+\n+   There are two flavors to this function:\n+\n+   slpeel_update_phi_nodes_for_guard1:\n+     Here the guard controls whether we enter or skip LOOP, where LOOP is a\n+     prolog_loop (loop1 below), and the new phis created in NEW_MERGE_BB are\n+     for variables that have phis in the loop header.\n+\n+   slpeel_update_phi_nodes_for_guard2:\n+     Here the guard controls whether we enter or skip LOOP, where LOOP is an\n+     epilog_loop (loop2 below), and the new phis created in NEW_MERGE_BB are\n+     for variables that have phis in the loop exit.\n+\n+   I.E., the overall structure is:\n+\n+        loop1_preheader_bb:\n+                guard1 (goto loop1/merge1_bb)\n+        loop1\n+        loop1_exit_bb:\n+                guard2 (goto merge1_bb/merge2_bb)\n+        merge1_bb\n+        loop2\n+        loop2_exit_bb\n+        merge2_bb\n+        next_bb\n+\n+   slpeel_update_phi_nodes_for_guard1 takes care of creating phis in\n+   loop1_exit_bb and merge1_bb. These are entry phis (phis for the vars\n+   that have phis in loop1->header).\n+\n+   slpeel_update_phi_nodes_for_guard2 takes care of creating phis in\n+   loop2_exit_bb and merge2_bb. These are exit phis (phis for the vars\n+   that have phis in next_bb). It also adds some of these phis to\n+   loop1_exit_bb.\n+\n+   slpeel_update_phi_nodes_for_guard1 is always called before\n+   slpeel_update_phi_nodes_for_guard2. They are both needed in order\n+   to create correct data-flow and loop-closed-ssa-form.\n+\n+   Generally slpeel_update_phi_nodes_for_guard1 creates phis for variables\n+   that change between iterations of a loop (and therefore have a phi-node\n+   at the loop entry), whereas slpeel_update_phi_nodes_for_guard2 creates\n+   phis for variables that are used out of the loop (and therefore have \n+   loop-closed exit phis). Some variables may be both updated between \n+   iterations and used after the loop. This is why in loop1_exit_bb we\n+   may need both entry_phis (created by slpeel_update_phi_nodes_for_guard1)\n+   and exit phis (created by slpeel_update_phi_nodes_for_guard2).\n+\n+   - IS_NEW_LOOP: if IS_NEW_LOOP is true, then LOOP is a newly created copy of\n+     an original loop. i.e., we have:\n+\n+           orig_loop\n+           guard_bb (goto LOOP/new_merge)\n+           new_loop <-- LOOP\n+           new_exit\n+           new_merge\n+           next_bb\n+\n+     If IS_NEW_LOOP is false, then LOOP is an original loop, in which case we\n+     have:\n+\n+           new_loop\n+           guard_bb (goto LOOP/new_merge)\n+           orig_loop <-- LOOP\n+           new_exit\n+           new_merge\n+           next_bb\n+\n+     The SSA names defined in the original loop have a current\n+     reaching definition that that records the corresponding new\n+     ssa-name used in the new duplicated loop copy.\n+  */\n+\n+/* Function slpeel_update_phi_nodes_for_guard1\n+   \n+   Input:\n+   - GUARD_EDGE, LOOP, IS_NEW_LOOP, NEW_EXIT_BB - as explained above.\n+   - DEFS - a bitmap of ssa names to mark new names for which we recorded\n+            information. \n+   \n+   In the context of the overall structure, we have:\n+\n+        loop1_preheader_bb: \n+                guard1 (goto loop1/merge1_bb)\n+LOOP->  loop1\n+        loop1_exit_bb:\n+                guard2 (goto merge1_bb/merge2_bb)\n+        merge1_bb\n+        loop2\n+        loop2_exit_bb\n+        merge2_bb\n+        next_bb\n+\n+   For each name updated between loop iterations (i.e - for each name that has\n+   an entry (loop-header) phi in LOOP) we create a new phi in:\n+   1. merge1_bb (to account for the edge from guard1)\n+   2. loop1_exit_bb (an exit-phi to keep LOOP in loop-closed form)\n+*/\n+\n+static void\n+slpeel_update_phi_nodes_for_guard1 (edge guard_edge, struct loop *loop,\n+                                    bool is_new_loop, basic_block *new_exit_bb,\n+                                    bitmap *defs)\n+{\n+  gimple orig_phi, new_phi;\n+  gimple update_phi, update_phi2;\n+  tree guard_arg, loop_arg;\n+  basic_block new_merge_bb = guard_edge->dest;\n+  edge e = EDGE_SUCC (new_merge_bb, 0);\n+  basic_block update_bb = e->dest;\n+  basic_block orig_bb = loop->header;\n+  edge new_exit_e;\n+  tree current_new_name;\n+  tree name;\n+  gimple_stmt_iterator gsi_orig, gsi_update;\n+\n+  /* Create new bb between loop and new_merge_bb.  */\n+  *new_exit_bb = split_edge (single_exit (loop));\n+\n+  new_exit_e = EDGE_SUCC (*new_exit_bb, 0);\n+\n+  for (gsi_orig = gsi_start_phis (orig_bb),\n+       gsi_update = gsi_start_phis (update_bb);\n+       !gsi_end_p (gsi_orig) && !gsi_end_p (gsi_update);\n+       gsi_next (&gsi_orig), gsi_next (&gsi_update))\n+    {\n+      orig_phi = gsi_stmt (gsi_orig);\n+      update_phi = gsi_stmt (gsi_update);\n+\n+      /* Virtual phi; Mark it for renaming. We actually want to call\n+\t mar_sym_for_renaming, but since all ssa renaming datastructures\n+\t are going to be freed before we get to call ssa_update, we just\n+\t record this name for now in a bitmap, and will mark it for\n+\t renaming later.  */\n+      name = PHI_RESULT (orig_phi);\n+      if (!is_gimple_reg (SSA_NAME_VAR (name)))\n+        bitmap_set_bit (vect_memsyms_to_rename, DECL_UID (SSA_NAME_VAR (name)));\n+\n+      /** 1. Handle new-merge-point phis  **/\n+\n+      /* 1.1. Generate new phi node in NEW_MERGE_BB:  */\n+      new_phi = create_phi_node (SSA_NAME_VAR (PHI_RESULT (orig_phi)),\n+                                 new_merge_bb);\n+\n+      /* 1.2. NEW_MERGE_BB has two incoming edges: GUARD_EDGE and the exit-edge\n+            of LOOP. Set the two phi args in NEW_PHI for these edges:  */\n+      loop_arg = PHI_ARG_DEF_FROM_EDGE (orig_phi, EDGE_SUCC (loop->latch, 0));\n+      guard_arg = PHI_ARG_DEF_FROM_EDGE (orig_phi, loop_preheader_edge (loop));\n+\n+      add_phi_arg (new_phi, loop_arg, new_exit_e);\n+      add_phi_arg (new_phi, guard_arg, guard_edge);\n+\n+      /* 1.3. Update phi in successor block.  */\n+      gcc_assert (PHI_ARG_DEF_FROM_EDGE (update_phi, e) == loop_arg\n+                  || PHI_ARG_DEF_FROM_EDGE (update_phi, e) == guard_arg);\n+      SET_PHI_ARG_DEF (update_phi, e->dest_idx, PHI_RESULT (new_phi));\n+      update_phi2 = new_phi;\n+\n+\n+      /** 2. Handle loop-closed-ssa-form phis  **/\n+\n+      if (!is_gimple_reg (PHI_RESULT (orig_phi)))\n+\tcontinue;\n+\n+      /* 2.1. Generate new phi node in NEW_EXIT_BB:  */\n+      new_phi = create_phi_node (SSA_NAME_VAR (PHI_RESULT (orig_phi)),\n+                                 *new_exit_bb);\n+\n+      /* 2.2. NEW_EXIT_BB has one incoming edge: the exit-edge of the loop.  */\n+      add_phi_arg (new_phi, loop_arg, single_exit (loop));\n+\n+      /* 2.3. Update phi in successor of NEW_EXIT_BB:  */\n+      gcc_assert (PHI_ARG_DEF_FROM_EDGE (update_phi2, new_exit_e) == loop_arg);\n+      SET_PHI_ARG_DEF (update_phi2, new_exit_e->dest_idx, PHI_RESULT (new_phi));\n+\n+      /* 2.4. Record the newly created name with set_current_def.\n+         We want to find a name such that\n+                name = get_current_def (orig_loop_name)\n+         and to set its current definition as follows:\n+                set_current_def (name, new_phi_name)\n+\n+         If LOOP is a new loop then loop_arg is already the name we're\n+         looking for. If LOOP is the original loop, then loop_arg is\n+         the orig_loop_name and the relevant name is recorded in its\n+         current reaching definition.  */\n+      if (is_new_loop)\n+        current_new_name = loop_arg;\n+      else\n+        {\n+          current_new_name = get_current_def (loop_arg);\n+\t  /* current_def is not available only if the variable does not\n+\t     change inside the loop, in which case we also don't care\n+\t     about recording a current_def for it because we won't be\n+\t     trying to create loop-exit-phis for it.  */\n+\t  if (!current_new_name)\n+\t    continue;\n+        }\n+      gcc_assert (get_current_def (current_new_name) == NULL_TREE);\n+\n+      set_current_def (current_new_name, PHI_RESULT (new_phi));\n+      bitmap_set_bit (*defs, SSA_NAME_VERSION (current_new_name));\n+    }\n+}\n+\n+\n+/* Function slpeel_update_phi_nodes_for_guard2\n+\n+   Input:\n+   - GUARD_EDGE, LOOP, IS_NEW_LOOP, NEW_EXIT_BB - as explained above.\n+\n+   In the context of the overall structure, we have:\n+\n+        loop1_preheader_bb: \n+                guard1 (goto loop1/merge1_bb)\n+        loop1\n+        loop1_exit_bb: \n+                guard2 (goto merge1_bb/merge2_bb)\n+        merge1_bb\n+LOOP->  loop2\n+        loop2_exit_bb\n+        merge2_bb\n+        next_bb\n+\n+   For each name used out side the loop (i.e - for each name that has an exit\n+   phi in next_bb) we create a new phi in:\n+   1. merge2_bb (to account for the edge from guard_bb) \n+   2. loop2_exit_bb (an exit-phi to keep LOOP in loop-closed form)\n+   3. guard2 bb (an exit phi to keep the preceding loop in loop-closed form),\n+      if needed (if it wasn't handled by slpeel_update_phis_nodes_for_phi1).\n+*/\n+\n+static void\n+slpeel_update_phi_nodes_for_guard2 (edge guard_edge, struct loop *loop,\n+                                    bool is_new_loop, basic_block *new_exit_bb)\n+{\n+  gimple orig_phi, new_phi;\n+  gimple update_phi, update_phi2;\n+  tree guard_arg, loop_arg;\n+  basic_block new_merge_bb = guard_edge->dest;\n+  edge e = EDGE_SUCC (new_merge_bb, 0);\n+  basic_block update_bb = e->dest;\n+  edge new_exit_e;\n+  tree orig_def, orig_def_new_name;\n+  tree new_name, new_name2;\n+  tree arg;\n+  gimple_stmt_iterator gsi;\n+\n+  /* Create new bb between loop and new_merge_bb.  */\n+  *new_exit_bb = split_edge (single_exit (loop));\n+\n+  new_exit_e = EDGE_SUCC (*new_exit_bb, 0);\n+\n+  for (gsi = gsi_start_phis (update_bb); !gsi_end_p (gsi); gsi_next (&gsi))\n+    {\n+      update_phi = gsi_stmt (gsi);\n+      orig_phi = update_phi;\n+      orig_def = PHI_ARG_DEF_FROM_EDGE (orig_phi, e);\n+      /* This loop-closed-phi actually doesn't represent a use\n+         out of the loop - the phi arg is a constant.  */ \n+      if (TREE_CODE (orig_def) != SSA_NAME)\n+        continue;\n+      orig_def_new_name = get_current_def (orig_def);\n+      arg = NULL_TREE;\n+\n+      /** 1. Handle new-merge-point phis  **/\n+\n+      /* 1.1. Generate new phi node in NEW_MERGE_BB:  */\n+      new_phi = create_phi_node (SSA_NAME_VAR (PHI_RESULT (orig_phi)),\n+                                 new_merge_bb);\n+\n+      /* 1.2. NEW_MERGE_BB has two incoming edges: GUARD_EDGE and the exit-edge\n+            of LOOP. Set the two PHI args in NEW_PHI for these edges:  */\n+      new_name = orig_def;\n+      new_name2 = NULL_TREE;\n+      if (orig_def_new_name)\n+        {\n+          new_name = orig_def_new_name;\n+\t  /* Some variables have both loop-entry-phis and loop-exit-phis.\n+\t     Such variables were given yet newer names by phis placed in\n+\t     guard_bb by slpeel_update_phi_nodes_for_guard1. I.e:\n+\t     new_name2 = get_current_def (get_current_def (orig_name)).  */\n+          new_name2 = get_current_def (new_name);\n+        }\n+  \n+      if (is_new_loop)\n+        {\n+          guard_arg = orig_def;\n+          loop_arg = new_name;\n+        }\n+      else\n+        {\n+          guard_arg = new_name;\n+          loop_arg = orig_def;\n+        }\n+      if (new_name2)\n+        guard_arg = new_name2;\n+  \n+      add_phi_arg (new_phi, loop_arg, new_exit_e);\n+      add_phi_arg (new_phi, guard_arg, guard_edge);\n+\n+      /* 1.3. Update phi in successor block.  */\n+      gcc_assert (PHI_ARG_DEF_FROM_EDGE (update_phi, e) == orig_def);\n+      SET_PHI_ARG_DEF (update_phi, e->dest_idx, PHI_RESULT (new_phi));\n+      update_phi2 = new_phi;\n+\n+\n+      /** 2. Handle loop-closed-ssa-form phis  **/\n+\n+      /* 2.1. Generate new phi node in NEW_EXIT_BB:  */\n+      new_phi = create_phi_node (SSA_NAME_VAR (PHI_RESULT (orig_phi)),\n+                                 *new_exit_bb);\n+\n+      /* 2.2. NEW_EXIT_BB has one incoming edge: the exit-edge of the loop.  */\n+      add_phi_arg (new_phi, loop_arg, single_exit (loop));\n+\n+      /* 2.3. Update phi in successor of NEW_EXIT_BB:  */\n+      gcc_assert (PHI_ARG_DEF_FROM_EDGE (update_phi2, new_exit_e) == loop_arg);\n+      SET_PHI_ARG_DEF (update_phi2, new_exit_e->dest_idx, PHI_RESULT (new_phi));\n+\n+\n+      /** 3. Handle loop-closed-ssa-form phis for first loop  **/\n+\n+      /* 3.1. Find the relevant names that need an exit-phi in\n+\t GUARD_BB, i.e. names for which\n+\t slpeel_update_phi_nodes_for_guard1 had not already created a\n+\t phi node. This is the case for names that are used outside\n+\t the loop (and therefore need an exit phi) but are not updated\n+\t across loop iterations (and therefore don't have a\n+\t loop-header-phi).\n+\n+\t slpeel_update_phi_nodes_for_guard1 is responsible for\n+\t creating loop-exit phis in GUARD_BB for names that have a\n+\t loop-header-phi.  When such a phi is created we also record\n+\t the new name in its current definition.  If this new name\n+\t exists, then guard_arg was set to this new name (see 1.2\n+\t above).  Therefore, if guard_arg is not this new name, this\n+\t is an indication that an exit-phi in GUARD_BB was not yet\n+\t created, so we take care of it here.  */\n+      if (guard_arg == new_name2)\n+\tcontinue;\n+      arg = guard_arg;\n+\n+      /* 3.2. Generate new phi node in GUARD_BB:  */\n+      new_phi = create_phi_node (SSA_NAME_VAR (PHI_RESULT (orig_phi)),\n+                                 guard_edge->src);\n+\n+      /* 3.3. GUARD_BB has one incoming edge:  */\n+      gcc_assert (EDGE_COUNT (guard_edge->src->preds) == 1);\n+      add_phi_arg (new_phi, arg, EDGE_PRED (guard_edge->src, 0));\n+\n+      /* 3.4. Update phi in successor of GUARD_BB:  */\n+      gcc_assert (PHI_ARG_DEF_FROM_EDGE (update_phi2, guard_edge)\n+                                                                == guard_arg);\n+      SET_PHI_ARG_DEF (update_phi2, guard_edge->dest_idx, PHI_RESULT (new_phi));\n+    }\n+}\n+\n+\n+/* Make the LOOP iterate NITERS times. This is done by adding a new IV\n+   that starts at zero, increases by one and its limit is NITERS.\n+\n+   Assumption: the exit-condition of LOOP is the last stmt in the loop.  */\n+\n+void\n+slpeel_make_loop_iterate_ntimes (struct loop *loop, tree niters)\n+{\n+  tree indx_before_incr, indx_after_incr;\n+  gimple cond_stmt;\n+  gimple orig_cond;\n+  edge exit_edge = single_exit (loop);\n+  gimple_stmt_iterator loop_cond_gsi;\n+  gimple_stmt_iterator incr_gsi;\n+  bool insert_after;\n+  tree init = build_int_cst (TREE_TYPE (niters), 0);\n+  tree step = build_int_cst (TREE_TYPE (niters), 1);\n+  LOC loop_loc;\n+  enum tree_code code;\n+\n+  orig_cond = get_loop_exit_condition (loop);\n+  gcc_assert (orig_cond);\n+  loop_cond_gsi = gsi_for_stmt (orig_cond);\n+\n+  standard_iv_increment_position (loop, &incr_gsi, &insert_after);\n+  create_iv (init, step, NULL_TREE, loop,\n+             &incr_gsi, insert_after, &indx_before_incr, &indx_after_incr);\n+\n+  indx_after_incr = force_gimple_operand_gsi (&loop_cond_gsi, indx_after_incr,\n+\t\t\t\t\t      true, NULL_TREE, true,\n+\t\t\t\t\t      GSI_SAME_STMT);\n+  niters = force_gimple_operand_gsi (&loop_cond_gsi, niters, true, NULL_TREE,\n+\t\t\t\t     true, GSI_SAME_STMT);\n+\n+  code = (exit_edge->flags & EDGE_TRUE_VALUE) ? GE_EXPR : LT_EXPR;\n+  cond_stmt = gimple_build_cond (code, indx_after_incr, niters, NULL_TREE,\n+\t\t\t\t NULL_TREE);\n+\n+  gsi_insert_before (&loop_cond_gsi, cond_stmt, GSI_SAME_STMT);\n+\n+  /* Remove old loop exit test:  */\n+  gsi_remove (&loop_cond_gsi, true);\n+\n+  loop_loc = find_loop_location (loop);\n+  if (dump_file && (dump_flags & TDF_DETAILS))\n+    {\n+      if (loop_loc != UNKNOWN_LOC)\n+        fprintf (dump_file, \"\\nloop at %s:%d: \",\n+                 LOC_FILE (loop_loc), LOC_LINE (loop_loc));\n+      print_gimple_stmt (dump_file, cond_stmt, 0, TDF_SLIM);\n+    }\n+\n+  loop->nb_iterations = niters;\n+}\n+\n+\n+/* Given LOOP this function generates a new copy of it and puts it \n+   on E which is either the entry or exit of LOOP.  */\n+\n+struct loop *\n+slpeel_tree_duplicate_loop_to_edge_cfg (struct loop *loop, edge e)\n+{\n+  struct loop *new_loop;\n+  basic_block *new_bbs, *bbs;\n+  bool at_exit;\n+  bool was_imm_dom;\n+  basic_block exit_dest; \n+  gimple phi;\n+  tree phi_arg;\n+  edge exit, new_exit;\n+  gimple_stmt_iterator gsi;\n+\n+  at_exit = (e == single_exit (loop)); \n+  if (!at_exit && e != loop_preheader_edge (loop))\n+    return NULL;\n+\n+  bbs = get_loop_body (loop);\n+\n+  /* Check whether duplication is possible.  */\n+  if (!can_copy_bbs_p (bbs, loop->num_nodes))\n+    {\n+      free (bbs);\n+      return NULL;\n+    }\n+\n+  /* Generate new loop structure.  */\n+  new_loop = duplicate_loop (loop, loop_outer (loop));\n+  if (!new_loop)\n+    {\n+      free (bbs);\n+      return NULL;\n+    }\n+\n+  exit_dest = single_exit (loop)->dest;\n+  was_imm_dom = (get_immediate_dominator (CDI_DOMINATORS, \n+\t\t\t\t\t  exit_dest) == loop->header ? \n+\t\t true : false);\n+\n+  new_bbs = XNEWVEC (basic_block, loop->num_nodes);\n+\n+  exit = single_exit (loop);\n+  copy_bbs (bbs, loop->num_nodes, new_bbs,\n+\t    &exit, 1, &new_exit, NULL,\n+\t    e->src);\n+\n+  /* Duplicating phi args at exit bbs as coming \n+     also from exit of duplicated loop.  */\n+  for (gsi = gsi_start_phis (exit_dest); !gsi_end_p (gsi); gsi_next (&gsi))\n+    {\n+      phi = gsi_stmt (gsi);\n+      phi_arg = PHI_ARG_DEF_FROM_EDGE (phi, single_exit (loop));\n+      if (phi_arg)\n+\t{\n+\t  edge new_loop_exit_edge;\n+\n+\t  if (EDGE_SUCC (new_loop->header, 0)->dest == new_loop->latch)\n+\t    new_loop_exit_edge = EDGE_SUCC (new_loop->header, 1);\n+\t  else\n+\t    new_loop_exit_edge = EDGE_SUCC (new_loop->header, 0);\n+  \n+\t  add_phi_arg (phi, phi_arg, new_loop_exit_edge);\t\n+\t}\n+    }    \n+   \n+  if (at_exit) /* Add the loop copy at exit.  */\n+    {\n+      redirect_edge_and_branch_force (e, new_loop->header);\n+      PENDING_STMT (e) = NULL;\n+      set_immediate_dominator (CDI_DOMINATORS, new_loop->header, e->src);\n+      if (was_imm_dom)\n+\tset_immediate_dominator (CDI_DOMINATORS, exit_dest, new_loop->header);\n+    }\n+  else /* Add the copy at entry.  */\n+    {\n+      edge new_exit_e;\n+      edge entry_e = loop_preheader_edge (loop);\n+      basic_block preheader = entry_e->src;\n+           \n+      if (!flow_bb_inside_loop_p (new_loop, \n+\t\t\t\t  EDGE_SUCC (new_loop->header, 0)->dest))\n+        new_exit_e = EDGE_SUCC (new_loop->header, 0);\n+      else\n+\tnew_exit_e = EDGE_SUCC (new_loop->header, 1); \n+\n+      redirect_edge_and_branch_force (new_exit_e, loop->header);\n+      PENDING_STMT (new_exit_e) = NULL;\n+      set_immediate_dominator (CDI_DOMINATORS, loop->header,\n+\t\t\t       new_exit_e->src);\n+\n+      /* We have to add phi args to the loop->header here as coming \n+\t from new_exit_e edge.  */\n+      for (gsi = gsi_start_phis (loop->header);\n+           !gsi_end_p (gsi);\n+           gsi_next (&gsi))\n+\t{\n+\t  phi = gsi_stmt (gsi);\n+\t  phi_arg = PHI_ARG_DEF_FROM_EDGE (phi, entry_e);\n+\t  if (phi_arg)\n+\t    add_phi_arg (phi, phi_arg, new_exit_e);\t\n+\t}    \n+\n+      redirect_edge_and_branch_force (entry_e, new_loop->header);\n+      PENDING_STMT (entry_e) = NULL;\n+      set_immediate_dominator (CDI_DOMINATORS, new_loop->header, preheader);\n+    }\n+\n+  free (new_bbs);\n+  free (bbs);\n+\n+  return new_loop;\n+}\n+\n+\n+/* Given the condition statement COND, put it as the last statement\n+   of GUARD_BB; EXIT_BB is the basic block to skip the loop;\n+   Assumes that this is the single exit of the guarded loop.  \n+   Returns the skip edge.  */\n+\n+static edge\n+slpeel_add_loop_guard (basic_block guard_bb, tree cond, basic_block exit_bb,\n+\t\t       basic_block dom_bb)\n+{\n+  gimple_stmt_iterator gsi;\n+  edge new_e, enter_e;\n+  gimple cond_stmt;\n+  gimple_seq gimplify_stmt_list = NULL;\n+\n+  enter_e = EDGE_SUCC (guard_bb, 0);\n+  enter_e->flags &= ~EDGE_FALLTHRU;\n+  enter_e->flags |= EDGE_FALSE_VALUE;\n+  gsi = gsi_last_bb (guard_bb);\n+\n+  cond = force_gimple_operand (cond, &gimplify_stmt_list, true, NULL_TREE);\n+  cond_stmt = gimple_build_cond (NE_EXPR,\n+\t\t\t\t cond, build_int_cst (TREE_TYPE (cond), 0),\n+\t\t\t\t NULL_TREE, NULL_TREE);\n+  if (gimplify_stmt_list)\n+    gsi_insert_seq_after (&gsi, gimplify_stmt_list, GSI_NEW_STMT);\n+\n+  gsi = gsi_last_bb (guard_bb);\n+  gsi_insert_after (&gsi, cond_stmt, GSI_NEW_STMT);\n+\n+  /* Add new edge to connect guard block to the merge/loop-exit block.  */\n+  new_e = make_edge (guard_bb, exit_bb, EDGE_TRUE_VALUE);\n+  set_immediate_dominator (CDI_DOMINATORS, exit_bb, dom_bb);\n+  return new_e;\n+}\n+\n+\n+/* This function verifies that the following restrictions apply to LOOP:\n+   (1) it is innermost\n+   (2) it consists of exactly 2 basic blocks - header, and an empty latch.\n+   (3) it is single entry, single exit\n+   (4) its exit condition is the last stmt in the header\n+   (5) E is the entry/exit edge of LOOP.\n+ */\n+\n+bool\n+slpeel_can_duplicate_loop_p (const struct loop *loop, const_edge e)\n+{\n+  edge exit_e = single_exit (loop);\n+  edge entry_e = loop_preheader_edge (loop);\n+  gimple orig_cond = get_loop_exit_condition (loop);\n+  gimple_stmt_iterator loop_exit_gsi = gsi_last_bb (exit_e->src);\n+\n+  if (need_ssa_update_p ())\n+    return false;\n+\n+  if (loop->inner\n+      /* All loops have an outer scope; the only case loop->outer is NULL is for\n+         the function itself.  */\n+      || !loop_outer (loop)\n+      || loop->num_nodes != 2\n+      || !empty_block_p (loop->latch)\n+      || !single_exit (loop)\n+      /* Verify that new loop exit condition can be trivially modified.  */\n+      || (!orig_cond || orig_cond != gsi_stmt (loop_exit_gsi))\n+      || (e != exit_e && e != entry_e))\n+    return false;\n+\n+  return true;\n+}\n+\n+#ifdef ENABLE_CHECKING\n+static void\n+slpeel_verify_cfg_after_peeling (struct loop *first_loop,\n+                                 struct loop *second_loop)\n+{\n+  basic_block loop1_exit_bb = single_exit (first_loop)->dest;\n+  basic_block loop2_entry_bb = loop_preheader_edge (second_loop)->src;\n+  basic_block loop1_entry_bb = loop_preheader_edge (first_loop)->src;\n+\n+  /* A guard that controls whether the second_loop is to be executed or skipped\n+     is placed in first_loop->exit.  first_loop->exit therefore has two\n+     successors - one is the preheader of second_loop, and the other is a bb\n+     after second_loop.\n+   */\n+  gcc_assert (EDGE_COUNT (loop1_exit_bb->succs) == 2);\n+   \n+  /* 1. Verify that one of the successors of first_loop->exit is the preheader\n+        of second_loop.  */\n+   \n+  /* The preheader of new_loop is expected to have two predecessors:\n+     first_loop->exit and the block that precedes first_loop.  */\n+\n+  gcc_assert (EDGE_COUNT (loop2_entry_bb->preds) == 2 \n+              && ((EDGE_PRED (loop2_entry_bb, 0)->src == loop1_exit_bb\n+                   && EDGE_PRED (loop2_entry_bb, 1)->src == loop1_entry_bb)\n+               || (EDGE_PRED (loop2_entry_bb, 1)->src ==  loop1_exit_bb\n+                   && EDGE_PRED (loop2_entry_bb, 0)->src == loop1_entry_bb)));\n+  \n+  /* Verify that the other successor of first_loop->exit is after the\n+     second_loop.  */\n+  /* TODO */\n+}\n+#endif\n+\n+/* If the run time cost model check determines that vectorization is\n+   not profitable and hence scalar loop should be generated then set\n+   FIRST_NITERS to prologue peeled iterations. This will allow all the\n+   iterations to be executed in the prologue peeled scalar loop.  */\n+\n+static void\n+set_prologue_iterations (basic_block bb_before_first_loop,\n+\t\t\t tree first_niters,\n+\t\t\t struct loop *loop,\n+\t\t\t unsigned int th)\n+{\n+  edge e;\n+  basic_block cond_bb, then_bb;\n+  tree var, prologue_after_cost_adjust_name;\n+  gimple_stmt_iterator gsi;\n+  gimple newphi;\n+  edge e_true, e_false, e_fallthru;\n+  gimple cond_stmt;\n+  gimple_seq gimplify_stmt_list = NULL, stmts = NULL;\n+  tree cost_pre_condition = NULL_TREE;\n+  tree scalar_loop_iters = \n+    unshare_expr (LOOP_VINFO_NITERS_UNCHANGED (loop_vec_info_for_loop (loop)));\n+\n+  e = single_pred_edge (bb_before_first_loop);\n+  cond_bb = split_edge(e);\n+\n+  e = single_pred_edge (bb_before_first_loop);\n+  then_bb = split_edge(e);\n+  set_immediate_dominator (CDI_DOMINATORS, then_bb, cond_bb);\n+\n+  e_false = make_single_succ_edge (cond_bb, bb_before_first_loop,\n+\t\t\t\t   EDGE_FALSE_VALUE);\n+  set_immediate_dominator (CDI_DOMINATORS, bb_before_first_loop, cond_bb);\n+\n+  e_true = EDGE_PRED (then_bb, 0);\n+  e_true->flags &= ~EDGE_FALLTHRU;\n+  e_true->flags |= EDGE_TRUE_VALUE;\n+\n+  e_fallthru = EDGE_SUCC (then_bb, 0);\n+\n+  cost_pre_condition =\n+    fold_build2 (LE_EXPR, boolean_type_node, scalar_loop_iters, \n+\t         build_int_cst (TREE_TYPE (scalar_loop_iters), th));\n+  cost_pre_condition =\n+    force_gimple_operand (cost_pre_condition, &gimplify_stmt_list,\n+\t\t\t  true, NULL_TREE);\n+  cond_stmt = gimple_build_cond (NE_EXPR, cost_pre_condition,\n+\t\t\t\t build_int_cst (TREE_TYPE (cost_pre_condition),\n+\t\t\t\t\t\t0), NULL_TREE, NULL_TREE);\n+\n+  gsi = gsi_last_bb (cond_bb);\n+  if (gimplify_stmt_list)\n+    gsi_insert_seq_after (&gsi, gimplify_stmt_list, GSI_NEW_STMT);\n+\n+  gsi = gsi_last_bb (cond_bb);\n+  gsi_insert_after (&gsi, cond_stmt, GSI_NEW_STMT);\n+\t\t\t\t\t  \n+  var = create_tmp_var (TREE_TYPE (scalar_loop_iters),\n+\t\t\t\"prologue_after_cost_adjust\");\n+  add_referenced_var (var);\n+  prologue_after_cost_adjust_name = \n+    force_gimple_operand (scalar_loop_iters, &stmts, false, var);\n+\n+  gsi = gsi_last_bb (then_bb);\n+  if (stmts)\n+    gsi_insert_seq_after (&gsi, stmts, GSI_NEW_STMT);\n+\n+  newphi = create_phi_node (var, bb_before_first_loop);\n+  add_phi_arg (newphi, prologue_after_cost_adjust_name, e_fallthru);\n+  add_phi_arg (newphi, first_niters, e_false);\n+\n+  first_niters = PHI_RESULT (newphi);\n+}\n+\n+\n+/* Function slpeel_tree_peel_loop_to_edge.\n+\n+   Peel the first (last) iterations of LOOP into a new prolog (epilog) loop\n+   that is placed on the entry (exit) edge E of LOOP. After this transformation\n+   we have two loops one after the other - first-loop iterates FIRST_NITERS\n+   times, and second-loop iterates the remainder NITERS - FIRST_NITERS times.\n+   If the cost model indicates that it is profitable to emit a scalar \n+   loop instead of the vector one, then the prolog (epilog) loop will iterate\n+   for the entire unchanged scalar iterations of the loop.\n+\n+   Input:\n+   - LOOP: the loop to be peeled.\n+   - E: the exit or entry edge of LOOP.\n+        If it is the entry edge, we peel the first iterations of LOOP. In this\n+        case first-loop is LOOP, and second-loop is the newly created loop.\n+        If it is the exit edge, we peel the last iterations of LOOP. In this\n+        case, first-loop is the newly created loop, and second-loop is LOOP.\n+   - NITERS: the number of iterations that LOOP iterates.\n+   - FIRST_NITERS: the number of iterations that the first-loop should iterate.\n+   - UPDATE_FIRST_LOOP_COUNT:  specified whether this function is responsible\n+        for updating the loop bound of the first-loop to FIRST_NITERS.  If it\n+        is false, the caller of this function may want to take care of this\n+        (this can be useful if we don't want new stmts added to first-loop).\n+   - TH: cost model profitability threshold of iterations for vectorization.\n+   - CHECK_PROFITABILITY: specify whether cost model check has not occurred\n+                          during versioning and hence needs to occur during\n+\t\t\t  prologue generation or whether cost model check \n+\t\t\t  has not occurred during prologue generation and hence\n+\t\t\t  needs to occur during epilogue generation.\n+\t    \n+\n+   Output:\n+   The function returns a pointer to the new loop-copy, or NULL if it failed\n+   to perform the transformation.\n+\n+   The function generates two if-then-else guards: one before the first loop,\n+   and the other before the second loop:\n+   The first guard is:\n+     if (FIRST_NITERS == 0) then skip the first loop,\n+     and go directly to the second loop.\n+   The second guard is:\n+     if (FIRST_NITERS == NITERS) then skip the second loop.\n+\n+   FORNOW only simple loops are supported (see slpeel_can_duplicate_loop_p).\n+   FORNOW the resulting code will not be in loop-closed-ssa form.\n+*/\n+\n+static struct loop*\n+slpeel_tree_peel_loop_to_edge (struct loop *loop, \n+\t\t\t       edge e, tree first_niters, \n+\t\t\t       tree niters, bool update_first_loop_count,\n+\t\t\t       unsigned int th, bool check_profitability)\n+{\n+  struct loop *new_loop = NULL, *first_loop, *second_loop;\n+  edge skip_e;\n+  tree pre_condition = NULL_TREE;\n+  bitmap definitions;\n+  basic_block bb_before_second_loop, bb_after_second_loop;\n+  basic_block bb_before_first_loop;\n+  basic_block bb_between_loops;\n+  basic_block new_exit_bb;\n+  edge exit_e = single_exit (loop);\n+  LOC loop_loc;\n+  tree cost_pre_condition = NULL_TREE;\n+  \n+  if (!slpeel_can_duplicate_loop_p (loop, e))\n+    return NULL;\n+  \n+  /* We have to initialize cfg_hooks. Then, when calling\n+   cfg_hooks->split_edge, the function tree_split_edge \n+   is actually called and, when calling cfg_hooks->duplicate_block,\n+   the function tree_duplicate_bb is called.  */\n+  gimple_register_cfg_hooks ();\n+\n+\n+  /* 1. Generate a copy of LOOP and put it on E (E is the entry/exit of LOOP).\n+        Resulting CFG would be:\n+\n+        first_loop:\n+        do {\n+        } while ...\n+\n+        second_loop:\n+        do {\n+        } while ...\n+\n+        orig_exit_bb:\n+   */\n+  \n+  if (!(new_loop = slpeel_tree_duplicate_loop_to_edge_cfg (loop, e)))\n+    {\n+      loop_loc = find_loop_location (loop);\n+      if (dump_file && (dump_flags & TDF_DETAILS))\n+        {\n+          if (loop_loc != UNKNOWN_LOC)\n+            fprintf (dump_file, \"\\n%s:%d: note: \",\n+                     LOC_FILE (loop_loc), LOC_LINE (loop_loc));\n+          fprintf (dump_file, \"tree_duplicate_loop_to_edge_cfg failed.\\n\");\n+        }\n+      return NULL;\n+    }\n+  \n+  if (e == exit_e)\n+    {\n+      /* NEW_LOOP was placed after LOOP.  */\n+      first_loop = loop;\n+      second_loop = new_loop;\n+    }\n+  else\n+    {\n+      /* NEW_LOOP was placed before LOOP.  */\n+      first_loop = new_loop;\n+      second_loop = loop;\n+    }\n+\n+  definitions = ssa_names_to_replace ();\n+  slpeel_update_phis_for_duplicate_loop (loop, new_loop, e == exit_e);\n+  rename_variables_in_loop (new_loop);\n+\n+\n+  /* 2.  Add the guard code in one of the following ways:\n+\n+     2.a Add the guard that controls whether the first loop is executed.\n+         This occurs when this function is invoked for prologue or epilogue\n+\t generation and when the cost model check can be done at compile time.\n+\n+         Resulting CFG would be:\n+\n+         bb_before_first_loop:\n+         if (FIRST_NITERS == 0) GOTO bb_before_second_loop\n+                                GOTO first-loop\n+\n+         first_loop:\n+         do {\n+         } while ...\n+\n+         bb_before_second_loop:\n+\n+         second_loop:\n+         do {\n+         } while ...\n+\n+         orig_exit_bb:\n+\n+     2.b Add the cost model check that allows the prologue\n+         to iterate for the entire unchanged scalar\n+         iterations of the loop in the event that the cost\n+         model indicates that the scalar loop is more\n+         profitable than the vector one. This occurs when\n+\t this function is invoked for prologue generation\n+\t and the cost model check needs to be done at run\n+\t time.\n+\n+         Resulting CFG after prologue peeling would be:\n+\n+         if (scalar_loop_iterations <= th)\n+           FIRST_NITERS = scalar_loop_iterations\n+\n+         bb_before_first_loop:\n+         if (FIRST_NITERS == 0) GOTO bb_before_second_loop\n+                                GOTO first-loop\n+\n+         first_loop:\n+         do {\n+         } while ...\n+\n+         bb_before_second_loop:\n+\n+         second_loop:\n+         do {\n+         } while ...\n+\n+         orig_exit_bb:\n+\n+     2.c Add the cost model check that allows the epilogue\n+         to iterate for the entire unchanged scalar\n+         iterations of the loop in the event that the cost\n+         model indicates that the scalar loop is more\n+         profitable than the vector one. This occurs when\n+\t this function is invoked for epilogue generation\n+\t and the cost model check needs to be done at run\n+\t time.\n+\n+         Resulting CFG after prologue peeling would be:\n+\n+         bb_before_first_loop:\n+         if ((scalar_loop_iterations <= th)\n+             ||\n+             FIRST_NITERS == 0) GOTO bb_before_second_loop\n+                                GOTO first-loop\n+\n+         first_loop:\n+         do {\n+         } while ...\n+\n+         bb_before_second_loop:\n+\n+         second_loop:\n+         do {\n+         } while ...\n+\n+         orig_exit_bb:\n+  */\n+\n+  bb_before_first_loop = split_edge (loop_preheader_edge (first_loop));\n+  bb_before_second_loop = split_edge (single_exit (first_loop));\n+\n+  /* Epilogue peeling.  */\n+  if (!update_first_loop_count)\n+    {\n+      pre_condition =\n+\tfold_build2 (LE_EXPR, boolean_type_node, first_niters, \n+\t\t     build_int_cst (TREE_TYPE (first_niters), 0));\n+      if (check_profitability)\n+\t{\n+\t  tree scalar_loop_iters\n+\t    = unshare_expr (LOOP_VINFO_NITERS_UNCHANGED\n+\t\t\t\t\t(loop_vec_info_for_loop (loop)));\n+\t  cost_pre_condition = \n+\t    fold_build2 (LE_EXPR, boolean_type_node, scalar_loop_iters, \n+\t\t         build_int_cst (TREE_TYPE (scalar_loop_iters), th));\n+\n+\t  pre_condition = fold_build2 (TRUTH_OR_EXPR, boolean_type_node,\n+\t\t\t\t       cost_pre_condition, pre_condition);\n+\t}\n+    }\n+\n+  /* Prologue peeling.  */  \n+  else\n+    {\n+      if (check_profitability)\n+\tset_prologue_iterations (bb_before_first_loop, first_niters,\n+\t\t\t\t loop, th);\n+\n+      pre_condition =\n+\tfold_build2 (LE_EXPR, boolean_type_node, first_niters, \n+\t\t     build_int_cst (TREE_TYPE (first_niters), 0));\n+    }\n+\n+  skip_e = slpeel_add_loop_guard (bb_before_first_loop, pre_condition,\n+                                  bb_before_second_loop, bb_before_first_loop);\n+  slpeel_update_phi_nodes_for_guard1 (skip_e, first_loop,\n+\t\t\t\t      first_loop == new_loop,\n+\t\t\t\t      &new_exit_bb, &definitions);\n+\n+\n+  /* 3. Add the guard that controls whether the second loop is executed.\n+        Resulting CFG would be:\n+\n+        bb_before_first_loop:\n+        if (FIRST_NITERS == 0) GOTO bb_before_second_loop (skip first loop)\n+                               GOTO first-loop\n+\n+        first_loop:\n+        do {\n+        } while ...\n+\n+        bb_between_loops:\n+        if (FIRST_NITERS == NITERS) GOTO bb_after_second_loop (skip second loop)\n+                                    GOTO bb_before_second_loop\n+\n+        bb_before_second_loop:\n+\n+        second_loop:\n+        do {\n+        } while ...\n+\n+        bb_after_second_loop:\n+\n+        orig_exit_bb:\n+   */\n+\n+  bb_between_loops = new_exit_bb;\n+  bb_after_second_loop = split_edge (single_exit (second_loop));\n+\n+  pre_condition = \n+\tfold_build2 (EQ_EXPR, boolean_type_node, first_niters, niters);\n+  skip_e = slpeel_add_loop_guard (bb_between_loops, pre_condition,\n+                                  bb_after_second_loop, bb_before_first_loop);\n+  slpeel_update_phi_nodes_for_guard2 (skip_e, second_loop,\n+                                     second_loop == new_loop, &new_exit_bb);\n+\n+  /* 4. Make first-loop iterate FIRST_NITERS times, if requested.\n+   */\n+  if (update_first_loop_count)\n+    slpeel_make_loop_iterate_ntimes (first_loop, first_niters);\n+\n+  BITMAP_FREE (definitions);\n+  delete_update_ssa ();\n+\n+  return new_loop;\n+}\n+\n+/* Function vect_get_loop_location.\n+\n+   Extract the location of the loop in the source code.\n+   If the loop is not well formed for vectorization, an estimated\n+   location is calculated.\n+   Return the loop location if succeed and NULL if not.  */\n+\n+LOC\n+find_loop_location (struct loop *loop)\n+{\n+  gimple stmt = NULL;\n+  basic_block bb;\n+  gimple_stmt_iterator si;\n+\n+  if (!loop)\n+    return UNKNOWN_LOC;\n+\n+  stmt = get_loop_exit_condition (loop);\n+\n+  if (stmt && gimple_location (stmt) != UNKNOWN_LOC)\n+    return gimple_location (stmt);\n+\n+  /* If we got here the loop is probably not \"well formed\",\n+     try to estimate the loop location */\n+\n+  if (!loop->header)\n+    return UNKNOWN_LOC;\n+\n+  bb = loop->header;\n+\n+  for (si = gsi_start_bb (bb); !gsi_end_p (si); gsi_next (&si))\n+    {\n+      stmt = gsi_stmt (si);\n+      if (gimple_location (stmt) != UNKNOWN_LOC)\n+        return gimple_location (stmt);\n+    }\n+\n+  return UNKNOWN_LOC;\n+}\n+\n+\n+/* This function builds ni_name = number of iterations loop executes\n+   on the loop preheader.  */\n+\n+static tree\n+vect_build_loop_niters (loop_vec_info loop_vinfo)\n+{\n+  tree ni_name, var;\n+  gimple_seq stmts = NULL;\n+  edge pe;\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  tree ni = unshare_expr (LOOP_VINFO_NITERS (loop_vinfo));\n+\n+  var = create_tmp_var (TREE_TYPE (ni), \"niters\");\n+  add_referenced_var (var);\n+  ni_name = force_gimple_operand (ni, &stmts, false, var);\n+\n+  pe = loop_preheader_edge (loop);\n+  if (stmts)\n+    {\n+      basic_block new_bb = gsi_insert_seq_on_edge_immediate (pe, stmts);\n+      gcc_assert (!new_bb);\n+    }\n+\n+  return ni_name;\n+}\n+\n+\n+/* This function generates the following statements:\n+\n+ ni_name = number of iterations loop executes\n+ ratio = ni_name / vf\n+ ratio_mult_vf_name = ratio * vf\n+\n+ and places them at the loop preheader edge.  */\n+\n+static void \n+vect_generate_tmps_on_preheader (loop_vec_info loop_vinfo, \n+\t\t\t\t tree *ni_name_ptr,\n+\t\t\t\t tree *ratio_mult_vf_name_ptr, \n+\t\t\t\t tree *ratio_name_ptr)\n+{\n+\n+  edge pe;\n+  basic_block new_bb;\n+  gimple_seq stmts;\n+  tree ni_name;\n+  tree var;\n+  tree ratio_name;\n+  tree ratio_mult_vf_name;\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  tree ni = LOOP_VINFO_NITERS (loop_vinfo);\n+  int vf = LOOP_VINFO_VECT_FACTOR (loop_vinfo);\n+  tree log_vf;\n+\n+  pe = loop_preheader_edge (loop);\n+\n+  /* Generate temporary variable that contains \n+     number of iterations loop executes.  */\n+\n+  ni_name = vect_build_loop_niters (loop_vinfo);\n+  log_vf = build_int_cst (TREE_TYPE (ni), exact_log2 (vf));\n+\n+  /* Create: ratio = ni >> log2(vf) */\n+\n+  ratio_name = fold_build2 (RSHIFT_EXPR, TREE_TYPE (ni_name), ni_name, log_vf);\n+  if (!is_gimple_val (ratio_name))\n+    {\n+      var = create_tmp_var (TREE_TYPE (ni), \"bnd\");\n+      add_referenced_var (var);\n+\n+      stmts = NULL;\n+      ratio_name = force_gimple_operand (ratio_name, &stmts, true, var);\n+      pe = loop_preheader_edge (loop);\n+      new_bb = gsi_insert_seq_on_edge_immediate (pe, stmts);\n+      gcc_assert (!new_bb);\n+    }\n+       \n+  /* Create: ratio_mult_vf = ratio << log2 (vf).  */\n+\n+  ratio_mult_vf_name = fold_build2 (LSHIFT_EXPR, TREE_TYPE (ratio_name),\n+\t\t\t\t    ratio_name, log_vf);\n+  if (!is_gimple_val (ratio_mult_vf_name))\n+    {\n+      var = create_tmp_var (TREE_TYPE (ni), \"ratio_mult_vf\");\n+      add_referenced_var (var);\n+\n+      stmts = NULL;\n+      ratio_mult_vf_name = force_gimple_operand (ratio_mult_vf_name, &stmts,\n+\t\t\t\t\t\t true, var);\n+      pe = loop_preheader_edge (loop);\n+      new_bb = gsi_insert_seq_on_edge_immediate (pe, stmts);\n+      gcc_assert (!new_bb);\n+    }\n+\n+  *ni_name_ptr = ni_name;\n+  *ratio_mult_vf_name_ptr = ratio_mult_vf_name;\n+  *ratio_name_ptr = ratio_name;\n+    \n+  return;  \n+}\n+\n+/* Function vect_can_advance_ivs_p\n+\n+   In case the number of iterations that LOOP iterates is unknown at compile \n+   time, an epilog loop will be generated, and the loop induction variables \n+   (IVs) will be \"advanced\" to the value they are supposed to take just before \n+   the epilog loop.  Here we check that the access function of the loop IVs\n+   and the expression that represents the loop bound are simple enough.\n+   These restrictions will be relaxed in the future.  */\n+\n+bool \n+vect_can_advance_ivs_p (loop_vec_info loop_vinfo)\n+{\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  basic_block bb = loop->header;\n+  gimple phi;\n+  gimple_stmt_iterator gsi;\n+\n+  /* Analyze phi functions of the loop header.  */\n+\n+  if (vect_print_dump_info (REPORT_DETAILS))\n+    fprintf (vect_dump, \"vect_can_advance_ivs_p:\");\n+\n+  for (gsi = gsi_start_phis (bb); !gsi_end_p (gsi); gsi_next (&gsi))\n+    {\n+      tree access_fn = NULL;\n+      tree evolution_part;\n+\n+      phi = gsi_stmt (gsi);\n+      if (vect_print_dump_info (REPORT_DETAILS))\n+\t{\n+          fprintf (vect_dump, \"Analyze phi: \");\n+          print_gimple_stmt (vect_dump, phi, 0, TDF_SLIM);\n+\t}\n+\n+      /* Skip virtual phi's. The data dependences that are associated with\n+         virtual defs/uses (i.e., memory accesses) are analyzed elsewhere.  */\n+\n+      if (!is_gimple_reg (SSA_NAME_VAR (PHI_RESULT (phi))))\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS))\n+\t    fprintf (vect_dump, \"virtual phi. skip.\");\n+\t  continue;\n+\t}\n+\n+      /* Skip reduction phis.  */\n+\n+      if (STMT_VINFO_DEF_TYPE (vinfo_for_stmt (phi)) == vect_reduction_def)\n+        {\n+          if (vect_print_dump_info (REPORT_DETAILS))\n+            fprintf (vect_dump, \"reduc phi. skip.\");\n+          continue;\n+        }\n+\n+      /* Analyze the evolution function.  */\n+\n+      access_fn = instantiate_parameters\n+\t(loop, analyze_scalar_evolution (loop, PHI_RESULT (phi)));\n+\n+      if (!access_fn)\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS))\n+\t    fprintf (vect_dump, \"No Access function.\");\n+\t  return false;\n+\t}\n+\n+      if (vect_print_dump_info (REPORT_DETAILS))\n+        {\n+\t  fprintf (vect_dump, \"Access function of PHI: \");\n+\t  print_generic_expr (vect_dump, access_fn, TDF_SLIM);\n+        }\n+\n+      evolution_part = evolution_part_in_loop_num (access_fn, loop->num);\n+      \n+      if (evolution_part == NULL_TREE)\n+        {\n+\t  if (vect_print_dump_info (REPORT_DETAILS))\n+\t    fprintf (vect_dump, \"No evolution.\");\n+\t  return false;\n+        }\n+  \n+      /* FORNOW: We do not transform initial conditions of IVs \n+\t which evolution functions are a polynomial of degree >= 2.  */\n+\n+      if (tree_is_chrec (evolution_part))\n+\treturn false;  \n+    }\n+\n+  return true;\n+}\n+\n+\n+/*   Function vect_update_ivs_after_vectorizer.\n+\n+     \"Advance\" the induction variables of LOOP to the value they should take\n+     after the execution of LOOP.  This is currently necessary because the\n+     vectorizer does not handle induction variables that are used after the\n+     loop.  Such a situation occurs when the last iterations of LOOP are\n+     peeled, because:\n+     1. We introduced new uses after LOOP for IVs that were not originally used\n+        after LOOP: the IVs of LOOP are now used by an epilog loop.\n+     2. LOOP is going to be vectorized; this means that it will iterate N/VF\n+        times, whereas the loop IVs should be bumped N times.\n+\n+     Input:\n+     - LOOP - a loop that is going to be vectorized. The last few iterations\n+              of LOOP were peeled.\n+     - NITERS - the number of iterations that LOOP executes (before it is\n+                vectorized). i.e, the number of times the ivs should be bumped.\n+     - UPDATE_E - a successor edge of LOOP->exit that is on the (only) path\n+                  coming out from LOOP on which there are uses of the LOOP ivs\n+\t\t  (this is the path from LOOP->exit to epilog_loop->preheader).\n+\n+                  The new definitions of the ivs are placed in LOOP->exit.\n+                  The phi args associated with the edge UPDATE_E in the bb\n+                  UPDATE_E->dest are updated accordingly.\n+\n+     Assumption 1: Like the rest of the vectorizer, this function assumes\n+     a single loop exit that has a single predecessor.\n+\n+     Assumption 2: The phi nodes in the LOOP header and in update_bb are\n+     organized in the same order.\n+\n+     Assumption 3: The access function of the ivs is simple enough (see\n+     vect_can_advance_ivs_p).  This assumption will be relaxed in the future.\n+\n+     Assumption 4: Exactly one of the successors of LOOP exit-bb is on a path\n+     coming out of LOOP on which the ivs of LOOP are used (this is the path \n+     that leads to the epilog loop; other paths skip the epilog loop).  This\n+     path starts with the edge UPDATE_E, and its destination (denoted update_bb)\n+     needs to have its phis updated.\n+ */\n+\n+static void\n+vect_update_ivs_after_vectorizer (loop_vec_info loop_vinfo, tree niters, \n+\t\t\t\t  edge update_e)\n+{\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  basic_block exit_bb = single_exit (loop)->dest;\n+  gimple phi, phi1;\n+  gimple_stmt_iterator gsi, gsi1;\n+  basic_block update_bb = update_e->dest;\n+\n+  /* gcc_assert (vect_can_advance_ivs_p (loop_vinfo)); */\n+\n+  /* Make sure there exists a single-predecessor exit bb:  */\n+  gcc_assert (single_pred_p (exit_bb));\n+\n+  for (gsi = gsi_start_phis (loop->header), gsi1 = gsi_start_phis (update_bb);\n+       !gsi_end_p (gsi) && !gsi_end_p (gsi1);\n+       gsi_next (&gsi), gsi_next (&gsi1))\n+    {\n+      tree access_fn = NULL;\n+      tree evolution_part;\n+      tree init_expr;\n+      tree step_expr;\n+      tree var, ni, ni_name;\n+      gimple_stmt_iterator last_gsi;\n+\n+      phi = gsi_stmt (gsi);\n+      phi1 = gsi_stmt (gsi1);\n+      if (vect_print_dump_info (REPORT_DETAILS))\n+        {\n+          fprintf (vect_dump, \"vect_update_ivs_after_vectorizer: phi: \");\n+\t  print_gimple_stmt (vect_dump, phi, 0, TDF_SLIM);\n+        }\n+\n+      /* Skip virtual phi's.  */\n+      if (!is_gimple_reg (SSA_NAME_VAR (PHI_RESULT (phi))))\n+\t{\n+\t  if (vect_print_dump_info (REPORT_DETAILS))\n+\t    fprintf (vect_dump, \"virtual phi. skip.\");\n+\t  continue;\n+\t}\n+\n+      /* Skip reduction phis.  */\n+      if (STMT_VINFO_DEF_TYPE (vinfo_for_stmt (phi)) == vect_reduction_def)\n+        { \n+          if (vect_print_dump_info (REPORT_DETAILS))\n+            fprintf (vect_dump, \"reduc phi. skip.\");\n+          continue;\n+        } \n+\n+      access_fn = analyze_scalar_evolution (loop, PHI_RESULT (phi)); \n+      gcc_assert (access_fn);\n+      STRIP_NOPS (access_fn);\n+      evolution_part =\n+\t unshare_expr (evolution_part_in_loop_num (access_fn, loop->num));\n+      gcc_assert (evolution_part != NULL_TREE);\n+      \n+      /* FORNOW: We do not support IVs whose evolution function is a polynomial\n+         of degree >= 2 or exponential.  */\n+      gcc_assert (!tree_is_chrec (evolution_part));\n+\n+      step_expr = evolution_part;\n+      init_expr = unshare_expr (initial_condition_in_loop_num (access_fn, \n+\t\t\t\t\t\t\t       loop->num));\n+\n+      if (POINTER_TYPE_P (TREE_TYPE (init_expr)))\n+\tni = fold_build2 (POINTER_PLUS_EXPR, TREE_TYPE (init_expr), \n+\t\t\t  init_expr, \n+\t\t\t  fold_convert (sizetype, \n+\t\t\t\t\tfold_build2 (MULT_EXPR, TREE_TYPE (niters),\n+\t\t\t\t\t\t     niters, step_expr)));\n+      else\n+\tni = fold_build2 (PLUS_EXPR, TREE_TYPE (init_expr),\n+\t\t\t  fold_build2 (MULT_EXPR, TREE_TYPE (init_expr),\n+\t\t\t\t       fold_convert (TREE_TYPE (init_expr),\n+\t\t\t\t\t\t     niters),\n+\t\t\t\t       step_expr),\n+\t\t\t  init_expr);\n+\n+\n+\n+      var = create_tmp_var (TREE_TYPE (init_expr), \"tmp\");\n+      add_referenced_var (var);\n+\n+      last_gsi = gsi_last_bb (exit_bb);\n+      ni_name = force_gimple_operand_gsi (&last_gsi, ni, false, var,\n+\t\t\t\t\t  true, GSI_SAME_STMT);\n+      \n+      /* Fix phi expressions in the successor bb.  */\n+      SET_PHI_ARG_DEF (phi1, update_e->dest_idx, ni_name);\n+    }\n+}\n+\n+/* Return the more conservative threshold between the\n+   min_profitable_iters returned by the cost model and the user\n+   specified threshold, if provided.  */\n+\n+static unsigned int\n+conservative_cost_threshold (loop_vec_info loop_vinfo,\n+\t\t\t     int min_profitable_iters)\n+{\n+  unsigned int th;\n+  int min_scalar_loop_bound;\n+\n+  min_scalar_loop_bound = ((PARAM_VALUE (PARAM_MIN_VECT_LOOP_BOUND)\n+\t\t\t    * LOOP_VINFO_VECT_FACTOR (loop_vinfo)) - 1);\n+\n+  /* Use the cost model only if it is more conservative than user specified\n+     threshold.  */\n+  th = (unsigned) min_scalar_loop_bound;\n+  if (min_profitable_iters\n+      && (!min_scalar_loop_bound\n+          || min_profitable_iters > min_scalar_loop_bound))\n+    th = (unsigned) min_profitable_iters;\n+\n+  if (th && vect_print_dump_info (REPORT_COST))\n+    fprintf (vect_dump, \"Vectorization may not be profitable.\");\n+\n+  return th;\n+}\n+\n+/* Function vect_do_peeling_for_loop_bound\n+\n+   Peel the last iterations of the loop represented by LOOP_VINFO.\n+   The peeled iterations form a new epilog loop.  Given that the loop now \n+   iterates NITERS times, the new epilog loop iterates\n+   NITERS % VECTORIZATION_FACTOR times.\n+   \n+   The original loop will later be made to iterate \n+   NITERS / VECTORIZATION_FACTOR times (this value is placed into RATIO).  */\n+\n+void \n+vect_do_peeling_for_loop_bound (loop_vec_info loop_vinfo, tree *ratio)\n+{\n+  tree ni_name, ratio_mult_vf_name;\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  struct loop *new_loop;\n+  edge update_e;\n+  basic_block preheader;\n+  int loop_num;\n+  bool check_profitability = false;\n+  unsigned int th = 0;\n+  int min_profitable_iters;\n+\n+  if (vect_print_dump_info (REPORT_DETAILS))\n+    fprintf (vect_dump, \"=== vect_do_peeling_for_loop_bound ===\");\n+\n+  initialize_original_copy_tables ();\n+\n+  /* Generate the following variables on the preheader of original loop:\n+\t \n+     ni_name = number of iteration the original loop executes\n+     ratio = ni_name / vf\n+     ratio_mult_vf_name = ratio * vf  */\n+  vect_generate_tmps_on_preheader (loop_vinfo, &ni_name,\n+\t\t\t\t   &ratio_mult_vf_name, ratio);\n+\n+  loop_num  = loop->num; \n+\n+  /* If cost model check not done during versioning and \n+     peeling for alignment.  */\n+  if (!VEC_length (gimple, LOOP_VINFO_MAY_MISALIGN_STMTS (loop_vinfo))\n+      && !VEC_length (ddr_p, LOOP_VINFO_MAY_ALIAS_DDRS (loop_vinfo))\n+      && !LOOP_PEELING_FOR_ALIGNMENT (loop_vinfo))\n+    {\n+      check_profitability = true;\n+\n+      /* Get profitability threshold for vectorized loop.  */\n+      min_profitable_iters = LOOP_VINFO_COST_MODEL_MIN_ITERS (loop_vinfo);\n+\n+      th = conservative_cost_threshold (loop_vinfo, \n+\t\t\t\t\tmin_profitable_iters);\n+    }\n+\n+  new_loop = slpeel_tree_peel_loop_to_edge (loop, single_exit (loop),\n+                                            ratio_mult_vf_name, ni_name, false,\n+                                            th, check_profitability);\n+  gcc_assert (new_loop);\n+  gcc_assert (loop_num == loop->num);\n+#ifdef ENABLE_CHECKING\n+  slpeel_verify_cfg_after_peeling (loop, new_loop);\n+#endif\n+\n+  /* A guard that controls whether the new_loop is to be executed or skipped\n+     is placed in LOOP->exit.  LOOP->exit therefore has two successors - one\n+     is the preheader of NEW_LOOP, where the IVs from LOOP are used.  The other\n+     is a bb after NEW_LOOP, where these IVs are not used.  Find the edge that\n+     is on the path where the LOOP IVs are used and need to be updated.  */\n+\n+  preheader = loop_preheader_edge (new_loop)->src;\n+  if (EDGE_PRED (preheader, 0)->src == single_exit (loop)->dest)\n+    update_e = EDGE_PRED (preheader, 0);\n+  else\n+    update_e = EDGE_PRED (preheader, 1);\n+\n+  /* Update IVs of original loop as if they were advanced \n+     by ratio_mult_vf_name steps.  */\n+  vect_update_ivs_after_vectorizer (loop_vinfo, ratio_mult_vf_name, update_e); \n+\n+  /* After peeling we have to reset scalar evolution analyzer.  */\n+  scev_reset ();\n+\n+  free_original_copy_tables ();\n+}\n+\n+\n+/* Function vect_gen_niters_for_prolog_loop\n+\n+   Set the number of iterations for the loop represented by LOOP_VINFO\n+   to the minimum between LOOP_NITERS (the original iteration count of the loop)\n+   and the misalignment of DR - the data reference recorded in\n+   LOOP_VINFO_UNALIGNED_DR (LOOP_VINFO).  As a result, after the execution of \n+   this loop, the data reference DR will refer to an aligned location.\n+\n+   The following computation is generated:\n+\n+   If the misalignment of DR is known at compile time:\n+     addr_mis = int mis = DR_MISALIGNMENT (dr);\n+   Else, compute address misalignment in bytes:\n+     addr_mis = addr & (vectype_size - 1)\n+\n+   prolog_niters = min (LOOP_NITERS, ((VF - addr_mis/elem_size)&(VF-1))/step)\n+\n+   (elem_size = element type size; an element is the scalar element whose type\n+   is the inner type of the vectype)\n+\n+   When the step of the data-ref in the loop is not 1 (as in interleaved data\n+   and SLP), the number of iterations of the prolog must be divided by the step\n+   (which is equal to the size of interleaved group).\n+\n+   The above formulas assume that VF == number of elements in the vector. This\n+   may not hold when there are multiple-types in the loop.\n+   In this case, for some data-references in the loop the VF does not represent\n+   the number of elements that fit in the vector.  Therefore, instead of VF we\n+   use TYPE_VECTOR_SUBPARTS.  */\n+\n+static tree \n+vect_gen_niters_for_prolog_loop (loop_vec_info loop_vinfo, tree loop_niters)\n+{\n+  struct data_reference *dr = LOOP_VINFO_UNALIGNED_DR (loop_vinfo);\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  tree var;\n+  gimple_seq stmts;\n+  tree iters, iters_name;\n+  edge pe;\n+  basic_block new_bb;\n+  gimple dr_stmt = DR_STMT (dr);\n+  stmt_vec_info stmt_info = vinfo_for_stmt (dr_stmt);\n+  tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n+  int vectype_align = TYPE_ALIGN (vectype) / BITS_PER_UNIT;\n+  tree niters_type = TREE_TYPE (loop_niters);\n+  int step = 1;\n+  int element_size = GET_MODE_SIZE (TYPE_MODE (TREE_TYPE (DR_REF (dr))));\n+  int nelements = TYPE_VECTOR_SUBPARTS (vectype);\n+\n+  if (STMT_VINFO_STRIDED_ACCESS (stmt_info))\n+    step = DR_GROUP_SIZE (vinfo_for_stmt (DR_GROUP_FIRST_DR (stmt_info)));\n+\n+  pe = loop_preheader_edge (loop); \n+\n+  if (LOOP_PEELING_FOR_ALIGNMENT (loop_vinfo) > 0)\n+    {\n+      int byte_misalign = LOOP_PEELING_FOR_ALIGNMENT (loop_vinfo);\n+      int elem_misalign = byte_misalign / element_size;\n+\n+      if (vect_print_dump_info (REPORT_DETAILS))\n+        fprintf (vect_dump, \"known alignment = %d.\", byte_misalign);\n+\n+      iters = build_int_cst (niters_type,\n+                     (((nelements - elem_misalign) & (nelements - 1)) / step));\n+    }\n+  else\n+    {\n+      gimple_seq new_stmts = NULL;\n+      tree start_addr = vect_create_addr_base_for_vector_ref (dr_stmt, \n+\t\t\t\t\t\t&new_stmts, NULL_TREE, loop);\n+      tree ptr_type = TREE_TYPE (start_addr);\n+      tree size = TYPE_SIZE (ptr_type);\n+      tree type = lang_hooks.types.type_for_size (tree_low_cst (size, 1), 1);\n+      tree vectype_size_minus_1 = build_int_cst (type, vectype_align - 1);\n+      tree elem_size_log =\n+        build_int_cst (type, exact_log2 (vectype_align/nelements));\n+      tree nelements_minus_1 = build_int_cst (type, nelements - 1);\n+      tree nelements_tree = build_int_cst (type, nelements);\n+      tree byte_misalign;\n+      tree elem_misalign;\n+\n+      new_bb = gsi_insert_seq_on_edge_immediate (pe, new_stmts);\n+      gcc_assert (!new_bb);\n+  \n+      /* Create:  byte_misalign = addr & (vectype_size - 1)  */\n+      byte_misalign = \n+        fold_build2 (BIT_AND_EXPR, type, fold_convert (type, start_addr), vectype_size_minus_1);\n+  \n+      /* Create:  elem_misalign = byte_misalign / element_size  */\n+      elem_misalign =\n+        fold_build2 (RSHIFT_EXPR, type, byte_misalign, elem_size_log);\n+\n+      /* Create:  (niters_type) (nelements - elem_misalign)&(nelements - 1)  */\n+      iters = fold_build2 (MINUS_EXPR, type, nelements_tree, elem_misalign);\n+      iters = fold_build2 (BIT_AND_EXPR, type, iters, nelements_minus_1);\n+      iters = fold_convert (niters_type, iters);\n+    }\n+\n+  /* Create:  prolog_loop_niters = min (iters, loop_niters) */\n+  /* If the loop bound is known at compile time we already verified that it is\n+     greater than vf; since the misalignment ('iters') is at most vf, there's\n+     no need to generate the MIN_EXPR in this case.  */\n+  if (TREE_CODE (loop_niters) != INTEGER_CST)\n+    iters = fold_build2 (MIN_EXPR, niters_type, iters, loop_niters);\n+\n+  if (vect_print_dump_info (REPORT_DETAILS))\n+    {\n+      fprintf (vect_dump, \"niters for prolog loop: \");\n+      print_generic_expr (vect_dump, iters, TDF_SLIM);\n+    }\n+\n+  var = create_tmp_var (niters_type, \"prolog_loop_niters\");\n+  add_referenced_var (var);\n+  stmts = NULL;\n+  iters_name = force_gimple_operand (iters, &stmts, false, var);\n+\n+  /* Insert stmt on loop preheader edge.  */\n+  if (stmts)\n+    {\n+      basic_block new_bb = gsi_insert_seq_on_edge_immediate (pe, stmts);\n+      gcc_assert (!new_bb);\n+    }\n+\n+  return iters_name; \n+}\n+\n+\n+/* Function vect_update_init_of_dr\n+\n+   NITERS iterations were peeled from LOOP.  DR represents a data reference\n+   in LOOP.  This function updates the information recorded in DR to\n+   account for the fact that the first NITERS iterations had already been \n+   executed.  Specifically, it updates the OFFSET field of DR.  */\n+\n+static void\n+vect_update_init_of_dr (struct data_reference *dr, tree niters)\n+{\n+  tree offset = DR_OFFSET (dr);\n+      \n+  niters = fold_build2 (MULT_EXPR, sizetype,\n+\t\t\tfold_convert (sizetype, niters),\n+\t\t\tfold_convert (sizetype, DR_STEP (dr)));\n+  offset = fold_build2 (PLUS_EXPR, sizetype, offset, niters);\n+  DR_OFFSET (dr) = offset;\n+}\n+\n+\n+/* Function vect_update_inits_of_drs\n+\n+   NITERS iterations were peeled from the loop represented by LOOP_VINFO.  \n+   This function updates the information recorded for the data references in \n+   the loop to account for the fact that the first NITERS iterations had \n+   already been executed.  Specifically, it updates the initial_condition of\n+   the access_function of all the data_references in the loop.  */\n+\n+static void\n+vect_update_inits_of_drs (loop_vec_info loop_vinfo, tree niters)\n+{\n+  unsigned int i;\n+  VEC (data_reference_p, heap) *datarefs = LOOP_VINFO_DATAREFS (loop_vinfo);\n+  struct data_reference *dr;\n+\n+  if (vect_print_dump_info (REPORT_DETAILS))\n+    fprintf (vect_dump, \"=== vect_update_inits_of_dr ===\");\n+\n+  for (i = 0; VEC_iterate (data_reference_p, datarefs, i, dr); i++)\n+    vect_update_init_of_dr (dr, niters);\n+}\n+\n+\n+/* Function vect_do_peeling_for_alignment\n+\n+   Peel the first 'niters' iterations of the loop represented by LOOP_VINFO.\n+   'niters' is set to the misalignment of one of the data references in the\n+   loop, thereby forcing it to refer to an aligned location at the beginning\n+   of the execution of this loop.  The data reference for which we are\n+   peeling is recorded in LOOP_VINFO_UNALIGNED_DR.  */\n+\n+void\n+vect_do_peeling_for_alignment (loop_vec_info loop_vinfo)\n+{\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  tree niters_of_prolog_loop, ni_name;\n+  tree n_iters;\n+  struct loop *new_loop;\n+  bool check_profitability = false;\n+  unsigned int th = 0;\n+  int min_profitable_iters;\n+\n+  if (vect_print_dump_info (REPORT_DETAILS))\n+    fprintf (vect_dump, \"=== vect_do_peeling_for_alignment ===\");\n+\n+  initialize_original_copy_tables ();\n+\n+  ni_name = vect_build_loop_niters (loop_vinfo);\n+  niters_of_prolog_loop = vect_gen_niters_for_prolog_loop (loop_vinfo, ni_name);\n+  \n+\n+  /* If cost model check not done during versioning.  */\n+  if (!VEC_length (gimple, LOOP_VINFO_MAY_MISALIGN_STMTS (loop_vinfo))\n+      && !VEC_length (ddr_p, LOOP_VINFO_MAY_ALIAS_DDRS (loop_vinfo)))\n+    {\n+      check_profitability = true;\n+\n+      /* Get profitability threshold for vectorized loop.  */\n+      min_profitable_iters = LOOP_VINFO_COST_MODEL_MIN_ITERS (loop_vinfo);\n+\n+      th = conservative_cost_threshold (loop_vinfo, \n+\t\t\t\t\tmin_profitable_iters);\n+    }\n+\n+  /* Peel the prolog loop and iterate it niters_of_prolog_loop.  */\n+  new_loop =\n+    slpeel_tree_peel_loop_to_edge (loop, loop_preheader_edge (loop),\n+\t\t\t\t   niters_of_prolog_loop, ni_name, true,\n+\t\t\t\t   th, check_profitability);\n+\n+  gcc_assert (new_loop);\n+#ifdef ENABLE_CHECKING\n+  slpeel_verify_cfg_after_peeling (new_loop, loop);\n+#endif\n+\n+  /* Update number of times loop executes.  */\n+  n_iters = LOOP_VINFO_NITERS (loop_vinfo);\n+  LOOP_VINFO_NITERS (loop_vinfo) = fold_build2 (MINUS_EXPR,\n+\t\tTREE_TYPE (n_iters), n_iters, niters_of_prolog_loop);\n+\n+  /* Update the init conditions of the access functions of all data refs.  */\n+  vect_update_inits_of_drs (loop_vinfo, niters_of_prolog_loop);\n+\n+  /* After peeling we have to reset scalar evolution analyzer.  */\n+  scev_reset ();\n+\n+  free_original_copy_tables ();\n+}\n+\n+\n+/* Function vect_create_cond_for_align_checks.\n+\n+   Create a conditional expression that represents the alignment checks for\n+   all of data references (array element references) whose alignment must be\n+   checked at runtime.\n+\n+   Input:\n+   COND_EXPR  - input conditional expression.  New conditions will be chained\n+                with logical AND operation.\n+   LOOP_VINFO - two fields of the loop information are used.\n+                LOOP_VINFO_PTR_MASK is the mask used to check the alignment.\n+                LOOP_VINFO_MAY_MISALIGN_STMTS contains the refs to be checked.\n+\n+   Output:\n+   COND_EXPR_STMT_LIST - statements needed to construct the conditional\n+                         expression.\n+   The returned value is the conditional expression to be used in the if\n+   statement that controls which version of the loop gets executed at runtime.\n+\n+   The algorithm makes two assumptions:\n+     1) The number of bytes \"n\" in a vector is a power of 2.\n+     2) An address \"a\" is aligned if a%n is zero and that this\n+        test can be done as a&(n-1) == 0.  For example, for 16\n+        byte vectors the test is a&0xf == 0.  */\n+\n+static void\n+vect_create_cond_for_align_checks (loop_vec_info loop_vinfo,\n+                                   tree *cond_expr,\n+\t\t\t\t   gimple_seq *cond_expr_stmt_list)\n+{\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  VEC(gimple,heap) *may_misalign_stmts\n+    = LOOP_VINFO_MAY_MISALIGN_STMTS (loop_vinfo);\n+  gimple ref_stmt;\n+  int mask = LOOP_VINFO_PTR_MASK (loop_vinfo);\n+  tree mask_cst;\n+  unsigned int i;\n+  tree psize;\n+  tree int_ptrsize_type;\n+  char tmp_name[20];\n+  tree or_tmp_name = NULL_TREE;\n+  tree and_tmp, and_tmp_name;\n+  gimple and_stmt;\n+  tree ptrsize_zero;\n+  tree part_cond_expr;\n+\n+  /* Check that mask is one less than a power of 2, i.e., mask is\n+     all zeros followed by all ones.  */\n+  gcc_assert ((mask != 0) && ((mask & (mask+1)) == 0));\n+\n+  /* CHECKME: what is the best integer or unsigned type to use to hold a\n+     cast from a pointer value?  */\n+  psize = TYPE_SIZE (ptr_type_node);\n+  int_ptrsize_type\n+    = lang_hooks.types.type_for_size (tree_low_cst (psize, 1), 0);\n+\n+  /* Create expression (mask & (dr_1 || ... || dr_n)) where dr_i is the address\n+     of the first vector of the i'th data reference. */\n+\n+  for (i = 0; VEC_iterate (gimple, may_misalign_stmts, i, ref_stmt); i++)\n+    {\n+      gimple_seq new_stmt_list = NULL;\n+      tree addr_base;\n+      tree addr_tmp, addr_tmp_name;\n+      tree or_tmp, new_or_tmp_name;\n+      gimple addr_stmt, or_stmt;\n+\n+      /* create: addr_tmp = (int)(address_of_first_vector) */\n+      addr_base =\n+\tvect_create_addr_base_for_vector_ref (ref_stmt, &new_stmt_list,\n+\t\t\t\t\t      NULL_TREE, loop);\n+      if (new_stmt_list != NULL)\n+\tgimple_seq_add_seq (cond_expr_stmt_list, new_stmt_list);\n+\n+      sprintf (tmp_name, \"%s%d\", \"addr2int\", i);\n+      addr_tmp = create_tmp_var (int_ptrsize_type, tmp_name);\n+      add_referenced_var (addr_tmp);\n+      addr_tmp_name = make_ssa_name (addr_tmp, NULL);\n+      addr_stmt = gimple_build_assign_with_ops (NOP_EXPR, addr_tmp_name,\n+\t\t\t\t\t\taddr_base, NULL_TREE);\n+      SSA_NAME_DEF_STMT (addr_tmp_name) = addr_stmt;\n+      gimple_seq_add_stmt (cond_expr_stmt_list, addr_stmt);\n+\n+      /* The addresses are OR together.  */\n+\n+      if (or_tmp_name != NULL_TREE)\n+        {\n+          /* create: or_tmp = or_tmp | addr_tmp */\n+          sprintf (tmp_name, \"%s%d\", \"orptrs\", i);\n+          or_tmp = create_tmp_var (int_ptrsize_type, tmp_name);\n+          add_referenced_var (or_tmp);\n+\t  new_or_tmp_name = make_ssa_name (or_tmp, NULL);\n+\t  or_stmt = gimple_build_assign_with_ops (BIT_IOR_EXPR,\n+\t\t\t\t\t\t  new_or_tmp_name,\n+\t\t\t\t\t\t  or_tmp_name, addr_tmp_name);\n+          SSA_NAME_DEF_STMT (new_or_tmp_name) = or_stmt;\n+\t  gimple_seq_add_stmt (cond_expr_stmt_list, or_stmt);\n+          or_tmp_name = new_or_tmp_name;\n+        }\n+      else\n+        or_tmp_name = addr_tmp_name;\n+\n+    } /* end for i */\n+\n+  mask_cst = build_int_cst (int_ptrsize_type, mask);\n+\n+  /* create: and_tmp = or_tmp & mask  */\n+  and_tmp = create_tmp_var (int_ptrsize_type, \"andmask\" );\n+  add_referenced_var (and_tmp);\n+  and_tmp_name = make_ssa_name (and_tmp, NULL);\n+\n+  and_stmt = gimple_build_assign_with_ops (BIT_AND_EXPR, and_tmp_name,\n+\t\t\t\t\t   or_tmp_name, mask_cst);\n+  SSA_NAME_DEF_STMT (and_tmp_name) = and_stmt;\n+  gimple_seq_add_stmt (cond_expr_stmt_list, and_stmt);\n+\n+  /* Make and_tmp the left operand of the conditional test against zero.\n+     if and_tmp has a nonzero bit then some address is unaligned.  */\n+  ptrsize_zero = build_int_cst (int_ptrsize_type, 0);\n+  part_cond_expr = fold_build2 (EQ_EXPR, boolean_type_node,\n+\t\t\t\tand_tmp_name, ptrsize_zero);\n+  if (*cond_expr)\n+    *cond_expr = fold_build2 (TRUTH_AND_EXPR, boolean_type_node,\n+\t\t\t      *cond_expr, part_cond_expr);\n+  else\n+    *cond_expr = part_cond_expr;\n+}\n+\n+\n+/* Function vect_vfa_segment_size.\n+\n+   Create an expression that computes the size of segment\n+   that will be accessed for a data reference.  The functions takes into\n+   account that realignment loads may access one more vector.\n+\n+   Input:\n+     DR: The data reference.\n+     VECT_FACTOR: vectorization factor.\n+\n+   Return an expression whose value is the size of segment which will be\n+   accessed by DR.  */\n+\n+static tree\n+vect_vfa_segment_size (struct data_reference *dr, tree vect_factor)\n+{\n+  tree segment_length = fold_build2 (MULT_EXPR, integer_type_node,\n+\t\t\t             DR_STEP (dr), vect_factor);\n+\n+  if (vect_supportable_dr_alignment (dr) == dr_explicit_realign_optimized)\n+    {\n+      tree vector_size = TYPE_SIZE_UNIT\n+\t\t\t  (STMT_VINFO_VECTYPE (vinfo_for_stmt (DR_STMT (dr))));\n+\n+      segment_length = fold_build2 (PLUS_EXPR, integer_type_node,\n+\t\t\t\t    segment_length, vector_size);\n+    }\n+  return fold_convert (sizetype, segment_length);\n+}\n+\n+\n+/* Function vect_create_cond_for_alias_checks.\n+\n+   Create a conditional expression that represents the run-time checks for\n+   overlapping of address ranges represented by a list of data references\n+   relations passed as input.\n+\n+   Input:\n+   COND_EXPR  - input conditional expression.  New conditions will be chained\n+                with logical AND operation.\n+   LOOP_VINFO - field LOOP_VINFO_MAY_ALIAS_STMTS contains the list of ddrs\n+\t        to be checked.\n+\n+   Output:\n+   COND_EXPR - conditional expression.\n+   COND_EXPR_STMT_LIST - statements needed to construct the conditional\n+                         expression.\n+\n+\n+   The returned value is the conditional expression to be used in the if\n+   statement that controls which version of the loop gets executed at runtime.\n+*/\n+\n+static void\n+vect_create_cond_for_alias_checks (loop_vec_info loop_vinfo,\n+\t\t\t\t   tree * cond_expr,\n+\t\t\t\t   gimple_seq * cond_expr_stmt_list)\n+{\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  VEC (ddr_p, heap) * may_alias_ddrs =\n+    LOOP_VINFO_MAY_ALIAS_DDRS (loop_vinfo);\n+  tree vect_factor =\n+    build_int_cst (integer_type_node, LOOP_VINFO_VECT_FACTOR (loop_vinfo));\n+\n+  ddr_p ddr;\n+  unsigned int i;\n+  tree part_cond_expr;\n+\n+  /* Create expression\n+     ((store_ptr_0 + store_segment_length_0) < load_ptr_0)\n+     || (load_ptr_0 + load_segment_length_0) < store_ptr_0))\n+     &&         \n+     ...\n+     &&\n+     ((store_ptr_n + store_segment_length_n) < load_ptr_n)\n+     || (load_ptr_n + load_segment_length_n) < store_ptr_n))  */\n+\n+  if (VEC_empty (ddr_p, may_alias_ddrs))\n+    return;\n+\n+  for (i = 0; VEC_iterate (ddr_p, may_alias_ddrs, i, ddr); i++)\n+    {\n+      struct data_reference *dr_a, *dr_b;\n+      gimple dr_group_first_a, dr_group_first_b;\n+      tree addr_base_a, addr_base_b;\n+      tree segment_length_a, segment_length_b;\n+      gimple stmt_a, stmt_b;\n+\n+      dr_a = DDR_A (ddr);\n+      stmt_a = DR_STMT (DDR_A (ddr));\n+      dr_group_first_a = DR_GROUP_FIRST_DR (vinfo_for_stmt (stmt_a));\n+      if (dr_group_first_a)\n+        {\n+\t  stmt_a = dr_group_first_a;\n+\t  dr_a = STMT_VINFO_DATA_REF (vinfo_for_stmt (stmt_a));\n+\t}\n+\n+      dr_b = DDR_B (ddr);\n+      stmt_b = DR_STMT (DDR_B (ddr));\n+      dr_group_first_b = DR_GROUP_FIRST_DR (vinfo_for_stmt (stmt_b));\n+      if (dr_group_first_b)\n+        {\n+\t  stmt_b = dr_group_first_b;\n+\t  dr_b = STMT_VINFO_DATA_REF (vinfo_for_stmt (stmt_b));\n+\t}\n+\n+      addr_base_a =\n+        vect_create_addr_base_for_vector_ref (stmt_a, cond_expr_stmt_list,\n+\t\t\t\t\t      NULL_TREE, loop);\n+      addr_base_b =\n+        vect_create_addr_base_for_vector_ref (stmt_b, cond_expr_stmt_list,\n+\t\t\t\t\t      NULL_TREE, loop);\n+\n+      segment_length_a = vect_vfa_segment_size (dr_a, vect_factor);\n+      segment_length_b = vect_vfa_segment_size (dr_b, vect_factor);\n+\n+      if (vect_print_dump_info (REPORT_DR_DETAILS))\n+\t{\n+\t  fprintf (vect_dump,\n+\t\t   \"create runtime check for data references \");\n+\t  print_generic_expr (vect_dump, DR_REF (dr_a), TDF_SLIM);\n+\t  fprintf (vect_dump, \" and \");\n+\t  print_generic_expr (vect_dump, DR_REF (dr_b), TDF_SLIM);\n+\t}\n+\n+\n+      part_cond_expr = \n+      \tfold_build2 (TRUTH_OR_EXPR, boolean_type_node,\n+\t  fold_build2 (LT_EXPR, boolean_type_node,\n+\t    fold_build2 (POINTER_PLUS_EXPR, TREE_TYPE (addr_base_a),\n+\t      addr_base_a,\n+\t      segment_length_a),\n+\t    addr_base_b),\n+\t  fold_build2 (LT_EXPR, boolean_type_node,\n+\t    fold_build2 (POINTER_PLUS_EXPR, TREE_TYPE (addr_base_b),\n+\t      addr_base_b,\n+\t      segment_length_b),\n+\t    addr_base_a));\n+      \n+      if (*cond_expr)\n+\t*cond_expr = fold_build2 (TRUTH_AND_EXPR, boolean_type_node,\n+\t\t\t\t  *cond_expr, part_cond_expr);\n+      else\n+\t*cond_expr = part_cond_expr;\n+    }\n+    if (vect_print_dump_info (REPORT_VECTORIZED_LOOPS))\n+      fprintf (vect_dump, \"created %u versioning for alias checks.\\n\",\n+               VEC_length (ddr_p, may_alias_ddrs));\n+\n+}\n+\n+\n+/* Function vect_loop_versioning.\n+ \n+   If the loop has data references that may or may not be aligned or/and\n+   has data reference relations whose independence was not proven then\n+   two versions of the loop need to be generated, one which is vectorized\n+   and one which isn't.  A test is then generated to control which of the\n+   loops is executed.  The test checks for the alignment of all of the\n+   data references that may or may not be aligned.  An additional\n+   sequence of runtime tests is generated for each pairs of DDRs whose\n+   independence was not proven.  The vectorized version of loop is \n+   executed only if both alias and alignment tests are passed.  \n+  \n+   The test generated to check which version of loop is executed\n+   is modified to also check for profitability as indicated by the \n+   cost model initially.  */\n+\n+void\n+vect_loop_versioning (loop_vec_info loop_vinfo)\n+{\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+  struct loop *nloop;\n+  tree cond_expr = NULL_TREE;\n+  gimple_seq cond_expr_stmt_list = NULL;\n+  basic_block condition_bb;\n+  gimple_stmt_iterator gsi, cond_exp_gsi;\n+  basic_block merge_bb;\n+  basic_block new_exit_bb;\n+  edge new_exit_e, e;\n+  gimple orig_phi, new_phi;\n+  tree arg;\n+  unsigned prob = 4 * REG_BR_PROB_BASE / 5;\n+  gimple_seq gimplify_stmt_list = NULL;\n+  tree scalar_loop_iters = LOOP_VINFO_NITERS (loop_vinfo);\n+  int min_profitable_iters = 0;\n+  unsigned int th;\n+\n+  /* Get profitability threshold for vectorized loop.  */\n+  min_profitable_iters = LOOP_VINFO_COST_MODEL_MIN_ITERS (loop_vinfo);\n+\n+  th = conservative_cost_threshold (loop_vinfo,\n+\t\t\t\t    min_profitable_iters);\n+\n+  cond_expr =\n+    fold_build2 (GT_EXPR, boolean_type_node, scalar_loop_iters, \n+ \t         build_int_cst (TREE_TYPE (scalar_loop_iters), th));\n+\n+  cond_expr = force_gimple_operand (cond_expr, &cond_expr_stmt_list,\n+\t\t\t\t    false, NULL_TREE);\n+\n+  if (VEC_length (gimple, LOOP_VINFO_MAY_MISALIGN_STMTS (loop_vinfo)))\n+      vect_create_cond_for_align_checks (loop_vinfo, &cond_expr,\n+\t\t\t\t\t &cond_expr_stmt_list);\n+\n+  if (VEC_length (ddr_p, LOOP_VINFO_MAY_ALIAS_DDRS (loop_vinfo)))\n+    vect_create_cond_for_alias_checks (loop_vinfo, &cond_expr, \n+\t\t\t\t       &cond_expr_stmt_list);\n+\n+  cond_expr =\n+    fold_build2 (NE_EXPR, boolean_type_node, cond_expr, integer_zero_node);\n+  cond_expr =\n+    force_gimple_operand (cond_expr, &gimplify_stmt_list, true, NULL_TREE);\n+  gimple_seq_add_seq (&cond_expr_stmt_list, gimplify_stmt_list);\n+\n+  initialize_original_copy_tables ();\n+  nloop = loop_version (loop, cond_expr, &condition_bb,\n+\t\t\tprob, prob, REG_BR_PROB_BASE - prob, true);\n+  free_original_copy_tables();\n+\n+  /* Loop versioning violates an assumption we try to maintain during \n+     vectorization - that the loop exit block has a single predecessor.\n+     After versioning, the exit block of both loop versions is the same\n+     basic block (i.e. it has two predecessors). Just in order to simplify\n+     following transformations in the vectorizer, we fix this situation\n+     here by adding a new (empty) block on the exit-edge of the loop,\n+     with the proper loop-exit phis to maintain loop-closed-form.  */\n+  \n+  merge_bb = single_exit (loop)->dest;\n+  gcc_assert (EDGE_COUNT (merge_bb->preds) == 2);\n+  new_exit_bb = split_edge (single_exit (loop));\n+  new_exit_e = single_exit (loop);\n+  e = EDGE_SUCC (new_exit_bb, 0);\n+\n+  for (gsi = gsi_start_phis (merge_bb); !gsi_end_p (gsi); gsi_next (&gsi))\n+    {\n+      orig_phi = gsi_stmt (gsi);\n+      new_phi = create_phi_node (SSA_NAME_VAR (PHI_RESULT (orig_phi)),\n+\t\t\t\t  new_exit_bb);\n+      arg = PHI_ARG_DEF_FROM_EDGE (orig_phi, e);\n+      add_phi_arg (new_phi, arg, new_exit_e);\n+      SET_PHI_ARG_DEF (orig_phi, e->dest_idx, PHI_RESULT (new_phi));\n+    } \n+\n+  /* End loop-exit-fixes after versioning.  */\n+\n+  update_ssa (TODO_update_ssa);\n+  if (cond_expr_stmt_list)\n+    {\n+      cond_exp_gsi = gsi_last_bb (condition_bb);\n+      gsi_insert_seq_before (&cond_exp_gsi, cond_expr_stmt_list, GSI_SAME_STMT);\n+    }\n+}\n+"}, {"sha": "77dcdd6137a5961a94c8e5fafe74ca7a6fb9e390", "filename": "gcc/tree-vect-loop.c", "status": "added", "additions": 3587, "deletions": 0, "changes": 3587, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vect-loop.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vect-loop.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-loop.c?ref=ebfd146af752ce6aa298f866e36f9a7e66e32ec5"}, {"sha": "372f836cf80a68e0c1b4a5813e1243af46e2fba1", "filename": "gcc/tree-vect-patterns.c", "status": "modified", "additions": 1, "deletions": 3, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vect-patterns.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vect-patterns.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-patterns.c?ref=ebfd146af752ce6aa298f866e36f9a7e66e32ec5", "patch": "@@ -1,5 +1,5 @@\n /* Analysis Utilities for Loop Vectorization.\n-   Copyright (C) 2006, 2007, 2008 Free Software Foundation, Inc.\n+   Copyright (C) 2006, 2007, 2008, 2009 Free Software Foundation, Inc.\n    Contributed by Dorit Nuzman <dorit@il.ibm.com>\n \n This file is part of GCC.\n@@ -24,13 +24,11 @@ along with GCC; see the file COPYING3.  If not see\n #include \"tm.h\"\n #include \"ggc.h\"\n #include \"tree.h\"\n-\n #include \"target.h\"\n #include \"basic-block.h\"\n #include \"diagnostic.h\"\n #include \"tree-flow.h\"\n #include \"tree-dump.h\"\n-#include \"timevar.h\"\n #include \"cfgloop.h\"\n #include \"expr.h\"\n #include \"optabs.h\""}, {"sha": "fe01a76beea621e4069dda94c00a59dbddec44fc", "filename": "gcc/tree-vect-slp.c", "status": "added", "additions": 1694, "deletions": 0, "changes": 1694, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vect-slp.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vect-slp.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-slp.c?ref=ebfd146af752ce6aa298f866e36f9a7e66e32ec5", "patch": "@@ -0,0 +1,1694 @@\n+/* SLP - Basic Block Vectorization\n+   Copyright (C) 2007, 2008, 2009 Free Software Foundation, Inc.\n+   Foundation, Inc.\n+   Contributed by Dorit Naishlos <dorit@il.ibm.com> \n+   and Ira Rosen <irar@il.ibm.com>\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 3, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING3.  If not see\n+<http://www.gnu.org/licenses/>.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"ggc.h\"\n+#include \"tree.h\"\n+#include \"target.h\"\n+#include \"basic-block.h\"\n+#include \"diagnostic.h\"\n+#include \"tree-flow.h\"\n+#include \"tree-dump.h\"\n+#include \"cfgloop.h\"\n+#include \"cfglayout.h\"\n+#include \"expr.h\"\n+#include \"recog.h\"\n+#include \"optabs.h\"\n+#include \"tree-vectorizer.h\"\n+\n+/* Recursively free the memory allocated for the SLP tree rooted at NODE.  */\n+\n+static void\n+vect_free_slp_tree (slp_tree node)\n+{\n+  if (!node)\n+    return;\n+\n+  if (SLP_TREE_LEFT (node))\n+    vect_free_slp_tree (SLP_TREE_LEFT (node));\n+   \n+  if (SLP_TREE_RIGHT (node))\n+    vect_free_slp_tree (SLP_TREE_RIGHT (node));\n+   \n+  VEC_free (gimple, heap, SLP_TREE_SCALAR_STMTS (node));\n+  \n+  if (SLP_TREE_VEC_STMTS (node))\n+    VEC_free (gimple, heap, SLP_TREE_VEC_STMTS (node));\n+\n+  free (node);\n+}\n+\n+\n+/* Free the memory allocated for the SLP instance.  */\n+\n+void\n+vect_free_slp_instance (slp_instance instance)\n+{\n+  vect_free_slp_tree (SLP_INSTANCE_TREE (instance));\n+  VEC_free (int, heap, SLP_INSTANCE_LOAD_PERMUTATION (instance));\n+  VEC_free (slp_tree, heap, SLP_INSTANCE_LOADS (instance));\n+}\n+\n+\n+/* Get the defs for the rhs of STMT (collect them in DEF_STMTS0/1), check that\n+   they are of a legal type and that they match the defs of the first stmt of\n+   the SLP group (stored in FIRST_STMT_...).  */\n+\n+static bool\n+vect_get_and_check_slp_defs (loop_vec_info loop_vinfo, slp_tree slp_node,\n+\t\t\t     gimple stmt, VEC (gimple, heap) **def_stmts0,\n+\t\t\t     VEC (gimple, heap) **def_stmts1,\n+\t\t\t     enum vect_def_type *first_stmt_dt0,\n+\t\t\t     enum vect_def_type *first_stmt_dt1,\n+\t\t\t     tree *first_stmt_def0_type, \n+\t\t\t     tree *first_stmt_def1_type,\n+\t\t\t     tree *first_stmt_const_oprnd,\n+\t\t\t     int ncopies_for_cost,\n+                             bool *pattern0, bool *pattern1)\n+{\n+  tree oprnd;\n+  unsigned int i, number_of_oprnds;\n+  tree def;\n+  gimple def_stmt;\n+  enum vect_def_type dt[2] = {vect_unknown_def_type, vect_unknown_def_type};\n+  stmt_vec_info stmt_info = \n+    vinfo_for_stmt (VEC_index (gimple, SLP_TREE_SCALAR_STMTS (slp_node), 0));\n+  enum gimple_rhs_class rhs_class;\n+  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n+\n+  rhs_class = get_gimple_rhs_class (gimple_assign_rhs_code (stmt));\n+  number_of_oprnds = gimple_num_ops (stmt) - 1;\t/* RHS only */\n+\n+  for (i = 0; i < number_of_oprnds; i++)\n+    {\n+      oprnd = gimple_op (stmt, i + 1);\n+\n+      if (!vect_is_simple_use (oprnd, loop_vinfo, &def_stmt, &def, &dt[i])\n+\t  || (!def_stmt && dt[i] != vect_constant_def))\n+\t{\n+\t  if (vect_print_dump_info (REPORT_SLP)) \n+\t    {\n+\t      fprintf (vect_dump, \"Build SLP failed: can't find def for \");\n+\t      print_generic_expr (vect_dump, oprnd, TDF_SLIM);\n+\t    }\n+\n+\t  return false;\n+\t}\n+\n+      /* Check if DEF_STMT is a part of a pattern and get the def stmt from\n+         the pattern. Check that all the stmts of the node are in the\n+         pattern.  */\n+      if (def_stmt && gimple_bb (def_stmt)\n+          && flow_bb_inside_loop_p (loop, gimple_bb (def_stmt))\n+          && vinfo_for_stmt (def_stmt)\n+          && STMT_VINFO_IN_PATTERN_P (vinfo_for_stmt (def_stmt)))\n+        {\n+          if (!*first_stmt_dt0)\n+            *pattern0 = true;\n+          else\n+            {\n+              if (i == 1 && !*first_stmt_dt1)\n+                *pattern1 = true;\n+              else if ((i == 0 && !*pattern0) || (i == 1 && !*pattern1))\n+                {\n+                  if (vect_print_dump_info (REPORT_DETAILS))\n+                    {\n+                      fprintf (vect_dump, \"Build SLP failed: some of the stmts\"\n+                                     \" are in a pattern, and others are not \");\n+                      print_generic_expr (vect_dump, oprnd, TDF_SLIM);\n+                    }\n+\n+                  return false;\n+                }\n+            }\n+\n+          def_stmt = STMT_VINFO_RELATED_STMT (vinfo_for_stmt (def_stmt));\n+          dt[i] = STMT_VINFO_DEF_TYPE (vinfo_for_stmt (def_stmt));\n+\n+          if (*dt == vect_unknown_def_type)\n+            {\n+              if (vect_print_dump_info (REPORT_DETAILS))\n+                fprintf (vect_dump, \"Unsupported pattern.\");\n+              return false;\n+            }\n+\n+          switch (gimple_code (def_stmt))\n+            {\n+              case GIMPLE_PHI:\n+                def = gimple_phi_result (def_stmt);\n+                break;\n+\n+              case GIMPLE_ASSIGN:\n+                def = gimple_assign_lhs (def_stmt);\n+                break;\n+\n+              default:\n+                if (vect_print_dump_info (REPORT_DETAILS))\n+                  fprintf (vect_dump, \"unsupported defining stmt: \");\n+                return false;\n+            }\n+        }\n+\n+      if (!*first_stmt_dt0)\n+\t{\n+\t  /* op0 of the first stmt of the group - store its info.  */\n+\t  *first_stmt_dt0 = dt[i];\n+\t  if (def)\n+\t    *first_stmt_def0_type = TREE_TYPE (def);\n+\t  else\n+\t    *first_stmt_const_oprnd = oprnd;\n+\n+\t  /* Analyze costs (for the first stmt of the group only).  */\n+\t  if (rhs_class != GIMPLE_SINGLE_RHS)\n+\t    /* Not memory operation (we don't call this functions for loads).  */\n+\t    vect_model_simple_cost (stmt_info, ncopies_for_cost, dt, slp_node);\n+\t  else\n+\t    /* Store.  */\n+\t    vect_model_store_cost (stmt_info, ncopies_for_cost, dt[0], slp_node);\n+\t}\n+      \n+      else\n+\t{\n+\t  if (!*first_stmt_dt1 && i == 1)\n+\t    {\n+\t      /* op1 of the first stmt of the group - store its info.  */\n+\t      *first_stmt_dt1 = dt[i];\n+\t      if (def)\n+\t\t*first_stmt_def1_type = TREE_TYPE (def);\n+\t      else\n+\t\t{\n+\t\t  /* We assume that the stmt contains only one constant \n+\t\t     operand. We fail otherwise, to be on the safe side.  */\n+\t\t  if (*first_stmt_const_oprnd)\n+\t\t    {\n+\t\t      if (vect_print_dump_info (REPORT_SLP)) \n+\t\t\tfprintf (vect_dump, \"Build SLP failed: two constant \"\n+\t\t\t\t \"oprnds in stmt\");\t\t    \n+\t\t      return false;\n+\t\t    }\n+\t\t  *first_stmt_const_oprnd = oprnd;\n+\t\t}\n+\t    }\n+\t  else\n+\t    {\n+\t      /* Not first stmt of the group, check that the def-stmt/s match \n+\t\t the def-stmt/s of the first stmt.  */\n+\t      if ((i == 0 \n+\t\t   && (*first_stmt_dt0 != dt[i]\n+\t\t       || (*first_stmt_def0_type && def\n+\t\t\t   && *first_stmt_def0_type != TREE_TYPE (def))))\n+\t\t  || (i == 1 \n+\t\t      && (*first_stmt_dt1 != dt[i]\n+\t\t\t  || (*first_stmt_def1_type && def\n+\t\t\t      && *first_stmt_def1_type != TREE_TYPE (def))))\t\t  \n+\t\t  || (!def \n+\t\t      && TREE_TYPE (*first_stmt_const_oprnd) \n+\t\t      != TREE_TYPE (oprnd)))\n+\t\t{ \n+\t\t  if (vect_print_dump_info (REPORT_SLP)) \n+\t\t    fprintf (vect_dump, \"Build SLP failed: different types \");\n+\t\t  \n+\t\t  return false;\n+\t\t}\n+\t    }\n+\t}\n+\n+      /* Check the types of the definitions.  */\n+      switch (dt[i])\n+\t{\n+\tcase vect_constant_def:\n+\tcase vect_invariant_def:\n+\t  break;\n+\t  \n+\tcase vect_loop_def:\n+\t  if (i == 0)\n+\t    VEC_safe_push (gimple, heap, *def_stmts0, def_stmt);\n+\t  else\n+\t    VEC_safe_push (gimple, heap, *def_stmts1, def_stmt);\n+\t  break;\n+\n+\tdefault:\n+\t  /* FORNOW: Not supported.  */\n+\t  if (vect_print_dump_info (REPORT_SLP)) \n+\t    {\n+\t      fprintf (vect_dump, \"Build SLP failed: illegal type of def \");\n+\t      print_generic_expr (vect_dump, def, TDF_SLIM);\n+\t    }\n+\n+\t  return false;\n+\t}\n+    }\n+\n+  return true;\n+}\n+\n+\n+/* Recursively build an SLP tree starting from NODE.\n+   Fail (and return FALSE) if def-stmts are not isomorphic, require data \n+   permutation or are of unsupported types of operation. Otherwise, return \n+   TRUE.  */\n+\n+static bool\n+vect_build_slp_tree (loop_vec_info loop_vinfo, slp_tree *node, \n+\t\t     unsigned int group_size, \n+\t\t     int *inside_cost, int *outside_cost,\n+\t\t     int ncopies_for_cost, unsigned int *max_nunits,\n+                     VEC (int, heap) **load_permutation,\n+                     VEC (slp_tree, heap) **loads)\n+{\n+  VEC (gimple, heap) *def_stmts0 = VEC_alloc (gimple, heap, group_size);\n+  VEC (gimple, heap) *def_stmts1 =  VEC_alloc (gimple, heap, group_size);\n+  unsigned int i;\n+  VEC (gimple, heap) *stmts = SLP_TREE_SCALAR_STMTS (*node);\n+  gimple stmt = VEC_index (gimple, stmts, 0);\n+  enum vect_def_type first_stmt_dt0 = 0, first_stmt_dt1 = 0;\n+  enum tree_code first_stmt_code = 0, rhs_code;\n+  tree first_stmt_def1_type = NULL_TREE, first_stmt_def0_type = NULL_TREE;\n+  tree lhs;\n+  bool stop_recursion = false, need_same_oprnds = false;\n+  tree vectype, scalar_type, first_op1 = NULL_TREE;\n+  unsigned int vectorization_factor = 0, ncopies;\n+  optab optab;\n+  int icode;\n+  enum machine_mode optab_op2_mode;\n+  enum machine_mode vec_mode;\n+  tree first_stmt_const_oprnd = NULL_TREE;\n+  struct data_reference *first_dr;\n+  bool pattern0 = false, pattern1 = false;\n+  HOST_WIDE_INT dummy;\n+  bool permutation = false;\n+  unsigned int load_place;\n+  gimple first_load;\n+\n+  /* For every stmt in NODE find its def stmt/s.  */\n+  for (i = 0; VEC_iterate (gimple, stmts, i, stmt); i++)\n+    {\n+      if (vect_print_dump_info (REPORT_SLP)) \n+\t{\n+\t  fprintf (vect_dump, \"Build SLP for \");\n+\t  print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n+\t}\n+\n+      lhs = gimple_get_lhs (stmt);\n+      if (lhs == NULL_TREE)\n+\t{\n+\t  if (vect_print_dump_info (REPORT_SLP)) \n+\t    {\n+\t      fprintf (vect_dump,\n+\t\t       \"Build SLP failed: not GIMPLE_ASSIGN nor GIMPLE_CALL\");\n+\t      print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n+\t    }\n+\t  \n+\t  return false;\n+\t}\n+\n+      scalar_type = vect_get_smallest_scalar_type (stmt, &dummy, &dummy); \n+      vectype = get_vectype_for_scalar_type (scalar_type);\n+      if (!vectype)\n+        {\n+          if (vect_print_dump_info (REPORT_SLP))\n+            {\n+              fprintf (vect_dump, \"Build SLP failed: unsupported data-type \");\n+              print_generic_expr (vect_dump, scalar_type, TDF_SLIM);\n+            }\n+          return false;\n+        }\n+\n+      gcc_assert (LOOP_VINFO_VECT_FACTOR (loop_vinfo));\n+      vectorization_factor = LOOP_VINFO_VECT_FACTOR (loop_vinfo);\n+      ncopies = vectorization_factor / TYPE_VECTOR_SUBPARTS (vectype);\n+      if (ncopies > 1 && vect_print_dump_info (REPORT_SLP))\n+        fprintf (vect_dump, \"SLP with multiple types \");\n+\n+      /* In case of multiple types we need to detect the smallest type.  */\n+      if (*max_nunits < TYPE_VECTOR_SUBPARTS (vectype))\n+        *max_nunits = TYPE_VECTOR_SUBPARTS (vectype);\n+\t  \n+      if (is_gimple_call (stmt))\n+\trhs_code = CALL_EXPR;\n+      else\n+\trhs_code = gimple_assign_rhs_code (stmt);\n+\n+      /* Check the operation.  */\n+      if (i == 0)\n+\t{\n+\t  first_stmt_code = rhs_code;\n+\n+\t  /* Shift arguments should be equal in all the packed stmts for a \n+\t     vector shift with scalar shift operand.  */\n+\t  if (rhs_code == LSHIFT_EXPR || rhs_code == RSHIFT_EXPR\n+\t      || rhs_code == LROTATE_EXPR\n+\t      || rhs_code == RROTATE_EXPR)\n+\t    {\n+\t      vec_mode = TYPE_MODE (vectype);\n+\n+\t      /* First see if we have a vector/vector shift.  */\n+\t      optab = optab_for_tree_code (rhs_code, vectype,\n+\t\t\t\t\t   optab_vector);\n+\n+\t      if (!optab\n+\t\t  || (optab->handlers[(int) vec_mode].insn_code\n+\t\t      == CODE_FOR_nothing))\n+\t\t{\n+\t\t  /* No vector/vector shift, try for a vector/scalar shift.  */\n+\t\t  optab = optab_for_tree_code (rhs_code, vectype,\n+\t\t\t\t\t       optab_scalar);\n+\n+\t\t  if (!optab)\n+\t\t    {\n+\t\t      if (vect_print_dump_info (REPORT_SLP))\n+\t\t\tfprintf (vect_dump, \"Build SLP failed: no optab.\");\n+\t\t      return false;\n+\t\t    }\n+\t\t  icode = (int) optab->handlers[(int) vec_mode].insn_code;\n+\t\t  if (icode == CODE_FOR_nothing)\n+\t\t    {\n+\t\t      if (vect_print_dump_info (REPORT_SLP))\n+\t\t\tfprintf (vect_dump, \"Build SLP failed: \"\n+\t\t\t\t            \"op not supported by target.\");\n+\t\t      return false;\n+\t\t    }\n+\t\t  optab_op2_mode = insn_data[icode].operand[2].mode;\n+\t\t  if (!VECTOR_MODE_P (optab_op2_mode))\n+\t\t    {\n+\t\t      need_same_oprnds = true;\n+\t\t      first_op1 = gimple_assign_rhs2 (stmt);\n+\t\t    }\n+\t\t}\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  if (first_stmt_code != rhs_code\n+\t      && (first_stmt_code != IMAGPART_EXPR\n+\t\t  || rhs_code != REALPART_EXPR)\n+\t      && (first_stmt_code != REALPART_EXPR\n+\t\t  || rhs_code != IMAGPART_EXPR))\n+\t    {\n+\t      if (vect_print_dump_info (REPORT_SLP)) \n+\t\t{\n+\t\t  fprintf (vect_dump, \n+\t\t\t   \"Build SLP failed: different operation in stmt \");\n+\t\t  print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n+\t\t}\n+\t      \n+\t      return false;\n+\t    }\n+\t  \n+\t  if (need_same_oprnds \n+\t      && !operand_equal_p (first_op1, gimple_assign_rhs2 (stmt), 0))\n+\t    {\n+\t      if (vect_print_dump_info (REPORT_SLP)) \n+\t\t{\n+\t\t  fprintf (vect_dump, \n+\t\t\t   \"Build SLP failed: different shift arguments in \");\n+\t\t  print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n+\t\t}\n+\t      \n+\t      return false;\n+\t    }\n+\t}\n+\n+      /* Strided store or load.  */\n+      if (STMT_VINFO_STRIDED_ACCESS (vinfo_for_stmt (stmt)))\n+\t{\n+\t  if (REFERENCE_CLASS_P (lhs))\n+\t    {\n+\t      /* Store.  */\n+\t      if (!vect_get_and_check_slp_defs (loop_vinfo, *node, stmt,\n+\t\t\t\t\t\t&def_stmts0, &def_stmts1, \n+\t\t\t\t\t\t&first_stmt_dt0, \n+\t\t\t\t\t\t&first_stmt_dt1, \n+\t\t\t\t\t\t&first_stmt_def0_type, \n+\t\t\t\t\t\t&first_stmt_def1_type,\n+\t\t\t\t\t\t&first_stmt_const_oprnd,\n+\t\t\t\t\t\tncopies_for_cost,\n+                                                &pattern0, &pattern1))\n+\t\treturn false;\n+\t    }\n+\t    else\n+\t      {\n+\t\t/* Load.  */\n+                /* FORNOW: Check that there is no gap between the loads.  */\n+                if ((DR_GROUP_FIRST_DR (vinfo_for_stmt (stmt)) == stmt\n+                     && DR_GROUP_GAP (vinfo_for_stmt (stmt)) != 0)\n+                    || (DR_GROUP_FIRST_DR (vinfo_for_stmt (stmt)) != stmt\n+                        && DR_GROUP_GAP (vinfo_for_stmt (stmt)) != 1))\n+                  {\n+                    if (vect_print_dump_info (REPORT_SLP))\n+                      {\n+                        fprintf (vect_dump, \"Build SLP failed: strided \"\n+                                            \"loads have gaps \");\n+                        print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n+                      }\n+ \n+                    return false;\n+                  }\n+ \n+                first_load = DR_GROUP_FIRST_DR (vinfo_for_stmt (stmt));\n+ \n+              if (first_load == stmt)\n+                {\n+                  first_dr = STMT_VINFO_DATA_REF (vinfo_for_stmt (stmt));\n+                  if (vect_supportable_dr_alignment (first_dr)\n+                      == dr_unaligned_unsupported)\n+                    {\n+                      if (vect_print_dump_info (REPORT_SLP))\n+                        {\n+                          fprintf (vect_dump, \"Build SLP failed: unsupported \"\n+                                              \"unaligned load \");\n+                          print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n+                        }\n+  \n+                      return false;\n+                    }\n+ \n+                  /* Analyze costs (for the first stmt in the group).  */\n+                  vect_model_load_cost (vinfo_for_stmt (stmt),\n+                                        ncopies_for_cost, *node);\n+                }\n+  \n+              /* Store the place of this load in the interleaving chain. In\n+                 case that permutation is needed we later decide if a specific\n+                 permutation is supported.  */\n+              load_place = vect_get_place_in_interleaving_chain (stmt,\n+                                                                 first_load);\n+              if (load_place != i)\n+                permutation = true;\n+ \n+              VEC_safe_push (int, heap, *load_permutation, load_place);\n+ \n+              /* We stop the tree when we reach a group of loads.  */\n+              stop_recursion = true;\n+             continue;\n+           }\n+        } /* Strided access.  */\n+      else\n+\t{\n+\t  if (TREE_CODE_CLASS (rhs_code) == tcc_reference)\n+\t    {\n+\t      /* Not strided load. */\n+\t      if (vect_print_dump_info (REPORT_SLP)) \n+\t\t{\n+\t\t  fprintf (vect_dump, \"Build SLP failed: not strided load \");\n+\t\t  print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n+\t\t}\n+\n+\t      /* FORNOW: Not strided loads are not supported.  */\n+\t      return false;\n+\t    }\n+\n+\t  /* Not memory operation.  */\n+\t  if (TREE_CODE_CLASS (rhs_code) != tcc_binary\n+\t      && TREE_CODE_CLASS (rhs_code) != tcc_unary)\n+\t    {\n+\t      if (vect_print_dump_info (REPORT_SLP)) \n+\t\t{\n+\t\t  fprintf (vect_dump, \"Build SLP failed: operation\");\n+\t\t  fprintf (vect_dump, \" unsupported \");\n+\t\t  print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n+\t\t}\n+\n+\t      return false;\n+\t    }\n+\n+\t  /* Find the def-stmts.  */ \n+\t  if (!vect_get_and_check_slp_defs (loop_vinfo, *node, stmt,\n+\t\t\t\t\t    &def_stmts0, &def_stmts1,\n+\t\t\t\t\t    &first_stmt_dt0, &first_stmt_dt1, \n+\t\t\t\t\t    &first_stmt_def0_type, \n+\t\t\t\t\t    &first_stmt_def1_type,\n+\t\t\t\t\t    &first_stmt_const_oprnd,\n+\t\t\t\t\t    ncopies_for_cost,\n+                                            &pattern0, &pattern1))\n+\t    return false;\n+\t}\n+    }\n+\n+  /* Add the costs of the node to the overall instance costs.  */\n+  *inside_cost += SLP_TREE_INSIDE_OF_LOOP_COST (*node); \n+  *outside_cost += SLP_TREE_OUTSIDE_OF_LOOP_COST (*node);\n+\n+  /* Strided loads were reached - stop the recursion.  */\n+  if (stop_recursion)\n+    {\n+      if (permutation)\n+        {\n+          VEC_safe_push (slp_tree, heap, *loads, *node); \n+          *inside_cost += TARG_VEC_PERMUTE_COST * group_size;  \n+        }\n+\n+      return true;\n+    }\n+\n+  /* Create SLP_TREE nodes for the definition node/s.  */ \n+  if (first_stmt_dt0 == vect_loop_def)\n+    {\n+      slp_tree left_node = XNEW (struct _slp_tree);\n+      SLP_TREE_SCALAR_STMTS (left_node) = def_stmts0;\n+      SLP_TREE_VEC_STMTS (left_node) = NULL;\n+      SLP_TREE_LEFT (left_node) = NULL;\n+      SLP_TREE_RIGHT (left_node) = NULL;\n+      SLP_TREE_OUTSIDE_OF_LOOP_COST (left_node) = 0;\n+      SLP_TREE_INSIDE_OF_LOOP_COST (left_node) = 0;\n+      if (!vect_build_slp_tree (loop_vinfo, &left_node, group_size, \n+\t\t\t\tinside_cost, outside_cost, ncopies_for_cost, \n+\t\t\t\tmax_nunits, load_permutation, loads))\n+\treturn false;\n+      \n+      SLP_TREE_LEFT (*node) = left_node;\n+    }\n+\n+  if (first_stmt_dt1 == vect_loop_def)\n+    {\n+      slp_tree right_node = XNEW (struct _slp_tree);\n+      SLP_TREE_SCALAR_STMTS (right_node) = def_stmts1;\n+      SLP_TREE_VEC_STMTS (right_node) = NULL;\n+      SLP_TREE_LEFT (right_node) = NULL;\n+      SLP_TREE_RIGHT (right_node) = NULL;\n+      SLP_TREE_OUTSIDE_OF_LOOP_COST (right_node) = 0;\n+      SLP_TREE_INSIDE_OF_LOOP_COST (right_node) = 0;\n+      if (!vect_build_slp_tree (loop_vinfo, &right_node, group_size,\n+\t\t\t\tinside_cost, outside_cost, ncopies_for_cost,\n+\t\t\t\tmax_nunits, load_permutation, loads))\n+\treturn false;\n+      \n+      SLP_TREE_RIGHT (*node) = right_node;\n+    }\n+\n+  return true;\n+}\n+\n+\n+static void\n+vect_print_slp_tree (slp_tree node)\n+{\n+  int i;\n+  gimple stmt;\n+\n+  if (!node)\n+    return;\n+\n+  fprintf (vect_dump, \"node \");\n+  for (i = 0; VEC_iterate (gimple, SLP_TREE_SCALAR_STMTS (node), i, stmt); i++)\n+    {\n+      fprintf (vect_dump, \"\\n\\tstmt %d \", i);\n+      print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);  \n+    }\n+  fprintf (vect_dump, \"\\n\");\n+\n+  vect_print_slp_tree (SLP_TREE_LEFT (node));\n+  vect_print_slp_tree (SLP_TREE_RIGHT (node));\n+}\n+\n+\n+/* Mark the tree rooted at NODE with MARK (PURE_SLP or HYBRID). \n+   If MARK is HYBRID, it refers to a specific stmt in NODE (the stmt at index \n+   J). Otherwise, MARK is PURE_SLP and J is -1, which indicates that all the \n+   stmts in NODE are to be marked.  */\n+\n+static void\n+vect_mark_slp_stmts (slp_tree node, enum slp_vect_type mark, int j)\n+{\n+  int i;\n+  gimple stmt;\n+\n+  if (!node)\n+    return;\n+\n+  for (i = 0; VEC_iterate (gimple, SLP_TREE_SCALAR_STMTS (node), i, stmt); i++)\n+    if (j < 0 || i == j)\n+      STMT_SLP_TYPE (vinfo_for_stmt (stmt)) = mark;\n+\n+  vect_mark_slp_stmts (SLP_TREE_LEFT (node), mark, j);\n+  vect_mark_slp_stmts (SLP_TREE_RIGHT (node), mark, j);\n+}\n+\n+\n+/* Check if the permutation required by the SLP INSTANCE is supported.  \n+   Reorganize the SLP nodes stored in SLP_INSTANCE_LOADS if needed.  */\n+\n+static bool\n+vect_supported_slp_permutation_p (slp_instance instance)\n+{\n+  slp_tree node = VEC_index (slp_tree, SLP_INSTANCE_LOADS (instance), 0);\n+  gimple stmt = VEC_index (gimple, SLP_TREE_SCALAR_STMTS (node), 0);\n+  gimple first_load = DR_GROUP_FIRST_DR (vinfo_for_stmt (stmt));\n+  VEC (slp_tree, heap) *sorted_loads = NULL;\n+  int index;\n+  slp_tree *tmp_loads = NULL;\n+  int group_size = SLP_INSTANCE_GROUP_SIZE (instance), i, j; \n+  slp_tree load;\n+ \n+  /* FORNOW: The only supported loads permutation is loads from the same \n+     location in all the loads in the node, when the data-refs in\n+     nodes of LOADS constitute an interleaving chain.  \n+     Sort the nodes according to the order of accesses in the chain.  */\n+  tmp_loads = (slp_tree *) xmalloc (sizeof (slp_tree) * group_size);\n+  for (i = 0, j = 0; \n+       VEC_iterate (int, SLP_INSTANCE_LOAD_PERMUTATION (instance), i, index) \n+       && VEC_iterate (slp_tree, SLP_INSTANCE_LOADS (instance), j, load); \n+       i += group_size, j++)\n+    {\n+      gimple scalar_stmt = VEC_index (gimple, SLP_TREE_SCALAR_STMTS (load), 0);\n+      /* Check that the loads are all in the same interleaving chain.  */\n+      if (DR_GROUP_FIRST_DR (vinfo_for_stmt (scalar_stmt)) != first_load)\n+        {\n+          if (vect_print_dump_info (REPORT_DETAILS))\n+            {\n+              fprintf (vect_dump, \"Build SLP failed: unsupported data \"\n+                                   \"permutation \");\n+              print_gimple_stmt (vect_dump, scalar_stmt, 0, TDF_SLIM);\n+            }\n+             \n+          free (tmp_loads);\n+          return false; \n+        }\n+\n+      tmp_loads[index] = load;\n+    }\n+  \n+  sorted_loads = VEC_alloc (slp_tree, heap, group_size);\n+  for (i = 0; i < group_size; i++)\n+     VEC_safe_push (slp_tree, heap, sorted_loads, tmp_loads[i]);\n+\n+  VEC_free (slp_tree, heap, SLP_INSTANCE_LOADS (instance));\n+  SLP_INSTANCE_LOADS (instance) = sorted_loads;\n+  free (tmp_loads);\n+\n+  if (!vect_transform_slp_perm_load (stmt, NULL, NULL,\n+                                     SLP_INSTANCE_UNROLLING_FACTOR (instance),\n+                                     instance, true))\n+    return false;\n+\n+  return true;\n+}\n+\n+\n+/* Check if the required load permutation is supported.\n+   LOAD_PERMUTATION contains a list of indices of the loads.\n+   In SLP this permutation is relative to the order of strided stores that are\n+   the base of the SLP instance.  */\n+\n+static bool\n+vect_supported_load_permutation_p (slp_instance slp_instn, int group_size,\n+                                   VEC (int, heap) *load_permutation)\n+{\n+  int i = 0, j, prev = -1, next, k;\n+  bool supported;\n+\n+  /* FORNOW: permutations are only supported for loop-aware SLP.  */\n+  if (!slp_instn)\n+    return false;\n+\n+  if (vect_print_dump_info (REPORT_SLP))\n+    {\n+      fprintf (vect_dump, \"Load permutation \");\n+      for (i = 0; VEC_iterate (int, load_permutation, i, next); i++)\n+        fprintf (vect_dump, \"%d \", next);\n+    }\n+\n+  /* FORNOW: the only supported permutation is 0..01..1.. of length equal to \n+     GROUP_SIZE and where each sequence of same drs is of GROUP_SIZE length as \n+     well.  */\n+  if (VEC_length (int, load_permutation)\n+      != (unsigned int) (group_size * group_size))\n+    return false;\n+\n+  supported = true;\n+  for (j = 0; j < group_size; j++)\n+    {\n+      for (i = j * group_size, k = 0;\n+           VEC_iterate (int, load_permutation, i, next) && k < group_size;\n+           i++, k++)\n+       {\n+         if (i != j * group_size && next != prev)\n+          {\n+            supported = false;\n+            break;\n+          }\n+\n+         prev = next;\n+       }  \n+    }\n+\n+  if (supported && i == group_size * group_size\n+      && vect_supported_slp_permutation_p (slp_instn))\n+    return true;\n+\n+  return false; \n+}\n+\n+\n+/* Find the first load in the loop that belongs to INSTANCE. \n+   When loads are in several SLP nodes, there can be a case in which the first\n+   load does not appear in the first SLP node to be transformed, causing \n+   incorrect order of statements. Since we generate all the loads together,\n+   they must be inserted before the first load of the SLP instance and not\n+   before the first load of the first node of the instance.  */\n+static gimple \n+vect_find_first_load_in_slp_instance (slp_instance instance) \n+{\n+  int i, j;\n+  slp_tree load_node;\n+  gimple first_load = NULL, load;\n+\n+  for (i = 0; \n+       VEC_iterate (slp_tree, SLP_INSTANCE_LOADS (instance), i, load_node); \n+       i++)\n+    for (j = 0; \n+         VEC_iterate (gimple, SLP_TREE_SCALAR_STMTS (load_node), j, load);\n+         j++)\n+      first_load = get_earlier_stmt (load, first_load);\n+  \n+  return first_load;\n+}\n+\n+\n+/* Analyze an SLP instance starting from a group of strided stores. Call\n+   vect_build_slp_tree to build a tree of packed stmts if possible.  \n+   Return FALSE if it's impossible to SLP any stmt in the loop.  */\n+\n+static bool\n+vect_analyze_slp_instance (loop_vec_info loop_vinfo, gimple stmt)\n+{\n+  slp_instance new_instance;\n+  slp_tree node = XNEW (struct _slp_tree);\n+  unsigned int group_size = DR_GROUP_SIZE (vinfo_for_stmt (stmt));\n+  unsigned int unrolling_factor = 1, nunits;\n+  tree vectype, scalar_type;\n+  gimple next;\n+  unsigned int vectorization_factor = 0, ncopies;\n+  bool slp_impossible = false; \n+  int inside_cost = 0, outside_cost = 0, ncopies_for_cost;\n+  unsigned int max_nunits = 0;\n+  VEC (int, heap) *load_permutation;\n+  VEC (slp_tree, heap) *loads;\n+ \n+  scalar_type = TREE_TYPE (DR_REF (STMT_VINFO_DATA_REF (\n+                                             vinfo_for_stmt (stmt))));\n+  vectype = get_vectype_for_scalar_type (scalar_type);\n+  if (!vectype)\n+    {\n+      if (vect_print_dump_info (REPORT_SLP))\n+        {\n+          fprintf (vect_dump, \"Build SLP failed: unsupported data-type \");\n+          print_generic_expr (vect_dump, scalar_type, TDF_SLIM);\n+        }\n+      return false;\n+    }\n+\n+  nunits = TYPE_VECTOR_SUBPARTS (vectype);\n+  vectorization_factor = LOOP_VINFO_VECT_FACTOR (loop_vinfo);\n+  ncopies = vectorization_factor / nunits;\n+\n+  /* Create a node (a root of the SLP tree) for the packed strided stores.  */ \n+  SLP_TREE_SCALAR_STMTS (node) = VEC_alloc (gimple, heap, group_size);\n+  next = stmt;\n+  /* Collect the stores and store them in SLP_TREE_SCALAR_STMTS.  */\n+  while (next)\n+    {\n+      VEC_safe_push (gimple, heap, SLP_TREE_SCALAR_STMTS (node), next);\n+      next = DR_GROUP_NEXT_DR (vinfo_for_stmt (next));\n+    }\n+\n+  SLP_TREE_VEC_STMTS (node) = NULL;\n+  SLP_TREE_NUMBER_OF_VEC_STMTS (node) = 0;\n+  SLP_TREE_LEFT (node) = NULL;\n+  SLP_TREE_RIGHT (node) = NULL;\n+  SLP_TREE_OUTSIDE_OF_LOOP_COST (node) = 0;\n+  SLP_TREE_INSIDE_OF_LOOP_COST (node) = 0;\n+\n+  /* Calculate the unrolling factor.  */\n+  unrolling_factor = least_common_multiple (nunits, group_size) / group_size;\n+\t\n+  /* Calculate the number of vector stmts to create based on the unrolling\n+     factor (number of vectors is 1 if NUNITS >= GROUP_SIZE, and is\n+     GROUP_SIZE / NUNITS otherwise.  */\n+  ncopies_for_cost = unrolling_factor * group_size / nunits;\n+  \n+  load_permutation = VEC_alloc (int, heap, group_size * group_size); \n+  loads = VEC_alloc (slp_tree, heap, group_size); \n+\n+  /* Build the tree for the SLP instance.  */\n+  if (vect_build_slp_tree (loop_vinfo, &node, group_size, &inside_cost,  \n+\t\t\t   &outside_cost, ncopies_for_cost, &max_nunits,\n+                           &load_permutation, &loads))\n+    {\n+      /* Create a new SLP instance.  */  \n+      new_instance = XNEW (struct _slp_instance);\n+      SLP_INSTANCE_TREE (new_instance) = node;\n+      SLP_INSTANCE_GROUP_SIZE (new_instance) = group_size;\n+      /* Calculate the unrolling factor based on the smallest type in the\n+         loop.  */\n+      if (max_nunits > nunits)\n+        unrolling_factor = least_common_multiple (max_nunits, group_size)\n+                           / group_size;\n+\n+      SLP_INSTANCE_UNROLLING_FACTOR (new_instance) = unrolling_factor;\n+      SLP_INSTANCE_OUTSIDE_OF_LOOP_COST (new_instance) = outside_cost;\n+      SLP_INSTANCE_INSIDE_OF_LOOP_COST (new_instance) = inside_cost;\n+      SLP_INSTANCE_LOADS (new_instance) = loads;\n+      SLP_INSTANCE_FIRST_LOAD_STMT (new_instance) = NULL;\n+      SLP_INSTANCE_LOAD_PERMUTATION (new_instance) = load_permutation;\n+      if (VEC_length (slp_tree, loads))\n+        {\n+          if (!vect_supported_load_permutation_p (new_instance, group_size,\n+                                                  load_permutation)) \n+            {\n+              if (vect_print_dump_info (REPORT_SLP))\n+                {\n+                  fprintf (vect_dump, \"Build SLP failed: unsupported load \"\n+                                      \"permutation \");\n+                  print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n+                }\n+\n+              vect_free_slp_instance (new_instance);\n+              return false;\n+            }\n+\n+          SLP_INSTANCE_FIRST_LOAD_STMT (new_instance)\n+             = vect_find_first_load_in_slp_instance (new_instance);\n+        }\n+      else\n+        VEC_free (int, heap, SLP_INSTANCE_LOAD_PERMUTATION (new_instance));\n+\n+      VEC_safe_push (slp_instance, heap, LOOP_VINFO_SLP_INSTANCES (loop_vinfo), \n+\t\t     new_instance);\n+      if (vect_print_dump_info (REPORT_SLP))\n+\tvect_print_slp_tree (node);\n+\n+      return true;\n+    }\n+\n+  /* Failed to SLP.  */\n+  /* Free the allocated memory.  */\n+  vect_free_slp_tree (node);\n+  VEC_free (int, heap, load_permutation);\n+  VEC_free (slp_tree, heap, loads);\n+   \n+  if (slp_impossible)\n+    return false;\n+\n+  /* SLP failed for this instance, but it is still possible to SLP other stmts \n+     in the loop.  */\n+  return true;\n+}\n+\n+\n+/* Check if there are stmts in the loop can be vectorized using SLP. Build SLP\n+   trees of packed scalar stmts if SLP is possible.  */\n+\n+bool\n+vect_analyze_slp (loop_vec_info loop_vinfo)\n+{\n+  unsigned int i;\n+  VEC (gimple, heap) *strided_stores = LOOP_VINFO_STRIDED_STORES (loop_vinfo);\n+  gimple store;\n+\n+  if (vect_print_dump_info (REPORT_SLP))\n+    fprintf (vect_dump, \"=== vect_analyze_slp ===\");\n+\n+  for (i = 0; VEC_iterate (gimple, strided_stores, i, store); i++)\n+    if (!vect_analyze_slp_instance (loop_vinfo, store))\n+      {\n+\t/* SLP failed. No instance can be SLPed in the loop.  */\n+\tif (vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS))\t\n+\t  fprintf (vect_dump, \"SLP failed.\");\n+\n+\treturn false;\n+      }\n+\n+  return true;\n+}\n+\n+\n+/* For each possible SLP instance decide whether to SLP it and calculate overall\n+   unrolling factor needed to SLP the loop.  */\n+\n+void\n+vect_make_slp_decision (loop_vec_info loop_vinfo)\n+{\n+  unsigned int i, unrolling_factor = 1;\n+  VEC (slp_instance, heap) *slp_instances = LOOP_VINFO_SLP_INSTANCES (loop_vinfo);\n+  slp_instance instance;\n+  int decided_to_slp = 0;\n+\n+  if (vect_print_dump_info (REPORT_SLP))\n+    fprintf (vect_dump, \"=== vect_make_slp_decision ===\");\n+\n+  for (i = 0; VEC_iterate (slp_instance, slp_instances, i, instance); i++)\n+    {\n+      /* FORNOW: SLP if you can.  */\n+      if (unrolling_factor < SLP_INSTANCE_UNROLLING_FACTOR (instance))\n+\tunrolling_factor = SLP_INSTANCE_UNROLLING_FACTOR (instance);\n+\n+      /* Mark all the stmts that belong to INSTANCE as PURE_SLP stmts. Later we \n+\t call vect_detect_hybrid_slp () to find stmts that need hybrid SLP and \n+\t loop-based vectorization. Such stmts will be marked as HYBRID.  */\n+      vect_mark_slp_stmts (SLP_INSTANCE_TREE (instance), pure_slp, -1);\n+      decided_to_slp++;\n+    }\n+\n+  LOOP_VINFO_SLP_UNROLLING_FACTOR (loop_vinfo) = unrolling_factor;\n+\n+  if (decided_to_slp && vect_print_dump_info (REPORT_SLP)) \n+    fprintf (vect_dump, \"Decided to SLP %d instances. Unrolling factor %d\", \n+\t     decided_to_slp, unrolling_factor);\n+}\n+\n+\n+/* Find stmts that must be both vectorized and SLPed (since they feed stmts that\n+   can't be SLPed) in the tree rooted at NODE. Mark such stmts as HYBRID.  */\n+\n+static void\n+vect_detect_hybrid_slp_stmts (slp_tree node)\n+{\n+  int i;\n+  gimple stmt;\n+  imm_use_iterator imm_iter;\n+  gimple use_stmt;\n+\n+  if (!node)\n+    return;\n+\n+  for (i = 0; VEC_iterate (gimple, SLP_TREE_SCALAR_STMTS (node), i, stmt); i++)\n+    if (PURE_SLP_STMT (vinfo_for_stmt (stmt))\n+\t&& TREE_CODE (gimple_op (stmt, 0)) == SSA_NAME)\n+      FOR_EACH_IMM_USE_STMT (use_stmt, imm_iter, gimple_op (stmt, 0))\n+\tif (vinfo_for_stmt (use_stmt)\n+\t    && !STMT_SLP_TYPE (vinfo_for_stmt (use_stmt))\n+            && STMT_VINFO_RELEVANT (vinfo_for_stmt (use_stmt)))\n+\t  vect_mark_slp_stmts (node, hybrid, i);\n+\n+  vect_detect_hybrid_slp_stmts (SLP_TREE_LEFT (node));\n+  vect_detect_hybrid_slp_stmts (SLP_TREE_RIGHT (node));\n+}\n+\n+\n+/* Find stmts that must be both vectorized and SLPed.  */\n+\n+void\n+vect_detect_hybrid_slp (loop_vec_info loop_vinfo)\n+{\n+  unsigned int i;\n+  VEC (slp_instance, heap) *slp_instances = LOOP_VINFO_SLP_INSTANCES (loop_vinfo);\n+  slp_instance instance;\n+\n+  if (vect_print_dump_info (REPORT_SLP))\n+    fprintf (vect_dump, \"=== vect_detect_hybrid_slp ===\");\n+\n+  for (i = 0; VEC_iterate (slp_instance, slp_instances, i, instance); i++)\n+    vect_detect_hybrid_slp_stmts (SLP_INSTANCE_TREE (instance));\n+}\n+\n+/* SLP costs are calculated according to SLP instance unrolling factor (i.e., \n+   the number of created vector stmts depends on the unrolling factor). However,\n+   the actual number of vector stmts for every SLP node depends on VF which is\n+   set later in vect_analyze_operations(). Hence, SLP costs should be updated.\n+   In this function we assume that the inside costs calculated in \n+   vect_model_xxx_cost are linear in ncopies.  */\n+\n+void\n+vect_update_slp_costs_according_to_vf (loop_vec_info loop_vinfo)\n+{\n+  unsigned int i, vf = LOOP_VINFO_VECT_FACTOR (loop_vinfo);\n+  VEC (slp_instance, heap) *slp_instances = LOOP_VINFO_SLP_INSTANCES (loop_vinfo);\n+  slp_instance instance;\n+\n+  if (vect_print_dump_info (REPORT_SLP))\n+    fprintf (vect_dump, \"=== vect_update_slp_costs_according_to_vf ===\");\n+\n+  for (i = 0; VEC_iterate (slp_instance, slp_instances, i, instance); i++)\n+    /* We assume that costs are linear in ncopies.  */\n+    SLP_INSTANCE_INSIDE_OF_LOOP_COST (instance) *= vf \n+      / SLP_INSTANCE_UNROLLING_FACTOR (instance);\t  \n+}\n+\n+/* For constant and loop invariant defs of SLP_NODE this function returns \n+   (vector) defs (VEC_OPRNDS) that will be used in the vectorized stmts.  \n+   OP_NUM determines if we gather defs for operand 0 or operand 1 of the scalar\n+   stmts. NUMBER_OF_VECTORS is the number of vector defs to create.  */\n+\n+static void\n+vect_get_constant_vectors (slp_tree slp_node, VEC(tree,heap) **vec_oprnds,\n+\t\t\t   unsigned int op_num, unsigned int number_of_vectors)\n+{\n+  VEC (gimple, heap) *stmts = SLP_TREE_SCALAR_STMTS (slp_node);\n+  gimple stmt = VEC_index (gimple, stmts, 0);\n+  stmt_vec_info stmt_vinfo = vinfo_for_stmt (stmt);\n+  tree vectype = STMT_VINFO_VECTYPE (stmt_vinfo);\n+  int nunits;\n+  tree vec_cst;\n+  tree t = NULL_TREE;\n+  int j, number_of_places_left_in_vector;\n+  tree vector_type;\n+  tree op, vop;\n+  int group_size = VEC_length (gimple, stmts);\n+  unsigned int vec_num, i;\n+  int number_of_copies = 1;\n+  VEC (tree, heap) *voprnds = VEC_alloc (tree, heap, number_of_vectors);\n+  bool constant_p, is_store;\n+\n+  if (STMT_VINFO_DATA_REF (stmt_vinfo))\n+    {\n+      is_store = true;\n+      op = gimple_assign_rhs1 (stmt);\n+    }\n+  else\n+    {\n+      is_store = false;\n+      op = gimple_op (stmt, op_num + 1);\n+    }\n+\n+  if (CONSTANT_CLASS_P (op))\n+    {\n+      vector_type = vectype;\n+      constant_p = true;\n+    }\n+  else\n+    {\n+      vector_type = get_vectype_for_scalar_type (TREE_TYPE (op)); \n+      gcc_assert (vector_type);\n+      constant_p = false;\n+    }\n+\n+  nunits = TYPE_VECTOR_SUBPARTS (vector_type);\n+\n+  /* NUMBER_OF_COPIES is the number of times we need to use the same values in\n+     created vectors. It is greater than 1 if unrolling is performed. \n+\n+     For example, we have two scalar operands, s1 and s2 (e.g., group of\n+     strided accesses of size two), while NUNITS is four (i.e., four scalars\n+     of this type can be packed in a vector). The output vector will contain\n+     two copies of each scalar operand: {s1, s2, s1, s2}. (NUMBER_OF_COPIES\n+     will be 2).\n+\n+     If GROUP_SIZE > NUNITS, the scalars will be split into several vectors \n+     containing the operands.\n+\n+     For example, NUNITS is four as before, and the group size is 8\n+     (s1, s2, ..., s8). We will create two vectors {s1, s2, s3, s4} and\n+     {s5, s6, s7, s8}.  */\n+    \n+  number_of_copies = least_common_multiple (nunits, group_size) / group_size;\n+\n+  number_of_places_left_in_vector = nunits;\n+  for (j = 0; j < number_of_copies; j++)\n+    {\n+      for (i = group_size - 1; VEC_iterate (gimple, stmts, i, stmt); i--)\n+        {\n+          if (is_store)\n+            op = gimple_assign_rhs1 (stmt);\n+          else\n+            op = gimple_op (stmt, op_num + 1);\n+    \n+          /* Create 'vect_ = {op0,op1,...,opn}'.  */\n+          t = tree_cons (NULL_TREE, op, t);\n+\n+          number_of_places_left_in_vector--;\n+\n+          if (number_of_places_left_in_vector == 0)\n+            {\n+              number_of_places_left_in_vector = nunits;\n+\n+\t      if (constant_p)\n+\t\tvec_cst = build_vector (vector_type, t);\n+\t      else\n+\t\tvec_cst = build_constructor_from_list (vector_type, t);\n+              VEC_quick_push (tree, voprnds,\n+                              vect_init_vector (stmt, vec_cst, vector_type, NULL));\n+              t = NULL_TREE;\n+            }\n+        }\n+    }\n+\n+  /* Since the vectors are created in the reverse order, we should invert \n+     them.  */\n+  vec_num = VEC_length (tree, voprnds);\n+  for (j = vec_num - 1; j >= 0; j--)\n+    {\n+      vop = VEC_index (tree, voprnds, j);\n+      VEC_quick_push (tree, *vec_oprnds, vop);\n+    }\n+\n+  VEC_free (tree, heap, voprnds);\n+\n+  /* In case that VF is greater than the unrolling factor needed for the SLP\n+     group of stmts, NUMBER_OF_VECTORS to be created is greater than \n+     NUMBER_OF_SCALARS/NUNITS or NUNITS/NUMBER_OF_SCALARS, and hence we have \n+     to replicate the vectors.  */\n+  while (number_of_vectors > VEC_length (tree, *vec_oprnds))\n+    {\n+      for (i = 0; VEC_iterate (tree, *vec_oprnds, i, vop) && i < vec_num; i++)\n+        VEC_quick_push (tree, *vec_oprnds, vop);\n+    }\n+}\n+\n+\n+/* Get vectorized definitions from SLP_NODE that contains corresponding\n+   vectorized def-stmts.  */\n+\n+static void\n+vect_get_slp_vect_defs (slp_tree slp_node, VEC (tree,heap) **vec_oprnds)\n+{\n+  tree vec_oprnd;\n+  gimple vec_def_stmt;\n+  unsigned int i;\n+\n+  gcc_assert (SLP_TREE_VEC_STMTS (slp_node));\n+\n+  for (i = 0;\n+       VEC_iterate (gimple, SLP_TREE_VEC_STMTS (slp_node), i, vec_def_stmt);\n+       i++)\n+    {\n+      gcc_assert (vec_def_stmt);\n+      vec_oprnd = gimple_get_lhs (vec_def_stmt);\n+      VEC_quick_push (tree, *vec_oprnds, vec_oprnd);\n+    }\n+}\n+\n+\n+/* Get vectorized definitions for SLP_NODE. \n+   If the scalar definitions are loop invariants or constants, collect them and \n+   call vect_get_constant_vectors() to create vector stmts.\n+   Otherwise, the def-stmts must be already vectorized and the vectorized stmts\n+   must be stored in the LEFT/RIGHT node of SLP_NODE, and we call\n+   vect_get_slp_vect_defs() to retrieve them.  \n+   If VEC_OPRNDS1 is NULL, don't get vector defs for the second operand (from\n+   the right node. This is used when the second operand must remain scalar.  */ \n+ \n+void\n+vect_get_slp_defs (slp_tree slp_node, VEC (tree,heap) **vec_oprnds0,\n+                   VEC (tree,heap) **vec_oprnds1)\n+{\n+  gimple first_stmt;\n+  enum tree_code code;\n+  int number_of_vects;\n+  HOST_WIDE_INT lhs_size_unit, rhs_size_unit; \n+\n+  first_stmt = VEC_index (gimple, SLP_TREE_SCALAR_STMTS (slp_node), 0);\n+  /* The number of vector defs is determined by the number of vector statements\n+     in the node from which we get those statements.  */\n+  if (SLP_TREE_LEFT (slp_node)) \n+    number_of_vects = SLP_TREE_NUMBER_OF_VEC_STMTS (SLP_TREE_LEFT (slp_node));\n+  else\n+    {\n+      number_of_vects = SLP_TREE_NUMBER_OF_VEC_STMTS (slp_node);\n+      /* Number of vector stmts was calculated according to LHS in\n+         vect_schedule_slp_instance(), fix it by replacing LHS with RHS, if\n+         necessary. See vect_get_smallest_scalar_type() for details.  */\n+      vect_get_smallest_scalar_type (first_stmt, &lhs_size_unit,\n+                                     &rhs_size_unit);\n+      if (rhs_size_unit != lhs_size_unit)\n+        {\n+          number_of_vects *= rhs_size_unit;\n+          number_of_vects /= lhs_size_unit;\n+        }\n+    }\n+\n+  /* Allocate memory for vectorized defs.  */\n+  *vec_oprnds0 = VEC_alloc (tree, heap, number_of_vects);\n+\n+  /* SLP_NODE corresponds either to a group of stores or to a group of\n+     unary/binary operations. We don't call this function for loads.  */\n+  if (SLP_TREE_LEFT (slp_node))\n+    /* The defs are already vectorized.  */\n+    vect_get_slp_vect_defs (SLP_TREE_LEFT (slp_node), vec_oprnds0);\n+  else\n+    /* Build vectors from scalar defs.  */\n+    vect_get_constant_vectors (slp_node, vec_oprnds0, 0, number_of_vects);\n+\n+  if (STMT_VINFO_DATA_REF (vinfo_for_stmt (first_stmt)))\n+    /* Since we don't call this function with loads, this is a group of\n+       stores.  */\n+    return;\n+\n+  code = gimple_assign_rhs_code (first_stmt);\n+  if (get_gimple_rhs_class (code) != GIMPLE_BINARY_RHS || !vec_oprnds1)\n+    return;\n+\n+  /* The number of vector defs is determined by the number of vector statements\n+     in the node from which we get those statements.  */\n+  if (SLP_TREE_RIGHT (slp_node))\n+    number_of_vects = SLP_TREE_NUMBER_OF_VEC_STMTS (SLP_TREE_RIGHT (slp_node));\n+  else\n+    number_of_vects = SLP_TREE_NUMBER_OF_VEC_STMTS (slp_node);\n+\n+  *vec_oprnds1 = VEC_alloc (tree, heap, number_of_vects);\n+\n+  if (SLP_TREE_RIGHT (slp_node))\n+    /* The defs are already vectorized.  */\n+    vect_get_slp_vect_defs (SLP_TREE_RIGHT (slp_node), vec_oprnds1);\n+  else\n+    /* Build vectors from scalar defs.  */\n+    vect_get_constant_vectors (slp_node, vec_oprnds1, 1, number_of_vects);\n+}\n+\n+/* Create NCOPIES permutation statements using the mask MASK_BYTES (by \n+   building a vector of type MASK_TYPE from it) and two input vectors placed in\n+   DR_CHAIN at FIRST_VEC_INDX and SECOND_VEC_INDX for the first copy and\n+   shifting by STRIDE elements of DR_CHAIN for every copy.\n+   (STRIDE is the number of vectorized stmts for NODE divided by the number of\n+   copies).  \n+   VECT_STMTS_COUNTER specifies the index in the vectorized stmts of NODE, where\n+   the created stmts must be inserted.  */\n+\n+static inline void\n+vect_create_mask_and_perm (gimple stmt, gimple next_scalar_stmt, \n+                           int *mask_array, int mask_nunits, \n+                           tree mask_element_type, tree mask_type,\n+                           int first_vec_indx, int second_vec_indx, \n+                           gimple_stmt_iterator *gsi, slp_tree node, \n+                           tree builtin_decl, tree vectype, \n+                           VEC(tree,heap) *dr_chain,\n+                           int ncopies, int vect_stmts_counter)\n+{\n+  tree t = NULL_TREE, mask_vec, mask, perm_dest;\n+  gimple perm_stmt = NULL;\n+  stmt_vec_info next_stmt_info;\n+  int i, group_size, stride, dr_chain_size;\n+  tree first_vec, second_vec, data_ref;\n+  tree sym;\n+  ssa_op_iter iter;\n+  VEC (tree, heap) *params = NULL;\n+\n+  /* Create a vector mask.  */\n+  for (i = mask_nunits - 1; i >= 0; --i)\n+    t = tree_cons (NULL_TREE, build_int_cst (mask_element_type, mask_array[i]),\n+                   t);\n+  mask_vec = build_vector (mask_type, t);\n+  mask = vect_init_vector (stmt, mask_vec, mask_type, NULL);\n+\n+  group_size = VEC_length (gimple, SLP_TREE_SCALAR_STMTS (node));\n+  stride = SLP_TREE_NUMBER_OF_VEC_STMTS (node) / ncopies;\n+  dr_chain_size = VEC_length (tree, dr_chain); \n+\n+  /* Initialize the vect stmts of NODE to properly insert the generated \n+     stmts later.  */\n+  for (i = VEC_length (gimple, SLP_TREE_VEC_STMTS (node)); \n+       i < (int) SLP_TREE_NUMBER_OF_VEC_STMTS (node); i++)\n+    VEC_quick_push (gimple, SLP_TREE_VEC_STMTS (node), NULL);\n+\n+  perm_dest = vect_create_destination_var (gimple_assign_lhs (stmt), vectype);\n+  for (i = 0; i < ncopies; i++)\n+    {\n+      first_vec = VEC_index (tree, dr_chain, first_vec_indx);\n+      second_vec = VEC_index (tree, dr_chain, second_vec_indx);\n+\n+      /* Build argument list for the vectorized call.  */\n+      VEC_free (tree, heap, params);\n+      params = VEC_alloc (tree, heap, 3);\n+      VEC_quick_push (tree, params, first_vec);\n+      VEC_quick_push (tree, params, second_vec);\n+      VEC_quick_push (tree, params, mask);\n+\n+      /* Generate the permute statement.  */\n+      perm_stmt = gimple_build_call_vec (builtin_decl, params);\n+      data_ref = make_ssa_name (perm_dest, perm_stmt);\n+      gimple_call_set_lhs (perm_stmt, data_ref);\n+      vect_finish_stmt_generation (stmt, perm_stmt, gsi);\n+      FOR_EACH_SSA_TREE_OPERAND (sym, perm_stmt, iter, SSA_OP_ALL_VIRTUALS)\n+        {\n+          if (TREE_CODE (sym) == SSA_NAME)\n+            sym = SSA_NAME_VAR (sym);\n+          mark_sym_for_renaming (sym);\n+        }\n+\n+      /* Store the vector statement in NODE.  */ \n+      VEC_replace (gimple, SLP_TREE_VEC_STMTS (node), \n+                   stride * i + vect_stmts_counter, perm_stmt);\n+\n+      first_vec_indx += stride;\n+      second_vec_indx += stride;\n+    }\n+\n+  /* Mark the scalar stmt as vectorized.  */\n+  next_stmt_info = vinfo_for_stmt (next_scalar_stmt);\n+  STMT_VINFO_VEC_STMT (next_stmt_info) = perm_stmt;\n+}\n+\n+\n+/* Given FIRST_MASK_ELEMENT - the mask element in element representation, \n+   return in CURRENT_MASK_ELEMENT its equivalent in target specific\n+   representation. Check that the mask is valid and return FALSE if not. \n+   Return TRUE in NEED_NEXT_VECTOR if the permutation requires to move to\n+   the next vector, i.e., the current first vector is not needed.  */\n+   \n+static bool\n+vect_get_mask_element (gimple stmt, int first_mask_element, int m, \n+                       int mask_nunits, bool only_one_vec, int index,\n+                       int *mask, int *current_mask_element, \n+                       bool *need_next_vector)\n+{\n+  int i;\n+  static int number_of_mask_fixes = 1;\n+  static bool mask_fixed = false;\n+  static bool needs_first_vector = false;\n+\n+  /* Convert to target specific representation.  */\n+  *current_mask_element = first_mask_element + m;\n+  /* Adjust the value in case it's a mask for second and third vectors.  */\n+  *current_mask_element -= mask_nunits * (number_of_mask_fixes - 1);\n+\n+  if (*current_mask_element < mask_nunits)\n+    needs_first_vector = true;\n+\n+  /* We have only one input vector to permute but the mask accesses values in\n+     the next vector as well.  */\n+  if (only_one_vec && *current_mask_element >= mask_nunits)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS))\n+        {\n+          fprintf (vect_dump, \"permutation requires at least two vectors \");\n+          print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n+        }\n+\n+      return false;\n+    }\n+\n+  /* The mask requires the next vector.  */\n+  if (*current_mask_element >= mask_nunits * 2)\n+    {\n+      if (needs_first_vector || mask_fixed)\n+        {\n+          /* We either need the first vector too or have already moved to the\n+             next vector. In both cases, this permutation needs three   \n+             vectors.  */\n+          if (vect_print_dump_info (REPORT_DETAILS))\n+            {\n+              fprintf (vect_dump, \"permutation requires at \"\n+                                  \"least three vectors \");\n+              print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n+            }\n+\n+          return false;\n+        }\n+\n+      /* We move to the next vector, dropping the first one and working with\n+         the second and the third - we need to adjust the values of the mask\n+         accordingly.  */\n+      *current_mask_element -= mask_nunits * number_of_mask_fixes;\n+\n+      for (i = 0; i < index; i++)\n+        mask[i] -= mask_nunits * number_of_mask_fixes;\n+\n+      (number_of_mask_fixes)++;\n+      mask_fixed = true;\n+    }\n+\n+  *need_next_vector = mask_fixed;\n+\n+  /* This was the last element of this mask. Start a new one.  */\n+  if (index == mask_nunits - 1)\n+    {\n+      number_of_mask_fixes = 1;\n+      mask_fixed = false;\n+      needs_first_vector = false;\n+    }\n+\n+  return true;\n+}\n+\n+\n+/* Generate vector permute statements from a list of loads in DR_CHAIN.\n+   If ANALYZE_ONLY is TRUE, only check that it is possible to create valid\n+   permute statements for SLP_NODE_INSTANCE.  */\n+bool\n+vect_transform_slp_perm_load (gimple stmt, VEC (tree, heap) *dr_chain,\n+                              gimple_stmt_iterator *gsi, int vf,\n+                              slp_instance slp_node_instance, bool analyze_only)\n+{\n+  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n+  tree mask_element_type = NULL_TREE, mask_type;\n+  int i, j, k, m, scale, mask_nunits, nunits, vec_index = 0, scalar_index;\n+  slp_tree node;\n+  tree vectype = STMT_VINFO_VECTYPE (stmt_info), builtin_decl;\n+  gimple next_scalar_stmt;\n+  int group_size = SLP_INSTANCE_GROUP_SIZE (slp_node_instance);\n+  int first_mask_element;\n+  int index, unroll_factor, *mask, current_mask_element, ncopies;\n+  bool only_one_vec = false, need_next_vector = false;\n+  int first_vec_index, second_vec_index, orig_vec_stmts_num, vect_stmts_counter;\n+\n+  if (!targetm.vectorize.builtin_vec_perm)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS))\n+        {\n+          fprintf (vect_dump, \"no builtin for vect permute for \");\n+          print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n+        }\n+\n+       return false;\n+    }\n+\n+  builtin_decl = targetm.vectorize.builtin_vec_perm (vectype,\n+                                                     &mask_element_type);\n+  if (!builtin_decl || !mask_element_type)\n+    {\n+      if (vect_print_dump_info (REPORT_DETAILS))\n+        {\n+          fprintf (vect_dump, \"no builtin for vect permute for \");\n+          print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n+        }\n+\n+       return false;\n+    }\n+\n+  mask_type = get_vectype_for_scalar_type (mask_element_type);\n+  mask_nunits = TYPE_VECTOR_SUBPARTS (mask_type);\n+  mask = (int *) xmalloc (sizeof (int) * mask_nunits);\n+  nunits = TYPE_VECTOR_SUBPARTS (vectype);\n+  scale = mask_nunits / nunits;\n+  unroll_factor = SLP_INSTANCE_UNROLLING_FACTOR (slp_node_instance);\n+\n+  /* The number of vector stmts to generate based only on SLP_NODE_INSTANCE\n+     unrolling factor.  */\n+  orig_vec_stmts_num = group_size * \n+                SLP_INSTANCE_UNROLLING_FACTOR (slp_node_instance) / nunits;\n+  if (orig_vec_stmts_num == 1)\n+    only_one_vec = true;\n+\n+  /* Number of copies is determined by the final vectorization factor \n+     relatively to SLP_NODE_INSTANCE unrolling factor.  */\n+  ncopies = vf / SLP_INSTANCE_UNROLLING_FACTOR (slp_node_instance); \n+\n+  /* Generate permutation masks for every NODE. Number of masks for each NODE \n+     is equal to GROUP_SIZE.  \n+     E.g., we have a group of three nodes with three loads from the same \n+     location in each node, and the vector size is 4. I.e., we have a \n+     a0b0c0a1b1c1... sequence and we need to create the following vectors: \n+     for a's: a0a0a0a1 a1a1a2a2 a2a3a3a3\n+     for b's: b0b0b0b1 b1b1b2b2 b2b3b3b3\n+     ...\n+\n+     The masks for a's should be: {0,0,0,3} {3,3,6,6} {6,9,9,9} (in target\n+     scpecific type, e.g., in bytes for Altivec.\n+     The last mask is illegal since we assume two operands for permute \n+     operation, and the mask element values can't be outside that range. Hence,\n+     the last mask must be converted into {2,5,5,5}.\n+     For the first two permutations we need the first and the second input \n+     vectors: {a0,b0,c0,a1} and {b1,c1,a2,b2}, and for the last permutation\n+     we need the second and the third vectors: {b1,c1,a2,b2} and \n+     {c2,a3,b3,c3}.  */\n+\n+  for (i = 0;\n+       VEC_iterate (slp_tree, SLP_INSTANCE_LOADS (slp_node_instance),\n+                    i, node);\n+       i++)\n+    {\n+      scalar_index = 0;\n+      index = 0;\n+      vect_stmts_counter = 0;\n+      vec_index = 0;\n+      first_vec_index = vec_index++;\n+      if (only_one_vec)\n+        second_vec_index = first_vec_index;\n+      else\n+        second_vec_index =  vec_index++;\n+\n+      for (j = 0; j < unroll_factor; j++)\n+        {\n+          for (k = 0; k < group_size; k++)\n+            {\n+              first_mask_element = (i + j * group_size) * scale;\n+              for (m = 0; m < scale; m++)\n+                {\n+                  if (!vect_get_mask_element (stmt, first_mask_element, m, \n+                                   mask_nunits, only_one_vec, index, mask,\n+                                   &current_mask_element, &need_next_vector))\n+                    return false;\n+\n+                  mask[index++] = current_mask_element;\n+                } \n+\n+              if (index == mask_nunits)\n+                {\n+                  index = 0;\n+                  if (!analyze_only)\n+                    {\n+                      if (need_next_vector)\n+                        {\n+                          first_vec_index = second_vec_index;\n+                          second_vec_index = vec_index;\n+                        }\n+\n+                      next_scalar_stmt = VEC_index (gimple,\n+                                SLP_TREE_SCALAR_STMTS (node), scalar_index++);\n+\n+                      vect_create_mask_and_perm (stmt, next_scalar_stmt,\n+                               mask, mask_nunits, mask_element_type, mask_type, \n+                               first_vec_index, second_vec_index, gsi, node, \n+                               builtin_decl, vectype, dr_chain, ncopies, \n+                               vect_stmts_counter++);\n+                    }\n+                } \n+            } \n+        } \n+    } \n+\n+  free (mask);\n+  return true;\n+}\n+\n+\n+\n+/* Vectorize SLP instance tree in postorder.  */\n+\n+static bool\n+vect_schedule_slp_instance (slp_tree node, slp_instance instance,\n+                            unsigned int vectorization_factor) \n+{\n+  gimple stmt;\n+  bool strided_store, is_store;\n+  gimple_stmt_iterator si;\n+  stmt_vec_info stmt_info;\n+  unsigned int vec_stmts_size, nunits, group_size;\n+  tree vectype;\n+  int i;\n+  slp_tree loads_node;\n+\n+  if (!node)\n+    return false;\n+\n+  vect_schedule_slp_instance (SLP_TREE_LEFT (node), instance,\n+                              vectorization_factor);\n+  vect_schedule_slp_instance (SLP_TREE_RIGHT (node), instance,\n+                              vectorization_factor);\n+  \n+  stmt = VEC_index (gimple, SLP_TREE_SCALAR_STMTS (node), 0);\n+  stmt_info = vinfo_for_stmt (stmt);\n+\n+  /* VECTYPE is the type of the destination.  */\n+  vectype = get_vectype_for_scalar_type (TREE_TYPE (gimple_assign_lhs (stmt)));\n+  nunits = (unsigned int) TYPE_VECTOR_SUBPARTS (vectype);\n+  group_size = SLP_INSTANCE_GROUP_SIZE (instance);\n+\n+  /* For each SLP instance calculate number of vector stmts to be created\n+     for the scalar stmts in each node of the SLP tree. Number of vector\n+     elements in one vector iteration is the number of scalar elements in\n+     one scalar iteration (GROUP_SIZE) multiplied by VF divided by vector\n+     size.  */\n+  vec_stmts_size = (vectorization_factor * group_size) / nunits;\n+\n+  /* In case of load permutation we have to allocate vectorized statements for\n+     all the nodes that participate in that permutation.  */\n+  if (SLP_INSTANCE_LOAD_PERMUTATION (instance))\n+    {\n+      for (i = 0;\n+           VEC_iterate (slp_tree, SLP_INSTANCE_LOADS (instance), i, loads_node);\n+           i++)\n+        {\n+          if (!SLP_TREE_VEC_STMTS (loads_node))\n+            {\n+              SLP_TREE_VEC_STMTS (loads_node) = VEC_alloc (gimple, heap,\n+                                                           vec_stmts_size);\n+              SLP_TREE_NUMBER_OF_VEC_STMTS (loads_node) = vec_stmts_size;\n+            }\n+        }\n+    }\n+\n+  if (!SLP_TREE_VEC_STMTS (node))\n+    {\n+      SLP_TREE_VEC_STMTS (node) = VEC_alloc (gimple, heap, vec_stmts_size);\n+      SLP_TREE_NUMBER_OF_VEC_STMTS (node) = vec_stmts_size;\n+    }\n+\n+  if (vect_print_dump_info (REPORT_DETAILS))\n+    {\n+      fprintf (vect_dump, \"------>vectorizing SLP node starting from: \");\n+      print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n+    }\t\n+\n+  /* Loads should be inserted before the first load.  */\n+  if (SLP_INSTANCE_FIRST_LOAD_STMT (instance)\n+      && STMT_VINFO_STRIDED_ACCESS (stmt_info)\n+      && !REFERENCE_CLASS_P (gimple_get_lhs (stmt)))\n+    si = gsi_for_stmt (SLP_INSTANCE_FIRST_LOAD_STMT (instance));\n+  else\n+    si = gsi_for_stmt (stmt);\n+\n+  is_store = vect_transform_stmt (stmt, &si, &strided_store, node, instance);\n+  if (is_store)\n+    {\n+      if (DR_GROUP_FIRST_DR (stmt_info))\n+\t/* If IS_STORE is TRUE, the vectorization of the\n+\t   interleaving chain was completed - free all the stores in\n+\t   the chain.  */\n+\tvect_remove_stores (DR_GROUP_FIRST_DR (stmt_info));\n+      else\n+\t/* FORNOW: SLP originates only from strided stores.  */\n+\tgcc_unreachable ();\n+\n+      return true;\n+    }\n+\n+  /* FORNOW: SLP originates only from strided stores.  */\n+  return false;\n+}\n+\n+\n+bool\n+vect_schedule_slp (loop_vec_info loop_vinfo)\n+{\n+  VEC (slp_instance, heap) *slp_instances = \n+    LOOP_VINFO_SLP_INSTANCES (loop_vinfo);\n+  slp_instance instance;\n+  unsigned int i;\n+  bool is_store = false;\n+\n+  for (i = 0; VEC_iterate (slp_instance, slp_instances, i, instance); i++)\n+    {\n+      /* Schedule the tree of INSTANCE.  */\n+      is_store = vect_schedule_slp_instance (SLP_INSTANCE_TREE (instance),\n+                            instance, LOOP_VINFO_VECT_FACTOR (loop_vinfo));\n+\t\t\t  \n+      if (vect_print_dump_info (REPORT_VECTORIZED_LOOPS)\n+\t  || vect_print_dump_info (REPORT_UNVECTORIZED_LOOPS))\n+\tfprintf (vect_dump, \"vectorizing stmts using SLP.\");\n+    }\n+\n+  return is_store;\n+}"}, {"sha": "8efe4d39f539de1e5375ce27d6a305cd2e8dcb5c", "filename": "gcc/tree-vect-stmts.c", "status": "added", "additions": 4928, "deletions": 0, "changes": 4928, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vect-stmts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vect-stmts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-stmts.c?ref=ebfd146af752ce6aa298f866e36f9a7e66e32ec5"}, {"sha": "a048342d8be6a1cefde4d2fa11dc72e4db3387b9", "filename": "gcc/tree-vect-transform.c", "status": "removed", "additions": 0, "deletions": 8524, "changes": 8524, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/40a1cfba7818100adbde7144be9f6515b9a6ed86/gcc%2Ftree-vect-transform.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/40a1cfba7818100adbde7144be9f6515b9a6ed86/gcc%2Ftree-vect-transform.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-transform.c?ref=40a1cfba7818100adbde7144be9f6515b9a6ed86"}, {"sha": "0636c6adbc67a9b3fbac3e536c01687b188385f0", "filename": "gcc/tree-vectorizer.c", "status": "modified", "additions": 138, "deletions": 2740, "changes": 2878, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vectorizer.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vectorizer.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.c?ref=ebfd146af752ce6aa298f866e36f9a7e66e32ec5", "patch": "@@ -1,7 +1,7 @@\n-/* Loop Vectorization\n-   Copyright (C) 2003, 2004, 2005, 2006, 2007, 2008 Free Software\n+/* Vectorizer\n+   Copyright (C) 2003, 2004, 2005, 2006, 2007, 2008, 2009 Free Software\n    Foundation, Inc.\n-   Contributed by Dorit Naishlos <dorit@il.ibm.com>\n+   Contributed by Dorit Naishlos <dorit@il.ibm.com> \n \n This file is part of GCC.\n \n@@ -19,2754 +19,147 @@ You should have received a copy of the GNU General Public License\n along with GCC; see the file COPYING3.  If not see\n <http://www.gnu.org/licenses/>.  */\n \n-/* Loop Vectorization Pass.\n-\n-   This pass tries to vectorize loops. This first implementation focuses on\n-   simple inner-most loops, with no conditional control flow, and a set of\n-   simple operations which vector form can be expressed using existing\n-   tree codes (PLUS, MULT etc).\n-\n-   For example, the vectorizer transforms the following simple loop:\n-\n-\tshort a[N]; short b[N]; short c[N]; int i;\n-\n-\tfor (i=0; i<N; i++){\n-\t  a[i] = b[i] + c[i];\n-\t}\n-\n-   as if it was manually vectorized by rewriting the source code into:\n-\n-\ttypedef int __attribute__((mode(V8HI))) v8hi;\n-\tshort a[N];  short b[N]; short c[N];   int i;\n-\tv8hi *pa = (v8hi*)a, *pb = (v8hi*)b, *pc = (v8hi*)c;\n-\tv8hi va, vb, vc;\n-\n-\tfor (i=0; i<N/8; i++){\n-\t  vb = pb[i];\n-\t  vc = pc[i];\n-\t  va = vb + vc;\n-\t  pa[i] = va;\n-\t}\n-\n-\tThe main entry to this pass is vectorize_loops(), in which\n-   the vectorizer applies a set of analyses on a given set of loops,\n-   followed by the actual vectorization transformation for the loops that\n-   had successfully passed the analysis phase.\n-\n-\tThroughout this pass we make a distinction between two types of\n-   data: scalars (which are represented by SSA_NAMES), and memory references\n-   (\"data-refs\"). These two types of data require different handling both \n-   during analysis and transformation. The types of data-refs that the \n-   vectorizer currently supports are ARRAY_REFS which base is an array DECL \n-   (not a pointer), and INDIRECT_REFS through pointers; both array and pointer\n-   accesses are required to have a  simple (consecutive) access pattern.\n-\n-   Analysis phase:\n-   ===============\n-\tThe driver for the analysis phase is vect_analyze_loop_nest().\n-   It applies a set of analyses, some of which rely on the scalar evolution \n-   analyzer (scev) developed by Sebastian Pop.\n-\n-\tDuring the analysis phase the vectorizer records some information\n-   per stmt in a \"stmt_vec_info\" struct which is attached to each stmt in the \n-   loop, as well as general information about the loop as a whole, which is\n-   recorded in a \"loop_vec_info\" struct attached to each loop.\n-\n-   Transformation phase:\n-   =====================\n-\tThe loop transformation phase scans all the stmts in the loop, and\n-   creates a vector stmt (or a sequence of stmts) for each scalar stmt S in\n-   the loop that needs to be vectorized. It insert the vector code sequence\n-   just before the scalar stmt S, and records a pointer to the vector code\n-   in STMT_VINFO_VEC_STMT (stmt_info) (stmt_info is the stmt_vec_info struct \n-   attached to S). This pointer will be used for the vectorization of following\n-   stmts which use the def of stmt S. Stmt S is removed if it writes to memory;\n-   otherwise, we rely on dead code elimination for removing it.\n-\n-\tFor example, say stmt S1 was vectorized into stmt VS1:\n-\n-   VS1: vb = px[i];\n-   S1:\tb = x[i];    STMT_VINFO_VEC_STMT (stmt_info (S1)) = VS1\n-   S2:  a = b;\n-\n-   To vectorize stmt S2, the vectorizer first finds the stmt that defines\n-   the operand 'b' (S1), and gets the relevant vector def 'vb' from the\n-   vector stmt VS1 pointed to by STMT_VINFO_VEC_STMT (stmt_info (S1)). The\n-   resulting sequence would be:\n-\n-   VS1: vb = px[i];\n-   S1:\tb = x[i];\tSTMT_VINFO_VEC_STMT (stmt_info (S1)) = VS1\n-   VS2: va = vb;\n-   S2:  a = b;          STMT_VINFO_VEC_STMT (stmt_info (S2)) = VS2\n-\n-\tOperands that are not SSA_NAMEs, are data-refs that appear in \n-   load/store operations (like 'x[i]' in S1), and are handled differently.\n-\n-   Target modeling:\n-   =================\n-\tCurrently the only target specific information that is used is the\n-   size of the vector (in bytes) - \"UNITS_PER_SIMD_WORD\". Targets that can \n-   support different sizes of vectors, for now will need to specify one value \n-   for \"UNITS_PER_SIMD_WORD\". More flexibility will be added in the future.\n-\n-\tSince we only vectorize operations which vector form can be\n-   expressed using existing tree codes, to verify that an operation is\n-   supported, the vectorizer checks the relevant optab at the relevant\n-   machine_mode (e.g, optab_handler (add_optab, V8HImode)->insn_code). If\n-   the value found is CODE_FOR_nothing, then there's no target support, and\n-   we can't vectorize the stmt.\n-\n-   For additional information on this project see:\n-   http://gcc.gnu.org/projects/tree-ssa/vectorization.html\n-*/\n-\n-#include \"config.h\"\n-#include \"system.h\"\n-#include \"coretypes.h\"\n-#include \"tm.h\"\n-#include \"ggc.h\"\n-#include \"tree.h\"\n-#include \"target.h\"\n-#include \"rtl.h\"\n-#include \"basic-block.h\"\n-#include \"diagnostic.h\"\n-#include \"tree-flow.h\"\n-#include \"tree-dump.h\"\n-#include \"timevar.h\"\n-#include \"cfgloop.h\"\n-#include \"cfglayout.h\"\n-#include \"expr.h\"\n-#include \"recog.h\"\n-#include \"optabs.h\"\n-#include \"params.h\"\n-#include \"toplev.h\"\n-#include \"tree-chrec.h\"\n-#include \"tree-data-ref.h\"\n-#include \"tree-scalar-evolution.h\"\n-#include \"input.h\"\n-#include \"hashtab.h\"\n-#include \"tree-vectorizer.h\"\n-#include \"tree-pass.h\"\n-#include \"langhooks.h\"\n-\n-/*************************************************************************\n-  General Vectorization Utilities\n- *************************************************************************/\n-\n-/* vect_dump will be set to stderr or dump_file if exist.  */\n-FILE *vect_dump;\n-\n-/* vect_verbosity_level set to an invalid value \n-   to mark that it's uninitialized.  */\n-enum verbosity_levels vect_verbosity_level = MAX_VERBOSITY_LEVEL;\n-\n-/* Loop location.  */\n-static LOC vect_loop_location;\n-\n-/* Bitmap of virtual variables to be renamed.  */\n-bitmap vect_memsyms_to_rename;\n-\n-/* Vector mapping GIMPLE stmt to stmt_vec_info. */\n-VEC(vec_void_p,heap) *stmt_vec_info_vec;\n-\n-\f\n-/*************************************************************************\n-  Simple Loop Peeling Utilities\n-\n-  Utilities to support loop peeling for vectorization purposes.\n- *************************************************************************/\n-\n-\n-/* Renames the use *OP_P.  */\n-\n-static void\n-rename_use_op (use_operand_p op_p)\n-{\n-  tree new_name;\n-\n-  if (TREE_CODE (USE_FROM_PTR (op_p)) != SSA_NAME)\n-    return;\n-\n-  new_name = get_current_def (USE_FROM_PTR (op_p));\n-\n-  /* Something defined outside of the loop.  */\n-  if (!new_name)\n-    return;\n-\n-  /* An ordinary ssa name defined in the loop.  */\n-\n-  SET_USE (op_p, new_name);\n-}\n-\n-\n-/* Renames the variables in basic block BB.  */\n-\n-void\n-rename_variables_in_bb (basic_block bb)\n-{\n-  gimple_stmt_iterator gsi;\n-  gimple stmt;\n-  use_operand_p use_p;\n-  ssa_op_iter iter;\n-  edge e;\n-  edge_iterator ei;\n-  struct loop *loop = bb->loop_father;\n-\n-  for (gsi = gsi_start_bb (bb); !gsi_end_p (gsi); gsi_next (&gsi))\n-    {\n-      stmt = gsi_stmt (gsi);\n-      FOR_EACH_SSA_USE_OPERAND (use_p, stmt, iter, SSA_OP_ALL_USES)\n-\trename_use_op (use_p);\n-    }\n-\n-  FOR_EACH_EDGE (e, ei, bb->succs)\n-    {\n-      if (!flow_bb_inside_loop_p (loop, e->dest))\n-\tcontinue;\n-      for (gsi = gsi_start_phis (e->dest); !gsi_end_p (gsi); gsi_next (&gsi))\n-        rename_use_op (PHI_ARG_DEF_PTR_FROM_EDGE (gsi_stmt (gsi), e));\n-    }\n-}\n-\n-\n-/* Renames variables in new generated LOOP.  */\n-\n-void\n-rename_variables_in_loop (struct loop *loop)\n-{\n-  unsigned i;\n-  basic_block *bbs;\n-\n-  bbs = get_loop_body (loop);\n-\n-  for (i = 0; i < loop->num_nodes; i++)\n-    rename_variables_in_bb (bbs[i]);\n-\n-  free (bbs);\n-}\n-\n-\n-/* Update the PHI nodes of NEW_LOOP.\n-\n-   NEW_LOOP is a duplicate of ORIG_LOOP.\n-   AFTER indicates whether NEW_LOOP executes before or after ORIG_LOOP:\n-   AFTER is true if NEW_LOOP executes after ORIG_LOOP, and false if it\n-   executes before it.  */\n-\n-static void\n-slpeel_update_phis_for_duplicate_loop (struct loop *orig_loop,\n-\t\t\t\t       struct loop *new_loop, bool after)\n-{\n-  tree new_ssa_name;\n-  gimple phi_new, phi_orig;\n-  tree def;\n-  edge orig_loop_latch = loop_latch_edge (orig_loop);\n-  edge orig_entry_e = loop_preheader_edge (orig_loop);\n-  edge new_loop_exit_e = single_exit (new_loop);\n-  edge new_loop_entry_e = loop_preheader_edge (new_loop);\n-  edge entry_arg_e = (after ? orig_loop_latch : orig_entry_e);\n-  gimple_stmt_iterator gsi_new, gsi_orig;\n-\n-  /*\n-     step 1. For each loop-header-phi:\n-             Add the first phi argument for the phi in NEW_LOOP\n-            (the one associated with the entry of NEW_LOOP)\n-\n-     step 2. For each loop-header-phi:\n-             Add the second phi argument for the phi in NEW_LOOP\n-            (the one associated with the latch of NEW_LOOP)\n-\n-     step 3. Update the phis in the successor block of NEW_LOOP.\n-\n-        case 1: NEW_LOOP was placed before ORIG_LOOP:\n-                The successor block of NEW_LOOP is the header of ORIG_LOOP.\n-                Updating the phis in the successor block can therefore be done\n-                along with the scanning of the loop header phis, because the\n-                header blocks of ORIG_LOOP and NEW_LOOP have exactly the same\n-                phi nodes, organized in the same order.\n-\n-        case 2: NEW_LOOP was placed after ORIG_LOOP:\n-                The successor block of NEW_LOOP is the original exit block of \n-                ORIG_LOOP - the phis to be updated are the loop-closed-ssa phis.\n-                We postpone updating these phis to a later stage (when\n-                loop guards are added).\n-   */\n-\n-\n-  /* Scan the phis in the headers of the old and new loops\n-     (they are organized in exactly the same order).  */\n-\n-  for (gsi_new = gsi_start_phis (new_loop->header),\n-       gsi_orig = gsi_start_phis (orig_loop->header);\n-       !gsi_end_p (gsi_new) && !gsi_end_p (gsi_orig);\n-       gsi_next (&gsi_new), gsi_next (&gsi_orig))\n-    {\n-      phi_new = gsi_stmt (gsi_new);\n-      phi_orig = gsi_stmt (gsi_orig);\n-\n-      /* step 1.  */\n-      def = PHI_ARG_DEF_FROM_EDGE (phi_orig, entry_arg_e);\n-      add_phi_arg (phi_new, def, new_loop_entry_e);\n-\n-      /* step 2.  */\n-      def = PHI_ARG_DEF_FROM_EDGE (phi_orig, orig_loop_latch);\n-      if (TREE_CODE (def) != SSA_NAME)\n-        continue;\n-\n-      new_ssa_name = get_current_def (def);\n-      if (!new_ssa_name)\n-\t{\n-\t  /* This only happens if there are no definitions\n-\t     inside the loop. use the phi_result in this case.  */\n-\t  new_ssa_name = PHI_RESULT (phi_new);\n-\t}\n-\n-      /* An ordinary ssa name defined in the loop.  */\n-      add_phi_arg (phi_new, new_ssa_name, loop_latch_edge (new_loop));\n-\n-      /* step 3 (case 1).  */\n-      if (!after)\n-        {\n-          gcc_assert (new_loop_exit_e == orig_entry_e);\n-          SET_PHI_ARG_DEF (phi_orig,\n-                           new_loop_exit_e->dest_idx,\n-                           new_ssa_name);\n-        }\n-    }\n-}\n-\n-\n-/* Update PHI nodes for a guard of the LOOP.\n-\n-   Input:\n-   - LOOP, GUARD_EDGE: LOOP is a loop for which we added guard code that\n-        controls whether LOOP is to be executed.  GUARD_EDGE is the edge that\n-        originates from the guard-bb, skips LOOP and reaches the (unique) exit\n-        bb of LOOP.  This loop-exit-bb is an empty bb with one successor.\n-        We denote this bb NEW_MERGE_BB because before the guard code was added\n-        it had a single predecessor (the LOOP header), and now it became a merge\n-        point of two paths - the path that ends with the LOOP exit-edge, and\n-        the path that ends with GUARD_EDGE.\n-   - NEW_EXIT_BB: New basic block that is added by this function between LOOP\n-        and NEW_MERGE_BB. It is used to place loop-closed-ssa-form exit-phis.\n-\n-   ===> The CFG before the guard-code was added:\n-        LOOP_header_bb:\n-          loop_body\n-          if (exit_loop) goto update_bb\n-          else           goto LOOP_header_bb\n-        update_bb:\n-\n-   ==> The CFG after the guard-code was added:\n-        guard_bb:\n-          if (LOOP_guard_condition) goto new_merge_bb\n-          else                      goto LOOP_header_bb\n-        LOOP_header_bb:\n-          loop_body\n-          if (exit_loop_condition) goto new_merge_bb\n-          else                     goto LOOP_header_bb\n-        new_merge_bb:\n-          goto update_bb\n-        update_bb:\n-\n-   ==> The CFG after this function:\n-        guard_bb:\n-          if (LOOP_guard_condition) goto new_merge_bb\n-          else                      goto LOOP_header_bb\n-        LOOP_header_bb:\n-          loop_body\n-          if (exit_loop_condition) goto new_exit_bb\n-          else                     goto LOOP_header_bb\n-        new_exit_bb:\n-        new_merge_bb:\n-          goto update_bb\n-        update_bb:\n-\n-   This function:\n-   1. creates and updates the relevant phi nodes to account for the new\n-      incoming edge (GUARD_EDGE) into NEW_MERGE_BB. This involves:\n-      1.1. Create phi nodes at NEW_MERGE_BB.\n-      1.2. Update the phi nodes at the successor of NEW_MERGE_BB (denoted\n-           UPDATE_BB).  UPDATE_BB was the exit-bb of LOOP before NEW_MERGE_BB\n-   2. preserves loop-closed-ssa-form by creating the required phi nodes\n-      at the exit of LOOP (i.e, in NEW_EXIT_BB).\n-\n-   There are two flavors to this function:\n-\n-   slpeel_update_phi_nodes_for_guard1:\n-     Here the guard controls whether we enter or skip LOOP, where LOOP is a\n-     prolog_loop (loop1 below), and the new phis created in NEW_MERGE_BB are\n-     for variables that have phis in the loop header.\n-\n-   slpeel_update_phi_nodes_for_guard2:\n-     Here the guard controls whether we enter or skip LOOP, where LOOP is an\n-     epilog_loop (loop2 below), and the new phis created in NEW_MERGE_BB are\n-     for variables that have phis in the loop exit.\n-\n-   I.E., the overall structure is:\n-\n-        loop1_preheader_bb:\n-                guard1 (goto loop1/merge1_bb)\n-        loop1\n-        loop1_exit_bb:\n-                guard2 (goto merge1_bb/merge2_bb)\n-        merge1_bb\n-        loop2\n-        loop2_exit_bb\n-        merge2_bb\n-        next_bb\n-\n-   slpeel_update_phi_nodes_for_guard1 takes care of creating phis in\n-   loop1_exit_bb and merge1_bb. These are entry phis (phis for the vars\n-   that have phis in loop1->header).\n-\n-   slpeel_update_phi_nodes_for_guard2 takes care of creating phis in\n-   loop2_exit_bb and merge2_bb. These are exit phis (phis for the vars\n-   that have phis in next_bb). It also adds some of these phis to\n-   loop1_exit_bb.\n-\n-   slpeel_update_phi_nodes_for_guard1 is always called before\n-   slpeel_update_phi_nodes_for_guard2. They are both needed in order\n-   to create correct data-flow and loop-closed-ssa-form.\n-\n-   Generally slpeel_update_phi_nodes_for_guard1 creates phis for variables\n-   that change between iterations of a loop (and therefore have a phi-node\n-   at the loop entry), whereas slpeel_update_phi_nodes_for_guard2 creates\n-   phis for variables that are used out of the loop (and therefore have \n-   loop-closed exit phis). Some variables may be both updated between \n-   iterations and used after the loop. This is why in loop1_exit_bb we\n-   may need both entry_phis (created by slpeel_update_phi_nodes_for_guard1)\n-   and exit phis (created by slpeel_update_phi_nodes_for_guard2).\n-\n-   - IS_NEW_LOOP: if IS_NEW_LOOP is true, then LOOP is a newly created copy of\n-     an original loop. i.e., we have:\n-\n-           orig_loop\n-           guard_bb (goto LOOP/new_merge)\n-           new_loop <-- LOOP\n-           new_exit\n-           new_merge\n-           next_bb\n-\n-     If IS_NEW_LOOP is false, then LOOP is an original loop, in which case we\n-     have:\n-\n-           new_loop\n-           guard_bb (goto LOOP/new_merge)\n-           orig_loop <-- LOOP\n-           new_exit\n-           new_merge\n-           next_bb\n-\n-     The SSA names defined in the original loop have a current\n-     reaching definition that that records the corresponding new\n-     ssa-name used in the new duplicated loop copy.\n-  */\n-\n-/* Function slpeel_update_phi_nodes_for_guard1\n-   \n-   Input:\n-   - GUARD_EDGE, LOOP, IS_NEW_LOOP, NEW_EXIT_BB - as explained above.\n-   - DEFS - a bitmap of ssa names to mark new names for which we recorded\n-            information. \n-   \n-   In the context of the overall structure, we have:\n-\n-        loop1_preheader_bb: \n-                guard1 (goto loop1/merge1_bb)\n-LOOP->  loop1\n-        loop1_exit_bb:\n-                guard2 (goto merge1_bb/merge2_bb)\n-        merge1_bb\n-        loop2\n-        loop2_exit_bb\n-        merge2_bb\n-        next_bb\n-\n-   For each name updated between loop iterations (i.e - for each name that has\n-   an entry (loop-header) phi in LOOP) we create a new phi in:\n-   1. merge1_bb (to account for the edge from guard1)\n-   2. loop1_exit_bb (an exit-phi to keep LOOP in loop-closed form)\n-*/\n-\n-static void\n-slpeel_update_phi_nodes_for_guard1 (edge guard_edge, struct loop *loop,\n-                                    bool is_new_loop, basic_block *new_exit_bb,\n-                                    bitmap *defs)\n-{\n-  gimple orig_phi, new_phi;\n-  gimple update_phi, update_phi2;\n-  tree guard_arg, loop_arg;\n-  basic_block new_merge_bb = guard_edge->dest;\n-  edge e = EDGE_SUCC (new_merge_bb, 0);\n-  basic_block update_bb = e->dest;\n-  basic_block orig_bb = loop->header;\n-  edge new_exit_e;\n-  tree current_new_name;\n-  tree name;\n-  gimple_stmt_iterator gsi_orig, gsi_update;\n-\n-  /* Create new bb between loop and new_merge_bb.  */\n-  *new_exit_bb = split_edge (single_exit (loop));\n-\n-  new_exit_e = EDGE_SUCC (*new_exit_bb, 0);\n-\n-  for (gsi_orig = gsi_start_phis (orig_bb),\n-       gsi_update = gsi_start_phis (update_bb);\n-       !gsi_end_p (gsi_orig) && !gsi_end_p (gsi_update);\n-       gsi_next (&gsi_orig), gsi_next (&gsi_update))\n-    {\n-      orig_phi = gsi_stmt (gsi_orig);\n-      update_phi = gsi_stmt (gsi_update);\n-\n-      /* Virtual phi; Mark it for renaming. We actually want to call\n-\t mar_sym_for_renaming, but since all ssa renaming datastructures\n-\t are going to be freed before we get to call ssa_update, we just\n-\t record this name for now in a bitmap, and will mark it for\n-\t renaming later.  */\n-      name = PHI_RESULT (orig_phi);\n-      if (!is_gimple_reg (SSA_NAME_VAR (name)))\n-        bitmap_set_bit (vect_memsyms_to_rename, DECL_UID (SSA_NAME_VAR (name)));\n-\n-      /** 1. Handle new-merge-point phis  **/\n-\n-      /* 1.1. Generate new phi node in NEW_MERGE_BB:  */\n-      new_phi = create_phi_node (SSA_NAME_VAR (PHI_RESULT (orig_phi)),\n-                                 new_merge_bb);\n-\n-      /* 1.2. NEW_MERGE_BB has two incoming edges: GUARD_EDGE and the exit-edge\n-            of LOOP. Set the two phi args in NEW_PHI for these edges:  */\n-      loop_arg = PHI_ARG_DEF_FROM_EDGE (orig_phi, EDGE_SUCC (loop->latch, 0));\n-      guard_arg = PHI_ARG_DEF_FROM_EDGE (orig_phi, loop_preheader_edge (loop));\n-\n-      add_phi_arg (new_phi, loop_arg, new_exit_e);\n-      add_phi_arg (new_phi, guard_arg, guard_edge);\n-\n-      /* 1.3. Update phi in successor block.  */\n-      gcc_assert (PHI_ARG_DEF_FROM_EDGE (update_phi, e) == loop_arg\n-                  || PHI_ARG_DEF_FROM_EDGE (update_phi, e) == guard_arg);\n-      SET_PHI_ARG_DEF (update_phi, e->dest_idx, PHI_RESULT (new_phi));\n-      update_phi2 = new_phi;\n-\n-\n-      /** 2. Handle loop-closed-ssa-form phis  **/\n-\n-      if (!is_gimple_reg (PHI_RESULT (orig_phi)))\n-\tcontinue;\n-\n-      /* 2.1. Generate new phi node in NEW_EXIT_BB:  */\n-      new_phi = create_phi_node (SSA_NAME_VAR (PHI_RESULT (orig_phi)),\n-                                 *new_exit_bb);\n-\n-      /* 2.2. NEW_EXIT_BB has one incoming edge: the exit-edge of the loop.  */\n-      add_phi_arg (new_phi, loop_arg, single_exit (loop));\n-\n-      /* 2.3. Update phi in successor of NEW_EXIT_BB:  */\n-      gcc_assert (PHI_ARG_DEF_FROM_EDGE (update_phi2, new_exit_e) == loop_arg);\n-      SET_PHI_ARG_DEF (update_phi2, new_exit_e->dest_idx, PHI_RESULT (new_phi));\n-\n-      /* 2.4. Record the newly created name with set_current_def.\n-         We want to find a name such that\n-                name = get_current_def (orig_loop_name)\n-         and to set its current definition as follows:\n-                set_current_def (name, new_phi_name)\n-\n-         If LOOP is a new loop then loop_arg is already the name we're\n-         looking for. If LOOP is the original loop, then loop_arg is\n-         the orig_loop_name and the relevant name is recorded in its\n-         current reaching definition.  */\n-      if (is_new_loop)\n-        current_new_name = loop_arg;\n-      else\n-        {\n-          current_new_name = get_current_def (loop_arg);\n-\t  /* current_def is not available only if the variable does not\n-\t     change inside the loop, in which case we also don't care\n-\t     about recording a current_def for it because we won't be\n-\t     trying to create loop-exit-phis for it.  */\n-\t  if (!current_new_name)\n-\t    continue;\n-        }\n-      gcc_assert (get_current_def (current_new_name) == NULL_TREE);\n-\n-      set_current_def (current_new_name, PHI_RESULT (new_phi));\n-      bitmap_set_bit (*defs, SSA_NAME_VERSION (current_new_name));\n-    }\n-}\n-\n-\n-/* Function slpeel_update_phi_nodes_for_guard2\n-\n-   Input:\n-   - GUARD_EDGE, LOOP, IS_NEW_LOOP, NEW_EXIT_BB - as explained above.\n-\n-   In the context of the overall structure, we have:\n-\n-        loop1_preheader_bb: \n-                guard1 (goto loop1/merge1_bb)\n-        loop1\n-        loop1_exit_bb: \n-                guard2 (goto merge1_bb/merge2_bb)\n-        merge1_bb\n-LOOP->  loop2\n-        loop2_exit_bb\n-        merge2_bb\n-        next_bb\n-\n-   For each name used out side the loop (i.e - for each name that has an exit\n-   phi in next_bb) we create a new phi in:\n-   1. merge2_bb (to account for the edge from guard_bb) \n-   2. loop2_exit_bb (an exit-phi to keep LOOP in loop-closed form)\n-   3. guard2 bb (an exit phi to keep the preceding loop in loop-closed form),\n-      if needed (if it wasn't handled by slpeel_update_phis_nodes_for_phi1).\n-*/\n-\n-static void\n-slpeel_update_phi_nodes_for_guard2 (edge guard_edge, struct loop *loop,\n-                                    bool is_new_loop, basic_block *new_exit_bb)\n-{\n-  gimple orig_phi, new_phi;\n-  gimple update_phi, update_phi2;\n-  tree guard_arg, loop_arg;\n-  basic_block new_merge_bb = guard_edge->dest;\n-  edge e = EDGE_SUCC (new_merge_bb, 0);\n-  basic_block update_bb = e->dest;\n-  edge new_exit_e;\n-  tree orig_def, orig_def_new_name;\n-  tree new_name, new_name2;\n-  tree arg;\n-  gimple_stmt_iterator gsi;\n-\n-  /* Create new bb between loop and new_merge_bb.  */\n-  *new_exit_bb = split_edge (single_exit (loop));\n-\n-  new_exit_e = EDGE_SUCC (*new_exit_bb, 0);\n-\n-  for (gsi = gsi_start_phis (update_bb); !gsi_end_p (gsi); gsi_next (&gsi))\n-    {\n-      update_phi = gsi_stmt (gsi);\n-      orig_phi = update_phi;\n-      orig_def = PHI_ARG_DEF_FROM_EDGE (orig_phi, e);\n-      /* This loop-closed-phi actually doesn't represent a use\n-         out of the loop - the phi arg is a constant.  */ \n-      if (TREE_CODE (orig_def) != SSA_NAME)\n-        continue;\n-      orig_def_new_name = get_current_def (orig_def);\n-      arg = NULL_TREE;\n-\n-      /** 1. Handle new-merge-point phis  **/\n-\n-      /* 1.1. Generate new phi node in NEW_MERGE_BB:  */\n-      new_phi = create_phi_node (SSA_NAME_VAR (PHI_RESULT (orig_phi)),\n-                                 new_merge_bb);\n-\n-      /* 1.2. NEW_MERGE_BB has two incoming edges: GUARD_EDGE and the exit-edge\n-            of LOOP. Set the two PHI args in NEW_PHI for these edges:  */\n-      new_name = orig_def;\n-      new_name2 = NULL_TREE;\n-      if (orig_def_new_name)\n-        {\n-          new_name = orig_def_new_name;\n-\t  /* Some variables have both loop-entry-phis and loop-exit-phis.\n-\t     Such variables were given yet newer names by phis placed in\n-\t     guard_bb by slpeel_update_phi_nodes_for_guard1. I.e:\n-\t     new_name2 = get_current_def (get_current_def (orig_name)).  */\n-          new_name2 = get_current_def (new_name);\n-        }\n-  \n-      if (is_new_loop)\n-        {\n-          guard_arg = orig_def;\n-          loop_arg = new_name;\n-        }\n-      else\n-        {\n-          guard_arg = new_name;\n-          loop_arg = orig_def;\n-        }\n-      if (new_name2)\n-        guard_arg = new_name2;\n-  \n-      add_phi_arg (new_phi, loop_arg, new_exit_e);\n-      add_phi_arg (new_phi, guard_arg, guard_edge);\n-\n-      /* 1.3. Update phi in successor block.  */\n-      gcc_assert (PHI_ARG_DEF_FROM_EDGE (update_phi, e) == orig_def);\n-      SET_PHI_ARG_DEF (update_phi, e->dest_idx, PHI_RESULT (new_phi));\n-      update_phi2 = new_phi;\n-\n-\n-      /** 2. Handle loop-closed-ssa-form phis  **/\n-\n-      /* 2.1. Generate new phi node in NEW_EXIT_BB:  */\n-      new_phi = create_phi_node (SSA_NAME_VAR (PHI_RESULT (orig_phi)),\n-                                 *new_exit_bb);\n-\n-      /* 2.2. NEW_EXIT_BB has one incoming edge: the exit-edge of the loop.  */\n-      add_phi_arg (new_phi, loop_arg, single_exit (loop));\n-\n-      /* 2.3. Update phi in successor of NEW_EXIT_BB:  */\n-      gcc_assert (PHI_ARG_DEF_FROM_EDGE (update_phi2, new_exit_e) == loop_arg);\n-      SET_PHI_ARG_DEF (update_phi2, new_exit_e->dest_idx, PHI_RESULT (new_phi));\n-\n-\n-      /** 3. Handle loop-closed-ssa-form phis for first loop  **/\n-\n-      /* 3.1. Find the relevant names that need an exit-phi in\n-\t GUARD_BB, i.e. names for which\n-\t slpeel_update_phi_nodes_for_guard1 had not already created a\n-\t phi node. This is the case for names that are used outside\n-\t the loop (and therefore need an exit phi) but are not updated\n-\t across loop iterations (and therefore don't have a\n-\t loop-header-phi).\n-\n-\t slpeel_update_phi_nodes_for_guard1 is responsible for\n-\t creating loop-exit phis in GUARD_BB for names that have a\n-\t loop-header-phi.  When such a phi is created we also record\n-\t the new name in its current definition.  If this new name\n-\t exists, then guard_arg was set to this new name (see 1.2\n-\t above).  Therefore, if guard_arg is not this new name, this\n-\t is an indication that an exit-phi in GUARD_BB was not yet\n-\t created, so we take care of it here.  */\n-      if (guard_arg == new_name2)\n-\tcontinue;\n-      arg = guard_arg;\n-\n-      /* 3.2. Generate new phi node in GUARD_BB:  */\n-      new_phi = create_phi_node (SSA_NAME_VAR (PHI_RESULT (orig_phi)),\n-                                 guard_edge->src);\n-\n-      /* 3.3. GUARD_BB has one incoming edge:  */\n-      gcc_assert (EDGE_COUNT (guard_edge->src->preds) == 1);\n-      add_phi_arg (new_phi, arg, EDGE_PRED (guard_edge->src, 0));\n-\n-      /* 3.4. Update phi in successor of GUARD_BB:  */\n-      gcc_assert (PHI_ARG_DEF_FROM_EDGE (update_phi2, guard_edge)\n-                                                                == guard_arg);\n-      SET_PHI_ARG_DEF (update_phi2, guard_edge->dest_idx, PHI_RESULT (new_phi));\n-    }\n-}\n-\n-\n-/* Make the LOOP iterate NITERS times. This is done by adding a new IV\n-   that starts at zero, increases by one and its limit is NITERS.\n-\n-   Assumption: the exit-condition of LOOP is the last stmt in the loop.  */\n-\n-void\n-slpeel_make_loop_iterate_ntimes (struct loop *loop, tree niters)\n-{\n-  tree indx_before_incr, indx_after_incr;\n-  gimple cond_stmt;\n-  gimple orig_cond;\n-  edge exit_edge = single_exit (loop);\n-  gimple_stmt_iterator loop_cond_gsi;\n-  gimple_stmt_iterator incr_gsi;\n-  bool insert_after;\n-  tree init = build_int_cst (TREE_TYPE (niters), 0);\n-  tree step = build_int_cst (TREE_TYPE (niters), 1);\n-  LOC loop_loc;\n-  enum tree_code code;\n-\n-  orig_cond = get_loop_exit_condition (loop);\n-  gcc_assert (orig_cond);\n-  loop_cond_gsi = gsi_for_stmt (orig_cond);\n-\n-  standard_iv_increment_position (loop, &incr_gsi, &insert_after);\n-  create_iv (init, step, NULL_TREE, loop,\n-             &incr_gsi, insert_after, &indx_before_incr, &indx_after_incr);\n-\n-  indx_after_incr = force_gimple_operand_gsi (&loop_cond_gsi, indx_after_incr,\n-\t\t\t\t\t      true, NULL_TREE, true,\n-\t\t\t\t\t      GSI_SAME_STMT);\n-  niters = force_gimple_operand_gsi (&loop_cond_gsi, niters, true, NULL_TREE,\n-\t\t\t\t     true, GSI_SAME_STMT);\n-\n-  code = (exit_edge->flags & EDGE_TRUE_VALUE) ? GE_EXPR : LT_EXPR;\n-  cond_stmt = gimple_build_cond (code, indx_after_incr, niters, NULL_TREE,\n-\t\t\t\t NULL_TREE);\n-\n-  gsi_insert_before (&loop_cond_gsi, cond_stmt, GSI_SAME_STMT);\n-\n-  /* Remove old loop exit test:  */\n-  gsi_remove (&loop_cond_gsi, true);\n-\n-  loop_loc = find_loop_location (loop);\n-  if (dump_file && (dump_flags & TDF_DETAILS))\n-    {\n-      if (loop_loc != UNKNOWN_LOC)\n-        fprintf (dump_file, \"\\nloop at %s:%d: \",\n-                 LOC_FILE (loop_loc), LOC_LINE (loop_loc));\n-      print_gimple_stmt (dump_file, cond_stmt, 0, TDF_SLIM);\n-    }\n-\n-  loop->nb_iterations = niters;\n-}\n-\n-\n-/* Given LOOP this function generates a new copy of it and puts it \n-   on E which is either the entry or exit of LOOP.  */\n-\n-struct loop *\n-slpeel_tree_duplicate_loop_to_edge_cfg (struct loop *loop, edge e)\n-{\n-  struct loop *new_loop;\n-  basic_block *new_bbs, *bbs;\n-  bool at_exit;\n-  bool was_imm_dom;\n-  basic_block exit_dest; \n-  gimple phi;\n-  tree phi_arg;\n-  edge exit, new_exit;\n-  gimple_stmt_iterator gsi;\n-\n-  at_exit = (e == single_exit (loop)); \n-  if (!at_exit && e != loop_preheader_edge (loop))\n-    return NULL;\n-\n-  bbs = get_loop_body (loop);\n-\n-  /* Check whether duplication is possible.  */\n-  if (!can_copy_bbs_p (bbs, loop->num_nodes))\n-    {\n-      free (bbs);\n-      return NULL;\n-    }\n-\n-  /* Generate new loop structure.  */\n-  new_loop = duplicate_loop (loop, loop_outer (loop));\n-  if (!new_loop)\n-    {\n-      free (bbs);\n-      return NULL;\n-    }\n-\n-  exit_dest = single_exit (loop)->dest;\n-  was_imm_dom = (get_immediate_dominator (CDI_DOMINATORS, \n-\t\t\t\t\t  exit_dest) == loop->header ? \n-\t\t true : false);\n-\n-  new_bbs = XNEWVEC (basic_block, loop->num_nodes);\n-\n-  exit = single_exit (loop);\n-  copy_bbs (bbs, loop->num_nodes, new_bbs,\n-\t    &exit, 1, &new_exit, NULL,\n-\t    e->src);\n-\n-  /* Duplicating phi args at exit bbs as coming \n-     also from exit of duplicated loop.  */\n-  for (gsi = gsi_start_phis (exit_dest); !gsi_end_p (gsi); gsi_next (&gsi))\n-    {\n-      phi = gsi_stmt (gsi);\n-      phi_arg = PHI_ARG_DEF_FROM_EDGE (phi, single_exit (loop));\n-      if (phi_arg)\n-\t{\n-\t  edge new_loop_exit_edge;\n-\n-\t  if (EDGE_SUCC (new_loop->header, 0)->dest == new_loop->latch)\n-\t    new_loop_exit_edge = EDGE_SUCC (new_loop->header, 1);\n-\t  else\n-\t    new_loop_exit_edge = EDGE_SUCC (new_loop->header, 0);\n-  \n-\t  add_phi_arg (phi, phi_arg, new_loop_exit_edge);\t\n-\t}\n-    }    \n-   \n-  if (at_exit) /* Add the loop copy at exit.  */\n-    {\n-      redirect_edge_and_branch_force (e, new_loop->header);\n-      PENDING_STMT (e) = NULL;\n-      set_immediate_dominator (CDI_DOMINATORS, new_loop->header, e->src);\n-      if (was_imm_dom)\n-\tset_immediate_dominator (CDI_DOMINATORS, exit_dest, new_loop->header);\n-    }\n-  else /* Add the copy at entry.  */\n-    {\n-      edge new_exit_e;\n-      edge entry_e = loop_preheader_edge (loop);\n-      basic_block preheader = entry_e->src;\n-           \n-      if (!flow_bb_inside_loop_p (new_loop, \n-\t\t\t\t  EDGE_SUCC (new_loop->header, 0)->dest))\n-        new_exit_e = EDGE_SUCC (new_loop->header, 0);\n-      else\n-\tnew_exit_e = EDGE_SUCC (new_loop->header, 1); \n-\n-      redirect_edge_and_branch_force (new_exit_e, loop->header);\n-      PENDING_STMT (new_exit_e) = NULL;\n-      set_immediate_dominator (CDI_DOMINATORS, loop->header,\n-\t\t\t       new_exit_e->src);\n-\n-      /* We have to add phi args to the loop->header here as coming \n-\t from new_exit_e edge.  */\n-      for (gsi = gsi_start_phis (loop->header);\n-           !gsi_end_p (gsi);\n-           gsi_next (&gsi))\n-\t{\n-\t  phi = gsi_stmt (gsi);\n-\t  phi_arg = PHI_ARG_DEF_FROM_EDGE (phi, entry_e);\n-\t  if (phi_arg)\n-\t    add_phi_arg (phi, phi_arg, new_exit_e);\t\n-\t}    \n-\n-      redirect_edge_and_branch_force (entry_e, new_loop->header);\n-      PENDING_STMT (entry_e) = NULL;\n-      set_immediate_dominator (CDI_DOMINATORS, new_loop->header, preheader);\n-    }\n-\n-  free (new_bbs);\n-  free (bbs);\n-\n-  return new_loop;\n-}\n-\n-\n-/* Given the condition statement COND, put it as the last statement\n-   of GUARD_BB; EXIT_BB is the basic block to skip the loop;\n-   Assumes that this is the single exit of the guarded loop.  \n-   Returns the skip edge.  */\n-\n-static edge\n-slpeel_add_loop_guard (basic_block guard_bb, tree cond, basic_block exit_bb,\n-\t\t       basic_block dom_bb)\n-{\n-  gimple_stmt_iterator gsi;\n-  edge new_e, enter_e;\n-  gimple cond_stmt;\n-  gimple_seq gimplify_stmt_list = NULL;\n-\n-  enter_e = EDGE_SUCC (guard_bb, 0);\n-  enter_e->flags &= ~EDGE_FALLTHRU;\n-  enter_e->flags |= EDGE_FALSE_VALUE;\n-  gsi = gsi_last_bb (guard_bb);\n-\n-  cond = force_gimple_operand (cond, &gimplify_stmt_list, true, NULL_TREE);\n-  cond_stmt = gimple_build_cond (NE_EXPR,\n-\t\t\t\t cond, build_int_cst (TREE_TYPE (cond), 0),\n-\t\t\t\t NULL_TREE, NULL_TREE);\n-  if (gimplify_stmt_list)\n-    gsi_insert_seq_after (&gsi, gimplify_stmt_list, GSI_NEW_STMT);\n-\n-  gsi = gsi_last_bb (guard_bb);\n-  gsi_insert_after (&gsi, cond_stmt, GSI_NEW_STMT);\n-\n-  /* Add new edge to connect guard block to the merge/loop-exit block.  */\n-  new_e = make_edge (guard_bb, exit_bb, EDGE_TRUE_VALUE);\n-  set_immediate_dominator (CDI_DOMINATORS, exit_bb, dom_bb);\n-  return new_e;\n-}\n-\n-\n-/* This function verifies that the following restrictions apply to LOOP:\n-   (1) it is innermost\n-   (2) it consists of exactly 2 basic blocks - header, and an empty latch.\n-   (3) it is single entry, single exit\n-   (4) its exit condition is the last stmt in the header\n-   (5) E is the entry/exit edge of LOOP.\n- */\n-\n-bool\n-slpeel_can_duplicate_loop_p (const struct loop *loop, const_edge e)\n-{\n-  edge exit_e = single_exit (loop);\n-  edge entry_e = loop_preheader_edge (loop);\n-  gimple orig_cond = get_loop_exit_condition (loop);\n-  gimple_stmt_iterator loop_exit_gsi = gsi_last_bb (exit_e->src);\n-\n-  if (need_ssa_update_p ())\n-    return false;\n-\n-  if (loop->inner\n-      /* All loops have an outer scope; the only case loop->outer is NULL is for\n-         the function itself.  */\n-      || !loop_outer (loop)\n-      || loop->num_nodes != 2\n-      || !empty_block_p (loop->latch)\n-      || !single_exit (loop)\n-      /* Verify that new loop exit condition can be trivially modified.  */\n-      || (!orig_cond || orig_cond != gsi_stmt (loop_exit_gsi))\n-      || (e != exit_e && e != entry_e))\n-    return false;\n-\n-  return true;\n-}\n-\n-#ifdef ENABLE_CHECKING\n-void\n-slpeel_verify_cfg_after_peeling (struct loop *first_loop,\n-                                 struct loop *second_loop)\n-{\n-  basic_block loop1_exit_bb = single_exit (first_loop)->dest;\n-  basic_block loop2_entry_bb = loop_preheader_edge (second_loop)->src;\n-  basic_block loop1_entry_bb = loop_preheader_edge (first_loop)->src;\n-\n-  /* A guard that controls whether the second_loop is to be executed or skipped\n-     is placed in first_loop->exit.  first_loop->exit therefore has two\n-     successors - one is the preheader of second_loop, and the other is a bb\n-     after second_loop.\n-   */\n-  gcc_assert (EDGE_COUNT (loop1_exit_bb->succs) == 2);\n-   \n-  /* 1. Verify that one of the successors of first_loop->exit is the preheader\n-        of second_loop.  */\n-   \n-  /* The preheader of new_loop is expected to have two predecessors:\n-     first_loop->exit and the block that precedes first_loop.  */\n-\n-  gcc_assert (EDGE_COUNT (loop2_entry_bb->preds) == 2 \n-              && ((EDGE_PRED (loop2_entry_bb, 0)->src == loop1_exit_bb\n-                   && EDGE_PRED (loop2_entry_bb, 1)->src == loop1_entry_bb)\n-               || (EDGE_PRED (loop2_entry_bb, 1)->src ==  loop1_exit_bb\n-                   && EDGE_PRED (loop2_entry_bb, 0)->src == loop1_entry_bb)));\n-  \n-  /* Verify that the other successor of first_loop->exit is after the\n-     second_loop.  */\n-  /* TODO */\n-}\n-#endif\n-\n-/* If the run time cost model check determines that vectorization is\n-   not profitable and hence scalar loop should be generated then set\n-   FIRST_NITERS to prologue peeled iterations. This will allow all the\n-   iterations to be executed in the prologue peeled scalar loop.  */\n-\n-void\n-set_prologue_iterations (basic_block bb_before_first_loop,\n-\t\t\t tree first_niters,\n-\t\t\t struct loop *loop,\n-\t\t\t unsigned int th)\n-{\n-  edge e;\n-  basic_block cond_bb, then_bb;\n-  tree var, prologue_after_cost_adjust_name;\n-  gimple_stmt_iterator gsi;\n-  gimple newphi;\n-  edge e_true, e_false, e_fallthru;\n-  gimple cond_stmt;\n-  gimple_seq gimplify_stmt_list = NULL, stmts = NULL;\n-  tree cost_pre_condition = NULL_TREE;\n-  tree scalar_loop_iters = \n-    unshare_expr (LOOP_VINFO_NITERS_UNCHANGED (loop_vec_info_for_loop (loop)));\n-\n-  e = single_pred_edge (bb_before_first_loop);\n-  cond_bb = split_edge(e);\n-\n-  e = single_pred_edge (bb_before_first_loop);\n-  then_bb = split_edge(e);\n-  set_immediate_dominator (CDI_DOMINATORS, then_bb, cond_bb);\n-\n-  e_false = make_single_succ_edge (cond_bb, bb_before_first_loop,\n-\t\t\t\t   EDGE_FALSE_VALUE);\n-  set_immediate_dominator (CDI_DOMINATORS, bb_before_first_loop, cond_bb);\n-\n-  e_true = EDGE_PRED (then_bb, 0);\n-  e_true->flags &= ~EDGE_FALLTHRU;\n-  e_true->flags |= EDGE_TRUE_VALUE;\n-\n-  e_fallthru = EDGE_SUCC (then_bb, 0);\n-\n-  cost_pre_condition =\n-    fold_build2 (LE_EXPR, boolean_type_node, scalar_loop_iters, \n-\t\t build_int_cst (TREE_TYPE (scalar_loop_iters), th));\n-  cost_pre_condition =\n-    force_gimple_operand (cost_pre_condition, &gimplify_stmt_list,\n-\t\t\t  true, NULL_TREE);\n-  cond_stmt = gimple_build_cond (NE_EXPR, cost_pre_condition,\n-\t\t\t\t build_int_cst (TREE_TYPE (cost_pre_condition),\n-\t\t\t\t\t\t0), NULL_TREE, NULL_TREE);\n-\n-  gsi = gsi_last_bb (cond_bb);\n-  if (gimplify_stmt_list)\n-    gsi_insert_seq_after (&gsi, gimplify_stmt_list, GSI_NEW_STMT);\n-\n-  gsi = gsi_last_bb (cond_bb);\n-  gsi_insert_after (&gsi, cond_stmt, GSI_NEW_STMT);\n-\t\t\t\t\t  \n-  var = create_tmp_var (TREE_TYPE (scalar_loop_iters),\n-\t\t\t\"prologue_after_cost_adjust\");\n-  add_referenced_var (var);\n-  prologue_after_cost_adjust_name = \n-    force_gimple_operand (scalar_loop_iters, &stmts, false, var);\n-\n-  gsi = gsi_last_bb (then_bb);\n-  if (stmts)\n-    gsi_insert_seq_after (&gsi, stmts, GSI_NEW_STMT);\n-\n-  newphi = create_phi_node (var, bb_before_first_loop);\n-  add_phi_arg (newphi, prologue_after_cost_adjust_name, e_fallthru);\n-  add_phi_arg (newphi, first_niters, e_false);\n-\n-  first_niters = PHI_RESULT (newphi);\n-}\n-\n-\n-/* Function slpeel_tree_peel_loop_to_edge.\n-\n-   Peel the first (last) iterations of LOOP into a new prolog (epilog) loop\n-   that is placed on the entry (exit) edge E of LOOP. After this transformation\n-   we have two loops one after the other - first-loop iterates FIRST_NITERS\n-   times, and second-loop iterates the remainder NITERS - FIRST_NITERS times.\n-   If the cost model indicates that it is profitable to emit a scalar \n-   loop instead of the vector one, then the prolog (epilog) loop will iterate\n-   for the entire unchanged scalar iterations of the loop.\n-\n-   Input:\n-   - LOOP: the loop to be peeled.\n-   - E: the exit or entry edge of LOOP.\n-        If it is the entry edge, we peel the first iterations of LOOP. In this\n-        case first-loop is LOOP, and second-loop is the newly created loop.\n-        If it is the exit edge, we peel the last iterations of LOOP. In this\n-        case, first-loop is the newly created loop, and second-loop is LOOP.\n-   - NITERS: the number of iterations that LOOP iterates.\n-   - FIRST_NITERS: the number of iterations that the first-loop should iterate.\n-   - UPDATE_FIRST_LOOP_COUNT:  specified whether this function is responsible\n-        for updating the loop bound of the first-loop to FIRST_NITERS.  If it\n-        is false, the caller of this function may want to take care of this\n-        (this can be useful if we don't want new stmts added to first-loop).\n-   - TH: cost model profitability threshold of iterations for vectorization.\n-   - CHECK_PROFITABILITY: specify whether cost model check has not occurred\n-                          during versioning and hence needs to occur during\n-\t\t\t  prologue generation or whether cost model check \n-\t\t\t  has not occurred during prologue generation and hence\n-\t\t\t  needs to occur during epilogue generation.\n-\t    \n-\n-   Output:\n-   The function returns a pointer to the new loop-copy, or NULL if it failed\n-   to perform the transformation.\n-\n-   The function generates two if-then-else guards: one before the first loop,\n-   and the other before the second loop:\n-   The first guard is:\n-     if (FIRST_NITERS == 0) then skip the first loop,\n-     and go directly to the second loop.\n-   The second guard is:\n-     if (FIRST_NITERS == NITERS) then skip the second loop.\n-\n-   FORNOW only simple loops are supported (see slpeel_can_duplicate_loop_p).\n-   FORNOW the resulting code will not be in loop-closed-ssa form.\n-*/\n-\n-struct loop*\n-slpeel_tree_peel_loop_to_edge (struct loop *loop, \n-\t\t\t       edge e, tree first_niters, \n-\t\t\t       tree niters, bool update_first_loop_count,\n-\t\t\t       unsigned int th, bool check_profitability)\n-{\n-  struct loop *new_loop = NULL, *first_loop, *second_loop;\n-  edge skip_e;\n-  tree pre_condition = NULL_TREE;\n-  bitmap definitions;\n-  basic_block bb_before_second_loop, bb_after_second_loop;\n-  basic_block bb_before_first_loop;\n-  basic_block bb_between_loops;\n-  basic_block new_exit_bb;\n-  edge exit_e = single_exit (loop);\n-  LOC loop_loc;\n-  tree cost_pre_condition = NULL_TREE;\n-  \n-  if (!slpeel_can_duplicate_loop_p (loop, e))\n-    return NULL;\n-  \n-  /* We have to initialize cfg_hooks. Then, when calling\n-   cfg_hooks->split_edge, the function tree_split_edge \n-   is actually called and, when calling cfg_hooks->duplicate_block,\n-   the function tree_duplicate_bb is called.  */\n-  gimple_register_cfg_hooks ();\n-\n-\n-  /* 1. Generate a copy of LOOP and put it on E (E is the entry/exit of LOOP).\n-        Resulting CFG would be:\n-\n-        first_loop:\n-        do {\n-        } while ...\n-\n-        second_loop:\n-        do {\n-        } while ...\n-\n-        orig_exit_bb:\n-   */\n-  \n-  if (!(new_loop = slpeel_tree_duplicate_loop_to_edge_cfg (loop, e)))\n-    {\n-      loop_loc = find_loop_location (loop);\n-      if (dump_file && (dump_flags & TDF_DETAILS))\n-        {\n-          if (loop_loc != UNKNOWN_LOC)\n-            fprintf (dump_file, \"\\n%s:%d: note: \",\n-                     LOC_FILE (loop_loc), LOC_LINE (loop_loc));\n-          fprintf (dump_file, \"tree_duplicate_loop_to_edge_cfg failed.\\n\");\n-        }\n-      return NULL;\n-    }\n-  \n-  if (e == exit_e)\n-    {\n-      /* NEW_LOOP was placed after LOOP.  */\n-      first_loop = loop;\n-      second_loop = new_loop;\n-    }\n-  else\n-    {\n-      /* NEW_LOOP was placed before LOOP.  */\n-      first_loop = new_loop;\n-      second_loop = loop;\n-    }\n-\n-  definitions = ssa_names_to_replace ();\n-  slpeel_update_phis_for_duplicate_loop (loop, new_loop, e == exit_e);\n-  rename_variables_in_loop (new_loop);\n-\n-\n-  /* 2.  Add the guard code in one of the following ways:\n-\n-     2.a Add the guard that controls whether the first loop is executed.\n-         This occurs when this function is invoked for prologue or epilogue\n-\t generation and when the cost model check can be done at compile time.\n-\n-         Resulting CFG would be:\n-\n-         bb_before_first_loop:\n-         if (FIRST_NITERS == 0) GOTO bb_before_second_loop\n-                                GOTO first-loop\n-\n-         first_loop:\n-         do {\n-         } while ...\n-\n-         bb_before_second_loop:\n-\n-         second_loop:\n-         do {\n-         } while ...\n-\n-         orig_exit_bb:\n-\n-     2.b Add the cost model check that allows the prologue\n-         to iterate for the entire unchanged scalar\n-         iterations of the loop in the event that the cost\n-         model indicates that the scalar loop is more\n-         profitable than the vector one. This occurs when\n-\t this function is invoked for prologue generation\n-\t and the cost model check needs to be done at run\n-\t time.\n-\n-         Resulting CFG after prologue peeling would be:\n-\n-         if (scalar_loop_iterations <= th)\n-           FIRST_NITERS = scalar_loop_iterations\n-\n-         bb_before_first_loop:\n-         if (FIRST_NITERS == 0) GOTO bb_before_second_loop\n-                                GOTO first-loop\n-\n-         first_loop:\n-         do {\n-         } while ...\n-\n-         bb_before_second_loop:\n-\n-         second_loop:\n-         do {\n-         } while ...\n-\n-         orig_exit_bb:\n-\n-     2.c Add the cost model check that allows the epilogue\n-         to iterate for the entire unchanged scalar\n-         iterations of the loop in the event that the cost\n-         model indicates that the scalar loop is more\n-         profitable than the vector one. This occurs when\n-\t this function is invoked for epilogue generation\n-\t and the cost model check needs to be done at run\n-\t time.\n-\n-         Resulting CFG after prologue peeling would be:\n-\n-         bb_before_first_loop:\n-         if ((scalar_loop_iterations <= th)\n-             ||\n-             FIRST_NITERS == 0) GOTO bb_before_second_loop\n-                                GOTO first-loop\n-\n-         first_loop:\n-         do {\n-         } while ...\n-\n-         bb_before_second_loop:\n-\n-         second_loop:\n-         do {\n-         } while ...\n-\n-         orig_exit_bb:\n-  */\n-\n-  bb_before_first_loop = split_edge (loop_preheader_edge (first_loop));\n-  bb_before_second_loop = split_edge (single_exit (first_loop));\n-\n-  /* Epilogue peeling.  */\n-  if (!update_first_loop_count)\n-    {\n-      pre_condition =\n-\tfold_build2 (LE_EXPR, boolean_type_node, first_niters, \n-\t\t     build_int_cst (TREE_TYPE (first_niters), 0));\n-      if (check_profitability)\n-\t{\n-\t  tree scalar_loop_iters\n-\t    = unshare_expr (LOOP_VINFO_NITERS_UNCHANGED\n-\t\t\t\t\t(loop_vec_info_for_loop (loop)));\n-\t  cost_pre_condition = \n-\t    fold_build2 (LE_EXPR, boolean_type_node, scalar_loop_iters, \n-\t\t\t build_int_cst (TREE_TYPE (scalar_loop_iters), th));\n-\n-\t  pre_condition = fold_build2 (TRUTH_OR_EXPR, boolean_type_node,\n-\t\t\t\t       cost_pre_condition, pre_condition);\n-\t}\n-    }\n-\n-  /* Prologue peeling.  */  \n-  else\n-    {\n-      if (check_profitability)\n-\tset_prologue_iterations (bb_before_first_loop, first_niters,\n-\t\t\t\t loop, th);\n-\n-      pre_condition =\n-\tfold_build2 (LE_EXPR, boolean_type_node, first_niters, \n-\t\t     build_int_cst (TREE_TYPE (first_niters), 0));\n-    }\n-\n-  skip_e = slpeel_add_loop_guard (bb_before_first_loop, pre_condition,\n-                                  bb_before_second_loop, bb_before_first_loop);\n-  slpeel_update_phi_nodes_for_guard1 (skip_e, first_loop,\n-\t\t\t\t      first_loop == new_loop,\n-\t\t\t\t      &new_exit_bb, &definitions);\n-\n-\n-  /* 3. Add the guard that controls whether the second loop is executed.\n-        Resulting CFG would be:\n-\n-        bb_before_first_loop:\n-        if (FIRST_NITERS == 0) GOTO bb_before_second_loop (skip first loop)\n-                               GOTO first-loop\n-\n-        first_loop:\n-        do {\n-        } while ...\n-\n-        bb_between_loops:\n-        if (FIRST_NITERS == NITERS) GOTO bb_after_second_loop (skip second loop)\n-                                    GOTO bb_before_second_loop\n-\n-        bb_before_second_loop:\n-\n-        second_loop:\n-        do {\n-        } while ...\n-\n-        bb_after_second_loop:\n-\n-        orig_exit_bb:\n-   */\n-\n-  bb_between_loops = new_exit_bb;\n-  bb_after_second_loop = split_edge (single_exit (second_loop));\n-\n-  pre_condition = \n-\tfold_build2 (EQ_EXPR, boolean_type_node, first_niters, niters);\n-  skip_e = slpeel_add_loop_guard (bb_between_loops, pre_condition,\n-                                  bb_after_second_loop, bb_before_first_loop);\n-  slpeel_update_phi_nodes_for_guard2 (skip_e, second_loop,\n-                                     second_loop == new_loop, &new_exit_bb);\n-\n-  /* 4. Make first-loop iterate FIRST_NITERS times, if requested.\n-   */\n-  if (update_first_loop_count)\n-    slpeel_make_loop_iterate_ntimes (first_loop, first_niters);\n-\n-  BITMAP_FREE (definitions);\n-  delete_update_ssa ();\n-\n-  return new_loop;\n-}\n-\n-/* Function vect_get_loop_location.\n-\n-   Extract the location of the loop in the source code.\n-   If the loop is not well formed for vectorization, an estimated\n-   location is calculated.\n-   Return the loop location if succeed and NULL if not.  */\n-\n-LOC\n-find_loop_location (struct loop *loop)\n-{\n-  gimple stmt = NULL;\n-  basic_block bb;\n-  gimple_stmt_iterator si;\n-\n-  if (!loop)\n-    return UNKNOWN_LOC;\n-\n-  stmt = get_loop_exit_condition (loop);\n-\n-  if (stmt && gimple_location (stmt) != UNKNOWN_LOC)\n-    return gimple_location (stmt);\n-\n-  /* If we got here the loop is probably not \"well formed\",\n-     try to estimate the loop location */\n-\n-  if (!loop->header)\n-    return UNKNOWN_LOC;\n-\n-  bb = loop->header;\n-\n-  for (si = gsi_start_bb (bb); !gsi_end_p (si); gsi_next (&si))\n-    {\n-      stmt = gsi_stmt (si);\n-      if (gimple_location (stmt) != UNKNOWN_LOC)\n-        return gimple_location (stmt);\n-    }\n-\n-  return UNKNOWN_LOC;\n-}\n-\n-\n-/*************************************************************************\n-  Vectorization Debug Information.\n- *************************************************************************/\n-\n-/* Function vect_set_verbosity_level.\n-\n-   Called from toplev.c upon detection of the\n-   -ftree-vectorizer-verbose=N option.  */\n-\n-void\n-vect_set_verbosity_level (const char *val)\n-{\n-   unsigned int vl;\n-\n-   vl = atoi (val);\n-   if (vl < MAX_VERBOSITY_LEVEL)\n-     vect_verbosity_level = vl;\n-   else\n-     vect_verbosity_level = MAX_VERBOSITY_LEVEL - 1;\n-}\n-\n-\n-/* Function vect_set_dump_settings.\n-\n-   Fix the verbosity level of the vectorizer if the\n-   requested level was not set explicitly using the flag\n-   -ftree-vectorizer-verbose=N.\n-   Decide where to print the debugging information (dump_file/stderr).\n-   If the user defined the verbosity level, but there is no dump file,\n-   print to stderr, otherwise print to the dump file.  */\n-\n-static void\n-vect_set_dump_settings (void)\n-{\n-  vect_dump = dump_file;\n-\n-  /* Check if the verbosity level was defined by the user:  */\n-  if (vect_verbosity_level != MAX_VERBOSITY_LEVEL)\n-    {\n-      /* If there is no dump file, print to stderr.  */\n-      if (!dump_file)\n-        vect_dump = stderr;\n-      return;\n-    }\n-\n-  /* User didn't specify verbosity level:  */\n-  if (dump_file && (dump_flags & TDF_DETAILS))\n-    vect_verbosity_level = REPORT_DETAILS;\n-  else if (dump_file && (dump_flags & TDF_STATS))\n-    vect_verbosity_level = REPORT_UNVECTORIZED_LOOPS;\n-  else\n-    vect_verbosity_level = REPORT_NONE;\n-\n-  gcc_assert (dump_file || vect_verbosity_level == REPORT_NONE);\n-}\n-\n-\n-/* Function debug_loop_details.\n-\n-   For vectorization debug dumps.  */\n-\n-bool\n-vect_print_dump_info (enum verbosity_levels vl)\n-{\n-  if (vl > vect_verbosity_level)\n-    return false;\n-\n-  if (!current_function_decl || !vect_dump)\n-    return false;\n-\n-  if (vect_loop_location == UNKNOWN_LOC)\n-    fprintf (vect_dump, \"\\n%s:%d: note: \",\n-\t     DECL_SOURCE_FILE (current_function_decl),\n-\t     DECL_SOURCE_LINE (current_function_decl));\n-  else\n-    fprintf (vect_dump, \"\\n%s:%d: note: \", \n-\t     LOC_FILE (vect_loop_location), LOC_LINE (vect_loop_location));\n-\n-  return true;\n-}\n-\n-\n-/*************************************************************************\n-  Vectorization Utilities.\n- *************************************************************************/\n-\n-/* Function new_stmt_vec_info.\n-\n-   Create and initialize a new stmt_vec_info struct for STMT.  */\n-\n-stmt_vec_info\n-new_stmt_vec_info (gimple stmt, loop_vec_info loop_vinfo)\n-{\n-  stmt_vec_info res;\n-  res = (stmt_vec_info) xcalloc (1, sizeof (struct _stmt_vec_info));\n-\n-  STMT_VINFO_TYPE (res) = undef_vec_info_type;\n-  STMT_VINFO_STMT (res) = stmt;\n-  STMT_VINFO_LOOP_VINFO (res) = loop_vinfo;\n-  STMT_VINFO_RELEVANT (res) = 0;\n-  STMT_VINFO_LIVE_P (res) = false;\n-  STMT_VINFO_VECTYPE (res) = NULL;\n-  STMT_VINFO_VEC_STMT (res) = NULL;\n-  STMT_VINFO_IN_PATTERN_P (res) = false;\n-  STMT_VINFO_RELATED_STMT (res) = NULL;\n-  STMT_VINFO_DATA_REF (res) = NULL;\n-\n-  STMT_VINFO_DR_BASE_ADDRESS (res) = NULL;\n-  STMT_VINFO_DR_OFFSET (res) = NULL;\n-  STMT_VINFO_DR_INIT (res) = NULL;\n-  STMT_VINFO_DR_STEP (res) = NULL;\n-  STMT_VINFO_DR_ALIGNED_TO (res) = NULL;\n-\n-  if (gimple_code (stmt) == GIMPLE_PHI\n-      && is_loop_header_bb_p (gimple_bb (stmt)))\n-    STMT_VINFO_DEF_TYPE (res) = vect_unknown_def_type;\n-  else\n-    STMT_VINFO_DEF_TYPE (res) = vect_loop_def;\n-  STMT_VINFO_SAME_ALIGN_REFS (res) = VEC_alloc (dr_p, heap, 5);\n-  STMT_VINFO_INSIDE_OF_LOOP_COST (res) = 0;\n-  STMT_VINFO_OUTSIDE_OF_LOOP_COST (res) = 0;\n-  STMT_SLP_TYPE (res) = 0;\n-  DR_GROUP_FIRST_DR (res) = NULL;\n-  DR_GROUP_NEXT_DR (res) = NULL;\n-  DR_GROUP_SIZE (res) = 0;\n-  DR_GROUP_STORE_COUNT (res) = 0;\n-  DR_GROUP_GAP (res) = 0;\n-  DR_GROUP_SAME_DR_STMT (res) = NULL;\n-  DR_GROUP_READ_WRITE_DEPENDENCE (res) = false;\n-\n-  return res;\n-}\n-\n-/* Create a hash table for stmt_vec_info. */\n-\n-void\n-init_stmt_vec_info_vec (void)\n-{\n-  gcc_assert (!stmt_vec_info_vec);\n-  stmt_vec_info_vec = VEC_alloc (vec_void_p, heap, 50);\n-}\n-\n-/* Free hash table for stmt_vec_info. */\n-\n-void\n-free_stmt_vec_info_vec (void)\n-{\n-  gcc_assert (stmt_vec_info_vec);\n-  VEC_free (vec_void_p, heap, stmt_vec_info_vec);\n-}\n-\n-/* Free stmt vectorization related info.  */\n-\n-void\n-free_stmt_vec_info (gimple stmt)\n-{\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n-\n-  if (!stmt_info)\n-    return;\n-\n-  VEC_free (dr_p, heap, STMT_VINFO_SAME_ALIGN_REFS (stmt_info));\n-  set_vinfo_for_stmt (stmt, NULL);\n-  free (stmt_info);\n-}\n-\n-\n-/* Function bb_in_loop_p\n-\n-   Used as predicate for dfs order traversal of the loop bbs.  */\n-\n-static bool\n-bb_in_loop_p (const_basic_block bb, const void *data)\n-{\n-  const struct loop *const loop = (const struct loop *)data;\n-  if (flow_bb_inside_loop_p (loop, bb))\n-    return true;\n-  return false;\n-}\n-\n-\n-/* Function new_loop_vec_info.\n-\n-   Create and initialize a new loop_vec_info struct for LOOP, as well as\n-   stmt_vec_info structs for all the stmts in LOOP.  */\n-\n-loop_vec_info\n-new_loop_vec_info (struct loop *loop)\n-{\n-  loop_vec_info res;\n-  basic_block *bbs;\n-  gimple_stmt_iterator si;\n-  unsigned int i, nbbs;\n-\n-  res = (loop_vec_info) xcalloc (1, sizeof (struct _loop_vec_info));\n-  LOOP_VINFO_LOOP (res) = loop;\n-\n-  bbs = get_loop_body (loop);\n-\n-  /* Create/Update stmt_info for all stmts in the loop.  */\n-  for (i = 0; i < loop->num_nodes; i++)\n-    {\n-      basic_block bb = bbs[i];\n-\n-      /* BBs in a nested inner-loop will have been already processed (because \n-\t we will have called vect_analyze_loop_form for any nested inner-loop).\n-\t Therefore, for stmts in an inner-loop we just want to update the \n-\t STMT_VINFO_LOOP_VINFO field of their stmt_info to point to the new \n-\t loop_info of the outer-loop we are currently considering to vectorize \n-\t (instead of the loop_info of the inner-loop).\n-\t For stmts in other BBs we need to create a stmt_info from scratch.  */\n-      if (bb->loop_father != loop)\n-\t{\n-\t  /* Inner-loop bb.  */\n-\t  gcc_assert (loop->inner && bb->loop_father == loop->inner);\n-\t  for (si = gsi_start_phis (bb); !gsi_end_p (si); gsi_next (&si))\n-\t    {\n-\t      gimple phi = gsi_stmt (si);\n-\t      stmt_vec_info stmt_info = vinfo_for_stmt (phi);\n-\t      loop_vec_info inner_loop_vinfo =\n-\t\tSTMT_VINFO_LOOP_VINFO (stmt_info);\n-\t      gcc_assert (loop->inner == LOOP_VINFO_LOOP (inner_loop_vinfo));\n-\t      STMT_VINFO_LOOP_VINFO (stmt_info) = res;\n-\t    }\n-\t  for (si = gsi_start_bb (bb); !gsi_end_p (si); gsi_next (&si))\n-\t   {\n-\t      gimple stmt = gsi_stmt (si);\n-\t      stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n-\t      loop_vec_info inner_loop_vinfo =\n-\t\t STMT_VINFO_LOOP_VINFO (stmt_info);\n-\t      gcc_assert (loop->inner == LOOP_VINFO_LOOP (inner_loop_vinfo));\n-\t      STMT_VINFO_LOOP_VINFO (stmt_info) = res;\n-\t   }\n-\t}\n-      else\n-\t{\n-\t  /* bb in current nest.  */\n-\t  for (si = gsi_start_phis (bb); !gsi_end_p (si); gsi_next (&si))\n-\t    {\n-\t      gimple phi = gsi_stmt (si);\n-\t      gimple_set_uid (phi, 0);\n-\t      set_vinfo_for_stmt (phi, new_stmt_vec_info (phi, res));\n-\t    }\n-\n-\t  for (si = gsi_start_bb (bb); !gsi_end_p (si); gsi_next (&si))\n-\t    {\n-\t      gimple stmt = gsi_stmt (si);\n-\t      gimple_set_uid (stmt, 0);\n-\t      set_vinfo_for_stmt (stmt, new_stmt_vec_info (stmt, res));\n-\t    }\n-\t}\n-    }\n-\n-  /* CHECKME: We want to visit all BBs before their successors (except for \n-     latch blocks, for which this assertion wouldn't hold).  In the simple \n-     case of the loop forms we allow, a dfs order of the BBs would the same \n-     as reversed postorder traversal, so we are safe.  */\n-\n-   free (bbs);\n-   bbs = XCNEWVEC (basic_block, loop->num_nodes);\n-   nbbs = dfs_enumerate_from (loop->header, 0, bb_in_loop_p, \n-\t\t\t      bbs, loop->num_nodes, loop);\n-   gcc_assert (nbbs == loop->num_nodes);\n-\n-  LOOP_VINFO_BBS (res) = bbs;\n-  LOOP_VINFO_NITERS (res) = NULL;\n-  LOOP_VINFO_NITERS_UNCHANGED (res) = NULL;\n-  LOOP_VINFO_COST_MODEL_MIN_ITERS (res) = 0;\n-  LOOP_VINFO_VECTORIZABLE_P (res) = 0;\n-  LOOP_PEELING_FOR_ALIGNMENT (res) = 0;\n-  LOOP_VINFO_VECT_FACTOR (res) = 0;\n-  LOOP_VINFO_DATAREFS (res) = VEC_alloc (data_reference_p, heap, 10);\n-  LOOP_VINFO_DDRS (res) = VEC_alloc (ddr_p, heap, 10 * 10);\n-  LOOP_VINFO_UNALIGNED_DR (res) = NULL;\n-  LOOP_VINFO_MAY_MISALIGN_STMTS (res) =\n-    VEC_alloc (gimple, heap,\n-\t       PARAM_VALUE (PARAM_VECT_MAX_VERSION_FOR_ALIGNMENT_CHECKS));\n-  LOOP_VINFO_MAY_ALIAS_DDRS (res) =\n-    VEC_alloc (ddr_p, heap,\n-\t       PARAM_VALUE (PARAM_VECT_MAX_VERSION_FOR_ALIAS_CHECKS));\n-  LOOP_VINFO_STRIDED_STORES (res) = VEC_alloc (gimple, heap, 10);\n-  LOOP_VINFO_SLP_INSTANCES (res) = VEC_alloc (slp_instance, heap, 10);\n-  LOOP_VINFO_SLP_UNROLLING_FACTOR (res) = 1;\n-\n-  return res;\n-}\n-\n-\n-/* Function destroy_loop_vec_info.\n- \n-   Free LOOP_VINFO struct, as well as all the stmt_vec_info structs of all the \n-   stmts in the loop.  */\n-\n-void\n-destroy_loop_vec_info (loop_vec_info loop_vinfo, bool clean_stmts)\n-{\n-  struct loop *loop;\n-  basic_block *bbs;\n-  int nbbs;\n-  gimple_stmt_iterator si;\n-  int j;\n-  VEC (slp_instance, heap) *slp_instances;\n-  slp_instance instance;\n-\n-  if (!loop_vinfo)\n-    return;\n-\n-  loop = LOOP_VINFO_LOOP (loop_vinfo);\n-\n-  bbs = LOOP_VINFO_BBS (loop_vinfo);\n-  nbbs = loop->num_nodes;\n-\n-  if (!clean_stmts)\n-    {\n-      free (LOOP_VINFO_BBS (loop_vinfo));\n-      free_data_refs (LOOP_VINFO_DATAREFS (loop_vinfo));\n-      free_dependence_relations (LOOP_VINFO_DDRS (loop_vinfo));\n-      VEC_free (gimple, heap, LOOP_VINFO_MAY_MISALIGN_STMTS (loop_vinfo));\n-\n-      free (loop_vinfo);\n-      loop->aux = NULL;\n-      return;\n-    }\n-\n-  for (j = 0; j < nbbs; j++)\n-    {\n-      basic_block bb = bbs[j];\n-\n-      for (si = gsi_start_phis (bb); !gsi_end_p (si); gsi_next (&si))\n-        free_stmt_vec_info (gsi_stmt (si));\n-\n-      for (si = gsi_start_bb (bb); !gsi_end_p (si); )\n-\t{\n-\t  gimple stmt = gsi_stmt (si);\n-\t  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n-\n-\t  if (stmt_info)\n-\t    {\n-\t      /* Check if this is a \"pattern stmt\" (introduced by the \n-\t\t vectorizer during the pattern recognition pass).  */\n-\t      bool remove_stmt_p = false;\n-\t      gimple orig_stmt = STMT_VINFO_RELATED_STMT (stmt_info);\n-\t      if (orig_stmt)\n-\t\t{\n-\t\t  stmt_vec_info orig_stmt_info = vinfo_for_stmt (orig_stmt);\n-\t\t  if (orig_stmt_info\n-\t\t      && STMT_VINFO_IN_PATTERN_P (orig_stmt_info))\n-\t\t    remove_stmt_p = true; \n-\t\t}\n-\t\t\t\n-\t      /* Free stmt_vec_info.  */\n-\t      free_stmt_vec_info (stmt);\n-\n-\t      /* Remove dead \"pattern stmts\".  */\n-\t      if (remove_stmt_p)\n-\t        gsi_remove (&si, true);\n-\t    }\n-\t  gsi_next (&si);\n-\t}\n-    }\n-\n-  free (LOOP_VINFO_BBS (loop_vinfo));\n-  free_data_refs (LOOP_VINFO_DATAREFS (loop_vinfo));\n-  free_dependence_relations (LOOP_VINFO_DDRS (loop_vinfo));\n-  VEC_free (gimple, heap, LOOP_VINFO_MAY_MISALIGN_STMTS (loop_vinfo));\n-  VEC_free (ddr_p, heap, LOOP_VINFO_MAY_ALIAS_DDRS (loop_vinfo));\n-  slp_instances = LOOP_VINFO_SLP_INSTANCES (loop_vinfo);\n-  for (j = 0; VEC_iterate (slp_instance, slp_instances, j, instance); j++)\n-    vect_free_slp_instance (instance);\n-\n-  VEC_free (slp_instance, heap, LOOP_VINFO_SLP_INSTANCES (loop_vinfo));\n-  VEC_free (gimple, heap, LOOP_VINFO_STRIDED_STORES (loop_vinfo));\n-\n-  free (loop_vinfo);\n-  loop->aux = NULL;\n-}\n-\n-\n-/* Function vect_force_dr_alignment_p.\n-\n-   Returns whether the alignment of a DECL can be forced to be aligned\n-   on ALIGNMENT bit boundary.  */\n-\n-bool \n-vect_can_force_dr_alignment_p (const_tree decl, unsigned int alignment)\n-{\n-  if (TREE_CODE (decl) != VAR_DECL)\n-    return false;\n-\n-  if (DECL_EXTERNAL (decl))\n-    return false;\n-\n-  if (TREE_ASM_WRITTEN (decl))\n-    return false;\n-\n-  if (TREE_STATIC (decl))\n-    return (alignment <= MAX_OFILE_ALIGNMENT);\n-  else\n-    return (alignment <= MAX_STACK_ALIGNMENT);\n-}\n-\n-\n-/* Function get_vectype_for_scalar_type.\n-\n-   Returns the vector type corresponding to SCALAR_TYPE as supported\n-   by the target.  */\n-\n-tree\n-get_vectype_for_scalar_type (tree scalar_type)\n-{\n-  enum machine_mode inner_mode = TYPE_MODE (scalar_type);\n-  int nbytes = GET_MODE_SIZE (inner_mode);\n-  int nunits;\n-  tree vectype;\n-\n-  if (nbytes == 0 || nbytes >= UNITS_PER_SIMD_WORD (inner_mode))\n-    return NULL_TREE;\n-\n-  /* FORNOW: Only a single vector size per mode (UNITS_PER_SIMD_WORD)\n-     is expected.  */\n-  nunits = UNITS_PER_SIMD_WORD (inner_mode) / nbytes;\n-\n-  vectype = build_vector_type (scalar_type, nunits);\n-  if (vect_print_dump_info (REPORT_DETAILS))\n-    {\n-      fprintf (vect_dump, \"get vectype with %d units of type \", nunits);\n-      print_generic_expr (vect_dump, scalar_type, TDF_SLIM);\n-    }\n-\n-  if (!vectype)\n-    return NULL_TREE;\n-\n-  if (vect_print_dump_info (REPORT_DETAILS))\n-    {\n-      fprintf (vect_dump, \"vectype: \");\n-      print_generic_expr (vect_dump, vectype, TDF_SLIM);\n-    }\n-\n-  if (!VECTOR_MODE_P (TYPE_MODE (vectype))\n-      && !INTEGRAL_MODE_P (TYPE_MODE (vectype)))\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"mode not supported by target.\");\n-      return NULL_TREE;\n-    }\n-\n-  return vectype;\n-}\n-\n-\n-/* Function vect_supportable_dr_alignment\n-\n-   Return whether the data reference DR is supported with respect to its\n-   alignment.  */\n-\n-enum dr_alignment_support\n-vect_supportable_dr_alignment (struct data_reference *dr)\n-{\n-  gimple stmt = DR_STMT (dr);\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n-  tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n-  enum machine_mode mode = (int) TYPE_MODE (vectype);\n-  struct loop *vect_loop = LOOP_VINFO_LOOP (STMT_VINFO_LOOP_VINFO (stmt_info));\n-  bool nested_in_vect_loop = nested_in_vect_loop_p (vect_loop, stmt);\n-  bool invariant_in_outerloop = false;\n-\n-  if (aligned_access_p (dr))\n-    return dr_aligned;\n-\n-  if (nested_in_vect_loop)\n-    {\n-      tree outerloop_step = STMT_VINFO_DR_STEP (stmt_info);\n-      invariant_in_outerloop =\n-\t(tree_int_cst_compare (outerloop_step, size_zero_node) == 0);\n-    }\n-\n-  /* Possibly unaligned access.  */\n-\n-  /* We can choose between using the implicit realignment scheme (generating\n-     a misaligned_move stmt) and the explicit realignment scheme (generating\n-     aligned loads with a REALIGN_LOAD). There are two variants to the explicit\n-     realignment scheme: optimized, and unoptimized.\n-     We can optimize the realignment only if the step between consecutive\n-     vector loads is equal to the vector size.  Since the vector memory\n-     accesses advance in steps of VS (Vector Size) in the vectorized loop, it\n-     is guaranteed that the misalignment amount remains the same throughout the\n-     execution of the vectorized loop.  Therefore, we can create the\n-     \"realignment token\" (the permutation mask that is passed to REALIGN_LOAD)\n-     at the loop preheader.\n-\n-     However, in the case of outer-loop vectorization, when vectorizing a\n-     memory access in the inner-loop nested within the LOOP that is now being\n-     vectorized, while it is guaranteed that the misalignment of the\n-     vectorized memory access will remain the same in different outer-loop\n-     iterations, it is *not* guaranteed that is will remain the same throughout\n-     the execution of the inner-loop.  This is because the inner-loop advances\n-     with the original scalar step (and not in steps of VS).  If the inner-loop\n-     step happens to be a multiple of VS, then the misalignment remains fixed\n-     and we can use the optimized realignment scheme.  For example:\n-\n-      for (i=0; i<N; i++)\n-        for (j=0; j<M; j++)\n-          s += a[i+j];\n-\n-     When vectorizing the i-loop in the above example, the step between\n-     consecutive vector loads is 1, and so the misalignment does not remain\n-     fixed across the execution of the inner-loop, and the realignment cannot\n-     be optimized (as illustrated in the following pseudo vectorized loop):\n-\n-      for (i=0; i<N; i+=4)\n-        for (j=0; j<M; j++){\n-          vs += vp[i+j]; // misalignment of &vp[i+j] is {0,1,2,3,0,1,2,3,...}\n-                         // when j is {0,1,2,3,4,5,6,7,...} respectively.\n-                         // (assuming that we start from an aligned address).\n-          }\n-\n-     We therefore have to use the unoptimized realignment scheme:\n-\n-      for (i=0; i<N; i+=4)\n-          for (j=k; j<M; j+=4)\n-          vs += vp[i+j]; // misalignment of &vp[i+j] is always k (assuming\n-                           // that the misalignment of the initial address is\n-                           // 0).\n-\n-     The loop can then be vectorized as follows:\n-\n-      for (k=0; k<4; k++){\n-        rt = get_realignment_token (&vp[k]);\n-        for (i=0; i<N; i+=4){\n-          v1 = vp[i+k];\n-          for (j=k; j<M; j+=4){\n-            v2 = vp[i+j+VS-1];\n-            va = REALIGN_LOAD <v1,v2,rt>;\n-            vs += va;\n-            v1 = v2;\n-          }\n-        }\n-    } */\n-\n-  if (DR_IS_READ (dr))\n-    {\n-      if (optab_handler (vec_realign_load_optab, mode)->insn_code != \n-\t\t\t\t\t\t   \t     CODE_FOR_nothing\n-\t  && (!targetm.vectorize.builtin_mask_for_load\n-\t      || targetm.vectorize.builtin_mask_for_load ()))\n-\t{\n-\t  tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n-\t  if (nested_in_vect_loop\n-\t      && (TREE_INT_CST_LOW (DR_STEP (dr))\n-\t\t  != GET_MODE_SIZE (TYPE_MODE (vectype))))\n-\t    return dr_explicit_realign;\n-\t  else\n-\t    return dr_explicit_realign_optimized;\n-\t}\n-\n-      if (optab_handler (movmisalign_optab, mode)->insn_code != \n-\t\t\t\t\t\t\t     CODE_FOR_nothing)\n-\t/* Can't software pipeline the loads, but can at least do them.  */\n-\treturn dr_unaligned_supported;\n-    }\n-\n-  /* Unsupported.  */\n-  return dr_unaligned_unsupported;\n-}\n-\n-\n-/* Function vect_is_simple_use.\n+/* Loop and basic block vectorizer.\n \n-   Input:\n-   LOOP - the loop that is being vectorized.\n-   OPERAND - operand of a stmt in LOOP.\n-   DEF - the defining stmt in case OPERAND is an SSA_NAME.\n-\n-   Returns whether a stmt with OPERAND can be vectorized.\n-   Supportable operands are constants, loop invariants, and operands that are\n-   defined by the current iteration of the loop. Unsupportable operands are \n-   those that are defined by a previous iteration of the loop (as is the case\n-   in reduction/induction computations).  */\n-\n-bool\n-vect_is_simple_use (tree operand, loop_vec_info loop_vinfo, gimple *def_stmt,\n-\t\t    tree *def, enum vect_def_type *dt)\n-{ \n-  basic_block bb;\n-  stmt_vec_info stmt_vinfo;\n-  struct loop *loop = LOOP_VINFO_LOOP (loop_vinfo);\n-\n-  *def_stmt = NULL;\n-  *def = NULL_TREE;\n+  This file contains drivers for the three vectorizers: \n+  (1) loop vectorizer (inter-iteration parallelism), \n+  (2) loop-aware SLP (intra-iteration parallelism) (invoked by the loop\n+      vectorizer)\n+  (3) BB vectorizer (out-of-loops), aka SLP\n   \n-  if (vect_print_dump_info (REPORT_DETAILS))\n-    {\n-      fprintf (vect_dump, \"vect_is_simple_use: operand \");\n-      print_generic_expr (vect_dump, operand, TDF_SLIM);\n-    }\n-    \n-  if (TREE_CODE (operand) == INTEGER_CST || TREE_CODE (operand) == REAL_CST)\n-    {\n-      *dt = vect_constant_def;\n-      return true;\n-    }\n-  if (is_gimple_min_invariant (operand))\n-    {\n-      *def = operand;\n-      *dt = vect_invariant_def;\n-      return true;\n-    }\n-\n-  if (TREE_CODE (operand) == PAREN_EXPR)\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"non-associatable copy.\");\n-      operand = TREE_OPERAND (operand, 0);\n-    }\n-  if (TREE_CODE (operand) != SSA_NAME)\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"not ssa-name.\");\n-      return false;\n-    }\n-    \n-  *def_stmt = SSA_NAME_DEF_STMT (operand);\n-  if (*def_stmt == NULL)\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"no def_stmt.\");\n-      return false;\n-    }\n-\n-  if (vect_print_dump_info (REPORT_DETAILS))\n-    {\n-      fprintf (vect_dump, \"def_stmt: \");\n-      print_gimple_stmt (vect_dump, *def_stmt, 0, TDF_SLIM);\n-    }\n-\n-  /* empty stmt is expected only in case of a function argument.\n-     (Otherwise - we expect a phi_node or a GIMPLE_ASSIGN).  */\n-  if (gimple_nop_p (*def_stmt))\n-    {\n-      *def = operand;\n-      *dt = vect_invariant_def;\n-      return true;\n-    }\n-\n-  bb = gimple_bb (*def_stmt);\n-  if (!flow_bb_inside_loop_p (loop, bb))\n-    *dt = vect_invariant_def;\n-  else\n-    {\n-      stmt_vinfo = vinfo_for_stmt (*def_stmt);\n-      *dt = STMT_VINFO_DEF_TYPE (stmt_vinfo);\n-    }\n-\n-  if (*dt == vect_unknown_def_type)\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"Unsupported pattern.\");\n-      return false;\n-    }\n-\n-  if (vect_print_dump_info (REPORT_DETAILS))\n-    fprintf (vect_dump, \"type of def: %d.\",*dt);\n-\n-  switch (gimple_code (*def_stmt))\n-    {\n-    case GIMPLE_PHI:\n-      *def = gimple_phi_result (*def_stmt);\n-      break;\n-\n-    case GIMPLE_ASSIGN:\n-      *def = gimple_assign_lhs (*def_stmt);\n-      break;\n-\n-    case GIMPLE_CALL:\n-      *def = gimple_call_lhs (*def_stmt);\n-      if (*def != NULL)\n-\tbreak;\n-      /* FALLTHRU */\n-    default:\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"unsupported defining stmt: \");\n-      return false;\n-    }\n-\n-  return true;\n-}\n-\n-\n-/* Function supportable_widening_operation\n-\n-   Check whether an operation represented by the code CODE is a \n-   widening operation that is supported by the target platform in \n-   vector form (i.e., when operating on arguments of type VECTYPE).\n-    \n-   Widening operations we currently support are NOP (CONVERT), FLOAT\n-   and WIDEN_MULT.  This function checks if these operations are supported\n-   by the target platform either directly (via vector tree-codes), or via\n-   target builtins.\n-\n-   Output:\n-   - CODE1 and CODE2 are codes of vector operations to be used when \n-   vectorizing the operation, if available. \n-   - DECL1 and DECL2 are decls of target builtin functions to be used\n-   when vectorizing the operation, if available. In this case,\n-   CODE1 and CODE2 are CALL_EXPR.  \n-   - MULTI_STEP_CVT determines the number of required intermediate steps in\n-   case of multi-step conversion (like char->short->int - in that case\n-   MULTI_STEP_CVT will be 1).\n-   - INTERM_TYPES contains the intermediate type required to perform the \n-   widening operation (short in the above example).  */   \n-\n-bool\n-supportable_widening_operation (enum tree_code code, gimple stmt, tree vectype,\n-                                tree *decl1, tree *decl2,\n-                                enum tree_code *code1, enum tree_code *code2,\n-                                int *multi_step_cvt,\n-                                VEC (tree, heap) **interm_types)\n-{\n-  stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n-  loop_vec_info loop_info = STMT_VINFO_LOOP_VINFO (stmt_info);\n-  struct loop *vect_loop = LOOP_VINFO_LOOP (loop_info);\n-  bool ordered_p;\n-  enum machine_mode vec_mode;\n-  enum insn_code icode1 = 0, icode2 = 0;\n-  optab optab1, optab2;\n-  tree type = gimple_expr_type (stmt);\n-  tree wide_vectype = get_vectype_for_scalar_type (type);\n-  enum tree_code c1, c2;\n-\n-  /* The result of a vectorized widening operation usually requires two vectors\n-     (because the widened results do not fit int one vector). The generated \n-     vector results would normally be expected to be generated in the same \n-     order as in the original scalar computation, i.e. if 8 results are\n-     generated in each vector iteration, they are to be organized as follows:\n-        vect1: [res1,res2,res3,res4], vect2: [res5,res6,res7,res8]. \n-\n-     However, in the special case that the result of the widening operation is \n-     used in a reduction computation only, the order doesn't matter (because\n-     when vectorizing a reduction we change the order of the computation). \n-     Some targets can take advantage of this and generate more efficient code.\n-     For example, targets like Altivec, that support widen_mult using a sequence\n-     of {mult_even,mult_odd} generate the following vectors:\n-        vect1: [res1,res3,res5,res7], vect2: [res2,res4,res6,res8].\n-\n-     When vectorizing outer-loops, we execute the inner-loop sequentially\n-     (each vectorized inner-loop iteration contributes to VF outer-loop \n-     iterations in parallel). We therefore don't allow to change the order \n-     of the computation in the inner-loop during outer-loop vectorization.  */\n-\n-   if (STMT_VINFO_RELEVANT (stmt_info) == vect_used_by_reduction\n-       && !nested_in_vect_loop_p (vect_loop, stmt))\n-     ordered_p = false;\n-   else\n-     ordered_p = true;\n-\n-  if (!ordered_p\n-      && code == WIDEN_MULT_EXPR\n-      && targetm.vectorize.builtin_mul_widen_even\n-      && targetm.vectorize.builtin_mul_widen_even (vectype)\n-      && targetm.vectorize.builtin_mul_widen_odd\n-      && targetm.vectorize.builtin_mul_widen_odd (vectype))\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"Unordered widening operation detected.\");\n-\n-      *code1 = *code2 = CALL_EXPR;\n-      *decl1 = targetm.vectorize.builtin_mul_widen_even (vectype);\n-      *decl2 = targetm.vectorize.builtin_mul_widen_odd (vectype);\n-      return true;\n-    }\n-\n-  switch (code)\n-    {\n-    case WIDEN_MULT_EXPR:\n-      if (BYTES_BIG_ENDIAN)\n-        {\n-          c1 = VEC_WIDEN_MULT_HI_EXPR;\n-          c2 = VEC_WIDEN_MULT_LO_EXPR;\n-        }\n-      else\n-        {\n-          c2 = VEC_WIDEN_MULT_HI_EXPR;\n-          c1 = VEC_WIDEN_MULT_LO_EXPR;\n-        }\n-      break;\n-\n-    CASE_CONVERT:\n-      if (BYTES_BIG_ENDIAN)\n-        {\n-          c1 = VEC_UNPACK_HI_EXPR;\n-          c2 = VEC_UNPACK_LO_EXPR;\n-        }\n-      else\n-        {\n-          c2 = VEC_UNPACK_HI_EXPR;\n-          c1 = VEC_UNPACK_LO_EXPR;\n-        }\n-      break;\n-\n-    case FLOAT_EXPR:\n-      if (BYTES_BIG_ENDIAN)\n-        {\n-          c1 = VEC_UNPACK_FLOAT_HI_EXPR;\n-          c2 = VEC_UNPACK_FLOAT_LO_EXPR;\n-        }\n-      else\n-        {\n-          c2 = VEC_UNPACK_FLOAT_HI_EXPR;\n-          c1 = VEC_UNPACK_FLOAT_LO_EXPR;\n-        }\n-      break;\n-\n-    case FIX_TRUNC_EXPR:\n-      /* ??? Not yet implemented due to missing VEC_UNPACK_FIX_TRUNC_HI_EXPR/\n-\t VEC_UNPACK_FIX_TRUNC_LO_EXPR tree codes and optabs used for\n-\t computing the operation.  */\n-      return false;\n-\n-    default:\n-      gcc_unreachable ();\n-    }\n-\n-  if (code == FIX_TRUNC_EXPR)\n-    {\n-      /* The signedness is determined from output operand.  */\n-      optab1 = optab_for_tree_code (c1, type, optab_default);\n-      optab2 = optab_for_tree_code (c2, type, optab_default);\n-    }\n-  else\n-    {\n-      optab1 = optab_for_tree_code (c1, vectype, optab_default);\n-      optab2 = optab_for_tree_code (c2, vectype, optab_default);\n-    }\n+  The rest of the vectorizer's code is organized as follows:\n+  - tree-vect-loop.c - loop specific parts such as reductions, etc. These are \n+    used by drivers (1) and (2). \n+  - tree-vect-loop-manip.c - vectorizer's loop control-flow utilities, used by \n+    drivers (1) and (2). \n+  - tree-vect-slp.c - BB vectorization specific analysis and transformation, \n+    used by drivers (2) and (3).\n+  - tree-vect-stmts.c - statements analysis and transformation (used by all).\n+  - tree-vect-data-refs.c - vectorizer specific data-refs analysis and \n+    manipulations (used by all).\n+  - tree-vect-patterns.c - vectorizable code patterns detector (used by all)\n+\n+  Here's a poor attempt at illustrating that:\n+\n+     tree-vectorizer.c:\n+     loop_vect()  loop_aware_slp()  slp_vect()\n+          |        /           \\          /\n+          |       /             \\        /\n+          tree-vect-loop.c  tree-vect-slp.c\n+                | \\      \\  /      /   |\n+                |  \\      \\/      /    |\n+                |   \\     /\\     /     |\n+                |    \\   /  \\   /      |\n+         tree-vect-stmts.c  tree-vect-data-refs.c\n+                       \\      /\n+                    tree-vect-patterns.c\n+*/\n \n-  if (!optab1 || !optab2)\n-    return false;\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"ggc.h\"\n+#include \"tree.h\"\n+#include \"diagnostic.h\"\n+#include \"tree-flow.h\"\n+#include \"tree-dump.h\"\n+#include \"cfgloop.h\"\n+#include \"cfglayout.h\"\n+#include \"tree-vectorizer.h\"\n+#include \"tree-pass.h\"\n \n-  vec_mode = TYPE_MODE (vectype);\n-  if ((icode1 = optab_handler (optab1, vec_mode)->insn_code) == CODE_FOR_nothing\n-       || (icode2 = optab_handler (optab2, vec_mode)->insn_code)\n-                                                       == CODE_FOR_nothing)\n-    return false;\n+/* vect_dump will be set to stderr or dump_file if exist.  */\n+FILE *vect_dump;\n \n-  /* Check if it's a multi-step conversion that can be done using intermediate \n-     types.  */\n-  if (insn_data[icode1].operand[0].mode != TYPE_MODE (wide_vectype)\n-       || insn_data[icode2].operand[0].mode != TYPE_MODE (wide_vectype))\n-    {\n-      int i;\n-      tree prev_type = vectype, intermediate_type;\n-      enum machine_mode intermediate_mode, prev_mode = vec_mode;\n-      optab optab3, optab4;\n-\n-      if (!CONVERT_EXPR_CODE_P (code))\n-        return false;\n-      \n-      *code1 = c1;\n-      *code2 = c2;\n-    \n-      /* We assume here that there will not be more than MAX_INTERM_CVT_STEPS\n-         intermediate  steps in promotion sequence. We try MAX_INTERM_CVT_STEPS\n-         to get to NARROW_VECTYPE, and fail if we do not.  */\n-      *interm_types = VEC_alloc (tree, heap, MAX_INTERM_CVT_STEPS);\n-      for (i = 0; i < 3; i++)\n-        {\n-          intermediate_mode = insn_data[icode1].operand[0].mode;\n-          intermediate_type = lang_hooks.types.type_for_mode (intermediate_mode,\n-                                                     TYPE_UNSIGNED (prev_type));\n-          optab3 = optab_for_tree_code (c1, intermediate_type, optab_default);\n-          optab4 = optab_for_tree_code (c2, intermediate_type, optab_default);\n-\n-          if (!optab3 || !optab4\n-              || (icode1 = optab1->handlers[(int) prev_mode].insn_code)\n-                                                        == CODE_FOR_nothing\n-              || insn_data[icode1].operand[0].mode != intermediate_mode\n-              || (icode2 = optab2->handlers[(int) prev_mode].insn_code)\n-                                                        == CODE_FOR_nothing\n-              || insn_data[icode2].operand[0].mode != intermediate_mode\n-              || (icode1 = optab3->handlers[(int) intermediate_mode].insn_code) \n-                                                        == CODE_FOR_nothing\n-              || (icode2 = optab4->handlers[(int) intermediate_mode].insn_code)\n-                                                        == CODE_FOR_nothing)\n-            return false;\n-\n-          VEC_quick_push (tree, *interm_types, intermediate_type);\n-          (*multi_step_cvt)++;\n-\n-          if (insn_data[icode1].operand[0].mode == TYPE_MODE (wide_vectype)\n-              && insn_data[icode2].operand[0].mode == TYPE_MODE (wide_vectype))\n-            return true;\n-\n-          prev_type = intermediate_type;\n-          prev_mode = intermediate_mode;\n-        }\n+/* vect_verbosity_level set to an invalid value \n+   to mark that it's uninitialized.  */\n+enum verbosity_levels vect_verbosity_level = MAX_VERBOSITY_LEVEL;\n \n-       return false;\n-    }\n+/* Loop location.  */\n+LOC vect_loop_location;\n \n-  *code1 = c1;\n-  *code2 = c2;\n-  return true;\n-}\n+/* Bitmap of virtual variables to be renamed.  */\n+bitmap vect_memsyms_to_rename;\n \n+/* Vector mapping GIMPLE stmt to stmt_vec_info. */\n+VEC(vec_void_p,heap) *stmt_vec_info_vec;\n \n-/* Function supportable_narrowing_operation\n+\f\n \n-   Check whether an operation represented by the code CODE is a \n-   narrowing operation that is supported by the target platform in \n-   vector form (i.e., when operating on arguments of type VECTYPE).\n-    \n-   Narrowing operations we currently support are NOP (CONVERT) and\n-   FIX_TRUNC. This function checks if these operations are supported by\n-   the target platform directly via vector tree-codes.\n+/* Function vect_set_verbosity_level.\n \n-   Output:\n-   - CODE1 is the code of a vector operation to be used when \n-   vectorizing the operation, if available. \n-   - MULTI_STEP_CVT determines the number of required intermediate steps in\n-   case of multi-step conversion (like int->short->char - in that case\n-   MULTI_STEP_CVT will be 1).\n-   - INTERM_TYPES contains the intermediate type required to perform the\n-   narrowing operation (short in the above example).   */ \n+   Called from toplev.c upon detection of the\n+   -ftree-vectorizer-verbose=N option.  */\n \n-bool\n-supportable_narrowing_operation (enum tree_code code,\n-\t\t\t\t const_gimple stmt, tree vectype,\n-\t\t\t\t enum tree_code *code1, int *multi_step_cvt,\n-                                 VEC (tree, heap) **interm_types)\n+void\n+vect_set_verbosity_level (const char *val)\n {\n-  enum machine_mode vec_mode;\n-  enum insn_code icode1;\n-  optab optab1, interm_optab;\n-  tree type = gimple_expr_type (stmt);\n-  tree narrow_vectype = get_vectype_for_scalar_type (type);\n-  enum tree_code c1;\n-  tree intermediate_type, prev_type;\n-  int i;\n-\n-  switch (code)\n-    {\n-    CASE_CONVERT:\n-      c1 = VEC_PACK_TRUNC_EXPR;\n-      break;\n-\n-    case FIX_TRUNC_EXPR:\n-      c1 = VEC_PACK_FIX_TRUNC_EXPR;\n-      break;\n-\n-    case FLOAT_EXPR:\n-      /* ??? Not yet implemented due to missing VEC_PACK_FLOAT_EXPR\n-\t tree code and optabs used for computing the operation.  */\n-      return false;\n-\n-    default:\n-      gcc_unreachable ();\n-    }\n-\n-  if (code == FIX_TRUNC_EXPR)\n-    /* The signedness is determined from output operand.  */\n-    optab1 = optab_for_tree_code (c1, type, optab_default);\n-  else\n-    optab1 = optab_for_tree_code (c1, vectype, optab_default);\n-\n-  if (!optab1)\n-    return false;\n-\n-  vec_mode = TYPE_MODE (vectype);\n-  if ((icode1 = optab_handler (optab1, vec_mode)->insn_code) \n-       == CODE_FOR_nothing)\n-    return false;\n-\n-  /* Check if it's a multi-step conversion that can be done using intermediate\n-     types.  */\n-  if (insn_data[icode1].operand[0].mode != TYPE_MODE (narrow_vectype))\n-    {\n-      enum machine_mode intermediate_mode, prev_mode = vec_mode;\n-\n-      *code1 = c1;\n-      prev_type = vectype;\n-      /* We assume here that there will not be more than MAX_INTERM_CVT_STEPS\n-         intermediate  steps in promotion sequence. We try MAX_INTERM_CVT_STEPS\n-         to get to NARROW_VECTYPE, and fail if we do not.  */\n-      *interm_types = VEC_alloc (tree, heap, MAX_INTERM_CVT_STEPS);\n-      for (i = 0; i < 3; i++)\n-        {\n-          intermediate_mode = insn_data[icode1].operand[0].mode;\n-          intermediate_type = lang_hooks.types.type_for_mode (intermediate_mode,\n-                                                     TYPE_UNSIGNED (prev_type));\n-          interm_optab = optab_for_tree_code (c1, intermediate_type, \n-                                              optab_default);\n-          if (!interm_optab  \n-              || (icode1 = optab1->handlers[(int) prev_mode].insn_code)\n-                                                        == CODE_FOR_nothing\n-              || insn_data[icode1].operand[0].mode != intermediate_mode\n-              || (icode1 \n-                  = interm_optab->handlers[(int) intermediate_mode].insn_code)\n-                 == CODE_FOR_nothing)\n-            return false;\n-\n-          VEC_quick_push (tree, *interm_types, intermediate_type);\n-          (*multi_step_cvt)++;\n-\n-          if (insn_data[icode1].operand[0].mode == TYPE_MODE (narrow_vectype))\n-            return true;\n-\n-          prev_type = intermediate_type;\n-          prev_mode = intermediate_mode;\n-        }\n-\n-      return false;\n-    }\n+   unsigned int vl;\n \n-  *code1 = c1;\n-  return true;\n+   vl = atoi (val);\n+   if (vl < MAX_VERBOSITY_LEVEL)\n+     vect_verbosity_level = vl;\n+   else\n+     vect_verbosity_level = MAX_VERBOSITY_LEVEL - 1;\n }\n \n \n-/* Function reduction_code_for_scalar_code\n-\n-   Input:\n-   CODE - tree_code of a reduction operations.\n-\n-   Output:\n-   REDUC_CODE - the corresponding tree-code to be used to reduce the\n-      vector of partial results into a single scalar result (which\n-      will also reside in a vector).\n-\n-   Return TRUE if a corresponding REDUC_CODE was found, FALSE otherwise.  */\n-\n-bool\n-reduction_code_for_scalar_code (enum tree_code code,\n-                                enum tree_code *reduc_code)\n-{\n-  switch (code)\n-  {\n-  case MAX_EXPR:\n-    *reduc_code = REDUC_MAX_EXPR;\n-    return true;\n-\n-  case MIN_EXPR:\n-    *reduc_code = REDUC_MIN_EXPR;\n-    return true;\n-\n-  case PLUS_EXPR:\n-    *reduc_code = REDUC_PLUS_EXPR;\n-    return true;\n-\n-  default:\n-    return false;\n-  }\n-}\n+/* Function vect_set_dump_settings.\n \n-/* Error reporting helper for vect_is_simple_reduction below. GIMPLE statement\n-   STMT is printed with a message MSG. */\n+   Fix the verbosity level of the vectorizer if the\n+   requested level was not set explicitly using the flag\n+   -ftree-vectorizer-verbose=N.\n+   Decide where to print the debugging information (dump_file/stderr).\n+   If the user defined the verbosity level, but there is no dump file,\n+   print to stderr, otherwise print to the dump file.  */\n \n static void\n-report_vect_op (gimple stmt, const char *msg)\n-{\n-  fprintf (vect_dump, \"%s\", msg);\n-  print_gimple_stmt (vect_dump, stmt, 0, TDF_SLIM);\n-}\n-\n-/* Function vect_is_simple_reduction\n-\n-   Detect a cross-iteration def-use cycle that represents a simple\n-   reduction computation. We look for the following pattern:\n-\n-   loop_header:\n-     a1 = phi < a0, a2 >\n-     a3 = ...\n-     a2 = operation (a3, a1)\n-  \n-   such that:\n-   1. operation is commutative and associative and it is safe to \n-      change the order of the computation.\n-   2. no uses for a2 in the loop (a2 is used out of the loop)\n-   3. no uses of a1 in the loop besides the reduction operation.\n-\n-   Condition 1 is tested here.\n-   Conditions 2,3 are tested in vect_mark_stmts_to_be_vectorized.  */\n-\n-gimple\n-vect_is_simple_reduction (loop_vec_info loop_info, gimple phi)\n+vect_set_dump_settings (void)\n {\n-  struct loop *loop = (gimple_bb (phi))->loop_father;\n-  struct loop *vect_loop = LOOP_VINFO_LOOP (loop_info);\n-  edge latch_e = loop_latch_edge (loop);\n-  tree loop_arg = PHI_ARG_DEF_FROM_EDGE (phi, latch_e);\n-  gimple def_stmt, def1, def2;\n-  enum tree_code code;\n-  tree op1, op2;\n-  tree type;\n-  int nloop_uses;\n-  tree name;\n-  imm_use_iterator imm_iter;\n-  use_operand_p use_p;\n-\n-  gcc_assert (loop == vect_loop || flow_loop_nested_p (vect_loop, loop));\n-\n-  name = PHI_RESULT (phi);\n-  nloop_uses = 0;\n-  FOR_EACH_IMM_USE_FAST (use_p, imm_iter, name)\n-    {\n-      gimple use_stmt = USE_STMT (use_p);\n-      if (flow_bb_inside_loop_p (loop, gimple_bb (use_stmt))\n-\t  && vinfo_for_stmt (use_stmt)\n-\t  && !is_pattern_stmt_p (vinfo_for_stmt (use_stmt)))\n-        nloop_uses++;\n-      if (nloop_uses > 1)\n-        {\n-          if (vect_print_dump_info (REPORT_DETAILS))\n-            fprintf (vect_dump, \"reduction used in loop.\");\n-          return NULL;\n-        }\n-    }\n-\n-  if (TREE_CODE (loop_arg) != SSA_NAME)\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-\t{\n-\t  fprintf (vect_dump, \"reduction: not ssa_name: \");\n-\t  print_generic_expr (vect_dump, loop_arg, TDF_SLIM);\n-\t}\n-      return NULL;\n-    }\n-\n-  def_stmt = SSA_NAME_DEF_STMT (loop_arg);\n-  if (!def_stmt)\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-\tfprintf (vect_dump, \"reduction: no def_stmt.\");\n-      return NULL;\n-    }\n-\n-  if (!is_gimple_assign (def_stmt))\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        print_gimple_stmt (vect_dump, def_stmt, 0, TDF_SLIM);\n-      return NULL;\n-    }\n-\n-  name = gimple_assign_lhs (def_stmt);\n-  nloop_uses = 0;\n-  FOR_EACH_IMM_USE_FAST (use_p, imm_iter, name)\n-    {\n-      gimple use_stmt = USE_STMT (use_p);\n-      if (flow_bb_inside_loop_p (loop, gimple_bb (use_stmt))\n-\t  && vinfo_for_stmt (use_stmt)\n-\t  && !is_pattern_stmt_p (vinfo_for_stmt (use_stmt)))\n-\tnloop_uses++;\n-      if (nloop_uses > 1)\n-\t{\n-\t  if (vect_print_dump_info (REPORT_DETAILS))\n-\t    fprintf (vect_dump, \"reduction used in loop.\");\n-\t  return NULL;\n-\t}\n-    }\n-\n-  code = gimple_assign_rhs_code (def_stmt);\n-\n-  if (!commutative_tree_code (code) || !associative_tree_code (code))\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        report_vect_op (def_stmt, \"reduction: not commutative/associative: \");\n-      return NULL;\n-    }\n-\n-  if (get_gimple_rhs_class (code) != GIMPLE_BINARY_RHS)\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-\treport_vect_op (def_stmt, \"reduction: not binary operation: \");\n-      return NULL;\n-    }\n-\n-  op1 = gimple_assign_rhs1 (def_stmt);\n-  op2 = gimple_assign_rhs2 (def_stmt);\n-  if (TREE_CODE (op1) != SSA_NAME || TREE_CODE (op2) != SSA_NAME)\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-\treport_vect_op (def_stmt, \"reduction: uses not ssa_names: \");\n-      return NULL;\n-    }\n-\n-  /* Check that it's ok to change the order of the computation.  */\n-  type = TREE_TYPE (gimple_assign_lhs (def_stmt));\n-  if (TYPE_MAIN_VARIANT (type) != TYPE_MAIN_VARIANT (TREE_TYPE (op1))\n-      || TYPE_MAIN_VARIANT (type) != TYPE_MAIN_VARIANT (TREE_TYPE (op2)))\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        {\n-          fprintf (vect_dump, \"reduction: multiple types: operation type: \");\n-          print_generic_expr (vect_dump, type, TDF_SLIM);\n-          fprintf (vect_dump, \", operands types: \");\n-          print_generic_expr (vect_dump, TREE_TYPE (op1), TDF_SLIM);\n-          fprintf (vect_dump, \",\");\n-          print_generic_expr (vect_dump, TREE_TYPE (op2), TDF_SLIM);\n-        }\n-      return NULL;\n-    }\n-\n-  /* Generally, when vectorizing a reduction we change the order of the\n-     computation.  This may change the behavior of the program in some\n-     cases, so we need to check that this is ok.  One exception is when \n-     vectorizing an outer-loop: the inner-loop is executed sequentially,\n-     and therefore vectorizing reductions in the inner-loop during\n-     outer-loop vectorization is safe.  */\n-\n-  /* CHECKME: check for !flag_finite_math_only too?  */\n-  if (SCALAR_FLOAT_TYPE_P (type) && !flag_associative_math\n-      && !nested_in_vect_loop_p (vect_loop, def_stmt)) \n-    {\n-      /* Changing the order of operations changes the semantics.  */\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-\treport_vect_op (def_stmt, \"reduction: unsafe fp math optimization: \");\n-      return NULL;\n-    }\n-  else if (INTEGRAL_TYPE_P (type) && TYPE_OVERFLOW_TRAPS (type)\n-\t   && !nested_in_vect_loop_p (vect_loop, def_stmt))\n-    {\n-      /* Changing the order of operations changes the semantics.  */\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-\treport_vect_op (def_stmt, \"reduction: unsafe int math optimization: \");\n-      return NULL;\n-    }\n-  else if (SAT_FIXED_POINT_TYPE_P (type))\n-    {\n-      /* Changing the order of operations changes the semantics.  */\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-\treport_vect_op (def_stmt, \n-\t\t\t\"reduction: unsafe fixed-point math optimization: \");\n-      return NULL;\n-    }\n+  vect_dump = dump_file;\n \n-  /* reduction is safe. we're dealing with one of the following:\n-     1) integer arithmetic and no trapv\n-     2) floating point arithmetic, and special flags permit this optimization.\n-   */\n-  def1 = SSA_NAME_DEF_STMT (op1);\n-  def2 = SSA_NAME_DEF_STMT (op2);\n-  if (!def1 || !def2 || gimple_nop_p (def1) || gimple_nop_p (def2))\n+  /* Check if the verbosity level was defined by the user:  */\n+  if (vect_verbosity_level != MAX_VERBOSITY_LEVEL)\n     {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-\treport_vect_op (def_stmt, \"reduction: no defs for operands: \");\n-      return NULL;\n+      /* If there is no dump file, print to stderr.  */\n+      if (!dump_file)\n+        vect_dump = stderr;\n+      return;\n     }\n \n-\n-  /* Check that one def is the reduction def, defined by PHI,\n-     the other def is either defined in the loop (\"vect_loop_def\"),\n-     or it's an induction (defined by a loop-header phi-node).  */\n-\n-  if (def2 == phi\n-      && flow_bb_inside_loop_p (loop, gimple_bb (def1))\n-      && (is_gimple_assign (def1)\n-\t  || STMT_VINFO_DEF_TYPE (vinfo_for_stmt (def1)) == vect_induction_def\n-\t  || (gimple_code (def1) == GIMPLE_PHI\n-\t      && STMT_VINFO_DEF_TYPE (vinfo_for_stmt (def1)) == vect_loop_def\n-\t      && !is_loop_header_bb_p (gimple_bb (def1)))))\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-\treport_vect_op (def_stmt, \"detected reduction:\");\n-      return def_stmt;\n-    }\n-  else if (def1 == phi\n-\t   && flow_bb_inside_loop_p (loop, gimple_bb (def2))\n-\t   && (is_gimple_assign (def2)\n-\t       || STMT_VINFO_DEF_TYPE (vinfo_for_stmt (def2)) == vect_induction_def\n-\t       || (gimple_code (def2) == GIMPLE_PHI\n-\t\t   && STMT_VINFO_DEF_TYPE (vinfo_for_stmt (def2)) == vect_loop_def\n-\t\t   && !is_loop_header_bb_p (gimple_bb (def2)))))\n-    {\n-      /* Swap operands (just for simplicity - so that the rest of the code\n-\t can assume that the reduction variable is always the last (second)\n-\t argument).  */\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-\treport_vect_op (def_stmt ,\n-\t\t        \"detected reduction: need to swap operands:\");\n-      swap_tree_operands (def_stmt, gimple_assign_rhs1_ptr (def_stmt),\n-\t\t\t  gimple_assign_rhs2_ptr (def_stmt));\n-      return def_stmt;\n-    }\n+  /* User didn't specify verbosity level:  */\n+  if (dump_file && (dump_flags & TDF_DETAILS))\n+    vect_verbosity_level = REPORT_DETAILS;\n+  else if (dump_file && (dump_flags & TDF_STATS))\n+    vect_verbosity_level = REPORT_UNVECTORIZED_LOOPS;\n   else\n-    {\n-      if (vect_print_dump_info (REPORT_DETAILS))\n-\treport_vect_op (def_stmt, \"reduction: unknown pattern.\");\n-      return NULL;\n-    }\n+    vect_verbosity_level = REPORT_NONE;\n+\n+  gcc_assert (dump_file || vect_verbosity_level == REPORT_NONE);\n }\n \n \n-/* Function vect_is_simple_iv_evolution.\n+/* Function debug_loop_details.\n \n-   FORNOW: A simple evolution of an induction variables in the loop is\n-   considered a polynomial evolution with constant step.  */\n+   For vectorization debug dumps.  */\n \n bool\n-vect_is_simple_iv_evolution (unsigned loop_nb, tree access_fn, tree * init, \n-\t\t\t     tree * step)\n+vect_print_dump_info (enum verbosity_levels vl)\n {\n-  tree init_expr;\n-  tree step_expr;\n-  tree evolution_part = evolution_part_in_loop_num (access_fn, loop_nb);\n-\n-  /* When there is no evolution in this loop, the evolution function\n-     is not \"simple\".  */  \n-  if (evolution_part == NULL_TREE)\n-    return false;\n-  \n-  /* When the evolution is a polynomial of degree >= 2\n-     the evolution function is not \"simple\".  */\n-  if (tree_is_chrec (evolution_part))\n+  if (vl > vect_verbosity_level)\n     return false;\n-  \n-  step_expr = evolution_part;\n-  init_expr = unshare_expr (initial_condition_in_loop_num (access_fn, loop_nb));\n-\n-  if (vect_print_dump_info (REPORT_DETAILS))\n-    {\n-      fprintf (vect_dump, \"step: \");\n-      print_generic_expr (vect_dump, step_expr, TDF_SLIM);\n-      fprintf (vect_dump, \",  init: \");\n-      print_generic_expr (vect_dump, init_expr, TDF_SLIM);\n-    }\n \n-  *init = init_expr;\n-  *step = step_expr;\n+  if (!current_function_decl || !vect_dump)\n+    return false;\n \n-  if (TREE_CODE (step_expr) != INTEGER_CST)\n-    { \n-      if (vect_print_dump_info (REPORT_DETAILS))\n-        fprintf (vect_dump, \"step unknown.\");\n-      return false;\n-    }\n+  if (vect_loop_location == UNKNOWN_LOC)\n+    fprintf (vect_dump, \"\\n%s:%d: note: \",\n+\t     DECL_SOURCE_FILE (current_function_decl),\n+\t     DECL_SOURCE_LINE (current_function_decl));\n+  else\n+    fprintf (vect_dump, \"\\n%s:%d: note: \", \n+\t     LOC_FILE (vect_loop_location), LOC_LINE (vect_loop_location));\n \n   return true;\n }\n@@ -2849,6 +242,7 @@ vectorize_loops (void)\n \n   return num_vectorized_loops > 0 ? TODO_cleanup_cfg : 0;\n }\n+ \n \n /* Increase alignment of global arrays to improve vectorization potential.\n    TODO:\n@@ -2871,49 +265,53 @@ increase_alignment (void)\n       unsigned int alignment;\n \n       if (TREE_CODE (TREE_TYPE (decl)) != ARRAY_TYPE)\n-\tcontinue;\n+        continue;\n       vectype = get_vectype_for_scalar_type (TREE_TYPE (TREE_TYPE (decl)));\n       if (!vectype)\n-\tcontinue;\n+        continue;\n       alignment = TYPE_ALIGN (vectype);\n       if (DECL_ALIGN (decl) >= alignment)\n-\tcontinue;\n+        continue;\n \n       if (vect_can_force_dr_alignment_p (decl, alignment))\n-\t{ \n-\t  DECL_ALIGN (decl) = TYPE_ALIGN (vectype);\n-\t  DECL_USER_ALIGN (decl) = 1;\n-\t  if (dump_file)\n-\t    { \n-\t      fprintf (dump_file, \"Increasing alignment of decl: \");\n-\t      print_generic_expr (dump_file, decl, TDF_SLIM);\n-\t    }\n-\t}\n+        {\n+          DECL_ALIGN (decl) = TYPE_ALIGN (vectype);\n+          DECL_USER_ALIGN (decl) = 1;\n+          if (dump_file)\n+            {\n+              fprintf (dump_file, \"Increasing alignment of decl: \");\n+              print_generic_expr (dump_file, decl, TDF_SLIM);\n+            }\n+        }\n     }\n   return 0;\n }\n \n+\n static bool\n gate_increase_alignment (void)\n {\n   return flag_section_anchors && flag_tree_vectorize;\n }\n \n-struct simple_ipa_opt_pass pass_ipa_increase_alignment = \n+\n+struct simple_ipa_opt_pass pass_ipa_increase_alignment =\n {\n  {\n   SIMPLE_IPA_PASS,\n-  \"increase_alignment\",\t\t\t/* name */\n-  gate_increase_alignment,\t\t/* gate */\n-  increase_alignment,\t\t\t/* execute */\n-  NULL,\t\t\t\t\t/* sub */\n-  NULL,\t\t\t\t\t/* next */\n-  0,\t\t\t\t\t/* static_pass_number */\n-  0,\t\t\t\t\t/* tv_id */\n-  0,\t\t\t\t\t/* properties_required */\n-  0,\t\t\t\t\t/* properties_provided */\n-  0,\t\t\t\t\t/* properties_destroyed */\n-  0,\t\t\t\t\t/* todo_flags_start */\n-  0 \t\t\t\t\t/* todo_flags_finish */\n+  \"increase_alignment\",                 /* name */\n+  gate_increase_alignment,              /* gate */\n+  increase_alignment,                   /* execute */\n+  NULL,                                 /* sub */\n+  NULL,                                 /* next */\n+  0,                                    /* static_pass_number */\n+  0,                                    /* tv_id */\n+  0,                                    /* properties_required */\n+  0,                                    /* properties_provided */\n+  0,                                    /* properties_destroyed */\n+  0,                                    /* todo_flags_start */\n+  0                                     /* todo_flags_finish */\n  }\n };\n+\n+"}, {"sha": "2645ebef4953b9314516edf6516d04c26f7eb384", "filename": "gcc/tree-vectorizer.h", "status": "modified", "additions": 102, "deletions": 85, "changes": 187, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vectorizer.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/ebfd146af752ce6aa298f866e36f9a7e66e32ec5/gcc%2Ftree-vectorizer.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vectorizer.h?ref=ebfd146af752ce6aa298f866e36f9a7e66e32ec5", "patch": "@@ -1,5 +1,6 @@\n-/* Loop Vectorization\n-   Copyright (C) 2003, 2004, 2005, 2006, 2007, 2008 Free Software Foundation, Inc.\n+/* Vectorizer\n+   Copyright (C) 2003, 2004, 2005, 2006, 2007, 2008, 2009 Free \n+   Software Foundation, Inc.\n    Contributed by Dorit Naishlos <dorit@il.ibm.com>\n \n This file is part of GCC.\n@@ -21,6 +22,8 @@ along with GCC; see the file COPYING3.  If not see\n #ifndef GCC_TREE_VECTORIZER_H\n #define GCC_TREE_VECTORIZER_H\n \n+#include \"tree-data-ref.h\"\n+\n typedef source_location LOC;\n #define UNKNOWN_LOC UNKNOWN_LOCATION\n #define EXPR_LOC(e) EXPR_LOCATION(e)\n@@ -687,119 +690,133 @@ known_alignment_for_access_p (struct data_reference *data_ref_info)\n \n /* vect_dump will be set to stderr or dump_file if exist.  */\n extern FILE *vect_dump;\n+extern LOC vect_loop_location;\n+\n extern enum verbosity_levels vect_verbosity_level;\n \n /* Bitmap of virtual variables to be renamed.  */\n extern bitmap vect_memsyms_to_rename;\n \n+\n /*-----------------------------------------------------------------*/\n /* Function prototypes.                                            */\n /*-----------------------------------------------------------------*/\n \n-/*************************************************************************\n-  Simple Loop Peeling Utilities - in tree-vectorizer.c\n- *************************************************************************/\n-/* Entry point for peeling of simple loops.\n-   Peel the first/last iterations of a loop.\n-   It can be used outside of the vectorizer for loops that are simple enough\n-   (see function documentation).  In the vectorizer it is used to peel the\n-   last few iterations when the loop bound is unknown or does not evenly\n-   divide by the vectorization factor, and to peel the first few iterations\n-   to force the alignment of data references in the loop.  */\n-extern struct loop *slpeel_tree_peel_loop_to_edge \n-  (struct loop *, edge, tree, tree, bool, unsigned int, bool);\n-extern void set_prologue_iterations (basic_block, tree,\n-\t\t\t\t     struct loop *, unsigned int);\n-struct loop *tree_duplicate_loop_on_edge (struct loop *, edge);\n+/* Simple loop peeling and versioning utilities for vectorizer's purposes - \n+   in tree-vect-loop-manip.c.  */\n extern void slpeel_make_loop_iterate_ntimes (struct loop *, tree);\n extern bool slpeel_can_duplicate_loop_p (const struct loop *, const_edge);\n-#ifdef ENABLE_CHECKING\n-extern void slpeel_verify_cfg_after_peeling (struct loop *, struct loop *);\n-#endif\n-\n+extern void vect_loop_versioning (loop_vec_info);\n+extern void vect_do_peeling_for_loop_bound (loop_vec_info, tree *);\n+extern void vect_do_peeling_for_alignment (loop_vec_info);\n+extern LOC find_loop_location (struct loop *);\n+extern bool vect_can_advance_ivs_p (loop_vec_info);\n \n-/*************************************************************************\n-  General Vectorization Utilities\n- *************************************************************************/\n-/** In tree-vectorizer.c **/\n+/* In tree-vect-stmts.c.  */\n extern tree get_vectype_for_scalar_type (tree);\n extern bool vect_is_simple_use (tree, loop_vec_info, gimple *, tree *,\n \t\t\t\tenum vect_def_type *);\n-extern bool vect_is_simple_iv_evolution (unsigned, tree, tree *, tree *);\n-extern gimple vect_is_simple_reduction (loop_vec_info, gimple);\n-extern bool vect_can_force_dr_alignment_p (const_tree, unsigned int);\n-extern enum dr_alignment_support vect_supportable_dr_alignment\n-  (struct data_reference *);\n-extern bool reduction_code_for_scalar_code (enum tree_code, enum tree_code *);\n extern bool supportable_widening_operation (enum tree_code, gimple, tree,\n-  tree *, tree *, enum tree_code *, enum tree_code *, \n-  int *, VEC (tree, heap) **);\n+                                            tree *, tree *, enum tree_code *, \n+                                            enum tree_code *, int *, \n+                                            VEC (tree, heap) **);\n extern bool supportable_narrowing_operation (enum tree_code, const_gimple,\n-\t     tree, enum tree_code *, int *, VEC (tree, heap) **);\n-\n-/* Creation and deletion of loop and stmt info structs.  */\n-extern loop_vec_info new_loop_vec_info (struct loop *loop);\n-extern void destroy_loop_vec_info (loop_vec_info, bool);\n+                                             tree, enum tree_code *, int *, \n+                                             VEC (tree, heap) **);\n extern stmt_vec_info new_stmt_vec_info (gimple stmt, loop_vec_info);\n extern void free_stmt_vec_info (gimple stmt);\n-\n-\n-/** In tree-vect-analyze.c  **/\n-/* Driver for analysis stage.  */\n+extern tree vectorizable_function (gimple, tree, tree);\n+extern void vect_model_simple_cost (stmt_vec_info, int, enum vect_def_type *,\n+                                    slp_tree);\n+extern void vect_model_store_cost (stmt_vec_info, int, enum vect_def_type,\n+                                   slp_tree);\n+extern void vect_model_load_cost (stmt_vec_info, int, slp_tree);\n+extern void vect_finish_stmt_generation (gimple, gimple,\n+                                         gimple_stmt_iterator *);\n+extern bool vect_mark_stmts_to_be_vectorized (loop_vec_info);\n+extern int cost_for_stmt (gimple);\n+extern tree vect_get_vec_def_for_operand (tree, gimple, tree *);\n+extern tree vect_init_vector (gimple, tree, tree,\n+                              gimple_stmt_iterator *);\n+extern tree vect_get_vec_def_for_stmt_copy (enum vect_def_type, tree);\n+extern bool vect_transform_stmt (gimple, gimple_stmt_iterator *,\n+                                 bool *, slp_tree, slp_instance);\n+extern void vect_remove_stores (gimple);\n+extern bool vect_analyze_operations (loop_vec_info);\n+\n+/* In tree-vect-data-refs.c.  */\n+extern bool vect_can_force_dr_alignment_p (const_tree, unsigned int);\n+extern enum dr_alignment_support vect_supportable_dr_alignment\n+                                           (struct data_reference *);\n+extern tree vect_get_smallest_scalar_type (gimple, HOST_WIDE_INT *,\n+                                           HOST_WIDE_INT *);\n+extern bool vect_analyze_data_ref_dependences (loop_vec_info);\n+extern bool vect_enhance_data_refs_alignment (loop_vec_info);\n+extern bool vect_analyze_data_refs_alignment (loop_vec_info);\n+extern bool vect_analyze_data_ref_accesses (loop_vec_info);\n+extern bool vect_prune_runtime_alias_test_list (loop_vec_info);\n+extern bool vect_analyze_data_refs (loop_vec_info);\n+extern tree vect_create_data_ref_ptr (gimple, struct loop *, tree, tree *,\n+                                      gimple *, bool, bool *, tree);\n+extern tree bump_vector_ptr (tree, gimple, gimple_stmt_iterator *, gimple, tree);\n+extern tree vect_create_destination_var (tree, tree);\n+extern bool vect_strided_store_supported (tree);\n+extern bool vect_strided_load_supported (tree);\n+extern bool vect_permute_store_chain (VEC(tree,heap) *,unsigned int, gimple,\n+                                    gimple_stmt_iterator *, VEC(tree,heap) **);\n+extern tree vect_setup_realignment (gimple, gimple_stmt_iterator *, tree *,\n+                                    enum dr_alignment_support, tree, \n+                                    struct loop **);\n+extern bool vect_permute_load_chain (VEC(tree,heap) *,unsigned int, gimple,\n+                                    gimple_stmt_iterator *, VEC(tree,heap) **);\n+extern bool vect_transform_strided_load (gimple, VEC(tree,heap) *, int,\n+                                         gimple_stmt_iterator *);\n+extern int vect_get_place_in_interleaving_chain (gimple, gimple);\n+extern tree vect_get_new_vect_var (tree, enum vect_var_kind, const char *);\n+extern tree vect_create_addr_base_for_vector_ref (gimple, gimple_seq *,\n+                                                  tree, struct loop *);\n+\n+/* In tree-vect-loop.c.  */\n+/* FORNOW: Used in tree-parloops.c.  */\n+extern void destroy_loop_vec_info (loop_vec_info, bool);\n+extern gimple vect_is_simple_reduction (loop_vec_info, gimple);\n+/* Drive for loop analysis stage.  */\n extern loop_vec_info vect_analyze_loop (struct loop *);\n-extern void vect_free_slp_instance (slp_instance);\n+/* Drive for loop transformation stage.  */\n+extern void vect_transform_loop (loop_vec_info);\n extern loop_vec_info vect_analyze_loop_form (struct loop *);\n-extern tree vect_get_smallest_scalar_type (gimple, HOST_WIDE_INT *, \n-                                           HOST_WIDE_INT *);\n+extern bool vectorizable_live_operation (gimple, gimple_stmt_iterator *,\n+                                         gimple *);\n+extern bool vectorizable_reduction (gimple, gimple_stmt_iterator *, gimple *);\n+extern bool vectorizable_induction (gimple, gimple_stmt_iterator *, gimple *);\n+extern int vect_estimate_min_profitable_iters (loop_vec_info);\n+extern tree get_initial_def_for_reduction (gimple, tree, tree *);\n+extern int vect_min_worthwhile_factor (enum tree_code);\n+\n \n-/** In tree-vect-patterns.c  **/\n+/* In tree-vect-slp.c.  */\n+extern void vect_free_slp_instance (slp_instance);\n+extern bool vect_transform_slp_perm_load (gimple, VEC (tree, heap) *,\n+                                          gimple_stmt_iterator *, int, \n+                                          slp_instance, bool);\n+extern bool vect_schedule_slp (loop_vec_info);\n+extern void vect_update_slp_costs_according_to_vf (loop_vec_info);\n+extern bool vect_analyze_slp (loop_vec_info);\n+extern void vect_make_slp_decision (loop_vec_info);\n+extern void vect_detect_hybrid_slp (loop_vec_info);\n+extern void vect_get_slp_defs (slp_tree, VEC (tree,heap) **,\n+                               VEC (tree,heap) **);\n+\n+/* In tree-vect-patterns.c.  */\n /* Pattern recognition functions.\n    Additional pattern recognition functions can (and will) be added\n    in the future.  */\n typedef gimple (* vect_recog_func_ptr) (gimple, tree *, tree *);\n #define NUM_PATTERNS 4\n void vect_pattern_recog (loop_vec_info);\n \n-\n-/** In tree-vect-transform.c  **/\n-extern bool vectorizable_load (gimple, gimple_stmt_iterator *, gimple *,\n-\t\t\t       slp_tree, slp_instance);\n-extern bool vectorizable_store (gimple, gimple_stmt_iterator *, gimple *,\n-\t\t\t\tslp_tree);\n-extern bool vectorizable_operation (gimple, gimple_stmt_iterator *, gimple *,\n-\t\t\t\t    slp_tree);\n-extern bool vectorizable_type_promotion (gimple, gimple_stmt_iterator *,\n-\t\t\t\t\t gimple *, slp_tree);\n-extern bool vectorizable_type_demotion (gimple, gimple_stmt_iterator *,\n-\t\t\t\t\tgimple *, slp_tree);\n-extern bool vectorizable_conversion (gimple, gimple_stmt_iterator *, gimple *,\n-\t\t\t\t     slp_tree);\n-extern bool vectorizable_assignment (gimple, gimple_stmt_iterator *, gimple *,\n-\t\t\t\t     slp_tree);\n-extern tree vectorizable_function (gimple, tree, tree);\n-extern bool vectorizable_call (gimple, gimple_stmt_iterator *, gimple *);\n-extern bool vectorizable_condition (gimple, gimple_stmt_iterator *, gimple *);\n-extern bool vectorizable_live_operation (gimple, gimple_stmt_iterator *,\n-\t\t\t\t\t gimple *);\n-extern bool vectorizable_reduction (gimple, gimple_stmt_iterator *, gimple *);\n-extern bool vectorizable_induction (gimple, gimple_stmt_iterator *, gimple *);\n-extern int  vect_estimate_min_profitable_iters (loop_vec_info);\n-extern void vect_model_simple_cost (stmt_vec_info, int, enum vect_def_type *, \n-\t\t\t\t    slp_tree);\n-extern void vect_model_store_cost (stmt_vec_info, int, enum vect_def_type, \n-\t\t\t\t   slp_tree);\n-extern void vect_model_load_cost (stmt_vec_info, int, slp_tree);\n-extern bool vect_transform_slp_perm_load (gimple, VEC (tree, heap) *, \n-                             gimple_stmt_iterator *, int, slp_instance, bool);\n-\n-/* Driver for transformation stage.  */\n-extern void vect_transform_loop (loop_vec_info);\n-\n-/*************************************************************************\n-  Vectorization Debug Information - in tree-vectorizer.c\n- *************************************************************************/\n+/*  Vectorization debug information - in tree-vectorizer.c.  */\n extern bool vect_print_dump_info (enum verbosity_levels);\n extern void vect_set_verbosity_level (const char *);\n-extern LOC find_loop_location (struct loop *);\n \n #endif  /* GCC_TREE_VECTORIZER_H  */"}]}