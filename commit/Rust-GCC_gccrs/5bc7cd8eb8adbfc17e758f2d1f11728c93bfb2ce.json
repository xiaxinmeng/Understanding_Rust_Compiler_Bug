{"sha": "5bc7cd8eb8adbfc17e758f2d1f11728c93bfb2ce", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NWJjN2NkOGViOGFkYmZjMTdlNzU4ZjJkMWYxMTcyOGM5M2JmYjJjZQ==", "commit": {"author": {"name": "Stan Cox", "email": "coxs@gnu.org", "date": "1997-01-09T18:00:12Z"}, "committer": {"name": "Stan Cox", "email": "coxs@gnu.org", "date": "1997-01-09T18:00:12Z"}, "message": "Use SImode instead of HImode if aligned.\n\nFrom-SVN: r13479", "tree": {"sha": "0e098d65142ab923aeae917512e35035d67eee9f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/0e098d65142ab923aeae917512e35035d67eee9f"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/5bc7cd8eb8adbfc17e758f2d1f11728c93bfb2ce", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5bc7cd8eb8adbfc17e758f2d1f11728c93bfb2ce", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5bc7cd8eb8adbfc17e758f2d1f11728c93bfb2ce", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5bc7cd8eb8adbfc17e758f2d1f11728c93bfb2ce/comments", "author": null, "committer": null, "parents": [{"sha": "72acf258c6df6782d4f49b7f638a116ce70ad0ff", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/72acf258c6df6782d4f49b7f638a116ce70ad0ff", "html_url": "https://github.com/Rust-GCC/gccrs/commit/72acf258c6df6782d4f49b7f638a116ce70ad0ff"}], "stats": {"total": 569, "additions": 485, "deletions": 84}, "files": [{"sha": "a4591f0c21b6c1e574d288f920230cc64d145d37", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 80, "deletions": 1, "changes": 81, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5bc7cd8eb8adbfc17e758f2d1f11728c93bfb2ce/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5bc7cd8eb8adbfc17e758f2d1f11728c93bfb2ce/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=5bc7cd8eb8adbfc17e758f2d1f11728c93bfb2ce", "patch": "@@ -388,6 +388,85 @@ optimization_options (level)\n #endif\n }\n \f\n+/* Sign-extend a 16-bit constant */\n+\n+struct rtx_def *\n+i386_sext16_if_const (op)\n+     struct rtx_def *op;\n+{\n+  if (GET_CODE (op) == CONST_INT)\n+    {\n+      HOST_WIDE_INT val = INTVAL (op);\n+      HOST_WIDE_INT sext_val;\n+      if (val & 0x8000)\n+\tsext_val = val | ~0xffff;\n+      else\n+\tsext_val = val & 0xffff;\n+      if (sext_val != val)\n+\top = GEN_INT (sext_val);\n+    }\n+  return op;\n+}\n+\f\n+/* Return nonzero if the rtx is aligned */\n+\n+static int\n+i386_aligned_reg_p (regno)\n+     int regno;\n+{\n+  return (regno == STACK_POINTER_REGNUM\n+\t  || (!flag_omit_frame_pointer\n+\t      && regno == FRAME_POINTER_REGNUM));\n+}\n+\n+int\n+i386_aligned_p (op)\n+     rtx op;\n+{\n+  /* registers and immediate operands are always \"aligned\" */\n+  if (GET_CODE (op) != MEM)\n+    return 1;\n+\n+  /* Don't even try to do any aligned optimizations with volatiles */\n+  if (MEM_VOLATILE_P (op))\n+    return 0;\n+\n+  /* Get address of memory operand */\n+  op = XEXP (op, 0);\n+\n+  switch (GET_CODE (op))\n+    {\n+    case CONST_INT:\n+\tif (INTVAL (op) & 3)\n+\t  break;\n+\treturn 1;\n+\n+    /* match \"reg + offset\" */\n+    case PLUS:\n+\tif (GET_CODE (XEXP (op, 1)) != CONST_INT)\n+\t  break;\n+\tif (INTVAL (XEXP (op, 1)) & 3)\n+\t  break;\n+\top = XEXP (op, 0);\n+\tif (GET_CODE (op) != REG)\n+\t  break;\n+\t/* fall through */\n+    case REG:\n+\treturn i386_aligned_reg_p (REGNO (op));\n+    }\n+  return 0;\n+}\n+\f\n+/* Return nonzero if INSN looks like it won't compute useful cc bits\n+   as a side effect.  This information is only a hint. */\n+\n+int\n+i386_cc_probably_useless_p (insn)\n+     rtx insn;\n+{\n+  return !next_cc0_user (insn);\n+}\n+\f\n /* Return nonzero if IDENTIFIER with arguments ARGS is a valid machine specific\n    attribute for DECL.  The attributes in ATTRIBUTES have previously been\n    assigned to DECL.  */\n@@ -854,7 +933,7 @@ asm_add (n, x)\n     output_asm_insn (AS1 (dec%L0,%0), xops);\n   else if (n == 1)\n     output_asm_insn (AS1 (inc%L0,%0), xops);\n-  else if (n < 0)\n+  else if (n < 0 || n == 128)\n     {\n       xops[1] = GEN_INT (-n);\n       output_asm_insn (AS2 (sub%L0,%1,%0), xops);"}, {"sha": "20ec844ca12df109c43ff0b8e4734ab1ae99cf83", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5bc7cd8eb8adbfc17e758f2d1f11728c93bfb2ce/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5bc7cd8eb8adbfc17e758f2d1f11728c93bfb2ce/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=5bc7cd8eb8adbfc17e758f2d1f11728c93bfb2ce", "patch": "@@ -157,7 +157,8 @@ extern int target_flags;\n #define TARGET_PENTIUMPRO (ix86_cpu == PROCESSOR_PENTIUMPRO)\n #define TARGET_USE_LEAVE (ix86_cpu == PROCESSOR_I386)\n #define TARGET_PUSH_MEMORY (ix86_cpu == PROCESSOR_I386)\n-#define TARGET_ZERO_EXTEND_WITH_AND (ix86_cpu != PROCESSOR_I386)\n+#define TARGET_ZERO_EXTEND_WITH_AND (ix86_cpu != PROCESSOR_I386 \\\n+\t\t\t\t     && ix86_cpu != PROCESSOR_PENTIUMPRO)\n #define TARGET_DOUBLE_WITH_ADD (ix86_cpu != PROCESSOR_I386)\n #define TARGET_USE_BIT_TEST (ix86_cpu == PROCESSOR_I386)\n #define TARGET_UNROLL_STRLEN (ix86_cpu != PROCESSOR_I386)\n@@ -2604,6 +2605,9 @@ do {\t\t\t\t\t\t\t\t\t\\\n extern void override_options ();\n extern void order_regs_for_local_alloc ();\n extern char *output_strlen_unroll ();\n+extern struct rtx_def *i386_sext16_if_const ();\n+extern int i386_aligned_p ();\n+extern int i386_cc_probably_useless_p ();\n extern int i386_valid_decl_attribute_p ();\n extern int i386_valid_type_attribute_p ();\n extern int i386_return_pops_args ();"}, {"sha": "7d8fb2b17c3ead6b1fca63eb80f5c895693b2610", "filename": "gcc/config/i386/i386.md", "status": "modified", "additions": 400, "deletions": 82, "changes": 482, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5bc7cd8eb8adbfc17e758f2d1f11728c93bfb2ce/gcc%2Fconfig%2Fi386%2Fi386.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5bc7cd8eb8adbfc17e758f2d1f11728c93bfb2ce/gcc%2Fconfig%2Fi386%2Fi386.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.md?ref=5bc7cd8eb8adbfc17e758f2d1f11728c93bfb2ce", "patch": "@@ -770,6 +770,12 @@\n \t}\n     }\n \n+  /* use 32-bit test instruction if there are no sign issues */\n+  if (GET_CODE (operands[1]) == CONST_INT\n+      && !(INTVAL (operands[1]) & ~0x7fff)\n+      && i386_aligned_p (operands[0]))\n+    return AS2 (test%L0,%1,%k0);\n+\n   if (CONSTANT_P (operands[1]) || GET_CODE (operands[0]) == MEM)\n     return AS2 (test%W0,%1,%0);\n \n@@ -931,10 +937,21 @@\n \n   if (REG_P (operands[0]))\n     {\n-      if (REG_P (operands[1]))\n-\treturn AS2 (mov%L0,%k1,%k0);\n-      else if (CONSTANT_P (operands[1]))\n-\treturn AS2 (mov%L0,%1,%k0);\n+      if (i386_aligned_p (operands[1]))\n+\t{\n+\t  operands[1] = i386_sext16_if_const (operands[1]);\n+\t  return AS2 (mov%L0,%k1,%k0);\n+\t}\n+      if (TARGET_PENTIUMPRO)\n+\t{\n+\t  /* movzwl is faster than movw on the Pentium Pro,\n+\t   * although not as fast as an aligned movl. */\n+#ifdef INTEL_SYNTAX\n+\t  return AS2 (movzx,%1,%k0);\n+#else\n+\t  return AS2 (movz%W0%L0,%1,%k0);\n+#endif\n+\t}\n     }\n \n   return AS2 (mov%W0,%1,%0);\n@@ -1040,7 +1057,7 @@\n {\n   rtx link;\n   if (operands[1] == const0_rtx && REG_P (operands[0]))\n-    return AS2 (xor%B0,%0,%0);\n+    return AS2 (xor%L0,%k0,%k0);\n \n   if (operands[1] == const1_rtx\n       && (link = find_reg_note (insn, REG_WAS_0, 0))\n@@ -1747,7 +1764,10 @@\n     {\n       xops[0] = operands[0];\n       xops[1] = gen_rtx (CONST_INT, VOIDmode, 0xffff);\n-      output_asm_insn (AS2 (mov%W0,%1,%w0),operands);\n+      if (i386_aligned_p (operands[1]))\n+\toutput_asm_insn (AS2 (mov%L0,%k1,%k0),operands);\n+      else\n+\toutput_asm_insn (AS2 (mov%W0,%1,%w0),operands);\n       output_asm_insn (AS2 (and%L0,%1,%k0), xops);\n       RET;\n     }\n@@ -2752,7 +2772,7 @@\n }\")\n \n (define_insn \"addsidi3_2\"\n-  [(set (match_operand:DI 0 \"nonimmediate_operand\" \"=&r,r,o,&r,!&r,r,o,o,!o\")\n+  [(set (match_operand:DI 0 \"nonimmediate_operand\" \"=&r,r,o,&r,!&r,&r,o,o,!o\")\n \t(plus:DI (zero_extend:DI (match_operand:SI 2 \"general_operand\" \"o,ri,ri,o,o,ri,ri,i,r\"))\n \t\t (match_operand:DI 1 \"general_operand\" \"0,0,0,iF,ro,roiF,riF,o,o\")))\n    (clobber (match_scratch:SI 3 \"=X,X,X,X,X,X,X,&r,&r\"))]\n@@ -2952,6 +2972,18 @@\n   if (operands[2] == constm1_rtx)\n     return AS1 (dec%L0,%0);\n \n+  /* subl $-128,%ebx is smaller than addl $128,%ebx. */\n+  if (GET_CODE (operands[2]) == CONST_INT\n+      && INTVAL (operands[2]) == 128)\n+    {\n+      /* This doesn't compute the carry bit in the same way\n+       * as add%L0, but we use inc and dec above and they\n+       * don't set the carry bit at all.  If inc/dec don't need\n+       * a CC_STATUS_INIT, this doesn't either... */\n+      operands[2] = GEN_INT (-128);\n+      return AS2 (sub%L0,%2,%0);\n+    }\n+\n   return AS2 (add%L0,%2,%0);\n }\")\n \n@@ -3003,9 +3035,11 @@\n   \"*\n {\n   /* ??? what about offsettable memory references? */\n-  if (QI_REG_P (operands[0])\n+  if (!TARGET_PENTIUMPRO /* partial stalls are just too painful to risk. */\n+      && QI_REG_P (operands[0])\n       && GET_CODE (operands[2]) == CONST_INT\n-      && (INTVAL (operands[2]) & 0xff) == 0)\n+      && (INTVAL (operands[2]) & 0xff) == 0\n+      && i386_cc_probably_useless_p (insn))\n     {\n       int byteval = (INTVAL (operands[2]) >> 8) & 0xff;\n       CC_STATUS_INIT;\n@@ -3019,6 +3053,28 @@\n       return AS2 (add%B0,%2,%h0);\n     }\n \n+  /* Use a 32-bit operation when possible, to avoid the prefix penalty. */\n+  if (REG_P (operands[0])\n+      && i386_aligned_p (operands[2])\n+      && i386_cc_probably_useless_p (insn))\n+    {\n+      CC_STATUS_INIT;\n+\n+      if (GET_CODE (operands[2]) == CONST_INT)\n+\t{\n+\t  HOST_WIDE_INT intval = 0xffff & INTVAL (operands[2]);\n+\n+\t  if (intval == 1)\n+\t    return AS1 (inc%L0,%k0);\n+\n+\t  if (intval == 0xffff)\n+\t    return AS1 (dec%L0,%k0);\n+\n+\t  operands[2] = i386_sext16_if_const (operands[2]);\n+\t}\n+      return AS2 (add%L0,%k2,%k0);\n+    }\n+\n   if (operands[2] == const1_rtx)\n     return AS1 (inc%W0,%0);\n \n@@ -3246,7 +3302,18 @@\n \t(minus:HI (match_operand:HI 1 \"nonimmediate_operand\" \"0,0\")\n \t\t  (match_operand:HI 2 \"general_operand\" \"ri,rm\")))]\n   \"ix86_binary_operator_ok (MINUS, HImode, operands)\"\n-  \"* return AS2 (sub%W0,%2,%0);\")\n+  \"*\n+{\n+  if (REG_P (operands[0])\n+      && i386_aligned_p (operands[2])\n+      && i386_cc_probably_useless_p (insn))\n+    {\n+      CC_STATUS_INIT;\n+      operands[2] = i386_sext16_if_const (operands[2]);\n+      return AS2 (sub%L0,%k2,%k0);\n+    }\n+ return AS2 (sub%W0,%2,%0);\n+}\")\n \n (define_expand \"subqi3\"\n   [(set (match_operand:QI 0 \"general_operand\" \"\")\n@@ -3518,14 +3585,14 @@\n ;; The `r' in `rm' for operand 3 looks redundant, but it causes\n ;; optional reloads to be generated if op 3 is a pseudo in a stack slot.\n \n-;; ??? What if we only change one byte of an offsettable memory reference?\n (define_insn \"andsi3\"\n   [(set (match_operand:SI 0 \"nonimmediate_operand\" \"=rm,r\")\n \t(and:SI (match_operand:SI 1 \"nonimmediate_operand\" \"%0,0\")\n \t\t(match_operand:SI 2 \"general_operand\" \"ri,rm\")))]\n   \"\"\n   \"*\n {\n+  HOST_WIDE_INT intval;\n   if (!rtx_equal_p (operands[0], operands[1])\n       && rtx_equal_p (operands[0], operands[2]))\n     {\n@@ -3534,10 +3601,14 @@\n       operands[1] = operands[2];\n       operands[2] = tmp;\n     }\n-  if (GET_CODE (operands[2]) == CONST_INT\n-      && ! (GET_CODE (operands[0]) == MEM && MEM_VOLATILE_P (operands[0])))\n+  switch (GET_CODE (operands[2]))\n     {\n-      if (INTVAL (operands[2]) == 0xffff && REG_P (operands[0])\n+    case CONST_INT:\n+      if (GET_CODE (operands[0]) == MEM && MEM_VOLATILE_P (operands[0]))\n+\tbreak;\n+      intval = INTVAL (operands[2]);\n+      /* zero-extend 16->32? */\n+      if (intval == 0xffff && REG_P (operands[0])\n \t  && (! REG_P (operands[1])\n \t      || REGNO (operands[0]) != 0 || REGNO (operands[1]) != 0)\n \t  && (!TARGET_ZERO_EXTEND_WITH_AND || ! rtx_equal_p (operands[0], operands[1])))\n@@ -3552,7 +3623,8 @@\n #endif\n \t}\n \n-      if (INTVAL (operands[2]) == 0xff && REG_P (operands[0])\n+      /* zero extend 8->32? */\n+      if (intval == 0xff && REG_P (operands[0])\n \t  && !(REG_P (operands[1]) && NON_QI_REG_P (operands[1]))\n \t  && (! REG_P (operands[1])\n \t      || REGNO (operands[0]) != 0 || REGNO (operands[1]) != 0)\n@@ -3568,39 +3640,99 @@\n #endif\n \t}\n \n-      if (QI_REG_P (operands[0]) && ~(INTVAL (operands[2]) | 0xff) == 0)\n-\t{\n-\t  CC_STATUS_INIT;\n+      /* Check partial bytes.. non-QI-regs are not available */\n+      if (REG_P (operands[0]) && ! QI_REG_P (operands[0]))\n+\tbreak;\n \n-\t  if (INTVAL (operands[2]) == 0xffffff00)\n+      /* only low byte has zero bits? */\n+      if (~(intval | 0xff) == 0)\n+\t{\n+\t  intval &= 0xff;\n+\t  if (REG_P (operands[0]))\n \t    {\n-\t      operands[2] = const0_rtx;\n-\t      return AS2 (mov%B0,%2,%b0);\n+\t      if (intval == 0)\n+\t\t{\n+\t\t  CC_STATUS_INIT;\n+\t\t  return AS2 (xor%B0,%b0,%b0);\n+\t\t}\n+\n+\t      /* we're better off with the 32-bit version if reg != EAX */\n+\t      /* the value is sign-extended in 8 bits */\n+\t      if (REGNO (operands[0]) != 0 && (intval & 0x80))\n+\t\tbreak;\n \t    }\n \n-\t  operands[2] = GEN_INT (INTVAL (operands[2]) & 0xff);\n+\t  CC_STATUS_INIT;\n+\n+\t  operands[2] = GEN_INT (intval);\n+\n+\t  if (intval == 0)\n+\t    return AS2 (mov%B0,%2,%b0);\n+\n \t  return AS2 (and%B0,%2,%b0);\n \t}\n \n-      if (QI_REG_P (operands[0]) && ~(INTVAL (operands[2]) | 0xff00) == 0)\n+      /* only second byte has zero? */\n+      if (~(intval | 0xff00) == 0)\n \t{\n \t  CC_STATUS_INIT;\n \n-\t  if (INTVAL (operands[2]) == 0xffff00ff)\n+\t  intval = (intval >> 8) & 0xff;\n+\t  operands[2] = GEN_INT (intval);\n+\t  if (intval == 0)\n \t    {\n-\t      operands[2] = const0_rtx;\n-\t      return AS2 (mov%B0,%2,%h0);\n+\t      if (REG_P (operands[0]))\n+\t\treturn AS2 (xor%B0,%h0,%h0);\n+\t      operands[0] = adj_offsettable_operand (operands[0], 1);\n+\t      return AS2 (mov%B0,%2,%b0);\n \t    }\n \n-\t  operands[2] = GEN_INT ((INTVAL (operands[2]) >> 8) & 0xff);\n-\t  return AS2 (and%B0,%2,%h0);\n+\t  if (REG_P (operands[0]))\n+\t    return AS2 (and%B0,%2,%h0);\n+\n+\t  operands[0] = adj_offsettable_operand (operands[0], 1);\n+\t  return AS2 (and%B0,%2,%b0);\n \t}\n \n-      if (GET_CODE (operands[0]) == MEM && INTVAL (operands[2]) == 0xffff0000)\n+      if (REG_P (operands[0]))\n+\tbreak;\n+\n+      /* third byte has zero bits? */\n+      if (~(intval | 0xff0000) == 0)\n+\t{\n+\t  intval = (intval >> 16) & 0xff;\n+\t  operands[0] = adj_offsettable_operand (operands[0], 2);\n+byte_and_operation:\n+\t  CC_STATUS_INIT;\n+\t  operands[2] = GEN_INT (intval);\n+\t  if (intval == 0)\n+\t    return AS2 (mov%B0,%2,%b0);\n+\t  return AS2 (and%B0,%2,%b0);\n+\t}\n+\n+      /* fourth byte has zero bits? */\n+      if (~(intval | 0xff000000) == 0)\n+\t{\n+\t  intval = (intval >> 24) & 0xff;\n+\t  operands[0] = adj_offsettable_operand (operands[0], 3);\n+\t  goto byte_and_operation;\n+\t}\n+\n+      /* Low word is zero? */\n+      if (intval == 0xffff0000)\n         {\n+word_zero_and_operation:\n+\t  CC_STATUS_INIT;\n \t  operands[2] = const0_rtx;\n \t  return AS2 (mov%W0,%2,%w0);\n \t}\n+\n+      /* High word is zero? */\n+      if (intval == 0x0000ffff)\n+        {\n+\t  operands[0] = adj_offsettable_operand (operands[0], 2);\n+\t  goto word_zero_and_operation;\n+\t}\n     }\n \n   return AS2 (and%L0,%2,%0);\n@@ -3647,6 +3779,38 @@\n \t  operands[2] = GEN_INT ((INTVAL (operands[2]) >> 8) & 0xff);\n \t  return AS2 (and%B0,%2,%h0);\n \t}\n+\n+      /* use 32-bit ops on registers when there are no sign issues.. */\n+      if (REG_P (operands[0]))\n+\t{\n+\t  if (!(INTVAL (operands[2]) & ~0x7fff))\n+\t    return AS2 (and%L0,%2,%k0);\n+\t}\n+    }\n+\n+  if (REG_P (operands[0])\n+      && i386_aligned_p (operands[2]))\n+    {\n+      CC_STATUS_INIT;\n+      /* If op[2] is constant, we should zero-extend it and */\n+      /* make a note that op[0] has been zero-extended, so  */\n+      /* that we could use 32-bit ops on it forthwith, but  */\n+      /* there is no such reg-note available. Instead we do */\n+      /* a sign extension as that can result in shorter asm */\n+      operands[2] = i386_sext16_if_const (operands[2]);\n+      return AS2 (and%L0,%k2,%k0);\n+    }\n+\n+  /* Use a 32-bit word with the upper bits set, invalidate CC */\n+  if (GET_CODE (operands[2]) == CONST_INT\n+      && i386_aligned_p (operands[0]))\n+    {\n+      HOST_WIDE_INT val = INTVAL (operands[2]);\n+      CC_STATUS_INIT;\n+      val |= ~0xffff;\n+      if (val != INTVAL (operands[2]))\n+\toperands[2] = GEN_INT (val);\n+      return AS2 (and%L0,%k2,%k0);\n     }\n \n   return AS2 (and%W0,%2,%0);\n@@ -3685,37 +3849,87 @@\n \f\n ;;- Bit set (inclusive or) instructions\n \n-;; ??? What if we only change one byte of an offsettable memory reference?\n+;; This optimizes known byte-wide operations to memory, and in some cases\n+;; to QI registers.. Note that we don't want to use the QI registers too\n+;; aggressively, because often the 32-bit register instruction is the same\n+;; size, and likely to be faster on PentiumPro.\n (define_insn \"iorsi3\"\n   [(set (match_operand:SI 0 \"nonimmediate_operand\" \"=rm,r\")\n \t(ior:SI (match_operand:SI 1 \"nonimmediate_operand\" \"%0,0\")\n \t\t(match_operand:SI 2 \"general_operand\" \"ri,rm\")))]\n   \"\"\n   \"*\n {\n-  if (GET_CODE (operands[2]) == CONST_INT\n-      && ! (GET_CODE (operands[0]) == MEM && MEM_VOLATILE_P (operands[0])))\n+  HOST_WIDE_INT intval;\n+  switch (GET_CODE (operands[2]))\n     {\n-      if ((! REG_P (operands[0]) || QI_REG_P (operands[0]))\n-\t  && (INTVAL (operands[2]) & ~0xff) == 0)\n-\t{\n-\t  CC_STATUS_INIT;\n+    case CONST_INT:\n \n-\t  if (INTVAL (operands[2]) == 0xff)\n-\t    return AS2 (mov%B0,%2,%b0);\n+      if (REG_P (operands[0]) && ! QI_REG_P (operands[0]))\n+\tbreak;\n+\n+      /* don't try to optimize volatile accesses */\n+      if (GET_CODE (operands[0]) == MEM && MEM_VOLATILE_P (operands[0]))\n+\tbreak;\n+\n+      intval = INTVAL (operands[2]);\n+      if ((intval & ~0xff) == 0)\n+        {\n+\t  if (REG_P (operands[0]))\n+\t    {\n+\t      /* Do low byte access only for %eax or when high bit is set */\n+\t      if (REGNO (operands[0]) != 0 && !(intval & 0x80))\n+\t        break;\n+\t    }\n \n-\t  return AS2 (or%B0,%2,%b0);\n+byte_or_operation:\n+\t    CC_STATUS_INIT;\n+\n+\t    if (intval != INTVAL (operands[2]))\n+\t      operands[2] = GEN_INT (intval);\n+\n+\t    if (intval == 0xff)\n+\t      return AS2 (mov%B0,%2,%b0);\n+\n+\t    return AS2 (or%B0,%2,%b0);\n \t}\n \n-      if (QI_REG_P (operands[0]) && (INTVAL (operands[2]) & ~0xff00) == 0)\n+      /* second byte? */\n+      if ((intval & ~0xff00) == 0)\n \t{\n-\t  CC_STATUS_INIT;\n-\t  operands[2] = GEN_INT (INTVAL (operands[2]) >> 8);\n+\t  intval >>= 8;\n \n-\t  if (INTVAL (operands[2]) == 0xff)\n-\t    return AS2 (mov%B0,%2,%h0);\n+\t  if (REG_P (operands[0]))\n+\t    {\n+\t      CC_STATUS_INIT;\n+\t      operands[2] = GEN_INT (intval);\n+\t      if (intval == 0xff)\n+\t\treturn AS2 (mov%B0,%2,%h0);\n \n-\t  return AS2 (or%B0,%2,%h0);\n+\t      return AS2 (or%B0,%2,%h0);\n+\t    }\n+\n+\t  operands[0] = adj_offsettable_operand (operands[0], 1);\n+\t  goto byte_or_operation;\n+\t}\n+\n+      if (REG_P (operands[0]))\n+\tbreak;\n+\n+      /* third byte? */\n+      if ((intval & ~0xff0000) == 0)\n+\t{\n+\t  intval >>= 16;\n+\t  operands[0] = adj_offsettable_operand (operands[0], 2);\n+\t  goto byte_or_operation;\n+\t}\n+\n+      /* fourth byte? */\n+      if ((intval & ~0xff000000) == 0)\n+\t{\n+\t  intval = (intval >> 24) & 0xff;\n+\t  operands[0] = adj_offsettable_operand (operands[0], 3);\n+\t  goto byte_or_operation;\n \t}\n     }\n \n@@ -3729,38 +3943,77 @@\n   \"\"\n   \"*\n {\n-  if (GET_CODE (operands[2]) == CONST_INT\n-      && ! (GET_CODE (operands[0]) == MEM && MEM_VOLATILE_P (operands[0])))\n+  HOST_WIDE_INT intval;\n+  switch (GET_CODE (operands[2]))\n     {\n-      /* Can we ignore the upper byte? */\n-      if ((! REG_P (operands[0]) || QI_REG_P (operands[0]))\n-\t  && (INTVAL (operands[2]) & 0xff00) == 0)\n-\t{\n-\t  CC_STATUS_INIT;\n-\t  if (INTVAL (operands[2]) & 0xffff0000)\n-\t    operands[2] = GEN_INT (INTVAL (operands[2]) & 0xffff);\n+    case CONST_INT:\n \n-\t  if (INTVAL (operands[2]) == 0xff)\n-\t    return AS2 (mov%B0,%2,%b0);\n+      if (REG_P (operands[0]) && ! QI_REG_P (operands[0]))\n+\tbreak;\n+\n+      /* don't try to optimize volatile accesses */\n+      if (GET_CODE (operands[0]) == MEM && MEM_VOLATILE_P (operands[0]))\n+\tbreak;\n+\n+      intval = 0xffff & INTVAL (operands[2]);\n+\n+      if ((intval & 0xff00) == 0)\n+        {\n+\t  if (REG_P (operands[0]))\n+\t    {\n+\t      /* Do low byte access only for %eax or when high bit is set */\n+\t      if (REGNO (operands[0]) != 0 && !(intval & 0x80))\n+\t        break;\n+\t    }\n+\n+byte_or_operation:\n+\t    CC_STATUS_INIT;\n+\n+\t    if (intval == 0xff)\n+\t      return AS2 (mov%B0,%2,%b0);\n \n-\t  return AS2 (or%B0,%2,%b0);\n+\t    return AS2 (or%B0,%2,%b0);\n \t}\n \n-      /* Can we ignore the lower byte? */\n-      /* ??? what about offsettable memory references? */\n-      if (QI_REG_P (operands[0])\n-\t  && (INTVAL (operands[2]) & 0xff) == 0)\n+      /* high byte? */\n+      if ((intval & 0xff) == 0)\n \t{\n-\t  CC_STATUS_INIT;\n-\t  operands[2] = GEN_INT ((INTVAL (operands[2]) >> 8) & 0xff);\n+\t  intval >>= 8;\n+\t  operands[2] = GEN_INT (intval);\n \n-\t  if (INTVAL (operands[2]) == 0xff)\n-\t    return AS2 (mov%B0,%2,%h0);\n+\t  if (REG_P (operands[0]))\n+\t    {\n+\t      CC_STATUS_INIT;\n+\t      if (intval == 0xff)\n+\t\treturn AS2 (mov%B0,%2,%h0);\n+\n+\t      return AS2 (or%B0,%2,%h0);\n+\t    }\n+\n+\t  operands[0] = adj_offsettable_operand (operands[0], 1);\n \n-\t  return AS2 (or%B0,%2,%h0);\n+\t  goto byte_or_operation;\n \t}\n     }\n \n+  if (REG_P (operands[0])\n+      && i386_aligned_p (operands[2]))\n+    {\n+      CC_STATUS_INIT;\n+      operands[2] = i386_sext16_if_const (operands[2]);\n+      return AS2 (or%L0,%k2,%k0);\n+    }\n+\n+  if (GET_CODE (operands[2]) == CONST_INT\n+      && i386_aligned_p (operands[0]))\n+    {\n+      CC_STATUS_INIT;\n+      intval = 0xffff & INTVAL (operands[2]);\n+      if (intval != INTVAL (operands[2]))\n+\toperands[2] = GEN_INT (intval);\n+      return AS2 (or%L0,%2,%k0);\n+    }\n+\n   return AS2 (or%W0,%2,%0);\n }\")\n \n@@ -3773,37 +4026,83 @@\n \f\n ;;- xor instructions\n \n-;; ??? What if we only change one byte of an offsettable memory reference?\n (define_insn \"xorsi3\"\n   [(set (match_operand:SI 0 \"nonimmediate_operand\" \"=rm,r\")\n \t(xor:SI (match_operand:SI 1 \"nonimmediate_operand\" \"%0,0\")\n \t\t(match_operand:SI 2 \"general_operand\" \"ri,rm\")))]\n   \"\"\n   \"*\n {\n-  if (GET_CODE (operands[2]) == CONST_INT\n-      && ! (GET_CODE (operands[0]) == MEM && MEM_VOLATILE_P (operands[0])))\n+  HOST_WIDE_INT intval;\n+  switch (GET_CODE (operands[2]))\n     {\n-      if ((! REG_P (operands[0]) || QI_REG_P (operands[0]))\n-\t  && (INTVAL (operands[2]) & ~0xff) == 0)\n-\t{\n-\t  CC_STATUS_INIT;\n+    case CONST_INT:\n \n-\t  if (INTVAL (operands[2]) == 0xff)\n-\t    return AS1 (not%B0,%b0);\n+      if (REG_P (operands[0]) && ! QI_REG_P (operands[0]))\n+\tbreak;\n \n-\t  return AS2 (xor%B0,%2,%b0);\n+      /* don't try to optimize volatile accesses */\n+      if (GET_CODE (operands[0]) == MEM && MEM_VOLATILE_P (operands[0]))\n+\tbreak;\n+\n+      intval = INTVAL (operands[2]);\n+      if ((intval & ~0xff) == 0)\n+        {\n+\t  if (REG_P (operands[0]))\n+\t    {\n+\t      /* Do low byte access only for %eax or when high bit is set */\n+\t      if (REGNO (operands[0]) != 0 && !(intval & 0x80))\n+\t        break;\n+\t    }\n+\n+byte_xor_operation:\n+\t    CC_STATUS_INIT;\n+\t      \n+\t    if (intval == 0xff)\n+\t      return AS1 (not%B0,%b0);\n+\n+\t    if (intval != INTVAL (operands[2]))\n+\t      operands[2] = GEN_INT (intval);\n+\t    return AS2 (xor%B0,%2,%b0);\n \t}\n \n-      if (QI_REG_P (operands[0]) && (INTVAL (operands[2]) & ~0xff00) == 0)\n+      /* second byte? */\n+      if ((intval & ~0xff00) == 0)\n \t{\n-\t  CC_STATUS_INIT;\n-\t  operands[2] = GEN_INT (INTVAL (operands[2]) >> 8);\n+\t  intval >>= 8;\n \n-\t  if (INTVAL (operands[2]) == 0xff)\n-\t    return AS1 (not%B0,%h0);\n+\t  if (REG_P (operands[0]))\n+\t    {\n+\t      CC_STATUS_INIT;\n+\t      if (intval == 0xff)\n+\t\treturn AS1 (not%B0,%h0);\n \n-\t  return AS2 (xor%B0,%2,%h0);\n+\t      operands[2] = GEN_INT (intval);\n+\t      return AS2 (xor%B0,%2,%h0);\n+\t    }\n+\n+\t  operands[0] = adj_offsettable_operand (operands[0], 1);\n+\n+\t  goto byte_xor_operation;\n+\t}\n+\n+      if (REG_P (operands[0]))\n+\tbreak;\n+\n+      /* third byte? */\n+      if ((intval & ~0xff0000) == 0)\n+\t{\n+\t  intval >>= 16;\n+\t  operands[0] = adj_offsettable_operand (operands[0], 2);\n+\t  goto byte_xor_operation;\n+\t}\n+\n+      /* fourth byte? */\n+      if ((intval & ~0xff000000) == 0)\n+\t{\n+\t  intval = (intval >> 24) & 0xff;\n+\t  operands[0] = adj_offsettable_operand (operands[0], 3);\n+\t  goto byte_xor_operation;\n \t}\n     }\n \n@@ -3849,6 +4148,25 @@\n \t}\n     }\n \n+  if (REG_P (operands[0])\n+      && i386_aligned_p (operands[2]))\n+    {\n+      CC_STATUS_INIT;\n+      operands[2] = i386_sext16_if_const (operands[2]);\n+      return AS2 (xor%L0,%k2,%k0);\n+    }\n+\n+  if (GET_CODE (operands[2]) == CONST_INT\n+      && i386_aligned_p (operands[0]))\n+    {\n+      HOST_WIDE_INT intval;\n+      CC_STATUS_INIT;\n+      intval = 0xffff & INTVAL (operands[2]);\n+      if (intval != INTVAL (operands[2]))\n+\toperands[2] = GEN_INT (intval);\n+      return AS2 (xor%L0,%2,%k0);\n+    }\n+\n   return AS2 (xor%W0,%2,%0);\n }\")\n "}]}