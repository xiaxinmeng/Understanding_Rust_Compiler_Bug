{"sha": "0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MGU0ZDYzYzVlZWY1YzA1NjVhYzFkNDJhZTFhYzQzYTEyYzlmZDA3MQ==", "commit": {"author": {"name": "Alex Velenko", "email": "Alex.Velenko@arm.com", "date": "2014-04-23T17:02:49Z"}, "committer": {"name": "Marcus Shawcroft", "email": "mshawcroft@gcc.gnu.org", "date": "2014-04-23T17:02:49Z"}, "message": "[AArch64] VDUP Testcases\n\nThis patch adds vdup intrinsic testcases for AArch64. those testcases\nare nice to have, as it allows to reason about vdup consistency for\nboth LE and BE compiler flavors.\n\nFrom-SVN: r209713", "tree": {"sha": "b512219630a48b64791368bf33332d57ce41fb9a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/b512219630a48b64791368bf33332d57ce41fb9a"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071/comments", "author": null, "committer": null, "parents": [{"sha": "36e170203e7f09f6c23c7378f835e21c402b57c9", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/36e170203e7f09f6c23c7378f835e21c402b57c9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/36e170203e7f09f6c23c7378f835e21c402b57c9"}], "stats": {"total": 1398, "additions": 1398, "deletions": 0}, "files": [{"sha": "126ad08300b39c8f1f1a745f805df1ea1486edf7", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 6, "deletions": 0, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071", "patch": "@@ -1,3 +1,9 @@\n+2014-04-23  Alex Velenko  <Alex.Velenko@arm.com>\n+\n+\t* gcc.target/aarch64/vdup_lane_1.c: New testcase.\n+\t* gcc.target/aarch64/vdup_lane_2.c: New testcase.\n+\t* gcc.target/aarch64/vdup_n_1.c: New testcase.\n+\n 2014-04-23  Kyrylo Tkachov  <kyrylo.tkachov@arm.com>\n \n \t* gcc.target/arm/rev16.c: New test."}, {"sha": "4582471c8aad3d855eb33494ac01a62c87978ca9", "filename": "gcc/testsuite/gcc.target/aarch64/vdup_lane_1.c", "status": "added", "additions": 430, "deletions": 0, "changes": 430, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_lane_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_lane_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_lane_1.c?ref=0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071", "patch": "@@ -0,0 +1,430 @@\n+/* Test vdup_lane intrinsics work correctly.  */\n+/* { dg-do run } */\n+/* { dg-options \"--save-temps -O1\" } */\n+\n+#include <arm_neon.h>\n+\n+extern void abort (void);\n+\n+float32x2_t __attribute__ ((noinline))\n+wrap_vdup_lane_f32_0 (float32x2_t a)\n+{\n+  return vdup_lane_f32 (a, 0);\n+}\n+\n+float32x2_t __attribute__ ((noinline))\n+wrap_vdup_lane_f32_1 (float32x2_t a)\n+{\n+  return vdup_lane_f32 (a, 1);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_lane_f32 ()\n+{\n+  float32x2_t a;\n+  float32x2_t b;\n+  int i;\n+  float32_t c[2] = { 0.0 , 3.14 };\n+  float32_t d[2];\n+\n+  a = vld1_f32 (c);\n+  b = wrap_vdup_lane_f32_0 (a);\n+  vst1_f32 (d, b);\n+  for (i = 0; i < 2; i++)\n+    if (c[0] != d[i])\n+      return 1;\n+\n+  b = wrap_vdup_lane_f32_1 (a);\n+  vst1_f32 (d, b);\n+  for (i = 0; i < 2; i++)\n+    if (c[1] != d[i])\n+      return 1;\n+  return 0;\n+}\n+\n+float32x4_t __attribute__ ((noinline))\n+wrap_vdupq_lane_f32_0 (float32x2_t a)\n+{\n+  return vdupq_lane_f32 (a, 0);\n+}\n+\n+float32x4_t __attribute__ ((noinline))\n+wrap_vdupq_lane_f32_1 (float32x2_t a)\n+{\n+  return vdupq_lane_f32 (a, 1);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_lane_f32 ()\n+{\n+  float32x2_t a;\n+  float32x4_t b;\n+  int i;\n+  float32_t c[2] = { 0.0 , 3.14 };\n+  float32_t d[4];\n+\n+  a = vld1_f32 (c);\n+  b = wrap_vdupq_lane_f32_0 (a);\n+  vst1q_f32 (d, b);\n+  for (i = 0; i < 4; i++)\n+    if (c[0] != d[i])\n+      return 1;\n+\n+  b = wrap_vdupq_lane_f32_1 (a);\n+  vst1q_f32 (d, b);\n+  for (i = 0; i < 4; i++)\n+    if (c[1] != d[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int8x8_t __attribute__ ((noinline))\n+wrap_vdup_lane_s8_0 (int8x8_t a)\n+{\n+  return vdup_lane_s8 (a, 0);\n+}\n+\n+int8x8_t __attribute__ ((noinline))\n+wrap_vdup_lane_s8_1 (int8x8_t a)\n+{\n+  return vdup_lane_s8 (a, 1);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_lane_s8 ()\n+{\n+  int8x8_t a;\n+  int8x8_t b;\n+  int i;\n+  /* Only two first cases are interesting.  */\n+  int8_t c[8] = { 0, 1, 2, 3, 4, 5, 6, 7 };\n+  int8_t d[8];\n+\n+  a = vld1_s8 (c);\n+  b = wrap_vdup_lane_s8_0 (a);\n+  vst1_s8 (d, b);\n+  for (i = 0; i < 8; i++)\n+    if (c[0] != d[i])\n+      return 1;\n+\n+  b = wrap_vdup_lane_s8_1 (a);\n+  vst1_s8 (d, b);\n+  for (i = 0; i < 8; i++)\n+    if (c[1] != d[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int8x16_t __attribute__ ((noinline))\n+wrap_vdupq_lane_s8_0 (int8x8_t a)\n+{\n+  return vdupq_lane_s8 (a, 0);\n+}\n+\n+int8x16_t __attribute__ ((noinline))\n+wrap_vdupq_lane_s8_1 (int8x8_t a)\n+{\n+  return vdupq_lane_s8 (a, 1);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_lane_s8 ()\n+{\n+  int8x8_t a;\n+  int8x16_t b;\n+  int i;\n+  /* Only two first cases are interesting.  */\n+  int8_t c[8] = { 0, 1, 2, 3, 4, 5, 6, 7 };\n+  int8_t d[16];\n+\n+  a = vld1_s8 (c);\n+  b = wrap_vdupq_lane_s8_0 (a);\n+  vst1q_s8 (d, b);\n+  for (i = 0; i < 16; i++)\n+    if (c[0] != d[i])\n+      return 1;\n+\n+  b = wrap_vdupq_lane_s8_1 (a);\n+  vst1q_s8 (d, b);\n+  for (i = 0; i < 16; i++)\n+    if (c[1] != d[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int16x4_t __attribute__ ((noinline))\n+wrap_vdup_lane_s16_0 (int16x4_t a)\n+{\n+  return vdup_lane_s16 (a, 0);\n+}\n+\n+int16x4_t __attribute__ ((noinline))\n+wrap_vdup_lane_s16_1 (int16x4_t a)\n+{\n+  return vdup_lane_s16 (a, 1);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_lane_s16 ()\n+{\n+  int16x4_t a;\n+  int16x4_t b;\n+  int i;\n+  /* Only two first cases are interesting.  */\n+  int16_t c[4] = { 0, 1, 2, 3 };\n+  int16_t d[4];\n+\n+  a = vld1_s16 (c);\n+  b = wrap_vdup_lane_s16_0 (a);\n+  vst1_s16 (d, b);\n+  for (i = 0; i < 4; i++)\n+    if (c[0] != d[i])\n+      return 1;\n+\n+  b = wrap_vdup_lane_s16_1 (a);\n+  vst1_s16 (d, b);\n+  for (i = 0; i < 4; i++)\n+    if (c[1] != d[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int16x8_t __attribute__ ((noinline))\n+wrap_vdupq_lane_s16_0 (int16x4_t a)\n+{\n+  return vdupq_lane_s16 (a, 0);\n+}\n+\n+int16x8_t __attribute__ ((noinline))\n+wrap_vdupq_lane_s16_1 (int16x4_t a)\n+{\n+  return vdupq_lane_s16 (a, 1);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_lane_s16 ()\n+{\n+  int16x4_t a;\n+  int16x8_t b;\n+  int i;\n+  /* Only two first cases are interesting.  */\n+  int16_t c[4] = { 0, 1, 2, 3 };\n+  int16_t d[8];\n+\n+  a = vld1_s16 (c);\n+  b = wrap_vdupq_lane_s16_0 (a);\n+  vst1q_s16 (d, b);\n+  for (i = 0; i < 8; i++)\n+    if (c[0] != d[i])\n+      return 1;\n+\n+  b = wrap_vdupq_lane_s16_1 (a);\n+  vst1q_s16 (d, b);\n+  for (i = 0; i < 8; i++)\n+    if (c[1] != d[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int32x2_t __attribute__ ((noinline))\n+wrap_vdup_lane_s32_0 (int32x2_t a)\n+{\n+  return vdup_lane_s32 (a, 0);\n+}\n+\n+int32x2_t __attribute__ ((noinline))\n+wrap_vdup_lane_s32_1 (int32x2_t a)\n+{\n+  return vdup_lane_s32 (a, 1);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_lane_s32 ()\n+{\n+  int32x2_t a;\n+  int32x2_t b;\n+  int i;\n+  int32_t c[2] = { 0, 1 };\n+  int32_t d[2];\n+\n+  a = vld1_s32 (c);\n+  b = wrap_vdup_lane_s32_0 (a);\n+  vst1_s32 (d, b);\n+  for (i = 0; i < 2; i++)\n+    if (c[0] != d[i])\n+      return 1;\n+\n+  b = wrap_vdup_lane_s32_1 (a);\n+  vst1_s32 (d, b);\n+  for (i = 0; i < 2; i++)\n+    if (c[1] != d[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int32x4_t __attribute__ ((noinline))\n+wrap_vdupq_lane_s32_0 (int32x2_t a)\n+{\n+  return vdupq_lane_s32 (a, 0);\n+}\n+\n+int32x4_t __attribute__ ((noinline))\n+wrap_vdupq_lane_s32_1 (int32x2_t a)\n+{\n+  return vdupq_lane_s32 (a, 1);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_lane_s32 ()\n+{\n+  int32x2_t a;\n+  int32x4_t b;\n+  int i;\n+  int32_t c[2] = { 0, 1 };\n+  int32_t d[4];\n+\n+  a = vld1_s32 (c);\n+  b = wrap_vdupq_lane_s32_0 (a);\n+  vst1q_s32 (d, b);\n+  for (i = 0; i < 4; i++)\n+    if (c[0] != d[i])\n+      return 1;\n+\n+  b = wrap_vdupq_lane_s32_1 (a);\n+  vst1q_s32 (d, b);\n+  for (i = 0; i < 4; i++)\n+    if (c[1] != d[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int64x1_t __attribute__ ((noinline))\n+wrap_vdup_lane_s64_0 (int64x1_t a)\n+{\n+  return vdup_lane_s64 (a, 0);\n+}\n+\n+int64x1_t __attribute__ ((noinline))\n+wrap_vdup_lane_s64_1 (int64x1_t a)\n+{\n+  return vdup_lane_s64 (a, 1);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_lane_s64 ()\n+{\n+  int64x1_t a;\n+  int64x1_t b;\n+  int64_t c[1];\n+  int64_t d[1];\n+\n+  c[0] = 0;\n+  a = vld1_s64 (c);\n+  b = wrap_vdup_lane_s64_0 (a);\n+  vst1_s64 (d, b);\n+  if (c[0] != d[0])\n+    return 1;\n+\n+  c[0] = 1;\n+  a = vld1_s64 (c);\n+  b = wrap_vdup_lane_s64_1 (a);\n+  vst1_s64 (d, b);\n+  if (c[0] != d[0])\n+    return 1;\n+  return 0;\n+}\n+\n+int64x2_t __attribute__ ((noinline))\n+wrap_vdupq_lane_s64_0 (int64x1_t a)\n+{\n+  return vdupq_lane_s64 (a, 0);\n+}\n+\n+int64x2_t __attribute__ ((noinline))\n+wrap_vdupq_lane_s64_1 (int64x1_t a)\n+{\n+  return vdupq_lane_s64 (a, 1);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_lane_s64 ()\n+{\n+  int64x1_t a;\n+  int64x2_t b;\n+  int i;\n+  int64_t c[1];\n+  int64_t d[2];\n+\n+  c[0] = 0;\n+  a = vld1_s64 (c);\n+  b = wrap_vdupq_lane_s64_0 (a);\n+  vst1q_s64 (d, b);\n+  for (i = 0; i < 2; i++)\n+    if (c[0] != d[i])\n+      return 1;\n+\n+  c[0] = 1;\n+  a = vld1_s64 (c);\n+  b = wrap_vdupq_lane_s64_1 (a);\n+  vst1q_s64 (d, b);\n+  for (i = 0; i < 2; i++)\n+    if (c[0] != d[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int\n+main ()\n+{\n+\n+  if (test_vdup_lane_f32 ())\n+    abort ();\n+  if (test_vdup_lane_s8 ())\n+    abort ();\n+  if (test_vdup_lane_s16 ())\n+    abort ();\n+  if (test_vdup_lane_s32 ())\n+    abort ();\n+  if (test_vdup_lane_s64 ())\n+    abort ();\n+  if (test_vdupq_lane_f32 ())\n+    abort ();\n+  if (test_vdupq_lane_s8 ())\n+    abort ();\n+  if (test_vdupq_lane_s16 ())\n+    abort ();\n+  if (test_vdupq_lane_s32 ())\n+    abort ();\n+  if (test_vdupq_lane_s64 ())\n+    abort ();\n+\n+  return 0;\n+}\n+\n+/* Asm check for test_vdup_lane_s8.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.8b, v\\[0-9\\]+\\.b\\\\\\[0\\\\\\]\" 1 } } */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.8b, v\\[0-9\\]+\\.b\\\\\\[1\\\\\\]\" 1 } } */\n+\n+/* Asm check for test_vdupq_lane_s8.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.16b, v\\[0-9\\]+\\.b\\\\\\[0\\\\\\]\" 1 } } */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.16b, v\\[0-9\\]+\\.b\\\\\\[1\\\\\\]\" 1 } } */\n+\n+/* Asm check for test_vdup_lane_s16.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.4h, v\\[0-9\\]+\\.h\\\\\\[0\\\\\\]\" 1 } } */\n+/* Asm check for test_vdup_lane_s16.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.4h, v\\[0-9\\]+\\.h\\\\\\[1\\\\\\]\" 1 } } */\n+\n+/* Asm check for test_vdupq_lane_s16.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.8h, v\\[0-9\\]+\\.h\\\\\\[0\\\\\\]\" 1 } } */\n+/* Asm check for test_vdupq_lane_s16.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.8h, v\\[0-9\\]+\\.h\\\\\\[1\\\\\\]\" 1 } } */\n+\n+/* Asm check for test_vdup_lane_f32 and test_vdup_lane_s32.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.2s, v\\[0-9\\]+\\.s\\\\\\[0\\\\\\]\" 2 } } */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.2s, v\\[0-9\\]+\\.s\\\\\\[1\\\\\\]\" 2 } } */\n+\n+/* Asm check for test_vdupq_lane_f32 and test_vdupq_lane_s32.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.4s, v\\[0-9\\]+\\.s\\\\\\[0\\\\\\]\" 2 } } */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.4s, v\\[0-9\\]+\\.s\\\\\\[1\\\\\\]\" 2 } } */\n+\n+/* { dg-final { cleanup-saved-temps } } */"}, {"sha": "7c04e759a5291bf5213ad5abf5c75289afad7359", "filename": "gcc/testsuite/gcc.target/aarch64/vdup_lane_2.c", "status": "added", "additions": 343, "deletions": 0, "changes": 343, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_lane_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_lane_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_lane_2.c?ref=0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071", "patch": "@@ -0,0 +1,343 @@\n+/* Test vdup_lane intrinsics work correctly.  */\n+/* { dg-do run } */\n+/* { dg-options \"-O1 --save-temps\" } */\n+\n+#include <arm_neon.h>\n+\n+#define force_simd(V1)   asm volatile (\"\"\t\\\n+          : \"=w\"(V1)\t\t\t\t\\\n+          : \"w\"(V1)\t\t\t\t\\\n+          : /* No clobbers */)\n+\n+extern void abort (void);\n+\n+float32_t __attribute__ ((noinline))\n+wrap_vdups_lane_f32_0 (float32x2_t dummy, float32x2_t a)\n+{\n+  return vdups_lane_f32 (a, 0);\n+}\n+\n+float32_t __attribute__ ((noinline))\n+wrap_vdups_lane_f32_1 (float32x2_t a)\n+{\n+  return vdups_lane_f32 (a, 1);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdups_lane_f32 ()\n+{\n+  float32x2_t a;\n+  float32_t b;\n+  float32_t c[2] = { 0.0, 1.0 };\n+\n+  a = vld1_f32 (c);\n+  b = wrap_vdups_lane_f32_0 (a, a);\n+  if (c[0] != b)\n+    return 1;\n+  b = wrap_vdups_lane_f32_1 (a);\n+  if (c[1] != b)\n+    return 1;\n+  return 0;\n+}\n+\n+float64_t __attribute__ ((noinline))\n+wrap_vdupd_lane_f64_0 (float64x1_t dummy, float64x1_t a)\n+{\n+  return vdupd_lane_f64 (a, 0);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupd_lane_f64 ()\n+{\n+  float64x1_t a;\n+  float64_t b;\n+  float64_t c[1] = { 0.0 };\n+  a = vld1_f64 (c);\n+  b = wrap_vdupd_lane_f64_0 (a, a);\n+  if (c[0] != b)\n+    return 1;\n+  return 0;\n+}\n+\n+int8_t __attribute__ ((noinline))\n+wrap_vdupb_lane_s8_0 (int8x8_t dummy, int8x8_t a)\n+{\n+  int8_t result = vdupb_lane_s8 (a, 0);\n+  force_simd (result);\n+  return result;\n+}\n+\n+int8_t __attribute__ ((noinline))\n+wrap_vdupb_lane_s8_1 (int8x8_t a)\n+{\n+  int8_t result = vdupb_lane_s8 (a, 1);\n+  force_simd (result);\n+  return result;\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupb_lane_s8 ()\n+{\n+  int8x8_t a;\n+  int8_t b;\n+  int8_t c[8] = { 0, 1, 2, 3, 4, 5, 6, 7 };\n+\n+  a = vld1_s8 (c);\n+  b = wrap_vdupb_lane_s8_0 (a, a);\n+  if (c[0] != b)\n+    return 1;\n+  b = wrap_vdupb_lane_s8_1 (a);\n+  if (c[1] != b)\n+    return 1;\n+\n+  return 0;\n+}\n+\n+uint8_t __attribute__ ((noinline))\n+wrap_vdupb_lane_u8_0 (uint8x8_t dummy, uint8x8_t a)\n+{\n+  uint8_t result = vdupb_lane_u8 (a, 0);\n+  force_simd (result);\n+  return result;\n+}\n+\n+uint8_t __attribute__ ((noinline))\n+wrap_vdupb_lane_u8_1 (uint8x8_t a)\n+{\n+  uint8_t result = vdupb_lane_u8 (a, 1);\n+  force_simd (result);\n+  return result;\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupb_lane_u8 ()\n+{\n+  uint8x8_t a;\n+  uint8_t b;\n+  uint8_t c[8] = { 0, 1, 2, 3, 4, 5, 6, 7 };\n+\n+  a = vld1_u8 (c);\n+  b = wrap_vdupb_lane_u8_0 (a, a);\n+  if (c[0] != b)\n+    return 1;\n+  b = wrap_vdupb_lane_u8_1 (a);\n+  if (c[1] != b)\n+    return 1;\n+  return 0;\n+}\n+\n+int16_t __attribute__ ((noinline))\n+wrap_vduph_lane_s16_0 (int16x4_t dummy, int16x4_t a)\n+{\n+  int16_t result = vduph_lane_s16 (a, 0);\n+  force_simd (result);\n+  return result;\n+}\n+\n+int16_t __attribute__ ((noinline))\n+wrap_vduph_lane_s16_1 (int16x4_t a)\n+{\n+  int16_t result = vduph_lane_s16 (a, 1);\n+  force_simd (result);\n+  return result;\n+}\n+\n+int __attribute__ ((noinline))\n+test_vduph_lane_s16 ()\n+{\n+  int16x4_t a;\n+  int16_t b;\n+  int16_t c[4] = { 0, 1, 2, 3 };\n+\n+  a = vld1_s16 (c);\n+  b = wrap_vduph_lane_s16_0 (a, a);\n+  if (c[0] != b)\n+    return 1;\n+  b = wrap_vduph_lane_s16_1 (a);\n+  if (c[1] != b)\n+    return 1;\n+  return 0;\n+}\n+\n+uint16_t __attribute__ ((noinline))\n+wrap_vduph_lane_u16_0 (uint16x4_t dummy, uint16x4_t a)\n+{\n+  uint16_t result = vduph_lane_u16 (a, 0);\n+  force_simd (result);\n+  return result;\n+}\n+\n+uint16_t __attribute__ ((noinline))\n+wrap_vduph_lane_u16_1 (uint16x4_t a)\n+{\n+  uint16_t result = vduph_lane_u16 (a, 1);\n+  force_simd (result);\n+  return result;\n+}\n+\n+int __attribute__ ((noinline))\n+test_vduph_lane_u16 ()\n+{\n+  uint16x4_t a;\n+  uint16_t b;\n+  uint16_t c[4] = { 0, 1, 2, 3 };\n+\n+  a = vld1_u16 (c);\n+  b = wrap_vduph_lane_u16_0 (a, a);\n+  if (c[0] != b)\n+    return 1;\n+  b = wrap_vduph_lane_u16_1 (a);\n+  if (c[1] != b)\n+    return 1;\n+  return 0;\n+}\n+\n+int32_t __attribute__ ((noinline))\n+wrap_vdups_lane_s32_0 (int32x2_t dummy, int32x2_t a)\n+{\n+  int32_t result = vdups_lane_s32 (a, 0);\n+  force_simd (result);\n+  return result;\n+}\n+\n+int32_t __attribute__ ((noinline))\n+wrap_vdups_lane_s32_1 (int32x2_t a)\n+{\n+  int32_t result = vdups_lane_s32 (a, 1);\n+  force_simd (result);\n+  return result;\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdups_lane_s32 ()\n+{\n+  int32x2_t a;\n+  int32_t b;\n+  int32_t c[2] = { 0, 1 };\n+\n+  a = vld1_s32 (c);\n+  b = wrap_vdups_lane_s32_0 (vcreate_s32 (0), a);\n+  if (c[0] != b)\n+    return 1;\n+  b = wrap_vdups_lane_s32_1 (a);\n+  if (c[1] != b)\n+    return 1;\n+  return 0;\n+}\n+\n+uint32_t __attribute__ ((noinline))\n+wrap_vdups_lane_u32_0 (uint32x2_t dummy, uint32x2_t a)\n+{\n+  uint32_t result = vdups_lane_u32 (a, 0);\n+  force_simd (result);\n+  return result;\n+}\n+\n+uint32_t __attribute__ ((noinline))\n+wrap_vdups_lane_u32_1 (uint32x2_t a)\n+{\n+  uint32_t result = vdups_lane_u32 (a, 1);\n+  force_simd (result);\n+  return result;\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdups_lane_u32 ()\n+{\n+  uint32x2_t a;\n+  uint32_t b;\n+  uint32_t c[2] = { 0, 1 };\n+  a = vld1_u32 (c);\n+  b = wrap_vdups_lane_u32_0 (a, a);\n+  if (c[0] != b)\n+    return 1;\n+  b = wrap_vdups_lane_u32_1 (a);\n+  if (c[1] != b)\n+    return 1;\n+  return 0;\n+}\n+\n+uint64_t __attribute__ ((noinline))\n+wrap_vdupd_lane_u64_0 (uint64x1_t dummy, uint64x1_t a)\n+{\n+  return vdupd_lane_u64 (a, 0);;\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupd_lane_u64 ()\n+{\n+  uint64x1_t a;\n+  uint64_t b;\n+  uint64_t c[1] = { 0 };\n+\n+  a = vld1_u64 (c);\n+  b = wrap_vdupd_lane_u64_0 (a, a);\n+  if (c[0] != b)\n+    return 1;\n+  return 0;\n+}\n+\n+int64_t __attribute__ ((noinline))\n+wrap_vdupd_lane_s64_0 (uint64x1_t dummy, int64x1_t a)\n+{\n+  return vdupd_lane_u64 (a, 0);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupd_lane_s64 ()\n+{\n+  int64x1_t a;\n+  int64_t b;\n+  int64_t c[1] = { 0 };\n+\n+  a = vld1_s64 (c);\n+  b = wrap_vdupd_lane_s64_0 (a, a);\n+  if (c[0] != b)\n+    return 1;\n+  return 0;\n+}\n+\n+int\n+main ()\n+{\n+  if (test_vdups_lane_f32 ())\n+    abort ();\n+  if (test_vdupd_lane_f64 ())\n+    abort ();\n+  if (test_vdupb_lane_s8 ())\n+    abort ();\n+  if (test_vdupb_lane_u8 ())\n+    abort ();\n+  if (test_vduph_lane_s16 ())\n+    abort ();\n+  if (test_vduph_lane_u16 ())\n+    abort ();\n+  if (test_vdups_lane_s32 ())\n+    abort ();\n+  if (test_vdups_lane_u32 ())\n+    abort ();\n+  if (test_vdupd_lane_s64 ())\n+    abort ();\n+  if (test_vdupd_lane_u64 ())\n+    abort ();\n+  return 0;\n+}\n+\n+/* Asm check for vdupb_lane_s8, vdupb_lane_u8.  */\n+/* { dg-final { scan-assembler-not \"dup\\\\tb\\[0-9\\]+, v\\[0-9\\]+\\.b\\\\\\[0\\\\\\]\" } } */\n+/* { dg-final { scan-assembler-times \"dup\\\\tb\\[0-9\\]+, v\\[0-9\\]+\\.b\\\\\\[1\\\\\\]\" 2 } } */\n+\n+/* Asm check for vduph_lane_h16, vduph_lane_h16.  */\n+/* { dg-final { scan-assembler-not \"dup\\\\th\\[0-9\\]+, v\\[0-9\\]+\\.h\\\\\\[0\\\\\\]\" } } */\n+/* { dg-final { scan-assembler-times \"dup\\\\th\\[0-9\\]+, v\\[0-9\\]+\\.h\\\\\\[1\\\\\\]\" 2 } } */\n+\n+/* Asm check for vdups_lane_f32, vdups_lane_s32, vdups_lane_u32.  */\n+/* Can't generate \"dup s<n>, v<m>[0]\" for vdups_lane_s32 and vdups_lane_u32.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\ts\\[0-9\\]+, v\\[0-9\\]+\\.s\\\\\\[0\\\\\\]\" 1} } */\n+/* { dg-final { scan-assembler-times \"dup\\\\ts\\[0-9\\]+, v\\[0-9\\]+\\.s\\\\\\[1\\\\\\]\" 3 } } */\n+\n+/* Asm check for vdupd_lane_f64, vdupd_lane_s64, vdupd_lane_u64.  */\n+/* Attempts to make the compiler generate vdupd are not practical.  */\n+/* { dg-final { scan-assembler-not \"dup\\\\td\\[0-9\\]+, v\\[0-9\\]+\\.d\\\\\\[0\\\\\\]\" } }\n+\n+/* { dg-final { cleanup-saved-temps } } */"}, {"sha": "a79910d68d744314205f9ecab67ec07560681725", "filename": "gcc/testsuite/gcc.target/aarch64/vdup_n_1.c", "status": "added", "additions": 619, "deletions": 0, "changes": 619, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_n_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_n_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fvdup_n_1.c?ref=0e4d63c5eef5c0565ac1d42ae1ac43a12c9fd071", "patch": "@@ -0,0 +1,619 @@\n+/* Test vdup_lane intrinsics work correctly.  */\n+/* { dg-do run } */\n+/* { dg-options \"-O1 --save-temps\" } */\n+\n+#include <arm_neon.h>\n+\n+extern void abort (void);\n+\n+float32x2_t __attribute__ ((noinline))\n+wrap_vdup_n_f32 (float32_t a)\n+{\n+  return vdup_n_f32 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_n_f32 ()\n+{\n+  float32_t a = 1.0;\n+  float32x2_t b;\n+  float32_t c[2];\n+  int i;\n+\n+  b = wrap_vdup_n_f32 (a);\n+  vst1_f32 (c, b);\n+  for (i = 0; i < 2; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+float32x4_t __attribute__ ((noinline))\n+wrap_vdupq_n_f32 (float32_t a)\n+{\n+  return vdupq_n_f32 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_n_f32 ()\n+{\n+  float32_t a = 1.0;\n+  float32x4_t b;\n+  float32_t c[4];\n+  int i;\n+\n+  b = wrap_vdupq_n_f32 (a);\n+  vst1q_f32 (c, b);\n+  for (i = 0; i < 4; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+float64x1_t __attribute__ ((noinline))\n+wrap_vdup_n_f64 (float64_t a)\n+{\n+  return vdup_n_f64 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_n_f64 ()\n+{\n+  float64_t a = 1.0;\n+  float64x1_t b;\n+  float64_t c[1];\n+  int i;\n+\n+  b = wrap_vdup_n_f64 (a);\n+  vst1_f64 (c, b);\n+  for (i = 0; i < 1; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+float64x2_t __attribute__ ((noinline))\n+wrap_vdupq_n_f64 (float64_t a)\n+{\n+  return vdupq_n_f64 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_n_f64 ()\n+{\n+  float64_t a = 1.0;\n+  float64x2_t b;\n+  float64_t c[2];\n+  int i;\n+\n+  b = wrap_vdupq_n_f64 (a);\n+  vst1q_f64 (c, b);\n+  for (i = 0; i < 2; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+poly8x8_t __attribute__ ((noinline))\n+wrap_vdup_n_p8 (poly8_t a)\n+{\n+  return vdup_n_p8 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_n_p8 ()\n+{\n+  poly8_t a = 1;\n+  poly8x8_t b;\n+  poly8_t c[8];\n+  int i;\n+\n+  b = wrap_vdup_n_p8 (a);\n+  vst1_p8 (c, b);\n+  for (i = 0; i < 8; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+poly8x16_t __attribute__ ((noinline))\n+wrap_vdupq_n_p8 (poly8_t a)\n+{\n+  return vdupq_n_p8 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_n_p8 ()\n+{\n+  poly8_t a = 1;\n+  poly8x16_t b;\n+  poly8_t c[16];\n+  int i;\n+\n+  b = wrap_vdupq_n_p8 (a);\n+  vst1q_p8 (c, b);\n+  for (i = 0; i < 16; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int8x8_t __attribute__ ((noinline))\n+wrap_vdup_n_s8 (int8_t a)\n+{\n+  return vdup_n_s8 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_n_s8 ()\n+{\n+  int8_t a = 1;\n+  int8x8_t b;\n+  int8_t c[8];\n+  int i;\n+\n+  b = wrap_vdup_n_s8 (a);\n+  vst1_s8 (c, b);\n+  for (i = 0; i < 8; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int8x16_t __attribute__ ((noinline))\n+wrap_vdupq_n_s8 (int8_t a)\n+{\n+  return vdupq_n_s8 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_n_s8 ()\n+{\n+  int8_t a = 1;\n+  int8x16_t b;\n+  int8_t c[16];\n+  int i;\n+\n+  b = wrap_vdupq_n_s8 (a);\n+  vst1q_s8 (c, b);\n+  for (i = 0; i < 16; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+uint8x8_t __attribute__ ((noinline))\n+wrap_vdup_n_u8 (uint8_t a)\n+{\n+  return vdup_n_u8 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_n_u8 ()\n+{\n+  uint8_t a = 1;\n+  uint8x8_t b;\n+  uint8_t c[8];\n+  int i;\n+\n+  b = wrap_vdup_n_u8 (a);\n+  vst1_u8 (c, b);\n+  for (i = 0; i < 8; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+uint8x16_t __attribute__ ((noinline))\n+wrap_vdupq_n_u8 (uint8_t a)\n+{\n+  return vdupq_n_u8 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_n_u8 ()\n+{\n+  uint8_t a = 1;\n+  uint8x16_t b;\n+  uint8_t c[16];\n+  int i;\n+\n+  b = wrap_vdupq_n_u8 (a);\n+  vst1q_u8 (c, b);\n+  for (i = 0; i < 16; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+poly16x4_t __attribute__ ((noinline))\n+wrap_vdup_n_p16 (poly16_t a)\n+{\n+  return vdup_n_p16 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_n_p16 ()\n+{\n+  poly16_t a = 1;\n+  poly16x4_t b;\n+  poly16_t c[4];\n+  int i;\n+\n+  b = wrap_vdup_n_p16 (a);\n+  vst1_p16 (c, b);\n+  for (i = 0; i < 4; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+poly16x8_t __attribute__ ((noinline))\n+wrap_vdupq_n_p16 (poly16_t a)\n+{\n+  return vdupq_n_p16 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_n_p16 ()\n+{\n+  poly16_t a = 1;\n+  poly16x8_t b;\n+  poly16_t c[8];\n+  int i;\n+\n+  b = wrap_vdupq_n_p16 (a);\n+  vst1q_p16 (c, b);\n+  for (i = 0; i < 8; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int16x4_t __attribute__ ((noinline))\n+wrap_vdup_n_s16 (int16_t a)\n+{\n+  return vdup_n_s16 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_n_s16 ()\n+{\n+  int16_t a = 1;\n+  int16x4_t b;\n+  int16_t c[4];\n+  int i;\n+\n+  b = wrap_vdup_n_s16 (a);\n+  vst1_s16 (c, b);\n+  for (i = 0; i < 4; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int16x8_t __attribute__ ((noinline))\n+wrap_vdupq_n_s16 (int16_t a)\n+{\n+  return vdupq_n_s16 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_n_s16 ()\n+{\n+  int16_t a = 1;\n+  int16x8_t b;\n+  int16_t c[8];\n+  int i;\n+\n+  b = wrap_vdupq_n_s16 (a);\n+  vst1q_s16 (c, b);\n+  for (i = 0; i < 8; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+uint16x4_t __attribute__ ((noinline))\n+wrap_vdup_n_u16 (uint16_t a)\n+{\n+  return vdup_n_u16 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_n_u16 ()\n+{\n+  uint16_t a = 1;\n+  uint16x4_t b;\n+  uint16_t c[4];\n+  int i;\n+\n+  b = wrap_vdup_n_u16 (a);\n+  vst1_u16 (c, b);\n+  for (i = 0; i < 4; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+uint16x8_t __attribute__ ((noinline))\n+wrap_vdupq_n_u16 (uint16_t a)\n+{\n+  return vdupq_n_u16 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_n_u16 ()\n+{\n+  uint16_t a = 1;\n+  uint16x8_t b;\n+  uint16_t c[8];\n+  int i;\n+\n+  b = wrap_vdupq_n_u16 (a);\n+  vst1q_u16 (c, b);\n+  for (i = 0; i < 8; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int32x2_t __attribute__ ((noinline))\n+wrap_vdup_n_s32 (int32_t a)\n+{\n+  return vdup_n_s32 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_n_s32 ()\n+{\n+  int32_t a = 1;\n+  int32x2_t b;\n+  int32_t c[2];\n+  int i;\n+\n+  b = wrap_vdup_n_s32 (a);\n+  vst1_s32 (c, b);\n+  for (i = 0; i < 2; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int32x4_t __attribute__ ((noinline))\n+wrap_vdupq_n_s32 (int32_t a)\n+{\n+  return vdupq_n_s32 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_n_s32 ()\n+{\n+  int32_t a = 1;\n+  int32x4_t b;\n+  int32_t c[4];\n+  int i;\n+\n+  b = wrap_vdupq_n_s32 (a);\n+  vst1q_s32 (c, b);\n+  for (i = 0; i < 4; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+uint32x2_t __attribute__ ((noinline))\n+wrap_vdup_n_u32 (uint32_t a)\n+{\n+  return vdup_n_u32 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_n_u32 ()\n+{\n+  uint32_t a = 1;\n+  uint32x2_t b;\n+  uint32_t c[2];\n+  int i;\n+\n+  b = wrap_vdup_n_u32 (a);\n+  vst1_u32 (c, b);\n+  for (i = 0; i < 2; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+uint32x4_t __attribute__ ((noinline))\n+wrap_vdupq_n_u32 (uint32_t a)\n+{\n+  return vdupq_n_u32 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_n_u32 ()\n+{\n+  uint32_t a = 1;\n+  uint32x4_t b;\n+  uint32_t c[4];\n+  int i;\n+\n+  b = wrap_vdupq_n_u32 (a);\n+  vst1q_u32 (c, b);\n+  for (i = 0; i < 4; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int64x1_t __attribute__ ((noinline))\n+wrap_vdup_n_s64 (int64_t a)\n+{\n+  return vdup_n_s64 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_n_s64 ()\n+{\n+  int64_t a = 1;\n+  int64x1_t b;\n+  int64_t c[1];\n+  int i;\n+\n+  b = wrap_vdup_n_s64 (a);\n+  vst1_s64 (c, b);\n+  for (i = 0; i < 1; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int64x2_t __attribute__ ((noinline))\n+wrap_vdupq_n_s64 (int64_t a)\n+{\n+  return vdupq_n_s64 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_n_s64 ()\n+{\n+  int64_t a = 1;\n+  int64x2_t b;\n+  int64_t c[2];\n+  int i;\n+\n+  b = wrap_vdupq_n_s64 (a);\n+  vst1q_s64 (c, b);\n+  for (i = 0; i < 2; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+uint64x1_t __attribute__ ((noinline))\n+wrap_vdup_n_u64 (uint64_t a)\n+{\n+  return vdup_n_u64 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdup_n_u64 ()\n+{\n+  uint64_t a = 1;\n+  uint64x1_t b;\n+  uint64_t c[1];\n+  int i;\n+\n+  b = wrap_vdup_n_u64 (a);\n+  vst1_u64 (c, b);\n+  for (i = 0; i < 1; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+uint64x2_t __attribute__ ((noinline))\n+wrap_vdupq_n_u64 (uint64_t a)\n+{\n+  return vdupq_n_u64 (a);\n+}\n+\n+int __attribute__ ((noinline))\n+test_vdupq_n_u64 ()\n+{\n+  uint64_t a = 1;\n+  uint64x2_t b;\n+  uint64_t c[2];\n+  int i;\n+\n+  b = wrap_vdupq_n_u64 (a);\n+  vst1q_u64 (c, b);\n+  for (i = 0; i < 2; i++)\n+    if (a != c[i])\n+      return 1;\n+  return 0;\n+}\n+\n+int\n+main ()\n+{\n+  if (test_vdup_n_f32 ())\n+    abort ();\n+  if (test_vdup_n_f64 ())\n+    abort ();\n+  if (test_vdup_n_p8 ())\n+    abort ();\n+  if (test_vdup_n_u8 ())\n+    abort ();\n+  if (test_vdup_n_s8 ())\n+    abort ();\n+  if (test_vdup_n_p16 ())\n+    abort ();\n+  if (test_vdup_n_s16 ())\n+    abort ();\n+  if (test_vdup_n_u16 ())\n+    abort ();\n+  if (test_vdup_n_s32 ())\n+    abort ();\n+  if (test_vdup_n_u32 ())\n+    abort ();\n+  if (test_vdup_n_s64 ())\n+    abort ();\n+  if (test_vdup_n_u64 ())\n+    abort ();\n+  if (test_vdupq_n_f32 ())\n+    abort ();\n+  if (test_vdupq_n_f64 ())\n+    abort ();\n+  if (test_vdupq_n_p8 ())\n+    abort ();\n+  if (test_vdupq_n_u8 ())\n+    abort ();\n+  if (test_vdupq_n_s8 ())\n+    abort ();\n+  if (test_vdupq_n_p16 ())\n+    abort ();\n+  if (test_vdupq_n_s16 ())\n+    abort ();\n+  if (test_vdupq_n_u16 ())\n+    abort ();\n+  if (test_vdupq_n_s32 ())\n+    abort ();\n+  if (test_vdupq_n_u32 ())\n+    abort ();\n+  if (test_vdupq_n_s64 ())\n+    abort ();\n+  if (test_vdupq_n_u64 ())\n+    abort ();\n+  return 0;\n+}\n+\n+/* No asm checks for vdup_n_f32, vdupq_n_f32, vdup_n_f64 and vdupq_n_f64.\n+   Cannot force floating point value in general purpose regester.  */\n+\n+/* Asm check for test_vdup_n_p8, test_vdup_n_s8, test_vdup_n_u8.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.8b, w\\[0-9\\]+\" 3 } } */\n+\n+/* Asm check for test_vdupq_n_p8, test_vdupq_n_s8, test_vdupq_n_u8.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.16b, w\\[0-9\\]+\" 3 } } */\n+\n+/* Asm check for test_vdup_n_p16, test_vdup_n_s16, test_vdup_n_u16.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.4h, w\\[0-9\\]+\" 3 } } */\n+\n+/* Asm check for test_vdupq_n_p16, test_vdupq_n_s16, test_vdupq_n_u16.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.8h, w\\[0-9\\]+\" 3 } } */\n+\n+/* Asm check for test_vdup_n_s32, test_vdup_n_u32.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.2s, w\\[0-9\\]+\" 2 } } */\n+\n+/* Asm check for test_vdupq_n_s32, test_vdupq_n_u32.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.4s, w\\[0-9\\]+\" 2 } } */\n+\n+/* Asm check for test_vdup_n_s64, test_vdup_n_u64 are left out.\n+   Attempts to make the compiler generate \"dup\\\\td\\[0-9\\]+, x\\[0-9\\]+\"\n+   are not practical.  */\n+\n+/* Asm check for test_vdupq_n_s64, test_vdupq_n_u64.  */\n+/* { dg-final { scan-assembler-times \"dup\\\\tv\\[0-9\\]+\\.2d, x\\[0-9\\]+\" 2 } } */\n+\n+/* { dg-final { cleanup-saved-temps } } */"}]}