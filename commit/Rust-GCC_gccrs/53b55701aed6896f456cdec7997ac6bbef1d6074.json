{"sha": "53b55701aed6896f456cdec7997ac6bbef1d6074", "node_id": "C_kwDOANBUbNoAKDUzYjU1NzAxYWVkNjg5NmY0NTZjZGVjNzk5N2FjNmJiZWYxZDYwNzQ", "commit": {"author": {"name": "Matthias Kretz", "email": "m.kretz@gsi.de", "date": "2023-01-14T16:07:59Z"}, "committer": {"name": "Matthias Kretz", "email": "m.kretz@gsi.de", "date": "2023-02-16T14:58:33Z"}, "message": "libstdc++: Annotate most lambdas with always_inline\n\nAll of the annotated lambdas are simply a necessary means for\nimplementing these functions and should never result in an actual\nfunction call. Many of these lambdas would go away if C++ had better\nlanguage support for packs.\n\nSigned-off-by: Matthias Kretz <m.kretz@gsi.de>\n\nlibstdc++-v3/ChangeLog:\n\n\tPR libstdc++/108030\n\t* include/experimental/bits/simd_detail.h: Define\n\t_GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA.\n\t* include/experimental/bits/simd.h: Annotate lambdas with\n\t_GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA.\n\t* include/experimental/bits/simd_builtin.h: Ditto.\n\t* include/experimental/bits/simd_converter.h: Ditto.\n\t* include/experimental/bits/simd_fixed_size.h: Ditto.\n\t* include/experimental/bits/simd_math.h: Ditto.\n\t* include/experimental/bits/simd_neon.h: Ditto.\n\t* include/experimental/bits/simd_x86.h: Ditto.", "tree": {"sha": "9e020cd1433beee586cc6bc2da6f9483428f0fcf", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/9e020cd1433beee586cc6bc2da6f9483428f0fcf"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/53b55701aed6896f456cdec7997ac6bbef1d6074", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/53b55701aed6896f456cdec7997ac6bbef1d6074", "html_url": "https://github.com/Rust-GCC/gccrs/commit/53b55701aed6896f456cdec7997ac6bbef1d6074", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/53b55701aed6896f456cdec7997ac6bbef1d6074/comments", "author": {"login": "mattkretz", "id": 3306474, "node_id": "MDQ6VXNlcjMzMDY0NzQ=", "avatar_url": "https://avatars.githubusercontent.com/u/3306474?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mattkretz", "html_url": "https://github.com/mattkretz", "followers_url": "https://api.github.com/users/mattkretz/followers", "following_url": "https://api.github.com/users/mattkretz/following{/other_user}", "gists_url": "https://api.github.com/users/mattkretz/gists{/gist_id}", "starred_url": "https://api.github.com/users/mattkretz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mattkretz/subscriptions", "organizations_url": "https://api.github.com/users/mattkretz/orgs", "repos_url": "https://api.github.com/users/mattkretz/repos", "events_url": "https://api.github.com/users/mattkretz/events{/privacy}", "received_events_url": "https://api.github.com/users/mattkretz/received_events", "type": "User", "site_admin": false}, "committer": {"login": "mattkretz", "id": 3306474, "node_id": "MDQ6VXNlcjMzMDY0NzQ=", "avatar_url": "https://avatars.githubusercontent.com/u/3306474?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mattkretz", "html_url": "https://github.com/mattkretz", "followers_url": "https://api.github.com/users/mattkretz/followers", "following_url": "https://api.github.com/users/mattkretz/following{/other_user}", "gists_url": "https://api.github.com/users/mattkretz/gists{/gist_id}", "starred_url": "https://api.github.com/users/mattkretz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mattkretz/subscriptions", "organizations_url": "https://api.github.com/users/mattkretz/orgs", "repos_url": "https://api.github.com/users/mattkretz/repos", "events_url": "https://api.github.com/users/mattkretz/events{/privacy}", "received_events_url": "https://api.github.com/users/mattkretz/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "fea34ee491104f325682cc5fb75683b7d74a0a3b", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fea34ee491104f325682cc5fb75683b7d74a0a3b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/fea34ee491104f325682cc5fb75683b7d74a0a3b"}], "stats": {"total": 1068, "additions": 575, "deletions": 493}, "files": [{"sha": "ffe72fa6ccf9e13a5affaa026cae7f4be06a4771", "filename": "libstdc++-v3/include/experimental/bits/simd.h", "status": "modified", "additions": 129, "deletions": 110, "changes": 239, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd.h?ref=53b55701aed6896f456cdec7997ac6bbef1d6074", "patch": "@@ -609,28 +609,34 @@ template <size_t _Bytes>\n \t  operator&(_Ip __rhs) const\n \t  {\n \t    return __generate_from_n_evaluations<_Np, _Ip>(\n-\t      [&](auto __i) { return __rhs._M_data[__i] & _M_data[__i]; });\n+\t      [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\treturn __rhs._M_data[__i] & _M_data[__i];\n+\t      });\n \t  }\n \n \t  _GLIBCXX_SIMD_INTRINSIC constexpr _Ip\n \t  operator|(_Ip __rhs) const\n \t  {\n \t    return __generate_from_n_evaluations<_Np, _Ip>(\n-\t      [&](auto __i) { return __rhs._M_data[__i] | _M_data[__i]; });\n+\t      [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\treturn __rhs._M_data[__i] | _M_data[__i];\n+\t      });\n \t  }\n \n \t  _GLIBCXX_SIMD_INTRINSIC constexpr _Ip\n \t  operator^(_Ip __rhs) const\n \t  {\n \t    return __generate_from_n_evaluations<_Np, _Ip>(\n-\t      [&](auto __i) { return __rhs._M_data[__i] ^ _M_data[__i]; });\n+\t      [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\treturn __rhs._M_data[__i] ^ _M_data[__i];\n+\t      });\n \t  }\n \n \t  _GLIBCXX_SIMD_INTRINSIC constexpr _Ip\n \t  operator~() const\n \t  {\n \t    return __generate_from_n_evaluations<_Np, _Ip>(\n-\t      [&](auto __i) { return ~_M_data[__i]; });\n+\t      [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { return ~_M_data[__i]; });\n \t  }\n \t};\n \treturn _Ip{};\n@@ -1391,23 +1397,23 @@ template <size_t _Np, bool _Sanitized>\n     operator^=(const _BitMask& __b) & noexcept\n     {\n       __execute_n_times<_S_array_size>(\n-\t[&](auto __i) { _M_bits[__i] ^= __b._M_bits[__i]; });\n+\t[&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { _M_bits[__i] ^= __b._M_bits[__i]; });\n       return *this;\n     }\n \n     constexpr _BitMask&\n     operator|=(const _BitMask& __b) & noexcept\n     {\n       __execute_n_times<_S_array_size>(\n-\t[&](auto __i) { _M_bits[__i] |= __b._M_bits[__i]; });\n+\t[&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { _M_bits[__i] |= __b._M_bits[__i]; });\n       return *this;\n     }\n \n     constexpr _BitMask&\n     operator&=(const _BitMask& __b) & noexcept\n     {\n       __execute_n_times<_S_array_size>(\n-\t[&](auto __i) { _M_bits[__i] &= __b._M_bits[__i]; });\n+\t[&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { _M_bits[__i] &= __b._M_bits[__i]; });\n       return *this;\n     }\n \n@@ -1797,8 +1803,9 @@ template <size_t _Np, typename _Tp>\n   __vector_broadcast(_Tp __x)\n   {\n     return __call_with_n_evaluations<_Np>(\n-      [](auto... __xx) { return __vector_type_t<_Tp, _Np>{__xx...}; },\n-      [&__x](int) { return __x; });\n+      [](auto... __xx) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\treturn __vector_type_t<_Tp, _Np>{__xx...};\n+      }, [&__x](int) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { return __x; });\n   }\n \n // }}}\n@@ -2205,7 +2212,7 @@ template <int _Offset,\n #endif\n \tconstexpr int _O = _Offset * __return_width;\n \treturn __call_with_subscripts<__return_width, _O>(\n-\t  __x, [](auto... __entries) {\n+\t  __x, [](auto... __entries) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t    return reinterpret_cast<_R>(_Up{__entries...});\n \t  });\n       }\n@@ -2607,7 +2614,7 @@ template <typename _Tp, size_t _Width>\n \n     _GLIBCXX_SIMD_INTRINSIC constexpr _SimdWrapper(initializer_list<_Tp> __init)\n       : _Base(__generate_from_n_evaluations<_Width, _BuiltinType>(\n-\t[&](auto __i) { return __init.begin()[__i.value]; })) {}\n+\t[&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { return __init.begin()[__i.value]; })) {}\n \n     _GLIBCXX_SIMD_INTRINSIC constexpr _SimdWrapper() = default;\n     _GLIBCXX_SIMD_INTRINSIC constexpr _SimdWrapper(const _SimdWrapper&)\n@@ -2632,10 +2639,9 @@ template <typename _Tp, size_t _Width>\n       _GLIBCXX_SIMD_INTRINSIC constexpr\n       operator _SimdTuple<_Tp, _As...>() const\n       {\n-\tconst auto& dd = _M_data; // workaround for GCC7 ICE\n-\treturn __generate_from_n_evaluations<sizeof...(_As),\n-\t\t\t\t\t     _SimdTuple<_Tp, _As...>>([&](\n-\t  auto __i) constexpr { return dd[int(__i)]; });\n+\treturn __generate_from_n_evaluations<sizeof...(_As), _SimdTuple<_Tp, _As...>>(\n+\t\t [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA\n+\t\t { return _M_data[int(__i)]; });\n       }\n \n     _GLIBCXX_SIMD_INTRINSIC constexpr operator const _BuiltinType&() const\n@@ -3192,21 +3198,19 @@ template <typename _Tp, int _Np>\n   { return __x; }\n \n template <typename _Tp, typename _Ap>\n-  _GLIBCXX_SIMD_INTRINSIC auto\n+  _GLIBCXX_SIMD_INTRINSIC fixed_size_simd<_Tp, simd_size_v<_Tp, _Ap>>\n   to_fixed_size(const simd<_Tp, _Ap>& __x)\n   {\n-    return simd<_Tp, simd_abi::fixed_size<simd_size_v<_Tp, _Ap>>>([&__x](\n-      auto __i) constexpr { return __x[__i]; });\n+    using _Rp = fixed_size_simd<_Tp, simd_size_v<_Tp, _Ap>>;\n+    return _Rp([&__x](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { return __x[__i]; });\n   }\n \n template <typename _Tp, typename _Ap>\n-  _GLIBCXX_SIMD_INTRINSIC auto\n+  _GLIBCXX_SIMD_INTRINSIC fixed_size_simd_mask<_Tp, simd_size_v<_Tp, _Ap>>\n   to_fixed_size(const simd_mask<_Tp, _Ap>& __x)\n   {\n-    constexpr int _Np = simd_mask<_Tp, _Ap>::size();\n-    fixed_size_simd_mask<_Tp, _Np> __r;\n-    __execute_n_times<_Np>([&](auto __i) constexpr { __r[__i] = __x[__i]; });\n-    return __r;\n+    return {__private_init,\n+\t    [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { return __x[__i]; }};\n   }\n \n // to_native {{{2\n@@ -3225,7 +3229,9 @@ template <typename _Tp, size_t _Np>\n   enable_if_t<(_Np == native_simd_mask<_Tp>::size()), native_simd_mask<_Tp>>\n   to_native(const fixed_size_simd_mask<_Tp, _Np>& __x)\n   {\n-    return native_simd_mask<_Tp>([&](auto __i) constexpr { return __x[__i]; });\n+    return native_simd_mask<_Tp>(\n+\t     __private_init,\n+\t     [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { return __x[__i]; });\n   }\n \n // to_compatible {{{2\n@@ -3242,7 +3248,10 @@ template <typename _Tp, size_t _Np>\n   _GLIBCXX_SIMD_INTRINSIC\n   enable_if_t<(_Np == simd_mask<_Tp>::size()), simd_mask<_Tp>>\n   to_compatible(const simd_mask<_Tp, simd_abi::fixed_size<_Np>>& __x)\n-  { return simd_mask<_Tp>([&](auto __i) constexpr { return __x[__i]; }); }\n+  {\n+    return simd_mask<_Tp>(\n+\t     [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { return __x[__i]; });\n+  }\n \n // masked assignment [simd_mask.where] {{{1\n \n@@ -3400,9 +3409,9 @@ template <typename _M, typename _Tp>\n       _Impl::template _S_masked_cassign(                                       \\\n \t__data(_M_k), __data(_M_value),                                        \\\n \t__to_value_type_or_member_type<_Tp>(static_cast<_Up&&>(__x)),          \\\n-\t[](auto __impl, auto __lhs, auto __rhs) constexpr {                    \\\n-\treturn __impl.__name(__lhs, __rhs);                                    \\\n-\t});                                                                    \\\n+\t[](auto __impl, auto __lhs, auto __rhs)                                \\\n+\t  constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA                         \\\n+\t{ return __impl.__name(__lhs, __rhs); });                              \\\n     }                                                                          \\\n   static_assert(true)\n     _GLIBCXX_SIMD_OP_(+, _S_plus);\n@@ -3899,12 +3908,11 @@ template <typename _V, typename _Ap,\n       }\n     else if (__x._M_is_constprop())\n       {\n-\treturn __generate_from_n_evaluations<Parts, array<_V, Parts>>([&](\n-\t  auto __i) constexpr {\n-\t  return _V([&](auto __j) constexpr {\n-\t    return __x[__i * _V::size() + __j];\n-\t  });\n-\t});\n+\treturn __generate_from_n_evaluations<Parts, array<_V, Parts>>(\n+\t\t [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   return _V([&](auto __j) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA\n+\t\t\t     { return __x[__i * _V::size() + __j]; });\n+\t\t });\n       }\n     else if constexpr (\n       __is_fixed_size_abi_v<_Ap>\n@@ -3917,41 +3925,40 @@ template <typename _V, typename _Ap,\n #ifdef _GLIBCXX_SIMD_USE_ALIASING_LOADS\n       const __may_alias<_Tp>* const __element_ptr\n \t= reinterpret_cast<const __may_alias<_Tp>*>(&__data(__x));\n-      return __generate_from_n_evaluations<Parts, array<_V, Parts>>([&](\n-\tauto __i) constexpr {\n-\treturn _V(__element_ptr + __i * _V::size(), vector_aligned);\n-      });\n+      return __generate_from_n_evaluations<Parts, array<_V, Parts>>(\n+\t       [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA\n+\t       { return _V(__element_ptr + __i * _V::size(), vector_aligned); });\n #else\n       const auto& __xx = __data(__x);\n-      return __generate_from_n_evaluations<Parts, array<_V, Parts>>([&](\n-\tauto __i) constexpr {\n-\t[[maybe_unused]] constexpr size_t __offset\n-\t  = decltype(__i)::value * _V::size();\n-\treturn _V([&](auto __j) constexpr {\n-\t  constexpr _SizeConstant<__j + __offset> __k;\n-\t  return __xx[__k];\n-\t});\n-      });\n+      return __generate_from_n_evaluations<Parts, array<_V, Parts>>(\n+\t       [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t [[maybe_unused]] constexpr size_t __offset\n+\t\t   = decltype(__i)::value * _V::size();\n+\t\t return _V([&](auto __j) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t  constexpr _SizeConstant<__j + __offset> __k;\n+\t\t\t  return __xx[__k];\n+\t\t\t});\n+\t       });\n #endif\n     }\n   else if constexpr (is_same_v<typename _V::abi_type, simd_abi::scalar>)\n     {\n       // normally memcpy should work here as well\n-      return __generate_from_n_evaluations<Parts, array<_V, Parts>>([&](\n-\tauto __i) constexpr { return __x[__i]; });\n+      return __generate_from_n_evaluations<Parts, array<_V, Parts>>(\n+\t       [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { return __x[__i]; });\n     }\n   else\n     {\n-      return __generate_from_n_evaluations<Parts, array<_V, Parts>>([&](\n-\tauto __i) constexpr {\n-\tif constexpr (__is_fixed_size_abi_v<typename _V::abi_type>)\n-\t  return _V([&](auto __j) constexpr {\n-\t    return __x[__i * _V::size() + __j];\n-\t  });\n-\telse\n-\t  return _V(__private_init,\n-\t\t    __extract_part<decltype(__i)::value, Parts>(__data(__x)));\n-      });\n+      return __generate_from_n_evaluations<Parts, array<_V, Parts>>(\n+\t       [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t if constexpr (__is_fixed_size_abi_v<typename _V::abi_type>)\n+\t\t   return _V([&](auto __j) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t    return __x[__i * _V::size() + __j];\n+\t\t\t  });\n+\t\t else\n+\t\t   return _V(__private_init,\n+\t\t\t     __extract_part<decltype(__i)::value, Parts>(__data(__x)));\n+\t       });\n     }\n   }\n \n@@ -3975,22 +3982,22 @@ template <typename _V, typename _Ap,\n     else if constexpr (_V::size() <= __CHAR_BIT__ * sizeof(_ULLong))\n       {\n \tconst bitset __bits = __x.__to_bitset();\n-\treturn __generate_from_n_evaluations<_Parts, array<_V, _Parts>>([&](\n-\t  auto __i) constexpr {\n-\t  constexpr size_t __offset = __i * _V::size();\n-\t  return _V(__bitset_init, (__bits >> __offset).to_ullong());\n-\t});\n+\treturn __generate_from_n_evaluations<_Parts, array<_V, _Parts>>(\n+\t\t [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   constexpr size_t __offset = __i * _V::size();\n+\t\t   return _V(__bitset_init, (__bits >> __offset).to_ullong());\n+\t\t });\n       }\n     else\n       {\n-\treturn __generate_from_n_evaluations<_Parts, array<_V, _Parts>>([&](\n-\t  auto __i) constexpr {\n-\t  constexpr size_t __offset = __i * _V::size();\n-\t  return _V(\n-\t    __private_init, [&](auto __j) constexpr {\n-\t      return __x[__j + __offset];\n-\t    });\n-\t});\n+\treturn __generate_from_n_evaluations<_Parts, array<_V, _Parts>>(\n+\t\t [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   constexpr size_t __offset = __i * _V::size();\n+\t\t   return _V(__private_init,\n+\t\t\t     [&](auto __j) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t       return __x[__j + __offset];\n+\t\t\t     });\n+\t\t });\n       }\n   }\n \n@@ -4008,12 +4015,14 @@ template <size_t... _Sizes, typename _Tp, typename _Ap, typename>\n     using _V = __deduced_simd<_Tp, _N0>;\n \n     if (__x._M_is_constprop())\n-      return __generate_from_n_evaluations<sizeof...(_Sizes), _Tuple>([&](\n-\tauto __i) constexpr {\n-\tusing _Vi = __deduced_simd<_Tp, _SL::_S_at(__i)>;\n-\tconstexpr size_t __offset = _SL::_S_before(__i);\n-\treturn _Vi([&](auto __j) constexpr { return __x[__offset + __j]; });\n-      });\n+      return __generate_from_n_evaluations<sizeof...(_Sizes), _Tuple>(\n+\t       [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t using _Vi = __deduced_simd<_Tp, _SL::_S_at(__i)>;\n+\t\t constexpr size_t __offset = _SL::_S_before(__i);\n+\t\t return _Vi([&](auto __j) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t  return __x[__offset + __j];\n+\t\t\t});\n+\t       });\n     else if constexpr (_Np == _N0)\n       {\n \tstatic_assert(sizeof...(_Sizes) == 1);\n@@ -4080,28 +4089,28 @@ template <size_t... _Sizes, typename _Tp, typename _Ap, typename>\n #ifdef _GLIBCXX_SIMD_USE_ALIASING_LOADS\n     const __may_alias<_Tp>* const __element_ptr\n       = reinterpret_cast<const __may_alias<_Tp>*>(&__x);\n-    return __generate_from_n_evaluations<sizeof...(_Sizes), _Tuple>([&](\n-      auto __i) constexpr {\n-      using _Vi = __deduced_simd<_Tp, _SL::_S_at(__i)>;\n-      constexpr size_t __offset = _SL::_S_before(__i);\n-      constexpr size_t __base_align = alignof(simd<_Tp, _Ap>);\n-      constexpr size_t __a\n-\t= __base_align - ((__offset * sizeof(_Tp)) % __base_align);\n-      constexpr size_t __b = ((__a - 1) & __a) ^ __a;\n-      constexpr size_t __alignment = __b == 0 ? __a : __b;\n-      return _Vi(__element_ptr + __offset, overaligned<__alignment>);\n-    });\n+    return __generate_from_n_evaluations<sizeof...(_Sizes), _Tuple>(\n+\t     [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t       using _Vi = __deduced_simd<_Tp, _SL::_S_at(__i)>;\n+\t       constexpr size_t __offset = _SL::_S_before(__i);\n+\t       constexpr size_t __base_align = alignof(simd<_Tp, _Ap>);\n+\t       constexpr size_t __a\n+\t\t = __base_align - ((__offset * sizeof(_Tp)) % __base_align);\n+\t       constexpr size_t __b = ((__a - 1) & __a) ^ __a;\n+\t       constexpr size_t __alignment = __b == 0 ? __a : __b;\n+\t       return _Vi(__element_ptr + __offset, overaligned<__alignment>);\n+\t     });\n #else\n-    return __generate_from_n_evaluations<sizeof...(_Sizes), _Tuple>([&](\n-      auto __i) constexpr {\n-      using _Vi = __deduced_simd<_Tp, _SL::_S_at(__i)>;\n-      const auto& __xx = __data(__x);\n-      using _Offset = decltype(_SL::_S_before(__i));\n-      return _Vi([&](auto __j) constexpr {\n-\tconstexpr _SizeConstant<_Offset::value + __j> __k;\n-\treturn __xx[__k];\n-      });\n-    });\n+    return __generate_from_n_evaluations<sizeof...(_Sizes), _Tuple>(\n+\t     [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t       using _Vi = __deduced_simd<_Tp, _SL::_S_at(__i)>;\n+\t       const auto& __xx = __data(__x);\n+\t       using _Offset = decltype(_SL::_S_before(__i));\n+\t       return _Vi([&](auto __j) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\tconstexpr _SizeConstant<_Offset::value + __j> __k;\n+\t\t\treturn __xx[__k];\n+\t\t      });\n+\t     });\n #endif\n   }\n \n@@ -4143,8 +4152,9 @@ template <typename _Tp, typename... _As, typename = __detail::__odr_helper>\n       return simd_cast<_Rp>(__xs...);\n     else if ((... && __xs._M_is_constprop()))\n       return simd<_Tp,\n-\t\t  simd_abi::deduce_t<_Tp, (simd_size_v<_Tp, _As> + ...)>>([&](\n-\tauto __i) constexpr { return __subscript_in_pack<__i>(__xs...); });\n+\t\t  simd_abi::deduce_t<_Tp, (simd_size_v<_Tp, _As> + ...)>>(\n+\t       [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA\n+\t       { return __subscript_in_pack<__i>(__xs...); });\n     else\n       {\n \t_Rp __r{};\n@@ -4160,9 +4170,10 @@ template <typename _Tp, typename _Abi, size_t _Np>\n   _GLIBCXX_SIMD_CONSTEXPR __deduced_simd<_Tp, simd_size_v<_Tp, _Abi> * _Np>\n   concat(const array<simd<_Tp, _Abi>, _Np>& __x)\n   {\n-    return __call_with_subscripts<_Np>(__x, [](const auto&... __xs) {\n-      return concat(__xs...);\n-    });\n+    return __call_with_subscripts<_Np>(\n+\t     __x, [](const auto&... __xs) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t       return concat(__xs...);\n+\t     });\n   }\n \n // }}}\n@@ -4695,7 +4706,7 @@ template <typename _Tp, typename _Abi>\n       simd_mask(_PrivateInit, _Fp&& __gen)\n       : _M_data()\n       {\n-\t__execute_n_times<size()>([&](auto __i) constexpr {\n+\t__execute_n_times<size()>([&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t  _Impl::_S_set(_M_data, __i, __gen(__i));\n \t});\n       }\n@@ -4881,7 +4892,9 @@ template <typename _Tp, typename _Abi>\n     if (__builtin_is_constant_evaluated() || __k._M_is_constprop())\n       {\n \tconst int __r = __call_with_subscripts<simd_size_v<_Tp, _Abi>>(\n-\t  __k, [](auto... __elements) { return ((__elements != 0) + ...); });\n+\t\t\t  __k, [](auto... __elements) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t    return ((__elements != 0) + ...);\n+\t\t\t  });\n \tif (__builtin_is_constant_evaluated() || __builtin_constant_p(__r))\n \t  return __r;\n       }\n@@ -4896,8 +4909,11 @@ template <typename _Tp, typename _Abi>\n       {\n \tconstexpr size_t _Np = simd_size_v<_Tp, _Abi>;\n \tconst size_t _Idx = __call_with_n_evaluations<_Np>(\n-\t  [](auto... __indexes) { return std::min({__indexes...}); },\n-\t  [&](auto __i) { return __k[__i] ? +__i : _Np; });\n+\t\t\t      [](auto... __indexes) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t\treturn std::min({__indexes...});\n+\t\t\t      }, [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t\treturn __k[__i] ? +__i : _Np;\n+\t\t\t      });\n \tif (_Idx >= _Np)\n \t  __invoke_ub(\"find_first_set(empty mask) is UB\");\n \tif (__builtin_constant_p(_Idx))\n@@ -4914,8 +4930,11 @@ template <typename _Tp, typename _Abi>\n       {\n \tconstexpr size_t _Np = simd_size_v<_Tp, _Abi>;\n \tconst int _Idx = __call_with_n_evaluations<_Np>(\n-\t  [](auto... __indexes) { return std::max({__indexes...}); },\n-\t  [&](auto __i) { return __k[__i] ? int(__i) : -1; });\n+\t\t\t   [](auto... __indexes) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t     return std::max({__indexes...});\n+\t\t\t   }, [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t     return __k[__i] ? int(__i) : -1;\n+\t\t\t   });\n \tif (_Idx < 0)\n \t  __invoke_ub(\"find_first_set(empty mask) is UB\");\n \tif (__builtin_constant_p(_Idx))"}, {"sha": "792439a81bf20db8032e24dadb18ceb06becc494", "filename": "libstdc++-v3/include/experimental/bits/simd_builtin.h", "status": "modified", "additions": 189, "deletions": 162, "changes": 351, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_builtin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_builtin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_builtin.h?ref=53b55701aed6896f456cdec7997ac6bbef1d6074", "patch": "@@ -194,8 +194,11 @@ template <unsigned __shift, typename _Tp, typename _TVT = _VectorTraits<_Tp>>\n \tusing _Up = decltype(__w);\n \treturn __intrin_bitcast<_Tp>(\n \t  __call_with_n_evaluations<(sizeof(_Tp) - __shift) / __chunksize>(\n-\t    [](auto... __chunks) { return _Up{__chunks...}; },\n-\t    [&](auto __i) { return __w[__shift / __chunksize + __i]; }));\n+\t    [](auto... __chunks) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t      return _Up{__chunks...};\n+\t    }, [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t      return __w[__shift / __chunksize + __i];\n+\t    }));\n       }\n   }\n \n@@ -225,7 +228,9 @@ template <int _Index, int _Total, int _Combine, typename _Tp, size_t _Np>\n \t// by _Total\");\n \tif (__x._M_is_constprop())\n \t  return __generate_from_n_evaluations<__return_size, _R>(\n-\t    [&](auto __i) { return __x[__values_to_skip + __i]; });\n+\t    [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t      return __x[__values_to_skip + __i];\n+\t    });\n \tif constexpr (_Index == 0 && _Total == 1)\n \t  return __x;\n \telse if constexpr (_Index == 0)\n@@ -570,7 +575,9 @@ template <typename _To,\n \tconstexpr auto _Np\n \t  = _NParts == 0 ? _FromVT::_S_partial_width - _Offset : _NParts;\n \treturn __generate_from_n_evaluations<_Np, array<_To, _Np>>(\n-\t  [&](auto __i) { return static_cast<_To>(__v[__i + _Offset]); });\n+\t\t [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   return static_cast<_To>(__v[__i + _Offset]);\n+\t\t });\n       }\n     else\n       {\n@@ -611,13 +618,14 @@ template <typename _To,\n \t      return __vector_bitcast<_FromT, decltype(__n)::value>(__vv);\n \t    };\n \t    [[maybe_unused]] const auto __vi = __to_intrin(__v);\n-\t    auto&& __make_array = [](auto __x0, [[maybe_unused]] auto __x1) {\n-\t      if constexpr (_Np == 1)\n-\t\treturn _R{__intrin_bitcast<_To>(__x0)};\n-\t      else\n-\t\treturn _R{__intrin_bitcast<_To>(__x0),\n-\t\t\t  __intrin_bitcast<_To>(__x1)};\n-\t    };\n+\t    auto&& __make_array\n+\t\t= [](auto __x0, [[maybe_unused]] auto __x1) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t  if constexpr (_Np == 1)\n+\t\t    return _R{__intrin_bitcast<_To>(__x0)};\n+\t\t  else\n+\t\t    return _R{__intrin_bitcast<_To>(__x0),\n+\t\t\t      __intrin_bitcast<_To>(__x1)};\n+\t\t};\n \n \t    if constexpr (_Np == 0)\n \t      return _R{};\n@@ -642,7 +650,7 @@ template <typename _To,\n \t\t      = __convert_all<__vector_type16_t<int>, _Np>(\n \t\t\t__adjust(_SizeConstant<_Np * 4>(), __v));\n \t\t    return __generate_from_n_evaluations<_Np, _R>(\n-\t\t      [&](auto __i) {\n+\t\t      [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t\treturn __vector_convert<_To>(__as_wrapper(__ints[__i]));\n \t\t      });\n \t\t  }\n@@ -687,36 +695,40 @@ template <typename _To,\n \t\t  __vector_bitcast<int>(_mm_unpacklo_epi16(__vv[1], __vv[1])),\n \t\t  __vector_bitcast<int>(_mm_unpackhi_epi16(__vv[1], __vv[1]))};\n \t\tif constexpr (sizeof(_ToT) == 4)\n-\t\t  return __generate_from_n_evaluations<_Np, _R>([&](auto __i) {\n-\t\t    return __vector_convert<_To>(\n-\t\t      _SimdWrapper<int, 4>(__vvvv[__i] >> 24));\n-\t\t  });\n+\t\t  return __generate_from_n_evaluations<_Np, _R>(\n+\t\t\t   [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t     return __vector_convert<_To>(\n+\t\t\t\t      _SimdWrapper<int, 4>(__vvvv[__i] >> 24));\n+\t\t\t   });\n \t\telse if constexpr (is_integral_v<_ToT>)\n-\t\t  return __generate_from_n_evaluations<_Np, _R>([&](auto __i) {\n-\t\t    const auto __signbits = __to_intrin(__vvvv[__i / 2] >> 31);\n-\t\t    const auto __sx32 = __to_intrin(__vvvv[__i / 2] >> 24);\n-\t\t    return __vector_bitcast<_ToT>(\n-\t\t      __i % 2 == 0 ? _mm_unpacklo_epi32(__sx32, __signbits)\n-\t\t\t\t   : _mm_unpackhi_epi32(__sx32, __signbits));\n-\t\t  });\n+\t\t  return __generate_from_n_evaluations<_Np, _R>(\n+\t\t\t   [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t     const auto __signbits = __to_intrin(__vvvv[__i / 2] >> 31);\n+\t\t\t     const auto __sx32 = __to_intrin(__vvvv[__i / 2] >> 24);\n+\t\t\t     return __vector_bitcast<_ToT>(\n+\t\t\t\t      __i % 2 == 0 ? _mm_unpacklo_epi32(__sx32, __signbits)\n+\t\t\t\t\t\t   : _mm_unpackhi_epi32(__sx32, __signbits));\n+\t\t\t   });\n \t\telse\n-\t\t  return __generate_from_n_evaluations<_Np, _R>([&](auto __i) {\n-\t\t    const _SimdWrapper<int, 4> __int4 = __vvvv[__i / 2] >> 24;\n-\t\t    return __vector_convert<_To>(\n-\t\t      __i % 2 == 0 ? __int4\n-\t\t\t\t   : _SimdWrapper<int, 4>(\n-\t\t\t\t     _mm_unpackhi_epi64(__to_intrin(__int4),\n-\t\t\t\t\t\t\t__to_intrin(__int4))));\n-\t\t  });\n+\t\t  return __generate_from_n_evaluations<_Np, _R>(\n+\t\t\t   [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t     const _SimdWrapper<int, 4> __int4 = __vvvv[__i / 2] >> 24;\n+\t\t\t     return __vector_convert<_To>(\n+\t\t\t\t      __i % 2 == 0 ? __int4\n+\t\t\t\t\t\t   : _SimdWrapper<int, 4>(\n+\t\t\t\t\t\t       _mm_unpackhi_epi64(__to_intrin(__int4),\n+\t\t\t\t\t\t\t\t\t  __to_intrin(__int4))));\n+\t\t\t   });\n \t      }\n \t    else if constexpr (sizeof(_FromT) == 1 && sizeof(_ToT) == 4)\n \t      {\n \t\tconst auto __shorts = __convert_all<__vector_type16_t<\n \t\t  conditional_t<is_signed_v<_FromT>, short, unsigned short>>>(\n \t\t  __adjust(_SizeConstant<(_Np + 1) / 2 * 8>(), __v));\n-\t\treturn __generate_from_n_evaluations<_Np, _R>([&](auto __i) {\n-\t\t  return __convert_all<_To>(__shorts[__i / 2])[__i % 2];\n-\t\t});\n+\t\treturn __generate_from_n_evaluations<_Np, _R>(\n+\t\t\t [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t   return __convert_all<_To>(__shorts[__i / 2])[__i % 2];\n+\t\t\t });\n \t      }\n \t    else if constexpr (sizeof(_FromT) == 2 && sizeof(_ToT) == 8\n \t\t\t       && is_signed_v<_FromT> && is_integral_v<_ToT>)\n@@ -736,9 +748,10 @@ template <typename _To,\n \t\t     __vector_bitcast<int>(\n \t\t       _mm_unpackhi_epi32(_mm_srai_epi32(__vv[1], 16),\n \t\t\t\t\t  _mm_srai_epi32(__vv[1], 31)))};\n-\t\treturn __generate_from_n_evaluations<_Np, _R>([&](auto __i) {\n-\t\t  return __vector_bitcast<_ToT>(__vvvv[__i]);\n-\t\t});\n+\t\treturn __generate_from_n_evaluations<_Np, _R>(\n+\t\t\t [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t   return __vector_bitcast<_ToT>(__vvvv[__i]);\n+\t\t\t });\n \t      }\n \t    else if constexpr (sizeof(_FromT) <= 2 && sizeof(_ToT) == 8)\n \t      {\n@@ -747,9 +760,10 @@ template <typename _To,\n \t\t    is_signed_v<_FromT> || is_floating_point_v<_ToT>, int,\n \t\t    unsigned int>>>(\n \t\t    __adjust(_SizeConstant<(_Np + 1) / 2 * 4>(), __v));\n-\t\treturn __generate_from_n_evaluations<_Np, _R>([&](auto __i) {\n-\t\t  return __convert_all<_To>(__ints[__i / 2])[__i % 2];\n-\t\t});\n+\t\treturn __generate_from_n_evaluations<_Np, _R>(\n+\t\t\t [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t   return __convert_all<_To>(__ints[__i / 2])[__i % 2];\n+\t\t\t });\n \t      }\n \t    else\n \t      __assert_unreachable<_To>();\n@@ -779,14 +793,14 @@ template <typename _To,\n \t\t__extract_part<_Offset, _FromVT::_S_partial_width,\n \t\t\t       _ToVT::_S_full_size>(__v))};\n \t    else\n-\t      return __generate_from_n_evaluations<_Np, _R>([&](\n-\t\tauto __i) constexpr {\n-\t\tauto __part\n-\t\t  = __extract_part<__i * _ToVT::_S_full_size + _Offset,\n-\t\t\t\t   _FromVT::_S_partial_width,\n-\t\t\t\t   _ToVT::_S_full_size>(__v);\n-\t\treturn __vector_convert<_To>(__part);\n-\t      });\n+\t      return __generate_from_n_evaluations<_Np, _R>(\n+\t\t       [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t auto __part\n+\t\t\t   = __extract_part<__i * _ToVT::_S_full_size + _Offset,\n+\t\t\t\t\t    _FromVT::_S_partial_width,\n+\t\t\t\t\t    _ToVT::_S_full_size>(__v);\n+\t\t\t return __vector_convert<_To>(__part);\n+\t\t       });\n \t  }\n \telse if constexpr (_Offset == 0)\n \t  return array<_To, 1>{__vector_convert<_To>(__v)};\n@@ -1017,8 +1031,9 @@ template <int _UsedBytes>\n \telse\n \t  {\n \t    constexpr auto __size = _S_size<_Tp>;\n-\t    _GLIBCXX_SIMD_USE_CONSTEXPR auto __r = __generate_vector<_UV>(\n-\t      [](auto __i) constexpr { return __i < __size ? -1 : 0; });\n+\t    _GLIBCXX_SIMD_USE_CONSTEXPR auto __r\n+\t      = __generate_vector<_UV>([](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA\n+\t\t\t\t       { return __i < __size ? -1 : 0; });\n \t    return __r;\n \t  }\n       }\n@@ -1208,7 +1223,7 @@ template <int _UsedBytes>\n \t    if constexpr (is_integral_v<typename _TVT::value_type>)\n \t      return __x\n \t\t     | __generate_vector<_Tp, _S_full_size<_Tp>>(\n-\t\t       [](auto __i) -> _Tp {\n+\t\t       [](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA -> _Tp {\n \t\t\t if (__i < _Np)\n \t\t\t   return 0;\n \t\t\t else\n@@ -1348,26 +1363,27 @@ struct _CommonImplBuiltin\n \t}\n       else\n \t{\n-\t  __execute_n_times<__div_roundup(_Np, 4)>([&](auto __i) {\n-\t    constexpr int __offset = __i * 4;\n-\t    constexpr int __remaining = _Np - __offset;\n-\t    if constexpr (__remaining > 4 && __remaining <= 7)\n-\t      {\n-\t\tconst _ULLong __bool7\n-\t\t  = (__x.template _M_extract<__offset>()._M_to_bits()\n-\t\t     * 0x40810204081ULL)\n-\t\t    & 0x0101010101010101ULL;\n-\t\t_S_store<__remaining>(__bool7, __mem + __offset);\n-\t      }\n-\t    else if constexpr (__remaining >= 4)\n-\t      {\n-\t\tint __bits = __x.template _M_extract<__offset>()._M_to_bits();\n-\t\tif constexpr (__remaining > 7)\n-\t\t  __bits &= 0xf;\n-\t\tconst int __bool4 = (__bits * 0x204081) & 0x01010101;\n-\t\t_S_store<4>(__bool4, __mem + __offset);\n-\t      }\n-\t  });\n+\t  __execute_n_times<__div_roundup(_Np, 4)>(\n+\t    [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t      constexpr int __offset = __i * 4;\n+\t      constexpr int __remaining = _Np - __offset;\n+\t      if constexpr (__remaining > 4 && __remaining <= 7)\n+\t\t{\n+\t\t  const _ULLong __bool7\n+\t\t    = (__x.template _M_extract<__offset>()._M_to_bits()\n+\t\t\t * 0x40810204081ULL)\n+\t\t\t& 0x0101010101010101ULL;\n+\t\t  _S_store<__remaining>(__bool7, __mem + __offset);\n+\t\t}\n+\t      else if constexpr (__remaining >= 4)\n+\t\t{\n+\t\t  int __bits = __x.template _M_extract<__offset>()._M_to_bits();\n+\t\t  if constexpr (__remaining > 7)\n+\t\t    __bits &= 0xf;\n+\t\t  const int __bool4 = (__bits * 0x204081) & 0x01010101;\n+\t\t  _S_store<4>(__bool4, __mem + __offset);\n+\t\t}\n+\t    });\n \t}\n     }\n \n@@ -1434,13 +1450,13 @@ template <typename _Abi, typename>\n       inline static constexpr _SimdMember<_Tp> _S_generator(_Fp&& __gen,\n \t\t\t\t\t\t\t    _TypeTag<_Tp>)\n       {\n-\treturn __generate_vector<_Tp, _S_full_size<_Tp>>([&](\n-\t  auto __i) constexpr {\n-\t  if constexpr (__i < _S_size<_Tp>)\n-\t    return __gen(__i);\n-\t  else\n-\t    return 0;\n-\t});\n+\treturn __generate_vector<_Tp, _S_full_size<_Tp>>(\n+\t\t [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   if constexpr (__i < _S_size<_Tp>)\n+\t\t     return __gen(__i);\n+\t\t   else\n+\t\t     return 0;\n+\t\t });\n       }\n \n     // _S_load {{{2\n@@ -1455,10 +1471,10 @@ template <typename _Abi, typename>\n \t\t\t\t\t\t\t\t      : 16;\n \tconstexpr size_t __bytes_to_load = sizeof(_Up) * _Np;\n \tif constexpr (sizeof(_Up) > 8)\n-\t  return __generate_vector<_Tp, _SimdMember<_Tp>::_S_full_size>([&](\n-\t    auto __i) constexpr {\n-\t    return static_cast<_Tp>(__i < _Np ? __mem[__i] : 0);\n-\t  });\n+\t  return __generate_vector<_Tp, _SimdMember<_Tp>::_S_full_size>(\n+\t\t   [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t     return static_cast<_Tp>(__i < _Np ? __mem[__i] : 0);\n+\t\t   });\n \telse if constexpr (is_same_v<_Up, _Tp>)\n \t  return _CommonImpl::template _S_load<_Tp, _S_full_size<_Tp>,\n \t\t\t\t\t       _Np * sizeof(_Tp)>(__mem);\n@@ -1470,13 +1486,12 @@ template <typename _Abi, typename>\n \t    constexpr size_t __n_loads = __bytes_to_load / __max_load_size;\n \t    constexpr size_t __elements_per_load = _Np / __n_loads;\n \t    return __call_with_n_evaluations<__n_loads>(\n-\t      [](auto... __uncvted) {\n-\t\treturn __convert<_SimdMember<_Tp>>(__uncvted...);\n-\t      },\n-\t      [&](auto __i) {\n-\t\treturn _CommonImpl::template _S_load<_Up, __elements_per_load>(\n-\t\t  __mem + __i * __elements_per_load);\n-\t      });\n+\t\t     [](auto... __uncvted) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t       return __convert<_SimdMember<_Tp>>(__uncvted...);\n+\t\t     }, [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t       return _CommonImpl::template _S_load<_Up, __elements_per_load>(\n+\t\t\t\t\t\t      __mem + __i * __elements_per_load);\n+\t\t     });\n \t  }\n \telse if constexpr (__bytes_to_load % (__max_load_size / 2) == 0\n \t\t\t   && __max_load_size > 16)\n@@ -1485,20 +1500,19 @@ template <typename _Abi, typename>\n \t      = __bytes_to_load / (__max_load_size / 2);\n \t    constexpr size_t __elements_per_load = _Np / __n_loads;\n \t    return __call_with_n_evaluations<__n_loads>(\n-\t      [](auto... __uncvted) {\n-\t\treturn __convert<_SimdMember<_Tp>>(__uncvted...);\n-\t      },\n-\t      [&](auto __i) {\n-\t\treturn _CommonImpl::template _S_load<_Up, __elements_per_load>(\n-\t\t  __mem + __i * __elements_per_load);\n-\t      });\n+\t\t     [](auto... __uncvted) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t       return __convert<_SimdMember<_Tp>>(__uncvted...);\n+\t\t     }, [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t       return _CommonImpl::template _S_load<_Up, __elements_per_load>(\n+\t\t\t\t\t\t      __mem + __i * __elements_per_load);\n+\t\t     });\n \t  }\n \telse // e.g. int[] -> <char, 9>\n \t  return __call_with_subscripts(\n-\t    __mem, make_index_sequence<_Np>(), [](auto... __args) {\n-\t      return __vector_type_t<_Tp, _S_full_size<_Tp>>{\n-\t\tstatic_cast<_Tp>(__args)...};\n-\t    });\n+\t    __mem, make_index_sequence<_Np>(),\n+\t\t   [](auto... __args) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t     return __vector_type_t<_Tp, _S_full_size<_Tp>>{static_cast<_Tp>(__args)...};\n+\t\t   });\n       }\n \n     // _S_masked_load {{{2\n@@ -1507,9 +1521,10 @@ template <typename _Abi, typename>\n       _S_masked_load(_SimdWrapper<_Tp, _Np> __merge, _MaskMember<_Tp> __k,\n \t\t     const _Up* __mem) noexcept\n       {\n-\t_BitOps::_S_bit_iteration(_MaskImpl::_S_to_bits(__k), [&](auto __i) {\n-\t  __merge._M_set(__i, static_cast<_Tp>(__mem[__i]));\n-\t});\n+\t_BitOps::_S_bit_iteration(_MaskImpl::_S_to_bits(__k),\n+\t\t\t\t  [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t\t    __merge._M_set(__i, static_cast<_Tp>(__mem[__i]));\n+\t\t\t\t  });\n \treturn __merge;\n       }\n \n@@ -1523,7 +1538,7 @@ template <typename _Abi, typename>\n \tconstexpr size_t __max_store_size\n \t  = _SuperImpl::template _S_max_store_size<_Up>;\n \tif constexpr (sizeof(_Up) > 8)\n-\t  __execute_n_times<_Np>([&](auto __i) constexpr {\n+\t  __execute_n_times<_Np>([&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t    __mem[__i] = __v[__i];\n \t  });\n \telse if constexpr (is_same_v<_Up, _Tp>)\n@@ -1540,9 +1555,10 @@ template <typename _Abi, typename>\n \t    using _V = __vector_type_t<_Up, __vsize>;\n \t    const array<_V, __stores> __converted\n \t      = __convert_all<_V, __stores>(__v);\n-\t    __execute_n_times<__full_stores>([&](auto __i) constexpr {\n-\t      _CommonImpl::_S_store(__converted[__i], __mem + __i * __vsize);\n-\t    });\n+\t    __execute_n_times<__full_stores>(\n+\t      [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t_CommonImpl::_S_store(__converted[__i], __mem + __i * __vsize);\n+\t      });\n \t    if constexpr (__full_stores < __stores)\n \t      _CommonImpl::template _S_store<(_Np - __full_stores * __vsize)\n \t\t\t\t\t     * sizeof(_Up)>(\n@@ -1557,7 +1573,8 @@ template <typename _Abi, typename>\n \t\t\t    _MaskMember<_Tp> __k)\n       {\n \t_BitOps::_S_bit_iteration(\n-\t  _MaskImpl::_S_to_bits(__k), [&](auto __i) constexpr {\n+\t  _MaskImpl::_S_to_bits(__k),\n+\t  [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t    __mem[__i] = __v[__i];\n \t  });\n       }\n@@ -1579,7 +1596,7 @@ template <typename _Abi, typename>\n \t    _Up> || (is_integral_v<_Tp> && is_integral_v<_Up> && sizeof(_Tp) == sizeof(_Up)))\n \t  {\n \t    // bitwise or no conversion, reinterpret:\n-\t    const _MaskMember<_Up> __kk = [&]() {\n+\t    const _MaskMember<_Up> __kk = [&]() _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t      if constexpr (__is_bitmask_v<decltype(__k)>)\n \t\treturn _MaskMember<_Up>(__k._M_data);\n \t      else\n@@ -1618,7 +1635,7 @@ template <typename _Abi, typename>\n \t\tconstexpr size_t _NParts = _S_full_size<_Tp> / _UW_size;\n \t\tconst array<_UV, _NAllStores> __converted\n \t\t  = __convert_all<_UV, _NAllStores>(__v);\n-\t\t__execute_n_times<_NFullStores>([&](auto __i) {\n+\t\t__execute_n_times<_NFullStores>([&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t  _SuperImpl::_S_masked_store_nocvt(\n \t\t    _UW(__converted[__i]), __mem + __i * _UW_size,\n \t\t    _UAbi::_MaskImpl::template _S_convert<\n@@ -1637,10 +1654,10 @@ template <typename _Abi, typename>\n \t      }\n \t  }\n \telse\n-\t  _BitOps::_S_bit_iteration(\n-\t    _MaskImpl::_S_to_bits(__k), [&](auto __i) constexpr {\n-\t      __mem[__i] = static_cast<_Up>(__v[__i]);\n-\t    });\n+\t  _BitOps::_S_bit_iteration(_MaskImpl::_S_to_bits(__k),\n+\t\t\t\t    [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t\t      __mem[__i] = static_cast<_Up>(__v[__i]);\n+\t\t\t\t    });\n       }\n \n     // _S_complement {{{2\n@@ -1932,7 +1949,9 @@ template <typename _Abi, typename>\n       static _Tp _S_##__name(const _Tp& __x, const _More&... __more)           \\\n       {                                                                        \\\n \treturn __generate_vector<_Tp>(                                         \\\n-\t  [&](auto __i) { return __name(__x[__i], __more[__i]...); });         \\\n+\t\t [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {            \\\n+\t\t   return __name(__x[__i], __more[__i]...);                    \\\n+\t\t });                                                           \\\n       }\n \n #define _GLIBCXX_SIMD_MATH_FALLBACK_MASKRET(__name)                            \\\n@@ -1941,23 +1960,25 @@ template <typename _Abi, typename>\n \t\t\t\t\t\t const _More&... __more)       \\\n       {                                                                        \\\n \treturn __generate_vector<_Tp>(                                         \\\n-\t  [&](auto __i) { return __name(__x[__i], __more[__i]...); });         \\\n-      }\n-\n-#define _GLIBCXX_SIMD_MATH_FALLBACK_FIXEDRET(_RetTp, __name)                   \\\n-    template <typename _Tp, typename... _More>                                 \\\n-      static auto _S_##__name(const _Tp& __x, const _More&... __more)          \\\n-      {                                                                        \\\n-\treturn __fixed_size_storage_t<_RetTp,                                  \\\n-\t\t\t\t      _VectorTraits<_Tp>::_S_partial_width>::  \\\n-\t  _S_generate([&](auto __meta) constexpr {                             \\\n-\t    return __meta._S_generator(                                        \\\n-\t      [&](auto __i) {                                                  \\\n-\t\treturn __name(__x[__meta._S_offset + __i],                     \\\n-\t\t\t      __more[__meta._S_offset + __i]...);              \\\n-\t      },                                                               \\\n-\t      static_cast<_RetTp*>(nullptr));                                  \\\n-\t  });                                                                  \\\n+\t\t [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {            \\\n+\t\t   return __name(__x[__i], __more[__i]...);                    \\\n+\t\t });                                                           \\\n+      }\n+\n+#define _GLIBCXX_SIMD_MATH_FALLBACK_FIXEDRET(_RetTp, __name)                          \\\n+    template <typename _Tp, typename... _More>                                        \\\n+      static auto _S_##__name(const _Tp& __x, const _More&... __more)                 \\\n+      {                                                                               \\\n+\treturn __fixed_size_storage_t<_RetTp,                                         \\\n+\t\t\t\t      _VectorTraits<_Tp>::_S_partial_width>::         \\\n+\t  _S_generate([&](auto __meta) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { \\\n+\t    return __meta._S_generator(                                               \\\n+\t      [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {                      \\\n+\t\treturn __name(__x[__meta._S_offset + __i],                            \\\n+\t\t\t      __more[__meta._S_offset + __i]...);                     \\\n+\t      },                                                                      \\\n+\t      static_cast<_RetTp*>(nullptr));                                         \\\n+\t  });                                                                         \\\n       }\n \n     _GLIBCXX_SIMD_MATH_FALLBACK(acos)\n@@ -2010,7 +2031,7 @@ template <typename _Abi, typename>\n       _S_remquo(const _Tp __x, const _Tp __y,\n \t\t__fixed_size_storage_t<int, _TVT::_S_partial_width>* __z)\n       {\n-\treturn __generate_vector<_Tp>([&](auto __i) {\n+\treturn __generate_vector<_Tp>([&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t  int __tmp;\n \t  auto __r = remquo(__x[__i], __y[__i], &__tmp);\n \t  __z->_M_set(__i, __tmp);\n@@ -2423,7 +2444,7 @@ template <typename _Abi, typename>\n   #endif // _GLIBCXX_SIMD_X86INTRIN\n       else if constexpr (__fixed_size_storage_t<int, _Np>::_S_tuple_size == 1)\n \treturn {__call_with_subscripts<_Np>(__vector_bitcast<_LLong>(__tmp),\n-\t\t\t\t\t    [](auto... __l) {\n+\t\t\t\t\t    [](auto... __l) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t\t\t\t      return __make_wrapper<int>(__l...);\n \t\t\t\t\t    })};\n       else\n@@ -2554,13 +2575,13 @@ struct _MaskImplBuiltinMixin\n     _S_to_maskvector(_BitMask<_Np, _Sanitized> __x)\n     {\n       static_assert(is_same_v<_Up, __int_for_sizeof_t<_Up>>);\n-      return __generate_vector<__vector_type_t<_Up, _ToN>>([&](\n-\tauto __i) constexpr {\n-\tif constexpr (__i < _Np)\n-\t  return __x[__i] ? ~_Up() : _Up();\n-\telse\n-\t  return _Up();\n-      });\n+      return __generate_vector<__vector_type_t<_Up, _ToN>>(\n+\t       [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t if constexpr (__i < _Np)\n+\t\t   return __x[__i] ? ~_Up() : _Up();\n+\t\t else\n+\t\t   return _Up();\n+\t       });\n     }\n \n   template <typename _Up, size_t _UpN = 0, typename _Tp, size_t _Np,\n@@ -2601,13 +2622,13 @@ struct _MaskImplBuiltinMixin\n \t  -1, -1, -1, -1, -1>(__y); else\n \t  */\n \t  {\n-\t    return __generate_vector<__vector_type_t<_Up, _ToN>>([&](\n-\t      auto __i) constexpr {\n-\t      if constexpr (__i < _Np)\n-\t\treturn _Up(__x[__i.value]);\n-\t      else\n-\t\treturn _Up();\n-\t    });\n+\t    return __generate_vector<__vector_type_t<_Up, _ToN>>(\n+\t\t     [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t       if constexpr (__i < _Np)\n+\t\t\t return _Up(__x[__i.value]);\n+\t\t       else\n+\t\t\t return _Up();\n+\t\t     });\n \t  }\n \t}\n     }\n@@ -2625,7 +2646,9 @@ struct _MaskImplBuiltinMixin\n \t= __vector_bitcast<_Up>(__x) >> (sizeof(_Up) * __CHAR_BIT__ - 1);\n       _ULLong __r = 0;\n       __execute_n_times<_Np>(\n-\t[&](auto __i) { __r |= _ULLong(__bools[__i.value]) << __i; });\n+\t[&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t  __r |= _ULLong(__bools[__i.value]) << __i;\n+\t});\n       return __r;\n     }\n \n@@ -2677,9 +2700,10 @@ template <typename _Abi, typename>\n \t    return __bools > 0;\n \t  }\n \telse\n-\t  return __generate_vector<_I, _S_size<_Tp>>([&](auto __i) constexpr {\n-\t    return __mem[__i] ? ~_I() : _I();\n-\t  });\n+\t  return __generate_vector<_I, _S_size<_Tp>>(\n+\t\t   [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t     return __mem[__i] ? ~_I() : _I();\n+\t\t   });\n       }\n \n     // }}}\n@@ -2752,7 +2776,7 @@ template <typename _Abi, typename>\n \t// AVX(2) has 32/64 bit maskload, but nothing at 8 bit granularity\n \tauto __tmp = __wrapper_bitcast<__int_for_sizeof_t<_Tp>>(__merge);\n \t_BitOps::_S_bit_iteration(_SuperImpl::_S_to_bits(__mask),\n-\t\t\t\t  [&](auto __i) {\n+\t\t\t\t  [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t\t\t    __tmp._M_set(__i, -__mem[__i]);\n \t\t\t\t  });\n \t__merge = __wrapper_bitcast<_Tp>(__tmp);\n@@ -2764,7 +2788,7 @@ template <typename _Abi, typename>\n       _GLIBCXX_SIMD_INTRINSIC static void _S_store(_SimdWrapper<_Tp, _Np> __v,\n \t\t\t\t\t\t   bool* __mem) noexcept\n       {\n-\t__execute_n_times<_Np>([&](auto __i) constexpr {\n+\t__execute_n_times<_Np>([&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t  __mem[__i] = __v[__i];\n \t});\n       }\n@@ -2775,10 +2799,10 @@ template <typename _Abi, typename>\n       _S_masked_store(const _SimdWrapper<_Tp, _Np> __v, bool* __mem,\n \t\t      const _SimdWrapper<_Tp, _Np> __k) noexcept\n       {\n-\t_BitOps::_S_bit_iteration(\n-\t  _SuperImpl::_S_to_bits(__k), [&](auto __i) constexpr {\n-\t    __mem[__i] = __v[__i];\n-\t  });\n+\t_BitOps::_S_bit_iteration(_SuperImpl::_S_to_bits(__k),\n+\t\t\t\t  [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t\t    __mem[__i] = __v[__i];\n+\t\t\t\t  });\n       }\n \n     // _S_from_bitmask{{{2\n@@ -2845,7 +2869,7 @@ template <typename _Abi, typename>\n \t      {\n \t\t__k = __generate_from_n_evaluations<_Np,\n \t\t\t\t\t\t    __vector_type_t<_Tp, _Np>>(\n-\t\t  [&](auto __j) {\n+\t\t  [&](auto __j) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t    if (__i == __j)\n \t\t      return _Tp(-__x);\n \t\t    else\n@@ -2890,7 +2914,8 @@ template <typename _Abi, typename>\n       {\n \treturn __call_with_subscripts(\n \t  __data(__k), make_index_sequence<_S_size<_Tp>>(),\n-\t  [](const auto... __ent) constexpr { return (... && !(__ent == 0)); });\n+\t  [](const auto... __ent) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA\n+\t  { return (... && !(__ent == 0)); });\n       }\n \n     // }}}\n@@ -2901,7 +2926,8 @@ template <typename _Abi, typename>\n       {\n \treturn __call_with_subscripts(\n \t  __data(__k), make_index_sequence<_S_size<_Tp>>(),\n-\t  [](const auto... __ent) constexpr { return (... || !(__ent == 0)); });\n+\t  [](const auto... __ent) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA\n+\t  { return (... || !(__ent == 0)); });\n       }\n \n     // }}}\n@@ -2912,7 +2938,8 @@ template <typename _Abi, typename>\n       {\n \treturn __call_with_subscripts(\n \t  __data(__k), make_index_sequence<_S_size<_Tp>>(),\n-\t  [](const auto... __ent) constexpr { return (... && (__ent == 0)); });\n+\t  [](const auto... __ent) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA\n+\t  { return (... && (__ent == 0)); });\n       }\n \n     // }}}"}, {"sha": "3160e25163209df5a614c295c9d023b03a69ac40", "filename": "libstdc++-v3/include/experimental/bits/simd_converter.h", "status": "modified", "additions": 13, "deletions": 9, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_converter.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_converter.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_converter.h?ref=53b55701aed6896f456cdec7997ac6bbef1d6074", "patch": "@@ -121,7 +121,7 @@ template <typename _From, typename _To, int _Np>\n \t{\n \t  return __call_with_subscripts(\n \t    __x, make_index_sequence<_Np>(),\n-\t    [](auto... __values) constexpr->_Ret {\n+\t    [](auto... __values) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA -> _Ret {\n \t      return __make_simd_tuple<_To, decltype((void) __values,\n \t\t\t\t\t\t     simd_abi::scalar())...>(\n \t\tstatic_cast<_To>(__values)...);\n@@ -233,15 +233,17 @@ template <typename _From, typename _To, int _Np>\n \t  static_assert(_Ret::_FirstAbi::template _S_is_partial<_To>);\n \t  return _Ret{__generate_from_n_evaluations<\n \t    _Np, typename _VectorTraits<typename _Ret::_FirstType>::type>(\n-\t    [&](auto __i) { return static_cast<_To>(__x[__i]); })};\n+\t    [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t      return static_cast<_To>(__x[__i]);\n+\t    })};\n \t}\n       else\n \t{\n \t  static_assert(_Arg::_S_tuple_size > 1);\n \t  constexpr auto __n\n \t    = __div_roundup(_Ret::_S_first_size, _Arg::_S_first_size);\n \t  return __call_with_n_evaluations<__n>(\n-\t    [&__x](auto... __uncvted) {\n+\t    [&__x](auto... __uncvted) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t      // assuming _Arg Abi tags for all __i are _Arg::_FirstAbi\n \t      _SimdConverter<_From, typename _Arg::_FirstAbi, _To,\n \t\t\t     typename _Ret::_FirstAbi>\n@@ -255,8 +257,9 @@ template <typename _From, typename _To, int _Np>\n \t\t    _From, simd_abi::fixed_size<_Np - _Ret::_S_first_size>, _To,\n \t\t    simd_abi::fixed_size<_Np - _Ret::_S_first_size>>()(\n \t\t    __simd_tuple_pop_front<_Ret::_S_first_size>(__x))};\n-\t    },\n-\t    [&__x](auto __i) { return __get_tuple_at<__i>(__x); });\n+\t    }, [&__x](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t      return __get_tuple_at<__i>(__x);\n+\t    });\n \t}\n     }\n   };\n@@ -322,13 +325,14 @@ template <typename _From, int _Np, typename _To, typename _Ap>\n \treturn __vector_convert<__vector_type_t<_To, _Np>>(__x.first);\n       else if constexpr (_Arg::_S_is_homogeneous)\n \treturn __call_with_n_evaluations<_Arg::_S_tuple_size>(\n-\t  [](auto... __members) {\n+\t  [](auto... __members) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t    if constexpr ((is_convertible_v<decltype(__members), _To> && ...))\n \t      return __vector_type_t<_To, _Np>{static_cast<_To>(__members)...};\n \t    else\n \t      return __vector_convert<__vector_type_t<_To, _Np>>(__members...);\n-\t  },\n-\t  [&](auto __i) { return __get_tuple_at<__i>(__x); });\n+\t  }, [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t    return __get_tuple_at<__i>(__x);\n+\t  });\n       else if constexpr (__fixed_size_storage_t<_To, _Np>::_S_tuple_size == 1)\n \t{\n \t  _SimdConverter<_From, simd_abi::fixed_size<_Np>, _To,\n@@ -340,7 +344,7 @@ template <typename _From, int _Np, typename _To, typename _Ap>\n \t{\n \t  const _SimdWrapper<_From, _Np> __xv\n \t    = __generate_from_n_evaluations<_Np, __vector_type_t<_From, _Np>>(\n-\t      [&](auto __i) { return __x[__i]; });\n+\t\t[&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { return __x[__i]; });\n \t  return __vector_convert<__vector_type_t<_To, _Np>>(__xv);\n \t}\n     }"}, {"sha": "a0ad10efe0f6adb0604fd3db367d7867601bc7e4", "filename": "libstdc++-v3/include/experimental/bits/simd_detail.h", "status": "modified", "additions": 3, "deletions": 0, "changes": 3, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_detail.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_detail.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_detail.h?ref=53b55701aed6896f456cdec7997ac6bbef1d6074", "patch": "@@ -262,6 +262,7 @@\n #define _GLIBCXX_SIMD_INTRINSIC                                                \\\n   [[__gnu__::__always_inline__, __gnu__::__artificial__]] inline\n #define _GLIBCXX_SIMD_ALWAYS_INLINE [[__gnu__::__always_inline__]] inline\n+#define _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA __attribute__((__always_inline__))\n #define _GLIBCXX_SIMD_IS_UNLIKELY(__x) __builtin_expect(__x, 0)\n #define _GLIBCXX_SIMD_IS_LIKELY(__x) __builtin_expect(__x, 1)\n \n@@ -294,6 +295,8 @@\n #ifdef _GLIBCXX_SIMD_NO_ALWAYS_INLINE\n #undef _GLIBCXX_SIMD_ALWAYS_INLINE\n #define _GLIBCXX_SIMD_ALWAYS_INLINE inline\n+#undef _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA\n+#define _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA\n #undef _GLIBCXX_SIMD_INTRINSIC\n #define _GLIBCXX_SIMD_INTRINSIC inline\n #endif"}, {"sha": "3ac6eaa3f6bf5edfbf8301b689aa1ed9522ec9a1", "filename": "libstdc++-v3/include/experimental/bits/simd_fixed_size.h", "status": "modified", "additions": 140, "deletions": 125, "changes": 265, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_fixed_size.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_fixed_size.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_fixed_size.h?ref=53b55701aed6896f456cdec7997ac6bbef1d6074", "patch": "@@ -434,14 +434,15 @@ template <typename _Tp, typename _Abi0, typename... _Abis>\n \tif constexpr (is_same_v<_SimdTuple, __remove_cvref_t<_Tup>>)\n \t  return __tup.first;\n \telse if (__builtin_is_constant_evaluated())\n-\t  return __fixed_size_storage_t<_TupT, _S_first_size>::_S_generate([&](\n-\t    auto __meta) constexpr {\n-\t    return __meta._S_generator(\n-\t      [&](auto __i) constexpr { return __tup[__i]; },\n-\t      static_cast<_TupT*>(nullptr));\n+\t  return __fixed_size_storage_t<_TupT, _S_first_size>::_S_generate(\n+\t\t   [&](auto __meta) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t     return __meta._S_generator(\n+\t\t\t      [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t\treturn __tup[__i];\n+\t\t\t      }, static_cast<_TupT*>(nullptr));\n \t  });\n \telse\n-\t  return [&]() {\n+\t  return [&]() { // not always_inline; allow the compiler to decide\n \t    __fixed_size_storage_t<_TupT, _S_first_size> __r;\n \t    __builtin_memcpy(__r._M_as_charptr(), __tup._M_as_charptr(),\n \t\t\t     sizeof(__r));\n@@ -515,12 +516,11 @@ template <typename _Tp, typename _Abi0, typename... _Abis>\n \t\t\t negation<is_const<remove_reference_t<_More>>>>) )\n \t  {\n \t    // need to write back at least one of __more after calling __fun\n-\t    auto&& __first = [&](auto... __args) constexpr\n-\t    {\n+\t    auto&& __first = [&](auto... __args) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t      auto __r = __fun(__tuple_element_meta<_Tp, _Abi0, 0>(), first,\n \t\t\t       __args...);\n \t      [[maybe_unused]] auto&& __ignore_me = {(\n-\t\t[](auto&& __dst, const auto& __src) {\n+\t\t[](auto&& __dst, const auto& __src) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t  if constexpr (is_assignable_v<decltype(__dst),\n \t\t\t\t\t\tdecltype(__dst)>)\n \t\t    {\n@@ -530,8 +530,7 @@ template <typename _Tp, typename _Abi0, typename... _Abis>\n \t\t}(static_cast<_More&&>(__more), __args),\n \t\t0)...};\n \t      return __r;\n-\t    }\n-\t    (_M_extract_argument(__more)...);\n+\t    }(_M_extract_argument(__more)...);\n \t    if constexpr (_S_tuple_size == 1)\n \t      return {__first};\n \t    else\n@@ -776,18 +775,18 @@ template <typename _Tp, size_t _Np, typename _V, size_t _NV, typename... _VX>\n \t  sizeof...(_VX) == 0,\n \t  \"An array of scalars must be the last argument to __to_simd_tuple\");\n \treturn __call_with_subscripts(\n-\t  __from,\n-\t  make_index_sequence<_NV>(), [&](const auto... __args) constexpr {\n-\t    return __simd_tuple_concat(\n-\t      _SimdTuple<_Tp, simd_abi::scalar>{__args}..., _SimdTuple<_Tp>());\n-\t  });\n+\t\t __from, make_index_sequence<_NV>(),\n+\t\t [&](const auto... __args) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   return __simd_tuple_concat(\n+\t\t\t    _SimdTuple<_Tp, simd_abi::scalar>{__args}..., _SimdTuple<_Tp>());\n+\t\t });\n       }\n     else\n       return __call_with_subscripts(\n-\t__from,\n-\tmake_index_sequence<_NV>(), [&](const auto... __args) constexpr {\n-\t  return __to_simd_tuple<_Tp, _Np>(__args..., __fromX...);\n-\t});\n+\t       __from, make_index_sequence<_NV>(),\n+\t       [&](const auto... __args) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t return __to_simd_tuple<_Tp, _Np>(__args..., __fromX...);\n+\t       });\n   }\n \n template <size_t, typename _Tp>\n@@ -841,7 +840,7 @@ template <typename _Tp, typename _A0, typename _A1, typename... _Abis,\n \t\t       || _A0::template _S_is_partial<_Tp>)\n       return {__generate_from_n_evaluations<_R::_S_first_size,\n \t\t\t\t\t    typename _R::_FirstType>(\n-\t\t[&](auto __i) { return __x[__i]; }),\n+\t\t[&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { return __x[__i]; }),\n \t      __optimize_simd_tuple(\n \t\t__simd_tuple_pop_front<_R::_S_first_size>(__x))};\n     else if constexpr (is_same_v<_A0, _A1>\n@@ -994,10 +993,11 @@ template <int _Index, int _Total, int _Combine, typename _Tp, typename _A0,\n \treturn __as_vector(simd<_Tp, _RetAbi>(element_ptr, element_aligned));\n #else\n \t[[maybe_unused]] constexpr size_t __offset = __values_to_skip;\n-\treturn __as_vector(simd<_Tp, _RetAbi>([&](auto __i) constexpr {\n-\t  constexpr _SizeConstant<__i + __offset> __k;\n-\t  return __x[__k];\n-\t}));\n+\treturn __as_vector(simd<_Tp, _RetAbi>(\n+\t\t\t     [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t       constexpr _SizeConstant<__i + __offset> __k;\n+\t\t\t       return __x[__k];\n+\t\t\t     }));\n #endif\n       }\n \n@@ -1286,34 +1286,37 @@ template <int _Np, typename>\n     template <typename _Tp>\n       static constexpr inline _SimdMember<_Tp> _S_broadcast(_Tp __x) noexcept\n       {\n-\treturn _SimdMember<_Tp>::_S_generate([&](auto __meta) constexpr {\n-\t  return __meta._S_broadcast(__x);\n-\t});\n+\treturn _SimdMember<_Tp>::_S_generate(\n+\t\t [&](auto __meta) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   return __meta._S_broadcast(__x);\n+\t\t });\n       }\n \n     // _S_generator {{{2\n     template <typename _Fp, typename _Tp>\n       static constexpr inline _SimdMember<_Tp> _S_generator(_Fp&& __gen,\n \t\t\t\t\t\t\t    _TypeTag<_Tp>)\n       {\n-\treturn _SimdMember<_Tp>::_S_generate([&__gen](auto __meta) constexpr {\n-\t  return __meta._S_generator(\n-\t    [&](auto __i) constexpr {\n-\t      return __i < _Np ? __gen(_SizeConstant<__meta._S_offset + __i>())\n-\t\t\t       : 0;\n-\t    },\n-\t    _TypeTag<_Tp>());\n-\t});\n+\treturn _SimdMember<_Tp>::_S_generate(\n+\t\t [&__gen](auto __meta) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   return __meta._S_generator(\n+\t\t\t    [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t      return __i < _Np ? __gen(_SizeConstant<__meta._S_offset + __i>())\n+\t\t\t\t\t       : 0;\n+\t\t\t    },\n+\t\t\t    _TypeTag<_Tp>());\n+\t\t });\n       }\n \n     // _S_load {{{2\n     template <typename _Tp, typename _Up>\n       static inline _SimdMember<_Tp> _S_load(const _Up* __mem,\n \t\t\t\t\t     _TypeTag<_Tp>) noexcept\n       {\n-\treturn _SimdMember<_Tp>::_S_generate([&](auto __meta) {\n-\t  return __meta._S_load(&__mem[__meta._S_offset], _TypeTag<_Tp>());\n-\t});\n+\treturn _SimdMember<_Tp>::_S_generate(\n+\t\t [&](auto __meta) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   return __meta._S_load(&__mem[__meta._S_offset], _TypeTag<_Tp>());\n+\t\t });\n       }\n \n     // _S_masked_load {{{2\n@@ -1323,7 +1326,7 @@ template <int _Np, typename>\n \t\t     const _MaskMember __bits, const _Up* __mem) noexcept\n       {\n \tauto __merge = __old;\n-\t__for_each(__merge, [&](auto __meta, auto& __native) {\n+\t__for_each(__merge, [&](auto __meta, auto& __native) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t  if (__meta._S_submask(__bits).any())\n #pragma GCC diagnostic push\n \t  // __mem + __mem._S_offset could be UB ([expr.add]/4.3, but it punts\n@@ -1344,7 +1347,7 @@ template <int _Np, typename>\n       static inline void _S_store(const _SimdMember<_Tp>& __v, _Up* __mem,\n \t\t\t\t  _TypeTag<_Tp>) noexcept\n       {\n-\t__for_each(__v, [&](auto __meta, auto __native) {\n+\t__for_each(__v, [&](auto __meta, auto __native) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t  __meta._S_store(__native, &__mem[__meta._S_offset], _TypeTag<_Tp>());\n \t});\n       }\n@@ -1355,7 +1358,7 @@ template <int _Np, typename>\n \t\t\t\t\t _Up* __mem,\n \t\t\t\t\t const _MaskMember __bits) noexcept\n       {\n-\t__for_each(__v, [&](auto __meta, auto __native) {\n+\t__for_each(__v, [&](auto __meta, auto __native) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t  if (__meta._S_submask(__bits).any())\n #pragma GCC diagnostic push\n \t  // __mem + __mem._S_offset could be UB ([expr.add]/4.3, but it punts\n@@ -1376,7 +1379,7 @@ template <int _Np, typename>\n       {\n \t_MaskMember __bits = 0;\n \t__for_each(\n-\t  __x, [&__bits](auto __meta, auto __native) constexpr {\n+\t  __x, [&__bits](auto __meta, auto __native) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t    __bits\n \t      |= __meta._S_mask_to_shifted_ullong(__meta._S_negate(__native));\n \t  });\n@@ -1414,7 +1417,7 @@ template <int _Np, typename>\n \t  {\n \t    const auto& __x2 = __call_with_n_evaluations<\n \t      __div_roundup(_Tup::_S_tuple_size, 2)>(\n-\t      [](auto __first_simd, auto... __remaining) {\n+\t      [](auto __first_simd, auto... __remaining) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\tif constexpr (sizeof...(__remaining) == 0)\n \t\t  return __first_simd;\n \t\telse\n@@ -1428,7 +1431,7 @@ template <int _Np, typename>\n \t\t      __make_simd_tuple(__first_simd, __remaining...));\n \t\t  }\n \t      },\n-\t      [&](auto __i) {\n+\t      [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\tauto __left = __tup.template _M_simd_at<2 * __i>();\n \t\tif constexpr (2 * __i + 1 == _Tup::_S_tuple_size)\n \t\t  return __left;\n@@ -1444,7 +1447,9 @@ template <int _Np, typename>\n \t\t\t_GLIBCXX_SIMD_USE_CONSTEXPR_API\n \t\t\ttypename _LT::mask_type __k(\n \t\t\t  __private_init,\n-\t\t\t  [](auto __j) constexpr { return __j < _RT::size(); });\n+\t\t\t  [](auto __j) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t    return __j < _RT::size();\n+\t\t\t  });\n \t\t\t_LT __ext_right = __left;\n \t\t\twhere(__k, __ext_right)\n \t\t\t  = __proposed::resizing_simd_cast<_LT>(__right);\n@@ -1464,7 +1469,7 @@ template <int _Np, typename>\n \t     const _SimdTuple<_Tp, _As...>& __b)\n       {\n \treturn __a._M_apply_per_chunk(\n-\t  [](auto __impl, auto __aa, auto __bb) constexpr {\n+\t  [](auto __impl, auto __aa, auto __bb) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t    return __impl._S_min(__aa, __bb);\n \t  },\n \t  __b);\n@@ -1476,7 +1481,7 @@ template <int _Np, typename>\n \t     const _SimdTuple<_Tp, _As...>& __b)\n       {\n \treturn __a._M_apply_per_chunk(\n-\t  [](auto __impl, auto __aa, auto __bb) constexpr {\n+\t  [](auto __impl, auto __aa, auto __bb) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t    return __impl._S_max(__aa, __bb);\n \t  },\n \t  __b);\n@@ -1487,33 +1492,35 @@ template <int _Np, typename>\n       static inline constexpr _SimdTuple<_Tp, _As...>\n       _S_complement(const _SimdTuple<_Tp, _As...>& __x) noexcept\n       {\n-\treturn __x._M_apply_per_chunk([](auto __impl, auto __xx) constexpr {\n-\t  return __impl._S_complement(__xx);\n-\t});\n+\treturn __x._M_apply_per_chunk(\n+\t\t [](auto __impl, auto __xx) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   return __impl._S_complement(__xx);\n+\t\t });\n       }\n \n     // _S_unary_minus {{{2\n     template <typename _Tp, typename... _As>\n       static inline constexpr _SimdTuple<_Tp, _As...>\n       _S_unary_minus(const _SimdTuple<_Tp, _As...>& __x) noexcept\n       {\n-\treturn __x._M_apply_per_chunk([](auto __impl, auto __xx) constexpr {\n-\t  return __impl._S_unary_minus(__xx);\n-\t});\n+\treturn __x._M_apply_per_chunk(\n+\t\t [](auto __impl, auto __xx) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   return __impl._S_unary_minus(__xx);\n+\t\t });\n       }\n \n     // arithmetic operators {{{2\n \n-#define _GLIBCXX_SIMD_FIXED_OP(name_, op_)                                     \\\n-    template <typename _Tp, typename... _As>                                   \\\n-      static inline constexpr _SimdTuple<_Tp, _As...> name_(                   \\\n-\tconst _SimdTuple<_Tp, _As...>& __x, const _SimdTuple<_Tp, _As...>& __y)\\\n-      {                                                                        \\\n-\treturn __x._M_apply_per_chunk(                                         \\\n-\t  [](auto __impl, auto __xx, auto __yy) constexpr {                    \\\n-\t    return __impl.name_(__xx, __yy);                                   \\\n-\t  },                                                                   \\\n-\t  __y);                                                                \\\n+#define _GLIBCXX_SIMD_FIXED_OP(name_, op_)                                                     \\\n+    template <typename _Tp, typename... _As>                                                   \\\n+      static inline constexpr _SimdTuple<_Tp, _As...> name_(                                   \\\n+\tconst _SimdTuple<_Tp, _As...>& __x, const _SimdTuple<_Tp, _As...>& __y)                \\\n+      {                                                                                        \\\n+\treturn __x._M_apply_per_chunk(                                                         \\\n+\t  [](auto __impl, auto __xx, auto __yy) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { \\\n+\t    return __impl.name_(__xx, __yy);                                                   \\\n+\t  },                                                                                   \\\n+\t  __y);                                                                                \\\n       }\n \n     _GLIBCXX_SIMD_FIXED_OP(_S_plus, +)\n@@ -1532,18 +1539,20 @@ template <int _Np, typename>\n       static inline constexpr _SimdTuple<_Tp, _As...>\n       _S_bit_shift_left(const _SimdTuple<_Tp, _As...>& __x, int __y)\n       {\n-\treturn __x._M_apply_per_chunk([__y](auto __impl, auto __xx) constexpr {\n-\t  return __impl._S_bit_shift_left(__xx, __y);\n-\t});\n+\treturn __x._M_apply_per_chunk(\n+\t\t [__y](auto __impl, auto __xx) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   return __impl._S_bit_shift_left(__xx, __y);\n+\t\t });\n       }\n \n     template <typename _Tp, typename... _As>\n       static inline constexpr _SimdTuple<_Tp, _As...>\n       _S_bit_shift_right(const _SimdTuple<_Tp, _As...>& __x, int __y)\n       {\n-\treturn __x._M_apply_per_chunk([__y](auto __impl, auto __xx) constexpr {\n-\t  return __impl._S_bit_shift_right(__xx, __y);\n-\t});\n+\treturn __x._M_apply_per_chunk(\n+\t\t [__y](auto __impl, auto __xx) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   return __impl._S_bit_shift_right(__xx, __y);\n+\t\t });\n       }\n \n   // math {{{2\n@@ -1557,35 +1566,40 @@ template <int _Np, typename>\n \t  {                                                                    \\\n \t    if constexpr (is_same_v<_Tp, _RetTp>)                              \\\n \t      return __x._M_apply_per_chunk(                                   \\\n-\t\t[](auto __impl, auto __xx) constexpr {                         \\\n-\t\t  using _V = typename decltype(__impl)::simd_type;             \\\n-\t\t  return __data(__name(_V(__private_init, __xx)));             \\\n-\t\t});                                                            \\\n+\t\t       [](auto __impl, auto __xx)                              \\\n+\t\t\t constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA          \\\n+\t\t       {                                                       \\\n+\t\t\t using _V = typename decltype(__impl)::simd_type;      \\\n+\t\t\t return __data(__name(_V(__private_init, __xx)));      \\\n+\t\t       });                                                     \\\n \t    else                                                               \\\n \t      return __optimize_simd_tuple(                                    \\\n-\t\t__x.template _M_apply_r<_RetTp>([](auto __impl, auto __xx) {   \\\n-\t\t  return __impl._S_##__name(__xx);                             \\\n-\t\t}));                                                           \\\n+\t\t       __x.template _M_apply_r<_RetTp>(                        \\\n+\t\t\t [](auto __impl, auto __xx)                            \\\n+\t\t\t   _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA                  \\\n+\t\t\t { return __impl._S_##__name(__xx); }));               \\\n \t  }                                                                    \\\n \telse if constexpr (                                                    \\\n \t  is_same_v<                                                           \\\n \t    _Tp,                                                               \\\n \t    _RetTp> && (... && is_same_v<_SimdTuple<_Tp, _As...>, _More>) )    \\\n \t  return __x._M_apply_per_chunk(                                       \\\n-\t    [](auto __impl, auto __xx, auto... __pack) constexpr {             \\\n-\t      using _V = typename decltype(__impl)::simd_type;                 \\\n-\t      return __data(__name(_V(__private_init, __xx),                   \\\n-\t\t\t\t   _V(__private_init, __pack)...));            \\\n-\t    },                                                                 \\\n-\t    __more...);                                                        \\\n+\t\t   [](auto __impl, auto __xx, auto... __pack)                  \\\n+\t\t     constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA              \\\n+\t\t   {                                                           \\\n+\t\t     using _V = typename decltype(__impl)::simd_type;          \\\n+\t\t     return __data(__name(_V(__private_init, __xx),            \\\n+\t\t\t\t\t  _V(__private_init, __pack)...));     \\\n+\t\t   }, __more...);                                              \\\n \telse if constexpr (is_same_v<_Tp, _RetTp>)                             \\\n \t  return __x._M_apply_per_chunk(                                       \\\n-\t    [](auto __impl, auto __xx, auto... __pack) constexpr {             \\\n-\t      using _V = typename decltype(__impl)::simd_type;                 \\\n-\t      return __data(__name(_V(__private_init, __xx),                   \\\n-\t\t\t\t   __autocvt_to_simd(__pack)...));             \\\n-\t    },                                                                 \\\n-\t    __more...);                                                        \\\n+\t\t   [](auto __impl, auto __xx, auto... __pack)                  \\\n+\t\t     constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA              \\\n+\t\t   {                                                           \\\n+\t\t     using _V = typename decltype(__impl)::simd_type;          \\\n+\t\t     return __data(__name(_V(__private_init, __xx),            \\\n+\t\t\t\t\t  __autocvt_to_simd(__pack)...));      \\\n+\t\t   }, __more...);                                              \\\n \telse                                                                   \\\n \t  __assert_unreachable<_Tp>();                                         \\\n       }\n@@ -1657,10 +1671,10 @@ template <int _Np, typename>\n \t__fixed_size_storage_t<int, _SimdTuple<_Tp, _Abis...>::_S_size()>* __z)\n       {\n \treturn __x._M_apply_per_chunk(\n-\t  [](auto __impl, const auto __xx, const auto __yy, auto& __zz) {\n-\t    return __impl._S_remquo(__xx, __yy, &__zz);\n-\t  },\n-\t  __y, *__z);\n+\t\t [](auto __impl, const auto __xx, const auto __yy, auto& __zz)\n+\t\t   _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA\n+\t\t { return __impl._S_remquo(__xx, __yy, &__zz); },\n+\t\t __y, *__z);\n       }\n \n     template <typename _Tp, typename... _As>\n@@ -1669,12 +1683,10 @@ template <int _Np, typename>\n \t       __fixed_size_storage_t<int, _Np>& __exp) noexcept\n       {\n \treturn __x._M_apply_per_chunk(\n-\t  [](auto __impl, const auto& __a, auto& __b) {\n-\t    return __data(\n-\t      frexp(typename decltype(__impl)::simd_type(__private_init, __a),\n-\t\t    __autocvt_to_simd(__b)));\n-\t  },\n-\t  __exp);\n+\t\t [](auto __impl, const auto& __a, auto& __b) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   return __data(frexp(typename decltype(__impl)::simd_type(__private_init, __a),\n+\t\t\t\t       __autocvt_to_simd(__b)));\n+\t\t }, __exp);\n       }\n \n #define _GLIBCXX_SIMD_TEST_ON_TUPLE_(name_)                                    \\\n@@ -1700,7 +1712,7 @@ template <int _Np, typename>\n       _S_increment(_SimdTuple<_Ts...>& __x)\n       {\n \t__for_each(\n-\t  __x, [](auto __meta, auto& native) constexpr {\n+\t  __x, [](auto __meta, auto& native) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t    __meta._S_increment(native);\n \t  });\n       }\n@@ -1710,7 +1722,7 @@ template <int _Np, typename>\n       _S_decrement(_SimdTuple<_Ts...>& __x)\n       {\n \t__for_each(\n-\t  __x, [](auto __meta, auto& native) constexpr {\n+\t  __x, [](auto __meta, auto& native) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t    __meta._S_decrement(native);\n \t  });\n       }\n@@ -1722,11 +1734,10 @@ template <int _Np, typename>\n       __cmp(const _SimdTuple<_Tp, _As...>& __x,                                \\\n \t    const _SimdTuple<_Tp, _As...>& __y)                                \\\n       {                                                                        \\\n-\treturn _M_test(                                                        \\\n-\t  [](auto __impl, auto __xx, auto __yy) constexpr {                    \\\n-\t    return __impl.__cmp(__xx, __yy);                                   \\\n-\t  },                                                                   \\\n-\t  __x, __y);                                                           \\\n+\treturn _M_test([](auto __impl, auto __xx, auto __yy)                   \\\n+\t\t\t constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA          \\\n+\t\t       { return __impl.__cmp(__xx, __yy); },                   \\\n+\t\t       __x, __y);                                              \\\n       }\n \n     _GLIBCXX_SIMD_CMP_OPERATIONS(_S_equal_to)\n@@ -1753,12 +1764,13 @@ template <int _Np, typename>\n       _S_masked_assign(const _MaskMember __bits, _SimdTuple<_Tp, _As...>& __lhs,\n \t\t       const __type_identity_t<_SimdTuple<_Tp, _As...>>& __rhs)\n       {\n-\t__for_each(\n-\t  __lhs, __rhs,\n-\t  [&](auto __meta, auto& __native_lhs, auto __native_rhs) constexpr {\n-\t    __meta._S_masked_assign(__meta._S_make_mask(__bits), __native_lhs,\n-\t\t\t\t    __native_rhs);\n-\t  });\n+\t__for_each(__lhs, __rhs,\n+\t\t   [&](auto __meta, auto& __native_lhs, auto __native_rhs)\n+\t\t     constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA\n+\t\t   {\n+\t\t     __meta._S_masked_assign(__meta._S_make_mask(__bits), __native_lhs,\n+\t\t\t\t\t     __native_rhs);\n+\t\t   });\n       }\n \n     // Optimization for the case where the RHS is a scalar. No need to broadcast\n@@ -1769,7 +1781,7 @@ template <int _Np, typename>\n \t\t       const __type_identity_t<_Tp> __rhs)\n       {\n \t__for_each(\n-\t  __lhs, [&](auto __meta, auto& __native_lhs) constexpr {\n+\t  __lhs, [&](auto __meta, auto& __native_lhs) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t    __meta._S_masked_assign(__meta._S_make_mask(__bits), __native_lhs,\n \t\t\t\t    __rhs);\n \t  });\n@@ -1782,12 +1794,13 @@ template <int _Np, typename>\n \t\t\t\t\t   const _SimdTuple<_Tp, _As...>& __rhs,\n \t\t\t\t\t   _Op __op)\n       {\n-\t__for_each(\n-\t  __lhs, __rhs,\n-\t  [&](auto __meta, auto& __native_lhs, auto __native_rhs) constexpr {\n-\t    __meta.template _S_masked_cassign(__meta._S_make_mask(__bits),\n-\t\t\t\t\t      __native_lhs, __native_rhs, __op);\n-\t  });\n+\t__for_each(__lhs, __rhs,\n+\t\t   [&](auto __meta, auto& __native_lhs, auto __native_rhs)\n+\t\t     constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA\n+\t\t   {\n+\t\t     __meta.template _S_masked_cassign(__meta._S_make_mask(__bits),\n+\t\t\t\t\t\t       __native_lhs, __native_rhs, __op);\n+\t\t   });\n       }\n \n     // Optimization for the case where the RHS is a scalar. No need to broadcast\n@@ -1798,7 +1811,7 @@ template <int _Np, typename>\n \t\t\t\t\t   const _Tp& __rhs, _Op __op)\n       {\n \t__for_each(\n-\t  __lhs, [&](auto __meta, auto& __native_lhs) constexpr {\n+\t  __lhs, [&](auto __meta, auto& __native_lhs) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t    __meta.template _S_masked_cassign(__meta._S_make_mask(__bits),\n \t\t\t\t\t      __native_lhs, __rhs, __op);\n \t  });\n@@ -1899,7 +1912,7 @@ template <int _Np, typename>\n       // _Np _UShort, _UInt, _ULLong, float, and double can be more efficient.\n       _ULLong __r = 0;\n       using _Vs = __fixed_size_storage_t<_UChar, _Np>;\n-      __for_each(_Vs{}, [&](auto __meta, auto) {\n+      __for_each(_Vs{}, [&](auto __meta, auto) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t__r |= __meta._S_mask_to_shifted_ullong(\n \t  __meta._S_mask_impl._S_load(&__mem[__meta._S_offset],\n \t\t\t\t      _SizeConstant<__meta._S_size()>()));\n@@ -1912,9 +1925,10 @@ template <int _Np, typename>\n \t\t\t\t\t     _MaskMember __mask,\n \t\t\t\t\t     const bool* __mem) noexcept\n     {\n-      _BitOps::_S_bit_iteration(__mask.to_ullong(), [&](auto __i) {\n-\t__merge.set(__i, __mem[__i]);\n-      });\n+      _BitOps::_S_bit_iteration(__mask.to_ullong(),\n+\t\t\t\t[&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t\t  __merge.set(__i, __mem[__i]);\n+\t\t\t\t});\n       return __merge;\n     }\n \n@@ -1932,7 +1946,8 @@ template <int _Np, typename>\n     static inline void _S_masked_store(const _MaskMember __v, bool* __mem,\n \t\t\t\t       const _MaskMember __k) noexcept\n     {\n-      _BitOps::_S_bit_iteration(__k, [&](auto __i) { __mem[__i] = __v[__i]; });\n+      _BitOps::_S_bit_iteration(\n+\t__k, [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA { __mem[__i] = __v[__i]; });\n     }\n \n     // logical and bitwise operators {{{2"}, {"sha": "c20315e4e3047a818874d49211c1c576f562ae99", "filename": "libstdc++-v3/include/experimental/bits/simd_math.h", "status": "modified", "additions": 29, "deletions": 23, "changes": 52, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_math.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_math.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_math.h?ref=53b55701aed6896f456cdec7997ac6bbef1d6074", "patch": "@@ -788,7 +788,7 @@ template <typename _Tp, typename _Abi, typename = __detail::__odr_helper>\n \n \t// __exponent(__x) returns the exponent value (bias removed) as\n \t// simd<_Up> with integral _Up\n-\tauto&& __exponent = [](const _V& __v) {\n+\tauto&& __exponent = [](const _V& __v) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t  using namespace std::experimental::__proposed;\n \t  using _IV = rebind_simd_t<\n \t    conditional_t<sizeof(_Tp) == sizeof(_LLong), _LLong, int>, _V>;\n@@ -931,7 +931,7 @@ template <typename _R, typename _ToApply, typename _Tp, typename... _Tps>\n   {\n     return {__private_init,\n \t    __data(__arg0)._M_apply_per_chunk(\n-\t      [&](auto __impl, const auto&... __inner) {\n+\t      [&](auto __impl, const auto&... __inner) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\tusing _V = typename decltype(__impl)::simd_type;\n \t\treturn __data(__apply(_V(__private_init, __inner)...));\n \t      },\n@@ -1092,8 +1092,9 @@ _GLIBCXX_SIMD_CVTING2(hypot)\n     if constexpr (__is_fixed_size_abi_v<_Abi> && _V::size() > 1)\n       {\n \treturn __fixed_size_apply<simd<_Tp, _Abi>>(\n-\t  [](auto __a, auto __b, auto __c) { return hypot(__a, __b, __c); },\n-\t  __x, __y, __z);\n+\t\t [](auto __a, auto __b, auto __c) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   return hypot(__a, __b, __c);\n+\t\t }, __x, __y, __z);\n       }\n     else\n       {\n@@ -1380,9 +1381,9 @@ template <typename _Tp, typename _Abi, typename = __detail::__odr_helper>\n \t\t const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __m,\n \t\t const simd<_Tp, _Abi>& __x)\n   {\n-    return simd<_Tp, _Abi>([&](auto __i) {\n-      return std::assoc_laguerre(__n[__i], __m[__i], __x[__i]);\n-    });\n+    return simd<_Tp, _Abi>([&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t     return std::assoc_laguerre(__n[__i], __m[__i], __x[__i]);\n+\t   });\n   }\n \n template <typename _Tp, typename _Abi, typename = __detail::__odr_helper>\n@@ -1391,9 +1392,9 @@ template <typename _Tp, typename _Abi, typename = __detail::__odr_helper>\n \t\t const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __m,\n \t\t const simd<_Tp, _Abi>& __x)\n   {\n-    return simd<_Tp, _Abi>([&](auto __i) {\n-      return std::assoc_legendre(__n[__i], __m[__i], __x[__i]);\n-    });\n+    return simd<_Tp, _Abi>([&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t     return std::assoc_legendre(__n[__i], __m[__i], __x[__i]);\n+\t   });\n   }\n \n _GLIBCXX_SIMD_MATH_CALL2_(beta, _Tp)\n@@ -1414,26 +1415,29 @@ template <typename _Tp, typename _Abi, typename = __detail::__odr_helper>\n   hermite(const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __n,\n \t  const simd<_Tp, _Abi>& __x)\n   {\n-    return simd<_Tp, _Abi>(\n-      [&](auto __i) { return std::hermite(__n[__i], __x[__i]); });\n+    return simd<_Tp, _Abi>([&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t     return std::hermite(__n[__i], __x[__i]);\n+\t   });\n   }\n \n template <typename _Tp, typename _Abi, typename = __detail::__odr_helper>\n   enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n   laguerre(const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __n,\n \t   const simd<_Tp, _Abi>& __x)\n   {\n-    return simd<_Tp, _Abi>(\n-      [&](auto __i) { return std::laguerre(__n[__i], __x[__i]); });\n+    return simd<_Tp, _Abi>([&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t     return std::laguerre(__n[__i], __x[__i]);\n+\t   });\n   }\n \n template <typename _Tp, typename _Abi, typename = __detail::__odr_helper>\n   enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n   legendre(const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __n,\n \t   const simd<_Tp, _Abi>& __x)\n   {\n-    return simd<_Tp, _Abi>(\n-      [&](auto __i) { return std::legendre(__n[__i], __x[__i]); });\n+    return simd<_Tp, _Abi>([&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t     return std::legendre(__n[__i], __x[__i]);\n+\t   });\n   }\n \n _GLIBCXX_SIMD_MATH_CALL_(riemann_zeta)\n@@ -1443,8 +1447,9 @@ template <typename _Tp, typename _Abi, typename = __detail::__odr_helper>\n   sph_bessel(const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __n,\n \t     const simd<_Tp, _Abi>& __x)\n   {\n-    return simd<_Tp, _Abi>(\n-      [&](auto __i) { return std::sph_bessel(__n[__i], __x[__i]); });\n+    return simd<_Tp, _Abi>([&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t     return std::sph_bessel(__n[__i], __x[__i]);\n+\t   });\n   }\n \n template <typename _Tp, typename _Abi, typename = __detail::__odr_helper>\n@@ -1453,18 +1458,19 @@ template <typename _Tp, typename _Abi, typename = __detail::__odr_helper>\n \t       const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __m,\n \t       const simd<_Tp, _Abi>& theta)\n   {\n-    return simd<_Tp, _Abi>([&](auto __i) {\n-      return std::assoc_legendre(__l[__i], __m[__i], theta[__i]);\n-    });\n+    return simd<_Tp, _Abi>([&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t     return std::assoc_legendre(__l[__i], __m[__i], theta[__i]);\n+\t   });\n   }\n \n template <typename _Tp, typename _Abi, typename = __detail::__odr_helper>\n   enable_if_t<is_floating_point_v<_Tp>, simd<_Tp, _Abi>>\n   sph_neumann(const fixed_size_simd<unsigned, simd_size_v<_Tp, _Abi>>& __n,\n \t      const simd<_Tp, _Abi>& __x)\n   {\n-    return simd<_Tp, _Abi>(\n-      [&](auto __i) { return std::sph_neumann(__n[__i], __x[__i]); });\n+    return simd<_Tp, _Abi>([&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t     return std::sph_neumann(__n[__i], __x[__i]);\n+\t   });\n   }\n // }}}\n "}, {"sha": "7e4cb17b205d882c890e7292370fccdeb607f32b", "filename": "libstdc++-v3/include/experimental/bits/simd_neon.h", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_neon.h?ref=53b55701aed6896f456cdec7997ac6bbef1d6074", "patch": "@@ -61,7 +61,7 @@ template <typename _Abi, typename>\n       _S_masked_load(_SimdWrapper<_Tp, _Np> __merge, _MaskMember<_Tp> __k,\n \t\t     const _Up* __mem) noexcept\n       {\n-\t__execute_n_times<_Np>([&](auto __i) {\n+\t__execute_n_times<_Np>([&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t  if (__k[__i] != 0)\n \t    __merge._M_set(__i, static_cast<_Tp>(__mem[__i]));\n \t});\n@@ -75,7 +75,7 @@ template <typename _Abi, typename>\n       _S_masked_store_nocvt(_SimdWrapper<_Tp, _Np> __v, _Tp* __mem,\n \t\t\t    _MaskMember<_Tp> __k)\n       {\n-\t__execute_n_times<_Np>([&](auto __i) {\n+\t__execute_n_times<_Np>([&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t  if (__k[__i] != 0)\n \t    __mem[__i] = __v[__i];\n \t});\n@@ -286,7 +286,7 @@ struct _MaskImplNeonMixin\n \t    {\n \t      constexpr auto __bitsel\n \t\t= __generate_from_n_evaluations<16, __vector_type_t<_I, 16>>(\n-\t\t  [&](auto __i) {\n+\t\t  [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t    return static_cast<_I>(\n \t\t      __i < _Np ? (__i < 8 ? 1 << __i : 1 << (__i - 8)) : 0);\n \t\t  });\n@@ -306,7 +306,7 @@ struct _MaskImplNeonMixin\n \t    {\n \t      constexpr auto __bitsel\n \t\t= __generate_from_n_evaluations<8, __vector_type_t<_I, 8>>(\n-\t\t  [&](auto __i) {\n+\t\t  [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t    return static_cast<_I>(__i < _Np ? 1 << __i : 0);\n \t\t  });\n \t      __asint &= __bitsel;\n@@ -322,7 +322,7 @@ struct _MaskImplNeonMixin\n \t    {\n \t      constexpr auto __bitsel\n \t\t= __generate_from_n_evaluations<4, __vector_type_t<_I, 4>>(\n-\t\t  [&](auto __i) {\n+\t\t  [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t    return static_cast<_I>(__i < _Np ? 1 << __i : 0);\n \t\t  });\n \t      __asint &= __bitsel;\n@@ -346,7 +346,7 @@ struct _MaskImplNeonMixin\n \t    {\n \t      constexpr auto __bitsel\n \t\t= __generate_from_n_evaluations<8, __vector_type_t<_I, 8>>(\n-\t\t  [&](auto __i) {\n+\t\t  [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t    return static_cast<_I>(__i < _Np ? 1 << __i : 0);\n \t\t  });\n \t      __asint &= __bitsel;\n@@ -361,7 +361,7 @@ struct _MaskImplNeonMixin\n \t    {\n \t      constexpr auto __bitsel\n \t\t= __generate_from_n_evaluations<4, __vector_type_t<_I, 4>>(\n-\t\t  [&](auto __i) {\n+\t\t  [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t    return static_cast<_I>(__i < _Np ? 1 << __i : 0);\n \t\t  });\n \t      __asint &= __bitsel;"}, {"sha": "60e80d394ba2cd36ae3cf44f08498a1b65d3bd7d", "filename": "libstdc++-v3/include/experimental/bits/simd_x86.h", "status": "modified", "additions": 65, "deletions": 57, "changes": 122, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_x86.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/53b55701aed6896f456cdec7997ac6bbef1d6074/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_x86.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/libstdc%2B%2B-v3%2Finclude%2Fexperimental%2Fbits%2Fsimd_x86.h?ref=53b55701aed6896f456cdec7997ac6bbef1d6074", "patch": "@@ -537,24 +537,25 @@ struct _CommonImplX86 : _CommonImplBuiltin\n     _S_store_bool_array(const _BitMask<_Np, _Sanitized> __x, bool* __mem)\n     {\n       if constexpr (__have_avx512bw_vl) // don't care for BW w/o VL\n-\t_S_store<_Np>(1 & __vector_bitcast<_UChar, _Np>([=]() constexpr {\n-\t\t\tif constexpr (_Np <= 16)\n-\t\t\t  return _mm_movm_epi8(__x._M_to_bits());\n-\t\t\telse if constexpr (_Np <= 32)\n-\t\t\t  return _mm256_movm_epi8(__x._M_to_bits());\n-\t\t\telse if constexpr (_Np <= 64)\n-\t\t\t  return _mm512_movm_epi8(__x._M_to_bits());\n-\t\t\telse\n-\t\t\t  __assert_unreachable<_SizeConstant<_Np>>();\n-\t\t      }()),\n+\t_S_store<_Np>(1 & __vector_bitcast<_UChar, _Np>(\n+\t\t\t    [=]() constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t      if constexpr (_Np <= 16)\n+\t\t\t\treturn _mm_movm_epi8(__x._M_to_bits());\n+\t\t\t      else if constexpr (_Np <= 32)\n+\t\t\t\treturn _mm256_movm_epi8(__x._M_to_bits());\n+\t\t\t      else if constexpr (_Np <= 64)\n+\t\t\t\treturn _mm512_movm_epi8(__x._M_to_bits());\n+\t\t\t      else\n+\t\t\t\t__assert_unreachable<_SizeConstant<_Np>>();\n+\t\t\t    }()),\n \t\t      __mem);\n       else if constexpr (__have_bmi2)\n \t{\n \t  if constexpr (_Np <= 4)\n \t    _S_store<_Np>(_pdep_u32(__x._M_to_bits(), 0x01010101U), __mem);\n \t  else\n \t    __execute_n_times<__div_roundup(_Np, sizeof(size_t))>(\n-\t      [&](auto __i) {\n+\t      [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\tconstexpr size_t __offset = __i * sizeof(size_t);\n \t\tconstexpr int __todo = std::min(sizeof(size_t), _Np - __offset);\n \t\tif constexpr (__todo == 1)\n@@ -575,7 +576,7 @@ struct _CommonImplX86 : _CommonImplBuiltin\n \t      });\n \t}\n       else if constexpr (__have_sse2 && _Np > 7)\n-\t__execute_n_times<__div_roundup(_Np, 16)>([&](auto __i) {\n+\t__execute_n_times<__div_roundup(_Np, 16)>([&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t  constexpr int __offset = __i * 16;\n \t  constexpr int __todo = std::min(16, int(_Np) - __offset);\n \t  const int __bits = __x.template _M_extract<__offset>()._M_to_bits();\n@@ -765,9 +766,10 @@ struct _CommonImplX86 : _CommonImplBuiltin\n       static_assert(is_same_v<_Tp, _Tp> && __have_avx512f);\n       if (__k._M_is_constprop() && __at0._M_is_constprop()\n \t  && __at1._M_is_constprop())\n-\treturn __generate_from_n_evaluations<_Np,\n-\t\t\t\t\t     __vector_type_t<_Tp, _Np>>([&](\n-\t  auto __i) constexpr { return __k[__i] ? __at1[__i] : __at0[__i]; });\n+\treturn __generate_from_n_evaluations<_Np, __vector_type_t<_Tp, _Np>>(\n+\t\t [&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t   return __k[__i] ? __at1[__i] : __at0[__i];\n+\t\t });\n       else if constexpr (sizeof(__at0) == 64\n \t\t\t || (__have_avx512vl && sizeof(__at0) >= 16))\n \treturn _S_blend_avx512(__k._M_data, __at0._M_data, __at1._M_data);\n@@ -994,9 +996,8 @@ template <typename _Abi, typename>\n \t      }\n \t    else\n \t      _BitOps::_S_bit_iteration(_MaskImpl::_S_to_bits(__k),\n-\t\t\t\t\t[&](auto __i) {\n-\t\t\t\t\t  __merge._M_set(__i, static_cast<_Tp>(\n-\t\t\t\t\t\t\t\t__mem[__i]));\n+\t\t\t\t\t[&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t\t\t\t  __merge._M_set(__i, static_cast<_Tp>(__mem[__i]));\n \t\t\t\t\t});\n \t  }\n \t/* Very uncertain, that the following improves anything. Needs\n@@ -1417,11 +1418,12 @@ template <typename _Abi, typename>\n \t      const auto __yf = __convert_all<_FloatV, __n_floatv>(\n \t\t_Abi::__make_padding_nonzero(__as_vector(__y)));\n \t      return __call_with_n_evaluations<__n_floatv>(\n-\t\t[](auto... __quotients) {\n+\t\t[](auto... __quotients) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t  return __vector_convert<_R>(__quotients...);\n \t\t},\n-\t\t[&__xf,\n-\t\t &__yf](auto __i) -> _SimdWrapper<_Float, __n_intermediate> {\n+\t\t[&__xf, &__yf](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA\n+\t\t  -> _SimdWrapper<_Float, __n_intermediate>\n+\t\t{\n #if !defined __clang__ && __GCC_IEC_559 == 0\n \t\t  // If -freciprocal-math is active, using the `/` operator is\n \t\t  // incorrect because it may be translated to an imprecise\n@@ -1980,7 +1982,7 @@ template <typename _Abi, typename>\n \t      {\n \t\tauto __mask = __vector_bitcast<_UChar>(\n \t\t  __vector_bitcast<_UShort>(__iy) << 5);\n-\t\tauto __maskl = [&]() {\n+\t\tauto __maskl = [&]() _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t  return __to_intrin(__vector_bitcast<_UShort>(__mask) << 8);\n \t\t};\n \t\tauto __xh = __vector_bitcast<short>(__ix);\n@@ -2067,19 +2069,20 @@ template <typename _Abi, typename>\n \t  }                                                      //}}}\n \telse if constexpr (sizeof(_Up) == 2 && sizeof(__x) >= 4) //{{{\n \t  {\n-\t    [[maybe_unused]] auto __blend_0xaa = [](auto __a, auto __b) {\n-\t      if constexpr (sizeof(__a) == 16)\n-\t\treturn _mm_blend_epi16(__to_intrin(__a), __to_intrin(__b),\n-\t\t\t\t       0xaa);\n-\t      else if constexpr (sizeof(__a) == 32)\n-\t\treturn _mm256_blend_epi16(__to_intrin(__a), __to_intrin(__b),\n-\t\t\t\t\t  0xaa);\n-\t      else if constexpr (sizeof(__a) == 64)\n-\t\treturn _mm512_mask_blend_epi16(0xaaaa'aaaaU, __to_intrin(__a),\n-\t\t\t\t\t       __to_intrin(__b));\n-\t      else\n-\t\t__assert_unreachable<decltype(__a)>();\n-\t    };\n+\t    [[maybe_unused]] auto __blend_0xaa\n+\t      = [](auto __a, auto __b) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\tif constexpr (sizeof(__a) == 16)\n+\t\t  return _mm_blend_epi16(__to_intrin(__a), __to_intrin(__b),\n+\t\t\t\t\t 0xaa);\n+\t\telse if constexpr (sizeof(__a) == 32)\n+\t\t  return _mm256_blend_epi16(__to_intrin(__a), __to_intrin(__b),\n+\t\t\t\t\t    0xaa);\n+\t\telse if constexpr (sizeof(__a) == 64)\n+\t\t  return _mm512_mask_blend_epi16(0xaaaa'aaaaU, __to_intrin(__a),\n+\t\t\t\t\t\t __to_intrin(__b));\n+\t\telse\n+\t\t  __assert_unreachable<decltype(__a)>();\n+\t      };\n \t    if constexpr (__have_avx512bw_vl && sizeof(_Tp) <= 16)\n \t      return __intrin_bitcast<_V>(is_signed_v<_Up>\n \t\t\t\t\t    ? _mm_srav_epi16(__ix, __iy)\n@@ -2136,9 +2139,10 @@ template <typename _Abi, typename>\n \t      {\n \t\tauto __k = __vector_bitcast<_UShort>(__iy) << 11;\n \t\tauto __x128 = __vector_bitcast<_Up>(__ix);\n-\t\tauto __mask = [](__vector_type16_t<_UShort> __kk) {\n-\t\t  return __vector_bitcast<short>(__kk) < 0;\n-\t\t};\n+\t\tauto __mask\n+\t\t  = [](__vector_type16_t<_UShort> __kk) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t    return __vector_bitcast<short>(__kk) < 0;\n+\t\t  };\n \t\t// do __x128 = 0 where __y[4] is set\n \t\t__x128 = __mask(__k) ? decltype(__x128)() : __x128;\n \t\t// do __x128 =>> 8 where __y[3] is set\n@@ -2178,7 +2182,7 @@ template <typename _Abi, typename>\n \t      }\n \t    else\n \t      {\n-\t\tauto __shift = [](auto __a, auto __b) {\n+\t\tauto __shift = [](auto __a, auto __b) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t  if constexpr (is_signed_v<_Up>)\n \t\t    return _mm_sra_epi32(__a, __b);\n \t\t  else\n@@ -3492,7 +3496,7 @@ struct _MaskImplX86Mixin\n \treturn _S_to_maskvector<_Up, _ToN>(__k);\n       else if (__x._M_is_constprop() || __builtin_is_constant_evaluated())\n \treturn __generate_from_n_evaluations<std::min(_ToN, _Np), _UV>(\n-\t  [&](auto __i) -> _Up { return -__x[__i.value]; });\n+\t  [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA -> _Up { return -__x[__i.value]; });\n       else if constexpr (sizeof(_Up) == 1)\n \t{\n \t  if constexpr (sizeof(_UI) == 16)\n@@ -3737,9 +3741,9 @@ struct _MaskImplX86Mixin\n       else if constexpr (__bits_per_element >= _ToN)\n \t{\n \t  constexpr auto __bitmask\n-\t    = __generate_vector<_V>([](auto __i) constexpr->_UpUInt {\n-\t\treturn __i < _ToN ? 1ull << __i : 0;\n-\t      });\n+\t    = __generate_vector<_V>([](auto __i)\n+\t\t\t\t    constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA -> _UpUInt\n+\t\t\t\t    { return __i < _ToN ? 1ull << __i : 0; });\n \t  const auto __bits\n \t    = __vector_broadcast<_ToN, _UpUInt>(__k) & __bitmask;\n \t  if constexpr (__bits_per_element > _ToN)\n@@ -3750,11 +3754,11 @@ struct _MaskImplX86Mixin\n       else\n \t{\n \t  const _V __tmp\n-\t    = __generate_vector<_V>([&](auto __i) constexpr {\n+\t    = __generate_vector<_V>([&](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\treturn static_cast<_UpUInt>(\n \t\t  __k >> (__bits_per_element * (__i / __bits_per_element)));\n \t      })\n-\t      & __generate_vector<_V>([](auto __i) constexpr {\n+\t      & __generate_vector<_V>([](auto __i) constexpr _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t  return static_cast<_UpUInt>(1ull\n \t\t\t\t\t      << (__i % __bits_per_element));\n \t\t}); // mask bit index\n@@ -3790,7 +3794,7 @@ struct _MaskImplX86Mixin\n \t      const auto __y = __vector_bitcast<__int_for_sizeof_t<_Tp>>(__x);\n \t      return __generate_from_n_evaluations<std::min(_ToN, _Np),\n \t\t\t\t\t\t   __vector_type_t<_Up, _ToN>>(\n-\t\t[&](auto __i) -> _Up { return __y[__i.value]; });\n+\t\t[&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA -> _Up { return __y[__i.value]; });\n \t    }\n \t  using _To = __vector_type_t<_Up, _ToN>;\n \t  [[maybe_unused]] constexpr size_t _FromN = _Np;\n@@ -4125,8 +4129,11 @@ struct _MaskImplX86Mixin\n \t    {\n \t      const auto __bools = -__x._M_data;\n \t      const _ULLong __k = __call_with_n_evaluations<_Np>(\n-\t\t[](auto... __bits) { return (__bits | ...); },\n-\t\t[&](auto __i) { return _ULLong(__bools[+__i]) << __i; });\n+\t\t[](auto... __bits) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t  return (__bits | ...);\n+\t\t}, [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n+\t\t  return _ULLong(__bools[+__i]) << __i;\n+\t\t});\n \t      if (__builtin_is_constant_evaluated()\n \t\t  || __builtin_constant_p(__k))\n \t\treturn __k;\n@@ -4282,13 +4289,14 @@ template <typename _Abi, typename>\n \tstatic_assert(is_same_v<_Tp, __int_for_sizeof_t<_Tp>>);\n \tif constexpr (__have_avx512bw)\n \t  {\n-\t    const auto __to_vec_or_bits = [](auto __bits) -> decltype(auto) {\n-\t      if constexpr (__is_avx512_abi<_Abi>())\n-\t\treturn __bits;\n-\t      else\n-\t\treturn _S_to_maskvector<_Tp>(\n-\t\t  _BitMask<_S_size<_Tp>>(__bits)._M_sanitized());\n-\t    };\n+\t    const auto __to_vec_or_bits\n+\t      = [](auto __bits) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA -> decltype(auto) {\n+\t\tif constexpr (__is_avx512_abi<_Abi>())\n+\t\t  return __bits;\n+\t\telse\n+\t\t  return _S_to_maskvector<_Tp>(\n+\t\t\t   _BitMask<_S_size<_Tp>>(__bits)._M_sanitized());\n+\t      };\n \n \t    if constexpr (_S_size<_Tp> <= 16 && __have_avx512vl)\n \t      {\n@@ -4475,7 +4483,7 @@ template <typename _Abi, typename>\n \t      }\n \t    else\n \t      {\n-\t\t_BitOps::_S_bit_iteration(__mask, [&](auto __i) {\n+\t\t_BitOps::_S_bit_iteration(__mask, [&](auto __i) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t  __merge._M_set(__i, __mem[__i]);\n \t\t});\n \t\treturn __merge;\n@@ -4554,7 +4562,7 @@ template <typename _Abi, typename>\n \t  {\n \t    if constexpr (__have_avx512bw_vl)\n \t      _CommonImplX86::_S_store<_Np>(\n-\t\t__vector_bitcast<char>([](auto __data) {\n+\t\t__vector_bitcast<char>([](auto __data) _GLIBCXX_SIMD_ALWAYS_INLINE_LAMBDA {\n \t\t  if constexpr (_Np <= 16)\n \t\t    return _mm_maskz_set1_epi8(__data, 1);\n \t\t  else if constexpr (_Np <= 32)"}]}