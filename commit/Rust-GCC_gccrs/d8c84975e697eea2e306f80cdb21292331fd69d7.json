{"sha": "d8c84975e697eea2e306f80cdb21292331fd69d7", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ZDhjODQ5NzVlNjk3ZWVhMmUzMDZmODBjZGIyMTI5MjMzMWZkNjlkNw==", "commit": {"author": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2013-10-31T19:06:49Z"}, "committer": {"name": "Jakub Jelinek", "email": "jakub@gcc.gnu.org", "date": "2013-10-31T19:06:49Z"}, "message": "optabs.c (expand_vec_perm): Avoid vector mode punning SUBREGs in SET_DEST.\n\n\t* optabs.c (expand_vec_perm): Avoid vector mode punning\n\tSUBREGs in SET_DEST.\n\t* expmed.c (store_bit_field_1): Likewise.\n\t* config/i386/sse.md (movdi_to_sse, vec_pack_sfix_trunc_v2df,\n\tvec_pack_sfix_v2df, vec_shl_<mode>, vec_shr_<mode>,\n\tvec_interleave_high<mode>, vec_interleave_low<mode>): Likewise.\n\t* config/i386/i386.c (ix86_expand_vector_move_misalign,\n\tix86_expand_sse_movcc, ix86_expand_int_vcond, ix86_expand_vec_perm,\n\tix86_expand_sse_unpack, ix86_expand_args_builtin,\n\tix86_expand_vector_init_duplicate, ix86_expand_vector_set,\n\temit_reduc_half, expand_vec_perm_blend, expand_vec_perm_pshufb,\n\texpand_vec_perm_interleave2, expand_vec_perm_pshufb2,\n\texpand_vec_perm_vpshufb2_vpermq,\n\texpand_vec_perm_vpshufb2_vpermq_even_odd, expand_vec_perm_even_odd_1,\n\texpand_vec_perm_broadcast_1, expand_vec_perm_vpshufb4_vpermq2,\n\tix86_expand_sse2_mulv4si3, ix86_expand_pinsr): Likewise.\n\t(expand_vec_perm_palignr): Likewise.  Modify a copy of *d rather\n\tthan *d itself.\n\nFrom-SVN: r204274", "tree": {"sha": "7ea25de19c8a02724d9cc355bb4b34e90e4f4b93", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/7ea25de19c8a02724d9cc355bb4b34e90e4f4b93"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/d8c84975e697eea2e306f80cdb21292331fd69d7", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d8c84975e697eea2e306f80cdb21292331fd69d7", "html_url": "https://github.com/Rust-GCC/gccrs/commit/d8c84975e697eea2e306f80cdb21292331fd69d7", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/d8c84975e697eea2e306f80cdb21292331fd69d7/comments", "author": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "5a9785fb4c3ceb3b338634f2ea17474eaebb4955", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5a9785fb4c3ceb3b338634f2ea17474eaebb4955", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5a9785fb4c3ceb3b338634f2ea17474eaebb4955"}], "stats": {"total": 443, "additions": 300, "deletions": 143}, "files": [{"sha": "94fb8ad54b4bdf074e80fc95f953ed5838cb7fcf", "filename": "gcc/ChangeLog", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d8c84975e697eea2e306f80cdb21292331fd69d7/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d8c84975e697eea2e306f80cdb21292331fd69d7/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=d8c84975e697eea2e306f80cdb21292331fd69d7", "patch": "@@ -1,3 +1,24 @@\n+2013-10-31  Jakub Jelinek  <jakub@redhat.com>\n+\n+\t* optabs.c (expand_vec_perm): Avoid vector mode punning\n+\tSUBREGs in SET_DEST.\n+\t* expmed.c (store_bit_field_1): Likewise.\n+\t* config/i386/sse.md (movdi_to_sse, vec_pack_sfix_trunc_v2df,\n+\tvec_pack_sfix_v2df, vec_shl_<mode>, vec_shr_<mode>,\n+\tvec_interleave_high<mode>, vec_interleave_low<mode>): Likewise.\n+\t* config/i386/i386.c (ix86_expand_vector_move_misalign,\n+\tix86_expand_sse_movcc, ix86_expand_int_vcond, ix86_expand_vec_perm,\n+\tix86_expand_sse_unpack, ix86_expand_args_builtin,\n+\tix86_expand_vector_init_duplicate, ix86_expand_vector_set,\n+\temit_reduc_half, expand_vec_perm_blend, expand_vec_perm_pshufb,\n+\texpand_vec_perm_interleave2, expand_vec_perm_pshufb2,\n+\texpand_vec_perm_vpshufb2_vpermq,\n+\texpand_vec_perm_vpshufb2_vpermq_even_odd, expand_vec_perm_even_odd_1,\n+\texpand_vec_perm_broadcast_1, expand_vec_perm_vpshufb4_vpermq2,\n+\tix86_expand_sse2_mulv4si3, ix86_expand_pinsr): Likewise.\n+\t(expand_vec_perm_palignr): Likewise.  Modify a copy of *d rather\n+\tthan *d itself.\n+\n 2013-10-31  Uros Bizjak  <ubizjak@gmail.com>\n \n \t* config/i386/i386.c (ix86_expand_sse2_abs): Rename function arguments."}, {"sha": "902e169960424d1f14ab478f9a71300c1225b887", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 215, "deletions": 105, "changes": 320, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d8c84975e697eea2e306f80cdb21292331fd69d7/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d8c84975e697eea2e306f80cdb21292331fd69d7/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=d8c84975e697eea2e306f80cdb21292331fd69d7", "patch": "@@ -16803,6 +16803,8 @@ ix86_expand_vector_move_misalign (enum machine_mode mode, rtx operands[])\n \t}\n       else\n         {\n+\t  rtx t;\n+\n \t  if (TARGET_AVX\n \t      || TARGET_SSE_UNALIGNED_LOAD_OPTIMAL\n \t      || TARGET_SSE_PACKED_SINGLE_INSN_OPTIMAL\n@@ -16821,18 +16823,22 @@ ix86_expand_vector_move_misalign (enum machine_mode mode, rtx operands[])\n \t      return;\n             }\n \n+\t  if (mode != V4SFmode)\n+\t    t = gen_reg_rtx (V4SFmode);\n+\t  else\n+\t    t = op0;\n+\t    \n \t  if (TARGET_SSE_PARTIAL_REG_DEPENDENCY)\n-\t    emit_move_insn (op0, CONST0_RTX (mode));\n+\t    emit_move_insn (t, CONST0_RTX (V4SFmode));\n \t  else\n-\t    emit_clobber (op0);\n-\n-\t  if (mode != V4SFmode)\n-\t    op0 = gen_lowpart (V4SFmode, op0);\n+\t    emit_clobber (t);\n \n \t  m = adjust_address (op1, V2SFmode, 0);\n-\t  emit_insn (gen_sse_loadlps (op0, op0, m));\n+\t  emit_insn (gen_sse_loadlps (t, t, m));\n \t  m = adjust_address (op1, V2SFmode, 8);\n-\t  emit_insn (gen_sse_loadhps (op0, op0, m));\n+\t  emit_insn (gen_sse_loadhps (t, t, m));\n+\t  if (mode != V4SFmode)\n+\t    emit_move_insn (op0, gen_lowpart (mode, t));\n \t}\n     }\n   else if (MEM_P (op0))\n@@ -20473,6 +20479,7 @@ ix86_expand_sse_movcc (rtx dest, rtx cmp, rtx op_true, rtx op_false)\n   else\n     {\n       rtx (*gen) (rtx, rtx, rtx, rtx) = NULL;\n+      rtx d = dest;\n \n       if (!nonimmediate_operand (op_true, mode))\n \top_true = force_reg (mode, op_true);\n@@ -20496,7 +20503,8 @@ ix86_expand_sse_movcc (rtx dest, rtx cmp, rtx op_true, rtx op_false)\n \t  if (TARGET_SSE4_1)\n \t    {\n \t      gen = gen_sse4_1_pblendvb;\n-\t      dest = gen_lowpart (V16QImode, dest);\n+\t      if (mode != V16QImode)\n+\t\td = gen_reg_rtx (V16QImode);\n \t      op_false = gen_lowpart (V16QImode, op_false);\n \t      op_true = gen_lowpart (V16QImode, op_true);\n \t      cmp = gen_lowpart (V16QImode, cmp);\n@@ -20517,7 +20525,8 @@ ix86_expand_sse_movcc (rtx dest, rtx cmp, rtx op_true, rtx op_false)\n \t  if (TARGET_AVX2)\n \t    {\n \t      gen = gen_avx2_pblendvb;\n-\t      dest = gen_lowpart (V32QImode, dest);\n+\t      if (mode != V32QImode)\n+\t\td = gen_reg_rtx (V32QImode);\n \t      op_false = gen_lowpart (V32QImode, op_false);\n \t      op_true = gen_lowpart (V32QImode, op_true);\n \t      cmp = gen_lowpart (V32QImode, cmp);\n@@ -20528,7 +20537,11 @@ ix86_expand_sse_movcc (rtx dest, rtx cmp, rtx op_true, rtx op_false)\n \t}\n \n       if (gen != NULL)\n-\temit_insn (gen (dest, op_false, op_true, cmp));\n+\t{\n+\t  emit_insn (gen (d, op_false, op_true, cmp));\n+\t  if (d != dest)\n+\t    emit_move_insn (dest, gen_lowpart (GET_MODE (dest), d));\n+\t}\n       else\n \t{\n \t  op_true = force_reg (mode, op_true);\n@@ -20849,8 +20862,7 @@ ix86_expand_int_vcond (rtx operands[])\n   else\n     {\n       gcc_assert (GET_MODE_SIZE (data_mode) == GET_MODE_SIZE (mode));\n-      x = ix86_expand_sse_cmp (gen_lowpart (mode, operands[0]),\n-\t\t\t       code, cop0, cop1,\n+      x = ix86_expand_sse_cmp (gen_reg_rtx (mode), code, cop0, cop1,\n \t\t\t       operands[1+negate], operands[2-negate]);\n       x = gen_lowpart (data_mode, x);\n     }\n@@ -20869,7 +20881,7 @@ ix86_expand_vec_perm (rtx operands[])\n   rtx op0 = operands[1];\n   rtx op1 = operands[2];\n   rtx mask = operands[3];\n-  rtx t1, t2, t3, t4, vt, vt2, vec[32];\n+  rtx t1, t2, t3, t4, t5, t6, t7, t8, vt, vt2, vec[32];\n   enum machine_mode mode = GET_MODE (op0);\n   enum machine_mode maskmode = GET_MODE (mask);\n   int w, e, i;\n@@ -20937,7 +20949,7 @@ ix86_expand_vec_perm (rtx operands[])\n \n \t  /* Continue as if V8SImode (resp. V32QImode) was used initially.  */\n \t  operands[3] = mask = t1;\n-\t  target = gen_lowpart (mode, target);\n+\t  target = gen_reg_rtx (mode);\n \t  op0 = gen_lowpart (mode, op0);\n \t  op1 = gen_lowpart (mode, op1);\n \t}\n@@ -20949,7 +20961,12 @@ ix86_expand_vec_perm (rtx operands[])\n \t     the high bits of the shuffle elements.  No need for us to\n \t     perform an AND ourselves.  */\n \t  if (one_operand_shuffle)\n-\t    emit_insn (gen_avx2_permvarv8si (target, op0, mask));\n+\t    {\n+\t      emit_insn (gen_avx2_permvarv8si (target, op0, mask));\n+\t      if (target != operands[0])\n+\t\temit_move_insn (operands[0],\n+\t\t\t\tgen_lowpart (GET_MODE (operands[0]), target));\n+\t    }\n \t  else\n \t    {\n \t      t1 = gen_reg_rtx (V8SImode);\n@@ -21022,13 +21039,13 @@ ix86_expand_vec_perm (rtx operands[])\n \t     stands for other 12 bytes.  */\n \t  /* The bit whether element is from the same lane or the other\n \t     lane is bit 4, so shift it up by 3 to the MSB position.  */\n-\t  emit_insn (gen_ashlv4di3 (gen_lowpart (V4DImode, t1),\n-\t\t\t\t    gen_lowpart (V4DImode, mask),\n+\t  t5 = gen_reg_rtx (V4DImode);\n+\t  emit_insn (gen_ashlv4di3 (t5, gen_lowpart (V4DImode, mask),\n \t\t\t\t    GEN_INT (3)));\n \t  /* Clear MSB bits from the mask just in case it had them set.  */\n \t  emit_insn (gen_avx2_andnotv32qi3 (t2, vt, mask));\n \t  /* After this t1 will have MSB set for elements from other lane.  */\n-\t  emit_insn (gen_xorv32qi3 (t1, t1, vt2));\n+\t  emit_insn (gen_xorv32qi3 (t1, gen_lowpart (V32QImode, t5), vt2));\n \t  /* Clear bits other than MSB.  */\n \t  emit_insn (gen_andv32qi3 (t1, t1, vt));\n \t  /* Or in the lower bits from mask into t3.  */\n@@ -21037,8 +21054,8 @@ ix86_expand_vec_perm (rtx operands[])\n \t     lane.  */\n \t  emit_insn (gen_xorv32qi3 (t1, t1, vt));\n \t  /* Swap 128-bit lanes in t3.  */\n-\t  emit_insn (gen_avx2_permv4di_1 (gen_lowpart (V4DImode, t3),\n-\t\t\t\t\t  gen_lowpart (V4DImode, t3),\n+\t  t6 = gen_reg_rtx (V4DImode);\n+\t  emit_insn (gen_avx2_permv4di_1 (t6, gen_lowpart (V4DImode, t3),\n \t\t\t\t\t  const2_rtx, GEN_INT (3),\n \t\t\t\t\t  const0_rtx, const1_rtx));\n \t  /* And or in the lower bits from mask into t1.  */\n@@ -21048,36 +21065,43 @@ ix86_expand_vec_perm (rtx operands[])\n \t      /* Each of these shuffles will put 0s in places where\n \t\t element from the other 128-bit lane is needed, otherwise\n \t\t will shuffle in the requested value.  */\n-\t      emit_insn (gen_avx2_pshufbv32qi3 (t3, op0, t3));\n+\t      emit_insn (gen_avx2_pshufbv32qi3 (t3, op0,\n+\t\t\t\t\t\tgen_lowpart (V32QImode, t6)));\n \t      emit_insn (gen_avx2_pshufbv32qi3 (t1, op0, t1));\n \t      /* For t3 the 128-bit lanes are swapped again.  */\n-\t      emit_insn (gen_avx2_permv4di_1 (gen_lowpart (V4DImode, t3),\n-\t\t\t\t\t      gen_lowpart (V4DImode, t3),\n+\t      t7 = gen_reg_rtx (V4DImode);\n+\t      emit_insn (gen_avx2_permv4di_1 (t7, gen_lowpart (V4DImode, t3),\n \t\t\t\t\t      const2_rtx, GEN_INT (3),\n \t\t\t\t\t      const0_rtx, const1_rtx));\n \t      /* And oring both together leads to the result.  */\n-\t      emit_insn (gen_iorv32qi3 (target, t1, t3));\n+\t      emit_insn (gen_iorv32qi3 (target, t1,\n+\t\t\t\t\tgen_lowpart (V32QImode, t7)));\n+\t      if (target != operands[0])\n+\t\temit_move_insn (operands[0],\n+\t\t\t\tgen_lowpart (GET_MODE (operands[0]), target));\n \t      return;\n \t    }\n \n \t  t4 = gen_reg_rtx (V32QImode);\n \t  /* Similarly to the above one_operand_shuffle code,\n \t     just for repeated twice for each operand.  merge_two:\n \t     code will merge the two results together.  */\n-\t  emit_insn (gen_avx2_pshufbv32qi3 (t4, op0, t3));\n-\t  emit_insn (gen_avx2_pshufbv32qi3 (t3, op1, t3));\n+\t  emit_insn (gen_avx2_pshufbv32qi3 (t4, op0,\n+\t\t\t\t\t    gen_lowpart (V32QImode, t6)));\n+\t  emit_insn (gen_avx2_pshufbv32qi3 (t3, op1,\n+\t\t\t\t\t    gen_lowpart (V32QImode, t6)));\n \t  emit_insn (gen_avx2_pshufbv32qi3 (t2, op0, t1));\n \t  emit_insn (gen_avx2_pshufbv32qi3 (t1, op1, t1));\n-\t  emit_insn (gen_avx2_permv4di_1 (gen_lowpart (V4DImode, t4),\n-\t\t\t\t\t  gen_lowpart (V4DImode, t4),\n+\t  t7 = gen_reg_rtx (V4DImode);\n+\t  emit_insn (gen_avx2_permv4di_1 (t7, gen_lowpart (V4DImode, t4),\n \t\t\t\t\t  const2_rtx, GEN_INT (3),\n \t\t\t\t\t  const0_rtx, const1_rtx));\n-\t  emit_insn (gen_avx2_permv4di_1 (gen_lowpart (V4DImode, t3),\n-\t\t\t\t\t  gen_lowpart (V4DImode, t3),\n+\t  t8 = gen_reg_rtx (V4DImode);\n+\t  emit_insn (gen_avx2_permv4di_1 (t8, gen_lowpart (V4DImode, t3),\n \t\t\t\t\t  const2_rtx, GEN_INT (3),\n \t\t\t\t\t  const0_rtx, const1_rtx));\n-\t  emit_insn (gen_iorv32qi3 (t4, t2, t4));\n-\t  emit_insn (gen_iorv32qi3 (t3, t1, t3));\n+\t  emit_insn (gen_iorv32qi3 (t4, t2, gen_lowpart (V32QImode, t7)));\n+\t  emit_insn (gen_iorv32qi3 (t3, t1, gen_lowpart (V32QImode, t8)));\n \t  t1 = t4;\n \t  t2 = t3;\n \t  goto merge_two;\n@@ -21146,15 +21170,24 @@ ix86_expand_vec_perm (rtx operands[])\n   /* The actual shuffle operations all operate on V16QImode.  */\n   op0 = gen_lowpart (V16QImode, op0);\n   op1 = gen_lowpart (V16QImode, op1);\n-  target = gen_lowpart (V16QImode, target);\n \n   if (TARGET_XOP)\n     {\n+      if (GET_MODE (target) != V16QImode)\n+\ttarget = gen_reg_rtx (V16QImode);\n       emit_insn (gen_xop_pperm (target, op0, op1, mask));\n+      if (target != operands[0])\n+\temit_move_insn (operands[0],\n+\t\t\tgen_lowpart (GET_MODE (operands[0]), target));\n     }\n   else if (one_operand_shuffle)\n     {\n+      if (GET_MODE (target) != V16QImode)\n+\ttarget = gen_reg_rtx (V16QImode);\n       emit_insn (gen_ssse3_pshufbv16qi3 (target, op0, mask));\n+      if (target != operands[0])\n+\temit_move_insn (operands[0],\n+\t\t\tgen_lowpart (GET_MODE (operands[0]), target));\n     }\n   else\n     {\n@@ -21194,14 +21227,19 @@ ix86_expand_vec_perm (rtx operands[])\n       mask = expand_simple_binop (maskmode, AND, mask, vt,\n \t\t\t\t  NULL_RTX, 0, OPTAB_DIRECT);\n \n-      xops[0] = gen_lowpart (mode, operands[0]);\n+      if (GET_MODE (target) != mode)\n+\ttarget = gen_reg_rtx (mode);\n+      xops[0] = target;\n       xops[1] = gen_lowpart (mode, t2);\n       xops[2] = gen_lowpart (mode, t1);\n       xops[3] = gen_rtx_EQ (maskmode, mask, vt);\n       xops[4] = mask;\n       xops[5] = vt;\n       ok = ix86_expand_int_vcond (xops);\n       gcc_assert (ok);\n+      if (target != operands[0])\n+\temit_move_insn (operands[0],\n+\t\t\tgen_lowpart (GET_MODE (operands[0]), target));\n     }\n }\n \n@@ -21280,10 +21318,10 @@ ix86_expand_sse_unpack (rtx dest, rtx src, bool unsigned_p, bool high_p)\n       else if (high_p)\n \t{\n \t  /* Shift higher 8 bytes to lower 8 bytes.  */\n-\t  tmp = gen_reg_rtx (imode);\n-\t  emit_insn (gen_sse2_lshrv1ti3 (gen_lowpart (V1TImode, tmp),\n-\t\t\t\t\t gen_lowpart (V1TImode, src),\n+\t  tmp = gen_reg_rtx (V1TImode);\n+\t  emit_insn (gen_sse2_lshrv1ti3 (tmp, gen_lowpart (V1TImode, src),\n \t\t\t\t\t GEN_INT (64)));\n+\t  tmp = gen_lowpart (imode, tmp);\n \t}\n       else\n \ttmp = src;\n@@ -21324,7 +21362,9 @@ ix86_expand_sse_unpack (rtx dest, rtx src, bool unsigned_p, bool high_p)\n \ttmp = ix86_expand_sse_cmp (gen_reg_rtx (imode), GT, CONST0_RTX (imode),\n \t\t\t\t   src, pc_rtx, pc_rtx);\n \n-      emit_insn (unpack (gen_lowpart (imode, dest), src, tmp));\n+      rtx tmp2 = gen_reg_rtx (imode);\n+      emit_insn (unpack (tmp2, src, tmp));\n+      emit_move_insn (dest, gen_lowpart (GET_MODE (dest), tmp2));\n     }\n }\n \n@@ -31967,8 +32007,8 @@ ix86_expand_args_builtin (const struct builtin_description *d,\n     }\n   else\n     {\n-      target = gen_reg_rtx (rmode);\n-      real_target = simplify_gen_subreg (tmode, target, rmode, 0);\n+      real_target = gen_reg_rtx (tmode);\n+      target = simplify_gen_subreg (rmode, real_target, tmode, 0);\n     }\n \n   for (i = 0; i < nargs; i++)\n@@ -36691,8 +36731,9 @@ ix86_expand_vector_init_duplicate (bool mmx_ok, enum machine_mode mode,\n \t  emit_move_insn (tmp1, gen_lowpart (SImode, val));\n \n \t  /* Insert the SImode value as low element of a V4SImode vector. */\n-\t  tmp2 = gen_lowpart (V4SImode, dperm.op0);\n+\t  tmp2 = gen_reg_rtx (V4SImode);\n \t  emit_insn (gen_vec_setv4si_0 (tmp2, CONST0_RTX (V4SImode), tmp1));\n+\t  emit_move_insn (dperm.op0, gen_lowpart (mode, tmp2));\n \n \t  ok = (expand_vec_perm_1 (&dperm)\n \t\t|| expand_vec_perm_broadcast_1 (&dperm));\n@@ -36722,9 +36763,10 @@ ix86_expand_vector_init_duplicate (bool mmx_ok, enum machine_mode mode,\n \t\t\t\t NULL_RTX, 1, OPTAB_LIB_WIDEN);\n \tval = expand_simple_binop (wsmode, IOR, val, x, x, 1, OPTAB_LIB_WIDEN);\n \n-\tx = gen_lowpart (wvmode, target);\n+\tx = gen_reg_rtx (wvmode);\n \tok = ix86_expand_vector_init_duplicate (mmx_ok, wvmode, x, val);\n \tgcc_assert (ok);\n+\temit_move_insn (target, gen_lowpart (GET_MODE (target), x));\n \treturn ok;\n       }\n \n@@ -37599,8 +37641,9 @@ ix86_expand_vector_set (bool mmx_ok, rtx target, rtx val, int elt)\n       else\n \t{\n \t  /* For SSE1, we have to reuse the V4SF code.  */\n-\t  ix86_expand_vector_set (false, gen_lowpart (V4SFmode, target),\n-\t\t\t\t  gen_lowpart (SFmode, val), elt);\n+\t  rtx t = gen_reg_rtx (V4SFmode);\n+\t  ix86_expand_vector_set (false, t, gen_lowpart (SFmode, val), elt);\n+\t  emit_move_insn (target, gen_lowpart (mode, t));\n \t}\n       return;\n \n@@ -37918,7 +37961,7 @@ ix86_expand_vector_extract (bool mmx_ok, rtx target, rtx vec, int elt)\n static void\n emit_reduc_half (rtx dest, rtx src, int i)\n {\n-  rtx tem;\n+  rtx tem, d = dest;\n   switch (GET_MODE (src))\n     {\n     case V4SFmode:\n@@ -37935,8 +37978,8 @@ emit_reduc_half (rtx dest, rtx src, int i)\n     case V8HImode:\n     case V4SImode:\n     case V2DImode:\n-      tem = gen_sse2_lshrv1ti3 (gen_lowpart (V1TImode, dest),\n-\t\t\t\tgen_lowpart (V1TImode, src),\n+      d = gen_reg_rtx (V1TImode);\n+      tem = gen_sse2_lshrv1ti3 (d, gen_lowpart (V1TImode, src),\n \t\t\t\tGEN_INT (i / 2));\n       break;\n     case V8SFmode:\n@@ -37957,19 +38000,26 @@ emit_reduc_half (rtx dest, rtx src, int i)\n     case V8SImode:\n     case V4DImode:\n       if (i == 256)\n-\ttem = gen_avx2_permv2ti (gen_lowpart (V4DImode, dest),\n-\t\t\t\t gen_lowpart (V4DImode, src),\n-\t\t\t\t gen_lowpart (V4DImode, src),\n-\t\t\t\t const1_rtx);\n+\t{\n+\t  if (GET_MODE (dest) != V4DImode)\n+\t    d = gen_reg_rtx (V4DImode);\n+\t  tem = gen_avx2_permv2ti (d, gen_lowpart (V4DImode, src),\n+\t\t\t\t   gen_lowpart (V4DImode, src),\n+\t\t\t\t   const1_rtx);\n+\t}\n       else\n-\ttem = gen_avx2_lshrv2ti3 (gen_lowpart (V2TImode, dest),\n-\t\t\t\t  gen_lowpart (V2TImode, src),\n-\t\t\t\t  GEN_INT (i / 2));\n+\t{\n+\t  d = gen_reg_rtx (V2TImode);\n+\t  tem = gen_avx2_lshrv2ti3 (d, gen_lowpart (V2TImode, src),\n+\t\t\t\t    GEN_INT (i / 2));\n+\t}\n       break;\n     default:\n       gcc_unreachable ();\n     }\n   emit_insn (tem);\n+  if (d != dest)\n+    emit_move_insn (dest, gen_lowpart (GET_MODE (dest), d));\n }\n \n /* Expand a vector reduction.  FN is the binary pattern to reduce;\n@@ -39462,6 +39512,8 @@ expand_vec_perm_blend (struct expand_vec_perm_d *d)\n \t      emit_insn (gen_sse4_1_pblendvb (target, op0, op1, vperm));\n \t    else\n \t      emit_insn (gen_avx2_pblendvb (target, op0, op1, vperm));\n+\t    if (target != d->target)\n+\t      emit_move_insn (d->target, gen_lowpart (d->vmode, target));\n \t    return true;\n \t  }\n \n@@ -39471,7 +39523,7 @@ expand_vec_perm_blend (struct expand_vec_perm_d *d)\n       /* FALLTHRU */\n \n     do_subreg:\n-      target = gen_lowpart (vmode, target);\n+      target = gen_reg_rtx (vmode);\n       op0 = gen_lowpart (vmode, op0);\n       op1 = gen_lowpart (vmode, op1);\n       break;\n@@ -39525,7 +39577,7 @@ expand_vec_perm_blend (struct expand_vec_perm_d *d)\n \n \t\tvmode = V32QImode;\n \t\tnelt = 32;\n-\t\ttarget = gen_lowpart (vmode, target);\n+\t\ttarget = gen_reg_rtx (vmode);\n \t\top0 = gen_lowpart (vmode, op0);\n \t\top1 = gen_lowpart (vmode, op1);\n \t\tgoto finish_pblendvb;\n@@ -39558,6 +39610,8 @@ expand_vec_perm_blend (struct expand_vec_perm_d *d)\n   x = gen_rtx_VEC_MERGE (vmode, op1, op0, GEN_INT (mask));\n   x = gen_rtx_SET (VOIDmode, target, x);\n   emit_insn (x);\n+  if (target != d->target)\n+    emit_move_insn (d->target, gen_lowpart (d->vmode, target));\n \n   return true;\n }\n@@ -39663,13 +39717,17 @@ expand_vec_perm_pshufb (struct expand_vec_perm_d *d)\n \n \t      /* Use vperm2i128 insn.  The pattern uses\n \t\t V4DImode instead of V2TImode.  */\n-\t      target = gen_lowpart (V4DImode, d->target);\n+\t      target = d->target;\n+\t      if (d->vmode != V4DImode)\n+\t\ttarget = gen_reg_rtx (V4DImode);\n \t      op0 = gen_lowpart (V4DImode, d->op0);\n \t      op1 = gen_lowpart (V4DImode, d->op1);\n \t      rperm[0]\n \t\t= GEN_INT (((d->perm[0] & (nelt / 2)) ? 1 : 0)\n \t\t\t   || ((d->perm[nelt / 2] & (nelt / 2)) ? 2 : 0));\n \t      emit_insn (gen_avx2_permv2ti (target, op0, op1, rperm[0]));\n+\t      if (target != d->target)\n+\t\temit_move_insn (d->target, gen_lowpart (d->vmode, target));\n \t      return true;\n \t    }\n \t  return false;\n@@ -39704,9 +39762,15 @@ expand_vec_perm_pshufb (struct expand_vec_perm_d *d)\n \t\t    perm[i] = (d->perm[i * nelt / 4] * 4 / nelt) & 3;\n \t\t  if (d->testing_p)\n \t\t    return true;\n-\t\t  return expand_vselect (gen_lowpart (V4DImode, d->target),\n-\t\t\t\t\t gen_lowpart (V4DImode, d->op0),\n-\t\t\t\t\t perm, 4, false);\n+\t\t  target = gen_reg_rtx (V4DImode);\n+\t\t  if (expand_vselect (target, gen_lowpart (V4DImode, d->op0),\n+\t\t\t\t      perm, 4, false))\n+\t\t    {\n+\t\t      emit_move_insn (d->target,\n+\t\t\t\t      gen_lowpart (d->vmode, target));\n+\t\t      return true;\n+\t\t    }\n+\t\t  return false;\n \t\t}\n \n \t      /* Next see if vpermd can be used.  */\n@@ -39758,7 +39822,9 @@ expand_vec_perm_pshufb (struct expand_vec_perm_d *d)\n \t\t\t\tgen_rtvec_v (GET_MODE_NUNITS (vmode), rperm));\n   vperm = force_reg (vmode, vperm);\n \n-  target = gen_lowpart (vmode, d->target);\n+  target = d->target;\n+  if (d->vmode != vmode)\n+    target = gen_reg_rtx (vmode);\n   op0 = gen_lowpart (vmode, d->op0);\n   if (d->one_operand_p)\n     {\n@@ -39776,6 +39842,8 @@ expand_vec_perm_pshufb (struct expand_vec_perm_d *d)\n       op1 = gen_lowpart (vmode, d->op1);\n       emit_insn (gen_xop_pperm (target, op0, op1, vperm));\n     }\n+  if (target != d->target)\n+    emit_move_insn (d->target, gen_lowpart (d->vmode, target));\n \n   return true;\n }\n@@ -39975,7 +40043,8 @@ expand_vec_perm_palignr (struct expand_vec_perm_d *d)\n   unsigned i, nelt = d->nelt;\n   unsigned min, max;\n   bool in_order, ok;\n-  rtx shift;\n+  rtx shift, target;\n+  struct expand_vec_perm_d dcopy;\n \n   /* Even with AVX, palignr only operates on 128-bit vectors.  */\n   if (!TARGET_SSSE3 || GET_MODE_SIZE (d->vmode) != 16)\n@@ -39998,29 +40067,33 @@ expand_vec_perm_palignr (struct expand_vec_perm_d *d)\n   if (d->testing_p)\n     return true;\n \n+  dcopy = *d;\n   shift = GEN_INT (min * GET_MODE_BITSIZE (GET_MODE_INNER (d->vmode)));\n-  emit_insn (gen_ssse3_palignrti (gen_lowpart (TImode, d->target),\n-\t\t\t\t  gen_lowpart (TImode, d->op1),\n+  target = gen_reg_rtx (TImode);\n+  emit_insn (gen_ssse3_palignrti (target, gen_lowpart (TImode, d->op1),\n \t\t\t\t  gen_lowpart (TImode, d->op0), shift));\n \n-  d->op0 = d->op1 = d->target;\n-  d->one_operand_p = true;\n+  dcopy.op0 = dcopy.op1 = gen_lowpart (d->vmode, target);\n+  dcopy.one_operand_p = true;\n \n   in_order = true;\n   for (i = 0; i < nelt; ++i)\n     {\n-      unsigned e = d->perm[i] - min;\n+      unsigned e = dcopy.perm[i] - min;\n       if (e != i)\n \tin_order = false;\n-      d->perm[i] = e;\n+      dcopy.perm[i] = e;\n     }\n \n   /* Test for the degenerate case where the alignment by itself\n      produces the desired permutation.  */\n   if (in_order)\n-    return true;\n+    {\n+      emit_move_insn (d->target, dcopy.op0);\n+      return true;\n+    }\n \n-  ok = expand_vec_perm_1 (d);\n+  ok = expand_vec_perm_1 (&dcopy);\n   gcc_assert (ok);\n \n   return ok;\n@@ -40274,10 +40347,10 @@ expand_vec_perm_interleave2 (struct expand_vec_perm_d *d)\n       else\n \tdfinal.perm[i] = e;\n     }\n-  dfinal.op0 = gen_reg_rtx (dfinal.vmode);\n+  dremap.target = gen_reg_rtx (dremap.vmode);\n+  dfinal.op0 = gen_lowpart (dfinal.vmode, dremap.target);\n   dfinal.op1 = dfinal.op0;\n   dfinal.one_operand_p = true;\n-  dremap.target = dfinal.op0;\n \n   /* Test if the final remap can be done with a single insn.  For V4SFmode or\n      V4SImode this *will* succeed.  For V8HImode or V16QImode it may not.  */\n@@ -40294,7 +40367,6 @@ expand_vec_perm_interleave2 (struct expand_vec_perm_d *d)\n \n   if (dremap.vmode != dfinal.vmode)\n     {\n-      dremap.target = gen_lowpart (dremap.vmode, dremap.target);\n       dremap.op0 = gen_lowpart (dremap.vmode, dremap.op0);\n       dremap.op1 = gen_lowpart (dremap.vmode, dremap.op1);\n     }\n@@ -40745,8 +40817,12 @@ expand_vec_perm_pshufb2 (struct expand_vec_perm_d *d)\n   op = gen_lowpart (V16QImode, d->op1);\n   emit_insn (gen_ssse3_pshufbv16qi3 (h, op, vperm));\n \n-  op = gen_lowpart (V16QImode, d->target);\n+  op = d->target;\n+  if (d->vmode != V16QImode)\n+    op = gen_reg_rtx (V16QImode);\n   emit_insn (gen_iorv16qi3 (op, l, h));\n+  if (op != d->target)\n+    emit_move_insn (d->target, gen_lowpart (d->vmode, op));\n \n   return true;\n }\n@@ -40812,8 +40888,12 @@ expand_vec_perm_vpshufb2_vpermq (struct expand_vec_perm_d *d)\n   op = gen_lowpart (V32QImode, d->op0);\n   emit_insn (gen_avx2_pshufbv32qi3 (l, op, vperm));\n \n-  op = gen_lowpart (V32QImode, d->target);\n+  op = d->target;\n+  if (d->vmode != V32QImode)\n+    op = gen_reg_rtx (V32QImode);\n   emit_insn (gen_iorv32qi3 (op, l, gen_lowpart (V32QImode, hp)));\n+  if (op != d->target)\n+    emit_move_insn (d->target, gen_lowpart (d->vmode, op));\n \n   return true;\n }\n@@ -40889,10 +40969,11 @@ expand_vec_perm_vpshufb2_vpermq_even_odd (struct expand_vec_perm_d *d)\n   emit_insn (gen_iorv32qi3 (ior, l, h));\n \n   /* Permute the V4DImode quarters using { 0, 2, 1, 3 } permutation.  */\n-  op = gen_lowpart (V4DImode, d->target);\n+  op = gen_reg_rtx (V4DImode);\n   ior = gen_lowpart (V4DImode, ior);\n   emit_insn (gen_avx2_permv4di_1 (op, ior, const0_rtx, const2_rtx,\n \t\t\t\t  const1_rtx, GEN_INT (3)));\n+  emit_move_insn (d->target, gen_lowpart (d->vmode, op));\n \n   return true;\n }\n@@ -40903,7 +40984,7 @@ expand_vec_perm_vpshufb2_vpermq_even_odd (struct expand_vec_perm_d *d)\n static bool\n expand_vec_perm_even_odd_1 (struct expand_vec_perm_d *d, unsigned odd)\n {\n-  rtx t1, t2, t3;\n+  rtx t1, t2, t3, t4, t5;\n \n   switch (d->vmode)\n     {\n@@ -41015,10 +41096,17 @@ expand_vec_perm_even_odd_1 (struct expand_vec_perm_d *d, unsigned odd)\n \t{\n \t  struct expand_vec_perm_d d_copy = *d;\n \t  d_copy.vmode = V4DFmode;\n-\t  d_copy.target = gen_lowpart (V4DFmode, d->target);\n+\t  d_copy.target = gen_reg_rtx (V4DFmode);\n \t  d_copy.op0 = gen_lowpart (V4DFmode, d->op0);\n \t  d_copy.op1 = gen_lowpart (V4DFmode, d->op1);\n-\t  return expand_vec_perm_even_odd_1 (&d_copy, odd);\n+\t  if (expand_vec_perm_even_odd_1 (&d_copy, odd))\n+\t    {\n+\t      if (!d->testing_p)\n+\t\temit_move_insn (d->target,\n+\t\t\t\tgen_lowpart (V4DImode, d_copy.target));\n+\t      return true;\n+\t    }\n+\t  return false;\n \t}\n \n       t1 = gen_reg_rtx (V4DImode);\n@@ -41041,44 +41129,51 @@ expand_vec_perm_even_odd_1 (struct expand_vec_perm_d *d, unsigned odd)\n \t{\n \t  struct expand_vec_perm_d d_copy = *d;\n \t  d_copy.vmode = V8SFmode;\n-\t  d_copy.target = gen_lowpart (V8SFmode, d->target);\n+\t  d_copy.target = gen_reg_rtx (V8SFmode);\n \t  d_copy.op0 = gen_lowpart (V8SFmode, d->op0);\n \t  d_copy.op1 = gen_lowpart (V8SFmode, d->op1);\n-\t  return expand_vec_perm_even_odd_1 (&d_copy, odd);\n+\t  if (expand_vec_perm_even_odd_1 (&d_copy, odd))\n+\t    {\n+\t      if (!d->testing_p)\n+\t\temit_move_insn (d->target,\n+\t\t\t\tgen_lowpart (V8SImode, d_copy.target));\n+\t      return true;\n+\t    }\n+\t  return false;\n \t}\n \n       t1 = gen_reg_rtx (V8SImode);\n       t2 = gen_reg_rtx (V8SImode);\n+      t3 = gen_reg_rtx (V4DImode);\n+      t4 = gen_reg_rtx (V4DImode);\n+      t5 = gen_reg_rtx (V4DImode);\n \n       /* Shuffle the lanes around into\n \t { 0 1 2 3 8 9 a b } and { 4 5 6 7 c d e f }.  */\n-      emit_insn (gen_avx2_permv2ti (gen_lowpart (V4DImode, t1),\n-\t\t\t\t    gen_lowpart (V4DImode, d->op0),\n+      emit_insn (gen_avx2_permv2ti (t3, gen_lowpart (V4DImode, d->op0),\n \t\t\t\t    gen_lowpart (V4DImode, d->op1),\n \t\t\t\t    GEN_INT (0x20)));\n-      emit_insn (gen_avx2_permv2ti (gen_lowpart (V4DImode, t2),\n-\t\t\t\t    gen_lowpart (V4DImode, d->op0),\n+      emit_insn (gen_avx2_permv2ti (t4, gen_lowpart (V4DImode, d->op0),\n \t\t\t\t    gen_lowpart (V4DImode, d->op1),\n \t\t\t\t    GEN_INT (0x31)));\n \n       /* Swap the 2nd and 3rd position in each lane into\n \t { 0 2 1 3 8 a 9 b } and { 4 6 5 7 c e d f }.  */\n-      emit_insn (gen_avx2_pshufdv3 (t1, t1,\n+      emit_insn (gen_avx2_pshufdv3 (t1, gen_lowpart (V8SImode, t3),\n \t\t\t\t    GEN_INT (2 * 4 + 1 * 16 + 3 * 64)));\n-      emit_insn (gen_avx2_pshufdv3 (t2, t2,\n+      emit_insn (gen_avx2_pshufdv3 (t2, gen_lowpart (V8SImode, t4),\n \t\t\t\t    GEN_INT (2 * 4 + 1 * 16 + 3 * 64)));\n \n       /* Now an vpunpck[lh]qdq will produce\n \t { 0 2 4 6 8 a c e } resp. { 1 3 5 7 9 b d f }.  */\n       if (odd)\n-\tt3 = gen_avx2_interleave_highv4di (gen_lowpart (V4DImode, d->target),\n-\t\t\t\t\t   gen_lowpart (V4DImode, t1),\n+\tt3 = gen_avx2_interleave_highv4di (t5, gen_lowpart (V4DImode, t1),\n \t\t\t\t\t   gen_lowpart (V4DImode, t2));\n       else\n-\tt3 = gen_avx2_interleave_lowv4di (gen_lowpart (V4DImode, d->target),\n-\t\t\t\t\t  gen_lowpart (V4DImode, t1),\n+\tt3 = gen_avx2_interleave_lowv4di (t5, gen_lowpart (V4DImode, t1),\n \t\t\t\t\t  gen_lowpart (V4DImode, t2));\n       emit_insn (t3);\n+      emit_move_insn (d->target, gen_lowpart (V8SImode, t5));\n       break;\n \n     default:\n@@ -41116,7 +41211,7 @@ expand_vec_perm_broadcast_1 (struct expand_vec_perm_d *d)\n   unsigned elt = d->perm[0], nelt2 = d->nelt / 2;\n   enum machine_mode vmode = d->vmode;\n   unsigned char perm2[4];\n-  rtx op0 = d->op0;\n+  rtx op0 = d->op0, dest;\n   bool ok;\n \n   switch (vmode)\n@@ -41162,9 +41257,11 @@ expand_vec_perm_broadcast_1 (struct expand_vec_perm_d *d)\n       while (vmode != V4SImode);\n \n       memset (perm2, elt, 4);\n-      ok = expand_vselect (gen_lowpart (V4SImode, d->target), op0, perm2, 4,\n-\t\t\t   d->testing_p);\n+      dest = gen_reg_rtx (V4SImode);\n+      ok = expand_vselect (dest, op0, perm2, 4, d->testing_p);\n       gcc_assert (ok);\n+      if (!d->testing_p)\n+\temit_move_insn (d->target, gen_lowpart (d->vmode, dest));\n       return true;\n \n     case V32QImode:\n@@ -41306,8 +41403,12 @@ expand_vec_perm_vpshufb4_vpermq2 (struct expand_vec_perm_d *d)\n     }\n \n   gcc_assert (l[0] && l[1]);\n-  op = gen_lowpart (V32QImode, d->target);\n+  op = d->target;\n+  if (d->vmode != V32QImode)\n+    op = gen_reg_rtx (V32QImode);\n   emit_insn (gen_iorv32qi3 (op, l[0], l[1]));\n+  if (op != d->target)\n+    emit_move_insn (d->target, gen_lowpart (d->vmode, op));\n   return true;\n }\n \n@@ -41875,7 +41976,9 @@ ix86_expand_mul_widen_hilo (rtx dest, rtx op1, rtx op2,\n \t\t\t op1, op2, NULL_RTX, uns_p, OPTAB_DIRECT);\n       gcc_assert (t1 && t2);\n \n-      ix86_expand_vec_interleave (gen_lowpart (mode, dest), t1, t2, high_p);\n+      t3 = gen_reg_rtx (mode);\n+      ix86_expand_vec_interleave (t3, t1, t2, high_p);\n+      emit_move_insn (dest, gen_lowpart (wmode, t3));\n       break;\n \n     case V16QImode:\n@@ -41896,14 +41999,14 @@ ix86_expand_mul_widen_hilo (rtx dest, rtx op1, rtx op2,\n void\n ix86_expand_sse2_mulv4si3 (rtx op0, rtx op1, rtx op2)\n {\n-  rtx res_1, res_2;\n+  rtx res_1, res_2, res_3, res_4;\n \n   res_1 = gen_reg_rtx (V4SImode);\n   res_2 = gen_reg_rtx (V4SImode);\n-  ix86_expand_mul_widen_evenodd (gen_lowpart (V2DImode, res_1),\n-\t\t\t\t op1, op2, true, false);\n-  ix86_expand_mul_widen_evenodd (gen_lowpart (V2DImode, res_2),\n-\t\t\t\t op1, op2, true, true);\n+  res_3 = gen_reg_rtx (V2DImode);\n+  res_4 = gen_reg_rtx (V2DImode);\n+  ix86_expand_mul_widen_evenodd (res_3, op1, op2, true, false);\n+  ix86_expand_mul_widen_evenodd (res_4, op1, op2, true, true);\n \n   /* Move the results in element 2 down to element 1; we don't care\n      what goes in elements 2 and 3.  Then we can merge the parts\n@@ -41917,9 +42020,11 @@ ix86_expand_sse2_mulv4si3 (rtx op0, rtx op1, rtx op2)\n      In both cases the cost of the reformatting stall was too high\n      and the overall sequence slower.  */\n \n-  emit_insn (gen_sse2_pshufd_1 (res_1, res_1, const0_rtx, const2_rtx,\n+  emit_insn (gen_sse2_pshufd_1 (res_1, gen_lowpart (V4SImode, res_3),\n+\t\t\t\tconst0_rtx, const2_rtx,\n \t\t\t\tconst0_rtx, const0_rtx));\n-  emit_insn (gen_sse2_pshufd_1 (res_2, res_2, const0_rtx, const2_rtx,\n+  emit_insn (gen_sse2_pshufd_1 (res_2, gen_lowpart (V4SImode, res_4),\n+\t\t\t\tconst0_rtx, const2_rtx,\n \t\t\t\tconst0_rtx, const0_rtx));\n   res_1 = emit_insn (gen_vec_interleave_lowv4si (op0, res_1, res_2));\n \n@@ -42138,12 +42243,17 @@ ix86_expand_pinsr (rtx *operands)\n \t    return false;\n \t  }\n \n-\tdst = gen_lowpart (dstmode, dst);\n+\trtx d = dst;\n+\tif (GET_MODE (dst) != dstmode)\n+\t  d = gen_reg_rtx (dstmode);\n \tsrc = gen_lowpart (srcmode, src);\n \n \tpos /= size;\n \n-\temit_insn (pinsr (dst, dst, src, GEN_INT (1 << pos)));\n+\temit_insn (pinsr (d, gen_lowpart (dstmode, dst), src,\n+\t\t\t  GEN_INT (1 << pos)));\n+\tif (d != dst)\n+\t  emit_move_insn (dst, gen_lowpart (GET_MODE (dst), d));\n \treturn true;\n       }\n "}, {"sha": "7187035a6f2fe380ea41ae2eb6d425f46481bb0f", "filename": "gcc/config/i386/sse.md", "status": "modified", "additions": 38, "deletions": 27, "changes": 65, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d8c84975e697eea2e306f80cdb21292331fd69d7/gcc%2Fconfig%2Fi386%2Fsse.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d8c84975e697eea2e306f80cdb21292331fd69d7/gcc%2Fconfig%2Fi386%2Fsse.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fsse.md?ref=d8c84975e697eea2e306f80cdb21292331fd69d7", "patch": "@@ -800,10 +800,13 @@\n \t\t\t\t  gen_rtx_SUBREG (SImode, operands[1], 4)));\n       emit_insn (gen_vec_interleave_lowv4si (operands[0], operands[0],\n \t\t\t\t\t     operands[2]));\n-    }\n+   }\n  else if (memory_operand (operands[1], DImode))\n-   emit_insn (gen_vec_concatv2di (gen_lowpart (V2DImode, operands[0]),\n-\t\t\t\t  operands[1], const0_rtx));\n+   {\n+     rtx tmp = gen_reg_rtx (V2DImode);\n+     emit_insn (gen_vec_concatv2di (tmp, operands[1], const0_rtx));\n+     emit_move_insn (operands[0], gen_lowpart (V4SImode, tmp));\n+   }\n  else\n    gcc_unreachable ();\n })\n@@ -4208,7 +4211,7 @@\n    (match_operand:V2DF 2 \"nonimmediate_operand\")]\n   \"TARGET_SSE2\"\n {\n-  rtx tmp0, tmp1;\n+  rtx tmp0, tmp1, tmp2;\n \n   if (TARGET_AVX && !TARGET_PREFER_AVX128)\n     {\n@@ -4222,13 +4225,14 @@\n     {\n       tmp0 = gen_reg_rtx (V4SImode);\n       tmp1 = gen_reg_rtx (V4SImode);\n+      tmp2 = gen_reg_rtx (V2DImode);\n \n       emit_insn (gen_sse2_cvttpd2dq (tmp0, operands[1]));\n       emit_insn (gen_sse2_cvttpd2dq (tmp1, operands[2]));\n-      emit_insn\n-       (gen_vec_interleave_lowv2di (gen_lowpart (V2DImode, operands[0]),\n-\t\t\t\t    gen_lowpart (V2DImode, tmp0),\n-\t\t\t\t    gen_lowpart (V2DImode, tmp1)));\n+      emit_insn (gen_vec_interleave_lowv2di (tmp2,\n+\t\t\t\t\t     gen_lowpart (V2DImode, tmp0),\n+\t\t\t\t\t     gen_lowpart (V2DImode, tmp1)));\n+      emit_move_insn (operands[0], gen_lowpart (V4SImode, tmp2));\n     }\n   DONE;\n })\n@@ -4289,7 +4293,7 @@\n    (match_operand:V2DF 2 \"nonimmediate_operand\")]\n   \"TARGET_SSE2\"\n {\n-  rtx tmp0, tmp1;\n+  rtx tmp0, tmp1, tmp2;\n \n   if (TARGET_AVX && !TARGET_PREFER_AVX128)\n     {\n@@ -4303,13 +4307,14 @@\n     {\n       tmp0 = gen_reg_rtx (V4SImode);\n       tmp1 = gen_reg_rtx (V4SImode);\n+      tmp2 = gen_reg_rtx (V2DImode);\n \n       emit_insn (gen_sse2_cvtpd2dq (tmp0, operands[1]));\n       emit_insn (gen_sse2_cvtpd2dq (tmp1, operands[2]));\n-      emit_insn\n-       (gen_vec_interleave_lowv2di (gen_lowpart (V2DImode, operands[0]),\n-\t\t\t\t    gen_lowpart (V2DImode, tmp0),\n-\t\t\t\t    gen_lowpart (V2DImode, tmp1)));\n+      emit_insn (gen_vec_interleave_lowv2di (tmp2,\n+\t\t\t\t\t     gen_lowpart (V2DImode, tmp0),\n+\t\t\t\t\t     gen_lowpart (V2DImode, tmp1)));\n+      emit_move_insn (operands[0], gen_lowpart (V4SImode, tmp2));\n     }\n   DONE;\n })\n@@ -7328,14 +7333,16 @@\n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n (define_expand \"vec_shl_<mode>\"\n-  [(set (match_operand:VI_128 0 \"register_operand\")\n+  [(set (match_dup 3)\n \t(ashift:V1TI\n \t (match_operand:VI_128 1 \"register_operand\")\n-\t (match_operand:SI 2 \"const_0_to_255_mul_8_operand\")))]\n+\t (match_operand:SI 2 \"const_0_to_255_mul_8_operand\")))\n+   (set (match_operand:VI_128 0 \"register_operand\") (match_dup 4))]\n   \"TARGET_SSE2\"\n {\n-  operands[0] = gen_lowpart (V1TImode, operands[0]);\n   operands[1] = gen_lowpart (V1TImode, operands[1]);\n+  operands[3] = gen_reg_rtx (V1TImode);\n+  operands[4] = gen_lowpart (<MODE>mode, operands[3]);\n })\n \n (define_insn \"<sse2_avx2>_ashl<mode>3\"\n@@ -7365,14 +7372,16 @@\n    (set_attr \"mode\" \"<sseinsnmode>\")])\n \n (define_expand \"vec_shr_<mode>\"\n-  [(set (match_operand:VI_128 0 \"register_operand\")\n+  [(set (match_dup 3)\n \t(lshiftrt:V1TI\n \t (match_operand:VI_128 1 \"register_operand\")\n-\t (match_operand:SI 2 \"const_0_to_255_mul_8_operand\")))]\n+\t (match_operand:SI 2 \"const_0_to_255_mul_8_operand\")))\n+   (set (match_operand:VI_128 0 \"register_operand\") (match_dup 4))]\n   \"TARGET_SSE2\"\n {\n-  operands[0] = gen_lowpart (V1TImode, operands[0]);\n   operands[1] = gen_lowpart (V1TImode, operands[1]);\n+  operands[3] = gen_reg_rtx (V1TImode);\n+  operands[4] = gen_lowpart (<MODE>mode, operands[3]);\n })\n \n (define_insn \"<sse2_avx2>_lshr<mode>3\"\n@@ -8542,12 +8551,13 @@\n {\n   rtx t1 = gen_reg_rtx (<MODE>mode);\n   rtx t2 = gen_reg_rtx (<MODE>mode);\n+  rtx t3 = gen_reg_rtx (V4DImode);\n   emit_insn (gen_avx2_interleave_low<mode> (t1, operands[1], operands[2]));\n   emit_insn (gen_avx2_interleave_high<mode> (t2,  operands[1], operands[2]));\n-  emit_insn (gen_avx2_permv2ti\n-\t     (gen_lowpart (V4DImode, operands[0]),\n-\t      gen_lowpart (V4DImode, t1),\n-\t      gen_lowpart (V4DImode, t2), GEN_INT (1 + (3 << 4))));\n+  emit_insn (gen_avx2_permv2ti (t3, gen_lowpart (V4DImode, t1),\n+\t\t\t\tgen_lowpart (V4DImode, t2),\n+\t\t\t\tGEN_INT (1 + (3 << 4))));\n+  emit_move_insn (operands[0], gen_lowpart (<MODE>mode, t3));\n   DONE;\n })\n \n@@ -8559,12 +8569,13 @@\n {\n   rtx t1 = gen_reg_rtx (<MODE>mode);\n   rtx t2 = gen_reg_rtx (<MODE>mode);\n+  rtx t3 = gen_reg_rtx (V4DImode);\n   emit_insn (gen_avx2_interleave_low<mode> (t1, operands[1], operands[2]));\n   emit_insn (gen_avx2_interleave_high<mode> (t2, operands[1], operands[2]));\n-  emit_insn (gen_avx2_permv2ti\n-\t     (gen_lowpart (V4DImode, operands[0]),\n-\t      gen_lowpart (V4DImode, t1),\n-\t      gen_lowpart (V4DImode, t2), GEN_INT (0 + (2 << 4))));\n+  emit_insn (gen_avx2_permv2ti (t3, gen_lowpart (V4DImode, t1),\n+\t\t\t\tgen_lowpart (V4DImode, t2),\n+\t\t\t\tGEN_INT (0 + (2 << 4))));\n+  emit_move_insn (operands[0], gen_lowpart (<MODE>mode, t3));\n   DONE;\n })\n "}, {"sha": "59f81df5adff6038ad1f934b802b5b0439fe0d84", "filename": "gcc/expmed.c", "status": "modified", "additions": 21, "deletions": 6, "changes": 27, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d8c84975e697eea2e306f80cdb21292331fd69d7/gcc%2Fexpmed.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d8c84975e697eea2e306f80cdb21292331fd69d7/gcc%2Fexpmed.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexpmed.c?ref=d8c84975e697eea2e306f80cdb21292331fd69d7", "patch": "@@ -624,13 +624,28 @@ store_bit_field_1 (rtx str_rtx, unsigned HOST_WIDE_INT bitsize,\n \t  || (bitsize % BITS_PER_WORD == 0 && bitnum % BITS_PER_WORD == 0)))\n     {\n       /* Use the subreg machinery either to narrow OP0 to the required\n-\t words or to cope with mode punning between equal-sized modes.  */\n-      rtx sub = simplify_gen_subreg (fieldmode, op0, GET_MODE (op0),\n-\t\t\t\t     bitnum / BITS_PER_UNIT);\n-      if (sub)\n+\t words or to cope with mode punning between equal-sized modes.\n+\t In the latter case, use subreg on the rhs side, not lhs.  */\n+      rtx sub;\n+\n+      if (bitsize == GET_MODE_BITSIZE (GET_MODE (op0)))\n \t{\n-\t  emit_move_insn (sub, value);\n-\t  return true;\n+\t  sub = simplify_gen_subreg (GET_MODE (op0), value, fieldmode, 0);\n+\t  if (sub)\n+\t    {\n+\t      emit_move_insn (op0, sub);\n+\t      return true;\n+\t    }\n+\t}\n+      else\n+\t{\n+\t  sub = simplify_gen_subreg (fieldmode, op0, GET_MODE (op0),\n+\t\t\t\t     bitnum / BITS_PER_UNIT);\n+\t  if (sub)\n+\t    {\n+\t      emit_move_insn (sub, value);\n+\t      return true;\n+\t    }\n \t}\n     }\n "}, {"sha": "3755670af84218003560873a235e2a8b556740c5", "filename": "gcc/optabs.c", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/d8c84975e697eea2e306f80cdb21292331fd69d7/gcc%2Foptabs.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/d8c84975e697eea2e306f80cdb21292331fd69d7/gcc%2Foptabs.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Foptabs.c?ref=d8c84975e697eea2e306f80cdb21292331fd69d7", "patch": "@@ -6624,8 +6624,8 @@ expand_vec_perm (enum machine_mode mode, rtx v0, rtx v1, rtx sel, rtx target)\n \t  icode = direct_optab_handler (vec_perm_const_optab, qimode);\n \t  if (icode != CODE_FOR_nothing)\n \t    {\n-\t      tmp = expand_vec_perm_1 (icode, gen_lowpart (qimode, target),\n-\t\t\t\t       gen_lowpart (qimode, v0),\n+\t      tmp = mode != qimode ? gen_reg_rtx (qimode) : target;\n+\t      tmp = expand_vec_perm_1 (icode, tmp, gen_lowpart (qimode, v0),\n \t\t\t\t       gen_lowpart (qimode, v1), sel_qi);\n \t      if (tmp)\n \t\treturn gen_lowpart (mode, tmp);\n@@ -6674,7 +6674,7 @@ expand_vec_perm (enum machine_mode mode, rtx v0, rtx v1, rtx sel, rtx target)\n \t}\n       tmp = gen_rtx_CONST_VECTOR (qimode, vec);\n       sel = gen_lowpart (qimode, sel);\n-      sel = expand_vec_perm (qimode, sel, sel, tmp, NULL);\n+      sel = expand_vec_perm (qimode, gen_reg_rtx (qimode), sel, tmp, NULL);\n       gcc_assert (sel != NULL);\n \n       /* Add the byte offset to each byte element.  */\n@@ -6689,8 +6689,8 @@ expand_vec_perm (enum machine_mode mode, rtx v0, rtx v1, rtx sel, rtx target)\n       gcc_assert (sel_qi != NULL);\n     }\n \n-  tmp = expand_vec_perm_1 (icode, gen_lowpart (qimode, target),\n-\t\t\t   gen_lowpart (qimode, v0),\n+  tmp = mode != qimode ? gen_reg_rtx (qimode) : target;\n+  tmp = expand_vec_perm_1 (icode, tmp, gen_lowpart (qimode, v0),\n \t\t\t   gen_lowpart (qimode, v1), sel_qi);\n   if (tmp)\n     tmp = gen_lowpart (mode, tmp);"}]}