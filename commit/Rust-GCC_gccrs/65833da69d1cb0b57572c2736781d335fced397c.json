{"sha": "65833da69d1cb0b57572c2736781d335fced397c", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NjU4MzNkYTY5ZDFjYjBiNTc1NzJjMjczNjc4MWQzMzVmY2VkMzk3Yw==", "commit": {"author": {"name": "Richard Biener", "email": "rguenther@suse.de", "date": "2016-04-18T11:38:05Z"}, "committer": {"name": "Richard Biener", "email": "rguenth@gcc.gnu.org", "date": "2016-04-18T11:38:05Z"}, "message": "tree-ssa-pre.c (postorder, [...]): Make locals ...\n\n2016-04-18  Richard Biener  <rguenther@suse.de>\n\n\t* tree-ssa-pre.c (postorder, postorder_num): Make locals ...\n\t(compute_antic): ... here.  For partial antic use regular\n\tpostorder and scrap iteration.\n\t(compute_partial_antic_aux): Remove unused return value.\n\t(init_pre): Do not allocate postorder.\n\t(fini_pre): Do not free postorder.\n\nFrom-SVN: r235130", "tree": {"sha": "9cfb693518ae09e34852ac3d54dcf698cfa38b24", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/9cfb693518ae09e34852ac3d54dcf698cfa38b24"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/65833da69d1cb0b57572c2736781d335fced397c", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/65833da69d1cb0b57572c2736781d335fced397c", "html_url": "https://github.com/Rust-GCC/gccrs/commit/65833da69d1cb0b57572c2736781d335fced397c", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/65833da69d1cb0b57572c2736781d335fced397c/comments", "author": {"login": "rguenth", "id": 2046526, "node_id": "MDQ6VXNlcjIwNDY1MjY=", "avatar_url": "https://avatars.githubusercontent.com/u/2046526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rguenth", "html_url": "https://github.com/rguenth", "followers_url": "https://api.github.com/users/rguenth/followers", "following_url": "https://api.github.com/users/rguenth/following{/other_user}", "gists_url": "https://api.github.com/users/rguenth/gists{/gist_id}", "starred_url": "https://api.github.com/users/rguenth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rguenth/subscriptions", "organizations_url": "https://api.github.com/users/rguenth/orgs", "repos_url": "https://api.github.com/users/rguenth/repos", "events_url": "https://api.github.com/users/rguenth/events{/privacy}", "received_events_url": "https://api.github.com/users/rguenth/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "b269f47786ffff084e874cd09ac8d87f895a1db6", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b269f47786ffff084e874cd09ac8d87f895a1db6", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b269f47786ffff084e874cd09ac8d87f895a1db6"}], "stats": {"total": 81, "additions": 33, "deletions": 48}, "files": [{"sha": "51bc8d764a1fe0ddac1c9cd148815fe87496c583", "filename": "gcc/ChangeLog", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/65833da69d1cb0b57572c2736781d335fced397c/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/65833da69d1cb0b57572c2736781d335fced397c/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=65833da69d1cb0b57572c2736781d335fced397c", "patch": "@@ -1,3 +1,12 @@\n+2016-04-18  Richard Biener  <rguenther@suse.de>\n+\n+\t* tree-ssa-pre.c (postorder, postorder_num): Make locals ...\n+\t(compute_antic): ... here.  For partial antic use regular\n+\tpostorder and scrap iteration.\n+\t(compute_partial_antic_aux): Remove unused return value.\n+\t(init_pre): Do not allocate postorder.\n+\t(fini_pre): Do not free postorder.\n+\n 2016-04-18  Richard Biener  <rguenther@suse.de>\n \n         PR middle-end/37870"}, {"sha": "c0e3b807c059307ee77481fa3edda0d12e0cf4b2", "filename": "gcc/tree-ssa-pre.c", "status": "modified", "additions": 24, "deletions": 48, "changes": 72, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/65833da69d1cb0b57572c2736781d335fced397c/gcc%2Ftree-ssa-pre.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/65833da69d1cb0b57572c2736781d335fced397c/gcc%2Ftree-ssa-pre.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-ssa-pre.c?ref=65833da69d1cb0b57572c2736781d335fced397c", "patch": "@@ -432,10 +432,6 @@ typedef struct bb_bitmap_sets\n #define BB_LIVE_VOP_ON_EXIT(BB) ((bb_value_sets_t) ((BB)->aux))->vop_on_exit\n \n \n-/* Basic block list in postorder.  */\n-static int *postorder;\n-static int postorder_num;\n-\n /* This structure is used to keep track of statistics on what\n    optimization PRE was able to perform.  */\n static struct\n@@ -2209,11 +2205,10 @@ compute_antic_aux (basic_block block, bool block_has_abnormal_pred_edge)\n \t\t\t\t  - ANTIC_IN[BLOCK])\n \n */\n-static bool\n+static void\n compute_partial_antic_aux (basic_block block,\n \t\t\t   bool block_has_abnormal_pred_edge)\n {\n-  bool changed = false;\n   bitmap_set_t old_PA_IN;\n   bitmap_set_t PA_OUT;\n   edge e;\n@@ -2312,9 +2307,6 @@ compute_partial_antic_aux (basic_block block,\n \n   dependent_clean (PA_IN (block), ANTIC_IN (block));\n \n-  if (!bitmap_set_equal (old_PA_IN, PA_IN (block)))\n-    changed = true;\n-\n  maybe_dump_sets:\n   if (dump_file && (dump_flags & TDF_DETAILS))\n     {\n@@ -2327,7 +2319,6 @@ compute_partial_antic_aux (basic_block block,\n     bitmap_set_free (old_PA_IN);\n   if (PA_OUT)\n     bitmap_set_free (PA_OUT);\n-  return changed;\n }\n \n /* Compute ANTIC and partial ANTIC sets.  */\n@@ -2349,23 +2340,33 @@ compute_antic (void)\n \n   FOR_ALL_BB_FN (block, cfun)\n     {\n+      BB_VISITED (block) = 0;\n+\n       FOR_EACH_EDGE (e, ei, block->preds)\n \tif (e->flags & EDGE_ABNORMAL)\n \t  {\n \t    bitmap_set_bit (has_abnormal_preds, block->index);\n+\n+\t    /* We also anticipate nothing.  */\n+\t    BB_VISITED (block) = 1;\n \t    break;\n \t  }\n \n-      BB_VISITED (block) = 0;\n-\n       /* While we are here, give empty ANTIC_IN sets to each block.  */\n       ANTIC_IN (block) = bitmap_set_new ();\n-      PA_IN (block) = bitmap_set_new ();\n+      if (do_partial_partial)\n+\tPA_IN (block) = bitmap_set_new ();\n     }\n \n   /* At the exit block we anticipate nothing.  */\n   BB_VISITED (EXIT_BLOCK_PTR_FOR_FN (cfun)) = 1;\n \n+  /* For ANTIC computation we need a postorder that also guarantees that\n+     a block with a single successor is visited after its successor.\n+     RPO on the inverted CFG has this property.  */\n+  int *postorder = XNEWVEC (int, n_basic_blocks_for_fn (cfun));\n+  int postorder_num = inverted_post_order_compute (postorder);\n+\n   sbitmap worklist = sbitmap_alloc (last_basic_block_for_fn (cfun) + 1);\n   bitmap_ones (worklist);\n   while (changed)\n@@ -2403,39 +2404,21 @@ compute_antic (void)\n \n   if (do_partial_partial)\n     {\n-      bitmap_ones (worklist);\n-      num_iterations = 0;\n-      changed = true;\n-      while (changed)\n+      /* For partial antic we ignore backedges and thus we do not need\n+         to perform any iteration when we process blocks in postorder.  */\n+      postorder_num = pre_and_rev_post_order_compute (NULL, postorder, false);\n+      for (i = postorder_num - 1 ; i >= 0; i--)\n \t{\n-\t  if (dump_file && (dump_flags & TDF_DETAILS))\n-\t    fprintf (dump_file, \"Starting iteration %d\\n\", num_iterations);\n-\t  num_iterations++;\n-\t  changed = false;\n-\t  for (i = postorder_num - 1 ; i >= 0; i--)\n-\t    {\n-\t      if (bitmap_bit_p (worklist, postorder[i]))\n-\t\t{\n-\t\t  basic_block block = BASIC_BLOCK_FOR_FN (cfun, postorder[i]);\n-\t\t  bitmap_clear_bit (worklist, block->index);\n-\t\t  if (compute_partial_antic_aux (block,\n-\t\t\t\t\t\t bitmap_bit_p (has_abnormal_preds,\n-\t\t\t\t\t\t\t       block->index)))\n-\t\t    {\n-\t\t      FOR_EACH_EDGE (e, ei, block->preds)\n-\t\t\tbitmap_set_bit (worklist, e->src->index);\n-\t\t      changed = true;\n-\t\t    }\n-\t\t}\n-\t    }\n-\t  /* Theoretically possible, but *highly* unlikely.  */\n-\t  gcc_checking_assert (num_iterations < 500);\n+\t  basic_block block = BASIC_BLOCK_FOR_FN (cfun, postorder[i]);\n+\t  compute_partial_antic_aux (block,\n+\t\t\t\t     bitmap_bit_p (has_abnormal_preds,\n+\t\t\t\t\t\t   block->index));\n \t}\n-      statistics_histogram_event (cfun, \"compute_partial_antic iterations\",\n-\t\t\t\t  num_iterations);\n     }\n+\n   sbitmap_free (has_abnormal_preds);\n   sbitmap_free (worklist);\n+  free (postorder);\n }\n \n \n@@ -4694,12 +4677,6 @@ init_pre (void)\n   connect_infinite_loops_to_exit ();\n   memset (&pre_stats, 0, sizeof (pre_stats));\n \n-  /* For ANTIC computation we need a postorder that also guarantees that\n-     a block with a single successor is visited after its successor.\n-     RPO on the inverted CFG has this property.  */\n-  postorder = XNEWVEC (int, n_basic_blocks_for_fn (cfun));\n-  postorder_num = inverted_post_order_compute (postorder);\n-\n   alloc_aux_for_blocks (sizeof (struct bb_bitmap_sets));\n \n   calculate_dominance_info (CDI_DOMINATORS);\n@@ -4722,7 +4699,6 @@ init_pre (void)\n static void\n fini_pre ()\n {\n-  free (postorder);\n   value_expressions.release ();\n   BITMAP_FREE (inserted_exprs);\n   bitmap_obstack_release (&grand_bitmap_obstack);"}]}