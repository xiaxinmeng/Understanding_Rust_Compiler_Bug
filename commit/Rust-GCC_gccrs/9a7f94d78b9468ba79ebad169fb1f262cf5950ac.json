{"sha": "9a7f94d78b9468ba79ebad169fb1f262cf5950ac", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6OWE3Zjk0ZDc4Yjk0NjhiYTc5ZWJhZDE2OWZiMWYyNjJjZjU5NTBhYw==", "commit": {"author": {"name": "H.J. Lu", "email": "hongjiu.lu@intel.com", "date": "2014-01-17T15:23:58Z"}, "committer": {"name": "H.J. Lu", "email": "hjl@gcc.gnu.org", "date": "2014-01-17T15:23:58Z"}, "message": "Add X86_TUNE_AVOID_LEA_FOR_ADDR\n\nix86_split_lea_for_addr transforms a single LEA instruction into a\nseries of MOV and ADD instructions.  For\n\nlea 0x400(%edx, %ecx, 8), %edx\n\nwe get\n\nmov %ecx, %edx\nadd %ecx, %edx\nadd %ecx, %edx\nadd %ecx, %edx\nadd %ecx, %edx\nadd %ecx, %edx\nadd %ecx, %edx\nadd %ecx, %edx\nadd $0x400, %edx\n\nFor -mtune=intel, we want to turn on X86_TUNE_OPT_AGU, but avoid\nix86_split_lea_for_addr to optimize for both Haswell and Silvermont.\nThis patch adds X86_TUNE_AVOID_LEA_FOR_ADDR and PROCESSOR_INTEL.\nWe keep PROCESSOR_INTEL the same as PROCESSOR_SILVERMONT, except that\nX86_TUNE_AVOID_LEA_FOR_ADDR isn't turned on for PROCESSOR_INTEL.\n\n\t* config/i386/i386-c.c (ix86_target_macros_internal): Handle\n\tPROCESSOR_INTEL.  Treat like PROCESSOR_GENERIC.\n\t* config/i386/i386.c (intel_memcpy): New.  Duplicate slm_memcpy.\n\t(intel_memset): New.  Duplicate slm_memset.\n\t(intel_cost): New.  Duplicate slm_cost.\n\t(m_INTEL): New macro.\n\t(processor_target_table): Add \"intel\".\n\t(ix86_option_override_internal): Replace PROCESSOR_SILVERMONT\n\twith PROCESSOR_INTEL for \"intel\".\n\t(ix86_lea_outperforms): Support PROCESSOR_INTEL.  Duplicate\n\tPROCESSOR_SILVERMONT.\n\t(ix86_avoid_lea_for_addr): Check TARGET_AVOID_LEA_FOR_ADDR\n\tinstead of TARGET_OPT_AGU.\n\t(ix86_issue_rate): Likewise.\n\t(ix86_adjust_cost): Likewise.\n\t(ia32_multipass_dfa_lookahead): Likewise.\n\t(swap_top_of_ready_list): Likewise.\n\t(ix86_sched_reorder): Likewise.\n\t* config/i386/i386.h (TARGET_INTEL): New.\n\t(TARGET_AVOID_LEA_FOR_ADDR): Likewise.\n\t(processor_type): Add PROCESSOR_INTEL.\n\t* config/i386/x86-tune.def: Support m_INTEL. Duplicate\n\tm_SILVERMONT.  Add X86_TUNE_AVOID_LEA_FOR_ADDR.\n\nFrom-SVN: r206717", "tree": {"sha": "85cbf2b0c5da82944051e42957aaab5b6f8e81ad", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/85cbf2b0c5da82944051e42957aaab5b6f8e81ad"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/9a7f94d78b9468ba79ebad169fb1f262cf5950ac", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9a7f94d78b9468ba79ebad169fb1f262cf5950ac", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9a7f94d78b9468ba79ebad169fb1f262cf5950ac", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9a7f94d78b9468ba79ebad169fb1f262cf5950ac/comments", "author": {"login": "hjl-tools", "id": 1072356, "node_id": "MDQ6VXNlcjEwNzIzNTY=", "avatar_url": "https://avatars.githubusercontent.com/u/1072356?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hjl-tools", "html_url": "https://github.com/hjl-tools", "followers_url": "https://api.github.com/users/hjl-tools/followers", "following_url": "https://api.github.com/users/hjl-tools/following{/other_user}", "gists_url": "https://api.github.com/users/hjl-tools/gists{/gist_id}", "starred_url": "https://api.github.com/users/hjl-tools/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hjl-tools/subscriptions", "organizations_url": "https://api.github.com/users/hjl-tools/orgs", "repos_url": "https://api.github.com/users/hjl-tools/repos", "events_url": "https://api.github.com/users/hjl-tools/events{/privacy}", "received_events_url": "https://api.github.com/users/hjl-tools/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "0ffc4683170aa3cdb3c244745f7d52ae798da520", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0ffc4683170aa3cdb3c244745f7d52ae798da520", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0ffc4683170aa3cdb3c244745f7d52ae798da520"}], "stats": {"total": 193, "additions": 161, "deletions": 32}, "files": [{"sha": "df6e491f0ffabed8680c34c0098a1938840c4acc", "filename": "gcc/ChangeLog", "status": "modified", "additions": 26, "deletions": 0, "changes": 26, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a7f94d78b9468ba79ebad169fb1f262cf5950ac/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a7f94d78b9468ba79ebad169fb1f262cf5950ac/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=9a7f94d78b9468ba79ebad169fb1f262cf5950ac", "patch": "@@ -1,3 +1,29 @@\n+2014-01-17  H.J. Lu  <hongjiu.lu@intel.com>\n+\n+\t* config/i386/i386-c.c (ix86_target_macros_internal): Handle\n+\tPROCESSOR_INTEL.  Treat like PROCESSOR_GENERIC.\n+\t* config/i386/i386.c (intel_memcpy): New.  Duplicate slm_memcpy.\n+\t(intel_memset): New.  Duplicate slm_memset.\n+\t(intel_cost): New.  Duplicate slm_cost.\n+\t(m_INTEL): New macro.\n+\t(processor_target_table): Add \"intel\".\n+\t(ix86_option_override_internal): Replace PROCESSOR_SILVERMONT\n+\twith PROCESSOR_INTEL for \"intel\".\n+\t(ix86_lea_outperforms): Support PROCESSOR_INTEL.  Duplicate\n+\tPROCESSOR_SILVERMONT.\n+\t(ix86_avoid_lea_for_addr): Check TARGET_AVOID_LEA_FOR_ADDR\n+\tinstead of TARGET_OPT_AGU.\n+\t(ix86_issue_rate): Likewise.\n+\t(ix86_adjust_cost): Likewise.\n+\t(ia32_multipass_dfa_lookahead): Likewise.\n+\t(swap_top_of_ready_list): Likewise.\n+\t(ix86_sched_reorder): Likewise.\n+\t* config/i386/i386.h (TARGET_INTEL): New.\n+\t(TARGET_AVOID_LEA_FOR_ADDR): Likewise.\n+\t(processor_type): Add PROCESSOR_INTEL.\n+\t* config/i386/x86-tune.def: Support m_INTEL. Duplicate\n+\tm_SILVERMONT.  Add X86_TUNE_AVOID_LEA_FOR_ADDR.\n+\n 2014-01-17  Marek Polacek  <polacek@redhat.com>\n \n \tPR c/58346"}, {"sha": "ce9ba95ede6a1f585ce45330562d85ad1746573e", "filename": "gcc/config/i386/i386-c.c", "status": "modified", "additions": 2, "deletions": 0, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a7f94d78b9468ba79ebad169fb1f262cf5950ac/gcc%2Fconfig%2Fi386%2Fi386-c.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a7f94d78b9468ba79ebad169fb1f262cf5950ac/gcc%2Fconfig%2Fi386%2Fi386-c.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386-c.c?ref=9a7f94d78b9468ba79ebad169fb1f262cf5950ac", "patch": "@@ -174,6 +174,7 @@ ix86_target_macros_internal (HOST_WIDE_INT isa_flag,\n     /* use PROCESSOR_max to not set/unset the arch macro.  */\n     case PROCESSOR_max:\n       break;\n+    case PROCESSOR_INTEL:\n     case PROCESSOR_GENERIC:\n       gcc_unreachable ();\n     }\n@@ -276,6 +277,7 @@ ix86_target_macros_internal (HOST_WIDE_INT isa_flag,\n       def_or_undef (parse_in, \"__tune_slm__\");\n       def_or_undef (parse_in, \"__tune_silvermont__\");\n       break;\n+    case PROCESSOR_INTEL:\n     case PROCESSOR_GENERIC:\n       break;\n     /* use PROCESSOR_max to not set/unset the tune macro.  */"}, {"sha": "8993331f4d3497a03a64e282fedf28efe7125c45", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 88, "deletions": 5, "changes": 93, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a7f94d78b9468ba79ebad169fb1f262cf5950ac/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a7f94d78b9468ba79ebad169fb1f262cf5950ac/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=9a7f94d78b9468ba79ebad169fb1f262cf5950ac", "patch": "@@ -1747,6 +1747,83 @@ struct processor_costs slm_cost = {\n   1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n };\n \n+static stringop_algs intel_memcpy[2] = {\n+  {libcall, {{11, loop, false}, {-1, rep_prefix_4_byte, false}}},\n+  {libcall, {{32, loop, false}, {64, rep_prefix_4_byte, false},\n+             {8192, rep_prefix_8_byte, false}, {-1, libcall, false}}}};\n+static stringop_algs intel_memset[2] = {\n+  {libcall, {{8, loop, false}, {15, unrolled_loop, false},\n+             {2048, rep_prefix_4_byte, false}, {-1, libcall, false}}},\n+  {libcall, {{24, loop, false}, {32, unrolled_loop, false},\n+             {8192, rep_prefix_8_byte, false}, {-1, libcall, false}}}};\n+static const\n+struct processor_costs intel_cost = {\n+  COSTS_N_INSNS (1),\t\t\t/* cost of an add instruction */\n+  COSTS_N_INSNS (1) + 1,\t\t/* cost of a lea instruction */\n+  COSTS_N_INSNS (1),\t\t\t/* variable shift costs */\n+  COSTS_N_INSNS (1),\t\t\t/* constant shift costs */\n+  {COSTS_N_INSNS (3),\t\t\t/* cost of starting multiply for QI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t HI */\n+   COSTS_N_INSNS (3),\t\t\t/*\t\t\t\t SI */\n+   COSTS_N_INSNS (4),\t\t\t/*\t\t\t\t DI */\n+   COSTS_N_INSNS (2)},\t\t\t/*\t\t\t      other */\n+  0,\t\t\t\t\t/* cost of multiply per each bit set */\n+  {COSTS_N_INSNS (18),\t\t\t/* cost of a divide/mod for QI */\n+   COSTS_N_INSNS (26),\t\t\t/*\t\t\t    HI */\n+   COSTS_N_INSNS (42),\t\t\t/*\t\t\t    SI */\n+   COSTS_N_INSNS (74),\t\t\t/*\t\t\t    DI */\n+   COSTS_N_INSNS (74)},\t\t\t/*\t\t\t    other */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movsx */\n+  COSTS_N_INSNS (1),\t\t\t/* cost of movzx */\n+  8,\t\t\t\t\t/* \"large\" insn */\n+  17,\t\t\t\t\t/* MOVE_RATIO */\n+  4,\t\t\t\t\t/* cost for loading QImode using movzbl */\n+  {4, 4, 4},\t\t\t\t/* cost of loading integer registers\n+\t\t\t\t\t   in QImode, HImode and SImode.\n+\t\t\t\t\t   Relative to reg-reg move (2).  */\n+  {4, 4, 4},\t\t\t\t/* cost of storing integer registers */\n+  4,\t\t\t\t\t/* cost of reg,reg fld/fst */\n+  {12, 12, 12},\t\t\t\t/* cost of loading fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  {6, 6, 8},\t\t\t\t/* cost of storing fp registers\n+\t\t\t\t\t   in SFmode, DFmode and XFmode */\n+  2,\t\t\t\t\t/* cost of moving MMX register */\n+  {8, 8},\t\t\t\t/* cost of loading MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  {8, 8},\t\t\t\t/* cost of storing MMX registers\n+\t\t\t\t\t   in SImode and DImode */\n+  2,\t\t\t\t\t/* cost of moving SSE register */\n+  {8, 8, 8},\t\t\t\t/* cost of loading SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  {8, 8, 8},\t\t\t\t/* cost of storing SSE registers\n+\t\t\t\t\t   in SImode, DImode and TImode */\n+  5,\t\t\t\t\t/* MMX or SSE register to integer */\n+  32,\t\t\t\t\t/* size of l1 cache.  */\n+  256,\t\t\t\t\t/* size of l2 cache.  */\n+  64,\t\t\t\t\t/* size of prefetch block */\n+  6,\t\t\t\t\t/* number of parallel prefetches */\n+  3,\t\t\t\t\t/* Branch cost */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FADD and FSUB insns.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FMUL instruction.  */\n+  COSTS_N_INSNS (20),\t\t\t/* cost of FDIV instruction.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FABS instruction.  */\n+  COSTS_N_INSNS (8),\t\t\t/* cost of FCHS instruction.  */\n+  COSTS_N_INSNS (40),\t\t\t/* cost of FSQRT instruction.  */\n+  intel_memcpy,\n+  intel_memset,\n+  1,\t\t\t\t\t/* scalar_stmt_cost.  */\n+  1,\t\t\t\t\t/* scalar load_cost.  */\n+  1,\t\t\t\t\t/* scalar_store_cost.  */\n+  1,\t\t\t\t\t/* vec_stmt_cost.  */\n+  1,\t\t\t\t\t/* vec_to_scalar_cost.  */\n+  1,\t\t\t\t\t/* scalar_to_vec_cost.  */\n+  1,\t\t\t\t\t/* vec_align_load_cost.  */\n+  2,\t\t\t\t\t/* vec_unalign_load_cost.  */\n+  1,\t\t\t\t\t/* vec_store_cost.  */\n+  3,\t\t\t\t\t/* cond_taken_branch_cost.  */\n+  1,\t\t\t\t\t/* cond_not_taken_branch_cost.  */\n+};\n+\n /* Generic should produce code tuned for Core-i7 (and newer chips)\n    and btver1 (and newer chips).  */\n \n@@ -1942,6 +2019,7 @@ const struct processor_costs *ix86_cost = &pentium_cost;\n #define m_CORE_ALL (m_CORE2 | m_NEHALEM  | m_SANDYBRIDGE | m_HASWELL)\n #define m_BONNELL (1<<PROCESSOR_BONNELL)\n #define m_SILVERMONT (1<<PROCESSOR_SILVERMONT)\n+#define m_INTEL (1<<PROCESSOR_INTEL)\n \n #define m_GEODE (1<<PROCESSOR_GEODE)\n #define m_K6 (1<<PROCESSOR_K6)\n@@ -2401,6 +2479,7 @@ static const struct ptt processor_target_table[PROCESSOR_max] =\n   {\"haswell\", &core_cost, 16, 10, 16, 10, 16},\n   {\"bonnell\", &atom_cost, 16, 15, 16, 7, 16},\n   {\"silvermont\", &slm_cost, 16, 15, 16, 7, 16},\n+  {\"intel\", &intel_cost, 16, 15, 16, 7, 16},\n   {\"geode\", &geode_cost, 0, 0, 0, 0, 0},\n   {\"k6\", &k6_cost, 32, 7, 32, 7, 32},\n   {\"athlon\", &athlon_cost, 16, 7, 16, 7, 16},\n@@ -3112,7 +3191,7 @@ ix86_option_override_internal (bool main_args_p,\n       {\"atom\", PROCESSOR_BONNELL, CPU_ATOM, PTA_BONNELL},\n       {\"silvermont\", PROCESSOR_SILVERMONT, CPU_SLM, PTA_SILVERMONT},\n       {\"slm\", PROCESSOR_SILVERMONT, CPU_SLM, PTA_SILVERMONT},\n-      {\"intel\", PROCESSOR_SILVERMONT, CPU_SLM, PTA_NEHALEM},\n+      {\"intel\", PROCESSOR_INTEL, CPU_SLM, PTA_NEHALEM},\n       {\"geode\", PROCESSOR_GEODE, CPU_GEODE,\n \tPTA_MMX | PTA_3DNOW | PTA_3DNOW_A | PTA_PREFETCH_SSE | PTA_PRFCHW},\n       {\"k6\", PROCESSOR_K6, CPU_K6, PTA_MMX},\n@@ -17941,7 +18020,7 @@ ix86_lea_outperforms (rtx insn, unsigned int regno0, unsigned int regno1,\n   /* For Silvermont if using a 2-source or 3-source LEA for\n      non-destructive destination purposes, or due to wanting\n      ability to use SCALE, the use of LEA is justified.  */\n-  if (ix86_tune == PROCESSOR_SILVERMONT)\n+  if (ix86_tune == PROCESSOR_SILVERMONT || ix86_tune == PROCESSOR_INTEL)\n     {\n       if (has_scale)\n \treturn true;\n@@ -18077,7 +18156,7 @@ ix86_avoid_lea_for_addr (rtx insn, rtx operands[])\n   int ok;\n \n   /* Check we need to optimize.  */\n-  if (!TARGET_OPT_AGU || optimize_function_for_size_p (cfun))\n+  if (!TARGET_AVOID_LEA_FOR_ADDR || optimize_function_for_size_p (cfun))\n     return false;\n \n   /* Check it is correct to split here.  */\n@@ -25200,6 +25279,7 @@ ix86_issue_rate (void)\n     case PROCESSOR_PENTIUM:\n     case PROCESSOR_BONNELL:\n     case PROCESSOR_SILVERMONT:\n+    case PROCESSOR_INTEL:\n     case PROCESSOR_K6:\n     case PROCESSOR_BTVER2:\n     case PROCESSOR_PENTIUM4:\n@@ -25541,6 +25621,7 @@ ix86_adjust_cost (rtx insn, rtx link, rtx dep_insn, int cost)\n       break;\n \n     case PROCESSOR_SILVERMONT:\n+    case PROCESSOR_INTEL:\n       if (!reload_completed)\n \treturn cost;\n \n@@ -25609,6 +25690,7 @@ ia32_multipass_dfa_lookahead (void)\n     case PROCESSOR_HASWELL:\n     case PROCESSOR_BONNELL:\n     case PROCESSOR_SILVERMONT:\n+    case PROCESSOR_INTEL:\n       /* Generally, we want haifa-sched:max_issue() to look ahead as far\n \t as many instructions can be executed on a cycle, i.e.,\n \t issue_rate.  I wonder why tuning for many CPUs does not do this.  */\n@@ -25830,7 +25912,7 @@ swap_top_of_ready_list (rtx *ready, int n_ready)\n   int clock2 = -1;\n   #define INSN_TICK(INSN) (HID (INSN)->tick)\n \n-  if (ix86_tune != PROCESSOR_SILVERMONT)\n+  if (ix86_tune != PROCESSOR_SILVERMONT && ix86_tune != PROCESSOR_INTEL)\n     return false;\n \n   if (!NONDEBUG_INSN_P (top))\n@@ -25904,7 +25986,8 @@ ix86_sched_reorder (FILE *dump, int sched_verbose, rtx *ready, int *pn_ready,\n \n   /* Do reodering for BONNELL/SILVERMONT only.  */\n   if (ix86_tune != PROCESSOR_BONNELL\n-      && ix86_tune != PROCESSOR_SILVERMONT)\n+      && ix86_tune != PROCESSOR_SILVERMONT\n+      && ix86_tune != PROCESSOR_INTEL)\n     return issue_rate;\n \n   /* Nothing to do if ready list contains only 1 instruction.  */"}, {"sha": "580a3196b27a65fba45ec8910847fcfe8d88e61d", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a7f94d78b9468ba79ebad169fb1f262cf5950ac/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a7f94d78b9468ba79ebad169fb1f262cf5950ac/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=9a7f94d78b9468ba79ebad169fb1f262cf5950ac", "patch": "@@ -308,6 +308,7 @@ extern const struct processor_costs ix86_size_cost;\n #define TARGET_HASWELL (ix86_tune == PROCESSOR_HASWELL)\n #define TARGET_BONNELL (ix86_tune == PROCESSOR_BONNELL)\n #define TARGET_SILVERMONT (ix86_tune == PROCESSOR_SILVERMONT)\n+#define TARGET_INTEL (ix86_tune == PROCESSOR_INTEL)\n #define TARGET_GENERIC (ix86_tune == PROCESSOR_GENERIC)\n #define TARGET_AMDFAM10 (ix86_tune == PROCESSOR_AMDFAM10)\n #define TARGET_BDVER1 (ix86_tune == PROCESSOR_BDVER1)\n@@ -429,6 +430,8 @@ extern unsigned char ix86_tune_features[X86_TUNE_LAST];\n #define TARGET_FUSE_ALU_AND_BRANCH \\\n \tix86_tune_features[X86_TUNE_FUSE_ALU_AND_BRANCH]\n #define TARGET_OPT_AGU ix86_tune_features[X86_TUNE_OPT_AGU]\n+#define TARGET_AVOID_LEA_FOR_ADDR \\\n+\tix86_tune_features[X86_TUNE_AVOID_LEA_FOR_ADDR]\n #define TARGET_VECTORIZE_DOUBLE \\\n \tix86_tune_features[X86_TUNE_VECTORIZE_DOUBLE]\n #define TARGET_SOFTWARE_PREFETCHING_BENEFICIAL \\\n@@ -2184,6 +2187,7 @@ enum processor_type\n   PROCESSOR_HASWELL,\n   PROCESSOR_BONNELL,\n   PROCESSOR_SILVERMONT,\n+  PROCESSOR_INTEL,\n   PROCESSOR_GEODE,\n   PROCESSOR_K6,\n   PROCESSOR_ATHLON,"}, {"sha": "f5affe6cdaf08835b845e0dfc25b32987178be3e", "filename": "gcc/config/i386/x86-tune.def", "status": "modified", "additions": 41, "deletions": 27, "changes": 68, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/9a7f94d78b9468ba79ebad169fb1f262cf5950ac/gcc%2Fconfig%2Fi386%2Fx86-tune.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/9a7f94d78b9468ba79ebad169fb1f262cf5950ac/gcc%2Fconfig%2Fi386%2Fx86-tune.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fx86-tune.def?ref=9a7f94d78b9468ba79ebad169fb1f262cf5950ac", "patch": "@@ -40,16 +40,16 @@ see the files COPYING3 and COPYING.RUNTIME respectively.  If not, see\n \n /* X86_TUNE_SCHEDULE: Enable scheduling.  */\n DEF_TUNE (X86_TUNE_SCHEDULE, \"schedule\",\n-          m_PENT | m_PPRO | m_CORE_ALL | m_BONNELL | m_SILVERMONT | m_K6_GEODE\n-          | m_AMD_MULTIPLE | m_GENERIC)\n+          m_PENT | m_PPRO | m_CORE_ALL | m_BONNELL | m_SILVERMONT | m_INTEL \n+\t  | m_K6_GEODE | m_AMD_MULTIPLE | m_GENERIC)\n \n /* X86_TUNE_PARTIAL_REG_DEPENDENCY: Enable more register renaming\n    on modern chips.  Preffer stores affecting whole integer register\n    over partial stores.  For example preffer MOVZBL or MOVQ to load 8bit\n    value over movb.  */\n DEF_TUNE (X86_TUNE_PARTIAL_REG_DEPENDENCY, \"partial_reg_dependency\",\n-          m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT | m_AMD_MULTIPLE\n-          | m_GENERIC)\n+          m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT | m_INTEL\n+\t  | m_AMD_MULTIPLE | m_GENERIC)\n \n /* X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY: This knob promotes all store\n    destinations to be 128bit to allow register renaming on 128bit SSE units,\n@@ -58,8 +58,8 @@ DEF_TUNE (X86_TUNE_PARTIAL_REG_DEPENDENCY, \"partial_reg_dependency\",\n    SPECfp regression, while enabling it on K8 brings roughly 2.4% regression\n    that can be partly masked by careful scheduling of moves.  */\n DEF_TUNE (X86_TUNE_SSE_PARTIAL_REG_DEPENDENCY, \"sse_partial_reg_dependency\",\n-          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT | m_AMDFAM10\n-          | m_BDVER | m_GENERIC)\n+          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT\n+\t  | m_INTEL | m_AMDFAM10 | m_BDVER | m_GENERIC)\n \n /* X86_TUNE_SSE_SPLIT_REGS: Set for machines where the type and dependencies\n    are resolved on SSE register parts instead of whole registers, so we may\n@@ -84,13 +84,14 @@ DEF_TUNE (X86_TUNE_PARTIAL_FLAG_REG_STALL, \"partial_flag_reg_stall\",\n /* X86_TUNE_MOVX: Enable to zero extend integer registers to avoid\n    partial dependencies.  */\n DEF_TUNE (X86_TUNE_MOVX, \"movx\",\n-          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT | m_GEODE\n-          | m_AMD_MULTIPLE  | m_GENERIC)\n+          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT\n+\t  | m_INTEL | m_GEODE | m_AMD_MULTIPLE  | m_GENERIC)\n \n /* X86_TUNE_MEMORY_MISMATCH_STALL: Avoid partial stores that are followed by\n    full sized loads.  */\n DEF_TUNE (X86_TUNE_MEMORY_MISMATCH_STALL, \"memory_mismatch_stall\",\n-          m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT | m_AMD_MULTIPLE | m_GENERIC)\n+          m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT | m_INTEL\n+\t  | m_AMD_MULTIPLE | m_GENERIC)\n \n /* X86_TUNE_FUSE_CMP_AND_BRANCH_32: Fuse compare with a subsequent\n    conditional jump instruction for 32 bit TARGET.\n@@ -124,7 +125,8 @@ DEF_TUNE (X86_TUNE_REASSOC_INT_TO_PARALLEL, \"reassoc_int_to_parallel\",\n /* X86_TUNE_REASSOC_FP_TO_PARALLEL: Try to produce parallel computations\n    during reassociation of fp computation.  */\n DEF_TUNE (X86_TUNE_REASSOC_FP_TO_PARALLEL, \"reassoc_fp_to_parallel\",\n-          m_BONNELL | m_SILVERMONT | m_HASWELL | m_BDVER1 | m_BDVER2 | m_GENERIC)\n+          m_BONNELL | m_SILVERMONT | m_HASWELL | m_INTEL | m_BDVER1\n+\t  | m_BDVER2 | m_GENERIC)\n \n /*****************************************************************************/\n /* Function prologue, epilogue and function calling sequences.               */\n@@ -143,7 +145,8 @@ DEF_TUNE (X86_TUNE_REASSOC_FP_TO_PARALLEL, \"reassoc_fp_to_parallel\",\n    regression on mgrid due to IRA limitation leading to unecessary\n    use of the frame pointer in 32bit mode.  */\n DEF_TUNE (X86_TUNE_ACCUMULATE_OUTGOING_ARGS, \"accumulate_outgoing_args\",\n-\t  m_PPRO | m_P4_NOCONA | m_BONNELL | m_SILVERMONT | m_AMD_MULTIPLE | m_GENERIC)\n+\t  m_PPRO | m_P4_NOCONA | m_BONNELL | m_SILVERMONT | m_INTEL\n+\t  | m_AMD_MULTIPLE | m_GENERIC)\n \n /* X86_TUNE_PROLOGUE_USING_MOVE: Do not use push/pop in prologues that are\n    considered on critical path.  */\n@@ -202,7 +205,8 @@ DEF_TUNE (X86_TUNE_PAD_RETURNS, \"pad_returns\",\n /* X86_TUNE_FOUR_JUMP_LIMIT: Some CPU cores are not able to predict more\n    than 4 branch instructions in the 16 byte window.  */\n DEF_TUNE (X86_TUNE_FOUR_JUMP_LIMIT, \"four_jump_limit\",\n-          m_PPRO | m_P4_NOCONA | m_BONNELL | m_SILVERMONT | m_ATHLON_K8 | m_AMDFAM10)\n+          m_PPRO | m_P4_NOCONA | m_BONNELL | m_SILVERMONT | m_INTEL |\n+\t  m_ATHLON_K8 | m_AMDFAM10)\n \n /*****************************************************************************/\n /* Integer instruction selection tuning                                      */\n@@ -224,17 +228,22 @@ DEF_TUNE (X86_TUNE_READ_MODIFY, \"read_modify\", ~(m_PENT | m_PPRO))\n \n /* X86_TUNE_USE_INCDEC: Enable use of inc/dec instructions.   */\n DEF_TUNE (X86_TUNE_USE_INCDEC, \"use_incdec\",\n-          ~(m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT | m_GENERIC))\n+          ~(m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT | m_INTEL\n+\t    | m_GENERIC))\n \n /* X86_TUNE_INTEGER_DFMODE_MOVES: Enable if integer moves are preferred\n    for DFmode copies */\n DEF_TUNE (X86_TUNE_INTEGER_DFMODE_MOVES, \"integer_dfmode_moves\",\n           ~(m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT\n-          | m_GEODE | m_AMD_MULTIPLE | m_GENERIC))\n+\t    | m_INTEL | m_GEODE | m_AMD_MULTIPLE | m_GENERIC))\n \n /* X86_TUNE_OPT_AGU: Optimize for Address Generation Unit. This flag\n    will impact LEA instruction selection. */\n-DEF_TUNE (X86_TUNE_OPT_AGU, \"opt_agu\", m_BONNELL | m_SILVERMONT)\n+DEF_TUNE (X86_TUNE_OPT_AGU, \"opt_agu\", m_BONNELL | m_SILVERMONT | m_INTEL)\n+\n+/* X86_TUNE_AVOID_LEA_FOR_ADDR: Avoid lea for address computation.  */\n+DEF_TUNE (X86_TUNE_AVOID_LEA_FOR_ADDR, \"avoid_lea_for_addr\",\n+\t  m_BONNELL | m_SILVERMONT)\n \n /* X86_TUNE_SLOW_IMUL_IMM32_MEM: Imul of 32-bit constant and memory is\n    vector path on AMD machines.\n@@ -251,7 +260,7 @@ DEF_TUNE (X86_TUNE_SLOW_IMUL_IMM8, \"slow_imul_imm8\",\n /* X86_TUNE_AVOID_MEM_OPND_FOR_CMOVE: Try to avoid memory operands for\n    a conditional move.  */\n DEF_TUNE (X86_TUNE_AVOID_MEM_OPND_FOR_CMOVE, \"avoid_mem_opnd_for_cmove\",\n-\t  m_BONNELL | m_SILVERMONT)\n+\t  m_BONNELL | m_SILVERMONT | m_INTEL)\n \n /* X86_TUNE_SINGLE_STRINGOP: Enable use of single string operations, such\n    as MOVS and STOS (without a REP prefix) to move/set sequences of bytes.  */\n@@ -268,15 +277,18 @@ DEF_TUNE (X86_TUNE_MISALIGNED_MOVE_STRING_PRO_EPILOGUES,\n \n /* X86_TUNE_USE_SAHF: Controls use of SAHF.  */\n DEF_TUNE (X86_TUNE_USE_SAHF, \"use_sahf\",\n-          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT | m_K6_GEODE\n-          | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER | m_GENERIC)\n+          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT\n+\t  | m_INTEL | m_K6_GEODE | m_K8 | m_AMDFAM10 | m_BDVER | m_BTVER\n+\t  | m_GENERIC)\n \n /* X86_TUNE_USE_CLTD: Controls use of CLTD and CTQO instructions.  */\n-DEF_TUNE (X86_TUNE_USE_CLTD, \"use_cltd\", ~(m_PENT | m_BONNELL | m_SILVERMONT | m_K6))\n+DEF_TUNE (X86_TUNE_USE_CLTD, \"use_cltd\",\n+\t  ~(m_PENT | m_BONNELL | m_SILVERMONT | m_INTEL  | m_K6))\n \n /* X86_TUNE_USE_BT: Enable use of BT (bit test) instructions.  */\n DEF_TUNE (X86_TUNE_USE_BT, \"use_bt\",\n-          m_CORE_ALL | m_BONNELL | m_SILVERMONT | m_AMD_MULTIPLE | m_GENERIC)\n+          m_CORE_ALL | m_BONNELL | m_SILVERMONT | m_INTEL | m_AMD_MULTIPLE\n+\t  | m_GENERIC)\n \n /*****************************************************************************/\n /* 387 instruction selection tuning                                          */\n@@ -291,16 +303,16 @@ DEF_TUNE (X86_TUNE_USE_HIMODE_FIOP, \"use_himode_fiop\",\n /* X86_TUNE_USE_SIMODE_FIOP: Enables use of x87 instructions with 32bit\n    integer operand.  */\n DEF_TUNE (X86_TUNE_USE_SIMODE_FIOP, \"use_simode_fiop\",\n-          ~(m_PENT | m_PPRO | m_CORE_ALL | m_BONNELL\n-            | m_SILVERMONT | m_AMD_MULTIPLE | m_GENERIC))\n+          ~(m_PENT | m_PPRO | m_CORE_ALL | m_BONNELL | m_SILVERMONT\n+\t    | m_INTEL | m_AMD_MULTIPLE | m_GENERIC))\n \n /* X86_TUNE_USE_FFREEP: Use freep instruction instead of fstp.  */\n DEF_TUNE (X86_TUNE_USE_FFREEP, \"use_ffreep\", m_AMD_MULTIPLE)\n \n /* X86_TUNE_EXT_80387_CONSTANTS: Use fancy 80387 constants, such as PI.  */\n DEF_TUNE (X86_TUNE_EXT_80387_CONSTANTS, \"ext_80387_constants\",\n-          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT | m_K6_GEODE\n-          | m_ATHLON_K8 | m_GENERIC)\n+          m_PPRO | m_P4_NOCONA | m_CORE_ALL | m_BONNELL | m_SILVERMONT\n+\t  | m_INTEL | m_K6_GEODE | m_ATHLON_K8 | m_GENERIC)\n \n /*****************************************************************************/\n /* SSE instruction selection tuning                                          */\n@@ -318,12 +330,14 @@ DEF_TUNE (X86_TUNE_GENERAL_REGS_SSE_SPILL, \"general_regs_sse_spill\",\n /* X86_TUNE_SSE_UNALIGNED_LOAD_OPTIMAL: Use movups for misaligned loads instead\n    of a sequence loading registers by parts.  */\n DEF_TUNE (X86_TUNE_SSE_UNALIGNED_LOAD_OPTIMAL, \"sse_unaligned_load_optimal\",\n-          m_NEHALEM | m_SANDYBRIDGE | m_HASWELL | m_AMDFAM10 | m_BDVER | m_BTVER | m_SILVERMONT | m_GENERIC)\n+          m_NEHALEM | m_SANDYBRIDGE | m_HASWELL | m_AMDFAM10 | m_BDVER\n+\t  | m_BTVER | m_SILVERMONT | m_INTEL | m_GENERIC)\n \n /* X86_TUNE_SSE_UNALIGNED_STORE_OPTIMAL: Use movups for misaligned stores instead\n    of a sequence loading registers by parts.  */\n DEF_TUNE (X86_TUNE_SSE_UNALIGNED_STORE_OPTIMAL, \"sse_unaligned_store_optimal\",\n-          m_NEHALEM | m_SANDYBRIDGE | m_HASWELL | m_BDVER | m_SILVERMONT | m_GENERIC)\n+          m_NEHALEM | m_SANDYBRIDGE | m_HASWELL | m_BDVER | m_SILVERMONT\n+\t  | m_INTEL | m_GENERIC)\n \n /* Use packed single precision instructions where posisble.  I.e. movups instead\n    of movupd.  */\n@@ -360,7 +374,7 @@ DEF_TUNE (X86_TUNE_INTER_UNIT_CONVERSIONS, \"inter_unit_conversions\",\n /* X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS: Try to split memory operand for\n    fp converts to destination register.  */\n DEF_TUNE (X86_TUNE_SPLIT_MEM_OPND_FOR_FP_CONVERTS, \"split_mem_opnd_for_fp_converts\",\n-          m_SILVERMONT)\n+          m_SILVERMONT | m_INTEL)\n \n /* X86_TUNE_USE_VECTOR_FP_CONVERTS: Prefer vector packed SSE conversion\n    from FP to FP.  This form of instructions avoids partial write to the"}]}