{"sha": "5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NTUxMWJjNWFkYTJiZDJiYzlhYjgzNWQwZWQ5ZmQ5NmE4M2QzMjYwZA==", "commit": {"author": {"name": "Bernd Schmidt", "email": "bernds@codesourcery.com", "date": "2011-07-06T23:16:39Z"}, "committer": {"name": "Bernd Schmidt", "email": "bernds@gcc.gnu.org", "date": "2011-07-06T23:16:39Z"}, "message": "explow.c (trunc_int_for_mode): Use GET_MODE_PRECISION instead of GET_MODE_BITSIZE where appropriate.\n\n\t* explow.c (trunc_int_for_mode): Use GET_MODE_PRECISION\n\tinstead of GET_MODE_BITSIZE where appropriate.\n\t* rtlanal.c (subreg_lsb_1, subreg_get_info, nonzero_bits1,\n\tnum_sign_bit_copies1, canonicalize_condition, low_bitmask_len,\n\tinit_num_sign_bit_copies_in_rep): Likewise.\n\t* cse.c (fold_rtx, cse_insn): Likewise.\n\t* loop-doloop.c (doloop_modify, doloop_optimize): Likewise.\n\t* simplify-rtx.c (simplify_unary_operation_1,\n\tsimplify_const_unary_operation, simplify_binary_operation_1,\n\tsimplify_const_binary_operation, simplify_ternary_operation,\n\tsimplify_const_relational_operation, simplify_subreg): Likewise.\n\t* combine.c (try_combine, find_split_point, combine_simplify_rtx,\n\tsimplify_if_then_else, simplify_set, expand_compound_operation,\n\texpand_field_assignment, make_extraction, if_then_else_cond,\n\tmake_compound_operation, force_to_mode, make_field_assignment,\n\treg_nonzero_bits_for_combine, reg_num_sign_bit_copies_for_combine,\n\textended_count, try_widen_shift_mode, simplify_shift_const_1,\n\tsimplify_comparison, record_promoted_value, simplify_compare_const,\n\trecord_dead_and_set_regs_1): Likewise.\n\nFrom-SVN: r175946", "tree": {"sha": "27c66cbb7f764ab1e0fc7b9262c11b806127fde0", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/27c66cbb7f764ab1e0fc7b9262c11b806127fde0"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/comments", "author": null, "committer": null, "parents": [{"sha": "46c9550f314d1eb1767d9636afda497a1cbd0797", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/46c9550f314d1eb1767d9636afda497a1cbd0797", "html_url": "https://github.com/Rust-GCC/gccrs/commit/46c9550f314d1eb1767d9636afda497a1cbd0797"}], "stats": {"total": 511, "additions": 267, "deletions": 244}, "files": [{"sha": "6c4d352053ae438e1e3fb4b6736b5b5ce8728fa1", "filename": "gcc/ChangeLog", "status": "modified", "additions": 20, "deletions": 0, "changes": 20, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d", "patch": "@@ -24,6 +24,26 @@\n \tsimplify_binary_operation_1, simplify_const_relational_operation):\n \tLikewise.\n \n+\t* explow.c (trunc_int_for_mode): Use GET_MODE_PRECISION\n+\tinstead of GET_MODE_BITSIZE where appropriate.\n+\t* rtlanal.c (subreg_lsb_1, subreg_get_info, nonzero_bits1,\n+\tnum_sign_bit_copies1, canonicalize_condition, low_bitmask_len,\n+\tinit_num_sign_bit_copies_in_rep): Likewise.\n+\t* cse.c (fold_rtx, cse_insn): Likewise.\n+\t* loop-doloop.c (doloop_modify, doloop_optimize): Likewise.\n+\t* simplify-rtx.c (simplify_unary_operation_1,\n+\tsimplify_const_unary_operation, simplify_binary_operation_1,\n+\tsimplify_const_binary_operation, simplify_ternary_operation,\n+\tsimplify_const_relational_operation, simplify_subreg): Likewise.\n+\t* combine.c (try_combine, find_split_point, combine_simplify_rtx,\n+\tsimplify_if_then_else, simplify_set, expand_compound_operation,\n+\texpand_field_assignment, make_extraction, if_then_else_cond,\n+\tmake_compound_operation, force_to_mode, make_field_assignment,\n+\treg_nonzero_bits_for_combine, reg_num_sign_bit_copies_for_combine,\n+\textended_count, try_widen_shift_mode, simplify_shift_const_1,\n+\tsimplify_comparison, record_promoted_value, simplify_compare_const,\n+\trecord_dead_and_set_regs_1): Likewise.\n+\n 2011-07-06  Michael Meissner  <meissner@linux.vnet.ibm.com>\n \n \t* config/rs6000/rs6000-protos.h (rs6000_call_indirect_aix): New"}, {"sha": "787b0db98208ad302c5939731de4698b8f54a888", "filename": "gcc/combine.c", "status": "modified", "additions": 129, "deletions": 128, "changes": 257, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/gcc%2Fcombine.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/gcc%2Fcombine.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcombine.c?ref=5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d", "patch": "@@ -2758,14 +2758,14 @@ try_combine (rtx i3, rtx i2, rtx i1, rtx i0, int *new_direct_jump_p,\n \t      offset = INTVAL (XEXP (dest, 2));\n \t      dest = XEXP (dest, 0);\n \t      if (BITS_BIG_ENDIAN)\n-\t\toffset = GET_MODE_BITSIZE (GET_MODE (dest)) - width - offset;\n+\t\toffset = GET_MODE_PRECISION (GET_MODE (dest)) - width - offset;\n \t    }\n \t}\n       else\n \t{\n \t  if (GET_CODE (dest) == STRICT_LOW_PART)\n \t    dest = XEXP (dest, 0);\n-\t  width = GET_MODE_BITSIZE (GET_MODE (dest));\n+\t  width = GET_MODE_PRECISION (GET_MODE (dest));\n \t  offset = 0;\n \t}\n \n@@ -2775,16 +2775,16 @@ try_combine (rtx i3, rtx i2, rtx i1, rtx i0, int *new_direct_jump_p,\n \t  if (subreg_lowpart_p (dest))\n \t    ;\n \t  /* Handle the case where inner is twice the size of outer.  */\n-\t  else if (GET_MODE_BITSIZE (GET_MODE (SET_DEST (temp)))\n-\t\t   == 2 * GET_MODE_BITSIZE (GET_MODE (dest)))\n-\t    offset += GET_MODE_BITSIZE (GET_MODE (dest));\n+\t  else if (GET_MODE_PRECISION (GET_MODE (SET_DEST (temp)))\n+\t\t   == 2 * GET_MODE_PRECISION (GET_MODE (dest)))\n+\t    offset += GET_MODE_PRECISION (GET_MODE (dest));\n \t  /* Otherwise give up for now.  */\n \t  else\n \t    offset = -1;\n \t}\n \n       if (offset >= 0\n-\t  && (GET_MODE_BITSIZE (GET_MODE (SET_DEST (temp)))\n+\t  && (GET_MODE_PRECISION (GET_MODE (SET_DEST (temp)))\n \t      <= HOST_BITS_PER_DOUBLE_INT))\n \t{\n \t  double_int m, o, i;\n@@ -3745,8 +3745,8 @@ try_combine (rtx i3, rtx i2, rtx i1, rtx i0, int *new_direct_jump_p,\n \t\t (REG_P (temp)\n \t\t  && VEC_index (reg_stat_type, reg_stat,\n \t\t\t\tREGNO (temp))->nonzero_bits != 0\n-\t\t  && GET_MODE_BITSIZE (GET_MODE (temp)) < BITS_PER_WORD\n-\t\t  && GET_MODE_BITSIZE (GET_MODE (temp)) < HOST_BITS_PER_INT\n+\t\t  && GET_MODE_PRECISION (GET_MODE (temp)) < BITS_PER_WORD\n+\t\t  && GET_MODE_PRECISION (GET_MODE (temp)) < HOST_BITS_PER_INT\n \t\t  && (VEC_index (reg_stat_type, reg_stat,\n \t\t\t\t REGNO (temp))->nonzero_bits\n \t\t      != GET_MODE_MASK (word_mode))))\n@@ -3755,8 +3755,8 @@ try_combine (rtx i3, rtx i2, rtx i1, rtx i0, int *new_direct_jump_p,\n \t\t     (REG_P (temp)\n \t\t      && VEC_index (reg_stat_type, reg_stat,\n \t\t\t\t    REGNO (temp))->nonzero_bits != 0\n-\t\t      && GET_MODE_BITSIZE (GET_MODE (temp)) < BITS_PER_WORD\n-\t\t      && GET_MODE_BITSIZE (GET_MODE (temp)) < HOST_BITS_PER_INT\n+\t\t      && GET_MODE_PRECISION (GET_MODE (temp)) < BITS_PER_WORD\n+\t\t      && GET_MODE_PRECISION (GET_MODE (temp)) < HOST_BITS_PER_INT\n \t\t      && (VEC_index (reg_stat_type, reg_stat,\n \t\t\t\t     REGNO (temp))->nonzero_bits\n \t\t\t  != GET_MODE_MASK (word_mode)))))\n@@ -4685,7 +4685,7 @@ find_split_point (rtx *loc, rtx insn, bool set_src)\n \t  && CONST_INT_P (SET_SRC (x))\n \t  && ((INTVAL (XEXP (SET_DEST (x), 1))\n \t       + INTVAL (XEXP (SET_DEST (x), 2)))\n-\t      <= GET_MODE_BITSIZE (GET_MODE (XEXP (SET_DEST (x), 0))))\n+\t      <= GET_MODE_PRECISION (GET_MODE (XEXP (SET_DEST (x), 0))))\n \t  && ! side_effects_p (XEXP (SET_DEST (x), 0)))\n \t{\n \t  HOST_WIDE_INT pos = INTVAL (XEXP (SET_DEST (x), 2));\n@@ -4698,7 +4698,7 @@ find_split_point (rtx *loc, rtx insn, bool set_src)\n \t  rtx or_mask;\n \n \t  if (BITS_BIG_ENDIAN)\n-\t    pos = GET_MODE_BITSIZE (mode) - len - pos;\n+\t    pos = GET_MODE_PRECISION (mode) - len - pos;\n \n \t  or_mask = gen_int_mode (src << pos, mode);\n \t  if (src == mask)\n@@ -4791,7 +4791,7 @@ find_split_point (rtx *loc, rtx insn, bool set_src)\n \t    break;\n \n \t  pos = 0;\n-\t  len = GET_MODE_BITSIZE (GET_MODE (inner));\n+\t  len = GET_MODE_PRECISION (GET_MODE (inner));\n \t  unsignedp = 0;\n \t  break;\n \n@@ -4805,7 +4805,7 @@ find_split_point (rtx *loc, rtx insn, bool set_src)\n \t      pos = INTVAL (XEXP (SET_SRC (x), 2));\n \n \t      if (BITS_BIG_ENDIAN)\n-\t\tpos = GET_MODE_BITSIZE (GET_MODE (inner)) - len - pos;\n+\t\tpos = GET_MODE_PRECISION (GET_MODE (inner)) - len - pos;\n \t      unsignedp = (code == ZERO_EXTRACT);\n \t    }\n \t  break;\n@@ -4814,7 +4814,8 @@ find_split_point (rtx *loc, rtx insn, bool set_src)\n \t  break;\n \t}\n \n-      if (len && pos >= 0 && pos + len <= GET_MODE_BITSIZE (GET_MODE (inner)))\n+      if (len && pos >= 0\n+\t  && pos + len <= GET_MODE_PRECISION (GET_MODE (inner)))\n \t{\n \t  enum machine_mode mode = GET_MODE (SET_SRC (x));\n \n@@ -4845,9 +4846,9 @@ find_split_point (rtx *loc, rtx insn, bool set_src)\n \t\t     (unsignedp ? LSHIFTRT : ASHIFTRT, mode,\n \t\t      gen_rtx_ASHIFT (mode,\n \t\t\t\t      gen_lowpart (mode, inner),\n-\t\t\t\t      GEN_INT (GET_MODE_BITSIZE (mode)\n+\t\t\t\t      GEN_INT (GET_MODE_PRECISION (mode)\n \t\t\t\t\t       - len - pos)),\n-\t\t      GEN_INT (GET_MODE_BITSIZE (mode) - len)));\n+\t\t      GEN_INT (GET_MODE_PRECISION (mode) - len)));\n \n \t      split = find_split_point (&SET_SRC (x), insn, true);\n \t      if (split && split != &SET_SRC (x))\n@@ -5544,7 +5545,7 @@ combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest,\n \n       if (GET_CODE (temp) == ASHIFTRT\n \t  && CONST_INT_P (XEXP (temp, 1))\n-\t  && INTVAL (XEXP (temp, 1)) == GET_MODE_BITSIZE (mode) - 1)\n+\t  && INTVAL (XEXP (temp, 1)) == GET_MODE_PRECISION (mode) - 1)\n \treturn simplify_shift_const (NULL_RTX, LSHIFTRT, mode, XEXP (temp, 0),\n \t\t\t\t     INTVAL (XEXP (temp, 1)));\n \n@@ -5563,8 +5564,8 @@ combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest,\n \t  rtx temp1 = simplify_shift_const\n \t    (NULL_RTX, ASHIFTRT, mode,\n \t     simplify_shift_const (NULL_RTX, ASHIFT, mode, temp,\n-\t\t\t\t   GET_MODE_BITSIZE (mode) - 1 - i),\n-\t     GET_MODE_BITSIZE (mode) - 1 - i);\n+\t\t\t\t   GET_MODE_PRECISION (mode) - 1 - i),\n+\t     GET_MODE_PRECISION (mode) - 1 - i);\n \n \t  /* If all we did was surround TEMP with the two shifts, we\n \t     haven't improved anything, so don't use it.  Otherwise,\n@@ -5639,14 +5640,14 @@ combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest,\n \t       && (UINTVAL (XEXP (XEXP (XEXP (x, 0), 0), 1))\n \t\t   == ((unsigned HOST_WIDE_INT) 1 << (i + 1)) - 1))\n \t      || (GET_CODE (XEXP (XEXP (x, 0), 0)) == ZERO_EXTEND\n-\t\t  && (GET_MODE_BITSIZE (GET_MODE (XEXP (XEXP (XEXP (x, 0), 0), 0)))\n+\t\t  && (GET_MODE_PRECISION (GET_MODE (XEXP (XEXP (XEXP (x, 0), 0), 0)))\n \t\t      == (unsigned int) i + 1))))\n \treturn simplify_shift_const\n \t  (NULL_RTX, ASHIFTRT, mode,\n \t   simplify_shift_const (NULL_RTX, ASHIFT, mode,\n \t\t\t\t XEXP (XEXP (XEXP (x, 0), 0), 0),\n-\t\t\t\t GET_MODE_BITSIZE (mode) - (i + 1)),\n-\t   GET_MODE_BITSIZE (mode) - (i + 1));\n+\t\t\t\t GET_MODE_PRECISION (mode) - (i + 1)),\n+\t   GET_MODE_PRECISION (mode) - (i + 1));\n \n       /* If only the low-order bit of X is possibly nonzero, (plus x -1)\n \t can become (ashiftrt (ashift (xor x 1) C) C) where C is\n@@ -5660,8 +5661,8 @@ combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest,\n \treturn simplify_shift_const (NULL_RTX, ASHIFTRT, mode,\n \t   simplify_shift_const (NULL_RTX, ASHIFT, mode,\n \t\t\t\t gen_rtx_XOR (mode, XEXP (x, 0), const1_rtx),\n-\t\t\t\t GET_MODE_BITSIZE (mode) - 1),\n-\t   GET_MODE_BITSIZE (mode) - 1);\n+\t\t\t\t GET_MODE_PRECISION (mode) - 1),\n+\t   GET_MODE_PRECISION (mode) - 1);\n \n       /* If we are adding two things that have no bits in common, convert\n \t the addition into an IOR.  This will often be further simplified,\n@@ -5793,7 +5794,7 @@ combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest,\n \t\t   && op1 == const0_rtx\n \t\t   && mode == GET_MODE (op0)\n \t\t   && (num_sign_bit_copies (op0, mode)\n-\t\t       == GET_MODE_BITSIZE (mode)))\n+\t\t       == GET_MODE_PRECISION (mode)))\n \t    {\n \t      op0 = expand_compound_operation (op0);\n \t      return simplify_gen_unary (NEG, mode,\n@@ -5818,7 +5819,7 @@ combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest,\n \t\t   && op1 == const0_rtx\n \t\t   && mode == GET_MODE (op0)\n \t\t   && (num_sign_bit_copies (op0, mode)\n-\t\t       == GET_MODE_BITSIZE (mode)))\n+\t\t       == GET_MODE_PRECISION (mode)))\n \t    {\n \t      op0 = expand_compound_operation (op0);\n \t      return plus_constant (gen_lowpart (mode, op0), 1);\n@@ -5833,7 +5834,7 @@ combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest,\n \t      && new_code == NE && GET_MODE_CLASS (mode) == MODE_INT\n \t      && op1 == const0_rtx\n \t      && (num_sign_bit_copies (op0, mode)\n-\t\t  == GET_MODE_BITSIZE (mode)))\n+\t\t  == GET_MODE_PRECISION (mode)))\n \t    return gen_lowpart (mode,\n \t\t\t\texpand_compound_operation (op0));\n \n@@ -5854,7 +5855,7 @@ combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest,\n \t\t   && op1 == const0_rtx\n \t\t   && mode == GET_MODE (op0)\n \t\t   && (num_sign_bit_copies (op0, mode)\n-\t\t       == GET_MODE_BITSIZE (mode)))\n+\t\t       == GET_MODE_PRECISION (mode)))\n \t    {\n \t      op0 = expand_compound_operation (op0);\n \t      return simplify_gen_unary (NOT, mode,\n@@ -5887,7 +5888,7 @@ combine_simplify_rtx (rtx x, enum machine_mode op0_mode, int in_dest,\n \t    {\n \t      x = simplify_shift_const (NULL_RTX, ASHIFT, mode,\n \t\t\t\t\texpand_compound_operation (op0),\n-\t\t\t\t\tGET_MODE_BITSIZE (mode) - 1 - i);\n+\t\t\t\t\tGET_MODE_PRECISION (mode) - 1 - i);\n \t      if (GET_CODE (x) == AND && XEXP (x, 1) == const_true_rtx)\n \t\treturn XEXP (x, 0);\n \t      else\n@@ -6011,7 +6012,7 @@ simplify_if_then_else (rtx x)\n \t}\n       else if (true_code == EQ && true_val == const0_rtx\n \t       && (num_sign_bit_copies (from, GET_MODE (from))\n-\t\t   == GET_MODE_BITSIZE (GET_MODE (from))))\n+\t\t   == GET_MODE_PRECISION (GET_MODE (from))))\n \t{\n \t  false_code = EQ;\n \t  false_val = constm1_rtx;\n@@ -6181,8 +6182,8 @@ simplify_if_then_else (rtx x)\n \t       && rtx_equal_p (SUBREG_REG (XEXP (XEXP (t, 0), 0)), f)\n \t       && (num_sign_bit_copies (f, GET_MODE (f))\n \t\t   > (unsigned int)\n-\t\t     (GET_MODE_BITSIZE (mode)\n-\t\t      - GET_MODE_BITSIZE (GET_MODE (XEXP (XEXP (t, 0), 0))))))\n+\t\t     (GET_MODE_PRECISION (mode)\n+\t\t      - GET_MODE_PRECISION (GET_MODE (XEXP (XEXP (t, 0), 0))))))\n \t{\n \t  c1 = XEXP (XEXP (t, 0), 1); z = f; op = GET_CODE (XEXP (t, 0));\n \t  extend_op = SIGN_EXTEND;\n@@ -6197,8 +6198,8 @@ simplify_if_then_else (rtx x)\n \t       && rtx_equal_p (SUBREG_REG (XEXP (XEXP (t, 0), 1)), f)\n \t       && (num_sign_bit_copies (f, GET_MODE (f))\n \t\t   > (unsigned int)\n-\t\t     (GET_MODE_BITSIZE (mode)\n-\t\t      - GET_MODE_BITSIZE (GET_MODE (XEXP (XEXP (t, 0), 1))))))\n+\t\t     (GET_MODE_PRECISION (mode)\n+\t\t      - GET_MODE_PRECISION (GET_MODE (XEXP (XEXP (t, 0), 1))))))\n \t{\n \t  c1 = XEXP (XEXP (t, 0), 0); z = f; op = GET_CODE (XEXP (t, 0));\n \t  extend_op = SIGN_EXTEND;\n@@ -6269,7 +6270,7 @@ simplify_if_then_else (rtx x)\n       && ((1 == nonzero_bits (XEXP (cond, 0), mode)\n \t   && (i = exact_log2 (UINTVAL (true_rtx))) >= 0)\n \t  || ((num_sign_bit_copies (XEXP (cond, 0), mode)\n-\t       == GET_MODE_BITSIZE (mode))\n+\t       == GET_MODE_PRECISION (mode))\n \t      && (i = exact_log2 (-UINTVAL (true_rtx))) >= 0)))\n     return\n       simplify_shift_const (NULL_RTX, ASHIFT, mode,\n@@ -6535,8 +6536,8 @@ simplify_set (rtx x)\n   if (dest == cc0_rtx\n       && GET_CODE (src) == SUBREG\n       && subreg_lowpart_p (src)\n-      && (GET_MODE_BITSIZE (GET_MODE (src))\n-\t  < GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (src)))))\n+      && (GET_MODE_PRECISION (GET_MODE (src))\n+\t  < GET_MODE_PRECISION (GET_MODE (SUBREG_REG (src)))))\n     {\n       rtx inner = SUBREG_REG (src);\n       enum machine_mode inner_mode = GET_MODE (inner);\n@@ -6588,7 +6589,7 @@ simplify_set (rtx x)\n #endif\n       && (num_sign_bit_copies (XEXP (XEXP (src, 0), 0),\n \t\t\t       GET_MODE (XEXP (XEXP (src, 0), 0)))\n-\t  == GET_MODE_BITSIZE (GET_MODE (XEXP (XEXP (src, 0), 0))))\n+\t  == GET_MODE_PRECISION (GET_MODE (XEXP (XEXP (src, 0), 0))))\n       && ! side_effects_p (src))\n     {\n       rtx true_rtx = (GET_CODE (XEXP (src, 0)) == NE\n@@ -6764,7 +6765,7 @@ expand_compound_operation (rtx x)\n       if (! SCALAR_INT_MODE_P (GET_MODE (XEXP (x, 0))))\n \treturn x;\n \n-      len = GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)));\n+      len = GET_MODE_PRECISION (GET_MODE (XEXP (x, 0)));\n       /* If the inner object has VOIDmode (the only way this can happen\n \t is if it is an ASM_OPERANDS), we can't do anything since we don't\n \t know how much masking to do.  */\n@@ -6798,11 +6799,11 @@ expand_compound_operation (rtx x)\n       pos = INTVAL (XEXP (x, 2));\n \n       /* This should stay within the object being extracted, fail otherwise.  */\n-      if (len + pos > GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0))))\n+      if (len + pos > GET_MODE_PRECISION (GET_MODE (XEXP (x, 0))))\n \treturn x;\n \n       if (BITS_BIG_ENDIAN)\n-\tpos = GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0))) - len - pos;\n+\tpos = GET_MODE_PRECISION (GET_MODE (XEXP (x, 0))) - len - pos;\n \n       break;\n \n@@ -6863,7 +6864,7 @@ expand_compound_operation (rtx x)\n       if (GET_CODE (XEXP (x, 0)) == TRUNCATE\n \t  && GET_MODE (XEXP (XEXP (x, 0), 0)) == GET_MODE (x)\n \t  && COMPARISON_P (XEXP (XEXP (x, 0), 0))\n-\t  && (GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)))\n+\t  && (GET_MODE_PRECISION (GET_MODE (XEXP (x, 0)))\n \t      <= HOST_BITS_PER_WIDE_INT)\n \t  && (STORE_FLAG_VALUE & ~GET_MODE_MASK (GET_MODE (XEXP (x, 0)))) == 0)\n \treturn XEXP (XEXP (x, 0), 0);\n@@ -6873,7 +6874,7 @@ expand_compound_operation (rtx x)\n \t  && GET_MODE (SUBREG_REG (XEXP (x, 0))) == GET_MODE (x)\n \t  && subreg_lowpart_p (XEXP (x, 0))\n \t  && COMPARISON_P (SUBREG_REG (XEXP (x, 0)))\n-\t  && (GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)))\n+\t  && (GET_MODE_PRECISION (GET_MODE (XEXP (x, 0)))\n \t      <= HOST_BITS_PER_WIDE_INT)\n \t  && (STORE_FLAG_VALUE & ~GET_MODE_MASK (GET_MODE (XEXP (x, 0)))) == 0)\n \treturn SUBREG_REG (XEXP (x, 0));\n@@ -6895,7 +6896,7 @@ expand_compound_operation (rtx x)\n      extraction.  Then the constant of 31 would be substituted in\n      to produce such a position.  */\n \n-  modewidth = GET_MODE_BITSIZE (GET_MODE (x));\n+  modewidth = GET_MODE_PRECISION (GET_MODE (x));\n   if (modewidth >= pos + len)\n     {\n       enum machine_mode mode = GET_MODE (x);\n@@ -6949,7 +6950,7 @@ expand_field_assignment (const_rtx x)\n \t  && GET_CODE (XEXP (SET_DEST (x), 0)) == SUBREG)\n \t{\n \t  inner = SUBREG_REG (XEXP (SET_DEST (x), 0));\n-\t  len = GET_MODE_BITSIZE (GET_MODE (XEXP (SET_DEST (x), 0)));\n+\t  len = GET_MODE_PRECISION (GET_MODE (XEXP (SET_DEST (x), 0)));\n \t  pos = GEN_INT (subreg_lsb (XEXP (SET_DEST (x), 0)));\n \t}\n       else if (GET_CODE (SET_DEST (x)) == ZERO_EXTRACT\n@@ -6961,23 +6962,23 @@ expand_field_assignment (const_rtx x)\n \n \t  /* A constant position should stay within the width of INNER.  */\n \t  if (CONST_INT_P (pos)\n-\t      && INTVAL (pos) + len > GET_MODE_BITSIZE (GET_MODE (inner)))\n+\t      && INTVAL (pos) + len > GET_MODE_PRECISION (GET_MODE (inner)))\n \t    break;\n \n \t  if (BITS_BIG_ENDIAN)\n \t    {\n \t      if (CONST_INT_P (pos))\n-\t\tpos = GEN_INT (GET_MODE_BITSIZE (GET_MODE (inner)) - len\n+\t\tpos = GEN_INT (GET_MODE_PRECISION (GET_MODE (inner)) - len\n \t\t\t       - INTVAL (pos));\n \t      else if (GET_CODE (pos) == MINUS\n \t\t       && CONST_INT_P (XEXP (pos, 1))\n \t\t       && (INTVAL (XEXP (pos, 1))\n-\t\t\t   == GET_MODE_BITSIZE (GET_MODE (inner)) - len))\n+\t\t\t   == GET_MODE_PRECISION (GET_MODE (inner)) - len))\n \t\t/* If position is ADJUST - X, new position is X.  */\n \t\tpos = XEXP (pos, 0);\n \t      else\n \t\tpos = simplify_gen_binary (MINUS, GET_MODE (pos),\n-\t\t\t\t\t   GEN_INT (GET_MODE_BITSIZE (\n+\t\t\t\t\t   GEN_INT (GET_MODE_PRECISION (\n \t\t\t\t\t\t    GET_MODE (inner))\n \t\t\t\t\t\t    - len),\n \t\t\t\t\t   pos);\n@@ -7152,7 +7153,7 @@ make_extraction (enum machine_mode mode, rtx inner, HOST_WIDE_INT pos,\n \t\t     : BITS_PER_UNIT)) == 0\n \t      /* We can't do this if we are widening INNER_MODE (it\n \t\t may not be aligned, for one thing).  */\n-\t      && GET_MODE_BITSIZE (inner_mode) >= GET_MODE_BITSIZE (tmode)\n+\t      && GET_MODE_PRECISION (inner_mode) >= GET_MODE_PRECISION (tmode)\n \t      && (inner_mode == tmode\n \t\t  || (! mode_dependent_address_p (XEXP (inner, 0))\n \t\t      && ! MEM_VOLATILE_P (inner))))))\n@@ -7170,7 +7171,7 @@ make_extraction (enum machine_mode mode, rtx inner, HOST_WIDE_INT pos,\n \n \t  /* POS counts from lsb, but make OFFSET count in memory order.  */\n \t  if (BYTES_BIG_ENDIAN)\n-\t    offset = (GET_MODE_BITSIZE (is_mode) - len - pos) / BITS_PER_UNIT;\n+\t    offset = (GET_MODE_PRECISION (is_mode) - len - pos) / BITS_PER_UNIT;\n \t  else\n \t    offset = pos / BITS_PER_UNIT;\n \n@@ -7275,7 +7276,7 @@ make_extraction (enum machine_mode mode, rtx inner, HOST_WIDE_INT pos,\n      other cases, we would only be going outside our object in cases when\n      an original shift would have been undefined.  */\n   if (MEM_P (inner)\n-      && ((pos_rtx == 0 && pos + len > GET_MODE_BITSIZE (is_mode))\n+      && ((pos_rtx == 0 && pos + len > GET_MODE_PRECISION (is_mode))\n \t  || (pos_rtx != 0 && len != 1)))\n     return 0;\n \n@@ -7550,7 +7551,7 @@ make_compound_operation (rtx x, enum rtx_code in_code)\n {\n   enum rtx_code code = GET_CODE (x);\n   enum machine_mode mode = GET_MODE (x);\n-  int mode_width = GET_MODE_BITSIZE (mode);\n+  int mode_width = GET_MODE_PRECISION (mode);\n   rtx rhs, lhs;\n   enum rtx_code next_code;\n   int i, j;\n@@ -7709,7 +7710,7 @@ make_compound_operation (rtx x, enum rtx_code in_code)\n \t{\n \t  new_rtx = make_compound_operation (XEXP (XEXP (x, 0), 0), next_code);\n \t  new_rtx = make_extraction (mode, new_rtx,\n-\t\t\t\t (GET_MODE_BITSIZE (mode)\n+\t\t\t\t (GET_MODE_PRECISION (mode)\n \t\t\t\t  - INTVAL (XEXP (XEXP (x, 0), 1))),\n \t\t\t\t NULL_RTX, i, 1, 0, in_code == COMPARE);\n \t}\n@@ -8100,7 +8101,7 @@ force_to_mode (rtx x, enum machine_mode mode, unsigned HOST_WIDE_INT mask,\n   /* It is not valid to do a right-shift in a narrower mode\n      than the one it came in with.  */\n   if ((code == LSHIFTRT || code == ASHIFTRT)\n-      && GET_MODE_BITSIZE (mode) < GET_MODE_BITSIZE (GET_MODE (x)))\n+      && GET_MODE_PRECISION (mode) < GET_MODE_PRECISION (GET_MODE (x)))\n     op_mode = GET_MODE (x);\n \n   /* Truncate MASK to fit OP_MODE.  */\n@@ -8208,7 +8209,7 @@ force_to_mode (rtx x, enum machine_mode mode, unsigned HOST_WIDE_INT mask,\n \t      unsigned HOST_WIDE_INT cval\n \t\t= UINTVAL (XEXP (x, 1))\n \t\t  | (GET_MODE_MASK (GET_MODE (x)) & ~mask);\n-\t      int width = GET_MODE_BITSIZE (GET_MODE (x));\n+\t      int width = GET_MODE_PRECISION (GET_MODE (x));\n \t      rtx y;\n \n \t      /* If MODE is narrower than HOST_WIDE_INT and CVAL is a negative\n@@ -8236,7 +8237,7 @@ force_to_mode (rtx x, enum machine_mode mode, unsigned HOST_WIDE_INT mask,\n \t This may eliminate that PLUS and, later, the AND.  */\n \n       {\n-\tunsigned int width = GET_MODE_BITSIZE (mode);\n+\tunsigned int width = GET_MODE_PRECISION (mode);\n \tunsigned HOST_WIDE_INT smask = mask;\n \n \t/* If MODE is narrower than HOST_WIDE_INT and mask is a negative\n@@ -8304,7 +8305,7 @@ force_to_mode (rtx x, enum machine_mode mode, unsigned HOST_WIDE_INT mask,\n \t  && CONST_INT_P (XEXP (x, 1))\n \t  && ((INTVAL (XEXP (XEXP (x, 0), 1))\n \t       + floor_log2 (INTVAL (XEXP (x, 1))))\n-\t      < GET_MODE_BITSIZE (GET_MODE (x)))\n+\t      < GET_MODE_PRECISION (GET_MODE (x)))\n \t  && (UINTVAL (XEXP (x, 1))\n \t      & ~nonzero_bits (XEXP (x, 0), GET_MODE (x))) == 0)\n \t{\n@@ -8349,18 +8350,18 @@ force_to_mode (rtx x, enum machine_mode mode, unsigned HOST_WIDE_INT mask,\n \n       if (! (CONST_INT_P (XEXP (x, 1))\n \t     && INTVAL (XEXP (x, 1)) >= 0\n-\t     && INTVAL (XEXP (x, 1)) < GET_MODE_BITSIZE (mode))\n+\t     && INTVAL (XEXP (x, 1)) < GET_MODE_PRECISION (mode))\n \t  && ! (GET_MODE (XEXP (x, 1)) != VOIDmode\n \t\t&& (nonzero_bits (XEXP (x, 1), GET_MODE (XEXP (x, 1)))\n-\t\t    < (unsigned HOST_WIDE_INT) GET_MODE_BITSIZE (mode))))\n+\t\t    < (unsigned HOST_WIDE_INT) GET_MODE_PRECISION (mode))))\n \tbreak;\n \n       /* If the shift count is a constant and we can do arithmetic in\n \t the mode of the shift, refine which bits we need.  Otherwise, use the\n \t conservative form of the mask.  */\n       if (CONST_INT_P (XEXP (x, 1))\n \t  && INTVAL (XEXP (x, 1)) >= 0\n-\t  && INTVAL (XEXP (x, 1)) < GET_MODE_BITSIZE (op_mode)\n+\t  && INTVAL (XEXP (x, 1)) < GET_MODE_PRECISION (op_mode)\n \t  && HWI_COMPUTABLE_MODE_P (op_mode))\n \tmask >>= INTVAL (XEXP (x, 1));\n       else\n@@ -8411,17 +8412,17 @@ force_to_mode (rtx x, enum machine_mode mode, unsigned HOST_WIDE_INT mask,\n \t     bit.  */\n \t  && ((INTVAL (XEXP (x, 1))\n \t       + num_sign_bit_copies (XEXP (x, 0), GET_MODE (XEXP (x, 0))))\n-\t      >= GET_MODE_BITSIZE (GET_MODE (x)))\n+\t      >= GET_MODE_PRECISION (GET_MODE (x)))\n \t  && exact_log2 (mask + 1) >= 0\n \t  /* Number of bits left after the shift must be more than the mask\n \t     needs.  */\n \t  && ((INTVAL (XEXP (x, 1)) + exact_log2 (mask + 1))\n-\t      <= GET_MODE_BITSIZE (GET_MODE (x)))\n+\t      <= GET_MODE_PRECISION (GET_MODE (x)))\n \t  /* Must be more sign bit copies than the mask needs.  */\n \t  && ((int) num_sign_bit_copies (XEXP (x, 0), GET_MODE (XEXP (x, 0)))\n \t      >= exact_log2 (mask + 1)))\n \tx = simplify_gen_binary (LSHIFTRT, GET_MODE (x), XEXP (x, 0),\n-\t\t\t\t GEN_INT (GET_MODE_BITSIZE (GET_MODE (x))\n+\t\t\t\t GEN_INT (GET_MODE_PRECISION (GET_MODE (x))\n \t\t\t\t\t  - exact_log2 (mask + 1)));\n \n       goto shiftrt;\n@@ -8448,20 +8449,20 @@ force_to_mode (rtx x, enum machine_mode mode, unsigned HOST_WIDE_INT mask,\n \t     represent a mask for all its bits in a single scalar.\n \t     But we only care about the lower bits, so calculate these.  */\n \n-\t  if (GET_MODE_BITSIZE (GET_MODE (x)) > HOST_BITS_PER_WIDE_INT)\n+\t  if (GET_MODE_PRECISION (GET_MODE (x)) > HOST_BITS_PER_WIDE_INT)\n \t    {\n \t      nonzero = ~(unsigned HOST_WIDE_INT) 0;\n \n-\t      /* GET_MODE_BITSIZE (GET_MODE (x)) - INTVAL (XEXP (x, 1))\n+\t      /* GET_MODE_PRECISION (GET_MODE (x)) - INTVAL (XEXP (x, 1))\n \t\t is the number of bits a full-width mask would have set.\n \t\t We need only shift if these are fewer than nonzero can\n \t\t hold.  If not, we must keep all bits set in nonzero.  */\n \n-\t      if (GET_MODE_BITSIZE (GET_MODE (x)) - INTVAL (XEXP (x, 1))\n+\t      if (GET_MODE_PRECISION (GET_MODE (x)) - INTVAL (XEXP (x, 1))\n \t\t  < HOST_BITS_PER_WIDE_INT)\n \t\tnonzero >>= INTVAL (XEXP (x, 1))\n \t\t\t    + HOST_BITS_PER_WIDE_INT\n-\t\t\t    - GET_MODE_BITSIZE (GET_MODE (x)) ;\n+\t\t\t    - GET_MODE_PRECISION (GET_MODE (x)) ;\n \t    }\n \t  else\n \t    {\n@@ -8481,7 +8482,7 @@ force_to_mode (rtx x, enum machine_mode mode, unsigned HOST_WIDE_INT mask,\n \t    {\n \t      x = simplify_shift_const\n \t\t  (NULL_RTX, LSHIFTRT, GET_MODE (x), XEXP (x, 0),\n-\t\t   GET_MODE_BITSIZE (GET_MODE (x)) - 1 - i);\n+\t\t   GET_MODE_PRECISION (GET_MODE (x)) - 1 - i);\n \n \t      if (GET_CODE (x) != ASHIFTRT)\n \t\treturn force_to_mode (x, mode, mask, next_select);\n@@ -8504,7 +8505,7 @@ force_to_mode (rtx x, enum machine_mode mode, unsigned HOST_WIDE_INT mask,\n \t  && CONST_INT_P (XEXP (x, 1))\n \t  && INTVAL (XEXP (x, 1)) >= 0\n \t  && (INTVAL (XEXP (x, 1))\n-\t      <= GET_MODE_BITSIZE (GET_MODE (x)) - (floor_log2 (mask) + 1))\n+\t      <= GET_MODE_PRECISION (GET_MODE (x)) - (floor_log2 (mask) + 1))\n \t  && GET_CODE (XEXP (x, 0)) == ASHIFT\n \t  && XEXP (XEXP (x, 0), 1) == XEXP (x, 1))\n \treturn force_to_mode (XEXP (XEXP (x, 0), 0), mode, mask,\n@@ -8552,7 +8553,7 @@ force_to_mode (rtx x, enum machine_mode mode, unsigned HOST_WIDE_INT mask,\n \t  && CONST_INT_P (XEXP (XEXP (x, 0), 1))\n \t  && INTVAL (XEXP (XEXP (x, 0), 1)) >= 0\n \t  && (INTVAL (XEXP (XEXP (x, 0), 1)) + floor_log2 (mask)\n-\t      < GET_MODE_BITSIZE (GET_MODE (x)))\n+\t      < GET_MODE_PRECISION (GET_MODE (x)))\n \t  && INTVAL (XEXP (XEXP (x, 0), 1)) < HOST_BITS_PER_WIDE_INT)\n \t{\n \t  temp = gen_int_mode (mask << INTVAL (XEXP (XEXP (x, 0), 1)),\n@@ -8804,7 +8805,7 @@ if_then_else_cond (rtx x, rtx *ptrue, rtx *pfalse)\n      false values when testing X.  */\n   else if (x == constm1_rtx || x == const0_rtx\n \t   || (mode != VOIDmode\n-\t       && num_sign_bit_copies (x, mode) == GET_MODE_BITSIZE (mode)))\n+\t       && num_sign_bit_copies (x, mode) == GET_MODE_PRECISION (mode)))\n     {\n       *ptrue = constm1_rtx, *pfalse = const0_rtx;\n       return x;\n@@ -9136,8 +9137,8 @@ make_field_assignment (rtx x)\n     return x;\n \n   pos = get_pos_from_mask ((~c1) & GET_MODE_MASK (GET_MODE (dest)), &len);\n-  if (pos < 0 || pos + len > GET_MODE_BITSIZE (GET_MODE (dest))\n-      || GET_MODE_BITSIZE (GET_MODE (dest)) > HOST_BITS_PER_WIDE_INT\n+  if (pos < 0 || pos + len > GET_MODE_PRECISION (GET_MODE (dest))\n+      || GET_MODE_PRECISION (GET_MODE (dest)) > HOST_BITS_PER_WIDE_INT\n       || (c1 & nonzero_bits (other, GET_MODE (dest))) != 0)\n     return x;\n \n@@ -9158,7 +9159,7 @@ make_field_assignment (rtx x)\n \t\t\t\t\t\t     other, pos),\n \t\t\t       dest);\n   src = force_to_mode (src, mode,\n-\t\t       GET_MODE_BITSIZE (mode) >= HOST_BITS_PER_WIDE_INT\n+\t\t       GET_MODE_PRECISION (mode) >= HOST_BITS_PER_WIDE_INT\n \t\t       ? ~(unsigned HOST_WIDE_INT) 0\n \t\t       : ((unsigned HOST_WIDE_INT) 1 << len) - 1,\n \t\t       0);\n@@ -9580,7 +9581,7 @@ reg_nonzero_bits_for_combine (const_rtx x, enum machine_mode mode,\n     {\n       unsigned HOST_WIDE_INT mask = rsp->nonzero_bits;\n \n-      if (GET_MODE_BITSIZE (GET_MODE (x)) < GET_MODE_BITSIZE (mode))\n+      if (GET_MODE_PRECISION (GET_MODE (x)) < GET_MODE_PRECISION (mode))\n \t/* We don't know anything about the upper bits.  */\n \tmask |= GET_MODE_MASK (mode) ^ GET_MODE_MASK (GET_MODE (x));\n       *nonzero &= mask;\n@@ -9626,7 +9627,7 @@ reg_num_sign_bit_copies_for_combine (const_rtx x, enum machine_mode mode,\n     return tem;\n \n   if (nonzero_sign_valid && rsp->sign_bit_copies != 0\n-      && GET_MODE_BITSIZE (GET_MODE (x)) == GET_MODE_BITSIZE (mode))\n+      && GET_MODE_PRECISION (GET_MODE (x)) == GET_MODE_PRECISION (mode))\n     *result = rsp->sign_bit_copies;\n \n   return NULL;\n@@ -9651,7 +9652,7 @@ extended_count (const_rtx x, enum machine_mode mode, int unsignedp)\n \n   return (unsignedp\n \t  ? (HWI_COMPUTABLE_MODE_P (mode)\n-\t     ? (unsigned int) (GET_MODE_BITSIZE (mode) - 1\n+\t     ? (unsigned int) (GET_MODE_PRECISION (mode) - 1\n \t\t\t       - floor_log2 (nonzero_bits (x, mode)))\n \t     : 0)\n \t  : num_sign_bit_copies (x, mode) - 1);\n@@ -9802,7 +9803,7 @@ try_widen_shift_mode (enum rtx_code code, rtx op, int count,\n {\n   if (orig_mode == mode)\n     return mode;\n-  gcc_assert (GET_MODE_BITSIZE (mode) > GET_MODE_BITSIZE (orig_mode));\n+  gcc_assert (GET_MODE_PRECISION (mode) > GET_MODE_PRECISION (orig_mode));\n \n   /* In general we can't perform in wider mode for right shift and rotate.  */\n   switch (code)\n@@ -9811,8 +9812,8 @@ try_widen_shift_mode (enum rtx_code code, rtx op, int count,\n       /* We can still widen if the bits brought in from the left are identical\n \t to the sign bit of ORIG_MODE.  */\n       if (num_sign_bit_copies (op, mode)\n-\t  > (unsigned) (GET_MODE_BITSIZE (mode)\n-\t\t\t- GET_MODE_BITSIZE (orig_mode)))\n+\t  > (unsigned) (GET_MODE_PRECISION (mode)\n+\t\t\t- GET_MODE_PRECISION (orig_mode)))\n \treturn mode;\n       return orig_mode;\n \n@@ -9829,7 +9830,7 @@ try_widen_shift_mode (enum rtx_code code, rtx op, int count,\n \t  int care_bits = low_bitmask_len (orig_mode, outer_const);\n \n \t  if (care_bits >= 0\n-\t      && GET_MODE_BITSIZE (orig_mode) - care_bits >= count)\n+\t      && GET_MODE_PRECISION (orig_mode) - care_bits >= count)\n \t    return mode;\n \t}\n       /* fall through */\n@@ -9845,9 +9846,9 @@ try_widen_shift_mode (enum rtx_code code, rtx op, int count,\n     }\n }\n \n-/* Simplify a shift of VAROP by COUNT bits.  CODE says what kind of shift.\n-   The result of the shift is RESULT_MODE.  Return NULL_RTX if we cannot\n-   simplify it.  Otherwise, return a simplified value.\n+/* Simplify a shift of VAROP by ORIG_COUNT bits.  CODE says what kind\n+   of shift.  The result of the shift is RESULT_MODE.  Return NULL_RTX\n+   if we cannot simplify it.  Otherwise, return a simplified value.\n \n    The shift is normally computed in the widest mode we find in VAROP, as\n    long as it isn't a different number of words than RESULT_MODE.  Exceptions\n@@ -9879,7 +9880,7 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n   /* If we were given an invalid count, don't do anything except exactly\n      what was requested.  */\n \n-  if (orig_count < 0 || orig_count >= (int) GET_MODE_BITSIZE (mode))\n+  if (orig_count < 0 || orig_count >= (int) GET_MODE_PRECISION (mode))\n     return NULL_RTX;\n \n   count = orig_count;\n@@ -9896,7 +9897,7 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n       /* Convert ROTATERT to ROTATE.  */\n       if (code == ROTATERT)\n \t{\n-\t  unsigned int bitsize = GET_MODE_BITSIZE (result_mode);;\n+\t  unsigned int bitsize = GET_MODE_PRECISION (result_mode);\n \t  code = ROTATE;\n \t  if (VECTOR_MODE_P (result_mode))\n \t    count = bitsize / GET_MODE_NUNITS (result_mode) - count;\n@@ -9917,12 +9918,12 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n \t multiple operations, each of which are defined, we know what the\n \t result is supposed to be.  */\n \n-      if (count > (GET_MODE_BITSIZE (shift_mode) - 1))\n+      if (count > (GET_MODE_PRECISION (shift_mode) - 1))\n \t{\n \t  if (code == ASHIFTRT)\n-\t    count = GET_MODE_BITSIZE (shift_mode) - 1;\n+\t    count = GET_MODE_PRECISION (shift_mode) - 1;\n \t  else if (code == ROTATE || code == ROTATERT)\n-\t    count %= GET_MODE_BITSIZE (shift_mode);\n+\t    count %= GET_MODE_PRECISION (shift_mode);\n \t  else\n \t    {\n \t      /* We can't simply return zero because there may be an\n@@ -9942,7 +9943,7 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n \t is a no-op.  */\n       if (code == ASHIFTRT\n \t  && (num_sign_bit_copies (varop, shift_mode)\n-\t      == GET_MODE_BITSIZE (shift_mode)))\n+\t      == GET_MODE_PRECISION (shift_mode)))\n \t{\n \t  count = 0;\n \t  break;\n@@ -9955,8 +9956,8 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n \n       if (code == ASHIFTRT\n \t  && (count + num_sign_bit_copies (varop, shift_mode)\n-\t      >= GET_MODE_BITSIZE (shift_mode)))\n-\tcount = GET_MODE_BITSIZE (shift_mode) - 1;\n+\t      >= GET_MODE_PRECISION (shift_mode)))\n+\tcount = GET_MODE_PRECISION (shift_mode) - 1;\n \n       /* We simplify the tests below and elsewhere by converting\n \t ASHIFTRT to LSHIFTRT if we know the sign bit is clear.\n@@ -10086,7 +10087,7 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n \t     AND of a new shift with a mask.  We compute the result below.  */\n \t  if (CONST_INT_P (XEXP (varop, 1))\n \t      && INTVAL (XEXP (varop, 1)) >= 0\n-\t      && INTVAL (XEXP (varop, 1)) < GET_MODE_BITSIZE (GET_MODE (varop))\n+\t      && INTVAL (XEXP (varop, 1)) < GET_MODE_PRECISION (GET_MODE (varop))\n \t      && HWI_COMPUTABLE_MODE_P (result_mode)\n \t      && HWI_COMPUTABLE_MODE_P (mode)\n \t      && !VECTOR_MODE_P (result_mode))\n@@ -10101,11 +10102,11 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n \t\t we have (ashift:M1 (subreg:M1 (ashiftrt:M2 FOO C1) 0) C2)\n \t\t with C2 == GET_MODE_BITSIZE (M1) - GET_MODE_BITSIZE (M2),\n \t\t we can convert it to\n-\t\t (ashiftrt:M1 (ashift:M1 (and:M1 (subreg:M1 FOO 0 C2) C3) C1).\n+\t\t (ashiftrt:M1 (ashift:M1 (and:M1 (subreg:M1 FOO 0) C3) C2) C1).\n \t\t This simplifies certain SIGN_EXTEND operations.  */\n \t      if (code == ASHIFT && first_code == ASHIFTRT\n-\t\t  && count == (GET_MODE_BITSIZE (result_mode)\n-\t\t\t       - GET_MODE_BITSIZE (GET_MODE (varop))))\n+\t\t  && count == (GET_MODE_PRECISION (result_mode)\n+\t\t\t       - GET_MODE_PRECISION (GET_MODE (varop))))\n \t\t{\n \t\t  /* C3 has the low-order C1 bits zero.  */\n \n@@ -10173,7 +10174,7 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n \n \t      if (code == ASHIFTRT\n \t\t  || (code == ROTATE && first_code == ASHIFTRT)\n-\t\t  || GET_MODE_BITSIZE (mode) > HOST_BITS_PER_WIDE_INT\n+\t\t  || GET_MODE_PRECISION (mode) > HOST_BITS_PER_WIDE_INT\n \t\t  || (GET_MODE (varop) != result_mode\n \t\t      && (first_code == ASHIFTRT || first_code == LSHIFTRT\n \t\t\t  || first_code == ROTATE\n@@ -10261,7 +10262,7 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n \t      && XEXP (XEXP (varop, 0), 1) == constm1_rtx\n \t      && (STORE_FLAG_VALUE == 1 || STORE_FLAG_VALUE == -1)\n \t      && (code == LSHIFTRT || code == ASHIFTRT)\n-\t      && count == (GET_MODE_BITSIZE (GET_MODE (varop)) - 1)\n+\t      && count == (GET_MODE_PRECISION (GET_MODE (varop)) - 1)\n \t      && rtx_equal_p (XEXP (XEXP (varop, 0), 0), XEXP (varop, 1)))\n \t    {\n \t      count = 0;\n@@ -10323,12 +10324,12 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n \tcase EQ:\n \t  /* Convert (lshiftrt (eq FOO 0) C) to (xor FOO 1) if STORE_FLAG_VALUE\n \t     says that the sign bit can be tested, FOO has mode MODE, C is\n-\t     GET_MODE_BITSIZE (MODE) - 1, and FOO has only its low-order bit\n+\t     GET_MODE_PRECISION (MODE) - 1, and FOO has only its low-order bit\n \t     that may be nonzero.  */\n \t  if (code == LSHIFTRT\n \t      && XEXP (varop, 1) == const0_rtx\n \t      && GET_MODE (XEXP (varop, 0)) == result_mode\n-\t      && count == (GET_MODE_BITSIZE (result_mode) - 1)\n+\t      && count == (GET_MODE_PRECISION (result_mode) - 1)\n \t      && HWI_COMPUTABLE_MODE_P (result_mode)\n \t      && STORE_FLAG_VALUE == -1\n \t      && nonzero_bits (XEXP (varop, 0), result_mode) == 1\n@@ -10345,7 +10346,7 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n \t  /* (lshiftrt (neg A) C) where A is either 0 or 1 and C is one less\n \t     than the number of bits in the mode is equivalent to A.  */\n \t  if (code == LSHIFTRT\n-\t      && count == (GET_MODE_BITSIZE (result_mode) - 1)\n+\t      && count == (GET_MODE_PRECISION (result_mode) - 1)\n \t      && nonzero_bits (XEXP (varop, 0), result_mode) == 1)\n \t    {\n \t      varop = XEXP (varop, 0);\n@@ -10369,7 +10370,7 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n \t     is one less than the number of bits in the mode is\n \t     equivalent to (xor A 1).  */\n \t  if (code == LSHIFTRT\n-\t      && count == (GET_MODE_BITSIZE (result_mode) - 1)\n+\t      && count == (GET_MODE_PRECISION (result_mode) - 1)\n \t      && XEXP (varop, 1) == constm1_rtx\n \t      && nonzero_bits (XEXP (varop, 0), result_mode) == 1\n \t      && merge_outer_ops (&outer_op, &outer_const, XOR, 1, result_mode,\n@@ -10453,7 +10454,7 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n \n \t  if ((STORE_FLAG_VALUE == 1 || STORE_FLAG_VALUE == -1)\n \t      && GET_CODE (XEXP (varop, 0)) == ASHIFTRT\n-\t      && count == (GET_MODE_BITSIZE (GET_MODE (varop)) - 1)\n+\t      && count == (GET_MODE_PRECISION (GET_MODE (varop)) - 1)\n \t      && (code == LSHIFTRT || code == ASHIFTRT)\n \t      && CONST_INT_P (XEXP (XEXP (varop, 0), 1))\n \t      && INTVAL (XEXP (XEXP (varop, 0), 1)) == count\n@@ -10477,8 +10478,8 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n \t      && GET_CODE (XEXP (varop, 0)) == LSHIFTRT\n \t      && CONST_INT_P (XEXP (XEXP (varop, 0), 1))\n \t      && (INTVAL (XEXP (XEXP (varop, 0), 1))\n-\t\t  >= (GET_MODE_BITSIZE (GET_MODE (XEXP (varop, 0)))\n-\t\t      - GET_MODE_BITSIZE (GET_MODE (varop)))))\n+\t\t  >= (GET_MODE_PRECISION (GET_MODE (XEXP (varop, 0)))\n+\t\t      - GET_MODE_PRECISION (GET_MODE (varop)))))\n \t    {\n \t      rtx varop_inner = XEXP (varop, 0);\n \n@@ -10550,7 +10551,7 @@ simplify_shift_const_1 (enum rtx_code code, enum machine_mode result_mode,\n   if (outer_op != UNKNOWN)\n     {\n       if (GET_RTX_CLASS (outer_op) != RTX_UNARY\n-\t  && GET_MODE_BITSIZE (result_mode) < HOST_BITS_PER_WIDE_INT)\n+\t  && GET_MODE_PRECISION (result_mode) < HOST_BITS_PER_WIDE_INT)\n \touter_const = trunc_int_for_mode (outer_const, result_mode);\n \n       if (outer_op == AND)\n@@ -10852,7 +10853,7 @@ static enum rtx_code\n simplify_compare_const (enum rtx_code code, rtx op0, rtx *pop1)\n {\n   enum machine_mode mode = GET_MODE (op0);\n-  unsigned int mode_width = GET_MODE_BITSIZE (mode);\n+  unsigned int mode_width = GET_MODE_PRECISION (mode);\n   HOST_WIDE_INT const_op = INTVAL (*pop1);\n \n   /* Get the constant we are comparing against and turn off all bits\n@@ -11065,8 +11066,8 @@ simplify_comparison (enum rtx_code code, rtx *pop0, rtx *pop1)\n \t  && XEXP (op0, 1) == XEXP (XEXP (op0, 0), 1)\n \t  && XEXP (op0, 1) == XEXP (XEXP (op1, 0), 1)\n \t  && (INTVAL (XEXP (op0, 1))\n-\t      == (GET_MODE_BITSIZE (GET_MODE (op0))\n-\t\t  - (GET_MODE_BITSIZE\n+\t      == (GET_MODE_PRECISION (GET_MODE (op0))\n+\t\t  - (GET_MODE_PRECISION\n \t\t     (GET_MODE (SUBREG_REG (XEXP (XEXP (op0, 0), 0))))))))\n \t{\n \t  op0 = SUBREG_REG (XEXP (XEXP (op0, 0), 0));\n@@ -11134,7 +11135,7 @@ simplify_comparison (enum rtx_code code, rtx *pop0, rtx *pop1)\n \t      && GET_CODE (inner_op1) == SUBREG\n \t      && (GET_MODE (SUBREG_REG (inner_op0))\n \t\t  == GET_MODE (SUBREG_REG (inner_op1)))\n-\t      && (GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (inner_op0)))\n+\t      && (GET_MODE_PRECISION (GET_MODE (SUBREG_REG (inner_op0)))\n \t\t  <= HOST_BITS_PER_WIDE_INT)\n \t      && (0 == ((~c0) & nonzero_bits (SUBREG_REG (inner_op0),\n \t\t\t\t\t     GET_MODE (SUBREG_REG (inner_op0)))))\n@@ -11197,7 +11198,7 @@ simplify_comparison (enum rtx_code code, rtx *pop0, rtx *pop1)\n   while (CONST_INT_P (op1))\n     {\n       enum machine_mode mode = GET_MODE (op0);\n-      unsigned int mode_width = GET_MODE_BITSIZE (mode);\n+      unsigned int mode_width = GET_MODE_PRECISION (mode);\n       unsigned HOST_WIDE_INT mask = GET_MODE_MASK (mode);\n       int equality_comparison_p;\n       int sign_bit_comparison_p;\n@@ -11231,7 +11232,7 @@ simplify_comparison (enum rtx_code code, rtx *pop0, rtx *pop1)\n       if (sign_bit_comparison_p && HWI_COMPUTABLE_MODE_P (mode))\n \top0 = force_to_mode (op0, mode,\n \t\t\t     (unsigned HOST_WIDE_INT) 1\n-\t\t\t     << (GET_MODE_BITSIZE (mode) - 1),\n+\t\t\t     << (GET_MODE_PRECISION (mode) - 1),\n \t\t\t     0);\n \n       /* Now try cases based on the opcode of OP0.  If none of the cases\n@@ -11262,7 +11263,7 @@ simplify_comparison (enum rtx_code code, rtx *pop0, rtx *pop1)\n \t\t  else\n \t\t    {\n \t\t      mode = new_mode;\n-\t\t      i = (GET_MODE_BITSIZE (mode) - 1 - i);\n+\t\t      i = (GET_MODE_PRECISION (mode) - 1 - i);\n \t\t    }\n \t\t}\n \n@@ -11426,7 +11427,7 @@ simplify_comparison (enum rtx_code code, rtx *pop0, rtx *pop1)\n \n \t  if (mode_width <= HOST_BITS_PER_WIDE_INT\n \t      && subreg_lowpart_p (op0)\n-\t      && GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (op0))) > mode_width\n+\t      && GET_MODE_PRECISION (GET_MODE (SUBREG_REG (op0))) > mode_width\n \t      && GET_CODE (SUBREG_REG (op0)) == PLUS\n \t      && CONST_INT_P (XEXP (SUBREG_REG (op0), 1)))\n \t    {\n@@ -11446,14 +11447,14 @@ simplify_comparison (enum rtx_code code, rtx *pop0, rtx *pop1)\n \t\t       /* (A - C1) sign-extends if it is positive and 1-extends\n \t\t\t  if it is negative, C2 both sign- and 1-extends.  */\n \t\t       || (num_sign_bit_copies (a, inner_mode)\n-\t\t\t   > (unsigned int) (GET_MODE_BITSIZE (inner_mode)\n+\t\t\t   > (unsigned int) (GET_MODE_PRECISION (inner_mode)\n \t\t\t\t\t     - mode_width)\n \t\t\t   && const_op < 0)))\n \t\t  || ((unsigned HOST_WIDE_INT) c1\n \t\t       < (unsigned HOST_WIDE_INT) 1 << (mode_width - 2)\n \t\t      /* (A - C1) always sign-extends, like C2.  */\n \t\t      && num_sign_bit_copies (a, inner_mode)\n-\t\t\t > (unsigned int) (GET_MODE_BITSIZE (inner_mode)\n+\t\t\t > (unsigned int) (GET_MODE_PRECISION (inner_mode)\n \t\t\t\t\t   - (mode_width - 1))))\n \t\t{\n \t\t  op0 = SUBREG_REG (op0);\n@@ -11464,7 +11465,7 @@ simplify_comparison (enum rtx_code code, rtx *pop0, rtx *pop1)\n \t  /* If the inner mode is narrower and we are extracting the low part,\n \t     we can treat the SUBREG as if it were a ZERO_EXTEND.  */\n \t  if (subreg_lowpart_p (op0)\n-\t      && GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (op0))) < mode_width)\n+\t      && GET_MODE_PRECISION (GET_MODE (SUBREG_REG (op0))) < mode_width)\n \t    /* Fall through */ ;\n \t  else\n \t    break;\n@@ -11713,10 +11714,10 @@ simplify_comparison (enum rtx_code code, rtx *pop0, rtx *pop1)\n \t\t     the code has been changed.  */\n \t\t  && (0\n #ifdef WORD_REGISTER_OPERATIONS\n-\t\t      || (mode_width > GET_MODE_BITSIZE (tmode)\n+\t\t      || (mode_width > GET_MODE_PRECISION (tmode)\n \t\t\t  && mode_width <= BITS_PER_WORD)\n #endif\n-\t\t      || (mode_width <= GET_MODE_BITSIZE (tmode)\n+\t\t      || (mode_width <= GET_MODE_PRECISION (tmode)\n \t\t\t  && subreg_lowpart_p (XEXP (op0, 0))))\n \t\t  && CONST_INT_P (XEXP (op0, 1))\n \t\t  && mode_width <= HOST_BITS_PER_WIDE_INT\n@@ -11983,7 +11984,7 @@ simplify_comparison (enum rtx_code code, rtx *pop0, rtx *pop1)\n \t      op1 = gen_lowpart (GET_MODE (op0), op1);\n \t    }\n \t}\n-      else if ((GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (op0)))\n+      else if ((GET_MODE_PRECISION (GET_MODE (SUBREG_REG (op0)))\n \t\t<= HOST_BITS_PER_WIDE_INT)\n \t       && (nonzero_bits (SUBREG_REG (op0),\n \t\t\t\t GET_MODE (SUBREG_REG (op0)))\n@@ -12045,11 +12046,11 @@ simplify_comparison (enum rtx_code code, rtx *pop0, rtx *pop1)\n \n \t  if (zero_extended\n \t      || ((num_sign_bit_copies (op0, tmode)\n-\t\t   > (unsigned int) (GET_MODE_BITSIZE (tmode)\n-\t\t\t\t     - GET_MODE_BITSIZE (mode)))\n+\t\t   > (unsigned int) (GET_MODE_PRECISION (tmode)\n+\t\t\t\t     - GET_MODE_PRECISION (mode)))\n \t\t  && (num_sign_bit_copies (op1, tmode)\n-\t\t      > (unsigned int) (GET_MODE_BITSIZE (tmode)\n-\t\t\t\t\t- GET_MODE_BITSIZE (mode)))))\n+\t\t      > (unsigned int) (GET_MODE_PRECISION (tmode)\n+\t\t\t\t\t- GET_MODE_PRECISION (mode)))))\n \t    {\n \t      /* If OP0 is an AND and we don't have an AND in MODE either,\n \t\t make a new AND in the proper mode.  */\n@@ -12348,7 +12349,7 @@ record_dead_and_set_regs_1 (rtx dest, const_rtx setter, void *data)\n       else if (GET_CODE (setter) == SET\n \t       && GET_CODE (SET_DEST (setter)) == SUBREG\n \t       && SUBREG_REG (SET_DEST (setter)) == dest\n-\t       && GET_MODE_BITSIZE (GET_MODE (dest)) <= BITS_PER_WORD\n+\t       && GET_MODE_PRECISION (GET_MODE (dest)) <= BITS_PER_WORD\n \t       && subreg_lowpart_p (SET_DEST (setter)))\n \trecord_value_for_reg (dest, record_dead_insn,\n \t\t\t      gen_lowpart (GET_MODE (dest),\n@@ -12445,7 +12446,7 @@ record_promoted_value (rtx insn, rtx subreg)\n   unsigned int regno = REGNO (SUBREG_REG (subreg));\n   enum machine_mode mode = GET_MODE (subreg);\n \n-  if (GET_MODE_BITSIZE (mode) > HOST_BITS_PER_WIDE_INT)\n+  if (GET_MODE_PRECISION (mode) > HOST_BITS_PER_WIDE_INT)\n     return;\n \n   for (links = LOG_LINKS (insn); links;)"}, {"sha": "a078329ac553c18c232a8af5c49ea3bf308fdc41", "filename": "gcc/cse.c", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/gcc%2Fcse.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/gcc%2Fcse.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcse.c?ref=5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d", "patch": "@@ -3650,7 +3650,7 @@ fold_rtx (rtx x, rtx insn)\n \t      enum rtx_code associate_code;\n \n \t      if (is_shift\n-\t\t  && (INTVAL (const_arg1) >= GET_MODE_BITSIZE (mode)\n+\t\t  && (INTVAL (const_arg1) >= GET_MODE_PRECISION (mode)\n \t\t      || INTVAL (const_arg1) < 0))\n \t\t{\n \t\t  if (SHIFT_COUNT_TRUNCATED)\n@@ -3699,7 +3699,7 @@ fold_rtx (rtx x, rtx insn)\n                 break;\n \n \t      if (is_shift\n-\t\t  && (INTVAL (inner_const) >= GET_MODE_BITSIZE (mode)\n+\t\t  && (INTVAL (inner_const) >= GET_MODE_PRECISION (mode)\n \t\t      || INTVAL (inner_const) < 0))\n \t\t{\n \t\t  if (SHIFT_COUNT_TRUNCATED)\n@@ -3729,7 +3729,7 @@ fold_rtx (rtx x, rtx insn)\n \n \t      if (is_shift\n \t\t  && CONST_INT_P (new_const)\n-\t\t  && INTVAL (new_const) >= GET_MODE_BITSIZE (mode))\n+\t\t  && INTVAL (new_const) >= GET_MODE_PRECISION (mode))\n \t\t{\n \t\t  /* As an exception, we can turn an ASHIFTRT of this\n \t\t     form into a shift of the number of bits - 1.  */\n@@ -4672,13 +4672,13 @@ cse_insn (rtx insn)\n \n       if (src_const && src_related == 0 && CONST_INT_P (src_const)\n \t  && GET_MODE_CLASS (mode) == MODE_INT\n-\t  && GET_MODE_BITSIZE (mode) < BITS_PER_WORD)\n+\t  && GET_MODE_PRECISION (mode) < BITS_PER_WORD)\n \t{\n \t  enum machine_mode wider_mode;\n \n \t  for (wider_mode = GET_MODE_WIDER_MODE (mode);\n \t       wider_mode != VOIDmode\n-\t       && GET_MODE_BITSIZE (wider_mode) <= BITS_PER_WORD\n+\t       && GET_MODE_PRECISION (wider_mode) <= BITS_PER_WORD\n \t       && src_related == 0;\n \t       wider_mode = GET_MODE_WIDER_MODE (wider_mode))\n \t    {\n@@ -5031,7 +5031,7 @@ cse_insn (rtx insn)\n \t      && CONST_INT_P (XEXP (SET_DEST (sets[i].rtl), 1))\n \t      && CONST_INT_P (XEXP (SET_DEST (sets[i].rtl), 2))\n \t      && REG_P (XEXP (SET_DEST (sets[i].rtl), 0))\n-\t      && (GET_MODE_BITSIZE (GET_MODE (SET_DEST (sets[i].rtl)))\n+\t      && (GET_MODE_PRECISION (GET_MODE (SET_DEST (sets[i].rtl)))\n \t\t  >= INTVAL (XEXP (SET_DEST (sets[i].rtl), 1)))\n \t      && ((unsigned) INTVAL (XEXP (SET_DEST (sets[i].rtl), 1))\n \t\t  + (unsigned) INTVAL (XEXP (SET_DEST (sets[i].rtl), 2))\n@@ -5058,7 +5058,7 @@ cse_insn (rtx insn)\n \t\t  HOST_WIDE_INT mask;\n \t\t  unsigned int shift;\n \t\t  if (BITS_BIG_ENDIAN)\n-\t\t    shift = GET_MODE_BITSIZE (GET_MODE (dest_reg))\n+\t\t    shift = GET_MODE_PRECISION (GET_MODE (dest_reg))\n \t\t\t    - INTVAL (pos) - INTVAL (width);\n \t\t  else\n \t\t    shift = INTVAL (pos);"}, {"sha": "3c692f4074ec986011e28deb5dfb143d0ec4ac4c", "filename": "gcc/explow.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/gcc%2Fexplow.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/gcc%2Fexplow.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fexplow.c?ref=5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d", "patch": "@@ -51,7 +51,7 @@ static rtx break_out_memory_refs (rtx);\n HOST_WIDE_INT\n trunc_int_for_mode (HOST_WIDE_INT c, enum machine_mode mode)\n {\n-  int width = GET_MODE_BITSIZE (mode);\n+  int width = GET_MODE_PRECISION (mode);\n \n   /* You want to truncate to a _what_?  */\n   gcc_assert (SCALAR_INT_MODE_P (mode));"}, {"sha": "f8429c4fd2800fc830ffd75040e45ab8a5e159ce", "filename": "gcc/loop-doloop.c", "status": "modified", "additions": 4, "deletions": 4, "changes": 8, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/gcc%2Floop-doloop.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/gcc%2Floop-doloop.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Floop-doloop.c?ref=5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d", "patch": "@@ -465,7 +465,7 @@ doloop_modify (struct loop *loop, struct niter_desc *desc,\n \t Note that the maximum value loaded is iterations_max - 1.  */\n       if (desc->niter_max\n \t  <= ((unsigned HOST_WIDEST_INT) 1\n-\t      << (GET_MODE_BITSIZE (mode) - 1)))\n+\t      << (GET_MODE_PRECISION (mode) - 1)))\n \tnonneg = 1;\n       break;\n \n@@ -677,18 +677,18 @@ doloop_optimize (struct loop *loop)\n   doloop_seq = gen_doloop_end (doloop_reg, iterations, iterations_max,\n \t\t\t       GEN_INT (level), start_label);\n \n-  word_mode_size = GET_MODE_BITSIZE (word_mode);\n+  word_mode_size = GET_MODE_PRECISION (word_mode);\n   word_mode_max\n \t  = ((unsigned HOST_WIDE_INT) 1 << (word_mode_size - 1) << 1) - 1;\n   if (! doloop_seq\n       && mode != word_mode\n       /* Before trying mode different from the one in that # of iterations is\n \t computed, we must be sure that the number of iterations fits into\n \t the new mode.  */\n-      && (word_mode_size >= GET_MODE_BITSIZE (mode)\n+      && (word_mode_size >= GET_MODE_PRECISION (mode)\n \t  || desc->niter_max <= word_mode_max))\n     {\n-      if (word_mode_size > GET_MODE_BITSIZE (mode))\n+      if (word_mode_size > GET_MODE_PRECISION (mode))\n \t{\n \t  zero_extend_p = true;\n \t  iterations = simplify_gen_unary (ZERO_EXTEND, word_mode,"}, {"sha": "ac9da152c3c7db300236876a3fc1a87a5c62d546", "filename": "gcc/rtlanal.c", "status": "modified", "additions": 39, "deletions": 39, "changes": 78, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/gcc%2Frtlanal.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/gcc%2Frtlanal.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtlanal.c?ref=5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d", "patch": "@@ -3177,7 +3177,7 @@ subreg_lsb_1 (enum machine_mode outer_mode,\n   unsigned int word;\n \n   /* A paradoxical subreg begins at bit position 0.  */\n-  if (GET_MODE_BITSIZE (outer_mode) > GET_MODE_BITSIZE (inner_mode))\n+  if (GET_MODE_PRECISION (outer_mode) > GET_MODE_PRECISION (inner_mode))\n     return 0;\n \n   if (WORDS_BIG_ENDIAN != BYTES_BIG_ENDIAN)\n@@ -3281,7 +3281,7 @@ subreg_get_info (unsigned int xregno, enum machine_mode xmode,\n   /* Paradoxical subregs are otherwise valid.  */\n   if (!rknown\n       && offset == 0\n-      && GET_MODE_SIZE (ymode) > GET_MODE_SIZE (xmode))\n+      && GET_MODE_PRECISION (ymode) > GET_MODE_PRECISION (xmode))\n     {\n       info->representable_p = true;\n       /* If this is a big endian paradoxical subreg, which uses more\n@@ -3850,19 +3850,19 @@ nonzero_bits1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n   unsigned HOST_WIDE_INT inner_nz;\n   enum rtx_code code;\n   enum machine_mode inner_mode;\n-  unsigned int mode_width = GET_MODE_BITSIZE (mode);\n+  unsigned int mode_width = GET_MODE_PRECISION (mode);\n \n   /* For floating-point and vector values, assume all bits are needed.  */\n   if (FLOAT_MODE_P (GET_MODE (x)) || FLOAT_MODE_P (mode)\n       || VECTOR_MODE_P (GET_MODE (x)) || VECTOR_MODE_P (mode))\n     return nonzero;\n \n   /* If X is wider than MODE, use its mode instead.  */\n-  if (GET_MODE_BITSIZE (GET_MODE (x)) > mode_width)\n+  if (GET_MODE_PRECISION (GET_MODE (x)) > mode_width)\n     {\n       mode = GET_MODE (x);\n       nonzero = GET_MODE_MASK (mode);\n-      mode_width = GET_MODE_BITSIZE (mode);\n+      mode_width = GET_MODE_PRECISION (mode);\n     }\n \n   if (mode_width > HOST_BITS_PER_WIDE_INT)\n@@ -3879,9 +3879,9 @@ nonzero_bits1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n      not known to be zero.  */\n \n   if (GET_MODE (x) != VOIDmode && GET_MODE (x) != mode\n-      && GET_MODE_BITSIZE (GET_MODE (x)) <= BITS_PER_WORD\n-      && GET_MODE_BITSIZE (GET_MODE (x)) <= HOST_BITS_PER_WIDE_INT\n-      && GET_MODE_BITSIZE (mode) > GET_MODE_BITSIZE (GET_MODE (x)))\n+      && GET_MODE_PRECISION (GET_MODE (x)) <= BITS_PER_WORD\n+      && GET_MODE_PRECISION (GET_MODE (x)) <= HOST_BITS_PER_WIDE_INT\n+      && GET_MODE_PRECISION (mode) > GET_MODE_PRECISION (GET_MODE (x)))\n     {\n       nonzero &= cached_nonzero_bits (x, GET_MODE (x),\n \t\t\t\t      known_x, known_mode, known_ret);\n@@ -3989,7 +3989,7 @@ nonzero_bits1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n       /* Disabled to avoid exponential mutual recursion between nonzero_bits\n \t and num_sign_bit_copies.  */\n       if (num_sign_bit_copies (XEXP (x, 0), GET_MODE (x))\n-\t  == GET_MODE_BITSIZE (GET_MODE (x)))\n+\t  == GET_MODE_PRECISION (GET_MODE (x)))\n \tnonzero = 1;\n #endif\n \n@@ -4002,7 +4002,7 @@ nonzero_bits1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n       /* Disabled to avoid exponential mutual recursion between nonzero_bits\n \t and num_sign_bit_copies.  */\n       if (num_sign_bit_copies (XEXP (x, 0), GET_MODE (x))\n-\t  == GET_MODE_BITSIZE (GET_MODE (x)))\n+\t  == GET_MODE_PRECISION (GET_MODE (x)))\n \tnonzero = 1;\n #endif\n       break;\n@@ -4075,7 +4075,7 @@ nonzero_bits1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n \tunsigned HOST_WIDE_INT nz1\n \t  = cached_nonzero_bits (XEXP (x, 1), mode,\n \t\t\t\t known_x, known_mode, known_ret);\n-\tint sign_index = GET_MODE_BITSIZE (GET_MODE (x)) - 1;\n+\tint sign_index = GET_MODE_PRECISION (GET_MODE (x)) - 1;\n \tint width0 = floor_log2 (nz0) + 1;\n \tint width1 = floor_log2 (nz1) + 1;\n \tint low0 = floor_log2 (nz0 & -nz0);\n@@ -4156,8 +4156,8 @@ nonzero_bits1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n       /* If the inner mode is a single word for both the host and target\n \t machines, we can compute this from which bits of the inner\n \t object might be nonzero.  */\n-      if (GET_MODE_BITSIZE (inner_mode) <= BITS_PER_WORD\n-\t  && (GET_MODE_BITSIZE (inner_mode) <= HOST_BITS_PER_WIDE_INT))\n+      if (GET_MODE_PRECISION (inner_mode) <= BITS_PER_WORD\n+\t  && (GET_MODE_PRECISION (inner_mode) <= HOST_BITS_PER_WIDE_INT))\n \t{\n \t  nonzero &= cached_nonzero_bits (SUBREG_REG (x), mode,\n \t\t\t\t\t  known_x, known_mode, known_ret);\n@@ -4174,8 +4174,8 @@ nonzero_bits1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n \t      /* On many CISC machines, accessing an object in a wider mode\n \t\t causes the high-order bits to become undefined.  So they are\n \t\t not known to be zero.  */\n-\t      if (GET_MODE_SIZE (GET_MODE (x))\n-\t\t  > GET_MODE_SIZE (inner_mode))\n+\t      if (GET_MODE_PRECISION (GET_MODE (x))\n+\t\t  > GET_MODE_PRECISION (inner_mode))\n \t\tnonzero |= (GET_MODE_MASK (GET_MODE (x))\n \t\t\t    & ~GET_MODE_MASK (inner_mode));\n \t    }\n@@ -4195,10 +4195,10 @@ nonzero_bits1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n       if (CONST_INT_P (XEXP (x, 1))\n \t  && INTVAL (XEXP (x, 1)) >= 0\n \t  && INTVAL (XEXP (x, 1)) < HOST_BITS_PER_WIDE_INT\n-\t  && INTVAL (XEXP (x, 1)) < GET_MODE_BITSIZE (GET_MODE (x)))\n+\t  && INTVAL (XEXP (x, 1)) < GET_MODE_PRECISION (GET_MODE (x)))\n \t{\n \t  enum machine_mode inner_mode = GET_MODE (x);\n-\t  unsigned int width = GET_MODE_BITSIZE (inner_mode);\n+\t  unsigned int width = GET_MODE_PRECISION (inner_mode);\n \t  int count = INTVAL (XEXP (x, 1));\n \t  unsigned HOST_WIDE_INT mode_mask = GET_MODE_MASK (inner_mode);\n \t  unsigned HOST_WIDE_INT op_nonzero\n@@ -4351,7 +4351,7 @@ num_sign_bit_copies1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n \t\t      unsigned int known_ret)\n {\n   enum rtx_code code = GET_CODE (x);\n-  unsigned int bitwidth = GET_MODE_BITSIZE (mode);\n+  unsigned int bitwidth = GET_MODE_PRECISION (mode);\n   int num0, num1, result;\n   unsigned HOST_WIDE_INT nonzero;\n \n@@ -4367,26 +4367,26 @@ num_sign_bit_copies1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n     return 1;\n \n   /* For a smaller object, just ignore the high bits.  */\n-  if (bitwidth < GET_MODE_BITSIZE (GET_MODE (x)))\n+  if (bitwidth < GET_MODE_PRECISION (GET_MODE (x)))\n     {\n       num0 = cached_num_sign_bit_copies (x, GET_MODE (x),\n \t\t\t\t\t known_x, known_mode, known_ret);\n       return MAX (1,\n-\t\t  num0 - (int) (GET_MODE_BITSIZE (GET_MODE (x)) - bitwidth));\n+\t\t  num0 - (int) (GET_MODE_PRECISION (GET_MODE (x)) - bitwidth));\n     }\n \n-  if (GET_MODE (x) != VOIDmode && bitwidth > GET_MODE_BITSIZE (GET_MODE (x)))\n+  if (GET_MODE (x) != VOIDmode && bitwidth > GET_MODE_PRECISION (GET_MODE (x)))\n     {\n #ifndef WORD_REGISTER_OPERATIONS\n-  /* If this machine does not do all register operations on the entire\n-     register and MODE is wider than the mode of X, we can say nothing\n-     at all about the high-order bits.  */\n+      /* If this machine does not do all register operations on the entire\n+\t register and MODE is wider than the mode of X, we can say nothing\n+\t at all about the high-order bits.  */\n       return 1;\n #else\n       /* Likewise on machines that do, if the mode of the object is smaller\n \t than a word and loads of that size don't sign extend, we can say\n \t nothing about the high order bits.  */\n-      if (GET_MODE_BITSIZE (GET_MODE (x)) < BITS_PER_WORD\n+      if (GET_MODE_PRECISION (GET_MODE (x)) < BITS_PER_WORD\n #ifdef LOAD_EXTEND_OP\n \t  && LOAD_EXTEND_OP (GET_MODE (x)) != SIGN_EXTEND\n #endif\n@@ -4408,7 +4408,7 @@ num_sign_bit_copies1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n       if (target_default_pointer_address_modes_p ()\n \t  && ! POINTERS_EXTEND_UNSIGNED && GET_MODE (x) == Pmode\n \t  && mode == Pmode && REG_POINTER (x))\n-\treturn GET_MODE_BITSIZE (Pmode) - GET_MODE_BITSIZE (ptr_mode) + 1;\n+\treturn GET_MODE_PRECISION (Pmode) - GET_MODE_PRECISION (ptr_mode) + 1;\n #endif\n \n       {\n@@ -4433,7 +4433,7 @@ num_sign_bit_copies1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n       /* Some RISC machines sign-extend all loads of smaller than a word.  */\n       if (LOAD_EXTEND_OP (GET_MODE (x)) == SIGN_EXTEND)\n \treturn MAX (1, ((int) bitwidth\n-\t\t\t- (int) GET_MODE_BITSIZE (GET_MODE (x)) + 1));\n+\t\t\t- (int) GET_MODE_PRECISION (GET_MODE (x)) + 1));\n #endif\n       break;\n \n@@ -4457,17 +4457,17 @@ num_sign_bit_copies1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n \t  num0 = cached_num_sign_bit_copies (SUBREG_REG (x), mode,\n \t\t\t\t\t     known_x, known_mode, known_ret);\n \t  return MAX ((int) bitwidth\n-\t\t      - (int) GET_MODE_BITSIZE (GET_MODE (x)) + 1,\n+\t\t      - (int) GET_MODE_PRECISION (GET_MODE (x)) + 1,\n \t\t      num0);\n \t}\n \n       /* For a smaller object, just ignore the high bits.  */\n-      if (bitwidth <= GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (x))))\n+      if (bitwidth <= GET_MODE_PRECISION (GET_MODE (SUBREG_REG (x))))\n \t{\n \t  num0 = cached_num_sign_bit_copies (SUBREG_REG (x), VOIDmode,\n \t\t\t\t\t     known_x, known_mode, known_ret);\n \t  return MAX (1, (num0\n-\t\t\t  - (int) (GET_MODE_BITSIZE (GET_MODE (SUBREG_REG (x)))\n+\t\t\t  - (int) (GET_MODE_PRECISION (GET_MODE (SUBREG_REG (x)))\n \t\t\t\t   - bitwidth)));\n \t}\n \n@@ -4498,15 +4498,15 @@ num_sign_bit_copies1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n       break;\n \n     case SIGN_EXTEND:\n-      return (bitwidth - GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)))\n+      return (bitwidth - GET_MODE_PRECISION (GET_MODE (XEXP (x, 0)))\n \t      + cached_num_sign_bit_copies (XEXP (x, 0), VOIDmode,\n \t\t\t\t\t    known_x, known_mode, known_ret));\n \n     case TRUNCATE:\n       /* For a smaller object, just ignore the high bits.  */\n       num0 = cached_num_sign_bit_copies (XEXP (x, 0), VOIDmode,\n \t\t\t\t\t known_x, known_mode, known_ret);\n-      return MAX (1, (num0 - (int) (GET_MODE_BITSIZE (GET_MODE (XEXP (x, 0)))\n+      return MAX (1, (num0 - (int) (GET_MODE_PRECISION (GET_MODE (XEXP (x, 0)))\n \t\t\t\t    - bitwidth)));\n \n     case NOT:\n@@ -4683,7 +4683,7 @@ num_sign_bit_copies1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n \t\t\t\t\t known_x, known_mode, known_ret);\n       if (CONST_INT_P (XEXP (x, 1))\n \t  && INTVAL (XEXP (x, 1)) > 0\n-\t  && INTVAL (XEXP (x, 1)) < GET_MODE_BITSIZE (GET_MODE (x)))\n+\t  && INTVAL (XEXP (x, 1)) < GET_MODE_PRECISION (GET_MODE (x)))\n \tnum0 = MIN ((int) bitwidth, num0 + INTVAL (XEXP (x, 1)));\n \n       return num0;\n@@ -4693,7 +4693,7 @@ num_sign_bit_copies1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n       if (!CONST_INT_P (XEXP (x, 1))\n \t  || INTVAL (XEXP (x, 1)) < 0\n \t  || INTVAL (XEXP (x, 1)) >= (int) bitwidth\n-\t  || INTVAL (XEXP (x, 1)) >= GET_MODE_BITSIZE (GET_MODE (x)))\n+\t  || INTVAL (XEXP (x, 1)) >= GET_MODE_PRECISION (GET_MODE (x)))\n \treturn 1;\n \n       num0 = cached_num_sign_bit_copies (XEXP (x, 0), mode,\n@@ -4729,7 +4729,7 @@ num_sign_bit_copies1 (const_rtx x, enum machine_mode mode, const_rtx known_x,\n      count those bits and return one less than that amount.  If we can't\n      safely compute the mask for this mode, always return BITWIDTH.  */\n \n-  bitwidth = GET_MODE_BITSIZE (mode);\n+  bitwidth = GET_MODE_PRECISION (mode);\n   if (bitwidth > HOST_BITS_PER_WIDE_INT)\n     return 1;\n \n@@ -4998,7 +4998,7 @@ canonicalize_condition (rtx insn, rtx cond, int reverse, rtx *earliest,\n   if (GET_MODE_CLASS (GET_MODE (op0)) != MODE_CC\n       && CONST_INT_P (op1)\n       && GET_MODE (op0) != VOIDmode\n-      && GET_MODE_BITSIZE (GET_MODE (op0)) <= HOST_BITS_PER_WIDE_INT)\n+      && GET_MODE_PRECISION (GET_MODE (op0)) <= HOST_BITS_PER_WIDE_INT)\n     {\n       HOST_WIDE_INT const_val = INTVAL (op1);\n       unsigned HOST_WIDE_INT uconst_val = const_val;\n@@ -5017,7 +5017,7 @@ canonicalize_condition (rtx insn, rtx cond, int reverse, rtx *earliest,\n \tcase GE:\n \t  if ((const_val & max_val)\n \t      != ((unsigned HOST_WIDE_INT) 1\n-\t\t  << (GET_MODE_BITSIZE (GET_MODE (op0)) - 1)))\n+\t\t  << (GET_MODE_PRECISION (GET_MODE (op0)) - 1)))\n \t    code = GT, op1 = gen_int_mode (const_val - 1, GET_MODE (op0));\n \t  break;\n \n@@ -5123,7 +5123,7 @@ init_num_sign_bit_copies_in_rep (void)\n \t\t   have to be sign-bit copies too.  */\n \t\t|| num_sign_bit_copies_in_rep [in_mode][mode])\n \t      num_sign_bit_copies_in_rep [in_mode][mode]\n-\t\t+= GET_MODE_BITSIZE (wider) - GET_MODE_BITSIZE (i);\n+\t\t+= GET_MODE_PRECISION (wider) - GET_MODE_PRECISION (i);\n \t  }\n       }\n }\n@@ -5183,7 +5183,7 @@ low_bitmask_len (enum machine_mode mode, unsigned HOST_WIDE_INT m)\n {\n   if (mode != VOIDmode)\n     {\n-      if (GET_MODE_BITSIZE (mode) > HOST_BITS_PER_WIDE_INT)\n+      if (GET_MODE_PRECISION (mode) > HOST_BITS_PER_WIDE_INT)\n \treturn -1;\n       m &= GET_MODE_MASK (mode);\n     }"}, {"sha": "5a98b69abbc97bb58b7fb89cef23bffaae5737e5", "filename": "gcc/simplify-rtx.c", "status": "modified", "additions": 67, "deletions": 65, "changes": 132, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/gcc%2Fsimplify-rtx.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d/gcc%2Fsimplify-rtx.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fsimplify-rtx.c?ref=5511bc5ada2bd2bc9ab835d0ed9fd96a83d3260d", "patch": "@@ -649,7 +649,7 @@ simplify_unary_operation_1 (enum rtx_code code, enum machine_mode mode, rtx op)\n       if (STORE_FLAG_VALUE == -1\n \t  && GET_CODE (op) == ASHIFTRT\n \t  && GET_CODE (XEXP (op, 1))\n-\t  && INTVAL (XEXP (op, 1)) == GET_MODE_BITSIZE (mode) - 1)\n+\t  && INTVAL (XEXP (op, 1)) == GET_MODE_PRECISION (mode) - 1)\n \treturn simplify_gen_relational (GE, mode, VOIDmode,\n \t\t\t\t\tXEXP (op, 0), const0_rtx);\n \n@@ -765,15 +765,15 @@ simplify_unary_operation_1 (enum rtx_code code, enum machine_mode mode, rtx op)\n \t C is equal to the width of MODE minus 1.  */\n       if (GET_CODE (op) == ASHIFTRT\n \t  && CONST_INT_P (XEXP (op, 1))\n-\t  && INTVAL (XEXP (op, 1)) == GET_MODE_BITSIZE (mode) - 1)\n+\t  && INTVAL (XEXP (op, 1)) == GET_MODE_PRECISION (mode) - 1)\n \treturn simplify_gen_binary (LSHIFTRT, mode,\n \t\t\t\t    XEXP (op, 0), XEXP (op, 1));\n \n       /* (neg (lshiftrt X C)) can be replaced by (ashiftrt X C) when\n \t C is equal to the width of MODE minus 1.  */\n       if (GET_CODE (op) == LSHIFTRT\n \t  && CONST_INT_P (XEXP (op, 1))\n-\t  && INTVAL (XEXP (op, 1)) == GET_MODE_BITSIZE (mode) - 1)\n+\t  && INTVAL (XEXP (op, 1)) == GET_MODE_PRECISION (mode) - 1)\n \treturn simplify_gen_binary (ASHIFTRT, mode,\n \t\t\t\t    XEXP (op, 0), XEXP (op, 1));\n \n@@ -790,14 +790,14 @@ simplify_unary_operation_1 (enum rtx_code code, enum machine_mode mode, rtx op)\n \t  && SCALAR_INT_MODE_P (GET_MODE (XEXP (op, 0))))\n \t{\n \t  enum machine_mode inner = GET_MODE (XEXP (op, 0));\n-\t  int isize = GET_MODE_BITSIZE (inner);\n+\t  int isize = GET_MODE_PRECISION (inner);\n \t  if (STORE_FLAG_VALUE == 1)\n \t    {\n \t      temp = simplify_gen_binary (ASHIFTRT, inner, XEXP (op, 0),\n \t\t\t\t\t  GEN_INT (isize - 1));\n \t      if (mode == inner)\n \t\treturn temp;\n-\t      if (GET_MODE_BITSIZE (mode) > isize)\n+\t      if (GET_MODE_PRECISION (mode) > isize)\n \t\treturn simplify_gen_unary (SIGN_EXTEND, mode, temp, inner);\n \t      return simplify_gen_unary (TRUNCATE, mode, temp, inner);\n \t    }\n@@ -807,7 +807,7 @@ simplify_unary_operation_1 (enum rtx_code code, enum machine_mode mode, rtx op)\n \t\t\t\t\t  GEN_INT (isize - 1));\n \t      if (mode == inner)\n \t\treturn temp;\n-\t      if (GET_MODE_BITSIZE (mode) > isize)\n+\t      if (GET_MODE_PRECISION (mode) > isize)\n \t\treturn simplify_gen_unary (ZERO_EXTEND, mode, temp, inner);\n \t      return simplify_gen_unary (TRUNCATE, mode, temp, inner);\n \t    }\n@@ -854,8 +854,8 @@ simplify_unary_operation_1 (enum rtx_code code, enum machine_mode mode, rtx op)\n          patterns.  */\n       if ((TRULY_NOOP_TRUNCATION_MODES_P (mode, GET_MODE (op))\n \t   ? (num_sign_bit_copies (op, GET_MODE (op))\n-\t      > (unsigned int) (GET_MODE_BITSIZE (GET_MODE (op))\n-\t\t\t\t- GET_MODE_BITSIZE (mode)))\n+\t      > (unsigned int) (GET_MODE_PRECISION (GET_MODE (op))\n+\t\t\t\t- GET_MODE_PRECISION (mode)))\n \t   : truncated_to_mode (mode, op))\n \t  && ! (GET_CODE (op) == LSHIFTRT\n \t\t&& GET_CODE (XEXP (op, 0)) == MULT))\n@@ -904,7 +904,7 @@ simplify_unary_operation_1 (enum rtx_code code, enum machine_mode mode, rtx op)\n \t  && (flag_unsafe_math_optimizations\n \t      || (SCALAR_FLOAT_MODE_P (GET_MODE (op))\n \t\t  && ((unsigned)significand_size (GET_MODE (op))\n-\t\t      >= (GET_MODE_BITSIZE (GET_MODE (XEXP (op, 0)))\n+\t\t      >= (GET_MODE_PRECISION (GET_MODE (XEXP (op, 0)))\n \t\t\t  - num_sign_bit_copies (XEXP (op, 0),\n \t\t\t\t\t\t GET_MODE (XEXP (op, 0))))))))\n \treturn simplify_gen_unary (FLOAT, mode,\n@@ -941,7 +941,7 @@ simplify_unary_operation_1 (enum rtx_code code, enum machine_mode mode, rtx op)\n \t  || (GET_CODE (op) == FLOAT\n \t      && SCALAR_FLOAT_MODE_P (GET_MODE (op))\n \t      && ((unsigned)significand_size (GET_MODE (op))\n-\t\t  >= (GET_MODE_BITSIZE (GET_MODE (XEXP (op, 0)))\n+\t\t  >= (GET_MODE_PRECISION (GET_MODE (XEXP (op, 0)))\n \t\t      - num_sign_bit_copies (XEXP (op, 0),\n \t\t\t\t\t     GET_MODE (XEXP (op, 0)))))))\n \treturn simplify_gen_unary (GET_CODE (op), mode,\n@@ -968,7 +968,7 @@ simplify_unary_operation_1 (enum rtx_code code, enum machine_mode mode, rtx op)\n \treturn op;\n \n       /* If operand is known to be only -1 or 0, convert ABS to NEG.  */\n-      if (num_sign_bit_copies (op, mode) == GET_MODE_BITSIZE (mode))\n+      if (num_sign_bit_copies (op, mode) == GET_MODE_PRECISION (mode))\n \treturn gen_rtx_NEG (mode, op);\n \n       break;\n@@ -1261,8 +1261,8 @@ rtx\n simplify_const_unary_operation (enum rtx_code code, enum machine_mode mode,\n \t\t\t\trtx op, enum machine_mode op_mode)\n {\n-  unsigned int width = GET_MODE_BITSIZE (mode);\n-  unsigned int op_width = GET_MODE_BITSIZE (op_mode);\n+  unsigned int width = GET_MODE_PRECISION (mode);\n+  unsigned int op_width = GET_MODE_PRECISION (op_mode);\n \n   if (code == VEC_DUPLICATE)\n     {\n@@ -1362,7 +1362,7 @@ simplify_const_unary_operation (enum rtx_code code, enum machine_mode mode,\n \t  if (hv < 0)\n \t    return 0;\n \t}\n-      else if (GET_MODE_BITSIZE (op_mode) >= HOST_BITS_PER_WIDE_INT * 2)\n+      else if (GET_MODE_PRECISION (op_mode) >= HOST_BITS_PER_WIDE_INT * 2)\n \t;\n       else\n \thv = 0, lv &= GET_MODE_MASK (op_mode);\n@@ -1403,17 +1403,17 @@ simplify_const_unary_operation (enum rtx_code code, enum machine_mode mode,\n \t  if (arg0 == 0 && CLZ_DEFINED_VALUE_AT_ZERO (op_mode, val))\n \t    ;\n \t  else\n-\t    val = GET_MODE_BITSIZE (op_mode) - floor_log2 (arg0) - 1;\n+\t    val = GET_MODE_PRECISION (op_mode) - floor_log2 (arg0) - 1;\n \t  break;\n \n \tcase CLRSB:\n \t  arg0 &= GET_MODE_MASK (op_mode);\n \t  if (arg0 == 0)\n-\t    val = GET_MODE_BITSIZE (op_mode) - 1;\n+\t    val = GET_MODE_PRECISION (op_mode) - 1;\n \t  else if (arg0 >= 0)\n-\t    val = GET_MODE_BITSIZE (op_mode) - floor_log2 (arg0) - 2;\n+\t    val = GET_MODE_PRECISION (op_mode) - floor_log2 (arg0) - 2;\n \t  else if (arg0 < 0)\n-\t    val = GET_MODE_BITSIZE (op_mode) - floor_log2 (~arg0) - 2;\n+\t    val = GET_MODE_PRECISION (op_mode) - floor_log2 (~arg0) - 2;\n \t  break;\n \n \tcase CTZ:\n@@ -1423,7 +1423,7 @@ simplify_const_unary_operation (enum rtx_code code, enum machine_mode mode,\n \t      /* Even if the value at zero is undefined, we have to come\n \t\t up with some replacement.  Seems good enough.  */\n \t      if (! CTZ_DEFINED_VALUE_AT_ZERO (op_mode, val))\n-\t\tval = GET_MODE_BITSIZE (op_mode);\n+\t\tval = GET_MODE_PRECISION (op_mode);\n \t    }\n \t  else\n \t    val = ctz_hwi (arg0);\n@@ -1467,12 +1467,12 @@ simplify_const_unary_operation (enum rtx_code code, enum machine_mode mode,\n \t  /* When zero-extending a CONST_INT, we need to know its\n              original mode.  */\n \t  gcc_assert (op_mode != VOIDmode);\n-\t  if (GET_MODE_BITSIZE (op_mode) == HOST_BITS_PER_WIDE_INT)\n+\t  if (op_width == HOST_BITS_PER_WIDE_INT)\n \t    {\n \t      /* If we were really extending the mode,\n \t\t we would have to distinguish between zero-extension\n \t\t and sign-extension.  */\n-\t      gcc_assert (width == GET_MODE_BITSIZE (op_mode));\n+\t      gcc_assert (width == op_width);\n \t      val = arg0;\n \t    }\n \t  else if (GET_MODE_BITSIZE (op_mode) < HOST_BITS_PER_WIDE_INT)\n@@ -1484,15 +1484,16 @@ simplify_const_unary_operation (enum rtx_code code, enum machine_mode mode,\n \tcase SIGN_EXTEND:\n \t  if (op_mode == VOIDmode)\n \t    op_mode = mode;\n-\t  if (GET_MODE_BITSIZE (op_mode) == HOST_BITS_PER_WIDE_INT)\n+\t  op_width = GET_MODE_PRECISION (op_mode);\n+\t  if (op_width == HOST_BITS_PER_WIDE_INT)\n \t    {\n \t      /* If we were really extending the mode,\n \t\t we would have to distinguish between zero-extension\n \t\t and sign-extension.  */\n-\t      gcc_assert (width == GET_MODE_BITSIZE (op_mode));\n+\t      gcc_assert (width == op_width);\n \t      val = arg0;\n \t    }\n-\t  else if (GET_MODE_BITSIZE (op_mode) < HOST_BITS_PER_WIDE_INT)\n+\t  else if (op_width < HOST_BITS_PER_WIDE_INT)\n \t    {\n \t      val = arg0 & GET_MODE_MASK (op_mode);\n \t      if (val_signbit_known_set_p (op_mode, val))\n@@ -1565,12 +1566,12 @@ simplify_const_unary_operation (enum rtx_code code, enum machine_mode mode,\n \tcase CLZ:\n \t  hv = 0;\n \t  if (h1 != 0)\n-\t    lv = GET_MODE_BITSIZE (mode) - floor_log2 (h1) - 1\n+\t    lv = GET_MODE_PRECISION (mode) - floor_log2 (h1) - 1\n \t      - HOST_BITS_PER_WIDE_INT;\n \t  else if (l1 != 0)\n-\t    lv = GET_MODE_BITSIZE (mode) - floor_log2 (l1) - 1;\n+\t    lv = GET_MODE_PRECISION (mode) - floor_log2 (l1) - 1;\n \t  else if (! CLZ_DEFINED_VALUE_AT_ZERO (mode, lv))\n-\t    lv = GET_MODE_BITSIZE (mode);\n+\t    lv = GET_MODE_PRECISION (mode);\n \t  break;\n \n \tcase CTZ:\n@@ -1580,7 +1581,7 @@ simplify_const_unary_operation (enum rtx_code code, enum machine_mode mode,\n \t  else if (h1 != 0)\n \t    lv = HOST_BITS_PER_WIDE_INT + ctz_hwi (h1);\n \t  else if (! CTZ_DEFINED_VALUE_AT_ZERO (mode, lv))\n-\t    lv = GET_MODE_BITSIZE (mode);\n+\t    lv = GET_MODE_PRECISION (mode);\n \t  break;\n \n \tcase POPCOUNT:\n@@ -1634,7 +1635,7 @@ simplify_const_unary_operation (enum rtx_code code, enum machine_mode mode,\n \tcase ZERO_EXTEND:\n \t  gcc_assert (op_mode != VOIDmode);\n \n-\t  if (GET_MODE_BITSIZE (op_mode) > HOST_BITS_PER_WIDE_INT)\n+\t  if (op_width > HOST_BITS_PER_WIDE_INT)\n \t    return 0;\n \n \t  hv = 0;\n@@ -1643,7 +1644,7 @@ simplify_const_unary_operation (enum rtx_code code, enum machine_mode mode,\n \n \tcase SIGN_EXTEND:\n \t  if (op_mode == VOIDmode\n-\t      || GET_MODE_BITSIZE (op_mode) > HOST_BITS_PER_WIDE_INT)\n+\t      || op_width > HOST_BITS_PER_WIDE_INT)\n \t    return 0;\n \t  else\n \t    {\n@@ -1920,7 +1921,7 @@ simplify_binary_operation_1 (enum rtx_code code, enum machine_mode mode,\n {\n   rtx tem, reversed, opleft, opright;\n   HOST_WIDE_INT val;\n-  unsigned int width = GET_MODE_BITSIZE (mode);\n+  unsigned int width = GET_MODE_PRECISION (mode);\n \n   /* Even if we can't compute a constant result,\n      there are some cases worth simplifying.  */\n@@ -2505,7 +2506,7 @@ simplify_binary_operation_1 (enum rtx_code code, enum machine_mode mode,\n           && CONST_INT_P (XEXP (opleft, 1))\n           && CONST_INT_P (XEXP (opright, 1))\n           && (INTVAL (XEXP (opleft, 1)) + INTVAL (XEXP (opright, 1))\n-              == GET_MODE_BITSIZE (mode)))\n+              == GET_MODE_PRECISION (mode)))\n         return gen_rtx_ROTATE (mode, XEXP (opright, 0), XEXP (opleft, 1));\n \n       /* Same, but for ashift that has been \"simplified\" to a wider mode\n@@ -2524,7 +2525,7 @@ simplify_binary_operation_1 (enum rtx_code code, enum machine_mode mode,\n           && CONST_INT_P (XEXP (SUBREG_REG (opleft), 1))\n           && CONST_INT_P (XEXP (opright, 1))\n           && (INTVAL (XEXP (SUBREG_REG (opleft), 1)) + INTVAL (XEXP (opright, 1))\n-              == GET_MODE_BITSIZE (mode)))\n+              == GET_MODE_PRECISION (mode)))\n         return gen_rtx_ROTATE (mode, XEXP (opright, 0),\n                                XEXP (SUBREG_REG (opleft), 1));\n \n@@ -2702,7 +2703,7 @@ simplify_binary_operation_1 (enum rtx_code code, enum machine_mode mode,\n \t  && trueop1 == const1_rtx\n \t  && GET_CODE (op0) == LSHIFTRT\n \t  && CONST_INT_P (XEXP (op0, 1))\n-\t  && INTVAL (XEXP (op0, 1)) == GET_MODE_BITSIZE (mode) - 1)\n+\t  && INTVAL (XEXP (op0, 1)) == GET_MODE_PRECISION (mode) - 1)\n \treturn gen_rtx_GE (mode, XEXP (op0, 0), const0_rtx);\n \n       /* (xor (comparison foo bar) (const_int sign-bit))\n@@ -3061,7 +3062,7 @@ simplify_binary_operation_1 (enum rtx_code code, enum machine_mode mode,\n \t  unsigned HOST_WIDE_INT zero_val = 0;\n \n \t  if (CLZ_DEFINED_VALUE_AT_ZERO (imode, zero_val)\n-\t      && zero_val == GET_MODE_BITSIZE (imode)\n+\t      && zero_val == GET_MODE_PRECISION (imode)\n \t      && INTVAL (trueop1) == exact_log2 (zero_val))\n \t    return simplify_gen_relational (EQ, mode, imode,\n \t\t\t\t\t    XEXP (op0, 0), const0_rtx);\n@@ -3351,7 +3352,7 @@ simplify_const_binary_operation (enum rtx_code code, enum machine_mode mode,\n {\n   HOST_WIDE_INT arg0, arg1, arg0s, arg1s;\n   HOST_WIDE_INT val;\n-  unsigned int width = GET_MODE_BITSIZE (mode);\n+  unsigned int width = GET_MODE_PRECISION (mode);\n \n   if (VECTOR_MODE_P (mode)\n       && code != VEC_CONCAT\n@@ -3636,24 +3637,24 @@ simplify_const_binary_operation (enum rtx_code code, enum machine_mode mode,\n \t    unsigned HOST_WIDE_INT cnt;\n \n \t    if (SHIFT_COUNT_TRUNCATED)\n-\t      o1 = double_int_zext (o1, GET_MODE_BITSIZE (mode));\n+\t      o1 = double_int_zext (o1, GET_MODE_PRECISION (mode));\n \n \t    if (!double_int_fits_in_uhwi_p (o1)\n-\t        || double_int_to_uhwi (o1) >= GET_MODE_BITSIZE (mode))\n+\t        || double_int_to_uhwi (o1) >= GET_MODE_PRECISION (mode))\n \t      return 0;\n \n \t    cnt = double_int_to_uhwi (o1);\n \n \t    if (code == LSHIFTRT || code == ASHIFTRT)\n-\t      res = double_int_rshift (o0, cnt, GET_MODE_BITSIZE (mode),\n+\t      res = double_int_rshift (o0, cnt, GET_MODE_PRECISION (mode),\n \t\t\t\t       code == ASHIFTRT);\n \t    else if (code == ASHIFT)\n-\t      res = double_int_lshift (o0, cnt, GET_MODE_BITSIZE (mode),\n+\t      res = double_int_lshift (o0, cnt, GET_MODE_PRECISION (mode),\n \t\t\t\t       true);\n \t    else if (code == ROTATE)\n-\t      res = double_int_lrotate (o0, cnt, GET_MODE_BITSIZE (mode));\n+\t      res = double_int_lrotate (o0, cnt, GET_MODE_PRECISION (mode));\n \t    else /* code == ROTATERT */\n-\t      res = double_int_rrotate (o0, cnt, GET_MODE_BITSIZE (mode));\n+\t      res = double_int_rrotate (o0, cnt, GET_MODE_PRECISION (mode));\n \t  }\n \t  break;\n \n@@ -4626,7 +4627,7 @@ simplify_const_relational_operation (enum rtx_code code,\n        && (GET_CODE (trueop1) == CONST_DOUBLE\n \t   || CONST_INT_P (trueop1)))\n     {\n-      int width = GET_MODE_BITSIZE (mode);\n+      int width = GET_MODE_PRECISION (mode);\n       HOST_WIDE_INT l0s, h0s, l1s, h1s;\n       unsigned HOST_WIDE_INT l0u, h0u, l1u, h1u;\n \n@@ -4814,7 +4815,7 @@ simplify_const_relational_operation (enum rtx_code code,\n \t  rtx inner_const = avoid_constant_pool_reference (XEXP (op0, 1));\n \t  if (CONST_INT_P (inner_const) && inner_const != const0_rtx)\n \t    {\n-\t      int sign_bitnum = GET_MODE_BITSIZE (mode) - 1;\n+\t      int sign_bitnum = GET_MODE_PRECISION (mode) - 1;\n \t      int has_sign = (HOST_BITS_PER_WIDE_INT >= sign_bitnum\n \t\t\t      && (UINTVAL (inner_const)\n \t\t\t\t  & ((unsigned HOST_WIDE_INT) 1\n@@ -4906,7 +4907,7 @@ simplify_ternary_operation (enum rtx_code code, enum machine_mode mode,\n \t\t\t    enum machine_mode op0_mode, rtx op0, rtx op1,\n \t\t\t    rtx op2)\n {\n-  unsigned int width = GET_MODE_BITSIZE (mode);\n+  unsigned int width = GET_MODE_PRECISION (mode);\n   bool any_change = false;\n   rtx tem;\n \n@@ -4951,21 +4952,22 @@ simplify_ternary_operation (enum rtx_code code, enum machine_mode mode,\n \t{\n \t  /* Extracting a bit-field from a constant */\n \t  unsigned HOST_WIDE_INT val = UINTVAL (op0);\n-\n+\t  HOST_WIDE_INT op1val = INTVAL (op1);\n+\t  HOST_WIDE_INT op2val = INTVAL (op2);\n \t  if (BITS_BIG_ENDIAN)\n-\t    val >>= GET_MODE_BITSIZE (op0_mode) - INTVAL (op2) - INTVAL (op1);\n+\t    val >>= GET_MODE_PRECISION (op0_mode) - op2val - op1val;\n \t  else\n-\t    val >>= INTVAL (op2);\n+\t    val >>= op2val;\n \n-\t  if (HOST_BITS_PER_WIDE_INT != INTVAL (op1))\n+\t  if (HOST_BITS_PER_WIDE_INT != op1val)\n \t    {\n \t      /* First zero-extend.  */\n-\t      val &= ((unsigned HOST_WIDE_INT) 1 << INTVAL (op1)) - 1;\n+\t      val &= ((unsigned HOST_WIDE_INT) 1 << op1val) - 1;\n \t      /* If desired, propagate sign bit.  */\n \t      if (code == SIGN_EXTRACT\n-\t\t  && (val & ((unsigned HOST_WIDE_INT) 1 << (INTVAL (op1) - 1)))\n+\t\t  && (val & ((unsigned HOST_WIDE_INT) 1 << (op1val - 1)))\n \t\t     != 0)\n-\t\tval |= ~ (((unsigned HOST_WIDE_INT) 1 << INTVAL (op1)) - 1);\n+\t\tval |= ~ (((unsigned HOST_WIDE_INT) 1 << op1val) - 1);\n \t    }\n \n \t  return gen_int_mode (val, mode);\n@@ -5610,7 +5612,7 @@ simplify_subreg (enum machine_mode outermode, rtx op,\n   /* Optimize SUBREG truncations of zero and sign extended values.  */\n   if ((GET_CODE (op) == ZERO_EXTEND\n        || GET_CODE (op) == SIGN_EXTEND)\n-      && GET_MODE_BITSIZE (outermode) < GET_MODE_BITSIZE (innermode))\n+      && GET_MODE_PRECISION (outermode) < GET_MODE_PRECISION (innermode))\n     {\n       unsigned int bitpos = subreg_lsb_1 (outermode, innermode, byte);\n \n@@ -5626,7 +5628,7 @@ simplify_subreg (enum machine_mode outermode, rtx op,\n \t  enum machine_mode origmode = GET_MODE (XEXP (op, 0));\n \t  if (outermode == origmode)\n \t    return XEXP (op, 0);\n-\t  if (GET_MODE_BITSIZE (outermode) <= GET_MODE_BITSIZE (origmode))\n+\t  if (GET_MODE_PRECISION (outermode) <= GET_MODE_PRECISION (origmode))\n \t    return simplify_gen_subreg (outermode, XEXP (op, 0), origmode,\n \t\t\t\t\tsubreg_lowpart_offset (outermode,\n \t\t\t\t\t\t\t       origmode));\n@@ -5638,7 +5640,7 @@ simplify_subreg (enum machine_mode outermode, rtx op,\n       /* A SUBREG resulting from a zero extension may fold to zero if\n \t it extracts higher bits that the ZERO_EXTEND's source bits.  */\n       if (GET_CODE (op) == ZERO_EXTEND\n-\t  && bitpos >= GET_MODE_BITSIZE (GET_MODE (XEXP (op, 0))))\n+\t  && bitpos >= GET_MODE_PRECISION (GET_MODE (XEXP (op, 0))))\n \treturn CONST0_RTX (outermode);\n     }\n \n@@ -5652,11 +5654,11 @@ simplify_subreg (enum machine_mode outermode, rtx op,\n \t to avoid the possibility that an outer LSHIFTRT shifts by more\n \t than the sign extension's sign_bit_copies and introduces zeros\n \t into the high bits of the result.  */\n-      && (2 * GET_MODE_BITSIZE (outermode)) <= GET_MODE_BITSIZE (innermode)\n+      && (2 * GET_MODE_PRECISION (outermode)) <= GET_MODE_PRECISION (innermode)\n       && CONST_INT_P (XEXP (op, 1))\n       && GET_CODE (XEXP (op, 0)) == SIGN_EXTEND\n       && GET_MODE (XEXP (XEXP (op, 0), 0)) == outermode\n-      && INTVAL (XEXP (op, 1)) < GET_MODE_BITSIZE (outermode)\n+      && INTVAL (XEXP (op, 1)) < GET_MODE_PRECISION (outermode)\n       && subreg_lsb_1 (outermode, innermode, byte) == 0)\n     return simplify_gen_binary (ASHIFTRT, outermode,\n \t\t\t\tXEXP (XEXP (op, 0), 0), XEXP (op, 1));\n@@ -5667,11 +5669,11 @@ simplify_subreg (enum machine_mode outermode, rtx op,\n   if ((GET_CODE (op) == LSHIFTRT\n        || GET_CODE (op) == ASHIFTRT)\n       && SCALAR_INT_MODE_P (outermode)\n-      && GET_MODE_BITSIZE (outermode) < GET_MODE_BITSIZE (innermode)\n+      && GET_MODE_PRECISION (outermode) < GET_MODE_PRECISION (innermode)\n       && CONST_INT_P (XEXP (op, 1))\n       && GET_CODE (XEXP (op, 0)) == ZERO_EXTEND\n       && GET_MODE (XEXP (XEXP (op, 0), 0)) == outermode\n-      && INTVAL (XEXP (op, 1)) < GET_MODE_BITSIZE (outermode)\n+      && INTVAL (XEXP (op, 1)) < GET_MODE_PRECISION (outermode)\n       && subreg_lsb_1 (outermode, innermode, byte) == 0)\n     return simplify_gen_binary (LSHIFTRT, outermode,\n \t\t\t\tXEXP (XEXP (op, 0), 0), XEXP (op, 1));\n@@ -5681,12 +5683,12 @@ simplify_subreg (enum machine_mode outermode, rtx op,\n      the outer subreg is effectively a truncation to the original mode.  */\n   if (GET_CODE (op) == ASHIFT\n       && SCALAR_INT_MODE_P (outermode)\n-      && GET_MODE_BITSIZE (outermode) < GET_MODE_BITSIZE (innermode)\n+      && GET_MODE_PRECISION (outermode) < GET_MODE_PRECISION (innermode)\n       && CONST_INT_P (XEXP (op, 1))\n       && (GET_CODE (XEXP (op, 0)) == ZERO_EXTEND\n \t  || GET_CODE (XEXP (op, 0)) == SIGN_EXTEND)\n       && GET_MODE (XEXP (XEXP (op, 0), 0)) == outermode\n-      && INTVAL (XEXP (op, 1)) < GET_MODE_BITSIZE (outermode)\n+      && INTVAL (XEXP (op, 1)) < GET_MODE_PRECISION (outermode)\n       && subreg_lsb_1 (outermode, innermode, byte) == 0)\n     return simplify_gen_binary (ASHIFT, outermode,\n \t\t\t\tXEXP (XEXP (op, 0), 0), XEXP (op, 1));\n@@ -5695,12 +5697,12 @@ simplify_subreg (enum machine_mode outermode, rtx op,\n   if ((GET_CODE (op) == LSHIFTRT\n        || GET_CODE (op) == ASHIFTRT)\n       && SCALAR_INT_MODE_P (outermode)\n-      && GET_MODE_BITSIZE (outermode) >= BITS_PER_WORD\n-      && GET_MODE_BITSIZE (innermode) >= (2 * GET_MODE_BITSIZE (outermode))\n+      && GET_MODE_PRECISION (outermode) >= BITS_PER_WORD\n+      && GET_MODE_PRECISION (innermode) >= (2 * GET_MODE_PRECISION (outermode))\n       && CONST_INT_P (XEXP (op, 1))\n-      && (INTVAL (XEXP (op, 1)) & (GET_MODE_BITSIZE (outermode) - 1)) == 0\n+      && (INTVAL (XEXP (op, 1)) & (GET_MODE_PRECISION (outermode) - 1)) == 0\n       && INTVAL (XEXP (op, 1)) >= 0\n-      && INTVAL (XEXP (op, 1)) < GET_MODE_BITSIZE (innermode)\n+      && INTVAL (XEXP (op, 1)) < GET_MODE_PRECISION (innermode)\n       && byte == subreg_lowpart_offset (outermode, innermode))\n     {\n       int shifted_bytes = INTVAL (XEXP (op, 1)) / BITS_PER_UNIT;"}]}