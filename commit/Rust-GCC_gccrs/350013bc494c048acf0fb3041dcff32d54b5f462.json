{"sha": "350013bc494c048acf0fb3041dcff32d54b5f462", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MzUwMDEzYmM0OTRjMDQ4YWNmMGZiMzA0MWRjZmYzMmQ1NGI1ZjQ2Mg==", "commit": {"author": {"name": "Bin Cheng", "email": "bin.cheng@arm.com", "date": "2014-12-05T17:06:33Z"}, "committer": {"name": "Marcus Shawcroft", "email": "mshawcroft@gcc.gnu.org", "date": "2014-12-05T17:06:33Z"}, "message": "[AArch64]load store pair optimization using sched_fusion pass.\n\nFrom-SVN: r218430", "tree": {"sha": "372d74eecb7f17a2eb1e06241033b8f8ef5eb601", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/372d74eecb7f17a2eb1e06241033b8f8ef5eb601"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/350013bc494c048acf0fb3041dcff32d54b5f462", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/350013bc494c048acf0fb3041dcff32d54b5f462", "html_url": "https://github.com/Rust-GCC/gccrs/commit/350013bc494c048acf0fb3041dcff32d54b5f462", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/350013bc494c048acf0fb3041dcff32d54b5f462/comments", "author": null, "committer": null, "parents": [{"sha": "a66272f6d2d307e7c06b2c4e48bbbd7ec331d919", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a66272f6d2d307e7c06b2c4e48bbbd7ec331d919", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a66272f6d2d307e7c06b2c4e48bbbd7ec331d919"}], "stats": {"total": 1214, "additions": 1181, "deletions": 33}, "files": [{"sha": "88a29ebcd7c75329823345fa5090025918c5249f", "filename": "gcc/ChangeLog", "status": "modified", "additions": 30, "deletions": 0, "changes": 30, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=350013bc494c048acf0fb3041dcff32d54b5f462", "patch": "@@ -1,3 +1,33 @@\n+2014-12-05  Bin Cheng  <bin.cheng@arm.com>\n+\n+\t* config/aarch64/aarch64.md (load_pair<mode>): Split to\n+\tload_pairsi, load_pairdi, load_pairsf and load_pairdf.\n+\t(load_pairsi, load_pairdi, load_pairsf, load_pairdf): Split\n+\tfrom load_pair<mode>.  New alternative to support int/fp\n+\tregisters in fp/int mode patterns.\n+\t(store_pair<mode>:): Split to store_pairsi, store_pairdi,\n+\tstore_pairsf and store_pairdi.\n+\t(store_pairsi, store_pairdi, store_pairsf, store_pairdf): Split\n+\tfrom store_pair<mode>.  New alternative to support int/fp\n+\tregisters in fp/int mode patterns.\n+\t(*load_pair_extendsidi2_aarch64): New pattern.\n+\t(*load_pair_zero_extendsidi2_aarch64): New pattern.\n+\t(aarch64-ldpstp.md): Include.\n+\t* config/aarch64/aarch64-ldpstp.md: New file.\n+\t* config/aarch64/aarch64-protos.h (aarch64_gen_adjusted_ldpstp):\n+\tNew.\n+\t(extract_base_offset_in_addr): New.\n+\t(aarch64_operands_ok_for_ldpstp): New.\n+\t(aarch64_operands_adjust_ok_for_ldpstp): New.\n+\t* config/aarch64/aarch64.c (enum sched_fusion_type): New enum.\n+\t(TARGET_SCHED_FUSION_PRIORITY): New hook.\n+\t(fusion_load_store): New functon.\n+\t(extract_base_offset_in_addr): New function.\n+\t(aarch64_gen_adjusted_ldpstp): New function.\n+\t(aarch64_sched_fusion_priority): New function.\n+\t(aarch64_operands_ok_for_ldpstp): New function.\n+\t(aarch64_operands_adjust_ok_for_ldpstp): New function.\n+\n 2014-12-05  Olivier Hainque  <hainque@adacore.com>\n \n \t* defaults.h: (DWARF_REG_TO_UNWIND_COLUMN): Define default."}, {"sha": "dacfe1c4d01988f892c99210742eee944d675f0d", "filename": "gcc/config/aarch64/aarch64-ldpstp.md", "status": "added", "additions": 410, "deletions": 0, "changes": 410, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Fconfig%2Faarch64%2Faarch64-ldpstp.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Fconfig%2Faarch64%2Faarch64-ldpstp.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-ldpstp.md?ref=350013bc494c048acf0fb3041dcff32d54b5f462", "patch": "@@ -0,0 +1,410 @@\n+;; AArch64 ldp/stp peephole optimizations.\n+;; Copyright (C) 2014 Free Software Foundation, Inc.\n+;; Contributed by ARM Ltd.\n+;;\n+;; This file is part of GCC.\n+;;\n+;; GCC is free software; you can redistribute it and/or modify it\n+;; under the terms of the GNU General Public License as published by\n+;; the Free Software Foundation; either version 3, or (at your option)\n+;; any later version.\n+;;\n+;; GCC is distributed in the hope that it will be useful, but\n+;; WITHOUT ANY WARRANTY; without even the implied warranty of\n+;; MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n+;; General Public License for more details.\n+;;\n+;; You should have received a copy of the GNU General Public License\n+;; along with GCC; see the file COPYING3.  If not see\n+;; <http://www.gnu.org/licenses/>.\n+\n+(define_peephole2\n+  [(set (match_operand:GPI 0 \"register_operand\" \"\")\n+\t(match_operand:GPI 1 \"aarch64_mem_pair_operand\" \"\"))\n+   (set (match_operand:GPI 2 \"register_operand\" \"\")\n+\t(match_operand:GPI 3 \"memory_operand\" \"\"))]\n+  \"aarch64_operands_ok_for_ldpstp (operands, true, <MODE>mode)\"\n+  [(parallel [(set (match_dup 0) (match_dup 1))\n+\t      (set (match_dup 2) (match_dup 3))])]\n+{\n+  rtx base, offset_1, offset_2, tmp;\n+\n+  extract_base_offset_in_addr (operands[1], &base, &offset_1);\n+  extract_base_offset_in_addr (operands[3], &base, &offset_2);\n+  if (INTVAL (offset_1) > INTVAL (offset_2))\n+    {\n+      tmp = operands[0];\n+      operands[0] = operands[2];\n+      operands[2] = tmp;\n+      tmp = operands[1];\n+      operands[1] = operands[3];\n+      operands[3] = tmp;\n+    }\n+})\n+\n+(define_peephole2\n+  [(set (match_operand:GPI 0 \"aarch64_mem_pair_operand\" \"\")\n+\t(match_operand:GPI 1 \"aarch64_reg_or_zero\" \"\"))\n+   (set (match_operand:GPI 2 \"memory_operand\" \"\")\n+\t(match_operand:GPI 3 \"aarch64_reg_or_zero\" \"\"))]\n+  \"aarch64_operands_ok_for_ldpstp (operands, false, <MODE>mode)\"\n+  [(parallel [(set (match_dup 0) (match_dup 1))\n+\t      (set (match_dup 2) (match_dup 3))])]\n+{\n+  rtx base, offset_1, offset_2, tmp;\n+\n+  extract_base_offset_in_addr (operands[0], &base, &offset_1);\n+  extract_base_offset_in_addr (operands[2], &base, &offset_2);\n+  if (INTVAL (offset_1) > INTVAL (offset_2))\n+    {\n+      tmp = operands[0];\n+      operands[0] = operands[2];\n+      operands[2] = tmp;\n+      tmp = operands[1];\n+      operands[1] = operands[3];\n+      operands[3] = tmp;\n+    }\n+})\n+\n+(define_peephole2\n+  [(set (match_operand:GPF 0 \"register_operand\" \"\")\n+\t(match_operand:GPF 1 \"aarch64_mem_pair_operand\" \"\"))\n+   (set (match_operand:GPF 2 \"register_operand\" \"\")\n+\t(match_operand:GPF 3 \"memory_operand\" \"\"))]\n+  \"aarch64_operands_ok_for_ldpstp (operands, true, <MODE>mode)\"\n+  [(parallel [(set (match_dup 0) (match_dup 1))\n+\t      (set (match_dup 2) (match_dup 3))])]\n+{\n+  rtx base, offset_1, offset_2, tmp;\n+\n+  extract_base_offset_in_addr (operands[1], &base, &offset_1);\n+  extract_base_offset_in_addr (operands[3], &base, &offset_2);\n+  if (INTVAL (offset_1) > INTVAL (offset_2))\n+    {\n+      tmp = operands[0];\n+      operands[0] = operands[2];\n+      operands[2] = tmp;\n+      tmp = operands[1];\n+      operands[1] = operands[3];\n+      operands[3] = tmp;\n+    }\n+})\n+\n+(define_peephole2\n+  [(set (match_operand:GPF 0 \"aarch64_mem_pair_operand\" \"\")\n+\t(match_operand:GPF 1 \"register_operand\" \"\"))\n+   (set (match_operand:GPF 2 \"memory_operand\" \"\")\n+\t(match_operand:GPF 3 \"register_operand\" \"\"))]\n+  \"aarch64_operands_ok_for_ldpstp (operands, false, <MODE>mode)\"\n+  [(parallel [(set (match_dup 0) (match_dup 1))\n+\t      (set (match_dup 2) (match_dup 3))])]\n+{\n+  rtx base, offset_1, offset_2, tmp;\n+\n+  extract_base_offset_in_addr (operands[0], &base, &offset_1);\n+  extract_base_offset_in_addr (operands[2], &base, &offset_2);\n+  if (INTVAL (offset_1) > INTVAL (offset_2))\n+    {\n+      tmp = operands[0];\n+      operands[0] = operands[2];\n+      operands[2] = tmp;\n+      tmp = operands[1];\n+      operands[1] = operands[3];\n+      operands[3] = tmp;\n+    }\n+})\n+\n+;; Handle sign/zero extended consecutive load/store.\n+\n+(define_peephole2\n+  [(set (match_operand:DI 0 \"register_operand\" \"\")\n+\t(sign_extend:DI (match_operand:SI 1 \"aarch64_mem_pair_operand\" \"\")))\n+   (set (match_operand:DI 2 \"register_operand\" \"\")\n+\t(sign_extend:DI (match_operand:SI 3 \"memory_operand\" \"\")))]\n+  \"aarch64_operands_ok_for_ldpstp (operands, true, SImode)\"\n+  [(parallel [(set (match_dup 0) (sign_extend:DI (match_dup 1)))\n+\t      (set (match_dup 2) (sign_extend:DI (match_dup 3)))])]\n+{\n+  rtx base, offset_1, offset_2, tmp;\n+\n+  extract_base_offset_in_addr (operands[1], &base, &offset_1);\n+  extract_base_offset_in_addr (operands[3], &base, &offset_2);\n+  if (INTVAL (offset_1) > INTVAL (offset_2))\n+    {\n+      tmp = operands[0];\n+      operands[0] = operands[2];\n+      operands[2] = tmp;\n+      tmp = operands[1];\n+      operands[1] = operands[3];\n+      operands[3] = tmp;\n+    }\n+})\n+\n+(define_peephole2\n+  [(set (match_operand:DI 0 \"register_operand\" \"\")\n+\t(zero_extend:DI (match_operand:SI 1 \"aarch64_mem_pair_operand\" \"\")))\n+   (set (match_operand:DI 2 \"register_operand\" \"\")\n+\t(zero_extend:DI (match_operand:SI 3 \"memory_operand\" \"\")))]\n+  \"aarch64_operands_ok_for_ldpstp (operands, true, SImode)\"\n+  [(parallel [(set (match_dup 0) (zero_extend:DI (match_dup 1)))\n+\t      (set (match_dup 2) (zero_extend:DI (match_dup 3)))])]\n+{\n+  rtx base, offset_1, offset_2, tmp;\n+\n+  extract_base_offset_in_addr (operands[1], &base, &offset_1);\n+  extract_base_offset_in_addr (operands[3], &base, &offset_2);\n+  if (INTVAL (offset_1) > INTVAL (offset_2))\n+    {\n+      tmp = operands[0];\n+      operands[0] = operands[2];\n+      operands[2] = tmp;\n+      tmp = operands[1];\n+      operands[1] = operands[3];\n+      operands[3] = tmp;\n+    }\n+})\n+\n+;; Handle consecutive load/store whose offset is out of the range\n+;; supported by ldp/ldpsw/stp.  We firstly adjust offset in a scratch\n+;; register, then merge them into ldp/ldpsw/stp by using the adjusted\n+;; offset.\n+\n+(define_peephole2\n+  [(match_scratch:DI 8 \"r\")\n+   (set (match_operand:GPI 0 \"register_operand\" \"\")\n+\t(match_operand:GPI 1 \"memory_operand\" \"\"))\n+   (set (match_operand:GPI 2 \"register_operand\" \"\")\n+\t(match_operand:GPI 3 \"memory_operand\" \"\"))\n+   (set (match_operand:GPI 4 \"register_operand\" \"\")\n+\t(match_operand:GPI 5 \"memory_operand\" \"\"))\n+   (set (match_operand:GPI 6 \"register_operand\" \"\")\n+\t(match_operand:GPI 7 \"memory_operand\" \"\"))\n+   (match_dup 8)]\n+  \"aarch64_operands_adjust_ok_for_ldpstp (operands, true, <MODE>mode)\"\n+  [(const_int 0)]\n+{\n+  rtx base, offset_1, offset_2, tmp;\n+\n+  extract_base_offset_in_addr (operands[1], &base, &offset_1);\n+  extract_base_offset_in_addr (operands[3], &base, &offset_2);\n+  if (INTVAL (offset_1) > INTVAL (offset_2))\n+    {\n+      tmp = operands[0];\n+      operands[0] = operands[6];\n+      operands[6] = tmp;\n+      tmp = operands[1];\n+      operands[1] = operands[7];\n+      operands[7] = tmp;\n+      tmp = operands[2];\n+      operands[2] = operands[4];\n+      operands[4] = tmp;\n+      tmp = operands[3];\n+      operands[3] = operands[5];\n+      operands[5] = tmp;\n+    }\n+\n+  if (aarch64_gen_adjusted_ldpstp (operands, true, <MODE>mode, UNKNOWN))\n+    DONE;\n+  else\n+    FAIL;\n+})\n+\n+(define_peephole2\n+  [(match_scratch:DI 8 \"r\")\n+   (set (match_operand:GPF 0 \"register_operand\" \"\")\n+\t(match_operand:GPF 1 \"memory_operand\" \"\"))\n+   (set (match_operand:GPF 2 \"register_operand\" \"\")\n+\t(match_operand:GPF 3 \"memory_operand\" \"\"))\n+   (set (match_operand:GPF 4 \"register_operand\" \"\")\n+\t(match_operand:GPF 5 \"memory_operand\" \"\"))\n+   (set (match_operand:GPF 6 \"register_operand\" \"\")\n+\t(match_operand:GPF 7 \"memory_operand\" \"\"))\n+   (match_dup 8)]\n+  \"aarch64_operands_adjust_ok_for_ldpstp (operands, true, <MODE>mode)\"\n+  [(const_int 0)]\n+{\n+  rtx base, offset_1, offset_2, tmp;\n+\n+  extract_base_offset_in_addr (operands[1], &base, &offset_1);\n+  extract_base_offset_in_addr (operands[3], &base, &offset_2);\n+  if (INTVAL (offset_1) > INTVAL (offset_2))\n+    {\n+      tmp = operands[0];\n+      operands[0] = operands[6];\n+      operands[6] = tmp;\n+      tmp = operands[1];\n+      operands[1] = operands[7];\n+      operands[7] = tmp;\n+      tmp = operands[2];\n+      operands[2] = operands[4];\n+      operands[4] = tmp;\n+      tmp = operands[3];\n+      operands[3] = operands[5];\n+      operands[5] = tmp;\n+    }\n+\n+  if (aarch64_gen_adjusted_ldpstp (operands, true, <MODE>mode, UNKNOWN))\n+    DONE;\n+  else\n+    FAIL;\n+})\n+\n+(define_peephole2\n+  [(match_scratch:DI 8 \"r\")\n+   (set (match_operand:DI 0 \"register_operand\" \"\")\n+\t(sign_extend:DI (match_operand:SI 1 \"memory_operand\" \"\")))\n+   (set (match_operand:DI 2 \"register_operand\" \"\")\n+\t(sign_extend:DI (match_operand:SI 3 \"memory_operand\" \"\")))\n+   (set (match_operand:DI 4 \"register_operand\" \"\")\n+\t(sign_extend:DI (match_operand:SI 5 \"memory_operand\" \"\")))\n+   (set (match_operand:DI 6 \"register_operand\" \"\")\n+\t(sign_extend:DI (match_operand:SI 7 \"memory_operand\" \"\")))\n+   (match_dup 8)]\n+  \"aarch64_operands_adjust_ok_for_ldpstp (operands, true, SImode)\"\n+  [(const_int 0)]\n+{\n+  rtx base, offset_1, offset_2, tmp;\n+\n+  extract_base_offset_in_addr (operands[1], &base, &offset_1);\n+  extract_base_offset_in_addr (operands[3], &base, &offset_2);\n+  if (INTVAL (offset_1) > INTVAL (offset_2))\n+    {\n+      tmp = operands[0];\n+      operands[0] = operands[6];\n+      operands[6] = tmp;\n+      tmp = operands[1];\n+      operands[1] = operands[7];\n+      operands[7] = tmp;\n+      tmp = operands[2];\n+      operands[2] = operands[4];\n+      operands[4] = tmp;\n+      tmp = operands[3];\n+      operands[3] = operands[5];\n+      operands[5] = tmp;\n+    }\n+\n+  if (aarch64_gen_adjusted_ldpstp (operands, true, SImode, SIGN_EXTEND))\n+    DONE;\n+  else\n+    FAIL;\n+})\n+\n+(define_peephole2\n+  [(match_scratch:DI 8 \"r\")\n+   (set (match_operand:DI 0 \"register_operand\" \"\")\n+\t(zero_extend:DI (match_operand:SI 1 \"memory_operand\" \"\")))\n+   (set (match_operand:DI 2 \"register_operand\" \"\")\n+\t(zero_extend:DI (match_operand:SI 3 \"memory_operand\" \"\")))\n+   (set (match_operand:DI 4 \"register_operand\" \"\")\n+\t(zero_extend:DI (match_operand:SI 5 \"memory_operand\" \"\")))\n+   (set (match_operand:DI 6 \"register_operand\" \"\")\n+\t(zero_extend:DI (match_operand:SI 7 \"memory_operand\" \"\")))\n+   (match_dup 8)]\n+  \"aarch64_operands_adjust_ok_for_ldpstp (operands, true, SImode)\"\n+  [(const_int 0)]\n+{\n+  rtx base, offset_1, offset_2, tmp;\n+\n+  extract_base_offset_in_addr (operands[1], &base, &offset_1);\n+  extract_base_offset_in_addr (operands[3], &base, &offset_2);\n+  if (INTVAL (offset_1) > INTVAL (offset_2))\n+    {\n+      tmp = operands[0];\n+      operands[0] = operands[6];\n+      operands[6] = tmp;\n+      tmp = operands[1];\n+      operands[1] = operands[7];\n+      operands[7] = tmp;\n+      tmp = operands[2];\n+      operands[2] = operands[4];\n+      operands[4] = tmp;\n+      tmp = operands[3];\n+      operands[3] = operands[5];\n+      operands[5] = tmp;\n+    }\n+\n+  if (aarch64_gen_adjusted_ldpstp (operands, true, SImode, ZERO_EXTEND))\n+    DONE;\n+  else\n+    FAIL;\n+})\n+\n+(define_peephole2\n+  [(match_scratch:DI 8 \"r\")\n+   (set (match_operand:GPI 0 \"memory_operand\" \"\")\n+\t(match_operand:GPI 1 \"aarch64_reg_or_zero\" \"\"))\n+   (set (match_operand:GPI 2 \"memory_operand\" \"\")\n+\t(match_operand:GPI 3 \"aarch64_reg_or_zero\" \"\"))\n+   (set (match_operand:GPI 4 \"memory_operand\" \"\")\n+\t(match_operand:GPI 5 \"aarch64_reg_or_zero\" \"\"))\n+   (set (match_operand:GPI 6 \"memory_operand\" \"\")\n+\t(match_operand:GPI 7 \"aarch64_reg_or_zero\" \"\"))\n+   (match_dup 8)]\n+  \"aarch64_operands_adjust_ok_for_ldpstp (operands, false, <MODE>mode)\"\n+  [(const_int 0)]\n+{\n+  rtx base, offset_1, offset_2, tmp;\n+\n+  extract_base_offset_in_addr (operands[0], &base, &offset_1);\n+  extract_base_offset_in_addr (operands[2], &base, &offset_2);\n+  if (INTVAL (offset_1) > INTVAL (offset_2))\n+    {\n+      tmp = operands[0];\n+      operands[0] = operands[6];\n+      operands[6] = tmp;\n+      tmp = operands[1];\n+      operands[1] = operands[7];\n+      operands[7] = tmp;\n+      tmp = operands[2];\n+      operands[2] = operands[4];\n+      operands[4] = tmp;\n+      tmp = operands[3];\n+      operands[3] = operands[5];\n+      operands[5] = tmp;\n+    }\n+\n+  if (aarch64_gen_adjusted_ldpstp (operands, false, <MODE>mode, UNKNOWN))\n+    DONE;\n+  else\n+    FAIL;\n+})\n+\n+(define_peephole2\n+  [(match_scratch:DI 8 \"r\")\n+   (set (match_operand:GPF 0 \"memory_operand\" \"\")\n+\t(match_operand:GPF 1 \"aarch64_reg_or_zero\" \"\"))\n+   (set (match_operand:GPF 2 \"memory_operand\" \"\")\n+\t(match_operand:GPF 3 \"aarch64_reg_or_zero\" \"\"))\n+   (set (match_operand:GPF 4 \"memory_operand\" \"\")\n+\t(match_operand:GPF 5 \"aarch64_reg_or_zero\" \"\"))\n+   (set (match_operand:GPF 6 \"memory_operand\" \"\")\n+\t(match_operand:GPF 7 \"aarch64_reg_or_zero\" \"\"))\n+   (match_dup 8)]\n+  \"aarch64_operands_adjust_ok_for_ldpstp (operands, false, <MODE>mode)\"\n+  [(const_int 0)]\n+{\n+  rtx base, offset_1, offset_2, tmp;\n+\n+  extract_base_offset_in_addr (operands[0], &base, &offset_1);\n+  extract_base_offset_in_addr (operands[2], &base, &offset_2);\n+  if (INTVAL (offset_1) > INTVAL (offset_2))\n+    {\n+      tmp = operands[0];\n+      operands[0] = operands[6];\n+      operands[6] = tmp;\n+      tmp = operands[1];\n+      operands[1] = operands[7];\n+      operands[7] = tmp;\n+      tmp = operands[2];\n+      operands[2] = operands[4];\n+      operands[4] = tmp;\n+      tmp = operands[3];\n+      operands[3] = operands[5];\n+      operands[5] = tmp;\n+    }\n+\n+  if (aarch64_gen_adjusted_ldpstp (operands, false, <MODE>mode, UNKNOWN))\n+    DONE;\n+  else\n+    FAIL;\n+})"}, {"sha": "ec4157a38fe084372fe8a6bd8e08ac43bbb01278", "filename": "gcc/config/aarch64/aarch64-protos.h", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-protos.h?ref=350013bc494c048acf0fb3041dcff32d54b5f462", "patch": "@@ -293,6 +293,7 @@ void aarch64_expand_compare_and_swap (rtx op[]);\n void aarch64_split_compare_and_swap (rtx op[]);\n void aarch64_split_atomic_op (enum rtx_code, rtx, rtx, rtx, rtx, rtx, rtx);\n \n+bool aarch64_gen_adjusted_ldpstp (rtx *, bool, enum machine_mode, RTX_CODE);\n #endif /* RTX_CODE */\n \n void aarch64_init_builtins (void);\n@@ -316,4 +317,8 @@ extern bool\n aarch64_expand_vec_perm_const (rtx target, rtx op0, rtx op1, rtx sel);\n void aarch64_atomic_assign_expand_fenv (tree *, tree *, tree *);\n int aarch64_ccmp_mode_to_code (enum machine_mode mode);\n+\n+bool extract_base_offset_in_addr (rtx mem, rtx *base, rtx *offset);\n+bool aarch64_operands_ok_for_ldpstp (rtx *, bool, enum machine_mode);\n+bool aarch64_operands_adjust_ok_for_ldpstp (rtx *, bool, enum machine_mode);\n #endif /* GCC_AARCH64_PROTOS_H */"}, {"sha": "79a8679e7485575e70a6c742e41471718bfce8dd", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 481, "deletions": 0, "changes": 481, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=350013bc494c048acf0fb3041dcff32d54b5f462", "patch": "@@ -10382,6 +10382,484 @@ aarch_macro_fusion_pair_p (rtx_insn *prev, rtx_insn *curr)\n   return false;\n }\n \n+/* If MEM is in the form of [base+offset], extract the two parts\n+   of address and set to BASE and OFFSET, otherwise return false\n+   after clearing BASE and OFFSET.  */\n+\n+bool\n+extract_base_offset_in_addr (rtx mem, rtx *base, rtx *offset)\n+{\n+  rtx addr;\n+\n+  gcc_assert (MEM_P (mem));\n+\n+  addr = XEXP (mem, 0);\n+\n+  if (REG_P (addr))\n+    {\n+      *base = addr;\n+      *offset = const0_rtx;\n+      return true;\n+    }\n+\n+  if (GET_CODE (addr) == PLUS\n+      && REG_P (XEXP (addr, 0)) && CONST_INT_P (XEXP (addr, 1)))\n+    {\n+      *base = XEXP (addr, 0);\n+      *offset = XEXP (addr, 1);\n+      return true;\n+    }\n+\n+  *base = NULL_RTX;\n+  *offset = NULL_RTX;\n+\n+  return false;\n+}\n+\n+/* Types for scheduling fusion.  */\n+enum sched_fusion_type\n+{\n+  SCHED_FUSION_NONE = 0,\n+  SCHED_FUSION_LD_SIGN_EXTEND,\n+  SCHED_FUSION_LD_ZERO_EXTEND,\n+  SCHED_FUSION_LD,\n+  SCHED_FUSION_ST,\n+  SCHED_FUSION_NUM\n+};\n+\n+/* If INSN is a load or store of address in the form of [base+offset],\n+   extract the two parts and set to BASE and OFFSET.  Return scheduling\n+   fusion type this INSN is.  */\n+\n+static enum sched_fusion_type\n+fusion_load_store (rtx_insn *insn, rtx *base, rtx *offset)\n+{\n+  rtx x, dest, src;\n+  enum sched_fusion_type fusion = SCHED_FUSION_LD;\n+\n+  gcc_assert (INSN_P (insn));\n+  x = PATTERN (insn);\n+  if (GET_CODE (x) != SET)\n+    return SCHED_FUSION_NONE;\n+\n+  src = SET_SRC (x);\n+  dest = SET_DEST (x);\n+\n+  if (GET_MODE (src) != SImode && GET_MODE (src) != DImode\n+      && GET_MODE (src) != SFmode && GET_MODE (src) != DFmode)\n+    return SCHED_FUSION_NONE;\n+\n+  if (GET_CODE (src) == SIGN_EXTEND)\n+    {\n+      fusion = SCHED_FUSION_LD_SIGN_EXTEND;\n+      src = XEXP (src, 0);\n+      if (GET_CODE (src) != MEM || GET_MODE (src) != SImode)\n+\treturn SCHED_FUSION_NONE;\n+    }\n+  else if (GET_CODE (src) == ZERO_EXTEND)\n+    {\n+      fusion = SCHED_FUSION_LD_ZERO_EXTEND;\n+      src = XEXP (src, 0);\n+      if (GET_CODE (src) != MEM || GET_MODE (src) != SImode)\n+\treturn SCHED_FUSION_NONE;\n+    }\n+\n+  if (GET_CODE (src) == MEM && REG_P (dest))\n+    extract_base_offset_in_addr (src, base, offset);\n+  else if (GET_CODE (dest) == MEM && (REG_P (src) || src == const0_rtx))\n+    {\n+      fusion = SCHED_FUSION_ST;\n+      extract_base_offset_in_addr (dest, base, offset);\n+    }\n+  else\n+    return SCHED_FUSION_NONE;\n+\n+  if (*base == NULL_RTX || *offset == NULL_RTX)\n+    fusion = SCHED_FUSION_NONE;\n+\n+  return fusion;\n+}\n+\n+/* Implement the TARGET_SCHED_FUSION_PRIORITY hook.\n+\n+   Currently we only support to fuse ldr or str instructions, so FUSION_PRI\n+   and PRI are only calculated for these instructions.  For other instruction,\n+   FUSION_PRI and PRI are simply set to MAX_PRI - 1.  In the future, other\n+   type instruction fusion can be added by returning different priorities.\n+\n+   It's important that irrelevant instructions get the largest FUSION_PRI.  */\n+\n+static void\n+aarch64_sched_fusion_priority (rtx_insn *insn, int max_pri,\n+\t\t\t       int *fusion_pri, int *pri)\n+{\n+  int tmp, off_val;\n+  rtx base, offset;\n+  enum sched_fusion_type fusion;\n+\n+  gcc_assert (INSN_P (insn));\n+\n+  tmp = max_pri - 1;\n+  fusion = fusion_load_store (insn, &base, &offset);\n+  if (fusion == SCHED_FUSION_NONE)\n+    {\n+      *pri = tmp;\n+      *fusion_pri = tmp;\n+      return;\n+    }\n+\n+  /* Set FUSION_PRI according to fusion type and base register.  */\n+  *fusion_pri = tmp - fusion * FIRST_PSEUDO_REGISTER - REGNO (base);\n+\n+  /* Calculate PRI.  */\n+  tmp /= 2;\n+\n+  /* INSN with smaller offset goes first.  */\n+  off_val = (int)(INTVAL (offset));\n+  if (off_val >= 0)\n+    tmp -= (off_val & 0xfffff);\n+  else\n+    tmp += ((- off_val) & 0xfffff);\n+\n+  *pri = tmp;\n+  return;\n+}\n+\n+/* Given OPERANDS of consecutive load/store, check if we can merge\n+   them into ldp/stp.  LOAD is true if they are load instructions.\n+   MODE is the mode of memory operands.  */\n+\n+bool\n+aarch64_operands_ok_for_ldpstp (rtx *operands, bool load,\n+\t\t\t\tenum machine_mode mode)\n+{\n+  HOST_WIDE_INT offval_1, offval_2, msize;\n+  enum reg_class rclass_1, rclass_2;\n+  rtx mem_1, mem_2, reg_1, reg_2, base_1, base_2, offset_1, offset_2;\n+\n+  if (load)\n+    {\n+      mem_1 = operands[1];\n+      mem_2 = operands[3];\n+      reg_1 = operands[0];\n+      reg_2 = operands[2];\n+      gcc_assert (REG_P (reg_1) && REG_P (reg_2));\n+      if (REGNO (reg_1) == REGNO (reg_2))\n+\treturn false;\n+    }\n+  else\n+    {\n+      mem_1 = operands[0];\n+      mem_2 = operands[2];\n+      reg_1 = operands[1];\n+      reg_2 = operands[3];\n+    }\n+\n+  /* Check if the addresses are in the form of [base+offset].  */\n+  extract_base_offset_in_addr (mem_1, &base_1, &offset_1);\n+  if (base_1 == NULL_RTX || offset_1 == NULL_RTX)\n+    return false;\n+  extract_base_offset_in_addr (mem_2, &base_2, &offset_2);\n+  if (base_2 == NULL_RTX || offset_2 == NULL_RTX)\n+    return false;\n+\n+  /* Check if the bases are same.  */\n+  if (!rtx_equal_p (base_1, base_2))\n+    return false;\n+\n+  offval_1 = INTVAL (offset_1);\n+  offval_2 = INTVAL (offset_2);\n+  msize = GET_MODE_SIZE (mode);\n+  /* Check if the offsets are consecutive.  */\n+  if (offval_1 != (offval_2 + msize) && offval_2 != (offval_1 + msize))\n+    return false;\n+\n+  /* Check if the addresses are clobbered by load.  */\n+  if (load)\n+    {\n+      if (reg_mentioned_p (reg_1, mem_1))\n+\treturn false;\n+\n+      /* In increasing order, the last load can clobber the address.  */\n+      if (offval_1 > offval_2 && reg_mentioned_p (reg_2, mem_2))\n+      return false;\n+    }\n+\n+  if (REG_P (reg_1) && FP_REGNUM_P (REGNO (reg_1)))\n+    rclass_1 = FP_REGS;\n+  else\n+    rclass_1 = GENERAL_REGS;\n+\n+  if (REG_P (reg_2) && FP_REGNUM_P (REGNO (reg_2)))\n+    rclass_2 = FP_REGS;\n+  else\n+    rclass_2 = GENERAL_REGS;\n+\n+  /* Check if the registers are of same class.  */\n+  if (rclass_1 != rclass_2)\n+    return false;\n+\n+  return true;\n+}\n+\n+/* Given OPERANDS of consecutive load/store, check if we can merge\n+   them into ldp/stp by adjusting the offset.  LOAD is true if they\n+   are load instructions.  MODE is the mode of memory operands.\n+\n+   Given below consecutive stores:\n+\n+     str  w1, [xb, 0x100]\n+     str  w1, [xb, 0x104]\n+     str  w1, [xb, 0x108]\n+     str  w1, [xb, 0x10c]\n+\n+   Though the offsets are out of the range supported by stp, we can\n+   still pair them after adjusting the offset, like:\n+\n+     add  scratch, xb, 0x100\n+     stp  w1, w1, [scratch]\n+     stp  w1, w1, [scratch, 0x8]\n+\n+   The peephole patterns detecting this opportunity should guarantee\n+   the scratch register is avaliable.  */\n+\n+bool\n+aarch64_operands_adjust_ok_for_ldpstp (rtx *operands, bool load,\n+\t\t\t\t       enum machine_mode mode)\n+{\n+  enum reg_class rclass_1, rclass_2, rclass_3, rclass_4;\n+  HOST_WIDE_INT offval_1, offval_2, offval_3, offval_4, msize;\n+  rtx mem_1, mem_2, mem_3, mem_4, reg_1, reg_2, reg_3, reg_4;\n+  rtx base_1, base_2, base_3, base_4, offset_1, offset_2, offset_3, offset_4;\n+\n+  if (load)\n+    {\n+      reg_1 = operands[0];\n+      mem_1 = operands[1];\n+      reg_2 = operands[2];\n+      mem_2 = operands[3];\n+      reg_3 = operands[4];\n+      mem_3 = operands[5];\n+      reg_4 = operands[6];\n+      mem_4 = operands[7];\n+      gcc_assert (REG_P (reg_1) && REG_P (reg_2)\n+\t\t  && REG_P (reg_3) && REG_P (reg_4));\n+      if (REGNO (reg_1) == REGNO (reg_2) || REGNO (reg_3) == REGNO (reg_4))\n+\treturn false;\n+    }\n+  else\n+    {\n+      mem_1 = operands[0];\n+      reg_1 = operands[1];\n+      mem_2 = operands[2];\n+      reg_2 = operands[3];\n+      mem_3 = operands[4];\n+      reg_3 = operands[5];\n+      mem_4 = operands[6];\n+      reg_4 = operands[7];\n+    }\n+  /* Skip if memory operand is by itslef valid for ldp/stp.  */\n+  if (!MEM_P (mem_1) || aarch64_mem_pair_operand (mem_1, mode))\n+    return false;\n+\n+  /* Check if the addresses are in the form of [base+offset].  */\n+  extract_base_offset_in_addr (mem_1, &base_1, &offset_1);\n+  if (base_1 == NULL_RTX || offset_1 == NULL_RTX)\n+    return false;\n+  extract_base_offset_in_addr (mem_2, &base_2, &offset_2);\n+  if (base_2 == NULL_RTX || offset_2 == NULL_RTX)\n+    return false;\n+  extract_base_offset_in_addr (mem_3, &base_3, &offset_3);\n+  if (base_3 == NULL_RTX || offset_3 == NULL_RTX)\n+    return false;\n+  extract_base_offset_in_addr (mem_4, &base_4, &offset_4);\n+  if (base_4 == NULL_RTX || offset_4 == NULL_RTX)\n+    return false;\n+\n+  /* Check if the bases are same.  */\n+  if (!rtx_equal_p (base_1, base_2)\n+      || !rtx_equal_p (base_2, base_3)\n+      || !rtx_equal_p (base_3, base_4))\n+    return false;\n+\n+  offval_1 = INTVAL (offset_1);\n+  offval_2 = INTVAL (offset_2);\n+  offval_3 = INTVAL (offset_3);\n+  offval_4 = INTVAL (offset_4);\n+  msize = GET_MODE_SIZE (mode);\n+  /* Check if the offsets are consecutive.  */\n+  if ((offval_1 != (offval_2 + msize)\n+       || offval_1 != (offval_3 + msize * 2)\n+       || offval_1 != (offval_4 + msize * 3))\n+      && (offval_4 != (offval_3 + msize)\n+\t  || offval_4 != (offval_2 + msize * 2)\n+\t  || offval_4 != (offval_1 + msize * 3)))\n+    return false;\n+\n+  /* Check if the addresses are clobbered by load.  */\n+  if (load)\n+    {\n+      if (reg_mentioned_p (reg_1, mem_1)\n+\t  || reg_mentioned_p (reg_2, mem_2)\n+\t  || reg_mentioned_p (reg_3, mem_3))\n+\treturn false;\n+\n+      /* In increasing order, the last load can clobber the address.  */\n+      if (offval_1 > offval_2 && reg_mentioned_p (reg_4, mem_4))\n+\treturn false;\n+    }\n+\n+  if (REG_P (reg_1) && FP_REGNUM_P (REGNO (reg_1)))\n+    rclass_1 = FP_REGS;\n+  else\n+    rclass_1 = GENERAL_REGS;\n+\n+  if (REG_P (reg_2) && FP_REGNUM_P (REGNO (reg_2)))\n+    rclass_2 = FP_REGS;\n+  else\n+    rclass_2 = GENERAL_REGS;\n+\n+  if (REG_P (reg_3) && FP_REGNUM_P (REGNO (reg_3)))\n+    rclass_3 = FP_REGS;\n+  else\n+    rclass_3 = GENERAL_REGS;\n+\n+  if (REG_P (reg_4) && FP_REGNUM_P (REGNO (reg_4)))\n+    rclass_4 = FP_REGS;\n+  else\n+    rclass_4 = GENERAL_REGS;\n+\n+  /* Check if the registers are of same class.  */\n+  if (rclass_1 != rclass_2 || rclass_2 != rclass_3 || rclass_3 != rclass_4)\n+    return false;\n+\n+  return true;\n+}\n+\n+/* Given OPERANDS of consecutive load/store, this function pairs them\n+   into ldp/stp after adjusting the offset.  It depends on the fact\n+   that addresses of load/store instructions are in increasing order.\n+   MODE is the mode of memory operands.  CODE is the rtl operator\n+   which should be applied to all memory operands, it's SIGN_EXTEND,\n+   ZERO_EXTEND or UNKNOWN.  */\n+\n+bool\n+aarch64_gen_adjusted_ldpstp (rtx *operands, bool load,\n+\t\t\t     enum machine_mode mode, RTX_CODE code)\n+{\n+  rtx base, offset, t1, t2;\n+  rtx mem_1, mem_2, mem_3, mem_4;\n+  HOST_WIDE_INT off_val, abs_off, adj_off, new_off, stp_off_limit, msize;\n+\n+  if (load)\n+    {\n+      mem_1 = operands[1];\n+      mem_2 = operands[3];\n+      mem_3 = operands[5];\n+      mem_4 = operands[7];\n+    }\n+  else\n+    {\n+      mem_1 = operands[0];\n+      mem_2 = operands[2];\n+      mem_3 = operands[4];\n+      mem_4 = operands[6];\n+      gcc_assert (code == UNKNOWN);\n+    }\n+\n+  extract_base_offset_in_addr (mem_1, &base, &offset);\n+  gcc_assert (base != NULL_RTX && offset != NULL_RTX);\n+\n+  /* Adjust offset thus it can fit in ldp/stp instruction.  */\n+  msize = GET_MODE_SIZE (mode);\n+  stp_off_limit = msize * 0x40;\n+  off_val = INTVAL (offset);\n+  abs_off = (off_val < 0) ? -off_val : off_val;\n+  new_off = abs_off % stp_off_limit;\n+  adj_off = abs_off - new_off;\n+\n+  /* Further adjust to make sure all offsets are OK.  */\n+  if ((new_off + msize * 2) >= stp_off_limit)\n+    {\n+      adj_off += stp_off_limit;\n+      new_off -= stp_off_limit;\n+    }\n+\n+  /* Make sure the adjustment can be done with ADD/SUB instructions.  */\n+  if (adj_off >= 0x1000)\n+    return false;\n+\n+  if (off_val < 0)\n+    {\n+      adj_off = -adj_off;\n+      new_off = -new_off;\n+    }\n+\n+  /* Create new memory references.  */\n+  mem_1 = change_address (mem_1, VOIDmode,\n+\t\t\t  plus_constant (DImode, operands[8], new_off));\n+\n+  /* Check if the adjusted address is OK for ldp/stp.  */\n+  if (!aarch64_mem_pair_operand (mem_1, mode))\n+    return false;\n+\n+  msize = GET_MODE_SIZE (mode);\n+  mem_2 = change_address (mem_2, VOIDmode,\n+\t\t\t  plus_constant (DImode,\n+\t\t\t\t\t operands[8],\n+\t\t\t\t\t new_off + msize));\n+  mem_3 = change_address (mem_3, VOIDmode,\n+\t\t\t  plus_constant (DImode,\n+\t\t\t\t\t operands[8],\n+\t\t\t\t\t new_off + msize * 2));\n+  mem_4 = change_address (mem_4, VOIDmode,\n+\t\t\t  plus_constant (DImode,\n+\t\t\t\t\t operands[8],\n+\t\t\t\t\t new_off + msize * 3));\n+\n+  if (code == ZERO_EXTEND)\n+    {\n+      mem_1 = gen_rtx_ZERO_EXTEND (DImode, mem_1);\n+      mem_2 = gen_rtx_ZERO_EXTEND (DImode, mem_2);\n+      mem_3 = gen_rtx_ZERO_EXTEND (DImode, mem_3);\n+      mem_4 = gen_rtx_ZERO_EXTEND (DImode, mem_4);\n+    }\n+  else if (code == SIGN_EXTEND)\n+    {\n+      mem_1 = gen_rtx_SIGN_EXTEND (DImode, mem_1);\n+      mem_2 = gen_rtx_SIGN_EXTEND (DImode, mem_2);\n+      mem_3 = gen_rtx_SIGN_EXTEND (DImode, mem_3);\n+      mem_4 = gen_rtx_SIGN_EXTEND (DImode, mem_4);\n+    }\n+\n+  if (load)\n+    {\n+      operands[1] = mem_1;\n+      operands[3] = mem_2;\n+      operands[5] = mem_3;\n+      operands[7] = mem_4;\n+    }\n+  else\n+    {\n+      operands[0] = mem_1;\n+      operands[2] = mem_2;\n+      operands[4] = mem_3;\n+      operands[6] = mem_4;\n+    }\n+\n+  /* Emit adjusting instruction.  */\n+  emit_insn (gen_rtx_SET (VOIDmode, operands[8],\n+\t\t\t  plus_constant (DImode, base, adj_off)));\n+  /* Emit ldp/stp instructions.  */\n+  t1 = gen_rtx_SET (VOIDmode, operands[0], operands[1]);\n+  t2 = gen_rtx_SET (VOIDmode, operands[2], operands[3]);\n+  emit_insn (gen_rtx_PARALLEL (VOIDmode, gen_rtvec (2, t1, t2)));\n+  t1 = gen_rtx_SET (VOIDmode, operands[4], operands[5]);\n+  t2 = gen_rtx_SET (VOIDmode, operands[6], operands[7]);\n+  emit_insn (gen_rtx_PARALLEL (VOIDmode, gen_rtvec (2, t1, t2)));\n+  return true;\n+}\n+\n #undef TARGET_ADDRESS_COST\n #define TARGET_ADDRESS_COST aarch64_address_cost\n \n@@ -10647,6 +11125,9 @@ aarch_macro_fusion_pair_p (rtx_insn *prev, rtx_insn *curr)\n #undef TARGET_SCHED_MACRO_FUSION_PAIR_P\n #define TARGET_SCHED_MACRO_FUSION_PAIR_P aarch_macro_fusion_pair_p\n \n+#undef TARGET_SCHED_FUSION_PRIORITY\n+#define TARGET_SCHED_FUSION_PRIORITY aarch64_sched_fusion_priority\n+\n struct gcc_target targetm = TARGET_INITIALIZER;\n \n #include \"gt-aarch64.h\""}, {"sha": "46be23999efd9130c17a89d382ccb21933dd153a", "filename": "gcc/config/aarch64/aarch64.md", "status": "modified", "additions": 138, "deletions": 32, "changes": 170, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Fconfig%2Faarch64%2Faarch64.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Fconfig%2Faarch64%2Faarch64.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.md?ref=350013bc494c048acf0fb3041dcff32d54b5f462", "patch": "@@ -1081,62 +1081,139 @@\n \n ;; Operands 1 and 3 are tied together by the final condition; so we allow\n ;; fairly lax checking on the second memory operation.\n-(define_insn \"load_pair<mode>\"\n-  [(set (match_operand:GPI 0 \"register_operand\" \"=r\")\n-\t(match_operand:GPI 1 \"aarch64_mem_pair_operand\" \"Ump\"))\n-   (set (match_operand:GPI 2 \"register_operand\" \"=r\")\n-        (match_operand:GPI 3 \"memory_operand\" \"m\"))]\n+(define_insn \"load_pairsi\"\n+  [(set (match_operand:SI 0 \"register_operand\" \"=r,*w\")\n+\t(match_operand:SI 1 \"aarch64_mem_pair_operand\" \"Ump,Ump\"))\n+   (set (match_operand:SI 2 \"register_operand\" \"=r,*w\")\n+\t(match_operand:SI 3 \"memory_operand\" \"m,m\"))]\n   \"rtx_equal_p (XEXP (operands[3], 0),\n \t\tplus_constant (Pmode,\n \t\t\t       XEXP (operands[1], 0),\n-\t\t\t       GET_MODE_SIZE (<MODE>mode)))\"\n-  \"ldp\\\\t%<w>0, %<w>2, %1\"\n-  [(set_attr \"type\" \"load2\")]\n+\t\t\t       GET_MODE_SIZE (SImode)))\"\n+  \"@\n+   ldp\\\\t%w0, %w2, %1\n+   ldp\\\\t%s0, %s2, %1\"\n+  [(set_attr \"type\" \"load2,neon_load1_2reg\")\n+   (set_attr \"fp\" \"*,yes\")]\n )\n \n+(define_insn \"load_pairdi\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r,*w\")\n+\t(match_operand:DI 1 \"aarch64_mem_pair_operand\" \"Ump,Ump\"))\n+   (set (match_operand:DI 2 \"register_operand\" \"=r,*w\")\n+\t(match_operand:DI 3 \"memory_operand\" \"m,m\"))]\n+  \"rtx_equal_p (XEXP (operands[3], 0),\n+\t\tplus_constant (Pmode,\n+\t\t\t       XEXP (operands[1], 0),\n+\t\t\t       GET_MODE_SIZE (DImode)))\"\n+  \"@\n+   ldp\\\\t%x0, %x2, %1\n+   ldp\\\\t%d0, %d2, %1\"\n+  [(set_attr \"type\" \"load2,neon_load1_2reg\")\n+   (set_attr \"fp\" \"*,yes\")]\n+)\n+\n+\n ;; Operands 0 and 2 are tied together by the final condition; so we allow\n ;; fairly lax checking on the second memory operation.\n-(define_insn \"store_pair<mode>\"\n-  [(set (match_operand:GPI 0 \"aarch64_mem_pair_operand\" \"=Ump\")\n-\t(match_operand:GPI 1 \"register_operand\" \"r\"))\n-   (set (match_operand:GPI 2 \"memory_operand\" \"=m\")\n-        (match_operand:GPI 3 \"register_operand\" \"r\"))]\n+(define_insn \"store_pairsi\"\n+  [(set (match_operand:SI 0 \"aarch64_mem_pair_operand\" \"=Ump,Ump\")\n+\t(match_operand:SI 1 \"aarch64_reg_or_zero\" \"rZ,*w\"))\n+   (set (match_operand:SI 2 \"memory_operand\" \"=m,m\")\n+\t(match_operand:SI 3 \"aarch64_reg_or_zero\" \"rZ,*w\"))]\n   \"rtx_equal_p (XEXP (operands[2], 0),\n \t\tplus_constant (Pmode,\n \t\t\t       XEXP (operands[0], 0),\n-\t\t\t       GET_MODE_SIZE (<MODE>mode)))\"\n-  \"stp\\\\t%<w>1, %<w>3, %0\"\n-  [(set_attr \"type\" \"store2\")]\n+\t\t\t       GET_MODE_SIZE (SImode)))\"\n+  \"@\n+   stp\\\\t%w1, %w3, %0\n+   stp\\\\t%s1, %s3, %0\"\n+  [(set_attr \"type\" \"store2,neon_store1_2reg\")\n+   (set_attr \"fp\" \"*,yes\")]\n+)\n+\n+(define_insn \"store_pairdi\"\n+  [(set (match_operand:DI 0 \"aarch64_mem_pair_operand\" \"=Ump,Ump\")\n+\t(match_operand:DI 1 \"aarch64_reg_or_zero\" \"rZ,*w\"))\n+   (set (match_operand:DI 2 \"memory_operand\" \"=m,m\")\n+\t(match_operand:DI 3 \"aarch64_reg_or_zero\" \"rZ,*w\"))]\n+  \"rtx_equal_p (XEXP (operands[2], 0),\n+\t\tplus_constant (Pmode,\n+\t\t\t       XEXP (operands[0], 0),\n+\t\t\t       GET_MODE_SIZE (DImode)))\"\n+  \"@\n+   stp\\\\t%x1, %x3, %0\n+   stp\\\\t%d1, %d3, %0\"\n+  [(set_attr \"type\" \"store2,neon_store1_2reg\")\n+   (set_attr \"fp\" \"*,yes\")]\n )\n \n ;; Operands 1 and 3 are tied together by the final condition; so we allow\n ;; fairly lax checking on the second memory operation.\n-(define_insn \"load_pair<mode>\"\n-  [(set (match_operand:GPF 0 \"register_operand\" \"=w\")\n-\t(match_operand:GPF 1 \"aarch64_mem_pair_operand\" \"Ump\"))\n-   (set (match_operand:GPF 2 \"register_operand\" \"=w\")\n-        (match_operand:GPF 3 \"memory_operand\" \"m\"))]\n+(define_insn \"load_pairsf\"\n+  [(set (match_operand:SF 0 \"register_operand\" \"=w,*r\")\n+\t(match_operand:SF 1 \"aarch64_mem_pair_operand\" \"Ump,Ump\"))\n+   (set (match_operand:SF 2 \"register_operand\" \"=w,*r\")\n+\t(match_operand:SF 3 \"memory_operand\" \"m,m\"))]\n   \"rtx_equal_p (XEXP (operands[3], 0),\n \t\tplus_constant (Pmode,\n \t\t\t       XEXP (operands[1], 0),\n-\t\t\t       GET_MODE_SIZE (<MODE>mode)))\"\n-  \"ldp\\\\t%<w>0, %<w>2, %1\"\n-  [(set_attr \"type\" \"neon_load1_2reg<q>\")]\n+\t\t\t       GET_MODE_SIZE (SFmode)))\"\n+  \"@\n+   ldp\\\\t%s0, %s2, %1\n+   ldp\\\\t%w0, %w2, %1\"\n+  [(set_attr \"type\" \"neon_load1_2reg,load2\")\n+   (set_attr \"fp\" \"yes,*\")]\n+)\n+\n+(define_insn \"load_pairdf\"\n+  [(set (match_operand:DF 0 \"register_operand\" \"=w,*r\")\n+\t(match_operand:DF 1 \"aarch64_mem_pair_operand\" \"Ump,Ump\"))\n+   (set (match_operand:DF 2 \"register_operand\" \"=w,*r\")\n+\t(match_operand:DF 3 \"memory_operand\" \"m,m\"))]\n+  \"rtx_equal_p (XEXP (operands[3], 0),\n+\t\tplus_constant (Pmode,\n+\t\t\t       XEXP (operands[1], 0),\n+\t\t\t       GET_MODE_SIZE (DFmode)))\"\n+  \"@\n+   ldp\\\\t%d0, %d2, %1\n+   ldp\\\\t%x0, %x2, %1\"\n+  [(set_attr \"type\" \"neon_load1_2reg,load2\")\n+   (set_attr \"fp\" \"yes,*\")]\n )\n \n ;; Operands 0 and 2 are tied together by the final condition; so we allow\n ;; fairly lax checking on the second memory operation.\n-(define_insn \"store_pair<mode>\"\n-  [(set (match_operand:GPF 0 \"aarch64_mem_pair_operand\" \"=Ump\")\n-\t(match_operand:GPF 1 \"register_operand\" \"w\"))\n-   (set (match_operand:GPF 2 \"memory_operand\" \"=m\")\n-        (match_operand:GPF 3 \"register_operand\" \"w\"))]\n+(define_insn \"store_pairsf\"\n+  [(set (match_operand:SF 0 \"aarch64_mem_pair_operand\" \"=Ump,Ump\")\n+\t(match_operand:SF 1 \"register_operand\" \"w,*r\"))\n+   (set (match_operand:SF 2 \"memory_operand\" \"=m,m\")\n+\t(match_operand:SF 3 \"register_operand\" \"w,*r\"))]\n   \"rtx_equal_p (XEXP (operands[2], 0),\n \t\tplus_constant (Pmode,\n \t\t\t       XEXP (operands[0], 0),\n-\t\t\t       GET_MODE_SIZE (<MODE>mode)))\"\n-  \"stp\\\\t%<w>1, %<w>3, %0\"\n-  [(set_attr \"type\" \"neon_store1_2reg<q>\")]\n+\t\t\t       GET_MODE_SIZE (SFmode)))\"\n+  \"@\n+   stp\\\\t%s1, %s3, %0\n+   stp\\\\t%w1, %w3, %0\"\n+  [(set_attr \"type\" \"neon_store1_2reg,store2\")\n+   (set_attr \"fp\" \"yes,*\")]\n+)\n+\n+(define_insn \"store_pairdf\"\n+  [(set (match_operand:DF 0 \"aarch64_mem_pair_operand\" \"=Ump,Ump\")\n+\t(match_operand:DF 1 \"register_operand\" \"w,*r\"))\n+   (set (match_operand:DF 2 \"memory_operand\" \"=m,m\")\n+\t(match_operand:DF 3 \"register_operand\" \"w,*r\"))]\n+  \"rtx_equal_p (XEXP (operands[2], 0),\n+\t\tplus_constant (Pmode,\n+\t\t\t       XEXP (operands[0], 0),\n+\t\t\t       GET_MODE_SIZE (DFmode)))\"\n+  \"@\n+   stp\\\\t%d1, %d3, %0\n+   stp\\\\t%x1, %x3, %0\"\n+  [(set_attr \"type\" \"neon_store1_2reg,store2\")\n+   (set_attr \"fp\" \"yes,*\")]\n )\n \n ;; Load pair with post-index writeback.  This is primarily used in function\n@@ -1225,6 +1302,19 @@\n   [(set_attr \"type\" \"extend,load1\")]\n )\n \n+(define_insn \"*load_pair_extendsidi2_aarch64\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+\t(sign_extend:DI (match_operand:SI 1 \"aarch64_mem_pair_operand\" \"Ump\")))\n+   (set (match_operand:DI 2 \"register_operand\" \"=r\")\n+\t(sign_extend:DI (match_operand:SI 3 \"memory_operand\" \"m\")))]\n+  \"rtx_equal_p (XEXP (operands[3], 0),\n+\t\tplus_constant (Pmode,\n+\t\t\t       XEXP (operands[1], 0),\n+\t\t\t       GET_MODE_SIZE (SImode)))\"\n+  \"ldpsw\\\\t%0, %2, %1\"\n+  [(set_attr \"type\" \"load2\")]\n+)\n+\n (define_insn \"*zero_extendsidi2_aarch64\"\n   [(set (match_operand:DI 0 \"register_operand\" \"=r,r\")\n         (zero_extend:DI (match_operand:SI 1 \"nonimmediate_operand\" \"r,m\")))]\n@@ -1235,6 +1325,19 @@\n   [(set_attr \"type\" \"extend,load1\")]\n )\n \n+(define_insn \"*load_pair_zero_extendsidi2_aarch64\"\n+  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n+\t(zero_extend:DI (match_operand:SI 1 \"aarch64_mem_pair_operand\" \"Ump\")))\n+   (set (match_operand:DI 2 \"register_operand\" \"=r\")\n+\t(zero_extend:DI (match_operand:SI 3 \"memory_operand\" \"m\")))]\n+  \"rtx_equal_p (XEXP (operands[3], 0),\n+\t\tplus_constant (Pmode,\n+\t\t\t       XEXP (operands[1], 0),\n+\t\t\t       GET_MODE_SIZE (SImode)))\"\n+  \"ldp\\\\t%w0, %w2, %1\"\n+  [(set_attr \"type\" \"load2\")]\n+)\n+\n (define_expand \"<ANY_EXTEND:optab><SHORT:mode><GPI:mode>2\"\n   [(set (match_operand:GPI 0 \"register_operand\")\n         (ANY_EXTEND:GPI (match_operand:SHORT 1 \"nonimmediate_operand\")))]\n@@ -4238,3 +4341,6 @@\n \n ;; Atomic Operations\n (include \"atomics.md\")\n+\n+;; ldp/stp peephole patterns\n+(include \"aarch64-ldpstp.md\")"}, {"sha": "f32f5c7c1bcf50d5eb5bd3084fde5763fea75242", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 10, "deletions": 0, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=350013bc494c048acf0fb3041dcff32d54b5f462", "patch": "@@ -1,3 +1,13 @@\n+2014-12-05  Bin Cheng  <bin.cheng@arm.com>\n+\n+\t* gcc.target/aarch64/ldp_stp_1.c: New test.\n+\t* gcc.target/aarch64/ldp_stp_2.c: New test.\n+\t* gcc.target/aarch64/ldp_stp_3.c: New test.\n+\t* gcc.target/aarch64/ldp_stp_4.c: New test.\n+\t* gcc.target/aarch64/ldp_stp_5.c: New test.\n+\t* gcc.target/aarch64/lr_free_1.c: Disable scheduling fusion\n+\tand peephole2 pass.\n+\n 2014-12-05  Sandra Loosemore  <sandra@codesourcery.com>\n \n \t* gcc.dg/vect/pr63341-1.c: Remove explicit \"dg-do run\"."}, {"sha": "f02e55f1cc2f01063aea35b3b88f793bb2f7c532", "filename": "gcc/testsuite/gcc.target/aarch64/ldp_stp_1.c", "status": "added", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_1.c?ref=350013bc494c048acf0fb3041dcff32d54b5f462", "patch": "@@ -0,0 +1,24 @@\n+/* { dg-options \"-O2\" } */\n+\n+int arr[4][4];\n+\n+void\n+foo ()\n+{\n+  arr[0][1] = 1;\n+  arr[1][0] = -1;\n+  arr[2][0] = 1;\n+  arr[1][1] = -1;\n+  arr[0][2] = 1;\n+  arr[0][3] = -1;\n+  arr[1][2] = 1;\n+  arr[2][1] = -1;\n+  arr[3][0] = 1;\n+  arr[3][1] = -1;\n+  arr[2][2] = 1;\n+  arr[1][3] = -1;\n+  arr[2][3] = 1;\n+  arr[3][2] = -1;\n+}\n+\n+/* { dg-final { scan-assembler-times \"stp\\tw\\[0-9\\]+, w\\[0-9\\]\" 7 } } */"}, {"sha": "e3b5641339090f2c35f55961d89895039c5bc834", "filename": "gcc/testsuite/gcc.target/aarch64/ldp_stp_2.c", "status": "added", "additions": 17, "deletions": 0, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_2.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_2.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_2.c?ref=350013bc494c048acf0fb3041dcff32d54b5f462", "patch": "@@ -0,0 +1,17 @@\n+/* { dg-options \"-O2\" } */\n+\n+extern void abort (void);\n+\n+int arr[4][4] = {{0, 1, 1, -1}, {-1, -1, 1, -1}, {1, -1, 1, 1}, {1, -1, -1, 0}};\n+long long\n+foo ()\n+{\n+  long long ll = 0;\n+  ll += arr[0][1];\n+  ll += arr[1][0];\n+  ll += arr[1][1];\n+  ll += arr[2][0];\n+  return ll;\n+}\n+\n+/* { dg-final { scan-assembler-times \"ldpsw\\tx\\[0-9\\]+, x\\[0-9\\]\" 1 } } */"}, {"sha": "c6c877bf87ac340740f18edda9f29bacfadce40d", "filename": "gcc/testsuite/gcc.target/aarch64/ldp_stp_3.c", "status": "added", "additions": 17, "deletions": 0, "changes": 17, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_3.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_3.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_3.c?ref=350013bc494c048acf0fb3041dcff32d54b5f462", "patch": "@@ -0,0 +1,17 @@\n+/* { dg-options \"-O2\" } */\n+\n+extern void abort (void);\n+\n+unsigned int arr[4][4] = {{0, 1, 1, 2}, {2, 2, 1, 2}, {1, 2, 1, 1}, {1, 2, 2, 0}};\n+unsigned long long\n+foo ()\n+{\n+  unsigned long long ll = 0;\n+  ll += arr[0][1];\n+  ll += arr[1][0];\n+  ll += arr[1][1];\n+  ll += arr[2][0];\n+  return ll;\n+}\n+\n+/* { dg-final { scan-assembler-times \"ldp\\tw\\[0-9\\]+, w\\[0-9\\]\" 1 } } */"}, {"sha": "40056b1adebd3fe3e473e378e123ee62041da9a2", "filename": "gcc/testsuite/gcc.target/aarch64/ldp_stp_4.c", "status": "added", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_4.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_4.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_4.c?ref=350013bc494c048acf0fb3041dcff32d54b5f462", "patch": "@@ -0,0 +1,24 @@\n+/* { dg-options \"-O2\" } */\n+\n+float arr[4][4];\n+\n+void\n+foo ()\n+{\n+  arr[0][1] = 1;\n+  arr[1][0] = -1;\n+  arr[2][0] = 1;\n+  arr[1][1] = -1;\n+  arr[0][2] = 1;\n+  arr[0][3] = -1;\n+  arr[1][2] = 1;\n+  arr[2][1] = -1;\n+  arr[3][0] = 1;\n+  arr[3][1] = -1;\n+  arr[2][2] = 1;\n+  arr[1][3] = -1;\n+  arr[2][3] = 1;\n+  arr[3][2] = -1;\n+}\n+\n+/* { dg-final { scan-assembler-times \"stp\\ts\\[0-9\\]+, s\\[0-9\\]\" 7 } } */"}, {"sha": "94266181df7edf53e5712be9c2f7474016d83724", "filename": "gcc/testsuite/gcc.target/aarch64/ldp_stp_5.c", "status": "added", "additions": 24, "deletions": 0, "changes": 24, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_5.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_5.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fldp_stp_5.c?ref=350013bc494c048acf0fb3041dcff32d54b5f462", "patch": "@@ -0,0 +1,24 @@\n+/* { dg-options \"-O2\" } */\n+\n+double arr[4][4];\n+\n+void\n+foo ()\n+{\n+  arr[0][1] = 1;\n+  arr[1][0] = -1;\n+  arr[2][0] = 1;\n+  arr[1][1] = -1;\n+  arr[0][2] = 1;\n+  arr[0][3] = -1;\n+  arr[1][2] = 1;\n+  arr[2][1] = -1;\n+  arr[3][0] = 1;\n+  arr[3][1] = -1;\n+  arr[2][2] = 1;\n+  arr[1][3] = -1;\n+  arr[2][3] = 1;\n+  arr[3][2] = -1;\n+}\n+\n+/* { dg-final { scan-assembler-times \"stp\\td\\[0-9\\]+, d\\[0-9\\]\" 7 } } */"}, {"sha": "84dcc4df01162fc13d1964893644a8c0ab4d7383", "filename": "gcc/testsuite/gcc.target/aarch64/lr_free_1.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Flr_free_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/350013bc494c048acf0fb3041dcff32d54b5f462/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Flr_free_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Flr_free_1.c?ref=350013bc494c048acf0fb3041dcff32d54b5f462", "patch": "@@ -1,5 +1,5 @@\n /* { dg-do run } */\n-/* { dg-options \"-fno-inline -O2 -fomit-frame-pointer -ffixed-x2 -ffixed-x3 -ffixed-x4 -ffixed-x5 -ffixed-x6 -ffixed-x7 -ffixed-x8 -ffixed-x9 -ffixed-x10 -ffixed-x11 -ffixed-x12 -ffixed-x13 -ffixed-x14 -ffixed-x15 -ffixed-x16 -ffixed-x17 -ffixed-x18 -ffixed-x19 -ffixed-x20 -ffixed-x21 -ffixed-x22 -ffixed-x23 -ffixed-x24 -ffixed-x25 -ffixed-x26 -ffixed-x27 -ffixed-28 -ffixed-29 --save-temps -mgeneral-regs-only -fno-ipa-cp\" } */\n+/* { dg-options \"-fno-inline -O2 -fomit-frame-pointer -ffixed-x2 -ffixed-x3 -ffixed-x4 -ffixed-x5 -ffixed-x6 -ffixed-x7 -ffixed-x8 -ffixed-x9 -ffixed-x10 -ffixed-x11 -ffixed-x12 -ffixed-x13 -ffixed-x14 -ffixed-x15 -ffixed-x16 -ffixed-x17 -ffixed-x18 -ffixed-x19 -ffixed-x20 -ffixed-x21 -ffixed-x22 -ffixed-x23 -ffixed-x24 -ffixed-x25 -ffixed-x26 -ffixed-x27 -ffixed-28 -ffixed-29 --save-temps -mgeneral-regs-only -fno-ipa-cp -fno-schedule-fusion -fno-peephole2\" } */\n \n extern void abort ();\n "}]}