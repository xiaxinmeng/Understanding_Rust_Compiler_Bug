{"sha": "83d796d3e58badcb864d179b882979f714ffd162", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ODNkNzk2ZDNlNThiYWRjYjg2NGQxNzliODgyOTc5ZjcxNGZmZDE2Mg==", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2021-08-03T12:00:44Z"}, "committer": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2021-08-03T12:00:44Z"}, "message": "aarch64: Add a simple fixed-point class for costing\n\nThis patch adds a simple fixed-point class for holding fractional\ncost values.  It can exactly represent the reciprocal of any\nsingle-vector SVE element count (including the non-power-of-2 ones).\nThis means that it can also hold 1/N for all N in [1, 16], which should\nbe enough for the various *_per_cycle fields.\n\nFor now the assumption is that the number of possible reciprocals\nis fixed at compile time and so the class should always be able\nto hold an exact value.\n\nThe class uses a uint64_t to hold the fixed-point value, which means\nthat it can hold any scaled uint32_t cost.  Normally we don't worry\nabout overflow when manipulating raw uint32_t costs, but just to be\non the safe side, the class uses saturating arithmetic for all\noperations.\n\nAs far as the changes to the cost routines themselves go:\n\n- The changes to aarch64_add_stmt_cost and its subroutines are\n  just laying groundwork for future patches; no functional change\n  intended.\n\n- The changes to aarch64_adjust_body_cost mean that we now\n  take fractional differences into account.\n\ngcc/\n\t* config/aarch64/fractional-cost.h: New file.\n\t* config/aarch64/aarch64.c: Include <algorithm> (indirectly)\n\tand cost_fraction.h.\n\t(vec_cost_fraction): New typedef.\n\t(aarch64_detect_scalar_stmt_subtype): Use it for statement costs.\n\t(aarch64_detect_vector_stmt_subtype): Likewise.\n\t(aarch64_sve_adjust_stmt_cost, aarch64_adjust_stmt_cost): Likewise.\n\t(aarch64_estimate_min_cycles_per_iter): Use vec_cost_fraction\n\tfor cycle counts.\n\t(aarch64_adjust_body_cost): Likewise.\n\t(aarch64_test_cost_fraction): New function.\n\t(aarch64_run_selftests): Call it.", "tree": {"sha": "3474ca36f725f90b443b2481f142a8fc78ec0517", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/3474ca36f725f90b443b2481f142a8fc78ec0517"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/83d796d3e58badcb864d179b882979f714ffd162", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/83d796d3e58badcb864d179b882979f714ffd162", "html_url": "https://github.com/Rust-GCC/gccrs/commit/83d796d3e58badcb864d179b882979f714ffd162", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/83d796d3e58badcb864d179b882979f714ffd162/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "fa3ca6151ccac0e215727641eee36abf6e437b26", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/fa3ca6151ccac0e215727641eee36abf6e437b26", "html_url": "https://github.com/Rust-GCC/gccrs/commit/fa3ca6151ccac0e215727641eee36abf6e437b26"}], "stats": {"total": 415, "additions": 377, "deletions": 38}, "files": [{"sha": "17fcb34b2c8a92c84fa53388d05af0a2ec8ac137", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 141, "deletions": 38, "changes": 179, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/83d796d3e58badcb864d179b882979f714ffd162/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/83d796d3e58badcb864d179b882979f714ffd162/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=83d796d3e58badcb864d179b882979f714ffd162", "patch": "@@ -20,8 +20,9 @@\n \n #define IN_TARGET_CODE 1\n \n-#include \"config.h\"\n #define INCLUDE_STRING\n+#define INCLUDE_ALGORITHM\n+#include \"config.h\"\n #include \"system.h\"\n #include \"coretypes.h\"\n #include \"backend.h\"\n@@ -76,6 +77,7 @@\n #include \"function-abi.h\"\n #include \"gimple-pretty-print.h\"\n #include \"tree-ssa-loop-niter.h\"\n+#include \"fractional-cost.h\"\n \n /* This file should be included last.  */\n #include \"target-def.h\"\n@@ -14912,10 +14914,10 @@ aarch64_in_loop_reduction_latency (vec_info *vinfo, stmt_vec_info stmt_info,\n    for STMT_INFO, which has cost kind KIND.  If this is a scalar operation,\n    try to subdivide the target-independent categorization provided by KIND\n    to get a more accurate cost.  */\n-static unsigned int\n+static fractional_cost\n aarch64_detect_scalar_stmt_subtype (vec_info *vinfo, vect_cost_for_stmt kind,\n \t\t\t\t    stmt_vec_info stmt_info,\n-\t\t\t\t    unsigned int stmt_cost)\n+\t\t\t\t    fractional_cost stmt_cost)\n {\n   /* Detect an extension of a loaded value.  In general, we'll be able to fuse\n      the extension with the load.  */\n@@ -14931,11 +14933,11 @@ aarch64_detect_scalar_stmt_subtype (vec_info *vinfo, vect_cost_for_stmt kind,\n    the target-independent categorization provided by KIND to get a more\n    accurate cost.  WHERE specifies where the cost associated with KIND\n    occurs.  */\n-static unsigned int\n+static fractional_cost\n aarch64_detect_vector_stmt_subtype (vec_info *vinfo, vect_cost_for_stmt kind,\n \t\t\t\t    stmt_vec_info stmt_info, tree vectype,\n \t\t\t\t    enum vect_cost_model_location where,\n-\t\t\t\t    unsigned int stmt_cost)\n+\t\t\t\t    fractional_cost stmt_cost)\n {\n   const simd_vec_cost *simd_costs = aarch64_simd_vec_costs (vectype);\n   const sve_vec_cost *sve_costs = nullptr;\n@@ -15016,10 +15018,10 @@ aarch64_detect_vector_stmt_subtype (vec_info *vinfo, vect_cost_for_stmt kind,\n    for STMT_INFO, which has cost kind KIND and which when vectorized would\n    operate on vector type VECTYPE.  Adjust the cost as necessary for SVE\n    targets.  */\n-static unsigned int\n+static fractional_cost\n aarch64_sve_adjust_stmt_cost (class vec_info *vinfo, vect_cost_for_stmt kind,\n \t\t\t      stmt_vec_info stmt_info, tree vectype,\n-\t\t\t      unsigned int stmt_cost)\n+\t\t\t      fractional_cost stmt_cost)\n {\n   /* Unlike vec_promote_demote, vector_stmt conversions do not change the\n      vector register size or number of units.  Integer promotions of this\n@@ -15083,9 +15085,9 @@ aarch64_sve_adjust_stmt_cost (class vec_info *vinfo, vect_cost_for_stmt kind,\n /* STMT_COST is the cost calculated for STMT_INFO, which has cost kind KIND\n    and which when vectorized would operate on vector type VECTYPE.  Add the\n    cost of any embedded operations.  */\n-static unsigned int\n+static fractional_cost\n aarch64_adjust_stmt_cost (vect_cost_for_stmt kind, stmt_vec_info stmt_info,\n-\t\t\t  tree vectype, unsigned int stmt_cost)\n+\t\t\t  tree vectype, fractional_cost stmt_cost)\n {\n   if (vectype)\n     {\n@@ -15339,7 +15341,7 @@ aarch64_add_stmt_cost (class vec_info *vinfo, void *data, int count,\n \n   if (flag_vect_cost_model)\n     {\n-      int stmt_cost\n+      fractional_cost stmt_cost\n \t= aarch64_builtin_vectorization_cost (kind, vectype, misalign);\n \n       /* Do one-time initialization based on the vinfo.  */\n@@ -15440,7 +15442,7 @@ aarch64_add_stmt_cost (class vec_info *vinfo, void *data, int count,\n \t  count *= LOOP_VINFO_INNER_LOOP_COST_FACTOR (loop_vinfo); /*  FIXME  */\n \t}\n \n-      retval = (unsigned) (count * stmt_cost);\n+      retval = (count * stmt_cost).ceil ();\n       costs->region[where] += retval;\n     }\n \n@@ -15472,17 +15474,17 @@ aarch64_sve_op_count::dump () const\n \n /* Use ISSUE_INFO to estimate the minimum number of cycles needed to issue\n    the operations described by OPS.  This is a very simplistic model!  */\n-static unsigned int\n+static fractional_cost\n aarch64_estimate_min_cycles_per_iter\n   (const aarch64_vec_op_count *ops,\n    const aarch64_base_vec_issue_info *issue_info)\n {\n-  unsigned int cycles = MAX (ops->reduction_latency, 1);\n-  cycles = MAX (cycles, CEIL (ops->stores, issue_info->stores_per_cycle));\n-  cycles = MAX (cycles, CEIL (ops->loads + ops->stores,\n-\t\t\t      issue_info->loads_stores_per_cycle));\n-  cycles = MAX (cycles, CEIL (ops->general_ops,\n-\t\t\t      issue_info->general_ops_per_cycle));\n+  fractional_cost cycles = MAX (ops->reduction_latency, 1);\n+  cycles = std::max (cycles, { ops->stores, issue_info->stores_per_cycle });\n+  cycles = std::max (cycles, { ops->loads + ops->stores,\n+\t\t\t       issue_info->loads_stores_per_cycle });\n+  cycles = std::max (cycles, { ops->general_ops,\n+\t\t\t       issue_info->general_ops_per_cycle });\n   return cycles;\n }\n \n@@ -15536,12 +15538,14 @@ aarch64_adjust_body_cost (aarch64_vector_costs *costs, unsigned int body_cost)\n   if (!issue_info)\n     return body_cost;\n \n-  unsigned int scalar_cycles_per_iter\n+  fractional_cost scalar_cycles_per_iter\n     = aarch64_estimate_min_cycles_per_iter (&costs->scalar_ops,\n \t\t\t\t\t    issue_info->scalar);\n-  unsigned int advsimd_cycles_per_iter\n+\n+  fractional_cost advsimd_cycles_per_iter\n     = aarch64_estimate_min_cycles_per_iter (&costs->advsimd_ops,\n \t\t\t\t\t    issue_info->advsimd);\n+\n   bool could_use_advsimd\n     = ((costs->vec_flags & VEC_ADVSIMD)\n        || (aarch64_autovec_preference != 2\n@@ -15558,51 +15562,54 @@ aarch64_adjust_body_cost (aarch64_vector_costs *costs, unsigned int body_cost)\n       dump_printf_loc (MSG_NOTE, vect_location, \"Scalar issue estimate:\\n\");\n       costs->scalar_ops.dump ();\n       dump_printf_loc (MSG_NOTE, vect_location,\n-\t\t       \"  estimated cycles per iteration = %d\\n\",\n-\t\t       scalar_cycles_per_iter);\n+\t\t       \"  estimated cycles per iteration = %f\\n\",\n+\t\t       scalar_cycles_per_iter.as_double ());\n       if (could_use_advsimd)\n \t{\n \t  dump_printf_loc (MSG_NOTE, vect_location,\n \t\t\t   \"Advanced SIMD issue estimate:\\n\");\n \t  costs->advsimd_ops.dump ();\n \t  dump_printf_loc (MSG_NOTE, vect_location,\n-\t\t\t   \"  estimated cycles per iteration = %d\\n\",\n-\t\t\t   advsimd_cycles_per_iter);\n+\t\t\t   \"  estimated cycles per iteration = %f\\n\",\n+\t\t\t   advsimd_cycles_per_iter.as_double ());\n \t}\n       else\n \tdump_printf_loc (MSG_NOTE, vect_location,\n \t\t\t \"Loop could not use Advanced SIMD\\n\");\n     }\n \n-  uint64_t vector_cycles_per_iter = advsimd_cycles_per_iter;\n+  fractional_cost vector_cycles_per_iter = advsimd_cycles_per_iter;\n   unsigned int vector_reduction_latency = costs->advsimd_ops.reduction_latency;\n+\n   if ((costs->vec_flags & VEC_ANY_SVE) && issue_info->sve)\n     {\n       /* Estimate the minimum number of cycles per iteration needed to issue\n \t non-predicate operations.  */\n-      unsigned int sve_cycles_per_iter\n+      fractional_cost sve_cycles_per_iter\n \t= aarch64_estimate_min_cycles_per_iter (&costs->sve_ops,\n \t\t\t\t\t\tissue_info->sve);\n \n       /* Separately estimate the minimum number of cycles per iteration needed\n \t to issue the predicate operations.  */\n-      unsigned int pred_cycles_per_iter\n-\t= CEIL (costs->sve_ops.pred_ops, issue_info->sve->pred_ops_per_cycle);\n+      fractional_cost pred_cycles_per_iter\n+\t= { costs->sve_ops.pred_ops, issue_info->sve->pred_ops_per_cycle };\n \n       if (dump_enabled_p ())\n \t{\n \t  dump_printf_loc (MSG_NOTE, vect_location, \"SVE issue estimate:\\n\");\n \t  costs->sve_ops.dump ();\n \t  dump_printf_loc (MSG_NOTE, vect_location,\n \t\t\t   \"  estimated cycles per iteration for non-predicate\"\n-\t\t\t   \" operations = %d\\n\", sve_cycles_per_iter);\n+\t\t\t   \" operations = %f\\n\",\n+\t\t\t   sve_cycles_per_iter.as_double ());\n \t  if (costs->sve_ops.pred_ops)\n \t    dump_printf_loc (MSG_NOTE, vect_location, \"  estimated cycles per\"\n \t\t\t     \" iteration for predicate operations = %d\\n\",\n-\t\t\t     pred_cycles_per_iter);\n+\t\t\t     pred_cycles_per_iter.as_double ());\n \t}\n \n-      vector_cycles_per_iter = MAX (sve_cycles_per_iter, pred_cycles_per_iter);\n+      vector_cycles_per_iter = std::max (sve_cycles_per_iter,\n+\t\t\t\t\t pred_cycles_per_iter);\n       vector_reduction_latency = costs->sve_ops.reduction_latency;\n \n       /* If the scalar version of the loop could issue at least as\n@@ -15616,7 +15623,7 @@ aarch64_adjust_body_cost (aarch64_vector_costs *costs, unsigned int body_cost)\n \t too drastic for scalar_cycles_per_iter vs. sve_cycles_per_iter;\n \t code later in the function handles that case in a more\n \t conservative way.  */\n-      uint64_t sve_estimate = pred_cycles_per_iter + 1;\n+      fractional_cost sve_estimate = pred_cycles_per_iter + 1;\n       if (scalar_cycles_per_iter < sve_estimate)\n \t{\n \t  unsigned int min_cost\n@@ -15656,8 +15663,10 @@ aarch64_adjust_body_cost (aarch64_vector_costs *costs, unsigned int body_cost)\n       if (could_use_advsimd && advsimd_cycles_per_iter < sve_estimate)\n \t{\n \t  /* This ensures that min_cost > orig_body_cost * 2.  */\n-\t  unsigned int min_cost\n-\t    = orig_body_cost * CEIL (sve_estimate, advsimd_cycles_per_iter) + 1;\n+\t  unsigned int factor\n+\t    = fractional_cost::scale (1, sve_estimate,\n+\t\t\t\t      advsimd_cycles_per_iter);\n+\t  unsigned int min_cost = orig_body_cost * factor + 1;\n \t  if (body_cost < min_cost)\n \t    {\n \t      if (dump_enabled_p ())\n@@ -15690,8 +15699,8 @@ aarch64_adjust_body_cost (aarch64_vector_costs *costs, unsigned int body_cost)\n      so minor differences should only result in minor changes.  */\n   else if (scalar_cycles_per_iter < vector_cycles_per_iter)\n     {\n-      body_cost = CEIL (body_cost * vector_cycles_per_iter,\n-\t\t\tscalar_cycles_per_iter);\n+      body_cost = fractional_cost::scale (body_cost, vector_cycles_per_iter,\n+\t\t\t\t\t  scalar_cycles_per_iter);\n       if (dump_enabled_p ())\n \tdump_printf_loc (MSG_NOTE, vect_location,\n \t\t\t \"Increasing body cost to %d because scalar code\"\n@@ -15716,8 +15725,8 @@ aarch64_adjust_body_cost (aarch64_vector_costs *costs, unsigned int body_cost)\n \t   && scalar_cycles_per_iter > vector_cycles_per_iter\n \t   && !should_disparage)\n     {\n-      body_cost = CEIL (body_cost * vector_cycles_per_iter,\n-\t\t\tscalar_cycles_per_iter);\n+      body_cost = fractional_cost::scale (body_cost, vector_cycles_per_iter,\n+\t\t\t\t\t  scalar_cycles_per_iter);\n       if (dump_enabled_p ())\n \tdump_printf_loc (MSG_NOTE, vect_location,\n \t\t\t \"Decreasing body cost to %d account for smaller\"\n@@ -25589,12 +25598,106 @@ aarch64_test_loading_full_dump ()\n   ASSERT_EQ (SImode, GET_MODE (crtl->return_rtx));\n }\n \n+/* Test the fractional_cost class.  */\n+\n+static void\n+aarch64_test_fractional_cost ()\n+{\n+  using cf = fractional_cost;\n+\n+  ASSERT_EQ (cf (0, 20), 0);\n+\n+  ASSERT_EQ (cf (4, 2), 2);\n+  ASSERT_EQ (3, cf (9, 3));\n+\n+  ASSERT_NE (cf (5, 2), 2);\n+  ASSERT_NE (3, cf (8, 3));\n+\n+  ASSERT_EQ (cf (7, 11) + cf (15, 11), 2);\n+  ASSERT_EQ (cf (2, 3) + cf (3, 5), cf (19, 15));\n+  ASSERT_EQ (cf (2, 3) + cf (1, 6) + cf (1, 6), 1);\n+\n+  ASSERT_EQ (cf (14, 15) - cf (4, 15), cf (2, 3));\n+  ASSERT_EQ (cf (1, 4) - cf (1, 2), 0);\n+  ASSERT_EQ (cf (3, 5) - cf (1, 10), cf (1, 2));\n+  ASSERT_EQ (cf (11, 3) - 3, cf (2, 3));\n+  ASSERT_EQ (3 - cf (7, 3), cf (2, 3));\n+  ASSERT_EQ (3 - cf (10, 3), 0);\n+\n+  ASSERT_EQ (cf (2, 3) * 5, cf (10, 3));\n+  ASSERT_EQ (14 * cf (11, 21), cf (22, 3));\n+\n+  ASSERT_TRUE (cf (4, 15) < cf (5, 15));\n+  ASSERT_FALSE (cf (5, 15) < cf (5, 15));\n+  ASSERT_FALSE (cf (6, 15) < cf (5, 15));\n+  ASSERT_TRUE (cf (1, 3) < cf (2, 5));\n+  ASSERT_TRUE (cf (1, 12) < cf (1, 6));\n+  ASSERT_FALSE (cf (5, 3) < cf (5, 3));\n+  ASSERT_TRUE (cf (239, 240) < 1);\n+  ASSERT_FALSE (cf (240, 240) < 1);\n+  ASSERT_FALSE (cf (241, 240) < 1);\n+  ASSERT_FALSE (2 < cf (207, 104));\n+  ASSERT_FALSE (2 < cf (208, 104));\n+  ASSERT_TRUE (2 < cf (209, 104));\n+\n+  ASSERT_TRUE (cf (4, 15) < cf (5, 15));\n+  ASSERT_FALSE (cf (5, 15) < cf (5, 15));\n+  ASSERT_FALSE (cf (6, 15) < cf (5, 15));\n+  ASSERT_TRUE (cf (1, 3) < cf (2, 5));\n+  ASSERT_TRUE (cf (1, 12) < cf (1, 6));\n+  ASSERT_FALSE (cf (5, 3) < cf (5, 3));\n+  ASSERT_TRUE (cf (239, 240) < 1);\n+  ASSERT_FALSE (cf (240, 240) < 1);\n+  ASSERT_FALSE (cf (241, 240) < 1);\n+  ASSERT_FALSE (2 < cf (207, 104));\n+  ASSERT_FALSE (2 < cf (208, 104));\n+  ASSERT_TRUE (2 < cf (209, 104));\n+\n+  ASSERT_FALSE (cf (4, 15) >= cf (5, 15));\n+  ASSERT_TRUE (cf (5, 15) >= cf (5, 15));\n+  ASSERT_TRUE (cf (6, 15) >= cf (5, 15));\n+  ASSERT_FALSE (cf (1, 3) >= cf (2, 5));\n+  ASSERT_FALSE (cf (1, 12) >= cf (1, 6));\n+  ASSERT_TRUE (cf (5, 3) >= cf (5, 3));\n+  ASSERT_FALSE (cf (239, 240) >= 1);\n+  ASSERT_TRUE (cf (240, 240) >= 1);\n+  ASSERT_TRUE (cf (241, 240) >= 1);\n+  ASSERT_TRUE (2 >= cf (207, 104));\n+  ASSERT_TRUE (2 >= cf (208, 104));\n+  ASSERT_FALSE (2 >= cf (209, 104));\n+\n+  ASSERT_FALSE (cf (4, 15) > cf (5, 15));\n+  ASSERT_FALSE (cf (5, 15) > cf (5, 15));\n+  ASSERT_TRUE (cf (6, 15) > cf (5, 15));\n+  ASSERT_FALSE (cf (1, 3) > cf (2, 5));\n+  ASSERT_FALSE (cf (1, 12) > cf (1, 6));\n+  ASSERT_FALSE (cf (5, 3) > cf (5, 3));\n+  ASSERT_FALSE (cf (239, 240) > 1);\n+  ASSERT_FALSE (cf (240, 240) > 1);\n+  ASSERT_TRUE (cf (241, 240) > 1);\n+  ASSERT_TRUE (2 > cf (207, 104));\n+  ASSERT_FALSE (2 > cf (208, 104));\n+  ASSERT_FALSE (2 > cf (209, 104));\n+\n+  ASSERT_EQ (cf (1, 2).ceil (), 1);\n+  ASSERT_EQ (cf (11, 7).ceil (), 2);\n+  ASSERT_EQ (cf (20, 1).ceil (), 20);\n+  ASSERT_EQ ((cf (0xfffffffd) + 1).ceil (), 0xfffffffe);\n+  ASSERT_EQ ((cf (0xfffffffd) + 2).ceil (), 0xffffffff);\n+  ASSERT_EQ ((cf (0xfffffffd) + 3).ceil (), 0xffffffff);\n+  ASSERT_EQ ((cf (0x7fffffff) * 2).ceil (), 0xfffffffe);\n+  ASSERT_EQ ((cf (0x80000000) * 2).ceil (), 0xffffffff);\n+\n+  ASSERT_EQ (cf (1, 2).as_double (), 0.5);\n+}\n+\n /* Run all target-specific selftests.  */\n \n static void\n aarch64_run_selftests (void)\n {\n   aarch64_test_loading_full_dump ();\n+  aarch64_test_fractional_cost ();\n }\n \n } // namespace selftest"}, {"sha": "6a01634905bb371227157340f55ad21f553cdef6", "filename": "gcc/config/aarch64/fractional-cost.h", "status": "added", "additions": 236, "deletions": 0, "changes": 236, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/83d796d3e58badcb864d179b882979f714ffd162/gcc%2Fconfig%2Faarch64%2Ffractional-cost.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/83d796d3e58badcb864d179b882979f714ffd162/gcc%2Fconfig%2Faarch64%2Ffractional-cost.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Ffractional-cost.h?ref=83d796d3e58badcb864d179b882979f714ffd162", "patch": "@@ -0,0 +1,236 @@\n+// Simple fixed-point representation of fractional costs\n+// Copyright (C) 2021 Free Software Foundation, Inc.\n+//\n+// This file is part of GCC.\n+//\n+// GCC is free software; you can redistribute it and/or modify it under\n+// the terms of the GNU General Public License as published by the Free\n+// Software Foundation; either version 3, or (at your option) any later\n+// version.\n+//\n+// GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+// WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+// FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+// for more details.\n+//\n+// You should have received a copy of the GNU General Public License\n+// along with GCC; see the file COPYING3.  If not see\n+// <http://www.gnu.org/licenses/>.\n+\n+// A simple saturating fixed-point type for representing fractional\n+// intermediate results in cost calculations.  The input and result\n+// costs are assumed to be uint32_ts.  Unlike sreal, the class can\n+// represent most values that we care about exactly (without rounding).\n+// See the comment above the SCALE field for the current set of\n+// exactly-representable reciprocals.\n+class fractional_cost\n+{\n+public:\n+  // Construct an object equal to INT_VALUE.\n+  constexpr fractional_cost (uint32_t int_value = 0)\n+    : m_value (uint64_t (int_value) * SCALE) {}\n+\n+  fractional_cost (uint32_t a, uint32_t b);\n+\n+  fractional_cost operator+ (const fractional_cost &) const;\n+  fractional_cost operator- (const fractional_cost &) const;\n+  fractional_cost operator* (uint32_t) const;\n+\n+  fractional_cost &operator+= (const fractional_cost &);\n+  fractional_cost &operator-= (const fractional_cost &);\n+  fractional_cost &operator*= (uint32_t);\n+\n+  bool operator== (const fractional_cost &) const;\n+  bool operator!= (const fractional_cost &) const;\n+  bool operator< (const fractional_cost &) const;\n+  bool operator<= (const fractional_cost &) const;\n+  bool operator>= (const fractional_cost &) const;\n+  bool operator> (const fractional_cost &) const;\n+\n+  uint32_t ceil () const;\n+\n+  static uint32_t scale (uint32_t, fractional_cost, fractional_cost);\n+\n+  explicit operator bool () const { return m_value != 0; }\n+\n+  // Convert the value to a double.\n+  double as_double () const { return double (m_value) / SCALE; }\n+\n+private:\n+  enum raw { RAW };\n+  constexpr fractional_cost (uint64_t value, raw) : m_value (value) {}\n+\n+  // A multiple of [1, 16] * 16.  This ensures that 1/N is representable\n+  // for every every possible SVE element count N, or for any \"X per cycle\"\n+  // value N in the range [1, 16].\n+  static const uint32_t SCALE = 11531520;\n+\n+  // The value multiplied by BIAS.\n+  uint64_t m_value;\n+};\n+\n+// Construct a representation of A / B, rounding up if (contrary to\n+// expectations) we can't represent the value exactly.  For now we\n+// treat inexact values as a bug, since all values of B should come\n+// from a small set of values that are known at compile time.\n+inline fractional_cost::fractional_cost (uint32_t a, uint32_t b)\n+  : m_value (CEIL (uint64_t (a) * SCALE, uint64_t (b)))\n+{\n+  gcc_checking_assert (SCALE % b == 0);\n+}\n+\n+inline fractional_cost\n+fractional_cost::operator+ (const fractional_cost &other) const\n+{\n+  uint64_t sum = m_value + other.m_value;\n+  return { sum >= m_value ? sum : ~uint64_t (0), RAW };\n+}\n+\n+inline fractional_cost &\n+fractional_cost::operator+= (const fractional_cost &other)\n+{\n+  *this = *this + other;\n+  return *this;\n+}\n+\n+inline fractional_cost\n+fractional_cost::operator- (const fractional_cost &other) const\n+{\n+  uint64_t diff = m_value - other.m_value;\n+  return { diff <= m_value ? diff : 0, RAW };\n+}\n+\n+inline fractional_cost &\n+fractional_cost::operator-= (const fractional_cost &other)\n+{\n+  *this = *this - other;\n+  return *this;\n+}\n+\n+inline fractional_cost\n+fractional_cost::operator* (uint32_t other) const\n+{\n+  if (other == 0)\n+    return 0;\n+\n+  uint64_t max = ~uint64_t (0);\n+  return { m_value <= max / other ? m_value * other : max, RAW };\n+}\n+\n+inline fractional_cost &\n+fractional_cost::operator*= (uint32_t other)\n+{\n+  *this = *this * other;\n+  return *this;\n+}\n+\n+inline bool\n+fractional_cost::operator== (const fractional_cost &other) const\n+{\n+  return m_value == other.m_value;\n+}\n+\n+inline bool\n+fractional_cost::operator!= (const fractional_cost &other) const\n+{\n+  return m_value != other.m_value;\n+}\n+\n+inline bool\n+fractional_cost::operator< (const fractional_cost &other) const\n+{\n+  return m_value < other.m_value;\n+}\n+\n+inline bool\n+fractional_cost::operator<= (const fractional_cost &other) const\n+{\n+  return m_value <= other.m_value;\n+}\n+\n+inline bool\n+fractional_cost::operator>= (const fractional_cost &other) const\n+{\n+  return m_value >= other.m_value;\n+}\n+\n+inline bool\n+fractional_cost::operator> (const fractional_cost &other) const\n+{\n+  return m_value > other.m_value;\n+}\n+\n+// Round the value up to the nearest integer and saturate to a uint32_t.\n+inline uint32_t\n+fractional_cost::ceil () const\n+{\n+  uint32_t max = ~uint32_t (0);\n+  if (m_value <= uint64_t (max - 1) * SCALE)\n+    return (m_value + SCALE - 1) / SCALE;\n+  return max;\n+}\n+\n+// Round (COST * A) / B up to the nearest integer and saturate to a uint32_t.\n+inline uint32_t\n+fractional_cost::scale (uint32_t cost, fractional_cost a, fractional_cost b)\n+{\n+  widest_int result = wi::div_ceil (widest_int (cost) * a.m_value,\n+\t\t\t\t    b.m_value, SIGNED);\n+  if (result < ~uint32_t (0))\n+    return result.to_shwi ();\n+  return ~uint32_t (0);\n+}\n+\n+inline fractional_cost\n+operator+ (uint32_t a, const fractional_cost &b)\n+{\n+  return b.operator+ (a);\n+}\n+\n+inline fractional_cost\n+operator- (uint32_t a, const fractional_cost &b)\n+{\n+  return fractional_cost (a).operator- (b);\n+}\n+\n+inline fractional_cost\n+operator* (uint32_t a, const fractional_cost &b)\n+{\n+  return b.operator* (a);\n+}\n+\n+inline bool\n+operator== (uint32_t a, const fractional_cost &b)\n+{\n+  return b.operator== (a);\n+}\n+\n+inline bool\n+operator!= (uint32_t a, const fractional_cost &b)\n+{\n+  return b.operator!= (a);\n+}\n+\n+inline bool\n+operator< (uint32_t a, const fractional_cost &b)\n+{\n+  return b.operator> (a);\n+}\n+\n+inline bool\n+operator<= (uint32_t a, const fractional_cost &b)\n+{\n+  return b.operator>= (a);\n+}\n+\n+inline bool\n+operator>= (uint32_t a, const fractional_cost &b)\n+{\n+  return b.operator<= (a);\n+}\n+\n+inline bool\n+operator> (uint32_t a, const fractional_cost &b)\n+{\n+  return b.operator< (a);\n+}"}]}