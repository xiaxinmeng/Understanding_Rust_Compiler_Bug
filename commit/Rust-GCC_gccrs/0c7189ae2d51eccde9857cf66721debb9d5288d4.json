{"sha": "0c7189ae2d51eccde9857cf66721debb9d5288d4", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MGM3MTg5YWUyZDUxZWNjZGU5ODU3Y2Y2NjcyMWRlYmI5ZDUyODhkNA==", "commit": {"author": {"name": "Jakub Jelinek", "email": "jakub@redhat.com", "date": "2011-10-12T22:05:58Z"}, "committer": {"name": "Jakub Jelinek", "email": "jakub@gcc.gnu.org", "date": "2011-10-12T22:05:58Z"}, "message": "i386.md (UNSPEC_VPERMDI): Remove.\n\n\t* config/i386/i386.md (UNSPEC_VPERMDI): Remove.\n\t* config/i386/i386.c (ix86_expand_vec_perm): Handle\n\tV16QImode and V32QImode for TARGET_AVX2.\n\t(MAX_VECT_LEN): Increase to 32.\n\t(expand_vec_perm_blend): Add support for 32-byte integer\n\tvectors with TARGET_AVX2.\n\t(valid_perm_using_mode_p): New function.\n\t(expand_vec_perm_pshufb): Add support for 32-byte integer\n\tvectors with TARGET_AVX2.\n\t(expand_vec_perm_vpshufb2_vpermq): New function.\n\t(expand_vec_perm_vpshufb2_vpermq_even_odd): New function.\n\t(expand_vec_perm_even_odd_1): Handle 32-byte integer vectors\n\twith TARGET_AVX2.\n\t(ix86_expand_vec_perm_builtin_1): Try expand_vec_perm_vpshufb2_vpermq\n\tand expand_vec_perm_vpshufb2_vpermq_even_odd.\n\t* config/i386/sse.md (VEC_EXTRACT_EVENODD_MODE): Add for TARGET_AVX2\n\t32-byte integer vector modes.\n\t(vec_pack_trunc_<mode>): Use VI248_AVX2 instead of VI248_128.\n\t(avx2_interleave_highv32qi, avx2_interleave_lowv32qi): Remove pasto.\n\t(avx2_pshufdv3, avx2_pshuflwv3, avx2_pshufhwv3): Generate\n\t4 new operands.\n\t(avx2_pshufd_1, avx2_pshuflw_1, avx2_pshufhw_1): Don't use\n\tmatch_dup, instead add 4 new operands and require they have\n\tright cross-lane values.\n\t(avx2_permv4di): Change into define_expand.\n\t(avx2_permv4di_1): New instruction.\n\t(avx2_permv2ti): Use nonimmediate_operand instead of register_operand\n\tfor \"xm\" constrained operand.\n\t(VEC_PERM_AVX2): Add V32QI and V16QI for TARGET_AVX2.\n\nFrom-SVN: r179870", "tree": {"sha": "3a7cdb5376be2fb5ba0b1e03a20946a4604b7612", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/3a7cdb5376be2fb5ba0b1e03a20946a4604b7612"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/0c7189ae2d51eccde9857cf66721debb9d5288d4", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0c7189ae2d51eccde9857cf66721debb9d5288d4", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0c7189ae2d51eccde9857cf66721debb9d5288d4", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0c7189ae2d51eccde9857cf66721debb9d5288d4/comments", "author": {"login": "jakubjelinek", "id": 9370665, "node_id": "MDQ6VXNlcjkzNzA2NjU=", "avatar_url": "https://avatars.githubusercontent.com/u/9370665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakubjelinek", "html_url": "https://github.com/jakubjelinek", "followers_url": "https://api.github.com/users/jakubjelinek/followers", "following_url": "https://api.github.com/users/jakubjelinek/following{/other_user}", "gists_url": "https://api.github.com/users/jakubjelinek/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakubjelinek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakubjelinek/subscriptions", "organizations_url": "https://api.github.com/users/jakubjelinek/orgs", "repos_url": "https://api.github.com/users/jakubjelinek/repos", "events_url": "https://api.github.com/users/jakubjelinek/events{/privacy}", "received_events_url": "https://api.github.com/users/jakubjelinek/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "9d901b0e8fbb741dc6ac72faa4be1a3dc6b9b599", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/9d901b0e8fbb741dc6ac72faa4be1a3dc6b9b599", "html_url": "https://github.com/Rust-GCC/gccrs/commit/9d901b0e8fbb741dc6ac72faa4be1a3dc6b9b599"}], "stats": {"total": 813, "additions": 701, "deletions": 112}, "files": [{"sha": "e774b5855da4cfb585e6e766e9fdadc8c3f771df", "filename": "gcc/ChangeLog", "status": "modified", "additions": 30, "deletions": 0, "changes": 30, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0c7189ae2d51eccde9857cf66721debb9d5288d4/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0c7189ae2d51eccde9857cf66721debb9d5288d4/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=0c7189ae2d51eccde9857cf66721debb9d5288d4", "patch": "@@ -1,5 +1,35 @@\n 2011-10-12  Jakub Jelinek  <jakub@redhat.com>\n \n+\t* config/i386/i386.md (UNSPEC_VPERMDI): Remove.\n+\t* config/i386/i386.c (ix86_expand_vec_perm): Handle\n+\tV16QImode and V32QImode for TARGET_AVX2.\n+\t(MAX_VECT_LEN): Increase to 32.\n+\t(expand_vec_perm_blend): Add support for 32-byte integer\n+\tvectors with TARGET_AVX2.\n+\t(valid_perm_using_mode_p): New function.\n+\t(expand_vec_perm_pshufb): Add support for 32-byte integer\n+\tvectors with TARGET_AVX2.\n+\t(expand_vec_perm_vpshufb2_vpermq): New function.\n+\t(expand_vec_perm_vpshufb2_vpermq_even_odd): New function.\n+\t(expand_vec_perm_even_odd_1): Handle 32-byte integer vectors\n+\twith TARGET_AVX2.\n+\t(ix86_expand_vec_perm_builtin_1): Try expand_vec_perm_vpshufb2_vpermq\n+\tand expand_vec_perm_vpshufb2_vpermq_even_odd.\n+\t* config/i386/sse.md (VEC_EXTRACT_EVENODD_MODE): Add for TARGET_AVX2\n+\t32-byte integer vector modes.\n+\t(vec_pack_trunc_<mode>): Use VI248_AVX2 instead of VI248_128.\n+\t(avx2_interleave_highv32qi, avx2_interleave_lowv32qi): Remove pasto.\n+\t(avx2_pshufdv3, avx2_pshuflwv3, avx2_pshufhwv3): Generate\n+\t4 new operands.\n+\t(avx2_pshufd_1, avx2_pshuflw_1, avx2_pshufhw_1): Don't use\n+\tmatch_dup, instead add 4 new operands and require they have\n+\tright cross-lane values.\n+\t(avx2_permv4di): Change into define_expand.\n+\t(avx2_permv4di_1): New instruction.\n+\t(avx2_permv2ti): Use nonimmediate_operand instead of register_operand\n+\tfor \"xm\" constrained operand.\n+\t(VEC_PERM_AVX2): Add V32QI and V16QI for TARGET_AVX2.\n+\n \t* config/i386/sse.md (avx2_gathersi<mode>,\n \tavx2_gatherdi<mode>, avx2_gatherdi<mode>256): Add clobber of\n \tmatch_scratch, change memory_operand to register_operand,"}, {"sha": "26a49241619253954f5485bc3a42f1cea5fc6a95", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 589, "deletions": 74, "changes": 663, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0c7189ae2d51eccde9857cf66721debb9d5288d4/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0c7189ae2d51eccde9857cf66721debb9d5288d4/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=0c7189ae2d51eccde9857cf66721debb9d5288d4", "patch": "@@ -19334,7 +19334,7 @@ ix86_expand_vec_perm (rtx operands[])\n   rtx op0 = operands[1];\n   rtx op1 = operands[2];\n   rtx mask = operands[3];\n-  rtx t1, t2, vt, vec[16];\n+  rtx t1, t2, t3, t4, vt, vt2, vec[32];\n   enum machine_mode mode = GET_MODE (op0);\n   enum machine_mode maskmode = GET_MODE (mask);\n   int w, e, i;\n@@ -19343,50 +19343,68 @@ ix86_expand_vec_perm (rtx operands[])\n   /* Number of elements in the vector.  */\n   w = GET_MODE_NUNITS (mode);\n   e = GET_MODE_UNIT_SIZE (mode);\n-  gcc_assert (w <= 16);\n+  gcc_assert (w <= 32);\n \n   if (TARGET_AVX2)\n     {\n-      if (mode == V4DImode || mode == V4DFmode)\n+      if (mode == V4DImode || mode == V4DFmode || mode == V16HImode)\n \t{\n \t  /* Unfortunately, the VPERMQ and VPERMPD instructions only support\n \t     an constant shuffle operand.  With a tiny bit of effort we can\n \t     use VPERMD instead.  A re-interpretation stall for V4DFmode is\n-\t     unfortunate but there's no avoiding it.  */\n-\t  t1 = gen_reg_rtx (V8SImode);\n+\t     unfortunate but there's no avoiding it.\n+\t     Similarly for V16HImode we don't have instructions for variable\n+\t     shuffling, while for V32QImode we can use after preparing suitable\n+\t     masks vpshufb; vpshufb; vpermq; vpor.  */\n+\n+\t  if (mode == V16HImode)\n+\t    {\n+\t      maskmode = mode = V32QImode;\n+\t      w = 32;\n+\t      e = 1;\n+\t    }\n+\t  else\n+\t    {\n+\t      maskmode = mode = V8SImode;\n+\t      w = 8;\n+\t      e = 4;\n+\t    }\n+\t  t1 = gen_reg_rtx (maskmode);\n \n \t  /* Replicate the low bits of the V4DImode mask into V8SImode:\n \t       mask = { A B C D }\n \t       t1 = { A A B B C C D D }.  */\n-\t  for (i = 0; i < 4; ++i)\n+\t  for (i = 0; i < w / 2; ++i)\n \t    vec[i*2 + 1] = vec[i*2] = GEN_INT (i * 2);\n-\t  vt = gen_rtx_CONST_VECTOR (V8SImode, gen_rtvec_v (8, vec));\n-\t  vt = force_reg (V8SImode, vt);\n-\t  mask = gen_lowpart (V8SImode, mask);\n-\t  emit_insn (gen_avx2_permvarv8si (t1, vt, mask));\n+\t  vt = gen_rtx_CONST_VECTOR (maskmode, gen_rtvec_v (w, vec));\n+\t  vt = force_reg (maskmode, vt);\n+\t  mask = gen_lowpart (maskmode, mask);\n+\t  if (maskmode == V8SImode)\n+\t    emit_insn (gen_avx2_permvarv8si (t1, vt, mask));\n+\t  else\n+\t    emit_insn (gen_avx2_pshufbv32qi3 (t1, mask, vt));\n \n \t  /* Multiply the shuffle indicies by two.  */\n-\t  emit_insn (gen_avx2_lshlv8si3 (t1, t1, const1_rtx));\n+\t  t1 = expand_simple_binop (maskmode, PLUS, t1, t1, t1, 1,\n+\t\t\t\t    OPTAB_DIRECT);\n \n \t  /* Add one to the odd shuffle indicies:\n \t\tt1 = { A*2, A*2+1, B*2, B*2+1, ... }.  */\n-\t  for (i = 0; i < 4; ++i)\n+\t  for (i = 0; i < w / 2; ++i)\n \t    {\n \t      vec[i * 2] = const0_rtx;\n \t      vec[i * 2 + 1] = const1_rtx;\n \t    }\n-\t  vt = gen_rtx_CONST_VECTOR (V8SImode, gen_rtvec_v (8, vec));\n-\t  vt = force_const_mem (V8SImode, vt);\n-\t  emit_insn (gen_addv8si3 (t1, t1, vt));\n+\t  vt = gen_rtx_CONST_VECTOR (maskmode, gen_rtvec_v (w, vec));\n+\t  vt = force_const_mem (maskmode, vt);\n+\t  t1 = expand_simple_binop (maskmode, PLUS, t1, vt, t1, 1,\n+\t\t\t\t    OPTAB_DIRECT);\n \n-\t  /* Continue as if V8SImode was used initially.  */\n+\t  /* Continue as if V8SImode (resp. V32QImode) was used initially.  */\n \t  operands[3] = mask = t1;\n-\t  target = gen_lowpart (V8SImode, target);\n-\t  op0 = gen_lowpart (V8SImode, op0);\n-\t  op1 = gen_lowpart (V8SImode, op1);\n-\t  maskmode = mode = V8SImode;\n-\t  w = 8;\n-\t  e = 4;\n+\t  target = gen_lowpart (mode, target);\n+\t  op0 = gen_lowpart (mode, op0);\n+\t  op1 = gen_lowpart (mode, op1);\n \t}\n \n       switch (mode)\n@@ -19443,6 +19461,92 @@ ix86_expand_vec_perm (rtx operands[])\n \t  emit_insn (gen_avx_vextractf128v8sf (target, t1, const0_rtx));\n \t  return;\n \n+\tcase V32QImode:\n+\t  t1 = gen_reg_rtx (V32QImode);\n+\t  t2 = gen_reg_rtx (V32QImode);\n+\t  t3 = gen_reg_rtx (V32QImode);\n+\t  vt2 = GEN_INT (128);\n+\t  for (i = 0; i < 32; i++)\n+\t    vec[i] = vt2;\n+\t  vt = gen_rtx_CONST_VECTOR (V32QImode, gen_rtvec_v (32, vec));\n+\t  vt = force_reg (V32QImode, vt);\n+\t  for (i = 0; i < 32; i++)\n+\t    vec[i] = i < 16 ? vt2 : const0_rtx;\n+\t  vt2 = gen_rtx_CONST_VECTOR (V32QImode, gen_rtvec_v (32, vec));\n+\t  vt2 = force_reg (V32QImode, vt2);\n+\t  /* From mask create two adjusted masks, which contain the same\n+\t     bits as mask in the low 7 bits of each vector element.\n+\t     The first mask will have the most significant bit clear\n+\t     if it requests element from the same 128-bit lane\n+\t     and MSB set if it requests element from the other 128-bit lane.\n+\t     The second mask will have the opposite values of the MSB,\n+\t     and additionally will have its 128-bit lanes swapped.\n+\t     E.g. { 07 12 1e 09 ... | 17 19 05 1f ... } mask vector will have\n+\t     t1   { 07 92 9e 09 ... | 17 19 85 1f ... } and\n+\t     t3   { 97 99 05 9f ... | 87 12 1e 89 ... } where each ...\n+\t     stands for other 12 bytes.  */\n+\t  /* The bit whether element is from the same lane or the other\n+\t     lane is bit 4, so shift it up by 3 to the MSB position.  */\n+\t  emit_insn (gen_avx2_lshlv4di3 (gen_lowpart (V4DImode, t1),\n+\t\t\t\t\t gen_lowpart (V4DImode, mask),\n+\t\t\t\t\t GEN_INT (3)));\n+\t  /* Clear MSB bits from the mask just in case it had them set.  */\n+\t  emit_insn (gen_avx2_andnotv32qi3 (t2, vt, mask));\n+\t  /* After this t1 will have MSB set for elements from other lane.  */\n+\t  emit_insn (gen_xorv32qi3 (t1, t1, vt2));\n+\t  /* Clear bits other than MSB.  */\n+\t  emit_insn (gen_andv32qi3 (t1, t1, vt));\n+\t  /* Or in the lower bits from mask into t3.  */\n+\t  emit_insn (gen_iorv32qi3 (t3, t1, t2));\n+\t  /* And invert MSB bits in t1, so MSB is set for elements from the same\n+\t     lane.  */\n+\t  emit_insn (gen_xorv32qi3 (t1, t1, vt));\n+\t  /* Swap 128-bit lanes in t3.  */\n+\t  emit_insn (gen_avx2_permv4di_1 (gen_lowpart (V4DImode, t3),\n+\t\t\t\t\t  gen_lowpart (V4DImode, t3),\n+\t\t\t\t\t  const2_rtx, GEN_INT (3),\n+\t\t\t\t\t  const0_rtx, const1_rtx));\n+\t  /* And or in the lower bits from mask into t1.  */\n+\t  emit_insn (gen_iorv32qi3 (t1, t1, t2));\n+\t  if (one_operand_shuffle)\n+\t    {\n+\t      /* Each of these shuffles will put 0s in places where\n+\t\t element from the other 128-bit lane is needed, otherwise\n+\t\t will shuffle in the requested value.  */\n+\t      emit_insn (gen_avx2_pshufbv32qi3 (t3, op0, t3));\n+\t      emit_insn (gen_avx2_pshufbv32qi3 (t1, op0, t1));\n+\t      /* For t3 the 128-bit lanes are swapped again.  */\n+\t      emit_insn (gen_avx2_permv4di_1 (gen_lowpart (V4DImode, t3),\n+\t\t\t\t\t      gen_lowpart (V4DImode, t3),\n+\t\t\t\t\t      const2_rtx, GEN_INT (3),\n+\t\t\t\t\t      const0_rtx, const1_rtx));\n+\t      /* And oring both together leads to the result.  */\n+\t      emit_insn (gen_iorv32qi3 (target, t1, t3));\n+\t      return;\n+\t    }\n+\n+\t  t4 = gen_reg_rtx (V32QImode);\n+\t  /* Similarly to the above one_operand_shuffle code,\n+\t     just for repeated twice for each operand.  merge_two:\n+\t     code will merge the two results together.  */\n+\t  emit_insn (gen_avx2_pshufbv32qi3 (t4, op0, t3));\n+\t  emit_insn (gen_avx2_pshufbv32qi3 (t3, op1, t3));\n+\t  emit_insn (gen_avx2_pshufbv32qi3 (t2, op0, t1));\n+\t  emit_insn (gen_avx2_pshufbv32qi3 (t1, op1, t1));\n+\t  emit_insn (gen_avx2_permv4di_1 (gen_lowpart (V4DImode, t4),\n+\t\t\t\t\t  gen_lowpart (V4DImode, t4),\n+\t\t\t\t\t  const2_rtx, GEN_INT (3),\n+\t\t\t\t\t  const0_rtx, const1_rtx));\n+\t  emit_insn (gen_avx2_permv4di_1 (gen_lowpart (V4DImode, t3),\n+\t\t\t\t\t  gen_lowpart (V4DImode, t3),\n+\t\t\t\t\t  const2_rtx, GEN_INT (3),\n+\t\t\t\t\t  const0_rtx, const1_rtx));\n+\t  emit_insn (gen_iorv32qi3 (t4, t2, t4));\n+\t  emit_insn (gen_iorv32qi3 (t3, t1, t3));\n+\t  t1 = t4;\n+\t  t2 = t3;\n+\t  goto merge_two;\n+\n \tdefault:\n \t  gcc_assert (GET_MODE_SIZE (mode) <= 16);\n \t  break;\n@@ -31773,9 +31877,9 @@ x86_emit_floatuns (rtx operands[2])\n   emit_label (donelab);\n }\n \f\n-/* AVX does not support 32-byte integer vector operations,\n-   thus the longest vector we are faced with is V16QImode.  */\n-#define MAX_VECT_LEN\t16\n+/* AVX2 does support 32-byte integer vector operations,\n+   thus the longest vector we are faced with is V32QImode.  */\n+#define MAX_VECT_LEN\t32\n \n struct expand_vec_perm_d\n {\n@@ -34582,18 +34686,25 @@ expand_vselect_vconcat (rtx target, rtx op0, rtx op1,\n }\n \n /* A subroutine of ix86_expand_vec_perm_builtin_1.  Try to implement D\n-   in terms of blendp[sd] / pblendw / pblendvb.  */\n+   in terms of blendp[sd] / pblendw / pblendvb / vpblendd.  */\n \n static bool\n expand_vec_perm_blend (struct expand_vec_perm_d *d)\n {\n   enum machine_mode vmode = d->vmode;\n   unsigned i, mask, nelt = d->nelt;\n   rtx target, op0, op1, x;\n+  rtx rperm[32], vperm;\n \n-  if (!TARGET_SSE4_1 || d->op0 == d->op1)\n+  if (d->op0 == d->op1)\n     return false;\n-  if (!(GET_MODE_SIZE (vmode) == 16 || vmode == V4DFmode || vmode == V8SFmode))\n+  if (TARGET_AVX2 && GET_MODE_SIZE (vmode) == 32)\n+    ;\n+  else if (TARGET_AVX && (vmode == V4DFmode || vmode == V8SFmode))\n+    ;\n+  else if (TARGET_SSE4_1 && GET_MODE_SIZE (vmode) == 16)\n+    ;\n+  else\n     return false;\n \n   /* This is a blend, not a permute.  Elements must stay in their\n@@ -34611,30 +34722,6 @@ expand_vec_perm_blend (struct expand_vec_perm_d *d)\n   /* ??? Without SSE4.1, we could implement this with and/andn/or.  This\n      decision should be extracted elsewhere, so that we only try that\n      sequence once all budget==3 options have been tried.  */\n-\n-  /* For bytes, see if bytes move in pairs so we can use pblendw with\n-     an immediate argument, rather than pblendvb with a vector argument.  */\n-  if (vmode == V16QImode)\n-    {\n-      bool pblendw_ok = true;\n-      for (i = 0; i < 16 && pblendw_ok; i += 2)\n-\tpblendw_ok = (d->perm[i] + 1 == d->perm[i + 1]);\n-\n-      if (!pblendw_ok)\n-\t{\n-\t  rtx rperm[16], vperm;\n-\n-\t  for (i = 0; i < nelt; ++i)\n-\t    rperm[i] = (d->perm[i] < nelt ? const0_rtx : constm1_rtx);\n-\n-\t  vperm = gen_rtx_CONST_VECTOR (V16QImode, gen_rtvec_v (16, rperm));\n-\t  vperm = force_reg (V16QImode, vperm);\n-\n-\t  emit_insn (gen_sse4_1_pblendvb (d->target, d->op0, d->op1, vperm));\n-\t  return true;\n-\t}\n-    }\n-\n   target = d->target;\n   op0 = d->op0;\n   op1 = d->op1;\n@@ -34647,31 +34734,130 @@ expand_vec_perm_blend (struct expand_vec_perm_d *d)\n     case V2DFmode:\n     case V4SFmode:\n     case V8HImode:\n+    case V8SImode:\n       for (i = 0; i < nelt; ++i)\n \tmask |= (d->perm[i] >= nelt) << i;\n       break;\n \n     case V2DImode:\n       for (i = 0; i < 2; ++i)\n \tmask |= (d->perm[i] >= 2 ? 15 : 0) << (i * 4);\n+      vmode = V8HImode;\n       goto do_subreg;\n \n     case V4SImode:\n       for (i = 0; i < 4; ++i)\n \tmask |= (d->perm[i] >= 4 ? 3 : 0) << (i * 2);\n+      vmode = V8HImode;\n       goto do_subreg;\n \n     case V16QImode:\n+      /* See if bytes move in pairs so we can use pblendw with\n+\t an immediate argument, rather than pblendvb with a vector\n+\t argument.  */\n+      for (i = 0; i < 16; i += 2)\n+\tif (d->perm[i] + 1 != d->perm[i + 1])\n+\t  {\n+\t  use_pblendvb:\n+\t    for (i = 0; i < nelt; ++i)\n+\t      rperm[i] = (d->perm[i] < nelt ? const0_rtx : constm1_rtx);\n+\n+\t  finish_pblendvb:\n+\t    vperm = gen_rtx_CONST_VECTOR (vmode, gen_rtvec_v (nelt, rperm));\n+\t    vperm = force_reg (vmode, vperm);\n+\n+\t    if (GET_MODE_SIZE (vmode) == 16)\n+\t      emit_insn (gen_sse4_1_pblendvb (target, op0, op1, vperm));\n+\t    else\n+\t      emit_insn (gen_avx2_pblendvb (target, op0, op1, vperm));\n+\t    return true;\n+\t  }\n+\n       for (i = 0; i < 8; ++i)\n \tmask |= (d->perm[i * 2] >= 16) << i;\n+      vmode = V8HImode;\n+      /* FALLTHRU */\n \n     do_subreg:\n-      vmode = V8HImode;\n       target = gen_lowpart (vmode, target);\n       op0 = gen_lowpart (vmode, op0);\n       op1 = gen_lowpart (vmode, op1);\n       break;\n \n+    case V32QImode:\n+      /* See if bytes move in pairs.  If not, vpblendvb must be used.  */\n+      for (i = 0; i < 32; i += 2)\n+\tif (d->perm[i] + 1 != d->perm[i + 1])\n+\t  goto use_pblendvb;\n+      /* See if bytes move in quadruplets.  If yes, vpblendd\n+\t with immediate can be used.  */\n+      for (i = 0; i < 32; i += 4)\n+\tif (d->perm[i] + 2 != d->perm[i + 2])\n+\t  break;\n+      if (i < 32)\n+\t{\n+\t  /* See if bytes move the same in both lanes.  If yes,\n+\t     vpblendw with immediate can be used.  */\n+\t  for (i = 0; i < 16; i += 2)\n+\t    if (d->perm[i] + 16 != d->perm[i + 16])\n+\t      goto use_pblendvb;\n+\n+\t  /* Use vpblendw.  */\n+\t  for (i = 0; i < 16; ++i)\n+\t    mask |= (d->perm[i * 2] >= 32) << i;\n+\t  vmode = V16HImode;\n+\t  goto do_subreg;\n+\t}\n+\n+      /* Use vpblendd.  */\n+      for (i = 0; i < 8; ++i)\n+\tmask |= (d->perm[i * 4] >= 32) << i;\n+      vmode = V8SImode;\n+      goto do_subreg;\n+\n+    case V16HImode:\n+      /* See if words move in pairs.  If yes, vpblendd can be used.  */\n+      for (i = 0; i < 16; i += 2)\n+\tif (d->perm[i] + 1 != d->perm[i + 1])\n+\t  break;\n+      if (i < 16)\n+\t{\n+\t  /* See if words move the same in both lanes.  If not,\n+\t     vpblendvb must be used.  */\n+\t  for (i = 0; i < 8; i++)\n+\t    if (d->perm[i] + 8 != d->perm[i + 8])\n+\t      {\n+\t\t/* Use vpblendvb.  */\n+\t\tfor (i = 0; i < 32; ++i)\n+\t\t  rperm[i] = (d->perm[i / 2] < 16 ? const0_rtx : constm1_rtx);\n+\n+\t\tvmode = V32QImode;\n+\t\tnelt = 32;\n+\t\ttarget = gen_lowpart (vmode, target);\n+\t\top0 = gen_lowpart (vmode, op0);\n+\t\top1 = gen_lowpart (vmode, op1);\n+\t\tgoto finish_pblendvb;\n+\t      }\n+\n+\t  /* Use vpblendw.  */\n+\t  for (i = 0; i < 16; ++i)\n+\t    mask |= (d->perm[i] >= 16) << i;\n+\t  break;\n+\t}\n+\n+      /* Use vpblendd.  */\n+      for (i = 0; i < 8; ++i)\n+\tmask |= (d->perm[i * 2] >= 16) << i;\n+      vmode = V8SImode;\n+      goto do_subreg;\n+\n+    case V4DImode:\n+      /* Use vpblendd.  */\n+      for (i = 0; i < 4; ++i)\n+\tmask |= (d->perm[i] >= 4 ? 3 : 0) << (i * 2);\n+      vmode = V8SImode;\n+      goto do_subreg;\n+\n     default:\n       gcc_unreachable ();\n     }\n@@ -34732,43 +34918,164 @@ expand_vec_perm_vpermil (struct expand_vec_perm_d *d)\n   return true;\n }\n \n-/* A subroutine of ix86_expand_vec_perm_builtin_1.  Try to implement D\n-   in terms of pshufb or vpperm.  */\n+/* Return true if permutation D can be performed as VMODE permutation\n+   instead.  */\n \n static bool\n-expand_vec_perm_pshufb (struct expand_vec_perm_d *d)\n+valid_perm_using_mode_p (enum machine_mode vmode, struct expand_vec_perm_d *d)\n {\n-  unsigned i, nelt, eltsz;\n-  rtx rperm[16], vperm, target, op0, op1;\n+  unsigned int i, j, chunk;\n \n-  if (!(d->op0 == d->op1 ? TARGET_SSSE3 : TARGET_XOP))\n-    return false;\n-  if (GET_MODE_SIZE (d->vmode) != 16)\n+  if (GET_MODE_CLASS (vmode) != MODE_VECTOR_INT\n+      || GET_MODE_CLASS (d->vmode) != MODE_VECTOR_INT\n+      || GET_MODE_SIZE (vmode) != GET_MODE_SIZE (d->vmode))\n     return false;\n \n-  if (d->testing_p)\n+  if (GET_MODE_NUNITS (vmode) >= d->nelt)\n     return true;\n \n+  chunk = d->nelt / GET_MODE_NUNITS (vmode);\n+  for (i = 0; i < d->nelt; i += chunk)\n+    if (d->perm[i] & (chunk - 1))\n+      return false;\n+    else\n+      for (j = 1; j < chunk; ++j)\n+\tif ((d->perm[i] & (d->nelt - 1)) + j\n+\t    != (d->perm[i + j] & (d->nelt - 1)))\n+\t  return false;\n+\n+  return true;\n+}\n+\n+/* A subroutine of ix86_expand_vec_perm_builtin_1.  Try to implement D\n+   in terms of pshufb, vpperm, vpermq, vpermd or vperm2i128.  */\n+\n+static bool\n+expand_vec_perm_pshufb (struct expand_vec_perm_d *d)\n+{\n+  unsigned i, nelt, eltsz, mask;\n+  unsigned char perm[32];\n+  enum machine_mode vmode = V16QImode;\n+  rtx rperm[32], vperm, target, op0, op1;\n+\n   nelt = d->nelt;\n-  eltsz = GET_MODE_SIZE (GET_MODE_INNER (d->vmode));\n \n-  for (i = 0; i < nelt; ++i)\n+  if (d->op0 != d->op1)\n     {\n-      unsigned j, e = d->perm[i];\n-      for (j = 0; j < eltsz; ++j)\n-\trperm[i * eltsz + j] = GEN_INT (e * eltsz + j);\n+      if (!TARGET_XOP || GET_MODE_SIZE (d->vmode) != 16)\n+\t{\n+\t  if (TARGET_AVX2\n+\t      && valid_perm_using_mode_p (V2TImode, d))\n+\t    {\n+\t      if (d->testing_p)\n+\t\treturn true;\n+\n+\t      /* Use vperm2i128 insn.  The pattern uses\n+\t\t V4DImode instead of V2TImode.  */\n+\t      target = gen_lowpart (V4DImode, d->target);\n+\t      op0 = gen_lowpart (V4DImode, d->op0);\n+\t      op1 = gen_lowpart (V4DImode, d->op1);\n+\t      rperm[0]\n+\t\t= GEN_INT (((d->perm[0] & (nelt / 2)) ? 1 : 0)\n+\t\t\t   || ((d->perm[nelt / 2] & (nelt / 2)) ? 2 : 0));\n+\t      emit_insn (gen_avx2_permv2ti (target, op0, op1, rperm[0]));\n+\t      return true;\n+\t    }\n+\t  return false;\n+\t}\n     }\n+  else\n+    {\n+      if (GET_MODE_SIZE (d->vmode) == 16)\n+\t{\n+\t  if (!TARGET_SSSE3)\n+\t    return false;\n+\t}\n+      else if (GET_MODE_SIZE (d->vmode) == 32)\n+\t{\n+\t  if (!TARGET_AVX2)\n+\t    return false;\n \n-  vperm = gen_rtx_CONST_VECTOR (V16QImode, gen_rtvec_v (16, rperm));\n-  vperm = force_reg (V16QImode, vperm);\n+\t  /* V4DImode should be already handled through\n+\t     expand_vselect by vpermq instruction.  */\n+\t  gcc_assert (d->vmode != V4DImode);\n \n-  target = gen_lowpart (V16QImode, d->target);\n-  op0 = gen_lowpart (V16QImode, d->op0);\n+\t  vmode = V32QImode;\n+\t  if (d->vmode == V8SImode\n+\t      || d->vmode == V16HImode\n+\t      || d->vmode == V32QImode)\n+\t    {\n+\t      /* First see if vpermq can be used for\n+\t\t V8SImode/V16HImode/V32QImode.  */\n+\t      if (valid_perm_using_mode_p (V4DImode, d))\n+\t\t{\n+\t\t  for (i = 0; i < 4; i++)\n+\t\t    perm[i] = (d->perm[i * nelt / 4] * 4 / nelt) & 3;\n+\t\t  if (d->testing_p)\n+\t\t    return true;\n+\t\t  return expand_vselect (gen_lowpart (V4DImode, d->target),\n+\t\t\t\t\t gen_lowpart (V4DImode, d->op0),\n+\t\t\t\t\t perm, 4);\n+\t\t}\n+\n+\t      /* Next see if vpermd can be used.  */\n+\t      if (valid_perm_using_mode_p (V8SImode, d))\n+\t\tvmode = V8SImode;\n+\t    }\n+\n+\t  if (vmode == V32QImode)\n+\t    {\n+\t      /* vpshufb only works intra lanes, it is not\n+\t\t possible to shuffle bytes in between the lanes.  */\n+\t      for (i = 0; i < nelt; ++i)\n+\t\tif ((d->perm[i] ^ i) & (nelt / 2))\n+\t\t  return false;\n+\t    }\n+\t}\n+      else\n+\treturn false;\n+    }\n+\n+  if (d->testing_p)\n+    return true;\n+\n+  if (vmode == V8SImode)\n+    for (i = 0; i < 8; ++i)\n+      rperm[i] = GEN_INT ((d->perm[i * nelt / 8] * 8 / nelt) & 7);\n+  else\n+    {\n+      eltsz = GET_MODE_SIZE (GET_MODE_INNER (d->vmode));\n+      if (d->op0 != d->op1)\n+\tmask = 2 * nelt - 1;\n+      else if (vmode == V16QImode)\n+\tmask = nelt - 1;\n+      else\n+\tmask = nelt / 2 - 1;\n+\n+      for (i = 0; i < nelt; ++i)\n+\t{\n+\t  unsigned j, e = d->perm[i] & mask;\n+\t  for (j = 0; j < eltsz; ++j)\n+\t    rperm[i * eltsz + j] = GEN_INT (e * eltsz + j);\n+\t}\n+    }\n+\n+  vperm = gen_rtx_CONST_VECTOR (vmode,\n+\t\t\t\tgen_rtvec_v (GET_MODE_NUNITS (vmode), rperm));\n+  vperm = force_reg (vmode, vperm);\n+\n+  target = gen_lowpart (vmode, d->target);\n+  op0 = gen_lowpart (vmode, d->op0);\n   if (d->op0 == d->op1)\n-    emit_insn (gen_ssse3_pshufbv16qi3 (target, op0, vperm));\n+    {\n+      if (vmode == V16QImode)\n+\temit_insn (gen_ssse3_pshufbv16qi3 (target, op0, vperm));\n+      else if (vmode == V32QImode)\n+\temit_insn (gen_avx2_pshufbv32qi3 (target, op0, vperm));\n+    }\n   else\n     {\n-      op1 = gen_lowpart (V16QImode, d->op1);\n+      op1 = gen_lowpart (vmode, d->op1);\n       emit_insn (gen_xop_pperm (target, op0, op1, vperm));\n     }\n \n@@ -34856,7 +35163,8 @@ expand_vec_perm_1 (struct expand_vec_perm_d *d)\n   if (expand_vec_perm_vpermil (d))\n     return true;\n \n-  /* Try the SSSE3 pshufb or XOP vpperm variable permutation.  */\n+  /* Try the SSSE3 pshufb or XOP vpperm or AVX2 vperm2i128,\n+     vpshufb, vpermd or vpermq variable permutation.  */\n   if (expand_vec_perm_pshufb (d))\n     return true;\n \n@@ -35156,6 +35464,150 @@ expand_vec_perm_pshufb2 (struct expand_vec_perm_d *d)\n   return true;\n }\n \n+/* Implement arbitrary permutation of one V32QImode and V16QImode operand\n+   with two vpshufb insns, vpermq and vpor.  We should have already failed\n+   all two or three instruction sequences.  */\n+\n+static bool\n+expand_vec_perm_vpshufb2_vpermq (struct expand_vec_perm_d *d)\n+{\n+  rtx rperm[2][32], vperm, l, h, hp, op, m128;\n+  unsigned int i, nelt, eltsz;\n+\n+  if (!TARGET_AVX2\n+      || d->op0 != d->op1\n+      || (d->vmode != V32QImode && d->vmode != V16HImode))\n+    return false;\n+\n+  nelt = d->nelt;\n+  eltsz = GET_MODE_SIZE (GET_MODE_INNER (d->vmode));\n+\n+  /* Generate two permutation masks.  If the required element is within\n+     the same lane, it is shuffled in.  If the required element from the\n+     other lane, force a zero by setting bit 7 in the permutation mask.\n+     In the other mask the mask has non-negative elements if element\n+     is requested from the other lane, but also moved to the other lane,\n+     so that the result of vpshufb can have the two V2TImode halves\n+     swapped.  */\n+  m128 = GEN_INT (-128);\n+  for (i = 0; i < nelt; ++i)\n+    {\n+      unsigned j, e = d->perm[i] & (nelt / 2 - 1);\n+      unsigned which = ((d->perm[i] ^ i) & (nelt / 2));\n+\n+      for (j = 0; j < eltsz; ++j)\n+\t{\n+\t  rperm[!!which][(i * eltsz + j) ^ which] = GEN_INT (e * eltsz + j);\n+\t  rperm[!which][(i * eltsz + j) ^ (which ^ (nelt / 2))] = m128;\n+\t}\n+    }\n+\n+  vperm = gen_rtx_CONST_VECTOR (V32QImode, gen_rtvec_v (32, rperm[1]));\n+  vperm = force_reg (V32QImode, vperm);\n+\n+  h = gen_reg_rtx (V32QImode);\n+  op = gen_lowpart (V32QImode, d->op0);\n+  emit_insn (gen_avx2_pshufbv32qi3 (h, op, vperm));\n+\n+  /* Swap the 128-byte lanes of h into hp.  */\n+  hp = gen_reg_rtx (V32QImode);\n+  op = gen_lowpart (V4DImode, h);\n+  emit_insn (gen_avx2_permv4di_1 (gen_lowpart (V4DImode, hp), op,\n+\t\t\t\t  const2_rtx, GEN_INT (3), const0_rtx,\n+\t\t\t\t  const1_rtx));\n+\n+  vperm = gen_rtx_CONST_VECTOR (V32QImode, gen_rtvec_v (32, rperm[0]));\n+  vperm = force_reg (V32QImode, vperm);\n+\n+  l = gen_reg_rtx (V32QImode);\n+  op = gen_lowpart (V32QImode, d->op0);\n+  emit_insn (gen_avx2_pshufbv32qi3 (l, op, vperm));\n+\n+  op = gen_lowpart (V32QImode, d->target);\n+  emit_insn (gen_iorv32qi3 (op, l, hp));\n+\n+  return true;\n+}\n+\n+/* A subroutine of expand_vec_perm_even_odd_1.  Implement extract-even\n+   and extract-odd permutations of two V32QImode and V16QImode operand\n+   with two vpshufb insns, vpor and vpermq.  We should have already\n+   failed all two or three instruction sequences.  */\n+\n+static bool\n+expand_vec_perm_vpshufb2_vpermq_even_odd (struct expand_vec_perm_d *d)\n+{\n+  rtx rperm[2][32], vperm, l, h, ior, op, m128;\n+  unsigned int i, nelt, eltsz;\n+\n+  if (!TARGET_AVX2\n+      || d->op0 == d->op1\n+      || (d->vmode != V32QImode && d->vmode != V16HImode))\n+    return false;\n+\n+  for (i = 0; i < d->nelt; ++i)\n+    if ((d->perm[i] ^ (i * 2)) & (3 * d->nelt / 2))\n+      return false;\n+\n+  if (d->testing_p)\n+    return true;\n+\n+  nelt = d->nelt;\n+  eltsz = GET_MODE_SIZE (GET_MODE_INNER (d->vmode));\n+\n+  /* Generate two permutation masks.  In the first permutation mask\n+     the first quarter will contain indexes for the first half\n+     of the op0, the second quarter will contain bit 7 set, third quarter\n+     will contain indexes for the second half of the op0 and the\n+     last quarter bit 7 set.  In the second permutation mask\n+     the first quarter will contain bit 7 set, the second quarter\n+     indexes for the first half of the op1, the third quarter bit 7 set\n+     and last quarter indexes for the second half of the op1.\n+     I.e. the first mask e.g. for V32QImode extract even will be:\n+     0, 2, ..., 0xe, -128, ..., -128, 0, 2, ..., 0xe, -128, ..., -128\n+     (all values masked with 0xf except for -128) and second mask\n+     for extract even will be\n+     -128, ..., -128, 0, 2, ..., 0xe, -128, ..., -128, 0, 2, ..., 0xe.  */\n+  m128 = GEN_INT (-128);\n+  for (i = 0; i < nelt; ++i)\n+    {\n+      unsigned j, e = d->perm[i] & (nelt / 2 - 1);\n+      unsigned which = d->perm[i] >= nelt;\n+      unsigned xorv = (i >= nelt / 4 && i < 3 * nelt / 4) ? 24 : 0;\n+\n+      for (j = 0; j < eltsz; ++j)\n+\t{\n+\t  rperm[which][(i * eltsz + j) ^ xorv] = GEN_INT (e * eltsz + j);\n+\t  rperm[1 - which][(i * eltsz + j) ^ xorv] = m128;\n+\t}\n+    }\n+\n+  vperm = gen_rtx_CONST_VECTOR (V32QImode, gen_rtvec_v (32, rperm[0]));\n+  vperm = force_reg (V32QImode, vperm);\n+\n+  l = gen_reg_rtx (V32QImode);\n+  op = gen_lowpart (V32QImode, d->op0);\n+  emit_insn (gen_avx2_pshufbv32qi3 (l, op, vperm));\n+\n+  vperm = gen_rtx_CONST_VECTOR (V32QImode, gen_rtvec_v (32, rperm[1]));\n+  vperm = force_reg (V32QImode, vperm);\n+\n+  h = gen_reg_rtx (V32QImode);\n+  op = gen_lowpart (V32QImode, d->op0);\n+  emit_insn (gen_avx2_pshufbv32qi3 (h, op, vperm));\n+\n+  ior = gen_reg_rtx (V32QImode);\n+  emit_insn (gen_iorv32qi3 (ior, l, h));\n+\n+  /* Permute the V4DImode quarters using { 0, 2, 1, 3 } permutation.  */\n+  op = gen_lowpart (V4DImode, d->target);\n+  ior = gen_lowpart (V4DImode, ior);\n+  emit_insn (gen_avx2_permv4di_1 (op, ior, const0_rtx, const2_rtx,\n+\t\t\t\t  const1_rtx, GEN_INT (3)));\n+\n+  return true;\n+}\n+\n /* A subroutine of ix86_expand_vec_perm_builtin_1.  Implement extract-even\n    and extract-odd permutations.  */\n \n@@ -35265,6 +35717,61 @@ expand_vec_perm_even_odd_1 (struct expand_vec_perm_d *d, unsigned odd)\n \t}\n       break;\n \n+    case V16HImode:\n+    case V32QImode:\n+      return expand_vec_perm_vpshufb2_vpermq_even_odd (d);\n+\n+    case V4DImode:\n+      t1 = gen_reg_rtx (V4DImode);\n+      t2 = gen_reg_rtx (V4DImode);\n+\n+      /* Shuffle the lanes around into { 0 1 4 5 } and { 2 3 6 7 }.  */\n+      emit_insn (gen_avx2_permv2ti (t1, d->op0, d->op1, GEN_INT (0x20)));\n+      emit_insn (gen_avx2_permv2ti (t2, d->op0, d->op1, GEN_INT (0x31)));\n+\n+      /* Now an vpunpck[lh]qdq will produce the result required.  */\n+      if (odd)\n+\tt3 = gen_avx2_interleave_highv4di (d->target, t1, t2);\n+      else\n+\tt3 = gen_avx2_interleave_lowv4di (d->target, t1, t2);\n+      emit_insn (t3);\n+      break;\n+\n+    case V8SImode:\n+      t1 = gen_reg_rtx (V8SImode);\n+      t2 = gen_reg_rtx (V8SImode);\n+\n+      /* Shuffle the lanes around into\n+\t { 0 1 2 3 8 9 a b } and { 4 5 6 7 c d e f }.  */\n+      emit_insn (gen_avx2_permv2ti (gen_lowpart (V4DImode, t1),\n+\t\t\t\t    gen_lowpart (V4DImode, d->op0),\n+\t\t\t\t    gen_lowpart (V4DImode, d->op1),\n+\t\t\t\t    GEN_INT (0x20)));\n+      emit_insn (gen_avx2_permv2ti (gen_lowpart (V4DImode, t2),\n+\t\t\t\t    gen_lowpart (V4DImode, d->op0),\n+\t\t\t\t    gen_lowpart (V4DImode, d->op1),\n+\t\t\t\t    GEN_INT (0x31)));\n+\n+      /* Swap the 2nd and 3rd position in each lane into\n+\t { 0 2 1 3 8 a 9 b } and { 4 6 5 7 c e d f }.  */\n+      emit_insn (gen_avx2_pshufdv3 (t1, t1,\n+\t\t\t\t    GEN_INT (2 * 2 + 1 * 16 + 3 * 64)));\n+      emit_insn (gen_avx2_pshufdv3 (t2, t2,\n+\t\t\t\t    GEN_INT (2 * 2 + 1 * 16 + 3 * 64)));\n+\n+      /* Now an vpunpck[lh]qdq will produce\n+\t { 0 2 4 6 8 a c e } resp. { 1 3 5 7 9 b d f }.  */\n+      if (odd)\n+\tt3 = gen_avx2_interleave_highv4di (gen_lowpart (V4DImode, d->target),\n+\t\t\t\t\t   gen_lowpart (V4DImode, t1),\n+\t\t\t\t\t   gen_lowpart (V4DImode, t2));\n+      else\n+\tt3 = gen_avx2_interleave_lowv4di (gen_lowpart (V4DImode, d->target),\n+\t\t\t\t\t  gen_lowpart (V4DImode, t1),\n+\t\t\t\t\t  gen_lowpart (V4DImode, t2));\n+      emit_insn (t3);\n+      break;\n+\n     default:\n       gcc_unreachable ();\n     }\n@@ -35399,6 +35906,14 @@ ix86_expand_vec_perm_builtin_1 (struct expand_vec_perm_d *d)\n   if (expand_vec_perm_pshufb2 (d))\n     return true;\n \n+  /* Try sequences of four instructions.  */\n+\n+  if (expand_vec_perm_vpshufb2_vpermq (d))\n+    return true;\n+\n+  if (expand_vec_perm_vpshufb2_vpermq_even_odd (d))\n+    return true;\n+\n   /* ??? Look for narrow permutations whose element orderings would\n      allow the promotion to a wider mode.  */\n "}, {"sha": "9c9508d278a22dffac78fb29b9a11b0cf876af30", "filename": "gcc/config/i386/i386.md", "status": "modified", "additions": 0, "deletions": 1, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0c7189ae2d51eccde9857cf66721debb9d5288d4/gcc%2Fconfig%2Fi386%2Fi386.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0c7189ae2d51eccde9857cf66721debb9d5288d4/gcc%2Fconfig%2Fi386%2Fi386.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.md?ref=0c7189ae2d51eccde9857cf66721debb9d5288d4", "patch": "@@ -235,7 +235,6 @@\n   UNSPEC_VPERMSI\n   UNSPEC_VPERMDF\n   UNSPEC_VPERMSF\n-  UNSPEC_VPERMDI\n   UNSPEC_VPERMTI\n   UNSPEC_GATHER\n "}, {"sha": "12331516cd6a92bec01069b4f5062b9a9924f8ac", "filename": "gcc/config/i386/sse.md", "status": "modified", "additions": 82, "deletions": 37, "changes": 119, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0c7189ae2d51eccde9857cf66721debb9d5288d4/gcc%2Fconfig%2Fi386%2Fsse.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0c7189ae2d51eccde9857cf66721debb9d5288d4/gcc%2Fconfig%2Fi386%2Fsse.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fsse.md?ref=0c7189ae2d51eccde9857cf66721debb9d5288d4", "patch": "@@ -4330,10 +4330,10 @@\n \n ;; Modes handled by vec_extract_even/odd pattern.\n (define_mode_iterator VEC_EXTRACT_EVENODD_MODE\n-  [(V16QI \"TARGET_SSE2\")\n-   (V8HI \"TARGET_SSE2\")\n-   (V4SI \"TARGET_SSE2\")\n-   (V2DI \"TARGET_SSE2\")\n+  [(V32QI \"TARGET_AVX2\") (V16QI \"TARGET_SSE2\")\n+   (V16HI \"TARGET_AVX2\") (V8HI \"TARGET_SSE2\")\n+   (V8SI \"TARGET_AVX2\") (V4SI \"TARGET_SSE2\")\n+   (V4DI \"TARGET_AVX2\") (V2DI \"TARGET_SSE2\")\n    (V8SF \"TARGET_AVX\") V4SF\n    (V4DF \"TARGET_AVX\") (V2DF \"TARGET_SSE2\")])\n \n@@ -6196,11 +6196,9 @@\n   DONE;\n })\n \n-;; ??? Irritatingly, the 256-bit VPSHUFB only shuffles within the 128-bit\n-;; lanes.  For now, we don't try to support V32QI or V16HImode.  So we\n-;; don't want to use VI_AVX2.\n (define_mode_iterator VEC_PERM_AVX2\n   [V16QI V8HI V4SI V2DI V4SF V2DF\n+   (V32QI \"TARGET_AVX2\") (V16HI \"TARGET_AVX2\")\n    (V8SI \"TARGET_AVX2\") (V4DI \"TARGET_AVX2\")\n    (V8SF \"TARGET_AVX2\") (V4DF \"TARGET_AVX2\")])\n \n@@ -6431,8 +6429,8 @@\n \n (define_expand \"vec_pack_trunc_<mode>\"\n   [(match_operand:<ssepackmode> 0 \"register_operand\" \"\")\n-   (match_operand:VI248_128 1 \"register_operand\" \"\")\n-   (match_operand:VI248_128 2 \"register_operand\" \"\")]\n+   (match_operand:VI248_AVX2 1 \"register_operand\" \"\")\n+   (match_operand:VI248_AVX2 2 \"register_operand\" \"\")]\n   \"TARGET_SSE2\"\n {\n   rtx op1 = gen_lowpart (<ssepackmode>mode, operands[1]);\n@@ -6513,8 +6511,7 @@\n \t\t     (const_int 28) (const_int 60)\n \t\t     (const_int 29) (const_int 61)\n \t\t     (const_int 30) (const_int 62)\n-\t\t     (const_int 31) (const_int 63)\n-\t\t     (const_int 32) (const_int 64)])))]\n+\t\t     (const_int 31) (const_int 63)])))]\n   \"TARGET_AVX2\"\n   \"vpunpckhbw\\t{%2, %1, %0|%0, %1, %2}\"\n   [(set_attr \"type\" \"sselog\")\n@@ -6559,7 +6556,6 @@\n \t\t     (const_int 5) (const_int 37)\n \t\t     (const_int 6) (const_int 38)\n \t\t     (const_int 7) (const_int 39)\n-\t\t     (const_int 15) (const_int 47)\n \t\t     (const_int 16) (const_int 48)\n \t\t     (const_int 17) (const_int 49)\n \t\t     (const_int 18) (const_int 50)\n@@ -6919,7 +6915,11 @@\n \t\t\t\tGEN_INT ((mask >> 0) & 3),\n \t\t\t\tGEN_INT ((mask >> 2) & 3),\n \t\t\t\tGEN_INT ((mask >> 4) & 3),\n-\t\t\t\tGEN_INT ((mask >> 6) & 3)));\n+\t\t\t\tGEN_INT ((mask >> 6) & 3),\n+\t\t\t\tGEN_INT (((mask >> 0) & 3) + 4),\n+\t\t\t\tGEN_INT (((mask >> 2) & 3) + 4),\n+\t\t\t\tGEN_INT (((mask >> 4) & 3) + 4),\n+\t\t\t\tGEN_INT (((mask >> 6) & 3) + 4)));\n   DONE;\n })\n \n@@ -6931,11 +6931,15 @@\n \t\t     (match_operand 3 \"const_0_to_3_operand\" \"\")\n \t\t     (match_operand 4 \"const_0_to_3_operand\" \"\")\n \t\t     (match_operand 5 \"const_0_to_3_operand\" \"\")\n-\t\t     (match_dup 2)\n-\t\t     (match_dup 3)\n-\t\t     (match_dup 4)\n-\t\t     (match_dup 5)])))]\n-  \"TARGET_AVX2\"\n+\t\t     (match_operand 6 \"const_4_to_7_operand\" \"\")\n+\t\t     (match_operand 7 \"const_4_to_7_operand\" \"\")\n+\t\t     (match_operand 8 \"const_4_to_7_operand\" \"\")\n+\t\t     (match_operand 9 \"const_4_to_7_operand\" \"\")])))]\n+  \"TARGET_AVX2\n+   && INTVAL (operands[2]) + 4 == INTVAL (operands[6])\n+   && INTVAL (operands[3]) + 4 == INTVAL (operands[7])\n+   && INTVAL (operands[4]) + 4 == INTVAL (operands[8])\n+   && INTVAL (operands[5]) + 4 == INTVAL (operands[9])\"\n {\n   int mask = 0;\n   mask |= INTVAL (operands[2]) << 0;\n@@ -7002,7 +7006,11 @@\n \t\t\t\t GEN_INT ((mask >> 0) & 3),\n \t\t\t\t GEN_INT ((mask >> 2) & 3),\n \t\t\t\t GEN_INT ((mask >> 4) & 3),\n-\t\t\t\t GEN_INT ((mask >> 6) & 3)));\n+\t\t\t\t GEN_INT ((mask >> 6) & 3),\n+\t\t\t\t GEN_INT (((mask >> 0) & 3) + 8),\n+\t\t\t\t GEN_INT (((mask >> 2) & 3) + 8),\n+\t\t\t\t GEN_INT (((mask >> 4) & 3) + 8),\n+\t\t\t\t GEN_INT (((mask >> 6) & 3) + 8)));\n   DONE;\n })\n \n@@ -7018,15 +7026,19 @@\n \t\t     (const_int 5)\n \t\t     (const_int 6)\n \t\t     (const_int 7)\n-\t\t     (match_dup 2)\n-\t\t     (match_dup 3)\n-\t\t     (match_dup 4)\n-\t\t     (match_dup 5)\n+\t\t     (match_operand 6 \"const_8_to_11_operand\" \"\")\n+\t\t     (match_operand 7 \"const_8_to_11_operand\" \"\")\n+\t\t     (match_operand 8 \"const_8_to_11_operand\" \"\")\n+\t\t     (match_operand 9 \"const_8_to_11_operand\" \"\")\n \t\t     (const_int 12)\n \t\t     (const_int 13)\n \t\t     (const_int 14)\n \t\t     (const_int 15)])))]\n-  \"TARGET_AVX2\"\n+  \"TARGET_AVX2\n+   && INTVAL (operands[2]) + 8 == INTVAL (operands[6])\n+   && INTVAL (operands[3]) + 8 == INTVAL (operands[7])\n+   && INTVAL (operands[4]) + 8 == INTVAL (operands[8])\n+   && INTVAL (operands[5]) + 8 == INTVAL (operands[9])\"\n {\n   int mask = 0;\n   mask |= INTVAL (operands[2]) << 0;\n@@ -7098,7 +7110,11 @@\n \t\t\t\t GEN_INT (((mask >> 0) & 3) + 4),\n \t\t\t\t GEN_INT (((mask >> 2) & 3) + 4),\n \t\t\t\t GEN_INT (((mask >> 4) & 3) + 4),\n-\t\t\t\t GEN_INT (((mask >> 6) & 3) + 4)));\n+\t\t\t\t GEN_INT (((mask >> 6) & 3) + 4),\n+\t\t\t\t GEN_INT (((mask >> 0) & 3) + 12),\n+\t\t\t\t GEN_INT (((mask >> 2) & 3) + 12),\n+\t\t\t\t GEN_INT (((mask >> 4) & 3) + 12),\n+\t\t\t\t GEN_INT (((mask >> 6) & 3) + 12)));\n   DONE;\n })\n \n@@ -7118,11 +7134,15 @@\n \t\t     (const_int 9)\n \t\t     (const_int 10)\n \t\t     (const_int 11)\n-\t\t     (match_dup 2)\n-\t\t     (match_dup 3)\n-\t\t     (match_dup 4)\n-\t\t     (match_dup 5)])))]\n-  \"TARGET_AVX2\"\n+\t\t     (match_operand 6 \"const_12_to_15_operand\" \"\")\n+\t\t     (match_operand 7 \"const_12_to_15_operand\" \"\")\n+\t\t     (match_operand 8 \"const_12_to_15_operand\" \"\")\n+\t\t     (match_operand 9 \"const_12_to_15_operand\" \"\")])))]\n+  \"TARGET_AVX2\n+   && INTVAL (operands[2]) + 8 == INTVAL (operands[6])\n+   && INTVAL (operands[3]) + 8 == INTVAL (operands[7])\n+   && INTVAL (operands[4]) + 8 == INTVAL (operands[8])\n+   && INTVAL (operands[5]) + 8 == INTVAL (operands[9])\"\n {\n   int mask = 0;\n   mask |= (INTVAL (operands[2]) - 4) << 0;\n@@ -11526,14 +11546,39 @@\n    (set_attr \"prefix\" \"vex\")\n    (set_attr \"mode\" \"OI\")])\n \n-(define_insn \"avx2_permv4di\"\n+(define_expand \"avx2_permv4di\"\n+  [(match_operand:V4DI 0 \"register_operand\" \"\")\n+   (match_operand:V4DI 1 \"nonimmediate_operand\" \"\")\n+   (match_operand:SI 2 \"const_0_to_255_operand\" \"\")]\n+  \"TARGET_AVX2\"\n+{\n+  int mask = INTVAL (operands[2]);\n+  emit_insn (gen_avx2_permv4di_1 (operands[0], operands[1],\n+\t\t\t\t  GEN_INT ((mask >> 0) & 3),\n+\t\t\t\t  GEN_INT ((mask >> 2) & 3),\n+\t\t\t\t  GEN_INT ((mask >> 4) & 3),\n+\t\t\t\t  GEN_INT ((mask >> 6) & 3)));\n+  DONE;\n+})\n+\n+(define_insn \"avx2_permv4di_1\"\n   [(set (match_operand:V4DI 0 \"register_operand\" \"=x\")\n-\t(unspec:V4DI\n-\t  [(match_operand:V4DI 1 \"register_operand\" \"xm\")\n-\t   (match_operand:SI 2 \"const_0_to_255_operand\" \"n\")]\n-\t  UNSPEC_VPERMDI))]\n+\t(vec_select:V4DI\n+\t  (match_operand:V4DI 1 \"nonimmediate_operand\" \"xm\")\n+\t  (parallel [(match_operand 2 \"const_0_to_3_operand\" \"\")\n+\t\t     (match_operand 3 \"const_0_to_3_operand\" \"\")\n+\t\t     (match_operand 4 \"const_0_to_3_operand\" \"\")\n+\t\t     (match_operand 5 \"const_0_to_3_operand\" \"\")])))]\n   \"TARGET_AVX2\"\n-  \"vpermq\\t{%2, %1, %0|%0, %1, %2}\"\n+{\n+  int mask = 0;\n+  mask |= INTVAL (operands[2]) << 0;\n+  mask |= INTVAL (operands[3]) << 2;\n+  mask |= INTVAL (operands[4]) << 4;\n+  mask |= INTVAL (operands[5]) << 6;\n+  operands[2] = GEN_INT (mask);\n+  return \"vpermq\\t{%2, %1, %0|%0, %1, %2}\";\n+}\n   [(set_attr \"type\" \"sselog\")\n    (set_attr \"prefix\" \"vex\")\n    (set_attr \"mode\" \"OI\")])\n@@ -11542,7 +11587,7 @@\n   [(set (match_operand:V4DI 0 \"register_operand\" \"=x\")\n \t(unspec:V4DI\n \t  [(match_operand:V4DI 1 \"register_operand\" \"x\")\n-\t   (match_operand:V4DI 2 \"register_operand\" \"xm\")\n+\t   (match_operand:V4DI 2 \"nonimmediate_operand\" \"xm\")\n \t   (match_operand:SI 3 \"const_0_to_255_operand\" \"n\")]\n \t  UNSPEC_VPERMTI))]\n   \"TARGET_AVX2\""}]}