{"sha": "7b5fc413c14b6134b55a06c906f9ac2d9aff0628", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6N2I1ZmM0MTNjMTRiNjEzNGI1NWEwNmM5MDZmOWFjMmQ5YWZmMDYyOA==", "commit": {"author": {"name": "Richard Biener", "email": "rguenther@suse.de", "date": "2015-05-08T15:13:55Z"}, "committer": {"name": "Richard Biener", "email": "rguenth@gcc.gnu.org", "date": "2015-05-08T15:13:55Z"}, "message": "re PR tree-optimization/66036 (strided group loads are not vectorized)\n\n2015-05-08  Richard Biener  <rguenther@suse.de>\n\n        PR tree-optimization/66036\n\t* tree-vect-data-refs.c (vect_compute_data_ref_alignment):\n\tHandle strided group loads.\n\t(vect_verify_datarefs_alignment): Likewise.\n\t(vect_enhance_data_refs_alignment): Likewise.\n\t(vect_analyze_group_access): Likewise.\n\t(vect_analyze_data_ref_access): Likewise.\n\t(vect_analyze_data_ref_accesses): Likewise.\n\t* tree-vect-stmts.c (vect_model_load_cost): Likewise.\n\t(vectorizable_load): Likewise.\n\n\t* gcc.dg/vect/slp-41.c: New testcase.\n\nFrom-SVN: r222914", "tree": {"sha": "4bd4096b1943e764494337c4f8e79ae1d9a5374f", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/4bd4096b1943e764494337c4f8e79ae1d9a5374f"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/7b5fc413c14b6134b55a06c906f9ac2d9aff0628", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7b5fc413c14b6134b55a06c906f9ac2d9aff0628", "html_url": "https://github.com/Rust-GCC/gccrs/commit/7b5fc413c14b6134b55a06c906f9ac2d9aff0628", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/7b5fc413c14b6134b55a06c906f9ac2d9aff0628/comments", "author": {"login": "rguenth", "id": 2046526, "node_id": "MDQ6VXNlcjIwNDY1MjY=", "avatar_url": "https://avatars.githubusercontent.com/u/2046526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rguenth", "html_url": "https://github.com/rguenth", "followers_url": "https://api.github.com/users/rguenth/followers", "following_url": "https://api.github.com/users/rguenth/following{/other_user}", "gists_url": "https://api.github.com/users/rguenth/gists{/gist_id}", "starred_url": "https://api.github.com/users/rguenth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rguenth/subscriptions", "organizations_url": "https://api.github.com/users/rguenth/orgs", "repos_url": "https://api.github.com/users/rguenth/repos", "events_url": "https://api.github.com/users/rguenth/events{/privacy}", "received_events_url": "https://api.github.com/users/rguenth/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "4a8108f0ab7310371123340e0181ff4afd84789e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4a8108f0ab7310371123340e0181ff4afd84789e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4a8108f0ab7310371123340e0181ff4afd84789e"}], "stats": {"total": 311, "additions": 217, "deletions": 94}, "files": [{"sha": "f7f03b3b8e379983ef71865ece286755867baf3b", "filename": "gcc/ChangeLog", "status": "modified", "additions": 13, "deletions": 0, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7b5fc413c14b6134b55a06c906f9ac2d9aff0628/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7b5fc413c14b6134b55a06c906f9ac2d9aff0628/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=7b5fc413c14b6134b55a06c906f9ac2d9aff0628", "patch": "@@ -1,3 +1,16 @@\n+2015-05-08  Richard Biener  <rguenther@suse.de>\n+\n+\tPR tree-optimization/66036\n+\t* tree-vect-data-refs.c (vect_compute_data_ref_alignment):\n+\tHandle strided group loads.\n+\t(vect_verify_datarefs_alignment): Likewise.\n+\t(vect_enhance_data_refs_alignment): Likewise.\n+\t(vect_analyze_group_access): Likewise.\n+\t(vect_analyze_data_ref_access): Likewise.\n+\t(vect_analyze_data_ref_accesses): Likewise.\n+\t* tree-vect-stmts.c (vect_model_load_cost): Likewise.\n+\t(vectorizable_load): Likewise.\n+\n 2015-05-08  Segher Boessenkool  <segher@kernel.crashing.org>\n \n \t* config/rs6000/rs6000.md: Require operand inequality in one"}, {"sha": "2b6f663cc01a708047c2e2dea7a59296ac40410b", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 5, "deletions": 0, "changes": 5, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7b5fc413c14b6134b55a06c906f9ac2d9aff0628/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7b5fc413c14b6134b55a06c906f9ac2d9aff0628/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=7b5fc413c14b6134b55a06c906f9ac2d9aff0628", "patch": "@@ -1,3 +1,8 @@\n+2015-05-08  Richard Biener  <rguenther@suse.de>\n+\n+\tPR tree-optimization/66036\n+\t* gcc.dg/vect/slp-41.c: New testcase.\n+\n 2015-05-08  Mikael Morin  <mikael@gcc.gnu.org>\n \n \t* gfortran.dg/elemental_optional_args_7.f90: New."}, {"sha": "7d487b4dad91018177a36de660456decb1472c96", "filename": "gcc/testsuite/gcc.dg/vect/slp-41.c", "status": "added", "additions": 69, "deletions": 0, "changes": 69, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7b5fc413c14b6134b55a06c906f9ac2d9aff0628/gcc%2Ftestsuite%2Fgcc.dg%2Fvect%2Fslp-41.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7b5fc413c14b6134b55a06c906f9ac2d9aff0628/gcc%2Ftestsuite%2Fgcc.dg%2Fvect%2Fslp-41.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.dg%2Fvect%2Fslp-41.c?ref=7b5fc413c14b6134b55a06c906f9ac2d9aff0628", "patch": "@@ -0,0 +1,69 @@\n+/* { dg-require-effective-target vect_int } */\n+/* { dg-require-effective-target vect_pack_trunc } */\n+/* { dg-require-effective-target vect_unpack } */\n+/* { dg-require-effective-target vect_hw_misalign } */\n+\n+#include \"tree-vect.h\"\n+\n+void __attribute__((noinline,noclone))\n+testi (int *p, short *q, int stride, int n)\n+{\n+  int i;\n+  for (i = 0; i < n; ++i)\n+    {\n+      q[i*4+0] = p[i*stride+0];\n+      q[i*4+1] = p[i*stride+1];\n+      q[i*4+2] = p[i*stride+2];\n+      q[i*4+3] = p[i*stride+3];\n+    }\n+}\n+\n+void __attribute__((noinline,noclone))\n+testi2 (int *q, short *p, int stride, int n)\n+{\n+  int i;\n+  for (i = 0; i < n; ++i)\n+    {\n+      q[i*4+0] = p[i*stride+0];\n+      q[i*4+1] = p[i*stride+1];\n+      q[i*4+2] = p[i*stride+2];\n+      q[i*4+3] = p[i*stride+3];\n+    }\n+}\n+\n+int ia[256];\n+short sa[256];\n+\n+extern void abort (void);\n+\n+int main()\n+{\n+  int i;\n+\n+  check_vect ();\n+\n+  for (i = 0; i < 256; ++i)\n+    {\n+      ia[i] = sa[i] = i;\n+       __asm__ volatile (\"\");\n+    }\n+  testi (ia, sa, 8, 32);\n+  for (i = 0; i < 128; ++i)\n+    if (sa[i] != ia[(i / 4) * 8 + i % 4])\n+      abort ();\n+\n+  for (i = 0; i < 256; ++i)\n+    {\n+      ia[i] = sa[i] = i;\n+       __asm__ volatile (\"\");\n+    }\n+  testi2 (ia, sa, 8, 32);\n+  for (i = 0; i < 128; ++i)\n+    if (ia[i] != sa[(i / 4) * 8 + i % 4])\n+      abort ();\n+\n+  return 0;\n+}\n+\n+/* { dg-final { scan-tree-dump-times \"vectorized 1 loops\" 2 \"vect\" } } */\n+/* { dg-final { cleanup-tree-dump \"vect\" } } */"}, {"sha": "7e938996866a11868309ccac91ca5fc7dce032be", "filename": "gcc/tree-vect-data-refs.c", "status": "modified", "additions": 54, "deletions": 69, "changes": 123, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7b5fc413c14b6134b55a06c906f9ac2d9aff0628/gcc%2Ftree-vect-data-refs.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7b5fc413c14b6134b55a06c906f9ac2d9aff0628/gcc%2Ftree-vect-data-refs.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-data-refs.c?ref=7b5fc413c14b6134b55a06c906f9ac2d9aff0628", "patch": "@@ -649,7 +649,7 @@ vect_compute_data_ref_alignment (struct data_reference *dr)\n   tree vectype;\n   tree base, base_addr;\n   bool base_aligned;\n-  tree misalign;\n+  tree misalign = NULL_TREE;\n   tree aligned_to;\n   unsigned HOST_WIDE_INT alignment;\n \n@@ -665,10 +665,12 @@ vect_compute_data_ref_alignment (struct data_reference *dr)\n \n   /* Strided loads perform only component accesses, misalignment information\n      is irrelevant for them.  */\n-  if (STMT_VINFO_STRIDE_LOAD_P (stmt_info))\n+  if (STMT_VINFO_STRIDE_LOAD_P (stmt_info)\n+      && !STMT_VINFO_GROUPED_ACCESS (stmt_info))\n     return true;\n \n-  misalign = DR_INIT (dr);\n+  if (tree_fits_shwi_p (DR_STEP (dr)))\n+    misalign = DR_INIT (dr);\n   aligned_to = DR_ALIGNED_TO (dr);\n   base_addr = DR_BASE_ADDRESS (dr);\n   vectype = STMT_VINFO_VECTYPE (stmt_info);\n@@ -682,9 +684,9 @@ vect_compute_data_ref_alignment (struct data_reference *dr)\n   if (loop && nested_in_vect_loop_p (loop, stmt))\n     {\n       tree step = DR_STEP (dr);\n-      HOST_WIDE_INT dr_step = TREE_INT_CST_LOW (step);\n \n-      if (dr_step % GET_MODE_SIZE (TYPE_MODE (vectype)) == 0)\n+      if (tree_fits_shwi_p (step)\n+\t  && tree_to_shwi (step) % GET_MODE_SIZE (TYPE_MODE (vectype)) == 0)\n         {\n           if (dump_enabled_p ())\n             dump_printf_loc (MSG_NOTE, vect_location,\n@@ -710,9 +712,9 @@ vect_compute_data_ref_alignment (struct data_reference *dr)\n   if (!loop)\n     {\n       tree step = DR_STEP (dr);\n-      HOST_WIDE_INT dr_step = TREE_INT_CST_LOW (step);\n \n-      if (dr_step % GET_MODE_SIZE (TYPE_MODE (vectype)) != 0)\n+      if (tree_fits_shwi_p (step)\n+\t  && tree_to_shwi (step) % GET_MODE_SIZE (TYPE_MODE (vectype)) != 0)\n \t{\n \t  if (dump_enabled_p ())\n \t    dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n@@ -942,7 +944,8 @@ vect_verify_datarefs_alignment (loop_vec_info loop_vinfo, bb_vec_info bb_vinfo)\n \n       /* Strided loads perform only component accesses, alignment is\n \t irrelevant for them.  */\n-      if (STMT_VINFO_STRIDE_LOAD_P (stmt_info))\n+      if (STMT_VINFO_STRIDE_LOAD_P (stmt_info)\n+\t  && !STMT_VINFO_GROUPED_ACCESS (stmt_info))\n \tcontinue;\n \n       supportable_dr_alignment = vect_supportable_dr_alignment (dr, false);\n@@ -1409,7 +1412,8 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \n       /* Strided loads perform only component accesses, alignment is\n \t irrelevant for them.  */\n-      if (STMT_VINFO_STRIDE_LOAD_P (stmt_info))\n+      if (STMT_VINFO_STRIDE_LOAD_P (stmt_info)\n+\t  && !STMT_VINFO_GROUPED_ACCESS (stmt_info))\n \tcontinue;\n \n       supportable_dr_alignment = vect_supportable_dr_alignment (dr, true);\n@@ -1701,7 +1705,8 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \n \t  /* Strided loads perform only component accesses, alignment is\n \t     irrelevant for them.  */\n-\t  if (STMT_VINFO_STRIDE_LOAD_P (stmt_info))\n+\t  if (STMT_VINFO_STRIDE_LOAD_P (stmt_info)\n+\t      && !STMT_VINFO_GROUPED_ACCESS (stmt_info))\n \t    continue;\n \n \t  save_misalignment = DR_MISALIGNMENT (dr);\n@@ -1819,10 +1824,15 @@ vect_enhance_data_refs_alignment (loop_vec_info loop_vinfo)\n \t\t  && GROUP_FIRST_ELEMENT (stmt_info) != stmt))\n \t    continue;\n \n-\t  /* Strided loads perform only component accesses, alignment is\n-\t     irrelevant for them.  */\n \t  if (STMT_VINFO_STRIDE_LOAD_P (stmt_info))\n-\t    continue;\n+\t    {\n+\t      /* Strided loads perform only component accesses, alignment is\n+\t\t irrelevant for them.  */\n+\t      if (!STMT_VINFO_GROUPED_ACCESS (stmt_info))\n+\t\tcontinue;\n+\t      do_versioning = false;\n+\t      break;\n+\t    }\n \n \t  supportable_dr_alignment = vect_supportable_dr_alignment (dr, false);\n \n@@ -2035,7 +2045,7 @@ vect_analyze_group_access (struct data_reference *dr)\n   stmt_vec_info stmt_info = vinfo_for_stmt (stmt);\n   loop_vec_info loop_vinfo = STMT_VINFO_LOOP_VINFO (stmt_info);\n   bb_vec_info bb_vinfo = STMT_VINFO_BB_VINFO (stmt_info);\n-  HOST_WIDE_INT dr_step = TREE_INT_CST_LOW (step);\n+  HOST_WIDE_INT dr_step = -1;\n   HOST_WIDE_INT groupsize, last_accessed_element = 1;\n   bool slp_impossible = false;\n   struct loop *loop = NULL;\n@@ -2045,7 +2055,13 @@ vect_analyze_group_access (struct data_reference *dr)\n \n   /* For interleaving, GROUPSIZE is STEP counted in elements, i.e., the\n      size of the interleaving group (including gaps).  */\n-  groupsize = absu_hwi (dr_step) / type_size;\n+  if (tree_fits_shwi_p (step))\n+    {\n+      dr_step = tree_to_shwi (step);\n+      groupsize = absu_hwi (dr_step) / type_size;\n+    }\n+  else\n+    groupsize = 0;\n \n   /* Not consecutive access is possible only if it is a part of interleaving.  */\n   if (!GROUP_FIRST_ELEMENT (vinfo_for_stmt (stmt)))\n@@ -2120,7 +2136,6 @@ vect_analyze_group_access (struct data_reference *dr)\n       tree prev_init = DR_INIT (data_ref);\n       gimple prev = stmt;\n       HOST_WIDE_INT diff, gaps = 0;\n-      unsigned HOST_WIDE_INT count_in_bytes;\n \n       while (next)\n         {\n@@ -2185,30 +2200,12 @@ vect_analyze_group_access (struct data_reference *dr)\n           count++;\n         }\n \n-      /* COUNT is the number of accesses found, we multiply it by the size of\n-         the type to get COUNT_IN_BYTES.  */\n-      count_in_bytes = type_size * count;\n-\n-      /* Check that the size of the interleaving (including gaps) is not\n-         greater than STEP.  */\n-      if (dr_step != 0\n-\t  && absu_hwi (dr_step) < count_in_bytes + gaps * type_size)\n-        {\n-          if (dump_enabled_p ())\n-            {\n-              dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-                               \"interleaving size is greater than step for \");\n-              dump_generic_expr (MSG_MISSED_OPTIMIZATION, TDF_SLIM,\n-                                 DR_REF (dr));\n-              dump_printf (MSG_MISSED_OPTIMIZATION, \"\\n\");\n-            }\n-          return false;\n-        }\n+      if (groupsize == 0)\n+        groupsize = count + gaps;\n \n-      /* Check that the size of the interleaving is equal to STEP for stores,\n+      /* Check that the size of the interleaving is equal to count for stores,\n          i.e., that there are no gaps.  */\n-      if (dr_step != 0\n-\t  && absu_hwi (dr_step) != count_in_bytes)\n+      if (groupsize != count)\n         {\n           if (DR_IS_READ (dr))\n             {\n@@ -2227,26 +2224,6 @@ vect_analyze_group_access (struct data_reference *dr)\n             }\n         }\n \n-      /* Check that STEP is a multiple of type size.  */\n-      if (dr_step != 0\n-\t  && (dr_step % type_size) != 0)\n-        {\n-          if (dump_enabled_p ())\n-            {\n-              dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n-                               \"step is not a multiple of type size: step \");\n-              dump_generic_expr (MSG_MISSED_OPTIMIZATION, TDF_SLIM, step);\n-              dump_printf (MSG_MISSED_OPTIMIZATION, \" size \");\n-              dump_generic_expr (MSG_MISSED_OPTIMIZATION, TDF_SLIM,\n-                                 TYPE_SIZE_UNIT (scalar_type));\n-              dump_printf (MSG_MISSED_OPTIMIZATION, \"\\n\");\n-            }\n-          return false;\n-        }\n-\n-      if (groupsize == 0)\n-        groupsize = count + gaps;\n-\n       GROUP_SIZE (vinfo_for_stmt (stmt)) = groupsize;\n       if (dump_enabled_p ())\n         dump_printf_loc (MSG_NOTE, vect_location,\n@@ -2366,9 +2343,12 @@ vect_analyze_data_ref_access (struct data_reference *dr)\n       return false;\n     }\n \n+\n   /* Assume this is a DR handled by non-constant strided load case.  */\n   if (TREE_CODE (step) != INTEGER_CST)\n-    return STMT_VINFO_STRIDE_LOAD_P (stmt_info);\n+    return (STMT_VINFO_STRIDE_LOAD_P (stmt_info)\n+\t    && (!STMT_VINFO_GROUPED_ACCESS (stmt_info)\n+\t\t|| vect_analyze_group_access (dr)));\n \n   /* Not consecutive access - check if it's a part of interleaving group.  */\n   return vect_analyze_group_access (dr);\n@@ -2570,15 +2550,16 @@ vect_analyze_data_ref_accesses (loop_vec_info loop_vinfo, bb_vec_info bb_vinfo)\n \t      || !gimple_assign_single_p (DR_STMT (drb)))\n \t    break;\n \n-\t  /* Check that the data-refs have the same constant size and step.  */\n+\t  /* Check that the data-refs have the same constant size.  */\n \t  tree sza = TYPE_SIZE_UNIT (TREE_TYPE (DR_REF (dra)));\n \t  tree szb = TYPE_SIZE_UNIT (TREE_TYPE (DR_REF (drb)));\n \t  if (!tree_fits_uhwi_p (sza)\n \t      || !tree_fits_uhwi_p (szb)\n-\t      || !tree_int_cst_equal (sza, szb)\n-\t      || !tree_fits_shwi_p (DR_STEP (dra))\n-\t      || !tree_fits_shwi_p (DR_STEP (drb))\n-\t      || !tree_int_cst_equal (DR_STEP (dra), DR_STEP (drb)))\n+\t      || !tree_int_cst_equal (sza, szb))\n+\t    break;\n+\n+\t  /* Check that the data-refs have the same step.  */\n+\t  if (!operand_equal_p (DR_STEP (dra), DR_STEP (drb), 0))\n \t    break;\n \n \t  /* Do not place the same access in the interleaving chain twice.  */\n@@ -2611,11 +2592,15 @@ vect_analyze_data_ref_accesses (loop_vec_info loop_vinfo, bb_vec_info bb_vinfo)\n \t\t  != type_size_a))\n \t    break;\n \n-\t  /* The step (if not zero) is greater than the difference between\n-\t     data-refs' inits.  This splits groups into suitable sizes.  */\n-\t  HOST_WIDE_INT step = tree_to_shwi (DR_STEP (dra));\n-\t  if (step != 0 && step <= (init_b - init_a))\n-\t    break;\n+\t  /* If the step (if not zero or non-constant) is greater than the\n+\t     difference between data-refs' inits this splits groups into\n+\t     suitable sizes.  */\n+\t  if (tree_fits_shwi_p (DR_STEP (dra)))\n+\t    {\n+\t      HOST_WIDE_INT step = tree_to_shwi (DR_STEP (dra));\n+\t      if (step != 0 && step <= (init_b - init_a))\n+\t\tbreak;\n+\t    }\n \n \t  if (dump_enabled_p ())\n \t    {"}, {"sha": "f82decb798e100f5e985da645aa61bfd0e0ad4f9", "filename": "gcc/tree-vect-stmts.c", "status": "modified", "additions": 76, "deletions": 25, "changes": 101, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/7b5fc413c14b6134b55a06c906f9ac2d9aff0628/gcc%2Ftree-vect-stmts.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/7b5fc413c14b6134b55a06c906f9ac2d9aff0628/gcc%2Ftree-vect-stmts.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftree-vect-stmts.c?ref=7b5fc413c14b6134b55a06c906f9ac2d9aff0628", "patch": "@@ -1112,7 +1112,8 @@ vect_model_load_cost (stmt_vec_info stmt_info, int ncopies,\n      equivalent to the cost of GROUP_SIZE separate loads.  If a grouped\n      access is instead being provided by a load-and-permute operation,\n      include the cost of the permutes.  */\n-  if (!load_lanes_p && group_size > 1)\n+  if (!load_lanes_p && group_size > 1\n+      && !STMT_VINFO_STRIDE_LOAD_P (stmt_info))\n     {\n       /* Uses an even and odd extract operations or shuffle operations\n \t for each needed permute.  */\n@@ -1127,22 +1128,24 @@ vect_model_load_cost (stmt_vec_info stmt_info, int ncopies,\n     }\n \n   /* The loads themselves.  */\n-  if (STMT_VINFO_STRIDE_LOAD_P (stmt_info))\n+  if (STMT_VINFO_STRIDE_LOAD_P (stmt_info)\n+      && !STMT_VINFO_GROUPED_ACCESS (stmt_info))\n     {\n       /* N scalar loads plus gathering them into a vector.  */\n       tree vectype = STMT_VINFO_VECTYPE (stmt_info);\n       inside_cost += record_stmt_cost (body_cost_vec,\n \t\t\t\t       ncopies * TYPE_VECTOR_SUBPARTS (vectype),\n \t\t\t\t       scalar_load, stmt_info, 0, vect_body);\n-      inside_cost += record_stmt_cost (body_cost_vec, ncopies, vec_construct,\n-\t\t\t\t       stmt_info, 0, vect_body);\n     }\n   else\n     vect_get_load_cost (first_dr, ncopies,\n \t\t\t((!STMT_VINFO_GROUPED_ACCESS (stmt_info))\n \t\t\t || group_size > 1 || slp_node),\n \t\t\t&inside_cost, &prologue_cost, \n \t\t\tprologue_cost_vec, body_cost_vec, true);\n+  if (STMT_VINFO_STRIDE_LOAD_P (stmt_info))\n+      inside_cost += record_stmt_cost (body_cost_vec, ncopies, vec_construct,\n+\t\t\t\t       stmt_info, 0, vect_body);\n \n   if (dump_enabled_p ())\n     dump_printf_loc (MSG_NOTE, vect_location,\n@@ -5657,7 +5660,7 @@ vectorizable_load (gimple stmt, gimple_stmt_iterator *gsi, gimple *vec_stmt,\n   gimple ptr_incr = NULL;\n   int nunits = TYPE_VECTOR_SUBPARTS (vectype);\n   int ncopies;\n-  int i, j, group_size, group_gap;\n+  int i, j, group_size = -1, group_gap;\n   tree msq = NULL_TREE, lsq;\n   tree offset = NULL_TREE;\n   tree byte_offset = NULL_TREE;\n@@ -5790,9 +5793,11 @@ vectorizable_load (gimple stmt, gimple_stmt_iterator *gsi, gimple *vec_stmt,\n \t  return false;\n \t}\n \n-      if (!slp && !PURE_SLP_STMT (stmt_info))\n+      group_size = GROUP_SIZE (vinfo_for_stmt (first_stmt));\n+      if (!slp\n+\t  && !PURE_SLP_STMT (stmt_info)\n+\t  && !STMT_VINFO_STRIDE_LOAD_P (stmt_info))\n \t{\n-\t  group_size = GROUP_SIZE (vinfo_for_stmt (first_stmt));\n \t  if (vect_load_lanes_supported (vectype, group_size))\n \t    load_lanes_p = true;\n \t  else if (!vect_grouped_load_supported (vectype, group_size))\n@@ -5847,7 +5852,22 @@ vectorizable_load (gimple stmt, gimple_stmt_iterator *gsi, gimple *vec_stmt,\n \t}\n     }\n   else if (STMT_VINFO_STRIDE_LOAD_P (stmt_info))\n-    ;\n+    {\n+      if ((grouped_load\n+\t   && (slp || PURE_SLP_STMT (stmt_info)))\n+\t  && (group_size > nunits\n+\t      || nunits % group_size != 0\n+\t      /* ???  During analysis phase we are not called with the\n+\t         slp node/instance we are in so whether we'll end up\n+\t\t with a permutation we don't know.  Still we don't\n+\t\t support load permutations.  */\n+\t      || slp_perm))\n+\t{\n+\t  dump_printf_loc (MSG_MISSED_OPTIMIZATION, vect_location,\n+\t\t\t   \"unhandled strided group load\\n\");\n+\t  return false;\n+\t}\n+    }\n   else\n     {\n       negative = tree_int_cst_compare (nested_in_vect_loop\n@@ -6136,34 +6156,65 @@ vectorizable_load (gimple stmt, gimple_stmt_iterator *gsi, gimple *vec_stmt,\n       prev_stmt_info = NULL;\n       running_off = offvar;\n       alias_off = build_int_cst (reference_alias_ptr_type (DR_REF (dr)), 0);\n+      int nloads = nunits;\n+      tree ltype = TREE_TYPE (vectype);\n+      if (slp)\n+\t{\n+\t  nloads = nunits / group_size;\n+\t  if (group_size < nunits)\n+\t    ltype = build_vector_type (TREE_TYPE (vectype), group_size);\n+\t  else\n+\t    ltype = vectype;\n+\t  ltype = build_aligned_type (ltype, TYPE_ALIGN (TREE_TYPE (vectype)));\n+\t  ncopies = SLP_TREE_NUMBER_OF_VEC_STMTS (slp_node);\n+\t  gcc_assert (!slp_perm);\n+\t}\n       for (j = 0; j < ncopies; j++)\n \t{\n \t  tree vec_inv;\n \n-\t  vec_alloc (v, nunits);\n-\t  for (i = 0; i < nunits; i++)\n+\t  if (nloads > 1)\n+\t    {\n+\t      vec_alloc (v, nloads);\n+\t      for (i = 0; i < nloads; i++)\n+\t\t{\n+\t\t  tree newref, newoff;\n+\t\t  gimple incr;\n+\t\t  newref = build2 (MEM_REF, ltype, running_off, alias_off);\n+\n+\t\t  newref = force_gimple_operand_gsi (gsi, newref, true,\n+\t\t\t\t\t\t     NULL_TREE, true,\n+\t\t\t\t\t\t     GSI_SAME_STMT);\n+\t\t  CONSTRUCTOR_APPEND_ELT (v, NULL_TREE, newref);\n+\t\t  newoff = copy_ssa_name (running_off);\n+\t\t  incr = gimple_build_assign (newoff, POINTER_PLUS_EXPR,\n+\t\t\t\t\t      running_off, stride_step);\n+\t\t  vect_finish_stmt_generation (stmt, incr, gsi);\n+\n+\t\t  running_off = newoff;\n+\t\t}\n+\n+\t      vec_inv = build_constructor (vectype, v);\n+\t      new_temp = vect_init_vector (stmt, vec_inv, vectype, gsi);\n+\t      new_stmt = SSA_NAME_DEF_STMT (new_temp);\n+\t    }\n+\t  else\n \t    {\n-\t      tree newref, newoff;\n-\t      gimple incr;\n-\t      newref = build2 (MEM_REF, TREE_TYPE (vectype),\n-\t\t\t       running_off, alias_off);\n-\n-\t      newref = force_gimple_operand_gsi (gsi, newref, true,\n-\t\t\t\t\t\t NULL_TREE, true,\n-\t\t\t\t\t\t GSI_SAME_STMT);\n-\t      CONSTRUCTOR_APPEND_ELT (v, NULL_TREE, newref);\n-\t      newoff = copy_ssa_name (running_off);\n-\t      incr = gimple_build_assign (newoff, POINTER_PLUS_EXPR,\n+\t      new_stmt = gimple_build_assign (make_ssa_name (ltype),\n+\t\t\t\t\t      build2 (MEM_REF, ltype,\n+\t\t\t\t\t\t      running_off, alias_off));\n+\t      vect_finish_stmt_generation (stmt, new_stmt, gsi);\n+\n+\t      tree newoff = copy_ssa_name (running_off);\n+\t      gimple incr = gimple_build_assign (newoff, POINTER_PLUS_EXPR,\n \t\t\t\t\t  running_off, stride_step);\n \t      vect_finish_stmt_generation (stmt, incr, gsi);\n \n \t      running_off = newoff;\n \t    }\n \n-\t  vec_inv = build_constructor (vectype, v);\n-\t  new_temp = vect_init_vector (stmt, vec_inv, vectype, gsi);\n-\t  new_stmt = SSA_NAME_DEF_STMT (new_temp);\n-\n+\t  if (slp)\n+\t    SLP_TREE_VEC_STMTS (slp_node).quick_push (new_stmt);\n \t  if (j == 0)\n \t    STMT_VINFO_VEC_STMT (stmt_info) = *vec_stmt = new_stmt;\n \t  else"}]}