{"sha": "0516f6fe82641daf7c1ac8812998049ac591201e", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MDUxNmY2ZmU4MjY0MWRhZjdjMWFjODgxMjk5ODA0OWFjNTkxMjAxZQ==", "commit": {"author": {"name": "Steven Bosscher", "email": "stevenb@suse.de", "date": "2004-08-18T20:53:59Z"}, "committer": {"name": "Steven Bosscher", "email": "steven@gcc.gnu.org", "date": "2004-08-18T20:53:59Z"}, "message": "Makefile.in (OBJS-common): Add postreload-gcse.c.\n\n\t* Makefile.in (OBJS-common): Add postreload-gcse.c.\n\tAdd new postreload-gcse.o.\n\t* cse.c (SAFE_HASH): Define as wrapper around safe_hash.\n\t(lookup_as_function, insert, rehash_using_reg, use_related_value,\n\tequiv_constant): Use SAFE_HASH instead of safe_hash.\n\t(exp_equiv_p): Export.  Add for_gcse argument when comparing\n\tfor GCSE.\n\t(lookup, lookup_for_remove, merge_equiv_classes, find_best_addr,\n\tfind_comparison_args, fold_rtx, cse_insn): Update callers.\n\t(hash_rtx): New function derived from old canon_hash and bits\n\tfrom gcse.c hash_expr_1.\n\t(canon_hash_string): Rename to hash_rtx_string.\n\t(canon_hash, safe_hash): Make static inline.  Call hash_rtx.\n\t* cselib.c (hash_rtx): Rename to cselib_hash_rtx.\n\t(cselib_lookup): Update this caller.\n\t* gcse.c (modify_mem_list_set, canon_modify_mem_list_set):\n\tMake static.\n\t(hash_expr): Call hash_rtx.\n\t(ldst_entry): Likewise.\n\t(expr_equiv_p): Call exp_equiv_p.\n\t(struct unoccr, hash_expr_1, hash_string_1, lookup_expr,\n\treg_used_on_edge, reg_set_between_after_reload_p,\n\treg_used_between_after_reload_p, get_avail_load_store_reg,\n\tis_jump_table_basic_block, bb_has_well_behaved_predecessors,\n\tget_bb_avail_insn, hash_scan_set_after_reload,\n\tcompute_hash_table_after_reload,\n\teliminate_partially_redundant_loads, gcse_after_reload,\n\tget_bb_avail_insn, gcse_after_reload_main): Remove.\n\t* postreload-gcse.c: New file, reincarnating most of the above.\n\t* rtl.h (exp_equiv_p, hash_rtx): New prototypes.\n\t(gcse_after_reload_main): Update prototype.\n\t* timevar.def (TV_GCSE_AFTER_RELOAD): New timevar.\n\t* passes.c (rest_of_handle_gcse2): Use it.\n\nFrom-SVN: r86206", "tree": {"sha": "93b7857ed347181c4c208adbe8b5fdd8d2f48e15", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/93b7857ed347181c4c208adbe8b5fdd8d2f48e15"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/0516f6fe82641daf7c1ac8812998049ac591201e", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0516f6fe82641daf7c1ac8812998049ac591201e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/0516f6fe82641daf7c1ac8812998049ac591201e", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/0516f6fe82641daf7c1ac8812998049ac591201e/comments", "author": null, "committer": null, "parents": [{"sha": "95013377bd98d4456996c3e7a2921b4c475239a1", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/95013377bd98d4456996c3e7a2921b4c475239a1", "html_url": "https://github.com/Rust-GCC/gccrs/commit/95013377bd98d4456996c3e7a2921b4c475239a1"}], "stats": {"total": 2891, "additions": 1668, "deletions": 1223}, "files": [{"sha": "58420cf30ae57565f33e82e9200c277591db8ac6", "filename": "gcc/ChangeLog", "status": "modified", "additions": 36, "deletions": 0, "changes": 36, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=0516f6fe82641daf7c1ac8812998049ac591201e", "patch": "@@ -1,3 +1,39 @@\n+2004-08-18  Steven Bosscher  <stevenb@suse.de>\n+\n+\t* Makefile.in (OBJS-common): Add postreload-gcse.c.\n+\tAdd new postreload-gcse.o.\n+\t* cse.c (SAFE_HASH): Define as wrapper around safe_hash.\n+\t(lookup_as_function, insert, rehash_using_reg, use_related_value,\n+\tequiv_constant): Use SAFE_HASH instead of safe_hash.\n+\t(exp_equiv_p): Export.  Add for_gcse argument when comparing\n+\tfor GCSE.\n+\t(lookup, lookup_for_remove, merge_equiv_classes, find_best_addr,\n+\tfind_comparison_args, fold_rtx, cse_insn): Update callers.\n+\t(hash_rtx): New function derived from old canon_hash and bits\n+\tfrom gcse.c hash_expr_1.\n+\t(canon_hash_string): Rename to hash_rtx_string.\n+\t(canon_hash, safe_hash): Make static inline.  Call hash_rtx.\n+\t* cselib.c (hash_rtx): Rename to cselib_hash_rtx.\n+\t(cselib_lookup): Update this caller.\n+\t* gcse.c (modify_mem_list_set, canon_modify_mem_list_set):\n+\tMake static.\n+\t(hash_expr): Call hash_rtx.\n+\t(ldst_entry): Likewise.\n+\t(expr_equiv_p): Call exp_equiv_p.\n+\t(struct unoccr, hash_expr_1, hash_string_1, lookup_expr,\n+\treg_used_on_edge, reg_set_between_after_reload_p,\n+\treg_used_between_after_reload_p, get_avail_load_store_reg,\n+\tis_jump_table_basic_block, bb_has_well_behaved_predecessors,\n+\tget_bb_avail_insn, hash_scan_set_after_reload,\n+\tcompute_hash_table_after_reload,\n+\teliminate_partially_redundant_loads, gcse_after_reload,\n+\tget_bb_avail_insn, gcse_after_reload_main): Remove.\n+\t* postreload-gcse.c: New file, reincarnating most of the above.\n+\t* rtl.h (exp_equiv_p, hash_rtx): New prototypes.\n+\t(gcse_after_reload_main): Update prototype.\n+\t* timevar.def (TV_GCSE_AFTER_RELOAD): New timevar.\n+\t* passes.c (rest_of_handle_gcse2): Use it.\n+\n 2004-08-18  Diego Novillo  <dnovillo@redhat.com>\n \n \t* tree-ssa-loop.c (pass_loop_init): Add TODO_dump_func."}, {"sha": "2a18bf25ac76081d2633ff06c1f3a2c884cdd053", "filename": "gcc/Makefile.in", "status": "modified", "additions": 11, "deletions": 4, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2FMakefile.in", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2FMakefile.in", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FMakefile.in?ref=0516f6fe82641daf7c1ac8812998049ac591201e", "patch": "@@ -899,15 +899,18 @@ OBJS-common = \\\n  genrtl.o ggc-common.o global.o graph.o gtype-desc.o\t\t\t   \\\n  haifa-sched.o hooks.o ifcvt.o insn-attrtab.o insn-emit.o insn-modes.o\t   \\\n  insn-extract.o insn-opinit.o insn-output.o insn-peep.o insn-recog.o\t   \\\n- insn-preds.o integrate.o intl.o jump.o  langhooks.o lcm.o lists.o \t   \\\n- local-alloc.o loop.o modulo-sched.o\t\t\t\t\t   \\\n- optabs.o options.o opts.o params.o postreload.o predict.o\t\t   \\\n+ integrate.o intl.o jump.o  langhooks.o lcm.o lists.o local-alloc.o  \t   \\\n+ loop.o modulo-sched.o optabs.o options.o opts.o\t\t\t   \\\n+ params.o postreload.o postreload-gcse.o predict.o\t\t\t   \\\n+ insn-preds.o integrate.o intl.o jump.o langhooks.o lcm.o lists.o \t   \\\n+ local-alloc.o loop.o modulo-sched.o optabs.o options.o opts.o\t\t   \\\n+ params.o postreload.o postreload-gcse.o predict.o\t\t\t   \\\n  print-rtl.o print-tree.o value-prof.o var-tracking.o\t\t\t   \\\n  profile.o ra.o ra-build.o ra-colorize.o ra-debug.o ra-rewrite.o\t   \\\n  real.o recog.o reg-stack.o regclass.o regmove.o regrename.o\t\t   \\\n  reload.o reload1.o reorg.o resource.o rtl.o rtlanal.o rtl-error.o\t   \\\n  sbitmap.o sched-deps.o sched-ebb.o sched-rgn.o sched-vis.o sdbout.o\t   \\\n- simplify-rtx.o sreal.o stmt.o stor-layout.o stringpool.o \t \t  \\\n+ simplify-rtx.o sreal.o stmt.o stor-layout.o stringpool.o\t\t   \\\n  targhooks.o timevar.o toplev.o tracer.o tree.o tree-dump.o unroll.o\t   \\\n  varasm.o varray.o vec.o version.o vmsdbgout.o xcoffout.o alloc-pool.o\t   \\\n  et-forest.o cfghooks.o bt-load.o pretty-print.o $(GGC) web.o passes.o\t   \\\n@@ -2047,6 +2050,10 @@ postreload.o : postreload.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H)\n    $(EXPR_H) $(OPTABS_H) reload.h $(REGS_H) hard-reg-set.h insn-config.h \\\n    $(BASIC_BLOCK_H) $(RECOG_H) output.h function.h toplev.h cselib.h $(TM_P_H) \\\n    except.h $(TREE_H)\n+postreload-gcse.o : postreload-gcse.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) \\\n+   $(RTL_H) $(REGS_H) hard-reg-set.h $(FLAGS_H) real.h insn-config.h $(GGC_H) \\\n+   $(RECOG_H) $(EXPR_H) $(BASIC_BLOCK_H) function.h output.h toplev.h $(TM_P_H) \\\n+   except.h $(TREE_H)\n caller-save.o : caller-save.c $(CONFIG_H) $(SYSTEM_H) coretypes.h $(TM_H) $(RTL_H) \\\n    $(FLAGS_H) $(REGS_H) hard-reg-set.h insn-config.h $(BASIC_BLOCK_H) function.h \\\n    $(RECOG_H) reload.h $(EXPR_H) toplev.h $(TM_P_H)"}, {"sha": "47708e21cb4d9ec3c519a1ab1f126b586936ad95", "filename": "gcc/cse.c", "status": "modified", "additions": 215, "deletions": 189, "changes": 404, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2Fcse.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2Fcse.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcse.c?ref=0516f6fe82641daf7c1ac8812998049ac591201e", "patch": "@@ -489,6 +489,12 @@ struct table_elt\n   ? (((unsigned) REG << 7) + (unsigned) REG_QTY (REGNO (X)))\t\\\n   : canon_hash (X, M)) & HASH_MASK)\n \n+/* Like HASH, but without side-effects.  */\n+#define SAFE_HASH(X, M)\t\\\n+ ((REG_P (X) && REGNO (X) >= FIRST_PSEUDO_REGISTER\t\\\n+  ? (((unsigned) REG << 7) + (unsigned) REG_QTY (REGNO (X)))\t\\\n+  : safe_hash (X, M)) & HASH_MASK)\n+\n /* Determine whether register number N is considered a fixed register for the\n    purpose of approximating register costs.\n    It is desirable to replace other regs with fixed regs, to reduce need for\n@@ -625,10 +631,11 @@ static void rehash_using_reg (rtx);\n static void invalidate_memory (void);\n static void invalidate_for_call (void);\n static rtx use_related_value (rtx, struct table_elt *);\n-static unsigned canon_hash (rtx, enum machine_mode);\n-static unsigned canon_hash_string (const char *);\n-static unsigned safe_hash (rtx, enum machine_mode);\n-static int exp_equiv_p (rtx, rtx, int, int);\n+\n+static inline unsigned canon_hash (rtx, enum machine_mode);\n+static inline unsigned safe_hash (rtx, enum machine_mode);\n+static unsigned hash_rtx_string (const char *);\n+\n static rtx canon_reg (rtx, rtx);\n static void find_best_addr (rtx, rtx *, enum machine_mode);\n static enum rtx_code find_comparison_args (enum rtx_code, rtx *, rtx *,\n@@ -1324,7 +1331,7 @@ lookup (rtx x, unsigned int hash, enum machine_mode mode)\n \n   for (p = table[hash]; p; p = p->next_same_hash)\n     if (mode == p->mode && ((x == p->exp && REG_P (x))\n-\t\t\t    || exp_equiv_p (x, p->exp, !REG_P (x), 0)))\n+\t\t\t    || exp_equiv_p (x, p->exp, !REG_P (x), false)))\n       return p;\n \n   return 0;\n@@ -1352,7 +1359,8 @@ lookup_for_remove (rtx x, unsigned int hash, enum machine_mode mode)\n   else\n     {\n       for (p = table[hash]; p; p = p->next_same_hash)\n-\tif (mode == p->mode && (x == p->exp || exp_equiv_p (x, p->exp, 0, 0)))\n+\tif (mode == p->mode\n+\t    && (x == p->exp || exp_equiv_p (x, p->exp, 0, false)))\n \t  return p;\n     }\n \n@@ -1366,7 +1374,7 @@ static rtx\n lookup_as_function (rtx x, enum rtx_code code)\n {\n   struct table_elt *p\n-    = lookup (x, safe_hash (x, VOIDmode) & HASH_MASK, GET_MODE (x));\n+    = lookup (x, SAFE_HASH (x, VOIDmode), GET_MODE (x));\n \n   /* If we are looking for a CONST_INT, the mode doesn't really matter, as\n      long as we are narrowing.  So if we looked in vain for a mode narrower\n@@ -1376,7 +1384,7 @@ lookup_as_function (rtx x, enum rtx_code code)\n     {\n       x = copy_rtx (x);\n       PUT_MODE (x, word_mode);\n-      p = lookup (x, safe_hash (x, VOIDmode) & HASH_MASK, word_mode);\n+      p = lookup (x, SAFE_HASH (x, VOIDmode), word_mode);\n     }\n \n   if (p == 0)\n@@ -1385,7 +1393,7 @@ lookup_as_function (rtx x, enum rtx_code code)\n   for (p = p->first_same_value; p; p = p->next_same_value)\n     if (GET_CODE (p->exp) == code\n \t/* Make sure this is a valid entry in the table.  */\n-\t&& exp_equiv_p (p->exp, p->exp, 1, 0))\n+\t&& exp_equiv_p (p->exp, p->exp, 1, false))\n       return p->exp;\n \n   return 0;\n@@ -1568,7 +1576,7 @@ insert (rtx x, struct table_elt *classp, unsigned int hash, enum machine_mode mo\n       if (subexp != 0)\n \t{\n \t  /* Get the integer-free subexpression in the hash table.  */\n-\t  subhash = safe_hash (subexp, mode) & HASH_MASK;\n+\t  subhash = SAFE_HASH (subexp, mode);\n \t  subelt = lookup (subexp, subhash, mode);\n \t  if (subelt == 0)\n \t    subelt = insert (subexp, NULL, subhash, mode);\n@@ -1622,7 +1630,7 @@ merge_equiv_classes (struct table_elt *class1, struct table_elt *class2)\n       /* Remove old entry, make a new one in CLASS1's class.\n \t Don't do this for invalid entries as we cannot find their\n \t hash code (it also isn't necessary).  */\n-      if (REG_P (exp) || exp_equiv_p (exp, exp, 1, 0))\n+      if (REG_P (exp) || exp_equiv_p (exp, exp, 1, false))\n \t{\n \t  bool need_rehash = false;\n \n@@ -1917,8 +1925,8 @@ rehash_using_reg (rtx x)\n       {\n \tnext = p->next_same_hash;\n \tif (reg_mentioned_p (x, p->exp)\n-\t    && exp_equiv_p (p->exp, p->exp, 1, 0)\n-\t    && i != (hash = safe_hash (p->exp, p->mode) & HASH_MASK))\n+\t    && exp_equiv_p (p->exp, p->exp, 1, false)\n+\t    && i != (hash = SAFE_HASH (p->exp, p->mode)))\n \t  {\n \t    if (p->next_same_hash)\n \t      p->next_same_hash->prev_same_hash = p->prev_same_hash;\n@@ -2017,7 +2025,7 @@ use_related_value (rtx x, struct table_elt *elt)\n       rtx subexp = get_related_value (x);\n       if (subexp != 0)\n \trelt = lookup (subexp,\n-\t\t       safe_hash (subexp, GET_MODE (subexp)) & HASH_MASK,\n+\t\t       SAFE_HASH (subexp, GET_MODE (subexp)),\n \t\t       GET_MODE (subexp));\n     }\n \n@@ -2068,7 +2076,7 @@ use_related_value (rtx x, struct table_elt *elt)\n \f\n /* Hash a string.  Just add its bytes up.  */\n static inline unsigned\n-canon_hash_string (const char *ps)\n+hash_rtx_string (const char *ps)\n {\n   unsigned hash = 0;\n   const unsigned char *p = (const unsigned char *) ps;\n@@ -2085,23 +2093,26 @@ canon_hash_string (const char *ps)\n    MODE is used in hashing for CONST_INTs only;\n    otherwise the mode of X is used.\n \n-   Store 1 in do_not_record if any subexpression is volatile.\n+   Store 1 in DO_NOT_RECORD_P if any subexpression is volatile.\n \n-   Store 1 in hash_arg_in_memory if X contains a MEM rtx\n-   which does not have the MEM_READONLY_P bit set.\n+   If HASH_ARG_IN_MEMORY_P is not NULL, store 1 in it if X contains\n+   a MEM rtx which does not have the RTX_UNCHANGING_P bit set.\n \n    Note that cse_insn knows that the hash code of a MEM expression\n    is just (int) MEM plus the hash code of the address.  */\n \n-static unsigned\n-canon_hash (rtx x, enum machine_mode mode)\n+unsigned\n+hash_rtx (rtx x, enum machine_mode mode, int *do_not_record_p,\n+\t  int *hash_arg_in_memory_p, bool have_reg_qty)\n {\n   int i, j;\n   unsigned hash = 0;\n   enum rtx_code code;\n   const char *fmt;\n \n-  /* repeat is used to turn tail-recursion into iteration.  */\n+  /* Used to turn recursion into iteration.  We can't rely on GCC's\n+     tail-recursion elimination since we need to keep accumulating values\n+     in HASH.  */\n  repeat:\n   if (x == 0)\n     return hash;\n@@ -2112,48 +2123,52 @@ canon_hash (rtx x, enum machine_mode mode)\n     case REG:\n       {\n \tunsigned int regno = REGNO (x);\n-\tbool record;\n \n-\t/* On some machines, we can't record any non-fixed hard register,\n-\t   because extending its life will cause reload problems.  We\n-\t   consider ap, fp, sp, gp to be fixed for this purpose.\n-\n-\t   We also consider CCmode registers to be fixed for this purpose;\n-\t   failure to do so leads to failure to simplify 0<100 type of\n-\t   conditionals.\n-\n-\t   On all machines, we can't record any global registers.\n-\t   Nor should we record any register that is in a small\n-\t   class, as defined by CLASS_LIKELY_SPILLED_P.  */\n-\n-\tif (regno >= FIRST_PSEUDO_REGISTER)\n-\t  record = true;\n-\telse if (x == frame_pointer_rtx\n-\t\t || x == hard_frame_pointer_rtx\n-\t\t || x == arg_pointer_rtx\n-\t\t || x == stack_pointer_rtx\n-\t\t || x == pic_offset_table_rtx)\n-\t  record = true;\n-\telse if (global_regs[regno])\n-\t  record = false;\n-\telse if (fixed_regs[regno])\n-\t  record = true;\n-\telse if (GET_MODE_CLASS (GET_MODE (x)) == MODE_CC)\n-\t  record = true;\n-\telse if (SMALL_REGISTER_CLASSES)\n-\t  record = false;\n-\telse if (CLASS_LIKELY_SPILLED_P (REGNO_REG_CLASS (regno)))\n-\t  record = false;\n-\telse\n-\t  record = true;\n-\n-\tif (!record)\n+\tif (!reload_completed)\n \t  {\n-\t    do_not_record = 1;\n-\t    return 0;\n+\t    /* On some machines, we can't record any non-fixed hard register,\n+\t       because extending its life will cause reload problems.  We\n+\t       consider ap, fp, sp, gp to be fixed for this purpose.\n+\n+\t       We also consider CCmode registers to be fixed for this purpose;\n+\t       failure to do so leads to failure to simplify 0<100 type of\n+\t       conditionals.\n+\n+\t       On all machines, we can't record any global registers.\n+\t       Nor should we record any register that is in a small\n+\t       class, as defined by CLASS_LIKELY_SPILLED_P.  */\n+\t    bool record;\n+\n+\t    if (regno >= FIRST_PSEUDO_REGISTER)\n+\t      record = true;\n+\t    else if (x == frame_pointer_rtx\n+\t\t     || x == hard_frame_pointer_rtx\n+\t\t     || x == arg_pointer_rtx\n+\t\t     || x == stack_pointer_rtx\n+\t\t     || x == pic_offset_table_rtx)\n+\t      record = true;\n+\t    else if (global_regs[regno])\n+\t      record = false;\n+\t    else if (fixed_regs[regno])\n+\t      record = true;\n+\t    else if (GET_MODE_CLASS (GET_MODE (x)) == MODE_CC)\n+\t      record = true;\n+\t    else if (SMALL_REGISTER_CLASSES)\n+\t      record = false;\n+\t    else if (CLASS_LIKELY_SPILLED_P (REGNO_REG_CLASS (regno)))\n+\t      record = false;\n+\t    else\n+\t      record = true;\n+\n+\t    if (!record)\n+\t      {\n+\t\t*do_not_record_p = 1;\n+\t\treturn 0;\n+\t      }\n \t  }\n \n-\thash += ((unsigned) REG << 7) + (unsigned) REG_QTY (regno);\n+\thash += ((unsigned int) REG << 7);\n+        hash += (have_reg_qty ? (unsigned) REG_QTY (regno) : regno);\n \treturn hash;\n       }\n \n@@ -2164,7 +2179,7 @@ canon_hash (rtx x, enum machine_mode mode)\n       {\n \tif (REG_P (SUBREG_REG (x)))\n \t  {\n-\t    hash += (((unsigned) SUBREG << 7)\n+\t    hash += (((unsigned int) SUBREG << 7)\n \t\t     + REGNO (SUBREG_REG (x))\n \t\t     + (SUBREG_BYTE (x) / UNITS_PER_WORD));\n \t    return hash;\n@@ -2173,21 +2188,19 @@ canon_hash (rtx x, enum machine_mode mode)\n       }\n \n     case CONST_INT:\n-      {\n-\tunsigned HOST_WIDE_INT tem = INTVAL (x);\n-\thash += ((unsigned) CONST_INT << 7) + (unsigned) mode + tem;\n-\treturn hash;\n-      }\n+      hash += (((unsigned int) CONST_INT << 7) + (unsigned int) mode\n+               + (unsigned int) INTVAL (x));\n+      return hash;\n \n     case CONST_DOUBLE:\n       /* This is like the general case, except that it only counts\n \t the integers representing the constant.  */\n-      hash += (unsigned) code + (unsigned) GET_MODE (x);\n+      hash += (unsigned int) code + (unsigned int) GET_MODE (x);\n       if (GET_MODE (x) != VOIDmode)\n \thash += real_hash (CONST_DOUBLE_REAL_VALUE (x));\n       else\n-\thash += ((unsigned) CONST_DOUBLE_LOW (x)\n-\t\t + (unsigned) CONST_DOUBLE_HIGH (x));\n+\thash += ((unsigned int) CONST_DOUBLE_LOW (x)\n+\t\t + (unsigned int) CONST_DOUBLE_HIGH (x));\n       return hash;\n \n     case CONST_VECTOR:\n@@ -2200,31 +2213,48 @@ canon_hash (rtx x, enum machine_mode mode)\n \tfor (i = 0; i < units; ++i)\n \t  {\n \t    elt = CONST_VECTOR_ELT (x, i);\n-\t    hash += canon_hash (elt, GET_MODE (elt));\n+\t    hash += hash_rtx (elt, GET_MODE (elt), do_not_record_p,\n+\t\t\t      hash_arg_in_memory_p, have_reg_qty);\n \t  }\n \n \treturn hash;\n       }\n \n       /* Assume there is only one rtx object for any given label.  */\n     case LABEL_REF:\n-      hash += ((unsigned) LABEL_REF << 7) + (unsigned long) XEXP (x, 0);\n+      /* We don't hash on the address of the CODE_LABEL to avoid bootstrap\n+\t differences and differences between each stage's debugging dumps.  */\n+\t hash += (((unsigned int) LABEL_REF << 7)\n+\t\t  + CODE_LABEL_NUMBER (XEXP (x, 0)));\n       return hash;\n \n     case SYMBOL_REF:\n-      hash += ((unsigned) SYMBOL_REF << 7) + (unsigned long) XSTR (x, 0);\n-      return hash;\n+      {\n+\t/* Don't hash on the symbol's address to avoid bootstrap differences.\n+\t   Different hash values may cause expressions to be recorded in\n+\t   different orders and thus different registers to be used in the\n+\t   final assembler.  This also avoids differences in the dump files\n+\t   between various stages.  */\n+\tunsigned int h = 0;\n+\tconst unsigned char *p = (const unsigned char *) XSTR (x, 0);\n+\n+\twhile (*p)\n+\t  h += (h << 7) + *p++; /* ??? revisit */\n+\n+\thash += ((unsigned int) SYMBOL_REF << 7) + h;\n+\treturn hash;\n+      }\n \n     case MEM:\n       /* We don't record if marked volatile or if BLKmode since we don't\n \t know the size of the move.  */\n       if (MEM_VOLATILE_P (x) || GET_MODE (x) == BLKmode)\n \t{\n-\t  do_not_record = 1;\n+\t  *do_not_record_p = 1;\n \t  return 0;\n \t}\n-      if (!MEM_READONLY_P (x))\n-\thash_arg_in_memory = 1;\n+      if (hash_arg_in_memory_p && !MEM_READONLY_P (x))\n+\t*hash_arg_in_memory_p = 1;\n \n       /* Now that we have already found this special case,\n \t might as well speed it up as much as possible.  */\n@@ -2236,15 +2266,16 @@ canon_hash (rtx x, enum machine_mode mode)\n       /* A USE that mentions non-volatile memory needs special\n \t handling since the MEM may be BLKmode which normally\n \t prevents an entry from being made.  Pure calls are\n-\t marked by a USE which mentions BLKmode memory.  */\n+\t marked by a USE which mentions BLKmode memory.\n+\t See calls.c:emit_call_1.  */\n       if (MEM_P (XEXP (x, 0))\n \t  && ! MEM_VOLATILE_P (XEXP (x, 0)))\n \t{\n \t  hash += (unsigned) USE;\n \t  x = XEXP (x, 0);\n \n-\t  if (!MEM_READONLY_P (x))\n-\t    hash_arg_in_memory = 1;\n+\t  if (hash_arg_in_memory_p && !MEM_READONLY_P (x))\n+\t    *hash_arg_in_memory_p = 1;\n \n \t  /* Now that we have already found this special case,\n \t     might as well speed it up as much as possible.  */\n@@ -2264,34 +2295,36 @@ canon_hash (rtx x, enum machine_mode mode)\n     case CC0:\n     case CALL:\n     case UNSPEC_VOLATILE:\n-      do_not_record = 1;\n+      *do_not_record_p = 1;\n       return 0;\n \n     case ASM_OPERANDS:\n       if (MEM_VOLATILE_P (x))\n \t{\n-\t  do_not_record = 1;\n+\t  *do_not_record_p = 1;\n \t  return 0;\n \t}\n       else\n \t{\n \t  /* We don't want to take the filename and line into account.  */\n \t  hash += (unsigned) code + (unsigned) GET_MODE (x)\n-\t    + canon_hash_string (ASM_OPERANDS_TEMPLATE (x))\n-\t    + canon_hash_string (ASM_OPERANDS_OUTPUT_CONSTRAINT (x))\n+\t    + hash_rtx_string (ASM_OPERANDS_TEMPLATE (x))\n+\t    + hash_rtx_string (ASM_OPERANDS_OUTPUT_CONSTRAINT (x))\n \t    + (unsigned) ASM_OPERANDS_OUTPUT_IDX (x);\n \n \t  if (ASM_OPERANDS_INPUT_LENGTH (x))\n \t    {\n \t      for (i = 1; i < ASM_OPERANDS_INPUT_LENGTH (x); i++)\n \t\t{\n-\t\t  hash += (canon_hash (ASM_OPERANDS_INPUT (x, i),\n-\t\t\t\t       GET_MODE (ASM_OPERANDS_INPUT (x, i)))\n-\t\t\t   + canon_hash_string (ASM_OPERANDS_INPUT_CONSTRAINT\n-\t\t\t\t\t\t(x, i)));\n+\t\t  hash += (hash_rtx (ASM_OPERANDS_INPUT (x, i),\n+\t\t\t\t     GET_MODE (ASM_OPERANDS_INPUT (x, i)),\n+\t\t\t\t     do_not_record_p, hash_arg_in_memory_p,\n+\t\t\t\t     have_reg_qty)\n+\t\t\t   + hash_rtx_string\n+\t\t\t\t(ASM_OPERANDS_INPUT_CONSTRAINT (x, i)));\n \t\t}\n \n-\t      hash += canon_hash_string (ASM_OPERANDS_INPUT_CONSTRAINT (x, 0));\n+\t      hash += hash_rtx_string (ASM_OPERANDS_INPUT_CONSTRAINT (x, 0));\n \t      x = ASM_OPERANDS_INPUT (x, 0);\n \t      mode = GET_MODE (x);\n \t      goto repeat;\n@@ -2312,48 +2345,59 @@ canon_hash (rtx x, enum machine_mode mode)\n     {\n       if (fmt[i] == 'e')\n \t{\n-\t  rtx tem = XEXP (x, i);\n-\n \t  /* If we are about to do the last recursive call\n \t     needed at this level, change it into iteration.\n \t     This function  is called enough to be worth it.  */\n \t  if (i == 0)\n \t    {\n-\t      x = tem;\n+\t      x = XEXP (x, i);\n \t      goto repeat;\n \t    }\n-\t  hash += canon_hash (tem, 0);\n+\n+\t  hash += hash_rtx (XEXP (x, i), 0, do_not_record_p,\n+\t\t\t    hash_arg_in_memory_p, have_reg_qty);\n \t}\n+\n       else if (fmt[i] == 'E')\n \tfor (j = 0; j < XVECLEN (x, i); j++)\n-\t  hash += canon_hash (XVECEXP (x, i, j), 0);\n+\t  {\n+\t    hash += hash_rtx (XVECEXP (x, i, j), 0, do_not_record_p,\n+\t\t\t      hash_arg_in_memory_p, have_reg_qty);\n+\t  }\n+\n       else if (fmt[i] == 's')\n-\thash += canon_hash_string (XSTR (x, i));\n+\thash += hash_rtx_string (XSTR (x, i));\n       else if (fmt[i] == 'i')\n-\t{\n-\t  unsigned tem = XINT (x, i);\n-\t  hash += tem;\n-\t}\n+\thash += (unsigned int) XINT (x, i);\n       else if (fmt[i] == '0' || fmt[i] == 't')\n \t/* Unused.  */\n \t;\n       else\n \tabort ();\n     }\n+\n   return hash;\n }\n \n-/* Like canon_hash but with no side effects.  */\n+/* Hash an rtx X for cse via hash_rtx.\n+   Stores 1 in do_not_record if any subexpression is volatile.\n+   Stores 1 in hash_arg_in_memory if X contains a mem rtx which\n+   does not have the RTX_UNCHANGING_P bit set.  */\n+\n+static inline unsigned\n+canon_hash (rtx x, enum machine_mode mode)\n+{\n+  return hash_rtx (x, mode, &do_not_record, &hash_arg_in_memory, true);\n+}\n+\n+/* Like canon_hash but with no side effects, i.e. do_not_record\n+   and hash_arg_in_memory are not changed.  */\n \n-static unsigned\n+static inline unsigned\n safe_hash (rtx x, enum machine_mode mode)\n {\n-  int save_do_not_record = do_not_record;\n-  int save_hash_arg_in_memory = hash_arg_in_memory;\n-  unsigned hash = canon_hash (x, mode);\n-  hash_arg_in_memory = save_hash_arg_in_memory;\n-  do_not_record = save_do_not_record;\n-  return hash;\n+  int dummy_do_not_record;\n+  return hash_rtx (x, mode, &dummy_do_not_record, NULL, true);\n }\n \f\n /* Return 1 iff X and Y would canonicalize into the same thing,\n@@ -2363,16 +2407,10 @@ safe_hash (rtx x, enum machine_mode mode)\n    and Y was found in the hash table.  We check register refs\n    in Y for being marked as valid.\n \n-   If EQUAL_VALUES is nonzero, we allow a register to match a constant value\n-   that is known to be in the register.  Ordinarily, we don't allow them\n-   to match, because letting them match would cause unpredictable results\n-   in all the places that search a hash table chain for an equivalent\n-   for a given value.  A possible equivalent that has different structure\n-   has its hash code computed from different data.  Whether the hash code\n-   is the same as that of the given value is pure luck.  */\n+   If FOR_GCSE is true, we compare X and Y for equivalence for GCSE.  */\n \n-static int\n-exp_equiv_p (rtx x, rtx y, int validate, int equal_values)\n+int\n+exp_equiv_p (rtx x, rtx y, int validate, bool for_gcse)\n {\n   int i, j;\n   enum rtx_code code;\n@@ -2382,42 +2420,13 @@ exp_equiv_p (rtx x, rtx y, int validate, int equal_values)\n      if VALIDATE is nonzero.  */\n   if (x == y && !validate)\n     return 1;\n+\n   if (x == 0 || y == 0)\n     return x == y;\n \n   code = GET_CODE (x);\n   if (code != GET_CODE (y))\n-    {\n-      if (!equal_values)\n-\treturn 0;\n-\n-      /* If X is a constant and Y is a register or vice versa, they may be\n-\t equivalent.  We only have to validate if Y is a register.  */\n-      if (CONSTANT_P (x) && REG_P (y)\n-\t  && REGNO_QTY_VALID_P (REGNO (y)))\n-\t{\n-\t  int y_q = REG_QTY (REGNO (y));\n-\t  struct qty_table_elem *y_ent = &qty_table[y_q];\n-\n-\t  if (GET_MODE (y) == y_ent->mode\n-\t      && rtx_equal_p (x, y_ent->const_rtx)\n-\t      && (! validate || REG_IN_TABLE (REGNO (y)) == REG_TICK (REGNO (y))))\n-\t    return 1;\n-\t}\n-\n-      if (CONSTANT_P (y) && code == REG\n-\t  && REGNO_QTY_VALID_P (REGNO (x)))\n-\t{\n-\t  int x_q = REG_QTY (REGNO (x));\n-\t  struct qty_table_elem *x_ent = &qty_table[x_q];\n-\n-\t  if (GET_MODE (x) == x_ent->mode\n-\t      && rtx_equal_p (y, x_ent->const_rtx))\n-\t    return 1;\n-\t}\n-\n-      return 0;\n-    }\n+    return 0;\n \n   /* (MULT:SI x y) and (MULT:HI x y) are NOT equivalent.  */\n   if (GET_MODE (x) != GET_MODE (y))\n@@ -2437,29 +2446,48 @@ exp_equiv_p (rtx x, rtx y, int validate, int equal_values)\n       return XSTR (x, 0) == XSTR (y, 0);\n \n     case REG:\n-      {\n-\tunsigned int regno = REGNO (y);\n-\tunsigned int endregno\n-\t  = regno + (regno >= FIRST_PSEUDO_REGISTER ? 1\n-\t\t     : hard_regno_nregs[regno][GET_MODE (y)]);\n-\tunsigned int i;\n+      if (for_gcse)\n+\treturn REGNO (x) == REGNO (y);\n+      else\n+\t{\n+\t  unsigned int regno = REGNO (y);\n+\t  unsigned int i;\n+\t  unsigned int endregno\n+\t    = regno + (regno >= FIRST_PSEUDO_REGISTER ? 1\n+\t\t       : hard_regno_nregs[regno][GET_MODE (y)]);\n \n-\t/* If the quantities are not the same, the expressions are not\n-\t   equivalent.  If there are and we are not to validate, they\n-\t   are equivalent.  Otherwise, ensure all regs are up-to-date.  */\n+\t  /* If the quantities are not the same, the expressions are not\n+\t     equivalent.  If there are and we are not to validate, they\n+\t     are equivalent.  Otherwise, ensure all regs are up-to-date.  */\n \n-\tif (REG_QTY (REGNO (x)) != REG_QTY (regno))\n-\t  return 0;\n+\t  if (REG_QTY (REGNO (x)) != REG_QTY (regno))\n+\t    return 0;\n+\n+\t  if (! validate)\n+\t    return 1;\n+\n+\t  for (i = regno; i < endregno; i++)\n+\t    if (REG_IN_TABLE (i) != REG_TICK (i))\n+\t      return 0;\n \n-\tif (! validate)\n \t  return 1;\n+\t}\n \n-\tfor (i = regno; i < endregno; i++)\n-\t  if (REG_IN_TABLE (i) != REG_TICK (i))\n+    case MEM:\n+      if (for_gcse)\n+\t{\n+\t  /* Can't merge two expressions in different alias sets, since we\n+\t     can decide that the expression is transparent in a block when\n+\t     it isn't, due to it being set with the different alias set.  */\n+\t  if (MEM_ALIAS_SET (x) != MEM_ALIAS_SET (y))\n \t    return 0;\n \n-\treturn 1;\n-      }\n+\t  /* A volatile mem should not be considered equivalent to any\n+\t     other.  */\n+\t  if (MEM_VOLATILE_P (x) || MEM_VOLATILE_P (y))\n+\t    return 0;\n+\t}\n+      break;\n \n     /*  For commutative operations, check both orders.  */\n     case PLUS:\n@@ -2469,13 +2497,14 @@ exp_equiv_p (rtx x, rtx y, int validate, int equal_values)\n     case XOR:\n     case NE:\n     case EQ:\n-      return ((exp_equiv_p (XEXP (x, 0), XEXP (y, 0), validate, equal_values)\n+      return ((exp_equiv_p (XEXP (x, 0), XEXP (y, 0),\n+\t\t\t     validate, for_gcse)\n \t       && exp_equiv_p (XEXP (x, 1), XEXP (y, 1),\n-\t\t\t       validate, equal_values))\n+\t\t\t\tvalidate, for_gcse))\n \t      || (exp_equiv_p (XEXP (x, 0), XEXP (y, 1),\n-\t\t\t       validate, equal_values)\n+\t\t\t\tvalidate, for_gcse)\n \t\t  && exp_equiv_p (XEXP (x, 1), XEXP (y, 0),\n-\t\t\t\t  validate, equal_values)));\n+\t\t\t\t   validate, for_gcse)));\n \n     case ASM_OPERANDS:\n       /* We don't use the generic code below because we want to\n@@ -2498,7 +2527,7 @@ exp_equiv_p (rtx x, rtx y, int validate, int equal_values)\n \t  for (i = ASM_OPERANDS_INPUT_LENGTH (x) - 1; i >= 0; i--)\n \t    if (! exp_equiv_p (ASM_OPERANDS_INPUT (x, i),\n \t\t\t       ASM_OPERANDS_INPUT (y, i),\n-\t\t\t       validate, equal_values)\n+\t\t\t       validate, for_gcse)\n \t\t|| strcmp (ASM_OPERANDS_INPUT_CONSTRAINT (x, i),\n \t\t\t   ASM_OPERANDS_INPUT_CONSTRAINT (y, i)))\n \t      return 0;\n@@ -2511,15 +2540,16 @@ exp_equiv_p (rtx x, rtx y, int validate, int equal_values)\n     }\n \n   /* Compare the elements.  If any pair of corresponding elements\n-     fail to match, return 0 for the whole things.  */\n+     fail to match, return 0 for the whole thing.  */\n \n   fmt = GET_RTX_FORMAT (code);\n   for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n     {\n       switch (fmt[i])\n \t{\n \tcase 'e':\n-\t  if (! exp_equiv_p (XEXP (x, i), XEXP (y, i), validate, equal_values))\n+\t  if (! exp_equiv_p (XEXP (x, i), XEXP (y, i),\n+\t\t\t      validate, for_gcse))\n \t    return 0;\n \t  break;\n \n@@ -2528,7 +2558,7 @@ exp_equiv_p (rtx x, rtx y, int validate, int equal_values)\n \t    return 0;\n \t  for (j = 0; j < XVECLEN (x, i); j++)\n \t    if (! exp_equiv_p (XVECEXP (x, i, j), XVECEXP (y, i, j),\n-\t\t\t       validate, equal_values))\n+\t\t\t\tvalidate, for_gcse))\n \t      return 0;\n \t  break;\n \n@@ -2827,7 +2857,7 @@ find_best_addr (rtx insn, rtx *loc, enum machine_mode mode)\n \t    if (! p->flag)\n \t      {\n \t\tif ((REG_P (p->exp)\n-\t\t     || exp_equiv_p (p->exp, p->exp, 1, 0))\n+\t\t     || exp_equiv_p (p->exp, p->exp, 1, false))\n \t\t    && ((exp_cost = address_cost (p->exp, mode)) < best_addr_cost\n \t\t\t|| (exp_cost == best_addr_cost\n \t\t\t    && ((p->cost + 1) >> 1) > best_rtx_cost)))\n@@ -2903,7 +2933,7 @@ find_best_addr (rtx insn, rtx *loc, enum machine_mode mode)\n \t       p = p->next_same_value, count++)\n \t    if (! p->flag\n \t\t&& (REG_P (p->exp)\n-\t\t    || exp_equiv_p (p->exp, p->exp, 1, 0)))\n+\t\t    || exp_equiv_p (p->exp, p->exp, 1, false)))\n \t      {\n \t\trtx new = simplify_gen_binary (GET_CODE (*loc), Pmode,\n \t\t\t\t\t       p->exp, op1);\n@@ -3012,8 +3042,7 @@ find_comparison_args (enum rtx_code code, rtx *parg1, rtx *parg2,\n       if (x == 0)\n \t/* Look up ARG1 in the hash table and see if it has an equivalence\n \t   that lets us see what is being compared.  */\n-\tp = lookup (arg1, safe_hash (arg1, GET_MODE (arg1)) & HASH_MASK,\n-\t\t    GET_MODE (arg1));\n+\tp = lookup (arg1, SAFE_HASH (arg1, GET_MODE (arg1)), GET_MODE (arg1));\n       if (p)\n \t{\n \t  p = p->first_same_value;\n@@ -3038,7 +3067,7 @@ find_comparison_args (enum rtx_code code, rtx *parg1, rtx *parg2,\n #endif\n \n \t  /* If the entry isn't valid, skip it.  */\n-\t  if (! exp_equiv_p (p->exp, p->exp, 1, 0))\n+\t  if (! exp_equiv_p (p->exp, p->exp, 1, false))\n \t    continue;\n \n \t  if (GET_CODE (p->exp) == COMPARE\n@@ -3235,7 +3264,7 @@ fold_rtx (rtx x, rtx insn)\n \n \t\tif (GET_CODE (elt->exp) == SUBREG\n \t\t    && GET_MODE (SUBREG_REG (elt->exp)) == mode\n-\t\t    && exp_equiv_p (elt->exp, elt->exp, 1, 0))\n+\t\t    && exp_equiv_p (elt->exp, elt->exp, 1, false))\n \t\t  return copy_rtx (SUBREG_REG (elt->exp));\n \t      }\n \n@@ -3264,8 +3293,6 @@ fold_rtx (rtx x, rtx insn)\n \t{\n \t  struct table_elt *elt;\n \n-\t  /* We can use HASH here since we know that canon_hash won't be\n-\t     called.  */\n \t  elt = lookup (folded_arg0,\n \t\t\tHASH (folded_arg0, GET_MODE (folded_arg0)),\n \t\t\tGET_MODE (folded_arg0));\n@@ -3370,7 +3397,7 @@ fold_rtx (rtx x, rtx insn)\n \t\t         && GET_MODE (SUBREG_REG (elt->exp)) == mode\n \t\t         && (GET_MODE_SIZE (GET_MODE (folded_arg0))\n \t\t\t     <= UNITS_PER_WORD)\n-\t\t         && exp_equiv_p (elt->exp, elt->exp, 1, 0))\n+\t\t         && exp_equiv_p (elt->exp, elt->exp, 1, false))\n \t\t  new = copy_rtx (SUBREG_REG (elt->exp));\n \n \t        if (new)\n@@ -3829,11 +3856,11 @@ fold_rtx (rtx x, rtx insn)\n \t\t      && (REG_QTY (REGNO (folded_arg0))\n \t\t\t  == REG_QTY (REGNO (folded_arg1))))\n \t\t  || ((p0 = lookup (folded_arg0,\n-\t\t\t\t    (safe_hash (folded_arg0, mode_arg0)\n-\t\t\t\t     & HASH_MASK), mode_arg0))\n+\t\t\t\t    SAFE_HASH (folded_arg0, mode_arg0),\n+\t\t\t\t    mode_arg0))\n \t\t      && (p1 = lookup (folded_arg1,\n-\t\t\t\t       (safe_hash (folded_arg1, mode_arg0)\n-\t\t\t\t\t& HASH_MASK), mode_arg0))\n+\t\t\t\t       SAFE_HASH (folded_arg1, mode_arg0),\n+\t\t\t\t       mode_arg0))\n \t\t      && p0->first_same_value == p1->first_same_value))\n \t\t{\n \t\t  /* Sadly two equal NaNs are not equivalent.  */\n@@ -4007,8 +4034,7 @@ fold_rtx (rtx x, rtx insn)\n \t    {\n \t      rtx new_const = GEN_INT (-INTVAL (const_arg1));\n \t      struct table_elt *p\n-\t\t= lookup (new_const, safe_hash (new_const, mode) & HASH_MASK,\n-\t\t\t  mode);\n+\t\t= lookup (new_const, SAFE_HASH (new_const, mode), mode);\n \n \t      if (p)\n \t\tfor (p = p->first_same_value; p; p = p->next_same_value)\n@@ -4195,7 +4221,7 @@ equiv_constant (rtx x)\n       if (CONSTANT_P (x))\n \treturn x;\n \n-      elt = lookup (x, safe_hash (x, GET_MODE (x)) & HASH_MASK, GET_MODE (x));\n+      elt = lookup (x, SAFE_HASH (x, GET_MODE (x)), GET_MODE (x));\n       if (elt == 0)\n \treturn 0;\n \n@@ -5182,7 +5208,7 @@ cse_insn (rtx insn, rtx libcall_insn)\n \t  /* If the expression is not valid, ignore it.  Then we do not\n \t     have to check for validity below.  In most cases, we can use\n \t     `rtx_equal_p', since canonicalization has already been done.  */\n-\t  if (code != REG && ! exp_equiv_p (p->exp, p->exp, 1, 0))\n+\t  if (code != REG && ! exp_equiv_p (p->exp, p->exp, 1, false))\n \t    continue;\n \n \t  /* Also skip paradoxical subregs, unless that's what we're\n@@ -5279,7 +5305,7 @@ cse_insn (rtx insn, rtx libcall_insn)\n \n \t  /* Skip invalid entries.  */\n \t  while (elt && !REG_P (elt->exp)\n-\t\t && ! exp_equiv_p (elt->exp, elt->exp, 1, 0))\n+\t\t && ! exp_equiv_p (elt->exp, elt->exp, 1, false))\n \t    elt = elt->next_same_value;\n \n \t  /* A paradoxical subreg would be bad here: it'll be the right\n@@ -6006,7 +6032,7 @@ cse_insn (rtx insn, rtx libcall_insn)\n \n \t\t/* Ignore invalid entries.  */\n \t\tif (!REG_P (elt->exp)\n-\t\t    && ! exp_equiv_p (elt->exp, elt->exp, 1, 0))\n+\t\t    && ! exp_equiv_p (elt->exp, elt->exp, 1, false))\n \t\t  continue;\n \n \t\t/* We may have already been playing subreg games.  If the\n@@ -6059,7 +6085,7 @@ cse_insn (rtx insn, rtx libcall_insn)\n \t\t/* Ignore invalid entries.  */\n \t\twhile (classp\n \t\t       && !REG_P (classp->exp)\n-\t\t       && ! exp_equiv_p (classp->exp, classp->exp, 1, 0))\n+\t\t       && ! exp_equiv_p (classp->exp, classp->exp, 1, false))\n \t\t  classp = classp->next_same_value;\n \t      }\n \t  }"}, {"sha": "bf50dcaf991234c83451598aae11371ef16e6dfa", "filename": "gcc/cselib.c", "status": "modified", "additions": 8, "deletions": 8, "changes": 16, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2Fcselib.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2Fcselib.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fcselib.c?ref=0516f6fe82641daf7c1ac8812998049ac591201e", "patch": "@@ -55,7 +55,7 @@ static int discard_useless_locs (void **, void *);\n static int discard_useless_values (void **, void *);\n static void remove_useless_values (void);\n static rtx wrap_constant (enum machine_mode, rtx);\n-static unsigned int hash_rtx (rtx, enum machine_mode, int);\n+static unsigned int cselib_hash_rtx (rtx, enum machine_mode, int);\n static cselib_val *new_cselib_val (unsigned int, enum machine_mode);\n static void add_mem_for_addr (cselib_val *, cselib_val *, rtx);\n static cselib_val *cselib_lookup_mem (rtx, int);\n@@ -257,8 +257,8 @@ entry_and_rtx_equal_p (const void *entry, const void *x_arg)\n }\n \n /* The hash function for our hash table.  The value is always computed with\n-   hash_rtx when adding an element; this function just extracts the hash\n-   value from a cselib_val structure.  */\n+   cselib_hash_rtx when adding an element; this function just extracts the\n+   hash value from a cselib_val structure.  */\n \n static hashval_t\n get_value_hash (const void *entry)\n@@ -554,7 +554,7 @@ wrap_constant (enum machine_mode mode, rtx x)\n    otherwise the mode of X is used.  */\n \n static unsigned int\n-hash_rtx (rtx x, enum machine_mode mode, int create)\n+cselib_hash_rtx (rtx x, enum machine_mode mode, int create)\n {\n   cselib_val *e;\n   int i, j;\n@@ -600,7 +600,7 @@ hash_rtx (rtx x, enum machine_mode mode, int create)\n \tfor (i = 0; i < units; ++i)\n \t  {\n \t    elt = CONST_VECTOR_ELT (x, i);\n-\t    hash += hash_rtx (elt, GET_MODE (elt), 0);\n+\t    hash += cselib_hash_rtx (elt, GET_MODE (elt), 0);\n \t  }\n \n \treturn hash;\n@@ -646,7 +646,7 @@ hash_rtx (rtx x, enum machine_mode mode, int create)\n       if (fmt[i] == 'e')\n \t{\n \t  rtx tem = XEXP (x, i);\n-\t  unsigned int tem_hash = hash_rtx (tem, 0, create);\n+\t  unsigned int tem_hash = cselib_hash_rtx (tem, 0, create);\n \n \t  if (tem_hash == 0)\n \t    return 0;\n@@ -656,7 +656,7 @@ hash_rtx (rtx x, enum machine_mode mode, int create)\n       else if (fmt[i] == 'E')\n \tfor (j = 0; j < XVECLEN (x, i); j++)\n \t  {\n-\t    unsigned int tem_hash = hash_rtx (XVECEXP (x, i, j), 0, create);\n+\t    unsigned int tem_hash = cselib_hash_rtx (XVECEXP (x, i, j), 0, create);\n \n \t    if (tem_hash == 0)\n \t      return 0;\n@@ -926,7 +926,7 @@ cselib_lookup (rtx x, enum machine_mode mode, int create)\n   if (MEM_P (x))\n     return cselib_lookup_mem (x, create);\n \n-  hashval = hash_rtx (x, mode, create);\n+  hashval = cselib_hash_rtx (x, mode, create);\n   /* Can't even create if hashing is not possible.  */\n   if (! hashval)\n     return 0;"}, {"sha": "16d76fe4d6c760cc9cfe9d9c7735f893ba347d43", "filename": "gcc/gcse.c", "status": "modified", "additions": 10, "deletions": 1018, "changes": 1028, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2Fgcse.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2Fgcse.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fgcse.c?ref=0516f6fe82641daf7c1ac8812998049ac591201e", "patch": "@@ -495,11 +495,12 @@ static sbitmap *reg_set_in_block;\n /* Array, indexed by basic block number for a list of insns which modify\n    memory within that block.  */\n static rtx * modify_mem_list;\n-bitmap modify_mem_list_set;\n+static bitmap modify_mem_list_set;\n \n /* This array parallels modify_mem_list, but is kept canonicalized.  */\n static rtx * canon_modify_mem_list;\n-bitmap canon_modify_mem_list_set;\n+static bitmap canon_modify_mem_list_set;\n+\n /* Various variables for statistics gathering.  */\n \n /* Memory used in a pass.\n@@ -564,8 +565,6 @@ static void insert_expr_in_table (rtx, enum machine_mode, rtx, int, int,\n \t\t\t\t  struct hash_table *);\n static void insert_set_in_table (rtx, rtx, struct hash_table *);\n static unsigned int hash_expr (rtx, enum machine_mode, int *, int);\n-static unsigned int hash_expr_1 (rtx, enum machine_mode, int *);\n-static unsigned int hash_string_1 (const char *);\n static unsigned int hash_set (int, int);\n static int expr_equiv_p (rtx, rtx);\n static void record_last_reg_set_info (rtx, int);\n@@ -576,7 +575,6 @@ static void alloc_hash_table (int, struct hash_table *, int);\n static void free_hash_table (struct hash_table *);\n static void compute_hash_table_work (struct hash_table *);\n static void dump_hash_table (FILE *, const char *, struct hash_table *);\n-static struct expr *lookup_expr (rtx, struct hash_table *);\n static struct expr *lookup_set (unsigned int, struct hash_table *);\n static struct expr *next_set (unsigned int, struct expr *);\n static void reset_opr_set_tables (void);\n@@ -1462,9 +1460,7 @@ oprs_available_p (rtx x, rtx insn)\n    MODE is only used if X is a CONST_INT.  DO_NOT_RECORD_P is a boolean\n    indicating if a volatile operand is found or if the expression contains\n    something we don't want to insert in the table.  HASH_TABLE_SIZE is\n-   the current size of the hash table to be probed.\n-\n-   ??? One might want to merge this with canon_hash.  Later.  */\n+   the current size of the hash table to be probed.  */\n \n static unsigned int\n hash_expr (rtx x, enum machine_mode mode, int *do_not_record_p,\n@@ -1474,208 +1470,11 @@ hash_expr (rtx x, enum machine_mode mode, int *do_not_record_p,\n \n   *do_not_record_p = 0;\n \n-  hash = hash_expr_1 (x, mode, do_not_record_p);\n+  hash = hash_rtx (x, mode, do_not_record_p,\n+\t\t   NULL,  /*have_reg_qty=*/false);\n   return hash % hash_table_size;\n }\n \n-/* Hash a string.  Just add its bytes up.  */\n-\n-static inline unsigned\n-hash_string_1 (const char *ps)\n-{\n-  unsigned hash = 0;\n-  const unsigned char *p = (const unsigned char *) ps;\n-\n-  if (p)\n-    while (*p)\n-      hash += *p++;\n-\n-  return hash;\n-}\n-\n-/* Subroutine of hash_expr to do the actual work.  */\n-\n-static unsigned int\n-hash_expr_1 (rtx x, enum machine_mode mode, int *do_not_record_p)\n-{\n-  int i, j;\n-  unsigned hash = 0;\n-  enum rtx_code code;\n-  const char *fmt;\n-\n-  if (x == 0)\n-    return hash;\n-\n-  /* Used to turn recursion into iteration.  We can't rely on GCC's\n-     tail-recursion elimination since we need to keep accumulating values\n-     in HASH.  */\n- repeat:\n-\n-  code = GET_CODE (x);\n-  switch (code)\n-    {\n-    case REG:\n-      hash += ((unsigned int) REG << 7) + REGNO (x);\n-      return hash;\n-\n-    case CONST_INT:\n-      hash += (((unsigned int) CONST_INT << 7) + (unsigned int) mode\n-\t       + (unsigned int) INTVAL (x));\n-      return hash;\n-\n-    case CONST_DOUBLE:\n-      /* This is like the general case, except that it only counts\n-\t the integers representing the constant.  */\n-      hash += (unsigned int) code + (unsigned int) GET_MODE (x);\n-      if (GET_MODE (x) != VOIDmode)\n-\tfor (i = 2; i < GET_RTX_LENGTH (CONST_DOUBLE); i++)\n-\t  hash += (unsigned int) XWINT (x, i);\n-      else\n-\thash += ((unsigned int) CONST_DOUBLE_LOW (x)\n-\t\t + (unsigned int) CONST_DOUBLE_HIGH (x));\n-      return hash;\n-\n-    case CONST_VECTOR:\n-      {\n-\tint units;\n-\trtx elt;\n-\n-\tunits = CONST_VECTOR_NUNITS (x);\n-\n-\tfor (i = 0; i < units; ++i)\n-\t  {\n-\t    elt = CONST_VECTOR_ELT (x, i);\n-\t    hash += hash_expr_1 (elt, GET_MODE (elt), do_not_record_p);\n-\t  }\n-\n-\treturn hash;\n-      }\n-\n-      /* Assume there is only one rtx object for any given label.  */\n-    case LABEL_REF:\n-      /* We don't hash on the address of the CODE_LABEL to avoid bootstrap\n-\t differences and differences between each stage's debugging dumps.  */\n-      hash += (((unsigned int) LABEL_REF << 7)\n-\t       + CODE_LABEL_NUMBER (XEXP (x, 0)));\n-      return hash;\n-\n-    case SYMBOL_REF:\n-      {\n-\t/* Don't hash on the symbol's address to avoid bootstrap differences.\n-\t   Different hash values may cause expressions to be recorded in\n-\t   different orders and thus different registers to be used in the\n-\t   final assembler.  This also avoids differences in the dump files\n-\t   between various stages.  */\n-\tunsigned int h = 0;\n-\tconst unsigned char *p = (const unsigned char *) XSTR (x, 0);\n-\n-\twhile (*p)\n-\t  h += (h << 7) + *p++; /* ??? revisit */\n-\n-\thash += ((unsigned int) SYMBOL_REF << 7) + h;\n-\treturn hash;\n-      }\n-\n-    case MEM:\n-      if (MEM_VOLATILE_P (x))\n-\t{\n-\t  *do_not_record_p = 1;\n-\t  return 0;\n-\t}\n-\n-      hash += (unsigned int) MEM;\n-      /* We used alias set for hashing, but this is not good, since the alias\n-\t set may differ in -fprofile-arcs and -fbranch-probabilities compilation\n-\t causing the profiles to fail to match.  */\n-      x = XEXP (x, 0);\n-      goto repeat;\n-\n-    case PRE_DEC:\n-    case PRE_INC:\n-    case POST_DEC:\n-    case POST_INC:\n-    case PC:\n-    case CC0:\n-    case CALL:\n-    case UNSPEC_VOLATILE:\n-      *do_not_record_p = 1;\n-      return 0;\n-\n-    case ASM_OPERANDS:\n-      if (MEM_VOLATILE_P (x))\n-\t{\n-\t  *do_not_record_p = 1;\n-\t  return 0;\n-\t}\n-      else\n-\t{\n-\t  /* We don't want to take the filename and line into account.  */\n-\t  hash += (unsigned) code + (unsigned) GET_MODE (x)\n-\t    + hash_string_1 (ASM_OPERANDS_TEMPLATE (x))\n-\t    + hash_string_1 (ASM_OPERANDS_OUTPUT_CONSTRAINT (x))\n-\t    + (unsigned) ASM_OPERANDS_OUTPUT_IDX (x);\n-\n-\t  if (ASM_OPERANDS_INPUT_LENGTH (x))\n-\t    {\n-\t      for (i = 1; i < ASM_OPERANDS_INPUT_LENGTH (x); i++)\n-\t\t{\n-\t\t  hash += (hash_expr_1 (ASM_OPERANDS_INPUT (x, i),\n-\t\t\t\t\tGET_MODE (ASM_OPERANDS_INPUT (x, i)),\n-\t\t\t\t\tdo_not_record_p)\n-\t\t\t   + hash_string_1 (ASM_OPERANDS_INPUT_CONSTRAINT\n-\t\t\t\t\t    (x, i)));\n-\t\t}\n-\n-\t      hash += hash_string_1 (ASM_OPERANDS_INPUT_CONSTRAINT (x, 0));\n-\t      x = ASM_OPERANDS_INPUT (x, 0);\n-\t      mode = GET_MODE (x);\n-\t      goto repeat;\n-\t    }\n-\t  return hash;\n-\t}\n-\n-    default:\n-      break;\n-    }\n-\n-  hash += (unsigned) code + (unsigned) GET_MODE (x);\n-  for (i = GET_RTX_LENGTH (code) - 1, fmt = GET_RTX_FORMAT (code); i >= 0; i--)\n-    {\n-      if (fmt[i] == 'e')\n-\t{\n-\t  /* If we are about to do the last recursive call\n-\t     needed at this level, change it into iteration.\n-\t     This function is called enough to be worth it.  */\n-\t  if (i == 0)\n-\t    {\n-\t      x = XEXP (x, i);\n-\t      goto repeat;\n-\t    }\n-\n-\t  hash += hash_expr_1 (XEXP (x, i), 0, do_not_record_p);\n-\t  if (*do_not_record_p)\n-\t    return 0;\n-\t}\n-\n-      else if (fmt[i] == 'E')\n-\tfor (j = 0; j < XVECLEN (x, i); j++)\n-\t  {\n-\t    hash += hash_expr_1 (XVECEXP (x, i, j), 0, do_not_record_p);\n-\t    if (*do_not_record_p)\n-\t      return 0;\n-\t  }\n-\n-      else if (fmt[i] == 's')\n-\thash += hash_string_1 (XSTR (x, i));\n-      else if (fmt[i] == 'i')\n-\thash += (unsigned int) XINT (x, i);\n-      else\n-\tabort ();\n-    }\n-\n-  return hash;\n-}\n-\n /* Hash a set of register REGNO.\n \n    Sets are hashed on the register that is set.  This simplifies the PRE copy\n@@ -1692,148 +1491,12 @@ hash_set (int regno, int hash_table_size)\n   return hash % hash_table_size;\n }\n \n-/* Return nonzero if exp1 is equivalent to exp2.\n-   ??? Borrowed from cse.c.  Might want to remerge with cse.c.  Later.  */\n+/* Return nonzero if exp1 is equivalent to exp2.  */\n \n static int\n expr_equiv_p (rtx x, rtx y)\n {\n-  int i, j;\n-  enum rtx_code code;\n-  const char *fmt;\n-\n-  if (x == y)\n-    return 1;\n-\n-  if (x == 0 || y == 0)\n-    return 0;\n-\n-  code = GET_CODE (x);\n-  if (code != GET_CODE (y))\n-    return 0;\n-\n-  /* (MULT:SI x y) and (MULT:HI x y) are NOT equivalent.  */\n-  if (GET_MODE (x) != GET_MODE (y))\n-    return 0;\n-\n-  switch (code)\n-    {\n-    case PC:\n-    case CC0:\n-    case CONST_INT:\n-      return 0;\n-\n-    case LABEL_REF:\n-      return XEXP (x, 0) == XEXP (y, 0);\n-\n-    case SYMBOL_REF:\n-      return XSTR (x, 0) == XSTR (y, 0);\n-\n-    case REG:\n-      return REGNO (x) == REGNO (y);\n-\n-    case MEM:\n-      /* Can't merge two expressions in different alias sets, since we can\n-\t decide that the expression is transparent in a block when it isn't,\n-\t due to it being set with the different alias set.  */\n-      if (MEM_ALIAS_SET (x) != MEM_ALIAS_SET (y))\n-\treturn 0;\n-\n-      /* A volatile mem should not be considered equivalent to any other.  */\n-      if (MEM_VOLATILE_P (x) || MEM_VOLATILE_P (y))\n-\treturn 0;\n-      break;\n-\n-    /*  For commutative operations, check both orders.  */\n-    case PLUS:\n-    case MULT:\n-    case AND:\n-    case IOR:\n-    case XOR:\n-    case NE:\n-    case EQ:\n-      return ((expr_equiv_p (XEXP (x, 0), XEXP (y, 0))\n-\t       && expr_equiv_p (XEXP (x, 1), XEXP (y, 1)))\n-\t      || (expr_equiv_p (XEXP (x, 0), XEXP (y, 1))\n-\t\t  && expr_equiv_p (XEXP (x, 1), XEXP (y, 0))));\n-\n-    case ASM_OPERANDS:\n-      /* We don't use the generic code below because we want to\n-\t disregard filename and line numbers.  */\n-\n-      /* A volatile asm isn't equivalent to any other.  */\n-      if (MEM_VOLATILE_P (x) || MEM_VOLATILE_P (y))\n-\treturn 0;\n-\n-      if (GET_MODE (x) != GET_MODE (y)\n-\t  || strcmp (ASM_OPERANDS_TEMPLATE (x), ASM_OPERANDS_TEMPLATE (y))\n-\t  || strcmp (ASM_OPERANDS_OUTPUT_CONSTRAINT (x),\n-\t\t     ASM_OPERANDS_OUTPUT_CONSTRAINT (y))\n-\t  || ASM_OPERANDS_OUTPUT_IDX (x) != ASM_OPERANDS_OUTPUT_IDX (y)\n-\t  || ASM_OPERANDS_INPUT_LENGTH (x) != ASM_OPERANDS_INPUT_LENGTH (y))\n-\treturn 0;\n-\n-      if (ASM_OPERANDS_INPUT_LENGTH (x))\n-\t{\n-\t  for (i = ASM_OPERANDS_INPUT_LENGTH (x) - 1; i >= 0; i--)\n-\t    if (! expr_equiv_p (ASM_OPERANDS_INPUT (x, i),\n-\t\t\t\tASM_OPERANDS_INPUT (y, i))\n-\t\t|| strcmp (ASM_OPERANDS_INPUT_CONSTRAINT (x, i),\n-\t\t\t   ASM_OPERANDS_INPUT_CONSTRAINT (y, i)))\n-\t      return 0;\n-\t}\n-\n-      return 1;\n-\n-    default:\n-      break;\n-    }\n-\n-  /* Compare the elements.  If any pair of corresponding elements\n-     fail to match, return 0 for the whole thing.  */\n-\n-  fmt = GET_RTX_FORMAT (code);\n-  for (i = GET_RTX_LENGTH (code) - 1; i >= 0; i--)\n-    {\n-      switch (fmt[i])\n-\t{\n-\tcase 'e':\n-\t  if (! expr_equiv_p (XEXP (x, i), XEXP (y, i)))\n-\t    return 0;\n-\t  break;\n-\n-\tcase 'E':\n-\t  if (XVECLEN (x, i) != XVECLEN (y, i))\n-\t    return 0;\n-\t  for (j = 0; j < XVECLEN (x, i); j++)\n-\t    if (! expr_equiv_p (XVECEXP (x, i, j), XVECEXP (y, i, j)))\n-\t      return 0;\n-\t  break;\n-\n-\tcase 's':\n-\t  if (strcmp (XSTR (x, i), XSTR (y, i)))\n-\t    return 0;\n-\t  break;\n-\n-\tcase 'i':\n-\t  if (XINT (x, i) != XINT (y, i))\n-\t    return 0;\n-\t  break;\n-\n-\tcase 'w':\n-\t  if (XWINT (x, i) != XWINT (y, i))\n-\t    return 0;\n-\tbreak;\n-\n-\tcase '0':\n-\t  break;\n-\n-\tdefault:\n-\t  abort ();\n-\t}\n-    }\n-\n-  return 1;\n+  return exp_equiv_p (x, y, 0, true);\n }\n \n /* Insert expression X in INSN in the hash TABLE.\n@@ -2556,28 +2219,6 @@ compute_hash_table (struct hash_table *table)\n \f\n /* Expression tracking support.  */\n \n-/* Lookup pattern PAT in the expression TABLE.\n-   The result is a pointer to the table entry, or NULL if not found.  */\n-\n-static struct expr *\n-lookup_expr (rtx pat, struct hash_table *table)\n-{\n-  int do_not_record_p;\n-  unsigned int hash = hash_expr (pat, GET_MODE (pat), &do_not_record_p,\n-\t\t\t\t table->size);\n-  struct expr *expr;\n-\n-  if (do_not_record_p)\n-    return NULL;\n-\n-  expr = table->table[hash];\n-\n-  while (expr && ! expr_equiv_p (expr->expr, pat))\n-    expr = expr->next_same_hash;\n-\n-  return expr;\n-}\n-\n /* Lookup REGNO in the set TABLE.  The result is a pointer to the\n    table entry, or NULL if not found.  */\n \n@@ -5426,7 +5067,8 @@ ldst_entry (rtx x)\n   struct ls_expr * ptr;\n   unsigned int hash;\n \n-  hash = hash_expr_1 (x, GET_MODE (x), & do_not_record_p);\n+  hash = hash_rtx (x, GET_MODE (x), &do_not_record_p,\n+\t\t   NULL,  /*have_reg_qty=*/false);\n \n   for (ptr = pre_ldst_mems; ptr != NULL; ptr = ptr->next)\n     if (ptr->hash_index == hash && expr_equiv_p (ptr->pattern, x))\n@@ -6945,654 +6587,4 @@ is_too_expensive (const char *pass)\n   return false;\n }\n \n-/* The following code implements gcse after reload, the purpose of this\n-   pass is to cleanup redundant loads generated by reload and other\n-   optimizations that come after gcse. It searches for simple inter-block\n-   redundancies and tries to eliminate them by adding moves and loads\n-   in cold places.  */\n-\n-/* The following structure holds the information about the occurrences of\n-   the redundant instructions.  */\n-struct unoccr\n-{\n-  struct unoccr *next;\n-  edge pred;\n-  rtx insn;\n-};\n-\n-static bool reg_used_on_edge (rtx, edge);\n-static rtx reg_set_between_after_reload_p (rtx, rtx, rtx);\n-static rtx reg_used_between_after_reload_p (rtx, rtx, rtx);\n-static rtx get_avail_load_store_reg (rtx);\n-static bool is_jump_table_basic_block (basic_block);\n-static bool bb_has_well_behaved_predecessors (basic_block);\n-static struct occr* get_bb_avail_insn (basic_block, struct occr *);\n-static void hash_scan_set_after_reload (rtx, rtx, struct hash_table *);\n-static void compute_hash_table_after_reload (struct hash_table *);\n-static void eliminate_partially_redundant_loads (basic_block,\n-\t\t\t\t\t\trtx,\n-\t\t\t\t\t\tstruct expr *);\n-static void gcse_after_reload (void);\n-static struct occr* get_bb_avail_insn (basic_block, struct occr *);\n-void gcse_after_reload_main (rtx, FILE *);\n-\n-\n-/* Check if register REG is used in any insn waiting to be inserted on E.\n-   Assumes no such insn can be a CALL_INSN; if so call reg_used_between_p\n-   with PREV(insn),NEXT(insn) instead of calling\n-   reg_overlap_mentioned_p.  */\n-\n-static bool\n-reg_used_on_edge (rtx reg, edge e)\n-{\n-  rtx insn;\n-\n-  for (insn = e->insns.r; insn; insn = NEXT_INSN (insn))\n-    if (INSN_P (insn) && reg_overlap_mentioned_p (reg, PATTERN (insn)))\n-      return true;\n-\n-  return false;\n-}\n-\n-/* Return the insn that sets register REG or clobbers it in between\n-   FROM_INSN and TO_INSN (exclusive of those two).\n-   Just like reg_set_between but for hard registers and not pseudos.  */\n-\n-static rtx\n-reg_set_between_after_reload_p (rtx reg, rtx from_insn, rtx to_insn)\n-{\n-  rtx insn;\n-  int regno;\n-\n-  if (! REG_P (reg))\n-    abort ();\n-  regno = REGNO (reg);\n-\n-  /* We are called after register allocation.  */\n-  if (regno >= FIRST_PSEUDO_REGISTER)\n-    abort ();\n-\n-  if (from_insn == to_insn)\n-    return NULL_RTX;\n-\n-  for (insn = NEXT_INSN (from_insn);\n-       insn != to_insn;\n-       insn = NEXT_INSN (insn))\n-    {\n-      if (INSN_P (insn))\n-\t{\n-\t  if (FIND_REG_INC_NOTE (insn, reg)\n-\t      || (CALL_P (insn)\n-\t\t  && call_used_regs[regno])\n-\t      || find_reg_fusage (insn, CLOBBER, reg))\n-\t    return insn;\n-\t}\n-      if (set_of (reg, insn) != NULL_RTX)\n-\treturn insn;\n-    }\n-  return NULL_RTX;\n-}\n-\n-/* Return the insn that uses register REG in between FROM_INSN and TO_INSN\n-   (exclusive of those two). Similar to reg_used_between but for hard\n-   registers and not pseudos.  */\n-\n-static rtx\n-reg_used_between_after_reload_p (rtx reg, rtx from_insn, rtx to_insn)\n-{\n-  rtx insn;\n-  int regno;\n-\n-  if (! REG_P (reg))\n-    return to_insn;\n-  regno = REGNO (reg);\n-\n-  /* We are called after register allocation.  */\n-  if (regno >= FIRST_PSEUDO_REGISTER)\n-    abort ();\n-  if (from_insn == to_insn)\n-    return NULL_RTX;\n-\n-  for (insn = NEXT_INSN (from_insn);\n-       insn != to_insn;\n-       insn = NEXT_INSN (insn))\n-    if (INSN_P (insn)\n-\t&& (reg_overlap_mentioned_p (reg, PATTERN (insn))\n-\t    || (CALL_P (insn)\n-\t\t&& call_used_regs[regno])\n-\t    || find_reg_fusage (insn, USE, reg)\n-\t    || find_reg_fusage (insn, CLOBBER, reg)))\n-      return insn;\n-  return NULL_RTX;\n-}\n-\n-/* Return the loaded/stored register of a load/store instruction.  */\n-\n-static rtx\n-get_avail_load_store_reg (rtx insn)\n-{\n-  if (REG_P (SET_DEST (PATTERN (insn))))  /* A load.  */\n-    return SET_DEST(PATTERN(insn));\n-  if (REG_P (SET_SRC (PATTERN (insn))))  /* A store.  */\n-    return SET_SRC (PATTERN (insn));\n-  abort ();\n-}\n-\n-/* Don't handle ABNORMAL edges or jump tables.  */\n-\n-static bool\n-is_jump_table_basic_block (basic_block bb)\n-{\n-  rtx insn = BB_END (bb);\n-\n-  if (JUMP_TABLE_DATA_P (insn))\n-    return true;\n-  return false;\n-}\n-\n-/* Return nonzero if the predecessors of BB are \"well behaved\".  */\n-\n-static bool\n-bb_has_well_behaved_predecessors (basic_block bb)\n-{\n-  edge pred;\n-\n-  if (! bb->pred)\n-    return false;\n-  for (pred = bb->pred; pred != NULL; pred = pred->pred_next)\n-    if (((pred->flags & EDGE_ABNORMAL) && EDGE_CRITICAL_P (pred))\n-\t|| is_jump_table_basic_block (pred->src))\n-      return false;\n-  return true;\n-}\n-\n-\n-/* Search for the occurrences of expression in BB.  */\n-\n-static struct occr*\n-get_bb_avail_insn (basic_block bb, struct occr *occr)\n-{\n-  for (; occr != NULL; occr = occr->next)\n-    if (BLOCK_FOR_INSN (occr->insn)->index == bb->index)\n-      return occr;\n-  return NULL;\n-}\n-\n-/* Perform partial GCSE pass after reload, try to eliminate redundant loads\n-   created by the reload pass. We try to look for a full or partial\n-   redundant loads fed by one or more loads/stores in predecessor BBs,\n-   and try adding loads to make them fully redundant. We also check if\n-   it's worth adding loads to be able to delete the redundant load.\n-\n-   Algorithm:\n-   1. Build available expressions hash table:\n-       For each load/store instruction, if the loaded/stored memory didn't\n-       change until the end of the basic block add this memory expression to\n-       the hash table.\n-   2. Perform Redundancy elimination:\n-      For each load instruction do the following:\n-\t perform partial redundancy elimination, check if it's worth adding\n-\t loads to make the load fully redundant. If so add loads and\n-\t register copies and delete the load.\n-\n-   Future enhancement:\n-     if loaded register is used/defined between load and some store,\n-     look for some other free register between load and all its stores,\n-     and replace load with a copy from this register to the loaded\n-     register.  */\n-\n-\n-/* This handles the case where several stores feed a partially redundant\n-   load. It checks if the redundancy elimination is possible and if it's\n-   worth it.  */\n-\n-static void\n-eliminate_partially_redundant_loads (basic_block bb, rtx insn,\n-\t\t\t\t     struct expr *expr)\n-{\n-  edge pred;\n-  rtx avail_insn = NULL_RTX;\n-  rtx avail_reg;\n-  rtx dest, pat;\n-  struct occr *a_occr;\n-  struct unoccr *occr, *avail_occrs = NULL;\n-  struct unoccr *unoccr, *unavail_occrs = NULL;\n-  int npred_ok = 0;\n-  gcov_type ok_count = 0; /* Redundant load execution count.  */\n-  gcov_type critical_count = 0; /* Execution count of critical edges.  */\n-\n-  /* The execution count of the loads to be added to make the\n-     load fully redundant.  */\n-  gcov_type not_ok_count = 0;\n-  basic_block pred_bb;\n-\n-  pat = PATTERN (insn);\n-  dest = SET_DEST (pat);\n-  /* Check that the loaded register is not used, set, or killed from the\n-     beginning of the block.  */\n-  if (reg_used_between_after_reload_p (dest,\n-                                       PREV_INSN (BB_HEAD (bb)), insn)\n-      || reg_set_between_after_reload_p (dest,\n-                                         PREV_INSN (BB_HEAD (bb)), insn))\n-    return;\n-\n-  /* Check potential for replacing load with copy for predecessors.  */\n-  for (pred = bb->pred; pred; pred = pred->pred_next)\n-    {\n-      rtx next_pred_bb_end;\n-\n-      avail_insn = NULL_RTX;\n-      pred_bb = pred->src;\n-      next_pred_bb_end = NEXT_INSN (BB_END (pred_bb));\n-      for (a_occr = get_bb_avail_insn (pred_bb, expr->avail_occr); a_occr;\n-\t   a_occr = get_bb_avail_insn (pred_bb, a_occr->next))\n-\t{\n-\t  /* Check if the loaded register is not used.  */\n-\t  avail_insn = a_occr->insn;\n-\t  if (! (avail_reg = get_avail_load_store_reg (avail_insn)))\n-\t    abort ();\n-\t  /* Make sure we can generate a move from register avail_reg to\n-\t     dest.  */\n-\t  extract_insn (gen_move_insn (copy_rtx (dest),\n-\t\t\t\t       copy_rtx (avail_reg)));\n-\t  if (! constrain_operands (1)\n-\t      || reg_killed_on_edge (avail_reg, pred)\n-\t      || reg_used_on_edge (dest, pred))\n-\t    {\n-\t      avail_insn = NULL;\n-\t      continue;\n-\t    }\n-\t  if (! reg_set_between_after_reload_p (avail_reg, avail_insn,\n-\t\t\t\t\t\tnext_pred_bb_end))\n-\t    /* AVAIL_INSN remains non-null.  */\n-\t    break;\n-\t  else\n-\t    avail_insn = NULL;\n-\t}\n-      if (avail_insn != NULL_RTX)\n-\t{\n-\t  npred_ok++;\n-\t  ok_count += pred->count;\n-          if (EDGE_CRITICAL_P (pred))\n-            critical_count += pred->count;\n-\t  occr = gmalloc (sizeof (struct unoccr));\n-\t  occr->insn = avail_insn;\n-\t  occr->pred = pred;\n-\t  occr->next = avail_occrs;\n-\t  avail_occrs = occr;\n-\t}\n-      else\n-\t{\n-\t  not_ok_count += pred->count;\n-          if (EDGE_CRITICAL_P (pred))\n-            critical_count += pred->count;\n-\t  unoccr = gmalloc (sizeof (struct unoccr));\n-\t  unoccr->insn = NULL_RTX;\n-\t  unoccr->pred = pred;\n-\t  unoccr->next = unavail_occrs;\n-\t  unavail_occrs = unoccr;\n-\t}\n-    }\n-\n-  if (npred_ok == 0    /* No load can be replaced by copy.  */\n-      || (optimize_size && npred_ok > 1)) /* Prevent exploding the code.  */\n-    goto cleanup;\n-\n-  /* Check if it's worth applying the partial redundancy elimination.  */\n-  if (ok_count < GCSE_AFTER_RELOAD_PARTIAL_FRACTION * not_ok_count)\n-    goto cleanup;\n-\n-  if (ok_count < GCSE_AFTER_RELOAD_CRITICAL_FRACTION * critical_count)\n-    goto cleanup;\n-\n-  /* Generate moves to the loaded register from where\n-     the memory is available.  */\n-  for (occr = avail_occrs; occr; occr = occr->next)\n-    {\n-      avail_insn = occr->insn;\n-      pred = occr->pred;\n-      /* Set avail_reg to be the register having the value of the\n-\t memory.  */\n-      avail_reg = get_avail_load_store_reg (avail_insn);\n-      if (! avail_reg)\n-\tabort ();\n-\n-      insert_insn_on_edge (gen_move_insn (copy_rtx (dest),\n-\t\t\t\t\t  copy_rtx (avail_reg)),\n-\t\t\t   pred);\n-\n-      if (gcse_file)\n-\tfprintf (gcse_file,\n-\t\t \"GCSE AFTER reload generating move from %d to %d on \\\n-\t\t edge from %d to %d\\n\",\n-\t\t REGNO (avail_reg),\n-\t\t REGNO (dest),\n-\t\t pred->src->index,\n-\t\t pred->dest->index);\n-    }\n-\n-  /* Regenerate loads where the memory is unavailable.  */\n-  for (unoccr = unavail_occrs; unoccr; unoccr = unoccr->next)\n-    {\n-      pred = unoccr->pred;\n-      insert_insn_on_edge (copy_insn (PATTERN (insn)), pred);\n-\n-      if (gcse_file)\n-\tfprintf (gcse_file,\n-\t\t \"GCSE AFTER reload: generating on edge from %d to %d\\\n-\t\t  a copy of load:\\n\",\n-\t\t pred->src->index,\n-\t\t pred->dest->index);\n-    }\n-\n-  /* Delete the insn if it is not available in this block and mark it\n-     for deletion if it is available. If insn is available it may help\n-     discover additional redundancies, so mark it for later deletion.*/\n-  for (a_occr = get_bb_avail_insn (bb, expr->avail_occr);\n-       a_occr && (a_occr->insn != insn);\n-       a_occr = get_bb_avail_insn (bb, a_occr->next));\n-\n-  if (!a_occr)\n-    delete_insn (insn);\n-  else\n-    a_occr->deleted_p = 1;\n-\n-cleanup:\n-\n-  while (unavail_occrs)\n-    {\n-      struct unoccr *temp = unavail_occrs->next;\n-      free (unavail_occrs);\n-      unavail_occrs = temp;\n-    }\n-\n-  while (avail_occrs)\n-    {\n-      struct unoccr *temp = avail_occrs->next;\n-      free (avail_occrs);\n-      avail_occrs = temp;\n-    }\n-}\n-\n-/* Performing the redundancy elimination as described before.  */\n-\n-static void\n-gcse_after_reload (void)\n-{\n-  unsigned int i;\n-  rtx insn;\n-  basic_block bb;\n-  struct expr *expr;\n-  struct occr *occr;\n-\n-  /* Note we start at block 1.  */\n-\n-  if (ENTRY_BLOCK_PTR->next_bb == EXIT_BLOCK_PTR)\n-    return;\n-\n-  FOR_BB_BETWEEN (bb,\n-\t\t  ENTRY_BLOCK_PTR->next_bb->next_bb,\n-\t\t  EXIT_BLOCK_PTR,\n-\t\t  next_bb)\n-    {\n-      if (! bb_has_well_behaved_predecessors (bb))\n-\tcontinue;\n-\n-      /* Do not try this optimization on cold basic blocks.  */\n-      if (probably_cold_bb_p (bb))\n-\tcontinue;\n-\n-      reset_opr_set_tables ();\n-\n-      for (insn = BB_HEAD (bb);\n-\t   insn != NULL\n-\t   && insn != NEXT_INSN (BB_END (bb));\n-\t   insn = NEXT_INSN (insn))\n-\t{\n-\t  /* Is it a load - of the form (set (reg) (mem))?  */\n-\t  if (NONJUMP_INSN_P (insn)\n-              && GET_CODE (PATTERN (insn)) == SET\n-\t      && REG_P (SET_DEST (PATTERN (insn)))\n-\t      && MEM_P (SET_SRC (PATTERN (insn))))\n-\t    {\n-\t      rtx pat = PATTERN (insn);\n-\t      rtx src = SET_SRC (pat);\n-\t      struct expr *expr;\n-\n-\t      if (general_operand (src, GET_MODE (src))\n-\t\t  /* Is the expression recorded?  */\n-\t\t  && (expr = lookup_expr (src, &expr_hash_table)) != NULL\n-\t\t  /* Are the operands unchanged since the start of the\n-\t\t     block?  */\n-\t\t  && oprs_not_set_p (src, insn)\n-\t\t  && ! MEM_VOLATILE_P (src)\n-\t\t  && GET_MODE (src) != BLKmode\n-\t\t  && !(flag_non_call_exceptions && may_trap_p (src))\n-\t\t  && !side_effects_p (src))\n-\t\t{\n-\t\t  /* We now have a load (insn) and an available memory at\n-\t\t     its BB start (expr). Try to remove the loads if it is\n-\t\t     redundant.  */\n-\t\t  eliminate_partially_redundant_loads (bb, insn, expr);\n-\t\t}\n-\t    }\n-\n-\t    /* Keep track of everything modified by this insn.  */\n-\t    if (INSN_P (insn))\n-\t      mark_oprs_set (insn);\n-\t}\n-    }\n-\n-  commit_edge_insertions ();\n-\n-  /* Go over the expression hash table and delete insns that were\n-     marked for later deletion.  */\n-  for (i = 0; i < expr_hash_table.size; i++)\n-    {\n-      for (expr = expr_hash_table.table[i];\n-\t   expr != NULL;\n-\t   expr = expr->next_same_hash)\n-\tfor (occr = expr->avail_occr; occr; occr = occr->next)\n-\t  if (occr->deleted_p)\n-\t    delete_insn (occr->insn);\n-    }\n-}\n-\n-/* Scan pattern PAT of INSN and add an entry to the hash TABLE.\n-   After reload we are interested in loads/stores only.  */\n-\n-static void\n-hash_scan_set_after_reload (rtx pat, rtx insn, struct hash_table *table)\n-{\n-  rtx src = SET_SRC (pat);\n-  rtx dest = SET_DEST (pat);\n-\n-  if (! MEM_P (src) && ! MEM_P (dest))\n-    return;\n-\n-  if (REG_P (dest))\n-    {\n-      if (/* Don't GCSE something if we can't do a reg/reg copy.  */\n-\t  can_copy_p (GET_MODE (dest))\n-\t  /* GCSE commonly inserts instruction after the insn.  We can't\n-\t     do that easily for EH_REGION notes so disable GCSE on these\n-\t     for now.  */\n-\t  && ! find_reg_note (insn, REG_EH_REGION, NULL_RTX)\n-\t  /* Is SET_SRC something we want to gcse?  */\n-\t  && general_operand (src, GET_MODE (src))\n-\t  /* Don't CSE a nop.  */\n-\t  && ! set_noop_p (pat)\n-\t  && ! JUMP_P (insn))\n-\t{\n-\t  /* An expression is not available if its operands are\n-\t     subsequently modified, including this insn.  */\n-\t  if (oprs_available_p (src, insn))\n-\t    insert_expr_in_table (src, GET_MODE (dest), insn, 0, 1, table);\n-\t}\n-    }\n-  else if (REG_P (src))\n-    {\n-      /* Only record sets of pseudo-regs in the hash table.  */\n-      if (/* Don't GCSE something if we can't do a reg/reg copy.  */\n-\t  can_copy_p (GET_MODE (src))\n-\t  /* GCSE commonly inserts instruction after the insn.  We can't\n-\t     do that easily for EH_REGION notes so disable GCSE on these\n-\t     for now.  */\n-\t  && ! find_reg_note (insn, REG_EH_REGION, NULL_RTX)\n-\t  /* Is SET_DEST something we want to gcse?  */\n-\t  && general_operand (dest, GET_MODE (dest))\n-\t  /* Don't CSE a nop.  */\n-\t  && ! set_noop_p (pat)\n-\t  &&! JUMP_P (insn)\n-\t  && ! (flag_float_store && FLOAT_MODE_P (GET_MODE (dest)))\n-\t  /* Check if the memory expression is killed after insn.  */\n-\t  && ! load_killed_in_block_p (BLOCK_FOR_INSN (insn),\n-\t\t\t\t       INSN_CUID (insn) + 1,\n-\t\t\t\t       dest,\n-\t\t\t\t       1)\n-\t  && oprs_unchanged_p (XEXP (dest, 0), insn, 1))\n-\t{\n-\t  insert_expr_in_table (dest, GET_MODE (dest), insn, 0, 1, table);\n-\t}\n-    }\n-}\n-\n-\n-/* Create hash table of memory expressions available at end of basic\n-   blocks.  */\n-\n-static void\n-compute_hash_table_after_reload (struct hash_table *table)\n-{\n-  unsigned int i;\n-\n-  table->set_p = 0;\n-\n-  /* Initialize count of number of entries in hash table.  */\n-  table->n_elems = 0;\n-  memset ((char *) table->table, 0,\n-\t  table->size * sizeof (struct expr *));\n-\n-  /* While we compute the hash table we also compute a bit array of which\n-     registers are set in which blocks.  */\n-  sbitmap_vector_zero (reg_set_in_block, last_basic_block);\n-\n-  /* Re-cache any INSN_LIST nodes we have allocated.  */\n-  clear_modify_mem_tables ();\n-\n-  /* Some working arrays used to track first and last set in each block.  */\n-  reg_avail_info = gmalloc (max_gcse_regno * sizeof (struct reg_avail_info));\n-\n-  for (i = 0; i < max_gcse_regno; ++i)\n-    reg_avail_info[i].last_bb = NULL;\n-\n-  FOR_EACH_BB (current_bb)\n-    {\n-      rtx insn;\n-      unsigned int regno;\n-\n-      /* First pass over the instructions records information used to\n-\t determine when registers and memory are first and last set.  */\n-      for (insn = BB_HEAD (current_bb);\n-\t   insn && insn != NEXT_INSN (BB_END (current_bb));\n-\t   insn = NEXT_INSN (insn))\n-\t{\n-\t  if (! INSN_P (insn))\n-\t    continue;\n-\n-\t  if (CALL_P (insn))\n-\t    {\n-\t      bool clobbers_all = false;\n-\n-#ifdef NON_SAVING_SETJMP\n-\t      if (NON_SAVING_SETJMP\n-\t\t  && find_reg_note (insn, REG_SETJMP, NULL_RTX))\n-\t\tclobbers_all = true;\n-#endif\n-\n-\t      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n-\t\tif (clobbers_all\n-\t\t    || TEST_HARD_REG_BIT (regs_invalidated_by_call,\n-\t\t\t\t\t  regno))\n-\t\t  record_last_reg_set_info (insn, regno);\n-\n-\t      mark_call (insn);\n-\t    }\n-\n-\t    note_stores (PATTERN (insn), record_last_set_info, insn);\n-\n-\t    if (GET_CODE (PATTERN (insn)) == SET)\n-\t      {\n-\t\trtx src, dest;\n-\n-\t\tsrc = SET_SRC (PATTERN (insn));\n-\t\tdest = SET_DEST (PATTERN (insn));\n-\t\tif (MEM_P (src) && auto_inc_p (XEXP (src, 0)))\n-\t\t  {\n-\t\t    regno = REGNO (XEXP (XEXP (src, 0), 0));\n-\t\t    record_last_reg_set_info (insn, regno);\n-\t\t  }\n-\t\tif (MEM_P (dest) && auto_inc_p (XEXP (dest, 0)))\n-\t\t  {\n-\t\t    regno = REGNO (XEXP (XEXP (dest, 0), 0));\n-\t\t    record_last_reg_set_info (insn, regno);\n-\t\t  }\n-\t\t}\n-\t  }\n-\n-\t/* The next pass builds the hash table.  */\n-\tfor (insn = BB_HEAD (current_bb);\n-\t     insn && insn != NEXT_INSN (BB_END (current_bb));\n-\t     insn = NEXT_INSN (insn))\n-\t  if (INSN_P (insn) && GET_CODE (PATTERN (insn)) == SET)\n-\t    if (! find_reg_note (insn, REG_LIBCALL, NULL_RTX))\n-\t      hash_scan_set_after_reload (PATTERN (insn), insn, table);\n-    }\n-\n-  free (reg_avail_info);\n-  reg_avail_info = NULL;\n-}\n-\n-\n-/* Main entry point of the GCSE after reload - clean some redundant loads\n-   due to spilling.  */\n-\n-void\n-gcse_after_reload_main (rtx f, FILE* file)\n-{\n-  gcse_subst_count = 0;\n-  gcse_create_count = 0;\n-\n-  gcse_file = file;\n-\n-  gcc_obstack_init (&gcse_obstack);\n-  bytes_used = 0;\n-\n-  /* We need alias.  */\n-  init_alias_analysis ();\n-\n-  max_gcse_regno = max_reg_num ();\n-\n-  alloc_reg_set_mem (max_gcse_regno);\n-  alloc_gcse_mem (f);\n-  alloc_hash_table (max_cuid, &expr_hash_table, 0);\n-  compute_hash_table_after_reload (&expr_hash_table);\n-\n-  if (gcse_file)\n-    dump_hash_table (gcse_file, \"Expression\", &expr_hash_table);\n-\n-  if (expr_hash_table.n_elems > 0)\n-    gcse_after_reload ();\n-\n-  free_hash_table (&expr_hash_table);\n-\n-  free_gcse_mem ();\n-  free_reg_set_mem ();\n-\n-  /* We are finished with alias.  */\n-  end_alias_analysis ();\n-\n-  obstack_free (&gcse_obstack, NULL);\n-}\n-\n #include \"gt-gcse.h\""}, {"sha": "da0beff238020d908fadc172a325f4a54f48798c", "filename": "gcc/passes.c", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2Fpasses.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2Fpasses.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fpasses.c?ref=0516f6fe82641daf7c1ac8812998049ac591201e", "patch": "@@ -841,10 +841,10 @@ rest_of_handle_sched2 (void)\n static void\n rest_of_handle_gcse2 (void)\n {\n-  timevar_push (TV_RELOAD_CSE_REGS);\n+  timevar_push (TV_GCSE_AFTER_RELOAD);\n   open_dump_file (DFI_gcse2, current_function_decl);\n \n-  gcse_after_reload_main (get_insns (), dump_file);\n+  gcse_after_reload_main (get_insns ());\n   rebuild_jump_labels (get_insns ());\n   delete_trivially_dead_insns (get_insns (), max_reg_num ());\n   close_dump_file (DFI_gcse2, print_rtl_with_bb, get_insns ());\n@@ -855,7 +855,7 @@ rest_of_handle_gcse2 (void)\n   verify_flow_info ();\n #endif\n \n-  timevar_pop (TV_RELOAD_CSE_REGS);\n+  timevar_pop (TV_GCSE_AFTER_RELOAD);\n }\n \n /* Register allocation pre-pass, to reduce number of moves necessary"}, {"sha": "a9d03440573d849ef2ecf2607897d66d2d2074b8", "filename": "gcc/postreload-gcse.c", "status": "added", "additions": 1379, "deletions": 0, "changes": 1379, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2Fpostreload-gcse.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2Fpostreload-gcse.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fpostreload-gcse.c?ref=0516f6fe82641daf7c1ac8812998049ac591201e", "patch": "@@ -0,0 +1,1379 @@\n+/* Post reload partially redundant load elimination\n+   Copyright (C) 2004\n+   Free Software Foundation, Inc.\n+\n+This file is part of GCC.\n+\n+GCC is free software; you can redistribute it and/or modify it under\n+the terms of the GNU General Public License as published by the Free\n+Software Foundation; either version 2, or (at your option) any later\n+version.\n+\n+GCC is distributed in the hope that it will be useful, but WITHOUT ANY\n+WARRANTY; without even the implied warranty of MERCHANTABILITY or\n+FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License\n+for more details.\n+\n+You should have received a copy of the GNU General Public License\n+along with GCC; see the file COPYING.  If not, write to the Free\n+Software Foundation, 59 Temple Place - Suite 330, Boston, MA\n+02111-1307, USA.  */\n+\n+#include \"config.h\"\n+#include \"system.h\"\n+#include \"coretypes.h\"\n+#include \"tm.h\"\n+#include \"toplev.h\"\n+\n+#include \"rtl.h\"\n+#include \"tree.h\"\n+#include \"tm_p.h\"\n+#include \"regs.h\"\n+#include \"hard-reg-set.h\"\n+#include \"flags.h\"\n+#include \"real.h\"\n+#include \"insn-config.h\"\n+#include \"recog.h\"\n+#include \"basic-block.h\"\n+#include \"output.h\"\n+#include \"function.h\"\n+#include \"expr.h\"\n+#include \"except.h\"\n+#include \"intl.h\"\n+#include \"obstack.h\"\n+#include \"hashtab.h\"\n+#include \"params.h\"\n+\n+/* The following code implements gcse after reload, the purpose of this\n+   pass is to cleanup redundant loads generated by reload and other\n+   optimizations that come after gcse. It searches for simple inter-block\n+   redundancies and tries to eliminate them by adding moves and loads\n+   in cold places.\n+\n+   Perform partially redundant load elimination, try to eliminate redundant\n+   loads created by the reload pass.  We try to look for full or partial\n+   redundant loads fed by one or more loads/stores in predecessor BBs,\n+   and try adding loads to make them fully redundant.  We also check if\n+   it's worth adding loads to be able to delete the redundant load.\n+\n+   Algorithm:\n+   1. Build available expressions hash table:\n+       For each load/store instruction, if the loaded/stored memory didn't\n+       change until the end of the basic block add this memory expression to\n+       the hash table.\n+   2. Perform Redundancy elimination:\n+      For each load instruction do the following:\n+\t perform partial redundancy elimination, check if it's worth adding\n+\t loads to make the load fully redundant.  If so add loads and\n+\t register copies and delete the load.\n+   3. Delete instructions made redundant in step 2.\n+\n+   Future enhancement:\n+     If the loaded register is used/defined between load and some store,\n+     look for some other free register between load and all its stores,\n+     and replace the load with a copy from this register to the loaded\n+     register.\n+*/\n+\f\n+\n+/* Keep statistics of this pass.  */\n+static struct\n+{\n+  int moves_inserted;\n+  int copies_inserted;\n+  int insns_deleted;\n+} stats;\n+\n+/* We need to keep a hash table of expressions.  The table entries are of\n+   type 'struct expr', and for each expression there is a single linked\n+   list of occurences.  */\n+\n+/* The table itself.  */\n+static htab_t expr_table;\n+\n+/* Expression elements in the hash table.  */\n+struct expr\n+{\n+  /* The expression (SET_SRC for expressions, PATTERN for assignments).  */\n+  rtx expr;\n+\n+  /* The same hash for this entry.  */\n+  hashval_t hash;\n+\n+  /* List of available occurrence in basic blocks in the function.  */\n+  struct occr *avail_occr;\n+};\n+\n+static struct obstack expr_obstack;\n+\n+/* Occurrence of an expression.\n+   There is at most one occurence per basic block.  If a pattern appears\n+   more than once, the last appearance is used.  */\n+\n+struct occr\n+{\n+  /* Next occurrence of this expression.  */\n+  struct occr *next;\n+  /* The insn that computes the expression.  */\n+  rtx insn;\n+  /* Nonzero if this [anticipatable] occurrence has been deleted.  */\n+  char deleted_p;\n+};\n+\n+static struct obstack occr_obstack;\n+\n+/* The following structure holds the information about the occurrences of\n+   the redundant instructions.  */\n+struct unoccr\n+{\n+  struct unoccr *next;\n+  edge pred;\n+  rtx insn;\n+};\n+\n+static struct obstack unoccr_obstack;\n+\n+/* Array where each element is the CUID if the insn that last set the hard\n+   register with the number of the element, since the start of the current\n+   basic block.  */\n+static int *reg_avail_info;\n+\n+/* A list of insns that may modify memory within the current basic block.  */\n+struct modifies_mem\n+{\n+  rtx insn;\n+  struct modifies_mem *next;\n+};\n+static struct modifies_mem *modifies_mem_list;\n+\n+/* The modifies_mem structs also go on an obstack, only this obstack is\n+   freed each time after completing the analysis or transformations on\n+   a basic block.  So we allocate a dummy modifies_mem_obstack_bottom\n+   object on the obstack to keep track of the bottom of the obstack.  */\n+static struct obstack modifies_mem_obstack;\n+static struct modifies_mem  *modifies_mem_obstack_bottom;\n+\n+/* Mapping of insn UIDs to CUIDs.\n+   CUIDs are like UIDs except they increase monotonically in each basic\n+   block, have no gaps, and only apply to real insns.  */\n+static int *uid_cuid;\n+#define INSN_CUID(INSN) (uid_cuid[INSN_UID (INSN)])\n+\f\n+\n+/* Helpers for memory allocation/freeing.  */\n+static void alloc_mem (void);\n+static void free_mem (void);\n+\n+/* Support for hash table construction and transformations.  */\n+static bool oprs_unchanged_p (rtx, rtx, bool);\n+static void record_last_reg_set_info (rtx, int);\n+static void record_last_mem_set_info (rtx);\n+static void record_last_set_info (rtx, rtx, void *);\n+static void mark_call (rtx);\n+static void mark_set (rtx, rtx);\n+static void mark_clobber (rtx, rtx);\n+static void mark_oprs_set (rtx);\n+\n+static void find_mem_conflicts (rtx, rtx, void *);\n+static int load_killed_in_block_p (int, rtx, bool);\n+static void reset_opr_set_tables (void);\n+\n+/* Hash table support.  */\n+static hashval_t hash_expr (rtx, int *);\n+static hashval_t hash_expr_for_htab (const void *);\n+static int expr_equiv_p (const void *, const void *);\n+static void insert_expr_in_table (rtx, rtx);\n+static struct expr *lookup_expr_in_table (rtx);\n+static int dump_hash_table_entry (void **, void *);\n+static void dump_hash_table (FILE *);\n+\n+/* Helpers for eliminate_partially_redundant_load.  */\n+static bool reg_killed_on_edge (rtx, edge);\n+static bool reg_used_on_edge (rtx, edge);\n+\n+static rtx reg_set_between_after_reload_p (rtx, rtx, rtx);\n+static rtx reg_used_between_after_reload_p (rtx, rtx, rtx);\n+static rtx get_avail_load_store_reg (rtx);\n+\n+static bool bb_has_well_behaved_predecessors (basic_block);\n+static struct occr* get_bb_avail_insn (basic_block, struct occr *);\n+static void hash_scan_set (rtx);\n+static void compute_hash_table (void);\n+\n+/* The work horses of this pass.  */\n+static void eliminate_partially_redundant_load (basic_block,\n+\t\t\t\t\t\trtx,\n+\t\t\t\t\t\tstruct expr *);\n+static void eliminate_partially_redundant_loads (void);\n+\f\n+\n+/* Allocate memory for the CUID mapping array and register/memory\n+   tracking tables.  */\n+\n+static void\n+alloc_mem (void)\n+{\n+  int i;\n+  basic_block bb;\n+  rtx insn;\n+\n+  /* Find the largest UID and create a mapping from UIDs to CUIDs.  */\n+  uid_cuid = xcalloc (get_max_uid () + 1, sizeof (int));\n+  i = 0;\n+  FOR_EACH_BB (bb)\n+    FOR_BB_INSNS (bb, insn)\n+      {\n+        if (INSN_P (insn))\n+\t  uid_cuid[INSN_UID (insn)] = i++;\n+\telse\n+\t  uid_cuid[INSN_UID (insn)] = i;\n+      }\n+\n+  /* Allocate the available expressions hash table.  We don't want to\n+     make the hash table too small, but unnecessarily making it too large\n+     also doesn't help.  The i/4 is a gcse.c relic, and seems like a\n+     reasonable choice.  */\n+  expr_table = htab_create (MAX (i / 4, 13),\n+\t\t\t    hash_expr_for_htab, expr_equiv_p, NULL);\n+\n+  /* We allocate everything on obstacks because we often can roll back\n+     the whole obstack to some point.  Freeing obstacks is very fast.  */\n+  gcc_obstack_init (&expr_obstack);\n+  gcc_obstack_init (&occr_obstack);\n+  gcc_obstack_init (&unoccr_obstack);\n+  gcc_obstack_init (&modifies_mem_obstack);\n+\n+  /* Working array used to track the last set for each register\n+     in the current block.  */\n+  reg_avail_info = (int *) xmalloc (FIRST_PSEUDO_REGISTER * sizeof (int));\n+\n+  /* Put a dummy modifies_mem object on the modifies_mem_obstack, so we\n+     can roll it back in reset_opr_set_tables.  */\n+  modifies_mem_obstack_bottom =\n+    (struct modifies_mem *) obstack_alloc (&modifies_mem_obstack,\n+\t\t\t\t\t   sizeof (struct modifies_mem));\n+}\n+\n+/* Free memory allocated by alloc_mem.  */\n+\n+static void\n+free_mem (void)\n+{\n+  free (uid_cuid);\n+\n+  htab_delete (expr_table);\n+\n+  obstack_free (&expr_obstack, NULL);\n+  obstack_free (&occr_obstack, NULL);\n+  obstack_free (&unoccr_obstack, NULL);\n+  obstack_free (&modifies_mem_obstack, NULL);\n+\n+  free (reg_avail_info);\n+}\n+\f\n+\n+/* Hash expression X.\n+   DO_NOT_RECORD_P is a boolean indicating if a volatile operand is found\n+   or if the expression contains something we don't want to insert in the\n+   table.  */\n+\n+static hashval_t\n+hash_expr (rtx x, int *do_not_record_p)\n+{\n+  *do_not_record_p = 0;\n+  return hash_rtx (x, GET_MODE (x), do_not_record_p,\n+\t\t   NULL,  /*have_reg_qty=*/false);\n+}\n+\n+/* Callback for hashtab.\n+   Return the hash value for expression EXP.  We don't actually hash\n+   here, we just return the cached hash value.  */\n+\n+static hashval_t\n+hash_expr_for_htab (const void *expp)\n+{\n+  struct expr *exp = (struct expr *) expp;\n+  return exp->hash;\n+}\n+\n+/* Callbach for hashtab.\n+   Return nonzero if exp1 is equivalent to exp2.  */\n+\n+static int\n+expr_equiv_p (const void *exp1p, const void *exp2p)\n+{\n+  struct expr *exp1 = (struct expr *) exp1p;\n+  struct expr *exp2 = (struct expr *) exp2p;\n+  int equiv_p = exp_equiv_p (exp1->expr, exp2->expr, 0, true);\n+  if (equiv_p\n+      && exp1->hash != exp2->hash)\n+    abort ();\n+  return equiv_p;\n+}\n+\f\n+\n+/* Insert expression X in INSN in the hash TABLE.\n+   If it is already present, record it as the last occurrence in INSN's\n+   basic block.  */\n+\n+static void\n+insert_expr_in_table (rtx x, rtx insn)\n+{\n+  int do_not_record_p;\n+  hashval_t hash;\n+  struct expr *cur_expr, **slot;\n+  struct occr *avail_occr, *last_occr = NULL;\n+\n+  hash = hash_expr (x, &do_not_record_p);\n+\n+  /* Do not insert expression in the table if it contains volatile operands,\n+     or if hash_expr determines the expression is something we don't want\n+     to or can't handle.  */\n+  if (do_not_record_p)\n+    return;\n+\n+  /* We anticipate that redundant expressions are rare, so for convenience\n+     allocate a new hash table element here already and set its fields.\n+     If we don't do this, we need a hack with a static struct expr.  Anyway,\n+     obstack_free is really fast and one more obstack_alloc doesn't hurt if\n+     we're going to see more expressions later on.  */\n+  cur_expr = (struct expr *) obstack_alloc (&expr_obstack,\n+\t\t\t\t\t    sizeof (struct expr));\n+  cur_expr->expr = x;\n+  cur_expr->hash = hash;\n+  cur_expr->avail_occr = NULL;\n+\n+  slot = (struct expr **) htab_find_slot_with_hash (expr_table, cur_expr,\n+\t\t\t\t\t\t    hash, INSERT);\n+  \n+  if (! (*slot))\n+    /* The expression isn't found, so insert it.  */\n+    *slot = cur_expr;\n+  else\n+    {\n+      /* The expression is already in the table, so roll back the\n+\t obstack and use the existing table entry.  */\n+      obstack_free (&expr_obstack, cur_expr);\n+      cur_expr = *slot;\n+    }\n+\n+  /* Search for another occurrence in the same basic block.  */\n+  avail_occr = cur_expr->avail_occr;\n+  while (avail_occr && BLOCK_NUM (avail_occr->insn) != BLOCK_NUM (insn))\n+    {\n+      /* If an occurrence isn't found, save a pointer to the end of\n+\t the list.  */\n+      last_occr = avail_occr;\n+      avail_occr = avail_occr->next;\n+    }\n+\n+  if (avail_occr)\n+    /* Found another instance of the expression in the same basic block.\n+       Prefer this occurrence to the currently recorded one.  We want\n+       the last one in the block and the block is scanned from start\n+       to end.  */\n+    avail_occr->insn = insn;\n+  else\n+    {\n+      /* First occurrence of this expression in this basic block.  */\n+      avail_occr = (struct occr *) obstack_alloc (&occr_obstack,\n+\t\t\t\t\t\t  sizeof (struct occr));\n+\n+      /* First occurrence of this expression in any block?  */\n+      if (cur_expr->avail_occr == NULL)\n+        cur_expr->avail_occr = avail_occr;\n+      else\n+        last_occr->next = avail_occr;\n+\n+      avail_occr->insn = insn;\n+      avail_occr->next = NULL;\n+      avail_occr->deleted_p = 0;\n+    }\n+}\n+\f\n+\n+/* Lookup pattern PAT in the expression hash table.\n+   The result is a pointer to the table entry, or NULL if not found.  */\n+\n+static struct expr *\n+lookup_expr_in_table (rtx pat)\n+{\n+  int do_not_record_p;\n+  struct expr **slot, *tmp_expr;\n+  hashval_t hash = hash_expr (pat, &do_not_record_p);\n+\n+  if (do_not_record_p)\n+    return NULL;\n+\n+  tmp_expr = (struct expr *) obstack_alloc (&expr_obstack,\n+\t\t\t\t\t    sizeof (struct expr));\n+  tmp_expr->expr = pat;\n+  tmp_expr->hash = hash;\n+  tmp_expr->avail_occr = NULL;\n+\n+  slot = (struct expr **) htab_find_slot_with_hash (expr_table, tmp_expr,\n+                                                    hash, INSERT);\n+  obstack_free (&expr_obstack, tmp_expr);\n+\n+  if (!slot)\n+    return NULL;\n+  else\n+    return (*slot);\n+}\n+\f\n+\n+/* Dump all expressions and occurences that are currently in the\n+   expression hash table to FILE.  */\n+\n+/* This helper is called via htab_traverse.  */\n+static int\n+dump_hash_table_entry (void **slot, void *filep)\n+{\n+  struct expr *expr = (struct expr *) *slot;\n+  FILE *file = (FILE *) filep;\n+  struct occr *occr;\n+\n+  fprintf (file, \"expr: \");\n+  print_rtl (file, expr->expr);\n+  fprintf (file,\"\\nhashcode: %u\\n\", expr->hash);\n+  fprintf (file,\"list of occurences:\\n\");\n+  occr = expr->avail_occr;\n+  while (occr)\n+    {\n+      rtx insn = occr->insn;\n+      print_rtl_single (file, insn);\n+      fprintf (file, \"\\n\");\n+      occr = occr->next;\n+    }\n+  fprintf (file, \"\\n\");\n+  return 1;\n+}\n+\n+static void\n+dump_hash_table (FILE *file)\n+{\n+  fprintf (file, \"\\n\\nexpression hash table\\n\");\n+  fprintf (file, \"size %ld, %ld elements, %f collision/search ratio\\n\",\n+           (long) htab_size (expr_table),\n+           (long) htab_elements (expr_table),\n+           htab_collisions (expr_table));\n+  if (htab_elements (expr_table) > 0)\n+    {\n+      fprintf (file, \"\\n\\ntable entries:\\n\");\n+      htab_traverse (expr_table, dump_hash_table_entry, file);\n+    }\n+  fprintf (file, \"\\n\");\n+}\n+\f\n+\n+/* Return nonzero if the operands of expression X are unchanged from the\n+   start of INSN's basic block up to but not including INSN if AFTER_INSN\n+   is false, or from INSN to the end of INSN's basic block if AFTER_INSN\n+   is true.  */\n+\n+static bool\n+oprs_unchanged_p (rtx x, rtx insn, bool after_insn)\n+{\n+  int i, j;\n+  enum rtx_code code;\n+  const char *fmt;\n+\n+  if (x == 0)\n+    return 1;\n+\n+  code = GET_CODE (x);\n+  switch (code)\n+    {\n+    case REG:\n+#ifdef ENABLE_CHECKING\n+      /* We are called after register allocation.  */\n+      if (REGNO (x) >= FIRST_PSEUDO_REGISTER)\n+\tabort ();\n+#endif\n+      if (after_insn)\n+\t/* If the last CUID setting the insn is less than the CUID of\n+\t   INSN, then reg X is not changed in or after INSN.  */\n+\treturn reg_avail_info[REGNO (x)] < INSN_CUID (insn);\n+      else\n+\t/* Reg X is not set before INSN in the current basic block if\n+\t   we have not yet recorded the CUID of an insn that touches\n+\t   the reg.  */\n+\treturn reg_avail_info[REGNO (x)] == 0;\n+\n+    case MEM:\n+      if (load_killed_in_block_p (INSN_CUID (insn), x, after_insn))\n+\treturn 0;\n+      else\n+\treturn oprs_unchanged_p (XEXP (x, 0), insn, after_insn);\n+\n+    case PC:\n+    case CC0: /*FIXME*/\n+    case CONST:\n+    case CONST_INT:\n+    case CONST_DOUBLE:\n+    case CONST_VECTOR:\n+    case SYMBOL_REF:\n+    case LABEL_REF:\n+    case ADDR_VEC:\n+    case ADDR_DIFF_VEC:\n+      return 1;\n+\n+    case PRE_DEC:\n+    case PRE_INC:\n+    case POST_DEC:\n+    case POST_INC:\n+    case PRE_MODIFY:\n+    case POST_MODIFY:\n+      if (after_insn)\n+\treturn 0;\n+      break;\n+\n+    default:\n+      break;\n+    }\n+\n+  for (i = GET_RTX_LENGTH (code) - 1, fmt = GET_RTX_FORMAT (code); i >= 0; i--)\n+    {\n+      if (fmt[i] == 'e')\n+\t{\n+\t  if (! oprs_unchanged_p (XEXP (x, i), insn, after_insn))\n+\t    return 0;\n+\t}\n+      else if (fmt[i] == 'E')\n+\tfor (j = 0; j < XVECLEN (x, i); j++)\n+\t  if (! oprs_unchanged_p (XVECEXP (x, i, j), insn, after_insn))\n+\t    return 0;\n+    }\n+\n+  return 1;\n+}\n+\f\n+\n+/* Used for communication between find_mem_conflicts and\n+   load_killed_in_block_p.  Nonzero if find_mem_conflicts finds a\n+   conflict between two memory references.\n+   This is a bit of a hack to work around the limitations of note_stores.  */\n+static int mems_conflict_p;\n+\n+/* DEST is the output of an instruction.  If it is a memory reference, and\n+   possibly conflicts with the load found in DATA, then set mems_conflict_p\n+   to a nonzero value.  */\n+\n+static void\n+find_mem_conflicts (rtx dest, rtx setter ATTRIBUTE_UNUSED,\n+\t\t    void *data)\n+{\n+  rtx mem_op = (rtx) data;\n+\n+  while (GET_CODE (dest) == SUBREG\n+\t || GET_CODE (dest) == ZERO_EXTRACT\n+\t || GET_CODE (dest) == SIGN_EXTRACT\n+\t || GET_CODE (dest) == STRICT_LOW_PART)\n+    dest = XEXP (dest, 0);\n+\n+  /* If DEST is not a MEM, then it will not conflict with the load.  Note\n+     that function calls are assumed to clobber memory, but are handled\n+     elsewhere.  */\n+  if (! MEM_P (dest))\n+    return;\n+\n+  if (true_dependence (dest, GET_MODE (dest), mem_op,\n+\t\t       rtx_addr_varies_p))\n+    mems_conflict_p = 1;\n+}\n+\f\n+\n+/* Return nonzero if the expression in X (a memory reference) is killed\n+   in block BB before if (AFTER_INSN is false) or after (if AFTER_INSN\n+   is true) the insn with the CUID in UID_LIMIT.  */\n+\n+static int\n+load_killed_in_block_p (int uid_limit, rtx x, bool after_insn)\n+{\n+  struct modifies_mem *list_entry = modifies_mem_list;\n+\n+  while (list_entry)\n+    {\n+      rtx setter = list_entry->insn;\n+\n+      /* Ignore entries in the list that do not apply.  */\n+      if ((after_insn\n+\t   && INSN_CUID (setter) < uid_limit)\n+\t  || (! after_insn\n+\t      && INSN_CUID (setter) > uid_limit))\n+\t{\n+\t  list_entry = list_entry->next;\n+\t  continue;\n+\t}\n+\n+      /* If SETTER is a call everything is clobbered.  Note that calls\n+\t to pure functions are never put on the list, so we need not\n+\t worry about them.  */\n+      if (CALL_P (setter))\n+\treturn 1;\n+\n+      /* SETTER must be an insn of some kind that sets memory.  Call\n+\t note_stores to examine each hunk of memory that is modified.\n+\t It will set mems_conflict_p to nonzero if there may be a\n+\t conflict between X and SETTER.  */\n+      mems_conflict_p = 0;\n+      note_stores (PATTERN (setter), find_mem_conflicts, x);\n+      if (mems_conflict_p)\n+\treturn 1;\n+\n+      list_entry = list_entry->next;\n+    }\n+  return 0;\n+}\n+\f\n+\n+/* Record register first/last/block set information for REGNO in INSN.  */\n+\n+static void\n+record_last_reg_set_info (rtx insn, int regno)\n+{\n+  reg_avail_info[regno] = INSN_CUID (insn);\n+}\n+\n+\n+/* Record memory modification information for INSN.  We do not actually care\n+   about the memory location(s) that are set, or even how they are set (consider\n+   a CALL_INSN).  We merely need to record which insns modify memory.  */\n+\n+static void\n+record_last_mem_set_info (rtx insn)\n+{\n+  struct modifies_mem *list_entry;\n+\n+  list_entry = (struct modifies_mem *) obstack_alloc (&modifies_mem_obstack,\n+\t\t\t\t\t\t      sizeof (struct modifies_mem));\n+  list_entry->insn = insn;\n+  list_entry->next = modifies_mem_list;\n+  modifies_mem_list = list_entry;\n+}\n+\n+/* Called from compute_hash_table via note_stores to handle one\n+   SET or CLOBBER in an insn.  DATA is really the instruction in which\n+   the SET is taking place.  */\n+\n+static void\n+record_last_set_info (rtx dest, rtx setter ATTRIBUTE_UNUSED, void *data)\n+{\n+  rtx last_set_insn = (rtx) data;\n+\n+  if (GET_CODE (dest) == SUBREG)\n+    dest = SUBREG_REG (dest);\n+\n+  if (REG_P (dest))\n+    record_last_reg_set_info (last_set_insn, REGNO (dest));\n+  else if (MEM_P (dest)\n+\t   /* Ignore pushes, they clobber nothing.  */\n+\t   && ! push_operand (dest, GET_MODE (dest)))\n+    record_last_mem_set_info (last_set_insn);\n+}\n+\f\n+\n+/* Reset tables used to keep track of what's still available since the\n+   start of the block.  */\n+\n+static void\n+reset_opr_set_tables (void)\n+{\n+  memset (reg_avail_info, 0, FIRST_PSEUDO_REGISTER * sizeof (int));\n+  obstack_free (&modifies_mem_obstack, modifies_mem_obstack_bottom);\n+  modifies_mem_list = NULL;\n+}\n+\n+/* Mark things set by a CALL.  */\n+\n+static void\n+mark_call (rtx insn)\n+{\n+  if (! CONST_OR_PURE_CALL_P (insn))\n+    record_last_mem_set_info (insn);\n+}\n+\n+/* Mark things set by a SET.  */\n+\n+static void\n+mark_set (rtx pat, rtx insn)\n+{\n+  rtx dest = SET_DEST (pat);\n+\n+  while (GET_CODE (dest) == SUBREG\n+\t || GET_CODE (dest) == ZERO_EXTRACT\n+\t || GET_CODE (dest) == SIGN_EXTRACT\n+\t || GET_CODE (dest) == STRICT_LOW_PART)\n+    dest = XEXP (dest, 0);\n+\n+  if (REG_P (dest))\n+    record_last_reg_set_info (insn, REGNO (dest));\n+  else if (MEM_P (dest))\n+    record_last_mem_set_info (insn);\n+\n+  if (GET_CODE (SET_SRC (pat)) == CALL)\n+    mark_call (insn);\n+}\n+\n+/* Record things set by a CLOBBER.  */\n+\n+static void\n+mark_clobber (rtx pat, rtx insn)\n+{\n+  rtx clob = XEXP (pat, 0);\n+\n+  while (GET_CODE (clob) == SUBREG\n+\t || GET_CODE (clob) == STRICT_LOW_PART)\n+    clob = XEXP (clob, 0);\n+\n+  if (REG_P (clob))\n+    record_last_reg_set_info (insn, REGNO (clob));\n+  else\n+    record_last_mem_set_info (insn);\n+}\n+\n+/* Record things set by INSN.\n+   This data is used by oprs_unchanged_p.  */\n+\n+static void\n+mark_oprs_set (rtx insn)\n+{\n+  rtx pat = PATTERN (insn);\n+  int i;\n+\n+  if (GET_CODE (pat) == SET)\n+    mark_set (pat, insn);\n+\n+  else if (GET_CODE (pat) == PARALLEL)\n+    for (i = 0; i < XVECLEN (pat, 0); i++)\n+      {\n+\trtx x = XVECEXP (pat, 0, i);\n+\n+\tif (GET_CODE (x) == SET)\n+\t  mark_set (x, insn);\n+\telse if (GET_CODE (x) == CLOBBER)\n+\t  mark_clobber (x, insn);\n+\telse if (GET_CODE (x) == CALL)\n+\t  mark_call (insn);\n+      }\n+\n+  else if (GET_CODE (pat) == CLOBBER)\n+    mark_clobber (pat, insn);\n+\n+  else if (GET_CODE (pat) == CALL)\n+    mark_call (insn);\n+}\n+\f\n+\n+/* Scan the pattern of INSN and add an entry to the hash TABLE.\n+   After reload we are interested in loads/stores only.  */\n+\n+static void\n+hash_scan_set (rtx insn)\n+{\n+  rtx pat = PATTERN (insn);\n+  rtx src = SET_SRC (pat);\n+  rtx dest = SET_DEST (pat);\n+\n+  /* We are only interested in loads and stores.  */\n+  if (! MEM_P (src) && ! MEM_P (dest))\n+    return;\n+\n+  /* Don't mess with jumps and nops.  */\n+  if (JUMP_P (insn) || set_noop_p (pat))\n+    return;\n+\n+#ifdef ENABLE_CHEKCING\n+  /* We shouldn't have any EH_REGION notes post reload.  */\n+  if (find_reg_note (insn, REG_EH_REGION, NULL_RTX))\n+    abort ();\n+#endif\n+\n+  if (REG_P (dest))\n+    {\n+      if (/* Don't GCSE something if we can't do a reg/reg copy.  */\n+\t  can_copy_p (GET_MODE (dest))\n+\t  /* Is SET_SRC something we want to gcse?  */\n+\t  && general_operand (src, GET_MODE (src))\n+\t  /* An expression is not available if its operands are\n+\t     subsequently modified, including this insn.  */\n+\t  && oprs_unchanged_p (src, insn, true))\n+\t{\n+\t  insert_expr_in_table (src, insn);\n+\t}\n+    }\n+  else if (REG_P (src))\n+    {\n+      /* Only record sets of pseudo-regs in the hash table.  */\n+      if (/* Don't GCSE something if we can't do a reg/reg copy.  */\n+\t  can_copy_p (GET_MODE (src))\n+\t  /* Is SET_DEST something we want to gcse?  */\n+\t  && general_operand (dest, GET_MODE (dest))\n+\t  && ! (flag_float_store && FLOAT_MODE_P (GET_MODE (dest)))\n+\t  /* Check if the memory expression is killed after insn.  */\n+\t  && ! load_killed_in_block_p (INSN_CUID (insn) + 1, dest, true)\n+\t  && oprs_unchanged_p (XEXP (dest, 0), insn, true))\n+\t{\n+\t  insert_expr_in_table (dest, insn);\n+\t}\n+    }\n+}\n+\f\n+/* Create hash table of memory expressions available at end of basic\n+   blocks.  */\n+\n+static void\n+compute_hash_table (void)\n+{\n+  basic_block bb;\n+\n+  FOR_EACH_BB (bb)\n+    {\n+      rtx insn;\n+      unsigned int regno;\n+\n+      reset_opr_set_tables ();\n+\n+      /* First pass over the instructions records information used to\n+\t determine when registers and memory are first and last set.  */\n+      FOR_BB_INSNS (bb, insn)\n+\t{\n+\t  if (! INSN_P (insn))\n+\t    continue;\n+\n+\t  if (CALL_P (insn))\n+\t    {\n+\t      bool clobbers_all = false;\n+\n+#ifdef NON_SAVING_SETJMP\n+\t      if (NON_SAVING_SETJMP\n+\t\t  && find_reg_note (insn, REG_SETJMP, NULL_RTX))\n+\t\tclobbers_all = true;\n+#endif\n+\n+\t      for (regno = 0; regno < FIRST_PSEUDO_REGISTER; regno++)\n+\t\tif (clobbers_all\n+\t\t    || TEST_HARD_REG_BIT (regs_invalidated_by_call,\n+\t\t\t\t\t  regno))\n+\t\t  record_last_reg_set_info (insn, regno);\n+\n+\t      if (! CONST_OR_PURE_CALL_P (insn))\n+\t\trecord_last_mem_set_info (insn);\n+\t    }\n+\n+\t  note_stores (PATTERN (insn), record_last_set_info, insn);\n+\n+\t  if (GET_CODE (PATTERN (insn)) == SET)\n+\t    {\n+\t      rtx src, dest;\n+\n+\t      src = SET_SRC (PATTERN (insn));\n+\t      dest = SET_DEST (PATTERN (insn));\n+\t      if (MEM_P (src) && auto_inc_p (XEXP (src, 0)))\n+\t\t{\n+\t\t  regno = REGNO (XEXP (XEXP (src, 0), 0));\n+\t\t  record_last_reg_set_info (insn, regno);\n+\t\t}\n+\t      if (MEM_P (dest) && auto_inc_p (XEXP (dest, 0)))\n+\t\t{\n+\t\t  regno = REGNO (XEXP (XEXP (dest, 0), 0));\n+\t\t  record_last_reg_set_info (insn, regno);\n+\t\t}\n+\t     }\n+\t  }\n+\n+      /* The next pass builds the hash table.  */\n+      FOR_BB_INSNS (bb, insn)\n+\tif (INSN_P (insn) && GET_CODE (PATTERN (insn)) == SET)\n+\t  hash_scan_set (insn);\n+    }\n+}\n+\f\n+\n+/* Check if register REG is killed in any insn waiting to be inserted on\n+   edge E.  This function is required to check that our data flow analysis\n+   is still valid prior to commit_edge_insertions.  */\n+\n+static bool\n+reg_killed_on_edge (rtx reg, edge e)\n+{\n+  rtx insn;\n+\n+  for (insn = e->insns.r; insn; insn = NEXT_INSN (insn))\n+    if (INSN_P (insn) && reg_set_p (reg, insn))\n+      return true;\n+\n+  return false;\n+}\n+\n+/* Similar to above - check if register REG is used in any insn waiting\n+   to be inserted on edge E.\n+   Assumes no such insn can be a CALL_INSN; if so call reg_used_between_p\n+   with PREV(insn),NEXT(insn) instead of calling reg_overlap_mentioned_p.  */\n+\n+static bool\n+reg_used_on_edge (rtx reg, edge e)\n+{\n+  rtx insn;\n+\n+  for (insn = e->insns.r; insn; insn = NEXT_INSN (insn))\n+    if (INSN_P (insn) && reg_overlap_mentioned_p (reg, PATTERN (insn)))\n+      return true;\n+\n+  return false;\n+}\n+\f\n+\n+/* Return the insn that sets register REG or clobbers it in between\n+   FROM_INSN and TO_INSN (exclusive of those two).\n+   Just like reg_set_between but for hard registers and not pseudos.  */\n+\n+static rtx\n+reg_set_between_after_reload_p (rtx reg, rtx from_insn, rtx to_insn)\n+{\n+  rtx insn;\n+  int regno;\n+\n+#ifdef ENABLE_CHECKING\n+  /* We are called after register allocation.  */\n+  if (!REG_P (reg) || REGNO (reg) >= FIRST_PSEUDO_REGISTER)\n+    abort ();\n+#endif\n+\n+  if (from_insn == to_insn)\n+    return NULL_RTX;\n+\n+  regno = REGNO (reg);\n+  for (insn = NEXT_INSN (from_insn);\n+       insn != to_insn;\n+       insn = NEXT_INSN (insn))\n+    {\n+      if (INSN_P (insn))\n+\t{\n+\t  if (FIND_REG_INC_NOTE (insn, reg)\n+\t      || (CALL_P (insn)\n+\t\t  && call_used_regs[regno])\n+\t      || find_reg_fusage (insn, CLOBBER, reg))\n+\t    return insn;\n+\t}\n+      if (set_of (reg, insn) != NULL_RTX)\n+\treturn insn;\n+    }\n+\n+  return NULL_RTX;\n+}\n+\n+/* Return the insn that uses register REG in between FROM_INSN and TO_INSN\n+   (exclusive of those two). Similar to reg_used_between but for hard\n+   registers and not pseudos.  */\n+\n+static rtx\n+reg_used_between_after_reload_p (rtx reg, rtx from_insn, rtx to_insn)\n+{\n+  rtx insn;\n+  int regno;\n+\n+#ifdef ENABLE_CHECKING\n+  /* We are called after register allocation.  */\n+  if (!REG_P (reg) || REGNO (reg) >= FIRST_PSEUDO_REGISTER)\n+    abort ();\n+#endif\n+\n+  if (from_insn == to_insn)\n+    return NULL_RTX;\n+\n+  regno = REGNO (reg);\n+  for (insn = NEXT_INSN (from_insn);\n+       insn != to_insn;\n+       insn = NEXT_INSN (insn))\n+    if (INSN_P (insn)\n+\t&& (reg_overlap_mentioned_p (reg, PATTERN (insn))\n+\t    || (CALL_P (insn)\n+\t\t&& call_used_regs[regno])\n+\t    || find_reg_fusage (insn, USE, reg)\n+\t    || find_reg_fusage (insn, CLOBBER, reg)))\n+      return insn;\n+\n+  return NULL_RTX;\n+}\n+\n+/* Return true if REG is used, set, or killed between the beginning of\n+   basic block BB and UP_TO_INSN.  Caches the result in reg_avail_info.  */\n+\n+static bool\n+reg_set_or_used_since_bb_start (rtx reg, basic_block bb, rtx up_to_insn)\n+{\n+  rtx insn, start = PREV_INSN (BB_HEAD (bb));\n+\n+  if (reg_avail_info[REGNO (reg)] != 0)\n+    return true;\n+\n+  insn = reg_used_between_after_reload_p (reg, start, up_to_insn);\n+  if (! insn)\n+    insn = reg_set_between_after_reload_p (reg, start, up_to_insn);\n+\n+  if (insn)\n+    reg_avail_info[REGNO (reg)] = INSN_CUID (insn);\n+\n+  return insn != NULL_RTX;\n+}\n+\n+/* Return the loaded/stored register of a load/store instruction.  */\n+\n+static rtx\n+get_avail_load_store_reg (rtx insn)\n+{\n+  if (REG_P (SET_DEST (PATTERN (insn))))  /* A load.  */\n+    return SET_DEST(PATTERN(insn));\n+  if (REG_P (SET_SRC (PATTERN (insn))))  /* A store.  */\n+    return SET_SRC (PATTERN (insn));\n+  abort ();\n+}\n+\n+/* Return nonzero if the predecessors of BB are \"well behaved\".  */\n+\n+static bool\n+bb_has_well_behaved_predecessors (basic_block bb)\n+{\n+  edge pred;\n+\n+  if (! bb->pred)\n+    return false;\n+\n+  for (pred = bb->pred; pred != NULL; pred = pred->pred_next)\n+    {\n+      if ((pred->flags & EDGE_ABNORMAL) && EDGE_CRITICAL_P (pred))\n+\treturn false;\n+\n+      if (JUMP_TABLE_DATA_P (BB_END (pred->src)))\n+\treturn false;\n+    }\n+  return true;\n+}\n+\n+\n+/* Search for the occurrences of expression in BB.  */\n+\n+static struct occr*\n+get_bb_avail_insn (basic_block bb, struct occr *occr)\n+{\n+  for (; occr != NULL; occr = occr->next)\n+    if (BLOCK_FOR_INSN (occr->insn) == bb)\n+      return occr;\n+  return NULL;\n+}\n+\n+\n+/* This handles the case where several stores feed a partially redundant\n+   load. It checks if the redundancy elimination is possible and if it's\n+   worth it.  */\n+\n+static void\n+eliminate_partially_redundant_load (basic_block bb, rtx insn,\n+\t\t\t\t    struct expr *expr)\n+{\n+  edge pred;\n+  rtx avail_insn = NULL_RTX;\n+  rtx avail_reg;\n+  rtx dest, pat;\n+  struct occr *a_occr;\n+  struct unoccr *occr, *avail_occrs = NULL;\n+  struct unoccr *unoccr, *unavail_occrs = NULL, *rollback_unoccr = NULL;\n+  int npred_ok = 0;\n+  gcov_type ok_count = 0; /* Redundant load execution count.  */\n+  gcov_type critical_count = 0; /* Execution count of critical edges.  */\n+\n+  /* The execution count of the loads to be added to make the\n+     load fully redundant.  */\n+  gcov_type not_ok_count = 0;\n+  basic_block pred_bb;\n+\n+  pat = PATTERN (insn);\n+  dest = SET_DEST (pat);\n+\n+  /* Check that the loaded register is not used, set, or killed from the\n+     beginning of the block.  */\n+  if (reg_set_or_used_since_bb_start (dest, bb, insn))\n+    return;\n+\n+  /* Check potential for replacing load with copy for predecessors.  */\n+  for (pred = bb->pred; pred; pred = pred->pred_next)\n+    {\n+      rtx next_pred_bb_end;\n+\n+      avail_insn = NULL_RTX;\n+      pred_bb = pred->src;\n+      next_pred_bb_end = NEXT_INSN (BB_END (pred_bb));\n+      for (a_occr = get_bb_avail_insn (pred_bb, expr->avail_occr); a_occr;\n+\t   a_occr = get_bb_avail_insn (pred_bb, a_occr->next))\n+\t{\n+\t  /* Check if the loaded register is not used.  */\n+\t  avail_insn = a_occr->insn;\n+\t  if (! (avail_reg = get_avail_load_store_reg (avail_insn)))\n+\t    abort ();\n+\t  /* Make sure we can generate a move from register avail_reg to\n+\t     dest.  */\n+\t  extract_insn (gen_move_insn (copy_rtx (dest),\n+\t\t\t\t       copy_rtx (avail_reg)));\n+\t  if (! constrain_operands (1)\n+\t      || reg_killed_on_edge (avail_reg, pred)\n+\t      || reg_used_on_edge (dest, pred))\n+\t    {\n+\t      avail_insn = NULL;\n+\t      continue;\n+\t    }\n+\t  if (! reg_set_between_after_reload_p (avail_reg, avail_insn,\n+\t\t\t\t\t\tnext_pred_bb_end))\n+\t    /* AVAIL_INSN remains non-null.  */\n+\t    break;\n+\t  else\n+\t    avail_insn = NULL;\n+\t}\n+\n+      if (EDGE_CRITICAL_P (pred))\n+\tcritical_count += pred->count;\n+\n+      if (avail_insn != NULL_RTX)\n+\t{\n+\t  npred_ok++;\n+\t  ok_count += pred->count;\n+\t  occr = (struct unoccr *) obstack_alloc (&unoccr_obstack,\n+\t\t\t\t\t\t  sizeof (struct occr));\n+\t  occr->insn = avail_insn;\n+\t  occr->pred = pred;\n+\t  occr->next = avail_occrs;\n+\t  avail_occrs = occr;\n+\t  if (! rollback_unoccr)\n+\t    rollback_unoccr = occr;\n+\t}\n+      else\n+\t{\n+\t  not_ok_count += pred->count;\n+\t  unoccr = (struct unoccr *) obstack_alloc (&unoccr_obstack,\n+\t\t\t\t\t\t    sizeof (struct unoccr));\n+\t  unoccr->insn = NULL_RTX;\n+\t  unoccr->pred = pred;\n+\t  unoccr->next = unavail_occrs;\n+\t  unavail_occrs = unoccr;\n+\t  if (! rollback_unoccr)\n+\t    rollback_unoccr = unoccr;\n+\t}\n+    }\n+\n+  if (/* No load can be replaced by copy.  */\n+      npred_ok == 0\n+      /* Prevent exploding the code.  */ \n+      || (optimize_size && npred_ok > 1))\n+    goto cleanup;\n+\n+  /* Check if it's worth applying the partial redundancy elimination.  */\n+  if (ok_count < GCSE_AFTER_RELOAD_PARTIAL_FRACTION * not_ok_count)\n+    goto cleanup;\n+  if (ok_count < GCSE_AFTER_RELOAD_CRITICAL_FRACTION * critical_count)\n+    goto cleanup;\n+\n+  /* Generate moves to the loaded register from where\n+     the memory is available.  */\n+  for (occr = avail_occrs; occr; occr = occr->next)\n+    {\n+      avail_insn = occr->insn;\n+      pred = occr->pred;\n+      /* Set avail_reg to be the register having the value of the\n+\t memory.  */\n+      avail_reg = get_avail_load_store_reg (avail_insn);\n+      if (! avail_reg)\n+\tabort ();\n+\n+      insert_insn_on_edge (gen_move_insn (copy_rtx (dest),\n+\t\t\t\t\t  copy_rtx (avail_reg)),\n+\t\t\t   pred);\n+      stats.moves_inserted++;\n+\n+      if (dump_file)\n+\tfprintf (dump_file,\n+\t\t \"generating move from %d to %d on edge from %d to %d\\n\",\n+\t\t REGNO (avail_reg),\n+\t\t REGNO (dest),\n+\t\t pred->src->index,\n+\t\t pred->dest->index);\n+    }\n+\n+  /* Regenerate loads where the memory is unavailable.  */\n+  for (unoccr = unavail_occrs; unoccr; unoccr = unoccr->next)\n+    {\n+      pred = unoccr->pred;\n+      insert_insn_on_edge (copy_insn (PATTERN (insn)), pred);\n+      stats.copies_inserted++;\n+\n+      if (dump_file)\n+\t{\n+\t  fprintf (dump_file,\n+\t\t   \"generating on edge from %d to %d a copy of load: \",\n+\t\t   pred->src->index,\n+\t\t   pred->dest->index);\n+\t  print_rtl (dump_file, PATTERN (insn));\n+\t  fprintf (dump_file, \"\\n\");\n+\t}\n+    }\n+\n+  /* Delete the insn if it is not available in this block and mark it\n+     for deletion if it is available. If insn is available it may help\n+     discover additional redundancies, so mark it for later deletion.  */\n+  for (a_occr = get_bb_avail_insn (bb, expr->avail_occr);\n+       a_occr && (a_occr->insn != insn);\n+       a_occr = get_bb_avail_insn (bb, a_occr->next));\n+\n+  if (!a_occr)\n+    delete_insn (insn);\n+  else\n+    a_occr->deleted_p = 1;\n+\n+cleanup:\n+  if (rollback_unoccr)\n+    obstack_free (&unoccr_obstack, rollback_unoccr);\n+}\n+\n+/* Performing the redundancy elimination as described before.  */\n+\n+static void\n+eliminate_partially_redundant_loads (void)\n+{\n+  rtx insn;\n+  basic_block bb;\n+\n+  /* Note we start at block 1.  */\n+\n+  if (ENTRY_BLOCK_PTR->next_bb == EXIT_BLOCK_PTR)\n+    return;\n+\n+  FOR_BB_BETWEEN (bb,\n+\t\t  ENTRY_BLOCK_PTR->next_bb->next_bb,\n+\t\t  EXIT_BLOCK_PTR,\n+\t\t  next_bb)\n+    {\n+      if (! bb_has_well_behaved_predecessors (bb))\n+\tcontinue;\n+\n+      /* Do not try this optimization on cold basic blocks.  */\n+      if (probably_cold_bb_p (bb))\n+\tcontinue;\n+\n+      reset_opr_set_tables ();\n+\n+      FOR_BB_INSNS (bb, insn)\n+\t{\n+\t  /* Is it a load - of the form (set (reg) (mem))?  */\n+\t  if (NONJUMP_INSN_P (insn)\n+              && GET_CODE (PATTERN (insn)) == SET\n+\t      && REG_P (SET_DEST (PATTERN (insn)))\n+\t      && MEM_P (SET_SRC (PATTERN (insn))))\n+\t    {\n+\t      rtx pat = PATTERN (insn);\n+\t      rtx src = SET_SRC (pat);\n+\t      struct expr *expr;\n+\n+\t      if (!MEM_VOLATILE_P (src)\n+\t\t  && GET_MODE (src) != BLKmode\n+\t\t  && general_operand (src, GET_MODE (src))\n+\t\t  /* Are the operands unchanged since the start of the\n+\t\t     block?  */\n+\t\t  && oprs_unchanged_p (src, insn, false)\n+\t\t  && !(flag_non_call_exceptions && may_trap_p (src))\n+\t\t  && !side_effects_p (src)\n+\t\t  /* Is the expression recorded?  */\n+\t\t  && (expr = lookup_expr_in_table (src)) != NULL)\n+\t\t{\n+\t\t  /* We now have a load (insn) and an available memory at\n+\t\t     its BB start (expr). Try to remove the loads if it is\n+\t\t     redundant.  */\n+\t\t  eliminate_partially_redundant_load (bb, insn, expr);\n+\t\t}\n+\t    }\n+\n+\t  /* Keep track of everything modified by this insn.  */\n+\t  if (INSN_P (insn))\n+\t    mark_oprs_set (insn);\n+\t}\n+    }\n+\n+  commit_edge_insertions ();\n+}\n+\n+/* Go over the expression hash table and delete insns that were\n+   marked for later deletion.  */\n+\n+/* This helper is called via htab_traverse.  */\n+static int\n+delete_redundant_insns_1 (void **slot, void *data ATTRIBUTE_UNUSED)\n+{\n+  struct expr *expr = (struct expr *) *slot;\n+  struct occr *occr;\n+\n+  for (occr = expr->avail_occr; occr != NULL; occr = occr->next)\n+    {\n+      if (occr->deleted_p)\n+\t{\n+\t  delete_insn (occr->insn);\n+\t  stats.insns_deleted++;\n+\n+\t  if (dump_file)\n+\t    {\n+\t      fprintf (dump_file, \"deleting insn:\\n\");\n+\t      print_rtl_single (dump_file, occr->insn);\n+\t      fprintf (dump_file, \"\\n\");\n+\t    }\n+\t}\n+    }\n+\n+  return 1;\n+}\n+\n+static void\n+delete_redundant_insns (void)\n+{\n+  htab_traverse (expr_table, delete_redundant_insns_1, NULL);\n+  if (dump_file)\n+    fprintf (dump_file, \"\\n\");\n+}\n+\n+/* Main entry point of the GCSE after reload - clean some redundant loads\n+   due to spilling.  */\n+\n+void\n+gcse_after_reload_main (rtx f ATTRIBUTE_UNUSED)\n+{\n+  memset (&stats, 0, sizeof (stats));\n+\n+  /* Allocate ememory for this pass.\n+     Also computes and initializes the insns' CUIDs.  */\n+  alloc_mem ();\n+\n+  /* We need alias analysis.  */\n+  init_alias_analysis ();\n+\n+  compute_hash_table ();\n+\n+  if (dump_file)\n+    dump_hash_table (dump_file);\n+\n+  if (htab_elements (expr_table) > 0)\n+    {\n+      eliminate_partially_redundant_loads ();\n+      delete_redundant_insns ();\n+\n+      if (dump_file)\n+\t{\n+\t  fprintf (dump_file, \"GCSE AFTER RELOAD stats:\\n\");\n+\t  fprintf (dump_file, \"copies inserted: %d\\n\", stats.copies_inserted);\n+\t  fprintf (dump_file, \"moves inserted:  %d\\n\", stats.moves_inserted);\n+\t  fprintf (dump_file, \"insns deleted:   %d\\n\", stats.insns_deleted);\n+\t  fprintf (dump_file, \"\\n\\n\");\n+\t}\n+    }\n+    \n+  /* We are finished with alias.  */\n+  end_alias_analysis ();\n+\n+  free_mem ();\n+}\n+"}, {"sha": "cebc616db36d81de65fd5d28d3751e128c9f4337", "filename": "gcc/rtl.h", "status": "modified", "additions": 5, "deletions": 1, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2Frtl.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2Frtl.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Frtl.h?ref=0516f6fe82641daf7c1ac8812998049ac591201e", "patch": "@@ -2119,6 +2119,8 @@ extern int rtx_to_tree_code (enum rtx_code);\n extern int delete_trivially_dead_insns (rtx, int);\n extern int cse_main (rtx, int, int, FILE *);\n extern void cse_condition_code_reg (void);\n+extern int exp_equiv_p (rtx, rtx, int, bool);\n+extern unsigned hash_rtx (rtx x, enum machine_mode, int *, int *, bool);\n \n /* In jump.c */\n extern int comparison_dominates_p (enum rtx_code, enum rtx_code);\n@@ -2265,7 +2267,9 @@ extern bool can_copy_p (enum machine_mode);\n extern rtx fis_get_condition (rtx);\n extern int gcse_main (rtx, FILE *);\n extern int bypass_jumps (FILE *);\n-extern void gcse_after_reload_main (rtx, FILE *);\n+\n+/* In postreload-gcse.c */\n+extern void gcse_after_reload_main (rtx);\n \n /* In global.c */\n extern void mark_elimination (int, int);"}, {"sha": "14591b5a7cd52347189896e5aeb21f20a54b2971", "filename": "gcc/timevar.def", "status": "modified", "additions": 1, "deletions": 0, "changes": 1, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2Ftimevar.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/0516f6fe82641daf7c1ac8812998049ac591201e/gcc%2Ftimevar.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftimevar.def?ref=0516f6fe82641daf7c1ac8812998049ac591201e", "patch": "@@ -122,6 +122,7 @@ DEFTIMEVAR (TV_SCHED                 , \"scheduling\")\n DEFTIMEVAR (TV_LOCAL_ALLOC           , \"local alloc\")\n DEFTIMEVAR (TV_GLOBAL_ALLOC          , \"global alloc\")\n DEFTIMEVAR (TV_RELOAD_CSE_REGS       , \"reload CSE regs\")\n+DEFTIMEVAR (TV_GCSE_AFTER_RELOAD      , \"load CSE after reload\")\n DEFTIMEVAR (TV_FLOW2                 , \"flow 2\")\n DEFTIMEVAR (TV_IFCVT2\t\t     , \"if-conversion 2\")\n DEFTIMEVAR (TV_PEEPHOLE2             , \"peephole 2\")"}]}