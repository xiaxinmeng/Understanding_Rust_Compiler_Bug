{"sha": "01914336a927902b9a4e726e41018b5e1223fcb6", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MDE5MTQzMzZhOTI3OTAyYjlhNGU3MjZlNDEwMThiNWUxMjIzZmNiNg==", "commit": {"author": {"name": "Martin Jambor", "email": "mjambor@suse.cz", "date": "2016-12-14T22:36:45Z"}, "committer": {"name": "Martin Jambor", "email": "jamborm@gcc.gnu.org", "date": "2016-12-14T22:36:45Z"}, "message": "Coding style fixes\n\n2016-12-14  Martin Jambor  <mjambor@suse.cz>\n\n\t    * omp-offload.c: Fix coding style.\n\t    * omp-expand.c: Likewise.\n\t    * omp-general.c: Likewise.\n\t    * omp-grid.c: Likewise.\n\t    * omp-low.c: Fix coding style of parts touched by the\n\t      previous splitting patch.\n\nFrom-SVN: r243674", "tree": {"sha": "03ad319068b4554899a471b7c87740cb3fcd107a", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/03ad319068b4554899a471b7c87740cb3fcd107a"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/01914336a927902b9a4e726e41018b5e1223fcb6", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/01914336a927902b9a4e726e41018b5e1223fcb6", "html_url": "https://github.com/Rust-GCC/gccrs/commit/01914336a927902b9a4e726e41018b5e1223fcb6", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/01914336a927902b9a4e726e41018b5e1223fcb6/comments", "author": {"login": "jamborm", "id": 2180070, "node_id": "MDQ6VXNlcjIxODAwNzA=", "avatar_url": "https://avatars.githubusercontent.com/u/2180070?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jamborm", "html_url": "https://github.com/jamborm", "followers_url": "https://api.github.com/users/jamborm/followers", "following_url": "https://api.github.com/users/jamborm/following{/other_user}", "gists_url": "https://api.github.com/users/jamborm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jamborm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jamborm/subscriptions", "organizations_url": "https://api.github.com/users/jamborm/orgs", "repos_url": "https://api.github.com/users/jamborm/repos", "events_url": "https://api.github.com/users/jamborm/events{/privacy}", "received_events_url": "https://api.github.com/users/jamborm/received_events", "type": "User", "site_admin": false}, "committer": null, "parents": [{"sha": "629b3d75c8c5a244d891a9c292bca6912d4b0dd9", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/629b3d75c8c5a244d891a9c292bca6912d4b0dd9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/629b3d75c8c5a244d891a9c292bca6912d4b0dd9"}], "stats": {"total": 220, "additions": 117, "deletions": 103}, "files": [{"sha": "b3a613147af032d372fc5bce30f4f8b4bd35f6e5", "filename": "gcc/ChangeLog", "status": "modified", "additions": 9, "deletions": 0, "changes": 9, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/01914336a927902b9a4e726e41018b5e1223fcb6/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/01914336a927902b9a4e726e41018b5e1223fcb6/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=01914336a927902b9a4e726e41018b5e1223fcb6", "patch": "@@ -1,3 +1,12 @@\n+2016-12-14  Martin Jambor  <mjambor@suse.cz>\n+\n+\t    * omp-offload.c: Fix coding style.\n+\t    * omp-expand.c: Likewise.\n+\t    * omp-general.c: Likewise.\n+\t    * omp-grid.c: Likewise.\n+\t    * omp-low.c: Fix coding style of parts touched by the\n+\t      previous splitting patch.\n+\n 2016-12-14  Martin Jambor  <mjambor@suse.cz>\n \n \t* omp-general.h: New file."}, {"sha": "1f1055cab9a628d6abd2882516af0b57ec38e5be", "filename": "gcc/omp-expand.c", "status": "modified", "additions": 49, "deletions": 46, "changes": 95, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/01914336a927902b9a4e726e41018b5e1223fcb6/gcc%2Fomp-expand.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/01914336a927902b9a4e726e41018b5e1223fcb6/gcc%2Fomp-expand.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fomp-expand.c?ref=01914336a927902b9a4e726e41018b5e1223fcb6", "patch": "@@ -137,7 +137,7 @@ is_combined_parallel (struct omp_region *region)\n \n    Is lowered into:\n \n-   \t# BLOCK 2 (PAR_ENTRY_BB)\n+\t# BLOCK 2 (PAR_ENTRY_BB)\n \t.omp_data_o.i = i;\n \t#pragma omp parallel [child fn: bar.omp_fn.0 ( ..., D.1598)\n \n@@ -1202,7 +1202,7 @@ expand_omp_taskreg (struct omp_region *region)\n \n \t\t  if (TREE_CODE (arg) == ADDR_EXPR\n \t\t      && TREE_OPERAND (arg, 0)\n-\t\t        == gimple_omp_taskreg_data_arg (entry_stmt))\n+\t\t\t== gimple_omp_taskreg_data_arg (entry_stmt))\n \t\t    {\n \t\t      parcopy_stmt = stmt;\n \t\t      break;\n@@ -1219,7 +1219,7 @@ expand_omp_taskreg (struct omp_region *region)\n \t\tgsi_remove (&gsi, true);\n \t      else\n \t\t{\n-\t          /* ?? Is setting the subcode really necessary ??  */\n+\t\t  /* ?? Is setting the subcode really necessary ??  */\n \t\t  gimple_omp_set_subcode (parcopy_stmt, TREE_CODE (arg));\n \t\t  gimple_assign_set_rhs1 (parcopy_stmt, arg);\n \t\t}\n@@ -1317,7 +1317,7 @@ expand_omp_taskreg (struct omp_region *region)\n \t  set_immediate_dominator (CDI_DOMINATORS, dest_bb, new_bb);\n \t}\n       /* When the OMP expansion process cannot guarantee an up-to-date\n-         loop tree arrange for the child function to fixup loops.  */\n+\t loop tree arrange for the child function to fixup loops.  */\n       if (loops_state_satisfies_p (LOOPS_NEED_FIXUP))\n \tchild_cfun->x_current_loops->state |= LOOPS_NEED_FIXUP;\n \n@@ -1401,7 +1401,7 @@ expand_omp_taskreg (struct omp_region *region)\n \n struct oacc_collapse\n {\n-  tree base;  /* Base value. */\n+  tree base;  /* Base value.  */\n   tree iters; /* Number of steps.  */\n   tree step;  /* step size.  */\n };\n@@ -1449,7 +1449,7 @@ expand_oacc_collapse_init (const struct omp_for_data *fd,\n       e = force_gimple_operand_gsi (gsi, e, true, NULL_TREE,\n \t\t\t\t    true, GSI_SAME_STMT);\n \n-      /* Convert the step, avoiding possible unsigned->signed overflow. */\n+      /* Convert the step, avoiding possible unsigned->signed overflow.  */\n       negating = !up && TYPE_UNSIGNED (TREE_TYPE (s));\n       if (negating)\n \ts = fold_build1 (NEGATE_EXPR, TREE_TYPE (s), s);\n@@ -1459,7 +1459,7 @@ expand_oacc_collapse_init (const struct omp_for_data *fd,\n       s = force_gimple_operand_gsi (gsi, s, true, NULL_TREE,\n \t\t\t\t    true, GSI_SAME_STMT);\n \n-      /* Determine the range, avoiding possible unsigned->signed overflow. */\n+      /* Determine the range, avoiding possible unsigned->signed overflow.  */\n       negating = !up && TYPE_UNSIGNED (iter_type);\n       expr = fold_build2 (MINUS_EXPR, plus_type,\n \t\t\t  fold_convert (plus_type, negating ? b : e),\n@@ -2749,7 +2749,7 @@ expand_omp_for_generic (struct omp_region *region,\n     t = fold_build2 (NE_EXPR, boolean_type_node,\n \t\t     t, build_int_cst (TREE_TYPE (t), 0));\n   t = force_gimple_operand_gsi (&gsi, t, true, NULL_TREE,\n-\t\t\t       \ttrue, GSI_SAME_STMT);\n+\t\t\t\ttrue, GSI_SAME_STMT);\n   if (arr && !TREE_STATIC (arr))\n     {\n       tree clobber = build_constructor (TREE_TYPE (arr), NULL);\n@@ -3696,7 +3696,7 @@ find_phi_with_arg_on_edge (tree arg, edge e)\n \t\t\t\t\t      if the loop is not entered\n     L0:\n \ts0 = (trip * nthreads + threadid) * CHUNK;\n-\te0 = min(s0 + CHUNK, n);\n+\te0 = min (s0 + CHUNK, n);\n \tif (s0 < n) goto L1; else goto L4;\n     L1:\n \tV = s0 * STEP + N1;\n@@ -4136,7 +4136,8 @@ expand_omp_for_static_chunk (struct omp_region *region,\n       find_edge (cont_bb, trip_update_bb)->flags\n \t= se ? EDGE_FALSE_VALUE : EDGE_FALLTHRU;\n \n-      redirect_edge_and_branch (single_succ_edge (trip_update_bb), iter_part_bb);\n+      redirect_edge_and_branch (single_succ_edge (trip_update_bb),\n+\t\t\t\titer_part_bb);\n     }\n \n   if (gimple_in_ssa_p (cfun))\n@@ -4319,9 +4320,9 @@ expand_cilk_for (struct omp_region *region, struct omp_for_data *fd)\n      where we should put low and high (reasoning given in header\n      comment).  */\n \n-  tree child_fndecl\n-    = gimple_omp_parallel_child_fn (\n-        as_a <gomp_parallel *> (last_stmt (region->outer->entry)));\n+  gomp_parallel *par_stmt\n+    = as_a <gomp_parallel *> (last_stmt (region->outer->entry));\n+  tree child_fndecl = gimple_omp_parallel_child_fn (par_stmt);\n   tree t, low_val = NULL_TREE, high_val = NULL_TREE;\n   for (t = DECL_ARGUMENTS (child_fndecl); t; t = TREE_CHAIN (t))\n     {\n@@ -4802,7 +4803,7 @@ expand_omp_simd (struct omp_region *region, struct omp_for_data *fd)\n \t the loop.  */\n       if ((flag_tree_loop_vectorize\n \t   || (!global_options_set.x_flag_tree_loop_vectorize\n-               && !global_options_set.x_flag_tree_vectorize))\n+\t       && !global_options_set.x_flag_tree_vectorize))\n \t  && flag_tree_loop_optimize\n \t  && loop->safelen > 1)\n \t{\n@@ -5373,7 +5374,7 @@ expand_oacc_for (struct omp_region *region, struct omp_for_data *fd)\n   b = force_gimple_operand_gsi (&gsi, b, true, NULL_TREE, true, GSI_SAME_STMT);\n   e = force_gimple_operand_gsi (&gsi, e, true, NULL_TREE, true, GSI_SAME_STMT);\n \n-  /* Convert the step, avoiding possible unsigned->signed overflow. */\n+  /* Convert the step, avoiding possible unsigned->signed overflow.  */\n   negating = !up && TYPE_UNSIGNED (TREE_TYPE (s));\n   if (negating)\n     s = fold_build1 (NEGATE_EXPR, TREE_TYPE (s), s);\n@@ -5387,7 +5388,7 @@ expand_oacc_for (struct omp_region *region, struct omp_for_data *fd)\n   expr = fold_convert (diff_type, chunk_size);\n   chunk_size = force_gimple_operand_gsi (&gsi, expr, true,\n \t\t\t\t\t NULL_TREE, true, GSI_SAME_STMT);\n-  /* Determine the range, avoiding possible unsigned->signed overflow. */\n+  /* Determine the range, avoiding possible unsigned->signed overflow.  */\n   negating = !up && TYPE_UNSIGNED (iter_type);\n   expr = fold_build2 (MINUS_EXPR, plus_type,\n \t\t      fold_convert (plus_type, negating ? b : e),\n@@ -5432,7 +5433,7 @@ expand_oacc_for (struct omp_region *region, struct omp_for_data *fd)\n   /* Remove the GIMPLE_OMP_FOR.  */\n   gsi_remove (&gsi, true);\n \n-  /* Fixup edges from head_bb */\n+  /* Fixup edges from head_bb.  */\n   be = BRANCH_EDGE (head_bb);\n   fte = FALLTHRU_EDGE (head_bb);\n   be->flags |= EDGE_FALSE_VALUE;\n@@ -5522,7 +5523,7 @@ expand_oacc_for (struct omp_region *region, struct omp_for_data *fd)\n       /*  Remove the GIMPLE_OMP_CONTINUE.  */\n       gsi_remove (&gsi, true);\n \n-      /* Fixup edges from cont_bb */\n+      /* Fixup edges from cont_bb.  */\n       be = BRANCH_EDGE (cont_bb);\n       fte = FALLTHRU_EDGE (cont_bb);\n       be->flags |= EDGE_TRUE_VALUE;\n@@ -5532,7 +5533,7 @@ expand_oacc_for (struct omp_region *region, struct omp_for_data *fd)\n \t{\n \t  /* Split the beginning of exit_bb to make bottom_bb.  We\n \t     need to insert a nop at the start, because splitting is\n-  \t     after a stmt, not before.  */\n+\t     after a stmt, not before.  */\n \t  gsi = gsi_start_bb (exit_bb);\n \t  stmt = gimple_build_nop ();\n \t  gsi_insert_before (&gsi, stmt, GSI_SAME_STMT);\n@@ -5552,7 +5553,7 @@ expand_oacc_for (struct omp_region *region, struct omp_for_data *fd)\n \t  gsi_insert_after (&gsi, gimple_build_cond_empty (expr),\n \t\t\t    GSI_CONTINUE_LINKING);\n \n-\t  /* Fixup edges from bottom_bb. */\n+\t  /* Fixup edges from bottom_bb.  */\n \t  split->flags ^= EDGE_FALLTHRU | EDGE_FALSE_VALUE;\n \t  make_edge (bottom_bb, head_bb, EDGE_TRUE_VALUE);\n \t}\n@@ -5577,7 +5578,7 @@ expand_oacc_for (struct omp_region *region, struct omp_for_data *fd)\n       gsi_insert_before (&gsi, ass, GSI_SAME_STMT);\n     }\n \n-  /* Remove the OMP_RETURN. */\n+  /* Remove the OMP_RETURN.  */\n   gsi_remove (&gsi, true);\n \n   if (cont_bb)\n@@ -5779,7 +5780,7 @@ expand_omp_sections (struct omp_region *region)\n       si = gsi_last_bb (e->dest);\n       l2 = NULL_TREE;\n       if (gsi_end_p (si)\n-          || gimple_code (gsi_stmt (si)) != GIMPLE_OMP_SECTION)\n+\t  || gimple_code (gsi_stmt (si)) != GIMPLE_OMP_SECTION)\n \tl2 = gimple_block_label (e->dest);\n       else\n \tFOR_EACH_EDGE (e, ei, l0_bb->succs)\n@@ -6277,7 +6278,7 @@ expand_omp_atomic_fetch_op (basic_block load_bb,\n \n       oldval = *addr;\n       repeat:\n-        newval = rhs;\t // with oldval replacing *addr in rhs\n+\tnewval = rhs;\t // with oldval replacing *addr in rhs\n \toldval = __sync_val_compare_and_swap (addr, oldval, newval);\n \tif (oldval != newval)\n \t  goto repeat;\n@@ -6398,11 +6399,11 @@ expand_omp_atomic_pipeline (basic_block load_bb, basic_block store_bb,\n   if (iaddr == addr)\n     storedi = stored_val;\n   else\n-    storedi =\n-      force_gimple_operand_gsi (&si,\n-\t\t\t\tbuild1 (VIEW_CONVERT_EXPR, itype,\n-\t\t\t\t\tstored_val), true, NULL_TREE, true,\n-\t\t\t\tGSI_SAME_STMT);\n+    storedi\n+      = force_gimple_operand_gsi (&si,\n+\t\t\t\t  build1 (VIEW_CONVERT_EXPR, itype,\n+\t\t\t\t\t  stored_val), true, NULL_TREE, true,\n+\t\t\t\t  GSI_SAME_STMT);\n \n   /* Build the compare&swap statement.  */\n   new_storedi = build_call_expr (cmpxchg, 3, iaddr, loadedi, storedi);\n@@ -6427,9 +6428,8 @@ expand_omp_atomic_pipeline (basic_block load_bb, basic_block store_bb,\n   /* Note that we always perform the comparison as an integer, even for\n      floating point.  This allows the atomic operation to properly\n      succeed even with NaNs and -0.0.  */\n-  stmt = gimple_build_cond_empty\n-           (build2 (NE_EXPR, boolean_type_node,\n-\t\t    new_storedi, old_vali));\n+  tree ne = build2 (NE_EXPR, boolean_type_node, new_storedi, old_vali);\n+  stmt = gimple_build_cond_empty (ne);\n   gsi_insert_before (&si, stmt, GSI_SAME_STMT);\n \n   /* Update cfg.  */\n@@ -6463,9 +6463,9 @@ expand_omp_atomic_pipeline (basic_block load_bb, basic_block store_bb,\n \n /* A subroutine of expand_omp_atomic.  Implement the atomic operation as:\n \n-\t\t \t\t  GOMP_atomic_start ();\n-\t\t \t\t  *addr = rhs;\n-\t\t \t\t  GOMP_atomic_end ();\n+\t\t\t\t  GOMP_atomic_start ();\n+\t\t\t\t  *addr = rhs;\n+\t\t\t\t  GOMP_atomic_end ();\n \n    The result is not globally atomic, but works so long as all parallel\n    references are within #pragma omp atomic directives.  According to\n@@ -6522,7 +6522,7 @@ expand_omp_atomic_mutex (basic_block load_bb, basic_block store_bb,\n }\n \n /* Expand an GIMPLE_OMP_ATOMIC statement.  We try to expand\n-   using expand_omp_atomic_fetch_op. If it failed, we try to\n+   using expand_omp_atomic_fetch_op.  If it failed, we try to\n    call expand_omp_atomic_pipeline, and if it fails too, the\n    ultimate fallback is wrapping the operation in a mutex\n    (expand_omp_atomic_mutex).  REGION is the atomic region built\n@@ -6618,7 +6618,9 @@ mark_loops_in_oacc_kernels_region (basic_block region_entry,\n   if (nr_outer_loops != 1)\n     return;\n \n-  for (struct loop *loop = single_outer->inner; loop != NULL; loop = loop->inner)\n+  for (struct loop *loop = single_outer->inner;\n+       loop != NULL;\n+       loop = loop->inner)\n     if (loop->next)\n       return;\n \n@@ -6800,7 +6802,7 @@ push_target_argument_according_to_value (gimple_stmt_iterator *gsi, int device,\n     }\n }\n \n-/* Create an array of arguments that is then passed to GOMP_target.   */\n+/* Create an array of arguments that is then passed to GOMP_target.  */\n \n static tree\n get_target_arguments (gimple_stmt_iterator *gsi, gomp_target *tgt_stmt)\n@@ -6828,8 +6830,8 @@ get_target_arguments (gimple_stmt_iterator *gsi, gomp_target *tgt_stmt)\n   if (omp_find_clause (gimple_omp_target_clauses (tgt_stmt),\n \t\t       OMP_CLAUSE__GRIDDIM_))\n     {\n-      t = get_target_argument_identifier (GOMP_DEVICE_HSA, true,\n-\t\t\t\t\t  GOMP_TARGET_ARG_HSA_KERNEL_ATTRIBUTES);\n+      int id = GOMP_TARGET_ARG_HSA_KERNEL_ATTRIBUTES;\n+      t = get_target_argument_identifier (GOMP_DEVICE_HSA, true, id);\n       args.quick_push (t);\n       args.quick_push (grid_get_kernel_launch_attributes (gsi, tgt_stmt));\n     }\n@@ -7378,7 +7380,7 @@ expand_omp_target (struct omp_region *region)\n /* Expand KFOR loop as a HSA grifidied kernel, i.e. as a body only with\n    iteration variable derived from the thread number.  INTRA_GROUP means this\n    is an expansion of a loop iterating over work-items within a separate\n-   iteration over groups. */\n+   iteration over groups.  */\n \n static void\n grid_expand_omp_for_loop (struct omp_region *kfor, bool intra_group)\n@@ -7390,7 +7392,7 @@ grid_expand_omp_for_loop (struct omp_region *kfor, bool intra_group)\n   size_t collapse = gimple_omp_for_collapse (for_stmt);\n   struct omp_for_data_loop *loops\n     = XALLOCAVEC (struct omp_for_data_loop,\n-                  gimple_omp_for_collapse (for_stmt));\n+\t\t  gimple_omp_for_collapse (for_stmt));\n   struct omp_for_data fd;\n \n   remove_edge (BRANCH_EDGE (kfor->entry));\n@@ -7448,7 +7450,7 @@ grid_expand_omp_for_loop (struct omp_region *kfor, bool intra_group)\n       gassign *assign_stmt = gimple_build_assign (startvar, t);\n       gsi_insert_before (&gsi, assign_stmt, GSI_SAME_STMT);\n     }\n-  /* Remove the omp for statement */\n+  /* Remove the omp for statement.  */\n   gsi = gsi_last_bb (kfor->entry);\n   gsi_remove (&gsi, true);\n \n@@ -7500,7 +7502,7 @@ grid_remap_kernel_arg_accesses (tree *tp, int *walk_subtrees, void *data)\n }\n \n /* If TARGET region contains a kernel body for loop, remove its region from the\n-   TARGET and expand it in HSA gridified kernel fashion. */\n+   TARGET and expand it in HSA gridified kernel fashion.  */\n \n static void\n grid_expand_target_grid_body (struct omp_region *target)\n@@ -7534,7 +7536,8 @@ grid_expand_target_grid_body (struct omp_region *target)\n \n   gcc_assert (omp_find_clause (gimple_omp_target_clauses (tgt_stmt),\n \t\t\t       OMP_CLAUSE__GRIDDIM_));\n-  tree inside_block = gimple_block (first_stmt (single_succ (gpukernel->entry)));\n+  tree inside_block\n+    = gimple_block (first_stmt (single_succ (gpukernel->entry)));\n   *pp = gpukernel->next;\n   for (pp = &gpukernel->inner; *pp; pp = &(*pp)->next)\n     if ((*pp)->type == GIMPLE_OMP_FOR)\n@@ -7596,7 +7599,7 @@ grid_expand_target_grid_body (struct omp_region *target)\n \n   grid_expand_omp_for_loop (kfor, false);\n \n-  /* Remove the omp for statement */\n+  /* Remove the omp for statement.  */\n   gimple_stmt_iterator gsi = gsi_last_bb (gpukernel->entry);\n   gsi_remove (&gsi, true);\n   /* Replace the GIMPLE_OMP_RETURN at the end of the kernel region with a real\n@@ -7685,7 +7688,7 @@ expand_omp (struct omp_region *region)\n       gimple *inner_stmt = NULL;\n \n       /* First, determine whether this is a combined parallel+workshare\n-       \t region.  */\n+\t region.  */\n       if (region->type == GIMPLE_OMP_PARALLEL)\n \tdetermine_parallel_type (region);\n       else if (region->type == GIMPLE_OMP_TARGET)"}, {"sha": "cac9bed9daf657e2570949008258a1039ab40a60", "filename": "gcc/omp-general.c", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/01914336a927902b9a4e726e41018b5e1223fcb6/gcc%2Fomp-general.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/01914336a927902b9a4e726e41018b5e1223fcb6/gcc%2Fomp-general.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fomp-general.c?ref=01914336a927902b9a4e726e41018b5e1223fcb6", "patch": "@@ -418,7 +418,7 @@ omp_max_vf (void)\n       || !flag_tree_loop_optimize\n       || (!flag_tree_loop_vectorize\n \t  && (global_options_set.x_flag_tree_loop_vectorize\n-              || global_options_set.x_flag_tree_vectorize)))\n+\t      || global_options_set.x_flag_tree_vectorize)))\n     return 1;\n \n   int vf = 1;\n@@ -442,7 +442,7 @@ omp_max_simt_vf (void)\n   if (!optimize)\n     return 0;\n   if (ENABLE_OFFLOADING)\n-    for (const char *c = getenv (\"OFFLOAD_TARGET_NAMES\"); c; )\n+    for (const char *c = getenv (\"OFFLOAD_TARGET_NAMES\"); c;)\n       {\n \tif (!strncmp (c, \"nvptx\", strlen (\"nvptx\")))\n \t  return 32;\n@@ -481,7 +481,7 @@ oacc_launch_pack (unsigned code, tree device, unsigned op)\n    represented as a list of INTEGER_CST.  Those that are runtime\n    exprs are represented as an INTEGER_CST of zero.\n \n-   TOOO. Normally the attribute will just contain a single such list.  If\n+   TODO: Normally the attribute will just contain a single such list.  If\n    however it contains a list of lists, this will represent the use of\n    device_type.  Each member of the outer list is an assoc list of\n    dimensions, keyed by the device type.  The first entry will be the\n@@ -566,8 +566,8 @@ tree\n oacc_build_routine_dims (tree clauses)\n {\n   /* Must match GOMP_DIM ordering.  */\n-  static const omp_clause_code ids[] =\n-    {OMP_CLAUSE_GANG, OMP_CLAUSE_WORKER, OMP_CLAUSE_VECTOR, OMP_CLAUSE_SEQ};\n+  static const omp_clause_code ids[]\n+    = {OMP_CLAUSE_GANG, OMP_CLAUSE_WORKER, OMP_CLAUSE_VECTOR, OMP_CLAUSE_SEQ};\n   int ix;\n   int level = -1;\n "}, {"sha": "2b469f2ce8f20ada13869642532b8ce7cba85f78", "filename": "gcc/omp-grid.c", "status": "modified", "additions": 23, "deletions": 18, "changes": 41, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/01914336a927902b9a4e726e41018b5e1223fcb6/gcc%2Fomp-grid.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/01914336a927902b9a4e726e41018b5e1223fcb6/gcc%2Fomp-grid.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fomp-grid.c?ref=01914336a927902b9a4e726e41018b5e1223fcb6", "patch": "@@ -48,7 +48,7 @@ omp_grid_lastprivate_predicate (struct omp_for_data *fd)\n   /* When dealing with a gridified loop, we need to check up to three collapsed\n      iteration variables but they are not actually captured in this fd.\n      Fortunately, we can easily rely on HSA builtins to get this\n-     information. */\n+     information.  */\n \n   tree id, size;\n   if (gimple_omp_for_kind (fd->for_stmt) == GF_OMP_FOR_KIND_GRID_LOOP\n@@ -81,7 +81,7 @@ omp_grid_lastprivate_predicate (struct omp_for_data *fd)\n }\n \n /* Structure describing the basic properties of the loop we ara analyzing\n-   whether it can be gridified and when it is gridified. */\n+   whether it can be gridified and when it is gridified.  */\n \n struct grid_prop\n {\n@@ -162,8 +162,9 @@ grid_find_single_omp_among_assignments_1 (gimple_seq seq, grid_prop *grid,\n \tcontinue;\n       if (gbind *bind = dyn_cast <gbind *> (stmt))\n \t{\n-\t  if (!grid_find_single_omp_among_assignments_1 (gimple_bind_body (bind),\n-\t\t\t\t\t\t\t grid, name, ret))\n+\t  gimple_seq bind_body = gimple_bind_body (bind);\n+\t  if (!grid_find_single_omp_among_assignments_1 (bind_body, grid, name,\n+\t\t\t\t\t\t\t ret))\n \t      return false;\n \t}\n       else if (is_gimple_omp (stmt))\n@@ -325,7 +326,7 @@ grid_parallel_clauses_gridifiable (gomp_parallel *par, location_t tloc)\n \n /* Examine clauses and the body of omp loop statement GFOR and if something\n    prevents gridification, issue a missed-optimization diagnostics and return\n-   false, otherwise return true. GRID describes hitherto discovered properties\n+   false, otherwise return true.  GRID describes hitherto discovered properties\n    of the loop that is evaluated for possible gridification.  */\n \n static bool\n@@ -414,7 +415,7 @@ grid_inner_loop_gridifiable_p (gomp_for *gfor, grid_prop *grid)\n \n /* Given distribute omp construct represented by DIST, which in the original\n    source forms a compound construct with a looping construct, return true if it\n-   can be turned into a gridified HSA kernel.  Otherwise return false. GRID\n+   can be turned into a gridified HSA kernel.  Otherwise return false.  GRID\n    describes hitherto discovered properties of the loop that is evaluated for\n    possible gridification.  */\n \n@@ -455,7 +456,7 @@ grid_dist_follows_simple_pattern (gomp_for *dist, grid_prop *grid)\n /* Given an omp loop statement GFOR, return true if it can participate in\n    tiling gridification, i.e. in one where the distribute and parallel for\n    loops do not form a compound statement.  GRID describes hitherto discovered\n-   properties of the loop that is evaluated for possible gridification. */\n+   properties of the loop that is evaluated for possible gridification.  */\n \n static bool\n grid_gfor_follows_tiling_pattern (gomp_for *gfor, grid_prop *grid)\n@@ -599,7 +600,7 @@ grid_handle_call_in_distribute (gimple_stmt_iterator *gsi)\n /* Given a sequence of statements within a distribute omp construct or a\n    parallel construct, which in the original source does not form a compound\n    construct with a looping construct, return true if it does not prevent us\n-   from turning it into a gridified HSA kernel.  Otherwise return false. GRID\n+   from turning it into a gridified HSA kernel.  Otherwise return false.  GRID\n    describes hitherto discovered properties of the loop that is evaluated for\n    possible gridification.  IN_PARALLEL must be true if seq is within a\n    parallel construct and flase if it is only within a distribute\n@@ -910,7 +911,7 @@ grid_mark_variable_segment (tree var, enum grid_var_segment segment)\n      their uses.  Fortunately, we do not have to do this because if they are\n      not addressable, it means they are not used in atomic or parallel\n      statements and so relaxed GPU consistency rules mean we can just keep them\n-     private. */\n+     private.  */\n   if (!TREE_ADDRESSABLE (var))\n     return;\n \n@@ -961,7 +962,9 @@ grid_copy_leading_local_assignments (gimple_seq src, gimple_stmt_iterator *dst,\n \t    (gimple_bind_body (bind), dst, tgt_bind, var_segment, wi);\n \n \t  if (var_segment != GRID_SEGMENT_PRIVATE)\n-\t    for (tree var = gimple_bind_vars (bind); var; var = DECL_CHAIN (var))\n+\t    for (tree var = gimple_bind_vars (bind);\n+\t\t var;\n+\t\t var = DECL_CHAIN (var))\n \t      grid_mark_variable_segment (var, var_segment);\n \t  if (r)\n \t    return r;\n@@ -1191,7 +1194,8 @@ grid_process_kernel_body_copy (grid_prop *grid, gimple_seq seq,\n   gcc_assert (teams);\n   gimple_omp_teams_set_grid_phony (teams, true);\n   stmt = grid_copy_leading_local_assignments (gimple_omp_body (teams), dst,\n-\t\t\t\t\t      tgt_bind, GRID_SEGMENT_GLOBAL, wi);\n+\t\t\t\t\t      tgt_bind, GRID_SEGMENT_GLOBAL,\n+\t\t\t\t\t      wi);\n   gcc_checking_assert (stmt);\n   gomp_for *dist = dyn_cast <gomp_for *> (stmt);\n   gcc_assert (dist);\n@@ -1278,7 +1282,8 @@ grid_attempt_target_gridification (gomp_target *target,\n   gomp_for *inner_loop = grid_process_kernel_body_copy (&grid, kernel_seq, gsi,\n \t\t\t\t\t\t\ttgt_bind, &wi);\n \n-  gbind *old_bind = as_a <gbind *> (gimple_seq_first (gimple_omp_body (target)));\n+  gbind *old_bind\n+    = as_a <gbind *> (gimple_seq_first (gimple_omp_body (target)));\n   gbind *new_bind = as_a <gbind *> (gimple_seq_first (kernel_seq));\n   tree new_block = gimple_bind_block (new_bind);\n   tree enc_block = BLOCK_SUPERCONTEXT (gimple_bind_block (old_bind));\n@@ -1324,11 +1329,11 @@ grid_attempt_target_gridification (gomp_target *target,\n       else\n \tt = fold_build2 (TRUNC_DIV_EXPR, itype, t, step);\n       if (grid.tiling)\n-        {\n-          if (cond_code == GT_EXPR)\n-            step = fold_build1 (NEGATE_EXPR, itype, step);\n-          t = fold_build2 (MULT_EXPR, itype, t, step);\n-        }\n+\t{\n+\t  if (cond_code == GT_EXPR)\n+\t    step = fold_build1 (NEGATE_EXPR, itype, step);\n+\t  t = fold_build2 (MULT_EXPR, itype, t, step);\n+\t}\n \n       tree gs = fold_convert (uint32_type_node, t);\n       gimple_seq tmpseq = NULL;\n@@ -1360,7 +1365,7 @@ grid_attempt_target_gridification (gomp_target *target,\n   return;\n }\n \n-/* Walker function doing all the work for create_target_kernels. */\n+/* Walker function doing all the work for create_target_kernels.  */\n \n static tree\n grid_gridify_all_targets_stmt (gimple_stmt_iterator *gsi,"}, {"sha": "e69b2b29de57cc80f40c6a7ec296194f51496af2", "filename": "gcc/omp-low.c", "status": "modified", "additions": 20, "deletions": 23, "changes": 43, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/01914336a927902b9a4e726e41018b5e1223fcb6/gcc%2Fomp-low.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/01914336a927902b9a4e726e41018b5e1223fcb6/gcc%2Fomp-low.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fomp-low.c?ref=01914336a927902b9a4e726e41018b5e1223fcb6", "patch": "@@ -2516,7 +2516,7 @@ check_omp_nesting_restrictions (gimple *stmt, omp_context *ctx)\n       if (gimple_omp_for_kind (stmt) == GF_OMP_FOR_KIND_OACC_LOOP)\n \t{\n \t  bool ok = false;\n-\t  \n+\n \t  if (ctx)\n \t    switch (gimple_code (ctx->stmt))\n \t      {\n@@ -3431,7 +3431,7 @@ omp_clause_aligned_alignment (tree clause)\n \t       && GET_MODE_SIZE (vmode) < vs\n \t       && GET_MODE_2XWIDER_MODE (vmode) != VOIDmode)\n \t  vmode = GET_MODE_2XWIDER_MODE (vmode);\n-\t\n+\n \ttree type = lang_hooks.types.type_for_mode (mode, 1);\n \tif (type == NULL_TREE || TYPE_MODE (type) != mode)\n \t  continue;\n@@ -4851,7 +4851,7 @@ lower_oacc_reductions (location_t loc, tree clauses, tree level, bool inner,\n \t  var = orig;\n \n \tincoming = outgoing = var;\n-\t\n+\n \tif (!inner)\n \t  {\n \t    /* See if an outer construct also reduces this variable.  */\n@@ -4879,7 +4879,7 @@ lower_oacc_reductions (location_t loc, tree clauses, tree level, bool inner,\n \t\t  default:\n \t\t    goto do_lookup;\n \t\t  }\n-\t\t\n+\n \t\touter = probe;\n \t\tfor (; cls;  cls = OMP_CLAUSE_CHAIN (cls))\n \t\t  if (OMP_CLAUSE_CODE (cls) == OMP_CLAUSE_REDUCTION\n@@ -4927,14 +4927,14 @@ lower_oacc_reductions (location_t loc, tree clauses, tree level, bool inner,\n \t\t  }\n \t\tincoming = outgoing = (t ? t : orig);\n \t      }\n-\t      \n+\n \t  has_outer_reduction:;\n \t  }\n \n \tif (!ref_to_res)\n \t  ref_to_res = integer_zero_node;\n \n-        if (omp_is_reference (orig))\n+\tif (omp_is_reference (orig))\n \t  {\n \t    tree type = TREE_TYPE (var);\n \t    const char *id = IDENTIFIER_POINTER (DECL_NAME (var));\n@@ -5859,9 +5859,9 @@ lower_omp_sections (gimple_stmt_iterator *gsi_p, omp_context *ctx)\n \n   new_body = maybe_catch_exception (new_body);\n \n-  t = gimple_build_omp_return\n-        (!!omp_find_clause (gimple_omp_sections_clauses (stmt),\n-\t\t\t    OMP_CLAUSE_NOWAIT));\n+  bool nowait = omp_find_clause (gimple_omp_sections_clauses (stmt),\n+\t\t\t\t OMP_CLAUSE_NOWAIT) != NULL_TREE;\n+  t = gimple_build_omp_return (nowait);\n   gimple_seq_add_stmt (&new_body, t);\n   maybe_add_implicit_barrier_cancel (ctx, &new_body);\n \n@@ -5993,7 +5993,6 @@ static void\n lower_omp_single (gimple_stmt_iterator *gsi_p, omp_context *ctx)\n {\n   tree block;\n-  gimple *t;\n   gomp_single *single_stmt = as_a <gomp_single *> (gsi_stmt (*gsi_p));\n   gbind *bind;\n   gimple_seq bind_body, bind_body_tail = NULL, dlist;\n@@ -6022,10 +6021,10 @@ lower_omp_single (gimple_stmt_iterator *gsi_p, omp_context *ctx)\n \n   bind_body = maybe_catch_exception (bind_body);\n \n-  t = gimple_build_omp_return\n-        (!!omp_find_clause (gimple_omp_single_clauses (single_stmt),\n-\t\t\t    OMP_CLAUSE_NOWAIT));\n-  gimple_seq_add_stmt (&bind_body_tail, t);\n+  bool nowait = omp_find_clause (gimple_omp_single_clauses (single_stmt),\n+\t\t\t\t OMP_CLAUSE_NOWAIT) != NULL_TREE;\n+  gimple *g = gimple_build_omp_return (nowait);\n+  gimple_seq_add_stmt (&bind_body_tail, g);\n   maybe_add_implicit_barrier_cancel (ctx, &bind_body_tail);\n   if (ctx->record_type)\n     {\n@@ -6534,7 +6533,8 @@ lower_omp_critical (gimple_stmt_iterator *gsi_p, omp_context *ctx)\n \t    }\n \n       lock = builtin_decl_explicit (BUILT_IN_GOMP_CRITICAL_NAME_START);\n-      lock = build_call_expr_loc (loc, lock, 1, build_fold_addr_expr_loc (loc, decl));\n+      lock = build_call_expr_loc (loc, lock, 1,\n+\t\t\t\t  build_fold_addr_expr_loc (loc, decl));\n \n       unlock = builtin_decl_explicit (BUILT_IN_GOMP_CRITICAL_NAME_END);\n       unlock = build_call_expr_loc (loc, unlock, 1,\n@@ -6811,10 +6811,10 @@ lower_omp_for (gimple_stmt_iterator *gsi_p, omp_context *ctx)\n \t\t\t  gimple_omp_for_clauses (stmt),\n \t\t\t  &oacc_head, &oacc_tail, ctx);\n \n-  /* Add OpenACC partitioning and reduction markers just before the loop  */\n+  /* Add OpenACC partitioning and reduction markers just before the loop.  */\n   if (oacc_head)\n     gimple_seq_add_seq (&body, oacc_head);\n-  \n+\n   lower_omp_for_lastprivate (&fd, &body, &dlist, ctx);\n \n   if (gimple_omp_for_kind (stmt) == GF_OMP_FOR_KIND_FOR)\n@@ -8667,8 +8667,7 @@ lower_omp_1 (gimple_stmt_iterator *gsi_p, omp_context *ctx)\n       lower_omp (gimple_try_cleanup_ptr (stmt), ctx);\n       break;\n     case GIMPLE_TRANSACTION:\n-      lower_omp (gimple_transaction_body_ptr (\n-                   as_a <gtransaction *> (stmt)),\n+      lower_omp (gimple_transaction_body_ptr (as_a <gtransaction *> (stmt)),\n \t\t ctx);\n       break;\n     case GIMPLE_BIND:\n@@ -8961,16 +8960,14 @@ diagnose_sb_0 (gimple_stmt_iterator *gsi_p,\n       kind = \"OpenMP\";\n     }\n \n-  /*\n-     Previously we kept track of the label's entire context in diagnose_sb_[12]\n+  /* Previously we kept track of the label's entire context in diagnose_sb_[12]\n      so we could traverse it and issue a correct \"exit\" or \"enter\" error\n      message upon a structured block violation.\n \n      We built the context by building a list with tree_cons'ing, but there is\n      no easy counterpart in gimple tuples.  It seems like far too much work\n      for issuing exit/enter error messages.  If someone really misses the\n-     distinct error message... patches welcome.\n-   */\n+     distinct error message... patches welcome.  */\n \n #if 0\n   /* Try to avoid confusing the user by producing and error message"}, {"sha": "8c2c6eb86246179ba629df9d2100581c92461f6a", "filename": "gcc/omp-offload.c", "status": "modified", "additions": 11, "deletions": 11, "changes": 22, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/01914336a927902b9a4e726e41018b5e1223fcb6/gcc%2Fomp-offload.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/01914336a927902b9a4e726e41018b5e1223fcb6/gcc%2Fomp-offload.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fomp-offload.c?ref=01914336a927902b9a4e726e41018b5e1223fcb6", "patch": "@@ -61,8 +61,8 @@ struct oacc_loop\n \n   gcall *marker; /* Initial head marker.  */\n \n-  gcall *heads[GOMP_DIM_MAX];  /* Head marker functions. */\n-  gcall *tails[GOMP_DIM_MAX];  /* Tail marker functions. */\n+  gcall *heads[GOMP_DIM_MAX];  /* Head marker functions.  */\n+  gcall *tails[GOMP_DIM_MAX];  /* Tail marker functions.  */\n \n   tree routine;  /* Pseudo-loop enclosing a routine.  */\n \n@@ -273,7 +273,7 @@ oacc_thread_numbers (bool pos, int mask, gimple_seq *seq)\n    operate on adjacent iterations.  At the worker and gang level,\n    each gang/warp executes a set of contiguous iterations.  Chunking\n    can override this such that each iteration engine executes a\n-   contiguous chunk, and then moves on to stride to the next chunk.   */\n+   contiguous chunk, and then moves on to stride to the next chunk.  */\n \n static void\n oacc_xform_loop (gcall *call)\n@@ -521,7 +521,7 @@ oacc_parse_default_dims (const char *dims)\n /* Validate and update the dimensions for offloaded FN.  ATTRS is the\n    raw attribute.  DIMS is an array of dimensions, which is filled in.\n    LEVEL is the partitioning level of a routine, or -1 for an offload\n-   region itself. USED is the mask of partitioned execution in the\n+   region itself.  USED is the mask of partitioned execution in the\n    function.  */\n \n static void\n@@ -638,7 +638,7 @@ new_oacc_loop (oacc_loop *parent, gcall *marker)\n   loop->marker = marker;\n \n   /* TODO: This is where device_type flattening would occur for the loop\n-     flags.   */\n+     flags.  */\n \n   loop->flags = TREE_INT_CST_LOW (gimple_call_arg (marker, 3));\n \n@@ -880,7 +880,7 @@ oacc_loop_sibling_nreverse (oacc_loop *loop)\n   do\n     {\n       if (loop->child)\n-\tloop->child = oacc_loop_sibling_nreverse  (loop->child);\n+\tloop->child = oacc_loop_sibling_nreverse (loop->child);\n \n       oacc_loop *next = loop->sibling;\n       loop->sibling = last;\n@@ -1066,8 +1066,8 @@ oacc_loop_fixed_partitions (oacc_loop *loop, unsigned outer_mask)\n \t  loop->flags &= ~OLF_AUTO;\n \t  if (seq_par)\n \t    {\n-\t      loop->flags &=\n-\t\t~((GOMP_DIM_MASK (GOMP_DIM_MAX) - 1) << OLF_DIM_BASE);\n+\t      loop->flags\n+\t\t&= ~((GOMP_DIM_MASK (GOMP_DIM_MAX) - 1) << OLF_DIM_BASE);\n \t      this_mask = 0;\n \t    }\n \t}\n@@ -1183,14 +1183,14 @@ oacc_loop_auto_partitions (oacc_loop *loop, unsigned outer_mask)\n       /* Allocate the loop at the innermost available level.  */\n       unsigned this_mask = 0;\n \n-      /* Determine the outermost partitioning used within this loop. */\n+      /* Determine the outermost partitioning used within this loop.  */\n       this_mask = loop->inner | GOMP_DIM_MASK (GOMP_DIM_MAX);\n       this_mask = least_bit_hwi (this_mask);\n \n       /* Pick the partitioning just inside that one.  */\n       this_mask >>= 1;\n \n-      /* And avoid picking one use by an outer loop. */\n+      /* And avoid picking one use by an outer loop.  */\n       this_mask &= ~outer_mask;\n \n       if (!this_mask && noisy)\n@@ -1476,7 +1476,7 @@ default_goacc_validate_dims (tree ARG_UNUSED (decl), int *dims,\n   return changed;\n }\n \n-/* Default dimension bound is unknown on accelerator and 1 on host. */\n+/* Default dimension bound is unknown on accelerator and 1 on host.  */\n \n int\n default_goacc_dim_limit (int ARG_UNUSED (axis))"}]}