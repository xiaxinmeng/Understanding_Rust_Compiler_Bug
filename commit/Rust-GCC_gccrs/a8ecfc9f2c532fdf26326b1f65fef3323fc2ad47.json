{"sha": "a8ecfc9f2c532fdf26326b1f65fef3323fc2ad47", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YThlY2ZjOWYyYzUzMmZkZjI2MzI2YjFmNjVmZWYzMzIzZmMyYWQ0Nw==", "commit": {"author": {"name": "Uros Bizjak", "email": "uros@gcc.gnu.org", "date": "2010-06-22T13:36:15Z"}, "committer": {"name": "Uros Bizjak", "email": "uros@gcc.gnu.org", "date": "2010-06-22T13:36:15Z"}, "message": "i386.md (SWI1248x): New mode iterator.\n\n\t* config/i386/i386.md (SWI1248x): New mode iterator.\n\t(SWI48x): Ditto.\n\t(SWI12): Ditto.\n\t(SWI24): Ditto.\n\t\n\t(mov<mode>): Macroize expander from mov{qi,hi,si,di} using\n\tSWI1248x mode iterator.\n\t(*push<mode>2_rex64): Macroize insn from *push{qi,hi,si}_rex64\n\tusing SWI124 mode iterator.\n\t(*push<mode>2): Macroize insn from *push{qi,hi} using SWI12\n\tmode iterator.\n\t(*push<mode>2_prologue): Macroize insn from  *pushsi2_prologue and\n\t*pushdi2_prologue_rex64 using P mode iterator.\n\t(*mov<mode>_xor): Macroize insn from *movsi_xor and *movdi_xor_rex64\n\tusing SWI48 mode iterator.\n\t(*mov<mode>_or): Ditto from *movsi_or and *movdi_or_rex64.\n\t(*movabs<mode>_1): Macroize insn from *movabs{qi,hi,si,di}_1_rex64\n\tusing SWI1248x mode iterator.\n\t(*movabs<mode>_2): Ditto from *movabs{qi,hi,si,di}_1_rex64.\n\t(*swap<mode>): Macroize insn from *swapsi and *swapdi_rex64 using\n\tSWI48 mode iterator.\n\t(*swap<mode>_1): Macroize insn from *swap{qi,hi}_1 using SWI12 mode\n\titerator.\n\t(*swap<mode>_2): Ditto from *swap{qi,hi}_2.\n\t(movstrict<mode>): Macroize expander from movstrict{qi,hi} using\n\tSWI12 mode iterator.\n\t(*movstrict<mode>_1): Macroize insn from *movstrict{qi,hi}_1 using\n\tSWI12 mode iterator.\n\t(*movstrict<mode>_xor): Ditto from *movstrict{qi,hi}_xor.\n\t(*mov<mode>_extv_1): Macroize insn from *mov{hi,si}_extv_1 using\n\tSWI24 mode iterator.\n\t(*mov<mode>_extzv_1): Macroize insn from *mov{si,di}_extzv_1 using\n\tSWI48 mode iterator.\n\t(mov<mode>_insn_1): New expander.\n\t(*mov<mode>_insv_1_rex64): Macroize insn from *mov{si,di}_insv_1_rex64\n\tusing SWI48x mode iterator.\n\n\t(*movoi_internal_avx): Rename from *movoi_internal.\n\t(*movti_internal_rex64): Rename from *movti_rex64.\n\t(*movti_internal_sse): Rename from *movti_sse.\n\t(*movdi_internal_rex64): Rename from *movdi_1_rex64.\n\t(*movdi_internal): Rename from *movdi_2.\n\t(*movsi_internal): Rename from *movsi_1.\n\t(*movhi_internal): Rename from *movhi_1.\n\t(*movqi_internal): Rename from *movqi_1.\n\n\t(insv): Update the call to gen_movsi_insv_1 for rename.\n\t* config/i386/i386.c (promote_duplicated_reg): Ditto.\n\nFrom-SVN: r161185", "tree": {"sha": "2ec5e581162ebc1c47b7164e006b8207380aab87", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/2ec5e581162ebc1c47b7164e006b8207380aab87"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/a8ecfc9f2c532fdf26326b1f65fef3323fc2ad47", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a8ecfc9f2c532fdf26326b1f65fef3323fc2ad47", "html_url": "https://github.com/Rust-GCC/gccrs/commit/a8ecfc9f2c532fdf26326b1f65fef3323fc2ad47", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/a8ecfc9f2c532fdf26326b1f65fef3323fc2ad47/comments", "author": null, "committer": null, "parents": [{"sha": "b81a5940b446f84086ec6dbf536c516ee117b250", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b81a5940b446f84086ec6dbf536c516ee117b250", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b81a5940b446f84086ec6dbf536c516ee117b250"}], "stats": {"total": 1899, "additions": 848, "deletions": 1051}, "files": [{"sha": "ca0ebf53abf2dfbf980de1f9b8da09556dba3afd", "filename": "gcc/ChangeLog", "status": "modified", "additions": 54, "deletions": 6, "changes": 60, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a8ecfc9f2c532fdf26326b1f65fef3323fc2ad47/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a8ecfc9f2c532fdf26326b1f65fef3323fc2ad47/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=a8ecfc9f2c532fdf26326b1f65fef3323fc2ad47", "patch": "@@ -1,3 +1,54 @@\n+2010-06-22  Uros Bizjak  <ubizjak@gmail.com>\n+\n+\t* config/i386/i386.md (SWI1248x): New mode iterator.\n+\t(SWI48x): Ditto.\n+\t(SWI12): Ditto.\n+\t(SWI24): Ditto.\n+\t\n+\t(mov<mode>): Macroize expander from mov{qi,hi,si,di} using\n+\tSWI1248x mode iterator.\n+\t(*push<mode>2_rex64): Macroize insn from *push{qi,hi,si}_rex64\n+\tusing SWI124 mode iterator.\n+\t(*push<mode>2): Macroize insn from *push{qi,hi} using SWI12\n+\tmode iterator.\n+\t(*push<mode>2_prologue): Macroize insn from  *pushsi2_prologue and\n+\t*pushdi2_prologue_rex64 using P mode iterator.\n+\t(*mov<mode>_xor): Macroize insn from *movsi_xor and *movdi_xor_rex64\n+\tusing SWI48 mode iterator.\n+\t(*mov<mode>_or): Ditto from *movsi_or and *movdi_or_rex64.\n+\t(*movabs<mode>_1): Macroize insn from *movabs{qi,hi,si,di}_1_rex64\n+\tusing SWI1248x mode iterator.\n+\t(*movabs<mode>_2): Ditto from *movabs{qi,hi,si,di}_1_rex64.\n+\t(*swap<mode>): Macroize insn from *swapsi and *swapdi_rex64 using\n+\tSWI48 mode iterator.\n+\t(*swap<mode>_1): Macroize insn from *swap{qi,hi}_1 using SWI12 mode\n+\titerator.\n+\t(*swap<mode>_2): Ditto from *swap{qi,hi}_2.\n+\t(movstrict<mode>): Macroize expander from movstrict{qi,hi} using\n+\tSWI12 mode iterator.\n+\t(*movstrict<mode>_1): Macroize insn from *movstrict{qi,hi}_1 using\n+\tSWI12 mode iterator.\n+\t(*movstrict<mode>_xor): Ditto from *movstrict{qi,hi}_xor.\n+\t(*mov<mode>_extv_1): Macroize insn from *mov{hi,si}_extv_1 using\n+\tSWI24 mode iterator.\n+\t(*mov<mode>_extzv_1): Macroize insn from *mov{si,di}_extzv_1 using\n+\tSWI48 mode iterator.\n+\t(mov<mode>_insn_1): New expander.\n+\t(*mov<mode>_insv_1_rex64): Macroize insn from *mov{si,di}_insv_1_rex64\n+\tusing SWI48x mode iterator.\n+\n+\t(*movoi_internal_avx): Rename from *movoi_internal.\n+\t(*movti_internal_rex64): Rename from *movti_rex64.\n+\t(*movti_internal_sse): Rename from *movti_sse.\n+\t(*movdi_internal_rex64): Rename from *movdi_1_rex64.\n+\t(*movdi_internal): Rename from *movdi_2.\n+\t(*movsi_internal): Rename from *movsi_1.\n+\t(*movhi_internal): Rename from *movhi_1.\n+\t(*movqi_internal): Rename from *movqi_1.\n+\n+\t(insv): Update the call to gen_movsi_insv_1 for rename.\n+\t* config/i386/i386.c (promote_duplicated_reg): Ditto.\n+\n 2010-06-22  Jan Hubicka  <jh@suse.cz>\n \n \t* passes.c (execute_function_todo): Move call of statistics_fini_pass\n@@ -231,8 +282,7 @@\n \n 2010-06-19  Philip Herron  <herron.philip@googlemail.com>\n \n-\t* c-decl.c (c_write_global_declarations): Don't check\n-\tflag_syntax_only.\n+\t* c-decl.c (c_write_global_declarations): Don't check flag_syntax_only.\n \n 2010-06-18  H.J. Lu  <hongjiu.lu@intel.com>\n \n@@ -269,8 +319,7 @@\n \n \t* function.h (types_used_by_cur_var_decl): Change type to a VEC.\n \t* function.c (types_used_by_cur_var_decl): Likewise.\n-\t(used_types_insert): Adjust for new type of\n-\ttypes_used_by_cur_var_decl.\n+\t(used_types_insert): Adjust for new type of types_used_by_cur_var_decl.\n \n 2010-06-18  Nathan Froyd  <froydnj@codesourcery.com>\n \n@@ -512,8 +561,7 @@\n \t(PRINT_OPERAND_PUNCT_VALID_P): Delete.\n \t(PRINT_OPERAND_ADDRESS, ARM_PRINT_OPERAND_ADDRESS):\n \t(THUMB_PRINT_OPERAND_ADDRESS): Delete and move code to...\n-\t* config/arm/arm.c (arm_print_operand_address): ...here.  New\n-\tfunction.\n+\t* config/arm/arm.c (arm_print_operand_address): ...here.  New function.\n \t(arm_print_operand): Make static.\n \t(arm_print_operand_punct_valid_p): New function.\n \t(TARGET_PRINT_OPERAND, TARGET_PRINT_OPERAND_ADDRESS):"}, {"sha": "76be5d5249ec7f08882855afb4e71f4862cb8d86", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a8ecfc9f2c532fdf26326b1f65fef3323fc2ad47/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a8ecfc9f2c532fdf26326b1f65fef3323fc2ad47/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=a8ecfc9f2c532fdf26326b1f65fef3323fc2ad47", "patch": "@@ -18838,7 +18838,7 @@ promote_duplicated_reg (enum machine_mode mode, rtx val)\n \tif (mode == SImode)\n \t  emit_insn (gen_movsi_insv_1 (reg, reg));\n \telse\n-\t  emit_insn (gen_movdi_insv_1_rex64 (reg, reg));\n+\t  emit_insn (gen_movdi_insv_1 (reg, reg));\n       else\n \t{\n \t  tmp = expand_simple_binop (mode, ASHIFT, reg, GEN_INT (8),"}, {"sha": "e9346963543937960c0a5e16530a2b2e23329f93", "filename": "gcc/config/i386/i386.md", "status": "modified", "additions": 793, "deletions": 1044, "changes": 1837, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/a8ecfc9f2c532fdf26326b1f65fef3323fc2ad47/gcc%2Fconfig%2Fi386%2Fi386.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/a8ecfc9f2c532fdf26326b1f65fef3323fc2ad47/gcc%2Fconfig%2Fi386%2Fi386.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.md?ref=a8ecfc9f2c532fdf26326b1f65fef3323fc2ad47", "patch": "@@ -765,12 +765,24 @@\n (define_code_attr sgnprefix [(sign_extend \"i\") (zero_extend \"\")\n \t\t\t     (div \"i\") (udiv \"\")])\n \n-;; All single word integer modes.\n+;; 64bit single word integer modes.\n+(define_mode_iterator SWI1248x [QI HI SI DI])\n+\n+;; 64bit single word integer modes without QImode and HImode.\n+(define_mode_iterator SWI48x [SI DI])\n+\n+;; Single word integer modes.\n (define_mode_iterator SWI [QI HI SI (DI \"TARGET_64BIT\")])\n \n+;; Single word integer modes without SImode and DImode.\n+(define_mode_iterator SWI12 [QI HI])\n+\n ;; Single word integer modes without DImode.\n (define_mode_iterator SWI124 [QI HI SI])\n \n+;; Single word integer modes without QImode and DImode.\n+(define_mode_iterator SWI24 [HI SI])\n+\n ;; Single word integer modes without QImode.\n (define_mode_iterator SWI248 [HI SI (DI \"TARGET_64BIT\")])\n \n@@ -1585,13 +1597,47 @@\n \f\n ;; Move instructions.\n \n-;; General case of fullword move.\n+(define_expand \"movoi\"\n+  [(set (match_operand:OI 0 \"nonimmediate_operand\" \"\")\n+\t(match_operand:OI 1 \"general_operand\" \"\"))]\n+  \"TARGET_AVX\"\n+  \"ix86_expand_move (OImode, operands); DONE;\")\n \n-(define_expand \"movsi\"\n-  [(set (match_operand:SI 0 \"nonimmediate_operand\" \"\")\n-\t(match_operand:SI 1 \"general_operand\" \"\"))]\n+(define_expand \"movti\"\n+  [(set (match_operand:TI 0 \"nonimmediate_operand\" \"\")\n+\t(match_operand:TI 1 \"nonimmediate_operand\" \"\"))]\n+  \"TARGET_64BIT || TARGET_SSE\"\n+{\n+  if (TARGET_64BIT)\n+    ix86_expand_move (TImode, operands);\n+  else if (push_operand (operands[0], TImode))\n+    ix86_expand_push (TImode, operands[1]);\n+  else\n+    ix86_expand_vector_move (TImode, operands);\n+  DONE;\n+})\n+\n+;; This expands to what emit_move_complex would generate if we didn't\n+;; have a movti pattern.  Having this avoids problems with reload on\n+;; 32-bit targets when SSE is present, but doesn't seem to be harmful\n+;; to have around all the time.\n+(define_expand \"movcdi\"\n+  [(set (match_operand:CDI 0 \"nonimmediate_operand\" \"\")\n+\t(match_operand:CDI 1 \"general_operand\" \"\"))]\n+  \"\"\n+{\n+  if (push_operand (operands[0], CDImode))\n+    emit_move_complex_push (CDImode, operands[0], operands[1]);\n+  else\n+    emit_move_complex_parts (operands[0], operands[1]);\n+  DONE;\n+})\n+\n+(define_expand \"mov<mode>\"\n+  [(set (match_operand:SWI1248x 0 \"nonimmediate_operand\" \"\")\n+\t(match_operand:SWI1248x 1 \"general_operand\" \"\"))]\n   \"\"\n-  \"ix86_expand_move (SImode, operands); DONE;\")\n+  \"ix86_expand_move (<MODE>mode, operands); DONE;\")\n \n ;; Push/pop instructions.  They are separate since autoinc/dec is not a\n ;; general_operand.\n@@ -1602,6 +1648,79 @@\n ;; targets without our curiosities, and it is just as easy to represent\n ;; this differently.\n \n+(define_insn \"*pushdi2_rex64\"\n+  [(set (match_operand:DI 0 \"push_operand\" \"=<,!<\")\n+\t(match_operand:DI 1 \"general_no_elim_operand\" \"re*m,n\"))]\n+  \"TARGET_64BIT\"\n+  \"@\n+   push{q}\\t%1\n+   #\"\n+  [(set_attr \"type\" \"push,multi\")\n+   (set_attr \"mode\" \"DI\")])\n+\n+;; Convert impossible pushes of immediate to existing instructions.\n+;; First try to get scratch register and go through it.  In case this\n+;; fails, push sign extended lower part first and then overwrite\n+;; upper part by 32bit move.\n+(define_peephole2\n+  [(match_scratch:DI 2 \"r\")\n+   (set (match_operand:DI 0 \"push_operand\" \"\")\n+        (match_operand:DI 1 \"immediate_operand\" \"\"))]\n+  \"TARGET_64BIT && !symbolic_operand (operands[1], DImode)\n+   && !x86_64_immediate_operand (operands[1], DImode)\"\n+  [(set (match_dup 2) (match_dup 1))\n+   (set (match_dup 0) (match_dup 2))]\n+  \"\")\n+\n+;; We need to define this as both peepholer and splitter for case\n+;; peephole2 pass is not run.\n+;; \"&& 1\" is needed to keep it from matching the previous pattern.\n+(define_peephole2\n+  [(set (match_operand:DI 0 \"push_operand\" \"\")\n+        (match_operand:DI 1 \"immediate_operand\" \"\"))]\n+  \"TARGET_64BIT && !symbolic_operand (operands[1], DImode)\n+   && !x86_64_immediate_operand (operands[1], DImode) && 1\"\n+  [(set (match_dup 0) (match_dup 1))\n+   (set (match_dup 2) (match_dup 3))]\n+{\n+  split_di (&operands[1], 1, &operands[2], &operands[3]);\n+\n+  operands[1] = gen_lowpart (DImode, operands[2]);\n+  operands[2] = gen_rtx_MEM (SImode, gen_rtx_PLUS (DImode, stack_pointer_rtx,\n+\t\t\t\t\t\t   GEN_INT (4)));\n+})\n+\n+(define_split\n+  [(set (match_operand:DI 0 \"push_operand\" \"\")\n+        (match_operand:DI 1 \"immediate_operand\" \"\"))]\n+  \"TARGET_64BIT && ((optimize > 0 && flag_peephole2)\n+\t\t    ? epilogue_completed : reload_completed)\n+   && !symbolic_operand (operands[1], DImode)\n+   && !x86_64_immediate_operand (operands[1], DImode)\"\n+  [(set (match_dup 0) (match_dup 1))\n+   (set (match_dup 2) (match_dup 3))]\n+{\n+  split_di (&operands[1], 1, &operands[2], &operands[3]);\n+\n+  operands[1] = gen_lowpart (DImode, operands[2]);\n+  operands[2] = gen_rtx_MEM (SImode, gen_rtx_PLUS (DImode, stack_pointer_rtx,\n+\t\t\t\t\t\t   GEN_INT (4)));\n+})\n+\n+(define_insn \"*pushdi2\"\n+  [(set (match_operand:DI 0 \"push_operand\" \"=<\")\n+\t(match_operand:DI 1 \"general_no_elim_operand\" \"riF*m\"))]\n+  \"!TARGET_64BIT\"\n+  \"#\")\n+\n+(define_split\n+  [(set (match_operand:DI 0 \"push_operand\" \"\")\n+        (match_operand:DI 1 \"general_operand\" \"\"))]\n+  \"!TARGET_64BIT && reload_completed\n+   && !(MMX_REG_P (operands[1]) || SSE_REG_P (operands[1]))\"\n+  [(const_int 0)]\n+  \"ix86_split_long_move (operands); DONE;\")\n+\n (define_insn \"*pushsi2\"\n   [(set (match_operand:SI 0 \"push_operand\" \"=<\")\n \t(match_operand:SI 1 \"general_no_elim_operand\" \"ri*m\"))]\n@@ -1610,761 +1729,335 @@\n   [(set_attr \"type\" \"push\")\n    (set_attr \"mode\" \"SI\")])\n \n+;; emit_push_insn when it calls move_by_pieces requires an insn to\n+;; \"push a byte/word\".  But actually we use pushl, which has the effect\n+;; of rounding the amount pushed up to a word.\n+\n ;; For 64BIT abi we always round up to 8 bytes.\n-(define_insn \"*pushsi2_rex64\"\n-  [(set (match_operand:SI 0 \"push_operand\" \"=X\")\n-\t(match_operand:SI 1 \"nonmemory_no_elim_operand\" \"ri\"))]\n+(define_insn \"*push<mode>2_rex64\"\n+  [(set (match_operand:SWI124 0 \"push_operand\" \"=X\")\n+\t(match_operand:SWI124 1 \"nonmemory_no_elim_operand\" \"r<i>\"))]\n   \"TARGET_64BIT\"\n   \"push{q}\\t%q1\"\n   [(set_attr \"type\" \"push\")\n-   (set_attr \"mode\" \"SI\")])\n+   (set_attr \"mode\" \"DI\")])\n \n-(define_insn \"*pushsi2_prologue\"\n-  [(set (match_operand:SI 0 \"push_operand\" \"=<\")\n-\t(match_operand:SI 1 \"general_no_elim_operand\" \"ri*m\"))\n-   (clobber (mem:BLK (scratch)))]\n+(define_insn \"*push<mode>2\"\n+  [(set (match_operand:SWI12 0 \"push_operand\" \"=X\")\n+\t(match_operand:SWI12 1 \"nonmemory_no_elim_operand\" \"rn\"))]\n   \"!TARGET_64BIT\"\n-  \"push{l}\\t%1\"\n+  \"push{l}\\t%k1\"\n   [(set_attr \"type\" \"push\")\n    (set_attr \"mode\" \"SI\")])\n \n-(define_insn \"*popsi1_epilogue\"\n+(define_insn \"*push<mode>2_prologue\"\n+  [(set (match_operand:P 0 \"push_operand\" \"=<\")\n+\t(match_operand:P 1 \"general_no_elim_operand\" \"r<i>*m\"))\n+   (clobber (mem:BLK (scratch)))]\n+  \"\"\n+  \"push{<imodesuffix>}\\t%1\"\n+  [(set_attr \"type\" \"push\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+(define_insn \"popdi1\"\n+  [(set (match_operand:DI 0 \"nonimmediate_operand\" \"=r*m\")\n+\t(mem:DI (reg:DI SP_REG)))\n+   (set (reg:DI SP_REG)\n+\t(plus:DI (reg:DI SP_REG) (const_int 8)))]\n+  \"TARGET_64BIT\"\n+  \"pop{q}\\t%0\"\n+  [(set_attr \"type\" \"pop\")\n+   (set_attr \"mode\" \"DI\")])\n+\n+(define_insn \"popsi1\"\n   [(set (match_operand:SI 0 \"nonimmediate_operand\" \"=r*m\")\n \t(mem:SI (reg:SI SP_REG)))\n    (set (reg:SI SP_REG)\n-\t(plus:SI (reg:SI SP_REG) (const_int 4)))\n-   (clobber (mem:BLK (scratch)))]\n+\t(plus:SI (reg:SI SP_REG) (const_int 4)))]\n   \"!TARGET_64BIT\"\n   \"pop{l}\\t%0\"\n   [(set_attr \"type\" \"pop\")\n    (set_attr \"mode\" \"SI\")])\n \n-(define_insn \"popsi1\"\n+(define_insn \"*popdi1_epilogue\"\n+  [(set (match_operand:DI 0 \"nonimmediate_operand\" \"=r*m\")\n+\t(mem:DI (reg:DI SP_REG)))\n+   (set (reg:DI SP_REG)\n+\t(plus:DI (reg:DI SP_REG) (const_int 8)))\n+   (clobber (mem:BLK (scratch)))]\n+  \"TARGET_64BIT\"\n+  \"pop{q}\\t%0\"\n+  [(set_attr \"type\" \"pop\")\n+   (set_attr \"mode\" \"DI\")])\n+\n+(define_insn \"*popsi1_epilogue\"\n   [(set (match_operand:SI 0 \"nonimmediate_operand\" \"=r*m\")\n \t(mem:SI (reg:SI SP_REG)))\n    (set (reg:SI SP_REG)\n-\t(plus:SI (reg:SI SP_REG) (const_int 4)))]\n+\t(plus:SI (reg:SI SP_REG) (const_int 4)))\n+   (clobber (mem:BLK (scratch)))]\n   \"!TARGET_64BIT\"\n   \"pop{l}\\t%0\"\n   [(set_attr \"type\" \"pop\")\n    (set_attr \"mode\" \"SI\")])\n \n-(define_insn \"*movsi_xor\"\n-  [(set (match_operand:SI 0 \"register_operand\" \"=r\")\n-\t(match_operand:SI 1 \"const0_operand\" \"\"))\n+(define_insn \"*mov<mode>_xor\"\n+  [(set (match_operand:SWI48 0 \"register_operand\" \"=r\")\n+\t(match_operand:SWI48 1 \"const0_operand\" \"\"))\n    (clobber (reg:CC FLAGS_REG))]\n   \"reload_completed\"\n-  \"xor{l}\\t%0, %0\"\n+  \"xor{l}\\t%k0, %k0\"\n   [(set_attr \"type\" \"alu1\")\n    (set_attr \"mode\" \"SI\")\n    (set_attr \"length_immediate\" \"0\")])\n \n-(define_insn \"*movsi_or\"\n-  [(set (match_operand:SI 0 \"register_operand\" \"=r\")\n-\t(match_operand:SI 1 \"immediate_operand\" \"i\"))\n+(define_insn \"*mov<mode>_or\"\n+  [(set (match_operand:SWI48 0 \"register_operand\" \"=r\")\n+\t(match_operand:SWI48 1 \"const_int_operand\" \"\"))\n    (clobber (reg:CC FLAGS_REG))]\n   \"reload_completed\n    && operands[1] == constm1_rtx\"\n-{\n-  operands[1] = constm1_rtx;\n-  return \"or{l}\\t{%1, %0|%0, %1}\";\n-}\n+  \"or{<imodesuffix>}\\t{%1, %0|%0, %1}\"\n   [(set_attr \"type\" \"alu1\")\n-   (set_attr \"mode\" \"SI\")\n+   (set_attr \"mode\" \"<MODE>\")\n    (set_attr \"length_immediate\" \"1\")])\n \n-(define_insn \"*movsi_1\"\n-  [(set (match_operand:SI 0 \"nonimmediate_operand\"\n-\t\t\t\"=r,m ,*y,*y,?rm,?*y,*x,*x,?r ,m ,?*Yi,*x\")\n-\t(match_operand:SI 1 \"general_operand\"\n-\t\t\t\"g ,ri,C ,*y,*y ,rm ,C ,*x,*Yi,*x,r   ,m \"))]\n-  \"!(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n+(define_insn \"*movoi_internal_avx\"\n+  [(set (match_operand:OI 0 \"nonimmediate_operand\" \"=x,x,m\")\n+\t(match_operand:OI 1 \"vector_move_operand\" \"C,xm,x\"))]\n+  \"TARGET_AVX && !(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n {\n-  switch (get_attr_type (insn))\n+  switch (which_alternative)\n     {\n-    case TYPE_SSELOG1:\n-      if (get_attr_mode (insn) == MODE_TI)\n-        return \"%vpxor\\t%0, %d0\";\n-      return \"%vxorps\\t%0, %d0\";\n+    case 0:\n+      return \"vxorps\\t%0, %0, %0\";\n+    case 1:\n+    case 2:\n+      if (misaligned_operand (operands[0], OImode)\n+\t  || misaligned_operand (operands[1], OImode))\n+\treturn \"vmovdqu\\t{%1, %0|%0, %1}\";\n+      else\n+\treturn \"vmovdqa\\t{%1, %0|%0, %1}\";\n+    default:\n+      gcc_unreachable ();\n+    }\n+}\n+  [(set_attr \"type\" \"sselog1,ssemov,ssemov\")\n+   (set_attr \"prefix\" \"vex\")\n+   (set_attr \"mode\" \"OI\")])\n \n-    case TYPE_SSEMOV:\n-      switch (get_attr_mode (insn))\n+(define_insn \"*movti_internal_rex64\"\n+  [(set (match_operand:TI 0 \"nonimmediate_operand\" \"=!r,o,x,x,xm\")\n+\t(match_operand:TI 1 \"general_operand\" \"riFo,riF,C,xm,x\"))]\n+  \"TARGET_64BIT && !(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n+{\n+  switch (which_alternative)\n+    {\n+    case 0:\n+    case 1:\n+      return \"#\";\n+    case 2:\n+      if (get_attr_mode (insn) == MODE_V4SF)\n+\treturn \"%vxorps\\t%0, %d0\";\n+      else\n+\treturn \"%vpxor\\t%0, %d0\";\n+    case 3:\n+    case 4:\n+      /* TDmode values are passed as TImode on the stack.  Moving them\n+\t to stack may result in unaligned memory access.  */\n+      if (misaligned_operand (operands[0], TImode)\n+\t  || misaligned_operand (operands[1], TImode))\n \t{\n-\tcase MODE_TI:\n-\t  return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n-\tcase MODE_V4SF:\n-\t  return \"%vmovaps\\t{%1, %0|%0, %1}\";\n-\tcase MODE_SI:\n-          return \"%vmovd\\t{%1, %0|%0, %1}\";\n-\tcase MODE_SF:\n-          return \"%vmovss\\t{%1, %0|%0, %1}\";\n-\tdefault:\n-\t  gcc_unreachable ();\n+\t  if (get_attr_mode (insn) == MODE_V4SF)\n+\t    return \"%vmovups\\t{%1, %0|%0, %1}\";\n+\t else\n+\t   return \"%vmovdqu\\t{%1, %0|%0, %1}\";\n+\t}\n+      else\n+\t{\n+\t  if (get_attr_mode (insn) == MODE_V4SF)\n+\t    return \"%vmovaps\\t{%1, %0|%0, %1}\";\n+\t else\n+\t   return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n \t}\n-\n-    case TYPE_MMX:\n-      return \"pxor\\t%0, %0\";\n-\n-    case TYPE_MMXMOV:\n-      if (get_attr_mode (insn) == MODE_DI)\n-\treturn \"movq\\t{%1, %0|%0, %1}\";\n-      return \"movd\\t{%1, %0|%0, %1}\";\n-\n-    case TYPE_LEA:\n-      return \"lea{l}\\t{%a1, %0|%0, %a1}\";\n-\n     default:\n-      gcc_assert (!flag_pic || LEGITIMATE_PIC_OPERAND_P (operands[1]));\n-      return \"mov{l}\\t{%1, %0|%0, %1}\";\n+      gcc_unreachable ();\n     }\n }\n-  [(set (attr \"type\")\n-     (cond [(eq_attr \"alternative\" \"2\")\n-\t      (const_string \"mmx\")\n-\t    (eq_attr \"alternative\" \"3,4,5\")\n-\t      (const_string \"mmxmov\")\n-\t    (eq_attr \"alternative\" \"6\")\n-\t      (const_string \"sselog1\")\n-\t    (eq_attr \"alternative\" \"7,8,9,10,11\")\n-\t      (const_string \"ssemov\")\n- \t    (match_operand:DI 1 \"pic_32bit_operand\" \"\")\n-\t      (const_string \"lea\")\n-\t   ]\n-\t   (const_string \"imov\")))\n-   (set (attr \"prefix\")\n-     (if_then_else (eq_attr \"alternative\" \"0,1,2,3,4,5\")\n-       (const_string \"orig\")\n-       (const_string \"maybe_vex\")))\n-   (set (attr \"prefix_data16\")\n-     (if_then_else (and (eq_attr \"type\" \"ssemov\") (eq_attr \"mode\" \"SI\"))\n-       (const_string \"1\")\n-       (const_string \"*\")))\n+  [(set_attr \"type\" \"*,*,sselog1,ssemov,ssemov\")\n+   (set_attr \"prefix\" \"*,*,maybe_vex,maybe_vex,maybe_vex\")\n    (set (attr \"mode\")\n-     (cond [(eq_attr \"alternative\" \"2,3\")\n-\t      (const_string \"DI\")\n-\t    (eq_attr \"alternative\" \"6,7\")\n-\t      (if_then_else\n-\t        (eq (symbol_ref \"TARGET_SSE2\") (const_int 0))\n-\t        (const_string \"V4SF\")\n-\t        (const_string \"TI\"))\n-\t    (and (eq_attr \"alternative\" \"8,9,10,11\")\n-\t         (eq (symbol_ref \"TARGET_SSE2\") (const_int 0)))\n-\t      (const_string \"SF\")\n-\t   ]\n-\t   (const_string \"SI\")))])\n-\n-;; Stores and loads of ax to arbitrary constant address.\n-;; We fake an second form of instruction to force reload to load address\n-;; into register when rax is not available\n-(define_insn \"*movabssi_1_rex64\"\n-  [(set (mem:SI (match_operand:DI 0 \"x86_64_movabs_operand\" \"i,r\"))\n-\t(match_operand:SI 1 \"nonmemory_operand\" \"a,er\"))]\n-  \"TARGET_64BIT && ix86_check_movabs (insn, 0)\"\n-  \"@\n-   movabs{l}\\t{%1, %P0|%P0, %1}\n-   mov{l}\\t{%1, %a0|%a0, %1}\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"modrm\" \"0,*\")\n-   (set_attr \"length_address\" \"8,0\")\n-   (set_attr \"length_immediate\" \"0,*\")\n-   (set_attr \"memory\" \"store\")\n-   (set_attr \"mode\" \"SI\")])\n-\n-(define_insn \"*movabssi_2_rex64\"\n-  [(set (match_operand:SI 0 \"register_operand\" \"=a,r\")\n-        (mem:SI (match_operand:DI 1 \"x86_64_movabs_operand\" \"i,r\")))]\n-  \"TARGET_64BIT && ix86_check_movabs (insn, 1)\"\n-  \"@\n-   movabs{l}\\t{%P1, %0|%0, %P1}\n-   mov{l}\\t{%a1, %0|%0, %a1}\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"modrm\" \"0,*\")\n-   (set_attr \"length_address\" \"8,0\")\n-   (set_attr \"length_immediate\" \"0\")\n-   (set_attr \"memory\" \"load\")\n-   (set_attr \"mode\" \"SI\")])\n-\n-(define_insn \"*swapsi\"\n-  [(set (match_operand:SI 0 \"register_operand\" \"+r\")\n-\t(match_operand:SI 1 \"register_operand\" \"+r\"))\n-   (set (match_dup 1)\n-\t(match_dup 0))]\n-  \"\"\n-  \"xchg{l}\\t%1, %0\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"mode\" \"SI\")\n-   (set_attr \"pent_pair\" \"np\")\n-   (set_attr \"athlon_decode\" \"vector\")\n-   (set_attr \"amdfam10_decode\" \"double\")])\n-\n-(define_expand \"movhi\"\n-  [(set (match_operand:HI 0 \"nonimmediate_operand\" \"\")\n-        (match_operand:HI 1 \"general_operand\" \"\"))]\n-  \"\"\n-  \"ix86_expand_move (HImode, operands); DONE;\")\n-\n-(define_insn \"*pushhi2\"\n-  [(set (match_operand:HI 0 \"push_operand\" \"=X\")\n-\t(match_operand:HI 1 \"nonmemory_no_elim_operand\" \"rn\"))]\n-  \"!TARGET_64BIT\"\n-  \"push{l}\\t%k1\"\n-  [(set_attr \"type\" \"push\")\n-   (set_attr \"mode\" \"SI\")])\n-\n-;; For 64BIT abi we always round up to 8 bytes.\n-(define_insn \"*pushhi2_rex64\"\n-  [(set (match_operand:HI 0 \"push_operand\" \"=X\")\n-\t(match_operand:HI 1 \"nonmemory_no_elim_operand\" \"rn\"))]\n-  \"TARGET_64BIT\"\n-  \"push{q}\\t%q1\"\n-  [(set_attr \"type\" \"push\")\n-   (set_attr \"mode\" \"DI\")])\n-\n-(define_insn \"*movhi_1\"\n-  [(set (match_operand:HI 0 \"nonimmediate_operand\" \"=r,r,r,m\")\n-\t(match_operand:HI 1 \"general_operand\" \"r,rn,rm,rn\"))]\n-  \"!(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n-{\n-  switch (get_attr_type (insn))\n-    {\n-    case TYPE_IMOVX:\n-      /* movzwl is faster than movw on p2 due to partial word stalls,\n-\t though not as fast as an aligned movl.  */\n-      return \"movz{wl|x}\\t{%1, %k0|%k0, %1}\";\n-    default:\n-      if (get_attr_mode (insn) == MODE_SI)\n-        return \"mov{l}\\t{%k1, %k0|%k0, %k1}\";\n-      else\n-        return \"mov{w}\\t{%1, %0|%0, %1}\";\n-    }\n-}\n-  [(set (attr \"type\")\n-     (cond [(ne (symbol_ref \"optimize_function_for_size_p (cfun)\") (const_int 0))\n-\t      (const_string \"imov\")\n-\t    (and (eq_attr \"alternative\" \"0\")\n-\t\t (ior (eq (symbol_ref \"TARGET_PARTIAL_REG_STALL\")\n-\t\t\t  (const_int 0))\n-\t\t      (eq (symbol_ref \"TARGET_HIMODE_MATH\")\n-\t\t\t  (const_int 0))))\n-\t      (const_string \"imov\")\n-\t    (and (eq_attr \"alternative\" \"1,2\")\n-\t\t (match_operand:HI 1 \"aligned_operand\" \"\"))\n-\t      (const_string \"imov\")\n-\t    (and (ne (symbol_ref \"TARGET_MOVX\")\n-\t\t     (const_int 0))\n-\t\t (eq_attr \"alternative\" \"0,2\"))\n-\t      (const_string \"imovx\")\n-\t   ]\n-\t   (const_string \"imov\")))\n-    (set (attr \"mode\")\n-      (cond [(eq_attr \"type\" \"imovx\")\n-\t       (const_string \"SI\")\n-\t     (and (eq_attr \"alternative\" \"1,2\")\n-\t\t  (match_operand:HI 1 \"aligned_operand\" \"\"))\n-\t       (const_string \"SI\")\n-\t     (and (eq_attr \"alternative\" \"0\")\n-\t\t  (ior (eq (symbol_ref \"TARGET_PARTIAL_REG_STALL\")\n-\t\t\t   (const_int 0))\n-\t\t       (eq (symbol_ref \"TARGET_HIMODE_MATH\")\n-\t\t\t   (const_int 0))))\n-\t       (const_string \"SI\")\n-\t    ]\n-\t    (const_string \"HI\")))])\n-\n-;; Stores and loads of ax to arbitrary constant address.\n-;; We fake an second form of instruction to force reload to load address\n-;; into register when rax is not available\n-(define_insn \"*movabshi_1_rex64\"\n-  [(set (mem:HI (match_operand:DI 0 \"x86_64_movabs_operand\" \"i,r\"))\n-\t(match_operand:HI 1 \"nonmemory_operand\" \"a,er\"))]\n-  \"TARGET_64BIT && ix86_check_movabs (insn, 0)\"\n-  \"@\n-   movabs{w}\\t{%1, %P0|%P0, %1}\n-   mov{w}\\t{%1, %a0|%a0, %1}\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"modrm\" \"0,*\")\n-   (set_attr \"length_address\" \"8,0\")\n-   (set_attr \"length_immediate\" \"0,*\")\n-   (set_attr \"memory\" \"store\")\n-   (set_attr \"mode\" \"HI\")])\n-\n-(define_insn \"*movabshi_2_rex64\"\n-  [(set (match_operand:HI 0 \"register_operand\" \"=a,r\")\n-        (mem:HI (match_operand:DI 1 \"x86_64_movabs_operand\" \"i,r\")))]\n-  \"TARGET_64BIT && ix86_check_movabs (insn, 1)\"\n-  \"@\n-   movabs{w}\\t{%P1, %0|%0, %P1}\n-   mov{w}\\t{%a1, %0|%0, %a1}\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"modrm\" \"0,*\")\n-   (set_attr \"length_address\" \"8,0\")\n-   (set_attr \"length_immediate\" \"0\")\n-   (set_attr \"memory\" \"load\")\n-   (set_attr \"mode\" \"HI\")])\n-\n-(define_insn \"*swaphi_1\"\n-  [(set (match_operand:HI 0 \"register_operand\" \"+r\")\n-\t(match_operand:HI 1 \"register_operand\" \"+r\"))\n-   (set (match_dup 1)\n-\t(match_dup 0))]\n-  \"!TARGET_PARTIAL_REG_STALL || optimize_function_for_size_p (cfun)\"\n-  \"xchg{l}\\t%k1, %k0\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"mode\" \"SI\")\n-   (set_attr \"pent_pair\" \"np\")\n-   (set_attr \"athlon_decode\" \"vector\")\n-   (set_attr \"amdfam10_decode\" \"double\")])\n-\n-;; Not added amdfam10_decode since TARGET_PARTIAL_REG_STALL is disabled for AMDFAM10\n-(define_insn \"*swaphi_2\"\n-  [(set (match_operand:HI 0 \"register_operand\" \"+r\")\n-\t(match_operand:HI 1 \"register_operand\" \"+r\"))\n-   (set (match_dup 1)\n-\t(match_dup 0))]\n-  \"TARGET_PARTIAL_REG_STALL\"\n-  \"xchg{w}\\t%1, %0\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"mode\" \"HI\")\n-   (set_attr \"pent_pair\" \"np\")\n-   (set_attr \"athlon_decode\" \"vector\")])\n+   \t(cond [(eq_attr \"alternative\" \"2,3\")\n+\t\t (if_then_else\n+\t\t   (ne (symbol_ref \"optimize_function_for_size_p (cfun)\")\n+\t\t       (const_int 0))\n+\t\t   (const_string \"V4SF\")\n+\t\t   (const_string \"TI\"))\n+\t       (eq_attr \"alternative\" \"4\")\n+\t\t (if_then_else\n+\t\t   (ior (ne (symbol_ref \"TARGET_SSE_TYPELESS_STORES\")\n+\t\t\t    (const_int 0))\n+\t\t\t(ne (symbol_ref \"optimize_function_for_size_p (cfun)\")\n+\t\t\t    (const_int 0)))\n+\t\t   (const_string \"V4SF\")\n+\t\t   (const_string \"TI\"))]\n+\t       (const_string \"DI\")))])\n \n-(define_expand \"movstricthi\"\n-  [(set (strict_low_part (match_operand:HI 0 \"nonimmediate_operand\" \"\"))\n-\t(match_operand:HI 1 \"general_operand\" \"\"))]\n-  \"\"\n-{\n-  if (TARGET_PARTIAL_REG_STALL && optimize_function_for_speed_p (cfun))\n-    FAIL;\n-  /* Don't generate memory->memory moves, go through a register */\n-  if (MEM_P (operands[0]) && MEM_P (operands[1]))\n-    operands[1] = force_reg (HImode, operands[1]);\n-})\n+(define_split\n+  [(set (match_operand:TI 0 \"nonimmediate_operand\" \"\")\n+\t(match_operand:TI 1 \"general_operand\" \"\"))]\n+  \"reload_completed\n+   && !SSE_REG_P (operands[0]) && !SSE_REG_P (operands[1])\"\n+  [(const_int 0)]\n+  \"ix86_split_long_move (operands); DONE;\")\n \n-(define_insn \"*movstricthi_1\"\n-  [(set (strict_low_part (match_operand:HI 0 \"nonimmediate_operand\" \"+rm,r\"))\n-\t(match_operand:HI 1 \"general_operand\" \"rn,m\"))]\n-  \"(! TARGET_PARTIAL_REG_STALL || optimize_function_for_size_p (cfun))\n+(define_insn \"*movti_internal_sse\"\n+  [(set (match_operand:TI 0 \"nonimmediate_operand\" \"=x,x,m\")\n+\t(match_operand:TI 1 \"vector_move_operand\" \"C,xm,x\"))]\n+  \"TARGET_SSE && !TARGET_64BIT\n    && !(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n-  \"mov{w}\\t{%1, %0|%0, %1}\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"mode\" \"HI\")])\n-\n-(define_insn \"*movstricthi_xor\"\n-  [(set (strict_low_part (match_operand:HI 0 \"register_operand\" \"+r\"))\n-\t(match_operand:HI 1 \"const0_operand\" \"\"))\n-   (clobber (reg:CC FLAGS_REG))]\n-  \"reload_completed\"\n-  \"xor{w}\\t%0, %0\"\n-  [(set_attr \"type\" \"alu1\")\n-   (set_attr \"mode\" \"HI\")\n-   (set_attr \"length_immediate\" \"0\")])\n-\n-(define_expand \"movqi\"\n-  [(set (match_operand:QI 0 \"nonimmediate_operand\" \"\")\n-\t(match_operand:QI 1 \"general_operand\" \"\"))]\n-  \"\"\n-  \"ix86_expand_move (QImode, operands); DONE;\")\n-\n-;; emit_push_insn when it calls move_by_pieces requires an insn to\n-;; \"push a byte\".  But actually we use pushl, which has the effect\n-;; of rounding the amount pushed up to a word.\n-\n-(define_insn \"*pushqi2\"\n-  [(set (match_operand:QI 0 \"push_operand\" \"=X\")\n-\t(match_operand:QI 1 \"nonmemory_no_elim_operand\" \"rn\"))]\n-  \"!TARGET_64BIT\"\n-  \"push{l}\\t%k1\"\n-  [(set_attr \"type\" \"push\")\n-   (set_attr \"mode\" \"SI\")])\n-\n-;; For 64BIT abi we always round up to 8 bytes.\n-(define_insn \"*pushqi2_rex64\"\n-  [(set (match_operand:QI 0 \"push_operand\" \"=X\")\n-\t(match_operand:QI 1 \"nonmemory_no_elim_operand\" \"qn\"))]\n-  \"TARGET_64BIT\"\n-  \"push{q}\\t%q1\"\n-  [(set_attr \"type\" \"push\")\n-   (set_attr \"mode\" \"DI\")])\n-\n-;; Situation is quite tricky about when to choose full sized (SImode) move\n-;; over QImode moves.  For Q_REG -> Q_REG move we use full size only for\n-;; partial register dependency machines (such as AMD Athlon), where QImode\n-;; moves issue extra dependency and for partial register stalls machines\n-;; that don't use QImode patterns (and QImode move cause stall on the next\n-;; instruction).\n-;;\n-;; For loads of Q_REG to NONQ_REG we use full sized moves except for partial\n-;; register stall machines with, where we use QImode instructions, since\n-;; partial register stall can be caused there.  Then we use movzx.\n-(define_insn \"*movqi_1\"\n-  [(set (match_operand:QI 0 \"nonimmediate_operand\" \"=q,q ,q ,r,r ,?r,m\")\n-\t(match_operand:QI 1 \"general_operand\"      \" q,qn,qm,q,rn,qm,qn\"))]\n-  \"!(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n {\n-  switch (get_attr_type (insn))\n+  switch (which_alternative)\n     {\n-    case TYPE_IMOVX:\n-      gcc_assert (ANY_QI_REG_P (operands[1]) || MEM_P (operands[1]));\n-      return \"movz{bl|x}\\t{%1, %k0|%k0, %1}\";\n-    default:\n-      if (get_attr_mode (insn) == MODE_SI)\n-        return \"mov{l}\\t{%k1, %k0|%k0, %k1}\";\n+    case 0:\n+      if (get_attr_mode (insn) == MODE_V4SF)\n+\treturn \"%vxorps\\t%0, %d0\";\n       else\n-        return \"mov{b}\\t{%1, %0|%0, %1}\";\n-    }\n-}\n-  [(set (attr \"type\")\n-     (cond [(and (eq_attr \"alternative\" \"5\")\n-\t\t (not (match_operand:QI 1 \"aligned_operand\" \"\")))\n-\t      (const_string \"imovx\")\n-\t    (ne (symbol_ref \"optimize_function_for_size_p (cfun)\") (const_int 0))\n-\t      (const_string \"imov\")\n-\t    (and (eq_attr \"alternative\" \"3\")\n-\t\t (ior (eq (symbol_ref \"TARGET_PARTIAL_REG_STALL\")\n-\t\t\t  (const_int 0))\n-\t\t      (eq (symbol_ref \"TARGET_QIMODE_MATH\")\n-\t\t\t  (const_int 0))))\n-\t      (const_string \"imov\")\n-\t    (eq_attr \"alternative\" \"3,5\")\n-\t      (const_string \"imovx\")\n-\t    (and (ne (symbol_ref \"TARGET_MOVX\")\n-\t\t     (const_int 0))\n-\t\t (eq_attr \"alternative\" \"2\"))\n-\t      (const_string \"imovx\")\n-\t   ]\n-\t   (const_string \"imov\")))\n-   (set (attr \"mode\")\n-      (cond [(eq_attr \"alternative\" \"3,4,5\")\n-\t       (const_string \"SI\")\n-\t     (eq_attr \"alternative\" \"6\")\n-\t       (const_string \"QI\")\n-\t     (eq_attr \"type\" \"imovx\")\n-\t       (const_string \"SI\")\n-\t     (and (eq_attr \"type\" \"imov\")\n-\t\t  (and (eq_attr \"alternative\" \"0,1\")\n-\t\t       (and (ne (symbol_ref \"TARGET_PARTIAL_REG_DEPENDENCY\")\n-\t\t\t\t(const_int 0))\n-\t\t\t    (and (eq (symbol_ref \"optimize_function_for_size_p (cfun)\")\n-\t\t\t\t     (const_int 0))\n-\t\t\t    \t (eq (symbol_ref \"TARGET_PARTIAL_REG_STALL\")\n-\t\t\t\t     (const_int 0))))))\n-\t       (const_string \"SI\")\n-\t     ;; Avoid partial register stalls when not using QImode arithmetic\n-\t     (and (eq_attr \"type\" \"imov\")\n-\t\t  (and (eq_attr \"alternative\" \"0,1\")\n-\t\t       (and (ne (symbol_ref \"TARGET_PARTIAL_REG_STALL\")\n-\t\t\t\t(const_int 0))\n-\t\t\t    (eq (symbol_ref \"TARGET_QIMODE_MATH\")\n-\t\t\t\t(const_int 0)))))\n-\t       (const_string \"SI\")\n-\t   ]\n-\t   (const_string \"QI\")))])\n-\n-(define_insn \"*swapqi_1\"\n-  [(set (match_operand:QI 0 \"register_operand\" \"+r\")\n-\t(match_operand:QI 1 \"register_operand\" \"+r\"))\n-   (set (match_dup 1)\n-\t(match_dup 0))]\n-  \"!TARGET_PARTIAL_REG_STALL || optimize_function_for_size_p (cfun)\"\n-  \"xchg{l}\\t%k1, %k0\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"mode\" \"SI\")\n-   (set_attr \"pent_pair\" \"np\")\n-   (set_attr \"athlon_decode\" \"vector\")\n-   (set_attr \"amdfam10_decode\" \"vector\")])\n-\n-;; Not added amdfam10_decode since TARGET_PARTIAL_REG_STALL is disabled for AMDFAM10\n-(define_insn \"*swapqi_2\"\n-  [(set (match_operand:QI 0 \"register_operand\" \"+q\")\n-\t(match_operand:QI 1 \"register_operand\" \"+q\"))\n-   (set (match_dup 1)\n-\t(match_dup 0))]\n-  \"TARGET_PARTIAL_REG_STALL\"\n-  \"xchg{b}\\t%1, %0\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"mode\" \"QI\")\n-   (set_attr \"pent_pair\" \"np\")\n-   (set_attr \"athlon_decode\" \"vector\")])\n-\n-(define_expand \"movstrictqi\"\n-  [(set (strict_low_part (match_operand:QI 0 \"nonimmediate_operand\" \"\"))\n-\t(match_operand:QI 1 \"general_operand\" \"\"))]\n-  \"\"\n-{\n-  if (TARGET_PARTIAL_REG_STALL && optimize_function_for_speed_p (cfun))\n-    FAIL;\n-  /* Don't generate memory->memory moves, go through a register.  */\n-  if (MEM_P (operands[0]) && MEM_P (operands[1]))\n-    operands[1] = force_reg (QImode, operands[1]);\n-})\n-\n-(define_insn \"*movstrictqi_1\"\n-  [(set (strict_low_part (match_operand:QI 0 \"nonimmediate_operand\" \"+qm,q\"))\n-\t(match_operand:QI 1 \"general_operand\" \"*qn,m\"))]\n-  \"(! TARGET_PARTIAL_REG_STALL || optimize_function_for_size_p (cfun))\n-   && !(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n-  \"mov{b}\\t{%1, %0|%0, %1}\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"mode\" \"QI\")])\n-\n-(define_insn \"*movstrictqi_xor\"\n-  [(set (strict_low_part (match_operand:QI 0 \"q_regs_operand\" \"+q\"))\n-\t(match_operand:QI 1 \"const0_operand\" \"\"))\n-   (clobber (reg:CC FLAGS_REG))]\n-  \"reload_completed\"\n-  \"xor{b}\\t%0, %0\"\n-  [(set_attr \"type\" \"alu1\")\n-   (set_attr \"mode\" \"QI\")\n-   (set_attr \"length_immediate\" \"0\")])\n-\n-(define_insn \"*movsi_extv_1\"\n-  [(set (match_operand:SI 0 \"register_operand\" \"=R\")\n-\t(sign_extract:SI (match_operand 1 \"ext_register_operand\" \"Q\")\n-\t\t\t (const_int 8)\n-\t\t\t (const_int 8)))]\n-  \"\"\n-  \"movs{bl|x}\\t{%h1, %0|%0, %h1}\"\n-  [(set_attr \"type\" \"imovx\")\n-   (set_attr \"mode\" \"SI\")])\n-\n-(define_insn \"*movhi_extv_1\"\n-  [(set (match_operand:HI 0 \"register_operand\" \"=R\")\n-\t(sign_extract:HI (match_operand 1 \"ext_register_operand\" \"Q\")\n-\t\t\t (const_int 8)\n-\t\t\t (const_int 8)))]\n-  \"\"\n-  \"movs{bl|x}\\t{%h1, %k0|%k0, %h1}\"\n-  [(set_attr \"type\" \"imovx\")\n-   (set_attr \"mode\" \"SI\")])\n-\n-(define_insn \"*movqi_extv_1\"\n-  [(set (match_operand:QI 0 \"nonimmediate_operand\" \"=Qm,?r\")\n-        (sign_extract:QI (match_operand 1 \"ext_register_operand\" \"Q,Q\")\n-                         (const_int 8)\n-                         (const_int 8)))]\n-  \"!TARGET_64BIT\"\n-{\n-  switch (get_attr_type (insn))\n-    {\n-    case TYPE_IMOVX:\n-      return \"movs{bl|x}\\t{%h1, %k0|%k0, %h1}\";\n-    default:\n-      return \"mov{b}\\t{%h1, %0|%0, %h1}\";\n-    }\n-}\n-  [(set (attr \"type\")\n-     (if_then_else (and (match_operand:QI 0 \"register_operand\" \"\")\n-\t\t\t(ior (not (match_operand:QI 0 \"q_regs_operand\" \"\"))\n-\t\t\t     (ne (symbol_ref \"TARGET_MOVX\")\n-\t\t\t\t (const_int 0))))\n-\t(const_string \"imovx\")\n-\t(const_string \"imov\")))\n-   (set (attr \"mode\")\n-     (if_then_else (eq_attr \"type\" \"imovx\")\n-\t(const_string \"SI\")\n-\t(const_string \"QI\")))])\n-\n-(define_insn \"*movqi_extv_1_rex64\"\n-  [(set (match_operand:QI 0 \"register_operand\" \"=Q,?R\")\n-        (sign_extract:QI (match_operand 1 \"ext_register_operand\" \"Q,Q\")\n-                         (const_int 8)\n-                         (const_int 8)))]\n-  \"TARGET_64BIT\"\n-{\n-  switch (get_attr_type (insn))\n-    {\n-    case TYPE_IMOVX:\n-      return \"movs{bl|x}\\t{%h1, %k0|%k0, %h1}\";\n+\treturn \"%vpxor\\t%0, %d0\";\n+    case 1:\n+    case 2:\n+      /* TDmode values are passed as TImode on the stack.  Moving them\n+\t to stack may result in unaligned memory access.  */\n+      if (misaligned_operand (operands[0], TImode)\n+\t  || misaligned_operand (operands[1], TImode))\n+\t{\n+\t  if (get_attr_mode (insn) == MODE_V4SF)\n+\t    return \"%vmovups\\t{%1, %0|%0, %1}\";\n+\t else\n+\t   return \"%vmovdqu\\t{%1, %0|%0, %1}\";\n+\t}\n+      else\n+\t{\n+\t  if (get_attr_mode (insn) == MODE_V4SF)\n+\t    return \"%vmovaps\\t{%1, %0|%0, %1}\";\n+\t else\n+\t   return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n+\t}\n     default:\n-      return \"mov{b}\\t{%h1, %0|%0, %h1}\";\n+      gcc_unreachable ();\n     }\n }\n-  [(set (attr \"type\")\n-     (if_then_else (and (match_operand:QI 0 \"register_operand\" \"\")\n-\t\t\t(ior (not (match_operand:QI 0 \"q_regs_operand\" \"\"))\n-\t\t\t     (ne (symbol_ref \"TARGET_MOVX\")\n-\t\t\t\t (const_int 0))))\n-\t(const_string \"imovx\")\n-\t(const_string \"imov\")))\n+  [(set_attr \"type\" \"sselog1,ssemov,ssemov\")\n+   (set_attr \"prefix\" \"maybe_vex\")\n    (set (attr \"mode\")\n-     (if_then_else (eq_attr \"type\" \"imovx\")\n-\t(const_string \"SI\")\n-\t(const_string \"QI\")))])\n-\n-;; Stores and loads of ax to arbitrary constant address.\n-;; We fake an second form of instruction to force reload to load address\n-;; into register when rax is not available\n-(define_insn \"*movabsqi_1_rex64\"\n-  [(set (mem:QI (match_operand:DI 0 \"x86_64_movabs_operand\" \"i,r\"))\n-\t(match_operand:QI 1 \"nonmemory_operand\" \"a,er\"))]\n-  \"TARGET_64BIT && ix86_check_movabs (insn, 0)\"\n-  \"@\n-   movabs{b}\\t{%1, %P0|%P0, %1}\n-   mov{b}\\t{%1, %a0|%a0, %1}\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"modrm\" \"0,*\")\n-   (set_attr \"length_address\" \"8,0\")\n-   (set_attr \"length_immediate\" \"0,*\")\n-   (set_attr \"memory\" \"store\")\n-   (set_attr \"mode\" \"QI\")])\n-\n-(define_insn \"*movabsqi_2_rex64\"\n-  [(set (match_operand:QI 0 \"register_operand\" \"=a,r\")\n-        (mem:QI (match_operand:DI 1 \"x86_64_movabs_operand\" \"i,r\")))]\n-  \"TARGET_64BIT && ix86_check_movabs (insn, 1)\"\n-  \"@\n-   movabs{b}\\t{%P1, %0|%0, %P1}\n-   mov{b}\\t{%a1, %0|%0, %a1}\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"modrm\" \"0,*\")\n-   (set_attr \"length_address\" \"8,0\")\n-   (set_attr \"length_immediate\" \"0\")\n-   (set_attr \"memory\" \"load\")\n-   (set_attr \"mode\" \"QI\")])\n-\n-(define_insn \"*movdi_extzv_1\"\n-  [(set (match_operand:DI 0 \"register_operand\" \"=R\")\n-\t(zero_extract:DI (match_operand 1 \"ext_register_operand\" \"Q\")\n-\t\t\t (const_int 8)\n-\t\t\t (const_int 8)))]\n-  \"TARGET_64BIT\"\n-  \"movz{bl|x}\\t{%h1, %k0|%k0, %h1}\"\n-  [(set_attr \"type\" \"imovx\")\n-   (set_attr \"mode\" \"SI\")])\n-\n-(define_insn \"*movsi_extzv_1\"\n-  [(set (match_operand:SI 0 \"register_operand\" \"=R\")\n-\t(zero_extract:SI (match_operand 1 \"ext_register_operand\" \"Q\")\n-\t\t\t (const_int 8)\n-\t\t\t (const_int 8)))]\n-  \"\"\n-  \"movz{bl|x}\\t{%h1, %0|%0, %h1}\"\n-  [(set_attr \"type\" \"imovx\")\n-   (set_attr \"mode\" \"SI\")])\n+\t(cond [(ior (eq (symbol_ref \"TARGET_SSE2\") (const_int 0))\n+\t\t    (ne (symbol_ref \"optimize_function_for_size_p (cfun)\")\n+\t\t\t(const_int 0)))\n+\t\t (const_string \"V4SF\")\n+\t       (and (eq_attr \"alternative\" \"2\")\n+\t\t    (ne (symbol_ref \"TARGET_SSE_TYPELESS_STORES\")\n+\t\t\t(const_int 0)))\n+\t\t (const_string \"V4SF\")]\n+\t      (const_string \"TI\")))])\n \n-(define_insn \"*movqi_extzv_2\"\n-  [(set (match_operand:QI 0 \"nonimmediate_operand\" \"=Qm,?R\")\n-        (subreg:QI (zero_extract:SI (match_operand 1 \"ext_register_operand\" \"Q,Q\")\n-\t\t\t\t    (const_int 8)\n-\t\t\t\t    (const_int 8)) 0))]\n-  \"!TARGET_64BIT\"\n+(define_insn \"*movdi_internal_rex64\"\n+  [(set (match_operand:DI 0 \"nonimmediate_operand\"\n+\t  \"=r,r  ,r,m ,!m,*y,*y,?r ,m ,?*Ym,?*y,*x,*x,?r ,m,?*Yi,*x,?*x,?*Ym\")\n+\t(match_operand:DI 1 \"general_operand\"\n+\t  \"Z ,rem,i,re,n ,C ,*y,*Ym,*y,r   ,m  ,C ,*x,*Yi,*x,r  ,m ,*Ym,*x\"))]\n+  \"TARGET_64BIT && !(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n {\n   switch (get_attr_type (insn))\n     {\n-    case TYPE_IMOVX:\n-      return \"movz{bl|x}\\t{%h1, %k0|%k0, %h1}\";\n-    default:\n-      return \"mov{b}\\t{%h1, %0|%0, %h1}\";\n-    }\n-}\n-  [(set (attr \"type\")\n-     (if_then_else (and (match_operand:QI 0 \"register_operand\" \"\")\n-\t\t\t(ior (not (match_operand:QI 0 \"q_regs_operand\" \"\"))\n-\t\t\t     (ne (symbol_ref \"TARGET_MOVX\")\n-\t\t\t\t (const_int 0))))\n-\t(const_string \"imovx\")\n-\t(const_string \"imov\")))\n-   (set (attr \"mode\")\n-     (if_then_else (eq_attr \"type\" \"imovx\")\n-\t(const_string \"SI\")\n-\t(const_string \"QI\")))])\n+    case TYPE_SSECVT:\n+      if (SSE_REG_P (operands[0]))\n+\treturn \"movq2dq\\t{%1, %0|%0, %1}\";\n+      else\n+\treturn \"movdq2q\\t{%1, %0|%0, %1}\";\n \n-(define_insn \"*movqi_extzv_2_rex64\"\n-  [(set (match_operand:QI 0 \"register_operand\" \"=Q,?R\")\n-        (subreg:QI (zero_extract:SI (match_operand 1 \"ext_register_operand\" \"Q,Q\")\n-\t\t\t\t    (const_int 8)\n-\t\t\t\t    (const_int 8)) 0))]\n-  \"TARGET_64BIT\"\n-{\n-  switch (get_attr_type (insn))\n-    {\n-    case TYPE_IMOVX:\n-      return \"movz{bl|x}\\t{%h1, %k0|%k0, %h1}\";\n-    default:\n-      return \"mov{b}\\t{%h1, %0|%0, %h1}\";\n-    }\n-}\n-  [(set (attr \"type\")\n-     (if_then_else (ior (not (match_operand:QI 0 \"q_regs_operand\" \"\"))\n-\t\t\t(ne (symbol_ref \"TARGET_MOVX\")\n-\t\t\t    (const_int 0)))\n-\t(const_string \"imovx\")\n-\t(const_string \"imov\")))\n-   (set (attr \"mode\")\n-     (if_then_else (eq_attr \"type\" \"imovx\")\n-\t(const_string \"SI\")\n-\t(const_string \"QI\")))])\n+    case TYPE_SSEMOV:\n+      if (TARGET_AVX)\n+\t{\n+\t  if (get_attr_mode (insn) == MODE_TI)\n+\t    return \"vmovdqa\\t{%1, %0|%0, %1}\";\n+\t  else\n+\t    return \"vmovq\\t{%1, %0|%0, %1}\";\n+\t}\n \n-(define_insn \"movsi_insv_1\"\n-  [(set (zero_extract:SI (match_operand 0 \"ext_register_operand\" \"+Q\")\n-\t\t\t (const_int 8)\n-\t\t\t (const_int 8))\n-\t(match_operand:SI 1 \"general_operand\" \"Qmn\"))]\n-  \"!TARGET_64BIT\"\n-  \"mov{b}\\t{%b1, %h0|%h0, %b1}\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"mode\" \"QI\")])\n+      if (get_attr_mode (insn) == MODE_TI)\n+\treturn \"movdqa\\t{%1, %0|%0, %1}\";\n+      /* FALLTHRU */\n \n-(define_insn \"*movsi_insv_1_rex64\"\n-  [(set (zero_extract:SI (match_operand 0 \"ext_register_operand\" \"+Q\")\n-\t\t\t (const_int 8)\n-\t\t\t (const_int 8))\n-\t(match_operand:SI 1 \"nonmemory_operand\" \"Qn\"))]\n-  \"TARGET_64BIT\"\n-  \"mov{b}\\t{%b1, %h0|%h0, %b1}\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"mode\" \"QI\")])\n+    case TYPE_MMXMOV:\n+      /* Moves from and into integer register is done using movd\n+\t opcode with REX prefix.  */\n+      if (GENERAL_REG_P (operands[0]) || GENERAL_REG_P (operands[1]))\n+\treturn \"movd\\t{%1, %0|%0, %1}\";\n+      return \"movq\\t{%1, %0|%0, %1}\";\n \n-(define_insn \"movdi_insv_1_rex64\"\n-  [(set (zero_extract:DI (match_operand 0 \"ext_register_operand\" \"+Q\")\n-\t\t\t (const_int 8)\n-\t\t\t (const_int 8))\n-\t(match_operand:DI 1 \"nonmemory_operand\" \"Qn\"))]\n-  \"TARGET_64BIT\"\n-  \"mov{b}\\t{%b1, %h0|%h0, %b1}\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"mode\" \"QI\")])\n+    case TYPE_SSELOG1:\n+      return \"%vpxor\\t%0, %d0\";\n \n-(define_insn \"*movqi_insv_2\"\n-  [(set (zero_extract:SI (match_operand 0 \"ext_register_operand\" \"+Q\")\n-\t\t\t (const_int 8)\n-\t\t\t (const_int 8))\n-\t(lshiftrt:SI (match_operand:SI 1 \"register_operand\" \"Q\")\n-\t\t     (const_int 8)))]\n-  \"\"\n-  \"mov{b}\\t{%h1, %h0|%h0, %h1}\"\n-  [(set_attr \"type\" \"imov\")\n-   (set_attr \"mode\" \"QI\")])\n+    case TYPE_MMX:\n+      return \"pxor\\t%0, %0\";\n \n-(define_expand \"movdi\"\n-  [(set (match_operand:DI 0 \"nonimmediate_operand\" \"\")\n-\t(match_operand:DI 1 \"general_operand\" \"\"))]\n-  \"\"\n-  \"ix86_expand_move (DImode, operands); DONE;\")\n+    case TYPE_MULTI:\n+      return \"#\";\n \n-(define_insn \"*pushdi\"\n-  [(set (match_operand:DI 0 \"push_operand\" \"=<\")\n-\t(match_operand:DI 1 \"general_no_elim_operand\" \"riF*m\"))]\n-  \"!TARGET_64BIT\"\n-  \"#\")\n+    case TYPE_LEA:\n+      return \"lea{q}\\t{%a1, %0|%0, %a1}\";\n \n-(define_insn \"*pushdi2_rex64\"\n-  [(set (match_operand:DI 0 \"push_operand\" \"=<,!<\")\n-\t(match_operand:DI 1 \"general_no_elim_operand\" \"re*m,n\"))]\n-  \"TARGET_64BIT\"\n-  \"@\n-   push{q}\\t%1\n-   #\"\n-  [(set_attr \"type\" \"push,multi\")\n-   (set_attr \"mode\" \"DI\")])\n+    default:\n+      gcc_assert (!flag_pic || LEGITIMATE_PIC_OPERAND_P (operands[1]));\n+      if (get_attr_mode (insn) == MODE_SI)\n+\treturn \"mov{l}\\t{%k1, %k0|%k0, %k1}\";\n+      else if (which_alternative == 2)\n+\treturn \"movabs{q}\\t{%1, %0|%0, %1}\";\n+      else\n+\treturn \"mov{q}\\t{%1, %0|%0, %1}\";\n+    }\n+}\n+  [(set (attr \"type\")\n+     (cond [(eq_attr \"alternative\" \"5\")\n+\t      (const_string \"mmx\")\n+\t    (eq_attr \"alternative\" \"6,7,8,9,10\")\n+\t      (const_string \"mmxmov\")\n+\t    (eq_attr \"alternative\" \"11\")\n+\t      (const_string \"sselog1\")\n+\t    (eq_attr \"alternative\" \"12,13,14,15,16\")\n+\t      (const_string \"ssemov\")\n+\t    (eq_attr \"alternative\" \"17,18\")\n+\t      (const_string \"ssecvt\")\n+\t    (eq_attr \"alternative\" \"4\")\n+\t      (const_string \"multi\")\n+ \t    (match_operand:DI 1 \"pic_32bit_operand\" \"\")\n+\t      (const_string \"lea\")\n+\t   ]\n+\t   (const_string \"imov\")))\n+   (set (attr \"modrm\")\n+     (if_then_else\n+       (and (eq_attr \"alternative\" \"2\") (eq_attr \"type\" \"imov\"))\n+\t (const_string \"0\")\n+\t (const_string \"*\")))\n+   (set (attr \"length_immediate\")\n+     (if_then_else\n+       (and (eq_attr \"alternative\" \"2\") (eq_attr \"type\" \"imov\"))\n+\t (const_string \"8\")\n+\t (const_string \"*\")))\n+   (set_attr \"prefix_rex\" \"*,*,*,*,*,*,*,1,*,1,*,*,*,*,*,*,*,*,*\")\n+   (set_attr \"prefix_data16\" \"*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,1,*,*,*\")\n+   (set (attr \"prefix\")\n+     (if_then_else (eq_attr \"alternative\" \"11,12,13,14,15,16\")\n+       (const_string \"maybe_vex\")\n+       (const_string \"orig\")))\n+   (set_attr \"mode\" \"SI,DI,DI,DI,SI,DI,DI,DI,DI,DI,DI,TI,TI,DI,DI,DI,DI,DI,DI\")])\n \n-;; Convert impossible pushes of immediate to existing instructions.\n+;; Convert impossible stores of immediate to existing instructions.\n ;; First try to get scratch register and go through it.  In case this\n-;; fails, push sign extended lower part first and then overwrite\n-;; upper part by 32bit move.\n+;; fails, move by 32bit parts.\n (define_peephole2\n   [(match_scratch:DI 2 \"r\")\n-   (set (match_operand:DI 0 \"push_operand\" \"\")\n+   (set (match_operand:DI 0 \"memory_operand\" \"\")\n         (match_operand:DI 1 \"immediate_operand\" \"\"))]\n   \"TARGET_64BIT && !symbolic_operand (operands[1], DImode)\n    && !x86_64_immediate_operand (operands[1], DImode)\"\n@@ -2376,94 +2069,26 @@\n ;; peephole2 pass is not run.\n ;; \"&& 1\" is needed to keep it from matching the previous pattern.\n (define_peephole2\n-  [(set (match_operand:DI 0 \"push_operand\" \"\")\n+  [(set (match_operand:DI 0 \"memory_operand\" \"\")\n         (match_operand:DI 1 \"immediate_operand\" \"\"))]\n   \"TARGET_64BIT && !symbolic_operand (operands[1], DImode)\n    && !x86_64_immediate_operand (operands[1], DImode) && 1\"\n-  [(set (match_dup 0) (match_dup 1))\n-   (set (match_dup 2) (match_dup 3))]\n-{\n-  split_di (&operands[1], 1, &operands[2], &operands[3]);\n-\n-  operands[1] = gen_lowpart (DImode, operands[2]);\n-  operands[2] = gen_rtx_MEM (SImode, gen_rtx_PLUS (DImode, stack_pointer_rtx,\n-\t\t\t\t\t\t   GEN_INT (4)));\n-})\n+  [(set (match_dup 2) (match_dup 3))\n+   (set (match_dup 4) (match_dup 5))]\n+  \"split_di (&operands[0], 2, &operands[2], &operands[4]);\")\n \n (define_split\n-  [(set (match_operand:DI 0 \"push_operand\" \"\")\n+  [(set (match_operand:DI 0 \"memory_operand\" \"\")\n         (match_operand:DI 1 \"immediate_operand\" \"\"))]\n   \"TARGET_64BIT && ((optimize > 0 && flag_peephole2)\n \t\t    ? epilogue_completed : reload_completed)\n    && !symbolic_operand (operands[1], DImode)\n    && !x86_64_immediate_operand (operands[1], DImode)\"\n-  [(set (match_dup 0) (match_dup 1))\n-   (set (match_dup 2) (match_dup 3))]\n-{\n-  split_di (&operands[1], 1, &operands[2], &operands[3]);\n-\n-  operands[1] = gen_lowpart (DImode, operands[2]);\n-  operands[2] = gen_rtx_MEM (SImode, gen_rtx_PLUS (DImode, stack_pointer_rtx,\n-\t\t\t\t\t\t   GEN_INT (4)));\n-})\n-\n-(define_insn \"*pushdi2_prologue_rex64\"\n-  [(set (match_operand:DI 0 \"push_operand\" \"=<\")\n-\t(match_operand:DI 1 \"general_no_elim_operand\" \"re*m\"))\n-   (clobber (mem:BLK (scratch)))]\n-  \"TARGET_64BIT\"\n-  \"push{q}\\t%1\"\n-  [(set_attr \"type\" \"push\")\n-   (set_attr \"mode\" \"DI\")])\n-\n-(define_insn \"*popdi1_epilogue_rex64\"\n-  [(set (match_operand:DI 0 \"nonimmediate_operand\" \"=r*m\")\n-\t(mem:DI (reg:DI SP_REG)))\n-   (set (reg:DI SP_REG)\n-\t(plus:DI (reg:DI SP_REG) (const_int 8)))\n-   (clobber (mem:BLK (scratch)))]\n-  \"TARGET_64BIT\"\n-  \"pop{q}\\t%0\"\n-  [(set_attr \"type\" \"pop\")\n-   (set_attr \"mode\" \"DI\")])\n-\n-(define_insn \"popdi1\"\n-  [(set (match_operand:DI 0 \"nonimmediate_operand\" \"=r*m\")\n-\t(mem:DI (reg:DI SP_REG)))\n-   (set (reg:DI SP_REG)\n-\t(plus:DI (reg:DI SP_REG) (const_int 8)))]\n-  \"TARGET_64BIT\"\n-  \"pop{q}\\t%0\"\n-  [(set_attr \"type\" \"pop\")\n-   (set_attr \"mode\" \"DI\")])\n-\n-(define_insn \"*movdi_xor_rex64\"\n-  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n-\t(match_operand:DI 1 \"const0_operand\" \"\"))\n-   (clobber (reg:CC FLAGS_REG))]\n-  \"TARGET_64BIT\n-   && reload_completed\"\n-  \"xor{l}\\t%k0, %k0\";\n-  [(set_attr \"type\" \"alu1\")\n-   (set_attr \"mode\" \"SI\")\n-   (set_attr \"length_immediate\" \"0\")])\n-\n-(define_insn \"*movdi_or_rex64\"\n-  [(set (match_operand:DI 0 \"register_operand\" \"=r\")\n-\t(match_operand:DI 1 \"const_int_operand\" \"i\"))\n-   (clobber (reg:CC FLAGS_REG))]\n-  \"TARGET_64BIT\n-   && reload_completed\n-   && operands[1] == constm1_rtx\"\n-{\n-  operands[1] = constm1_rtx;\n-  return \"or{q}\\t{%1, %0|%0, %1}\";\n-}\n-  [(set_attr \"type\" \"alu1\")\n-   (set_attr \"mode\" \"DI\")\n-   (set_attr \"length_immediate\" \"1\")])\n+  [(set (match_dup 2) (match_dup 3))\n+   (set (match_dup 4) (match_dup 5))]\n+  \"split_di (&operands[0], 2, &operands[2], &operands[4]);\")\n \n-(define_insn \"*movdi_2\"\n+(define_insn \"*movdi_internal\"\n   [(set (match_operand:DI 0 \"nonimmediate_operand\"\n \t\t\t\"=r  ,o  ,*y,m*y,*y,*Y2,m  ,*Y2,*Y2,*x,m ,*x,*x\")\n \t(match_operand:DI 1 \"general_operand\"\n@@ -2490,370 +2115,494 @@\n        (const_string \"orig\")))\n    (set_attr \"mode\" \"DI,DI,DI,DI,DI,TI,DI,TI,DI,V4SF,V2SF,V4SF,V2SF\")])\n \n-(define_split\n-  [(set (match_operand:DI 0 \"push_operand\" \"\")\n-        (match_operand:DI 1 \"general_operand\" \"\"))]\n-  \"!TARGET_64BIT && reload_completed\n-   && (! MMX_REG_P (operands[1]) && !SSE_REG_P (operands[1]))\"\n-  [(const_int 0)]\n-  \"ix86_split_long_move (operands); DONE;\")\n-\n-;; %%% This multiword shite has got to go.\n (define_split\n   [(set (match_operand:DI 0 \"nonimmediate_operand\" \"\")\n         (match_operand:DI 1 \"general_operand\" \"\"))]\n   \"!TARGET_64BIT && reload_completed\n-   && (!MMX_REG_P (operands[0]) && !SSE_REG_P (operands[0]))\n-   && (!MMX_REG_P (operands[1]) && !SSE_REG_P (operands[1]))\"\n+   && !(MMX_REG_P (operands[0]) || SSE_REG_P (operands[0]))\n+   && !(MMX_REG_P (operands[1]) || SSE_REG_P (operands[1]))\"\n   [(const_int 0)]\n   \"ix86_split_long_move (operands); DONE;\")\n \n-(define_insn \"*movdi_1_rex64\"\n-  [(set (match_operand:DI 0 \"nonimmediate_operand\"\n-\t  \"=r,r  ,r,m ,!m,*y,*y,?r ,m ,?*Ym,?*y,*x,*x,?r ,m,?*Yi,*x,?*x,?*Ym\")\n-\t(match_operand:DI 1 \"general_operand\"\n-\t  \"Z ,rem,i,re,n ,C ,*y,*Ym,*y,r   ,m  ,C ,*x,*Yi,*x,r  ,m ,*Ym,*x\"))]\n-  \"TARGET_64BIT && !(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n+(define_insn \"*movsi_internal\"\n+  [(set (match_operand:SI 0 \"nonimmediate_operand\"\n+\t\t\t\"=r,m ,*y,*y,?rm,?*y,*x,*x,?r ,m ,?*Yi,*x\")\n+\t(match_operand:SI 1 \"general_operand\"\n+\t\t\t\"g ,ri,C ,*y,*y ,rm ,C ,*x,*Yi,*x,r   ,m \"))]\n+  \"!(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n {\n   switch (get_attr_type (insn))\n     {\n-    case TYPE_SSECVT:\n-      if (SSE_REG_P (operands[0]))\n-\treturn \"movq2dq\\t{%1, %0|%0, %1}\";\n-      else\n-\treturn \"movdq2q\\t{%1, %0|%0, %1}\";\n+    case TYPE_SSELOG1:\n+      if (get_attr_mode (insn) == MODE_TI)\n+        return \"%vpxor\\t%0, %d0\";\n+      return \"%vxorps\\t%0, %d0\";\n \n     case TYPE_SSEMOV:\n-      if (TARGET_AVX)\n+      switch (get_attr_mode (insn))\n \t{\n-\t  if (get_attr_mode (insn) == MODE_TI)\n-\t    return \"vmovdqa\\t{%1, %0|%0, %1}\";\n-\t  else\n-\t    return \"vmovq\\t{%1, %0|%0, %1}\";\n+\tcase MODE_TI:\n+\t  return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n+\tcase MODE_V4SF:\n+\t  return \"%vmovaps\\t{%1, %0|%0, %1}\";\n+\tcase MODE_SI:\n+          return \"%vmovd\\t{%1, %0|%0, %1}\";\n+\tcase MODE_SF:\n+          return \"%vmovss\\t{%1, %0|%0, %1}\";\n+\tdefault:\n+\t  gcc_unreachable ();\n \t}\n \n-      if (get_attr_mode (insn) == MODE_TI)\n-\treturn \"movdqa\\t{%1, %0|%0, %1}\";\n-      /* FALLTHRU */\n-\n-    case TYPE_MMXMOV:\n-      /* Moves from and into integer register is done using movd\n-\t opcode with REX prefix.  */\n-      if (GENERAL_REG_P (operands[0]) || GENERAL_REG_P (operands[1]))\n-\treturn \"movd\\t{%1, %0|%0, %1}\";\n-      return \"movq\\t{%1, %0|%0, %1}\";\n-\n-    case TYPE_SSELOG1:\n-      return \"%vpxor\\t%0, %d0\";\n-\n     case TYPE_MMX:\n       return \"pxor\\t%0, %0\";\n \n-    case TYPE_MULTI:\n-      return \"#\";\n+    case TYPE_MMXMOV:\n+      if (get_attr_mode (insn) == MODE_DI)\n+\treturn \"movq\\t{%1, %0|%0, %1}\";\n+      return \"movd\\t{%1, %0|%0, %1}\";\n \n     case TYPE_LEA:\n-      return \"lea{q}\\t{%a1, %0|%0, %a1}\";\n+      return \"lea{l}\\t{%a1, %0|%0, %a1}\";\n \n     default:\n       gcc_assert (!flag_pic || LEGITIMATE_PIC_OPERAND_P (operands[1]));\n-      if (get_attr_mode (insn) == MODE_SI)\n-\treturn \"mov{l}\\t{%k1, %k0|%k0, %k1}\";\n-      else if (which_alternative == 2)\n-\treturn \"movabs{q}\\t{%1, %0|%0, %1}\";\n-      else\n-\treturn \"mov{q}\\t{%1, %0|%0, %1}\";\n+      return \"mov{l}\\t{%1, %0|%0, %1}\";\n     }\n }\n   [(set (attr \"type\")\n-     (cond [(eq_attr \"alternative\" \"5\")\n+     (cond [(eq_attr \"alternative\" \"2\")\n \t      (const_string \"mmx\")\n-\t    (eq_attr \"alternative\" \"6,7,8,9,10\")\n+\t    (eq_attr \"alternative\" \"3,4,5\")\n \t      (const_string \"mmxmov\")\n-\t    (eq_attr \"alternative\" \"11\")\n+\t    (eq_attr \"alternative\" \"6\")\n \t      (const_string \"sselog1\")\n-\t    (eq_attr \"alternative\" \"12,13,14,15,16\")\n+\t    (eq_attr \"alternative\" \"7,8,9,10,11\")\n \t      (const_string \"ssemov\")\n-\t    (eq_attr \"alternative\" \"17,18\")\n-\t      (const_string \"ssecvt\")\n-\t    (eq_attr \"alternative\" \"4\")\n-\t      (const_string \"multi\")\n  \t    (match_operand:DI 1 \"pic_32bit_operand\" \"\")\n \t      (const_string \"lea\")\n \t   ]\n \t   (const_string \"imov\")))\n-   (set (attr \"modrm\")\n-     (if_then_else\n-       (and (eq_attr \"alternative\" \"2\") (eq_attr \"type\" \"imov\"))\n-\t (const_string \"0\")\n-\t (const_string \"*\")))\n-   (set (attr \"length_immediate\")\n-     (if_then_else\n-       (and (eq_attr \"alternative\" \"2\") (eq_attr \"type\" \"imov\"))\n-\t (const_string \"8\")\n-\t (const_string \"*\")))\n-   (set_attr \"prefix_rex\" \"*,*,*,*,*,*,*,1,*,1,*,*,*,*,*,*,*,*,*\")\n-   (set_attr \"prefix_data16\" \"*,*,*,*,*,*,*,*,*,*,*,*,*,*,*,1,*,*,*\")\n    (set (attr \"prefix\")\n-     (if_then_else (eq_attr \"alternative\" \"11,12,13,14,15,16\")\n-       (const_string \"maybe_vex\")\n-       (const_string \"orig\")))\n-   (set_attr \"mode\" \"SI,DI,DI,DI,SI,DI,DI,DI,DI,DI,DI,TI,TI,DI,DI,DI,DI,DI,DI\")])\n+     (if_then_else (eq_attr \"alternative\" \"0,1,2,3,4,5\")\n+       (const_string \"orig\")\n+       (const_string \"maybe_vex\")))\n+   (set (attr \"prefix_data16\")\n+     (if_then_else (and (eq_attr \"type\" \"ssemov\") (eq_attr \"mode\" \"SI\"))\n+       (const_string \"1\")\n+       (const_string \"*\")))\n+   (set (attr \"mode\")\n+     (cond [(eq_attr \"alternative\" \"2,3\")\n+\t      (const_string \"DI\")\n+\t    (eq_attr \"alternative\" \"6,7\")\n+\t      (if_then_else\n+\t        (eq (symbol_ref \"TARGET_SSE2\") (const_int 0))\n+\t        (const_string \"V4SF\")\n+\t        (const_string \"TI\"))\n+\t    (and (eq_attr \"alternative\" \"8,9,10,11\")\n+\t         (eq (symbol_ref \"TARGET_SSE2\") (const_int 0)))\n+\t      (const_string \"SF\")\n+\t   ]\n+\t   (const_string \"SI\")))])\n+\n+(define_insn \"*movhi_internal\"\n+  [(set (match_operand:HI 0 \"nonimmediate_operand\" \"=r,r,r,m\")\n+\t(match_operand:HI 1 \"general_operand\" \"r,rn,rm,rn\"))]\n+  \"!(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n+{\n+  switch (get_attr_type (insn))\n+    {\n+    case TYPE_IMOVX:\n+      /* movzwl is faster than movw on p2 due to partial word stalls,\n+\t though not as fast as an aligned movl.  */\n+      return \"movz{wl|x}\\t{%1, %k0|%k0, %1}\";\n+    default:\n+      if (get_attr_mode (insn) == MODE_SI)\n+        return \"mov{l}\\t{%k1, %k0|%k0, %k1}\";\n+      else\n+        return \"mov{w}\\t{%1, %0|%0, %1}\";\n+    }\n+}\n+  [(set (attr \"type\")\n+     (cond [(ne (symbol_ref \"optimize_function_for_size_p (cfun)\")\n+\t\t(const_int 0))\n+\t      (const_string \"imov\")\n+\t    (and (eq_attr \"alternative\" \"0\")\n+\t\t (ior (eq (symbol_ref \"TARGET_PARTIAL_REG_STALL\")\n+\t\t\t  (const_int 0))\n+\t\t      (eq (symbol_ref \"TARGET_HIMODE_MATH\")\n+\t\t\t  (const_int 0))))\n+\t      (const_string \"imov\")\n+\t    (and (eq_attr \"alternative\" \"1,2\")\n+\t\t (match_operand:HI 1 \"aligned_operand\" \"\"))\n+\t      (const_string \"imov\")\n+\t    (and (ne (symbol_ref \"TARGET_MOVX\")\n+\t\t     (const_int 0))\n+\t\t (eq_attr \"alternative\" \"0,2\"))\n+\t      (const_string \"imovx\")\n+\t   ]\n+\t   (const_string \"imov\")))\n+    (set (attr \"mode\")\n+      (cond [(eq_attr \"type\" \"imovx\")\n+\t       (const_string \"SI\")\n+\t     (and (eq_attr \"alternative\" \"1,2\")\n+\t\t  (match_operand:HI 1 \"aligned_operand\" \"\"))\n+\t       (const_string \"SI\")\n+\t     (and (eq_attr \"alternative\" \"0\")\n+\t\t  (ior (eq (symbol_ref \"TARGET_PARTIAL_REG_STALL\")\n+\t\t\t   (const_int 0))\n+\t\t       (eq (symbol_ref \"TARGET_HIMODE_MATH\")\n+\t\t\t   (const_int 0))))\n+\t       (const_string \"SI\")\n+\t    ]\n+\t    (const_string \"HI\")))])\n+\n+;; Situation is quite tricky about when to choose full sized (SImode) move\n+;; over QImode moves.  For Q_REG -> Q_REG move we use full size only for\n+;; partial register dependency machines (such as AMD Athlon), where QImode\n+;; moves issue extra dependency and for partial register stalls machines\n+;; that don't use QImode patterns (and QImode move cause stall on the next\n+;; instruction).\n+;;\n+;; For loads of Q_REG to NONQ_REG we use full sized moves except for partial\n+;; register stall machines with, where we use QImode instructions, since\n+;; partial register stall can be caused there.  Then we use movzx.\n+(define_insn \"*movqi_internal\"\n+  [(set (match_operand:QI 0 \"nonimmediate_operand\" \"=q,q ,q ,r,r ,?r,m\")\n+\t(match_operand:QI 1 \"general_operand\"      \" q,qn,qm,q,rn,qm,qn\"))]\n+  \"!(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n+{\n+  switch (get_attr_type (insn))\n+    {\n+    case TYPE_IMOVX:\n+      gcc_assert (ANY_QI_REG_P (operands[1]) || MEM_P (operands[1]));\n+      return \"movz{bl|x}\\t{%1, %k0|%k0, %1}\";\n+    default:\n+      if (get_attr_mode (insn) == MODE_SI)\n+        return \"mov{l}\\t{%k1, %k0|%k0, %k1}\";\n+      else\n+        return \"mov{b}\\t{%1, %0|%0, %1}\";\n+    }\n+}\n+  [(set (attr \"type\")\n+     (cond [(and (eq_attr \"alternative\" \"5\")\n+\t\t (not (match_operand:QI 1 \"aligned_operand\" \"\")))\n+\t      (const_string \"imovx\")\n+\t    (ne (symbol_ref \"optimize_function_for_size_p (cfun)\")\n+\t\t(const_int 0))\n+\t      (const_string \"imov\")\n+\t    (and (eq_attr \"alternative\" \"3\")\n+\t\t (ior (eq (symbol_ref \"TARGET_PARTIAL_REG_STALL\")\n+\t\t\t  (const_int 0))\n+\t\t      (eq (symbol_ref \"TARGET_QIMODE_MATH\")\n+\t\t\t  (const_int 0))))\n+\t      (const_string \"imov\")\n+\t    (eq_attr \"alternative\" \"3,5\")\n+\t      (const_string \"imovx\")\n+\t    (and (ne (symbol_ref \"TARGET_MOVX\")\n+\t\t     (const_int 0))\n+\t\t (eq_attr \"alternative\" \"2\"))\n+\t      (const_string \"imovx\")\n+\t   ]\n+\t   (const_string \"imov\")))\n+   (set (attr \"mode\")\n+      (cond [(eq_attr \"alternative\" \"3,4,5\")\n+\t       (const_string \"SI\")\n+\t     (eq_attr \"alternative\" \"6\")\n+\t       (const_string \"QI\")\n+\t     (eq_attr \"type\" \"imovx\")\n+\t       (const_string \"SI\")\n+\t     (and (eq_attr \"type\" \"imov\")\n+\t\t  (and (eq_attr \"alternative\" \"0,1\")\n+\t\t       (and (ne (symbol_ref \"TARGET_PARTIAL_REG_DEPENDENCY\")\n+\t\t\t\t(const_int 0))\n+\t\t\t    (and (eq (symbol_ref \"optimize_function_for_size_p (cfun)\")\n+\t\t\t\t     (const_int 0))\n+\t\t\t\t (eq (symbol_ref \"TARGET_PARTIAL_REG_STALL\")\n+\t\t\t\t     (const_int 0))))))\n+\t       (const_string \"SI\")\n+\t     ;; Avoid partial register stalls when not using QImode arithmetic\n+\t     (and (eq_attr \"type\" \"imov\")\n+\t\t  (and (eq_attr \"alternative\" \"0,1\")\n+\t\t       (and (ne (symbol_ref \"TARGET_PARTIAL_REG_STALL\")\n+\t\t\t\t(const_int 0))\n+\t\t\t    (eq (symbol_ref \"TARGET_QIMODE_MATH\")\n+\t\t\t\t(const_int 0)))))\n+\t       (const_string \"SI\")\n+\t   ]\n+\t   (const_string \"QI\")))])\n \n ;; Stores and loads of ax to arbitrary constant address.\n ;; We fake an second form of instruction to force reload to load address\n ;; into register when rax is not available\n-(define_insn \"*movabsdi_1_rex64\"\n-  [(set (mem:DI (match_operand:DI 0 \"x86_64_movabs_operand\" \"i,r\"))\n-\t(match_operand:DI 1 \"nonmemory_operand\" \"a,er\"))]\n+(define_insn \"*movabs<mode>_1\"\n+  [(set (mem:SWI1248x (match_operand:DI 0 \"x86_64_movabs_operand\" \"i,r\"))\n+\t(match_operand:SWI1248x 1 \"nonmemory_operand\" \"a,er\"))]\n   \"TARGET_64BIT && ix86_check_movabs (insn, 0)\"\n   \"@\n-   movabs{q}\\t{%1, %P0|%P0, %1}\n-   mov{q}\\t{%1, %a0|%a0, %1}\"\n+   movabs{<imodesuffix>}\\t{%1, %P0|%P0, %1}\n+   mov{<imodesuffix>}\\t{%1, %a0|%a0, %1}\"\n   [(set_attr \"type\" \"imov\")\n    (set_attr \"modrm\" \"0,*\")\n    (set_attr \"length_address\" \"8,0\")\n    (set_attr \"length_immediate\" \"0,*\")\n    (set_attr \"memory\" \"store\")\n-   (set_attr \"mode\" \"DI\")])\n+   (set_attr \"mode\" \"<MODE>\")])\n \n-(define_insn \"*movabsdi_2_rex64\"\n-  [(set (match_operand:DI 0 \"register_operand\" \"=a,r\")\n-        (mem:DI (match_operand:DI 1 \"x86_64_movabs_operand\" \"i,r\")))]\n+(define_insn \"*movabs<mode>_2\"\n+  [(set (match_operand:SWI1248x 0 \"register_operand\" \"=a,r\")\n+        (mem:SWI1248x (match_operand:DI 1 \"x86_64_movabs_operand\" \"i,r\")))]\n   \"TARGET_64BIT && ix86_check_movabs (insn, 1)\"\n   \"@\n-   movabs{q}\\t{%P1, %0|%0, %P1}\n-   mov{q}\\t{%a1, %0|%0, %a1}\"\n+   movabs{<imodesuffix>}\\t{%P1, %0|%0, %P1}\n+   mov{<imodesuffix>}\\t{%a1, %0|%0, %a1}\"\n   [(set_attr \"type\" \"imov\")\n    (set_attr \"modrm\" \"0,*\")\n    (set_attr \"length_address\" \"8,0\")\n    (set_attr \"length_immediate\" \"0\")\n    (set_attr \"memory\" \"load\")\n-   (set_attr \"mode\" \"DI\")])\n-\n-;; Convert impossible stores of immediate to existing instructions.\n-;; First try to get scratch register and go through it.  In case this\n-;; fails, move by 32bit parts.\n-(define_peephole2\n-  [(match_scratch:DI 2 \"r\")\n-   (set (match_operand:DI 0 \"memory_operand\" \"\")\n-        (match_operand:DI 1 \"immediate_operand\" \"\"))]\n-  \"TARGET_64BIT && !symbolic_operand (operands[1], DImode)\n-   && !x86_64_immediate_operand (operands[1], DImode)\"\n-  [(set (match_dup 2) (match_dup 1))\n-   (set (match_dup 0) (match_dup 2))]\n-  \"\")\n-\n-;; We need to define this as both peepholer and splitter for case\n-;; peephole2 pass is not run.\n-;; \"&& 1\" is needed to keep it from matching the previous pattern.\n-(define_peephole2\n-  [(set (match_operand:DI 0 \"memory_operand\" \"\")\n-        (match_operand:DI 1 \"immediate_operand\" \"\"))]\n-  \"TARGET_64BIT && !symbolic_operand (operands[1], DImode)\n-   && !x86_64_immediate_operand (operands[1], DImode) && 1\"\n-  [(set (match_dup 2) (match_dup 3))\n-   (set (match_dup 4) (match_dup 5))]\n-  \"split_di (&operands[0], 2, &operands[2], &operands[4]);\")\n+   (set_attr \"mode\" \"<MODE>\")])\n \n-(define_split\n-  [(set (match_operand:DI 0 \"memory_operand\" \"\")\n-        (match_operand:DI 1 \"immediate_operand\" \"\"))]\n-  \"TARGET_64BIT && ((optimize > 0 && flag_peephole2)\n-\t\t    ? epilogue_completed : reload_completed)\n-   && !symbolic_operand (operands[1], DImode)\n-   && !x86_64_immediate_operand (operands[1], DImode)\"\n-  [(set (match_dup 2) (match_dup 3))\n-   (set (match_dup 4) (match_dup 5))]\n-  \"split_di (&operands[0], 2, &operands[2], &operands[4]);\")\n+(define_insn \"*swap<mode>\"\n+  [(set (match_operand:SWI48 0 \"register_operand\" \"+r\")\n+\t(match_operand:SWI48 1 \"register_operand\" \"+r\"))\n+   (set (match_dup 1)\n+\t(match_dup 0))]\n+  \"\"\n+  \"xchg{<imodesuffix>}\\t%1, %0\"\n+  [(set_attr \"type\" \"imov\")\n+   (set_attr \"mode\" \"<MODE>\")\n+   (set_attr \"pent_pair\" \"np\")\n+   (set_attr \"athlon_decode\" \"vector\")\n+   (set_attr \"amdfam10_decode\" \"double\")])\n \n-(define_insn \"*swapdi_rex64\"\n-  [(set (match_operand:DI 0 \"register_operand\" \"+r\")\n-\t(match_operand:DI 1 \"register_operand\" \"+r\"))\n+(define_insn \"*swap<mode>_1\"\n+  [(set (match_operand:SWI12 0 \"register_operand\" \"+r\")\n+\t(match_operand:SWI12 1 \"register_operand\" \"+r\"))\n    (set (match_dup 1)\n \t(match_dup 0))]\n-  \"TARGET_64BIT\"\n-  \"xchg{q}\\t%1, %0\"\n+  \"!TARGET_PARTIAL_REG_STALL || optimize_function_for_size_p (cfun)\"\n+  \"xchg{l}\\t%k1, %k0\"\n   [(set_attr \"type\" \"imov\")\n-   (set_attr \"mode\" \"DI\")\n+   (set_attr \"mode\" \"SI\")\n    (set_attr \"pent_pair\" \"np\")\n    (set_attr \"athlon_decode\" \"vector\")\n    (set_attr \"amdfam10_decode\" \"double\")])\n \n-(define_expand \"movoi\"\n-  [(set (match_operand:OI 0 \"nonimmediate_operand\" \"\")\n-\t(match_operand:OI 1 \"general_operand\" \"\"))]\n-  \"TARGET_AVX\"\n-  \"ix86_expand_move (OImode, operands); DONE;\")\n+;; Not added amdfam10_decode since TARGET_PARTIAL_REG_STALL\n+;; is disabled for AMDFAM10\n+(define_insn \"*swap<mode>_2\"\n+  [(set (match_operand:SWI12 0 \"register_operand\" \"+<r>\")\n+\t(match_operand:SWI12 1 \"register_operand\" \"+<r>\"))\n+   (set (match_dup 1)\n+\t(match_dup 0))]\n+  \"TARGET_PARTIAL_REG_STALL\"\n+  \"xchg{<imodesuffix>}\\t%1, %0\"\n+  [(set_attr \"type\" \"imov\")\n+   (set_attr \"mode\" \"<MODE>\")\n+   (set_attr \"pent_pair\" \"np\")\n+   (set_attr \"athlon_decode\" \"vector\")])\n+\n+(define_expand \"movstrict<mode>\"\n+  [(set (strict_low_part (match_operand:SWI12 0 \"nonimmediate_operand\" \"\"))\n+\t(match_operand:SWI12 1 \"general_operand\" \"\"))]\n+  \"\"\n+{\n+  if (TARGET_PARTIAL_REG_STALL && optimize_function_for_speed_p (cfun))\n+    FAIL;\n+  /* Don't generate memory->memory moves, go through a register */\n+  if (MEM_P (operands[0]) && MEM_P (operands[1]))\n+    operands[1] = force_reg (<MODE>mode, operands[1]);\n+})\n+\n+(define_insn \"*movstrict<mode>_1\"\n+  [(set (strict_low_part\n+\t  (match_operand:SWI12 0 \"nonimmediate_operand\" \"+<r>m,<r>\"))\n+\t(match_operand:SWI12 1 \"general_operand\" \"<r>n,m\"))]\n+  \"(!TARGET_PARTIAL_REG_STALL || optimize_function_for_size_p (cfun))\n+   && !(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n+  \"mov{<imodesuffix>}\\t{%1, %0|%0, %1}\"\n+  [(set_attr \"type\" \"imov\")\n+   (set_attr \"mode\" \"<MODE>\")])\n+\n+(define_insn \"*movstrict<mode>_xor\"\n+  [(set (strict_low_part (match_operand:SWI12 0 \"register_operand\" \"+<r>\"))\n+\t(match_operand:SWI12 1 \"const0_operand\" \"\"))\n+   (clobber (reg:CC FLAGS_REG))]\n+  \"reload_completed\"\n+  \"xor{<imodesuffix>}\\t%0, %0\"\n+  [(set_attr \"type\" \"alu1\")\n+   (set_attr \"mode\" \"<MODE>\")\n+   (set_attr \"length_immediate\" \"0\")])\n+\n+(define_insn \"*mov<mode>_extv_1\"\n+  [(set (match_operand:SWI24 0 \"register_operand\" \"=R\")\n+\t(sign_extract:SWI24 (match_operand 1 \"ext_register_operand\" \"Q\")\n+\t\t\t    (const_int 8)\n+\t\t\t    (const_int 8)))]\n+  \"\"\n+  \"movs{bl|x}\\t{%h1, %k0|%k0, %h1}\"\n+  [(set_attr \"type\" \"imovx\")\n+   (set_attr \"mode\" \"SI\")])\n \n-(define_insn \"*movoi_internal\"\n-  [(set (match_operand:OI 0 \"nonimmediate_operand\" \"=x,x,m\")\n-\t(match_operand:OI 1 \"vector_move_operand\" \"C,xm,x\"))]\n-  \"TARGET_AVX\n-   && !(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n+(define_insn \"*movqi_extv_1_rex64\"\n+  [(set (match_operand:QI 0 \"register_operand\" \"=Q,?R\")\n+        (sign_extract:QI (match_operand 1 \"ext_register_operand\" \"Q,Q\")\n+                         (const_int 8)\n+                         (const_int 8)))]\n+  \"TARGET_64BIT\"\n {\n-  switch (which_alternative)\n+  switch (get_attr_type (insn))\n     {\n-    case 0:\n-      return \"vxorps\\t%0, %0, %0\";\n-    case 1:\n-    case 2:\n-      if (misaligned_operand (operands[0], OImode)\n-\t  || misaligned_operand (operands[1], OImode))\n-\treturn \"vmovdqu\\t{%1, %0|%0, %1}\";\n-      else\n-\treturn \"vmovdqa\\t{%1, %0|%0, %1}\";\n+    case TYPE_IMOVX:\n+      return \"movs{bl|x}\\t{%h1, %k0|%k0, %h1}\";\n     default:\n-      gcc_unreachable ();\n+      return \"mov{b}\\t{%h1, %0|%0, %h1}\";\n     }\n }\n-  [(set_attr \"type\" \"sselog1,ssemov,ssemov\")\n-   (set_attr \"prefix\" \"vex\")\n-   (set_attr \"mode\" \"OI\")])\n+  [(set (attr \"type\")\n+     (if_then_else (and (match_operand:QI 0 \"register_operand\" \"\")\n+\t\t\t(ior (not (match_operand:QI 0 \"q_regs_operand\" \"\"))\n+\t\t\t     (ne (symbol_ref \"TARGET_MOVX\")\n+\t\t\t\t (const_int 0))))\n+\t(const_string \"imovx\")\n+\t(const_string \"imov\")))\n+   (set (attr \"mode\")\n+     (if_then_else (eq_attr \"type\" \"imovx\")\n+\t(const_string \"SI\")\n+\t(const_string \"QI\")))])\n \n-(define_expand \"movti\"\n-  [(set (match_operand:TI 0 \"nonimmediate_operand\" \"\")\n-\t(match_operand:TI 1 \"nonimmediate_operand\" \"\"))]\n-  \"TARGET_SSE || TARGET_64BIT\"\n+(define_insn \"*movqi_extv_1\"\n+  [(set (match_operand:QI 0 \"nonimmediate_operand\" \"=Qm,?r\")\n+        (sign_extract:QI (match_operand 1 \"ext_register_operand\" \"Q,Q\")\n+                         (const_int 8)\n+                         (const_int 8)))]\n+  \"!TARGET_64BIT\"\n {\n-  if (TARGET_64BIT)\n-    ix86_expand_move (TImode, operands);\n-  else if (push_operand (operands[0], TImode))\n-    ix86_expand_push (TImode, operands[1]);\n-  else\n-    ix86_expand_vector_move (TImode, operands);\n-  DONE;\n-})\n+  switch (get_attr_type (insn))\n+    {\n+    case TYPE_IMOVX:\n+      return \"movs{bl|x}\\t{%h1, %k0|%k0, %h1}\";\n+    default:\n+      return \"mov{b}\\t{%h1, %0|%0, %h1}\";\n+    }\n+}\n+  [(set (attr \"type\")\n+     (if_then_else (and (match_operand:QI 0 \"register_operand\" \"\")\n+\t\t\t(ior (not (match_operand:QI 0 \"q_regs_operand\" \"\"))\n+\t\t\t     (ne (symbol_ref \"TARGET_MOVX\")\n+\t\t\t\t (const_int 0))))\n+\t(const_string \"imovx\")\n+\t(const_string \"imov\")))\n+   (set (attr \"mode\")\n+     (if_then_else (eq_attr \"type\" \"imovx\")\n+\t(const_string \"SI\")\n+\t(const_string \"QI\")))])\n \n-(define_insn \"*movti_internal\"\n-  [(set (match_operand:TI 0 \"nonimmediate_operand\" \"=x,x,m\")\n-\t(match_operand:TI 1 \"vector_move_operand\" \"C,xm,x\"))]\n-  \"TARGET_SSE && !TARGET_64BIT\n-   && !(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n+(define_insn \"*mov<mode>_extzv_1\"\n+  [(set (match_operand:SWI48 0 \"register_operand\" \"=R\")\n+\t(zero_extract:SWI48 (match_operand 1 \"ext_register_operand\" \"Q\")\n+\t\t\t    (const_int 8)\n+\t\t\t    (const_int 8)))]\n+  \"\"\n+  \"movz{bl|x}\\t{%h1, %k0|%k0, %h1}\"\n+  [(set_attr \"type\" \"imovx\")\n+   (set_attr \"mode\" \"SI\")])\n+\n+(define_insn \"*movqi_extzv_2_rex64\"\n+  [(set (match_operand:QI 0 \"register_operand\" \"=Q,?R\")\n+        (subreg:QI\n+\t  (zero_extract:SI (match_operand 1 \"ext_register_operand\" \"Q,Q\")\n+\t\t\t   (const_int 8)\n+\t\t\t   (const_int 8)) 0))]\n+  \"TARGET_64BIT\"\n {\n-  switch (which_alternative)\n+  switch (get_attr_type (insn))\n     {\n-    case 0:\n-      if (get_attr_mode (insn) == MODE_V4SF)\n-\treturn \"%vxorps\\t%0, %d0\";\n-      else\n-\treturn \"%vpxor\\t%0, %d0\";\n-    case 1:\n-    case 2:\n-      /* TDmode values are passed as TImode on the stack.  Moving them\n-\t to stack may result in unaligned memory access.  */\n-      if (misaligned_operand (operands[0], TImode)\n-\t  || misaligned_operand (operands[1], TImode))\n-\t{\n-\t  if (get_attr_mode (insn) == MODE_V4SF)\n-\t    return \"%vmovups\\t{%1, %0|%0, %1}\";\n-\t else\n-\t   return \"%vmovdqu\\t{%1, %0|%0, %1}\";\n-\t}\n-      else\n-\t{\n-\t  if (get_attr_mode (insn) == MODE_V4SF)\n-\t    return \"%vmovaps\\t{%1, %0|%0, %1}\";\n-\t else\n-\t   return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n-\t}\n+    case TYPE_IMOVX:\n+      return \"movz{bl|x}\\t{%h1, %k0|%k0, %h1}\";\n     default:\n-      gcc_unreachable ();\n+      return \"mov{b}\\t{%h1, %0|%0, %h1}\";\n     }\n }\n-  [(set_attr \"type\" \"sselog1,ssemov,ssemov\")\n-   (set_attr \"prefix\" \"maybe_vex\")\n+  [(set (attr \"type\")\n+     (if_then_else (ior (not (match_operand:QI 0 \"q_regs_operand\" \"\"))\n+\t\t\t(ne (symbol_ref \"TARGET_MOVX\")\n+\t\t\t    (const_int 0)))\n+\t(const_string \"imovx\")\n+\t(const_string \"imov\")))\n    (set (attr \"mode\")\n-\t(cond [(ior (eq (symbol_ref \"TARGET_SSE2\") (const_int 0))\n-\t\t    (ne (symbol_ref \"optimize_function_for_size_p (cfun)\") (const_int 0)))\n-\t\t (const_string \"V4SF\")\n-\t       (and (eq_attr \"alternative\" \"2\")\n-\t\t    (ne (symbol_ref \"TARGET_SSE_TYPELESS_STORES\")\n-\t\t\t(const_int 0)))\n-\t\t (const_string \"V4SF\")]\n-\t      (const_string \"TI\")))])\n+     (if_then_else (eq_attr \"type\" \"imovx\")\n+\t(const_string \"SI\")\n+\t(const_string \"QI\")))])\n \n-(define_insn \"*movti_rex64\"\n-  [(set (match_operand:TI 0 \"nonimmediate_operand\" \"=!r,o,x,x,xm\")\n-\t(match_operand:TI 1 \"general_operand\" \"riFo,riF,C,xm,x\"))]\n-  \"TARGET_64BIT\n-   && !(MEM_P (operands[0]) && MEM_P (operands[1]))\"\n+(define_insn \"*movqi_extzv_2\"\n+  [(set (match_operand:QI 0 \"nonimmediate_operand\" \"=Qm,?R\")\n+        (subreg:QI\n+\t  (zero_extract:SI (match_operand 1 \"ext_register_operand\" \"Q,Q\")\n+\t\t\t   (const_int 8)\n+\t\t\t   (const_int 8)) 0))]\n+  \"!TARGET_64BIT\"\n {\n-  switch (which_alternative)\n+  switch (get_attr_type (insn))\n     {\n-    case 0:\n-    case 1:\n-      return \"#\";\n-    case 2:\n-      if (get_attr_mode (insn) == MODE_V4SF)\n-\treturn \"%vxorps\\t%0, %d0\";\n-      else\n-\treturn \"%vpxor\\t%0, %d0\";\n-    case 3:\n-    case 4:\n-      /* TDmode values are passed as TImode on the stack.  Moving them\n-\t to stack may result in unaligned memory access.  */\n-      if (misaligned_operand (operands[0], TImode)\n-\t  || misaligned_operand (operands[1], TImode))\n-\t{\n-\t  if (get_attr_mode (insn) == MODE_V4SF)\n-\t    return \"%vmovups\\t{%1, %0|%0, %1}\";\n-\t else\n-\t   return \"%vmovdqu\\t{%1, %0|%0, %1}\";\n-\t}\n-      else\n-\t{\n-\t  if (get_attr_mode (insn) == MODE_V4SF)\n-\t    return \"%vmovaps\\t{%1, %0|%0, %1}\";\n-\t else\n-\t   return \"%vmovdqa\\t{%1, %0|%0, %1}\";\n-\t}\n+    case TYPE_IMOVX:\n+      return \"movz{bl|x}\\t{%h1, %k0|%k0, %h1}\";\n     default:\n-      gcc_unreachable ();\n+      return \"mov{b}\\t{%h1, %0|%0, %h1}\";\n     }\n }\n-  [(set_attr \"type\" \"*,*,sselog1,ssemov,ssemov\")\n-   (set_attr \"prefix\" \"*,*,maybe_vex,maybe_vex,maybe_vex\")\n+  [(set (attr \"type\")\n+     (if_then_else (and (match_operand:QI 0 \"register_operand\" \"\")\n+\t\t\t(ior (not (match_operand:QI 0 \"q_regs_operand\" \"\"))\n+\t\t\t     (ne (symbol_ref \"TARGET_MOVX\")\n+\t\t\t\t (const_int 0))))\n+\t(const_string \"imovx\")\n+\t(const_string \"imov\")))\n    (set (attr \"mode\")\n-        (cond [(eq_attr \"alternative\" \"2,3\")\n-\t\t (if_then_else\n-\t\t   (ne (symbol_ref \"optimize_function_for_size_p (cfun)\")\n-\t\t       (const_int 0))\n-\t\t   (const_string \"V4SF\")\n-\t\t   (const_string \"TI\"))\n-\t       (eq_attr \"alternative\" \"4\")\n-\t\t (if_then_else\n-\t\t   (ior (ne (symbol_ref \"TARGET_SSE_TYPELESS_STORES\")\n-\t\t\t    (const_int 0))\n-\t\t\t(ne (symbol_ref \"optimize_function_for_size_p (cfun)\")\n-\t\t\t    (const_int 0)))\n-\t\t   (const_string \"V4SF\")\n-\t\t   (const_string \"TI\"))]\n-\t       (const_string \"DI\")))])\n+     (if_then_else (eq_attr \"type\" \"imovx\")\n+\t(const_string \"SI\")\n+\t(const_string \"QI\")))])\n \n-(define_split\n-  [(set (match_operand:TI 0 \"nonimmediate_operand\" \"\")\n-        (match_operand:TI 1 \"general_operand\" \"\"))]\n-  \"reload_completed && !SSE_REG_P (operands[0])\n-   && !SSE_REG_P (operands[1])\"\n-  [(const_int 0)]\n-  \"ix86_split_long_move (operands); DONE;\")\n+(define_expand \"mov<mode>_insv_1\"\n+  [(set (zero_extract:SWI48 (match_operand 0 \"ext_register_operand\" \"\")\n+\t\t\t    (const_int 8)\n+\t\t\t    (const_int 8))\n+\t(match_operand:SWI48 1 \"nonmemory_operand\" \"\"))]\n+  \"\"\n+  \"\")\n \n-;; This expands to what emit_move_complex would generate if we didn't\n-;; have a movti pattern.  Having this avoids problems with reload on\n-;; 32-bit targets when SSE is present, but doesn't seem to be harmful\n-;; to have around all the time.\n-(define_expand \"movcdi\"\n-  [(set (match_operand:CDI 0 \"nonimmediate_operand\" \"\")\n-\t(match_operand:CDI 1 \"general_operand\" \"\"))]\n+(define_insn \"*mov<mode>_insv_1_rex64\"\n+  [(set (zero_extract:SWI48x (match_operand 0 \"ext_register_operand\" \"+Q\")\n+\t\t\t     (const_int 8)\n+\t\t\t     (const_int 8))\n+\t(match_operand:SWI48x 1 \"nonmemory_operand\" \"Qn\"))]\n+  \"TARGET_64BIT\"\n+  \"mov{b}\\t{%b1, %h0|%h0, %b1}\"\n+  [(set_attr \"type\" \"imov\")\n+   (set_attr \"mode\" \"QI\")])\n+\n+(define_insn \"*movsi_insv_1\"\n+  [(set (zero_extract:SI (match_operand 0 \"ext_register_operand\" \"+Q\")\n+\t\t\t (const_int 8)\n+\t\t\t (const_int 8))\n+\t(match_operand:SI 1 \"general_operand\" \"Qmn\"))]\n+  \"!TARGET_64BIT\"\n+  \"mov{b}\\t{%b1, %h0|%h0, %b1}\"\n+  [(set_attr \"type\" \"imov\")\n+   (set_attr \"mode\" \"QI\")])\n+\n+(define_insn \"*movqi_insv_2\"\n+  [(set (zero_extract:SI (match_operand 0 \"ext_register_operand\" \"+Q\")\n+\t\t\t (const_int 8)\n+\t\t\t (const_int 8))\n+\t(lshiftrt:SI (match_operand:SI 1 \"register_operand\" \"Q\")\n+\t\t     (const_int 8)))]\n   \"\"\n-{\n-  if (push_operand (operands[0], CDImode))\n-    emit_move_complex_push (CDImode, operands[0], operands[1]);\n-  else\n-    emit_move_complex_parts (operands[0], operands[1]);\n-  DONE;\n-})\n+  \"mov{b}\\t{%h1, %h0|%h0, %h1}\"\n+  [(set_attr \"type\" \"imov\")\n+   (set_attr \"mode\" \"QI\")])\n+\f\n+;; Floating point move instructions.\n \n (define_expand \"movsf\"\n   [(set (match_operand:SF 0 \"nonimmediate_operand\" \"\")\n@@ -4137,7 +3886,7 @@\n \t(zero_extend:DI (match_operand:SI 1 \"general_operand\" \"\")))\n    (clobber (reg:CC FLAGS_REG))]\n   \"!TARGET_64BIT && reload_completed\n-   && !SSE_REG_P (operands[0]) && !MMX_REG_P (operands[0])\"\n+   && !(MMX_REG_P (operands[0]) || SSE_REG_P (operands[0]))\"\n   [(set (match_dup 3) (match_dup 1))\n    (set (match_dup 4) (const_int 0))]\n   \"split_di (&operands[0], 1, &operands[3], &operands[4]);\")\n@@ -10520,7 +10269,7 @@\n     FAIL;\n \n   if (TARGET_64BIT)\n-    emit_insn (gen_movdi_insv_1_rex64 (operands[0], operands[3]));\n+    emit_insn (gen_movdi_insv_1 (operands[0], operands[3]));\n   else\n     emit_insn (gen_movsi_insv_1 (operands[0], operands[3]));\n "}]}