{"sha": "4f3f76e6c39bbd7924032dca7ed5de404f247e3b", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NGYzZjc2ZTZjMzliYmQ3OTI0MDMyZGNhN2VkNWRlNDA0ZjI0N2UzYg==", "commit": {"author": {"name": "Michael Meissner", "email": "meissner@gcc.gnu.org", "date": "2007-09-19T21:41:08Z"}, "committer": {"name": "Michael Meissner", "email": "meissner@gcc.gnu.org", "date": "2007-09-19T21:41:08Z"}, "message": "Eliminate trailing whitespace\n\nFrom-SVN: r128605", "tree": {"sha": "388fd26895e48eecd5226317111e04f710d0ed07", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/388fd26895e48eecd5226317111e04f710d0ed07"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/4f3f76e6c39bbd7924032dca7ed5de404f247e3b", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4f3f76e6c39bbd7924032dca7ed5de404f247e3b", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4f3f76e6c39bbd7924032dca7ed5de404f247e3b", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/comments", "author": null, "committer": null, "parents": [{"sha": "b486fc0a25f880e9f1ba3f0b5115b6c202d9e353", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/b486fc0a25f880e9f1ba3f0b5115b6c202d9e353", "html_url": "https://github.com/Rust-GCC/gccrs/commit/b486fc0a25f880e9f1ba3f0b5115b6c202d9e353"}], "stats": {"total": 400, "additions": 200, "deletions": 200}, "files": [{"sha": "c9860def7c65face8b8d9b87db0f3131f6ee5c01", "filename": "gcc/config/i386/athlon.md", "status": "modified", "additions": 7, "deletions": 7, "changes": 14, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fathlon.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fathlon.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fathlon.md?ref=4f3f76e6c39bbd7924032dca7ed5de404f247e3b", "patch": "@@ -1,7 +1,7 @@\n ;; AMD Athlon Scheduling\n ;;\n ;; The Athlon does contain three pipelined FP units, three integer units and\n-;; three address generation units. \n+;; three address generation units.\n ;;\n ;; The predecode logic is determining boundaries of instructions in the 64\n ;; byte cache line. So the cache line straddling problem of K6 might be issue\n@@ -206,7 +206,7 @@\n \t\t\t      (and (eq_attr \"type\" \"imul\")\n \t\t\t\t   (and (eq_attr \"mode\" \"HI\")\n \t\t\t\t\t(eq_attr \"memory\" \"none,unknown\"))))\n-\t\t\t \"athlon-vector,athlon-ieu0,athlon-mult,nothing,athlon-ieu0\")\t\t\t \n+\t\t\t \"athlon-vector,athlon-ieu0,athlon-mult,nothing,athlon-ieu0\")\n (define_insn_reservation \"athlon_imul_mem\" 8\n \t\t\t (and (eq_attr \"cpu\" \"athlon\")\n \t\t\t      (and (eq_attr \"type\" \"imul\")\n@@ -328,7 +328,7 @@\n \t\t\t\t\t(eq_attr \"memory\" \"both\"))))\n \t\t\t \"athlon-direct,athlon-load,\n \t\t\t  athlon-ieu,athlon-store,\n-\t\t\t  athlon-store\")\t\t\t  \n+\t\t\t  athlon-store\")\n \n (define_insn_reservation \"athlon_ivector_both\" 6\n \t\t\t (and (eq_attr \"cpu\" \"athlon,k8,generic64\")\n@@ -617,7 +617,7 @@\n \t\t\t (and (eq_attr \"cpu\" \"amdfam10\")\n \t\t\t      (and (eq_attr \"type\" \"mmxmov\")\n \t\t\t\t   (eq_attr \"memory\" \"load\")))\n-\t\t\t \"athlon-direct,athlon-fploadk8, athlon-fany\")\t\t\t \n+\t\t\t \"athlon-direct,athlon-fploadk8, athlon-fany\")\n (define_insn_reservation \"athlon_mmxssest\" 3\n \t\t\t (and (eq_attr \"cpu\" \"k8,generic64\")\n \t\t\t      (and (eq_attr \"type\" \"mmxmov,ssemov\")\n@@ -926,7 +926,7 @@\n \t\t\t\t   (and (eq_attr \"amdfam10_decode\" \"double\")\n \t\t\t\t\t(and (eq_attr \"mode\" \"SF,DF\")\n \t\t\t\t\t     (eq_attr \"memory\" \"load\")))))\n-\t\t\t \"athlon-double,athlon-fploadk8,(athlon-faddmul+athlon-fstore)\")\t\t\t \n+\t\t\t \"athlon-double,athlon-fploadk8,(athlon-faddmul+athlon-fstore)\")\n ;; cvtsi2sd reg,reg is double decoded (vector on Athlon)\n (define_insn_reservation \"athlon_sseicvt_cvtsi2sd_k8\" 11\n \t\t\t (and (eq_attr \"cpu\" \"k8,athlon,generic64\")\n@@ -1115,7 +1115,7 @@\n (define_insn_reservation \"athlon_ssemulvector_amdfam10\" 4\n \t\t\t (and (eq_attr \"cpu\" \"amdfam10\")\n \t\t\t      (eq_attr \"type\" \"ssemul\"))\n-\t\t\t \"athlon-direct,athlon-fpsched,athlon-fmul\")\t\t\t \n+\t\t\t \"athlon-direct,athlon-fpsched,athlon-fmul\")\n ;; divsd timings.  divss is faster\n (define_insn_reservation \"athlon_ssediv_load\" 20\n \t\t\t (and (eq_attr \"cpu\" \"athlon\")\n@@ -1148,7 +1148,7 @@\n \t\t\t (and (eq_attr \"cpu\" \"amdfam10\")\n \t\t\t      (and (eq_attr \"type\" \"ssediv\")\n \t\t\t\t   (eq_attr \"memory\" \"load\")))\n-\t\t\t \"athlon-direct,athlon-fploadk8,athlon-fmul*17\")\t\t\t \n+\t\t\t \"athlon-direct,athlon-fploadk8,athlon-fmul*17\")\n (define_insn_reservation \"athlon_ssedivvector\" 39\n \t\t\t (and (eq_attr \"cpu\" \"athlon\")\n \t\t\t      (eq_attr \"type\" \"ssediv\"))"}, {"sha": "35c9831ded85c2a454c1af9492856567521fb111", "filename": "gcc/config/i386/bmmintrin.h", "status": "modified", "additions": 123, "deletions": 123, "changes": 246, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fbmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fbmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fbmmintrin.h?ref=4f3f76e6c39bbd7924032dca7ed5de404f247e3b", "patch": "@@ -154,7 +154,7 @@ _mm_maccsd_epi16(__m128i __A, __m128i __B, __m128i __C)\n static __inline __m128i __attribute__((__always_inline__))\n _mm_maccd_epi16(__m128i __A, __m128i __B, __m128i __C)\n {\n-  return  (__m128i) __builtin_ia32_pmacswd ((__v8hi)__A, (__v8hi)__B, (__v4si)__C); \n+  return  (__m128i) __builtin_ia32_pmacswd ((__v8hi)__A, (__v8hi)__B, (__v4si)__C);\n }\n \n static __inline __m128i __attribute__((__always_inline__))\n@@ -178,19 +178,19 @@ _mm_maccslo_epi32(__m128i __A, __m128i __B, __m128i __C)\n static __inline __m128i __attribute__((__always_inline__))\n _mm_macclo_epi32(__m128i __A, __m128i __B, __m128i __C)\n {\n-  return  (__m128i) __builtin_ia32_pmacsdql ((__v4si)__A, (__v4si)__B, (__v2di)__C); \n+  return  (__m128i) __builtin_ia32_pmacsdql ((__v4si)__A, (__v4si)__B, (__v2di)__C);\n }\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_maccshi_epi32(__m128i __A, __m128i __B, __m128i __C)\n {\n-  return  (__m128i) __builtin_ia32_pmacssdqh ((__v4si)__A, (__v4si)__B, (__v2di)__C); \n+  return  (__m128i) __builtin_ia32_pmacssdqh ((__v4si)__A, (__v4si)__B, (__v2di)__C);\n }\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_macchi_epi32(__m128i __A, __m128i __B, __m128i __C)\n {\n-  return  (__m128i) __builtin_ia32_pmacsdqh ((__v4si)__A, (__v4si)__B, (__v2di)__C); \n+  return  (__m128i) __builtin_ia32_pmacsdqh ((__v4si)__A, (__v4si)__B, (__v2di)__C);\n }\n \n static __inline __m128i __attribute__((__always_inline__))\n@@ -324,25 +324,25 @@ _mm_perm_pd(__m128d __A, __m128d __B, __m128i __C)\n /* Packed Integer Rotates and Shifts */\n \n /* Rotates - Non-Immediate form */\n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_rot_epi8(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_protb ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_rot_epi16(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_protw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_rot_epi32(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_protd ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_rot_epi64(__m128i __A,  __m128i __B)\n {\n   return (__m128i)  __builtin_ia32_protq ((__v2di)__A, (__v2di)__B);\n@@ -351,25 +351,25 @@ _mm_rot_epi64(__m128i __A,  __m128i __B)\n \n /* Rotates - Immediate form */\n #ifdef __OPTIMIZE__\n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_roti_epi8(__m128i __A,  int __B)\n {\n   return  (__m128i) __builtin_ia32_protbi ((__v16qi)__A, __B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_roti_epi16(__m128i __A, int __B)\n {\n   return  (__m128i) __builtin_ia32_protwi ((__v8hi)__A, __B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_roti_epi32(__m128i __A, int __B)\n {\n   return  (__m128i) __builtin_ia32_protdi ((__v4si)__A, __B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_roti_epi64(__m128i __A, int __B)\n {\n   return  (__m128i) __builtin_ia32_protqi ((__v2di)__A, __B);\n@@ -383,50 +383,50 @@ _mm_roti_epi64(__m128i __A, int __B)\n \n /* pshl */\n \n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_shl_epi8(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshlb ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_shl_epi16(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshlw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_shl_epi32(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshld ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_shl_epi64(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshlq ((__v2di)__A, (__v2di)__B);\n }\n \n /* psha */\n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_sha_epi8(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshab ((__v16qi)__A, (__v16qi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_sha_epi16(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshaw ((__v8hi)__A, (__v8hi)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_sha_epi32(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshad ((__v4si)__A, (__v4si)__B);\n }\n \n-static __inline __m128i __attribute__((__always_inline__)) \n+static __inline __m128i __attribute__((__always_inline__))\n _mm_sha_epi64(__m128i __A,  __m128i __B)\n {\n   return  (__m128i) __builtin_ia32_pshaq ((__v2di)__A, (__v2di)__B);\n@@ -465,14 +465,14 @@ _mm_comneq_ps(__m128 __A, __m128 __B)\n   return (__m128) __builtin_ia32_comuneqps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comnlt_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comunltps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n-_mm_comnle_ps(__m128 __A, __m128 __B) \n+static __inline __m128 __attribute__((__always_inline__))\n+_mm_comnle_ps(__m128 __A, __m128 __B)\n {\n   return (__m128)  __builtin_ia32_comunleps ((__v4sf)__A, (__v4sf)__B);\n }\n@@ -491,43 +491,43 @@ _mm_comueq_ps(__m128 __A, __m128 __B)\n   return (__m128) __builtin_ia32_comueqps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comnge_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comungeps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comngt_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comungtps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comfalse_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comfalseps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comoneq_ps(__m128 __A, __m128 __B)\n {\n-  return (__m128) __builtin_ia32_comneqps ((__v4sf)__A, (__v4sf)__B); \n+  return (__m128) __builtin_ia32_comneqps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comge_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comgeps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comgt_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comgtps ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comtrue_ps(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comtrueps ((__v4sf)__A, (__v4sf)__B);\n@@ -565,14 +565,14 @@ _mm_comneq_pd(__m128d __A, __m128d __B)\n   return (__m128d) __builtin_ia32_comuneqpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n+static __inline __m128d __attribute__((__always_inline__))\n _mm_comnlt_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comunltpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n-_mm_comnle_pd(__m128d __A, __m128d __B) \n+static __inline __m128d __attribute__((__always_inline__))\n+_mm_comnle_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comunlepd ((__v2df)__A, (__v2df)__B);\n }\n@@ -590,19 +590,19 @@ _mm_comueq_pd(__m128d __A, __m128d __B)\n   return (__m128d) __builtin_ia32_comueqpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n+static __inline __m128d __attribute__((__always_inline__))\n _mm_comnge_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comungepd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n+static __inline __m128d __attribute__((__always_inline__))\n _mm_comngt_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comungtpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n+static __inline __m128d __attribute__((__always_inline__))\n _mm_comfalse_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comfalsepd ((__v2df)__A, (__v2df)__B);\n@@ -614,19 +614,19 @@ _mm_comoneq_pd(__m128d __A, __m128d __B)\n   return (__m128d) __builtin_ia32_comneqpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n+static __inline __m128d __attribute__((__always_inline__))\n _mm_comge_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comgepd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n+static __inline __m128d __attribute__((__always_inline__))\n _mm_comgt_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comgtpd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n+static __inline __m128d __attribute__((__always_inline__))\n _mm_comtrue_pd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comtruepd ((__v2df)__A, (__v2df)__B);\n@@ -663,14 +663,14 @@ _mm_comneq_ss(__m128 __A, __m128 __B)\n   return (__m128) __builtin_ia32_comuneqss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comnlt_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comunltss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n-_mm_comnle_ss(__m128 __A, __m128 __B) \n+static __inline __m128 __attribute__((__always_inline__))\n+_mm_comnle_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comunless ((__v4sf)__A, (__v4sf)__B);\n }\n@@ -688,43 +688,43 @@ _mm_comueq_ss(__m128 __A, __m128 __B)\n   return (__m128) __builtin_ia32_comueqss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comnge_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comungess ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comngt_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comungtss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comfalse_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comfalsess ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comoneq_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comneqss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comge_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comgess ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comgt_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comgtss ((__v4sf)__A, (__v4sf)__B);\n }\n \n-static __inline __m128 __attribute__((__always_inline__)) \n+static __inline __m128 __attribute__((__always_inline__))\n _mm_comtrue_ss(__m128 __A, __m128 __B)\n {\n   return (__m128) __builtin_ia32_comtruess ((__v4sf)__A, (__v4sf)__B);\n@@ -762,14 +762,14 @@ _mm_comneq_sd(__m128d __A, __m128d __B)\n   return (__m128d) __builtin_ia32_comuneqsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n+static __inline __m128d __attribute__((__always_inline__))\n _mm_comnlt_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comunltsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n-_mm_comnle_sd(__m128d __A, __m128d __B) \n+static __inline __m128d __attribute__((__always_inline__))\n+_mm_comnle_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comunlesd ((__v2df)__A, (__v2df)__B);\n }\n@@ -787,19 +787,19 @@ _mm_comueq_sd(__m128d __A, __m128d __B)\n   return (__m128d) __builtin_ia32_comueqsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n+static __inline __m128d __attribute__((__always_inline__))\n _mm_comnge_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comungesd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n+static __inline __m128d __attribute__((__always_inline__))\n _mm_comngt_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comungtsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n+static __inline __m128d __attribute__((__always_inline__))\n _mm_comfalse_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comfalsesd ((__v2df)__A, (__v2df)__B);\n@@ -811,19 +811,19 @@ _mm_comoneq_sd(__m128d __A, __m128d __B)\n   return (__m128d) __builtin_ia32_comneqsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n+static __inline __m128d __attribute__((__always_inline__))\n _mm_comge_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comgesd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n+static __inline __m128d __attribute__((__always_inline__))\n _mm_comgt_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comgtsd ((__v2df)__A, (__v2df)__B);\n }\n \n-static __inline __m128d __attribute__((__always_inline__)) \n+static __inline __m128d __attribute__((__always_inline__))\n _mm_comtrue_sd(__m128d __A, __m128d __B)\n {\n   return (__m128d) __builtin_ia32_comtruesd ((__v2df)__A, (__v2df)__B);\n@@ -836,399 +836,399 @@ static __inline __m128i __attribute__((__always_inline__))\n _mm_comlt_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltub ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comle_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomleub ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comgt_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtub ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comge_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgeub ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comeq_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomequb ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comneq_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomnequb ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comfalse_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalseub ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comtrue_epu8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtrueub ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n /*pcom (integer, unsinged words) */\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comlt_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltuw ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comle_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomleuw ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comgt_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtuw ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comge_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgeuw ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comeq_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomequw ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comneq_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomnequw ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comfalse_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalseuw ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comtrue_epu16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtrueuw ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n /*pcom (integer, unsinged double words) */\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comlt_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltud ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comle_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomleud ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comgt_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtud ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comge_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgeud ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comeq_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomequd ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comneq_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomnequd ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comfalse_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalseud ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comtrue_epu32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtrueud ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n /*pcom (integer, unsinged quad words) */\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comlt_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltuq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comle_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomleuq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comgt_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtuq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comge_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgeuq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comeq_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomequq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comneq_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomnequq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comfalse_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalseuq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comtrue_epu64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtrueuq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n /*pcom (integer, signed bytes) */\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comlt_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltb ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comle_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomleb ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comgt_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtb ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comge_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgeb ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comeq_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomeqb ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comneq_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomneqb ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comfalse_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalseb ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comtrue_epi8(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtrueb ((__v16qi)__A, (__v16qi)__B);\n-} \n+}\n \n /*pcom (integer, signed words) */\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comlt_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltw ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comle_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomlew ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comgt_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtw ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comge_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgew ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comeq_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomeqw ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comneq_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomneqw ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comfalse_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalsew ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comtrue_epi16(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtruew ((__v8hi)__A, (__v8hi)__B);\n-} \n+}\n \n /*pcom (integer, signed double words) */\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comlt_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltd ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comle_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomled ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comgt_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtd ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comge_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomged ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comeq_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomeqd ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comneq_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomneqd ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comfalse_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalsed ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comtrue_epi32(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtrued ((__v4si)__A, (__v4si)__B);\n-} \n+}\n \n /*pcom (integer, signed quad words) */\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comlt_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomltq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comle_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomleq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comgt_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgtq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comge_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomgeq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comeq_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomeqq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comneq_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomneqq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comfalse_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomfalseq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n static __inline __m128i __attribute__((__always_inline__))\n _mm_comtrue_epi64(__m128i __A, __m128i __B)\n {\n   return (__m128i) __builtin_ia32_pcomtrueq ((__v2di)__A, (__v2di)__B);\n-} \n+}\n \n /* FRCZ */\n static __inline __m128 __attribute__((__always_inline__))"}, {"sha": "b9bec56e9af9441d06d379910e3807cd2b967b27", "filename": "gcc/config/i386/constraints.md", "status": "modified", "additions": 2, "deletions": 2, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fconstraints.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fconstraints.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fconstraints.md?ref=4f3f76e6c39bbd7924032dca7ed5de404f247e3b", "patch": "@@ -18,7 +18,7 @@\n ;; <http://www.gnu.org/licenses/>.\n \n ;;; Unused letters:\n-;;;     B     H           TU W   \n+;;;     B     H           TU W\n ;;;           h jk          vw  z\n \n ;; Integer register constraints.\n@@ -129,7 +129,7 @@\n        (match_test \"IN_RANGE (ival, 0, 3)\")))\n \n (define_constraint \"N\"\n-  \"Unsigned 8-bit integer constant (for @code{in} and @code{out} \n+  \"Unsigned 8-bit integer constant (for @code{in} and @code{out}\n    instructions).\"\n   (and (match_code \"const_int\")\n        (match_test \"IN_RANGE (ival, 0, 255)\")))"}, {"sha": "7bd5cb6a719a09a03210a08bfd01d530be5f4017", "filename": "gcc/config/i386/i386.c", "status": "modified", "additions": 9, "deletions": 9, "changes": 18, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fi386.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fi386.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.c?ref=4f3f76e6c39bbd7924032dca7ed5de404f247e3b", "patch": "@@ -2011,7 +2011,7 @@ override_options (void)\n       {\"pentium4\", PROCESSOR_PENTIUM4, PTA_MMX |PTA_SSE | PTA_SSE2},\n       {\"pentium4m\", PROCESSOR_PENTIUM4, PTA_MMX | PTA_SSE | PTA_SSE2},\n       {\"prescott\", PROCESSOR_NOCONA, PTA_MMX | PTA_SSE | PTA_SSE2 | PTA_SSE3},\n-      {\"nocona\", PROCESSOR_NOCONA, (PTA_64BIT \n+      {\"nocona\", PROCESSOR_NOCONA, (PTA_64BIT\n \t\t\t\t    | PTA_MMX | PTA_SSE | PTA_SSE2 | PTA_SSE3\n \t\t\t\t    | PTA_CX16 | PTA_NO_SAHF)},\n       {\"core2\", PROCESSOR_CORE2, (PTA_64BIT\n@@ -2181,7 +2181,7 @@ override_options (void)\n     ix86_arch_string = TARGET_64BIT ? \"x86-64\" : \"i386\";\n   else\n     ix86_arch_specified = 1;\n-  \n+\n   if (!strcmp (ix86_arch_string, \"generic\"))\n     error (\"generic CPU can be used only for -mtune= switch\");\n   if (!strncmp (ix86_arch_string, \"generic\", 7))\n@@ -4558,7 +4558,7 @@ function_value_32 (enum machine_mode orig_mode, enum machine_mode mode,\n   else\n     /* Most things go in %eax.  */\n     regno = 0;\n-  \n+\n   /* Override FP return register with %xmm0 for local functions when\n      SSE math is enabled or for functions with sseregparm attribute.  */\n   if ((fn || fntype) && (mode == SFmode || mode == DFmode))\n@@ -4746,7 +4746,7 @@ ix86_return_in_memory (const_tree type)\n    but differs notably in that when MMX is available, 8-byte vectors\n    are returned in memory, rather than in MMX registers.  */\n \n-int \n+int\n ix86_sol10_return_in_memory (const_tree type)\n {\n   int size;\n@@ -4889,7 +4889,7 @@ setup_incoming_varargs_64 (CUMULATIVE_ARGS *cum)\n   ix86_save_varrargs_registers = 1;\n   /* We need 16-byte stack alignment to save SSE registers.  If user\n      asked for lower preferred_stack_boundary, lets just hope that he knows\n-     what he is doing and won't varargs SSE values.  \n+     what he is doing and won't varargs SSE values.\n \n      We also may end up assuming that only 64bit values are stored in SSE\n      register let some floating point program work.  */\n@@ -19794,7 +19794,7 @@ ix86_expand_multi_arg_builtin (enum insn_code icode, tree exp, rtx target,\n \n \t  gcc_assert (GET_MODE (op) == mode || GET_MODE (op) == VOIDmode);\n \n-\t  if (optimize \n+\t  if (optimize\n \t      || ! (*insn_data[icode].operand[i+adjust+1].predicate) (op, mode)\n \t      || num_memory > 1)\n \t    op = force_reg (mode, op);\n@@ -21295,7 +21295,7 @@ ix86_veclibabi_acml (enum built_in_function fn, tree type_out, tree type_in)\n \t  || n != 4)\n \treturn NULL_TREE;\n       break;\n-    \n+\n     default:\n       return NULL_TREE;\n     }\n@@ -24944,7 +24944,7 @@ bool ix86_sse5_valid_op_p (rtx operands[], rtx insn, int num, bool uses_oc0, int\n \n       /* format, example pmacsdd:\n \t xmm1, xmm2, xmm3/mem, xmm1\n-      \n+\n          For the integer multiply/add instructions be more restrictive and\n          require operands[2] and operands[3] to be the memory operands.  */\n       else\n@@ -25044,7 +25044,7 @@ static const struct attribute_spec ix86_attribute_table[] =\n };\n \n /* Implement targetm.vectorize.builtin_vectorization_cost.  */\n-static int \n+static int\n x86_builtin_vectorization_cost (bool runtime_test)\n {\n   /* If the branch of the runtime test is taken - i.e. - the vectorized"}, {"sha": "bc3e5f6dc9d35dccb1c6905590dcab49240aab3c", "filename": "gcc/config/i386/i386.h", "status": "modified", "additions": 5, "deletions": 5, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fi386.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fi386.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.h?ref=4f3f76e6c39bbd7924032dca7ed5de404f247e3b", "patch": "@@ -75,9 +75,9 @@ enum stringop_alg\n    When size is unknown, the UNKNOWN_SIZE alg is used.  When size is\n    known at compile time or estimated via feedback, the SIZE array\n    is walked in order until MAX is greater then the estimate (or -1\n-   means infinity).  Corresponding ALG is used then.  \n+   means infinity).  Corresponding ALG is used then.\n    For example initializer:\n-    {{256, loop}, {-1, rep_prefix_4_byte}}\t\t\n+    {{256, loop}, {-1, rep_prefix_4_byte}}\n    will use loop for blocks smaller or equal to 256 bytes, rep prefix will\n    be used otherwise.  */\n struct stringop_algs\n@@ -153,7 +153,7 @@ struct processor_costs {\n                                    scalar-to-vector operation.  */\n   const int vec_to_scalar_cost;    /* Cost of vect-to-scalar operation.  */\n   const int scalar_to_vec_cost;    /* Cost of scalar-to-vector operation.  */\n-  const int vec_align_load_cost;   /* Cost of aligned vector load.  */ \n+  const int vec_align_load_cost;   /* Cost of aligned vector load.  */\n   const int vec_unalign_load_cost; /* Cost of unaligned vector load.  */\n   const int vec_store_cost;        /* Cost of vector store.  */\n   const int cond_taken_branch_cost;    /* Cost of taken branch for vectorizer\n@@ -375,7 +375,7 @@ enum ix86_arch_indices {\n \n   X86_ARCH_LAST\n };\n-  \n+\n extern unsigned int ix86_arch_features[X86_ARCH_LAST];\n \n #define TARGET_CMOVE\t\tix86_arch_features[X86_ARCH_CMOVE]\n@@ -2507,7 +2507,7 @@ struct machine_function GTY(())\n #define TARG_SCALAR_STORE_COST          ix86_cost->scalar_store_cost\n \n /* Cost of any vector operation, excluding load, store or vector to scalar\n-   operation.  */ \n+   operation.  */\n #undef TARG_VEC_STMT_COST\n #define TARG_VEC_STMT_COST              ix86_cost->vec_stmt_cost\n "}, {"sha": "89ae741d694bf4101bbf28249bb809383b797109", "filename": "gcc/config/i386/i386.md", "status": "modified", "additions": 31, "deletions": 31, "changes": 62, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fi386.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fi386.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fi386.md?ref=4f3f76e6c39bbd7924032dca7ed5de404f247e3b", "patch": "@@ -158,7 +158,7 @@\n \n    ; For SSE4A support\n    (UNSPEC_EXTRQI               130)\n-   (UNSPEC_EXTRQ                131)   \n+   (UNSPEC_EXTRQ                131)\n    (UNSPEC_INSERTQI             132)\n    (UNSPEC_INSERTQ              133)\n \n@@ -1166,7 +1166,7 @@\n    (set_attr \"mode\" \"SI\")])\n \n ;; Pentium Pro can do steps 1 through 3 in one go.\n-;; comi*, ucomi*, fcomi*, ficomi*,fucomi* (i387 instructions set condition codes) \n+;; comi*, ucomi*, fcomi*, ficomi*,fucomi* (i387 instructions set condition codes)\n (define_insn \"*cmpfp_i_mixed\"\n   [(set (reg:CCFP FLAGS_REG)\n \t(compare:CCFP (match_operand 0 \"register_operand\" \"f,x\")\n@@ -1476,7 +1476,7 @@\n    (set_attr \"mode\" \"SI\")\n    (set_attr \"pent_pair\" \"np\")\n    (set_attr \"athlon_decode\" \"vector\")\n-   (set_attr \"amdfam10_decode\" \"double\")])   \n+   (set_attr \"amdfam10_decode\" \"double\")])\n \n (define_expand \"movhi\"\n   [(set (match_operand:HI 0 \"nonimmediate_operand\" \"\")\n@@ -1594,7 +1594,7 @@\n    (set_attr \"mode\" \"SI\")\n    (set_attr \"pent_pair\" \"np\")\n    (set_attr \"athlon_decode\" \"vector\")\n-   (set_attr \"amdfam10_decode\" \"double\")])   \n+   (set_attr \"amdfam10_decode\" \"double\")])\n \n ;; Not added amdfam10_decode since TARGET_PARTIAL_REG_STALL is disabled for AMDFAM10\n (define_insn \"*swaphi_2\"\n@@ -1770,7 +1770,7 @@\n    (set_attr \"mode\" \"SI\")\n    (set_attr \"pent_pair\" \"np\")\n    (set_attr \"athlon_decode\" \"vector\")\n-   (set_attr \"amdfam10_decode\" \"vector\")])   \n+   (set_attr \"amdfam10_decode\" \"vector\")])\n \n ;; Not added amdfam10_decode since TARGET_PARTIAL_REG_STALL is disabled for AMDFAM10\n (define_insn \"*swapqi_2\"\n@@ -2343,7 +2343,7 @@\n    (set_attr \"mode\" \"DI\")\n    (set_attr \"pent_pair\" \"np\")\n    (set_attr \"athlon_decode\" \"vector\")\n-   (set_attr \"amdfam10_decode\" \"double\")])   \n+   (set_attr \"amdfam10_decode\" \"double\")])\n \n (define_expand \"movti\"\n   [(set (match_operand:TI 0 \"nonimmediate_operand\" \"\")\n@@ -3950,7 +3950,7 @@\n   [(set (match_operand:DF 0 \"register_operand\" \"\")\n         (float_extend:DF\n \t  (match_operand:SF 1 \"nonimmediate_operand\" \"\")))]\n-  \"(TARGET_USE_VECTOR_CONVERTS || TARGET_GENERIC) && !optimize_size \n+  \"(TARGET_USE_VECTOR_CONVERTS || TARGET_GENERIC) && !optimize_size\n    && reload_completed && SSE_REG_P (operands[0])\"\n    [(set (match_dup 2)\n \t (float_extend:V2DF\n@@ -3978,7 +3978,7 @@\n       emit_insn (gen_sse_unpcklps (operands[3], operands[3], operands[3]));\n     }\n   else\n-    emit_insn (gen_vec_setv4sf_0 (operands[3], \n+    emit_insn (gen_vec_setv4sf_0 (operands[3],\n \t\t\t\t  CONST0_RTX (V4SFmode), operands[1]));\n })\n \n@@ -4086,7 +4086,7 @@\n   [(set (match_operand:SF 0 \"register_operand\" \"\")\n         (float_truncate:SF\n \t  (match_operand:DF 1 \"nonimmediate_operand\" \"\")))]\n-  \"(TARGET_USE_VECTOR_CONVERTS || TARGET_GENERIC) && !optimize_size \n+  \"(TARGET_USE_VECTOR_CONVERTS || TARGET_GENERIC) && !optimize_size\n    && reload_completed && SSE_REG_P (operands[0])\"\n    [(set (match_dup 2)\n \t (vec_concat:V4SF\n@@ -4756,7 +4756,7 @@\n    (set_attr \"mode\" \"HI\")\n    (set_attr \"unit\" \"i387\")\n    (set_attr \"athlon_decode\" \"vector\")\n-   (set_attr \"amdfam10_decode\" \"vector\")])   \n+   (set_attr \"amdfam10_decode\" \"vector\")])\n \f\n ;; Conversion between fixed point and floating point.\n \n@@ -4851,7 +4851,7 @@\n (define_insn \"*floatsisf2_mixed_vector\"\n   [(set (match_operand:SF 0 \"register_operand\" \"=x,f,?f\")\n \t(float:SF (match_operand:SI 1 \"nonimmediate_operand\" \"x,m,r\")))]\n-  \"TARGET_MIX_SSE_I387 && !flag_trapping_math \n+  \"TARGET_MIX_SSE_I387 && !flag_trapping_math\n    && TARGET_USE_VECTOR_CONVERTS && !optimize_size\"\n   \"@\n    cvtdq2ps\\t{%1, %0|%0, %1}\n@@ -4914,7 +4914,7 @@\n   \"#\"\n   [(set_attr \"type\" \"multi\")])\n \n-(define_split \n+(define_split\n   [(set (match_operand:SF 0 \"register_operand\" \"\")\n \t(float:SF (match_operand:SI 1 \"nonimmediate_operand\" \"\")))]\n   \"flag_trapping_math\n@@ -4929,7 +4929,7 @@\n   emit_insn (gen_sse2_loadld (operands[2], CONST0_RTX (V4SImode), operands[1]));\n })\n \n-(define_split \n+(define_split\n   [(set (match_operand:SF 0 \"register_operand\" \"\")\n \t(float:SF (match_operand:SI 1 \"register_operand\" \"\")))]\n   \"flag_trapping_math\n@@ -5042,7 +5042,7 @@\n    (set_attr \"amdfam10_decode\" \"double\")\n    (set_attr \"fp_int_src\" \"true\")])\n \n-(define_split \n+(define_split\n   [(set (match_operand:DF 0 \"register_operand\" \"\")\n \t(float:DF (match_operand:SI 1 \"memory_operand\" \"\")))]\n   \"TARGET_USE_VECTOR_CONVERTS && reload_completed\n@@ -7483,11 +7483,11 @@\n   \"TARGET_64BIT\"\n   \"\")\n \n-;; On AMDFAM10 \n+;; On AMDFAM10\n ;; IMUL reg64, reg64, imm8 \tDirect\n ;; IMUL reg64, mem64, imm8 \tVectorPath\n ;; IMUL reg64, reg64, imm32 \tDirect\n-;; IMUL reg64, mem64, imm32 \tVectorPath \n+;; IMUL reg64, mem64, imm32 \tVectorPath\n ;; IMUL reg64, reg64 \t\tDirect\n ;; IMUL reg64, mem64 \t\tDirect\n \n@@ -7517,7 +7517,7 @@\n \t(cond [(and (eq_attr \"alternative\" \"0,1\")\n \t\t    (match_operand 1 \"memory_operand\" \"\"))\n \t\t  (const_string \"vector\")]\n-\t      (const_string \"direct\")))\t      \n+\t      (const_string \"direct\")))\n    (set_attr \"mode\" \"DI\")])\n \n (define_expand \"mulsi3\"\n@@ -7528,7 +7528,7 @@\n   \"\"\n   \"\")\n \n-;; On AMDFAM10 \n+;; On AMDFAM10\n ;; IMUL reg32, reg32, imm8 \tDirect\n ;; IMUL reg32, mem32, imm8 \tVectorPath\n ;; IMUL reg32, reg32, imm32 \tDirect\n@@ -7561,7 +7561,7 @@\n \t(cond [(and (eq_attr \"alternative\" \"0,1\")\n \t\t    (match_operand 1 \"memory_operand\" \"\"))\n \t\t  (const_string \"vector\")]\n-\t      (const_string \"direct\")))\t      \n+\t      (const_string \"direct\")))\n    (set_attr \"mode\" \"SI\")])\n \n (define_insn \"*mulsi3_1_zext\"\n@@ -7591,7 +7591,7 @@\n \t(cond [(and (eq_attr \"alternative\" \"0,1\")\n \t\t    (match_operand 1 \"memory_operand\" \"\"))\n \t\t  (const_string \"vector\")]\n-\t      (const_string \"direct\")))\t      \n+\t      (const_string \"direct\")))\n    (set_attr \"mode\" \"SI\")])\n \n (define_expand \"mulhi3\"\n@@ -7659,7 +7659,7 @@\n      (if_then_else (eq_attr \"cpu\" \"athlon\")\n         (const_string \"vector\")\n         (const_string \"direct\")))\n-   (set_attr \"amdfam10_decode\" \"direct\")        \n+   (set_attr \"amdfam10_decode\" \"direct\")\n    (set_attr \"mode\" \"QI\")])\n \n (define_expand \"umulqihi3\"\n@@ -7711,7 +7711,7 @@\n      (if_then_else (eq_attr \"cpu\" \"athlon\")\n         (const_string \"vector\")\n         (const_string \"direct\")))\n-   (set_attr \"amdfam10_decode\" \"direct\")        \n+   (set_attr \"amdfam10_decode\" \"direct\")\n    (set_attr \"mode\" \"QI\")])\n \n (define_expand \"umulditi3\"\n@@ -7738,7 +7738,7 @@\n      (if_then_else (eq_attr \"cpu\" \"athlon\")\n         (const_string \"vector\")\n         (const_string \"double\")))\n-   (set_attr \"amdfam10_decode\" \"double\")        \n+   (set_attr \"amdfam10_decode\" \"double\")\n    (set_attr \"mode\" \"DI\")])\n \n ;; We can't use this pattern in 64bit mode, since it results in two separate 32bit registers\n@@ -7766,7 +7766,7 @@\n      (if_then_else (eq_attr \"cpu\" \"athlon\")\n         (const_string \"vector\")\n         (const_string \"double\")))\n-   (set_attr \"amdfam10_decode\" \"double\")        \n+   (set_attr \"amdfam10_decode\" \"double\")\n    (set_attr \"mode\" \"SI\")])\n \n (define_expand \"mulditi3\"\n@@ -7820,7 +7820,7 @@\n      (if_then_else (eq_attr \"cpu\" \"athlon\")\n         (const_string \"vector\")\n         (const_string \"double\")))\n-   (set_attr \"amdfam10_decode\" \"double\")        \n+   (set_attr \"amdfam10_decode\" \"double\")\n    (set_attr \"mode\" \"SI\")])\n \n (define_expand \"umuldi3_highpart\"\n@@ -7857,7 +7857,7 @@\n      (if_then_else (eq_attr \"cpu\" \"athlon\")\n         (const_string \"vector\")\n         (const_string \"double\")))\n-   (set_attr \"amdfam10_decode\" \"double\")        \n+   (set_attr \"amdfam10_decode\" \"double\")\n    (set_attr \"mode\" \"DI\")])\n \n (define_expand \"umulsi3_highpart\"\n@@ -10991,7 +10991,7 @@\n    (set_attr \"prefix_0f\" \"1\")\n    (set_attr \"mode\" \"DI\")\n    (set_attr \"athlon_decode\" \"vector\")\n-   (set_attr \"amdfam10_decode\" \"vector\")])   \n+   (set_attr \"amdfam10_decode\" \"vector\")])\n \n (define_expand \"x86_64_shift_adj\"\n   [(set (reg:CCZ FLAGS_REG)\n@@ -11209,7 +11209,7 @@\n    (set_attr \"mode\" \"SI\")\n    (set_attr \"pent_pair\" \"np\")\n    (set_attr \"athlon_decode\" \"vector\")\n-   (set_attr \"amdfam10_decode\" \"vector\")])   \n+   (set_attr \"amdfam10_decode\" \"vector\")])\n \n (define_expand \"x86_shift_adj_1\"\n   [(set (reg:CCZ FLAGS_REG)\n@@ -11971,7 +11971,7 @@\n    (set_attr \"prefix_0f\" \"1\")\n    (set_attr \"mode\" \"DI\")\n    (set_attr \"athlon_decode\" \"vector\")\n-   (set_attr \"amdfam10_decode\" \"vector\")])   \n+   (set_attr \"amdfam10_decode\" \"vector\")])\n \n (define_expand \"ashrdi3\"\n   [(set (match_operand:DI 0 \"shiftdi_operand\" \"\")\n@@ -16606,7 +16606,7 @@\n   \"fsqrt\"\n   [(set_attr \"type\" \"fpspc\")\n    (set_attr \"mode\" \"XF\")\n-   (set_attr \"athlon_decode\" \"direct\")   \n+   (set_attr \"athlon_decode\" \"direct\")\n    (set_attr \"amdfam10_decode\" \"direct\")])\n \n (define_insn \"*rsqrtsf2_sse\"\n@@ -19734,7 +19734,7 @@\n ;; SSE5 conditional move\n (define_insn \"*sse5_pcmov_<mode>\"\n   [(set (match_operand:MODEF 0 \"register_operand\" \"=x,x,x,x\")\n-\t(if_then_else:MODEF \n+\t(if_then_else:MODEF\n \t  (match_operand:MODEF 1 \"nonimmediate_operand\" \"xm,x,0,0\")\n \t  (match_operand:MODEF 2 \"nonimmediate_operand\" \"0,0,x,xm\")\n \t  (match_operand:MODEF 3 \"vector_move_operand\" \"x,xm,xm,x\")))]"}, {"sha": "20c4fb05b575637645aef3653bee3922dafc8bcb", "filename": "gcc/config/i386/mmx.md", "status": "modified", "additions": 3, "deletions": 3, "changes": 6, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fmmx.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fmmx.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fmmx.md?ref=4f3f76e6c39bbd7924032dca7ed5de404f247e3b", "patch": "@@ -22,7 +22,7 @@\n ;; the same register file, and 3dNOW! adds a number of extensions to\n ;; the base integer MMX isa.\n \n-;; Note!  Except for the basic move instructions, *all* of these \n+;; Note!  Except for the basic move instructions, *all* of these\n ;; patterns are outside the normal optabs namespace.  This is because\n ;; use of these registers requires the insertion of emms or femms\n ;; instructions to return to normal fpu mode.  The compiler doesn't\n@@ -301,7 +301,7 @@\n   \"pfrsqrt\\\\t{%1, %0|%0, %1}\"\n   [(set_attr \"type\" \"mmx\")\n    (set_attr \"mode\" \"V2SF\")])\n-\t\t\n+\n (define_insn \"mmx_rsqit1v2sf3\"\n   [(set (match_operand:V2SF 0 \"register_operand\" \"=y\")\n \t(unspec:V2SF [(match_operand:V2SF 1 \"register_operand\" \"0\")\n@@ -1429,4 +1429,4 @@\n   \"TARGET_3DNOW\"\n   \"femms\"\n   [(set_attr \"type\" \"mmx\")\n-   (set_attr \"memory\" \"none\")]) \n+   (set_attr \"memory\" \"none\")])"}, {"sha": "b971e51089e68c38a62539fce088cf5a7198940b", "filename": "gcc/config/i386/ppro.md", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fppro.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fppro.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fppro.md?ref=4f3f76e6c39bbd7924032dca7ed5de404f247e3b", "patch": "@@ -78,7 +78,7 @@\n ;;    but still in only one cycle.\n ;;  - a complex (microcode) instruction can also only be decoded by\n ;;    decoder 0, and this takes an unspecified number of cycles.\n-;;    \n+;;\n ;; The goal is to schedule such that we have a few-one-one uops sequence\n ;; in each cycle, to decode as many instructions per cycle as possible.\n (define_cpu_unit \"decoder0\" \"ppro_decoder\")"}, {"sha": "a810ad68e5c255c8ea7f08607a21f07fd0317f2c", "filename": "gcc/config/i386/sse.md", "status": "modified", "additions": 18, "deletions": 18, "changes": 36, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fsse.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fsse.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fsse.md?ref=4f3f76e6c39bbd7924032dca7ed5de404f247e3b", "patch": "@@ -602,7 +602,7 @@\n    (set_attr \"mode\" \"SF\")])\n \n ;; ??? For !flag_finite_math_only, the representation with SMIN/SMAX\n-;; isn't really correct, as those rtl operators aren't defined when \n+;; isn't really correct, as those rtl operators aren't defined when\n ;; applied to NaNs.  Hopefully the optimizers won't get too smart on us.\n \n (define_expand \"smaxv4sf3\"\n@@ -754,7 +754,7 @@\n \t(vec_concat:V4SF\n \t  (vec_concat:V2SF\n \t    (plus:SF\n-\t      (vec_select:SF \n+\t      (vec_select:SF\n \t\t(match_operand:V4SF 1 \"register_operand\" \"0\")\n \t\t(parallel [(const_int 0)]))\n \t      (vec_select:SF (match_dup 1) (parallel [(const_int 1)])))\n@@ -781,7 +781,7 @@\n \t(vec_concat:V4SF\n \t  (vec_concat:V2SF\n \t    (minus:SF\n-\t      (vec_select:SF \n+\t      (vec_select:SF\n \t\t(match_operand:V4SF 1 \"register_operand\" \"0\")\n \t\t(parallel [(const_int 0)]))\n \t      (vec_select:SF (match_dup 1) (parallel [(const_int 1)])))\n@@ -2290,7 +2290,7 @@\n    (set_attr \"mode\" \"DF\")])\n \n ;; ??? For !flag_finite_math_only, the representation with SMIN/SMAX\n-;; isn't really correct, as those rtl operators aren't defined when \n+;; isn't really correct, as those rtl operators aren't defined when\n ;; applied to NaNs.  Hopefully the optimizers won't get too smart on us.\n \n (define_expand \"smaxv2df3\"\n@@ -3527,7 +3527,7 @@\n   emit_insn (gen_sse2_punpcklbw (t[3], operands[2], operands[2]));\n \n   /* Multiply words.  The end-of-line annotations here give a picture of what\n-     the output of that instruction looks like.  Dot means don't care; the \n+     the output of that instruction looks like.  Dot means don't care; the\n      letters are the bytes of the result with A being the most significant.  */\n   emit_insn (gen_mulv8hi3 (gen_lowpart (V8HImode, t[4]), /* .A.B.C.D.E.F.G.H */\n \t\t\t   gen_lowpart (V8HImode, t[0]),\n@@ -3569,16 +3569,16 @@\n (define_expand \"smulv8hi3_highpart\"\n   [(set (match_operand:V8HI 0 \"register_operand\" \"\")\n         (truncate:V8HI\n-          (lshiftrt:V8SI \n-            (mult:V8SI \n+          (lshiftrt:V8SI\n+            (mult:V8SI\n               (sign_extend:V8SI\n                 (match_operand:V8HI 1 \"nonimmediate_operand\" \"\"))\n               (sign_extend:V8SI\n                 (match_operand:V8HI 2 \"nonimmediate_operand\" \"\")))\n             (const_int 16))))]\n   \"TARGET_SSE2\"\n   \"ix86_fixup_binary_operands_no_copy (MULT, V8HImode, operands);\")\n-  \n+\n (define_insn \"*smulv8hi3_highpart\"\n   [(set (match_operand:V8HI 0 \"register_operand\" \"=x\")\n \t(truncate:V8HI\n@@ -3776,7 +3776,7 @@\n \t\t\t       thirtytwo));\n   emit_insn (gen_sse2_lshrti3 (gen_lowpart (TImode, t3),\n \t\t\t       gen_lowpart (TImode, op2),\n-\t\t\t       thirtytwo)); \n+\t\t\t       thirtytwo));\n   /* Multiply elements 3 and 1.  */\n   emit_insn (gen_sse2_umulv2siv2di3 (gen_lowpart (V2DImode, t4),\n \t\t\t\t     t2, t3));\n@@ -4011,8 +4011,8 @@\n })\n \n (define_expand \"udot_prodv4si\"\n-  [(match_operand:V2DI 0 \"register_operand\" \"\") \n-   (match_operand:V4SI 1 \"register_operand\" \"\") \n+  [(match_operand:V2DI 0 \"register_operand\" \"\")\n+   (match_operand:V4SI 1 \"register_operand\" \"\")\n    (match_operand:V4SI 2 \"register_operand\" \"\")\n    (match_operand:V2DI 3 \"register_operand\" \"\")]\n   \"TARGET_SSE2\"\n@@ -4875,7 +4875,7 @@\n \t\t     (const_int 9)  (const_int 25)\n \t\t     (const_int 10) (const_int 26)\n \t\t     (const_int 11) (const_int 27)\n-\t\t     (const_int 12) (const_int 28) \n+\t\t     (const_int 12) (const_int 28)\n \t\t     (const_int 13) (const_int 29)\n \t\t     (const_int 14) (const_int 30)\n \t\t     (const_int 15) (const_int 31)])))]\n@@ -5283,7 +5283,7 @@\n   \"TARGET_SSE\"\n   \"#\"\n   \"&& reload_completed\n-   && (TARGET_INTER_UNIT_MOVES \n+   && (TARGET_INTER_UNIT_MOVES\n        || MEM_P (operands [0])\n        || !GENERAL_REGNO_P (true_regnum (operands [0])))\"\n   [(set (match_dup 0) (match_dup 1))]\n@@ -5343,7 +5343,7 @@\n \t  (parallel [(const_int 0)])))]\n   \"TARGET_SSE\n    && reload_completed\n-   && (TARGET_INTER_UNIT_MOVES \n+   && (TARGET_INTER_UNIT_MOVES\n        || MEM_P (operands [0])\n        || !GENERAL_REGNO_P (true_regnum (operands [0])))\"\n   [(set (match_dup 0) (match_dup 1))]\n@@ -5839,7 +5839,7 @@\n    (set_attr \"prefix_data16\" \"1\")\n    (set_attr \"mode\" \"TI\")])\n \n-;; The correct representation for this is absolutely enormous, and \n+;; The correct representation for this is absolutely enormous, and\n ;; surely not generally useful.\n (define_insn \"sse2_psadbw\"\n   [(set (match_operand:V2DI 0 \"register_operand\" \"=x\")\n@@ -6641,7 +6641,7 @@\n \n (define_insn \"sse4a_vmmovntv2df\"\n   [(set (match_operand:DF 0 \"memory_operand\" \"=m\")\n-        (unspec:DF [(vec_select:DF \n+        (unspec:DF [(vec_select:DF\n                       (match_operand:V2DF 1 \"register_operand\" \"x\")\n                       (parallel [(const_int 0)]))]\n                    UNSPEC_MOVNT))]\n@@ -6661,7 +6661,7 @@\n \n (define_insn \"sse4a_vmmovntv4sf\"\n   [(set (match_operand:SF 0 \"memory_operand\" \"=m\")\n-\t(unspec:SF [(vec_select:SF \n+\t(unspec:SF [(vec_select:SF\n \t              (match_operand:V4SF 1 \"register_operand\" \"x\")\n \t\t      (parallel [(const_int 0)]))]\n \t\t   UNSPEC_MOVNT))]\n@@ -7893,7 +7893,7 @@\n ;; SSE5 parallel XMM conditional moves\n (define_insn \"sse5_pcmov_<mode>\"\n   [(set (match_operand:SSEMODE 0 \"register_operand\" \"=x,x,x,x,x,x\")\n-\t(if_then_else:SSEMODE \n+\t(if_then_else:SSEMODE\n \t  (match_operand:SSEMODE 3 \"nonimmediate_operand\" \"0,0,xm,xm,0,0\")\n \t  (match_operand:SSEMODE 1 \"vector_move_operand\" \"x,xm,0,x,C,x\")\n \t  (match_operand:SSEMODE 2 \"vector_move_operand\" \"xm,x,x,0,x,C\")))]"}, {"sha": "a4f6d38456c91e64454c1ad2bd4f69bc50ae65cb", "filename": "gcc/config/i386/sync.md", "status": "modified", "additions": 1, "deletions": 1, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fsync.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4f3f76e6c39bbd7924032dca7ed5de404f247e3b/gcc%2Fconfig%2Fi386%2Fsync.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fsync.md?ref=4f3f76e6c39bbd7924032dca7ed5de404f247e3b", "patch": "@@ -34,7 +34,7 @@\n ;; ??? It would be possible to use cmpxchg8b on pentium for DImode\n ;; changes.  It's complicated because the insn uses ecx:ebx as the\n ;; new value; note that the registers are reversed from the order\n-;; that they'd be in with (reg:DI 2 ecx).  Similarly for TImode \n+;; that they'd be in with (reg:DI 2 ecx).  Similarly for TImode\n ;; data in 64-bit mode.\n \n (define_expand \"sync_compare_and_swap<mode>\""}]}