{"sha": "c98aabc1427a4d2a25a2176c89dc709148a04707", "node_id": "C_kwDOANBUbNoAKGM5OGFhYmMxNDI3YTRkMmEyNWEyMTc2Yzg5ZGM3MDkxNDhhMDQ3MDc", "commit": {"author": {"name": "Tamar Christina", "email": "tamar.christina@arm.com", "date": "2022-11-14T15:52:26Z"}, "committer": {"name": "Tamar Christina", "email": "tamar.christina@arm.com", "date": "2022-11-14T17:41:33Z"}, "message": "AArch64: Add implementation for pow2 bitmask division.\n\nThis adds an implementation for the new optab for unsigned pow2 bitmask for\nAArch64.\n\nThe implementation rewrites:\n\n   x = y / (2 ^ (sizeof (y)/2)-1\n\ninto e.g. (for bytes)\n\n   (x + ((x + 257) >> 8)) >> 8\n\nwhere it's required that the additions be done in double the precision of x\nsuch that we don't lose any bits during an overflow.\n\nEssentially the sequence decomposes the division into doing two smaller\ndivisions, one for the top and bottom parts of the number and adding the results\nback together.\n\nTo account for the fact that shift by 8 would be division by 256 we add 1 to\nboth parts of x such that when 255 we still get 1 as the answer.\n\nBecause the amount we shift are half the original datatype we can use the\nhalfing instructions the ISA provides to do the operation instead of using\nactual shifts.\n\nFor AArch64 this means we generate for:\n\nvoid draw_bitmap1(uint8_t* restrict pixel, uint8_t level, int n)\n{\n  for (int i = 0; i < (n & -16); i+=1)\n    pixel[i] = (pixel[i] * level) / 0xff;\n}\n\nthe following:\n\n\tmovi    v3.16b, 0x1\n\tumull2  v1.8h, v0.16b, v2.16b\n\tumull   v0.8h, v0.8b, v2.8b\n\taddhn   v5.8b, v1.8h, v3.8h\n\taddhn   v4.8b, v0.8h, v3.8h\n\tuaddw   v1.8h, v1.8h, v5.8b\n\tuaddw   v0.8h, v0.8h, v4.8b\n\tuzp2    v0.16b, v0.16b, v1.16b\n\ninstead of:\n\n\tumull   v2.8h, v1.8b, v5.8b\n\tumull2  v1.8h, v1.16b, v5.16b\n\tumull   v0.4s, v2.4h, v3.4h\n\tumull2  v2.4s, v2.8h, v3.8h\n\tumull   v4.4s, v1.4h, v3.4h\n\tumull2  v1.4s, v1.8h, v3.8h\n\tuzp2    v0.8h, v0.8h, v2.8h\n\tuzp2    v1.8h, v4.8h, v1.8h\n\tshrn    v0.8b, v0.8h, 7\n\tshrn2   v0.16b, v1.8h, 7\n\nWhich results in significantly faster code.\n\nThanks for Wilco for the concept.\n\ngcc/ChangeLog:\n\n\t* config/aarch64/aarch64-simd.md (@aarch64_bitmask_udiv<mode>3): New.\n\t* config/aarch64/aarch64.cc (aarch64_vectorize_can_special_div_by_constant): New.\n\ngcc/testsuite/ChangeLog:\n\n\t* gcc.target/aarch64/div-by-bitmask.c: New test.", "tree": {"sha": "462bce69513c43ae37001eb7aac4708d04b19bd8", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/462bce69513c43ae37001eb7aac4708d04b19bd8"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/c98aabc1427a4d2a25a2176c89dc709148a04707", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c98aabc1427a4d2a25a2176c89dc709148a04707", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c98aabc1427a4d2a25a2176c89dc709148a04707", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c98aabc1427a4d2a25a2176c89dc709148a04707/comments", "author": {"login": "TamarChristinaArm", "id": 48126768, "node_id": "MDQ6VXNlcjQ4MTI2NzY4", "avatar_url": "https://avatars.githubusercontent.com/u/48126768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TamarChristinaArm", "html_url": "https://github.com/TamarChristinaArm", "followers_url": "https://api.github.com/users/TamarChristinaArm/followers", "following_url": "https://api.github.com/users/TamarChristinaArm/following{/other_user}", "gists_url": "https://api.github.com/users/TamarChristinaArm/gists{/gist_id}", "starred_url": "https://api.github.com/users/TamarChristinaArm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TamarChristinaArm/subscriptions", "organizations_url": "https://api.github.com/users/TamarChristinaArm/orgs", "repos_url": "https://api.github.com/users/TamarChristinaArm/repos", "events_url": "https://api.github.com/users/TamarChristinaArm/events{/privacy}", "received_events_url": "https://api.github.com/users/TamarChristinaArm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "TamarChristinaArm", "id": 48126768, "node_id": "MDQ6VXNlcjQ4MTI2NzY4", "avatar_url": "https://avatars.githubusercontent.com/u/48126768?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TamarChristinaArm", "html_url": "https://github.com/TamarChristinaArm", "followers_url": "https://api.github.com/users/TamarChristinaArm/followers", "following_url": "https://api.github.com/users/TamarChristinaArm/following{/other_user}", "gists_url": "https://api.github.com/users/TamarChristinaArm/gists{/gist_id}", "starred_url": "https://api.github.com/users/TamarChristinaArm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TamarChristinaArm/subscriptions", "organizations_url": "https://api.github.com/users/TamarChristinaArm/orgs", "repos_url": "https://api.github.com/users/TamarChristinaArm/repos", "events_url": "https://api.github.com/users/TamarChristinaArm/events{/privacy}", "received_events_url": "https://api.github.com/users/TamarChristinaArm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "8beff04a325ba3c3707d8a6dd954ec881193d655", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/8beff04a325ba3c3707d8a6dd954ec881193d655", "html_url": "https://github.com/Rust-GCC/gccrs/commit/8beff04a325ba3c3707d8a6dd954ec881193d655"}], "stats": {"total": 156, "additions": 156, "deletions": 0}, "files": [{"sha": "104088f67d2eb005880e2a62b9421421a8ba6a8c", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 57, "deletions": 0, "changes": 57, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/c98aabc1427a4d2a25a2176c89dc709148a04707/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/c98aabc1427a4d2a25a2176c89dc709148a04707/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=c98aabc1427a4d2a25a2176c89dc709148a04707", "patch": "@@ -4867,6 +4867,63 @@\n   }\n )\n \n+;; div optimizations using narrowings\n+;; we can do the division e.g. shorts by 255 faster by calculating it as\n+;; (x + ((x + 257) >> 8)) >> 8 assuming the operation is done in\n+;; double the precision of x.\n+;;\n+;; If we imagine a short as being composed of two blocks of bytes then\n+;; adding 257 or 0b0000_0001_0000_0001 to the number is equivalent to\n+;; adding 1 to each sub component:\n+;;\n+;;      short value of 16-bits\n+;; \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n+;; \u2502              \u2502                \u2502\n+;; \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n+;;   8-bit part1 \u25b2  8-bit part2   \u25b2\n+;;               \u2502                \u2502\n+;;               \u2502                \u2502\n+;;              +1               +1\n+;;\n+;; after the first addition, we have to shift right by 8, and narrow the\n+;; results back to a byte.  Remember that the addition must be done in\n+;; double the precision of the input.  Since 8 is half the size of a short\n+;; we can use a narrowing halfing instruction in AArch64, addhn which also\n+;; does the addition in a wider precision and narrows back to a byte.  The\n+;; shift itself is implicit in the operation as it writes back only the top\n+;; half of the result. i.e. bits 2*esize-1:esize.\n+;;\n+;; Since we have narrowed the result of the first part back to a byte, for\n+;; the second addition we can use a widening addition, uaddw.\n+;;\n+;; For the final shift, since it's unsigned arithmetic we emit an ushr by 8.\n+;;\n+;; The shift is later optimized by combine to a uzp2 with movi #0.\n+(define_expand \"@aarch64_bitmask_udiv<mode>3\"\n+  [(match_operand:VQN 0 \"register_operand\")\n+   (match_operand:VQN 1 \"register_operand\")\n+   (match_operand:VQN 2 \"immediate_operand\")]\n+  \"TARGET_SIMD\"\n+{\n+  unsigned HOST_WIDE_INT size\n+    = (1ULL << GET_MODE_UNIT_BITSIZE (<VNARROWQ>mode)) - 1;\n+  rtx elt = unwrap_const_vec_duplicate (operands[2]);\n+  if (!CONST_INT_P (elt) || UINTVAL (elt) != size)\n+    FAIL;\n+\n+  rtx addend = gen_reg_rtx (<MODE>mode);\n+  rtx val = aarch64_simd_gen_const_vector_dup (<VNARROWQ2>mode, 1);\n+  emit_move_insn (addend, lowpart_subreg (<MODE>mode, val, <VNARROWQ2>mode));\n+  rtx tmp1 = gen_reg_rtx (<VNARROWQ>mode);\n+  rtx tmp2 = gen_reg_rtx (<MODE>mode);\n+  emit_insn (gen_aarch64_addhn<mode> (tmp1, operands[1], addend));\n+  unsigned bitsize = GET_MODE_UNIT_BITSIZE (<VNARROWQ>mode);\n+  rtx shift_vector = aarch64_simd_gen_const_vector_dup (<MODE>mode, bitsize);\n+  emit_insn (gen_aarch64_uaddw<Vnarrowq> (tmp2, operands[1], tmp1));\n+  emit_insn (gen_aarch64_simd_lshr<mode> (operands[0], tmp2, shift_vector));\n+  DONE;\n+})\n+\n ;; pmul.\n \n (define_insn \"aarch64_pmul<mode>\""}, {"sha": "c91df6f5006c257690aafb75398933d628a970e1", "filename": "gcc/config/aarch64/aarch64.cc", "status": "modified", "additions": 38, "deletions": 0, "changes": 38, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/c98aabc1427a4d2a25a2176c89dc709148a04707/gcc%2Fconfig%2Faarch64%2Faarch64.cc", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/c98aabc1427a4d2a25a2176c89dc709148a04707/gcc%2Fconfig%2Faarch64%2Faarch64.cc", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.cc?ref=c98aabc1427a4d2a25a2176c89dc709148a04707", "patch": "@@ -24306,6 +24306,40 @@ aarch64_vectorize_vec_perm_const (machine_mode vmode, machine_mode op_mode,\n   return ret;\n }\n \n+/* Implement TARGET_VECTORIZE_CAN_SPECIAL_DIV_BY_CONST.  */\n+\n+bool\n+aarch64_vectorize_can_special_div_by_constant (enum tree_code code,\n+\t\t\t\t\t       tree vectype, wide_int cst,\n+\t\t\t\t\t       rtx *output, rtx in0, rtx in1)\n+{\n+  if (code != TRUNC_DIV_EXPR\n+      || !TYPE_UNSIGNED (vectype))\n+    return false;\n+\n+  unsigned int flags = aarch64_classify_vector_mode (TYPE_MODE (vectype));\n+  if ((flags & VEC_ANY_SVE) && !TARGET_SVE2)\n+    return false;\n+\n+  if (in0 == NULL_RTX && in1 == NULL_RTX)\n+    {\n+      wide_int val = wi::add (cst, 1);\n+      int pow = wi::exact_log2 (val);\n+      return pow == (int)(element_precision (vectype) / 2);\n+    }\n+\n+  if (!VECTOR_TYPE_P (vectype))\n+   return false;\n+\n+  gcc_assert (output);\n+\n+  if (!*output)\n+    *output = gen_reg_rtx (TYPE_MODE (vectype));\n+\n+  emit_insn (gen_aarch64_bitmask_udiv3 (TYPE_MODE (vectype), *output, in0, in1));\n+  return true;\n+}\n+\n /* Generate a byte permute mask for a register of mode MODE,\n    which has NUNITS units.  */\n \n@@ -27796,6 +27830,10 @@ aarch64_libgcc_floating_mode_supported_p\n #undef TARGET_VECTOR_ALIGNMENT\n #define TARGET_VECTOR_ALIGNMENT aarch64_simd_vector_alignment\n \n+#undef TARGET_VECTORIZE_CAN_SPECIAL_DIV_BY_CONST\n+#define TARGET_VECTORIZE_CAN_SPECIAL_DIV_BY_CONST \\\n+  aarch64_vectorize_can_special_div_by_constant\n+\n #undef TARGET_VECTORIZE_PREFERRED_VECTOR_ALIGNMENT\n #define TARGET_VECTORIZE_PREFERRED_VECTOR_ALIGNMENT \\\n   aarch64_vectorize_preferred_vector_alignment"}, {"sha": "2a535791ba7258302e0c2cf44ab211cd246d82d5", "filename": "gcc/testsuite/gcc.target/aarch64/div-by-bitmask.c", "status": "added", "additions": 61, "deletions": 0, "changes": 61, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/c98aabc1427a4d2a25a2176c89dc709148a04707/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fdiv-by-bitmask.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/c98aabc1427a4d2a25a2176c89dc709148a04707/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fdiv-by-bitmask.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fdiv-by-bitmask.c?ref=c98aabc1427a4d2a25a2176c89dc709148a04707", "patch": "@@ -0,0 +1,61 @@\n+/* { dg-do compile } */\n+/* { dg-additional-options \"-O3 -std=c99\" } */\n+/* { dg-final { check-function-bodies \"**\" \"\" \"\" { target { le } } } } */\n+\n+#include <stdint.h>\n+\n+#pragma GCC target \"+nosve\"\n+\n+/*\n+** draw_bitmap1:\n+** ...\n+** \taddhn\tv[0-9]+.8b, v[0-9]+.8h, v[0-9]+.8h\n+** \taddhn\tv[0-9]+.8b, v[0-9]+.8h, v[0-9]+.8h\n+** \tuaddw\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.8b\n+** \tuaddw\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.8b\n+** \tuzp2\tv[0-9]+.16b, v[0-9]+.16b, v[0-9]+.16b\n+** ...\n+*/\n+void draw_bitmap1(uint8_t* restrict pixel, uint8_t level, int n)\n+{\n+  for (int i = 0; i < (n & -16); i+=1)\n+    pixel[i] = (pixel[i] * level) / 0xff;\n+}\n+\n+void draw_bitmap2(uint8_t* restrict pixel, uint8_t level, int n)\n+{\n+  for (int i = 0; i < (n & -16); i+=1)\n+    pixel[i] = (pixel[i] * level) / 0xfe;\n+}\n+\n+/*\n+** draw_bitmap3:\n+** ...\n+** \taddhn\tv[0-9]+.4h, v[0-9]+.4s, v[0-9]+.4s\n+** \taddhn\tv[0-9]+.4h, v[0-9]+.4s, v[0-9]+.4s\n+** \tuaddw\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.4h\n+** \tuaddw\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.4h\n+** \tuzp2\tv[0-9]+.8h, v[0-9]+.8h, v[0-9]+.8h\n+** ...\n+*/\n+void draw_bitmap3(uint16_t* restrict pixel, uint16_t level, int n)\n+{\n+  for (int i = 0; i < (n & -16); i+=1)\n+    pixel[i] = (pixel[i] * level) / 0xffffU;\n+}\n+\n+/*\n+** draw_bitmap4:\n+** ...\n+** \taddhn\tv[0-9]+.2s, v[0-9]+.2d, v[0-9]+.2d\n+** \taddhn\tv[0-9]+.2s, v[0-9]+.2d, v[0-9]+.2d\n+** \tuaddw\tv[0-9]+.2d, v[0-9]+.2d, v[0-9]+.2s\n+** \tuaddw\tv[0-9]+.2d, v[0-9]+.2d, v[0-9]+.2s\n+** \tuzp2\tv[0-9]+.4s, v[0-9]+.4s, v[0-9]+.4s\n+** ...\n+*/\n+void draw_bitmap4(uint32_t* restrict pixel, uint32_t level, int n)\n+{\n+  for (int i = 0; i < (n & -16); i+=1)\n+    pixel[i] = (pixel[i] * (uint64_t)level) / 0xffffffffUL;\n+}"}]}