{"sha": "25adc5d044047b926181ebcaf6284b39df8ee306", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6MjVhZGM1ZDA0NDA0N2I5MjYxODFlYmNhZjYyODRiMzlkZjhlZTMwNg==", "commit": {"author": {"name": "Michael Meissner", "email": "meissner@linux.vnet.ibm.com", "date": "2014-11-17T22:32:26Z"}, "committer": {"name": "Michael Meissner", "email": "meissner@gcc.gnu.org", "date": "2014-11-17T22:32:26Z"}, "message": "rs6000.c (RELOAD_REG_AND_M16): Add support for Altivec style vector loads that ignore the bottom 3 bits of the...\n\n[gcc]\n2014-11-17  Michael Meissner  <meissner@linux.vnet.ibm.com>\n\t    Ulrich Weigand  <Ulrich.Weigand@de.ibm.com>\n\n\t* config/rs6000/rs6000.c (RELOAD_REG_AND_M16): Add support for\n\tAltivec style vector loads that ignore the bottom 3 bits of the\n\taddress.\n\t(rs6000_debug_addr_mask): New function to print the addr_mask\n\tvalues if debugging.\n\t(rs6000_debug_print_mode): Call rs6000_debug_addr_mask to print\n\tout addr_mask.\n\t(rs6000_setup_reg_addr_masks): Add support for Altivec style\n\tvector loads that ignore the bottom 3 bits of the address.  Allow\n\tpre-increment and pre-decrement on floating point, even if the\n\t-mupper-regs-{sf,df} options were used.\n\t(rs6000_init_hard_regno_mode_ok): Rework DFmode support if\n\t-mupper-regs-df.  Add support for -mupper-regs-sf.  Rearrange code\n\tplacement for direct move support.\n\t(rs6000_option_override_internal): Add checks for -mupper-regs-df\n\trequiring -mvsx, and -mupper-regs-sf requiring -mpower8-vector.\n\tIf -mupper-regs, set both -mupper-regs-sf and -mupper-regs-df,\n\tdepending on the underlying cpu.\n\t(rs6000_secondary_reload_fail): Add ATTRIBUTE_NORETURN.\n\t(rs6000_secondary_reload_toc_costs): Helper function to identify\n\tcosts of a TOC load for secondary reload support.\n\t(rs6000_secondary_reload_memory): Helper function for secondary\n\treload, to determine if a particular memory operation is directly\n\thandled by the hardware, or if it needs support from secondary\n\treload to create a valid address.\n\t(rs6000_secondary_reload): Rework code, to be clearer.  If the\n\tappropriate -mupper-regs-{sf,df} is used, use FPR registers to\n\treload scalar values, since the FPR registers have D-form\n\taddressing. Move most of the code handling memory to the function\n\trs6000_secondary_reload_memory, and use the reg_addr structure to\n\tdetermine what type of address modes are supported.  Print more\n\tdebug information if -mdebug=addr.\n\t(rs6000_secondary_reload_inner): Rework entire function to be more\n\tgeneral.  Use the reg_addr bits to determine what type of\n\taddressing is supported.\n\t(rs6000_preferred_reload_class): Rework.  Move constant handling\n\tinto a single place.  Prefer using FLOAT_REGS for scalar floating\n\tpoint.\n\t(rs6000_secondary_reload_class): Use a FPR register to move a\n\tvalue from an Altivec register to a GPR, and vice versa.  Move VSX\n\thandling above traditional floating point.\n\n\t* config/rs6000/rs6000.md (mov<mode>_hardfloat, FMOVE32 case):\n\tDelete some spaces in the constraints.\n\t(DF->DF move peephole2): Disable if -mupper-regs-{sf,df} to\n\tallow using FPR registers to load/store an Altivec register for\n\tscalar floating point types.\n\t(SF->SF move peephole2): Likewise.\n\t(DFmode splitter): Add a define_split to move floating point\n\tconstants to the constant pool before register allocation.\n\tNormally constants are put into the pool immediately, but\n\t-ffast-math delays putting them into the constant pool for the\n\treciprocal approximation support.\n\t(SFmode splitter): Likewise.\n\n\t* config/rs6000/rs6000.opt (-mupper-regs-df): Make option public.\n\t(-mupper-regs-sf): Likewise.\n\n\t* config/rs6000/rs6000-c.c (rs6000_target_modify_macros): Define\n\t__UPPER_REGS_DF__ if -mupper-regs-df.  Define __UPPER_REGS_SF__ if\n\t-mupper-regs-sf.\n\t(-mupper-regs): New combination option that sets -mupper-regs-sf\n\tand -mupper-regs-df by default if the cpu supports the instructions.\n\n\t* doc/invoke.texi (RS/6000 and PowerPC Options): Document\n\t-mupper-regs, -mupper-regs-sf, and -mupper-regs-df.\n\n\t* config/rs6000/predicates.md (memory_fp_constant): New predicate\n\tto return true if the operand is a floating point constant that\n\tmust be put into the constant pool, before register allocation\n\toccurs.\n\n\t* config/rs6000/rs6000-cpus.def (ISA_2_6_MASKS_SERVER): Enable\n\t-mupper-regs-df by default.\n\t(ISA_2_7_MASKS_SERVER): Enable -mupper-regs-sf by default.\n\t(POWERPC_MASKS): Add -mupper-regs-{sf,df} as options set by the\n\tvarious -mcpu=... options.\n\t(power7 cpu): Enable -mupper-regs-df by default.\n\n\t* doc/invoke.texi (RS/6000 and PowerPC Options): Document\n\t-mupper-regs.\n\n[gcc/testsuite]\n2014-11-17  Michael Meissner  <meissner@linux.vnet.ibm.com>\n\n\t* gcc.target/powerpc/p8vector-ldst.c: Rewrite to use 40 live\n\tfloating point variables instead of using asm to test allocating\n\tvalues to the Altivec registers.\n\n\t* gcc.target/powerpc/upper-regs-sf.c: New -mupper-regs-sf and\n\t-mupper-regs-df tests.\n\t* gcc.target/powerpc/upper-regs-df.c: Likewise.\n\n\t* config/rs6000/predicates.md (memory_fp_constant): New predicate\n\n\nCo-Authored-By: Ulrich Weigand <uweigand@de.ibm.com>\n\nFrom-SVN: r217679", "tree": {"sha": "5839553437c95da1cf37623bc65b65a198077cf9", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/5839553437c95da1cf37623bc65b65a198077cf9"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/25adc5d044047b926181ebcaf6284b39df8ee306", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/25adc5d044047b926181ebcaf6284b39df8ee306", "html_url": "https://github.com/Rust-GCC/gccrs/commit/25adc5d044047b926181ebcaf6284b39df8ee306", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/25adc5d044047b926181ebcaf6284b39df8ee306/comments", "author": null, "committer": null, "parents": [{"sha": "5a4e7cade9b48522942a62b1064a4bd6b02f95e0", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/5a4e7cade9b48522942a62b1064a4bd6b02f95e0", "html_url": "https://github.com/Rust-GCC/gccrs/commit/5a4e7cade9b48522942a62b1064a4bd6b02f95e0"}], "stats": {"total": 3409, "additions": 2941, "deletions": 468}, "files": [{"sha": "bd856b5ef04abcad8abad6f96f05912fe9633ba0", "filename": "gcc/ChangeLog", "status": "modified", "additions": 85, "deletions": 0, "changes": 85, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=25adc5d044047b926181ebcaf6284b39df8ee306", "patch": "@@ -1,3 +1,88 @@\n+2014-11-17  Michael Meissner  <meissner@linux.vnet.ibm.com>\n+\t    Ulrich Weigand  <Ulrich.Weigand@de.ibm.com>\n+\n+\t* config/rs6000/rs6000.c (RELOAD_REG_AND_M16): Add support for\n+\tAltivec style vector loads that ignore the bottom 3 bits of the\n+\taddress.\n+\t(rs6000_debug_addr_mask): New function to print the addr_mask\n+\tvalues if debugging.\n+\t(rs6000_debug_print_mode): Call rs6000_debug_addr_mask to print\n+\tout addr_mask.\n+\t(rs6000_setup_reg_addr_masks): Add support for Altivec style\n+\tvector loads that ignore the bottom 3 bits of the address.  Allow\n+\tpre-increment and pre-decrement on floating point, even if the\n+\t-mupper-regs-{sf,df} options were used.\n+\t(rs6000_init_hard_regno_mode_ok): Rework DFmode support if\n+\t-mupper-regs-df.  Add support for -mupper-regs-sf.  Rearrange code\n+\tplacement for direct move support.\n+\t(rs6000_option_override_internal): Add checks for -mupper-regs-df\n+\trequiring -mvsx, and -mupper-regs-sf requiring -mpower8-vector.\n+\tIf -mupper-regs, set both -mupper-regs-sf and -mupper-regs-df,\n+\tdepending on the underlying cpu.\n+\t(rs6000_secondary_reload_fail): Add ATTRIBUTE_NORETURN.\n+\t(rs6000_secondary_reload_toc_costs): Helper function to identify\n+\tcosts of a TOC load for secondary reload support.\n+\t(rs6000_secondary_reload_memory): Helper function for secondary\n+\treload, to determine if a particular memory operation is directly\n+\thandled by the hardware, or if it needs support from secondary\n+\treload to create a valid address.\n+\t(rs6000_secondary_reload): Rework code, to be clearer.  If the\n+\tappropriate -mupper-regs-{sf,df} is used, use FPR registers to\n+\treload scalar values, since the FPR registers have D-form\n+\taddressing. Move most of the code handling memory to the function\n+\trs6000_secondary_reload_memory, and use the reg_addr structure to\n+\tdetermine what type of address modes are supported.  Print more\n+\tdebug information if -mdebug=addr.\n+\t(rs6000_secondary_reload_inner): Rework entire function to be more\n+\tgeneral.  Use the reg_addr bits to determine what type of\n+\taddressing is supported.\n+\t(rs6000_preferred_reload_class): Rework.  Move constant handling\n+\tinto a single place.  Prefer using FLOAT_REGS for scalar floating\n+\tpoint.\n+\t(rs6000_secondary_reload_class): Use a FPR register to move a\n+\tvalue from an Altivec register to a GPR, and vice versa.  Move VSX\n+\thandling above traditional floating point.\n+\n+\t* config/rs6000/rs6000.md (mov<mode>_hardfloat, FMOVE32 case):\n+\tDelete some spaces in the constraints.\n+\t(DF->DF move peephole2): Disable if -mupper-regs-{sf,df} to\n+\tallow using FPR registers to load/store an Altivec register for\n+\tscalar floating point types.\n+\t(SF->SF move peephole2): Likewise.\n+\t(DFmode splitter): Add a define_split to move floating point\n+\tconstants to the constant pool before register allocation.\n+\tNormally constants are put into the pool immediately, but\n+\t-ffast-math delays putting them into the constant pool for the\n+\treciprocal approximation support.\n+\t(SFmode splitter): Likewise.\n+\n+\t* config/rs6000/rs6000.opt (-mupper-regs-df): Make option public.\n+\t(-mupper-regs-sf): Likewise.\n+\n+\t* config/rs6000/rs6000-c.c (rs6000_target_modify_macros): Define\n+\t__UPPER_REGS_DF__ if -mupper-regs-df.  Define __UPPER_REGS_SF__ if\n+\t-mupper-regs-sf.\n+\t(-mupper-regs): New combination option that sets -mupper-regs-sf\n+\tand -mupper-regs-df by default if the cpu supports the instructions.\n+\n+\t* doc/invoke.texi (RS/6000 and PowerPC Options): Document\n+\t-mupper-regs, -mupper-regs-sf, and -mupper-regs-df.\n+\n+\t* config/rs6000/predicates.md (memory_fp_constant): New predicate\n+\tto return true if the operand is a floating point constant that\n+\tmust be put into the constant pool, before register allocation\n+\toccurs.\n+\n+\t* config/rs6000/rs6000-cpus.def (ISA_2_6_MASKS_SERVER): Enable\n+\t-mupper-regs-df by default.\n+\t(ISA_2_7_MASKS_SERVER): Enable -mupper-regs-sf by default.\n+\t(POWERPC_MASKS): Add -mupper-regs-{sf,df} as options set by the\n+\tvarious -mcpu=... options.\n+\t(power7 cpu): Enable -mupper-regs-df by default.\n+\n+\t* doc/invoke.texi (RS/6000 and PowerPC Options): Document\n+\t-mupper-regs.\n+\n 2014-11-17  Zhouyi Zhou <yizhouzhou@ict.ac.cn>\n \n \t* ira-conflicts.c (build_conflict_bit_table): Add the current"}, {"sha": "de7fa4ebc7669077c2f9d6ad0d3c4effa4d0dafb", "filename": "gcc/config/rs6000/predicates.md", "status": "modified", "additions": 21, "deletions": 0, "changes": 21, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Fconfig%2Frs6000%2Fpredicates.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Fconfig%2Frs6000%2Fpredicates.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Fpredicates.md?ref=25adc5d044047b926181ebcaf6284b39df8ee306", "patch": "@@ -521,6 +521,27 @@\n   }\n })\n \n+;; Return 1 if the operand must be loaded from memory.  This is used by a\n+;; define_split to insure constants get pushed to the constant pool before\n+;; reload.  If -ffast-math is used, easy_fp_constant will allow move insns to\n+;; have constants in order not interfere with reciprocal estimation.  However,\n+;; with -mupper-regs support, these constants must be moved to the constant\n+;; pool before register allocation.\n+\n+(define_predicate \"memory_fp_constant\"\n+  (match_code \"const_double\")\n+{\n+  if (TARGET_VSX && op == CONST0_RTX (mode))\n+    return 0;\n+\n+  if (!TARGET_HARD_FLOAT || !TARGET_FPRS\n+      || (mode == SFmode && !TARGET_SINGLE_FLOAT)\n+      || (mode == DFmode && !TARGET_DOUBLE_FLOAT))\n+    return 0;\n+\t  \n+  return 1;\n+})\n+\n ;; Return 1 if the operand is a CONST_VECTOR and can be loaded into a\n ;; vector register without using memory.\n (define_predicate \"easy_vector_constant\""}, {"sha": "0deeaf16f8fa039d020f6e9c3d70ed976df74e11", "filename": "gcc/config/rs6000/rs6000-c.c", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Fconfig%2Frs6000%2Frs6000-c.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Fconfig%2Frs6000%2Frs6000-c.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000-c.c?ref=25adc5d044047b926181ebcaf6284b39df8ee306", "patch": "@@ -380,6 +380,10 @@ rs6000_target_modify_macros (bool define_p, HOST_WIDE_INT flags,\n     rs6000_define_or_undefine_macro (define_p, \"__QUAD_MEMORY_ATOMIC__\");\n   if ((flags & OPTION_MASK_CRYPTO) != 0)\n     rs6000_define_or_undefine_macro (define_p, \"__CRYPTO__\");\n+  if ((flags & OPTION_MASK_UPPER_REGS_DF) != 0)\n+    rs6000_define_or_undefine_macro (define_p, \"__UPPER_REGS_DF__\");\n+  if ((flags & OPTION_MASK_UPPER_REGS_SF) != 0)\n+    rs6000_define_or_undefine_macro (define_p, \"__UPPER_REGS_SF__\");\n \n   /* options from the builtin masks.  */\n   if ((bu_mask & RS6000_BTM_SPE) != 0)"}, {"sha": "c1a7649c3fb3dcffe6bd48eba08dbe3f446a5246", "filename": "gcc/config/rs6000/rs6000-cpus.def", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Fconfig%2Frs6000%2Frs6000-cpus.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Fconfig%2Frs6000%2Frs6000-cpus.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000-cpus.def?ref=25adc5d044047b926181ebcaf6284b39df8ee306", "patch": "@@ -44,7 +44,8 @@\n #define ISA_2_6_MASKS_SERVER\t(ISA_2_5_MASKS_SERVER\t\t\t\\\n \t\t\t\t | OPTION_MASK_POPCNTD\t\t\t\\\n \t\t\t\t | OPTION_MASK_ALTIVEC\t\t\t\\\n-\t\t\t\t | OPTION_MASK_VSX)\n+\t\t\t\t | OPTION_MASK_VSX\t\t\t\\\n+\t\t\t\t | OPTION_MASK_UPPER_REGS_DF)\n \n /* For now, don't provide an embedded version of ISA 2.07.  */\n #define ISA_2_7_MASKS_SERVER\t(ISA_2_6_MASKS_SERVER\t\t\t\\\n@@ -54,7 +55,8 @@\n \t\t\t\t | OPTION_MASK_DIRECT_MOVE\t\t\\\n \t\t\t\t | OPTION_MASK_HTM\t\t\t\\\n \t\t\t\t | OPTION_MASK_QUAD_MEMORY\t\t\\\n-  \t\t\t\t | OPTION_MASK_QUAD_MEMORY_ATOMIC)\n+  \t\t\t\t | OPTION_MASK_QUAD_MEMORY_ATOMIC\t\\\n+\t\t\t\t | OPTION_MASK_UPPER_REGS_SF)\n \n #define POWERPC_7400_MASK\t(OPTION_MASK_PPC_GFXOPT | OPTION_MASK_ALTIVEC)\n \n@@ -94,6 +96,8 @@\n \t\t\t\t | OPTION_MASK_RECIP_PRECISION\t\t\\\n \t\t\t\t | OPTION_MASK_SOFT_FLOAT\t\t\\\n \t\t\t\t | OPTION_MASK_STRICT_ALIGN_OPTIONAL\t\\\n+\t\t\t\t | OPTION_MASK_UPPER_REGS_DF\t\t\\\n+\t\t\t\t | OPTION_MASK_UPPER_REGS_SF\t\t\\\n \t\t\t\t | OPTION_MASK_VSX\t\t\t\\\n \t\t\t\t | OPTION_MASK_VSX_TIMODE)\n \n@@ -184,7 +188,7 @@ RS6000_CPU (\"power6x\", PROCESSOR_POWER6, MASK_POWERPC64 | MASK_PPC_GPOPT\n RS6000_CPU (\"power7\", PROCESSOR_POWER7,   /* Don't add MASK_ISEL by default */\n \t    POWERPC_7400_MASK | MASK_POWERPC64 | MASK_PPC_GPOPT | MASK_MFCRF\n \t    | MASK_POPCNTB | MASK_FPRND | MASK_CMPB | MASK_DFP | MASK_POPCNTD\n-\t    | MASK_VSX | MASK_RECIP_PRECISION)\n+\t    | MASK_VSX | MASK_RECIP_PRECISION | OPTION_MASK_UPPER_REGS_DF)\n RS6000_CPU (\"power8\", PROCESSOR_POWER8, MASK_POWERPC64 | ISA_2_7_MASKS_SERVER)\n RS6000_CPU (\"powerpc\", PROCESSOR_POWERPC, 0)\n RS6000_CPU (\"powerpc64\", PROCESSOR_POWERPC64, MASK_PPC_GFXOPT | MASK_POWERPC64)"}, {"sha": "4f66840fd4a67477365218bd5c1f79796dfc7aa0", "filename": "gcc/config/rs6000/rs6000.c", "status": "modified", "additions": 690, "deletions": 434, "changes": 1124, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Fconfig%2Frs6000%2Frs6000.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Fconfig%2Frs6000%2Frs6000.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000.c?ref=25adc5d044047b926181ebcaf6284b39df8ee306", "patch": "@@ -394,6 +394,7 @@ typedef unsigned char addr_mask_type;\n #define RELOAD_REG_OFFSET\t0x08\t/* Reg+offset addressing. */\n #define RELOAD_REG_PRE_INCDEC\t0x10\t/* PRE_INC/PRE_DEC valid.  */\n #define RELOAD_REG_PRE_MODIFY\t0x20\t/* PRE_MODIFY valid.  */\n+#define RELOAD_REG_AND_M16\t0x40\t/* AND -16 addressing.  */\n \n /* Register type masks based on the type, of valid addressing modes.  */\n struct rs6000_reg_addr {\n@@ -1928,6 +1929,54 @@ rs6000_debug_vector_unit (enum rs6000_vector v)\n   return ret;\n }\n \n+/* Inner function printing just the address mask for a particular reload\n+   register class.  */\n+DEBUG_FUNCTION char *\n+rs6000_debug_addr_mask (addr_mask_type mask, bool keep_spaces)\n+{\n+  static char ret[8];\n+  char *p = ret;\n+\n+  if ((mask & RELOAD_REG_VALID) != 0)\n+    *p++ = 'v';\n+  else if (keep_spaces)\n+    *p++ = ' ';\n+\n+  if ((mask & RELOAD_REG_MULTIPLE) != 0)\n+    *p++ = 'm';\n+  else if (keep_spaces)\n+    *p++ = ' ';\n+\n+  if ((mask & RELOAD_REG_INDEXED) != 0)\n+    *p++ = 'i';\n+  else if (keep_spaces)\n+    *p++ = ' ';\n+\n+  if ((mask & RELOAD_REG_OFFSET) != 0)\n+    *p++ = 'o';\n+  else if (keep_spaces)\n+    *p++ = ' ';\n+\n+  if ((mask & RELOAD_REG_PRE_INCDEC) != 0)\n+    *p++ = '+';\n+  else if (keep_spaces)\n+    *p++ = ' ';\n+\n+  if ((mask & RELOAD_REG_PRE_MODIFY) != 0)\n+    *p++ = '+';\n+  else if (keep_spaces)\n+    *p++ = ' ';\n+\n+  if ((mask & RELOAD_REG_AND_M16) != 0)\n+    *p++ = '&';\n+  else if (keep_spaces)\n+    *p++ = ' ';\n+\n+  *p = '\\0';\n+\n+  return ret;\n+}\n+\n /* Print the address masks in a human readble fashion.  */\n DEBUG_FUNCTION void\n rs6000_debug_print_mode (ssize_t m)\n@@ -1936,18 +1985,8 @@ rs6000_debug_print_mode (ssize_t m)\n \n   fprintf (stderr, \"Mode: %-5s\", GET_MODE_NAME (m));\n   for (rc = 0; rc < N_RELOAD_REG; rc++)\n-    {\n-      addr_mask_type mask = reg_addr[m].addr_mask[rc];\n-      fprintf (stderr,\n-\t       \"  %s: %c%c%c%c%c%c\",\n-\t       reload_reg_map[rc].name,\n-\t       (mask & RELOAD_REG_VALID)      != 0 ? 'v' : ' ',\n-\t       (mask & RELOAD_REG_MULTIPLE)   != 0 ? 'm' : ' ',\n-\t       (mask & RELOAD_REG_INDEXED)    != 0 ? 'i' : ' ',\n-\t       (mask & RELOAD_REG_OFFSET)     != 0 ? 'o' : ' ',\n-\t       (mask & RELOAD_REG_PRE_INCDEC) != 0 ? '+' : ' ',\n-\t       (mask & RELOAD_REG_PRE_MODIFY) != 0 ? '+' : ' ');\n-    }\n+    fprintf (stderr, \" %s: %s\", reload_reg_map[rc].name,\n+\t     rs6000_debug_addr_mask (reg_addr[m].addr_mask[rc], true));\n \n   if (rs6000_vector_unit[m] != VECTOR_NONE\n       || rs6000_vector_mem[m] != VECTOR_NONE\n@@ -2423,18 +2462,15 @@ rs6000_setup_reg_addr_masks (void)\n \t      /* Figure out if we can do PRE_INC, PRE_DEC, or PRE_MODIFY\n \t\t addressing.  Restrict addressing on SPE for 64-bit types\n \t\t because of the SUBREG hackery used to address 64-bit floats in\n-\t\t '32-bit' GPRs.  To simplify secondary reload, don't allow\n-\t\t update forms on scalar floating point types that can go in the\n-\t\t upper registers.  */\n+\t\t '32-bit' GPRs.  */\n \n \t      if (TARGET_UPDATE\n \t\t  && (rc == RELOAD_REG_GPR || rc == RELOAD_REG_FPR)\n \t\t  && GET_MODE_SIZE (m2) <= 8\n \t\t  && !VECTOR_MODE_P (m2)\n \t\t  && !COMPLEX_MODE_P (m2)\n \t\t  && !indexed_only_p\n-\t\t  && !(TARGET_E500_DOUBLE && GET_MODE_SIZE (m2) == 8)\n-\t\t  && !reg_addr[m2].scalar_in_vmx_p)\n+\t\t  && !(TARGET_E500_DOUBLE && GET_MODE_SIZE (m2) == 8))\n \t\t{\n \t\t  addr_mask |= RELOAD_REG_PRE_INCDEC;\n \n@@ -2467,6 +2503,11 @@ rs6000_setup_reg_addr_masks (void)\n \t      && (rc == RELOAD_REG_GPR || rc == RELOAD_REG_FPR))\n \t    addr_mask |= RELOAD_REG_OFFSET;\n \n+\t  /* VMX registers can do (REG & -16) and ((REG+REG) & -16)\n+\t     addressing on 128-bit types.  */\n+\t  if (rc == RELOAD_REG_VMX && GET_MODE_SIZE (m2) == 16)\n+\t    addr_mask |= RELOAD_REG_AND_M16;\n+\n \t  reg_addr[m].addr_mask[rc] = addr_mask;\n \t  any_addr_mask |= addr_mask;\n \t}\n@@ -2633,13 +2674,19 @@ rs6000_init_hard_regno_mode_ok (bool global_init_p)\n       rs6000_vector_align[V1TImode] = 128;\n     }\n \n-  /* DFmode, see if we want to use the VSX unit.  */\n+  /* DFmode, see if we want to use the VSX unit.  Memory is handled\n+     differently, so don't set rs6000_vector_mem.  */\n   if (TARGET_VSX && TARGET_VSX_SCALAR_DOUBLE)\n     {\n       rs6000_vector_unit[DFmode] = VECTOR_VSX;\n-      rs6000_vector_mem[DFmode]\n-\t= (TARGET_UPPER_REGS_DF ? VECTOR_VSX : VECTOR_NONE);\n-      rs6000_vector_align[DFmode] = align64;\n+      rs6000_vector_align[DFmode] = 64;\n+    }\n+\n+  /* SFmode, see if we want to use the VSX unit.  */\n+  if (TARGET_P8_VECTOR && TARGET_VSX_SCALAR_FLOAT)\n+    {\n+      rs6000_vector_unit[SFmode] = VECTOR_VSX;\n+      rs6000_vector_align[SFmode] = 32;\n     }\n \n   /* Allow TImode in VSX register and set the VSX memory macros.  */\n@@ -2774,58 +2821,42 @@ rs6000_init_hard_regno_mode_ok (bool global_init_p)\n \t  reg_addr[V4SFmode].reload_load   = CODE_FOR_reload_v4sf_di_load;\n \t  reg_addr[V2DFmode].reload_store  = CODE_FOR_reload_v2df_di_store;\n \t  reg_addr[V2DFmode].reload_load   = CODE_FOR_reload_v2df_di_load;\n-\t  if (TARGET_VSX && TARGET_UPPER_REGS_DF)\n-\t    {\n-\t      reg_addr[DFmode].reload_store    = CODE_FOR_reload_df_di_store;\n-\t      reg_addr[DFmode].reload_load     = CODE_FOR_reload_df_di_load;\n-\t      reg_addr[DFmode].scalar_in_vmx_p = true;\n-\t      reg_addr[DDmode].reload_store    = CODE_FOR_reload_dd_di_store;\n-\t      reg_addr[DDmode].reload_load     = CODE_FOR_reload_dd_di_load;\n-\t    }\n-\t  if (TARGET_P8_VECTOR)\n-\t    {\n-\t      reg_addr[SFmode].reload_store  = CODE_FOR_reload_sf_di_store;\n-\t      reg_addr[SFmode].reload_load   = CODE_FOR_reload_sf_di_load;\n-\t      reg_addr[SDmode].reload_store  = CODE_FOR_reload_sd_di_store;\n-\t      reg_addr[SDmode].reload_load   = CODE_FOR_reload_sd_di_load;\n-\t      if (TARGET_UPPER_REGS_SF)\n-\t\treg_addr[SFmode].scalar_in_vmx_p = true;\n-\t    }\n+\t  reg_addr[DFmode].reload_store    = CODE_FOR_reload_df_di_store;\n+\t  reg_addr[DFmode].reload_load     = CODE_FOR_reload_df_di_load;\n+\t  reg_addr[DDmode].reload_store    = CODE_FOR_reload_dd_di_store;\n+\t  reg_addr[DDmode].reload_load     = CODE_FOR_reload_dd_di_load;\n+\t  reg_addr[SFmode].reload_store    = CODE_FOR_reload_sf_di_store;\n+\t  reg_addr[SFmode].reload_load     = CODE_FOR_reload_sf_di_load;\n+\t  reg_addr[SDmode].reload_store    = CODE_FOR_reload_sd_di_store;\n+\t  reg_addr[SDmode].reload_load     = CODE_FOR_reload_sd_di_load;\n+\n \t  if (TARGET_VSX_TIMODE)\n \t    {\n \t      reg_addr[TImode].reload_store  = CODE_FOR_reload_ti_di_store;\n \t      reg_addr[TImode].reload_load   = CODE_FOR_reload_ti_di_load;\n \t    }\n+\n \t  if (TARGET_DIRECT_MOVE)\n \t    {\n-\t      if (TARGET_POWERPC64)\n-\t\t{\n-\t\t  reg_addr[TImode].reload_gpr_vsx    = CODE_FOR_reload_gpr_from_vsxti;\n-\t\t  reg_addr[V1TImode].reload_gpr_vsx  = CODE_FOR_reload_gpr_from_vsxv1ti;\n-\t\t  reg_addr[V2DFmode].reload_gpr_vsx  = CODE_FOR_reload_gpr_from_vsxv2df;\n-\t\t  reg_addr[V2DImode].reload_gpr_vsx  = CODE_FOR_reload_gpr_from_vsxv2di;\n-\t\t  reg_addr[V4SFmode].reload_gpr_vsx  = CODE_FOR_reload_gpr_from_vsxv4sf;\n-\t\t  reg_addr[V4SImode].reload_gpr_vsx  = CODE_FOR_reload_gpr_from_vsxv4si;\n-\t\t  reg_addr[V8HImode].reload_gpr_vsx  = CODE_FOR_reload_gpr_from_vsxv8hi;\n-\t\t  reg_addr[V16QImode].reload_gpr_vsx = CODE_FOR_reload_gpr_from_vsxv16qi;\n-\t\t  reg_addr[SFmode].reload_gpr_vsx    = CODE_FOR_reload_gpr_from_vsxsf;\n-\n-\t\t  reg_addr[TImode].reload_vsx_gpr    = CODE_FOR_reload_vsx_from_gprti;\n-\t\t  reg_addr[V1TImode].reload_vsx_gpr  = CODE_FOR_reload_vsx_from_gprv1ti;\n-\t\t  reg_addr[V2DFmode].reload_vsx_gpr  = CODE_FOR_reload_vsx_from_gprv2df;\n-\t\t  reg_addr[V2DImode].reload_vsx_gpr  = CODE_FOR_reload_vsx_from_gprv2di;\n-\t\t  reg_addr[V4SFmode].reload_vsx_gpr  = CODE_FOR_reload_vsx_from_gprv4sf;\n-\t\t  reg_addr[V4SImode].reload_vsx_gpr  = CODE_FOR_reload_vsx_from_gprv4si;\n-\t\t  reg_addr[V8HImode].reload_vsx_gpr  = CODE_FOR_reload_vsx_from_gprv8hi;\n-\t\t  reg_addr[V16QImode].reload_vsx_gpr = CODE_FOR_reload_vsx_from_gprv16qi;\n-\t\t  reg_addr[SFmode].reload_vsx_gpr    = CODE_FOR_reload_vsx_from_gprsf;\n-\t\t}\n-\t      else\n-\t\t{\n-\t\t  reg_addr[DImode].reload_fpr_gpr = CODE_FOR_reload_fpr_from_gprdi;\n-\t\t  reg_addr[DDmode].reload_fpr_gpr = CODE_FOR_reload_fpr_from_gprdd;\n-\t\t  reg_addr[DFmode].reload_fpr_gpr = CODE_FOR_reload_fpr_from_gprdf;\n-\t\t}\n+\t      reg_addr[TImode].reload_gpr_vsx    = CODE_FOR_reload_gpr_from_vsxti;\n+\t      reg_addr[V1TImode].reload_gpr_vsx  = CODE_FOR_reload_gpr_from_vsxv1ti;\n+\t      reg_addr[V2DFmode].reload_gpr_vsx  = CODE_FOR_reload_gpr_from_vsxv2df;\n+\t      reg_addr[V2DImode].reload_gpr_vsx  = CODE_FOR_reload_gpr_from_vsxv2di;\n+\t      reg_addr[V4SFmode].reload_gpr_vsx  = CODE_FOR_reload_gpr_from_vsxv4sf;\n+\t      reg_addr[V4SImode].reload_gpr_vsx  = CODE_FOR_reload_gpr_from_vsxv4si;\n+\t      reg_addr[V8HImode].reload_gpr_vsx  = CODE_FOR_reload_gpr_from_vsxv8hi;\n+\t      reg_addr[V16QImode].reload_gpr_vsx = CODE_FOR_reload_gpr_from_vsxv16qi;\n+\t      reg_addr[SFmode].reload_gpr_vsx    = CODE_FOR_reload_gpr_from_vsxsf;\n+\n+\t      reg_addr[TImode].reload_vsx_gpr    = CODE_FOR_reload_vsx_from_gprti;\n+\t      reg_addr[V1TImode].reload_vsx_gpr  = CODE_FOR_reload_vsx_from_gprv1ti;\n+\t      reg_addr[V2DFmode].reload_vsx_gpr  = CODE_FOR_reload_vsx_from_gprv2df;\n+\t      reg_addr[V2DImode].reload_vsx_gpr  = CODE_FOR_reload_vsx_from_gprv2di;\n+\t      reg_addr[V4SFmode].reload_vsx_gpr  = CODE_FOR_reload_vsx_from_gprv4sf;\n+\t      reg_addr[V4SImode].reload_vsx_gpr  = CODE_FOR_reload_vsx_from_gprv4si;\n+\t      reg_addr[V8HImode].reload_vsx_gpr  = CODE_FOR_reload_vsx_from_gprv8hi;\n+\t      reg_addr[V16QImode].reload_vsx_gpr = CODE_FOR_reload_vsx_from_gprv16qi;\n+\t      reg_addr[SFmode].reload_vsx_gpr    = CODE_FOR_reload_vsx_from_gprsf;\n \t    }\n \t}\n       else\n@@ -2844,29 +2875,34 @@ rs6000_init_hard_regno_mode_ok (bool global_init_p)\n \t  reg_addr[V4SFmode].reload_load   = CODE_FOR_reload_v4sf_si_load;\n \t  reg_addr[V2DFmode].reload_store  = CODE_FOR_reload_v2df_si_store;\n \t  reg_addr[V2DFmode].reload_load   = CODE_FOR_reload_v2df_si_load;\n-\t  if (TARGET_VSX && TARGET_UPPER_REGS_DF)\n-\t    {\n-\t      reg_addr[DFmode].reload_store    = CODE_FOR_reload_df_si_store;\n-\t      reg_addr[DFmode].reload_load     = CODE_FOR_reload_df_si_load;\n-\t      reg_addr[DFmode].scalar_in_vmx_p = true;\n-\t      reg_addr[DDmode].reload_store    = CODE_FOR_reload_dd_si_store;\n-\t      reg_addr[DDmode].reload_load     = CODE_FOR_reload_dd_si_load;\n-\t    }\n-\t  if (TARGET_P8_VECTOR)\n-\t    {\n-\t      reg_addr[SFmode].reload_store  = CODE_FOR_reload_sf_si_store;\n-\t      reg_addr[SFmode].reload_load   = CODE_FOR_reload_sf_si_load;\n-\t      reg_addr[SDmode].reload_store  = CODE_FOR_reload_sd_si_store;\n-\t      reg_addr[SDmode].reload_load   = CODE_FOR_reload_sd_si_load;\n-\t      if (TARGET_UPPER_REGS_SF)\n-\t\treg_addr[SFmode].scalar_in_vmx_p = true;\n-\t    }\n+\t  reg_addr[DFmode].reload_store    = CODE_FOR_reload_df_si_store;\n+\t  reg_addr[DFmode].reload_load     = CODE_FOR_reload_df_si_load;\n+\t  reg_addr[DDmode].reload_store    = CODE_FOR_reload_dd_si_store;\n+\t  reg_addr[DDmode].reload_load     = CODE_FOR_reload_dd_si_load;\n+\t  reg_addr[SFmode].reload_store    = CODE_FOR_reload_sf_si_store;\n+\t  reg_addr[SFmode].reload_load     = CODE_FOR_reload_sf_si_load;\n+\t  reg_addr[SDmode].reload_store    = CODE_FOR_reload_sd_si_store;\n+\t  reg_addr[SDmode].reload_load     = CODE_FOR_reload_sd_si_load;\n+\n \t  if (TARGET_VSX_TIMODE)\n \t    {\n \t      reg_addr[TImode].reload_store  = CODE_FOR_reload_ti_si_store;\n \t      reg_addr[TImode].reload_load   = CODE_FOR_reload_ti_si_load;\n \t    }\n+\n+\t  if (TARGET_DIRECT_MOVE)\n+\t    {\n+\t      reg_addr[DImode].reload_fpr_gpr = CODE_FOR_reload_fpr_from_gprdi;\n+\t      reg_addr[DDmode].reload_fpr_gpr = CODE_FOR_reload_fpr_from_gprdd;\n+\t      reg_addr[DFmode].reload_fpr_gpr = CODE_FOR_reload_fpr_from_gprdf;\n+\t    }\n \t}\n+\n+      if (TARGET_UPPER_REGS_DF)\n+\treg_addr[DFmode].scalar_in_vmx_p = true;\n+\n+      if (TARGET_UPPER_REGS_SF)\n+\treg_addr[SFmode].scalar_in_vmx_p = true;\n     }\n \n   /* Precalculate HARD_REGNO_NREGS.  */\n@@ -3470,6 +3506,54 @@ rs6000_option_override_internal (bool global_init_p)\n       rs6000_isa_flags &= ~OPTION_MASK_DFP;\n     }\n \n+  /* Allow an explicit -mupper-regs to set both -mupper-regs-df and\n+     -mupper-regs-sf, depending on the cpu, unless the user explicitly also set\n+     the individual option.  */\n+  if (TARGET_UPPER_REGS > 0)\n+    {\n+      if (TARGET_VSX\n+\t  && !(rs6000_isa_flags_explicit & OPTION_MASK_UPPER_REGS_DF))\n+\t{\n+\t  rs6000_isa_flags |= OPTION_MASK_UPPER_REGS_DF;\n+\t  rs6000_isa_flags_explicit |= OPTION_MASK_UPPER_REGS_DF;\n+\t}\n+      if (TARGET_P8_VECTOR\n+\t  && !(rs6000_isa_flags_explicit & OPTION_MASK_UPPER_REGS_SF))\n+\t{\n+\t  rs6000_isa_flags |= OPTION_MASK_UPPER_REGS_SF;\n+\t  rs6000_isa_flags_explicit |= OPTION_MASK_UPPER_REGS_SF;\n+\t}\n+    }\n+  else if (TARGET_UPPER_REGS == 0)\n+    {\n+      if (TARGET_VSX\n+\t  && !(rs6000_isa_flags_explicit & OPTION_MASK_UPPER_REGS_DF))\n+\t{\n+\t  rs6000_isa_flags &= ~OPTION_MASK_UPPER_REGS_DF;\n+\t  rs6000_isa_flags_explicit |= OPTION_MASK_UPPER_REGS_DF;\n+\t}\n+      if (TARGET_P8_VECTOR\n+\t  && !(rs6000_isa_flags_explicit & OPTION_MASK_UPPER_REGS_SF))\n+\t{\n+\t  rs6000_isa_flags &= ~OPTION_MASK_UPPER_REGS_SF;\n+\t  rs6000_isa_flags_explicit |= OPTION_MASK_UPPER_REGS_SF;\n+\t}\n+    }\n+\n+  if (TARGET_UPPER_REGS_DF && !TARGET_VSX)\n+    {\n+      if (rs6000_isa_flags_explicit & OPTION_MASK_UPPER_REGS_DF)\n+\terror (\"-mupper-regs-df requires -mvsx\");\n+      rs6000_isa_flags &= ~OPTION_MASK_UPPER_REGS_DF;\n+    }\n+\n+  if (TARGET_UPPER_REGS_SF && !TARGET_P8_VECTOR)\n+    {\n+      if (rs6000_isa_flags_explicit & OPTION_MASK_UPPER_REGS_SF)\n+\terror (\"-mupper-regs-sf requires -mpower8-vector\");\n+      rs6000_isa_flags &= ~OPTION_MASK_UPPER_REGS_SF;\n+    }\n+\n   /* The quad memory instructions only works in 64-bit mode. In 32-bit mode,\n      silently turn off quad memory mode.  */\n   if ((TARGET_QUAD_MEMORY || TARGET_QUAD_MEMORY_ATOMIC) && !TARGET_POWERPC64)\n@@ -16401,6 +16485,278 @@ register_to_reg_type (rtx reg, bool *is_altivec)\n   return reg_class_to_reg_type[(int)rclass];\n }\n \n+/* Helper function to return the cost of adding a TOC entry address.  */\n+\n+static inline int\n+rs6000_secondary_reload_toc_costs (addr_mask_type addr_mask)\n+{\n+  int ret;\n+\n+  if (TARGET_CMODEL != CMODEL_SMALL)\n+    ret = ((addr_mask & RELOAD_REG_OFFSET) == 0) ? 1 : 2;\n+\n+  else\n+    ret = (TARGET_MINIMAL_TOC) ? 6 : 3;\n+\n+  return ret;\n+}\n+\n+/* Helper function for rs6000_secondary_reload to determine whether the memory\n+   address (ADDR) with a given register class (RCLASS) and machine mode (MODE)\n+   needs reloading.  Return negative if the memory is not handled by the memory\n+   helper functions and to try a different reload method, 0 if no additional\n+   instructions are need, and positive to give the extra cost for the\n+   memory.  */\n+\n+static int\n+rs6000_secondary_reload_memory (rtx addr,\n+\t\t\t\tenum reg_class rclass,\n+\t\t\t\tenum machine_mode mode)\n+{\n+  int extra_cost = 0;\n+  rtx reg, and_arg, plus_arg0, plus_arg1;\n+  addr_mask_type addr_mask;\n+  const char *type = NULL;\n+  const char *fail_msg = NULL;\n+\n+  if (GPR_REG_CLASS_P (rclass))\n+    addr_mask = reg_addr[mode].addr_mask[RELOAD_REG_GPR];\n+\n+  else if (rclass == FLOAT_REGS)\n+    addr_mask = reg_addr[mode].addr_mask[RELOAD_REG_FPR];\n+\n+  else if (rclass == ALTIVEC_REGS)\n+    addr_mask = reg_addr[mode].addr_mask[RELOAD_REG_VMX];\n+\n+  /* For the combined VSX_REGS, turn off Altivec AND -16.  */\n+  else if (rclass == VSX_REGS)\n+    addr_mask = (reg_addr[mode].addr_mask[RELOAD_REG_VMX]\n+\t\t & ~RELOAD_REG_AND_M16);\n+\n+  else\n+    {\n+      if (TARGET_DEBUG_ADDR)\n+\tfprintf (stderr,\n+\t\t \"rs6000_secondary_reload_memory: mode = %s, class = %s, \"\n+\t\t \"class is not GPR, FPR, VMX\\n\",\n+\t\t GET_MODE_NAME (mode), reg_class_names[rclass]);\n+\n+      return -1;\n+    }\n+\n+  /* If the register isn't valid in this register class, just return now.  */\n+  if ((addr_mask & RELOAD_REG_VALID) == 0)\n+    {\n+      if (TARGET_DEBUG_ADDR)\n+\tfprintf (stderr,\n+\t\t \"rs6000_secondary_reload_memory: mode = %s, class = %s, \"\n+\t\t \"not valid in class\\n\",\n+\t\t GET_MODE_NAME (mode), reg_class_names[rclass]);\n+\n+      return -1;\n+    }\n+\n+  switch (GET_CODE (addr))\n+    {\n+      /* Does the register class supports auto update forms for this mode?  We\n+\t don't need a scratch register, since the powerpc only supports\n+\t PRE_INC, PRE_DEC, and PRE_MODIFY.  */\n+    case PRE_INC:\n+    case PRE_DEC:\n+      reg = XEXP (addr, 0);\n+      if (!base_reg_operand (addr, GET_MODE (reg)))\n+\t{\n+\t  fail_msg = \"no base register #1\";\n+\t  extra_cost = -1;\n+\t}\n+\n+      else if ((addr_mask & RELOAD_REG_PRE_INCDEC) == 0)\n+\t{\n+\t  extra_cost = 1;\n+\t  type = \"update\";\n+\t}\n+      break;\n+\n+    case PRE_MODIFY:\n+      reg = XEXP (addr, 0);\n+      plus_arg1 = XEXP (addr, 1);\n+      if (!base_reg_operand (reg, GET_MODE (reg))\n+\t  || GET_CODE (plus_arg1) != PLUS\n+\t  || !rtx_equal_p (reg, XEXP (plus_arg1, 0)))\n+\t{\n+\t  fail_msg = \"bad PRE_MODIFY\";\n+\t  extra_cost = -1;\n+\t}\n+\n+      else if ((addr_mask & RELOAD_REG_PRE_MODIFY) == 0)\n+\t{\n+\t  extra_cost = 1;\n+\t  type = \"update\";\n+\t}\n+      break;\n+\n+      /* Do we need to simulate AND -16 to clear the bottom address bits used\n+\t in VMX load/stores?  Only allow the AND for vector sizes.  */\n+    case AND:\n+      and_arg = XEXP (addr, 0);\n+      if (GET_MODE_SIZE (mode) != 16\n+\t  || GET_CODE (XEXP (addr, 1)) != CONST_INT\n+\t  || INTVAL (XEXP (addr, 1)) != -16)\n+\t{\n+\t  fail_msg = \"bad Altivec AND #1\";\n+\t  extra_cost = -1;\n+\t}\n+\n+      if (rclass != ALTIVEC_REGS)\n+\t{\n+\t  if (legitimate_indirect_address_p (and_arg, false))\n+\t    extra_cost = 1;\n+\n+\t  else if (legitimate_indexed_address_p (and_arg, false))\n+\t    extra_cost = 2;\n+\n+\t  else\n+\t    {\n+\t      fail_msg = \"bad Altivec AND #2\";\n+\t      extra_cost = -1;\n+\t    }\n+\n+\t  type = \"and\";\n+\t}\n+      break;\n+\n+      /* If this is an indirect address, make sure it is a base register.  */\n+    case REG:\n+    case SUBREG:\n+      if (!legitimate_indirect_address_p (addr, false))\n+\t{\n+\t  extra_cost = 1;\n+\t  type = \"move\";\n+\t}\n+      break;\n+\n+      /* If this is an indexed address, make sure the register class can handle\n+\t indexed addresses for this mode.  */\n+    case PLUS:\n+      plus_arg0 = XEXP (addr, 0);\n+      plus_arg1 = XEXP (addr, 1);\n+\n+      /* (plus (plus (reg) (constant)) (constant)) is generated during\n+\t push_reload processing, so handle it now.  */\n+      if (GET_CODE (plus_arg0) == PLUS && CONST_INT_P (plus_arg1))\n+\t{\n+\t  if ((addr_mask & RELOAD_REG_OFFSET) == 0)\n+\t    {\n+\t      extra_cost = 1;\n+\t      type = \"offset\";\n+\t    }\n+\t}\n+\n+      else if (!base_reg_operand (plus_arg0, GET_MODE (plus_arg0)))\n+\t{\n+\t  fail_msg = \"no base register #2\";\n+\t  extra_cost = -1;\n+\t}\n+\n+      else if (int_reg_operand (plus_arg1, GET_MODE (plus_arg1)))\n+\t{\n+\t  if ((addr_mask & RELOAD_REG_INDEXED) == 0\n+\t      || !legitimate_indexed_address_p (addr, false))\n+\t    {\n+\t      extra_cost = 1;\n+\t      type = \"indexed\";\n+\t    }\n+\t}\n+\n+      /* Make sure the register class can handle offset addresses.  */\n+      else if (rs6000_legitimate_offset_address_p (mode, addr, false, true))\n+\t{\n+\t  if ((addr_mask & RELOAD_REG_OFFSET) == 0)\n+\t    {\n+\t      extra_cost = 1;\n+\t      type = \"offset\";\n+\t    }\n+\t}\n+\n+      else\n+\t{\n+\t  fail_msg = \"bad PLUS\";\n+\t  extra_cost = -1;\n+\t}\n+\n+      break;\n+\n+    case LO_SUM:\n+      if (!legitimate_lo_sum_address_p (mode, addr, false))\n+\t{\n+\t  fail_msg = \"bad LO_SUM\";\n+\t  extra_cost = -1;\n+\t}\n+\n+      if ((addr_mask & RELOAD_REG_OFFSET) == 0)\n+\t{\n+\t  extra_cost = 1;\n+\t  type = \"lo_sum\";\n+\t}\n+      break;\n+\n+      /* Static addresses need to create a TOC entry.  */\n+    case CONST:\n+    case SYMBOL_REF:\n+    case LABEL_REF:\n+      type = \"address\";\n+      extra_cost = rs6000_secondary_reload_toc_costs (addr_mask);\n+      break;\n+\n+      /* TOC references look like offsetable memory.  */\n+    case UNSPEC:\n+      if (TARGET_CMODEL == CMODEL_SMALL || XINT (addr, 1) != UNSPEC_TOCREL)\n+\t{\n+\t  fail_msg = \"bad UNSPEC\";\n+\t  extra_cost = -1;\n+\t}\n+\n+      else if ((addr_mask & RELOAD_REG_OFFSET) == 0)\n+\t{\n+\t  extra_cost = 1;\n+\t  type = \"toc reference\";\n+\t}\n+      break;\n+\n+    default:\n+\t{\n+\t  fail_msg = \"bad address\";\n+\t  extra_cost = -1;\n+\t}\n+    }\n+\n+  if (TARGET_DEBUG_ADDR /* && extra_cost != 0 */)\n+    {\n+      if (extra_cost < 0)\n+\tfprintf (stderr,\n+\t\t \"rs6000_secondary_reload_memory error: mode = %s, \"\n+\t\t \"class = %s, addr_mask = '%s', %s\\n\",\n+\t\t GET_MODE_NAME (mode),\n+\t\t reg_class_names[rclass],\n+\t\t rs6000_debug_addr_mask (addr_mask, false),\n+\t\t (fail_msg != NULL) ? fail_msg : \"<bad address>\");\n+\n+      else\n+\tfprintf (stderr,\n+\t\t \"rs6000_secondary_reload_memory: mode = %s, class = %s, \"\n+\t\t \"addr_mask = '%s', extra cost = %d, %s\\n\",\n+\t\t GET_MODE_NAME (mode),\n+\t\t reg_class_names[rclass],\n+\t\t rs6000_debug_addr_mask (addr_mask, false),\n+\t\t extra_cost,\n+\t\t (type) ? type : \"<none>\");\n+\n+      debug_rtx (addr);\n+    }\n+\n+  return extra_cost;\n+}\n+\n /* Helper function for rs6000_secondary_reload to return true if a move to a\n    different register classe is really a simple move.  */\n \n@@ -16607,8 +16963,15 @@ rs6000_secondary_reload (bool in_p,\n   reg_class_t ret = ALL_REGS;\n   enum insn_code icode;\n   bool default_p = false;\n+  bool done_p = false;\n+\n+  /* Allow subreg of memory before/during reload.  */\n+  bool memory_p = (MEM_P (x)\n+\t\t   || (!reload_completed && GET_CODE (x) == SUBREG\n+\t\t       && MEM_P (SUBREG_REG (x))));\n \n   sri->icode = CODE_FOR_nothing;\n+  sri->extra_cost = 0;\n   icode = ((in_p)\n \t   ? reg_addr[mode].reload_load\n \t   : reg_addr[mode].reload_store);\n@@ -16632,121 +16995,54 @@ rs6000_secondary_reload (bool in_p,\n \t{\n \t  icode = (enum insn_code)sri->icode;\n \t  default_p = false;\n+\t  done_p = true;\n \t  ret = NO_REGS;\n \t}\n     }\n \n-  /* Handle vector moves with reload helper functions.  */\n-  if (ret == ALL_REGS && icode != CODE_FOR_nothing)\n+  /* Make sure 0.0 is not reloaded or forced into memory.  */\n+  if (x == CONST0_RTX (mode) && VSX_REG_CLASS_P (rclass))\n     {\n       ret = NO_REGS;\n-      sri->icode = CODE_FOR_nothing;\n-      sri->extra_cost = 0;\n+      default_p = false;\n+      done_p = true;\n+    }\n \n-      if (GET_CODE (x) == MEM)\n-\t{\n-\t  rtx addr = XEXP (x, 0);\n+  /* If this is a scalar floating point value and we want to load it into the\n+     traditional Altivec registers, do it via a move via a traditional floating\n+     point register.  Also make sure that non-zero constants use a FPR.  */\n+  if (!done_p && reg_addr[mode].scalar_in_vmx_p\n+      && (rclass == VSX_REGS || rclass == ALTIVEC_REGS)\n+      && (memory_p || (GET_CODE (x) == CONST_DOUBLE)))\n+    {\n+      ret = FLOAT_REGS;\n+      default_p = false;\n+      done_p = true;\n+    }\n \n-\t  /* Loads to and stores from gprs can do reg+offset, and wouldn't need\n-\t     an extra register in that case, but it would need an extra\n-\t     register if the addressing is reg+reg or (reg+reg)&(-16).  Special\n-\t     case load/store quad.  */\n-\t  if (rclass == GENERAL_REGS || rclass == BASE_REGS)\n-\t    {\n-\t      if (TARGET_POWERPC64 && TARGET_QUAD_MEMORY\n-\t\t  && GET_MODE_SIZE (mode) == 16\n-\t\t  && quad_memory_operand (x, mode))\n-\t\t{\n-\t\t  sri->icode = icode;\n-\t\t  sri->extra_cost = 2;\n-\t\t}\n+  /* Handle reload of load/stores if we have reload helper functions.  */\n+  if (!done_p && icode != CODE_FOR_nothing && memory_p)\n+    {\n+      int extra_cost = rs6000_secondary_reload_memory (XEXP (x, 0), rclass,\n+\t\t\t\t\t\t       mode);\n \n-\t      else if (!legitimate_indirect_address_p (addr, false)\n-\t\t       && !rs6000_legitimate_offset_address_p (PTImode, addr,\n-\t\t\t\t\t\t\t       false, true))\n-\t\t{\n-\t\t  sri->icode = icode;\n-\t\t  /* account for splitting the loads, and converting the\n-\t\t     address from reg+reg to reg.  */\n-\t\t  sri->extra_cost = (((TARGET_64BIT) ? 3 : 5)\n-\t\t\t\t     + ((GET_CODE (addr) == AND) ? 1 : 0));\n-\t\t}\n-\t    }\n-         /* Allow scalar loads to/from the traditional floating point\n-            registers, even if VSX memory is set.  */\n-         else if ((rclass == FLOAT_REGS || rclass == NO_REGS)\n-                  && (GET_MODE_SIZE (mode) == 4 || GET_MODE_SIZE (mode) == 8)\n-                  && (legitimate_indirect_address_p (addr, false)\n-                      || legitimate_indirect_address_p (addr, false)\n-                      || rs6000_legitimate_offset_address_p (mode, addr,\n-                                                             false, true)))\n-\n-           ;\n-         /* Loads to and stores from vector registers can only do reg+reg\n-            addressing.  Altivec registers can also do (reg+reg)&(-16).  Allow\n-            scalar modes loading up the traditional floating point registers\n-            to use offset addresses.  */\n-\t  else if (rclass == VSX_REGS || rclass == ALTIVEC_REGS\n-\t\t   || rclass == FLOAT_REGS || rclass == NO_REGS)\n-\t    {\n-\t      if (!VECTOR_MEM_ALTIVEC_P (mode)\n-\t\t  && GET_CODE (addr) == AND\n-\t\t  && GET_CODE (XEXP (addr, 1)) == CONST_INT\n-\t\t  && INTVAL (XEXP (addr, 1)) == -16\n-\t\t  && (legitimate_indirect_address_p (XEXP (addr, 0), false)\n-\t\t      || legitimate_indexed_address_p (XEXP (addr, 0), false)))\n-\t\t{\n-\t\t  sri->icode = icode;\n-\t\t  sri->extra_cost = ((GET_CODE (XEXP (addr, 0)) == PLUS)\n-\t\t\t\t     ? 2 : 1);\n-\t\t}\n-\t      else if (!legitimate_indirect_address_p (addr, false)\n-\t\t       && (rclass == NO_REGS\n-\t\t\t   || !legitimate_indexed_address_p (addr, false)))\n-\t\t{\n-\t\t  sri->icode = icode;\n-\t\t  sri->extra_cost = 1;\n-\t\t}\n-\t      else\n-\t\ticode = CODE_FOR_nothing;\n-\t    }\n-\t  /* Any other loads, including to pseudo registers which haven't been\n-\t     assigned to a register yet, default to require a scratch\n-\t     register.  */\n-\t  else\n-\t    {\n-\t      sri->icode = icode;\n-\t      sri->extra_cost = 2;\n-\t    }\n-\t}\n-      else if (REG_P (x))\n+      if (extra_cost >= 0)\n \t{\n-\t  int regno = true_regnum (x);\n-\n-\t  icode = CODE_FOR_nothing;\n-\t  if (regno < 0 || regno >= FIRST_PSEUDO_REGISTER)\n-\t    default_p = true;\n-\t  else\n+\t  done_p = true;\n+\t  ret = NO_REGS;\n+\t  if (extra_cost > 0)\n \t    {\n-\t      enum reg_class xclass = REGNO_REG_CLASS (regno);\n-\t      enum rs6000_reg_type rtype1 = reg_class_to_reg_type[(int)rclass];\n-\t      enum rs6000_reg_type rtype2 = reg_class_to_reg_type[(int)xclass];\n-\n-\t      /* If memory is needed, use default_secondary_reload to create the\n-\t\t stack slot.  */\n-\t      if (rtype1 != rtype2 || !IS_STD_REG_TYPE (rtype1))\n-\t\tdefault_p = true;\n-\t      else\n-\t\tret = NO_REGS;\n+\t      sri->extra_cost = extra_cost;\n+\t      sri->icode = icode;\n \t    }\n \t}\n-      else\n-\tdefault_p = true;\n     }\n-  else if (TARGET_POWERPC64\n-\t   && reg_class_to_reg_type[(int)rclass] == GPR_REG_TYPE\n-\t   && MEM_P (x)\n-\t   && GET_MODE_SIZE (GET_MODE (x)) >= UNITS_PER_WORD)\n+\n+  /* Handle unaligned loads and stores of integer registers.  */\n+  if (!done_p && TARGET_POWERPC64\n+      && reg_class_to_reg_type[(int)rclass] == GPR_REG_TYPE\n+      && memory_p\n+      && GET_MODE_SIZE (GET_MODE (x)) >= UNITS_PER_WORD)\n     {\n       rtx addr = XEXP (x, 0);\n       rtx off = address_offset (addr);\n@@ -16775,17 +17071,19 @@ rs6000_secondary_reload (bool in_p,\n \t\tsri->icode = CODE_FOR_reload_di_store;\n \t      sri->extra_cost = 2;\n \t      ret = NO_REGS;\n+\t      done_p = true;\n \t    }\n \t  else\n \t    default_p = true;\n \t}\n       else\n \tdefault_p = true;\n     }\n-  else if (!TARGET_POWERPC64\n-\t   && reg_class_to_reg_type[(int)rclass] == GPR_REG_TYPE\n-\t   && MEM_P (x)\n-\t   && GET_MODE_SIZE (GET_MODE (x)) > UNITS_PER_WORD)\n+\n+  if (!done_p && !TARGET_POWERPC64\n+      && reg_class_to_reg_type[(int)rclass] == GPR_REG_TYPE\n+      && memory_p\n+      && GET_MODE_SIZE (GET_MODE (x)) > UNITS_PER_WORD)\n     {\n       rtx addr = XEXP (x, 0);\n       rtx off = address_offset (addr);\n@@ -16821,14 +17119,16 @@ rs6000_secondary_reload (bool in_p,\n \t\tsri->icode = CODE_FOR_reload_si_store;\n \t      sri->extra_cost = 2;\n \t      ret = NO_REGS;\n+\t      done_p = true;\n \t    }\n \t  else\n \t    default_p = true;\n \t}\n       else\n \tdefault_p = true;\n     }\n-  else\n+\n+  if (!done_p)\n     default_p = true;\n \n   if (default_p)\n@@ -16846,15 +17146,20 @@ rs6000_secondary_reload (bool in_p,\n \t       reg_class_names[rclass],\n \t       GET_MODE_NAME (mode));\n \n+      if (reload_completed)\n+\tfputs (\", after reload\", stderr);\n+\n+      if (!done_p)\n+\tfputs (\", done_p not set\", stderr);\n+\n       if (default_p)\n-\tfprintf (stderr, \", default secondary reload\");\n+\tfputs (\", default secondary reload\", stderr);\n \n       if (sri->icode != CODE_FOR_nothing)\n-\tfprintf (stderr, \", reload func = %s, extra cost = %d\\n\",\n+\tfprintf (stderr, \", reload func = %s, extra cost = %d\",\n \t\t insn_data[sri->icode].name, sri->extra_cost);\n-      else\n-\tfprintf (stderr, \"\\n\");\n \n+      fputs (\"\\n\", stderr);\n       debug_rtx (x);\n     }\n \n@@ -16883,6 +17188,9 @@ rs6000_secondary_reload_trace (int line, rtx reg, rtx mem, rtx scratch,\n   debug_rtx (gen_rtx_PARALLEL (VOIDmode, gen_rtvec (2, set, clobber)));\n }\n \n+static void rs6000_secondary_reload_fail (int, rtx, rtx, rtx, bool)\n+  ATTRIBUTE_NORETURN;\n+\n static void\n rs6000_secondary_reload_fail (int line, rtx reg, rtx mem, rtx scratch,\n \t\t\t      bool store_p)\n@@ -16891,265 +17199,205 @@ rs6000_secondary_reload_fail (int line, rtx reg, rtx mem, rtx scratch,\n   gcc_unreachable ();\n }\n \n-/* Fixup reload addresses for Altivec or VSX loads/stores to change SP+offset\n-   to SP+reg addressing.  */\n+/* Fixup reload addresses for values in GPR, FPR, and VMX registers that have\n+   reload helper functions.  These were identified in\n+   rs6000_secondary_reload_memory, and if reload decided to use the secondary\n+   reload, it calls the insns:\n+\treload_<RELOAD:mode>_<P:mptrsize>_store\n+\treload_<RELOAD:mode>_<P:mptrsize>_load\n+\n+   which in turn calls this function, to do whatever is necessary to create\n+   valid addresses.  */\n \n void\n rs6000_secondary_reload_inner (rtx reg, rtx mem, rtx scratch, bool store_p)\n {\n   int regno = true_regnum (reg);\n   machine_mode mode = GET_MODE (reg);\n-  enum reg_class rclass;\n+  addr_mask_type addr_mask;\n   rtx addr;\n-  rtx and_op2 = NULL_RTX;\n-  rtx addr_op1;\n-  rtx addr_op2;\n-  rtx scratch_or_premodify = scratch;\n-  rtx and_rtx;\n+  rtx new_addr;\n+  rtx op_reg, op0, op1;\n+  rtx and_op;\n   rtx cc_clobber;\n+  rtvec rv;\n \n-  if (TARGET_DEBUG_ADDR)\n-    rs6000_secondary_reload_trace (__LINE__, reg, mem, scratch, store_p);\n+  if (regno < 0 || regno >= FIRST_PSEUDO_REGISTER || !MEM_P (mem)\n+      || !base_reg_operand (scratch, GET_MODE (scratch)))\n+    rs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n+\n+  if (IN_RANGE (regno, FIRST_GPR_REGNO, LAST_GPR_REGNO))\n+    addr_mask = reg_addr[mode].addr_mask[RELOAD_REG_GPR];\n+\n+  else if (IN_RANGE (regno, FIRST_FPR_REGNO, LAST_FPR_REGNO))\n+    addr_mask = reg_addr[mode].addr_mask[RELOAD_REG_FPR];\n \n-  if (regno < 0 || regno >= FIRST_PSEUDO_REGISTER)\n+  else if (IN_RANGE (regno, FIRST_ALTIVEC_REGNO, LAST_ALTIVEC_REGNO))\n+    addr_mask = reg_addr[mode].addr_mask[RELOAD_REG_VMX];\n+\n+  else\n     rs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n \n-  if (GET_CODE (mem) != MEM)\n+  /* Make sure the mode is valid in this register class.  */\n+  if ((addr_mask & RELOAD_REG_VALID) == 0)\n     rs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n \n-  rclass = REGNO_REG_CLASS (regno);\n-  addr = find_replacement (&XEXP (mem, 0));\n+  if (TARGET_DEBUG_ADDR)\n+    rs6000_secondary_reload_trace (__LINE__, reg, mem, scratch, store_p);\n \n-  switch (rclass)\n+  new_addr = addr = XEXP (mem, 0);\n+  switch (GET_CODE (addr))\n     {\n-      /* GPRs can handle reg + small constant, all other addresses need to use\n-\t the scratch register.  */\n-    case GENERAL_REGS:\n-    case BASE_REGS:\n-      if (GET_CODE (addr) == AND)\n+      /* Does the register class support auto update forms for this mode?  If\n+\t not, do the update now.  We don't need a scratch register, since the\n+\t powerpc only supports PRE_INC, PRE_DEC, and PRE_MODIFY.  */\n+    case PRE_INC:\n+    case PRE_DEC:\n+      op_reg = XEXP (addr, 0);\n+      if (!base_reg_operand (op_reg, Pmode))\n+\trs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n+\n+      if ((addr_mask & RELOAD_REG_PRE_INCDEC) == 0)\n \t{\n-\t  and_op2 = XEXP (addr, 1);\n-\t  addr = find_replacement (&XEXP (addr, 0));\n+\t  emit_insn (gen_add2_insn (op_reg, GEN_INT (GET_MODE_SIZE (mode))));\n+\t  new_addr = op_reg;\n \t}\n+      break;\n \n-      if (GET_CODE (addr) == PRE_MODIFY)\n-\t{\n-\t  scratch_or_premodify = find_replacement (&XEXP (addr, 0));\n-\t  if (!REG_P (scratch_or_premodify))\n-\t    rs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n+    case PRE_MODIFY:\n+      op0 = XEXP (addr, 0);\n+      op1 = XEXP (addr, 1);\n+      if (!base_reg_operand (op0, Pmode)\n+\t  || GET_CODE (op1) != PLUS\n+\t  || !rtx_equal_p (op0, XEXP (op1, 0)))\n+\trs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n \n-\t  addr = find_replacement (&XEXP (addr, 1));\n-\t  if (GET_CODE (addr) != PLUS)\n-\t    rs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n+      if ((addr_mask & RELOAD_REG_PRE_MODIFY) == 0)\n+\t{\n+\t  emit_insn (gen_rtx_SET (VOIDmode, op0, op1));\n+\t  new_addr = reg;\n \t}\n+      break;\n \n-      if (GET_CODE (addr) == PLUS\n-\t  && (and_op2 != NULL_RTX\n-\t      || !rs6000_legitimate_offset_address_p (PTImode, addr,\n-\t\t\t\t\t\t      false, true)))\n+      /* Do we need to simulate AND -16 to clear the bottom address bits used\n+\t in VMX load/stores?  */\n+    case AND:\n+      op0 = XEXP (addr, 0);\n+      op1 = XEXP (addr, 1);\n+      if ((addr_mask & RELOAD_REG_AND_M16) == 0)\n \t{\n-\t  /* find_replacement already recurses into both operands of\n-\t     PLUS so we don't need to call it here.  */\n-\t  addr_op1 = XEXP (addr, 0);\n-\t  addr_op2 = XEXP (addr, 1);\n-\t  if (!legitimate_indirect_address_p (addr_op1, false))\n-\t    rs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n+\t  if (REG_P (op0) || GET_CODE (op0) == SUBREG)\n+\t    op_reg = op0;\n \n-\t  if (!REG_P (addr_op2)\n-\t      && (GET_CODE (addr_op2) != CONST_INT\n-\t\t  || !satisfies_constraint_I (addr_op2)))\n+\t  else if (GET_CODE (op1) == PLUS)\n \t    {\n-\t      if (TARGET_DEBUG_ADDR)\n-\t\t{\n-\t\t  fprintf (stderr,\n-\t\t\t   \"\\nMove plus addr to register %s, mode = %s: \",\n-\t\t\t   rs6000_reg_names[REGNO (scratch)],\n-\t\t\t   GET_MODE_NAME (mode));\n-\t\t  debug_rtx (addr_op2);\n-\t\t}\n-\t      rs6000_emit_move (scratch, addr_op2, Pmode);\n-\t      addr_op2 = scratch;\n+\t      emit_insn (gen_rtx_SET (VOIDmode, scratch, op1));\n+\t      op_reg = scratch;\n \t    }\n \n-\t  emit_insn (gen_rtx_SET (VOIDmode,\n-\t\t\t\t  scratch_or_premodify,\n-\t\t\t\t  gen_rtx_PLUS (Pmode,\n-\t\t\t\t\t\taddr_op1,\n-\t\t\t\t\t\taddr_op2)));\n+\t  else\n+\t    rs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n \n-\t  addr = scratch_or_premodify;\n-\t  scratch_or_premodify = scratch;\n-\t}\n-      else if (!legitimate_indirect_address_p (addr, false)\n-\t       && !rs6000_legitimate_offset_address_p (PTImode, addr,\n-\t\t\t\t\t\t       false, true))\n-\t{\n-\t  if (TARGET_DEBUG_ADDR)\n-\t    {\n-\t      fprintf (stderr, \"\\nMove addr to register %s, mode = %s: \",\n-\t\t       rs6000_reg_names[REGNO (scratch_or_premodify)],\n-\t\t       GET_MODE_NAME (mode));\n-\t      debug_rtx (addr);\n-\t    }\n-\t  rs6000_emit_move (scratch_or_premodify, addr, Pmode);\n-\t  addr = scratch_or_premodify;\n-\t  scratch_or_premodify = scratch;\n+\t  and_op = gen_rtx_AND (GET_MODE (scratch), op_reg, op1);\n+\t  cc_clobber = gen_rtx_CLOBBER (VOIDmode, gen_rtx_SCRATCH (CCmode));\n+\t  rv = gen_rtvec (2, gen_rtx_SET (VOIDmode, scratch, and_op), cc_clobber);\n+\t  emit_insn (gen_rtx_PARALLEL (VOIDmode, rv));\n+\t  new_addr = scratch;\n \t}\n       break;\n \n-      /* Float registers can do offset+reg addressing for scalar types.  */\n-    case FLOAT_REGS:\n-      if (legitimate_indirect_address_p (addr, false)\t/* reg */\n-\t  || legitimate_indexed_address_p (addr, false)\t/* reg+reg */\n-\t  || ((GET_MODE_SIZE (mode) == 4 || GET_MODE_SIZE (mode) == 8)\n-\t      && and_op2 == NULL_RTX\n-\t      && scratch_or_premodify == scratch\n-\t      && rs6000_legitimate_offset_address_p (mode, addr, false, false)))\n-\tbreak;\n-\n-      /* If this isn't a legacy floating point load/store, fall through to the\n-\t VSX defaults.  */\n-\n-      /* VSX/Altivec registers can only handle reg+reg addressing.  Move other\n-\t addresses into a scratch register.  */\n-    case VSX_REGS:\n-    case ALTIVEC_REGS:\n-\n-      /* With float regs, we need to handle the AND ourselves, since we can't\n-\t use the Altivec instruction with an implicit AND -16.  Allow scalar\n-\t loads to float registers to use reg+offset even if VSX.  */\n-      if (GET_CODE (addr) == AND\n-\t  && (rclass != ALTIVEC_REGS || GET_MODE_SIZE (mode) != 16\n-\t      || GET_CODE (XEXP (addr, 1)) != CONST_INT\n-\t      || INTVAL (XEXP (addr, 1)) != -16\n-\t      || !VECTOR_MEM_ALTIVEC_P (mode)))\n-\t{\n-\t  and_op2 = XEXP (addr, 1);\n-\t  addr = find_replacement (&XEXP (addr, 0));\n-\t}\n-\n-      /* If we aren't using a VSX load, save the PRE_MODIFY register and use it\n-\t as the address later.  */\n-      if (GET_CODE (addr) == PRE_MODIFY\n-\t  && ((ALTIVEC_OR_VSX_VECTOR_MODE (mode)\n-\t       && (rclass != FLOAT_REGS\n-\t\t   || (GET_MODE_SIZE (mode) != 4 && GET_MODE_SIZE (mode) != 8)))\n-\t      || and_op2 != NULL_RTX\n-\t      || !legitimate_indexed_address_p (XEXP (addr, 1), false)))\n-\t{\n-\t  scratch_or_premodify = find_replacement (&XEXP (addr, 0));\n-\t  if (!legitimate_indirect_address_p (scratch_or_premodify, false))\n-\t    rs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n-\n-\t  addr = find_replacement (&XEXP (addr, 1));\n-\t  if (GET_CODE (addr) != PLUS)\n-\t    rs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n+      /* If this is an indirect address, make sure it is a base register.  */\n+    case REG:\n+    case SUBREG:\n+      if (!base_reg_operand (addr, GET_MODE (addr)))\n+\t{\n+\t  emit_insn (gen_rtx_SET (VOIDmode, scratch, addr));\n+\t  new_addr = scratch;\n \t}\n+      break;\n \n-      if (legitimate_indirect_address_p (addr, false)\t/* reg */\n-\t  || legitimate_indexed_address_p (addr, false)\t/* reg+reg */\n-\t  || (GET_CODE (addr) == AND\t\t\t/* Altivec memory */\n-\t      && rclass == ALTIVEC_REGS\n-\t      && GET_CODE (XEXP (addr, 1)) == CONST_INT\n-\t      && INTVAL (XEXP (addr, 1)) == -16\n-\t      && (legitimate_indirect_address_p (XEXP (addr, 0), false)\n-\t\t  || legitimate_indexed_address_p (XEXP (addr, 0), false))))\n-\t;\n+      /* If this is an indexed address, make sure the register class can handle\n+\t indexed addresses for this mode.  */\n+    case PLUS:\n+      op0 = XEXP (addr, 0);\n+      op1 = XEXP (addr, 1);\n+      if (!base_reg_operand (op0, Pmode))\n+\trs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n \n-      else if (GET_CODE (addr) == PLUS)\n+      else if (int_reg_operand (op1, Pmode))\n \t{\n-\t  addr_op1 = XEXP (addr, 0);\n-\t  addr_op2 = XEXP (addr, 1);\n-\t  if (!REG_P (addr_op1))\n-\t    rs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n-\n-\t  if (TARGET_DEBUG_ADDR)\n+\t  if ((addr_mask & RELOAD_REG_INDEXED) == 0)\n \t    {\n-\t      fprintf (stderr, \"\\nMove plus addr to register %s, mode = %s: \",\n-\t\t       rs6000_reg_names[REGNO (scratch)], GET_MODE_NAME (mode));\n-\t      debug_rtx (addr_op2);\n+\t      emit_insn (gen_rtx_SET (VOIDmode, scratch, addr));\n+\t      new_addr = scratch;\n \t    }\n-\t  rs6000_emit_move (scratch, addr_op2, Pmode);\n-\t  emit_insn (gen_rtx_SET (VOIDmode,\n-\t\t\t\t  scratch_or_premodify,\n-\t\t\t\t  gen_rtx_PLUS (Pmode,\n-\t\t\t\t\t\taddr_op1,\n-\t\t\t\t\t\tscratch)));\n-\t  addr = scratch_or_premodify;\n-\t  scratch_or_premodify = scratch;\n \t}\n \n-      else if (GET_CODE (addr) == SYMBOL_REF || GET_CODE (addr) == CONST\n-\t       || GET_CODE (addr) == CONST_INT || GET_CODE (addr) == LO_SUM\n-\t       || REG_P (addr))\n+      /* Make sure the register class can handle offset addresses.  */\n+      else if (rs6000_legitimate_offset_address_p (mode, addr, false, true))\n \t{\n-\t  if (TARGET_DEBUG_ADDR)\n+\t  if ((addr_mask & RELOAD_REG_OFFSET) == 0)\n \t    {\n-\t      fprintf (stderr, \"\\nMove addr to register %s, mode = %s: \",\n-\t\t       rs6000_reg_names[REGNO (scratch_or_premodify)],\n-\t\t       GET_MODE_NAME (mode));\n-\t      debug_rtx (addr);\n+\t      emit_insn (gen_rtx_SET (VOIDmode, scratch, addr));\n+\t      new_addr = scratch;\n \t    }\n-\n-\t  rs6000_emit_move (scratch_or_premodify, addr, Pmode);\n-\t  addr = scratch_or_premodify;\n-\t  scratch_or_premodify = scratch;\n \t}\n \n       else\n \trs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n \n       break;\n \n-    default:\n-      rs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n-    }\n-\n-  /* If the original address involved a pre-modify that we couldn't use the VSX\n-     memory instruction with update, and we haven't taken care of already,\n-     store the address in the pre-modify register and use that as the\n-     address.  */\n-  if (scratch_or_premodify != scratch && scratch_or_premodify != addr)\n-    {\n-      emit_insn (gen_rtx_SET (VOIDmode, scratch_or_premodify, addr));\n-      addr = scratch_or_premodify;\n-    }\n+    case LO_SUM:\n+      op0 = XEXP (addr, 0);\n+      op1 = XEXP (addr, 1);\n+      if (!base_reg_operand (op0, Pmode))\n+\trs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n \n-  /* If the original address involved an AND -16 and we couldn't use an ALTIVEC\n-     memory instruction, recreate the AND now, including the clobber which is\n-     generated by the general ANDSI3/ANDDI3 patterns for the\n-     andi. instruction.  */\n-  if (and_op2 != NULL_RTX)\n-    {\n-      if (! legitimate_indirect_address_p (addr, false))\n+      else if (int_reg_operand (op1, Pmode))\n \t{\n-\t  emit_insn (gen_rtx_SET (VOIDmode, scratch, addr));\n-\t  addr = scratch;\n+\t  if ((addr_mask & RELOAD_REG_INDEXED) == 0)\n+\t    {\n+\t      emit_insn (gen_rtx_SET (VOIDmode, scratch, addr));\n+\t      new_addr = scratch;\n+\t    }\n \t}\n \n-      if (TARGET_DEBUG_ADDR)\n+      /* Make sure the register class can handle offset addresses.  */\n+      else if (legitimate_lo_sum_address_p (mode, addr, false))\n \t{\n-\t  fprintf (stderr, \"\\nAnd addr to register %s, mode = %s: \",\n-\t\t   rs6000_reg_names[REGNO (scratch)], GET_MODE_NAME (mode));\n-\t  debug_rtx (and_op2);\n+\t  if ((addr_mask & RELOAD_REG_OFFSET) == 0)\n+\t    {\n+\t      emit_insn (gen_rtx_SET (VOIDmode, scratch, addr));\n+\t      new_addr = scratch;\n+\t    }\n \t}\n \n-      and_rtx = gen_rtx_SET (VOIDmode,\n-\t\t\t     scratch,\n-\t\t\t     gen_rtx_AND (Pmode,\n-\t\t\t\t\t  addr,\n-\t\t\t\t\t  and_op2));\n+      else\n+\trs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n+\n+      break;\n \n-      cc_clobber = gen_rtx_CLOBBER (CCmode, gen_rtx_SCRATCH (CCmode));\n-      emit_insn (gen_rtx_PARALLEL (VOIDmode,\n-\t\t\t\t   gen_rtvec (2, and_rtx, cc_clobber)));\n-      addr = scratch;\n+    case SYMBOL_REF:\n+    case CONST:\n+    case LABEL_REF:\n+      if (TARGET_TOC)\n+\temit_insn (gen_rtx_SET (VOIDmode, scratch,\n+\t\t\t\tcreate_TOC_reference (addr, scratch)));\n+      else\n+\trs6000_emit_move (scratch, addr, Pmode);\n+\n+      new_addr = scratch;\n+      break;\n+\n+    default:\n+      rs6000_secondary_reload_fail (__LINE__, reg, mem, scratch, store_p);\n     }\n \n   /* Adjust the address if it changed.  */\n-  if (addr != XEXP (mem, 0))\n+  if (addr != new_addr)\n     {\n-      mem = replace_equiv_address_nv (mem, addr);\n+      mem = replace_equiv_address_nv (mem, new_addr);\n       if (TARGET_DEBUG_ADDR)\n \tfprintf (stderr, \"\\nrs6000_secondary_reload_inner, mem adjusted.\\n\");\n     }\n@@ -17294,50 +17542,54 @@ static enum reg_class\n rs6000_preferred_reload_class (rtx x, enum reg_class rclass)\n {\n   machine_mode mode = GET_MODE (x);\n+  bool is_constant = CONSTANT_P (x);\n \n-  if (TARGET_VSX && x == CONST0_RTX (mode) && VSX_REG_CLASS_P (rclass))\n-    return rclass;\n-\n-  if (VECTOR_UNIT_ALTIVEC_OR_VSX_P (mode)\n-      && (rclass == ALTIVEC_REGS || rclass == VSX_REGS)\n-      && easy_vector_constant (x, mode))\n-    return ALTIVEC_REGS;\n-\n-  if ((CONSTANT_P (x) || GET_CODE (x) == PLUS))\n+  /* Do VSX tests before handling traditional floaitng point registers.  */\n+  if (TARGET_VSX && VSX_REG_CLASS_P (rclass))\n     {\n-      if (reg_class_subset_p (GENERAL_REGS, rclass))\n-\treturn GENERAL_REGS;\n-      if (reg_class_subset_p (BASE_REGS, rclass))\n-\treturn BASE_REGS;\n-      return NO_REGS;\n-    }\n+      if (is_constant)\n+\t{\n+\t  /* Zero is always allowed in all VSX registers.  */\n+\t  if (x == CONST0_RTX (mode))\n+\t    return rclass;\n \n-  if (GET_MODE_CLASS (mode) == MODE_INT && rclass == NON_SPECIAL_REGS)\n-    return GENERAL_REGS;\n+\t  /* If this is a vector constant that can be formed with a few Altivec\n+\t     instructions, we want altivec registers.  */\n+\t  if (GET_CODE (x) == CONST_VECTOR && easy_vector_constant (x, mode))\n+\t    return ALTIVEC_REGS;\n \n-  /* For VSX, prefer the traditional registers for 64-bit values because we can\n-     use the non-VSX loads.  Prefer the Altivec registers if Altivec is\n-     handling the vector operations (i.e. V16QI, V8HI, and V4SI), or if we\n-     prefer Altivec loads..  */\n-  if (rclass == VSX_REGS)\n-    {\n-      if (MEM_P (x) && reg_addr[mode].scalar_in_vmx_p)\n-\t{\n-\t  rtx addr = XEXP (x, 0);\n-\t  if (rs6000_legitimate_offset_address_p (mode, addr, false, true)\n-\t      || legitimate_lo_sum_address_p (mode, addr, false))\n-\t    return FLOAT_REGS;\n+\t  /* Force constant to memory.  */\n+\t  return NO_REGS;\n \t}\n-      else if (GET_MODE_SIZE (mode) <= 8 && !reg_addr[mode].scalar_in_vmx_p)\n+\n+      /* If this is a scalar floating point value, prefer the traditional\n+\t floating point registers so that we can use D-form (register+offset)\n+\t addressing.  */\n+      if (GET_MODE_SIZE (mode) < 16)\n \treturn FLOAT_REGS;\n \n+      /* Prefer the Altivec registers if Altivec is handling the vector\n+\t operations (i.e. V16QI, V8HI, and V4SI), or if we prefer Altivec\n+\t loads.  */\n       if (VECTOR_UNIT_ALTIVEC_P (mode) || VECTOR_MEM_ALTIVEC_P (mode)\n \t  || mode == V1TImode)\n \treturn ALTIVEC_REGS;\n \n       return rclass;\n     }\n \n+  if (is_constant || GET_CODE (x) == PLUS)\n+    {\n+      if (reg_class_subset_p (GENERAL_REGS, rclass))\n+\treturn GENERAL_REGS;\n+      if (reg_class_subset_p (BASE_REGS, rclass))\n+\treturn BASE_REGS;\n+      return NO_REGS;\n+    }\n+\n+  if (GET_MODE_CLASS (mode) == MODE_INT && rclass == NON_SPECIAL_REGS)\n+    return GENERAL_REGS;\n+\n   return rclass;\n }\n \n@@ -17457,30 +17709,34 @@ rs6000_secondary_reload_class (enum reg_class rclass, machine_mode mode,\n   else\n     regno = -1;\n \n+  /* If we have VSX register moves, prefer moving scalar values between\n+     Altivec registers and GPR by going via an FPR (and then via memory)\n+     instead of reloading the secondary memory address for Altivec moves.  */\n+  if (TARGET_VSX\n+      && GET_MODE_SIZE (mode) < 16\n+      && (((rclass == GENERAL_REGS || rclass == BASE_REGS)\n+           && (regno >= 0 && ALTIVEC_REGNO_P (regno)))\n+          || ((rclass == VSX_REGS || rclass == ALTIVEC_REGS)\n+              && (regno >= 0 && INT_REGNO_P (regno)))))\n+    return FLOAT_REGS;\n+\n   /* We can place anything into GENERAL_REGS and can put GENERAL_REGS\n      into anything.  */\n   if (rclass == GENERAL_REGS || rclass == BASE_REGS\n       || (regno >= 0 && INT_REGNO_P (regno)))\n     return NO_REGS;\n \n+  /* Constants, memory, and VSX registers can go into VSX registers (both the\n+     traditional floating point and the altivec registers).  */\n+  if (rclass == VSX_REGS\n+      && (regno == -1 || VSX_REGNO_P (regno)))\n+    return NO_REGS;\n+\n   /* Constants, memory, and FP registers can go into FP registers.  */\n   if ((regno == -1 || FP_REGNO_P (regno))\n       && (rclass == FLOAT_REGS || rclass == NON_SPECIAL_REGS))\n     return (mode != SDmode || lra_in_progress) ? NO_REGS : GENERAL_REGS;\n \n-  /* Memory, and FP/altivec registers can go into fp/altivec registers under\n-     VSX.  However, for scalar variables, use the traditional floating point\n-     registers so that we can use offset+register addressing.  */\n-  if (TARGET_VSX\n-      && (regno == -1 || VSX_REGNO_P (regno))\n-      && VSX_REG_CLASS_P (rclass))\n-    {\n-      if (GET_MODE_SIZE (mode) < 16)\n-\treturn FLOAT_REGS;\n-\n-      return NO_REGS;\n-    }\n-\n   /* Memory, and AltiVec registers can go into AltiVec registers.  */\n   if ((regno == -1 || ALTIVEC_REGNO_P (regno))\n       && rclass == ALTIVEC_REGS)"}, {"sha": "fe73acff2d4295ba20f335815abf501693b3f9c9", "filename": "gcc/config/rs6000/rs6000.md", "status": "modified", "additions": 21, "deletions": 2, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Fconfig%2Frs6000%2Frs6000.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Fconfig%2Frs6000%2Frs6000.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000.md?ref=25adc5d044047b926181ebcaf6284b39df8ee306", "patch": "@@ -7850,7 +7850,7 @@\n \n (define_insn \"mov<mode>_hardfloat\"\n   [(set (match_operand:FMOVE32 0 \"nonimmediate_operand\" \"=!r,!r,m,f,<f32_vsx>,<f32_vsx>,<f32_lr>,<f32_sm>,<f32_av>,Z,?<f32_dm>,?r,*c*l,!r,*h,!r,!r\")\n-\t(match_operand:FMOVE32 1 \"input_operand\" \"r,m,r,f,<f32_vsx>,j,<f32_lm>,<f32_sr>,Z,<f32_av>,r,<f32_dm>,r, h, 0, G,Fn\"))]\n+\t(match_operand:FMOVE32 1 \"input_operand\" \"r,m,r,f,<f32_vsx>,j,<f32_lm>,<f32_sr>,Z,<f32_av>,r,<f32_dm>,r,h,0,G,Fn\"))]\n   \"(gpc_reg_operand (operands[0], <MODE>mode)\n    || gpc_reg_operand (operands[1], <MODE>mode))\n    && (TARGET_HARD_FLOAT && TARGET_FPRS && TARGET_SINGLE_FLOAT)\"\n@@ -8137,6 +8137,21 @@\n { rs6000_split_multireg_move (operands[0], operands[1]); DONE; }\n   [(set_attr \"length\" \"20,20,16\")])\n \n+;; If we are using -ffast-math, easy_fp_constant assumes all constants are\n+;; 'easy' in order to allow for reciprocal estimation.  Make sure the constant\n+;; is in the constant pool before reload occurs.  This simplifies accessing\n+;; scalars in the traditional Altivec registers.\n+\n+(define_split\n+  [(set (match_operand:SFDF 0 \"register_operand\" \"\")\n+\t(match_operand:SFDF 1 \"memory_fp_constant\" \"\"))]\n+  \"TARGET_<MODE>_FPR && flag_unsafe_math_optimizations\n+   && !reload_in_progress && !reload_completed && !lra_in_progress\"\n+  [(set (match_dup 0) (match_dup 2))]\n+{\n+  operands[2] = validize_mem (force_const_mem (<MODE>mode, operands[1]));\n+})\n+\n (define_expand \"extenddftf2\"\n   [(set (match_operand:TF 0 \"nonimmediate_operand\" \"\")\n \t(float_extend:TF (match_operand:DF 1 \"input_operand\" \"\")))]\n@@ -9816,12 +9831,15 @@\n ;; sequences, using get_attr_length here will smash the operands\n ;; array.  Neither is there an early_cobbler_p predicate.\n ;; Disallow subregs for E500 so we don't munge frob_di_df_2.\n+;; Also this optimization interferes with scalars going into\n+;; altivec registers (the code does reloading through the FPRs).\n (define_peephole2\n   [(set (match_operand:DF 0 \"gpc_reg_operand\" \"\")\n \t(match_operand:DF 1 \"any_operand\" \"\"))\n    (set (match_operand:DF 2 \"gpc_reg_operand\" \"\")\n \t(match_dup 0))]\n   \"!(TARGET_E500_DOUBLE && GET_CODE (operands[2]) == SUBREG)\n+   && !TARGET_UPPER_REGS_DF\n    && peep2_reg_dead_p (2, operands[0])\"\n   [(set (match_dup 2) (match_dup 1))])\n \n@@ -9830,7 +9848,8 @@\n \t(match_operand:SF 1 \"any_operand\" \"\"))\n    (set (match_operand:SF 2 \"gpc_reg_operand\" \"\")\n \t(match_dup 0))]\n-  \"peep2_reg_dead_p (2, operands[0])\"\n+  \"!TARGET_UPPER_REGS_SF\n+   && peep2_reg_dead_p (2, operands[0])\"\n   [(set (match_dup 2) (match_dup 1))])\n \n \f"}, {"sha": "eb3e3237935c0e836ecf74efa5d9c4bf51ae51ce", "filename": "gcc/config/rs6000/rs6000.opt", "status": "modified", "additions": 7, "deletions": 3, "changes": 10, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Fconfig%2Frs6000%2Frs6000.opt", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Fconfig%2Frs6000%2Frs6000.opt", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Frs6000%2Frs6000.opt?ref=25adc5d044047b926181ebcaf6284b39df8ee306", "patch": "@@ -582,12 +582,16 @@ Target Report Var(rs6000_compat_align_parm) Init(0) Save\n Generate aggregate parameter passing code with at most 64-bit alignment.\n \n mupper-regs-df\n-Target Undocumented Mask(UPPER_REGS_DF) Var(rs6000_isa_flags)\n+Target Report Mask(UPPER_REGS_DF) Var(rs6000_isa_flags)\n Allow double variables in upper registers with -mcpu=power7 or -mvsx\n \n mupper-regs-sf\n-Target Undocumented Mask(UPPER_REGS_SF) Var(rs6000_isa_flags)\n-Allow float variables in upper registers with -mcpu=power8 or -mp8-vector\n+Target Report Mask(UPPER_REGS_SF) Var(rs6000_isa_flags)\n+Allow float variables in upper registers with -mcpu=power8 or -mpower8-vector\n+\n+mupper-regs\n+Target Report Var(TARGET_UPPER_REGS) Init(-1) Save\n+Allow float/double variables in upper registers if cpu allows it\n \n moptimize-swaps\n Target Undocumented Var(rs6000_optimize_swaps) Init(1) Save"}, {"sha": "9846a73079d67b89129e55b4dc752537ea1d07e4", "filename": "gcc/doc/invoke.texi", "status": "modified", "additions": 36, "deletions": 1, "changes": 37, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Fdoc%2Finvoke.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Fdoc%2Finvoke.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Finvoke.texi?ref=25adc5d044047b926181ebcaf6284b39df8ee306", "patch": "@@ -940,7 +940,9 @@ See RS/6000 and PowerPC Options.\n -mcrypto -mno-crypto -mdirect-move -mno-direct-move @gol\n -mquad-memory -mno-quad-memory @gol\n -mquad-memory-atomic -mno-quad-memory-atomic @gol\n--mcompat-align-parm -mno-compat-align-parm}\n+-mcompat-align-parm -mno-compat-align-parm @gol\n+-mupper-regs-df -mno-upper-regs-df -mupper-regs-sf -mno-upper-regs-sf @gol\n+-mupper-regs -mno-upper-regs}\n \n @emph{RX Options}\n @gccoptlist{-m64bit-doubles  -m32bit-doubles  -fpu  -nofpu@gol\n@@ -19729,6 +19731,39 @@ Generate code that uses (does not use) the atomic quad word memory\n instructions.  The @option{-mquad-memory-atomic} option requires use of\n 64-bit mode.\n \n+@item -mupper-regs-df\n+@itemx -mno-upper-regs-df\n+@opindex mupper-regs-df\n+@opindex mno-upper-regs-df\n+Generate code that uses (does not use) the scalar double precision\n+instructions that target all 64 registers in the vector/scalar\n+floating point register set that were added in version 2.06 of the\n+PowerPC ISA.  The @option{-mupper-regs-df} turned on by default if you\n+use either of the @option{-mcpu=power7}, @option{-mcpu=power8}, or\n+@option{-mvsx} options.\n+\n+@item -mupper-regs-sf\n+@itemx -mno-upper-regs-sf\n+@opindex mupper-regs-sf\n+@opindex mno-upper-regs-sf\n+Generate code that uses (does not use) the scalar single precision\n+instructions that target all 64 registers in the vector/scalar\n+floating point register set that were added in version 2.07 of the\n+PowerPC ISA.  The @option{-mupper-regs-sf} turned on by default if you\n+use either of the @option{-mcpu=power8}, or @option{-mpower8-vector}\n+options.\n+\n+@item -mupper-regs\n+@itemx -mno-upper-regs\n+@opindex mupper-regs\n+@opindex mno-upper-regs\n+Generate code that uses (does not use) the scalar\n+instructions that target all 64 registers in the vector/scalar\n+floating point register set, depending on the model of the machine.\n+\n+If the @option{-mno-upper-regs} option was used, it will turn off both\n+@option{-mupper-regs-sf} and @option{-mupper-regs-df} options.\n+\n @item -mfloat-gprs=@var{yes/single/double/no}\n @itemx -mfloat-gprs\n @opindex mfloat-gprs"}, {"sha": "e6f11b42d527407e1f307eeb7623b20455fe0d04", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 12, "deletions": 0, "changes": 12, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=25adc5d044047b926181ebcaf6284b39df8ee306", "patch": "@@ -1,3 +1,15 @@\n+2014-11-17  Michael Meissner  <meissner@linux.vnet.ibm.com>\n+\n+\t* gcc.target/powerpc/p8vector-ldst.c: Rewrite to use 40 live\n+\tfloating point variables instead of using asm to test allocating\n+\tvalues to the Altivec registers.\n+\n+\t* gcc.target/powerpc/upper-regs-sf.c: New -mupper-regs-sf and\n+\t-mupper-regs-df tests.\n+\t* gcc.target/powerpc/upper-regs-df.c: Likewise.\n+\n+\t* config/rs6000/predicates.md (memory_fp_constant): New predicate\n+\n 2014-11-17  Tom de Vries  <tom@codesourcery.com>\n \n \t* gcc.dg/pr43864-2.c: Add -ftree-tail-merge to dg-options."}, {"sha": "5da7388097b706d79755b31a4ccad4797a711003", "filename": "gcc/testsuite/gcc.target/powerpc/p8vector-ldst.c", "status": "modified", "additions": 606, "deletions": 25, "changes": 631, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fp8vector-ldst.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fp8vector-ldst.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fp8vector-ldst.c?ref=25adc5d044047b926181ebcaf6284b39df8ee306", "patch": "@@ -1,43 +1,624 @@\n-/* { dg-do compile { target { powerpc*-*-* } } } */\n+/* { dg-do compile { target { powerpc*-*-* && lp64 } } } */\n /* { dg-skip-if \"\" { powerpc*-*-darwin* } { \"*\" } { \"\" } } */\n /* { dg-require-effective-target powerpc_p8vector_ok } */\n /* { dg-skip-if \"do not override -mcpu\" { powerpc*-*-* } { \"-mcpu=*\" } { \"-mcpu=power8\" } } */\n /* { dg-options \"-mcpu=power8 -O2 -mupper-regs-df -mupper-regs-sf\" } */\n \n-float load_sf (float *p)\n+float\n+load_store_sf (unsigned long num,\n+\t       const float *from_ptr,\n+\t       float *to_ptr,\n+\t       const unsigned long *in_mask_ptr,\n+\t       const unsigned long *out_mask_ptr)\n {\n-  float f = *p;\n-  __asm__ (\"# reg %x0\" : \"+v\" (f));\n-  return f;\n-}\n+  float value00\t= 0.0f;\n+  float value01\t= 0.0f;\n+  float value02\t= 0.0f;\n+  float value03\t= 0.0f;\n+  float value04\t= 0.0f;\n+  float value05\t= 0.0f;\n+  float value06\t= 0.0f;\n+  float value07\t= 0.0f;\n+  float value08\t= 0.0f;\n+  float value09\t= 0.0f;\n+  float value10\t= 0.0f;\n+  float value11\t= 0.0f;\n+  float value12\t= 0.0f;\n+  float value13\t= 0.0f;\n+  float value14\t= 0.0f;\n+  float value15\t= 0.0f;\n+  float value16\t= 0.0f;\n+  float value17\t= 0.0f;\n+  float value18\t= 0.0f;\n+  float value19\t= 0.0f;\n+  float value20\t= 0.0f;\n+  float value21\t= 0.0f;\n+  float value22\t= 0.0f;\n+  float value23\t= 0.0f;\n+  float value24\t= 0.0f;\n+  float value25\t= 0.0f;\n+  float value26\t= 0.0f;\n+  float value27\t= 0.0f;\n+  float value28\t= 0.0f;\n+  float value29\t= 0.0f;\n+  float value30\t= 0.0f;\n+  float value31\t= 0.0f;\n+  float value32\t= 0.0f;\n+  float value33\t= 0.0f;\n+  float value34\t= 0.0f;\n+  float value35\t= 0.0f;\n+  float value36\t= 0.0f;\n+  float value37\t= 0.0f;\n+  float value38\t= 0.0f;\n+  float value39\t= 0.0f;\n+  unsigned long in_mask;\n+  unsigned long out_mask;\n+  unsigned long i;\n \n-double load_df (double *p)\n-{\n-  double d = *p;\n-  __asm__ (\"# reg %x0\" : \"+v\" (d));\n-  return d;\n-}\n+  for (i = 0; i < num; i++)\n+    {\n+      in_mask = *in_mask_ptr++;\n+      if ((in_mask & (1L <<  0)) != 0L)\n+\tvalue00 = *from_ptr++;\n \n-double load_dfsf (float *p)\n-{\n-  double d = (double) *p;\n-  __asm__ (\"# reg %x0\" : \"+v\" (d));\n-  return d;\n-}\n+      if ((in_mask & (1L <<  1)) != 0L)\n+\tvalue01 = *from_ptr++;\n \n-void store_sf (float *p, float f)\n-{\n-  __asm__ (\"# reg %x0\" : \"+v\" (f));\n-  *p = f;\n+      if ((in_mask & (1L <<  2)) != 0L)\n+\tvalue02 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  3)) != 0L)\n+\tvalue03 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  4)) != 0L)\n+\tvalue04 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  5)) != 0L)\n+\tvalue05 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  6)) != 0L)\n+\tvalue06 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  7)) != 0L)\n+\tvalue07 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  8)) != 0L)\n+\tvalue08 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  9)) != 0L)\n+\tvalue09 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 10)) != 0L)\n+\tvalue10 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 11)) != 0L)\n+\tvalue11 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 12)) != 0L)\n+\tvalue12 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 13)) != 0L)\n+\tvalue13 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 14)) != 0L)\n+\tvalue14 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 15)) != 0L)\n+\tvalue15 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 16)) != 0L)\n+\tvalue16 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 17)) != 0L)\n+\tvalue17 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 18)) != 0L)\n+\tvalue18 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 19)) != 0L)\n+\tvalue19 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 20)) != 0L)\n+\tvalue20 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 21)) != 0L)\n+\tvalue21 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 22)) != 0L)\n+\tvalue22 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 23)) != 0L)\n+\tvalue23 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 24)) != 0L)\n+\tvalue24 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 25)) != 0L)\n+\tvalue25 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 26)) != 0L)\n+\tvalue26 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 27)) != 0L)\n+\tvalue27 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 28)) != 0L)\n+\tvalue28 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 29)) != 0L)\n+\tvalue29 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 30)) != 0L)\n+\tvalue30 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 31)) != 0L)\n+\tvalue31 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 32)) != 0L)\n+\tvalue32 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 33)) != 0L)\n+\tvalue33 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 34)) != 0L)\n+\tvalue34 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 35)) != 0L)\n+\tvalue35 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 36)) != 0L)\n+\tvalue36 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 37)) != 0L)\n+\tvalue37 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 38)) != 0L)\n+\tvalue38 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 39)) != 0L)\n+\tvalue39 = *from_ptr++;\n+\n+      out_mask = *out_mask_ptr++;\n+      if ((out_mask & (1L <<  0)) != 0L)\n+\t*to_ptr++ = value00;\n+\n+      if ((out_mask & (1L <<  1)) != 0L)\n+\t*to_ptr++ = value01;\n+\n+      if ((out_mask & (1L <<  2)) != 0L)\n+\t*to_ptr++ = value02;\n+\n+      if ((out_mask & (1L <<  3)) != 0L)\n+\t*to_ptr++ = value03;\n+\n+      if ((out_mask & (1L <<  4)) != 0L)\n+\t*to_ptr++ = value04;\n+\n+      if ((out_mask & (1L <<  5)) != 0L)\n+\t*to_ptr++ = value05;\n+\n+      if ((out_mask & (1L <<  6)) != 0L)\n+\t*to_ptr++ = value06;\n+\n+      if ((out_mask & (1L <<  7)) != 0L)\n+\t*to_ptr++ = value07;\n+\n+      if ((out_mask & (1L <<  8)) != 0L)\n+\t*to_ptr++ = value08;\n+\n+      if ((out_mask & (1L <<  9)) != 0L)\n+\t*to_ptr++ = value09;\n+\n+      if ((out_mask & (1L << 10)) != 0L)\n+\t*to_ptr++ = value10;\n+\n+      if ((out_mask & (1L << 11)) != 0L)\n+\t*to_ptr++ = value11;\n+\n+      if ((out_mask & (1L << 12)) != 0L)\n+\t*to_ptr++ = value12;\n+\n+      if ((out_mask & (1L << 13)) != 0L)\n+\t*to_ptr++ = value13;\n+\n+      if ((out_mask & (1L << 14)) != 0L)\n+\t*to_ptr++ = value14;\n+\n+      if ((out_mask & (1L << 15)) != 0L)\n+\t*to_ptr++ = value15;\n+\n+      if ((out_mask & (1L << 16)) != 0L)\n+\t*to_ptr++ = value16;\n+\n+      if ((out_mask & (1L << 17)) != 0L)\n+\t*to_ptr++ = value17;\n+\n+      if ((out_mask & (1L << 18)) != 0L)\n+\t*to_ptr++ = value18;\n+\n+      if ((out_mask & (1L << 19)) != 0L)\n+\t*to_ptr++ = value19;\n+\n+      if ((out_mask & (1L << 20)) != 0L)\n+\t*to_ptr++ = value20;\n+\n+      if ((out_mask & (1L << 21)) != 0L)\n+\t*to_ptr++ = value21;\n+\n+      if ((out_mask & (1L << 22)) != 0L)\n+\t*to_ptr++ = value22;\n+\n+      if ((out_mask & (1L << 23)) != 0L)\n+\t*to_ptr++ = value23;\n+\n+      if ((out_mask & (1L << 24)) != 0L)\n+\t*to_ptr++ = value24;\n+\n+      if ((out_mask & (1L << 25)) != 0L)\n+\t*to_ptr++ = value25;\n+\n+      if ((out_mask & (1L << 26)) != 0L)\n+\t*to_ptr++ = value26;\n+\n+      if ((out_mask & (1L << 27)) != 0L)\n+\t*to_ptr++ = value27;\n+\n+      if ((out_mask & (1L << 28)) != 0L)\n+\t*to_ptr++ = value28;\n+\n+      if ((out_mask & (1L << 29)) != 0L)\n+\t*to_ptr++ = value29;\n+\n+      if ((out_mask & (1L << 30)) != 0L)\n+\t*to_ptr++ = value30;\n+\n+      if ((out_mask & (1L << 31)) != 0L)\n+\t*to_ptr++ = value31;\n+\n+      if ((out_mask & (1L << 32)) != 0L)\n+\t*to_ptr++ = value32;\n+\n+      if ((out_mask & (1L << 33)) != 0L)\n+\t*to_ptr++ = value33;\n+\n+      if ((out_mask & (1L << 34)) != 0L)\n+\t*to_ptr++ = value34;\n+\n+      if ((out_mask & (1L << 35)) != 0L)\n+\t*to_ptr++ = value35;\n+\n+      if ((out_mask & (1L << 36)) != 0L)\n+\t*to_ptr++ = value36;\n+\n+      if ((out_mask & (1L << 37)) != 0L)\n+\t*to_ptr++ = value37;\n+\n+      if ((out_mask & (1L << 38)) != 0L)\n+\t*to_ptr++ = value38;\n+\n+      if ((out_mask & (1L << 39)) != 0L)\n+\t*to_ptr++ = value39;\n+    }\n+\n+  return (  value00 + value01 + value02 + value03 + value04\n+\t  + value05 + value06 + value07 + value08 + value09\n+\t  + value10 + value11 + value12 + value13 + value14\n+\t  + value15 + value16 + value17 + value18 + value19\n+\t  + value20 + value21 + value22 + value23 + value24\n+\t  + value25 + value26 + value27 + value28 + value29\n+\t  + value30 + value31 + value32 + value33 + value34\n+\t  + value35 + value36 + value37 + value38 + value39);\n }\n \n-void store_df (double *p, double d)\n+double\n+load_store_df (unsigned long num,\n+\t       const double *from_ptr,\n+\t       double *to_ptr,\n+\t       const unsigned long *in_mask_ptr,\n+\t       const unsigned long *out_mask_ptr)\n {\n-  __asm__ (\"# reg %x0\" : \"+v\" (d));\n-  *p = d;\n+  double value00\t= 0.0;\n+  double value01\t= 0.0;\n+  double value02\t= 0.0;\n+  double value03\t= 0.0;\n+  double value04\t= 0.0;\n+  double value05\t= 0.0;\n+  double value06\t= 0.0;\n+  double value07\t= 0.0;\n+  double value08\t= 0.0;\n+  double value09\t= 0.0;\n+  double value10\t= 0.0;\n+  double value11\t= 0.0;\n+  double value12\t= 0.0;\n+  double value13\t= 0.0;\n+  double value14\t= 0.0;\n+  double value15\t= 0.0;\n+  double value16\t= 0.0;\n+  double value17\t= 0.0;\n+  double value18\t= 0.0;\n+  double value19\t= 0.0;\n+  double value20\t= 0.0;\n+  double value21\t= 0.0;\n+  double value22\t= 0.0;\n+  double value23\t= 0.0;\n+  double value24\t= 0.0;\n+  double value25\t= 0.0;\n+  double value26\t= 0.0;\n+  double value27\t= 0.0;\n+  double value28\t= 0.0;\n+  double value29\t= 0.0;\n+  double value30\t= 0.0;\n+  double value31\t= 0.0;\n+  double value32\t= 0.0;\n+  double value33\t= 0.0;\n+  double value34\t= 0.0;\n+  double value35\t= 0.0;\n+  double value36\t= 0.0;\n+  double value37\t= 0.0;\n+  double value38\t= 0.0;\n+  double value39\t= 0.0;\n+  unsigned long in_mask;\n+  unsigned long out_mask;\n+  unsigned long i;\n+\n+  for (i = 0; i < num; i++)\n+    {\n+      in_mask = *in_mask_ptr++;\n+      if ((in_mask & (1L <<  0)) != 0L)\n+\tvalue00 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  1)) != 0L)\n+\tvalue01 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  2)) != 0L)\n+\tvalue02 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  3)) != 0L)\n+\tvalue03 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  4)) != 0L)\n+\tvalue04 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  5)) != 0L)\n+\tvalue05 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  6)) != 0L)\n+\tvalue06 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  7)) != 0L)\n+\tvalue07 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  8)) != 0L)\n+\tvalue08 = *from_ptr++;\n+\n+      if ((in_mask & (1L <<  9)) != 0L)\n+\tvalue09 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 10)) != 0L)\n+\tvalue10 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 11)) != 0L)\n+\tvalue11 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 12)) != 0L)\n+\tvalue12 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 13)) != 0L)\n+\tvalue13 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 14)) != 0L)\n+\tvalue14 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 15)) != 0L)\n+\tvalue15 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 16)) != 0L)\n+\tvalue16 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 17)) != 0L)\n+\tvalue17 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 18)) != 0L)\n+\tvalue18 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 19)) != 0L)\n+\tvalue19 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 20)) != 0L)\n+\tvalue20 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 21)) != 0L)\n+\tvalue21 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 22)) != 0L)\n+\tvalue22 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 23)) != 0L)\n+\tvalue23 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 24)) != 0L)\n+\tvalue24 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 25)) != 0L)\n+\tvalue25 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 26)) != 0L)\n+\tvalue26 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 27)) != 0L)\n+\tvalue27 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 28)) != 0L)\n+\tvalue28 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 29)) != 0L)\n+\tvalue29 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 30)) != 0L)\n+\tvalue30 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 31)) != 0L)\n+\tvalue31 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 32)) != 0L)\n+\tvalue32 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 33)) != 0L)\n+\tvalue33 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 34)) != 0L)\n+\tvalue34 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 35)) != 0L)\n+\tvalue35 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 36)) != 0L)\n+\tvalue36 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 37)) != 0L)\n+\tvalue37 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 38)) != 0L)\n+\tvalue38 = *from_ptr++;\n+\n+      if ((in_mask & (1L << 39)) != 0L)\n+\tvalue39 = *from_ptr++;\n+\n+      out_mask = *out_mask_ptr++;\n+      if ((out_mask & (1L <<  0)) != 0L)\n+\t*to_ptr++ = value00;\n+\n+      if ((out_mask & (1L <<  1)) != 0L)\n+\t*to_ptr++ = value01;\n+\n+      if ((out_mask & (1L <<  2)) != 0L)\n+\t*to_ptr++ = value02;\n+\n+      if ((out_mask & (1L <<  3)) != 0L)\n+\t*to_ptr++ = value03;\n+\n+      if ((out_mask & (1L <<  4)) != 0L)\n+\t*to_ptr++ = value04;\n+\n+      if ((out_mask & (1L <<  5)) != 0L)\n+\t*to_ptr++ = value05;\n+\n+      if ((out_mask & (1L <<  6)) != 0L)\n+\t*to_ptr++ = value06;\n+\n+      if ((out_mask & (1L <<  7)) != 0L)\n+\t*to_ptr++ = value07;\n+\n+      if ((out_mask & (1L <<  8)) != 0L)\n+\t*to_ptr++ = value08;\n+\n+      if ((out_mask & (1L <<  9)) != 0L)\n+\t*to_ptr++ = value09;\n+\n+      if ((out_mask & (1L << 10)) != 0L)\n+\t*to_ptr++ = value10;\n+\n+      if ((out_mask & (1L << 11)) != 0L)\n+\t*to_ptr++ = value11;\n+\n+      if ((out_mask & (1L << 12)) != 0L)\n+\t*to_ptr++ = value12;\n+\n+      if ((out_mask & (1L << 13)) != 0L)\n+\t*to_ptr++ = value13;\n+\n+      if ((out_mask & (1L << 14)) != 0L)\n+\t*to_ptr++ = value14;\n+\n+      if ((out_mask & (1L << 15)) != 0L)\n+\t*to_ptr++ = value15;\n+\n+      if ((out_mask & (1L << 16)) != 0L)\n+\t*to_ptr++ = value16;\n+\n+      if ((out_mask & (1L << 17)) != 0L)\n+\t*to_ptr++ = value17;\n+\n+      if ((out_mask & (1L << 18)) != 0L)\n+\t*to_ptr++ = value18;\n+\n+      if ((out_mask & (1L << 19)) != 0L)\n+\t*to_ptr++ = value19;\n+\n+      if ((out_mask & (1L << 20)) != 0L)\n+\t*to_ptr++ = value20;\n+\n+      if ((out_mask & (1L << 21)) != 0L)\n+\t*to_ptr++ = value21;\n+\n+      if ((out_mask & (1L << 22)) != 0L)\n+\t*to_ptr++ = value22;\n+\n+      if ((out_mask & (1L << 23)) != 0L)\n+\t*to_ptr++ = value23;\n+\n+      if ((out_mask & (1L << 24)) != 0L)\n+\t*to_ptr++ = value24;\n+\n+      if ((out_mask & (1L << 25)) != 0L)\n+\t*to_ptr++ = value25;\n+\n+      if ((out_mask & (1L << 26)) != 0L)\n+\t*to_ptr++ = value26;\n+\n+      if ((out_mask & (1L << 27)) != 0L)\n+\t*to_ptr++ = value27;\n+\n+      if ((out_mask & (1L << 28)) != 0L)\n+\t*to_ptr++ = value28;\n+\n+      if ((out_mask & (1L << 29)) != 0L)\n+\t*to_ptr++ = value29;\n+\n+      if ((out_mask & (1L << 30)) != 0L)\n+\t*to_ptr++ = value30;\n+\n+      if ((out_mask & (1L << 31)) != 0L)\n+\t*to_ptr++ = value31;\n+\n+      if ((out_mask & (1L << 32)) != 0L)\n+\t*to_ptr++ = value32;\n+\n+      if ((out_mask & (1L << 33)) != 0L)\n+\t*to_ptr++ = value33;\n+\n+      if ((out_mask & (1L << 34)) != 0L)\n+\t*to_ptr++ = value34;\n+\n+      if ((out_mask & (1L << 35)) != 0L)\n+\t*to_ptr++ = value35;\n+\n+      if ((out_mask & (1L << 36)) != 0L)\n+\t*to_ptr++ = value36;\n+\n+      if ((out_mask & (1L << 37)) != 0L)\n+\t*to_ptr++ = value37;\n+\n+      if ((out_mask & (1L << 38)) != 0L)\n+\t*to_ptr++ = value38;\n+\n+      if ((out_mask & (1L << 39)) != 0L)\n+\t*to_ptr++ = value39;\n+    }\n+\n+  return (  value00 + value01 + value02 + value03 + value04\n+\t  + value05 + value06 + value07 + value08 + value09\n+\t  + value10 + value11 + value12 + value13 + value14\n+\t  + value15 + value16 + value17 + value18 + value19\n+\t  + value20 + value21 + value22 + value23 + value24\n+\t  + value25 + value26 + value27 + value28 + value29\n+\t  + value30 + value31 + value32 + value33 + value34\n+\t  + value35 + value36 + value37 + value38 + value39);\n }\n \n /* { dg-final { scan-assembler \"lxsspx\"  } } */\n /* { dg-final { scan-assembler \"lxsdx\"   } } */\n /* { dg-final { scan-assembler \"stxsspx\" } } */\n /* { dg-final { scan-assembler \"stxsdx\"  } } */\n+/* { dg-final { scan-assembler \"xsaddsp\" } } */\n+/* { dg-final { scan-assembler \"xsadddp\" } } */"}, {"sha": "e3a284ca0da7f6f632d4d2907ccae6917b3a5812", "filename": "gcc/testsuite/gcc.target/powerpc/upper-regs-df.c", "status": "added", "additions": 726, "deletions": 0, "changes": 726, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fupper-regs-df.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fupper-regs-df.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fupper-regs-df.c?ref=25adc5d044047b926181ebcaf6284b39df8ee306", "patch": "@@ -0,0 +1,726 @@\n+/* { dg-do compile { target { powerpc*-*-* } } } */\n+/* { dg-require-effective-target powerpc_vsx_ok } */\n+/* { dg-skip-if \"\" { powerpc*-*-darwin* } { \"*\" } { \"\" } } */\n+/* { dg-skip-if \"do not override -mcpu\" { powerpc*-*-* } { \"-mcpu=*\" } { \"-mcpu=power8\" } } */\n+/* { dg-options \"-mcpu=power7 -O2 -mupper-regs-df\" } */\n+\n+/* Test for the -mupper-regs-df option to make sure double values are allocated\n+   to the Altivec registers as well as the traditional FPR registers.  */\n+\n+#ifndef TYPE\n+#define TYPE double\n+#endif\n+\n+#ifndef MASK_TYPE\n+#define MASK_TYPE unsigned long long\n+#endif\n+\n+#define MASK_ONE\t((MASK_TYPE)1)\n+#define ZERO\t\t((TYPE) 0.0)\n+\n+TYPE\n+test_add (const MASK_TYPE *add_mask, const TYPE *add_values,\n+\t  const MASK_TYPE *sub_mask, const TYPE *sub_values,\n+\t  const MASK_TYPE *mul_mask, const TYPE *mul_values,\n+\t  const MASK_TYPE *div_mask, const TYPE *div_values,\n+\t  const MASK_TYPE *eq0_mask, int *eq0_ptr)\n+{\n+  TYPE value;\n+  TYPE value00\t= ZERO;\n+  TYPE value01\t= ZERO;\n+  TYPE value02\t= ZERO;\n+  TYPE value03\t= ZERO;\n+  TYPE value04\t= ZERO;\n+  TYPE value05\t= ZERO;\n+  TYPE value06\t= ZERO;\n+  TYPE value07\t= ZERO;\n+  TYPE value08\t= ZERO;\n+  TYPE value09\t= ZERO;\n+  TYPE value10\t= ZERO;\n+  TYPE value11\t= ZERO;\n+  TYPE value12\t= ZERO;\n+  TYPE value13\t= ZERO;\n+  TYPE value14\t= ZERO;\n+  TYPE value15\t= ZERO;\n+  TYPE value16\t= ZERO;\n+  TYPE value17\t= ZERO;\n+  TYPE value18\t= ZERO;\n+  TYPE value19\t= ZERO;\n+  TYPE value20\t= ZERO;\n+  TYPE value21\t= ZERO;\n+  TYPE value22\t= ZERO;\n+  TYPE value23\t= ZERO;\n+  TYPE value24\t= ZERO;\n+  TYPE value25\t= ZERO;\n+  TYPE value26\t= ZERO;\n+  TYPE value27\t= ZERO;\n+  TYPE value28\t= ZERO;\n+  TYPE value29\t= ZERO;\n+  TYPE value30\t= ZERO;\n+  TYPE value31\t= ZERO;\n+  TYPE value32\t= ZERO;\n+  TYPE value33\t= ZERO;\n+  TYPE value34\t= ZERO;\n+  TYPE value35\t= ZERO;\n+  TYPE value36\t= ZERO;\n+  TYPE value37\t= ZERO;\n+  TYPE value38\t= ZERO;\n+  TYPE value39\t= ZERO;\n+  MASK_TYPE mask;\n+  int eq0;\n+\n+  while ((mask = *add_mask++) != 0)\n+    {\n+      value = *add_values++;\n+\n+      __asm__ (\" #reg %0\" : \"+d\" (value));\n+\n+      if ((mask & (MASK_ONE <<  0)) != 0)\n+\tvalue00 += value;\n+\n+      if ((mask & (MASK_ONE <<  1)) != 0)\n+\tvalue01 += value;\n+\n+      if ((mask & (MASK_ONE <<  2)) != 0)\n+\tvalue02 += value;\n+\n+      if ((mask & (MASK_ONE <<  3)) != 0)\n+\tvalue03 += value;\n+\n+      if ((mask & (MASK_ONE <<  4)) != 0)\n+\tvalue04 += value;\n+\n+      if ((mask & (MASK_ONE <<  5)) != 0)\n+\tvalue05 += value;\n+\n+      if ((mask & (MASK_ONE <<  6)) != 0)\n+\tvalue06 += value;\n+\n+      if ((mask & (MASK_ONE <<  7)) != 0)\n+\tvalue07 += value;\n+\n+      if ((mask & (MASK_ONE <<  8)) != 0)\n+\tvalue08 += value;\n+\n+      if ((mask & (MASK_ONE <<  9)) != 0)\n+\tvalue09 += value;\n+\n+      if ((mask & (MASK_ONE << 10)) != 0)\n+\tvalue10 += value;\n+\n+      if ((mask & (MASK_ONE << 11)) != 0)\n+\tvalue11 += value;\n+\n+      if ((mask & (MASK_ONE << 12)) != 0)\n+\tvalue12 += value;\n+\n+      if ((mask & (MASK_ONE << 13)) != 0)\n+\tvalue13 += value;\n+\n+      if ((mask & (MASK_ONE << 14)) != 0)\n+\tvalue14 += value;\n+\n+      if ((mask & (MASK_ONE << 15)) != 0)\n+\tvalue15 += value;\n+\n+      if ((mask & (MASK_ONE << 16)) != 0)\n+\tvalue16 += value;\n+\n+      if ((mask & (MASK_ONE << 17)) != 0)\n+\tvalue17 += value;\n+\n+      if ((mask & (MASK_ONE << 18)) != 0)\n+\tvalue18 += value;\n+\n+      if ((mask & (MASK_ONE << 19)) != 0)\n+\tvalue19 += value;\n+\n+      if ((mask & (MASK_ONE << 20)) != 0)\n+\tvalue20 += value;\n+\n+      if ((mask & (MASK_ONE << 21)) != 0)\n+\tvalue21 += value;\n+\n+      if ((mask & (MASK_ONE << 22)) != 0)\n+\tvalue22 += value;\n+\n+      if ((mask & (MASK_ONE << 23)) != 0)\n+\tvalue23 += value;\n+\n+      if ((mask & (MASK_ONE << 24)) != 0)\n+\tvalue24 += value;\n+\n+      if ((mask & (MASK_ONE << 25)) != 0)\n+\tvalue25 += value;\n+\n+      if ((mask & (MASK_ONE << 26)) != 0)\n+\tvalue26 += value;\n+\n+      if ((mask & (MASK_ONE << 27)) != 0)\n+\tvalue27 += value;\n+\n+      if ((mask & (MASK_ONE << 28)) != 0)\n+\tvalue28 += value;\n+\n+      if ((mask & (MASK_ONE << 29)) != 0)\n+\tvalue29 += value;\n+\n+      if ((mask & (MASK_ONE << 30)) != 0)\n+\tvalue30 += value;\n+\n+      if ((mask & (MASK_ONE << 31)) != 0)\n+\tvalue31 += value;\n+\n+      if ((mask & (MASK_ONE << 32)) != 0)\n+\tvalue32 += value;\n+\n+      if ((mask & (MASK_ONE << 33)) != 0)\n+\tvalue33 += value;\n+\n+      if ((mask & (MASK_ONE << 34)) != 0)\n+\tvalue34 += value;\n+\n+      if ((mask & (MASK_ONE << 35)) != 0)\n+\tvalue35 += value;\n+\n+      if ((mask & (MASK_ONE << 36)) != 0)\n+\tvalue36 += value;\n+\n+      if ((mask & (MASK_ONE << 37)) != 0)\n+\tvalue37 += value;\n+\n+      if ((mask & (MASK_ONE << 38)) != 0)\n+\tvalue38 += value;\n+\n+      if ((mask & (MASK_ONE << 39)) != 0)\n+\tvalue39 += value;\n+    }\n+\n+  while ((mask = *sub_mask++) != 0)\n+    {\n+      value = *sub_values++;\n+\n+      __asm__ (\" #reg %0\" : \"+d\" (value));\n+\n+      if ((mask & (MASK_ONE <<  0)) != 0)\n+\tvalue00 -= value;\n+\n+      if ((mask & (MASK_ONE <<  1)) != 0)\n+\tvalue01 -= value;\n+\n+      if ((mask & (MASK_ONE <<  2)) != 0)\n+\tvalue02 -= value;\n+\n+      if ((mask & (MASK_ONE <<  3)) != 0)\n+\tvalue03 -= value;\n+\n+      if ((mask & (MASK_ONE <<  4)) != 0)\n+\tvalue04 -= value;\n+\n+      if ((mask & (MASK_ONE <<  5)) != 0)\n+\tvalue05 -= value;\n+\n+      if ((mask & (MASK_ONE <<  6)) != 0)\n+\tvalue06 -= value;\n+\n+      if ((mask & (MASK_ONE <<  7)) != 0)\n+\tvalue07 -= value;\n+\n+      if ((mask & (MASK_ONE <<  8)) != 0)\n+\tvalue08 -= value;\n+\n+      if ((mask & (MASK_ONE <<  9)) != 0)\n+\tvalue09 -= value;\n+\n+      if ((mask & (MASK_ONE << 10)) != 0)\n+\tvalue10 -= value;\n+\n+      if ((mask & (MASK_ONE << 11)) != 0)\n+\tvalue11 -= value;\n+\n+      if ((mask & (MASK_ONE << 12)) != 0)\n+\tvalue12 -= value;\n+\n+      if ((mask & (MASK_ONE << 13)) != 0)\n+\tvalue13 -= value;\n+\n+      if ((mask & (MASK_ONE << 14)) != 0)\n+\tvalue14 -= value;\n+\n+      if ((mask & (MASK_ONE << 15)) != 0)\n+\tvalue15 -= value;\n+\n+      if ((mask & (MASK_ONE << 16)) != 0)\n+\tvalue16 -= value;\n+\n+      if ((mask & (MASK_ONE << 17)) != 0)\n+\tvalue17 -= value;\n+\n+      if ((mask & (MASK_ONE << 18)) != 0)\n+\tvalue18 -= value;\n+\n+      if ((mask & (MASK_ONE << 19)) != 0)\n+\tvalue19 -= value;\n+\n+      if ((mask & (MASK_ONE << 20)) != 0)\n+\tvalue20 -= value;\n+\n+      if ((mask & (MASK_ONE << 21)) != 0)\n+\tvalue21 -= value;\n+\n+      if ((mask & (MASK_ONE << 22)) != 0)\n+\tvalue22 -= value;\n+\n+      if ((mask & (MASK_ONE << 23)) != 0)\n+\tvalue23 -= value;\n+\n+      if ((mask & (MASK_ONE << 24)) != 0)\n+\tvalue24 -= value;\n+\n+      if ((mask & (MASK_ONE << 25)) != 0)\n+\tvalue25 -= value;\n+\n+      if ((mask & (MASK_ONE << 26)) != 0)\n+\tvalue26 -= value;\n+\n+      if ((mask & (MASK_ONE << 27)) != 0)\n+\tvalue27 -= value;\n+\n+      if ((mask & (MASK_ONE << 28)) != 0)\n+\tvalue28 -= value;\n+\n+      if ((mask & (MASK_ONE << 29)) != 0)\n+\tvalue29 -= value;\n+\n+      if ((mask & (MASK_ONE << 30)) != 0)\n+\tvalue30 -= value;\n+\n+      if ((mask & (MASK_ONE << 31)) != 0)\n+\tvalue31 -= value;\n+\n+      if ((mask & (MASK_ONE << 32)) != 0)\n+\tvalue32 -= value;\n+\n+      if ((mask & (MASK_ONE << 33)) != 0)\n+\tvalue33 -= value;\n+\n+      if ((mask & (MASK_ONE << 34)) != 0)\n+\tvalue34 -= value;\n+\n+      if ((mask & (MASK_ONE << 35)) != 0)\n+\tvalue35 -= value;\n+\n+      if ((mask & (MASK_ONE << 36)) != 0)\n+\tvalue36 -= value;\n+\n+      if ((mask & (MASK_ONE << 37)) != 0)\n+\tvalue37 -= value;\n+\n+      if ((mask & (MASK_ONE << 38)) != 0)\n+\tvalue38 -= value;\n+\n+      if ((mask & (MASK_ONE << 39)) != 0)\n+\tvalue39 -= value;\n+    }\n+\n+  while ((mask = *mul_mask++) != 0)\n+    {\n+      value = *mul_values++;\n+\n+      __asm__ (\" #reg %0\" : \"+d\" (value));\n+\n+      if ((mask & (MASK_ONE <<  0)) != 0)\n+\tvalue00 *= value;\n+\n+      if ((mask & (MASK_ONE <<  1)) != 0)\n+\tvalue01 *= value;\n+\n+      if ((mask & (MASK_ONE <<  2)) != 0)\n+\tvalue02 *= value;\n+\n+      if ((mask & (MASK_ONE <<  3)) != 0)\n+\tvalue03 *= value;\n+\n+      if ((mask & (MASK_ONE <<  4)) != 0)\n+\tvalue04 *= value;\n+\n+      if ((mask & (MASK_ONE <<  5)) != 0)\n+\tvalue05 *= value;\n+\n+      if ((mask & (MASK_ONE <<  6)) != 0)\n+\tvalue06 *= value;\n+\n+      if ((mask & (MASK_ONE <<  7)) != 0)\n+\tvalue07 *= value;\n+\n+      if ((mask & (MASK_ONE <<  8)) != 0)\n+\tvalue08 *= value;\n+\n+      if ((mask & (MASK_ONE <<  9)) != 0)\n+\tvalue09 *= value;\n+\n+      if ((mask & (MASK_ONE << 10)) != 0)\n+\tvalue10 *= value;\n+\n+      if ((mask & (MASK_ONE << 11)) != 0)\n+\tvalue11 *= value;\n+\n+      if ((mask & (MASK_ONE << 12)) != 0)\n+\tvalue12 *= value;\n+\n+      if ((mask & (MASK_ONE << 13)) != 0)\n+\tvalue13 *= value;\n+\n+      if ((mask & (MASK_ONE << 14)) != 0)\n+\tvalue14 *= value;\n+\n+      if ((mask & (MASK_ONE << 15)) != 0)\n+\tvalue15 *= value;\n+\n+      if ((mask & (MASK_ONE << 16)) != 0)\n+\tvalue16 *= value;\n+\n+      if ((mask & (MASK_ONE << 17)) != 0)\n+\tvalue17 *= value;\n+\n+      if ((mask & (MASK_ONE << 18)) != 0)\n+\tvalue18 *= value;\n+\n+      if ((mask & (MASK_ONE << 19)) != 0)\n+\tvalue19 *= value;\n+\n+      if ((mask & (MASK_ONE << 20)) != 0)\n+\tvalue20 *= value;\n+\n+      if ((mask & (MASK_ONE << 21)) != 0)\n+\tvalue21 *= value;\n+\n+      if ((mask & (MASK_ONE << 22)) != 0)\n+\tvalue22 *= value;\n+\n+      if ((mask & (MASK_ONE << 23)) != 0)\n+\tvalue23 *= value;\n+\n+      if ((mask & (MASK_ONE << 24)) != 0)\n+\tvalue24 *= value;\n+\n+      if ((mask & (MASK_ONE << 25)) != 0)\n+\tvalue25 *= value;\n+\n+      if ((mask & (MASK_ONE << 26)) != 0)\n+\tvalue26 *= value;\n+\n+      if ((mask & (MASK_ONE << 27)) != 0)\n+\tvalue27 *= value;\n+\n+      if ((mask & (MASK_ONE << 28)) != 0)\n+\tvalue28 *= value;\n+\n+      if ((mask & (MASK_ONE << 29)) != 0)\n+\tvalue29 *= value;\n+\n+      if ((mask & (MASK_ONE << 30)) != 0)\n+\tvalue30 *= value;\n+\n+      if ((mask & (MASK_ONE << 31)) != 0)\n+\tvalue31 *= value;\n+\n+      if ((mask & (MASK_ONE << 32)) != 0)\n+\tvalue32 *= value;\n+\n+      if ((mask & (MASK_ONE << 33)) != 0)\n+\tvalue33 *= value;\n+\n+      if ((mask & (MASK_ONE << 34)) != 0)\n+\tvalue34 *= value;\n+\n+      if ((mask & (MASK_ONE << 35)) != 0)\n+\tvalue35 *= value;\n+\n+      if ((mask & (MASK_ONE << 36)) != 0)\n+\tvalue36 *= value;\n+\n+      if ((mask & (MASK_ONE << 37)) != 0)\n+\tvalue37 *= value;\n+\n+      if ((mask & (MASK_ONE << 38)) != 0)\n+\tvalue38 *= value;\n+\n+      if ((mask & (MASK_ONE << 39)) != 0)\n+\tvalue39 *= value;\n+    }\n+\n+  while ((mask = *div_mask++) != 0)\n+    {\n+      value = *div_values++;\n+\n+      __asm__ (\" #reg %0\" : \"+d\" (value));\n+\n+      if ((mask & (MASK_ONE <<  0)) != 0)\n+\tvalue00 /= value;\n+\n+      if ((mask & (MASK_ONE <<  1)) != 0)\n+\tvalue01 /= value;\n+\n+      if ((mask & (MASK_ONE <<  2)) != 0)\n+\tvalue02 /= value;\n+\n+      if ((mask & (MASK_ONE <<  3)) != 0)\n+\tvalue03 /= value;\n+\n+      if ((mask & (MASK_ONE <<  4)) != 0)\n+\tvalue04 /= value;\n+\n+      if ((mask & (MASK_ONE <<  5)) != 0)\n+\tvalue05 /= value;\n+\n+      if ((mask & (MASK_ONE <<  6)) != 0)\n+\tvalue06 /= value;\n+\n+      if ((mask & (MASK_ONE <<  7)) != 0)\n+\tvalue07 /= value;\n+\n+      if ((mask & (MASK_ONE <<  8)) != 0)\n+\tvalue08 /= value;\n+\n+      if ((mask & (MASK_ONE <<  9)) != 0)\n+\tvalue09 /= value;\n+\n+      if ((mask & (MASK_ONE << 10)) != 0)\n+\tvalue10 /= value;\n+\n+      if ((mask & (MASK_ONE << 11)) != 0)\n+\tvalue11 /= value;\n+\n+      if ((mask & (MASK_ONE << 12)) != 0)\n+\tvalue12 /= value;\n+\n+      if ((mask & (MASK_ONE << 13)) != 0)\n+\tvalue13 /= value;\n+\n+      if ((mask & (MASK_ONE << 14)) != 0)\n+\tvalue14 /= value;\n+\n+      if ((mask & (MASK_ONE << 15)) != 0)\n+\tvalue15 /= value;\n+\n+      if ((mask & (MASK_ONE << 16)) != 0)\n+\tvalue16 /= value;\n+\n+      if ((mask & (MASK_ONE << 17)) != 0)\n+\tvalue17 /= value;\n+\n+      if ((mask & (MASK_ONE << 18)) != 0)\n+\tvalue18 /= value;\n+\n+      if ((mask & (MASK_ONE << 19)) != 0)\n+\tvalue19 /= value;\n+\n+      if ((mask & (MASK_ONE << 20)) != 0)\n+\tvalue20 /= value;\n+\n+      if ((mask & (MASK_ONE << 21)) != 0)\n+\tvalue21 /= value;\n+\n+      if ((mask & (MASK_ONE << 22)) != 0)\n+\tvalue22 /= value;\n+\n+      if ((mask & (MASK_ONE << 23)) != 0)\n+\tvalue23 /= value;\n+\n+      if ((mask & (MASK_ONE << 24)) != 0)\n+\tvalue24 /= value;\n+\n+      if ((mask & (MASK_ONE << 25)) != 0)\n+\tvalue25 /= value;\n+\n+      if ((mask & (MASK_ONE << 26)) != 0)\n+\tvalue26 /= value;\n+\n+      if ((mask & (MASK_ONE << 27)) != 0)\n+\tvalue27 /= value;\n+\n+      if ((mask & (MASK_ONE << 28)) != 0)\n+\tvalue28 /= value;\n+\n+      if ((mask & (MASK_ONE << 29)) != 0)\n+\tvalue29 /= value;\n+\n+      if ((mask & (MASK_ONE << 30)) != 0)\n+\tvalue30 /= value;\n+\n+      if ((mask & (MASK_ONE << 31)) != 0)\n+\tvalue31 /= value;\n+\n+      if ((mask & (MASK_ONE << 32)) != 0)\n+\tvalue32 /= value;\n+\n+      if ((mask & (MASK_ONE << 33)) != 0)\n+\tvalue33 /= value;\n+\n+      if ((mask & (MASK_ONE << 34)) != 0)\n+\tvalue34 /= value;\n+\n+      if ((mask & (MASK_ONE << 35)) != 0)\n+\tvalue35 /= value;\n+\n+      if ((mask & (MASK_ONE << 36)) != 0)\n+\tvalue36 /= value;\n+\n+      if ((mask & (MASK_ONE << 37)) != 0)\n+\tvalue37 /= value;\n+\n+      if ((mask & (MASK_ONE << 38)) != 0)\n+\tvalue38 /= value;\n+\n+      if ((mask & (MASK_ONE << 39)) != 0)\n+\tvalue39 /= value;\n+    }\n+\n+  while ((mask = *eq0_mask++) != 0)\n+    {\n+      eq0 = 0;\n+\n+      if ((mask & (MASK_ONE <<  0)) != 0)\n+\teq0 |= (value00 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  1)) != 0)\n+\teq0 |= (value01 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  2)) != 0)\n+\teq0 |= (value02 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  3)) != 0)\n+\teq0 |= (value03 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  4)) != 0)\n+\teq0 |= (value04 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  5)) != 0)\n+\teq0 |= (value05 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  6)) != 0)\n+\teq0 |= (value06 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  7)) != 0)\n+\teq0 |= (value07 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  8)) != 0)\n+\teq0 |= (value08 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  9)) != 0)\n+\teq0 |= (value09 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 10)) != 0)\n+\teq0 |= (value10 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 11)) != 0)\n+\teq0 |= (value11 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 12)) != 0)\n+\teq0 |= (value12 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 13)) != 0)\n+\teq0 |= (value13 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 14)) != 0)\n+\teq0 |= (value14 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 15)) != 0)\n+\teq0 |= (value15 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 16)) != 0)\n+\teq0 |= (value16 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 17)) != 0)\n+\teq0 |= (value17 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 18)) != 0)\n+\teq0 |= (value18 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 19)) != 0)\n+\teq0 |= (value19 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 20)) != 0)\n+\teq0 |= (value20 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 21)) != 0)\n+\teq0 |= (value21 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 22)) != 0)\n+\teq0 |= (value22 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 23)) != 0)\n+\teq0 |= (value23 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 24)) != 0)\n+\teq0 |= (value24 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 25)) != 0)\n+\teq0 |= (value25 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 26)) != 0)\n+\teq0 |= (value26 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 27)) != 0)\n+\teq0 |= (value27 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 28)) != 0)\n+\teq0 |= (value28 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 29)) != 0)\n+\teq0 |= (value29 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 30)) != 0)\n+\teq0 |= (value30 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 31)) != 0)\n+\teq0 |= (value31 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 32)) != 0)\n+\teq0 |= (value32 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 33)) != 0)\n+\teq0 |= (value33 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 34)) != 0)\n+\teq0 |= (value34 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 35)) != 0)\n+\teq0 |= (value35 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 36)) != 0)\n+\teq0 |= (value36 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 37)) != 0)\n+\teq0 |= (value37 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 38)) != 0)\n+\teq0 |= (value38 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 39)) != 0)\n+\teq0 |= (value39 == ZERO);\n+\n+      *eq0_ptr++ = eq0;\n+    }\n+\n+  return (  value00 + value01 + value02 + value03 + value04\n+\t  + value05 + value06 + value07 + value08 + value09\n+\t  + value10 + value11 + value12 + value13 + value14\n+\t  + value15 + value16 + value17 + value18 + value19\n+\t  + value20 + value21 + value22 + value23 + value24\n+\t  + value25 + value26 + value27 + value28 + value29\n+\t  + value30 + value31 + value32 + value33 + value34\n+\t  + value35 + value36 + value37 + value38 + value39);\n+}\n+\n+/* { dg-final { scan-assembler \"fadd\"     } } */\n+/* { dg-final { scan-assembler \"fsub\"     } } */\n+/* { dg-final { scan-assembler \"fmul\"     } } */\n+/* { dg-final { scan-assembler \"fdiv\"     } } */\n+/* { dg-final { scan-assembler \"fcmpu\"    } } */\n+/* { dg-final { scan-assembler \"xsadddp\"  } } */\n+/* { dg-final { scan-assembler \"xssubdp\"  } } */\n+/* { dg-final { scan-assembler \"xsmuldp\"  } } */\n+/* { dg-final { scan-assembler \"xsdivdp\"  } } */\n+/* { dg-final { scan-assembler \"xscmpudp\" } } */"}, {"sha": "401b5c16ffadc01b3ff64ce1f20faa7dc922a4a9", "filename": "gcc/testsuite/gcc.target/powerpc/upper-regs-sf.c", "status": "added", "additions": 726, "deletions": 0, "changes": 726, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fupper-regs-sf.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/25adc5d044047b926181ebcaf6284b39df8ee306/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fupper-regs-sf.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Fpowerpc%2Fupper-regs-sf.c?ref=25adc5d044047b926181ebcaf6284b39df8ee306", "patch": "@@ -0,0 +1,726 @@\n+/* { dg-do compile { target { powerpc*-*-* } } } */\n+/* { dg-require-effective-target powerpc_p8vector_ok } */\n+/* { dg-skip-if \"\" { powerpc*-*-darwin* } { \"*\" } { \"\" } } */\n+/* { dg-skip-if \"do not override -mcpu\" { powerpc*-*-* } { \"-mcpu=*\" } { \"-mcpu=power8\" } } */\n+/* { dg-options \"-mcpu=power8 -O2 -mupper-regs-df -mupper-regs-sf\" } */\n+\n+/* Test for the -mupper-regs-df option to make sure double values are allocated\n+   to the Altivec registers as well as the traditional FPR registers.  */\n+\n+#ifndef TYPE\n+#define TYPE float\n+#endif\n+\n+#ifndef MASK_TYPE\n+#define MASK_TYPE unsigned long long\n+#endif\n+\n+#define MASK_ONE\t((MASK_TYPE)1)\n+#define ZERO\t\t((TYPE) 0.0)\n+\n+TYPE\n+test_add (const MASK_TYPE *add_mask, const TYPE *add_values,\n+\t  const MASK_TYPE *sub_mask, const TYPE *sub_values,\n+\t  const MASK_TYPE *mul_mask, const TYPE *mul_values,\n+\t  const MASK_TYPE *div_mask, const TYPE *div_values,\n+\t  const MASK_TYPE *eq0_mask, int *eq0_ptr)\n+{\n+  TYPE value;\n+  TYPE value00\t= ZERO;\n+  TYPE value01\t= ZERO;\n+  TYPE value02\t= ZERO;\n+  TYPE value03\t= ZERO;\n+  TYPE value04\t= ZERO;\n+  TYPE value05\t= ZERO;\n+  TYPE value06\t= ZERO;\n+  TYPE value07\t= ZERO;\n+  TYPE value08\t= ZERO;\n+  TYPE value09\t= ZERO;\n+  TYPE value10\t= ZERO;\n+  TYPE value11\t= ZERO;\n+  TYPE value12\t= ZERO;\n+  TYPE value13\t= ZERO;\n+  TYPE value14\t= ZERO;\n+  TYPE value15\t= ZERO;\n+  TYPE value16\t= ZERO;\n+  TYPE value17\t= ZERO;\n+  TYPE value18\t= ZERO;\n+  TYPE value19\t= ZERO;\n+  TYPE value20\t= ZERO;\n+  TYPE value21\t= ZERO;\n+  TYPE value22\t= ZERO;\n+  TYPE value23\t= ZERO;\n+  TYPE value24\t= ZERO;\n+  TYPE value25\t= ZERO;\n+  TYPE value26\t= ZERO;\n+  TYPE value27\t= ZERO;\n+  TYPE value28\t= ZERO;\n+  TYPE value29\t= ZERO;\n+  TYPE value30\t= ZERO;\n+  TYPE value31\t= ZERO;\n+  TYPE value32\t= ZERO;\n+  TYPE value33\t= ZERO;\n+  TYPE value34\t= ZERO;\n+  TYPE value35\t= ZERO;\n+  TYPE value36\t= ZERO;\n+  TYPE value37\t= ZERO;\n+  TYPE value38\t= ZERO;\n+  TYPE value39\t= ZERO;\n+  MASK_TYPE mask;\n+  int eq0;\n+\n+  while ((mask = *add_mask++) != 0)\n+    {\n+      value = *add_values++;\n+\n+      __asm__ (\" #reg %0\" : \"+d\" (value));\n+\n+      if ((mask & (MASK_ONE <<  0)) != 0)\n+\tvalue00 += value;\n+\n+      if ((mask & (MASK_ONE <<  1)) != 0)\n+\tvalue01 += value;\n+\n+      if ((mask & (MASK_ONE <<  2)) != 0)\n+\tvalue02 += value;\n+\n+      if ((mask & (MASK_ONE <<  3)) != 0)\n+\tvalue03 += value;\n+\n+      if ((mask & (MASK_ONE <<  4)) != 0)\n+\tvalue04 += value;\n+\n+      if ((mask & (MASK_ONE <<  5)) != 0)\n+\tvalue05 += value;\n+\n+      if ((mask & (MASK_ONE <<  6)) != 0)\n+\tvalue06 += value;\n+\n+      if ((mask & (MASK_ONE <<  7)) != 0)\n+\tvalue07 += value;\n+\n+      if ((mask & (MASK_ONE <<  8)) != 0)\n+\tvalue08 += value;\n+\n+      if ((mask & (MASK_ONE <<  9)) != 0)\n+\tvalue09 += value;\n+\n+      if ((mask & (MASK_ONE << 10)) != 0)\n+\tvalue10 += value;\n+\n+      if ((mask & (MASK_ONE << 11)) != 0)\n+\tvalue11 += value;\n+\n+      if ((mask & (MASK_ONE << 12)) != 0)\n+\tvalue12 += value;\n+\n+      if ((mask & (MASK_ONE << 13)) != 0)\n+\tvalue13 += value;\n+\n+      if ((mask & (MASK_ONE << 14)) != 0)\n+\tvalue14 += value;\n+\n+      if ((mask & (MASK_ONE << 15)) != 0)\n+\tvalue15 += value;\n+\n+      if ((mask & (MASK_ONE << 16)) != 0)\n+\tvalue16 += value;\n+\n+      if ((mask & (MASK_ONE << 17)) != 0)\n+\tvalue17 += value;\n+\n+      if ((mask & (MASK_ONE << 18)) != 0)\n+\tvalue18 += value;\n+\n+      if ((mask & (MASK_ONE << 19)) != 0)\n+\tvalue19 += value;\n+\n+      if ((mask & (MASK_ONE << 20)) != 0)\n+\tvalue20 += value;\n+\n+      if ((mask & (MASK_ONE << 21)) != 0)\n+\tvalue21 += value;\n+\n+      if ((mask & (MASK_ONE << 22)) != 0)\n+\tvalue22 += value;\n+\n+      if ((mask & (MASK_ONE << 23)) != 0)\n+\tvalue23 += value;\n+\n+      if ((mask & (MASK_ONE << 24)) != 0)\n+\tvalue24 += value;\n+\n+      if ((mask & (MASK_ONE << 25)) != 0)\n+\tvalue25 += value;\n+\n+      if ((mask & (MASK_ONE << 26)) != 0)\n+\tvalue26 += value;\n+\n+      if ((mask & (MASK_ONE << 27)) != 0)\n+\tvalue27 += value;\n+\n+      if ((mask & (MASK_ONE << 28)) != 0)\n+\tvalue28 += value;\n+\n+      if ((mask & (MASK_ONE << 29)) != 0)\n+\tvalue29 += value;\n+\n+      if ((mask & (MASK_ONE << 30)) != 0)\n+\tvalue30 += value;\n+\n+      if ((mask & (MASK_ONE << 31)) != 0)\n+\tvalue31 += value;\n+\n+      if ((mask & (MASK_ONE << 32)) != 0)\n+\tvalue32 += value;\n+\n+      if ((mask & (MASK_ONE << 33)) != 0)\n+\tvalue33 += value;\n+\n+      if ((mask & (MASK_ONE << 34)) != 0)\n+\tvalue34 += value;\n+\n+      if ((mask & (MASK_ONE << 35)) != 0)\n+\tvalue35 += value;\n+\n+      if ((mask & (MASK_ONE << 36)) != 0)\n+\tvalue36 += value;\n+\n+      if ((mask & (MASK_ONE << 37)) != 0)\n+\tvalue37 += value;\n+\n+      if ((mask & (MASK_ONE << 38)) != 0)\n+\tvalue38 += value;\n+\n+      if ((mask & (MASK_ONE << 39)) != 0)\n+\tvalue39 += value;\n+    }\n+\n+  while ((mask = *sub_mask++) != 0)\n+    {\n+      value = *sub_values++;\n+\n+      __asm__ (\" #reg %0\" : \"+d\" (value));\n+\n+      if ((mask & (MASK_ONE <<  0)) != 0)\n+\tvalue00 -= value;\n+\n+      if ((mask & (MASK_ONE <<  1)) != 0)\n+\tvalue01 -= value;\n+\n+      if ((mask & (MASK_ONE <<  2)) != 0)\n+\tvalue02 -= value;\n+\n+      if ((mask & (MASK_ONE <<  3)) != 0)\n+\tvalue03 -= value;\n+\n+      if ((mask & (MASK_ONE <<  4)) != 0)\n+\tvalue04 -= value;\n+\n+      if ((mask & (MASK_ONE <<  5)) != 0)\n+\tvalue05 -= value;\n+\n+      if ((mask & (MASK_ONE <<  6)) != 0)\n+\tvalue06 -= value;\n+\n+      if ((mask & (MASK_ONE <<  7)) != 0)\n+\tvalue07 -= value;\n+\n+      if ((mask & (MASK_ONE <<  8)) != 0)\n+\tvalue08 -= value;\n+\n+      if ((mask & (MASK_ONE <<  9)) != 0)\n+\tvalue09 -= value;\n+\n+      if ((mask & (MASK_ONE << 10)) != 0)\n+\tvalue10 -= value;\n+\n+      if ((mask & (MASK_ONE << 11)) != 0)\n+\tvalue11 -= value;\n+\n+      if ((mask & (MASK_ONE << 12)) != 0)\n+\tvalue12 -= value;\n+\n+      if ((mask & (MASK_ONE << 13)) != 0)\n+\tvalue13 -= value;\n+\n+      if ((mask & (MASK_ONE << 14)) != 0)\n+\tvalue14 -= value;\n+\n+      if ((mask & (MASK_ONE << 15)) != 0)\n+\tvalue15 -= value;\n+\n+      if ((mask & (MASK_ONE << 16)) != 0)\n+\tvalue16 -= value;\n+\n+      if ((mask & (MASK_ONE << 17)) != 0)\n+\tvalue17 -= value;\n+\n+      if ((mask & (MASK_ONE << 18)) != 0)\n+\tvalue18 -= value;\n+\n+      if ((mask & (MASK_ONE << 19)) != 0)\n+\tvalue19 -= value;\n+\n+      if ((mask & (MASK_ONE << 20)) != 0)\n+\tvalue20 -= value;\n+\n+      if ((mask & (MASK_ONE << 21)) != 0)\n+\tvalue21 -= value;\n+\n+      if ((mask & (MASK_ONE << 22)) != 0)\n+\tvalue22 -= value;\n+\n+      if ((mask & (MASK_ONE << 23)) != 0)\n+\tvalue23 -= value;\n+\n+      if ((mask & (MASK_ONE << 24)) != 0)\n+\tvalue24 -= value;\n+\n+      if ((mask & (MASK_ONE << 25)) != 0)\n+\tvalue25 -= value;\n+\n+      if ((mask & (MASK_ONE << 26)) != 0)\n+\tvalue26 -= value;\n+\n+      if ((mask & (MASK_ONE << 27)) != 0)\n+\tvalue27 -= value;\n+\n+      if ((mask & (MASK_ONE << 28)) != 0)\n+\tvalue28 -= value;\n+\n+      if ((mask & (MASK_ONE << 29)) != 0)\n+\tvalue29 -= value;\n+\n+      if ((mask & (MASK_ONE << 30)) != 0)\n+\tvalue30 -= value;\n+\n+      if ((mask & (MASK_ONE << 31)) != 0)\n+\tvalue31 -= value;\n+\n+      if ((mask & (MASK_ONE << 32)) != 0)\n+\tvalue32 -= value;\n+\n+      if ((mask & (MASK_ONE << 33)) != 0)\n+\tvalue33 -= value;\n+\n+      if ((mask & (MASK_ONE << 34)) != 0)\n+\tvalue34 -= value;\n+\n+      if ((mask & (MASK_ONE << 35)) != 0)\n+\tvalue35 -= value;\n+\n+      if ((mask & (MASK_ONE << 36)) != 0)\n+\tvalue36 -= value;\n+\n+      if ((mask & (MASK_ONE << 37)) != 0)\n+\tvalue37 -= value;\n+\n+      if ((mask & (MASK_ONE << 38)) != 0)\n+\tvalue38 -= value;\n+\n+      if ((mask & (MASK_ONE << 39)) != 0)\n+\tvalue39 -= value;\n+    }\n+\n+  while ((mask = *mul_mask++) != 0)\n+    {\n+      value = *mul_values++;\n+\n+      __asm__ (\" #reg %0\" : \"+d\" (value));\n+\n+      if ((mask & (MASK_ONE <<  0)) != 0)\n+\tvalue00 *= value;\n+\n+      if ((mask & (MASK_ONE <<  1)) != 0)\n+\tvalue01 *= value;\n+\n+      if ((mask & (MASK_ONE <<  2)) != 0)\n+\tvalue02 *= value;\n+\n+      if ((mask & (MASK_ONE <<  3)) != 0)\n+\tvalue03 *= value;\n+\n+      if ((mask & (MASK_ONE <<  4)) != 0)\n+\tvalue04 *= value;\n+\n+      if ((mask & (MASK_ONE <<  5)) != 0)\n+\tvalue05 *= value;\n+\n+      if ((mask & (MASK_ONE <<  6)) != 0)\n+\tvalue06 *= value;\n+\n+      if ((mask & (MASK_ONE <<  7)) != 0)\n+\tvalue07 *= value;\n+\n+      if ((mask & (MASK_ONE <<  8)) != 0)\n+\tvalue08 *= value;\n+\n+      if ((mask & (MASK_ONE <<  9)) != 0)\n+\tvalue09 *= value;\n+\n+      if ((mask & (MASK_ONE << 10)) != 0)\n+\tvalue10 *= value;\n+\n+      if ((mask & (MASK_ONE << 11)) != 0)\n+\tvalue11 *= value;\n+\n+      if ((mask & (MASK_ONE << 12)) != 0)\n+\tvalue12 *= value;\n+\n+      if ((mask & (MASK_ONE << 13)) != 0)\n+\tvalue13 *= value;\n+\n+      if ((mask & (MASK_ONE << 14)) != 0)\n+\tvalue14 *= value;\n+\n+      if ((mask & (MASK_ONE << 15)) != 0)\n+\tvalue15 *= value;\n+\n+      if ((mask & (MASK_ONE << 16)) != 0)\n+\tvalue16 *= value;\n+\n+      if ((mask & (MASK_ONE << 17)) != 0)\n+\tvalue17 *= value;\n+\n+      if ((mask & (MASK_ONE << 18)) != 0)\n+\tvalue18 *= value;\n+\n+      if ((mask & (MASK_ONE << 19)) != 0)\n+\tvalue19 *= value;\n+\n+      if ((mask & (MASK_ONE << 20)) != 0)\n+\tvalue20 *= value;\n+\n+      if ((mask & (MASK_ONE << 21)) != 0)\n+\tvalue21 *= value;\n+\n+      if ((mask & (MASK_ONE << 22)) != 0)\n+\tvalue22 *= value;\n+\n+      if ((mask & (MASK_ONE << 23)) != 0)\n+\tvalue23 *= value;\n+\n+      if ((mask & (MASK_ONE << 24)) != 0)\n+\tvalue24 *= value;\n+\n+      if ((mask & (MASK_ONE << 25)) != 0)\n+\tvalue25 *= value;\n+\n+      if ((mask & (MASK_ONE << 26)) != 0)\n+\tvalue26 *= value;\n+\n+      if ((mask & (MASK_ONE << 27)) != 0)\n+\tvalue27 *= value;\n+\n+      if ((mask & (MASK_ONE << 28)) != 0)\n+\tvalue28 *= value;\n+\n+      if ((mask & (MASK_ONE << 29)) != 0)\n+\tvalue29 *= value;\n+\n+      if ((mask & (MASK_ONE << 30)) != 0)\n+\tvalue30 *= value;\n+\n+      if ((mask & (MASK_ONE << 31)) != 0)\n+\tvalue31 *= value;\n+\n+      if ((mask & (MASK_ONE << 32)) != 0)\n+\tvalue32 *= value;\n+\n+      if ((mask & (MASK_ONE << 33)) != 0)\n+\tvalue33 *= value;\n+\n+      if ((mask & (MASK_ONE << 34)) != 0)\n+\tvalue34 *= value;\n+\n+      if ((mask & (MASK_ONE << 35)) != 0)\n+\tvalue35 *= value;\n+\n+      if ((mask & (MASK_ONE << 36)) != 0)\n+\tvalue36 *= value;\n+\n+      if ((mask & (MASK_ONE << 37)) != 0)\n+\tvalue37 *= value;\n+\n+      if ((mask & (MASK_ONE << 38)) != 0)\n+\tvalue38 *= value;\n+\n+      if ((mask & (MASK_ONE << 39)) != 0)\n+\tvalue39 *= value;\n+    }\n+\n+  while ((mask = *div_mask++) != 0)\n+    {\n+      value = *div_values++;\n+\n+      __asm__ (\" #reg %0\" : \"+d\" (value));\n+\n+      if ((mask & (MASK_ONE <<  0)) != 0)\n+\tvalue00 /= value;\n+\n+      if ((mask & (MASK_ONE <<  1)) != 0)\n+\tvalue01 /= value;\n+\n+      if ((mask & (MASK_ONE <<  2)) != 0)\n+\tvalue02 /= value;\n+\n+      if ((mask & (MASK_ONE <<  3)) != 0)\n+\tvalue03 /= value;\n+\n+      if ((mask & (MASK_ONE <<  4)) != 0)\n+\tvalue04 /= value;\n+\n+      if ((mask & (MASK_ONE <<  5)) != 0)\n+\tvalue05 /= value;\n+\n+      if ((mask & (MASK_ONE <<  6)) != 0)\n+\tvalue06 /= value;\n+\n+      if ((mask & (MASK_ONE <<  7)) != 0)\n+\tvalue07 /= value;\n+\n+      if ((mask & (MASK_ONE <<  8)) != 0)\n+\tvalue08 /= value;\n+\n+      if ((mask & (MASK_ONE <<  9)) != 0)\n+\tvalue09 /= value;\n+\n+      if ((mask & (MASK_ONE << 10)) != 0)\n+\tvalue10 /= value;\n+\n+      if ((mask & (MASK_ONE << 11)) != 0)\n+\tvalue11 /= value;\n+\n+      if ((mask & (MASK_ONE << 12)) != 0)\n+\tvalue12 /= value;\n+\n+      if ((mask & (MASK_ONE << 13)) != 0)\n+\tvalue13 /= value;\n+\n+      if ((mask & (MASK_ONE << 14)) != 0)\n+\tvalue14 /= value;\n+\n+      if ((mask & (MASK_ONE << 15)) != 0)\n+\tvalue15 /= value;\n+\n+      if ((mask & (MASK_ONE << 16)) != 0)\n+\tvalue16 /= value;\n+\n+      if ((mask & (MASK_ONE << 17)) != 0)\n+\tvalue17 /= value;\n+\n+      if ((mask & (MASK_ONE << 18)) != 0)\n+\tvalue18 /= value;\n+\n+      if ((mask & (MASK_ONE << 19)) != 0)\n+\tvalue19 /= value;\n+\n+      if ((mask & (MASK_ONE << 20)) != 0)\n+\tvalue20 /= value;\n+\n+      if ((mask & (MASK_ONE << 21)) != 0)\n+\tvalue21 /= value;\n+\n+      if ((mask & (MASK_ONE << 22)) != 0)\n+\tvalue22 /= value;\n+\n+      if ((mask & (MASK_ONE << 23)) != 0)\n+\tvalue23 /= value;\n+\n+      if ((mask & (MASK_ONE << 24)) != 0)\n+\tvalue24 /= value;\n+\n+      if ((mask & (MASK_ONE << 25)) != 0)\n+\tvalue25 /= value;\n+\n+      if ((mask & (MASK_ONE << 26)) != 0)\n+\tvalue26 /= value;\n+\n+      if ((mask & (MASK_ONE << 27)) != 0)\n+\tvalue27 /= value;\n+\n+      if ((mask & (MASK_ONE << 28)) != 0)\n+\tvalue28 /= value;\n+\n+      if ((mask & (MASK_ONE << 29)) != 0)\n+\tvalue29 /= value;\n+\n+      if ((mask & (MASK_ONE << 30)) != 0)\n+\tvalue30 /= value;\n+\n+      if ((mask & (MASK_ONE << 31)) != 0)\n+\tvalue31 /= value;\n+\n+      if ((mask & (MASK_ONE << 32)) != 0)\n+\tvalue32 /= value;\n+\n+      if ((mask & (MASK_ONE << 33)) != 0)\n+\tvalue33 /= value;\n+\n+      if ((mask & (MASK_ONE << 34)) != 0)\n+\tvalue34 /= value;\n+\n+      if ((mask & (MASK_ONE << 35)) != 0)\n+\tvalue35 /= value;\n+\n+      if ((mask & (MASK_ONE << 36)) != 0)\n+\tvalue36 /= value;\n+\n+      if ((mask & (MASK_ONE << 37)) != 0)\n+\tvalue37 /= value;\n+\n+      if ((mask & (MASK_ONE << 38)) != 0)\n+\tvalue38 /= value;\n+\n+      if ((mask & (MASK_ONE << 39)) != 0)\n+\tvalue39 /= value;\n+    }\n+\n+  while ((mask = *eq0_mask++) != 0)\n+    {\n+      eq0 = 0;\n+\n+      if ((mask & (MASK_ONE <<  0)) != 0)\n+\teq0 |= (value00 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  1)) != 0)\n+\teq0 |= (value01 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  2)) != 0)\n+\teq0 |= (value02 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  3)) != 0)\n+\teq0 |= (value03 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  4)) != 0)\n+\teq0 |= (value04 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  5)) != 0)\n+\teq0 |= (value05 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  6)) != 0)\n+\teq0 |= (value06 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  7)) != 0)\n+\teq0 |= (value07 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  8)) != 0)\n+\teq0 |= (value08 == ZERO);\n+\n+      if ((mask & (MASK_ONE <<  9)) != 0)\n+\teq0 |= (value09 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 10)) != 0)\n+\teq0 |= (value10 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 11)) != 0)\n+\teq0 |= (value11 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 12)) != 0)\n+\teq0 |= (value12 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 13)) != 0)\n+\teq0 |= (value13 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 14)) != 0)\n+\teq0 |= (value14 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 15)) != 0)\n+\teq0 |= (value15 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 16)) != 0)\n+\teq0 |= (value16 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 17)) != 0)\n+\teq0 |= (value17 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 18)) != 0)\n+\teq0 |= (value18 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 19)) != 0)\n+\teq0 |= (value19 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 20)) != 0)\n+\teq0 |= (value20 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 21)) != 0)\n+\teq0 |= (value21 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 22)) != 0)\n+\teq0 |= (value22 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 23)) != 0)\n+\teq0 |= (value23 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 24)) != 0)\n+\teq0 |= (value24 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 25)) != 0)\n+\teq0 |= (value25 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 26)) != 0)\n+\teq0 |= (value26 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 27)) != 0)\n+\teq0 |= (value27 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 28)) != 0)\n+\teq0 |= (value28 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 29)) != 0)\n+\teq0 |= (value29 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 30)) != 0)\n+\teq0 |= (value30 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 31)) != 0)\n+\teq0 |= (value31 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 32)) != 0)\n+\teq0 |= (value32 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 33)) != 0)\n+\teq0 |= (value33 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 34)) != 0)\n+\teq0 |= (value34 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 35)) != 0)\n+\teq0 |= (value35 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 36)) != 0)\n+\teq0 |= (value36 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 37)) != 0)\n+\teq0 |= (value37 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 38)) != 0)\n+\teq0 |= (value38 == ZERO);\n+\n+      if ((mask & (MASK_ONE << 39)) != 0)\n+\teq0 |= (value39 == ZERO);\n+\n+      *eq0_ptr++ = eq0;\n+    }\n+\n+  return (  value00 + value01 + value02 + value03 + value04\n+\t  + value05 + value06 + value07 + value08 + value09\n+\t  + value10 + value11 + value12 + value13 + value14\n+\t  + value15 + value16 + value17 + value18 + value19\n+\t  + value20 + value21 + value22 + value23 + value24\n+\t  + value25 + value26 + value27 + value28 + value29\n+\t  + value30 + value31 + value32 + value33 + value34\n+\t  + value35 + value36 + value37 + value38 + value39);\n+}\n+\n+/* { dg-final { scan-assembler \"fadds\"    } } */\n+/* { dg-final { scan-assembler \"fsubs\"    } } */\n+/* { dg-final { scan-assembler \"fmuls\"    } } */\n+/* { dg-final { scan-assembler \"fdivs\"    } } */\n+/* { dg-final { scan-assembler \"fcmpu\"    } } */\n+/* { dg-final { scan-assembler \"xsaddsp\"  } } */\n+/* { dg-final { scan-assembler \"xssubsp\"  } } */\n+/* { dg-final { scan-assembler \"xsmulsp\"  } } */\n+/* { dg-final { scan-assembler \"xsdivsp\"  } } */\n+/* { dg-final { scan-assembler \"xscmpudp\" } } */"}]}