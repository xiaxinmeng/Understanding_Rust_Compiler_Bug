{"sha": "6756706ea636d6f9aab85cef22659cc35143476f", "node_id": "C_kwDOANBUbNoAKDY3NTY3MDZlYTYzNmQ2ZjlhYWI4NWNlZjIyNjU5Y2MzNTE0MzQ3NmY", "commit": {"author": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2021-11-12T17:33:01Z"}, "committer": {"name": "Richard Sandiford", "email": "richard.sandiford@arm.com", "date": "2021-11-12T17:33:01Z"}, "message": "aarch64: Use real scalar op counts\n\nNow that vector finish_costs is passed the associated scalar costs,\nwe can record the scalar issue information while computing the scalar\ncosts, rather than trying to estimate it while computing the vector\ncosts.\n\nThis simplifies things a little, but the main motivation is to improve\naccuracy.\n\ngcc/\n\t* config/aarch64/aarch64.c (aarch64_vector_costs::m_scalar_ops)\n\t(aarch64_vector_costs::m_sve_ops): Replace with...\n\t(aarch64_vector_costs::m_ops): ...this.\n\t(aarch64_vector_costs::analyze_loop_vinfo): Update accordingly.\n\t(aarch64_vector_costs::adjust_body_cost_sve): Likewise.\n\t(aarch64_vector_costs::aarch64_vector_costs): Likewise.\n\tInitialize m_vec_flags here rather than in add_stmt_cost.\n\t(aarch64_vector_costs::count_ops): Test for scalar reductions too.\n\tAllow vectype to be null.\n\t(aarch64_vector_costs::add_stmt_cost): Call count_ops for scalar\n\tcode too.  Don't require vectype to be nonnull.\n\t(aarch64_vector_costs::adjust_body_cost): Take the loop_vec_info\n\tand scalar costs as parameters.  Use the scalar costs to determine\n\tthe cycles per iteration of the scalar loop, then multiply it\n\tby the estimated VF.\n\t(aarch64_vector_costs::finish_cost): Update call accordingly.", "tree": {"sha": "a77dfd353b0ade909d2be0f62e845af95b9f2591", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/a77dfd353b0ade909d2be0f62e845af95b9f2591"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/6756706ea636d6f9aab85cef22659cc35143476f", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6756706ea636d6f9aab85cef22659cc35143476f", "html_url": "https://github.com/Rust-GCC/gccrs/commit/6756706ea636d6f9aab85cef22659cc35143476f", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/6756706ea636d6f9aab85cef22659cc35143476f/comments", "author": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "committer": {"login": "rsandifo-arm", "id": 28043039, "node_id": "MDQ6VXNlcjI4MDQzMDM5", "avatar_url": "https://avatars.githubusercontent.com/u/28043039?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rsandifo-arm", "html_url": "https://github.com/rsandifo-arm", "followers_url": "https://api.github.com/users/rsandifo-arm/followers", "following_url": "https://api.github.com/users/rsandifo-arm/following{/other_user}", "gists_url": "https://api.github.com/users/rsandifo-arm/gists{/gist_id}", "starred_url": "https://api.github.com/users/rsandifo-arm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rsandifo-arm/subscriptions", "organizations_url": "https://api.github.com/users/rsandifo-arm/orgs", "repos_url": "https://api.github.com/users/rsandifo-arm/repos", "events_url": "https://api.github.com/users/rsandifo-arm/events{/privacy}", "received_events_url": "https://api.github.com/users/rsandifo-arm/received_events", "type": "User", "site_admin": false}, "parents": [{"sha": "902b7c9e1835bc11a6127f9bd0c18928bfdbf18d", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/902b7c9e1835bc11a6127f9bd0c18928bfdbf18d", "html_url": "https://github.com/Rust-GCC/gccrs/commit/902b7c9e1835bc11a6127f9bd0c18928bfdbf18d"}], "stats": {"total": 182, "additions": 88, "deletions": 94}, "files": [{"sha": "3944c095e1d898411ea6ec0c1a07b9a5c314a868", "filename": "gcc/config/aarch64/aarch64.c", "status": "modified", "additions": 88, "deletions": 94, "changes": 182, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/6756706ea636d6f9aab85cef22659cc35143476f/gcc%2Fconfig%2Faarch64%2Faarch64.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/6756706ea636d6f9aab85cef22659cc35143476f/gcc%2Fconfig%2Faarch64%2Faarch64.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64.c?ref=6756706ea636d6f9aab85cef22659cc35143476f", "patch": "@@ -14813,7 +14813,8 @@ class aarch64_vector_costs : public vector_costs\n \t\t\t\t\tfractional_cost, fractional_cost,\n \t\t\t\t\tbool, unsigned int, unsigned int *,\n \t\t\t\t\tbool *);\n-  unsigned int adjust_body_cost (unsigned int);\n+  unsigned int adjust_body_cost (loop_vec_info, const aarch64_vector_costs *,\n+\t\t\t\t unsigned int);\n \n   /* True if we have performed one-time initialization based on the\n      vec_info.  */\n@@ -14850,22 +14851,16 @@ class aarch64_vector_costs : public vector_costs\n      iterate, otherwise it is zero.  */\n   uint64_t m_num_vector_iterations = 0;\n \n-  /* Used only when vectorizing loops.  Estimates the number and kind of scalar\n-     operations that would be needed to perform the same work as one iteration\n-     of the vector loop.  */\n-  aarch64_vec_op_count m_scalar_ops;\n+  /* Used only when vectorizing loops.  Estimates the number and kind of\n+     operations that would be needed by one iteration of the scalar\n+     or vector loop.  */\n+  aarch64_vec_op_count m_ops;\n \n-  /* Used only when vectorizing loops.  If M_VEC_FLAGS & VEC_ADVSIMD,\n-     this structure estimates the number and kind of operations that the\n-     vector loop would contain.  If M_VEC_FLAGS & VEC_SVE, the structure\n-     estimates what the equivalent Advanced SIMD-only code would need in\n-     order to perform the same work as one iteration of the SVE loop.  */\n+  /* Used only when vectorizing loops for SVE.  It estimates what the\n+     equivalent Advanced SIMD-only code would need in order to perform\n+     the same work as one iteration of the SVE loop.  */\n   aarch64_vec_op_count m_advsimd_ops;\n \n-  /* Used only when vectorizing loops with SVE.  It estimates the number and\n-     kind of operations that the SVE loop would contain.  */\n-  aarch64_vec_op_count m_sve_ops;\n-\n   /* Used to detect cases in which we end up costing the same load twice,\n      once to account for results that are actually used and once to account\n      for unused results.  */\n@@ -14875,9 +14870,10 @@ class aarch64_vector_costs : public vector_costs\n aarch64_vector_costs::aarch64_vector_costs (vec_info *vinfo,\n \t\t\t\t\t    bool costing_for_scalar)\n   : vector_costs (vinfo, costing_for_scalar),\n-    m_scalar_ops (aarch64_tune_params.vec_costs->issue_info, 0),\n-    m_advsimd_ops (aarch64_tune_params.vec_costs->issue_info, VEC_ADVSIMD),\n-    m_sve_ops (aarch64_tune_params.vec_costs->issue_info, VEC_ANY_SVE)\n+    m_vec_flags (costing_for_scalar ? 0\n+\t\t : aarch64_classify_vector_mode (vinfo->vector_mode)),\n+    m_ops (aarch64_tune_params.vec_costs->issue_info, m_vec_flags),\n+    m_advsimd_ops (aarch64_tune_params.vec_costs->issue_info, VEC_ADVSIMD)\n {\n }\n \n@@ -15016,7 +15012,7 @@ aarch64_vector_costs::analyze_loop_vinfo (loop_vec_info loop_vinfo)\n       FOR_EACH_VEC_ELT (LOOP_VINFO_MASKS (loop_vinfo), num_vectors_m1, rgm)\n \tif (rgm->type)\n \t  num_masks += num_vectors_m1 + 1;\n-      m_sve_ops.pred_ops += num_masks * issue_info->sve->while_pred_ops;\n+      m_ops.pred_ops += num_masks * issue_info->sve->while_pred_ops;\n     }\n }\n \n@@ -15550,8 +15546,8 @@ aarch64_adjust_stmt_cost (vect_cost_for_stmt kind, stmt_vec_info stmt_info,\n /* COUNT, KIND, STMT_INFO and VECTYPE are the same as for\n    vector_costs::add_stmt_cost and they describe an operation in the\n    body of a vector loop.  Record issue information relating to the vector\n-   operation in OPS, where OPS is one of m_scalar_ops, m_advsimd_ops\n-   or m_sve_ops; see the comments above those variables for details.\n+   operation in OPS, where OPS is one of m_ops or m_advsimd_ops; see the\n+   comments above those variables for details.\n \n    FACTOR says how many iterations of the loop described by VEC_FLAGS would be\n    needed to match one iteration of the vector loop in VINFO.  */\n@@ -15570,14 +15566,14 @@ aarch64_vector_costs::count_ops (unsigned int count, vect_cost_for_stmt kind,\n \n   /* Calculate the minimum cycles per iteration imposed by a reduction\n      operation.  */\n-  if ((kind == vector_stmt || kind == vec_to_scalar)\n+  if ((kind == scalar_stmt || kind == vector_stmt || kind == vec_to_scalar)\n       && vect_is_reduction (stmt_info))\n     {\n       unsigned int base\n \t= aarch64_in_loop_reduction_latency (m_vinfo, stmt_info, vec_flags);\n       if (vect_reduc_type (m_vinfo, stmt_info) == FOLD_LEFT_REDUCTION)\n \t{\n-\t  if (aarch64_sve_mode_p (TYPE_MODE (vectype)))\n+\t  if (vectype && aarch64_sve_mode_p (TYPE_MODE (vectype)))\n \t    {\n \t      /* When costing an SVE FADDA, the vectorizer treats vec_to_scalar\n \t\t as a single operation, whereas for Advanced SIMD it is a\n@@ -15744,11 +15740,6 @@ aarch64_vector_costs::add_stmt_cost (int count, vect_cost_for_stmt kind,\n   loop_vec_info loop_vinfo = dyn_cast<loop_vec_info> (m_vinfo);\n   if (!m_analyzed_vinfo && aarch64_use_new_vector_costs_p ())\n     {\n-      /* If we're costing the vector code, record whether we're vectorizing\n-\t for Advanced SIMD or SVE.  */\n-      if (!m_costing_for_scalar)\n-\tm_vec_flags = aarch64_classify_vector_mode (m_vinfo->vector_mode);\n-\n       if (loop_vinfo)\n \tanalyze_loop_vinfo (loop_vinfo);\n \n@@ -15793,31 +15784,16 @@ aarch64_vector_costs::add_stmt_cost (int count, vect_cost_for_stmt kind,\n \t innermost loop, also estimate the operations that would need\n \t to be issued by all relevant implementations of the loop.  */\n       if (loop_vinfo\n-\t  && m_vec_flags\n-\t  && where == vect_body\n+\t  && (m_costing_for_scalar || where == vect_body)\n \t  && (!LOOP_VINFO_LOOP (loop_vinfo)->inner || in_inner_loop_p)\n-\t  && vectype\n \t  && stmt_cost != 0)\n \t{\n-\t  /* Record estimates for the scalar code.  */\n-\t  count_ops (count, kind, stmt_info, vectype, &m_scalar_ops,\n-\t\t     vect_nunits_for_cost (vectype));\n-\n-\t  if (aarch64_sve_mode_p (m_vinfo->vector_mode)\n-\t      && m_sve_ops.base_issue_info ())\n-\t    {\n-\t      /* Record estimates for a possible Advanced SIMD version\n-\t\t of the SVE code.  */\n-\t      count_ops (count, kind, stmt_info, vectype, &m_advsimd_ops,\n-\t\t\t aarch64_estimated_sve_vq ());\n-\n-\t      /* Record estimates for the SVE code itself.  */\n-\t      count_ops (count, kind, stmt_info, vectype, &m_sve_ops, 1);\n-\t    }\n-\t  else\n-\t    /* Record estimates for the Advanced SIMD code.  Treat SVE like\n-\t       Advanced SIMD if the CPU has no specific SVE costs.  */\n-\t    count_ops (count, kind, stmt_info, vectype, &m_advsimd_ops, 1);\n+\t  count_ops (count, kind, stmt_info, vectype, &m_ops, 1);\n+\t  if (aarch64_sve_mode_p (m_vinfo->vector_mode))\n+\t    /* Record estimates for a possible Advanced SIMD version\n+\t       of the SVE code.  */\n+\t    count_ops (count, kind, stmt_info, vectype,\n+\t\t       &m_advsimd_ops, aarch64_estimated_sve_vq ());\n \t}\n \n       /* If we're applying the SVE vs. Advanced SIMD unrolling heuristic,\n@@ -15885,7 +15861,7 @@ adjust_body_cost_sve (const aarch64_vec_issue_info *issue_info,\n   /* Estimate the minimum number of cycles per iteration needed to issue\n      non-predicate operations.  */\n   fractional_cost sve_nonpred_issue_cycles_per_iter\n-    = aarch64_estimate_min_cycles_per_iter (&m_sve_ops, issue_info->sve);\n+    = aarch64_estimate_min_cycles_per_iter (&m_ops, issue_info->sve);\n \n   /* Estimate the minimum number of cycles per iteration needed to rename\n      SVE instructions.\n@@ -15901,9 +15877,7 @@ adjust_body_cost_sve (const aarch64_vec_issue_info *issue_info,\n        ??? This value is very much on the pessimistic side, but seems to work\n        pretty well in practice.  */\n     sve_rename_cycles_per_iter\n-      = { m_sve_ops.general_ops\n-\t  + m_sve_ops.loads\n-\t  + m_sve_ops.pred_ops + 1, 5 };\n+      = { m_ops.general_ops + m_ops.loads + m_ops.pred_ops + 1, 5 };\n \n   /* Combine the rename and non-predicate issue limits into a single value.  */\n   fractional_cost sve_nonpred_cycles_per_iter\n@@ -15912,23 +15886,23 @@ adjust_body_cost_sve (const aarch64_vec_issue_info *issue_info,\n   /* Separately estimate the minimum number of cycles per iteration needed\n      to issue the predicate operations.  */\n   fractional_cost sve_pred_issue_cycles_per_iter\n-    = { m_sve_ops.pred_ops, issue_info->sve->pred_ops_per_cycle };\n+    = { m_ops.pred_ops, issue_info->sve->pred_ops_per_cycle };\n \n   /* Calculate the overall limit on the number of cycles per iteration.  */\n   fractional_cost sve_cycles_per_iter\n     = std::max (sve_nonpred_cycles_per_iter, sve_pred_issue_cycles_per_iter);\n \n   if (dump_enabled_p ())\n     {\n-      m_sve_ops.dump ();\n+      m_ops.dump ();\n       dump_printf_loc (MSG_NOTE, vect_location,\n \t\t       \"  estimated cycles per iteration = %f\\n\",\n \t\t       sve_cycles_per_iter.as_double ());\n-      if (m_sve_ops.pred_ops)\n+      if (m_ops.pred_ops)\n \tdump_printf_loc (MSG_NOTE, vect_location,\n \t\t\t \"    predicate issue = %f\\n\",\n \t\t\t sve_pred_issue_cycles_per_iter.as_double ());\n-      if (m_sve_ops.pred_ops || sve_rename_cycles_per_iter)\n+      if (m_ops.pred_ops || sve_rename_cycles_per_iter)\n \tdump_printf_loc (MSG_NOTE, vect_location,\n \t\t\t \"    non-predicate issue = %f\\n\",\n \t\t\t sve_nonpred_issue_cycles_per_iter.as_double ());\n@@ -16008,8 +15982,13 @@ adjust_body_cost_sve (const aarch64_vec_issue_info *issue_info,\n /* BODY_COST is the cost of a vector loop body.  Adjust the cost as necessary\n    and return the new cost.  */\n unsigned int\n-aarch64_vector_costs::adjust_body_cost (unsigned int body_cost)\n+aarch64_vector_costs::\n+adjust_body_cost (loop_vec_info loop_vinfo,\n+\t\t  const aarch64_vector_costs *scalar_costs,\n+\t\t  unsigned int body_cost)\n {\n+  const auto &scalar_ops = scalar_costs->m_ops;\n+  unsigned int estimated_vf = vect_vf_for_cost (loop_vinfo);\n   unsigned int orig_body_cost = body_cost;\n   bool should_disparage = false;\n \n@@ -16056,19 +16035,11 @@ aarch64_vector_costs::adjust_body_cost (unsigned int body_cost)\n     return body_cost;\n \n   fractional_cost scalar_cycles_per_iter\n-    = aarch64_estimate_min_cycles_per_iter (&m_scalar_ops,\n-\t\t\t\t\t    issue_info->scalar);\n-\n-  fractional_cost advsimd_cycles_per_iter\n-    = aarch64_estimate_min_cycles_per_iter (&m_advsimd_ops,\n-\t\t\t\t\t    issue_info->advsimd);\n+    = aarch64_estimate_min_cycles_per_iter (&scalar_ops, issue_info->scalar);\n+  scalar_cycles_per_iter *= estimated_vf;\n \n-  bool could_use_advsimd\n-    = ((m_vec_flags & VEC_ADVSIMD)\n-       || (aarch64_autovec_preference != 2\n-\t   && (aarch64_tune_params.extra_tuning_flags\n-\t       & AARCH64_EXTRA_TUNE_MATCHED_VECTOR_THROUGHPUT)\n-\t   && !m_saw_sve_only_op));\n+  fractional_cost vector_cycles_per_iter\n+    = aarch64_estimate_min_cycles_per_iter (&m_ops, m_ops.base_issue_info ());\n \n   if (dump_enabled_p ())\n     {\n@@ -16077,32 +16048,40 @@ aarch64_vector_costs::adjust_body_cost (unsigned int body_cost)\n \t\t\t \"Vector loop iterates at most %wd times\\n\",\n \t\t\t m_num_vector_iterations);\n       dump_printf_loc (MSG_NOTE, vect_location, \"Scalar issue estimate:\\n\");\n-      m_scalar_ops.dump ();\n+      scalar_ops.dump ();\n       dump_printf_loc (MSG_NOTE, vect_location,\n-\t\t       \"  estimated cycles per iteration = %f\\n\",\n-\t\t       scalar_cycles_per_iter.as_double ());\n-      if (could_use_advsimd)\n-\t{\n-\t  dump_printf_loc (MSG_NOTE, vect_location,\n-\t\t\t   \"Advanced SIMD issue estimate:\\n\");\n-\t  m_advsimd_ops.dump ();\n-\t  dump_printf_loc (MSG_NOTE, vect_location,\n-\t\t\t   \"  estimated cycles per iteration = %f\\n\",\n-\t\t\t   advsimd_cycles_per_iter.as_double ());\n-\t}\n-      else\n-\tdump_printf_loc (MSG_NOTE, vect_location,\n-\t\t\t \"Loop could not use Advanced SIMD\\n\");\n+\t\t       \"  estimated cycles per vector iteration\"\n+\t\t       \" (for VF %d) = %f\\n\",\n+\t\t       estimated_vf, scalar_cycles_per_iter.as_double ());\n     }\n \n-  fractional_cost vector_cycles_per_iter = advsimd_cycles_per_iter;\n-  unsigned int vector_reduction_latency = m_advsimd_ops.reduction_latency;\n-\n   if ((m_vec_flags & VEC_ANY_SVE) && issue_info->sve)\n     {\n+      bool could_use_advsimd\n+\t= (aarch64_autovec_preference != 2\n+\t   && (aarch64_tune_params.extra_tuning_flags\n+\t       & AARCH64_EXTRA_TUNE_MATCHED_VECTOR_THROUGHPUT)\n+\t   && !m_saw_sve_only_op);\n+\n+      fractional_cost advsimd_cycles_per_iter\n+\t= aarch64_estimate_min_cycles_per_iter (&m_advsimd_ops,\n+\t\t\t\t\t\tissue_info->advsimd);\n       if (dump_enabled_p ())\n-\tdump_printf_loc (MSG_NOTE, vect_location, \"SVE issue estimate:\\n\");\n-      vector_reduction_latency = m_sve_ops.reduction_latency;\n+\t{\n+\t  if (could_use_advsimd)\n+\t    {\n+\t      dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t       \"Advanced SIMD issue estimate:\\n\");\n+\t      m_advsimd_ops.dump ();\n+\t      dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t       \"  estimated cycles per iteration = %f\\n\",\n+\t\t\t       advsimd_cycles_per_iter.as_double ());\n+\t    }\n+\t  else\n+\t    dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t     \"Loop could not use Advanced SIMD\\n\");\n+\t  dump_printf_loc (MSG_NOTE, vect_location, \"SVE issue estimate:\\n\");\n+\t}\n       vector_cycles_per_iter\n \t= adjust_body_cost_sve (issue_info, scalar_cycles_per_iter,\n \t\t\t\tadvsimd_cycles_per_iter, could_use_advsimd,\n@@ -16123,6 +16102,18 @@ aarch64_vector_costs::adjust_body_cost (unsigned int body_cost)\n \t\t\t\t&body_cost, &should_disparage);\n \t}\n     }\n+  else\n+    {\n+      if (dump_enabled_p ())\n+\t{\n+\t  dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t   \"Vector issue estimate:\\n\");\n+\t  m_ops.dump ();\n+\t  dump_printf_loc (MSG_NOTE, vect_location,\n+\t\t\t   \"  estimated cycles per iteration = %f\\n\",\n+\t\t\t   vector_cycles_per_iter.as_double ());\n+\t}\n+    }\n \n   /* Decide whether to stick to latency-based costs or whether to try to\n      take issue rates into account.  */\n@@ -16164,8 +16155,8 @@ aarch64_vector_costs::adjust_body_cost (unsigned int body_cost)\n      vector code is an improvement, even if adding the other (non-loop-carried)\n      latencies tends to hide this saving.  We therefore reduce the cost of the\n      vector loop body in proportion to the saving.  */\n-  else if (m_scalar_ops.reduction_latency > vector_reduction_latency\n-\t   && m_scalar_ops.reduction_latency == scalar_cycles_per_iter\n+  else if (scalar_ops.reduction_latency > m_ops.reduction_latency\n+\t   && scalar_ops.reduction_latency == scalar_cycles_per_iter\n \t   && scalar_cycles_per_iter > vector_cycles_per_iter\n \t   && !should_disparage)\n     {\n@@ -16181,13 +16172,16 @@ aarch64_vector_costs::adjust_body_cost (unsigned int body_cost)\n }\n \n void\n-aarch64_vector_costs::finish_cost (const vector_costs *scalar_costs)\n+aarch64_vector_costs::finish_cost (const vector_costs *uncast_scalar_costs)\n {\n+  auto *scalar_costs\n+    = static_cast<const aarch64_vector_costs *> (uncast_scalar_costs);\n   loop_vec_info loop_vinfo = dyn_cast<loop_vec_info> (m_vinfo);\n   if (loop_vinfo\n       && m_vec_flags\n       && aarch64_use_new_vector_costs_p ())\n-    m_costs[vect_body] = adjust_body_cost (m_costs[vect_body]);\n+    m_costs[vect_body] = adjust_body_cost (loop_vinfo, scalar_costs,\n+\t\t\t\t\t   m_costs[vect_body]);\n \n   vector_costs::finish_cost (scalar_costs);\n }"}]}