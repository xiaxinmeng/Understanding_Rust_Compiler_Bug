{"sha": "831315d08c44b0419e63621aed289cfd1de68615", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6ODMxMzE1ZDA4YzQ0YjA0MTllNjM2MjFhZWQyODljZmQxZGU2ODYxNQ==", "commit": {"author": {"name": "Diego Novillo", "email": "dnovillo@gcc.gnu.org", "date": "2012-06-05T00:28:56Z"}, "committer": {"name": "Diego Novillo", "email": "dnovillo@gcc.gnu.org", "date": "2012-06-05T00:28:56Z"}, "message": "Extend validate_failures.py to run outside the build directory.\n\nThis patch adds three new arguments to validate_failures.py so\nit can be used outside the build directory:\n\n--ignore_missing_failures\n    When a failure is expected in the manifest but it is not found in\n    the actual results, the script produces a note alerting to this\n    fact. This means that the expected failure has been fixed, or it\n    did not run, or it may simply be flaky.\n\n    With this option, one can ask the script not to show the missing\n    failures.\n\n--manifest\n    Name of the manifest file to use.  By default, the script will\n    look for the manifest file in the source directory associated with\n    this build.  With this option, one can point to any arbitrary\n    manifest file.  I renamed the old --manifest flag to\n    --produce_manifest.\n\n--results\n    Space-separated list of .sum files with the testing results to\n    check. The only content needed from these files are the lines\n    starting with FAIL, XPASS or UNRESOLVED.\n\nFrom-SVN: r188217", "tree": {"sha": "c77879cf2b6cbb99c2ca5a756d1bd3f2ff9b3309", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/c77879cf2b6cbb99c2ca5a756d1bd3f2ff9b3309"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/831315d08c44b0419e63621aed289cfd1de68615", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/831315d08c44b0419e63621aed289cfd1de68615", "html_url": "https://github.com/Rust-GCC/gccrs/commit/831315d08c44b0419e63621aed289cfd1de68615", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/831315d08c44b0419e63621aed289cfd1de68615/comments", "author": null, "committer": null, "parents": [{"sha": "98786f0fd6a4a3b00199d58d0aefaa76dcf43d0e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/98786f0fd6a4a3b00199d58d0aefaa76dcf43d0e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/98786f0fd6a4a3b00199d58d0aefaa76dcf43d0e"}], "stats": {"total": 66, "additions": 48, "deletions": 18}, "files": [{"sha": "c07be84cd0311e2c60cb5e519abc25b64387fa0f", "filename": "contrib/testsuite-management/validate_failures.py", "status": "modified", "additions": 48, "deletions": 18, "changes": 66, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/831315d08c44b0419e63621aed289cfd1de68615/contrib%2Ftestsuite-management%2Fvalidate_failures.py", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/831315d08c44b0419e63621aed289cfd1de68615/contrib%2Ftestsuite-management%2Fvalidate_failures.py", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/contrib%2Ftestsuite-management%2Fvalidate_failures.py?ref=831315d08c44b0419e63621aed289cfd1de68615", "patch": "@@ -1,4 +1,4 @@\n-#!/usr/bin/python\n+#!/usr/bin/python2.6\n \n # Script to compare testsuite failures against a list of known-to-fail\n # tests.\n@@ -206,10 +206,8 @@ def GetSumFiles(builddir):\n   return sum_files\n \n \n-def GetResults(builddir):\n-  \"\"\"Collect all the test results from .sum files under the given build\n-  directory.\"\"\"\n-  sum_files = GetSumFiles(builddir)\n+def GetResults(sum_files):\n+  \"\"\"Collect all the test results from the given .sum files.\"\"\"\n   build_results = set()\n   for sum_fname in sum_files:\n     print '\\t%s' % sum_fname\n@@ -258,16 +256,26 @@ def PrintSummary(msg, summary):\n \n \n def CheckExpectedResults(options):\n-  (srcdir, target, valid_build) = GetBuildData(options)\n-  if not valid_build:\n-    return False\n+  if not options.manifest:\n+    (srcdir, target, valid_build) = GetBuildData(options)\n+    if not valid_build:\n+      return False\n+    manifest_name = _MANIFEST_PATH_PATTERN % (srcdir, target)\n+  else:\n+    manifest_name = options.manifest\n+    if not os.path.exists(manifest_name):\n+      Error('Manifest file %s does not exist.' % manifest_name)\n \n-  manifest_name = _MANIFEST_PATH_PATTERN % (srcdir, target)\n   print 'Manifest:         %s' % manifest_name\n   manifest = GetManifest(manifest_name)\n \n-  print 'Getting actual results from build'\n-  actual = GetResults(options.build_dir)\n+  if not options.results:\n+    print 'Getting actual results from build'\n+    sum_files = GetSumFiles(options.build_dir)\n+  else:\n+    print 'Getting actual results from user-provided results'\n+    sum_files = options.results.split()\n+  actual = GetResults(sum_files)\n \n   if options.verbosity >= 1:\n     PrintSummary('Tests expected to fail', manifest)\n@@ -280,7 +288,7 @@ def CheckExpectedResults(options):\n     PrintSummary('Build results not in the manifest', actual_vs_manifest)\n     tests_ok = False\n \n-  if len(manifest_vs_actual) > 0:\n+  if not options.ignore_missing_failures and len(manifest_vs_actual) > 0:\n     PrintSummary('Manifest results not present in the build'\n                  '\\n\\nNOTE: This is not a failure.  It just means that the '\n                  'manifest expected\\nthese tests to fail, '\n@@ -314,20 +322,42 @@ def ProduceManifest(options):\n \n def Main(argv):\n   parser = optparse.OptionParser(usage=__doc__)\n+\n+  # Keep the following list sorted by option name.\n   parser.add_option('--build_dir', action='store', type='string',\n                     dest='build_dir', default='.',\n                     help='Build directory to check (default = .)')\n-  parser.add_option('--manifest', action='store_true', dest='manifest',\n-                    default=False, help='Produce the manifest for the current '\n-                    'build (default = False)')\n   parser.add_option('--force', action='store_true', dest='force',\n-                    default=False, help='When used with --manifest, it will '\n-                    'overwrite an existing manifest file (default = False)')\n+                    default=False, help='When used with --produce_manifest, '\n+                    'it will overwrite an existing manifest file '\n+                    '(default = False)')\n+  parser.add_option('--ignore_missing_failures', action='store_true',\n+                    dest='ignore_missing_failures', default=False,\n+                    help='When a failure is expected in the manifest but '\n+                    'it is not found in the actual results, the script '\n+                    'produces a note alerting to this fact. This means '\n+                    'that the expected failure has been fixed, or '\n+                    'it did not run, or it may simply be flaky '\n+                    '(default = False)')\n+  parser.add_option('--manifest', action='store', type='string',\n+                    dest='manifest', default=None,\n+                    help='Name of the manifest file to use (default = '\n+                    'taken from contrib/testsuite-managment/<target>.xfail)')\n+  parser.add_option('--produce_manifest', action='store_true',\n+                    dest='produce_manifest', default=False,\n+                    help='Produce the manifest for the current '\n+                    'build (default = False)')\n+  parser.add_option('--results', action='store', type='string',\n+                    dest='results', default=None, help='Space-separated list '\n+                    'of .sum files with the testing results to check. The '\n+                    'only content needed from these files are the lines '\n+                    'starting with FAIL, XPASS or UNRESOLVED (default = '\n+                    '.sum files collected from the build directory).')\n   parser.add_option('--verbosity', action='store', dest='verbosity',\n                     type='int', default=0, help='Verbosity level (default = 0)')\n   (options, _) = parser.parse_args(argv[1:])\n \n-  if options.manifest:\n+  if options.produce_manifest:\n     retval = ProduceManifest(options)\n   else:\n     retval = CheckExpectedResults(options)"}]}