{"sha": "4cf817a7ebc85c4c12a3480b3179fb517964fdde", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NGNmODE3YTdlYmM4NWM0YzEyYTM0ODBiMzE3OWZiNTE3OTY0ZmRkZQ==", "commit": {"author": {"name": "Neil Booth", "email": "neil@daikokuya.demon.co.uk", "date": "2001-09-27T11:10:40Z"}, "committer": {"name": "Neil Booth", "email": "neil@gcc.gnu.org", "date": "2001-09-27T11:10:40Z"}, "message": "* doc/cppinternals.texi: Update.\n\nFrom-SVN: r45839", "tree": {"sha": "0a1a38965fbe8c58fe43ca9216a358d5a2fc6322", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/0a1a38965fbe8c58fe43ca9216a358d5a2fc6322"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/4cf817a7ebc85c4c12a3480b3179fb517964fdde", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4cf817a7ebc85c4c12a3480b3179fb517964fdde", "html_url": "https://github.com/Rust-GCC/gccrs/commit/4cf817a7ebc85c4c12a3480b3179fb517964fdde", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/4cf817a7ebc85c4c12a3480b3179fb517964fdde/comments", "author": null, "committer": null, "parents": [{"sha": "ef1d8fc882e76aa12ac17b1a4ff6daca5ba8e5a8", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/ef1d8fc882e76aa12ac17b1a4ff6daca5ba8e5a8", "html_url": "https://github.com/Rust-GCC/gccrs/commit/ef1d8fc882e76aa12ac17b1a4ff6daca5ba8e5a8"}], "stats": {"total": 190, "additions": 137, "deletions": 53}, "files": [{"sha": "e79797440b91740c87a437028dec7166262d0f40", "filename": "gcc/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4cf817a7ebc85c4c12a3480b3179fb517964fdde/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4cf817a7ebc85c4c12a3480b3179fb517964fdde/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=4cf817a7ebc85c4c12a3480b3179fb517964fdde", "patch": "@@ -1,3 +1,7 @@\n+2001-09-27  Neil Booth  <neil@daikokuya.demon.co.uk>\n+\n+\t* doc/cppinternals.texi: Update.\n+\n 2001-09-26  Neil Booth  <neil@daikokuya.demon.co.uk>\n \n \t* cpphash.h (struct cpp_pool): Remove locks and locked."}, {"sha": "bcd0fd3117eeb5e41daef260afc18d186233109c", "filename": "gcc/doc/cppinternals.texi", "status": "modified", "additions": 133, "deletions": 53, "changes": 186, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/4cf817a7ebc85c4c12a3480b3179fb517964fdde/gcc%2Fdoc%2Fcppinternals.texi", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/4cf817a7ebc85c4c12a3480b3179fb517964fdde/gcc%2Fdoc%2Fcppinternals.texi", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fdoc%2Fcppinternals.texi?ref=4cf817a7ebc85c4c12a3480b3179fb517964fdde", "patch": "@@ -41,8 +41,8 @@ into another language, under the above conditions for modified versions.\n @titlepage\n @c @finalout\n @title Cpplib Internals\n-@subtitle Last revised Jan 2001\n-@subtitle for GCC version 3.0\n+@subtitle Last revised September 2001\n+@subtitle for GCC version 3.1\n @author Neil Booth\n @page\n @vskip 0pt plus 1filll\n@@ -69,14 +69,14 @@ into another language, under the above conditions for modified versions.\n @node Top, Conventions,, (DIR)\n @chapter Cpplib---the core of the GNU C Preprocessor\n \n-The GNU C preprocessor in GCC 3.0 has been completely rewritten.  It is\n+The GNU C preprocessor in GCC 3.x has been completely rewritten.  It is\n now implemented as a library, cpplib, so it can be easily shared between\n a stand-alone preprocessor, and a preprocessor integrated with the C,\n C++ and Objective-C front ends.  It is also available for use by other\n programs, though this is not recommended as its exposed interface has\n not yet reached a point of reasonable stability.\n \n-This library has been written to be re-entrant, so that it can be used\n+The library has been written to be re-entrant, so that it can be used\n to preprocess many files simultaneously if necessary.  It has also been\n written with the preprocessing token as the fundamental unit; the\n preprocessor in previous versions of GCC would operate on text strings\n@@ -86,8 +86,6 @@ This brief manual documents some of the internals of cpplib, and a few\n tricky issues encountered.  It also describes certain behaviour we would\n like to preserve, such as the format and spacing of its output.\n \n-Identifiers, macro expansion, hash nodes, lexing.\n-\n @menu\n * Conventions::\t    Conventions used in the code.\n * Lexer::\t    The combined C, C++ and Objective-C Lexer.\n@@ -123,18 +121,106 @@ behaviour.\n @node Lexer, Whitespace, Conventions, Top\n @unnumbered The Lexer\n @cindex lexer\n-@cindex tokens\n-\n-The lexer is contained in the file @file{cpplex.c}.  We want to have a\n-lexer that is single-pass, for efficiency reasons.  We would also like\n-the lexer to only step forwards through the input files, and not step\n-back.  This will make future changes to support different character\n-sets, in particular state or shift-dependent ones, much easier.\n \n-This file also contains all information needed to spell a token, i.e.@: to\n-output it either in a diagnostic or to a preprocessed output file.  This\n-information is not exported, but made available to clients through such\n-functions as @samp{cpp_spell_token} and @samp{cpp_token_len}.\n+@section Overview\n+The lexer is contained in the file @file{cpplex.c}.  It is a hand-coded\n+lexer, and not implemented as a state machine.  It can understand C, C++\n+and Objective-C source code, and has been extended to allow reasonably\n+successful preprocessing of assembly language.  The lexer does not make\n+an initial pass to strip out trigraphs and escaped newlines, but handles\n+them as they are encountered in a single pass of the input file.  It\n+returns preprocessing tokens individually, not a line at a time.\n+\n+It is mostly transparent to users of the library, since the library's\n+interface for obtaining the next token, @code{cpp_get_token}, takes care\n+of lexing new tokens, handling directives, and expanding macros as\n+necessary.  However, the lexer does expose some functionality so that\n+clients of the library can easily spell a given token, such as\n+@code{cpp_spell_token} and @code{cpp_token_len}.  These functions are\n+useful when generating diagnostics, and for emitting the preprocessed\n+output.\n+\n+@section Lexing a token\n+Lexing of an individual token is handled by @code{_cpp_lex_direct} and\n+its subroutines.  In its current form the code is quite complicated,\n+with read ahead characters and suchlike, since it strives to not step\n+back in the character stream in preparation for handling non-ASCII file\n+encodings.  The current plan is to convert any such files to UTF-8\n+before processing them.  This complexity is therefore unnecessary and\n+will be removed, so I'll not discuss it further here.\n+\n+The job of @code{_cpp_lex_direct} is simply to lex a token.  It is not\n+responsible for issues like directive handling, returning lookahead\n+tokens directly, multiple-include optimisation, or conditional block\n+skipping.  It necessarily has a minor r@^ole to play in memory\n+management of lexed lines.  I discuss these issues in a separate section\n+(@pxref{Lexing a line}).\n+\n+The lexer places the token it lexes into storage pointed to by the\n+variable @var{cur_token}, and then increments it.  This variable is\n+important for correct diagnostic positioning.  Unless a specific line\n+and column are passed to the diagnostic routines, they will examine the\n+@var{line} and @var{col} values of the token just before the location\n+that @var{cur_token} points to, and use that location to report the\n+diagnostic.\n+\n+The lexer does not consider whitespace to be a token in its own right.\n+If whitespace (other than a new line) precedes a token, it sets the\n+@code{PREV_WHITE} bit in the token's flags.  Each token has its\n+@var{line} and @var{col} variables set to the line and column of the\n+first character of the token.  This line number is the line number in\n+the translation unit, and can be converted to a source (file, line) pair\n+using the line map code.\n+\n+The first token on a logical, i.e.@: unescaped, line has the flag\n+@code{BOL} set for beginning-of-line.  This flag is intended for\n+internal use, both to distinguish a @samp{#} that begins a directive\n+from one that doesn't, and to generate a callback to clients that want\n+to be notified about the start of every non-directive line with tokens\n+on it.  Clients cannot reliably determine this for themselves: the first\n+token might be a macro, and the tokens of a macro expansion do not have\n+the @code{BOL} flag set.  The macro expansion may even be empty, and the\n+next token on the line certainly won't have the @code{BOL} flag set.\n+\n+New lines are treated specially; exactly how the lexer handles them is\n+context-dependent.  The C standard mandates that directives are\n+terminated by the first unescaped newline character, even if it appears\n+in the middle of a macro expansion.  Therefore, if the state variable\n+@var{in_directive} is set, the lexer returns a @code{CPP_EOF} token,\n+which is normally used to indicate end-of-file, to indicate\n+end-of-directive.  In a directive a @code{CPP_EOF} token never means\n+end-of-file.  Conveniently, if the caller was @code{collect_args}, it\n+already handles @code{CPP_EOF} as if it were end-of-file, and reports an\n+error about an unterminated macro argument list.\n+\n+The C standard also specifies that a new line in the middle of the\n+arguments to a macro is treated as whitespace.  This white space is\n+important in case the macro argument is stringified.  The state variable\n+@code{parsing_args} is non-zero when the preprocessor is collecting the\n+arguments to a macro call.  It is set to 1 when looking for the opening\n+parenthesis to a function-like macro, and 2 when collecting the actual\n+arguments up to the closing parenthesis, since these two cases need to\n+be distinguished sometimes.  One such time is here: the lexer sets the\n+@code{PREV_WHITE} flag of a token if it meets a new line when\n+@code{parsing_args} is set to 2.  It doesn't set it if it meets a new\n+line when @code{parsing_args} is 1, since then code like\n+\n+@smallexample\n+#define foo() bar\n+foo\n+baz\n+@end smallexample\n+\n+@noindent would be output with an erroneous space before @samp{baz}:\n+\n+@smallexample\n+foo\n+ baz\n+@end smallexample\n+\n+This is a good example of the subtlety of getting token spacing correct\n+in the preprocessor; there are plenty of tests in the testsuite for\n+corner cases like this.\n \n The most painful aspect of lexing ISO-standard C and C++ is handling\n trigraphs and backlash-escaped newlines.  Trigraphs are processed before\n@@ -148,62 +234,56 @@ within the characters of an identifier, and even between the @samp{*}\n and @samp{/} that terminates a comment.  Moreover, you cannot be sure\n there is just one---there might be an arbitrarily long sequence of them.\n \n-So the routine @samp{parse_identifier}, that lexes an identifier, cannot\n-assume that it can scan forwards until the first non-identifier\n+So, for example, the routine that lexes a number, @code{parse_number},\n+cannot assume that it can scan forwards until the first non-number\n character and be done with it, because this could be the @samp{\\}\n introducing an escaped newline, or the @samp{?} introducing the trigraph\n-sequence that represents the @samp{\\} of an escaped newline.  Similarly\n-for the routine that handles numbers, @samp{parse_number}.  If these\n-routines stumble upon a @samp{?} or @samp{\\}, they call\n-@samp{skip_escaped_newlines} to skip over any potential escaped newlines\n-before checking whether they can finish.\n+sequence that represents the @samp{\\} of an escaped newline.  If it\n+encounters a @samp{?} or @samp{\\}, it calls @code{skip_escaped_newlines}\n+to skip over any potential escaped newlines before checking whether the\n+number has been finished.\n \n-Similarly code in the main body of @samp{_cpp_lex_token} cannot simply\n+Similarly code in the main body of @code{_cpp_lex_direct} cannot simply\n check for a @samp{=} after a @samp{+} character to determine whether it\n has a @samp{+=} token; it needs to be prepared for an escaped newline of\n-some sort.  These cases use the function @samp{get_effective_char},\n-which returns the first character after any intervening newlines.\n+some sort.  Such cases use the function @code{get_effective_char}, which\n+returns the first character after any intervening escaped newlines.\n \n-The lexer needs to keep track of the correct column position,\n-including counting tabs as specified by the @option{-ftabstop=} option.\n-This should be done even within comments; C-style comments can appear in\n-the middle of a line, and we want to report diagnostics in the correct\n+The lexer needs to keep track of the correct column position, including\n+counting tabs as specified by the @option{-ftabstop=} option.  This\n+should be done even within C-style comments; they can appear in the\n+middle of a line, and we want to report diagnostics in the correct\n position for text appearing after the end of the comment.\n \n-Some identifiers, such as @samp{__VA_ARGS__} and poisoned identifiers,\n+Some identifiers, such as @code{__VA_ARGS__} and poisoned identifiers,\n may be invalid and require a diagnostic.  However, if they appear in a\n macro expansion we don't want to complain with each use of the macro.\n It is therefore best to catch them during the lexing stage, in\n-@samp{parse_identifier}.  In both cases, whether a diagnostic is needed\n-or not is dependent upon lexer state.  For example, we don't want to\n-issue a diagnostic for re-poisoning a poisoned identifier, or for using\n-@samp{__VA_ARGS__} in the expansion of a variable-argument macro.\n-Therefore @samp{parse_identifier} makes use of flags to determine\n+@code{parse_identifier}.  In both cases, whether a diagnostic is needed\n+or not is dependent upon the lexer's state.  For example, we don't want\n+to issue a diagnostic for re-poisoning a poisoned identifier, or for\n+using @code{__VA_ARGS__} in the expansion of a variable-argument macro.\n+Therefore @code{parse_identifier} makes use of state flags to determine\n whether a diagnostic is appropriate.  Since we change state on a\n per-token basis, and don't lex whole lines at a time, this is not a\n problem.\n \n Another place where state flags are used to change behaviour is whilst\n-parsing header names.  Normally, a @samp{<} would be lexed as a single\n-token.  After a @code{#include} directive, though, it should be lexed\n-as a single token as far as the nearest @samp{>} character.  Note that\n-we don't allow the terminators of header names to be escaped; the first\n+lexing header names.  Normally, a @samp{<} would be lexed as a single\n+token.  After a @code{#include} directive, though, it should be lexed as\n+a single token as far as the nearest @samp{>} character.  Note that we\n+don't allow the terminators of header names to be escaped; the first\n @samp{\"} or @samp{>} terminates the header name.\n \n Interpretation of some character sequences depends upon whether we are\n lexing C, C++ or Objective-C, and on the revision of the standard in\n-force.  For example, @samp{::} is a single token in C++, but two\n-separate @samp{:} tokens, and almost certainly a syntax error, in C@.\n-Such cases are handled in the main function @samp{_cpp_lex_token}, based\n-upon the flags set in the @samp{cpp_options} structure.\n-\n-Note we have almost, but not quite, achieved the goal of not stepping\n-backwards in the input stream.  Currently @samp{skip_escaped_newlines}\n-does step back, though with care it should be possible to adjust it so\n-that this does not happen.  For example, one tricky issue is if we meet\n-a trigraph, but the command line option @option{-trigraphs} is not in\n-force but @option{-Wtrigraphs} is, we need to warn about it but then\n-buffer it and continue to treat it as 3 separate characters.\n+force.  For example, @samp{::} is a single token in C++, but in C it is\n+two separate @samp{:} tokens and almost certainly a syntax error.  Such\n+cases are handled by @code{_cpp_lex_direct} based upon command-line\n+flags stored in the @code{cpp_options} structure.\n+\n+@anchor{Lexing a line}\n+@section Lexing a line\n \n @node Whitespace, Hash Nodes, Lexer, Top\n @unnumbered Whitespace"}]}