{"sha": "c220e3a948cc132c6177d0563b2e8c119231bef9", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6YzIyMGUzYTk0OGNjMTMyYzYxNzdkMDU2M2IyZThjMTE5MjMxYmVmOQ==", "commit": {"author": {"name": "H.J. Lu", "email": "hjl@gcc.gnu.org", "date": "2003-06-06T14:06:41Z"}, "committer": {"name": "H.J. Lu", "email": "hjl@gcc.gnu.org", "date": "2003-06-06T14:06:41Z"}, "message": "Apply the right patch.\n\nFrom-SVN: r67544", "tree": {"sha": "242daf825119e8031a01e4fac2cd52a75bf2b440", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/242daf825119e8031a01e4fac2cd52a75bf2b440"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/c220e3a948cc132c6177d0563b2e8c119231bef9", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c220e3a948cc132c6177d0563b2e8c119231bef9", "html_url": "https://github.com/Rust-GCC/gccrs/commit/c220e3a948cc132c6177d0563b2e8c119231bef9", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/c220e3a948cc132c6177d0563b2e8c119231bef9/comments", "author": null, "committer": null, "parents": [{"sha": "30fb3231107d372c2e9df88e18714baae783870e", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/30fb3231107d372c2e9df88e18714baae783870e", "html_url": "https://github.com/Rust-GCC/gccrs/commit/30fb3231107d372c2e9df88e18714baae783870e"}], "stats": {"total": 528, "additions": 447, "deletions": 81}, "files": [{"sha": "8c1771265330468729ecd94625063b9f938fb6b3", "filename": "gcc/config/i386/mmintrin.h", "status": "modified", "additions": 330, "deletions": 57, "changes": 387, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/c220e3a948cc132c6177d0563b2e8c119231bef9/gcc%2Fconfig%2Fi386%2Fmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/c220e3a948cc132c6177d0563b2e8c119231bef9/gcc%2Fconfig%2Fi386%2Fmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fmmintrin.h?ref=c220e3a948cc132c6177d0563b2e8c119231bef9", "patch": "@@ -48,6 +48,12 @@ _mm_empty (void)\n   __builtin_ia32_emms ();\n }\n \n+static __inline void\n+_m_empty (void)\n+{\n+  _mm_empty ();\n+}\n+\n /* Convert I to a __m64 object.  The integer is zero-extended to 64-bits.  */\n static __inline __m64 \n _mm_cvtsi32_si64 (int __i)\n@@ -56,6 +62,12 @@ _mm_cvtsi32_si64 (int __i)\n   return (__m64) __tmp;\n }\n \n+static __inline __m64 \n+_m_from_int (int __i)\n+{\n+  return _mm_cvtsi32_si64 (__i);\n+}\n+\n #ifdef __x86_64__\n /* Convert I to a __m64 object.  */\n static __inline __m64 \n@@ -80,6 +92,12 @@ _mm_cvtsi64_si32 (__m64 __i)\n   return __tmp;\n }\n \n+static __inline int\n+_m_to_int (__m64 __i)\n+{\n+  return _mm_cvtsi64_si32 (__i);\n+}\n+\n #ifdef __x86_64__\n /* Convert the lower 32 bits of the __m64 object into an integer.  */\n static __inline long long\n@@ -98,6 +116,12 @@ _mm_packs_pi16 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_packsswb ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_packsswb (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_packs_pi16 (__m1, __m2);\n+}\n+\n /* Pack the two 32-bit values from M1 in to the lower two 16-bit values of\n    the result, and the two 32-bit values from M2 into the upper two 16-bit\n    values of the result, all with signed saturation.  */\n@@ -107,6 +131,12 @@ _mm_packs_pi32 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_packssdw ((__v2si)__m1, (__v2si)__m2);\n }\n \n+static __inline __m64\n+_m_packssdw (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_packs_pi32 (__m1, __m2);\n+}\n+\n /* Pack the four 16-bit values from M1 into the lower four 8-bit values of\n    the result, and the four 16-bit values from M2 into the upper four 8-bit\n    values of the result, all with unsigned saturation.  */\n@@ -116,6 +146,12 @@ _mm_packs_pu16 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_packuswb ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_packuswb (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_packs_pu16 (__m1, __m2);\n+}\n+\n /* Interleave the four 8-bit values from the high half of M1 with the four\n    8-bit values from the high half of M2.  */\n static __inline __m64\n@@ -124,6 +160,12 @@ _mm_unpackhi_pi8 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_punpckhbw ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n+static __inline __m64\n+_m_punpckhbw (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_unpackhi_pi8 (__m1, __m2);\n+}\n+\n /* Interleave the two 16-bit values from the high half of M1 with the two\n    16-bit values from the high half of M2.  */\n static __inline __m64\n@@ -132,6 +174,12 @@ _mm_unpackhi_pi16 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_punpckhwd ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_punpckhwd (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_unpackhi_pi16 (__m1, __m2);\n+}\n+\n /* Interleave the 32-bit value from the high half of M1 with the 32-bit\n    value from the high half of M2.  */\n static __inline __m64\n@@ -140,6 +188,12 @@ _mm_unpackhi_pi32 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_punpckhdq ((__v2si)__m1, (__v2si)__m2);\n }\n \n+static __inline __m64\n+_m_punpckhdq (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_unpackhi_pi32 (__m1, __m2);\n+}\n+\n /* Interleave the four 8-bit values from the low half of M1 with the four\n    8-bit values from the low half of M2.  */\n static __inline __m64\n@@ -148,6 +202,12 @@ _mm_unpacklo_pi8 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_punpcklbw ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n+static __inline __m64\n+_m_punpcklbw (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_unpacklo_pi8 (__m1, __m2);\n+}\n+\n /* Interleave the two 16-bit values from the low half of M1 with the two\n    16-bit values from the low half of M2.  */\n static __inline __m64\n@@ -156,6 +216,12 @@ _mm_unpacklo_pi16 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_punpcklwd ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_punpcklwd (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_unpacklo_pi16 (__m1, __m2);\n+}\n+\n /* Interleave the 32-bit value from the low half of M1 with the 32-bit\n    value from the low half of M2.  */\n static __inline __m64\n@@ -164,27 +230,51 @@ _mm_unpacklo_pi32 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_punpckldq ((__v2si)__m1, (__v2si)__m2);\n }\n \n+static __inline __m64\n+_m_punpckldq (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_unpacklo_pi32 (__m1, __m2);\n+}\n+\n /* Add the 8-bit values in M1 to the 8-bit values in M2.  */\n static __inline __m64\n _mm_add_pi8 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_paddb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n+static __inline __m64\n+_m_paddb (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_add_pi8 (__m1, __m2);\n+}\n+\n /* Add the 16-bit values in M1 to the 16-bit values in M2.  */\n static __inline __m64\n _mm_add_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_paddw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_paddw (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_add_pi16 (__m1, __m2);\n+}\n+\n /* Add the 32-bit values in M1 to the 32-bit values in M2.  */\n static __inline __m64\n _mm_add_pi32 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_paddd ((__v2si)__m1, (__v2si)__m2);\n }\n \n+static __inline __m64\n+_m_paddd (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_add_pi32 (__m1, __m2);\n+}\n+\n /* Add the 64-bit values in M1 to the 64-bit values in M2.  */\n static __inline __m64\n _mm_add_si64 (__m64 __m1, __m64 __m2)\n@@ -200,6 +290,12 @@ _mm_adds_pi8 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_paddsb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n+static __inline __m64\n+_m_paddsb (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_adds_pi8 (__m1, __m2);\n+}\n+\n /* Add the 16-bit values in M1 to the 16-bit values in M2 using signed\n    saturated arithmetic.  */\n static __inline __m64\n@@ -208,6 +304,12 @@ _mm_adds_pi16 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_paddsw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_paddsw (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_adds_pi16 (__m1, __m2);\n+}\n+\n /* Add the 8-bit values in M1 to the 8-bit values in M2 using unsigned\n    saturated arithmetic.  */\n static __inline __m64\n@@ -216,6 +318,12 @@ _mm_adds_pu8 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_paddusb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n+static __inline __m64\n+_m_paddusb (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_adds_pu8 (__m1, __m2);\n+}\n+\n /* Add the 16-bit values in M1 to the 16-bit values in M2 using unsigned\n    saturated arithmetic.  */\n static __inline __m64\n@@ -224,27 +332,51 @@ _mm_adds_pu16 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_paddusw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_paddusw (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_adds_pu16 (__m1, __m2);\n+}\n+\n /* Subtract the 8-bit values in M2 from the 8-bit values in M1.  */\n static __inline __m64\n _mm_sub_pi8 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_psubb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n+static __inline __m64\n+_m_psubb (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_sub_pi8 (__m1, __m2);\n+}\n+\n /* Subtract the 16-bit values in M2 from the 16-bit values in M1.  */\n static __inline __m64\n _mm_sub_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_psubw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_psubw (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_sub_pi16 (__m1, __m2);\n+}\n+\n /* Subtract the 32-bit values in M2 from the 32-bit values in M1.  */\n static __inline __m64\n _mm_sub_pi32 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_psubd ((__v2si)__m1, (__v2si)__m2);\n }\n \n+static __inline __m64\n+_m_psubd (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_sub_pi32 (__m1, __m2);\n+}\n+\n /* Add the 64-bit values in M1 to the 64-bit values in M2.  */\n static __inline __m64\n _mm_sub_si64 (__m64 __m1, __m64 __m2)\n@@ -260,6 +392,12 @@ _mm_subs_pi8 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_psubsb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n+static __inline __m64\n+_m_psubsb (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_subs_pi8 (__m1, __m2);\n+}\n+\n /* Subtract the 16-bit values in M2 from the 16-bit values in M1 using\n    signed saturating arithmetic.  */\n static __inline __m64\n@@ -268,6 +406,12 @@ _mm_subs_pi16 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_psubsw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_psubsw (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_subs_pi16 (__m1, __m2);\n+}\n+\n /* Subtract the 8-bit values in M2 from the 8-bit values in M1 using\n    unsigned saturating arithmetic.  */\n static __inline __m64\n@@ -276,6 +420,12 @@ _mm_subs_pu8 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_psubusb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n+static __inline __m64\n+_m_psubusb (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_subs_pu8 (__m1, __m2);\n+}\n+\n /* Subtract the 16-bit values in M2 from the 16-bit values in M1 using\n    unsigned saturating arithmetic.  */\n static __inline __m64\n@@ -284,6 +434,12 @@ _mm_subs_pu16 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_psubusw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_psubusw (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_subs_pu16 (__m1, __m2);\n+}\n+\n /* Multiply four 16-bit values in M1 by four 16-bit values in M2 producing\n    four 32-bit intermediate results, which are then summed by pairs to\n    produce two 32-bit results.  */\n@@ -293,6 +449,12 @@ _mm_madd_pi16 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_pmaddwd ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_pmaddwd (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_madd_pi16 (__m1, __m2);\n+}\n+\n /* Multiply four signed 16-bit values in M1 by four signed 16-bit values in\n    M2 and produce the high 16 bits of the 32-bit results.  */\n static __inline __m64\n@@ -301,6 +463,12 @@ _mm_mulhi_pi16 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_pmulhw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_pmulhw (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_mulhi_pi16 (__m1, __m2);\n+}\n+\n /* Multiply four 16-bit values in M1 by four 16-bit values in M2 and produce\n    the low 16 bits of the results.  */\n static __inline __m64\n@@ -309,117 +477,225 @@ _mm_mullo_pi16 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_pmullw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_pmullw (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_mullo_pi16 (__m1, __m2);\n+}\n+\n /* Shift four 16-bit values in M left by COUNT.  */\n static __inline __m64\n _mm_sll_pi16 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_psllw ((__v4hi)__m, (long long)__count);\n }\n \n+static __inline __m64\n+_m_psllw (__m64 __m, __m64 __count)\n+{\n+  return _mm_sll_pi16 (__m, __count);\n+}\n+\n static __inline __m64\n _mm_slli_pi16 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_psllw ((__v4hi)__m, __count);\n }\n \n+static __inline __m64\n+_m_psllwi (__m64 __m, int __count)\n+{\n+  return _mm_slli_pi16 (__m, __count);\n+}\n+\n /* Shift two 32-bit values in M left by COUNT.  */\n static __inline __m64\n _mm_sll_pi32 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_pslld ((__v2si)__m, (long long)__count);\n }\n \n+static __inline __m64\n+_m_pslld (__m64 __m, __m64 __count)\n+{\n+  return _mm_sll_pi32 (__m, __count);\n+}\n+\n static __inline __m64\n _mm_slli_pi32 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_pslld ((__v2si)__m, __count);\n }\n \n+static __inline __m64\n+_m_pslldi (__m64 __m, int __count)\n+{\n+  return _mm_slli_pi32 (__m, __count);\n+}\n+\n /* Shift the 64-bit value in M left by COUNT.  */\n static __inline __m64\n _mm_sll_si64 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_psllq ((long long)__m, (long long)__count);\n }\n \n+static __inline __m64\n+_m_psllq (__m64 __m, __m64 __count)\n+{\n+  return _mm_sll_si64 (__m, __count);\n+}\n+\n static __inline __m64\n _mm_slli_si64 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_psllq ((long long)__m, (long long)__count);\n }\n \n+static __inline __m64\n+_m_psllqi (__m64 __m, int __count)\n+{\n+  return _mm_slli_si64 (__m, __count);\n+}\n+\n /* Shift four 16-bit values in M right by COUNT; shift in the sign bit.  */\n static __inline __m64\n _mm_sra_pi16 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_psraw ((__v4hi)__m, (long long)__count);\n }\n \n+static __inline __m64\n+_m_psraw (__m64 __m, __m64 __count)\n+{\n+  return _mm_sra_pi16 (__m, __count);\n+}\n+\n static __inline __m64\n _mm_srai_pi16 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_psraw ((__v4hi)__m, __count);\n }\n \n+static __inline __m64\n+_m_psrawi (__m64 __m, int __count)\n+{\n+  return _mm_srai_pi16 (__m, __count);\n+}\n+\n /* Shift two 32-bit values in M right by COUNT; shift in the sign bit.  */\n static __inline __m64\n _mm_sra_pi32 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_psrad ((__v2si)__m, (long long)__count);\n }\n \n+static __inline __m64\n+_m_psrad (__m64 __m, __m64 __count)\n+{\n+  return _mm_sra_pi32 (__m, __count);\n+}\n+\n static __inline __m64\n _mm_srai_pi32 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_psrad ((__v2si)__m, __count);\n }\n \n+static __inline __m64\n+_m_psradi (__m64 __m, int __count)\n+{\n+  return _mm_srai_pi32 (__m, __count);\n+}\n+\n /* Shift four 16-bit values in M right by COUNT; shift in zeros.  */\n static __inline __m64\n _mm_srl_pi16 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_psrlw ((__v4hi)__m, (long long)__count);\n }\n \n+static __inline __m64\n+_m_psrlw (__m64 __m, __m64 __count)\n+{\n+  return _mm_srl_pi16 (__m, __count);\n+}\n+\n static __inline __m64\n _mm_srli_pi16 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_psrlw ((__v4hi)__m, __count);\n }\n \n+static __inline __m64\n+_m_psrlwi (__m64 __m, int __count)\n+{\n+  return _mm_srli_pi16 (__m, __count);\n+}\n+\n /* Shift two 32-bit values in M right by COUNT; shift in zeros.  */\n static __inline __m64\n _mm_srl_pi32 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_psrld ((__v2si)__m, (long long)__count);\n }\n \n+static __inline __m64\n+_m_psrld (__m64 __m, __m64 __count)\n+{\n+  return _mm_srl_pi32 (__m, __count);\n+}\n+\n static __inline __m64\n _mm_srli_pi32 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_psrld ((__v2si)__m, __count);\n }\n \n+static __inline __m64\n+_m_psrldi (__m64 __m, int __count)\n+{\n+  return _mm_srli_pi32 (__m, __count);\n+}\n+\n /* Shift the 64-bit value in M left by COUNT; shift in zeros.  */\n static __inline __m64\n _mm_srl_si64 (__m64 __m, __m64 __count)\n {\n   return (__m64) __builtin_ia32_psrlq ((long long)__m, (long long)__count);\n }\n \n+static __inline __m64\n+_m_psrlq (__m64 __m, __m64 __count)\n+{\n+  return _mm_srl_si64 (__m, __count);\n+}\n+\n static __inline __m64\n _mm_srli_si64 (__m64 __m, int __count)\n {\n   return (__m64) __builtin_ia32_psrlq ((long long)__m, (long long)__count);\n }\n \n+static __inline __m64\n+_m_psrlqi (__m64 __m, int __count)\n+{\n+  return _mm_srli_si64 (__m, __count);\n+}\n+\n /* Bit-wise AND the 64-bit values in M1 and M2.  */\n static __inline __m64\n _mm_and_si64 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_pand ((long long)__m1, (long long)__m2);\n }\n \n+static __inline __m64\n+_m_pand (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_and_si64 (__m1, __m2);\n+}\n+\n /* Bit-wise complement the 64-bit value in M1 and bit-wise AND it with the\n    64-bit value in M2.  */\n static __inline __m64\n@@ -428,20 +704,38 @@ _mm_andnot_si64 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_pandn ((long long)__m1, (long long)__m2);\n }\n \n+static __inline __m64\n+_m_pandn (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_andnot_si64 (__m1, __m2);\n+}\n+\n /* Bit-wise inclusive OR the 64-bit values in M1 and M2.  */\n static __inline __m64\n _mm_or_si64 (__m64 __m1, __m64 __m2)\n {\n   return (__m64)__builtin_ia32_por ((long long)__m1, (long long)__m2);\n }\n \n+static __inline __m64\n+_m_por (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_or_si64 (__m1, __m2);\n+}\n+\n /* Bit-wise exclusive OR the 64-bit values in M1 and M2.  */\n static __inline __m64\n _mm_xor_si64 (__m64 __m1, __m64 __m2)\n {\n   return (__m64)__builtin_ia32_pxor ((long long)__m1, (long long)__m2);\n }\n \n+static __inline __m64\n+_m_pxor (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_xor_si64 (__m1, __m2);\n+}\n+\n /* Compare eight 8-bit values.  The result of the comparison is 0xFF if the\n    test is true and zero if false.  */\n static __inline __m64\n@@ -450,12 +744,24 @@ _mm_cmpeq_pi8 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_pcmpeqb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n+static __inline __m64\n+_m_pcmpeqb (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_cmpeq_pi8 (__m1, __m2);\n+}\n+\n static __inline __m64\n _mm_cmpgt_pi8 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_pcmpgtb ((__v8qi)__m1, (__v8qi)__m2);\n }\n \n+static __inline __m64\n+_m_pcmpgtb (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_cmpgt_pi8 (__m1, __m2);\n+}\n+\n /* Compare four 16-bit values.  The result of the comparison is 0xFFFF if\n    the test is true and zero if false.  */\n static __inline __m64\n@@ -464,12 +770,24 @@ _mm_cmpeq_pi16 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_pcmpeqw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_pcmpeqw (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_cmpeq_pi16 (__m1, __m2);\n+}\n+\n static __inline __m64\n _mm_cmpgt_pi16 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_pcmpgtw ((__v4hi)__m1, (__v4hi)__m2);\n }\n \n+static __inline __m64\n+_m_pcmpgtw (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_cmpgt_pi16 (__m1, __m2);\n+}\n+\n /* Compare two 32-bit values.  The result of the comparison is 0xFFFFFFFF if\n    the test is true and zero if false.  */\n static __inline __m64\n@@ -478,12 +796,24 @@ _mm_cmpeq_pi32 (__m64 __m1, __m64 __m2)\n   return (__m64) __builtin_ia32_pcmpeqd ((__v2si)__m1, (__v2si)__m2);\n }\n \n+static __inline __m64\n+_m_pcmpeqd (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_cmpeq_pi32 (__m1, __m2);\n+}\n+\n static __inline __m64\n _mm_cmpgt_pi32 (__m64 __m1, __m64 __m2)\n {\n   return (__m64) __builtin_ia32_pcmpgtd ((__v2si)__m1, (__v2si)__m2);\n }\n \n+static __inline __m64\n+_m_pcmpgtd (__m64 __m1, __m64 __m2)\n+{\n+  return _mm_cmpgt_pi32 (__m1, __m2);\n+}\n+\n /* Creates a 64-bit zero.  */\n static __inline __m64\n _mm_setzero_si64 (void)\n@@ -583,62 +913,5 @@ _mm_set1_pi8 (char __b)\n   return _mm_set1_pi32 (__i);\n }\n \n-/* Alternate intrinsic name definitions.  */\n-#define\t_m_empty\t_mm_empty\n-#define\t_m_from_int\t_mm_cvtsi32_si64\n-#define\t_m_to_int\t_mm_cvtsi64_si32\n-#define\t_m_packsswb\t_mm_packs_pi16\n-#define\t_m_packssdw\t_mm_packs_pi32\n-#define\t_m_packuswb\t_mm_packs_pu16\n-#define\t_m_punpckhbw\t_mm_unpackhi_pi8\n-#define\t_m_punpckhwd\t_mm_unpackhi_pi16\n-#define\t_m_punpckhdq\t_mm_unpackhi_pi32\n-#define\t_m_punpcklbw\t_mm_unpacklo_pi8\n-#define\t_m_punpcklwd\t_mm_unpacklo_pi16\n-#define\t_m_punpckldq\t_mm_unpacklo_pi32\n-#define\t_m_paddb\t_mm_add_pi8\n-#define\t_m_paddw\t_mm_add_pi16\n-#define\t_m_paddd\t_mm_add_pi32\n-#define\t_m_paddsb\t_mm_adds_pi8\n-#define\t_m_paddsw\t_mm_adds_pi16\n-#define\t_m_paddusb\t_mm_adds_pu8\n-#define\t_m_paddusw\t_mm_adds_pu16\n-#define\t_m_psubb\t_mm_sub_pi8\n-#define\t_m_psubw\t_mm_sub_pi16\n-#define\t_m_psubd\t_mm_sub_pi32\n-#define\t_m_psubsb\t_mm_subs_pi8\n-#define\t_m_psubsw\t_mm_subs_pi16\n-#define\t_m_psubusb\t_mm_subs_pu8\n-#define\t_m_psubusw\t_mm_subs_pu16\n-#define\t_m_pmaddwd\t_mm_madd_pi16\n-#define\t_m_pmulhw\t_mm_mulhi_pi16\n-#define\t_m_pmullw\t_mm_mullo_pi16\n-#define\t_m_psllw\t_mm_sll_pi16\n-#define\t_m_psllwi\t_mm_slli_pi16\n-#define\t_m_pslld\t_mm_sll_pi32\n-#define\t_m_pslldi\t_mm_slli_pi32\n-#define\t_m_psllq\t_mm_sll_si64\n-#define\t_m_psllqi\t_mm_slli_si64\n-#define\t_m_psraw\t_mm_sra_pi16\n-#define\t_m_psrawi\t_mm_srai_pi16\n-#define\t_m_psrad\t_mm_sra_pi32\n-#define\t_m_psradi\t_mm_srai_pi32\n-#define\t_m_psrlw\t_mm_srl_pi16\n-#define\t_m_psrlwi\t_mm_srli_pi16\n-#define\t_m_psrld\t_mm_srl_pi32\n-#define\t_m_psrldi\t_mm_srli_pi32\n-#define\t_m_psrlq\t_mm_srl_si64\n-#define\t_m_psrlqi\t_mm_srli_si64\n-#define\t_m_pand\t\t_mm_and_si64\n-#define\t_m_pandn\t_mm_andnot_si64\n-#define\t_m_por\t\t_mm_or_si64\n-#define\t_m_pxor\t\t_mm_xor_si64\n-#define\t_m_pcmpeqb\t_mm_cmpeq_pi8\n-#define\t_m_pcmpeqw\t_mm_cmpeq_pi16\n-#define\t_m_pcmpeqd\t_mm_cmpeq_pi32\n-#define\t_m_pcmpgtb\t_mm_cmpgt_pi8\n-#define\t_m_pcmpgtw\t_mm_cmpgt_pi16\n-#define\t_m_pcmpgtd\t_mm_cmpgt_pi32\n-\n #endif /* __MMX__ */\n #endif /* _MMINTRIN_H_INCLUDED */"}, {"sha": "8125ff385d74c13e483e2bd1df40829b482e9b23", "filename": "gcc/config/i386/xmmintrin.h", "status": "modified", "additions": 117, "deletions": 24, "changes": 141, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/c220e3a948cc132c6177d0563b2e8c119231bef9/gcc%2Fconfig%2Fi386%2Fxmmintrin.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/c220e3a948cc132c6177d0563b2e8c119231bef9/gcc%2Fconfig%2Fi386%2Fxmmintrin.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Fi386%2Fxmmintrin.h?ref=c220e3a948cc132c6177d0563b2e8c119231bef9", "patch": "@@ -475,6 +475,12 @@ _mm_cvtss_si32 (__m128 __A)\n   return __builtin_ia32_cvtss2si ((__v4sf) __A);\n }\n \n+static __inline int\n+_mm_cvt_ss2si (__m128 __A)\n+{\n+  return _mm_cvtss_si32 (__A);\n+}\n+\n #ifdef __x86_64__\n /* Convert the lower SPFP value to a 32-bit integer according to the current\n    rounding mode.  */\n@@ -493,13 +499,25 @@ _mm_cvtps_pi32 (__m128 __A)\n   return (__m64) __builtin_ia32_cvtps2pi ((__v4sf) __A);\n }\n \n+static __inline __m64\n+_mm_cvt_ps2pi (__m128 __A)\n+{\n+  return _mm_cvtps_pi32 (__A);\n+}\n+\n /* Truncate the lower SPFP value to a 32-bit integer.  */\n static __inline int\n _mm_cvttss_si32 (__m128 __A)\n {\n   return __builtin_ia32_cvttss2si ((__v4sf) __A);\n }\n \n+static __inline int\n+_mm_cvtt_ss2si (__m128 __A)\n+{\n+  return _mm_cvttss_si32 (__A);\n+}\n+\n #ifdef __x86_64__\n /* Truncate the lower SPFP value to a 32-bit integer.  */\n static __inline long long\n@@ -517,13 +535,25 @@ _mm_cvttps_pi32 (__m128 __A)\n   return (__m64) __builtin_ia32_cvttps2pi ((__v4sf) __A);\n }\n \n+static __inline __m64\n+_mm_cvtt_ps2pi (__m128 __A)\n+{\n+  return _mm_cvttps_pi32 (__A);\n+}\n+\n /* Convert B to a SPFP value and insert it as element zero in A.  */\n static __inline __m128\n _mm_cvtsi32_ss (__m128 __A, int __B)\n {\n   return (__m128) __builtin_ia32_cvtsi2ss ((__v4sf) __A, __B);\n }\n \n+static __inline __m128\n+_mm_cvt_si2ss (__m128 __A, int __B)\n+{\n+  return _mm_cvtsi32_ss (__A, __B);\n+}\n+\n #ifdef __x86_64__\n /* Convert B to a SPFP value and insert it as element zero in A.  */\n static __inline __m128\n@@ -541,6 +571,12 @@ _mm_cvtpi32_ps (__m128 __A, __m64 __B)\n   return (__m128) __builtin_ia32_cvtpi2ps ((__v4sf) __A, (__v2si)__B);\n }\n \n+static __inline __m128\n+_mm_cvt_pi2ps (__m128 __A, __m64 __B)\n+{\n+  return _mm_cvtpi32_ps (__A, __B);\n+}\n+\n /* Convert the four signed 16-bit values in A to SPFP form.  */\n static __inline __m128\n _mm_cvtpi16_ps (__m64 __A)\n@@ -942,9 +978,16 @@ _mm_extract_pi16 (__m64 __A, int __N)\n {\n   return __builtin_ia32_pextrw ((__v4hi)__A, __N);\n }\n+\n+static __inline int\n+_m_pextrw (__m64 __A, int __N)\n+{\n+  return _mm_extract_pi16 (__A, __N);\n+}\n #else\n #define _mm_extract_pi16(A, N) \\\n   __builtin_ia32_pextrw ((__v4hi)(A), (N))\n+#define _m_pextrw(A, N)\t\t_mm_extract_pi16((A), (N))\n #endif\n \n /* Inserts word D into one of four words of A.  The selector N must be\n@@ -955,9 +998,16 @@ _mm_insert_pi16 (__m64 __A, int __D, int __N)\n {\n   return (__m64)__builtin_ia32_pinsrw ((__v4hi)__A, __D, __N);\n }\n+\n+static __inline __m64\n+_m_pinsrw (__m64 __A, int __D, int __N)\n+{\n+  return _mm_insert_pi16 (__A, __D, __N);\n+}\n #else\n #define _mm_insert_pi16(A, D, N) \\\n   ((__m64) __builtin_ia32_pinsrw ((__v4hi)(A), (D), (N)))\n+#define _m_pinsrw(A, D, N)\t _mm_insert_pi16((A), (D), (N))\n #endif\n \n /* Compute the element-wise maximum of signed 16-bit values.  */\n@@ -967,34 +1017,64 @@ _mm_max_pi16 (__m64 __A, __m64 __B)\n   return (__m64) __builtin_ia32_pmaxsw ((__v4hi)__A, (__v4hi)__B);\n }\n \n+static __inline __m64\n+_m_pmaxsw (__m64 __A, __m64 __B)\n+{\n+  return _mm_max_pi16 (__A, __B);\n+}\n+\n /* Compute the element-wise maximum of unsigned 8-bit values.  */\n static __inline __m64\n _mm_max_pu8 (__m64 __A, __m64 __B)\n {\n   return (__m64) __builtin_ia32_pmaxub ((__v8qi)__A, (__v8qi)__B);\n }\n \n+static __inline __m64\n+_m_pmaxub (__m64 __A, __m64 __B)\n+{\n+  return _mm_max_pu8 (__A, __B);\n+}\n+\n /* Compute the element-wise minimum of signed 16-bit values.  */\n static __inline __m64\n _mm_min_pi16 (__m64 __A, __m64 __B)\n {\n   return (__m64) __builtin_ia32_pminsw ((__v4hi)__A, (__v4hi)__B);\n }\n \n+static __inline __m64\n+_m_pminsw (__m64 __A, __m64 __B)\n+{\n+  return _mm_min_pi16 (__A, __B);\n+}\n+\n /* Compute the element-wise minimum of unsigned 8-bit values.  */\n static __inline __m64\n _mm_min_pu8 (__m64 __A, __m64 __B)\n {\n   return (__m64) __builtin_ia32_pminub ((__v8qi)__A, (__v8qi)__B);\n }\n \n+static __inline __m64\n+_m_pminub (__m64 __A, __m64 __B)\n+{\n+  return _mm_min_pu8 (__A, __B);\n+}\n+\n /* Create an 8-bit mask of the signs of 8-bit values.  */\n static __inline int\n _mm_movemask_pi8 (__m64 __A)\n {\n   return __builtin_ia32_pmovmskb ((__v8qi)__A);\n }\n \n+static __inline int\n+_m_pmovmskb (__m64 __A)\n+{\n+  return _mm_movemask_pi8 (__A);\n+}\n+\n /* Multiply four unsigned 16-bit values in A by four unsigned 16-bit values\n    in B and produce the high 16 bits of the 32-bit results.  */\n static __inline __m64\n@@ -1003,6 +1083,12 @@ _mm_mulhi_pu16 (__m64 __A, __m64 __B)\n   return (__m64) __builtin_ia32_pmulhuw ((__v4hi)__A, (__v4hi)__B);\n }\n \n+static __inline __m64\n+_m_pmulhuw (__m64 __A, __m64 __B)\n+{\n+  return _mm_mulhi_pu16 (__A, __B);\n+}\n+\n /* Return a combination of the four 16-bit values in A.  The selector\n    must be an immediate.  */\n #if 0\n@@ -1011,9 +1097,16 @@ _mm_shuffle_pi16 (__m64 __A, int __N)\n {\n   return (__m64) __builtin_ia32_pshufw ((__v4hi)__A, __N);\n }\n+\n+static __inline __m64\n+_m_pshufw (__m64 __A, int __N)\n+{\n+  return _mm_shuffle_pi16 (__A, __N);\n+}\n #else\n #define _mm_shuffle_pi16(A, N) \\\n   ((__m64) __builtin_ia32_pshufw ((__v4hi)(A), (N)))\n+#define _m_pshufw(A, N)\t\t_mm_shuffle_pi16 ((A), (N))\n #endif\n \n /* Conditionally store byte elements of A into P.  The high bit of each\n@@ -1025,20 +1118,38 @@ _mm_maskmove_si64 (__m64 __A, __m64 __N, char *__P)\n   __builtin_ia32_maskmovq ((__v8qi)__A, (__v8qi)__N, __P);\n }\n \n+static __inline void\n+_m_maskmovq (__m64 __A, __m64 __N, char *__P)\n+{\n+  _mm_maskmove_si64 (__A, __N, __P);\n+}\n+\n /* Compute the rounded averages of the unsigned 8-bit values in A and B.  */\n static __inline __m64\n _mm_avg_pu8 (__m64 __A, __m64 __B)\n {\n   return (__m64) __builtin_ia32_pavgb ((__v8qi)__A, (__v8qi)__B);\n }\n \n+static __inline __m64\n+_m_pavgb (__m64 __A, __m64 __B)\n+{\n+  return _mm_avg_pu8 (__A, __B);\n+}\n+\n /* Compute the rounded averages of the unsigned 16-bit values in A and B.  */\n static __inline __m64\n _mm_avg_pu16 (__m64 __A, __m64 __B)\n {\n   return (__m64) __builtin_ia32_pavgw ((__v4hi)__A, (__v4hi)__B);\n }\n \n+static __inline __m64\n+_m_pavgw (__m64 __A, __m64 __B)\n+{\n+  return _mm_avg_pu16 (__A, __B);\n+}\n+\n /* Compute the sum of the absolute differences of the unsigned 8-bit\n    values in A and B.  Return the value in the lower 16-bit word; the\n    upper words are cleared.  */\n@@ -1048,6 +1159,12 @@ _mm_sad_pu8 (__m64 __A, __m64 __B)\n   return (__m64) __builtin_ia32_psadbw ((__v8qi)__A, (__v8qi)__B);\n }\n \n+static __inline __m64\n+_m_psadbw (__m64 __A, __m64 __B)\n+{\n+  return _mm_sad_pu8 (__A, __B);\n+}\n+\n /* Loads one cache line from address P to a location \"closer\" to the\n    processor.  The selector I specifies the type of prefetch operation.  */\n #if 0\n@@ -1106,30 +1223,6 @@ do {\t\t\t\t\t\t\t\t\t\\\n   (row3) = __builtin_ia32_shufps (__t2, __t3, 0xDD);\t\t\t\\\n } while (0)\n \n-/* Alternate intrinsic name definitions.  */\n-#define\t_mm_cvt_ss2si\t_mm_cvtss_si32\n-#define\t_mm_cvt_ps2pi\t_mm_cvtps_pi32\n-#define\t_mm_cvtt_ss2si\t_mm_cvttss_si32\n-#define\t_mm_cvtt_ps2pi\t_mm_cvttps_pi32\n-#define\t_mm_cvt_si2ss\t_mm_cvtsi32_ss\n-#define\t_mm_cvt_pi2ps\t_mm_cvtpi32_ps\n-#define\t_m_pextrw\t_mm_extract_pi16\n-#define\t_m_pinsrw\t_mm_insert_pi16\n-#define\t_m_pmaxsw\t_mm_max_pi16\n-#define\t_m_pmaxub\t_mm_max_pu8\n-#define\t_m_pminsw\t_mm_min_pi16\n-#define\t_m_pminub\t_mm_min_pu8\n-#define\t_m_pmovmskb\t_mm_movemask_pi8\n-#define\t_m_pmulhuw\t_mm_mulhi_pu16\n-#define\t_m_pshufw\t_mm_shuffle_pi16\n-#define\t_m_maskmovq\t_mm_maskmove_si64\n-#define\t_m_pavgb\t_mm_avg_pu8\n-#define\t_m_pavgw\t_mm_avg_pu16\n-#define\t_m_psadbw\t_mm_sad_pu8\n-#define\t_mm_set_ps1\t_mm_set1_ps\n-#define\t_mm_load_ps1\t_mm_load1_ps\n-#define\t_mm_store_ps1\t_mm_store1_ps\n-\n /* For backward source compatibility.  */\n #include <emmintrin.h>\n "}]}