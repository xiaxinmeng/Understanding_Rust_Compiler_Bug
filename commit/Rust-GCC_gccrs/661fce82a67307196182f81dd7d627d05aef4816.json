{"sha": "661fce82a67307196182f81dd7d627d05aef4816", "node_id": "MDY6Q29tbWl0MTM2NTMxMDA6NjYxZmNlODJhNjczMDcxOTYxODJmODFkZDdkNjI3ZDA1YWVmNDgxNg==", "commit": {"author": {"name": "Alan Lawrence", "email": "alan.lawrence@arm.com", "date": "2014-12-09T19:52:22Z"}, "committer": {"name": "Alan Lawrence", "email": "alalaw01@gcc.gnu.org", "date": "2014-12-09T19:52:22Z"}, "message": "[AArch64] Fix ICE on non-constant indices to __builtin_aarch64_im_lane_boundsi\n\ngcc/:\n\n\t* config/aarch64/aarch64-builtins.c (aarch64_types_binopv_qualifiers,\n\tTYPES_BINOPV): Delete.\n\t(enum aarch64_builtins): Add AARCH64_BUILTIN_SIMD_LANE_CHECK and\n\tAARCH64_SIMD_PATTERN_START.\n\t(aarch64_init_simd_builtins): Register\n\t__builtin_aarch64_im_lane_boundsi; use  AARCH64_SIMD_PATTERN_START.\n\t(aarch64_simd_expand_builtin): Handle AARCH64_BUILTIN_LANE_CHECK; use\n\tAARCH64_SIMD_PATTERN_START.\n\n\t* config/aarch64/aarch64-simd.md (aarch64_im_lane_boundsi): Delete.\n\t* config/aarch64/aarch64-simd-builtins.def (im_lane_bound): Delete.\n\n\t* config/aarch64/arm_neon.h (__AARCH64_LANE_CHECK): New.\n\t(__aarch64_vget_lane_f64, __aarch64_vget_lane_s64,\n\t__aarch64_vget_lane_u64, __aarch64_vset_lane_any, vdupd_lane_f64,\n\tvdupd_lane_s64, vdupd_lane_u64, vext_f32, vext_f64, vext_p8, vext_p16,\n\tvext_s8, vext_s16, vext_s32, vext_s64, vext_u8, vext_u16, vext_u32,\n\tvext_u64, vextq_f32, vextq_f64, vextq_p8, vextq_p16, vextq_s8,\n\tvextq_s16, vextq_s32, vextq_s64, vextq_u8, vextq_u16, vextq_u32,\n\tvextq_u64, vmulq_lane_f64): Use __AARCH64_LANE_CHECK.\n\ngcc/testsuite/:\n\n\t* gcc.target/aarch64/simd/vset_lane_s16_const_1.c: New test.\n\nFrom-SVN: r218532", "tree": {"sha": "c7ce28ff777c725cc235ee91a90c532dad5f1375", "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/trees/c7ce28ff777c725cc235ee91a90c532dad5f1375"}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/git/commits/661fce82a67307196182f81dd7d627d05aef4816", "comment_count": 0, "verification": {"verified": false, "reason": "unsigned", "signature": null, "payload": null}}, "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/661fce82a67307196182f81dd7d627d05aef4816", "html_url": "https://github.com/Rust-GCC/gccrs/commit/661fce82a67307196182f81dd7d627d05aef4816", "comments_url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/661fce82a67307196182f81dd7d627d05aef4816/comments", "author": null, "committer": null, "parents": [{"sha": "2310e29f49d5b2c7fe9c6ec920f79192728d0f87", "url": "https://api.github.com/repos/Rust-GCC/gccrs/commits/2310e29f49d5b2c7fe9c6ec920f79192728d0f87", "html_url": "https://github.com/Rust-GCC/gccrs/commit/2310e29f49d5b2c7fe9c6ec920f79192728d0f87"}], "stats": {"total": 160, "additions": 104, "deletions": 56}, "files": [{"sha": "d6bb178733276e3d1efc4ee1a92ee3936aacef28", "filename": "gcc/ChangeLog", "status": "modified", "additions": 23, "deletions": 0, "changes": 23, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/661fce82a67307196182f81dd7d627d05aef4816/gcc%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/661fce82a67307196182f81dd7d627d05aef4816/gcc%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2FChangeLog?ref=661fce82a67307196182f81dd7d627d05aef4816", "patch": "@@ -1,3 +1,26 @@\n+2014-12-09  Alan Lawrence  <alan.lawrence@arm.com>\n+\n+\t* config/aarch64/aarch64-builtins.c (aarch64_types_binopv_qualifiers,\n+\tTYPES_BINOPV): Delete.\n+\t(enum aarch64_builtins): Add AARCH64_BUILTIN_SIMD_LANE_CHECK and\n+\tAARCH64_SIMD_PATTERN_START.\n+\t(aarch64_init_simd_builtins): Register\n+\t__builtin_aarch64_im_lane_boundsi; use  AARCH64_SIMD_PATTERN_START.\n+\t(aarch64_simd_expand_builtin): Handle AARCH64_BUILTIN_LANE_CHECK; use\n+\tAARCH64_SIMD_PATTERN_START.\n+\n+\t* config/aarch64/aarch64-simd.md (aarch64_im_lane_boundsi): Delete.\n+\t* config/aarch64/aarch64-simd-builtins.def (im_lane_bound): Delete.\n+\n+\t* config/aarch64/arm_neon.h (__AARCH64_LANE_CHECK): New.\n+\t(__aarch64_vget_lane_f64, __aarch64_vget_lane_s64,\n+\t__aarch64_vget_lane_u64, __aarch64_vset_lane_any, vdupd_lane_f64,\n+\tvdupd_lane_s64, vdupd_lane_u64, vext_f32, vext_f64, vext_p8, vext_p16,\n+\tvext_s8, vext_s16, vext_s32, vext_s64, vext_u8, vext_u16, vext_u32,\n+\tvext_u64, vextq_f32, vextq_f64, vextq_p8, vextq_p16, vextq_s8,\n+\tvextq_s16, vextq_s32, vextq_s64, vextq_u8, vextq_u16, vextq_u32,\n+\tvextq_u64, vmulq_lane_f64): Use __AARCH64_LANE_CHECK.\n+\n 2014-12-09  Alan Lawrence  <alan.lawrence@arm.com>\n \n \t* config/aarch64/arm_neon.h (__AARCH64_NUM_LANES, __aarch64_lane *2):"}, {"sha": "f52f524e55e25f73dfc6ebfa8c8fe302a7f3803b", "filename": "gcc/config/aarch64/aarch64-builtins.c", "status": "modified", "additions": 28, "deletions": 8, "changes": 36, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/661fce82a67307196182f81dd7d627d05aef4816/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/661fce82a67307196182f81dd7d627d05aef4816/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-builtins.c?ref=661fce82a67307196182f81dd7d627d05aef4816", "patch": "@@ -141,10 +141,6 @@ aarch64_types_binop_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_none, qualifier_none, qualifier_maybe_immediate };\n #define TYPES_BINOP (aarch64_types_binop_qualifiers)\n static enum aarch64_type_qualifiers\n-aarch64_types_binopv_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n-  = { qualifier_void, qualifier_none, qualifier_none };\n-#define TYPES_BINOPV (aarch64_types_binopv_qualifiers)\n-static enum aarch64_type_qualifiers\n aarch64_types_binopu_qualifiers[SIMD_MAX_BUILTIN_ARGS]\n   = { qualifier_unsigned, qualifier_unsigned, qualifier_unsigned };\n #define TYPES_BINOPU (aarch64_types_binopu_qualifiers)\n@@ -342,9 +338,12 @@ enum aarch64_builtins\n   AARCH64_BUILTIN_SET_FPSR,\n \n   AARCH64_SIMD_BUILTIN_BASE,\n+  AARCH64_SIMD_BUILTIN_LANE_CHECK,\n #include \"aarch64-simd-builtins.def\"\n-  AARCH64_SIMD_BUILTIN_MAX = AARCH64_SIMD_BUILTIN_BASE\n-\t\t\t      + ARRAY_SIZE (aarch64_simd_builtin_data),\n+  /* The first enum element which is based on an insn_data pattern.  */\n+  AARCH64_SIMD_PATTERN_START = AARCH64_SIMD_BUILTIN_LANE_CHECK + 1,\n+  AARCH64_SIMD_BUILTIN_MAX = AARCH64_SIMD_PATTERN_START\n+\t\t\t      + ARRAY_SIZE (aarch64_simd_builtin_data) - 1,\n   AARCH64_CRC32_BUILTIN_BASE,\n   AARCH64_CRC32_BUILTINS\n   AARCH64_CRC32_BUILTIN_MAX,\n@@ -685,7 +684,7 @@ aarch64_init_simd_builtin_scalar_types (void)\n static void\n aarch64_init_simd_builtins (void)\n {\n-  unsigned int i, fcode = AARCH64_SIMD_BUILTIN_BASE + 1;\n+  unsigned int i, fcode = AARCH64_SIMD_PATTERN_START;\n \n   aarch64_init_simd_builtin_types ();\n \n@@ -695,6 +694,15 @@ aarch64_init_simd_builtins (void)\n      system.  */\n   aarch64_init_simd_builtin_scalar_types ();\n  \n+  tree lane_check_fpr = build_function_type_list (void_type_node,\n+\t\t\t\t\t\t  intSI_type_node,\n+\t\t\t\t\t\t  intSI_type_node,\n+\t\t\t\t\t\t  NULL);\n+  aarch64_builtin_decls[AARCH64_SIMD_BUILTIN_LANE_CHECK] =\n+      add_builtin_function (\"__builtin_aarch64_im_lane_boundsi\", lane_check_fpr,\n+\t\t\t    AARCH64_SIMD_BUILTIN_LANE_CHECK, BUILT_IN_MD,\n+\t\t\t    NULL, NULL_TREE);\n+\n   for (i = 0; i < ARRAY_SIZE (aarch64_simd_builtin_data); i++, fcode++)\n     {\n       bool print_type_signature_p = false;\n@@ -974,8 +982,20 @@ aarch64_simd_expand_args (rtx target, int icode, int have_retval,\n rtx\n aarch64_simd_expand_builtin (int fcode, tree exp, rtx target)\n {\n+  if (fcode == AARCH64_SIMD_BUILTIN_LANE_CHECK)\n+    {\n+      tree nlanes = CALL_EXPR_ARG (exp, 0);\n+      gcc_assert (TREE_CODE (nlanes) == INTEGER_CST);\n+      rtx lane_idx = expand_normal (CALL_EXPR_ARG (exp, 1));\n+      if (CONST_INT_P (lane_idx))\n+\taarch64_simd_lane_bounds (lane_idx, 0, TREE_INT_CST_LOW (nlanes), exp);\n+      else\n+\terror (\"%Klane index must be a constant immediate\", exp);\n+      /* Don't generate any RTL.  */\n+      return const0_rtx;\n+    }\n   aarch64_simd_builtin_datum *d =\n-\t\t&aarch64_simd_builtin_data[fcode - (AARCH64_SIMD_BUILTIN_BASE + 1)];\n+\t\t&aarch64_simd_builtin_data[fcode - AARCH64_SIMD_PATTERN_START];\n   enum insn_code icode = d->code;\n   builtin_simd_arg args[SIMD_MAX_BUILTIN_ARGS];\n   int num_args = insn_data[d->code].n_operands;"}, {"sha": "9f479e8b07f5ca9ea9905c003e340f645c41d35a", "filename": "gcc/config/aarch64/aarch64-simd-builtins.def", "status": "modified", "additions": 0, "deletions": 2, "changes": 2, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/661fce82a67307196182f81dd7d627d05aef4816/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/661fce82a67307196182f81dd7d627d05aef4816/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd-builtins.def?ref=661fce82a67307196182f81dd7d627d05aef4816", "patch": "@@ -397,5 +397,3 @@\n   VAR1 (BINOPP, crypto_pmull, 0, di)\n   VAR1 (BINOPP, crypto_pmull, 0, v2di)\n \n-  /* Meta-op to check lane bounds of immediate in aarch64_expand_builtin.  */\n-  VAR1 (BINOPV, im_lane_bound, 0, si)"}, {"sha": "49a2e888ae7298eb7d80340e1c7b8b325e8b4979", "filename": "gcc/config/aarch64/aarch64-simd.md", "status": "modified", "additions": 0, "deletions": 13, "changes": 13, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/661fce82a67307196182f81dd7d627d05aef4816/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/661fce82a67307196182f81dd7d627d05aef4816/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Faarch64-simd.md?ref=661fce82a67307196182f81dd7d627d05aef4816", "patch": "@@ -4596,19 +4596,6 @@\n   [(set_attr \"type\" \"neon_ext<q>\")]\n )\n \n-;; This exists solely to check the arguments to the corresponding __builtin.\n-;; Used where we want an error for out-of-range indices which would otherwise\n-;; be silently wrapped (e.g. the mask to a __builtin_shuffle).\n-(define_expand \"aarch64_im_lane_boundsi\"\n-  [(match_operand:SI 0 \"immediate_operand\" \"i\")\n-   (match_operand:SI 1 \"immediate_operand\" \"i\")]\n-  \"TARGET_SIMD\"\n-{\n-  aarch64_simd_lane_bounds (operands[0], 0, INTVAL (operands[1]), NULL);\n-  DONE;\n-}\n-)\n-\n (define_insn \"aarch64_rev<REVERSE:rev_op><mode>\"\n   [(set (match_operand:VALL 0 \"register_operand\" \"=w\")\n \t(unspec:VALL [(match_operand:VALL 1 \"register_operand\" \"w\")]"}, {"sha": "d4d4ee973efa92a40048e2774b8cd803840860d7", "filename": "gcc/config/aarch64/arm_neon.h", "status": "modified", "additions": 34, "deletions": 33, "changes": 67, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/661fce82a67307196182f81dd7d627d05aef4816/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/661fce82a67307196182f81dd7d627d05aef4816/gcc%2Fconfig%2Faarch64%2Farm_neon.h", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Fconfig%2Faarch64%2Farm_neon.h?ref=661fce82a67307196182f81dd7d627d05aef4816", "patch": "@@ -436,7 +436,7 @@ typedef struct poly16x8x4_t\n   __aarch64_vget_lane_any (v2sf, , , __a, __b)\n #define __aarch64_vget_lane_f64(__a, __b) __extension__\t\\\n   ({\t\t\t\t\t\t\t\\\n-    __builtin_aarch64_im_lane_boundsi (__b, 1);\t\t\\\n+    __AARCH64_LANE_CHECK (__a, __b);\t\t\\\n     __a[0];\t\t\t\t\t\t\\\n   })\n \n@@ -453,7 +453,7 @@ typedef struct poly16x8x4_t\n   __aarch64_vget_lane_any (v2si, , ,__a, __b)\n #define __aarch64_vget_lane_s64(__a, __b) __extension__\t\\\n   ({\t\t\t\t\t\t\t\\\n-    __builtin_aarch64_im_lane_boundsi (__b, 1);\t\t\\\n+    __AARCH64_LANE_CHECK (__a, __b);\t\t\\\n     __a[0];\t\t\t\t\t\t\\\n   })\n \n@@ -465,7 +465,7 @@ typedef struct poly16x8x4_t\n   __aarch64_vget_lane_any (v2si, (uint32_t), (int32x2_t), __a, __b)\n #define __aarch64_vget_lane_u64(__a, __b) __extension__\t\\\n   ({\t\t\t\t\t\t\t\\\n-    __builtin_aarch64_im_lane_boundsi (__b, 1);\t\t\\\n+    __AARCH64_LANE_CHECK (__a, __b);\t\t\\\n     __a[0];\t\t\t\t\t\t\\\n   })\n \n@@ -607,6 +607,8 @@ typedef struct poly16x8x4_t\n /* Internal macro for lane indices.  */\n \n #define __AARCH64_NUM_LANES(__v) (sizeof (__v) / sizeof (__v[0]))\n+#define __AARCH64_LANE_CHECK(__vec, __idx)\t\\\n+\t__builtin_aarch64_im_lane_boundsi (__AARCH64_NUM_LANES (__vec), __idx)\n \n /* For big-endian, GCC's vector indices are the opposite way around\n    to the architectural lane indices used by Neon intrinsics.  */\n@@ -621,8 +623,7 @@ typedef struct poly16x8x4_t\n #define __aarch64_vset_lane_any(__elem, __vec, __index)\t\t\t\\\n   __extension__\t\t\t\t\t\t\t\t\\\n   ({\t\t\t\t\t\t\t\t\t\\\n-    __builtin_aarch64_im_lane_boundsi (__index,\t\t\t\\\n-       __AARCH64_NUM_LANES (__vec));\t\t\t\t\t\\\n+    __AARCH64_LANE_CHECK (__vec, __index);\t\t\t\t\\\n     __vec[__aarch64_lane (__vec, __index)] = __elem;\t\t\t\\\n     __vec;\t\t\t\t\t\t\t\t\\\n   })\n@@ -14764,21 +14765,21 @@ vdups_lane_u32 (uint32x2_t __a, const int __b)\n __extension__ static __inline float64_t __attribute__ ((__always_inline__))\n vdupd_lane_f64 (float64x1_t __a, const int __b)\n {\n-  __builtin_aarch64_im_lane_boundsi (__b, 1);\n+  __AARCH64_LANE_CHECK (__a, __b);\n   return __a[0];\n }\n \n __extension__ static __inline int64_t __attribute__ ((__always_inline__))\n vdupd_lane_s64 (int64x1_t __a, const int __b)\n {\n-  __builtin_aarch64_im_lane_boundsi (__b, 1);\n+  __AARCH64_LANE_CHECK (__a, __b);\n   return __a[0];\n }\n \n __extension__ static __inline uint64_t __attribute__ ((__always_inline__))\n vdupd_lane_u64 (uint64x1_t __a, const int __b)\n {\n-  __builtin_aarch64_im_lane_boundsi (__b, 1);\n+  __AARCH64_LANE_CHECK (__a, __b);\n   return __a[0];\n }\n \n@@ -14863,7 +14864,7 @@ vdupd_laneq_u64 (uint64x2_t __a, const int __b)\n __extension__ static __inline float32x2_t __attribute__ ((__always_inline__))\n vext_f32 (float32x2_t __a, float32x2_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 2);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint32x2_t) {2-__c, 3-__c});\n #else\n@@ -14874,14 +14875,14 @@ vext_f32 (float32x2_t __a, float32x2_t __b, __const int __c)\n __extension__ static __inline float64x1_t __attribute__ ((__always_inline__))\n vext_f64 (float64x1_t __a, float64x1_t __b, __const int __c)\n {\n+  __AARCH64_LANE_CHECK (__a, __c);\n   /* The only possible index to the assembler instruction returns element 0.  */\n-  __builtin_aarch64_im_lane_boundsi (__c, 1);\n   return __a;\n }\n __extension__ static __inline poly8x8_t __attribute__ ((__always_inline__))\n vext_p8 (poly8x8_t __a, poly8x8_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 8);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint8x8_t)\n       {8-__c, 9-__c, 10-__c, 11-__c, 12-__c, 13-__c, 14-__c, 15-__c});\n@@ -14894,7 +14895,7 @@ vext_p8 (poly8x8_t __a, poly8x8_t __b, __const int __c)\n __extension__ static __inline poly16x4_t __attribute__ ((__always_inline__))\n vext_p16 (poly16x4_t __a, poly16x4_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 4);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a,\n       (uint16x4_t) {4-__c, 5-__c, 6-__c, 7-__c});\n@@ -14906,7 +14907,7 @@ vext_p16 (poly16x4_t __a, poly16x4_t __b, __const int __c)\n __extension__ static __inline int8x8_t __attribute__ ((__always_inline__))\n vext_s8 (int8x8_t __a, int8x8_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 8);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint8x8_t)\n       {8-__c, 9-__c, 10-__c, 11-__c, 12-__c, 13-__c, 14-__c, 15-__c});\n@@ -14919,7 +14920,7 @@ vext_s8 (int8x8_t __a, int8x8_t __b, __const int __c)\n __extension__ static __inline int16x4_t __attribute__ ((__always_inline__))\n vext_s16 (int16x4_t __a, int16x4_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 4);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a,\n       (uint16x4_t) {4-__c, 5-__c, 6-__c, 7-__c});\n@@ -14931,7 +14932,7 @@ vext_s16 (int16x4_t __a, int16x4_t __b, __const int __c)\n __extension__ static __inline int32x2_t __attribute__ ((__always_inline__))\n vext_s32 (int32x2_t __a, int32x2_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 2);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint32x2_t) {2-__c, 3-__c});\n #else\n@@ -14942,15 +14943,15 @@ vext_s32 (int32x2_t __a, int32x2_t __b, __const int __c)\n __extension__ static __inline int64x1_t __attribute__ ((__always_inline__))\n vext_s64 (int64x1_t __a, int64x1_t __b, __const int __c)\n {\n+  __AARCH64_LANE_CHECK (__a, __c);\n   /* The only possible index to the assembler instruction returns element 0.  */\n-  __builtin_aarch64_im_lane_boundsi (__c, 1);\n   return __a;\n }\n \n __extension__ static __inline uint8x8_t __attribute__ ((__always_inline__))\n vext_u8 (uint8x8_t __a, uint8x8_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 8);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint8x8_t)\n       {8-__c, 9-__c, 10-__c, 11-__c, 12-__c, 13-__c, 14-__c, 15-__c});\n@@ -14963,7 +14964,7 @@ vext_u8 (uint8x8_t __a, uint8x8_t __b, __const int __c)\n __extension__ static __inline uint16x4_t __attribute__ ((__always_inline__))\n vext_u16 (uint16x4_t __a, uint16x4_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 4);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a,\n       (uint16x4_t) {4-__c, 5-__c, 6-__c, 7-__c});\n@@ -14975,7 +14976,7 @@ vext_u16 (uint16x4_t __a, uint16x4_t __b, __const int __c)\n __extension__ static __inline uint32x2_t __attribute__ ((__always_inline__))\n vext_u32 (uint32x2_t __a, uint32x2_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 2);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint32x2_t) {2-__c, 3-__c});\n #else\n@@ -14986,15 +14987,15 @@ vext_u32 (uint32x2_t __a, uint32x2_t __b, __const int __c)\n __extension__ static __inline uint64x1_t __attribute__ ((__always_inline__))\n vext_u64 (uint64x1_t __a, uint64x1_t __b, __const int __c)\n {\n+  __AARCH64_LANE_CHECK (__a, __c);\n   /* The only possible index to the assembler instruction returns element 0.  */\n-  __builtin_aarch64_im_lane_boundsi (__c, 1);\n   return __a;\n }\n \n __extension__ static __inline float32x4_t __attribute__ ((__always_inline__))\n vextq_f32 (float32x4_t __a, float32x4_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 4);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a,\n       (uint32x4_t) {4-__c, 5-__c, 6-__c, 7-__c});\n@@ -15006,7 +15007,7 @@ vextq_f32 (float32x4_t __a, float32x4_t __b, __const int __c)\n __extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n vextq_f64 (float64x2_t __a, float64x2_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 2);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint64x2_t) {2-__c, 3-__c});\n #else\n@@ -15017,7 +15018,7 @@ vextq_f64 (float64x2_t __a, float64x2_t __b, __const int __c)\n __extension__ static __inline poly8x16_t __attribute__ ((__always_inline__))\n vextq_p8 (poly8x16_t __a, poly8x16_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 16);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint8x16_t)\n       {16-__c, 17-__c, 18-__c, 19-__c, 20-__c, 21-__c, 22-__c, 23-__c,\n@@ -15032,7 +15033,7 @@ vextq_p8 (poly8x16_t __a, poly8x16_t __b, __const int __c)\n __extension__ static __inline poly16x8_t __attribute__ ((__always_inline__))\n vextq_p16 (poly16x8_t __a, poly16x8_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 8);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint16x8_t)\n       {8-__c, 9-__c, 10-__c, 11-__c, 12-__c, 13-__c, 14-__c, 15-__c});\n@@ -15045,7 +15046,7 @@ vextq_p16 (poly16x8_t __a, poly16x8_t __b, __const int __c)\n __extension__ static __inline int8x16_t __attribute__ ((__always_inline__))\n vextq_s8 (int8x16_t __a, int8x16_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 16);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint8x16_t)\n       {16-__c, 17-__c, 18-__c, 19-__c, 20-__c, 21-__c, 22-__c, 23-__c,\n@@ -15060,7 +15061,7 @@ vextq_s8 (int8x16_t __a, int8x16_t __b, __const int __c)\n __extension__ static __inline int16x8_t __attribute__ ((__always_inline__))\n vextq_s16 (int16x8_t __a, int16x8_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 8);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint16x8_t)\n       {8-__c, 9-__c, 10-__c, 11-__c, 12-__c, 13-__c, 14-__c, 15-__c});\n@@ -15073,7 +15074,7 @@ vextq_s16 (int16x8_t __a, int16x8_t __b, __const int __c)\n __extension__ static __inline int32x4_t __attribute__ ((__always_inline__))\n vextq_s32 (int32x4_t __a, int32x4_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 4);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a,\n       (uint32x4_t) {4-__c, 5-__c, 6-__c, 7-__c});\n@@ -15085,7 +15086,7 @@ vextq_s32 (int32x4_t __a, int32x4_t __b, __const int __c)\n __extension__ static __inline int64x2_t __attribute__ ((__always_inline__))\n vextq_s64 (int64x2_t __a, int64x2_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 2);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint64x2_t) {2-__c, 3-__c});\n #else\n@@ -15096,7 +15097,7 @@ vextq_s64 (int64x2_t __a, int64x2_t __b, __const int __c)\n __extension__ static __inline uint8x16_t __attribute__ ((__always_inline__))\n vextq_u8 (uint8x16_t __a, uint8x16_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 16);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint8x16_t)\n       {16-__c, 17-__c, 18-__c, 19-__c, 20-__c, 21-__c, 22-__c, 23-__c,\n@@ -15111,7 +15112,7 @@ vextq_u8 (uint8x16_t __a, uint8x16_t __b, __const int __c)\n __extension__ static __inline uint16x8_t __attribute__ ((__always_inline__))\n vextq_u16 (uint16x8_t __a, uint16x8_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 8);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint16x8_t)\n       {8-__c, 9-__c, 10-__c, 11-__c, 12-__c, 13-__c, 14-__c, 15-__c});\n@@ -15124,7 +15125,7 @@ vextq_u16 (uint16x8_t __a, uint16x8_t __b, __const int __c)\n __extension__ static __inline uint32x4_t __attribute__ ((__always_inline__))\n vextq_u32 (uint32x4_t __a, uint32x4_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 4);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a,\n       (uint32x4_t) {4-__c, 5-__c, 6-__c, 7-__c});\n@@ -15136,7 +15137,7 @@ vextq_u32 (uint32x4_t __a, uint32x4_t __b, __const int __c)\n __extension__ static __inline uint64x2_t __attribute__ ((__always_inline__))\n vextq_u64 (uint64x2_t __a, uint64x2_t __b, __const int __c)\n {\n-  __builtin_aarch64_im_lane_boundsi (__c, 2);\n+  __AARCH64_LANE_CHECK (__a, __c);\n #ifdef __AARCH64EB__\n   return __builtin_shuffle (__b, __a, (uint64x2_t) {2-__c, 3-__c});\n #else\n@@ -18965,7 +18966,7 @@ vmulq_lane_f32 (float32x4_t __a, float32x2_t __b, const int __lane)\n __extension__ static __inline float64x2_t __attribute__ ((__always_inline__))\n vmulq_lane_f64 (float64x2_t __a, float64x1_t __b, const int __lane)\n {\n-  __builtin_aarch64_im_lane_boundsi (__lane, 1);\n+  __AARCH64_LANE_CHECK (__a, __lane);\n   return __a * __b[0];\n }\n "}, {"sha": "24822feb40974f59052423bd24cb0c240c7195d4", "filename": "gcc/testsuite/ChangeLog", "status": "modified", "additions": 4, "deletions": 0, "changes": 4, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/661fce82a67307196182f81dd7d627d05aef4816/gcc%2Ftestsuite%2FChangeLog", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/661fce82a67307196182f81dd7d627d05aef4816/gcc%2Ftestsuite%2FChangeLog", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2FChangeLog?ref=661fce82a67307196182f81dd7d627d05aef4816", "patch": "@@ -1,3 +1,7 @@\n+2014-12-09  Alan Lawrence  <alan.lawrence@arm.com>\n+\n+\t* gcc.target/aarch64/simd/vset_lane_s16_const_1.c: New test.\n+\n 2014-12-09  Alan Lawrence  <alan.lawrence@arm.com>\n \n \t* gcc.target/aarch64/vld1_lane-o0.c: New test."}, {"sha": "b28d67f74b076412e5dc4982449735aa227322bb", "filename": "gcc/testsuite/gcc.target/aarch64/simd/vset_lane_s16_const_1.c", "status": "added", "additions": 15, "deletions": 0, "changes": 15, "blob_url": "https://github.com/Rust-GCC/gccrs/blob/661fce82a67307196182f81dd7d627d05aef4816/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvset_lane_s16_const_1.c", "raw_url": "https://github.com/Rust-GCC/gccrs/raw/661fce82a67307196182f81dd7d627d05aef4816/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvset_lane_s16_const_1.c", "contents_url": "https://api.github.com/repos/Rust-GCC/gccrs/contents/gcc%2Ftestsuite%2Fgcc.target%2Faarch64%2Fsimd%2Fvset_lane_s16_const_1.c?ref=661fce82a67307196182f81dd7d627d05aef4816", "patch": "@@ -0,0 +1,15 @@\n+/* Test error message when passing a non-constant value in as a lane index.  */\n+\n+/* { dg-do assemble } */\n+/* { dg-options \"-std=c99\" } */\n+\n+#include <arm_neon.h>\n+\n+int\n+main (int argc, char **argv)\n+{\n+  int16x4_t in = vcreate_s16 (0xdeadbeef00000000ULL);\n+  /* { dg-error \"must be a constant immediate\" \"\" { target *-*-* } 0 } */\n+  int16x4_t out = vset_lane_s16 (65535, in, argc);\n+  return vget_lane_s16 (out, 0);\n+}"}]}